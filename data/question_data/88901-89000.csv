,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there a reflexive Banach space whose ball is not the convex hull of its extreme points?,Is there a reflexive Banach space whose ball is not the convex hull of its extreme points?,,"Let $X$ be a reflexive Banach space. Then the convex hull of the extreme points of the unit ball is weakly dense by the Krein-Milman theorem and Kakutani's theorem. My question is, if there is an example of a reflexive Banach space $X$ whose unit ball does not equal the convex hull of its extreme points? Such an example $X$ must be infinite-dimensional and can't be strictly convex.","Let be a reflexive Banach space. Then the convex hull of the extreme points of the unit ball is weakly dense by the Krein-Milman theorem and Kakutani's theorem. My question is, if there is an example of a reflexive Banach space whose unit ball does not equal the convex hull of its extreme points? Such an example must be infinite-dimensional and can't be strictly convex.",X X X,"['functional-analysis', 'banach-spaces']"
1,Distance of the product of two nocommuting positive operators from normal operators,Distance of the product of two nocommuting positive operators from normal operators,,"This question is inspired by problem . Given two noncommuting positive operators $A,B\in B(\mathcal{H}).$ Let $\mathcal{N}$ denote the subspace of all normal operators, i.e. the operators $N$ such that $NN^*=N^*N.$ Determine $$d(AB,\mathcal{N}):=\inf\{\|AB-N\|\,:\,N\in \mathcal{N}\}$$ My contribution: In one of the answers to problem it was shown that $$\inf\{\|AB-N\|\,:\,N=N^*,\ N\in B(\mathcal{H})\}={1\over 2}\|AB-BA\|$$ hence $$d(AB,\mathcal{N})\le {1\over 2}\|AB-BA\|$$ Moreover $AB$ is not normal. Indeed, since $AB\neq BA,$ the operator $AB$ is not self-adjoint. Furthermore we have $$\{0\}\cup \sigma(AB)=\{0\}\cup \sigma(A^{1/2}BA^{1/2})\subset [0,\infty)$$ Therefore $$\sigma(AB)\subset [0,\infty)$$ If $AB$ was normal then $AB$ would be self-adjoint by this , a contradiction. Once we know that $AB$ is not normal, the number $d(AB,\mathcal{N})$ is strictly positive as the subspace $\mathcal{N}$ is closed in $B(\mathcal{H}).$ I wonder if $$d(AB,\mathcal{N})={1\over 2}\|AB-BA\|$$ or there can be the strict inequality.","This question is inspired by problem . Given two noncommuting positive operators Let denote the subspace of all normal operators, i.e. the operators such that Determine My contribution: In one of the answers to problem it was shown that hence Moreover is not normal. Indeed, since the operator is not self-adjoint. Furthermore we have Therefore If was normal then would be self-adjoint by this , a contradiction. Once we know that is not normal, the number is strictly positive as the subspace is closed in I wonder if or there can be the strict inequality.","A,B\in B(\mathcal{H}). \mathcal{N} N NN^*=N^*N. d(AB,\mathcal{N}):=\inf\{\|AB-N\|\,:\,N\in \mathcal{N}\} \inf\{\|AB-N\|\,:\,N=N^*,\ N\in B(\mathcal{H})\}={1\over 2}\|AB-BA\| d(AB,\mathcal{N})\le {1\over 2}\|AB-BA\| AB AB\neq BA, AB \{0\}\cup \sigma(AB)=\{0\}\cup \sigma(A^{1/2}BA^{1/2})\subset [0,\infty) \sigma(AB)\subset [0,\infty) AB AB AB d(AB,\mathcal{N}) \mathcal{N} B(\mathcal{H}). d(AB,\mathcal{N})={1\over 2}\|AB-BA\|","['linear-algebra', 'functional-analysis']"
2,A problem concerning continous function spaces and divergence of series,A problem concerning continous function spaces and divergence of series,,"Saw this on a website, having no clue. Let $C([0,1])$ be the space of real continous functions on the interval $[0,1].$ Let $\{a_n\}$ be a series such that $\sum_{n=1}^{\infty}\dfrac1{a_n}$ diverges, $a_0=0,$ and $a_n\to\infty$ when $n\to\infty.$ Show that, if $\int_0^1x^{a_n}f(x)dx=0$ for any $n\in \mathbb{N},$ then $f\equiv 0.$ I know that the proposition to be proved is equivalent to the completeness of the system $\{x^{a_n}\}$ in $C([0,1])$ with the integral(convolution) inner product. I can't see the relation between the series' divergence and the system's completeness. An example is that when $a_n=n,$ the series is well-defined and divergent, and we get the desired result by the Weierstrass Approximation Theorem. But there is still no clue when $a_n$ is a general series. I can't even construct an example when $a_n$ converges and a non-vanishing function that satisfies the orthogonal condition. Any thoughts, solutions, and other clues are welcome.","Saw this on a website, having no clue. Let be the space of real continous functions on the interval Let be a series such that diverges, and when Show that, if for any then I know that the proposition to be proved is equivalent to the completeness of the system in with the integral(convolution) inner product. I can't see the relation between the series' divergence and the system's completeness. An example is that when the series is well-defined and divergent, and we get the desired result by the Weierstrass Approximation Theorem. But there is still no clue when is a general series. I can't even construct an example when converges and a non-vanishing function that satisfies the orthogonal condition. Any thoughts, solutions, and other clues are welcome.","C([0,1]) [0,1]. \{a_n\} \sum_{n=1}^{\infty}\dfrac1{a_n} a_0=0, a_n\to\infty n\to\infty. \int_0^1x^{a_n}f(x)dx=0 n\in \mathbb{N}, f\equiv 0. \{x^{a_n}\} C([0,1]) a_n=n, a_n a_n","['functional-analysis', 'analysis', 'continuity', 'orthogonality']"
3,Strength of Krein-Milman vs Dependent Choice,Strength of Krein-Milman vs Dependent Choice,,"I am wondering about the relationship between the Krein-Milman theorem (KM) and some other weak forms of the axiom of choice (AC). I currently basically know the following: KM + BPI (Boolean prime ideal theorem) implies AC. (Bell, Fremlin, A geometric form of the axiom of choice ) I also know the following: DC (dependent choice) + BPI does not imply AC (consequently DC does not imply KM). (Pincus, Adding Dependent Choice to the Prime Ideal Theorem ) I have two closely related questions about these axioms, but have been unable to find an answer. These are Does KM imply DC? Is KM consistent with AD (axiom of determinacy)? If we replace ''KM'' with ''DC'' in the above questions, the answer is always ""yes"" (for question 1, this is of course trivial). So my underlying motivation is to figure out how similar KM and DC are.","I am wondering about the relationship between the Krein-Milman theorem (KM) and some other weak forms of the axiom of choice (AC). I currently basically know the following: KM + BPI (Boolean prime ideal theorem) implies AC. (Bell, Fremlin, A geometric form of the axiom of choice ) I also know the following: DC (dependent choice) + BPI does not imply AC (consequently DC does not imply KM). (Pincus, Adding Dependent Choice to the Prime Ideal Theorem ) I have two closely related questions about these axioms, but have been unable to find an answer. These are Does KM imply DC? Is KM consistent with AD (axiom of determinacy)? If we replace ''KM'' with ''DC'' in the above questions, the answer is always ""yes"" (for question 1, this is of course trivial). So my underlying motivation is to figure out how similar KM and DC are.",,"['functional-analysis', 'set-theory', 'axiom-of-choice']"
4,Why is the multiplication map $C_c^\infty \times C^\infty \to C_c^\infty$ not continuous?,Why is the multiplication map  not continuous?,C_c^\infty \times C^\infty \to C_c^\infty,"Let $C_c^\infty(\mathbf{R}^d)$ be the family of all compactly supported smooth functions equipped with the test function topology. Let $C^\infty_{\text{loc}}(\mathbf{R}^d)$ be the class of all smooth functions, equipped with the topology of locally uniform convergence of the function and all it's derivatives. It is stated in Treves, Chapter 41 that the multiplication operator $$ C^\infty_c(\mathbf{R}^d) \times C^\infty_{\text{loc}}(\mathbf{R}^d) \to C_c^\infty(\mathbf{R}^d) $$ is not continuous. It is certainly separately continuous, and  sequentially continuous in each variable. But I can't find a resource that proves that the map is not continuous. Why is this the case?","Let be the family of all compactly supported smooth functions equipped with the test function topology. Let be the class of all smooth functions, equipped with the topology of locally uniform convergence of the function and all it's derivatives. It is stated in Treves, Chapter 41 that the multiplication operator is not continuous. It is certainly separately continuous, and  sequentially continuous in each variable. But I can't find a resource that proves that the map is not continuous. Why is this the case?",C_c^\infty(\mathbf{R}^d) C^\infty_{\text{loc}}(\mathbf{R}^d)  C^\infty_c(\mathbf{R}^d) \times C^\infty_{\text{loc}}(\mathbf{R}^d) \to C_c^\infty(\mathbf{R}^d) ,"['functional-analysis', 'distribution-theory', 'topological-vector-spaces']"
5,Trace distance of tensor products of operators.,Trace distance of tensor products of operators.,,"Let us focuse on the set of positive operators of trace 1 acting on some infinite dimensional Hilbert space, call it $S(H_{1})$ where $H$ is the mentioned Hilbert space. Let $S(H_{2})$ be another such space and now let $\rho \in S(H_{1})$ and $\sigma \in S(H_{2})$ . The tensor product operator $\rho\otimes\sigma \in S(H_{1})\otimes S(H_{2})$ . Now, let $U$ be some unitary map that mapping, i.e. an automorphism on $S(H_{1})\otimes S(H_{2})$ , and $\Lambda$ a completely positive map acting on $S(H_{1})\otimes S(H_{2})$ acting non trivially only on the subspace $S(H_{2})$ of $S(H_{1})\otimes S(H_{2})$ . i.e. for $\rho\otimes\sigma \in S(H_{1})\otimes S(H_{2})$ $$\Lambda(\rho\otimes\sigma) = \rho\otimes\Lambda_{2}\big(\sigma\big)$$ where $\Lambda_{2}$ is acompletly positive map. I would like to know if there are any helpful results that help in the treatment of trace distances of the following sort. I know that the trace has a nice seperability property i.e. $Tr(A\otimes B) = Tr(A)Tr(B)$ and the trace distance also shares this $\|A \otimes B \|_{1} = \|A\|_{1}\|B\|_{1}$ . I am wondering if there is anything, some separability or linearity property, that might render the following into a product of traces which are not traces of tensor product states. $$\| U\Big(\rho\otimes \sigma\Big) - \Lambda\Big( U\Big(\rho\otimes \sigma\Big)\Big)\|_{1}$$ where the trace distance is defined as follows $$ \|A-B\|_{1} = \frac{1}{2}Tr\{\sqrt{(A-B)^{\dagger}(A-B)}\}$$ Thanks in advance.","Let us focuse on the set of positive operators of trace 1 acting on some infinite dimensional Hilbert space, call it where is the mentioned Hilbert space. Let be another such space and now let and . The tensor product operator . Now, let be some unitary map that mapping, i.e. an automorphism on , and a completely positive map acting on acting non trivially only on the subspace of . i.e. for where is acompletly positive map. I would like to know if there are any helpful results that help in the treatment of trace distances of the following sort. I know that the trace has a nice seperability property i.e. and the trace distance also shares this . I am wondering if there is anything, some separability or linearity property, that might render the following into a product of traces which are not traces of tensor product states. where the trace distance is defined as follows Thanks in advance.",S(H_{1}) H S(H_{2}) \rho \in S(H_{1}) \sigma \in S(H_{2}) \rho\otimes\sigma \in S(H_{1})\otimes S(H_{2}) U S(H_{1})\otimes S(H_{2}) \Lambda S(H_{1})\otimes S(H_{2}) S(H_{2}) S(H_{1})\otimes S(H_{2}) \rho\otimes\sigma \in S(H_{1})\otimes S(H_{2}) \Lambda(\rho\otimes\sigma) = \rho\otimes\Lambda_{2}\big(\sigma\big) \Lambda_{2} Tr(A\otimes B) = Tr(A)Tr(B) \|A \otimes B \|_{1} = \|A\|_{1}\|B\|_{1} \| U\Big(\rho\otimes \sigma\Big) - \Lambda\Big( U\Big(\rho\otimes \sigma\Big)\Big)\|_{1}  \|A-B\|_{1} = \frac{1}{2}Tr\{\sqrt{(A-B)^{\dagger}(A-B)}\},"['linear-algebra', 'functional-analysis', 'reference-request', 'operator-theory', 'quantum-computation']"
6,Derivative of a map $f:\mathbb R \to \ell^2 $ to a separable Hilbert space vs derivative of each component of the Hilbert basis.,Derivative of a map  to a separable Hilbert space vs derivative of each component of the Hilbert basis.,f:\mathbb R \to \ell^2 ,"${\newcommand{\R}{\mathbb{R}}}$ Let $\ell^2 $ be the Hilbert space of square summable sequences of real numbers. Consider a map $f: \R \to \ell^2$ ,  that has components $f_n:\R\to \R$ , i.e. $f(t)=\{f_n(t)\}_{n\in \mathbb N}$ so that $\sum_n(f_n(t))^2<+\infty$ . Motivation If the derivative $\frac d {dt}f(t)$ exists, then it is given by $\{\partial_tf_n(t)\}_n\in \ell^2$ componentwise. However the existence of the derivatives $\{\partial_tf_n(t)\}_n\in \ell^2$ does not imply the existence of $\frac d {dt}f(t)$ , because we need the remainder $$ R_n(t,h) = f_n(t+h) - f_n(t) - \partial_t f_n(t)h $$ to be $o(|h|)$ for $h\to 0$ uniformely in $n$ . I would like to know if adding a Sobolev-type assumption we obtain existence of the derivative, precisely: Question: Let $f$ as above, and suppose that the componentwise derivativee $\{\partial^{(k)}_t f_n\}_n \in \ell^2$ and the $\partial^{(k)}_t f_n$ are continuous for $k\leq 2$ . Moreover assume that $$\int_\R \sum_n n^2|\partial_t^{(k)}f_n(t)|^2 dt <\infty$$ Does this  imply that the derivative $\frac {d}{dt}f(t) \in \ell^2$ exists for all $t$ ? (and thus coincide with $\{\partial_t f_n\}_n$ ).","Let be the Hilbert space of square summable sequences of real numbers. Consider a map ,  that has components , i.e. so that . Motivation If the derivative exists, then it is given by componentwise. However the existence of the derivatives does not imply the existence of , because we need the remainder to be for uniformely in . I would like to know if adding a Sobolev-type assumption we obtain existence of the derivative, precisely: Question: Let as above, and suppose that the componentwise derivativee and the are continuous for . Moreover assume that Does this  imply that the derivative exists for all ? (and thus coincide with ).","{\newcommand{\R}{\mathbb{R}}} \ell^2  f: \R \to \ell^2 f_n:\R\to \R f(t)=\{f_n(t)\}_{n\in \mathbb N} \sum_n(f_n(t))^2<+\infty \frac d {dt}f(t) \{\partial_tf_n(t)\}_n\in \ell^2 \{\partial_tf_n(t)\}_n\in \ell^2 \frac d {dt}f(t)  R_n(t,h) = f_n(t+h) - f_n(t) - \partial_t f_n(t)h  o(|h|) h\to 0 n f \{\partial^{(k)}_t f_n\}_n \in \ell^2 \partial^{(k)}_t f_n k\leq 2 \int_\R \sum_n n^2|\partial_t^{(k)}f_n(t)|^2 dt <\infty \frac {d}{dt}f(t) \in \ell^2 t \{\partial_t f_n\}_n","['functional-analysis', 'measure-theory', 'hilbert-spaces', 'lebesgue-measure', 'sobolev-spaces']"
7,Examples of non-self-induced algebras,Examples of non-self-induced algebras,,"Let $A$ be a (possibly non-unital) algebra over $\mathbb C$ .  We say that $A$ is self-induced if the product map $m:A \otimes_A A \rightarrow A$ is an isomorphism.  Here $A \otimes_A A$ is the balanced tensor product, the quotient of $A\otimes A$ by the linear span of elements of the form $ab\otimes c - a\otimes bc$ . This notion seems to have been introduced by Gronbaek in Morita equivalence for self-induced Banach algebras and was further studied by Meyer in Smooth and rough modules over self-induced algebras .  If $A$ is unital, or more generally, has local, one-sided, units, then $A$ is self-induced.  I am interested in non-trivial examples of $A$ which are not self-induced. What is an example of $A$ which is not self-induced, but such that the product map is surjective, and such that the product is non-degenerate (so for each non-zero $a\in A$ there are $b,c\in A$ with $ba\not=0, ac\not=0$ ). I have tagged this functional analysis, as the notion seems to have arisen in the context of topological algebras (and so people working in this area might know examples).  But the question does not ask about the topological case.  I would be interested to know if this idea is studied in non-topological contexts under a different name?","Let be a (possibly non-unital) algebra over .  We say that is self-induced if the product map is an isomorphism.  Here is the balanced tensor product, the quotient of by the linear span of elements of the form . This notion seems to have been introduced by Gronbaek in Morita equivalence for self-induced Banach algebras and was further studied by Meyer in Smooth and rough modules over self-induced algebras .  If is unital, or more generally, has local, one-sided, units, then is self-induced.  I am interested in non-trivial examples of which are not self-induced. What is an example of which is not self-induced, but such that the product map is surjective, and such that the product is non-degenerate (so for each non-zero there are with ). I have tagged this functional analysis, as the notion seems to have arisen in the context of topological algebras (and so people working in this area might know examples).  But the question does not ask about the topological case.  I would be interested to know if this idea is studied in non-topological contexts under a different name?","A \mathbb C A m:A \otimes_A A \rightarrow A A \otimes_A A A\otimes A ab\otimes c - a\otimes bc A A A A a\in A b,c\in A ba\not=0, ac\not=0","['abstract-algebra', 'functional-analysis', 'algebras']"
8,Decay of convolution with measure,Decay of convolution with measure,,"In order to get some estimates of certain integral my teacher has used the following lemma that, in his opinion it is trivial: Let us consider $\nu$ a radon measure  in $\mathbb{R}^2$ such that $\int_{\mathbb{R}^2}d\nu=0$ . If we define $F(x)=g*\nu(x)$ with $g(x)=\ln|x|$ , then there exists $C>0$ such that $|F(x)|$ deacays like $C\frac{1}{|x|}$ . Unfortunately, it doesn't seem so obvious to me. Does anyone know of any proof  a result like this? I thought that I would get it by using Poincare's inequality, but it seems to me that it doesn't work. Thanks in advance.","In order to get some estimates of certain integral my teacher has used the following lemma that, in his opinion it is trivial: Let us consider a radon measure  in such that . If we define with , then there exists such that deacays like . Unfortunately, it doesn't seem so obvious to me. Does anyone know of any proof  a result like this? I thought that I would get it by using Poincare's inequality, but it seems to me that it doesn't work. Thanks in advance.",\nu \mathbb{R}^2 \int_{\mathbb{R}^2}d\nu=0 F(x)=g*\nu(x) g(x)=\ln|x| C>0 |F(x)| C\frac{1}{|x|},"['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'partial-differential-equations']"
9,Generalized Ladyzhenskaya inequality for $n=2$.,Generalized Ladyzhenskaya inequality for .,n=2,"Is there a way to prove the following generalized Ladyzhenskaya inequality for $n=2$ ? The statement goes as follows: Let $n = 2$ and $0 < p < \infty$ . Then, for all $f \in C^1(\mathbb{R}^2)$ , we have $$\|f\|^{p+2}_{L^{p+2}} \leq C \|f\|_{L^2}^{2} \|\nabla f\|_{L^2}^p$$ with the constant $C$ uniform in $f$ . We know that for $p = 2$ , this reduces to the usual Ladyzhenskaya inequality: $$\|f\|_{L^4}^2 \leq \|u\|_{L^2} \|\nabla f\|_{L^2} $$ and a proof of this can be readily found online, as such in this blog post . However, I find it hard to generalize this proof for $L^{p+2}$ . Although this might follow from the general class of Gagliardo–Nirenberg interpolation inequalities, I was thinking if there is a somewhat elementary way of doing this. Any help would be appreciated!","Is there a way to prove the following generalized Ladyzhenskaya inequality for ? The statement goes as follows: Let and . Then, for all , we have with the constant uniform in . We know that for , this reduces to the usual Ladyzhenskaya inequality: and a proof of this can be readily found online, as such in this blog post . However, I find it hard to generalize this proof for . Although this might follow from the general class of Gagliardo–Nirenberg interpolation inequalities, I was thinking if there is a somewhat elementary way of doing this. Any help would be appreciated!",n=2 n = 2 0 < p < \infty f \in C^1(\mathbb{R}^2) \|f\|^{p+2}_{L^{p+2}} \leq C \|f\|_{L^2}^{2} \|\nabla f\|_{L^2}^p C f p = 2 \|f\|_{L^4}^2 \leq \|u\|_{L^2} \|\nabla f\|_{L^2}  L^{p+2},"['functional-analysis', 'inequality', 'partial-differential-equations']"
10,Proof that this is a Norm,Proof that this is a Norm,,"Let $k$ be any natural number. I'd like to prove that the expression $$ \|f\|_{C^{k}[a, b]}=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right| $$ is a norm on $C^{k}[a, b]$ . What I have accomplished: For $f \in C^k[a, b]$ we have $0 \leq\|f\|<\infty$ . Positivity: $\|f\| \geq 0$ is obvious and $\|f\|=0$ is equivalent to $$ \max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|=0, $$ i.e. $f(t)=0$ for all $t \in [a,b]$ , i.e. $f=0.$ Absolute homogeneity: $$ \|\lambda f\|=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} \lambda f(t)}{d t^{l}}\right|=|\lambda| \max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|=|\lambda|\|f\| . $$ Triangle inequality: For each $t \in [a,b]$ , the following applies $$ \left|\frac{d^{l}}{d t^{l}} (f(t)+g(t))\right | \leq\left|\frac{d^{l}}{d t^{l}} f(t)\right |+\left|\frac{d^{l}}{d t^{l}} g(t)\right | \leq\|f\|+\|g\|, $$ from which follows $$ \|f+g\|=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l}}{d t^{l}} (f(t)+g(t))\right | \leq|f\|+\|g\| .  $$ I'd like to know if this is flawless or if there is room for enhancement. Thanks in advance!","Let be any natural number. I'd like to prove that the expression is a norm on . What I have accomplished: For we have . Positivity: is obvious and is equivalent to i.e. for all , i.e. Absolute homogeneity: Triangle inequality: For each , the following applies from which follows I'd like to know if this is flawless or if there is room for enhancement. Thanks in advance!","k 
\|f\|_{C^{k}[a, b]}=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|
 C^{k}[a, b] f \in C^k[a, b] 0 \leq\|f\|<\infty \|f\| \geq 0 \|f\|=0 
\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|=0,
 f(t)=0 t \in [a,b] f=0. 
\|\lambda f\|=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} \lambda f(t)}{d t^{l}}\right|=|\lambda| \max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|=|\lambda|\|f\| .
 t \in [a,b] 
\left|\frac{d^{l}}{d t^{l}} (f(t)+g(t))\right | \leq\left|\frac{d^{l}}{d t^{l}} f(t)\right |+\left|\frac{d^{l}}{d t^{l}} g(t)\right | \leq\|f\|+\|g\|,
 
\|f+g\|=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l}}{d t^{l}} (f(t)+g(t))\right | \leq|f\|+\|g\| . 
","['functional-analysis', 'solution-verification', 'normed-spaces']"
11,"If $|\phi| \approx |\psi|$, then Is $|| \mathcal{F}^{-1} [\phi \hat{f}] ||_p \leq C || \mathcal{F}^{-1} [\psi \hat{f}] ||_p$?","If , then Is ?",|\phi| \approx |\psi| || \mathcal{F}^{-1} [\phi \hat{f}] ||_p \leq C || \mathcal{F}^{-1} [\psi \hat{f}] ||_p,"$\underline{\textbf{Background:}}$ I am currently trying to estimate the following $L^p$ norm: \begin{align} & \Big{|}\Big{|} A \Big{|}\Big{|}_p \\ & = \int_{\mathbb{R}^d} \Big{|} \int_{|\xi|<2t^{1/2}}  \exp\Big{(} ix\cdot\xi  \Big{)} \Big{(}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{)} \exp\Big{(}-\frac{|\xi|^2}{2} \big{(}1+i\sqrt{ \frac{4t}{|\xi|^2} -1  }\big{)}\Big{)} \text{d}\xi \Big{|}^p \text{d}x  \Big{\} }^{1/p} , \end{align} where $t > 0$ , and $d \geq 2$ . The above complicated function was obtained from part of an inverse Fourier transform (which needs to be split into small and large $|\xi|$ , as the $\sqrt{\frac{4t}{|\xi|^2} - 1}$ part blows up at $|\xi| = 2t^{1/2}$ ). As such, I would like to treat it as a Fourier transform still (formally), by rewriting as \begin{align} \Big{|}\Big{|} A \Big{|}\Big{|}_p = \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[} \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \hat{f} \Big{]}  \Big{|}\Big{|}_p , \end{align} where \begin{align} \hat{f}(t,\xi) := \Big{(}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{)} \exp\Big{(}-\frac{|\xi|^2}{2} \Big{)}\Bigg{|}_{|\xi|<2t^{1/2}} \end{align} (I hope to get around the issue of smoothness by taking the limit of $\hat{f}$ times some test function.) $\underline{\textbf{Problem:}}$ I would like to simplify the problem further. Note that the multiplier in $\hat{f}$ is pointwise bounded above and below: \begin{align} C^{-1} \frac{t^{1/2}}{|\xi|} \leq  \Big{|}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{|} \leq C \frac{t^{1/2}}{|\xi|}, \text{ for all } C \geq 4. \end{align} I am wondering if this permits the following estimate: \begin{align} & \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[} \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \hat{f} \Big{]}  \Big{|}\Big{|}_p \\ & \leq  C \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[}   \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \frac{t^{1/2}}{|\xi|} \exp\Big{(}-\frac{|\xi|^2}{2} \Big{)}  \Big{]}  \Big{|}\Big{|}_p. \end{align} In more general terms, if we have pointwise boundedness above and below for a Fourier multiplier \begin{align} C^{-1}\psi(\xi) \leq \phi(\xi) \leq C\psi(\xi), \text{ for all } \xi \in \mathbb{R}^d, \end{align} can we then say \begin{align} \Big{|}\Big{|} \mathcal{F}^{-1} \Big{[} \phi \hat{g} \Big{]} \Big{|}\Big{|}_p \leq C \Big{|}\Big{|} \mathcal{F}^{-1} \Big{[} \psi \hat{g} \Big{]} \Big{|}\Big{|}_p, \end{align} for sufficiently well behaved $\hat{g}, \phi, \psi$ ? \begin{align} \end{align}","I am currently trying to estimate the following norm: where , and . The above complicated function was obtained from part of an inverse Fourier transform (which needs to be split into small and large , as the part blows up at ). As such, I would like to treat it as a Fourier transform still (formally), by rewriting as where (I hope to get around the issue of smoothness by taking the limit of times some test function.) I would like to simplify the problem further. Note that the multiplier in is pointwise bounded above and below: I am wondering if this permits the following estimate: In more general terms, if we have pointwise boundedness above and below for a Fourier multiplier can we then say for sufficiently well behaved ?","\underline{\textbf{Background:}} L^p \begin{align}
& \Big{|}\Big{|} A \Big{|}\Big{|}_p \\
& =
\int_{\mathbb{R}^d} \Big{|} \int_{|\xi|<2t^{1/2}} 
\exp\Big{(} ix\cdot\xi  \Big{)}
\Big{(}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{)} \exp\Big{(}-\frac{|\xi|^2}{2} \big{(}1+i\sqrt{ \frac{4t}{|\xi|^2} -1  }\big{)}\Big{)} \text{d}\xi \Big{|}^p \text{d}x  \Big{\} }^{1/p} ,
\end{align} t > 0 d \geq 2 |\xi| \sqrt{\frac{4t}{|\xi|^2} - 1} |\xi| = 2t^{1/2} \begin{align}
\Big{|}\Big{|} A \Big{|}\Big{|}_p = \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[} \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \hat{f} \Big{]}  \Big{|}\Big{|}_p ,
\end{align} \begin{align}
\hat{f}(t,\xi) := \Big{(}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{)} \exp\Big{(}-\frac{|\xi|^2}{2} \Big{)}\Bigg{|}_{|\xi|<2t^{1/2}}
\end{align} \hat{f} \underline{\textbf{Problem:}} \hat{f} \begin{align}
C^{-1} \frac{t^{1/2}}{|\xi|} \leq  \Big{|}1 - i \sqrt{\frac{4t}{|\xi|^2} -1 } \Big{|} \leq C \frac{t^{1/2}}{|\xi|}, \text{ for all } C \geq 4.
\end{align} \begin{align}
& \Big{|}\Big{|} \mathcal{F}^{-1}\Big{[} \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \hat{f} \Big{]}  \Big{|}\Big{|}_p \\
& \leq 
C
\Big{|}\Big{|} \mathcal{F}^{-1}\Big{[}   \exp\Big{(}i t^{1/2} |\xi|^2 \sqrt{ \frac{4}{|\xi|^2} -1  }\Big{)}  \frac{t^{1/2}}{|\xi|} \exp\Big{(}-\frac{|\xi|^2}{2} \Big{)}  \Big{]}  \Big{|}\Big{|}_p.
\end{align} \begin{align}
C^{-1}\psi(\xi) \leq \phi(\xi) \leq C\psi(\xi), \text{ for all } \xi \in \mathbb{R}^d,
\end{align} \begin{align}
\Big{|}\Big{|} \mathcal{F}^{-1} \Big{[} \phi \hat{g} \Big{]} \Big{|}\Big{|}_p \leq C \Big{|}\Big{|} \mathcal{F}^{-1} \Big{[} \psi \hat{g} \Big{]} \Big{|}\Big{|}_p,
\end{align} \hat{g}, \phi, \psi \begin{align}
\end{align}","['complex-analysis', 'functional-analysis', 'fourier-analysis', 'lp-spaces', 'fourier-transform']"
12,A proof without using net in Brezis's Ex 3.14,A proof without using net in Brezis's Ex 3.14,,"I'm doing Ex 3.14 in Brezis's book of Functional Analysis. Let $E$ be a reflexive Banach space and $I$ a set of indices. Consider a collection $(f_{i})_{i \in I}$ in the dual space $E'$ and a collection $(\alpha_{i})_{i \in I}$ in $\mathbb{R}$ . Let $M>0$ . Show that the following properties are equivalent: There exists some $a \in E$ with $|a| \le M$ such that $\langle f_{i}, a\rangle=\alpha_{i}$ for every $i \in I$ . One has $|\sum_{i \in J} \beta_{i} \alpha_{i}| \leq M\|\sum_{i \in J} \beta_{i} f_{i}\|$ for every collection $(\beta_{i})_{i \in J}$ in $\mathbb{R}$ with $J \subset I, J$ finite. My below solution uses net characterization of compactness, which is probably not the author's intention. Could you have a check on my attempt and suggest another solution without using net? My attempt: WLOG, we can assume $M:=1$ . The direction $1 \implies 2$ is trivial. Let's prove the converse. Let $B_E$ be the closed unit ball of $E$ . Let $\mathcal J$ be the collection of all finite subsets of $I$ . By Helly's theorem ( Lemma 3.3 in the book), for each $(J,n) \in \mathcal J \times \mathbb N$ , there is $x_{J,n} \in B_E$ such that $$ |\langle f_i, x_{J,n} \rangle - \alpha_i| < 1/n \quad \forall i \in J. $$ We endow $\mathcal J \times \mathbb N$ with a partial order $\le$ defined by $(J, n) \le (J', n') \iff J \subseteq J' \text{ and } n \le n'$ . Then $(\mathcal J \times \mathbb N, \le)$ is a directed set and thus $(x_{(J,n)})$ a net in $B_E$ . Because $E$ is reflexive, $B_E$ is compact in weak topology $\sigma(E', E)$ . Then there are $a \in B_E$ and a subnet $(x_{\varphi(d)})_{d\in D}$ such that $x_{\varphi(d)} \rightharpoonup a$ . Next we prove that $a$ satisfies the requirement of 1. Assume the contrary that there is $i_0 \in I$ such that $\langle f_{i_0}, a \rangle \neq \alpha_{i_0}$ . There is $n_0$ such that $|\langle f_{i_0}, a \rangle - \alpha_{i_0}| > 1/n_0$ . Let $J_0 := \{i_0\}$ and $$ U := \{x\in E \mid |\langle f_{i_0}, x \rangle - \alpha_{i_0}| > 1/n_0\}. $$ Then $U$ is an open neighborhood of $a$ in $\sigma(E', E)$ . There is $d_0$ such that $x_{\varphi(d)} \in U$ for all $d \ge d_0$ . There is $d_1 \ge d_0$ such that $\varphi(d_1) \ge (J_0, n_0+1)$ . It follows that $$ 1/(n_0+1) > |\langle f_{i_0}, x_{\varphi(d_1)} \rangle - \alpha_{i_0}| > 1/n_0. $$ This is a contradiction. This completes the proof.","I'm doing Ex 3.14 in Brezis's book of Functional Analysis. Let be a reflexive Banach space and a set of indices. Consider a collection in the dual space and a collection in . Let . Show that the following properties are equivalent: There exists some with such that for every . One has for every collection in with finite. My below solution uses net characterization of compactness, which is probably not the author's intention. Could you have a check on my attempt and suggest another solution without using net? My attempt: WLOG, we can assume . The direction is trivial. Let's prove the converse. Let be the closed unit ball of . Let be the collection of all finite subsets of . By Helly's theorem ( Lemma 3.3 in the book), for each , there is such that We endow with a partial order defined by . Then is a directed set and thus a net in . Because is reflexive, is compact in weak topology . Then there are and a subnet such that . Next we prove that satisfies the requirement of 1. Assume the contrary that there is such that . There is such that . Let and Then is an open neighborhood of in . There is such that for all . There is such that . It follows that This is a contradiction. This completes the proof.","E I (f_{i})_{i \in I} E' (\alpha_{i})_{i \in I} \mathbb{R} M>0 a \in E |a| \le M \langle f_{i}, a\rangle=\alpha_{i} i \in I |\sum_{i \in J} \beta_{i} \alpha_{i}| \leq M\|\sum_{i \in J} \beta_{i} f_{i}\| (\beta_{i})_{i \in J} \mathbb{R} J \subset I, J M:=1 1 \implies 2 B_E E \mathcal J I (J,n) \in \mathcal J \times \mathbb N x_{J,n} \in B_E 
|\langle f_i, x_{J,n} \rangle - \alpha_i| < 1/n \quad \forall i \in J.
 \mathcal J \times \mathbb N \le (J, n) \le (J', n') \iff J \subseteq J' \text{ and } n \le n' (\mathcal J \times \mathbb N, \le) (x_{(J,n)}) B_E E B_E \sigma(E', E) a \in B_E (x_{\varphi(d)})_{d\in D} x_{\varphi(d)} \rightharpoonup a a i_0 \in I \langle f_{i_0}, a \rangle \neq \alpha_{i_0} n_0 |\langle f_{i_0}, a \rangle - \alpha_{i_0}| > 1/n_0 J_0 := \{i_0\} 
U := \{x\in E \mid |\langle f_{i_0}, x \rangle - \alpha_{i_0}| > 1/n_0\}.
 U a \sigma(E', E) d_0 x_{\varphi(d)} \in U d \ge d_0 d_1 \ge d_0 \varphi(d_1) \ge (J_0, n_0+1) 
1/(n_0+1) > |\langle f_{i_0}, x_{\varphi(d_1)} \rangle - \alpha_{i_0}| > 1/n_0.
","['functional-analysis', 'solution-verification', 'banach-spaces', 'weak-convergence', 'reflexive-space']"
13,Functional Poincaré Lemma for second-order PDE $\Delta u(x) + F(\nabla u(x))=0$?,Functional Poincaré Lemma for second-order PDE ?,\Delta u(x) + F(\nabla u(x))=0,"Suppose I can write a second-order elliptic PDE for an unknown function $u:\mathbb R^n\to\mathbb R$ in the form $$\Delta u(x) + F(\nabla u(x))=0\qquad\forall x\in\mathbb R^n.$$ Under what conditions on the (potentially nonlinear) function $F:\mathbb R^n\to \mathbb R$ does this PDE correspond to the Euler-Lagrange equation of some functional acting on $u$ ? For example, if $F(v)\equiv0$ then the resulting PDE $\Delta u=0$ (Laplace's Equation) is the Euler-Lagrange equation corresponding to the Dirichlet energy $u\mapsto \int \|\nabla u(x)\|_2^2\,d\mathrm{Vol}(x)$ . More generically, I'm looking for a functional version of Poincaré's Lemma relevant to second-order PDE.  Posts like this one and this one are relevant, but I had trouble translating them into the case I'm interested in.","Suppose I can write a second-order elliptic PDE for an unknown function in the form Under what conditions on the (potentially nonlinear) function does this PDE correspond to the Euler-Lagrange equation of some functional acting on ? For example, if then the resulting PDE (Laplace's Equation) is the Euler-Lagrange equation corresponding to the Dirichlet energy . More generically, I'm looking for a functional version of Poincaré's Lemma relevant to second-order PDE.  Posts like this one and this one are relevant, but I had trouble translating them into the case I'm interested in.","u:\mathbb R^n\to\mathbb R \Delta u(x) + F(\nabla u(x))=0\qquad\forall x\in\mathbb R^n. F:\mathbb R^n\to \mathbb R u F(v)\equiv0 \Delta u=0 u\mapsto \int \|\nabla u(x)\|_2^2\,d\mathrm{Vol}(x)","['functional-analysis', 'partial-differential-equations', 'calculus-of-variations', 'classical-mechanics', 'euler-lagrange-equation']"
14,"Prob. 7, Sec. 4.3, in Kreyszig's Functional Analysis Book: Existence of a particular kind of a bounded linear functional on a Hilbert space","Prob. 7, Sec. 4.3, in Kreyszig's Functional Analysis Book: Existence of a particular kind of a bounded linear functional on a Hilbert space",,"Here is Prob. 7, Sec. 4.3, in the book Introductory Functional Analysis With Applications by Erwine Kreyszig: Give another proof of Theorem 4.3-3 in the case of a Hilbert space. And, here is Theorem 4.3-3 in Kreyszig: Let $X$ be a normed space and let $x_0 \neq 0$ be any element of $X$ . Then there exists a bounded linear functional $\tilde{f}$ on $X$ such that $$ \left\lVert \tilde{f} \right\rVert = 1, \qquad \tilde{f} \left( x_0 \right) = \left\lVert x_0 \right\rVert. $$ My Attempt: Let $x_0$ be a non-zero point of the Hilbert space $X$ , and let $$ e_0 := \frac{1}{\left\lVert x_0 \right\rVert} x_0. $$ Then we have $$ \left\lVert e_0 \right\rVert = 1. $$ Now the functional $\tilde{f}$ on $X$ defined by $$ \tilde{f} (x) := \left\langle x, e_0 \right\rangle \qquad \mbox{for all } x \in X $$ is linear and bounded and has norm $$ \left\lVert \tilde{f} \right\rVert = \left\lVert e_0 \right\rVert = 1. $$ Moreover, $$ \begin{align}  \tilde{f} \left( x_0 \right) &= \left\langle x_0, e_0 \right\rangle \\ &= \left\langle x_0, \frac{1}{ \left\lVert x_0 \right\rVert } x_0 \right\rangle \\ &= \frac{1}{\left\lVert x_0 \right\rVert } \left\langle x_0, x_0 \right\rangle \\ &= \frac{1}{\left\lVert x_0 \right\rVert } \left\lVert x_0 \right\rVert^2 \\ &= \left\lVert x_0 \right\rVert, \end{align} $$ as required. Is this proof satisfactory enough? Or, is it lacking in some respect?","Here is Prob. 7, Sec. 4.3, in the book Introductory Functional Analysis With Applications by Erwine Kreyszig: Give another proof of Theorem 4.3-3 in the case of a Hilbert space. And, here is Theorem 4.3-3 in Kreyszig: Let be a normed space and let be any element of . Then there exists a bounded linear functional on such that My Attempt: Let be a non-zero point of the Hilbert space , and let Then we have Now the functional on defined by is linear and bounded and has norm Moreover, as required. Is this proof satisfactory enough? Or, is it lacking in some respect?","X x_0 \neq 0 X \tilde{f} X 
\left\lVert \tilde{f} \right\rVert = 1, \qquad \tilde{f} \left( x_0 \right) = \left\lVert x_0 \right\rVert.
 x_0 X  e_0 := \frac{1}{\left\lVert x_0 \right\rVert} x_0.  
\left\lVert e_0 \right\rVert = 1.
 \tilde{f} X 
\tilde{f} (x) := \left\langle x, e_0 \right\rangle \qquad \mbox{for all } x \in X
 
\left\lVert \tilde{f} \right\rVert = \left\lVert e_0 \right\rVert = 1.
 
\begin{align} 
\tilde{f} \left( x_0 \right) &= \left\langle x_0, e_0 \right\rangle \\
&= \left\langle x_0, \frac{1}{ \left\lVert x_0 \right\rVert } x_0 \right\rangle \\
&= \frac{1}{\left\lVert x_0 \right\rVert } \left\langle x_0, x_0 \right\rangle \\
&= \frac{1}{\left\lVert x_0 \right\rVert } \left\lVert x_0 \right\rVert^2 \\
&= \left\lVert x_0 \right\rVert,
\end{align}
","['real-analysis', 'functional-analysis', 'analysis', 'solution-verification', 'hilbert-spaces']"
15,Adjoint of unbounded integral operator,Adjoint of unbounded integral operator,,"Consider a Borel-measurable function $a:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{C}$ , and let $T_a$ be the linear operator on $L^2(\mathbb{R})$ with domain \begin{equation} \mathcal{D}(T_a)=\left\{f\in L^2(\mathbb{R}):\int\left|\int a(x,y)f(y)\;\mathrm{d}y\,\right|^2\mathrm{d}x<\infty\right\} \end{equation} acting as \begin{equation} \left(T_af\right)(x)=\int a(x,y)f(y)\;\mathrm{d}y, \end{equation} that is, the integral operator with kernel $a(x,y)$ . We assume $\mathcal{D}(T_a)$ to be dense in $L^2(\mathbb{R})$ . It is fairly immediate to prove that, if the kernel is assumed to be square-integrable with respect to both entries: \begin{equation} \int\left|a(x,y)\right|^2\;\mathrm{d}x\,\mathrm{d}y<\infty, \end{equation} then $T_a$ is a bounded operator (in fact, it is Hilbert-Schmidt) whose adjoint $T_a^*$ is equal to the integral operator associated with the function $a^*(x,y)=\overline{a(y,x)}$ , that is, $T_a^*=T_{a^*}$ . In particular, if $\overline{a(y,x)}=a(x,y)$ , then $T_a$ is self-adjoint. My question is whether, or under what conditions, the same holds if the condition above is relaxed, i.e. if the equality $T_a^*=T_{a^*}$ also holds for unbounded integral operators. In general, it is easy to prove that $T_{a^*}$ and $T_a$ have the same domain and \begin{equation} T_{a^*}\subseteq T_a^*, \end{equation} since, given $f,g\in\mathcal{D}(T_a)$ , \begin{equation} \left\langle T_{a^*}g,f\right\rangle=\left\langle g,T_af\right\rangle. \end{equation} However, we are left with proving the converse inequality $\mathcal{D}(T_a^*)\subseteq\mathcal{D}(T_a)$ . I was trying to do that by using the very definition of adjoint: given $g\in\mathcal{D}(T_a^*)$ , there must exist some $\tilde{g}\in L^2(\mathbb{R})$ such that, for every $f\in\mathcal{D}(T_a)$ , \begin{equation} \left\langle\tilde{g},f\right\rangle=\left\langle g,T_af\right\rangle, \end{equation} that is, writing the integrals explicitly and taking complex conjugates, for every $f\in\mathcal{D}(T_a)$ , \begin{equation} \int\left[\tilde{g}(y)-\int \overline{a(y,x)}(x)\,\mathrm{d}x\right]\overline{f(y)}\:\mathrm{d}y=0. \end{equation} The idea is to use the arbitrariety of $f$ above to show that the quantity between square brackets must vanish. EDIT : More precisely, I thought about that: given $f\in L^2(\mathbb{R})$ , by assumption there is a sequence of functions $\{f_n\}_{n\in\mathbb{N}}\subset\mathcal{D}(T_a)$ with $f_n\to f$ ; for such a sequence, we have \begin{equation} \int\left[\tilde{g}(y)-\int \overline{a(y,x)}g(x)\,\mathrm{d}x\right]\overline{f_n(y)}\:\mathrm{d}y=0. \end{equation} Formally, taking the limit $n\to\infty$ and using the continuity of the $L^2(\mathbb{R})$ scalar product, one would show that indeed \begin{equation} \int\left[\tilde{g}(y)-\int \overline{a(y,x)}g(x)\,\mathrm{d}x\right]\overline{f(y)}\:\mathrm{d}y=0, \end{equation} which would prove the desired property, since $f$ is arbitrary. However, to do that, the assumption $g\in\mathcal{D}(T_a^*)$ alone must imply that, indeed, the function $y\mapsto\int \overline{a(y,x)}g(x)\,\mathrm{d}x$ be square-integrable. Notice, as a final consideration, that a similar procedure is usually carried out for multiplication operators $(M_Af)(x)=A(x)f(x)$ (which, roughly speaking, may be interpreted as integral operators with a degenerate kernel $a(x,y)=A(x)\delta(x-y)$ , the latter being the Dirac distribution). In such a case, a procedure analogous to the one above allows one to prove that indeed $M_{A}^*=M_{A^*}$ .","Consider a Borel-measurable function , and let be the linear operator on with domain acting as that is, the integral operator with kernel . We assume to be dense in . It is fairly immediate to prove that, if the kernel is assumed to be square-integrable with respect to both entries: then is a bounded operator (in fact, it is Hilbert-Schmidt) whose adjoint is equal to the integral operator associated with the function , that is, . In particular, if , then is self-adjoint. My question is whether, or under what conditions, the same holds if the condition above is relaxed, i.e. if the equality also holds for unbounded integral operators. In general, it is easy to prove that and have the same domain and since, given , However, we are left with proving the converse inequality . I was trying to do that by using the very definition of adjoint: given , there must exist some such that, for every , that is, writing the integrals explicitly and taking complex conjugates, for every , The idea is to use the arbitrariety of above to show that the quantity between square brackets must vanish. EDIT : More precisely, I thought about that: given , by assumption there is a sequence of functions with ; for such a sequence, we have Formally, taking the limit and using the continuity of the scalar product, one would show that indeed which would prove the desired property, since is arbitrary. However, to do that, the assumption alone must imply that, indeed, the function be square-integrable. Notice, as a final consideration, that a similar procedure is usually carried out for multiplication operators (which, roughly speaking, may be interpreted as integral operators with a degenerate kernel , the latter being the Dirac distribution). In such a case, a procedure analogous to the one above allows one to prove that indeed .","a:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{C} T_a L^2(\mathbb{R}) \begin{equation}
\mathcal{D}(T_a)=\left\{f\in L^2(\mathbb{R}):\int\left|\int a(x,y)f(y)\;\mathrm{d}y\,\right|^2\mathrm{d}x<\infty\right\}
\end{equation} \begin{equation}
\left(T_af\right)(x)=\int a(x,y)f(y)\;\mathrm{d}y,
\end{equation} a(x,y) \mathcal{D}(T_a) L^2(\mathbb{R}) \begin{equation}
\int\left|a(x,y)\right|^2\;\mathrm{d}x\,\mathrm{d}y<\infty,
\end{equation} T_a T_a^* a^*(x,y)=\overline{a(y,x)} T_a^*=T_{a^*} \overline{a(y,x)}=a(x,y) T_a T_a^*=T_{a^*} T_{a^*} T_a \begin{equation}
T_{a^*}\subseteq T_a^*,
\end{equation} f,g\in\mathcal{D}(T_a) \begin{equation}
\left\langle T_{a^*}g,f\right\rangle=\left\langle g,T_af\right\rangle.
\end{equation} \mathcal{D}(T_a^*)\subseteq\mathcal{D}(T_a) g\in\mathcal{D}(T_a^*) \tilde{g}\in L^2(\mathbb{R}) f\in\mathcal{D}(T_a) \begin{equation}
\left\langle\tilde{g},f\right\rangle=\left\langle g,T_af\right\rangle,
\end{equation} f\in\mathcal{D}(T_a) \begin{equation}
\int\left[\tilde{g}(y)-\int \overline{a(y,x)}(x)\,\mathrm{d}x\right]\overline{f(y)}\:\mathrm{d}y=0.
\end{equation} f f\in L^2(\mathbb{R}) \{f_n\}_{n\in\mathbb{N}}\subset\mathcal{D}(T_a) f_n\to f \begin{equation}
\int\left[\tilde{g}(y)-\int \overline{a(y,x)}g(x)\,\mathrm{d}x\right]\overline{f_n(y)}\:\mathrm{d}y=0.
\end{equation} n\to\infty L^2(\mathbb{R}) \begin{equation}
\int\left[\tilde{g}(y)-\int \overline{a(y,x)}g(x)\,\mathrm{d}x\right]\overline{f(y)}\:\mathrm{d}y=0,
\end{equation} f g\in\mathcal{D}(T_a^*) y\mapsto\int \overline{a(y,x)}g(x)\,\mathrm{d}x (M_Af)(x)=A(x)f(x) a(x,y)=A(x)\delta(x-y) M_{A}^*=M_{A^*}","['functional-analysis', 'hilbert-spaces', 'adjoint-operators']"
16,de Rham theorem for tempered distribution,de Rham theorem for tempered distribution,,"I am wondering if the following statement holds. If $u\in \mathscr{S}'$ and $u=\nabla p$ for some $p\in \mathscr{D}'$ , then $p$ is in $\mathscr{S}'$ . Here $\mathscr{S}'$ is the space of tempered distributions and $\mathscr{D}'$ is the space of distributions. I raised this question when I want to show the following claim: If $u\in \mathscr{S}'$ satisfies $\left< u,\Phi\right>=0$ for all $\Phi \in \mathscr{S}$ with $\mathrm{div}\, \Phi=0$ , then there exists $p\in \mathscr{S}'$ such that $u=\nabla p$ in $\mathscr{S}'$ . Another guess for my question is based on the generalized Lions' lemma: If $f\in \mathscr{D}'$ and $\nabla f \in H^{-1}$ , then $f\in L^2$ . Thank you for your time.","I am wondering if the following statement holds. If and for some , then is in . Here is the space of tempered distributions and is the space of distributions. I raised this question when I want to show the following claim: If satisfies for all with , then there exists such that in . Another guess for my question is based on the generalized Lions' lemma: If and , then . Thank you for your time.","u\in \mathscr{S}' u=\nabla p p\in \mathscr{D}' p \mathscr{S}' \mathscr{S}' \mathscr{D}' u\in \mathscr{S}' \left< u,\Phi\right>=0 \Phi \in \mathscr{S} \mathrm{div}\, \Phi=0 p\in \mathscr{S}' u=\nabla p \mathscr{S}' f\in \mathscr{D}' \nabla f \in H^{-1} f\in L^2","['functional-analysis', 'distribution-theory', 'harmonic-analysis']"
17,Show $\lim \limits_{n \to \infty} \frac{a_{n+1}}{a_n} = \|f\|_{\infty}$ for $f \in L^{\infty}$,Show  for,\lim \limits_{n \to \infty} \frac{a_{n+1}}{a_n} = \|f\|_{\infty} f \in L^{\infty},"I have a question that I need help with getting started (possibly I would be back for more help). I have a measure space $(X,A,\mu)$ that is finite, and $f \in L^{\infty}(\mu)$. Also, defined is $a_n = \int_X\,|f|^n\,d\mu$. I need to show that the limit is: $$\lim_{n\to\infty}\,\frac{a_{n+1}}{a_n} = \|f\|_{\infty} .$$ I am stuck on getting started, anybody have any suggestions? thanks much","I have a question that I need help with getting started (possibly I would be back for more help). I have a measure space $(X,A,\mu)$ that is finite, and $f \in L^{\infty}(\mu)$. Also, defined is $a_n = \int_X\,|f|^n\,d\mu$. I need to show that the limit is: $$\lim_{n\to\infty}\,\frac{a_{n+1}}{a_n} = \|f\|_{\infty} .$$ I am stuck on getting started, anybody have any suggestions? thanks much",,"['real-analysis', 'measure-theory']"
18,How to use Calculus of Variation with Lagrange Multipliers when the marginal conditions only depend on the multipliers?,How to use Calculus of Variation with Lagrange Multipliers when the marginal conditions only depend on the multipliers?,,"I am trying to solve an optimization problem: $$\begin{aligned}&\max_{G(v)}\int_p^1 \frac{G(v)}{v} dv\\ s.t.& \int_0^1(1+\log u)G'(u)du=-\overline v<0\\ &\int_0^x(1-\frac{G(v)}{v})dv\leq\int_0^xH(v)dv, \text{  for any } x\in[0,1]\\ &\frac{G(v)-v*G'(v)}{v^2}>0, \text{  for any }v\in(0,1]\\ &G(1)=0 \text{ and } G'(p)=0, \end{aligned} $$ where $p\in[0,1]$ and $H(v)$ is a weakly increasing function on $[0,1]$ with $H(0)=0$ and $H(1)=1$ . (Actually, this problem is a second version of the problem in my another question Functional Extremum Problem with inhomgeneous boundary condition , just simply let $G(v)=v(1-F(v))$ to avoid inhomgeneous boundary condition.) Now with the constraint I describe above, the new objective function should be $$\begin{aligned}&\int_p^1 \left[\frac{G(v)}{v}+\lambda(1+\log v)G'(v)+\mu(v)\frac{G(v)-v*G'(v)}{v^2}\right]dv\\+&\int_0^1\gamma(x)\int_0^x(1-\frac{G(v)}{v})dvdx\\ =&\int_p^1 \left[\frac{G(v)}{v}+\lambda(1+\log v)G'(v)+\mu(v)\frac{G(v)-v*G'(v)}{v^2}+\int_v^1\gamma(x)dx\times (1-\frac{G(v)}{v})\right]dv, \end{aligned}$$ where $\lambda,\gamma(\cdot),\mu(\cdot)$ are the Lagrange Multiplier (Function), and the equality is from the exchange of integral sequence. By the Calculus of Variations, the marginal condition is $$\frac{1}{v}+\lambda\cdot(-\frac{1}{v})+\int_v^1\gamma(x)dx\cdot(-\frac{1}{v})+\left[\mu(v)\cdot(\frac{1}{v^2}-\frac{1}{v^2})+\mu'(v)\cdot\frac{1}{v}\right]=0,$$ or, $$1-\lambda-\int_v^1\gamma(x)dx+\mu'(v)=0.$$ Neither $G(v)$ nor $G'(v)$ disappears in the condition! I know that this is the similar case when we use Lagrange Multiplier in linear programming problem. But in linear programming, there is other ways. In my case, I don't know how to solve this problem at all without the Lagrange method.","I am trying to solve an optimization problem: where and is a weakly increasing function on with and . (Actually, this problem is a second version of the problem in my another question Functional Extremum Problem with inhomgeneous boundary condition , just simply let to avoid inhomgeneous boundary condition.) Now with the constraint I describe above, the new objective function should be where are the Lagrange Multiplier (Function), and the equality is from the exchange of integral sequence. By the Calculus of Variations, the marginal condition is or, Neither nor disappears in the condition! I know that this is the similar case when we use Lagrange Multiplier in linear programming problem. But in linear programming, there is other ways. In my case, I don't know how to solve this problem at all without the Lagrange method.","\begin{aligned}&\max_{G(v)}\int_p^1 \frac{G(v)}{v} dv\\
s.t.& \int_0^1(1+\log u)G'(u)du=-\overline v<0\\
&\int_0^x(1-\frac{G(v)}{v})dv\leq\int_0^xH(v)dv, \text{  for any } x\in[0,1]\\
&\frac{G(v)-v*G'(v)}{v^2}>0, \text{  for any }v\in(0,1]\\
&G(1)=0 \text{ and } G'(p)=0,
\end{aligned}
 p\in[0,1] H(v) [0,1] H(0)=0 H(1)=1 G(v)=v(1-F(v)) \begin{aligned}&\int_p^1 \left[\frac{G(v)}{v}+\lambda(1+\log v)G'(v)+\mu(v)\frac{G(v)-v*G'(v)}{v^2}\right]dv\\+&\int_0^1\gamma(x)\int_0^x(1-\frac{G(v)}{v})dvdx\\ =&\int_p^1 \left[\frac{G(v)}{v}+\lambda(1+\log v)G'(v)+\mu(v)\frac{G(v)-v*G'(v)}{v^2}+\int_v^1\gamma(x)dx\times (1-\frac{G(v)}{v})\right]dv, \end{aligned} \lambda,\gamma(\cdot),\mu(\cdot) \frac{1}{v}+\lambda\cdot(-\frac{1}{v})+\int_v^1\gamma(x)dx\cdot(-\frac{1}{v})+\left[\mu(v)\cdot(\frac{1}{v^2}-\frac{1}{v^2})+\mu'(v)\cdot\frac{1}{v}\right]=0, 1-\lambda-\int_v^1\gamma(x)dx+\mu'(v)=0. G(v) G'(v)","['functional-analysis', 'optimization', 'calculus-of-variations', 'euler-lagrange-equation']"
19,PDE: A priori estimate,PDE: A priori estimate,,"Let $\Omega \subset \textbf{R}^N$ be open and bounded. Let $f \in L^2(\Omega)$ and let $u \in H^1(\Omega)$ be a weak solution of the equation \begin{align} Lu \equiv - \sum_{i,j=1}^n D_i(a_{ij}D_ju) = f. \tag{1} \end{align} This is defined by \begin{align}     \int_{\Omega} \sum_{i,j=1}^n a_{ij}D_i u D_j v ~dx = \int_{\Omega}fv~dx     \tag{2} \end{align} for all $v \in H_0^1(\Omega)$ . Suppose that there is a constant $\Theta > 0$ such that for all $x \in \Omega$ \begin{align}     \sqrt{     \sum_{i,j = 1}^n (a_{ij}(x))^2     }     \leq \Theta.     \tag{3} \end{align} By the Cauchy-Schwarz inequality in $\textbf{R}^n$ this also implies that \begin{equation}     \sum_{i,j=1}^n a_{ij}(x) \zeta_i \xi_j \leq \Theta |\zeta| |\xi|     \tag{4} \end{equation} for all $x \in \Omega$ and all $\zeta, \xi$ in $\textbf{R}^n$ . Let us furthermore assume that there is a constant $\theta > 0$ s.t. \begin{align}     \sum_{i,j = 1}^n a_{ij}(x)\xi_i \xi_j \geq \theta |\xi|^2     \tag{5} \end{align} for all $x \in \Omega$ and $\xi \in \textbf{R}^n$ . Let $\eta \in C^1_c(\Omega)$ be arbitrary. Prove the following a priori estimate: There holds \begin{align}     \int_{\Omega}|Du|^2\eta^2dx      \leq C_0      \left (     \int_{\Omega}u^2(|D\eta|^2 + \eta^2)dx +     \int_U f^2\eta^2 dx     \right )     \tag{*} \end{align} where $C_0$ depends only on $\theta$ and $\Theta$ . The first step here is to use $v = u \eta^2$ and plug it into the definition of the weak solution which is stated in equation $(2)$ . The next step would be to use Cauchy-Schwarz and Peter-Paul together with the above conditions on $(a_{ij})$ , but this is the point where im currently stuck. I would be really happy if someone could help me out! Cheers, Pinch","Let be open and bounded. Let and let be a weak solution of the equation This is defined by for all . Suppose that there is a constant such that for all By the Cauchy-Schwarz inequality in this also implies that for all and all in . Let us furthermore assume that there is a constant s.t. for all and . Let be arbitrary. Prove the following a priori estimate: There holds where depends only on and . The first step here is to use and plug it into the definition of the weak solution which is stated in equation . The next step would be to use Cauchy-Schwarz and Peter-Paul together with the above conditions on , but this is the point where im currently stuck. I would be really happy if someone could help me out! Cheers, Pinch","\Omega \subset \textbf{R}^N f \in L^2(\Omega) u \in H^1(\Omega) \begin{align}
Lu \equiv - \sum_{i,j=1}^n D_i(a_{ij}D_ju) = f. \tag{1}
\end{align} \begin{align}
    \int_{\Omega} \sum_{i,j=1}^n a_{ij}D_i u D_j v ~dx = \int_{\Omega}fv~dx
    \tag{2}
\end{align} v \in H_0^1(\Omega) \Theta > 0 x \in \Omega \begin{align}
    \sqrt{
    \sum_{i,j = 1}^n (a_{ij}(x))^2
    }
    \leq \Theta.
    \tag{3}
\end{align} \textbf{R}^n \begin{equation}
    \sum_{i,j=1}^n a_{ij}(x) \zeta_i \xi_j \leq \Theta |\zeta| |\xi|
    \tag{4}
\end{equation} x \in \Omega \zeta, \xi \textbf{R}^n \theta > 0 \begin{align}
    \sum_{i,j = 1}^n a_{ij}(x)\xi_i \xi_j \geq \theta |\xi|^2
    \tag{5}
\end{align} x \in \Omega \xi \in \textbf{R}^n \eta \in C^1_c(\Omega) \begin{align}
    \int_{\Omega}|Du|^2\eta^2dx 
    \leq C_0 
    \left (
    \int_{\Omega}u^2(|D\eta|^2 + \eta^2)dx +
    \int_U f^2\eta^2 dx
    \right )
    \tag{*}
\end{align} C_0 \theta \Theta v = u \eta^2 (2) (a_{ij})","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
20,In what situations are Path Integrals well-defined?,In what situations are Path Integrals well-defined?,,"In physics I have come across contexts where apparently path integrals are well-defined, and others where they are not. However I have no clear understanding of when and why they succeed or fail to be well-defined. Question 1. What are the obstructions to path integrals being well-defined? When path integrals are well-defined, what goes ‘right’ which otherwise typically goes wrong? I have seen other questions on here that are similar, but I have never really understood the answers. For instance often approaches involving the limit of a discretization lead to divergences. Can a regularization scheme not be included in the path integral definition, to avoid this? Even if this does not work, are there no other integration measures over path or configuartion spaces which give sensible answers? Or do appropriate measures only exist in special cases? Question 2. Is there a simple summary of situations where we currently know path integrals are well-defined? For instance, this question appears to indicate that path integrals with Euclidean action over paths in $\mathbb{R}^n$ are always well defined. Question 3. I have heard path integrals are always well-defined for TQFTs. If this is true, then why?","In physics I have come across contexts where apparently path integrals are well-defined, and others where they are not. However I have no clear understanding of when and why they succeed or fail to be well-defined. Question 1. What are the obstructions to path integrals being well-defined? When path integrals are well-defined, what goes ‘right’ which otherwise typically goes wrong? I have seen other questions on here that are similar, but I have never really understood the answers. For instance often approaches involving the limit of a discretization lead to divergences. Can a regularization scheme not be included in the path integral definition, to avoid this? Even if this does not work, are there no other integration measures over path or configuartion spaces which give sensible answers? Or do appropriate measures only exist in special cases? Question 2. Is there a simple summary of situations where we currently know path integrals are well-defined? For instance, this question appears to indicate that path integrals with Euclidean action over paths in are always well defined. Question 3. I have heard path integrals are always well-defined for TQFTs. If this is true, then why?",\mathbb{R}^n,"['functional-analysis', 'quantum-mechanics', 'quantum-field-theory', 'functional-calculus', 'topological-quantum-field-theory']"
21,What is the Double and Triple Dual of $\ell^\infty$,What is the Double and Triple Dual of,\ell^\infty,"Suppose that we start with $c^0$ and keep taking duals, $$c^0 \to \ell^1 \to \ell^\infty \to \ell^{\infty*} \to \ell^{\infty **} \to \ell^{\infty***} \to \ell^{\infty****}\to \dots$$ Is this an infinite sequence of distinct spaces?  If not, when does it ""stabilise"", i.e. stop generating new spaces?  What about other Lebesgue spaces $$L^1(\Omega,\mu) \to L^\infty(\Omega,\mu) \to L^{\infty*}(\Omega,\mu) \to L^{\infty **}(\Omega,\mu) \to L^{\infty***}(\Omega,\mu) \to L^{\infty****}(\Omega,\mu)\to \dots?$$ I know that a Banach space is non-reflexive iff its dual is non-reflexive, but do not know anything beyond that.  Thanks! Edit: Thanks Jose for posting the mathoverflow link.  Follow-up questions: What is the characterisation of $(L^{\infty})^{n*}, n\in\mathbb{N}$ ? What uses do these spaces have? Thanks!","Suppose that we start with and keep taking duals, Is this an infinite sequence of distinct spaces?  If not, when does it ""stabilise"", i.e. stop generating new spaces?  What about other Lebesgue spaces I know that a Banach space is non-reflexive iff its dual is non-reflexive, but do not know anything beyond that.  Thanks! Edit: Thanks Jose for posting the mathoverflow link.  Follow-up questions: What is the characterisation of ? What uses do these spaces have? Thanks!","c^0 c^0 \to \ell^1 \to \ell^\infty \to \ell^{\infty*} \to \ell^{\infty **} \to \ell^{\infty***} \to \ell^{\infty****}\to \dots L^1(\Omega,\mu) \to L^\infty(\Omega,\mu) \to L^{\infty*}(\Omega,\mu) \to L^{\infty **}(\Omega,\mu) \to L^{\infty***}(\Omega,\mu) \to L^{\infty****}(\Omega,\mu)\to \dots? (L^{\infty})^{n*}, n\in\mathbb{N}","['functional-analysis', 'lp-spaces', 'dual-spaces']"
22,When an ordinary partial derivative is a Frechet derivative on a Banach space?,When an ordinary partial derivative is a Frechet derivative on a Banach space?,,"We have a function $p(x, \theta)\in \mathbb{R},\ x\in\mathbb{R},\ \theta\in\mathbb{R}^n$ . We can also consider $p(\cdot, \theta)$ to be a map from $\mathbb{R}^n$ to a Banach space of functions. For example, assuming $p(\cdot, \theta)\in \mathbb{L}^1(\mathbb{R})$ for each $\theta$ we have that $\hat p:\theta\to p(\cdot, \theta)$ is a map from $\mathbb{R}^n$ to $\mathbb{L}^1(\mathbb{R})$ . My question is about relation of the ordinary partial derivative $\frac{\partial p(x, \theta)}{\partial\theta}$ (assuming it is $\in\mathbb{L}^1(\mathbb{R})$ )  to the Frechet derivative of $\hat p:\theta\to p(\cdot, \theta)$ . When are they the same? Addition after the comments of Zerox: For the ordinary partial derivative for each $x$ $$ \lim_{d\theta\to0}\frac{\left|p(x,\theta+d\theta)-p(x, \theta)-\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta\right|}{\Vert d\theta \Vert}=0 $$ For the Frechet derivative we have different convergence $$\lim_{d\theta\to0}\frac{\Vert p(\cdot,\theta+d\theta)-p(\cdot, \theta)-L(d\theta)\Vert_{\mathbb{L}^1}}{\Vert d\theta\Vert}=0$$ where $L(d\theta)$ is a continuous linear map from $\mathbb{R}^n$ to $\mathbb{L}^1(\mathbb{R})$ . The linear map $d\theta\to\frac{\partial p(\cdot, \theta)}{\partial\theta}\cdot d\theta$ from $\mathbb{R}^n$ to $\mathbb{L}^1(\mathbb{R})$ is continuous in the Banach norm. $$\left\|\frac{\partial p(\cdot, \theta)}{\partial\theta}\cdot d\theta\right\|_{\mathbb{L}^1}=\left\|\sum_i\frac{\partial p(\cdot, \theta)}{\partial\theta_i}d\theta_i\right\|_{\mathbb{L}^1}\leq\sum_i\left\Vert\frac{\partial p(\cdot, \theta)}{\partial\theta_i}\right\Vert_{\mathbb{L}^1}|d\theta_i|\leq\operatorname{max}_i\left\Vert\frac{\partial p(\cdot, \theta)}{\partial\theta_i}\right\|_{\mathbb{L}^1}\Vert d\theta\Vert_{\mathbb{L}^1}$$ So this map seems to be a candidate for $L(d\theta)$ , but i don't see why (or when) they have to coincide. Pointwise convergence (in $x$ ) doesn't imply $\mathbb{L}^1$ convergence and vice versa. Of course in the topology of pointwise convergence on the Banach space they would coincide, but i am interested in the $\mathbb{L}^1$ convergence. A special case of parametrized probability density functions: If there are no convenient conditions for the general case, then maybe there are some for this case? $$\int p(x,\theta)dx=1,\ p(x,\theta)\geq0\ \forall\theta$$ Upd: deleted the incorrect Scheffe's Lemma application. The case of dominated partial derivatives: Assume $\left|\frac{\partial p(x, \theta)}{\partial\theta_i}\right|\leq g(x)$ for some $g(x)\in \mathbb{L}^1(\mathbb{R})$ . Then by the multivariate MVT $$ \begin{split} \frac{\left|p(x,\theta+d\theta)-p(x, \theta)-\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta\right|}{\Vert d\theta\Vert}\leq&\frac{\left|\frac{\partial p(x, c)}{\partial\theta}\cdot d\theta\right|+\left|\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta\right|}{\Vert d\theta\Vert}\\ \leq&\left\Vert\frac{\partial p(x, c)}{\partial\theta}\right\Vert+\left\Vert\frac{\partial p(x, \theta)}{\partial\theta}\right\Vert\leq2\sqrt n g(x) \end{split} $$ So the whole expression is also dominated by the $\mathbb{L}^1(\mathbb{R})$ function. By the dominated convergence theorem we have the convergence of the expression above in $\mathbb{L}^1$ , so $L(d\theta)=\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta$ . If both derivatives exist, they have to be equal: One more observation. For any $\mathbb R \ni h_m\to0$ the existence of the Frechet derivative implies the $\mathbb L^1$ convergence to $0$ of the corresponding expression for each $i=1,\cdots,n$ . $$ \left\Vert\frac{p(\cdot,\theta+e_ih_m)-p(\cdot, \theta)}{h_m}-L(e_i)\right\Vert_{\mathbb{L}^1}\to0$$ Where $e_i$ is $i$ th vector of the standard basis. Then there exists a subsequence of $h_m$ , call it $\mathbb R \ni g_l\to0$ , for which the expression above converges pointwise a.e. to $0$ . $$\left|\frac{p(x,\theta+e_ig_l)-p(x, \theta)}{g_l}-L(e_i)(x)\right|\to0\ \textbf{a.e.}$$ But we also have for the ordinary partial derivative $$\left|\frac{p(x,\theta+e_ig_l)-p(x, \theta)}{g_l}-\frac{\partial p(x, \theta)}{\partial\theta_i}\right|\to0$$ So $L(e_i)=\frac{\partial p(\cdot, \theta)}{\partial\theta_i}$ and thus $L(d\theta)=\frac{\partial p(\cdot, \theta)}{\partial\theta}\cdot d\theta$ . Additions, corrections, indications of errors are still welcome.","We have a function . We can also consider to be a map from to a Banach space of functions. For example, assuming for each we have that is a map from to . My question is about relation of the ordinary partial derivative (assuming it is )  to the Frechet derivative of . When are they the same? Addition after the comments of Zerox: For the ordinary partial derivative for each For the Frechet derivative we have different convergence where is a continuous linear map from to . The linear map from to is continuous in the Banach norm. So this map seems to be a candidate for , but i don't see why (or when) they have to coincide. Pointwise convergence (in ) doesn't imply convergence and vice versa. Of course in the topology of pointwise convergence on the Banach space they would coincide, but i am interested in the convergence. A special case of parametrized probability density functions: If there are no convenient conditions for the general case, then maybe there are some for this case? Upd: deleted the incorrect Scheffe's Lemma application. The case of dominated partial derivatives: Assume for some . Then by the multivariate MVT So the whole expression is also dominated by the function. By the dominated convergence theorem we have the convergence of the expression above in , so . If both derivatives exist, they have to be equal: One more observation. For any the existence of the Frechet derivative implies the convergence to of the corresponding expression for each . Where is th vector of the standard basis. Then there exists a subsequence of , call it , for which the expression above converges pointwise a.e. to . But we also have for the ordinary partial derivative So and thus . Additions, corrections, indications of errors are still welcome.","p(x, \theta)\in \mathbb{R},\ x\in\mathbb{R},\ \theta\in\mathbb{R}^n p(\cdot, \theta) \mathbb{R}^n p(\cdot, \theta)\in \mathbb{L}^1(\mathbb{R}) \theta \hat p:\theta\to p(\cdot, \theta) \mathbb{R}^n \mathbb{L}^1(\mathbb{R}) \frac{\partial p(x, \theta)}{\partial\theta} \in\mathbb{L}^1(\mathbb{R}) \hat p:\theta\to p(\cdot, \theta) x 
\lim_{d\theta\to0}\frac{\left|p(x,\theta+d\theta)-p(x, \theta)-\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta\right|}{\Vert d\theta \Vert}=0
 \lim_{d\theta\to0}\frac{\Vert p(\cdot,\theta+d\theta)-p(\cdot, \theta)-L(d\theta)\Vert_{\mathbb{L}^1}}{\Vert d\theta\Vert}=0 L(d\theta) \mathbb{R}^n \mathbb{L}^1(\mathbb{R}) d\theta\to\frac{\partial p(\cdot, \theta)}{\partial\theta}\cdot d\theta \mathbb{R}^n \mathbb{L}^1(\mathbb{R}) \left\|\frac{\partial p(\cdot, \theta)}{\partial\theta}\cdot d\theta\right\|_{\mathbb{L}^1}=\left\|\sum_i\frac{\partial p(\cdot, \theta)}{\partial\theta_i}d\theta_i\right\|_{\mathbb{L}^1}\leq\sum_i\left\Vert\frac{\partial p(\cdot, \theta)}{\partial\theta_i}\right\Vert_{\mathbb{L}^1}|d\theta_i|\leq\operatorname{max}_i\left\Vert\frac{\partial p(\cdot, \theta)}{\partial\theta_i}\right\|_{\mathbb{L}^1}\Vert d\theta\Vert_{\mathbb{L}^1} L(d\theta) x \mathbb{L}^1 \mathbb{L}^1 \int p(x,\theta)dx=1,\ p(x,\theta)\geq0\ \forall\theta \left|\frac{\partial p(x, \theta)}{\partial\theta_i}\right|\leq g(x) g(x)\in \mathbb{L}^1(\mathbb{R}) 
\begin{split}
\frac{\left|p(x,\theta+d\theta)-p(x, \theta)-\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta\right|}{\Vert d\theta\Vert}\leq&\frac{\left|\frac{\partial p(x, c)}{\partial\theta}\cdot d\theta\right|+\left|\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta\right|}{\Vert d\theta\Vert}\\
\leq&\left\Vert\frac{\partial p(x, c)}{\partial\theta}\right\Vert+\left\Vert\frac{\partial p(x, \theta)}{\partial\theta}\right\Vert\leq2\sqrt n g(x)
\end{split}
 \mathbb{L}^1(\mathbb{R}) \mathbb{L}^1 L(d\theta)=\frac{\partial p(x, \theta)}{\partial\theta}\cdot d\theta \mathbb R \ni h_m\to0 \mathbb L^1 0 i=1,\cdots,n 
\left\Vert\frac{p(\cdot,\theta+e_ih_m)-p(\cdot, \theta)}{h_m}-L(e_i)\right\Vert_{\mathbb{L}^1}\to0 e_i i h_m \mathbb R \ni g_l\to0 0 \left|\frac{p(x,\theta+e_ig_l)-p(x, \theta)}{g_l}-L(e_i)(x)\right|\to0\ \textbf{a.e.} \left|\frac{p(x,\theta+e_ig_l)-p(x, \theta)}{g_l}-\frac{\partial p(x, \theta)}{\partial\theta_i}\right|\to0 L(e_i)=\frac{\partial p(\cdot, \theta)}{\partial\theta_i} L(d\theta)=\frac{\partial p(\cdot, \theta)}{\partial\theta}\cdot d\theta","['real-analysis', 'functional-analysis', 'analysis', 'banach-spaces', 'frechet-derivative']"
23,Prove that set of limits bound is open.,Prove that set of limits bound is open.,,"Consider the following problem: $\dot{x} = A(t) x$ . Let $X_{A}(t,s)$ be the Cauchy operator of the following system. Let $M$ be the space of continuous functional matrices. Let's denote by: $$ \rho(A, B) = \sup_{t \in \mathbb{R}_{+}} \|A(t) - B(t)\|, \text{ where } A,B \in M  $$ Let's consider the upper-limit function $\phi(A) = \underset{{t-s \to +\infty}}{\overline{\lim}} \dfrac{1}{t - s}\log\|X_A(t,s)\|$ . We want to show that $\forall a \in \mathbb{R}_{+}$ the set: $\{A\in M: \phi(A) < a\}$ (i.e. Lyapunov's exponent is bounded with $a$ ) is open w.r.t. the metrics defined above. I'was trying to use the simple definition of the openness in metric space (assume $a$ is fixed): $$ B_{\epsilon} (A) \subset A, \text{ for } A \in M $$ Which is equal to: $\exists B : \phi(A), \phi(B) < r$ and $\rho(A,B) < \epsilon$ . We can try to denote: $\{\phi(A) <a \} \sim \{A: \|X_A\| \sim e^{ r(t-s) + o(t-s)}\}$ , but here I guess we need to use some more fundamental fact about $X_A$ (maybe continuous dependency on $A$ ). UPD. We can consider the following problem as following. There is exists $Q(t) \in M$ : $B(t) = A(t) + Q(t)$ , $\rho(A,B) = \sup_t \|Q(t)\| < \epsilon$ and $\phi(A), \phi(B) < a$ . I thought about diagonal operator with small enough values on the diagonal (since I guess we can only consider the diagonal matrices). Any hints?","Consider the following problem: . Let be the Cauchy operator of the following system. Let be the space of continuous functional matrices. Let's denote by: Let's consider the upper-limit function . We want to show that the set: (i.e. Lyapunov's exponent is bounded with ) is open w.r.t. the metrics defined above. I'was trying to use the simple definition of the openness in metric space (assume is fixed): Which is equal to: and . We can try to denote: , but here I guess we need to use some more fundamental fact about (maybe continuous dependency on ). UPD. We can consider the following problem as following. There is exists : , and . I thought about diagonal operator with small enough values on the diagonal (since I guess we can only consider the diagonal matrices). Any hints?","\dot{x} = A(t) x X_{A}(t,s) M 
\rho(A, B) = \sup_{t \in \mathbb{R}_{+}} \|A(t) - B(t)\|, \text{ where } A,B \in M 
 \phi(A) = \underset{{t-s \to +\infty}}{\overline{\lim}} \dfrac{1}{t - s}\log\|X_A(t,s)\| \forall a \in \mathbb{R}_{+} \{A\in M: \phi(A) < a\} a a 
B_{\epsilon} (A) \subset A, \text{ for } A \in M
 \exists B : \phi(A), \phi(B) < r \rho(A,B) < \epsilon \{\phi(A) <a \} \sim \{A: \|X_A\| \sim e^{ r(t-s) + o(t-s)}\} X_A A Q(t) \in M B(t) = A(t) + Q(t) \rho(A,B) = \sup_t \|Q(t)\| < \epsilon \phi(A), \phi(B) < a","['functional-analysis', 'ordinary-differential-equations', 'cauchy-problem']"
24,"Composition of $C^{k,\alpha}$ Holder Functions",Composition of  Holder Functions,"C^{k,\alpha}","Let $k,\ell \geq 0$ be integers and $\alpha,\beta \in [0,1]$ . Let $\Omega\subseteq \mathbb R^n$ be a closed set and set $f \in C^{k,\alpha}(\mathbb R;\mathbb R)$ and $g \in C^{\ell,\beta}(\Omega;\mathbb R)$ . I am wondering whether there are any citeable references which state the largest integer $h \geq 0$ and real number $\gamma \in [0,1]$ such that $f\circ g \in C^{h, \gamma}(\Omega;\mathbb R)$ . I am also interested in inequalities which upper bound $\|f \circ g\|_{C^{h,\gamma}(\Omega;\mathbb R)}$ in terms of $\|f\|_{C^{k,\alpha}(\mathbb R;\mathbb R)}$ and $\|g \|_{C^{\ell,\beta}(\Omega;\mathbb R)}$ . As mentioned in this post , when $k = \ell = 0$ ,   one may take $\gamma = \alpha\beta$ (and, of course, $h=0$ ). Furthermore, when $\alpha = \beta = 0$ , the chain rule would suggest that one may take $h = \min\{k,\ell\}$ and no better. Perhaps these results can be generalized to show that $f\circ g \in C^{\min\{k,\ell\},\alpha\beta}(\Omega;\mathbb R)$ . For example, if $n=k=\ell=1$ , then for all $x,y \in \Omega \subseteq \mathbb R$ , $$ \begin{align*} |(f \circ g)'(x) - (f \circ g)'(y)|  &= |g'(x) f'(g(x)) - g'(y) f'(g(y))| \\  &\leq  |g'(x) f'(g(x)) - g'(x) f'(g(y))| + |g'(x) f'(g(y)) - g'(x) f'(g(y))| \\  &\lesssim  \|g\|_{C^{1,\beta}} \|f \|_{C^{1,\alpha}} |g(x) - g(y)|^\alpha + \| f\|_{C^{1,\alpha}} |g'(x)   - g'(y) | \\  &\lesssim  \|g\|_{C^{1,\beta}}^{1+\alpha} \|f \|_{C^{1,\alpha}} |x - y|^{\alpha\beta} + \| f\|_{C^{1,\alpha}} \| g\|_{C^{1,\beta}} |x   - y |^\beta, \end{align*}$$ suggesting that $f \circ g\in C^{1,\alpha\beta}(\Omega;\mathbb R)$ and $$\|f \circ g\|_{C^{1,\alpha\beta}} \lesssim 1+  \|g\|_{C^{1,\beta}}^{1+\alpha} \|f \|_{C^{1,\alpha}}.$$ Presumably this argument can be generalized, for instance using the Faa di Bruno formula (a similar suggestion was made in this post ). But I assume this has been done somewhere already. Does anyone know of a citeable reference containing such a result?","Let be integers and . Let be a closed set and set and . I am wondering whether there are any citeable references which state the largest integer and real number such that . I am also interested in inequalities which upper bound in terms of and . As mentioned in this post , when ,   one may take (and, of course, ). Furthermore, when , the chain rule would suggest that one may take and no better. Perhaps these results can be generalized to show that . For example, if , then for all , suggesting that and Presumably this argument can be generalized, for instance using the Faa di Bruno formula (a similar suggestion was made in this post ). But I assume this has been done somewhere already. Does anyone know of a citeable reference containing such a result?","k,\ell \geq 0 \alpha,\beta \in [0,1] \Omega\subseteq \mathbb R^n f \in C^{k,\alpha}(\mathbb R;\mathbb R) g \in C^{\ell,\beta}(\Omega;\mathbb R) h \geq 0 \gamma \in [0,1] f\circ g \in C^{h, \gamma}(\Omega;\mathbb R) \|f \circ g\|_{C^{h,\gamma}(\Omega;\mathbb R)} \|f\|_{C^{k,\alpha}(\mathbb R;\mathbb R)} \|g \|_{C^{\ell,\beta}(\Omega;\mathbb R)} k = \ell = 0 \gamma = \alpha\beta h=0 \alpha = \beta = 0 h = \min\{k,\ell\} f\circ g \in C^{\min\{k,\ell\},\alpha\beta}(\Omega;\mathbb R) n=k=\ell=1 x,y \in \Omega \subseteq \mathbb R 
\begin{align*}
|(f \circ g)'(x) - (f \circ g)'(y)|
 &= |g'(x) f'(g(x)) - g'(y) f'(g(y))| \\
 &\leq  |g'(x) f'(g(x)) - g'(x) f'(g(y))| + |g'(x) f'(g(y)) - g'(x) f'(g(y))| \\
 &\lesssim  \|g\|_{C^{1,\beta}} \|f \|_{C^{1,\alpha}} |g(x) - g(y)|^\alpha + \| f\|_{C^{1,\alpha}} |g'(x)   - g'(y) | \\
 &\lesssim  \|g\|_{C^{1,\beta}}^{1+\alpha} \|f \|_{C^{1,\alpha}} |x - y|^{\alpha\beta} + \| f\|_{C^{1,\alpha}} \| g\|_{C^{1,\beta}} |x   - y |^\beta,
\end{align*} f \circ g\in C^{1,\alpha\beta}(\Omega;\mathbb R) \|f \circ g\|_{C^{1,\alpha\beta}} \lesssim 1+  \|g\|_{C^{1,\beta}}^{1+\alpha} \|f \|_{C^{1,\alpha}}.","['real-analysis', 'functional-analysis', 'holder-spaces']"
25,Prove or disprove a Hölder type bound on antiderivative,Prove or disprove a Hölder type bound on antiderivative,,"Let $f:(0,1]\rightarrow \mathbb R_{\ge0}$ be a continuous function such that for every $t_0\in [0,1]$ , $$\int_0^{t_0} [f(t)-f(t_0)]\,dt\le c_1 t_0^\gamma$$ for $0<\gamma<1$ and $c_1>0$ . Does it follow that $$\int_0^{t_0} f(t)dt\le c_2t_0^\gamma$$ for some $c_2>0$ ? I know that this fails to hold if for example $\gamma=1$ we can have $f(t)=-\log t$ but I have no idea how to prove this for $0<\gamma<1$ . Any help is appreciated.","Let be a continuous function such that for every , for and . Does it follow that for some ? I know that this fails to hold if for example we can have but I have no idea how to prove this for . Any help is appreciated.","f:(0,1]\rightarrow \mathbb R_{\ge0} t_0\in [0,1] \int_0^{t_0} [f(t)-f(t_0)]\,dt\le c_1 t_0^\gamma 0<\gamma<1 c_1>0 \int_0^{t_0} f(t)dt\le c_2t_0^\gamma c_2>0 \gamma=1 f(t)=-\log t 0<\gamma<1","['calculus', 'integration', 'functional-analysis', 'upper-lower-bounds']"
26,"Inequality on the unit cube $[0, 1]^d$",Inequality on the unit cube,"[0, 1]^d","I was trying to solve prove the following inequality: For all compactly supported smooth functions on the $d$ -dimensional unit cube $u\in C^\infty([0,1]^d)$ (where if $\Omega\subset\mathbb{R}^d$ is open, we define $C^\infty(\overline{\Omega})=\{u\in C^\infty(\Omega)\mid u \text{ has > a continuous extension to } \overline{\Omega}\}$ ), we have $$\int_{\Omega}|u|^{2} d x \leq \alpha \int_{\Omega} \operatorname{dist}(x, \partial \Omega)^{2} \left(|u|^{2}+\|\nabla u\|^{2} \right) d x, $$ where $\Omega=(0,1)^d$ and if $A, B\subset\mathbb{R}^d$ , then $\operatorname{dist}(A,B)=\inf_{a\in A,b\in B}d(a,b)$ . To prove it, I thought I better study how the function $\operatorname{dist}(x,\partial\Omega)$ behaves on $\Omega$ . For that, I determined the following open subsets of $\Omega$ : for each $1\leq i\leq d$ , $$ U_i^+=\{x\in\Omega:x_i>\max\{x_j,1-x_j\mid 1\leq j\leq n,\;j\neq i\}\},\\ U_i^-=\{x\in\Omega:x_i<\min\{x_j,1-x_j\mid 1\leq j\leq n,\;j\neq i\}\}. $$ These sets (which on $d=1$ are just $(0,1/2)$ and $(1/2,0)$ , on $d=2$ can be seen to be some triangles, and on $d=3$ some piramids) satisfy the property $$ \begin{align} x\in U_i^+&\Rightarrow d(x,\partial\Omega)=1-x_i,\\ x\in U_i^-&\Rightarrow d(x,\partial\Omega)=x_i. \end{align} $$ Furthermore, if $\Omega'=\bigcup_{i=1}^d U_i^+\cup U_i^-$ , then $\Omega\setminus\Omega'$ is a null set (i.e., the $d$ -dimensional Lebesgue measure equals zero). From here, I was thinking of using some partition of unity which involved the $U_i^\pm$ 's to ""break down"" the integral in different manageable parts. But I could not find out exactly which and how to apply it. I was even trying to prove it on $d=1$ but I could not get the proof to work. Trying to get the bound taking a compactly supported function $\theta$ on $(0,1)$ with values $0\leq\theta\leq 1$ and which evaluates $1$ on $[1/n,1/2-1/n]\cup[1/2+1/n,1-1/n]$ and trying to bound $\int_0^1\theta udx$ . But I could not control the value of the derivative of $\theta$ . Any thoughts on the problem will be appreciated :)","I was trying to solve prove the following inequality: For all compactly supported smooth functions on the -dimensional unit cube (where if is open, we define ), we have where and if , then . To prove it, I thought I better study how the function behaves on . For that, I determined the following open subsets of : for each , These sets (which on are just and , on can be seen to be some triangles, and on some piramids) satisfy the property Furthermore, if , then is a null set (i.e., the -dimensional Lebesgue measure equals zero). From here, I was thinking of using some partition of unity which involved the 's to ""break down"" the integral in different manageable parts. But I could not find out exactly which and how to apply it. I was even trying to prove it on but I could not get the proof to work. Trying to get the bound taking a compactly supported function on with values and which evaluates on and trying to bound . But I could not control the value of the derivative of . Any thoughts on the problem will be appreciated :)","d u\in C^\infty([0,1]^d) \Omega\subset\mathbb{R}^d C^\infty(\overline{\Omega})=\{u\in C^\infty(\Omega)\mid u \text{ has
> a continuous extension to } \overline{\Omega}\} \int_{\Omega}|u|^{2} d x \leq \alpha \int_{\Omega} \operatorname{dist}(x, \partial \Omega)^{2}
\left(|u|^{2}+\|\nabla u\|^{2}
\right) d x,
 \Omega=(0,1)^d A, B\subset\mathbb{R}^d \operatorname{dist}(A,B)=\inf_{a\in A,b\in B}d(a,b) \operatorname{dist}(x,\partial\Omega) \Omega \Omega 1\leq i\leq d 
U_i^+=\{x\in\Omega:x_i>\max\{x_j,1-x_j\mid 1\leq j\leq n,\;j\neq i\}\},\\
U_i^-=\{x\in\Omega:x_i<\min\{x_j,1-x_j\mid 1\leq j\leq n,\;j\neq i\}\}.
 d=1 (0,1/2) (1/2,0) d=2 d=3 
\begin{align}
x\in U_i^+&\Rightarrow d(x,\partial\Omega)=1-x_i,\\
x\in U_i^-&\Rightarrow d(x,\partial\Omega)=x_i.
\end{align}
 \Omega'=\bigcup_{i=1}^d U_i^+\cup U_i^- \Omega\setminus\Omega' d U_i^\pm d=1 \theta (0,1) 0\leq\theta\leq 1 1 [1/n,1/2-1/n]\cup[1/2+1/n,1-1/n] \int_0^1\theta udx \theta","['real-analysis', 'functional-analysis', 'upper-lower-bounds', 'smooth-functions']"
27,Polynomial of compact operator on Hilbert space.,Polynomial of compact operator on Hilbert space.,,"Apologies if this is a duplicate. And just to note, this is self-study. Let $B$ be a compact self-adjoint operator on a Hilbert space $H$ . We consider the polynomial $p(x) = a_0+ \sum_{i=1}^n a_i x^i$ . Consider now the polynomial of $B$ , $$ p(B) = a_0 I +\sum_{i=1}^n a_i B^i.  $$ I am to show that $p(B)$ be a compact operator for all real values of the coefficients. I believe that this cannot be the case unless $a_0 = 0$ . My thinking is that we know that the (positive) powers and sums of compact operators are again compact, so $\sum_{i=1}^n a_i B^i$ is compact. If we assume that $p(B)$ is compact, then $p(B)-\sum_{i=1}^n a_i B^i$ should be compact, but this is $a_0 I$ , and the identity operator is not compact (unless H is finite-dimensional), so $p(B)$ cannot be compact... but nevertheless, the lecture notes claim that this is the case, so where am I making a mistake?","Apologies if this is a duplicate. And just to note, this is self-study. Let be a compact self-adjoint operator on a Hilbert space . We consider the polynomial . Consider now the polynomial of , I am to show that be a compact operator for all real values of the coefficients. I believe that this cannot be the case unless . My thinking is that we know that the (positive) powers and sums of compact operators are again compact, so is compact. If we assume that is compact, then should be compact, but this is , and the identity operator is not compact (unless H is finite-dimensional), so cannot be compact... but nevertheless, the lecture notes claim that this is the case, so where am I making a mistake?","B H p(x) = a_0+ \sum_{i=1}^n a_i x^i B 
p(B) = a_0 I +\sum_{i=1}^n a_i B^i. 
 p(B) a_0 = 0 \sum_{i=1}^n a_i B^i p(B) p(B)-\sum_{i=1}^n a_i B^i a_0 I p(B)","['functional-analysis', 'compact-operators']"
28,A detailed and self-contained proof of Fubini's theorem for Banach spaces,A detailed and self-contained proof of Fubini's theorem for Banach spaces,,"After so much preparation (in proving auxiliary lemmas), I finally complete the proof of Fubini's theorem for Banach spaces. This is what I have desired after proving Tonelli's theorem :) The journey to the proof is very enriching. It solidifies my understanding in some aspects. Why are the $\sigma$ -finiteness and completeness of measure important and where do we use them? Why does $E$ need to be complete, i.e. $E$ is a Banach space? Where is Tonelli's theorem used? The relation between $\mathcal L_1$ convergence and a.e. convergence. Why is the quotient space $L_1$ of $\mathcal L_1$ useful? Could you please have a check on my proof? It is detailed and thus easy to read. I would be grateful if any mistake is spotted. Related definitions of Bochner integrals can be found here . Let $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ be complete $\sigma$ -finite measure spaces, and $(E, | \cdot |)$ a Banach space. $\mathcal C :=\mathcal A \otimes \mathcal B$ the product $\sigma$ -algebra of $\mathcal A$ and $\mathcal B$ . $\lambda := \mu \otimes \nu$ the product measure of $\mu$ and $\nu$ . $\mathcal S (X \times Y, \lambda, E)$ the space of $\lambda$ -simple functions from $X \times Y$ to $E$ . $\mathcal  L_0 (Y, \nu, E)$ the space of $\nu$ -measurable functions from $Y$ to $E$ . $\mathcal  L_1 (Y, \nu, E)$ the space of $\nu$ -integrable functions from $Y$ to $E$ . Fubini's theorem: Let $f: X \times Y \to E$ $\lambda$ -integrable. $f_x: Y \to E, \, y \mapsto f(x, y)$ for all $x \in X$ . $f_y: X \to E, \, x \mapsto f(x, y)$ for all $y \in Y$ . Then The map $f_x$ is $\nu$ -integrable for $\mu$ -a.e. $x \in X$ . The map $f_y$ is $\mu$ -integrable for $\nu$ -a.e. $y \in Y$ . The map $\Phi: X \ni x \mapsto \int_Y f_x  \mathrm d \nu$ is $\mu$ -a.e. defined and $\mu$ -integrable. The map $\Psi: Y \ni y \mapsto \int_X f_y \mathrm d \mu$ is $\nu$ -a.e. defined and $\nu$ -integrable. The following identity holds: $$\int_X \Phi \mathrm d \mu = \int_{X \times Y} f \mathrm d \lambda = \int_Y \Psi \mathrm d \nu.$$ My proof: By symmetry, it's enough to prove for the case of $\Phi$ . Lemma 1: Let $(f_n)$ be a Cauchy sequence in $\mathcal S (X \times Y, \lambda, E)$ that converges $\lambda$ -a.e. to $f$ . We define $f_x,f_{n,x}: Y \to E$ by $f_x(y) := f (x, y)$ and $f_{n,x} (y) := f_n (x, y)$ . Then there is a subsequence $\varphi$ of $(0, 1, 2, \ldots, )$ such that for $\mu$ -a.e. $x \in X$ , $(f_{\varphi (n), x})_n$ is a Cauchy sequence in $\mathcal S (Y, \nu, E)$ and converges to $f_x$ both in $\mathcal  L_1 (Y, \nu, E)$ and $\nu$ -a.e. [A proof can be found here ] Lemma 2: Let $(h_n)$ be a sequence in $\mathcal  L_1 (Y, \nu, E)$ that converges to $h$ in $\mathcal  L_1 (Y, \nu, E)$ . Then there exists a subsequence $\varphi$ of $(0, 1, 2, \ldots, )$ such that $(h_{\varphi(n)})$ converges $\nu$ -a.e. to $h$ . [A proof can be found here ] (i) Let $f= 1_G$ where $G \in \mathcal C$ and $\lambda(G) < \infty$ . Notice that $f$ is a characteristic function in $\mathcal S(X \times Y, \lambda, \mathbb R)$ . By Tonelli's theorem, $\Phi$ is measurable and $\int_{X \times Y} f \mathrm d \lambda = \int_X \Phi \mathrm d \mu$ . On the other hand, $\int_{X \times Y} f \mathrm d \lambda = \lambda(G) < \infty$ . Hence $\Phi$ is also integrable and thus Fubini's theorem holds in this case. (ii) Let $f= e1_G$ where $0 \neq e \in E$ , $G \in \mathcal C$ , and $\lambda(G) < \infty$ . Notice that $f$ is the atomic function in $\mathcal S(X \times Y, \lambda, E)$ . Let $1_{G, x} := (1_G)_x$ . Then $\Phi (x) = \int_Y e 1_{G, x} \mathrm d \nu = e \int_Y 1_{G, x} \mathrm d \nu$ . By (i), $x \to \int_Y 1_{G, x} \mathrm d \nu$ is integrable, so is $\Phi:x \to e \int_Y 1_{G, x} \mathrm d \nu$ . Also, \begin{align} \int_X \Phi \mathrm d \mu &= \int_X \left [ e \int_Y 1_{G, x} \mathrm d \nu \right ] \mathrm d \mu (x) &&= e \int_X \int_Y 1_{G, x} \mathrm d \nu \mathrm d \mu (x) \\ &\overset{(\star)}{=} e \int_{X \times Y} 1_G \mathrm d \lambda \quad \text{by (i)} &&= \int_{X \times Y} e1_G \mathrm d \lambda \\ &= \int_{X \times Y} f \mathrm d \lambda. \end{align} Here $(\star)$ is due to Tonelli's theorem. Hence Fubini's theorem also holds in this case. By linearity of integrals, Fubini's theorem also holds for all simple functions $f \in \mathcal S(X \times Y, \lambda, E)$ . (iii) Let $f \in \mathcal L_1 (X \times Y, \lambda, E)$ . This means $f$ is $\lambda$ -a.e. limit of a Cauchy sequence $(f_n)$ in $\mathcal S (X \times Y, \lambda, E)$ . Let $$f_{n,x} (y) := f_n (x, y) \quad \text{and} \quad\Phi_n (x) := \int_Y f_{n, x} \mathrm d \nu.$$ By (ii), $$\Phi_n \in \mathcal L_1(X, \mu, E) \quad \text{and} \quad \int_X \Phi_n \mathrm d \mu = \int_{X \times Y} f_n \mathrm d \lambda, \quad n \in \mathbb N.$$ By Lemma 1 , there is a subsequence $\varphi$ of $(0, 1, 2, \ldots, )$ such that for $\mu$ -a.e. $x \in X$ , $(f_{\varphi (n), x})_n$ is a Cauchy sequence in $\mathcal S (Y, \nu, E)$ and converges to $f_x$ both in $\mathcal  L_1 (Y, \nu, E)$ and $\nu$ -a.e. It follows that for $\mu$ -a.e. $x \in X$ , $$\int_Y f_{\varphi (n), x} \mathrm d \nu \xrightarrow{n \to \infty} \int_Y f_x\mathrm d \nu.$$ This means $\Phi_{\varphi (n)} \to \Phi$ $\mu$ -a.e. Let's prove that $(\Phi_{\varphi (n)})$ is indeed a Cauchy sequence in $\mathcal L_1(X, \mu, E)$ . In fact, \begin{align} \| \Phi_{\varphi (n)} - \Phi_{\varphi (m)}\|_1 &= \int_X \left  |\int_Y \big ( f_{\varphi (n), x}- f_{\varphi (m), x} \big ) \mathrm d \nu \right | \mathrm d \mu(x) \\ &\le  \int_X \int_Y \left  | f_{\varphi (n), x} - f_{\varphi (m), x} \right | \mathrm d \nu  \mathrm d \mu(x) \\ &=  \int_{X \times Y} | f_{\varphi (n)} - f_{\varphi (m)} |  \mathrm d \lambda \quad \text{by Tonelli's theorem} \\ &= \| f_{\varphi (n)} - f_{\varphi (m)} \|_1. \end{align} It follows that there is $\Phi' \in \mathcal L_1(X, \mu, E)$ such that $\Phi_{\varphi (n)} \to \Phi'$ in $\mathcal L_1(X, \mu, E)$ . By Lemma 2 , there is a subsequence $\tau$ of $\varphi$ such that $\Phi_{\tau (n)} \to \Phi'$ $\mu$ -a.e. We have already proved that $\Phi_{\varphi (n)} \to \Phi$ $\mu$ -a.e., so $\Phi = \Phi' \in \mathcal L_1(X, \mu, E)$ $\mu$ -a.e. As such, $$\int_X \Phi_{\varphi (n)} \mathrm d \mu \xrightarrow{n \to \infty} \int_X \Phi \mathrm d \mu.$$ By definition of integrals, $$\int_{X \times Y} f_n \mathrm d \lambda \xrightarrow{n \to \infty} \int_{X \times Y} f \mathrm d \lambda.$$ Recall that $$\int_X \Phi_n \mathrm d \mu = \int_{X \times Y} f_n \mathrm d \lambda, \quad n \in \mathbb N.$$ Hence $$\int_X \Phi \mathrm d \mu = \int_{X \times Y} f \mathrm d \lambda.$$ This completes the proof.","After so much preparation (in proving auxiliary lemmas), I finally complete the proof of Fubini's theorem for Banach spaces. This is what I have desired after proving Tonelli's theorem :) The journey to the proof is very enriching. It solidifies my understanding in some aspects. Why are the -finiteness and completeness of measure important and where do we use them? Why does need to be complete, i.e. is a Banach space? Where is Tonelli's theorem used? The relation between convergence and a.e. convergence. Why is the quotient space of useful? Could you please have a check on my proof? It is detailed and thus easy to read. I would be grateful if any mistake is spotted. Related definitions of Bochner integrals can be found here . Let and be complete -finite measure spaces, and a Banach space. the product -algebra of and . the product measure of and . the space of -simple functions from to . the space of -measurable functions from to . the space of -integrable functions from to . Fubini's theorem: Let -integrable. for all . for all . Then The map is -integrable for -a.e. . The map is -integrable for -a.e. . The map is -a.e. defined and -integrable. The map is -a.e. defined and -integrable. The following identity holds: My proof: By symmetry, it's enough to prove for the case of . Lemma 1: Let be a Cauchy sequence in that converges -a.e. to . We define by and . Then there is a subsequence of such that for -a.e. , is a Cauchy sequence in and converges to both in and -a.e. [A proof can be found here ] Lemma 2: Let be a sequence in that converges to in . Then there exists a subsequence of such that converges -a.e. to . [A proof can be found here ] (i) Let where and . Notice that is a characteristic function in . By Tonelli's theorem, is measurable and . On the other hand, . Hence is also integrable and thus Fubini's theorem holds in this case. (ii) Let where , , and . Notice that is the atomic function in . Let . Then . By (i), is integrable, so is . Also, Here is due to Tonelli's theorem. Hence Fubini's theorem also holds in this case. By linearity of integrals, Fubini's theorem also holds for all simple functions . (iii) Let . This means is -a.e. limit of a Cauchy sequence in . Let By (ii), By Lemma 1 , there is a subsequence of such that for -a.e. , is a Cauchy sequence in and converges to both in and -a.e. It follows that for -a.e. , This means -a.e. Let's prove that is indeed a Cauchy sequence in . In fact, It follows that there is such that in . By Lemma 2 , there is a subsequence of such that -a.e. We have already proved that -a.e., so -a.e. As such, By definition of integrals, Recall that Hence This completes the proof.","\sigma E E \mathcal L_1 L_1 \mathcal L_1 (X, \mathcal A, \mu) (Y, \mathcal B, \nu) \sigma (E, | \cdot |) \mathcal C :=\mathcal A \otimes \mathcal B \sigma \mathcal A \mathcal B \lambda := \mu \otimes \nu \mu \nu \mathcal S (X \times Y, \lambda, E) \lambda X \times Y E \mathcal  L_0 (Y, \nu, E) \nu Y E \mathcal  L_1 (Y, \nu, E) \nu Y E f: X \times Y \to E \lambda f_x: Y \to E, \, y \mapsto f(x, y) x \in X f_y: X \to E, \, x \mapsto f(x, y) y \in Y f_x \nu \mu x \in X f_y \mu \nu y \in Y \Phi: X \ni x \mapsto \int_Y f_x  \mathrm d \nu \mu \mu \Psi: Y \ni y \mapsto \int_X f_y \mathrm d \mu \nu \nu \int_X \Phi \mathrm d \mu = \int_{X \times Y} f \mathrm d \lambda = \int_Y \Psi \mathrm d \nu. \Phi (f_n) \mathcal S (X \times Y, \lambda, E) \lambda f f_x,f_{n,x}: Y \to E f_x(y) := f (x, y) f_{n,x} (y) := f_n (x, y) \varphi (0, 1, 2, \ldots, ) \mu x \in X (f_{\varphi (n), x})_n \mathcal S (Y, \nu, E) f_x \mathcal  L_1 (Y, \nu, E) \nu (h_n) \mathcal  L_1 (Y, \nu, E) h \mathcal  L_1 (Y, \nu, E) \varphi (0, 1, 2, \ldots, ) (h_{\varphi(n)}) \nu h f= 1_G G \in \mathcal C \lambda(G) < \infty f \mathcal S(X \times Y, \lambda, \mathbb R) \Phi \int_{X \times Y} f \mathrm d \lambda = \int_X \Phi \mathrm d \mu \int_{X \times Y} f \mathrm d \lambda = \lambda(G) < \infty \Phi f= e1_G 0 \neq e \in E G \in \mathcal C \lambda(G) < \infty f \mathcal S(X \times Y, \lambda, E) 1_{G, x} := (1_G)_x \Phi (x) = \int_Y e 1_{G, x} \mathrm d \nu = e \int_Y 1_{G, x} \mathrm d \nu x \to \int_Y 1_{G, x} \mathrm d \nu \Phi:x \to e \int_Y 1_{G, x} \mathrm d \nu \begin{align}
\int_X \Phi \mathrm d \mu &= \int_X \left [ e \int_Y 1_{G, x} \mathrm d \nu \right ] \mathrm d \mu (x)
&&= e \int_X \int_Y 1_{G, x} \mathrm d \nu \mathrm d \mu (x) \\
&\overset{(\star)}{=} e \int_{X \times Y} 1_G \mathrm d \lambda \quad \text{by (i)}
&&= \int_{X \times Y} e1_G \mathrm d \lambda \\
&= \int_{X \times Y} f \mathrm d \lambda.
\end{align} (\star) f \in \mathcal S(X \times Y, \lambda, E) f \in \mathcal L_1 (X \times Y, \lambda, E) f \lambda (f_n) \mathcal S (X \times Y, \lambda, E) f_{n,x} (y) := f_n (x, y) \quad \text{and} \quad\Phi_n (x) := \int_Y f_{n, x} \mathrm d \nu. \Phi_n \in \mathcal L_1(X, \mu, E) \quad \text{and} \quad \int_X \Phi_n \mathrm d \mu = \int_{X \times Y} f_n \mathrm d \lambda, \quad n \in \mathbb N. \varphi (0, 1, 2, \ldots, ) \mu x \in X (f_{\varphi (n), x})_n \mathcal S (Y, \nu, E) f_x \mathcal  L_1 (Y, \nu, E) \nu \mu x \in X \int_Y f_{\varphi (n), x} \mathrm d \nu \xrightarrow{n \to \infty} \int_Y f_x\mathrm d \nu. \Phi_{\varphi (n)} \to \Phi \mu (\Phi_{\varphi (n)}) \mathcal L_1(X, \mu, E) \begin{align}
\| \Phi_{\varphi (n)} - \Phi_{\varphi (m)}\|_1 &= \int_X \left  |\int_Y \big ( f_{\varphi (n), x}- f_{\varphi (m), x} \big ) \mathrm d \nu \right | \mathrm d \mu(x) \\
&\le  \int_X \int_Y \left  | f_{\varphi (n), x} - f_{\varphi (m), x} \right | \mathrm d \nu  \mathrm d \mu(x) \\
&=  \int_{X \times Y} | f_{\varphi (n)} - f_{\varphi (m)} |  \mathrm d \lambda \quad \text{by Tonelli's theorem} \\
&= \| f_{\varphi (n)} - f_{\varphi (m)} \|_1.
\end{align} \Phi' \in \mathcal L_1(X, \mu, E) \Phi_{\varphi (n)} \to \Phi' \mathcal L_1(X, \mu, E) \tau \varphi \Phi_{\tau (n)} \to \Phi' \mu \Phi_{\varphi (n)} \to \Phi \mu \Phi = \Phi' \in \mathcal L_1(X, \mu, E) \mu \int_X \Phi_{\varphi (n)} \mathrm d \mu \xrightarrow{n \to \infty} \int_X \Phi \mathrm d \mu. \int_{X \times Y} f_n \mathrm d \lambda \xrightarrow{n \to \infty} \int_{X \times Y} f \mathrm d \lambda. \int_X \Phi_n \mathrm d \mu = \int_{X \times Y} f_n \mathrm d \lambda, \quad n \in \mathbb N. \int_X \Phi \mathrm d \mu = \int_{X \times Y} f \mathrm d \lambda.","['functional-analysis', 'measure-theory', 'solution-verification', 'banach-spaces', 'fubini-tonelli-theorems']"
29,When does there exist a subsequence which converges to the Cesaro mean,When does there exist a subsequence which converges to the Cesaro mean,,"I have a sequence of continuous (even analytic) functions on $[0,1]$ . I know that the Cesaro mean of the sequence of functions converges uniformly on $[0,1]$ : $$\frac{1}{N}\sum_{n=1}^Nf_n\rightarrow f$$ I was also able to show that this sequence is uniformly Lipschitz and bounded, so I know that there exists a subsequence of functions which converges uniformly on $[0,1]$ (using Arzela-Ascoli). I'd like to determine if there possibly exists a subsequence of functions which converges uniformly to the Cesaro mean itself ( $f$ ). So basically I'd be happy to hear some ideas about possible criterions I could use. I am obviously not saying that there should always be a subsequence which converges to the Cesaro mean, but maybe there are some useful special cases which I can use. At first I tried to possibly think in the direction of the Banach-Saks theorem and weak- $*$ topology. I know that the unit ball in $C([0,1])$ is weak- $*$ compact. I am actually not sure if it is weak- $*$ sequentially compact, but let's say it is. Then my bounded sequence contains a subsequence which converges in the weak- $*$ topology. Then I could maybe apply some version of Banach-Saks to show that this subsequence has a smaller subsequence which actually converges to the mean value in norm (so uniformly). But as I said, I am not sure if there is a version of Banach-Saks which applies in this case, and I am overall not sure about the details and if this should work, this was just a hunch. Anyway, I'd be happy to hear some ideas as to what tools might come in handy and under what assumptions I can actually show something like this. Thanks in advance.","I have a sequence of continuous (even analytic) functions on . I know that the Cesaro mean of the sequence of functions converges uniformly on : I was also able to show that this sequence is uniformly Lipschitz and bounded, so I know that there exists a subsequence of functions which converges uniformly on (using Arzela-Ascoli). I'd like to determine if there possibly exists a subsequence of functions which converges uniformly to the Cesaro mean itself ( ). So basically I'd be happy to hear some ideas about possible criterions I could use. I am obviously not saying that there should always be a subsequence which converges to the Cesaro mean, but maybe there are some useful special cases which I can use. At first I tried to possibly think in the direction of the Banach-Saks theorem and weak- topology. I know that the unit ball in is weak- compact. I am actually not sure if it is weak- sequentially compact, but let's say it is. Then my bounded sequence contains a subsequence which converges in the weak- topology. Then I could maybe apply some version of Banach-Saks to show that this subsequence has a smaller subsequence which actually converges to the mean value in norm (so uniformly). But as I said, I am not sure if there is a version of Banach-Saks which applies in this case, and I am overall not sure about the details and if this should work, this was just a hunch. Anyway, I'd be happy to hear some ideas as to what tools might come in handy and under what assumptions I can actually show something like this. Thanks in advance.","[0,1] [0,1] \frac{1}{N}\sum_{n=1}^Nf_n\rightarrow f [0,1] f * C([0,1]) * * *","['sequences-and-series', 'functional-analysis', 'banach-spaces', 'uniform-convergence', 'weak-convergence']"
30,"If $(f_n)$ is Cauchy and converges a.e. to $f$, there is a Cauchy subsequence $(f_{\psi (n), x})_n$ converging to $f_x$ in $\mathcal L_1$ and a.e.","If  is Cauchy and converges a.e. to , there is a Cauchy subsequence  converging to  in  and a.e.","(f_n) f (f_{\psi (n), x})_n f_x \mathcal L_1","In generalizing Fubini's theorem to functions on Banach space, I encounter below result that takes me a great deal of time to prove. The proof is delicate for me because we play with $\mathcal L_1$ as well as its quotient space $L_1$ . So it's likely that there are subtle mistakes that I'm unable to recognize. I really hope that somebody helps me check it out. Can we have a stronger result, i.e., $(f_{n, x})_n$ is a Cauchy sequence that converges to $f_x$ $\nu$ -a.e.? Related definitions of Bochner integrals can be found here . Let $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ be complete $\sigma$ -finite measure spaces, and $(E, | \cdot |)$ a Banach space. $\Sigma := \mathcal A \times \mathcal B$ . $\mathcal C :=\mathcal A \otimes \mathcal B$ the product $\sigma$ -algebra of $\mathcal A$ and $\mathcal B$ . $\lambda := \mu \otimes \nu$ the product measure of $\mu$ and $\nu$ . $\mathcal S (X \times Y, \lambda, E)$ the space of $\lambda$ -simple functions from $X \times Y$ to $E$ . $\mathcal  L_0 (Y, \nu, E)$ the space of $\nu$ -measurable functions from $Y$ to $E$ . $\mathcal  L_1 (Y, \nu, E)$ the space of $\nu$ -integrable functions from $Y$ to $E$ . Theorem: Let $(f_n)$ be a Cauchy sequence in $\mathcal S (X \times Y, \lambda, E)$ that converges $\lambda$ -a.e. to $f$ . We define $f_x,f_{n,x}: Y \to E$ by $f_x(y) := f (x, y)$ and $f_{n,x} (y) := f_n (x, y)$ . Then there is a subsequence $\varphi$ of $(0, 1, 2, \ldots, )$ such that for $\mu$ -a.e. $x \in X$ , $(f_{\varphi (n), x})_n$ is a Cauchy sequence in $\mathcal S (Y, \nu, E)$ and converges to $f_x$ both in $\mathcal  L_1 (Y, \nu, E)$ and $\nu$ -a.e. My attempt: First we need some lemmas. Lemma 1: Let $f \in \mathcal S (X \times Y, \lambda, E)$ and $f_x : Y \to E, y \mapsto f(x, y)$ . Then $f_x \in\mathcal S (Y, \nu, E)$ for $\mu$ -a.e. $x \in X$ . [A proof can be found here ] Lemma 2: Let $f \in \mathcal S (X \times Y, \lambda, E)$ and $F' := L_1 (Y, \nu, E)$ be the quotient space of $\mathcal  L_1 (Y, \nu, E)$ . Then $\Phi : x \mapsto [f_x]$ is defined $\mu$ -a.e. and belongs to $\mathcal L_1 (X, \mu, F)$ , i.e., $\Phi$ is integrable. Here $[f_x] \in F'$ is the equivalence class containing $f_x \in \mathcal  L_1 (Y, \nu, E)$ . [A proof can be found here ] Lemma 3: Let $(h_n)$ be a sequence in $\mathcal  L_1 (Y, \nu, E)$ that converges to $h$ in $\mathcal  L_1 (Y, \nu, E)$ . Then there exists a subsequence $(h_{\varphi(n)})$ that converges $\nu$ -a.e. to $h$ . [A proof can be found here ] Let $C \in \mathcal C$ be a set on which $f_n \not \to f$ . Let $C_x$ and $C^y$ be the $x$ -section and $y$ -section of $C$ respectively. By Tonelli's theorem, $$0 = \lambda (C)=\int_X \nu (C_x) \mathrm d\mu(x) = \int_Y \mu (C^y) \mathrm d \nu(y).$$ This means for $\mu$ -a.e. $x \in X$ , $\nu(C_x) =0$ . Then there is a null set $A \in \mathcal A$ such that for each $x \in A^c$ , there is a null set $B_x \in \mathcal B$ such that $f_{n,x} (y) \to f_x (y)$ for all $y \in B_x^c$ . By Lemma 1 , there is a null set $A \in \mathcal A$ such that $f_x,f_{n, x} \in \mathcal S (Y, \nu, E)$ for all $x \in A^c$ and for all $n \in \mathbb N$ . It follows that for each $x \in A^c$ , $(f_{n,x})_n$ is a sequence in $\mathcal S (Y, \nu, E)$ that converges to $f_x \in \mathcal S (Y, \nu, E)$ on $B_x^c$ . Let $F' := L_1 (Y, \nu, E)$ be the quotient space of $\mathcal  L_1 (Y, \nu, E)$ . By Lemma 2 , $\Phi_n: x \mapsto [f_{n, x}]$ belongs to $\mathcal L_1 (X, \mu, F')$ . Here $[f_{n, x}] \in F'$ is the equivalence class containing $f_{n, x} \in \mathcal  L_1 (Y, \nu, E)$ . We have \begin{align} \| \Phi_n - \Phi_m \|_1 &= \int_X \|\Phi_n (x) - \Phi_m (x) \|_{F'} \mathrm d \mu(x) \\ &= \int_X \int_Y |f_{n, x} (y) - f_{m, x} (y)| \mathrm d \nu (y) \mathrm d \mu (x) \\ &= \int_X \int_Y |f_{n} (x, y) - f_{m} (x, y)| \mathrm d \nu (y) \mathrm d \mu (x) \\ &= \int_{X \times Y} |f_n - f_m| \mathrm d \lambda, \quad \text{ by Tonelli's theorem} \\ &= \|f_n - f_m \|_1. \end{align} It follows that $(\Phi_n)$ is a Cauchy sequence in $\mathcal L_1 (X, \mu, F')$ . Because $\mathcal L_1 (X, \mu, F')$ is complete, there exists $\Phi \in \mathcal L_1 (X, \mu, F')$ such that $\Phi_n \to \Phi$ in $\mathcal L_1 (X, \mu, F')$ . By Lemma 3 , there is a subsequence $\varphi$ of $(0, 1, \ldots,)$ such that $\Phi_{\varphi(n)} \to \Phi$ $\mu$ -a.e. Let $\Phi: x \mapsto [\hat f_x]$ with $\hat f_x \in \mathcal  L_1 (Y, \nu, E)$ . Then for $\mu$ -a.e. $x \in X$ , $[f_{\varphi(n), x}] \xrightarrow{n \to \infty} [\hat f_{x}]$ in $F'$ and thus $f_{\varphi(n), x} \xrightarrow{n \to \infty} \hat f_{x}$ in $\mathcal  L_1 (Y, \nu, E)$ . Notice that $\mathcal  L_1 (Y, \nu, E)$ is complete (w.r.t. the semi-norm $\| \cdot \|_1$ ), so $(f_{\varphi(n), x})_n$ is a Cauchy sequence in $\mathcal  L_1 (Y, \nu, E)$ for $\mu$ -a.e. $x \in X$ . By Lemma 3 again, for $\mu$ -a.e. $x \in X$ , there is a subsequence $\tau_x$ of $\varphi$ such that $f_{\tau_x(n), x} \xrightarrow{n \to \infty} \hat f_{x}$ $\nu$ -a.e. We have already proved that for $\mu$ -a.e. $x \in X$ , $f_{n,x} \xrightarrow{n \to \infty} f_x$ $\nu$ -a.e. Hence $\hat f_x = f_x$ $\nu$ -a.e. As such, for $\mu$ -a.e. $x \in X$ , $(f_{\varphi (n), x})_n$ is a Cauchy sequence in $\mathcal  L_1 (Y, \nu, E)$ that converges to $f_x$ both in $\mathcal  L_1 (Y, \nu, E)$ and $\nu$ -a.e. This completes the proof.","In generalizing Fubini's theorem to functions on Banach space, I encounter below result that takes me a great deal of time to prove. The proof is delicate for me because we play with as well as its quotient space . So it's likely that there are subtle mistakes that I'm unable to recognize. I really hope that somebody helps me check it out. Can we have a stronger result, i.e., is a Cauchy sequence that converges to -a.e.? Related definitions of Bochner integrals can be found here . Let and be complete -finite measure spaces, and a Banach space. . the product -algebra of and . the product measure of and . the space of -simple functions from to . the space of -measurable functions from to . the space of -integrable functions from to . Theorem: Let be a Cauchy sequence in that converges -a.e. to . We define by and . Then there is a subsequence of such that for -a.e. , is a Cauchy sequence in and converges to both in and -a.e. My attempt: First we need some lemmas. Lemma 1: Let and . Then for -a.e. . [A proof can be found here ] Lemma 2: Let and be the quotient space of . Then is defined -a.e. and belongs to , i.e., is integrable. Here is the equivalence class containing . [A proof can be found here ] Lemma 3: Let be a sequence in that converges to in . Then there exists a subsequence that converges -a.e. to . [A proof can be found here ] Let be a set on which . Let and be the -section and -section of respectively. By Tonelli's theorem, This means for -a.e. , . Then there is a null set such that for each , there is a null set such that for all . By Lemma 1 , there is a null set such that for all and for all . It follows that for each , is a sequence in that converges to on . Let be the quotient space of . By Lemma 2 , belongs to . Here is the equivalence class containing . We have It follows that is a Cauchy sequence in . Because is complete, there exists such that in . By Lemma 3 , there is a subsequence of such that -a.e. Let with . Then for -a.e. , in and thus in . Notice that is complete (w.r.t. the semi-norm ), so is a Cauchy sequence in for -a.e. . By Lemma 3 again, for -a.e. , there is a subsequence of such that -a.e. We have already proved that for -a.e. , -a.e. Hence -a.e. As such, for -a.e. , is a Cauchy sequence in that converges to both in and -a.e. This completes the proof.","\mathcal L_1 L_1 (f_{n, x})_n f_x \nu (X, \mathcal A, \mu) (Y, \mathcal B, \nu) \sigma (E, | \cdot |) \Sigma := \mathcal A \times \mathcal B \mathcal C :=\mathcal A \otimes \mathcal B \sigma \mathcal A \mathcal B \lambda := \mu \otimes \nu \mu \nu \mathcal S (X \times Y, \lambda, E) \lambda X \times Y E \mathcal  L_0 (Y, \nu, E) \nu Y E \mathcal  L_1 (Y, \nu, E) \nu Y E (f_n) \mathcal S (X \times Y, \lambda, E) \lambda f f_x,f_{n,x}: Y \to E f_x(y) := f (x, y) f_{n,x} (y) := f_n (x, y) \varphi (0, 1, 2, \ldots, ) \mu x \in X (f_{\varphi (n), x})_n \mathcal S (Y, \nu, E) f_x \mathcal  L_1 (Y, \nu, E) \nu f \in \mathcal S (X \times Y, \lambda, E) f_x : Y \to E, y \mapsto f(x, y) f_x \in\mathcal S (Y, \nu, E) \mu x \in X f \in \mathcal S (X \times Y, \lambda, E) F' := L_1 (Y, \nu, E) \mathcal  L_1 (Y, \nu, E) \Phi : x \mapsto [f_x] \mu \mathcal L_1 (X, \mu, F) \Phi [f_x] \in F' f_x \in \mathcal  L_1 (Y, \nu, E) (h_n) \mathcal  L_1 (Y, \nu, E) h \mathcal  L_1 (Y, \nu, E) (h_{\varphi(n)}) \nu h C \in \mathcal C f_n \not \to f C_x C^y x y C 0 = \lambda (C)=\int_X \nu (C_x) \mathrm d\mu(x) = \int_Y \mu (C^y) \mathrm d \nu(y). \mu x \in X \nu(C_x) =0 A \in \mathcal A x \in A^c B_x \in \mathcal B f_{n,x} (y) \to f_x (y) y \in B_x^c A \in \mathcal A f_x,f_{n, x} \in \mathcal S (Y, \nu, E) x \in A^c n \in \mathbb N x \in A^c (f_{n,x})_n \mathcal S (Y, \nu, E) f_x \in \mathcal S (Y, \nu, E) B_x^c F' := L_1 (Y, \nu, E) \mathcal  L_1 (Y, \nu, E) \Phi_n: x \mapsto [f_{n, x}] \mathcal L_1 (X, \mu, F') [f_{n, x}] \in F' f_{n, x} \in \mathcal  L_1 (Y, \nu, E) \begin{align}
\| \Phi_n - \Phi_m \|_1 &= \int_X \|\Phi_n (x) - \Phi_m (x) \|_{F'} \mathrm d \mu(x) \\
&= \int_X \int_Y |f_{n, x} (y) - f_{m, x} (y)| \mathrm d \nu (y) \mathrm d \mu (x) \\
&= \int_X \int_Y |f_{n} (x, y) - f_{m} (x, y)| \mathrm d \nu (y) \mathrm d \mu (x) \\
&= \int_{X \times Y} |f_n - f_m| \mathrm d \lambda, \quad \text{ by Tonelli's theorem} \\
&= \|f_n - f_m \|_1.
\end{align} (\Phi_n) \mathcal L_1 (X, \mu, F') \mathcal L_1 (X, \mu, F') \Phi \in \mathcal L_1 (X, \mu, F') \Phi_n \to \Phi \mathcal L_1 (X, \mu, F') \varphi (0, 1, \ldots,) \Phi_{\varphi(n)} \to \Phi \mu \Phi: x \mapsto [\hat f_x] \hat f_x \in \mathcal  L_1 (Y, \nu, E) \mu x \in X [f_{\varphi(n), x}] \xrightarrow{n \to \infty} [\hat f_{x}] F' f_{\varphi(n), x} \xrightarrow{n \to \infty} \hat f_{x} \mathcal  L_1 (Y, \nu, E) \mathcal  L_1 (Y, \nu, E) \| \cdot \|_1 (f_{\varphi(n), x})_n \mathcal  L_1 (Y, \nu, E) \mu x \in X \mu x \in X \tau_x \varphi f_{\tau_x(n), x} \xrightarrow{n \to \infty} \hat f_{x} \nu \mu x \in X f_{n,x} \xrightarrow{n \to \infty} f_x \nu \hat f_x = f_x \nu \mu x \in X (f_{\varphi (n), x})_n \mathcal  L_1 (Y, \nu, E) f_x \mathcal  L_1 (Y, \nu, E) \nu","['functional-analysis', 'measure-theory', 'solution-verification', 'banach-spaces', 'cauchy-sequences']"
31,Derivation of Mehler kernel,Derivation of Mehler kernel,,"I'm trying to get an intuitive feel for Fourier multipliers and using it to derive formulas for kernels. What I do know for instance is that the Laplacian $\Delta$ is multiplication by $M = -\lvert{\xi\rvert}^2$ in Fourier space, in the sense $\Delta = F^{-1} \circ M \circ F$ where $M$ is the multiplication operator, and that this idea can be used to find the heat kernel: the action of $e^{t\Delta}$ is then multiplication by $M' = e^{-t\lvert{\xi\rvert}^2}$ in Fourier coordinates and so $$ e^{t\Delta}f(x) = (F^{-1} \circ M'\circ F)f(x) = (2\pi)^{-n} \int\int e^{i\langle{x-y, \xi\rangle}}e^{-t\lvert{\xi\rvert}^2}f(y)dyd\xi. $$ We can then swap the order of integration to get $$\int e^{i\langle{x-y, \xi\rangle}}e^{-t\lvert{\xi\rvert}^2}d\xi = \pi^{n/2}t^{-n/2} e^{-\lvert{x-y\rvert}^2/(4t)} $$ which multiplied by $(2\pi)^{-n}$ gives the correct normalisation factor $(4\pi t)^{-n/2}$ out front. For the Ornstein-Uhlenbeck operator $L = -\frac{1}{2}\Delta + x \cdot \nabla$ , I am trying to apply the same ideas to get the Mehler kernel which according to http://www.math.chalmers.se/Math/Research/Preprints/2012/12.pdf (p14) is given by $$ M_t(x, y) = \frac{1}{\pi^{n/2}(1 - e^{-2t})^{n/2}} \exp\left(-\frac{\lvert{y-e^{-t}x\rvert}^2}{1-e^{-2t}}\right). $$ If I apply the same ideas, $L$ in Fourier coordinates should be multiplication by $\frac{1}{2}\lvert{\xi\rvert}^2 + i\langle{x, \xi\rangle}$ so I should be able to recover the kernel of $e^{-tL}$ through the integral $$ \int e^{i\langle{x-y, \xi\rangle}}e^{-\frac{t}{2}\lvert{\xi\rvert}^2 - it\langle{x, \xi\rangle}} d\xi $$ but I can't get this to agree even in dimension $1$ . What am I doing wrong?","I'm trying to get an intuitive feel for Fourier multipliers and using it to derive formulas for kernels. What I do know for instance is that the Laplacian is multiplication by in Fourier space, in the sense where is the multiplication operator, and that this idea can be used to find the heat kernel: the action of is then multiplication by in Fourier coordinates and so We can then swap the order of integration to get which multiplied by gives the correct normalisation factor out front. For the Ornstein-Uhlenbeck operator , I am trying to apply the same ideas to get the Mehler kernel which according to http://www.math.chalmers.se/Math/Research/Preprints/2012/12.pdf (p14) is given by If I apply the same ideas, in Fourier coordinates should be multiplication by so I should be able to recover the kernel of through the integral but I can't get this to agree even in dimension . What am I doing wrong?","\Delta M = -\lvert{\xi\rvert}^2 \Delta = F^{-1} \circ M \circ F M e^{t\Delta} M' = e^{-t\lvert{\xi\rvert}^2}  e^{t\Delta}f(x) = (F^{-1} \circ M'\circ F)f(x) = (2\pi)^{-n} \int\int e^{i\langle{x-y, \xi\rangle}}e^{-t\lvert{\xi\rvert}^2}f(y)dyd\xi.  \int e^{i\langle{x-y, \xi\rangle}}e^{-t\lvert{\xi\rvert}^2}d\xi = \pi^{n/2}t^{-n/2} e^{-\lvert{x-y\rvert}^2/(4t)}  (2\pi)^{-n} (4\pi t)^{-n/2} L = -\frac{1}{2}\Delta + x \cdot \nabla  M_t(x, y) = \frac{1}{\pi^{n/2}(1 - e^{-2t})^{n/2}} \exp\left(-\frac{\lvert{y-e^{-t}x\rvert}^2}{1-e^{-2t}}\right).  L \frac{1}{2}\lvert{\xi\rvert}^2 + i\langle{x, \xi\rangle} e^{-tL}  \int e^{i\langle{x-y, \xi\rangle}}e^{-\frac{t}{2}\lvert{\xi\rvert}^2 - it\langle{x, \xi\rangle}} d\xi  1","['functional-analysis', 'analysis', 'partial-differential-equations']"
32,"Prove that $(A_0,A_1)_{\theta,q}$ is Banach.",Prove that  is Banach.,"(A_0,A_1)_{\theta,q}","Let $A_0$ , $A_1$ be two Banach spaces, both embedded continuously in a Hausdorff topological vector space $\mathcal{A}$ . Then we can consider the normed spaces $A_0 \cap A_1$ and $A_0 + A_1$ with the norms $$ ||a||_{A_0 \cap A_1} = \max\{ ||a||_{A_0}, ||a||_{A_1}\}, $$ $$ ||a||_{A_0 + A_1} = \inf \{ ||a_0||_{A_0} + ||a_1||_{A_1} : a=a_0+a_1, a_j \in A_j\}, $$ and following this, define the real interpolation space $(A_0,A_1)_{\theta,q}$ consisting of those elements $a \in A_0 + A_1$ with finite norm: $$ ||a||_{\theta,q} = \left( \int_0^{\infty} \left(t^{-\theta}K(t,a) \right)^q \frac{dt}{t} \right)^{\frac{1}{q}}, 1\leq q <\infty, $$ where $K(t,a) = \inf\{ ||a_0||_{A_0} + t||a_1||_{A_1}: a=a_0+a_1, a_j \in A_j\}$ is Peetre K-functional. My task now is to prove that $(A_0,A_1)_{\theta,q}$ is a Banach space. I have already proven that, with this construction, $A_0 + A_1$ is a Banach space, as well as the fact that $(A_0,A_1)_{\theta,q}$ is continuously embedded in $A_0 + A_1$ with the norms defined above. So, if I'm not mistaken, it would be enough to prove that the interpolation space $(A_0,A_1)_{\theta,q}$ is a closed subspace of $A_0+A_1$ and using the fact that the latter is Banach, the former would be Banach as well. However, my efforts have been unfruitful so far. I'd like to show that if I take a succession $\{x_n\} \subset (A_0,A_1)_{\theta,q}$ that converges to $x\in A_0+A_1$ , then: $$ ||x||_{\theta,q} = \left( \int_0^{\infty} \left(t^{-\theta}K(t,x) \right)^q \frac{dt}{t} \right)^{\frac{1}{q}} <\infty, $$ so $x \in (A_0,A_1)_{\theta,q}$ , proving it's a closed subspace. Any hints on how to proceed towards my goal would be appreciated.","Let , be two Banach spaces, both embedded continuously in a Hausdorff topological vector space . Then we can consider the normed spaces and with the norms and following this, define the real interpolation space consisting of those elements with finite norm: where is Peetre K-functional. My task now is to prove that is a Banach space. I have already proven that, with this construction, is a Banach space, as well as the fact that is continuously embedded in with the norms defined above. So, if I'm not mistaken, it would be enough to prove that the interpolation space is a closed subspace of and using the fact that the latter is Banach, the former would be Banach as well. However, my efforts have been unfruitful so far. I'd like to show that if I take a succession that converges to , then: so , proving it's a closed subspace. Any hints on how to proceed towards my goal would be appreciated.","A_0 A_1 \mathcal{A} A_0 \cap A_1 A_0 + A_1 
||a||_{A_0 \cap A_1} = \max\{ ||a||_{A_0}, ||a||_{A_1}\},
 
||a||_{A_0 + A_1} = \inf \{ ||a_0||_{A_0} + ||a_1||_{A_1} : a=a_0+a_1, a_j \in A_j\},
 (A_0,A_1)_{\theta,q} a \in A_0 + A_1 
||a||_{\theta,q} = \left( \int_0^{\infty} \left(t^{-\theta}K(t,a) \right)^q \frac{dt}{t} \right)^{\frac{1}{q}}, 1\leq q <\infty,
 K(t,a) = \inf\{ ||a_0||_{A_0} + t||a_1||_{A_1}: a=a_0+a_1, a_j \in A_j\} (A_0,A_1)_{\theta,q} A_0 + A_1 (A_0,A_1)_{\theta,q} A_0 + A_1 (A_0,A_1)_{\theta,q} A_0+A_1 \{x_n\} \subset (A_0,A_1)_{\theta,q} x\in A_0+A_1 
||x||_{\theta,q} = \left( \int_0^{\infty} \left(t^{-\theta}K(t,x) \right)^q \frac{dt}{t} \right)^{\frac{1}{q}} <\infty,
 x \in (A_0,A_1)_{\theta,q}","['functional-analysis', 'banach-spaces', 'interpolation-theory']"
33,Let $X$ be an infinite dimensional Banach space. Prove that every Hamel basis of $X$ is uncountable.,Let  be an infinite dimensional Banach space. Prove that every Hamel basis of  is uncountable.,X X,Let $X$ be an infinite dimensional Banach space. Prove that every Hamel basis of $X$ is uncountable. Can anyone help how can I solve the above problem?,Let be an infinite dimensional Banach space. Prove that every Hamel basis of is uncountable. Can anyone help how can I solve the above problem?,X X,"['functional-analysis', 'banach-spaces', 'normed-spaces', 'baire-category', 'hamel-basis']"
34,A simple maximization problem in a convex set and the projection operator,A simple maximization problem in a convex set and the projection operator,,"Let $H$ be a Hilbert space and let $g \in H$ be a fixed nonzero element. Consider the maximization problem $$\max \left\{ (f,g): f \in H \quad \text{and} \quad \|f\|\leq 1 \right\}$$ This maximum is equal to $\|g\|$ and a maximizer function is $f_0 = g/\|g\|$ . Suppose now $K \subset H$ is convex and closed. Consider the above maximization problem, but restricted to $K$ $$\max \left\{ (f,g): f \in K \quad \text{and} \quad \|f\|\leq 1 \right\} \tag{1}$$ My question is, what is the above maximum value and can we describe a maximizer using the projection operator $P:H \to K$ ? If not, under what general assumptions (on $K$ and $g$ ) can we do this? In special cases this is possible. For example suppose $K$ is a cone (or a subspace), then $g$ satisfies the condition $$(g-Pg,Pg) = 0 \tag{2}$$ Now by the characterization of the projection $(g-Pg,k-Pg) \leq 0$ for any $k \in K$ . Applying this to $k=f$ and using (2) gives $(g-Pg,f) \leq 0$ , hence $$(f,g) = (f,Pg) + (f, g-Pg) \leq (f,Pg)$$ So assuming in addition that $K$ is a cone, the maximum in (1) is bounded by $\|Pg\|$ and this value is in fact attained and the maximizer is $f_1 = Pg/\|Pg\|$ . More generally, what can be said when $K$ is just a closed and convex set? Any insight or references will be much appreciated!","Let be a Hilbert space and let be a fixed nonzero element. Consider the maximization problem This maximum is equal to and a maximizer function is . Suppose now is convex and closed. Consider the above maximization problem, but restricted to My question is, what is the above maximum value and can we describe a maximizer using the projection operator ? If not, under what general assumptions (on and ) can we do this? In special cases this is possible. For example suppose is a cone (or a subspace), then satisfies the condition Now by the characterization of the projection for any . Applying this to and using (2) gives , hence So assuming in addition that is a cone, the maximum in (1) is bounded by and this value is in fact attained and the maximizer is . More generally, what can be said when is just a closed and convex set? Any insight or references will be much appreciated!","H g \in H \max \left\{ (f,g): f \in H \quad \text{and} \quad \|f\|\leq 1 \right\} \|g\| f_0 = g/\|g\| K \subset H K \max \left\{ (f,g): f \in K \quad \text{and} \quad \|f\|\leq 1 \right\} \tag{1} P:H \to K K g K g (g-Pg,Pg) = 0 \tag{2} (g-Pg,k-Pg) \leq 0 k \in K k=f (g-Pg,f) \leq 0 (f,g) = (f,Pg) + (f, g-Pg) \leq (f,Pg) K \|Pg\| f_1 = Pg/\|Pg\| K","['functional-analysis', 'convex-analysis', 'hilbert-spaces', 'convex-optimization', 'quadratic-programming']"
35,Convergence in the resolvent-sense and spectral properties,Convergence in the resolvent-sense and spectral properties,,"I'm reading the chapter about unbounded operators in [Reed,Simon,""Methods in modern mathematical physics"", vol. 1] Let $\{T_k\}_k, T$ be unbounded selfadjoint operators on a Hilbert space $H$ . If $T_k\to T$ in the norm-resolvent sense, then for any $(a,b)\subset \mathbb R$ with $\{a,b\}\cap \sigma(T)=\emptyset$ , the spectral projections $P_{(a,b)}(T_k)\to P_{(a,b)}(T)$ converge in norm. This in particular means that the spectrum of the limiting operator cannot suddenly contract. While working on a specific case, I constructed a sequence of selfadjoint operators satisfying, for some purely imaginary number $\lambda$ $$\|\Pi_FR(\lambda, T_k)\Pi_F-\Pi_FR(\lambda,T)\Pi_F\|\to 0.$$ $\Pi$ is the orthogonal projection and $F$ is a subspace of $H$ . What does this convergence say about the spectrum of $T_k,T$ ? Maybe about $\Pi_F T\Pi_F$ ? Is there a weaker version of the previous statement?","I'm reading the chapter about unbounded operators in [Reed,Simon,""Methods in modern mathematical physics"", vol. 1] Let be unbounded selfadjoint operators on a Hilbert space . If in the norm-resolvent sense, then for any with , the spectral projections converge in norm. This in particular means that the spectrum of the limiting operator cannot suddenly contract. While working on a specific case, I constructed a sequence of selfadjoint operators satisfying, for some purely imaginary number is the orthogonal projection and is a subspace of . What does this convergence say about the spectrum of ? Maybe about ? Is there a weaker version of the previous statement?","\{T_k\}_k, T H T_k\to T (a,b)\subset \mathbb R \{a,b\}\cap \sigma(T)=\emptyset P_{(a,b)}(T_k)\to P_{(a,b)}(T) \lambda \|\Pi_FR(\lambda, T_k)\Pi_F-\Pi_FR(\lambda,T)\Pi_F\|\to 0. \Pi F H T_k,T \Pi_F T\Pi_F","['functional-analysis', 'spectral-theory', 'unbounded-operators']"
36,Does eigenvector of Stokes operator are eigenvectors of the Laplacian?,Does eigenvector of Stokes operator are eigenvectors of the Laplacian?,,"The Stokes operator is defined by $Au=-P_H\Delta u$ , where $u\in D(A)=H^2(\Omega)\cap V$ and $P_H$ is the Leray's projector. From Spectral Theorem, we can obtain an orthonormal basis of eigenvectors $(w_i)_{i\in \mathbb{N}}$ of H, i.e., $$Aw_j=\lambda_iw_i,\ \forall i\in\mathbb{N}.$$ I was reading a paper where the authors uses the following property $$-\Delta w_i=\lambda_i w_i,\tag{1}\label{1}$$ i.e, $w_i$ is an eigenvector of the Laplacian too. Someone told me that's because $P_H$ and $-\Delta$ commutes for the eigenvectors. First I would like to know if they commutes for the eigenvectors of the Stokes operator and why. If they don't, I would like to know if \eqref{1} is true. I will be satisfied with a good reference, I was not able to find one. I've read they do not commute, in general. In the absence of boundaries, for example, they commute, but I couldn't find anything with respect to the eigenvectors.","The Stokes operator is defined by , where and is the Leray's projector. From Spectral Theorem, we can obtain an orthonormal basis of eigenvectors of H, i.e., I was reading a paper where the authors uses the following property i.e, is an eigenvector of the Laplacian too. Someone told me that's because and commutes for the eigenvectors. First I would like to know if they commutes for the eigenvectors of the Stokes operator and why. If they don't, I would like to know if \eqref{1} is true. I will be satisfied with a good reference, I was not able to find one. I've read they do not commute, in general. In the absence of boundaries, for example, they commute, but I couldn't find anything with respect to the eigenvectors.","Au=-P_H\Delta u u\in D(A)=H^2(\Omega)\cap V P_H (w_i)_{i\in \mathbb{N}} Aw_j=\lambda_iw_i,\ \forall i\in\mathbb{N}. -\Delta w_i=\lambda_i w_i,\tag{1}\label{1} w_i P_H -\Delta","['functional-analysis', 'partial-differential-equations', 'fluid-dynamics']"
37,Functional that leads to hollow,Functional that leads to hollow,,"Consider a field $\phi(x,y)$ in the 2D plane. The minimum of the following Functional leads to formation of a 2D disk in which $\phi(x,y)=1$ deep inside the disk and $\phi(x,y)=0$ outside the disk.The boundary of the disk has a thickness so that $\phi$ continuously gors from 1 to 0. $$\DeclareMathOperator{\Dm}{\operatorname{d\!}} F=\int \Dm x \Dm y \big(\phi^2(1-\phi)^2 + K (\nabla \phi)^2 \big) $$ with $K>0$ and a constraint on $\phi$ as following: $\int \phi dx dy =c$ where $c>>1$ . Is it possible to add a term to the above functional so that the minimum of $F$ becomes a disk with a hollow circle at the centre? I thought about considering a term which favours negative curvature like $-\nabla^2 \phi$ , but since this term can be written as a divergence, its integral is zero and would not change the $F$ . I would appreciate it if some one could clarify if this question has a solution or not.","Consider a field in the 2D plane. The minimum of the following Functional leads to formation of a 2D disk in which deep inside the disk and outside the disk.The boundary of the disk has a thickness so that continuously gors from 1 to 0. with and a constraint on as following: where . Is it possible to add a term to the above functional so that the minimum of becomes a disk with a hollow circle at the centre? I thought about considering a term which favours negative curvature like , but since this term can be written as a divergence, its integral is zero and would not change the . I would appreciate it if some one could clarify if this question has a solution or not.","\phi(x,y) \phi(x,y)=1 \phi(x,y)=0 \phi \DeclareMathOperator{\Dm}{\operatorname{d\!}}
F=\int \Dm x \Dm y \big(\phi^2(1-\phi)^2 + K (\nabla \phi)^2 \big)
 K>0 \phi \int \phi dx dy =c c>>1 F -\nabla^2 \phi F","['functional-analysis', 'optimization', 'functional-equations', 'calculus-of-variations', 'variational-analysis']"
38,"Proper name for a ""polytope"" when defined by linear functionals","Proper name for a ""polytope"" when defined by linear functionals",,"Is there a name for the set of functions $f:[0,1]\to\mathbb{R}$ which satisfy a finite set of equations $\int_0^1 f(x)w_i(x)dx\geq c_i$ for $i=1,\ldots,n$ ? If $f$ had a discrete domain and the integral was replaced with summation, the feasible set could have been called a polytope ..","Is there a name for the set of functions which satisfy a finite set of equations for ? If had a discrete domain and the integral was replaced with summation, the feasible set could have been called a polytope ..","f:[0,1]\to\mathbb{R} \int_0^1 f(x)w_i(x)dx\geq c_i i=1,\ldots,n f","['linear-algebra', 'functional-analysis', 'functional-inequalities']"
39,"Lebesgue integral of two functions are equal, then the functions are equal a.e.","Lebesgue integral of two functions are equal, then the functions are equal a.e.",,"Let $\psi,\phi$ be non negative measurable functions on the measure space $(X,\Sigma,\mu)$ . Such that, $\psi \le \phi$ . If, $$\int_{X} \psi ~ d\mu = \int_{X} \phi ~ d\mu,$$ then $\psi = \phi$ a.e. on $X$ . Let, $g(x) = \phi(x) - \psi(x).$ Then $\forall \lambda \in \mathbb{R}^{+},$ $$ \mu(\{ x \in X: g(x) \ge \lambda \} ) \le \frac{1}{\lambda}\int_{X}g(x) \le \frac{1}{\lambda}(\int_{X}\phi(x) ~ d\mu  ~  - ~ \int_{X}\psi(x) ~ d\mu  ) = 0.$$ Let, $V = \{x \in X: g(x) \ne 0 \}$ and $E_{n} = \{x \in X: g(x) \ge \frac{1}{n} \}$ for all  natural numbers $n \ge 1$ . We have that, $$V = \bigcup^{\infty}_{n=1}E_n $$ Then, $$\mu(V) = \mu(\bigcup^{\infty}_{n=1}E_n) = \lim_{n \rightarrow \infty} \mu(E_n) = 0. $$ Therefore, $\psi = \phi$ a.e. on $X$ . Is this proof correct?","Let be non negative measurable functions on the measure space . Such that, . If, then a.e. on . Let, Then Let, and for all  natural numbers . We have that, Then, Therefore, a.e. on . Is this proof correct?","\psi,\phi (X,\Sigma,\mu) \psi \le \phi \int_{X} \psi ~ d\mu = \int_{X} \phi ~ d\mu, \psi = \phi X g(x) = \phi(x) - \psi(x). \forall \lambda \in \mathbb{R}^{+},  \mu(\{ x \in X: g(x) \ge \lambda \} ) \le \frac{1}{\lambda}\int_{X}g(x) \le \frac{1}{\lambda}(\int_{X}\phi(x) ~ d\mu  ~  - ~ \int_{X}\psi(x) ~ d\mu  ) = 0. V = \{x \in X: g(x) \ne 0 \} E_{n} = \{x \in X: g(x) \ge \frac{1}{n} \} n \ge 1 V = \bigcup^{\infty}_{n=1}E_n  \mu(V) = \mu(\bigcup^{\infty}_{n=1}E_n) = \lim_{n \rightarrow \infty} \mu(E_n) = 0.  \psi = \phi X","['functional-analysis', 'solution-verification']"
40,What is the dual space of a signed measures with zero mean,What is the dual space of a signed measures with zero mean,,"For locally compact space $X$ , Let $C_c(X)$ be the compactly supported, continuous functions on $X$ to $\mathbb{C}$ . It is known theorem ( Riesz-Markov-Kakutani ) that the dual space of $C_c(X)$ is characterized by Radon measures on $X$ , say ${\cal M}(X)$ . My question is, what is the dual space of the subspace. $$ {\cal M}_0(X) = \left\{\mu \in {\cal M}(X)\bigg| \int_X \mathrm d\mu = 0\right\} $$ Here is my initial idea: Consider an equivalence relation for $f, g \in C_c(X)$ : $$ f\sim g \quad \text{if} \quad f(x)-g(x) = c \quad \text{for some }c\in\mathbb{C}, \forall x\in X $$ and we can consider equivalence class $[f]\subset C_c(X)$ . The claim is the family of this equivalence class is dual to ${\cal M}_0(X)$ . The duality pairing is the natural pairing, while it is well-defined because $\mu(f) = \mu(g)$ if $f\sim g$ . For metric or norm, $\|[f]\| = \inf_{f\in[f]} \|f\|$ is adopted. How can I validate the guess, or is there other conditions that I need to refine?","For locally compact space , Let be the compactly supported, continuous functions on to . It is known theorem ( Riesz-Markov-Kakutani ) that the dual space of is characterized by Radon measures on , say . My question is, what is the dual space of the subspace. Here is my initial idea: Consider an equivalence relation for : and we can consider equivalence class . The claim is the family of this equivalence class is dual to . The duality pairing is the natural pairing, while it is well-defined because if . For metric or norm, is adopted. How can I validate the guess, or is there other conditions that I need to refine?","X C_c(X) X \mathbb{C} C_c(X) X {\cal M}(X) 
{\cal M}_0(X) = \left\{\mu \in {\cal M}(X)\bigg| \int_X \mathrm d\mu = 0\right\}
 f, g \in C_c(X) 
f\sim g \quad \text{if} \quad f(x)-g(x) = c \quad \text{for some }c\in\mathbb{C}, \forall x\in X
 [f]\subset C_c(X) {\cal M}_0(X) \mu(f) = \mu(g) f\sim g \|[f]\| = \inf_{f\in[f]} \|f\|","['functional-analysis', 'measure-theory', 'duality-theorems', 'dual-spaces']"
41,Sufficient Condition for stable Convergence,Sufficient Condition for stable Convergence,,"Let $X$ be some Banach space, $\mathcal{L}(X)$ be the space of bounded linear operators $T:X\to X$ with $\mathrm{dom}(T)=X$ . Let $T\in \mathcal{L}(X)$ and $(T_n)$ be a sequence in $\mathcal{L}(X)$ . We define stable convergence of $T_n$ to $T$ ( $T_n\overset{s}{\to}T)$ by $\forall x\in X:\ T_nx\to Tx$ , $\exists M>0,N\in \mathbb{N}:\forall n>N: T_n^{-1}\in \mathcal{L}(X)$ and $\Vert T_n^{-1}\Vert \leq M$ . I am struggling now with following argument: Let $S\in \mathcal{L}(X)$ and $z\in\rho(S)=\{z\in \mathbb{C}:(S-z)^{-1}\in \mathcal{L}(X)\}$ , consider a sequence $S_n\in \mathcal{L}(X)$ with $S_nx\to S x\ \forall x\in X$ . The proof I am trying to understand now uses Stability of $S_n-z$ can be written as $\Vert (S_n-z)x_n \Vert\geq M \Vert x_n\Vert$ for $x_n\in X, n\geq N$ . (*) I See that (*) is necessary for the stable convergence, but why is this condition sufficient for $S_n-z$ having an inverse in $\mathcal{L}(X)$ ? For example: If I take $T:\mathcal{l}^2\to\mathcal{l}^2,\ (x_1,x_2,x_3,\dots,)\mapsto (0,x_1,x_2,\dots)$ , I clearly have $\Vert Tx\Vert=\Vert x \Vert$ but $T$ is not surjective. What am I missing?","Let be some Banach space, be the space of bounded linear operators with . Let and be a sequence in . We define stable convergence of to ( by , and . I am struggling now with following argument: Let and , consider a sequence with . The proof I am trying to understand now uses Stability of can be written as for . (*) I See that (*) is necessary for the stable convergence, but why is this condition sufficient for having an inverse in ? For example: If I take , I clearly have but is not surjective. What am I missing?","X \mathcal{L}(X) T:X\to X \mathrm{dom}(T)=X T\in \mathcal{L}(X) (T_n) \mathcal{L}(X) T_n T T_n\overset{s}{\to}T) \forall x\in X:\ T_nx\to Tx \exists M>0,N\in \mathbb{N}:\forall n>N: T_n^{-1}\in \mathcal{L}(X) \Vert T_n^{-1}\Vert \leq M S\in \mathcal{L}(X) z\in\rho(S)=\{z\in \mathbb{C}:(S-z)^{-1}\in \mathcal{L}(X)\} S_n\in \mathcal{L}(X) S_nx\to S x\ \forall x\in X S_n-z \Vert (S_n-z)x_n \Vert\geq M \Vert x_n\Vert x_n\in X, n\geq N S_n-z \mathcal{L}(X) T:\mathcal{l}^2\to\mathcal{l}^2,\ (x_1,x_2,x_3,\dots,)\mapsto (0,x_1,x_2,\dots) \Vert Tx\Vert=\Vert x \Vert T",['functional-analysis']
42,Coercivity of a bilinear form on $L^2$,Coercivity of a bilinear form on,L^2,"This problem is from a mathematical analysis exam Define this bilinear form on $L^2([0,1])$ : $$ a(u,v)=\int_{[0,1]^2}{k(x,y) u(x) v(y) dx dy} \qquad \forall u,v \in L^2([0,1]) $$ Can this bilinear form be coercive on $L^2$ for some choice of $k \in L^{\infty}([0,1]^2)$ ? I am quite concerned by the ""official"" solution, which is Yes, the bilinear form $ a $ is coercive as long as we make the following assumptions regarding the Kernel function $ k $ : $ k(x,x) \in C([0,1]) $ ; $ \inf_{x \in [0,1]} k(x,x) = a_0 > 0 $ ; $ k(x,y) = 0 \;\;\; \forall \; x \ne y $ . Using these three conditions, $ \forall u \in L^2([0,1]) $ we get $$ \begin{align} a(u,u) & = \int_{[0,1]^2} k(x,y) \: u(x) \: u(y) \:  dx \, dy \\ & = \int_{0}^{1} k(x,x) \: u(x)^2 \: dx \\ & \ge a_0 \int_{0}^{1} u(x)^2 dx = a_0 \| u \|_2^{2} \end{align} $$ My point is that in the previous example, $k$ is almost everywhere zero on $[0,1]^2$ , so, the bilinear form should be identically zero. Instead, in order to make this passages, one should take $k(x,y)=\delta_x(y)$ , which of course is not in $L^\infty$","This problem is from a mathematical analysis exam Define this bilinear form on : Can this bilinear form be coercive on for some choice of ? I am quite concerned by the ""official"" solution, which is Yes, the bilinear form is coercive as long as we make the following assumptions regarding the Kernel function : ; ; . Using these three conditions, we get My point is that in the previous example, is almost everywhere zero on , so, the bilinear form should be identically zero. Instead, in order to make this passages, one should take , which of course is not in","L^2([0,1]) 
a(u,v)=\int_{[0,1]^2}{k(x,y) u(x) v(y) dx dy} \qquad \forall u,v \in
L^2([0,1])  L^2 k \in L^{\infty}([0,1]^2)  a   k   k(x,x) \in C([0,1])   \inf_{x \in [0,1]} k(x,x) = a_0 > 0   k(x,y) = 0 \;\;\; \forall \; x \ne y   \forall u \in L^2([0,1])   \begin{align} a(u,u) & = \int_{[0,1]^2} k(x,y) \: u(x) \: u(y) \:
 dx \, dy \\ & = \int_{0}^{1} k(x,x) \: u(x)^2 \: dx \\ & \ge a_0 \int_{0}^{1} u(x)^2 dx = a_0 \| u \|_2^{2} \end{align}  k [0,1]^2 k(x,y)=\delta_x(y) L^\infty","['functional-analysis', 'lp-spaces', 'bilinear-form']"
43,generator of semigroup of multiplication operators on $L^p$,generator of semigroup of multiplication operators on,L^p,"Suppose $1 \leq p <\infty$ . Let $(T_t)_{t \geq 0}$ be a strongly continuous semigroup of multiplication operators on $L^p(0,1)$ defined by $T_t(f)=m_t \times f$ where the function $m_t \colon [0,1] \to \mathbb{R}$ is measurable. How show that there exists a measurable function $g$ on $[0,1]$ such that $m_t=e^{tg}$ almost everywhere for all $t \geq 0 $ ? I tried the following. For any $t,t' \geq 0$ , the relation $T_tT_{t'}(1_{[0,1]})=T_{t+t'}(1_{[0,1]})$ gives $m_t(x)m_{t'}(x)=m_{t+t'}(x)$ for almost all $x$ and $m_0=1_{[0,1]}$ . By the characterization of the exponential function (problem of the measurability of $t \mapsto m_t(x)$ ?), we obtain $m_t(x)=e^{tg(x)}$ for some $g(x)$ . Now, why $g$ is measurable and why we can define $g$ on all $[0,1]$ ?","Suppose . Let be a strongly continuous semigroup of multiplication operators on defined by where the function is measurable. How show that there exists a measurable function on such that almost everywhere for all ? I tried the following. For any , the relation gives for almost all and . By the characterization of the exponential function (problem of the measurability of ?), we obtain for some . Now, why is measurable and why we can define on all ?","1 \leq p <\infty (T_t)_{t \geq 0} L^p(0,1) T_t(f)=m_t \times f m_t \colon [0,1] \to \mathbb{R} g [0,1] m_t=e^{tg} t \geq 0  t,t' \geq 0 T_tT_{t'}(1_{[0,1]})=T_{t+t'}(1_{[0,1]}) m_t(x)m_{t'}(x)=m_{t+t'}(x) x m_0=1_{[0,1]} t \mapsto m_t(x) m_t(x)=e^{tg(x)} g(x) g g [0,1]","['real-analysis', 'functional-analysis', 'lp-spaces', 'semigroup-of-operators']"
44,What is the dual of $l^p$ for $0<p<1$?,What is the dual of  for ?,l^p 0<p<1,"$l^p$ for $1\leq p<\infty$ is defined as the vector space on which $$x\mapsto\sqrt[p]{\sum_j{|x_j|^p}}$$ is a norm.  For $0<p<1$ , this cannot be a norm, but (as Wiki indicates ) the function $$\mapsto\sum_j{|x_j|^p}$$ is still a metric, so it's a (complete) topological vector space. If $1\leq p<\infty$ , the dual of $l^p$ is well-known to be $l^q$ , where $\frac{1}{p}+\frac{1}{q}=1$ . What is the (topological) dual of $l^p$ for $0<p<1$ ? In that case, $l^p\subseteq l^1$ , so $l^{\infty}\subseteq(l^p)^*$ .  But I don't know what else is in there.","for is defined as the vector space on which is a norm.  For , this cannot be a norm, but (as Wiki indicates ) the function is still a metric, so it's a (complete) topological vector space. If , the dual of is well-known to be , where . What is the (topological) dual of for ? In that case, , so .  But I don't know what else is in there.",l^p 1\leq p<\infty x\mapsto\sqrt[p]{\sum_j{|x_j|^p}} 0<p<1 \mapsto\sum_j{|x_j|^p} 1\leq p<\infty l^p l^q \frac{1}{p}+\frac{1}{q}=1 l^p 0<p<1 l^p\subseteq l^1 l^{\infty}\subseteq(l^p)^*,"['functional-analysis', 'banach-spaces', 'dual-spaces']"
45,these two norms are equivalent,these two norms are equivalent,,"We have from book Bases in Banach spaces II by ivan singer ""Proposition"" Let $\{G_n\}$ be a sequence of closed linear subspaces of a Banach space $E$ and let $D_1$ be the linear space of sequences of elements $$D_1 = \Bigl\{\{y_n\} \subset E \big| y_n \in G_n \text{ }(n=1,2,...),\text{ }\displaystyle\sum_{i=1}^\infty y_i \text{ }converges\Big \},$$ endowed with the norm $$\|\{y_n\}\| = \displaystyle \sup_{1 \leq n <\infty}\|\displaystyle\sum_{i=1}^n y_i\|.$$ Then $D_1$ is a Banach space. and ""Proposition"" Let $\{G_n\}$ be a decomposition of a Banach space $E$ , such that each $G_n (n = 1,2, ...)$ is closed, and let $\{v_n\}$ be the associated sequence of coordinate projections to $\{G_n\}$ . Then a) The Banach space $D_1$ introduced in the last proposition is isomorphic to $E$ , by the mapping $$w: \{y_n\} \rightarrow \displaystyle\sum_{i=1}^\infty y_i.$$ b) The numbers $$|||x||| = \displaystyle\sup_{1 \leq n < \infty} \|\sum_{i=1}^n v_i(x)\| (x \in E)$$ define a norm on the space $E,$ equivalent to the initial norm of $E.$ On the other hand, If we have a sequence of Banach spaces $(X_n)_{n=1}^\infty$ and a number $1 \leq p <\infty$ , we define the $l_p$ -sum of $X_1, X_2,...$ to be the space of all sequences $(x_n)_{n=1}^\infty$ , with $x_n \in X_n$ for $n=1, 2, ...$ , for which $\sum_{n=1}^\infty \|x_n\|_{X_n}^p < \infty,$ in case $p < \infty,$ or $\|(x_n)\|_\infty = \sup_n \|x_n\|_{X_n} < \infty,$ in case $p = \infty,$ and we use also the shorthand $(X_1 \oplus X_2 \oplus ...)_p$ to denote this new space. In brief, for any $1 \leq p \leq \infty,$ we have $$(X_1 \oplus X_2 \oplus ...)_p = \{(x_n): x_n \in X_n \text{ and } (\|x_n\|)_{n=1}^\infty \in l_p \}.$$ Also, The $c_0$ -sum of spaces is defined in an entirely analogous fashion. In this case we write $$(X_1 \oplus X_2 \oplus ...)_0 = \{(x_n): x_n \in X_n \text{ and } (\|x_n\|)_{n=1}^\infty \in c_0\}.$$ We also always have $$(l_p \oplus l_p \oplus ...)_p = l_p \text{ and } (c_0 \oplus c_0 \oplus ...) = c_0,$$ So say in $(l_p \oplus l_p \oplus ...)_p = l_p$ space if we take $x = (v_i(x))_{i=1}^\infty \in l_p$ space then so $|||x|||= \displaystyle\sup_{1 \leq n < \infty} \|\sum_{i=1}^n v_i(x)\|$ is equivalent to $(\sum_{i=1}^\infty \|v_i(x)\|^p)^{1/p}$ Question So, if $x \in l_p$ space, as the two norms are equivalent does $$|||x||| = \sup_{1 \leq n < \infty} \|\sum_{i=1}^n (0,0,...,0,v_i(x),0,...)\| \leq c (\sum_{i=1}^\infty \|(0,0,...,0,v_i(x),0,...)\|_{l_p}^p)^{1/p} = c (\sum_{i=1}^\infty ((\|v_i(x)\|^p)^{1/p})^p)^{1/p} = c (\sum_{i=1}^\infty \|v_i(x)\|^p)^{1/p} = c \|x\|??$$","We have from book Bases in Banach spaces II by ivan singer ""Proposition"" Let be a sequence of closed linear subspaces of a Banach space and let be the linear space of sequences of elements endowed with the norm Then is a Banach space. and ""Proposition"" Let be a decomposition of a Banach space , such that each is closed, and let be the associated sequence of coordinate projections to . Then a) The Banach space introduced in the last proposition is isomorphic to , by the mapping b) The numbers define a norm on the space equivalent to the initial norm of On the other hand, If we have a sequence of Banach spaces and a number , we define the -sum of to be the space of all sequences , with for , for which in case or in case and we use also the shorthand to denote this new space. In brief, for any we have Also, The -sum of spaces is defined in an entirely analogous fashion. In this case we write We also always have So say in space if we take space then so is equivalent to Question So, if space, as the two norms are equivalent does","\{G_n\} E D_1 D_1 = \Bigl\{\{y_n\} \subset E \big| y_n \in G_n \text{ }(n=1,2,...),\text{ }\displaystyle\sum_{i=1}^\infty y_i \text{ }converges\Big \}, \|\{y_n\}\| = \displaystyle \sup_{1 \leq n <\infty}\|\displaystyle\sum_{i=1}^n y_i\|. D_1 \{G_n\} E G_n (n = 1,2, ...) \{v_n\} \{G_n\} D_1 E w: \{y_n\} \rightarrow \displaystyle\sum_{i=1}^\infty y_i. |||x||| = \displaystyle\sup_{1 \leq n < \infty} \|\sum_{i=1}^n v_i(x)\| (x \in E) E, E. (X_n)_{n=1}^\infty 1 \leq p <\infty l_p X_1, X_2,... (x_n)_{n=1}^\infty x_n \in X_n n=1, 2, ... \sum_{n=1}^\infty \|x_n\|_{X_n}^p < \infty, p < \infty, \|(x_n)\|_\infty = \sup_n \|x_n\|_{X_n} < \infty, p = \infty, (X_1 \oplus X_2 \oplus ...)_p 1 \leq p \leq \infty, (X_1 \oplus X_2 \oplus ...)_p = \{(x_n): x_n \in X_n \text{ and } (\|x_n\|)_{n=1}^\infty \in l_p \}. c_0 (X_1 \oplus X_2 \oplus ...)_0 = \{(x_n): x_n \in X_n \text{ and } (\|x_n\|)_{n=1}^\infty \in c_0\}. (l_p \oplus l_p \oplus ...)_p = l_p \text{ and } (c_0 \oplus c_0 \oplus ...) = c_0, (l_p \oplus l_p \oplus ...)_p = l_p x = (v_i(x))_{i=1}^\infty \in l_p |||x|||= \displaystyle\sup_{1 \leq n < \infty} \|\sum_{i=1}^n v_i(x)\| (\sum_{i=1}^\infty \|v_i(x)\|^p)^{1/p} x \in l_p |||x||| = \sup_{1 \leq n < \infty} \|\sum_{i=1}^n (0,0,...,0,v_i(x),0,...)\| \leq c (\sum_{i=1}^\infty \|(0,0,...,0,v_i(x),0,...)\|_{l_p}^p)^{1/p} = c (\sum_{i=1}^\infty ((\|v_i(x)\|^p)^{1/p})^p)^{1/p} = c (\sum_{i=1}^\infty \|v_i(x)\|^p)^{1/p} = c \|x\|??","['sequences-and-series', 'functional-analysis', 'normed-spaces', 'banach-spaces', 'schauder-basis']"
46,Inclusion of Sobolev spaces (from a proof by Bogachev - Krylov - Röckner - Shaposhnikov),Inclusion of Sobolev spaces (from a proof by Bogachev - Krylov - Röckner - Shaposhnikov),,"In a book by Bogachev-Krylov-Rockner-Shaposhnikov, I found the following statement that concludes a proof but I do not understand. I underlined in red the critical parts. $\rho(\cdot,t)>0$ is a probability density function solving a certain parabolic PDE in a weak sense. The function $f_\epsilon$ approximates $\rho(\cdot,t)$ as $\epsilon\to0$ , precisely: $$ f_\epsilon(x,t) \,:=\, \big(\rho(\cdot,t)*w_\epsilon\big)(x) \,+\, \epsilon\,\max(1,|x|)^{-d-1}$$ where $w_\epsilon(x)=\frac{1}{(2\pi\epsilon^2)^{d/2}}\,e^{-|x|^2/(2\epsilon^2)}\,$ . The Sobolev space $W^{2,1}$ denotes those functions with weak derivatives up to the first order belonging to $L^2$ (notice that the indices of differentiability and integrability are reversed with respect to ""usual"" notation). How can I deduce the last red statement ? The first red part is clear, since $$\nabla \sqrt{\rho(\cdot,t)} \,=\, \frac{1}{2}\,\frac{\nabla\rho(\cdot,t)}{\sqrt{\rho(\cdot,t)}}$$ hence the approximation argument shows that this weak gradient exists and belongs to $L^2(\mathbb R^d)$ . Now, since $\rho(\cdot,t)>0$ is a probability density function (by hypothesis), the second red part follows from the first red part by Cauchy-Schwarz inequality: $$\int_{\mathbb R^d} |\nabla\rho(x,t)|\,d x \,\leq\, \int_{\mathbb R^d} \frac{|\nabla\rho(x,t)|^2}{\rho(x,t)}\,d x \ \underbrace{\int_{\mathbb R^d} \rho(x,t)\,d x}_{=\,1} \ <\infty $$ for almost all $t\in(0,\tau)$ , hence $\rho(\cdot,t)\in W^{1,1}(\mathbb R^d)$ for almost all $t\in(0,\tau)$ . I don't understand why the last red statement holds true. $\sqrt{\rho}\in\mathbb H^{2,2}$ means -by definition of this space- that $\partial_i\partial_j \sqrt{\rho(\cdot,t)}\in L^2(\mathbb R^d)$ for almost all $t\in(0,\tau)$ and $$\int_0^\tau \int|\partial_i\partial_j \sqrt{\rho(x,t)}|^2 d x\, dt <\infty\,.$$ Why this is the case? Why the weak second derivatives even exist? Edit. The full reference is Theorem 7.4.1 in the book ""Fokker-Planck-Kolmogorov equations"" by Bogachev, Krylov, Röckner, Shaposhnikov. Here's the statement:","In a book by Bogachev-Krylov-Rockner-Shaposhnikov, I found the following statement that concludes a proof but I do not understand. I underlined in red the critical parts. is a probability density function solving a certain parabolic PDE in a weak sense. The function approximates as , precisely: where . The Sobolev space denotes those functions with weak derivatives up to the first order belonging to (notice that the indices of differentiability and integrability are reversed with respect to ""usual"" notation). How can I deduce the last red statement ? The first red part is clear, since hence the approximation argument shows that this weak gradient exists and belongs to . Now, since is a probability density function (by hypothesis), the second red part follows from the first red part by Cauchy-Schwarz inequality: for almost all , hence for almost all . I don't understand why the last red statement holds true. means -by definition of this space- that for almost all and Why this is the case? Why the weak second derivatives even exist? Edit. The full reference is Theorem 7.4.1 in the book ""Fokker-Planck-Kolmogorov equations"" by Bogachev, Krylov, Röckner, Shaposhnikov. Here's the statement:","\rho(\cdot,t)>0 f_\epsilon \rho(\cdot,t) \epsilon\to0  f_\epsilon(x,t) \,:=\, \big(\rho(\cdot,t)*w_\epsilon\big)(x) \,+\, \epsilon\,\max(1,|x|)^{-d-1} w_\epsilon(x)=\frac{1}{(2\pi\epsilon^2)^{d/2}}\,e^{-|x|^2/(2\epsilon^2)}\, W^{2,1} L^2 \nabla \sqrt{\rho(\cdot,t)} \,=\, \frac{1}{2}\,\frac{\nabla\rho(\cdot,t)}{\sqrt{\rho(\cdot,t)}} L^2(\mathbb R^d) \rho(\cdot,t)>0 \int_{\mathbb R^d} |\nabla\rho(x,t)|\,d x \,\leq\, \int_{\mathbb R^d} \frac{|\nabla\rho(x,t)|^2}{\rho(x,t)}\,d x \ \underbrace{\int_{\mathbb R^d} \rho(x,t)\,d x}_{=\,1} \ <\infty  t\in(0,\tau) \rho(\cdot,t)\in W^{1,1}(\mathbb R^d) t\in(0,\tau) \sqrt{\rho}\in\mathbb H^{2,2} \partial_i\partial_j \sqrt{\rho(\cdot,t)}\in L^2(\mathbb R^d) t\in(0,\tau) \int_0^\tau \int|\partial_i\partial_j \sqrt{\rho(x,t)}|^2 d x\, dt <\infty\,.","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'weak-derivatives']"
47,Hilbert-Schmidt integral operator for a positive definite kernel is positive,Hilbert-Schmidt integral operator for a positive definite kernel is positive,,"Let $(X, \mathcal{A}, \mu)$ be a measure space and $k \colon X \times X \to \mathbb{R}$ be a measurable positive-definite kernel such that $\int_{X} \int_X k^2(x,y) \,\mathrm{d}\mu(x) \, \mathrm{d}\mu(y) < \infty$ . Then it is well known that the operator $T \colon L^2(\mu) \to L^2(\mu)$ defined by $$ Tf(x) := \int_X k(x,y) f(y) \, \mathrm{d}\mu(y) $$ is a Hilbert-Schmidt operator. Since $k(x,y) = k(y,x)$ , it is easy to see from Fubini's theorem that $$ \langle Tf, g \rangle = \int_X \left( \int_X k(x,y) f(y) \, \mathrm{d} \mu(y) \right) g(x) \, \mathrm{d} \mu(x) = \int_X \left( \int_X k(y,x) g(x) \, \mathrm{d} \mu(x) \right) f(y) \, \mathrm{d} \mu(y) = \langle f, Tg \rangle $$ and hence $T$ is self-adjoint. How do I show that $T$ is positive? That is, I want to show that for every $f \in L^2(\mu)$ $$ \langle Tf, f \rangle = \int_X \left( \int_X k(x,y) f(y) \, \mathrm{d} \mu(y) \right) f(x) \, \mathrm{d} \mu(x) \ge 0 $$ We need to somehow use the positive-definiteness of $k$ , but I am unable to do that unless I assume $\mu$ is something simple like a counting measure. Ultimately, I want to be able to say that the eigenvalues of $T$ are such that $\lambda_1 \ge \lambda_2 \ge \cdots > 0$ .","Let be a measure space and be a measurable positive-definite kernel such that . Then it is well known that the operator defined by is a Hilbert-Schmidt operator. Since , it is easy to see from Fubini's theorem that and hence is self-adjoint. How do I show that is positive? That is, I want to show that for every We need to somehow use the positive-definiteness of , but I am unable to do that unless I assume is something simple like a counting measure. Ultimately, I want to be able to say that the eigenvalues of are such that .","(X, \mathcal{A}, \mu) k \colon X \times X \to \mathbb{R} \int_{X} \int_X k^2(x,y) \,\mathrm{d}\mu(x) \, \mathrm{d}\mu(y) < \infty T \colon L^2(\mu) \to L^2(\mu) 
Tf(x) := \int_X k(x,y) f(y) \, \mathrm{d}\mu(y)
 k(x,y) = k(y,x) 
\langle Tf, g \rangle = \int_X \left( \int_X k(x,y) f(y) \, \mathrm{d} \mu(y) \right) g(x) \, \mathrm{d} \mu(x) = \int_X \left( \int_X k(y,x) g(x) \, \mathrm{d} \mu(x) \right) f(y) \, \mathrm{d} \mu(y) = \langle f, Tg \rangle
 T T f \in L^2(\mu) 
\langle Tf, f \rangle = \int_X \left( \int_X k(x,y) f(y) \, \mathrm{d} \mu(y) \right) f(x) \, \mathrm{d} \mu(x) \ge 0
 k \mu T \lambda_1 \ge \lambda_2 \ge \cdots > 0","['functional-analysis', 'measure-theory', 'operator-theory', 'hilbert-spaces', 'integral-transforms']"
48,Question about the definition of feature map arising in machine learning,Question about the definition of feature map arising in machine learning,,"I'm working through the following paper of learning a non-negative function in a reproducing kernel hilbert space setting (RKHS). In particular, section 2.2 on page 3 is a bit confusing to me in terms of the feature map. We are given a kernel $K:X\times X\to\mathbb{R}$ and we denote with $\mathcal{H}$ the corresponding RKHS of $K$ . Usually, one denotes the feature map by $$k_x:=K(\cdot, x):X\to \mathcal{H}$$ with the property then that $K(x,y) = \langle k_y, k_x\rangle$ . A function $\phi$ satisfying the property $$ K(x,y) = \langle \phi(y), \phi(x)\rangle $$ is often called the feature map and the function $k_x$ the canonical feature map. It is well known that such a feature map is not unique. The idea of the paper is to look at sum of square functions. This is strongly motivated from polynomials. In the polynomial case one says the polynomial $p(x)$ has a sum of square (SoS) representation if there are other polynomials $q_i$ such that $$ p(x) = \sum_{i=1}^d q_i(x)^2$$ If we denote the vector of monomials up to degree $d$ by $\phi(x)=(1,x, \dots,x^d)^T$ one can write $q(x)=(q_1(x),\dots,q_d(x))^T$ as $$q(x)=V\phi(x)$$ where the rows of $V$ contain the coefficient of the polynomials $q_i$ . For univariate polynimoials its well known that being positive and having such a SoS representation is equivalent. In the above context one can then write $$p(x)=\sum_i^dq_i(x)^2=q(x)^Tq(x) = \phi(x)^TV^TV\phi(x)$$ with $Q:=V^TV$ being pos semidefinite. Even for generic feature map $\phi$ (not necessary monomials) the condition on the pos. semidefiniteness of $Q$ leads to a non-negative function $p(x)$ . In the paper they try to generalize the above idea. For a kenerl $K:X\times X\to\mathbb{R}$ they define an associated feature map via $\phi:X\to\mathcal{H}$ , where $\phi(x):=(\phi_i(x))_{i\in I}$ for $\phi_i:X\to\mathbb{R}$ and a index set $I$ , e.g $\{1, 2, 3,\dots\}$ . They define the space of sum of squares as $$\mathcal{S}:=\{x\mapsto \phi^T(x)Q\phi(x): Q\ge 0\}\tag{1}$$ where $Q\ge 0$ means a positive semidefinite matrix $Q$ . Couple of question to these definitions Why is such a $\phi$ a feature map? It seems to me that $\phi$ is a tuple of real valued functions while $K$ is scalar valued. That doesn't make much sense to me. In the polynomial motivation $\phi_i$ where basically a basis to represent $q$ . But I don't see how this translate to the general one. They allow for countably infinite index set. How does this make sense in $(1)$ ? They claim that $\mathcal{S}$ is a subspace of the Hilbert Space $\mathcal{H}\otimes\mathcal{H}$ . Why can $\mathcal{H}\otimes\mathcal{H}$ be represented as $$\{x\mapsto \sum_{i,j} Q_{i,j}\phi_i(x)\phi_j(x):Q\in\mathbb{R}^{d\times d}\} $$","I'm working through the following paper of learning a non-negative function in a reproducing kernel hilbert space setting (RKHS). In particular, section 2.2 on page 3 is a bit confusing to me in terms of the feature map. We are given a kernel and we denote with the corresponding RKHS of . Usually, one denotes the feature map by with the property then that . A function satisfying the property is often called the feature map and the function the canonical feature map. It is well known that such a feature map is not unique. The idea of the paper is to look at sum of square functions. This is strongly motivated from polynomials. In the polynomial case one says the polynomial has a sum of square (SoS) representation if there are other polynomials such that If we denote the vector of monomials up to degree by one can write as where the rows of contain the coefficient of the polynomials . For univariate polynimoials its well known that being positive and having such a SoS representation is equivalent. In the above context one can then write with being pos semidefinite. Even for generic feature map (not necessary monomials) the condition on the pos. semidefiniteness of leads to a non-negative function . In the paper they try to generalize the above idea. For a kenerl they define an associated feature map via , where for and a index set , e.g . They define the space of sum of squares as where means a positive semidefinite matrix . Couple of question to these definitions Why is such a a feature map? It seems to me that is a tuple of real valued functions while is scalar valued. That doesn't make much sense to me. In the polynomial motivation where basically a basis to represent . But I don't see how this translate to the general one. They allow for countably infinite index set. How does this make sense in ? They claim that is a subspace of the Hilbert Space . Why can be represented as","K:X\times X\to\mathbb{R} \mathcal{H} K k_x:=K(\cdot, x):X\to \mathcal{H} K(x,y) = \langle k_y, k_x\rangle \phi  K(x,y) = \langle \phi(y), \phi(x)\rangle  k_x p(x) q_i  p(x) = \sum_{i=1}^d q_i(x)^2 d \phi(x)=(1,x, \dots,x^d)^T q(x)=(q_1(x),\dots,q_d(x))^T q(x)=V\phi(x) V q_i p(x)=\sum_i^dq_i(x)^2=q(x)^Tq(x) = \phi(x)^TV^TV\phi(x) Q:=V^TV \phi Q p(x) K:X\times X\to\mathbb{R} \phi:X\to\mathcal{H} \phi(x):=(\phi_i(x))_{i\in I} \phi_i:X\to\mathbb{R} I \{1, 2, 3,\dots\} \mathcal{S}:=\{x\mapsto \phi^T(x)Q\phi(x): Q\ge 0\}\tag{1} Q\ge 0 Q \phi \phi K \phi_i q (1) \mathcal{S} \mathcal{H}\otimes\mathcal{H} \mathcal{H}\otimes\mathcal{H} \{x\mapsto \sum_{i,j} Q_{i,j}\phi_i(x)\phi_j(x):Q\in\mathbb{R}^{d\times d}\} ","['functional-analysis', 'tensor-products', 'machine-learning', 'reproducing-kernel-hilbert-spaces']"
49,"Prove a subset of $C[-1,1]$ is a closed subspace.",Prove a subset of  is a closed subspace.,"C[-1,1]","Consider $C[-1,1]$ under the sup-norm. Let $$X= \{f\in C[-1,1]:\int_{-1}^{0}f=\int_{0}^{1}f=0\},$$ Prove $X$ is a closed subspace of $C[-1,1]$ My attempt: It is trivial to prove $X$ is a subspace. To prove $X$ is closed: consider a sequence of function $\{f_k\}$ in $X$ that converges to $f$ in $C[-1,1]$ . Then $\forall \epsilon>0,\exists N\in \mathbb{N}$ such that $$\lVert f_n-f_m\rVert_{\infty}\leq\epsilon$$ for $n,m\geq N$ , and $$\lim_{n\rightarrow\infty}\lVert f_n-f_m\rVert_{\infty}=\lVert\lim_{n\rightarrow\infty}f_n-f_m\rVert_{\infty}=\lVert f-f_m\rVert_{\infty}\leq\epsilon$$ by continuity of the sup-norm. By the property of $X$ : $$\int_{-1}^{0}f-f_m\leq\int_{-1}^{0}\vert f-f_m \vert\leq \int_{-1}^{0}\lVert f-f_m\rVert_{\infty}\leq\epsilon$$ Then $-\epsilon\leq\int_{-1}^{0}f\leq\epsilon$ since $\int_{-1}^{0}f_m=0$ . We conclude $\int_{-1}^{0}f=0 $ and similarly $\int_{0}^{1}f=0$ . As $\{f_k\}$ are continuous functions uniformly converges to $f$ , $f$ is continuous. Therefore, $f\in X$ and $X$ is a closed subspace in $C[-1,1]$ . Thanks in advance.","Consider under the sup-norm. Let Prove is a closed subspace of My attempt: It is trivial to prove is a subspace. To prove is closed: consider a sequence of function in that converges to in . Then such that for , and by continuity of the sup-norm. By the property of : Then since . We conclude and similarly . As are continuous functions uniformly converges to , is continuous. Therefore, and is a closed subspace in . Thanks in advance.","C[-1,1] X= \{f\in C[-1,1]:\int_{-1}^{0}f=\int_{0}^{1}f=0\}, X C[-1,1] X X \{f_k\} X f C[-1,1] \forall \epsilon>0,\exists N\in \mathbb{N} \lVert f_n-f_m\rVert_{\infty}\leq\epsilon n,m\geq N \lim_{n\rightarrow\infty}\lVert f_n-f_m\rVert_{\infty}=\lVert\lim_{n\rightarrow\infty}f_n-f_m\rVert_{\infty}=\lVert f-f_m\rVert_{\infty}\leq\epsilon X \int_{-1}^{0}f-f_m\leq\int_{-1}^{0}\vert f-f_m \vert\leq \int_{-1}^{0}\lVert f-f_m\rVert_{\infty}\leq\epsilon -\epsilon\leq\int_{-1}^{0}f\leq\epsilon \int_{-1}^{0}f_m=0 \int_{-1}^{0}f=0  \int_{0}^{1}f=0 \{f_k\} f f f\in X X C[-1,1]","['functional-analysis', 'continuity', 'solution-verification']"
50,Prove that $\sigma_{\text {ess}} (A)$ is a closed subset of $\mathbb R.$,Prove that  is a closed subset of,\sigma_{\text {ess}} (A) \mathbb R.,"Let $A$ be a self-adjoint operator on a Hilbert space $\mathcal H.$ Let $E_A$ be the unique spectral measure associated to $A$ obtained from spectral theory for self-adjoint operators defined on the Borel- $\sigma$ -algebra of subsets of $\left [-\|A\|, \|A\| \right ]$ i.e. $$A = \int_{\left [-\|A\|, \|A\| \right ]} t\ dE_A(t).$$ Let $\sigma (A)$ denote the spectrum of $A.$ Then what I know is that for any self-adjoint operator $A$ on a Hilbert space $\mathcal H$ we have $\sigma (A) = \text {supp} (E_A),$ where $\text {supp} (E_A)$ denotes the support of $E_A.$ This shows that if $\lambda \in \sigma (A)$ then $E_A (\lambda - \varepsilon, \lambda + \varepsilon) \neq 0,$ for every $\varepsilon \gt 0.$ This leads us to the following subdivision of the spectrum $\sigma (A)$ of $A.$ An element $\lambda \in \sigma (A)$ is said to be an essential spectrum of $A$ if the range of the projection $E_A (\lambda - \varepsilon, \lambda + \varepsilon)$ is infinite dimensional for every $\varepsilon \gt 0.$ Otherwise we say that $\lambda$ is a discrete spectrum of $A.$ The collection of all essential spectrum of $A$ is denoted by $\sigma_{\text {ess}} (A)$ and the collection of all discrete spectrum of $A$ is denoted by $\sigma_{\text {disc}} (A).$ Now two results have been left as ( easy ) exercises which are the following $:$ $(1)$ $\sigma_{\text {ess}} (A)$ is a closed subset of $\mathbb R$ for any self-adjoint operator $A$ on a Hilbert space $\mathcal H.$ $(2)$ If $\lambda \in \sigma_p(A)$ has infinite multiplicity then $\lambda \in \sigma_{\text {ess}} (A),$ where $\sigma_p (A)$ denotes the point spectrum (or the collection of eigenvalues) of $A.$ But I find it difficult to prove the first one. I have tried by taking a sequence $\{\lambda_n\}_{n \geq 1}$ in $\sigma_{\text {ess}} (A)$ converging to $\lambda.$ Then the range of the projection $E_A (\lambda_n - \varepsilon, \lambda_n + \varepsilon)$ is infinite dimensional for every $\varepsilon \gt 0$ and for all $n \geq 1.$ But how does it guarantee that the range of the projection $E_A (\lambda - \varepsilon, \lambda + \varepsilon)$ is also infinite dimensional for all $\varepsilon \gt 0\ $ ? I have asked about it to our instructor. He told me that it is an one line argument . But I don't know why can't I able to see the proof. Also I don't have any idea about the second one. May be I am so stupid. Would anybody give me some suggestion here? I am totally confused at thus stage about how to proceed further. Any help regarding this will be warmly appreciated. Thanks! EDIT $:$ Finally I am able to prove the first one. Let us take $\varepsilon \gt 0$ arbitrarily. Let $\{\lambda_n\}_{n \geq 1}$ be a sequence in $\sigma_{\text {ess}} (A)$ converging to $\lambda \in \mathbb R.$ So there exists $N \geq 1$ such that $\lambda_n \in (\lambda - \varepsilon, \lambda + \varepsilon),$ for all $n \geq N.$ In particular $\lambda_N \in (\lambda - \varepsilon, \lambda + \varepsilon).$ Choose $\delta \gt 0$ small enough so that $(\lambda_N - \delta, \lambda_N + \delta) \subseteq (\lambda - \varepsilon, \lambda + \varepsilon).$ This implies that $E_A ((\lambda_N - \delta, \lambda_N + \delta)) \leq E_A ((\lambda - \varepsilon, \lambda + \varepsilon)).$ But this in turn implies that $$\text {Range} \left (E_A ((\lambda_N - \delta, \lambda_N + \delta)) \right ) \subseteq \text {Range} \left ( E_A ((\lambda - \varepsilon, \lambda + \varepsilon)) \right ).$$ Now since $\lambda_N \in \sigma_{\text {ess}} (A)$ it follows that $\text {Range} \left (E_A ((\lambda_N - \delta, \lambda_N + \delta)) \right )$ is infinite dimensional and hence we have $\text {Range} \left (E_A ((\lambda - \varepsilon, \lambda + \varepsilon)) \right )$ is infinite dimensional. This completes the proof. Now how do I prove the second one? Do anybody give any idea about it? Thanks!",Let be a self-adjoint operator on a Hilbert space Let be the unique spectral measure associated to obtained from spectral theory for self-adjoint operators defined on the Borel- -algebra of subsets of i.e. Let denote the spectrum of Then what I know is that for any self-adjoint operator on a Hilbert space we have where denotes the support of This shows that if then for every This leads us to the following subdivision of the spectrum of An element is said to be an essential spectrum of if the range of the projection is infinite dimensional for every Otherwise we say that is a discrete spectrum of The collection of all essential spectrum of is denoted by and the collection of all discrete spectrum of is denoted by Now two results have been left as ( easy ) exercises which are the following is a closed subset of for any self-adjoint operator on a Hilbert space If has infinite multiplicity then where denotes the point spectrum (or the collection of eigenvalues) of But I find it difficult to prove the first one. I have tried by taking a sequence in converging to Then the range of the projection is infinite dimensional for every and for all But how does it guarantee that the range of the projection is also infinite dimensional for all ? I have asked about it to our instructor. He told me that it is an one line argument . But I don't know why can't I able to see the proof. Also I don't have any idea about the second one. May be I am so stupid. Would anybody give me some suggestion here? I am totally confused at thus stage about how to proceed further. Any help regarding this will be warmly appreciated. Thanks! EDIT Finally I am able to prove the first one. Let us take arbitrarily. Let be a sequence in converging to So there exists such that for all In particular Choose small enough so that This implies that But this in turn implies that Now since it follows that is infinite dimensional and hence we have is infinite dimensional. This completes the proof. Now how do I prove the second one? Do anybody give any idea about it? Thanks!,"A \mathcal H. E_A A \sigma \left [-\|A\|, \|A\| \right ] A = \int_{\left [-\|A\|, \|A\| \right ]} t\ dE_A(t). \sigma (A) A. A \mathcal H \sigma (A) = \text {supp} (E_A), \text {supp} (E_A) E_A. \lambda \in \sigma (A) E_A (\lambda - \varepsilon, \lambda + \varepsilon) \neq 0, \varepsilon \gt 0. \sigma (A) A. \lambda \in \sigma (A) A E_A (\lambda - \varepsilon, \lambda + \varepsilon) \varepsilon \gt 0. \lambda A. A \sigma_{\text {ess}} (A) A \sigma_{\text {disc}} (A). : (1) \sigma_{\text {ess}} (A) \mathbb R A \mathcal H. (2) \lambda \in \sigma_p(A) \lambda \in \sigma_{\text {ess}} (A), \sigma_p (A) A. \{\lambda_n\}_{n \geq 1} \sigma_{\text {ess}} (A) \lambda. E_A (\lambda_n - \varepsilon, \lambda_n + \varepsilon) \varepsilon \gt 0 n \geq 1. E_A (\lambda - \varepsilon, \lambda + \varepsilon) \varepsilon \gt 0\  : \varepsilon \gt 0 \{\lambda_n\}_{n \geq 1} \sigma_{\text {ess}} (A) \lambda \in \mathbb R. N \geq 1 \lambda_n \in (\lambda - \varepsilon, \lambda + \varepsilon), n \geq N. \lambda_N \in (\lambda - \varepsilon, \lambda + \varepsilon). \delta \gt 0 (\lambda_N - \delta, \lambda_N + \delta) \subseteq (\lambda - \varepsilon, \lambda + \varepsilon). E_A ((\lambda_N - \delta, \lambda_N + \delta)) \leq E_A ((\lambda - \varepsilon, \lambda + \varepsilon)). \text {Range} \left (E_A ((\lambda_N - \delta, \lambda_N + \delta)) \right ) \subseteq \text {Range} \left ( E_A ((\lambda - \varepsilon, \lambda + \varepsilon)) \right ). \lambda_N \in \sigma_{\text {ess}} (A) \text {Range} \left (E_A ((\lambda_N - \delta, \lambda_N + \delta)) \right ) \text {Range} \left (E_A ((\lambda - \varepsilon, \lambda + \varepsilon)) \right )","['functional-analysis', 'hilbert-spaces', 'spectral-theory', 'self-adjoint-operators']"
51,How does the Cameron-Martin space of a Gaussian Markov process with initial condition $0$ change when we consider a random initial condition,How does the Cameron-Martin space of a Gaussian Markov process with initial condition  change when we consider a random initial condition,0,"Suppose we are given a real-valued Gaussian Markov process with initial condition zero $$Y_t = \int_0^t f(s) dW_s \quad t \in [0,T], f\in L^2([0,T].$$ Following [Example 4.5, 1], $Y$ is distributed according to Gaussian measure on the canonical path space $C[0,T]$ with Cameron-Martin space given by $$H_Y =\left\{ h: h(t)= \int_0^t f(s)l(s)\: ds, l\in L^2([0,T]) \right\}$$ I would like to show that for a process $$Y_t = Y_0 +\int_0^t f(s) dW_s \quad t \in [0,T]$$ with initial condition $Y_0 \sim \mathcal N(0, \sigma), \sigma >0$ , the Cameron-Martin space changes as one would expect $$H_Y =\left\{ h: h(t)= y_0+ \int_0^t f(s)l(s)\: ds, l\in L^2([0,T]), y_0\in \mathbb R \right\}$$ This holds true in a couple of examples, however, I cannot seem to prove it in the general case. Any help is appreciated! [1] Lifshits (2012). Lectures on Gaussian processes.","Suppose we are given a real-valued Gaussian Markov process with initial condition zero Following [Example 4.5, 1], is distributed according to Gaussian measure on the canonical path space with Cameron-Martin space given by I would like to show that for a process with initial condition , the Cameron-Martin space changes as one would expect This holds true in a couple of examples, however, I cannot seem to prove it in the general case. Any help is appreciated! [1] Lifshits (2012). Lectures on Gaussian processes.","Y_t = \int_0^t f(s) dW_s \quad t \in [0,T], f\in L^2([0,T]. Y C[0,T] H_Y =\left\{ h: h(t)= \int_0^t f(s)l(s)\: ds, l\in L^2([0,T]) \right\} Y_t = Y_0 +\int_0^t f(s) dW_s \quad t \in [0,T] Y_0 \sim \mathcal N(0, \sigma), \sigma >0 H_Y =\left\{ h: h(t)= y_0+ \int_0^t f(s)l(s)\: ds, l\in L^2([0,T]), y_0\in \mathbb R \right\}","['functional-analysis', 'probability-distributions', 'stochastic-processes', 'markov-process', 'gaussian-measure']"
52,Nuclearity of the free product of C* algebras,Nuclearity of the free product of C* algebras,,"Let $A, B$ be unital $C^*$ algebras and $A\star B$ their unital (universal) free product. Assume that $A$ and $B$ are nuclear. When can we say that $A\star B$ is nuclear? I know that it can happen, e.g. $\mathbb{C}^{2}\star \mathbb{C}^{2}=\mathrm{C}^*(\mathbb{Z}_2\star \mathbb{Z}_2)$ is nuclear as $\mathbb{Z}_2\star \mathbb{Z}_2$ is amenable, but on the other hand $\mathrm{C}^*(\mathbb{Z})\star \mathrm{C}^*( \mathbb{Z})=\mathrm{C}^*(F_2)$ is not nuclear. I would be also interested in an analogous question for the reduced free products.","Let be unital algebras and their unital (universal) free product. Assume that and are nuclear. When can we say that is nuclear? I know that it can happen, e.g. is nuclear as is amenable, but on the other hand is not nuclear. I would be also interested in an analogous question for the reduced free products.","A, B C^* A\star B A B A\star B \mathbb{C}^{2}\star \mathbb{C}^{2}=\mathrm{C}^*(\mathbb{Z}_2\star \mathbb{Z}_2) \mathbb{Z}_2\star \mathbb{Z}_2 \mathrm{C}^*(\mathbb{Z})\star \mathrm{C}^*( \mathbb{Z})=\mathrm{C}^*(F_2)","['functional-analysis', 'operator-algebras', 'harmonic-analysis']"
53,Distance from intersection inequality.,Distance from intersection inequality.,,"Let $X$ be a Banach space and $Y$ and $Z$ closed subspaces of $X$ . Suppose that $Y+Z$ is closed in $X$ . Prove that there exists some $M > 0$ such that $\text{dist}(x, Y\cap Z) \leq M\left(\text{dist}(x, Y) + \text{dist}(x, Z)\right)$ for all $x\in X$ . Attempt: Since $Y$ , $Z$ and $Y+Z$ are closed subspaces we know that $Y\times Z$ and $Y+Z$ are Banach spaces where $\lVert (y,z)\rVert_{Y\times Z} = \lVert y \rVert + \lVert z \rVert$ . Define $T:Y\times Z \to Y + Z$ with $T(y,z) = y+z$ . Clearly $T$ is surjective, linear and bounded operator. From the open mapping theorem we deduce that there exists some $M > 0$ such that for all $x \in Y+Z$ there exist $y \in Y$ and $z \in Z$ such that $x=y+z$ and $\lVert y \rVert + \lVert z \rVert \leq M \lVert x \rVert$ . I can't continue form there. Any hint would be appreciated. Thanks.","Let be a Banach space and and closed subspaces of . Suppose that is closed in . Prove that there exists some such that for all . Attempt: Since , and are closed subspaces we know that and are Banach spaces where . Define with . Clearly is surjective, linear and bounded operator. From the open mapping theorem we deduce that there exists some such that for all there exist and such that and . I can't continue form there. Any hint would be appreciated. Thanks.","X Y Z X Y+Z X M > 0 \text{dist}(x, Y\cap Z) \leq M\left(\text{dist}(x, Y) + \text{dist}(x, Z)\right) x\in X Y Z Y+Z Y\times Z Y+Z \lVert (y,z)\rVert_{Y\times Z} = \lVert y \rVert + \lVert z \rVert T:Y\times Z \to Y + Z T(y,z) = y+z T M > 0 x \in Y+Z y \in Y z \in Z x=y+z \lVert y \rVert + \lVert z \rVert \leq M \lVert x \rVert","['real-analysis', 'functional-analysis', 'operator-theory', 'banach-spaces']"
54,Signal Processing in Functional (Dual) Space of the Schwartz Space,Signal Processing in Functional (Dual) Space of the Schwartz Space,,"I am reading a book on signal processing, rigor level of which is lower than a thorough introduction to functional analysis and is higher than an engineering introduction of signals and systems. The biggest difference of the material, from engineering signal processing books, is to explain the space of tempered distributions using Schwartz functions and to explain how $\delta$ is defined a tempered distribution, and to introduce Fourier transforms on the space of tempered distributions. I am half way through the book, and I have some questions to ask. The biggest question I have, is how to view previous engineering writing. For instance, we know that $\delta$ is not a function, but a distribution. We are used to seeing equations of the following style: \begin{equation} \delta * f = f, \end{equation} and \begin{equation} \delta(x-a) * f\left(x\right) = f\left(x - a\right). \end{equation} My current thought is that, these writings are abbreviations for the following steps: 1. do convolution in functional space 2. convert the result back to function space (if there is a preimage). In this case, there is no need to raise $f$ to a tempered distribution, as a function convolving a distribution is well-defined. In general, my understanding is that, whenever some operation in an engineering signals book is invalid in the function space (no dirac delta, Fourier transform doesn't exist, etc), raise that to the functional space, and use the corresponding operation in the functional space to find a distribution. If the distribution is of a preimage in the function space, convert it back for engineering understanding. Of course, previous conclusions are still valid, but the true theoretical thoughts behind them are more complicated than they look. Is this understanding correct?","I am reading a book on signal processing, rigor level of which is lower than a thorough introduction to functional analysis and is higher than an engineering introduction of signals and systems. The biggest difference of the material, from engineering signal processing books, is to explain the space of tempered distributions using Schwartz functions and to explain how is defined a tempered distribution, and to introduce Fourier transforms on the space of tempered distributions. I am half way through the book, and I have some questions to ask. The biggest question I have, is how to view previous engineering writing. For instance, we know that is not a function, but a distribution. We are used to seeing equations of the following style: and My current thought is that, these writings are abbreviations for the following steps: 1. do convolution in functional space 2. convert the result back to function space (if there is a preimage). In this case, there is no need to raise to a tempered distribution, as a function convolving a distribution is well-defined. In general, my understanding is that, whenever some operation in an engineering signals book is invalid in the function space (no dirac delta, Fourier transform doesn't exist, etc), raise that to the functional space, and use the corresponding operation in the functional space to find a distribution. If the distribution is of a preimage in the function space, convert it back for engineering understanding. Of course, previous conclusions are still valid, but the true theoretical thoughts behind them are more complicated than they look. Is this understanding correct?","\delta \delta \begin{equation}
\delta * f = f,
\end{equation} \begin{equation}
\delta(x-a) * f\left(x\right) = f\left(x - a\right).
\end{equation} f","['functional-analysis', 'signal-processing']"
55,Pre-dual of the measure space $\mathcal{M}(X)$,Pre-dual of the measure space,\mathcal{M}(X),"I have to find the 'pre-dual of the measure space $\mathcal{M}(X)$ '. $X$ can be assumed to be Polish and equipped with the Borel $\sigma$ algebra. This is all I'm given and it's a bit vague. What I found so far is this: if $X$ is locally compact and Hausdorff, then any element in the dual of $C_c(X)$ (the space of continuous compactly supported complex-valued functions on $X$ ) corresponds to a unique regular Borel measure on $X$ , i.e. for $\psi \in (C_c(X))'$ , we have $$\psi(f)=\int_X f(x) \,d\mu(x)$$ for such a measure $\mu$ . But do we 'hit' all such measures, i.e. can we really identify the dual of $C_c(X)$ with the space of regular Borel measures? Would the mapping $\psi\mapsto\mu$ only be bijective, or can we make it, say, into an isomorphism (using the total variation norm)? Is there another space whose dual is the space of all Borel measures on $X$ , without making the assumptions that $X$ is locally compact and Hausdorff? (Also, if you know good literature on this please let me know, I was having a hard time finding something.) (Edit: for anyone interested (and German speaking): Funktionalanalysis by Dirk Werner seems to cover the topic well.)","I have to find the 'pre-dual of the measure space '. can be assumed to be Polish and equipped with the Borel algebra. This is all I'm given and it's a bit vague. What I found so far is this: if is locally compact and Hausdorff, then any element in the dual of (the space of continuous compactly supported complex-valued functions on ) corresponds to a unique regular Borel measure on , i.e. for , we have for such a measure . But do we 'hit' all such measures, i.e. can we really identify the dual of with the space of regular Borel measures? Would the mapping only be bijective, or can we make it, say, into an isomorphism (using the total variation norm)? Is there another space whose dual is the space of all Borel measures on , without making the assumptions that is locally compact and Hausdorff? (Also, if you know good literature on this please let me know, I was having a hard time finding something.) (Edit: for anyone interested (and German speaking): Funktionalanalysis by Dirk Werner seems to cover the topic well.)","\mathcal{M}(X) X \sigma X C_c(X) X X \psi \in (C_c(X))' \psi(f)=\int_X f(x) \,d\mu(x) \mu C_c(X) \psi\mapsto\mu X X","['functional-analysis', 'measure-theory', 'dual-spaces', 'borel-measures']"
56,realization of an operator,realization of an operator,,"In [s. Cerrai, Normal deviations from the averaged motion for some reaction–diffusion equations with fast oscillating perturbation] it is given a reaction diffusion PDE with operator $\mathcal{A}$ and some Dirichlet Boundary conditions. Then  it's written as a differential equation in abstract form on some Hilbert space $H$ with operator $A$ which is the realization of $\mathcal{A}$ in $H$ endowed with Dirichlet boundary conditions. What do we mean with the realization of $\mathcal{A}$ in $H$ ?","In [s. Cerrai, Normal deviations from the averaged motion for some reaction–diffusion equations with fast oscillating perturbation] it is given a reaction diffusion PDE with operator and some Dirichlet Boundary conditions. Then  it's written as a differential equation in abstract form on some Hilbert space with operator which is the realization of in endowed with Dirichlet boundary conditions. What do we mean with the realization of in ?",\mathcal{A} H A \mathcal{A} H \mathcal{A} H,"['functional-analysis', 'partial-differential-equations']"
57,Introduction to Hardy spaces,Introduction to Hardy spaces,,"What text would you recommend for a (not too long) introduction to Hardy spaces? I am specially interested in real Hardy spaces but I also want to learn about complex ones. I'm a graduate student with some background in harmonic, complex and functional analysis.","What text would you recommend for a (not too long) introduction to Hardy spaces? I am specially interested in real Hardy spaces but I also want to learn about complex ones. I'm a graduate student with some background in harmonic, complex and functional analysis.",,"['real-analysis', 'functional-analysis', 'reference-request', 'harmonic-analysis']"
58,"Almost first-order, almost-differential functional equations","Almost first-order, almost-differential functional equations",,"The ODE $y'(x)+P(x)y(x)=Q(x)$ has solution $$I(x)y(x)=\int I(x)Q(x)\,dx$$ where $I(x)=\exp\int P(x)\,dx$ . Equivalently, $$Y(x)+P(x)\int_0^xY(t)\,dt=Q(x)\tag1$$ has solution $$Y(x)=\frac d{dx}\frac{\int I(x)Q^*(x)\,dx}{I(x)}=\frac{I(x)^2Q^*(x)-I'(x)\int I(x)Q^*(x)\,dx}{I(x)^2}$$ where $Y=y'$ and $Q^*(x)=Q(x)+P(x)y(0)$ . Equation $(1)$ gives the limiting case, where $$\int_0^xY(t)\,dt=\lim_{n\to\infty}\frac xn\sum_{k=0}^nY\left(\frac{kx}n\right).$$ Given $P(x),Q(x)$ , what could be said about the solutions of the functional equation $$Y(nx)+\frac{xP(x)}n\sum_{k=0}^nY(kx)=Q(x)\tag2,$$ where $n$ is no longer under the limit? That is, what is the behaviour of the families of solutions to $(2)$ as $n$ increases? (Cross-posted on MathOverflow .)","The ODE has solution where . Equivalently, has solution where and . Equation gives the limiting case, where Given , what could be said about the solutions of the functional equation where is no longer under the limit? That is, what is the behaviour of the families of solutions to as increases? (Cross-posted on MathOverflow .)","y'(x)+P(x)y(x)=Q(x) I(x)y(x)=\int I(x)Q(x)\,dx I(x)=\exp\int P(x)\,dx Y(x)+P(x)\int_0^xY(t)\,dt=Q(x)\tag1 Y(x)=\frac d{dx}\frac{\int I(x)Q^*(x)\,dx}{I(x)}=\frac{I(x)^2Q^*(x)-I'(x)\int I(x)Q^*(x)\,dx}{I(x)^2} Y=y' Q^*(x)=Q(x)+P(x)y(0) (1) \int_0^xY(t)\,dt=\lim_{n\to\infty}\frac xn\sum_{k=0}^nY\left(\frac{kx}n\right). P(x),Q(x) Y(nx)+\frac{xP(x)}n\sum_{k=0}^nY(kx)=Q(x)\tag2, n (2) n","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'riemann-sum']"
59,Hardy's inequality with power weights,Hardy's inequality with power weights,,"we have that $$\hspace{20mm}\int_{0}^{\infty} \left( \frac{1}{x}\int_{0}^{x} f(t) dx \right)^{p} x^{\alpha} dx \leq \left( \frac{p}{p-\alpha-1} \right)^{p} \int_{0}^{\infty} f(x)^{p}x^{\alpha} dx     \hspace{20mm}(1)$$ for $p \geq 1$ and $\alpha<p-1$ $$\hspace{20mm}\int_{0}^{\infty} \left( \frac{1}{x}\int_{x}^{\infty} f(t) dt \right)^{p} x^{\alpha} dx \leq \left( \frac{p}{\alpha+1-p} \right)^{p} \int_{0}^{\infty} f(x)^{p}x^{\alpha} dx     \hspace{20mm}(2)$$ for $p \geq 1$ and $\alpha>p-1$ Conditions for $p \geq 1$ and $\alpha<p-1$ are essential for (1). Indeed, if I consider the functions $f_{a}(x) = \mathcal{X}_{(a,a+1)}(x)$ with $a>0$ then for $x \geq a+1$ we have $\mathcal{H}f_{a}(x) = \frac{1}{x} \int_{0}^{x} f_{a}(t)dt = \frac{1}{x}$ . let's see the two cases we have, Case $p\geq 1$ y $\alpha \geq p-1$ . $$\int_{0}^{\infty} \left[ \mathcal{H}f_{a}(x) \right]^{p}x^{\alpha} dx = \int_{0}^{\infty}  \frac{1}{x^{p}} x^{\alpha} dx  \geq \int_{a+1}^{\infty} x^{\alpha-p} dx = \left[ \frac{x^{\alpha-p+1}}{\alpha-p+1}\right]_{a+1}^{\infty} = \infty$$ Case $0<p<1$ y $\alpha<p-1$ . We have a) $$\int_{0}^{\infty} \left[ \mathcal{H}f_{a}(x) \right]^{p}x^{\alpha} dx  \geq \int_{a+1}^{\infty} x^{\alpha-p} dx = \left[ \frac{x^{\alpha-p+1}}{\alpha-p+1}\right]_{a+1}^{\infty} = \frac{(a+1)^{\alpha-p+1}}{p-\alpha-1}$$ b) $$\int_{0}^{\infty} f_{a}(x)^{p}x^{\alpha} dx = \int_{0}^{\infty} \left(  \mathcal{X}_{(a,a+1)}(x)\right)^{p}x^{\alpha} dx = \int_{a}^{a+1} x^{\alpha}dx \leq a^{\alpha}$$ so $$\frac{\int_{0}^{\infty} \left[ \mathcal{H}f_{a}(x) \right]^{p}x^{\alpha} dx }{\int_{0}^{\infty} f_{a}(x)^{p}x^{\alpha} dx } \geq \frac{\frac{(a+1)^{\alpha-p+1}}{p-\alpha-1}}{a^{\alpha}} = \frac{1}{p-\alpha-1} \frac{(a+1)^{\alpha-p+1}}{a^{\alpha}} \rightarrow \infty$$ when $a \rightarrow \infty$ Therefore it would be proven. However, for dual inequality (2) I would not know how to do it. Conditions for $p \geq 1$ and $\alpha>p-1$ are essential for (2), if I take the same function I get that for $x<a$ we have $$\frac{1}{x}\int_{x}^{\infty} f_{a}(t) dt = \frac{1}{x}  $$ let's see the thow cases we have, Case $0<p<1$ and $\alpha \geq p-1$ $$\int_{0}^{\infty} \left( \frac{1}{x} \int_{0}^{\infty} f_{a}(t)dx \right)^{p}x^{\alpha}dx \geq \int_{0}^{a} x^{\alpha-p}dx = \frac{a^{\alpha-p+1}}{\alpha-p+1} \rightarrow \infty $$ when $a \rightarrow \infty$ Case $p\geq$ and $\alpha \leq p-1$ . I don't know how I could do this step Thanks you so much.","we have that for and for and Conditions for and are essential for (1). Indeed, if I consider the functions with then for we have . let's see the two cases we have, Case y . Case y . We have a) b) so when Therefore it would be proven. However, for dual inequality (2) I would not know how to do it. Conditions for and are essential for (2), if I take the same function I get that for we have let's see the thow cases we have, Case and when Case and . I don't know how I could do this step Thanks you so much.","\hspace{20mm}\int_{0}^{\infty} \left( \frac{1}{x}\int_{0}^{x} f(t) dx \right)^{p} x^{\alpha} dx \leq \left( \frac{p}{p-\alpha-1} \right)^{p} \int_{0}^{\infty} f(x)^{p}x^{\alpha} dx     \hspace{20mm}(1) p \geq 1 \alpha<p-1 \hspace{20mm}\int_{0}^{\infty} \left( \frac{1}{x}\int_{x}^{\infty} f(t) dt \right)^{p} x^{\alpha} dx \leq \left( \frac{p}{\alpha+1-p} \right)^{p} \int_{0}^{\infty} f(x)^{p}x^{\alpha} dx     \hspace{20mm}(2) p \geq 1 \alpha>p-1 p \geq 1 \alpha<p-1 f_{a}(x) = \mathcal{X}_{(a,a+1)}(x) a>0 x \geq a+1 \mathcal{H}f_{a}(x) = \frac{1}{x} \int_{0}^{x} f_{a}(t)dt = \frac{1}{x} p\geq 1 \alpha \geq p-1 \int_{0}^{\infty} \left[ \mathcal{H}f_{a}(x) \right]^{p}x^{\alpha} dx = \int_{0}^{\infty}  \frac{1}{x^{p}} x^{\alpha} dx  \geq \int_{a+1}^{\infty} x^{\alpha-p} dx = \left[ \frac{x^{\alpha-p+1}}{\alpha-p+1}\right]_{a+1}^{\infty} = \infty 0<p<1 \alpha<p-1 \int_{0}^{\infty} \left[ \mathcal{H}f_{a}(x) \right]^{p}x^{\alpha} dx  \geq \int_{a+1}^{\infty} x^{\alpha-p} dx = \left[ \frac{x^{\alpha-p+1}}{\alpha-p+1}\right]_{a+1}^{\infty} = \frac{(a+1)^{\alpha-p+1}}{p-\alpha-1} \int_{0}^{\infty} f_{a}(x)^{p}x^{\alpha} dx = \int_{0}^{\infty} \left(  \mathcal{X}_{(a,a+1)}(x)\right)^{p}x^{\alpha} dx = \int_{a}^{a+1} x^{\alpha}dx \leq a^{\alpha} \frac{\int_{0}^{\infty} \left[ \mathcal{H}f_{a}(x) \right]^{p}x^{\alpha} dx }{\int_{0}^{\infty} f_{a}(x)^{p}x^{\alpha} dx } \geq \frac{\frac{(a+1)^{\alpha-p+1}}{p-\alpha-1}}{a^{\alpha}} = \frac{1}{p-\alpha-1} \frac{(a+1)^{\alpha-p+1}}{a^{\alpha}} \rightarrow \infty a \rightarrow \infty p \geq 1 \alpha>p-1 x<a \frac{1}{x}\int_{x}^{\infty} f_{a}(t) dt = \frac{1}{x}   0<p<1 \alpha \geq p-1 \int_{0}^{\infty} \left( \frac{1}{x} \int_{0}^{\infty} f_{a}(t)dx \right)^{p}x^{\alpha}dx \geq \int_{0}^{a} x^{\alpha-p}dx = \frac{a^{\alpha-p+1}}{\alpha-p+1} \rightarrow \infty  a \rightarrow \infty p\geq \alpha \leq p-1","['integration', 'functional-analysis', 'analysis', 'inequality']"
60,Computation with Gaussian measures on Hilbert spaces,Computation with Gaussian measures on Hilbert spaces,,"What follows is based on the paper https://arxiv.org/abs/1509.02093v2 , specifically section 1.2 about Gibbs measures. The setup is the following: Let $\mu$ be the Gaussian measure on $H^s(\mathbb{T}^2)$ ( $s<0$ , and $H^s$ are Sobolev spaces defined in the usual sense of Fourier multipliers) defined by $$ d\mu = \exp\left(-\frac{1}{2} \int |u|^2 + |\nabla u|^2 dx \right) du $$ In the paper, Oh and Thomann show that you can compute the covariance operator associated to this measure explicitly ( $C = (1 - \Delta)^{-1+s}$ ), but they then claim that $\mu$ is only a probability measure on $H^s(\mathbb{T}^2)$ when $s < 0$ , as "" $\mu(L^2(\mathbb{T}^2)) = 0$ "". How does one go about showing this equality, and why does this show failure of $\mu$ to be a probability measure?","What follows is based on the paper https://arxiv.org/abs/1509.02093v2 , specifically section 1.2 about Gibbs measures. The setup is the following: Let be the Gaussian measure on ( , and are Sobolev spaces defined in the usual sense of Fourier multipliers) defined by In the paper, Oh and Thomann show that you can compute the covariance operator associated to this measure explicitly ( ), but they then claim that is only a probability measure on when , as "" "". How does one go about showing this equality, and why does this show failure of to be a probability measure?","\mu H^s(\mathbb{T}^2) s<0 H^s 
d\mu = \exp\left(-\frac{1}{2} \int |u|^2 + |\nabla u|^2 dx \right) du  C = (1 - \Delta)^{-1+s} \mu H^s(\mathbb{T}^2) s < 0 \mu(L^2(\mathbb{T}^2)) = 0 \mu","['functional-analysis', 'measure-theory', 'hilbert-spaces', 'lebesgue-measure']"
61,An infinite product function,An infinite product function,,"Before I start, must be said that I am not a math wizard, nor a math student. I just love nudging around with math, and I came across a random function I thought in my head (I do not take credit for nothing, what I mean by 'I though'=what came to my mind) So sorry if the maths you see here are corrupt, and sorry if I post about something that was discovered long time ago. The function is $$ P(k) = \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^k} \right )$$ For any $k \in (0, \infty)$ . I tried playing around with different values of $k$ and go these results: $$\begin{array}{|c|c|}\hline  k& P(k)  \\ \hline  2 & 0.5  \\ \hline  3 & 0.80939 \\ \hline  4 & 0.91901 \\ \hline  5 & 0.96325\\ \hline  \end{array}$$ Now, of-course this is not 'that' interesting for $k \in \mathbb{Z}$ because it tends to $1$ pretty quickly, even from $k \le 9$ we are close to $1$ by a per-mille - $P(9) = 0.99799$ . And so I wanted to see what happens for values $k = [0, 10]$ using the numpy library to use linspace as so: space = np.linspace(0, 10, 100000) # This is our range Which splits the intervals $[0,10]$ into 10K values (each are equally far away from each-other) - then calculated $P(k) ~~ \forall k \in \text{space}$ and got this beautiful output: It look roughly like what I expected it to look like, with an asymptote $y=1$ as we tend to $1$ as $k$ grows. Meaning our function (not specifically the one in the picture) is defined as so: $$ P : [0, \infty) \to [0, 1) \\ P(k) = \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^k} \right ) $$ We can calculate $P(2)$ very easily as so: $$ p(2) := \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^2} \right) =\prod_{i=2}^{\infty} \left( \frac{i^2 - 1}{i^2} \right) = \prod_{i=2}^{\infty} \left( \frac{(i+1)(i-1)}{i^2} \right)  ~~ \text{\\} \lg$$ $$\begin{aligned} \lg(p(2)) &= \sum_{i=2}^{\infty} \lg \left( \frac{(i+1)(i-1)}{i^2} \right)   = \lim_{N \to \infty} \sum_{i=2}^{N} \lg \left( (i+1)(i-1) \right) - 2\lg (i) \\&= \lim_{N \to \infty} \lg \left ( \frac{(N+1)!}{2} \cdot (N-1)!  \cdot \frac{1}{(N!)^2} \right) = \lg(0.5) = \lg(P(2)) \implies P(2) = \frac{1}{2} \end{aligned}$$ As for any other $k$ , it might be impossible to solve it as we did for $P(2)$ because $\prod \left (i^3 - 1 \right)$ does not factor nicely. The Python Code (Uses numpy and matplotlib ): import numpy as np import matplotlib.pyplot as plt  space = np.linspace(0, 10, 10000) Y = np.array([0]) X = space for x in X[1:]:     a = 1     for i in range(2, 1000):         a *= (1 - 1/(i ** x))     Y = np.append(Y, [a]) plt.scatter(X, Y) plt.show() The for-loop with the range (2, 1000) tries to approximate $\prod_{i=2}^{\infty}$ (And of-course I don't have enough 'time' 😉 to wait until $\infty$ .. so I chose a big enough value that would output a good approximation). My questions are a bit ""dry"" when it comes to the mathematics behind this function. Is this kind of function known somewhere? Used in any way? The function (for me) looks like it has a logarithmic style, is it related in any way? any approximations can be done to this function? (As I said, I am not a math-wiz unfortunately..) Thank you!","Before I start, must be said that I am not a math wizard, nor a math student. I just love nudging around with math, and I came across a random function I thought in my head (I do not take credit for nothing, what I mean by 'I though'=what came to my mind) So sorry if the maths you see here are corrupt, and sorry if I post about something that was discovered long time ago. The function is For any . I tried playing around with different values of and go these results: Now, of-course this is not 'that' interesting for because it tends to pretty quickly, even from we are close to by a per-mille - . And so I wanted to see what happens for values using the numpy library to use linspace as so: space = np.linspace(0, 10, 100000) # This is our range Which splits the intervals into 10K values (each are equally far away from each-other) - then calculated and got this beautiful output: It look roughly like what I expected it to look like, with an asymptote as we tend to as grows. Meaning our function (not specifically the one in the picture) is defined as so: We can calculate very easily as so: As for any other , it might be impossible to solve it as we did for because does not factor nicely. The Python Code (Uses numpy and matplotlib ): import numpy as np import matplotlib.pyplot as plt  space = np.linspace(0, 10, 10000) Y = np.array([0]) X = space for x in X[1:]:     a = 1     for i in range(2, 1000):         a *= (1 - 1/(i ** x))     Y = np.append(Y, [a]) plt.scatter(X, Y) plt.show() The for-loop with the range (2, 1000) tries to approximate (And of-course I don't have enough 'time' 😉 to wait until .. so I chose a big enough value that would output a good approximation). My questions are a bit ""dry"" when it comes to the mathematics behind this function. Is this kind of function known somewhere? Used in any way? The function (for me) looks like it has a logarithmic style, is it related in any way? any approximations can be done to this function? (As I said, I am not a math-wiz unfortunately..) Thank you!"," P(k) = \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^k} \right ) k \in (0, \infty) k \begin{array}{|c|c|}\hline 
k& P(k)  \\ \hline 
2 & 0.5  \\ \hline 
3 & 0.80939 \\ \hline 
4 & 0.91901 \\ \hline 
5 & 0.96325\\ \hline 
\end{array} k \in \mathbb{Z} 1 k \le 9 1 P(9) = 0.99799 k = [0, 10] [0,10] P(k) ~~ \forall k \in \text{space} y=1 1 k  P : [0, \infty) \to [0, 1) \\ P(k) = \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^k} \right )  P(2)  p(2) := \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^2} \right) =\prod_{i=2}^{\infty} \left( \frac{i^2 - 1}{i^2} \right) = \prod_{i=2}^{\infty} \left( \frac{(i+1)(i-1)}{i^2} \right)  ~~ \text{\\} \lg \begin{aligned} \lg(p(2)) &= \sum_{i=2}^{\infty} \lg \left( \frac{(i+1)(i-1)}{i^2} \right) 
 = \lim_{N \to \infty} \sum_{i=2}^{N} \lg \left( (i+1)(i-1) \right) - 2\lg (i) \\&= \lim_{N \to \infty} \lg \left ( \frac{(N+1)!}{2} \cdot (N-1)!  \cdot \frac{1}{(N!)^2} \right) = \lg(0.5) = \lg(P(2)) \implies P(2) = \frac{1}{2} \end{aligned} k P(2) \prod \left (i^3 - 1 \right) \prod_{i=2}^{\infty} \infty","['real-analysis', 'functional-analysis', 'products']"
62,Are they known properties for the self-convolution application $H : f \mapsto f \star f$?,Are they known properties for the self-convolution application ?,H : f \mapsto f \star f,"I was wondering if the application $H : L^1(\mathbb{R}) \rightarrow L^1(\mathbb{R})$ defined by : $$H(f) := f \star f, \quad \forall f \in L^1(\mathbb{R})$$ had some known properties ? For example we know that , using Young inequality, that $||H(f)||_{L^1(\mathbb{R})} \leq ||f||^2_{L^1(\mathbb{R})}$ , but that's very common. Can we caracterize $H(L^1(\mathbb{R}))$ or $H^{-1}(\{0 \})$ ? Is the application $H$ surjective ? Are they counter-example to the surjectivity ? I'm looking for any properties or references since I'm just curious about it. I guess using Fourier transform will probably be a very useful tool. Apart from these first questions, I'm also wondering if there exists properties for $G : L^p(\mathbb{R}) \rightarrow L^r(\mathbb{R})$ defined by : $$G(f) = g \star f\quad \forall f \in L^p(\mathbb{R})$$ for a fixed $g$ in $L^p(\mathbb{R})$ with $1 \leq p,q,r \leq + \infty$ verifying the young equality : $$\frac{1}{p} + \frac{1}{q} = 1 + \frac{1}{r}.$$","I was wondering if the application defined by : had some known properties ? For example we know that , using Young inequality, that , but that's very common. Can we caracterize or ? Is the application surjective ? Are they counter-example to the surjectivity ? I'm looking for any properties or references since I'm just curious about it. I guess using Fourier transform will probably be a very useful tool. Apart from these first questions, I'm also wondering if there exists properties for defined by : for a fixed in with verifying the young equality :","H : L^1(\mathbb{R}) \rightarrow L^1(\mathbb{R}) H(f) := f \star f, \quad \forall f \in L^1(\mathbb{R}) ||H(f)||_{L^1(\mathbb{R})} \leq ||f||^2_{L^1(\mathbb{R})} H(L^1(\mathbb{R})) H^{-1}(\{0 \}) H G : L^p(\mathbb{R}) \rightarrow L^r(\mathbb{R}) G(f) = g \star f\quad \forall f \in L^p(\mathbb{R}) g L^p(\mathbb{R}) 1 \leq p,q,r \leq + \infty \frac{1}{p} + \frac{1}{q} = 1 + \frac{1}{r}.","['functional-analysis', 'reference-request', 'lp-spaces', 'convolution']"
63,Is the space of smooth functions on a noncompact manifold nuclear?,Is the space of smooth functions on a noncompact manifold nuclear?,,"I have two questions, really: Q1) Suppose $M$ is a noncompact, finite-dimensional smooth manifold. Is it true that $C^{\infty}(M)$ , by which I will always mean $C^{\infty}(-; \mathbb{R})$ , is a nuclear Frechet space? Reason for asking: if you quickly google for ``examples of nuclear spaces'', it's usually cited that $C^{\infty}(M)$ is nuclear if $M$ is compact. But compactness seems like rather serious overkill; indeed, Treves, Theorem 51.5 + subsequent Corollaries, suggests $C^{\infty}(U)$ is nuclear for $U \subset \mathbb{R}^n$ open. In this case, I would suspect that if, say, $M$ may be covered by finitely many charts (as is certainly the case if $M$ is compact but is true far more generally), then $C^{\infty}(M)$ is nuclear by embedding $C^{\infty}(M) \hookrightarrow \prod C^{\infty}(U_{\alpha})$ as a closed subspace for $\{U_{\alpha}\}$ a finite open cover of $M$ by charts. This same embedding trick seems to suggest that $C^{\infty}(M)$ is nuclear far more generally still -- in which case, what are the natural conditions on $M$ such that this holds? (e.g., paracompactness, second countability, ...?) Q2) And then secretly my real reason for asking is to know conditions on manifolds $M, N$ such that $C^{\infty}(M \times N) \simeq C^{\infty}(M) \otimes C^{\infty}(N)$ for some reasonable tensor product. Indeed, if $\otimes$ is taken to be the projective tensor product $\hat{\otimes}_{\pi}$ , this statement seems to be commonly asserted - and I think I understand this in the case that at least one space is nuclear so that, e.g., projective and injective tensor products agree. But, right, does this hold for reasonably general noncompact manifolds $M, N$ ? How about if $M, N$ are in some reasonable class of infinite-dimensional manifolds?","I have two questions, really: Q1) Suppose is a noncompact, finite-dimensional smooth manifold. Is it true that , by which I will always mean , is a nuclear Frechet space? Reason for asking: if you quickly google for ``examples of nuclear spaces'', it's usually cited that is nuclear if is compact. But compactness seems like rather serious overkill; indeed, Treves, Theorem 51.5 + subsequent Corollaries, suggests is nuclear for open. In this case, I would suspect that if, say, may be covered by finitely many charts (as is certainly the case if is compact but is true far more generally), then is nuclear by embedding as a closed subspace for a finite open cover of by charts. This same embedding trick seems to suggest that is nuclear far more generally still -- in which case, what are the natural conditions on such that this holds? (e.g., paracompactness, second countability, ...?) Q2) And then secretly my real reason for asking is to know conditions on manifolds such that for some reasonable tensor product. Indeed, if is taken to be the projective tensor product , this statement seems to be commonly asserted - and I think I understand this in the case that at least one space is nuclear so that, e.g., projective and injective tensor products agree. But, right, does this hold for reasonably general noncompact manifolds ? How about if are in some reasonable class of infinite-dimensional manifolds?","M C^{\infty}(M) C^{\infty}(-; \mathbb{R}) C^{\infty}(M) M C^{\infty}(U) U \subset \mathbb{R}^n M M C^{\infty}(M) C^{\infty}(M) \hookrightarrow \prod C^{\infty}(U_{\alpha}) \{U_{\alpha}\} M C^{\infty}(M) M M, N C^{\infty}(M \times N) \simeq C^{\infty}(M) \otimes C^{\infty}(N) \otimes \hat{\otimes}_{\pi} M, N M, N",['functional-analysis']
64,Eigenfunction expansion of Gaussian kernel over a closed interval,Eigenfunction expansion of Gaussian kernel over a closed interval,,"$\newcommand{\Xc}{\mathcal{X}}$ Let $\Xc=[-1,1]$ and consider a Gaussian kernel $k(x,t)\propto \exp(-(x-t)^2/2\sigma^2)$ for some $\sigma>0$ on $\Xc$ . I am looking for an eigenfunction expansion of the induced integral operator $\mathbf{K}\colon L^2(\Xc)\to L^2(\Xc)$ . That is, I wish to find a countable set of functions $\{\phi_n\colon \Xc\to \mathbb{R}\}_{n=1}^\infty$ such that $$ k(x,t) = \sum_{n=1}^{\infty} \lambda_n \phi_n(x)\phi_n(t) $$ and $\int_{\Xc}\phi_m(x)\phi_n(x)d x=\delta_{mn}$ . Here are some related results I am aware of: Hermite polynomials characterize the $L^2(\rho)$ -orthonormal eigenfunctions of Gaussian kernels over $\mathbb{R}$ for a Gaussian weight $\rho$ [See Sec. 6.2] . Spherical harmonics characterize the $L^2(\Xc)$ -orthonormal eigenfunctions of Gaussian kernels over the hypersphere $\Xc=S^{d-1}$ (for a uniform weight) [See Thm. 2.] .","Let and consider a Gaussian kernel for some on . I am looking for an eigenfunction expansion of the induced integral operator . That is, I wish to find a countable set of functions such that and . Here are some related results I am aware of: Hermite polynomials characterize the -orthonormal eigenfunctions of Gaussian kernels over for a Gaussian weight [See Sec. 6.2] . Spherical harmonics characterize the -orthonormal eigenfunctions of Gaussian kernels over the hypersphere (for a uniform weight) [See Thm. 2.] .","\newcommand{\Xc}{\mathcal{X}} \Xc=[-1,1] k(x,t)\propto \exp(-(x-t)^2/2\sigma^2) \sigma>0 \Xc \mathbf{K}\colon L^2(\Xc)\to L^2(\Xc) \{\phi_n\colon \Xc\to \mathbb{R}\}_{n=1}^\infty 
k(x,t) = \sum_{n=1}^{\infty} \lambda_n \phi_n(x)\phi_n(t)
 \int_{\Xc}\phi_m(x)\phi_n(x)d x=\delta_{mn} L^2(\rho) \mathbb{R} \rho L^2(\Xc) \Xc=S^{d-1}","['functional-analysis', 'eigenfunctions', 'gaussian', 'reproducing-kernel-hilbert-spaces']"
65,How to build taylor series for infinite dimensional objects?,How to build taylor series for infinite dimensional objects?,,"Given an operator $O(f(z)): \left(\mathbb{C} \rightarrow \mathbb{C}\right) \rightarrow \left(\mathbb{C} \rightarrow \mathbb{C}\right) $ We can define its functional derivative up to a perturbation $\lambda$ as $$ \delta_\lambda O = \lim_{\epsilon \rightarrow 0} \frac{O(f+\epsilon \lambda) - O(f)}{\epsilon} $$ This can be (with a slight abuse of notation expressed as) $$ \delta_\lambda O = \frac{\partial O}{\partial f} \lambda + \frac{\partial O}{\partial f'}\lambda ' + ... = \sum_{n=0}^{\infty} \frac{\partial O}{\partial f^{(n)}}\lambda^{(n)} $$ We will adjust this notation to the leibniz style (for reasons that will be obvious in the next next step): $$ \delta_\lambda O = \frac{\partial O}{\partial f} \lambda + \frac{\partial O}{\partial \frac{df}{dz}}\lambda ' + ... = \sum_{n=0}^{\infty} \frac{\partial O}{\partial \frac{d^nf}{dz^n}} \frac{d^n \lambda }{dz^n} $$ You then get some nice identities (I call them the taylor extractants): $$ \frac{\partial O}{\partial f} = \delta_{1}O \\ \frac{\partial O}{\partial f'} = \delta_xO-x\delta_1 O \\ \frac{\partial O}{\partial f''} =  \delta_{\frac{1}{2}x^2}O-x\delta_{x}O+\frac{1}{2}x^2\delta_1O$$ etc... Now it would be nice to assume that all such operators $O$ can be written as an infinite sum of terms of the form $$a(z) f^{n_1}\left(\frac{df}{dz}\right)^{n_2}\left(\frac{d^2f}{dz^2}\right)^{n_2}...  = a(z) \prod_{k=0}^{\infty}\left[ \left( \frac{d^k f}{dz^k} \right)^{n_k}\right]$$ Much like how all holomorphic functions can be written as an infinite sum of terms of the form $$ a_n z^n$$ We then get a definition of an ""infinite dimensional taylor series"" as a series of the form $$O(f) =  \sum_{g \in \left( \mathbb{N}_0 \rightarrow \mathbb{N}_0 \right)} \left[ \mathfrak{a}(g) \prod_{k=0}^{\infty}\left[ \left( \frac{d^k f}{dz^k}\right)^{g(k)} \right] \right]$$ Where $\mathfrak{a}: (\mathbb{N}_0 \rightarrow \mathbb{N}_0) \rightarrow (\mathbb{C} \rightarrow \mathbb{C})$ assigns complex function coefficients to our terms, which are products of the derivatives of different orders of $f$ . This is a direct generalization of the usual expression of a taylor series as $$ f(z) = \sum_{k \in \mathbb{N}_0} a_k z^k $$ With this set up, I was hoping then to try to decompose the operator $O(f) = f(f)$ (ex: it sends $x^2$ to $x^4$ and $e^z$ to $e^{e^z}$ ) into this taylor series framework. But I unfortunately ran into some problems. It seems that besides the expressions $\frac{\partial^n O}{ \partial f^n}$ all other ""functional-partial-derivatives"" including the mixed derivative terms equal $0$ . Which means that my infinite dimensional taylor series for $f(f)$ CANNOT be made to converge except on a single function. I.E. it offers no utility in extrapolating the value of $f(f)$ to new functions. Now this isn't super shocking to me, because sometimes taylor series have radius of convergence zero, see for example $e^{-\frac{1}{x^2}}$ . So I'm guessing that $f(f)$ just happens to have $0$ -radius of convergence infinite dimensional space but i'm not ENTIRELY sure if that might mean I forgot to look at it from the correct angle (much like how $e^{-\frac{1}{x^2}}$ has a useful laurent series, could it be that f(f) has a useful infinite-dimensional-laurent series/some other generalization thereof?) Has anyone else worked with/seen similar infinite-dimensional taylor series, and am I potentially missing important terms here? Visualization tool: Visualizing the infinite dimensional sum is not easy, as you are attempting to visualize a commutative free group with countably infinitely many generators. One approach for ""seeing"" the picture is via the partition sum representation $$ O =  c_0(z) + a_0(z)f  \begin{matrix} a_1(z)f' \\ + \\  a_2(z)f^2 \end{matrix}   \begin{matrix} a_3(z) f'' \\ + \\  a_4(z) ff' \\ + \\ a_5(z)f^3 \end{matrix}  + \begin{matrix} a_6(z) f''' \\ + \\  a_7(z) f''f \\ + \\ a_8(z)f'f^2 \\ + \\ a_9(z)(f')^2 \\ + \\ a_{10}(z)f^4 \end{matrix}  + ... $$ Where the height of each column is equal to the number of partitions (in the combinatorial/number theoretic sense) of the index of the column. Update: It turns out that I made a mistake. Seeing the ramifications now. FWIW I was able to successfully derive $f(x+1) = f + f' + \frac{1}{2}f'' + \frac{1}{6}f''' + ... $ using this framework So far: Unforunately still stuck Observe that for the case of $O(f) = f(f)$ we have $$ \delta_\lambda O = \lim_{\epsilon \rightarrow 0} \frac{f(f+\epsilon \lambda)+\epsilon\lambda(f+\epsilon \lambda)-f(f)}{\epsilon} = f'(f)\lambda+\lambda(f)$$ $$ \frac{\partial O}{\partial f} = \delta_1 O =  f'(f)+1$$ $$ \frac{\partial O}{\partial f'} = \delta_x O - x\delta_1O = xf'(f)+f - x(f'(f)+1) = f-x$$ $$ \frac{\partial O}{\partial f''} = \delta_{\frac{1}{2}x^2} O - x\delta_xO + \frac{1}{2}x^2\delta_1O= \frac{1}{2}x^2f'(f)+\frac{1}{2}f^2- x^2 f(f)-xf+\frac{1}{2}x^2f'(f)+\frac{1}{2} = \frac{1}{2}(f-x)^2$$ Generally then we can extract the rest of the terms this way so we find that $$ f(f) =_{expected} (f'(f)+1)f + (f-x)f'(x) + \frac{1}{2}(f-x)^2f''(x) + ...$$ But in reality we all know $$ f(f) = f(x) + f'(x)(f-x)+ \frac{1}{2}f''(x)(f-x)^2 + ... $$ So something here is quite off.","Given an operator We can define its functional derivative up to a perturbation as This can be (with a slight abuse of notation expressed as) We will adjust this notation to the leibniz style (for reasons that will be obvious in the next next step): You then get some nice identities (I call them the taylor extractants): etc... Now it would be nice to assume that all such operators can be written as an infinite sum of terms of the form Much like how all holomorphic functions can be written as an infinite sum of terms of the form We then get a definition of an ""infinite dimensional taylor series"" as a series of the form Where assigns complex function coefficients to our terms, which are products of the derivatives of different orders of . This is a direct generalization of the usual expression of a taylor series as With this set up, I was hoping then to try to decompose the operator (ex: it sends to and to ) into this taylor series framework. But I unfortunately ran into some problems. It seems that besides the expressions all other ""functional-partial-derivatives"" including the mixed derivative terms equal . Which means that my infinite dimensional taylor series for CANNOT be made to converge except on a single function. I.E. it offers no utility in extrapolating the value of to new functions. Now this isn't super shocking to me, because sometimes taylor series have radius of convergence zero, see for example . So I'm guessing that just happens to have -radius of convergence infinite dimensional space but i'm not ENTIRELY sure if that might mean I forgot to look at it from the correct angle (much like how has a useful laurent series, could it be that f(f) has a useful infinite-dimensional-laurent series/some other generalization thereof?) Has anyone else worked with/seen similar infinite-dimensional taylor series, and am I potentially missing important terms here? Visualization tool: Visualizing the infinite dimensional sum is not easy, as you are attempting to visualize a commutative free group with countably infinitely many generators. One approach for ""seeing"" the picture is via the partition sum representation Where the height of each column is equal to the number of partitions (in the combinatorial/number theoretic sense) of the index of the column. Update: It turns out that I made a mistake. Seeing the ramifications now. FWIW I was able to successfully derive using this framework So far: Unforunately still stuck Observe that for the case of we have Generally then we can extract the rest of the terms this way so we find that But in reality we all know So something here is quite off.",O(f(z)): \left(\mathbb{C} \rightarrow \mathbb{C}\right) \rightarrow \left(\mathbb{C} \rightarrow \mathbb{C}\right)  \lambda  \delta_\lambda O = \lim_{\epsilon \rightarrow 0} \frac{O(f+\epsilon \lambda) - O(f)}{\epsilon}   \delta_\lambda O = \frac{\partial O}{\partial f} \lambda + \frac{\partial O}{\partial f'}\lambda ' + ... = \sum_{n=0}^{\infty} \frac{\partial O}{\partial f^{(n)}}\lambda^{(n)}   \delta_\lambda O = \frac{\partial O}{\partial f} \lambda + \frac{\partial O}{\partial \frac{df}{dz}}\lambda ' + ... = \sum_{n=0}^{\infty} \frac{\partial O}{\partial \frac{d^nf}{dz^n}} \frac{d^n \lambda }{dz^n}   \frac{\partial O}{\partial f} = \delta_{1}O \\ \frac{\partial O}{\partial f'} = \delta_xO-x\delta_1 O \\ \frac{\partial O}{\partial f''} =  \delta_{\frac{1}{2}x^2}O-x\delta_{x}O+\frac{1}{2}x^2\delta_1O O a(z) f^{n_1}\left(\frac{df}{dz}\right)^{n_2}\left(\frac{d^2f}{dz^2}\right)^{n_2}...  = a(z) \prod_{k=0}^{\infty}\left[ \left( \frac{d^k f}{dz^k} \right)^{n_k}\right]  a_n z^n O(f) =  \sum_{g \in \left( \mathbb{N}_0 \rightarrow \mathbb{N}_0 \right)} \left[ \mathfrak{a}(g) \prod_{k=0}^{\infty}\left[ \left( \frac{d^k f}{dz^k}\right)^{g(k)} \right] \right] \mathfrak{a}: (\mathbb{N}_0 \rightarrow \mathbb{N}_0) \rightarrow (\mathbb{C} \rightarrow \mathbb{C}) f  f(z) = \sum_{k \in \mathbb{N}_0} a_k z^k  O(f) = f(f) x^2 x^4 e^z e^{e^z} \frac{\partial^n O}{ \partial f^n} 0 f(f) f(f) e^{-\frac{1}{x^2}} f(f) 0 e^{-\frac{1}{x^2}}  O =  c_0(z) + a_0(z)f  \begin{matrix} a_1(z)f' \\ + \\  a_2(z)f^2 \end{matrix}   \begin{matrix} a_3(z) f'' \\ + \\  a_4(z) ff' \\ + \\ a_5(z)f^3 \end{matrix}  + \begin{matrix} a_6(z) f''' \\ + \\  a_7(z) f''f \\ + \\ a_8(z)f'f^2 \\ + \\ a_9(z)(f')^2 \\ + \\ a_{10}(z)f^4 \end{matrix}  + ...  f(x+1) = f + f' + \frac{1}{2}f'' + \frac{1}{6}f''' + ...  O(f) = f(f)  \delta_\lambda O = \lim_{\epsilon \rightarrow 0} \frac{f(f+\epsilon \lambda)+\epsilon\lambda(f+\epsilon \lambda)-f(f)}{\epsilon} = f'(f)\lambda+\lambda(f)  \frac{\partial O}{\partial f} = \delta_1 O =  f'(f)+1  \frac{\partial O}{\partial f'} = \delta_x O - x\delta_1O = xf'(f)+f - x(f'(f)+1) = f-x  \frac{\partial O}{\partial f''} = \delta_{\frac{1}{2}x^2} O - x\delta_xO + \frac{1}{2}x^2\delta_1O= \frac{1}{2}x^2f'(f)+\frac{1}{2}f^2- x^2 f(f)-xf+\frac{1}{2}x^2f'(f)+\frac{1}{2} = \frac{1}{2}(f-x)^2  f(f) =_{expected} (f'(f)+1)f + (f-x)f'(x) + \frac{1}{2}(f-x)^2f''(x) + ...  f(f) = f(x) + f'(x)(f-x)+ \frac{1}{2}f''(x)(f-x)^2 + ... ,"['functional-analysis', 'operator-theory', 'functional-equations', 'calculus-of-variations']"
66,Does weak convergence implies convergence in measure when $\sup_{n\in\mathbb N}\|f_n\|_{L^p}<+\infty$? [closed],Does weak convergence implies convergence in measure when ? [closed],\sup_{n\in\mathbb N}\|f_n\|_{L^p}<+\infty,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $f_n,f\colon [0,1]\rightarrow \mathbb{R}$ be integrable functions satisfying $\sup\limits_{n\in\mathbb N}\|f_n\|_{L^p}<+\infty$ , where $p>1$ . If, for all measurable set $A\in \mathcal{B}[0,1]$ , $$\int\limits_{A}f_n d\mu\rightarrow\int\limits_{A}fd\mu, $$ is it true that $\{f_n\}$ converges to $f$ in measure?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let be integrable functions satisfying , where . If, for all measurable set , is it true that converges to in measure?","f_n,f\colon [0,1]\rightarrow \mathbb{R} \sup\limits_{n\in\mathbb N}\|f_n\|_{L^p}<+\infty p>1 A\in \mathcal{B}[0,1] \int\limits_{A}f_n d\mu\rightarrow\int\limits_{A}fd\mu,  \{f_n\} f","['real-analysis', 'functional-analysis', 'measure-theory', 'lebesgue-integral']"
67,Question about isometric isomorphism between normed spaces.,Question about isometric isomorphism between normed spaces.,,"Let $(\Omega, \mathcal F)$ be a measurable space. Let $\mu$ and $\nu$ be two $\sigma$ -finite measures on $(\Omega,\mathcal F).$ Define two norms $\|\cdot\|_{\mu}$ and $\|\cdot\|_{\nu}$ on $L^1(\mu)$ and $L^1(\nu)$ respectively defined by $\|f\|_{\mu} : = \int_{\Omega} |f|\ d\mu,\ f \in L^1(\mu)$ and $\|f\|_{\nu} : = \int_{\Omega} |f|\ d\nu,\ f \in L^1 (\nu).$ Is there any necessary and sufficient condition for the normed linear spaces $(L^1(\mu),\|\cdot\|_{\mu})$ and $(L^1(\nu),\|\cdot\|_{\nu})$ to be isometrically isomorphic? What I have found is as follows $:$ If $\nu \lt \lt \mu$ (or $\mu \lt \lt \nu$ ) and if the corresponding Radon-Nikodym derivative $\frac {d\nu} {d\mu} = \alpha$ (say) be such that $\alpha \gt 0$ and $\alpha \in [c,C]$ for some $0 \lt c \lt C$ a.e. $\mu$ (and hence a.e. $\nu$ ) then $L^1(\mu) = L^1 (\nu)$ and the map $f \mapsto \frac {f} {\alpha}$ gives the required isometric isomorphism from $(L^1(\mu),\|\cdot\|_{\mu})$ to $(L^1(\nu),\|\cdot\|_{\nu}).$ So the above gives a sufficient condition for the two normed linear spaces to be isometrically isomorphic. I have also found a weaker converse of the above result which is as follows $:$ If $L^1(\mu) = L^1(\nu)$ and the normed linear spaces $(L^1(\mu),\|\cdot\|_{\mu})$ and $(L^1(\nu),\|\cdot\|_{\nu})$ are isometrically isomorphic then $\nu$ is absolutely continuous with respect to $\mu$ and vice-versa and the corresponding Radon-Nikodym derivative is bounded by $[c,C]$ a.e. $\mu$ (resp. $\nu$ ) for some $0 \lt c \lt C.$ How do I proceed if $L^1(\mu) \neq L^1(\nu)\ $ ? Any help in this regard will be appreciated. Thanks for your time.",Let be a measurable space. Let and be two -finite measures on Define two norms and on and respectively defined by and Is there any necessary and sufficient condition for the normed linear spaces and to be isometrically isomorphic? What I have found is as follows If (or ) and if the corresponding Radon-Nikodym derivative (say) be such that and for some a.e. (and hence a.e. ) then and the map gives the required isometric isomorphism from to So the above gives a sufficient condition for the two normed linear spaces to be isometrically isomorphic. I have also found a weaker converse of the above result which is as follows If and the normed linear spaces and are isometrically isomorphic then is absolutely continuous with respect to and vice-versa and the corresponding Radon-Nikodym derivative is bounded by a.e. (resp. ) for some How do I proceed if ? Any help in this regard will be appreciated. Thanks for your time.,"(\Omega, \mathcal F) \mu \nu \sigma (\Omega,\mathcal F). \|\cdot\|_{\mu} \|\cdot\|_{\nu} L^1(\mu) L^1(\nu) \|f\|_{\mu} : = \int_{\Omega} |f|\ d\mu,\ f \in L^1(\mu) \|f\|_{\nu} : = \int_{\Omega} |f|\ d\nu,\ f \in L^1 (\nu). (L^1(\mu),\|\cdot\|_{\mu}) (L^1(\nu),\|\cdot\|_{\nu}) : \nu \lt \lt \mu \mu \lt \lt \nu \frac {d\nu} {d\mu} = \alpha \alpha \gt 0 \alpha \in [c,C] 0 \lt c \lt C \mu \nu L^1(\mu) = L^1 (\nu) f \mapsto \frac {f} {\alpha} (L^1(\mu),\|\cdot\|_{\mu}) (L^1(\nu),\|\cdot\|_{\nu}). : L^1(\mu) = L^1(\nu) (L^1(\mu),\|\cdot\|_{\mu}) (L^1(\nu),\|\cdot\|_{\nu}) \nu \mu [c,C] \mu \nu 0 \lt c \lt C. L^1(\mu) \neq L^1(\nu)\ ","['functional-analysis', 'measure-theory', 'normed-spaces', 'isometry', 'vector-space-isomorphism']"
68,Whenever $f=T(x)$ then $f(x) \geq 0$. Prove $T$ is bounded by closed graph theorem.,Whenever  then . Prove  is bounded by closed graph theorem.,f=T(x) f(x) \geq 0 T,Let X be a Banach space and suppose $T:X \rightarrow X^*$ is linear and has the property that whenever $f=T(x)$ then $f(x)\geq 0$ . Prove T is bounded. The book gives a hint: using closed graph theorem. I stucked at proving $|T(x_n)-T(x)|=\sup_{\|y\|\leq1}|f_{x_n}(y)-f_x(y)|\rightarrow0$ . Any suggestion is appriciated.,Let X be a Banach space and suppose is linear and has the property that whenever then . Prove T is bounded. The book gives a hint: using closed graph theorem. I stucked at proving . Any suggestion is appriciated.,T:X \rightarrow X^* f=T(x) f(x)\geq 0 |T(x_n)-T(x)|=\sup_{\|y\|\leq1}|f_{x_n}(y)-f_x(y)|\rightarrow0,"['functional-analysis', 'banach-spaces', 'closed-graph']"
69,Relationship between the spectrum of $AA^*$ and $A^*A$,Relationship between the spectrum of  and,AA^* A^*A,"I'm currently reading something on unbounded operators and their spectra and the author notes that the nonzero spectra of $AA^*$ and $A^*A$ are equal. $A$ here is unbounded and densely defined (on $L^2$ , if that helps), but not self-adjoint. The only other useful point might be that $AA^*$ and $A^*A$ differ by a constant. This for me does not seem to help however; if $\lambda$ belongs to the resolvent of $AA^*$ , i.e. $AA^* - \lambda$ is invertible, and $A^*A = AA^* + c$ where $c \neq 0$ is the constant, then $A^*A - (c + \lambda) = AA^* - \lambda$ is invertible, so $c + \lambda$ belongs to the resolvent of $A^*A$ . But I guess if this was to be a proof I would expect $\lambda \in \sigma(AA^*) \setminus \{0\}$ if and only if $\lambda \in (A^*A) \setminus \{0\}$ ? On the other hand, some finite dimensional tests with real matrices in Mathematica seem to suggest this result is true (i.e. that the eigenvalues of $MM^t$ are equal to the eigenvalues of $M^tM$ , but the eigenvectors are not the same). I can't see why however? Do I need more hypotheses for this to be true? If not can someone provide a hint or proof? (Especially if someone can explain why zero is of particular importance too...)","I'm currently reading something on unbounded operators and their spectra and the author notes that the nonzero spectra of and are equal. here is unbounded and densely defined (on , if that helps), but not self-adjoint. The only other useful point might be that and differ by a constant. This for me does not seem to help however; if belongs to the resolvent of , i.e. is invertible, and where is the constant, then is invertible, so belongs to the resolvent of . But I guess if this was to be a proof I would expect if and only if ? On the other hand, some finite dimensional tests with real matrices in Mathematica seem to suggest this result is true (i.e. that the eigenvalues of are equal to the eigenvalues of , but the eigenvectors are not the same). I can't see why however? Do I need more hypotheses for this to be true? If not can someone provide a hint or proof? (Especially if someone can explain why zero is of particular importance too...)",AA^* A^*A A L^2 AA^* A^*A \lambda AA^* AA^* - \lambda A^*A = AA^* + c c \neq 0 A^*A - (c + \lambda) = AA^* - \lambda c + \lambda A^*A \lambda \in \sigma(AA^*) \setminus \{0\} \lambda \in (A^*A) \setminus \{0\} MM^t M^tM,"['functional-analysis', 'unbounded-operators']"
70,A vectorial version of the div/curl lemma with divergence free potential.,A vectorial version of the div/curl lemma with divergence free potential.,,"Introduction Let $Q \subset \mathbb{R}^3$ a bounded connected regular domain. Lets denote $A$ and $B$ the following spaces : $A$ the space of $3 \times 3$ function matrix with coefficient valued in $L^2(Q)$ , noted $G$ , such as there exists $u \in (H^1_0(Q))^3, \ div \ u =0$ in $Q$ such as $G=\nabla u$ (i.e $G$ derives from a divergence free potential). $B$ the space of $3 \times 3$ function matrix with coefficient valued in $L^2(Q)$ , noted $F$ , that verifies $$ \int_Q F:\nabla \varphi  \ \mathrm{d} x=0, \quad \forall \varphi \in (H^1_0(Q))^3, \quad div \ \varphi =0 \text{ in } Q$$ One can verify easily that the following orthogonal decomposition for the space of $3 \times 3$ function matrix with coefficient valued in $L^2(Q)$ : $$\mathcal{M}_3(L^2(Q))= A \  \oplus^\perp B$$ with the scalar product $<F|G>=\int_Q F:G \ \mathrm{d}x$ with the symbol $"":""$ being the usual scalar product for matrices : $C:D=\sum_{1 \leq i,j \leq 3} c_{ij} d_{ij}$ . ( $A$ is indeed closed in $\mathcal{M}_3(L^2(Q))$ using Poincaré inequality...) My question Let's take $F_n$ and $G_n$ two sequences in $A$ , respectivly $B$ . We assume that - $F_n$ weakly converges toward $G_0$ in $\mathcal{M}_3(L^2(Q))$ (i.e $\int_Q G_n :u \underset{n \rightarrow +\infty}{\longrightarrow} \int_Q G_0:u$ for any $u$ in $\mathcal{M}_3(L^2(Q))$ ) - $G_n$ weakly converges toward $G_0$ in $\mathcal{M}_3(L^2(Q))$ Now, for a fixed function $\varphi \in C^\infty_0(Q))$ , I am looking at the following quantity : $$\int_Q \varphi F_n: G_n \ \mathrm{d} x$$ and I would like to show that it converges toward $$\int_Q \varphi F_0: G_0\ \mathrm{d} x .$$ This look look exactly like a div/curl formula with a twist on the hypothesis through the divergence free potential. I have tried to prove it the same way using : $$\int_Q  \varphi F_n : \nabla u_n \ \mathrm{d} x = \int_Q F_n : \nabla (u_n \varphi) \ \mathrm{d} x - \int_Q F_n : (u_n \nabla \varphi^t) \ \mathrm{d} x$$ where $G_n=\nabla u_n$ . The last term converges thanks to Rellich injection that gives me that $u_n$ converges strongly in $(L^2(Q))^3$ , and therefore I have a scalar product between a weak converging term and a strong converging term : $$\int_Q F_n : (u_n \nabla \varphi^t) \ \mathrm{d} x \underset{n \rightarrow + \infty}{\longrightarrow}  \int_Q F_0 : (u_0 \nabla \varphi^t) \ \mathrm{d} x.$$ However, for the first integral, I can't conclude anything since $div(u \varphi)$ is not $0$ , and so I can't use that $F_n$ belongs to $B$ ... Any ideas are welcomed. I was thinking of maybe using the Leray Projector $P$ that projects a function of $L^2(Q)$ on the space of divergence free $L^2(Q)$ functions., but then I need to be able to control the following quantity : $$\int_Q F_n : \nabla (P (u_n \varphi) - u_n \varphi) \ \mathrm{d} x$$","Introduction Let a bounded connected regular domain. Lets denote and the following spaces : the space of function matrix with coefficient valued in , noted , such as there exists in such as (i.e derives from a divergence free potential). the space of function matrix with coefficient valued in , noted , that verifies One can verify easily that the following orthogonal decomposition for the space of function matrix with coefficient valued in : with the scalar product with the symbol being the usual scalar product for matrices : . ( is indeed closed in using Poincaré inequality...) My question Let's take and two sequences in , respectivly . We assume that - weakly converges toward in (i.e for any in ) - weakly converges toward in Now, for a fixed function , I am looking at the following quantity : and I would like to show that it converges toward This look look exactly like a div/curl formula with a twist on the hypothesis through the divergence free potential. I have tried to prove it the same way using : where . The last term converges thanks to Rellich injection that gives me that converges strongly in , and therefore I have a scalar product between a weak converging term and a strong converging term : However, for the first integral, I can't conclude anything since is not , and so I can't use that belongs to ... Any ideas are welcomed. I was thinking of maybe using the Leray Projector that projects a function of on the space of divergence free functions., but then I need to be able to control the following quantity :","Q \subset \mathbb{R}^3 A B A 3 \times 3 L^2(Q) G u \in (H^1_0(Q))^3, \ div \ u =0 Q G=\nabla u G B 3 \times 3 L^2(Q) F  \int_Q F:\nabla \varphi  \ \mathrm{d} x=0, \quad \forall \varphi \in (H^1_0(Q))^3, \quad div \ \varphi =0 \text{ in } Q 3 \times 3 L^2(Q) \mathcal{M}_3(L^2(Q))= A \  \oplus^\perp B <F|G>=\int_Q F:G \ \mathrm{d}x "":"" C:D=\sum_{1 \leq i,j \leq 3} c_{ij} d_{ij} A \mathcal{M}_3(L^2(Q)) F_n G_n A B F_n G_0 \mathcal{M}_3(L^2(Q)) \int_Q G_n :u \underset{n \rightarrow +\infty}{\longrightarrow} \int_Q G_0:u u \mathcal{M}_3(L^2(Q)) G_n G_0 \mathcal{M}_3(L^2(Q)) \varphi \in C^\infty_0(Q)) \int_Q \varphi F_n: G_n \ \mathrm{d} x \int_Q \varphi F_0: G_0\ \mathrm{d} x . \int_Q  \varphi F_n : \nabla u_n \ \mathrm{d} x = \int_Q F_n : \nabla (u_n \varphi) \ \mathrm{d} x - \int_Q F_n : (u_n \nabla \varphi^t) \ \mathrm{d} x G_n=\nabla u_n u_n (L^2(Q))^3 \int_Q F_n : (u_n \nabla \varphi^t) \ \mathrm{d} x \underset{n \rightarrow + \infty}{\longrightarrow}  \int_Q F_0 : (u_0 \nabla \varphi^t) \ \mathrm{d} x. div(u \varphi) 0 F_n B P L^2(Q) L^2(Q) \int_Q F_n : \nabla (P (u_n \varphi) - u_n \varphi) \ \mathrm{d} x","['functional-analysis', 'partial-differential-equations', 'divergence-operator']"
71,"Approximation of $f(x, y)$ by $\sum_{n=1}^N c_n \chi_{A_n}(x) \chi_{B_n}(y)$",Approximation of  by,"f(x, y) \sum_{n=1}^N c_n \chi_{A_n}(x) \chi_{B_n}(y)","I believe the following statement is true, but I cannot prove it. I would be grateful for your advice. Let $(X, \mathcal{A}, \mu)$ and $(Y, \mathcal{B}, \nu)$ be $\sigma$ -finite measure spaces, and let $(X \times Y, \mathcal{A} \times \mathcal{B}, \mu \times \nu)$ be product measure space. Define \begin{equation} D := \left \{\sum_{n=1}^N c_n \chi_{A_n \times B_n} \mid N \in \mathbb{Z}_+ , c_n \in \mathbb{C}, A_n \in \mathcal{A} , B_n \in \mathcal{B}\right \}. \end{equation} Then, $D$ is dense in $L^p(X \times Y; \mathbb{C})$ for all $p \in [1, \infty)$ .","I believe the following statement is true, but I cannot prove it. I would be grateful for your advice. Let and be -finite measure spaces, and let be product measure space. Define Then, is dense in for all .","(X, \mathcal{A}, \mu) (Y, \mathcal{B}, \nu) \sigma (X \times Y, \mathcal{A} \times \mathcal{B}, \mu \times \nu) \begin{equation}
D := \left \{\sum_{n=1}^N c_n \chi_{A_n \times B_n} \mid N \in \mathbb{Z}_+ , c_n \in \mathbb{C}, A_n \in \mathcal{A} , B_n \in \mathcal{B}\right \}.
\end{equation} D L^p(X \times Y; \mathbb{C}) p \in [1, \infty)","['real-analysis', 'functional-analysis', 'measure-theory', 'functions', 'lp-spaces']"
72,A concave function on a compact and convex set is upper semi continuous,A concave function on a compact and convex set is upper semi continuous,,"Let $X$ be a compact and convex set and $f:X \to \mathbb{R}$ be a concave function. Therefore, $f$ is continuous on interior of $X$ . I want to know whether $f$ is upper semi continuous on $X$ . If not, under which (minimum) assumption is?","Let be a compact and convex set and be a concave function. Therefore, is continuous on interior of . I want to know whether is upper semi continuous on . If not, under which (minimum) assumption is?",X f:X \to \mathbb{R} f X f X,"['real-analysis', 'calculus', 'functional-analysis', 'convex-analysis', 'semicontinuous-functions']"
73,"If $\langle f,\varphi\rangle$ is integrable for every functional $φ$, is there an $x$ with $\langle x,\varphi\rangle=\int\langle f,φ\rangle$?","If  is integrable for every functional , is there an  with ?","\langle f,\varphi\rangle φ x \langle x,\varphi\rangle=\int\langle f,φ\rangle","Let $(\Omega,\mathcal A,\mu)$ be a measure space, $E$ be a normed space and $f:\Omega\to E$ be strongly $^1$ $\mathcal A$ -measurable with $$\langle f,\varphi\rangle\in\mathcal L^1(\mu)\;\;\;\text{for all }\varphi\in E'.$$ Are we able to show that $$\int\langle f,\varphi\rangle\:{\rm d}\mu=\langle x,\varphi\rangle\;\;\;\text{for all }\varphi\in E'\tag1$$ for some $x\in E$ ? If not, what are sufficient conditions to ensure that? If $f$ has finite range, the claim is obviously true. If not, there are $\mathcal A$ -measurable $f_n:\Omega\to\mathbb R$ with finite range for $n\in\mathbb N$ satisfying $\left\|f_n\right\|_E\le\left\|f\right\|_E$ and $\left\|f_n-f\right\|_E\xrightarrow{n\to\infty}0$ . So, there are $(x_n)_{n\in\mathbb N}\subseteq E$ with $$\int\langle f_n,\varphi\rangle\:{\rm d}\mu=\langle x_n,\varphi\rangle\;\;\;\text{for all }\varphi\in E'\text{ and }n\in\mathbb N\tag2.$$ Using Lebesgue’s dominated convergence theorem, it's now easy to see that $$\langle x_n,\varphi\rangle\xrightarrow{n\to\infty}\int\langle f,\varphi\rangle\:{\rm d}\mu\tag3\;\;\;\text{for all }\varphi\in E',$$ which at least implies that $(x_n)_{n\in\mathbb N}$ is weakly Cauchy. However, we know that a weak Cauchy sequence doesn't need to weakly converge. So, maybe the problem I'm asking here is a particular instance of the fact that if $(x_n)_{n\in\mathbb N}$ is any weakl Cauchy sequence, then $$L\varphi:=\lim_{n\to\infty}\langle x_n,\varphi\rangle\;\;\;\text{for all }\varphi\in E'$$ is a well-defined linear functional (since the field over which $E$ is a vector space is complete), but $L$ is not necessarily of the form $$L\varphi=\langle x,\varphi\rangle\tag4$$ for some $x\in E$ . Maybe all of this boils down to the properties of the canonical embedding $$\iota:E\to E''\;,\;\;\;x\mapsto\left(E'\ni\varphi\mapsto\langle x,\varphi\rangle\right).$$ In any case, I would really appreciate if someone could show under which conditions we are able to prove the desired result; but I'm also interested in results related to the general case for the functional $L$ . EDIT As David Mitra pointed out in the comments, it seems like the question boils down to find conditions under which a Dunford integrable function is even Pettis integrable. Let me fix the terminology: $f:\Omega\to E$ to is called Dunford $\mu$ -integrable if $\langle f,\varphi\rangle\mathcal L^1(\mu)$ . In that case, $$\int f\:{\rm d}\mu:E'\to\mathbb K\;,\;\;\;\varphi\mapsto\int\langle f,\varphi\rangle\:{\rm d}\mu\tag5$$ is a bounded linear functional on $E'$ , i.e. $$\int f\:{\rm d}\mu\in E''.\tag6$$ Now $f$ is called Pettis $\mu$ -integrable if $f$ is Dunford $\mu$ -integrable and $$\int f\:{\rm d}\mu\in\iota E.\tag7$$ First approach for a positive result : I've often seen the assertion that the desired claim is true when $\Omega=[a,b]$ for some $a<b$ , $\mathcal A=\mathcal B([a,b])$ , $\mu$ is the restriction of the Lebesgue measure on $\mathcal B(\mathbb R)$ to $[a,b]$ and $f$ is continuous. The crucial argument is that $[a,b]$ is compact, hence $f$ is even uniformly continuous, which allows us to find $(x_n)_{n\in\mathbb N}\subseteq E$ such that $$\langle x_n,\varphi\rangle\xrightarrow{n\to\infty}\int\langle f,\varphi\rangle\:{\rm d}\mu\tag8$$ uniformly with respect to $\left\|\varphi\right\|_{E'}\le1$ . By Lemma 1 below, this is enough to conclude, but I'm quite sure that this result holds in way more generality. Lemma 1 : In general, $(x_n)_{n\in\mathbb N}\subseteq E$ is norm convergent if and only if $$\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\langle x_m-x_n,\varphi\rangle\right|\xrightarrow{m,\:n\to\infty}0.$$ Second approach for a positive result : Let me continue with the setting I've described at the beginning of this post, i.e. the situation where $f$ is strongly $\mathcal A$ -measurable. Most probably I'm missing something, but if $(x_n)_{n\in\mathbb N}$ and $(f_n)_{n\in\mathbb N}$ are as in $(2)$ and $(3)$ , then we should have \begin{equation}\begin{split}\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\langle x_m-x_n,\varphi\rangle\right|&=\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\int\langle f_m-f_n,\varphi\rangle\:{\rm d}\mu\right|\\&\le\int\left\|f_m-f_n\right\|_E\:{\rm d}\mu\xrightarrow{m,\:n\:\to\:\infty}0\end{split}\tag9.\end{equation} So, by Lemma 1, we should be able to conclude. Am I missing something? If not and this is correct, is this somehow a generalization of what I've described in the ""first approach""? Therein $f$ is continuous (hence $\mathcal A$ -measurable) and, since $[a,b]$ is separable, $f([a,b])$ is separable. So, $f$ is strongly $\mathcal A$ -measurable. Maybe that's the crucial point. $^1$ i.e. $f$ is $\mathcal A$ -measurable and $f(\Omega)$ is separable. The crucial point is that a function $f:\Omega\to E$ is strongly $\mathcal A$ if and only if there are $\mathcal A$ -measurable $f_n:\Omega\to\mathbb R$ with finite range for $n\in\mathbb N$ satisfying $\left\|f_n\right\|_E\le\left\|f\right\|_E$ and $\left\|f_n-f\right\|_E\xrightarrow{n\to\infty}0$ .","Let be a measure space, be a normed space and be strongly -measurable with Are we able to show that for some ? If not, what are sufficient conditions to ensure that? If has finite range, the claim is obviously true. If not, there are -measurable with finite range for satisfying and . So, there are with Using Lebesgue’s dominated convergence theorem, it's now easy to see that which at least implies that is weakly Cauchy. However, we know that a weak Cauchy sequence doesn't need to weakly converge. So, maybe the problem I'm asking here is a particular instance of the fact that if is any weakl Cauchy sequence, then is a well-defined linear functional (since the field over which is a vector space is complete), but is not necessarily of the form for some . Maybe all of this boils down to the properties of the canonical embedding In any case, I would really appreciate if someone could show under which conditions we are able to prove the desired result; but I'm also interested in results related to the general case for the functional . EDIT As David Mitra pointed out in the comments, it seems like the question boils down to find conditions under which a Dunford integrable function is even Pettis integrable. Let me fix the terminology: to is called Dunford -integrable if . In that case, is a bounded linear functional on , i.e. Now is called Pettis -integrable if is Dunford -integrable and First approach for a positive result : I've often seen the assertion that the desired claim is true when for some , , is the restriction of the Lebesgue measure on to and is continuous. The crucial argument is that is compact, hence is even uniformly continuous, which allows us to find such that uniformly with respect to . By Lemma 1 below, this is enough to conclude, but I'm quite sure that this result holds in way more generality. Lemma 1 : In general, is norm convergent if and only if Second approach for a positive result : Let me continue with the setting I've described at the beginning of this post, i.e. the situation where is strongly -measurable. Most probably I'm missing something, but if and are as in and , then we should have So, by Lemma 1, we should be able to conclude. Am I missing something? If not and this is correct, is this somehow a generalization of what I've described in the ""first approach""? Therein is continuous (hence -measurable) and, since is separable, is separable. So, is strongly -measurable. Maybe that's the crucial point. i.e. is -measurable and is separable. The crucial point is that a function is strongly if and only if there are -measurable with finite range for satisfying and .","(\Omega,\mathcal A,\mu) E f:\Omega\to E ^1 \mathcal A \langle f,\varphi\rangle\in\mathcal L^1(\mu)\;\;\;\text{for all }\varphi\in E'. \int\langle f,\varphi\rangle\:{\rm d}\mu=\langle x,\varphi\rangle\;\;\;\text{for all }\varphi\in E'\tag1 x\in E f \mathcal A f_n:\Omega\to\mathbb R n\in\mathbb N \left\|f_n\right\|_E\le\left\|f\right\|_E \left\|f_n-f\right\|_E\xrightarrow{n\to\infty}0 (x_n)_{n\in\mathbb N}\subseteq E \int\langle f_n,\varphi\rangle\:{\rm d}\mu=\langle x_n,\varphi\rangle\;\;\;\text{for all }\varphi\in E'\text{ and }n\in\mathbb N\tag2. \langle x_n,\varphi\rangle\xrightarrow{n\to\infty}\int\langle f,\varphi\rangle\:{\rm d}\mu\tag3\;\;\;\text{for all }\varphi\in E', (x_n)_{n\in\mathbb N} (x_n)_{n\in\mathbb N} L\varphi:=\lim_{n\to\infty}\langle x_n,\varphi\rangle\;\;\;\text{for all }\varphi\in E' E L L\varphi=\langle x,\varphi\rangle\tag4 x\in E \iota:E\to E''\;,\;\;\;x\mapsto\left(E'\ni\varphi\mapsto\langle x,\varphi\rangle\right). L f:\Omega\to E \mu \langle f,\varphi\rangle\mathcal L^1(\mu) \int f\:{\rm d}\mu:E'\to\mathbb K\;,\;\;\;\varphi\mapsto\int\langle f,\varphi\rangle\:{\rm d}\mu\tag5 E' \int f\:{\rm d}\mu\in E''.\tag6 f \mu f \mu \int f\:{\rm d}\mu\in\iota E.\tag7 \Omega=[a,b] a<b \mathcal A=\mathcal B([a,b]) \mu \mathcal B(\mathbb R) [a,b] f [a,b] f (x_n)_{n\in\mathbb N}\subseteq E \langle x_n,\varphi\rangle\xrightarrow{n\to\infty}\int\langle f,\varphi\rangle\:{\rm d}\mu\tag8 \left\|\varphi\right\|_{E'}\le1 (x_n)_{n\in\mathbb N}\subseteq E \sup_{\left\|\varphi\right\|_{E'}\le1}\left|\langle x_m-x_n,\varphi\rangle\right|\xrightarrow{m,\:n\to\infty}0. f \mathcal A (x_n)_{n\in\mathbb N} (f_n)_{n\in\mathbb N} (2) (3) \begin{equation}\begin{split}\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\langle x_m-x_n,\varphi\rangle\right|&=\sup_{\left\|\varphi\right\|_{E'}\le1}\left|\int\langle f_m-f_n,\varphi\rangle\:{\rm d}\mu\right|\\&\le\int\left\|f_m-f_n\right\|_E\:{\rm d}\mu\xrightarrow{m,\:n\:\to\:\infty}0\end{split}\tag9.\end{equation} f \mathcal A [a,b] f([a,b]) f \mathcal A ^1 f \mathcal A f(\Omega) f:\Omega\to E \mathcal A \mathcal A f_n:\Omega\to\mathbb R n\in\mathbb N \left\|f_n\right\|_E\le\left\|f\right\|_E \left\|f_n-f\right\|_E\xrightarrow{n\to\infty}0","['functional-analysis', 'measure-theory', 'banach-spaces', 'dual-spaces']"
74,Why do we want $C^\infty_c$ to be complete in distribution theory?,Why do we want  to be complete in distribution theory?,C^\infty_c,"I am studying distribution theory. Denoting by $C_c^\infty$ the space of infinitely diferenciable function with compact support. Many authors point out the topology generated by the norms on the space $C_c^\infty(K)$ with $K$ compact (thats is the funcions in $C_c^\infty$ with support in $K$ ) is not complete to justify the introduction of the machinery of inductive limit topology on $C_c^\infty$ to turn it complete. Well, completeness is certainly usefull, however up to now I did not see where exactly the theory break down, without completness. Somebody could give me some hints? Maybe it arises in applications... Sincerely. Edit - A kind of self answer: Let $C^\infty_c(\mathbb R)$ equipped with the non complete topology, it is the topology corresponding to the uniform conergence (of any derivatives) on the compacts (I am right?). Now, let $\Lambda\in C^\infty_c(\mathbb R)*$ (the algebraic dual), $\Lambda$ is continuous implies $\Lambda$ is continuous on any $C^\infty_c(K)$ but the converse is false. Take $\varphi_n=\xi(x-n)$ with $\xi$ in $C^\infty_c$ with support in $[0,1]$ and $\int \xi =1$ . We have $\varphi_n \to 0$ (as well as its derivatives) uniformely on the compact, but $$\Lambda(\varphi_n) = \int \varphi_n = 1 \neq \Lambda(0)$$ also for any compact $K$ there exists a constant $C>0$ such that: $$|\Lambda(\varphi)| \leq C\sup_{x\in K} |\varphi|$$ so we have no ""good"" continuity criterion for the linear form. The inductive limit topology provides the good one since bounded subset belongs to a $C_c^\infty(K)$ . Is I am right!? If it is the case, it is confuse to me, because all the textbook I found, such as the one of Rudin say something like: ""this topo is not complete, so let construct an other one which turn it complete"" to justify the construction and this messed up. It seems (to me) the problem is more about characterization of continuous linear form on $C^\infty_c(\Omega)$ , and for free we get completeness...","I am studying distribution theory. Denoting by the space of infinitely diferenciable function with compact support. Many authors point out the topology generated by the norms on the space with compact (thats is the funcions in with support in ) is not complete to justify the introduction of the machinery of inductive limit topology on to turn it complete. Well, completeness is certainly usefull, however up to now I did not see where exactly the theory break down, without completness. Somebody could give me some hints? Maybe it arises in applications... Sincerely. Edit - A kind of self answer: Let equipped with the non complete topology, it is the topology corresponding to the uniform conergence (of any derivatives) on the compacts (I am right?). Now, let (the algebraic dual), is continuous implies is continuous on any but the converse is false. Take with in with support in and . We have (as well as its derivatives) uniformely on the compact, but also for any compact there exists a constant such that: so we have no ""good"" continuity criterion for the linear form. The inductive limit topology provides the good one since bounded subset belongs to a . Is I am right!? If it is the case, it is confuse to me, because all the textbook I found, such as the one of Rudin say something like: ""this topo is not complete, so let construct an other one which turn it complete"" to justify the construction and this messed up. It seems (to me) the problem is more about characterization of continuous linear form on , and for free we get completeness...","C_c^\infty C_c^\infty(K) K C_c^\infty K C_c^\infty C^\infty_c(\mathbb R) \Lambda\in C^\infty_c(\mathbb R)* \Lambda \Lambda C^\infty_c(K) \varphi_n=\xi(x-n) \xi C^\infty_c [0,1] \int \xi =1 \varphi_n \to 0 \Lambda(\varphi_n) = \int \varphi_n = 1 \neq \Lambda(0) K C>0 |\Lambda(\varphi)| \leq C\sup_{x\in K} |\varphi| C_c^\infty(K) C^\infty_c(\Omega)","['functional-analysis', 'distribution-theory']"
75,The definition of the Klein form and its fundamental properties,The definition of the Klein form and its fundamental properties,,"In Kubert-Lang's ""Units in the modular function field II"", for a fixed integer $N$ , the Klein form is defined as, for $r, s \in \mathbb{Z}$ (at least one of them is not $\equiv 0 \mod N$ ) and for $\omega_i \in \mathbb{C}$ with $\Im \omega_1/\omega_2 \gt 0$ , $$ k_{r,s}(\omega_1, \omega_2)  = \exp( - \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}) \sigma(\frac{r\omega_1 + s\omega_2}{N}),$$ where $\eta$ is the Weierstrass eta function, $\sigma$ is the Weierstrass sigma function, with respect to the latice $ \Lambda = \omega_1 \mathbb{Z} + \omega_2 \mathbb{Z}$ . (i.e., $\eta(\omega) = \zeta(z + \omega) - \zeta(z)$ for any $z \in \mathbb{C}$ , where $\zeta$ is the Weierstrass zeta function with respect to the latice, and $$ \sigma(z)  = z \Pi_{(a,b) \in \mathbb{Z}^2 - \{ 0 \}}(1 - \frac{z}{a\omega_1 + b\omega_2}) \exp(\frac{z}{a\omega_1 + b\omega_2} + \frac{1}{2} (\frac{z}{a\omega_1 + b\omega_2})^2).$$ ) The authors says that, for integers $a, b \in \mathbb{Z}$ , the Klein form satisfies $$k_{r + aN, s + bN} (\omega_1, \omega_2)  = (-1)^{ab + a +b } \exp(-2 \pi i \frac {(as - br)}{2N}) k_{r,s}(\omega_1, \omega_2).$$ Using the relation $$ \sigma(z + \omega)/\sigma(z) = (-1)^{ab + a + b}\exp(\eta(\omega)(z + \frac{1}{2} \omega))$$ for $\omega = a\omega_1 + b\omega_2 \in \Lambda$ , and using the Legendre relation, in order to show the relation, we must show that $$\exp( - \frac{(r+aN)\eta(\omega_1) + (s + bN)\eta(\omega_2)}{N} \frac{(r+aN)\omega_1 + (s + bN)\omega_2}{N} + \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}  + \eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))) \\ = \exp(-\frac{2 \pi i (as - br)}{2N}). $$ I tried again and again, but I can't get the desired result. If the term $\eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))$ were replaced by $2\eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))$ , then I could show the relation. And the authors says that for $\gamma \in \Gamma(N)$ , $k_{0,a}(\gamma \tau)/k_{0,a}(\tau)$ is a $2N$ root of $1$ . But p108 of the Mazur's paper ""Modular curves and the Eisenstein ideal"", the authors says that $k_{0,a}(\gamma \tau)/k_{0,a}(\tau)$ is not constant. And, in that Mazur's paper, the author also says that the $q$ expansion of $k_{0,a}(\tau)$ is ( $q = \exp( 2 \pi i \tau)$ ) $$ - \frac{1}{2 \pi i}\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}.$$ (where $\zeta_N = \exp(\frac{2 \pi i}{N}).$ ) But computing it, I've got the wrong result: $$ - \exp(- \frac{1}{2}\eta(1) (\frac{a}{N})^2) \frac{1}{2 \pi i}\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}.$$ Now the $q$ expansion of $\sigma(a/N, \tau \mathbb{Z} + \mathbb{Z})$ is $$ -  \frac{1}{2 \pi i} \exp(\frac{1}{2}\eta(1) (\frac{a}{N})^2)\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}.$$ So if the term $\exp(\frac{1}{2}\eta(1) (\frac{a}{N})^2)$ were replaced by $\exp(\eta(1) (\frac{a}{N})^2)$ , then I could show it. What is wrong in my opinion? Or are there another definitions of the functions? (It seems that if the definition of the Klein form is $$ k_{r,s}(\omega_1, \omega_2)  = \exp( - 2 \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}) \sigma(\frac{r\omega_1 + s\omega_2}{N}),$$ then I can show everything, except the ""transformation law"" described in p108 of Mazur's paper.)","In Kubert-Lang's ""Units in the modular function field II"", for a fixed integer , the Klein form is defined as, for (at least one of them is not ) and for with , where is the Weierstrass eta function, is the Weierstrass sigma function, with respect to the latice . (i.e., for any , where is the Weierstrass zeta function with respect to the latice, and ) The authors says that, for integers , the Klein form satisfies Using the relation for , and using the Legendre relation, in order to show the relation, we must show that I tried again and again, but I can't get the desired result. If the term were replaced by , then I could show the relation. And the authors says that for , is a root of . But p108 of the Mazur's paper ""Modular curves and the Eisenstein ideal"", the authors says that is not constant. And, in that Mazur's paper, the author also says that the expansion of is ( ) (where ) But computing it, I've got the wrong result: Now the expansion of is So if the term were replaced by , then I could show it. What is wrong in my opinion? Or are there another definitions of the functions? (It seems that if the definition of the Klein form is then I can show everything, except the ""transformation law"" described in p108 of Mazur's paper.)","N r, s \in \mathbb{Z} \equiv 0 \mod N \omega_i \in \mathbb{C} \Im \omega_1/\omega_2 \gt 0  k_{r,s}(\omega_1, \omega_2)
 = \exp( - \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}) \sigma(\frac{r\omega_1 + s\omega_2}{N}), \eta \sigma  \Lambda = \omega_1 \mathbb{Z} + \omega_2 \mathbb{Z} \eta(\omega) = \zeta(z + \omega) - \zeta(z) z \in \mathbb{C} \zeta  \sigma(z)
 = z \Pi_{(a,b) \in \mathbb{Z}^2 - \{ 0 \}}(1 - \frac{z}{a\omega_1 + b\omega_2}) \exp(\frac{z}{a\omega_1 + b\omega_2} + \frac{1}{2} (\frac{z}{a\omega_1 + b\omega_2})^2). a, b \in \mathbb{Z} k_{r + aN, s + bN} (\omega_1, \omega_2)
 = (-1)^{ab + a +b } \exp(-2 \pi i \frac {(as - br)}{2N}) k_{r,s}(\omega_1, \omega_2).  \sigma(z + \omega)/\sigma(z) = (-1)^{ab + a + b}\exp(\eta(\omega)(z + \frac{1}{2} \omega)) \omega = a\omega_1 + b\omega_2 \in \Lambda \exp( - \frac{(r+aN)\eta(\omega_1) + (s + bN)\eta(\omega_2)}{N} \frac{(r+aN)\omega_1 + (s + bN)\omega_2}{N} + \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N} 
+ \eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))) \\
= \exp(-\frac{2 \pi i (as - br)}{2N}).  \eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2)) 2\eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2)) \gamma \in \Gamma(N) k_{0,a}(\gamma \tau)/k_{0,a}(\tau) 2N 1 k_{0,a}(\gamma \tau)/k_{0,a}(\tau) q k_{0,a}(\tau) q = \exp( 2 \pi i \tau)  - \frac{1}{2 \pi i}\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}. \zeta_N = \exp(\frac{2 \pi i}{N}).  - \exp(- \frac{1}{2}\eta(1) (\frac{a}{N})^2) \frac{1}{2 \pi i}\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}. q \sigma(a/N, \tau \mathbb{Z} + \mathbb{Z})  -  \frac{1}{2 \pi i} \exp(\frac{1}{2}\eta(1) (\frac{a}{N})^2)\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}. \exp(\frac{1}{2}\eta(1) (\frac{a}{N})^2) \exp(\eta(1) (\frac{a}{N})^2)  k_{r,s}(\omega_1, \omega_2)
 = \exp( - 2 \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}) \sigma(\frac{r\omega_1 + s\omega_2}{N}),","['complex-analysis', 'functional-analysis', 'modular-arithmetic', 'modular-forms', 'elliptic-functions']"
76,"If $T_{s+t}(\varphi)=T_s(\varphi)T_t(\varphi)$ and $T_0(\varphi)=1$, is there a function $f$ with $T_t(\varphi)=e^{-tf(\varphi)}$?","If  and , is there a function  with ?",T_{s+t}(\varphi)=T_s(\varphi)T_t(\varphi) T_0(\varphi)=1 f T_t(\varphi)=e^{-tf(\varphi)},"Let $E$ be a $\mathbb R$ -Banach space and $T_t:E'\to\mathbb C$ for $t\ge0$ with $$T_{s+t}(\varphi)=T_s(\varphi)T_t(\varphi)\;\;\;\text{for all }\varphi\in E'\text{ and }s,t\ge0\tag1$$ and $$T_0(\varphi)=1\;\;\;\text{for all }\varphi\in E'\tag2.$$ Are we able to conclude that $$T_t(\varphi)=e^{-tf(\varphi)}\;\;\;\text{for all }\varphi\in E'\text{ and }t\ge0\tag3$$ for some $f:E'\to\mathbb C$ with $f(0)=0$ ? Moreover, assuming $[0,\infty)\ni t\mapsto T_t(\varphi)$ is continuous for all $\varphi\in E'$ , are we able to conclude that $f$ is continuous? I guess the pure existence of $f$ is somehow an easy consequence of the functional equation solved by the exponential function. If it simplifies the matter, I'm also interested in the case where $E$ is a $\mathbb R$ -Hilbert space and/or separable.","Let be a -Banach space and for with and Are we able to conclude that for some with ? Moreover, assuming is continuous for all , are we able to conclude that is continuous? I guess the pure existence of is somehow an easy consequence of the functional equation solved by the exponential function. If it simplifies the matter, I'm also interested in the case where is a -Hilbert space and/or separable.","E \mathbb R T_t:E'\to\mathbb C t\ge0 T_{s+t}(\varphi)=T_s(\varphi)T_t(\varphi)\;\;\;\text{for all }\varphi\in E'\text{ and }s,t\ge0\tag1 T_0(\varphi)=1\;\;\;\text{for all }\varphi\in E'\tag2. T_t(\varphi)=e^{-tf(\varphi)}\;\;\;\text{for all }\varphi\in E'\text{ and }t\ge0\tag3 f:E'\to\mathbb C f(0)=0 [0,\infty)\ni t\mapsto T_t(\varphi) \varphi\in E' f f E \mathbb R","['functional-analysis', 'exponential-function', 'hilbert-spaces', 'banach-spaces', 'functional-equations']"
77,compact subset of $\mathbb{C}$ is spectrum. (Banach space / hilbert space),compact subset of  is spectrum. (Banach space / hilbert space),\mathbb{C},"I'm studying that what set can be the spectrum of operator. If $\mathcal{H}$ is an infinite dimension Hilbert space and $K$ is a non-empty compact subset of $\mathbb{C}$ , show that there is an $A$ in $\mathcal{B}(\mathcal{H})$ such that $\sigma(A) = K$ . Show that any compact set in $\mathbb{C}$ is the spectrum of an operator. This is related question about this question. But it gave an answer when $\mathcal{H}$ is separable, not the infinite dimension case. But I believe that my question can be reduced to link. How to do that. In addition, what about this question? If $K$ is a non-empty compact subset of $\mathbb{C}$ , does there exist an operator $A$ in $\mathcal{B}(C[0,1])$ such that $\sigma(A) = K$ . I saw the result that in Banach space, this property does not hold in general, but I think in this case, we can find this one, but I cannot find this example with above example. can you help me?","I'm studying that what set can be the spectrum of operator. If is an infinite dimension Hilbert space and is a non-empty compact subset of , show that there is an in such that . Show that any compact set in $\mathbb{C}$ is the spectrum of an operator. This is related question about this question. But it gave an answer when is separable, not the infinite dimension case. But I believe that my question can be reduced to link. How to do that. In addition, what about this question? If is a non-empty compact subset of , does there exist an operator in such that . I saw the result that in Banach space, this property does not hold in general, but I think in this case, we can find this one, but I cannot find this example with above example. can you help me?","\mathcal{H} K \mathbb{C} A \mathcal{B}(\mathcal{H}) \sigma(A) = K \mathcal{H} K \mathbb{C} A \mathcal{B}(C[0,1]) \sigma(A) = K","['functional-analysis', 'banach-spaces', 'banach-algebras']"
78,"If $x$ maximizes $x^T(u+v)$ subject to $\|x\|^*\le 1$, then $x^T u \ge x^T v \Leftrightarrow \|u\| \ge\|v\|$","If  maximizes  subject to , then",x x^T(u+v) \|x\|^*\le 1 x^T u \ge x^T v \Leftrightarrow \|u\| \ge\|v\|,"Suppose that $\|\cdot\|$ is a norm on $\mathbb{R}^n$ and $\|\cdot\|^*$ is its dual norm. Consider two vectors $u$ and $v$ in $\mathbb{R}^n$ such that $u+v\neq 0$ . Let $x$ be a vector that maximizes $x^T(u+v)$ subject to $\|x\|^* \le 1$ , the optimal value being $\|u+v\|$ as we know. If $\|\cdot\|$ is the $2$ -norm, then $x= t(u+v)$ with a scalar $t>0$ . Hence $$ x^T u -x^T v= t(u+v)^T(u-v) = t(\|u\|^2 -\|v\|^2), $$ which tells us that $$ x^T u \ge x^T v \quad \Longleftrightarrow \quad \|u\|\ge \|v\|. $$ Roughly speaking, if $u+v$ is closer to $u$ than to $v$ , then $x^Tu$ is larger than $x^T v$ with this special $x$ , and vice versa. Why is this interesting ? Imagine that $u$ is a signal and $v$ is a perturbation. In some scenarios, we can observe $u+v$ only by the linear functional $x$ . Naturally, we hope that the observation contains more truth than error provided that the signal is stronger than the perturbation. It turns out true if the geometry of $\mathbb{R}^n$ is the Eucleadian one. What if we consider other geometry? If $\|\cdot\|$ is induced by an inner product, we still have $x^T u \ge x^T v \Longleftrightarrow  \|u\|\ge \|v\|$ , which can be proved by a similar argument. However, the same implication does not hold if $\|\cdot\|$ is the $1$ -norm. Consider the following vectors in $\mathbb{R}^2$ : $$ u=(1,1)^T \quad \text{ and } \quad v = (-1-\epsilon, -1+2\epsilon)^T, $$ where $\epsilon\in(0,1/2)$ . Then $u+v=(-\epsilon, 2\epsilon)^T$ , and hence $x = (-1, 1)^T$ . It is easy to check that $\|u\|=2>2-\epsilon = \|v\|$ whereas $x^T u = 0 < 3\epsilon = x^T v$ . Question . Under the setting specified in the first paragraph , can we guarantee the equivalence between $x^Tu\ge x^T v$ and $\|u\| \ge \|v\|$ for a class of norms more general than those induced by inner products? An inference that may be wrong : by continuity, the abovementioned example for the $1$ -norm may also work for $p$ -norm with $p>1$ but close to $1$ . Assuming this is true , the desired implication is not ensured if $\|\cdot\|$ is uniformly convex. Then maybe inner-product-induced norms are the only possible ones to ensure it. It would not be extraordinary if this turns out true because dot product and inner products are nothing but representations of linear functionals on $\mathbb{R}^n$ and they are naturally connected. A rigorous proof would be very intriguing. This is inspired by interactions with @supinf in the comments. Even though it is not the primary objective, an extension to more general spaces will also be interesting --- doing so for inner-product spaces should not be difficult. Any comments or criticism will be appreciated. Thanks.","Suppose that is a norm on and is its dual norm. Consider two vectors and in such that . Let be a vector that maximizes subject to , the optimal value being as we know. If is the -norm, then with a scalar . Hence which tells us that Roughly speaking, if is closer to than to , then is larger than with this special , and vice versa. Why is this interesting ? Imagine that is a signal and is a perturbation. In some scenarios, we can observe only by the linear functional . Naturally, we hope that the observation contains more truth than error provided that the signal is stronger than the perturbation. It turns out true if the geometry of is the Eucleadian one. What if we consider other geometry? If is induced by an inner product, we still have , which can be proved by a similar argument. However, the same implication does not hold if is the -norm. Consider the following vectors in : where . Then , and hence . It is easy to check that whereas . Question . Under the setting specified in the first paragraph , can we guarantee the equivalence between and for a class of norms more general than those induced by inner products? An inference that may be wrong : by continuity, the abovementioned example for the -norm may also work for -norm with but close to . Assuming this is true , the desired implication is not ensured if is uniformly convex. Then maybe inner-product-induced norms are the only possible ones to ensure it. It would not be extraordinary if this turns out true because dot product and inner products are nothing but representations of linear functionals on and they are naturally connected. A rigorous proof would be very intriguing. This is inspired by interactions with @supinf in the comments. Even though it is not the primary objective, an extension to more general spaces will also be interesting --- doing so for inner-product spaces should not be difficult. Any comments or criticism will be appreciated. Thanks.","\|\cdot\| \mathbb{R}^n \|\cdot\|^* u v \mathbb{R}^n u+v\neq 0 x x^T(u+v) \|x\|^* \le 1 \|u+v\| \|\cdot\| 2 x= t(u+v) t>0 
x^T u -x^T v= t(u+v)^T(u-v) = t(\|u\|^2 -\|v\|^2),
 
x^T u \ge x^T v \quad \Longleftrightarrow \quad \|u\|\ge \|v\|.
 u+v u v x^Tu x^T v x u v u+v x \mathbb{R}^n \|\cdot\| x^T u \ge x^T v \Longleftrightarrow  \|u\|\ge \|v\| \|\cdot\| 1 \mathbb{R}^2 
u=(1,1)^T \quad \text{ and } \quad v = (-1-\epsilon, -1+2\epsilon)^T,
 \epsilon\in(0,1/2) u+v=(-\epsilon, 2\epsilon)^T x = (-1, 1)^T \|u\|=2>2-\epsilon = \|v\| x^T u = 0 < 3\epsilon = x^T v x^Tu\ge x^T v \|u\| \ge \|v\| 1 p p>1 1 \|\cdot\| \mathbb{R}^n","['functional-analysis', 'optimization', 'convex-optimization', 'inner-products', 'nonlinear-optimization']"
79,How to extend the Radon transform to $L^2(\mathbb{R}^2)$?,How to extend the Radon transform to ?,L^2(\mathbb{R}^2),"The (2D) Radon transform $R$ is usually defined for functions in the Schwartz space $S(\mathbb{R}^2)$ or bump functions $C_c^\infty(\mathbb{R}^2)$ by \begin{align*} R\colon C_c^\infty(\mathbb{R}^2)&\to L^2(\mathbb{R}\times[0,\pi))\newline (Rf)(r,\varphi)&:=\int_{L(r,\varphi)}f(x) dx \end{align*} where $L(r,\varphi)$ is the line in the plane parameterized by the angle $\varphi$ and the directed distance from the origin $r$ . This definition ensures that the integral exists. It is easy to show (by estimating the operator norm) that the Radon transform is continuous when it is (for example) restricted to $C_c^\infty(\Omega)$ , where $\Omega$ is the unit ball in $\mathbb{R}^2$ . Because $C_c^\infty(\Omega)$ is dense in $L^2(\Omega)$ , the Radon transform then has a unique continuous extension to $L^2(\Omega)$ . In the literature however, I often see references to the Radon transform defined on $L^2(\mathbb{R}^2)$ . But I do not understand how the Radon transform can be extended from the bump functions to $L^2(\mathbb{R}^2)$ since the proof for continuity only works for bounded sets. Is the Radon transform continuous when defined on $C_c^\infty(\mathbb{R}^2)$ ? If so, how can this be shown? If not, how can it then be extended to $L^2(\mathbb{R}^2)$ ?","The (2D) Radon transform is usually defined for functions in the Schwartz space or bump functions by where is the line in the plane parameterized by the angle and the directed distance from the origin . This definition ensures that the integral exists. It is easy to show (by estimating the operator norm) that the Radon transform is continuous when it is (for example) restricted to , where is the unit ball in . Because is dense in , the Radon transform then has a unique continuous extension to . In the literature however, I often see references to the Radon transform defined on . But I do not understand how the Radon transform can be extended from the bump functions to since the proof for continuity only works for bounded sets. Is the Radon transform continuous when defined on ? If so, how can this be shown? If not, how can it then be extended to ?","R S(\mathbb{R}^2) C_c^\infty(\mathbb{R}^2) \begin{align*}
R\colon C_c^\infty(\mathbb{R}^2)&\to L^2(\mathbb{R}\times[0,\pi))\newline
(Rf)(r,\varphi)&:=\int_{L(r,\varphi)}f(x) dx
\end{align*} L(r,\varphi) \varphi r C_c^\infty(\Omega) \Omega \mathbb{R}^2 C_c^\infty(\Omega) L^2(\Omega) L^2(\Omega) L^2(\mathbb{R}^2) L^2(\mathbb{R}^2) C_c^\infty(\mathbb{R}^2) L^2(\mathbb{R}^2)","['functional-analysis', 'integral-transforms', 'inverse-problems']"
80,Distributional Fourier transform of $\arctan$,Distributional Fourier transform of,\arctan,"I understand that for a Schwartz function $f:\mathbb{R}\to\mathbb{C}$ , its Fourier transform is another Schwartz function, while if $f$ was a bounded continuous function, then in general its Fourier transform is a distribution. In particular, one can talk about the support of a distribution. Question: Consider the bounded function $f(x)=\arctan(x)$ . Is its distributional Fourier transform $\widehat{f}$ a compactly supported distribution?","I understand that for a Schwartz function , its Fourier transform is another Schwartz function, while if was a bounded continuous function, then in general its Fourier transform is a distribution. In particular, one can talk about the support of a distribution. Question: Consider the bounded function . Is its distributional Fourier transform a compactly supported distribution?",f:\mathbb{R}\to\mathbb{C} f f(x)=\arctan(x) \widehat{f},"['functional-analysis', 'fourier-analysis', 'fourier-transform', 'distribution-theory']"
81,"Ellipsoid in $L^p([0,1],\lambda)$ spaces?",Ellipsoid in  spaces?,"L^p([0,1],\lambda)","Let us consider $L^p([0,1],\lambda)$ spaces, were $\lambda$ is simply the lebesgue measure. These are Banach spaces for $p\ge1$ (of course). It is well known (see for example here: Embedding of Lp spaces ), that for $ 1\leq p < q \leq +\infty$ we have an inclusion (embedding) $$ T^q_p \ : \ L^q([0,1],\lambda) \rightarrow L^p([0,1], \lambda),$$ which is linear (thus also affine). One thing I know about affine transformations of real $\mathbb{R}^{n}$ spaces is, that it can transform an ellipsoid into a sphere (and vice versa). This is why I wonder what can we say about this fact in the more abstract setting of $L^p$ spaces. Let as take $p=2$ and any $q>2$ . Define $$S^q_2=\{f\in L^q:||f||_2=1\}.$$ Then $S_2^q$ considered in the $L^2$ space is a subset of the unit sphere. The question is As $T_2^q(S_2^q)=S_2^q$ , and $T_2^q$ is affine, does it hold true, that $S_{2}^q$ is ""some kind"" of an ellipsoid in the $L^q$ space? If so, how is an appropriate notion of such ellipsoid defined? The only thing that comes to my mind would possibly  be as a set of points such that a sum of their distances from some given $f$ and $g$ functions is fixed. But this is only a childish guess... I would be glad for any insight.","Let us consider spaces, were is simply the lebesgue measure. These are Banach spaces for (of course). It is well known (see for example here: Embedding of Lp spaces ), that for we have an inclusion (embedding) which is linear (thus also affine). One thing I know about affine transformations of real spaces is, that it can transform an ellipsoid into a sphere (and vice versa). This is why I wonder what can we say about this fact in the more abstract setting of spaces. Let as take and any . Define Then considered in the space is a subset of the unit sphere. The question is As , and is affine, does it hold true, that is ""some kind"" of an ellipsoid in the space? If so, how is an appropriate notion of such ellipsoid defined? The only thing that comes to my mind would possibly  be as a set of points such that a sum of their distances from some given and functions is fixed. But this is only a childish guess... I would be glad for any insight.","L^p([0,1],\lambda) \lambda p\ge1  1\leq p < q \leq +\infty  T^q_p \ : \ L^q([0,1],\lambda) \rightarrow L^p([0,1], \lambda), \mathbb{R}^{n} L^p p=2 q>2 S^q_2=\{f\in L^q:||f||_2=1\}. S_2^q L^2 T_2^q(S_2^q)=S_2^q T_2^q S_{2}^q L^q f g",['functional-analysis']
82,"Boundedness of $\log(\|A^{-1}\|)/\log(\|A\|)$ as $A$ ranges over $SL(n,\mathbb R)$",Boundedness of  as  ranges over,"\log(\|A^{-1}\|)/\log(\|A\|) A SL(n,\mathbb R)","I wonder if the quantity $\frac{\log(\|A^{-1}\|)}{\log(\|A\|)}$ is bounded by a constant number that only depends on the size $n$ of the matrix as $A$ ranges over $SL(n,\mathbb R)$ (the determinant one matrices). I couldn't find a counterexample, nor can I prove it. Here I require $\|\cdot \|$ as the operator norm of the matrix w.r.t the 2-norm of the Euclidean spaces. I am not sure if the answer is the same regardless of the norm of the matrix. Please let me know.","I wonder if the quantity is bounded by a constant number that only depends on the size of the matrix as ranges over (the determinant one matrices). I couldn't find a counterexample, nor can I prove it. Here I require as the operator norm of the matrix w.r.t the 2-norm of the Euclidean spaces. I am not sure if the answer is the same regardless of the norm of the matrix. Please let me know.","\frac{\log(\|A^{-1}\|)}{\log(\|A\|)} n A SL(n,\mathbb R) \|\cdot \|","['functional-analysis', 'matrix-calculus']"
83,"Real spectrum for an ""almost self-adjoint"" Fredholm mapping","Real spectrum for an ""almost self-adjoint"" Fredholm mapping",,"Consider the Sobolev space on the torus $\mathbb{T}=\mathbb{R}/2\pi \mathbb{Z}$ given by $H^m(\mathbb{T})$ is the $L^2$ -functions such that the Fourier coefficients satisfy the decay rate $\sum_{n\in\mathbb{Z}} |\hat{f}(n)|^2(1+n^2)^m<\infty$ (this is the square of the norm). Suppose $A: H^m(\mathbb{T})\rightarrow L^2(\mathbb{T})$ is bounded and linear, and there exists $B:L^2(\mathbb{T})\rightarrow H^m(\mathbb{T})$ such that $AB-I=E$ and $BA-I=F$ where $I$ is the identity and $E,F$ are compact. Suppose also that $(Af,g) = (f,Ag)$ whenever $f,g\in H^m(\mathbb{T})$ , where $(,)$ is the $L^2$ inner product. The goal is to show that $A-\lambda I: H^m(\mathbb{T})\rightarrow L^2(\mathbb{T})$ is invertible when $\lambda\notin \mathbb{R}$ . As a hint, it is given that $E^*$ maps $L^2$ into $H^m$ and $(Bf,g)=(f,Bg)$ for all $f,g\in L^2(\mathbb{T})$ . I was able to show that $(Ef,g)=(f,Fg)$ whenever $g\in H^m$ and $f\in L^2$ but I wasn't sure how to make use of this. I'm not sure how to mimic the proof of the spectral theorem for compact operators into this case.","Consider the Sobolev space on the torus given by is the -functions such that the Fourier coefficients satisfy the decay rate (this is the square of the norm). Suppose is bounded and linear, and there exists such that and where is the identity and are compact. Suppose also that whenever , where is the inner product. The goal is to show that is invertible when . As a hint, it is given that maps into and for all . I was able to show that whenever and but I wasn't sure how to make use of this. I'm not sure how to mimic the proof of the spectral theorem for compact operators into this case.","\mathbb{T}=\mathbb{R}/2\pi \mathbb{Z} H^m(\mathbb{T}) L^2 \sum_{n\in\mathbb{Z}} |\hat{f}(n)|^2(1+n^2)^m<\infty A: H^m(\mathbb{T})\rightarrow L^2(\mathbb{T}) B:L^2(\mathbb{T})\rightarrow H^m(\mathbb{T}) AB-I=E BA-I=F I E,F (Af,g) = (f,Ag) f,g\in H^m(\mathbb{T}) (,) L^2 A-\lambda I: H^m(\mathbb{T})\rightarrow L^2(\mathbb{T}) \lambda\notin \mathbb{R} E^* L^2 H^m (Bf,g)=(f,Bg) f,g\in L^2(\mathbb{T}) (Ef,g)=(f,Fg) g\in H^m f\in L^2","['functional-analysis', 'fourier-analysis', 'sobolev-spaces']"
84,Prove 'dual to the dual norm is the original norm' without using Hahn-Banach theorem?,Prove 'dual to the dual norm is the original norm' without using Hahn-Banach theorem?,,"Let $\|\cdot\|$ be a norm on $\mathbb{R}^{n} .$ The associated dual norm, denoted $\|\cdot\|_{*},$ is defined as $$ \|z\|_{*}=\sup \left\{z^{\top} x \mid\|x\| \leq 1\right\} $$ I'm trying to prove $$\|x\|_{**} = \|x\|$$ here says we can prove it by Hahn-Banach theorem. i.e. $$\|y\|=\max _{x \neq 0} \frac{x^{T} y}{\|y\|_{*}}$$ But I think by definition, this is what we need to prove. So it seems says 'because this is correct, this is correct'. I think it proves nothing. Are there any proof without using Hahn-Banach theorem?","Let be a norm on The associated dual norm, denoted is defined as I'm trying to prove here says we can prove it by Hahn-Banach theorem. i.e. But I think by definition, this is what we need to prove. So it seems says 'because this is correct, this is correct'. I think it proves nothing. Are there any proof without using Hahn-Banach theorem?","\|\cdot\| \mathbb{R}^{n} . \|\cdot\|_{*}, 
\|z\|_{*}=\sup \left\{z^{\top} x \mid\|x\| \leq 1\right\}
 \|x\|_{**} = \|x\| \|y\|=\max _{x \neq 0} \frac{x^{T} y}{\|y\|_{*}}","['functional-analysis', 'normed-spaces', 'duality-theorems', 'dual-spaces']"
85,"Can I solve this Cauchy problem? If yes, how?","Can I solve this Cauchy problem? If yes, how?",,"Background: I am still studying This question and I could solve my problems with it if the following question can be answered. My problem: Suppose $F:\mathbb{R}\times \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ is $C^{\infty}$ . Suppose $F$ is strictly convex in the third variable but possibly $F_{pp}(t_0,u_0,\beta_0)=0$ for some $(t_0,u_0,\beta_0) \in \mathbb{R}\times \mathbb{R} \times \mathbb{R}$ . For example $F(x,z,p)=p^4-e^x\sin(z)$ as in the quoted question. Can I say that $\frac {\partial}{\partial x}F_p(x,u(x),u'(x))=F_z(x,u(x),u'(x))$ has local solution for every $t_0,u_0,\beta$ such that $u(t_0)=u_0$ and $u'(t_0)=\beta$ ? My attempt: If $F_{pp}$ was strictly positive than I would write $u''=F_{pp}^{-1}(F_z-F_{xp}-F_{zp})$ and I would solve the question with the classical theory about ordinary Cauchy's problems. But if for some value $F_{pp}=0$ ? For the case $F(x,z,p)=p^4-e^x\sin(z)$ we could observe that $F_{pppp}>0$ thus we could reproduce the reasonment as above. But there are strictly convex function such that does not exists $k \geq 0$ with $\frac{\partial ^k}{\partial p^k}F>0$ as $F(x,z,p)=e^{-\frac{1}{p^2}}$ .",Background: I am still studying This question and I could solve my problems with it if the following question can be answered. My problem: Suppose is . Suppose is strictly convex in the third variable but possibly for some . For example as in the quoted question. Can I say that has local solution for every such that and ? My attempt: If was strictly positive than I would write and I would solve the question with the classical theory about ordinary Cauchy's problems. But if for some value ? For the case we could observe that thus we could reproduce the reasonment as above. But there are strictly convex function such that does not exists with as .,"F:\mathbb{R}\times \mathbb{R} \times \mathbb{R} \to \mathbb{R} C^{\infty} F F_{pp}(t_0,u_0,\beta_0)=0 (t_0,u_0,\beta_0) \in \mathbb{R}\times \mathbb{R} \times \mathbb{R} F(x,z,p)=p^4-e^x\sin(z) \frac {\partial}{\partial x}F_p(x,u(x),u'(x))=F_z(x,u(x),u'(x)) t_0,u_0,\beta u(t_0)=u_0 u'(t_0)=\beta F_{pp} u''=F_{pp}^{-1}(F_z-F_{xp}-F_{zp}) F_{pp}=0 F(x,z,p)=p^4-e^x\sin(z) F_{pppp}>0 k \geq 0 \frac{\partial ^k}{\partial p^k}F>0 F(x,z,p)=e^{-\frac{1}{p^2}}","['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation']"
86,Properties for analogues of Bessel function,Properties for analogues of Bessel function,,"Consider the unit circle $\mathbb{S}^1 \subset \mathbb{R}^2$ and consider the uniform measure $\nu$ on $\mathbb{S}^1$ normalised so that $\nu (\mathbb{S}^1)=1$ . The collection of functions $\{1,z,\overline{z},z^2,\overline{z^2}\ldots\}$ forms an orthonormal basis for $L^2(\mathbb{S}^1,\nu)$ . For each $n\geq 0$ , we can think of the Bessel function $J_n$ as the Fourier transform of $z^n$ , that is for $w \in \mathbb{R}^2$ $$ J_n(w) = \int_{\mathbb{S}^1} e^{-i\langle w,(\cos \theta, \sin \theta) \rangle} e^{in\theta} d\nu(\theta). $$ Note that by symmetry, $J_n(w) = J_n(\|w\|)$ and we can think of $J_n$ as just a real valued function $J_n :\mathbb{R} \to \mathbb{R}$ . The Poisson representation formula for $J_n$ is known and it is as follows $$ 	J_{n}(r) = \frac{(r/2)^n}{\Gamma(n+\frac{1}{2}) \sqrt{\pi}} \int_{-1}^{1} e^{irt} (1-t^2)^{n-\frac{1}{2}}~ dt.  $$ It is easy to see the vanishing property of $J_n$ near the origin from the above formula, that is we can conclude $$ |J_{n}(r)| \leq 10 \cdot \frac{(r/2)^n}{\sqrt{2\pi} \left(n/e\right)^n \sqrt{\pi}} \leq \left( \frac{2r}{n}\right)^n. $$ My question is: Suppose we have an arbitrary probability measure $\mu$ on $\mathbb{S}^1$ (which has no atoms) and let $\{f_1,f_2,\ldots\}$ be an orthonormal basis for $L^2(\mathbb{S}^1,\mu)$ . Is it possible to conclude similar vanishing properties near the origin for the Fourier transform of $f_n$ ? Thanks!","Consider the unit circle and consider the uniform measure on normalised so that . The collection of functions forms an orthonormal basis for . For each , we can think of the Bessel function as the Fourier transform of , that is for Note that by symmetry, and we can think of as just a real valued function . The Poisson representation formula for is known and it is as follows It is easy to see the vanishing property of near the origin from the above formula, that is we can conclude My question is: Suppose we have an arbitrary probability measure on (which has no atoms) and let be an orthonormal basis for . Is it possible to conclude similar vanishing properties near the origin for the Fourier transform of ? Thanks!","\mathbb{S}^1 \subset \mathbb{R}^2 \nu \mathbb{S}^1 \nu (\mathbb{S}^1)=1 \{1,z,\overline{z},z^2,\overline{z^2}\ldots\} L^2(\mathbb{S}^1,\nu) n\geq 0 J_n z^n w \in \mathbb{R}^2 
J_n(w) = \int_{\mathbb{S}^1} e^{-i\langle w,(\cos \theta, \sin \theta) \rangle} e^{in\theta} d\nu(\theta).
 J_n(w) = J_n(\|w\|) J_n J_n :\mathbb{R} \to \mathbb{R} J_n 
	J_{n}(r) = \frac{(r/2)^n}{\Gamma(n+\frac{1}{2}) \sqrt{\pi}} \int_{-1}^{1} e^{irt} (1-t^2)^{n-\frac{1}{2}}~ dt. 
 J_n 
|J_{n}(r)| \leq 10 \cdot \frac{(r/2)^n}{\sqrt{2\pi} \left(n/e\right)^n \sqrt{\pi}} \leq \left( \frac{2r}{n}\right)^n.
 \mu \mathbb{S}^1 \{f_1,f_2,\ldots\} L^2(\mathbb{S}^1,\mu) f_n","['functional-analysis', 'measure-theory', 'fourier-transform', 'orthogonal-polynomials']"
87,"If $G$ is a locally compact group equipped with a Haar measure, then $L^{1}(G)$ is a Banach algebra with the convolution as the product operation.","If  is a locally compact group equipped with a Haar measure, then  is a Banach algebra with the convolution as the product operation.",G L^{1}(G),"I'm currently working on a research project based on John B. Conway's ""A Course in Functional Analysis"", specifically Fourier theory on locally compact groups. In his book, Conway claims that $L^{1}(G)$ has the structure of a Banach algebra with the convolution of functions as a product. Most of the properties follow immediately from standard results in measure theory. However, I'm struggling with the proof of the inequality $\Vert f\ast g\Vert\leq\Vert f\Vert\Vert g\Vert$ . All the proofs I've seen at some point rely on a change in the order of integration, which is easily dealt with by making use of Fubini's theorem, but this only works when the space is $\sigma$ -finite, which apparently isn't true for an arbitrary choice of $G$ . It doesn't look like Conway implicitly assumes that this condition is satisfied since in later sections he does calculations involving discrete groups, which fail to satisfy $\sigma$ -finiteness. So I'm guessing there must be a way around this. I saw on a similar thread that you can use a special case of Young's inequality, but after looking into it, the proof of this inequality is also dependent on the ability to change the order of integration. On the other hand, since the product operation in Banach algebras is required to be associative, I have a similar problem when trying to prove this property for the convolution, since the standard proof of associativity of convolution also makes use of Fubini's theorem. So... what gives? Everything works out fine when the space is $\sigma$ -finite, but seems to break down when we drop this hypothesis. I'd be grateful if you could give me some advice on how to work around these. Edit: I found in the same book that by proving that the set of all finite Borel Measures over $G$ is a Banach algebra with an adequate norm and product then if follows that $L^{1}(G)$ is a Banach algebra with the convolution as the product. So, that seems to solve the problem. However, out of curiosity, can we prove this directly, without resorting to a more general object?","I'm currently working on a research project based on John B. Conway's ""A Course in Functional Analysis"", specifically Fourier theory on locally compact groups. In his book, Conway claims that has the structure of a Banach algebra with the convolution of functions as a product. Most of the properties follow immediately from standard results in measure theory. However, I'm struggling with the proof of the inequality . All the proofs I've seen at some point rely on a change in the order of integration, which is easily dealt with by making use of Fubini's theorem, but this only works when the space is -finite, which apparently isn't true for an arbitrary choice of . It doesn't look like Conway implicitly assumes that this condition is satisfied since in later sections he does calculations involving discrete groups, which fail to satisfy -finiteness. So I'm guessing there must be a way around this. I saw on a similar thread that you can use a special case of Young's inequality, but after looking into it, the proof of this inequality is also dependent on the ability to change the order of integration. On the other hand, since the product operation in Banach algebras is required to be associative, I have a similar problem when trying to prove this property for the convolution, since the standard proof of associativity of convolution also makes use of Fubini's theorem. So... what gives? Everything works out fine when the space is -finite, but seems to break down when we drop this hypothesis. I'd be grateful if you could give me some advice on how to work around these. Edit: I found in the same book that by proving that the set of all finite Borel Measures over is a Banach algebra with an adequate norm and product then if follows that is a Banach algebra with the convolution as the product. So, that seems to solve the problem. However, out of curiosity, can we prove this directly, without resorting to a more general object?",L^{1}(G) \Vert f\ast g\Vert\leq\Vert f\Vert\Vert g\Vert \sigma G \sigma \sigma G L^{1}(G),"['functional-analysis', 'measure-theory', 'convolution', 'topological-groups', 'haar-measure']"
88,$L^2$ norm of inverse differential operator,norm of inverse differential operator,L^2,"This has come up in Lemma 1 of Mandache's 2001 paper on exponential instability for the inverse problem of the Schrodinger operator. Let $\Omega = B(0,1)$ in $\mathbb{R}^d$ . Suppose $r_0\in (0,1)$ and $q$ is a function in $L^{\infty}$ supported on $B(0,r_0)$ . Suppose also that $0$ is not a Dirichlet eigenvalue of $-\Delta+ q$ , so that the boundary value problem $(-\Delta + q)u = F$ in $\Omega$ , $u = f$ on $\partial\Omega$ has a unique solution $u\in H^1(\Omega)$ for any $F\in H^{-1}(\Omega)$ , $f\in H^{1/2}(\partial\Omega)$ . I have two questions: In the paper, the author uses the notation $(-\Delta + q)^{-1}$ , which I would assume the operator that takes some $F\in H^{1/2}(\partial \Omega)$ and maps it to the unique $u\in H^1$ such that $(-\Delta + f)u = F$ ; is this correct? He also uses the norm $||(-\Delta + q)^{-1}||_{L^2}$ ; how am I to interpret this?","This has come up in Lemma 1 of Mandache's 2001 paper on exponential instability for the inverse problem of the Schrodinger operator. Let in . Suppose and is a function in supported on . Suppose also that is not a Dirichlet eigenvalue of , so that the boundary value problem in , on has a unique solution for any , . I have two questions: In the paper, the author uses the notation , which I would assume the operator that takes some and maps it to the unique such that ; is this correct? He also uses the norm ; how am I to interpret this?","\Omega = B(0,1) \mathbb{R}^d r_0\in (0,1) q L^{\infty} B(0,r_0) 0 -\Delta+ q (-\Delta + q)u = F \Omega u = f \partial\Omega u\in H^1(\Omega) F\in H^{-1}(\Omega) f\in H^{1/2}(\partial\Omega) (-\Delta + q)^{-1} F\in H^{1/2}(\partial \Omega) u\in H^1 (-\Delta + f)u = F ||(-\Delta + q)^{-1}||_{L^2}","['functional-analysis', 'partial-differential-equations', 'lp-spaces', 'inverse-problems']"
89,Groups where every continuous function is uniformly continuous,Groups where every continuous function is uniformly continuous,,"Let $G$ be a topological group, and $(X, d)$ a metric space. The function $f : G \to X$ is left uniformly continuous if for all $\varepsilon > 0$ there exists an identity neighbourhood $U$ such that for all $g\in G, u \in U$ it holds $d(f(g), f(ug)) < \varepsilon$ . If $G$ is compact or discrete, every continuous function is uniformly continuous. Is there a larger class of groups for which this holds, maybe with some extra assumptions on $X$ or $f$ ? I am particularly interested in totally disconnected locally compact groups $G$ , ultrametric normed vector spaces $X$ and bounded functions $f$ .","Let be a topological group, and a metric space. The function is left uniformly continuous if for all there exists an identity neighbourhood such that for all it holds . If is compact or discrete, every continuous function is uniformly continuous. Is there a larger class of groups for which this holds, maybe with some extra assumptions on or ? I am particularly interested in totally disconnected locally compact groups , ultrametric normed vector spaces and bounded functions .","G (X, d) f : G \to X \varepsilon > 0 U g\in G, u \in U d(f(g), f(ug)) < \varepsilon G X f G X f","['abstract-algebra', 'functional-analysis', 'topological-groups', 'uniform-continuity', 'topological-vector-spaces']"
90,Convergence of difference quotient in $L^{p}(\mathbb{R}^{n})$,Convergence of difference quotient in,L^{p}(\mathbb{R}^{n}),"Let $f \in W^{1,p}(\mathbb{R}^{n})$ , where $p \in (1,\infty)$ . Let us define $f^{i}_{h}$ as $$ f^{i}_{h}(x) := \frac{f(x+he_{i}) - f(x)}{|h|}. $$ Prove that $$ ||f^{i}_{h} - D_{i}f||_{L^{p}(\mathbb{R}^{n})} \to 0. $$ I don't have any idea how to prove that. I've just heard about 7.11 from Trudinger's book about elliptic equations, but I don't have any concludes from that lemmas. Maybe it's a simple fact, but I will be really grateful for help, proof or source, where can I find that.","Let , where . Let us define as Prove that I don't have any idea how to prove that. I've just heard about 7.11 from Trudinger's book about elliptic equations, but I don't have any concludes from that lemmas. Maybe it's a simple fact, but I will be really grateful for help, proof or source, where can I find that.","f \in W^{1,p}(\mathbb{R}^{n}) p \in (1,\infty) f^{i}_{h} 
f^{i}_{h}(x) := \frac{f(x+he_{i}) - f(x)}{|h|}.
 
||f^{i}_{h} - D_{i}f||_{L^{p}(\mathbb{R}^{n})} \to 0.
","['functional-analysis', 'sobolev-spaces', 'lp-spaces', 'weak-derivatives']"
91,Show that this set is empty,Show that this set is empty,,"Let $E$ be a $\mathbb R$ -Banach space, $v:E\to[1,\infty)$ be continuous and $v_i:[0,\infty)\to[1,\infty)$ be continuous and nondecreasing with $$v_1(\left\|x\right\|_E)\le v(x)\le v_2(\left\|x\right\|_E)\;\;\;\text{for all }x\in E,\tag1$$ $$v_1(a)\xrightarrow{a\to\infty}\infty\tag2$$ and $$av_2(a)\le C_1v_1^\theta(a)\;\;\;\text{for all }a>0\tag3$$ for some $C_1\ge0$ and $\theta\ge1$ . Now, let $$\rho_r(x,y):=\inf_{\substack{\gamma\:\in\:C^1([0,\:1],\:E)\\ \gamma(0)\:=\:x\\ \gamma(1)\:=\:y}}\int_0^1v^r\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t\;\;\;\text{for }x,y\in E$$ for $r\in(0,1]$ . Let $r\in(0,1]$ , $k,\delta>0$ and $$B:=\left\{(x,y)\in E^2:\rho_1(x,y)<k\text{ and }\rho_r(x,y)\ge\delta\right\}.$$ How can we show that if $v_1^{r-1}(0)k<\delta$ , then $B=\emptyset$ ? I'm unsure how we should approach this task. However, let me note the following: Let $(x,y)\in B$ and $\varepsilon>0$ . By definition of the infimum, there is a $\gamma\in C^1([0,1],E)$ with $\gamma(0)=x$ , $\gamma(1)=y$ and $$\rho_1(x,y)\le\int_0^1v\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t<\rho_1(x,y)+\varepsilon<k+\varepsilon\tag4.$$ In particular, since $v\ge1$ , $$\left\|\gamma(t)-x\right\|_E\le\int_0^t\left\|\gamma'(s)\right\|_E\:{\rm d}s<k+\varepsilon\tag5$$ for all $t\in[0,1]$ . Now, \begin{equation}\begin{split}\rho_r(x,y)&\le\int_0^1v^r(\gamma(s))\left\|\gamma'(s)\right\|_E\:{\rm d}s\\&\le\sup_{B_{k+\varepsilon}(x)}v^{r-1}\int_0^1v(\gamma(s))\left\|\gamma'(s)\right\|_E\:{\rm d}s\\&\le\sup_{B_{k+\varepsilon}(x)}v^{r-1}(k+\varepsilon)\\&=\left(\inf_{B_{k+\varepsilon}(x)}v\right)^{r-1}(k+\varepsilon),\end{split}\tag6\end{equation} but I'm not able to derive a contradiction from these observations. (Can we somehow extend $(6)$ to $\varepsilon=0$ ?) EDIT : Another useful fact seems to be that, since $r-1\in(-1,0]$ , we obtain from $(1)$ that $$v^{r-1}(z)\le v_1^{r-1}(\left\|z\right\|_E)\le v_1^{r-1}(0)\;\;\;\text{for all }z\in E\tag7.$$ EDIT 2 : Please take note of my related question: Bound length of a curve in a ball .","Let be a -Banach space, be continuous and be continuous and nondecreasing with and for some and . Now, let for . Let , and How can we show that if , then ? I'm unsure how we should approach this task. However, let me note the following: Let and . By definition of the infimum, there is a with , and In particular, since , for all . Now, but I'm not able to derive a contradiction from these observations. (Can we somehow extend to ?) EDIT : Another useful fact seems to be that, since , we obtain from that EDIT 2 : Please take note of my related question: Bound length of a curve in a ball .","E \mathbb R v:E\to[1,\infty) v_i:[0,\infty)\to[1,\infty) v_1(\left\|x\right\|_E)\le v(x)\le v_2(\left\|x\right\|_E)\;\;\;\text{for all }x\in E,\tag1 v_1(a)\xrightarrow{a\to\infty}\infty\tag2 av_2(a)\le C_1v_1^\theta(a)\;\;\;\text{for all }a>0\tag3 C_1\ge0 \theta\ge1 \rho_r(x,y):=\inf_{\substack{\gamma\:\in\:C^1([0,\:1],\:E)\\ \gamma(0)\:=\:x\\ \gamma(1)\:=\:y}}\int_0^1v^r\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t\;\;\;\text{for }x,y\in E r\in(0,1] r\in(0,1] k,\delta>0 B:=\left\{(x,y)\in E^2:\rho_1(x,y)<k\text{ and }\rho_r(x,y)\ge\delta\right\}. v_1^{r-1}(0)k<\delta B=\emptyset (x,y)\in B \varepsilon>0 \gamma\in C^1([0,1],E) \gamma(0)=x \gamma(1)=y \rho_1(x,y)\le\int_0^1v\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t<\rho_1(x,y)+\varepsilon<k+\varepsilon\tag4. v\ge1 \left\|\gamma(t)-x\right\|_E\le\int_0^t\left\|\gamma'(s)\right\|_E\:{\rm d}s<k+\varepsilon\tag5 t\in[0,1] \begin{equation}\begin{split}\rho_r(x,y)&\le\int_0^1v^r(\gamma(s))\left\|\gamma'(s)\right\|_E\:{\rm d}s\\&\le\sup_{B_{k+\varepsilon}(x)}v^{r-1}\int_0^1v(\gamma(s))\left\|\gamma'(s)\right\|_E\:{\rm d}s\\&\le\sup_{B_{k+\varepsilon}(x)}v^{r-1}(k+\varepsilon)\\&=\left(\inf_{B_{k+\varepsilon}(x)}v\right)^{r-1}(k+\varepsilon),\end{split}\tag6\end{equation} (6) \varepsilon=0 r-1\in(-1,0] (1) v^{r-1}(z)\le v_1^{r-1}(\left\|z\right\|_E)\le v_1^{r-1}(0)\;\;\;\text{for all }z\in E\tag7.","['functional-analysis', 'differential-geometry', 'continuity', 'metric-spaces', 'curves']"
92,"I can say that : according to the previous lemma we have: $ \| u_k \|_2\leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1.$",I can say that : according to the previous lemma we have:," \| u_k \|_2\leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1.","Let $(E,\mathcal{A},\mu)$ be a finite measure space. Let $ X $ be a Banach space and $ H $ a Hilbert space. For $ t \in E $ , we set $ F_a (f)(t) = f (t) 1_{\| f \| \leq a} (t)$ Lemma :   Let $ \{x_n \} $ be a sequence of $ X $ weakly converging to $ x_\infty \in X $ . Then there is an integer $ N \geq 1 $ such that: $$\|x_\infty\|\leq 2\inf_{n\geq N}\|x_n\|$$ Let $\{f_n\}$ be a bounded sequenece in $\mathcal{L}^1_H(:=\{f:E\to H: f \text{ Bochner-integrable function}\})$ My problem is to show that: there exists a subsequence of $\{f_n^{'}\}$ of $\{f_n\}$ for all $ k\geq 1 $ it exists $ u_k \in \mathcal {L}_H^2 $ such that: $$ F_k(f_n^{'}) \overset {\sigma (\mathcal{L}_H^2, \mathcal{L}_H ^ 2)} {\underset {n} {\longrightarrow}} u_k \qquad \forall k \geq 1 \qquad \text{and} \qquad \| u_k \|_2 \leq 2 \| F_k (f_n^{'}) \|_2, \qquad \forall n \geq k. $$ My effort Let $ k \geq 1 $ , the sequence $ \{F_k (f_n) \} $ is bounded in $ \mathcal {L}_H ^2 $ , and therefore it is relatively weakly compact. There then exists (for each $ k $ ) a subsequence $ \{f_n^k\} $ of $ \{f_n \} $ and $ u_k \in \mathcal{L}_H^2 $ so that $ f_n^{k + 1} $ is a subsequence of $ \{f_n ^ k \} $ and $$ F_k (f_n ^ k) \overset {\sigma (\mathcal{L}_H^2, \mathcal{L}_H^2)} {\underset{n}{\longrightarrow}} u_k, $$ and according to the previous lemma  we have: $$ \| u_k \|_2 \leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1, $$ because $ \{F_k (f_n ^ k) \} $ is a weakly convergent sequence in the Banach space $ (L^2_H, \|. \| _2) $ . We set $ f_n^{'} = f_n^n $ $ (n \geq 1) $ . So $$ F_k (f_n^{'}) \overset {\sigma (\mathcal {L}_H^2, \mathcal {L}_H ^ 2)} {\underset {n} {\longrightarrow}} u_k \qquad \forall k \geq 1 \qquad \text {and} \qquad \| u_k \|_2 \leq 2 \| F_k (f_n ^ {'}) \|_2, \qquad \forall n \geq k. $$ Questions : I can say that : because $ \{F_k (f_n ^ k) \} $ is a weakly convergent sequence in the Banach space $ (L^2_H, \|. \| _2) $ . I can say that : according to the previous lemma  we have: $$ \| u_k \|_2\leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1.$$ An idea please.","Let be a finite measure space. Let be a Banach space and a Hilbert space. For , we set Lemma :   Let be a sequence of weakly converging to . Then there is an integer such that: Let be a bounded sequenece in My problem is to show that: there exists a subsequence of of for all it exists such that: My effort Let , the sequence is bounded in , and therefore it is relatively weakly compact. There then exists (for each ) a subsequence of and so that is a subsequence of and and according to the previous lemma  we have: because is a weakly convergent sequence in the Banach space . We set . So Questions : I can say that : because is a weakly convergent sequence in the Banach space . I can say that : according to the previous lemma  we have: An idea please.","(E,\mathcal{A},\mu)  X   H   t \in E   F_a (f)(t) = f (t) 1_{\| f \| \leq a} (t)  \{x_n \}   X   x_\infty \in X   N \geq 1  \|x_\infty\|\leq 2\inf_{n\geq N}\|x_n\| \{f_n\} \mathcal{L}^1_H(:=\{f:E\to H: f \text{ Bochner-integrable function}\}) \{f_n^{'}\} \{f_n\}  k\geq 1   u_k \in \mathcal {L}_H^2  
F_k(f_n^{'}) \overset {\sigma (\mathcal{L}_H^2, \mathcal{L}_H ^ 2)} {\underset {n} {\longrightarrow}} u_k \qquad \forall k \geq 1 \qquad \text{and} \qquad \| u_k \|_2 \leq 2 \| F_k (f_n^{'}) \|_2, \qquad \forall n \geq k.
  k \geq 1   \{F_k (f_n) \}   \mathcal {L}_H ^2   k   \{f_n^k\}   \{f_n \}   u_k \in \mathcal{L}_H^2   f_n^{k + 1}   \{f_n ^ k \}  
F_k (f_n ^ k) \overset {\sigma (\mathcal{L}_H^2, \mathcal{L}_H^2)} {\underset{n}{\longrightarrow}} u_k,
  \| u_k \|_2 \leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1,   \{F_k (f_n ^ k) \}   (L^2_H, \|. \| _2)   f_n^{'} = f_n^n   (n \geq 1)  
F_k (f_n^{'}) \overset {\sigma (\mathcal {L}_H^2, \mathcal {L}_H ^ 2)} {\underset {n} {\longrightarrow}} u_k \qquad \forall k \geq 1 \qquad \text {and} \qquad \| u_k \|_2 \leq 2 \| F_k (f_n ^ {'}) \|_2, \qquad \forall n \geq k.
  \{F_k (f_n ^ k) \}   (L^2_H, \|. \| _2)   \| u_k \|_2\leq 2 \| F_k (f_n ^ k) \|_2, \qquad \forall n \geq 1.","['functional-analysis', 'probability-theory', 'measure-theory', 'hilbert-spaces']"
93,Hilbert space version of the notion of conditionally weakly-mixing functions.,Hilbert space version of the notion of conditionally weakly-mixing functions.,,"$\newcommand{\norm}[1]{\|#1\|}$ $\newcommand{\ab}[1]{\langle #1\rangle}$ $\newcommand{\mr}{\mathscr}$ $\newcommand{\mc}{\mathcal}$ $\newcommand{\E}{\mathbb E}$ $\newcommand{\C}{\mathbf C}$ Background/Definitions Let $(X, \mc X, \mu, T)$ be an invertible measure preserving system. A function $f\in L^2$ is said to be weakly-mixing if $$ \lim_{N\to \infty} \frac{1}{N} \sum_{n=0}^{N-1} |\ab{f, T^n f}|^2 = 0 $$ This definition can be given for a more abstract setting. Let $H$ be a Hilbert space equipped with a unitary operator $U:H\to H$ . A point $x\in H$ is said to be weakly-mixing if $$ \lim_{N\to \infty} \frac{1}{N} \sum_{n=0}^{N-1} |\ab{x, T^nx}|^2 = 0 $$ Now suppose $\mc D$ and $\mc A$ are $T$ -invariant sub- $\sigma$ -algebras of $\mc X$ with $\mc A\subseteq \mc D$ . Then we have a factor map $(X, \mc D, \mu, T)\to (X, \mc A, \mu, T)$ mapping $x$ to $x$ . Thus $(X, \mc D)$ is an extension of $(X, \mc A)$ . We define a conditional inner product on $L^2(\mc D)$ as $$ \ab{f, g}_{(\mc D|\mc A)} = \E[f\bar g| \mc A] $$ for all $f, g\in L^2(\mc D)$ . Note that the ""inner product"" is valued in $L^1(\mc A)$ and not in $\C$ . Let $L^2(\mc D|\mc A)$ be the subspace of $L^2(\mc D)$ consisting of all $f\in L^2(\mc D)$ such that $$ \norm{f}_{(\mc D|\mc A)} := \sqrt{\E[|f|^2|\mc A]} $$ is in $L^\infty(\mc A)$ . Definition. A function $f\in L^2(\mc D| \mc A)$ is said to be conditionally weakly-mixing if for each $g\in L^2(\mc D|\mc A)$ we have $$ \lim_{N\to \infty} \frac{1}{N}\sum_{n=0}^{N-1} \|\ab{T^nf, g}_{(\mc D|\mc A)}\|_{L^2(\mc A)}^2 = 0 $$ This is a natural conditional version of the non-conditional definition given above. Question Question. Is there a Hilbert space version of definition of conditionally weakly-mixing. The first thing that comes to mind is to consider a Hilbert space $K$ equipped with a unitary operator $U$ , and let $H$ be a $U$ -invariant closed linear subspace of $K$ . Thus $K$ mimics $L^2(\mc D)$ and $H$ mimics $L^2(\mc A)$ . Since the adjoint of the inclusion $L^2(\mc A)\to L^2(\mc D)$ is the conditional expectation map $\E[\cdot|\mc A]:L^2(\mc D)\to L^2(\mc A)$ , we may define the conditional inner product of $x$ and $y$ in $K$ as $\ab{x, y}_{(K|H)} = \ab{i^*x, i^*y}_H$ , where $i:H\to K$ is the inclusion map and $i^*$ is the adjoint of $i$ . However, $\ab{x, y}_{(K|H)}$ is a complex number, unlike in the case of the conditional inner product. So perhaps we need more structure on the Hilbert space to come up with a proper definition.","Background/Definitions Let be an invertible measure preserving system. A function is said to be weakly-mixing if This definition can be given for a more abstract setting. Let be a Hilbert space equipped with a unitary operator . A point is said to be weakly-mixing if Now suppose and are -invariant sub- -algebras of with . Then we have a factor map mapping to . Thus is an extension of . We define a conditional inner product on as for all . Note that the ""inner product"" is valued in and not in . Let be the subspace of consisting of all such that is in . Definition. A function is said to be conditionally weakly-mixing if for each we have This is a natural conditional version of the non-conditional definition given above. Question Question. Is there a Hilbert space version of definition of conditionally weakly-mixing. The first thing that comes to mind is to consider a Hilbert space equipped with a unitary operator , and let be a -invariant closed linear subspace of . Thus mimics and mimics . Since the adjoint of the inclusion is the conditional expectation map , we may define the conditional inner product of and in as , where is the inclusion map and is the adjoint of . However, is a complex number, unlike in the case of the conditional inner product. So perhaps we need more structure on the Hilbert space to come up with a proper definition.","\newcommand{\norm}[1]{\|#1\|} \newcommand{\ab}[1]{\langle #1\rangle} \newcommand{\mr}{\mathscr} \newcommand{\mc}{\mathcal} \newcommand{\E}{\mathbb E} \newcommand{\C}{\mathbf C} (X, \mc X, \mu, T) f\in L^2 
\lim_{N\to \infty} \frac{1}{N} \sum_{n=0}^{N-1} |\ab{f, T^n f}|^2 = 0
 H U:H\to H x\in H 
\lim_{N\to \infty} \frac{1}{N} \sum_{n=0}^{N-1} |\ab{x, T^nx}|^2 = 0
 \mc D \mc A T \sigma \mc X \mc A\subseteq \mc D (X, \mc D, \mu, T)\to (X, \mc A, \mu, T) x x (X, \mc D) (X, \mc A) L^2(\mc D) 
\ab{f, g}_{(\mc D|\mc A)} = \E[f\bar g| \mc A]
 f, g\in L^2(\mc D) L^1(\mc A) \C L^2(\mc D|\mc A) L^2(\mc D) f\in L^2(\mc D) 
\norm{f}_{(\mc D|\mc A)} := \sqrt{\E[|f|^2|\mc A]}
 L^\infty(\mc A) f\in L^2(\mc D| \mc A) g\in L^2(\mc D|\mc A) 
\lim_{N\to \infty} \frac{1}{N}\sum_{n=0}^{N-1} \|\ab{T^nf, g}_{(\mc D|\mc A)}\|_{L^2(\mc A)}^2 = 0
 K U H U K K L^2(\mc D) H L^2(\mc A) L^2(\mc A)\to L^2(\mc D) \E[\cdot|\mc A]:L^2(\mc D)\to L^2(\mc A) x y K \ab{x, y}_{(K|H)} = \ab{i^*x, i^*y}_H i:H\to K i^* i \ab{x, y}_{(K|H)}","['functional-analysis', 'measure-theory', 'ergodic-theory']"
94,Generalization of Kolmogorov precompactness criterion,Generalization of Kolmogorov precompactness criterion,,"In the book Elements in functional analysis from Hirsch and Lacombe, the Kolmogorov precompactness criterion for families of $L^p$ functions is stated as follows: Theorem : Let $H \subseteq L^p(\mathbb{R}^d)$ , with $p \in [1,\infty)$ . Then $H$ is precompact if and only if the following conditions hold: $H$ is bounded w.r.t. the norm $||\cdot||_{L^p(\mathbb{R}^d)}$ ; $\lim_{R \to +\infty} \sup_{f \in H} \int_{B_R^c} |f(x)|^p dx = 0$ , where $B_R^c$ denotes the complement in $\mathbb{R}^d$ of the ball of radius $R$ ; $\lim_{|a| \to 0} \sup_{f \in H} ||\tau_af-f||_{L^p(\mathbb{R}^d)}=0$ , where $\tau_a$ denotes the translation operator . Now, I think that one can modify a little bit this statement in order to make it valid also when we consider families of functions in $L^p(\Omega)$ , where $\Omega$ is an open set in $\mathbb{R}^d$ : Claim : Let $\Omega$ be an open set in $\mathbb{R}^d$ , let $p \in [1,\infty)$ , and let $H \subseteq L^p(\Omega)$ . For all $R > 0$ , define $$\Omega_R = \{x \in \Omega: \text{dist}(x, {\Omega}^c) > \frac{1}{R}\} \cap B(0,R)$$ Then $H$ is precompact if and only id the following holds $H$ is bounded w.r.t. the norm $||\cdot||_{L^p(\Omega)}$ ; $\lim_{R \to +\infty} \sup_{f \in H} \int_{\Omega \setminus \Omega_R} |f(x)|^p dx = 0$ ; $\lim_{|a| \to 0} \sup_{f \in H} ||\tau_af-f||_{L^p(\Omega_R)}=0$ for all $R >0$ . I'm trying to prove this last equivalence using the theorem I've written above. So far this is my strategy: First, notice that we can look at $L^p(\Omega)$ as a subset of $L^p(\mathbb{R}^d)$ extending every function $f$ in $L^p(\Omega)$ to a function $\tilde{f}$ in $L^p(\mathbb{R}^d)$ in this way: $$\tilde{f}(x) =\begin{cases}        f(x) & x \in \Omega \\       0 & \text{otherwise}     \end{cases}$$ Then $H$ is precompact in $L^p(\Omega)$ if and only if it's precompact in $L^p(\mathbb{R}^d)$ . If I prove that conditions 1,2,3 of the claim are equivalent to conditions 1,2,3 of the theorem, I'm done. Condition 1 is easy, but I can't really see how conditions 2 and 3 of the theorem should be equivalent to conditions 2 and 3 of the claim. Any help, remark or suggestion is appreciated, thank you.","In the book Elements in functional analysis from Hirsch and Lacombe, the Kolmogorov precompactness criterion for families of functions is stated as follows: Theorem : Let , with . Then is precompact if and only if the following conditions hold: is bounded w.r.t. the norm ; , where denotes the complement in of the ball of radius ; , where denotes the translation operator . Now, I think that one can modify a little bit this statement in order to make it valid also when we consider families of functions in , where is an open set in : Claim : Let be an open set in , let , and let . For all , define Then is precompact if and only id the following holds is bounded w.r.t. the norm ; ; for all . I'm trying to prove this last equivalence using the theorem I've written above. So far this is my strategy: First, notice that we can look at as a subset of extending every function in to a function in in this way: Then is precompact in if and only if it's precompact in . If I prove that conditions 1,2,3 of the claim are equivalent to conditions 1,2,3 of the theorem, I'm done. Condition 1 is easy, but I can't really see how conditions 2 and 3 of the theorem should be equivalent to conditions 2 and 3 of the claim. Any help, remark or suggestion is appreciated, thank you.","L^p H \subseteq L^p(\mathbb{R}^d) p \in [1,\infty) H H ||\cdot||_{L^p(\mathbb{R}^d)} \lim_{R \to +\infty} \sup_{f \in H} \int_{B_R^c} |f(x)|^p dx = 0 B_R^c \mathbb{R}^d R \lim_{|a| \to 0} \sup_{f \in H} ||\tau_af-f||_{L^p(\mathbb{R}^d)}=0 \tau_a L^p(\Omega) \Omega \mathbb{R}^d \Omega \mathbb{R}^d p \in [1,\infty) H \subseteq L^p(\Omega) R > 0 \Omega_R = \{x \in \Omega: \text{dist}(x, {\Omega}^c) > \frac{1}{R}\} \cap B(0,R) H H ||\cdot||_{L^p(\Omega)} \lim_{R \to +\infty} \sup_{f \in H} \int_{\Omega \setminus \Omega_R} |f(x)|^p dx = 0 \lim_{|a| \to 0} \sup_{f \in H} ||\tau_af-f||_{L^p(\Omega_R)}=0 R >0 L^p(\Omega) L^p(\mathbb{R}^d) f L^p(\Omega) \tilde{f} L^p(\mathbb{R}^d) \tilde{f}(x) =\begin{cases} 
      f(x) & x \in \Omega \\
      0 & \text{otherwise} 
   \end{cases} H L^p(\Omega) L^p(\mathbb{R}^d)","['real-analysis', 'functional-analysis', 'compactness', 'lp-spaces']"
95,Dimension of kernel of Fredholm operator,Dimension of kernel of Fredholm operator,,"Let $X$ be a vector space and let $T\colon X\to X$ be a Fredholm operator. Fix $V$ a finite dimensional subspace of $X$ such that $T(X)+V=X$ . Define $S\colon X\oplus V\to X$ by the formula $S(x,v) = Tx+v$ . It is clear that $S$ is surjective and that $\ker S = \{(x,v) : Tx=-v\}$ . I was told that $S$ is a Fredholm operator and that $\mbox{ind}(S)=\mbox{ind}(T)$ , so of course we must have $$\dim\ker S = \mbox{ind}(T) \ \dot{=} \ \dim\ker T - \dim X/T(X)$$ but I am not able to prove it by calculating the dimension of $\ker S$ . Is there a way to do this? Thanks in advance! PS. You can assume that $X$ is Banach, or even Hilbert, and that $T$ is a bounded Fredholm operator (therefore $T(X)$ is closed).","Let be a vector space and let be a Fredholm operator. Fix a finite dimensional subspace of such that . Define by the formula . It is clear that is surjective and that . I was told that is a Fredholm operator and that , so of course we must have but I am not able to prove it by calculating the dimension of . Is there a way to do this? Thanks in advance! PS. You can assume that is Banach, or even Hilbert, and that is a bounded Fredholm operator (therefore is closed).","X T\colon X\to X V X T(X)+V=X S\colon X\oplus V\to X S(x,v) = Tx+v S \ker S = \{(x,v) : Tx=-v\} S \mbox{ind}(S)=\mbox{ind}(T) \dim\ker S = \mbox{ind}(T) \ \dot{=} \ \dim\ker T - \dim X/T(X) \ker S X T T(X)","['linear-algebra', 'functional-analysis', 'linear-transformations']"
96,How to prove that we can solve limits by substitution?,How to prove that we can solve limits by substitution?,,"I am currently learning analysis and my professor used substitution to solve a lot of limit problems, so I want to know under what circumstances can we use substitution and how to prove it. Example: $\lim_{x\rightarrow 0}\frac{\sin x^2}{x^2}=\lim_{u\rightarrow 0}\frac{\sin u}{u}$ by substitute $u=x^2$ Here is my attempt. My understanding of limit solving by substitution is that $\lim_{x\rightarrow a}u(x)=b\implies\lim_{x\rightarrow a}f(u(x))=\lim_{u\rightarrow b}f(u)$ Proof(probably wrong): Suppose $\lim_{x\rightarrow a}u(x)=b$ and $\lim_{x\rightarrow a}f(u(x))=L$ then $\forall\epsilon\gt 0 \exists\delta_1$ s.t $0\lt|x-a|\lt\delta_1\implies|f(u(x))-L|\lt\epsilon$ then $\forall\delta_1\gt 0 \exists\delta_2$ s.t $0\lt|x-a|\lt\delta_2\implies|u(x)-b|\lt\delta_1$ then fix $\delta=\min(\delta_1,\delta_2)$ we have $\forall\epsilon\gt 0 \exists\delta$ s.t $0\lt|x-a|\lt\delta$ implies $|f(u(x))-L|\lt\epsilon$ and $|u(x)-b|\lt\delta_1$ Since $P\wedge Q\implies(P\implies Q)$ we have $\lim_{u\rightarrow b}f(u)=L$ and do the same thing for the reverse case then the statement is proved.","I am currently learning analysis and my professor used substitution to solve a lot of limit problems, so I want to know under what circumstances can we use substitution and how to prove it. Example: by substitute Here is my attempt. My understanding of limit solving by substitution is that Proof(probably wrong): Suppose and then s.t then s.t then fix we have s.t implies and Since we have and do the same thing for the reverse case then the statement is proved.","\lim_{x\rightarrow 0}\frac{\sin x^2}{x^2}=\lim_{u\rightarrow 0}\frac{\sin u}{u} u=x^2 \lim_{x\rightarrow a}u(x)=b\implies\lim_{x\rightarrow a}f(u(x))=\lim_{u\rightarrow b}f(u) \lim_{x\rightarrow a}u(x)=b \lim_{x\rightarrow a}f(u(x))=L \forall\epsilon\gt 0 \exists\delta_1 0\lt|x-a|\lt\delta_1\implies|f(u(x))-L|\lt\epsilon \forall\delta_1\gt 0 \exists\delta_2 0\lt|x-a|\lt\delta_2\implies|u(x)-b|\lt\delta_1 \delta=\min(\delta_1,\delta_2) \forall\epsilon\gt 0 \exists\delta 0\lt|x-a|\lt\delta |f(u(x))-L|\lt\epsilon |u(x)-b|\lt\delta_1 P\wedge Q\implies(P\implies Q) \lim_{u\rightarrow b}f(u)=L","['real-analysis', 'functional-analysis', 'limits', 'analysis']"
97,Why is this $L^1$-sequence relatively weakly sequentially compact?,Why is this -sequence relatively weakly sequentially compact?,L^1,"Let $(E,\mathcal E,m)$ be a probability space, $\theta$ be a measurable map on $(E,\mathcal E)$ with $m\circ\theta^{-1}=m$ , $s_n$ be a real-valued nonpositive integrable random variable on $(E,\mathcal E,m)$ for $n\in\mathbb N$ with $\lambda:=\inf_{n\in\mathbb N}\int s_n\:{\rm d}m>-\infty$ and $$\varphi_n:=\frac1n\sum_{i=1}^n(s_i-s_{i-1}\circ\theta)\;\;\;\text{for }n\in\mathbb N.$$ It's easy to see that $$\int\varphi_n\:{\rm d}m=\frac{\int s_n\:{\rm d}m}n\xrightarrow{n\to\infty}\lambda\tag1.$$ Moreover, $$Tf:=f\circ\theta\;\;\;\text{for }f\in\mathcal L^1(m)$$ is a linear isometry on $L^p(m)$ for all $p\in[1,\infty]$ . In particular, it is continuous with respect to the weak topology on $L^1(m)$ . How can we show that, for all $i\in\mathbb N_0$ and $p\in\mathbb N$ , there is an increasing $(n_k)_{k\in\mathbb N}\subseteq\mathbb N$ with $$\max\left(\varphi_{n_k}\circ\theta^i,-p\right)\xrightarrow{k\to\infty}\lambda_{i,\:p}\tag3$$ with respect to the weak topology on $L^1(m)$ for some $\lambda_{i,\:o}\in L^1(m)$ ? And how can we show that $\lambda_{i,\:p}$ is nondecreasing in $p$ almost surely? These claims are made in the proof of Theorem 6.7 (in Chapter 4, Paragraph 6) of Revuz' Markov Chains book: I don't understand his arguments. For example, I guess he's talking about relative sequential compactness (I'm not sure, but may it be that this is equivalent to relative compactness in the weak topology?). It seems like he's using that $(\varphi_n)_{n\in\mathbb N}$ is contained in a relatively sequentially compact set; but why is that the case? And why do the $\lambda_{i,\:p}$ need to satisfy the claimed monotonicity in $p$ ? Or does he mean that they can be chosen such that they satisfy this condition?","Let be a probability space, be a measurable map on with , be a real-valued nonpositive integrable random variable on for with and It's easy to see that Moreover, is a linear isometry on for all . In particular, it is continuous with respect to the weak topology on . How can we show that, for all and , there is an increasing with with respect to the weak topology on for some ? And how can we show that is nondecreasing in almost surely? These claims are made in the proof of Theorem 6.7 (in Chapter 4, Paragraph 6) of Revuz' Markov Chains book: I don't understand his arguments. For example, I guess he's talking about relative sequential compactness (I'm not sure, but may it be that this is equivalent to relative compactness in the weak topology?). It seems like he's using that is contained in a relatively sequentially compact set; but why is that the case? And why do the need to satisfy the claimed monotonicity in ? Or does he mean that they can be chosen such that they satisfy this condition?","(E,\mathcal E,m) \theta (E,\mathcal E) m\circ\theta^{-1}=m s_n (E,\mathcal E,m) n\in\mathbb N \lambda:=\inf_{n\in\mathbb N}\int s_n\:{\rm d}m>-\infty \varphi_n:=\frac1n\sum_{i=1}^n(s_i-s_{i-1}\circ\theta)\;\;\;\text{for }n\in\mathbb N. \int\varphi_n\:{\rm d}m=\frac{\int s_n\:{\rm d}m}n\xrightarrow{n\to\infty}\lambda\tag1. Tf:=f\circ\theta\;\;\;\text{for }f\in\mathcal L^1(m) L^p(m) p\in[1,\infty] L^1(m) i\in\mathbb N_0 p\in\mathbb N (n_k)_{k\in\mathbb N}\subseteq\mathbb N \max\left(\varphi_{n_k}\circ\theta^i,-p\right)\xrightarrow{k\to\infty}\lambda_{i,\:p}\tag3 L^1(m) \lambda_{i,\:o}\in L^1(m) \lambda_{i,\:p} p (\varphi_n)_{n\in\mathbb N} \lambda_{i,\:p} p","['functional-analysis', 'probability-theory', 'measure-theory', 'weak-convergence', 'ergodic-theory']"
98,Prove that $F\in L^1(\mathbb{R})$,Prove that,F\in L^1(\mathbb{R}),"Let $f\in L^1(\mathbb{R})$ and continuous on $\mathbb{R}$ such that its Fourier transform $\hat f$ equals zero in a neighborhood of zero. Let $F$ be function such that $\hat F$ exists and $$\hat f(x) =x\hat F(x),\quad \forall x\in \mathbb{R}$$ Prove that $F\in L^1(\mathbb{R})$ . Any hints on how to prove that? Thanks!",Let and continuous on such that its Fourier transform equals zero in a neighborhood of zero. Let be function such that exists and Prove that . Any hints on how to prove that? Thanks!,"f\in L^1(\mathbb{R}) \mathbb{R} \hat f F \hat F \hat f(x) =x\hat F(x),\quad \forall x\in \mathbb{R} F\in L^1(\mathbb{R})","['functional-analysis', 'lebesgue-integral', 'fourier-transform']"
99,Normal cone to the union of sets,Normal cone to the union of sets,,"For a set $A \subseteq \Bbb R^n$ and a point $\bar{x} \in A,$ the limiting (Mordukhovich) normal cone of $A$ at $\bar{x}$ is defined as $$N(\bar{x},A):= \limsup_{x\to \bar{x}} \widehat{N}(x,A),$$ where $$\widehat{N}(x, A):= \left\{u \in \Bbb R^n \mid \limsup_{x'\to x,\; x'\in A}\frac{u^\top (x'-x)}{\|x'-x\|}\leq 0\right\} $$ is the so-called Frechet normal cone and the limit is understood in the sense of Painleve-Kuratowski. Furthermore, for a functional $f: \Bbb R^n \to \Bbb R$ , the limiting subdifferential of $f$ at $\bar{x}$ is defined as $$\partial f(\bar{x}):= {u \in \Bbb R^n \mid (u,-1) \in N((\bar{x},f(\bar{x}), epi f) },$$ where $epi f$ is the epigraph of $f.$ All of these definitions and more are from the book ""Variational Analysis and Generalized Differentiation"" by Boris Mordukhovich. Now, consider two closed sets $A,B \subseteq \Bbb R^n$ and a point $\bar{x} \in A\cap B.$ My question is: does the inclusion $$N(\bar{x}, A\cup B) \subseteq N(\bar{x}, A) \cup N(\bar{x},B)$$ holds? If so, does there exists a reference for the result or a tighter upper bound? This question seems to be very natural, given the normal cone intersection formula, see Theorem 3.4 in Mordukhovich's book. However, I am not able to find such a result in the literature, and I need to cite it. My attempt at a proof is the following: Consider $\delta_A$ and $\delta_B,$ the indicator functions of $A$ and $B$ respectively. Then, it is easy to verify that $\delta_{A\cup B}= \min\{\delta_A, \delta_B\}.$ By Proposition 1.79 in Mordukhovich's book, we have $\partial \delta_A(\bar{x}) = N(\bar{x},A).$ Therefore, $$N(\bar{x}, A\cup B) = \partial (\min\{\delta_A, \delta_B\})(\bar{x}).$$ Moreover, by Proposition 1.113 (Mordukhovich's book) we have $$\partial (\min\{\delta_A, \delta_B\})(\bar{x}) \subseteq \partial \delta_A(\bar{x}) \cup \partial \delta_B(\bar{x}) =  N(\bar{x}, A) \cup N(\bar{x},B),$$ and hence the upper bound follows. Is this correct? A direct reference would be more beneficial in any case.","For a set and a point the limiting (Mordukhovich) normal cone of at is defined as where is the so-called Frechet normal cone and the limit is understood in the sense of Painleve-Kuratowski. Furthermore, for a functional , the limiting subdifferential of at is defined as where is the epigraph of All of these definitions and more are from the book ""Variational Analysis and Generalized Differentiation"" by Boris Mordukhovich. Now, consider two closed sets and a point My question is: does the inclusion holds? If so, does there exists a reference for the result or a tighter upper bound? This question seems to be very natural, given the normal cone intersection formula, see Theorem 3.4 in Mordukhovich's book. However, I am not able to find such a result in the literature, and I need to cite it. My attempt at a proof is the following: Consider and the indicator functions of and respectively. Then, it is easy to verify that By Proposition 1.79 in Mordukhovich's book, we have Therefore, Moreover, by Proposition 1.113 (Mordukhovich's book) we have and hence the upper bound follows. Is this correct? A direct reference would be more beneficial in any case.","A \subseteq \Bbb R^n \bar{x} \in A, A \bar{x} N(\bar{x},A):= \limsup_{x\to \bar{x}} \widehat{N}(x,A), \widehat{N}(x, A):= \left\{u \in \Bbb R^n \mid \limsup_{x'\to x,\; x'\in A}\frac{u^\top (x'-x)}{\|x'-x\|}\leq 0\right\}  f: \Bbb R^n \to \Bbb R f \bar{x} \partial f(\bar{x}):= {u \in \Bbb R^n \mid (u,-1) \in N((\bar{x},f(\bar{x}), epi f) }, epi f f. A,B \subseteq \Bbb R^n \bar{x} \in A\cap B. N(\bar{x}, A\cup B) \subseteq N(\bar{x}, A) \cup N(\bar{x},B) \delta_A \delta_B, A B \delta_{A\cup B}= \min\{\delta_A, \delta_B\}. \partial \delta_A(\bar{x}) = N(\bar{x},A). N(\bar{x}, A\cup B) = \partial (\min\{\delta_A, \delta_B\})(\bar{x}). \partial (\min\{\delta_A, \delta_B\})(\bar{x}) \subseteq \partial \delta_A(\bar{x}) \cup \partial \delta_B(\bar{x}) =  N(\bar{x}, A) \cup N(\bar{x},B),","['functional-analysis', 'convex-analysis', 'nonlinear-optimization', 'non-convex-optimization', 'variational-analysis']"
