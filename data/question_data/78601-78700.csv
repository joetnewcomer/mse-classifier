,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Rotate polygon around center and get the coordinates,Rotate polygon around center and get the coordinates,,"First off I am a computer programmer, so excuse my lack of math understanding. Also I know this question has been asked before, but the answers don't seem to apply to this specific situation. Given a polygon, which is made up of points. For example: [{""x"":301.1848472789287,""y"":216.523742955658},{""x"":299.92410285162424,""y"":241.37037128550003},{""x"":296.227787218953,""y"":264.523742955658},{""x"":290.347798182831,""y"":284.40599394956655},{""x"":282.68484727892877,""y"":299.6621817189641},{""x"":273.761151947722,""y"":309.25262227940857},{""x"":264.18484727892877,""y"":312.523742955658},{""x"":254.60854261013552,""y"":309.25262227940857},{""x"":245.6848472789288,""y"":299.6621817189641},{""x"":238.02189637502653,""y"":284.40599394956655},{""x"":232.14190733890456,""y"":264.523742955658},{""x"":228.44559170623327,""y"":241.37037128550003},{""x"":227.1848472789288,""y"":216.52374295565804},{""x"":228.44559170623327,""y"":191.67711462581605},{""x"":232.14190733890456,""y"":168.52374295565807},{""x"":238.02189637502653,""y"":148.64149196174952},{""x"":245.68484727892877,""y"":133.38530419235195},{""x"":254.60854261013552,""y"":123.79486363190748},{""x"":264.18484727892877,""y"":120.52374295565804},{""x"":273.761151947722,""y"":123.79486363190746},{""x"":282.68484727892877,""y"":133.38530419235192},{""x"":290.347798182831,""y"":148.64149196174947},{""x"":296.2277872189529,""y"":168.52374295565798},{""x"":299.92410285162424,""y"":191.67711462581596}] Where each coordinate is in the global coordinate system (top left is 0,0 and right and down is positive). I want to figure out where each point would be if I were to rotate the polygon around it's center (is that called centroid?). These polygons may be whatever a user decides to draw on the screen, so they might be concave or convex or anything. In the array of coordinates above, it is a circle as a bunch of points. So basically instead of rotating the shape and keeping the points local, I want to move the points in the global coordinate system, and figure out where they should be when rotated around a point. I know in the past I used matrix math, but I am just not sure how to do it. I've been at this for weeks now. I believe it's something like moving the shape to 0,0 and rotating it to 0 and doing the rotation matrix on the shape and moving it back but it does not seem to work for me. Edit: I know the center of the bounding box of the polygon.","First off I am a computer programmer, so excuse my lack of math understanding. Also I know this question has been asked before, but the answers don't seem to apply to this specific situation. Given a polygon, which is made up of points. For example: [{""x"":301.1848472789287,""y"":216.523742955658},{""x"":299.92410285162424,""y"":241.37037128550003},{""x"":296.227787218953,""y"":264.523742955658},{""x"":290.347798182831,""y"":284.40599394956655},{""x"":282.68484727892877,""y"":299.6621817189641},{""x"":273.761151947722,""y"":309.25262227940857},{""x"":264.18484727892877,""y"":312.523742955658},{""x"":254.60854261013552,""y"":309.25262227940857},{""x"":245.6848472789288,""y"":299.6621817189641},{""x"":238.02189637502653,""y"":284.40599394956655},{""x"":232.14190733890456,""y"":264.523742955658},{""x"":228.44559170623327,""y"":241.37037128550003},{""x"":227.1848472789288,""y"":216.52374295565804},{""x"":228.44559170623327,""y"":191.67711462581605},{""x"":232.14190733890456,""y"":168.52374295565807},{""x"":238.02189637502653,""y"":148.64149196174952},{""x"":245.68484727892877,""y"":133.38530419235195},{""x"":254.60854261013552,""y"":123.79486363190748},{""x"":264.18484727892877,""y"":120.52374295565804},{""x"":273.761151947722,""y"":123.79486363190746},{""x"":282.68484727892877,""y"":133.38530419235192},{""x"":290.347798182831,""y"":148.64149196174947},{""x"":296.2277872189529,""y"":168.52374295565798},{""x"":299.92410285162424,""y"":191.67711462581596}] Where each coordinate is in the global coordinate system (top left is 0,0 and right and down is positive). I want to figure out where each point would be if I were to rotate the polygon around it's center (is that called centroid?). These polygons may be whatever a user decides to draw on the screen, so they might be concave or convex or anything. In the array of coordinates above, it is a circle as a bunch of points. So basically instead of rotating the shape and keeping the points local, I want to move the points in the global coordinate system, and figure out where they should be when rotated around a point. I know in the past I used matrix math, but I am just not sure how to do it. I've been at this for weeks now. I believe it's something like moving the shape to 0,0 and rotating it to 0 and doing the rotation matrix on the shape and moving it back but it does not seem to work for me. Edit: I know the center of the bounding box of the polygon.",,"['matrices', 'geometry', 'trigonometry', 'rotations']"
1,Prove that the determinant is $(a-b)(b-c)(c-a)(a^2 + b^2 + c^2 )$,Prove that the determinant is,(a-b)(b-c)(c-a)(a^2 + b^2 + c^2 ),"Prove that   $$         \begin{vmatrix}         1 & a^2 + bc  & a^3 \\         1 & b^2 + ac & b^3 \\         1 & c^2 + ab & c^3 \\         \end{vmatrix}  =(a-b)(b-c)(c-a)(a^2 + b^2 + c^2 )$$ My work I am getting - sign extra , please tell me why I am getting this","Prove that   $$         \begin{vmatrix}         1 & a^2 + bc  & a^3 \\         1 & b^2 + ac & b^3 \\         1 & c^2 + ab & c^3 \\         \end{vmatrix}  =(a-b)(b-c)(c-a)(a^2 + b^2 + c^2 )$$ My work I am getting - sign extra , please tell me why I am getting this",,"['matrices', 'determinant']"
2,Rank of an Augmented matrix,Rank of an Augmented matrix,,"Can the rank of coeffecient matrix be greater than augmented matrix ? Also,what is the condition for an inconsistent set of linear equations?","Can the rank of coeffecient matrix be greater than augmented matrix ? Also,what is the condition for an inconsistent set of linear equations?",,"['matrices', 'matrix-equations', 'matrix-calculus', 'matrix-rank']"
3,"If A is a $m\times n$ matrix, and $\text{rank}(A)=1$, then $A^2 =\lambda A$ [duplicate]","If A is a  matrix, and , then  [duplicate]",m\times n \text{rank}(A)=1 A^2 =\lambda A,"This question already has answers here : If $A$ is rank-$1$, show that $A^2=cA$ for some scalar $c$ (4 answers) Closed 8 years ago . If $A$ is $m \times n$ matrix with rank$(A)= 1$. How do we show that   $$A^2 =\lambda A$$  for some $\lambda$? How do we show this? All I could show is that if rank$(A)=1$ then rank$(A^2)=1.$ Am I heading in the right direction?","This question already has answers here : If $A$ is rank-$1$, show that $A^2=cA$ for some scalar $c$ (4 answers) Closed 8 years ago . If $A$ is $m \times n$ matrix with rank$(A)= 1$. How do we show that   $$A^2 =\lambda A$$  for some $\lambda$? How do we show this? All I could show is that if rank$(A)=1$ then rank$(A^2)=1.$ Am I heading in the right direction?",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-rank']"
4,How to calculate the determinant of this matrix?,How to calculate the determinant of this matrix?,,"The matrix is $\mathbf{A}=\bigl[a_{ij}\bigr]_{1\leqslant i,j\leqslant n}$ and is defined as follows: $$a_{ij}= \begin{cases} i\; \mbox{if } i = j,\\ n\; \mbox{otherwise.} \end{cases} $$ or $$\mathbf{A}= \begin{bmatrix} 1 & n & \ldots & n\\ n & 2 & \ldots & n\\ \vdots & \vdots & \ddots & \vdots\\ n & n & \ldots & n\\ \end{bmatrix}$$ I tried one by one, for $n=1,2,3,$ and $4$ and I found a formula like: $$\det(\mathbf{A})=(-1)^{n+1}\cdot n!.$$ I could not prove it by induction.","The matrix is $\mathbf{A}=\bigl[a_{ij}\bigr]_{1\leqslant i,j\leqslant n}$ and is defined as follows: $$a_{ij}= \begin{cases} i\; \mbox{if } i = j,\\ n\; \mbox{otherwise.} \end{cases} $$ or $$\mathbf{A}= \begin{bmatrix} 1 & n & \ldots & n\\ n & 2 & \ldots & n\\ \vdots & \vdots & \ddots & \vdots\\ n & n & \ldots & n\\ \end{bmatrix}$$ I tried one by one, for $n=1,2,3,$ and $4$ and I found a formula like: $$\det(\mathbf{A})=(-1)^{n+1}\cdot n!.$$ I could not prove it by induction.",,"['matrices', 'determinant']"
5,Is this a homomorphism? Matrix homomorphism $\phi:\Bbb R\to GL_2(\Bbb R)$,Is this a homomorphism? Matrix homomorphism,\phi:\Bbb R\to GL_2(\Bbb R),"My book claims that the following is a homomorphism: $$\phi:\Bbb R\to GL_2(\Bbb R),\phi(a)=\begin{bmatrix}1&0\\a&1\end{bmatrix}$$ But it doesn't seem to be: $$\phi(ab)=\begin{bmatrix}1&0\\ab&1\end{bmatrix}$$ $$\phi(a)\phi(b)=\begin{bmatrix}1&0\\a&1\end{bmatrix}\begin{bmatrix}1&0\\b&1\end{bmatrix}=\begin{bmatrix}1&0\\a+b&1\end{bmatrix}$$ Right? What am I missing?","My book claims that the following is a homomorphism: $$\phi:\Bbb R\to GL_2(\Bbb R),\phi(a)=\begin{bmatrix}1&0\\a&1\end{bmatrix}$$ But it doesn't seem to be: $$\phi(ab)=\begin{bmatrix}1&0\\ab&1\end{bmatrix}$$ $$\phi(a)\phi(b)=\begin{bmatrix}1&0\\a&1\end{bmatrix}\begin{bmatrix}1&0\\b&1\end{bmatrix}=\begin{bmatrix}1&0\\a+b&1\end{bmatrix}$$ Right? What am I missing?",,"['abstract-algebra', 'matrices', 'group-homomorphism']"
6,Prove that a matrix and its inverse are over the same field,Prove that a matrix and its inverse are over the same field,,"Let $A$ be some matrix over $\mathbb{Q}$ (then it's also over   $\mathbb{R}$). Suppose $A$ is invertible over $\mathbb{R}$ (that is,   $A^{-1}$ is over $\mathbb{R}$). Prove that $A^{-1}$ is also over   $\mathbb{Q}$. I know that I have to prove that $A^{-1}$ contains no irrational numbers but I fail to do so. I would appreciate any suggestions.","Let $A$ be some matrix over $\mathbb{Q}$ (then it's also over   $\mathbb{R}$). Suppose $A$ is invertible over $\mathbb{R}$ (that is,   $A^{-1}$ is over $\mathbb{R}$). Prove that $A^{-1}$ is also over   $\mathbb{Q}$. I know that I have to prove that $A^{-1}$ contains no irrational numbers but I fail to do so. I would appreciate any suggestions.",,"['linear-algebra', 'matrices']"
7,How to encode matrices uniquely,How to encode matrices uniquely,,"Given a square matrix $A=[a_{ij}]_{n \times n}$, an operation $swap(A, i, j)$ is defined to swap row $i$ and $j$ of $A$ and do the same thing with the corresponding columns. For example, in the following example, we can see the $swap(A, 1, 3)$: $$A=\left[ \begin{array}{ccc} a & b & c \\ d & e & f \\ g & h & i \end{array} \right] ~~~~~\Rightarrow~~~~~ swap(A,1,3)=\left[ \begin{array}{ccc} i & h & g \\ f & e & d \\ c & b & a \end{array} \right]$$ For a matrix $A$, let $swap^k(A)$ be the set of matrices produced by exactly $k$ swaps.  For example $B \in swap^3(A)$ means that there is a sequence of 3 swaps that can convert $A$ into $B$. A unique encoding $\phi$ of this matrix is a function of $A$ such that: $$\phi(A)=\phi(B) ~~~~ \Leftrightarrow ~~~~ \exists k ~:~ B \in swap^k(A)$$ For simplicity, we can assume that the matrix is symetric and its elements are in $\{0,1\}$. I am interested to know whether there exist such encoding for this case of matrices or if there is a proof for its non-existence. Edit. What is the most efficient way to do this?","Given a square matrix $A=[a_{ij}]_{n \times n}$, an operation $swap(A, i, j)$ is defined to swap row $i$ and $j$ of $A$ and do the same thing with the corresponding columns. For example, in the following example, we can see the $swap(A, 1, 3)$: $$A=\left[ \begin{array}{ccc} a & b & c \\ d & e & f \\ g & h & i \end{array} \right] ~~~~~\Rightarrow~~~~~ swap(A,1,3)=\left[ \begin{array}{ccc} i & h & g \\ f & e & d \\ c & b & a \end{array} \right]$$ For a matrix $A$, let $swap^k(A)$ be the set of matrices produced by exactly $k$ swaps.  For example $B \in swap^3(A)$ means that there is a sequence of 3 swaps that can convert $A$ into $B$. A unique encoding $\phi$ of this matrix is a function of $A$ such that: $$\phi(A)=\phi(B) ~~~~ \Leftrightarrow ~~~~ \exists k ~:~ B \in swap^k(A)$$ For simplicity, we can assume that the matrix is symetric and its elements are in $\{0,1\}$. I am interested to know whether there exist such encoding for this case of matrices or if there is a proof for its non-existence. Edit. What is the most efficient way to do this?",,"['linear-algebra', 'matrices', 'computer-science']"
8,Determinant of identity minus product of orthogonal matrix and rank-$1$ matrix,Determinant of identity minus product of orthogonal matrix and rank- matrix,1,"I am interested in calculating, or bounding in some way, the following determinant  \begin{equation} \det\left[\mathcal{I}-Rxx^t\right] \end{equation} Here, $Rxx^t$ is clearly a singular matrix. Im the case that I am particularily interested in, $R$ is an orthogonal matrix with determinant $+1$. I understand that there are ways of doing this using characteristic polynomials and such, but im not educated in those things. Any help greatly appreciated! NOTE: I do not know $R$, it can be any special orthogonal matrix. I started my derivation with the `matrix determinant lemma' and am not looking to return to this step, but rather to attack the above determinant","I am interested in calculating, or bounding in some way, the following determinant  \begin{equation} \det\left[\mathcal{I}-Rxx^t\right] \end{equation} Here, $Rxx^t$ is clearly a singular matrix. Im the case that I am particularily interested in, $R$ is an orthogonal matrix with determinant $+1$. I understand that there are ways of doing this using characteristic polynomials and such, but im not educated in those things. Any help greatly appreciated! NOTE: I do not know $R$, it can be any special orthogonal matrix. I started my derivation with the `matrix determinant lemma' and am not looking to return to this step, but rather to attack the above determinant",,"['linear-algebra', 'matrices', 'determinant', 'orthogonal-matrices']"
9,Prove that $\det(xI_m-AB)=x^{m-n}\det(xI_n-BA)$,Prove that,\det(xI_m-AB)=x^{m-n}\det(xI_n-BA),"I want to prove that $$\det \left( x I_m - A B \right) = x^{m-n} \det \left( x I_n - B A \right)$$ where $A \in \mathbb{F}^{m \times n}$ and $B \in \mathbb{F}^{n\times m}$ . It is easy to show that $0$ has algebraic multiplicity of at least $m-n$ for $AB$ , but how can I show that the other eigenvalues of $AB$ are actually eigenvalues of $BA$ ? I know that the sum of eigenvalues of $AB$ and $BA$ are the same, because $\operatorname{trace}(AB)=\operatorname{trace}(BA)$ , but ... I appreciate any help, thanks ! ${{{}}}$","I want to prove that where and . It is easy to show that has algebraic multiplicity of at least for , but how can I show that the other eigenvalues of are actually eigenvalues of ? I know that the sum of eigenvalues of and are the same, because , but ... I appreciate any help, thanks !",\det \left( x I_m - A B \right) = x^{m-n} \det \left( x I_n - B A \right) A \in \mathbb{F}^{m \times n} B \in \mathbb{F}^{n\times m} 0 m-n AB AB BA AB BA \operatorname{trace}(AB)=\operatorname{trace}(BA) {{{}}},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
10,"Find rank of the matrix $a_{ij}=(i-j)^2$, $i,j=1,\dots, n$","Find rank of the matrix ,","a_{ij}=(i-j)^2 i,j=1,\dots, n",Let is $A$ $n\times n$ matrix defined in following way $a_{ij} = (i-j)^2$. For example when $n=4$ $$ A= \begin{pmatrix} 0&1&4&9\\ 1&0&1&4\\ 4&1&0&1\\ 9&4&1&0 \end{pmatrix} $$ Find $\operatorname{rank}(A)$. It seems we can make this matrix upper triangular and I suspect that $\operatorname{rank}(A)=n$.,Let is $A$ $n\times n$ matrix defined in following way $a_{ij} = (i-j)^2$. For example when $n=4$ $$ A= \begin{pmatrix} 0&1&4&9\\ 1&0&1&4\\ 4&1&0&1\\ 9&4&1&0 \end{pmatrix} $$ Find $\operatorname{rank}(A)$. It seems we can make this matrix upper triangular and I suspect that $\operatorname{rank}(A)=n$.,,"['linear-algebra', 'matrices']"
11,rank($A$)=rank($A^T$) [duplicate],rank()=rank() [duplicate],A A^T,"This question already has answers here : Is the rank of a matrix the same of its transpose? If yes, how can I prove it? (5 answers) Closed 10 years ago . Is there an elementary explanation of why the row-rank of a matrix equals its column-rank (without using adjoint maps, resp. lots of technical computations)? What is the geometric intuition behind this?","This question already has answers here : Is the rank of a matrix the same of its transpose? If yes, how can I prove it? (5 answers) Closed 10 years ago . Is there an elementary explanation of why the row-rank of a matrix equals its column-rank (without using adjoint maps, resp. lots of technical computations)? What is the geometric intuition behind this?",,"['linear-algebra', 'matrices', 'geometry', 'intuition', 'matrix-rank']"
12,Is the square root of a symmetric positive definite matrix also symmetric?,Is the square root of a symmetric positive definite matrix also symmetric?,,"The inverse of a SPD matrix is also symmetric. But what about the square root? Intuitively, I would say yes. But I'm not sure about it.","The inverse of a SPD matrix is also symmetric. But what about the square root? Intuitively, I would say yes. But I'm not sure about it.",,['matrices']
13,Find the axis of rotation from the rotation matrix.,Find the axis of rotation from the rotation matrix.,,"This is a problem from the book ""Mathematical Methods in the Physical Sciences"" Third Edition by author Mary L. Boas. on page 129, Example 5, just in case any of you are familiar with it. So I actually know what the correct answers are. Its how to get the answers is what I so desperately need to know. $$G=\ \begin{pmatrix} 0 & 0 & 1\\  0 & -1 & 0\\  1 & 0 & 0 \end{pmatrix}$$ $$K=\ \begin{pmatrix} 0 & 0 & 1\\  -1 & 0 & 0\\  0 & -1 & 0 \end{pmatrix}$$ Find the axis of rotation for the rotation matrices $G$ and $K$. I know that many of you can do this by ""inspection"". But I don't understand what that is or how it works. The book tells me I can solve the equations $Gr=r$ & $Kr=r$ to get the axis of rotations since $r$ is some vector unchanged by the transformation. I haven't reached the Eigenvector section of the book yet so if someone would kindly show me all the working for solving these equations I would be most grateful. As according to Mary Boas I don't need to know about Eigenvectors to solve this problem. Many thanks to any response, regards, BLAZE","This is a problem from the book ""Mathematical Methods in the Physical Sciences"" Third Edition by author Mary L. Boas. on page 129, Example 5, just in case any of you are familiar with it. So I actually know what the correct answers are. Its how to get the answers is what I so desperately need to know. $$G=\ \begin{pmatrix} 0 & 0 & 1\\  0 & -1 & 0\\  1 & 0 & 0 \end{pmatrix}$$ $$K=\ \begin{pmatrix} 0 & 0 & 1\\  -1 & 0 & 0\\  0 & -1 & 0 \end{pmatrix}$$ Find the axis of rotation for the rotation matrices $G$ and $K$. I know that many of you can do this by ""inspection"". But I don't understand what that is or how it works. The book tells me I can solve the equations $Gr=r$ & $Kr=r$ to get the axis of rotations since $r$ is some vector unchanged by the transformation. I haven't reached the Eigenvector section of the book yet so if someone would kindly show me all the working for solving these equations I would be most grateful. As according to Mary Boas I don't need to know about Eigenvectors to solve this problem. Many thanks to any response, regards, BLAZE",,"['linear-algebra', 'matrices', 'rotations', 'vectors', 'block-matrices']"
14,"Show that $e^{t(A+B)} = e^{tA}e^{tB}$ for all $t \in \mathbb{R}$ if, and only if $AB = BA$.","Show that  for all  if, and only if .",e^{t(A+B)} = e^{tA}e^{tB} t \in \mathbb{R} AB = BA,"Let A,B real or complex matrixes. Show that $e^{t(A+B)} = e^{tA}e^{tB}$ for all $t \in \mathbb{R}$ if, and only if $AB = BA$. I demonstrated the reciprocal: $\Leftarrow )$ The two equations are solutions of $X' = (A+B)X$, $X(0)= I$, because $(e^{tA}e^{tB})' = Ae^{tA}e^{tB} + e^{tA}Be^{tB} = Ae^{tA}e^{tB} + Be^{tA}e^{tB} = (A+B)e^{tA}e^{tB}$   and $(e^t(A+B))' = (A+B)e^{t(A+B)}$. Thus, $e^{t(A+B)} = e^{tA}e^{tB}$. I have problems to demonstrate the implication.","Let A,B real or complex matrixes. Show that $e^{t(A+B)} = e^{tA}e^{tB}$ for all $t \in \mathbb{R}$ if, and only if $AB = BA$. I demonstrated the reciprocal: $\Leftarrow )$ The two equations are solutions of $X' = (A+B)X$, $X(0)= I$, because $(e^{tA}e^{tB})' = Ae^{tA}e^{tB} + e^{tA}Be^{tB} = Ae^{tA}e^{tB} + Be^{tA}e^{tB} = (A+B)e^{tA}e^{tB}$   and $(e^t(A+B))' = (A+B)e^{t(A+B)}$. Thus, $e^{t(A+B)} = e^{tA}e^{tB}$. I have problems to demonstrate the implication.",,"['matrices', 'ordinary-differential-equations', 'exponential-function']"
15,Matrix Exponential using the Cayley-Hamilton theorem,Matrix Exponential using the Cayley-Hamilton theorem,,"For the matrix $$P=\left( \begin{matrix} 0&1&0 \\ 0&0&1 \\ 0&1&0 \end{matrix} \right)$$ how do you find $e^{Pt}$ using the Cayley-Hamilton theorem? I have found it by diagonalising $P$, but the question states to use the Cayley-Hamilton theorem.","For the matrix $$P=\left( \begin{matrix} 0&1&0 \\ 0&0&1 \\ 0&1&0 \end{matrix} \right)$$ how do you find $e^{Pt}$ using the Cayley-Hamilton theorem? I have found it by diagonalising $P$, but the question states to use the Cayley-Hamilton theorem.",,"['linear-algebra', 'matrices', 'matrix-calculus', 'matrix-exponential', 'cayley-hamilton']"
16,finding bases for row space and null space of matrix.,finding bases for row space and null space of matrix.,,"My problem is: For the matrix $$A = \begin{bmatrix}       1&  4&  5&  6&  9\\       3& −2&  1&  4& −1\\      −1&  0& −1& −2& −1\\       2&  3&  5&  7&  8\end{bmatrix}$$ (a) Find a basis for the row space of A. (b) Find a basis for the null space of A. (c) Find the rank and nullity of A. I tried searching online and I became more confused, take the example here. http://www2.kenyon.edu/Depts/Math/Paquin/PracticeExam1Solns.pdf As you can see for the column space he takes the columns of the original matrix instead of the rref of A, which I don't understand.","My problem is: For the matrix $$A = \begin{bmatrix}       1&  4&  5&  6&  9\\       3& −2&  1&  4& −1\\      −1&  0& −1& −2& −1\\       2&  3&  5&  7&  8\end{bmatrix}$$ (a) Find a basis for the row space of A. (b) Find a basis for the null space of A. (c) Find the rank and nullity of A. I tried searching online and I became more confused, take the example here. http://www2.kenyon.edu/Depts/Math/Paquin/PracticeExam1Solns.pdf As you can see for the column space he takes the columns of the original matrix instead of the rref of A, which I don't understand.",,"['linear-algebra', 'matrices']"
17,determinant of the linear transformation $T(X) =\frac{1}{2} (AX+XA)$,determinant of the linear transformation,T(X) =\frac{1}{2} (AX+XA),"Let $V$ vector space of all matrices $3\times3$, and let $A$ be the diagonal matrix : $$ \begin{pmatrix} 1 &  0 & 0\\ 0  &  2& 0 \\ 0  &  0& 1\end{pmatrix} $$ Compute thee determinant of the linear transformation $T(X) =\frac{1}{2} (AX+XA)$. Any hints would be appreciated.","Let $V$ vector space of all matrices $3\times3$, and let $A$ be the diagonal matrix : $$ \begin{pmatrix} 1 &  0 & 0\\ 0  &  2& 0 \\ 0  &  0& 1\end{pmatrix} $$ Compute thee determinant of the linear transformation $T(X) =\frac{1}{2} (AX+XA)$. Any hints would be appreciated.",,"['linear-algebra', 'matrices', 'vector-spaces', 'determinant', 'linear-transformations']"
18,Solving matrix equation $AX = B$,Solving matrix equation,AX = B,"I want to solve the matrix equation $AX = B$, where the matrix $A$ and $B$ are given as follows $A =    \begin{bmatrix}      0.1375 &   0.0737  &  0.1380  & 0.1169 &  0.1166 \\     0.0926  &   0.0707   &  0.0957   &  0.0873  &   0.0733 \\     0.0767   &   0.0642  &   0.0810    &  0.0766  &    0.0599 \\     0.1593 &    0.1020 &    0.1636 &    0.1451   &  0.1317   \end{bmatrix}$ $B =    \begin{bmatrix}      0.2794   &   0.0065  &    0.2271    &  0.1265   &   0.2773\\     0.1676  &  0.2365  &  0.1430  &  0.1015 &   0.0632 \\      0.0645  &   0.2274 &    0.1009 &    0.1806 &    0.0503\\     0.2326  &  0.1261  &  0.2867 &   0.2846  &  0.1979   \end{bmatrix}$ Could anybody help me how to solve this problem? I need help with this. Thanks for the help.","I want to solve the matrix equation $AX = B$, where the matrix $A$ and $B$ are given as follows $A =    \begin{bmatrix}      0.1375 &   0.0737  &  0.1380  & 0.1169 &  0.1166 \\     0.0926  &   0.0707   &  0.0957   &  0.0873  &   0.0733 \\     0.0767   &   0.0642  &   0.0810    &  0.0766  &    0.0599 \\     0.1593 &    0.1020 &    0.1636 &    0.1451   &  0.1317   \end{bmatrix}$ $B =    \begin{bmatrix}      0.2794   &   0.0065  &    0.2271    &  0.1265   &   0.2773\\     0.1676  &  0.2365  &  0.1430  &  0.1015 &   0.0632 \\      0.0645  &   0.2274 &    0.1009 &    0.1806 &    0.0503\\     0.2326  &  0.1261  &  0.2867 &   0.2846  &  0.1979   \end{bmatrix}$ Could anybody help me how to solve this problem? I need help with this. Thanks for the help.",,"['linear-algebra', 'matrices', 'matrix-equations']"
19,Diagonalization and eigenvalues [duplicate],Diagonalization and eigenvalues [duplicate],,This question already has an answer here : a linear algebra multiple choice problem (1 answer) Closed 6 years ago . Let $A$ $\in M_3(\mathbb R)$ which is not a diagonal matrix. Pick out the cases when $A$ is diagonalizable over $\mathbb R$: a. when $A^2 = A$; b. when $(A - 3I)^2 = 0$; c. when $A^2 + I = 0$. I could eliminate c. by using the equation $\lambda^2+1$ and showing if a matrix has to satisfy this then it has to be diagonal. I am also in doubt whether all the eigen values of $A$ satisfy this or not.,This question already has an answer here : a linear algebra multiple choice problem (1 answer) Closed 6 years ago . Let $A$ $\in M_3(\mathbb R)$ which is not a diagonal matrix. Pick out the cases when $A$ is diagonalizable over $\mathbb R$: a. when $A^2 = A$; b. when $(A - 3I)^2 = 0$; c. when $A^2 + I = 0$. I could eliminate c. by using the equation $\lambda^2+1$ and showing if a matrix has to satisfy this then it has to be diagonal. I am also in doubt whether all the eigen values of $A$ satisfy this or not.,,"['linear-algebra', 'matrices']"
20,Why are the inverses of triangular matrices always triangular?,Why are the inverses of triangular matrices always triangular?,,The inverse of an (invertible) upper triangular matrix is always upper triangular and the inverse of an (invertible) lower triangular matrix is always lower triangular. Why? I don't have any work to show and this isn't homework.,The inverse of an (invertible) upper triangular matrix is always upper triangular and the inverse of an (invertible) lower triangular matrix is always lower triangular. Why? I don't have any work to show and this isn't homework.,,"['linear-algebra', 'matrices']"
21,Prove: symmetric positive matrix multiplied by skew symmetric matrix equals 0,Prove: symmetric positive matrix multiplied by skew symmetric matrix equals 0,,My teacher gave me this task as preparation for the exam but I'm stuck and not sure if it's true anymore.,My teacher gave me this task as preparation for the exam but I'm stuck and not sure if it's true anymore.,,"['linear-algebra', 'matrices']"
22,"If $A$ is a diagonalizable $n\times n$ matrix for which the eigenvalues are $0$ and $1$, then $A^2=A$.","If  is a diagonalizable  matrix for which the eigenvalues are  and , then .",A n\times n 0 1 A^2=A,"If $A$ is a diagonalizable $n\times n$ matrix for which the eigenvalues are $0$ and $1$, then $A^2=A$. I know how to prove this in the opposite direction, however I can't seem to find a way prove this. Could anyone please help?","If $A$ is a diagonalizable $n\times n$ matrix for which the eigenvalues are $0$ and $1$, then $A^2=A$. I know how to prove this in the opposite direction, however I can't seem to find a way prove this. Could anyone please help?",,"['linear-algebra', 'matrices']"
23,"Prove that if A is an invertible matrix, then A*A is Hermitian and positive definite.","Prove that if A is an invertible matrix, then A*A is Hermitian and positive definite.",,"If I'm not mistaken, if a matrix $M$ has its conjugate $M^*=M$, then $M$ is Hermitian. In this case then, am I asked to show that $(A^*A)^*=A^*A$? What does it have to do with $A$ being invertible though? And positive definite? How do I show that it's positive definite? Thanks!","If I'm not mistaken, if a matrix $M$ has its conjugate $M^*=M$, then $M$ is Hermitian. In this case then, am I asked to show that $(A^*A)^*=A^*A$? What does it have to do with $A$ being invertible though? And positive definite? How do I show that it's positive definite? Thanks!",,"['linear-algebra', 'matrices', 'inverse']"
24,Does this kind of matrix have a name?,Does this kind of matrix have a name?,,"Are these kind of matrices generally known in mathematics?  Do they have a name? $$   \left[\begin{array}{rrr}     A & B \\     B & A \\   \end{array}\right] $$ $$   \left[\begin{array}{rrr}     A & B & C \\     C & A & B \\     B & C & A \\   \end{array}\right] $$ $$   \left[\begin{array}{rrr}     A & B & C & D \\     D & A & B & C \\     C & D & A & B \\     B & C & D & A \\   \end{array}\right] $$ $$   \left[\begin{array}{rrr}     A & B & C & D & E \\     E & A & B & C & D \\     D & E & A & B & C \\     C & D & E & A & B \\     B & C & D & E & A \\   \end{array}\right] $$ The main thing is that each letter will be in the same columnn/row just once. I'm trying to do some combination calculations with big matrices following this pattern, so knowing effective ways to generate and compute these would help. (The pattern here is that the next row is made by shifting the previous row to one right.)","Are these kind of matrices generally known in mathematics?  Do they have a name? $$   \left[\begin{array}{rrr}     A & B \\     B & A \\   \end{array}\right] $$ $$   \left[\begin{array}{rrr}     A & B & C \\     C & A & B \\     B & C & A \\   \end{array}\right] $$ $$   \left[\begin{array}{rrr}     A & B & C & D \\     D & A & B & C \\     C & D & A & B \\     B & C & D & A \\   \end{array}\right] $$ $$   \left[\begin{array}{rrr}     A & B & C & D & E \\     E & A & B & C & D \\     D & E & A & B & C \\     C & D & E & A & B \\     B & C & D & E & A \\   \end{array}\right] $$ The main thing is that each letter will be in the same columnn/row just once. I'm trying to do some combination calculations with big matrices following this pattern, so knowing effective ways to generate and compute these would help. (The pattern here is that the next row is made by shifting the previous row to one right.)",,"['matrices', 'terminology']"
25,"Is there a standard operation to ""rotate rings on matrices""?","Is there a standard operation to ""rotate rings on matrices""?",,"Is there some standard operation to ""rotate rings on matrices""? Look at the image below: The numbers around the four empty squares are what I'm calling ring , In the second matrix, this ring has been rotated counterclockwise. I'm aware that ring may not be the right name.","Is there some standard operation to ""rotate rings on matrices""? Look at the image below: The numbers around the four empty squares are what I'm calling ring , In the second matrix, this ring has been rotated counterclockwise. I'm aware that ring may not be the right name.",,"['matrices', 'reference-request']"
26,How to multiply a matrix of block matrices?,How to multiply a matrix of block matrices?,,"I came across a question on SE Math and was reading a proof on Sylvester's Determinant Theorem posted by anon. But I have a few doubts as I'm reading it. I will re-produce from the pdf file on the part that I don't understand so that it makes it easier for reference. In Theorem 9, it says if matrix $A$ and $D$ are invertible, then in the block matrix below, $$ \begin{vmatrix} A_{m \times m} & B_{m \times n}\\  C_{n \times m} & D_{n \times n} \end{vmatrix} = \left | A \right |\left | D-CA^{-1}B \right |  = \left | D \right |\left | A-BD^{-1}C \right | $$ The proof given to the claim was: $$\begin{align*} \begin{bmatrix} A_{m \times m} & B_{m \times n}\\  C_{n \times m} & D_{n \times n} \end{bmatrix} &= \begin{bmatrix} A_{m \times m} & 0_{m \times n}\\  C_{n \times m} & I_{n \times n} \end{bmatrix}  \begin{bmatrix} I_{m \times m} & A^{-1}B_{m \times n}\\  C_{n \times m} & D-CA^{-1}B_{n \times n} \end{bmatrix}\\ &=  \begin{bmatrix} I_{m \times m} & B_{m \times n}\\  0_{n \times m} & D_{n \times n} \end{bmatrix}  \begin{bmatrix} A-BD^{-1}C_{m \times m} & 0_{m \times n}\\  D^{-1}C_{n \times m} & I_{n \times n} \end{bmatrix} \end{align*}$$ First, I don't understand how $\begin{bmatrix} A_{m \times m} & 0_{m \times n}\\ C_{n \times m} & I_{n \times n} \end{bmatrix} \begin{bmatrix} I_{m \times m} & A^{-1}B_{m \times n}\\ C_{n \times m} & D-CA^{-1}B_{n \times n} \end{bmatrix}$ was derived from $\begin{bmatrix} A_{m \times m} & B_{m \times n}\\ C_{n \times m} & D_{n \times n} \end{bmatrix}$. Second, when I did a multiplication between the matrices,  $$ \begin{align*} &\begin{bmatrix} A_{m \times m} & 0_{m \times n}\\ C_{n \times m} & I_{n \times n} \end{bmatrix} \begin{bmatrix} I_{m \times m} & A^{-1}B_{m \times n}\\ C_{n \times m} & D-CA^{-1}B_{n \times n} \end{bmatrix} =\\ &\begin{bmatrix} A_{m \times m}I_{m \times m}+0_{m \times n}C_{n \times m} & A_{m \times m}A^{-1}B_{m \times n}+0_{m \times n}(D-CA^{-1}B_{m \times n})\\ C_{n \times m}I_{m \times m}+I_{m \times m}C_{n \times m} &  C_{n \times m}A^{-1}B_{m \times n}+I_{n \times n}(D-CA^{-1}B_{m \times n}) \end{bmatrix}=\\ &\begin{bmatrix} A_{m \times m} & B_{m \times n}\\\ 2C_{n \times m} & D_{n \times n} \end{bmatrix}\neq \begin{bmatrix} A_{m \times m} & B_{m \times n}\\  C_{n \times m} & D_{n \times n} \end{bmatrix} \end{align*}$$ It's weird that in my multiplication doesn't get back the original matrix. How was the first equation being derived by sort of ""splitting"" the matrix into 2 matrices? Also, why doesn't my multiplication get back the original matrix? Thanks for any help.","I came across a question on SE Math and was reading a proof on Sylvester's Determinant Theorem posted by anon. But I have a few doubts as I'm reading it. I will re-produce from the pdf file on the part that I don't understand so that it makes it easier for reference. In Theorem 9, it says if matrix $A$ and $D$ are invertible, then in the block matrix below, $$ \begin{vmatrix} A_{m \times m} & B_{m \times n}\\  C_{n \times m} & D_{n \times n} \end{vmatrix} = \left | A \right |\left | D-CA^{-1}B \right |  = \left | D \right |\left | A-BD^{-1}C \right | $$ The proof given to the claim was: $$\begin{align*} \begin{bmatrix} A_{m \times m} & B_{m \times n}\\  C_{n \times m} & D_{n \times n} \end{bmatrix} &= \begin{bmatrix} A_{m \times m} & 0_{m \times n}\\  C_{n \times m} & I_{n \times n} \end{bmatrix}  \begin{bmatrix} I_{m \times m} & A^{-1}B_{m \times n}\\  C_{n \times m} & D-CA^{-1}B_{n \times n} \end{bmatrix}\\ &=  \begin{bmatrix} I_{m \times m} & B_{m \times n}\\  0_{n \times m} & D_{n \times n} \end{bmatrix}  \begin{bmatrix} A-BD^{-1}C_{m \times m} & 0_{m \times n}\\  D^{-1}C_{n \times m} & I_{n \times n} \end{bmatrix} \end{align*}$$ First, I don't understand how $\begin{bmatrix} A_{m \times m} & 0_{m \times n}\\ C_{n \times m} & I_{n \times n} \end{bmatrix} \begin{bmatrix} I_{m \times m} & A^{-1}B_{m \times n}\\ C_{n \times m} & D-CA^{-1}B_{n \times n} \end{bmatrix}$ was derived from $\begin{bmatrix} A_{m \times m} & B_{m \times n}\\ C_{n \times m} & D_{n \times n} \end{bmatrix}$. Second, when I did a multiplication between the matrices,  $$ \begin{align*} &\begin{bmatrix} A_{m \times m} & 0_{m \times n}\\ C_{n \times m} & I_{n \times n} \end{bmatrix} \begin{bmatrix} I_{m \times m} & A^{-1}B_{m \times n}\\ C_{n \times m} & D-CA^{-1}B_{n \times n} \end{bmatrix} =\\ &\begin{bmatrix} A_{m \times m}I_{m \times m}+0_{m \times n}C_{n \times m} & A_{m \times m}A^{-1}B_{m \times n}+0_{m \times n}(D-CA^{-1}B_{m \times n})\\ C_{n \times m}I_{m \times m}+I_{m \times m}C_{n \times m} &  C_{n \times m}A^{-1}B_{m \times n}+I_{n \times n}(D-CA^{-1}B_{m \times n}) \end{bmatrix}=\\ &\begin{bmatrix} A_{m \times m} & B_{m \times n}\\\ 2C_{n \times m} & D_{n \times n} \end{bmatrix}\neq \begin{bmatrix} A_{m \times m} & B_{m \times n}\\  C_{n \times m} & D_{n \times n} \end{bmatrix} \end{align*}$$ It's weird that in my multiplication doesn't get back the original matrix. How was the first equation being derived by sort of ""splitting"" the matrix into 2 matrices? Also, why doesn't my multiplication get back the original matrix? Thanks for any help.",,"['linear-algebra', 'matrices', 'block-matrices']"
27,Whether this matrix is positive definite,Whether this matrix is positive definite,,"Let $A$ be a nonsingular real square matrix. Is it true that the matrix  $$\frac{1}{2}(A+A')-2(A^{-1}+(A^{-1})')^{-1}$$  is positive semidefinite? Here, $A'$ denotes the transpose of $A$. Edited Let $A,B$ be positive definite matrices of the same size, is it true that $\frac{1}{2}(AB+BA)−2((AB)^{−1}+(BA)^{−1})^{−1}$ is positive semidefinite?","Let $A$ be a nonsingular real square matrix. Is it true that the matrix  $$\frac{1}{2}(A+A')-2(A^{-1}+(A^{-1})')^{-1}$$  is positive semidefinite? Here, $A'$ denotes the transpose of $A$. Edited Let $A,B$ be positive definite matrices of the same size, is it true that $\frac{1}{2}(AB+BA)−2((AB)^{−1}+(BA)^{−1})^{−1}$ is positive semidefinite?",,['matrices']
28,Positive symmetric matrices and positive-definiteness,Positive symmetric matrices and positive-definiteness,,Is a symmetric real matrix with diagonal entries strictly greater than $1$ and off-diagonal entries positive but strictly less than $1$ necessarily positive-semidefinite?,Is a symmetric real matrix with diagonal entries strictly greater than $1$ and off-diagonal entries positive but strictly less than $1$ necessarily positive-semidefinite?,,"['linear-algebra', 'matrices', 'optimization', 'quadratic-forms']"
29,Balancing an acid-base chemical reaction,Balancing an acid-base chemical reaction,,"I tried balancing this chemical equation $$\mathrm{Al(OH)_3  + H_2SO_4 \to Al_2(SO_4)_3 + H_2O}$$ with a system of equations , but the answer doesn't seem to map well. I get a negative coefficient which is prohibitive in this equations. How do I interpret the answer?","I tried balancing this chemical equation $$\mathrm{Al(OH)_3  + H_2SO_4 \to Al_2(SO_4)_3 + H_2O}$$ with a system of equations , but the answer doesn't seem to map well. I get a negative coefficient which is prohibitive in this equations. How do I interpret the answer?",,"['matrices', 'chemistry']"
30,Help computing eigenvalues of matrix $M = (m_{ij})$ where $m_{ij} = r^{|i-j|} - r^{2(n+1) - (i+j)}$,Help computing eigenvalues of matrix  where,M = (m_{ij}) m_{ij} = r^{|i-j|} - r^{2(n+1) - (i+j)},"Let $M$ be the $n \times n$ real symmetric matrix with entries $m_{ij}$ given by $$ m_{ij} =  r^{|i-j|} - r^{2(n+1) - (i+j)},  $$ for some $r \in [0, 1]$ . Numerically, it can be verified that this matrix is positive definite, however I struggled to show that this is the case by hand. I am mainly interested in computing the eigenvalues of this matrix. I know that the trace of this matrix has the form $$ n - \frac{1 - r^{2(n+1)}}{1 - r^2}, $$ but so far this is really the most information I could figure out about this matrix. This matrix arises in the study of Markov chains.","Let be the real symmetric matrix with entries given by for some . Numerically, it can be verified that this matrix is positive definite, however I struggled to show that this is the case by hand. I am mainly interested in computing the eigenvalues of this matrix. I know that the trace of this matrix has the form but so far this is really the most information I could figure out about this matrix. This matrix arises in the study of Markov chains.","M n \times n m_{ij} 
m_{ij} =  r^{|i-j|} - r^{2(n+1) - (i+j)}, 
 r \in [0, 1] 
n - \frac{1 - r^{2(n+1)}}{1 - r^2},
","['matrices', 'eigenvalues-eigenvectors']"
31,Simplifying the determinant of a matrix.,Simplifying the determinant of a matrix.,,"Suppose $$A = \begin{pmatrix} 1+a_{1}+a_{1}b_{1}+b_{2} & 1+a_{1} & 1 & 0\\ a_{2}+a_{2}b_{1}+b_{3} & 1+a_{2} & 1 & 1\\ a_{3}+a_{3}b_{1} + b_{4} & a_{3} & 1 & 1\\ a_{4} + a_{4}b_{1} & a_{4} & 0 & 1\end{pmatrix}$$ Show that $$\det(A) = \det\begin{pmatrix} b_{1}-b_{2}+b_{4} & 1+a_{1}-a_{3}+a_{4}\\ -1-b_{2}+b_{3} & a_{1}-a_{2}+a_{4}\end{pmatrix}$$ I am not sure how to show this. I tried to perform some row and column operations but could simplify matrix $A$ . Note that for a general matrix of size $(3k+1)\times (3k+1)$ , the determinant of $A$ can be written in a similar form. Say for a $7 \times 7$ matrix, $$A_{7 \times 7} = \begin{pmatrix} 1+a_1+a_1b_1+b_2 & 1+a_1 & 1& 0 &0 &0 &0\\ a_2 + a_2b_1 + b_3 & 1+a_2 & 1 & 1&0&0&0\\ a_3+a_3b_1+b_4 & a_3 & 1 & 1 & 1& 0 &0\\ a_4+a_4b_1+b_5 & a_4 & 0 & 1 & 1&1 &0\\ a_5+a_5b_1+b_6 & a_5 & 0 & 0 & 1&1 &1\\ a_6+a_6b_1+b_7 & a_6 & 0 & 0 & 0&1 &1\\ a_7 + a_7b_1 & a_7 & 0 & 0 & 0&0 &1 \end{pmatrix}$$ $$\det(A) = \det\begin{pmatrix} b_{1}-b_{2}+b_{4}-b_{5}+b_{7} & 1+a_{1}-a_{3}+a_{4}-a_{6}+a_{7}\\ -1-b_{2}+b_{3}-b_{5}+b_{6} & a_{1}-a_{2}+a_{4}-a_{5}+a_{7}\end{pmatrix}$$ Any thoughts on how to show this in general?","Suppose Show that I am not sure how to show this. I tried to perform some row and column operations but could simplify matrix . Note that for a general matrix of size , the determinant of can be written in a similar form. Say for a matrix, Any thoughts on how to show this in general?","A = \begin{pmatrix} 1+a_{1}+a_{1}b_{1}+b_{2} & 1+a_{1} & 1 & 0\\ a_{2}+a_{2}b_{1}+b_{3} & 1+a_{2} & 1 & 1\\ a_{3}+a_{3}b_{1} + b_{4} & a_{3} & 1 & 1\\ a_{4} + a_{4}b_{1} & a_{4} & 0 & 1\end{pmatrix} \det(A) = \det\begin{pmatrix} b_{1}-b_{2}+b_{4} & 1+a_{1}-a_{3}+a_{4}\\ -1-b_{2}+b_{3} & a_{1}-a_{2}+a_{4}\end{pmatrix} A (3k+1)\times (3k+1) A 7 \times 7 A_{7 \times 7} = \begin{pmatrix} 1+a_1+a_1b_1+b_2 & 1+a_1 & 1& 0 &0 &0 &0\\ a_2 + a_2b_1 + b_3 & 1+a_2 & 1 & 1&0&0&0\\ a_3+a_3b_1+b_4 & a_3 & 1 & 1 & 1& 0 &0\\ a_4+a_4b_1+b_5 & a_4 & 0 & 1 & 1&1 &0\\ a_5+a_5b_1+b_6 & a_5 & 0 & 0 & 1&1 &1\\ a_6+a_6b_1+b_7 & a_6 & 0 & 0 & 0&1 &1\\ a_7 + a_7b_1 & a_7 & 0 & 0 & 0&0 &1
\end{pmatrix} \det(A) = \det\begin{pmatrix} b_{1}-b_{2}+b_{4}-b_{5}+b_{7} & 1+a_{1}-a_{3}+a_{4}-a_{6}+a_{7}\\ -1-b_{2}+b_{3}-b_{5}+b_{6} & a_{1}-a_{2}+a_{4}-a_{5}+a_{7}\end{pmatrix}","['linear-algebra', 'matrices', 'determinant']"
32,Is there a 12x12 symmetric Hadamard matrix? [closed],Is there a 12x12 symmetric Hadamard matrix? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question More generally, is there a simple condition for which $n$ there are symmetric Hadamard matrices of order $n$ ? This set of $n$ is closed under multiplication via the Kronecker product.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question More generally, is there a simple condition for which there are symmetric Hadamard matrices of order ? This set of is closed under multiplication via the Kronecker product.",n n n,"['matrices', 'hadamard-matrices']"
33,What is the solution to this vector differential equation?,What is the solution to this vector differential equation?,,"What is the solution to the matrix differential equation: $$ \frac{dx}{dt} = \boldsymbol{A}x(t) + \boldsymbol{B}u$$ Where, $ x(0) = x_0 $ , $A_{n \times n}$ is a square matrix and $B_{n\times1}$ is $n\times1$ matrix Assuming u is constant for the time interval $[0, T]$ Also please refer me to a resource where I can learn to solve this types of equations, because solving the equations were not covered in my linear algebra course.","What is the solution to the matrix differential equation: Where, , is a square matrix and is matrix Assuming u is constant for the time interval Also please refer me to a resource where I can learn to solve this types of equations, because solving the equations were not covered in my linear algebra course."," \frac{dx}{dt} = \boldsymbol{A}x(t) + \boldsymbol{B}u  x(0) = x_0  A_{n \times n} B_{n\times1} n\times1 [0, T]","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'matrix-calculus']"
34,What about the rank of this matrix?,What about the rank of this matrix?,,Problem: Suppose $0<r<n$ . Suppose $A:\mathbb{R} \to \mathcal{M}_2(\mathbb{R})$ is $\mathcal{C}^1$ . Suppose that he rank of $A(t)$ il less or equal $r$ for all $t \in \mathbb{R}$ . What be said about the rank of $A'(0)$ ? Attempt: I tried using the fact that $rank(A)=k$ if and only $k$ is the maximum size of an invertible minor $B$ . I would like to say that $rank(A'(0)) \leq r$ but I am not sure if it is true. Of course $rank(A'(0))$ could be $r$ because we can define $A(t)$ as the diagonal matrix with the first $r$ elements equal to $t$ and anywhere else equal to $0$ .,Problem: Suppose . Suppose is . Suppose that he rank of il less or equal for all . What be said about the rank of ? Attempt: I tried using the fact that if and only is the maximum size of an invertible minor . I would like to say that but I am not sure if it is true. Of course could be because we can define as the diagonal matrix with the first elements equal to and anywhere else equal to .,0<r<n A:\mathbb{R} \to \mathcal{M}_2(\mathbb{R}) \mathcal{C}^1 A(t) r t \in \mathbb{R} A'(0) rank(A)=k k B rank(A'(0)) \leq r rank(A'(0)) r A(t) r t 0,"['linear-algebra', 'matrices', 'matrix-rank']"
35,Is true that $\det{\Big(A^T\cdot B\cdot A\Big)}=\det\Big(A^T\cdot A\Big)\det B$ when $B$ is a symmetric matrix?,Is true that  when  is a symmetric matrix?,\det{\Big(A^T\cdot B\cdot A\Big)}=\det\Big(A^T\cdot A\Big)\det B B,"If $B$ is a square symmetric matrix of order $n\times n$ then is true that $$ \det{\Big(A^T\cdot B\cdot A\Big)}=\det\Big(A^T\cdot A\Big)\det B $$ where $A$ is a matrix of order $n\times m$ ? Unfortunately I did not find a counterexample: in particular I tried to show that $$ A^T\cdot B\cdot A=A\cdot A^T\cdot B $$ so that the statement follows directely applying the Binet formula. So could someone help me, please?","If is a square symmetric matrix of order then is true that where is a matrix of order ? Unfortunately I did not find a counterexample: in particular I tried to show that so that the statement follows directely applying the Binet formula. So could someone help me, please?","B n\times n 
\det{\Big(A^T\cdot B\cdot A\Big)}=\det\Big(A^T\cdot A\Big)\det B
 A n\times m 
A^T\cdot B\cdot A=A\cdot A^T\cdot B
","['linear-algebra', 'matrices', 'solution-verification', 'determinant', 'symmetric-matrices']"
36,Proof or disprove: There exists a real $n \times n$ matrix $A$ that satisfies the equation when $n$ is even. [closed],Proof or disprove: There exists a real  matrix  that satisfies the equation when  is even. [closed],n \times n A n,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Prove or disprove: There exists a real $n \times n$ matrix $A$ with $$ A^2+2\cdot A+5\cdot I_n = 0 $$ if and only if n is even. I could not find a counterexample for an odd $n$ . Therefore, I suspect that the statement is true, but I have not yet found a solution.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Prove or disprove: There exists a real matrix with if and only if n is even. I could not find a counterexample for an odd . Therefore, I suspect that the statement is true, but I have not yet found a solution.","n \times n A 
A^2+2\cdot A+5\cdot I_n = 0
 n","['linear-algebra', 'matrices', 'matrix-equations']"
37,"If $A<B$, does it follow that $A^2\leq B^2$?","If , does it follow that ?",A<B A^2\leq B^2,"Suppose $A$ and $B$ are positive semidefinite matrices satisfying $A\leq B$ (meaning that $x^TAx \leq x^TBx$ for any vector $x$ ).  Does it follow that $A^2\leq B^2$ ?  It certainly follows if $A$ and $B$ commute, but beyond this case I am not so sure -- I suspect there is a counterexample.","Suppose and are positive semidefinite matrices satisfying (meaning that for any vector ).  Does it follow that ?  It certainly follows if and commute, but beyond this case I am not so sure -- I suspect there is a counterexample.",A B A\leq B x^TAx \leq x^TBx x A^2\leq B^2 A B,"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices', 'positive-semidefinite']"
38,A problem on symmetric matrix,A problem on symmetric matrix,,"Let $A\in \operatorname{Mat}_n(\mathbb R)$ be a symmetric real matrix. Prove that there exists a diagonal matrix $D \in  \operatorname{Mat}_n (\mathbb R)$ whose entries are chosen from $\{1,-1\}$ satisfying $$\det(A+D)\neq 0$$ It appears in a problem set of linear algebra... I have thought about diagonalizing $A$ and calculating the determinant, but a congruent of $D$ is hard to handle. So I want to reduce the problem to the special case: $D$ is of the form $\operatorname{diag}(-I_s,I_{n-s})$ ( $0\leq s \leq n )$ . But I could not achieve it. It seems hard to commute diagonal entries of $D$ ... Could you share some ideas? Thank you!","Let be a symmetric real matrix. Prove that there exists a diagonal matrix whose entries are chosen from satisfying It appears in a problem set of linear algebra... I have thought about diagonalizing and calculating the determinant, but a congruent of is hard to handle. So I want to reduce the problem to the special case: is of the form ( . But I could not achieve it. It seems hard to commute diagonal entries of ... Could you share some ideas? Thank you!","A\in \operatorname{Mat}_n(\mathbb R) D \in  \operatorname{Mat}_n (\mathbb R) \{1,-1\} \det(A+D)\neq 0 A D D \operatorname{diag}(-I_s,I_{n-s}) 0\leq s \leq n ) D","['linear-algebra', 'matrices', 'determinant', 'symmetric-matrices']"
39,largest singular value of real symmetric matrix,largest singular value of real symmetric matrix,,"I am trying to find a proof of the following fact: Let $M$ be a real symmetric matrix, then the largest singular value satisfies: $$ \sigma_1(M) = \lim_{k \to \infty} \left[\text{Trace}(M^{2k})\right]^{\frac{1}{2k}} $$","I am trying to find a proof of the following fact: Let be a real symmetric matrix, then the largest singular value satisfies:","M 
\sigma_1(M) = \lim_{k \to \infty} \left[\text{Trace}(M^{2k})\right]^{\frac{1}{2k}}
","['linear-algebra', 'matrices', 'reference-request', 'trace', 'singular-values']"
40,How to solve the matrix equations: $\frac{1}{2}(\Omega Q - Q\Omega)=F$ for $\Omega$,How to solve the matrix equations:  for,\frac{1}{2}(\Omega Q - Q\Omega)=F \Omega,"How to solve the following matrix equation: $$\frac{1}{2}(\Omega Q + Q\Omega^T)=F$$ $Q$ is of rank $1$ , $\text{tr}(Q)=1$ , $Q\succeq0$ (positive semidefinite).  So we can werite $Q = qq^T$ for some $q$ with $\|q\|=1$ . $\Omega$ is skew-symmetric, i.e., $\Omega^{T} = -\Omega$ . So diagonal entries are all $0$ . $\Omega$ , $Q$ , $F$ are $4\times 4$ square matrices. $\Omega$ is the variable I want to solve. I cannot find any method to solve it. Can anyone suggest me how to solve $\Omega$ in terms of $Q$ and $F$ ? Even though we cannot obtain the closed form, any approximation or trick can we use? Thanks! Note: $\Omega$ should look like $$\begin{bmatrix}0 & \omega_3 & -\omega_2 & \omega_1 \\   -\omega_3 & 0 & \omega_1 & \omega_2 \\    \omega_2 & -\omega_1 & 0 & \omega_3 \\    -\omega_1 & -\omega_2 & -\omega_3 & 0 \end{bmatrix}$$","How to solve the following matrix equation: is of rank , , (positive semidefinite).  So we can werite for some with . is skew-symmetric, i.e., . So diagonal entries are all . , , are square matrices. is the variable I want to solve. I cannot find any method to solve it. Can anyone suggest me how to solve in terms of and ? Even though we cannot obtain the closed form, any approximation or trick can we use? Thanks! Note: should look like","\frac{1}{2}(\Omega Q + Q\Omega^T)=F Q 1 \text{tr}(Q)=1 Q\succeq0 Q = qq^T q \|q\|=1 \Omega \Omega^{T} = -\Omega 0 \Omega Q F 4\times 4 \Omega \Omega Q F \Omega \begin{bmatrix}0 & \omega_3 & -\omega_2 & \omega_1 \\
  -\omega_3 & 0 & \omega_1 & \omega_2 \\
   \omega_2 & -\omega_1 & 0 & \omega_3 \\
   -\omega_1 & -\omega_2 & -\omega_3 & 0 \end{bmatrix}","['matrices', 'matrix-equations']"
41,Why does definition of the inverse of a matrix involves having $AB=I=BA$?,Why does definition of the inverse of a matrix involves having ?,AB=I=BA,"So, I was reviewing the first course in Linear Algebra which I took and got curious about the reason behind defining the inverse of a matrix in the following way (from Wikipedia): In linear algebra, an $n$ -by- $n$ square matrix $A$ is called invertible (also nonsingular or nondegenerate) if there exists an $n$ -by- $n$ square matrix $B$ such that $$ AB=BA=I $$ Now, I had an exercise to prove that if $AB=I$ , then $BA=I$ . Then, what is the reason to put both the equalities in the definition ? Is that somewhat traditional or is it because of some specific reason which I'm not aware of? I'd be happy if someone could help me out. Thanks in advance!","So, I was reviewing the first course in Linear Algebra which I took and got curious about the reason behind defining the inverse of a matrix in the following way (from Wikipedia): In linear algebra, an -by- square matrix is called invertible (also nonsingular or nondegenerate) if there exists an -by- square matrix such that Now, I had an exercise to prove that if , then . Then, what is the reason to put both the equalities in the definition ? Is that somewhat traditional or is it because of some specific reason which I'm not aware of? I'd be happy if someone could help me out. Thanks in advance!","n n A n n B 
AB=BA=I
 AB=I BA=I","['linear-algebra', 'matrices', 'definition', 'inverse']"
42,General formula for eigenvectors of a 3x3 matrix,General formula for eigenvectors of a 3x3 matrix,,"Sorry if this is a dumb question but given a general 3x3 matrix $A = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} $ and assuming it has 3 distinct eigenvalues $\lambda_1, \lambda_2, \lambda_3$ , is there a general (analytical) formula for the eigenvectors of this matrix?","Sorry if this is a dumb question but given a general 3x3 matrix and assuming it has 3 distinct eigenvalues , is there a general (analytical) formula for the eigenvectors of this matrix?","A = \begin{pmatrix} a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix}  \lambda_1, \lambda_2, \lambda_3","['linear-algebra', 'matrices']"
43,"If $A^3 = I_n$, then $\operatorname{tr}(A)\in\Bbb Z$","If , then",A^3 = I_n \operatorname{tr}(A)\in\Bbb Z,"Let $A\in M_{n\times n}(\Bbb R)$ s.t. $A^3 = I_n$ . Show that $\operatorname{tr}(A) \in \Bbb Z$ . I know that $P(A) = 0$ , where $ P(x) = x^3 - 1 = (x-1)(x^2+x+1)$ , that is, $1$ is an eigenvalue of $A$ . Also, trace is the sums of the eigevalues and in $\Bbb C$ have $1, \omega, \omega^2$ are the eigenvalues of $A$ . However I'm stuck in $\Bbb R$ . I guess, it's not true that the trace in $\Bbb R$ is equal to the trace in $\Bbb C$ . Can you help me?","Let s.t. . Show that . I know that , where , that is, is an eigenvalue of . Also, trace is the sums of the eigevalues and in have are the eigenvalues of . However I'm stuck in . I guess, it's not true that the trace in is equal to the trace in . Can you help me?","A\in M_{n\times n}(\Bbb R) A^3 = I_n \operatorname{tr}(A) \in \Bbb Z P(A) = 0  P(x) = x^3 - 1 = (x-1)(x^2+x+1) 1 A \Bbb C 1, \omega, \omega^2 A \Bbb R \Bbb R \Bbb C","['linear-algebra', 'matrices', 'trace']"
44,"How to prove that when $n$ is even, and $A = (a_{ij})_{i,j}$, $1\leq i,j\leq n$ to show that $\mathrm{det} A = 1$? [closed]","How to prove that when  is even, and ,  to show that ? [closed]","n A = (a_{ij})_{i,j} 1\leq i,j\leq n \mathrm{det} A = 1","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to prove that when $n$ is even, and $A = (a_{ij})_{i,j}$ , $1\leq i,j\leq n$ with $$a_{ij}=\begin{cases} 1& i<j,\\ 0 & i=j,\\ -1& i>j,\end{cases}$$ to show that $\mathrm{det} A = 1$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How to prove that when is even, and , with to show that ?","n A = (a_{ij})_{i,j} 1\leq i,j\leq n a_{ij}=\begin{cases} 1& i<j,\\ 0 & i=j,\\ -1& i>j,\end{cases} \mathrm{det} A = 1","['linear-algebra', 'matrices', 'determinant']"
45,Prove existence of evaluation points such that the matrix has nonzero determinant,Prove existence of evaluation points such that the matrix has nonzero determinant,,"I've been struggling with the following exercise for quite some time already: Consider a linear space $\mathbb{V} = \mathcal{C}\left(\left[a, b\right]\right)$ and let $f_{1},\ldots, f_{n}$ be linearly independent functions in $\mathbb{V}$ . Prove there exist numbers $a \leq x_{1} < \cdots < x_{n} \leq b$ such that $$ \det \begin{bmatrix} f_{1}(x_{1}) & f_{1}(x_{2}) & \cdots & f_{1}(x_{n})\\ f_{2}(x_{1}) & f_{2}(x_{2}) & \cdots & f_{2}(x_{n}) \\ \vdots & \vdots  & \ddots & \vdots     \\ f_{n}(x_{1}) & f_{n}(x_{2}) & \cdots & f_{n}(x_{n}) \end{bmatrix} \neq 0.$$ The statement is extremely easy to prove by means of induction. However, I'm interested if there's another (and more elegant) proof which doesn't involve induction . Any hints appreciated.","I've been struggling with the following exercise for quite some time already: Consider a linear space and let be linearly independent functions in . Prove there exist numbers such that The statement is extremely easy to prove by means of induction. However, I'm interested if there's another (and more elegant) proof which doesn't involve induction . Any hints appreciated.","\mathbb{V} = \mathcal{C}\left(\left[a, b\right]\right) f_{1},\ldots, f_{n} \mathbb{V} a \leq x_{1} < \cdots < x_{n} \leq b  \det \begin{bmatrix}
f_{1}(x_{1}) & f_{1}(x_{2}) & \cdots & f_{1}(x_{n})\\
f_{2}(x_{1}) & f_{2}(x_{2}) & \cdots & f_{2}(x_{n}) \\
\vdots & \vdots  & \ddots & \vdots     \\
f_{n}(x_{1}) & f_{n}(x_{2}) & \cdots & f_{n}(x_{n})
\end{bmatrix} \neq 0.","['linear-algebra', 'matrices', 'functional-analysis', 'determinant']"
46,Prove that $A^5 \neq I$,Prove that,A^5 \neq I,"$A \in M_{5}(\mathbb{C})$ , $\operatorname{trace}(A) = 0$ , $I-A$ is invertible. Prove that $A^5 \neq I$ I think this problem has something to do with eigenvalue and the fact that trace of a matrix = sum of the eigenvalues. But, I have no idea how to proceed! Thanks!",", , is invertible. Prove that I think this problem has something to do with eigenvalue and the fact that trace of a matrix = sum of the eigenvalues. But, I have no idea how to proceed! Thanks!",A \in M_{5}(\mathbb{C}) \operatorname{trace}(A) = 0 I-A A^5 \neq I,"['linear-algebra', 'matrices', 'trace']"
47,Are Hessian matrices always symmetric? [duplicate],Are Hessian matrices always symmetric? [duplicate],,"This question already has answers here : Are there any Hessian matrices that are asymmetric on a large set? (2 answers) Closed 3 years ago . The objective function of interest is: $$ \phi = \text{log}|PWP^T| + \text{tr}((PWP^T)^{-1}PVP^T) $$ where $P = J + XU^T$ and $V$ , $J$ and $U^T$ are known matrices. I assume that the $V$ and $W$ are positive definite. The first partial derivative of $\phi$ with respect to $X$ is \begin{align*} Y^{-1}(JWU + JVU) + Y^{-1}X(U^TWU + U^TVU) - Y^{-1}ZY^{-1}(JWU + XU^TWU)  \end{align*} where $Y = PWP^{T}$ and $Z = PVP^T$ . The first partial derivative with respect to $W$ gives $$ PWP^T = PVP^T. $$ Hence combining the solutions of partial derivatives $X = -JVU(U^TVU)^{-1}$ . If I compute $\nabla_{xx}\phi$ and evaluate at the solution what I would get is $$ [(U^TWUX^T + U^TWJ^T)Y^{-1} \otimes Y^{-1}](I + K)[(U^TWUX^T + U^TWJ^T)^T \otimes I] + (U^TVU \otimes Y^{-1}) $$ where $K$ is the commutation matrix. However, I can't see that it is symmetric. If this is not symmetric, Hessian would not be symmetric. Is it always the case that the Hessian needs to be symmetric?","This question already has answers here : Are there any Hessian matrices that are asymmetric on a large set? (2 answers) Closed 3 years ago . The objective function of interest is: where and , and are known matrices. I assume that the and are positive definite. The first partial derivative of with respect to is where and . The first partial derivative with respect to gives Hence combining the solutions of partial derivatives . If I compute and evaluate at the solution what I would get is where is the commutation matrix. However, I can't see that it is symmetric. If this is not symmetric, Hessian would not be symmetric. Is it always the case that the Hessian needs to be symmetric?","
\phi = \text{log}|PWP^T| + \text{tr}((PWP^T)^{-1}PVP^T)
 P = J + XU^T V J U^T V W \phi X \begin{align*}
Y^{-1}(JWU + JVU) + Y^{-1}X(U^TWU + U^TVU) - Y^{-1}ZY^{-1}(JWU + XU^TWU) 
\end{align*} Y = PWP^{T} Z = PVP^T W 
PWP^T = PVP^T.
 X = -JVU(U^TVU)^{-1} \nabla_{xx}\phi 
[(U^TWUX^T + U^TWJ^T)Y^{-1} \otimes Y^{-1}](I + K)[(U^TWUX^T + U^TWJ^T)^T \otimes I] + (U^TVU \otimes Y^{-1})
 K","['linear-algebra', 'matrices', 'optimization', 'matrix-calculus', 'hessian-matrix']"
48,"Is $ f(A) = A + 2A^{T} $ an isomorphism of $ \mathbb R^{5,5} $ onto itself?",Is  an isomorphism of  onto itself?," f(A) = A + 2A^{T}   \mathbb R^{5,5} ","I have problem with prove or disprove this hypothesis: Is the linear transformation $ f \in L(\mathbb R^{5,5},\mathbb R^{5,5}) $ $$ f(A) = A + 2A^{T} $$ an isomorphism of the space $ \mathbb R^{5,5} $ onto itself? In order to be an isomorphism, $f$ must be injective and surjective. Checking if it is surjective: Assume that as a result we want to get: $$\mathbf{Q} =   \begin{bmatrix}  q_{11} & q_{12} & \cdots & q_{1n} \\  q_{21} & q_{22} & \cdots & q_{2n} \\  \vdots & \vdots & \ddots & \vdots \\  q_{m1} & q_{m2} & \cdots & q_{mn}  \end{bmatrix} $$ and we put as argument $$  \mathbf{A} =   \begin{bmatrix}  a_{11} & a_{12} & \cdots & a_{1n} \\  a_{21} & a_{22} & \cdots & a_{2n} \\  \vdots & \vdots & \ddots & \vdots \\  a_{m1} & a_{m2} & \cdots & a_{mn}  \end{bmatrix} $$ then choose $i,j$ . Factors $a_{ij}$ $a_{ji}$ $q_{ij}$ $q_{ji}$ are only in this linear system: $$q_{ij} = a_{ij} + 2a_{ji}  \wedge  q_{ji} = a_{ji} + 2a_{ij}$$ so $$ a_{ij} = \frac{2q_{ij}-q_{ji}}{3} $$ and $$ a_{ji} = \frac{2q_{ji}-q_{ij}}{3} $$ so the factors $ a_{ij}$ $ a_{ji}$ are determined unambiguously. So it is surjective. But how to deal with checking injectivity? Suppose that $$ A+2A^T=B+2B^T $$ $$ A-B = 2(A^T-B^T) $$ and I don't know how to finish that.","I have problem with prove or disprove this hypothesis: Is the linear transformation an isomorphism of the space onto itself? In order to be an isomorphism, must be injective and surjective. Checking if it is surjective: Assume that as a result we want to get: and we put as argument then choose . Factors are only in this linear system: so and so the factors are determined unambiguously. So it is surjective. But how to deal with checking injectivity? Suppose that and I don't know how to finish that."," f \in L(\mathbb R^{5,5},\mathbb R^{5,5})   f(A) = A + 2A^{T}   \mathbb R^{5,5}  f \mathbf{Q} = 
 \begin{bmatrix}
 q_{11} & q_{12} & \cdots & q_{1n} \\
 q_{21} & q_{22} & \cdots & q_{2n} \\
 \vdots & \vdots & \ddots & \vdots \\
 q_{m1} & q_{m2} & \cdots & q_{mn}
 \end{bmatrix}    \mathbf{A} = 
 \begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1n} \\
 a_{21} & a_{22} & \cdots & a_{2n} \\
 \vdots & \vdots & \ddots & \vdots \\
 a_{m1} & a_{m2} & \cdots & a_{mn}
 \end{bmatrix}  i,j a_{ij} a_{ji} q_{ij} q_{ji} q_{ij} = a_{ij} + 2a_{ji}  \wedge  q_{ji} = a_{ji} + 2a_{ij}  a_{ij} = \frac{2q_{ij}-q_{ji}}{3}   a_{ji} = \frac{2q_{ji}-q_{ij}}{3}   a_{ij}  a_{ji}  A+2A^T=B+2B^T   A-B = 2(A^T-B^T) ",['linear-algebra']
49,Hadamard product: Optimal bound on operator norm,Hadamard product: Optimal bound on operator norm,,"Let $A,B$ be $n\times n $ matrices and denote by $A\star B$ the Hadamard product $(A\star B)(i,j)=A(i,j)B(i,j)$ (pointwise matrix multiplication). For $A$ positive definite it is known that  $$\|A\star B\| \leq \sup_{i,j} |A(i,j)| \|B\|.$$ My question is what happens if we drop the positive definiteness assumption, i.e. what is the best constant $C>0$ such that $$\|A\star B\| \leq C \sup_{i,j} |A(i,j)| \|B\|$$ holds for arbitrary $n\times n$ matrices $A,B$. Is the constant $C$ independent of the size of the matrix $n$?","Let $A,B$ be $n\times n $ matrices and denote by $A\star B$ the Hadamard product $(A\star B)(i,j)=A(i,j)B(i,j)$ (pointwise matrix multiplication). For $A$ positive definite it is known that  $$\|A\star B\| \leq \sup_{i,j} |A(i,j)| \|B\|.$$ My question is what happens if we drop the positive definiteness assumption, i.e. what is the best constant $C>0$ such that $$\|A\star B\| \leq C \sup_{i,j} |A(i,j)| \|B\|$$ holds for arbitrary $n\times n$ matrices $A,B$. Is the constant $C$ independent of the size of the matrix $n$?",,"['linear-algebra', 'matrices', 'functional-analysis']"
50,"Given a $3\times3$ real matrix $A$ with eigenvalues $0,1,2$, find real constants $a,b,c$ such that $aI+bA+cA^2$ has eigenvalues 0,1,3","Given a  real matrix  with eigenvalues , find real constants  such that  has eigenvalues 0,1,3","3\times3 A 0,1,2 a,b,c aI+bA+cA^2","Suppose $A$ is a $3\times 3$ real matrix with three distinct eigenvalues $0,1,2$. Find real constants $a,b,c$ such that the matrix $aI+bA+cA^2$ has eigenvalues $0,1,3$. My only initial thought on how to approach this problem is to use the Cayley-Hamilton theorem. i.e.  $$A^3+3A^2+2A=0=(cA^2+bA+aI)^3-4(cA^2+bA+aI)^2+3(cA^2+bA+aI)$$ but this is a pretty ugly system (I didn't bother to show the algebraic steps of expanding the characteristic polynomials, but they are $C_p(A)=x(x-1)(x-2)$ and $C_p(aI+bA+cA^2)=x(x-1)(x-3)$). Thanks in advance for any assistance.","Suppose $A$ is a $3\times 3$ real matrix with three distinct eigenvalues $0,1,2$. Find real constants $a,b,c$ such that the matrix $aI+bA+cA^2$ has eigenvalues $0,1,3$. My only initial thought on how to approach this problem is to use the Cayley-Hamilton theorem. i.e.  $$A^3+3A^2+2A=0=(cA^2+bA+aI)^3-4(cA^2+bA+aI)^2+3(cA^2+bA+aI)$$ but this is a pretty ugly system (I didn't bother to show the algebraic steps of expanding the characteristic polynomials, but they are $C_p(A)=x(x-1)(x-2)$ and $C_p(aI+bA+cA^2)=x(x-1)(x-3)$). Thanks in advance for any assistance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'cayley-hamilton']"
51,Does $A$ is similar to a symmetric matrix $\implies $ $A$ is symmetric?,Does  is similar to a symmetric matrix   is symmetric?,A \implies  A,"Let $A\in \mathbb R^{n\times n}$. Is it true that $A$ similar to a symmetric matrix $\implies $ $A$ symmetric ? Let $B$ symmetric s.t. $A=PBP^{-1}$. Then $$A^T=(P^{-1})^TB^T P^T=(P^{-1})^T BP^T.$$ For me there is no reason that $P$ is orthogonal, so I would say it's false a priori. But in the same time, this theorem should be true since   operator is self adjoint $\iff$ it's diagonalizable. I also know that matrices in any basis of Self Adjoint operator are symmetric. But if A is similar to a symmetric matrix, then it's diagonalizable and thus self adjoint, and thus, it should be symmetric in any basis... this is wrong ? If yes, why ?","Let $A\in \mathbb R^{n\times n}$. Is it true that $A$ similar to a symmetric matrix $\implies $ $A$ symmetric ? Let $B$ symmetric s.t. $A=PBP^{-1}$. Then $$A^T=(P^{-1})^TB^T P^T=(P^{-1})^T BP^T.$$ For me there is no reason that $P$ is orthogonal, so I would say it's false a priori. But in the same time, this theorem should be true since   operator is self adjoint $\iff$ it's diagonalizable. I also know that matrices in any basis of Self Adjoint operator are symmetric. But if A is similar to a symmetric matrix, then it's diagonalizable and thus self adjoint, and thus, it should be symmetric in any basis... this is wrong ? If yes, why ?",,"['linear-algebra', 'matrices', 'symmetric-matrices']"
52,$A$ is a symmetric matrix such that $A^4=A$. Prove that $A$ is idempotent,is a symmetric matrix such that . Prove that  is idempotent,A A^4=A A,"Let $A$ be a real symmetric matrix such that $A^4=A$. Prove that $A$ is idempotent. I have tried using eigenvalues and only inferred that the eigen values may be $0,1$. But I cannot proceed with this.","Let $A$ be a real symmetric matrix such that $A^4=A$. Prove that $A$ is idempotent. I have tried using eigenvalues and only inferred that the eigen values may be $0,1$. But I cannot proceed with this.",,"['linear-algebra', 'matrices']"
53,Largest eigenvalues of matrix and its doubled symmetric part,Largest eigenvalues of matrix and its doubled symmetric part,,"Is it true that the largest eigenvalue of $A+A^{\rm T}$ is always (in some sense) bigger than the largest eigenvalue of $A$? For example, does it always hold that  $$ \quad {\rm Re \,} \lambda_{\rm max}(A) \le {\rm Re \,} \lambda_{\rm max}(A+A^{\rm T})  \quad ? $$","Is it true that the largest eigenvalue of $A+A^{\rm T}$ is always (in some sense) bigger than the largest eigenvalue of $A$? For example, does it always hold that  $$ \quad {\rm Re \,} \lambda_{\rm max}(A) \le {\rm Re \,} \lambda_{\rm max}(A+A^{\rm T})  \quad ? $$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
54,Singularity of the product of two rectangular matrices?,Singularity of the product of two rectangular matrices?,,"Let $A$ be an $m \times n$ matrix and $B$ be an $n \times m$ matrix where $m<n$. Then can we say that the product $AB_{m \times m}$ is always singular or always non-singular? Also, can we say that $BA_{n \times n}$ is always singular or non-singular?. Does this change any thing? I was thinking that since $m<n$ we have Rank$(A) \leq m$ and similarly Rank$(B) \leq n$ and also that Rank$(AB) \leq min($Rank$(A)$,Rank$(B)$), but will that help? How can I think about this problem?","Let $A$ be an $m \times n$ matrix and $B$ be an $n \times m$ matrix where $m<n$. Then can we say that the product $AB_{m \times m}$ is always singular or always non-singular? Also, can we say that $BA_{n \times n}$ is always singular or non-singular?. Does this change any thing? I was thinking that since $m<n$ we have Rank$(A) \leq m$ and similarly Rank$(B) \leq n$ and also that Rank$(AB) \leq min($Rank$(A)$,Rank$(B)$), but will that help? How can I think about this problem?",,"['linear-algebra', 'matrices', 'determinant']"
55,What is the relationship between singular values and eigenvalues of a matrix?,What is the relationship between singular values and eigenvalues of a matrix?,,Suppose I have a general $n\times n$ real matrix $A$. And suppose that $A$ has an SVD of the form $A=U^T S V$ with S of the form $I_m \oplus D$ where $I_m$ is the identity $m\times m$ matrix and $D$ is a matrix of size $n-m \times n-m$. This means that $A$ has $m$ singular values equal to 1. Would this suffice to  conclude that $A$ has $m$ eigenvalues of modulus 1? Why? Why not?,Suppose I have a general $n\times n$ real matrix $A$. And suppose that $A$ has an SVD of the form $A=U^T S V$ with S of the form $I_m \oplus D$ where $I_m$ is the identity $m\times m$ matrix and $D$ is a matrix of size $n-m \times n-m$. This means that $A$ has $m$ singular values equal to 1. Would this suffice to  conclude that $A$ has $m$ eigenvalues of modulus 1? Why? Why not?,,"['matrices', 'eigenvalues-eigenvectors', 'svd', 'singular-values']"
56,Finding the Jordan canonical form of A and Choose the correct option,Finding the Jordan canonical form of A and Choose the correct option,,"Let $$ A = \begin{pmatrix}   0&0&0&-4 \\ 1&0&0&0 \\ 0&1&0&5 \\ 0&0&1&0  \end{pmatrix}$$ Then a Jordan canonical form of  A is Choose the correct option $a) \begin{pmatrix}   -1&0&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ $b) \begin{pmatrix}   -1&1&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ $c) \begin{pmatrix}   1&1&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ $d) \begin{pmatrix}   -1&1&0&0 \\ 0&-1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ My attempt : I know that  Determinant   of  A = product  of eigenvalues of A,  as  option c and d  is  not correct because  Here Determinant of A = 4 that is $ \det A =  -(-4) \begin{pmatrix}1 & 0 &0\\0& 1 & 0\\ 0&0&1\end{pmatrix}$ I'm  in  confusion  about  option  a) and  b).......how  can I find the  Jordan canonical form of A ? PLiz  help  me. Any hints/solution will be appreciated. Thanks in advance","Let $$ A = \begin{pmatrix}   0&0&0&-4 \\ 1&0&0&0 \\ 0&1&0&5 \\ 0&0&1&0  \end{pmatrix}$$ Then a Jordan canonical form of  A is Choose the correct option $a) \begin{pmatrix}   -1&0&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ $b) \begin{pmatrix}   -1&1&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ $c) \begin{pmatrix}   1&1&0&0 \\ 0&1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ $d) \begin{pmatrix}   -1&1&0&0 \\ 0&-1&0&0 \\ 0&0&2&0 \\ 0&0&0&-2  \end{pmatrix}$ My attempt : I know that  Determinant   of  A = product  of eigenvalues of A,  as  option c and d  is  not correct because  Here Determinant of A = 4 that is $ \det A =  -(-4) \begin{pmatrix}1 & 0 &0\\0& 1 & 0\\ 0&0&1\end{pmatrix}$ I'm  in  confusion  about  option  a) and  b).......how  can I find the  Jordan canonical form of A ? PLiz  help  me. Any hints/solution will be appreciated. Thanks in advance",,['linear-algebra']
57,An orthogonal matrix which sends a vector to other vector with same length.,An orthogonal matrix which sends a vector to other vector with same length.,,"I know that For every to vectors $u,v\in \mathbb R^n$ where $|u|=|v|$ there exists an orthogonal matrix $A$ such that, $Au=v$. I have a problem so construct this matrix. is there any method to construct this matrix?","I know that For every to vectors $u,v\in \mathbb R^n$ where $|u|=|v|$ there exists an orthogonal matrix $A$ such that, $Au=v$. I have a problem so construct this matrix. is there any method to construct this matrix?",,"['linear-algebra', 'matrices', 'functional-analysis', 'orthogonal-matrices']"
58,Limit of Square Root of a Tridiagonal Matrix,Limit of Square Root of a Tridiagonal Matrix,,"Consider a tridiagonal $n\times n$ matrix with the following format $$ \mathbf A = \begin{bmatrix} a & b &        & \phantom{\ddots} & \phantom{\ddots}    \\ b & a & b      & \phantom{\ddots} &     \\   & b & a      & \ddots           &     \\   &   & \ddots & \ddots           & b   \\ \phantom{\ddots}  & \phantom{\ddots}  &  \phantom{\ddots}      & b                & a   \\ \end{bmatrix}.$$ This matrix has eigenvalues equal to $$\lambda_k = a-2b\cos\left(\frac{k\pi}{1+n}\right),\;\text{ for }\;k\in\{1,\dots,n\}.$$  It follows that $$\lim_{n\to\infty}\lambda_k=a-2b.$$ It is also symmetric which mean it can be diagonalized into $$\mathbf A= \mathbf Q \mathbf \Lambda \mathbf Q^T,\;\text{ so that  }\;\mathbf A^{1/2}= \mathbf Q \mathbf \Lambda^{1/2} \mathbf Q^T.$$ Is it possible to obtain an analitical formula for the elements of $\lim_{n\to\infty}\mathbf A^{1/2}$? Edit: This paper on tridiagonal Toeplitz matrices appears to be helpful .","Consider a tridiagonal $n\times n$ matrix with the following format $$ \mathbf A = \begin{bmatrix} a & b &        & \phantom{\ddots} & \phantom{\ddots}    \\ b & a & b      & \phantom{\ddots} &     \\   & b & a      & \ddots           &     \\   &   & \ddots & \ddots           & b   \\ \phantom{\ddots}  & \phantom{\ddots}  &  \phantom{\ddots}      & b                & a   \\ \end{bmatrix}.$$ This matrix has eigenvalues equal to $$\lambda_k = a-2b\cos\left(\frac{k\pi}{1+n}\right),\;\text{ for }\;k\in\{1,\dots,n\}.$$  It follows that $$\lim_{n\to\infty}\lambda_k=a-2b.$$ It is also symmetric which mean it can be diagonalized into $$\mathbf A= \mathbf Q \mathbf \Lambda \mathbf Q^T,\;\text{ so that  }\;\mathbf A^{1/2}= \mathbf Q \mathbf \Lambda^{1/2} \mathbf Q^T.$$ Is it possible to obtain an analitical formula for the elements of $\lim_{n\to\infty}\mathbf A^{1/2}$? Edit: This paper on tridiagonal Toeplitz matrices appears to be helpful .",,"['linear-algebra', 'matrices']"
59,Prove that $\det (A^n + B^n) \geq 0 $,Prove that,\det (A^n + B^n) \geq 0 ,"Let $A$ and $B$ from $M_n(\mathbb{R})$ with $AB=BA$ and $\det(A+B) \geq 0$ Prove that $ \forall n \in \mathbb{N^{*}} $, $\det (A^n + B^n) \geq 0 $ I tried to use induction but I got stuck.","Let $A$ and $B$ from $M_n(\mathbb{R})$ with $AB=BA$ and $\det(A+B) \geq 0$ Prove that $ \forall n \in \mathbb{N^{*}} $, $\det (A^n + B^n) \geq 0 $ I tried to use induction but I got stuck.",,"['linear-algebra', 'matrices', 'inequality', 'determinant']"
60,Intuition for the differences between characteristic and minimal polynomial,Intuition for the differences between characteristic and minimal polynomial,,"I understand the definitions of the characteristic and minimal polynomials, but I don't quite see an easy way to explicitly come up with examples of matrices for which the two polynomials satisfy some properties. For example, consider the following three exercises from Sheldon Axler's Linear Algebra Done Right , chapter 8C: Give an example of an operator on $\mathbb{C}^4$ whose characteristic polynomial equals $(z-1)(z-5)^3$ and whose minimal polynomial equals $(z-1)(z-5)^2$ . Give an example of an operator on $\mathbb{C}^4$ whose characteristic and minimal polynomials both equal $z(z-1)^2(z-3)$ . Give an example of an operator on $\mathbb{C}^4$ whose characteristic polynomial equals $z(z-1)^2(z-3)$ and whose minimal polynomial equals $z(z-1)(z-3)$ . For Exercise 6, it is obvious that we just take a diagonal matrix with diagonal $0,1,1,3$ . However, I don't quite see how to come up with examples of operators in questions 4 and 5 - I don't have any intuition between how the structure of an operator/matrix relates to its characteristic and minimal polynomials. Can someone please explain the reasoning one can use to find answers to the questions? From a quick search I understand that Jordan forms of matrices could be useful in finding explicit forms of matrices satisfying these properties, but these exercises are actually before the introduction of Jordan forms in the book so I would appreciate not using them in the answers. Thank you.","I understand the definitions of the characteristic and minimal polynomials, but I don't quite see an easy way to explicitly come up with examples of matrices for which the two polynomials satisfy some properties. For example, consider the following three exercises from Sheldon Axler's Linear Algebra Done Right , chapter 8C: Give an example of an operator on whose characteristic polynomial equals and whose minimal polynomial equals . Give an example of an operator on whose characteristic and minimal polynomials both equal . Give an example of an operator on whose characteristic polynomial equals and whose minimal polynomial equals . For Exercise 6, it is obvious that we just take a diagonal matrix with diagonal . However, I don't quite see how to come up with examples of operators in questions 4 and 5 - I don't have any intuition between how the structure of an operator/matrix relates to its characteristic and minimal polynomials. Can someone please explain the reasoning one can use to find answers to the questions? From a quick search I understand that Jordan forms of matrices could be useful in finding explicit forms of matrices satisfying these properties, but these exercises are actually before the introduction of Jordan forms in the book so I would appreciate not using them in the answers. Thank you.","\mathbb{C}^4 (z-1)(z-5)^3 (z-1)(z-5)^2 \mathbb{C}^4 z(z-1)^2(z-3) \mathbb{C}^4 z(z-1)^2(z-3) z(z-1)(z-3) 0,1,1,3","['linear-algebra', 'matrices']"
61,Find the orthogonal complement of the subspace of diagonal matrices,Find the orthogonal complement of the subspace of diagonal matrices,,"In $\mathbb{C^{3\times3}}$ , with the inner product of matrices defined as $$\langle A,B\rangle = \operatorname{tr}(A^*B)$$ find the orthogonal complement of the subspace of diagonal matrices. Then, considering the following matrices $\in\mathbb{C^{3\times3}}$ $$A=\begin{pmatrix} a_{11}&0&0\\0&a_{22}&0\\0&0&a_{33}\end{pmatrix}$$ and $$B=\begin{pmatrix} b_{11}&0&0\\0&b_{22}&0\\0&0&b_{33}\end{pmatrix}$$ I concluded by the statement and the definition of orthogonal complement: $$\langle A,B\rangle=\operatorname{tr}\begin{pmatrix} \overline{a_{11}}b_{11}&0&0\\0&\overline{a_{22}}  b_{22}&0\\0&0&\overline{a_{22}}b_{33}\end{pmatrix}=0$$ After, I get the trace of the $3$ -by- $3$ square matrix of the inner product $\langle A,B\rangle$ $$\overline{a_{11}}  b_{11}+ \overline{a_{22}}b_{22}+ \overline{a_{33}}b_{33}=0$$ I'm stuck from here, how can I do to get the orthogonal complement?","In , with the inner product of matrices defined as find the orthogonal complement of the subspace of diagonal matrices. Then, considering the following matrices and I concluded by the statement and the definition of orthogonal complement: After, I get the trace of the -by- square matrix of the inner product I'm stuck from here, how can I do to get the orthogonal complement?","\mathbb{C^{3\times3}} \langle A,B\rangle = \operatorname{tr}(A^*B) \in\mathbb{C^{3\times3}} A=\begin{pmatrix} a_{11}&0&0\\0&a_{22}&0\\0&0&a_{33}\end{pmatrix} B=\begin{pmatrix} b_{11}&0&0\\0&b_{22}&0\\0&0&b_{33}\end{pmatrix} \langle A,B\rangle=\operatorname{tr}\begin{pmatrix}
\overline{a_{11}}b_{11}&0&0\\0&\overline{a_{22}}  b_{22}&0\\0&0&\overline{a_{22}}b_{33}\end{pmatrix}=0 3 3 \langle A,B\rangle \overline{a_{11}}  b_{11}+ \overline{a_{22}}b_{22}+ \overline{a_{33}}b_{33}=0","['linear-algebra', 'matrices', 'inner-products', 'orthogonality']"
62,"Is it true that $m=n\implies A$ is invertible, for an $m\times n$ matrix satisfying $(AA^T)^r=I$","Is it true that  is invertible, for an  matrix satisfying",m=n\implies A m\times n (AA^T)^r=I,"If $m,n,r\in \Bbb N$ and $A$ is an $m\times n$ matrix satisfying $(AA^T)^r=I$ is it true that $m=n\implies A$ is invertible. I think it's true since $\det (AA^T)^r=1\implies \det A^{2r}=1\implies (\det A)^{2r}=1\implies \det A\neq 0\implies A $ is invertible. So the result is true. Is this correct?","If $m,n,r\in \Bbb N$ and $A$ is an $m\times n$ matrix satisfying $(AA^T)^r=I$ is it true that $m=n\implies A$ is invertible. I think it's true since $\det (AA^T)^r=1\implies \det A^{2r}=1\implies (\det A)^{2r}=1\implies \det A\neq 0\implies A $ is invertible. So the result is true. Is this correct?",,"['linear-algebra', 'matrices', 'proof-verification']"
63,Complex matrix decomposition into the sum of a diagonalizable and a nilpotent matrix.,Complex matrix decomposition into the sum of a diagonalizable and a nilpotent matrix.,,"Let $A$ be any $n \times n$ complex matrix. Prove that $A$ can be written as $A = B + N$ where $B$ is diagonalizable, $N$ is nilpotent (some power is the zero matrix) and the matrices $B$ and $N$ commute.","Let $A$ be any $n \times n$ complex matrix. Prove that $A$ can be written as $A = B + N$ where $B$ is diagonalizable, $N$ is nilpotent (some power is the zero matrix) and the matrices $B$ and $N$ commute.",,"['linear-algebra', 'matrices']"
64,Can a non-symmetric projection matrix exist?,Can a non-symmetric projection matrix exist?,,Can a non-symmetric projection matrix exist? I am currently using the matrix $M = I - wi^T$ where $i$ is a vector of ones and $w$ is a vector of weights so $w^T i = 1$. Is $M$ a projection matrix? Is it possible for non-symmetric projection matrices  to exist (as I believe this matrix to be)? Thanks in advance.,Can a non-symmetric projection matrix exist? I am currently using the matrix $M = I - wi^T$ where $i$ is a vector of ones and $w$ is a vector of weights so $w^T i = 1$. Is $M$ a projection matrix? Is it possible for non-symmetric projection matrices  to exist (as I believe this matrix to be)? Thanks in advance.,,"['linear-algebra', 'matrices', 'projection-matrices']"
65,How to derive the following formula for the inverse of a matrix?,How to derive the following formula for the inverse of a matrix?,,I came across the following theorem: Let $A$ be a nonsingular square $p \times p$ matrix and $z$ be a p-dimensional column vector. The matrix $(A - z z^T)^{-1}$ is given by $$(A- zz^T)^{-1} = A^{-1} + \frac{A^{-1}zz^TA^{-1}}{1-z^T A^{-1}z}$$ Now I tried using $A-zz^T$ multiply the matrix on the right side of the above formula and I cannot obtain an identity matrix. I tried: $$(A^{-1} + \frac{A^{-1}zz^TA^{-1}}{1-z^T A^{-1}z})(A-zz^T) = I - A^{-1}zz^T - \frac{1}{1-z^TA^{-1}z}(A^{-1}zz^T+A^{-1}zz^TA^{-1}zz^T)$$ This is where I got stuck. Can someone help me on this please?,I came across the following theorem: Let $A$ be a nonsingular square $p \times p$ matrix and $z$ be a p-dimensional column vector. The matrix $(A - z z^T)^{-1}$ is given by $$(A- zz^T)^{-1} = A^{-1} + \frac{A^{-1}zz^TA^{-1}}{1-z^T A^{-1}z}$$ Now I tried using $A-zz^T$ multiply the matrix on the right side of the above formula and I cannot obtain an identity matrix. I tried: $$(A^{-1} + \frac{A^{-1}zz^TA^{-1}}{1-z^T A^{-1}z})(A-zz^T) = I - A^{-1}zz^T - \frac{1}{1-z^TA^{-1}z}(A^{-1}zz^T+A^{-1}zz^TA^{-1}zz^T)$$ This is where I got stuck. Can someone help me on this please?,,"['linear-algebra', 'matrices', 'inverse']"
66,Similar matrices-Identity,Similar matrices-Identity,,"Can anyone help me to understand what is suppose to do in this question? ""A matrix that is similar to the identity matrix"" I should say something about this but I do not understand what is meant to do. I know that two n x n matrices A and B are similar if $B=P^{-1}AP $. They also have some properties such as: The same: Rank; Characteristic equation;  Determinant; Trace; Eigenvalues etc. However I do not understand the question and consequently I do not know how to start. Can anyone help me on this? Thanks","Can anyone help me to understand what is suppose to do in this question? ""A matrix that is similar to the identity matrix"" I should say something about this but I do not understand what is meant to do. I know that two n x n matrices A and B are similar if $B=P^{-1}AP $. They also have some properties such as: The same: Rank; Characteristic equation;  Determinant; Trace; Eigenvalues etc. However I do not understand the question and consequently I do not know how to start. Can anyone help me on this? Thanks",,"['linear-algebra', 'matrices']"
67,Minimize trace($MX$) with $M$ rank-deficient and $X$ positive semidefinite,Minimize trace() with  rank-deficient and  positive semidefinite,MX M X,"I have an optimization problem of the following form: $$\min_{X\succeq0} \mathrm{trace\;} MX$$ under the linear constraint $\mbox{diag} (X) = \mathrm{Id}$ and the non-convex constraint $\mbox{rank} (X) = 1$. The matrix $M$ is square and rank-deficient. The convex relaxation of this problem corresponds to dropping the rank-1 constraint, and merely keeping $X$ positive semidefinite. I tried running a standard SDP solver (Mosek) on this problem but it yields a matrix $X$ which, despite satisfying the linear constraint and being positive semidefinite, is not of rank one. Instead, it is typically of rank $(n - \mathrm{rank\;} M)$ where $n$ is the number of rows of $X$. Can you explain why I am getting this result?","I have an optimization problem of the following form: $$\min_{X\succeq0} \mathrm{trace\;} MX$$ under the linear constraint $\mbox{diag} (X) = \mathrm{Id}$ and the non-convex constraint $\mbox{rank} (X) = 1$. The matrix $M$ is square and rank-deficient. The convex relaxation of this problem corresponds to dropping the rank-1 constraint, and merely keeping $X$ positive semidefinite. I tried running a standard SDP solver (Mosek) on this problem but it yields a matrix $X$ which, despite satisfying the linear constraint and being positive semidefinite, is not of rank one. Instead, it is typically of rank $(n - \mathrm{rank\;} M)$ where $n$ is the number of rows of $X$. Can you explain why I am getting this result?",,"['linear-algebra', 'matrices', 'convex-optimization', 'semidefinite-programming', 'relaxations']"
68,"If $P$ and $Q$ are invertible matrices $PQ=-QP$, then which claim about their traces is true?","If  and  are invertible matrices , then which claim about their traces is true?",P Q PQ=-QP,"If $P$, $Q$ are invertible and $PQ=-QP$, then what can we say about traces of $P$ and $Q$. I faced this question in an exam but according to me this question is wrong as $Q=-P^{-1}QP$, which implies $\det(Q)=0$ and it implies $Q$ is not invertible? But it is given invertible in hypothesis. Options were both traces $0$, both $1$, $Tr(Q)\neq Tr(P)$ or $Tr(Q)=-Tr(P)$","If $P$, $Q$ are invertible and $PQ=-QP$, then what can we say about traces of $P$ and $Q$. I faced this question in an exam but according to me this question is wrong as $Q=-P^{-1}QP$, which implies $\det(Q)=0$ and it implies $Q$ is not invertible? But it is given invertible in hypothesis. Options were both traces $0$, both $1$, $Tr(Q)\neq Tr(P)$ or $Tr(Q)=-Tr(P)$",,"['linear-algebra', 'matrices', 'trace']"
69,Is the matrix $A$ symmetric in the quadratic form?,Is the matrix  symmetric in the quadratic form?,A,"Given $x \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$ ($A$ is not necessarily symmetric), the quadratic form is written as $x^TAx$, a scaler. We have, $$x^TAx=(x^TAx)^T=x^TA^Tx$$ that is $x^T(A-A^T)x=0$ Why couldn't conclude $A=A^T$ from $x^T(A-A^T)x=0$, where $x \ne \boldsymbol{0}$? I know it's a false statement and there are counter examples, but it seems to me, mathematically, $A$ should be symmetric. Could someone help explain why I couldn't make such an inference?","Given $x \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$ ($A$ is not necessarily symmetric), the quadratic form is written as $x^TAx$, a scaler. We have, $$x^TAx=(x^TAx)^T=x^TA^Tx$$ that is $x^T(A-A^T)x=0$ Why couldn't conclude $A=A^T$ from $x^T(A-A^T)x=0$, where $x \ne \boldsymbol{0}$? I know it's a false statement and there are counter examples, but it seems to me, mathematically, $A$ should be symmetric. Could someone help explain why I couldn't make such an inference?",,"['linear-algebra', 'matrices', 'quadratic-forms']"
70,Can the determinant of a matrix can be made $0$?,Can the determinant of a matrix can be made ?,0,An entry of an $n \times n$ matrix with nonzero determinant is defined as interesting if by changing this entry (and only this entry) the determinant of the matrix can be made $0$. Is it true that each entry of every matrix with nonzero determinant is interesting? Is it true that there is an interesting entry in each row of a matrix with nonzero determinant?,An entry of an $n \times n$ matrix with nonzero determinant is defined as interesting if by changing this entry (and only this entry) the determinant of the matrix can be made $0$. Is it true that each entry of every matrix with nonzero determinant is interesting? Is it true that there is an interesting entry in each row of a matrix with nonzero determinant?,,"['matrices', 'determinant']"
71,Calculating Gramian matrix from Euclidean distance matrix,Calculating Gramian matrix from Euclidean distance matrix,,"For $v_1, \dots, v_n \in \mathbb{R}^n$ we have Euclidean distance matrix $D = (\|v_i - v_j\|^2)_{ij}$ and Gramian matrix $G = (v_i \cdot v_j)_{ij} = V^TV$, where $V = (v_1, \dots, v_n)$. If the $v_i$ are mean-centered ($\sum_i v_i = 0$) then the Gramian matrix can be calculated from the Euclidean distance matrix by $G = -\frac{1}{2}CDC,$ where $C = I - \frac{1}{n}\bf{1}^T \bf{1}$ is the n-by-n centering matrix. What is an elegant way to show this identity? Note: I can prove this, but only by entering ""indices hell"": Write the components of $D$ and $G$ as $d_{ij}$ and $g_{ij}$ respectively. Then: $$d_{ij}=g_{ii}+g_{jj}-2g_{ij}$$ $$\sum_{j} g_{ij} = v_i \cdot (\sum_j v_j) = 0$$ $$\sum_{j}d_{ij} = \sum_{j}d_{ji} = \sum_j (g_{ii} + g_{jj} - 2g_{ij})=ng_{ii}+Tr(G)$$ Let $CD = (h_{ij})_{ij}$ and $CDC = (h'_{ij})_{ij}$. Then $$h_{ij} = d_{ij} - \frac{1}{n}\sum_k d_{kj} = (g_{ii} + g_{jj}-2g_{ij}) - \frac{1}{n}(ng_{jj}+Tr(G))$$ $$= g_{ii}-2g_{ij}-\frac{1}{n}Tr(G)$$ $$\sum_k h_{ik} = n g_{ii}-Tr(G)$$ $$h'_{ij} = h_{ij}-\frac{1}{n}\sum_k h_{ik} = 2g_{ij}$$ and hence $G = \frac{1}{2}CDC$. I've thought about whether one might be able to make use of the Cholesky decomposition of $D$ (since then $G = V^TV$ and $D = L^TL$, so the identity would be $V^TV=\frac{1}{2}H^TL^TLH$ and then perhaps once could relate $V$ to $LH$ somehow), but I'm not sure when $D$ is positive-definite. Also, I am aware of this related question .","For $v_1, \dots, v_n \in \mathbb{R}^n$ we have Euclidean distance matrix $D = (\|v_i - v_j\|^2)_{ij}$ and Gramian matrix $G = (v_i \cdot v_j)_{ij} = V^TV$, where $V = (v_1, \dots, v_n)$. If the $v_i$ are mean-centered ($\sum_i v_i = 0$) then the Gramian matrix can be calculated from the Euclidean distance matrix by $G = -\frac{1}{2}CDC,$ where $C = I - \frac{1}{n}\bf{1}^T \bf{1}$ is the n-by-n centering matrix. What is an elegant way to show this identity? Note: I can prove this, but only by entering ""indices hell"": Write the components of $D$ and $G$ as $d_{ij}$ and $g_{ij}$ respectively. Then: $$d_{ij}=g_{ii}+g_{jj}-2g_{ij}$$ $$\sum_{j} g_{ij} = v_i \cdot (\sum_j v_j) = 0$$ $$\sum_{j}d_{ij} = \sum_{j}d_{ji} = \sum_j (g_{ii} + g_{jj} - 2g_{ij})=ng_{ii}+Tr(G)$$ Let $CD = (h_{ij})_{ij}$ and $CDC = (h'_{ij})_{ij}$. Then $$h_{ij} = d_{ij} - \frac{1}{n}\sum_k d_{kj} = (g_{ii} + g_{jj}-2g_{ij}) - \frac{1}{n}(ng_{jj}+Tr(G))$$ $$= g_{ii}-2g_{ij}-\frac{1}{n}Tr(G)$$ $$\sum_k h_{ik} = n g_{ii}-Tr(G)$$ $$h'_{ij} = h_{ij}-\frac{1}{n}\sum_k h_{ik} = 2g_{ij}$$ and hence $G = \frac{1}{2}CDC$. I've thought about whether one might be able to make use of the Cholesky decomposition of $D$ (since then $G = V^TV$ and $D = L^TL$, so the identity would be $V^TV=\frac{1}{2}H^TL^TLH$ and then perhaps once could relate $V$ to $LH$ somehow), but I'm not sure when $D$ is positive-definite. Also, I am aware of this related question .",,"['linear-algebra', 'matrices']"
72,"Matrices problem: $AB=B$ and $BA=A$, what is $A^2+B^2$?","Matrices problem:  and , what is ?",AB=B BA=A A^2+B^2,"If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$, then $A^2+B^2$ would be equal to?","If $A$ and $B$ are two matrices such that $AB=B$ and $BA=A$, then $A^2+B^2$ would be equal to?",,"['linear-algebra', 'matrices']"
73,Derivative of $\log |AA^T|$ with respect to $A$,Derivative of  with respect to,\log |AA^T| A,"What is the derivative of $\log |AA^T|$ with respect to $A$, where $|A|$ denotes the determinant of $A$?","What is the derivative of $\log |AA^T|$ with respect to $A$, where $|A|$ denotes the determinant of $A$?",,"['matrices', 'derivatives', 'determinant', 'matrix-calculus', 'scalar-fields']"
74,Infinite matrix product,Infinite matrix product,,"Let $$X=\left(\begin{array}{c} x_1 \\ x_2\\ \vdots \end{array}\right)$$ be an infinite real vector and $$A=(a_{ij}), \ 0<i,j<\infty$$ be an infinite real matrix. (1) For which $A$ can one define multiplication $AX$ on the space $\mathbb{R}^\infty$ of all infinite real vectors? (2) What if we restrict $X$ to $Z=\{ X\in \mathbb{R}^\infty| x_n=0 \text{ for all but finitely many }n\}$ My answer is (1) $A$ must be such that for all $i$, $a_{ij}=0$ for all but finitely many $j$. (2) It is well defined for all $A=(a_{ij}), \ 0<i,j<\infty$ Is that correct?","Let $$X=\left(\begin{array}{c} x_1 \\ x_2\\ \vdots \end{array}\right)$$ be an infinite real vector and $$A=(a_{ij}), \ 0<i,j<\infty$$ be an infinite real matrix. (1) For which $A$ can one define multiplication $AX$ on the space $\mathbb{R}^\infty$ of all infinite real vectors? (2) What if we restrict $X$ to $Z=\{ X\in \mathbb{R}^\infty| x_n=0 \text{ for all but finitely many }n\}$ My answer is (1) $A$ must be such that for all $i$, $a_{ij}=0$ for all but finitely many $j$. (2) It is well defined for all $A=(a_{ij}), \ 0<i,j<\infty$ Is that correct?",,"['linear-algebra', 'matrices', 'vector-spaces', 'infinite-matrices']"
75,Prove a covariance matrix is positive semidefinite,Prove a covariance matrix is positive semidefinite,,"Given a random vector c with zero mean, the covariance matrix $\Sigma = E[cc^T]$. The following steps were given to prove that it is positive semidefinite. $u^T\Sigma u = u^TE[cc^T]u = E[u^Tcc^Tu] = ||u^Tc|| \ge 0$ I don't understand how the expectation can equate to a norm.","Given a random vector c with zero mean, the covariance matrix $\Sigma = E[cc^T]$. The following steps were given to prove that it is positive semidefinite. $u^T\Sigma u = u^TE[cc^T]u = E[u^Tcc^Tu] = ||u^Tc|| \ge 0$ I don't understand how the expectation can equate to a norm.",,"['linear-algebra', 'matrices', 'positive-definite']"
76,nilpotent endomorphism on finitely generated modules over a domain,nilpotent endomorphism on finitely generated modules over a domain,,"If $R$ is a domain and $f: R^n \to R^n$ is an $R$-module endomorphism. Suppose $f^m = 0$ for some $m> 0$. Show that $f^n = 0$. The cases $ m \le n$ is trivial. When $m>n$, I don't have much idea how to start. I tried to apply the Cayley-Hamilton theorem but doesn't seem to help much.","If $R$ is a domain and $f: R^n \to R^n$ is an $R$-module endomorphism. Suppose $f^m = 0$ for some $m> 0$. Show that $f^n = 0$. The cases $ m \le n$ is trivial. When $m>n$, I don't have much idea how to start. I tried to apply the Cayley-Hamilton theorem but doesn't seem to help much.",,"['linear-algebra', 'matrices', 'modules', 'integral-domain', 'nilpotence']"
77,Determinant of an $n\times n$ Toeplitz matrix,Determinant of an  Toeplitz matrix,n\times n,Let $A = (a_{ij}) \in R^{n\times n}$. Find the determinant if: $$a_{ij}= |i-j|$$ So we have the symmetric matrix \begin{bmatrix} 0 & 1 & 2 & 3 & 4 & \dots & n-1 \\ 1 & 0 & 1 & 2 & 3 & \dots & n-2 \\ 2 & 1 & 0 & 1 & 2 & \dots & n-3 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ n-1 & n-2 & n-3 & \dots & \dots & \dots & 0 \end{bmatrix} But i can't find a way to diagonalize the matrix nor find the determinant by Laplace expansion. Any ideas ??,Let $A = (a_{ij}) \in R^{n\times n}$. Find the determinant if: $$a_{ij}= |i-j|$$ So we have the symmetric matrix \begin{bmatrix} 0 & 1 & 2 & 3 & 4 & \dots & n-1 \\ 1 & 0 & 1 & 2 & 3 & \dots & n-2 \\ 2 & 1 & 0 & 1 & 2 & \dots & n-3 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ n-1 & n-2 & n-3 & \dots & \dots & \dots & 0 \end{bmatrix} But i can't find a way to diagonalize the matrix nor find the determinant by Laplace expansion. Any ideas ??,,"['linear-algebra', 'matrices', 'determinant', 'toeplitz-matrices']"
78,"Is the function $f(x)=x^TAx$ convex when $A \in S^n,A\geq0$,$x\in R^n$?","Is the function  convex when ,?","f(x)=x^TAx A \in S^n,A\geq0 x\in R^n","Is the function $f(x)=x^TAx$ convex  when $A \in S^n,A\geq0$,$x\in R^n$? Notation $S^n$: Symmetric $n$ x $n$ matrix. $R^n$: Column vector $n$x$1$ $A \geq 0$: $A$ is positive semi-definite matrix I know that $x^TAx$ is  convex  when $A \in S^n,S\geq0$,$x\in R^n$ but I not sure why exactly this is happening. Can somebody elaborate on it? Is it safe to say that when $A\geq 0$ and $x$ is non negative the function $x^TAx$ is a parabola in $R^n$ towards $+\infty$. I mean in $R^2$  $f(x)=ax^2+bx+c$ is a parabola. But is there parabola in $R^n$ for example in $R^3$, is $f(x,y)=ax^2+by^2+cxy +d$  a parabola on $R^3$ or there is no such thing?","Is the function $f(x)=x^TAx$ convex  when $A \in S^n,A\geq0$,$x\in R^n$? Notation $S^n$: Symmetric $n$ x $n$ matrix. $R^n$: Column vector $n$x$1$ $A \geq 0$: $A$ is positive semi-definite matrix I know that $x^TAx$ is  convex  when $A \in S^n,S\geq0$,$x\in R^n$ but I not sure why exactly this is happening. Can somebody elaborate on it? Is it safe to say that when $A\geq 0$ and $x$ is non negative the function $x^TAx$ is a parabola in $R^n$ towards $+\infty$. I mean in $R^2$  $f(x)=ax^2+bx+c$ is a parabola. But is there parabola in $R^n$ for example in $R^3$, is $f(x,y)=ax^2+by^2+cxy +d$  a parabola on $R^3$ or there is no such thing?",,"['linear-algebra', 'matrices', 'convex-analysis', 'matrix-equations']"
79,Why is the Householder matrix orthogonal?,Why is the Householder matrix orthogonal?,,"A Householder matrix $H = I - c u u^T$ , where $c$ is a constant and $u$ is a unit vector, always comes out orthogonal and full rank. Why is $H$ orthogonal? I am looking for an intuitive proof rather than a rigorous one. How come it is full rank when $\mbox{rank} \left( u u^T \right) = \mbox{rank} (u) = 1$ ?","A Householder matrix , where is a constant and is a unit vector, always comes out orthogonal and full rank. Why is orthogonal? I am looking for an intuitive proof rather than a rigorous one. How come it is full rank when ?",H = I - c u u^T c u H \mbox{rank} \left( u u^T \right) = \mbox{rank} (u) = 1,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'reflection']"
80,Prove $\det(A - nI_n) = 0$.,Prove .,\det(A - nI_n) = 0,"Problem: Prove that $\det(A - n I_n) = 0$ when $A$ is the $(n \times n)$-matrix with all components equal to $1$. Attempt at solution: I tried to use Laplace expansion but that didn't work. I see the matrix will be of the form \begin{align*} \begin{pmatrix} 1-n & 1 & \cdots & 1 \\ 1 & 1-n & \cdots & 1 \\ \vdots \\ 1 & 1 & \cdots & 1-n \end{pmatrix} \end{align*} I want to somehow get two equal rows or columns here, or a row/column of zero using elementary operations. But I don't see what I should do?","Problem: Prove that $\det(A - n I_n) = 0$ when $A$ is the $(n \times n)$-matrix with all components equal to $1$. Attempt at solution: I tried to use Laplace expansion but that didn't work. I see the matrix will be of the form \begin{align*} \begin{pmatrix} 1-n & 1 & \cdots & 1 \\ 1 & 1-n & \cdots & 1 \\ \vdots \\ 1 & 1 & \cdots & 1-n \end{pmatrix} \end{align*} I want to somehow get two equal rows or columns here, or a row/column of zero using elementary operations. But I don't see what I should do?",,"['linear-algebra', 'matrices', 'determinant']"
81,Set of all orthogonal matrices over $\mathbb C$ is compact/not,Set of all orthogonal matrices over  is compact/not,\mathbb C,How to show the fact that the set of all orthogonal matrices over $\mathbb C$ is compact By an orthogonal matrix over $\mathbb C$ I mean a matrix $A$ satisfying $AA^T=I$ and here $A^T=(a_{ji})$ where $A=(a_{ij})$ It is not the same as unitary matrix where in unitary matrix we take transpose and then conjugate or vice versa I know that set of all orthogonal matrices over $\mathbb R$ is compact. I think the closedness of the set will follow from the same arguements as in the above case. But the boundedness part not sure,How to show the fact that the set of all orthogonal matrices over $\mathbb C$ is compact By an orthogonal matrix over $\mathbb C$ I mean a matrix $A$ satisfying $AA^T=I$ and here $A^T=(a_{ji})$ where $A=(a_{ij})$ It is not the same as unitary matrix where in unitary matrix we take transpose and then conjugate or vice versa I know that set of all orthogonal matrices over $\mathbb R$ is compact. I think the closedness of the set will follow from the same arguements as in the above case. But the boundedness part not sure,,"['matrices', 'compactness']"
82,Why does Gaussian elimination not preserve similarity of a matrix?,Why does Gaussian elimination not preserve similarity of a matrix?,,"I am trying to understand reduction of an unsymmetric real square matrix to Hessenberg form from Numerical Recipes Vol. 3. In it, the author states that one does not use Gaussian elimination for reducing to Hessenberg form because the Gaussian elimination does not preserve similarity and hence ends up changing the eigenvalues of the matrix, which is undesirable. Why does Gaussian elimination not preserve similarity? Also, what conditions decide whether a particular matrix transformation preserves similarity? Heres a Google Books link to where I found it. In case you cant view that, it's on page 594 of Numerical Recipes in C++ (Volume 3).","I am trying to understand reduction of an unsymmetric real square matrix to Hessenberg form from Numerical Recipes Vol. 3. In it, the author states that one does not use Gaussian elimination for reducing to Hessenberg form because the Gaussian elimination does not preserve similarity and hence ends up changing the eigenvalues of the matrix, which is undesirable. Why does Gaussian elimination not preserve similarity? Also, what conditions decide whether a particular matrix transformation preserves similarity? Heres a Google Books link to where I found it. In case you cant view that, it's on page 594 of Numerical Recipes in C++ (Volume 3).",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
83,"If $A=\pmatrix{1 &0\\-1&1}$, show that $A^2-2A+I_2=0$. Hence find $A^{50}$","If , show that . Hence find",A=\pmatrix{1 &0\\-1&1} A^2-2A+I_2=0 A^{50},"If $$A=\pmatrix{1 &0\\-1&1},$$ show that $$A^2-2A+I_2=0,$$ where $I_{2}$ is the $2x2$ Identity matrix. Hence find $A^{50}$. We have $$A^2-2A+I_2=A(A-2I_3)+I_=\pmatrix{1 &0\\-1&1}\pmatrix{-1 &0\\-1&-1}+I_2   =-I_2+I_2=0.$$ How can I show the second part?","If $$A=\pmatrix{1 &0\\-1&1},$$ show that $$A^2-2A+I_2=0,$$ where $I_{2}$ is the $2x2$ Identity matrix. Hence find $A^{50}$. We have $$A^2-2A+I_2=A(A-2I_3)+I_=\pmatrix{1 &0\\-1&1}\pmatrix{-1 &0\\-1&-1}+I_2   =-I_2+I_2=0.$$ How can I show the second part?",,['matrices']
84,Determinant-like expression for non-square matrices,Determinant-like expression for non-square matrices,,"I'm interested in whether for any real matrix of size $m \times n$ there is a real number with the following properties: It is a polynomial expression with real coefficients in the entries of the matrix. The expression depends on $m,n$ only. It is zero precisely when the matrix is not of full rank ($\min\left\{m,n\right\}$). For square matrices, the determinant has these properties. If this is a known thing, what is it called and where can I read about it?","I'm interested in whether for any real matrix of size $m \times n$ there is a real number with the following properties: It is a polynomial expression with real coefficients in the entries of the matrix. The expression depends on $m,n$ only. It is zero precisely when the matrix is not of full rank ($\min\left\{m,n\right\}$). For square matrices, the determinant has these properties. If this is a known thing, what is it called and where can I read about it?",,"['linear-algebra', 'matrices', 'determinant']"
85,Determinant involving recurrence,Determinant involving recurrence,,"Evaluate $$\left| A \right| = \left| {\matrix{    {x + y} & {xy} & 0 &  \cdots  &  \cdots  & 0  \cr     1 & {x + y} & {xy} &  \cdots  &  \cdots  & 0  \cr     0 & 1 & {x + y} &  \cdots  &  \cdots  & 0  \cr      \cdots  &  \cdots  &  \cdots  &  \cdots  &  \cdots  &  \vdots   \cr     0 &  \cdots  & 0 & 1 & {x + y} & {xy}  \cr     0 &  \cdots  & 0 & 0 & 1 & {x + y}  \cr   } } \right|$$ And show that $\det(A) = \frac{x^{n+1}-y^{n+1}}{x-y}$ if $x\ne y$ and $\det(A) = (n+1)x^n$ if $x=y$. I actually was able to get this recurrence formula: $$D_n = (x+y)\cdot D_{n-1} + xy\cdot D_{n-2}$$ I tried to prove it by induciton, but the algebric calculation didn't bring me to the desired result.","Evaluate $$\left| A \right| = \left| {\matrix{    {x + y} & {xy} & 0 &  \cdots  &  \cdots  & 0  \cr     1 & {x + y} & {xy} &  \cdots  &  \cdots  & 0  \cr     0 & 1 & {x + y} &  \cdots  &  \cdots  & 0  \cr      \cdots  &  \cdots  &  \cdots  &  \cdots  &  \cdots  &  \vdots   \cr     0 &  \cdots  & 0 & 1 & {x + y} & {xy}  \cr     0 &  \cdots  & 0 & 0 & 1 & {x + y}  \cr   } } \right|$$ And show that $\det(A) = \frac{x^{n+1}-y^{n+1}}{x-y}$ if $x\ne y$ and $\det(A) = (n+1)x^n$ if $x=y$. I actually was able to get this recurrence formula: $$D_n = (x+y)\cdot D_{n-1} + xy\cdot D_{n-2}$$ I tried to prove it by induciton, but the algebric calculation didn't bring me to the desired result.",,"['linear-algebra', 'matrices', 'induction', 'recurrence-relations', 'determinant']"
86,Hadamard and tensor products: $\operatorname{rank}(A\circ B)\le\operatorname{rank}(A)\operatorname{rank}(B)=\operatorname{rank}(A\otimes B)$,Hadamard and tensor products:,\operatorname{rank}(A\circ B)\le\operatorname{rank}(A)\operatorname{rank}(B)=\operatorname{rank}(A\otimes B),"$\newcommand{\rank}{\operatorname{rank}}$ For two matrices $A$ and $B$, the Hadamard product $A \circ B$ is the matrix obtained by $(A \circ B)_{i, j} = A_{i, j} B_{i, j}$. Prove that $\rank(A \circ B) \le \rank(A) \rank(B) = \rank (A \otimes B)$ The first inequality is quite easy but I can't seem to put it into words (so maybe it isn't). Any help on either would be much appreciated.","$\newcommand{\rank}{\operatorname{rank}}$ For two matrices $A$ and $B$, the Hadamard product $A \circ B$ is the matrix obtained by $(A \circ B)_{i, j} = A_{i, j} B_{i, j}$. Prove that $\rank(A \circ B) \le \rank(A) \rank(B) = \rank (A \otimes B)$ The first inequality is quite easy but I can't seem to put it into words (so maybe it isn't). Any help on either would be much appreciated.",,"['linear-algebra', 'matrices', 'tensor-products', 'matrix-rank', 'hadamard-product']"
87,"Is there a way in matrix math notation to show the 'flip up-down', and 'flip left-right' of a matrix?","Is there a way in matrix math notation to show the 'flip up-down', and 'flip left-right' of a matrix?",,Title says it all - is there an accepted mathematical way in matrix notation to show those operations on a matrix? Thanks.,Title says it all - is there an accepted mathematical way in matrix notation to show those operations on a matrix? Thanks.,,"['linear-algebra', 'matrices', 'notation', 'terminology']"
88,"Find all non-singular $3 \times 3$ matrices, such as $A$ and $A^{-1}$ elements are non-negative","Find all non-singular  matrices, such as  and  elements are non-negative",3 \times 3 A A^{-1},"Task is to describe all non-singular $3 \times 3$ matrices $A$  for which holds:  all elements of $A$ and $A^{-1}$ is non-negative. As I discovered linear algebra is  the most problematic part of math for me. Expect to get better with a bit of your help. I have trouble even approaching the problem, would you provide a slight hint where to go?","Task is to describe all non-singular $3 \times 3$ matrices $A$  for which holds:  all elements of $A$ and $A^{-1}$ is non-negative. As I discovered linear algebra is  the most problematic part of math for me. Expect to get better with a bit of your help. I have trouble even approaching the problem, would you provide a slight hint where to go?",,"['linear-algebra', 'matrices']"
89,Why and When is a determinant of a larger matrix equal to a determinant of a smaller matrix?,Why and When is a determinant of a larger matrix equal to a determinant of a smaller matrix?,,The following is written in the solution of my textbook. $$|A|= \left| \begin{array} {cccc}  1 & 2& -1& 4 \\ 0& 5& -1& 6 \\ 0& -3& 3& -6 \\ 0& 2& 2& -1\\ \end{array} \right| = \left| \begin{array} {ccc}  5& -1& 6 \\ -3& 3& -6 \\ 2& 2& -1\\ \end{array} \right|$$ where $A=\left[ \begin{array} {cccc}  1 & 2& -1& 4 \\ 0& 5& -1& 6 \\ 0& -3& 3& -6 \\ 0& 2& 2& -1\\ \end{array} \right]$ I can see that we are ignoring column 1 and row 1 of $A$ to compute the determinant. But I don't understand why this is a valid operation. Can someone please show me what are the properties of determinants/matrices that are being used to justify this?,The following is written in the solution of my textbook. $$|A|= \left| \begin{array} {cccc}  1 & 2& -1& 4 \\ 0& 5& -1& 6 \\ 0& -3& 3& -6 \\ 0& 2& 2& -1\\ \end{array} \right| = \left| \begin{array} {ccc}  5& -1& 6 \\ -3& 3& -6 \\ 2& 2& -1\\ \end{array} \right|$$ where $A=\left[ \begin{array} {cccc}  1 & 2& -1& 4 \\ 0& 5& -1& 6 \\ 0& -3& 3& -6 \\ 0& 2& 2& -1\\ \end{array} \right]$ I can see that we are ignoring column 1 and row 1 of $A$ to compute the determinant. But I don't understand why this is a valid operation. Can someone please show me what are the properties of determinants/matrices that are being used to justify this?,,"['matrices', 'determinant']"
90,Symmetric matrix under orthogonal transformation still symmetric?,Symmetric matrix under orthogonal transformation still symmetric?,,"is there any way to see that a symmetric matrix is still symmetric after applying an orthogonal basis transformation to it? I would say that a proof that refers to the entries of the matrix may be cumbersome, therefore I am asking here for clever ways to show this.","is there any way to see that a symmetric matrix is still symmetric after applying an orthogonal basis transformation to it? I would say that a proof that refers to the entries of the matrix may be cumbersome, therefore I am asking here for clever ways to show this.",,['linear-algebra']
91,Rotation Matrix inverse using Gauss-Jordan elimination,Rotation Matrix inverse using Gauss-Jordan elimination,,"I'd like to calculate the inverse of a rotation matrix, let take the simplest case which is a $2$ x $2$ rotation matrix: $R =\begin{bmatrix} \cos \theta & -\sin \theta \\[0.3em] \sin \theta & \cos \theta \end{bmatrix}$ I know that the inverse is the following $R^{-1} =\begin{bmatrix} \cos \theta & \sin \theta \\[0.3em] -\sin \theta & \cos \theta \end{bmatrix}$ and I know that I can calculate it using the transpose method as such: $R^{-1}=R^T$ but I fail to calculate the inverse using $Gauss-Jordan$ elimination, that is I don't know how to substract $\cos \theta$ from $\sin \theta$ in the second row. It all gets a bit complicated; I've looked around and nobody has a full step method using $G.-J.$ only the solution or the transpose method. Could someone provide me a full-step solution using $G.-J.$?","I'd like to calculate the inverse of a rotation matrix, let take the simplest case which is a $2$ x $2$ rotation matrix: $R =\begin{bmatrix} \cos \theta & -\sin \theta \\[0.3em] \sin \theta & \cos \theta \end{bmatrix}$ I know that the inverse is the following $R^{-1} =\begin{bmatrix} \cos \theta & \sin \theta \\[0.3em] -\sin \theta & \cos \theta \end{bmatrix}$ and I know that I can calculate it using the transpose method as such: $R^{-1}=R^T$ but I fail to calculate the inverse using $Gauss-Jordan$ elimination, that is I don't know how to substract $\cos \theta$ from $\sin \theta$ in the second row. It all gets a bit complicated; I've looked around and nobody has a full step method using $G.-J.$ only the solution or the transpose method. Could someone provide me a full-step solution using $G.-J.$?",,['matrices']
92,Finding the inverse of a matrix by elementary transformations.,Finding the inverse of a matrix by elementary transformations.,,"While using the elementary transformation method to find the inverse of a matrix, our goal is to convert the given matrix into an identity matrix. We can use  three transformations:- 1) Multiplying a row by a constant 2) Adding a multiple of another row 3) Swapping two rows The thing is, I can't seem to figure out what to do to achieve that identity matrix. There are so many steps which I can start off with, but how do I know which one to do? I think of one step to get a certain position to a $1$ or a $0$, and then get a new matrix. Now again there are so many options, it's boggling. Is there some specific procedure to be followed? Like, first convert the top row into: \begin{bmatrix} 1&0&0\\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{bmatrix} Then do the second row and then the third? What do I start off with? I hope I've made my question clear enough. Thanks to @Brian M. Scott. $P.S:$ Does anyone have any other methods? Brian's works perfectly, but it's always great to know more than one method. :)","While using the elementary transformation method to find the inverse of a matrix, our goal is to convert the given matrix into an identity matrix. We can use  three transformations:- 1) Multiplying a row by a constant 2) Adding a multiple of another row 3) Swapping two rows The thing is, I can't seem to figure out what to do to achieve that identity matrix. There are so many steps which I can start off with, but how do I know which one to do? I think of one step to get a certain position to a $1$ or a $0$, and then get a new matrix. Now again there are so many options, it's boggling. Is there some specific procedure to be followed? Like, first convert the top row into: \begin{bmatrix} 1&0&0\\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{bmatrix} Then do the second row and then the third? What do I start off with? I hope I've made my question clear enough. Thanks to @Brian M. Scott. $P.S:$ Does anyone have any other methods? Brian's works perfectly, but it's always great to know more than one method. :)",,"['matrices', 'inverse']"
93,How to generate unique id from each element in matrix?,How to generate unique id from each element in matrix?,,"I'm coming from the programming world , and I need to create unique number for each element in a matrix. Say I have a $4\times4$ matrix $A$. I want to find a simple formula that will give each of the $16$ elements a unique number id.  Can you suggest me where to start ?","I'm coming from the programming world , and I need to create unique number for each element in a matrix. Say I have a $4\times4$ matrix $A$. I want to find a simple formula that will give each of the $16$ elements a unique number id.  Can you suggest me where to start ?",,"['matrices', 'functions', 'algorithms']"
94,Projection matrix onto null space,Projection matrix onto null space,,I have a matrix $H$ and I want to find the projection matrix onto null space. How can I do this?,I have a matrix $H$ and I want to find the projection matrix onto null space. How can I do this?,,"['linear-algebra', 'matrices', 'matlab']"
95,about transpose of matrix,about transpose of matrix,,"Let $A$ be a real $n\times n$ matrix with $A^{T}=\alpha_{0}I+\alpha_{1}A$, where $\alpha_{0}$ and $\alpha_{1}$ are real numbers. Show that either $A^{T}=\pm A$ or $A=\lambda I$ for some real number $\lambda$. Can someone give me hint?","Let $A$ be a real $n\times n$ matrix with $A^{T}=\alpha_{0}I+\alpha_{1}A$, where $\alpha_{0}$ and $\alpha_{1}$ are real numbers. Show that either $A^{T}=\pm A$ or $A=\lambda I$ for some real number $\lambda$. Can someone give me hint?",,"['linear-algebra', 'matrices']"
96,For which $t \in \mathbb{R}$ is matrix diagonalizable,For which  is matrix diagonalizable,t \in \mathbb{R},"$\begin{bmatrix} 1 & t & 25 \\  0 & t & t+1 \\  0 & 0 & -1 \end{bmatrix}$ I calculated the characteristic polynomial which is $p(\lambda) = (\lambda^2 - 1)(t-\lambda)$. But how do I find specific $t$, for which the matrix is not diagonalizable?","$\begin{bmatrix} 1 & t & 25 \\  0 & t & t+1 \\  0 & 0 & -1 \end{bmatrix}$ I calculated the characteristic polynomial which is $p(\lambda) = (\lambda^2 - 1)(t-\lambda)$. But how do I find specific $t$, for which the matrix is not diagonalizable?",,"['linear-algebra', 'matrices', 'diagonalization']"
97,number of 8 x 8 matrices with specific conditions,number of 8 x 8 matrices with specific conditions,,How can we find the number of 8 by 8 matrices in which each entry is 0 or 1. In addition each row and each column contains odd number of 1's. Thanks for help.,How can we find the number of 8 by 8 matrices in which each entry is 0 or 1. In addition each row and each column contains odd number of 1's. Thanks for help.,,"['linear-algebra', 'matrices']"
98,On similar matrices and polynomial matrices,On similar matrices and polynomial matrices,,"Let $A,B,P\in M_n(F)$. Suppose that $A$ and $B$ are similar, thus $A=P^{-1}BP$. If $p(x)=a_0+\ldots+a_nx^n$, and $T:V\to V$ be a linear transformation. Defining $$p(T)=a_0I+\ldots+a_nIT^n$$ How to prove that $$p(A)=P^{-1}p(B)P$$ Thank you very much for any help.","Let $A,B,P\in M_n(F)$. Suppose that $A$ and $B$ are similar, thus $A=P^{-1}BP$. If $p(x)=a_0+\ldots+a_nx^n$, and $T:V\to V$ be a linear transformation. Defining $$p(T)=a_0I+\ldots+a_nIT^n$$ How to prove that $$p(A)=P^{-1}p(B)P$$ Thank you very much for any help.",,"['linear-algebra', 'matrices', 'polynomials']"
99,Is the sum or product of idempotent matrices idempotent?,Is the sum or product of idempotent matrices idempotent?,,"If you have two idempotent matrices $A$ and $B$, is $A+B$ an idempotent matrix? Also, is $AB$ an idempotent Matrix? If both are true, Can I see the proof? I am completley lost in how to prove both cases. Thanks!","If you have two idempotent matrices $A$ and $B$, is $A+B$ an idempotent matrix? Also, is $AB$ an idempotent Matrix? If both are true, Can I see the proof? I am completley lost in how to prove both cases. Thanks!",,"['linear-algebra', 'matrices']"
