,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Verifying an Epsilon-Delta Limit,Verifying an Epsilon-Delta Limit,,"I have to prove that  $$\lim_{x \to a} \frac{1}{\sqrt{x}}=\frac{1}{\sqrt{a}}$$ where x>0 and a>0. So given $\epsilon>0$, I let $\delta<a/2$. Then: $$|x-a|<\delta \\ -\delta<x-a<\delta \\ a-\delta<x<a+\delta \\ a/2<x<3a/2$$ So now $$|1/(\sqrt{x})-1/(\sqrt{a})| =|(\sqrt{a}-\sqrt{x})/(\sqrt{xa})|\\ =|(\sqrt{a}-\sqrt{x})(\sqrt{a}+\sqrt{x})/(\sqrt{xc}(\sqrt{a}+\sqrt{x}))| \\ =|(x-a)/(\sqrt{xa}(\sqrt{a}+\sqrt{x}))| \\ <|(x-a)/(\sqrt{xa}(\sqrt{a})| \\ <\delta/(\sqrt{a/2*a}\sqrt{a})\\ <2\delta/(a^{3/2})<\epsilon$$ so given any $\epsilon>0$, let $\delta<min(a/2, \epsilon*a^{3/2}/2)$ so now if $\delta<\epsilon*a^{3/2}/2<a/2$ $|1/(\sqrt{x})-1/(\sqrt{a})|<2\delta/(a^{3/2})<2\epsilon*a^{3/2}/(2a^{3/2})=\epsilon$, and now if $\delta<a/2<\epsilon*a^{3/2}/2$, then $|1/(\sqrt{x})-1/(\sqrt{a})|<2\delta/(a^{3/2})<2(a/2)/a^{3/2}<2\epsilon*a^{3/2}/(2a^{3/2})=\epsilon$ Does this proof work?  Thanks","I have to prove that  $$\lim_{x \to a} \frac{1}{\sqrt{x}}=\frac{1}{\sqrt{a}}$$ where x>0 and a>0. So given $\epsilon>0$, I let $\delta<a/2$. Then: $$|x-a|<\delta \\ -\delta<x-a<\delta \\ a-\delta<x<a+\delta \\ a/2<x<3a/2$$ So now $$|1/(\sqrt{x})-1/(\sqrt{a})| =|(\sqrt{a}-\sqrt{x})/(\sqrt{xa})|\\ =|(\sqrt{a}-\sqrt{x})(\sqrt{a}+\sqrt{x})/(\sqrt{xc}(\sqrt{a}+\sqrt{x}))| \\ =|(x-a)/(\sqrt{xa}(\sqrt{a}+\sqrt{x}))| \\ <|(x-a)/(\sqrt{xa}(\sqrt{a})| \\ <\delta/(\sqrt{a/2*a}\sqrt{a})\\ <2\delta/(a^{3/2})<\epsilon$$ so given any $\epsilon>0$, let $\delta<min(a/2, \epsilon*a^{3/2}/2)$ so now if $\delta<\epsilon*a^{3/2}/2<a/2$ $|1/(\sqrt{x})-1/(\sqrt{a})|<2\delta/(a^{3/2})<2\epsilon*a^{3/2}/(2a^{3/2})=\epsilon$, and now if $\delta<a/2<\epsilon*a^{3/2}/2$, then $|1/(\sqrt{x})-1/(\sqrt{a})|<2\delta/(a^{3/2})<2(a/2)/a^{3/2}<2\epsilon*a^{3/2}/(2a^{3/2})=\epsilon$ Does this proof work?  Thanks",,"['real-analysis', 'epsilon-delta']"
1,"Weak Law of Large Numbers for a non-iid, non-ergodic sequence","Weak Law of Large Numbers for a non-iid, non-ergodic sequence",,"I have a somewhat open-ended question. Let's say I have a sequence of random variables $(X_n: n \geq 1)$ which are neither independent, ergodic, nor identically distributed . Normally I would say that I am completely dead in the water, but let's say that $X_n \overset{d}{\to} X$ . Are there any additional assumptions under which I can say that: $$ \frac{1}{n} \sum_{i=1}^n X_n \;\overset{P}{\to}\; \mathbb{E}X $$ Even if I assume that expectation of the left-hand side converges to $\mathbb{E}X$, I'm stuck thinking about this more generally. Any tips? EDIT: Thinking about this some more, I feel like making a martingale out of the LHS and then checking under what conditions we have the desired martingale convergence would be a reasonable route to follow. Any thoughts on this? EDIT 2: per Nate Eldredge's comment below, I need to assume that the expectation of the LHS of the partial-sum object converges to $EX$... it doesn't follow from $X_n \overset{d}{\to} X$.","I have a somewhat open-ended question. Let's say I have a sequence of random variables $(X_n: n \geq 1)$ which are neither independent, ergodic, nor identically distributed . Normally I would say that I am completely dead in the water, but let's say that $X_n \overset{d}{\to} X$ . Are there any additional assumptions under which I can say that: $$ \frac{1}{n} \sum_{i=1}^n X_n \;\overset{P}{\to}\; \mathbb{E}X $$ Even if I assume that expectation of the left-hand side converges to $\mathbb{E}X$, I'm stuck thinking about this more generally. Any tips? EDIT: Thinking about this some more, I feel like making a martingale out of the LHS and then checking under what conditions we have the desired martingale convergence would be a reasonable route to follow. Any thoughts on this? EDIT 2: per Nate Eldredge's comment below, I need to assume that the expectation of the LHS of the partial-sum object converges to $EX$... it doesn't follow from $X_n \overset{d}{\to} X$.",,"['real-analysis', 'probability', 'probability-theory', 'convergence-divergence', 'law-of-large-numbers']"
2,Give an example of an infinite compact set $A$ such that its supremum is not a limit point,Give an example of an infinite compact set  such that its supremum is not a limit point,A,"I got this one on a quiz the other day (We're only working in the reals). My solution was $$A=[0,1]\cup\{3\}$$ The closed interval has the infinite points, and $\sup A=3$ is not a limit-point since each neighborhood of $3$ contains only the point $3$. I was marked wrong but I can't figure out why.  Also let me know if there's any issues with my posting-- it's my first on here. Thanks for the input.","I got this one on a quiz the other day (We're only working in the reals). My solution was $$A=[0,1]\cup\{3\}$$ The closed interval has the infinite points, and $\sup A=3$ is not a limit-point since each neighborhood of $3$ contains only the point $3$. I was marked wrong but I can't figure out why.  Also let me know if there's any issues with my posting-- it's my first on here. Thanks for the input.",,"['real-analysis', 'limits', 'compactness']"
3,"Limit theorems, prove function has a limit at every point","Limit theorems, prove function has a limit at every point",,"Suppose that $f:R\to R$ is a function such that $f(x+y)=f(x)+f(y)$ for all $x,y∈R$. Assume that $f$ has a limit at $0$, $f(1)=1$. Prove that $f(x)=x$ for all $x \in R$ Hint: Show first that $f$ is continuous at any point $c ∈ R$. Prove then that for any rational number $r$, $f(r) = r$, and deduce the statement using the continuity of $f$. My attempt: To show continuity at any point $c\in R$ $$\lim_{x \to c}f(x)=\lim_{ h\to0}f(c+h)=\lim_{h\to0}f(h)+f(c)=f(c)$$ Suppose $\lim_{x\to0}f(x)$ exists (as given). We need to show that for $c∈R$ arbitrary, also $\lim_{x\to c}f(x)$ exists. We have $$f(x)=f(x−c+c)=f(x−c)+f(c)$$ Letting $x→c$ and setting $y=x−c$ we see that $y→0$, so the limit in question exists by assumption and equals $$\lim_{x→c}f(x)=f(c)+\lim_{y→0}f(y)$$ which implies $f(x)=x$ QED My question is how to use $f(1)=1$ Then how to prove continuity for any rational number?","Suppose that $f:R\to R$ is a function such that $f(x+y)=f(x)+f(y)$ for all $x,y∈R$. Assume that $f$ has a limit at $0$, $f(1)=1$. Prove that $f(x)=x$ for all $x \in R$ Hint: Show first that $f$ is continuous at any point $c ∈ R$. Prove then that for any rational number $r$, $f(r) = r$, and deduce the statement using the continuity of $f$. My attempt: To show continuity at any point $c\in R$ $$\lim_{x \to c}f(x)=\lim_{ h\to0}f(c+h)=\lim_{h\to0}f(h)+f(c)=f(c)$$ Suppose $\lim_{x\to0}f(x)$ exists (as given). We need to show that for $c∈R$ arbitrary, also $\lim_{x\to c}f(x)$ exists. We have $$f(x)=f(x−c+c)=f(x−c)+f(c)$$ Letting $x→c$ and setting $y=x−c$ we see that $y→0$, so the limit in question exists by assumption and equals $$\lim_{x→c}f(x)=f(c)+\lim_{y→0}f(y)$$ which implies $f(x)=x$ QED My question is how to use $f(1)=1$ Then how to prove continuity for any rational number?",,"['real-analysis', 'limits', 'functions', 'continuity']"
4,Family of Morse functions made constant,Family of Morse functions made constant,,"I'm looking for a proof of the following theorem: Let $f_t$ be a family of real-valued Morse functions defined on a smooth compact manifold $M$, and where $t$ is in $[0,1]$ (So for all value of $t$, $f_t$ is Morse). Also, $f_t$ depends smoothly on $t$.  Then, it is possible to find $\phi_t$ a family of diffeomorphism on $M$, and $\psi_t$ a family of diffeomorphism on $\mathbb{R}$, with $t$ in $[0,1]$ such that, $\phi_t$ and $\psi_t$ depends smoothly on $t$, and $\forall t \in [0,1] \psi_t \circ f_t \circ \phi_t = f_0$. Does anyone know where to find a proof of this result? Thank you for your help!","I'm looking for a proof of the following theorem: Let $f_t$ be a family of real-valued Morse functions defined on a smooth compact manifold $M$, and where $t$ is in $[0,1]$ (So for all value of $t$, $f_t$ is Morse). Also, $f_t$ depends smoothly on $t$.  Then, it is possible to find $\phi_t$ a family of diffeomorphism on $M$, and $\psi_t$ a family of diffeomorphism on $\mathbb{R}$, with $t$ in $[0,1]$ such that, $\phi_t$ and $\psi_t$ depends smoothly on $t$, and $\forall t \in [0,1] \psi_t \circ f_t \circ \phi_t = f_0$. Does anyone know where to find a proof of this result? Thank you for your help!",,"['real-analysis', 'general-topology', 'homology-cohomology', 'morse-theory']"
5,inequality $ \prod_{n=2}^{\infty}n^{\zeta(n)-1} <\frac{\pi^2+6}{6}$,inequality, \prod_{n=2}^{\infty}n^{\zeta(n)-1} <\frac{\pi^2+6}{6},"Let $ \zeta(s) $be the riemann zeta function, then  $$ \prod_{n=2}^{\infty}n^{\zeta(n)-1} <1+\frac{\pi^2}{6}$$ The problem is difficult, I don't know how to go started Thank you very much!","Let $ \zeta(s) $be the riemann zeta function, then  $$ \prod_{n=2}^{\infty}n^{\zeta(n)-1} <1+\frac{\pi^2}{6}$$ The problem is difficult, I don't know how to go started Thank you very much!",,"['calculus', 'real-analysis', 'analysis']"
6,"Definition of ""the surface measure""?","Definition of ""the surface measure""?",,"Let $\mu_n$ be the $n$-dimensional Lebesgue measure. Let $||\cdot||$ be a norm on $\mathbb{R}^n$. Define $S^{n-1}=\{x\in\mathbb{R}:||x||=1\}$. I have proven that $\forall A\in\mathscr{B}_{S^{n-1}}, (0,1]A\in \mathscr{B}_{\mathbb{R}^n}$. ($\mathscr{B}$ denotes the Borel-algebra and $(0,1]A$ is defined as $\{rb:r\in(0,1] , b\in A\}$) Define $\sigma(A)=n\mu_n((0,1]A), \forall A\in\mathscr{B}_{S^{n-1}}$ Then, $\sigma$ is a measure. Is this ""the surface measure"" or the completion of $\sigma$ the surface measure?","Let $\mu_n$ be the $n$-dimensional Lebesgue measure. Let $||\cdot||$ be a norm on $\mathbb{R}^n$. Define $S^{n-1}=\{x\in\mathbb{R}:||x||=1\}$. I have proven that $\forall A\in\mathscr{B}_{S^{n-1}}, (0,1]A\in \mathscr{B}_{\mathbb{R}^n}$. ($\mathscr{B}$ denotes the Borel-algebra and $(0,1]A$ is defined as $\{rb:r\in(0,1] , b\in A\}$) Define $\sigma(A)=n\mu_n((0,1]A), \forall A\in\mathscr{B}_{S^{n-1}}$ Then, $\sigma$ is a measure. Is this ""the surface measure"" or the completion of $\sigma$ the surface measure?",,"['real-analysis', 'measure-theory', 'definition', 'polar-coordinates']"
7,Showing that $\sum_{n=1}^\infty \frac{x}{n(1+nx^2)}$ Converges Uniformly via the Weierstrass M-test,Showing that  Converges Uniformly via the Weierstrass M-test,\sum_{n=1}^\infty \frac{x}{n(1+nx^2)},"Setting: Let $f_n(x):\mathbb{R} \rightarrow  \mathbb{R}$ s.t. $$ f_n(x) = \frac{x}{n(1+nx^2)} $$ and let $f$ denote the series of the $\{f_n\}$: $$ f(x) = \sum_{n=1}^\infty f_n = \sum_{n=1}^\infty \frac{x}{n(1+nx^2)} $$ I am trying to show that $\{\sum_{n=1}^n f_n\}$ is uniformly convergent to $f$.  Assuming I'm not mistaken below, I've shown the desired result up to $|x| \ge 1$.  But I'm not sure how to proceed in case $|x| < 1$. Attempt in case $|x| \ge 1$: Let $x \in \mathbb{R}$ satisfy $|x| \ge 1$. Then consider that $$ \left| \frac{x}{n(1+nx^2)} \right| = \left| \frac{x}{n+n^2x^2} \right| \le \frac{|x|}{n^2x^2} = \underbrace{\frac{1}{|x| n^2} \le \frac{1}{n^2}}_{\text{ making use of $|x| \ge 1$}} $$ Since $\sum_{n=1}^\infty \frac{1}{n^2} < 2$ (a fact from calculus), we have that $\sum_{n=1}^\infty \frac{x}{n(1+nx^2)}$ converges uniformly by the Weierstrass M-test (with the $\{1/n^2\}$ serving as the $\{M_n\}$ using the notation from the link). But this argument doesn't work in case $|x| < 1$ (as flagged in the inequality).  Is there a simple fix in my inequality reasoning that gets me to the desired result?","Setting: Let $f_n(x):\mathbb{R} \rightarrow  \mathbb{R}$ s.t. $$ f_n(x) = \frac{x}{n(1+nx^2)} $$ and let $f$ denote the series of the $\{f_n\}$: $$ f(x) = \sum_{n=1}^\infty f_n = \sum_{n=1}^\infty \frac{x}{n(1+nx^2)} $$ I am trying to show that $\{\sum_{n=1}^n f_n\}$ is uniformly convergent to $f$.  Assuming I'm not mistaken below, I've shown the desired result up to $|x| \ge 1$.  But I'm not sure how to proceed in case $|x| < 1$. Attempt in case $|x| \ge 1$: Let $x \in \mathbb{R}$ satisfy $|x| \ge 1$. Then consider that $$ \left| \frac{x}{n(1+nx^2)} \right| = \left| \frac{x}{n+n^2x^2} \right| \le \frac{|x|}{n^2x^2} = \underbrace{\frac{1}{|x| n^2} \le \frac{1}{n^2}}_{\text{ making use of $|x| \ge 1$}} $$ Since $\sum_{n=1}^\infty \frac{1}{n^2} < 2$ (a fact from calculus), we have that $\sum_{n=1}^\infty \frac{x}{n(1+nx^2)}$ converges uniformly by the Weierstrass M-test (with the $\{1/n^2\}$ serving as the $\{M_n\}$ using the notation from the link). But this argument doesn't work in case $|x| < 1$ (as flagged in the inequality).  Is there a simple fix in my inequality reasoning that gets me to the desired result?",,"['real-analysis', 'analysis']"
8,Does there exist any continuous but not uniformly continuous function $f(x)$ such that $\sin(f(x))$ is uniformly continuous?,Does there exist any continuous but not uniformly continuous function  such that  is uniformly continuous?,f(x) \sin(f(x)),Does there exist any continuous but not uniformly continuous function $f(x)$ such that $\sin(f(x))$ is uniformly continuous? Actually all the examples I am taking for $f$ makes the composite function non uniformly continuous. I am not sure enough.,Does there exist any continuous but not uniformly continuous function $f(x)$ such that $\sin(f(x))$ is uniformly continuous? Actually all the examples I am taking for $f$ makes the composite function non uniformly continuous. I am not sure enough.,,"['real-analysis', 'uniform-continuity']"
9,Reference request: Behavior of power series at endpoints,Reference request: Behavior of power series at endpoints,,"I would like to find a calculus book (or a book on real analysis or advanced calculus) which has the following result: If a power series $\displaystyle\sum_{n=0}^{\infty}a_{n}x^{n}$ has a radius of convergence $r>0$, and the power series diverges at one of the endpoints, then $\displaystyle\sum_{n=1}^{\infty}n a_{n}x^{n-1}$ also diverges at that endpoint. (I'm not asking for a proof, just a reference.)","I would like to find a calculus book (or a book on real analysis or advanced calculus) which has the following result: If a power series $\displaystyle\sum_{n=0}^{\infty}a_{n}x^{n}$ has a radius of convergence $r>0$, and the power series diverges at one of the endpoints, then $\displaystyle\sum_{n=1}^{\infty}n a_{n}x^{n-1}$ also diverges at that endpoint. (I'm not asking for a proof, just a reference.)",,"['calculus', 'real-analysis', 'reference-request']"
10,$G_\delta$ sets,sets,G_\delta,"I understand that a $G_\delta$ set is a set which is a countable intersection of open sets. My question is: Is there any other characterization for $G_\delta$ sets (on $\mathbb{R}$)? For example, can I say that the interior of this sets is not empty? or that they are dense somewhere (means that they are not nowhere dense)? or any other topological characterization? Somehow I find it difficult to imagine those sets. Also, How can I prove that the $\mathbb{Q}$ is not a $G_\delta$ set. Thank you, Shir","I understand that a $G_\delta$ set is a set which is a countable intersection of open sets. My question is: Is there any other characterization for $G_\delta$ sets (on $\mathbb{R}$)? For example, can I say that the interior of this sets is not empty? or that they are dense somewhere (means that they are not nowhere dense)? or any other topological characterization? Somehow I find it difficult to imagine those sets. Also, How can I prove that the $\mathbb{Q}$ is not a $G_\delta$ set. Thank you, Shir",,"['real-analysis', 'general-topology']"
11,How to show that $g$ attains maximum at $0$ or $1$,How to show that  attains maximum at  or,g 0 1,"Suppose $f:[0,1]\to\mathbb{R}$ is continuous，define $$g:[0,1]\to\mathbb{R},\quad g(x):=\int_0^1|f(t)-x|dt$$ Show that $g$ attains maximum at $0$ or $1$. I don't know how to approach, any hints?","Suppose $f:[0,1]\to\mathbb{R}$ is continuous，define $$g:[0,1]\to\mathbb{R},\quad g(x):=\int_0^1|f(t)-x|dt$$ Show that $g$ attains maximum at $0$ or $1$. I don't know how to approach, any hints?",,"['calculus', 'real-analysis', 'integration']"
12,Show $\lim_{N\to\infty}\int_0^\pi\left(\frac1{\sin\frac{x}2}-\frac2x\right)\sin\left((N+\frac12)x\right)dx=0$,Show,\lim_{N\to\infty}\int_0^\pi\left(\frac1{\sin\frac{x}2}-\frac2x\right)\sin\left((N+\frac12)x\right)dx=0,"Prove that the function $\csc(x/2)-2/x$ is integrable on $(0,\pi)$. In fact, prove that it is bounded. In fact, prove that it tends to zero as $x\to0$. Use this to show that $$\lim_{N\to\infty}\int_0^\pi\left(\frac1{\sin\frac{x}2}-\frac2x\right)\sin\left((N+\frac12)x\right)dx=0$$ Then prove that $$\lim_{N\to\infty}\int_0^\pi\frac{\sin(N+\frac12)x}xdx=\pi/2$$ Finally, prove that $$\int_0^\infty\frac{\sin x}xdx=\pi/2$$ One observation is $\sum_{n=-N}^Ne^{inx}=\frac{\sin(N+\frac12)x}{\sin\frac{x}2}$, and the integral of this from $0$ to $2\pi$ is $2\pi$, since the integral $\int_{0}^{2\pi}e^{inx}dx=0$ if $n\neq0$.","Prove that the function $\csc(x/2)-2/x$ is integrable on $(0,\pi)$. In fact, prove that it is bounded. In fact, prove that it tends to zero as $x\to0$. Use this to show that $$\lim_{N\to\infty}\int_0^\pi\left(\frac1{\sin\frac{x}2}-\frac2x\right)\sin\left((N+\frac12)x\right)dx=0$$ Then prove that $$\lim_{N\to\infty}\int_0^\pi\frac{\sin(N+\frac12)x}xdx=\pi/2$$ Finally, prove that $$\int_0^\infty\frac{\sin x}xdx=\pi/2$$ One observation is $\sum_{n=-N}^Ne^{inx}=\frac{\sin(N+\frac12)x}{\sin\frac{x}2}$, and the integral of this from $0$ to $2\pi$ is $2\pi$, since the integral $\int_{0}^{2\pi}e^{inx}dx=0$ if $n\neq0$.",,"['real-analysis', 'integration', 'analysis', 'fourier-analysis', 'definite-integrals']"
13,Completeness of a normed vector space,Completeness of a normed vector space,,"This is captured from a chapter talking about completeness of metric space in Real Analysis, Carothers, 1ed. I have been confused by two questions: What does absolutely summable mean in metric space? Does it mean the norm of xi(i=1,2,3,...) that belongs to norm vector space X is summable? 2nd part of the proof should try to show that if ∑xn(n from 1 to infinity) converges in X whenever ||xn||(n from 1 to infinity) is summable, then X is complete. However, why does the author prove the subsequence of {xn} converges? Thanks^_^","This is captured from a chapter talking about completeness of metric space in Real Analysis, Carothers, 1ed. I have been confused by two questions: What does absolutely summable mean in metric space? Does it mean the norm of xi(i=1,2,3,...) that belongs to norm vector space X is summable? 2nd part of the proof should try to show that if ∑xn(n from 1 to infinity) converges in X whenever ||xn||(n from 1 to infinity) is summable, then X is complete. However, why does the author prove the subsequence of {xn} converges? Thanks^_^",,"['real-analysis', 'analysis']"
14,"Define a sequence by $a_1 = 1, a_2 = 1/2$, and $a_{n+2} = a_{n+1} - a_na_{n+1}/2$ for $n$ a positive integer.","Define a sequence by , and  for  a positive integer.","a_1 = 1, a_2 = 1/2 a_{n+2} = a_{n+1} - a_na_{n+1}/2 n","Define a sequence by $a_1 = 1, a_2 = 1/2$, and $$a_{n+2} = a_{n+1} - a_na_{n+1}/2$$ for $n$ a positive integer. Find $$\lim_{n\to\infty}na_n$$ if it exists. Well, we can deduce that $\lim a_n=0$ by checking $(a_n)$ is decreasing and bounded. But finding $\lim na_n$ is another story.","Define a sequence by $a_1 = 1, a_2 = 1/2$, and $$a_{n+2} = a_{n+1} - a_na_{n+1}/2$$ for $n$ a positive integer. Find $$\lim_{n\to\infty}na_n$$ if it exists. Well, we can deduce that $\lim a_n=0$ by checking $(a_n)$ is decreasing and bounded. But finding $\lim na_n$ is another story.",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'contest-math']"
15,Uniform Convergence of $\sum_{n=1}^\infty -x^{2n} \ln x$,Uniform Convergence of,\sum_{n=1}^\infty -x^{2n} \ln x,"Last month, I read a proof of $\displaystyle \sum_{n=0}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$ that used the following integral: $$ \int_0^\infty \int_0^\infty \frac{1}{(1+y)(1+x^2y)} \, dx \, dy$$ It was in August-September 's issue of the American Mathematical Monthly. Anyway, I was able to follow most of  it upto the last step where he switches integral and summation. $$\int_0^1 \frac{\ln x}{x^2-1} dx = \sum_{n=0}^\infty \int_0^1 -x^{2n} \ln x\,dx$$ I think I grasp how uniform convergence of a series can allow you to switch infinite sums and integral signs. But from what I can see $\sum_{n=1}^\infty -x^{2n} \ln x$ doesn't uniformly converge on $[0,1]$. It may be uniformly convergent on $[0,k]$ if $k \in (0,1)$? Is that enough to justify the switch? Alternatively, the partial sums are strictly increasing and bounded. But how do you prove they converge to the integral on the left side? Edit: Is there a theorem that says: if $f_n \to f$ pointwise on $[a,b]$ and $f_n$ is uniformly bounded then, $\int_a^b f_n(x) \, dx \to \int_a^b f(x) \, dx$? Is the following a proper proof? I'm trying to avoid using Lebesgue Integrals as I haven't studied them. Suppose that $\displaystyle \left| \int_0^1 f(x)\,dx - \lim_{n\to \infty} \int_0^1 f_n(x)\,dx \right| = \epsilon>0$. Since $\displaystyle \int_0^k f(x)\, dx = \lim_{n \to \infty} \int_0^k f_n(x)\,dx$ by uniform convergence on $[0,k]$. It remains to show that if $k$ is chosen properly then $\displaystyle \left| \int_k^1 f(x)\,dx-\lim_{n\to\infty} \int_k^1 f_n(x)\,dx \right| < \epsilon$, a contradiction. $f_n$ being uniformly bounded implies $$ \left| \int_k^1 f(x)\,dx - \lim_{n\to\infty} \int_k^1 f_n(x)\,dx \right| < (1-k)M$$ So let $1-\frac{\epsilon}{M}<k<1$ so that $(1-k)M < \epsilon$.","Last month, I read a proof of $\displaystyle \sum_{n=0}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$ that used the following integral: $$ \int_0^\infty \int_0^\infty \frac{1}{(1+y)(1+x^2y)} \, dx \, dy$$ It was in August-September 's issue of the American Mathematical Monthly. Anyway, I was able to follow most of  it upto the last step where he switches integral and summation. $$\int_0^1 \frac{\ln x}{x^2-1} dx = \sum_{n=0}^\infty \int_0^1 -x^{2n} \ln x\,dx$$ I think I grasp how uniform convergence of a series can allow you to switch infinite sums and integral signs. But from what I can see $\sum_{n=1}^\infty -x^{2n} \ln x$ doesn't uniformly converge on $[0,1]$. It may be uniformly convergent on $[0,k]$ if $k \in (0,1)$? Is that enough to justify the switch? Alternatively, the partial sums are strictly increasing and bounded. But how do you prove they converge to the integral on the left side? Edit: Is there a theorem that says: if $f_n \to f$ pointwise on $[a,b]$ and $f_n$ is uniformly bounded then, $\int_a^b f_n(x) \, dx \to \int_a^b f(x) \, dx$? Is the following a proper proof? I'm trying to avoid using Lebesgue Integrals as I haven't studied them. Suppose that $\displaystyle \left| \int_0^1 f(x)\,dx - \lim_{n\to \infty} \int_0^1 f_n(x)\,dx \right| = \epsilon>0$. Since $\displaystyle \int_0^k f(x)\, dx = \lim_{n \to \infty} \int_0^k f_n(x)\,dx$ by uniform convergence on $[0,k]$. It remains to show that if $k$ is chosen properly then $\displaystyle \left| \int_k^1 f(x)\,dx-\lim_{n\to\infty} \int_k^1 f_n(x)\,dx \right| < \epsilon$, a contradiction. $f_n$ being uniformly bounded implies $$ \left| \int_k^1 f(x)\,dx - \lim_{n\to\infty} \int_k^1 f_n(x)\,dx \right| < (1-k)M$$ So let $1-\frac{\epsilon}{M}<k<1$ so that $(1-k)M < \epsilon$.",,"['calculus', 'real-analysis']"
16,A problem about Cantor set and found when learning dynamical systems.,A problem about Cantor set and found when learning dynamical systems.,,"Consider the family of functions F(x)=$x^3 -\alpha$x,   for $\alpha \gt 0$ Prove that if $\alpha$ is sufficiently large, then the set of points |$F^n(x)$| which do not tend to infinity is a Cantor set. Note:$F^n(x)$ means the iteration of the function; I have proved that if |$x$| is sufficiently large, then |$F^n(x)$|$\rightarrow\infty$ Your answeres will be greatly appreciated! ( This problem is from < An introduction to chaotic dynamical systems > Robert L. Devaney )","Consider the family of functions F(x)=$x^3 -\alpha$x,   for $\alpha \gt 0$ Prove that if $\alpha$ is sufficiently large, then the set of points |$F^n(x)$| which do not tend to infinity is a Cantor set. Note:$F^n(x)$ means the iteration of the function; I have proved that if |$x$| is sufficiently large, then |$F^n(x)$|$\rightarrow\infty$ Your answeres will be greatly appreciated! ( This problem is from < An introduction to chaotic dynamical systems > Robert L. Devaney )",,"['real-analysis', 'analysis', 'dynamical-systems']"
17,proof of the continuity of as function,proof of the continuity of as function,,"Let $f: \mathbb{R} \to \mathbb{R}$ be a surjective function such that for all non convergent sequences $(x_n)$, the sequence $(f(x_n))$ is non convergent. Prove that $f$ is continuous. Thank you","Let $f: \mathbb{R} \to \mathbb{R}$ be a surjective function such that for all non convergent sequences $(x_n)$, the sequence $(f(x_n))$ is non convergent. Prove that $f$ is continuous. Thank you",,['real-analysis']
18,Question about a proof that $\mathbb{Q}$ is dense in $\mathbb{R}$,Question about a proof that  is dense in,\mathbb{Q} \mathbb{R},"This is from Ross's elementary analysis book. The statement is if $a,b \in \mathbb{R}$ such that $a<b$ then there exists a rational $r \in \mathbb{Q}$ such that $a<r<b$. I don't understand an important part of the proof which I will point out. Here is the proof: Since $b-a>0$ then by the archimidean property there exists a natural number call it $n$ such that $n(b-a)>1$. Now we must prove there exists an integer $m$ such that $an<m<bn$. By the archimidean property again there exists an integer $k$ such that $k>\max(|an|, |bn|)$ so that $-k<an<bn<k$. Then the set $$\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$$ is finite and nonempty and we can set $m = \min\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$. Then $an < m$ but $m - 1 \leq an$. Also, we have $m = (m - 1) + 1 \leq an + 1 < an + (bn - an) = bn$. Comment: I get up to the point how the author sets$ k$ to be the integer that is larger than both $an$ and $bn$. And since $a<b$ we get the bounded inequality $-k<an<bn<k$. From here on this is where I get confused.  He creates a set  $\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$ which he calls finite which I see and nonempty (I'm guessing since this set has a least upper bound $k$ thus it is nonempty). But then he lets  $m = \min\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$ which follows $an<m$ but $m - 1 \leq an$. I don't get how the author gets to that point?","This is from Ross's elementary analysis book. The statement is if $a,b \in \mathbb{R}$ such that $a<b$ then there exists a rational $r \in \mathbb{Q}$ such that $a<r<b$. I don't understand an important part of the proof which I will point out. Here is the proof: Since $b-a>0$ then by the archimidean property there exists a natural number call it $n$ such that $n(b-a)>1$. Now we must prove there exists an integer $m$ such that $an<m<bn$. By the archimidean property again there exists an integer $k$ such that $k>\max(|an|, |bn|)$ so that $-k<an<bn<k$. Then the set $$\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$$ is finite and nonempty and we can set $m = \min\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$. Then $an < m$ but $m - 1 \leq an$. Also, we have $m = (m - 1) + 1 \leq an + 1 < an + (bn - an) = bn$. Comment: I get up to the point how the author sets$ k$ to be the integer that is larger than both $an$ and $bn$. And since $a<b$ we get the bounded inequality $-k<an<bn<k$. From here on this is where I get confused.  He creates a set  $\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$ which he calls finite which I see and nonempty (I'm guessing since this set has a least upper bound $k$ thus it is nonempty). But then he lets  $m = \min\{j \in \mathbb{Z}: {-k<j \leq k}\text{ and }an<j\}$ which follows $an<m$ but $m - 1 \leq an$. I don't get how the author gets to that point?",,"['real-analysis', 'general-topology', 'analysis']"
19,Prove that $\lim\limits_{n\to\infty} \int\limits_X f_nd\mu=\int\limits_X fd\mu$,Prove that,\lim\limits_{n\to\infty} \int\limits_X f_nd\mu=\int\limits_X fd\mu,"$(X,\mathscr{M},\mu)$ is a measurable space and $f_n,g_n,f,g$ are all measurable functions defined on $X$. The following conditions are satisfied: (i) $f_n\to f\;\&\;g_n\to g$ almost everywhere on $X$ (ii) $|f_n|\leq g_n$ and $\int\limits_X g_nd\mu<\infty\;\forall\; n\geq1$ (iii) $\lim\limits_{n\to\infty}\int\limits_X g_nd\mu=\int\limits_X gd\mu<\infty$ Prove $\lim\limits_{n\to\infty}\int\limits_X f_nd\mu=\int\limits_X fd\mu$. Attempt at answer: I know that since $f_n,g_n,f,g$ are all measurable, then $|f_n|,|g_n|,|f|,|g|$ are all measurable. Also that $f_n\to f\;\&\;g_n\to g$ almost everywhere on $X$ implies $f_n\to f\;\&\;g_n\to g$ almost uniform convergence on $X$ implies $f_n\to f\;\&\;g_n\to g$ in measure on $X$. It's almost like ""Lebesgue's Dominated Convergence Thm"", but I have ""almost everyhwere"" convergence. Does that make a difference, or am I just making up differences where there are none? Any help/advice would be remarkably useful.","$(X,\mathscr{M},\mu)$ is a measurable space and $f_n,g_n,f,g$ are all measurable functions defined on $X$. The following conditions are satisfied: (i) $f_n\to f\;\&\;g_n\to g$ almost everywhere on $X$ (ii) $|f_n|\leq g_n$ and $\int\limits_X g_nd\mu<\infty\;\forall\; n\geq1$ (iii) $\lim\limits_{n\to\infty}\int\limits_X g_nd\mu=\int\limits_X gd\mu<\infty$ Prove $\lim\limits_{n\to\infty}\int\limits_X f_nd\mu=\int\limits_X fd\mu$. Attempt at answer: I know that since $f_n,g_n,f,g$ are all measurable, then $|f_n|,|g_n|,|f|,|g|$ are all measurable. Also that $f_n\to f\;\&\;g_n\to g$ almost everywhere on $X$ implies $f_n\to f\;\&\;g_n\to g$ almost uniform convergence on $X$ implies $f_n\to f\;\&\;g_n\to g$ in measure on $X$. It's almost like ""Lebesgue's Dominated Convergence Thm"", but I have ""almost everyhwere"" convergence. Does that make a difference, or am I just making up differences where there are none? Any help/advice would be remarkably useful.",,"['real-analysis', 'measure-theory']"
20,Proving that a set is closed in $L^2(\mathbb{R})$,Proving that a set is closed in,L^2(\mathbb{R}),"this is my first question here so i hope i don't do anything wrong. Excuse any spelling or grammar mistakes, english isn't my mother tongue. I'm reading this paper for my bachelor thesis and have problems with corrolary 3.3 on page 8. I tried to solve this problem for some time, asked on a different forum for help but the only advice i received was to contact the authors( it was a german forum and i marked the question as answered there, because no one was able to help me, in case there are rules against cross-postings here). I did that and got a very quick response which was something of a surprise for me. The reply consisted in the promise, that he would think about this problem and get back to me. The same thing sad my professor, who gave me the paper after i asked him today. First i will summarize everything which is needed to understand what's going on. For $f\in L^2$ with $\lVert f\rVert =1$ and $p>1$ define $\varphi_f:\ \mathbb{R}\rightarrow \mathbb R\ ;\ a\mapsto \int|t-a|^p|f(t)|^2\mathrm dt$, this Integral and all others without specified domain of integration are taken over $\mathbb R$ or the complement of a set of measure zero. The following properties hold for $\varphi_f$: if there is one $a_0\in\mathbb R$ with $\varphi_f(a_0)<\infty$, then i) $  \varphi_f(a)<\infty$ for every $a\in\mathbb R$ ii)$\varphi_f$ is stricly convex, continuous and satisfies $\lim\limits_{|a|\to\infty}\varphi_f=\infty$ iii)$\varphi_f$ has a unique minimum $\Delta_p^2(f)$ at the point $\mu_p(f)$ ( with $\Delta_p(f)\geq 0$). Now we can define the following set for $A>0$ and $p,q>1$: $K=\{f\in L^2:\ \|f\|=1,|\Delta_p(f)|\leq A, |\mu_p(f)|\leq A,|\Delta_q(\hat f)|\leq A,|\mu_q(\hat f)|\leq A\}$, where $\hat f$ is the Fourier transform of f. The authors claim without further justification that this set is closed and i wanted to proof this. Note that $\mu_p(\cdot),\ \Delta_p(\cdot)$ are not continuous, for a counterexample see page 4 of the paper just before proposition 2.2. That's how far i come: Let $f\in\overline K$, i.e. there is a sequence $(f_n)_n\in K^\mathbb N$ with $f_n\xrightarrow[n\to\infty]{L^2}f$. Since $\Delta_p(f_n),\ \mu_p(f_n)\in[-A,A]$ for all $n$, there exists a subsequence with i)$\Delta_p(f_{n_k})\xrightarrow[k\to\infty]{}\underline\Delta_p=\liminf\Delta_p(f_n)\in[-A,A]$ ii)$\mu_p(f_{n_k})\xrightarrow[k\to\infty]{}\mu_p\in[-A,A]$. iii)Because $f_{n_k}\xrightarrow[k\to\infty]{L^2}f$ there exists another subsequence, which i will denote by $(f_{n_k})_k$ for convenience, that converges pointwise almost everywhere to f. That $\|f\|=1$ is easy to see and the lemma of Fatou implies $\varphi_f(\mu_p)=\int|t-\mu_p|^p|f|^2\mathrm d t=\int\liminf|t-\mu_p(f_{n_k})|^p|f_{n_k}|^2\mathrm d t$ $\leq\liminf\int|t-\mu_p(f_{n_k})|^p|f_{n_k}|^2\mathrm d t=\liminf\Delta_p^2(f_{n_k})=\lim\Delta_p^2(f_{n_k})=\underline\Delta_p^2\leq A^2$ This means $\varphi_f(\mu_p)<\infty$, so that $\Delta_p(f)$ and $\mu_p(f)$ are well defined, and since $\Delta_p^2(f)$ is the minimum of $\varphi_f$ it holds $\Delta_p^2(f)\leq\varphi_f(\mu_p)\leq A^2\Rightarrow  |\Delta_p(f)|\leq A$. The same argument can be applied to $\hat f$ (because the Fourier transform is an unitary operator on $L^2$), so $|\Delta_q(\hat f)|\leq A$ is also satisfied. My problem is to show that the inequality also holds for $\mu_p(f)$ and $\mu_q(\hat f)$. I tried to prove that for $a\notin [-A,A]$ it is $\varphi_f(\mu_p)\leq\varphi_f(a)$, because then $\varphi_f$ couldn't attain it's minimum outside of $[-A,A]$. One idea is to use the $\limsup$ version of Fatou's lemma, but for that i need an integrable majorant for $(|t-a|^p|f_{n_k}|)_k$ then one could argue just like before: $\varphi_f(\mu_p)\leq\liminf\int|t-\mu_p(f_{n_k})|^p|f_{n_k}|^2\mathrm d t\leq \liminf\int|t-a|^p|f_{n_k}|^2\mathrm d t$ the last inequality holds because $\mu_p(f_{n_k})$ is the point where the minimum of $\varphi_{f_{n_k}}$ is attained and thus $\liminf\int|t-a|^p|f_{n_k}|^2\mathrm d t\leq\limsup\int|t-a|^p|f_{n_k}|^2\mathrm d t$ $\leq\int\limsup|t-a|^p|f_{n_k}|^2\mathrm d t=\int|t-a|^p|f|^2\mathrm d t=\varphi_f(a)$. One also knows that $K$ is relative compact, maybe this could in some way be used. I would be very grateful for responses and remarks!","this is my first question here so i hope i don't do anything wrong. Excuse any spelling or grammar mistakes, english isn't my mother tongue. I'm reading this paper for my bachelor thesis and have problems with corrolary 3.3 on page 8. I tried to solve this problem for some time, asked on a different forum for help but the only advice i received was to contact the authors( it was a german forum and i marked the question as answered there, because no one was able to help me, in case there are rules against cross-postings here). I did that and got a very quick response which was something of a surprise for me. The reply consisted in the promise, that he would think about this problem and get back to me. The same thing sad my professor, who gave me the paper after i asked him today. First i will summarize everything which is needed to understand what's going on. For $f\in L^2$ with $\lVert f\rVert =1$ and $p>1$ define $\varphi_f:\ \mathbb{R}\rightarrow \mathbb R\ ;\ a\mapsto \int|t-a|^p|f(t)|^2\mathrm dt$, this Integral and all others without specified domain of integration are taken over $\mathbb R$ or the complement of a set of measure zero. The following properties hold for $\varphi_f$: if there is one $a_0\in\mathbb R$ with $\varphi_f(a_0)<\infty$, then i) $  \varphi_f(a)<\infty$ for every $a\in\mathbb R$ ii)$\varphi_f$ is stricly convex, continuous and satisfies $\lim\limits_{|a|\to\infty}\varphi_f=\infty$ iii)$\varphi_f$ has a unique minimum $\Delta_p^2(f)$ at the point $\mu_p(f)$ ( with $\Delta_p(f)\geq 0$). Now we can define the following set for $A>0$ and $p,q>1$: $K=\{f\in L^2:\ \|f\|=1,|\Delta_p(f)|\leq A, |\mu_p(f)|\leq A,|\Delta_q(\hat f)|\leq A,|\mu_q(\hat f)|\leq A\}$, where $\hat f$ is the Fourier transform of f. The authors claim without further justification that this set is closed and i wanted to proof this. Note that $\mu_p(\cdot),\ \Delta_p(\cdot)$ are not continuous, for a counterexample see page 4 of the paper just before proposition 2.2. That's how far i come: Let $f\in\overline K$, i.e. there is a sequence $(f_n)_n\in K^\mathbb N$ with $f_n\xrightarrow[n\to\infty]{L^2}f$. Since $\Delta_p(f_n),\ \mu_p(f_n)\in[-A,A]$ for all $n$, there exists a subsequence with i)$\Delta_p(f_{n_k})\xrightarrow[k\to\infty]{}\underline\Delta_p=\liminf\Delta_p(f_n)\in[-A,A]$ ii)$\mu_p(f_{n_k})\xrightarrow[k\to\infty]{}\mu_p\in[-A,A]$. iii)Because $f_{n_k}\xrightarrow[k\to\infty]{L^2}f$ there exists another subsequence, which i will denote by $(f_{n_k})_k$ for convenience, that converges pointwise almost everywhere to f. That $\|f\|=1$ is easy to see and the lemma of Fatou implies $\varphi_f(\mu_p)=\int|t-\mu_p|^p|f|^2\mathrm d t=\int\liminf|t-\mu_p(f_{n_k})|^p|f_{n_k}|^2\mathrm d t$ $\leq\liminf\int|t-\mu_p(f_{n_k})|^p|f_{n_k}|^2\mathrm d t=\liminf\Delta_p^2(f_{n_k})=\lim\Delta_p^2(f_{n_k})=\underline\Delta_p^2\leq A^2$ This means $\varphi_f(\mu_p)<\infty$, so that $\Delta_p(f)$ and $\mu_p(f)$ are well defined, and since $\Delta_p^2(f)$ is the minimum of $\varphi_f$ it holds $\Delta_p^2(f)\leq\varphi_f(\mu_p)\leq A^2\Rightarrow  |\Delta_p(f)|\leq A$. The same argument can be applied to $\hat f$ (because the Fourier transform is an unitary operator on $L^2$), so $|\Delta_q(\hat f)|\leq A$ is also satisfied. My problem is to show that the inequality also holds for $\mu_p(f)$ and $\mu_q(\hat f)$. I tried to prove that for $a\notin [-A,A]$ it is $\varphi_f(\mu_p)\leq\varphi_f(a)$, because then $\varphi_f$ couldn't attain it's minimum outside of $[-A,A]$. One idea is to use the $\limsup$ version of Fatou's lemma, but for that i need an integrable majorant for $(|t-a|^p|f_{n_k}|)_k$ then one could argue just like before: $\varphi_f(\mu_p)\leq\liminf\int|t-\mu_p(f_{n_k})|^p|f_{n_k}|^2\mathrm d t\leq \liminf\int|t-a|^p|f_{n_k}|^2\mathrm d t$ the last inequality holds because $\mu_p(f_{n_k})$ is the point where the minimum of $\varphi_{f_{n_k}}$ is attained and thus $\liminf\int|t-a|^p|f_{n_k}|^2\mathrm d t\leq\limsup\int|t-a|^p|f_{n_k}|^2\mathrm d t$ $\leq\int\limsup|t-a|^p|f_{n_k}|^2\mathrm d t=\int|t-a|^p|f|^2\mathrm d t=\varphi_f(a)$. One also knows that $K$ is relative compact, maybe this could in some way be used. I would be very grateful for responses and remarks!",,"['real-analysis', 'general-topology', 'functional-analysis']"
21,"$f:[a,b]\to(a,b)$ be continuous how prove $f(c)+f(c+d)+\cdots+f(c+nd)=(n+1)(c+\frac{nd}{2})$",be continuous how prove,"f:[a,b]\to(a,b) f(c)+f(c+d)+\cdots+f(c+nd)=(n+1)(c+\frac{nd}{2})","let $f:[a,b]\to(a,b)$ be continuous how prove   $\forall n\in\mathbb N$ $\exists d\gt0$ ,$\exists c\in(a,b) $ such that $$f(c)+f(c+d)+\cdots+f(c+nd)=(n+1)\left(c+\frac{nd}{2}\right)$$thanks in advance","let $f:[a,b]\to(a,b)$ be continuous how prove   $\forall n\in\mathbb N$ $\exists d\gt0$ ,$\exists c\in(a,b) $ such that $$f(c)+f(c+d)+\cdots+f(c+nd)=(n+1)\left(c+\frac{nd}{2}\right)$$thanks in advance",,"['real-analysis', 'analysis', 'continuity', 'contest-math']"
22,"Which of the following sets are open in $C^2[0,1]$? Explain. (Topology on normed spaces)",Which of the following sets are open in ? Explain. (Topology on normed spaces),"C^2[0,1]","(a) $A= \{f \in C^2[0,1]:f(x)>0,\parallel f'\parallel_{\infty}<1, |f''(0)|>2\}$ (b) $B= \{f\in C^2[0,1]:f(1)<0,f'(1)=0,f''(1)>0 \}$ (c) $C= \{f\in C^2[0,1]:f(x)f'(x)>0$ for $0\le x \le 1\}$ (d) $D= \{f\in C^2[0,1]: f(x)f'(x)>0$ for $0\lt x \lt 1\}$ I have the intuition that $A$ is an open set since it includes all $f(x)>0$, $B$ is a neither a closed nor open set since it only holds true for one element in the interval, $C$ is a closed set since it includes an open set and has a boundary and $D$ is an open set since it doesn't include the boundary. Any feed back would be appreciated, Thanks.","(a) $A= \{f \in C^2[0,1]:f(x)>0,\parallel f'\parallel_{\infty}<1, |f''(0)|>2\}$ (b) $B= \{f\in C^2[0,1]:f(1)<0,f'(1)=0,f''(1)>0 \}$ (c) $C= \{f\in C^2[0,1]:f(x)f'(x)>0$ for $0\le x \le 1\}$ (d) $D= \{f\in C^2[0,1]: f(x)f'(x)>0$ for $0\lt x \lt 1\}$ I have the intuition that $A$ is an open set since it includes all $f(x)>0$, $B$ is a neither a closed nor open set since it only holds true for one element in the interval, $C$ is a closed set since it includes an open set and has a boundary and $D$ is an open set since it doesn't include the boundary. Any feed back would be appreciated, Thanks.",,"['real-analysis', 'general-topology', 'normed-spaces']"
23,"Question about the functional equation $B(t,T)=c(t)B^2(t,T)$ arising from certain differential equations",Question about the functional equation  arising from certain differential equations,"B(t,T)=c(t)B^2(t,T)","We are looking at a theorem which characterizes the affine term structure (ats) models in interes rate theory. What follows is from ""Filipović, D. (2009): ""Term-structure models: A graduate course"", Springer-Verlag"", chpt. 5, page 84. We denote by $F(t,r,T)$ the bond price and say it is of (ats) if and only if $$F(t,r,T)=e^{-A(t,T)-B(t,T)r}$$ for smooth functions $A$ and $B$ . $r$ denotes the interest rate and is a stochastic process. Then the theorem states a short rate model of the form $$\mathrm dr(t)=b(t,r)\mathrm dt+\sigma(t,r)\mathrm dW(t)\tag{*}\label{*}$$ for continuous $b$ and $\sigma$ provides ats if and only if $$\sigma^2(t,r)=a(t)+\alpha(t)r \text{ and } b(t,r)=b(t)+\beta(t)r$$ for continuous function $a$ , $\alpha$ , $b$ and $\beta$ , and the functions $A$ and $B$ satisfy the system of ODE, for all $t\le T$ : $$\partial_tA(t,T)=\frac{1}{2}a(t)B^2(t,T)-b(t)B(t,T), \, A(T,T)=0$$ $$\partial_tB(t,T)=\frac{1}{2}\alpha(t)B^2(t,T)-\beta(t)B(t,T)-1, \, B(T,T)=0$$ The key point of the proof is that $F$ should satisfy the following equation $$ \partial_t F+b\partial_rF+\frac{1}{2}\sigma^2\partial_{rr}F-rF=0\tag{1}\label{1}$$ where $b,\sigma$ are from \eqref{*}. For the proof, you put the explicit formula of $F$ into \eqref{1}, we see that the short rate model provides an ats if and only if $$\frac{1}{2}\sigma^2B^2-bB=\partial_tA+(\partial_tB+1)r \tag{2}\label{2}$$ where I wrote $B$ for $B(t,T)$ and the same for $A$ . Looking about the equation above the direction "" $\Leftarrow$ "" is proved. For the direction "" $\Rightarrow$ "", they first assume that $B$ and $B^2$ are linearly independent for fixed $t$ and show the claim. After that the only case which we now have to look at, is $$B(t,T)=c(t)B^2(t,T)\tag{3}\label{3}$$ for some constant $c(t)$ . I guess we also fix here $t$ . Then they conclude the following things, which I do not understand: \eqref{3} should imply that $B(t,\cdot)=B(t,t)=0$ . Why is that true? From there they say, well then \eqref{2} implies that $\partial_tB(t,T)=-1$ . I also do not get that conclusion. After all they conclude that the set of elements $t$ , for which $B(t,\cdot)$ and $B^2(t,\cdot)$ are linearly independent is open and dense in $\mathbb{R}_+$ . I have no idea how one can conclude all these things. Some help would really be appreciated.","We are looking at a theorem which characterizes the affine term structure (ats) models in interes rate theory. What follows is from ""Filipović, D. (2009): ""Term-structure models: A graduate course"", Springer-Verlag"", chpt. 5, page 84. We denote by the bond price and say it is of (ats) if and only if for smooth functions and . denotes the interest rate and is a stochastic process. Then the theorem states a short rate model of the form for continuous and provides ats if and only if for continuous function , , and , and the functions and satisfy the system of ODE, for all : The key point of the proof is that should satisfy the following equation where are from \eqref{*}. For the proof, you put the explicit formula of into \eqref{1}, we see that the short rate model provides an ats if and only if where I wrote for and the same for . Looking about the equation above the direction "" "" is proved. For the direction "" "", they first assume that and are linearly independent for fixed and show the claim. After that the only case which we now have to look at, is for some constant . I guess we also fix here . Then they conclude the following things, which I do not understand: \eqref{3} should imply that . Why is that true? From there they say, well then \eqref{2} implies that . I also do not get that conclusion. After all they conclude that the set of elements , for which and are linearly independent is open and dense in . I have no idea how one can conclude all these things. Some help would really be appreciated.","F(t,r,T) F(t,r,T)=e^{-A(t,T)-B(t,T)r} A B r \mathrm dr(t)=b(t,r)\mathrm dt+\sigma(t,r)\mathrm dW(t)\tag{*}\label{*} b \sigma \sigma^2(t,r)=a(t)+\alpha(t)r \text{ and } b(t,r)=b(t)+\beta(t)r a \alpha b \beta A B t\le T \partial_tA(t,T)=\frac{1}{2}a(t)B^2(t,T)-b(t)B(t,T), \, A(T,T)=0 \partial_tB(t,T)=\frac{1}{2}\alpha(t)B^2(t,T)-\beta(t)B(t,T)-1, \, B(T,T)=0 F  \partial_t F+b\partial_rF+\frac{1}{2}\sigma^2\partial_{rr}F-rF=0\tag{1}\label{1} b,\sigma F \frac{1}{2}\sigma^2B^2-bB=\partial_tA+(\partial_tB+1)r \tag{2}\label{2} B B(t,T) A \Leftarrow \Rightarrow B B^2 t B(t,T)=c(t)B^2(t,T)\tag{3}\label{3} c(t) t B(t,\cdot)=B(t,t)=0 \partial_tB(t,T)=-1 t B(t,\cdot) B^2(t,\cdot) \mathbb{R}_+",['real-analysis']
24,Showing that a sequence is Cauchy but not convergent,Showing that a sequence is Cauchy but not convergent,,"Consider $X$ to be the set of all continuous real-valued functions on $C[0,1]$ with metric $$d(x,y) = \int_{0}^{1} |x(t) - y(t)| dt$$ Show that $x_n(t) = \left\{\begin{matrix}  n& 0 \leq t \leq n^{-2} \\   t^{-1/2}& n^{-2} \leq t \leq 1 \end{matrix}\right.$ is Cauchy, but does not converge. So here is what I did, I basically assumed that $m > n$, (for some $m$) $$d(x_n, x_m) = \int_{0}^{m^{-2}}|x_n-x_m|dt + \int_{m^{-2}}^{n^{-2}}|x_n-x_m|dt + \int_{n^{-2}}^{1}|x_n-x_m|dt = \int_{0}^{m^{-2}}|n-m|dt + 0 + 0 = (m - n)m^{-2}$$ Now according to the book, the answer should've been $m^{-1} - n^{-1}$ for $n > m$. As for convergence, I got $d(x_n,x) = \int_{0}^{n^{-2}} |x_n - x| dt + \int_{n^{-2}}^{1} |t^{-1/2} - x| dt \to 0$ implies that $x(t) =1/\sqrt{t}$ on $[0,1]$. But $x(0)$ is undefined, so not continuous. I am not sure if I did this right, I have more confidence in the convergence part over the Cauchy part.","Consider $X$ to be the set of all continuous real-valued functions on $C[0,1]$ with metric $$d(x,y) = \int_{0}^{1} |x(t) - y(t)| dt$$ Show that $x_n(t) = \left\{\begin{matrix}  n& 0 \leq t \leq n^{-2} \\   t^{-1/2}& n^{-2} \leq t \leq 1 \end{matrix}\right.$ is Cauchy, but does not converge. So here is what I did, I basically assumed that $m > n$, (for some $m$) $$d(x_n, x_m) = \int_{0}^{m^{-2}}|x_n-x_m|dt + \int_{m^{-2}}^{n^{-2}}|x_n-x_m|dt + \int_{n^{-2}}^{1}|x_n-x_m|dt = \int_{0}^{m^{-2}}|n-m|dt + 0 + 0 = (m - n)m^{-2}$$ Now according to the book, the answer should've been $m^{-1} - n^{-1}$ for $n > m$. As for convergence, I got $d(x_n,x) = \int_{0}^{n^{-2}} |x_n - x| dt + \int_{n^{-2}}^{1} |t^{-1/2} - x| dt \to 0$ implies that $x(t) =1/\sqrt{t}$ on $[0,1]$. But $x(0)$ is undefined, so not continuous. I am not sure if I did this right, I have more confidence in the convergence part over the Cauchy part.",,"['real-analysis', 'convergence-divergence', 'metric-spaces', 'cauchy-sequences']"
25,"$f$ bounded on $[a,b]$ with one or finite discontinuities implies $f$ Riemann-integrable.",bounded on  with one or finite discontinuities implies  Riemann-integrable.,"f [a,b] f","I have two problems: Prove that if $f$ is bounded on $[a, b]$ and has exactly one discontinuity in $[a, b]$ then $f$ is Riemann-integrable on $[a, b]$. Prove that if $f$ is bounded on $[a, b]$ and $f$ has only finitely many discontinuities in $[a, b]$ then $f$ is Riemann-integrable on $[a, b]$. I have a possible proof for the first one which I would like checked: Theorem 6.1 states that $f$ is Riemann-integrable on $[a, b]$ iff given any $\varepsilon > 0$ there exists a partition $P$ of $[a, b]$ with $U(P, f) - L(P, f) < \varepsilon$. Theorem 6.2 states that if $f$ is continuous on $[a, b]$ then it is Riemann-integrable on $[a, b]$. Suppose $f$ is bounded on $[a, b]$. Then by definition there exists $M > 0$ such that $\vert f(x) \vert \leq M$ for all $x \in [a, b]$. Suppose $f$ has exactly one discontinuity in $[a, b]$ call it $c$. Without loss of generality suppose $c \in (a, b)$. Let $\varepsilon > 0$. Choose $\delta > 0$ such that $\displaystyle \delta = \frac{\varepsilon}{12M}$. Observe that $f$ is continuous on $[a, c - \delta]$ and $[c + \delta, b]$. By Theorem 6.2 $f$ is Riemann-integrable on $[a, c - \delta]$ and $[c + \delta, b]$. By Theorem 6.1 there exists a partition $P_1$ of $[a, c - \delta]$ with $U(P_1, f) - L(P_1, f) < \varepsilon/3$. Again, by Theorem 6.1 there exists a partition $P_2$ of $[c + \delta, b]$ with $U(P_2, f) - L(P_2, f) < \varepsilon/3$. Define $P = P_1 \cup P_2$. Now observe that            \begin{align*} 			U(P, f) &=  U(P_1, f) + 2\delta \cdot \sup_{x \in [c - \delta, c+ \delta]} f(x) + U(P_2, f) \\ 			&\leq U(P_1, f) + 2\delta \cdot M + U(P_2, f) 		\end{align*}   and            \begin{align*} 			L(P, f) &= L(P_1, f) + 2\delta \cdot \inf_{x \in [c - \delta, c+ \delta]} f(x) + L(P_2, f)\\ 			& \geq L(P_1, f) + 2\delta \cdot (-M) + L(P_2, f)  		\end{align*}   which is implies $$-L(P, f) \leq - L(P_1, f) + 2\delta \cdot M - L(P_2, f) $$ Hence we have           \begin{align*} 			U(P, f) - L(P, f) &\leq U(P_1, f) + 2\delta \cdot M + U(P_2, f) - L(P_1, f) + 2\delta \cdot M - L(P_2, f)\\ 			&= \big[U(P_1, f) - L(P_1, f) \big] + 4 \delta \cdot M + \big[U(P_2, f) - L(P_2, f)\big]\\ 			&< \frac\varepsilon3 + 4M\frac{\varepsilon}{12M} + \frac\varepsilon3\\ 			&= \frac\varepsilon3 +\frac\varepsilon3 +\frac\varepsilon3 \\ 			&= \varepsilon 		\end{align*}           Thus by Theorem 6.1 we can conclude that $f$ is Riemann-integrable on $[a, b]$. My question namely is: is it sufficient to check that $c \in (a, b)$ or should I explicitly show that it holds when the discontinuity is on the endpoints? For the second problem, I am thinking of using induction on the first problem, but I haven't seen an induction problem done like that so I don't know how to get started. Any push in the right direction would be greatly appreciated!","I have two problems: Prove that if $f$ is bounded on $[a, b]$ and has exactly one discontinuity in $[a, b]$ then $f$ is Riemann-integrable on $[a, b]$. Prove that if $f$ is bounded on $[a, b]$ and $f$ has only finitely many discontinuities in $[a, b]$ then $f$ is Riemann-integrable on $[a, b]$. I have a possible proof for the first one which I would like checked: Theorem 6.1 states that $f$ is Riemann-integrable on $[a, b]$ iff given any $\varepsilon > 0$ there exists a partition $P$ of $[a, b]$ with $U(P, f) - L(P, f) < \varepsilon$. Theorem 6.2 states that if $f$ is continuous on $[a, b]$ then it is Riemann-integrable on $[a, b]$. Suppose $f$ is bounded on $[a, b]$. Then by definition there exists $M > 0$ such that $\vert f(x) \vert \leq M$ for all $x \in [a, b]$. Suppose $f$ has exactly one discontinuity in $[a, b]$ call it $c$. Without loss of generality suppose $c \in (a, b)$. Let $\varepsilon > 0$. Choose $\delta > 0$ such that $\displaystyle \delta = \frac{\varepsilon}{12M}$. Observe that $f$ is continuous on $[a, c - \delta]$ and $[c + \delta, b]$. By Theorem 6.2 $f$ is Riemann-integrable on $[a, c - \delta]$ and $[c + \delta, b]$. By Theorem 6.1 there exists a partition $P_1$ of $[a, c - \delta]$ with $U(P_1, f) - L(P_1, f) < \varepsilon/3$. Again, by Theorem 6.1 there exists a partition $P_2$ of $[c + \delta, b]$ with $U(P_2, f) - L(P_2, f) < \varepsilon/3$. Define $P = P_1 \cup P_2$. Now observe that            \begin{align*} 			U(P, f) &=  U(P_1, f) + 2\delta \cdot \sup_{x \in [c - \delta, c+ \delta]} f(x) + U(P_2, f) \\ 			&\leq U(P_1, f) + 2\delta \cdot M + U(P_2, f) 		\end{align*}   and            \begin{align*} 			L(P, f) &= L(P_1, f) + 2\delta \cdot \inf_{x \in [c - \delta, c+ \delta]} f(x) + L(P_2, f)\\ 			& \geq L(P_1, f) + 2\delta \cdot (-M) + L(P_2, f)  		\end{align*}   which is implies $$-L(P, f) \leq - L(P_1, f) + 2\delta \cdot M - L(P_2, f) $$ Hence we have           \begin{align*} 			U(P, f) - L(P, f) &\leq U(P_1, f) + 2\delta \cdot M + U(P_2, f) - L(P_1, f) + 2\delta \cdot M - L(P_2, f)\\ 			&= \big[U(P_1, f) - L(P_1, f) \big] + 4 \delta \cdot M + \big[U(P_2, f) - L(P_2, f)\big]\\ 			&< \frac\varepsilon3 + 4M\frac{\varepsilon}{12M} + \frac\varepsilon3\\ 			&= \frac\varepsilon3 +\frac\varepsilon3 +\frac\varepsilon3 \\ 			&= \varepsilon 		\end{align*}           Thus by Theorem 6.1 we can conclude that $f$ is Riemann-integrable on $[a, b]$. My question namely is: is it sufficient to check that $c \in (a, b)$ or should I explicitly show that it holds when the discontinuity is on the endpoints? For the second problem, I am thinking of using induction on the first problem, but I haven't seen an induction problem done like that so I don't know how to get started. Any push in the right direction would be greatly appreciated!",,"['real-analysis', 'integration', 'continuity']"
26,Understanding a proof in Spivak,Understanding a proof in Spivak,,"This is from Spivak's chapter 23. He wants to show that the function $$f(x) = \sum_{n=1}^{\infty} \frac{1}{10^n}\{10^nx\}$$ is continuous everywhere, but differentiable nowhere I have only an excerpt of the proof and that is all I need because my question lies in his choice of $h_m = 10^{-m}$. I know he wants a decreasing sequence such that it converges to $0$ as he stated, but why $10^{-m}$ in particular? Why not something like $1/m^2$? And why is the difference quotient $$\lim_{m\to\infty}\dfrac{f(a+h_m)-f(a)}{h_m}$$, but not $$\lim_{n\to\infty}\dfrac{f(a+h_n)-f(a)}{h_n}$$ Why the dependence on a new number $m$? Also why does he think it is sufficient for $0 < a \leq 1$? Finally, I may be wrong but he set $$\{ 10^n (a + h_m)\} + \{ 10^n a\} = 0$$ for the reason that the partial sums of the convergent sequence (though it is finite) must go to $0$? EDIT : I will write out the rest of the proof later today.","This is from Spivak's chapter 23. He wants to show that the function $$f(x) = \sum_{n=1}^{\infty} \frac{1}{10^n}\{10^nx\}$$ is continuous everywhere, but differentiable nowhere I have only an excerpt of the proof and that is all I need because my question lies in his choice of $h_m = 10^{-m}$. I know he wants a decreasing sequence such that it converges to $0$ as he stated, but why $10^{-m}$ in particular? Why not something like $1/m^2$? And why is the difference quotient $$\lim_{m\to\infty}\dfrac{f(a+h_m)-f(a)}{h_m}$$, but not $$\lim_{n\to\infty}\dfrac{f(a+h_n)-f(a)}{h_n}$$ Why the dependence on a new number $m$? Also why does he think it is sufficient for $0 < a \leq 1$? Finally, I may be wrong but he set $$\{ 10^n (a + h_m)\} + \{ 10^n a\} = 0$$ for the reason that the partial sums of the convergent sequence (though it is finite) must go to $0$? EDIT : I will write out the rest of the proof later today.",,"['calculus', 'real-analysis']"
27,Problem involving infinite intersection of closed bounded nested sets.,Problem involving infinite intersection of closed bounded nested sets.,,"If ${E_n}$ is a sequence of closed nonempty and bounded sets in a complete metric space $X,$ if $E_{n+1} \subset E_n,$ and if $\lim_{n\rightarrow \infty}\text{diam}(E_n) =0,$ then $\bigcap_1^\infty E_n$ contains exactly one point. I am not sure why I need that the sets are bounded. My proof is as follows:  Begin by constructing a sequence $(p_n)$ such that the $n^{th}$ term is an element of $E_n.$ $\lim_{n\rightarrow \infty}\text{diam}(E_n)=0,$ for each $\epsilon>0$ there is an $N$ such that if $n\geq N$, $|\text{diam}(E_n)-0| <\epsilon.$ Fix N. This implies if $p$ and $q$ are in $E_N$, $d(p,q)<\epsilon$. By the way we constructed our sequence, we know that there is some $p_N\in E_N$ and for all $p_{N'}$ and $p_{M'}$  where $N'\geq N',M'\geq N$, $d(p_{N'},p_{M'})<\epsilon$ because $p_{N'}\in E_{N'} \subset E_{N}$ and $p_{M'}\in E_{M'} \subset E_{N}$. Thus this sequence is Cauchy. We are given that $X$ is complete, thus every Cauchy sequence converges. Let $\lim_{n\rightarrow\infty}p_n=p$. We know every open ball around $p$ must contain all but finitely many elements of the sequence--thus it must contain some element of each $E_n$.  Therefore $p$ is a limit point of each $E_n$ since the sets are nested. Since each $E_n$ is closed, $p$ must be an element of each $E_n$.  Thus $\bigcap_1^\infty E_n$ is nonempty. At this point we just need to show that $p$ is the only element in the grand intersection. Assume via contradiction that there exists more than one point $p$ in the  grand intersection. Call it $q$.  Let $L=d(p,q)$.  Choose $\epsilon<L$.  Then for each $n$, $p\in E_n$ and $q\in E_n$ and $\text{diam}(E_n)\geq L>\epsilon$.  Thus,$\lim_{n\rightarrow \infty}\text{diam}(E_n)\neq 0$--a contradiction. Therefore $\bigcap_1^\infty E_n$ contains only one point, namely $p$. I didn't use that the sets were bounded anywhere, which leads me to believe this logic is in some way flawed.","If ${E_n}$ is a sequence of closed nonempty and bounded sets in a complete metric space $X,$ if $E_{n+1} \subset E_n,$ and if $\lim_{n\rightarrow \infty}\text{diam}(E_n) =0,$ then $\bigcap_1^\infty E_n$ contains exactly one point. I am not sure why I need that the sets are bounded. My proof is as follows:  Begin by constructing a sequence $(p_n)$ such that the $n^{th}$ term is an element of $E_n.$ $\lim_{n\rightarrow \infty}\text{diam}(E_n)=0,$ for each $\epsilon>0$ there is an $N$ such that if $n\geq N$, $|\text{diam}(E_n)-0| <\epsilon.$ Fix N. This implies if $p$ and $q$ are in $E_N$, $d(p,q)<\epsilon$. By the way we constructed our sequence, we know that there is some $p_N\in E_N$ and for all $p_{N'}$ and $p_{M'}$  where $N'\geq N',M'\geq N$, $d(p_{N'},p_{M'})<\epsilon$ because $p_{N'}\in E_{N'} \subset E_{N}$ and $p_{M'}\in E_{M'} \subset E_{N}$. Thus this sequence is Cauchy. We are given that $X$ is complete, thus every Cauchy sequence converges. Let $\lim_{n\rightarrow\infty}p_n=p$. We know every open ball around $p$ must contain all but finitely many elements of the sequence--thus it must contain some element of each $E_n$.  Therefore $p$ is a limit point of each $E_n$ since the sets are nested. Since each $E_n$ is closed, $p$ must be an element of each $E_n$.  Thus $\bigcap_1^\infty E_n$ is nonempty. At this point we just need to show that $p$ is the only element in the grand intersection. Assume via contradiction that there exists more than one point $p$ in the  grand intersection. Call it $q$.  Let $L=d(p,q)$.  Choose $\epsilon<L$.  Then for each $n$, $p\in E_n$ and $q\in E_n$ and $\text{diam}(E_n)\geq L>\epsilon$.  Thus,$\lim_{n\rightarrow \infty}\text{diam}(E_n)\neq 0$--a contradiction. Therefore $\bigcap_1^\infty E_n$ contains only one point, namely $p$. I didn't use that the sets were bounded anywhere, which leads me to believe this logic is in some way flawed.",,"['real-analysis', 'sequences-and-series']"
28,Lebesgue Integral Questions,Lebesgue Integral Questions,,"I'd like to show some properties of the Lebesgue integral. I'd like to show that if $f$ is a simple function which is zero almost everywhere, then the Lebesgue integral $\int f(x) dx = 0$. Similarly, I'd like to show this is also true for a measurable function $f$ which is zero almost everywhere. I'm working through a real analysis textbook on my own, and I'm not quite sure what to do about this ""almost everywhere."" Do I have to consider separately a set of measure zero? Thank you as always. Attempt for simple function: If $f$ is a simple function that is zero almost everywhere, then $f = \sum_{i=1}^{n}a_{i}\chi_{E_i} = 0$. Then, for each $i$, either $a_i = 0$ or else $m(E_i)= 0$. By definition, $\int f(x) = \sum_{i=1}^{n}a_{1}m(E_i)$. This summation is the sum of zeros. Thus,  $\int f(x) = 0$ as desired. Attempt for measurable function: Assume $f$ is a non-negative measurable function that is zero almost everywhere. By definition, $\int f(x) dx = \lim_{n \to \infty}\int f_n(x) dx$ where $\{f_n\}$ is a sequence of increasing, non-negative, simple functions that are all less than $f$. Since $f$ is zero almost everywhere, then each non-negative, simple $f_n$ must also be zero almost everywhere. Now, from above, we know that for each $n \in \mathbb{N}$,  $\int f_n(x) dx = 0$. Then, $\int f(x) dx = \lim_{n \to \infty} 0 = 0$. Thus, $\int f(x) dx = 0$. Now, for the general case... We can write any measurable function $f$ in terms of its positive and negative parts. So, $f(x) = f^+(x) - f^-(x)$. Both $f^+(x)$ and $f^-(x)$ are non-negative. Now if $f$ is zero almost everywhere, then both $f^+(x)$ and $f^-(x)$ are non-negative and zero almost everywhere. Then, by above, $\int f^+(x) dx= \int f^-(x) dx= 0$ And by definition, $\int f(x) dx = \int f^+(x) dx - \int f^-(x) dx$. So, $\int f(x) dx = 0 - 0 = 0$ as desired.","I'd like to show some properties of the Lebesgue integral. I'd like to show that if $f$ is a simple function which is zero almost everywhere, then the Lebesgue integral $\int f(x) dx = 0$. Similarly, I'd like to show this is also true for a measurable function $f$ which is zero almost everywhere. I'm working through a real analysis textbook on my own, and I'm not quite sure what to do about this ""almost everywhere."" Do I have to consider separately a set of measure zero? Thank you as always. Attempt for simple function: If $f$ is a simple function that is zero almost everywhere, then $f = \sum_{i=1}^{n}a_{i}\chi_{E_i} = 0$. Then, for each $i$, either $a_i = 0$ or else $m(E_i)= 0$. By definition, $\int f(x) = \sum_{i=1}^{n}a_{1}m(E_i)$. This summation is the sum of zeros. Thus,  $\int f(x) = 0$ as desired. Attempt for measurable function: Assume $f$ is a non-negative measurable function that is zero almost everywhere. By definition, $\int f(x) dx = \lim_{n \to \infty}\int f_n(x) dx$ where $\{f_n\}$ is a sequence of increasing, non-negative, simple functions that are all less than $f$. Since $f$ is zero almost everywhere, then each non-negative, simple $f_n$ must also be zero almost everywhere. Now, from above, we know that for each $n \in \mathbb{N}$,  $\int f_n(x) dx = 0$. Then, $\int f(x) dx = \lim_{n \to \infty} 0 = 0$. Thus, $\int f(x) dx = 0$. Now, for the general case... We can write any measurable function $f$ in terms of its positive and negative parts. So, $f(x) = f^+(x) - f^-(x)$. Both $f^+(x)$ and $f^-(x)$ are non-negative. Now if $f$ is zero almost everywhere, then both $f^+(x)$ and $f^-(x)$ are non-negative and zero almost everywhere. Then, by above, $\int f^+(x) dx= \int f^-(x) dx= 0$ And by definition, $\int f(x) dx = \int f^+(x) dx - \int f^-(x) dx$. So, $\int f(x) dx = 0 - 0 = 0$ as desired.",,['real-analysis']
29,Question about limit points of a Subset of $\mathbb{R}$,Question about limit points of a Subset of,\mathbb{R},"The question : Let $D$ be a nonempty subset of the reals that is bounded above. Is the supremum of $D$ a limit point of $D$? My Reasoning: I think this is false for these two cases. Case 1:If I look at $D = \{n \in \mathbb{Z} | n \le 0\}$ the supremum is $0$. And since I need a convergent sequence $\{x_n\} \subset D/\{0\}$ the converges to $0$ for it to be a limit point I can say in this case if I look at $\epsilon = \frac{1}{2}$ for the converges of the sequence it will fail to converge and so $0$ isn't a limit point. Case 2: Also if I look at $D = {0}$ then the supremum is $0$. And $D$ is a subset of the reals. So if I look for a sequence $\{x_n\} \subset D/\{0\}$ I can't make one because $D/\{0\}$ is the empty set. My question is this. Since the problem asked about an arbitrary subset of the reals $D$, can I define $D$ to give a counterexample like above or have I misunderstood the question? --Thanks in advance.","The question : Let $D$ be a nonempty subset of the reals that is bounded above. Is the supremum of $D$ a limit point of $D$? My Reasoning: I think this is false for these two cases. Case 1:If I look at $D = \{n \in \mathbb{Z} | n \le 0\}$ the supremum is $0$. And since I need a convergent sequence $\{x_n\} \subset D/\{0\}$ the converges to $0$ for it to be a limit point I can say in this case if I look at $\epsilon = \frac{1}{2}$ for the converges of the sequence it will fail to converge and so $0$ isn't a limit point. Case 2: Also if I look at $D = {0}$ then the supremum is $0$. And $D$ is a subset of the reals. So if I look for a sequence $\{x_n\} \subset D/\{0\}$ I can't make one because $D/\{0\}$ is the empty set. My question is this. Since the problem asked about an arbitrary subset of the reals $D$, can I define $D$ to give a counterexample like above or have I misunderstood the question? --Thanks in advance.",,"['real-analysis', 'general-topology']"
30,Existence of mixed partials in Clairaut's theorem.,Existence of mixed partials in Clairaut's theorem.,,"In almost all statements of Clairaut's theorem, the equality of the mixed partials is given only after the assumption that both mixed partials exist and are continuous. In this blog post here , the author proves a stronger statement ""At some point $\vec{x}=(x_0,y_0)$ , if the partials $f_x$ and $f_y$ are both continuous and if $f_{xy}$ exists and is continuous, then $f_{yx}$ exists and we have $f_{yx} = f_{xy}$ ."" The proof goes as follows: Using the mean value theorem, the author obtains the statement $$\lim_{h\rightarrow 0}\ \lim_{k\rightarrow 0}\ f_{xy}(x_0 + \overline{h}, y_0 +\overline{k}) = f_{yx}(x_0, y_0)$$ for some point $\overline{h} \in (0, h)$ and $\overline{k} \in (0, k)$ . Since $\overline{h}$ depends on $k$ , the limit cannot be taken trivially. From the continuity of $f_{xy}$ we get $$\mid f_{xy}(x,y) - f_{xy}(x_0, y_0)\mid < \frac{\epsilon}{2}$$ for $(x,y)$ within $\delta$ of $(x_0, y_0)$ . If we take $\mid h \mid$ and $\mid k\mid$ small enough, say less than $\frac{\delta}{2}$ , it follows that the point $(x_0 + \overline{h}, y_0 +\overline{k})$ satisfies the limit. So we fix $h$ small enough to obtain $$\mid f_{xy}(x_0 + \overline{h},y_0 + \overline{k}) - f_{xy}(x_0, y_0)\mid < \frac{\epsilon}{2}$$ for $\mid k\mid < \frac{\delta}{2}$ . What he does next is totally lost to me. He says we can take the limit from $k\rightarrow 0$ , and when we do so, the inequality may become an equality (why?). But since we chose $\frac{\epsilon}{2}$ initially, we obtain $$\mid \lim_{k\rightarrow 0}\left[f_{xy}(x_0 + \overline{h},y_0 + \overline{k})\right] - f_{xy}(x_0, y_0)\mid \le \frac{\epsilon}{2} < \epsilon$$ from which the limit is established. My main questions concern what exactly he did in the last few steps. How could the inequality possibly become an equality? And it seems to me that what he did is no different than just trivially taking $k$ to zero and then $h$ to zero.","In almost all statements of Clairaut's theorem, the equality of the mixed partials is given only after the assumption that both mixed partials exist and are continuous. In this blog post here , the author proves a stronger statement ""At some point , if the partials and are both continuous and if exists and is continuous, then exists and we have ."" The proof goes as follows: Using the mean value theorem, the author obtains the statement for some point and . Since depends on , the limit cannot be taken trivially. From the continuity of we get for within of . If we take and small enough, say less than , it follows that the point satisfies the limit. So we fix small enough to obtain for . What he does next is totally lost to me. He says we can take the limit from , and when we do so, the inequality may become an equality (why?). But since we chose initially, we obtain from which the limit is established. My main questions concern what exactly he did in the last few steps. How could the inequality possibly become an equality? And it seems to me that what he did is no different than just trivially taking to zero and then to zero.","\vec{x}=(x_0,y_0) f_x f_y f_{xy} f_{yx} f_{yx} = f_{xy} \lim_{h\rightarrow 0}\ \lim_{k\rightarrow 0}\ f_{xy}(x_0 + \overline{h}, y_0 +\overline{k}) = f_{yx}(x_0, y_0) \overline{h} \in (0, h) \overline{k} \in (0, k) \overline{h} k f_{xy} \mid f_{xy}(x,y) - f_{xy}(x_0, y_0)\mid < \frac{\epsilon}{2} (x,y) \delta (x_0, y_0) \mid h \mid \mid k\mid \frac{\delta}{2} (x_0 + \overline{h}, y_0 +\overline{k}) h \mid f_{xy}(x_0 + \overline{h},y_0 + \overline{k}) - f_{xy}(x_0, y_0)\mid < \frac{\epsilon}{2} \mid k\mid < \frac{\delta}{2} k\rightarrow 0 \frac{\epsilon}{2} \mid \lim_{k\rightarrow 0}\left[f_{xy}(x_0 + \overline{h},y_0 + \overline{k})\right] - f_{xy}(x_0, y_0)\mid \le \frac{\epsilon}{2} < \epsilon k h","['calculus', 'real-analysis', 'multivariable-calculus']"
31,Separability $\mathbb{R}^2$,Separability,\mathbb{R}^2,"I know that $\mathbb{R}^2$ with the post office metric is not separable. And the post office metric is defined by $m(x,y) = d(x,0) + d(y,0)$ for distinct points $x$ and $y$, and $m(x,x) = 0$ where $d$ is metric on $\mathbb{R}^2$. My idea for the proof: For every $x \in \mathbb R^2$ we can find an $r_x$ such that $x$ is seperated from other elements of $\mathbb R^2$ (for instance if $r_x = d(x,0)$). And for a $D$ which is dense in $\mathbb R^2$ we can find a $y \in D$ st $y \in B(x,r_x)$. Then, we can match each $x \in \mathbb R^2$ with a $y \in D$. And since $\mathbb R^2$ is uncountable, $D$ is also uncountable. Thus, $\mathbb R^2$ is not separable since any dense set in it is uncountable. Is my proof correct? I am not sure since $r_x$ depends on $x$.","I know that $\mathbb{R}^2$ with the post office metric is not separable. And the post office metric is defined by $m(x,y) = d(x,0) + d(y,0)$ for distinct points $x$ and $y$, and $m(x,x) = 0$ where $d$ is metric on $\mathbb{R}^2$. My idea for the proof: For every $x \in \mathbb R^2$ we can find an $r_x$ such that $x$ is seperated from other elements of $\mathbb R^2$ (for instance if $r_x = d(x,0)$). And for a $D$ which is dense in $\mathbb R^2$ we can find a $y \in D$ st $y \in B(x,r_x)$. Then, we can match each $x \in \mathbb R^2$ with a $y \in D$. And since $\mathbb R^2$ is uncountable, $D$ is also uncountable. Thus, $\mathbb R^2$ is not separable since any dense set in it is uncountable. Is my proof correct? I am not sure since $r_x$ depends on $x$.",,['real-analysis']
32,$\int_3^5{\frac{x^2}{1+x^2}dx}$ by differentiation under the integral,by differentiation under the integral,\int_3^5{\frac{x^2}{1+x^2}dx},"I'm trying an easy problem to get my bearings using the method here . The integral is $$\int_3^5{\frac{x^2}{1+x^2}dx}$$. I would like to proceed, if possible to solve by defining: $$F(y) = \int_3^5{\frac{\sin{(y\cdot x})}{1+x^2}dx}$$ Then obtaining $$-F''(y) = \int_3^5{\frac{x^2\sin{(y\cdot x})}{1+x^2}dx}$$ Adding gives $$F(y) - F''(y)= \int_3^5{\frac{(1+x^2)\sin{(y\cdot x})}{1+x^2}dx}$$ or  $$F(y) - F''(y) - \frac{\cos{3y}-\cos{5y}}{y} = 0.$$ This is where I'm stuck.  I don't know how to solve the differential equation.  Any help would be greatly appreciated.  I'm assuming that this can be done.","I'm trying an easy problem to get my bearings using the method here . The integral is $$\int_3^5{\frac{x^2}{1+x^2}dx}$$. I would like to proceed, if possible to solve by defining: $$F(y) = \int_3^5{\frac{\sin{(y\cdot x})}{1+x^2}dx}$$ Then obtaining $$-F''(y) = \int_3^5{\frac{x^2\sin{(y\cdot x})}{1+x^2}dx}$$ Adding gives $$F(y) - F''(y)= \int_3^5{\frac{(1+x^2)\sin{(y\cdot x})}{1+x^2}dx}$$ or  $$F(y) - F''(y) - \frac{\cos{3y}-\cos{5y}}{y} = 0.$$ This is where I'm stuck.  I don't know how to solve the differential equation.  Any help would be greatly appreciated.  I'm assuming that this can be done.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
33,Proving that $\sum_{n=1}^{\infty}\left (\left|a_{n}\right|+\left|b_{n}\right|\right) <\infty $.,Proving that .,\sum_{n=1}^{\infty}\left (\left|a_{n}\right|+\left|b_{n}\right|\right) <\infty ,"Let $\frac{a_{0}}{2}+\sum_{n=1}^{\infty} a_{n} \cos n x+b_{n} \sin n x$ is absolutely convergent on $E$ , where $E$ is a measurable set with $mE>0$ . Prove that $\sum_{n=1}^{\infty}\left (\left|a_{n}\right|+\left|b_{n}\right|\right) <\infty $ . $a_{n} \cos n x+b_{n} \sin n x=r_{n} \cos \left (n x+\theta_{n}\right) $ , where $r_{n}=\sqrt{a_{n}^{2}+b_{n}^{2}} $ .   How to do then? I know Cantor-Lebesgue theorem: $a_n\cos nx+b_n\sin nx\to 0, x\in E$ implies that $a_n\to 0, b_n\to 0$ . But here? Using Cauchy criteria: $\sum_{k=m}^n |a_k\cos kx+b_k\sin kx|\to 0$ , as $m\to\infty$ ? But it seems not easy (or impossible) to change it to the form $a_n\cos nx+b_n\sin nx$","Let is absolutely convergent on , where is a measurable set with . Prove that . , where .   How to do then? I know Cantor-Lebesgue theorem: implies that . But here? Using Cauchy criteria: , as ? But it seems not easy (or impossible) to change it to the form","\frac{a_{0}}{2}+\sum_{n=1}^{\infty} a_{n} \cos n x+b_{n} \sin n x E E mE>0 \sum_{n=1}^{\infty}\left (\left|a_{n}\right|+\left|b_{n}\right|\right) <\infty  a_{n} \cos n x+b_{n} \sin n x=r_{n} \cos \left (n x+\theta_{n}\right)  r_{n}=\sqrt{a_{n}^{2}+b_{n}^{2}}  a_n\cos nx+b_n\sin nx\to 0, x\in E a_n\to 0, b_n\to 0 \sum_{k=m}^n |a_k\cos kx+b_k\sin kx|\to 0 m\to\infty a_n\cos nx+b_n\sin nx","['real-analysis', 'sequences-and-series']"
34,The Weak Besicovitch Covering Property and the Lebesgue Differentation Property,The Weak Besicovitch Covering Property and the Lebesgue Differentation Property,,"Some preliminary terminology. Before making the question let me introduce some terminology. Notation. Let $X$ be a set and $A$ a subset of $X$ . I denote by $\chi_A\colon X\to\{0,1\}$ the characteristic function of $A$ . Definition $1$ . We say that a metric space $(X,d)$ satisfies the Weak Besicovitch Covering Property $1$ (WBCP1) if there exists a constant $K\in\mathbb{N}^+$ such that every finite Besicovitch family of balls of $(X,d)$ has cardinality $\le K$ . Recall that a Besicovitch family of balls of $(X,d)$ is a family $\mathcal{F}$ of closed balls of $(X,d)$ such that the centre of each ball does not belong to any other ball of the family and such that $\bigcap\mathcal{F}\ne\emptyset$ . Proposition $a$ . The WBCP $1$ is equivalent to require that there exists a constant $K\in\mathbb{N}^+$ such that for every finite family of closed balls of $(X,d)$ there exists a subfamily $\mathcal{G}\subseteq\mathcal{F}$ such that \begin{equation} \chi_C\le\sum_{G\in\mathcal{G}}\chi_G\le K \end{equation} where $C$ is the set of all the centres of the balls of $\mathcal{F}$ . Definition $2$ . We say that a metric space $(X,d)$ satisfies the Weak Besicovitch Covering Property $2$ (WBCP2) if there exists a constant $N\in\mathbb{N}^+$ such that for every bounded set $A$ of $X$ and for every family of closed balls $\mathcal{F}$ of $(X,d)$ such that each point of $A$ is the centre of some ball of $\mathcal{F}$ and such that either $\sup\{r_B\mid B\in\mathcal{F}\}=+\infty$ or $\{r_B\mid B\in\mathcal{F}\}$ is a discrete subset of $(0,+\infty)$ , there exists a countable subfamily $\mathcal{G}\subseteq\mathcal{F}$ such that it holds \begin{equation} \chi_A\le\sum_{G\in\mathcal{G}}\chi_G\le N. \end{equation} Now, we are ready for the question. How can I prove the following proposition? It is taken from page 109 of the "" New Trends on Analysis and Geometry in Metric Spaces "", Levico Terme, Italy 2017. Proposition $b$ . The WBCP $1$ implies the WBCP $2$ in every metric space $(X,d)$ . A possible way to prove it could be to follow the idea of the proof of the fact that in every doubling metric space $(X,d)$ the WBCP $1$ is equivalent to the BCP (see Proposition 3.7 of the Article "" BESICOVITCH COVERING PROPERTY ON GRADED GROUPS AND APPLICATIONS TO MEASURE DIFFERENTIATION "" of Le Donne and Rigot.). I recall the BCP in the following definition. Definition 3. We say that a metric space $(X,d)$ satisfies the Besicovitch Covering Property (BCP) if there exists a constant $N\in\mathbb{N}^+$ such that for every bounded set $A$ of $X$ and for every family of closed balls $\mathcal{F}$ of $(X,d)$ such that each point of $A$ is the centre of some ball of $\mathcal{F}$ , there exists a countable subfamily $\mathcal{G}\subseteq\mathcal{F}$ such that it holds \begin{equation} \chi_A\le\sum_{G\in\mathcal{G}}\chi_G\le N. \end{equation} Why I need the proof of Proposition b? Because I'm trying to prove the implication 1 $\implies$ 2 of the Preiss Theorem that is the following one. Theorem. Let $(X,d)$ be a separable complete metric space. Then the following are equivalent. $(X,d)$ is $\sigma$ -finite dimensional, that is to say, there exists a sequence $\{X_n\}_{n\in\N}$ of subsets of $X$ such that $X=\bigcup_{n=0}^{+\infty}X_n$ and a sequence $\{s_n\}_{n\in\N}\subseteq(0,+\infty]$ such that every set $X_n$ has finite Nagata dimension inside $X$ on scale $s_n$ . $(X,d)$ has the Lebesegue Differentation Property, that is to say, for every locally finite Borel measure $\mu$ on $X$ it exists \begin{equation} \lim_{r\to0^+}\frac{1}{\mu(\mathbb{B}(x,r))}\int_{\mathbb{B}(x,r)}f\,d\mu=f(x)\quad\text{ for }\mu-a.e.\, x\in X \end{equation} for every $\mu$ -measurable function $f\colon X\to\overline{\mathbb{R}}$ such that \begin{equation} \int_{A}|f|\,d\mu<+\infty\text{ for all $\mu$-measurable bounded set $A\subseteq X$}. \end{equation} If anyone knows where I can find a detalied Proof of the latter theorem, please let me know becasuse it would be very usefull for me. I know that David Preiss proved it the "" Dimension of metrics and differentiation of measures "", General topology and its relations to modern analysis and algebra, V (Prague, 1981), Sigma Ser. Pure Math., vol. 3, Heldermann, Berlin, 1983, pp. 565–568. But I cannot find this article anywhere.","Some preliminary terminology. Before making the question let me introduce some terminology. Notation. Let be a set and a subset of . I denote by the characteristic function of . Definition . We say that a metric space satisfies the Weak Besicovitch Covering Property (WBCP1) if there exists a constant such that every finite Besicovitch family of balls of has cardinality . Recall that a Besicovitch family of balls of is a family of closed balls of such that the centre of each ball does not belong to any other ball of the family and such that . Proposition . The WBCP is equivalent to require that there exists a constant such that for every finite family of closed balls of there exists a subfamily such that where is the set of all the centres of the balls of . Definition . We say that a metric space satisfies the Weak Besicovitch Covering Property (WBCP2) if there exists a constant such that for every bounded set of and for every family of closed balls of such that each point of is the centre of some ball of and such that either or is a discrete subset of , there exists a countable subfamily such that it holds Now, we are ready for the question. How can I prove the following proposition? It is taken from page 109 of the "" New Trends on Analysis and Geometry in Metric Spaces "", Levico Terme, Italy 2017. Proposition . The WBCP implies the WBCP in every metric space . A possible way to prove it could be to follow the idea of the proof of the fact that in every doubling metric space the WBCP is equivalent to the BCP (see Proposition 3.7 of the Article "" BESICOVITCH COVERING PROPERTY ON GRADED GROUPS AND APPLICATIONS TO MEASURE DIFFERENTIATION "" of Le Donne and Rigot.). I recall the BCP in the following definition. Definition 3. We say that a metric space satisfies the Besicovitch Covering Property (BCP) if there exists a constant such that for every bounded set of and for every family of closed balls of such that each point of is the centre of some ball of , there exists a countable subfamily such that it holds Why I need the proof of Proposition b? Because I'm trying to prove the implication 1 2 of the Preiss Theorem that is the following one. Theorem. Let be a separable complete metric space. Then the following are equivalent. is -finite dimensional, that is to say, there exists a sequence of subsets of such that and a sequence such that every set has finite Nagata dimension inside on scale . has the Lebesegue Differentation Property, that is to say, for every locally finite Borel measure on it exists for every -measurable function such that If anyone knows where I can find a detalied Proof of the latter theorem, please let me know becasuse it would be very usefull for me. I know that David Preiss proved it the "" Dimension of metrics and differentiation of measures "", General topology and its relations to modern analysis and algebra, V (Prague, 1981), Sigma Ser. Pure Math., vol. 3, Heldermann, Berlin, 1983, pp. 565–568. But I cannot find this article anywhere.","X A X \chi_A\colon X\to\{0,1\} A 1 (X,d) 1 K\in\mathbb{N}^+ (X,d) \le K (X,d) \mathcal{F} (X,d) \bigcap\mathcal{F}\ne\emptyset a 1 K\in\mathbb{N}^+ (X,d) \mathcal{G}\subseteq\mathcal{F} \begin{equation}
\chi_C\le\sum_{G\in\mathcal{G}}\chi_G\le K
\end{equation} C \mathcal{F} 2 (X,d) 2 N\in\mathbb{N}^+ A X \mathcal{F} (X,d) A \mathcal{F} \sup\{r_B\mid B\in\mathcal{F}\}=+\infty \{r_B\mid B\in\mathcal{F}\} (0,+\infty) \mathcal{G}\subseteq\mathcal{F} \begin{equation}
\chi_A\le\sum_{G\in\mathcal{G}}\chi_G\le N.
\end{equation} b 1 2 (X,d) (X,d) 1 (X,d) N\in\mathbb{N}^+ A X \mathcal{F} (X,d) A \mathcal{F} \mathcal{G}\subseteq\mathcal{F} \begin{equation}
\chi_A\le\sum_{G\in\mathcal{G}}\chi_G\le N.
\end{equation} \implies (X,d) (X,d) \sigma \{X_n\}_{n\in\N} X X=\bigcup_{n=0}^{+\infty}X_n \{s_n\}_{n\in\N}\subseteq(0,+\infty] X_n X s_n (X,d) \mu X \begin{equation}
\lim_{r\to0^+}\frac{1}{\mu(\mathbb{B}(x,r))}\int_{\mathbb{B}(x,r)}f\,d\mu=f(x)\quad\text{ for }\mu-a.e.\, x\in X
\end{equation} \mu f\colon X\to\overline{\mathbb{R}} \begin{equation}
\int_{A}|f|\,d\mu<+\infty\text{ for all \mu-measurable bounded set A\subseteq X}.
\end{equation}","['real-analysis', 'measure-theory', 'metric-spaces', 'geometric-measure-theory']"
35,"For $ \{n\pmod{2 \pi}: n \in \{\ 0, 1, 2 ... 2^x\ \} \} $, is it possible to put a bound in terms of $x$ on the smallest difference between elements?","For , is it possible to put a bound in terms of  on the smallest difference between elements?"," \{n\pmod{2 \pi}: n \in \{\ 0, 1, 2 ... 2^x\ \} \}  x","Is it possible to put some sort of bounds on the smallest difference between elements for a limited range of integers $\mod 2\pi$ ? $$ A =\bigl\{n\pmod{2\pi} : n \in \{\ 0, 1, 2, ... 2^x\ \} \bigl\} $$ (or similarly $ A = \bigl\{n\cdot \frac{1}{2\pi} \bmod 1 : n \in \{\ 0, 1, 2, ... 2^x\ \}\} $ ) Where the smallest difference is defined as: $$d_{min} = \min(\ \{\ |a - b|\ :\ a,b\in \ A,\  a \ne b\}\ )$$ Can I show that some sort of bounds exist for $d_{min}$ such as (intuitively/pessimistically via Dirichlet approximation theorem): $$ 2^{-2x} < d_{min} < 2^{-x} $$ If not, what are the tightest bounds we can place on $d_{min}$ ? I'm comfortable with the answer here that shows $\{n\pmod{2 \pi}: n\in\Bbb N\}$ is dense in $[0,{2 \pi}]$ since $\pi$ is irrational, but the proof depends on arbitrarily large $n$ . In my concrete (software) case, I need to represent 44-bit integers as an angle in radians i.e. $(x=44)$ , and would like to know the smallest number of fixed point precision bits that would be needed to uniquely represent these values $\pmod{2 \pi}$ to ensure no collisions. It's easy to show that some of the smallest distances where $ b = 0$ are likely to occur for values of $a$ that are numerators in convergents of continued fraction approximations  of $2\pi$ , e.g. for: $$a = 44 ,\  b = 0,\ \ (2\pi \approx \frac{2\times22}{7})$$ or $$a = 710,\  b = 0,\ \ (2\pi \approx \frac{2\times355}{113})$$ or even the semiconvergents - the sum of numerators/denominators such that the denominator becomes even, e.g. $$a = 377,\  b = 0, \ \ (2\pi \approx \frac{355+22}{\frac{113+7}{2}})$$ So one might suppose that the smallest distances occur with numerators of the most accurate continued fraction approximations of $2\pi$ , but pairs like $$a = 103993, \ b = 0, \ \ (2\pi \approx \frac{103993}{\frac{33102}{2}}) $$ (ostensibly $\pi$ accurate to 9.2 decimal places) produce smaller gaps than the next (more accurate) approximation of $\pi$ (9.5 decimal places) $$a = 208696,\  b = 0, \ \ (2\pi \approx \frac{2\times104348}{33215}  \approx \frac{208341 + 355}{\frac{66317 + 113}{2}})$$ (This is probably because the smaller numerator already has an even denominator, so there's no need to double the numerator, and thus double the error). Edit: as Eric points out in his answer below, I should be looking at convergents of $2\pi$ directly, since I was using double the convergents of $\pi$ , the above musing can be ignored For the $b = 0$ case, is it sufficient to only look at numerators of convergents/semiconvergents which have an even denominator? Would another approach to this be better? Edit, having thought about it a bit more: If distance is defined $$ d(m, n) = | (m\pmod{2\pi}) - (n\pmod{2\pi}) |\ \ \text{for}\ m,n \in \Bbb{N}$$ I think we can say that the distance $d(a + b, b) = d(a,0)$ for $b \in \Bbb N$ , so all the minima from $a$ being a convergent numerator where $n = 0$ will be trivially replicated at $ m = a+b $ for $n = b$ . The same applies for $d(a, b - a) = d(0,b)$ for $a \in \Bbb N$ , and due to the symmetry of $a$ and $b$ , this allows the reduction of any pair of values into a single measure of accuracy of an approximation of $2\pi$ . Could there be other minima where $ n > 0 $ that don't correspond to a trivial $ m = a+b $ case, i.e. could there be a $b$ such that $d(a, b) < d(a,0)$ ? I think all members of $A$ must be positive, and so finding a smaller minimum where $b > 0$ is a matter of looking for $n$ such that $n\pmod{2\pi}$ is as close as possible to but not equal to $ m\pmod{2\pi}$ . In the case where $$n\pmod{2\pi} < m\pmod{2\pi}$$ this is effectively trying to find a tighter approximation of ${2\pi}$ , and can likely only happen with a ""better"" continued fraction convergent numerator. In the case of $$(m\ \text{mod}\ {2\pi}) < (n\ \text{mod}\ {2\pi}) < 2(m\ \text{mod}\ {2\pi}) $$ due to the symmetry in the definition of distance, $m$ and $n$ can be swapped, and the problem again becomes one of trying to find a  ""better"" continued fraction convergent numerator. The relationship for a convergent $ \frac{p}{q}$ is: $$p\pmod{2\pi} = (2\pi - \frac{p}{q})\times q$$ Via the mediant inequality, I think we can always find a midpoint between 2 convergents by adding the numerators together (and denominators), and I think this applies whether the 2 convergents are overestimates or underestimates... If you have one overestimate and one underestimate, then the new estimate for $2\pi$ becomes better than either. $$\frac{p}{q} < \frac{p+a}{q+b} < \frac{a}{b}$$ It follows that you can do this an arbitrary number of times to get successively closer distances between pairs of convergents, but how quickly does that grow the $d_{min}$ ? This implies a sort of fractal scale which evenly distributes integer values across $[0,2\pi]$ at a granularity roughly the ""scale"" of the preceding convergent numerator, then as soon as the next convergent numerator is reached, the scale gets smaller, and so on, so the $d_{min}$ for any number $n$ can never be smaller than the value of the largest convergent numerator smaller than n $\pmod{2\pi}$ . So my question rearranges to $$\log(p\pmod{2\pi}) = \log(\lvert2\pi - \frac{p}{q}\rvert) + \log(q)$$ Which is almost the same question as this Does that mean $2^{−(x+\epsilon)}$ is enough, where $\epsilon$ is how much better the convergent is than the size of the denominator (the negation of the right hand term above)? If so, does that mean for $\mu(\pi) =2$ that $\epsilon \leq x$ ?","Is it possible to put some sort of bounds on the smallest difference between elements for a limited range of integers ? (or similarly ) Where the smallest difference is defined as: Can I show that some sort of bounds exist for such as (intuitively/pessimistically via Dirichlet approximation theorem): If not, what are the tightest bounds we can place on ? I'm comfortable with the answer here that shows is dense in since is irrational, but the proof depends on arbitrarily large . In my concrete (software) case, I need to represent 44-bit integers as an angle in radians i.e. , and would like to know the smallest number of fixed point precision bits that would be needed to uniquely represent these values to ensure no collisions. It's easy to show that some of the smallest distances where are likely to occur for values of that are numerators in convergents of continued fraction approximations  of , e.g. for: or or even the semiconvergents - the sum of numerators/denominators such that the denominator becomes even, e.g. So one might suppose that the smallest distances occur with numerators of the most accurate continued fraction approximations of , but pairs like (ostensibly accurate to 9.2 decimal places) produce smaller gaps than the next (more accurate) approximation of (9.5 decimal places) (This is probably because the smaller numerator already has an even denominator, so there's no need to double the numerator, and thus double the error). Edit: as Eric points out in his answer below, I should be looking at convergents of directly, since I was using double the convergents of , the above musing can be ignored For the case, is it sufficient to only look at numerators of convergents/semiconvergents which have an even denominator? Would another approach to this be better? Edit, having thought about it a bit more: If distance is defined I think we can say that the distance for , so all the minima from being a convergent numerator where will be trivially replicated at for . The same applies for for , and due to the symmetry of and , this allows the reduction of any pair of values into a single measure of accuracy of an approximation of . Could there be other minima where that don't correspond to a trivial case, i.e. could there be a such that ? I think all members of must be positive, and so finding a smaller minimum where is a matter of looking for such that is as close as possible to but not equal to . In the case where this is effectively trying to find a tighter approximation of , and can likely only happen with a ""better"" continued fraction convergent numerator. In the case of due to the symmetry in the definition of distance, and can be swapped, and the problem again becomes one of trying to find a  ""better"" continued fraction convergent numerator. The relationship for a convergent is: Via the mediant inequality, I think we can always find a midpoint between 2 convergents by adding the numerators together (and denominators), and I think this applies whether the 2 convergents are overestimates or underestimates... If you have one overestimate and one underestimate, then the new estimate for becomes better than either. It follows that you can do this an arbitrary number of times to get successively closer distances between pairs of convergents, but how quickly does that grow the ? This implies a sort of fractal scale which evenly distributes integer values across at a granularity roughly the ""scale"" of the preceding convergent numerator, then as soon as the next convergent numerator is reached, the scale gets smaller, and so on, so the for any number can never be smaller than the value of the largest convergent numerator smaller than n . So my question rearranges to Which is almost the same question as this Does that mean is enough, where is how much better the convergent is than the size of the denominator (the negation of the right hand term above)? If so, does that mean for that ?","\mod 2\pi  A =\bigl\{n\pmod{2\pi} : n \in \{\ 0, 1, 2, ... 2^x\ \} \bigl\}   A = \bigl\{n\cdot \frac{1}{2\pi} \bmod 1 : n \in \{\ 0, 1, 2, ... 2^x\ \}\}  d_{min} = \min(\ \{\ |a - b|\ :\ a,b\in \ A,\  a \ne b\}\ ) d_{min}  2^{-2x} < d_{min} < 2^{-x}  d_{min} \{n\pmod{2 \pi}: n\in\Bbb N\} [0,{2 \pi}] \pi n (x=44) \pmod{2 \pi}  b = 0 a 2\pi a = 44 ,\  b = 0,\ \ (2\pi \approx \frac{2\times22}{7}) a = 710,\  b = 0,\ \ (2\pi \approx \frac{2\times355}{113}) a = 377,\  b = 0, \ \ (2\pi \approx \frac{355+22}{\frac{113+7}{2}}) 2\pi a = 103993, \ b = 0, \ \ (2\pi \approx \frac{103993}{\frac{33102}{2}})  \pi \pi a = 208696,\  b = 0, \ \ (2\pi \approx \frac{2\times104348}{33215}  \approx \frac{208341 + 355}{\frac{66317 + 113}{2}}) 2\pi \pi b = 0  d(m, n) = | (m\pmod{2\pi}) - (n\pmod{2\pi}) |\ \ \text{for}\ m,n \in \Bbb{N} d(a + b, b) = d(a,0) b \in \Bbb N a n = 0  m = a+b  n = b d(a, b - a) = d(0,b) a \in \Bbb N a b 2\pi  n > 0   m = a+b  b d(a, b) < d(a,0) A b > 0 n n\pmod{2\pi}  m\pmod{2\pi} n\pmod{2\pi} < m\pmod{2\pi} {2\pi} (m\ \text{mod}\ {2\pi}) < (n\ \text{mod}\ {2\pi}) < 2(m\ \text{mod}\ {2\pi})  m n  \frac{p}{q} p\pmod{2\pi} = (2\pi - \frac{p}{q})\times q 2\pi \frac{p}{q} < \frac{p+a}{q+b} < \frac{a}{b} d_{min} [0,2\pi] d_{min} n \pmod{2\pi} \log(p\pmod{2\pi}) = \log(\lvert2\pi - \frac{p}{q}\rvert) + \log(q) 2^{−(x+\epsilon)} \epsilon \mu(\pi) =2 \epsilon \leq x","['real-analysis', 'integers', 'continued-fractions', 'diophantine-approximation']"
36,Bijection Between $\mathbb{R}$ and $\mathbb{R}\setminus\{1\}$,Bijection Between  and,\mathbb{R} \mathbb{R}\setminus\{1\},"I was looking for a way to establish the equinumerosity of $\mathbb{R}$ and $\mathbb{R}\setminus\{1\}.$ This is what I came up with: Consider $f:\mathbb{R}\rightarrow\mathbb{R}\setminus\{1\}$ such that $f(x)=x,$ if $x\in\{0\}\cup\{-\frac{F_{n}}{F_{n+1}}|n\in\mathbb{N}\},$ and $f(x)=1+\frac{1}{x},$ otherwise. Here, $F_{i}$ is the $i^{th}$ Fibonacci number. Is this a valid bijection? If it is, then I'll be really happy with myself because this took me a while to come up with. Perhaps there's a more straightforward bijection I'm missing? Additionally, how would one go about constructing a bijection from $\mathbb{R}$ to $\mathbb{R}\setminus\{1,2,3,\ldots,k\},$ where $k\in\mathbb{N}?$ Note: Just so there is no ambiguity, $F_{0}=0,F_1={1},F_2={1},$ and $F_{n+2}=F_{n+1}+F_{n}\forall n\in\mathbb{N}.$","I was looking for a way to establish the equinumerosity of and This is what I came up with: Consider such that if and otherwise. Here, is the Fibonacci number. Is this a valid bijection? If it is, then I'll be really happy with myself because this took me a while to come up with. Perhaps there's a more straightforward bijection I'm missing? Additionally, how would one go about constructing a bijection from to where Note: Just so there is no ambiguity, and","\mathbb{R} \mathbb{R}\setminus\{1\}. f:\mathbb{R}\rightarrow\mathbb{R}\setminus\{1\} f(x)=x, x\in\{0\}\cup\{-\frac{F_{n}}{F_{n+1}}|n\in\mathbb{N}\}, f(x)=1+\frac{1}{x}, F_{i} i^{th} \mathbb{R} \mathbb{R}\setminus\{1,2,3,\ldots,k\}, k\in\mathbb{N}? F_{0}=0,F_1={1},F_2={1}, F_{n+2}=F_{n+1}+F_{n}\forall n\in\mathbb{N}.","['real-analysis', 'elementary-set-theory', 'fibonacci-numbers']"
37,$m(\xi)=(-1)^{[\xi^2]}$ is not a Fourier multiplier,is not a Fourier multiplier,m(\xi)=(-1)^{[\xi^2]},"Let $$m(\xi)=(-1)^{[\xi^2]},\qquad \xi\in\mathbb R,$$ where $[a]$ means the largest integer less than or equal to $a$ . Prove that $m$ is not a Fourier multiplier on $L^p(\mathbb R)$ for some $p\neq 2$ . Note that $m:\mathbb{R}\to \mathbb{C}$ is called an $L^p(\mathbb R)$ Fourier multiplier if $$\|T_m f\|_p := \|(m\hat{f})^\vee\|_p \leq C\|f\|_p$$ for some $C>0$ , where $\hat{f}$ is the Fourier transform of $f$ , and $f^\vee$ is its inverse Fourier transform: $$\hat f(\xi)=\int_\mathbb{R} f(x)e^{-2\pi ix\xi}\,dx,\qquad f^\vee(x)=\int_\mathbb{R} f(\xi)e^{2\pi ix\xi}\,d\xi.$$ The problem comes from the following theorem: Theorem. If $m\in C(\mathbb R)$ is bouneded such that $m^2$ has bounded variation, then $m$ is an $L^p(\mathbb R)$ Fourier multiplier. I was wondering whether the condition $m\in C(\mathbb R)$ is redundant, and my teacher told me this counterexample. But I don't know how to show the counterexpamle. My try. I tried the functions $f_N$ such that $\hat{f_N}=\chi_{[0,\sqrt N]}$ . I wish to show that $$\sup_{N}\frac{\|T_m f_N\|_p}{\|f_N\|_p}=\infty.$$ It is direct to calculate that $f_N(x)=\frac{e^{2\pi i\sqrt N\ x}\ -1}{2\pi ix}$ , so $\|f_N\|_p\sim N^{\frac12-\frac1{2p}}.$ But I don't know how to estimate $\|T_m f_N\|p$ , so I stuck. Any help would be appreciated!","Let where means the largest integer less than or equal to . Prove that is not a Fourier multiplier on for some . Note that is called an Fourier multiplier if for some , where is the Fourier transform of , and is its inverse Fourier transform: The problem comes from the following theorem: Theorem. If is bouneded such that has bounded variation, then is an Fourier multiplier. I was wondering whether the condition is redundant, and my teacher told me this counterexample. But I don't know how to show the counterexpamle. My try. I tried the functions such that . I wish to show that It is direct to calculate that , so But I don't know how to estimate , so I stuck. Any help would be appreciated!","m(\xi)=(-1)^{[\xi^2]},\qquad \xi\in\mathbb R, [a] a m L^p(\mathbb R) p\neq 2 m:\mathbb{R}\to \mathbb{C} L^p(\mathbb R) \|T_m f\|_p := \|(m\hat{f})^\vee\|_p \leq C\|f\|_p C>0 \hat{f} f f^\vee \hat f(\xi)=\int_\mathbb{R} f(x)e^{-2\pi ix\xi}\,dx,\qquad f^\vee(x)=\int_\mathbb{R} f(\xi)e^{2\pi ix\xi}\,d\xi. m\in C(\mathbb R) m^2 m L^p(\mathbb R) m\in C(\mathbb R) f_N \hat{f_N}=\chi_{[0,\sqrt N]} \sup_{N}\frac{\|T_m f_N\|_p}{\|f_N\|_p}=\infty. f_N(x)=\frac{e^{2\pi i\sqrt N\ x}\ -1}{2\pi ix} \|f_N\|_p\sim N^{\frac12-\frac1{2p}}. \|T_m f_N\|p","['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
38,Why does my topology textbook (Munkres) define positive integers as the intersection of all inductive subsets of the reals?,Why does my topology textbook (Munkres) define positive integers as the intersection of all inductive subsets of the reals?,,"This is how the topology textbook I'm reading (Munkres) defines integers: A subset of the real numbers is ""inductive"" if it contains 1 and $1+x$ for all $x$ in the subset. The intersection of all inductive subsets of the reals is the set of positive integers. Why take this route involving the intersection of so many sets? I could define the positive integers given reals as $1$ along with any sum of positive integers and get the same set much more easily.","This is how the topology textbook I'm reading (Munkres) defines integers: A subset of the real numbers is ""inductive"" if it contains 1 and for all in the subset. The intersection of all inductive subsets of the reals is the set of positive integers. Why take this route involving the intersection of so many sets? I could define the positive integers given reals as along with any sum of positive integers and get the same set much more easily.",1+x x 1,"['real-analysis', 'foundations', 'natural-numbers']"
39,Which of these reasonings is correct?,Which of these reasonings is correct?,,"If $K(x)=-\ln\|x\|$ , we know that $\Delta K=\delta_0$ in $\mathbb{R^2}.$ Furthermore, we consider $f$ as the characteristic function of the ball $B(0,1)$ .  I have obtained two different results when I use divergence theorem, but I am not able to find which way is correct. Any help would be welcome because I am really stuck with this question. On the one hand \begin{align} \int_{\mathbb{R}^2}(K*f)(x)f(x)dx&=\int_{\mathbb{R}^2}(K*f)(x)(\delta_0*f)(x)dx=\int_{\mathbb{R}^2}(K*f)(x)\Delta K*f(x)dx\\&=\int_{\mathbb{R}^2}(K*f)(x)div (\nabla K*f(x))dx \\&=\lim_{r\to\infty}\Big[ \int_{C(0,r)}(K*f)(x)(\nabla K*f)(x)\cdot n(x)dx   \Big]-\int_{\mathbb{R}^2}(\nabla K*f)(x)\cdot(\nabla K*f)(x)dx. \end{align} On the other hand \begin{align} \int_{\mathbb{R}^2}(K*f)(x)f(x)dx&=\int_{B(0,1)}(K*f)(x)f(x)dx=\int_{B(0,1)}(K*f)(x)(\delta_0*f(x))dx\\&=\int_{B(0,1)}(K*f)(x)(\Delta K*f(x))dx=\int_{B(0,1)}(K*f)(x)div (\nabla K*f(x))dx \\&= \int_{C(0,1)}(K*f)(x)(\nabla K*f(x))\cdot n(x)dx-\int_{B(0,1)}(\nabla K*f(x))\cdot(\nabla K*f(x))dx. \end{align}","If , we know that in Furthermore, we consider as the characteristic function of the ball .  I have obtained two different results when I use divergence theorem, but I am not able to find which way is correct. Any help would be welcome because I am really stuck with this question. On the one hand On the other hand","K(x)=-\ln\|x\| \Delta K=\delta_0 \mathbb{R^2}. f B(0,1) \begin{align}
\int_{\mathbb{R}^2}(K*f)(x)f(x)dx&=\int_{\mathbb{R}^2}(K*f)(x)(\delta_0*f)(x)dx=\int_{\mathbb{R}^2}(K*f)(x)\Delta K*f(x)dx\\&=\int_{\mathbb{R}^2}(K*f)(x)div (\nabla K*f(x))dx
\\&=\lim_{r\to\infty}\Big[ \int_{C(0,r)}(K*f)(x)(\nabla K*f)(x)\cdot n(x)dx   \Big]-\int_{\mathbb{R}^2}(\nabla K*f)(x)\cdot(\nabla K*f)(x)dx.
\end{align} \begin{align}
\int_{\mathbb{R}^2}(K*f)(x)f(x)dx&=\int_{B(0,1)}(K*f)(x)f(x)dx=\int_{B(0,1)}(K*f)(x)(\delta_0*f(x))dx\\&=\int_{B(0,1)}(K*f)(x)(\Delta K*f(x))dx=\int_{B(0,1)}(K*f)(x)div (\nabla K*f(x))dx
\\&= \int_{C(0,1)}(K*f)(x)(\nabla K*f(x))\cdot n(x)dx-\int_{B(0,1)}(\nabla K*f(x))\cdot(\nabla K*f(x))dx.
\end{align}","['real-analysis', 'integration', 'functional-analysis', 'multivariable-calculus', 'partial-differential-equations']"
40,"Is $f(x,y)$ differentiable on some neighborhood of $(x_0,y_0) ?$",Is  differentiable on some neighborhood of,"f(x,y) (x_0,y_0) ?","A function $f:O\to \mathbb{R}$ , $O$ is an open subset in $\mathbb{R}^2$ , all of its first partial derivatives $f_{x}$ , $f_{y}$ are defined on $O$ . If we assume both $f_{x}$ and $f_{y}$ are differentiable at $(x_0,y_0)\in O$ , is $f(x,y)$ differentiable on some neighborhood of $(x_0,y_0) ?$ From above conditions,it’s apparent that $f(x,y)$ is continuous on a neighborhood of $(x_0,y_0)$ and differentiable at $(x_0,y_0)$ . I don’t think  there exists a neighborhood of $(x_0,y_0)$ such that $f(x,y)$ is differentiable on whole of it. My question is how to find the example which fails to differentiate on any neighborhood of $(x_0,y_0)$ but satisfies above conditions?","A function , is an open subset in , all of its first partial derivatives , are defined on . If we assume both and are differentiable at , is differentiable on some neighborhood of From above conditions,it’s apparent that is continuous on a neighborhood of and differentiable at . I don’t think  there exists a neighborhood of such that is differentiable on whole of it. My question is how to find the example which fails to differentiate on any neighborhood of but satisfies above conditions?","f:O\to \mathbb{R} O \mathbb{R}^2 f_{x} f_{y} O f_{x} f_{y} (x_0,y_0)\in O f(x,y) (x_0,y_0) ? f(x,y) (x_0,y_0) (x_0,y_0) (x_0,y_0) f(x,y) (x_0,y_0)","['real-analysis', 'multivariable-calculus', 'examples-counterexamples']"
41,Continuous functions on the unit simplex,Continuous functions on the unit simplex,,"For some $n\in\mathbb N$ , define the unit simplex as $$S\equiv\left\{\mathbf x\in\mathbb R_+^n\,\middle|\,\sum_{i=1}^nx_i=1\right\}.$$ For each $i\in\{1,\ldots,n\}$ , suppose that $f^i:S\to\mathbb R_+$ is a non-negative continuous function, $\alpha^i\geq0$ is a number, and let $\mathbf e^i\in S$ denote the vector whose $i$ th coordinate is $1$ and all other coordinates are $0$ . Suppose that the functions are such that $f^i(\mathbf e^i)\geq\alpha^i$ for every $i\in\{1,\ldots,n\}$ ; and $\displaystyle \sum_{i=1}^nf^i(\mathbf x)\geq\sum_{i=1}^n\alpha^i$ for every $\mathbf x\in S$ . Question: Does there necessarily exist some $\mathbf x^*\in S$ such that $f^i(\mathbf x^*)\geq\alpha^i$ for every $i\in\{1,\ldots,n\}$ ? The answer is affirmative for $n=2$ and it easily follows from the intermediate-value theorem. For higher dimensions, I am stuck with coming up with a proof—in fact, I feel diffidence as to whether the answer is affirmative in the first place. Any hints or counterexamples would be appreciated.","For some , define the unit simplex as For each , suppose that is a non-negative continuous function, is a number, and let denote the vector whose th coordinate is and all other coordinates are . Suppose that the functions are such that for every ; and for every . Question: Does there necessarily exist some such that for every ? The answer is affirmative for and it easily follows from the intermediate-value theorem. For higher dimensions, I am stuck with coming up with a proof—in fact, I feel diffidence as to whether the answer is affirmative in the first place. Any hints or counterexamples would be appreciated.","n\in\mathbb N S\equiv\left\{\mathbf x\in\mathbb R_+^n\,\middle|\,\sum_{i=1}^nx_i=1\right\}. i\in\{1,\ldots,n\} f^i:S\to\mathbb R_+ \alpha^i\geq0 \mathbf e^i\in S i 1 0 f^i(\mathbf e^i)\geq\alpha^i i\in\{1,\ldots,n\} \displaystyle \sum_{i=1}^nf^i(\mathbf x)\geq\sum_{i=1}^n\alpha^i \mathbf x\in S \mathbf x^*\in S f^i(\mathbf x^*)\geq\alpha^i i\in\{1,\ldots,n\} n=2","['real-analysis', 'continuity', 'convex-analysis']"
42,An interesting problem involving recursion,An interesting problem involving recursion,,"Given a continuous function $f:[0, 1] \rightarrow [0, 1]$ . Here we denote $f^n(x) = f(f^{n-1}(x))$ . For every $x_0 \in [0, 1]$ there exists $n \in \mathbb{N}$ such that $f^n(x_0) = 0$ . Prove that $f^N(x) \equiv 0$ for some $N$ . I've only managed to show that $f(0) = 0$ ( $f$ must have a fixed point but then it couldn't be anything than zero), hence all zeroes of $f^n(x)$ are also zeroes of $f^{n+1}(x)$ . Using Baire Theorem I concluded that there exists such $n_0 \in \mathbb{N}$ and a non-trivial open interval $(a, b)$ such that $\forall x \in (a, b) \ \ f^{n_0}(x) = 0$ (Let $A_n =$ $\{x \in [0,1] \mid f^n(x) = 0\}$ , then $\bigcup\limits_{i=1}^{\infty} A_{i} = [0, 1]$ and $[0, 1]$ is of second category) but I've no idea what to do next. How should I go about proving this statement?","Given a continuous function . Here we denote . For every there exists such that . Prove that for some . I've only managed to show that ( must have a fixed point but then it couldn't be anything than zero), hence all zeroes of are also zeroes of . Using Baire Theorem I concluded that there exists such and a non-trivial open interval such that (Let , then and is of second category) but I've no idea what to do next. How should I go about proving this statement?","f:[0, 1] \rightarrow [0, 1] f^n(x) = f(f^{n-1}(x)) x_0 \in [0, 1] n \in \mathbb{N} f^n(x_0) = 0 f^N(x) \equiv 0 N f(0) = 0 f f^n(x) f^{n+1}(x) n_0 \in \mathbb{N} (a, b) \forall x \in (a, b) \ \ f^{n_0}(x) = 0 A_n = \{x \in [0,1] \mid f^n(x) = 0\} \bigcup\limits_{i=1}^{\infty} A_{i} = [0, 1] [0, 1]","['real-analysis', 'calculus']"
43,The right way of applying square root to an equation,The right way of applying square root to an equation,,"I am trying to learn, formally speaking, what is the correct way of taking the square root of the two sides of an equation in order to solve for the variable. For the discussion, suppose we have to solve $$ (x-1)^2 = 16 \tag{1} $$ with the existence domain of $x$ being $x\in \mathbb R.$ Method A: setting the argument of the square to be equal to the $\pm$ of rhs. (I deem this method to be wrong, but I am not sure.) \begin{align} (1): \sqrt{(x-1)^2} =& \sqrt{16} \\ \Leftrightarrow x-1=-4 \quad \vee& \quad x-1 =4\\ \Leftrightarrow x=-3 \quad \vee& \quad x=5 \end{align} Although the solutions are right, I think this method is wrong because it does not ensure the square root of the clearly nonnegative $(x-1)^2,$ i.e. $(x-1),$ is nonnegative. Method B: ensuring the nonnegativity of the square root with absolute values \begin{align} (1): \sqrt{(x-1)^2} =& \sqrt{16} \\ \Leftrightarrow |x-1| =& 4 \tag{2} \end{align} Removing the absolute values: $$ |x-1|=\begin{cases} x-1 &\text{ if } x\ge 1 \\ -x+1 &\text{ if } x<1. \end{cases} $$ With the piecewise definition, $(2)$ becomes: \begin{align} &|x-1| = 4 \\ &\Leftrightarrow x-1=4 \quad \vee \quad -x+1=4 \\ &\Leftrightarrow x=5 \quad \vee \quad x=-3. \end{align} This is what I've learned to be the correct way of taking the square root of an equation, ensuring there are no sign ambiguities. Method C: factorizing using the difference of two squares identity \begin{align} (1): &\Leftrightarrow (x-1)^2 -16 = 0 \\ &\Leftrightarrow (x-1-4)(x-1+4) = 0\\ &\Leftrightarrow x=5 \quad \vee \quad x=-3. \end{align} In this method, we forgo having to take the square root thanks to the used identity, so there are no ambiguities to resolve in the process. Questions: Am I right in treating Method A as formally imprecise? or is it really equivalent to Method B? If Method A is indeed wrong, then how do we justify the more commonly used method for solving equations of type \begin{align} x^2&=4 \\ x &= \pm 2 \end{align} without ever specifying the intermediate step invoking the absolute values? Am I right in considering Method C being correct and devoid of any sign ambiguities? Admittedly, these are naive questions, but honestly, I am really confused as to what the right way of going about such equations is anymore. So any input would be much appreciated.","I am trying to learn, formally speaking, what is the correct way of taking the square root of the two sides of an equation in order to solve for the variable. For the discussion, suppose we have to solve with the existence domain of being Method A: setting the argument of the square to be equal to the of rhs. (I deem this method to be wrong, but I am not sure.) Although the solutions are right, I think this method is wrong because it does not ensure the square root of the clearly nonnegative i.e. is nonnegative. Method B: ensuring the nonnegativity of the square root with absolute values Removing the absolute values: With the piecewise definition, becomes: This is what I've learned to be the correct way of taking the square root of an equation, ensuring there are no sign ambiguities. Method C: factorizing using the difference of two squares identity In this method, we forgo having to take the square root thanks to the used identity, so there are no ambiguities to resolve in the process. Questions: Am I right in treating Method A as formally imprecise? or is it really equivalent to Method B? If Method A is indeed wrong, then how do we justify the more commonly used method for solving equations of type without ever specifying the intermediate step invoking the absolute values? Am I right in considering Method C being correct and devoid of any sign ambiguities? Admittedly, these are naive questions, but honestly, I am really confused as to what the right way of going about such equations is anymore. So any input would be much appreciated.","
(x-1)^2 = 16 \tag{1}
 x x\in \mathbb R. \pm \begin{align}
(1): \sqrt{(x-1)^2} =& \sqrt{16} \\
\Leftrightarrow x-1=-4 \quad \vee& \quad x-1 =4\\
\Leftrightarrow x=-3 \quad \vee& \quad x=5
\end{align} (x-1)^2, (x-1), \begin{align}
(1): \sqrt{(x-1)^2} =& \sqrt{16} \\
\Leftrightarrow |x-1| =& 4 \tag{2}
\end{align} 
|x-1|=\begin{cases}
x-1 &\text{ if } x\ge 1 \\
-x+1 &\text{ if } x<1.
\end{cases}
 (2) \begin{align}
&|x-1| = 4 \\
&\Leftrightarrow x-1=4 \quad \vee \quad -x+1=4 \\
&\Leftrightarrow x=5 \quad \vee \quad x=-3.
\end{align} \begin{align}
(1): &\Leftrightarrow (x-1)^2 -16 = 0 \\
&\Leftrightarrow (x-1-4)(x-1+4) = 0\\
&\Leftrightarrow x=5 \quad \vee \quad x=-3.
\end{align} \begin{align}
x^2&=4 \\
x &= \pm 2
\end{align}","['real-analysis', 'algebra-precalculus', 'definition', 'radicals', 'absolute-value']"
44,$L^1(G)$ is a $C^*$-algebra $\iff$ $G$ is trivial,is a -algebra   is trivial,L^1(G) C^* \iff G,"Let $L^1(G)$ be the group Banach $*$ -algebra of a locally compact Hausdorff group $G$ . Then it is known that $L^1(G)$ is a $C^*$ -algebra if and only if the group $G$ is trivial. The proof of this fact can be found here and it is not elementary. To my surprise, I came across a seemingly much simpler and completely elementary proof in Vahid Shirbisheh's Lectures on $C^*$ -algebras , Proposition 2.2.21., Page 24. However, upon more careful reading, I think that the proof is wrong. It goes like this: Assume that $G$ is nontrivial and pick $s \in G\setminus \{1\}$ . Pick an open relatively compact neighbourhood $U$ of $1$ in $G$ such that $U \cap sU = \emptyset$ . Then by properties of left Haar measure we have $0 < \mu(U) < +\infty$ . If we define $$f : G \to \Bbb{C}, \quad f(x) := \frac1{\sqrt{\Delta(x)}}(\mathbf{1}_{U} - i \mathbf{1}_{sU})(x), \quad x \in G$$ it is easy to show that $f \in L^1(G)$ , where $\Delta : G \to \langle0,+\infty\rangle$ is the modular function. We have \begin{align} \|f * f^*\|_{L^1(G)} &= \int_G |(f * f^*)(x)|\,d\mu(x)\\ &=\int_G \left|\int_G f(y)\Delta(x^{-1}y)\overline{f(x^{-1}y)}\,d\mu(y)\right|d\mu(x)\\ &= \int_G \left|\int_G \Delta(x^{-1}y)\cdot \frac{\mathbf{1}_{U}(y) - i \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\cdot \frac{\mathbf{1}_{U}(x^{-1}y) + i \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\,d\mu(y)\right|d\mu(x)\\ &= \int_G \frac1{\sqrt{\Delta(x)}} \left|\int_G (\mathbf{1}_U(y)\mathbf{1}_{U}(x^{-1}y) + i\mathbf{1}_U(y)\mathbf{1}_{sU}(x^{-1}y) - i\mathbf{1}_U(x^{-1}y)\mathbf{1}_{sU}(y) + \mathbf{1}_{sU}(y)\mathbf{1}_{sU}(x^{-1}y))\,d\mu(y)\right|d\mu(x)\\ &= \int_G \frac1{\sqrt{\Delta(x)}} \left|\mu(U \cap x U) + i \mu(U \cap xsU) - i\mu(xU \cap sU) + \mu(sU \cap xsU)\right|d\mu(x). \end{align} Now comes the dubious step. The author writes (paraphrased): ""In the last integral each term of the integrand is non-zero at least for some values of $x\in G$ and when one of the terms is non-zero the other three terms are zero. This feature justifies the following steps of our argument:"" \begin{align} \|f * f^*\|_{L^1(G)} &< \int_G \frac1{\sqrt{\Delta(x)}} (\mu(U \cap x U) +  \mu(U \cap xsU) +\mu(xU \cap sU) + \mu(sU \cap xsU))\,d\mu(x)\\ &= \int_G \frac1{\sqrt{\Delta(x)}} \left|\int_G (\mathbf{1}_U(y)\mathbf{1}_{U}(x^{-1}y) + \mathbf{1}_U(y)\mathbf{1}_{sU}(x^{-1}y) + \mathbf{1}_U(x^{-1}y)\mathbf{1}_{sU}(y) + \mathbf{1}_{sU}(y)\mathbf{1}_{sU}(x^{-1}y))\,d\mu(y)\right|d\mu(x)\\ &= \int_G \left(\int_G \Delta(x^{-1}y)\cdot \frac{\mathbf{1}_{U}(y) + \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\cdot \frac{\mathbf{1}_{U}(x^{-1}y) + \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\,d\mu(y)\right)d\mu(x)\\ &= \int_G \left(\int_G \Delta(x^{-1}y)\cdot \left|\frac{\mathbf{1}_{U}(y) -i \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\right|\cdot \left|\frac{\mathbf{1}_{U}(x^{-1}y) -i \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\right|\,d\mu(y)\right)d\mu(x)\\ &= \int_G \left(\int_G \Delta(x^{-1}y)|f(y)||f(x^{-1}y)|\,d\mu(y)\right)d\mu(x)\\ &= \int_G \Delta(y)|f(y)|\left(\int_G |f(xy)|\,d\mu(x)\right)\,d\mu(y)\\ &= \int_G \Delta(y)|f(y)|\Delta(y^{-1})\left(\int_G |f(x)|\,d\mu(x)\right)\,d\mu(y)\\ &= \|f\|_{L^1(G)}^2. \end{align} So, the idea seems to be to notice that the supports of the functions $$x \mapsto \mu(U \cap x U), \quad x \mapsto \mu(U \cap xsU), \quad x \mapsto \mu(xU \cap sU), \quad x \mapsto \mu(sU \cap xsU)$$ are pairwise disjoint and nonempty. Hence we somehow get the strict inequality, but I argue that it should in fact be equality. Precisely because the supports are pairwise disjoint we have $$\left|\mu(U \cap x U) + i \mu(U \cap xsU) - i\mu(xU \cap sU) + \mu(sU \cap xsU)\right|=\mu(U \cap x U) +  \mu(U \cap xsU) +\mu(xU \cap sU) + \mu(sU \cap xsU)$$ for all $x \in G$ . It is irrelevant that the supports are nonempty. My questions are: Is the proof indeed wrong? If yes, can it be easily fixed?","Let be the group Banach -algebra of a locally compact Hausdorff group . Then it is known that is a -algebra if and only if the group is trivial. The proof of this fact can be found here and it is not elementary. To my surprise, I came across a seemingly much simpler and completely elementary proof in Vahid Shirbisheh's Lectures on -algebras , Proposition 2.2.21., Page 24. However, upon more careful reading, I think that the proof is wrong. It goes like this: Assume that is nontrivial and pick . Pick an open relatively compact neighbourhood of in such that . Then by properties of left Haar measure we have . If we define it is easy to show that , where is the modular function. We have Now comes the dubious step. The author writes (paraphrased): ""In the last integral each term of the integrand is non-zero at least for some values of and when one of the terms is non-zero the other three terms are zero. This feature justifies the following steps of our argument:"" So, the idea seems to be to notice that the supports of the functions are pairwise disjoint and nonempty. Hence we somehow get the strict inequality, but I argue that it should in fact be equality. Precisely because the supports are pairwise disjoint we have for all . It is irrelevant that the supports are nonempty. My questions are: Is the proof indeed wrong? If yes, can it be easily fixed?","L^1(G) * G L^1(G) C^* G C^* G s \in G\setminus \{1\} U 1 G U \cap sU = \emptyset 0 < \mu(U) < +\infty f : G \to \Bbb{C}, \quad f(x) := \frac1{\sqrt{\Delta(x)}}(\mathbf{1}_{U} - i \mathbf{1}_{sU})(x), \quad x \in G f \in L^1(G) \Delta : G \to \langle0,+\infty\rangle \begin{align}
\|f * f^*\|_{L^1(G)} &= \int_G |(f * f^*)(x)|\,d\mu(x)\\
&=\int_G \left|\int_G f(y)\Delta(x^{-1}y)\overline{f(x^{-1}y)}\,d\mu(y)\right|d\mu(x)\\
&= \int_G \left|\int_G \Delta(x^{-1}y)\cdot \frac{\mathbf{1}_{U}(y) - i \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\cdot \frac{\mathbf{1}_{U}(x^{-1}y) + i \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\,d\mu(y)\right|d\mu(x)\\
&= \int_G \frac1{\sqrt{\Delta(x)}} \left|\int_G (\mathbf{1}_U(y)\mathbf{1}_{U}(x^{-1}y) + i\mathbf{1}_U(y)\mathbf{1}_{sU}(x^{-1}y) - i\mathbf{1}_U(x^{-1}y)\mathbf{1}_{sU}(y) + \mathbf{1}_{sU}(y)\mathbf{1}_{sU}(x^{-1}y))\,d\mu(y)\right|d\mu(x)\\
&= \int_G \frac1{\sqrt{\Delta(x)}} \left|\mu(U \cap x U) + i \mu(U \cap xsU) - i\mu(xU \cap sU) + \mu(sU \cap xsU)\right|d\mu(x).
\end{align} x\in G \begin{align}
\|f * f^*\|_{L^1(G)} &< \int_G \frac1{\sqrt{\Delta(x)}} (\mu(U \cap x U) +  \mu(U \cap xsU) +\mu(xU \cap sU) + \mu(sU \cap xsU))\,d\mu(x)\\
&= \int_G \frac1{\sqrt{\Delta(x)}} \left|\int_G (\mathbf{1}_U(y)\mathbf{1}_{U}(x^{-1}y) + \mathbf{1}_U(y)\mathbf{1}_{sU}(x^{-1}y) + \mathbf{1}_U(x^{-1}y)\mathbf{1}_{sU}(y) + \mathbf{1}_{sU}(y)\mathbf{1}_{sU}(x^{-1}y))\,d\mu(y)\right|d\mu(x)\\
&= \int_G \left(\int_G \Delta(x^{-1}y)\cdot \frac{\mathbf{1}_{U}(y) + \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\cdot \frac{\mathbf{1}_{U}(x^{-1}y) + \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\,d\mu(y)\right)d\mu(x)\\
&= \int_G \left(\int_G \Delta(x^{-1}y)\cdot \left|\frac{\mathbf{1}_{U}(y) -i \mathbf{1}_{sU}(y)}{\sqrt{\Delta(y)}}\right|\cdot \left|\frac{\mathbf{1}_{U}(x^{-1}y) -i \mathbf{1}_{sU}(x^{-1}y)}{\sqrt{\Delta(x^{-1}y)}}\right|\,d\mu(y)\right)d\mu(x)\\
&= \int_G \left(\int_G \Delta(x^{-1}y)|f(y)||f(x^{-1}y)|\,d\mu(y)\right)d\mu(x)\\
&= \int_G \Delta(y)|f(y)|\left(\int_G |f(xy)|\,d\mu(x)\right)\,d\mu(y)\\
&= \int_G \Delta(y)|f(y)|\Delta(y^{-1})\left(\int_G |f(x)|\,d\mu(x)\right)\,d\mu(y)\\
&= \|f\|_{L^1(G)}^2.
\end{align} x \mapsto \mu(U \cap x U), \quad x \mapsto \mu(U \cap xsU), \quad x \mapsto \mu(xU \cap sU), \quad x \mapsto \mu(sU \cap xsU) \left|\mu(U \cap x U) + i \mu(U \cap xsU) - i\mu(xU \cap sU) + \mu(sU \cap xsU)\right|=\mu(U \cap x U) +  \mu(U \cap xsU) +\mu(xU \cap sU) + \mu(sU \cap xsU) x \in G","['real-analysis', 'functional-analysis', 'measure-theory', 'representation-theory', 'c-star-algebras']"
45,"Lower bound in $L^2([0,1])$ for the derivative under the constraint $f(x)\ge\sqrt x$ for $x\ge \varepsilon$",Lower bound in  for the derivative under the constraint  for,"L^2([0,1]) f(x)\ge\sqrt x x\ge \varepsilon","Let $f\in C([0,1]) \cap C^1(0,1)$ and $f(0)=0$ . Furthermore, let $1>\varepsilon>0$ and suppose that $f(x)\geq \sqrt{x}$ for $x\in [\varepsilon, 1]$ . I would like to show that $\int_0^1 (f'(y))^2 dy\geq c\cdot |\ln\varepsilon|$ for some constant $c>0$ . I expect that the minimizer of the functional $I(f)=\int_0^1 (f'(y))^2dy$ for $f\in H^1(0,1)$ with respect to the above constraints is given by \begin{align} g(x)=\begin{cases} \frac{x}{\sqrt{\varepsilon}} \qquad \text{for }x\in[0,\varepsilon]\\ \sqrt{x}\qquad \text{for }x\in[\varepsilon,1], \end{cases} \end{align} which obviously satisfies the required bound. This is most easily seen by drawing a picture and observing that one can decrease the value of the functional if the function lies above $g$ for some point $x\in[0,1]$ . Since I need to generalise this to a similar $2D$ -problem, where this line of reasoning is more difficult, I hope someone can provide a simple analytical proof using only some basic calculus tools.","Let and . Furthermore, let and suppose that for . I would like to show that for some constant . I expect that the minimizer of the functional for with respect to the above constraints is given by which obviously satisfies the required bound. This is most easily seen by drawing a picture and observing that one can decrease the value of the functional if the function lies above for some point . Since I need to generalise this to a similar -problem, where this line of reasoning is more difficult, I hope someone can provide a simple analytical proof using only some basic calculus tools.","f\in C([0,1]) \cap C^1(0,1) f(0)=0 1>\varepsilon>0 f(x)\geq \sqrt{x} x\in [\varepsilon, 1] \int_0^1 (f'(y))^2 dy\geq c\cdot |\ln\varepsilon| c>0 I(f)=\int_0^1 (f'(y))^2dy f\in H^1(0,1) \begin{align}
g(x)=\begin{cases}
\frac{x}{\sqrt{\varepsilon}} \qquad \text{for }x\in[0,\varepsilon]\\
\sqrt{x}\qquad \text{for }x\in[\varepsilon,1],
\end{cases}
\end{align} g x\in[0,1] 2D","['real-analysis', 'calculus', 'analysis']"
46,"Showing a biconditional statement about function lim sups in $\Bbb R^n$, and codifying the intuition into a proof","Showing a biconditional statement about function lim sups in , and codifying the intuition into a proof",\Bbb R^n,"$ \newcommand{\R}{\mathbb{R}} \newcommand{\N}{\mathbb{N}} \newcommand{\BB}{\mathcal{B}} \newcommand{\ve}{\varepsilon} \newcommand{\para}[1]{\left( #1 \right)} \newcommand{\set}[1]{\left\{ #1 \right\} } $ The Statement: Let $f : E \subseteq \R^n \to \R$ . The claim to prove: $$\beta = \limsup_{x \to x_0} f(x) \iff \text{conditions (i) & (ii) below hold}$$ The conditions: (i) $\exists \set{x_n}_{n \in \N} \subseteq \R^n$ a sequence such that $f(x_n) \to \beta$ (ii) $(\forall b > \beta)(\exists \delta > 0)(f(x) < b \text{ for } x \in B^* \cap E)$ Notable Definitions & Notations: I think the more relevant definitions to go over would be these two: We denote the ""punctured ball"" centered at $x_0$ with radius $\delta$ by $B^*$ , and more explicitly may also write $$B^* := B^*(x_0;\delta) := B(x_0;\delta) \setminus \set{x_0}$$ (Some of you may be more familiar with the notation $B_\delta(x_0)$ to denote a ball of radius $\delta$ centered at $x_0$ .) Essentially the punctured ball $B^*$ is the usual ball with the center removed. We define the limit supremum of $f$ as $x \to x_0$ as so: $$\limsup_{x \to x_0} f(x) = \lim_{\delta \to 0} \left( \sup_{x \in B^* \cap E} f(x) \right)$$ Context & Attempts: Ultimately this is a homework problem, so I would prefer to not have full proofs given out; moreso just nudges and such. Now, in the senses of geometry, visuals, and intuition, I think I have some ideas as to what is going on, though not quite the whole picture. In the forward direction: let $\beta$ be $\limsup f(x)$ . Then that means two things. Firstly, we can find a sequence $x_n$ in the domain where that sequence under $f$ , $f(x_n)$ , converges to $\beta$ . I think that helps get $\beta \le \sup f(x)$ on the relevant set. Moreover, whenever we have a number $b$ larger than $\beta$ , then condition (ii) essentially constricts $b$ and $\beta$ in a sense. The closer $b$ and $\beta$ , the smaller $\delta$ is, and eventually I think that will help us get to equality. In the backwards direction: we know there is a sequence $\set{x_n}$ where $f(x_n) \to \beta$ , and whenever $b > \beta$ , $x \in B^*(x_0;\delta) \cap E$ have $f(x) < b$ (though where $\beta$ lies between the two is not known). I think this would give that $\sup f(x) \le b$ for the relevant set, and as $\delta \to 0$ , $b$ and $\beta$ get closer and closer together until we somehow get equality. My issues with these are more like ... trying to codify my intuitions. I have written down some things, but I'm not too certain they're on the right track. For instance, for the forward direction: Attempt at the Forward Direction: Suppose $f : E \subseteq \R^n \to \R$ and $\beta := \displaystyle \limsup_{x \to x_0} f(x)$ . Then by definition, $$ \beta = \lim_{\delta \to 0} \para{ \sup_{x \in B^*(x_0;\delta) \cap E} f(x) } $$ Let $\delta_k := 1/k$ . Then define $\BB_k := B^*(x_0;\delta_k)$ . Since $\delta_k \to 0$ , it holds that $$ \sup_{x \in \BB_k \cap E} f(x) \to \beta $$ Thus, define $x_k$ by $x_k \in \BB_k \cap E \subseteq E$ , and then $\set{x_k}_{k \in \N}$ is one such that $f(x_k) \to \beta$ , satisfying condition (i). Suppose $b > \beta$ . To see condition (ii) holds, suppose otherwise, that $\forall \delta > 0$ , $f(x) \ge b$ for $x \in B^* \cap E$ . This would mean that $$ \sup_{x \in B^*(x_0;\delta) \cap E} f(x) \ge b > \beta $$ for all $\delta > 0$ and in particular the case $\delta \to 0$ . This would give that $ \displaystyle\limsup_{x \to x_0} f(x) = b$ instead from the definition, contradicting that $\beta$ is the lim sup. Thus, a contradiction is reached and condition (ii) holds. Attempt at the Backwards Direction: (which is moreso scratch work that stalled hard) We're going to suppose (i) & (ii) hold. Since $\exists \set{x_k}_{k \in \N} \subseteq E$ such that $f(x_k) \to \beta$ , from definition, $\forall \ve > 0$ , $\exists N \in \N$ such that, $\forall n \ge N$ , $|f(x_n) - \beta| < \ve$ . In particular, we can consider the elements of $\set{x_k}$ which are within a radius $\delta > 0$ of $x_0$ and are different from $x_0$ ; these elements define a subsequence $\set{x_{n_k}}_{k \in \N} \subseteq B^* \cap E$ . From the fact that limits are unique, and if a sequence $z_k \to L$ , then all subsequences of $\set{z_k}_{k \in \N}$ converge to $L$ as well. Thus, $\set{x_{n_k}}_{k\in\N}$ is a sequence in $B^* \cap E$ . If we take the limit as $\ve,\delta \to 0$ , it then follows from these and the definition of supremum that $$ \lim_{\delta \to 0} \para{ \sup_{B^* \cap E} f(x) } \le \beta $$ Consider condition (ii) and that for a $b > \beta$ , $\exists \delta > 0$ such that $f(x) < b$ for $x \in B^* \cap E$ . (That's where I get lost though...) Questions & Concerns: Is my intuition for what's going on correct? If not, what's the correct intuition and the general approach I should use? Is there anything remotely salvageable from my approaches? If there are errors with my approach, what are they? How might I rectify them? Thanks for any insight you can give me! Edit: (thoughts for converse as of 1/27/2021) I had a thought for the approach that seems to neatly tie conditions (i) and (ii) together. Let $\set{b_k}_{k \in \N}$ be a monotone decreasing sequence with limit $\beta$ , i.e. $b_k > \beta$ for every $k$ , and $b_k \searrow \beta$ . Define $\ve_k := b_k - \beta$ . Consider the preimage $f^{-1}(\beta,\ve_k)$ . This may consist of a set of disjoint sets, so consider only the one $\mathcal{A_k}$ the point $x_0$ lies in. For each preimage determined by $\ve_k$ , we can get a $\mathcal{A_k}$ , containing a ball $B(x_0;\delta_k)$ . Thus, each $\ve_k$ determines a preimage, which determines a ball (a subset of the preimage), which determines a $\delta_k$ . Define a set of points $x_k$ by being in that preimage and different from $x_0$ , i.e. take $x_k \in B^*(x_0;\delta_k)$ . Obviously, as $b_k \to \beta$ , then $\delta_k \to 0$ . Moreover, $x_k \to x_0$ , and $f(x_k) \to \beta$ . For each $k \in \N$ , we have that $$\sup_{x \in B^*(x_0;\delta_k) \cap E} f(x) = b_k$$ and, as $\delta_k \to 0$ , we have $b_k \to \beta$ , giving the lim sup as desired. (I'm sure there are details to iron out in this, but I feel there's a grain of truth at the solution in there somewhere...)","The Statement: Let . The claim to prove: The conditions: (i) a sequence such that (ii) Notable Definitions & Notations: I think the more relevant definitions to go over would be these two: We denote the ""punctured ball"" centered at with radius by , and more explicitly may also write (Some of you may be more familiar with the notation to denote a ball of radius centered at .) Essentially the punctured ball is the usual ball with the center removed. We define the limit supremum of as as so: Context & Attempts: Ultimately this is a homework problem, so I would prefer to not have full proofs given out; moreso just nudges and such. Now, in the senses of geometry, visuals, and intuition, I think I have some ideas as to what is going on, though not quite the whole picture. In the forward direction: let be . Then that means two things. Firstly, we can find a sequence in the domain where that sequence under , , converges to . I think that helps get on the relevant set. Moreover, whenever we have a number larger than , then condition (ii) essentially constricts and in a sense. The closer and , the smaller is, and eventually I think that will help us get to equality. In the backwards direction: we know there is a sequence where , and whenever , have (though where lies between the two is not known). I think this would give that for the relevant set, and as , and get closer and closer together until we somehow get equality. My issues with these are more like ... trying to codify my intuitions. I have written down some things, but I'm not too certain they're on the right track. For instance, for the forward direction: Attempt at the Forward Direction: Suppose and . Then by definition, Let . Then define . Since , it holds that Thus, define by , and then is one such that , satisfying condition (i). Suppose . To see condition (ii) holds, suppose otherwise, that , for . This would mean that for all and in particular the case . This would give that instead from the definition, contradicting that is the lim sup. Thus, a contradiction is reached and condition (ii) holds. Attempt at the Backwards Direction: (which is moreso scratch work that stalled hard) We're going to suppose (i) & (ii) hold. Since such that , from definition, , such that, , . In particular, we can consider the elements of which are within a radius of and are different from ; these elements define a subsequence . From the fact that limits are unique, and if a sequence , then all subsequences of converge to as well. Thus, is a sequence in . If we take the limit as , it then follows from these and the definition of supremum that Consider condition (ii) and that for a , such that for . (That's where I get lost though...) Questions & Concerns: Is my intuition for what's going on correct? If not, what's the correct intuition and the general approach I should use? Is there anything remotely salvageable from my approaches? If there are errors with my approach, what are they? How might I rectify them? Thanks for any insight you can give me! Edit: (thoughts for converse as of 1/27/2021) I had a thought for the approach that seems to neatly tie conditions (i) and (ii) together. Let be a monotone decreasing sequence with limit , i.e. for every , and . Define . Consider the preimage . This may consist of a set of disjoint sets, so consider only the one the point lies in. For each preimage determined by , we can get a , containing a ball . Thus, each determines a preimage, which determines a ball (a subset of the preimage), which determines a . Define a set of points by being in that preimage and different from , i.e. take . Obviously, as , then . Moreover, , and . For each , we have that and, as , we have , giving the lim sup as desired. (I'm sure there are details to iron out in this, but I feel there's a grain of truth at the solution in there somewhere...)","
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\ve}{\varepsilon}
\newcommand{\para}[1]{\left( #1 \right)}
\newcommand{\set}[1]{\left\{ #1 \right\} }
 f : E \subseteq \R^n \to \R \beta = \limsup_{x \to x_0} f(x) \iff \text{conditions (i) & (ii) below hold} \exists \set{x_n}_{n \in \N} \subseteq \R^n f(x_n) \to \beta (\forall b > \beta)(\exists \delta > 0)(f(x) < b \text{ for } x \in B^* \cap E) x_0 \delta B^* B^* := B^*(x_0;\delta) := B(x_0;\delta) \setminus \set{x_0} B_\delta(x_0) \delta x_0 B^* f x \to x_0 \limsup_{x \to x_0} f(x) = \lim_{\delta \to 0} \left( \sup_{x \in B^* \cap E} f(x) \right) \beta \limsup f(x) x_n f f(x_n) \beta \beta \le \sup f(x) b \beta b \beta b \beta \delta \set{x_n} f(x_n) \to \beta b > \beta x \in B^*(x_0;\delta) \cap E f(x) < b \beta \sup f(x) \le b \delta \to 0 b \beta f : E \subseteq \R^n \to \R \beta := \displaystyle \limsup_{x \to x_0} f(x) 
\beta = \lim_{\delta \to 0} \para{ \sup_{x \in B^*(x_0;\delta) \cap E} f(x) }
 \delta_k := 1/k \BB_k := B^*(x_0;\delta_k) \delta_k \to 0 
\sup_{x \in \BB_k \cap E} f(x) \to \beta
 x_k x_k \in \BB_k \cap E \subseteq E \set{x_k}_{k \in \N} f(x_k) \to \beta b > \beta \forall \delta > 0 f(x) \ge b x \in B^* \cap E 
\sup_{x \in B^*(x_0;\delta) \cap E} f(x) \ge b > \beta
 \delta > 0 \delta \to 0  \displaystyle\limsup_{x \to x_0} f(x) = b \beta \exists \set{x_k}_{k \in \N} \subseteq E f(x_k) \to \beta \forall \ve > 0 \exists N \in \N \forall n \ge N |f(x_n) - \beta| < \ve \set{x_k} \delta > 0 x_0 x_0 \set{x_{n_k}}_{k \in \N} \subseteq B^* \cap E z_k \to L \set{z_k}_{k \in \N} L \set{x_{n_k}}_{k\in\N} B^* \cap E \ve,\delta \to 0 
\lim_{\delta \to 0} \para{ \sup_{B^* \cap E} f(x) } \le \beta
 b > \beta \exists \delta > 0 f(x) < b x \in B^* \cap E \set{b_k}_{k \in \N} \beta b_k > \beta k b_k \searrow \beta \ve_k := b_k - \beta f^{-1}(\beta,\ve_k) \mathcal{A_k} x_0 \ve_k \mathcal{A_k} B(x_0;\delta_k) \ve_k \delta_k x_k x_0 x_k \in B^*(x_0;\delta_k) b_k \to \beta \delta_k \to 0 x_k \to x_0 f(x_k) \to \beta k \in \N \sup_{x \in B^*(x_0;\delta_k) \cap E} f(x) = b_k \delta_k \to 0 b_k \to \beta","['real-analysis', 'sequences-and-series', 'proof-writing', 'metric-spaces', 'limsup-and-liminf']"
47,Known bounds on $\int_{-\infty}^{\infty}|f(x)-f(x-a)|dx$?,Known bounds on ?,\int_{-\infty}^{\infty}|f(x)-f(x-a)|dx,"Suppose that $f(x)$ is continuous on $\mathbb{R}$ such that $|f(x)|\leq M$ . Are there any know result for estimating the quantity $$ I(a)=\int_{-\infty}^\infty|f(x)-f(x-a)|dx. $$ I have the following results: (i) If $f$ is strictly increasing (equivalently $f(-x)$ strictly decreasing), $$ \begin{align} I(a)&=\int_{f(\mathbb{R})} \left[f^{-1}(y)+a-f^{-1}(y)\right]dy \\ &\leq\int_{[-M,M]} a\,dy \\ &= 2Ma \end{align} $$ (ii) If $f\geq0$ , $f$ strictly increasing on $(-\infty,0)$ and strictly decreasing on $(0,\infty)$ . By the intermediate value theorem, there exist $c\in(0,a)$ such that $f(c)=f(c-a)$ , then $$ \begin{align} I(a)&=\int_{-\infty}^c \left[f(x)-f(x-a)\right]dx+\int_c^\infty\left[f(x-a)-f(x)\right]dx \\ &=\int_{-\infty}^cf(x)dx-\int_{-\infty}^{c-a}f(x)dx+\int_{c-a}^\infty f(x)dx-\int_c^\infty f(x)dx \\ &=2\int_{c-a}^c f(x)dx \\ &\leq 2Ma \end{align}  $$ Is there a generalization of such bound? My guess would be that for any bounded continuous function that has finitely many extrema, $$ I(a)\leq 2aMn, $$ where $n$ is the number of extrema. Any insight would be greatly appreciated.","Suppose that is continuous on such that . Are there any know result for estimating the quantity I have the following results: (i) If is strictly increasing (equivalently strictly decreasing), (ii) If , strictly increasing on and strictly decreasing on . By the intermediate value theorem, there exist such that , then Is there a generalization of such bound? My guess would be that for any bounded continuous function that has finitely many extrema, where is the number of extrema. Any insight would be greatly appreciated.","f(x) \mathbb{R} |f(x)|\leq M 
I(a)=\int_{-\infty}^\infty|f(x)-f(x-a)|dx.
 f f(-x) 
\begin{align}
I(a)&=\int_{f(\mathbb{R})} \left[f^{-1}(y)+a-f^{-1}(y)\right]dy \\
&\leq\int_{[-M,M]} a\,dy \\
&= 2Ma
\end{align}
 f\geq0 f (-\infty,0) (0,\infty) c\in(0,a) f(c)=f(c-a) 
\begin{align}
I(a)&=\int_{-\infty}^c \left[f(x)-f(x-a)\right]dx+\int_c^\infty\left[f(x-a)-f(x)\right]dx \\
&=\int_{-\infty}^cf(x)dx-\int_{-\infty}^{c-a}f(x)dx+\int_{c-a}^\infty f(x)dx-\int_c^\infty f(x)dx \\
&=2\int_{c-a}^c f(x)dx \\
&\leq 2Ma
\end{align} 
 
I(a)\leq 2aMn,
 n","['real-analysis', 'integration', 'analysis']"
48,Evaluating $\lim_{n\to\infty}( f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0) )$,Evaluating,\lim_{n\to\infty}( f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0) ),"I'm doing an exercise in Real Analysis and I would like to have my proof check since I'm not 100% sure of it. The problem says to consider a function $f:\mathbb{R}\rightarrow\mathbb{R}$ differentiable at $x_0$ and with $f(x_0)>0$ (which I guess it's useless at this point but the exercise had other points in which it was necessary so I put it anyway). Then, we are asked to evaluate the following $$\lim_{n\to\infty} \left(f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0)\right).$$ Here is how I would do it: $f$ is differentiable and hence continuous at $x_0$ ; We apply Taylor's Theorem at $x_0$ , so that there is a neighborhood of $x_0$ , $I=(x_0-\delta,x_0+\delta)$ , and a function $h$ such that, for all $x\in I$ , $$f(x)=f(x_0)+f'(x_0)(x-x_0)+h(x)(x-x_0)$$ and $h(x)\rightarrow 0$ as $x\rightarrow x_0$ . Now, if $n>\frac 1{\delta}$ then each of the $n$ points $x_0+\frac 1{n^2},\dots,x_0+\frac 1n$ are in $I$ so we can apply the previous point to get $$f\left(x_0+\frac k{n^2}\right)=f(x_0)+f'(x_0)\frac k{n^2}+h\left(x_0+\frac k{n^2}\right)\frac k{n^2}.$$ Summing from $k=1$ to $k=n$ we get $$f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)=nf(x_0)+\frac {f'(x_0)}{n^2}(1+\dots+n)+\frac 1{n^2}\left(h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right)$$ Using the fact that $1+\dots+n=\frac {n(n+1)}2$ our original expression simplifies to $$f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0)=\frac {f'(x_0)}{n^2}\frac {n(n+1)}2+\frac 1{n^2}\left(h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right).$$ Fix $\epsilon>0$ , since $\frac {n(n+1)}{2n^2}\rightarrow \frac 12$ there is an $\bar{n}$ such that $n\geq \bar{n}$ implies that $\left|\frac {n(n+1)}{2n^2}-\frac 12\right|<\epsilon$ . On the other hand, there is a $\delta'$ such that $|h(x)|<\epsilon$ on $(x_0-\delta',x_0+\delta')$ . Since $x_0+\frac k{n^2}\rightarrow x_0$ and $\frac k{n^2}\leq \frac 1n$ we have that for $n>\frac 1{\delta'}$ $$\left|h\left(x_0+\frac k{n^2}\right)\right|<\epsilon.$$ By picking $n\geq\max\left\{\frac 1{\delta},\bar{n},\frac 1{\delta'}\right\}$ we obtain that $$\left|\frac {f'(x_0)}{n^2}\frac {n(n+1)}2+\frac 1{n^2}\left(h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right)-\frac {f'(x_0)}2\right|\leq$$ $$f'(x_0)\left|\frac {n(n+1)}{2n^2}-\frac 12\right|+\frac 1{n^2}\left|h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right|<$$ $$f'(x_0)\epsilon+\frac 1{n^2}\left|h\left(x_0+\frac 1{n^2}\right)\right|+\dots+\frac n{n^2}\left|h\left(x_0+\frac 1n\right)\right|<f'(x_0)\epsilon+\epsilon(1+\dots+n)=$$ $$=f'(x_0)\epsilon+\frac {\epsilon}{n^2}\frac {n(n+1)}2<f'(x_0)\epsilon+\epsilon\left(\frac 12+\epsilon\right).$$ Since $\epsilon$ is arbitrary, we can conclude that $$\lim_{n\to\infty} \left(f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0)\right) = \frac {f'(x_0)}2.$$ Is my proof correct? Is there any smarter/shorter way to do that? Thanks is advance for any help!","I'm doing an exercise in Real Analysis and I would like to have my proof check since I'm not 100% sure of it. The problem says to consider a function differentiable at and with (which I guess it's useless at this point but the exercise had other points in which it was necessary so I put it anyway). Then, we are asked to evaluate the following Here is how I would do it: is differentiable and hence continuous at ; We apply Taylor's Theorem at , so that there is a neighborhood of , , and a function such that, for all , and as . Now, if then each of the points are in so we can apply the previous point to get Summing from to we get Using the fact that our original expression simplifies to Fix , since there is an such that implies that . On the other hand, there is a such that on . Since and we have that for By picking we obtain that Since is arbitrary, we can conclude that Is my proof correct? Is there any smarter/shorter way to do that? Thanks is advance for any help!","f:\mathbb{R}\rightarrow\mathbb{R} x_0 f(x_0)>0 \lim_{n\to\infty} \left(f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0)\right). f x_0 x_0 x_0 I=(x_0-\delta,x_0+\delta) h x\in I f(x)=f(x_0)+f'(x_0)(x-x_0)+h(x)(x-x_0) h(x)\rightarrow 0 x\rightarrow x_0 n>\frac 1{\delta} n x_0+\frac 1{n^2},\dots,x_0+\frac 1n I f\left(x_0+\frac k{n^2}\right)=f(x_0)+f'(x_0)\frac k{n^2}+h\left(x_0+\frac k{n^2}\right)\frac k{n^2}. k=1 k=n f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)=nf(x_0)+\frac {f'(x_0)}{n^2}(1+\dots+n)+\frac 1{n^2}\left(h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right) 1+\dots+n=\frac {n(n+1)}2 f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0)=\frac {f'(x_0)}{n^2}\frac {n(n+1)}2+\frac 1{n^2}\left(h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right). \epsilon>0 \frac {n(n+1)}{2n^2}\rightarrow \frac 12 \bar{n} n\geq \bar{n} \left|\frac {n(n+1)}{2n^2}-\frac 12\right|<\epsilon \delta' |h(x)|<\epsilon (x_0-\delta',x_0+\delta') x_0+\frac k{n^2}\rightarrow x_0 \frac k{n^2}\leq \frac 1n n>\frac 1{\delta'} \left|h\left(x_0+\frac k{n^2}\right)\right|<\epsilon. n\geq\max\left\{\frac 1{\delta},\bar{n},\frac 1{\delta'}\right\} \left|\frac {f'(x_0)}{n^2}\frac {n(n+1)}2+\frac 1{n^2}\left(h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right)-\frac {f'(x_0)}2\right|\leq f'(x_0)\left|\frac {n(n+1)}{2n^2}-\frac 12\right|+\frac 1{n^2}\left|h\left(x_0+\frac 1{n^2}\right)+\dots+nh\left(x_0+\frac 1n\right)\right|< f'(x_0)\epsilon+\frac 1{n^2}\left|h\left(x_0+\frac 1{n^2}\right)\right|+\dots+\frac n{n^2}\left|h\left(x_0+\frac 1n\right)\right|<f'(x_0)\epsilon+\epsilon(1+\dots+n)= =f'(x_0)\epsilon+\frac {\epsilon}{n^2}\frac {n(n+1)}2<f'(x_0)\epsilon+\epsilon\left(\frac 12+\epsilon\right). \epsilon \lim_{n\to\infty} \left(f\left(x_0+\frac 1{n^2}\right)+\dots+ f\left(x_0+\frac n{n^2}\right)-nf(x_0)\right) = \frac {f'(x_0)}2.","['real-analysis', 'calculus', 'limits', 'derivatives', 'taylor-expansion']"
49,Prove that $a^{\lambda b} + b^{\lambda a} + a^{\lambda b^2} + b^{\lambda a^2} \le 2$ for positive reals $a+b=1$,Prove that  for positive reals,a^{\lambda b} + b^{\lambda a} + a^{\lambda b^2} + b^{\lambda a^2} \le 2 a+b=1,"Problem 1 : Let $\lambda = \frac{\ln (7 + 3\sqrt{5})}{\ln 2} - 1 \approx 2.77697$ . Let $a, b$ be positive real numbers with $a + b = 1$ . Prove (or disprove) that $$a^{\lambda b} + b^{\lambda a} + a^{\lambda b^2} + b^{\lambda a^2} \le 2$$ with equality if and only if $a = b = \frac{1}{2}$ Problem 2 (a weaker version of Problem 1): Let $\lambda_1 = \frac{25}{9}$ . Let $a, b$ be positive real numbers with $a + b = 1$ . Prove (or disprove) that $$a^{\lambda_1 b} + b^{\lambda_1 a} + a^{\lambda_1 b^2} + b^{\lambda_1 a^2} \le 2.$$ Background Information : In Proposition 5.2 in [1], Vasile Cirtoaje gives the following result: Problem 3 : If $a, b$ are nonnegative real numbers satisfying $a + b = 1$ , then $a^{2b} + b^{2a} \le 1$ . In Conjecture 5.1 in [1], Vasile Cirtoaje proposes the following conjecture: Let $a, b$ be nonnegative real numbers satisfying $a + b = 1$ . If $k \ge 1$ , then $a^{(2b)^k} + b^{(2a)^k} \le 1$ . The case $k=2$ has been proved: Problem 4 : Let $a, b$ be positive real numbers with $a + b = 1$ . Prove that $a^{4b^2} + b^{4a^2} \le 1$ . See If $a+b=1$ so $a^{4b^2}+b^{4a^2}\leq1$ I combine Proposition 5.2 and Problem 4 to come up with the problems. I may use appropriate bounds to prove Problems 3 and 4 (something like Inequality $a^{2b}+b^{2a}\leq \cos(ab)^{(a-b)^2}$ ). Now the problems are more difficult. Reference [1] Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"", The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938","Problem 1 : Let . Let be positive real numbers with . Prove (or disprove) that with equality if and only if Problem 2 (a weaker version of Problem 1): Let . Let be positive real numbers with . Prove (or disprove) that Background Information : In Proposition 5.2 in [1], Vasile Cirtoaje gives the following result: Problem 3 : If are nonnegative real numbers satisfying , then . In Conjecture 5.1 in [1], Vasile Cirtoaje proposes the following conjecture: Let be nonnegative real numbers satisfying . If , then . The case has been proved: Problem 4 : Let be positive real numbers with . Prove that . See If $a+b=1$ so $a^{4b^2}+b^{4a^2}\leq1$ I combine Proposition 5.2 and Problem 4 to come up with the problems. I may use appropriate bounds to prove Problems 3 and 4 (something like Inequality $a^{2b}+b^{2a}\leq \cos(ab)^{(a-b)^2}$ ). Now the problems are more difficult. Reference [1] Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"", The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938","\lambda = \frac{\ln (7 + 3\sqrt{5})}{\ln 2} - 1 \approx 2.77697 a, b a + b = 1 a^{\lambda b} + b^{\lambda a} + a^{\lambda b^2} + b^{\lambda a^2} \le 2 a = b = \frac{1}{2} \lambda_1 = \frac{25}{9} a, b a + b = 1 a^{\lambda_1 b} + b^{\lambda_1 a} + a^{\lambda_1 b^2} + b^{\lambda_1 a^2} \le 2. a, b a + b = 1 a^{2b} + b^{2a} \le 1 a, b a + b = 1 k \ge 1 a^{(2b)^k} + b^{(2a)^k} \le 1 k=2 a, b a + b = 1 a^{4b^2} + b^{4a^2} \le 1","['real-analysis', 'inequality']"
50,Proving Tonelli's Theorem for $n$ Factors,Proving Tonelli's Theorem for  Factors,n,"Am trying to prove the following extension of Tonelli's theorem: Proposition. Let $(\Omega_j,\mathcal{A}_j,\mu_j)$ $j=1,\dots,n$ be $\sigma$ -finite measure spaces. Let $f\to[0,\infty]$ be an $\mathcal{A}_1\otimes \dots\otimes\mathcal{A}_n$ measurable function on $\Omega_1\times\dots\times\Omega_n$ . Then for every permutation $j_1,\dots,j_n$ of $1,\dots,n$ we have $$\int f(\omega_1,\dots,\omega_n) \,d (\mu_1 \otimes \dots \otimes \mu_n)=\int \dots \int f(\omega_1,\dots,\omega_n)\,d\mu_{j_1}\dots d\mu_{j_n}$$ where each integral on th RHS is measurable with respect to the product of the $\mathcal{A}_j$ corresponding to coordinates in which integration has not yet occured. My book says it's a simple induction but somehow my proof seems complicated. I believe it is sufficient to consider the case of the identity permutation. This is because we have the equality $$\int f(\omega_1,\dots,\omega_n) \,d (\mu_1 \otimes \dots \otimes \mu_n)=\int f(\omega_{1},\dots,\omega_{n}) \,d (\mu_{j_1} \otimes \dots \otimes \mu_{j_n})$$ see here . In other words, it does not matter whether we regard $f$ as a function on $\Omega_1\times\dots\times\Omega_n$ or on $\Omega_{j_1}\times\dots\times\Omega_{j_n}$ . Is this correct? The comments seem to indicate that are multiple possible approaches here. Any proof outline is greatly appreciated.","Am trying to prove the following extension of Tonelli's theorem: Proposition. Let be -finite measure spaces. Let be an measurable function on . Then for every permutation of we have where each integral on th RHS is measurable with respect to the product of the corresponding to coordinates in which integration has not yet occured. My book says it's a simple induction but somehow my proof seems complicated. I believe it is sufficient to consider the case of the identity permutation. This is because we have the equality see here . In other words, it does not matter whether we regard as a function on or on . Is this correct? The comments seem to indicate that are multiple possible approaches here. Any proof outline is greatly appreciated.","(\Omega_j,\mathcal{A}_j,\mu_j) j=1,\dots,n \sigma f\to[0,\infty] \mathcal{A}_1\otimes \dots\otimes\mathcal{A}_n \Omega_1\times\dots\times\Omega_n j_1,\dots,j_n 1,\dots,n \int f(\omega_1,\dots,\omega_n) \,d (\mu_1 \otimes \dots \otimes \mu_n)=\int \dots \int f(\omega_1,\dots,\omega_n)\,d\mu_{j_1}\dots d\mu_{j_n} \mathcal{A}_j \int f(\omega_1,\dots,\omega_n) \,d (\mu_1 \otimes \dots \otimes \mu_n)=\int f(\omega_{1},\dots,\omega_{n}) \,d (\mu_{j_1} \otimes \dots \otimes \mu_{j_n}) f \Omega_1\times\dots\times\Omega_n \Omega_{j_1}\times\dots\times\Omega_{j_n}","['real-analysis', 'measure-theory', 'lebesgue-integral', 'measurable-functions', 'fubini-tonelli-theorems']"
51,"A continuous function $f:\left[-\frac{\pi}{4},\frac{\pi}{4}\right]\to[-1,1]$ and differentiable on $\left(-\frac{\pi}{4},\frac{\pi}{4}\right)$.",A continuous function  and differentiable on .,"f:\left[-\frac{\pi}{4},\frac{\pi}{4}\right]\to[-1,1] \left(-\frac{\pi}{4},\frac{\pi}{4}\right)","$\blacksquare~$ Problem: Suppose a continuous function $f:\left[-\frac{\pi}{4},\frac{\pi}{4}\right]\to[-1,1]$ and differentiable on $\left(-\frac{\pi}{4},\frac{\pi}{4}\right)$ . Then, there exists a point $x_0\in \left(-\frac{\pi}{4},\frac{\pi}{4}\right)$ such that $$|f'(x_0)|\leqslant 1+f(x_0)^2$$ $\blacksquare~$ My Solution: Let's take $g(x) = \tan^{-1} f(x) $ . Then $g : \left[ - \frac{\pi}{4}, \frac{\pi}{4}\right] \to \left[- \frac{\pi}{4}, \frac{\pi}{4} \right] $ . Now, as $f$ is cont in $\left[- \frac{\pi}{4}, \frac{\pi}{4} \right]$ and differentiable in $\left(- \frac{\pi}{4}, \frac{\pi}{4} \right)$ , $g$ is also the same. By LMVT, we have that $$\frac{g\left(\frac{\pi}{4}\right) - g\left(-\frac{\pi}{4} \right) }{\frac{\pi}{2}} = g'(x_0) \quad \text{for some } x_0 \in \left(- \frac{\pi}{4}, \frac{\pi}{4} \right)$$ $$\implies  \frac{ \frac{\pi}{4} - \left(- \frac{\pi}{4}\right) }{ \frac{\pi}{2} } \geqslant \frac{g\left(\frac{\pi}{4}\right) - g\left(-\frac{\pi}{4} \right) }{\frac{\pi}{2}} = \left(\tan^{-1}f(x_0) \right)' = \frac{f'(x_0)}{1 + f(x_0)^2} $$ $$ \implies 1 + f(x_0)^2 \geqslant f'(x_0) $$ Again, after the LMVT part, we have that $$ \implies  \frac{ - \frac{\pi}{4} - \left( \frac{\pi}{4}\right) }{ \frac{\pi}{2} } \leqslant \frac{g\left(\frac{\pi}{4}\right) - g\left(-\frac{\pi}{4} \right) }{\frac{\pi}{2}} = \left(\tan^{-1}f(x_0) \right)' = \frac{f'(x_0)}{1 + f(x_0)^2} $$ $$ \implies - \left( 1 + f(x_0)^2 \right) \leqslant f'(x_0) $$ Hence, combining these two we have that $$ \lvert f'(x_0) \rvert \leqslant 1 + f(x_0)^2 \quad \text{for some } x_0 \in \left( - \frac{\pi}{4}, \frac{\pi}{4} \right) $$ Is this fine? Is there any glitch? Another way of a solution will be great! Regards, Ralph","Problem: Suppose a continuous function and differentiable on . Then, there exists a point such that My Solution: Let's take . Then . Now, as is cont in and differentiable in , is also the same. By LMVT, we have that Again, after the LMVT part, we have that Hence, combining these two we have that Is this fine? Is there any glitch? Another way of a solution will be great! Regards, Ralph","\blacksquare~ f:\left[-\frac{\pi}{4},\frac{\pi}{4}\right]\to[-1,1] \left(-\frac{\pi}{4},\frac{\pi}{4}\right) x_0\in \left(-\frac{\pi}{4},\frac{\pi}{4}\right) |f'(x_0)|\leqslant 1+f(x_0)^2 \blacksquare~ g(x) = \tan^{-1} f(x)  g : \left[ - \frac{\pi}{4}, \frac{\pi}{4}\right] \to \left[- \frac{\pi}{4}, \frac{\pi}{4} \right]  f \left[- \frac{\pi}{4}, \frac{\pi}{4} \right] \left(- \frac{\pi}{4}, \frac{\pi}{4} \right) g \frac{g\left(\frac{\pi}{4}\right) - g\left(-\frac{\pi}{4} \right) }{\frac{\pi}{2}} = g'(x_0) \quad \text{for some } x_0 \in \left(- \frac{\pi}{4}, \frac{\pi}{4} \right) \implies  \frac{ \frac{\pi}{4} - \left(- \frac{\pi}{4}\right) }{ \frac{\pi}{2} } \geqslant \frac{g\left(\frac{\pi}{4}\right) - g\left(-\frac{\pi}{4} \right) }{\frac{\pi}{2}} = \left(\tan^{-1}f(x_0) \right)' = \frac{f'(x_0)}{1 + f(x_0)^2}   \implies 1 + f(x_0)^2 \geqslant f'(x_0)   \implies  \frac{ - \frac{\pi}{4} - \left( \frac{\pi}{4}\right) }{ \frac{\pi}{2} } \leqslant \frac{g\left(\frac{\pi}{4}\right) - g\left(-\frac{\pi}{4} \right) }{\frac{\pi}{2}} = \left(\tan^{-1}f(x_0) \right)' = \frac{f'(x_0)}{1 + f(x_0)^2}   \implies - \left( 1 + f(x_0)^2 \right) \leqslant f'(x_0)   \lvert f'(x_0) \rvert \leqslant 1 + f(x_0)^2 \quad \text{for some } x_0 \in \left( - \frac{\pi}{4}, \frac{\pi}{4} \right) ","['real-analysis', 'calculus', 'solution-verification']"
52,Flint Hills series $\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n}$,Flint Hills series,\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n},"The well known Flint Hills series , $$\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n}$$ Since I'm aware   of the fact that the series convergence issue is still unknown or falls under  unsolved problem however, making  check on WA ,  the infinite sum has been approximated $\sim 4.80$ . Similarly, I came here where the proposer, Tobi Ope claims that $$\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n} =3\zeta(5)-\frac{2}{\pi^2}\zeta(2)\zeta(3)-2\sum_{k=1}^{\infty}\frac{\cot k}{k^4}\tag{1}$$ which doesn't meet the approximation done by Wolfram alpha. Whether the series converges or not which is still a big unsolved issue so I believe the equality $(1)$ done is incorrect? Similarly, what is wrong with Wolfram alpha? Is it possible to approximate or find the possible closed form for any such infinite series whose convergence is unknown? Thank you","The well known Flint Hills series , Since I'm aware   of the fact that the series convergence issue is still unknown or falls under  unsolved problem however, making  check on WA ,  the infinite sum has been approximated . Similarly, I came here where the proposer, Tobi Ope claims that which doesn't meet the approximation done by Wolfram alpha. Whether the series converges or not which is still a big unsolved issue so I believe the equality done is incorrect? Similarly, what is wrong with Wolfram alpha? Is it possible to approximate or find the possible closed form for any such infinite series whose convergence is unknown? Thank you",\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n} \sim 4.80 \sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n} =3\zeta(5)-\frac{2}{\pi^2}\zeta(2)\zeta(3)-2\sum_{k=1}^{\infty}\frac{\cot k}{k^4}\tag{1} (1),"['real-analysis', 'sequences-and-series', 'summation']"
53,Changing variables in integration over spheres,Changing variables in integration over spheres,,"Suppose we would like to change variables in the integral $$I:=\int_{\mathbb{S}^{n-1}}f(\omega_1,\omega_2,...,\omega_{n})d\sigma_{n-1},$$ where $\mathbb{S}^{n-1}$ is the standard unit sphere in $\mathbb{R}^{n}$ , $n\geq 2$ , $d\sigma_{n-1}$ is the surface measure induced by the Lebesgue measure on $\mathbb{R}^{n}$ , and $\left(\omega_{1}(\theta_{1},...,\theta_{n-1}), \omega_{2}(\theta_{1},...,\theta_{n-1}),...,\omega_{n}(\theta_{1},...,\theta_{n-1})\right)$ is a unit vector that gives the parametric spherical representation of every point $(x_1,...,x_n)$ that lies on the sphere. So, for example, every $(x,y)\in\mathbb{S}^{1}$ has the representation $(x,y)=(\omega_1,\omega_2)=(\cos{\theta_{1}},\sin{\theta_1})$ , $\theta_{1}\in [0,2\pi]$ , and every $(x,y,z)\in\mathbb{S}^{2}$ has the representation $(x,y,z)=(\omega_1,\omega_2,\omega_3)=(\sin{\theta_{1}}\cos{\theta_2},\sin{\theta_1}\sin{\theta_{2}},\cos{\theta_{1}})$ , $\theta_{1}\in[0,\pi], \theta_{2}\in[0,2\pi]$ . Question: How to change variables in the integral $I$ ? My question is about the Jacobian. Precisely, if we change variables $\omega_{i}=\phi_{i}(\omega_{1},\omega_{2},...,\omega_{n})$ where $\phi_{i}$ are continuously differentiable and invertible, is it correct that $$I=\int_{\cup_{\theta_1,\theta_2,...,\theta_{n-1}}{(\phi_{1},...,\phi_{n})}} f(\phi_{1},...,\phi_{n})\det\left(\frac{\partial(\omega_1,...,\omega_n)}{\partial(\phi_1,...,\phi_n)}\right)\,d\sigma_{n-1} ?$$","Suppose we would like to change variables in the integral where is the standard unit sphere in , , is the surface measure induced by the Lebesgue measure on , and is a unit vector that gives the parametric spherical representation of every point that lies on the sphere. So, for example, every has the representation , , and every has the representation , . Question: How to change variables in the integral ? My question is about the Jacobian. Precisely, if we change variables where are continuously differentiable and invertible, is it correct that","I:=\int_{\mathbb{S}^{n-1}}f(\omega_1,\omega_2,...,\omega_{n})d\sigma_{n-1}, \mathbb{S}^{n-1} \mathbb{R}^{n} n\geq 2 d\sigma_{n-1} \mathbb{R}^{n} \left(\omega_{1}(\theta_{1},...,\theta_{n-1}),
\omega_{2}(\theta_{1},...,\theta_{n-1}),...,\omega_{n}(\theta_{1},...,\theta_{n-1})\right) (x_1,...,x_n) (x,y)\in\mathbb{S}^{1} (x,y)=(\omega_1,\omega_2)=(\cos{\theta_{1}},\sin{\theta_1}) \theta_{1}\in [0,2\pi] (x,y,z)\in\mathbb{S}^{2} (x,y,z)=(\omega_1,\omega_2,\omega_3)=(\sin{\theta_{1}}\cos{\theta_2},\sin{\theta_1}\sin{\theta_{2}},\cos{\theta_{1}}) \theta_{1}\in[0,\pi], \theta_{2}\in[0,2\pi] I \omega_{i}=\phi_{i}(\omega_{1},\omega_{2},...,\omega_{n}) \phi_{i} I=\int_{\cup_{\theta_1,\theta_2,...,\theta_{n-1}}{(\phi_{1},...,\phi_{n})}} f(\phi_{1},...,\phi_{n})\det\left(\frac{\partial(\omega_1,...,\omega_n)}{\partial(\phi_1,...,\phi_n)}\right)\,d\sigma_{n-1} ?","['real-analysis', 'multivariable-calculus', 'change-of-variable']"
54,Approximation of smooth diffeomorphisms by polynomial diffeomorphisms?,Approximation of smooth diffeomorphisms by polynomial diffeomorphisms?,,"Is it possible to (locally) approximate an arbitrary smooth diffeomorphism by a polynomial diffeomorphism ? More precisely: Let $f:\mathbb{R}^d\rightarrow\mathbb{R}^d$ be a smooth diffeomorphism. For $U\subset\mathbb{R}^d$ bounded and open and $\varepsilon>0$ , is there a diffeomorphism $p=(p_1, \cdots, p_d) : U\rightarrow\mathbb{R}^d$ (with inverse $q:=p^{-1} : p(U)\rightarrow U$ ) such that both $\|f - p\|_{\infty;\,U}:=\sup_{x\in U}|f(x) - p(x)| < \varepsilon$ , $\ \textbf{and}$ each component of $p$ and of $q=(q_1,\cdots,q_d)$ is a polynomial, i.e. $p_i, q_i\in\mathbb{R}[x_1, \ldots, x_d]$ for each $i=1, \ldots, d$ ? Clearly, by Stone-Weierstrass there is a polynomial map $p : \mathbb{R}^d\rightarrow\mathbb{R}^d$ with $\|f - p\|_{\infty;\,U} < \varepsilon$ and such that $q:=(\left.p\right|_U)^{-1}$ exists; in general, however, this $q$ will not be a polynomial map. Do you have any ideas/references under which conditions on $f$ an approximation of the above kind can be guaranteed nonetheless?","Is it possible to (locally) approximate an arbitrary smooth diffeomorphism by a polynomial diffeomorphism ? More precisely: Let be a smooth diffeomorphism. For bounded and open and , is there a diffeomorphism (with inverse ) such that both , each component of and of is a polynomial, i.e. for each ? Clearly, by Stone-Weierstrass there is a polynomial map with and such that exists; in general, however, this will not be a polynomial map. Do you have any ideas/references under which conditions on an approximation of the above kind can be guaranteed nonetheless?","f:\mathbb{R}^d\rightarrow\mathbb{R}^d U\subset\mathbb{R}^d \varepsilon>0 p=(p_1, \cdots, p_d) : U\rightarrow\mathbb{R}^d q:=p^{-1} : p(U)\rightarrow U \|f - p\|_{\infty;\,U}:=\sup_{x\in U}|f(x) - p(x)| < \varepsilon \ \textbf{and} p q=(q_1,\cdots,q_d) p_i, q_i\in\mathbb{R}[x_1, \ldots, x_d] i=1, \ldots, d p : \mathbb{R}^d\rightarrow\mathbb{R}^d \|f - p\|_{\infty;\,U} < \varepsilon q:=(\left.p\right|_U)^{-1} q f","['real-analysis', 'polynomials', 'inverse', 'approximation-theory', 'diffeomorphism']"
55,Distribution of $\{n^p\alpha\}$ for irrational $\alpha$,Distribution of  for irrational,\{n^p\alpha\} \alpha,"Let $\alpha$ be an irational number. Consider sequence $x_n=\{n^p\alpha\}$ , $n\in\mathbb{N}$ (it's the fractional part of $n^p\alpha$ ), where $p$ is a nonzero real number. Question. For which values of $p$ the sequence $\{x_n\}_{n\in\mathbb{N}}$ is equidistributed on $[0,1)$ ? The other qusetion is when $\{x_n\}_{n\in\mathbb{N}}$ is dense on $[0,1)$ . It's known that if $p\in\mathbb{N}$ , then it's true (it's a consequence of Weyl's equdsitribution criterion and van der Corput's difference theorem). However, it's not clear how to apply Weyl's criterion in case when $p\notin\mathbb{N}$ . I encountered similar problem when I was working on this question Convergence of the product $\prod_{n=1}^{\infty}\left(1+\frac{x^n}{n^p}\right)\cos\frac{x^n}{n^q}$ (in order to study the behaviour of $\cos\frac{1}{n^q}$ , so $\alpha=1/\pi$ in this case). Update. It would be also intersting to investigate the distbution of $\left\{\frac{x^n}{n^p}\right\}$ . If the result is known, please give a link or reference. Any help would be appreciated.","Let be an irational number. Consider sequence , (it's the fractional part of ), where is a nonzero real number. Question. For which values of the sequence is equidistributed on ? The other qusetion is when is dense on . It's known that if , then it's true (it's a consequence of Weyl's equdsitribution criterion and van der Corput's difference theorem). However, it's not clear how to apply Weyl's criterion in case when . I encountered similar problem when I was working on this question Convergence of the product $\prod_{n=1}^{\infty}\left(1+\frac{x^n}{n^p}\right)\cos\frac{x^n}{n^q}$ (in order to study the behaviour of , so in this case). Update. It would be also intersting to investigate the distbution of . If the result is known, please give a link or reference. Any help would be appreciated.","\alpha x_n=\{n^p\alpha\} n\in\mathbb{N} n^p\alpha p p \{x_n\}_{n\in\mathbb{N}} [0,1) \{x_n\}_{n\in\mathbb{N}} [0,1) p\in\mathbb{N} p\notin\mathbb{N} \cos\frac{1}{n^q} \alpha=1/\pi \left\{\frac{x^n}{n^p}\right\}","['real-analysis', 'irrational-numbers', 'equidistribution']"
56,Surjective sum measure,Surjective sum measure,,"Let $(a_n)$ be a sequence of positive real numbers convergent to $0$ . Let $\mu(A) = \sum_{n \in A}a_n$ for every $A \subset \mathbb{N}$ . What is the ""if and only if"" condition on $a_n$ for $\mu$ to attain every value from $[0, \mu(\mathbb{N})]$ ?","Let be a sequence of positive real numbers convergent to . Let for every . What is the ""if and only if"" condition on for to attain every value from ?","(a_n) 0 \mu(A) = \sum_{n \in A}a_n A \subset \mathbb{N} a_n \mu [0, \mu(\mathbb{N})]","['real-analysis', 'sequences-and-series', 'measure-theory']"
57,Can we approximate harmonic functions with harmonic functions with non-vanishing differential?,Can we approximate harmonic functions with harmonic functions with non-vanishing differential?,,"Let $\mathbb{D}^2$ be the closed $2$ -dimensional unit disk, and let $g:\mathbb{D}^2 \to \mathbb{R}$ be a non-constant harmonic function (in particular smooth up to the boundary). Does there exist a sequence of smooth harmonic functions $g_n$ on $\mathbb{D}^2 $ , such that $g_n \to g$ in $W^{1,2}$ and $dg_n \neq 0$ everywhere on $ \text{int}(\mathbb{D}^2)$ ? Since we can add additive constants to the $g_n$ , we can arrange $\int_{\mathbb D^2} g_n=\int_{\mathbb D^2} g$ , so the $W^{1,2}$ convergence of the $g_n$ is essentially equivalent to $dg_n \to dg$ in $L^2$ . (via Poincare inequality). Thinking on $dg$ as a vector field, I think that we can always approximate it with a non-zero vector field in $L^2$ . However, the only procedure I know for doing that does not produce approximating vector fields which are gradients of harmonic functions (or gradients of anything, really).","Let be the closed -dimensional unit disk, and let be a non-constant harmonic function (in particular smooth up to the boundary). Does there exist a sequence of smooth harmonic functions on , such that in and everywhere on ? Since we can add additive constants to the , we can arrange , so the convergence of the is essentially equivalent to in . (via Poincare inequality). Thinking on as a vector field, I think that we can always approximate it with a non-zero vector field in . However, the only procedure I know for doing that does not produce approximating vector fields which are gradients of harmonic functions (or gradients of anything, really).","\mathbb{D}^2 2 g:\mathbb{D}^2 \to \mathbb{R} g_n \mathbb{D}^2  g_n \to g W^{1,2} dg_n \neq 0  \text{int}(\mathbb{D}^2) g_n \int_{\mathbb D^2} g_n=\int_{\mathbb D^2} g W^{1,2} g_n dg_n \to dg L^2 dg L^2","['real-analysis', 'differential-geometry', 'partial-differential-equations', 'differential-topology', 'harmonic-functions']"
58,Devise winning strategy to hit moving target,Devise winning strategy to hit moving target,,"(Based on a problem from Brilliant). Suppose there is an enemy submarine at unknown location on real number line moving at unknown real-valued  velocity. You can fire one missile per minute in attempt to hit the submarine, and your missile will hit any submarine within fixed radius $\varepsilon>0$ . Is there a strategy to guarantee you hit the submarine in a finite number of steps? Note that if the submarine has integer position and integer velocity, then we can simply enumerate all position-velocity pairs $(x_1,v_1),(x_2,v_2),(x_3,v_3),\dots$ (since $\Bbb Z^2$ is countable) then on the $n$ -th step, fire your missile at position $x_n+nv_n$ , which is where the $n$ -th submarine would be at step $n$ . For integer-valued position and velocity, this is a winning strategy. For real-valued position and velocity, the problem can be formalized like this: Let $\varepsilon>0$ . Does there exist a function $f:\Bbb N\to\Bbb R$ such that for every linear function $s:\Bbb N\to\Bbb R$ , given by $s(n)=x+nv$ for $x,v\in\Bbb R$ , there exists $n\in\Bbb N$ such that $|f(n)-s(n)|<\varepsilon$ ? Attempt. For each $\varepsilon>0$ , $n\in\Bbb N$ , $f\in\Bbb R$ , the set $$\{(x,v)\in\Bbb R^2:|f-(x+nv)|<\varepsilon\}$$ (the set of all submarines you kill by firing at position $f$ and step $n$ ) forms a thin band in $\Bbb R^2$ . The question is whether for $n=1,2,3,\dots$ we can generate sequence of bands that cover the entire $\Bbb R^2$ (thus hitting every possible submarine, a winning strategy). Observe that the width of the bands $\to0$ as $n\to\infty$ , but the width is similar to $1/n$ so it's like the harmonic series which is not finite. This provides good hope that these bands can cover $\Bbb R^2$ but I cannot dream of a sequence $f$ which guarantees this.","(Based on a problem from Brilliant). Suppose there is an enemy submarine at unknown location on real number line moving at unknown real-valued  velocity. You can fire one missile per minute in attempt to hit the submarine, and your missile will hit any submarine within fixed radius . Is there a strategy to guarantee you hit the submarine in a finite number of steps? Note that if the submarine has integer position and integer velocity, then we can simply enumerate all position-velocity pairs (since is countable) then on the -th step, fire your missile at position , which is where the -th submarine would be at step . For integer-valued position and velocity, this is a winning strategy. For real-valued position and velocity, the problem can be formalized like this: Let . Does there exist a function such that for every linear function , given by for , there exists such that ? Attempt. For each , , , the set (the set of all submarines you kill by firing at position and step ) forms a thin band in . The question is whether for we can generate sequence of bands that cover the entire (thus hitting every possible submarine, a winning strategy). Observe that the width of the bands as , but the width is similar to so it's like the harmonic series which is not finite. This provides good hope that these bands can cover but I cannot dream of a sequence which guarantees this.","\varepsilon>0 (x_1,v_1),(x_2,v_2),(x_3,v_3),\dots \Bbb Z^2 n x_n+nv_n n n \varepsilon>0 f:\Bbb N\to\Bbb R s:\Bbb N\to\Bbb R s(n)=x+nv x,v\in\Bbb R n\in\Bbb N |f(n)-s(n)|<\varepsilon \varepsilon>0 n\in\Bbb N f\in\Bbb R \{(x,v)\in\Bbb R^2:|f-(x+nv)|<\varepsilon\} f n \Bbb R^2 n=1,2,3,\dots \Bbb R^2 \to0 n\to\infty 1/n \Bbb R^2 f",['real-analysis']
59,The necessity of absolute convergence in the convergence of the Cauchy product of series?,The necessity of absolute convergence in the convergence of the Cauchy product of series?,,"The Mertens' theorem claims that Suppose $\sum_{n=0}^\infty a_n,\sum_{n=0}^\infty b_n$ are two convergent series of complex numbers, convergent to $A,\beta$ respectively. If $\sum_na_n$ converges absolutely, then the Cauchy product $\sum_{n=0}^\infty c_n$ converges to $A\beta$ , where $c_n=\sum_{k=0}^na_kb_{n-k}$ . I wonder whether the absolute convergence of $\sum_na_n$ is generally necessary. I know that there are counterexamples where $\sum_na_n, \sum_nb_n$ converge conditionally but the Cauchy product $\sum_nc_n$ diverges. However, they are too special. I also know that the Cesàro sum of $(c_n)$ is $A\beta$ . The precise question of which I wonder the answer is that: Given a series $\sum_{n=0}^\infty a_n$ of complex numbers. Suppose that for all convergent series $\sum_{n=0}^\infty b_n$ of complex numbers, the Cauchy product $\sum_{n=0}^\infty c_n$ converges. Does this imply that the series $\sum_na_n$ converges absolutely? Denote by $\beta_n$ the partial sum $\sum_{k=0}^nb_n$ . The previous question is equivalent to the following: Given a series $\sum_{n=0}^\infty a_n$ of complex numbers. Suppose that for all convergent sequences $(\beta_n)$ of complex numbers, the convolution sequence $(\sum_{k=0}^na_k\beta_{n-k})_{n\in\mathbb N}$ converges as $n\to\infty$ . Does this imply that the series $\sum_na_n$ converges absolutely? We have some immediate consequences: First we take $\beta_n=1$ for all $n\in\mathbb N$ to deduce that $\sum_na_n$ converges. In this case, we can only test with those $(\beta_n)$ such that $\lim_{n\to\infty}\beta_n=0$ . And the Silverman-Töplitz theorem tells us that the Cesàro mean of $(\sum_ka_k\beta_{n-k})_n$ tends to zero, hence what we know is that $\lim_{n\to\infty}\sum_ka_k\beta_{n-k}=0$ under the assumption that $\lim_{n\to\infty}\beta_n=0$ . My idea to attack is that, given a conditionally convergent series $\sum_na_n$ , we try to find a sequence $\beta_n$ and some $\epsilon>0$ such that there are infinitely many $n$ such that $\lvert\sum_ka_k\beta_{n-k}\rvert>\epsilon$ . I don't know how to proceed next. Note There is a more highbrow aspect. Denote by $c_0\subseteq \ell^\infty$ the closed subspace of sequences which converge to zero. Mertens' theorem claim that the convolution map $a*-\colon \ell^\infty\to \ell^\infty$ restricts to a map $c_0\to c_0$ for any absolutely convergent series $\sum_{n=0}^\infty a_n$ . This hints that the preceeding question might be solved by tools in functional analysis, such as Baire's category theorem. Update I suddenly come up with a possible solution: As indicated above, our assumption is that $a*-\colon c_0\to c_0$ is a well-defined linear operator. To invoke closed graph theorem, assume that $\beta^{(n)}\to \beta$ and $a*\beta^{(n)}\to\gamma$ in $c_0$ . The convergence in $c_0$ implies the pointwise convergence, therefore $a*\beta=\gamma$ . By closed graph theorem, the operator $a*-$ is continuous, that is to say, there is a constant $M$ such that $\lVert a*\beta\rVert_{\ell^\infty}\le M\lVert \beta\rVert_{\ell^\infty}$ for all $\beta\in c_0$ . Then for all $m\in\mathbb N$ , we take $(\beta_n)_n\in c_0$ such that $\beta_na_{m-n}=\lvert a_{m-n}\rvert$ and $\lvert\beta_n\rvert=1$ for $n\le m$ , and $\beta_n=0$ for $n>m$ . Then we have $\sum_{n=0}^m\lvert a_n\rvert\le\lVert a*\beta\rVert_{\ell^\infty}\le M$ . Q.E.D. Is it correct?","The Mertens' theorem claims that Suppose are two convergent series of complex numbers, convergent to respectively. If converges absolutely, then the Cauchy product converges to , where . I wonder whether the absolute convergence of is generally necessary. I know that there are counterexamples where converge conditionally but the Cauchy product diverges. However, they are too special. I also know that the Cesàro sum of is . The precise question of which I wonder the answer is that: Given a series of complex numbers. Suppose that for all convergent series of complex numbers, the Cauchy product converges. Does this imply that the series converges absolutely? Denote by the partial sum . The previous question is equivalent to the following: Given a series of complex numbers. Suppose that for all convergent sequences of complex numbers, the convolution sequence converges as . Does this imply that the series converges absolutely? We have some immediate consequences: First we take for all to deduce that converges. In this case, we can only test with those such that . And the Silverman-Töplitz theorem tells us that the Cesàro mean of tends to zero, hence what we know is that under the assumption that . My idea to attack is that, given a conditionally convergent series , we try to find a sequence and some such that there are infinitely many such that . I don't know how to proceed next. Note There is a more highbrow aspect. Denote by the closed subspace of sequences which converge to zero. Mertens' theorem claim that the convolution map restricts to a map for any absolutely convergent series . This hints that the preceeding question might be solved by tools in functional analysis, such as Baire's category theorem. Update I suddenly come up with a possible solution: As indicated above, our assumption is that is a well-defined linear operator. To invoke closed graph theorem, assume that and in . The convergence in implies the pointwise convergence, therefore . By closed graph theorem, the operator is continuous, that is to say, there is a constant such that for all . Then for all , we take such that and for , and for . Then we have . Q.E.D. Is it correct?","\sum_{n=0}^\infty a_n,\sum_{n=0}^\infty b_n A,\beta \sum_na_n \sum_{n=0}^\infty c_n A\beta c_n=\sum_{k=0}^na_kb_{n-k} \sum_na_n \sum_na_n, \sum_nb_n \sum_nc_n (c_n) A\beta \sum_{n=0}^\infty a_n \sum_{n=0}^\infty b_n \sum_{n=0}^\infty c_n \sum_na_n \beta_n \sum_{k=0}^nb_n \sum_{n=0}^\infty a_n (\beta_n) (\sum_{k=0}^na_k\beta_{n-k})_{n\in\mathbb N} n\to\infty \sum_na_n \beta_n=1 n\in\mathbb N \sum_na_n (\beta_n) \lim_{n\to\infty}\beta_n=0 (\sum_ka_k\beta_{n-k})_n \lim_{n\to\infty}\sum_ka_k\beta_{n-k}=0 \lim_{n\to\infty}\beta_n=0 \sum_na_n \beta_n \epsilon>0 n \lvert\sum_ka_k\beta_{n-k}\rvert>\epsilon c_0\subseteq \ell^\infty a*-\colon \ell^\infty\to \ell^\infty c_0\to c_0 \sum_{n=0}^\infty a_n a*-\colon c_0\to c_0 \beta^{(n)}\to \beta a*\beta^{(n)}\to\gamma c_0 c_0 a*\beta=\gamma a*- M \lVert a*\beta\rVert_{\ell^\infty}\le M\lVert \beta\rVert_{\ell^\infty} \beta\in c_0 m\in\mathbb N (\beta_n)_n\in c_0 \beta_na_{m-n}=\lvert a_{m-n}\rvert \lvert\beta_n\rvert=1 n\le m \beta_n=0 n>m \sum_{n=0}^m\lvert a_n\rvert\le\lVert a*\beta\rVert_{\ell^\infty}\le M","['real-analysis', 'sequences-and-series', 'functional-analysis', 'convergence-divergence', 'cauchy-product']"
60,A maximization problem in functional analysis and data,A maximization problem in functional analysis and data,,"Consider the minimization problem described this paper . Let $f_{\lambda}$ be the minimizer. As a part of extending my work, I am able to show the following facts $$\lim_\limits{\lambda \to 0}\|f_{\lambda}\|_{L^2} = 0$$ and $$\lim_\limits{\lambda \to \infty}\|f_{\lambda}\|_{L^2} = 0$$ My problem now is (as I would like to extend my work), find $\lambda \in (0,\infty)$ for which $\|f_{\lambda}\|_{L^2}$ is maximum. Appreciate your suggestions to solve this problem. The minimization problem from the linked paper is given below for the self containment of the post. If given that $k>\frac{m}{2}$ , the paper proves that there is a unique minimizer for the functional $C(f)$ in the set $S$ . It is given that $k>\frac{m}{2}$ My progress Partial Progress : Progress : I am able to derive the corresponding PDE equations for the problem. Let $f(\lambda,x) = f_{\lambda}(x)$ . Then The first equation corresponds to maximizing $\|f_{\lambda}\|$ , while the second PDE is for the minimization problem associated with the parameter $\lambda$ . The second equation (minimization problem), given any $\lambda$ , I can solve for $f(\lambda,.)$ either using linear algebra or steepest descent algorithm, which I have described in my article. Now I need to use this solution and the first equation to obtain $\lambda$ , which is a problem I am facing. Trying to solve using linear algebra, by formulating the discrete version of the problem using Fourier series coefficients and Plancheral theorem, I get stuck at a matrix problem. More Partial Progress An Iterative algorithm which is a modified steepest descent. Initialize $f$ . Assuming some $\lambda$ and assuming gradient of $C_{\lambda}(f)$ wrt $f$ be $\nabla_f C_{\lambda}(f)$ , and if we were to update $f$ with this gradient as in we do in steepest descent, it would be $f^u_\lambda = f - \delta \nabla_f C_{\lambda}(f)$ , where $\delta$ is a constant learning rate. Now set $\frac{\partial\|f^u_\lambda\|}{\partial \lambda} = 0$ and solve for $\lambda$ . Let the root be $\lambda_0$ . Update $f = f^u_{\lambda_0}$ . (update $f$ as in steepest descent, but using $\lambda$ value as $\lambda_0$ which was computed in step 2.) check some convergence criterion and if not met, go to step 2. I have implemented this numerically and it converges as desired. Need to work on the proof. PS : This was first posted on MO by me, 3 months back. Link","Consider the minimization problem described this paper . Let be the minimizer. As a part of extending my work, I am able to show the following facts and My problem now is (as I would like to extend my work), find for which is maximum. Appreciate your suggestions to solve this problem. The minimization problem from the linked paper is given below for the self containment of the post. If given that , the paper proves that there is a unique minimizer for the functional in the set . It is given that My progress Partial Progress : Progress : I am able to derive the corresponding PDE equations for the problem. Let . Then The first equation corresponds to maximizing , while the second PDE is for the minimization problem associated with the parameter . The second equation (minimization problem), given any , I can solve for either using linear algebra or steepest descent algorithm, which I have described in my article. Now I need to use this solution and the first equation to obtain , which is a problem I am facing. Trying to solve using linear algebra, by formulating the discrete version of the problem using Fourier series coefficients and Plancheral theorem, I get stuck at a matrix problem. More Partial Progress An Iterative algorithm which is a modified steepest descent. Initialize . Assuming some and assuming gradient of wrt be , and if we were to update with this gradient as in we do in steepest descent, it would be , where is a constant learning rate. Now set and solve for . Let the root be . Update . (update as in steepest descent, but using value as which was computed in step 2.) check some convergence criterion and if not met, go to step 2. I have implemented this numerically and it converges as desired. Need to work on the proof. PS : This was first posted on MO by me, 3 months back. Link","f_{\lambda} \lim_\limits{\lambda \to 0}\|f_{\lambda}\|_{L^2} = 0 \lim_\limits{\lambda \to \infty}\|f_{\lambda}\|_{L^2} = 0 \lambda \in (0,\infty) \|f_{\lambda}\|_{L^2} k>\frac{m}{2} C(f) S k>\frac{m}{2} f(\lambda,x) = f_{\lambda}(x) \|f_{\lambda}\| \lambda \lambda f(\lambda,.) \lambda f \lambda C_{\lambda}(f) f \nabla_f C_{\lambda}(f) f f^u_\lambda = f - \delta \nabla_f C_{\lambda}(f) \delta \frac{\partial\|f^u_\lambda\|}{\partial \lambda} = 0 \lambda \lambda_0 f = f^u_{\lambda_0} f \lambda \lambda_0","['real-analysis', 'functional-analysis', 'machine-learning']"
61,Application of the Arzela-Ascoli Theorem,Application of the Arzela-Ascoli Theorem,,"Set-Up $\Omega$ is an open subset of $\mathbb{R}^{n}$ . The family $\{\phi_{j}\}_{j \in \mathbb{N}}$ is a Complete Orthonormal Set in $L^{2}_{\sigma}(\Omega) = \{f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n} \ | \ f \in L^{2}(\Omega), \   \text{div}(f) = 0 \}$ . In particular the closure $\overline{\text{span} \{\phi_{j}\}_{j \in \mathbb{N}}} = L^{2}_{\sigma}(\Omega) $ $\{u_{m}\}_{m \in \mathbb{N}}$ is a family of functions $u_{m} : \mathbb{R}^{n} \times [0,T] \rightarrow \mathbb{R}^{n}$ , $u_{m} \in L^{2}([0,T] ; H^{1}(\Omega) \cap L^{2}_{\sigma}(\Omega))$ such that: The family $\{ (u_{m}(\cdot,t) , \phi_{j} ) \}_{m \in \mathbb{N}}$ is uniformly bounded and equitcontinuous with respect to $t \in [0,T]$ , for each $j$ fixed. Here $(\cdot , \cdot)$ denotes the inner product of $L_2(\Omega)$ , ie $(u(\cdot, t),f) = \int_{\Omega} u(x,t) \cdot f(x) \text{d}x$ . Then, by the Arzela-Ascoli Theorem, it holds that there exists a subsequence $\{ (u_{k}(\cdot,t) , \phi_{j} ) \}_{k \in \mathbb{N}}$ which converges uniformly to some function on $[0,T]$ . We also have the following inequality on $u_{m}$ : $||u_{m}(t)||^2 + \int^{t}_{0} ||\nabla u_{m}(\tau)||^2 \text{d}\tau \leq M_{T}$ , for all $0 \leq t < T$ , where $M_{T}$ is a constant independent of $m, t$ . Problem In the paper I am reading, it is stated that from these two facts, we can use ""the usual diagonal argument"" to show that: There exists a subsequence $\{u_{k}\}$ such that $u_{k}(t)$ converges to some $u(t)$ uniformly on $[0,T]$ , in the weak topology of $L^{2}_{\sigma}(\Omega)$ . I understand, and have been able to confirm my self, that the sequence $\{ (u_{k}(\cdot,t) , \phi_{j} ) \}_{k \in \mathbb{N}}$ does converge uniformly to a function on $[0,T]$ , but how do I show from this that the functions $u_{k}$ converge weakly in $L^{2}_{\sigma}$ ? Is it possible to show that there exists some $u \in L^{2}_{\sigma}$ such that $\{ (u_{k}(\cdot,t) , \phi_{j} ) \}_{k \in \mathbb{N}} \rightarrow \{ (u(\cdot,t) , \phi_{j} ) \}$ , for all $\phi_{j}$ ? If we could show this, then we definitely have weak convergence of $u_{k}$ . Attempted Answer Due to the definition of $\{\phi_{j}\}_{j \in \mathbb{N}}$ , we can show that $\{ (u_{m}(\cdot,t) , f ) \}_{m \in \mathbb{N}}$ converges uniformly with respect to $t \in [0,T]$ to some function in $L^{2}$ , for arbitrary $f \in L^{2}(\Omega)$ . We claim that this limit is of the following form: $\{ (u_{m}(\cdot,t) , f ) \}_{m \in \mathbb{N}} \rightarrow \{ (u(\cdot,t) , f ) \}$ for some $u \in L^{2}_{\sigma}$ . In which case, it is clear that $u_{m}$ converges to this $u$ weakly in $L^{2}_{\sigma}(\Omega)$ . We consider $u_{m}(\cdot, t)$ as a function $(u_{m}(\cdot, t), \cdot)$ in the dual space $(L^{2})^{\ast}$ . Since $\{ (u_{m}(\cdot,t) , f ) \}_{m \in \mathbb{N}}$ converges uniformly with respect to $t \in [0,T]$ , for all $f \in L^{2}(\Omega)$ , we conclude that $(u_{m}(\cdot, t), f)$ converges pointwise with respect to $f$ to some function $u_{t}(f)$ . Is it possible from this to conclude that the function $u_{t}(f)$ is in fact in $(L^{2})^{\ast}$ ???","Set-Up is an open subset of . The family is a Complete Orthonormal Set in . In particular the closure is a family of functions , such that: The family is uniformly bounded and equitcontinuous with respect to , for each fixed. Here denotes the inner product of , ie . Then, by the Arzela-Ascoli Theorem, it holds that there exists a subsequence which converges uniformly to some function on . We also have the following inequality on : , for all , where is a constant independent of . Problem In the paper I am reading, it is stated that from these two facts, we can use ""the usual diagonal argument"" to show that: There exists a subsequence such that converges to some uniformly on , in the weak topology of . I understand, and have been able to confirm my self, that the sequence does converge uniformly to a function on , but how do I show from this that the functions converge weakly in ? Is it possible to show that there exists some such that , for all ? If we could show this, then we definitely have weak convergence of . Attempted Answer Due to the definition of , we can show that converges uniformly with respect to to some function in , for arbitrary . We claim that this limit is of the following form: for some . In which case, it is clear that converges to this weakly in . We consider as a function in the dual space . Since converges uniformly with respect to , for all , we conclude that converges pointwise with respect to to some function . Is it possible from this to conclude that the function is in fact in ???","\Omega \mathbb{R}^{n} \{\phi_{j}\}_{j \in \mathbb{N}} L^{2}_{\sigma}(\Omega) = \{f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n} \ | \ f \in L^{2}(\Omega), \   \text{div}(f) = 0 \} \overline{\text{span} \{\phi_{j}\}_{j \in \mathbb{N}}} = L^{2}_{\sigma}(\Omega)  \{u_{m}\}_{m \in \mathbb{N}} u_{m} : \mathbb{R}^{n} \times [0,T] \rightarrow \mathbb{R}^{n} u_{m} \in L^{2}([0,T] ; H^{1}(\Omega) \cap L^{2}_{\sigma}(\Omega)) \{ (u_{m}(\cdot,t) , \phi_{j} ) \}_{m \in \mathbb{N}} t \in [0,T] j (\cdot , \cdot) L_2(\Omega) (u(\cdot, t),f) = \int_{\Omega} u(x,t) \cdot f(x) \text{d}x \{ (u_{k}(\cdot,t) , \phi_{j} ) \}_{k \in \mathbb{N}} [0,T] u_{m} ||u_{m}(t)||^2 + \int^{t}_{0} ||\nabla u_{m}(\tau)||^2 \text{d}\tau \leq M_{T} 0 \leq t < T M_{T} m, t \{u_{k}\} u_{k}(t) u(t) [0,T] L^{2}_{\sigma}(\Omega) \{ (u_{k}(\cdot,t) , \phi_{j} ) \}_{k \in \mathbb{N}} [0,T] u_{k} L^{2}_{\sigma} u \in L^{2}_{\sigma} \{ (u_{k}(\cdot,t) , \phi_{j} ) \}_{k \in \mathbb{N}} \rightarrow \{ (u(\cdot,t) , \phi_{j} ) \} \phi_{j} u_{k} \{\phi_{j}\}_{j \in \mathbb{N}} \{ (u_{m}(\cdot,t) , f ) \}_{m \in \mathbb{N}} t \in [0,T] L^{2} f \in L^{2}(\Omega) \{ (u_{m}(\cdot,t) , f ) \}_{m \in \mathbb{N}} \rightarrow \{ (u(\cdot,t) , f ) \} u \in L^{2}_{\sigma} u_{m} u L^{2}_{\sigma}(\Omega) u_{m}(\cdot, t) (u_{m}(\cdot, t), \cdot) (L^{2})^{\ast} \{ (u_{m}(\cdot,t) , f ) \}_{m \in \mathbb{N}} t \in [0,T] f \in L^{2}(\Omega) (u_{m}(\cdot, t), f) f u_{t}(f) u_{t}(f) (L^{2})^{\ast}","['real-analysis', 'functional-analysis', 'functions', 'partial-differential-equations', 'weak-convergence']"
62,Proving that $\lVert u \rVert_{L^2} \leq Ce^{-\nu t}$ for certain pde,Proving that  for certain pde,\lVert u \rVert_{L^2} \leq Ce^{-\nu t},"Given that $$\frac{\partial u}{\partial t}+\sin(y)\frac{\partial u}{\partial x}=\nu\Bigl(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}\Bigr)$$ With the following periodic boundary conditions: $$u(-\pi,y,t)=u(\pi,y,t) \\ u(x,-\pi,t)=u(x,\pi,t) \\u_x(-\pi,y,t)=u_x(\pi,y,t)\\ u_y(x,-\pi,t)=u_y(x,\pi,t)\\ u(x,y,0)=F(x,y)$$ Prove that $$\lVert u \rVert_{L^2} \leq Ce^{-\nu t}$$ I have used the finite Fourier transform to get that $$\frac{du_{mn}}{dt}=-\nu (n^2+m^2)u_{mn} -\int_{-\pi}^{\pi}\sin(y) u_ne^{-imy}dy$$ Where $$u_{mn}=\int_{-\pi}^{\pi} \int_{-\pi}^{\pi} u(x,y,t)e^{-imx} *e^{-iny} dxdy$$ Second I tried Energy method Multiply by u and then integrate, still I didn't get the required result. How to get the required result ? Any Hint ?","Given that With the following periodic boundary conditions: Prove that I have used the finite Fourier transform to get that Where Second I tried Energy method Multiply by u and then integrate, still I didn't get the required result. How to get the required result ? Any Hint ?","\frac{\partial u}{\partial t}+\sin(y)\frac{\partial u}{\partial x}=\nu\Bigl(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}\Bigr) u(-\pi,y,t)=u(\pi,y,t) \\ u(x,-\pi,t)=u(x,\pi,t) \\u_x(-\pi,y,t)=u_x(\pi,y,t)\\
u_y(x,-\pi,t)=u_y(x,\pi,t)\\
u(x,y,0)=F(x,y) \lVert u \rVert_{L^2} \leq Ce^{-\nu t} \frac{du_{mn}}{dt}=-\nu (n^2+m^2)u_{mn} -\int_{-\pi}^{\pi}\sin(y) u_ne^{-imy}dy u_{mn}=\int_{-\pi}^{\pi} \int_{-\pi}^{\pi} u(x,y,t)e^{-imx} *e^{-iny} dxdy","['real-analysis', 'integration', 'functional-analysis', 'partial-differential-equations']"
63,Interpretation of Differentials,Interpretation of Differentials,,"$$ \newcommand{\qa}{P} \newcommand{\qb}{Q} \newcommand{\da}{dP} \newcommand{\db}{dQ} \newcommand{\positiverealnumbers}{\mathbb{R}_+} \newcommand{\realnumbers}{\mathbb{R}} \newcommand{\naturalnumbers}{\mathbb{N}} \newcommand{\positiveintegers}{\mathbb{Z}_+} $$ We frequently solve geometrical and physical problems by obtaining an approximate expression for differential $\da$ in terms of differential $\db$ and then integrating $\da$ to obtain $\qa$ . We assume that the expression for $\qa$ is exact even though we used an approximate formula for $\da$ . This is justified by saying that the differentials are infinitely small quantities. For example, when we derive an expression for the area of a circular disc (see example 1) we set $dA = 2 \pi r dr$ which is an approximate expression when the diffentials are interpreted as real numbers. In this article we try to define a method for computing $\qa$ so that we don't need approximate expressions in the derivation. Theorem 1 Let $a,b \in \realnumbers$ and $a < b$ . Let $f$ be a function from $[a,b]$ into $\realnumbers$ and define $\Delta f = f(x + \Delta x) - f(x)$ where $x \in \realnumbers$ and $\Delta x \in \realnumbers \backslash \{0\}$ and $x, x + \Delta x \in [a, b]$ . Suppose that $$   \Delta f = g(x) \Delta x + h(x, \Delta x)   $$ where $x$ and $\Delta x$ are defined as before. Suppose also that $g$ is Riemann integrable and $$     \lim_{\Delta x \to 0} \frac{h(x,\Delta x)}{\Delta x} = 0   $$ for all $x \in [a, b]$ . Now $df = g(x) dx$ and $$ f(x) - f(a) = \int_a^x g(t) dt . $$ Proof This is a direct consequence of the definition of differentiability and Fundamental Theory of Calculus. $$\tag*{$\blacksquare$}$$ The condition (1) can be weakened to $$   \lim_{\Delta x \to 0+} \frac{h(x, \Delta x)}{\Delta x} = 0 .   \tag{2} $$ Theorem 2 A sufficient condition for inequality (2) is that there exist $S, C \in \positiverealnumbers$ so that $$   \vert h(x, \Delta x) \vert < C \vert \Delta x \vert^2   $$ for all $x, x + \Delta x \in [a, b]$ and $0 < \Delta x < S$ . Proof $$   \left\vert \frac{h(x, \Delta x)}{\Delta x} \right\vert   < C \vert \Delta x \vert \to 0   $$ as $\Delta x \to 0+$ . $$\tag*{$\blacksquare$}$$ Example 1 Derive an expression for the area of a disc whose inner radius is $r_a$ and outer radius $r_b$ . The area to be computed in example 1 Solution Define $\Delta A$ to be the area of a disc with inner radius $r$ and width $\Delta r$ . We have $$ 2 \pi r \Delta r \leq \Delta A \leq 2 \pi (r + \Delta r) \Delta r $$ By setting $g(r) := 2 \pi r$ and $h(r, \Delta r) := 2 \pi (\Delta r)^2$ we get $A = \pi r_b^2 - \pi r_a^2$ by Theorems 1 and 2. Example 2 Suppose that a particle is moving under influence of a constant force $F = ma$ for time $T$ and the particle is initially at rest. Derive an expression for the kinetic energy of the particle. Assume that the work done by a constant force $F$ is $W = F s$ where $s$ is the distance that the particle moves in the direction of the force. Assume also that the kinetic energy of a particle at rest is $0$ . Solution We define $\Delta s$ to be the distance that the particle moves in the time interval $[t, t + \Delta t]$ . We have $v = at$ , $$ a t \Delta t \leq \Delta s \leq a (t + \Delta t) \Delta t , $$ and $$ a (t + \Delta t) \Delta t = a t \Delta t + a (\Delta t)^2 . $$ Set $g(t) := a t$ and $h(t, \Delta t) := a (\Delta t)^2$ and it follows from Theorems 1 and 2 that the distance that the particle moves in time $T$ is $$ s  = \int_0^T a t dt = \frac{1}{2} a T^2 $$ By setting $v_f = a T$ we obtain $$ E_k = W = \frac{1}{2} F a T^2 = \frac{1}{2} m a^2 T^2 = \frac{1}{2} m v_f^2 . $$ Alternative Solution Assume that the particle moves distance $\Delta s$ in time $\Delta t$ . Define $\Delta W := F \Delta s$ . Now the acceleration $a = \Delta v / \Delta t$ is a constant and we have $$   \Delta W = m a \Delta s = m \Delta v \frac{\Delta s}{\Delta t}   \tag{2} $$ Let $v_{\mathrm{min}}$ and $v_{\mathrm{max}}$ be the minimum and maximum velocities of the particle. We now have $$   v_{\mathrm{min}} \leq \frac{\Delta s}{\Delta t} \leq   v_{\mathrm{max}} . $$ If $\Delta v \geq 0$ we get $$   m v_{\mathrm{min}} \Delta v \leq \Delta W \leq m v_{\mathrm{max}}   \Delta v , $$ which is equivalent to $$   m v \Delta v \leq \Delta W \leq m (v + \Delta v) \Delta v . $$ If $\Delta v < 0$ we have $$   v + \Delta v \leq \frac{\Delta s}{\Delta t} \leq v , $$ from which it follows that $$   m v \Delta v \leq \Delta W \leq m (v + \Delta v) \Delta v . $$ Define $$   h(v, \Delta v) := \Delta W - m v \Delta v . $$ Now $$   0 \leq h(v, \Delta v) \leq m \Delta v^2 . $$ By setting $g(v) = m v$ and assuming that the kinetic energy is zero when $v = 0$ it follows from Theorems 1 and 2 that $$   E_k = W = \frac{1}{2} m v^2 . $$ Do you find this formalism useful? Tommi Höynälänmaa","We frequently solve geometrical and physical problems by obtaining an approximate expression for differential in terms of differential and then integrating to obtain . We assume that the expression for is exact even though we used an approximate formula for . This is justified by saying that the differentials are infinitely small quantities. For example, when we derive an expression for the area of a circular disc (see example 1) we set which is an approximate expression when the diffentials are interpreted as real numbers. In this article we try to define a method for computing so that we don't need approximate expressions in the derivation. Theorem 1 Let and . Let be a function from into and define where and and . Suppose that where and are defined as before. Suppose also that is Riemann integrable and for all . Now and Proof This is a direct consequence of the definition of differentiability and Fundamental Theory of Calculus. The condition (1) can be weakened to Theorem 2 A sufficient condition for inequality (2) is that there exist so that for all and . Proof as . Example 1 Derive an expression for the area of a disc whose inner radius is and outer radius . The area to be computed in example 1 Solution Define to be the area of a disc with inner radius and width . We have By setting and we get by Theorems 1 and 2. Example 2 Suppose that a particle is moving under influence of a constant force for time and the particle is initially at rest. Derive an expression for the kinetic energy of the particle. Assume that the work done by a constant force is where is the distance that the particle moves in the direction of the force. Assume also that the kinetic energy of a particle at rest is . Solution We define to be the distance that the particle moves in the time interval . We have , and Set and and it follows from Theorems 1 and 2 that the distance that the particle moves in time is By setting we obtain Alternative Solution Assume that the particle moves distance in time . Define . Now the acceleration is a constant and we have Let and be the minimum and maximum velocities of the particle. We now have If we get which is equivalent to If we have from which it follows that Define Now By setting and assuming that the kinetic energy is zero when it follows from Theorems 1 and 2 that Do you find this formalism useful? Tommi Höynälänmaa","
\newcommand{\qa}{P}
\newcommand{\qb}{Q}
\newcommand{\da}{dP}
\newcommand{\db}{dQ}
\newcommand{\positiverealnumbers}{\mathbb{R}_+}
\newcommand{\realnumbers}{\mathbb{R}}
\newcommand{\naturalnumbers}{\mathbb{N}}
\newcommand{\positiveintegers}{\mathbb{Z}_+}
 \da \db \da \qa \qa \da dA = 2 \pi r dr \qa a,b \in \realnumbers a < b f [a,b] \realnumbers \Delta f = f(x + \Delta x) - f(x) x \in \realnumbers \Delta x \in \realnumbers \backslash \{0\} x, x + \Delta x \in [a, b] 
  \Delta f = g(x) \Delta x + h(x, \Delta x)
   x \Delta x g 
    \lim_{\Delta x \to 0} \frac{h(x,\Delta x)}{\Delta x} = 0
   x \in [a, b] df = g(x) dx  f(x) - f(a) = \int_a^x g(t) dt .  \tag*{\blacksquare} 
  \lim_{\Delta x \to 0+} \frac{h(x, \Delta x)}{\Delta x} = 0 .
  \tag{2}
 S, C \in \positiverealnumbers 
  \vert h(x, \Delta x) \vert < C \vert \Delta x \vert^2
   x, x + \Delta x \in [a, b] 0 < \Delta x < S 
  \left\vert \frac{h(x, \Delta x)}{\Delta x} \right\vert
  < C \vert \Delta x \vert \to 0
   \Delta x \to 0+ \tag*{\blacksquare} r_a r_b \Delta A r \Delta r 
2 \pi r \Delta r \leq \Delta A \leq 2 \pi (r + \Delta r) \Delta r
 g(r) := 2 \pi r h(r, \Delta r) := 2 \pi (\Delta r)^2 A = \pi r_b^2 - \pi r_a^2 F = ma T F W = F s s 0 \Delta s [t, t + \Delta t] v = at 
a t \Delta t \leq \Delta s \leq a (t + \Delta t) \Delta t ,
 
a (t + \Delta t) \Delta t = a t \Delta t + a (\Delta t)^2 .
 g(t) := a t h(t, \Delta t) := a (\Delta t)^2 T 
s  = \int_0^T a t dt = \frac{1}{2} a T^2
 v_f = a T 
E_k = W = \frac{1}{2} F a T^2 = \frac{1}{2} m a^2 T^2
= \frac{1}{2} m v_f^2 .
 \Delta s \Delta
t \Delta W := F \Delta s a =
\Delta v / \Delta t 
  \Delta W = m a \Delta s = m \Delta v \frac{\Delta s}{\Delta t}
  \tag{2}
 v_{\mathrm{min}} v_{\mathrm{max}} 
  v_{\mathrm{min}} \leq \frac{\Delta s}{\Delta t} \leq
  v_{\mathrm{max}} .
 \Delta v \geq 0 
  m v_{\mathrm{min}} \Delta v \leq \Delta W \leq m v_{\mathrm{max}}
  \Delta v ,
 
  m v \Delta v \leq \Delta W \leq m (v + \Delta v) \Delta v .
 \Delta v < 0 
  v + \Delta v \leq \frac{\Delta s}{\Delta t} \leq v ,
 
  m v \Delta v \leq \Delta W \leq m (v + \Delta v) \Delta v .
 
  h(v, \Delta v) := \Delta W - m v \Delta v .
 
  0 \leq h(v, \Delta v) \leq m \Delta v^2 .
 g(v) = m v v = 0 
  E_k = W = \frac{1}{2} m v^2 .
","['real-analysis', 'calculus', 'integration', 'limits']"
64,"If $\mu$ has a density with respect to the Lebesgue measure, is $C_c(\mathbb R)$ dense in $L^p(\mu)$?","If  has a density with respect to the Lebesgue measure, is  dense in ?",\mu C_c(\mathbb R) L^p(\mu),"Let $\mu$ be a probability measure on $(\mathbb R,\mathcal B(\mathbb R))$ . Is $C_c^\infty(\mathbb R)$ dense in $L^p(\mu)$ for all $p\ge1$ ? Let $\lambda$ denote the Lebesgue measure on $(\mathbb R,\mathcal B(\mathbb R))$ . We know that $C_c(\mathbb R)$ is dense in $L^p(\lambda)$ for all $p\ge1$ . Since, $C_c^\infty(\mathbb R)$ is dense in $C_c(\mathbb R)$ , we can conclude that $C_c^\infty(\mathbb R)$ is dense in $L^p(\lambda)$ for all $p\ge1$ . Now, I'm especially interested in the case where $\mu$ has a density $f$ with respect to $\lambda$ . It would be even fine for me to assume that $f\in C^2(\mathbb R)$ and that $f>0$ . Moreover, it would be sufficient for me to obtain the desired claim for $p=2$ ? Is there any chance to use the known result for the Lebesgue measure?","Let be a probability measure on . Is dense in for all ? Let denote the Lebesgue measure on . We know that is dense in for all . Since, is dense in , we can conclude that is dense in for all . Now, I'm especially interested in the case where has a density with respect to . It would be even fine for me to assume that and that . Moreover, it would be sufficient for me to obtain the desired claim for ? Is there any chance to use the known result for the Lebesgue measure?","\mu (\mathbb R,\mathcal B(\mathbb R)) C_c^\infty(\mathbb R) L^p(\mu) p\ge1 \lambda (\mathbb R,\mathcal B(\mathbb R)) C_c(\mathbb R) L^p(\lambda) p\ge1 C_c^\infty(\mathbb R) C_c(\mathbb R) C_c^\infty(\mathbb R) L^p(\lambda) p\ge1 \mu f \lambda f\in C^2(\mathbb R) f>0 p=2","['real-analysis', 'functional-analysis', 'measure-theory']"
65,"Integration/measure theory ""paradox""?","Integration/measure theory ""paradox""?",,"I have encountered the following ""paradox."" Consider a dense countable subset of $\mathbb{R}$ , e.g. $\mathbb{Q}$ . Because the set is countable we may parametrise it by $\mathbb{Q} = \{ a_n \}_{n=1}^\infty$ . Then consider the function (for some $\epsilon >0$ ) $$\sum_{n=1}^\infty \chi_{[a_n, a_n + \epsilon/2^n)} $$ where $\chi$ is the indicator function. Because the set $\mathbb{Q}$ is dense, this function converges to infinity everywhere. But its integral according to Lebesgue measure is $$\int_{\mathbb{R}} \sum_{n=1}^\infty \chi_{[a_n, a_n + \epsilon/2^n)} d\mu = \sum_{n=1}^\infty \int_{\mathbb{R}}  \chi_{[a_n, a_n + \epsilon/2^n)} d\mu = \sum_{n=1}^\infty \frac{\epsilon}{2^n} = \epsilon$$ where we have commuted summation and integral using B. Levi's theorem on monotone convergence. Where is my mistake? EDIT: Soon after posting this I realised that my intuition the function converges to infinity everywhere is wrong.","I have encountered the following ""paradox."" Consider a dense countable subset of , e.g. . Because the set is countable we may parametrise it by . Then consider the function (for some ) where is the indicator function. Because the set is dense, this function converges to infinity everywhere. But its integral according to Lebesgue measure is where we have commuted summation and integral using B. Levi's theorem on monotone convergence. Where is my mistake? EDIT: Soon after posting this I realised that my intuition the function converges to infinity everywhere is wrong.","\mathbb{R} \mathbb{Q} \mathbb{Q} = \{ a_n \}_{n=1}^\infty \epsilon >0 \sum_{n=1}^\infty \chi_{[a_n, a_n + \epsilon/2^n)}  \chi \mathbb{Q} \int_{\mathbb{R}} \sum_{n=1}^\infty \chi_{[a_n, a_n + \epsilon/2^n)} d\mu = \sum_{n=1}^\infty \int_{\mathbb{R}}  \chi_{[a_n, a_n + \epsilon/2^n)} d\mu = \sum_{n=1}^\infty \frac{\epsilon}{2^n} = \epsilon","['real-analysis', 'measure-theory', 'lebesgue-integral']"
66,Separation of closed sets with distance $>0$ by function $f \in C_b^k(\mathbb{R}^d)$,Separation of closed sets with distance  by function,>0 f \in C_b^k(\mathbb{R}^d),"I'm interested in the following problem on the separation of closed sets: Let $A,B \subseteq \mathbb{R}^d$ be closed sets such that $$d(A,B) := \inf\{|x-y|; x \in A, y \in B\}>0.$$ Question: Does there exist a function $f \in C_b^k(\mathbb{R}^d)$ such that $$f^{-1}(\{0\}) = A \quad \text{and} \quad f^{-1}(\{1\})=B \tag{1}$$ ...? Here, $C_b^k(\mathbb{R}^d)$ denotes the space of functions $f: \mathbb{R}^d \to \mathbb{R}$ with bounded derivatives up to order $k$ , and $k \in \mathbb{N}$ is some fixed number. It is a classical result that there exists a continuous function $f$ satisfying $(1)$ . With a bit more effort it can be shown that there exists a smooth function $f \in C^{\infty}(\mathbb{R}^d)$ which satisfies $(1)$ ; this is e.g. discussed in this question . I strongly believe that the answer to my question is ""yes"" but I couldn't find any results concerning the boundedness of the derivates of Urysohn function $f$ . The condition $d(A,B)>0$ means intuitively that the function $f$ does not need to be arbitrarily steep, and hence it would be naturally that $f$ can be chosen in such a way that its derivates are bounded.  However, it is not obvious for me how to prove this rigorously; the construction discussed in the question, which I linked above, does not seem to be helpful. I would be happy about references and/or your thoughts on the question.","I'm interested in the following problem on the separation of closed sets: Let be closed sets such that Question: Does there exist a function such that ...? Here, denotes the space of functions with bounded derivatives up to order , and is some fixed number. It is a classical result that there exists a continuous function satisfying . With a bit more effort it can be shown that there exists a smooth function which satisfies ; this is e.g. discussed in this question . I strongly believe that the answer to my question is ""yes"" but I couldn't find any results concerning the boundedness of the derivates of Urysohn function . The condition means intuitively that the function does not need to be arbitrarily steep, and hence it would be naturally that can be chosen in such a way that its derivates are bounded.  However, it is not obvious for me how to prove this rigorously; the construction discussed in the question, which I linked above, does not seem to be helpful. I would be happy about references and/or your thoughts on the question.","A,B \subseteq \mathbb{R}^d d(A,B) := \inf\{|x-y|; x \in A, y \in B\}>0. f \in C_b^k(\mathbb{R}^d) f^{-1}(\{0\}) = A \quad \text{and} \quad f^{-1}(\{1\})=B \tag{1} C_b^k(\mathbb{R}^d) f: \mathbb{R}^d \to \mathbb{R} k k \in \mathbb{N} f (1) f \in C^{\infty}(\mathbb{R}^d) (1) f d(A,B)>0 f f","['real-analysis', 'general-topology']"
67,Is a convex function on an infinite dimensional cube bounded?,Is a convex function on an infinite dimensional cube bounded?,,"Suppose $f : [0, 1]^\alpha \to \mathbb{R}$ is convex. Does it follow that $f$ has a lower bound? I've shown that this holds for finite $α$ . My question is whether it also holds for infinite $α$ . In the finite case, I can prove it by induction. First, we show this for an interval $[0, 1]$ . (For convex $f : [0, 1] \to \mathbb{R}$ , for $x ≥ \frac{1}{2}$ we have $f(x) ≥ 2f(\frac{1}{2}) - f(0)$ , and a similar bound holds for $x ≤ \frac{1}{2}$ .) Next, suppose that $f : [0, 1]^{n + 1} \to \mathbb{R}$ is convex. Then for each $x ∈ [0, 1]$ , let $f_x$ be the function that takes $y ∈ [0, 1]^n$ to $f(x, y_1, …, y_n)$ . Then each function $f_x$ is convex, and thus by the inductive hypothesis it is bounded. Let $b(x)$ be the greatest lower bound of $f_x$ . Then $b : [0, 1] \to \mathbb{R}$ is also convex. (For any $ε > 0$ , there are $y$ and $y'$ such that $f_x(y) < b(x) + \frac{ε}{2}$ and $f_{x'}(y') < b(x') + \frac{ε}{2}$ . So \begin{align} b(λx + (1-λ)x')  & ≤ f(λx + (1-λ)x', y) \\ & ≤ λf(x, y) + (1 - λ)f(x', y') \\ & < λb(x) + (1 - λ)b(x') + ε \end{align} So $b(λx + (1-λ)x') ≤ λb(x) + (1 - λ)b(x')$ .) So by the first part, $b$ also has a lower bound, and this must be a lower bound for $f$ . It seems like a similar argument using transfinite induction might work in general, but I don't know how the limit step would go.","Suppose is convex. Does it follow that has a lower bound? I've shown that this holds for finite . My question is whether it also holds for infinite . In the finite case, I can prove it by induction. First, we show this for an interval . (For convex , for we have , and a similar bound holds for .) Next, suppose that is convex. Then for each , let be the function that takes to . Then each function is convex, and thus by the inductive hypothesis it is bounded. Let be the greatest lower bound of . Then is also convex. (For any , there are and such that and . So So .) So by the first part, also has a lower bound, and this must be a lower bound for . It seems like a similar argument using transfinite induction might work in general, but I don't know how the limit step would go.","f : [0, 1]^\alpha \to \mathbb{R} f α α [0, 1] f : [0, 1] \to \mathbb{R} x ≥ \frac{1}{2} f(x) ≥ 2f(\frac{1}{2}) - f(0) x ≤ \frac{1}{2} f : [0, 1]^{n + 1} \to \mathbb{R} x ∈ [0, 1] f_x y ∈ [0, 1]^n f(x, y_1, …, y_n) f_x b(x) f_x b : [0, 1] \to \mathbb{R} ε > 0 y y' f_x(y) < b(x) + \frac{ε}{2} f_{x'}(y') < b(x') + \frac{ε}{2} \begin{align}
b(λx + (1-λ)x') 
& ≤ f(λx + (1-λ)x', y) \\
& ≤ λf(x, y) + (1 - λ)f(x', y') \\
& < λb(x) + (1 - λ)b(x') + ε
\end{align} b(λx + (1-λ)x') ≤ λb(x) + (1 - λ)b(x') b f","['real-analysis', 'linear-algebra', 'convex-analysis']"
68,Proving any polynomial of odd degree must have at least one real root,Proving any polynomial of odd degree must have at least one real root,,"Here's my attempt: Suppose that $p(x)=a_0 + a_1x + a_2 x^2 + \ldots + a_n x^n$ where $n$ is odd, $a_i$ are constants with $a_n \ne 0$. Assume that $a_n > 0$. Then $$ p(x)=a_0 + a_1x + a_2 x^2 + \ldots + a_n x^n = x^n \left( \frac{a_0}{x^n} + \ldots + a_n \right) $$ Now as $x \to -\infty $, $x^n \to - \infty$ and $\left( \frac{a_0}{x^n} + \ldots + a_n \right) \to a_n > 0$, and so $p(x) \to -\infty $. And similarly as $x\to +\infty $ , $p(x) \to +\infty$. By definition of $\lim_{x \to \pm \infty } p(x) = \pm \infty$, for any $M>0$ there are points $x_1$ and $x_2$ such that $p(x_1) < -M < 0 < M < p(x_2)$ and thus from the intermediate value theorem, there is a point c between $x_1$ and $x_2$ with $p(c)=0$ If $a_n <0$, we can do the same as above. Is my proof correct? And I was wondering how would I write my proof in a single case setting since IVT is applied in two very similar cases ($a_n > 0$ and $a_n <0$). Hints would be enough.","Here's my attempt: Suppose that $p(x)=a_0 + a_1x + a_2 x^2 + \ldots + a_n x^n$ where $n$ is odd, $a_i$ are constants with $a_n \ne 0$. Assume that $a_n > 0$. Then $$ p(x)=a_0 + a_1x + a_2 x^2 + \ldots + a_n x^n = x^n \left( \frac{a_0}{x^n} + \ldots + a_n \right) $$ Now as $x \to -\infty $, $x^n \to - \infty$ and $\left( \frac{a_0}{x^n} + \ldots + a_n \right) \to a_n > 0$, and so $p(x) \to -\infty $. And similarly as $x\to +\infty $ , $p(x) \to +\infty$. By definition of $\lim_{x \to \pm \infty } p(x) = \pm \infty$, for any $M>0$ there are points $x_1$ and $x_2$ such that $p(x_1) < -M < 0 < M < p(x_2)$ and thus from the intermediate value theorem, there is a point c between $x_1$ and $x_2$ with $p(c)=0$ If $a_n <0$, we can do the same as above. Is my proof correct? And I was wondering how would I write my proof in a single case setting since IVT is applied in two very similar cases ($a_n > 0$ and $a_n <0$). Hints would be enough.",,"['real-analysis', 'continuity']"
69,Studying the convergence of the series $\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$,Studying the convergence of the series,\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right),Study the convergence of the series $$\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$$ This is what I came up with $$\lim_{x\to \infty}\frac{\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)} {\sin^2\frac1{n}}= 1 $$  This implies that $$\sin\frac1{n}\log\left(1+\sin\frac1{n}\right) \sim {\sin^2\frac1{n}}$$ using the inequality $\sin{x}\lt x$   $\left(0\le x \lt \pi\right)$ $${\sin^2\frac1{n}} \lt \frac1{n^2}$$ Since  $\sum_{n=1}^\infty\frac1{n^2}$   converges so does $\sum_{n=1}^\infty\sin^2\frac1{n}$ this implies the convergence of $$\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$$ Is this right?,Study the convergence of the series $$\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$$ This is what I came up with $$\lim_{x\to \infty}\frac{\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)} {\sin^2\frac1{n}}= 1 $$  This implies that $$\sin\frac1{n}\log\left(1+\sin\frac1{n}\right) \sim {\sin^2\frac1{n}}$$ using the inequality $\sin{x}\lt x$   $\left(0\le x \lt \pi\right)$ $${\sin^2\frac1{n}} \lt \frac1{n^2}$$ Since  $\sum_{n=1}^\infty\frac1{n^2}$   converges so does $\sum_{n=1}^\infty\sin^2\frac1{n}$ this implies the convergence of $$\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$$ Is this right?,,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
70,Sets homeomorphic to convex sets,Sets homeomorphic to convex sets,,"I have recently encountered the "" Brouwer fixed-point theorem "". The theorem states (per Wikipedia) that any convex compact set has the fixed point property. This statement struck me as odd as the theorem requires convexity, which isn't a topological property (i.e preserved by homeomorphisms) while the fixed-point property is. My question then: Is there a simple or well known topological property which is equivalent to ""homeomorphic to a convex set"" In the case of $A \subseteq \mathbb{R}$ it's not hard to see that $A$ is homeomorphic to (and is) a convex subset of $\mathbb{R}$ if and only if it is connected. My intuition tells me that in the case of $\mathbb{R}^2$ this extends to simply connected sets, and in higher dimensions to sets where all homotopy groups are trivial, but I don't know if my intuition is right on this. EDIT: thinking on the comments and answers so far it seems another necessary condition is that the set also be locally connected. However that is still not sufficient as any 'Y shaped' subset of $\mathbb{R}^2$ still isn't homeomorphic to a convex set. Would being a simply connected manifold, possibly with boundary, be sufficient in $\mathbb{R}^2$? Please note when answering that I'm mostly familiar with point-set topology and only know very little about algebraic topology.","I have recently encountered the "" Brouwer fixed-point theorem "". The theorem states (per Wikipedia) that any convex compact set has the fixed point property. This statement struck me as odd as the theorem requires convexity, which isn't a topological property (i.e preserved by homeomorphisms) while the fixed-point property is. My question then: Is there a simple or well known topological property which is equivalent to ""homeomorphic to a convex set"" In the case of $A \subseteq \mathbb{R}$ it's not hard to see that $A$ is homeomorphic to (and is) a convex subset of $\mathbb{R}$ if and only if it is connected. My intuition tells me that in the case of $\mathbb{R}^2$ this extends to simply connected sets, and in higher dimensions to sets where all homotopy groups are trivial, but I don't know if my intuition is right on this. EDIT: thinking on the comments and answers so far it seems another necessary condition is that the set also be locally connected. However that is still not sufficient as any 'Y shaped' subset of $\mathbb{R}^2$ still isn't homeomorphic to a convex set. Would being a simply connected manifold, possibly with boundary, be sufficient in $\mathbb{R}^2$? Please note when answering that I'm mostly familiar with point-set topology and only know very little about algebraic topology.",,"['real-analysis', 'general-topology', 'algebraic-topology', 'fixed-point-theorems']"
71,Consider the metric space of infinite sequences of 0s and 1s under this metric.,Consider the metric space of infinite sequences of 0s and 1s under this metric.,,"For $x, y ∈ \{0, 1\}^\mathbb{N}$, define $d(x, y) = 2^{-n}$ where $n$ is the first position where the sequences $x$ and $y$ are different. Show that ($\{0,1\}^\mathbb{N}, d$) is compact. Show that the set P of periodic elements (sequences where there exist $m>0$ such that $x_i = x_{m+i}$ for all $i$) is countable and dense. What I tried For part 1, I have been trying to use the fact that a set is compact if every sequence has a convergent subsequence. But I keep confusing myself with the fact that the elements are sequences and then if I’m supposed to take sequences of those sequences. For part 2, I’m really not sure where to start. With regards to countability, my initial thought is that if an element is periodic then there is a finite number of elements that get repeated. And since there are only 2 possible options for any given entry, we are arranging a finite number of elements in finite ways? But I am not sure about density.","For $x, y ∈ \{0, 1\}^\mathbb{N}$, define $d(x, y) = 2^{-n}$ where $n$ is the first position where the sequences $x$ and $y$ are different. Show that ($\{0,1\}^\mathbb{N}, d$) is compact. Show that the set P of periodic elements (sequences where there exist $m>0$ such that $x_i = x_{m+i}$ for all $i$) is countable and dense. What I tried For part 1, I have been trying to use the fact that a set is compact if every sequence has a convergent subsequence. But I keep confusing myself with the fact that the elements are sequences and then if I’m supposed to take sequences of those sequences. For part 2, I’m really not sure where to start. With regards to countability, my initial thought is that if an element is periodic then there is a finite number of elements that get repeated. And since there are only 2 possible options for any given entry, we are arranging a finite number of elements in finite ways? But I am not sure about density.",,"['real-analysis', 'sequences-and-series', 'metric-spaces', 'compactness']"
72,"Prove $f(x,y)$ is continuous or not continuous.",Prove  is continuous or not continuous.,"f(x,y)","Let $$f(x,y)= \begin{cases}\dfrac{xy^3}{x^3+y^6}& \text{if } x^3+y^6\not =0\\ \\ 0&\text{if } x^3+y^6=0 \end{cases} $$ Is $f$ continuous? If $x=y^2$ then we have: $$\lim_{y\rightarrow 0}\frac{y^2y^3}{y^6+y^6}=\lim_{y\rightarrow 0}\frac{y^5}{2y^6}=\frac{1}{2}\lim_{y\rightarrow0} \frac{1}{y}$$ And that limit doesn't exist. Then $f$ is not continuous. I'm not sure of the process, can someone review my proof?","Let $$f(x,y)= \begin{cases}\dfrac{xy^3}{x^3+y^6}& \text{if } x^3+y^6\not =0\\ \\ 0&\text{if } x^3+y^6=0 \end{cases} $$ Is $f$ continuous? If $x=y^2$ then we have: $$\lim_{y\rightarrow 0}\frac{y^2y^3}{y^6+y^6}=\lim_{y\rightarrow 0}\frac{y^5}{2y^6}=\frac{1}{2}\lim_{y\rightarrow0} \frac{1}{y}$$ And that limit doesn't exist. Then $f$ is not continuous. I'm not sure of the process, can someone review my proof?",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
73,"Show that $\lim_{k\to\infty} \int_0^1 f(x) \sin(kx) \, dx = 0$",Show that,"\lim_{k\to\infty} \int_0^1 f(x) \sin(kx) \, dx = 0","I'm working through some old test problems in preparation for a Real Analysis exam. I got stuck on the following problem: Show that if $f$ is (Lebesgue) integrable on $[0,1]$, then $$\lim_{k\to\infty} \int_0^1 f(x) \sin(kx) \, dx = 0$$ It seems to me that the idea is to move the limit inside the integral. I know that $f(x)\sin(kx)$ is bounded by $f(x)$ which is integrable, so I thought maybe the bounded or dominated convergence theorems might be useful. However, I don't have a sequence of integrable functions converging point-wise to $0$. How can I proceed, or am I approaching this incorrectly?","I'm working through some old test problems in preparation for a Real Analysis exam. I got stuck on the following problem: Show that if $f$ is (Lebesgue) integrable on $[0,1]$, then $$\lim_{k\to\infty} \int_0^1 f(x) \sin(kx) \, dx = 0$$ It seems to me that the idea is to move the limit inside the integral. I know that $f(x)\sin(kx)$ is bounded by $f(x)$ which is integrable, so I thought maybe the bounded or dominated convergence theorems might be useful. However, I don't have a sequence of integrable functions converging point-wise to $0$. How can I proceed, or am I approaching this incorrectly?",,"['real-analysis', 'lebesgue-integral']"
74,"Computing $\int\limits_0^\infty x \left \lfloor{\frac1x}\right \rfloor \, dx$",Computing,"\int\limits_0^\infty x \left \lfloor{\frac1x}\right \rfloor \, dx","This is an integral I computed but can't find the result online or on wolfram. So here's a proof sketch, please indulge this sanity check: $$\int_0^\infty x \left \lfloor{\frac1x}\right \rfloor \ dx = \int_0^1 x \left \lfloor{\frac1x}\right \rfloor \ dx$$ $$= \sum_{n=1}^\infty \int_{1/(n+1)}^{1/n} nx \ dx =\sum_{n=1}^\infty\frac n2 \left(\frac{1}{n^2} - \frac{1}{(n+1)^2}\right) $$ $$= \sum_{n=1}^\infty\frac n2 \left(\frac{2n+1}{n^2(n+1)^2}\right)$$ $$= \sum_{n=1}^\infty\frac{1}{(n+1)^2} + \frac12 \sum_{n=1}^\infty \frac{1}{n(n+1)^2}$$ $$= \frac{\pi^2}{6} -1 + \frac12\left(\sum_{n=1}^\infty \frac1n - \frac{1}{n+1} - \frac{1}{(n+1)^2}\right)$$ $$=\frac{\pi^2}{6} -1 + \frac12\left(\sum_{n=1}^\infty \frac1n - \frac{1}{n+1}\right) -\frac12\left(\sum_{n=1}^\infty\frac{1}{(n+1)^2}\right)$$ $$= \left(\frac{\pi^2}{6} -1\right) + \left(\frac12\cdot 1\right) - \frac12\left(\frac{\pi^2}{6} -1\right)$$ $$= \frac{\pi^2}{12}.$$ Basically, I used the Basel sum several times, and the fifth line follows from a partial sum decomposition. The seventh follows from the known result for the Basel sum, as well as the fact that the first series in the 6th line telescopes. I hope this is all correct.","This is an integral I computed but can't find the result online or on wolfram. So here's a proof sketch, please indulge this sanity check: $$\int_0^\infty x \left \lfloor{\frac1x}\right \rfloor \ dx = \int_0^1 x \left \lfloor{\frac1x}\right \rfloor \ dx$$ $$= \sum_{n=1}^\infty \int_{1/(n+1)}^{1/n} nx \ dx =\sum_{n=1}^\infty\frac n2 \left(\frac{1}{n^2} - \frac{1}{(n+1)^2}\right) $$ $$= \sum_{n=1}^\infty\frac n2 \left(\frac{2n+1}{n^2(n+1)^2}\right)$$ $$= \sum_{n=1}^\infty\frac{1}{(n+1)^2} + \frac12 \sum_{n=1}^\infty \frac{1}{n(n+1)^2}$$ $$= \frac{\pi^2}{6} -1 + \frac12\left(\sum_{n=1}^\infty \frac1n - \frac{1}{n+1} - \frac{1}{(n+1)^2}\right)$$ $$=\frac{\pi^2}{6} -1 + \frac12\left(\sum_{n=1}^\infty \frac1n - \frac{1}{n+1}\right) -\frac12\left(\sum_{n=1}^\infty\frac{1}{(n+1)^2}\right)$$ $$= \left(\frac{\pi^2}{6} -1\right) + \left(\frac12\cdot 1\right) - \frac12\left(\frac{\pi^2}{6} -1\right)$$ $$= \frac{\pi^2}{12}.$$ Basically, I used the Basel sum several times, and the fifth line follows from a partial sum decomposition. The seventh follows from the known result for the Basel sum, as well as the fact that the first series in the 6th line telescopes. I hope this is all correct.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'proof-verification']"
75,"On convergence of Bertrand series $\sum\limits_{n=2}^{\infty} \frac{1}{n^{\alpha}\ln^{\beta}(n)}$ where $\alpha, \beta \in \mathbb{R}$",On convergence of Bertrand series  where,"\sum\limits_{n=2}^{\infty} \frac{1}{n^{\alpha}\ln^{\beta}(n)} \alpha, \beta \in \mathbb{R}","Study the convergence of $$\sum_{n=2}^{\infty}  \frac{1}{n^{\alpha}\ln^{\beta}(n)}$$ where $\alpha, \beta \in  \mathbb{R}$ I have proved that: This series diverges when $\alpha \leq 0$ . This series converges when $\alpha > 1, \beta > 0$ This series diverges when $0 < \alpha < 1, \beta > 0$ This series converges when $\alpha = 1, \beta > 1$ Question: What happens when $\alpha > 0$ and $ \beta < 0$ ? There are other questions on MSE which ask about this series, but this question is distinct because I would like an argument which does not rely on the integral test for series convergence, and this question considers all real $\alpha$ and $\beta$ , while other questions ask only about $\alpha, \beta > 0$ , where we can apply Cauchy condensation criterion","Study the convergence of where I have proved that: This series diverges when . This series converges when This series diverges when This series converges when Question: What happens when and ? There are other questions on MSE which ask about this series, but this question is distinct because I would like an argument which does not rely on the integral test for series convergence, and this question considers all real and , while other questions ask only about , where we can apply Cauchy condensation criterion","\sum_{n=2}^{\infty}
 \frac{1}{n^{\alpha}\ln^{\beta}(n)} \alpha, \beta \in
 \mathbb{R} \alpha \leq 0 \alpha > 1, \beta > 0 0 < \alpha < 1, \beta > 0 \alpha = 1, \beta > 1 \alpha > 0  \beta < 0 \alpha \beta \alpha, \beta > 0","['calculus', 'real-analysis']"
76,lebesgue integral and counterexample,lebesgue integral and counterexample,,"Good day, under the definitions: Let the sequence of measurable functions $f_{i}(x)$ be defined and finite almost everywhere on a measurable set $E$. Let $g(x)$ be a measurable function which is finite almost everywhere. If $\lim_{n\rightarrow \infty}m(E\cap (|f_n-g|\geq t))=0$, for all positive numbers $t$, then the sequence is said to converge in measure to the  function $g(x)$. Let $M = \left \{ f(x) \right \}$ be a family of Lebesgue integrable functions defined on a set $E$. If for every $\varepsilon > 0$ there exists a $\delta > 0$ such that the relations $e\subset E$, $me< \delta$ imply $|\int_{e}f(x)dx|< \varepsilon $ for all functions of the family $M$, then the functions of the family $M$ are said to have  equiabsolutely continuous integrals. We have the Vitali theorem: Let a sequence of Lebesgue integrable functions ${f_i(x)}$ converging in measure to $g(x)$, be defined on a measurable set $E$. If the functions of the sequence have equiabsolutely continuous integrals, then $g(x)$ is Lebesgue integrable function and $\lim_{n\rightarrow \infty} \int_{E}f_n(x)dx=\int_{E}g(x)dx$. I need an example such that if the functions of the sequence have not equiabsolutely continuous integrals, the theorem of Vitali does not apply. I I doubt if I can work it with the variants $\int_{e}|f_n(x)|dx, \int_{e}|g(x)|dx$?","Good day, under the definitions: Let the sequence of measurable functions $f_{i}(x)$ be defined and finite almost everywhere on a measurable set $E$. Let $g(x)$ be a measurable function which is finite almost everywhere. If $\lim_{n\rightarrow \infty}m(E\cap (|f_n-g|\geq t))=0$, for all positive numbers $t$, then the sequence is said to converge in measure to the  function $g(x)$. Let $M = \left \{ f(x) \right \}$ be a family of Lebesgue integrable functions defined on a set $E$. If for every $\varepsilon > 0$ there exists a $\delta > 0$ such that the relations $e\subset E$, $me< \delta$ imply $|\int_{e}f(x)dx|< \varepsilon $ for all functions of the family $M$, then the functions of the family $M$ are said to have  equiabsolutely continuous integrals. We have the Vitali theorem: Let a sequence of Lebesgue integrable functions ${f_i(x)}$ converging in measure to $g(x)$, be defined on a measurable set $E$. If the functions of the sequence have equiabsolutely continuous integrals, then $g(x)$ is Lebesgue integrable function and $\lim_{n\rightarrow \infty} \int_{E}f_n(x)dx=\int_{E}g(x)dx$. I need an example such that if the functions of the sequence have not equiabsolutely continuous integrals, the theorem of Vitali does not apply. I I doubt if I can work it with the variants $\int_{e}|f_n(x)|dx, \int_{e}|g(x)|dx$?",,"['real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure', 'examples-counterexamples']"
77,Linear asymptotics for the solution to $\sum_{n=0}^{\infty} \left(n^k -\frac{1}{\alpha}\right) c^{n^k}=0$,Linear asymptotics for the solution to,\sum_{n=0}^{\infty} \left(n^k -\frac{1}{\alpha}\right) c^{n^k}=0,"Consider the function $f_k(c):=\sum_{n=0}^{\infty} c^{n^k}$ where $k\ge 1$ is an integer. This one obviously converges for $\left\lvert c \right\rvert <1.$ In the following we study the solution to the equation $$\sum_{n=0}^{\infty} \left(n^k -\frac{1}{\alpha}\right) c^{n^k}=0.$$ This one always exists as long as $\alpha \in (0,1).$ Numerically, I discovered something that I would like to understand: As $\alpha \rightarrow 0$ we have that $c= 1-\frac{\gamma\alpha}{k}$ for some constant $\gamma.$ So first the solution $c$ seems to depend in a linear way on $\alpha$ for $\alpha$ small and second, the dependence on $k$ also seems to be just $1/k$. I would like to understand these two observations.","Consider the function $f_k(c):=\sum_{n=0}^{\infty} c^{n^k}$ where $k\ge 1$ is an integer. This one obviously converges for $\left\lvert c \right\rvert <1.$ In the following we study the solution to the equation $$\sum_{n=0}^{\infty} \left(n^k -\frac{1}{\alpha}\right) c^{n^k}=0.$$ This one always exists as long as $\alpha \in (0,1).$ Numerically, I discovered something that I would like to understand: As $\alpha \rightarrow 0$ we have that $c= 1-\frac{\gamma\alpha}{k}$ for some constant $\gamma.$ So first the solution $c$ seems to depend in a linear way on $\alpha$ for $\alpha$ small and second, the dependence on $k$ also seems to be just $1/k$. I would like to understand these two observations.",,"['real-analysis', 'linear-algebra']"
78,Proving a set is not Lebesgue measurable,Proving a set is not Lebesgue measurable,,"I am looking for a non measurable set. I found a set I am almost sure it is not measurable, but am unable to prove it. First we define $f:[0,1]\to [0,9]$ by $$f(0.a_1a_2a_3...)=\begin{cases}\lim_{n\to\infty}(1/n\sum_{i=1}^n a_i) &\text{if the limit exists } \\0 &\text{if the limit does not exist}\end{cases}$$ That means $f$ gives the mean value of the decimals, for example $f(1/3)=3$ and $ f(1/7)=4.5$ Remark: if a number has 2 decimal representations ($0.2$ and $0.1999...$) we define $f(x)=0$ to avoid problems. Now we define $$A=\{x\in[0,1]:f(x)=4.5\}$$ I am convinced that both $A$ and $A^c$ have an outer measure of $1$, and thus are not measurable, but I can't prove it. Here is what makes me believe their exterior measure is $1$: For $A$: -$A$ is uncountable (necessary for a non zero measure) -$A$ is dense (necessary for an outer measure of $1$) proof: we can take the first n decimals as we want, and make the limit converge to 4.5 by alternating 4's and 5's (for example to find a number in $A$ between 0.2 and 0.201 we can take 0.20454545...) -We can chose half of the decimals (that means $a_1,a_3,...$) of a number as we wish, and then chose the others to make sure the number is in $A$. This implies $A$ is uncountable, but is much stronger. Proof: simply make $a_{2k}+a_{2k+1}=9$ -If we take a number randomly (that means by choosing $a_1,a_2,...$ randomly), we have a probability of $1$ that this number is in $A$, which means ""almost all the numbers"" are in $A$. For $A^c$: -$A^c$ is uncountable -$A^c$ is dense. Proof: the numbers with a finite decimal representation are dense in $[0,1]$ and not in $A$ -We can chose any non 1 proportion of the decimals as we wish and still make sure the number is not in $A$. For example, we can set all the decimals $a_k$ such that $k\neq 0$ mod n, and change only the decimals $a_k$ such that $k=0$ mod n, and make sure the number is not in $A$. This means you can chose ""almost all the decimals"" as you wish, and $A^c$ must be ""very big"". Proof: set all decimals you are allowed to modify to $0$. If the number is not in $A$, you are done. If the number is in $A$, set all the decimals you can modify to $1$. As this proportion was not $0$, you have increased $f(x)$, thus it is no longer equal to $4.5$, and the number is not in $A$. That are all properties of $A$ and $A^c$ that may be important for the outer measure I was able to find. For approximations of the outer measure, I only have the trivial properties that they are at most $1$, and their sum is at least $1$. Can anyone prove or disprove the fact $A$ is measurable? Or can someone find a better upper/lower bound for the outer measures of $A$ and $A^c$? Thank you for your help, and if you don't understand my notations or claims, feel free to ask, I know it may not all be very clear.","I am looking for a non measurable set. I found a set I am almost sure it is not measurable, but am unable to prove it. First we define $f:[0,1]\to [0,9]$ by $$f(0.a_1a_2a_3...)=\begin{cases}\lim_{n\to\infty}(1/n\sum_{i=1}^n a_i) &\text{if the limit exists } \\0 &\text{if the limit does not exist}\end{cases}$$ That means $f$ gives the mean value of the decimals, for example $f(1/3)=3$ and $ f(1/7)=4.5$ Remark: if a number has 2 decimal representations ($0.2$ and $0.1999...$) we define $f(x)=0$ to avoid problems. Now we define $$A=\{x\in[0,1]:f(x)=4.5\}$$ I am convinced that both $A$ and $A^c$ have an outer measure of $1$, and thus are not measurable, but I can't prove it. Here is what makes me believe their exterior measure is $1$: For $A$: -$A$ is uncountable (necessary for a non zero measure) -$A$ is dense (necessary for an outer measure of $1$) proof: we can take the first n decimals as we want, and make the limit converge to 4.5 by alternating 4's and 5's (for example to find a number in $A$ between 0.2 and 0.201 we can take 0.20454545...) -We can chose half of the decimals (that means $a_1,a_3,...$) of a number as we wish, and then chose the others to make sure the number is in $A$. This implies $A$ is uncountable, but is much stronger. Proof: simply make $a_{2k}+a_{2k+1}=9$ -If we take a number randomly (that means by choosing $a_1,a_2,...$ randomly), we have a probability of $1$ that this number is in $A$, which means ""almost all the numbers"" are in $A$. For $A^c$: -$A^c$ is uncountable -$A^c$ is dense. Proof: the numbers with a finite decimal representation are dense in $[0,1]$ and not in $A$ -We can chose any non 1 proportion of the decimals as we wish and still make sure the number is not in $A$. For example, we can set all the decimals $a_k$ such that $k\neq 0$ mod n, and change only the decimals $a_k$ such that $k=0$ mod n, and make sure the number is not in $A$. This means you can chose ""almost all the decimals"" as you wish, and $A^c$ must be ""very big"". Proof: set all decimals you are allowed to modify to $0$. If the number is not in $A$, you are done. If the number is in $A$, set all the decimals you can modify to $1$. As this proportion was not $0$, you have increased $f(x)$, thus it is no longer equal to $4.5$, and the number is not in $A$. That are all properties of $A$ and $A^c$ that may be important for the outer measure I was able to find. For approximations of the outer measure, I only have the trivial properties that they are at most $1$, and their sum is at least $1$. Can anyone prove or disprove the fact $A$ is measurable? Or can someone find a better upper/lower bound for the outer measures of $A$ and $A^c$? Thank you for your help, and if you don't understand my notations or claims, feel free to ask, I know it may not all be very clear.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
79,Definition of the space of Hölder continuous functions,Definition of the space of Hölder continuous functions,,"I have a question concerning the definition of the space of Hölder continuous functions $C^{k,\alpha}(\Omega)$, where $\Omega\subset\mathbb{R}^n$ is an open set and $k\in\mathbb{N}$ and $\alpha\in (0,1]$. Often the following definition is used: $f\in C^{k,\alpha}(\Omega)$ if and only if $f\in C^{k}(\Omega)$ such that all its derivatives up to order $k$ are bounded functions and the $k-th$ order derivatives are Hölder continuous. I am wondering why only $D^{\beta}f:U\rightarrow \mathbb{R}$ with $\vert \beta \vert=k$ has to be Hölder continuous? Does it maybe follow that $f$ and its derivatives up to order $k-1$ have to be Hölder continuous as well? Otherwise the space of Hölder continuous functions would contain functions which are not Hölder continuous, which sounds very awkward to me. Best wishes","I have a question concerning the definition of the space of Hölder continuous functions $C^{k,\alpha}(\Omega)$, where $\Omega\subset\mathbb{R}^n$ is an open set and $k\in\mathbb{N}$ and $\alpha\in (0,1]$. Often the following definition is used: $f\in C^{k,\alpha}(\Omega)$ if and only if $f\in C^{k}(\Omega)$ such that all its derivatives up to order $k$ are bounded functions and the $k-th$ order derivatives are Hölder continuous. I am wondering why only $D^{\beta}f:U\rightarrow \mathbb{R}$ with $\vert \beta \vert=k$ has to be Hölder continuous? Does it maybe follow that $f$ and its derivatives up to order $k-1$ have to be Hölder continuous as well? Otherwise the space of Hölder continuous functions would contain functions which are not Hölder continuous, which sounds very awkward to me. Best wishes",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'continuity', 'holder-spaces']"
80,Solve $f'(ax+by)=[f(y)-f(x)]/(y-x)$,Solve,f'(ax+by)=[f(y)-f(x)]/(y-x),"Let $a,b$ be two reals. Find all differentiable functions $f$ satisfying: $$f'(ax+by)= \frac{f(y)-f(x)}{y-x}$$ for all $y\neq x$. I have solved the problem but my solution is not as elegant as I would like it to be so I'm hoping someone can post their solution. The answer is: If $a=b=0.5$, the solution set is all quadratics; else, the solution set is all linear functions. A brief outline of my method, for which the key idea is l'Hopistal: First I show that $f$ is of class $C^2$ on $\mathbb{R}$. Then fix $x=x_0$ and take the limit of both sides of the given equation as $y \to x_0$, applying l'Hospital to the right side. We find that $f'(x_0)=f'[(a+b)x_0]$. Thus $f'(x)=f'[(a+b)x]$ for all $x$. Next, keeping $x=x_0$ fixed we differentiate both sides of the original equation w.r.t $y$ and take the limit again as $y \to x_0$, applying l'Hospital once more. You find $2bf''((a+b)x_0)=f''(x_0)$. But since $f''[(a+b)x_0](a+b)=f''(x_0)$, either $f''(x_0)=0$ for all $x_0$ or $a=b$. The first case implies that solutions are polynomials of degree at most $2$. The second case is easily dealt with by setting $a=b$ and $y=-x$ in the original equation.","Let $a,b$ be two reals. Find all differentiable functions $f$ satisfying: $$f'(ax+by)= \frac{f(y)-f(x)}{y-x}$$ for all $y\neq x$. I have solved the problem but my solution is not as elegant as I would like it to be so I'm hoping someone can post their solution. The answer is: If $a=b=0.5$, the solution set is all quadratics; else, the solution set is all linear functions. A brief outline of my method, for which the key idea is l'Hopistal: First I show that $f$ is of class $C^2$ on $\mathbb{R}$. Then fix $x=x_0$ and take the limit of both sides of the given equation as $y \to x_0$, applying l'Hospital to the right side. We find that $f'(x_0)=f'[(a+b)x_0]$. Thus $f'(x)=f'[(a+b)x]$ for all $x$. Next, keeping $x=x_0$ fixed we differentiate both sides of the original equation w.r.t $y$ and take the limit again as $y \to x_0$, applying l'Hospital once more. You find $2bf''((a+b)x_0)=f''(x_0)$. But since $f''[(a+b)x_0](a+b)=f''(x_0)$, either $f''(x_0)=0$ for all $x_0$ or $a=b$. The first case implies that solutions are polynomials of degree at most $2$. The second case is easily dealt with by setting $a=b$ and $y=-x$ in the original equation.",,"['real-analysis', 'ordinary-differential-equations', 'functions']"
81,Smooth Integrable Functions with Integrable Derivatives,Smooth Integrable Functions with Integrable Derivatives,,"Let $f: \mathbb{R}^n \rightarrow \mathbb{C}$ be a smooth function, that is $f \in C^{\infty}(\mathbb{R}^n)$, such that $f$ and all its derivatives belong to the Lebesgue space $L^p(\mathbb{R}^n)$, with $1 \leq p < \infty$. I am trying to prove that f must vanish at infinity, that is it satisfies \begin{equation} \lim_{|x| \rightarrow \infty} |f(x)| = 0. \end{equation} I could only prove the statement for $p=1$ (see the note below). Any help is welcome. Thank you very much for your attention in advance. NOTE (Motivation and Case p=1) First a matter of notation: if $\beta_i$ is a non-negative integer for $i=1,\dots,n$, we call $\beta=(\beta_1,\dots,\beta_n)$ a multi-index, we set $|\beta|=\beta_1+\dots+\beta_n$ as usual, and  \begin{equation} D^{\beta}f = \frac{\partial^{|\beta|} f}{\partial x_{1}^{\beta_1} \dots \partial x_{n}^{\beta_n}}. \end{equation} We also denote by $D_i$ the partial derivative with respect to the i-th coordinate. The relevance of the question comes from the fact that the space of functions \begin{equation} \mathcal{D}_{L^p} = \left \{ f \in C^{\infty}(\mathbb{R}^n) :  D^{\beta} f \in L^p(\mathbb{R}^n) \textrm{ for each multi-index $\beta$} \right \} \end{equation} is relevant in the theory of distributions. This vector space is topologised through the family of semi-norms \begin{equation} || f ||_{p,N} = \max \left \{ || D^{\beta} f ||_p : |\beta| \leq N \right \} \quad \quad (N=0,1,2,\dots), \end{equation} where $|| . ||_p$ denotes the norm of the space $L^p(\mathbb{R}^n)$. It was introduced by Schwartz in Théories des Distributions, Chapter VI, $\S{8}$, where I found the statement I am trying to prove, which is a crucial step in a remarkable embedding theorem. To state it, let us also introduce the space $\mathcal{B_0}$ (in the notation used by Schwartz this space is denoted with the symbol $\dot{\mathcal{B}}$) which is the vector space of all $f \in C^{\infty}(\mathbb{R}^n)$ such that $f$ and all its derivatives vanish at infinity. We topologise $\mathcal{B_0}$ thought the family of semi-norms: \begin{equation} || f ||_{\infty,N} = \max \left \{ || D^{\beta} f ||_{\infty} : |\beta| \leq N \right \} \quad \quad (N=0,1,2,\dots), \end{equation} where $|| . ||_{\infty}$ denotes the norm of the space $L^{\infty}(\mathbb{R}^n)$. Schwartz states that if $1 \leq p < \infty$ then each $f \in \mathcal{D}_{L^p}$ not only is bounded, but it also vanishes at infinity. This clearly implies that for $q \geq p$, we have $\mathcal{D}_{L^p} \subset \mathcal{D}_{L^q} \subset \mathcal{B_0}$. Moreover each inclusion is continuous. Now let us come back to our question. The case $p=1$ can be proved as follows. Let $g \in C^{\infty}(\mathbb{R}^n)$ be a function with compact support such that $g=1$ on the unit ball with center $0$. Fix $r > 0$ and for any $x \in \mathbb{R}^n$ define $g_r(x)=g(x/r)$. Then set $\phi_r=fg_r$. If $d > 0$ is such that $[-d,d]^n$ contains the support of $\phi_r$, then by repeated integration we get for any $x=(x_1,\dots,x_n)$: \begin{equation} \phi_r(x) = \int_{-d}^{x_1} \dots \int_{-d}^{x_n} (T\phi_r)(y) dy = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n} (T\phi_r)(y) dy, \end{equation} where $T=D_1...D_n$ (by the way, note that the last equality just says that $\phi_r$ is the convolution of $T\phi_r$ and Heaviside function $H$ on $\mathbb{R}^n$) . For any $R > 0$ define  \begin{equation} Q(R)= \{ x=(x_1,\dots,x_n) \in \mathbb{R}^n : \min\{x_1,\dots,x_n\} \leq -R \}. \end{equation} We have that for any $x \in Q(R), z \in Q(R)$: \begin{equation} \left| \phi_r(x) - \phi_r(z) \right| \leq \int_{Q(R)} |(T\phi_r)(y)| dy. \end{equation} By using Leibniz formula we get that there exists $C> 0$ such that for any $r \geq 1$ we have \begin{equation} \int_{Q(R)} |(T\phi_r)(y)| dy \leq C M(R), \end{equation} where \begin{equation} M(R) = \max \left \{ \int_{Q(R)} \left| (D^{\beta}f)(x) \right| dx : |\beta| \leq n \right \}. \end{equation} By taking $R$ large enough, we get that for any $\epsilon > 0$, there exists $R > 0$ such that for any $x \in Q(R)$, $y \in Q(R)$, we have $|f(x) - f(y)| < \epsilon$. Since $f \in L^1(\mathbb{R}^n)$, we conclude that for any $\epsilon > 0$ there exists $r > 0$ such that $|f(x)| < \epsilon$ for any $x \in Q(r)$. By applying this result to $f(-x)$, we then get the desired conclusion. A final note. Clearly, the fact that $f$ vanishes at infinity implies that $f$ is bounded. In the case $p=1$, this fact can be directly proved by using the representation above \begin{equation} \phi_r(x) = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n} (T\phi_r)(y) dy. \end{equation} Actually, from this representation and Leibniz formula we get  \begin{equation} ||f||_{\infty} \leq 2^{n} || g ||_{\infty,n} ||f||_{1,n}, \end{equation} so that, if we set $A=2^{n} || g ||_{\infty,n}$, we conclude that \begin{equation} ||f||_{\infty} \leq A ||f||_{1,n}. \end{equation} Clearly the same inequality applies to each derivative of f.  So we conclude that $\mathcal{D}_{L^1} \subset \mathcal{B_0}$ and that the inclusion $\mathcal{D}_{L^1} \hookrightarrow \mathcal{B_0}$ is continuous. Moreover, we also get that for $1 < q < \infty$ we have $\mathcal{D}_{L^1} \subset \mathcal{D}_{L^q}$ and that the inclusion $\mathcal{D}_{L^1} \hookrightarrow \mathcal{D}_{L^q}$ is continuous. We have so proved two particular cases of the general embedding theorem stated by Schwartz.","Let $f: \mathbb{R}^n \rightarrow \mathbb{C}$ be a smooth function, that is $f \in C^{\infty}(\mathbb{R}^n)$, such that $f$ and all its derivatives belong to the Lebesgue space $L^p(\mathbb{R}^n)$, with $1 \leq p < \infty$. I am trying to prove that f must vanish at infinity, that is it satisfies \begin{equation} \lim_{|x| \rightarrow \infty} |f(x)| = 0. \end{equation} I could only prove the statement for $p=1$ (see the note below). Any help is welcome. Thank you very much for your attention in advance. NOTE (Motivation and Case p=1) First a matter of notation: if $\beta_i$ is a non-negative integer for $i=1,\dots,n$, we call $\beta=(\beta_1,\dots,\beta_n)$ a multi-index, we set $|\beta|=\beta_1+\dots+\beta_n$ as usual, and  \begin{equation} D^{\beta}f = \frac{\partial^{|\beta|} f}{\partial x_{1}^{\beta_1} \dots \partial x_{n}^{\beta_n}}. \end{equation} We also denote by $D_i$ the partial derivative with respect to the i-th coordinate. The relevance of the question comes from the fact that the space of functions \begin{equation} \mathcal{D}_{L^p} = \left \{ f \in C^{\infty}(\mathbb{R}^n) :  D^{\beta} f \in L^p(\mathbb{R}^n) \textrm{ for each multi-index $\beta$} \right \} \end{equation} is relevant in the theory of distributions. This vector space is topologised through the family of semi-norms \begin{equation} || f ||_{p,N} = \max \left \{ || D^{\beta} f ||_p : |\beta| \leq N \right \} \quad \quad (N=0,1,2,\dots), \end{equation} where $|| . ||_p$ denotes the norm of the space $L^p(\mathbb{R}^n)$. It was introduced by Schwartz in Théories des Distributions, Chapter VI, $\S{8}$, where I found the statement I am trying to prove, which is a crucial step in a remarkable embedding theorem. To state it, let us also introduce the space $\mathcal{B_0}$ (in the notation used by Schwartz this space is denoted with the symbol $\dot{\mathcal{B}}$) which is the vector space of all $f \in C^{\infty}(\mathbb{R}^n)$ such that $f$ and all its derivatives vanish at infinity. We topologise $\mathcal{B_0}$ thought the family of semi-norms: \begin{equation} || f ||_{\infty,N} = \max \left \{ || D^{\beta} f ||_{\infty} : |\beta| \leq N \right \} \quad \quad (N=0,1,2,\dots), \end{equation} where $|| . ||_{\infty}$ denotes the norm of the space $L^{\infty}(\mathbb{R}^n)$. Schwartz states that if $1 \leq p < \infty$ then each $f \in \mathcal{D}_{L^p}$ not only is bounded, but it also vanishes at infinity. This clearly implies that for $q \geq p$, we have $\mathcal{D}_{L^p} \subset \mathcal{D}_{L^q} \subset \mathcal{B_0}$. Moreover each inclusion is continuous. Now let us come back to our question. The case $p=1$ can be proved as follows. Let $g \in C^{\infty}(\mathbb{R}^n)$ be a function with compact support such that $g=1$ on the unit ball with center $0$. Fix $r > 0$ and for any $x \in \mathbb{R}^n$ define $g_r(x)=g(x/r)$. Then set $\phi_r=fg_r$. If $d > 0$ is such that $[-d,d]^n$ contains the support of $\phi_r$, then by repeated integration we get for any $x=(x_1,\dots,x_n)$: \begin{equation} \phi_r(x) = \int_{-d}^{x_1} \dots \int_{-d}^{x_n} (T\phi_r)(y) dy = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n} (T\phi_r)(y) dy, \end{equation} where $T=D_1...D_n$ (by the way, note that the last equality just says that $\phi_r$ is the convolution of $T\phi_r$ and Heaviside function $H$ on $\mathbb{R}^n$) . For any $R > 0$ define  \begin{equation} Q(R)= \{ x=(x_1,\dots,x_n) \in \mathbb{R}^n : \min\{x_1,\dots,x_n\} \leq -R \}. \end{equation} We have that for any $x \in Q(R), z \in Q(R)$: \begin{equation} \left| \phi_r(x) - \phi_r(z) \right| \leq \int_{Q(R)} |(T\phi_r)(y)| dy. \end{equation} By using Leibniz formula we get that there exists $C> 0$ such that for any $r \geq 1$ we have \begin{equation} \int_{Q(R)} |(T\phi_r)(y)| dy \leq C M(R), \end{equation} where \begin{equation} M(R) = \max \left \{ \int_{Q(R)} \left| (D^{\beta}f)(x) \right| dx : |\beta| \leq n \right \}. \end{equation} By taking $R$ large enough, we get that for any $\epsilon > 0$, there exists $R > 0$ such that for any $x \in Q(R)$, $y \in Q(R)$, we have $|f(x) - f(y)| < \epsilon$. Since $f \in L^1(\mathbb{R}^n)$, we conclude that for any $\epsilon > 0$ there exists $r > 0$ such that $|f(x)| < \epsilon$ for any $x \in Q(r)$. By applying this result to $f(-x)$, we then get the desired conclusion. A final note. Clearly, the fact that $f$ vanishes at infinity implies that $f$ is bounded. In the case $p=1$, this fact can be directly proved by using the representation above \begin{equation} \phi_r(x) = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n} (T\phi_r)(y) dy. \end{equation} Actually, from this representation and Leibniz formula we get  \begin{equation} ||f||_{\infty} \leq 2^{n} || g ||_{\infty,n} ||f||_{1,n}, \end{equation} so that, if we set $A=2^{n} || g ||_{\infty,n}$, we conclude that \begin{equation} ||f||_{\infty} \leq A ||f||_{1,n}. \end{equation} Clearly the same inequality applies to each derivative of f.  So we conclude that $\mathcal{D}_{L^1} \subset \mathcal{B_0}$ and that the inclusion $\mathcal{D}_{L^1} \hookrightarrow \mathcal{B_0}$ is continuous. Moreover, we also get that for $1 < q < \infty$ we have $\mathcal{D}_{L^1} \subset \mathcal{D}_{L^q}$ and that the inclusion $\mathcal{D}_{L^1} \hookrightarrow \mathcal{D}_{L^q}$ is continuous. We have so proved two particular cases of the general embedding theorem stated by Schwartz.",,"['real-analysis', 'functional-analysis', 'distribution-theory']"
82,"If $f$ is differentiable at $c$, then $F'$ is continuous at $c$ where $ F(x)=\int_a^x f(t)\ dt$?","If  is differentiable at , then  is continuous at  where ?",f c F' c  F(x)=\int_a^x f(t)\ dt,"Let $f$ be Riemann integrable on $[a, b]$, let $c\in(a, b)$, and let $\displaystyle F(x)=\int_a^x f(t)\ dt$, $a\le x\le b$. For the following statement, give either a proof or a counterexample: If $f$ is differentiable at $c$, then $F'$ is continuous at $c$. My attempt: My hunch is that the statement is false. Since $f$ is differentiable at $c$, $f$ is continuous at $c$ and so by the first Fundamental Theorem of Calculus, $F'(c)=f(c)$. But $F'$ need not be equal to $f$ at other points. Is this correct? If so, how do I find a counterexample? If not, how do I prove the statement?","Let $f$ be Riemann integrable on $[a, b]$, let $c\in(a, b)$, and let $\displaystyle F(x)=\int_a^x f(t)\ dt$, $a\le x\le b$. For the following statement, give either a proof or a counterexample: If $f$ is differentiable at $c$, then $F'$ is continuous at $c$. My attempt: My hunch is that the statement is false. Since $f$ is differentiable at $c$, $f$ is continuous at $c$ and so by the first Fundamental Theorem of Calculus, $F'(c)=f(c)$. But $F'$ need not be equal to $f$ at other points. Is this correct? If so, how do I find a counterexample? If not, how do I prove the statement?",,['real-analysis']
83,Proving $f(x+a) \geq f(x)$ almost everywhere,Proving  almost everywhere,f(x+a) \geq f(x),"I encountered the following problem in past analysis qualifying exam: Problem. Let $f \in L_{loc}^{1}(\mathbf{R})$ be real valued and assume that for each $n > 0$ , we have $f(x+ \frac{1}{n}) \geq f(x)$ for almost all $x \in \mathbf{R}$ . Show that for each real number $a \geq 0$ we have $f(x+ a) \geq f(x)$ , for almost all $x \in \mathbf{R}$ . It is trivial to check that the statement is true for any nonnegative rational number $a$ but what about irrationals? I don't know how to use the given condition $f \in L_{loc}^{1}(\mathbf{R})$ . Would anyone help me? Thanks in advance!","I encountered the following problem in past analysis qualifying exam: Problem. Let be real valued and assume that for each , we have for almost all . Show that for each real number we have , for almost all . It is trivial to check that the statement is true for any nonnegative rational number but what about irrationals? I don't know how to use the given condition . Would anyone help me? Thanks in advance!",f \in L_{loc}^{1}(\mathbf{R}) n > 0 f(x+ \frac{1}{n}) \geq f(x) x \in \mathbf{R} a \geq 0 f(x+ a) \geq f(x) x \in \mathbf{R} a f \in L_{loc}^{1}(\mathbf{R}),['real-analysis']
84,Functions with rational image of algebraic elements,Functions with rational image of algebraic elements,,"Does there exist a non constant continuous fonction $f:\mathbb{R}\rightarrow\mathbb{R}$ such that for any real algebraic number $x$, $f(x)$ is rational? Thank you for your answers!","Does there exist a non constant continuous fonction $f:\mathbb{R}\rightarrow\mathbb{R}$ such that for any real algebraic number $x$, $f(x)$ is rational? Thank you for your answers!",,"['real-analysis', 'functions']"
85,"Which properties characterize $\sin, \cos$?",Which properties characterize ?,"\sin, \cos","I know a few properties of $\sin$ and $\cos$, for example: $\sin^2+\cos^2=1$ $\sin (a+b) = \sin a\cos b+\cos a\sin b$. $\cos (a+b) = \cos a\cos b-\sin a\sin b$. $\sin (x+\delta) = \sin x$ for some real $\delta$. $\cos (x+\delta) = \cos x$ for the same real $\delta$. Obviously, the list is incomplete. My question is: Which set of properties characterize $\sin$ and $\cos$ (i.e, if $s,s',c,c'$ satisfy the properties, then $s=s', c=c'$) ? I'm looking for a set of such properties that doesn't reference differentiability, continuity, or the number $\pi$. E: I've given up on the requirement of not assuming continuity (although, if someone does get a proof without assuming it, that person will get a nice bounty $:$-$)$ ), however, I still don't want differentiability or $\pi$ in this characterization. Just to be clear, I'm looking for a proof of something like The continuous solutions to $f^2+g^2=1$, $f(a+b)=f(a)g(b)+g(a)f(b)$ and $g(a+b)=g(a)g(b)-f(a)f(b)$ are unique.","I know a few properties of $\sin$ and $\cos$, for example: $\sin^2+\cos^2=1$ $\sin (a+b) = \sin a\cos b+\cos a\sin b$. $\cos (a+b) = \cos a\cos b-\sin a\sin b$. $\sin (x+\delta) = \sin x$ for some real $\delta$. $\cos (x+\delta) = \cos x$ for the same real $\delta$. Obviously, the list is incomplete. My question is: Which set of properties characterize $\sin$ and $\cos$ (i.e, if $s,s',c,c'$ satisfy the properties, then $s=s', c=c'$) ? I'm looking for a set of such properties that doesn't reference differentiability, continuity, or the number $\pi$. E: I've given up on the requirement of not assuming continuity (although, if someone does get a proof without assuming it, that person will get a nice bounty $:$-$)$ ), however, I still don't want differentiability or $\pi$ in this characterization. Just to be clear, I'm looking for a proof of something like The continuous solutions to $f^2+g^2=1$, $f(a+b)=f(a)g(b)+g(a)f(b)$ and $g(a+b)=g(a)g(b)-f(a)f(b)$ are unique.",,"['real-analysis', 'trigonometry']"
86,Oscillating integral converges to zero?,Oscillating integral converges to zero?,,"An integral like, say,  $$\int_0^1 \cos[ nf(x)]~dx$$ with some function $f$ which is well behaved, and maybe almost everywhere non-zero, should be very small for large $n$ since the positive and negative contributions should about cancel each other whenever the oscillation frequency is high enough. Is there a way to formalise this? For $f(x)=x$, one could compute $$\int_0^1 \cos nx ~dx=\int_0^1 \frac{d}{dx}\left(\frac{\sin nx}{n}\right) dx=\frac{\sin n}{n}$$ but how could one argue when there is no expression for the antiderivative available? For certain cases a substitution $y=f(x)$ and some integration by parts might help, but it seems that the restrictions on $f$ imposed by that are stronger than necessary.","An integral like, say,  $$\int_0^1 \cos[ nf(x)]~dx$$ with some function $f$ which is well behaved, and maybe almost everywhere non-zero, should be very small for large $n$ since the positive and negative contributions should about cancel each other whenever the oscillation frequency is high enough. Is there a way to formalise this? For $f(x)=x$, one could compute $$\int_0^1 \cos nx ~dx=\int_0^1 \frac{d}{dx}\left(\frac{\sin nx}{n}\right) dx=\frac{\sin n}{n}$$ but how could one argue when there is no expression for the antiderivative available? For certain cases a substitution $y=f(x)$ and some integration by parts might help, but it seems that the restrictions on $f$ imposed by that are stronger than necessary.",,"['calculus', 'real-analysis', 'integration', 'limits']"
87,"For a compact metric space $X ,f: X \rightarrow X $ s.t $ d(x,y)\leq d(f(x),f(y))$ is surjective [duplicate]",For a compact metric space  s.t  is surjective [duplicate],"X ,f: X \rightarrow X   d(x,y)\leq d(f(x),f(y))","This question already has an answer here : Let $f:K\to K$ with $\|f(x)-f(y)\|\geq ||x-y||$ for all $x,y$. Show that equality holds and that $f$ is surjective. [duplicate] (1 answer) Closed 8 years ago . I want to show that in a compact metrix space $X$ ,the function $f :X \to X$ such that $d(x,y) \le d(f(x),f(y))$ is surjective! I tried to show that f is continuous and injective but i don't think it really have to...","This question already has an answer here : Let $f:K\to K$ with $\|f(x)-f(y)\|\geq ||x-y||$ for all $x,y$. Show that equality holds and that $f$ is surjective. [duplicate] (1 answer) Closed 8 years ago . I want to show that in a compact metrix space $X$ ,the function $f :X \to X$ such that $d(x,y) \le d(f(x),f(y))$ is surjective! I tried to show that f is continuous and injective but i don't think it really have to...",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
88,Book Recommendation for Linear Algebra using analysis.,Book Recommendation for Linear Algebra using analysis.,,"Today in my Analysis class my teacher proved that (real)symmetric bilinear forms (in an inner-product space) are orthogonally diagonalizable using compactness, differentiablity of innerproducts and other concepts of analysis and then he proceeded to prove that the k'th largest eigenvalue can be derived by  $$\min_{W\subset V,\dim W=k} \max_{x\in W , \|x\|=1} \langle Ax,x\rangle$$ where the bilinear form is $B(x,y)=\langle Ax,y\rangle$. Can anyone suggest me some books which have these kind of ""analysis"" flavoured linear algebra?","Today in my Analysis class my teacher proved that (real)symmetric bilinear forms (in an inner-product space) are orthogonally diagonalizable using compactness, differentiablity of innerproducts and other concepts of analysis and then he proceeded to prove that the k'th largest eigenvalue can be derived by  $$\min_{W\subset V,\dim W=k} \max_{x\in W , \|x\|=1} \langle Ax,x\rangle$$ where the bilinear form is $B(x,y)=\langle Ax,y\rangle$. Can anyone suggest me some books which have these kind of ""analysis"" flavoured linear algebra?",,"['real-analysis', 'linear-algebra', 'reference-request', 'book-recommendation']"
89,Set of discontinuities of a function that has both limits at each point of $R$ [duplicate],Set of discontinuities of a function that has both limits at each point of  [duplicate],R,"This question already has an answer here : Can we construct a function that has uncountable many jump discontinuities? (1 answer) Closed 8 years ago . $f:\mathbb R\rightarrow \mathbb R$ has both a left limit and a right limit at each point of $\mathbb R.$ Then the number of discontinuities of $f$ is what $?$ Now the G reatest I nteger F unction is one such function and has countable infinite discontinuities. Any continuous function  also has both limits at each point and they are same and number of discontinuity is $0.$ So , is there  a function with both limits at each point that has number of discontinuities uncountable $?$ Is that possible $?$","This question already has an answer here : Can we construct a function that has uncountable many jump discontinuities? (1 answer) Closed 8 years ago . $f:\mathbb R\rightarrow \mathbb R$ has both a left limit and a right limit at each point of $\mathbb R.$ Then the number of discontinuities of $f$ is what $?$ Now the G reatest I nteger F unction is one such function and has countable infinite discontinuities. Any continuous function  also has both limits at each point and they are same and number of discontinuity is $0.$ So , is there  a function with both limits at each point that has number of discontinuities uncountable $?$ Is that possible $?$",,"['real-analysis', 'continuity']"
90,Is Rudin correct here? Fubini's theorem and product measures,Is Rudin correct here? Fubini's theorem and product measures,,"Let $X, Y$ be locally compact Hausdorff spaces with nonnegative regular measures $\mu, \lambda$.  By definition (in the book I'm reading) a regular measure is a Borel measure for which every Borel set can be approximated from below by compact sets, or from above by open sets.  In the appendix in Rudin's book Fourier Analysis on Groups , he makes a couple of statements which I have not found anywhere else.  I just wanted to confirm that they are correct.  First there is: There is a unique regular measure $\mu \times \lambda$ on $X \times Y$ for which $\mu \times \lambda(A \times B) = \mu(A) \lambda(B)$ for $A \subseteq X, B \subseteq Y$ Borel. This statement seems fishy to me, because I know product measures are in general not uniquely determined by their values on products of Borel sets.  Some things can happen when $X$ or $Y$ is not $\sigma$-finite.  Next there is: (""Fubini's theorem"") If $f \geq 0$ is a Borel function on $X \times Y$, then $$ \int\limits f d(\mu \times \lambda) = \int\limits_X \int\limits_Y f(x,y)d\lambda(y) d\mu(x) = \int\limits_Y \int\limits_X f(x,y)d \mu(x) d \lambda(y) $$ The reason I am concerned with this is that there is a very similar result stated in Hewitt and Ross, Abstract Harmonic Analysis , Theorem 13.9, which makes the same claim for $f \geq 0$ Borel (although they allow $f$ to take on the value infinity, Rudin clearly does not).  This theorem gives the same result, but requires the addition hypothesis that $f$ vanishes off of a countable union of $(\mu \times \lambda)$-measurable sets which each have finite measure. Would any experts on the subtleties of regular measures be willing to explain why or confirm that Rudin's statements are correct?  One thing I am wondering is whether Rudin is tacitly assuming $X,Y$ and $X \times Y$ have an additional property (which is true of locally compact Hausdorff abelian groups, which is what his book is mainly concerned with), namely that each space is a disjoint union of $\sigma$-compact subspaces $X_{\alpha}$, such that every $\sigma$-compact subset of $X$ is contained in a countable union of the $X_{\alpha}$.","Let $X, Y$ be locally compact Hausdorff spaces with nonnegative regular measures $\mu, \lambda$.  By definition (in the book I'm reading) a regular measure is a Borel measure for which every Borel set can be approximated from below by compact sets, or from above by open sets.  In the appendix in Rudin's book Fourier Analysis on Groups , he makes a couple of statements which I have not found anywhere else.  I just wanted to confirm that they are correct.  First there is: There is a unique regular measure $\mu \times \lambda$ on $X \times Y$ for which $\mu \times \lambda(A \times B) = \mu(A) \lambda(B)$ for $A \subseteq X, B \subseteq Y$ Borel. This statement seems fishy to me, because I know product measures are in general not uniquely determined by their values on products of Borel sets.  Some things can happen when $X$ or $Y$ is not $\sigma$-finite.  Next there is: (""Fubini's theorem"") If $f \geq 0$ is a Borel function on $X \times Y$, then $$ \int\limits f d(\mu \times \lambda) = \int\limits_X \int\limits_Y f(x,y)d\lambda(y) d\mu(x) = \int\limits_Y \int\limits_X f(x,y)d \mu(x) d \lambda(y) $$ The reason I am concerned with this is that there is a very similar result stated in Hewitt and Ross, Abstract Harmonic Analysis , Theorem 13.9, which makes the same claim for $f \geq 0$ Borel (although they allow $f$ to take on the value infinity, Rudin clearly does not).  This theorem gives the same result, but requires the addition hypothesis that $f$ vanishes off of a countable union of $(\mu \times \lambda)$-measurable sets which each have finite measure. Would any experts on the subtleties of regular measures be willing to explain why or confirm that Rudin's statements are correct?  One thing I am wondering is whether Rudin is tacitly assuming $X,Y$ and $X \times Y$ have an additional property (which is true of locally compact Hausdorff abelian groups, which is what his book is mainly concerned with), namely that each space is a disjoint union of $\sigma$-compact subspaces $X_{\alpha}$, such that every $\sigma$-compact subset of $X$ is contained in a countable union of the $X_{\alpha}$.",,"['real-analysis', 'measure-theory', 'reference-request', 'harmonic-analysis']"
91,"Limit of $\int_E f(nx) dx$ for a $1$-periodic function $f$ on $[0,2\pi]$",Limit of  for a -periodic function  on,"\int_E f(nx) dx 1 f [0,2\pi]","Let $E$ be a measurable subset of $[0, 2\pi]$. Assume that $f \in C(\mathbb R)$ is $1$-periodic, i.e. $f(x + 1) = f(x)$. Compute $$\lim_{n\to\infty} \int_{E} f(nx) dx$$. Since $f$ is continuous on $\mathbb R$ so it is continuous in each measurable subset of $[0, 2\pi]$ and so $f(nx)$ is Lebesgue integrable for each $n$. But how can I use $1$ periodicity of $f$?","Let $E$ be a measurable subset of $[0, 2\pi]$. Assume that $f \in C(\mathbb R)$ is $1$-periodic, i.e. $f(x + 1) = f(x)$. Compute $$\lim_{n\to\infty} \int_{E} f(nx) dx$$. Since $f$ is continuous on $\mathbb R$ so it is continuous in each measurable subset of $[0, 2\pi]$ and so $f(nx)$ is Lebesgue integrable for each $n$. But how can I use $1$ periodicity of $f$?",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'periodic-functions']"
92,Criterion for Membership in Hardy Space $H^{1}(\mathbb{R}^{n})$,Criterion for Membership in Hardy Space,H^{1}(\mathbb{R}^{n}),"Let $H^{1}(\mathbb{R}^{n})$ denote the real Hardy space (I am agnostic about the choice of characterization). It is known that if $f:\mathbb{R}^{n}\rightarrow\mathbb{C}$ is a compactly supported function (say in a ball $B$) such that $\int f=0$ and $$\int_{\mathbb{R}^{n}}|f(x)|\log^{+}|f(x)|dx<\infty$$ then $f\in H^{1}(\mathbb{R}^{n})$ and moreover, we have an estimate of the sort $$\|f\|_{H^{1}}\lesssim |B|\|f\|_{L\log L(dx/|B|)}$$ See Lemma 3.10 in these notes for details. One can also show this result by means of a Calderon-Zygmund decomposition for a truncated Hardy-Littlewood maximal function to produce an atomic decomposition for $f$. Suppose we now consider measurable functions $f$, not necessarily compactly supported, such that $\int f=0$ and $$\int_{\mathbb{R}^{n}}\left|f(x)\right|\log\left||2+|f(x)|\right|dx<\infty$$ Is it true that $f\in H^{1}(\mathbb{R}^{n})$? I don't have my intuition for the answer to this question at the moment. A corresponding result for functions $f\in L^{p}(\mathbb{R}^{n})$, where $1<p<\infty$, with $\int f=0$ fails. In one dimension, take $$f(x):=\dfrac{\text{sgn}(x)}{|x|^{(1+\epsilon)/p}}\chi_{(-1,1)^{c}}(x),$$ where $\epsilon>0$ is sufficiently small so that $(1+\epsilon)/p<1$. Since $f\notin L^{1}(\mathbb{R})$, we see that $f\notin H^{1}(\mathbb{R})$. The problem here is that we can have $f\in L^{p}\setminus L^{1}$. However, by considering the factor $\log(2+|f|)$, we have the estimate $$\int|f|\leq(\log|2|)^{-1}\int|f|\log|2+|f||,$$ which addresses this issue.","Let $H^{1}(\mathbb{R}^{n})$ denote the real Hardy space (I am agnostic about the choice of characterization). It is known that if $f:\mathbb{R}^{n}\rightarrow\mathbb{C}$ is a compactly supported function (say in a ball $B$) such that $\int f=0$ and $$\int_{\mathbb{R}^{n}}|f(x)|\log^{+}|f(x)|dx<\infty$$ then $f\in H^{1}(\mathbb{R}^{n})$ and moreover, we have an estimate of the sort $$\|f\|_{H^{1}}\lesssim |B|\|f\|_{L\log L(dx/|B|)}$$ See Lemma 3.10 in these notes for details. One can also show this result by means of a Calderon-Zygmund decomposition for a truncated Hardy-Littlewood maximal function to produce an atomic decomposition for $f$. Suppose we now consider measurable functions $f$, not necessarily compactly supported, such that $\int f=0$ and $$\int_{\mathbb{R}^{n}}\left|f(x)\right|\log\left||2+|f(x)|\right|dx<\infty$$ Is it true that $f\in H^{1}(\mathbb{R}^{n})$? I don't have my intuition for the answer to this question at the moment. A corresponding result for functions $f\in L^{p}(\mathbb{R}^{n})$, where $1<p<\infty$, with $\int f=0$ fails. In one dimension, take $$f(x):=\dfrac{\text{sgn}(x)}{|x|^{(1+\epsilon)/p}}\chi_{(-1,1)^{c}}(x),$$ where $\epsilon>0$ is sufficiently small so that $(1+\epsilon)/p<1$. Since $f\notin L^{1}(\mathbb{R})$, we see that $f\notin H^{1}(\mathbb{R})$. The problem here is that we can have $f\in L^{p}\setminus L^{1}$. However, by considering the factor $\log(2+|f|)$, we have the estimate $$\int|f|\leq(\log|2|)^{-1}\int|f|\log|2+|f||,$$ which addresses this issue.",,"['real-analysis', 'functional-analysis', 'harmonic-analysis', 'hardy-spaces']"
93,"Minimizing the Frobenius norm of a matrix involving the Hadamard product, $\|X(A\odot Y)-S\|_F$","Minimizing the Frobenius norm of a matrix involving the Hadamard product,",\|X(A\odot Y)-S\|_F,"Let $S\in\mathbb{R}^{L\times N}$ and $A\in\mathbb{R}^{M\times N}$ be known and arbitrary. I'd like to solve the following system: \begin{align} \min_{X\in\mathbb{R}^{L\times M},Y\in\mathbb{R}^{M\times N}} \frac{1}{2}\|X(A\odot Y)-S\|_F^2, \end{align} where $\odot$ is the Hadamard product. I've proceeded by defining $Z=X(A\odot Y)-S$ and then rewriting the objective function by $f=\frac{1}{2}Z:Z$, where ($:$) is the Frobenius product. Differentiating, we have \begin{align} \text{d}f&=Z:dZ\\ &=Z:\text{d}[X(A\odot Y)-S]\\ &=Z:[X\text{d}(A\odot Y)+\text{d}X(A\odot Y)-\underbrace{\text{d}S}_{=0}]\\ &=Z:[X(A\odot \text{d}Y+\underbrace{\text{d}A}_{=0}\odot Y)+\text{d}X(A\odot Y)]\\ &=Z:[X(A\odot \text{d}Y)+\text{d}X(A\odot Y)]\\ &=[X(A\odot Y)-S]:[X(A\odot \text{d}Y)+\text{d}X(A\odot Y)]\\ &=[X(A\odot Y)-S]:[X(A\odot \text{d}Y)]+[X(A\odot Y)-S]:[\text{d}X(A\odot Y)]\\ &=[X(A\odot Y)]:[X(A\odot \text{d}Y)]+[X(A\odot Y)]:[\text{d}X(A\odot Y)]\\ &\qquad-S:[X(A\odot \text{d}Y)]-S:[\text{d}X(A\odot Y)] \end{align} This is as far as I have been able to get. I know that I need to find the gradient of $f$ with respect to $X$ and $Y$ and then try to find a pair $(X^*,Y^*)$ that satisfies the two first-order conditions. I'm also aware that such a pair may not be unique (e.g., for any $c\in\mathbb{R}\not\cap\{0\}$, the pair $(cX^*,c^{-1}Y^*)$ is also a solution to this system), but I'm willing to overlook this for now, as I want to first characterize what a solution set would look like. I've done a search, but haven't been able to figure out whether operations of the form $X(A\odot Y)$ simplify nicely, so I'm unsure how to proceed from here.","Let $S\in\mathbb{R}^{L\times N}$ and $A\in\mathbb{R}^{M\times N}$ be known and arbitrary. I'd like to solve the following system: \begin{align} \min_{X\in\mathbb{R}^{L\times M},Y\in\mathbb{R}^{M\times N}} \frac{1}{2}\|X(A\odot Y)-S\|_F^2, \end{align} where $\odot$ is the Hadamard product. I've proceeded by defining $Z=X(A\odot Y)-S$ and then rewriting the objective function by $f=\frac{1}{2}Z:Z$, where ($:$) is the Frobenius product. Differentiating, we have \begin{align} \text{d}f&=Z:dZ\\ &=Z:\text{d}[X(A\odot Y)-S]\\ &=Z:[X\text{d}(A\odot Y)+\text{d}X(A\odot Y)-\underbrace{\text{d}S}_{=0}]\\ &=Z:[X(A\odot \text{d}Y+\underbrace{\text{d}A}_{=0}\odot Y)+\text{d}X(A\odot Y)]\\ &=Z:[X(A\odot \text{d}Y)+\text{d}X(A\odot Y)]\\ &=[X(A\odot Y)-S]:[X(A\odot \text{d}Y)+\text{d}X(A\odot Y)]\\ &=[X(A\odot Y)-S]:[X(A\odot \text{d}Y)]+[X(A\odot Y)-S]:[\text{d}X(A\odot Y)]\\ &=[X(A\odot Y)]:[X(A\odot \text{d}Y)]+[X(A\odot Y)]:[\text{d}X(A\odot Y)]\\ &\qquad-S:[X(A\odot \text{d}Y)]-S:[\text{d}X(A\odot Y)] \end{align} This is as far as I have been able to get. I know that I need to find the gradient of $f$ with respect to $X$ and $Y$ and then try to find a pair $(X^*,Y^*)$ that satisfies the two first-order conditions. I'm also aware that such a pair may not be unique (e.g., for any $c\in\mathbb{R}\not\cap\{0\}$, the pair $(cX^*,c^{-1}Y^*)$ is also a solution to this system), but I'm willing to overlook this for now, as I want to first characterize what a solution set would look like. I've done a search, but haven't been able to figure out whether operations of the form $X(A\odot Y)$ simplify nicely, so I'm unsure how to proceed from here.",,"['real-analysis', 'linear-algebra', 'optimization', 'normed-spaces', 'matrix-calculus']"
94,Prove that the closed unit ball is closed directly.,Prove that the closed unit ball is closed directly.,,"I'm trying to prove the following theorem, and I'm not sure my proof holds. Theorem. Let $(X,d)$ be a metric space, $p\in X$, and $r >0$. Then $$E = \left\{x\in X\ |\ d(x,p)\leq r\right\}$$ is a closed subset of   $X$. Proof. $E$ is closed if and only if it contains all of its limit points. Let $\{x_n\} \subset E$ such that $x_n \to x$. We claim that   $x \in E$. Since $x_n$ converges, for all $\epsilon > 0$, there exists   an $N$, such that for all $n \geq N$, $d(x,x_n) < \epsilon.$ By the triangle inequality $d(p,x) \leq d(p, x_n) + d(x_n, x)$ and so   $d(p,x) < r + \epsilon$ for all $\epsilon > 0$. So, at the least   $d(p,x) \leq r$ , and $x$ is contained in E. The proof is complete. Is my bolded step  logically wrong? Thanks!","I'm trying to prove the following theorem, and I'm not sure my proof holds. Theorem. Let $(X,d)$ be a metric space, $p\in X$, and $r >0$. Then $$E = \left\{x\in X\ |\ d(x,p)\leq r\right\}$$ is a closed subset of   $X$. Proof. $E$ is closed if and only if it contains all of its limit points. Let $\{x_n\} \subset E$ such that $x_n \to x$. We claim that   $x \in E$. Since $x_n$ converges, for all $\epsilon > 0$, there exists   an $N$, such that for all $n \geq N$, $d(x,x_n) < \epsilon.$ By the triangle inequality $d(p,x) \leq d(p, x_n) + d(x_n, x)$ and so   $d(p,x) < r + \epsilon$ for all $\epsilon > 0$. So, at the least   $d(p,x) \leq r$ , and $x$ is contained in E. The proof is complete. Is my bolded step  logically wrong? Thanks!",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
95,A possible dumb question about derivative,A possible dumb question about derivative,,"I was solving some differentiation problems when I found the function $$g(x)=\sqrt{x+\sqrt{x+\sqrt{x}}}.$$ So  I thought: If I define the function $f:\mathbb{R_{x>0}}\to \mathbb{R}$ as $$f(x)=\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+...}}}}$$ What kind of informations have I about the function $f$? Is it continous or differentiable, in some ""sense""? If yes, is it correct to say by implicit differentiation that $$f'(x)=\frac{1}{2f(x)-1}?$$ Thanks so much.","I was solving some differentiation problems when I found the function $$g(x)=\sqrt{x+\sqrt{x+\sqrt{x}}}.$$ So  I thought: If I define the function $f:\mathbb{R_{x>0}}\to \mathbb{R}$ as $$f(x)=\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+...}}}}$$ What kind of informations have I about the function $f$? Is it continous or differentiable, in some ""sense""? If yes, is it correct to say by implicit differentiation that $$f'(x)=\frac{1}{2f(x)-1}?$$ Thanks so much.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
96,Question on Egoroff-like theorem,Question on Egoroff-like theorem,,"Hi all I was tackled by this question from Folland's real analysis second edition in the second chapter, it looks like a modified Egoroff theorem but I cannot really tackle it, it is question 41 of chapter 2 which reads as follows: Let $\mu$ be a $\sigma$-finite  measure and $ f_n \rightarrow f $ a.e. Then there exist measurable sets $E_1,E_2,\ldots \subset X $ such that    $$\mu\left(\left(\bigcup_{i=1}^\infty E_i\right)^C\right)=0$$ and $f_n \rightarrow f $ uniformly on each $ E_i. $ I think this might have something to do with Egoroff's theorem but that theorem mentions nothing about complement having measure zero, only as small as you would like, which is what confuses me. Can anyone point out a proof of this with an explanation?","Hi all I was tackled by this question from Folland's real analysis second edition in the second chapter, it looks like a modified Egoroff theorem but I cannot really tackle it, it is question 41 of chapter 2 which reads as follows: Let $\mu$ be a $\sigma$-finite  measure and $ f_n \rightarrow f $ a.e. Then there exist measurable sets $E_1,E_2,\ldots \subset X $ such that    $$\mu\left(\left(\bigcup_{i=1}^\infty E_i\right)^C\right)=0$$ and $f_n \rightarrow f $ uniformly on each $ E_i. $ I think this might have something to do with Egoroff's theorem but that theorem mentions nothing about complement having measure zero, only as small as you would like, which is what confuses me. Can anyone point out a proof of this with an explanation?",,"['real-analysis', 'measure-theory', 'uniform-convergence']"
97,Extension of measure on sigma-algebra,Extension of measure on sigma-algebra,,"Suppose $\mu:\mathcal{F}\rightarrow[0,\infty)$ be a countable additive measure on a $\sigma$-algebra $\mathcal{F}$ over a set $\Omega$. Take any $E\subseteq \Omega$ . Let $\mathcal{F}_{E}:=\sigma(\mathcal{F}\cup\{E\})$. Then, PROVE there is a countable additive measure $\nu:\mathcal{F}_{E}\rightarrow [0,\infty)$ such that $\nu(A)=\mu(A)$ for any $A\in\mathcal{F}$ . I already know the measure extension theorem. But it is based on algebra and the extension is only about $\sigma(\mathcal{F})$. Can someone give me hint? Or how can I make use of measure extension theorem.","Suppose $\mu:\mathcal{F}\rightarrow[0,\infty)$ be a countable additive measure on a $\sigma$-algebra $\mathcal{F}$ over a set $\Omega$. Take any $E\subseteq \Omega$ . Let $\mathcal{F}_{E}:=\sigma(\mathcal{F}\cup\{E\})$. Then, PROVE there is a countable additive measure $\nu:\mathcal{F}_{E}\rightarrow [0,\infty)$ such that $\nu(A)=\mu(A)$ for any $A\in\mathcal{F}$ . I already know the measure extension theorem. But it is based on algebra and the extension is only about $\sigma(\mathcal{F})$. Can someone give me hint? Or how can I make use of measure extension theorem.",,"['real-analysis', 'measure-theory', 'probability-theory']"
98,How do I follow part of this simple proof that $\lim\limits_{n\to \infty} (a_n b_n) = a b$?,How do I follow part of this simple proof that ?,\lim\limits_{n\to \infty} (a_n b_n) = a b,"I'm working through some analysis textbooks on my own, so I don't want the full answer. I'm only looking for a hint on this problem. In Rosenlicht's Introduction to Analysis , he proves, in a few short steps, that if $\lim\limits_{n\to \infty} a_n = a$ and $\lim\limits_{n\to \infty} b_n = b$, then $\lim\limits_{n\to \infty} (a_n b_n) = a b$. Although I'm able to follow parts of the proof, there's one nexus I'm having trouble understanding. His steps (up until I get stuck) are: Start with the fact that all convergent sequences are bounded to get a number $M > 0$ such that $|a_n| < M$ and $|b_n| < M$. My work: This is saying that the sequences of points $\{a_n\}$ and $\{b_n\}$ are contained in the open ball centered at 0 with radius $M$. I understand this because if I pick any $\epsilon > 0$, the fact that the sequence $\{a_n\}$ converges to $a$ means that $\exists N \in \mathbb{Z}^+$ such that $|a - a_n|< \epsilon$ whenever $n > N$. Then $\{a_n\}$ is contained in the closed ball of radius $r_a = \max\{\epsilon, |a - a_1|, |a - a_2|, \dots, |a - a_N|\}$ centered at $a$. The same holds true for $\{b_n\}$ and $b$, which gives me some $r_b$ for any $\epsilon > 0$. Then, I can pick $r = \max\{d(0, a)+r_a, \ d(0, b)+r_b\} = \max\{|a|+r_a, \ |b|+r_b\}$ so all points in $\{a_n\}$ and $\{b_n\}$ the closed ball centered at 0 with radius $r \geq 0$. I can then pick some real number $M > r$, which completes this step of the proof. Since a closed ball is a closed set, the preceding theorem implies $|a|, |b| \leq M$. My work: This is the step I don't understand. The preceding theorem states that if $S$ is a subset of the metric space $E$, then $S$ is closed if and only iff, whenever $p_1, p_2, \dots, $ is a sequence of points of $S$ that is convergent to $p$ in $E$, we have $\lim\limits_{n\to \infty} p_n = p \in S$. I think this is how this step follows, but I'm not sure. Since the points in $\{a_n\}$ are contained in the closed ball $B_a$ centered at 0 with radius $r^*_a = d(0, a)+r_a = |a| + r_a$, and since an earlier proposition proved that a closed ball is a closed set, we have $a \in B_a$ by the theorem above. Since $a$ is in the closed ball centered at 0 with radius $r^*_a$ and by construction, $M > r^*_a$, we know that $a$ is also in the closed ball centered at 0 with radius $M$, so $|a| \leq M$. The same holds true for $|b|$. Is my logic correct?","I'm working through some analysis textbooks on my own, so I don't want the full answer. I'm only looking for a hint on this problem. In Rosenlicht's Introduction to Analysis , he proves, in a few short steps, that if $\lim\limits_{n\to \infty} a_n = a$ and $\lim\limits_{n\to \infty} b_n = b$, then $\lim\limits_{n\to \infty} (a_n b_n) = a b$. Although I'm able to follow parts of the proof, there's one nexus I'm having trouble understanding. His steps (up until I get stuck) are: Start with the fact that all convergent sequences are bounded to get a number $M > 0$ such that $|a_n| < M$ and $|b_n| < M$. My work: This is saying that the sequences of points $\{a_n\}$ and $\{b_n\}$ are contained in the open ball centered at 0 with radius $M$. I understand this because if I pick any $\epsilon > 0$, the fact that the sequence $\{a_n\}$ converges to $a$ means that $\exists N \in \mathbb{Z}^+$ such that $|a - a_n|< \epsilon$ whenever $n > N$. Then $\{a_n\}$ is contained in the closed ball of radius $r_a = \max\{\epsilon, |a - a_1|, |a - a_2|, \dots, |a - a_N|\}$ centered at $a$. The same holds true for $\{b_n\}$ and $b$, which gives me some $r_b$ for any $\epsilon > 0$. Then, I can pick $r = \max\{d(0, a)+r_a, \ d(0, b)+r_b\} = \max\{|a|+r_a, \ |b|+r_b\}$ so all points in $\{a_n\}$ and $\{b_n\}$ the closed ball centered at 0 with radius $r \geq 0$. I can then pick some real number $M > r$, which completes this step of the proof. Since a closed ball is a closed set, the preceding theorem implies $|a|, |b| \leq M$. My work: This is the step I don't understand. The preceding theorem states that if $S$ is a subset of the metric space $E$, then $S$ is closed if and only iff, whenever $p_1, p_2, \dots, $ is a sequence of points of $S$ that is convergent to $p$ in $E$, we have $\lim\limits_{n\to \infty} p_n = p \in S$. I think this is how this step follows, but I'm not sure. Since the points in $\{a_n\}$ are contained in the closed ball $B_a$ centered at 0 with radius $r^*_a = d(0, a)+r_a = |a| + r_a$, and since an earlier proposition proved that a closed ball is a closed set, we have $a \in B_a$ by the theorem above. Since $a$ is in the closed ball centered at 0 with radius $r^*_a$ and by construction, $M > r^*_a$, we know that $a$ is also in the closed ball centered at 0 with radius $M$, so $|a| \leq M$. The same holds true for $|b|$. Is my logic correct?",,"['real-analysis', 'limits']"
99,"If $f(x+y)=f(x)+f(y)$ and $f$ is monotone, prove that $f(x)=ax$","If  and  is monotone, prove that",f(x+y)=f(x)+f(y) f f(x)=ax,"Suppose $f:\Bbb{R}\to\Bbb{R}$ is a monotone function satisfying $$f(x+y)=f(x)+f(y) \quad \forall \ x,y\in\Bbb{R}$$ Prove that $$\exists a\in \Bbb R,\forall x\in\Bbb{R}, f(x)=ax $$ I proved that $f(x)=ax \quad \forall x\in \Bbb{Q}$ . Assume it is monotonically increasing. Now let $\alpha$ be any irrational number. Then, choose sequences $\{x_n\}$ increasing to $\alpha$ and $\{y_n\}$ decreasing to $\alpha$ . Thus, $$f(x_n)\le f(\alpha)\le f(y_n)$$ i.e. $$ax_n\le f(\alpha)\le ay_n$$ Now, letting $n\to \infty$ ,  we get $f(\alpha)=a\cdot \alpha$ Is my proof correct for irrationals?","Suppose is a monotone function satisfying Prove that I proved that . Assume it is monotonically increasing. Now let be any irrational number. Then, choose sequences increasing to and decreasing to . Thus, i.e. Now, letting ,  we get Is my proof correct for irrationals?","f:\Bbb{R}\to\Bbb{R} f(x+y)=f(x)+f(y) \quad \forall \ x,y\in\Bbb{R} \exists a\in \Bbb R,\forall x\in\Bbb{R}, f(x)=ax  f(x)=ax \quad \forall x\in \Bbb{Q} \alpha \{x_n\} \alpha \{y_n\} \alpha f(x_n)\le f(\alpha)\le f(y_n) ax_n\le f(\alpha)\le ay_n n\to \infty f(\alpha)=a\cdot \alpha","['real-analysis', 'proof-verification']"
