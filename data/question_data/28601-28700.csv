,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the expected area of a polygon whose vertices lie on a circle?,What is the expected area of a polygon whose vertices lie on a circle?,,I came across a nice problem that I would like to share. Problem : What is expected value of the area of an $n$-gon whose vertices lie on a circle of radius $r$? The vertices are uniformly distributed.,I came across a nice problem that I would like to share. Problem : What is expected value of the area of an $n$-gon whose vertices lie on a circle of radius $r$? The vertices are uniformly distributed.,,"['probability', 'euclidean-geometry']"
1,Finding convolution of exponential and uniform distribution- how to set integral limits?,Finding convolution of exponential and uniform distribution- how to set integral limits?,,"I am studying the convolution method for creating the density function of two independent random variables and I am struggling with understanding how the bounds for integrals are created. There is one example of a problem where I don't get the right answer: Let $X$ be an exponential random variable with parameter $\lambda$ and $Y$ be a uniform random variable on $[0,1]$ independent of $X$. Find the probability density function of  $X + Y$: so I have two marginal functions $\mathbf{\\ f_X(x)=\begin{cases} \lambda e^{-\lambda x} & x\geq 0\\  0 & \text{otherwise}  \end{cases}} \ \hspace{20pt} \ \mathbf{f_Y(y)=\begin{cases} 1 & 0\leq x\leq 1\\  0 & \text{otherwise}  \end{cases}}$ I am looking for $Z=X+Y$. My understanding is that two cases should be considered. First for $0\leq z\leq 1$  and second for $z>1$ for $\mathbf{0\leq z\leq 1\\ (f_X*f_Y)(z)=\int_{0}^{z} f_Y(z-y) \ f_X(y) \ dy = \int_0^z \lambda e^{-\lambda y} dy = [-e^{-\lambda y}]_{0}^{z} = 1 - e^{-\lambda z}}$ It seems that I got it correct to this point (in line with the given answer). for  $\mathbf{z>1}\\  \mathbf{(f_X*f_Y)(z)= {\color{Red} \int_1^z f_Y(z-y) \ f_X(y) \ dy = \int_1^z \lambda e^{-\lambda y} \, dy = [-e^{-\lambda y}]_{1}^{z} = 1 - e^{-\lambda z}=e^{-\lambda}(1-e^{-z})}}$ The result I got for $z>1$ is incorrect. I guess it is because I apply incorrect bounds to the integral and that's because I don't quite get the whole concept. I am looking for help with this example and more for general hint how to construct those intervals. The correct answer should be: $\mathbf{f_{X+Y}(z)=\begin{cases} 1-e^{-\lambda z} & 0 \leq  z \leq 1 & \\  e^{-\lambda z}(e^\lambda -1) & z \geq 1 & \\  0 & \text{otherwise} &  \end{cases}}$","I am studying the convolution method for creating the density function of two independent random variables and I am struggling with understanding how the bounds for integrals are created. There is one example of a problem where I don't get the right answer: Let $X$ be an exponential random variable with parameter $\lambda$ and $Y$ be a uniform random variable on $[0,1]$ independent of $X$. Find the probability density function of  $X + Y$: so I have two marginal functions $\mathbf{\\ f_X(x)=\begin{cases} \lambda e^{-\lambda x} & x\geq 0\\  0 & \text{otherwise}  \end{cases}} \ \hspace{20pt} \ \mathbf{f_Y(y)=\begin{cases} 1 & 0\leq x\leq 1\\  0 & \text{otherwise}  \end{cases}}$ I am looking for $Z=X+Y$. My understanding is that two cases should be considered. First for $0\leq z\leq 1$  and second for $z>1$ for $\mathbf{0\leq z\leq 1\\ (f_X*f_Y)(z)=\int_{0}^{z} f_Y(z-y) \ f_X(y) \ dy = \int_0^z \lambda e^{-\lambda y} dy = [-e^{-\lambda y}]_{0}^{z} = 1 - e^{-\lambda z}}$ It seems that I got it correct to this point (in line with the given answer). for  $\mathbf{z>1}\\  \mathbf{(f_X*f_Y)(z)= {\color{Red} \int_1^z f_Y(z-y) \ f_X(y) \ dy = \int_1^z \lambda e^{-\lambda y} \, dy = [-e^{-\lambda y}]_{1}^{z} = 1 - e^{-\lambda z}=e^{-\lambda}(1-e^{-z})}}$ The result I got for $z>1$ is incorrect. I guess it is because I apply incorrect bounds to the integral and that's because I don't quite get the whole concept. I am looking for help with this example and more for general hint how to construct those intervals. The correct answer should be: $\mathbf{f_{X+Y}(z)=\begin{cases} 1-e^{-\lambda z} & 0 \leq  z \leq 1 & \\  e^{-\lambda z}(e^\lambda -1) & z \geq 1 & \\  0 & \text{otherwise} &  \end{cases}}$",,"['probability', 'probability-distributions', 'random-variables', 'convolution']"
2,Rolling a die with n sides to get a cumulative score of n,Rolling a die with n sides to get a cumulative score of n,,"I was told this problem a while ago, and recently someone explained the answer to me, which I didn't understand; could someone please explain in layman's terms (ish)? You have a die with $n$ sides. Each side is numbered - uniquely - from $1$ to $n$, and has an equal probability of landing on top as the other sides (i.e. a fair die). For large $n$ (I was given it with $n = 1,000,000$), on average how many rolls does it take to achieve a cumulative score of $n$ (or greater)? That is, when you roll it, you add the result to your total score, then keep rolling and adding, and you stop when your score exceeds or is equal to $n$. The cool thing about this problem: apparently, the answer is $e$. I would like to know exactly how this is derived.","I was told this problem a while ago, and recently someone explained the answer to me, which I didn't understand; could someone please explain in layman's terms (ish)? You have a die with $n$ sides. Each side is numbered - uniquely - from $1$ to $n$, and has an equal probability of landing on top as the other sides (i.e. a fair die). For large $n$ (I was given it with $n = 1,000,000$), on average how many rolls does it take to achieve a cumulative score of $n$ (or greater)? That is, when you roll it, you add the result to your total score, then keep rolling and adding, and you stop when your score exceeds or is equal to $n$. The cool thing about this problem: apparently, the answer is $e$. I would like to know exactly how this is derived.",,"['probability', 'probability-theory']"
3,Can we qualitatively predict the strategy of the German and US teams in today's World Cup soccer match?,Can we qualitatively predict the strategy of the German and US teams in today's World Cup soccer match?,,"In today's World Cup soccer match between Germany and the US , both teams only need a draw to advance to the next round. There's been speculation about possible collusion, especially given the friendly relationship between the two coaches, but let's assume that both teams will follow their professional ethos and will maximize their own chances of getting to the next round (not of winning this particular game) without trying to make a deal with the other team. This article claims that there's no reason to worry that the game will be boring, and both teams will play to win, until at some point late in the game, if the score is even, they may decide to go for a draw. That makes some intuitive sense; so I was wondering: Can we provide mathematical support for this qualitative prediction about the optimal strategies of the teams? For simplicity, let's assume that the teams advance if they win or draw and don't advance if they lose (which is not quite true, since if they lose the result will depend also on the result of the other game in the group, in different ways for the two teams); and also ignore the incentive to win that arises because that's likely to lead to a weaker opponent in the next round.","In today's World Cup soccer match between Germany and the US , both teams only need a draw to advance to the next round. There's been speculation about possible collusion, especially given the friendly relationship between the two coaches, but let's assume that both teams will follow their professional ethos and will maximize their own chances of getting to the next round (not of winning this particular game) without trying to make a deal with the other team. This article claims that there's no reason to worry that the game will be boring, and both teams will play to win, until at some point late in the game, if the score is even, they may decide to go for a draw. That makes some intuitive sense; so I was wondering: Can we provide mathematical support for this qualitative prediction about the optimal strategies of the teams? For simplicity, let's assume that the teams advance if they win or draw and don't advance if they lose (which is not quite true, since if they lose the result will depend also on the result of the other game in the group, in different ways for the two teams); and also ignore the incentive to win that arises because that's likely to lead to a weaker opponent in the next round.",,"['probability', 'ordinary-differential-equations', 'markov-chains', 'game-theory']"
4,Expected value of maximum consecutive distance in a uniformly random permutation,Expected value of maximum consecutive distance in a uniformly random permutation,,"How does one compute $\mathbb{E}[\max_{1\le i < n} |\sigma(i) - \sigma(i+1)|]$ where the expectation is taken over a uniformly random permutation $\sigma \in \mathbb{P}_n$, the set of all permutations on $[n]$? Is there a recursion for $c^n_t$ that counts the number of permutations $\sigma$ on $[n]$ such that $\max_{1\le i < n} |\sigma(i) - \sigma(i+1)| \le t$?","How does one compute $\mathbb{E}[\max_{1\le i < n} |\sigma(i) - \sigma(i+1)|]$ where the expectation is taken over a uniformly random permutation $\sigma \in \mathbb{P}_n$, the set of all permutations on $[n]$? Is there a recursion for $c^n_t$ that counts the number of permutations $\sigma$ on $[n]$ such that $\max_{1\le i < n} |\sigma(i) - \sigma(i+1)| \le t$?",,"['probability', 'combinatorics']"
5,How interpret convergence in probability?,How interpret convergence in probability?,,"Q1) We say that $X_n\to X$ in probability if $$\forall \varepsilon>0, \lim_{n\to \infty }\mathbb P\{|X_n-X|>\varepsilon\}=0.$$ What does it mean concretely ? What could be the interpretation behind ? Q2) What would be the difference between 1) $$\forall \varepsilon>0, \lim_{n\to \infty }\mathbb P\{|X_n-X|\leq \varepsilon\}=1$$ 2) $$\forall \varepsilon>0, \mathbb P\{\lim_{n\to \infty }|X_n-X|\leq \varepsilon\}=1$$ 3) $$\mathbb P\{\forall \varepsilon>0, \lim_{n\to \infty }|X_n-X|\leq \varepsilon\}=1$$ 4) $$\lim_{n\to \infty }\mathbb P\{\forall \varepsilon>0, |X_n-X|\leq \varepsilon\}=1.$$ I'm not really sure how to interpret all these four limits since they look almost the same for me. I can see that 1) is nothing more than the convergence in probability. If someone could explain me the difference between all these limit, it would help me very much to understand better those concept of convergence.","Q1) We say that $X_n\to X$ in probability if $$\forall \varepsilon>0, \lim_{n\to \infty }\mathbb P\{|X_n-X|>\varepsilon\}=0.$$ What does it mean concretely ? What could be the interpretation behind ? Q2) What would be the difference between 1) $$\forall \varepsilon>0, \lim_{n\to \infty }\mathbb P\{|X_n-X|\leq \varepsilon\}=1$$ 2) $$\forall \varepsilon>0, \mathbb P\{\lim_{n\to \infty }|X_n-X|\leq \varepsilon\}=1$$ 3) $$\mathbb P\{\forall \varepsilon>0, \lim_{n\to \infty }|X_n-X|\leq \varepsilon\}=1$$ 4) $$\lim_{n\to \infty }\mathbb P\{\forall \varepsilon>0, |X_n-X|\leq \varepsilon\}=1.$$ I'm not really sure how to interpret all these four limits since they look almost the same for me. I can see that 1) is nothing more than the convergence in probability. If someone could explain me the difference between all these limit, it would help me very much to understand better those concept of convergence.",,"['probability', 'probability-theory', 'measure-theory']"
6,Brownian motion and covariance,Brownian motion and covariance,,"Show that for $B = (B_t)$ Brownian motion, its covariance is $cov(B_s, B_t) = min(s, t)$. The solution I was given was: For $s ≤ t$, $B_t = B_s + (B_t − B_s)$, $B_sB_t = B_s^2 + Bs(Bt − Bs)$ $cov(B_s,B_t)=E[B_sB_t]$(as $E(B_i)=0)$ so $cov(B_s,B_t)=E[B_s^2]+E[B_s(B_t-B_s)]$, as all increments of Brown motion are independent the second term in the RHS $=E[B_s]E[B_t-B_s]=0*0=0$. Now $cov(B_s,B_t)=E(B_s^2)=Var(B_s)=s$. Similarly if $t\leq s$ we get $=t$. My question is, could I not have done this arguement the same way without assuming that $s\leq t$ in the first place? I mean, if $s\leq t$ why cant I say  $B_s = B_t + (B_s − B_t)$ and conitnue like this to get the answer with a max instead of a min? Is it because $B_t-B_s$ is only an increment if $t\geq s$? I mean otherwise we dont have independency or something?","Show that for $B = (B_t)$ Brownian motion, its covariance is $cov(B_s, B_t) = min(s, t)$. The solution I was given was: For $s ≤ t$, $B_t = B_s + (B_t − B_s)$, $B_sB_t = B_s^2 + Bs(Bt − Bs)$ $cov(B_s,B_t)=E[B_sB_t]$(as $E(B_i)=0)$ so $cov(B_s,B_t)=E[B_s^2]+E[B_s(B_t-B_s)]$, as all increments of Brown motion are independent the second term in the RHS $=E[B_s]E[B_t-B_s]=0*0=0$. Now $cov(B_s,B_t)=E(B_s^2)=Var(B_s)=s$. Similarly if $t\leq s$ we get $=t$. My question is, could I not have done this arguement the same way without assuming that $s\leq t$ in the first place? I mean, if $s\leq t$ why cant I say  $B_s = B_t + (B_s − B_t)$ and conitnue like this to get the answer with a max instead of a min? Is it because $B_t-B_s$ is only an increment if $t\geq s$? I mean otherwise we dont have independency or something?",,"['probability', 'brownian-motion', 'finance']"
7,Probability of getting A to K on single scan of shuffled deck,Probability of getting A to K on single scan of shuffled deck,,"Let us say we have a regular 52-card well-shuffled deck. We scan through the deck (first to last) till we find an Ace. Then we continue (from that Ace) till we find a 2. Then we scan (from the 2) till we find a 3, and so on. We stop when we find a King. (Suits don't matter.) What is the probability that we can complete this process within one scan of the deck? It seems an extremely hard question, and I haven't made any progress. I don't know if this has been answered elsewhere, if so, a link is enough.","Let us say we have a regular 52-card well-shuffled deck. We scan through the deck (first to last) till we find an Ace. Then we continue (from that Ace) till we find a 2. Then we scan (from the 2) till we find a 3, and so on. We stop when we find a King. (Suits don't matter.) What is the probability that we can complete this process within one scan of the deck? It seems an extremely hard question, and I haven't made any progress. I don't know if this has been answered elsewhere, if so, a link is enough.",,"['probability', 'combinatorics', 'probability-theory', 'card-games']"
8,Losing at Spider Solitaire,Losing at Spider Solitaire,,"Spider Solitaire has the property that sometimes none of the cards in the final deal can ""go"" and so you lose, regardless of how much progress you have made beforehand. You would have known that you would lose had you seen the final ten cards before the game started. I wonder if we can calculate the probability of this happening. To be clear, I want to find the probability that the final ten cards out of two packs of well-shuffled cards comprise cards no two of which are exactly one away from each other numerically (only the values matter, not the suits). Note: there are several variants of Spider solitaire. I'm primarily interested in the standard 104-card, four-suit game.","Spider Solitaire has the property that sometimes none of the cards in the final deal can ""go"" and so you lose, regardless of how much progress you have made beforehand. You would have known that you would lose had you seen the final ten cards before the game started. I wonder if we can calculate the probability of this happening. To be clear, I want to find the probability that the final ten cards out of two packs of well-shuffled cards comprise cards no two of which are exactly one away from each other numerically (only the values matter, not the suits). Note: there are several variants of Spider solitaire. I'm primarily interested in the standard 104-card, four-suit game.",,"['probability', 'combinatorics', 'inclusion-exclusion', 'card-games']"
9,A riddle about guessing hat colours (which is not among the commonly known ones),A riddle about guessing hat colours (which is not among the commonly known ones),,"This is a riddle I heard recently, and my question is if someone happens to know the solution. I'm asking this out of curiosity more than anything else. So here it is. The riddle is one of the countless variations of the ""prisoners have to guess their hat colour"" puzzle. $n$ prisoners are put a hat on top of their head, which can be red or blue. The colours are chosen at random by $n$ independent fair coin tosses. Then each prisoner can guess their own hat colour (red or blue) or pass. The prisoners can see each other, but not hear each other's calls and of course they have no other means of communication. This means that each call can only depend on the other prisoners' hat colours. However, before the distributing of hats begins, the prisoners are told the rules and can agree on a strategy. The prisoners win iff no prisoner guesses wrong and at least one prisoner guesses right. Which strategy should the prisoners use so that the winning probability becomes maximal? Some remarks: A simple strategy is that one player just guesses and all other players pass, so that the maximal probabilty is at least 1/2. For $n=2$ this strategy is optimal. For $n=3$, there is a strategy that wins in 6 out of 8 cases: When a player sees (red,red) he guesses blue, for (blue,blue) he guesses red, and otherwise he passes. More generally this shows that the maximal probability is at least 3/4 for $n\ge 3$. It's possible to show that any strategy fails for at least 2 hat colour configurations (unless $n=1$), which shows that the above strategy is optimal for $n=3$. For $n=4$ there are more than $10^{15}$ strategies, and for $n=5$ it's about $10^{38}$ strategies, making it quite infeasible to just use a brute-force computer program (maybe for $n=4$ it's possible when exploiting the obvious symmetries). When changing the rules slightly by forbidding players to pass, then the maximal winning probability is always 1/2. This is a nice little exercise. Actually I heard the riddle only for $n=3$ and then thought about the general $n$. So it's entirely possible that there is no nice solution.","This is a riddle I heard recently, and my question is if someone happens to know the solution. I'm asking this out of curiosity more than anything else. So here it is. The riddle is one of the countless variations of the ""prisoners have to guess their hat colour"" puzzle. $n$ prisoners are put a hat on top of their head, which can be red or blue. The colours are chosen at random by $n$ independent fair coin tosses. Then each prisoner can guess their own hat colour (red or blue) or pass. The prisoners can see each other, but not hear each other's calls and of course they have no other means of communication. This means that each call can only depend on the other prisoners' hat colours. However, before the distributing of hats begins, the prisoners are told the rules and can agree on a strategy. The prisoners win iff no prisoner guesses wrong and at least one prisoner guesses right. Which strategy should the prisoners use so that the winning probability becomes maximal? Some remarks: A simple strategy is that one player just guesses and all other players pass, so that the maximal probabilty is at least 1/2. For $n=2$ this strategy is optimal. For $n=3$, there is a strategy that wins in 6 out of 8 cases: When a player sees (red,red) he guesses blue, for (blue,blue) he guesses red, and otherwise he passes. More generally this shows that the maximal probability is at least 3/4 for $n\ge 3$. It's possible to show that any strategy fails for at least 2 hat colour configurations (unless $n=1$), which shows that the above strategy is optimal for $n=3$. For $n=4$ there are more than $10^{15}$ strategies, and for $n=5$ it's about $10^{38}$ strategies, making it quite infeasible to just use a brute-force computer program (maybe for $n=4$ it's possible when exploiting the obvious symmetries). When changing the rules slightly by forbidding players to pass, then the maximal winning probability is always 1/2. This is a nice little exercise. Actually I heard the riddle only for $n=3$ and then thought about the general $n$. So it's entirely possible that there is no nice solution.",,"['probability', 'puzzle']"
10,"Why Does The ""Bootstrap Method"" Work?","Why Does The ""Bootstrap Method"" Work?",,"Consider the ""Bootstrap Method"" ( https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ) in Probability and Statistics. As I understand, the Bootstrap Method is a useful procedure that can be used to estimate the ""empirical distribution"" of some ""statistic"" (e.g. mean) for some observed data. In the Bootstrap Method: First, we take a random sample (e.g. 70%) of the collected data and calculate the ""statistic"" from this random sample. Next, repeat this above step many times. Each time, you will have a ""version"" of this ""statistic"" corresponding to each random resample. Finally, rank all these ""versions"" from smallest to largest - by taking the ""version"" corresponding to the 5th percentile and the 95th percentile from this ranked list, you can effectively place ""Confidence Intervals"" on this ""statistic"". The Bootstrap Method is said to be particularly advantageous as it allegedly ""works"" in many otherwise difficult circumstances where a closed-form distribution for the ""statistic"" of interest might not be readily known or available. Our professor demonstrated that the Bootstrap Method does in fact work, and showed us some examples with randomly simulated data  where the closed-form distributions for the ""statistic"" of interest is known - and it is easy to compare the solutions generated from the Bootstrap Simulations and the analytical answer. But in the back of my mind, I always play ""Devils Advocate"" and wonder - how do I know that the Bootstrap ""just happens"" to work in this example, and perhaps in the next example, we might not be as lucky. I tried asking one of my professors as to why exactly the Bootstrap Method works - but the professor replied that its because of the Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ). While this is probably true, I was hoping to find a more ""detailed reason"" as to why the Bootstrap Method works. As an example, (my understanding of) the Law of Large Numbers applies in situations where you have access to the entire population and can you can resample this population an infinite number of times - whereas in situations where the Bootstrap is used, you have a (possibly imperfect) sample from the original population, and can only resample this sample of the population. This makes me a bit unsure if extending the use of the Law of Large Numbers to justify the correctness of the Bootstrap is legitimate. I found what seems to be a very informative University Lecture on this subject ( https://www.stat.cmu.edu/~larry/=sml/Boot.pdf ) in which proofs are even provided - but I don't think my knowledge of mathematics is currently adequate enough to understand this proof by myself. I was hoping that perhaps someone here might be able to walk me through a simplified version of this proof - or perhaps provide another simplified version of a similar proof which demonstrates why the Bootstrap Method ""works"". Thanks!","Consider the ""Bootstrap Method"" ( https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ) in Probability and Statistics. As I understand, the Bootstrap Method is a useful procedure that can be used to estimate the ""empirical distribution"" of some ""statistic"" (e.g. mean) for some observed data. In the Bootstrap Method: First, we take a random sample (e.g. 70%) of the collected data and calculate the ""statistic"" from this random sample. Next, repeat this above step many times. Each time, you will have a ""version"" of this ""statistic"" corresponding to each random resample. Finally, rank all these ""versions"" from smallest to largest - by taking the ""version"" corresponding to the 5th percentile and the 95th percentile from this ranked list, you can effectively place ""Confidence Intervals"" on this ""statistic"". The Bootstrap Method is said to be particularly advantageous as it allegedly ""works"" in many otherwise difficult circumstances where a closed-form distribution for the ""statistic"" of interest might not be readily known or available. Our professor demonstrated that the Bootstrap Method does in fact work, and showed us some examples with randomly simulated data  where the closed-form distributions for the ""statistic"" of interest is known - and it is easy to compare the solutions generated from the Bootstrap Simulations and the analytical answer. But in the back of my mind, I always play ""Devils Advocate"" and wonder - how do I know that the Bootstrap ""just happens"" to work in this example, and perhaps in the next example, we might not be as lucky. I tried asking one of my professors as to why exactly the Bootstrap Method works - but the professor replied that its because of the Law of Large Numbers ( https://en.wikipedia.org/wiki/Law_of_large_numbers ). While this is probably true, I was hoping to find a more ""detailed reason"" as to why the Bootstrap Method works. As an example, (my understanding of) the Law of Large Numbers applies in situations where you have access to the entire population and can you can resample this population an infinite number of times - whereas in situations where the Bootstrap is used, you have a (possibly imperfect) sample from the original population, and can only resample this sample of the population. This makes me a bit unsure if extending the use of the Law of Large Numbers to justify the correctness of the Bootstrap is legitimate. I found what seems to be a very informative University Lecture on this subject ( https://www.stat.cmu.edu/~larry/=sml/Boot.pdf ) in which proofs are even provided - but I don't think my knowledge of mathematics is currently adequate enough to understand this proof by myself. I was hoping that perhaps someone here might be able to walk me through a simplified version of this proof - or perhaps provide another simplified version of a similar proof which demonstrates why the Bootstrap Method ""works"". Thanks!",,"['probability', 'statistics']"
11,3 person bet based on the perceived likelihoods of an outcome,3 person bet based on the perceived likelihoods of an outcome,,"Suppose 3 friends want to bet \$100 on whether candidate John Doe will win the next election. They state their perceived likelihood that the event will occur: Alice believes John Doe will win with probability $35\%$ Bob believes John Doe will win with probability $50\%$ Charlie believes John Doe will win with probability $40\%$ How would you go about setting up this bet? In other words, what amount should each friend put down (all three amounts totalling \$100) and what should the payoffs be if John Doe is elected, and what should the payoffs be if John Doe loses? (Asking for a friend)","Suppose 3 friends want to bet \$100 on whether candidate John Doe will win the next election. They state their perceived likelihood that the event will occur: Alice believes John Doe will win with probability $35\%$ Bob believes John Doe will win with probability $50\%$ Charlie believes John Doe will win with probability $40\%$ How would you go about setting up this bet? In other words, what amount should each friend put down (all three amounts totalling \$100) and what should the payoffs be if John Doe is elected, and what should the payoffs be if John Doe loses? (Asking for a friend)",,"['probability', 'gambling']"
12,Proof of Pearson's chi squared test,Proof of Pearson's chi squared test,,"i was reading proof of this theorem on http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/lec23.pdf They showed, that $\frac{v_j-np_j}{\sqrt{np_j}} \stackrel{D}{\longrightarrow} N(0,1-p_j)$. I don't understand however why $\sum_{j=1}^r \frac{(v_j-np_j)^2}{np_j} \stackrel{D}{\longrightarrow} \sum_{i=1}^r Z_i^2$ holds? I know that if $X_n \stackrel{D}{\longrightarrow} X$, then for every continuous function $f$ we have $f(X_n) \stackrel{D}{\longrightarrow} f(X)$, so $\frac{(v_j-np_j)^2}{np_j} \stackrel{D}{\longrightarrow} Z_j^2$. But I know as well, that it's not true that $X_n \stackrel{D}{\longrightarrow} X$ and $Y_n \stackrel{D}{\longrightarrow} Y$ imply $X_n+Y_n \stackrel{D}{\longrightarrow} X+Y$.","i was reading proof of this theorem on http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/lec23.pdf They showed, that $\frac{v_j-np_j}{\sqrt{np_j}} \stackrel{D}{\longrightarrow} N(0,1-p_j)$. I don't understand however why $\sum_{j=1}^r \frac{(v_j-np_j)^2}{np_j} \stackrel{D}{\longrightarrow} \sum_{i=1}^r Z_i^2$ holds? I know that if $X_n \stackrel{D}{\longrightarrow} X$, then for every continuous function $f$ we have $f(X_n) \stackrel{D}{\longrightarrow} f(X)$, so $\frac{(v_j-np_j)^2}{np_j} \stackrel{D}{\longrightarrow} Z_j^2$. But I know as well, that it's not true that $X_n \stackrel{D}{\longrightarrow} X$ and $Y_n \stackrel{D}{\longrightarrow} Y$ imply $X_n+Y_n \stackrel{D}{\longrightarrow} X+Y$.",,"['probability', 'statistics', 'probability-theory', 'weak-convergence']"
13,Hilbert's Barber Shop,Hilbert's Barber Shop,,"Hilbert opens a barber shop with an infinite number of chairs and an infinite number of barbers. Customers arrive via a Poisson random process with an expected 1 person every 10 minutes. Upon arrival, they sit in the first unoccupied chair and their haircut begins immediately. The haircut lasts 15 minutes, after which the person leaves. A long time after the barber shop has opened, what is the probability that the third chair is occupied?","Hilbert opens a barber shop with an infinite number of chairs and an infinite number of barbers. Customers arrive via a Poisson random process with an expected 1 person every 10 minutes. Upon arrival, they sit in the first unoccupied chair and their haircut begins immediately. The haircut lasts 15 minutes, after which the person leaves. A long time after the barber shop has opened, what is the probability that the third chair is occupied?",,"['probability', 'stochastic-processes', 'markov-process']"
14,Probability of opening all piggy banks,Probability of opening all piggy banks,,"Interesting problem I found, and can't solve it since the morning: We have $n$ keys and $n$ piggy banks. Each key fits only one piggy bank. We randomly put exactly one key in each piggy bank. Then we randomly shatter $1\le k\le n$ piggy banks. What is the probability that we are able to open all other piggy banks? Really interested me. I thought about approach with cycles in permutation. When we number all piggy banks and their keys with numbers $1...n$ and then denote $f(i)$ - the number of key that is in $i$-th piggy bank then we have permutation $f$. So to open all piggy banks we need at least one key from each cycle in permutation $f$. But I completely don't know how to count it. Can anybody help?","Interesting problem I found, and can't solve it since the morning: We have $n$ keys and $n$ piggy banks. Each key fits only one piggy bank. We randomly put exactly one key in each piggy bank. Then we randomly shatter $1\le k\le n$ piggy banks. What is the probability that we are able to open all other piggy banks? Really interested me. I thought about approach with cycles in permutation. When we number all piggy banks and their keys with numbers $1...n$ and then denote $f(i)$ - the number of key that is in $i$-th piggy bank then we have permutation $f$. So to open all piggy banks we need at least one key from each cycle in permutation $f$. But I completely don't know how to count it. Can anybody help?",,"['probability', 'combinatorics', 'permutations']"
15,Does this modified random walk (2D) return with probability 1?,Does this modified random walk (2D) return with probability 1?,,"Pólya showed that a random walk (with the directions at each step uniformly distributed) on the integer lattice returns with probability 1. What if instead we consider the random walk where we are not allowed to go backwards? That is we start at (0,0) and facing ""north"", at each step we either turn left, stay straight, or turn right each with Pr = 1/3. For example the possible positions after 1 step are: (-1,0) (facing west) (0,1) (facing north) (1,0) (facing east) The distribution on the next step depends on the last step you took, so you cannot immediately return to the spot you just came from (this is what my friend does when he ""randomly"" walks in Manhattan, it is boring to turn completely around). Thus one possible return sequence would be 4 consecutive left turns. Does this return with probability 1?","Pólya showed that a random walk (with the directions at each step uniformly distributed) on the integer lattice returns with probability 1. What if instead we consider the random walk where we are not allowed to go backwards? That is we start at (0,0) and facing ""north"", at each step we either turn left, stay straight, or turn right each with Pr = 1/3. For example the possible positions after 1 step are: (-1,0) (facing west) (0,1) (facing north) (1,0) (facing east) The distribution on the next step depends on the last step you took, so you cannot immediately return to the spot you just came from (this is what my friend does when he ""randomly"" walks in Manhattan, it is boring to turn completely around). Thus one possible return sequence would be 4 consecutive left turns. Does this return with probability 1?",,"['probability', 'stochastic-processes', 'random-walk']"
16,"The ""find my car"" problem: proper interpretation and solution?","The ""find my car"" problem: proper interpretation and solution?",,"This has been asked at least twice here, but both questions have accepted answers which are wrong. I don't really know any good way to draw attention to the question besides asking again and being a bit more careful about not accepting incorrect or incomplete answers, so here goes. Say I am standing at the origin of the real line, and I know my car is somewhere on the real line. The PDF is a normal curve. I want to search for it optimally walking at a fixed pace. The problem is, if I look right and it really was left, I'm going to, at some point, have to back track to find it. So let's see if we can model this more formally. The set of times is $\mathbb R^+$, so I want a continuous, rigid (so that one ""second"" becomes one ""unit"") transformation $f$ with $f(0) = 0$. A few definitions: Let $S_f(t)$ (success of $f$) be the probability that the car is found by time $t$. It equals $\int_{\min f}^{\max f}N(\mu, \sigma)(x)dx$, Where the $\min$ and $\max$ are taken on $[0,t]$. So it's the amount of area under the normal curve that is covered. For a fixed $x \in \mathbb R$ where the car might be, let $T_f(x)$ be the amount of time it takes to find $c$. This is $\min \{t: f(t) = c\}$. Define $P_f(p)$ for $0 < p < 1$ to be the amount of time it will take to have found the car with probability $p$. That is, $P_f(p) := \min \{t \in \mathbb R^+ : S_f(t) \geq p \}$. I think there are a few valid interpretations of this problem: For a fixed amount of time $t$, find $f$ to maximize $S_f$. I don't think $f$ is independent of $t$, because backtracking takes time, and taking a function that backtracks and telling it, ""you don't have time to backtrack all the way back,"" would tell it not to backtrack at all. Find an $f$ whose expected value of $T_f$ where $x$ is normally distributed is smallest. I think this is closest to the real world problem. For a fixed probability $p$, find $f$ such that $P_f(p)$, which returns an amount of time, is minimal. Let's put these together. Say you're willing to abort once you have reached a $p$ chance of finding the car. This models reality in the Bayesian sense that once I should have found my car with probability $99.99\%$ by now, then maybe my model needs to be revised and I should look on another street. Consider $J_f(x) := \min(T_f(x), P_f(p))$. Now find $f$ such that the expected value of $J_f$ is smallest over normally distributed $x$. So the first question is whether I interpreted this question correctly. The second is how to solve any of them.","This has been asked at least twice here, but both questions have accepted answers which are wrong. I don't really know any good way to draw attention to the question besides asking again and being a bit more careful about not accepting incorrect or incomplete answers, so here goes. Say I am standing at the origin of the real line, and I know my car is somewhere on the real line. The PDF is a normal curve. I want to search for it optimally walking at a fixed pace. The problem is, if I look right and it really was left, I'm going to, at some point, have to back track to find it. So let's see if we can model this more formally. The set of times is $\mathbb R^+$, so I want a continuous, rigid (so that one ""second"" becomes one ""unit"") transformation $f$ with $f(0) = 0$. A few definitions: Let $S_f(t)$ (success of $f$) be the probability that the car is found by time $t$. It equals $\int_{\min f}^{\max f}N(\mu, \sigma)(x)dx$, Where the $\min$ and $\max$ are taken on $[0,t]$. So it's the amount of area under the normal curve that is covered. For a fixed $x \in \mathbb R$ where the car might be, let $T_f(x)$ be the amount of time it takes to find $c$. This is $\min \{t: f(t) = c\}$. Define $P_f(p)$ for $0 < p < 1$ to be the amount of time it will take to have found the car with probability $p$. That is, $P_f(p) := \min \{t \in \mathbb R^+ : S_f(t) \geq p \}$. I think there are a few valid interpretations of this problem: For a fixed amount of time $t$, find $f$ to maximize $S_f$. I don't think $f$ is independent of $t$, because backtracking takes time, and taking a function that backtracks and telling it, ""you don't have time to backtrack all the way back,"" would tell it not to backtrack at all. Find an $f$ whose expected value of $T_f$ where $x$ is normally distributed is smallest. I think this is closest to the real world problem. For a fixed probability $p$, find $f$ such that $P_f(p)$, which returns an amount of time, is minimal. Let's put these together. Say you're willing to abort once you have reached a $p$ chance of finding the car. This models reality in the Bayesian sense that once I should have found my car with probability $99.99\%$ by now, then maybe my model needs to be revised and I should look on another street. Consider $J_f(x) := \min(T_f(x), P_f(p))$. Now find $f$ such that the expected value of $J_f$ is smallest over normally distributed $x$. So the first question is whether I interpreted this question correctly. The second is how to solve any of them.",,"['probability', 'measure-theory', 'searching']"
17,Can a probability density function take negative values?,Can a probability density function take negative values?,,"From a textbook: Theorem 3.5. A function can serve as a probability density of a continuous random variable $X$ if its values, $f(x)$ , satisfy the conditions^ $f(x)\ge0$ for $-\infty <x<\infty$ ; $\int_{-\infty}^\infty f(x)\,dx=1$ . ^The conditions are not ""if and only if"" as in Theorem 3.1 because $f(x)$ could be negative for some value of the random variable without affecting any of the probabilities. However, both conditions of Theorem 3.5 will be satisfied by nearly all the probability densities used in practice and studied in this text. Could someone explain this further? Thanks in advance","From a textbook: Theorem 3.5. A function can serve as a probability density of a continuous random variable if its values, , satisfy the conditions^ for ; . ^The conditions are not ""if and only if"" as in Theorem 3.1 because could be negative for some value of the random variable without affecting any of the probabilities. However, both conditions of Theorem 3.5 will be satisfied by nearly all the probability densities used in practice and studied in this text. Could someone explain this further? Thanks in advance","X f(x) f(x)\ge0 -\infty <x<\infty \int_{-\infty}^\infty f(x)\,dx=1 f(x)",['probability']
18,Birthday Paradox with Leap Year,Birthday Paradox with Leap Year,,"I looked online, and found more than one and inconsistent answers to the Birthday Paradox when we throw the leap year into the mix. None of the answers I saw match with my own. I am posting my solution to see if it is correct, or if I am missing something. Question: Assume that the leap year occurs every four years. (i.e. ignore the 100 and 400 year rule). Also assume that the number of people born each day is the same. What is the probability that in a group of $n$ people (each one selected randomly), no two people share the same birthday ? My Solution: Let $\mathcal{D}$ be the set of all possible dates in an year. (Thus $\mathcal{D}$ contains 366 elements. Note that these possibilities are not equally likely, since a person is four times as likely to be born on (say) Jan 1 than on Feb 29. This is true for any given day other than Feb 29, and it is encoded in the probability assignments given below.) Now probability that a randomly selected person is born on Feb 29 is $\frac{1}{1 + 4 \times 365} = \frac{0.25}{365.25}$ . Also, the probability that a randomly selected person is born on a given day other than Feb 29 is $\frac{1}{365.25}$ . Now, for a group of $n$ randomly selected people, the Sample Space of birthdays is $\mathcal{D}^n$ . Let $\mathcal{A} \subset \mathcal{D}^{n}$ be the subset such that no two people share the same birthday. Divide $\mathcal{A}$ into two disjoint sets $\mathcal{A}_1$ and $\mathcal{A}_2$ such that $\mathcal{A}_1 = \{\xi: \xi \in \mathcal{D}^n \text{ and no two people have same birthday, and none is born on Feb 29} \}$ , and $\mathcal{A}_2 = \{\eta: \eta \in \mathcal{D}^n \text{ and no two people have same birthday, and exactly one is born on Feb 29} \}$ Now, $\mathbb{P}(\xi) = \frac{1}{(365.25)^n}$ for each $\xi \in \mathcal{A}_1$ , and $\mathbb{P}(\eta) = \frac{0.25}{(365.25)^n}$ for each $\eta \in \mathcal{A}_2$ . Also, $|\mathcal{A}_1| = \; ^{365}P_{n}$ , and $|\mathcal{A}_2| = \; n \; \cdot \; ^{365}P_{n-1}$ Finally, $\mathcal{A}_1$ and $\mathcal{A}_2$ being disjoint, it follows that $$\mathbb{P}(\mathcal{A}) = \mathbb{P}(\mathcal{A}_1) + \mathbb{P}(\mathcal{A}_2) = \frac{^{365}P_{n}}{(365.25)^n} + \frac{0.25 \; \cdot \; n \; \cdot \; ^{365}P_{n-1}}{(365.25)^n}$$ PS: As usual, if you want to find out the probability that at least two people share the same birthday then you would calculate $1 - \mathbb{P}(\mathcal{A})$ . Interestingly, the number of people required so that this probability is more than 0.5 is still $n = 23$ , same as the birthday paradox without leap year. Please let me know if the solution above looks accurate.","I looked online, and found more than one and inconsistent answers to the Birthday Paradox when we throw the leap year into the mix. None of the answers I saw match with my own. I am posting my solution to see if it is correct, or if I am missing something. Question: Assume that the leap year occurs every four years. (i.e. ignore the 100 and 400 year rule). Also assume that the number of people born each day is the same. What is the probability that in a group of people (each one selected randomly), no two people share the same birthday ? My Solution: Let be the set of all possible dates in an year. (Thus contains 366 elements. Note that these possibilities are not equally likely, since a person is four times as likely to be born on (say) Jan 1 than on Feb 29. This is true for any given day other than Feb 29, and it is encoded in the probability assignments given below.) Now probability that a randomly selected person is born on Feb 29 is . Also, the probability that a randomly selected person is born on a given day other than Feb 29 is . Now, for a group of randomly selected people, the Sample Space of birthdays is . Let be the subset such that no two people share the same birthday. Divide into two disjoint sets and such that , and Now, for each , and for each . Also, , and Finally, and being disjoint, it follows that PS: As usual, if you want to find out the probability that at least two people share the same birthday then you would calculate . Interestingly, the number of people required so that this probability is more than 0.5 is still , same as the birthday paradox without leap year. Please let me know if the solution above looks accurate.","n \mathcal{D} \mathcal{D} \frac{1}{1 + 4 \times 365} = \frac{0.25}{365.25} \frac{1}{365.25} n \mathcal{D}^n \mathcal{A} \subset \mathcal{D}^{n} \mathcal{A} \mathcal{A}_1 \mathcal{A}_2 \mathcal{A}_1 = \{\xi: \xi \in \mathcal{D}^n \text{ and no two people have same birthday, and none is born on Feb 29} \} \mathcal{A}_2 = \{\eta: \eta \in \mathcal{D}^n \text{ and no two people have same birthday, and exactly one is born on Feb 29} \} \mathbb{P}(\xi) = \frac{1}{(365.25)^n} \xi \in \mathcal{A}_1 \mathbb{P}(\eta) = \frac{0.25}{(365.25)^n} \eta \in \mathcal{A}_2 |\mathcal{A}_1| = \; ^{365}P_{n} |\mathcal{A}_2| = \; n \; \cdot \; ^{365}P_{n-1} \mathcal{A}_1 \mathcal{A}_2 \mathbb{P}(\mathcal{A}) = \mathbb{P}(\mathcal{A}_1) + \mathbb{P}(\mathcal{A}_2) = \frac{^{365}P_{n}}{(365.25)^n} + \frac{0.25 \; \cdot \; n \; \cdot \; ^{365}P_{n-1}}{(365.25)^n} 1 - \mathbb{P}(\mathcal{A}) n = 23","['probability', 'solution-verification', 'birthday']"
19,Is it possible to use physics or other form of non-canonical reasoning to study functions?,Is it possible to use physics or other form of non-canonical reasoning to study functions?,,"It is well-known (see, for example, the books New Horizons in geometry , Maxima and minima without calculus and The Mathematical Mechanic ) that it is possible to use some forms of ""physical reasoning"", ""geometric reasoning"", or ""probabilistic reasoning"" or otherwise non-canonical arguments to find minima and maxima of some functions or to solve some problems that normally require calculus. Are there such methods to calculate limits, find derivatives, or   verify the continuity of a function? Could you provide some examples?","It is well-known (see, for example, the books New Horizons in geometry , Maxima and minima without calculus and The Mathematical Mechanic ) that it is possible to use some forms of ""physical reasoning"", ""geometric reasoning"", or ""probabilistic reasoning"" or otherwise non-canonical arguments to find minima and maxima of some functions or to solve some problems that normally require calculus. Are there such methods to calculate limits, find derivatives, or   verify the continuity of a function? Could you provide some examples?",,"['probability', 'geometry', 'reference-request', 'soft-question', 'physics']"
20,"Does ""independence"" of moments imply independence?","Does ""independence"" of moments imply independence?",,"Suppse you have two random variables $X,Y$ and you are given that for any $m,n$ that: $$E(X^n Y^m) = E(X^n)E(Y^m)$$ Does this imply that $X$ and $Y$ are independent? Are there some condtions on how fast the moments grow can be added to help? Attempt at solution: I know that if the characteristic functions split like $E(e^{i(X,Y)\cdot(s,t)}) = E(e^{iXs})E(e^{iYt})$, (where $.$ is the Schur product) then the RV's are independent. I would try to approximate this by the moments. However, I think you might need some condition that the moments don't grow too fast to make this work.","Suppse you have two random variables $X,Y$ and you are given that for any $m,n$ that: $$E(X^n Y^m) = E(X^n)E(Y^m)$$ Does this imply that $X$ and $Y$ are independent? Are there some condtions on how fast the moments grow can be added to help? Attempt at solution: I know that if the characteristic functions split like $E(e^{i(X,Y)\cdot(s,t)}) = E(e^{iXs})E(e^{iYt})$, (where $.$ is the Schur product) then the RV's are independent. I would try to approximate this by the moments. However, I think you might need some condition that the moments don't grow too fast to make this work.",,"['probability', 'probability-theory', 'characteristic-functions']"
21,A card game with no decisions,A card game with no decisions,,"A friend showed me a mindless card game he plays, in which the initial state of the deck completely determines whether he wins or loses.  The game is played as follows: Shuffle a standard $52$ card deck Lay the top $8$ cards face up If any two cards show the same value, lay on top of them the top two cards from the remaining deck, face up. Repeat step $3$ until no two cards showing have the same value, in which case you lose, or until all cards have been played, in which case you win. What is the probability of a win?  Just as a simple observation, if there are no pairs in the first $8$ cards, the game is lost immediately after step $2$. Of course, we could generalize by varying the number of total cards, the number of different possible values, and the number of cards with a particular value.  We could also change the number of cards we are allowed to lay face up to begin with, and as noted in the comments, we could require that all cards of the same value be covered up at step $3$, and not just pairs.","A friend showed me a mindless card game he plays, in which the initial state of the deck completely determines whether he wins or loses.  The game is played as follows: Shuffle a standard $52$ card deck Lay the top $8$ cards face up If any two cards show the same value, lay on top of them the top two cards from the remaining deck, face up. Repeat step $3$ until no two cards showing have the same value, in which case you lose, or until all cards have been played, in which case you win. What is the probability of a win?  Just as a simple observation, if there are no pairs in the first $8$ cards, the game is lost immediately after step $2$. Of course, we could generalize by varying the number of total cards, the number of different possible values, and the number of cards with a particular value.  We could also change the number of cards we are allowed to lay face up to begin with, and as noted in the comments, we could require that all cards of the same value be covered up at step $3$, and not just pairs.",,"['probability', 'combinatorics', 'recreational-mathematics', 'card-games']"
22,Guessing number of colors of beads in an urn,Guessing number of colors of beads in an urn,,"Motivation from cocktail bar Every time when I order the cocktail “Latex and Prejudice” (“Латекс и предубеждение”) in the Tesla bar in Saint Petersburg (Russia) the barkeeper selects by random a small interesting photo $^1$ and attaches it with a clamp to the cocktail glass. At the beginning I got always different pictures and started to collect them. The more cocktails I ordered the more often the motifs repeated. Finally, I had drunken so much that almost every time I had to ask for a different photo. I was wondering how many different pictures there are but due to drunkenness couldn't solve the problem by myself. Mathematical form using urn model Given is an urn with an unknown number of balls that have an unknown number of colors. It is assumed that every color has equal probability. From this urn in total $n$ balls with $m$ different colors were drawn, $k_i$ balls for color $i$ were sampled, i.e. $n=\sum_{i=1}^m k_i$ . How many different colors $M$ are in the urn? Sampling with and without replacement is of interest. Open questions Some answers were already given. Now I am looking for either alternative answers or/and answers to the following more specific questions: Let's only for the first question assume that we do not know if the balls were drawn with or without replacement. Is the maximum likelihood for drawing with replacement always higher than the maximum likelihood for drawing without replacement? Is this answer helpful? If yes: Can we consider the calculated likelihoods as discrete probability distributions if they were be normalized (with support on integers in the range $(m,\infty)$ )? What can we say about variance, standard error? What can we say about variance, standard error of another answer ? Related problems In this SE post the number of colors in the urn is also unknown but in the problem given here it is assumed that the colors have equal probability. Another SE post deals with lending books from a library that were already lent at an earlier time. Annotation 2023 The bar was visited a lot of times before the war. $\small{^1 \text{Because the site is accessible to minors,$\\$ the content of the photos is not discussed here.}}$","Motivation from cocktail bar Every time when I order the cocktail “Latex and Prejudice” (“Латекс и предубеждение”) in the Tesla bar in Saint Petersburg (Russia) the barkeeper selects by random a small interesting photo and attaches it with a clamp to the cocktail glass. At the beginning I got always different pictures and started to collect them. The more cocktails I ordered the more often the motifs repeated. Finally, I had drunken so much that almost every time I had to ask for a different photo. I was wondering how many different pictures there are but due to drunkenness couldn't solve the problem by myself. Mathematical form using urn model Given is an urn with an unknown number of balls that have an unknown number of colors. It is assumed that every color has equal probability. From this urn in total balls with different colors were drawn, balls for color were sampled, i.e. . How many different colors are in the urn? Sampling with and without replacement is of interest. Open questions Some answers were already given. Now I am looking for either alternative answers or/and answers to the following more specific questions: Let's only for the first question assume that we do not know if the balls were drawn with or without replacement. Is the maximum likelihood for drawing with replacement always higher than the maximum likelihood for drawing without replacement? Is this answer helpful? If yes: Can we consider the calculated likelihoods as discrete probability distributions if they were be normalized (with support on integers in the range )? What can we say about variance, standard error? What can we say about variance, standard error of another answer ? Related problems In this SE post the number of colors in the urn is also unknown but in the problem given here it is assumed that the colors have equal probability. Another SE post deals with lending books from a library that were already lent at an earlier time. Annotation 2023 The bar was visited a lot of times before the war.","^1 n m k_i i n=\sum_{i=1}^m k_i M (m,\infty) \small{^1 \text{Because the site is accessible to minors,\\ the content of the photos is not discussed here.}}","['probability', 'combinatorics', 'statistics', 'recreational-mathematics', 'multinomial-distribution']"
23,Infection in a village,Infection in a village,,"Consider the following problem: Suppose a lonely wanderer infected with a virus came into an isolated village with $M$ villagers and stayed there. Every week each of the infected villagers coughs onto $n$ random other villagers (each of them chosen uniformly and independently among everyone) and then develops antibodies becoming immune to it. All villagers who are coughed upon become infected if they are not immune. Nobody left or entered the village after the arrival of the lonely wanderer. Consider time to be discrete and measured in weeks. We say, that the virus survives as long as someone is infected with it. For what $n$ is the expected time of its survival the longest? The extremum clearly is not achieved in the border cases here. Indeed, if $n = 0$ the lonely wanderer becomes immune before being able to infect anyone else, thus the virus will survive only for $1$ week. If $n \to \infty$ the probability that the lonely wanderer infects everyone in the first week tends to $1$ . Thus the expected time of the survival of the virus tends to $2$ in this case. So, we must look for optimal $n$ somewhere in between. However, I have no idea how to find it (or even its asymptotic for large $M$ )… At the first glance the problem looked to be somewhat similar to two well studied problems: branching processes (villagers infected by a given infected villager - their descendants in terms of branching processes) and coupon collector problem (uninfected villagers as coupons to be collected). However, it is different from both of them (the number of ‘descendants’ changes each turn here, which makes it different from a Galton-Watson branching process, and the number of ‘coupons collected per turn’ depends on the number of ‘coupons collected on the previous turn’, which makes it different from a classical coupon collector) and methods, similar to the ones used to solve them, are unlikely to work here.","Consider the following problem: Suppose a lonely wanderer infected with a virus came into an isolated village with villagers and stayed there. Every week each of the infected villagers coughs onto random other villagers (each of them chosen uniformly and independently among everyone) and then develops antibodies becoming immune to it. All villagers who are coughed upon become infected if they are not immune. Nobody left or entered the village after the arrival of the lonely wanderer. Consider time to be discrete and measured in weeks. We say, that the virus survives as long as someone is infected with it. For what is the expected time of its survival the longest? The extremum clearly is not achieved in the border cases here. Indeed, if the lonely wanderer becomes immune before being able to infect anyone else, thus the virus will survive only for week. If the probability that the lonely wanderer infects everyone in the first week tends to . Thus the expected time of the survival of the virus tends to in this case. So, we must look for optimal somewhere in between. However, I have no idea how to find it (or even its asymptotic for large )… At the first glance the problem looked to be somewhat similar to two well studied problems: branching processes (villagers infected by a given infected villager - their descendants in terms of branching processes) and coupon collector problem (uninfected villagers as coupons to be collected). However, it is different from both of them (the number of ‘descendants’ changes each turn here, which makes it different from a Galton-Watson branching process, and the number of ‘coupons collected per turn’ depends on the number of ‘coupons collected on the previous turn’, which makes it different from a classical coupon collector) and methods, similar to the ones used to solve them, are unlikely to work here.",M n n n = 0 1 n \to \infty 1 2 n M,"['probability', 'optimization', 'stochastic-processes', 'coupon-collector']"
24,Brownian Motion and stochastic integration on the complete real line,Brownian Motion and stochastic integration on the complete real line,,"I'm struggling to understand stochastic integration over intervals containing zero, i.e. integrals of the form $\int_{a}^{b} X_s \, d B_s$ where $-\infty \leq a < b \leq \infty$, $(X_t)_{t \in \mathbb{R}}$ is some adapted process and $(B_t)_{t \in \mathbb{R}}$ is a Brownian motion. Integrals of this type appear over and over again in several articles (of established authors) and the usual martingale techniques (Burkholder-Davis-Gundy, Ito-isometry, etc.) are happily applied as if $a>0$, but always without any justification, so I guess that there's some ""standard way"" these type of integrals are understood. I've searched for literature on this subject but couldn't find anything, any reference on this is highly appreciated! As far as I understand, one can define filtrations, (adapted) stochastic processes and  martingales in the usual way as all these definitions don't depend on the index set. A Browian motion on $\mathbb{R}$ is then simply an almost surely continuous stochastic process with independent increments such that $B_0=0$ and $B_t-B_s \sim \mathcal{N}(0,t-s)$ for $s \leq t$. Apparently, one way to realize it is by constructing a two-sided Brownian motion $B$ by taking two independent Brownian motions $(B_{1,t})_{t \geq 0}$, $(B_2,t)_{t \geq 0}$ and letting them run in opposite directions: $B_t := B_{1,t}$ if $t \geq 0$ and $B_t := B_{2,-t}$ if $t < 0$. Here is my first problem: Is there a filtration (on $\mathbb{R}$) in which the two-sided Brownian Motion is a Brownian motion on $\mathbb{R}$? The two filtrations of the two-sided Brownian motion increase in opposite directions... Now, assuming that the filtration problem can be solved somehow, a naive way of defining a stochastic integral with respect to a two sided Brownian motion $B$ on an interval $(a,b)$ containing zero would be $$\int_{a}^{b} X_s \, d B_s := \int_{a}^0 X_s \, d B_s + \int_0^a X_s d B_s,$$ where  $$ \int_a^0 X_s \, d B_s := - \int_0^{-a} X_{-s} \, d B_2(s),$$  but with this definition I see no way of associating a martingale on $\mathbb{R}$ with this integral. Is there another definition? To give you an example where such a stochastic integral is used, see https://sites.google.com/site/giovannipeccati/Home/Publications-by-G-Peccati/PEC1.pdf On page 7 (Thm. 3.1) a Brownian motion on the real line is introduced and on page 10 in the proof a stochastic integral over an interval containing zero appears.","I'm struggling to understand stochastic integration over intervals containing zero, i.e. integrals of the form $\int_{a}^{b} X_s \, d B_s$ where $-\infty \leq a < b \leq \infty$, $(X_t)_{t \in \mathbb{R}}$ is some adapted process and $(B_t)_{t \in \mathbb{R}}$ is a Brownian motion. Integrals of this type appear over and over again in several articles (of established authors) and the usual martingale techniques (Burkholder-Davis-Gundy, Ito-isometry, etc.) are happily applied as if $a>0$, but always without any justification, so I guess that there's some ""standard way"" these type of integrals are understood. I've searched for literature on this subject but couldn't find anything, any reference on this is highly appreciated! As far as I understand, one can define filtrations, (adapted) stochastic processes and  martingales in the usual way as all these definitions don't depend on the index set. A Browian motion on $\mathbb{R}$ is then simply an almost surely continuous stochastic process with independent increments such that $B_0=0$ and $B_t-B_s \sim \mathcal{N}(0,t-s)$ for $s \leq t$. Apparently, one way to realize it is by constructing a two-sided Brownian motion $B$ by taking two independent Brownian motions $(B_{1,t})_{t \geq 0}$, $(B_2,t)_{t \geq 0}$ and letting them run in opposite directions: $B_t := B_{1,t}$ if $t \geq 0$ and $B_t := B_{2,-t}$ if $t < 0$. Here is my first problem: Is there a filtration (on $\mathbb{R}$) in which the two-sided Brownian Motion is a Brownian motion on $\mathbb{R}$? The two filtrations of the two-sided Brownian motion increase in opposite directions... Now, assuming that the filtration problem can be solved somehow, a naive way of defining a stochastic integral with respect to a two sided Brownian motion $B$ on an interval $(a,b)$ containing zero would be $$\int_{a}^{b} X_s \, d B_s := \int_{a}^0 X_s \, d B_s + \int_0^a X_s d B_s,$$ where  $$ \int_a^0 X_s \, d B_s := - \int_0^{-a} X_{-s} \, d B_2(s),$$  but with this definition I see no way of associating a martingale on $\mathbb{R}$ with this integral. Is there another definition? To give you an example where such a stochastic integral is used, see https://sites.google.com/site/giovannipeccati/Home/Publications-by-G-Peccati/PEC1.pdf On page 7 (Thm. 3.1) a Brownian motion on the real line is introduced and on page 10 in the proof a stochastic integral over an interval containing zero appears.",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-integrals']"
25,"How to estimate the number of articles on Wikipedia using the ""random article"" function?","How to estimate the number of articles on Wikipedia using the ""random article"" function?",,"There is a Wikipedia-type website of a fixed size of $S$ number of articles. You start at any article on Wikipedia. You then start to press the ""random article"" button and count the number of times $N$ that it takes for you to randomly generate the original page again. The goal is to find the best estimate for $S$ given only the number $N$. Assume that the ""random article"" function generates any particular article with a uniform probability and that each click is a (EDIT: independent) random event. The page that you start on does not count as a click and is thus not counted in $N$. This problem reminds me of the solutions to the locomotive problem and the German tank problem . Since the button is clicked N times, the case with the highest number of articles has all N pages being different pages, yielding an estimate of (I think) $2N+1$. This would mean that the best estimate would be less than $2N+1$, because it is always possible for $N$ to contain an article, other than the one you were looking for, to be repeated multiple times. Another possible solution is based off of the method of mark and recapture to estimate the size of animal populations in the wild. This method gives the estimate as $S=N$. This would because out of $N$ samples taken during the ""recapture"", exactly $1/N$ of them are articles viewed during the ""mark"" phase (original article that is remembered). Since $1/N$ of the pages are known to be picked from the group of  one particular page, then the estimate would be that there would be $N$ pages in total.","There is a Wikipedia-type website of a fixed size of $S$ number of articles. You start at any article on Wikipedia. You then start to press the ""random article"" button and count the number of times $N$ that it takes for you to randomly generate the original page again. The goal is to find the best estimate for $S$ given only the number $N$. Assume that the ""random article"" function generates any particular article with a uniform probability and that each click is a (EDIT: independent) random event. The page that you start on does not count as a click and is thus not counted in $N$. This problem reminds me of the solutions to the locomotive problem and the German tank problem . Since the button is clicked N times, the case with the highest number of articles has all N pages being different pages, yielding an estimate of (I think) $2N+1$. This would mean that the best estimate would be less than $2N+1$, because it is always possible for $N$ to contain an article, other than the one you were looking for, to be repeated multiple times. Another possible solution is based off of the method of mark and recapture to estimate the size of animal populations in the wild. This method gives the estimate as $S=N$. This would because out of $N$ samples taken during the ""recapture"", exactly $1/N$ of them are articles viewed during the ""mark"" phase (original article that is remembered). Since $1/N$ of the pages are known to be picked from the group of  one particular page, then the estimate would be that there would be $N$ pages in total.",,"['probability', 'recreational-mathematics']"
26,Mathematical description of a random sample,Mathematical description of a random sample,,"Mathematical description of a random sample: which one is it and why? $X_1(\omega), X_2(\omega), ..., X_n(\omega)$, where $X_1, ..., X_n$ are different but i.i.d. random variables. $X(\omega_1), X(\omega_2), ..., X(\omega_n)$, where $X$ is a (single) random variable.","Mathematical description of a random sample: which one is it and why? $X_1(\omega), X_2(\omega), ..., X_n(\omega)$, where $X_1, ..., X_n$ are different but i.i.d. random variables. $X(\omega_1), X(\omega_2), ..., X(\omega_n)$, where $X$ is a (single) random variable.",,"['probability', 'probability-theory', 'random-functions']"
27,Probability of getting into my favorite PhD,Probability of getting into my favorite PhD,,"Suppose there are $n$ potential grad students and $n$ Universities. Each University has one scholarship to do a PhD. Each student has a strict ranking of Universities so that each University is comparable and she is not indifferent between any two Universities.  The preferences of each student are independent and generated uniformly at random. Students are ordered according to their height (or some other irrelevant attribute) and can choose their favourite University to study in based on this order. Universities accept the first student who applies to them and their decision is final and irrevocable. My first question is: for the $k$ tallest applicant, what is the probability that they end up in their $j$ -th best University? I'm thinking for the k-th tallest applicant, the chance of getting into her top choice is $\frac{n+1-k}{n}$ . Second question: Suppose now that, if a student gets her first choice, she completes her PhD with probability 1, if she gets her second choice, she does with probability 1/2 and so on, so in general, if a student gets her k choice, she finishes her PhD with probability $\frac{1}{2^{k-1}}$ . What is the expected number of students that fail to complete their PhD? Simulations tell me it is $0.38 n$ , but I would like to understand why.","Suppose there are potential grad students and Universities. Each University has one scholarship to do a PhD. Each student has a strict ranking of Universities so that each University is comparable and she is not indifferent between any two Universities.  The preferences of each student are independent and generated uniformly at random. Students are ordered according to their height (or some other irrelevant attribute) and can choose their favourite University to study in based on this order. Universities accept the first student who applies to them and their decision is final and irrevocable. My first question is: for the tallest applicant, what is the probability that they end up in their -th best University? I'm thinking for the k-th tallest applicant, the chance of getting into her top choice is . Second question: Suppose now that, if a student gets her first choice, she completes her PhD with probability 1, if she gets her second choice, she does with probability 1/2 and so on, so in general, if a student gets her k choice, she finishes her PhD with probability . What is the expected number of students that fail to complete their PhD? Simulations tell me it is , but I would like to understand why.",n n k j \frac{n+1-k}{n} \frac{1}{2^{k-1}} 0.38 n,"['probability', 'order-statistics']"
28,"Tied chess matches and the monotonicity of $\sum_{k=0}^n \binom{2n}{k,k,2n-2k} (pq)^k (1-p-q)^{2n-2k}$",Tied chess matches and the monotonicity of,"\sum_{k=0}^n \binom{2n}{k,k,2n-2k} (pq)^k (1-p-q)^{2n-2k}","In the upcoming World Chess Championship 14 games in the classical time format will be played compared to 12 in the previous matches. This change appears to have been made mainly to reduce the number of draws by allowing the players to take more risks, but it has also been stated that this decreases the chances of a tie in the overall series. Intuitively, this seems clear: the higher the total number of games, the less likely it is that both players win exactly the same number of games. A mathematical proof would be better, of course, and this is what I am after here. I want to consider a simple model for the general case with $2n$ games, $n \in \mathbb{N}$ . Let $p$ and $q$ be the probability for the first and second player to win any given game (independent of previous results, current form, etc.), respectively. Then $0 \leq p, q, p + q \leq 1$ holds and $1-p-q$ is the probability of a draw (we ignore the advantage for the player with the white pieces and interpret $p$ and $q$ as colour averages). Even though the series is not continued if one player takes an unassailable lead, we can pretend that all $2n$ games are played and compute the probability of $k$ wins for the first and $l$ wins for the second player from a trinomial distribution : \begin{equation} f(p,q,n;k,l) =  \binom{2n}{k,l,2n-k-l} p^k q^l (1-p-q)^{2n-k-l} .  \end{equation} The probability of an overall tie is \begin{align} t(p,q,n) &= \sum \limits_{k=0}^n f(p,q,n;k,k) = \sum \limits_{k=0}^n \frac{(2n)!}{k!^2 (2n-2k)!} (pq)^k (1-p-q)^{2n-2k} \\ &= (1-p-q)^{2n} {}_2F_1 \left(\frac{1}{2}-n,-n;1;\frac{4 p q}{(1-p-q)^2}\right) . \end{align} Provided that this reasoning is correct, my question is: How can we show that $t(p,q,n)$ is decreasing in $n$ for any given $p,q,p+q \in [0,1]$ ? Some thoughts: Numerically, this clearly appears to be true. This is the probability of a tie for $q=p$ : In the special case $p=q=\frac{1}{2}$ we have $$ t\left(\frac{1}{2},\frac{1}{2},n\right) = f\left(\frac{1}{2},\frac{1}{2},n;n,n\right) = \frac{1}{4^n} \binom{2n}{n} = \frac{2}{\pi} \int \limits_0^\infty \frac{\mathrm{d} x}{(1+x^2)^{n+1}} \, , $$ which is clearly decreasing in $n$ . Maybe an integral representation can also be helpful in the general case. I do not know a lot about hypergeometric functions though and cannot find a transformation which would allow us to use Euler's integral formula .","In the upcoming World Chess Championship 14 games in the classical time format will be played compared to 12 in the previous matches. This change appears to have been made mainly to reduce the number of draws by allowing the players to take more risks, but it has also been stated that this decreases the chances of a tie in the overall series. Intuitively, this seems clear: the higher the total number of games, the less likely it is that both players win exactly the same number of games. A mathematical proof would be better, of course, and this is what I am after here. I want to consider a simple model for the general case with games, . Let and be the probability for the first and second player to win any given game (independent of previous results, current form, etc.), respectively. Then holds and is the probability of a draw (we ignore the advantage for the player with the white pieces and interpret and as colour averages). Even though the series is not continued if one player takes an unassailable lead, we can pretend that all games are played and compute the probability of wins for the first and wins for the second player from a trinomial distribution : The probability of an overall tie is Provided that this reasoning is correct, my question is: How can we show that is decreasing in for any given ? Some thoughts: Numerically, this clearly appears to be true. This is the probability of a tie for : In the special case we have which is clearly decreasing in . Maybe an integral representation can also be helpful in the general case. I do not know a lot about hypergeometric functions though and cannot find a transformation which would allow us to use Euler's integral formula .","2n n \in \mathbb{N} p q 0 \leq p, q, p + q \leq 1 1-p-q p q 2n k l \begin{equation}
f(p,q,n;k,l) =  \binom{2n}{k,l,2n-k-l} p^k q^l (1-p-q)^{2n-k-l} . 
\end{equation} \begin{align}
t(p,q,n) &= \sum \limits_{k=0}^n f(p,q,n;k,k) = \sum \limits_{k=0}^n \frac{(2n)!}{k!^2 (2n-2k)!} (pq)^k (1-p-q)^{2n-2k} \\
&= (1-p-q)^{2n} {}_2F_1 \left(\frac{1}{2}-n,-n;1;\frac{4 p q}{(1-p-q)^2}\right) .
\end{align} t(p,q,n) n p,q,p+q \in [0,1] q=p p=q=\frac{1}{2}  t\left(\frac{1}{2},\frac{1}{2},n\right) = f\left(\frac{1}{2},\frac{1}{2},n;n,n\right) = \frac{1}{4^n} \binom{2n}{n} = \frac{2}{\pi} \int \limits_0^\infty \frac{\mathrm{d} x}{(1+x^2)^{n+1}} \, ,  n","['probability', 'sequences-and-series', 'hypergeometric-function', 'multinomial-coefficients', 'multinomial-distribution']"
29,Playing the St. Petersburg Lottery until I lose everything,Playing the St. Petersburg Lottery until I lose everything,,"This question continues the following question: Calculating the probability of winning at least $128$ dollars in a lottery St. Petersburg Paradox Here is a lottery: A fair coin is flipped repeatedly until it produces ""heads."" If the first occurrence of heads is on the $n$th toss, you are paid $2^{n−1}$. So for instance, if heads appears on the first toss, you are paid 1 dollar; if heads appears for the first time on the second toss, you are paid 2 dollars, and so on. Say the cost of entering the lottery is 10 dollars and I start with 100 dollars. I'll play the lottery again and again until I have more than 10 dollars to play the game. Now my question is, what is the probability that I can go on playing the game? Example: Say during the first game, I get a heads on the first toss then I'll be left with 91(=100-10+1) dollars. I'll play the game again by paying 10 dollars and this time I may get a heads on the 7 toss and now I'll have 145(=91-10+64) dollars and so on. Edit: Or Is it possible to find the probability of surviving the nth round given that I have survived all the previous rounds??","This question continues the following question: Calculating the probability of winning at least $128$ dollars in a lottery St. Petersburg Paradox Here is a lottery: A fair coin is flipped repeatedly until it produces ""heads."" If the first occurrence of heads is on the $n$th toss, you are paid $2^{n−1}$. So for instance, if heads appears on the first toss, you are paid 1 dollar; if heads appears for the first time on the second toss, you are paid 2 dollars, and so on. Say the cost of entering the lottery is 10 dollars and I start with 100 dollars. I'll play the lottery again and again until I have more than 10 dollars to play the game. Now my question is, what is the probability that I can go on playing the game? Example: Say during the first game, I get a heads on the first toss then I'll be left with 91(=100-10+1) dollars. I'll play the game again by paying 10 dollars and this time I may get a heads on the 7 toss and now I'll have 145(=91-10+64) dollars and so on. Edit: Or Is it possible to find the probability of surviving the nth round given that I have survived all the previous rounds??",,"['probability', 'probability-theory']"
30,Is there a symmetric alternative to Kullback-Leibler divergence?,Is there a symmetric alternative to Kullback-Leibler divergence?,,"I have two samples of probability distributions that I would like to compare. I have previously heard about the Kullback-Leibler divergence, but reading up on this it seems like  its non-symmetricity makes it more suitable for comparing a sample to a model, rather than comparing two samples. What would you propose that I use instead, or maybe the KL divergence actually is a good choice?","I have two samples of probability distributions that I would like to compare. I have previously heard about the Kullback-Leibler divergence, but reading up on this it seems like  its non-symmetricity makes it more suitable for comparing a sample to a model, rather than comparing two samples. What would you propose that I use instead, or maybe the KL divergence actually is a good choice?",,['probability']
31,Simple dice game: Optimal strategy?,Simple dice game: Optimal strategy?,,"Here's the description of a dice game which puzzles me since quite some time (the game comes from a book which offered a quite unsatisfactory solution — but then, its focus was on programming, so this is probably excusable). The game goes as follows: Two players play against each other, starting with score 0 each. Winner is the player to first reach a score of 100 or more. The players play in turn. The score added in each round is determined as follows: The player throws a die. If the die does not show an 1, he has the option to stop and have the points added to his score, or to continue throwing until either he stops or gets an 1. As soon as he gets an 1, his turn ends and no points are added to his score, any points he has accumulated in this round are lost. Afterward it is the second player's turn. The question is now what is the best strategy for that game. The book suggested to try out which of the following two strategies gives better result: Throw 5 times (if possible), then stop. If the accumulated points in this round add up to 20 or more, stop, otherwise continue. The rationale is that you want the next throw to increase the expected score. Of course it doesn't need testing to see that the second strategy is better: If you've accumulated e.g. 10 points, it doesn't matter whether you accumulated them with 5 times throwing a 2, or with 2 times throwing a 5. However it is also easy to see that this second strategy isn't the best one either: After all, the ultimate goal is not to maximize the increase per round, but to maximize the probability to win.; both are related, but not the same. For example, imagine you have been very unlucky and are still at a very low score, but your opponent has already 99 points. It's your turn, and you've already accumulated some points (but those points don't get you above 100) and have to decide whether to stop, or to continue. If you stop, you secure the points, but your opponent has a 5/6 chance to win in the next move. Let's say that if you stop, the optimal strategy in the next move will be to try to get 100 points in one run, and that the probability to reach that is $p$. Then if you stop, since your opponent then has his chance to win first, your total probability to win is just $1/6(p + (1-p)/6 (p + (1-p)/6 (p + ...))) = p/(p+5)$. On the other hand, if you continue to 100 points right now, you have the chance $p$ to win this round before the other has a chance to try, but a lower probability $p'$ to win in later rounds, giving a probability $p + p'/(5+p')$. It is obvious that even if we had $p'=0$ (i.e. if you don't succeed now, you'll lose), you'd still have the probability $p>p/(p+5)$ to win by continuing, so you should continue no matter how slim your chances, and even if your accumulated points this round are above 20, because if you stop, you chances will be worse for sure. Since at some time, the optimal strategy will have a step where you try to go beyond 100 (because that's where you win), by induction you can say that if your opponent has already 99 points, your best strategy is, unconditionally, to try to get 100 points in one run. Of course this ""brute force rule"" is for that specific situation (it also applies if the opponent has 98 points, for obvious reasons). If you'd play that brute-force rule from the beginning, you'd lose even against someone who just throws once each round. Indeed, if both are about equal, and far enough from the final 100 points, intuitively I think the 20 points rule is quite good. Also, intuitively I think if you are far advanced against your opponent, you should even play more safe and stop earlier. As the current game situation is described by the three numbers your score ($Y$), your opponent's score ($O$) and the points already collected in this round ($P$), and your decision is to either continue ($C$) or to stop ($S$), a strategy is completely given by a function $$s:\{(Y, O, P)\}\to \{C,S\}$$ where the following rules are obvious: If $Y+S\ge 100$ then $s(Y,O,P)=S$ (if you already have collected 100 points, the only reasonable move is to stop). $s(Y, O, 0)=C$ (it doesn't make sense to stop before you threw at least once). Also, I just above derived the following rule: $f(Y,98,P)=f(Y,99,P)=C$ unless the first rule kicks in. I believe the following rule should also hold (but have no idea how to prove it): If $f(Y,98,P)=S$ then also $f(Y,98,P+1)=S$ If that believe is true, then the description of a strategy can be simplified to a function $g(Y,O)$ which gives the smallest $P$ at which you should stop. However, that's all I've figured out. What I'd really like to know is: What is the optimal strategy for this game?","Here's the description of a dice game which puzzles me since quite some time (the game comes from a book which offered a quite unsatisfactory solution — but then, its focus was on programming, so this is probably excusable). The game goes as follows: Two players play against each other, starting with score 0 each. Winner is the player to first reach a score of 100 or more. The players play in turn. The score added in each round is determined as follows: The player throws a die. If the die does not show an 1, he has the option to stop and have the points added to his score, or to continue throwing until either he stops or gets an 1. As soon as he gets an 1, his turn ends and no points are added to his score, any points he has accumulated in this round are lost. Afterward it is the second player's turn. The question is now what is the best strategy for that game. The book suggested to try out which of the following two strategies gives better result: Throw 5 times (if possible), then stop. If the accumulated points in this round add up to 20 or more, stop, otherwise continue. The rationale is that you want the next throw to increase the expected score. Of course it doesn't need testing to see that the second strategy is better: If you've accumulated e.g. 10 points, it doesn't matter whether you accumulated them with 5 times throwing a 2, or with 2 times throwing a 5. However it is also easy to see that this second strategy isn't the best one either: After all, the ultimate goal is not to maximize the increase per round, but to maximize the probability to win.; both are related, but not the same. For example, imagine you have been very unlucky and are still at a very low score, but your opponent has already 99 points. It's your turn, and you've already accumulated some points (but those points don't get you above 100) and have to decide whether to stop, or to continue. If you stop, you secure the points, but your opponent has a 5/6 chance to win in the next move. Let's say that if you stop, the optimal strategy in the next move will be to try to get 100 points in one run, and that the probability to reach that is $p$. Then if you stop, since your opponent then has his chance to win first, your total probability to win is just $1/6(p + (1-p)/6 (p + (1-p)/6 (p + ...))) = p/(p+5)$. On the other hand, if you continue to 100 points right now, you have the chance $p$ to win this round before the other has a chance to try, but a lower probability $p'$ to win in later rounds, giving a probability $p + p'/(5+p')$. It is obvious that even if we had $p'=0$ (i.e. if you don't succeed now, you'll lose), you'd still have the probability $p>p/(p+5)$ to win by continuing, so you should continue no matter how slim your chances, and even if your accumulated points this round are above 20, because if you stop, you chances will be worse for sure. Since at some time, the optimal strategy will have a step where you try to go beyond 100 (because that's where you win), by induction you can say that if your opponent has already 99 points, your best strategy is, unconditionally, to try to get 100 points in one run. Of course this ""brute force rule"" is for that specific situation (it also applies if the opponent has 98 points, for obvious reasons). If you'd play that brute-force rule from the beginning, you'd lose even against someone who just throws once each round. Indeed, if both are about equal, and far enough from the final 100 points, intuitively I think the 20 points rule is quite good. Also, intuitively I think if you are far advanced against your opponent, you should even play more safe and stop earlier. As the current game situation is described by the three numbers your score ($Y$), your opponent's score ($O$) and the points already collected in this round ($P$), and your decision is to either continue ($C$) or to stop ($S$), a strategy is completely given by a function $$s:\{(Y, O, P)\}\to \{C,S\}$$ where the following rules are obvious: If $Y+S\ge 100$ then $s(Y,O,P)=S$ (if you already have collected 100 points, the only reasonable move is to stop). $s(Y, O, 0)=C$ (it doesn't make sense to stop before you threw at least once). Also, I just above derived the following rule: $f(Y,98,P)=f(Y,99,P)=C$ unless the first rule kicks in. I believe the following rule should also hold (but have no idea how to prove it): If $f(Y,98,P)=S$ then also $f(Y,98,P+1)=S$ If that believe is true, then the description of a strategy can be simplified to a function $g(Y,O)$ which gives the smallest $P$ at which you should stop. However, that's all I've figured out. What I'd really like to know is: What is the optimal strategy for this game?",,"['probability', 'game-theory']"
32,Random points in a rectangular grid defining a closed path,Random points in a rectangular grid defining a closed path,,"Suppose we have a $n\times m$ rectangular grid (namely: $nm$ points disposed as a matrix with $n$ rows and $m$ columns). We randomly pick $h$ different points in the grid, where every point is equally likely. If only horizontal or vertical movements between two points are allowed, what is the probability that the points define at least one closed path? ps: we can suppose $m=n$ to simplify For example, let $n=m=4$ and $h=6$. 1 denotes a selected point, 0 a non-selected one. These $6$ points define a closed path: 1  0  0  1 0  0  0  0 1  1  0  0 0  1  0  1 as these $6$ do (the $4$ in the bottom-right corner): 1  0  0  1 0  0  0  0 0  1  0  1 0  1  0  1 while the following $6$ points do not: 1  0  0  0 0  0  0  1 1  1  0  0 0  1  0  1 Substantially, the $h$ points define a closed path if and only if there exist a subset of these $h$ points such that every point in the subset has one other point of the subset on the same row and one on the same column. Thanks for your help.","Suppose we have a $n\times m$ rectangular grid (namely: $nm$ points disposed as a matrix with $n$ rows and $m$ columns). We randomly pick $h$ different points in the grid, where every point is equally likely. If only horizontal or vertical movements between two points are allowed, what is the probability that the points define at least one closed path? ps: we can suppose $m=n$ to simplify For example, let $n=m=4$ and $h=6$. 1 denotes a selected point, 0 a non-selected one. These $6$ points define a closed path: 1  0  0  1 0  0  0  0 1  1  0  0 0  1  0  1 as these $6$ do (the $4$ in the bottom-right corner): 1  0  0  1 0  0  0  0 0  1  0  1 0  1  0  1 while the following $6$ points do not: 1  0  0  0 0  0  0  1 1  1  0  0 0  1  0  1 Substantially, the $h$ points define a closed path if and only if there exist a subset of these $h$ points such that every point in the subset has one other point of the subset on the same row and one on the same column. Thanks for your help.",,['probability']
33,Objective metric to describe skill vs luck in games that include randomness,Objective metric to describe skill vs luck in games that include randomness,,"There are many games that even though they include some random component (for example dice rolls or dealing of cards) they are skill games. In other words, there is definite skill in how one can play the game taking into account the random component. Think of backgammon or poker as good examples of such games. Moreover, novice players or outsiders might fail to recognise the skill involved and attribute wins purely to luck. As someone gains experience, they usually start to appreciate the skill involved, and concede that there is more to the game than just luck. They might even concede that there is ""more"" skill than luck. How do we quantify this? How ""much"" luck vs skill ? People can have very subjective feelings about this balance. Recently, I was reading someone arguing that backgammon is $9/10$ luck, while another one was saying it's $6/10$. These numbers mean very little other than expressing gut feelings. Can we do better? Can we have an objective metric to give us a good sense of the skill vs luck component of a game. I was thinking along these lines: Given a game and a lot of empirical data on matches between players with different skills, a metric could be: How many games on the average do we need to have an overall win (positive win-loss balance) with probability $P$ (let's use $P=0.95$) between a top level player and a novice? For the game of chess this metric would be $1$ (or very close to $1$). For the game of scissors-paper-rock it would be $\infty$. This is an objective measure (we can calculate it based on empirical data) and it is intuitive. There is however an ambiguity in what top-level and novice players mean. Empirical data alone does not suffice to classify the players as novices or experts. For example, imagine that we have the results from 10,000 chess games between 20 chess grandmasters. Some will be better, some will be worse, but analysing the data with the criterion I defined, we will conclude that chess has a certain (significant) element of luck. Can we make this more robust? Also, given a set of empirical data (match outcomes) how do we know we have enough data? What other properties do we want to include? Maybe a rating between $[0, 1]$, zero meaning no luck, and one meaning all luck, would be easier to talk about. I am happy to hear completely different approaches too.","There are many games that even though they include some random component (for example dice rolls or dealing of cards) they are skill games. In other words, there is definite skill in how one can play the game taking into account the random component. Think of backgammon or poker as good examples of such games. Moreover, novice players or outsiders might fail to recognise the skill involved and attribute wins purely to luck. As someone gains experience, they usually start to appreciate the skill involved, and concede that there is more to the game than just luck. They might even concede that there is ""more"" skill than luck. How do we quantify this? How ""much"" luck vs skill ? People can have very subjective feelings about this balance. Recently, I was reading someone arguing that backgammon is $9/10$ luck, while another one was saying it's $6/10$. These numbers mean very little other than expressing gut feelings. Can we do better? Can we have an objective metric to give us a good sense of the skill vs luck component of a game. I was thinking along these lines: Given a game and a lot of empirical data on matches between players with different skills, a metric could be: How many games on the average do we need to have an overall win (positive win-loss balance) with probability $P$ (let's use $P=0.95$) between a top level player and a novice? For the game of chess this metric would be $1$ (or very close to $1$). For the game of scissors-paper-rock it would be $\infty$. This is an objective measure (we can calculate it based on empirical data) and it is intuitive. There is however an ambiguity in what top-level and novice players mean. Empirical data alone does not suffice to classify the players as novices or experts. For example, imagine that we have the results from 10,000 chess games between 20 chess grandmasters. Some will be better, some will be worse, but analysing the data with the criterion I defined, we will conclude that chess has a certain (significant) element of luck. Can we make this more robust? Also, given a set of empirical data (match outcomes) how do we know we have enough data? What other properties do we want to include? Maybe a rating between $[0, 1]$, zero meaning no luck, and one meaning all luck, would be easier to talk about. I am happy to hear completely different approaches too.",,"['probability', 'soft-question', 'game-theory']"
34,Average number of tosses to WIN the gambler's ruin game.,Average number of tosses to WIN the gambler's ruin game.,,"In the ""gambler's ruin"" game, you start with 'n' dollars. You keep betting 1 dollar (on heads), on coin tosses.  The coin is biased to come out heads with probability = p.  A game ends when you have gotten N dollars,  N>n, (you won the game)  or you go broke ( you have $0 --you lost the game).  Many sources derive the average length of a game, win OR lose .  But, considering ONLY the WON games,-- what is the average length of a  WON game? i.e. how many coin tosses happen, on the average, in the won games?  Give the average number of tosses in a won game as a function of 'p'  and 'N'.","In the ""gambler's ruin"" game, you start with 'n' dollars. You keep betting 1 dollar (on heads), on coin tosses.  The coin is biased to come out heads with probability = p.  A game ends when you have gotten N dollars,  N>n, (you won the game)  or you go broke ( you have $0 --you lost the game).  Many sources derive the average length of a game, win OR lose .  But, considering ONLY the WON games,-- what is the average length of a  WON game? i.e. how many coin tosses happen, on the average, in the won games?  Give the average number of tosses in a won game as a function of 'p'  and 'N'.",,['probability']
35,Is Entropy = Information circular or trivial?,Is Entropy = Information circular or trivial?,,"I have seen several ""maximum entropy distributions"" used in the mathematical and statistical literature, often with the justification that they are ""minimally informed"" beyond the assumptions and data used to construct them. However, it seems that the appeal to information content of a signal or distribution is via an appeal to entropy, hence to say that increasing entropy decreases informativeness seems circular, lacking any external foundation that would compel us to rationally equate Shannon entropy with information content (or, more precisely, the lack thereof). What is the foundational science/concept that warrants Entropy as a measure of information content? NOTE: I have read Shannon's original paper, where he discusses an axiomatic derivation of his entropy function -- but...he also takes pains to point out that this is not his primary justification (pp. 10-11 and App. 2, Shannon 1948 ). Instead, it was its empirical success in communications engineering that he felt warranted it's use. Also, note that the work of Renyi and Uffink have pointed out that Shannon's tenets are not the only way to construct a plausible information measure. However, nowadays, we are applying his formula to constructs like uncertainty distributions, which have little hope of experimental verification. In addition, it is not clear that using a max-entropy distribution has any advantage over any other distribution when there is a high level of uncertainty.","I have seen several ""maximum entropy distributions"" used in the mathematical and statistical literature, often with the justification that they are ""minimally informed"" beyond the assumptions and data used to construct them. However, it seems that the appeal to information content of a signal or distribution is via an appeal to entropy, hence to say that increasing entropy decreases informativeness seems circular, lacking any external foundation that would compel us to rationally equate Shannon entropy with information content (or, more precisely, the lack thereof). What is the foundational science/concept that warrants Entropy as a measure of information content? NOTE: I have read Shannon's original paper, where he discusses an axiomatic derivation of his entropy function -- but...he also takes pains to point out that this is not his primary justification (pp. 10-11 and App. 2, Shannon 1948 ). Instead, it was its empirical success in communications engineering that he felt warranted it's use. Also, note that the work of Renyi and Uffink have pointed out that Shannon's tenets are not the only way to construct a plausible information measure. However, nowadays, we are applying his formula to constructs like uncertainty distributions, which have little hope of experimental verification. In addition, it is not clear that using a max-entropy distribution has any advantage over any other distribution when there is a high level of uncertainty.",,"['probability', 'statistics']"
36,An Optimal Strategy for a Coin Flipping Game,An Optimal Strategy for a Coin Flipping Game,,"Consider a fair coin, tossed 100 times to create a sequence of $H$ s and $T$ s. A participant is allowed to ask 1 yes or no question (e.g. was the first coin flip heads?), then plays a game where he tries to guess all 100 coins. The participant is awarded $\$1$ for every coin guessed correctly, and loses $\$1$ for each incorrect guess. Find and prove an optimal strategy for the player. I have a hunch that the optimal strategy may be to ask ""Were there more heads than tails?"" and then, depending on the answer, proceed to guess either all $H$ s or all $T$ s. With this strategy, the player is guaranteed nonnegative earnings, and I believe the expected value is $$\sum_{i=0}^{50}{\binom{100}{i}\left(\frac{1}{2}\right)^{99}(100-2i)} \approx \$7.96$$ I've confirmed the expected value with a Monte-Carlo simulation in Python, but I'm having trouble proving that this is optimal. My best attempt to translate this into more rigorous mathematics is to consider the yes/no question as a partition. Let $X$ be the set of $2^{100}$ possible sequences and $x$ be the sequence rolled. A yes/no question will always partition the set into two. Suppose that set $A$ is the set of all sequences in which the answer to our question is ""yes"", then the expected value of our game would be $$E[G] = \frac{|A|}{2^{100}}E[G|x\in A]\space + \left(1-\frac{|A|}{2^{100}}\right)E[G|x \notin A],$$ where G is the expected value of the game, playing with some optimal strategy. I've also made the note that given any specific set $A$ , $x \in A$ implies there is an optimal (but not necessarily unique) guess. For instance, if we know that there are more heads than tails, a sequence of 100 $H$ s is an optimal guess.","Consider a fair coin, tossed 100 times to create a sequence of s and s. A participant is allowed to ask 1 yes or no question (e.g. was the first coin flip heads?), then plays a game where he tries to guess all 100 coins. The participant is awarded for every coin guessed correctly, and loses for each incorrect guess. Find and prove an optimal strategy for the player. I have a hunch that the optimal strategy may be to ask ""Were there more heads than tails?"" and then, depending on the answer, proceed to guess either all s or all s. With this strategy, the player is guaranteed nonnegative earnings, and I believe the expected value is I've confirmed the expected value with a Monte-Carlo simulation in Python, but I'm having trouble proving that this is optimal. My best attempt to translate this into more rigorous mathematics is to consider the yes/no question as a partition. Let be the set of possible sequences and be the sequence rolled. A yes/no question will always partition the set into two. Suppose that set is the set of all sequences in which the answer to our question is ""yes"", then the expected value of our game would be where G is the expected value of the game, playing with some optimal strategy. I've also made the note that given any specific set , implies there is an optimal (but not necessarily unique) guess. For instance, if we know that there are more heads than tails, a sequence of 100 s is an optimal guess.","H T \1 \1 H T \sum_{i=0}^{50}{\binom{100}{i}\left(\frac{1}{2}\right)^{99}(100-2i)} \approx \7.96 X 2^{100} x A E[G] = \frac{|A|}{2^{100}}E[G|x\in A]\space + \left(1-\frac{|A|}{2^{100}}\right)E[G|x \notin A], A x \in A H","['probability', 'expected-value']"
37,Show that an increasing function has derivative $0$ a.e.,Show that an increasing function has derivative  a.e.,0,"Let $0<p<1$ and define $F:[0,1]\rightarrow[0,1]$ by $$F(x)=\begin{cases} pF(2x),&x\in\left[0,\frac12\right]\\ p+qF(2x-1),&x\in\left[\frac12,1\right] \end{cases}$$ where $q=1-p$ .  I would like to prove that $F'(x)=0$ a.e. I am working my way through ""How to Gamble If You Must"" by Kyle Siegerst, which is basically a series of exercises. $F(x)$ is the probability that a gambler starting with a bankroll $0\leq x\leq 1$ will reach his target of $1$ if he engages in ""bold play"" in the game of red and black.  When his bankroll is $\leq\frac12$ he bets it all, winning the amount bet with probability $p$ , and losing it with probability $q$ .  When his bankroll is $>\frac12$ , he bets just enough to reach the target, that is, $1-x$ . In the exercises, I have shown that there is a unique function $F$ satisfying the functional equation above, and that it is continuous and strictly increasing.  Following exercise $33$ , the author remarks that when $p\neq\frac12$ , $F'(X)=0$ a.e., so that $F$ is a devil's staircase.  I have been trying to prove this statement.  (I know that an increasing function is differentiable a.e.  It's the value that I'm having trouble with.) Vague $50$ -year-old memories of measure theory have led me to Proposition 3.31 in Folland's ""Real Analysis"", to wit If $F\in NBV, \text{ then }F\in L^1(m).$ Moreover, $\mu_F\perp m \text{ iff } F' =0$ a.e., and $\mu_F \ll m \text{ iff } F(x)=\int_{-\infty}^xF'(t)dt. $ Here $m$ is Lebesgue measure, and a.e. is with respect to Lebesgue measure. $\mu_F$ is the Borel measure defined by $\mu_F([a,b])=F(b)-F(a)$ .  Folland uses $NBV$ to mean that $F$ is of bounded variation, $F(-\infty)=0$ and $F$ is right continuous.  This is no problem, as we can extend $F$ to $\mathbb{R}$ by defining $F(x)=0$ for $x<0$ and $F(x)=1$ for $x>1$ . So it seems to come down to showing $\mu_F\perp m$ .  This means that there is an $E\subset[0,1]$ with $m(E)=0$ and $\mu_F(E)=1$ if I'm not mistaken.  I don't see how to prove this.  Indeed it doesn't seem at all likely to me, so I must misunderstand something. In exercise 29, I proved that $$F(x)=\sum_{n=1}^\infty p_{x_1}\cdots p_{x_{n-1}}px_n$$ where $x_i$ is bit number $i$ of $x$ , and $p_0=p,\ p_1=q$ .  (When $x$ is a dyadic rational, we take the terminating representation.)   If we represent wins by $1$ and losses by $0$ , this means that the gambler reaches the goal if and only if the first time a bit in his bankroll matches the corresponding game bit, those bits are both $1$ .  This is the most concrete representation of $F$ in the paper, but I don't see how it helps. Can you cast any light on this for me?","Let and define by where .  I would like to prove that a.e. I am working my way through ""How to Gamble If You Must"" by Kyle Siegerst, which is basically a series of exercises. is the probability that a gambler starting with a bankroll will reach his target of if he engages in ""bold play"" in the game of red and black.  When his bankroll is he bets it all, winning the amount bet with probability , and losing it with probability .  When his bankroll is , he bets just enough to reach the target, that is, . In the exercises, I have shown that there is a unique function satisfying the functional equation above, and that it is continuous and strictly increasing.  Following exercise , the author remarks that when , a.e., so that is a devil's staircase.  I have been trying to prove this statement.  (I know that an increasing function is differentiable a.e.  It's the value that I'm having trouble with.) Vague -year-old memories of measure theory have led me to Proposition 3.31 in Folland's ""Real Analysis"", to wit If Moreover, a.e., and Here is Lebesgue measure, and a.e. is with respect to Lebesgue measure. is the Borel measure defined by .  Folland uses to mean that is of bounded variation, and is right continuous.  This is no problem, as we can extend to by defining for and for . So it seems to come down to showing .  This means that there is an with and if I'm not mistaken.  I don't see how to prove this.  Indeed it doesn't seem at all likely to me, so I must misunderstand something. In exercise 29, I proved that where is bit number of , and .  (When is a dyadic rational, we take the terminating representation.)   If we represent wins by and losses by , this means that the gambler reaches the goal if and only if the first time a bit in his bankroll matches the corresponding game bit, those bits are both .  This is the most concrete representation of in the paper, but I don't see how it helps. Can you cast any light on this for me?","0<p<1 F:[0,1]\rightarrow[0,1] F(x)=\begin{cases}
pF(2x),&x\in\left[0,\frac12\right]\\
p+qF(2x-1),&x\in\left[\frac12,1\right]
\end{cases} q=1-p F'(x)=0 F(x) 0\leq x\leq 1 1 \leq\frac12 p q >\frac12 1-x F 33 p\neq\frac12 F'(X)=0 F 50 F\in NBV, \text{ then }F\in L^1(m). \mu_F\perp m \text{ iff } F' =0 \mu_F \ll m \text{ iff } F(x)=\int_{-\infty}^xF'(t)dt.  m \mu_F \mu_F([a,b])=F(b)-F(a) NBV F F(-\infty)=0 F F \mathbb{R} F(x)=0 x<0 F(x)=1 x>1 \mu_F\perp m E\subset[0,1] m(E)=0 \mu_F(E)=1 F(x)=\sum_{n=1}^\infty p_{x_1}\cdots p_{x_{n-1}}px_n x_i i x p_0=p,\ p_1=q x 1 0 1 F","['probability', 'measure-theory', 'radon-nikodym']"
38,Finding the expected value of coin flip experiment (Dark Souls problem),Finding the expected value of coin flip experiment (Dark Souls problem),,"I'm trying to find the formula for an expected value, but testing has shown that my formula is incorrect. Can you find either a closed form or recursive formula for the expected values in this experiment, and explain how you found that formula? The Game You have $n$ coins labelled $1$ through $n$ . We will label the set of coins $[n]$ . We have a function $f$ that maps coin $i$ to the probability coin $i$ lands heads. A coin landing heads is a success. A coin landing tails is a failure. We start with flipping coin $1$ . If coin $1$ lands heads, we move on to coin $2$ . If coin $1$ lands tails, we start over with coin 1. On coin $i$ , if coin $i$ lands heads, we move on to coin $i+1$ . If coin $i$ lands tails, we return to coin $1$ . If we land heads on coin $n$ , we win and the game ends. The Variables and Counters We will be keeping track of $n$ variables $\{ X_1, X_2, \ldots , X_n \}$ . Each of these will count some, but not all, of the times we fail coin $n$ . To make the explanation easier, we will treat these as counters in an algorithm, and their final values will be the random variables. If we flip heads on coin $i$ , we do nothing. If we flip tails on coin $i$ , one of two things happens. If the last coin we failed, $j$ , appeared later than $i$ (that is, $j > i$ ), then we reset all counters $\{ X_1, X_2, \ldots , X_n \}$ to $0$ and then set $X_i=1$ . If the last coin we failed, $j$ , appeared earlier than or is equal to $i$ (that is, $j \leq i$ ), then we increment $X_i$ by $1$ and move back to coin $1$ as described in the rules. Edit: If we have not failed a coin previously, then we do $(2)$ . That is, we increment $X_i$ by $1$ and move back to coin $1$ without changing any other counters. The Problem We want to find the expected value of each $X_i$ after successfully flipping heads on coin $n$ . My Solution It's hard to put my solution in math terms, because my solution is wrong. I write the expansion $$E(X_i) = 0*P(X_i = 0) + 1*P(X_i = 1) +  2*P(X_i = 2) + 3*P(X_i = 3) + \ldots \text{.}\tag{1}$$ My intuition then leads me to say $$P(X_i=k+1) = P(X_i=k)*(1-f(i))*\prod_{j = 1}^{j=i-1}f(j)\text{.}\tag{2}$$ The above, $(2)$ , is probably wrong, but here is the intuition. $(1-f(i))$ represents the probability of an extra failed flip of coin $i$ at a particular point in the game followed by successful coin-flips on coins $1$ through $i-1$ represented by the term $\prod_{j = 1}^{j=i-1}f(j)$ . For notational convenience, we set $x=(1-f(i))*\prod_{j = 1}^{j=i-1}f(j)$ . We can then use $P(X_i =0) + P(X_i=1) + P(X_i=2) +\ldots = 1$ to find that $$P(X_i=k)= (1-x)*x^{k} \tag{3}$$ and then $$\mathbb{E}(X_i) = \frac{x}{1-x}\text{.}\tag{4}$$ Displays $(3)$ and $(4)$ are just algebra, and even if there is a typo, I'm fairly certain that is not where the problem lies. Data If $n=5$ and $f(i)=.5$ for all $i$ , then some approximations of the expected values are $\mathbb{E}(X_1) = 1.56902$ , $\mathbb{E}(X_2)= .4036$ , $\mathbb{E}(X_3)= .15541$ , $\mathbb{E}(X_4)= .06952$ , and $\mathbb{E}(X_5)= .03185$ . These were found by running experiments in C++. I provide these so you can test your formulas against the actual values. My formula in display $(4)$ will give you values $\mathbb{E}(X_2)= \frac{1}{3}$ , $\mathbb{E}(X_3)= \frac{1}{7}$ , $\mathbb{E}(X_4)= \frac{1}{15}$ , and $\mathbb{E}(X_5)= \frac{1}{31}$ . Note I really don't care about $\mathbb{E}(X_1)$ , so my rules, formula, and data may not all match up for that case.","I'm trying to find the formula for an expected value, but testing has shown that my formula is incorrect. Can you find either a closed form or recursive formula for the expected values in this experiment, and explain how you found that formula? The Game You have coins labelled through . We will label the set of coins . We have a function that maps coin to the probability coin lands heads. A coin landing heads is a success. A coin landing tails is a failure. We start with flipping coin . If coin lands heads, we move on to coin . If coin lands tails, we start over with coin 1. On coin , if coin lands heads, we move on to coin . If coin lands tails, we return to coin . If we land heads on coin , we win and the game ends. The Variables and Counters We will be keeping track of variables . Each of these will count some, but not all, of the times we fail coin . To make the explanation easier, we will treat these as counters in an algorithm, and their final values will be the random variables. If we flip heads on coin , we do nothing. If we flip tails on coin , one of two things happens. If the last coin we failed, , appeared later than (that is, ), then we reset all counters to and then set . If the last coin we failed, , appeared earlier than or is equal to (that is, ), then we increment by and move back to coin as described in the rules. Edit: If we have not failed a coin previously, then we do . That is, we increment by and move back to coin without changing any other counters. The Problem We want to find the expected value of each after successfully flipping heads on coin . My Solution It's hard to put my solution in math terms, because my solution is wrong. I write the expansion My intuition then leads me to say The above, , is probably wrong, but here is the intuition. represents the probability of an extra failed flip of coin at a particular point in the game followed by successful coin-flips on coins through represented by the term . For notational convenience, we set . We can then use to find that and then Displays and are just algebra, and even if there is a typo, I'm fairly certain that is not where the problem lies. Data If and for all , then some approximations of the expected values are , , , , and . These were found by running experiments in C++. I provide these so you can test your formulas against the actual values. My formula in display will give you values , , , and . Note I really don't care about , so my rules, formula, and data may not all match up for that case.","n 1 n [n] f i i 1 1 2 1 i i i+1 i 1 n n \{ X_1, X_2, \ldots , X_n \} n i i j i j > i \{ X_1, X_2, \ldots , X_n \} 0 X_i=1 j i j \leq i X_i 1 1 (2) X_i 1 1 X_i n E(X_i) = 0*P(X_i = 0) + 1*P(X_i = 1) +  2*P(X_i = 2) + 3*P(X_i = 3) + \ldots \text{.}\tag{1} P(X_i=k+1) = P(X_i=k)*(1-f(i))*\prod_{j = 1}^{j=i-1}f(j)\text{.}\tag{2} (2) (1-f(i)) i 1 i-1 \prod_{j = 1}^{j=i-1}f(j) x=(1-f(i))*\prod_{j = 1}^{j=i-1}f(j) P(X_i =0) + P(X_i=1) + P(X_i=2) +\ldots = 1 P(X_i=k)= (1-x)*x^{k} \tag{3} \mathbb{E}(X_i) = \frac{x}{1-x}\text{.}\tag{4} (3) (4) n=5 f(i)=.5 i \mathbb{E}(X_1) = 1.56902 \mathbb{E}(X_2)= .4036 \mathbb{E}(X_3)= .15541 \mathbb{E}(X_4)= .06952 \mathbb{E}(X_5)= .03185 (4) \mathbb{E}(X_2)= \frac{1}{3} \mathbb{E}(X_3)= \frac{1}{7} \mathbb{E}(X_4)= \frac{1}{15} \mathbb{E}(X_5)= \frac{1}{31} \mathbb{E}(X_1)","['probability', 'expected-value']"
39,Size of the vocabulary in Laplace smoothing for a trigram language model,Size of the vocabulary in Laplace smoothing for a trigram language model,,"Let's say we have a text document with $N$ unique words making up a vocabulary $V$, $|V| = N$. For a bigram language model with add-one smoothing, we define a conditional probability of any word $w_{i}$ given  the preceeding word $w_{i-1}$ as: $$P(w_{i}|w_{i-1}) = \frac{count(w_{i-1}w_{i}) + 1}{count(w_{i-1}) + |V|}$$ As far as I understand (or not) the conditional probability, and basing on a 3rd point of this Wikipedia article , $w_{i-1}$ might be assumed to be ""constant"" here, so by summing this expression for all possible $w_{i}$ we should obtain 1, and so it is, which is obvious. However, I do not understand the answers given for this question saying that for n-gram model the size of the vocabulary should be the count of the unique (n-1)-grams occuring in a document, for example, given a 3-gram model (let $V_{2}$ be the dictionary of bigrams): $$P(w_{i}|w_{i-2}w_{i-1}) = \frac{count(w_{i-2}w_{i-1}w_{i}) + 1}{count(w_{i-2}w_{i-1}) + |V_{2}|}$$ It just doesn't add up to 1 when we try to sum it for every possible $w_{i}$. Therefore - should the $|V|$ really be equal to the count of unique (n-1)-grams given an n-gram language model or should it be the count of unique unigrams?","Let's say we have a text document with $N$ unique words making up a vocabulary $V$, $|V| = N$. For a bigram language model with add-one smoothing, we define a conditional probability of any word $w_{i}$ given  the preceeding word $w_{i-1}$ as: $$P(w_{i}|w_{i-1}) = \frac{count(w_{i-1}w_{i}) + 1}{count(w_{i-1}) + |V|}$$ As far as I understand (or not) the conditional probability, and basing on a 3rd point of this Wikipedia article , $w_{i-1}$ might be assumed to be ""constant"" here, so by summing this expression for all possible $w_{i}$ we should obtain 1, and so it is, which is obvious. However, I do not understand the answers given for this question saying that for n-gram model the size of the vocabulary should be the count of the unique (n-1)-grams occuring in a document, for example, given a 3-gram model (let $V_{2}$ be the dictionary of bigrams): $$P(w_{i}|w_{i-2}w_{i-1}) = \frac{count(w_{i-2}w_{i-1}w_{i}) + 1}{count(w_{i-2}w_{i-1}) + |V_{2}|}$$ It just doesn't add up to 1 when we try to sum it for every possible $w_{i}$. Therefore - should the $|V|$ really be equal to the count of unique (n-1)-grams given an n-gram language model or should it be the count of unique unigrams?",,"['probability', 'machine-learning']"
40,Why are linear combinations of independent standard normal random variables also normally distributed?,Why are linear combinations of independent standard normal random variables also normally distributed?,,"My professor has given a list of questions that will not be appearing on my test, with this being one of them. I still feel this is extremely important to understand. How can I prove the following If $X$ and $Y$ are independent, standard normal random variables, then the   linear combination $aX+bY,\;\forall a,b>0$ is also   normally distributed. If I am not mistaken, I believe I can find the distribution of the linear combination If we let $Z=aX+bY$, knowing $X,Y \sim N(0,1)$, we can find the expectation and variance as   $$\mathbb{E}(Z)=\mathbb{E}(aX+bY)=a\mathbb{E}(X)+b\mathbb{E}(Y)=0$$   $$Var(Z)=Var(aX+bY)=a^2Var(X)+b^2Var(Y)=a^2+b^2$$   $$$$   Thus, $Z \sim N(0,a^2+b^2)$. I just don't think this proves that linear combination is normally distributed. I tried looking in some reference books that my professor reserved at the library, but they all just state the fact and I can't figure out how to prove it.","My professor has given a list of questions that will not be appearing on my test, with this being one of them. I still feel this is extremely important to understand. How can I prove the following If $X$ and $Y$ are independent, standard normal random variables, then the   linear combination $aX+bY,\;\forall a,b>0$ is also   normally distributed. If I am not mistaken, I believe I can find the distribution of the linear combination If we let $Z=aX+bY$, knowing $X,Y \sim N(0,1)$, we can find the expectation and variance as   $$\mathbb{E}(Z)=\mathbb{E}(aX+bY)=a\mathbb{E}(X)+b\mathbb{E}(Y)=0$$   $$Var(Z)=Var(aX+bY)=a^2Var(X)+b^2Var(Y)=a^2+b^2$$   $$$$   Thus, $Z \sim N(0,a^2+b^2)$. I just don't think this proves that linear combination is normally distributed. I tried looking in some reference books that my professor reserved at the library, but they all just state the fact and I can't figure out how to prove it.",,"['probability', 'probability-theory', 'probability-distributions', 'proof-writing']"
41,Intuition on Harris recurrence,Intuition on Harris recurrence,,"I am trying to get some intuition on Harris recurrence in Markov chains. Define state space $\mathcal S$ comprising a single communication class, $f_{ii}^{(n)}=P(X_n=i, X_{n-1}\ne i,\ldots X_1\ne i\mid X_0=i)$, $f_{ii}=\sum_n f_{ii}^{(n)}$, $T_{ii}=\inf_n \{X_n=i\mid X_0=i\}$ and $E(T_{ii})=\sum_n nf_{ii}^{(n)}$ and $V_i=\sum_n\mathbb  1_{X_n=i}$, we have the following. Transience : $f_{ii}<1$ Null recurrence :$f_{ii}=1$, $E(T_{ii})=\infty$ Positive recurrence : $f_{ii}=1$, $E(T_{ii})<\infty$, $E(V_i)=\infty\ \forall i\in \mathcal S$ Harris recurrence : $f_{ii}=1$, $P(\omega:V_i(\omega)=\infty)=1\ \forall i\in \mathcal S$ Are the above relations correct? I do not see how the last bullet relates to the definition in Wikipedia . Are there any examples of finite Markov chains that are positive but not Harris recurrent?","I am trying to get some intuition on Harris recurrence in Markov chains. Define state space $\mathcal S$ comprising a single communication class, $f_{ii}^{(n)}=P(X_n=i, X_{n-1}\ne i,\ldots X_1\ne i\mid X_0=i)$, $f_{ii}=\sum_n f_{ii}^{(n)}$, $T_{ii}=\inf_n \{X_n=i\mid X_0=i\}$ and $E(T_{ii})=\sum_n nf_{ii}^{(n)}$ and $V_i=\sum_n\mathbb  1_{X_n=i}$, we have the following. Transience : $f_{ii}<1$ Null recurrence :$f_{ii}=1$, $E(T_{ii})=\infty$ Positive recurrence : $f_{ii}=1$, $E(T_{ii})<\infty$, $E(V_i)=\infty\ \forall i\in \mathcal S$ Harris recurrence : $f_{ii}=1$, $P(\omega:V_i(\omega)=\infty)=1\ \forall i\in \mathcal S$ Are the above relations correct? I do not see how the last bullet relates to the definition in Wikipedia . Are there any examples of finite Markov chains that are positive but not Harris recurrent?",,"['probability', 'stochastic-processes', 'markov-chains']"
42,Random sum of random variables,Random sum of random variables,,"Say you sum i.i.d. variables $X_i$  a total of $Y$ times.  If you know the distribution of random variables $Y$ and $X_i$, what is the calculation you have to do to get the distribution of the sum?","Say you sum i.i.d. variables $X_i$  a total of $Y$ times.  If you know the distribution of random variables $Y$ and $X_i$, what is the calculation you have to do to get the distribution of the sum?",,"['probability', 'summation', 'random-variables']"
43,How can I prove this bijection between random walks?,How can I prove this bijection between random walks?,,"Let $R_n$ be the set of simple random walk paths such that $S_n=0.$ $P_n$ be the set of simple random walk paths such that $\forall i \in \{1,2,...,n\},$ $S_i > 0$ . $N_n$ be the set of paths such that $\forall i \in \{1,2,...,n\}, S_i \geq 0$ . Assume that all random walk paths start at the origin. How can I show that there is a bijection between $P_{2n}$ and $N_{2n-1}$ and that there is a bijection between $R_{2n}$ and $N_{2n}$ . Basically I want to show that a path in $R_{2n}$ with minimum value $k$ corresponds to a path in $N_{2n}$ with terminal value $2k$ . For this I'm thinking about cutting or shifting or reflecting paths. I don't think probability matters here. But I'm stuck on formulating the proofs. If we have sequence $S_0,S_1,...,S_n$ which is represented by a polygonal line with segments $(k-1,S_{k-1}) \rightarrow (k,S_k)$ a path is a polygonal line that is a possible outcome of simple random walk.",Let be the set of simple random walk paths such that be the set of simple random walk paths such that . be the set of paths such that . Assume that all random walk paths start at the origin. How can I show that there is a bijection between and and that there is a bijection between and . Basically I want to show that a path in with minimum value corresponds to a path in with terminal value . For this I'm thinking about cutting or shifting or reflecting paths. I don't think probability matters here. But I'm stuck on formulating the proofs. If we have sequence which is represented by a polygonal line with segments a path is a polygonal line that is a possible outcome of simple random walk.,"R_n S_n=0. P_n \forall i \in \{1,2,...,n\}, S_i > 0 N_n \forall i \in \{1,2,...,n\}, S_i \geq 0 P_{2n} N_{2n-1} R_{2n} N_{2n} R_{2n} k N_{2n} 2k S_0,S_1,...,S_n (k-1,S_{k-1}) \rightarrow (k,S_k)","['probability', 'probability-theory', 'random-walk']"
44,Is this condition enough to determine a random variable?,Is this condition enough to determine a random variable?,,"For two positive random variables X,Y, we know that if $E[X^r]=E[Y^r]$ for any r holds, the cdf of X and Y can still be different. Then it occur to me what about changing $r\in \mathbb{N}$ to $r\in \mathbb{R+}$? Is this enough to determine a r.v.?","For two positive random variables X,Y, we know that if $E[X^r]=E[Y^r]$ for any r holds, the cdf of X and Y can still be different. Then it occur to me what about changing $r\in \mathbb{N}$ to $r\in \mathbb{R+}$? Is this enough to determine a r.v.?",,"['probability', 'probability-theory', 'statistics']"
45,How to find $E(f(f(f(\ldots f(x)))$,How to find,E(f(f(f(\ldots f(x))),"I have a random function $f(x)$ which returns one of the integers in the range $[0, x-1]$ with equal probability and $f(0) = 0$. What is the expected value $E(f(f(f(\ldots f(x)))$ ($n$-times $f(x)$)? The answer should be a function of $x$ and $n$.","I have a random function $f(x)$ which returns one of the integers in the range $[0, x-1]$ with equal probability and $f(0) = 0$. What is the expected value $E(f(f(f(\ldots f(x)))$ ($n$-times $f(x)$)? The answer should be a function of $x$ and $n$.",,['probability']
46,Hyper Birthday Paradox?,Hyper Birthday Paradox?,,"There are $N$ buckets. Each second we add one new ball to a random bucket - so at $t=k$, there are a total of $k$ balls collectively in the buckets. At $t=1$, we expect that at least one bucket contains one ball. At $t=\sqrt{2N\ln{2}}$, due to birthday paradox, we expect that at least one bucket contains two balls. . . At $t=f(m)$, we expect at least one bucket to contain $m$ balls. What is the function $f(m)$?","There are $N$ buckets. Each second we add one new ball to a random bucket - so at $t=k$, there are a total of $k$ balls collectively in the buckets. At $t=1$, we expect that at least one bucket contains one ball. At $t=\sqrt{2N\ln{2}}$, due to birthday paradox, we expect that at least one bucket contains two balls. . . At $t=f(m)$, we expect at least one bucket to contain $m$ balls. What is the function $f(m)$?",,"['probability', 'combinatorics', 'statistics', 'balls-in-bins', 'birthday']"
47,Help with a Bollobás proof - Switching between random graph models,Help with a Bollobás proof - Switching between random graph models,,"I'm trying to make my way through Bollobás' book 'Models of Random Graphs', and unfortunately I've come entirely unstuck on one of his typical 2-line ""and of course, this is entirely trivial""-style proofs. Despite spending many hours staring at it in vain, I have progressed precisely nowhere and was hoping someone could explain what's going on to me in as much detail as they can possibly summon the energy to give, so that I might finally understand it. The book's theorem (Theorem 2.2) is about switching between the $\mathcal{G}(n,p)$ and $\mathcal{G}(n,m)$ models of random graphs, and goes as follows: Theorem: (i) Let Q be any graph property and suppose $pqN \to \infty$ (where we have $n$ vertices and $N={n \choose 2}$ possible edges). Then the following 2 assertions are equivalent (my comments in italics): a) Almost every graph in $\mathcal{G}(n,p)$ has Q. (Here $\mathcal{G}(n,p)$ represents the probability space of random graphs on $n$ vertices with edges distributed randomly with probability $p$ , i.e. edge-number binomially distributed $\operatorname{Bin} (N,p)$ ) b) Given $x>0$ and $\epsilon > 0$ , if $n$ is sufficiently large, there are $l \geq (1- \epsilon) 2x(pqN)^{1/2}$ integers $M_1,\ldots,\,M_l$ with $pN-x(pqN)^{1/2}<M_1<M_2<\ldots < M_l<pN + x(pqN)^{1/2}$ such that $P_{M_i}(Q)>1-\epsilon$ for every $1 \leq i \leq l$ . Proof: (i) By the De Moivre-Laplace theorem, for every fixed $x > 0$ , $\mathbb{P}_p(|e(G)-pN|<x(pqN)^{-1/2})\sim \Phi(x) - \Phi(-x).$ Since also $\mathbb{P}_p(e(G)=M) = b(M;N,p) < (pqN)^{-1/2}$ for every M, the equivalence follows. So, where to start. With regards to what he did say, I believe the statement of $\Phi$ which we get from the De Moivre-Laplace theorem; I'm happy with that. I can't however see why that probabilty stated is $< (pqN)^{1/2}$ - I can believe it's true, but I can't see the most obvious way to prove it. With regards to the proof of (i) as a whole: I guess the intuitive notion of this part of the theorem is to be able to say 'This property holds iff it holds for lots of graphs close to the mean number of edges', i.e. we can focus on just the behaviour near the mean. So, we do the obvious thing and approximate by a normal distribution, since then we can say lots of nice things in terms of 'number of variances away from the mean'. Sadly though, I can't even figure out which direction he's trying to deal with in his proof, let alone fill in the gaps. We know roughly how likely we are to land within $x(Npq)^{1/2}$ of the mean, and we know an upper bound for the probability of getting $M$ edges: is that really sufficient to just say ""result follows""? So I think that's everything - sorry for the substantial length of this question, if there's anything you think I can remove for brevity then I'll be happy to. As above, I've spent hours and hours getting nowhere with what's meant to be a pretty straightforward theorem here, the material is new to me so maybe that's why, but what I'd be extremely grateful for is a very thorough (and fairly basic if possible) explanation of what's going on here, what I'm missing in the proof and why it is indeed true. Sincere thanks for your help in advance. (Edit: removed second half of theorem due to misunderstanding)","I'm trying to make my way through Bollobás' book 'Models of Random Graphs', and unfortunately I've come entirely unstuck on one of his typical 2-line ""and of course, this is entirely trivial""-style proofs. Despite spending many hours staring at it in vain, I have progressed precisely nowhere and was hoping someone could explain what's going on to me in as much detail as they can possibly summon the energy to give, so that I might finally understand it. The book's theorem (Theorem 2.2) is about switching between the and models of random graphs, and goes as follows: Theorem: (i) Let Q be any graph property and suppose (where we have vertices and possible edges). Then the following 2 assertions are equivalent (my comments in italics): a) Almost every graph in has Q. (Here represents the probability space of random graphs on vertices with edges distributed randomly with probability , i.e. edge-number binomially distributed ) b) Given and , if is sufficiently large, there are integers with such that for every . Proof: (i) By the De Moivre-Laplace theorem, for every fixed , Since also for every M, the equivalence follows. So, where to start. With regards to what he did say, I believe the statement of which we get from the De Moivre-Laplace theorem; I'm happy with that. I can't however see why that probabilty stated is - I can believe it's true, but I can't see the most obvious way to prove it. With regards to the proof of (i) as a whole: I guess the intuitive notion of this part of the theorem is to be able to say 'This property holds iff it holds for lots of graphs close to the mean number of edges', i.e. we can focus on just the behaviour near the mean. So, we do the obvious thing and approximate by a normal distribution, since then we can say lots of nice things in terms of 'number of variances away from the mean'. Sadly though, I can't even figure out which direction he's trying to deal with in his proof, let alone fill in the gaps. We know roughly how likely we are to land within of the mean, and we know an upper bound for the probability of getting edges: is that really sufficient to just say ""result follows""? So I think that's everything - sorry for the substantial length of this question, if there's anything you think I can remove for brevity then I'll be happy to. As above, I've spent hours and hours getting nowhere with what's meant to be a pretty straightforward theorem here, the material is new to me so maybe that's why, but what I'd be extremely grateful for is a very thorough (and fairly basic if possible) explanation of what's going on here, what I'm missing in the proof and why it is indeed true. Sincere thanks for your help in advance. (Edit: removed second half of theorem due to misunderstanding)","\mathcal{G}(n,p) \mathcal{G}(n,m) pqN \to \infty n N={n \choose 2} \mathcal{G}(n,p) \mathcal{G}(n,p) n p \operatorname{Bin} (N,p) x>0 \epsilon > 0 n l \geq (1- \epsilon) 2x(pqN)^{1/2} M_1,\ldots,\,M_l pN-x(pqN)^{1/2}<M_1<M_2<\ldots < M_l<pN + x(pqN)^{1/2} P_{M_i}(Q)>1-\epsilon 1 \leq i \leq l x > 0 \mathbb{P}_p(|e(G)-pN|<x(pqN)^{-1/2})\sim \Phi(x) - \Phi(-x). \mathbb{P}_p(e(G)=M) = b(M;N,p) < (pqN)^{-1/2} \Phi < (pqN)^{1/2} x(Npq)^{1/2} M","['probability', 'graph-theory', 'probability-distributions']"
48,Calculating growth rate of a population of Minecraft chickens,Calculating growth rate of a population of Minecraft chickens,,"I have a rather strange question (for this Stack Exchange anyway). It felt too mathematical to ask elsewhere. If this is out of place here, please let me know. A chicken in Minecraft lays eggs; the time between layings is uniformly distributed between 5 and 10 minutes in intervals of 0.05 seconds. An egg, when thrown, produces one chick with probability $\frac3{32}$ or four chicks with probability $\frac1{32}$ , and is destroyed afterwards regardless. A chick matures into an egg-laying chicken in 20 minutes. Assuming eggs are immediately thrown upon laying, how can I estimate the number of chickens after $X$ minutes starting with 1 chicken (that is at the start of its egg-laying cycle)? Chickens are immortal. I can figure most of it out myself, but the thing that's giving me the most trouble is the last bit. I don't know how to take into account the time delay between an egg hatching and the chick growing up.","I have a rather strange question (for this Stack Exchange anyway). It felt too mathematical to ask elsewhere. If this is out of place here, please let me know. A chicken in Minecraft lays eggs; the time between layings is uniformly distributed between 5 and 10 minutes in intervals of 0.05 seconds. An egg, when thrown, produces one chick with probability or four chicks with probability , and is destroyed afterwards regardless. A chick matures into an egg-laying chicken in 20 minutes. Assuming eggs are immediately thrown upon laying, how can I estimate the number of chickens after minutes starting with 1 chicken (that is at the start of its egg-laying cycle)? Chickens are immortal. I can figure most of it out myself, but the thing that's giving me the most trouble is the last bit. I don't know how to take into account the time delay between an egg hatching and the chick growing up.",\frac3{32} \frac1{32} X,"['probability', 'sequences-and-series', 'recurrence-relations', 'random-variables', 'exponential-function']"
49,Expected values of some properties of the convex hull of a random set of points,Expected values of some properties of the convex hull of a random set of points,,"$N$ points are selected in a uniformly distributed random way in a disk of the unit radius. Let $P(N)$ and $A(N)$ denote the expected perimeter and the expected area of their convex hull . For what $N$ do we know the exact values of $P(N)$, $A(N)$? Is there a general formula for $P(N)$ or $A(N)$? What is the asymptotic behavior of $P(N)$ and $A(N)$ as $N\to\infty$? $N$ points are selected in a uniformly distributed random way in a ball of the unit radius. Let $S(N)$, $V(N)$ denote the expected surface area and the expected volume of their convex hull. For what $N$ do we know the exact values of $S(N)$, $V(N)$? Is there a general formula for $S(N)$ or $V(N)$? What is the asymptotic behavior of $S(N)$ and $V(N)$ as $N\to\infty$?","$N$ points are selected in a uniformly distributed random way in a disk of the unit radius. Let $P(N)$ and $A(N)$ denote the expected perimeter and the expected area of their convex hull . For what $N$ do we know the exact values of $P(N)$, $A(N)$? Is there a general formula for $P(N)$ or $A(N)$? What is the asymptotic behavior of $P(N)$ and $A(N)$ as $N\to\infty$? $N$ points are selected in a uniformly distributed random way in a ball of the unit radius. Let $S(N)$, $V(N)$ denote the expected surface area and the expected volume of their convex hull. For what $N$ do we know the exact values of $S(N)$, $V(N)$? Is there a general formula for $S(N)$ or $V(N)$? What is the asymptotic behavior of $S(N)$ and $V(N)$ as $N\to\infty$?",,"['probability', 'asymptotics', 'convex-analysis', 'combinatorial-geometry', 'geometric-probability']"
50,Probability of finding at least $k$ consecutive heads in $N$ coin tosses?,Probability of finding at least  consecutive heads in  coin tosses?,k N,There are quite a few topics on this question already but I couldn't find a well-explained solution. Please point me towards some relevant literature or theory to analyze this problem. $K$ consecutive heads with a biased coin? Probability of tossing a biased coin without having k heads consecutively in a row Probability of tossing a fair coin with at least $k$ consecutive heads Thanks.,There are quite a few topics on this question already but I couldn't find a well-explained solution. Please point me towards some relevant literature or theory to analyze this problem. $K$ consecutive heads with a biased coin? Probability of tossing a biased coin without having k heads consecutively in a row Probability of tossing a fair coin with at least $k$ consecutive heads Thanks.,,"['probability', 'reference-request']"
51,Expected number of operations on a vector until one of the coordinates becomes zero.,Expected number of operations on a vector until one of the coordinates becomes zero.,,"Let's say we have a vector $v = (x_1, ..., x_n) \in \mathbb{N}^n$ where $x_1 = x_2 = ... = x_n$. Next we choose an ordered pair of coordinates at random $(i, j)$ where $i, j \in \{1, ..., n\}$ and $i \neq j$. Finally we substitute the vector $v$ with a new vector $v' = (x_1, ..., x_i + 1, ..., x_j - 1, ..., x_n)$. Now we choose again an ordered pair of coordinates at random and substitute the vector $v'$ with a new vector doing the same we did for $v$. We continue doing this until one of the coordinates becomes zero. What is the expected number of operations we are going to make? I know the answer for $n = 2$ because you can model this process with a random walk. If $v = (x, x)$, then the expected number of operations is the same as the expected number of steps it will take to hit $x$ or $-x$ doing a random walk starting at zero. In this case the expected number of step starting at $y$ satisfies the recurrence relation $$E_y = 1 + \frac{1}{2} E_{y - 1} +  \frac{1}{2} E_{y + 1}. $$ Then one can solve this linear recurrence. I tried to do the same for the original problem but the recurrence relation is more difficult. Let $F_{(x_1, ..., x_n)}$ be the expected number of operations one can make to vector $v= (x_1, ..., x_n)$ before one of the coordinates becomes zero (in this case we allow $x_1, ..., x_n$ to be different). If I'm not wrong $F$ satisfies the following relation $$F_{(x_1, ..., x_n)} = 1 + \sum_{i, j} \frac{1}{n (n - 1)} F_{(x_1, ..., x_n) + e_{i, j}}, $$ where the $i$-th coordiante of $e_{i, j}$ is $1$, the $j$-th is $-1$ and the rest are all zero (the sum runs through all possible operations).","Let's say we have a vector $v = (x_1, ..., x_n) \in \mathbb{N}^n$ where $x_1 = x_2 = ... = x_n$. Next we choose an ordered pair of coordinates at random $(i, j)$ where $i, j \in \{1, ..., n\}$ and $i \neq j$. Finally we substitute the vector $v$ with a new vector $v' = (x_1, ..., x_i + 1, ..., x_j - 1, ..., x_n)$. Now we choose again an ordered pair of coordinates at random and substitute the vector $v'$ with a new vector doing the same we did for $v$. We continue doing this until one of the coordinates becomes zero. What is the expected number of operations we are going to make? I know the answer for $n = 2$ because you can model this process with a random walk. If $v = (x, x)$, then the expected number of operations is the same as the expected number of steps it will take to hit $x$ or $-x$ doing a random walk starting at zero. In this case the expected number of step starting at $y$ satisfies the recurrence relation $$E_y = 1 + \frac{1}{2} E_{y - 1} +  \frac{1}{2} E_{y + 1}. $$ Then one can solve this linear recurrence. I tried to do the same for the original problem but the recurrence relation is more difficult. Let $F_{(x_1, ..., x_n)}$ be the expected number of operations one can make to vector $v= (x_1, ..., x_n)$ before one of the coordinates becomes zero (in this case we allow $x_1, ..., x_n$ to be different). If I'm not wrong $F$ satisfies the following relation $$F_{(x_1, ..., x_n)} = 1 + \sum_{i, j} \frac{1}{n (n - 1)} F_{(x_1, ..., x_n) + e_{i, j}}, $$ where the $i$-th coordiante of $e_{i, j}$ is $1$, the $j$-th is $-1$ and the rest are all zero (the sum runs through all possible operations).",,['probability']
52,How to solve probability when sample space is infinite?,How to solve probability when sample space is infinite?,,"I came up with a random problem yesterday: Suppose that in a random trial, each point $(x,y)$ where $x,y \in \mathbb{R}$ and $0 \leq x,y \leq 1$ is assigned a value of $0$ with 50% chance and a value of $1$ with 50% chance. What is a chance that there is a 'continuous path' that joins the point (0,0) and the point (1,1), such that along the path every single point has the same assigned value? My intuition sort of says that the probability should be zero, but I don't really have a good reason why. For one thing, any particular path from (0,0) to (1,1) should have a 0% chance of occurring, but there are also an infinite number of possible paths from the point (0,0) to (1,1). In my head, the probability should depend on whether the ""zero"" was smaller than the ""infinity"" $$P \simeq \text{num paths} * P(\text{chance a path happens})$$ To evaluate the number of total paths, I sort of imagined constructing paths with straight lines. To construct an arbitrary path between (0,0) and (1,1), I would pick some number of intermediate points inbetween the two corners, and specify their $x$ coordinate and $y$ coordinate. Something like: $$\text{num paths} \simeq \aleph_1^2 + \aleph_1^4 + \cdot\cdot\cdot + \aleph_1^{\aleph_0} \simeq \aleph_1^{\aleph_0}$$ (First term is like I have to pick 2 real numbers for a single intermediate point, second term is if I pick 4 real numbers for two intermediate points, and so on) To evaluate the chance that any particular path happens, it's sort of like saying that for every point along the line, it must be assigned the same value. Hence, its sort of like: $$P(\text{chance a path happens}) \simeq \frac{1}{2}^{\aleph_1}$$ So combining the two, we have (very unrigorously): $$P \simeq \aleph_1^{\aleph_0} * \frac{1}{2}^{\aleph_1}$$ This seems not only super unrigorous, but I'm not even sure if you can even do this kind of thing. My friend's words keep echoing in my head, ""something something probability only means something when the sample space is actually defined properly something something""  Is the problem (as I have posed it) even meaningful in any sense? And does my poor attempt at a solution have any grain of meaningfulness in it?","I came up with a random problem yesterday: Suppose that in a random trial, each point $(x,y)$ where $x,y \in \mathbb{R}$ and $0 \leq x,y \leq 1$ is assigned a value of $0$ with 50% chance and a value of $1$ with 50% chance. What is a chance that there is a 'continuous path' that joins the point (0,0) and the point (1,1), such that along the path every single point has the same assigned value? My intuition sort of says that the probability should be zero, but I don't really have a good reason why. For one thing, any particular path from (0,0) to (1,1) should have a 0% chance of occurring, but there are also an infinite number of possible paths from the point (0,0) to (1,1). In my head, the probability should depend on whether the ""zero"" was smaller than the ""infinity"" $$P \simeq \text{num paths} * P(\text{chance a path happens})$$ To evaluate the number of total paths, I sort of imagined constructing paths with straight lines. To construct an arbitrary path between (0,0) and (1,1), I would pick some number of intermediate points inbetween the two corners, and specify their $x$ coordinate and $y$ coordinate. Something like: $$\text{num paths} \simeq \aleph_1^2 + \aleph_1^4 + \cdot\cdot\cdot + \aleph_1^{\aleph_0} \simeq \aleph_1^{\aleph_0}$$ (First term is like I have to pick 2 real numbers for a single intermediate point, second term is if I pick 4 real numbers for two intermediate points, and so on) To evaluate the chance that any particular path happens, it's sort of like saying that for every point along the line, it must be assigned the same value. Hence, its sort of like: $$P(\text{chance a path happens}) \simeq \frac{1}{2}^{\aleph_1}$$ So combining the two, we have (very unrigorously): $$P \simeq \aleph_1^{\aleph_0} * \frac{1}{2}^{\aleph_1}$$ This seems not only super unrigorous, but I'm not even sure if you can even do this kind of thing. My friend's words keep echoing in my head, ""something something probability only means something when the sample space is actually defined properly something something""  Is the problem (as I have posed it) even meaningful in any sense? And does my poor attempt at a solution have any grain of meaningfulness in it?",,"['probability', 'elementary-set-theory', 'cardinals']"
53,The total number of subsets is $2^n$ for $n$ elements,The total number of subsets is  for  elements,2^n n,"In my probability book, it says that to count the total number of subsets of n elements is a process of $n$ stages with binary choice of either adding this element to the subset or not to add it. Therefore, the total number is $$2^n$$ But, for instance, we have 3 elements, according to this formula, there are 2 to the power of 3 elements, namely 8, which are $${\emptyset},A,B,C, AB, AC, BC, ABC$$ However, I have a hard time of imagining the process or N stages binary choice that form this many subsets. Can anyone explain/help me to understand it? I mean, ABC, if we are making the choice of A, put it in or do not put it in, exactly which subset are we choosing to put in or not? Thank you.","In my probability book, it says that to count the total number of subsets of n elements is a process of stages with binary choice of either adding this element to the subset or not to add it. Therefore, the total number is But, for instance, we have 3 elements, according to this formula, there are 2 to the power of 3 elements, namely 8, which are However, I have a hard time of imagining the process or N stages binary choice that form this many subsets. Can anyone explain/help me to understand it? I mean, ABC, if we are making the choice of A, put it in or do not put it in, exactly which subset are we choosing to put in or not? Thank you.","n 2^n {\emptyset},A,B,C, AB, AC, BC, ABC","['probability', 'probability-theory', 'permutations', 'combinations']"
54,Is this lot drawing fair?,Is this lot drawing fair?,,"Sorry for a stupid question, but it is bugging me a lot. Let's say there are $30$ classmates in my class and one of us has to clean the classroom. No one wants to do that. So we decided to draw a lot - thirty pieces of paper in a hat, one of which is with ""X"" on it. The one who draws ""X"" has to do the cleaning. Each one starts to draw... Is this kind of lot drawing fair or not fair? It looks to me like the first one's chances to get an ""X"" are equal to $1/29$, while the second one's chances would be equal either to  $1/28$ (in case the first one didn't draw an ""X"") or zero $0/29 = 0$ (in case the first one drew an ""X""). However, neither $1/28$, nor $0/29$ is equal to $1/29$.","Sorry for a stupid question, but it is bugging me a lot. Let's say there are $30$ classmates in my class and one of us has to clean the classroom. No one wants to do that. So we decided to draw a lot - thirty pieces of paper in a hat, one of which is with ""X"" on it. The one who draws ""X"" has to do the cleaning. Each one starts to draw... Is this kind of lot drawing fair or not fair? It looks to me like the first one's chances to get an ""X"" are equal to $1/29$, while the second one's chances would be equal either to  $1/28$ (in case the first one didn't draw an ""X"") or zero $0/29 = 0$ (in case the first one drew an ""X""). However, neither $1/28$, nor $0/29$ is equal to $1/29$.",,['probability']
55,Can purchase of insurance be justified mathematically? [closed],Can purchase of insurance be justified mathematically? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question When I ask people to explain why they buy insurance, I often hear vaguely of ""spreading the risk"", but I am not actually sure what that means nor if insurance does this. How is an insurance company any different than a casino? In a thought experiment where some large number of people who purchase insurance are compared against an equal number of people who do not, it seems to me when one takes into account the cost of insurance, the people who do not purchase it end up better financially than those who do not. It is argued that insurance is needed to protect against catastrophic events but isn't poverty in old age a catastrophic event also? I realize that these are not strictly mathematical questions but at its base, insurance must be either a good or bad choice based on statistics and probability. EDIT: More succinctly: Buying insurance is making a bet with a negative expectation. If there is some way to justify this mathematically then are there other bets with negative expectation, like buying lottery tickets or roulette that can be justified and how? EDIT: People are saying, this is not a mathematical question but the question: Is a person likely to be better off financially if the buy insurance is a pretty mathematical question. If you took 100 people and half bought insurance and the other did not, which group would have more money at the end of some period, is mathematical. I can answer this question about any negative-expectation betting, so why is insurance any different?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question When I ask people to explain why they buy insurance, I often hear vaguely of ""spreading the risk"", but I am not actually sure what that means nor if insurance does this. How is an insurance company any different than a casino? In a thought experiment where some large number of people who purchase insurance are compared against an equal number of people who do not, it seems to me when one takes into account the cost of insurance, the people who do not purchase it end up better financially than those who do not. It is argued that insurance is needed to protect against catastrophic events but isn't poverty in old age a catastrophic event also? I realize that these are not strictly mathematical questions but at its base, insurance must be either a good or bad choice based on statistics and probability. EDIT: More succinctly: Buying insurance is making a bet with a negative expectation. If there is some way to justify this mathematically then are there other bets with negative expectation, like buying lottery tickets or roulette that can be justified and how? EDIT: People are saying, this is not a mathematical question but the question: Is a person likely to be better off financially if the buy insurance is a pretty mathematical question. If you took 100 people and half bought insurance and the other did not, which group would have more money at the end of some period, is mathematical. I can answer this question about any negative-expectation betting, so why is insurance any different?",,"['probability', 'finance', 'actuarial-science']"
56,What is the probability that two random subsets of a superset have no intersection?,What is the probability that two random subsets of a superset have no intersection?,,"Let $$ S={1,2,...,n} $$ From the power set, $ P(S) $ , of $S$ , two subsets $A$ , $B$ are chosen at random. If each subset is equally likely to be chosen, then, the probability that the sets $ A $ and $ B $ have no elements in common is.... Here are some of my tries: Probability that two particular sets will be chosen is $ 2^{-2n} $ .  If from one set $\phi$ is chosen, no intersection, is guaranteed, if a Singelton is chosen from first set then if $ \phi $ is chosen from second set, then no intersection is guaranteed if some set with cardinality = 1 is chosen from second set then no intersection guaranteed for all $n$ sets except for 1 which is the same as first set. if some set with cardinality = 2 is chosen from second set then for no intersection guaranteed for all $^{n}C _2 $ sets except for $n-1$ which contain the singleton in first set. if some set with cardinality = 3 is chosen from second set then for no intersection guaranteed for all $^ {n}C _2 $ sets except for???? I can't think further.  I can't figure out a general term for $n$ cardinality. So far, I have figured. $ P(A\bigcap B = \phi) =  2^{-2n} [^{n}C_0 * 1 + ^{n}C_1(^{n}C_0 * 1 + ^{n}C_1 -1 + ^{n}C_2 -(n-1) ..... ) + ^{n}C_2(....) .......^{n}C_n(....) ] $ I have tried finding complement of the question (finding probability that they have at least something in common) I have tried using de Morgans law, $ P(A\bigcap B = \phi) =  P((A^c \bigcup B^c)^c) = \phi) $ but I can't figure it out. I created a python program to find probability however computer cant calculate probabilities for $n >= 25$ and the result does not seem to have converged. At $ n = 25 $ , probability is around $0.001$ . I have looked at other similar questions however they don't deal with subsets of superset but  some simple subset of simple sets. The probability that two randomly selected subsets of the set $\{1 , 2, 3, 4 , 5\}$ have exactly two elements in their intersection , is","Let From the power set, , of , two subsets , are chosen at random. If each subset is equally likely to be chosen, then, the probability that the sets and have no elements in common is.... Here are some of my tries: Probability that two particular sets will be chosen is .  If from one set is chosen, no intersection, is guaranteed, if a Singelton is chosen from first set then if is chosen from second set, then no intersection is guaranteed if some set with cardinality = 1 is chosen from second set then no intersection guaranteed for all sets except for 1 which is the same as first set. if some set with cardinality = 2 is chosen from second set then for no intersection guaranteed for all sets except for which contain the singleton in first set. if some set with cardinality = 3 is chosen from second set then for no intersection guaranteed for all sets except for???? I can't think further.  I can't figure out a general term for cardinality. So far, I have figured. I have tried finding complement of the question (finding probability that they have at least something in common) I have tried using de Morgans law, but I can't figure it out. I created a python program to find probability however computer cant calculate probabilities for and the result does not seem to have converged. At , probability is around . I have looked at other similar questions however they don't deal with subsets of superset but  some simple subset of simple sets. The probability that two randomly selected subsets of the set $\{1 , 2, 3, 4 , 5\}$ have exactly two elements in their intersection , is"," S={1,2,...,n}   P(S)  S A B  A   B   2^{-2n}  \phi  \phi  n ^{n}C _2  n-1 ^ {n}C _2  n  P(A\bigcap B = \phi) =  2^{-2n} [^{n}C_0 * 1 + ^{n}C_1(^{n}C_0 * 1 + ^{n}C_1 -1 + ^{n}C_2 -(n-1) ..... ) + ^{n}C_2(....) .......^{n}C_n(....) ]   P(A\bigcap B = \phi) =  P((A^c \bigcup B^c)^c) = \phi)  n >= 25  n = 25  0.001","['probability', 'measure-theory']"
57,"You have 3 cakes. Everytime you eat one, there's 17% chance the number of cakes is reset to 3. Find average number of cakes eaten?","You have 3 cakes. Everytime you eat one, there's 17% chance the number of cakes is reset to 3. Find average number of cakes eaten?",,"I did a Python simulation and the answer is 4.40. But I think there should be a theoretical approach for this problem. I remember having seen variations of it mutiple times but don't know the technical terms for it. If anyone is interested, here's my simulation: def reset_to_original(starting_number, reset_probability):     '''     Given <starting_number>, countdown to zero. At each turn, there's a <reset_probability>      that the number of remaining turns will be reset to <starting_number>.      Return the total number of turns.     '''     import random     remaining = starting_number     total_count = 0     while remaining:         remaining -= 1         total_count += 1         is_reset = random.random() < reset_probability         if is_reset: remaining = starting_number     return total_count   def main(num, starting_number, reset_probability):     total = 0.0     for i in range(num):         total += reset_to_original(starting_number, reset_probability)     print('Average: {}'.format(total / num))   main(pow(10, 6), 3, 0.17) Can someone guide me through calculating it theoretically?","I did a Python simulation and the answer is 4.40. But I think there should be a theoretical approach for this problem. I remember having seen variations of it mutiple times but don't know the technical terms for it. If anyone is interested, here's my simulation: def reset_to_original(starting_number, reset_probability):     '''     Given <starting_number>, countdown to zero. At each turn, there's a <reset_probability>      that the number of remaining turns will be reset to <starting_number>.      Return the total number of turns.     '''     import random     remaining = starting_number     total_count = 0     while remaining:         remaining -= 1         total_count += 1         is_reset = random.random() < reset_probability         if is_reset: remaining = starting_number     return total_count   def main(num, starting_number, reset_probability):     total = 0.0     for i in range(num):         total += reset_to_original(starting_number, reset_probability)     print('Average: {}'.format(total / num))   main(pow(10, 6), 3, 0.17) Can someone guide me through calculating it theoretically?",,['probability']
58,Odds of Winning the Lottery Using the Same Numbers Repeatedly Better/Worse?,Odds of Winning the Lottery Using the Same Numbers Repeatedly Better/Worse?,,"Does the probability of winning the lottery differ between randomly generated numbers vs. selecting the same numbers every time? Specifically. I'm interested in a breakdown of the odds per number for a given set of numbers that comprise a single US Powerball drawing (five white numbers plus the one powerball number), and how they arrive at the odds seen here: http://www.powerball.com/powerball/pb_prizes.asp Tying that back to my original question, I was interested if playing the same numbers every drawing changes those odds.","Does the probability of winning the lottery differ between randomly generated numbers vs. selecting the same numbers every time? Specifically. I'm interested in a breakdown of the odds per number for a given set of numbers that comprise a single US Powerball drawing (five white numbers plus the one powerball number), and how they arrive at the odds seen here: http://www.powerball.com/powerball/pb_prizes.asp Tying that back to my original question, I was interested if playing the same numbers every drawing changes those odds.",,['probability']
59,"A king has n children, at least one of them is a daughter. What’s the probability that all of them are daughters?","A king has n children, at least one of them is a daughter. What’s the probability that all of them are daughters?",,"A king has n children, at least one of them is a daughter. What’s the probability that all of them are daughters? So far I've considered the case of 3 children, which gives a probability of 1/7. But I'm confused about how to generalise this?","A king has n children, at least one of them is a daughter. What’s the probability that all of them are daughters? So far I've considered the case of 3 children, which gives a probability of 1/7. But I'm confused about how to generalise this?",,['probability']
60,The definition of independence is not intuitive,The definition of independence is not intuitive,,"In the book ""Introduction to Probability"" by J. Charles M. Grinstead and Laurie Snell independent events are introduced in the following way: ""It often happens that the knowledge that a certain event $E$ has occurred has no effect on the probability that some other event $F$ has occurred, that is, that $P (F |E) =P (F )$"". This is then taken as the definition (notice that the setting in which this is done is that of discrete probabilities - if this makes any difference). But I can't come to terms with this definition, because of the following two reasons: a) If I have a die then the event of getting a face with one of the numbers $1,2$, if rolling once, seems intuitively to be independent of getting a face with one of the numbers $5,6$. But since these two events are disjoint, but neither is the empty space, by the above definition, they should be dependent, which seems very counter-intuitive. b) If we chose to somehow ""interpret visually"" this definition, then that would mean that the sum of the probabilities of all elements in $F$ weighted with the weight $1$ is equal to the sum of all probabilities in $F\cap E$ weighted with $\frac{1}{P(E)}$, because $$\sum_{\omega\in F} \omega=P (F )=P (F |E) =\frac{P(F\cap E)}{P(E)}=\sum_{\omega\in F\cap E} \omega \cdot \frac{1}{P(E)}.$$ Or - if I use the equivalent definition of independence, $P(E\cap F)=P(E) P(F)$ - then the same interpretation of this definition would say that the sum of all elements in $F$ multiplied the sum of all elements in $E$, both  weighted with the weight $1$,   is again equal to the sum of all probabilities in $F\cap E$ weighted with $\frac{1}{P(E)}$. But both these interpretations seem artificial - out of this I can't deduce, why this definition is calling $E,F$ ""independent"". Could you please solve this ""paradox"" for me ? If I were to give a definition of independent events, I would say that $E,F$ are independent (which, for me, would mean ""they don't have anything to do with each other""), if $E\cap F =\emptyset$. Why would this be a bad definition ?","In the book ""Introduction to Probability"" by J. Charles M. Grinstead and Laurie Snell independent events are introduced in the following way: ""It often happens that the knowledge that a certain event $E$ has occurred has no effect on the probability that some other event $F$ has occurred, that is, that $P (F |E) =P (F )$"". This is then taken as the definition (notice that the setting in which this is done is that of discrete probabilities - if this makes any difference). But I can't come to terms with this definition, because of the following two reasons: a) If I have a die then the event of getting a face with one of the numbers $1,2$, if rolling once, seems intuitively to be independent of getting a face with one of the numbers $5,6$. But since these two events are disjoint, but neither is the empty space, by the above definition, they should be dependent, which seems very counter-intuitive. b) If we chose to somehow ""interpret visually"" this definition, then that would mean that the sum of the probabilities of all elements in $F$ weighted with the weight $1$ is equal to the sum of all probabilities in $F\cap E$ weighted with $\frac{1}{P(E)}$, because $$\sum_{\omega\in F} \omega=P (F )=P (F |E) =\frac{P(F\cap E)}{P(E)}=\sum_{\omega\in F\cap E} \omega \cdot \frac{1}{P(E)}.$$ Or - if I use the equivalent definition of independence, $P(E\cap F)=P(E) P(F)$ - then the same interpretation of this definition would say that the sum of all elements in $F$ multiplied the sum of all elements in $E$, both  weighted with the weight $1$,   is again equal to the sum of all probabilities in $F\cap E$ weighted with $\frac{1}{P(E)}$. But both these interpretations seem artificial - out of this I can't deduce, why this definition is calling $E,F$ ""independent"". Could you please solve this ""paradox"" for me ? If I were to give a definition of independent events, I would say that $E,F$ are independent (which, for me, would mean ""they don't have anything to do with each other""), if $E\cap F =\emptyset$. Why would this be a bad definition ?",,"['probability', 'intuition']"
61,expected value of a sum of a 10 sided die,expected value of a sum of a 10 sided die,,Suppose you have a fair die with 10 sides with numbers from 1 to 10. You roll the die and take the sum until the sum is greater than 100. What is the expected value of this sum?,Suppose you have a fair die with 10 sides with numbers from 1 to 10. You roll the die and take the sum until the sum is greater than 100. What is the expected value of this sum?,,"['probability', 'recreational-mathematics', 'puzzle']"
62,Probability of getting heads given that first flip was a head?,Probability of getting heads given that first flip was a head?,,"What's the probability of getting heads on the second toss given that the first toss was a head.  (Trying to refresh my probability a bit).  I've seen this analyzed like this: HH 1/4 HT 1/4 TH 1/4 TT 1/4 So since we are given information (Head on first flip), then TT goes away and were are left with: HH 1/3 HT 1/3 TH 1/3 So we could say that HH now has a 1/3 probability.  Should we not also get rid of TH, since we know that the first flip is a head?  So now we have: HH 1/2 HT 1/2","What's the probability of getting heads on the second toss given that the first toss was a head.  (Trying to refresh my probability a bit).  I've seen this analyzed like this: HH 1/4 HT 1/4 TH 1/4 TT 1/4 So since we are given information (Head on first flip), then TT goes away and were are left with: HH 1/3 HT 1/3 TH 1/3 So we could say that HH now has a 1/3 probability.  Should we not also get rid of TH, since we know that the first flip is a head?  So now we have: HH 1/2 HT 1/2",,"['probability', 'probability-theory', 'bayesian', 'bayes-theorem']"
63,On the Cramér-Granville Conjecture and finding prime pairs whose difference is 666,On the Cramér-Granville Conjecture and finding prime pairs whose difference is 666,,"Questions If $p= \text{NextPrime}[q]$ (the smallest prime greater than $p$), and $p-q = 666,$ what are $p$ and $q$? (There may be multiple choices. I am interested in finding one.) Cramér-Granville Conjecture: Defining $p_0=2$, and $p_n$ as the nth odd prime, and the nth prime gap as $g_n=p_{n+1}-p_n$, then  $g_n< M \log(p_n)^2$ for some $M>1$. Reference link: http://mathworld.wolfram.com/Cramer-GranvilleConjecture.html If $g_n =666$,  what are generally good estimates for $M$ and $p_n$ according to the Cramér-Granville Conjecture? Can we make use of current sieve technology and probabilistic modeling to solve our problem efficiently? Commentary & Previous Work If $g_n = 666$ and $p_n = 18691113008663$, then the conjecture is satisfied for any $M>1$. We let  $\text{primegap}_{avg} = x/\pi(x) = g_n = 666$.  If $\pi(x) = x/(li(x) + sqrt(x) * log(x)/(8\pi))$, then we have  $x/\pi(x) = 666$ which implies $x = 4.73231\times10^{289}$ and $\pi(x) =7.10558\times10^{286}$.  So our upper bound estimate of $p_n$ will be less than $4.73231\times10^{289}$. Therefore, we shall focus our attention on finding consecutive prime pairs whose difference is $666$ in the open interval, $(18691113009329, 4.73231*10^{289})$. And we should expect to find approximately $(c/333)(\pi(4.73231×10^{289})-\pi(18691113009329)) = c *7.10558×10^{286}/333$, or $c * 2.133808*10^{284}$ consecutive prime pairs whose difference is $666$ where  $0.5 < c < 1$. Note:  $c \to 1$ as $x\to\infty$ according to the Polignac Conjecture. Furthermore, we also expect to discover sufficiently many prime gaps greater than $666$ in the open interval, $(18691113009329, 4.73231×10^{289})$, so that the prime gap density or average of $666$ is maintained.  And according to the Cramér-Granville Conjecture, the maximum prime gap of $\log(4.73231\times10^{289})^2 = 444892$, more or less, exists in the open interval. Reference links https://terrytao.wordpress.com/2009/08/18/the-least-quadratic-nonresidue-and-the-square-root-barrier/#comment-472548 ; http://www.ams.org/journals/mcom/1989-52-185/S0025-5718-1989-0947470-1/S0025-5718-1989-0947470-1.pdf/ ; https://en.wikipedia.org/wiki/Polignac%27s_conjecture ; https://www.quora.com/What-great-conjectures-in-mathematics-combine-additive-theory-of-numbers-with-the-multiplicative-theory-of-numbers/answer/David-Cole-146 ; https://terrytao.files.wordpress.com/2009/08/prime-number-theory1.pdf ; Boeyens, Jan C. A.; Levendis, Demetrius C. (2008), Number Theory and the Periodicity of Matter, Berlin: Springer-Verlag, ISBN 978-1-4020-6659-7; 'A Primer in Density Functional Theory', http://link.springer.com/book/10.1007%2F3-540-37072-2 ; https://www.wired.com/2016/11/physicists-uncover-strange-numbers-particle-collisions/ . “Repetition and growth of prime gaps are essential for the efficient generation of the integers.”","Questions If $p= \text{NextPrime}[q]$ (the smallest prime greater than $p$), and $p-q = 666,$ what are $p$ and $q$? (There may be multiple choices. I am interested in finding one.) Cramér-Granville Conjecture: Defining $p_0=2$, and $p_n$ as the nth odd prime, and the nth prime gap as $g_n=p_{n+1}-p_n$, then  $g_n< M \log(p_n)^2$ for some $M>1$. Reference link: http://mathworld.wolfram.com/Cramer-GranvilleConjecture.html If $g_n =666$,  what are generally good estimates for $M$ and $p_n$ according to the Cramér-Granville Conjecture? Can we make use of current sieve technology and probabilistic modeling to solve our problem efficiently? Commentary & Previous Work If $g_n = 666$ and $p_n = 18691113008663$, then the conjecture is satisfied for any $M>1$. We let  $\text{primegap}_{avg} = x/\pi(x) = g_n = 666$.  If $\pi(x) = x/(li(x) + sqrt(x) * log(x)/(8\pi))$, then we have  $x/\pi(x) = 666$ which implies $x = 4.73231\times10^{289}$ and $\pi(x) =7.10558\times10^{286}$.  So our upper bound estimate of $p_n$ will be less than $4.73231\times10^{289}$. Therefore, we shall focus our attention on finding consecutive prime pairs whose difference is $666$ in the open interval, $(18691113009329, 4.73231*10^{289})$. And we should expect to find approximately $(c/333)(\pi(4.73231×10^{289})-\pi(18691113009329)) = c *7.10558×10^{286}/333$, or $c * 2.133808*10^{284}$ consecutive prime pairs whose difference is $666$ where  $0.5 < c < 1$. Note:  $c \to 1$ as $x\to\infty$ according to the Polignac Conjecture. Furthermore, we also expect to discover sufficiently many prime gaps greater than $666$ in the open interval, $(18691113009329, 4.73231×10^{289})$, so that the prime gap density or average of $666$ is maintained.  And according to the Cramér-Granville Conjecture, the maximum prime gap of $\log(4.73231\times10^{289})^2 = 444892$, more or less, exists in the open interval. Reference links https://terrytao.wordpress.com/2009/08/18/the-least-quadratic-nonresidue-and-the-square-root-barrier/#comment-472548 ; http://www.ams.org/journals/mcom/1989-52-185/S0025-5718-1989-0947470-1/S0025-5718-1989-0947470-1.pdf/ ; https://en.wikipedia.org/wiki/Polignac%27s_conjecture ; https://www.quora.com/What-great-conjectures-in-mathematics-combine-additive-theory-of-numbers-with-the-multiplicative-theory-of-numbers/answer/David-Cole-146 ; https://terrytao.files.wordpress.com/2009/08/prime-number-theory1.pdf ; Boeyens, Jan C. A.; Levendis, Demetrius C. (2008), Number Theory and the Periodicity of Matter, Berlin: Springer-Verlag, ISBN 978-1-4020-6659-7; 'A Primer in Density Functional Theory', http://link.springer.com/book/10.1007%2F3-540-37072-2 ; https://www.wired.com/2016/11/physicists-uncover-strange-numbers-particle-collisions/ . “Repetition and growth of prime gaps are essential for the efficient generation of the integers.”",,"['probability', 'number-theory', 'prime-numbers', 'mathematical-modeling', 'sieve-theory']"
64,Hitting probability of biased random walk on the integer line,Hitting probability of biased random walk on the integer line,,"Lets say we start at point 1.  Each successive point you have a, say, 2/3 chance of increasing your position by 1 and a 1/3 chance of decreasing your position by 1.  The walk ends when you reach 0. The question, what is the probability that you will eventually reach 0? Also, is there any generalization for different probabilities or different starting positions or different rules (say you increase 2 and decrease 1)? NOTE: I have never taken a course that considered random walks.  So, if possible, could no prior knowledge of random walks be assumed?","Lets say we start at point 1.  Each successive point you have a, say, 2/3 chance of increasing your position by 1 and a 1/3 chance of decreasing your position by 1.  The walk ends when you reach 0. The question, what is the probability that you will eventually reach 0? Also, is there any generalization for different probabilities or different starting positions or different rules (say you increase 2 and decrease 1)? NOTE: I have never taken a course that considered random walks.  So, if possible, could no prior knowledge of random walks be assumed?",,"['probability', 'probability-theory', 'random-walk']"
65,"independent, identically distributed (IID) random variables [closed]","independent, identically distributed (IID) random variables [closed]",,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I am having trouble understanding IID random variables. I've tried reading http://scipp.ucsc.edu/~haber/ph116C/iid.pdf , http://www.math.ntu.edu.tw/~hchen/teaching/StatInference/notes/lecture32.pdf , and http://www-inst.eecs.berkeley.edu/%7Ecs70/sp13/notes/n17.sp13.pdf but I don't get it. Would someone explain in simple terms what IID random variables are and give me an example?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I am having trouble understanding IID random variables. I've tried reading http://scipp.ucsc.edu/~haber/ph116C/iid.pdf , http://www.math.ntu.edu.tw/~hchen/teaching/StatInference/notes/lecture32.pdf , and http://www-inst.eecs.berkeley.edu/%7Ecs70/sp13/notes/n17.sp13.pdf but I don't get it. Would someone explain in simple terms what IID random variables are and give me an example?",,"['probability', 'probability-theory', 'random-variables', 'independence']"
66,Applications of Probability Theory in pure mathematics,Applications of Probability Theory in pure mathematics,,"My (maybe wrong) impression is that while probability is widely used in science (for example, in statistical mechanics), it is rarely seen in pure mathematics. Which leads me to the question - Are there some interesting application of Probability Theory in pure mathematics, outside Probability Theory itself?","My (maybe wrong) impression is that while probability is widely used in science (for example, in statistical mechanics), it is rarely seen in pure mathematics. Which leads me to the question - Are there some interesting application of Probability Theory in pure mathematics, outside Probability Theory itself?",,"['big-list', 'probability', 'probability-theory', 'applications']"
67,a problem on Polya's urn scheme,a problem on Polya's urn scheme,,"In an urn with $b$ blue and $r$ red balls, each time (call it a trial) a ball is chosen at random and then put again in the urn along with $c$ extra balls of the same color. Now probability of getting a blue ball in the 1st trial = $\frac{b}{b+r}$. Surprisingly, I also see that probability of getting a blue ball in the $n$-th trial is also $\frac{b}{b+r}$. I don't understand the intuition behind this. Another question is the intuitive argument to prove that the number of red balls in the first $n$ trials follow an uniform distribution between $0$ and $n$ when $b=r=c$.","In an urn with $b$ blue and $r$ red balls, each time (call it a trial) a ball is chosen at random and then put again in the urn along with $c$ extra balls of the same color. Now probability of getting a blue ball in the 1st trial = $\frac{b}{b+r}$. Surprisingly, I also see that probability of getting a blue ball in the $n$-th trial is also $\frac{b}{b+r}$. I don't understand the intuition behind this. Another question is the intuitive argument to prove that the number of red balls in the first $n$ trials follow an uniform distribution between $0$ and $n$ when $b=r=c$.",,['probability']
68,What is the probability that the first head will appear on the even numbered tosses,What is the probability that the first head will appear on the even numbered tosses,,Question $\text{Consider a coin with probability R to be heads. What is the probability}$ $\text{that the first head will appear on the even numbered tosses?}$ My Approach let the required Probability $=P$ . Hence we can write our eauation as-: $$P=(1-R) \times R+(1-R) \times (1-R) \times (1-R) \times P $$ $$P(1-(1-R) \times (1-R) \times (1-R))=(1-R) \times R $$ $$P=\frac{(1-R) \times R }{(1-(1-R) \times (1-R) \times (1-R)}$$ Am i correct? Answer is given as-: $$P=\frac{(1 - R)}{(2 - R)}$$,Question My Approach let the required Probability . Hence we can write our eauation as-: Am i correct? Answer is given as-:,\text{Consider a coin with probability R to be heads. What is the probability} \text{that the first head will appear on the even numbered tosses?} =P P=(1-R) \times R+(1-R) \times (1-R) \times (1-R) \times P  P(1-(1-R) \times (1-R) \times (1-R))=(1-R) \times R  P=\frac{(1-R) \times R }{(1-(1-R) \times (1-R) \times (1-R)} P=\frac{(1 - R)}{(2 - R)},"['probability', 'probability-theory']"
69,Symmetry in Probability (AMC 12A 2023),Symmetry in Probability (AMC 12A 2023),,"Flora the frog starts at $0$ on the number line and makes a sequence of jumps to the right. In any one jump, independent of previous jumps, Flora leaps a positive integer distance $m$ with probability $\frac{1}{2^m}$ . What is the probability that Flora will eventually land at $10$ ? (AMC 12A 2023/17) Solution 1 says: At any point, the probabilities of landing at $10$ and landing past $10$ are exactly the same. Therefore, the probability must be $\frac{1}{2}$ . If you apply any of solutions 2(recursion), 3 (combinations), or 7(induction), then solution 1 follows. But is there some elaboration of solution 1 that does not include 2, 3, or 7. Or some really elegant way to solve the question?","Flora the frog starts at on the number line and makes a sequence of jumps to the right. In any one jump, independent of previous jumps, Flora leaps a positive integer distance with probability . What is the probability that Flora will eventually land at ? (AMC 12A 2023/17) Solution 1 says: At any point, the probabilities of landing at and landing past are exactly the same. Therefore, the probability must be . If you apply any of solutions 2(recursion), 3 (combinations), or 7(induction), then solution 1 follows. But is there some elaboration of solution 1 that does not include 2, 3, or 7. Or some really elegant way to solve the question?",0 m \frac{1}{2^m} 10 10 10 \frac{1}{2},"['probability', 'combinatorics', 'induction', 'contest-math', 'recursion']"
70,the expectation of a chocolate bar,the expectation of a chocolate bar,,"So my buddy claims that if I split a chocolate bar at random into two pieces, then the expected size of the larger piece is $\frac{3}{4}$ of the bar. I can't figure out how he came up with this value... Can someone explain this? If you can, can you provide some kind of a proof? p.s. it would be helpful to think of this chocolate bar as a 1D array :) UPDATE Imagine the candy bar is a world-famous chocolate bar, the ones that are broken into chunks. However, this special chocolate bar has n chunks. If we broke the chocolate bar randomly along these chunks, what would the expected size of the larger chunk be? My buddy claims it to be $\leq{\frac{3}{4}}$.","So my buddy claims that if I split a chocolate bar at random into two pieces, then the expected size of the larger piece is $\frac{3}{4}$ of the bar. I can't figure out how he came up with this value... Can someone explain this? If you can, can you provide some kind of a proof? p.s. it would be helpful to think of this chocolate bar as a 1D array :) UPDATE Imagine the candy bar is a world-famous chocolate bar, the ones that are broken into chunks. However, this special chocolate bar has n chunks. If we broke the chocolate bar randomly along these chunks, what would the expected size of the larger chunk be? My buddy claims it to be $\leq{\frac{3}{4}}$.",,['probability']
71,Probability of All Distinct Faces When Six Dice Are Rolled,Probability of All Distinct Faces When Six Dice Are Rolled,,If six fair dice are rolled what is probability that each of the six numbers will appear exactly once?,If six fair dice are rolled what is probability that each of the six numbers will appear exactly once?,,"['probability', 'dice']"
72,What is the probability that the player $P_4$ reaches the semi final,What is the probability that the player  reaches the semi final,P_4,"$16$ players $P_1,P_2,....,P_{16}$ play a knockout tournament. It is known that whenever the players $P_i$ and $P_j$ play, the player $P_i$ will win if $i<j$. Assuming the players are paired at random in each round, what is the probability that the player $P_4$ reaches the semi final? I know that $P_1$ will anyhow reach final and $P_{16}$ will not clear the first round. I dont know how to solve further.","$16$ players $P_1,P_2,....,P_{16}$ play a knockout tournament. It is known that whenever the players $P_i$ and $P_j$ play, the player $P_i$ will win if $i<j$. Assuming the players are paired at random in each round, what is the probability that the player $P_4$ reaches the semi final? I know that $P_1$ will anyhow reach final and $P_{16}$ will not clear the first round. I dont know how to solve further.",,"['probability', 'combinatorics']"
73,"Four balls with different colors in a box, how many times do I need to pick to see all four colors?","Four balls with different colors in a box, how many times do I need to pick to see all four colors?",,"I have one white ball, one yellow ball, one red ball, one black ball. I put the four balls in a nontransparent box. I pick a ball from the box to see its color and put it back to the box. Assuming picking is random, how many times on average do I need to pick in order to see all four colors? If I'm lucky, I only need to pick four times. If I'm out of luck, I may get red, red, red, red, red, red,.... But what is it on average?","I have one white ball, one yellow ball, one red ball, one black ball. I put the four balls in a nontransparent box. I pick a ball from the box to see its color and put it back to the box. Assuming picking is random, how many times on average do I need to pick in order to see all four colors? If I'm lucky, I only need to pick four times. If I'm out of luck, I may get red, red, red, red, red, red,.... But what is it on average?",,"['probability', 'coupon-collector']"
74,Sum of three dice is eleven,Sum of three dice is eleven,,"the sample space of one toss of three dice is: $\Omega = \left \{ (1,1,1), ..., (6,6,6) \right \}$ so there are $6^3 = 216$ possible outcomes. What is the probability to obtain an outcome where the sum of its three components is equal to 11? I've considered the possible value can assume dice without a particular position and then I have considered the permutations to include every position: $(6,4,1), 3! = 6 \\ (6,3,2), 3! = 6 \\ (5,5,1), \frac{3!}{2!} = 3 \\ (5,4,2), 3! = 6 \\ (5,3,3), \frac{3!}{2!} = 3 \\ (4,4,3), \frac{3!}{2!} = 3$ so I've summed up obtaining $27$ and the probability would be $\frac{27}{216}$ Now, consider if I have to do this same passages for sums from 3 to 18, it is very exhausting. So, my question is: Is there any ""faster"" way to do that?","the sample space of one toss of three dice is: $\Omega = \left \{ (1,1,1), ..., (6,6,6) \right \}$ so there are $6^3 = 216$ possible outcomes. What is the probability to obtain an outcome where the sum of its three components is equal to 11? I've considered the possible value can assume dice without a particular position and then I have considered the permutations to include every position: $(6,4,1), 3! = 6 \\ (6,3,2), 3! = 6 \\ (5,5,1), \frac{3!}{2!} = 3 \\ (5,4,2), 3! = 6 \\ (5,3,3), \frac{3!}{2!} = 3 \\ (4,4,3), \frac{3!}{2!} = 3$ so I've summed up obtaining $27$ and the probability would be $\frac{27}{216}$ Now, consider if I have to do this same passages for sums from 3 to 18, it is very exhausting. So, my question is: Is there any ""faster"" way to do that?",,"['probability', 'combinatorics', 'dice']"
75,Probability about three independent exponential random variables,Probability about three independent exponential random variables,,"Suppose we have three independent exponential random variables $A$, $B$ and $C$  with respective parameters $1$, $2$ and $3$. Calculate $P(A<B<C)$. The hint says this problem could be solved with calculus and without calculus. I am really curious how to approach it with different methods.","Suppose we have three independent exponential random variables $A$, $B$ and $C$  with respective parameters $1$, $2$ and $3$. Calculate $P(A<B<C)$. The hint says this problem could be solved with calculus and without calculus. I am really curious how to approach it with different methods.",,['probability']
76,What is the expected number of runs of same color in a standard deck of cards?,What is the expected number of runs of same color in a standard deck of cards?,,"Standard deck has $52$ cards, $26$ Red and $26$ Black. A run is a maximum contiguous block of cards, which has the same color. Eg. $(R,B,R,B,...,R,B)$ has $52$ runs. $(R,R,R,...,R,B,B,B,...,B)$ has $2$ runs. What is the expected number of runs in a shuffled deck of cards?","Standard deck has $52$ cards, $26$ Red and $26$ Black. A run is a maximum contiguous block of cards, which has the same color. Eg. $(R,B,R,B,...,R,B)$ has $52$ runs. $(R,R,R,...,R,B,B,B,...,B)$ has $2$ runs. What is the expected number of runs in a shuffled deck of cards?",,"['probability', 'statistics', 'expected-value', 'card-games']"
77,What is the probability of getting an even number from a Poisson random draw?,What is the probability of getting an even number from a Poisson random draw?,,"Below is a graph showing the probability of drawing an odd number (y-axis) from a Poisson distribution with a given expected value (x-axis) x = seq(0,1e4,1)         // range of values to explore  lambdas = seq(0,4,0.01)  // expected value of the Poison distribution  FracOdd = numeric(n+1)       // response variable for (i in 1:length(lambdas))  {    FracOdd[i] = sum(dpois(x,lambdas[i])[seq(2,length(x),2)])  // calculate probability of drawing an odd number }  plot(y=FracOdd,x=lambdas, type=""l"", lwd=3, xlab=""Expected value"", ylab=""Probability of drawing an odd number"")   // plot the data It seems that the probability of drawing an odd number from a Poisson distribution with non-infinite mean is always lower than the probability of drawing an even number. What is the intuition for why the probability of drawing an odd number from a Poisson distribution always below 0.5? Is the function I drew numerically, easy to derive analytically?","Below is a graph showing the probability of drawing an odd number (y-axis) from a Poisson distribution with a given expected value (x-axis) x = seq(0,1e4,1)         // range of values to explore  lambdas = seq(0,4,0.01)  // expected value of the Poison distribution  FracOdd = numeric(n+1)       // response variable for (i in 1:length(lambdas))  {    FracOdd[i] = sum(dpois(x,lambdas[i])[seq(2,length(x),2)])  // calculate probability of drawing an odd number }  plot(y=FracOdd,x=lambdas, type=""l"", lwd=3, xlab=""Expected value"", ylab=""Probability of drawing an odd number"")   // plot the data It seems that the probability of drawing an odd number from a Poisson distribution with non-infinite mean is always lower than the probability of drawing an even number. What is the intuition for why the probability of drawing an odd number from a Poisson distribution always below 0.5? Is the function I drew numerically, easy to derive analytically?",,"['probability', 'probability-distributions', 'poisson-distribution', 'poisson-process']"
78,Convolution of 2 uniform random variables,Convolution of 2 uniform random variables,,"I really do not know how to do this. Let $X$ have a uniform distribution on $(0,2)$ and let $Y$ be independent of $X$ with a uniform distribution over $(0,3)$. Determine the cumulative distribution function of $S=X+Y$.","I really do not know how to do this. Let $X$ have a uniform distribution on $(0,2)$ and let $Y$ be independent of $X$ with a uniform distribution over $(0,3)$. Determine the cumulative distribution function of $S=X+Y$.",,"['probability', 'probability-theory']"
79,What is the (fully rigorous) definition of a confidence interval?,What is the (fully rigorous) definition of a confidence interval?,,"In a nutshell: what is the (fully rigorous) definition of a confidence interval? In page $92$ of Wasserman's All of Statistics , it is written that A $1 − α$ confidence interval for a parameter $θ$ is an interval $C_n = (a, b)$ where $a = a(X_1,...,X_n)$ and $b = b(X_1,...,X_n)$ are functions of the data such that $$P_θ(θ ∈ C_n) ≥ 1 − α, \ \ \ \ \text{ for all } θ ∈ Θ.$$ In words, $(a, b)$ traps $θ$ with probability $1 − α$ . We call $1 − α$ the coverage of the confidence interval. Warning! $C_n$ is random and $θ$ is fixed. I cannot understand the expression $P_\theta(\theta\in C_n)$ . In general, if we have a random variable $X:(\Omega,\mathcal{F},P)\to (\mathbb{R},\mathcal{B})$ , we define $$P(X\in S) := X_*P(S) = P(X^{-1}(S))$$ for $S$ in the Borel $\sigma$ -algebra $\mathcal{B}$ . Note the expression "" $P(X\in S)$ "" requires that $X$ be a random variable. $S$ be a fixed set. Neither of these conditions seem to be met with the expression "" $P(\theta\in C_n)$ "", as $\theta$ is an element of the parameter space $\Theta$ , which is itself a subset of $\mathbb{R}^n$ for some $n$ . That is, it seems to me that $\theta$ is a (fixed) vector, not a function (and thus not a random variable either). As $a$ and $b$ are functions of $X_1,\ldots,X_n$ , the interval $C_n := (a,b)$ seems to be ""variable"", when it should it be a fixed set for the expression to make sense. In case it is relevant, on page $89$ Wasserman explains that ... $P_\theta(X\in A) = \int_A f(x;\theta) dx$ ... which does make sense, as $\theta$ is fixed here, so that $f(x;\theta)$ is some random variable, while $A$ is a fixed set. However, the author is using the expressio $P_\theta$ differently in the main (first) quote of this post. I remain confused after reading this and this post.","In a nutshell: what is the (fully rigorous) definition of a confidence interval? In page of Wasserman's All of Statistics , it is written that A confidence interval for a parameter is an interval where and are functions of the data such that In words, traps with probability . We call the coverage of the confidence interval. Warning! is random and is fixed. I cannot understand the expression . In general, if we have a random variable , we define for in the Borel -algebra . Note the expression "" "" requires that be a random variable. be a fixed set. Neither of these conditions seem to be met with the expression "" "", as is an element of the parameter space , which is itself a subset of for some . That is, it seems to me that is a (fixed) vector, not a function (and thus not a random variable either). As and are functions of , the interval seems to be ""variable"", when it should it be a fixed set for the expression to make sense. In case it is relevant, on page Wasserman explains that ... ... which does make sense, as is fixed here, so that is some random variable, while is a fixed set. However, the author is using the expressio differently in the main (first) quote of this post. I remain confused after reading this and this post.","92 1 − α θ C_n = (a, b) a = a(X_1,...,X_n) b = b(X_1,...,X_n) P_θ(θ ∈ C_n) ≥ 1 − α, \ \ \ \ \text{ for all } θ ∈ Θ. (a, b) θ 1 − α 1 − α C_n θ P_\theta(\theta\in C_n) X:(\Omega,\mathcal{F},P)\to (\mathbb{R},\mathcal{B}) P(X\in S) := X_*P(S) = P(X^{-1}(S)) S \sigma \mathcal{B} P(X\in S) X S P(\theta\in C_n) \theta \Theta \mathbb{R}^n n \theta a b X_1,\ldots,X_n C_n := (a,b) 89 P_\theta(X\in A) = \int_A f(x;\theta) dx \theta f(x;\theta) A P_\theta","['probability', 'measure-theory', 'statistics', 'definition', 'confidence-interval']"
80,Conditional expectation of a joint normal distribution,Conditional expectation of a joint normal distribution,,"Let $X_1, X_2$ be jointly normal $N(\mu, \Sigma)$. I know that in general, $\mathbb{E}[X_2|X_1]$ can be computed by integrating the conditional density, but in the case of jointly normal variables, it suffices to do a linear projection: $\mathbb{E}[X_2 | \sigma(X_1)] = \mathbb{E}[X_2|\mathrm{span}(\mathbf{1}, X_1)] = \mu_2 + \frac{\mathrm{cov}(X_2, X_1)}{\mathrm{var}(X_1)} (X_1 - \mu_1) $ Is there a neat proof of this fact (one doesn't require doing any integrals)? Looking for references too.","Let $X_1, X_2$ be jointly normal $N(\mu, \Sigma)$. I know that in general, $\mathbb{E}[X_2|X_1]$ can be computed by integrating the conditional density, but in the case of jointly normal variables, it suffices to do a linear projection: $\mathbb{E}[X_2 | \sigma(X_1)] = \mathbb{E}[X_2|\mathrm{span}(\mathbf{1}, X_1)] = \mu_2 + \frac{\mathrm{cov}(X_2, X_1)}{\mathrm{var}(X_1)} (X_1 - \mu_1) $ Is there a neat proof of this fact (one doesn't require doing any integrals)? Looking for references too.",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'conditional-expectation']"
81,The law of absolute value of a standard Brownian motion,The law of absolute value of a standard Brownian motion,,"How can we easily compute $\mathbb{E} [ \left|W_t\right|]$, where $W = (W_t)_{t \geq 0}$ is the one dimensional standard Brownian motion (or wiener process)?","How can we easily compute $\mathbb{E} [ \left|W_t\right|]$, where $W = (W_t)_{t \geq 0}$ is the one dimensional standard Brownian motion (or wiener process)?",,"['probability', 'probability-theory', 'stochastic-processes']"
82,Finding a CDF given a PDF,Finding a CDF given a PDF,,"The PDF for $Y$ is $$f_Y(y) = \begin{cases}       0 & |y|> 1 \\       1-|y| & |y|\leq 1 \end{cases}$$ How do I find the corresponding CDF $F_Y(y)$? I integrated the above piecewise function to get $$F_Y(y)=\begin{cases}       1/2 -y/2-y^2/2 & [-1,0] \\       1/2-y/2+y^2/2 & [0,1] \end{cases} $$ by using the fact that $F_Y(y)=\int _{-\infty}^{y}{f_Y(y)}\,dy$, however my text claims the answer is  $$F_Y(y)=\begin{cases}       1/2 +y+y^2/2 & [-1,0] \\       1/2+y-y^2/2 & [0,1] \end{cases} $$ I am struggling with pdf and cdfs, so I asssume I did something wrong other than the simple integration. Who's correct? Me or the Text!?? $:)$","The PDF for $Y$ is $$f_Y(y) = \begin{cases}       0 & |y|> 1 \\       1-|y| & |y|\leq 1 \end{cases}$$ How do I find the corresponding CDF $F_Y(y)$? I integrated the above piecewise function to get $$F_Y(y)=\begin{cases}       1/2 -y/2-y^2/2 & [-1,0] \\       1/2-y/2+y^2/2 & [0,1] \end{cases} $$ by using the fact that $F_Y(y)=\int _{-\infty}^{y}{f_Y(y)}\,dy$, however my text claims the answer is  $$F_Y(y)=\begin{cases}       1/2 +y+y^2/2 & [-1,0] \\       1/2+y-y^2/2 & [0,1] \end{cases} $$ I am struggling with pdf and cdfs, so I asssume I did something wrong other than the simple integration. Who's correct? Me or the Text!?? $:)$",,"['probability', 'probability-distributions']"
83,"How many rolls are sufficient to ensure, with probability 99%, that the sum is greater than 100?","How many rolls are sufficient to ensure, with probability 99%, that the sum is greater than 100?",,"I roll a pair of fair dice $n$ times, and calculate the sum of all $2n$ faces which come up: Suppose each roll of each die is independent of other rolls. How many rolls are sufficient to ensure, with probability $99\%$ , that the sum is greater than $100?$ . I've calculated that the expected value and variance of sum are $\mathbb{E}(X) = 7n\text{ and } \mathrm{Var}(X)=\frac{35}{6},$ where $X$ represents the random variable of sum of the $2n$ faces. I think I need to find a lower bound on $\operatorname{Pr}(X > 100)$ . I've attempted applying Markov's inequality: $\operatorname{Pr}(X \geq 100)\leq \frac{\mathbb{E}(X)}{100}$ , which isn't anywhere near what I want. Any tips $?$ .","I roll a pair of fair dice times, and calculate the sum of all faces which come up: Suppose each roll of each die is independent of other rolls. How many rolls are sufficient to ensure, with probability , that the sum is greater than . I've calculated that the expected value and variance of sum are where represents the random variable of sum of the faces. I think I need to find a lower bound on . I've attempted applying Markov's inequality: , which isn't anywhere near what I want. Any tips .","n 2n 99\% 100? \mathbb{E}(X) = 7n\text{ and } \mathrm{Var}(X)=\frac{35}{6}, X 2n \operatorname{Pr}(X > 100) \operatorname{Pr}(X \geq 100)\leq \frac{\mathbb{E}(X)}{100} ?","['probability', 'random-variables', 'expected-value', 'variance', 'dice']"
84,"Probability that three independent uniform $(0,1)$ random variables can form a triangle",Probability that three independent uniform  random variables can form a triangle,"(0,1)","I am preparing for a probability exam and while practicing stuck on this question. Do not even know how to begin. Let $X_1$, $X_2$, $X_3$ be independent uniform $(0,1)$ random variables. What is the probability that we can form a triangle with three sticks of length $X_1$, $X_2$, $X_3$? I am thinking of using $X_1 + X_2 > X_3$ for this to happen and there are three such combinations. But how to proceed next ?","I am preparing for a probability exam and while practicing stuck on this question. Do not even know how to begin. Let $X_1$, $X_2$, $X_3$ be independent uniform $(0,1)$ random variables. What is the probability that we can form a triangle with three sticks of length $X_1$, $X_2$, $X_3$? I am thinking of using $X_1 + X_2 > X_3$ for this to happen and there are three such combinations. But how to proceed next ?",,['probability']
85,Efficient computation of $E\left[\left(1+X_1+\cdots+X_n\right)^{-1}\right]$ with $(X_i)$ independent Bernoulli with varying parameter,Efficient computation of  with  independent Bernoulli with varying parameter,E\left[\left(1+X_1+\cdots+X_n\right)^{-1}\right] (X_i),"Suppose we have the random variables $X_1, \ldots, X_n$ that have Bernoulli distributions with the (possibly different) probabilities $p_1, \ldots, p_n$. For example, $X_1$ = 1 with probability $p_1$ and 0 with probability $1-p_1$. Is there an efficient way to compute $$E\left[\frac{1}{1+\sum_iX_i}\right]$$ in polynomial time in $n$? If not, is there an approximate solution?","Suppose we have the random variables $X_1, \ldots, X_n$ that have Bernoulli distributions with the (possibly different) probabilities $p_1, \ldots, p_n$. For example, $X_1$ = 1 with probability $p_1$ and 0 with probability $1-p_1$. Is there an efficient way to compute $$E\left[\frac{1}{1+\sum_iX_i}\right]$$ in polynomial time in $n$? If not, is there an approximate solution?",,"['probability', 'statistics', 'computational-complexity']"
86,Conditional and Total Variance,Conditional and Total Variance,,Why does $ \text{Var}(Y) = E(\text{Var}(Y|X))+ \text{Var}(E(Y|X))$? What is the intuitive explanation for this? In laymen's terms it seems to say that the variance of $Y$ equals the expected value of the conditional variance plus the variance of the conditional expectation.,Why does $ \text{Var}(Y) = E(\text{Var}(Y|X))+ \text{Var}(E(Y|X))$? What is the intuitive explanation for this? In laymen's terms it seems to say that the variance of $Y$ equals the expected value of the conditional variance plus the variance of the conditional expectation.,,['probability']
87,What is the difference between a Poisson and an Exponential distribution?,What is the difference between a Poisson and an Exponential distribution?,,"For a Poisson distribution: $$\mathsf{P}(X=x)=\frac{e^{-\mu}\times \mu^x}{x!}$$ where $\mu$ is the mean number of occurrences. For an Exponential distribution: $$f(x;\lambda) = \begin{cases} \lambda e^{-\lambda x}  &  x \ge 0 \\ 0 & x < 0 \end{cases}$$ where $\lambda$ is the rate parameter. Apart from the fact that the formulas are obviously different, in layman's terms what is the difference between an Exponential and Poisson distribution? Or put in another way, why do we need them both? What does one of them do that the other doesn't? What is the difference between an Exponential distribution and an Exponential Density function? And yes, this is all very new to me as I was never taught distribution theory. Thanks.","For a Poisson distribution: $$\mathsf{P}(X=x)=\frac{e^{-\mu}\times \mu^x}{x!}$$ where $\mu$ is the mean number of occurrences. For an Exponential distribution: $$f(x;\lambda) = \begin{cases} \lambda e^{-\lambda x}  &  x \ge 0 \\ 0 & x < 0 \end{cases}$$ where $\lambda$ is the rate parameter. Apart from the fact that the formulas are obviously different, in layman's terms what is the difference between an Exponential and Poisson distribution? Or put in another way, why do we need them both? What does one of them do that the other doesn't? What is the difference between an Exponential distribution and an Exponential Density function? And yes, this is all very new to me as I was never taught distribution theory. Thanks.",,"['probability', 'probability-distributions', 'intuition', 'poisson-distribution', 'exponential-distribution']"
88,How to merge two Gaussians,How to merge two Gaussians,,"I have two multivariate Gaussians each defined by mean vectors and Covariance matrices (diagonal matrices). I want to merge them to have a single Gaussian i.e. I assume there is only one Gaussian but I separated observations randomly into two groups to get two different Gaussians which are not too different than each other. Since I know the number of observations in each of two Gaussians, combined mean estimation is straight forward : $\frac{n_1\mu_1 + n_2\mu_2}{n_1+n_2}$ But, what about the Covariance matrix? Thanks EDIT: The question was confusing in the original post, especially the ""merging Gaussians"" part. Maybe the following paragraph would be a better choice. I have two sets of observations drawn from two multivariate Gaussians each defined by mean vectors and Covariance matrices (diagonal matrices). I want to merge the observations to have a single sample, and I assume to have another Gaussian (i.e. I assume initially there was only a single Gaussian, and observations were separated into two groups to get two different Gaussians).","I have two multivariate Gaussians each defined by mean vectors and Covariance matrices (diagonal matrices). I want to merge them to have a single Gaussian i.e. I assume there is only one Gaussian but I separated observations randomly into two groups to get two different Gaussians which are not too different than each other. Since I know the number of observations in each of two Gaussians, combined mean estimation is straight forward : $\frac{n_1\mu_1 + n_2\mu_2}{n_1+n_2}$ But, what about the Covariance matrix? Thanks EDIT: The question was confusing in the original post, especially the ""merging Gaussians"" part. Maybe the following paragraph would be a better choice. I have two sets of observations drawn from two multivariate Gaussians each defined by mean vectors and Covariance matrices (diagonal matrices). I want to merge the observations to have a single sample, and I assume to have another Gaussian (i.e. I assume initially there was only a single Gaussian, and observations were separated into two groups to get two different Gaussians).",,"['probability', 'normal-distribution']"
89,Probability about a coin games,Probability about a coin games,,"Independent flips of a biased coin that lands on heads with probability is 0.7 are made. Each of two players, A and B has chosen one out of the eight triplets HHH, HHT, HTH, HTT, THH, THT, TTH and TTT, and the player whose triplet occurs first wins. For example, suppose A had chosen HHT and B had chosen THT. Then if the flipped coin shows the sequence HHHT....., A wins, and if the flipped coin shows the sequence TTTHT..., B wins. Since the coin is biased towards heads, the triplet HHH seems to be a good choice. a) What is the probability that the pattern THH occurs before the pattern HHH? b) What is the probability that the pattern HTH occurs before the pattern THH? c) What is the probability that the pattern HHH occurs before the pattern HTH?","Independent flips of a biased coin that lands on heads with probability is 0.7 are made. Each of two players, A and B has chosen one out of the eight triplets HHH, HHT, HTH, HTT, THH, THT, TTH and TTT, and the player whose triplet occurs first wins. For example, suppose A had chosen HHT and B had chosen THT. Then if the flipped coin shows the sequence HHHT....., A wins, and if the flipped coin shows the sequence TTTHT..., B wins. Since the coin is biased towards heads, the triplet HHH seems to be a good choice. a) What is the probability that the pattern THH occurs before the pattern HHH? b) What is the probability that the pattern HTH occurs before the pattern THH? c) What is the probability that the pattern HHH occurs before the pattern HTH?",,['probability']
90,On the probability that two positive integers are relatively prime,On the probability that two positive integers are relatively prime,,"In many of the sources I have consulted about this, the ""probability"" that two positive integers chosen at random are relatively prime is calculated as the limit as $n \to \infty$ of the probability that two randomly chosen integers in the set {1,2, ..., $n$} are relatively prime (the limit being $1/\zeta(2)$). My first question is: Is this limit really a probability? Also, the nonrigorous/heuristic proofs that I have seen of this start by mentioning that ""the probability that a prime $p$ divides a positive integer is $1/p$"". This makes intuitive sense. I was wondering though: Is there a way of defining a probability measure on the positive integers in such a way that the set {$n \in \mathbf Z_+$ | $p$ divides $n$} has measure $1/p$ (that we can use for a rigorous proof)?","In many of the sources I have consulted about this, the ""probability"" that two positive integers chosen at random are relatively prime is calculated as the limit as $n \to \infty$ of the probability that two randomly chosen integers in the set {1,2, ..., $n$} are relatively prime (the limit being $1/\zeta(2)$). My first question is: Is this limit really a probability? Also, the nonrigorous/heuristic proofs that I have seen of this start by mentioning that ""the probability that a prime $p$ divides a positive integer is $1/p$"". This makes intuitive sense. I was wondering though: Is there a way of defining a probability measure on the positive integers in such a way that the set {$n \in \mathbf Z_+$ | $p$ divides $n$} has measure $1/p$ (that we can use for a rigorous proof)?",,"['number-theory', 'probability']"
91,Calculating the median in the St. Petersburg paradox,Calculating the median in the St. Petersburg paradox,,"I am studying a recreational probability problem (which from the comments here I discovered it has a name and long history). One way to address the paradox created by the problem is to study the median value instead of the expected value. I want to calculate the median value exactly (not only find bounds or asymptotic values). I have found a certain approach and I am stuck in a specific step. I present my analysis and I would like some help on that specific step. [Note: Other solutions to the general problem are welcome (however after the revelation of the long history I found a lot of material) but what I really want is to know the answer to the sub-problem that my approach raises.] The problem We have the following game: I toss a coin as many times is needed to get tails. Then I count the number of consecutive heads that preceded (call it h) and I give you $2^h$ dollars. How much are you willing to pay to play such a game? In other words, what is the maximum buy-in for that game you are willing to pay? Note also, that we can play this game any amount of finite times (each time with you paying the buy-in). A naive answer One straightforward way to answer this is to calculate the expected value of one game. This should be the upper limit for the buy-in. The expected value is the infinite sum of the return of each case times the probability of each case. More specifically $$\sum_{i=0}^\infty (2^{i-1}\cdot\frac{1}{2^i}) = \sum_{i=0}^\infty \frac{1}{2} = \infty$$  This might seem counter-intuitive but it is true: Whatever constant and finite amount you bet per game, you are expected to win on the long run! Why is this so counter-intuitive though? Would you be willing to play this in practice with say 1000 dollars per game? The answer is no, because you would need an immensely large amount of games to actually win. So if we care about a more practical measure, the expected value is of no help. What we need is the median (or any other percentile value). If we know the median return for N games, we can at least know that if the buy-in is $\frac{median}{N}$, half of the possible cases you will lose and for half you will win. We will not know how much we will win or lose (we do have an upper bound on the losses though) but at least we know the chances to win or lose for a finite N number of games. Finding the median So how do you calculate the median return from N games (or more generally any ith percentile)? If we play only one game (N=1) then it is trivial. The median is 1. For N=2 it starts getting more complicated. With probability 0.25 we'll get back 1+1, with 0.125 1+2, with 0.125 2+1. These 3 cases already bring us to a total of 0.5, so the median is 3 (and so the maximum bet is 1.5 per game). For any N, how do we enumerate all the cases and find the 50% point (or any i% point)? I realized that this is (partly) an ordering problem. We do not want just to enumerate random cases, we have to order them, starting from the case with the smallest possible return, then getting the one(s) with the next smallest return and so on. As we are doing this ordering we are adding the probabilities of these cases. When we reach 50% (or i%) we stop. The return value for that case is our median value (ith percentile value). The ordering is where I am stuck. Sub-problem formulation We can depict the possible space of returns with a matrix where the N columns are the N games and the infinite rows are the return for each game: $$\begin{array}{c} \text{row 1} \\ \text{row 2} \\ \text{row 3} \\ \vdots \\ \text{row i} \\ \vdots \end{array} \;\;\;\; \overbrace{\begin{array}{cccc} 1 & 1 & \cdots & 1 \\ 2 & 2 & \cdots & 2 \\ 4 & 4 & \cdots & 4 \\ \vdots & \vdots & \ddots & \vdots \\ 2^{i-1} & 2^{i-1} & \cdots & 2^{i-1} \\ \vdots & \vdots & & \vdots \end{array}}^N$$ A series of N games consists of picking values for each column (i.e., picking a game outcome for each game). The smallest possible total return is when all game outcomes are 1. So total return = N. The next possible one is when we get one outcome from the second row (total return N+1). The next smallest total return is N+2 (2 game outcomes from the second row). Notice though that for total return N+3 we have two ""configurations"": 1) cases where we have N-3 outcomes from the first row and 3 from the second row, OR 2) cases where we have N-1 outcomes from the 1st row and 1 outcome from the 3rd row! So ordering is not such an easy process. Configurations vs. cases Notice how I talked about ""configurations"" instead of individual cases. An individual case is a sequence of game outcomes (which are completely described by the game returns). For example a case of 4 games could be (1, 1, 16, 8) for a total return of 26. A configuration on the other hand is a more general construct which specifies how many outcomes we have from each row. A configuration completely determines the total return, but not the individual order that the outcomes happened. For example, the case given above is part of the configuration ""2 outcomes from row 1, 1 outcome from row 4, 1 outcome from row 5"". Cases (1,16,1,8) and (8,1,1,16) belong to the same configuration. From a configuration I can calculate how many distinct cases it has and what is the probability of each case. For example, for the configuration "" $N_i$ outcomes from row i, $N_j$ from row j, $N_k$ from row k"" we have: The number of distinct cases is ${N\choose {N_i}}\cdot{{N-N_i}\choose{N_j}}\cdot{{N-N_i-N_j}\choose{N_k}}$ The probability for each of these cases is $2^{-(i\cdot N_i + j\cdot N_j + k\cdot N_k)}$ The total return value for any of these cases is $N_i \cdot 2^{i-1}+N_j \cdot 2^{j-1}+N_k \cdot 2^{k-1}$ The example above shows a configuration with 3 rows, just to get a taste of the complexity of the problem. I can generalise the formulas to find distinct cases, their probabilities and their total returns for any given configuration. The problem is ordering the configurations . Can we find an algorithm that orders and lists the configurations based on their total return value? Let's describe each configuration as a series of pairs {(x,i), (y,j), ...} where the first number of a pair denotes the row number and the second number of a pair denotes how many outcomes do we have from that row. For example, {(1,4), (3,1), (4,2)} means that we get 4 outcomes from row 1, 1 outcome from row 3, and 2 outcomes from row 4. This also means that we played 4 + 1 + 2 = 7 games. I manually computed the first terms of the ordered configurations list, for N games. I give the configuration(s) on the left and the total return on the right. Note that some total returns have more than one configurations that produce them. $\begin{array}{ll} \text{Configurations} & \text{Total return} \\  \{(1,N)\} & N \\ \{(1,N-1),\; (2,1)\} & N+1 \\  \{(1,N-2),\; (2,2)\} & N+2 \\ \{(1,N-3),\; (2,3)\},\;\; \{(1,N-1),\; (3,1)\} & N+3 \\  \{(1,N-4),\; (2,4)\},\;\; \{(1,N-2),\; (2,1),\; (3,1)\} & N+4 \\   \{(1,N-5),\; (2,5)\},\;\; \{(1,N-3),\; (2,2),\; (3,1)\} & N+5 \\ \{(1,N-6),\; (2,6)\},\;\; \{(1,N-4),\; (2,3),\; (3,1)\},\;\; \{(1,N-2),\; (3,2)\} & N+6 \\ \end{array}$ If I can produce this order algorithmically then I will be able to calculate the median (or ith percentile) for any N. I would also appreciate any help in formulating the problem in more accepted/mainstream terms. I believe that the formulation is valid and clear(?), but if we use a formulation from an established subfield maybe it will point to the solution too. Thanks!","I am studying a recreational probability problem (which from the comments here I discovered it has a name and long history). One way to address the paradox created by the problem is to study the median value instead of the expected value. I want to calculate the median value exactly (not only find bounds or asymptotic values). I have found a certain approach and I am stuck in a specific step. I present my analysis and I would like some help on that specific step. [Note: Other solutions to the general problem are welcome (however after the revelation of the long history I found a lot of material) but what I really want is to know the answer to the sub-problem that my approach raises.] The problem We have the following game: I toss a coin as many times is needed to get tails. Then I count the number of consecutive heads that preceded (call it h) and I give you $2^h$ dollars. How much are you willing to pay to play such a game? In other words, what is the maximum buy-in for that game you are willing to pay? Note also, that we can play this game any amount of finite times (each time with you paying the buy-in). A naive answer One straightforward way to answer this is to calculate the expected value of one game. This should be the upper limit for the buy-in. The expected value is the infinite sum of the return of each case times the probability of each case. More specifically $$\sum_{i=0}^\infty (2^{i-1}\cdot\frac{1}{2^i}) = \sum_{i=0}^\infty \frac{1}{2} = \infty$$  This might seem counter-intuitive but it is true: Whatever constant and finite amount you bet per game, you are expected to win on the long run! Why is this so counter-intuitive though? Would you be willing to play this in practice with say 1000 dollars per game? The answer is no, because you would need an immensely large amount of games to actually win. So if we care about a more practical measure, the expected value is of no help. What we need is the median (or any other percentile value). If we know the median return for N games, we can at least know that if the buy-in is $\frac{median}{N}$, half of the possible cases you will lose and for half you will win. We will not know how much we will win or lose (we do have an upper bound on the losses though) but at least we know the chances to win or lose for a finite N number of games. Finding the median So how do you calculate the median return from N games (or more generally any ith percentile)? If we play only one game (N=1) then it is trivial. The median is 1. For N=2 it starts getting more complicated. With probability 0.25 we'll get back 1+1, with 0.125 1+2, with 0.125 2+1. These 3 cases already bring us to a total of 0.5, so the median is 3 (and so the maximum bet is 1.5 per game). For any N, how do we enumerate all the cases and find the 50% point (or any i% point)? I realized that this is (partly) an ordering problem. We do not want just to enumerate random cases, we have to order them, starting from the case with the smallest possible return, then getting the one(s) with the next smallest return and so on. As we are doing this ordering we are adding the probabilities of these cases. When we reach 50% (or i%) we stop. The return value for that case is our median value (ith percentile value). The ordering is where I am stuck. Sub-problem formulation We can depict the possible space of returns with a matrix where the N columns are the N games and the infinite rows are the return for each game: $$\begin{array}{c} \text{row 1} \\ \text{row 2} \\ \text{row 3} \\ \vdots \\ \text{row i} \\ \vdots \end{array} \;\;\;\; \overbrace{\begin{array}{cccc} 1 & 1 & \cdots & 1 \\ 2 & 2 & \cdots & 2 \\ 4 & 4 & \cdots & 4 \\ \vdots & \vdots & \ddots & \vdots \\ 2^{i-1} & 2^{i-1} & \cdots & 2^{i-1} \\ \vdots & \vdots & & \vdots \end{array}}^N$$ A series of N games consists of picking values for each column (i.e., picking a game outcome for each game). The smallest possible total return is when all game outcomes are 1. So total return = N. The next possible one is when we get one outcome from the second row (total return N+1). The next smallest total return is N+2 (2 game outcomes from the second row). Notice though that for total return N+3 we have two ""configurations"": 1) cases where we have N-3 outcomes from the first row and 3 from the second row, OR 2) cases where we have N-1 outcomes from the 1st row and 1 outcome from the 3rd row! So ordering is not such an easy process. Configurations vs. cases Notice how I talked about ""configurations"" instead of individual cases. An individual case is a sequence of game outcomes (which are completely described by the game returns). For example a case of 4 games could be (1, 1, 16, 8) for a total return of 26. A configuration on the other hand is a more general construct which specifies how many outcomes we have from each row. A configuration completely determines the total return, but not the individual order that the outcomes happened. For example, the case given above is part of the configuration ""2 outcomes from row 1, 1 outcome from row 4, 1 outcome from row 5"". Cases (1,16,1,8) and (8,1,1,16) belong to the same configuration. From a configuration I can calculate how many distinct cases it has and what is the probability of each case. For example, for the configuration "" $N_i$ outcomes from row i, $N_j$ from row j, $N_k$ from row k"" we have: The number of distinct cases is ${N\choose {N_i}}\cdot{{N-N_i}\choose{N_j}}\cdot{{N-N_i-N_j}\choose{N_k}}$ The probability for each of these cases is $2^{-(i\cdot N_i + j\cdot N_j + k\cdot N_k)}$ The total return value for any of these cases is $N_i \cdot 2^{i-1}+N_j \cdot 2^{j-1}+N_k \cdot 2^{k-1}$ The example above shows a configuration with 3 rows, just to get a taste of the complexity of the problem. I can generalise the formulas to find distinct cases, their probabilities and their total returns for any given configuration. The problem is ordering the configurations . Can we find an algorithm that orders and lists the configurations based on their total return value? Let's describe each configuration as a series of pairs {(x,i), (y,j), ...} where the first number of a pair denotes the row number and the second number of a pair denotes how many outcomes do we have from that row. For example, {(1,4), (3,1), (4,2)} means that we get 4 outcomes from row 1, 1 outcome from row 3, and 2 outcomes from row 4. This also means that we played 4 + 1 + 2 = 7 games. I manually computed the first terms of the ordered configurations list, for N games. I give the configuration(s) on the left and the total return on the right. Note that some total returns have more than one configurations that produce them. $\begin{array}{ll} \text{Configurations} & \text{Total return} \\  \{(1,N)\} & N \\ \{(1,N-1),\; (2,1)\} & N+1 \\  \{(1,N-2),\; (2,2)\} & N+2 \\ \{(1,N-3),\; (2,3)\},\;\; \{(1,N-1),\; (3,1)\} & N+3 \\  \{(1,N-4),\; (2,4)\},\;\; \{(1,N-2),\; (2,1),\; (3,1)\} & N+4 \\   \{(1,N-5),\; (2,5)\},\;\; \{(1,N-3),\; (2,2),\; (3,1)\} & N+5 \\ \{(1,N-6),\; (2,6)\},\;\; \{(1,N-4),\; (2,3),\; (3,1)\},\;\; \{(1,N-2),\; (3,2)\} & N+6 \\ \end{array}$ If I can produce this order algorithmically then I will be able to calculate the median (or ith percentile) for any N. I would also appreciate any help in formulating the problem in more accepted/mainstream terms. I believe that the formulation is valid and clear(?), but if we use a formulation from an established subfield maybe it will point to the solution too. Thanks!",,"['probability', 'combinatorics', 'number-theory', 'median']"
92,Rolling a $6$-sided dice indefinitely until lower than the previous throw,Rolling a -sided dice indefinitely until lower than the previous throw,6,"You will play a game with a fair 6-sided die. You will throw the die and as long as the result of the throw is greater than or equal to the previous throw, you will continue throwing. If the throw is lower than the previous one, you will stop and get as many points as the sum of all throws, including the last one. For example, if you get 2, 5, 5, and 3 as a result of 4 throws, the game will end with 15 points. What is the expected value of the points you will get at the end of the game?","You will play a game with a fair 6-sided die. You will throw the die and as long as the result of the throw is greater than or equal to the previous throw, you will continue throwing. If the throw is lower than the previous one, you will stop and get as many points as the sum of all throws, including the last one. For example, if you get 2, 5, 5, and 3 as a result of 4 throws, the game will end with 15 points. What is the expected value of the points you will get at the end of the game?",,"['probability', 'sequences-and-series', 'contest-math']"
93,Normalized vector of Gaussian variables is uniformly distributed on the sphere [duplicate],Normalized vector of Gaussian variables is uniformly distributed on the sphere [duplicate],,"This question already has an answer here : Uniform distribution on the surface of unit sphere (1 answer) Closed 4 years ago . I have seen in various places the following claim: Let $X_1$ , $X_2$ , $\cdots$ , $X_n \sim \mathcal{N}(0, 1)$ and be independent. Then, the vector $$ X = \left(\frac{X_1}{Z}, \frac{X_2}{Z}, \cdots, \frac{X_n}{Z}\right) $$ is a uniform random vector on $S^{n-1}$ , where $Z = \sqrt{X_1^2 + \cdots + X_n^2}$ . Many sources claimed this fact follows easily from the orthogonal-invariance of the normal distribution, but somehow I couldn't construct a rigorous proof. (one such ""sketch"" can be found here ). How to prove this rigorously? Edit: It has been brought to my attention that this question was already asked before here .  However, I find the answer there to be incomplete-it shows that $X$ is orthogonally-invariant, but does not explicitly explains why that implies it is uniform. Therefore I think there is value in keeping this copy as well, as I guess we cannot transfer the answer. In my question, I explicitly asked for a complete rigorous proof-and I find the answer there to be incomplete.","This question already has an answer here : Uniform distribution on the surface of unit sphere (1 answer) Closed 4 years ago . I have seen in various places the following claim: Let , , , and be independent. Then, the vector is a uniform random vector on , where . Many sources claimed this fact follows easily from the orthogonal-invariance of the normal distribution, but somehow I couldn't construct a rigorous proof. (one such ""sketch"" can be found here ). How to prove this rigorously? Edit: It has been brought to my attention that this question was already asked before here .  However, I find the answer there to be incomplete-it shows that is orthogonally-invariant, but does not explicitly explains why that implies it is uniform. Therefore I think there is value in keeping this copy as well, as I guess we cannot transfer the answer. In my question, I explicitly asked for a complete rigorous proof-and I find the answer there to be incomplete.","X_1 X_2 \cdots X_n \sim \mathcal{N}(0, 1) 
X = \left(\frac{X_1}{Z}, \frac{X_2}{Z}, \cdots, \frac{X_n}{Z}\right)
 S^{n-1} Z = \sqrt{X_1^2 + \cdots + X_n^2} X","['probability', 'probability-distributions', 'random-variables', 'normal-distribution', 'uniform-distribution']"
94,The probability that x birthdays lie within n days of each other,The probability that x birthdays lie within n days of each other,,"This is a question that has bugged me for quite some time: what is the chance that x people happen to have their birthdays within n days of each other? A bit more specific, since this is how a colleague one phrased it: what is the probability that 5 people have their birthdays within 40 days? Both the birthdays and the ""distance"" are supposed to be random: there is no fixed time span (e.g., April 1 to May 10) that the birthdays are to lie within. The birthdays should be such, that two birthdays are always within 40 days of each other. The thing that bugs me, is that it seems to be some kind of recursive calculation, and that I can't find a way to put it into a straightforward mathematical formulation. To explain that, consider 2 people: the first person is free to have his or her birthday $b_1$ any day of the year, and the second person has 81 days to pick a birthday date $b_2$ from (the 40 day timespan is inclusive , so up to 40 days before $b_1$, plus up to days after $b_1$, plus one on $b_1$ itself. This may be more logically phrased as 41 days for some; I don't know what is best, so please be clear about it in your answer). Now, for the third person, the number of birthdays he or she can have, is limited by the second person's birthday: if $b_2 = b_1$, then $b_3$ can be among 81 days, but if $b_2 = b_1 + 1$ or $b_2 = b_1 - 1$, there are only 80 days for each option, and 79 for $\|b_1 - b_2\| = 2$, etc. For the fourth person, the limitation is given by person 2 and 3, complicating things; the fifth person makes things even more complicated. I've also tried to go the ""exclusion"" way (what is the chance that 5 people do not share their birthdays within 40 days of each other), but I didn't get anywhere that way. But perhaps I'm going entirely the wrong way about this. By now, I've computed it in various way, and I'm quite confident of the answer, but I'm still looking for the mathematical formulation of the general (x birthdays, n days) problem. The answer I've got, btw, is $7.581428 \cdot 10^{-4}$, or $\frac{13456201}{365^4}$. NB: this obviously assumes no leap years. NB2: Extension of the Birthday Problem appears related, though I can't readily see if I can use any of that formulation here.","This is a question that has bugged me for quite some time: what is the chance that x people happen to have their birthdays within n days of each other? A bit more specific, since this is how a colleague one phrased it: what is the probability that 5 people have their birthdays within 40 days? Both the birthdays and the ""distance"" are supposed to be random: there is no fixed time span (e.g., April 1 to May 10) that the birthdays are to lie within. The birthdays should be such, that two birthdays are always within 40 days of each other. The thing that bugs me, is that it seems to be some kind of recursive calculation, and that I can't find a way to put it into a straightforward mathematical formulation. To explain that, consider 2 people: the first person is free to have his or her birthday $b_1$ any day of the year, and the second person has 81 days to pick a birthday date $b_2$ from (the 40 day timespan is inclusive , so up to 40 days before $b_1$, plus up to days after $b_1$, plus one on $b_1$ itself. This may be more logically phrased as 41 days for some; I don't know what is best, so please be clear about it in your answer). Now, for the third person, the number of birthdays he or she can have, is limited by the second person's birthday: if $b_2 = b_1$, then $b_3$ can be among 81 days, but if $b_2 = b_1 + 1$ or $b_2 = b_1 - 1$, there are only 80 days for each option, and 79 for $\|b_1 - b_2\| = 2$, etc. For the fourth person, the limitation is given by person 2 and 3, complicating things; the fifth person makes things even more complicated. I've also tried to go the ""exclusion"" way (what is the chance that 5 people do not share their birthdays within 40 days of each other), but I didn't get anywhere that way. But perhaps I'm going entirely the wrong way about this. By now, I've computed it in various way, and I'm quite confident of the answer, but I'm still looking for the mathematical formulation of the general (x birthdays, n days) problem. The answer I've got, btw, is $7.581428 \cdot 10^{-4}$, or $\frac{13456201}{365^4}$. NB: this obviously assumes no leap years. NB2: Extension of the Birthday Problem appears related, though I can't readily see if I can use any of that formulation here.",,[]
95,"What is the name of this theorem, and are there any caveats?","What is the name of this theorem, and are there any caveats?",,"For random variable $X$ that follows some distribution, $f(x)$ is the probability density function of that distribution if and only if $$\mathbb{E}[\phi(X)] = \int_{-\infty}^\infty \phi(x) f(x)dx$$ for all functions $\phi$. Context: My professor used this in lecture to demonstrate a way to find the distribution of $cX$ given random variable $X$ that follows a specific distribution. What is the name of this theorem? Have I missed any qualifications/caveats? I am in particular curious about whether ""for all functions"" is correct. My professor mentioned ""positive, bounded, deterministic functions,"" but I am not sure what he meant by that. Where can I find a proof of this theorem?","For random variable $X$ that follows some distribution, $f(x)$ is the probability density function of that distribution if and only if $$\mathbb{E}[\phi(X)] = \int_{-\infty}^\infty \phi(x) f(x)dx$$ for all functions $\phi$. Context: My professor used this in lecture to demonstrate a way to find the distribution of $cX$ given random variable $X$ that follows a specific distribution. What is the name of this theorem? Have I missed any qualifications/caveats? I am in particular curious about whether ""for all functions"" is correct. My professor mentioned ""positive, bounded, deterministic functions,"" but I am not sure what he meant by that. Where can I find a proof of this theorem?",,"['probability', 'probability-theory', 'probability-distributions', 'expectation']"
96,"Determine $\lim\limits_{n \to \infty}{{n} \choose {\frac{n}{2}}}\frac{1}{2^n}$, where each $n$ is even","Determine , where each  is even",\lim\limits_{n \to \infty}{{n} \choose {\frac{n}{2}}}\frac{1}{2^n} n,"For each positive even integer $n$, set $$P_n = \displaystyle {{n} \choose {\frac{n}{2}}}\frac{1}{2^n}.$$ Show that $\displaystyle \lim_{n \to \infty} P_n$ exists and determine its value. Here's what I have so far: each $P_n$ can be thought of as the probability of tossing a fair coin $n$ times, and obtaining exactly $\frac{n}{2}$ heads. My expectation is that $P_n \to 1$, since in any large trial, one would expect to record just about as many heads as tails. But I'm not sure how to mathematically justify this hunch. Hints or solutions are greatly appreciated.","For each positive even integer $n$, set $$P_n = \displaystyle {{n} \choose {\frac{n}{2}}}\frac{1}{2^n}.$$ Show that $\displaystyle \lim_{n \to \infty} P_n$ exists and determine its value. Here's what I have so far: each $P_n$ can be thought of as the probability of tossing a fair coin $n$ times, and obtaining exactly $\frac{n}{2}$ heads. My expectation is that $P_n \to 1$, since in any large trial, one would expect to record just about as many heads as tails. But I'm not sure how to mathematically justify this hunch. Hints or solutions are greatly appreciated.",,"['probability', 'limits', 'binomial-coefficients']"
97,How to prove Bonferroni inequalities?,How to prove Bonferroni inequalities?,,"Define $$S_1 = \sum_{i=1}^n P(A_i)$$ and $$S_2 =\sum_{1 \le i < j \le n}^n P(A_i \cap A_j)$$ as well as $$S_k =\sum_{1 \le i_1 < \cdots < i_k \le n}^n P(A_{i_1} \cap \cdots \cap A_{i_k})$$ Then for odd $k$ in $\{1,\ldots,n\}$ $$P\left(\bigcup_{i=1}^n A_i\right) \le \sum_{j=1}^{k}(-1)^{j-1} S_j$$ For even $k$ in $\{2,\ldots,n\}$ $$P\left(\bigcup_{i=1}^n A_i\right) \ge \sum_{j=1}^{k}(-1)^{j-1}S_j$$ More details of Bonferroni inequalities or Boole's inequality is here .","Define $$S_1 = \sum_{i=1}^n P(A_i)$$ and $$S_2 =\sum_{1 \le i < j \le n}^n P(A_i \cap A_j)$$ as well as $$S_k =\sum_{1 \le i_1 < \cdots < i_k \le n}^n P(A_{i_1} \cap \cdots \cap A_{i_k})$$ Then for odd $k$ in $\{1,\ldots,n\}$ $$P\left(\bigcup_{i=1}^n A_i\right) \le \sum_{j=1}^{k}(-1)^{j-1} S_j$$ For even $k$ in $\{2,\ldots,n\}$ $$P\left(\bigcup_{i=1}^n A_i\right) \ge \sum_{j=1}^{k}(-1)^{j-1}S_j$$ More details of Bonferroni inequalities or Boole's inequality is here .",,['probability']
98,What is the distribution of gaps?,What is the distribution of gaps?,,"Randomly select $n$ numbers from the universe $\{1,2\dots,m\}$ with or without replacement, and sort the numbers in ascending order. We can get a list of number $\{a_1,a_2,\dots,a_n\}$, and then we can get the difference between two consecutive numbers and get the gap list: $\{a_1, a_2-a_1,\dots ,a_n-a_{n-1}\}$ So my question is: what is the distribution of the gaps. Let $A_i$ be the number of gaps which are equal to $i$, what is the distribution of $A_i$? Update: I clarify that there should two situation to consider: with or without replacement.","Randomly select $n$ numbers from the universe $\{1,2\dots,m\}$ with or without replacement, and sort the numbers in ascending order. We can get a list of number $\{a_1,a_2,\dots,a_n\}$, and then we can get the difference between two consecutive numbers and get the gap list: $\{a_1, a_2-a_1,\dots ,a_n-a_{n-1}\}$ So my question is: what is the distribution of the gaps. Let $A_i$ be the number of gaps which are equal to $i$, what is the distribution of $A_i$? Update: I clarify that there should two situation to consider: with or without replacement.",,"['probability', 'combinatorics', 'sampling']"
99,Cool examples of the Central Limit Theorem in action,Cool examples of the Central Limit Theorem in action,,"Sir Francis Galton has described the Central Limit Theorem as follows. I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the ""Law of Frequency of Error"". The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along. I have since become fascinated with the CLT, and have looked for cool examples in real life where this holds and wherein one can actually ""see"" the Bell curve. The only example I could find till now was the Galton Box . Are there any more examples of this sort?","Sir Francis Galton has described the Central Limit Theorem as follows. I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the ""Law of Frequency of Error"". The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along. I have since become fascinated with the CLT, and have looked for cool examples in real life where this holds and wherein one can actually ""see"" the Bell curve. The only example I could find till now was the Galton Box . Are there any more examples of this sort?",,"['probability', 'big-list', 'normal-distribution']"
