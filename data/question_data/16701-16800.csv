,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$\int\limits_0^1{\frac{{E\left( {\sqrt{\frac{t}{{1+t}}}}\right)-K\left({\sqrt{\frac{t}{{1+t}}}}\right)}}{{t\sqrt {1+t} }}}\ln \left( {1 - t}\right)dt$,,\int\limits_0^1{\frac{{E\left( {\sqrt{\frac{t}{{1+t}}}}\right)-K\left({\sqrt{\frac{t}{{1+t}}}}\right)}}{{t\sqrt {1+t} }}}\ln \left( {1 - t}\right)dt,"I found this integral from a Facebook page : Find a closed form for this integral $$\int\limits_0^1{\frac{{E\left( {\sqrt{\frac{t}{{1+t}}}}\right)-K\left({\sqrt{\frac{t}{{1+t}}}}\right)}}{{t\sqrt {1+t} }}}\ln \left( {1 - t}\right)dt$$ $${\text{Where}}:K\left( k \right),{\text{ }}E\left( k \right){\text{ are in order the complete elliptic integral of the first and second kind}}{\text{.}}$$ $${\text{And}}:K\left( k \right) = \int\limits_0^1 {\frac{1}{{\sqrt {\left( {1 - {x^2}} \right)\left( {1 - {k^2}{x^2}} \right)} }}dx} ,E\left( k \right) = \int\limits_0^1 {\frac{{\sqrt {1 - {k^2}{x^2}} }}{{\sqrt {1 - {x^2}} }}dx} $$ I really don't know how to express a relation between $E(k)$ and $K(k)$ to reduce the problem. I think this is an intriguing integral, but I can't evaluate it yet. May you guys please help me with this? Thank you very much.","I found this integral from a Facebook page : Find a closed form for this integral I really don't know how to express a relation between and to reduce the problem. I think this is an intriguing integral, but I can't evaluate it yet. May you guys please help me with this? Thank you very much.","\int\limits_0^1{\frac{{E\left( {\sqrt{\frac{t}{{1+t}}}}\right)-K\left({\sqrt{\frac{t}{{1+t}}}}\right)}}{{t\sqrt {1+t} }}}\ln \left( {1 - t}\right)dt {\text{Where}}:K\left( k \right),{\text{ }}E\left( k \right){\text{ are in order the complete elliptic integral of the first and second kind}}{\text{.}} {\text{And}}:K\left( k \right) = \int\limits_0^1 {\frac{1}{{\sqrt {\left( {1 - {x^2}} \right)\left( {1 - {k^2}{x^2}} \right)} }}dx} ,E\left( k \right) = \int\limits_0^1 {\frac{{\sqrt {1 - {k^2}{x^2}} }}{{\sqrt {1 - {x^2}} }}dx}  E(k) K(k)","['calculus', 'integration', 'elliptic-integrals']"
1,Limit $\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2r\right)\frac{\left(-1\right)^{r+k}}{\left(2k-2r-1\right)!}\right)$,Limit,\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2r\right)\frac{\left(-1\right)^{r+k}}{\left(2k-2r-1\right)!}\right),"I am interested in finding this limit and the answer seems to be: $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2r\right)\frac{\left(-1\right)^{r+k}}{\left(2k-2r-1\right)!}\right)=-\sin1$$ I kind of have a method for this but that is based on how I came across this series. I would love to see other solutions that are not based on how it was derived. EDIT: Also as a side note it seems that, $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\frac{\left(-1\right)^{k+r}}{\left(2k-2r-1\right)!}\right)=-\sin1$$ The series without $\zeta$ equals the same. EDIT 2: I realized we could first simplify a bit by $r\to k-1+1-r=k-r$ $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2k-2r\right)\frac{\left(-1\right)^{r}}{\left(2r-1\right)!}\right)=-\sin1$$ Also notice that, $$\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\frac{\left(-1\right)^{r}}{\left(2r-1\right)!}\right)=-\sin1$$ So I thought lets try it on other series, Numerically, $$\lim_{k\to \infty}\left(\sum_{r=0}^{k}\frac{\zeta\left(2k-2r\right)}{r!}\right)=e$$ Also, notice that. $$\lim_{k\to \infty}\left(\sum_{r=0}^{k}\frac{1}{r!}\right)=e$$ This is kind of interesting, shouldn't the $\zeta$ term affect the summation?","I am interested in finding this limit and the answer seems to be: I kind of have a method for this but that is based on how I came across this series. I would love to see other solutions that are not based on how it was derived. EDIT: Also as a side note it seems that, The series without equals the same. EDIT 2: I realized we could first simplify a bit by Also notice that, So I thought lets try it on other series, Numerically, Also, notice that. This is kind of interesting, shouldn't the term affect the summation?",\lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2r\right)\frac{\left(-1\right)^{r+k}}{\left(2k-2r-1\right)!}\right)=-\sin1 \lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\frac{\left(-1\right)^{k+r}}{\left(2k-2r-1\right)!}\right)=-\sin1 \zeta r\to k-1+1-r=k-r \lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\zeta\left(2k-2r\right)\frac{\left(-1\right)^{r}}{\left(2r-1\right)!}\right)=-\sin1 \lim_{k\to\infty}\left(\sum_{r=1}^{k-1}\frac{\left(-1\right)^{r}}{\left(2r-1\right)!}\right)=-\sin1 \lim_{k\to \infty}\left(\sum_{r=0}^{k}\frac{\zeta\left(2k-2r\right)}{r!}\right)=e \lim_{k\to \infty}\left(\sum_{r=0}^{k}\frac{1}{r!}\right)=e \zeta,"['calculus', 'limits', 'summation', 'riemann-zeta']"
2,"Compute ; $\Gamma=\lim_{n\to\infty} \int_{t_n}^{t_{n+1}} \frac{(f(x-t_n))^{g(t_{n+1}-x)}}{(f(t_{n+1}-x))^{g(x-t_n)}+(f(x-t_n))^{g(t_{n+1}-x)}} \, dx$",Compute ;,"\Gamma=\lim_{n\to\infty} \int_{t_n}^{t_{n+1}} \frac{(f(x-t_n))^{g(t_{n+1}-x)}}{(f(t_{n+1}-x))^{g(x-t_n)}+(f(x-t_n))^{g(t_{n+1}-x)}} \, dx","Let the functions be defined as $g:R\to R$ and $f:R \to (1,\infty)$ . Both f and g are continuous. Let $F_n$ denote the $n^{th}$ Fibonacci number, then for $n ∈ N$ let $\Gamma$ and $t_n$ be defined as follows; $$t_n=\sqrt [n]{(2n-1)!! F_n}$$ $$\Gamma=\lim_{n\to\infty} \int_{t_n}^{t_{n+1}}  \frac{(f(x-t_n))^{g(t_{n+1}-x)}}{(f(t_{n+1}-x))^{g(x-t_n)}+(f(x-t_n))^{g(t_{n+1}-x)}}  \, dx$$ My initial attempt involved the recurrence relation of Fibonacci numbers for $n>1$ ; $\boxed{F_n=F_{n-1}+F_{n-2}}$ and $\boxed{F_0=0, F_1=1}$ . The closed form being; $\boxed{F_n=\frac{1}{\sqrt 5}\left[(\phi)^n-(\psi)^n\right]}$ $\phi$ being the golden ratio and $\psi$ being the conjugate of golden ratio. Not being familiar with the double factorial notation, here's what I have understood/found; $\boxed{n!!=\sum_{k=0}^{\lceil \frac{n}{2} \rceil-1}(n-2k)=n(n-2)(n-4)\cdots}$ and $\boxed{n!=n!!(n-1)!!}$ $$\boxed{t_n=\sqrt[n] {\frac{(2n-1)!!(\phi^n-\psi^n)}{\sqrt5}}}$$ $$\boxed{t_{n+1}=\sqrt[n+1] {\frac{(2n+1)!!(\phi^{n+1}-\psi^{n+1})}{\sqrt5}}}$$ Substituting, $t_n=\sqrt [n]{(2n-1)!! F_n}$ and $t_{n+1}=\sqrt [n+1]{(2n+1)!! F_{n+1}}$ . Let $$\Gamma=\lim_{n\to\infty} \beta_n$$ where $\beta_n$ is; $$\boxed{\int_{\sqrt [n]{(2n-1)!! F_n}}^{\sqrt [n+1]{(2n+1)!! F_{n+1}}} \frac{(f(x-\sqrt [n]{(2n-1)!! F_n}))^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)} \, dx}{(f(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x))^{g\left(x-\sqrt [n]{(2n-1)!! F_n}\right)}+(f(x-\sqrt [n]{(2n-1)!! F_n}))^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)}}}$$ On just a little simplification; $$\boxed{\int_{\sqrt [n]{(2n-1)!! F_n}}^{\sqrt [n+1]{(2n+1)!! F_{n+1}}} \frac{1}{\left[\frac{\left(f\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)\right)^{g\left(x-\sqrt [n]{(2n-1)!! F_n}\right)}}{\left(f\left(x-\sqrt [n]{(2n-1)!! F_n}\right)\right)^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)}}\right]+1}\,dx}$$ I'm now left more confused than before, was this the right approach at all? If so, how does one continue from here? Note - This question did not come with the solution, so I am not certain if the limit converges (if that is necessary)","Let the functions be defined as and . Both f and g are continuous. Let denote the Fibonacci number, then for let and be defined as follows; My initial attempt involved the recurrence relation of Fibonacci numbers for ; and . The closed form being; being the golden ratio and being the conjugate of golden ratio. Not being familiar with the double factorial notation, here's what I have understood/found; and Substituting, and . Let where is; On just a little simplification; I'm now left more confused than before, was this the right approach at all? If so, how does one continue from here? Note - This question did not come with the solution, so I am not certain if the limit converges (if that is necessary)","g:R\to R f:R \to (1,\infty) F_n n^{th} n ∈ N \Gamma t_n t_n=\sqrt [n]{(2n-1)!! F_n} \Gamma=\lim_{n\to\infty} \int_{t_n}^{t_{n+1}}
 \frac{(f(x-t_n))^{g(t_{n+1}-x)}}{(f(t_{n+1}-x))^{g(x-t_n)}+(f(x-t_n))^{g(t_{n+1}-x)}}
 \, dx n>1 \boxed{F_n=F_{n-1}+F_{n-2}} \boxed{F_0=0, F_1=1} \boxed{F_n=\frac{1}{\sqrt 5}\left[(\phi)^n-(\psi)^n\right]} \phi \psi \boxed{n!!=\sum_{k=0}^{\lceil \frac{n}{2} \rceil-1}(n-2k)=n(n-2)(n-4)\cdots} \boxed{n!=n!!(n-1)!!} \boxed{t_n=\sqrt[n] {\frac{(2n-1)!!(\phi^n-\psi^n)}{\sqrt5}}} \boxed{t_{n+1}=\sqrt[n+1] {\frac{(2n+1)!!(\phi^{n+1}-\psi^{n+1})}{\sqrt5}}} t_n=\sqrt [n]{(2n-1)!! F_n} t_{n+1}=\sqrt [n+1]{(2n+1)!! F_{n+1}} \Gamma=\lim_{n\to\infty} \beta_n \beta_n \boxed{\int_{\sqrt [n]{(2n-1)!! F_n}}^{\sqrt [n+1]{(2n+1)!! F_{n+1}}} \frac{(f(x-\sqrt [n]{(2n-1)!! F_n}))^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)} \, dx}{(f(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x))^{g\left(x-\sqrt [n]{(2n-1)!! F_n}\right)}+(f(x-\sqrt [n]{(2n-1)!! F_n}))^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)}}} \boxed{\int_{\sqrt [n]{(2n-1)!! F_n}}^{\sqrt [n+1]{(2n+1)!! F_{n+1}}} \frac{1}{\left[\frac{\left(f\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)\right)^{g\left(x-\sqrt [n]{(2n-1)!! F_n}\right)}}{\left(f\left(x-\sqrt [n]{(2n-1)!! F_n}\right)\right)^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)}}\right]+1}\,dx}","['calculus', 'integration', 'recurrence-relations']"
3,Probability of circumcircle of 3 random points in a circle to be contained within that circle,Probability of circumcircle of 3 random points in a circle to be contained within that circle,,"Imagine a circle $C_1$ that has radius $a$ . We then chose at random, and independently, three points from the interior of that circle. These three points, if non-collinear, uniquely determine another circle $C_2$ ; $C_2$ may or may not be totally contained within $C_1$ . What is the probability that $C_2$ lies totally inside $C_1$ ? This is from Paul Nahin's book 'Inside Interesting Integrals' (pg. 25 in second edition) Below is the full analytic solution given in the book. However, it goes on to discuss that performing a Monte Carlo simulation gives a value in the range $0.39992$ to $0.400972$ while our answer is $0.418879$ ; alternatively, the answer seems to be $0.4$ which is $\frac{2\cdot3}{15}$ , while our answer is $\frac{2\cdot \pi}{15}$ . So where exactly did we make a mistake? Here are my thoughts- When we are considering all those $C_2$ inside $C_1$ , there will clearly be overlap between the cases. Say case 1 is circle of radius $\frac{a}{3}$ centered at $\frac{a}{2}$ , and case 2 is same radius but centered at $\frac{a}{4}$ , these will have some intersection area along radical axis. Here he seems to have assumed the two cases to be disjoint, but are they really? We call it as 'probability of all 3 being in circumference band of $C_2$ , but in reality ofcourse it is sum over its own continuum of cases- probability of all 3 being in this little arc of the $\Delta(x)$ band of C2 + probability of all 3 being in the next little arc of C2 + and so on. NOW my point is- that little intersection area along radical axis, it will appear in this expansion in both cases, so we are in fact overcounting it. It seems insignificant since we're thinking of only that particular pair, but any particular intersection area square in the plane will be counted infinitely many times (there’s an infinitude of circles of just the right appropriately diff radius and centers that intersect at the exact same place). In other words, number of times an intersection area is overcounted is proportional to number of pairs of circles, but number of times it is supposed to be counted is proportional to just the number of circles.","Imagine a circle that has radius . We then chose at random, and independently, three points from the interior of that circle. These three points, if non-collinear, uniquely determine another circle ; may or may not be totally contained within . What is the probability that lies totally inside ? This is from Paul Nahin's book 'Inside Interesting Integrals' (pg. 25 in second edition) Below is the full analytic solution given in the book. However, it goes on to discuss that performing a Monte Carlo simulation gives a value in the range to while our answer is ; alternatively, the answer seems to be which is , while our answer is . So where exactly did we make a mistake? Here are my thoughts- When we are considering all those inside , there will clearly be overlap between the cases. Say case 1 is circle of radius centered at , and case 2 is same radius but centered at , these will have some intersection area along radical axis. Here he seems to have assumed the two cases to be disjoint, but are they really? We call it as 'probability of all 3 being in circumference band of , but in reality ofcourse it is sum over its own continuum of cases- probability of all 3 being in this little arc of the band of C2 + probability of all 3 being in the next little arc of C2 + and so on. NOW my point is- that little intersection area along radical axis, it will appear in this expansion in both cases, so we are in fact overcounting it. It seems insignificant since we're thinking of only that particular pair, but any particular intersection area square in the plane will be counted infinitely many times (there’s an infinitude of circles of just the right appropriately diff radius and centers that intersect at the exact same place). In other words, number of times an intersection area is overcounted is proportional to number of pairs of circles, but number of times it is supposed to be counted is proportional to just the number of circles.",C_1 a C_2 C_2 C_1 C_2 C_1 0.39992 0.400972 0.418879 0.4 \frac{2\cdot3}{15} \frac{2\cdot \pi}{15} C_2 C_1 \frac{a}{3} \frac{a}{2} \frac{a}{4} C_2 \Delta(x),"['calculus', 'probability', 'integration', 'geometry', 'solution-verification']"
4,Solving $\min\int_{0}^{1} \left( x^2 + 2tx + tx\dot{x} + \dot{x}^2 \right) {\rm d} t$ with $x(0)=0$ and $x(1)=1$,Solving  with  and,\min\int_{0}^{1} \left( x^2 + 2tx + tx\dot{x} + \dot{x}^2 \right) {\rm d} t x(0)=0 x(1)=1,"Solve $$\min\int_{0}^{1} \left( x^2 + 2tx + tx\dot{x} + \dot{x}^2 \right) {\rm d} t, \qquad x(0)=0, \quad x(1)=1$$ We need to solve the problem with the Euler Equation $$\frac{\partial F}{\partial x}-\frac{d}{dt}\left(\frac{\partial F}{\partial\dot{x}}\right)=0$$ We define $F$ as $$F(t,x,\dot{x})=x^2+2tx+tx\dot{x}+\dot{x}^2$$ Then subsequently we have: $$ \begin{aligned} \frac{\partial F}{\partial x} &= 2x+2t+t\dot{x} \\ \frac{\partial F}{\partial\dot{x}} &= tx+2\dot{x} \end{aligned} $$ When substituting this into the above mentioned equation: $\dfrac{\partial F}{\partial x}-\dfrac{d}{dt}\left(\dfrac{\partial F}{\partial\dot{x}}\right)=0$ , my final equation becomes $\ddot{x}-\frac{1}{2}x=t$ which is incorrect recording to the Student's Manual. The correct 'answer' should be $\ddot{x}-\frac{1}{2}x=\frac{1}{2}t$ . Could someone point out where I made a small/big mistake?","Solve We need to solve the problem with the Euler Equation We define as Then subsequently we have: When substituting this into the above mentioned equation: , my final equation becomes which is incorrect recording to the Student's Manual. The correct 'answer' should be . Could someone point out where I made a small/big mistake?","\min\int_{0}^{1} \left( x^2 + 2tx + tx\dot{x} + \dot{x}^2 \right) {\rm d} t, \qquad x(0)=0, \quad x(1)=1 \frac{\partial F}{\partial x}-\frac{d}{dt}\left(\frac{\partial F}{\partial\dot{x}}\right)=0 F F(t,x,\dot{x})=x^2+2tx+tx\dot{x}+\dot{x}^2  \begin{aligned} \frac{\partial F}{\partial x} &= 2x+2t+t\dot{x} \\ \frac{\partial F}{\partial\dot{x}} &= tx+2\dot{x} \end{aligned}  \dfrac{\partial F}{\partial x}-\dfrac{d}{dt}\left(\dfrac{\partial F}{\partial\dot{x}}\right)=0 \ddot{x}-\frac{1}{2}x=t \ddot{x}-\frac{1}{2}x=\frac{1}{2}t","['calculus', 'ordinary-differential-equations', 'derivatives', 'calculus-of-variations', 'euler-lagrange-equation']"
5,Closed form for $\int \sqrt[n]{\tan x}\ dx$,Closed form for,\int \sqrt[n]{\tan x}\ dx,"I was solving $\displaystyle\int\sqrt[n]{\tan x}\ dx$ . Here's my method: $$\begin{align}\int\sqrt[n]{\tan x}\ dx &= \int\frac{n \cdot t^n}{1 + (t)^{2n}}\tag{1}\ dt\\& = n \int\sum_{k=0}^\infty (-1)^k (t)^{2nk}\cdot t^n \ dt\tag{2} \\& = n\sum_{k=0}^\infty(-1)^k \int t^{2nk+n}\ dt \tag{3}\\& = n \sum_{k=0}^\infty(-1)^k \frac{t^{2nk +n+1}}{2nk +n+1} + C\tag{4}\\& =n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{\frac{2nk +n+1}n}}{2nk +n+1} + C\tag{5} \\& = \boxed{n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{2k +\frac{n+1}n}}{2nk +n+1} + C}\end{align}$$ Steps: $(1)$ Substitution: $\tan{x} = t^n$ $(2)$ Taylor series: $\displaystyle\frac{1}{1+t} = \sum_{k=0}^\infty (-1)^k (t)^k\implies \frac{1}{1+t^{2n}} = \sum_{k=0}^\infty (-1)^k (t^{2n})^k$ . $(3)$ Interchanged integral and summation symbols. $(4)$ Used power rule of integration. $(5)$ Undone the substitution. Source: I was practicing integral calculus and came across $\displaystyle \int \sqrt{\tan x}\ dx$ and $\displaystyle \int \sqrt[3]{\tan x}\ dx $ . Both of them were nice and I solved them. So I thought there would be definitely a general solution for $\displaystyle \int \sqrt[n]{\tan x}\ dx$ where $n\in \mathbb{Z}^+$ . My question: Answers for $\int \sqrt{\tan x}\ dx$ and $\sqrt[3]{\tan {x}} $ were looking good, at least elementary (Having closed form). I expected the same for $\int \sqrt[n]{\tan x}\ dx$ . Is there any closed form for $n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{2k +\frac{n+1}n}}{2nk +n+1} + C$ ? And is my method right?","I was solving . Here's my method: Steps: Substitution: Taylor series: . Interchanged integral and summation symbols. Used power rule of integration. Undone the substitution. Source: I was practicing integral calculus and came across and . Both of them were nice and I solved them. So I thought there would be definitely a general solution for where . My question: Answers for and were looking good, at least elementary (Having closed form). I expected the same for . Is there any closed form for ? And is my method right?",\displaystyle\int\sqrt[n]{\tan x}\ dx \begin{align}\int\sqrt[n]{\tan x}\ dx &= \int\frac{n \cdot t^n}{1 + (t)^{2n}}\tag{1}\ dt\\& = n \int\sum_{k=0}^\infty (-1)^k (t)^{2nk}\cdot t^n \ dt\tag{2} \\& = n\sum_{k=0}^\infty(-1)^k \int t^{2nk+n}\ dt \tag{3}\\& = n \sum_{k=0}^\infty(-1)^k \frac{t^{2nk +n+1}}{2nk +n+1} + C\tag{4}\\& =n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{\frac{2nk +n+1}n}}{2nk +n+1} + C\tag{5} \\& = \boxed{n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{2k +\frac{n+1}n}}{2nk +n+1} + C}\end{align} (1) \tan{x} = t^n (2) \displaystyle\frac{1}{1+t} = \sum_{k=0}^\infty (-1)^k (t)^k\implies \frac{1}{1+t^{2n}} = \sum_{k=0}^\infty (-1)^k (t^{2n})^k (3) (4) (5) \displaystyle \int \sqrt{\tan x}\ dx \displaystyle \int \sqrt[3]{\tan x}\ dx  \displaystyle \int \sqrt[n]{\tan x}\ dx n\in \mathbb{Z}^+ \int \sqrt{\tan x}\ dx \sqrt[3]{\tan {x}}  \int \sqrt[n]{\tan x}\ dx n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{2k +\frac{n+1}n}}{2nk +n+1} + C,"['calculus', 'integration', 'power-series', 'indefinite-integrals', 'closed-form']"
6,"What is $\lim_{t\rightarrow \infty }\int_{a}^{b}f(x,\sin(tx))dx$？ [closed]",What is ？ [closed],"\lim_{t\rightarrow \infty }\int_{a}^{b}f(x,\sin(tx))dx","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question Let f $\in C([a,b]\times [-1,1]) $ ,What is the value of this integral: $\lim_{t\rightarrow \infty }\int_{a}^{b}f(x,sin(tx))dx$ This is a topic I encountered when I was studying mathematical analysis recently, I want to know how to solve this problem, is there some theoretical background? First of all, f is a bounded continuous binary function, so we can get that this integral must also be bounded.Then I tried the commutation method so that tx=u, but I couldn't push the final result. This is the result of my appeal operation： $\lim_{t\rightarrow \infty }\frac{1}{t}\int_{at}^{bt}f(\frac{u}{t},sin(u))du$ I would appreciate it if you could solve it.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question Let f ,What is the value of this integral: This is a topic I encountered when I was studying mathematical analysis recently, I want to know how to solve this problem, is there some theoretical background? First of all, f is a bounded continuous binary function, so we can get that this integral must also be bounded.Then I tried the commutation method so that tx=u, but I couldn't push the final result. This is the result of my appeal operation： I would appreciate it if you could solve it.","\in C([a,b]\times [-1,1])  \lim_{t\rightarrow \infty }\int_{a}^{b}f(x,sin(tx))dx \lim_{t\rightarrow \infty }\frac{1}{t}\int_{at}^{bt}f(\frac{u}{t},sin(u))du","['calculus', 'integration', 'limits', 'analysis']"
7,evaluation of $\int\tan^{8}{x}dx$,evaluation of,\int\tan^{8}{x}dx,"Compute the indefinite integral $$\displaystyle \int\tan^{8}{x}dx$$ My Attempt:(proposed solution) $$\displaystyle \tan{x}=z\Rightarrow dz=(1+\tan^{2}{x})dx$$ $$\displaystyle dx=\frac{dz}{1+z^{2}}$$ $$\displaystyle z^{8}=(1+z^{2})(z^{6}-z^{4}+z^{2}-1)+1$$ $$\displaystyle \int\tan^{8}{x}dx=\int((z^{6}-z^{4}+z^{2}-1))dz+\int dx$$ $$\displaystyle =\frac{\tan^{7}{x}}{7}-\frac{\tan^{5}{x}}{5}+\frac{\tan^{3}{x}}{3}-\tan{x}+x+c$$ Thank you (I learned a lot thanks to you, thank you)","Compute the indefinite integral My Attempt:(proposed solution) Thank you (I learned a lot thanks to you, thank you)",\displaystyle \int\tan^{8}{x}dx \displaystyle \tan{x}=z\Rightarrow dz=(1+\tan^{2}{x})dx \displaystyle dx=\frac{dz}{1+z^{2}} \displaystyle z^{8}=(1+z^{2})(z^{6}-z^{4}+z^{2}-1)+1 \displaystyle \int\tan^{8}{x}dx=\int((z^{6}-z^{4}+z^{2}-1))dz+\int dx \displaystyle =\frac{\tan^{7}{x}}{7}-\frac{\tan^{5}{x}}{5}+\frac{\tan^{3}{x}}{3}-\tan{x}+x+c,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
8,Evaluate $\int_{0}^{\pi}\sqrt\frac{1+\cos2x}{x^2+1}dx$.,Evaluate .,\int_{0}^{\pi}\sqrt\frac{1+\cos2x}{x^2+1}dx,"Evaluate $$\int_{0}^{\pi}\sqrt\frac{1+\cos2x}{x^2+1}\,\mathrm dx$$ We have $\sqrt{1+\cos2x}=\sqrt{2\cos^2x}=\sqrt{2} |\cos x|$ . Then we need to solve $\int_{0}^{\pi}\frac{\sqrt{2} |\cos x|}{\sqrt{x^2+1}}dx$ . Using symmetry of $\cos x$ from $0$ to $\pi$ the integral becomes $I=2\sqrt2\int_{0}^{\pi/2}\frac{ \cos x}{\sqrt{x^2+1}}dx$ . I tried using ""By Parts"", it didn't helped much. Take the substitution $x=\tan \theta$ then we have $I=\int_{0}^{\tan^{-1}\pi/2}\cos (\tan \theta)\sec \theta d\theta$ . I am having difficulty in solving with the existing methods that I know. Edit : After knowing that I can not assume the symmetry of the given integral the question will be to solve $$\sqrt2\left(\int_{0}^{\pi/2} \frac{\cos x}{\sqrt {x^2+1}}\,\mathrm dx-\int_{\pi/2}^{\pi} \frac{\cos x}{\sqrt {x^2+1}}\,\mathrm dx\right) $$ Again I will face same problem to proceed ahead.","Evaluate We have . Then we need to solve . Using symmetry of from to the integral becomes . I tried using ""By Parts"", it didn't helped much. Take the substitution then we have . I am having difficulty in solving with the existing methods that I know. Edit : After knowing that I can not assume the symmetry of the given integral the question will be to solve Again I will face same problem to proceed ahead.","\int_{0}^{\pi}\sqrt\frac{1+\cos2x}{x^2+1}\,\mathrm dx \sqrt{1+\cos2x}=\sqrt{2\cos^2x}=\sqrt{2} |\cos x| \int_{0}^{\pi}\frac{\sqrt{2} |\cos x|}{\sqrt{x^2+1}}dx \cos x 0 \pi I=2\sqrt2\int_{0}^{\pi/2}\frac{ \cos x}{\sqrt{x^2+1}}dx x=\tan \theta I=\int_{0}^{\tan^{-1}\pi/2}\cos (\tan \theta)\sec \theta d\theta \sqrt2\left(\int_{0}^{\pi/2} \frac{\cos x}{\sqrt {x^2+1}}\,\mathrm dx-\int_{\pi/2}^{\pi} \frac{\cos x}{\sqrt {x^2+1}}\,\mathrm dx\right) ","['calculus', 'integration', 'definite-integrals']"
9,Are there at least 45 variants of l'Hopital's Rule and how to prove variant with $\lim\limits_{x\to a^+} f(x)= \lim\limits_{x\to a^+} g(x) = \infty$?,Are there at least 45 variants of l'Hopital's Rule and how to prove variant with ?,\lim\limits_{x\to a^+} f(x)= \lim\limits_{x\to a^+} g(x) = \infty,"The following is a problem in Ch. 11 ""The Significance of the Derivative"" from Spivak's Calculus To complete the orgy of variations on l'Hopital's Rule, use Problem 55 to prove a few more cases of the following general statement (there are so many possibilities that you should select just a few, if any, that interest you): If $\lim\limits_{x\to [\ ]} f(x)= \lim\limits_{x\to [\ ]} g(x) = \{\  \}$ and $\lim\limits_{x\to [\ ]} \frac{f'(x)}{g'(x)}=(\ )$ , then $\lim\limits_{x\to [\ ]}  \frac{f(x)}{g(x)}=(\ )$ . Here $[\ ]$ can be $a$ or $a^+$ or $a^-$ or $\infty$ or $-\infty$ , and $\{\ \}$ can be $0$ or $\infty$ or $-\infty$ , and $(\ )$ can be $l$ or $\infty$ or $-\infty$ . First of all, given the problem statement above, is it accurate to say that there are at least $5\cdot 3 \cdot 3=45$ variations of L'Hopital's Rule? I've seen questions on proving L'Hopital's Rule but given that there are many variations on the rule, and the fact that the proofs are not all exactly the same (I've gone through at least ten so far), I'd like to ask about one particular variation that I can't seem to find a proof for. When we let $[\ ]$ be $a$ , $a^+$ , or $a^-$ and let $\{\ \}$ be $\infty$ , for example, I can't seem to come up with a proof. To cut to the chase, I'd like to know how to prove the following If $\lim\limits_{x\to a^+} f(x)= \lim\limits_{x\to a^+} g(x) = \infty$ and $\lim\limits_{x\to a^+} \frac{f'(x)}{g'(x)}=l$ , then $\lim\limits_{x\to a^+} \frac{f(x)}{g(x)}=l$ . The proofs of l'Hopital's Rule I have seen usually start by using the assumption that the limit $\lim\limits_{x\to [\ ]} \frac{f'(x)}{g'(x)}=(\ )$ exists to establish existence of $f'$ and $g'$ and $g'\neq 0$ on a certain interval $A$ that depends on what the limiting value is in the limits, ie what is chosen as the $[\ ]$ parameter. Next the Mean Value Theorem is invoked to establish either that $g(x)\neq 0$ or $g(x)-g(a)\neq 0$ on interval $A$ . Then the Cauchy-Schwarz MVT is invoked to obtain some relationship of one of the forms $$\frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}$$ $$\frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}$$ where $\alpha_x \in A$ . At this point the limit of both sides is taken and we end up with the conclusion of the particular variant L'Hopital's Rule we are proving. Obviously I skipped over a few steps in this description, but this is a general outline. I believe the issue I have for proving the statement I asked about above is application of MVT. If the limit of $f$ and $g$ when $x$ approaches $a$ is $\infty$ , we can't use point $a$ to apply the MVT as is done in the proofs of all the other variations of L'Hopital's Rule as far as I can tell.","The following is a problem in Ch. 11 ""The Significance of the Derivative"" from Spivak's Calculus To complete the orgy of variations on l'Hopital's Rule, use Problem 55 to prove a few more cases of the following general statement (there are so many possibilities that you should select just a few, if any, that interest you): If and , then . Here can be or or or or , and can be or or , and can be or or . First of all, given the problem statement above, is it accurate to say that there are at least variations of L'Hopital's Rule? I've seen questions on proving L'Hopital's Rule but given that there are many variations on the rule, and the fact that the proofs are not all exactly the same (I've gone through at least ten so far), I'd like to ask about one particular variation that I can't seem to find a proof for. When we let be , , or and let be , for example, I can't seem to come up with a proof. To cut to the chase, I'd like to know how to prove the following If and , then . The proofs of l'Hopital's Rule I have seen usually start by using the assumption that the limit exists to establish existence of and and on a certain interval that depends on what the limiting value is in the limits, ie what is chosen as the parameter. Next the Mean Value Theorem is invoked to establish either that or on interval . Then the Cauchy-Schwarz MVT is invoked to obtain some relationship of one of the forms where . At this point the limit of both sides is taken and we end up with the conclusion of the particular variant L'Hopital's Rule we are proving. Obviously I skipped over a few steps in this description, but this is a general outline. I believe the issue I have for proving the statement I asked about above is application of MVT. If the limit of and when approaches is , we can't use point to apply the MVT as is done in the proofs of all the other variations of L'Hopital's Rule as far as I can tell.","\lim\limits_{x\to [\ ]} f(x)= \lim\limits_{x\to [\ ]} g(x) = \{\
 \} \lim\limits_{x\to [\ ]} \frac{f'(x)}{g'(x)}=(\ ) \lim\limits_{x\to [\ ]}  \frac{f(x)}{g(x)}=(\ ) [\ ] a a^+ a^- \infty -\infty \{\ \} 0 \infty -\infty (\ ) l \infty -\infty 5\cdot 3 \cdot 3=45 [\ ] a a^+ a^- \{\ \} \infty \lim\limits_{x\to a^+} f(x)= \lim\limits_{x\to a^+} g(x) = \infty \lim\limits_{x\to a^+} \frac{f'(x)}{g'(x)}=l \lim\limits_{x\to a^+} \frac{f(x)}{g(x)}=l \lim\limits_{x\to [\ ]} \frac{f'(x)}{g'(x)}=(\ ) f' g' g'\neq 0 A [\ ] g(x)\neq 0 g(x)-g(a)\neq 0 A \frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)} \frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f'(\alpha_x)}{g'(\alpha_x)} \alpha_x \in A f g x a \infty a","['calculus', 'limits', 'derivatives']"
10,Proof verification related to the Intermediate value theorem.,Proof verification related to the Intermediate value theorem.,,"Suppose that f is a continuous function on [0, 2] such that f(0) = f(2). Show that there is a real number ξ ∈ [1, 2] with f(ξ) = f(ξ − 1). ξ ∈ [1, 2], ξ-1 ∈ [0, 1] Case 1: if f(0)=f(1) then ξ = 2, f(ξ) = f(0)= f(1) = f(2) therefore f(ξ)= f(ξ-1)=f(2)=f(1) Case 2: if f(0)>f(1) then f(2) > f(1). By continuity there must exist some m where f(2) ≥ m ≥ f(1) and by IMV theorem there exists some ξ ∈ [1, 2] where f(ξ)=m. Now, ξ-1 ∈ [0, 1] and by continuity there must also exist the same m, f(0)≥ m ≥ f(1). Therefore by IMV theorem again there must exist at least 1 value where f(ξ − 1) = m and since m is common there must exist at least 1 value where f(ξ − 1) = f(ξ). Case 3: f(1)>f(0). This is pretty much the same thing as case 2. -This is a very difficult calculus question and I've never written a proof and I'm in Algebra II. Pls don't flame me if this is very stupid.","Suppose that f is a continuous function on [0, 2] such that f(0) = f(2). Show that there is a real number ξ ∈ [1, 2] with f(ξ) = f(ξ − 1). ξ ∈ [1, 2], ξ-1 ∈ [0, 1] Case 1: if f(0)=f(1) then ξ = 2, f(ξ) = f(0)= f(1) = f(2) therefore f(ξ)= f(ξ-1)=f(2)=f(1) Case 2: if f(0)>f(1) then f(2) > f(1). By continuity there must exist some m where f(2) ≥ m ≥ f(1) and by IMV theorem there exists some ξ ∈ [1, 2] where f(ξ)=m. Now, ξ-1 ∈ [0, 1] and by continuity there must also exist the same m, f(0)≥ m ≥ f(1). Therefore by IMV theorem again there must exist at least 1 value where f(ξ − 1) = m and since m is common there must exist at least 1 value where f(ξ − 1) = f(ξ). Case 3: f(1)>f(0). This is pretty much the same thing as case 2. -This is a very difficult calculus question and I've never written a proof and I'm in Algebra II. Pls don't flame me if this is very stupid.",,"['calculus', 'solution-verification', 'learning']"
11,Looking for a direct way to evaluate $\int_0^1\frac{\ln(x)\ln(2+x)}{1+x}dx$,Looking for a direct way to evaluate,\int_0^1\frac{\ln(x)\ln(2+x)}{1+x}dx,"The following integral $$I=\int_0^1\frac{\ln(x)\ln(2+x)}{1+x}dx=-\frac{13}{24}\zeta(3)$$ has been evaluated in different ways ( see here , here , here , and here ) but these four solutions involve many arguments of polylogs and much simplifications are required to reach the final closed form. I am wondering if there is a simpler direct way to prove it. I tried some tricks but none worked out. Here is my best try: Following @Zacky's idea , lets denote: $$ I(a)=\int_0^1\frac{\ln(x)\ln(1+a(1+x))}{1+x}dx$$ and note that $I(1)=I$ and $I(0)=0,$ $$I'(a)=\int_0^1\frac{\ln(x)}{1+a(1+x)}dx=\frac{\text{Li}_2\left(-\frac{a}{1+a}\right)}{a}.$$ Integrate both sides from $a=0$ to $1$ , $$ \int_0^1 I'(a)=I(a)|_0^1=I(1)-I(0)=I-0=I=\int_0^1\frac{\text{Li}_2\left(-\frac{a}{1+a}\right)}{a}da$$ Integrate by parts, $$I=\int_0^1\frac{\ln(a)\ln(1+2a)}{a(1+a)}da-\int_0^1\frac{\ln(a)\ln(1+a)}{a(1+a)}da$$ $$=\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+\frac{5}{8}\zeta(3).$$ To finish the proof, we need to show $$\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx=-\frac76\zeta(3)$$ and i am stuck here. I also added $I$ to both sides: $$2I=\int_0^1\frac{\ln(x)\ln\left(\frac{2+x}{1+2x}\right)}{1+x}dx+\int_0^1\frac{\ln(x)\ln(1+2x)}{x}dx$$ then used the subbing $x\to (1-x)/(1+x)$ for the first integral: $$2I=\int_0^1\frac{\ln\left(\frac{1-x}{1+x}\right)\ln\left(\frac{3+x}{3-x}\right)}{1+x}dx+\int_0^1\frac{\ln(x)\ln(1+2x)}{x}dx$$ and I made it even worse. Any other thoughts ( without complicated arguments of polylogs ) ? thanks, Bonus: Let's use the relation we obtained above: $$I=\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+\frac{5}{8}\zeta(3)$$ $$I=\int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx-\underbrace{\int_1^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx}_{x\to 1/x}+\frac{5}{8}\zeta(3)$$ $$I=\int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+I-\int_0^1\frac{\ln^2(x)}{1+x}dx+\frac{5}{8}\zeta(3)$$ $$\Longrightarrow \int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx=\int_0^1\frac{\ln^2(x)}{1+x}dx-\frac{5}{8}\zeta(3)=\frac32\zeta(3)-\frac58\zeta(3)=\frac78\zeta(3).$$","The following integral has been evaluated in different ways ( see here , here , here , and here ) but these four solutions involve many arguments of polylogs and much simplifications are required to reach the final closed form. I am wondering if there is a simpler direct way to prove it. I tried some tricks but none worked out. Here is my best try: Following @Zacky's idea , lets denote: and note that and Integrate both sides from to , Integrate by parts, To finish the proof, we need to show and i am stuck here. I also added to both sides: then used the subbing for the first integral: and I made it even worse. Any other thoughts ( without complicated arguments of polylogs ) ? thanks, Bonus: Let's use the relation we obtained above:","I=\int_0^1\frac{\ln(x)\ln(2+x)}{1+x}dx=-\frac{13}{24}\zeta(3)  I(a)=\int_0^1\frac{\ln(x)\ln(1+a(1+x))}{1+x}dx I(1)=I I(0)=0, I'(a)=\int_0^1\frac{\ln(x)}{1+a(1+x)}dx=\frac{\text{Li}_2\left(-\frac{a}{1+a}\right)}{a}. a=0 1  \int_0^1 I'(a)=I(a)|_0^1=I(1)-I(0)=I-0=I=\int_0^1\frac{\text{Li}_2\left(-\frac{a}{1+a}\right)}{a}da I=\int_0^1\frac{\ln(a)\ln(1+2a)}{a(1+a)}da-\int_0^1\frac{\ln(a)\ln(1+a)}{a(1+a)}da =\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+\frac{5}{8}\zeta(3). \int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx=-\frac76\zeta(3) I 2I=\int_0^1\frac{\ln(x)\ln\left(\frac{2+x}{1+2x}\right)}{1+x}dx+\int_0^1\frac{\ln(x)\ln(1+2x)}{x}dx x\to (1-x)/(1+x) 2I=\int_0^1\frac{\ln\left(\frac{1-x}{1+x}\right)\ln\left(\frac{3+x}{3-x}\right)}{1+x}dx+\int_0^1\frac{\ln(x)\ln(1+2x)}{x}dx I=\int_0^1\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+\frac{5}{8}\zeta(3) I=\int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx-\underbrace{\int_1^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx}_{x\to 1/x}+\frac{5}{8}\zeta(3) I=\int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx+I-\int_0^1\frac{\ln^2(x)}{1+x}dx+\frac{5}{8}\zeta(3) \Longrightarrow \int_0^\infty\frac{\ln(x)\ln(1+2x)}{x(1+x)}dx=\int_0^1\frac{\ln^2(x)}{1+x}dx-\frac{5}{8}\zeta(3)=\frac32\zeta(3)-\frac58\zeta(3)=\frac78\zeta(3).","['calculus', 'integration', 'definite-integrals', 'alternative-proof']"
12,How many ways to deal with the integral $\int \frac{d x}{\sqrt{1+x}-\sqrt{1-x}}$?,How many ways to deal with the integral ?,\int \frac{d x}{\sqrt{1+x}-\sqrt{1-x}},"I tackle the integral by rationalization on the integrand first. $$ \frac{1}{\sqrt{1+x}-\sqrt{1-x}}=\frac{\sqrt{1+x}+\sqrt{1-x}}{2 x} $$ Then splitting into two simpler integrals yields $$ \int \frac{d x}{\sqrt{1+x}-\sqrt{1-x}}=\frac{1}{2}\left [\underbrace{\int\frac{\sqrt{1+x}}{x}}_{J} d x+\underbrace{\int\frac{\sqrt{1-x}}{x} d x}_{K}\right] $$ To deal with $J$ , we use rationalization instead of substitution. $$ \begin{aligned} J &=\int \frac{\sqrt{1+x}}{x} d x \\ &=\int \frac{1+x}{x \sqrt{1+x}} d x \\ &=2 \int\left(\frac{1}{x}+1\right) d(\sqrt{1+x}) \\ &=2 \int \frac{d(\sqrt{1+x})}{x}+2 \sqrt{1+x} \\ &=2 \int \frac{d(\sqrt{1+x})}{(\sqrt{1+x})^{2}-1}+2 \sqrt{1+x} \\ &=\ln \left|\frac{\sqrt{1+x}-1}{\sqrt{1+x}+1} \right| +2 \sqrt{1+x}+C_{1} \end{aligned} $$ $\text {Replacing } x \text { by } -x \text { yields }$ $$ \begin{array}{l} \\ \displaystyle K=\int \frac{\sqrt{1-x}}{-x}(-d x)=\ln \left|\frac{\sqrt{1-x}-1}{\sqrt{1-x}+1}\right|+2 \sqrt{1-x}+C_{2} \end{array} $$ Now we can conclude that $$ I=\sqrt{1+x}+\sqrt{1-x}+\frac{1}{2}\left(\ln \left|\frac{\sqrt{1+x}-1}{\sqrt{1+x}+1}\right|+\ln \left|\frac{\sqrt{1-x}-1}{\sqrt{1-x}+1}\right|\right)+C $$ My question is whether there are  any simpler methods such as integration by parts , trigonometric substitution, etc… Please help if you have. Thank you for your attention.","I tackle the integral by rationalization on the integrand first. Then splitting into two simpler integrals yields To deal with , we use rationalization instead of substitution. Now we can conclude that My question is whether there are  any simpler methods such as integration by parts , trigonometric substitution, etc… Please help if you have. Thank you for your attention.","
\frac{1}{\sqrt{1+x}-\sqrt{1-x}}=\frac{\sqrt{1+x}+\sqrt{1-x}}{2 x}
 
\int \frac{d x}{\sqrt{1+x}-\sqrt{1-x}}=\frac{1}{2}\left [\underbrace{\int\frac{\sqrt{1+x}}{x}}_{J} d x+\underbrace{\int\frac{\sqrt{1-x}}{x} d x}_{K}\right]
 J 
\begin{aligned}
J &=\int \frac{\sqrt{1+x}}{x} d x \\
&=\int \frac{1+x}{x \sqrt{1+x}} d x \\
&=2 \int\left(\frac{1}{x}+1\right) d(\sqrt{1+x}) \\
&=2 \int \frac{d(\sqrt{1+x})}{x}+2 \sqrt{1+x} \\
&=2 \int \frac{d(\sqrt{1+x})}{(\sqrt{1+x})^{2}-1}+2 \sqrt{1+x} \\
&=\ln \left|\frac{\sqrt{1+x}-1}{\sqrt{1+x}+1} \right| +2 \sqrt{1+x}+C_{1}
\end{aligned}
 \text {Replacing } x \text { by } -x \text { yields } 
\begin{array}{l} \\
\displaystyle K=\int \frac{\sqrt{1-x}}{-x}(-d x)=\ln \left|\frac{\sqrt{1-x}-1}{\sqrt{1-x}+1}\right|+2 \sqrt{1-x}+C_{2}
\end{array}
 
I=\sqrt{1+x}+\sqrt{1-x}+\frac{1}{2}\left(\ln \left|\frac{\sqrt{1+x}-1}{\sqrt{1+x}+1}\right|+\ln \left|\frac{\sqrt{1-x}-1}{\sqrt{1-x}+1}\right|\right)+C
","['calculus', 'integration', 'indefinite-integrals', 'radicals']"
13,If $f(f(f(x)))+f(x)=2$ for all $0≤x≤2$ then find $\int_0^2 f(x) dx$,If  for all  then find,f(f(f(x)))+f(x)=2 0≤x≤2 \int_0^2 f(x) dx,"If $f(f(f(x)))+f(x)=2$ for all $0≤x≤2$ , where $f(x)$ is a continuous function, then find $\int_0^2 f(x) dx$ Substitute $f(x)=2-t$ to get $$\begin{equation} f(f(2-t))=t \end{equation}$$ Take inverse on both sides and then integrate from $0$ to $2$ $$\int_0^2f(2-t)=\int_0^2f^{-1}(t)$$ On the other hand, substituting $f(f(x))=z$ in original equation gives $$f(z)=2-f^{-1}(z)$$ Putting the value of $f^{-1}(z)$ back we can solve for $\int_0^2 f(x) dx=2$ My solution gave me the right answer, but it's clearly wrong because 1) the range of $f(x)$ or $f(f(x))$ need not be a subset of the domain and 2) inverse of $f(x)$ may not exist. Also, the given data $0≤x≤2$ seems to be superfluous here. So what is the proper way of doing it?","If for all , where is a continuous function, then find Substitute to get Take inverse on both sides and then integrate from to On the other hand, substituting in original equation gives Putting the value of back we can solve for My solution gave me the right answer, but it's clearly wrong because 1) the range of or need not be a subset of the domain and 2) inverse of may not exist. Also, the given data seems to be superfluous here. So what is the proper way of doing it?",f(f(f(x)))+f(x)=2 0≤x≤2 f(x) \int_0^2 f(x) dx f(x)=2-t \begin{equation} f(f(2-t))=t \end{equation} 0 2 \int_0^2f(2-t)=\int_0^2f^{-1}(t) f(f(x))=z f(z)=2-f^{-1}(z) f^{-1}(z) \int_0^2 f(x) dx=2 f(x) f(f(x)) f(x) 0≤x≤2,"['calculus', 'functions', 'definite-integrals', 'functional-equations']"
14,"Evaluate $\int_{0}^{\frac{\pi}{2}}\ln(2+\sin x) \,\mathrm dx $ and $\int_{0}^{\frac{\pi}{2}}\ln(2-\sin x) \,\mathrm dx$",Evaluate  and,"\int_{0}^{\frac{\pi}{2}}\ln(2+\sin x) \,\mathrm dx  \int_{0}^{\frac{\pi}{2}}\ln(2-\sin x) \,\mathrm dx","I had to calculate $$I= \int_{0}^{1}\frac{1}{1+x^2}\ln\bigg[\frac{x^2+x+1}{x^2-x+1}\bigg]\,\mathrm dx$$ from my Previous Question [ 1 ] (which is now solved) but I wanted to have another solution ,So I proceed like this. $$I=\int_{0}^{1}\frac{\ln(x^2+x+1)}{x^2+1}\,\mathrm dx-\int_{0}^{1}\frac{\ln(x^2-x+1)}{x^2+1}\,\mathrm dx$$ Let $x=\tan t \implies \mathrm dx=\sec^2 t \,\mathrm dt$ $$\implies I=\int_{0}^{\frac{\pi}{4}}\ln(\tan^2 x+\tan x+1)\,\mathrm dx-\int_{0}^{ \frac{\pi}{4}}\ln(\tan^2 x-\tan x+1)\,\mathrm dx$$ $$2(\tan^2 x+\tan x+1)=(2+\sin 2x)(1+\tan^2 x)$$ $$\implies I=\int_{0}^{  \frac{\pi}{4} }\Big[\ln(2+\sin 2x)+\ln(1+\tan^2 x)-\ln(2)\Big]\,\mathrm dx-\int_{0}^{  \frac{\pi}{4} }\Big[\ln(2-\sin 2x)+\ln(1+\tan^2 x)-\ln(2)\Big]\,\mathrm dx$$ $$\implies I=\frac12 \Bigg[\int_{0}^{  \frac{\pi}{2} }\ln(2+\sin x)\,\mathrm dx-  \int_{0}^{  \frac{\pi}{2} }\ln(2-\sin x)\,\mathrm dx\Bigg]$$ We can group together the two log terms in the integral but then this Question will become same as my previous question[ 1 ], I want to calculate both of the integrals separately.","I had to calculate from my Previous Question [ 1 ] (which is now solved) but I wanted to have another solution ,So I proceed like this. Let We can group together the two log terms in the integral but then this Question will become same as my previous question[ 1 ], I want to calculate both of the integrals separately.","I= \int_{0}^{1}\frac{1}{1+x^2}\ln\bigg[\frac{x^2+x+1}{x^2-x+1}\bigg]\,\mathrm dx I=\int_{0}^{1}\frac{\ln(x^2+x+1)}{x^2+1}\,\mathrm dx-\int_{0}^{1}\frac{\ln(x^2-x+1)}{x^2+1}\,\mathrm dx x=\tan t \implies \mathrm dx=\sec^2 t \,\mathrm dt \implies I=\int_{0}^{\frac{\pi}{4}}\ln(\tan^2 x+\tan x+1)\,\mathrm dx-\int_{0}^{ \frac{\pi}{4}}\ln(\tan^2 x-\tan x+1)\,\mathrm dx 2(\tan^2 x+\tan x+1)=(2+\sin 2x)(1+\tan^2 x) \implies I=\int_{0}^{ 
\frac{\pi}{4} }\Big[\ln(2+\sin 2x)+\ln(1+\tan^2 x)-\ln(2)\Big]\,\mathrm dx-\int_{0}^{ 
\frac{\pi}{4} }\Big[\ln(2-\sin 2x)+\ln(1+\tan^2 x)-\ln(2)\Big]\,\mathrm dx \implies I=\frac12 \Bigg[\int_{0}^{ 
\frac{\pi}{2} }\ln(2+\sin x)\,\mathrm dx-  \int_{0}^{ 
\frac{\pi}{2} }\ln(2-\sin x)\,\mathrm dx\Bigg]","['calculus', 'integration', 'definite-integrals', 'logarithms']"
15,"Asymptotic expansion of $\int_0^{2\pi} \mathrm{d}\theta\, \sqrt{k^2\cos^2\theta - \cos\theta + 1}$ as $k\to0$",Asymptotic expansion of  as,"\int_0^{2\pi} \mathrm{d}\theta\, \sqrt{k^2\cos^2\theta - \cos\theta + 1} k\to0","I have the integral $$I = \int_0^{2\pi} \mathrm{d}\theta\, \sqrt{k^2\cos^2\theta - \cos\theta + 1}$$ and I would need the asymptotic expansion of the integral for small values of $k$ . For $k=0$ we get quite easily $I=4\sqrt{2}$ , while for $k\gg 1$ , we have $I\sim 4|k|$ . Now, trying to perform the integration using special functions (at first glance, one would think that it could be done in terms of elliptic functions) yields no result. Gradštejn and Ryžik are of no help, nor Mathematica or other softwares. We can Taylor expand the integrand and integrate each term of the expansion, but the $2n$ -th term, for $n>1$ , reads $$ \frac{k^{2n}}{(2n)!}\frac{ (-1)^{n-1} (2n-3)!! }{2^n} \frac{\cos^{2n}\theta}{(1-\cos \theta)^{n-1/2}}$$ having set $k=0$ in the derivative. Of course, integrating each term would mean performing $$\int_0^{2\pi}\mathrm{d} \theta\,\frac{\cos^{2n}\theta}{(1-\cos \theta)^{n-1/2}}$$ which is divergent for $n>1$ . Any ideas about how to proceed? Are there some kind of 'regularization' techniques that could help?","I have the integral and I would need the asymptotic expansion of the integral for small values of . For we get quite easily , while for , we have . Now, trying to perform the integration using special functions (at first glance, one would think that it could be done in terms of elliptic functions) yields no result. Gradštejn and Ryžik are of no help, nor Mathematica or other softwares. We can Taylor expand the integrand and integrate each term of the expansion, but the -th term, for , reads having set in the derivative. Of course, integrating each term would mean performing which is divergent for . Any ideas about how to proceed? Are there some kind of 'regularization' techniques that could help?","I = \int_0^{2\pi} \mathrm{d}\theta\, \sqrt{k^2\cos^2\theta - \cos\theta + 1} k k=0 I=4\sqrt{2} k\gg 1 I\sim 4|k| 2n n>1  \frac{k^{2n}}{(2n)!}\frac{ (-1)^{n-1} (2n-3)!! }{2^n} \frac{\cos^{2n}\theta}{(1-\cos \theta)^{n-1/2}} k=0 \int_0^{2\pi}\mathrm{d} \theta\,\frac{\cos^{2n}\theta}{(1-\cos \theta)^{n-1/2}} n>1","['calculus', 'integration', 'definite-integrals', 'taylor-expansion']"
16,Resemblance of the graph of $\ln (ax^2+by^2)=y^{n}$ with ellipse $ax^2+by^2=1$ for large values of $n$.,Resemblance of the graph of  with ellipse  for large values of .,\ln (ax^2+by^2)=y^{n} ax^2+by^2=1 n,"Coincidentally using Desmos, I found out that the graph (specifically its curvature) of $\ln (x^2+y^2)=y^{n}$ for large values of $n$ resembles with that of circle $x^2+y^2=1$ . This resemblance increases Further as the value of $n$ increases. Another Fascinating thing I observed is that The Graph of $\ln (ax^2+by^2)=y^{n}$ for large values of $n$ resembles with the graph of ellipse $ax^2+by^2=1$ . So our circle is the special case of the second case, the ellipse. Can we offer any satisfactory explanation for this?","Coincidentally using Desmos, I found out that the graph (specifically its curvature) of for large values of resembles with that of circle . This resemblance increases Further as the value of increases. Another Fascinating thing I observed is that The Graph of for large values of resembles with the graph of ellipse . So our circle is the special case of the second case, the ellipse. Can we offer any satisfactory explanation for this?",\ln (x^2+y^2)=y^{n} n x^2+y^2=1 n \ln (ax^2+by^2)=y^{n} n ax^2+by^2=1,"['calculus', 'logarithms']"
17,The integral: $\int_0^1(e^x)dx$ via Darboux's sums,The integral:  via Darboux's sums,\int_0^1(e^x)dx,"Need to calculate the integral: $\int_0^1(e^x)dx$ via Darboux's sums. My attempt: First, we calculate the lower summation: $$L(p,f)=\sum_{i=1}^n m_i\Delta x_i=\sum_{i=1}^n(e^{\frac{i-1}{n}}\cdot\frac{1}{n})=\frac{1}{n}\cdot e^{\frac{n(n-1)}{2n}}=\frac{1}{n}\cdot e^{\frac{n-1}{2}}$$ Second, we calculate the upper summation: $$U(p,f)=\sum_{i=1}^n M_i\Delta x_i=\sum_{i=1}^n(e^{\frac{i}{n}}\cdot\frac{1}{n})=\frac{1}{n}\cdot e^{\frac{n(n+1)}{2n}}=\frac{1}{n}\cdot e^{\frac{n+1}{2}}$$ From the result, it easy to see that both of the limits are $\infty$ . I know that there is a mistake over here, I would appreciate that, if you can enlighten me.","Need to calculate the integral: via Darboux's sums. My attempt: First, we calculate the lower summation: Second, we calculate the upper summation: From the result, it easy to see that both of the limits are . I know that there is a mistake over here, I would appreciate that, if you can enlighten me.","\int_0^1(e^x)dx L(p,f)=\sum_{i=1}^n m_i\Delta x_i=\sum_{i=1}^n(e^{\frac{i-1}{n}}\cdot\frac{1}{n})=\frac{1}{n}\cdot e^{\frac{n(n-1)}{2n}}=\frac{1}{n}\cdot e^{\frac{n-1}{2}} U(p,f)=\sum_{i=1}^n M_i\Delta x_i=\sum_{i=1}^n(e^{\frac{i}{n}}\cdot\frac{1}{n})=\frac{1}{n}\cdot e^{\frac{n(n+1)}{2n}}=\frac{1}{n}\cdot e^{\frac{n+1}{2}} \infty","['calculus', 'integration', 'definite-integrals']"
18,Solving the definite integral $\int_{0}^{2} \sqrt{(1+x)\sqrt{4x+1}-3x+1}dx$,Solving the definite integral,\int_{0}^{2} \sqrt{(1+x)\sqrt{4x+1}-3x+1}dx,"I need to solve the definite integral: $$\int_{0}^{2} \sqrt{(1+x)\sqrt{4x+1}-3x+1}dx$$ The integral was proposed by my algebraic geometry professor as a warm up excercise, he hinted us to research about elliptic functions and curves, but I cannot find anything related to this integral. I tried substition and integration by parts, by I can't seem to reduce the problem. I already solved the integral already by proving the convergence of it Taylor Series around $x_0=2$ and integrating said power series. But, its only an approximation, since I can't integrate infinite terms. Is it possible to solve it analytically? What am I missing?","I need to solve the definite integral: The integral was proposed by my algebraic geometry professor as a warm up excercise, he hinted us to research about elliptic functions and curves, but I cannot find anything related to this integral. I tried substition and integration by parts, by I can't seem to reduce the problem. I already solved the integral already by proving the convergence of it Taylor Series around and integrating said power series. But, its only an approximation, since I can't integrate infinite terms. Is it possible to solve it analytically? What am I missing?",\int_{0}^{2} \sqrt{(1+x)\sqrt{4x+1}-3x+1}dx x_0=2,"['calculus', 'integration', 'definite-integrals']"
19,Convergence of $\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{i+n}$ [duplicate],Convergence of  [duplicate],\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{i+n},"This question already has answers here : Suppose a series $a_n$ is greater than 0 for all positive integer n, and that $\sum \frac {a_n} n$ converges, then does the following also converge? (2 answers) Closed 3 years ago . If $\sum_{n=1}^{\infty}\frac{a_n}{n}=a$ converges to a finite number with $a_n \geq 0$ for all $n \geq 1$ does this also imply that $$\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{i+n}=0$$ I tried using $\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{n}$ as a bound so that I can use comparison test but I can't prove that it converges either. How would I go about proving this statement?","This question already has answers here : Suppose a series $a_n$ is greater than 0 for all positive integer n, and that $\sum \frac {a_n} n$ converges, then does the following also converge? (2 answers) Closed 3 years ago . If converges to a finite number with for all does this also imply that I tried using as a bound so that I can use comparison test but I can't prove that it converges either. How would I go about proving this statement?",\sum_{n=1}^{\infty}\frac{a_n}{n}=a a_n \geq 0 n \geq 1 \lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{i+n}=0 \lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{n},"['calculus', 'integration', 'sequences-and-series', 'limits', 'infinity']"
20,Taylor series for $\sqrt[3]{x+1}$,Taylor series for,\sqrt[3]{x+1},"I'm trying to find the Taylor series for $\sqrt[3]{x}$ , but since the n-th derivative of the function $$f(x)=\sqrt[3]{x}$$ not definded at $x=0$ , I've swithched to $f(x)=\sqrt[3]{x+1}$ , This is my Attempt: $$f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(0)x^n}{n!}$$ $$\sqrt[3]{x+1}=1+\frac{x}{3}-\frac{2x^2}{9\cdot2!}+\frac{10x^3}{27\cdot 3!}-\frac{80x^4}{81\cdot4!}+...$$ but when i plug $x=125 \iff \sqrt[3]{x+1}=\sqrt[3]{126}$ i get something way bigger than the acttual value which is about $5.01...$ , you can check it here . But I can't find the mistake in the equation.","I'm trying to find the Taylor series for , but since the n-th derivative of the function not definded at , I've swithched to , This is my Attempt: but when i plug i get something way bigger than the acttual value which is about , you can check it here . But I can't find the mistake in the equation.",\sqrt[3]{x} f(x)=\sqrt[3]{x} x=0 f(x)=\sqrt[3]{x+1} f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(0)x^n}{n!} \sqrt[3]{x+1}=1+\frac{x}{3}-\frac{2x^2}{9\cdot2!}+\frac{10x^3}{27\cdot 3!}-\frac{80x^4}{81\cdot4!}+... x=125 \iff \sqrt[3]{x+1}=\sqrt[3]{126} 5.01...,"['calculus', 'taylor-expansion']"
21,"Calculating the improper integral $\int _{1}^{\infty}\frac{e^{\sin x}\cos x}{x}\,dx$",Calculating the improper integral,"\int _{1}^{\infty}\frac{e^{\sin x}\cos x}{x}\,dx","I want to calculate the value of $$\int_{1}^{\infty}\frac{e^{\sin x}\cos x}{x}\,dx$$ I was able to prove using Dirichlet's test that it does converge, but how can I calculate its value?","I want to calculate the value of I was able to prove using Dirichlet's test that it does converge, but how can I calculate its value?","\int_{1}^{\infty}\frac{e^{\sin x}\cos x}{x}\,dx","['calculus', 'integration', 'improper-integrals']"
22,"What is the true, formal meaning and reason for the ""dx"" symbol in integrals","What is the true, formal meaning and reason for the ""dx"" symbol in integrals",,"When I encountered integrals for the first time, and learned to write the ""dx"" at the end of every integral, I had no problem interpreting it as something that told me what the variable of integration is, or where the integral ends, and nothing more. But when I encountered u-substitution, we started doing things like du = u'(x)dx, and replacing u'(x) and dx with du in the integral. Well, that seems like ""dx"" was never just a delimiter, but something being multiplied with the function itself. I asked around about this, and people told me that dx is in fact, only a delimiter to tell us our variable of integration, and that the ""multiplication"" I did was just some sort of mnemonic for the reverse chain rule. I thought it was weird to use a mnemonic like that, but I understood it. But then others told me that ""dx"" is part of what's being integrated, and they started saying that we're led to believe that its just a delimiter in early courses because it'd be impossible for teachers to introduce ""differentials,"" which is what things like dx and du are, so u-substitution isn't just a mnemonic, and the multiplication is completely formal. They also said that I haven't been integrating functions, but rather differential forms, and have only been told I'm integrating functions to make things easier until I learn the truth. This is all extremely confusing to me. I have no idea how I've heard so many differing opinions that can't be true at the same time. This all, once again, leaves me wondering, what is the real, formal meaning of the notation we use for integrals, what does that ""dx"" truly represent - is it a part of the computation, or is it something easily replaceable by a string like ""with respect to x""? Do we integrate functions, or do we integrate something called a differential form? How much of what I've been told is true, and what haven't I been told? This has been bothering me for some time, so I'd greatly appreciate it if anyone could try to clear this up for me!","When I encountered integrals for the first time, and learned to write the ""dx"" at the end of every integral, I had no problem interpreting it as something that told me what the variable of integration is, or where the integral ends, and nothing more. But when I encountered u-substitution, we started doing things like du = u'(x)dx, and replacing u'(x) and dx with du in the integral. Well, that seems like ""dx"" was never just a delimiter, but something being multiplied with the function itself. I asked around about this, and people told me that dx is in fact, only a delimiter to tell us our variable of integration, and that the ""multiplication"" I did was just some sort of mnemonic for the reverse chain rule. I thought it was weird to use a mnemonic like that, but I understood it. But then others told me that ""dx"" is part of what's being integrated, and they started saying that we're led to believe that its just a delimiter in early courses because it'd be impossible for teachers to introduce ""differentials,"" which is what things like dx and du are, so u-substitution isn't just a mnemonic, and the multiplication is completely formal. They also said that I haven't been integrating functions, but rather differential forms, and have only been told I'm integrating functions to make things easier until I learn the truth. This is all extremely confusing to me. I have no idea how I've heard so many differing opinions that can't be true at the same time. This all, once again, leaves me wondering, what is the real, formal meaning of the notation we use for integrals, what does that ""dx"" truly represent - is it a part of the computation, or is it something easily replaceable by a string like ""with respect to x""? Do we integrate functions, or do we integrate something called a differential form? How much of what I've been told is true, and what haven't I been told? This has been bothering me for some time, so I'd greatly appreciate it if anyone could try to clear this up for me!",,"['calculus', 'integration', 'notation', 'differential-forms']"
23,What would the picture for partial fractions look like?,What would the picture for partial fractions look like?,,"As how integration by parts has the picture below, what would the picture for partial fractions look like? Although there's probably no way to escape the heavy algebra necessary for partial fractions, is there a intuitive or geometric visualization of partial fractions? And is there one that doesn't rely on animated pictures? The closest thing that seems are those diagrams where kids do basic operations on fractions by colouring in boxes like . And so, for partial fractions, each of those coloured sections would be different rational functions. Probably would be hard or impossible to generalize though.","As how integration by parts has the picture below, what would the picture for partial fractions look like? Although there's probably no way to escape the heavy algebra necessary for partial fractions, is there a intuitive or geometric visualization of partial fractions? And is there one that doesn't rely on animated pictures? The closest thing that seems are those diagrams where kids do basic operations on fractions by colouring in boxes like . And so, for partial fractions, each of those coloured sections would be different rational functions. Probably would be hard or impossible to generalize though.",,"['calculus', 'integration']"
24,What is the proof for $F(x)=\int_{a}^{x} e^{t^2}dt$ not being elementary?,What is the proof for  not being elementary?,F(x)=\int_{a}^{x} e^{t^2}dt,"Simply as stated above: What is the proof, or how does one prove, $F(x)=\int_{a}^{b} e^{t^2}dt$ isn't elementary? All I know is that it can be proven , but I couldn't find a proof for it.","Simply as stated above: What is the proof, or how does one prove, isn't elementary? All I know is that it can be proven , but I couldn't find a proof for it.",F(x)=\int_{a}^{b} e^{t^2}dt,"['calculus', 'integration']"
25,Show $\frac{f'(x)}{f(x)}\ge\frac{g'(x)}{g(x)}-\left|\frac{f'(1)}{f(1)}-\frac{g'(1)}{g(1)} \right|$ if $\frac{f''(x)}{f(x)}\ge\frac{g''(x)}{g(x)}$.,Show  if .,\frac{f'(x)}{f(x)}\ge\frac{g'(x)}{g(x)}-\left|\frac{f'(1)}{f(1)}-\frac{g'(1)}{g(1)} \right| \frac{f''(x)}{f(x)}\ge\frac{g''(x)}{g(x)},"Suppose $f$ and $g$ are positive increasing functions on $\left[ 1,+\infty \right)$ such that $$\frac{f''\left( x \right)}{f\left( x \right)}\ge \frac{g''\left( x \right)}{g\left( x \right)}\,.$$ Prove that $$\frac{f'\left( x \right)}{f\left( x \right)}\ge \frac{g'\left( x \right)}{g\left( x \right)}-\left| \frac{f'\left( 1 \right)}{f\left( 1 \right)}-\frac{g'\left( 1 \right)}{g\left( 1 \right)} \right| \,.$$ The problem is  taken from： Mitrinovic, D.S. Pecaric, j. E, Fink, A. M., Classical and new inequalities in analysis, Kluwer Acad tools. Publ, Dordrecht, 1993. But I couldn't find the book, and I didn't come up with a good solution. $$\varphi \left( x \right) =f'\left( x \right) g\left( x \right) -f\left( x \right) g'\left( x \right) \,\,$$ $$\varphi '\left( x \right) =f''\left( x \right) g\left( x \right) -f\left( x \right) g''\left( x \right) \ge 0$$ $$\varphi \left( x \right) \ge \varphi \left( 1 \right) \Rightarrow \frac{f'\left( x \right)}{f\left( x \right)}-\frac{g'\left( x \right)}{g\left( x \right)}\ge \frac{f\left( 1 \right) g\left( 1 \right)}{f\left( x \right) g\left( x \right)}\left[ \frac{f'\left( 1 \right)}{f\left( 1 \right)}-\frac{g\left( 1 \right)}{g\left( 1 \right)} \right] $$ The rest of the work is just to take the absolute value out of the case.","Suppose and are positive increasing functions on such that Prove that The problem is  taken from： Mitrinovic, D.S. Pecaric, j. E, Fink, A. M., Classical and new inequalities in analysis, Kluwer Acad tools. Publ, Dordrecht, 1993. But I couldn't find the book, and I didn't come up with a good solution. The rest of the work is just to take the absolute value out of the case.","f g \left[ 1,+\infty \right) \frac{f''\left( x \right)}{f\left( x \right)}\ge \frac{g''\left( x \right)}{g\left( x \right)}\,. \frac{f'\left( x \right)}{f\left( x \right)}\ge \frac{g'\left( x \right)}{g\left( x \right)}-\left| \frac{f'\left( 1 \right)}{f\left( 1 \right)}-\frac{g'\left( 1 \right)}{g\left( 1 \right)} \right|
\,. \varphi \left( x \right) =f'\left( x \right) g\left( x \right) -f\left( x \right) g'\left( x \right) \,\, \varphi '\left( x \right) =f''\left( x \right) g\left( x \right) -f\left( x \right) g''\left( x \right) \ge 0 \varphi \left( x \right) \ge \varphi \left( 1 \right) \Rightarrow \frac{f'\left( x \right)}{f\left( x \right)}-\frac{g'\left( x \right)}{g\left( x \right)}\ge \frac{f\left( 1 \right) g\left( 1 \right)}{f\left( x \right) g\left( x \right)}\left[ \frac{f'\left( 1 \right)}{f\left( 1 \right)}-\frac{g\left( 1 \right)}{g\left( 1 \right)} \right] ","['calculus', 'analysis', 'derivatives', 'inequality']"
26,Prove that $\frac{1}{a^2}+\frac{1}{(a+1)^2}+\frac{1}{(a+2)^2}+\dotsm\infty=\frac{1}{a}+\frac{1}{2a(a+1)}+\frac{2!}{3a(a+1)(a+2)}+\dotsm\infty$,Prove that,\frac{1}{a^2}+\frac{1}{(a+1)^2}+\frac{1}{(a+2)^2}+\dotsm\infty=\frac{1}{a}+\frac{1}{2a(a+1)}+\frac{2!}{3a(a+1)(a+2)}+\dotsm\infty,"Question:-  Prove that $$\frac{1}{a^2}+\frac{1}{(a+1)^2}+\frac{1}{(a+2)^2}+\dotsm\infty=\frac{1}{a}+\frac{1}{2a(a+1)}+\frac{2!}{3a(a+1)(a+2)}+\dotsm\infty$$ Nothing is mentioned in question about nature of $a$ I write it in summation form,but I got stuck and unable to proceed further. $$\sum_{k=0}^{\infty}\frac{1}{(a+k)^2}=\sum_{n=0}^{\infty}\frac{n!}{(n+1)\prod_{k=0}^{n}(a+k)}$$ Then I take all the terms to LHS in hope that terms may cancel out each other to give zero but that also doesn't help me since with each term degree of both numerator and denominator increases. Can anybody help me to Prove the result!!","Question:-  Prove that Nothing is mentioned in question about nature of I write it in summation form,but I got stuck and unable to proceed further. Then I take all the terms to LHS in hope that terms may cancel out each other to give zero but that also doesn't help me since with each term degree of both numerator and denominator increases. Can anybody help me to Prove the result!!",\frac{1}{a^2}+\frac{1}{(a+1)^2}+\frac{1}{(a+2)^2}+\dotsm\infty=\frac{1}{a}+\frac{1}{2a(a+1)}+\frac{2!}{3a(a+1)(a+2)}+\dotsm\infty a \sum_{k=0}^{\infty}\frac{1}{(a+k)^2}=\sum_{n=0}^{\infty}\frac{n!}{(n+1)\prod_{k=0}^{n}(a+k)},"['calculus', 'sequences-and-series']"
27,"Could you suggest basic mathematics textbooks (calculus, linear algebra) that are written in an intuitive manner?","Could you suggest basic mathematics textbooks (calculus, linear algebra) that are written in an intuitive manner?",,"For instance, I really loved reading the book "" Div, Grad, Curl, and All That"" by H.M.Schey. I would like texts closely written in that particular style.","For instance, I really loved reading the book "" Div, Grad, Curl, and All That"" by H.M.Schey. I would like texts closely written in that particular style.",,"['calculus', 'linear-algebra', 'reference-request', 'soft-question', 'education']"
28,"Why is it incorrect to say, $\lim_{x\rightarrow a}f(x)\notin\mathbb{C}$?","Why is it incorrect to say, ?",\lim_{x\rightarrow a}f(x)\notin\mathbb{C},"My grade $12$ calculus teacher told me I cannot write the following: $$\lim_{x\rightarrow a}f(x)\notin\mathbb{C}$$ to say that the limit does not exist. The only reasoning she gave was ""I think you should have more experience working with complex numbers before you say that."" It's been said a million times that there is no formal notation to say a limit does not exist and that it's best to just write it out or use 'D.N.E'. But I'm curious to know what is wrong with this statement mathematically. Can the limit of $f(x)$ exist and be outside $\mathbb{C}$ ? Thanks in advance.","My grade calculus teacher told me I cannot write the following: to say that the limit does not exist. The only reasoning she gave was ""I think you should have more experience working with complex numbers before you say that."" It's been said a million times that there is no formal notation to say a limit does not exist and that it's best to just write it out or use 'D.N.E'. But I'm curious to know what is wrong with this statement mathematically. Can the limit of exist and be outside ? Thanks in advance.",12 \lim_{x\rightarrow a}f(x)\notin\mathbb{C} f(x) \mathbb{C},"['calculus', 'limits', 'complex-numbers', 'notation']"
29,evaluate the summation : $\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(n+1)(n+2x+3)}$,evaluate the summation :,\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(n+1)(n+2x+3)},"Find $$S=\displaystyle\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(n+1)(n+2x+3)}$$ for $x≥0$ . At first, I use a partial fraction $$S=\displaystyle\sum_{n=0}^{\infty}\left(\frac{(-1)^{n}}{2(x+1)(n+1)}-\frac{(-1)^{n}}{2(x+1)(n+2x+3)}\right) =\frac{1}{2(x+1)}(I-J),$$ where $$I=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+1} =\ln 2$$ and $$J=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+3+2x}$$ I think to use: $$\ln (1+y)=\sum_{n=0}^{\infty}\frac{(-1)^{n}y^{n+1}}{n+1}$$ Is my work correct? How to complete this work ?","Find for . At first, I use a partial fraction where and I think to use: Is my work correct? How to complete this work ?","S=\displaystyle\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(n+1)(n+2x+3)} x≥0 S=\displaystyle\sum_{n=0}^{\infty}\left(\frac{(-1)^{n}}{2(x+1)(n+1)}-\frac{(-1)^{n}}{2(x+1)(n+2x+3)}\right)
=\frac{1}{2(x+1)}(I-J), I=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+1}
=\ln 2 J=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+3+2x} \ln (1+y)=\sum_{n=0}^{\infty}\frac{(-1)^{n}y^{n+1}}{n+1}","['calculus', 'summation', 'closed-form']"
30,Existence of improper integral.,Existence of improper integral.,,"I want to show that the improper integral $$\int_0^{\infty} \frac{\sin(x)}{x(1+x)}dx$$ exists. In order to do that, I split the integral into $\int_0^1 \frac{\sin(x)}{x(1+x)}dx + \int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx$ , so that I can compute a value for $\int_0^1 \frac{\sin(x)}{x(1+x)}dx$ , which proves the existence for that part. For the other part, is it sufficient to show that $\int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx < \int_1^{\infty} \frac{1}{x^2}dx$ knowing that $\int_1^{\infty} \frac{1}{x^2}dx$ exists, to say that $\int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx$ exist and therefore the whole improper integral exists? Thanks for an answer in advance.","I want to show that the improper integral exists. In order to do that, I split the integral into , so that I can compute a value for , which proves the existence for that part. For the other part, is it sufficient to show that knowing that exists, to say that exist and therefore the whole improper integral exists? Thanks for an answer in advance.",\int_0^{\infty} \frac{\sin(x)}{x(1+x)}dx \int_0^1 \frac{\sin(x)}{x(1+x)}dx + \int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx \int_0^1 \frac{\sin(x)}{x(1+x)}dx \int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx < \int_1^{\infty} \frac{1}{x^2}dx \int_1^{\infty} \frac{1}{x^2}dx \int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx,"['calculus', 'integration', 'improper-integrals']"
31,real valued functions that are invariant under convolution,real valued functions that are invariant under convolution,,"Consider the function $f(x) = e^{-x^2}$ , notice that $f$ convoluted with itself is of the form $a\cdot f(bx+c)$ for reals $a, b$ and $c$ . Another way of saying this is that the shape of the function $f(x)$ does not change when convoluted with itself. It only gets stretch and shifted. Here is another such function: $f(x) = \frac1{1+x^2}$ I am interested in a categorization of all such functions. Is such a categorization known? I am interested in any resources discussing this topic. If you are unable to do either, I would still be interested in any example of such a function other than the two I listed.","Consider the function , notice that convoluted with itself is of the form for reals and . Another way of saying this is that the shape of the function does not change when convoluted with itself. It only gets stretch and shifted. Here is another such function: I am interested in a categorization of all such functions. Is such a categorization known? I am interested in any resources discussing this topic. If you are unable to do either, I would still be interested in any example of such a function other than the two I listed.","f(x) = e^{-x^2} f a\cdot f(bx+c) a, b c f(x) f(x) = \frac1{1+x^2}","['calculus', 'probability-distributions', 'convolution']"
32,"Assuming that $f''(a)$ exists, show that: $f''(a)=\lim_{h\to 0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}$ [duplicate]","Assuming that  exists, show that:  [duplicate]",f''(a) f''(a)=\lim_{h\to 0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2},"This question already has answers here : Second derivative ""formula derivation"" (3 answers) Closed 5 years ago . Assuming that $f''(a)$ exists, show that: $$f''(a)=\lim_{h\to 0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2}$$ I got the first derivative by using the limit rule $(f(a+h)-f(a))/h$ . But when taking the second derivative, I thought of doing the same thing on $f(a)$ of the equation, but I couldn't reach this answer.","This question already has answers here : Second derivative ""formula derivation"" (3 answers) Closed 5 years ago . Assuming that exists, show that: I got the first derivative by using the limit rule . But when taking the second derivative, I thought of doing the same thing on of the equation, but I couldn't reach this answer.",f''(a) f''(a)=\lim_{h\to 0}\frac{f(a+h)-2f(a)+f(a-h)}{h^2} (f(a+h)-f(a))/h f(a),"['calculus', 'limits']"
33,"Showing that $\cos^p{\Theta} \le \cos{p\Theta}$, for $0<\Theta<\frac{\pi}{2}$ and $0<p<1$, by analyzing $f(\theta)=\frac{\cos^p\Theta}{\cos p\Theta}$","Showing that , for  and , by analyzing",\cos^p{\Theta} \le \cos{p\Theta} 0<\Theta<\frac{\pi}{2} 0<p<1 f(\theta)=\frac{\cos^p\Theta}{\cos p\Theta},"Given $0 < \Theta < \frac{\pi}{2}$ and $0 < p < 1$ , show that $$\cos^p{\Theta} \le \cos{p\Theta}$$ Can you please check if my proof is correct? Would also love to know if there're other ways of proving this. Proof. Let $$f(\Theta) = \frac{\cos^p{\Theta}}{\cos{p\Theta}}$$ considering $p$ fixed. The derivative is $$f'(\Theta) = \frac{p*\cos^{p-1}{\Theta}*{(-\sin{\Theta})*\cos{p\Theta} -\cos^p{\Theta}*(-\sin{p\Theta}*p) } } {\cos^2{p\Theta}} = \frac{p*\cos^{p-1}{\Theta}*\sin{(p-1)\Theta}}{\cos^2{p\Theta}}$$ We have $f(0)=1$ , and because $0<p<1$ , the term $\sin(p-1)\Theta$ makes the derivative negative: $f'(\Theta) < 0$ on $(0,\frac{\pi}{2})$ . At $0$ we have $f'(0)=0$ , so we cannot immediately deduce that $f$ is decreasing on all of $[0,\frac{\pi}{2})$ . However, considering that $f(0)=1, f(\frac{\pi}{2})=0$ , and both $f$ and $f'$ are continuous on $[0,\frac{\pi}{2})$ , if it were the case that $f$ rises above $1$ at some $\Theta \in (0,\frac{\pi}{2})$ , $f$ would have to have a local maximum somewhere in $(0,\frac{\pi}{2})$ , with the derivative vanishing at that point, which cannot happen. Therefore $f(\Theta) \le 1$ throughout $[0,\frac{\pi}{2}]$ , Q.E.D.","Given and , show that Can you please check if my proof is correct? Would also love to know if there're other ways of proving this. Proof. Let considering fixed. The derivative is We have , and because , the term makes the derivative negative: on . At we have , so we cannot immediately deduce that is decreasing on all of . However, considering that , and both and are continuous on , if it were the case that rises above at some , would have to have a local maximum somewhere in , with the derivative vanishing at that point, which cannot happen. Therefore throughout , Q.E.D.","0 < \Theta < \frac{\pi}{2} 0 < p < 1 \cos^p{\Theta} \le \cos{p\Theta} f(\Theta) = \frac{\cos^p{\Theta}}{\cos{p\Theta}} p f'(\Theta) = \frac{p*\cos^{p-1}{\Theta}*{(-\sin{\Theta})*\cos{p\Theta} -\cos^p{\Theta}*(-\sin{p\Theta}*p) } } {\cos^2{p\Theta}} = \frac{p*\cos^{p-1}{\Theta}*\sin{(p-1)\Theta}}{\cos^2{p\Theta}} f(0)=1 0<p<1 \sin(p-1)\Theta f'(\Theta) < 0 (0,\frac{\pi}{2}) 0 f'(0)=0 f [0,\frac{\pi}{2}) f(0)=1, f(\frac{\pi}{2})=0 f f' [0,\frac{\pi}{2}) f 1 \Theta \in (0,\frac{\pi}{2}) f (0,\frac{\pi}{2}) f(\Theta) \le 1 [0,\frac{\pi}{2}]","['calculus', 'trigonometry', 'proof-verification']"
34,"For a function defined by parts study continuity, and differentiability at two points","For a function defined by parts study continuity, and differentiability at two points",,"For the function defined by $$F(x)=\begin{cases}\displaystyle\int_x^{2x}\sin t^2\,\mathrm dt,&x\neq0\\0,&x=0\end{cases}$$ analyze continuity and derivability at the origin. Is $F$ derivable at point $x_0=\sqrt{\pi/2}$ ? Justify the answer, and if possible, calculate $F'(x_0)$ . I have been told that I must use the Fundamental Theorem of Integral Calculus but I do not know how to apply it to this case. For the function to be continuous at the origin, it must happen that $F(0)=\lim_{x\to0}F(x)$ . We know that $F(0)=0$ , and $$\lim_{x\to0}F(x)=\lim_{x\to0}\int_x^{2x}\sin t^2\,\mathrm dt\;{\bf\color{red}=}\int_0^{2\cdot0}\sin t^2\,\mathrm dt=0,$$ so the statement holds, but here I do now how to justify the $\bf\color{red}=$ . To find the derivative at $x_0=0$ I tried the differentiate directly $F(x)$ but it is wrong, so I have been told that I must use the definition. So we have to find $$F'(0)=\lim_{x\to0}\frac{F(x)-F(0)}{x-0}=\lim_{x\to0}\frac{\int_x^{2x}\sin t^2\,\mathrm dt}x.$$ Why we have to bound $\left|\sin t^2\right|\leq t^2$ ? How can we do that? Finally, I do not know how to use the aforementioned theorem to justify that the function is derivable in $\sqrt{\pi/2}$ . Using the definition again: \begin{align*} F'\left(\sqrt{\frac\pi2}\right)&=\lim_{x\to\sqrt{\frac\pi2}}\frac{F(x)-F\left(\sqrt{\frac\pi2}\right)}{x-\sqrt{\frac\pi2}}\\ &=\lim_{x\to\sqrt{\frac\pi2}}\frac{\int_x^{2x}\sin t^2\,\mathrm dt-\int_{\sqrt{\pi/2}}^{2\sqrt{\pi/2}}\sin t^2\,\mathrm dt}{x-\sqrt{\frac\pi2}}\\ &\leq\lim_{x\to\sqrt{\frac\pi2}}\frac{\int_x^{2x}t^2\,\mathrm dt-\int_{\sqrt{\pi/2}}^{2\sqrt{\pi/2}}t^2\,\mathrm dt}{x-\sqrt{\frac\pi2}}\\ &\underbrace=_{A=\sqrt{\pi/2}}\lim_{x\to A}\frac{1/3((2x)^3-x^3)-1/3((2A)^3-(A^3))}{x-A}\\ &=\frac73\lim_{x\to A}\frac{x^3-A^3}{x-A}\\ &=\frac73\lim_{x\to A}\frac{(x-A)(x^2+Ax+A^2)}{x-A}\\ &=\frac73(A^2+A^2+A^2)\\ &=7A^2\\ &=\frac{7\pi}2, \end{align*} but it is wrong. How can we solve the statement? Thanks!","For the function defined by analyze continuity and derivability at the origin. Is derivable at point ? Justify the answer, and if possible, calculate . I have been told that I must use the Fundamental Theorem of Integral Calculus but I do not know how to apply it to this case. For the function to be continuous at the origin, it must happen that . We know that , and so the statement holds, but here I do now how to justify the . To find the derivative at I tried the differentiate directly but it is wrong, so I have been told that I must use the definition. So we have to find Why we have to bound ? How can we do that? Finally, I do not know how to use the aforementioned theorem to justify that the function is derivable in . Using the definition again: but it is wrong. How can we solve the statement? Thanks!","F(x)=\begin{cases}\displaystyle\int_x^{2x}\sin t^2\,\mathrm dt,&x\neq0\\0,&x=0\end{cases} F x_0=\sqrt{\pi/2} F'(x_0) F(0)=\lim_{x\to0}F(x) F(0)=0 \lim_{x\to0}F(x)=\lim_{x\to0}\int_x^{2x}\sin t^2\,\mathrm dt\;{\bf\color{red}=}\int_0^{2\cdot0}\sin t^2\,\mathrm dt=0, \bf\color{red}= x_0=0 F(x) F'(0)=\lim_{x\to0}\frac{F(x)-F(0)}{x-0}=\lim_{x\to0}\frac{\int_x^{2x}\sin t^2\,\mathrm dt}x. \left|\sin t^2\right|\leq t^2 \sqrt{\pi/2} \begin{align*}
F'\left(\sqrt{\frac\pi2}\right)&=\lim_{x\to\sqrt{\frac\pi2}}\frac{F(x)-F\left(\sqrt{\frac\pi2}\right)}{x-\sqrt{\frac\pi2}}\\
&=\lim_{x\to\sqrt{\frac\pi2}}\frac{\int_x^{2x}\sin t^2\,\mathrm dt-\int_{\sqrt{\pi/2}}^{2\sqrt{\pi/2}}\sin t^2\,\mathrm dt}{x-\sqrt{\frac\pi2}}\\
&\leq\lim_{x\to\sqrt{\frac\pi2}}\frac{\int_x^{2x}t^2\,\mathrm dt-\int_{\sqrt{\pi/2}}^{2\sqrt{\pi/2}}t^2\,\mathrm dt}{x-\sqrt{\frac\pi2}}\\
&\underbrace=_{A=\sqrt{\pi/2}}\lim_{x\to A}\frac{1/3((2x)^3-x^3)-1/3((2A)^3-(A^3))}{x-A}\\
&=\frac73\lim_{x\to A}\frac{x^3-A^3}{x-A}\\
&=\frac73\lim_{x\to A}\frac{(x-A)(x^2+Ax+A^2)}{x-A}\\
&=\frac73(A^2+A^2+A^2)\\
&=7A^2\\
&=\frac{7\pi}2,
\end{align*}","['calculus', 'derivatives', 'continuity']"
35,Is the sum of all rational numbers between two integers infinity,Is the sum of all rational numbers between two integers infinity,,"If there are infinite numbers between two rational numbers then would that entail that the sum of all numbers, say between 1 and 2, be infinity? I believe that this cannot be true and has to do something with area under a curve?","If there are infinite numbers between two rational numbers then would that entail that the sum of all numbers, say between 1 and 2, be infinity? I believe that this cannot be true and has to do something with area under a curve?",,"['calculus', 'integration']"
36,Related Rates Calculus Trigonometric Problem,Related Rates Calculus Trigonometric Problem,,"I've been stuck on this related rates problem for a while now and I just can't figure out how to even approach it. The problem goes something like this: The above diagram shows two objects moving at different speeds. Both objects are 0.5 miles from the origin. The blue object is moving at 50mph. The straight-line distance between the blue object and the red object is increasing at a rate of 35 mph. Find the speed of the red object. I tried to solve it using Pythagorean Theorem and finding the derivative of the top side of the triangle. Anyway, I ended up getting a negative number, and even ignoring the sign, the answer I got was obviously wrong. I know the speed of the red object is obviously greater than the blue object because the distance between the objects is increasing. I just really don't know how to calculate the magnitude of said number. There's also a variation of this problem where the straight-line distance is changing at a rate of -35mph but I think that should be doable once I understand how to go about solving the original. Any responses would be appreciated!","I've been stuck on this related rates problem for a while now and I just can't figure out how to even approach it. The problem goes something like this: The above diagram shows two objects moving at different speeds. Both objects are 0.5 miles from the origin. The blue object is moving at 50mph. The straight-line distance between the blue object and the red object is increasing at a rate of 35 mph. Find the speed of the red object. I tried to solve it using Pythagorean Theorem and finding the derivative of the top side of the triangle. Anyway, I ended up getting a negative number, and even ignoring the sign, the answer I got was obviously wrong. I know the speed of the red object is obviously greater than the blue object because the distance between the objects is increasing. I just really don't know how to calculate the magnitude of said number. There's also a variation of this problem where the straight-line distance is changing at a rate of -35mph but I think that should be doable once I understand how to go about solving the original. Any responses would be appreciated!",,['calculus']
37,Does this sequence of polynomials have a name?,Does this sequence of polynomials have a name?,,"I'm very interested in the function $$f : (0,\infty) \rightarrow (0,\infty)$$ $$x \mapsto - \log(1-e^{-x}).$$ When I use Wolfram alpha to compute the $n$ th derivatives of $f$ , I find that there exists a sequence of polynomials $P_1,P_2,\ldots$ such that for $n \geq 1$ we have $$f^{(n)}(x) = \frac{e^x}{(1-e^x)^n}P_n(e^x).$$ For example: $$\frac{d^6}{dx^6}(-\log(1 - e^{-x})) = \frac{e^x}{(1-e^x)^6} (1+26 e^x + 66 e^{2 x} + 26 e^{3 x} + e^{4 x})$$ Question. Is there a name for this sequence of polynomials? If these polynomials don't have a name, I'd also be satisfied with a name for the variant on Pascal's triangle whose entries are the coefficients of these polynomials.","I'm very interested in the function When I use Wolfram alpha to compute the th derivatives of , I find that there exists a sequence of polynomials such that for we have For example: Question. Is there a name for this sequence of polynomials? If these polynomials don't have a name, I'd also be satisfied with a name for the variant on Pascal's triangle whose entries are the coefficients of these polynomials.","f : (0,\infty) \rightarrow (0,\infty) x \mapsto - \log(1-e^{-x}). n f P_1,P_2,\ldots n \geq 1 f^{(n)}(x) = \frac{e^x}{(1-e^x)^n}P_n(e^x). \frac{d^6}{dx^6}(-\log(1 - e^{-x})) = \frac{e^x}{(1-e^x)^6} (1+26 e^x + 66 e^{2 x} + 26 e^{3 x} + e^{4 x})","['calculus', 'polynomials']"
38,Deriving Polya’s Random Walk Constants,Deriving Polya’s Random Walk Constants,,"It is a well known theorem of Pólya that a random walk in 1 or 2 dimensions has a probability of 1 of returning to the origin. However, the probability in the 3-dimensional case is given by a strange triple integral. How is this integral derived?","It is a well known theorem of Pólya that a random walk in 1 or 2 dimensions has a probability of 1 of returning to the origin. However, the probability in the 3-dimensional case is given by a strange triple integral. How is this integral derived?",,"['calculus', 'random-walk']"
39,Utility of consumption with mortality and finite termination,Utility of consumption with mortality and finite termination,,"In [Calvo, Guillermo A., and Maurice Obstfeld. Optimal time-consistent fiscal policy with finite lifetimes. Econometrica: Journal of the Econometric Society (1988): 411-432] , the authors derive $$ \mu(b)\equiv\int_{b}^{\infty}u\left[c(b,t)\right]\left[1-F(t-b)\right]e^{-\beta(t-b)}dt $$ as the utility of an individual where $b$ is the birth time of the individual, $c$ is their consumption, $F$ is the CDF of a random variable corresponding to the length of their life, $u$ is their utility, and $\beta$ is their discount rate. I would like to instead study the case in which there is a terminal time $T>b$ beyond which we do not care about the individual's utility. How can I formulate this as an integral?","In [Calvo, Guillermo A., and Maurice Obstfeld. Optimal time-consistent fiscal policy with finite lifetimes. Econometrica: Journal of the Econometric Society (1988): 411-432] , the authors derive $$ \mu(b)\equiv\int_{b}^{\infty}u\left[c(b,t)\right]\left[1-F(t-b)\right]e^{-\beta(t-b)}dt $$ as the utility of an individual where $b$ is the birth time of the individual, $c$ is their consumption, $F$ is the CDF of a random variable corresponding to the length of their life, $u$ is their utility, and $\beta$ is their discount rate. I would like to instead study the case in which there is a terminal time $T>b$ beyond which we do not care about the individual's utility. How can I formulate this as an integral?",,"['calculus', 'probability', 'integration', 'definite-integrals', 'expectation']"
40,Evaluate $\sum_{r=1}^{\infty} \sqrt {\frac {r}{r^4+r^2+1}}$,Evaluate,\sum_{r=1}^{\infty} \sqrt {\frac {r}{r^4+r^2+1}},One of friend gave me a question today to solve which is as follows $$\sum_{r=1}^{\infty} \sqrt {\frac {r}{r^4+r^2+1}}$$ In spite of much efforts I couldn't solve it and so I asked him to check whether the question was correct or was it this one $$\sum_{r=1}^{\infty} \frac {r}{r^4+r^2+1}$$ I thought the question would be this one because the terms inside the root can be telescoped in absence of root. And indeed I was right. The question was as I expected the latter one. But even after that I thought about whether the first question containing the square root  could also be solved or not. For just checking out the convergence of the sequence I tried the ratio test but it wasn't quite helpful. Then I tried using the integral test and indeed $$\lim_{t\to \infty} \int_{1}^{t} \sqrt{\frac {x}{x^4+x^2+1}} dx$$ this integral converges to $1.80984$ according to Wolfy. Upon lot of efforts too I am not an able to solve the first summation (with the square root terms) .  Can someone please lend me some help over this problem.,One of friend gave me a question today to solve which is as follows $$\sum_{r=1}^{\infty} \sqrt {\frac {r}{r^4+r^2+1}}$$ In spite of much efforts I couldn't solve it and so I asked him to check whether the question was correct or was it this one $$\sum_{r=1}^{\infty} \frac {r}{r^4+r^2+1}$$ I thought the question would be this one because the terms inside the root can be telescoped in absence of root. And indeed I was right. The question was as I expected the latter one. But even after that I thought about whether the first question containing the square root  could also be solved or not. For just checking out the convergence of the sequence I tried the ratio test but it wasn't quite helpful. Then I tried using the integral test and indeed $$\lim_{t\to \infty} \int_{1}^{t} \sqrt{\frac {x}{x^4+x^2+1}} dx$$ this integral converges to $1.80984$ according to Wolfy. Upon lot of efforts too I am not an able to solve the first summation (with the square root terms) .  Can someone please lend me some help over this problem.,,"['calculus', 'integration', 'sequences-and-series', 'limits', 'convergence-divergence']"
41,Composition of Riemann integrable and continuous function is integrable,Composition of Riemann integrable and continuous function is integrable,,"Prove that if $f : [a,b] \to [c,d]$ is Riemann integrable , and $g: [c,d] \to \mathbb{R}$ is continuous then $g\circ f$ is integrable. By Lebesgue we know because $f$ is integrable then $f$ must be discontinuous on at most a set of measure zero, so I need to show that $g\circ f$ is continuous except for at most a set of discontinuous points of measure zero. I need some hints on how to do that, please help.","Prove that if $f : [a,b] \to [c,d]$ is Riemann integrable , and $g: [c,d] \to \mathbb{R}$ is continuous then $g\circ f$ is integrable. By Lebesgue we know because $f$ is integrable then $f$ must be discontinuous on at most a set of measure zero, so I need to show that $g\circ f$ is continuous except for at most a set of discontinuous points of measure zero. I need some hints on how to do that, please help.",,"['calculus', 'riemann-integration']"
42,Calculus Puzzle Book?,Calculus Puzzle Book?,,"I'm looking for a puzzle book to get my Dad for his birthday, and he's very much a mathematician and loves puzzles. I'm trying to find essentially a compendium of challenging integrals (or maths in general). I'd be looking for questions around a Masters student's level, so quite challenging questions. I've come across 'Irresistible Integrals' and 'A treatise on the integral calculus; with applications, examples and problems' but neither are quite what I'm looking for, they're both very textbook-y. I'm trying to find something which isn't trying to teach calculus, but just straight up challenges with solutions. Questions similar to ones in, for example, the MIT Integration Bees. I tried looking for a collection of MIT Integration Bee questions, but couldn't find one. Thanks for reading, has anyone got any ideas? It'd be greatly appreciated.","I'm looking for a puzzle book to get my Dad for his birthday, and he's very much a mathematician and loves puzzles. I'm trying to find essentially a compendium of challenging integrals (or maths in general). I'd be looking for questions around a Masters student's level, so quite challenging questions. I've come across 'Irresistible Integrals' and 'A treatise on the integral calculus; with applications, examples and problems' but neither are quite what I'm looking for, they're both very textbook-y. I'm trying to find something which isn't trying to teach calculus, but just straight up challenges with solutions. Questions similar to ones in, for example, the MIT Integration Bees. I tried looking for a collection of MIT Integration Bee questions, but couldn't find one. Thanks for reading, has anyone got any ideas? It'd be greatly appreciated.",,"['calculus', 'recreational-mathematics', 'puzzle', 'book-recommendation']"
43,How can one show that $\int_{0}^{1}{x\ln{x}\ln(1-x^2)\over \sqrt{1-x^2}}\mathrm dx=4-{\pi^2\over 4}-\ln{4}?$,How can one show that,\int_{0}^{1}{x\ln{x}\ln(1-x^2)\over \sqrt{1-x^2}}\mathrm dx=4-{\pi^2\over 4}-\ln{4}?,How can one show that $$\int_{0}^{1}{x\ln{x}\ln(1-x^2)\over \sqrt{1-x^2}}\mathrm dx=4-{\pi^2\over 4}-\ln{4}?\tag1$$ I have tried IBP but it is seem too complicate. I am not sure what to do or how to tackle this problem. Please any help? Thank!,How can one show that $$\int_{0}^{1}{x\ln{x}\ln(1-x^2)\over \sqrt{1-x^2}}\mathrm dx=4-{\pi^2\over 4}-\ln{4}?\tag1$$ I have tried IBP but it is seem too complicate. I am not sure what to do or how to tackle this problem. Please any help? Thank!,,"['calculus', 'integration', 'improper-integrals', 'closed-form']"
44,How would you prove this idea?,How would you prove this idea?,,"Is it true that if the slope of the secant line between A and B is less than the slope of the secant line between B and C, then the slope of the secant line between A and B is less than the slope of A to C? Is this true and if so, how would you prove it? Edit: A, B, and C are all points on a function Edit2: A possible diagram for the question Edit3: Sorry, I realized I was missing information thanks to the comment and answer. The x values of A, B, and C are in increasing order i.e. A's x < B's x < C's x.","Is it true that if the slope of the secant line between A and B is less than the slope of the secant line between B and C, then the slope of the secant line between A and B is less than the slope of A to C? Is this true and if so, how would you prove it? Edit: A, B, and C are all points on a function Edit2: A possible diagram for the question Edit3: Sorry, I realized I was missing information thanks to the comment and answer. The x values of A, B, and C are in increasing order i.e. A's x < B's x < C's x.",,['calculus']
45,Shortcut to $x\uparrow \uparrow n$ for very large $n$ and $x\approx e^{(e^{-1})}$?,Shortcut to  for very large  and ?,x\uparrow \uparrow n n x\approx e^{(e^{-1})},"If the number $x$ is very close to $e^{(e^{-1})}$ , but a bit larger, for example $x=e^{(e^{-1})}+10^{-20}$, then tetrating $x$ many times can still be small. With $x=e^{(e^{-1})}+10^{-20}$ , even $x\uparrow \uparrow (10^8)$ (This is a power tower of $10^8$ $x's$ calculated from above) is smaller than $e$ Is there any shortcut to calculate such huge power-towers ? In other words, can I efficiently calculate $x\uparrow\uparrow n$ reasonably exact (lets say, with $6$ digits accuracy) ? The brute force method is quite slow and I am not sure whether it is even numerically stable.","If the number $x$ is very close to $e^{(e^{-1})}$ , but a bit larger, for example $x=e^{(e^{-1})}+10^{-20}$, then tetrating $x$ many times can still be small. With $x=e^{(e^{-1})}+10^{-20}$ , even $x\uparrow \uparrow (10^8)$ (This is a power tower of $10^8$ $x's$ calculated from above) is smaller than $e$ Is there any shortcut to calculate such huge power-towers ? In other words, can I efficiently calculate $x\uparrow\uparrow n$ reasonably exact (lets say, with $6$ digits accuracy) ? The brute force method is quite slow and I am not sure whether it is even numerically stable.",,"['calculus', 'tetration', 'complex-dynamics', 'power-towers']"
46,"Limit points of the set $\{\frac{1}{2m} - \frac{1}{2n}\mid n, m \in \mathbb{N}\}$",Limit points of the set,"\{\frac{1}{2m} - \frac{1}{2n}\mid n, m \in \mathbb{N}\}","Let S := $\{\frac{1}{2m} - \frac{1}{2n}\mid n, m \in \mathbb{N}\}$ Then, the limit points will be: $\lim\limits_{n \to \infty} S = \frac{1}{2m}$ $\lim\limits_{m\to \infty} S = \frac{-1}{2n}$. $\lim\limits_{m, n \to \infty} S = 0$. Thus, the limit points of S will be, at least, the following: $ C := \{\frac{1}{2m}, \frac{-1}{2n}, 0\} \subset S'$ Yet, I am not sure that I can to prove that there are no other limit points, that is, that $S' \subset C$. Here's what I've got: Let $x \notin C$ be a limit point of S $\Rightarrow x = \frac{1}{2m} - \frac{1}{2n} \text{ for some } n, m \in \mathbb{N}$. Let $\epsilon = \frac{\sqrt{2}}{2} * (\frac{1}{2m} - \frac{1}{2(m + 1)}) \Rightarrow \nexists s \in S : s \in B(x, \epsilon) \Rightarrow s \notin S'. $ Though I am not sure that the procedure was correct. Any help will be appreciated. Thank you beforehand.","Let S := $\{\frac{1}{2m} - \frac{1}{2n}\mid n, m \in \mathbb{N}\}$ Then, the limit points will be: $\lim\limits_{n \to \infty} S = \frac{1}{2m}$ $\lim\limits_{m\to \infty} S = \frac{-1}{2n}$. $\lim\limits_{m, n \to \infty} S = 0$. Thus, the limit points of S will be, at least, the following: $ C := \{\frac{1}{2m}, \frac{-1}{2n}, 0\} \subset S'$ Yet, I am not sure that I can to prove that there are no other limit points, that is, that $S' \subset C$. Here's what I've got: Let $x \notin C$ be a limit point of S $\Rightarrow x = \frac{1}{2m} - \frac{1}{2n} \text{ for some } n, m \in \mathbb{N}$. Let $\epsilon = \frac{\sqrt{2}}{2} * (\frac{1}{2m} - \frac{1}{2(m + 1)}) \Rightarrow \nexists s \in S : s \in B(x, \epsilon) \Rightarrow s \notin S'. $ Though I am not sure that the procedure was correct. Any help will be appreciated. Thank you beforehand.",,"['calculus', 'sequences-and-series']"
47,Finding the area inside an implicitly defined curve $x^2 + (y + \sqrt[3]{|x|})^2=1$,Finding the area inside an implicitly defined curve,x^2 + (y + \sqrt[3]{|x|})^2=1,"Need help finding the area inside an implicitly defined curve $x^2 + (y + \sqrt[3]{|x|})^2=1$ . (I think it is a heart shape). I've been trying to parameterize it with no luck. I also tried to restrict my attention to $0 \leq x \leq 1$ and do an integral against the $y$ axis from $y=-1$ to $y=1$ (since $x$ is a function of $y$ in that interval) and then add that to the area of the curve in $1 \leq y \leq 1.7$ , which I would do with another integral but against $x$ this time. None of these have worked, because it's difficult to rearrange well enough to get $y$ or $x$ alone on one side. How would you go about solving this? Is there a neat parameterization I'm missing? Also, I believe the area is $\pi$ but I don't know how they figured that.","Need help finding the area inside an implicitly defined curve . (I think it is a heart shape). I've been trying to parameterize it with no luck. I also tried to restrict my attention to and do an integral against the axis from to (since is a function of in that interval) and then add that to the area of the curve in , which I would do with another integral but against this time. None of these have worked, because it's difficult to rearrange well enough to get or alone on one side. How would you go about solving this? Is there a neat parameterization I'm missing? Also, I believe the area is but I don't know how they figured that.",x^2 + (y + \sqrt[3]{|x|})^2=1 0 \leq x \leq 1 y y=-1 y=1 x y 1 \leq y \leq 1.7 x y x \pi,"['calculus', 'integration', 'parametrization']"
48,If we knew Taylor series of $f(x)$ is it possible to get Taylor series of $f^{-1}(x)$?,If we knew Taylor series of  is it possible to get Taylor series of ?,f(x) f^{-1}(x),"If we knew Taylor series of $f(x)$ is it possible to get Taylor series of $f^{-1}(x)$? Suppose, $$f(x)=a_0+a_1x+a_2x^2+\cdots$$ Then $$f^{-1}(x)=?$$ If possible,how do we get it? Easiest would be to expand at $x=a_0$","If we knew Taylor series of $f(x)$ is it possible to get Taylor series of $f^{-1}(x)$? Suppose, $$f(x)=a_0+a_1x+a_2x^2+\cdots$$ Then $$f^{-1}(x)=?$$ If possible,how do we get it? Easiest would be to expand at $x=a_0$",,"['calculus', 'derivatives', 'taylor-expansion']"
49,What is the physical meaning of an integral?,What is the physical meaning of an integral?,,"The derivative $\frac{dy}{dx}$ of a function $y=f(x)$ tells us how has the function $y=f(x)$ changes with the change in $x$ at the point $(x,y)$. What is the physical meaning of the integral of the function $y=f(x)$ i.e., $I(a,b)=\int\limits_{a}^{b} f(x)dx$ except the fact that it represents the area under the curve bounded by $x=a$, $x=b$ and $y=f(x)$? To be specific the work done under a force, in one-dimension, is given by $\int F(x)dx$. Why should it be called a continuous sum? How does the area interpretation work out if the function being integrated is a function of several variables?","The derivative $\frac{dy}{dx}$ of a function $y=f(x)$ tells us how has the function $y=f(x)$ changes with the change in $x$ at the point $(x,y)$. What is the physical meaning of the integral of the function $y=f(x)$ i.e., $I(a,b)=\int\limits_{a}^{b} f(x)dx$ except the fact that it represents the area under the curve bounded by $x=a$, $x=b$ and $y=f(x)$? To be specific the work done under a force, in one-dimension, is given by $\int F(x)dx$. Why should it be called a continuous sum? How does the area interpretation work out if the function being integrated is a function of several variables?",,"['calculus', 'integration', 'physics']"
50,Sum of $\sum_{n=1}^{\infty}\frac{n}{2^n}$. [duplicate],Sum of . [duplicate],\sum_{n=1}^{\infty}\frac{n}{2^n},"This question already has an answer here : Limit of the infinite sum of $\frac{n}{2^n}$? [duplicate] (1 answer) Closed 6 years ago . I was trying to find the sum of $\sum_{n=1}^{\infty}\frac{n}{2^n}$. I tried like this $S = \sum_{n=1}^{\infty}\frac{n}{2^n} = \sum_{n=1}^{\infty}(\frac{n+1}{2^n} - \frac{1}{2^n}) = 2\sum_{n=2}^{\infty}\frac{n}{2^n} - 1$ So,$S = 2(S - \frac{1}{2}) - 1$ Implying $S =2$. EDIT: Also from many posts in the comment below I found a method of looking at the sum $\sum x^n$ and differentiating and plugging for $x$. Is there any other method of looking at the problem and calculating the sum apart from the above method,may be it would be interesting to see different approaches to the same problem which may be used to visualize other problems based on summation of series?","This question already has an answer here : Limit of the infinite sum of $\frac{n}{2^n}$? [duplicate] (1 answer) Closed 6 years ago . I was trying to find the sum of $\sum_{n=1}^{\infty}\frac{n}{2^n}$. I tried like this $S = \sum_{n=1}^{\infty}\frac{n}{2^n} = \sum_{n=1}^{\infty}(\frac{n+1}{2^n} - \frac{1}{2^n}) = 2\sum_{n=2}^{\infty}\frac{n}{2^n} - 1$ So,$S = 2(S - \frac{1}{2}) - 1$ Implying $S =2$. EDIT: Also from many posts in the comment below I found a method of looking at the sum $\sum x^n$ and differentiating and plugging for $x$. Is there any other method of looking at the problem and calculating the sum apart from the above method,may be it would be interesting to see different approaches to the same problem which may be used to visualize other problems based on summation of series?",,"['calculus', 'sequences-and-series']"
51,"Show that, for each rational function $h(x)$, the function $f(x)=e^{x^2}$ doesn't admit a primitive in the form $G(x)=e^{x^2}h(x)$.","Show that, for each rational function , the function  doesn't admit a primitive in the form .",h(x) f(x)=e^{x^2} G(x)=e^{x^2}h(x),"I'm trying this exercise: Show that, for each rational function $h(x)$, the function $f(x)=e^{x^2}$ doesn't admit ( on any interval in $\mathbb{R}$ ) a primitive in the form $G(x)=e^{x^2}h(x)$. I suppose that $G'(x)=f(x) \ \ \ \rm \forall x \in I \subset \mathbb{R}$ for a interval, so $G'(x)=e^{x^2}h'(x) + h(x)e^{x^2}2x$. But $h(x)=\dfrac{p(x)}{q(x)}$ where $p(x)$ and $q(x)$ are polynomials, so $G'(x)=e^{x^2}\dfrac{p'(x)q(x)-q'(x)p(x)}{(q(x))^2}+\dfrac{2xe^{x^2}p(x)}{q(x)}=e^{x^2}$. Simplifying the equation I get $p'(x)q(x)-q'(x)p(x)+2xp(x)q(x)=(q(x))^2$. Now, I would like to find a contradiction, but I can't. Perhaps the I interpreted wrongly the text and the polynomials $p(x)$ and $q(x)$ are integer polynomials?","I'm trying this exercise: Show that, for each rational function $h(x)$, the function $f(x)=e^{x^2}$ doesn't admit ( on any interval in $\mathbb{R}$ ) a primitive in the form $G(x)=e^{x^2}h(x)$. I suppose that $G'(x)=f(x) \ \ \ \rm \forall x \in I \subset \mathbb{R}$ for a interval, so $G'(x)=e^{x^2}h'(x) + h(x)e^{x^2}2x$. But $h(x)=\dfrac{p(x)}{q(x)}$ where $p(x)$ and $q(x)$ are polynomials, so $G'(x)=e^{x^2}\dfrac{p'(x)q(x)-q'(x)p(x)}{(q(x))^2}+\dfrac{2xe^{x^2}p(x)}{q(x)}=e^{x^2}$. Simplifying the equation I get $p'(x)q(x)-q'(x)p(x)+2xp(x)q(x)=(q(x))^2$. Now, I would like to find a contradiction, but I can't. Perhaps the I interpreted wrongly the text and the polynomials $p(x)$ and $q(x)$ are integer polynomials?",,"['calculus', 'derivatives', 'polynomials']"
52,Polar Coordinates tranformation for Linear Homogeneous Differential Equations (1st order),Polar Coordinates tranformation for Linear Homogeneous Differential Equations (1st order),,"While studying a book of Differential Equations I found this problem so interesting. Suppose  \begin{equation}  M(x,y)dx+N(x,y)dy=0          \tag 1 \end{equation} is  a homogeneous ODE. Show that the transformation $x=r \cos (\theta) $ and $y= r \sin (\theta) $ reduces the equation to a separable  equation in the variables $r$ and $\theta$ Is from the book Diff. Eq's, Shepley L.  Ross. So starting from the hypothesis that the equation is homogenous then $(1)$ is equivalent to \begin{equation}  \frac{dy}{dx}=g\left(\frac{y}{x}\right)          \tag 2 \end{equation} So the thing is thatI don't know how to relate $x$ and $y$ , or more precisely how to find the relation $\dfrac{dr}{d\theta}$ (or maybe the other way around) The first thing that came to  mind was $r^2=x^2+y^2$ but how do I differentiate it ? I mean, I don´t see clearly how to use the chain rule I have seen this $2rr'=2xx'+2yy'$. Although, still not clear how did they do it. Later, a silly approach (I think so) was to take the differentials of either $x$ and $y$ with respect to $r$ and $\theta$, respectively. Therefore: $$x=r \cos (\theta) \Rightarrow dx=\cos (\theta) dr  $$ and  $$y= r \sin (\theta) \Rightarrow dy= r \cos (\theta) d\theta .$$  Later $$\frac{dy}{dx}=\frac{r \cos (\theta) d\theta} { \cos (\theta) dr} = \frac{r d\theta} {dr}$$ So after substituting in $(2)$ \begin{equation}  \frac{r d\theta} {dr}=g\left(\frac{\sin \theta }{ \cos \theta }\right)   \end{equation} which reduces it to a separable equation \begin{equation}  \frac{dr } {r }=\frac{d\theta} { g(\tan \theta )}    \end{equation} But.... come on! At least I tried... Later the book  has also as an exercise to prove that the same equations is invariant under the tranformations $x=k\alpha$ and $y=k\beta$ with $k$ constant. But I think that the previous one seems more approachable. Could someone help me with this kind of problems? Thanks. :)","While studying a book of Differential Equations I found this problem so interesting. Suppose  \begin{equation}  M(x,y)dx+N(x,y)dy=0          \tag 1 \end{equation} is  a homogeneous ODE. Show that the transformation $x=r \cos (\theta) $ and $y= r \sin (\theta) $ reduces the equation to a separable  equation in the variables $r$ and $\theta$ Is from the book Diff. Eq's, Shepley L.  Ross. So starting from the hypothesis that the equation is homogenous then $(1)$ is equivalent to \begin{equation}  \frac{dy}{dx}=g\left(\frac{y}{x}\right)          \tag 2 \end{equation} So the thing is thatI don't know how to relate $x$ and $y$ , or more precisely how to find the relation $\dfrac{dr}{d\theta}$ (or maybe the other way around) The first thing that came to  mind was $r^2=x^2+y^2$ but how do I differentiate it ? I mean, I don´t see clearly how to use the chain rule I have seen this $2rr'=2xx'+2yy'$. Although, still not clear how did they do it. Later, a silly approach (I think so) was to take the differentials of either $x$ and $y$ with respect to $r$ and $\theta$, respectively. Therefore: $$x=r \cos (\theta) \Rightarrow dx=\cos (\theta) dr  $$ and  $$y= r \sin (\theta) \Rightarrow dy= r \cos (\theta) d\theta .$$  Later $$\frac{dy}{dx}=\frac{r \cos (\theta) d\theta} { \cos (\theta) dr} = \frac{r d\theta} {dr}$$ So after substituting in $(2)$ \begin{equation}  \frac{r d\theta} {dr}=g\left(\frac{\sin \theta }{ \cos \theta }\right)   \end{equation} which reduces it to a separable equation \begin{equation}  \frac{dr } {r }=\frac{d\theta} { g(\tan \theta )}    \end{equation} But.... come on! At least I tried... Later the book  has also as an exercise to prove that the same equations is invariant under the tranformations $x=k\alpha$ and $y=k\beta$ with $k$ constant. But I think that the previous one seems more approachable. Could someone help me with this kind of problems? Thanks. :)",,"['calculus', 'ordinary-differential-equations', 'linear-transformations', 'polar-coordinates']"
53,Volume of $E=\{5x^8 \le y \le 7x^8; 2y^5 \le z \le 3y^5; z^7 \le x \le 6z^7\}$.,Volume of .,E=\{5x^8 \le y \le 7x^8; 2y^5 \le z \le 3y^5; z^7 \le x \le 6z^7\},"I am stuck with the following problem: Compute volume of $E=\{5x^8 \le y \le 7x^8; 2y^5  \le z  \le 3y^5; z^7 \le x \le 6z^7\}$. My progress: It is easy to see that $x,y,z \ge 0$. Therefore I can add all inequalities and after simple algebra get the following: $$5x^8 + 2y^5+z^7 \le x+y+z \le 7x^8 + 3y^5+6x^7$$ Update: as it was pointed out by A.Γ. adding inequalities does not work because different sets can give the same inequalities. So, it looks like I should solve or at least get some kind of information for integration directly from the system somehow. Any hint? Thanks a lot for your help!","I am stuck with the following problem: Compute volume of $E=\{5x^8 \le y \le 7x^8; 2y^5  \le z  \le 3y^5; z^7 \le x \le 6z^7\}$. My progress: It is easy to see that $x,y,z \ge 0$. Therefore I can add all inequalities and after simple algebra get the following: $$5x^8 + 2y^5+z^7 \le x+y+z \le 7x^8 + 3y^5+6x^7$$ Update: as it was pointed out by A.Γ. adding inequalities does not work because different sets can give the same inequalities. So, it looks like I should solve or at least get some kind of information for integration directly from the system somehow. Any hint? Thanks a lot for your help!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
54,Proof that $ \int_0^\infty \frac{(f'(x))^2}{f(x)^{1980}} dx < \infty$,Proof that, \int_0^\infty \frac{(f'(x))^2}{f(x)^{1980}} dx < \infty,"Suppose that $f(x), f'(x), f''(x)$ are continuous on $(0,+\infty)$ and $f(x) \geq \alpha > 0,$ and the improper integral $\displaystyle \int_{0}^{\infty} |f''(x)| dx$ exists. Proof that $$\displaystyle \int_0^\infty \dfrac{(f'(x))^2}{f(x)^{1980}} dx < \infty$$ My attempt is to use $\displaystyle \int \frac{f'(x)}{f(x)^{990}} dx = \frac{-1}{989 f(x)^{989}} $ but I got stuck. How can I use the continuity of $f , f', f''$","Suppose that $f(x), f'(x), f''(x)$ are continuous on $(0,+\infty)$ and $f(x) \geq \alpha > 0,$ and the improper integral $\displaystyle \int_{0}^{\infty} |f''(x)| dx$ exists. Proof that $$\displaystyle \int_0^\infty \dfrac{(f'(x))^2}{f(x)^{1980}} dx < \infty$$ My attempt is to use $\displaystyle \int \frac{f'(x)}{f(x)^{990}} dx = \frac{-1}{989 f(x)^{989}} $ but I got stuck. How can I use the continuity of $f , f', f''$",,"['calculus', 'integration', 'improper-integrals']"
55,Why do I get a saddle point and not a maximum?,Why do I get a saddle point and not a maximum?,,"We have the function $f(x,y)=36xy$ and we want to maximize it subject to $3x+6y=m$.  Using the Lagrange method I found that a critical point is $\left (\frac{m}{6}, \frac{m}{12}\right )$. Using the second order condition and the Hessian matrix I found that that point is a saddle point. For example the Hessian matrix is: \begin{equation*}H_F\left (\frac{m}{6}, \frac{m}{12}\right )=\begin{pmatrix}0 & 36  \\ 36 & 0\end{pmatrix}\end{equation*} Then the eigenvalues are $\pm 36$. Since we have positive and negative eigenvalues, the matrix is indefinite, and this means that the point is a saddle point. Right? But according to Wolfram it is a maximum point. (for example for m=40 ). So, why do I get a saddle point?","We have the function $f(x,y)=36xy$ and we want to maximize it subject to $3x+6y=m$.  Using the Lagrange method I found that a critical point is $\left (\frac{m}{6}, \frac{m}{12}\right )$. Using the second order condition and the Hessian matrix I found that that point is a saddle point. For example the Hessian matrix is: \begin{equation*}H_F\left (\frac{m}{6}, \frac{m}{12}\right )=\begin{pmatrix}0 & 36  \\ 36 & 0\end{pmatrix}\end{equation*} Then the eigenvalues are $\pm 36$. Since we have positive and negative eigenvalues, the matrix is indefinite, and this means that the point is a saddle point. Right? But according to Wolfram it is a maximum point. (for example for m=40 ). So, why do I get a saddle point?",,"['calculus', 'multivariable-calculus', 'optimization', 'maxima-minima']"
56,Find what the given sum is equal to,Find what the given sum is equal to,,"I am given the following sum: $$\sum_{n=0}^\infty{\frac{1}{2^{n+2}(n+2)}}$$ How do I find what is it equal to? It sort of looks like a logarithm, but not exactly, so how do I proceed to solve this problem?","I am given the following sum: $$\sum_{n=0}^\infty{\frac{1}{2^{n+2}(n+2)}}$$ How do I find what is it equal to? It sort of looks like a logarithm, but not exactly, so how do I proceed to solve this problem?",,['calculus']
57,Prove that $f(x) \leq f(0)e^{-x}$,Prove that,f(x) \leq f(0)e^{-x},"function $f : [0,\infty) \to [0,\infty)$ satisfies next two conditions. a) $$\lim_{x \to \infty} f(x)=0$$ b) $f''(x)$ exists, is continuous and $$f''(x) \geq f(x)$$ for all $x$ in domain. Then, prove that $$f(x) \leq f(0)e^{-x}$$ for $\forall x \in [0,\infty)$. I don't know how to prove it...","function $f : [0,\infty) \to [0,\infty)$ satisfies next two conditions. a) $$\lim_{x \to \infty} f(x)=0$$ b) $f''(x)$ exists, is continuous and $$f''(x) \geq f(x)$$ for all $x$ in domain. Then, prove that $$f(x) \leq f(0)e^{-x}$$ for $\forall x \in [0,\infty)$. I don't know how to prove it...",,"['calculus', 'analysis']"
58,Evaluate $\int x^2e^{x^2} dx$,Evaluate,\int x^2e^{x^2} dx,"Evaluate $\displaystyle\int x^2e^{x^2} dx$ Try($1$) (integral by parts) (unsuccessful) $$\displaystyle\int x^2e^{x^2}dx=x^2\left(\int e^{x^2} dx\right)-2\int x\left(\int e^{x^2} dx\right)dx$$ I don't know how to calculate $\left(\displaystyle\int e^{x^2} dx\right)$, as well. Try($2$) (integral by parts) (unsuccessful) $$\displaystyle\int x^2e^{x^2}dx=e^{x^2}x^3/3-2/3\int x^4e^{x^2}dx $$$$\rightarrow$$$$\int x^4e^{x^2}dx=x^5e^{x^2}/5-2/5\int x^6e^{x^2}dx$$$$\vdots$$ Try($3$) (Integration by substitution) (unsuccessful) $$x=\sqrt t$$$$\displaystyle\int x^2e^{x^2}dx=1/2\int \sqrt t\;e^t\; dt$$ Let's apply ""integral by parts"" $$\int \sqrt t\;e^t\; dt=\sqrt t\;e^t-1/2\int\dfrac{e^t}{\sqrt t}dt$$$$\vdots$$ Try($4$) (Integration by substitution(trigonometric)) (unsuccessful) $$x=\sin u$$ $$\displaystyle\int x^2e^{x^2}dx=\int e^{\sin^2 u}\sin^2 u\cos u du$$","Evaluate $\displaystyle\int x^2e^{x^2} dx$ Try($1$) (integral by parts) (unsuccessful) $$\displaystyle\int x^2e^{x^2}dx=x^2\left(\int e^{x^2} dx\right)-2\int x\left(\int e^{x^2} dx\right)dx$$ I don't know how to calculate $\left(\displaystyle\int e^{x^2} dx\right)$, as well. Try($2$) (integral by parts) (unsuccessful) $$\displaystyle\int x^2e^{x^2}dx=e^{x^2}x^3/3-2/3\int x^4e^{x^2}dx $$$$\rightarrow$$$$\int x^4e^{x^2}dx=x^5e^{x^2}/5-2/5\int x^6e^{x^2}dx$$$$\vdots$$ Try($3$) (Integration by substitution) (unsuccessful) $$x=\sqrt t$$$$\displaystyle\int x^2e^{x^2}dx=1/2\int \sqrt t\;e^t\; dt$$ Let's apply ""integral by parts"" $$\int \sqrt t\;e^t\; dt=\sqrt t\;e^t-1/2\int\dfrac{e^t}{\sqrt t}dt$$$$\vdots$$ Try($4$) (Integration by substitution(trigonometric)) (unsuccessful) $$x=\sin u$$ $$\displaystyle\int x^2e^{x^2}dx=\int e^{\sin^2 u}\sin^2 u\cos u du$$",,"['calculus', 'integration']"
59,"Navier-Stokes equation, about pressure...","Navier-Stokes equation, about pressure...",,"I'm a computer science student writting a dissertation about fluid simulation on real time applications. I'm trying to understand a few things regarding the pressure term: 1) When talking about the Helmholtz-Hodge decomposition, my books say that you can decompose any vector field into a divergence-free field, and a gradient of a scalar fied, and that scalar field ""turns out"" to be the pressure. How do they get to that conclusion? ( http://http.developer.nvidia.com/GPUGems/gpugems_ch38.html chapter 38.2.4 section ""The Helmholtz-Hodge Decomposition"") 2) In many books or articles when computing pressure people say to take the divergence of the momentum equation, that ends up in a poisson equation. I don't understand why they take the divergence of everything, probably due to my lack of understanding of how poisson equation works or something related. ( http://http.developer.nvidia.com/GPUGems/gpugems_ch38.html chapter 38.2.4 section ""The Helmholtz-Hodge Decomposition - Second realization"" , in this article they don't take the divergence of the momentum equation but it's similar and with the same purpose) Keep in mind that I'm not a physics student so I kinda had to learn everything myself because in my university we don't really see much of this things, so probably my confusion is due to the lack of some basic concept that I haven't seen yet. Any help is appreciated, thanks.","I'm a computer science student writting a dissertation about fluid simulation on real time applications. I'm trying to understand a few things regarding the pressure term: 1) When talking about the Helmholtz-Hodge decomposition, my books say that you can decompose any vector field into a divergence-free field, and a gradient of a scalar fied, and that scalar field ""turns out"" to be the pressure. How do they get to that conclusion? ( http://http.developer.nvidia.com/GPUGems/gpugems_ch38.html chapter 38.2.4 section ""The Helmholtz-Hodge Decomposition"") 2) In many books or articles when computing pressure people say to take the divergence of the momentum equation, that ends up in a poisson equation. I don't understand why they take the divergence of everything, probably due to my lack of understanding of how poisson equation works or something related. ( http://http.developer.nvidia.com/GPUGems/gpugems_ch38.html chapter 38.2.4 section ""The Helmholtz-Hodge Decomposition - Second realization"" , in this article they don't take the divergence of the momentum equation but it's similar and with the same purpose) Keep in mind that I'm not a physics student so I kinda had to learn everything myself because in my university we don't really see much of this things, so probably my confusion is due to the lack of some basic concept that I haven't seen yet. Any help is appreciated, thanks.",,"['calculus', 'fluid-dynamics']"
60,$\int \sec^3x dx$ in disguise,in disguise,\int \sec^3x dx,"I found this integral,  $$\int \sqrt{x^2+1}dx$$ on a problem list and I think it is a sneaky way of hiding a $\int \sec^3xdx$ problem but I am not sure if what I did was correct though, because of what happens at the end. So what I did was use trig-substitution and let $u=\tan\theta$ and $du=\sec^2\theta d\theta$ $$\int \sqrt{x^2+1}dx=\int\sqrt{\tan^2\theta+1}\sec^2\theta d\theta=\int \sec^3\theta d\theta$$ $$= \int (\tan^2\theta+1)\sec\theta d\theta = \int \tan^2\theta \sec\theta +\int \sec\theta d\theta$$ I will label $I_s=\int \sec\theta d\theta = \ln|\sec\theta+\tan\theta|$ so I have $$\int \sec^3xdx=\int\tan^2x\sec x dx+I_s$$ and use integration by parts to solve $\int tan^2\theta \sec\theta d\theta$ that remains. I let $u=\sec\theta$ so $du=\sec\theta\tan\theta d\theta$ and $dv=\tan^2\theta\, d\theta$ such that $v=\tan\theta -\theta$ $$\int\tan^2x\sec x dx =\sec\theta (\tan\theta -\theta) - \int (\tan\theta-\theta)\sec\theta\tan\theta d\theta$$ $$= \frac 12 \sec\theta (\tan\theta -\theta) + \frac 12\int \theta \sec\theta\tan\theta \, d\theta$$ Again I use integration by parts, this time on $\frac 12\int \theta \sec\theta\tan\theta \, d\theta$ and let $u=\theta$ so $du=d\theta$ and $dv=\sec\theta\tan\theta \,d\theta$ so $v=\sec\theta$ $$\frac 12\int \theta \sec\theta\tan\theta \, d\theta = \frac 12 \theta \sec\theta - \frac 12 I_s$$ $$= \frac 12(\theta\sec\theta - I_s)$$ and use this result to get $$\int\tan^2x\sec x dx = \frac 12 \sec\theta (\tan\theta -\theta) + \frac 12(\theta\sec\theta - I_s)$$ $$= \frac 12 \sec\theta\tan\theta - \frac 12 I_s$$ Putting everything back together and including an arbitrary constant $C$ i get $$\int \sec^3\theta\,d\theta = \int \tan^2\theta\sec\theta d\theta + I_s$$ $$= \frac 12 \sec\theta\tan\theta - \frac 12 I_s + I_s + C$$ $$=\frac 12\sec\theta\tan\theta +\frac 12 \ln|\sec\theta+\tan\theta|+C$$ and substituting back into $x$-variable terms using $\tan\theta = x$ I have $\sec\theta = \sqrt{x^2+1}$ so  $$\int \sqrt{x^2+1}dx =\frac 12x\sqrt{x^2+1} +\frac 12 \ln|\sqrt{x^2+1}+x|+C$$ The reason why I am not sure about this is because when substituting back I have $\sec\theta = \sqrt{x^2+1}$ which is the same as the integrand $\sqrt{x^2+1}$ that became $\sec^3\theta$ in the initial substitutions. What is going on here? Thank you all for the precious help!","I found this integral,  $$\int \sqrt{x^2+1}dx$$ on a problem list and I think it is a sneaky way of hiding a $\int \sec^3xdx$ problem but I am not sure if what I did was correct though, because of what happens at the end. So what I did was use trig-substitution and let $u=\tan\theta$ and $du=\sec^2\theta d\theta$ $$\int \sqrt{x^2+1}dx=\int\sqrt{\tan^2\theta+1}\sec^2\theta d\theta=\int \sec^3\theta d\theta$$ $$= \int (\tan^2\theta+1)\sec\theta d\theta = \int \tan^2\theta \sec\theta +\int \sec\theta d\theta$$ I will label $I_s=\int \sec\theta d\theta = \ln|\sec\theta+\tan\theta|$ so I have $$\int \sec^3xdx=\int\tan^2x\sec x dx+I_s$$ and use integration by parts to solve $\int tan^2\theta \sec\theta d\theta$ that remains. I let $u=\sec\theta$ so $du=\sec\theta\tan\theta d\theta$ and $dv=\tan^2\theta\, d\theta$ such that $v=\tan\theta -\theta$ $$\int\tan^2x\sec x dx =\sec\theta (\tan\theta -\theta) - \int (\tan\theta-\theta)\sec\theta\tan\theta d\theta$$ $$= \frac 12 \sec\theta (\tan\theta -\theta) + \frac 12\int \theta \sec\theta\tan\theta \, d\theta$$ Again I use integration by parts, this time on $\frac 12\int \theta \sec\theta\tan\theta \, d\theta$ and let $u=\theta$ so $du=d\theta$ and $dv=\sec\theta\tan\theta \,d\theta$ so $v=\sec\theta$ $$\frac 12\int \theta \sec\theta\tan\theta \, d\theta = \frac 12 \theta \sec\theta - \frac 12 I_s$$ $$= \frac 12(\theta\sec\theta - I_s)$$ and use this result to get $$\int\tan^2x\sec x dx = \frac 12 \sec\theta (\tan\theta -\theta) + \frac 12(\theta\sec\theta - I_s)$$ $$= \frac 12 \sec\theta\tan\theta - \frac 12 I_s$$ Putting everything back together and including an arbitrary constant $C$ i get $$\int \sec^3\theta\,d\theta = \int \tan^2\theta\sec\theta d\theta + I_s$$ $$= \frac 12 \sec\theta\tan\theta - \frac 12 I_s + I_s + C$$ $$=\frac 12\sec\theta\tan\theta +\frac 12 \ln|\sec\theta+\tan\theta|+C$$ and substituting back into $x$-variable terms using $\tan\theta = x$ I have $\sec\theta = \sqrt{x^2+1}$ so  $$\int \sqrt{x^2+1}dx =\frac 12x\sqrt{x^2+1} +\frac 12 \ln|\sqrt{x^2+1}+x|+C$$ The reason why I am not sure about this is because when substituting back I have $\sec\theta = \sqrt{x^2+1}$ which is the same as the integrand $\sqrt{x^2+1}$ that became $\sec^3\theta$ in the initial substitutions. What is going on here? Thank you all for the precious help!",,"['calculus', 'integration']"
61,Two interesting results in integration $\int_{0}^{a}f(a-x) \ \mathrm{d}x= \int_{0}^{a}f(x)\ \mathrm{d}x$ and differentiation of powers of functions,Two interesting results in integration  and differentiation of powers of functions,\int_{0}^{a}f(a-x) \ \mathrm{d}x= \int_{0}^{a}f(x)\ \mathrm{d}x,"I am investigating the following result in integration $\displaystyle\int_{0}^{a}f(a-x) \ \mathrm{d}x = \int_{0}^{a}f(x) \ \mathrm{d}x \ \ \ (*)$ This neat little result forms the basis for many questions in calculus exams, often then asking one to evaluate something like $\displaystyle\int_{0}^{\frac{\pi}{2}}\frac{\sin^n x}{\sin^n x + \cos^n x} \ \mathrm{d}x$ where $n$ is a positive integer. The process of solving this integral isn't too challenging, and is almost immediate from $(*)$. My question is this: can anyone think of any more challenging integrals out there (possibly requiring some clever substitution, integration by parts etc.) that $(*)$ can help solve? UPDATE I also came across another identity involving differentiation: $\displaystyle \frac{\mathrm{d}}{\mathrm{d}x}(u(x))^{v(x)} = (u(x))^{v(x)}\left(\frac{\mathrm{d}v(x)}{\mathrm{d}x}\ln u(x) + \frac{v(x)}{u(x)}\frac{\mathrm{d}u(x)}{\mathrm{d}x}\right)$. This is another identity that can be used to solve integrals, but I am again unable to find any creative examples, so if anyone could suggest some I'd be happy to give them a go.","I am investigating the following result in integration $\displaystyle\int_{0}^{a}f(a-x) \ \mathrm{d}x = \int_{0}^{a}f(x) \ \mathrm{d}x \ \ \ (*)$ This neat little result forms the basis for many questions in calculus exams, often then asking one to evaluate something like $\displaystyle\int_{0}^{\frac{\pi}{2}}\frac{\sin^n x}{\sin^n x + \cos^n x} \ \mathrm{d}x$ where $n$ is a positive integer. The process of solving this integral isn't too challenging, and is almost immediate from $(*)$. My question is this: can anyone think of any more challenging integrals out there (possibly requiring some clever substitution, integration by parts etc.) that $(*)$ can help solve? UPDATE I also came across another identity involving differentiation: $\displaystyle \frac{\mathrm{d}}{\mathrm{d}x}(u(x))^{v(x)} = (u(x))^{v(x)}\left(\frac{\mathrm{d}v(x)}{\mathrm{d}x}\ln u(x) + \frac{v(x)}{u(x)}\frac{\mathrm{d}u(x)}{\mathrm{d}x}\right)$. This is another identity that can be used to solve integrals, but I am again unable to find any creative examples, so if anyone could suggest some I'd be happy to give them a go.",,"['calculus', 'integration']"
62,Prove that $f$ is increasing if and only if a given inequality holds,Prove that  is increasing if and only if a given inequality holds,f,"Let $f: [0, \infty) \to \mathbb{R}$ be a continuous function. Prove that $f$ is increasing if and only if: $$\int_a^b f(x) dx \leq bf(b) - af(a), \, \forall \, \, 0 \leq a \leq b.$$ I have no difficulties in proving that if $f$ is increasing then the inequality holds. But I haven't figured out yet how to prove it the other way around, that is knowing the inequality and proving that $f$ is increasing. Thank you!","Let be a continuous function. Prove that is increasing if and only if: I have no difficulties in proving that if is increasing then the inequality holds. But I haven't figured out yet how to prove it the other way around, that is knowing the inequality and proving that is increasing. Thank you!","f: [0, \infty) \to \mathbb{R} f \int_a^b f(x) dx \leq bf(b) - af(a), \, \forall \, \, 0 \leq a \leq b. f f","['calculus', 'inequality', 'definite-integrals']"
63,Prove $\sum\limits_{k=1}^n a_k\sum\limits_{k=1}^n \frac1{a_k}\le\left(n+\frac12\right)^2$ then $\max a_k \le 4\min a_k$,Prove  then,\sum\limits_{k=1}^n a_k\sum\limits_{k=1}^n \frac1{a_k}\le\left(n+\frac12\right)^2 \max a_k \le 4\min a_k,"Prove that if   \begin{align} &0<a_1,a_2,\dots,a_n \in \mathbb R, &\left(\sum_{k=1}^n a_k\right)\left(\sum_{k=1}^n \frac1{a_k}\right)\le\left(n+\frac12\right)^2\\ \end{align} then   $$\max_k \space a_k \le 4\times\min_k\space a_k$$ What I've tried was $$\left(\sum_{k=1}^n a_k\right)\left(\sum_{k=1}^n \frac1{a_k}\right)=n+\sum_{i\ne j}\frac{a_i}{a_j}=n+\sum_{i< j}\left(\frac{a_i}{a_j}+\frac{a_j}{a_i}\right)$$ which didn't help at all. Thanks.","Prove that if   \begin{align} &0<a_1,a_2,\dots,a_n \in \mathbb R, &\left(\sum_{k=1}^n a_k\right)\left(\sum_{k=1}^n \frac1{a_k}\right)\le\left(n+\frac12\right)^2\\ \end{align} then   $$\max_k \space a_k \le 4\times\min_k\space a_k$$ What I've tried was $$\left(\sum_{k=1}^n a_k\right)\left(\sum_{k=1}^n \frac1{a_k}\right)=n+\sum_{i\ne j}\frac{a_i}{a_j}=n+\sum_{i< j}\left(\frac{a_i}{a_j}+\frac{a_j}{a_i}\right)$$ which didn't help at all. Thanks.",,"['calculus', 'algebra-precalculus', 'inequality']"
64,Does the order of a differential equation necessarily equal the number of arbitrary constants in the general solution?,Does the order of a differential equation necessarily equal the number of arbitrary constants in the general solution?,,Consider the differential equation $(y')^2+3y'+2=0$. The general solution seems to be $(y+x+c1)(y+2x+c2)=0$ with effective two arbitrary constants.But the order of the equation is 1. Can this discrepancy be avoided?Or is there some clause in the law relating order to the number of constants which i am missing?,Consider the differential equation $(y')^2+3y'+2=0$. The general solution seems to be $(y+x+c1)(y+2x+c2)=0$ with effective two arbitrary constants.But the order of the equation is 1. Can this discrepancy be avoided?Or is there some clause in the law relating order to the number of constants which i am missing?,,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
65,"Prove that the function f(x,y) is not differentiable","Prove that the function f(x,y) is not differentiable",,"I have the following problem: Prove that the function: $f(x,y)= \ \begin{cases}        \frac{x^3-x\cdot y^2}{x^2+y^2} & (x,y)\neq (0,0) \\        \\0 & (x,y)=(0,0)    \end{cases} \\$ is continuous on $R^2$ and has its first order partial derivatives.  everywhere on $R^2$, but $f$ is not differentiable at $(0,0)$ I know how to prove that it is continuous on $R^2$ and its partial derivatives exist at $(0,0)$ (I use limit definition of a derivative). But I do not know how to prove that this function is not differentiable.","I have the following problem: Prove that the function: $f(x,y)= \ \begin{cases}        \frac{x^3-x\cdot y^2}{x^2+y^2} & (x,y)\neq (0,0) \\        \\0 & (x,y)=(0,0)    \end{cases} \\$ is continuous on $R^2$ and has its first order partial derivatives.  everywhere on $R^2$, but $f$ is not differentiable at $(0,0)$ I know how to prove that it is continuous on $R^2$ and its partial derivatives exist at $(0,0)$ (I use limit definition of a derivative). But I do not know how to prove that this function is not differentiable.",,"['calculus', 'multivariable-calculus']"
66,How to prove easily that a generalized ellipse is $C^1$,How to prove easily that a generalized ellipse is,C^1,"Consider the graphical representation below: It describes a generalized ellipse $(E)$ in the following sense: $$(E)=\{M \ \ | \ \ d(M,(S_1))+d(M,(S_2))=18 \}$$ where $(S_1)$ and $(S_2)$, playing a role of ""generalized foci"", are the squares centered in $F_1(-7,0)$ and $F_2(7,0)$ with sides' length 2, $d(M,(C))$ is the distance from a point $M$ to a convex set $(C)$ defined by $inf_{C \in (C)} d(M,C)$ where $d(.,.)$ is the usual (Euclidean) distance. $d(M,(C))$ being a continuous function of $M$ (see for example ( Distance to a closed set is continuous. )), generalized ellipse $(E)$ is piecewise continuous. I would like to show that it is more than that: that it is smooth, in a geometrical sense: (what is called sometimes $\Delta^1$) in each connection point of two arcs (see below) there is a common tangent (as a consequence, it will be possible to define a $C^1$ parameterization). Question: is there a simple mean to establish this ""smoothness"" ? My work: I have established this smoothness explicitly by first computing the equations of the different constituent arcs of $(E)$. then for each connecting point (such as $L, N, I$...), by computing the left and right derivative (when it exists) and checking that their values are equal or checking the presence of a vertical tangent. Due to $Ox$ and $Oy$ symmetries, it suffices to describe the first quadrant part of (E), or, in an equivalent form, the different parts of arc MLNIS. Here are the equations of the constituent arcs: $$\begin{cases} \text{Arc ML:}\ &\text{line segment:} \ \ & x=10 & -1 \leq y \leq 1\\ \text{Arc LN:}\ &\text{ellipse with foci B and C:} \ \ & \frac{(x-1)^2}{81}+\frac{(y-1)^2}{32}=1& 8 \leq x \leq 10, 1 \leq y \leq \frac{41}{9}\\ \text{Arc NI:}\ &\text{parabola:} \ \ & y=- \frac{1}{36}(x+6)^2+10& 6 \leq x \leq 8, \frac{41}{9} \leq y \leq 6\\ \text{Arc IS:}\ &\text{ellipse with foci B and A:} \ \ & \frac{x^2}{81}+\frac{(y-1)^2}{45}=1& -6 \leq x \leq 6, 6 \leq y \leq 8 \end{cases}$$ But of course, it is very tedious... Moreover I would like to generalize this study to ""generalized foci"" that could be any polygonal shape, and even any convex curve.","Consider the graphical representation below: It describes a generalized ellipse $(E)$ in the following sense: $$(E)=\{M \ \ | \ \ d(M,(S_1))+d(M,(S_2))=18 \}$$ where $(S_1)$ and $(S_2)$, playing a role of ""generalized foci"", are the squares centered in $F_1(-7,0)$ and $F_2(7,0)$ with sides' length 2, $d(M,(C))$ is the distance from a point $M$ to a convex set $(C)$ defined by $inf_{C \in (C)} d(M,C)$ where $d(.,.)$ is the usual (Euclidean) distance. $d(M,(C))$ being a continuous function of $M$ (see for example ( Distance to a closed set is continuous. )), generalized ellipse $(E)$ is piecewise continuous. I would like to show that it is more than that: that it is smooth, in a geometrical sense: (what is called sometimes $\Delta^1$) in each connection point of two arcs (see below) there is a common tangent (as a consequence, it will be possible to define a $C^1$ parameterization). Question: is there a simple mean to establish this ""smoothness"" ? My work: I have established this smoothness explicitly by first computing the equations of the different constituent arcs of $(E)$. then for each connecting point (such as $L, N, I$...), by computing the left and right derivative (when it exists) and checking that their values are equal or checking the presence of a vertical tangent. Due to $Ox$ and $Oy$ symmetries, it suffices to describe the first quadrant part of (E), or, in an equivalent form, the different parts of arc MLNIS. Here are the equations of the constituent arcs: $$\begin{cases} \text{Arc ML:}\ &\text{line segment:} \ \ & x=10 & -1 \leq y \leq 1\\ \text{Arc LN:}\ &\text{ellipse with foci B and C:} \ \ & \frac{(x-1)^2}{81}+\frac{(y-1)^2}{32}=1& 8 \leq x \leq 10, 1 \leq y \leq \frac{41}{9}\\ \text{Arc NI:}\ &\text{parabola:} \ \ & y=- \frac{1}{36}(x+6)^2+10& 6 \leq x \leq 8, \frac{41}{9} \leq y \leq 6\\ \text{Arc IS:}\ &\text{ellipse with foci B and A:} \ \ & \frac{x^2}{81}+\frac{(y-1)^2}{45}=1& -6 \leq x \leq 6, 6 \leq y \leq 8 \end{cases}$$ But of course, it is very tedious... Moreover I would like to generalize this study to ""generalized foci"" that could be any polygonal shape, and even any convex curve.",,"['calculus', 'geometry', 'conic-sections', 'convex-geometry']"
67,How to prove $A\cos(\omega t-\phi)$ = $a\cos(\omega t)$ + $b\sin(\omega t)$ using $e^{i\theta}$?,How to prove  =  +  using ?,A\cos(\omega t-\phi) a\cos(\omega t) b\sin(\omega t) e^{i\theta},"I want to show that $A\cos\left(\omega t-\phi\right)$ = $a\cos\left(\omega t\right)$ + $b\sin\left(\omega t\right)$ First I verified for myself through the angle addition proof that: $$ \cos\left(\omega t+ \phi\right) = \cos\left(\omega t\right)\cos\left(\phi\right) - \sin\left(\omega t\right)\sin\left(\phi\right) $$ and that $$ \cos\left(\omega t - \phi\right) = \cos\left(\omega t\right)\cos\left(-\phi\right) - \sin\left(\omega t\right)\sin\left(-\phi\right) $$ therefore I am able to show that: $$ \cos\left(\omega t - \phi\right) = \cos\left(\omega t\right)\cos\left(\phi\right) + \sin\left(\omega t\right)\sin\left(\phi\right) $$ as cos is an even function and sine is an odd function. I then multiply through by A: $$ A\cos\left(\omega t - \phi\right) = A\cos\left(\omega t\right)\cos\left(\phi\right) + A\sin\left(\omega t\right)\sin\left(\phi\right) $$ so that there are constants:  $ A\cos\left(\phi\right) = a $ and $ A\sin\left(\phi\right) = b $ which gives the function: $A\cos\left(\omega t-\phi\right)$ = $a\cos\left(\omega t\right)$ + $b\sin\left(\omega t\right)$ used to model sinusoidal solutions of differential equations. If we take the case that $\phi = 0$ then there is no phase lag and: $A\cos\left(\omega t\right)$ = $A\cos\left(\omega t\right)$ with no $\sin\left(\phi\right)$ term, which is bizarre to me because it seems that if $\phi \neq 0 $ the equation $A\cos\left(\omega t-\phi\right)$ can be broken into  $a\cos\left(\omega t\right)$ + $b\sin\left(\omega t\right)$ whereas if $\phi = 0 $ the input of $\sin $ is ""lost"". Why can't $\cos\left( \omega t\right) $ always be decomposed to $\cos\left( \omega t\right) $  and $\sin\left( \omega t\right) $, I know that for $t = 0$ it is simply because $\sin\left( \omega t\right)  = 0$, but what about when $\omega \neq 0$, why can we not describe  $\cos\left( \omega t\right) $ in terms of $\cos\left( \omega t\right) $  and $\sin\left( \omega t\right) $? What are some better proofs or correct/more rigorous proofs of the existence of this relationship? i.e. there any proof using the $e^{i\theta} $ and complex numbers? Is there a way to relate amplitude $A$ to $ \phi, \omega $, and/or $t$, or is amplitude truly independent of the parameters and variables of any function of this type?","I want to show that $A\cos\left(\omega t-\phi\right)$ = $a\cos\left(\omega t\right)$ + $b\sin\left(\omega t\right)$ First I verified for myself through the angle addition proof that: $$ \cos\left(\omega t+ \phi\right) = \cos\left(\omega t\right)\cos\left(\phi\right) - \sin\left(\omega t\right)\sin\left(\phi\right) $$ and that $$ \cos\left(\omega t - \phi\right) = \cos\left(\omega t\right)\cos\left(-\phi\right) - \sin\left(\omega t\right)\sin\left(-\phi\right) $$ therefore I am able to show that: $$ \cos\left(\omega t - \phi\right) = \cos\left(\omega t\right)\cos\left(\phi\right) + \sin\left(\omega t\right)\sin\left(\phi\right) $$ as cos is an even function and sine is an odd function. I then multiply through by A: $$ A\cos\left(\omega t - \phi\right) = A\cos\left(\omega t\right)\cos\left(\phi\right) + A\sin\left(\omega t\right)\sin\left(\phi\right) $$ so that there are constants:  $ A\cos\left(\phi\right) = a $ and $ A\sin\left(\phi\right) = b $ which gives the function: $A\cos\left(\omega t-\phi\right)$ = $a\cos\left(\omega t\right)$ + $b\sin\left(\omega t\right)$ used to model sinusoidal solutions of differential equations. If we take the case that $\phi = 0$ then there is no phase lag and: $A\cos\left(\omega t\right)$ = $A\cos\left(\omega t\right)$ with no $\sin\left(\phi\right)$ term, which is bizarre to me because it seems that if $\phi \neq 0 $ the equation $A\cos\left(\omega t-\phi\right)$ can be broken into  $a\cos\left(\omega t\right)$ + $b\sin\left(\omega t\right)$ whereas if $\phi = 0 $ the input of $\sin $ is ""lost"". Why can't $\cos\left( \omega t\right) $ always be decomposed to $\cos\left( \omega t\right) $  and $\sin\left( \omega t\right) $, I know that for $t = 0$ it is simply because $\sin\left( \omega t\right)  = 0$, but what about when $\omega \neq 0$, why can we not describe  $\cos\left( \omega t\right) $ in terms of $\cos\left( \omega t\right) $  and $\sin\left( \omega t\right) $? What are some better proofs or correct/more rigorous proofs of the existence of this relationship? i.e. there any proof using the $e^{i\theta} $ and complex numbers? Is there a way to relate amplitude $A$ to $ \phi, \omega $, and/or $t$, or is amplitude truly independent of the parameters and variables of any function of this type?",,"['calculus', 'ordinary-differential-equations', 'applications']"
68,What should $\int \frac{1}{x} dx$ equal to?,What should  equal to?,\int \frac{1}{x} dx,"Before you say that $\int \frac{1}{x} dx$ is equal to $\ln|x| +C$ due to positve and negative, I would like to show you why it is not convincing to me. Problem 1 and its possible solution. Evaluate $$ \begin{equation} \sum_{n=1}^\infty \frac{\sin(n)}{n} \end{equation} $$ From infinite geometric series \begin{equation} \sum_{n=1}^\infty x^{n-1}=\frac{1}{1-x} ;|x|<1 \end{equation} Integrating this with respect to $x$ we would get (Constant vanishes due to $x=0$) \begin{equation} \sum_{n=1}^\infty \frac{x^n}{n}=-\ln|1-x| \end{equation} So $\sum_{n=1}^\infty \frac{\sin(n)}{n}$ is just an imaginary part of \begin{equation} \sum_{n=1}^\infty \frac{z^n}{n}=-\ln|1-z| \end{equation} Where $z=e^i$ But the right hand side has no imaginary part at all, but the summation is clearly exists, this seems to suggest that the integral is equal to $\ln(x) +C$ There is another curious way to evaluate integral on negative reals if we only consider only principal values. Problem 2 and its possible solution. Evaluate \begin{equation} \int_{-4}^{-2} \frac{1}{x} dx \end{equation} If we give that $\int \frac{1}{x} dx=\ln(x) +C $ then the integral is \begin{equation} \int_{-4}^{-2} \frac{1}{x} dx=\ln(-2)-\ln(-4) \end{equation} Using principal values we will get \begin{equation} \ln(-2)-\ln(-4)=\ln(2)+i\pi-\ln(4)-i\pi=-\ln(2) \end{equation} Which is exactly equal to when we use $\int \frac{1}{x} dx=\ln|x|+C$ These 2 problems are the reasons why the result $\ln |x| +C$ not convincing but there might be flaws in the proposed solutions. If there is a flaws please explain them too.","Before you say that $\int \frac{1}{x} dx$ is equal to $\ln|x| +C$ due to positve and negative, I would like to show you why it is not convincing to me. Problem 1 and its possible solution. Evaluate $$ \begin{equation} \sum_{n=1}^\infty \frac{\sin(n)}{n} \end{equation} $$ From infinite geometric series \begin{equation} \sum_{n=1}^\infty x^{n-1}=\frac{1}{1-x} ;|x|<1 \end{equation} Integrating this with respect to $x$ we would get (Constant vanishes due to $x=0$) \begin{equation} \sum_{n=1}^\infty \frac{x^n}{n}=-\ln|1-x| \end{equation} So $\sum_{n=1}^\infty \frac{\sin(n)}{n}$ is just an imaginary part of \begin{equation} \sum_{n=1}^\infty \frac{z^n}{n}=-\ln|1-z| \end{equation} Where $z=e^i$ But the right hand side has no imaginary part at all, but the summation is clearly exists, this seems to suggest that the integral is equal to $\ln(x) +C$ There is another curious way to evaluate integral on negative reals if we only consider only principal values. Problem 2 and its possible solution. Evaluate \begin{equation} \int_{-4}^{-2} \frac{1}{x} dx \end{equation} If we give that $\int \frac{1}{x} dx=\ln(x) +C $ then the integral is \begin{equation} \int_{-4}^{-2} \frac{1}{x} dx=\ln(-2)-\ln(-4) \end{equation} Using principal values we will get \begin{equation} \ln(-2)-\ln(-4)=\ln(2)+i\pi-\ln(4)-i\pi=-\ln(2) \end{equation} Which is exactly equal to when we use $\int \frac{1}{x} dx=\ln|x|+C$ These 2 problems are the reasons why the result $\ln |x| +C$ not convincing but there might be flaws in the proposed solutions. If there is a flaws please explain them too.",,"['calculus', 'integration', 'complex-numbers', 'logarithms']"
69,Prove that sum is convergent,Prove that sum is convergent,,How to prove that the following sum is convergent? $$\sum_1^\infty\frac{\sin(n + \ln{n})}{n}$$ I tried to use formula $$\sin(n+ \ln{n}) = \sin{n}\cos \ln{n} + \sin \ln{n}\cos{n}$$ and $$\sum_1^N \sin{n} \leq \frac{1}{\sin{1/2}}$$ But I can't make same estimates for $\sin{n}\cos \ln{n} $ and $\sin \ln{n} \cos{n}$.,How to prove that the following sum is convergent? $$\sum_1^\infty\frac{\sin(n + \ln{n})}{n}$$ I tried to use formula $$\sin(n+ \ln{n}) = \sin{n}\cos \ln{n} + \sin \ln{n}\cos{n}$$ and $$\sum_1^N \sin{n} \leq \frac{1}{\sin{1/2}}$$ But I can't make same estimates for $\sin{n}\cos \ln{n} $ and $\sin \ln{n} \cos{n}$.,,"['calculus', 'sequences-and-series', 'convergence-divergence', 'summation']"
70,Dividing both sides by $y(x)$ when solving separable differential equations,Dividing both sides by  when solving separable differential equations,y(x),"Consider, for example, the differential equation $$\frac{dy(x)}{dx} = y(x)$$ This is generally solved as follows $$\frac{dy(x)}{dx} = y(x) \Longleftrightarrow \frac{1}{y(x)} \frac{dy(x)}{dx}= 1 \Longleftrightarrow \int \left( \frac{1}{y(x)} \frac{dy(x)}{dx}\right) dx = \int dx \Longleftrightarrow \log|y(x)| = x+C_1 \Longleftrightarrow y(x) = C_2\exp(x)$$ In the first step, why are we allowed to divide both sides by $y(x)$? We are making the a priori assumption that $y(x) \neq 0$ for all $x$. In other words, the above argument holds only if we assume that $y(x)$ vanishes nowhere. What if there are solutions where $y(c) = 0$ for some $c$?  In fact, what if there are solutions where $y(c) = 0$ and $y$ is not the zero function? Of course, there are other ways to prove that $C\exp(x)$ uniquely satisfies the equation, but this was merely an example: Why are we allowed to do this in general when solving separable ODEs?","Consider, for example, the differential equation $$\frac{dy(x)}{dx} = y(x)$$ This is generally solved as follows $$\frac{dy(x)}{dx} = y(x) \Longleftrightarrow \frac{1}{y(x)} \frac{dy(x)}{dx}= 1 \Longleftrightarrow \int \left( \frac{1}{y(x)} \frac{dy(x)}{dx}\right) dx = \int dx \Longleftrightarrow \log|y(x)| = x+C_1 \Longleftrightarrow y(x) = C_2\exp(x)$$ In the first step, why are we allowed to divide both sides by $y(x)$? We are making the a priori assumption that $y(x) \neq 0$ for all $x$. In other words, the above argument holds only if we assume that $y(x)$ vanishes nowhere. What if there are solutions where $y(c) = 0$ for some $c$?  In fact, what if there are solutions where $y(c) = 0$ and $y$ is not the zero function? Of course, there are other ways to prove that $C\exp(x)$ uniquely satisfies the equation, but this was merely an example: Why are we allowed to do this in general when solving separable ODEs?",,"['calculus', 'ordinary-differential-equations']"
71,Does rationality of $\cosh(nx)$ and $\cosh((n+1)x)$ imply rationality of $\cosh(x)$?,Does rationality of  and  imply rationality of ?,\cosh(nx) \cosh((n+1)x) \cosh(x),"Suppose that $x\in\mathbb{R}^+$ and $n\in \mathbb{N}$. If $\cosh(nx)$ and $\cosh((n+1)x)$ are rational, can we show that $\cosh(x)$ is rational too? I guess the following equalities should be useful: $$\begin{eqnarray} \cosh(x) &=& \cosh((n+1)x−xn)\\ &=& \cosh((n+1)n).\cosh(nx) − \sinh((n+1)x).\sinh(nx)\\ &=& \cosh((n+1)n).\cosh(nx) − \sqrt{\cosh^2((n+1)x) − 1}.\sqrt{\cosh^2(nx) − 1}. \end{eqnarray}$$","Suppose that $x\in\mathbb{R}^+$ and $n\in \mathbb{N}$. If $\cosh(nx)$ and $\cosh((n+1)x)$ are rational, can we show that $\cosh(x)$ is rational too? I guess the following equalities should be useful: $$\begin{eqnarray} \cosh(x) &=& \cosh((n+1)x−xn)\\ &=& \cosh((n+1)n).\cosh(nx) − \sinh((n+1)x).\sinh(nx)\\ &=& \cosh((n+1)n).\cosh(nx) − \sqrt{\cosh^2((n+1)x) − 1}.\sqrt{\cosh^2(nx) − 1}. \end{eqnarray}$$",,"['calculus', 'elementary-number-theory', 'trigonometry']"
72,Convergence of the series $\sum_n (-1)^{\lfloor \sqrt{n-1}\rfloor} \frac 1 n$,Convergence of the series,\sum_n (-1)^{\lfloor \sqrt{n-1}\rfloor} \frac 1 n,"Let's consider the series $\sum a_n$, with $$a_n = (-1)^{\lfloor \sqrt{n-1}\rfloor} \frac 1 n$$ It looks absolutely like an example for the Leibniz test but here the signs don't interchange one for one. How can we prove the convergence of this series?","Let's consider the series $\sum a_n$, with $$a_n = (-1)^{\lfloor \sqrt{n-1}\rfloor} \frac 1 n$$ It looks absolutely like an example for the Leibniz test but here the signs don't interchange one for one. How can we prove the convergence of this series?",,"['calculus', 'sequences-and-series']"
73,"Can we create an ""integration by parts"" with quotient rule?","Can we create an ""integration by parts"" with quotient rule?",,"Product rule says that $(uv)' = u'v + uv'$, so $\int (uv)' = \int (u'v + uv')$ implies $uv = \int u'v + \int uv'$ and this implies $$\int uv' ~dx = uv - \int u'v ~dx$$ This is integration by parts. I am wondering if this also works with quotient rule: $(u/v)' = \frac{vu' - uv'}{v^2}$ so $\int(u/v)' = \int\frac{vu' - uv'}{v^2}$ implies $u/v = \int\frac{vu'}{v^2} - \int\frac{uv'}{v^2}$ and also $$\int\frac{u'}{v} ~ dx = u/v +  \int\frac{uv'}{v^2} ~ dx$$ I am not sure if this relationship would have any uses but would it be a valid method?","Product rule says that $(uv)' = u'v + uv'$, so $\int (uv)' = \int (u'v + uv')$ implies $uv = \int u'v + \int uv'$ and this implies $$\int uv' ~dx = uv - \int u'v ~dx$$ This is integration by parts. I am wondering if this also works with quotient rule: $(u/v)' = \frac{vu' - uv'}{v^2}$ so $\int(u/v)' = \int\frac{vu' - uv'}{v^2}$ implies $u/v = \int\frac{vu'}{v^2} - \int\frac{uv'}{v^2}$ and also $$\int\frac{u'}{v} ~ dx = u/v +  \int\frac{uv'}{v^2} ~ dx$$ I am not sure if this relationship would have any uses but would it be a valid method?",,"['calculus', 'integration', 'soft-question']"
74,Find the minimum roots of $f'(x)\cdot f'''(x)+(f''(x))^2 =0$ given certain conditions on $f(x)$.,Find the minimum roots of  given certain conditions on .,f'(x)\cdot f'''(x)+(f''(x))^2 =0 f(x),"Problem: Let $f(x)$ be a thrice differentiable function satisfying: $$|f(x) - f(4-x)| + |f(4-x)-f(4+x)| = 0, \forall x \in R$$ If $f'(1)=0$, then find the minimum number of roots of $f'(x)\cdot f'''(x)+(f''(x))^2 =0$, on $x \in [0,6]$ My attempt: We know: $f(x)=f(4-x)$ and $f(4-x)=f(4+x)$ So, $f(x)=f(x+4)$. That is, the period of the given function is $4$. It can also be noted that the function is symmetric about $2$ and $4$. I also know that the second equation is nothing but $\frac{d}{dx}(f'(x) \cdot f''(x))$ I don't know how to proceed from here.","Problem: Let $f(x)$ be a thrice differentiable function satisfying: $$|f(x) - f(4-x)| + |f(4-x)-f(4+x)| = 0, \forall x \in R$$ If $f'(1)=0$, then find the minimum number of roots of $f'(x)\cdot f'''(x)+(f''(x))^2 =0$, on $x \in [0,6]$ My attempt: We know: $f(x)=f(4-x)$ and $f(4-x)=f(4+x)$ So, $f(x)=f(x+4)$. That is, the period of the given function is $4$. It can also be noted that the function is symmetric about $2$ and $4$. I also know that the second equation is nothing but $\frac{d}{dx}(f'(x) \cdot f''(x))$ I don't know how to proceed from here.",,"['calculus', 'functions', 'contest-math']"
75,Calculate Fourier transform of $V(r)=\frac{-g^2}{4\pi|r|}e^{-\mu|r|}$ in Quantum Field Theory,Calculate Fourier transform of  in Quantum Field Theory,V(r)=\frac{-g^2}{4\pi|r|}e^{-\mu|r|},"(This is purely for personal study - the exercise is 20.2(a) from Lancaster and Blundell (2014), Oxford Uni. Press - an excellent textbook btw.) ""Confirm that the Fourier transform of $V(\underline{r})=\frac{-g^2}{4\pi|\underline{r}|}e^{-\mu|\underline{r}|}$ is $\tilde{V}(\underline{q})=\frac{-g^2}{|\underline{q}|^2+\mu^2}$."" The definition of the 3D transform (equation 22) is $\tilde{f}(\underline{k})=\int d^3x\,\,e^{-i\underline{k}.\underline{x}}f(\underline{x})$; therefore using spherical co-ordinates, I can get as far as $\tilde{V}(\underline{q})=\int_0^\infty dr\,|\underline{r}|^2\frac{-g^2}{4\pi|\underline{r}|}e^{-\mu|\underline{r}|}e^{i|\underline{q}||\underline{r}|}\int_0^\pi \sin\theta d\theta\int_0^{2\pi}d\phi =4\pi\int_0^\infty dr\,|\underline{r}|\frac{-g^2}{4\pi}e^{-\mu|\underline{r}|+i|\underline{q}||\underline{r}|}\\ =-g^2\int_0^\infty dr\,|\underline{r}|e^{-(\mu-i|\underline{q}|)|\underline{r}|}.$ However, this integral appears to yield $\frac{-1}{\mu-i|\underline{q}|}$, for constant $\mu$ and $q$, since (with $Z_q=\mu-i|\underline{q}|$) $-g^2\int_0^\infty dr\,|\underline{r}|e^{-Z_q|\underline{r}|} =Z_q^{-1}\lim(c\rightarrow\infty)[(1-c)e^{-Z_q c}-1]$, and the first term vanishes in the limit (due to L'Hopital's rule). One last comment is that I have had a sneaking suspicion this is somehow related to Laplace transforms, a) because the formula for $\tilde{V}(\underline{q})$ is so very Laplace-looking, and b) the Laplace formula itself containing a factor of $\exp(-st)$ (where $s$ is the integration variable). If someone could just please tell me what I am missing I'd be grateful!","(This is purely for personal study - the exercise is 20.2(a) from Lancaster and Blundell (2014), Oxford Uni. Press - an excellent textbook btw.) ""Confirm that the Fourier transform of $V(\underline{r})=\frac{-g^2}{4\pi|\underline{r}|}e^{-\mu|\underline{r}|}$ is $\tilde{V}(\underline{q})=\frac{-g^2}{|\underline{q}|^2+\mu^2}$."" The definition of the 3D transform (equation 22) is $\tilde{f}(\underline{k})=\int d^3x\,\,e^{-i\underline{k}.\underline{x}}f(\underline{x})$; therefore using spherical co-ordinates, I can get as far as $\tilde{V}(\underline{q})=\int_0^\infty dr\,|\underline{r}|^2\frac{-g^2}{4\pi|\underline{r}|}e^{-\mu|\underline{r}|}e^{i|\underline{q}||\underline{r}|}\int_0^\pi \sin\theta d\theta\int_0^{2\pi}d\phi =4\pi\int_0^\infty dr\,|\underline{r}|\frac{-g^2}{4\pi}e^{-\mu|\underline{r}|+i|\underline{q}||\underline{r}|}\\ =-g^2\int_0^\infty dr\,|\underline{r}|e^{-(\mu-i|\underline{q}|)|\underline{r}|}.$ However, this integral appears to yield $\frac{-1}{\mu-i|\underline{q}|}$, for constant $\mu$ and $q$, since (with $Z_q=\mu-i|\underline{q}|$) $-g^2\int_0^\infty dr\,|\underline{r}|e^{-Z_q|\underline{r}|} =Z_q^{-1}\lim(c\rightarrow\infty)[(1-c)e^{-Z_q c}-1]$, and the first term vanishes in the limit (due to L'Hopital's rule). One last comment is that I have had a sneaking suspicion this is somehow related to Laplace transforms, a) because the formula for $\tilde{V}(\underline{q})$ is so very Laplace-looking, and b) the Laplace formula itself containing a factor of $\exp(-st)$ (where $s$ is the integration variable). If someone could just please tell me what I am missing I'd be grateful!",,"['calculus', 'fourier-transform', 'quantum-field-theory']"
76,"Are partial derivatives always commutative? When is $\frac{\partial^2}{ \partial x\partial y}f(x,y)\neq\frac{\partial^2}{\partial y\partial x}f(x,y)$?",Are partial derivatives always commutative? When is ?,"\frac{\partial^2}{ \partial x\partial y}f(x,y)\neq\frac{\partial^2}{\partial y\partial x}f(x,y)","I learned in my Calculus 3 class that $\frac{\partial}{\partial x}\left(\frac{\partial}{\partial y}f(x,y)\right) = \frac{\partial}{\partial y}\left(\frac{\partial}{\partial x}f(x,y)\right)$ Are there any counter examples to this?","I learned in my Calculus 3 class that $\frac{\partial}{\partial x}\left(\frac{\partial}{\partial y}f(x,y)\right) = \frac{\partial}{\partial y}\left(\frac{\partial}{\partial x}f(x,y)\right)$ Are there any counter examples to this?",,"['calculus', 'examples-counterexamples']"
77,"Integral ${\large\int}_0^{\pi/2}\frac{\sin\left(x-a\ln2\cdot\tan x\right)}{\left(e^{2\pi a\tan x}-1\right)\cdot\cos x}\,dx$",Integral,"{\large\int}_0^{\pi/2}\frac{\sin\left(x-a\ln2\cdot\tan x\right)}{\left(e^{2\pi a\tan x}-1\right)\cdot\cos x}\,dx","Analyzing results Mathematica returned for several integrals and series, and putting pieces together, I came up with this conjecture: $$\int_0^{\pi/2}\frac{\sin\left(x-a\ln2\cdot\tan x\right)}{\left(e^{2\pi a\tan x}-1\right)\cdot\cos x}\,dx\stackrel{\color{gray}?}=\frac{\operatorname{li}\left(2^a\right)}{2^{a+1}}+\frac{2\,a-1+a\cdot\Re\,{_2F_1}(1,1;a;2)}{4\,a\,(1-a)}\color{gray}{,\,a>0\land a\ne1 }$$ Can we prove this result? It is possible to generalize it to complex values of the parameter $a$?","Analyzing results Mathematica returned for several integrals and series, and putting pieces together, I came up with this conjecture: $$\int_0^{\pi/2}\frac{\sin\left(x-a\ln2\cdot\tan x\right)}{\left(e^{2\pi a\tan x}-1\right)\cdot\cos x}\,dx\stackrel{\color{gray}?}=\frac{\operatorname{li}\left(2^a\right)}{2^{a+1}}+\frac{2\,a-1+a\cdot\Re\,{_2F_1}(1,1;a;2)}{4\,a\,(1-a)}\color{gray}{,\,a>0\land a\ne1 }$$ Can we prove this result? It is possible to generalize it to complex values of the parameter $a$?",,"['calculus', 'integration', 'definite-integrals', 'hypergeometric-function', 'trigonometry']"
78,Inverse Laplace transform of one complicated function,Inverse Laplace transform of one complicated function,,"I want to ask the inverse Laplace transform of the following function: $$F(s) = \frac{1}{s \cdot (1 + a \cdot s)^{m} \cdot (1 + b \cdot s)^{m-k}} \cdot \Bigl[\exp{(\frac{- c \cdot s}{ 1 + b \cdot s } )}\Bigr]^{m-k}$$ where $a,b,c,m,k$ are all positive constants. $m$ and $k$ are integers with $m\ge k$. It is possible to get an approximate solution using the Euler summation-based technique. But I want to have an exact solution for the inverse transform. Some similar questions but in a much less simple form are also found on Stackexchane, i.e., Inverse Laplace Transform of e$^{-c \sqrt{s}}/(\sqrt{s}(a - s))$ or find the inverse Laplace transform of complex function . Especially I am wondering how to deal with the point $s = \frac{1}{b}$ since it is a pole of order $(m-k)$ but also appears in the exponential function and also I am not sure about the contour integral considering that $s$ exists in both the nominator and denominator of the argument of the exponential function. Thanks a lot.","I want to ask the inverse Laplace transform of the following function: $$F(s) = \frac{1}{s \cdot (1 + a \cdot s)^{m} \cdot (1 + b \cdot s)^{m-k}} \cdot \Bigl[\exp{(\frac{- c \cdot s}{ 1 + b \cdot s } )}\Bigr]^{m-k}$$ where $a,b,c,m,k$ are all positive constants. $m$ and $k$ are integers with $m\ge k$. It is possible to get an approximate solution using the Euler summation-based technique. But I want to have an exact solution for the inverse transform. Some similar questions but in a much less simple form are also found on Stackexchane, i.e., Inverse Laplace Transform of e$^{-c \sqrt{s}}/(\sqrt{s}(a - s))$ or find the inverse Laplace transform of complex function . Especially I am wondering how to deal with the point $s = \frac{1}{b}$ since it is a pole of order $(m-k)$ but also appears in the exponential function and also I am not sure about the contour integral considering that $s$ exists in both the nominator and denominator of the argument of the exponential function. Thanks a lot.",,"['calculus', 'complex-analysis', 'inverse', 'laplace-transform', 'contour-integration']"
79,Calculation of the convolution of Cauchy density function $\int_{-\infty}^{\infty}\frac{ab}{\pi^2}\frac{1}{y^2+a^2}\frac{1}{(x-y)^2+b^2}dy$,Calculation of the convolution of Cauchy density function,\int_{-\infty}^{\infty}\frac{ab}{\pi^2}\frac{1}{y^2+a^2}\frac{1}{(x-y)^2+b^2}dy,"I tried to calculate the following integral, which is the convolution of Cauchy density function: $$\int_{-\infty}^{\infty}\frac{ab}{\pi^2}\frac{1}{y^2+a^2}\frac{1}{(x-y)^2+b^2}dy$$ I tried to use substitution, let $x-y=t$, then $y=x-t$ So  $$\int_{-\infty}^{\infty}\frac{ab}{\pi^2}\frac{1}{(x-t)^2+a^2}\frac{1}{t^2+b^2}dt$$ But it is still hard to solve. Could someone kindly provide some help? Thanks!","I tried to calculate the following integral, which is the convolution of Cauchy density function: $$\int_{-\infty}^{\infty}\frac{ab}{\pi^2}\frac{1}{y^2+a^2}\frac{1}{(x-y)^2+b^2}dy$$ I tried to use substitution, let $x-y=t$, then $y=x-t$ So  $$\int_{-\infty}^{\infty}\frac{ab}{\pi^2}\frac{1}{(x-t)^2+a^2}\frac{1}{t^2+b^2}dt$$ But it is still hard to solve. Could someone kindly provide some help? Thanks!",,"['calculus', 'probability', 'convolution']"
80,Line for set of three-dimensional vectors,Line for set of three-dimensional vectors,,"If there is a set for 3D vectors $v$ where $ v \times \begin{pmatrix} -1 \\ 1 \\ 4 \end{pmatrix} = \begin{pmatrix} 5 \\ -27 \\ 8 \end{pmatrix}$ is a line, what is this line's equation? I'm not sure how to solve this, except for setting $v = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ and maybe having a linear system of equations, but I don't see how I could use that to solve the problem.","If there is a set for 3D vectors $v$ where $ v \times \begin{pmatrix} -1 \\ 1 \\ 4 \end{pmatrix} = \begin{pmatrix} 5 \\ -27 \\ 8 \end{pmatrix}$ is a line, what is this line's equation? I'm not sure how to solve this, except for setting $v = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ and maybe having a linear system of equations, but I don't see how I could use that to solve the problem.",,"['calculus', 'matrices', 'algebra-precalculus', 'trigonometry', 'vectors']"
81,"For which values of $p,q$ does the integral $\int_0^1 x^p (\ln\frac{1}{x})^qdx$ converge?",For which values of  does the integral  converge?,"p,q \int_0^1 x^p (\ln\frac{1}{x})^qdx","For which values of $p,q$ does the integral $\int_0^1 x^p (\ln\frac{1}{x})^qdx$ converge? I use the substitution $t=1/x$ to obtain this better looking integral: $\int_1^\infty \frac{(\ln t)^q}{t^{p+2}}$ . Integration by parts gives me a recurence formula for the integral, but $p,q$ are not necessarily integers so I don't think that's the right approach.","For which values of does the integral converge? I use the substitution to obtain this better looking integral: . Integration by parts gives me a recurence formula for the integral, but are not necessarily integers so I don't think that's the right approach.","p,q \int_0^1 x^p (\ln\frac{1}{x})^qdx t=1/x \int_1^\infty \frac{(\ln t)^q}{t^{p+2}} p,q","['calculus', 'improper-integrals']"
82,Defining the area under an oscillating function,Defining the area under an oscillating function,,"I was curious about taking a definite integral of an oscillating function. For example, $$\lim_{a\to 0} \int_a^1 \sin \frac1x\,dx$$ I know that there is some area under the function, but since it oscillates infinitely is it possible to define it? Do you use the limit superior and the limit inferior? I know it is probably possible (somehow) to take the antiderivative, FTC, etc. but I am wondering what this really intuitively means, given that this function oscillates (so we can't really ""see"" the area under the function).","I was curious about taking a definite integral of an oscillating function. For example, $$\lim_{a\to 0} \int_a^1 \sin \frac1x\,dx$$ I know that there is some area under the function, but since it oscillates infinitely is it possible to define it? Do you use the limit superior and the limit inferior? I know it is probably possible (somehow) to take the antiderivative, FTC, etc. but I am wondering what this really intuitively means, given that this function oscillates (so we can't really ""see"" the area under the function).",,"['calculus', 'definite-integrals']"
83,Question about Geometric-Harmonic Mean.,Question about Geometric-Harmonic Mean.,,"Define our Harmonic sequence for two numbers such that \begin{equation} a_{n+1} = \frac{2a_nb_n}{a_n + b_n} \end{equation} and our geometric sequence  \begin{equation}b_{n+1} = \sqrt{a_nb_n} \end{equation} such that as $n \rightarrow \infty$ we tend towards the Geometric-Harmonic Mean. The arithmetic-geometric mean can be defined by the following two sequences. First compute the arithmetic mean of two numbers $a,b \in  \mathbb{R_+}$ such that $a_1$ = $\frac{1}{2}(a + b)$ and then compute the geometric mean such that $b_1$ = $\sqrt{ab}$. If we continue to iterate this operation we can define our two sequences {$a_n$} and {$b_n$} as follows:  \begin{gather*} a_{n+1} = \frac{1}{2}(a_n + b_n) \\b_{n+1} = \sqrt{{(a_nb_n)}}. %Try and get this all under 1 square root sign \end{gather*}  As ${n\to\infty}$ we approach our arithmetic-geometric mean (agM). I have proved that the Geometric-Harmonic Mean exists and that $a_n$ = $b_n$ as $n \rightarrow \infty$. However, Mathworld states without proof that the limit for our Geometric-Harmonic mean can be written as \begin{equation*} \lim_{n \to \infty}a_n = \frac{1}{M({a_n}^{-1},{b_n}^{-1})} (*) \end{equation*} Where M is the arithmetic geometric mean.  You can find the link here: http://mathworld.wolfram.com/Harmonic-GeometricMean.html Can anyone help me prove (*)?","Define our Harmonic sequence for two numbers such that \begin{equation} a_{n+1} = \frac{2a_nb_n}{a_n + b_n} \end{equation} and our geometric sequence  \begin{equation}b_{n+1} = \sqrt{a_nb_n} \end{equation} such that as $n \rightarrow \infty$ we tend towards the Geometric-Harmonic Mean. The arithmetic-geometric mean can be defined by the following two sequences. First compute the arithmetic mean of two numbers $a,b \in  \mathbb{R_+}$ such that $a_1$ = $\frac{1}{2}(a + b)$ and then compute the geometric mean such that $b_1$ = $\sqrt{ab}$. If we continue to iterate this operation we can define our two sequences {$a_n$} and {$b_n$} as follows:  \begin{gather*} a_{n+1} = \frac{1}{2}(a_n + b_n) \\b_{n+1} = \sqrt{{(a_nb_n)}}. %Try and get this all under 1 square root sign \end{gather*}  As ${n\to\infty}$ we approach our arithmetic-geometric mean (agM). I have proved that the Geometric-Harmonic Mean exists and that $a_n$ = $b_n$ as $n \rightarrow \infty$. However, Mathworld states without proof that the limit for our Geometric-Harmonic mean can be written as \begin{equation*} \lim_{n \to \infty}a_n = \frac{1}{M({a_n}^{-1},{b_n}^{-1})} (*) \end{equation*} Where M is the arithmetic geometric mean.  You can find the link here: http://mathworld.wolfram.com/Harmonic-GeometricMean.html Can anyone help me prove (*)?",,"['calculus', 'analysis', 'optimization', 'means']"
84,"Evaluate $\int_0^1 \frac1 {x^2+2x+3}\,\mathrm dx$",Evaluate,"\int_0^1 \frac1 {x^2+2x+3}\,\mathrm dx","I first completed the square: $$\int_0^1 \frac1 {2+(x+1)^2}\,\mathrm dx$$ Made the substitution $x+1=\sqrt2 \tan u$. Thus $dx=\sqrt2\sec^2udu$ substituting this in and changing the limits (please check that I have done this bit right). $$\frac{\sqrt2}{2}\int_{\tan^{-1}\frac1{\sqrt2}}^{\tan^{-1}\frac2{\sqrt2}}\,\mathrm du$$ after using the identity $\tan^2u+1=\sec^2u$ and factoring out the constants. Clearly this leads to a horrible result with many decimal places. Since this is meant to be done without a calculator, I suspect that I have done something wrong. Please help.","I first completed the square: $$\int_0^1 \frac1 {2+(x+1)^2}\,\mathrm dx$$ Made the substitution $x+1=\sqrt2 \tan u$. Thus $dx=\sqrt2\sec^2udu$ substituting this in and changing the limits (please check that I have done this bit right). $$\frac{\sqrt2}{2}\int_{\tan^{-1}\frac1{\sqrt2}}^{\tan^{-1}\frac2{\sqrt2}}\,\mathrm du$$ after using the identity $\tan^2u+1=\sec^2u$ and factoring out the constants. Clearly this leads to a horrible result with many decimal places. Since this is meant to be done without a calculator, I suspect that I have done something wrong. Please help.",,"['calculus', 'integration', 'definite-integrals']"
85,Derivative of a norm vs norm of a derivative,Derivative of a norm vs norm of a derivative,,"Consider a vector-valued function of the time, say $$v: \tau\in\mathbb{R}\to\mathbb{R}_N.$$ Suppose that for $\tau=t$, the function is equal to the zero vector, i.e. $$v(t)=0_N.$$ Denote as $\alpha\in\mathbb{R}_N$ the value of the time derivative of $v$ in $\tau=t$, i.e. $$\dot{v}(t)=\frac{d}{d\tau}v(\tau)|_{\tau=t}=\alpha.$$ Of course we can write that $$||\dot{v}(t)||=||\frac{d}{d\tau}v(\tau)|_{\tau=t}||=||\alpha||.$$ But I wonder if I can say that $$(\frac{d}{d\tau}||v(\tau)||)_{\tau=t} \le ||\alpha||.$$ And if so, why?","Consider a vector-valued function of the time, say $$v: \tau\in\mathbb{R}\to\mathbb{R}_N.$$ Suppose that for $\tau=t$, the function is equal to the zero vector, i.e. $$v(t)=0_N.$$ Denote as $\alpha\in\mathbb{R}_N$ the value of the time derivative of $v$ in $\tau=t$, i.e. $$\dot{v}(t)=\frac{d}{d\tau}v(\tau)|_{\tau=t}=\alpha.$$ Of course we can write that $$||\dot{v}(t)||=||\frac{d}{d\tau}v(\tau)|_{\tau=t}||=||\alpha||.$$ But I wonder if I can say that $$(\frac{d}{d\tau}||v(\tau)||)_{\tau=t} \le ||\alpha||.$$ And if so, why?",,"['calculus', 'derivatives', 'normed-spaces']"
86,Evaluating $\int_0^{\infty}\left(\frac{\ln(1+x^2)}{1+x^2}\right)^3 dx$,Evaluating,\int_0^{\infty}\left(\frac{\ln(1+x^2)}{1+x^2}\right)^3 dx,"I’m looking for a closed form of this integral $$I_3=\int_0^{\infty}\left(\frac{\ln(1+x^2)}{1+x^2}\right)^3 dx$$ I’ve managed to evaluate $$I_1=\int_0^{\infty} \frac{\ln(1+x^2)}{1+x^2}dx=\pi \ln 2$$ with the change of variable $x =\tan \theta $, $dx=(1+(\tan \theta)^2) d\theta $ giving $$I_1=-2\int_0^{\frac{\pi}{2}} \ln(\cos \theta ) d\theta$$ which is classic. But I’m stuck with the power $(\ln (\cos \theta))^3$ in $I_3$. Thanks for your help.","I’m looking for a closed form of this integral $$I_3=\int_0^{\infty}\left(\frac{\ln(1+x^2)}{1+x^2}\right)^3 dx$$ I’ve managed to evaluate $$I_1=\int_0^{\infty} \frac{\ln(1+x^2)}{1+x^2}dx=\pi \ln 2$$ with the change of variable $x =\tan \theta $, $dx=(1+(\tan \theta)^2) d\theta $ giving $$I_1=-2\int_0^{\frac{\pi}{2}} \ln(\cos \theta ) d\theta$$ which is classic. But I’m stuck with the power $(\ln (\cos \theta))^3$ in $I_3$. Thanks for your help.",,"['calculus', 'integration', 'definite-integrals']"
87,How to calculate $\int\limits_{-\infty}^\infty\frac{e^{ix}}{x}dx$,How to calculate,\int\limits_{-\infty}^\infty\frac{e^{ix}}{x}dx,"I need to calculate    $$I_0=\int_{-\infty}^\infty\frac{e^{ix}}{x}dx=\lim_{R\rightarrow\infty}\int_{-R}^R\frac{e^{ix}}{x}dx=\lim_{R\rightarrow 0}(\lim_{\varepsilon\rightarrow 0}(\int_{-R}^\varepsilon\frac{e^{ix}}{x}dx+\int_{-\varepsilon}^R\frac{e^{ix}}{x}dx))=\lim_{R\rightarrow 0}(\lim_{\varepsilon\rightarrow 0}I(R,\varepsilon))$$ I have tried to close the contour of the complex integral on the complex plain. Let $C$ be the semicircle with radius R above the real axis and $\gamma$ be the semicircle under the real axis with radius $\varepsilon$, both centered at $O$. Let $L$ be the closed contour contained $C,[-R,-\varepsilon],\gamma,[\varepsilon,R]$. Denote the $+$ direction by the $+$ direction of $L$. Then we have: $$\int_{L^+}\frac{e^{iz}}{z}dz=I(R,\varepsilon)+\int_{C^+}\frac{e^{iz}}{z}dz+\int_{\gamma^+}\frac{e^{iz}}{z}dz$$ By Cauchy Integral Formula we have $\int_{L^+}\frac{e^{iz}}{z}dz=2\pi i$ so $$I(R,\varepsilon)=2\pi i-\int_{C^+}\frac{e^{iz}}{z}dz-\int_{\gamma^+}\frac{e^{iz}}{z}dz$$ Take $\varepsilon\rightarrow 0$ we have: $$I(R,0)=\pi i-\int_{C^+}\frac{e^{iz}}{z}dz=\pi i-\int_0^{\pi}\frac{e^{iRe^{i\phi}}}{Re^{i\phi}}iRe^{i\phi}d\phi$$ $$=\pi i-i\int_0^\pi\exp(iR(\cos\phi+i\sin\phi))d\phi=\pi i-i\int_0^\pi e^{iR\cos\phi}e^{-R\sin\phi}d\phi$$ Now I must take the limit $R\rightarrow\infty$. I evaluate: $$\left|\int_0^\pi e^{iR\cos\phi}e^{-R\sin\phi}d\phi\right|\le\int_0^\pi e^{-R\sin\phi}d\phi$$ Is there anything wrong in my argument so far? How can I proceed further? EDIT1: Looking at it for quite long I think I can write $$\int_0^\pi e^{-R\sin\phi}d\phi=2\int_0^{\pi/2} e^{-R\sin\phi}d\phi\le 2\int_0^{\pi/2} e^{-R\frac{\phi}{2}}d\phi=\frac{4}{R}(1-e^{-R\frac{\pi}{4}})\le\frac{4}{R}$$ So in the end take $R\rightarrow\infty$ we get $I_0=i\pi$. Is it true?","I need to calculate    $$I_0=\int_{-\infty}^\infty\frac{e^{ix}}{x}dx=\lim_{R\rightarrow\infty}\int_{-R}^R\frac{e^{ix}}{x}dx=\lim_{R\rightarrow 0}(\lim_{\varepsilon\rightarrow 0}(\int_{-R}^\varepsilon\frac{e^{ix}}{x}dx+\int_{-\varepsilon}^R\frac{e^{ix}}{x}dx))=\lim_{R\rightarrow 0}(\lim_{\varepsilon\rightarrow 0}I(R,\varepsilon))$$ I have tried to close the contour of the complex integral on the complex plain. Let $C$ be the semicircle with radius R above the real axis and $\gamma$ be the semicircle under the real axis with radius $\varepsilon$, both centered at $O$. Let $L$ be the closed contour contained $C,[-R,-\varepsilon],\gamma,[\varepsilon,R]$. Denote the $+$ direction by the $+$ direction of $L$. Then we have: $$\int_{L^+}\frac{e^{iz}}{z}dz=I(R,\varepsilon)+\int_{C^+}\frac{e^{iz}}{z}dz+\int_{\gamma^+}\frac{e^{iz}}{z}dz$$ By Cauchy Integral Formula we have $\int_{L^+}\frac{e^{iz}}{z}dz=2\pi i$ so $$I(R,\varepsilon)=2\pi i-\int_{C^+}\frac{e^{iz}}{z}dz-\int_{\gamma^+}\frac{e^{iz}}{z}dz$$ Take $\varepsilon\rightarrow 0$ we have: $$I(R,0)=\pi i-\int_{C^+}\frac{e^{iz}}{z}dz=\pi i-\int_0^{\pi}\frac{e^{iRe^{i\phi}}}{Re^{i\phi}}iRe^{i\phi}d\phi$$ $$=\pi i-i\int_0^\pi\exp(iR(\cos\phi+i\sin\phi))d\phi=\pi i-i\int_0^\pi e^{iR\cos\phi}e^{-R\sin\phi}d\phi$$ Now I must take the limit $R\rightarrow\infty$. I evaluate: $$\left|\int_0^\pi e^{iR\cos\phi}e^{-R\sin\phi}d\phi\right|\le\int_0^\pi e^{-R\sin\phi}d\phi$$ Is there anything wrong in my argument so far? How can I proceed further? EDIT1: Looking at it for quite long I think I can write $$\int_0^\pi e^{-R\sin\phi}d\phi=2\int_0^{\pi/2} e^{-R\sin\phi}d\phi\le 2\int_0^{\pi/2} e^{-R\frac{\phi}{2}}d\phi=\frac{4}{R}(1-e^{-R\frac{\pi}{4}})\le\frac{4}{R}$$ So in the end take $R\rightarrow\infty$ we get $I_0=i\pi$. Is it true?",,"['calculus', 'complex-analysis', 'analysis', 'residue-calculus', 'cauchy-integral-formula']"
88,How to solve $1.1^x +1.5^x = 3.46$,How to solve,1.1^x +1.5^x = 3.46,"I'm drawing a total blank as to how to solve this for x ... $$ 1.1^x + 1.5^x = 3.46 $$ I thought I could differentiate and substitute the first derivative back into the equation, but I must be rustier than I thought because I'm it's not working out correctly. Incidentally, the answer is $x=2$. Any thoughts on how to solve this type of equations would be very much appreciated. Kindest regards, Jack","I'm drawing a total blank as to how to solve this for x ... $$ 1.1^x + 1.5^x = 3.46 $$ I thought I could differentiate and substitute the first derivative back into the equation, but I must be rustier than I thought because I'm it's not working out correctly. Incidentally, the answer is $x=2$. Any thoughts on how to solve this type of equations would be very much appreciated. Kindest regards, Jack",,['calculus']
89,The limit of reciprocal is the reciprocal of the limit,The limit of reciprocal is the reciprocal of the limit,,"While I was reading the book Exploratory examples for real analysis , I came across this: to show that : $$\lim_{x\rightarrow x_o}\frac{1}{h(x)} = \frac{1}{\lim_{x\rightarrow x_o}{h(x)}} = \frac{1}{M}$$ Discussion:Let $\epsilon \gt 0$.we want to find $\delta \gt 0$ such that if  $0 \lt |x-x_0| \lt \delta $,then $\Big|\frac{1}{h(x)}-\frac{1}{M}\Big|\lt \epsilon$. In order to get expression involving $|h(x)-M|$,we first find the common denominator:        $\Big|\frac{1}{h(x)}-\frac{1}{M}\Big|=\Big|\frac{M-h(x)}{M~h(x)}\Big|$      $~=\Big|\frac{h(x)-M}{|M|~|h(x)|}\Big|$. As we have to deal with variable term $|h(x)|$ in denominator.We can construct bound .Let $\epsilon =\frac{|M|}{2}$ ,since lim$_{x\to x_o}h(x)=M$,there exists $\delta_1\gt 0~~$ s.t. if $0 \lt |x-x_0| \lt \delta_1 $,then $|h(x)-{M}|\lt \frac{|M|}{2}$. As you study steps below,identify that step where $h(x)\neq 0$ for all $0 \lt |x-x_0| \lt \delta_1 $ plays a critical role: $|M|=|M-h(x)+h(x)|$ $~~~~~~\leq |M-h(x)|+|h(x)|$ $~~~~~~= |h(x)-M|+|h(x)|$ $~~~~~~\lt \frac{M}{2}+|h(x)| \implies$ $\frac{|M|}{2} \lt |h(x)| \implies$ $\frac{1}{h(x)}\lt \frac{2}{|M|}$,where $0 \lt |x-x_0| \lt \delta_1 $. Now that we've bound for $1/h(x)$,we can apply the definition to term $|h(x)-M|$. Since     lim$_{x\rightarrow x_o}h(x)=M$,we can find $\delta_2 \gt 0$ such that, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~$if  $0 \lt |x-x_0| \lt \delta_2$, then $|h(x)-M|\lt \frac{|M|^2 \epsilon }{2}$ The thing I can't understand is for what purpose did the author work with such complicated expression involving $\epsilon $ in the end of the discussion?","While I was reading the book Exploratory examples for real analysis , I came across this: to show that : $$\lim_{x\rightarrow x_o}\frac{1}{h(x)} = \frac{1}{\lim_{x\rightarrow x_o}{h(x)}} = \frac{1}{M}$$ Discussion:Let $\epsilon \gt 0$.we want to find $\delta \gt 0$ such that if  $0 \lt |x-x_0| \lt \delta $,then $\Big|\frac{1}{h(x)}-\frac{1}{M}\Big|\lt \epsilon$. In order to get expression involving $|h(x)-M|$,we first find the common denominator:        $\Big|\frac{1}{h(x)}-\frac{1}{M}\Big|=\Big|\frac{M-h(x)}{M~h(x)}\Big|$      $~=\Big|\frac{h(x)-M}{|M|~|h(x)|}\Big|$. As we have to deal with variable term $|h(x)|$ in denominator.We can construct bound .Let $\epsilon =\frac{|M|}{2}$ ,since lim$_{x\to x_o}h(x)=M$,there exists $\delta_1\gt 0~~$ s.t. if $0 \lt |x-x_0| \lt \delta_1 $,then $|h(x)-{M}|\lt \frac{|M|}{2}$. As you study steps below,identify that step where $h(x)\neq 0$ for all $0 \lt |x-x_0| \lt \delta_1 $ plays a critical role: $|M|=|M-h(x)+h(x)|$ $~~~~~~\leq |M-h(x)|+|h(x)|$ $~~~~~~= |h(x)-M|+|h(x)|$ $~~~~~~\lt \frac{M}{2}+|h(x)| \implies$ $\frac{|M|}{2} \lt |h(x)| \implies$ $\frac{1}{h(x)}\lt \frac{2}{|M|}$,where $0 \lt |x-x_0| \lt \delta_1 $. Now that we've bound for $1/h(x)$,we can apply the definition to term $|h(x)-M|$. Since     lim$_{x\rightarrow x_o}h(x)=M$,we can find $\delta_2 \gt 0$ such that, $~~~~~~~~~~~~~~~~~~~~~~~~~~~~$if  $0 \lt |x-x_0| \lt \delta_2$, then $|h(x)-M|\lt \frac{|M|^2 \epsilon }{2}$ The thing I can't understand is for what purpose did the author work with such complicated expression involving $\epsilon $ in the end of the discussion?",,"['calculus', 'limits', 'proof-explanation']"
90,Help with problem 1-5 from Calculus by Spivak (multiplying two inequalities together),Help with problem 1-5 from Calculus by Spivak (multiplying two inequalities together),,"This question involves problem 5(viii) from chapter 1 of Spivak's Calculus, third edition. I'm a layman trying to teach myself some more advanced mathematics, and although I've been making slow progress through the book, this problem has me stumped. The question is as follows: If $0 \leq a < b$ and $0 \leq c < d$, prove $ac < bd$. This question has different answers, depending on whether $a$ and $b$ are equal to zero or not. I think the most difficult case is the one where $a, b \ne 0$, so that's the one I will use here. My attempt: In an earlier question, I've proved that if $a < b$ and $c > 0$ then $ac < bc$. This is done by multiplying: $(a < b)c = ac < bc$. From now on I will use the notation that Spivak uses, where $a < b$ means that $b - a$ is in $P$. This leaves me with $b - a$ and $d - c$, both in $P$. Spivak has established that this means you can multiply the two together. I will list the steps that I followed: $(b - a)(d - c) = bd - bc - ad + ac$ $ bd - bc - ad + ac = bd - (bc + ad - ac)$ $bd - (bc + ad - ac) = bd > (bc + ad - ac)$ $bd > (bc + ad - ac) = bd > (bc + ad > ac)$ $bd > (bc + ad > ac) = bd > bc + ad > ac$ This would appear to prove that $ac < bd$ (which is what the question asked) but the part $bd > bc + ad$ is quite obviously incorrect. Just a simple example proves this: If $2 > 1$ and $3 > 2$, then $6 > 7 > 2$. Intuition tells the me that the two seperate inequalities $bd > bc > ac$ and $bd > ad > ac$ are both correct, but I have no idea how I can formally get these inequalities to 'split', so to speak. Help would be much appreciated! ... Edited for spelling. And thanks for helping me with the problem guys!","This question involves problem 5(viii) from chapter 1 of Spivak's Calculus, third edition. I'm a layman trying to teach myself some more advanced mathematics, and although I've been making slow progress through the book, this problem has me stumped. The question is as follows: If $0 \leq a < b$ and $0 \leq c < d$, prove $ac < bd$. This question has different answers, depending on whether $a$ and $b$ are equal to zero or not. I think the most difficult case is the one where $a, b \ne 0$, so that's the one I will use here. My attempt: In an earlier question, I've proved that if $a < b$ and $c > 0$ then $ac < bc$. This is done by multiplying: $(a < b)c = ac < bc$. From now on I will use the notation that Spivak uses, where $a < b$ means that $b - a$ is in $P$. This leaves me with $b - a$ and $d - c$, both in $P$. Spivak has established that this means you can multiply the two together. I will list the steps that I followed: $(b - a)(d - c) = bd - bc - ad + ac$ $ bd - bc - ad + ac = bd - (bc + ad - ac)$ $bd - (bc + ad - ac) = bd > (bc + ad - ac)$ $bd > (bc + ad - ac) = bd > (bc + ad > ac)$ $bd > (bc + ad > ac) = bd > bc + ad > ac$ This would appear to prove that $ac < bd$ (which is what the question asked) but the part $bd > bc + ad$ is quite obviously incorrect. Just a simple example proves this: If $2 > 1$ and $3 > 2$, then $6 > 7 > 2$. Intuition tells the me that the two seperate inequalities $bd > bc > ac$ and $bd > ad > ac$ are both correct, but I have no idea how I can formally get these inequalities to 'split', so to speak. Help would be much appreciated! ... Edited for spelling. And thanks for helping me with the problem guys!",,"['calculus', 'inequality']"
91,"Interesting, unusual max/min problems?","Interesting, unusual max/min problems?",,"So I've got to that stage of my elementary mathematics subject for engineers when we talk about differentiation and solution of max/min problems.  And I'd like to entertain and engage the students with some interesting problems.  Pretty much every book and website talks about maximizing rectangular areas of land with fences of a given length, or maximizing the volume of a box with square cross section (and given surface area) etc.  No student is ever excited by these.  Slightly more interesting is the problem of maximizing the length of a pipe (considered as having zero cross section) which can be manoeuvred around a right-angled hallway.  There must be more interesting problems, or at least, more interesting ways to dress up these problems. But I don't know of any, and would be delighted to know of some!","So I've got to that stage of my elementary mathematics subject for engineers when we talk about differentiation and solution of max/min problems.  And I'd like to entertain and engage the students with some interesting problems.  Pretty much every book and website talks about maximizing rectangular areas of land with fences of a given length, or maximizing the volume of a box with square cross section (and given surface area) etc.  No student is ever excited by these.  Slightly more interesting is the problem of maximizing the length of a pipe (considered as having zero cross section) which can be manoeuvred around a right-angled hallway.  There must be more interesting problems, or at least, more interesting ways to dress up these problems. But I don't know of any, and would be delighted to know of some!",,"['calculus', 'derivatives', 'optimization', 'education']"
92,Is there a theory of integration in elementary terms for definite integrals?,Is there a theory of integration in elementary terms for definite integrals?,,"Let's call a real number explicit if it can be expressed starting from integers by using arithmetic operations, radicals, exponents, logarithms, trigonometric and inverse trigonometric functions. For complex numbers $i$ is allowed in addition to integers, and we get what Chow calls EL numbers . Rational numbers, $e=\exp(1)$ and $\pi=\cos^{-1}(-1)$ are explicit, some algebraic numbers and Euler's $\gamma$ probably are not. If a function has an elementary anti-derivative then a definite integral of it with explicit limits is also explicit. The converse is not true, $\int x^{-2}e^{-\frac1{x^2}}dx$ is not elementary, but $\int_0^1 x^{-2}e^{-\frac1{x^2}}dx=\frac{\sqrt{\pi}}{2}$ is explicit (this is the Gaussian integral up to substitution). There is a theory of Liouville that allows proving that some anti-derivatives are not elementary, is there something similar for definite integrals? There was a long standing unanswered question on MSE about computing $\int_0^{\frac{\pi}{2}}\frac1{(1+x^2)(1+\tan x)}dx$, and the consensus was that it's not expressible in elementary terms. How does one prove something like that? EDIT: Since there doesn't appear to be a general theory any non-trivial example of a proof for a particular case would be interesting. Also, explicit or EL numbers above are only an example, proving non-expressibility in some other reasonable terms would also be interesting.","Let's call a real number explicit if it can be expressed starting from integers by using arithmetic operations, radicals, exponents, logarithms, trigonometric and inverse trigonometric functions. For complex numbers $i$ is allowed in addition to integers, and we get what Chow calls EL numbers . Rational numbers, $e=\exp(1)$ and $\pi=\cos^{-1}(-1)$ are explicit, some algebraic numbers and Euler's $\gamma$ probably are not. If a function has an elementary anti-derivative then a definite integral of it with explicit limits is also explicit. The converse is not true, $\int x^{-2}e^{-\frac1{x^2}}dx$ is not elementary, but $\int_0^1 x^{-2}e^{-\frac1{x^2}}dx=\frac{\sqrt{\pi}}{2}$ is explicit (this is the Gaussian integral up to substitution). There is a theory of Liouville that allows proving that some anti-derivatives are not elementary, is there something similar for definite integrals? There was a long standing unanswered question on MSE about computing $\int_0^{\frac{\pi}{2}}\frac1{(1+x^2)(1+\tan x)}dx$, and the consensus was that it's not expressible in elementary terms. How does one prove something like that? EDIT: Since there doesn't appear to be a general theory any non-trivial example of a proof for a particular case would be interesting. Also, explicit or EL numbers above are only an example, proving non-expressibility in some other reasonable terms would also be interesting.",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'elementary-functions']"
93,Explanation of line element formula $dl^2 = dx^2 + dy^2$,Explanation of line element formula,dl^2 = dx^2 + dy^2,"I found this in a physics textbook without justification: $$dl^2 = dx^2 +dy^2,$$ where I presume that $l = \sqrt{x^2+y^2}$. Why is this so? By my calculations I obtain $$ dl = \dfrac{\partial l}{\partial x} \, dx + \dfrac{\partial l}{\partial y} \, dy= \dfrac{x \, dx + y \, dy}{\sqrt{x^2+y^2}},$$ and squaring $$ dl^2 = \dfrac{x^2 \, dx^2 + y^2 \, dy^2 +2xy \, dx \, dy}{x^2+y^2}. $$ Are they the same? Am I doing something wrong?","I found this in a physics textbook without justification: $$dl^2 = dx^2 +dy^2,$$ where I presume that $l = \sqrt{x^2+y^2}$. Why is this so? By my calculations I obtain $$ dl = \dfrac{\partial l}{\partial x} \, dx + \dfrac{\partial l}{\partial y} \, dy= \dfrac{x \, dx + y \, dy}{\sqrt{x^2+y^2}},$$ and squaring $$ dl^2 = \dfrac{x^2 \, dx^2 + y^2 \, dy^2 +2xy \, dx \, dy}{x^2+y^2}. $$ Are they the same? Am I doing something wrong?",,"['calculus', 'derivatives', 'physics', 'differential']"
94,How do I prove $\int_{0}^{2a}f(x) dx = \int_{0}^{a}f(x)dx +\int_{0}^{a}f(2a-x)dx$,How do I prove,\int_{0}^{2a}f(x) dx = \int_{0}^{a}f(x)dx +\int_{0}^{a}f(2a-x)dx,"I am stuck on this one. I was able to prove $\int_{0}^{a}f(x) dx = \int_{0}^{a}f(a-x)dx $ just by using substitution with $t = a - x$ , but I'm not sure about this one. I've tried quite a few subsitutions, such as $t = 2a - x$ , but none seem to work?","I am stuck on this one. I was able to prove just by using substitution with , but I'm not sure about this one. I've tried quite a few subsitutions, such as , but none seem to work?",\int_{0}^{a}f(x) dx = \int_{0}^{a}f(a-x)dx  t = a - x t = 2a - x,"['calculus', 'integration', 'definite-integrals']"
95,Radius of $\sum a_n b_n x^n$ via radii of $\sum a_n x^n$ and $\sum b_n x^n$,Radius of  via radii of  and,\sum a_n b_n x^n \sum a_n x^n \sum b_n x^n,"Series $\sum a_n x^n$ and $\sum b_n x^n$ have radii of convergence of   1 and 2, respectively. Then radius of convergence R of $\sum a_n b_n x^n$ is 2 1 $\geq 1$ $ \leq 2$ My attempt was to apply ratio test: $$\lim_{n \rightarrow \infty} \frac{a_{n} b_{n}}{a_{n+1} b_{n+1}} = \lim_{n \rightarrow \infty} \frac{a_{n}}{a_{n+1}} \cdot \lim_{n \rightarrow \infty} \frac{ b_{n}}{ b_{n+1}} = 1 \cdot 2 = 2$$ i.e. option (1). But the answer is (3). Why?","Series $\sum a_n x^n$ and $\sum b_n x^n$ have radii of convergence of   1 and 2, respectively. Then radius of convergence R of $\sum a_n b_n x^n$ is 2 1 $\geq 1$ $ \leq 2$ My attempt was to apply ratio test: $$\lim_{n \rightarrow \infty} \frac{a_{n} b_{n}}{a_{n+1} b_{n+1}} = \lim_{n \rightarrow \infty} \frac{a_{n}}{a_{n+1}} \cdot \lim_{n \rightarrow \infty} \frac{ b_{n}}{ b_{n+1}} = 1 \cdot 2 = 2$$ i.e. option (1). But the answer is (3). Why?",,"['calculus', 'sequences-and-series', 'power-series']"
96,Solution of differential equation with Dirac Delta,Solution of differential equation with Dirac Delta,,"Is it possible to solve a differential equation of the following form? $\partial_x^2y + \delta(x) \partial_x y = 0$ where $\delta(x)$ is the dirac delta function. I need the solution for periodic boundary conditions from -1 to 1, but I'll be fine if you direct me for any sort of boundary conditions.. I've realised that I can solve this for some types of boundary conditions. What i'd be really interested in is how to do this for periodic boundary conditions... Technically, if I approach the problem by splitting the regions $x<0$ and $x>0$ and solve in each part separately, I can solve it and get linear equations in both regions. This will give me $4$ variables. Periodicity, and periodicity of the derivative will give me 2 equations. Continuity at $x=0$ will give me one more. How do i relate the derivative around the $x=0$ interface? A little background: If there was no delta function, but rather say some gaussian approximation, I would be expect to be able to solve it, but I don't see why I can't get the information of the derivative around $x=0$ when i put in a dirac delta function. My actual problem is reasonably more complicated but this is the quickest simple example I could reduce my problem to. If I try to integrate in an epsilon region around $0$, then I end up with an expression in $y^\prime(0)$, which isn't defined. If it helps, i'm actually interested in a physical problem, so this delta function is just an approximation, but i'm unable to take any function in it's place whose limit I can make tend to a delta function.. Any help or direction would be greatly appreciated! EDIT: I guess I should make my actual problem a bit clearer as well. In this simplistic case i can simplify the differential equation by means of the substitution of $z=y^\prime$, however I can't really do that in my original equation. I'm basically interested in some technique by which I can get the information for the change in derivative of the function around the delta function. A better example might be $\partial_x^2y + \delta(x) \partial_x y +y= 0$","Is it possible to solve a differential equation of the following form? $\partial_x^2y + \delta(x) \partial_x y = 0$ where $\delta(x)$ is the dirac delta function. I need the solution for periodic boundary conditions from -1 to 1, but I'll be fine if you direct me for any sort of boundary conditions.. I've realised that I can solve this for some types of boundary conditions. What i'd be really interested in is how to do this for periodic boundary conditions... Technically, if I approach the problem by splitting the regions $x<0$ and $x>0$ and solve in each part separately, I can solve it and get linear equations in both regions. This will give me $4$ variables. Periodicity, and periodicity of the derivative will give me 2 equations. Continuity at $x=0$ will give me one more. How do i relate the derivative around the $x=0$ interface? A little background: If there was no delta function, but rather say some gaussian approximation, I would be expect to be able to solve it, but I don't see why I can't get the information of the derivative around $x=0$ when i put in a dirac delta function. My actual problem is reasonably more complicated but this is the quickest simple example I could reduce my problem to. If I try to integrate in an epsilon region around $0$, then I end up with an expression in $y^\prime(0)$, which isn't defined. If it helps, i'm actually interested in a physical problem, so this delta function is just an approximation, but i'm unable to take any function in it's place whose limit I can make tend to a delta function.. Any help or direction would be greatly appreciated! EDIT: I guess I should make my actual problem a bit clearer as well. In this simplistic case i can simplify the differential equation by means of the substitution of $z=y^\prime$, however I can't really do that in my original equation. I'm basically interested in some technique by which I can get the information for the change in derivative of the function around the delta function. A better example might be $\partial_x^2y + \delta(x) \partial_x y +y= 0$",,"['calculus', 'ordinary-differential-equations', 'physics', 'dirac-delta']"
97,Find the maximum possible value.,Find the maximum possible value.,,"For all ordered triples $(p,q,r)$ define the polynomial $$f_{p,q,r}(x)=x^3-px^2+qx-r$$ Let $a_{1},a_{2},a_{3},b_{1},b_{2},b_{3},c_{1},c_{2},c_{3}$ be (not necessarily distinct) positive reals such that the roots of $f_{a_{1},a_{2},a_{3}}(x)$ are $b_{1},b_{2},b_{3}$ and the roots of $f_{b_{1},b_{2},b_{3}}(x)$ are $c_{1},c_{2},c_{3}$. Determine the maximum possible value of $$ \frac{9\sqrt[3]{b_{3}}}{b_{1}+3} + \frac{4+3b_{1}+2b_{2}+b_{3}}{a_{1}+1} $$ I used Vieta's formulas combined with calculus. I set this expression equal to $y$ and then cubed both sides. Then I tried to use the fact that since $y$ is real, the cubic in $y$ (generated by cubing both sides) will have three real roots. Now, I differentiated the equation w.r.t. $y$ (assuming everything else to be constant). I got a quadratic in $y$ and I then made its discriminant $>0$. Now I used Vieta's formulas. After that I'm stuck since I still have more than one variable left. Also, I'm not yet familiar with multivariable calculus. Any help will be greately appreciated.   Thanks!","For all ordered triples $(p,q,r)$ define the polynomial $$f_{p,q,r}(x)=x^3-px^2+qx-r$$ Let $a_{1},a_{2},a_{3},b_{1},b_{2},b_{3},c_{1},c_{2},c_{3}$ be (not necessarily distinct) positive reals such that the roots of $f_{a_{1},a_{2},a_{3}}(x)$ are $b_{1},b_{2},b_{3}$ and the roots of $f_{b_{1},b_{2},b_{3}}(x)$ are $c_{1},c_{2},c_{3}$. Determine the maximum possible value of $$ \frac{9\sqrt[3]{b_{3}}}{b_{1}+3} + \frac{4+3b_{1}+2b_{2}+b_{3}}{a_{1}+1} $$ I used Vieta's formulas combined with calculus. I set this expression equal to $y$ and then cubed both sides. Then I tried to use the fact that since $y$ is real, the cubic in $y$ (generated by cubing both sides) will have three real roots. Now, I differentiated the equation w.r.t. $y$ (assuming everything else to be constant). I got a quadratic in $y$ and I then made its discriminant $>0$. Now I used Vieta's formulas. After that I'm stuck since I still have more than one variable left. Also, I'm not yet familiar with multivariable calculus. Any help will be greately appreciated.   Thanks!",,"['calculus', 'algebra-precalculus', 'polynomials', 'roots', 'cubics']"
98,Help needed in solving a system of DE,Help needed in solving a system of DE,,"The system of DE is: $$\frac{dI}{db}=-\frac{b}{c}\frac{dJ}{db}-\frac{2ab+1}{2c}J$$ $$\frac{dJ}{db}=\frac{b}{c}\frac{dI}{db}-\frac{2ab-1}{2c}I$$ Assume that $a$ and $c$ are constants and both $I$ and $J$ tends to zero as $b$ tends to $\infty$. To be honest, I have never solved a system of DE before. I usually use W|A if I am in a situation like this but in this case, even W|A fails. I am not sure if it is even possible to solve the above system. Any help is appreciated. Many thanks!","The system of DE is: $$\frac{dI}{db}=-\frac{b}{c}\frac{dJ}{db}-\frac{2ab+1}{2c}J$$ $$\frac{dJ}{db}=\frac{b}{c}\frac{dI}{db}-\frac{2ab-1}{2c}I$$ Assume that $a$ and $c$ are constants and both $I$ and $J$ tends to zero as $b$ tends to $\infty$. To be honest, I have never solved a system of DE before. I usually use W|A if I am in a situation like this but in this case, even W|A fails. I am not sure if it is even possible to solve the above system. Any help is appreciated. Many thanks!",,"['calculus', 'integration', 'ordinary-differential-equations']"
99,Differential inequation problem,Differential inequation problem,,"Let $f:[0,1]→\mathbb{R}$ be a function. Suppose the function $f$ is twice differentiable, $f(0)=f(1)=0$ and satisfies $f''(x)-2f'(x)+f(x)\geq e^x$, $x\in [0,1]$. Which of the following is true for $0<x<1$? A) $0<f(x)<\infty$ B) $-\frac{1}{2}<f(x)<\frac{1}{2}$ C) $-\frac{1}{4}<f(x)<1$ D) $-\infty<f(x)<0$ This appeared in one of my test papers and I went blank over this. I am still out of ideas because I have never come across differential inequations. Moreover, the above inequation is second-order which isn't even in my course. How am I supposed to deal with this? It would be greatly appreciated is someone could provide a resource or the name of book where these kind of problems are found. Many thanks!","Let $f:[0,1]→\mathbb{R}$ be a function. Suppose the function $f$ is twice differentiable, $f(0)=f(1)=0$ and satisfies $f''(x)-2f'(x)+f(x)\geq e^x$, $x\in [0,1]$. Which of the following is true for $0<x<1$? A) $0<f(x)<\infty$ B) $-\frac{1}{2}<f(x)<\frac{1}{2}$ C) $-\frac{1}{4}<f(x)<1$ D) $-\infty<f(x)<0$ This appeared in one of my test papers and I went blank over this. I am still out of ideas because I have never come across differential inequations. Moreover, the above inequation is second-order which isn't even in my course. How am I supposed to deal with this? It would be greatly appreciated is someone could provide a resource or the name of book where these kind of problems are found. Many thanks!",,"['calculus', 'ordinary-differential-equations']"
