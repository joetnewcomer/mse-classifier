,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Riemann-Stieltjes integral of unbounded function,Riemann-Stieltjes integral of unbounded function,,"In many theorems about the Riemann-Stieltjes integral they required the hypothesis of $f$ to be bounded to then conclude that $f$ is Riemann-Stieltjes integrable. For example, suppose that $f$ is bounded in $I = [a,b]$, $f$ has only finitely many points of discontinuity in $I$, and that the monotonically increasing function $\alpha$ is continuous at each point of discontinuity of $f$, then $f$ is Riemann-Stieltjes integrable. What if we remove the bounded hypothesis? Could there exist an unbounded function $f$ in a given interval $[a,b]$ such that $\int_a^bf\,d\alpha$ exist? Maybe a counterexample?","In many theorems about the Riemann-Stieltjes integral they required the hypothesis of $f$ to be bounded to then conclude that $f$ is Riemann-Stieltjes integrable. For example, suppose that $f$ is bounded in $I = [a,b]$, $f$ has only finitely many points of discontinuity in $I$, and that the monotonically increasing function $\alpha$ is continuous at each point of discontinuity of $f$, then $f$ is Riemann-Stieltjes integrable. What if we remove the bounded hypothesis? Could there exist an unbounded function $f$ in a given interval $[a,b]$ such that $\int_a^bf\,d\alpha$ exist? Maybe a counterexample?",,"['real-analysis', 'integration', 'analysis', 'riemann-integration', 'stieltjes-integral']"
1,Evaluate series $\sum\limits_{n=1}^{\infty}\frac{x^{2^{n-1}}}{1-x^{2^n}}$,Evaluate series,\sum\limits_{n=1}^{\infty}\frac{x^{2^{n-1}}}{1-x^{2^n}},"Determine the value of   $$\sum_{n=1}^{\infty}\frac{x^{2^{n-1}}}{1-x^{2^n}}$$   or $$\frac{x}{1-x^2}+\frac{x^2}{1-x^4}+\frac{x^4}{1-x^8}+\cdots$$   for $x\in\mathbb{R}$. The answer is $\dfrac{x}{1-x}$ for $x\in(0,1)$. To prove this, notice $$\frac{x}{1-x^2}=x+x^3+x^5+\cdots$$ $$\frac{x^2}{1-x^4}=x^2+x^6+x^{10}+\cdots$$ $$\cdots$$ Add them all and get the answer. Unfortunately, I havn't got a direct method to calculate it. Appreciate for your help!","Determine the value of   $$\sum_{n=1}^{\infty}\frac{x^{2^{n-1}}}{1-x^{2^n}}$$   or $$\frac{x}{1-x^2}+\frac{x^2}{1-x^4}+\frac{x^4}{1-x^8}+\cdots$$   for $x\in\mathbb{R}$. The answer is $\dfrac{x}{1-x}$ for $x\in(0,1)$. To prove this, notice $$\frac{x}{1-x^2}=x+x^3+x^5+\cdots$$ $$\frac{x^2}{1-x^4}=x^2+x^6+x^{10}+\cdots$$ $$\cdots$$ Add them all and get the answer. Unfortunately, I havn't got a direct method to calculate it. Appreciate for your help!",,"['real-analysis', 'sequences-and-series', 'power-series']"
2,Suppose $f$ is differentiable on $\mathbb{R}$ and that $\lim_{x \rightarrow 0} f'(x)=L$. May we conclude that $f'(0)=L$ [duplicate],Suppose  is differentiable on  and that . May we conclude that  [duplicate],f \mathbb{R} \lim_{x \rightarrow 0} f'(x)=L f'(0)=L,"This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) If $\lim_{x\to\infty}f(x)$ and $\lim_{x\to\infty}f^{\prime}(x)$ both exist, then $\lim_{x\to\infty}f^{\prime}(x) = 0$ (3 answers) Closed 7 years ago . This seems like another good question for consideration. I think the answer is yes just because I cannot think of a way to make it break down because the domain is defined for all of $\mathbb{R}$. Any thoughts? Thank you in advance. This question is not a duplicate, it deals with the limit of the derivative specifically at 0. The other two cited questions do not. Thank you.","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) If $\lim_{x\to\infty}f(x)$ and $\lim_{x\to\infty}f^{\prime}(x)$ both exist, then $\lim_{x\to\infty}f^{\prime}(x) = 0$ (3 answers) Closed 7 years ago . This seems like another good question for consideration. I think the answer is yes just because I cannot think of a way to make it break down because the domain is defined for all of $\mathbb{R}$. Any thoughts? Thank you in advance. This question is not a duplicate, it deals with the limit of the derivative specifically at 0. The other two cited questions do not. Thank you.",,"['real-analysis', 'limits']"
3,"What is the second derivative?, Part I","What is the second derivative?, Part I",,"I've been interested in learning about higher order derivatives of vector functions recently and inspired by this answer by @Bye_World , I have some questions. So first, how exactly do we define higher order derivatives of functions $f:\Bbb R^m \to \Bbb R^n$?  I know that the first derivative is the linear function $L$ such that $$\lim_{h\to 0}\frac{\|f(x+h)-f(x)-L(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0$$ I asked a question earlier today about a possible limit definition of the second derivative but found that my guess at one didn't work.  Is there no better way than to define it recursively by $$\lim_{h\to 0}\frac{\|D^{n-1}f(x+h)-D^{n-1}f(x)-L(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0\, ?$$ And even in that formula, I don't know how you take account of the fact that the $n$th derivative is evaluated at $n$ different points (or vectors?). Also @Bye_World mentions ""the isomorphism $\mathcal L(X,\mathcal L(X,Y)) \simeq \operatorname{Bil}(X\times X\to Y)$"".  What does that mean and how is it related?","I've been interested in learning about higher order derivatives of vector functions recently and inspired by this answer by @Bye_World , I have some questions. So first, how exactly do we define higher order derivatives of functions $f:\Bbb R^m \to \Bbb R^n$?  I know that the first derivative is the linear function $L$ such that $$\lim_{h\to 0}\frac{\|f(x+h)-f(x)-L(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0$$ I asked a question earlier today about a possible limit definition of the second derivative but found that my guess at one didn't work.  Is there no better way than to define it recursively by $$\lim_{h\to 0}\frac{\|D^{n-1}f(x+h)-D^{n-1}f(x)-L(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0\, ?$$ And even in that formula, I don't know how you take account of the fact that the $n$th derivative is evaluated at $n$ different points (or vectors?). Also @Bye_World mentions ""the isomorphism $\mathcal L(X,\mathcal L(X,Y)) \simeq \operatorname{Bil}(X\times X\to Y)$"".  What does that mean and how is it related?",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
4,"If $\sum n a_n$ converges, then $\sum a_n$ converges and $\sum |a_n|^p$ converges for $p>1$.","If  converges, then  converges and  converges for .",\sum n a_n \sum a_n \sum |a_n|^p p>1,"As in the title, consider the problem: If $\sum n a_n$ converges, then $\sum a_n$ converges and $\sum |a_n|^p$ converges for $p>1$. My intuition for the first part of the proof is to use this equality, $$\sum a_n = \sum n a_n -\big(\sum a_n(n-1)\big) $$ Obviously the first part of the RHS converges by assumption, and I want to say the second part of the RHS does as well, however I don't know if it's entirely obvious that it does. Is this reasoning valid? And if there is a more clever way, please let me know. Additionally, I'm not sure where to begin in showing that the hypothesis implies $\sum|a_n|^p$ converges. All help is greatly appreciated.","As in the title, consider the problem: If $\sum n a_n$ converges, then $\sum a_n$ converges and $\sum |a_n|^p$ converges for $p>1$. My intuition for the first part of the proof is to use this equality, $$\sum a_n = \sum n a_n -\big(\sum a_n(n-1)\big) $$ Obviously the first part of the RHS converges by assumption, and I want to say the second part of the RHS does as well, however I don't know if it's entirely obvious that it does. Is this reasoning valid? And if there is a more clever way, please let me know. Additionally, I'm not sure where to begin in showing that the hypothesis implies $\sum|a_n|^p$ converges. All help is greatly appreciated.",,"['real-analysis', 'sequences-and-series']"
5,"Suppose that $(s_n)$ converges to $s$, $(t_n)$ converges to $t$, and $s_n \leq t_n \: \forall \: n$. Prove that $s \leq t$.","Suppose that  converges to ,  converges to , and . Prove that .",(s_n) s (t_n) t s_n \leq t_n \: \forall \: n s \leq t,"I'm stuck with the proof of the following: Suppose that $(s_n)$ converges to $s$, $(t_n)$ converges to $t$, and $s_n \leq t_n \: \forall \: n$. Prove that $s \leq t$. I've tried starting with $s_n \leq t_n \: \forall : n$ and the definitions of each limit (i.e. $|s_n - s| \leq \epsilon \: \forall \: n > N_1$), but I'm not really getting very far. Any help is appreciated!","I'm stuck with the proof of the following: Suppose that $(s_n)$ converges to $s$, $(t_n)$ converges to $t$, and $s_n \leq t_n \: \forall \: n$. Prove that $s \leq t$. I've tried starting with $s_n \leq t_n \: \forall : n$ and the definitions of each limit (i.e. $|s_n - s| \leq \epsilon \: \forall \: n > N_1$), but I'm not really getting very far. Any help is appreciated!",,"['real-analysis', 'sequences-and-series', 'limits', 'inequality']"
6,"$\{ (x,y) \in R^2 \mid x^2 + y^2 -2x + 4y - 11 = 0 \}$ is closed and bounded",is closed and bounded,"\{ (x,y) \in R^2 \mid x^2 + y^2 -2x + 4y - 11 = 0 \}","As a part of an exercise It would help me if I could prove formally that the set $\{ (x,y) \in R^2 \mid x^2 + y^2 -2x + 4y - 11  = 0 \}$ is closed and bounded. Plotting it with a software I can see this immediately but I am bit doubtful on a formal proof, any help?","As a part of an exercise It would help me if I could prove formally that the set $\{ (x,y) \in R^2 \mid x^2 + y^2 -2x + 4y - 11  = 0 \}$ is closed and bounded. Plotting it with a software I can see this immediately but I am bit doubtful on a formal proof, any help?",,"['real-analysis', 'compactness']"
7,Is every Cauchy sequence in a non-complete metric space convergent?,Is every Cauchy sequence in a non-complete metric space convergent?,,"A metric space $X$ is called complete if every Cauchy sequence in $X$ has a limit in $X$. For a non-complete metric space $X$, can we say that every Cauchy sequence is convergent? (even though the limit is not in $X$) In other word, is every Cauchy sequence convergent?","A metric space $X$ is called complete if every Cauchy sequence in $X$ has a limit in $X$. For a non-complete metric space $X$, can we say that every Cauchy sequence is convergent? (even though the limit is not in $X$) In other word, is every Cauchy sequence convergent?",,"['real-analysis', 'limits']"
8,"Why is $\sinh$ often pronounced ""shine""? [closed]","Why is  often pronounced ""shine""? [closed]",\sinh,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 8 years ago . Improve this question I talked to some guys from the UK and they told me that they would pronounce $\sinh$ as ""shine"". I am not a native English speaker so I don't know, but in my country we call this function ""sintsh"" with a $sh$ as in ""sharp"" and a $s$ as in ""saw"". This seems to be the natural thing to do if you have not a clue how to call this function. Despite, in my country people try to avoid giving this function a name because they are afraid of making a fool out of themselves. My question is: What is the standard name for this function and where does this ""shine"" come from?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 8 years ago . Improve this question I talked to some guys from the UK and they told me that they would pronounce $\sinh$ as ""shine"". I am not a native English speaker so I don't know, but in my country we call this function ""sintsh"" with a $sh$ as in ""sharp"" and a $s$ as in ""saw"". This seems to be the natural thing to do if you have not a clue how to call this function. Despite, in my country people try to avoid giving this function a name because they are afraid of making a fool out of themselves. My question is: What is the standard name for this function and where does this ""shine"" come from?",,"['real-analysis', 'functions', 'terminology', 'hyperbolic-functions', 'pronunciation']"
9,Differentiablility of a function,Differentiablility of a function,,"Given: $f(x)=\cos(1/x)$ When $x \neq 0$ and zero at $x=0$, is: $$ F(x)= \int^x_0f(t)\,dt $$ differentiable at zero? I believe this is differntiable, since intuitively we are shrinking the area we are integrating over so it should approach zero, but I could use a hand starting the proof.","Given: $f(x)=\cos(1/x)$ When $x \neq 0$ and zero at $x=0$, is: $$ F(x)= \int^x_0f(t)\,dt $$ differentiable at zero? I believe this is differntiable, since intuitively we are shrinking the area we are integrating over so it should approach zero, but I could use a hand starting the proof.",,"['real-analysis', 'integration']"
10,Evaluating $\int_0^1 \frac{t^{a-1}}{1-t}-\frac{ct^{b-1}}{1-t^c}\ dt$,Evaluating,\int_0^1 \frac{t^{a-1}}{1-t}-\frac{ct^{b-1}}{1-t^c}\ dt,At first sight it looks like the integral below $$\int_0^1 \frac{t^{a-1}}{1-t}-\frac{ct^{b-1}}{1-t^c}\ dt$$ can be evaluated by using some geometric series. What else can we do? Is there a fast easy way to finish it?,At first sight it looks like the integral below $$\int_0^1 \frac{t^{a-1}}{1-t}-\frac{ct^{b-1}}{1-t^c}\ dt$$ can be evaluated by using some geometric series. What else can we do? Is there a fast easy way to finish it?,,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
11,Smooth function proof,Smooth function proof,,"Given function $f\colon \mathbb{R}\to \mathbb{R}$ be defined by $$ f(x)=\begin{cases}          e^{-\frac{1}{x^2}} & \text{if $x > 0$;}\\         0 & \text{if $x \leq 0$.}      \end{cases} $$ a) Show that $g(x)=f(x-a)f(b-x)$ is a smooth function, positive on $(a,b)$ for $a<b$ and zero else where then $$ h(x)=\frac{\displaystyle\int_{-\infty}^{x} g(t)\,dt}{\displaystyle\int_{-\infty}^{\infty} g(t)\,dt} $$ is smooth function satisfying $h(x)=0$ for $x<a$ , $h(x)=1$ for $x>b$ and $0<h(x)<1$ for $x \in (a,b)$ . b) Construct a smooth function on $\mathbb{R}^k$ that equals 1 on the ball of radius $a$ and zero outside the ball of radius $b$ and is strictly between $0$ and $1$ at intermediate points $(0<a<b)$ Here is what I got so far First I need to show that $f(x)$ is smooth. For $x < 0$ , $f(x)$ is differentiable. For $x>0$ , same thing happen. For $x=0$ $\displaystyle\lim _{h\to0^-} \frac{e^{-\frac{1}{h^2}}}{h}=0$ also by L'Hospital rule $\displaystyle\lim _{h\to0^+} \frac{e^{-\frac{1}{h^2}}}{h}=0$ by L'Hospital rule So for $x=0$ , $f(x)$ also differentiable. Now we need to check if $f'$ is continuous. For $x>0$ and $x<0$ , f(x) is continuous, by calculus rule. At $x=0$ we have $f'(x)= -\frac {2}{x^3} e^{-\frac{1}{x^2}}$ so by L'Hospital rule $\displaystyle \lim _{x\to0^+}  -\frac {2}{x^3} e^{-\frac{1}{x^2}}=0=\lim _{x\to0^-}  -\frac {2}{x^3} e^{-\frac{1}{x^2}}$ so $f'$ is continuous at $x=0$ . Hence $f(x)$ is smooth. Since $f(x)$ is smooth $g(x)$ is smooth. Now I'm stuck","Given function be defined by a) Show that is a smooth function, positive on for and zero else where then is smooth function satisfying for , for and for . b) Construct a smooth function on that equals 1 on the ball of radius and zero outside the ball of radius and is strictly between and at intermediate points Here is what I got so far First I need to show that is smooth. For , is differentiable. For , same thing happen. For also by L'Hospital rule by L'Hospital rule So for , also differentiable. Now we need to check if is continuous. For and , f(x) is continuous, by calculus rule. At we have so by L'Hospital rule so is continuous at . Hence is smooth. Since is smooth is smooth. Now I'm stuck","f\colon \mathbb{R}\to \mathbb{R} 
f(x)=\begin{cases}
         e^{-\frac{1}{x^2}} & \text{if x > 0;}\\
        0 & \text{if x \leq 0.}
     \end{cases}
 g(x)=f(x-a)f(b-x) (a,b) a<b 
h(x)=\frac{\displaystyle\int_{-\infty}^{x} g(t)\,dt}{\displaystyle\int_{-\infty}^{\infty} g(t)\,dt}
 h(x)=0 x<a h(x)=1 x>b 0<h(x)<1 x \in (a,b) \mathbb{R}^k a b 0 1 (0<a<b) f(x) x < 0 f(x) x>0 x=0 \displaystyle\lim _{h\to0^-} \frac{e^{-\frac{1}{h^2}}}{h}=0 \displaystyle\lim _{h\to0^+} \frac{e^{-\frac{1}{h^2}}}{h}=0 x=0 f(x) f' x>0 x<0 x=0 f'(x)= -\frac {2}{x^3} e^{-\frac{1}{x^2}} \displaystyle \lim _{x\to0^+}  -\frac {2}{x^3} e^{-\frac{1}{x^2}}=0=\lim _{x\to0^-}  -\frac {2}{x^3} e^{-\frac{1}{x^2}} f' x=0 f(x) f(x) g(x)","['calculus', 'real-analysis']"
12,What is the intuition and motivation behind a norm on a space?,What is the intuition and motivation behind a norm on a space?,,"I have a question regarding norms. I am looking for an intuitive understanding of what norms do. From what I know so far, in any arbitrary space, if I have a norm, then it appears that it allows me to define distances. That is, if I have two points in the space, I know where they are in relation to one another. Is the motivation then that I can sort of ""classify"" and discriminate where things are once I have a metric or norm on the space? Also, if a space doesnt have a metric or norm, what would that mean? Thank you","I have a question regarding norms. I am looking for an intuitive understanding of what norms do. From what I know so far, in any arbitrary space, if I have a norm, then it appears that it allows me to define distances. That is, if I have two points in the space, I know where they are in relation to one another. Is the motivation then that I can sort of ""classify"" and discriminate where things are once I have a metric or norm on the space? Also, if a space doesnt have a metric or norm, what would that mean? Thank you",,"['real-analysis', 'linear-algebra']"
13,Proof of Riemann-Lebesgue lemma,Proof of Riemann-Lebesgue lemma,,"I read a book, and this mention to the following lemma of Rieman-Lebesgue type. Lemma. Let $-\infty<a<b<\infty$ and $f(x,y):[a,b]^2\to\mathbb R$ be an integrable and nonnegative function. Then we the followings. a) $\lim\limits_{r\to\infty}\int\limits_{[a,b]^2}f(x,y)\sin\left(\pi r x\right) \, dx \, dy=0$ b) $\lim\limits_{r\to\infty}\int\limits_{[a,b]^2}f(x,y)\sin\left(\pi r x\right)\sin(\pi ry) \, dx \, dy=0$. I try to find a proof, my idea is to use step function, but I fail. Does anyone know some proof for this lemma? Thank you.","I read a book, and this mention to the following lemma of Rieman-Lebesgue type. Lemma. Let $-\infty<a<b<\infty$ and $f(x,y):[a,b]^2\to\mathbb R$ be an integrable and nonnegative function. Then we the followings. a) $\lim\limits_{r\to\infty}\int\limits_{[a,b]^2}f(x,y)\sin\left(\pi r x\right) \, dx \, dy=0$ b) $\lim\limits_{r\to\infty}\int\limits_{[a,b]^2}f(x,y)\sin\left(\pi r x\right)\sin(\pi ry) \, dx \, dy=0$. I try to find a proof, my idea is to use step function, but I fail. Does anyone know some proof for this lemma? Thank you.",,"['real-analysis', 'analysis', 'reference-request', 'lebesgue-integral', 'harmonic-analysis']"
14,Fundamental theorem of algebra: a proof for undergrads?,Fundamental theorem of algebra: a proof for undergrads?,,The fundamental theorem of algebra is the statement that a complex polynomial of positive degree has at least one root. I do not know complex analysis but I searched for proofs of the statement and came across proofs using complex analysis which seemed rather short and elegant. This is to say: I am aware that there exist very easy proofs using tools of complex analysis. Since I do not know complex analysis yet I started to wonder if there are any known proofs that are short and easy that use only tools that a first year undergrad knows? Like real analysis and linear algebra?,The fundamental theorem of algebra is the statement that a complex polynomial of positive degree has at least one root. I do not know complex analysis but I searched for proofs of the statement and came across proofs using complex analysis which seemed rather short and elegant. This is to say: I am aware that there exist very easy proofs using tools of complex analysis. Since I do not know complex analysis yet I started to wonder if there are any known proofs that are short and easy that use only tools that a first year undergrad knows? Like real analysis and linear algebra?,,"['real-analysis', 'linear-algebra', 'alternative-proof']"
15,Continuous extension of a function,Continuous extension of a function,,"Can anybody help me with this problem? Justify whether the following statement is true or false: Every continuous function on $\Bbb Q\cap [0,1]$ can be extended to a continuous function on $[0,1]$ . Any help will be appreciated.",Can anybody help me with this problem? Justify whether the following statement is true or false: Every continuous function on can be extended to a continuous function on . Any help will be appreciated.,"\Bbb Q\cap [0,1] [0,1]",['real-analysis']
16,Weierstrass Approximation Theorem for continuous functions on open interval,Weierstrass Approximation Theorem for continuous functions on open interval,,"I am studying for my introductory real analysis final exam, and here is a problem I am somewhat stuck on. It is Question 2, in page 3 of the following past exam (no answer key unfortunately!): http://www.math.ubc.ca/Ugrad/pastExams/Math_321_April_2006.pdf Give an example of each of the following, together with a brief explanation of your example. If an example does not exist, explain why not. (c) A continuous function $f : (−1,1) → \mathbb{R}$ that cannot be uniformly approximated by a polynomial. By Weierstrass Approximation Theorem, every continuous real-valued function on closed interval can be uniformly approximated by a sequence of polynomials. Since in this question the domain of the function is an open interval $(-1, 1)$ , I have a feeling that such example must exist. My attempts: The proof of Weierstrass approximation theory uses the fact that a continuous function a compact set (a closed interval by Heine-Borel Theorem) achieves a maximum, so we can guess that the example we are looking after will not achieve a maximum on $(-1, 1)$ . Such example of continuous function is $$ f(x)=\frac{1}{x+1} $$ So now my question: is it true $f$ cannot be uniformly approximated by a sequence of polynomials? And if so, how do proceed to prove such a statement? Thanks!","I am studying for my introductory real analysis final exam, and here is a problem I am somewhat stuck on. It is Question 2, in page 3 of the following past exam (no answer key unfortunately!): http://www.math.ubc.ca/Ugrad/pastExams/Math_321_April_2006.pdf Give an example of each of the following, together with a brief explanation of your example. If an example does not exist, explain why not. (c) A continuous function that cannot be uniformly approximated by a polynomial. By Weierstrass Approximation Theorem, every continuous real-valued function on closed interval can be uniformly approximated by a sequence of polynomials. Since in this question the domain of the function is an open interval , I have a feeling that such example must exist. My attempts: The proof of Weierstrass approximation theory uses the fact that a continuous function a compact set (a closed interval by Heine-Borel Theorem) achieves a maximum, so we can guess that the example we are looking after will not achieve a maximum on . Such example of continuous function is So now my question: is it true cannot be uniformly approximated by a sequence of polynomials? And if so, how do proceed to prove such a statement? Thanks!","f : (−1,1) → \mathbb{R} (-1, 1) (-1, 1)  f(x)=\frac{1}{x+1}  f","['real-analysis', 'approximation']"
17,Prove that the following set is dense,Prove that the following set is dense,,"It is hard for me to show that the set $\{\sqrt{m}-\sqrt{n}; m,n\in \Bbb N\}$ is dense in $\Bbb R$. Please help me.","It is hard for me to show that the set $\{\sqrt{m}-\sqrt{n}; m,n\in \Bbb N\}$ is dense in $\Bbb R$. Please help me.",,['real-analysis']
18,Proof of the inequality $e^x-x>0$ for all $x\in \Bbb R.$,Proof of the inequality  for all,e^x-x>0 x\in \Bbb R.,Prove that the following inequality is true for all real numbers $$e^x-x>0.$$,Prove that the following inequality is true for all real numbers $$e^x-x>0.$$,,"['calculus', 'real-analysis', 'inequality', 'exponential-function']"
19,What's the reason why this sequence of function doesn't converge uniformly to $f$?,What's the reason why this sequence of function doesn't converge uniformly to ?,f,"Consider $f_n = \sqrt[n]{x}$ on $[0,1]$ So it converges to the step function $f = 0$ if $x = 0$ and $f=1$ otherwise I could see why it doesn't converge if i draw an epsilon rectangle over one part since for each $n$, $f_n$ lies completely outside of the function. If I draw an epsilon rectangle over the whole $f$, I don't see why this isn't uniform convergence EDIT: I got this from Spivak, so keep it at that level please... Added question : if $f_n\nrightarrow f$ for some $x \in \mathbb{R}$, can I conclude that it is not uniformly convergent over $\mathbb{R}$?","Consider $f_n = \sqrt[n]{x}$ on $[0,1]$ So it converges to the step function $f = 0$ if $x = 0$ and $f=1$ otherwise I could see why it doesn't converge if i draw an epsilon rectangle over one part since for each $n$, $f_n$ lies completely outside of the function. If I draw an epsilon rectangle over the whole $f$, I don't see why this isn't uniform convergence EDIT: I got this from Spivak, so keep it at that level please... Added question : if $f_n\nrightarrow f$ for some $x \in \mathbb{R}$, can I conclude that it is not uniformly convergent over $\mathbb{R}$?",,"['real-analysis', 'sequences-and-series']"
20,Limit superior and inferior reference request.,Limit superior and inferior reference request.,,"Is there any book where I can find theory on the limit superior and limit inferior, plus a nice deal of excercises in the same spirit as Spivak's type of excercises?","Is there any book where I can find theory on the limit superior and limit inferior, plus a nice deal of excercises in the same spirit as Spivak's type of excercises?",,"['real-analysis', 'limits', 'reference-request', 'limsup-and-liminf']"
21,Looking for Proofs Of Basic Properties Of Real Numbers,Looking for Proofs Of Basic Properties Of Real Numbers,,"I have just begun my study of complex numbers and I learned where imaginary numbers came from and their importance. However there's one thing that I need to clarify and that is the properties of real numbers and their proofs. Closure Laws For all $a,b \in \mathbb{R}$, $a+b$, $a-b$, $ab$, $a/b$ are real numbers. Thus $\mathbb{R}$ is closed under four fundamental operations. Commutative Laws For all $a,b \in \mathbb{R}$ $a+b = b+a$ and $ab = ba$. Associative Laws For all $a,b,c \in \mathbb{R}$ $a+(b+c) = (a+b)+c$ and $a(bc) = (ab)c$. Additive Identity For all $a \in \mathbb{R}$ there exists $0\in \mathbb{R}$ such that $a+0 = 0+a = a$. Additive inverse For all $a \in \mathbb{R}$ there exists a $b \in \mathbb{R}$ such that $a+b = b+a = 0$, the additive identity $b = -a$ is called the additive inverse or the negative of $a$. and similarly Multiplicative Identity, Multiplicative inverse, Distributive Law, Trichotomy Law, Transitivity of order, Monotone Law of Addition, Monotone law of multiplication. I understand that the above laws hold good throughout mathematics. Should these laws be accepted as being true ""on faith"" or are there proofs? If yes, I am curious to know the proofs. As per my understanding no textbook has ever talked about proofs for these.","I have just begun my study of complex numbers and I learned where imaginary numbers came from and their importance. However there's one thing that I need to clarify and that is the properties of real numbers and their proofs. Closure Laws For all $a,b \in \mathbb{R}$, $a+b$, $a-b$, $ab$, $a/b$ are real numbers. Thus $\mathbb{R}$ is closed under four fundamental operations. Commutative Laws For all $a,b \in \mathbb{R}$ $a+b = b+a$ and $ab = ba$. Associative Laws For all $a,b,c \in \mathbb{R}$ $a+(b+c) = (a+b)+c$ and $a(bc) = (ab)c$. Additive Identity For all $a \in \mathbb{R}$ there exists $0\in \mathbb{R}$ such that $a+0 = 0+a = a$. Additive inverse For all $a \in \mathbb{R}$ there exists a $b \in \mathbb{R}$ such that $a+b = b+a = 0$, the additive identity $b = -a$ is called the additive inverse or the negative of $a$. and similarly Multiplicative Identity, Multiplicative inverse, Distributive Law, Trichotomy Law, Transitivity of order, Monotone Law of Addition, Monotone law of multiplication. I understand that the above laws hold good throughout mathematics. Should these laws be accepted as being true ""on faith"" or are there proofs? If yes, I am curious to know the proofs. As per my understanding no textbook has ever talked about proofs for these.",,"['real-analysis', 'soft-question', 'proof-writing']"
22,$e^{e^x} = 10^{10} x^{10}e^{10^{10} x^{10}}$ - Find $x$ (G.H. Hardy),- Find  (G.H. Hardy),e^{e^x} = 10^{10} x^{10}e^{10^{10} x^{10}} x,Approximate the large positive root of the equation $$e^{e^x} = 10^{10} x^{10}e^{10^{10} x^{10}}$$ (attributed to G.H. Hardy) This is a great problem which I can find no verifiable source or discussion of online.  Is there a source where Hardy actually asks this question?  How did Hardy (or others) approach it?,Approximate the large positive root of the equation (attributed to G.H. Hardy) This is a great problem which I can find no verifiable source or discussion of online.  Is there a source where Hardy actually asks this question?  How did Hardy (or others) approach it?,e^{e^x} = 10^{10} x^{10}e^{10^{10} x^{10}},"['real-analysis', 'calculus', 'reference-request', 'contest-math']"
23,How to reasonably (numerically) estimate $n\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t+\ldots+{{t^{m-1}}\over{(m-1)!}}\right)\right)^n\right)dt$?,How to reasonably (numerically) estimate ?,n\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t+\ldots+{{t^{m-1}}\over{(m-1)!}}\right)\right)^n\right)dt,"Recently in doing some expected value calculations, I've derived the following two integrals: $$6\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t\right)\right)^6\right)dt$$ $$6\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t + {{t^2}\over2}\right)\right)^6\right)dt$$ Plugging these into Wolfram Alpha can get us numerical answers. For instance, the first integral comes out to about $24.1$ , and the second integral comes out to about $32.7$ . However, I'm wondering if anyone can give a reasonable numerical estimate for these $2$ integrals from first principles (pencil and paper) without using a calculator, Wolfram Alpha, or a computer. I've tried but made little to no progress, and I consulted some nearby PhD students and they didn't know either, so asking here.","Recently in doing some expected value calculations, I've derived the following two integrals: Plugging these into Wolfram Alpha can get us numerical answers. For instance, the first integral comes out to about , and the second integral comes out to about . However, I'm wondering if anyone can give a reasonable numerical estimate for these integrals from first principles (pencil and paper) without using a calculator, Wolfram Alpha, or a computer. I've tried but made little to no progress, and I consulted some nearby PhD students and they didn't know either, so asking here.",6\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t\right)\right)^6\right)dt 6\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t + {{t^2}\over2}\right)\right)^6\right)dt 24.1 32.7 2,"['real-analysis', 'calculus', 'probability', 'integration', 'estimation']"
24,Can a graph of a continuous function intersect its vertical asymptote?,Can a graph of a continuous function intersect its vertical asymptote?,,"I know a graph can intersect its horizontal asymptote ( 1 , 2 , 3 ). I think it's possible for a graph of a function to intersect its vertical asymptote. Example: Define $f:\mathbb{R}\rightarrow \mathbb{R}$ by $f(x)=1/x$ for $x\neq 0$ and $f(x)=0$ for $x=0$ . Then $x=0$ is a vertical asymptote for and intersects $f$ . (Correct me if this example is mistaken.) I think though that it's not possible for a graph of a continuous function to intersect its vertical asymptote. Is this true? How do I prove this?","I know a graph can intersect its horizontal asymptote ( 1 , 2 , 3 ). I think it's possible for a graph of a function to intersect its vertical asymptote. Example: Define by for and for . Then is a vertical asymptote for and intersects . (Correct me if this example is mistaken.) I think though that it's not possible for a graph of a continuous function to intersect its vertical asymptote. Is this true? How do I prove this?",f:\mathbb{R}\rightarrow \mathbb{R} f(x)=1/x x\neq 0 f(x)=0 x=0 x=0 f,"['real-analysis', 'limits']"
25,Can we define the set of reals as the set containing all the convergences of the sequences?,Can we define the set of reals as the set containing all the convergences of the sequences?,,"My analysis teacher defined the set of reals as, and I quote, a collection of finite numbers $\{A_0, A_1, ..., A_n\}$ and a collection of finite or infinite numbers $\{d_1, d_2, ..., d_m, ...\}$ all in the range $0$ to $9$ ; we match this collection with $x_+=+A_n...A_1A_0.d_1d_2...d_m$ a positive real $x_-=-A_n...A_1A_0.d_1d_2...d_m$ a negative real For example, $\pi=3.141...$ is defined such as $A_0=3$ , $d_1=1$ , $d_2=4$ , $d_3=1$ , ... I find this definition very strange, as it seems to be based on syntax alone (or almost). In the rest of the course, we went on to discuss Cauchy sequences and the Cauchy criterion and what it implies: The sequence $(U_n)_{n\in\mathbb{N}}$ converges if and only if it is a Cauchy one. This equivalence gives $\mathbb{R}$ the property of being a complete set, i.e. any Cauchy sequence of real numbers converges in $\mathbb{R}$ . My intuition then led me to think that the set of reals could be defined as the set containing all the limits of the real Cauchy suites. However, this is only an intuition and I don't know how to show this, I haven't found any result on the internet. I wonder therefore if this intuition is good, if not (or if it is, because it is always interesting), I would like to know in what other ways we could define the reals, other than by the definition I gave above. PS: obviously, we could define $\mathbb{R}$ in ensemblistic terms, and say that it contains all naturals, integers, rationals and irrationals, but I was looking for another definition than in terms of sets.","My analysis teacher defined the set of reals as, and I quote, a collection of finite numbers and a collection of finite or infinite numbers all in the range to ; we match this collection with a positive real a negative real For example, is defined such as , , , , ... I find this definition very strange, as it seems to be based on syntax alone (or almost). In the rest of the course, we went on to discuss Cauchy sequences and the Cauchy criterion and what it implies: The sequence converges if and only if it is a Cauchy one. This equivalence gives the property of being a complete set, i.e. any Cauchy sequence of real numbers converges in . My intuition then led me to think that the set of reals could be defined as the set containing all the limits of the real Cauchy suites. However, this is only an intuition and I don't know how to show this, I haven't found any result on the internet. I wonder therefore if this intuition is good, if not (or if it is, because it is always interesting), I would like to know in what other ways we could define the reals, other than by the definition I gave above. PS: obviously, we could define in ensemblistic terms, and say that it contains all naturals, integers, rationals and irrationals, but I was looking for another definition than in terms of sets.","\{A_0, A_1, ..., A_n\} \{d_1, d_2, ..., d_m, ...\} 0 9 x_+=+A_n...A_1A_0.d_1d_2...d_m x_-=-A_n...A_1A_0.d_1d_2...d_m \pi=3.141... A_0=3 d_1=1 d_2=4 d_3=1 (U_n)_{n\in\mathbb{N}} \mathbb{R} \mathbb{R} \mathbb{R}","['real-analysis', 'elementary-set-theory', 'real-numbers']"
26,How to find a real sequence $x_n$ such that $n x_n \to 0$ as $n \to \infty$ and $\sum_{n=1}^{\infty} x_n$ diverges?,How to find a real sequence  such that  as  and  diverges?,x_n n x_n \to 0 n \to \infty \sum_{n=1}^{\infty} x_n,"I'm stuck on trying to find a real sequence $x_n$ such that $n x_n \to 0$ as $n \to \infty$ and $\sum_{n=1}^{\infty} x_n$ diverges. Here's what I have tried so far: I've started with trying to find $x_n$ as a real-valued function of $n$ . In order to have $n x_n \to 0$ , we require $$\frac{x_n}{1/n} \to 0$$ and since the denominator tends to $0$ as $n$ increases, we require that $x_n \to 0$ . We also require that $x_n$ tends to $0$ ""faster"" than $1/n$ does, in order for the above quotient to go to $0$ . However if you try functions such as $x_n=1/n^2$ or $x_n=n/2^n$ (for example) which tend to $0$ faster than $1/n$ does, then you see that $\sum_{n=1}^{\infty} x_n$ converges and I haven't been able to find a function for $x_n$ that satisfies the above conditions and also has a divergent infinite sum. It's certainly possible that the form for $x_n$ might not be a simple elementary function of $n$ , but I don't know how you'd go about finding any ""unusual constructions"" that work.","I'm stuck on trying to find a real sequence such that as and diverges. Here's what I have tried so far: I've started with trying to find as a real-valued function of . In order to have , we require and since the denominator tends to as increases, we require that . We also require that tends to ""faster"" than does, in order for the above quotient to go to . However if you try functions such as or (for example) which tend to faster than does, then you see that converges and I haven't been able to find a function for that satisfies the above conditions and also has a divergent infinite sum. It's certainly possible that the form for might not be a simple elementary function of , but I don't know how you'd go about finding any ""unusual constructions"" that work.",x_n n x_n \to 0 n \to \infty \sum_{n=1}^{\infty} x_n x_n n n x_n \to 0 \frac{x_n}{1/n} \to 0 0 n x_n \to 0 x_n 0 1/n 0 x_n=1/n^2 x_n=n/2^n 0 1/n \sum_{n=1}^{\infty} x_n x_n x_n n,"['real-analysis', 'sequences-and-series']"
27,Limit of $n^{\frac{1}{n}}$.,Limit of .,n^{\frac{1}{n}},"We have to prove that $n^{\frac{1}{n}}$ converges to $1$ . I have proved it using the binomial theorem where we can substitute $(1+t)$ in place of $n$ and proceed forward. However along with the question another approach was mentioned where we can make use of the Monotone Convergence Theorem. For this approach, I proved that $a_n=\left(1+\frac{1}{n}\right)^n$ is increasing and using $a_n<a_{n+1}$ I proved that $n^{\frac{1}{n}}$ is decreasing after the third term. Also it is bounded as all terms are greater than $0$ . So by MCT it should be convergent. But I am not able to evaluate the limit and am only able to prove its existence here. Please help.","We have to prove that converges to . I have proved it using the binomial theorem where we can substitute in place of and proceed forward. However along with the question another approach was mentioned where we can make use of the Monotone Convergence Theorem. For this approach, I proved that is increasing and using I proved that is decreasing after the third term. Also it is bounded as all terms are greater than . So by MCT it should be convergent. But I am not able to evaluate the limit and am only able to prove its existence here. Please help.",n^{\frac{1}{n}} 1 (1+t) n a_n=\left(1+\frac{1}{n}\right)^n a_n<a_{n+1} n^{\frac{1}{n}} 0,['real-analysis']
28,How many irrationals are there which are unique upto addition with rationals?,How many irrationals are there which are unique upto addition with rationals?,,"I'm studying real analysis and I know from previous courses that there are countably infinite rationals but uncountably infinite irrationals. However, I haven't done a formal proof about uncountability of irrationals. I've been thinking about which irrationals are ""unique"" (in terms of being able to be expressed in terms of other irrationals / rationals), and for this informal thought, I've come up with the following situation which I'll describe formally: Suppose we define equivalence classes on irrationals, such that $$[r] = \{ p + r : \forall p \in \mathbb{Q}  \}$$ Or stated in other words, $$x \in [y] \lor y \in [x] \rightarrow x - y \in \mathbb{Q}$$ How many such distinct equivalent classes would exist? Countably infinite? uncountably infinite?","I'm studying real analysis and I know from previous courses that there are countably infinite rationals but uncountably infinite irrationals. However, I haven't done a formal proof about uncountability of irrationals. I've been thinking about which irrationals are ""unique"" (in terms of being able to be expressed in terms of other irrationals / rationals), and for this informal thought, I've come up with the following situation which I'll describe formally: Suppose we define equivalence classes on irrationals, such that Or stated in other words, How many such distinct equivalent classes would exist? Countably infinite? uncountably infinite?",[r] = \{ p + r : \forall p \in \mathbb{Q}  \} x \in [y] \lor y \in [x] \rightarrow x - y \in \mathbb{Q},['real-analysis']
29,Understanding a proof about a set being closed,Understanding a proof about a set being closed,,"Let $r > 0$ be a positive number, and define $F = \{u \in  \mathbb{R}^{n} \mid \|u\| \leq r\}$ . Prove $F$ is closed in $\mathbb{R}^{n}$ . Proof: We want to show that if a sequence $\{u_{k}\}$ lies in $F$ and $\lim_{k\to\infty} u_{k} = u$ , then $u \in F$ . Let $\{u_{k}\}$ be in arbitrary sequence in $F$ . By the set definition of $F$ , it follows that $\|u_{k}\| \leq r$ for each index $k$ . Thus, $$\lim_{k\to\infty} \|u_{k}\| \leq \lim_{k\to\infty} r = r.$$ From here, it suffices to show $\lim_{k\to\infty} \|u_{k}\| = \|u\|$ . The proof goes on and shows that $\lim_{k\to\infty} \|u_{k}\| = \|u\|$ My question: I don't understand why it suffices to show that the sequence of norms converges to the norm of the limit point? After they prove this fact, they say that $\lim_{k\to\infty} \|u_{k}\| = \|u\| = r$ , from which it follows that $u \in F$ , hence $F$ is closed. But, I don't get why proving this shows that $u$ is contained in $F$ .","Let be a positive number, and define . Prove is closed in . Proof: We want to show that if a sequence lies in and , then . Let be in arbitrary sequence in . By the set definition of , it follows that for each index . Thus, From here, it suffices to show . The proof goes on and shows that My question: I don't understand why it suffices to show that the sequence of norms converges to the norm of the limit point? After they prove this fact, they say that , from which it follows that , hence is closed. But, I don't get why proving this shows that is contained in .","r > 0 F = \{u \in
 \mathbb{R}^{n} \mid \|u\| \leq r\} F \mathbb{R}^{n} \{u_{k}\} F \lim_{k\to\infty} u_{k} = u u \in F \{u_{k}\} F F \|u_{k}\| \leq r k \lim_{k\to\infty} \|u_{k}\| \leq \lim_{k\to\infty} r = r. \lim_{k\to\infty} \|u_{k}\| = \|u\| \lim_{k\to\infty} \|u_{k}\| = \|u\| \lim_{k\to\infty} \|u_{k}\| = \|u\| = r u \in F F u F","['real-analysis', 'general-topology']"
30,"What does $\lim\limits_{(x,y)\rightarrow0}$ mean and how to show $ \lim\limits_{(x,y)\rightarrow0}\frac{x^3}{x^2+y^2}=0$?",What does  mean and how to show ?,"\lim\limits_{(x,y)\rightarrow0}  \lim\limits_{(x,y)\rightarrow0}\frac{x^3}{x^2+y^2}=0","Consider $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ where $$f(x,y):=\begin{cases}       \frac{x^3}{x^2+y^2} & \textit{ if }  (x,y)\neq (0,0) \\       0 & \textit{ if }  (x,y)= (0,0)     \end{cases} $$ If one wants to show the continuity of $f$, I mainly want to show that $$ \lim\limits_{(x,y)\rightarrow0}\frac{x^3}{x^2+y^2}=0$$ But what does $\lim\limits_{(x,y)\rightarrow0}$ mean? Is it equal to $\lim\limits_{(x,y)\rightarrow0}=\lim\limits_{||(x,y)||\rightarrow0}$ or does it mean $\lim\limits_{x\rightarrow0}\lim\limits_{y\rightarrow0}$? If so, how does one show that the above function tends to zero?","Consider $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ where $$f(x,y):=\begin{cases}       \frac{x^3}{x^2+y^2} & \textit{ if }  (x,y)\neq (0,0) \\       0 & \textit{ if }  (x,y)= (0,0)     \end{cases} $$ If one wants to show the continuity of $f$, I mainly want to show that $$ \lim\limits_{(x,y)\rightarrow0}\frac{x^3}{x^2+y^2}=0$$ But what does $\lim\limits_{(x,y)\rightarrow0}$ mean? Is it equal to $\lim\limits_{(x,y)\rightarrow0}=\lim\limits_{||(x,y)||\rightarrow0}$ or does it mean $\lim\limits_{x\rightarrow0}\lim\limits_{y\rightarrow0}$? If so, how does one show that the above function tends to zero?",,"['real-analysis', 'multivariable-calculus']"
31,Does absolute integrability imply integrability?,Does absolute integrability imply integrability?,,"I am quite confused about the notion of integrability. In the context of an introduction to complex analysis and Fourier transforms, I am told that if $f$, a complex or real valued function, satisfies the following: $$ \int_{-\infty}^{\infty} \lvert f(x) \rvert dx<\infty$$ then it is absolutely integrable. However, does this also imply that $f$ is integrable? What can we conclude about $f$ given the above condition on its absolute value (or modulus). I have no notions of Lebesgue integrability.","I am quite confused about the notion of integrability. In the context of an introduction to complex analysis and Fourier transforms, I am told that if $f$, a complex or real valued function, satisfies the following: $$ \int_{-\infty}^{\infty} \lvert f(x) \rvert dx<\infty$$ then it is absolutely integrable. However, does this also imply that $f$ is integrable? What can we conclude about $f$ given the above condition on its absolute value (or modulus). I have no notions of Lebesgue integrability.",,"['real-analysis', 'complex-analysis', 'lebesgue-integral']"
32,limit of $(e^x-1)^{\frac{1}{x}}$ - Without L'hopital,limit of  - Without L'hopital,(e^x-1)^{\frac{1}{x}},"I am having trouble calculating the following limit: $$\lim_{x\to\infty}(e^x-1)^{\frac{1}{x}}$$ I figured out (With help from wolfram) that the limit is $e$, but I can't understand why. I tried to use $x^a = e^{a\ln(x)}$ but that didn't help because the base is $e^x-1$ Any hint or explanation is welcome! PS: I can't use L'hopital (tagged so specifically)","I am having trouble calculating the following limit: $$\lim_{x\to\infty}(e^x-1)^{\frac{1}{x}}$$ I figured out (With help from wolfram) that the limit is $e$, but I can't understand why. I tried to use $x^a = e^{a\ln(x)}$ but that didn't help because the base is $e^x-1$ Any hint or explanation is welcome! PS: I can't use L'hopital (tagged so specifically)",,"['real-analysis', 'limits', 'exponential-function', 'limits-without-lhopital']"
33,How do you define sequences that converge to infinity?,How do you define sequences that converge to infinity?,,"For instance consider the sequence $\{1,0,2,0,3,0,4,0,..\}$ Intuitively we know that the sequence converges to $\infty$ but how do we check that rigorously. If I imitate the formal definition of convergence then I believe that we can at best come up with something like this: $(x_n)\to\infty$ if for any $\epsilon>0$ there exists $N\in\mathbb{N}$ such that for $n\geq N$ we have $x_n>\epsilon.$ Now this definiton does help us in proving the convergence of some sequences such as $x_n=\sqrt{n}$ because in this case we can let $N\geq \epsilon^2.$ However this definition fails to show that the aforementioned sequence $\{1,0,2,0,3,0,4,0,..\}$ converges to $\infty$. I am thus guessing that ""there exists""   a better definition out there. So please suggest me some references or maybe provide me with a definition that is able to take care of convergence to $\infty$ in general.","For instance consider the sequence $\{1,0,2,0,3,0,4,0,..\}$ Intuitively we know that the sequence converges to $\infty$ but how do we check that rigorously. If I imitate the formal definition of convergence then I believe that we can at best come up with something like this: $(x_n)\to\infty$ if for any $\epsilon>0$ there exists $N\in\mathbb{N}$ such that for $n\geq N$ we have $x_n>\epsilon.$ Now this definiton does help us in proving the convergence of some sequences such as $x_n=\sqrt{n}$ because in this case we can let $N\geq \epsilon^2.$ However this definition fails to show that the aforementioned sequence $\{1,0,2,0,3,0,4,0,..\}$ converges to $\infty$. I am thus guessing that ""there exists""   a better definition out there. So please suggest me some references or maybe provide me with a definition that is able to take care of convergence to $\infty$ in general.",,['real-analysis']
34,"What is $\lim_{(x,y) \to (0,0)} \arctan(xy)/\sqrt{x^2+y^2}$?",What is ?,"\lim_{(x,y) \to (0,0)} \arctan(xy)/\sqrt{x^2+y^2}","The limit is this: $$\lim\limits_{(x,y) \to (0,0)} \frac{\arctan(xy)}{\sqrt{x^2+y^2}}$$ It's not necessary to give a whole solution, I want the path to see how to solve it. I tried both with sequences characterization and definition of limit but I don't know many things to do with $\arctan(xy)$. I only know it's bounded $-{\pi \over 2} < \arctan(xy) < {\pi \over 2}$, it's odd and strictly increasing.","The limit is this: $$\lim\limits_{(x,y) \to (0,0)} \frac{\arctan(xy)}{\sqrt{x^2+y^2}}$$ It's not necessary to give a whole solution, I want the path to see how to solve it. I tried both with sequences characterization and definition of limit but I don't know many things to do with $\arctan(xy)$. I only know it's bounded $-{\pi \over 2} < \arctan(xy) < {\pi \over 2}$, it's odd and strictly increasing.",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus']"
35,Find the limit $\lim\limits_{x\to\infty} f(x)= \lim\limits_{x \to \infty} \left(\frac{x}{x+1}\right)^x$,Find the limit,\lim\limits_{x\to\infty} f(x)= \lim\limits_{x \to \infty} \left(\frac{x}{x+1}\right)^x,"I need to find the following: $$\lim_{x\to\infty} f(x)= \lim_{x \to \infty}\left (\frac{x}{x+1} \right )^x$$ I know that this limit = $\frac{1}{e}$ from plugging it into a calculator, but I have to prove it without using the fact that: $$\lim_{x\to\infty} f(x)=\lim_{x \to \infty}\left (\frac{x}{x+k} \right )^x=\frac{1}{e^k}$$ I started by exponentiating: $$\lim_{x\to\infty} f(x)=\lim_{x \to \infty}e^{\ln \left (\frac{x}{x+1} \right )^x}$$ and from here I've dropped the exponent $x$ in front of the $\ln$, and from here I'm getting stuck. Should I separate the $\ln$ like this? $$\lim_{x\to\infty} f(x)=\lim_{x \to \infty}e^{x(\ln(x)-\ln(x+1))}$$ This doesn't seem to be leading me down the right path, but I'm not sure how else to do it. Is there a way to apply L'Hopital? If so, how?","I need to find the following: $$\lim_{x\to\infty} f(x)= \lim_{x \to \infty}\left (\frac{x}{x+1} \right )^x$$ I know that this limit = $\frac{1}{e}$ from plugging it into a calculator, but I have to prove it without using the fact that: $$\lim_{x\to\infty} f(x)=\lim_{x \to \infty}\left (\frac{x}{x+k} \right )^x=\frac{1}{e^k}$$ I started by exponentiating: $$\lim_{x\to\infty} f(x)=\lim_{x \to \infty}e^{\ln \left (\frac{x}{x+1} \right )^x}$$ and from here I've dropped the exponent $x$ in front of the $\ln$, and from here I'm getting stuck. Should I separate the $\ln$ like this? $$\lim_{x\to\infty} f(x)=\lim_{x \to \infty}e^{x(\ln(x)-\ln(x+1))}$$ This doesn't seem to be leading me down the right path, but I'm not sure how else to do it. Is there a way to apply L'Hopital? If so, how?",,"['calculus', 'real-analysis', 'limits', 'exponential-function']"
36,"If $[a,b]$ is an interval of minimal length with $\int_a^bf(x)dx=\alpha$ then $f(a)=f(b)$",If  is an interval of minimal length with  then,"[a,b] \int_a^bf(x)dx=\alpha f(a)=f(b)","Suppose $f$ is a real positive continuous function on $\mathbb{R}$ with $\int_{-\infty}^{\infty}f(x)dx=1$. Let $0<\alpha<1$, and suppose $[a,b]$ is an interval of minimal length with $\int_a^bf(x)dx=\alpha$. Show that $f(a)=f(b)$. I have a lot of difficulty from the resolution. Is there someone who could give me a helping hand?","Suppose $f$ is a real positive continuous function on $\mathbb{R}$ with $\int_{-\infty}^{\infty}f(x)dx=1$. Let $0<\alpha<1$, and suppose $[a,b]$ is an interval of minimal length with $\int_a^bf(x)dx=\alpha$. Show that $f(a)=f(b)$. I have a lot of difficulty from the resolution. Is there someone who could give me a helping hand?",,['real-analysis']
37,"Prove that if A is a non-(Lebesgue)measurable set and $d(A,B)>0$, show that $A⋃B$ is non-measurable","Prove that if A is a non-(Lebesgue)measurable set and , show that  is non-measurable","d(A,B)>0 A⋃B","Prove that if A is a non-(Lebesgue)measurable set and $d(A,B)>0$, show that $A⋃B$ is non-measurable. $d(A,B)$ is the inf of distance $d(x,y)$ between two points $x\in A, y \in B$. I have tried to prove by contradiction using definition of measurable sets but was not successful. There are two versions of definitions in my textbook. These are two definitions on my textbook: a set $M$ is measurable iff. $|M⋂A|_e+|M^C⋂A|_e=|A|_e$ for any $A⊆\Bbb{R}$, or $∀ϵ>0$ there exists an open set $M⊆G$ st. $|G−M|_e<ϵ$. There is a theorem in my textbook saying that if $d(Ω_1,Ω_2 )>0$ then $|Ω_1⋃Ω_2 |_e=|Ω_1 |_e+|Ω_2 |_e$, where $|* |_e$ is outer measure. I don't know if this is useful. Thank you!","Prove that if A is a non-(Lebesgue)measurable set and $d(A,B)>0$, show that $A⋃B$ is non-measurable. $d(A,B)$ is the inf of distance $d(x,y)$ between two points $x\in A, y \in B$. I have tried to prove by contradiction using definition of measurable sets but was not successful. There are two versions of definitions in my textbook. These are two definitions on my textbook: a set $M$ is measurable iff. $|M⋂A|_e+|M^C⋂A|_e=|A|_e$ for any $A⊆\Bbb{R}$, or $∀ϵ>0$ there exists an open set $M⊆G$ st. $|G−M|_e<ϵ$. There is a theorem in my textbook saying that if $d(Ω_1,Ω_2 )>0$ then $|Ω_1⋃Ω_2 |_e=|Ω_1 |_e+|Ω_2 |_e$, where $|* |_e$ is outer measure. I don't know if this is useful. Thank you!",,"['real-analysis', 'measure-theory']"
38,Prove $\sum na_n$ converge if $\sum (a-s_n)$ converge,Prove  converge if  converge,\sum na_n \sum (a-s_n),Let $\sum a_n=a$ with terms non-negative. Let $ s_n$ the n-nth partial sum. Prove $\sum na_n$ converge if $\sum (a-s_n)$ converge,Let $\sum a_n=a$ with terms non-negative. Let $ s_n$ the n-nth partial sum. Prove $\sum na_n$ converge if $\sum (a-s_n)$ converge,,['real-analysis']
39,"When a periodic function is squared (or cubed, and so on...) does it always lose its periodicity?","When a periodic function is squared (or cubed, and so on...) does it always lose its periodicity?",,"For instance $$\sin^{2}\left(-\frac{\pi}{6}\right) = \sin^{2}\left(\frac{\pi}{6}\right)$$ i.e., $\sin^2 (x)$ is an even function and loses the $2\pi$-periodicity of $\sin x $. Is this true in general? Does $\tan^2 x$ lose the $\pi$-periodicity of $\tan x$?  The $\tan^2 x$ function still blows up wherever $\cos^2 x$ is equal to zero - I am currently studying $\tan^2 x$'s singularities to try to understand a solution to a problem I've been working on.","For instance $$\sin^{2}\left(-\frac{\pi}{6}\right) = \sin^{2}\left(\frac{\pi}{6}\right)$$ i.e., $\sin^2 (x)$ is an even function and loses the $2\pi$-periodicity of $\sin x $. Is this true in general? Does $\tan^2 x$ lose the $\pi$-periodicity of $\tan x$?  The $\tan^2 x$ function still blows up wherever $\cos^2 x$ is equal to zero - I am currently studying $\tan^2 x$'s singularities to try to understand a solution to a problem I've been working on.",,"['calculus', 'real-analysis', 'limits', 'trigonometry', 'periodic-functions']"
40,"If $f'$ is increasing and $f(0)=0$, then $f(x)/x$ is increasing [duplicate]","If  is increasing and , then  is increasing [duplicate]",f' f(0)=0 f(x)/x,"This question already has answers here : Show that if $f'$ is strictly increasing, then $\frac{f(x)}{x}$ is increasing over $(0,\infty)$ (2 answers) Closed 7 years ago . Let $a>0$ and $f:[0,a] \to \mathbb{R}$ continuous function that is twice differentiable on $(0,a).$ Also $f(0)=0$ and $f'$ is strictly increasing function on $(0,a).$ I have to show that the function $g$ defined as $$g(x) = \frac{f(x)}{x}$$ is strictly increasing on $(0,a].$ Progress : I computed $g'$ and got $g'(x) = \frac{f'(x)x-f(x)}{x^2}.$ I have to show that $g'(x)>0 \ \ \forall x \in (0,a].$  Since $x^2$ is always positive for $x \in (0,a]$ I have to show that $f'(x)x-f(x) >0 \ \ \forall x \in (0,a].$ I know both terms are positive, but I don't know how to show that $f'(x)x>f(x) \ \ \forall x \in (0,a].$","This question already has answers here : Show that if $f'$ is strictly increasing, then $\frac{f(x)}{x}$ is increasing over $(0,\infty)$ (2 answers) Closed 7 years ago . Let $a>0$ and $f:[0,a] \to \mathbb{R}$ continuous function that is twice differentiable on $(0,a).$ Also $f(0)=0$ and $f'$ is strictly increasing function on $(0,a).$ I have to show that the function $g$ defined as $$g(x) = \frac{f(x)}{x}$$ is strictly increasing on $(0,a].$ Progress : I computed $g'$ and got $g'(x) = \frac{f'(x)x-f(x)}{x^2}.$ I have to show that $g'(x)>0 \ \ \forall x \in (0,a].$  Since $x^2$ is always positive for $x \in (0,a]$ I have to show that $f'(x)x-f(x) >0 \ \ \forall x \in (0,a].$ I know both terms are positive, but I don't know how to show that $f'(x)x>f(x) \ \ \forall x \in (0,a].$",,"['real-analysis', 'inequality', 'derivatives']"
41,In-Depth Explanation of How to Do Mathematical Induction Over the Set $\mathbb{R}$ of All Real Numbers?,In-Depth Explanation of How to Do Mathematical Induction Over the Set  of All Real Numbers?,\mathbb{R},"I've seen in the answers to a few different questions here on the Mathematics Stack Exchange that one can clearly do mathematical induction over the set $\mathbb{R}$ of all real numbers.  I am, however, having quite a difficult time understanding how the methods described in both those questions' answers and some reference materials to which they link.  In particular,  I can't seem to figure out exactly how the techniques described therein parallel the methods codified in the axiom of induction for use when doing mathematical induction over the set $\mathbb{N}$ of all natural numbers. If somebody would be so kind as to provide me with a more detailed explanation of how to do mathematical induction over the set $\mathbb{R}$ of all real numbers within about the next day or so, then I would be very grateful!  The answer should be understandable by any beginning calculus student who also has a rudimentary understanding of set theory and mathematical logic.  I've provided links to both the relevant questions and whatever reference material mentioned in them that seemed like good leads when I found them no matter how inscrutable they might have been at the time. Questions About Induction Over the Real Numbers: Induction on Real Numbers Is it possible to use mathematical induction to prove a statement concerning all real numbers, not necessarily just the integers? [duplicate] Extending a theorem true over the integers to reals and complex numbers Question-Derived Reference Material: 'The Instructor's Guide to Real Induction' by Pete L. Clark P. S.:  I also have the following follow-up questions: Version of the Axiom of Induction for Real Induction? Real Induction Over Multiple Variables?","I've seen in the answers to a few different questions here on the Mathematics Stack Exchange that one can clearly do mathematical induction over the set of all real numbers.  I am, however, having quite a difficult time understanding how the methods described in both those questions' answers and some reference materials to which they link.  In particular,  I can't seem to figure out exactly how the techniques described therein parallel the methods codified in the axiom of induction for use when doing mathematical induction over the set of all natural numbers. If somebody would be so kind as to provide me with a more detailed explanation of how to do mathematical induction over the set of all real numbers within about the next day or so, then I would be very grateful!  The answer should be understandable by any beginning calculus student who also has a rudimentary understanding of set theory and mathematical logic.  I've provided links to both the relevant questions and whatever reference material mentioned in them that seemed like good leads when I found them no matter how inscrutable they might have been at the time. Questions About Induction Over the Real Numbers: Induction on Real Numbers Is it possible to use mathematical induction to prove a statement concerning all real numbers, not necessarily just the integers? [duplicate] Extending a theorem true over the integers to reals and complex numbers Question-Derived Reference Material: 'The Instructor's Guide to Real Induction' by Pete L. Clark P. S.:  I also have the following follow-up questions: Version of the Axiom of Induction for Real Induction? Real Induction Over Multiple Variables?",\mathbb{R} \mathbb{N} \mathbb{R},"['real-analysis', 'induction', 'real-numbers']"
42,Prove that Hölder condition in $\Bbb R^n$ implies continuity,Prove that Hölder condition in  implies continuity,\Bbb R^n,"$f:I\subset \Bbb R^n \rightarrow  \Bbb R^m$ is said to be Hölder continuous if $\exists$ $\alpha>0$ and $M>0$ such that $\|{f(x)-f(y)}\| \leq M\|x-y\|^\alpha$ , $ \forall x,y \in I$ , $0<\alpha\leq 1$ . Prove that $f$ is Hölder $\Rightarrow f$ is continuous. To prove that $f$ Hölder $\Rightarrow f$ continuous, it is enough to note that $\|f(x)-f(y)\| \leq M \|x-y\|^\alpha \leq M\|x-y\|$ , since $\alpha \leq 1$ . This implies that $f$ is Lipschitz $\Rightarrow f$ is continuous. But how can I prove continuity for the case in which $\alpha >1$ ? If we were on $\Bbb R$ it is clear that, by the definition of derivative, the function is constant and therefore continuous, not sure if this is the case in $\Bbb R^n$ .","is said to be Hölder continuous if and such that , , . Prove that is Hölder is continuous. To prove that Hölder continuous, it is enough to note that , since . This implies that is Lipschitz is continuous. But how can I prove continuity for the case in which ? If we were on it is clear that, by the definition of derivative, the function is constant and therefore continuous, not sure if this is the case in .","f:I\subset \Bbb R^n \rightarrow  \Bbb R^m \exists \alpha>0 M>0 \|{f(x)-f(y)}\| \leq M\|x-y\|^\alpha  \forall x,y \in I 0<\alpha\leq 1 f \Rightarrow f f \Rightarrow f \|f(x)-f(y)\| \leq M \|x-y\|^\alpha \leq M\|x-y\| \alpha \leq 1 f \Rightarrow f \alpha >1 \Bbb R \Bbb R^n","['real-analysis', 'analysis', 'functional-analysis']"
43,Closed form of $\int_0^{\frac{\pi}{3}} \log^2(\sin x) \mathrm{d}x$,Closed form of,\int_0^{\frac{\pi}{3}} \log^2(\sin x) \mathrm{d}x,"Inspired by the popularity of these kind of integrals appearing on MSE lately, I actually learned new methods to attack weird integrals by studying the beautiful answers on the similar past questions, so I conjecture $$\int_0^{\frac{\pi}{3}} \log^2(\sin x) \mathrm{d}x$$ has a closed form. How would someone approach this? PS; To be honest, I'm more interested in the methodology, as on the previous questions I lacked the mathematical background to understand them completely anyway. Which implies I don't want Cleo-like answers.","Inspired by the popularity of these kind of integrals appearing on MSE lately, I actually learned new methods to attack weird integrals by studying the beautiful answers on the similar past questions, so I conjecture $$\int_0^{\frac{\pi}{3}} \log^2(\sin x) \mathrm{d}x$$ has a closed form. How would someone approach this? PS; To be honest, I'm more interested in the methodology, as on the previous questions I lacked the mathematical background to understand them completely anyway. Which implies I don't want Cleo-like answers.",,"['calculus', 'real-analysis', 'integration']"
44,"Prove that if f(x) is integrable, then so is e^(f(x)).","Prove that if f(x) is integrable, then so is e^(f(x)).",,"So here is my question: I'm working on a homework problem that deals with Jensen's Inequality. It is a rather simple application, I believe, but I'm a little stuck. Here is the problem, along with my work. Let $f$ be integrable over $[0,1]$. Show that $\exp\left[\int_0^1 f(x) dx \right] \leq \int_0^1\exp[f(x)]dx$ On the surface, this appeared like a simple application of Jensen's Inequality, since $\phi (x) = e^x$ is convex on $\mathbb{R}$. We know that $f$ is integrable, but it remains to show that $\exp[f(x)]$ is integrable. I know that the composition ($\exp[f(x)]$) is measureable, but I don't feel like that is enough to conclude that it is integrable.","So here is my question: I'm working on a homework problem that deals with Jensen's Inequality. It is a rather simple application, I believe, but I'm a little stuck. Here is the problem, along with my work. Let $f$ be integrable over $[0,1]$. Show that $\exp\left[\int_0^1 f(x) dx \right] \leq \int_0^1\exp[f(x)]dx$ On the surface, this appeared like a simple application of Jensen's Inequality, since $\phi (x) = e^x$ is convex on $\mathbb{R}$. We know that $f$ is integrable, but it remains to show that $\exp[f(x)]$ is integrable. I know that the composition ($\exp[f(x)]$) is measureable, but I don't feel like that is enough to conclude that it is integrable.",,"['real-analysis', 'analysis', 'integral-inequality']"
45,Exponential function and uniform convergence of polynomials.,Exponential function and uniform convergence of polynomials.,,How can I prove that no sequence of polynomials converges uniformly to the exponential function? Thanks in advance for any help.,How can I prove that no sequence of polynomials converges uniformly to the exponential function? Thanks in advance for any help.,,"['real-analysis', 'sequences-and-series', 'exponential-function', 'uniform-convergence']"
46,Young's inequality for three variables,Young's inequality for three variables,,"Let $x, y, z \geqslant 0$ and let $p, q, r > 1$ be such that  $$ \frac{1}{p} + \frac{1}{q} + \frac{1}{r} = 1. $$ How can one show that under these hypotheses we have $$ xyz \leqslant \frac{x^p}{p} + \frac{y^q}{q} + \frac{z^r}{r} $$ with equality if and only if $x^p = y^q = z^r$, using twice the standard two-parameters Young's inequality which says that for all $x, y \geq 0$ and for all $p, q > 1$ for which $\frac{1}{p} + \frac{1}{q} = 1$ we have $$ xy \leqslant \frac{x^p}{p} + \frac{y^q}{q} $$ with equality if and only if $x^p = y^q$ ? I've tried to apply it twice directly, to multiply two inequalities and to add two inequalities, but in each case it gets quite messy and I can't get the desired result, even though I'm sure it should be quite simple.","Let $x, y, z \geqslant 0$ and let $p, q, r > 1$ be such that  $$ \frac{1}{p} + \frac{1}{q} + \frac{1}{r} = 1. $$ How can one show that under these hypotheses we have $$ xyz \leqslant \frac{x^p}{p} + \frac{y^q}{q} + \frac{z^r}{r} $$ with equality if and only if $x^p = y^q = z^r$, using twice the standard two-parameters Young's inequality which says that for all $x, y \geq 0$ and for all $p, q > 1$ for which $\frac{1}{p} + \frac{1}{q} = 1$ we have $$ xy \leqslant \frac{x^p}{p} + \frac{y^q}{q} $$ with equality if and only if $x^p = y^q$ ? I've tried to apply it twice directly, to multiply two inequalities and to add two inequalities, but in each case it gets quite messy and I can't get the desired result, even though I'm sure it should be quite simple.",,"['real-analysis', 'inequality', 'young-inequality']"
47,A limit without Taylor series or l'Hôpital's rule $\lim_{n\to\infty}\prod_{k=1}^{n}\cos \frac{k}{n\sqrt{n}}$,A limit without Taylor series or l'Hôpital's rule,\lim_{n\to\infty}\prod_{k=1}^{n}\cos \frac{k}{n\sqrt{n}},Computing without Taylor series or l'Hôpital's  rule $$\lim_{n\to\infty}\prod_{k=1}^{n}\cos \frac{k}{n\sqrt{n}}$$ What options would I have here? Thanks!,Computing without Taylor series or l'Hôpital's  rule $$\lim_{n\to\infty}\prod_{k=1}^{n}\cos \frac{k}{n\sqrt{n}}$$ What options would I have here? Thanks!,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
48,Prove that $f(x)=e^x$ is Riemann integrable using Riemann sums,Prove that  is Riemann integrable using Riemann sums,f(x)=e^x,Does anyone know how to prove that $f(x)=e^x$ is Riemann integrable using right or left hand Riemann sums?,Does anyone know how to prove that $f(x)=e^x$ is Riemann integrable using right or left hand Riemann sums?,,['real-analysis']
49,Proving that $\lim_{x\rightarrow0}\frac{H_x}x=\frac{\pi^2}6$,Proving that,\lim_{x\rightarrow0}\frac{H_x}x=\frac{\pi^2}6,"Yes, you read that correctly. It can be proven that $$\lim_{x\rightarrow0}\frac{H_x}x=\frac{\pi^2}6$$ Where $H_x$ is the $x$ th harmonic number and is analytically continued to all positive reals. To prove this, we use a formula in the OP of this question where it is stated that $$H_x=\sum_{k=1}^\infty\frac{x}{k(x+k)}$$ And the rest is obvious. But this proof is very boring and not intuitive. So I want to see other methods of proving this limit.","Yes, you read that correctly. It can be proven that Where is the th harmonic number and is analytically continued to all positive reals. To prove this, we use a formula in the OP of this question where it is stated that And the rest is obvious. But this proof is very boring and not intuitive. So I want to see other methods of proving this limit.",\lim_{x\rightarrow0}\frac{H_x}x=\frac{\pi^2}6 H_x x H_x=\sum_{k=1}^\infty\frac{x}{k(x+k)},"['real-analysis', 'calculus', 'limits', 'analysis', 'harmonic-numbers']"
50,How do you evaluate: $\int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}+1} \ \mathrm dx$,How do you evaluate:,\int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}+1} \ \mathrm dx,"I want to find the value of $\displaystyle \tag*{} \int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}+1} \ \mathrm dx$ At first, I solved this elementary integral: $\displaystyle \tag*{} \int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}} \ \mathrm dx$ Using the same method, I couldn't find my asked integral. Are there any ways to connect them? Any help would be appreciated.","I want to find the value of At first, I solved this elementary integral: Using the same method, I couldn't find my asked integral. Are there any ways to connect them? Any help would be appreciated.",\displaystyle \tag*{} \int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}+1} \ \mathrm dx \displaystyle \tag*{} \int _{0}^{\infty} \frac{\log x}{e^x+e^{-x}} \ \mathrm dx,"['real-analysis', 'calculus', 'integration', 'complex-analysis', 'definite-integrals']"
51,Is there a closed form for this recursion equation?,Is there a closed form for this recursion equation?,,"Given $$f(a)=\frac{1}{6} \sum _{i=1}^6 \left[1-(1-f (a-1)) \left(\frac{i}{6}\right)^a\right]$$ with $f(1)=0$ , $a\in\mathbb{N}$ and $f:\mathbb{N}\to\mathbb{Q},$ is there a way to write a closed form solution for the above recursion function. I am conducting a research and obtained the above result, but was interested if a function of $f(a)$ only in terms of $a$ exists, for arbitrary $a$ . This would save a lot of computational work for my research. This in fact, is a small result which will be used in more complex calculations, therefore a closed form would be optimal. Thank you.","Given with , and is there a way to write a closed form solution for the above recursion function. I am conducting a research and obtained the above result, but was interested if a function of only in terms of exists, for arbitrary . This would save a lot of computational work for my research. This in fact, is a small result which will be used in more complex calculations, therefore a closed form would be optimal. Thank you.","f(a)=\frac{1}{6} \sum _{i=1}^6 \left[1-(1-f (a-1)) \left(\frac{i}{6}\right)^a\right] f(1)=0 a\in\mathbb{N} f:\mathbb{N}\to\mathbb{Q}, f(a) a a","['real-analysis', 'summation', 'recurrence-relations', 'recursion']"
52,use Schwarz inequality to get $\int_x^{+\infty }f(t) dt \leq \frac{1}{1+x^2}$,use Schwarz inequality to get,\int_x^{+\infty }f(t) dt \leq \frac{1}{1+x^2},"$f(x)\ge 0,\forall x\in \Bbb R$ . If $$\int_{-\infty}^{+\infty}f(x) dx =1, \int_{-\infty}^{+\infty}xf(x) dx =0,\int_{-\infty}^{+\infty}x^2f(x) dx =1. $$ Show that , for every $x\gt0$ , $$\int_x^{+\infty }f(t) dt \leq \frac{1}{1+x^2}$$ I have a method: proof by contradiction If $a\gt0$ such that $\int_a^{+\infty }f(t) dt \gt \frac{1}{1+a^2}$ , then $$ \int_{a}^{+\infty}xf(x) dx \gt \dfrac{a}{1+a^2},\int_{a}^{+\infty}x^2f(x) dx \gt\frac{a^2}{1+a^2}. $$ so $$\int_{-\infty}^a f(x) dx \lt -\dfrac{a^2}{1+a^2}, \int_{-\infty}^a xf(x) dx \lt -\dfrac{a}{1+a^2},\int_{-\infty}^a x^2f(x) dx \lt\frac{1}{1+a^2}. $$ $$ \int_{-\infty}^a (ax+1)^2f(x) dx \lt a^2 \frac{1}{1+a^2}-2a\frac{a}{1+a^2}+\frac{a^2}{1+a^2}=0. $$ I heard that the inequality is related to probability theory，and it can't be improved, how to cite a function to get this?  Are there any other methods, use Schwarz inequality ?",". If Show that , for every , I have a method: proof by contradiction If such that , then so I heard that the inequality is related to probability theory，and it can't be improved, how to cite a function to get this?  Are there any other methods, use Schwarz inequality ?","f(x)\ge 0,\forall x\in \Bbb R \int_{-\infty}^{+\infty}f(x) dx =1, \int_{-\infty}^{+\infty}xf(x) dx =0,\int_{-\infty}^{+\infty}x^2f(x) dx =1.  x\gt0 \int_x^{+\infty }f(t) dt \leq \frac{1}{1+x^2} a\gt0 \int_a^{+\infty }f(t) dt \gt \frac{1}{1+a^2}  \int_{a}^{+\infty}xf(x) dx \gt \dfrac{a}{1+a^2},\int_{a}^{+\infty}x^2f(x) dx \gt\frac{a^2}{1+a^2}.  \int_{-\infty}^a f(x) dx \lt -\dfrac{a^2}{1+a^2}, \int_{-\infty}^a xf(x) dx \lt -\dfrac{a}{1+a^2},\int_{-\infty}^a x^2f(x) dx \lt\frac{1}{1+a^2}.   \int_{-\infty}^a (ax+1)^2f(x) dx \lt a^2 \frac{1}{1+a^2}-2a\frac{a}{1+a^2}+\frac{a^2}{1+a^2}=0. ","['real-analysis', 'calculus', 'probability-theory', 'inequality']"
53,Does the staircase paradox apply to areas or volume?,Does the staircase paradox apply to areas or volume?,,"So there is the “ staircase paradox ” that is sometimes used to “show” that $\pi = 4$ (in the case of approximating a circle), or that $\sqrt{2} = 2$ (in the case of approximating the hypotenuse of a triangle). But when we define things like the integral, don’t we also speak of approximating something, in a similar fashion to approaching the hypotenuse of a triangle by staircases from above and below? For example, for the Riemann integral, we speak of approaching the area by looking at the supremum of the lower sums and the infimum of the upper sums; if these two numbers agree, then we say it is integrable. So why do we seem to not “run into such problems” when considering area/volume (or at least, the problems we run into are not quite the same), whereas we have problems when we (naively) consider lengths? Is it a matter of defining the area/volume as the limit (but this doesn’t seem satisfactory, because after all isn't a big purpose to model phenomena and solve problems, and avoid paradoxes that conflict with basic geometry)? Does it perhaps have to do with measure or dimension, e.g. approaching a line segment by line segments is sort of like approaching a 1-dimensional object by another 1-dimensional object, allowing some “space for things to go wrong”? (And, in general, approaching an $n$ -dimensional object by $n$ -dimensional ones?) My basic question is that, a priori, there might not appear much of a reason (not that I’m saying we need one) to confidently/fully believe that our definitions and axioms of area/volume do not run into such paradoxes or do not conflict with some basic geometric properties that we seek to have. So how do we work around this and ensure that our definitions and considerations are consistent and good and all that (whatever that means)?","So there is the “ staircase paradox ” that is sometimes used to “show” that (in the case of approximating a circle), or that (in the case of approximating the hypotenuse of a triangle). But when we define things like the integral, don’t we also speak of approximating something, in a similar fashion to approaching the hypotenuse of a triangle by staircases from above and below? For example, for the Riemann integral, we speak of approaching the area by looking at the supremum of the lower sums and the infimum of the upper sums; if these two numbers agree, then we say it is integrable. So why do we seem to not “run into such problems” when considering area/volume (or at least, the problems we run into are not quite the same), whereas we have problems when we (naively) consider lengths? Is it a matter of defining the area/volume as the limit (but this doesn’t seem satisfactory, because after all isn't a big purpose to model phenomena and solve problems, and avoid paradoxes that conflict with basic geometry)? Does it perhaps have to do with measure or dimension, e.g. approaching a line segment by line segments is sort of like approaching a 1-dimensional object by another 1-dimensional object, allowing some “space for things to go wrong”? (And, in general, approaching an -dimensional object by -dimensional ones?) My basic question is that, a priori, there might not appear much of a reason (not that I’m saying we need one) to confidently/fully believe that our definitions and axioms of area/volume do not run into such paradoxes or do not conflict with some basic geometric properties that we seek to have. So how do we work around this and ensure that our definitions and considerations are consistent and good and all that (whatever that means)?",\pi = 4 \sqrt{2} = 2 n n,"['real-analysis', 'calculus', 'geometry']"
54,Integrating $\int_0^1\frac{\ln^2x\ln(1+x)}{1+x^2} dx$ using real methods,Integrating  using real methods,\int_0^1\frac{\ln^2x\ln(1+x)}{1+x^2} dx,"How to evaluate, without contour integration the following integral: $$I=\int_0^1\frac{\ln^2x\ln(1+x)}{1+x^2}\ dx\ ?$$ @Cody mentioned  in this solution that $$I=\frac{\pi^{2}}{6}G+\frac{\pi^{3}}{32}\ln2-\frac{1}{768}\left[\psi_{3}\left(1/4\right)-\psi_{3}\left(3/4\right)\right]$$ but no proof was provided there, so any idea how to approach it? The result from above can be further simplified by using digamma's reflection formula $$\psi(1-x)-\psi(x)=\pi\cot(\pi x)$$ And differentiating both sides three times then set $x=3/4$ to get $$\psi_{3}(3/4)=16\pi^4-\psi_{3}(1/4)$$ $$\Rightarrow I=\frac{\pi^2}{6}G+\frac{\pi^{3}}{32}\ln2+\frac{\pi^4}{48}-\frac{1}{384}\psi_{3}(1/4)$$ Added: Is it possible to evaluate $I$ using harmonic series?","How to evaluate, without contour integration the following integral: @Cody mentioned  in this solution that but no proof was provided there, so any idea how to approach it? The result from above can be further simplified by using digamma's reflection formula And differentiating both sides three times then set to get Added: Is it possible to evaluate using harmonic series?",I=\int_0^1\frac{\ln^2x\ln(1+x)}{1+x^2}\ dx\ ? I=\frac{\pi^{2}}{6}G+\frac{\pi^{3}}{32}\ln2-\frac{1}{768}\left[\psi_{3}\left(1/4\right)-\psi_{3}\left(3/4\right)\right] \psi(1-x)-\psi(x)=\pi\cot(\pi x) x=3/4 \psi_{3}(3/4)=16\pi^4-\psi_{3}(1/4) \Rightarrow I=\frac{\pi^2}{6}G+\frac{\pi^{3}}{32}\ln2+\frac{\pi^4}{48}-\frac{1}{384}\psi_{3}(1/4) I,"['real-analysis', 'calculus', 'integration', 'harmonic-numbers', 'polygamma']"
55,Elementary problems solved with Functional Analysis,Elementary problems solved with Functional Analysis,,"Many times people come and ask me what Functional Analysis is used for and why it's interesting. Of course interest is a matter of taste, and I for one love the subject as it is. There are far reaching applications to Physics, PDE, other areas of analysis and other advanced subjects that I wouldn't be able to demonstrate say to a first or even second year undergraduate student. What about examples that can be stated in very simple terms, and are somehow ""familiar"" to the broader audience? I am not aware of classical or definitive examples, so I wanted to ask: What are some (preferably mathematical) applications of Functional Analysis, that are as elementary as possible?","Many times people come and ask me what Functional Analysis is used for and why it's interesting. Of course interest is a matter of taste, and I for one love the subject as it is. There are far reaching applications to Physics, PDE, other areas of analysis and other advanced subjects that I wouldn't be able to demonstrate say to a first or even second year undergraduate student. What about examples that can be stated in very simple terms, and are somehow ""familiar"" to the broader audience? I am not aware of classical or definitive examples, so I wanted to ask: What are some (preferably mathematical) applications of Functional Analysis, that are as elementary as possible?",,"['real-analysis', 'functional-analysis', 'reference-request', 'soft-question']"
56,"$f$ convex, $g$ concave and increasing, $\int_0^1 f = \int_0^1 g$, then $\int_0^1(f)^2 \geq \int_0^1(g)^2$","convex,  concave and increasing, , then",f g \int_0^1 f = \int_0^1 g \int_0^1(f)^2 \geq \int_0^1(g)^2,"Let $f,g:[0,1] \to [0, \infty)$ be two continuous functions such that $$f(0) = g(0) = 0,$$ $f$ is convex, $g$ is concave and increasing and $$\displaystyle \int_0^1f(x)dx = \int_0^1g(x)dx.$$ Prove that $$ \displaystyle \int_0^1\left(f(x) \right)^2dx \geq \int_0^1\left(g(x) \right)^2dx.$$ I don't quite know how to approach the problem. I thought about using Chebyshev's inequality due to the fact that $g$ is increasing, $F(x) = \int_0^xf(t)dt$ is increasing (since $f$ is convex) and $G(x) = \int_0^xg(t)dt$ is increasing (since $g$ is increasing and $g(0) = 0$), but it didn't help me. I also tried to obtain something by writing the convexity and concavity point-wise and using the fact that $\displaystyle h(x) = \frac{f(x)}{x}$ is increasing and $p(x) = \displaystyle \frac{g(x)}{x}$ is decreasing, but I got nothing.","Let $f,g:[0,1] \to [0, \infty)$ be two continuous functions such that $$f(0) = g(0) = 0,$$ $f$ is convex, $g$ is concave and increasing and $$\displaystyle \int_0^1f(x)dx = \int_0^1g(x)dx.$$ Prove that $$ \displaystyle \int_0^1\left(f(x) \right)^2dx \geq \int_0^1\left(g(x) \right)^2dx.$$ I don't quite know how to approach the problem. I thought about using Chebyshev's inequality due to the fact that $g$ is increasing, $F(x) = \int_0^xf(t)dt$ is increasing (since $f$ is convex) and $G(x) = \int_0^xg(t)dt$ is increasing (since $g$ is increasing and $g(0) = 0$), but it didn't help me. I also tried to obtain something by writing the convexity and concavity point-wise and using the fact that $\displaystyle h(x) = \frac{f(x)}{x}$ is increasing and $p(x) = \displaystyle \frac{g(x)}{x}$ is decreasing, but I got nothing.",,"['real-analysis', 'inequality', 'definite-integrals', 'integral-inequality']"
57,"If $b_n$ is convergent, then $a_n$ is also convergent","If  is convergent, then  is also convergent",b_n a_n,"Let $(a_n)_{n\geq 1}$ and $(b_n)_{n \geq 1}$ be two sequences of real numbers such that $$b_n=a_{n+2}-5a_{n+1}+6a_{n}, \: \forall n \geq 1$$   Prove that if $(b_n)$ is convergent, then $(a_n)$ is also convergent. I defined $c_n=a_{n+1}-2a_n$ and the relation became $b_n=c_{n+1}-3c_n.$ Then I tried to prove that $c_n$ is convergent by expressing $c_n$ only in terms of $b_n, b_{n-1}, \dots b_1$ and $c_1$, but the convergence doesn't follow from here and I got stuck. EDIT: As proven below, this statement is false !","Let $(a_n)_{n\geq 1}$ and $(b_n)_{n \geq 1}$ be two sequences of real numbers such that $$b_n=a_{n+2}-5a_{n+1}+6a_{n}, \: \forall n \geq 1$$   Prove that if $(b_n)$ is convergent, then $(a_n)$ is also convergent. I defined $c_n=a_{n+1}-2a_n$ and the relation became $b_n=c_{n+1}-3c_n.$ Then I tried to prove that $c_n$ is convergent by expressing $c_n$ only in terms of $b_n, b_{n-1}, \dots b_1$ and $c_1$, but the convergence doesn't follow from here and I got stuck. EDIT: As proven below, this statement is false !",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'recurrence-relations']"
58,"If $f(x)\to +\infty$ as $x\to +\infty$, then why is $\lim_{x\to \infty}\frac{\sin{(x^2+x+1)}}{f(x)}=0$?","If  as , then why is ?",f(x)\to +\infty x\to +\infty \lim_{x\to \infty}\frac{\sin{(x^2+x+1)}}{f(x)}=0,"If $f(x)\to +\infty$ as $x\to +\infty$, then $$\frac{\sin{(x^2+x+1)}}{f(x)}\to 0, \qquad \text{ as } x\to+\infty$$ I know the following is true by the Squeeze Theorem. I am just not sure how to apply it. Any suggestions?","If $f(x)\to +\infty$ as $x\to +\infty$, then $$\frac{\sin{(x^2+x+1)}}{f(x)}\to 0, \qquad \text{ as } x\to+\infty$$ I know the following is true by the Squeeze Theorem. I am just not sure how to apply it. Any suggestions?",,['real-analysis']
59,Infinitely many accumulation points in a bounded sequence?,Infinitely many accumulation points in a bounded sequence?,,"Any bounded sequence of real numbers contains at least one accumulation point. If it doesn't converge it has more than one. In fact, $$a_n \equiv n (mod m)$$ has exactly m limit points. Question: Can a bounded sequence of real numbers have infinitely many limit points?","Any bounded sequence of real numbers contains at least one accumulation point. If it doesn't converge it has more than one. In fact, $$a_n \equiv n (mod m)$$ has exactly m limit points. Question: Can a bounded sequence of real numbers have infinitely many limit points?",,"['real-analysis', 'sequences-and-series']"
60,Locating zeros of polynomial,Locating zeros of polynomial,,"Given polynomial with real coefficients  $$p(x) = x^n + a_1x^{n-1}+...+a_{n-1}x+a_n,$$ is there some method for deciding if it has zeros in some interval, for example $(0,1)$? There exist methods for either positive or negative zeros but I am interested for zeros in a specific interval.","Given polynomial with real coefficients  $$p(x) = x^n + a_1x^{n-1}+...+a_{n-1}x+a_n,$$ is there some method for deciding if it has zeros in some interval, for example $(0,1)$? There exist methods for either positive or negative zeros but I am interested for zeros in a specific interval.",,"['real-analysis', 'polynomials', 'numerical-methods', 'roots']"
61,"Calculate $\lim_{n\rightarrow\infty} \int_0^1 nx^2(1-x^2)^n \, dx$",Calculate,"\lim_{n\rightarrow\infty} \int_0^1 nx^2(1-x^2)^n \, dx","I have to calculate $$\lim_{n\rightarrow\infty} \int_0^1 nx^2(1-x^2)^n \, dx$$ I've created the series $(f_n)_{n\in \mathbb{N}}$ with $f_n:[0,1]\rightarrow \mathbb{R}, f_n(x)=nx^2(1-x^2)^n$. I considered $x\in[0,1]$ a scalar and proceed to calculated the limit $\lim_{n\rightarrow\infty}f_n(x)$, which equals to $0, \forall x\in[0,1)$ (if I'm not wrong), but I'm getting stuck at calculating the limit for the particular case of $x=1$. Thanks for help!","I have to calculate $$\lim_{n\rightarrow\infty} \int_0^1 nx^2(1-x^2)^n \, dx$$ I've created the series $(f_n)_{n\in \mathbb{N}}$ with $f_n:[0,1]\rightarrow \mathbb{R}, f_n(x)=nx^2(1-x^2)^n$. I considered $x\in[0,1]$ a scalar and proceed to calculated the limit $\lim_{n\rightarrow\infty}f_n(x)$, which equals to $0, \forall x\in[0,1)$ (if I'm not wrong), but I'm getting stuck at calculating the limit for the particular case of $x=1$. Thanks for help!",,"['real-analysis', 'sequences-and-series', 'limits', 'definite-integrals']"
62,Find the limit $\lim_\limits{n\to{\infty}}n^2(\sqrt[n]{x}-\sqrt[{n + 1}]{x})$ and my inquiry about Stolz-Cesàro theorem,Find the limit  and my inquiry about Stolz-Cesàro theorem,\lim_\limits{n\to{\infty}}n^2(\sqrt[n]{x}-\sqrt[{n + 1}]{x}),"Let $x>0$, find the limit $$\lim_\limits{n\to{\infty}}{n^2\left(\sqrt[n]{x}-\sqrt[{n + 1}]{x}\right)}$$ I use Maclaurin series and find out that the limit is $\ln x$. And this is the answer I get from a math forum: ""Let $x_n=\sqrt[n]{x}-1$, $y_n=\dfrac{1}{n}$, then $(x_n)\to0,(y_n)\to0,(y_n)\downarrow$ when $n\to\infty$. Use Stolz-Cesaro theorem, we have: $$\mathop {\lim }\limits_{n \to \infty } \dfrac{{\sqrt[n]{x} - \sqrt[{n + 1}]{x}}}{{\frac{1}{n} - \frac{1}{{n + 1}}}} = \mathop {\lim }\limits_{n \to \infty } \dfrac{{\sqrt[n]{x} - 1}}{{\frac{1}{n}}} = \ln x$$ Then $\mathop {\lim }\limits_{n \to \infty } {n^2}\left( {\sqrt[n]{x} - \sqrt[{n + 1}]{x}} \right) = \mathop {\lim }\limits_{n \to \infty } \dfrac{{{n^2}}}{{n\left( {n + 1} \right)}}\cdot\dfrac{{\sqrt[n]{x} - \sqrt[{n + 1}]{x}}}{{\frac{1}{n} - \frac{1}{{n + 1}}}} = \ln x$. /END"" I just wonder what form Stolz-Cesaro theorem that he used, because I just learn Stolz-Cesaro theorem when $(y_n)\to\infty$. Could anyone help me to get this? Thank you in advance. This is ""another form of Stolz theorem"" in the forum that I mentioned: Let $(x_n),(y_n)$     be two sequences of real numbers. Assume $\mathop {\lim }\limits_{n \to \infty } {x_n} = \mathop {\lim }\limits_{n \to \infty } {y_n} = 0$; $y_n>0$, $(y_n)$ is strictly decreasing  and the following limit exist $\mathop {\lim }\limits_{n \to \infty } \dfrac{{{x_{n + 1}} - {x_n}}}{{{y_{n + 1}} - {y_n}}} = L$. Then $\mathop {\lim }\limits_{n \to \infty } \dfrac{{{x_n}}}{{{y_n}}} = L$. Is this correct?","Let $x>0$, find the limit $$\lim_\limits{n\to{\infty}}{n^2\left(\sqrt[n]{x}-\sqrt[{n + 1}]{x}\right)}$$ I use Maclaurin series and find out that the limit is $\ln x$. And this is the answer I get from a math forum: ""Let $x_n=\sqrt[n]{x}-1$, $y_n=\dfrac{1}{n}$, then $(x_n)\to0,(y_n)\to0,(y_n)\downarrow$ when $n\to\infty$. Use Stolz-Cesaro theorem, we have: $$\mathop {\lim }\limits_{n \to \infty } \dfrac{{\sqrt[n]{x} - \sqrt[{n + 1}]{x}}}{{\frac{1}{n} - \frac{1}{{n + 1}}}} = \mathop {\lim }\limits_{n \to \infty } \dfrac{{\sqrt[n]{x} - 1}}{{\frac{1}{n}}} = \ln x$$ Then $\mathop {\lim }\limits_{n \to \infty } {n^2}\left( {\sqrt[n]{x} - \sqrt[{n + 1}]{x}} \right) = \mathop {\lim }\limits_{n \to \infty } \dfrac{{{n^2}}}{{n\left( {n + 1} \right)}}\cdot\dfrac{{\sqrt[n]{x} - \sqrt[{n + 1}]{x}}}{{\frac{1}{n} - \frac{1}{{n + 1}}}} = \ln x$. /END"" I just wonder what form Stolz-Cesaro theorem that he used, because I just learn Stolz-Cesaro theorem when $(y_n)\to\infty$. Could anyone help me to get this? Thank you in advance. This is ""another form of Stolz theorem"" in the forum that I mentioned: Let $(x_n),(y_n)$     be two sequences of real numbers. Assume $\mathop {\lim }\limits_{n \to \infty } {x_n} = \mathop {\lim }\limits_{n \to \infty } {y_n} = 0$; $y_n>0$, $(y_n)$ is strictly decreasing  and the following limit exist $\mathop {\lim }\limits_{n \to \infty } \dfrac{{{x_{n + 1}} - {x_n}}}{{{y_{n + 1}} - {y_n}}} = L$. Then $\mathop {\lim }\limits_{n \to \infty } \dfrac{{{x_n}}}{{{y_n}}} = L$. Is this correct?",,"['real-analysis', 'limits']"
63,Understanding the proof of $\displaystyle\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$,Understanding the proof of,\displaystyle\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1,"I need help understanding this: Prove that $$\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$$ $$(\sqrt n)^\frac{1}{n}\geq  (\sqrt1)^\frac{1}{n}=1, \forall n\in\Bbb N$$   By binomial theorem we get for $n\geq 2$ $$n=((\sqrt n)^\frac{1}{n})^n=[1+((\sqrt n)^\frac{1}{n}-1)]^n=\sum_{k=0}^{n}{{n}\choose{k}}1^{n-k}(\sqrt n^\frac{1}{n}-1)^k$$   $$\geq 1+{{n}\choose {2}}(\sqrt n^\frac{1}{n}-1)^2=1+\frac{n(n-1)}{2}(\sqrt n^\frac{1}{n}-1)^2$$   $$\Rightarrow (\sqrt n^\frac{1}{n}-1)^2\leq \frac{2}{n}$$   $$\Rightarrow \sqrt n^\frac{1}{n}\leq 1+\frac{\sqrt 2}{\sqrt n}$$   $1+\frac{\sqrt 2}{\sqrt n}$ approaches $1$ for $n \rightarrow \infty$ so by the sandwich theorem we get $\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$ I don't understand this part:  $$ \sum_{k=0}^{n}{{n}\choose{k}}1^{n-k}(\sqrt n^\frac{1}{n}-1)^k\geq 1+{{n}\choose {2}}(\sqrt n^\frac{1}{n}-1)^2.$$ Could someone explain how they got that on the RHS?","I need help understanding this: Prove that $$\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$$ $$(\sqrt n)^\frac{1}{n}\geq  (\sqrt1)^\frac{1}{n}=1, \forall n\in\Bbb N$$   By binomial theorem we get for $n\geq 2$ $$n=((\sqrt n)^\frac{1}{n})^n=[1+((\sqrt n)^\frac{1}{n}-1)]^n=\sum_{k=0}^{n}{{n}\choose{k}}1^{n-k}(\sqrt n^\frac{1}{n}-1)^k$$   $$\geq 1+{{n}\choose {2}}(\sqrt n^\frac{1}{n}-1)^2=1+\frac{n(n-1)}{2}(\sqrt n^\frac{1}{n}-1)^2$$   $$\Rightarrow (\sqrt n^\frac{1}{n}-1)^2\leq \frac{2}{n}$$   $$\Rightarrow \sqrt n^\frac{1}{n}\leq 1+\frac{\sqrt 2}{\sqrt n}$$   $1+\frac{\sqrt 2}{\sqrt n}$ approaches $1$ for $n \rightarrow \infty$ so by the sandwich theorem we get $\lim_{n\to\infty}(\sqrt n)^\frac{1}{n}=1$ I don't understand this part:  $$ \sum_{k=0}^{n}{{n}\choose{k}}1^{n-k}(\sqrt n^\frac{1}{n}-1)^k\geq 1+{{n}\choose {2}}(\sqrt n^\frac{1}{n}-1)^2.$$ Could someone explain how they got that on the RHS?",,"['real-analysis', 'limits']"
64,Evaluate $\int_0^{\infty}\frac{e^x-1}{xe^x(e^x+1)}dx.$ [closed],Evaluate  [closed],\int_0^{\infty}\frac{e^x-1}{xe^x(e^x+1)}dx.,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Evaluate the integral   $$ \int_{0}^{\infty}{\mathrm{e}^{x} - 1 \over x\,\mathrm{e}^{x}\left(\mathrm{e}^x+1\right)}\,\mathrm{d}x\,. $$ I have no idea how to approach integrals like this and I can't get any valuable result. Any hints will be appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Evaluate the integral   $$ \int_{0}^{\infty}{\mathrm{e}^{x} - 1 \over x\,\mathrm{e}^{x}\left(\mathrm{e}^x+1\right)}\,\mathrm{d}x\,. $$ I have no idea how to approach integrals like this and I can't get any valuable result. Any hints will be appreciated.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
65,Showing the radius of convergence for a power series is equal to the radius of convergence for its derivative,Showing the radius of convergence for a power series is equal to the radius of convergence for its derivative,,"Consider the power series: $$ \sum_{n=0}^{\infty} a_n (x - c)^n $$ Now consider its derivative: $$ \sum_{n=1}^{\infty} n a_n (x - c)^{n-1} $$ We can say at first that the Radius of Convergence for the original power series is $$ R = \lim_{n \to \infty} |a_{n+1} / a_{n}| $$ (via the Ratio Test). On the other hand, can we not also say that the radius of convergence for the derivative of the power series is $$ \lim_{n \to \infty} \left|\frac{(n+1) a_{n+1}}{n a_{n}} \right| = |a_{n+1} / a_{n}| = R? $$ via the same argument?  Is my reasoning correct?  That is, is the argument that the Radius of Convergence the same for both a power series and its derivative really this simple? :)","Consider the power series: $$ \sum_{n=0}^{\infty} a_n (x - c)^n $$ Now consider its derivative: $$ \sum_{n=1}^{\infty} n a_n (x - c)^{n-1} $$ We can say at first that the Radius of Convergence for the original power series is $$ R = \lim_{n \to \infty} |a_{n+1} / a_{n}| $$ (via the Ratio Test). On the other hand, can we not also say that the radius of convergence for the derivative of the power series is $$ \lim_{n \to \infty} \left|\frac{(n+1) a_{n+1}}{n a_{n}} \right| = |a_{n+1} / a_{n}| = R? $$ via the same argument?  Is my reasoning correct?  That is, is the argument that the Radius of Convergence the same for both a power series and its derivative really this simple? :)",,"['real-analysis', 'power-series']"
66,What is the cardinality of a non-measurable set?,What is the cardinality of a non-measurable set?,,We know that $|P( \Bbb{R} )|=|L (\Bbb{R} )|$ ( $L (\Bbb{R} )$ is the set of all Lebesgue-measurable sets). Note that $L (\Bbb{R} ) \subsetneq P( \Bbb{R} )$ . What is the cardinality of non-measurable set? Is this set countable?,We know that ( is the set of all Lebesgue-measurable sets). Note that . What is the cardinality of non-measurable set? Is this set countable?,|P( \Bbb{R} )|=|L (\Bbb{R} )| L (\Bbb{R} ) L (\Bbb{R} ) \subsetneq P( \Bbb{R} ),"['real-analysis', 'measure-theory', 'elementary-set-theory', 'lebesgue-measure']"
67,New series formula for $\arctan(x)$? $\ln(x)$?,New series formula for ? ?,\arctan(x) \ln(x),"I discovered this equation, but have no idea if it has been previously discovered.  Please help determine if it has been previously developed. Or please prove that the equation is not correct. $$\sum_{n=0}^\infty \frac{x^{2n+1}}{(x^2+1)^{n+1}}\cdot\frac{(2n)!!}{(2n+1)!!}=\arctan(x),$$ for $|x|\leq \pi$ , or possibly all $x$ . Likewise, using the same method for $x> .001$ , or possibly $x > 0$ . $$\sum_{n=1}^\infty \frac{x^{n}-1}{(1+x)^{n}}\cdot\frac{(1)}{(n)}=\ln(x),$$ all follows from $dx/dx =1$ .","I discovered this equation, but have no idea if it has been previously discovered.  Please help determine if it has been previously developed. Or please prove that the equation is not correct. for , or possibly all . Likewise, using the same method for , or possibly . all follows from .","\sum_{n=0}^\infty \frac{x^{2n+1}}{(x^2+1)^{n+1}}\cdot\frac{(2n)!!}{(2n+1)!!}=\arctan(x), |x|\leq \pi x x> .001 x > 0 \sum_{n=1}^\infty \frac{x^{n}-1}{(1+x)^{n}}\cdot\frac{(1)}{(n)}=\ln(x), dx/dx =1","['calculus', 'real-analysis', 'sequences-and-series', 'power-series']"
68,A puzzle about integrability,A puzzle about integrability,,"I know there is a Proposition: for $f(x)$ is bounded on $[a,b]$,then $f(x)$ is integrable if and only if given $\epsilon>0$, there exists a partition such that $U(f,P)-L(f,P)<\epsilon$ But my question is that: Assume $f(x)$ is bounded and integrable on $[a,b]$, then given  $\epsilon>0$   does it must exist a $\delta>0$ for any partition $|P|<\delta$,$U(f,P)-L(f,P)<\epsilon$. If it is true, how to prove it? Thanks in advance.","I know there is a Proposition: for $f(x)$ is bounded on $[a,b]$,then $f(x)$ is integrable if and only if given $\epsilon>0$, there exists a partition such that $U(f,P)-L(f,P)<\epsilon$ But my question is that: Assume $f(x)$ is bounded and integrable on $[a,b]$, then given  $\epsilon>0$   does it must exist a $\delta>0$ for any partition $|P|<\delta$,$U(f,P)-L(f,P)<\epsilon$. If it is true, how to prove it? Thanks in advance.",,"['real-analysis', 'integration']"
69,"closure of inverse image is subset of inverse image of closure, given that $f$ is continuous","closure of inverse image is subset of inverse image of closure, given that  is continuous",f,"Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function. Prove then that $$ \overline{f^{-1}(X)} \subset f^{-1} (\overline{X}) $$  for every $X \subset \mathbb{R}$. Attempt at proof: Let $a \in \overline{f^{-1}(X)}$ be arbitrary. Then by definition we have $\forall \delta > 0$ that $$ ] a - \delta, a + \delta [ \cap f^{-1}(X) \neq \emptyset. $$ Let $x$ be an element in this intersection. Thus $x \in ]a - \delta, a + \delta [ $ and $x \in f^{-1}(X)$. It follows that $f(x) \in X$. Because $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous $a$, we can find $\forall \epsilon > 0$ a $\delta > 0$ such that $\forall x \in \mathbb{R}$  it holds that $$ | f(x) - f(a) | < \epsilon $$ if $| x - a | < \delta$. Now we have $$f^{-1} (\overline{X})  = \left\{a \in \overline{X} \mid f(a) \in f(\overline{X}) \right\}. $$ This means I have to show that $a \in \overline{X}$ and then show that $f(a) \in f(\overline{X})$. This is the part where I'm stuck. Help would be appreciated.","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function. Prove then that $$ \overline{f^{-1}(X)} \subset f^{-1} (\overline{X}) $$  for every $X \subset \mathbb{R}$. Attempt at proof: Let $a \in \overline{f^{-1}(X)}$ be arbitrary. Then by definition we have $\forall \delta > 0$ that $$ ] a - \delta, a + \delta [ \cap f^{-1}(X) \neq \emptyset. $$ Let $x$ be an element in this intersection. Thus $x \in ]a - \delta, a + \delta [ $ and $x \in f^{-1}(X)$. It follows that $f(x) \in X$. Because $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous $a$, we can find $\forall \epsilon > 0$ a $\delta > 0$ such that $\forall x \in \mathbb{R}$  it holds that $$ | f(x) - f(a) | < \epsilon $$ if $| x - a | < \delta$. Now we have $$f^{-1} (\overline{X})  = \left\{a \in \overline{X} \mid f(a) \in f(\overline{X}) \right\}. $$ This means I have to show that $a \in \overline{X}$ and then show that $f(a) \in f(\overline{X})$. This is the part where I'm stuck. Help would be appreciated.",,"['real-analysis', 'general-topology', 'continuity']"
70,Pointwise convergence implies $L^p$,Pointwise convergence implies,L^p,"Simply, why is it that convergence pointwise, $u_j \rightarrow u$, implies convergence in $L^p$ if $|u_j(x)| \le g(x)$ for some $g$ in $L_+^p$?","Simply, why is it that convergence pointwise, $u_j \rightarrow u$, implies convergence in $L^p$ if $|u_j(x)| \le g(x)$ for some $g$ in $L_+^p$?",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
71,"Is every function $f$ on $ \mathbb R^2$ such that $f(x,y) \le g(x) + g(y)$ for every $(x,y)$, for some function $g$ on $\mathbb R$?","Is every function  on  such that  for every , for some function  on ?","f  \mathbb R^2 f(x,y) \le g(x) + g(y) (x,y) g \mathbb R","Is the following statement true? For every $f: \mathbb R^2 \to \mathbb R$, there exists $g:\mathbb R \to \mathbb R$, such that $f(x,y) \le g(x) + g(y)$ for all $x,y \in \mathbb R$. I do not think so. However, I couldn't find a counterexample. In case that doesn't hold, what conditions could we impose on $f$ so that the statement becomes true?","Is the following statement true? For every $f: \mathbb R^2 \to \mathbb R$, there exists $g:\mathbb R \to \mathbb R$, such that $f(x,y) \le g(x) + g(y)$ for all $x,y \in \mathbb R$. I do not think so. However, I couldn't find a counterexample. In case that doesn't hold, what conditions could we impose on $f$ so that the statement becomes true?",,[]
72,Partition of real numbers into dense subsets of positive measure [duplicate],Partition of real numbers into dense subsets of positive measure [duplicate],,"This question already has answers here : Construction of a Borel set with positive but not full measure in each interval (7 answers) Closed 8 years ago . Does there exist a partition of real numbers (with standard topology; Lebesgue measure) into two measurable sets $A$ and $B$, satisfying the following properties: $A$, $B$ are both dense in the real numbers. $A$, $B$ both have positive measure. Edited: as pointed out by Henry, actually I am looking for A B such that for any open interval I, the intersection of I and A, the intersection of B and I both have positive measure. Sorry for the confusion.","This question already has answers here : Construction of a Borel set with positive but not full measure in each interval (7 answers) Closed 8 years ago . Does there exist a partition of real numbers (with standard topology; Lebesgue measure) into two measurable sets $A$ and $B$, satisfying the following properties: $A$, $B$ are both dense in the real numbers. $A$, $B$ both have positive measure. Edited: as pointed out by Henry, actually I am looking for A B such that for any open interval I, the intersection of I and A, the intersection of B and I both have positive measure. Sorry for the confusion.",,"['real-analysis', 'measure-theory', 'real-numbers']"
73,$f$ differentiable at $0\iff\lim_{x\to 0}\frac{f(2x)-f(x)}{x}$ exists,differentiable at  exists,f 0\iff\lim_{x\to 0}\frac{f(2x)-f(x)}{x},"Let $f$ be a real function that is continuous at $0$ . Prove that $f$ differentiable at $0\iff\lim_{x\to 0}\frac{f(2x)-f(x)}{x}$ exists The $\Rightarrow$ part is trivial, and $\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=f'(0)$ What about $\Leftarrow$ ? I find it surprisingly hard, as I have made no progress toward a proof. Any suggestion is welcome.","Let be a real function that is continuous at . Prove that differentiable at exists The part is trivial, and What about ? I find it surprisingly hard, as I have made no progress toward a proof. Any suggestion is welcome.",f 0 f 0\iff\lim_{x\to 0}\frac{f(2x)-f(x)}{x} \Rightarrow \lim_{x\to 0}\frac{f(2x)-f(x)}{x}=f'(0) \Leftarrow,"['real-analysis', 'derivatives']"
74,"How to integrate $\int_{0}^{1}\ln\left(\, x\,\right)\,{\rm d}x$?",How to integrate ?,"\int_{0}^{1}\ln\left(\, x\,\right)\,{\rm d}x","I encountered this integral in the quantum field theory calculation. Can I do this: $$ \left. \int_{0}^{1}\ln\left(\, x\,\right)\,{\rm d}x =x\ln\left(\, x\,\right)\right\vert_{0}^{1} -\int_{0}^{1}\,{\rm d}x =\left. x\ln\left(\, x\,\right)\right\vert_{\, x\ =\ 0}\ -\ 1 $$ So the first term looks divergent. But Mathematica gives finite result and the integral is $-1$. Why isn't the first term divergent ?.","I encountered this integral in the quantum field theory calculation. Can I do this: $$ \left. \int_{0}^{1}\ln\left(\, x\,\right)\,{\rm d}x =x\ln\left(\, x\,\right)\right\vert_{0}^{1} -\int_{0}^{1}\,{\rm d}x =\left. x\ln\left(\, x\,\right)\right\vert_{\, x\ =\ 0}\ -\ 1 $$ So the first term looks divergent. But Mathematica gives finite result and the integral is $-1$. Why isn't the first term divergent ?.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
75,Tangent line of a convex function,Tangent line of a convex function,,"Let $V : \mathbb{R}^n \rightarrow  \mathbb{R}$ be (strictly) convex and continuously differentiable on $\mathbb{R}^n$. Show that for any $y \in \mathbb{R}^n$ and any $x \in \mathbb{R}^n$ $$ V(y) \geq V(x) + \nabla V(x)^\top(y-x) $$ For $n=1$ this implies that for any $x \in \mathbb{R}$ and for any $y \in \mathbb{R}$, $V(y)$ lies above the tangent line to $V$ at $x$.","Let $V : \mathbb{R}^n \rightarrow  \mathbb{R}$ be (strictly) convex and continuously differentiable on $\mathbb{R}^n$. Show that for any $y \in \mathbb{R}^n$ and any $x \in \mathbb{R}^n$ $$ V(y) \geq V(x) + \nabla V(x)^\top(y-x) $$ For $n=1$ this implies that for any $x \in \mathbb{R}$ and for any $y \in \mathbb{R}$, $V(y)$ lies above the tangent line to $V$ at $x$.",,"['real-analysis', 'convex-analysis', 'vector-analysis']"
76,"Prove that $2^n\alpha-[2^n\alpha]$ is dense in [0,1]","Prove that  is dense in [0,1]",2^n\alpha-[2^n\alpha],"Prove that $2^n\alpha-[2^n\alpha]$ is dense in $[0,1]$, if $\alpha$ is a positive irrational number. $[x]$ represents the largest integer smaller than $x$. I only know how to prove $n\alpha-[n\alpha]$ is dense in $[0,1]$, using pigeon hole principle. I am looking for similar method to solve this problem.","Prove that $2^n\alpha-[2^n\alpha]$ is dense in $[0,1]$, if $\alpha$ is a positive irrational number. $[x]$ represents the largest integer smaller than $x$. I only know how to prove $n\alpha-[n\alpha]$ is dense in $[0,1]$, using pigeon hole principle. I am looking for similar method to solve this problem.",,['real-analysis']
77,Fourier transform of a compactly supported function,Fourier transform of a compactly supported function,,"In which space does the Fourier transform of a smooth compactly supported function $\phi$ lie? I would not say it lies in $\mathcal{S}$, heuristically as one can approximate the step function which is $1$ in $[0,1]$ and $0$ outside by smooth functions, and the Fourier transform of that function decays very slowly. What if I add the requirement that the integral average of $\phi$ is $0$? I would expect cancellation in the phase space, the higher the frequency, the higher the cancellation. Any hint would be appreciated!","In which space does the Fourier transform of a smooth compactly supported function $\phi$ lie? I would not say it lies in $\mathcal{S}$, heuristically as one can approximate the step function which is $1$ in $[0,1]$ and $0$ outside by smooth functions, and the Fourier transform of that function decays very slowly. What if I add the requirement that the integral average of $\phi$ is $0$? I would expect cancellation in the phase space, the higher the frequency, the higher the cancellation. Any hint would be appreciated!",,"['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
78,"In a metric space, if a set is compact, then it is closed: improving proof","In a metric space, if a set is compact, then it is closed: improving proof",,"Let $(M,d)$ be a metric space. If $K\subset M$ is compact, then it is closed (and bounded). Proof Let's see that $M\setminus K$ is open. Let $x\in K$ $$\exists \varepsilon_1 (x), \varepsilon_2(x) \text{ so that } B(x, \varepsilon_1(x))\cap B(y,\varepsilon_2(x)) = \emptyset$$ then $K \subset \cup_{x\in K} B(x, \varepsilon_1(x))$ and, since $K$ is compact $\exists N \in \mathbb N \; \exists x_1,...,x_N \in K$ s.t. $$ K\subset \bigcup_{i=1}^N B(x_i, \varepsilon_1(x_i)) $$ let $r = \min\{\varepsilon_2(x), i = 1,...,N \} > 0$, then $$ B(y,r)\cap B(x_i, \varepsilon_1(x_i)) = \emptyset \quad\forall i = 1,...,N $$ therefore $B(y,r)\subset M\setminus K$ and $K$ is closed. Question Why uses $\varepsilon_1(x)$ and $\varepsilon_2(x)$? If we consider, in $\mathbb R$ the interval $[0,1]$ it can't be covered using open balls without covering elements of $\mathbb{R}\setminus[0,1]$, so what happens when choosing $r$? Am I missing something? Thanks in advance.","Let $(M,d)$ be a metric space. If $K\subset M$ is compact, then it is closed (and bounded). Proof Let's see that $M\setminus K$ is open. Let $x\in K$ $$\exists \varepsilon_1 (x), \varepsilon_2(x) \text{ so that } B(x, \varepsilon_1(x))\cap B(y,\varepsilon_2(x)) = \emptyset$$ then $K \subset \cup_{x\in K} B(x, \varepsilon_1(x))$ and, since $K$ is compact $\exists N \in \mathbb N \; \exists x_1,...,x_N \in K$ s.t. $$ K\subset \bigcup_{i=1}^N B(x_i, \varepsilon_1(x_i)) $$ let $r = \min\{\varepsilon_2(x), i = 1,...,N \} > 0$, then $$ B(y,r)\cap B(x_i, \varepsilon_1(x_i)) = \emptyset \quad\forall i = 1,...,N $$ therefore $B(y,r)\subset M\setminus K$ and $K$ is closed. Question Why uses $\varepsilon_1(x)$ and $\varepsilon_2(x)$? If we consider, in $\mathbb R$ the interval $[0,1]$ it can't be covered using open balls without covering elements of $\mathbb{R}\setminus[0,1]$, so what happens when choosing $r$? Am I missing something? Thanks in advance.",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
79,proving convergence of a sequence and then finding its limit,proving convergence of a sequence and then finding its limit,,"For every $n$ in $\mathbb{N}$, let: $$a_{n}=n\sum_{k=n}^{\infty }\frac{1}{k^{2}}$$ Show that the sequence $\left \{ a_{n} \right \}$ is convergent and then calculate its limit. To prove it is convergent, I was thinking of using theorems like the monotone convergence theorem. Obviously, all the terms $a_{n}$ are positive. So, if I prove that the sequence is decreasing, then by the monotone convergence theorem it follows that the sequence itself is convergent. $a_{n+1}-a_{n}=-\frac{1}{n}+\sum_{k=n+1}^{\infty }\frac{1}{k^{2}}$. But, I can't tell from this that the difference $a_{n+1}-a_{n}$ is negative. If anybody knows how to solve this problem, please share.","For every $n$ in $\mathbb{N}$, let: $$a_{n}=n\sum_{k=n}^{\infty }\frac{1}{k^{2}}$$ Show that the sequence $\left \{ a_{n} \right \}$ is convergent and then calculate its limit. To prove it is convergent, I was thinking of using theorems like the monotone convergence theorem. Obviously, all the terms $a_{n}$ are positive. So, if I prove that the sequence is decreasing, then by the monotone convergence theorem it follows that the sequence itself is convergent. $a_{n+1}-a_{n}=-\frac{1}{n}+\sum_{k=n+1}^{\infty }\frac{1}{k^{2}}$. But, I can't tell from this that the difference $a_{n+1}-a_{n}$ is negative. If anybody knows how to solve this problem, please share.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
80,"$f$ is integrable, prove $F(x) = \int_{-\infty}^x f(t) dt$ is uniformly continuous.","is integrable, prove  is uniformly continuous.",f F(x) = \int_{-\infty}^x f(t) dt,"I am not sure how to do this. I can prove it if I know $f$ is bounded, but otherwise I am stuck. $f$ is integrable, prove $F(x) = \int_{-\infty}^x f(t) dt$ is uniformly continuous.","I am not sure how to do this. I can prove it if I know $f$ is bounded, but otherwise I am stuck. $f$ is integrable, prove $F(x) = \int_{-\infty}^x f(t) dt$ is uniformly continuous.",,[]
81,Limit of $ u_{n}=\sin(\frac{1}{n+1})+\cdots+\sin(\frac{1}{2n})$,Limit of, u_{n}=\sin(\frac{1}{n+1})+\cdots+\sin(\frac{1}{2n}),How can I find the limit of $$ u_n =\sin\left(\frac{1}{n+1}\right)+\cdots+\sin\left(\frac{1}{2n}\right)$$ when $n\rightarrow\infty$? We have: $$ \sum_{n=1}^\infty u_{n+1}-u_n =u_\infty -\sin\left(\frac{1}{2}\right)$$ So how can I find $$ \sum_{n=1}^\infty u_{n+1}-u_n =\sum_{n=1}^\infty  \sin\left(\frac{1}{2n+2}\right)+\sin\left(\frac{1}{2n+1}\right)-\sin\left(\frac{1}{n+1}\right)\  ?$$,How can I find the limit of $$ u_n =\sin\left(\frac{1}{n+1}\right)+\cdots+\sin\left(\frac{1}{2n}\right)$$ when $n\rightarrow\infty$? We have: $$ \sum_{n=1}^\infty u_{n+1}-u_n =u_\infty -\sin\left(\frac{1}{2}\right)$$ So how can I find $$ \sum_{n=1}^\infty u_{n+1}-u_n =\sum_{n=1}^\infty  \sin\left(\frac{1}{2n+2}\right)+\sin\left(\frac{1}{2n+1}\right)-\sin\left(\frac{1}{n+1}\right)\  ?$$,,"['real-analysis', 'sequences-and-series']"
82,Does the completeness of a normed vector space only depend on its topology?,Does the completeness of a normed vector space only depend on its topology?,,"Let $V \space$ be a vector space over $\mathbb{R}$, and $\Vert \cdot \Vert_1$, $\Vert \cdot \Vert_2$ norms over $V$, which generate the same topology. Is it always true that if $v_n$ is a Cauchy sequence with respect to both norms, and $v_n$ converges in $(V, \Vert \cdot \Vert_1)$ then it converges in $(V, \Vert \cdot \Vert_2)$? If $dim(V)<+\infty$ the assertion is true, because there exist constants $c,C\in\mathbb{R}$ such that $\forall v\in V \space$ $c\Vert v \Vert_1 \leq \Vert v \Vert_2 \leq C\Vert v \Vert_1$, but I have a feeling it isn't in general (this would be strange, since completeness isn't a topological property; however maybe the additional structure of vector space might be used in some way). EDIT Thanks to everyone's answers I have realized that the above question was badly stated to begin with. What I meant to ask was whether the property of a normed vector space of being complete only depends on the topology generated by the norm, and not by the norm itself. As stated indirectly by many users, convergence, unlike the property of being a Cauchy sequence, is a topological property. Therefore the answer to the question I actually asked is always affermative. What I should have asked was: given two topologically equivalent norms $\Vert \cdot \Vert_1$, $\Vert \cdot \Vert_2$ on a vector space V, is it true that a sequence $v_n$ is a Cauchy sequence in $(V, \Vert \cdot \Vert_1)$ if and only if it is in $(V, \Vert \cdot \Vert_2)$?","Let $V \space$ be a vector space over $\mathbb{R}$, and $\Vert \cdot \Vert_1$, $\Vert \cdot \Vert_2$ norms over $V$, which generate the same topology. Is it always true that if $v_n$ is a Cauchy sequence with respect to both norms, and $v_n$ converges in $(V, \Vert \cdot \Vert_1)$ then it converges in $(V, \Vert \cdot \Vert_2)$? If $dim(V)<+\infty$ the assertion is true, because there exist constants $c,C\in\mathbb{R}$ such that $\forall v\in V \space$ $c\Vert v \Vert_1 \leq \Vert v \Vert_2 \leq C\Vert v \Vert_1$, but I have a feeling it isn't in general (this would be strange, since completeness isn't a topological property; however maybe the additional structure of vector space might be used in some way). EDIT Thanks to everyone's answers I have realized that the above question was badly stated to begin with. What I meant to ask was whether the property of a normed vector space of being complete only depends on the topology generated by the norm, and not by the norm itself. As stated indirectly by many users, convergence, unlike the property of being a Cauchy sequence, is a topological property. Therefore the answer to the question I actually asked is always affermative. What I should have asked was: given two topologically equivalent norms $\Vert \cdot \Vert_1$, $\Vert \cdot \Vert_2$ on a vector space V, is it true that a sequence $v_n$ is a Cauchy sequence in $(V, \Vert \cdot \Vert_1)$ if and only if it is in $(V, \Vert \cdot \Vert_2)$?",,"['real-analysis', 'general-topology', 'convergence-divergence', 'banach-spaces']"
83,Testing convergence of $\sum\limits_{n=2}^{\infty} \frac{\cos{\log{n}}}{n \cdot \log{n}}$,Testing convergence of,\sum\limits_{n=2}^{\infty} \frac{\cos{\log{n}}}{n \cdot \log{n}},"Does the series:  $$\sum\limits_{n=2}^{\infty} \frac{\cos(\log{n})}{n \cdot \log{n}}$$ converge or diverge? I know that $|\cos(\log{n})| \leq 1$, but I really cannot apply it here. Any ideas on how to attack this problem","Does the series:  $$\sum\limits_{n=2}^{\infty} \frac{\cos(\log{n})}{n \cdot \log{n}}$$ converge or diverge? I know that $|\cos(\log{n})| \leq 1$, but I really cannot apply it here. Any ideas on how to attack this problem",,['real-analysis']
84,Is the number of circles in the Apollonian gasket countable?,Is the number of circles in the Apollonian gasket countable?,,"Is it correct to say that the number of circles in an Apollonian gasket is countable becuase we can form a correspondence with a Cantor set, as their methods of construction are similar? What about if we apply the Apollonian gasket construction inside of a fractal like the Koch snowflake? (I think that will still be countable.) What if you did the Apollonian gasket construction between f(x) = sin(1/x) and  g(x) = 2 - sin(1/x) between -1 and 1?(I think that will still be countable too, but it's not matching my intuition... which says ""no way is that countable!"") Is there any closed curve that would result in the number of circles being uncountable? What if we consider the Apollonian gasket made of spheres in $\mathbb{R}^3$? (Please keep in mind I have only had two courses in Analysis. My apologies if any of this is too naive.)","Is it correct to say that the number of circles in an Apollonian gasket is countable becuase we can form a correspondence with a Cantor set, as their methods of construction are similar? What about if we apply the Apollonian gasket construction inside of a fractal like the Koch snowflake? (I think that will still be countable.) What if you did the Apollonian gasket construction between f(x) = sin(1/x) and  g(x) = 2 - sin(1/x) between -1 and 1?(I think that will still be countable too, but it's not matching my intuition... which says ""no way is that countable!"") Is there any closed curve that would result in the number of circles being uncountable? What if we consider the Apollonian gasket made of spheres in $\mathbb{R}^3$? (Please keep in mind I have only had two courses in Analysis. My apologies if any of this is too naive.)",,"['geometry', 'real-analysis', 'infinity', 'fractals']"
85,Polynomials which satisfy $p^{2}(x)-1 = p(x^{2}+1)$,Polynomials which satisfy,p^{2}(x)-1 = p(x^{2}+1),Can we find a polynomial $p(x) \in \mathbb{R}$ such that $\text{deg}\ p(x)>1$ and which satisfies $$p^{2}(x)-1=p(x^{2}+1)$$ for all $x \in \mathbb{R}$. This question can be very well identified with my previous question .,Can we find a polynomial $p(x) \in \mathbb{R}$ such that $\text{deg}\ p(x)>1$ and which satisfies $$p^{2}(x)-1=p(x^{2}+1)$$ for all $x \in \mathbb{R}$. This question can be very well identified with my previous question .,,"['calculus', 'real-analysis']"
86,Prove ${2n \choose n+i} \geq e^{-8 i^2/n} {2n \choose n}$,Prove,{2n \choose n+i} \geq e^{-8 i^2/n} {2n \choose n},"I am trying to prove ${2n \choose n+i} \geq e^{-8 i^2/n} {2n \choose n}$ for $0\leq i \leq n$ . My attempt: I rewrote ${2n \choose n+i}$ to $${2n \choose n+i} = {2n \choose n} \prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) $$ So all I need is to prove $$\prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) \geq e^{-8i^2/n}$$ I tries using $1-x \geq e^{-x/(1-x)}$ inequality to get $$\prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) \geq e^{- \sum_{j=1}^i \frac{2j-1}{n+1-j}}$$ However, I think this inequality is too loose. For example, for $n=400$ and $i=399$ , it violates what we are trying to prove. $$8 i^2/n < \sum_{j=1}^i \frac{2j-1}{n+1-j}$$","I am trying to prove for . My attempt: I rewrote to So all I need is to prove I tries using inequality to get However, I think this inequality is too loose. For example, for and , it violates what we are trying to prove.",{2n \choose n+i} \geq e^{-8 i^2/n} {2n \choose n} 0\leq i \leq n {2n \choose n+i} {2n \choose n+i} = {2n \choose n} \prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right)  \prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) \geq e^{-8i^2/n} 1-x \geq e^{-x/(1-x)} \prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) \geq e^{- \sum_{j=1}^i \frac{2j-1}{n+1-j}} n=400 i=399 8 i^2/n < \sum_{j=1}^i \frac{2j-1}{n+1-j},"['real-analysis', 'inequality']"
87,"Product of limit, sin, infinity, error?","Product of limit, sin, infinity, error?",,"Hello I would like to know if there is a mistake : I have to show that for any $t\geqslant0$ fixed $$\lim_{n\to \infty}\sin\sqrt{t+4\pi n^{2}}=0$$ That's what I said, Since $\sin(\cdot)$ is continuous and $$\sqrt{t+4\pi n^{2}}=2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}$$ for $n\geqslant1$ . $$\lim_{n\to \infty}\sin\sqrt{t+4\pi n^{2}}=\lim_{n\to \infty}\sin\left(2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}\right)$$ $$=\sin\left(\lim_{n\to\infty}2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}\right)=\\=\sin\left(\lim_{n\to\infty}2n\pi\cdot\lim_{n\to\infty}\sqrt{1+\frac{t}{4\pi n^{2}}}\right)=$$ $$=\sin\left(\lim_{n\to\infty}2n\pi\right)=\lim_{n\to\infty}\sin 2n\pi=\lim_{n\to\infty}0=0$$","Hello I would like to know if there is a mistake : I have to show that for any fixed That's what I said, Since is continuous and for .",t\geqslant0 \lim_{n\to \infty}\sin\sqrt{t+4\pi n^{2}}=0 \sin(\cdot) \sqrt{t+4\pi n^{2}}=2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}} n\geqslant1 \lim_{n\to \infty}\sin\sqrt{t+4\pi n^{2}}=\lim_{n\to \infty}\sin\left(2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}\right) =\sin\left(\lim_{n\to\infty}2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}\right)=\\=\sin\left(\lim_{n\to\infty}2n\pi\cdot\lim_{n\to\infty}\sqrt{1+\frac{t}{4\pi n^{2}}}\right)= =\sin\left(\lim_{n\to\infty}2n\pi\right)=\lim_{n\to\infty}\sin 2n\pi=\lim_{n\to\infty}0=0,"['real-analysis', 'sequences-and-series']"
88,What does a function of its own arc length look like?,What does a function of its own arc length look like?,,"Introduction What does a function of its own arc length look like? A strange question for sure, but first let me elaborate: Imagine a function that starts at the point $\left( 0, 0 \right)$ . If we now assume that the two closest (from the left and right) points also have $y = 0$ , then we can take the closest point of the function with a value which has the distance between the two points. If we now want to have the point after this, it has the value of the length of the previous curve (from $0$ ). This goes on and on, so we can say that the $y$ -value of a point of the function is the length of that function from $0$ to the point infinitely close to the point. Now you might ask yourself why you should look for something. I don't have a plan but it looks like fun and I couldn't find anything online about it. My Thoughts $f\left( n \right) \in \mathbb{R}$ and $2 < n \in \mathbb{N}$ Since I don't see an obvious solution, I would first try to find such a function for $f\left( n \right) \in \mathbb{R}$ and $n \in \mathbb{N}$ . Since the starting point is $\left( 0, 0 \right)$ aka $f\left( 0 \right) = 0$ , we can already take $\left( 0, 0 \right)$ as a point of the function. The next point would be at $n = 1$ , which due to the function having no length (which has length $0$ ) also gets the function value $0$ aka $f\left( 1 \right) = 0$ , so we get $\left( 1, 0 \right)$ . The next point would be at $n = 2$ , which by virtue of the function having length as the distance between the two previous two points also has the function value as that length. With this we'll get $\left( 3, 1 \right)$ aka $f\left( 3 \right) = 1$ . For the next points we do the same, only that instead of just calculating the newly added length, we also add it to the existing one, which gives us a recursiv formula: $$ \begin{align*} f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( \left( n - 1 \right) - \left( n - 2 \right) \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\ f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( n - n - 1 + 2 \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\ f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( 1 \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\ f\left( n \right) &= f\left( n - 1 \right) + \sqrt{1 + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\ \\ f\left( n \right) &= f\left( n - 1 \right) + \sqrt{1 + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}} \tag{1.}\\ \end{align*} $$ with the graph (from $0$ to $4$ ): $f\left( x \right) \in \mathbb{R}$ , $\Delta x \in \mathbb{Q}$ and $x - \Delta x > 2$ where $\Delta x$ is the distance between $x$ -values ​​of the two closest points Since the principle worked well for $f\left( n \right) \in \mathbb{R}$ and $n \in \mathbb{N}$ I would simply want to apply it to $\lim_{{\Delta x} \to {0}^{+}} \Delta x, \Delta x \in \mathbb{Q}$ for decreasing distances between $x$ -values ​​of the two closest points. With some work, the logic behind $\left( 1. \right)$ and the help of some more vector addition I found the generalized recursive formula: $$ \begin{align*} f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( \Delta f \right)^{2}}\\ f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \left(  x - 1 \cdot \Delta x \right) - \left(  x - 2 \cdot \Delta x \right) \right)^{2} + \left( f\left( x - 1 \cdot \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\ f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \left(  x - \Delta x \right) - \left(  x - 2 \cdot \Delta x \right) \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\ f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( x - x - \Delta x + 2 \cdot \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\ f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\ \\ f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \tag{2.}\\ \end{align*} $$ with the graph: (from $x = 0$ to $x = 4$ , from $y = 0$ to $y = 8$ and $\Delta x \in \left\{ {\color{Red} 1}, {\color{Blue} 0} {\color{Blue} .} {\color{Blue} 5}, {\color{Purple} 0} {\color{Purple}.} {\color{Purple} 2} {\color{Purple} 5}, {\color{Black} 0} {\color{Black} .} {\color{Black} 1} {\color{Black} 2} {\color{Black} 5} \right\})$ : It seems to me that if we let $\Delta x \to 0^{+}$ , $f\left( x \right)$ with $x \to 1^{+}$ itself goes to $+\infty$ , which I think is kinda cool as I wasn't expecting it. $y\left( x \right) \in \mathbb{R}$ , $x \in \mathbb{R}$ , $\lim_{{\Delta x} \to {0}^{+}} \Delta x$ and $\Delta x \in \mathbb{R}$ where $\Delta x$ is the distance between $x$ -values ​​of the two closest points If we now extend the formula from $\left( 2. \right)$ by the fact that $\Delta x$ should approach $0$ , we get the recursive formula: $$ \begin{align*} y\left( x \right) &:= \lim_{{\Delta x} \to {0^{+}}} f\left( x \right) = \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right]\\ y\left( x \right) &\equiv \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right]\\ \\ y\left( x \right) &\equiv \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right] \tag{3.}\\ \end{align*} $$ Now comes the challenge... Finding an explicit function formula. I know that we can calculate the length of a function $f$ in $\left[ a, b \right]$ using ""the arc length"" formula $\operatorname{arc length}\left( a, b, \frac{\operatorname{d}y}{\operatorname{d}x} \right) = \int_{a}^{b} \sqrt{1 + \left( \frac{\operatorname{d}y}{\operatorname{d}x} \right)^{2}} \operatorname{d}x$ if $y$ is continuously differentiable in $x \in \left[ a, b \right]$ but I don't know how to deal with it here. I am grateful for every help, correction and suggestion.","Introduction What does a function of its own arc length look like? A strange question for sure, but first let me elaborate: Imagine a function that starts at the point . If we now assume that the two closest (from the left and right) points also have , then we can take the closest point of the function with a value which has the distance between the two points. If we now want to have the point after this, it has the value of the length of the previous curve (from ). This goes on and on, so we can say that the -value of a point of the function is the length of that function from to the point infinitely close to the point. Now you might ask yourself why you should look for something. I don't have a plan but it looks like fun and I couldn't find anything online about it. My Thoughts and Since I don't see an obvious solution, I would first try to find such a function for and . Since the starting point is aka , we can already take as a point of the function. The next point would be at , which due to the function having no length (which has length ) also gets the function value aka , so we get . The next point would be at , which by virtue of the function having length as the distance between the two previous two points also has the function value as that length. With this we'll get aka . For the next points we do the same, only that instead of just calculating the newly added length, we also add it to the existing one, which gives us a recursiv formula: with the graph (from to ): , and where is the distance between -values ​​of the two closest points Since the principle worked well for and I would simply want to apply it to for decreasing distances between -values ​​of the two closest points. With some work, the logic behind and the help of some more vector addition I found the generalized recursive formula: with the graph: (from to , from to and : It seems to me that if we let , with itself goes to , which I think is kinda cool as I wasn't expecting it. , , and where is the distance between -values ​​of the two closest points If we now extend the formula from by the fact that should approach , we get the recursive formula: Now comes the challenge... Finding an explicit function formula. I know that we can calculate the length of a function in using ""the arc length"" formula if is continuously differentiable in but I don't know how to deal with it here. I am grateful for every help, correction and suggestion.","\left( 0, 0 \right) y = 0 0 y 0 f\left( n \right) \in \mathbb{R} 2 < n \in \mathbb{N} f\left( n \right) \in \mathbb{R} n \in \mathbb{N} \left( 0, 0 \right) f\left( 0 \right) = 0 \left( 0, 0 \right) n = 1 0 0 f\left( 1 \right) = 0 \left( 1, 0 \right) n = 2 \left( 3, 1 \right) f\left( 3 \right) = 1 
\begin{align*}
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( \left( n - 1 \right) - \left( n - 2 \right) \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( n - n - 1 + 2 \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( 1 \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{1 + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{1 + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}} \tag{1.}\\
\end{align*}
 0 4 f\left( x \right) \in \mathbb{R} \Delta x \in \mathbb{Q} x - \Delta x > 2 \Delta x x f\left( n \right) \in \mathbb{R} n \in \mathbb{N} \lim_{{\Delta x} \to {0}^{+}} \Delta x, \Delta x \in \mathbb{Q} x \left( 1. \right) 
\begin{align*}
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( \Delta f \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \left(  x - 1 \cdot \Delta x \right) - \left(  x - 2 \cdot \Delta x \right) \right)^{2} + \left( f\left( x - 1 \cdot \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \left(  x - \Delta x \right) - \left(  x - 2 \cdot \Delta x \right) \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( x - x - \Delta x + 2 \cdot \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \tag{2.}\\
\end{align*}
 x = 0 x = 4 y = 0 y = 8 \Delta x \in \left\{ {\color{Red} 1}, {\color{Blue} 0} {\color{Blue} .} {\color{Blue} 5}, {\color{Purple} 0} {\color{Purple}.} {\color{Purple} 2} {\color{Purple} 5}, {\color{Black} 0} {\color{Black} .} {\color{Black} 1} {\color{Black} 2} {\color{Black} 5} \right\}) \Delta x \to 0^{+} f\left( x \right) x \to 1^{+} +\infty y\left( x \right) \in \mathbb{R} x \in \mathbb{R} \lim_{{\Delta x} \to {0}^{+}} \Delta x \Delta x \in \mathbb{R} \Delta x x \left( 2. \right) \Delta x 0 
\begin{align*}
y\left( x \right) &:= \lim_{{\Delta x} \to {0^{+}}} f\left( x \right) = \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right]\\
y\left( x \right) &\equiv \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right]\\
\\
y\left( x \right) &\equiv \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right] \tag{3.}\\
\end{align*}
 f \left[ a, b \right] \operatorname{arc length}\left( a, b, \frac{\operatorname{d}y}{\operatorname{d}x} \right) = \int_{a}^{b} \sqrt{1 + \left( \frac{\operatorname{d}y}{\operatorname{d}x} \right)^{2}} \operatorname{d}x y x \in \left[ a, b \right]","['real-analysis', 'calculus', 'analysis', 'arc-length', 'infinitesimals']"
89,Proof that $\pi =\lim_{n\to\infty}\frac{2^{4n}n!^4}{n(2n)!^2}$,Proof that,\pi =\lim_{n\to\infty}\frac{2^{4n}n!^4}{n(2n)!^2},"In this post, the symbol $\sim$ means asymptotically equivalent . The relationship between $\pi$ and factorials hinges on Stirling's formula : $$n!\sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^n\implies n!^2\sim 2\pi n\left(\frac{n}{e}\right)^{2n}.$$ In a similar manner, $$(2n)!\sim \sqrt{4\pi n}\left(\frac{2n}{e}\right)^{2n}.$$ Then, by taking the ratio of the two quantities and simplifying one gets $$\frac{(2n)!}{n!^2}\sim \frac{2^{2n}}{\sqrt{\pi n}}.$$ By squaring both sides and further manipulation: $$\pi =\lim_{n\to\infty}\frac{2^{4n}n!^4}{n(2n)!^2}.$$ Also, note that the last limit for $\pi$ above and $$\frac{n!^2}{2n}\left(\frac{e}{n}\right)^{2n}$$ (which can be obtained as a limit for $\pi$ from Stirling's formula dircetly) have a different rate of convergence. My question is, how can one justify the manipulations (e.g. squaring both sides) when the expressions are not equivalent but only asymptotically equivalent? When I used this I ran into problems: $$\ln n!\sim n\ln n-n\, \text{(true)},$$ $$n!\sim e^{n\ln n-n}\, \text{(false)}.$$","In this post, the symbol means asymptotically equivalent . The relationship between and factorials hinges on Stirling's formula : In a similar manner, Then, by taking the ratio of the two quantities and simplifying one gets By squaring both sides and further manipulation: Also, note that the last limit for above and (which can be obtained as a limit for from Stirling's formula dircetly) have a different rate of convergence. My question is, how can one justify the manipulations (e.g. squaring both sides) when the expressions are not equivalent but only asymptotically equivalent? When I used this I ran into problems:","\sim \pi n!\sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^n\implies n!^2\sim 2\pi n\left(\frac{n}{e}\right)^{2n}. (2n)!\sim \sqrt{4\pi n}\left(\frac{2n}{e}\right)^{2n}. \frac{(2n)!}{n!^2}\sim \frac{2^{2n}}{\sqrt{\pi n}}. \pi =\lim_{n\to\infty}\frac{2^{4n}n!^4}{n(2n)!^2}. \pi \frac{n!^2}{2n}\left(\frac{e}{n}\right)^{2n} \pi \ln n!\sim n\ln n-n\, \text{(true)}, n!\sim e^{n\ln n-n}\, \text{(false)}.","['real-analysis', 'limits', 'asymptotics', 'factorial', 'pi']"
90,Finding $\lim_{n\to\infty}{\frac{n}{a^{n+1}}\left(a+\frac{a^2}{2}+\frac{a^3}{3}+\cdots+\frac{a^n}{n}\right)}$ where $a>1$,Finding  where,\lim_{n\to\infty}{\frac{n}{a^{n+1}}\left(a+\frac{a^2}{2}+\frac{a^3}{3}+\cdots+\frac{a^n}{n}\right)} a>1,"$$\underset{n\rightarrow\infty}\lim{\frac{n}{a^{n+1}}\left(a+\frac{a^2}{2}+\frac{a^3}{3}+\cdots+\frac{a^n}{n}\right)}=?, \;\;a>1$$ In Shaum's Mathematical handbook of formulas and table s I've seen: $$\;\;\;\;\;\;\;\;\;\;\ln(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots\;,x\in\langle-1,1]\;\;\;\;\;\;\;$$ $$\frac{1}{2}\ln{\Bigg(\frac{1+x}{1-x}\Bigg)}=1+\frac{x^3}{3}+\frac{x^5}{5}+\frac{x^7}{7}+\cdots\;\;\;,x\in\langle-1,1\rangle$$ The term in parentheses reminded me of the harmonic series. I thought of using the Taylor series. Is that a good idea? It says $a>0$ so I probably can't use these two formulas. On the other hand: $$e^x=x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots\;\;\;\;\;\;,$$ but there are no factorials in the denominators. Source in Croatian: 2.kolokvij, matematička analiza","In Shaum's Mathematical handbook of formulas and table s I've seen: The term in parentheses reminded me of the harmonic series. I thought of using the Taylor series. Is that a good idea? It says so I probably can't use these two formulas. On the other hand: but there are no factorials in the denominators. Source in Croatian: 2.kolokvij, matematička analiza","\underset{n\rightarrow\infty}\lim{\frac{n}{a^{n+1}}\left(a+\frac{a^2}{2}+\frac{a^3}{3}+\cdots+\frac{a^n}{n}\right)}=?, \;\;a>1 \;\;\;\;\;\;\;\;\;\;\ln(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots\;,x\in\langle-1,1]\;\;\;\;\;\;\; \frac{1}{2}\ln{\Bigg(\frac{1+x}{1-x}\Bigg)}=1+\frac{x^3}{3}+\frac{x^5}{5}+\frac{x^7}{7}+\cdots\;\;\;,x\in\langle-1,1\rangle a>0 e^x=x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots\;\;\;\;\;\;,","['real-analysis', 'calculus', 'limits']"
91,"If $a_n \to z$, then does $\frac{{n \choose 1} a_1 + {n \choose 2} a_2 \dots {n \choose n} a_n}{2^n} \to z$?","If , then does ?",a_n \to z \frac{{n \choose 1} a_1 + {n \choose 2} a_2 \dots {n \choose n} a_n}{2^n} \to z,"It is well known that $\sum_{k=0}^n{n\choose k} =2^n$ . My question: If $z$ is the limit point of an infinite sequence of real numbers $\{ a_n \}$ , then does $$\frac{{n \choose 1} a_1 + {n \choose 2} a_2+ \cdots+ {n \choose n} a_n}{2^n}$$ converge to $z$ as $n\ \to \infty$ ?","It is well known that . My question: If is the limit point of an infinite sequence of real numbers , then does converge to as ?",\sum_{k=0}^n{n\choose k} =2^n z \{ a_n \} \frac{{n \choose 1} a_1 + {n \choose 2} a_2+ \cdots+ {n \choose n} a_n}{2^n} z n\ \to \infty,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'summation-method']"
92,Closed Form of $a_n = \int_0^1 \ln(1+x^n) dx$,Closed Form of,a_n = \int_0^1 \ln(1+x^n) dx,"I want to know the closed form of : $$a_n = \int_0^1 \ln(1+x^n)dx, \quad \forall n \in \mathbb{N}$$ I found : $$0<a_n<\frac{1}{n+1}, \quad \lim_{n\to\infty} a_n =0$$ I started from \begin{align} &\ln (1+x)=\sum_{k=0}^\infty \frac{(-1)^k x^{k+1}}{k+1} \\ & \Rightarrow \ln(1+x^n)=\sum_{k=0}^\infty \frac{(-1)^k x^{nk+n}}{k+1} \\ & a_n = \int_0^1 \ln(1+x^n)dx = \sum_{k=0}^\infty \frac{(-1)^k}{k+1}\int_0^1 x^{n+nk}dx \\ &=\sum_{k=0}^\infty \frac{(-1)^k}{k+1} \times \frac{1}{n+nk+1} \\ &=\sum_{k=0}^\infty \frac{(-1)^k}{k+1} \times \frac{1}{(n+1)(k+1)-k}  \end{align} But I stucked here. Is there any closed (or approximated) from exist? These are some results for litte $n$ : \begin{align} & a_1 = 2\ln 2 - 1 \\ & a_2 = \ln2 - 2 + \frac{\pi}{2} \\ & a_3 = 2\ln 2 - 3 + \frac{\pi \sqrt{3}}{3} \\ \end{align}",I want to know the closed form of : I found : I started from But I stucked here. Is there any closed (or approximated) from exist? These are some results for litte :,"a_n = \int_0^1 \ln(1+x^n)dx, \quad \forall n \in \mathbb{N} 0<a_n<\frac{1}{n+1}, \quad \lim_{n\to\infty} a_n =0 \begin{align}
&\ln (1+x)=\sum_{k=0}^\infty \frac{(-1)^k x^{k+1}}{k+1} \\
& \Rightarrow \ln(1+x^n)=\sum_{k=0}^\infty \frac{(-1)^k x^{nk+n}}{k+1} \\
& a_n = \int_0^1 \ln(1+x^n)dx = \sum_{k=0}^\infty \frac{(-1)^k}{k+1}\int_0^1 x^{n+nk}dx \\
&=\sum_{k=0}^\infty \frac{(-1)^k}{k+1} \times \frac{1}{n+nk+1} \\
&=\sum_{k=0}^\infty \frac{(-1)^k}{k+1} \times \frac{1}{(n+1)(k+1)-k} 
\end{align} n \begin{align}
& a_1 = 2\ln 2 - 1 \\
& a_2 = \ln2 - 2 + \frac{\pi}{2} \\
& a_3 = 2\ln 2 - 3 + \frac{\pi \sqrt{3}}{3} \\
\end{align}","['real-analysis', 'integration', 'closed-form']"
93,Benefit of studying series,Benefit of studying series,,I think one reason for studying series is for power series. Are there any other application for studying series of numbers? Please let me know if you have any idea or comment for it. Thanks in advance!,I think one reason for studying series is for power series. Are there any other application for studying series of numbers? Please let me know if you have any idea or comment for it. Thanks in advance!,,"['calculus', 'real-analysis', 'sequences-and-series', 'number-theory', 'soft-question']"
94,"Find $\lim_{x\to \infty} \left[f\!\left(\sqrt{\frac{2}{x}}\,\right)\right]^x$.",Find .,"\lim_{x\to \infty} \left[f\!\left(\sqrt{\frac{2}{x}}\,\right)\right]^x","Let $f:\mathbb{R} \to \mathbb{R}$ be such that $f''$ is continuous on $\mathbb{R}$ and $f(0)=1$ ,$f'(0)=0$ and $f''(0)=-1$ . Then what is $\displaystyle\lim_{x\to \infty} \left[f\!\left(\sqrt{\frac{2}{x}}\,\right)\right]^x?$ When I was solving this problem, I supposed $f(x)$ to be a polynomial of degree two (because $f''$ is continuous) i.e. $f(x)=ax^2+bx+c$ and found coefficients with the help of given values . I got $f(x)=\frac{-x^2}{2} +1$. After solving , I found limit to be $e^{-1}$. I know this is a particular case. Questions $1$ : Will the limit be same for all functions with these properties ? $2$ : Please give me some method which works for all such $f(x)$. $3$: I want to practice more questions of this kind, please give me some references i.e. books, problem books, any online source. Any kind of help will be highly appreciated. Thanks!","Let $f:\mathbb{R} \to \mathbb{R}$ be such that $f''$ is continuous on $\mathbb{R}$ and $f(0)=1$ ,$f'(0)=0$ and $f''(0)=-1$ . Then what is $\displaystyle\lim_{x\to \infty} \left[f\!\left(\sqrt{\frac{2}{x}}\,\right)\right]^x?$ When I was solving this problem, I supposed $f(x)$ to be a polynomial of degree two (because $f''$ is continuous) i.e. $f(x)=ax^2+bx+c$ and found coefficients with the help of given values . I got $f(x)=\frac{-x^2}{2} +1$. After solving , I found limit to be $e^{-1}$. I know this is a particular case. Questions $1$ : Will the limit be same for all functions with these properties ? $2$ : Please give me some method which works for all such $f(x)$. $3$: I want to practice more questions of this kind, please give me some references i.e. books, problem books, any online source. Any kind of help will be highly appreciated. Thanks!",,"['calculus', 'real-analysis', 'reference-request']"
95,Can a 'closed' curve be a function?,Can a 'closed' curve be a function?,,"For a 'closed' curve (I don't know if there is such in mathematics called closed curve, but I mean a curve which is 'closed') e.g.1) It is well-known unit circle with equation$$x^2+y^2=1$$ e.g.2) It is the function $f$ which is in my another post . I observed that they cannot be a function, since one $x$-value may correspond to several $y$-values, or will there be a 'closed' curve, such that one $x$-value can only correspond to at most one $y$-value, i.e., it is a function?","For a 'closed' curve (I don't know if there is such in mathematics called closed curve, but I mean a curve which is 'closed') e.g.1) It is well-known unit circle with equation$$x^2+y^2=1$$ e.g.2) It is the function $f$ which is in my another post . I observed that they cannot be a function, since one $x$-value may correspond to several $y$-values, or will there be a 'closed' curve, such that one $x$-value can only correspond to at most one $y$-value, i.e., it is a function?",,"['real-analysis', 'general-topology', 'functions', 'implicit-function']"
96,Compute $\lim\limits_{n \to \infty} n\sum\limits_{k=1}^n(f(k/n) - f((k-1)/n))\int_{(k-1)/n}^{k/n}f(t)dt$,Compute,\lim\limits_{n \to \infty} n\sum\limits_{k=1}^n(f(k/n) - f((k-1)/n))\int_{(k-1)/n}^{k/n}f(t)dt,"Let $f : [0,1] \to \mathbb{R}$ be a continuous function and let $(a_n)_{n>0}$ and $(b_n)_{n>0}$ be two sequences such that $$\displaystyle{ a_n = \sum_{k=1}^n{f \left(\frac{k-1}{n}\right) \cdot \int_\frac{k-1}{n}^\frac{k}{n}}{f(t)dt}},$$ $$\displaystyle  b_n = \sum_{k=1}^n{f \left(\frac{k}{n}\right) \cdot \int_\frac{k-1}{n}^\frac{k}{n}}{f(t)dt}, $$ $\forall n \in \mathbb{N}^*$ . a) Prove that $\displaystyle{\lim_{n \to \infty}{(b_n - a_n)} = 0}$ . b) Compute $\displaystyle{\lim_{n \to \infty}{n(b_n - a_n)}}$ . I have managed to solve a). Proof for a) : From the mean value theorem, we know that $\displaystyle{\exists c_k \in \left(\frac{k-1}{n}, \frac{k}{n}\right)}$ such that $\displaystyle{\int_{\frac{k-1}{n}}^{\frac{k}{n}}{f(t)dt} = \frac{1}{n}f(c_k)}$ So, $b_n = \displaystyle{\frac{1}{n}\sum_{k=1}^n {f(c_k) \cdot f\left(\frac{k}{n}\right)} = \frac{1}{2} \cdot \frac{1}{n}\sum_{k=1}^n{\left(\frac{f(c_k) + f\left(\frac{k}{n}\right)}{2}\right)^2 \cdot 4 - (f(c_k))^2 - \left(f\left(\frac{k}{n}\right)\right)^2}}$ . Since $f$ has the intermediate value property, then $\exists x_k \in \displaystyle{\left(c_k, \frac{k}{n}\right) \subset \left(\frac{k-1}{n}, \frac{k}{n} \right)}$ such that $\displaystyle{\frac{f(c_k) + f\left(\frac{k}{n}\right)}{2} = f(x_k)}$ . Therefore, $\displaystyle{\lim_{n \to \infty}b_n = \int_0^1{(f(t))^2dt}}$ . Using the same method, $\displaystyle{\lim_{n \to \infty}a_n = \int_0^1{(f(t))^2dt}}$ , so $\displaystyle{\lim_{n \to \infty}{(b_n - a_n)} = 0}$ . I have trouble solving b). I tried using the same method but it doesn't work, not unless $f$ is differentiable (so I can use Lagrange's theorem). $\displaystyle{n(b_n - a_n) = \sum_{k=1}^n{f(c_k) \left( f\left(\frac{k}{n}\right) - f\left(\frac{k-1}{n} \right) \right)}}$ and if $f$ is differentiable, then $\displaystyle{f\left(\frac{k}{n}\right) - f\left(\frac{k-1}{n} \right) = \frac{1}{n}f'(c_k)}$ .","Let be a continuous function and let and be two sequences such that . a) Prove that . b) Compute . I have managed to solve a). Proof for a) : From the mean value theorem, we know that such that So, . Since has the intermediate value property, then such that . Therefore, . Using the same method, , so . I have trouble solving b). I tried using the same method but it doesn't work, not unless is differentiable (so I can use Lagrange's theorem). and if is differentiable, then .","f : [0,1] \to \mathbb{R} (a_n)_{n>0} (b_n)_{n>0} \displaystyle{ a_n = \sum_{k=1}^n{f \left(\frac{k-1}{n}\right) \cdot \int_\frac{k-1}{n}^\frac{k}{n}}{f(t)dt}}, \displaystyle  b_n = \sum_{k=1}^n{f \left(\frac{k}{n}\right) \cdot \int_\frac{k-1}{n}^\frac{k}{n}}{f(t)dt},  \forall n \in \mathbb{N}^* \displaystyle{\lim_{n \to \infty}{(b_n - a_n)} = 0} \displaystyle{\lim_{n \to \infty}{n(b_n - a_n)}} \displaystyle{\exists c_k \in \left(\frac{k-1}{n}, \frac{k}{n}\right)} \displaystyle{\int_{\frac{k-1}{n}}^{\frac{k}{n}}{f(t)dt} = \frac{1}{n}f(c_k)} b_n = \displaystyle{\frac{1}{n}\sum_{k=1}^n {f(c_k) \cdot f\left(\frac{k}{n}\right)} = \frac{1}{2} \cdot \frac{1}{n}\sum_{k=1}^n{\left(\frac{f(c_k) + f\left(\frac{k}{n}\right)}{2}\right)^2 \cdot 4 - (f(c_k))^2 - \left(f\left(\frac{k}{n}\right)\right)^2}} f \exists x_k \in \displaystyle{\left(c_k, \frac{k}{n}\right) \subset \left(\frac{k-1}{n}, \frac{k}{n} \right)} \displaystyle{\frac{f(c_k) + f\left(\frac{k}{n}\right)}{2} = f(x_k)} \displaystyle{\lim_{n \to \infty}b_n = \int_0^1{(f(t))^2dt}} \displaystyle{\lim_{n \to \infty}a_n = \int_0^1{(f(t))^2dt}} \displaystyle{\lim_{n \to \infty}{(b_n - a_n)} = 0} f \displaystyle{n(b_n - a_n) = \sum_{k=1}^n{f(c_k) \left( f\left(\frac{k}{n}\right) - f\left(\frac{k-1}{n} \right) \right)}} f \displaystyle{f\left(\frac{k}{n}\right) - f\left(\frac{k-1}{n} \right) = \frac{1}{n}f'(c_k)}","['real-analysis', 'integration', 'definite-integrals']"
97,An inner product which does not produce a Hilbert norm.,An inner product which does not produce a Hilbert norm.,,"I have a simple question: Consider the space of sequences $x = (x_1, x_2, \ldots$, $x_i,\ldots) \in \mathbb{R}$, for all $i \in \mathbb{N},$ such that $\sum_{k=1}^{+ \infty} x_k^2 < + \infty,$ with the product $\langle x,y \rangle = \sum_{k=1}^{+\infty} \frac{x_k y_k}{\sqrt{k}}$. Prove that this space is Euclidean, but not Hilbert. $\textbf{Idea for to show it:}$ For being Euclidean it is clear from the assumption $\sum_{k=1}^{+ \infty} x_{k}^{2} < + \infty$. For to not be Hilbert, we have to show that the norm induced by the defined inner product not satisfies in parallelogram equality $$\|x + y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2)$$ For example, if we suppose $x=(0, 0, \ldots , 1 , 0 , \ldots , 0)$, for 1 in the $m$th component and $y=(0, 0, \cdots , 1 , 0 , \ldots , 0)$ , for $1$ in the $n$th component for $m <n$, then we have $$\|x\| = \sqrt{\langle x,x \rangle}= \sqrt{\frac{1}{\sqrt{m}}}$$ and $$\|y\| = \sqrt{\langle y,y \rangle}= \sqrt{\frac{1}{\sqrt{n}}}$$ and $$\|x+y\| = \sqrt{\langle x+y,x+y \rangle}= \sqrt{\frac{1}{\sqrt{n}}} + \sqrt{\frac{1}{\sqrt{m}}}$$ and $$\|x-y\| = \sqrt{\langle x-y,x-y \rangle}= \sqrt{\frac{1}{\sqrt{m}}} - \sqrt{\frac{1}{\sqrt{n}}}.$$ So we will have $\left(\sqrt{\frac{1}{\sqrt{n}}} + \sqrt{\frac{1}{\sqrt{m}}}\right)^2 + \left(\sqrt{\frac{1}{\sqrt{m}}} - \sqrt{\frac{1}{\sqrt{n}}}\right)^2 = 2\left(\sqrt{\frac{1}{\sqrt{m}}}\right)^2 + 2\left(\sqrt{\frac{1}{\sqrt{n}}}\right)^2 $ which will give us $2\left(\frac{1}{n} + \frac{1}{m}\right) = 2\left(\frac{1}{n} + \frac{1}{m}\right)$. So this example was not our counter example and we need to find an another example which shows the parallelogram equality not satisfies. Can you please give me a counter example? Thanks!","I have a simple question: Consider the space of sequences $x = (x_1, x_2, \ldots$, $x_i,\ldots) \in \mathbb{R}$, for all $i \in \mathbb{N},$ such that $\sum_{k=1}^{+ \infty} x_k^2 < + \infty,$ with the product $\langle x,y \rangle = \sum_{k=1}^{+\infty} \frac{x_k y_k}{\sqrt{k}}$. Prove that this space is Euclidean, but not Hilbert. $\textbf{Idea for to show it:}$ For being Euclidean it is clear from the assumption $\sum_{k=1}^{+ \infty} x_{k}^{2} < + \infty$. For to not be Hilbert, we have to show that the norm induced by the defined inner product not satisfies in parallelogram equality $$\|x + y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2)$$ For example, if we suppose $x=(0, 0, \ldots , 1 , 0 , \ldots , 0)$, for 1 in the $m$th component and $y=(0, 0, \cdots , 1 , 0 , \ldots , 0)$ , for $1$ in the $n$th component for $m <n$, then we have $$\|x\| = \sqrt{\langle x,x \rangle}= \sqrt{\frac{1}{\sqrt{m}}}$$ and $$\|y\| = \sqrt{\langle y,y \rangle}= \sqrt{\frac{1}{\sqrt{n}}}$$ and $$\|x+y\| = \sqrt{\langle x+y,x+y \rangle}= \sqrt{\frac{1}{\sqrt{n}}} + \sqrt{\frac{1}{\sqrt{m}}}$$ and $$\|x-y\| = \sqrt{\langle x-y,x-y \rangle}= \sqrt{\frac{1}{\sqrt{m}}} - \sqrt{\frac{1}{\sqrt{n}}}.$$ So we will have $\left(\sqrt{\frac{1}{\sqrt{n}}} + \sqrt{\frac{1}{\sqrt{m}}}\right)^2 + \left(\sqrt{\frac{1}{\sqrt{m}}} - \sqrt{\frac{1}{\sqrt{n}}}\right)^2 = 2\left(\sqrt{\frac{1}{\sqrt{m}}}\right)^2 + 2\left(\sqrt{\frac{1}{\sqrt{n}}}\right)^2 $ which will give us $2\left(\frac{1}{n} + \frac{1}{m}\right) = 2\left(\frac{1}{n} + \frac{1}{m}\right)$. So this example was not our counter example and we need to find an another example which shows the parallelogram equality not satisfies. Can you please give me a counter example? Thanks!",,"['real-analysis', 'functional-analysis']"
98,Find limits of sequences,Find limits of sequences,,"Prove that $$a) \lim\limits_{n \to \infty} (\frac{1^p+2^p+...+n^p}{n^p} - \frac{n}{p+1})=\frac{1}{2},$$ $$b) \lim\limits_{n \to \infty} \frac{1^p+3^p+...+(2n-1)^p}{n^{p+1}}=\frac{2^p}{p+1},$$ where is $p \in \Bbb N $. Thanks to Stolz–Cesàro theorem in $a)$ I went to $$\lim\limits_{n \to \infty} \frac{(n+1)^p}{(n+1)^p-n^p}$$ which after dividing by $n^p$ goes to $$\lim\limits_{n \to \infty} \frac{1+\frac{p-1}{n}+\frac{p(p-1)}{2n^2}+...+\frac{1}{n^p}}{\frac{p-1}{n}+\frac{p(p-1)}{2n^2}+...+\frac{1}{n^p}} = \frac{1}{\infty}$$ At the same time $$\lim\limits_{n \to \infty} \frac{n}{p+1}=\infty,$$ so initial limit should be $\frac{1}{\infty}-\infty = ?$ I can't figure out what I'm missing ( $b)$ seems to complicated to me so I didn't even try it).","Prove that $$a) \lim\limits_{n \to \infty} (\frac{1^p+2^p+...+n^p}{n^p} - \frac{n}{p+1})=\frac{1}{2},$$ $$b) \lim\limits_{n \to \infty} \frac{1^p+3^p+...+(2n-1)^p}{n^{p+1}}=\frac{2^p}{p+1},$$ where is $p \in \Bbb N $. Thanks to Stolz–Cesàro theorem in $a)$ I went to $$\lim\limits_{n \to \infty} \frac{(n+1)^p}{(n+1)^p-n^p}$$ which after dividing by $n^p$ goes to $$\lim\limits_{n \to \infty} \frac{1+\frac{p-1}{n}+\frac{p(p-1)}{2n^2}+...+\frac{1}{n^p}}{\frac{p-1}{n}+\frac{p(p-1)}{2n^2}+...+\frac{1}{n^p}} = \frac{1}{\infty}$$ At the same time $$\lim\limits_{n \to \infty} \frac{n}{p+1}=\infty,$$ so initial limit should be $\frac{1}{\infty}-\infty = ?$ I can't figure out what I'm missing ( $b)$ seems to complicated to me so I didn't even try it).",,['real-analysis']
99,"Does there exist a bijective differentiable function $f:\mathbb{R^+}\rightarrow \mathbb{R^+}$, whose derivative is not a continuous function?","Does there exist a bijective differentiable function , whose derivative is not a continuous function?",f:\mathbb{R^+}\rightarrow \mathbb{R^+},"Does there exist a bijective differentiable function $f:\mathbb{R^+}\rightarrow \mathbb{R^+}$, whose derivative is not a continuous function? $x^2\sin \dfrac{1}{x}$ is a good example for non-continuous derivative function, that will not work here, I guess.","Does there exist a bijective differentiable function $f:\mathbb{R^+}\rightarrow \mathbb{R^+}$, whose derivative is not a continuous function? $x^2\sin \dfrac{1}{x}$ is a good example for non-continuous derivative function, that will not work here, I guess.",,"['real-analysis', 'derivatives', 'examples-counterexamples']"
