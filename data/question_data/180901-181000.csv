,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $F(x+y+z, x^2+y^2+z^2)=0$ then find $\frac{\partial^2 z}{\partial x \partial y }$",If  then find,"F(x+y+z, x^2+y^2+z^2)=0 \frac{\partial^2 z}{\partial x \partial y }","If $F(x+y+z, x^2+y^2+z^2)=0$ then find $\frac{\partial^2 z}{\partial x \partial y }$ . Attempt I think that here we must apply the implicit differentiation Theorem, but IÂ´dont know how I should do it, first I try use $X=x+y+z$ and $Y=x^2+y^2+z^2$ and then In case of be useful calculate $$\frac{\partial X}{\partial x}=1, \, \frac{\partial X}{\partial y}=1, \, \frac{\partial X}{\partial z}=1$$ and also $$ \frac{\partial Y}{\partial x}=2x, \, \frac{\partial Y}{\partial y}=2y, \, \frac{\partial Y}{\partial z}=2z$$ and hence my Function should looks as $$F(X,Y)=0$$ From here I Try apply the implicit function theorem which states that I should find a function $z(X)$ such that $F(X,Z(X))=0$ and that in fact $z$ is differentiable with differential equal to $$\frac{\partial z}{\partial x}=-\frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial z}}$$ But IÂ´m not sure about if the form of I use actually is valid, and in other case someone can clarify what is the answer(step by step (because iÂ´m learning Analysis by myself)  and more important how I should apply and understand this famous theorem.","If then find . Attempt I think that here we must apply the implicit differentiation Theorem, but IÂ´dont know how I should do it, first I try use and and then In case of be useful calculate and also and hence my Function should looks as From here I Try apply the implicit function theorem which states that I should find a function such that and that in fact is differentiable with differential equal to But IÂ´m not sure about if the form of I use actually is valid, and in other case someone can clarify what is the answer(step by step (because iÂ´m learning Analysis by myself)  and more important how I should apply and understand this famous theorem.","F(x+y+z, x^2+y^2+z^2)=0 \frac{\partial^2 z}{\partial x \partial y } X=x+y+z Y=x^2+y^2+z^2 \frac{\partial X}{\partial x}=1, \, \frac{\partial X}{\partial y}=1, \, \frac{\partial X}{\partial z}=1  \frac{\partial Y}{\partial x}=2x, \, \frac{\partial Y}{\partial y}=2y, \, \frac{\partial Y}{\partial z}=2z F(X,Y)=0 z(X) F(X,Z(X))=0 z \frac{\partial z}{\partial x}=-\frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial z}}","['real-analysis', 'calculus', 'multivariable-calculus', 'implicit-function-theorem']"
1,Linear Taylor expansion of Dieterici equation,Linear Taylor expansion of Dieterici equation,,"once again I am dealing with the Dieterici equation. As I am conducting a peer review with my fellow students, it would really be nice to know the exact result of the question. It is asked to find the linear Taylor expansion of following expression, assuming a<<RT: I just assumed it would be correct to Taylor-expand the exponential function at the origin point, that means using the MacLaurin expansion of e^z: My result is the equation at the bottom. But I am unsure if this is correct. My thoughts were: Linear Taylor expansion would mean terminating the sequence after the first n which is not zero. Furthermore, I thought that expanding the e-function is the way to go. But I am unsure where a<<RT would play into this to be honest (maybe it is the reason why we could expand in the first place). Any tips and/or advice or real solutions would really be appreciated, thanks!","once again I am dealing with the Dieterici equation. As I am conducting a peer review with my fellow students, it would really be nice to know the exact result of the question. It is asked to find the linear Taylor expansion of following expression, assuming a<<RT: I just assumed it would be correct to Taylor-expand the exponential function at the origin point, that means using the MacLaurin expansion of e^z: My result is the equation at the bottom. But I am unsure if this is correct. My thoughts were: Linear Taylor expansion would mean terminating the sequence after the first n which is not zero. Furthermore, I thought that expanding the e-function is the way to go. But I am unsure where a<<RT would play into this to be honest (maybe it is the reason why we could expand in the first place). Any tips and/or advice or real solutions would really be appreciated, thanks!",,"['multivariable-calculus', 'taylor-expansion']"
2,"$F: B(0,1)\to R$ is differentiable, $|F|\leq 1$. show $\exists\ \xi\in B(0,1)$, $|\nabla F(\xi)|\leq 2$","is differentiable, . show ,","F: B(0,1)\to R |F|\leq 1 \exists\ \xi\in B(0,1) |\nabla F(\xi)|\leq 2","$F: B(0,1)\to R$ is differentiable, $|F|\leq 1$ . show $\exists\ \xi\in B(0,1)$ , $|\nabla F(\xi)|\leq 2$ . Here $B(0,1)$ is the unit ball in $\Bbb R^d$ . If I just use Lagrange intermediate value theorem on two points of the boundary, I could just deduce some $\xi$ exists, such that one directional deriative has absolute value $\leq 1$ ...What idea for the problem?","is differentiable, . show , . Here is the unit ball in . If I just use Lagrange intermediate value theorem on two points of the boundary, I could just deduce some exists, such that one directional deriative has absolute value ...What idea for the problem?","F: B(0,1)\to R |F|\leq 1 \exists\ \xi\in B(0,1) |\nabla F(\xi)|\leq 2 B(0,1) \Bbb R^d \xi \leq 1",['multivariable-calculus']
3,Obscure passage in the calculus of a limit,Obscure passage in the calculus of a limit,,"Here is the multivariable limit with $\lambda\in\mathbb R$ $$\lim_{(x,y)\to (0,0)\\(x,y)\neq(0,0)} \frac{1-\cos(x^{3}y^{2\lambda})}{(x^{2}+y^{2})^{2\lambda+1}}$$ I'm studying 2 different cases: $\lambda\ge0$ and $\lambda < 0$ . Let $\lambda$ be strictly negative: $(x^{2}+y^{2})^{2\lambda+1}$ is evaluated to a negative power if $\lambda < -\frac{1}{2}$ . So the limit is equal to $0$ . But how can I study the subcase $-\frac{1}{2}\le\lambda<0$ ? If I exclude the the direction $y=0$ (x-axis) I find that the limit exist and is equal to zero but how about the x-axis? The limit doesn't exist? Or the domain is not defined in $y=0$ for $-\frac{1}{2}\le\lambda<0$ ? I don't understand this conceptual passage. Thanks in advice!",Here is the multivariable limit with I'm studying 2 different cases: and . Let be strictly negative: is evaluated to a negative power if . So the limit is equal to . But how can I study the subcase ? If I exclude the the direction (x-axis) I find that the limit exist and is equal to zero but how about the x-axis? The limit doesn't exist? Or the domain is not defined in for ? I don't understand this conceptual passage. Thanks in advice!,"\lambda\in\mathbb R \lim_{(x,y)\to (0,0)\\(x,y)\neq(0,0)} \frac{1-\cos(x^{3}y^{2\lambda})}{(x^{2}+y^{2})^{2\lambda+1}} \lambda\ge0 \lambda < 0 \lambda (x^{2}+y^{2})^{2\lambda+1} \lambda < -\frac{1}{2} 0 -\frac{1}{2}\le\lambda<0 y=0 y=0 -\frac{1}{2}\le\lambda<0","['calculus', 'limits', 'analysis', 'multivariable-calculus']"
4,Jacobian Matrix of inverse map [closed],Jacobian Matrix of inverse map [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I have a question which asks me to show that the map f: $R^2 \rightarrow R^2$ defined by $$f(x,y)=(e^x+e^y, e^x+e^{-y})$$ is locally invertible about any point $(a,b) \in R^2$ , and compute the Jacobian matrix of the inverse map. I know if is locally invertible because the determinant of the Jacobian matrix is not zero. However, how do I find the Jacobian matrix of the inverse map? Do I find the inverse matrix directly or should I find the inverse of this map and then find the Jacobian? There are several similar problems so I would really appreciate it if someone could walk me through the process.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I have a question which asks me to show that the map f: defined by is locally invertible about any point , and compute the Jacobian matrix of the inverse map. I know if is locally invertible because the determinant of the Jacobian matrix is not zero. However, how do I find the Jacobian matrix of the inverse map? Do I find the inverse matrix directly or should I find the inverse of this map and then find the Jacobian? There are several similar problems so I would really appreciate it if someone could walk me through the process.","R^2 \rightarrow R^2 f(x,y)=(e^x+e^y, e^x+e^{-y}) (a,b) \in R^2",['multivariable-calculus']
5,Find total charge of triangular region,Find total charge of triangular region,,"Charge is distributed over a triangular region in the ð‘¥ð‘¦-plane bounded by the ð‘¦-axis and the lines ð‘¦=5âˆ’ð‘¥ and ð‘¦=1+ð‘¥. The charge density at a point (ð‘¥,ð‘¦) is given by ðœŽ(ð‘¥,ð‘¦)=ð‘¥+ð‘¦, measured in coulombs per square meter (C/m2). Find the total charge. I've graphed a triangle which then made me form the integral $$\int_0^3 \int_{y-1}^{5-y} (x+y) \,dx \, dy$$ , making me get $27$ . I believe there is something wrong with my calculation. Is it that I am using the wrong variable or something else?","Charge is distributed over a triangular region in the ð‘¥ð‘¦-plane bounded by the ð‘¦-axis and the lines ð‘¦=5âˆ’ð‘¥ and ð‘¦=1+ð‘¥. The charge density at a point (ð‘¥,ð‘¦) is given by ðœŽ(ð‘¥,ð‘¦)=ð‘¥+ð‘¦, measured in coulombs per square meter (C/m2). Find the total charge. I've graphed a triangle which then made me form the integral , making me get . I believe there is something wrong with my calculation. Is it that I am using the wrong variable or something else?","\int_0^3 \int_{y-1}^{5-y} (x+y) \,dx \, dy 27",['multivariable-calculus']
6,Proof the existence of two roots in $(x-1)\ln y=\ln x$ when $y \neq e$,Proof the existence of two roots in  when,(x-1)\ln y=\ln x y \neq e,"Problem statement Proof that $y^x\cdot x$ and $yx^2$ for $x, y \ge 1$ have a single intersection point when $y \ge e$ or $y=1$ and at least two when $1<y<e$ . I am not sure if my reasoning has flaws, how to finish it or if there is any other more elegant/simpler way of proving it. I am studying the behavior of these two functions, i.e. when is $y^x\cdot x>yx^2$ . After graphing it, I have seen that in some regimes, one is greater than the other but not after a specific point. I can prove that there is a single solution for $y=e$ but I struggle to prove the other fact. Attempt $$y^x\cdot x=yx^2\ \overset{x\ge1}{\rightarrow}\ y^x=yx$$ Fixing $y$ to $k$ (for clarity of the reasoning): $$k^x=kx\ \rightarrow x\ln k=\ln k + \ln x$$ $$(x-1)\ln k = \ln x$$ And this is not further simplificable, so we have to find where a line intersects the logarithm function. Case 1: $y=e$ Observing that the derivative of $\ln x$ at $x=1$ is $(x-1)\ln y$ for $y=e$ , and the fact that the derivative of a function at a point is the tangent line at that point, this is the only intersection point. Proof: $f(x)=\ln x,\ f(1)=0 \rightarrow f'(x)=\frac{1}{x};\ f'(1)=1 \rightarrow \int f'(x)|_{x=1}\mathrm{d}x=x+c\overset{f(1)=0}{=}x-1 = (x-1)\ln e\quad \blacksquare$ Case 2: $y\neq e$ Here I tried to find a closed expression of the points by Taylor expansion and then find the roots via Ruffini but $f(x)=\ln x$ is not well defined at $x=1$ . Another idea was to try random points below and above $x=1$ and see what approximate root I found via Newton, but neither option seemed feasible. After thinking and playing with the graph a bit more, I realized that $x=1$ is like the z-rotation axis of the line $(x-1)\ln y$ so I decided to go this way: Fact #1: $x=1$ is a trivial solution of $(x-1)\ln y = \ln x\quad\forall\ y\in[1,\infty)$ Proof: $\ln x|_{x=1}\ln 1 = 0 = 0\ln y=(x-1)\ln y|_{x=1}\quad \blacksquare$ . Fact #2: both functions are strictly monotonically increasing.... and I guess this is relevant? And here is pretty much where I am stuck.","Problem statement Proof that and for have a single intersection point when or and at least two when . I am not sure if my reasoning has flaws, how to finish it or if there is any other more elegant/simpler way of proving it. I am studying the behavior of these two functions, i.e. when is . After graphing it, I have seen that in some regimes, one is greater than the other but not after a specific point. I can prove that there is a single solution for but I struggle to prove the other fact. Attempt Fixing to (for clarity of the reasoning): And this is not further simplificable, so we have to find where a line intersects the logarithm function. Case 1: Observing that the derivative of at is for , and the fact that the derivative of a function at a point is the tangent line at that point, this is the only intersection point. Proof: Case 2: Here I tried to find a closed expression of the points by Taylor expansion and then find the roots via Ruffini but is not well defined at . Another idea was to try random points below and above and see what approximate root I found via Newton, but neither option seemed feasible. After thinking and playing with the graph a bit more, I realized that is like the z-rotation axis of the line so I decided to go this way: Fact #1: is a trivial solution of Proof: . Fact #2: both functions are strictly monotonically increasing.... and I guess this is relevant? And here is pretty much where I am stuck.","y^x\cdot x yx^2 x, y \ge 1 y \ge e y=1 1<y<e y^x\cdot x>yx^2 y=e y^x\cdot x=yx^2\ \overset{x\ge1}{\rightarrow}\ y^x=yx y k k^x=kx\ \rightarrow x\ln k=\ln k + \ln x (x-1)\ln k = \ln x y=e \ln x x=1 (x-1)\ln y y=e f(x)=\ln x,\ f(1)=0 \rightarrow f'(x)=\frac{1}{x};\ f'(1)=1 \rightarrow \int f'(x)|_{x=1}\mathrm{d}x=x+c\overset{f(1)=0}{=}x-1 = (x-1)\ln e\quad \blacksquare y\neq e f(x)=\ln x x=1 x=1 x=1 (x-1)\ln y x=1 (x-1)\ln y = \ln x\quad\forall\ y\in[1,\infty) \ln x|_{x=1}\ln 1 = 0 = 0\ln y=(x-1)\ln y|_{x=1}\quad \blacksquare","['multivariable-calculus', 'proof-writing', 'solution-verification']"
7,Multivariate Calculus: finding mass,Multivariate Calculus: finding mass,,"Consider a solid $E$ bounded by the $yz$ -plane, the $xy$ -plane, the cone $z^2 = x^2 + y^2$ and the surface $x^2 + y^2 + z^2 - 2y=0$ . (There are four such solids, but it doesn't matter which one you use). Suppose the density of a chunk of metal of the shape of this solid at the point $(x,y,z)$ is $\sqrt{x^2 + y^2 + z^2}$ . Find the mass of the chunk of metal. What I did so far: $$\iiint_Ef(x, y, z) \,\mathrm dV =\iiint_E \sqrt{x^2 + y^2 + z^2}\,\mathrm dV$$ with $$E = \{(Ï\sinÏ•\cosÎ¸, Ï\sinÏ•\sinÎ¸, Ï\cosÏ•): 0â‰¤Î¸â‰¤{Ï€\over 2}, 0â‰¤Ï•â‰¤{Ï€\over 4}, 0â‰¤Ïâ‰¤2\sinÏ•\sinÎ¸\}.$$ Would appreciate any help on how to solve. Thanks.","Consider a solid bounded by the -plane, the -plane, the cone and the surface . (There are four such solids, but it doesn't matter which one you use). Suppose the density of a chunk of metal of the shape of this solid at the point is . Find the mass of the chunk of metal. What I did so far: with Would appreciate any help on how to solve. Thanks.","E yz xy z^2 = x^2 + y^2 x^2 + y^2 + z^2 - 2y=0 (x,y,z) \sqrt{x^2 + y^2 + z^2} \iiint_Ef(x, y, z) \,\mathrm dV =\iiint_E \sqrt{x^2 + y^2 + z^2}\,\mathrm dV E = \{(Ï\sinÏ•\cosÎ¸, Ï\sinÏ•\sinÎ¸, Ï\cosÏ•): 0â‰¤Î¸â‰¤{Ï€\over 2}, 0â‰¤Ï•â‰¤{Ï€\over 4}, 0â‰¤Ïâ‰¤2\sinÏ•\sinÎ¸\}.","['calculus', 'integration', 'multivariable-calculus']"
8,Inverse of two variable function,Inverse of two variable function,,"I do not have much experience in finding the inverse of a multivariable function, so any help is appreciated. I have function $f(\theta_a,\theta_b) =\begin{pmatrix}\frac{\cos(\theta_a)+\cos(\theta_b)}{1+\cos(\theta_a)\cos(\theta_b)}\\ \frac{\sin(\theta_a)+\sin(\theta_b)}{1+\sin(\theta_a)\sin(\theta_b)}\end{pmatrix} $ The right hand side above is a vector, so the function takes two real numbers $\theta_a,\theta_b \in [0,2\pi)$ in a vector and outputs a vector. My strategy was to try and solve the system of equations: $u = \frac{\cos(\theta_a)+\cos(\theta_b)}{1+\cos(\theta_a)\cos(\theta_b)}, \quad v = \frac{\sin(\theta_a)+\sin(\theta_b)}{1+\sin(\theta_a)\sin(\theta_b)} $ . by first isolating $\theta_a$ in the first equation, and inserting this expression into the second equation, then isolating $\theta_b$ and reinserting to get $\theta_a$ and $\theta_b$ as functions of $u$ and $v$ . This quickly becomes some very complex expressions however, and I have not been able to complete the computations, leading me to wondering if there is a different way. Thanks in advance!","I do not have much experience in finding the inverse of a multivariable function, so any help is appreciated. I have function The right hand side above is a vector, so the function takes two real numbers in a vector and outputs a vector. My strategy was to try and solve the system of equations: . by first isolating in the first equation, and inserting this expression into the second equation, then isolating and reinserting to get and as functions of and . This quickly becomes some very complex expressions however, and I have not been able to complete the computations, leading me to wondering if there is a different way. Thanks in advance!","f(\theta_a,\theta_b) =\begin{pmatrix}\frac{\cos(\theta_a)+\cos(\theta_b)}{1+\cos(\theta_a)\cos(\theta_b)}\\ \frac{\sin(\theta_a)+\sin(\theta_b)}{1+\sin(\theta_a)\sin(\theta_b)}\end{pmatrix}  \theta_a,\theta_b \in [0,2\pi) u = \frac{\cos(\theta_a)+\cos(\theta_b)}{1+\cos(\theta_a)\cos(\theta_b)}, \quad v = \frac{\sin(\theta_a)+\sin(\theta_b)}{1+\sin(\theta_a)\sin(\theta_b)}  \theta_a \theta_b \theta_a \theta_b u v","['calculus', 'multivariable-calculus', 'trigonometry']"
9,Write the PDE $xu_{xx}+u_{yy}=x^2$ in canonical form,Write the PDE  in canonical form,xu_{xx}+u_{yy}=x^2,"I am having trouble getting the transformation right. Let me show my work: We know $b^2-4ac=-4x$ , so if $x>0$ the equation is eliptic. Let's make our substitution: $\epsilon= -bx+2ay=2xy $ $n= \sqrt{4ac-b^2}x=\sqrt{4x}x=2x^{\frac{3}{2}}$ Notice that $\epsilon_{x}=2y, \epsilon_{y}=2x, n_{x}=3x^{\frac{1}{2}}, n_{y}=0$ . Thus, we have: $u_{x}=u_{\epsilon}\epsilon_{x}+u_{n}n_{x}=2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n}$ $u_{xx}= D_{\epsilon}[2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} ] +D_{n}[ 2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} ] = (2yu_{\epsilon\epsilon}+3x^{\frac{1}{2}}u_{n\epsilon})(2y) + (2yu_{\epsilon n}+3x^{\frac{1}{2}}u_{nn})(3x^\frac{1}{2})=4y^2u_{\epsilon\epsilon}+6x^{\frac{1}{2}} yu_{\epsilon n} + 9x u_{nn}$ $u_{y}=u_{\epsilon}\epsilon_{y}+u_{n}n_{y}=2xu_{\epsilon}$ $u_{yy}= D_{\epsilon}(2x u_{\epsilon})(2x)=4x^2u_{\epsilon\epsilon}$ When I substitute all of these expresions into our PDE I get: $xu_{xx}+u_{yy}=x^2 = x(4y^2u_{\epsilon\epsilon}+6x^{\frac{1}{2}} yu_{\epsilon n} + 9x u_{nn})+  4x^2u_{\epsilon\epsilon}=x^2$ Unfortunately, this is not a proper canonical form. I appreciate it if you can tell me what I'm doing wrong, I need to complete this exercise as quickly as possible, thank you for help!","I am having trouble getting the transformation right. Let me show my work: We know , so if the equation is eliptic. Let's make our substitution: Notice that . Thus, we have: When I substitute all of these expresions into our PDE I get: Unfortunately, this is not a proper canonical form. I appreciate it if you can tell me what I'm doing wrong, I need to complete this exercise as quickly as possible, thank you for help!","b^2-4ac=-4x x>0 \epsilon= -bx+2ay=2xy  n= \sqrt{4ac-b^2}x=\sqrt{4x}x=2x^{\frac{3}{2}} \epsilon_{x}=2y, \epsilon_{y}=2x, n_{x}=3x^{\frac{1}{2}}, n_{y}=0 u_{x}=u_{\epsilon}\epsilon_{x}+u_{n}n_{x}=2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} u_{xx}= D_{\epsilon}[2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} ] +D_{n}[ 2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} ] = (2yu_{\epsilon\epsilon}+3x^{\frac{1}{2}}u_{n\epsilon})(2y) + (2yu_{\epsilon n}+3x^{\frac{1}{2}}u_{nn})(3x^\frac{1}{2})=4y^2u_{\epsilon\epsilon}+6x^{\frac{1}{2}} yu_{\epsilon n} + 9x u_{nn} u_{y}=u_{\epsilon}\epsilon_{y}+u_{n}n_{y}=2xu_{\epsilon} u_{yy}= D_{\epsilon}(2x u_{\epsilon})(2x)=4x^2u_{\epsilon\epsilon} xu_{xx}+u_{yy}=x^2 = x(4y^2u_{\epsilon\epsilon}+6x^{\frac{1}{2}} yu_{\epsilon n} + 9x u_{nn})+ 
4x^2u_{\epsilon\epsilon}=x^2","['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
10,A n-variable fractional inequality,A n-variable fractional inequality,,"Problem: For positive numbers $a_1,a_2,\dots,a_n,$ note that $A=\sum\limits_{i=1}^{n}a_i, \,b_i=A-a_i,\,B=\sum\limits_{i=1}^{n}b_i.$ Prove $$ \frac{\prod\limits_{i=1}^{n}a_i}{\prod\limits_{i=1}^{n}(A-a_i)} \leqslant \frac{\prod\limits_{i=1}^{n}b_i}{\prod\limits_{i=1}^{n}(B-b_i)} $$ I know this inequality from a friend of mine who claimed the problem came from the Internet. My friend and I tried it for days. It's clear that for $a_1=a_2=\dots=a_n=1$ the equality holds. The first thought is to use Jesen inequality after taking the logarithm. However for $f(x)=\ln\frac{x}{A-x}(A>x)$ we have $f''(x)=-\frac{A(A-2x)}{x^2(A-x)^2}$ which implies zero or one inflection point, so we can't use Jesen inequality directly. Rewriting the inequality as $$ \prod\limits_{i=1}^{n}(B-b_i)  \leqslant \frac{\prod\limits_{i=1}^{n}(A-a_i)^2}{\prod\limits_{i=1}^{n}a_i} $$ makes the fact that $\prod\limits_{i=1}^{n}(B-b_i)$ is hard to deal clear. I wonder if there would be a nice solution.","Problem: For positive numbers note that Prove I know this inequality from a friend of mine who claimed the problem came from the Internet. My friend and I tried it for days. It's clear that for the equality holds. The first thought is to use Jesen inequality after taking the logarithm. However for we have which implies zero or one inflection point, so we can't use Jesen inequality directly. Rewriting the inequality as makes the fact that is hard to deal clear. I wonder if there would be a nice solution.","a_1,a_2,\dots,a_n, A=\sum\limits_{i=1}^{n}a_i, \,b_i=A-a_i,\,B=\sum\limits_{i=1}^{n}b_i. 
\frac{\prod\limits_{i=1}^{n}a_i}{\prod\limits_{i=1}^{n}(A-a_i)} \leqslant \frac{\prod\limits_{i=1}^{n}b_i}{\prod\limits_{i=1}^{n}(B-b_i)}
 a_1=a_2=\dots=a_n=1 f(x)=\ln\frac{x}{A-x}(A>x) f''(x)=-\frac{A(A-2x)}{x^2(A-x)^2} 
\prod\limits_{i=1}^{n}(B-b_i)  \leqslant \frac{\prod\limits_{i=1}^{n}(A-a_i)^2}{\prod\limits_{i=1}^{n}a_i}
 \prod\limits_{i=1}^{n}(B-b_i)","['multivariable-calculus', 'inequality', 'roots', 'products']"
11,Evaluating partial derivatives while using chain rule,Evaluating partial derivatives while using chain rule,,"On p. 184 of ""Methods of Mathematical Physics, Vol. 1"" by Courant & Hilbert, they take the following integral (variation of a functional) $$ \Phi(\epsilon) = \int_{x_0}^{x_1} F(x, y + \epsilon \eta, y^{\prime} + \epsilon \eta^{\prime} ) \,dx $$ and differentiate it with respect to $\epsilon$ under the integral. They then claim that $$ \Phi^{\prime} (0) = \int_{x_0}^{x_1} \left (\frac{\partial F}{\partial y} \eta + \frac{\partial F}{\partial y^{\prime}} \eta^{\prime} \right)\,dx $$ If I understand correctly, they used the chain rule for total derivatives. In other words, if we label the arguments of the multi-variable function $F$ as $F=F(x,f,g)$ , where $f\left(y,\varepsilon,\eta\right) = y+\varepsilon\eta$ and $g\left(y^{\prime},\varepsilon,\eta^{\prime}\right) = y^{\prime}+\varepsilon\eta^{\prime}$ , then, using the chain rule, one can write $$ \frac{\partial}{\partial\varepsilon}F\left(x,f,g\right)=\frac{\partial F}{\partial f}\frac{\partial f}{\partial\varepsilon}+\frac{\partial F}{\partial g}\frac{\partial g}{\partial\varepsilon}=\frac{\partial F}{\partial f}\eta+\frac{\partial F}{\partial g}\eta^{\prime} $$ Notice that $F$ is differentiated with respect to $f$ and $g$ , whereas in their case it's $y$ and $y^{\prime}$ . It seems that the claim is $$ \left.\frac{\partial F}{\partial f}\right|_{\varepsilon=0}=\frac{\partial F}{\partial f\left(y,\varepsilon=0,\eta\right)}=\frac{\partial F}{\partial y} $$ (And similarly for $g$ ). But is there a rigorous proof that such evaluation is allowed? Because if we take the simple example of $F(x,f) = f$ and $f\left(y,\varepsilon,\eta\right) = \varepsilon$ , we get, on the one hand, $$\left.\frac{\partial F}{\partial f}\right|_{\varepsilon=0}=\frac{\partial F}{\partial f}=1$$ but on the other hand, notationally, $$\frac{\partial F}{\partial f\left(y,\varepsilon=0,\eta\right)}=\frac{\partial F}{\partial0}$$ which is a complete nonsense.","On p. 184 of ""Methods of Mathematical Physics, Vol. 1"" by Courant & Hilbert, they take the following integral (variation of a functional) and differentiate it with respect to under the integral. They then claim that If I understand correctly, they used the chain rule for total derivatives. In other words, if we label the arguments of the multi-variable function as , where and , then, using the chain rule, one can write Notice that is differentiated with respect to and , whereas in their case it's and . It seems that the claim is (And similarly for ). But is there a rigorous proof that such evaluation is allowed? Because if we take the simple example of and , we get, on the one hand, but on the other hand, notationally, which is a complete nonsense.","
\Phi(\epsilon) = \int_{x_0}^{x_1} F(x, y + \epsilon \eta, y^{\prime} + \epsilon \eta^{\prime} ) \,dx
 \epsilon 
\Phi^{\prime} (0) = \int_{x_0}^{x_1} \left (\frac{\partial F}{\partial y} \eta + \frac{\partial F}{\partial y^{\prime}} \eta^{\prime} \right)\,dx
 F F=F(x,f,g) f\left(y,\varepsilon,\eta\right) = y+\varepsilon\eta g\left(y^{\prime},\varepsilon,\eta^{\prime}\right) = y^{\prime}+\varepsilon\eta^{\prime} 
\frac{\partial}{\partial\varepsilon}F\left(x,f,g\right)=\frac{\partial F}{\partial f}\frac{\partial f}{\partial\varepsilon}+\frac{\partial F}{\partial g}\frac{\partial g}{\partial\varepsilon}=\frac{\partial F}{\partial f}\eta+\frac{\partial F}{\partial g}\eta^{\prime}
 F f g y y^{\prime} 
\left.\frac{\partial F}{\partial f}\right|_{\varepsilon=0}=\frac{\partial F}{\partial f\left(y,\varepsilon=0,\eta\right)}=\frac{\partial F}{\partial y}
 g F(x,f) = f f\left(y,\varepsilon,\eta\right) = \varepsilon \left.\frac{\partial F}{\partial f}\right|_{\varepsilon=0}=\frac{\partial F}{\partial f}=1 \frac{\partial F}{\partial f\left(y,\varepsilon=0,\eta\right)}=\frac{\partial F}{\partial0}","['calculus', 'multivariable-calculus']"
12,Derivative with respect to $y$ and $x$ are equal?,Derivative with respect to  and  are equal?,y x,"Consider the wave equation $\frac{\partial^2u}{\partial x^2} = a^2\frac{\partial^2 u}{\partial y^2}$ .  Show that $u(x,y) = \sin(y-ax)$ is a solution of the wave equation.  More generally, show that $u(x,y) = f(x-ay) + g(x+ay)$ satisfies the wave equation. Hi, so in this problem for the second part, I got $f_{xx}(x-ay)+g_{xx}(x-ay)$ for the left hand side and $a^2( f_{yy}(x-ay)+g_{yy}(x-ay))$ for the right hand side and I don't get why the derivatives with respect to $x$ and $y$ have to be equal or am I confusing and the $x$ and $y$ don't matter when taking the derivative of the whole thing?","Consider the wave equation .  Show that is a solution of the wave equation.  More generally, show that satisfies the wave equation. Hi, so in this problem for the second part, I got for the left hand side and for the right hand side and I don't get why the derivatives with respect to and have to be equal or am I confusing and the and don't matter when taking the derivative of the whole thing?","\frac{\partial^2u}{\partial x^2} = a^2\frac{\partial^2 u}{\partial y^2} u(x,y) = \sin(y-ax) u(x,y) = f(x-ay) + g(x+ay) f_{xx}(x-ay)+g_{xx}(x-ay) a^2( f_{yy}(x-ay)+g_{yy}(x-ay)) x y x y","['multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'wave-equation']"
13,Gradient of a multi-dimensional multi-variable function,Gradient of a multi-dimensional multi-variable function,,"(First of all, feel free to suggest a better title for the question, I might just be totally missing the naming, hence not finding my answer because of that :) ) I understand how to compute the partial derivative of some function $f(x, y)$ , with respect its different variables, and how to get the gradient of the function from that. Now, if I have a function $f$ that takes, let say, two 2D vectors $p1$ and $p2$ as inputs and I want to find the gradient of this function with respect to each point $\nabla_{p1}f(p1, p2)$ and $\nabla_{p2}f(p1, p2)$ . This is where I'm totally lost. How is this computed ? For example, if $f(p1,p2) = |p1 - p2| - d$ , $|p1 - p2|$ being the distance between the two points (or the norm of the vector defined by those points) and $d$ being a constant, how does one compute $\nabla_{p1}f(p1, p2)$ and $\nabla_{p2}f(p1, p2)$ ? In that case, the results I need to find are $$\nabla_{p1}f(p1, p2) = \frac{p1-p2}{|p1 - p2|}$$ and $$\nabla_{p2}f(p1, p2) = -\frac{p1-p2}{|p1 - p2|}$$ but I do not understand how to find this result. Edit: To add a bit more context, I want to understand how to compute those formulas to be able to put them in a computer graphics physics simulation loop (namely using Position-Based Dynamics ). The function $f$ is in fact a constraint between the inputs (in that case, we want the two points to keep a certain distance from each other).","(First of all, feel free to suggest a better title for the question, I might just be totally missing the naming, hence not finding my answer because of that :) ) I understand how to compute the partial derivative of some function , with respect its different variables, and how to get the gradient of the function from that. Now, if I have a function that takes, let say, two 2D vectors and as inputs and I want to find the gradient of this function with respect to each point and . This is where I'm totally lost. How is this computed ? For example, if , being the distance between the two points (or the norm of the vector defined by those points) and being a constant, how does one compute and ? In that case, the results I need to find are and but I do not understand how to find this result. Edit: To add a bit more context, I want to understand how to compute those formulas to be able to put them in a computer graphics physics simulation loop (namely using Position-Based Dynamics ). The function is in fact a constraint between the inputs (in that case, we want the two points to keep a certain distance from each other).","f(x, y) f p1 p2 \nabla_{p1}f(p1, p2) \nabla_{p2}f(p1, p2) f(p1,p2) = |p1 - p2| - d |p1 - p2| d \nabla_{p1}f(p1, p2) \nabla_{p2}f(p1, p2) \nabla_{p1}f(p1, p2) = \frac{p1-p2}{|p1 - p2|} \nabla_{p2}f(p1, p2) = -\frac{p1-p2}{|p1 - p2|} f","['calculus', 'multivariable-calculus']"
14,Taking second derivative of multivariate normal density wrt covariance matrix,Taking second derivative of multivariate normal density wrt covariance matrix,,"In attempting to compute the second derivative of the density of a $d$ -dimensional $\mathrm{MVN}(\pmb0,\Sigma)$ random variable with respect to $\Sigma$ , I am running into an issue. In particular, I am having trouble figuring out the order of multiplication from element-wise notation. I am also having difficulties figuring out how to translate an expression from element-wise notation to matrix notation. I am looking for an answer to the order of multiplication and at least some tips on how to move forward with the final expression below. For $\Sigma=(\sigma_{ij})$ , set $\nabla_\Sigma=(\partial_{\sigma_{ij}})$ . For the pdf, write $$p(x)=(2\pi)^{-d/2}|\Sigma|^{-1/2}\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right).$$ Then we have $$\nabla_\Sigma p(x)=(2\pi)^{-d/2}\left[|\Sigma|^{-1/2}\nabla_\Sigma\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right)\\ +\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right)\nabla_\Sigma|\Sigma|^{-1/2}\right]$$ $$=-\frac{1}{2}(2\pi)^{-1/2}\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right)\left[|\Sigma|^{-1/2}\Sigma^{-1}xx^\top\Sigma^{-1}-|\Sigma|^{-3/2}|\Sigma|\Sigma^{-1}\right]$$ $$=\frac{p(x)}{2}\left[\Sigma^{-1}-\Sigma^{-1}xx^\top\Sigma^{-1}\right].$$ Hence, $\nabla_\Sigma\nabla_\Sigma p(x)=[\nabla_\Sigma(p(x)\Sigma^{-1})-\nabla_\Sigma(p(x)\Sigma^{-1}xx^\top\Sigma^{-1})]/2$ . Beginning with $\nabla_\Sigma(p(x)\Sigma^{-1})$ , I go component-wise to get $$\partial_{\sigma_{ij}}p(x)(\Sigma^{-1})_{kl}=p(x)\partial_{\sigma_{ij}}(\Sigma^{-1})_{kl}+(\Sigma^{-1})_{kl}\partial_{\sigma_{ij}}p(x)$$ $$=-p(x)(\Sigma^{-1})_{ik}(\Sigma^{-1})_{lj}+(\Sigma^{-1})_{kl}\partial_{\sigma_{ij}}p(x).$$ Here I come to my first question. Should $(\Sigma^{-1})_{kl}\partial_{\sigma_{ij}}p(x)$ correspond to $\Sigma^{-1}\otimes\nabla_\Sigma p(x)$ or $\nabla_\Sigma p(x)\otimes\Sigma^{-1}$ ? How can I tell? Assuming the former provides $$\nabla_\Sigma(p(x)\Sigma^{-1})=-p(x)\Sigma^{-1}\otimes\Sigma^{-1}+\Sigma^{-1}\otimes\nabla_\Sigma p(x)$$ $$=\frac{p(x)}{2}\Sigma^{-1}\otimes\left[\Sigma^{-1}-\Sigma^{-1}xx^\top\Sigma^{-1}\right].$$ For $\nabla_\Sigma[p(x)\Sigma^{-1}xx^\top\Sigma^{-1}]$ , I also proceed component-wise: $$\partial_{\sigma_{ij}}\left[p(x)\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}\right]=p(x)\partial_{\sigma_{ij}}\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}+\left[\partial_{\sigma_{ij}}p(x)\right]\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}.$$ Since the right-most component corresponds to either $\nabla_\Sigma p(x)\otimes\Sigma^{-1}xx^\top\Sigma^{-1}$ or $\Sigma^{-1}xx^\top\Sigma^{-1}\otimes\nabla_\Sigma p(x)$ , I focus on the left component, dropping the $p(x)$ for brevity $$\partial_{\sigma_{ij}}\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}=\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k\partial_{\sigma_{ij}}(\Sigma^{-1})_{kj}+(\Sigma^{-1})_{kj}x_lx_k\partial_{\sigma_{ij}}(\Sigma^{-1})_{il}$$ $$=-\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{ki}(\Sigma^{-1})_{jj}+(\Sigma^{-1})_{kj}x_lx_k(\Sigma^{-1})_{ii}(\Sigma^{-1})_{jl}$$ $$=-(\Sigma^{-1})_{jj}\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{ki}-(\Sigma^{-1})_{ii}\sum_{k,l}(\Sigma^{-1})_{kj}x_lx_k(\Sigma^{-1})_{jl}.$$ Since I am not sure how to write this last expression in terms of matrices and Kronecker products, this is where my journey has ended so far. Any ideas for finishing this calculation would be greatly appreciated.","In attempting to compute the second derivative of the density of a -dimensional random variable with respect to , I am running into an issue. In particular, I am having trouble figuring out the order of multiplication from element-wise notation. I am also having difficulties figuring out how to translate an expression from element-wise notation to matrix notation. I am looking for an answer to the order of multiplication and at least some tips on how to move forward with the final expression below. For , set . For the pdf, write Then we have Hence, . Beginning with , I go component-wise to get Here I come to my first question. Should correspond to or ? How can I tell? Assuming the former provides For , I also proceed component-wise: Since the right-most component corresponds to either or , I focus on the left component, dropping the for brevity Since I am not sure how to write this last expression in terms of matrices and Kronecker products, this is where my journey has ended so far. Any ideas for finishing this calculation would be greatly appreciated.","d \mathrm{MVN}(\pmb0,\Sigma) \Sigma \Sigma=(\sigma_{ij}) \nabla_\Sigma=(\partial_{\sigma_{ij}}) p(x)=(2\pi)^{-d/2}|\Sigma|^{-1/2}\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right). \nabla_\Sigma p(x)=(2\pi)^{-d/2}\left[|\Sigma|^{-1/2}\nabla_\Sigma\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right)\\
+\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right)\nabla_\Sigma|\Sigma|^{-1/2}\right] =-\frac{1}{2}(2\pi)^{-1/2}\exp\left(-\frac{1}{2}x^\top\Sigma^{-1}x\right)\left[|\Sigma|^{-1/2}\Sigma^{-1}xx^\top\Sigma^{-1}-|\Sigma|^{-3/2}|\Sigma|\Sigma^{-1}\right] =\frac{p(x)}{2}\left[\Sigma^{-1}-\Sigma^{-1}xx^\top\Sigma^{-1}\right]. \nabla_\Sigma\nabla_\Sigma p(x)=[\nabla_\Sigma(p(x)\Sigma^{-1})-\nabla_\Sigma(p(x)\Sigma^{-1}xx^\top\Sigma^{-1})]/2 \nabla_\Sigma(p(x)\Sigma^{-1}) \partial_{\sigma_{ij}}p(x)(\Sigma^{-1})_{kl}=p(x)\partial_{\sigma_{ij}}(\Sigma^{-1})_{kl}+(\Sigma^{-1})_{kl}\partial_{\sigma_{ij}}p(x) =-p(x)(\Sigma^{-1})_{ik}(\Sigma^{-1})_{lj}+(\Sigma^{-1})_{kl}\partial_{\sigma_{ij}}p(x). (\Sigma^{-1})_{kl}\partial_{\sigma_{ij}}p(x) \Sigma^{-1}\otimes\nabla_\Sigma p(x) \nabla_\Sigma p(x)\otimes\Sigma^{-1} \nabla_\Sigma(p(x)\Sigma^{-1})=-p(x)\Sigma^{-1}\otimes\Sigma^{-1}+\Sigma^{-1}\otimes\nabla_\Sigma p(x) =\frac{p(x)}{2}\Sigma^{-1}\otimes\left[\Sigma^{-1}-\Sigma^{-1}xx^\top\Sigma^{-1}\right]. \nabla_\Sigma[p(x)\Sigma^{-1}xx^\top\Sigma^{-1}] \partial_{\sigma_{ij}}\left[p(x)\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}\right]=p(x)\partial_{\sigma_{ij}}\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}+\left[\partial_{\sigma_{ij}}p(x)\right]\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}. \nabla_\Sigma p(x)\otimes\Sigma^{-1}xx^\top\Sigma^{-1} \Sigma^{-1}xx^\top\Sigma^{-1}\otimes\nabla_\Sigma p(x) p(x) \partial_{\sigma_{ij}}\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{kj}=\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k\partial_{\sigma_{ij}}(\Sigma^{-1})_{kj}+(\Sigma^{-1})_{kj}x_lx_k\partial_{\sigma_{ij}}(\Sigma^{-1})_{il} =-\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{ki}(\Sigma^{-1})_{jj}+(\Sigma^{-1})_{kj}x_lx_k(\Sigma^{-1})_{ii}(\Sigma^{-1})_{jl} =-(\Sigma^{-1})_{jj}\sum_{k,l}(\Sigma^{-1})_{il}x_lx_k(\Sigma^{-1})_{ki}-(\Sigma^{-1})_{ii}\sum_{k,l}(\Sigma^{-1})_{kj}x_lx_k(\Sigma^{-1})_{jl}.","['multivariable-calculus', 'normal-distribution', 'tensor-products']"
15,"Is the space $H(\operatorname{curl},\Omega)$ stable under the action of symmetric definite positive matrix?",Is the space  stable under the action of symmetric definite positive matrix?,"H(\operatorname{curl},\Omega)","$\newcommand{\curl}{\operatorname{curl}}$ Let $M$ be a $3 \times 3 $ constant real symmetric positive definite matrix, $\Omega\subset \mathbb{R}^3$ a bounded lipschitz domain and define: $$ H(\curl,\Omega)=\{u\in (L^2(\Omega))^3\,|\,\nabla\times u\in (L^2(\Omega))^3 \} $$ where $(L^2(\Omega))^3$ is the space of square integrable functions on $\Omega$ . Let $u$ be a function such that $M\nabla\times u\in H(\curl,\Omega)$ . Then, does we have $\nabla\times u\in H(\curl,\Omega)$ ? If $M\nabla\times u\in H(\curl,\Omega)$ , then there exists $v\in H(\curl,\Omega)$ such that $M\nabla\times u=v$ . The matrix $M$ is invertible, so we have $\nabla\times u= M^{-1}v$ which is in $(L^2(\Omega))^3$ . But, how to check if $\nabla\times\nabla\times u=\nabla\times  (M^{-1}v)$ is in $(L^2(\Omega))^3$ or not?","Let be a constant real symmetric positive definite matrix, a bounded lipschitz domain and define: where is the space of square integrable functions on . Let be a function such that . Then, does we have ? If , then there exists such that . The matrix is invertible, so we have which is in . But, how to check if is in or not?","\newcommand{\curl}{\operatorname{curl}} M 3 \times 3  \Omega\subset \mathbb{R}^3 
H(\curl,\Omega)=\{u\in (L^2(\Omega))^3\,|\,\nabla\times u\in (L^2(\Omega))^3 \}
 (L^2(\Omega))^3 \Omega u M\nabla\times u\in H(\curl,\Omega) \nabla\times u\in H(\curl,\Omega) M\nabla\times u\in H(\curl,\Omega) v\in H(\curl,\Omega) M\nabla\times u=v M \nabla\times u= M^{-1}v (L^2(\Omega))^3 \nabla\times\nabla\times u=\nabla\times  (M^{-1}v) (L^2(\Omega))^3","['matrices', 'functional-analysis', 'multivariable-calculus', 'differential-geometry', 'sobolev-spaces']"
16,Questions about a linear PDE,Questions about a linear PDE,,"Consider the following linear PDE: $$u\frac{\partial}{\partial u}\left(u\frac{\partial f}{\partial u}\right) + v\frac{\partial}{\partial v}\left(v\frac{\partial f}{\partial v}\right)=\frac{\partial^2 f}{\partial u^2} + \frac{\partial^2 f}{\partial v^2}\tag{1}$$ where we have $f(u,v)$ . Has anyone studied this PDE in the literature? Are there weak solutions to $(1)?$ I constructed this PDE by deriving the Laplacian in two different coordinate systems and then made the coordinate variables the same and equated the expressions, which is what you see on the LHS and RHS. I'm not sure what this process is called or if anyone has used it before.","Consider the following linear PDE: where we have . Has anyone studied this PDE in the literature? Are there weak solutions to I constructed this PDE by deriving the Laplacian in two different coordinate systems and then made the coordinate variables the same and equated the expressions, which is what you see on the LHS and RHS. I'm not sure what this process is called or if anyone has used it before.","u\frac{\partial}{\partial u}\left(u\frac{\partial f}{\partial u}\right) + v\frac{\partial}{\partial v}\left(v\frac{\partial f}{\partial v}\right)=\frac{\partial^2 f}{\partial u^2} + \frac{\partial^2 f}{\partial v^2}\tag{1} f(u,v) (1)?","['multivariable-calculus', 'partial-differential-equations', 'coordinate-systems', 'laplacian']"
17,inner product between homogenous harmonic polynomials,inner product between homogenous harmonic polynomials,,"Suppose $v_i$ and $v_j$ are homogenous harmonic polynomials of degrees $i$ and $j$ respectively in $B_1 \subset \mathbb{R}^n$ . I'm trying to show for all $i\neq j$ $$ \int_{B_1} \nabla v_j \cdot \nabla v_i dx = 0.$$ I know $$\nabla v_j(x) = r^{1-n}\nabla v_j(rx),$$ it then follows that for all $0<r<1$ $$\int_{B_1} \nabla v_j \cdot \nabla v_i \,dx = C \int_{B_{r}} \nabla v_j \cdot \nabla v_i \,dx$$ from a change of variables. By green's identity I get $$\int_{B_{r}} \nabla v_j \cdot \nabla v_i \,dx = \int_{\partial B_r} v_j d_n v_i \,d\mathcal{H}^{n-1}=\int_{\partial B_r} v_i d_n v_j \,d\mathcal{H}^{n-1}.$$ I don't know how to proceed from here (vector calc not my strong suit). I've tried brute forcing through the gradients of $v_i$ dot with the gradient of $v_j$ in hopes of applying a symmetry argument to show why the integral would be $0$ but it got too messy and confusing notation wise so this is what I've got so far. Would appreciate any hint!",Suppose and are homogenous harmonic polynomials of degrees and respectively in . I'm trying to show for all I know it then follows that for all from a change of variables. By green's identity I get I don't know how to proceed from here (vector calc not my strong suit). I've tried brute forcing through the gradients of dot with the gradient of in hopes of applying a symmetry argument to show why the integral would be but it got too messy and confusing notation wise so this is what I've got so far. Would appreciate any hint!,"v_i v_j i j B_1 \subset \mathbb{R}^n i\neq j  \int_{B_1} \nabla v_j \cdot \nabla v_i dx = 0. \nabla v_j(x) = r^{1-n}\nabla v_j(rx), 0<r<1 \int_{B_1} \nabla v_j \cdot \nabla v_i \,dx = C \int_{B_{r}} \nabla v_j \cdot \nabla v_i \,dx \int_{B_{r}} \nabla v_j \cdot \nabla v_i \,dx = \int_{\partial B_r} v_j d_n v_i \,d\mathcal{H}^{n-1}=\int_{\partial B_r} v_i d_n v_j \,d\mathcal{H}^{n-1}. v_i v_j 0","['multivariable-calculus', 'partial-differential-equations']"
18,"Proving $\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=c\phi(0,0)$ for $\phi$ Schwartz",Proving  for  Schwartz,"\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=c\phi(0,0) \phi","I'm trying to prove that $$\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=c\phi(0,0)$$ for $\phi\in\mathcal S(\mathbb R^2)$ for some constant $c$ . I can see that $(x+iy)^{-1}$ is  smooth and bounded away from the origin, while it has finite integral over the unit ball. Hence by dominated convergence (using that $\phi$ is Schwartz), we can write $$\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=\lim_{\epsilon\to0}\int_{\mathbb R^2-B_\epsilon(0)}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)$$ Away from $(0,0)$ we have $(\partial_x+i\partial_y)(x+iy)=0$ . After this, to manipulate the integral, I'd like to use some divergence formula/ integration by parts formula, and was wondering if we could make sense of something like the following: \begin{align*}&\phantom=\int_{\mathbb R^2-B_\epsilon(0)}\frac{(\partial_ x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)+\int_{\mathbb R^2-B_\epsilon(0)}(\partial_x+i\partial_y)\left(\frac{1}{x+iy}\right)\phi(x,y)\ \mathrm d(x,y)\\&=\int_{\partial B_\epsilon(0)}\frac1{x+iy}\phi(x,y)\cdot \mathbf n\ \mathrm d(x,y)\\&=\int_0^{2\pi}\frac1{\epsilon e^{i\theta}}\phi(\epsilon,\theta)e^{i\theta}\epsilon\ \mathrm d\theta,\end{align*} which would be $\approx2\pi\phi(0,0)$ for $\epsilon$ small, by continuity of $\phi$ . Then the second integral in the first line of the previous display equals zero since on the domain $(\partial_ x+i\partial_y)(x+iy)=0$ . Is such a divergence type formula justified? If not, how to calculate this integral?","I'm trying to prove that for for some constant . I can see that is  smooth and bounded away from the origin, while it has finite integral over the unit ball. Hence by dominated convergence (using that is Schwartz), we can write Away from we have . After this, to manipulate the integral, I'd like to use some divergence formula/ integration by parts formula, and was wondering if we could make sense of something like the following: which would be for small, by continuity of . Then the second integral in the first line of the previous display equals zero since on the domain . Is such a divergence type formula justified? If not, how to calculate this integral?","\int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=c\phi(0,0) \phi\in\mathcal S(\mathbb R^2) c (x+iy)^{-1} \phi \int_{\mathbb R^2}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)=\lim_{\epsilon\to0}\int_{\mathbb R^2-B_\epsilon(0)}\frac{(\partial_x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y) (0,0) (\partial_x+i\partial_y)(x+iy)=0 \begin{align*}&\phantom=\int_{\mathbb R^2-B_\epsilon(0)}\frac{(\partial_ x+i\partial_y)(\phi(x,y))}{x+iy}\ \mathrm d(x,y)+\int_{\mathbb R^2-B_\epsilon(0)}(\partial_x+i\partial_y)\left(\frac{1}{x+iy}\right)\phi(x,y)\ \mathrm d(x,y)\\&=\int_{\partial B_\epsilon(0)}\frac1{x+iy}\phi(x,y)\cdot \mathbf n\ \mathrm d(x,y)\\&=\int_0^{2\pi}\frac1{\epsilon e^{i\theta}}\phi(\epsilon,\theta)e^{i\theta}\epsilon\ \mathrm d\theta,\end{align*} \approx2\pi\phi(0,0) \epsilon \phi (\partial_ x+i\partial_y)(x+iy)=0","['real-analysis', 'multivariable-calculus']"
19,Show that $4(u^2+v^2)\left(\left(\frac{\partial z}{\partial x} \right)^2+\left(\frac{\partial z}{\partial y} \right)^2\right)$,Show that,4(u^2+v^2)\left(\left(\frac{\partial z}{\partial x} \right)^2+\left(\frac{\partial z}{\partial y} \right)^2\right),"Let $x = u^2-v^2$ and $y = 2uv$ , and suppose that $z = f(x,y)$ is differentiable. Show that: $$ \left(\frac{\partial z}{\partial u}\right) ^2+\left(\frac{\partial z}{\partial v} \right)^2 =  4(u^2+v^2)\left(\left(\frac{\partial z}{\partial x} \right)^2+\left(\frac{\partial z}{\partial y} \right)^2\right)$$ By taking the derivative in respect to $u$ and $v$ then taking their composite values: $\frac{d}{dx}=2u-2v; \frac{d}{dy} = 2v+2u$ Which are equivalent to: $2(u-v)2(u+v)=4(u^2-v^2)$ However the signs are different as I'm expected to get a positive between $u$ and $v$ .","Let and , and suppose that is differentiable. Show that: By taking the derivative in respect to and then taking their composite values: Which are equivalent to: However the signs are different as I'm expected to get a positive between and .","x = u^2-v^2 y = 2uv z = f(x,y)  \left(\frac{\partial z}{\partial u}\right) ^2+\left(\frac{\partial z}{\partial v}
\right)^2 =  4(u^2+v^2)\left(\left(\frac{\partial z}{\partial x} \right)^2+\left(\frac{\partial z}{\partial y} \right)^2\right) u v \frac{d}{dx}=2u-2v; \frac{d}{dy} = 2v+2u 2(u-v)2(u+v)=4(u^2-v^2) u v",['multivariable-calculus']
20,Directional Derivative of a piecewise defined function,Directional Derivative of a piecewise defined function,,"Here is a problem and the solution to it. $\quad$ let $f: \mathbb{R}^{3} \rightarrow \mathbb{R}$ be a continuously differentiable function with: $$ f(t, 2 t, 0)=e^{3 t}+1, \quad f(t,-t,-t)=2 \cos \left(t^{3}\right)+3 t, \quad f(0, t, 3 t)=\log \left(t^{2}+1\right)+2 $$ a) Compute the directional derivatives $D_{v} f(0,0,0)$ for $v_{1}=(1,2,0), v_{2}=(-1,1,1)$ and $v_{3}=(0,1,3)$ . b) Find $D f(0,0,0)$ . Solution: a) $f(0,0,0)=2 .$ We take the directional derivatives $$ \begin{aligned} &D_{(1,2,0)} f(0,0,0)=\lim _{t \rightarrow 0} \frac{f(t, 2 t, 0)-f(0,0,0)}{t}=\lim _{t \rightarrow 0} \frac{e^{3 t}+1-2}{t}=\lim _{t \rightarrow 0} 3 e^{3 t}=3 \\ &D_{(-1,1,1)} f(0,0,0)=\lim _{t \rightarrow 0} \frac{f(-t, t, t)-f(0,0,0)}{t}=\lim _{t \rightarrow 0} \frac{2 \cos \left(-t^{3}\right)-3 t-2}{t}=\lim _{t \rightarrow 0} 2 \sin \left(-t^{3}\right) 3 t^{2}-3=-3, \\ &D_{(0,1,3)} f(0,0,0)=\lim _{t \rightarrow 0} \frac{f(0, t, 3 t)-f(0,0,0)}{t}=\lim _{t \rightarrow 0} \frac{\log \left(t^{2}+1\right)+2-2}{t}=\lim _{t \rightarrow 0} \frac{2 t}{t^{2}+1}=0 \end{aligned} $$ b) We can compute the directional derivatives the following way $D_{v} f(0,0,0)=D f(0,0,0) \cdot v$ . By $$ D f(0,0,0)=\left(\frac{\partial}{\partial x_{1}} f(0,0,0), \frac{\partial}{\partial x_{2}} f(0,0,0), \frac{\partial}{\partial x_{3}} f(0,0,0)\right)=:(a, b, c) $$ we get the system of equation $$ a+2 b=3 \quad \wedge \quad-a+b+c=-3 \quad \wedge \quad b+3 c=0 $$ so, $a=3$ und $b=c=0$ . We conclude $D f(0,0,0)=(3,0,0)$ . My first question is: How can I (if possible) find an explicit form of the gradient such that I don't have to have a system of equations to find $D f(x,y,z)$ usually regarding tasks about directional derivatives I just computed the partial derivatives to get the gradient, plugged in the point in question and made a dot product with the direction. Second question: How do I (if possible at all) compute the directional derivative at a different point in the directions of the $v_{i=1,2,3}$ or even better in other directions? is the following possible? $$ \begin{aligned} &D_{(1,2,0)} f(5,4,3)=... \\ &D_{(-1,1,1)} f(5,4,3)=... \\ &D_{(0,1,3)} f(5,4,3)=... \end{aligned} $$ or $$ \begin{aligned} &D_{(3,2,1)} f(5,4,3)=... \\ &D_{(-1,2,4)} f(5,4,3)=... \\ &D_{(0,-1,2)} f(5,4,3)=... \end{aligned} $$ My feeling was none of the two suggestions is possible, because the function is not defined at the point $(5,4,3)$ and the new randomly chosen $v_{i=4,5,6}$ in the second example? I was also playing with the thought that any point could be decomposed into a linear combination of the original $v_{i=1,2,3}$ such that it'd be possible to take the directional derivative at any point? I could not find similar tasks to extrapolate some information from them, maybe someone knows how I could find similar examples - I didn't know what to google for to get similar tasks. Thanks!","Here is a problem and the solution to it. let be a continuously differentiable function with: a) Compute the directional derivatives for and . b) Find . Solution: a) We take the directional derivatives b) We can compute the directional derivatives the following way . By we get the system of equation so, und . We conclude . My first question is: How can I (if possible) find an explicit form of the gradient such that I don't have to have a system of equations to find usually regarding tasks about directional derivatives I just computed the partial derivatives to get the gradient, plugged in the point in question and made a dot product with the direction. Second question: How do I (if possible at all) compute the directional derivative at a different point in the directions of the or even better in other directions? is the following possible? or My feeling was none of the two suggestions is possible, because the function is not defined at the point and the new randomly chosen in the second example? I was also playing with the thought that any point could be decomposed into a linear combination of the original such that it'd be possible to take the directional derivative at any point? I could not find similar tasks to extrapolate some information from them, maybe someone knows how I could find similar examples - I didn't know what to google for to get similar tasks. Thanks!","\quad f: \mathbb{R}^{3} \rightarrow \mathbb{R} 
f(t, 2 t, 0)=e^{3 t}+1, \quad f(t,-t,-t)=2 \cos \left(t^{3}\right)+3 t, \quad f(0, t, 3 t)=\log \left(t^{2}+1\right)+2
 D_{v} f(0,0,0) v_{1}=(1,2,0), v_{2}=(-1,1,1) v_{3}=(0,1,3) D f(0,0,0) f(0,0,0)=2 . 
\begin{aligned}
&D_{(1,2,0)} f(0,0,0)=\lim _{t \rightarrow 0} \frac{f(t, 2 t, 0)-f(0,0,0)}{t}=\lim _{t \rightarrow 0} \frac{e^{3 t}+1-2}{t}=\lim _{t \rightarrow 0} 3 e^{3 t}=3 \\
&D_{(-1,1,1)} f(0,0,0)=\lim _{t \rightarrow 0} \frac{f(-t, t, t)-f(0,0,0)}{t}=\lim _{t \rightarrow 0} \frac{2 \cos \left(-t^{3}\right)-3 t-2}{t}=\lim _{t \rightarrow 0} 2 \sin \left(-t^{3}\right) 3 t^{2}-3=-3, \\
&D_{(0,1,3)} f(0,0,0)=\lim _{t \rightarrow 0} \frac{f(0, t, 3 t)-f(0,0,0)}{t}=\lim _{t \rightarrow 0} \frac{\log \left(t^{2}+1\right)+2-2}{t}=\lim _{t \rightarrow 0} \frac{2 t}{t^{2}+1}=0
\end{aligned}
 D_{v} f(0,0,0)=D f(0,0,0) \cdot v 
D f(0,0,0)=\left(\frac{\partial}{\partial x_{1}} f(0,0,0), \frac{\partial}{\partial x_{2}} f(0,0,0), \frac{\partial}{\partial x_{3}} f(0,0,0)\right)=:(a, b, c)
 
a+2 b=3 \quad \wedge \quad-a+b+c=-3 \quad \wedge \quad b+3 c=0
 a=3 b=c=0 D f(0,0,0)=(3,0,0) D f(x,y,z) v_{i=1,2,3} 
\begin{aligned} &D_{(1,2,0)} f(5,4,3)=... \\
&D_{(-1,1,1)} f(5,4,3)=... \\
&D_{(0,1,3)} f(5,4,3)=...
\end{aligned}
 
\begin{aligned} &D_{(3,2,1)} f(5,4,3)=... \\
&D_{(-1,2,4)} f(5,4,3)=... \\
&D_{(0,-1,2)} f(5,4,3)=...
\end{aligned}
 (5,4,3) v_{i=4,5,6} v_{i=1,2,3}",['multivariable-calculus']
21,Properly express the integral to interchange the order of integration,Properly express the integral to interchange the order of integration,,"Calculate the integral, draw the region A and express it appropriately to interchange the order of integration $$\int_Af=\int_{-1}^1\int _{-2|x|}^{|x|}e^{x+y}\,dy\,dx$$ Let's first see what $A$ is like Note that in our drawing of A $ -1 \leq x \leq 1 $ y $ -2 \leq y \leq 1 $ We have already found the new outer limits, now we will find the inner ones. On the other hand, let us note that we can express our integral as the sum of two integrals, taking the left part as $ B $ and the right part as $ C $ , so $$\int_Af=\int_Bf+\int_Cf= \int_{-1}^1\int _{2x}^{-x} e^{x+y} \,dy\,dx+\int_{-1}^1\int _{-2x}^x e^{x+y} \, dy \, dx$$ Let's start by changing the order of integration in $B$ , we know that $ -2 \leq y \leq 1 $ if $ y = -x $ then $ -y = x $ , on the other hand if $ y = 2x $ then $\frac{y}{2} = x$ . Then $$\int_Bf=\int_{-1}^1\int _{2x}^{-x} e^{x+y} \, dy \, dx = \int_{-2}^1\int_{\frac{y}{2}}^{-y}e^{x+y} \, dx \, dy$$ In the same way for the integral in $C,$ $ y = x $ and if $ y = -2x $ then $ - \frac {y} {2} = x $ then $$\int_Cf=\int_{-1}^1\int_{-2x}^x e^{x+y} \, dy \, dx = \int_{-2}^1 \int_{-\frac{y}{2}}^y e^{x+y} \, dx \, dy$$ then $$\int_Af = \int_{-2}^1\int_{\frac{y}{2}}^{-y} e^{x+y} \, dx \, dy + \int_{-2}^1\int_{-\frac{y}{2}}^y e^{x+y} \, dx \, dy$$ However, at the time of integral I do not get the desired result, I suppose that the way in which I change the integration limits is not adequate, can someone help me to change the integration limits appropriately?","Calculate the integral, draw the region A and express it appropriately to interchange the order of integration Let's first see what is like Note that in our drawing of A y We have already found the new outer limits, now we will find the inner ones. On the other hand, let us note that we can express our integral as the sum of two integrals, taking the left part as and the right part as , so Let's start by changing the order of integration in , we know that if then , on the other hand if then . Then In the same way for the integral in and if then then then However, at the time of integral I do not get the desired result, I suppose that the way in which I change the integration limits is not adequate, can someone help me to change the integration limits appropriately?","\int_Af=\int_{-1}^1\int _{-2|x|}^{|x|}e^{x+y}\,dy\,dx A  -1 \leq x \leq 1   -2 \leq y \leq 1   B   C  \int_Af=\int_Bf+\int_Cf= \int_{-1}^1\int _{2x}^{-x} e^{x+y} \,dy\,dx+\int_{-1}^1\int _{-2x}^x e^{x+y} \, dy \, dx B  -2 \leq y \leq 1   y = -x   -y = x   y = 2x  \frac{y}{2} = x \int_Bf=\int_{-1}^1\int _{2x}^{-x} e^{x+y} \, dy \, dx = \int_{-2}^1\int_{\frac{y}{2}}^{-y}e^{x+y} \, dx \, dy C,  y = x   y = -2x   - \frac {y} {2} = x  \int_Cf=\int_{-1}^1\int_{-2x}^x e^{x+y} \, dy \, dx = \int_{-2}^1 \int_{-\frac{y}{2}}^y e^{x+y} \, dx \, dy \int_Af = \int_{-2}^1\int_{\frac{y}{2}}^{-y} e^{x+y} \, dx \, dy + \int_{-2}^1\int_{-\frac{y}{2}}^y e^{x+y} \, dx \, dy","['multivariable-calculus', 'multiple-integral']"
22,Partial derivative of a derivative of a function with respect to itself,Partial derivative of a derivative of a function with respect to itself,,"Similar post, but opposite conclusion? I have a function $x(t)$ and its derivative as a function of other variables $\frac{dx}{dt}(x(t))$ (a differential equation?) and I want to find $\frac{\partial }{\partial x}\frac{dx}{dt}$ . The linked post above says this is zero, and I can see where they are coming from swapping the order of the derivatives. However, I just don't believe it, I have likely misunderstood their point. For instance, say $\frac{dx}{dt}=x^2$ . This is easily solvable, e.g. $x=\frac{-1}{t}$ . But $\frac{\partial}{\partial x}\frac{dx}{dt}=2x$ which is not zero in general. Yet $\frac{\partial}{\partial x}\frac{d}{dt}x=\frac{d}{dt}\frac{\partial}{\partial x}x=\frac{d}{dt}1=0$ ? What goes wrong here? EDIT: is it because $x$ is a function of $t$ ? So we cant swap them?","Similar post, but opposite conclusion? I have a function and its derivative as a function of other variables (a differential equation?) and I want to find . The linked post above says this is zero, and I can see where they are coming from swapping the order of the derivatives. However, I just don't believe it, I have likely misunderstood their point. For instance, say . This is easily solvable, e.g. . But which is not zero in general. Yet ? What goes wrong here? EDIT: is it because is a function of ? So we cant swap them?",x(t) \frac{dx}{dt}(x(t)) \frac{\partial }{\partial x}\frac{dx}{dt} \frac{dx}{dt}=x^2 x=\frac{-1}{t} \frac{\partial}{\partial x}\frac{dx}{dt}=2x \frac{\partial}{\partial x}\frac{d}{dt}x=\frac{d}{dt}\frac{\partial}{\partial x}x=\frac{d}{dt}1=0 x t,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
23,Flux as a mapping,Flux as a mapping,,"I'm reading the proof of Theorem 5.1 in this paper . I have a few questions about flux. They say that for a vector field $\xi \in C^\infty_0(\mathbb{R}^n)$ , the corresponding flux is $\{\Phi_\tau\}_{\tau \in \mathbb{R}}$ satisfying $$ \partial_\tau \Phi_\tau = \xi \circ \Phi_\tau \;\;\;\;\; \text{ for all }\tau \in \mathbb{R}.$$ Then, they say that $\Phi_\tau$ defines a pushforward mapping on measures, i.e. for a given measure $\rho^k(y)$ , its pushforward under $\Phi_\tau$ is the measure $\rho_\tau (y)$ with $$ \int_{\mathbb{R}^n} \rho_\tau(y) \zeta(y)dy = \int_{\mathbb{R}^n} \rho^k (y) \zeta(\Phi_\tau(y))dy \forall \zeta \in C^0_0(\mathbb{R}^n).$$ I don't understand the first equation (definition of the flux). I thought that flux was a surface integral. Here, since the vector field has bounded support, I would guess that the corresponding flux would be the flux out of the boundary. I don't know what $\tau$ represents here, either. Furthermore, how can $\Phi_\tau$ be a mapping on measures?","I'm reading the proof of Theorem 5.1 in this paper . I have a few questions about flux. They say that for a vector field , the corresponding flux is satisfying Then, they say that defines a pushforward mapping on measures, i.e. for a given measure , its pushforward under is the measure with I don't understand the first equation (definition of the flux). I thought that flux was a surface integral. Here, since the vector field has bounded support, I would guess that the corresponding flux would be the flux out of the boundary. I don't know what represents here, either. Furthermore, how can be a mapping on measures?",\xi \in C^\infty_0(\mathbb{R}^n) \{\Phi_\tau\}_{\tau \in \mathbb{R}}  \partial_\tau \Phi_\tau = \xi \circ \Phi_\tau \;\;\;\;\; \text{ for all }\tau \in \mathbb{R}. \Phi_\tau \rho^k(y) \Phi_\tau \rho_\tau (y)  \int_{\mathbb{R}^n} \rho_\tau(y) \zeta(y)dy = \int_{\mathbb{R}^n} \rho^k (y) \zeta(\Phi_\tau(y))dy \forall \zeta \in C^0_0(\mathbb{R}^n). \tau \Phi_\tau,"['integration', 'measure-theory', 'multivariable-calculus', 'physics']"
24,"Prove that the Jacobian matrix is the matrix representation of the derivative. Non-ambiguous statement for ""is the matrix representation of""?","Prove that the Jacobian matrix is the matrix representation of the derivative. Non-ambiguous statement for ""is the matrix representation of""?",,"I'm trying to prove that the matrix representation of the derivative is the jacobian matrix, but I can't find a non-ambiguous statement to complete the proof. I've tried the following proof technique: I'm looking to establish the identity between the $j$ th partial derivative of $f(\vec a)$ designated by $f'(\vec a)(\vec e_j)$ and the $j$ th column of the Jacobian matrix, but I still can't complete the proof. I don't know if I'm making a mistake or if I'm lacking linear algebra here somewhere. Here's the argument: Let $\{ e_1, ..., e_n\}$ be the standard basis of $\mathbb{R}^n$ . Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ , and suppose $f$ is differentiable at $\vec a$ . Therefore, we have $$\lim \limits_{\vec h \to \vec 0} \frac{f(\vec a + \vec h) - f(\vec a) - f'(\vec a)(\vec h)}{||\vec h||} =0$$ If we let $\vec h = t\vec e_j$ for any $j = 1, ..., n$ and do some manipulations, we can arrive at $$f'(\vec a)(\vec e_j) = \lim \limits_{t \to 0} \frac{f(\vec a + t\vec e_j) - f(\vec a)}{t}$$ Which is the definition of the $j$ th partial derivative of $f(\vec a)$ , from which we can conclude that $f'(\vec a)(\vec e_j)$ is the $j$ th partial derivative of $f(\vec a)$ . This where I'm stuck . What would be a meaningful statement establishing ""the matrix representation of the derivative is the jacobian matrix""? I'm thinking that if I can say: the $j$ th column of the jacobian is identical to $f'(\vec a)(\vec e_j)$ , this could be acceptable, but what exactly is the definition of identity that I'm looking for here? It seems to me that the statement "" $f'(\vec a)(\vec e_j)$ is identical to the $j$ th column of the Jacobian matrix"" is an ambiguous statement. To give an example the element $a_{ij}$ of the Jacobian is not identical to the ith component of the vector $f'(\vec a)(\vec e_j)$ , the former being a real number and the latter being a vector. Any help is appreciated!","I'm trying to prove that the matrix representation of the derivative is the jacobian matrix, but I can't find a non-ambiguous statement to complete the proof. I've tried the following proof technique: I'm looking to establish the identity between the th partial derivative of designated by and the th column of the Jacobian matrix, but I still can't complete the proof. I don't know if I'm making a mistake or if I'm lacking linear algebra here somewhere. Here's the argument: Let be the standard basis of . Suppose , and suppose is differentiable at . Therefore, we have If we let for any and do some manipulations, we can arrive at Which is the definition of the th partial derivative of , from which we can conclude that is the th partial derivative of . This where I'm stuck . What would be a meaningful statement establishing ""the matrix representation of the derivative is the jacobian matrix""? I'm thinking that if I can say: the th column of the jacobian is identical to , this could be acceptable, but what exactly is the definition of identity that I'm looking for here? It seems to me that the statement "" is identical to the th column of the Jacobian matrix"" is an ambiguous statement. To give an example the element of the Jacobian is not identical to the ith component of the vector , the former being a real number and the latter being a vector. Any help is appreciated!","j f(\vec a) f'(\vec a)(\vec e_j) j \{ e_1, ..., e_n\} \mathbb{R}^n f: \mathbb{R}^n \rightarrow \mathbb{R}^m f \vec a \lim \limits_{\vec h \to \vec 0} \frac{f(\vec a + \vec h) - f(\vec a) - f'(\vec a)(\vec h)}{||\vec h||} =0 \vec h = t\vec e_j j = 1, ..., n f'(\vec a)(\vec e_j) = \lim \limits_{t \to 0} \frac{f(\vec a + t\vec e_j) - f(\vec a)}{t} j f(\vec a) f'(\vec a)(\vec e_j) j f(\vec a) j f'(\vec a)(\vec e_j) f'(\vec a)(\vec e_j) j a_{ij} f'(\vec a)(\vec e_j)","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'proof-writing']"
25,Solution Verification: Solving this limit with two variables.,Solution Verification: Solving this limit with two variables.,,"$$\lim_{(x,y)\to(0,0)}\frac{y-\sin(y)+2x^2+2y^2}{x^2+y^2}$$ What I did: The limit is equal to: $\lim_{(x,y)\to(0,0)}\frac{y-\sin(y)}{x^2+y^2}+2$ I'm trying to find how does $y-\sin(y)$ ""act"" near $0$ . I know that $\sin(y)\sim y$ near $0$ . But what I have here is $y-\sin(y)$ , so I've thought of trying to find it using Taylor's polynomial at $(0)$ , $\sin(y)\sim y-\frac{y^3}{3!} \Longrightarrow y-\sin(y)\sim y-(y-\frac{y^3}{6})=\frac{y^3}{6}$ . Now I could say ""The power in the numerator is higher than the denominator which means the numerator goes faster to $0$ , and the limit exists"". But in order to prove it mathematically, I tried to do this: $$\lim_{(x,y)\to(0,0)}\frac{y-\sin(y)}{x^2+y^2}=\lim_{(x,y)\to(0,0)}\frac{y-\sin(y)}{y^3} \frac{y^3}{x^2+y^2}$$ And now if both of the limits exist, then the multiplication of them exist. $$\lim_{y\to 0}\frac{y-\sin(y)}{y^3}=\lim_{y\to 0}\frac{1-\cos(y)}{3y^2}=\lim_{y\to 0}\frac{\sin(y)}{6y}=\lim_{y\to 0}\frac{\cos(y)}{6}=\frac{1}{6}$$ And it's easy to show that the right fraction goes to zero, and so this limit goes to $0$ , and my whole limit goes to $2$ . I would love to get an approval of my method, and if anything could have been done better. I would appreciate any other methods of solving this limit as I've struggled to come up with this solution and would love to see other cool tricks.","What I did: The limit is equal to: I'm trying to find how does ""act"" near . I know that near . But what I have here is , so I've thought of trying to find it using Taylor's polynomial at , . Now I could say ""The power in the numerator is higher than the denominator which means the numerator goes faster to , and the limit exists"". But in order to prove it mathematically, I tried to do this: And now if both of the limits exist, then the multiplication of them exist. And it's easy to show that the right fraction goes to zero, and so this limit goes to , and my whole limit goes to . I would love to get an approval of my method, and if anything could have been done better. I would appreciate any other methods of solving this limit as I've struggled to come up with this solution and would love to see other cool tricks.","\lim_{(x,y)\to(0,0)}\frac{y-\sin(y)+2x^2+2y^2}{x^2+y^2} \lim_{(x,y)\to(0,0)}\frac{y-\sin(y)}{x^2+y^2}+2 y-\sin(y) 0 \sin(y)\sim y 0 y-\sin(y) (0) \sin(y)\sim y-\frac{y^3}{3!} \Longrightarrow y-\sin(y)\sim y-(y-\frac{y^3}{6})=\frac{y^3}{6} 0 \lim_{(x,y)\to(0,0)}\frac{y-\sin(y)}{x^2+y^2}=\lim_{(x,y)\to(0,0)}\frac{y-\sin(y)}{y^3} \frac{y^3}{x^2+y^2} \lim_{y\to 0}\frac{y-\sin(y)}{y^3}=\lim_{y\to 0}\frac{1-\cos(y)}{3y^2}=\lim_{y\to 0}\frac{\sin(y)}{6y}=\lim_{y\to 0}\frac{\cos(y)}{6}=\frac{1}{6} 0 2","['limits', 'multivariable-calculus', 'solution-verification']"
26,Wrong variable substitution that I am doing,Wrong variable substitution that I am doing,,"I spent 3 hours today on the following integral, (I am trying to verify why my variable substitution fails): $$\int\int\frac{x^2\sin(xy)}{y}dxdy$$ over $$\Omega=\{x^2<y<2x^2, y^2<x<2y^2\}$$ which is (everything is positive): $$\Omega=\{1<y/x^2<2, 1<x/y^2<2\}$$ So, I deduced that for a proper transform $T$ we will get that: $$T\Omega=\{(u,v)=T(x,y):1<u<2, 1<v<2\}$$ So we want the opposite of $T(x,y)=(y/x^2,x/y^2)$ . I will skip some of the calculations, but I got that: $$|J_T|=3x^-2y^-2\Longrightarrow |J_{T^-1}|=3^{-1}x^2y^2$$ and that $$T^{-1}(x,y)=(y^{-\frac{1}{3}}x^{-\frac{2}{3}},y^{-\frac{2}{3}}x^{-\frac{1}{3}})$$ and that $$\int_\Omega f=\int_{T\Omega}f\circ T^{-1}|J_{T^{-1}}|$$ which is after putting all of the information: $$\frac{1}{3}\int_1^2\int_1^2xy^2\sin(y^{-1}x^{-1})$$ And this is not leading to anywhere, and I also compared to a full solution that does somewhat similar variable substitution, and after that, he got similar, but solveable integral: $$\frac{1}{3}\int_{\frac{1}{2}}^1\int_{\frac{1}{2}}^1u\sin(uv)$$ I will appreciate any kind of help, I spent too much time on this","I spent 3 hours today on the following integral, (I am trying to verify why my variable substitution fails): over which is (everything is positive): So, I deduced that for a proper transform we will get that: So we want the opposite of . I will skip some of the calculations, but I got that: and that and that which is after putting all of the information: And this is not leading to anywhere, and I also compared to a full solution that does somewhat similar variable substitution, and after that, he got similar, but solveable integral: I will appreciate any kind of help, I spent too much time on this","\int\int\frac{x^2\sin(xy)}{y}dxdy \Omega=\{x^2<y<2x^2, y^2<x<2y^2\} \Omega=\{1<y/x^2<2, 1<x/y^2<2\} T T\Omega=\{(u,v)=T(x,y):1<u<2, 1<v<2\} T(x,y)=(y/x^2,x/y^2) |J_T|=3x^-2y^-2\Longrightarrow |J_{T^-1}|=3^{-1}x^2y^2 T^{-1}(x,y)=(y^{-\frac{1}{3}}x^{-\frac{2}{3}},y^{-\frac{2}{3}}x^{-\frac{1}{3}}) \int_\Omega f=\int_{T\Omega}f\circ T^{-1}|J_{T^{-1}}| \frac{1}{3}\int_1^2\int_1^2xy^2\sin(y^{-1}x^{-1}) \frac{1}{3}\int_{\frac{1}{2}}^1\int_{\frac{1}{2}}^1u\sin(uv)","['multivariable-calculus', 'multiple-integral', 'change-of-variable']"
27,Calculating triple integral between paraboloid and plane,Calculating triple integral between paraboloid and plane,,"I have the following integral: $$ \iiint zdxdydz$$ on the area bound by the following surfaces: $$z=\frac{a^2}{b^2} (x^2+y^2)$$ $$z=a$$ $$a,b>0$$ The first surface is a paraboloid and the second is a plane. Now, this is how I proceeded: Introducing cylindrical coordinates: $x=r\cos\phi$ $y=r\sin\phi$ $|J| = r$ now, what I did next was substitute $z=a$ in the equation of the paraboloid. I got that $x^2+y^2 = \dfrac{b^2}{a}$ , or that $r=\dfrac{b}{\sqrt{a}}$ which led me to believe that $$r\in \left[0, \frac{b}{\sqrt{a}}\right]$$ $$\phi \in \left[0, 2\pi\right]$$ Now regarding $z$ , I know that it is bound from below by the paraboloid and from above by the plane, so I deduced that the boundaries are: $$z\in \left[\frac{a^2}{b^2}r^2, a\right]$$ . My integral gets the form: $$\int_0^{\frac{b}{\sqrt{a}}} \int_0^{2\pi} \int_{r\frac{a^2}{b^2}}^a zr dz d\phi dr$$ Which evaluates to $\dfrac{\pi ab^2}{3}$ , but my workbook solution is $\dfrac{a^2 b^2 \pi}{4}$ . Could anyone tell me where I went wrong?","I have the following integral: on the area bound by the following surfaces: The first surface is a paraboloid and the second is a plane. Now, this is how I proceeded: Introducing cylindrical coordinates: now, what I did next was substitute in the equation of the paraboloid. I got that , or that which led me to believe that Now regarding , I know that it is bound from below by the paraboloid and from above by the plane, so I deduced that the boundaries are: . My integral gets the form: Which evaluates to , but my workbook solution is . Could anyone tell me where I went wrong?"," \iiint zdxdydz z=\frac{a^2}{b^2} (x^2+y^2) z=a a,b>0 x=r\cos\phi y=r\sin\phi |J| = r z=a x^2+y^2 = \dfrac{b^2}{a} r=\dfrac{b}{\sqrt{a}} r\in \left[0, \frac{b}{\sqrt{a}}\right] \phi \in \left[0, 2\pi\right] z z\in \left[\frac{a^2}{b^2}r^2, a\right] \int_0^{\frac{b}{\sqrt{a}}} \int_0^{2\pi} \int_{r\frac{a^2}{b^2}}^a zr dz d\phi dr \dfrac{\pi ab^2}{3} \dfrac{a^2 b^2 \pi}{4}","['integration', 'multivariable-calculus', 'multiple-integral', 'cylindrical-coordinates']"
28,Circle of Best Approximation and Curvature,Circle of Best Approximation and Curvature,,"Recently I have been studying the curvature formula. I understood the change in direction with respect to distance definition $||\frac{dT}{ds}||$ , but was bothered when introduced to the radius of curvature definition. Since most explanations use the differential definition to explain the derivation of the formula, they only gloss over the radius form. On Wikipedia it this is what it states about Radius of Curvature: ""For a curve, it equals the radius of the circular arc which best approximates the curve at that point."" https://en.wikipedia.org/wiki/Curvature What my question is, is how can the circle of best approximation be defined? in Taylor Series, we learn about the best possible polynomial approximation of a certain degree, so how do we define the best circle approximation? thank you for reading :)","Recently I have been studying the curvature formula. I understood the change in direction with respect to distance definition , but was bothered when introduced to the radius of curvature definition. Since most explanations use the differential definition to explain the derivation of the formula, they only gloss over the radius form. On Wikipedia it this is what it states about Radius of Curvature: ""For a curve, it equals the radius of the circular arc which best approximates the curve at that point."" https://en.wikipedia.org/wiki/Curvature What my question is, is how can the circle of best approximation be defined? in Taylor Series, we learn about the best possible polynomial approximation of a certain degree, so how do we define the best circle approximation? thank you for reading :)",||\frac{dT}{ds}||,"['multivariable-calculus', 'differential-geometry']"
29,"Question about improper integral, Munkres Problem 15.5","Question about improper integral, Munkres Problem 15.5",,"I have a question about improper integral from Munkres' Analysis on Manifold chapter 15. This is problem 5: What I have done was set up the double iterated integral over A and B and showed that the first one goes to infinity and the second one has a defined value and exist. My question is whether that is sufficient to prove that the first (improper) integral does not exist and the second one exist? The reason is because the integral may or may not exist, while the iterated integral may exist but be different than the integral. Both A and B are simple regions so Fubini's theorem can apply, but I think it needs to be a bounded region. So what I should do is to build a increasing sequence of compact rectifiable sets $U_n$ so that $U_1$ is in $U_2$ and so on and the union of all $U_n$ is the region A or B. How should I construct these sequence of sets?","I have a question about improper integral from Munkres' Analysis on Manifold chapter 15. This is problem 5: What I have done was set up the double iterated integral over A and B and showed that the first one goes to infinity and the second one has a defined value and exist. My question is whether that is sufficient to prove that the first (improper) integral does not exist and the second one exist? The reason is because the integral may or may not exist, while the iterated integral may exist but be different than the integral. Both A and B are simple regions so Fubini's theorem can apply, but I think it needs to be a bounded region. So what I should do is to build a increasing sequence of compact rectifiable sets so that is in and so on and the union of all is the region A or B. How should I construct these sequence of sets?",U_n U_1 U_2 U_n,"['real-analysis', 'integration', 'multivariable-calculus']"
30,The polar coordinates change of variables,The polar coordinates change of variables,,"In my textbook, the author states that $$ \iint_R f(r\cos\theta,r\sin \theta)r\,dr\,d\theta =\iint_{G(R)} f(x,y)\,dy\,dx, $$ where $G(r,\theta)=(r\cos\theta, r\sin \theta)$ and he says that this is the formula for the change of variables between rectangular coordinates and polar ones. But for me, it doesn't look right, if we want to change coordinates then the formula should be like this $$ \iint_R f(x,y)\,dy\,dx=\iint_{G(R)} f(r\cos\theta,r\sin \theta)r\,dr\,d\theta. $$ Am I right? or are both formulas the same thing? This is a picture from the textbook, it says that the map $G$ takes rectangular to polar","In my textbook, the author states that where and he says that this is the formula for the change of variables between rectangular coordinates and polar ones. But for me, it doesn't look right, if we want to change coordinates then the formula should be like this Am I right? or are both formulas the same thing? This is a picture from the textbook, it says that the map takes rectangular to polar","
\iint_R f(r\cos\theta,r\sin \theta)r\,dr\,d\theta =\iint_{G(R)} f(x,y)\,dy\,dx,
 G(r,\theta)=(r\cos\theta, r\sin \theta) 
\iint_R f(x,y)\,dy\,dx=\iint_{G(R)} f(r\cos\theta,r\sin \theta)r\,dr\,d\theta.
 G","['multivariable-calculus', 'polar-coordinates', 'change-of-variable']"
31,Solving PDE with Laplace Transform method.,Solving PDE with Laplace Transform method.,,"can someone assist me in solving the following PDE with the Laplace Transform method: $ \qquad \qquad \qquad \qquad \qquad \quad u_t = u_{xx} - a^2 (u-T_0) \qquad 0 < x < 1, \qquad   t>0 $ Boundary conditions: $ \qquad u_x (0,t) = 0  \quad \text{and}                      \quad u_x (1,t) = 0  \qquad \qquad t>0  $ Initial condition: $ \qquad \qquad  u(x,0) = 0  \qquad \qquad \qquad \quad 0 < x < 1 $ The answer given in the notes is: $ \qquad u(x,t) = T_0(1-e^{-a^2t}) $ I attempted to solve the problem as follows : The first thing I did was take the Laplace transform with respect to the variable t on both sides. (transformed variable os equal to s) Taking Laplace transform on both sides gives me the following: $ \qquad \qquad \qquad \qquad sU(x,s) - U(x,0) = U_{xx} - \frac 1s .[a^2.(u-T_0)] $ Substituting the initial condition and rearranging the equation I get: $ \qquad \qquad \qquad \qquad U_{xx} - sU(x,s) = \frac 1s .[a^2.(u-T_0)] $ From here I attempted to solve the equation using the d-operator method which gives me the following complementary function: $ \qquad \qquad \qquad \qquad  U(x,s) $ = $ A.\cosh(\sqrt {s} . x  ) $ $ + B.\sinh(\sqrt {s} . x  ) $ This is the part where I get stucked as I don't know how to obtain the particular integral of $ \frac 1s .[a^2.(u-T_0)] $ Can anyone please assist me in moving forward to solve this problem? Thanks a lot for your help!",can someone assist me in solving the following PDE with the Laplace Transform method: Boundary conditions: Initial condition: The answer given in the notes is: I attempted to solve the problem as follows : The first thing I did was take the Laplace transform with respect to the variable t on both sides. (transformed variable os equal to s) Taking Laplace transform on both sides gives me the following: Substituting the initial condition and rearranging the equation I get: From here I attempted to solve the equation using the d-operator method which gives me the following complementary function: = This is the part where I get stucked as I don't know how to obtain the particular integral of Can anyone please assist me in moving forward to solve this problem? Thanks a lot for your help!," \qquad \qquad \qquad \qquad \qquad \quad u_t = u_{xx} - a^2 (u-T_0) \qquad 0 < x < 1, \qquad   t>0   \qquad u_x (0,t) = 0  \quad \text{and}
                     \quad u_x (1,t) = 0  \qquad \qquad t>0    \qquad \qquad  u(x,0) = 0  \qquad \qquad \qquad \quad 0 < x < 1   \qquad u(x,t) = T_0(1-e^{-a^2t})   \qquad \qquad \qquad \qquad sU(x,s) - U(x,0) = U_{xx} - \frac 1s .[a^2.(u-T_0)]   \qquad \qquad \qquad \qquad U_{xx} - sU(x,s) = \frac 1s .[a^2.(u-T_0)]   \qquad \qquad \qquad \qquad  U(x,s)   A.\cosh(\sqrt {s} . x  )   + B.\sinh(\sqrt {s} . x  )   \frac 1s .[a^2.(u-T_0)] ","['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'laplace-transform']"
32,An integration on the sphere (a rescaling problem),An integration on the sphere (a rescaling problem),,"Let $\mathbb{S}^{n-1}$ be the unit sphere in $\mathbb{R}^{n}$ and let $f$ be smooth on $\mathbb{R}^{n}$ . Is it  possible to express the integral $$I(\lambda):=\int_{\mathbb{S}^{n-1}}f(\lambda x_1,x_2,\cdots,x_n) d\sigma(x),\qquad \lambda >0$$ in terms of the integral $$J:=\int_{\mathbb{S}^{n-1}}f(x_1,x_2,\cdots,x_n) d\sigma(x).$$ This is a very simple special case of the my old question here Changing variables in integration over spheres Some thought: If the point $(x_1,x_2,\cdots,x_n)$ lives on the sphere $x_1^2+\cdots+x_n^2=1$ then $(\lambda x_1,x_2,\cdots,x_n)$ lives on the ellipsoid $E_{\lambda}:=\{(x_1,x_2,\cdots,x_n)\in \mathbb{R}^n: x_1^2/\lambda^2+\cdots+x_n^2=1\}$ . So, I am guessing $$I(\lambda):=\int_{E_{\lambda}}f( x_1,x_2,\cdots,x_n) d\tilde{\sigma}(x)\qquad (1)$$ where $d\tilde{\sigma}$ is the surface measure on the ellipsoid $E_\lambda$ . An edit : Thanks to the comments below I realize the question so put does not make sense. So, I modify: Can we express $I(\lambda)$ in terms of $J$ where $$J:=\int_{\mathbb{S}^{n-1}}f(x_1,x_2,\cdots,x_n) d\widetilde{\sigma}(x)$$ where $d\widetilde{\sigma}$ is some kind of a weighted surface measure on the sphere, obviously realted to the natural measure $d{\sigma}$ ?","Let be the unit sphere in and let be smooth on . Is it  possible to express the integral in terms of the integral This is a very simple special case of the my old question here Changing variables in integration over spheres Some thought: If the point lives on the sphere then lives on the ellipsoid . So, I am guessing where is the surface measure on the ellipsoid . An edit : Thanks to the comments below I realize the question so put does not make sense. So, I modify: Can we express in terms of where where is some kind of a weighted surface measure on the sphere, obviously realted to the natural measure ?","\mathbb{S}^{n-1} \mathbb{R}^{n} f \mathbb{R}^{n} I(\lambda):=\int_{\mathbb{S}^{n-1}}f(\lambda x_1,x_2,\cdots,x_n) d\sigma(x),\qquad \lambda >0 J:=\int_{\mathbb{S}^{n-1}}f(x_1,x_2,\cdots,x_n) d\sigma(x). (x_1,x_2,\cdots,x_n) x_1^2+\cdots+x_n^2=1 (\lambda x_1,x_2,\cdots,x_n) E_{\lambda}:=\{(x_1,x_2,\cdots,x_n)\in \mathbb{R}^n: x_1^2/\lambda^2+\cdots+x_n^2=1\} I(\lambda):=\int_{E_{\lambda}}f( x_1,x_2,\cdots,x_n) d\tilde{\sigma}(x)\qquad (1) d\tilde{\sigma} E_\lambda I(\lambda) J J:=\int_{\mathbb{S}^{n-1}}f(x_1,x_2,\cdots,x_n) d\widetilde{\sigma}(x) d\widetilde{\sigma} d{\sigma}","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'spherical-harmonics']"
33,Homework Question -Vector Calculus Area,Homework Question -Vector Calculus Area,,"I want to calculate the area of a semi-circle. I can use this $\iint x^2+y^2\,\mathrm dx\,\mathrm dy$ or I can use this $\iint r^2 r^2 \sin(\phi) \,\mathrm \,d\theta\,\mathrm dr$ . I can see where the $r\,\mathrm d\theta \, dr$ comes from but why is it wrong to say $x=r\cos\theta$ and $y=r\sin \theta$ , then differentiate and multiply and get $$(dr)^2\cos\theta \sin\theta +r\,dr\,d\theta \cos^2 \theta âˆ’r\,dr\,d \theta \sin^2\theta âˆ’r^2(d\theta)^2\sin\theta \cos\theta?$$ Obviously I can't use the above in the integral but I cant see where this is wrong.","I want to calculate the area of a semi-circle. I can use this or I can use this . I can see where the comes from but why is it wrong to say and , then differentiate and multiply and get Obviously I can't use the above in the integral but I cant see where this is wrong.","\iint x^2+y^2\,\mathrm dx\,\mathrm dy \iint r^2 r^2 \sin(\phi) \,\mathrm \,d\theta\,\mathrm dr r\,\mathrm d\theta \, dr x=r\cos\theta y=r\sin \theta (dr)^2\cos\theta \sin\theta +r\,dr\,d\theta \cos^2 \theta âˆ’r\,dr\,d \theta \sin^2\theta âˆ’r^2(d\theta)^2\sin\theta \cos\theta?","['multivariable-calculus', 'vector-analysis', 'area', 'multiple-integral']"
34,"Weak solution of $u_t+(1-2u)u_x=0$ with $u(x,0)=\frac{1}{4}$ for $x<0$ and $u(x,0)=1$ for $x\geq 0$?",Weak solution of  with  for  and  for ?,"u_t+(1-2u)u_x=0 u(x,0)=\frac{1}{4} x<0 u(x,0)=1 x\geq 0","We want to find the weak solution  of $\frac{\partial{u}}{\partial{t}}+(1-2u)\frac{\partial{u}}{\partial{x}}=0$ with $u(x,0)=\begin{cases} \mbox{$\frac{1}{4}$} & \mbox{if } x<0 \\ \mbox{1} & \mbox{if $x\geq 0$} \end{cases} $ We have that $ x\in [-L,L]$ and $t\geq 0$ . By using the method of characteristics, I get: $$u(x,t)=\begin{cases} \mbox{$\frac{1}{4}$} & \mbox{if } x-\frac{1}{2}t<0 \\ \mbox{1} & \mbox{if $x+t\geq 0$} \end{cases} $$ I know that it can be shown that $\int_{-L}^Lu(x,t)dx=\frac{L}{4}+L=\frac{5}{4}L$ for all $ t>0$ . I'm not sure how I would proceed from here? If we let $x_D(t)$ be the position of the discontinuity, we require that: $\frac{5}{4}L=\int_{-L}^{x_D}u_{-}(x,t)dx+\int_{x_D}^{L}u_{+}(x,t)dx$ , however I'm unsure how to determine $u_{-}$ and $u_{+}$ ? I think this would help to determine the weak solution.","We want to find the weak solution  of with We have that and . By using the method of characteristics, I get: I know that it can be shown that for all . I'm not sure how I would proceed from here? If we let be the position of the discontinuity, we require that: , however I'm unsure how to determine and ? I think this would help to determine the weak solution.","\frac{\partial{u}}{\partial{t}}+(1-2u)\frac{\partial{u}}{\partial{x}}=0 u(x,0)=\begin{cases} \mbox{\frac{1}{4}} & \mbox{if } x<0 \\ \mbox{1} & \mbox{if x\geq 0} \end{cases}   x\in [-L,L] t\geq 0 u(x,t)=\begin{cases} \mbox{\frac{1}{4}} & \mbox{if } x-\frac{1}{2}t<0 \\ \mbox{1} & \mbox{if x+t\geq 0} \end{cases}  \int_{-L}^Lu(x,t)dx=\frac{L}{4}+L=\frac{5}{4}L  t>0 x_D(t) \frac{5}{4}L=\int_{-L}^{x_D}u_{-}(x,t)dx+\int_{x_D}^{L}u_{+}(x,t)dx u_{-} u_{+}","['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'continuity']"
35,Is the way I computed this triple integral correct?,Is the way I computed this triple integral correct?,,"I had to compute the following triple integral: Let $$D = \{(x, y, z) \in \mathbb{R}^3 \ | \ 1 \leq x^2 + y^2 + z^2 \leq 4 \ | \ x, y, z \geq 0\}$$ Compute $$\int_D x \ dx dy dz$$ I chose to integrate over x as my outer integral, then y and then z as my inner integral. I deduced the bounds for each variable as such: $$0 \leq x \leq 2$$ $$\sqrt{1-x^2} \leq y \leq \sqrt{4-x^2}$$ $$\sqrt{1-x^2-y^2} \leq z \leq \sqrt{4-x^2-y^2}$$ Then I split up the integral: $$\int_0^2 \int_\sqrt{1-x^2}^\sqrt{4-x^2} \int_\sqrt{1-x^2-y^2}^\sqrt{4-x^2-y^2} x \ dz \ dy \ dx$$ Is my approach correct? Can I now just normally calculate the integrals from the inside to the outside? Edit: So I tried to do it with spherical coordinates. However, I get $0$ as a result which cannot be. In the end I got to this triple integral: $$\int_0^{2\pi} \int_0^\pi \int_1^2 r \ \sin{\phi} \ \cos{\theta} \ r^2 \sin{\phi} \ dr \ d\phi \ d\theta$$ Is this correct?","I had to compute the following triple integral: Let Compute I chose to integrate over x as my outer integral, then y and then z as my inner integral. I deduced the bounds for each variable as such: Then I split up the integral: Is my approach correct? Can I now just normally calculate the integrals from the inside to the outside? Edit: So I tried to do it with spherical coordinates. However, I get as a result which cannot be. In the end I got to this triple integral: Is this correct?","D = \{(x, y, z) \in \mathbb{R}^3 \ | \ 1 \leq x^2 + y^2 + z^2 \leq 4 \ | \ x, y, z \geq 0\} \int_D x \ dx dy dz 0 \leq x \leq 2 \sqrt{1-x^2} \leq y \leq \sqrt{4-x^2} \sqrt{1-x^2-y^2} \leq z \leq \sqrt{4-x^2-y^2} \int_0^2 \int_\sqrt{1-x^2}^\sqrt{4-x^2} \int_\sqrt{1-x^2-y^2}^\sqrt{4-x^2-y^2} x \ dz \ dy \ dx 0 \int_0^{2\pi} \int_0^\pi \int_1^2 r \ \sin{\phi} \ \cos{\theta} \ r^2 \sin{\phi} \ dr \ d\phi \ d\theta","['real-analysis', 'integration', 'multivariable-calculus']"
36,"Solution Verification: Find the Global Max and Min of $z=x+y$, in $D=\{x\ge 0, y\ge 0, y\le 3x, x^2+y^2\le 25 \}$","Solution Verification: Find the Global Max and Min of , in","z=x+y D=\{x\ge 0, y\ge 0, y\le 3x, x^2+y^2\le 25 \}","$z=x+y$ $D=\{x\ge 0, y\ge 0, y\le 3x, x^2+y^2\le 25 \}$ My attempt: Plan: Find inner critical points. Find maxmin of border curves. Find intersecting points of border curves. Find values of all points and decide global maximum and minimum. Inner critical points: First of all, I checked the inner points of the domain by taking $z_x,z_y=0$ . And obviously there's no critical points there since $z_x=z_y=1$ . Borders or domain curves (not sure what it's called): Next step, is finding the maximum and minimum on the domain curves (Or the borders), $x^2+y^2=25 \Longrightarrow x = \sqrt{25-y^2}$ , Substituting that into my function: $z = \sqrt{25-y^2}+y \Longrightarrow z'=\frac{-2y}{2\sqrt{25-y^2}}+1=0$ (To find critical points). $-y+\sqrt{25-y^2}=0 \Longrightarrow y^2=25-y^2 \Longrightarrow y^2 =\frac{25}{2} \Longrightarrow y=\frac{5}{\sqrt{2}}$ (I took positive value only because $y \ge 0$ ). The point $(x,\frac{5}{\sqrt{2}})$ must be on the circle, so $x^2+\frac{25}{2}=25 \Longrightarrow x=\frac{5}{\sqrt{2}}$ ( $x \ge 0$ ). So I've found a point $(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})$ Next, $y=0 \Longrightarrow z=x \Longrightarrow z_x=1$ There's no critical points. (same for $x=0$ ). Next, $y=3x\Longrightarrow z=4x \Longrightarrow z_x = 4$ (no critical points too). Intersecting points of border curves: Now my next step is to check points where $x^2+y^2=25$ and $y=3x$ and $y=0$ and $x=0$ meet, I can see points $(0,0)$ , $(0,5)$ , now I need to find the intersection of the circle and $y=3x$ . $(x^2 + 9x^2)=25 \Longrightarrow 10x^2=25 \Longrightarrow x=\frac{5}{\sqrt{10}}$ Substituting in the $y=3x \Longrightarrow y=\frac{15}{\sqrt{10}}$ And so my last point is $(\frac{5}{\sqrt{10}}, \frac{15}{\sqrt{10}})$ Calculating the value of the function in the points I've found: My next and last step, is to calculate the value of $z=f(x,y)$ in all the points I have found: $f(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})= \frac{10}{\sqrt{2}} = 7.071...$ $f(0,0)=0$ $f(0,5)=5$ $f(\frac{5}{\sqrt{10}}, \frac{15}{\sqrt{10}})= \frac{20}{\sqrt{10}}=6.324...$ Final Answer: And so, my global maximum is $(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})$ , Global minimum is $(0,0)$ . I would love to hear feedback about my solution and to know if I missed some points, checked some unnecessary points or some of my work flow doesn't seem okay. Thanks in advance!","My attempt: Plan: Find inner critical points. Find maxmin of border curves. Find intersecting points of border curves. Find values of all points and decide global maximum and minimum. Inner critical points: First of all, I checked the inner points of the domain by taking . And obviously there's no critical points there since . Borders or domain curves (not sure what it's called): Next step, is finding the maximum and minimum on the domain curves (Or the borders), , Substituting that into my function: (To find critical points). (I took positive value only because ). The point must be on the circle, so ( ). So I've found a point Next, There's no critical points. (same for ). Next, (no critical points too). Intersecting points of border curves: Now my next step is to check points where and and and meet, I can see points , , now I need to find the intersection of the circle and . Substituting in the And so my last point is Calculating the value of the function in the points I've found: My next and last step, is to calculate the value of in all the points I have found: Final Answer: And so, my global maximum is , Global minimum is . I would love to hear feedback about my solution and to know if I missed some points, checked some unnecessary points or some of my work flow doesn't seem okay. Thanks in advance!","z=x+y D=\{x\ge 0, y\ge 0, y\le 3x, x^2+y^2\le 25 \} z_x,z_y=0 z_x=z_y=1 x^2+y^2=25 \Longrightarrow x = \sqrt{25-y^2} z = \sqrt{25-y^2}+y \Longrightarrow z'=\frac{-2y}{2\sqrt{25-y^2}}+1=0 -y+\sqrt{25-y^2}=0 \Longrightarrow y^2=25-y^2 \Longrightarrow y^2 =\frac{25}{2} \Longrightarrow y=\frac{5}{\sqrt{2}} y \ge 0 (x,\frac{5}{\sqrt{2}}) x^2+\frac{25}{2}=25 \Longrightarrow x=\frac{5}{\sqrt{2}} x \ge 0 (\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}}) y=0 \Longrightarrow z=x \Longrightarrow z_x=1 x=0 y=3x\Longrightarrow z=4x \Longrightarrow z_x = 4 x^2+y^2=25 y=3x y=0 x=0 (0,0) (0,5) y=3x (x^2 + 9x^2)=25 \Longrightarrow 10x^2=25 \Longrightarrow x=\frac{5}{\sqrt{10}} y=3x \Longrightarrow y=\frac{15}{\sqrt{10}} (\frac{5}{\sqrt{10}}, \frac{15}{\sqrt{10}}) z=f(x,y) f(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})= \frac{10}{\sqrt{2}} = 7.071... f(0,0)=0 f(0,5)=5 f(\frac{5}{\sqrt{10}}, \frac{15}{\sqrt{10}})= \frac{20}{\sqrt{10}}=6.324... (\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}}) (0,0)","['multivariable-calculus', 'solution-verification', 'maxima-minima']"
37,Evaluating a Double Integral Involving Fractional Part Functions,Evaluating a Double Integral Involving Fractional Part Functions,,"In my project, I am stuck with the following integral. Please help. $$  I = \int_{x = a}^b \int_{y = a}^b \left\{ \frac xy\right\} \left\{\frac >yx\right\}~dx~dy$$ $0 < a < b < 1, b < 2a$ , where $\lbrace \rbrace$ is the fractional part function. I found an approximation of the integral calculated on the square $(0,\alpha)^2, \alpha < 1$ on your forum. I tried using the known values of the integral on the squares $(0,a)^2, (0,b)^2$ in calculation of the integral in discussion. Let $f(x,y)=\left\{ \frac xy\right\} \left\{\frac yx\right\}$ , then $$\int_{x = a}^b \int_{y = a}^b  f(x,y)~dx~dy = \int_{x = 0}^b \int_{y = 0}^b  f(x,y)~dx~dy-  \int_{x = 0}^a \int_{y = 0}^a  f(x,y)~dx~dy - 2   \int_{x = 0}^b \int_{y = 0}^a  f(x,y)~dx~dy$$ I get stuck trying to follow the idea used in calculation of the integrals on the squares $(0,a)^2$ and $(0,b)^2$ in calculation of the last one. Even taking $b = 2a$ was not useful. I hope I'll have the chance to receive a response. Thank you in advance. Al","In my project, I am stuck with the following integral. Please help. , where is the fractional part function. I found an approximation of the integral calculated on the square on your forum. I tried using the known values of the integral on the squares in calculation of the integral in discussion. Let , then I get stuck trying to follow the idea used in calculation of the integrals on the squares and in calculation of the last one. Even taking was not useful. I hope I'll have the chance to receive a response. Thank you in advance. Al","  I = \int_{x = a}^b \int_{y = a}^b \left\{ \frac xy\right\} \left\{\frac >yx\right\}~dx~dy 0 < a < b < 1, b < 2a \lbrace \rbrace (0,\alpha)^2, \alpha < 1 (0,a)^2, (0,b)^2 f(x,y)=\left\{ \frac xy\right\} \left\{\frac yx\right\} \int_{x = a}^b \int_{y = a}^b  f(x,y)~dx~dy = \int_{x = 0}^b \int_{y = 0}^b  f(x,y)~dx~dy-  \int_{x = 0}^a \int_{y = 0}^a  f(x,y)~dx~dy - 2   \int_{x = 0}^b \int_{y = 0}^a  f(x,y)~dx~dy (0,a)^2 (0,b)^2 b = 2a","['multivariable-calculus', 'multiple-integral', 'fractional-part']"
38,"How can I ensure the existence of this right-handed limit of the form $\lim_{\phi\to0^+}\{\frac1{\phi}\frac{\partial{F(\phi,\psi)}}{\partial\phi}\}$?",How can I ensure the existence of this right-handed limit of the form ?,"\lim_{\phi\to0^+}\{\frac1{\phi}\frac{\partial{F(\phi,\psi)}}{\partial\phi}\}","I'm working on a class of  models defined by a certain function $F(\phi,\psi)$ (there is a lot of freedom in this choice). Here, $\phi$ and $\psi$ are in fact real functions, which I know are non-negative, have at least two derivatives, and are bounded. For simplicity, let's say both have a maximum at one, so that $\phi, \psi\in[0,1]$ . However, I can't constrain the behavior of $\phi$ and $\psi$ , so I'd rather think of F as as function $F:[0,1]\times[0,1]\to\mathbb{R}^{+}$ for the purposes of this question. I have assumed that $F$ is bounded and twice differentiable (although I think I must use semi-derivatives at the boundary to make this rigorous, since the interval is closed). At one point in my calculations, the fraction $\frac{1}{\phi}\frac{\partial{F(\phi,\psi)}}{\partial\phi}$ appears. The problem is I that know $\phi$ and $\psi$ have a zero, so I'd like to ensure, under as few assumptions as possible, that this won't diverge when $\phi$ approaches zero from the right. In other words want to know what assumptions I must make about $F$ in order to ensure that the right-handed limit $$\lim_{\phi\to 0+}\left\{\frac{1}{\phi}\frac{\partial{F(\phi,\psi)}}{\partial\phi}\right\} $$ exists for every $\psi$ (it doesn't have to be the same number for every $\psi$ , I just don't want it to blow up). Now, I'm not very familiar with multi-variable analysis, semi-derivatives etc, so I'm not sure on how to proceed. I thought about assuming the second derivatives $\frac{\partial{^2F(\phi,\psi)}}{\partial\phi^2}$ and $\frac{\partial{^2F(\phi,\psi)}}{\partial\phi\partial\chi}$ are bounded (which would make physical sense in this case) to get a Lipschitz condition on the and partial derivatives. Then I could impose $\lim_{\phi\to 0^+}\frac{\partial{F(\phi,\psi)}}{\partial\phi}=0$ or something similar to make the derivative bounded. But I don't know if there is a version of this argument that makes sense when $\phi=0$ , since the usual derivative is not defined there. So, could someone help me find some simple (as in not too restrictive) conditions that would make this limit well defined?","I'm working on a class of  models defined by a certain function (there is a lot of freedom in this choice). Here, and are in fact real functions, which I know are non-negative, have at least two derivatives, and are bounded. For simplicity, let's say both have a maximum at one, so that . However, I can't constrain the behavior of and , so I'd rather think of F as as function for the purposes of this question. I have assumed that is bounded and twice differentiable (although I think I must use semi-derivatives at the boundary to make this rigorous, since the interval is closed). At one point in my calculations, the fraction appears. The problem is I that know and have a zero, so I'd like to ensure, under as few assumptions as possible, that this won't diverge when approaches zero from the right. In other words want to know what assumptions I must make about in order to ensure that the right-handed limit exists for every (it doesn't have to be the same number for every , I just don't want it to blow up). Now, I'm not very familiar with multi-variable analysis, semi-derivatives etc, so I'm not sure on how to proceed. I thought about assuming the second derivatives and are bounded (which would make physical sense in this case) to get a Lipschitz condition on the and partial derivatives. Then I could impose or something similar to make the derivative bounded. But I don't know if there is a version of this argument that makes sense when , since the usual derivative is not defined there. So, could someone help me find some simple (as in not too restrictive) conditions that would make this limit well defined?","F(\phi,\psi) \phi \psi \phi, \psi\in[0,1] \phi \psi F:[0,1]\times[0,1]\to\mathbb{R}^{+} F \frac{1}{\phi}\frac{\partial{F(\phi,\psi)}}{\partial\phi} \phi \psi \phi F \lim_{\phi\to 0+}\left\{\frac{1}{\phi}\frac{\partial{F(\phi,\psi)}}{\partial\phi}\right\}  \psi \psi \frac{\partial{^2F(\phi,\psi)}}{\partial\phi^2} \frac{\partial{^2F(\phi,\psi)}}{\partial\phi\partial\chi} \lim_{\phi\to 0^+}\frac{\partial{F(\phi,\psi)}}{\partial\phi}=0 \phi=0","['limits', 'multivariable-calculus', 'partial-derivative', 'lipschitz-functions']"
39,Use Stokes Theorem to Prove Two Integrals on Differentiable Manifiolds are equivalent,Use Stokes Theorem to Prove Two Integrals on Differentiable Manifiolds are equivalent,,"I have to answer the following problem: Let $\omega= ydx + xzdy + xdz$ . Let $S_1$ be the portion of the upper hemisphere given by $\phi_1(u,v)=(u,v,\sqrt{4-u^2-v^2})$ ; where $u^2 + v^2 \leq 2$ . Let $S_2$ be the disc in the plane $z=\sqrt{2}$ , given by $\phi_2(u,v)=(u,v,\sqrt{2})$ ; where $u^2 + v^2 \leq 2$ . Use Stokes theorem to show $\int\int_{s_1}d\omega=\int\int_{s_2}d\omega$ I know that I could technically calculate each integral to prove this, but since the directions are asking for the use of Stokes theorem, I feel like there's some trick that I'm missing. The two parameterizations look like they have equivalent boundaries, but I'm getting stuck on exactly how to show that. Any help would be greatly appreciated","I have to answer the following problem: Let . Let be the portion of the upper hemisphere given by ; where . Let be the disc in the plane , given by ; where . Use Stokes theorem to show I know that I could technically calculate each integral to prove this, but since the directions are asking for the use of Stokes theorem, I feel like there's some trick that I'm missing. The two parameterizations look like they have equivalent boundaries, but I'm getting stuck on exactly how to show that. Any help would be greatly appreciated","\omega= ydx + xzdy + xdz S_1 \phi_1(u,v)=(u,v,\sqrt{4-u^2-v^2}) u^2 + v^2 \leq 2 S_2 z=\sqrt{2} \phi_2(u,v)=(u,v,\sqrt{2}) u^2 + v^2 \leq 2 \int\int_{s_1}d\omega=\int\int_{s_2}d\omega","['multivariable-calculus', 'differential-geometry', 'manifolds', 'stokes-theorem', 'manifolds-with-boundary']"
40,"Lipschitz-continuity of the vector valued function $f(t,y_1,y_2) = (y_2, At - B\sin y_1)$",Lipschitz-continuity of the vector valued function,"f(t,y_1,y_2) = (y_2, At - B\sin y_1)","In order to show that the function $f(t,y_1,y_2) = (y_2, At - B\sin y_1)$ , is Lipschitz-continuous with respect to the variable $\underline{y}=(y_1,y_2)$ , it is sufficient to show there exists a number $K\geq 0$ , such that $\vert \partial_{j} f(t,y_1,y_2)\vert\leq K$ for all $j = (y_1,y_2)$ . So this one $K$ must bound each of the partials of $f$ . Because $$\vert \partial_{1} f(t,y_1,y_2)\vert=\vert (0,-B\cos y_1)\vert\leq \vert B\vert$$ and $$\vert \partial_{2} f(t,y_1,y_2)\vert=\vert (1,0)\vert\leq1$$ then choosing $K:=\vert B\vert+1$ implies that the function is lipschitz-continuous with respect to $\underline{y}=(y_1,y_2)$ . This $K$ is not necessarily the minimal choice, but it has no bearing on the conclusion, correct?","In order to show that the function , is Lipschitz-continuous with respect to the variable , it is sufficient to show there exists a number , such that for all . So this one must bound each of the partials of . Because and then choosing implies that the function is lipschitz-continuous with respect to . This is not necessarily the minimal choice, but it has no bearing on the conclusion, correct?","f(t,y_1,y_2) = (y_2, At - B\sin y_1) \underline{y}=(y_1,y_2) K\geq 0 \vert \partial_{j} f(t,y_1,y_2)\vert\leq K j = (y_1,y_2) K f \vert \partial_{1} f(t,y_1,y_2)\vert=\vert (0,-B\cos y_1)\vert\leq \vert B\vert \vert \partial_{2} f(t,y_1,y_2)\vert=\vert (1,0)\vert\leq1 K:=\vert B\vert+1 \underline{y}=(y_1,y_2) K","['multivariable-calculus', 'lipschitz-functions']"
41,Different approaches of find $P(X+Y>1/2)$ yielding different results,Different approaches of find  yielding different results,P(X+Y>1/2),"I have the following question: Let X denote the diameter of an armored electric cable and Y denote the diameter of the ceramic mold that makes the cable. Both X and Y are scaled so that they range between 0 and 1. Suppose that X and Y have the joint density $f(x,y)= (1/y), 0<x<y<1$ $=0,$ elsewhere. Find $P(X+Y >1/2)$ . Approach by the solution manual: $P(X+Y >1/2)=1-P(X+Y<1/2)$ then they continued accordingly and got $0.6534$ I have no issues with this approach, my problem is that I am following the more ""direct"" approach of directly computing $P(X+Y>1/2)$ and getting $0.5$ instead. My solution: First, I have the following graph: For a more interactive view, you can check the Desmos link here The orange area is my region of integration, I decided to integrate from the $y$ direction first while dividing my region into 2 parts to have 2 parts with one single entry and exit each, I divided it using a vertical line passing through the tip of the triangular shape at the bottom, this yields two double integrals: $\int_0^{1/4}\int_{1/2-x}^1 (1/y)dydx$ and $\int_{1/4}^{1}\int_{x}^1 (1/y)dydx$ Each of these integrals is equal to $1/4$ (done on WolframAlpha, so there should be no error in this part) which makes them sum to $1/2$ Before posting this I reviewed the given, the graph, and my integral set-up multiple times and couldn't find a mistake, any help would be appreciated.","I have the following question: Let X denote the diameter of an armored electric cable and Y denote the diameter of the ceramic mold that makes the cable. Both X and Y are scaled so that they range between 0 and 1. Suppose that X and Y have the joint density elsewhere. Find . Approach by the solution manual: then they continued accordingly and got I have no issues with this approach, my problem is that I am following the more ""direct"" approach of directly computing and getting instead. My solution: First, I have the following graph: For a more interactive view, you can check the Desmos link here The orange area is my region of integration, I decided to integrate from the direction first while dividing my region into 2 parts to have 2 parts with one single entry and exit each, I divided it using a vertical line passing through the tip of the triangular shape at the bottom, this yields two double integrals: and Each of these integrals is equal to (done on WolframAlpha, so there should be no error in this part) which makes them sum to Before posting this I reviewed the given, the graph, and my integral set-up multiple times and couldn't find a mistake, any help would be appreciated.","f(x,y)= (1/y), 0<x<y<1 =0, P(X+Y >1/2) P(X+Y >1/2)=1-P(X+Y<1/2) 0.6534 P(X+Y>1/2) 0.5 y \int_0^{1/4}\int_{1/2-x}^1 (1/y)dydx \int_{1/4}^{1}\int_{x}^1 (1/y)dydx 1/4 1/2","['probability', 'integration', 'multivariable-calculus']"
42,"Maximum of $\frac{\sin{(\sin{t}})}{{\cos{(\cos{t})}}}$ on $t\in [0,2\pi]$",Maximum of  on,"\frac{\sin{(\sin{t}})}{{\cos{(\cos{t})}}} t\in [0,2\pi]","I would like to analytically derive the maximum of $\frac{\sin{(\sin{t}})}{{\cos{(\cos{t})}}}$ on $t\in [0,2\pi]$ . I feel like we wouldn't have a closed form, but still I gave it a try. My attempt: I thought this problem can be seen as optimizing the $2$ -variable function $f(x,y) = \sin{x}/\cos{y}$ on $g(x,y)=0$ where $g(x,y)=x^2+y^2-1$ . I used the Lagrange Multiplier method to solve this problem. Since $f_x = \cos{x}/\cos{y}, f_y=\sin{x}\sin{y}/\cos^2{y}$ , we need to solve the following: $\begin{cases} \frac{\cos{x}}{\cos{y}}-\lambda \cdot 2x=0, \\ \frac{\sin{x}\sin{y}}{\cos{y}}-\lambda \cdot 2y=0, \\ x^2+y^2-1=0.\end{cases}$ By first two equations, we have $2xy\lambda = \frac{y\cos{x}}{\cos{y}}=\frac{x\sin{x}\sin{y}}{\cos^2{y}}$ and therefore we have $y\cos{x}\cos{y}=x\sin{x}\sin{y}$ . Now we need to solve $\begin{cases} y\cos{x}\cos{y}=x\sin{x}\sin{y}, \\ x^2+y^2=1.\end{cases}$ $x=\pm1,y=0$ are the trivial solution of this equation. According to Wolfram|Alpha , this seems to have few more solutions, but I couldn't find a way to find them. Any ideas?","I would like to analytically derive the maximum of on . I feel like we wouldn't have a closed form, but still I gave it a try. My attempt: I thought this problem can be seen as optimizing the -variable function on where . I used the Lagrange Multiplier method to solve this problem. Since , we need to solve the following: By first two equations, we have and therefore we have . Now we need to solve are the trivial solution of this equation. According to Wolfram|Alpha , this seems to have few more solutions, but I couldn't find a way to find them. Any ideas?","\frac{\sin{(\sin{t}})}{{\cos{(\cos{t})}}} t\in [0,2\pi] 2 f(x,y) = \sin{x}/\cos{y} g(x,y)=0 g(x,y)=x^2+y^2-1 f_x = \cos{x}/\cos{y}, f_y=\sin{x}\sin{y}/\cos^2{y} \begin{cases} \frac{\cos{x}}{\cos{y}}-\lambda \cdot 2x=0, \\ \frac{\sin{x}\sin{y}}{\cos{y}}-\lambda \cdot 2y=0, \\ x^2+y^2-1=0.\end{cases} 2xy\lambda = \frac{y\cos{x}}{\cos{y}}=\frac{x\sin{x}\sin{y}}{\cos^2{y}} y\cos{x}\cos{y}=x\sin{x}\sin{y} \begin{cases} y\cos{x}\cos{y}=x\sin{x}\sin{y}, \\ x^2+y^2=1.\end{cases} x=\pm1,y=0","['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
43,"Finding the global minimum of $f(\mathbf{x})=\|(1-x_1,x_1-x_2,x_2-x_3,\ldots,x_{n-1}-x_n,x_n-2)\|_2^2$",Finding the global minimum of,"f(\mathbf{x})=\|(1-x_1,x_1-x_2,x_2-x_3,\ldots,x_{n-1}-x_n,x_n-2)\|_2^2","I am self-learning optimization algorithms. A certain assignment problem is as follows: Show that the $n$ -dimensional function $f(\mathbf{x})=\lvert \lvert (1-x_1,x_1-x_2,x_2-x_3,\ldots,x_{n-1}-x_n,x_n-2\rvert\rvert_2^2$ has exactly one stationary point which is a global minimum. Compute this minimum. I would like someone to verify my optimal solution; does the math checkout? Solution. We have, \begin{align*} f(\mathbf{x})= (1-x_1)^2+(x_1-x_2)^2+\ldots+(x_{n-1}-x_{n})^2 + (x_n-2)^2 \end{align*} The partial derivatives of $f$ are as follows: \begin{align*} f_{x_1}(\mathbf{x}) &= 2(1-x_1)(-1)+2(x_1-x_2) = -2 + 4x_1 - 2x_2\\ f_{x_2}(\mathbf{x}) &= 2(x_1-x_2)(-1)+2(x_2-x_3) = -2x_1 + 4x_2 -2x_3 \\ f_{x_3}(\mathbf{x}) &= 2(x_2-x_3)(-1)+2(x_3-x_4) = -2x_2 + 4x_3 -2x_4 \\ \vdots\\ f_{x_{n-1}}(\mathbf{x}) &= 2(x_{n-2}-x_{n-1})(-1)+2(x_{n-1}-x_n) = -2x_{n-2} + 4x_{n-1} -2x_n \\ f_{x_n}(\mathbf{x}) &= 2(x_{n-1}-x_n)(-1)+2(x_n-2) = -2x_{n-1} + 4x_n -4 \end{align*} The critical points of $f$ are given by, $$\nabla f(\mathbf{x}) = 0$$ Therefore, we have the system of equations: \begin{align*} \begin{bmatrix} 4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0\\  -2& 4 &-2 & 0 & 0 & \ldots & 0 & 0 & 0\\ 0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0\\ 0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0\\ 0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0\\ 0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0\\ \vdots\\ 0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 \end{bmatrix}\begin{bmatrix} x_1\\ x_2\\ x_3\\ x_4\\ x_5\\ x_6\\ \vdots\\ x_{n-1}\\ x_n \end{bmatrix}=\begin{bmatrix} 2\\ 0\\ 0\\ 0\\ 0\\ 0\\ \vdots\\ 0\\ 4 \end{bmatrix} \end{align*} These are $n$ equations in $n$ unknowns. This tridiagonal system has a unique solution vector $\mathbf{x}$ . Applying Gaussian elimination to the augmented matrix $[A \quad b]$ , by hand, we have: \begin{align*} \begin{bmatrix} 4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\  -2& 4 &-2 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 0\\ 0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ 0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ 0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ 0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ \vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm|  & 0\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm|  & 4\\ \end{bmatrix} \end{align*} Applying \begin{align*} \{R_2 \rightarrow 2R_2 + R_1\} \end{align*} \begin{align*} \begin{bmatrix} 4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\  0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ 0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ 0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ 0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\ \vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm|  & 0\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm|  & 4\\ \end{bmatrix} \end{align*} Applying \begin{align*} \{R_3 \rightarrow 3R_3 + R_2\} \end{align*} \begin{align*} \begin{bmatrix} 4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\  0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\ 0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm| & 0\\ 0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\ \vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm| & 0\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm| & 4\\ \end{bmatrix} \end{align*} Applying \begin{align*} \{R_4 \rightarrow 4R_4 + R_3\} \end{align*} \begin{align*} \begin{bmatrix} 4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\  0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 0 &10 &-8 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm| & 0\\ 0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\ \vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm| & 0\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm| & 4\\ \end{bmatrix} \end{align*} Continuing in this fashion, we obtain after $\{R_n\rightarrow n R_n + R_{n-1}\}$ , \begin{align*} \begin{bmatrix} 4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\  0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 0 &10 &-8 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 0 &0 & 12 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 0 & 0 & 0 & 0 &0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ \vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 & 2n &-2(n-1) &\bigm| & 2\\ 0 & 0 & 0 & 0 & 0 & \ldots & 0 &0 & 2n+2 &\bigm| & 4n+2 \end{bmatrix} \end{align*} By back-substitution, \begin{align} (2n+2)x_n &= 4n+2\\ x_n &= \frac{2n+1}{n+1} \tag{1} \end{align} Substituting the value of $x_n$ in $2n x_{n-1} -2(n-1)x_n = 2$ , we have: \begin{align} 2n x_{n-1} &= 2 + 2(n-1)x_n\\ x_{n-1} &= \frac{1}{n}\left[1 + (n-1)x_n\right]\\ &= \frac{1}{n}\left[1 + (n-1)\frac{2n+1}{n+1}\right]\\ &= \frac{1}{n}\left[\frac{(n+1)+(n-1)(2n+1)}{n+1}\right]\\ &= \frac{1}{n}\left[\frac{(n+1)+n(2n+1)-(2n+1)}{n+1}\right]\\ &= \frac{1}{n}\left[\frac{n + 1 + 2n^2 + n - 2n - 1}{n+1}\right]\\ &= \frac{1}{n}\left[\frac{2n^2}{n+1}\right]\\ &= \frac{2n}{n+1} \tag{2} \end{align} As the recurrence relationship stays the same for $n \in \{1,2,3,4,\ldots,n-1\}$ , we have: \begin{align*} x_{n-1} &= \frac{2n}{n+1}\\ x_{n-2} &= \frac{2n-1}{n+1}\\ x_{n-3} &= \frac{2n-2}{n+1}\\ \vdots\\ x_3 &= \frac{n+4}{n+1}\\ x_2 &= \frac{n+3}{n+1}\\ x_1 &= \frac{n+2}{n+1} \end{align*} Now, the Hessian of $f$ , $Hf(\mathbf{x})$ is the same tridiagonal matrix $A$ as above. If we find the sequence of determinants $det(B_k)$ , where $B_k$ is a $k \times k$ sub-matrix of $A$ in the upper-left corner, we see that $det(B_1) = 4, det(B_2) = 8, det(B_3) = 16, \ldots, det(B_n)=2^{n+1}$ . Consequently, the critical point $\mathbf{x}$ is a global minima. The minimum value of $f$ is given by, \begin{align} \min \{f(\mathbf{x}):\mathbf{x}\in \mathbf{R}^n \} &=\left(1-\frac{n+2}{n+1}\right)^2 + \left(\frac{1}{n+1}\right)^2 + \ldots +  \left(\frac{2n+1}{n+1}-2\right)^2\\ &= \frac{(n+1)}{(n+1)^2}\\ &= \frac{1}{n+1} \end{align}","I am self-learning optimization algorithms. A certain assignment problem is as follows: Show that the -dimensional function has exactly one stationary point which is a global minimum. Compute this minimum. I would like someone to verify my optimal solution; does the math checkout? Solution. We have, The partial derivatives of are as follows: The critical points of are given by, Therefore, we have the system of equations: These are equations in unknowns. This tridiagonal system has a unique solution vector . Applying Gaussian elimination to the augmented matrix , by hand, we have: Applying Applying Applying Continuing in this fashion, we obtain after , By back-substitution, Substituting the value of in , we have: As the recurrence relationship stays the same for , we have: Now, the Hessian of , is the same tridiagonal matrix as above. If we find the sequence of determinants , where is a sub-matrix of in the upper-left corner, we see that . Consequently, the critical point is a global minima. The minimum value of is given by,","n f(\mathbf{x})=\lvert \lvert (1-x_1,x_1-x_2,x_2-x_3,\ldots,x_{n-1}-x_n,x_n-2\rvert\rvert_2^2 \begin{align*}
f(\mathbf{x})= (1-x_1)^2+(x_1-x_2)^2+\ldots+(x_{n-1}-x_{n})^2 + (x_n-2)^2
\end{align*} f \begin{align*}
f_{x_1}(\mathbf{x}) &= 2(1-x_1)(-1)+2(x_1-x_2) = -2 + 4x_1 - 2x_2\\
f_{x_2}(\mathbf{x}) &= 2(x_1-x_2)(-1)+2(x_2-x_3) = -2x_1 + 4x_2 -2x_3 \\
f_{x_3}(\mathbf{x}) &= 2(x_2-x_3)(-1)+2(x_3-x_4) = -2x_2 + 4x_3 -2x_4 \\
\vdots\\
f_{x_{n-1}}(\mathbf{x}) &= 2(x_{n-2}-x_{n-1})(-1)+2(x_{n-1}-x_n) = -2x_{n-2} + 4x_{n-1} -2x_n \\
f_{x_n}(\mathbf{x}) &= 2(x_{n-1}-x_n)(-1)+2(x_n-2) = -2x_{n-1} + 4x_n -4
\end{align*} f \nabla f(\mathbf{x}) = 0 \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0\\ 
-2& 4 &-2 & 0 & 0 & \ldots & 0 & 0 & 0\\
0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0\\
\vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_5\\
x_6\\
\vdots\\
x_{n-1}\\
x_n
\end{bmatrix}=\begin{bmatrix}
2\\
0\\
0\\
0\\
0\\
0\\
\vdots\\
0\\
4
\end{bmatrix}
\end{align*} n n \mathbf{x} [A \quad b] \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
-2& 4 &-2 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm|  & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm|  & 4\\
\end{bmatrix}
\end{align*} \begin{align*}
\{R_2 \rightarrow 2R_2 + R_1\}
\end{align*} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm|  & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm|  & 4\\
\end{bmatrix}
\end{align*} \begin{align*}
\{R_3 \rightarrow 3R_3 + R_2\}
\end{align*} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm| & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm| & 4\\
\end{bmatrix}
\end{align*} \begin{align*}
\{R_4 \rightarrow 4R_4 + R_3\}
\end{align*} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &10 &-8 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm| & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm| & 4\\
\end{bmatrix}
\end{align*} \{R_n\rightarrow n R_n + R_{n-1}\} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &10 &-8 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &0 & 12 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 & 0 &0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 & 2n &-2(n-1) &\bigm| & 2\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &0 & 2n+2 &\bigm| & 4n+2
\end{bmatrix}
\end{align*} \begin{align}
(2n+2)x_n &= 4n+2\\
x_n &= \frac{2n+1}{n+1} \tag{1}
\end{align} x_n 2n x_{n-1} -2(n-1)x_n = 2 \begin{align}
2n x_{n-1} &= 2 + 2(n-1)x_n\\
x_{n-1} &= \frac{1}{n}\left[1 + (n-1)x_n\right]\\
&= \frac{1}{n}\left[1 + (n-1)\frac{2n+1}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{(n+1)+(n-1)(2n+1)}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{(n+1)+n(2n+1)-(2n+1)}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{n + 1 + 2n^2 + n - 2n - 1}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{2n^2}{n+1}\right]\\
&= \frac{2n}{n+1} \tag{2}
\end{align} n \in \{1,2,3,4,\ldots,n-1\} \begin{align*}
x_{n-1} &= \frac{2n}{n+1}\\
x_{n-2} &= \frac{2n-1}{n+1}\\
x_{n-3} &= \frac{2n-2}{n+1}\\
\vdots\\
x_3 &= \frac{n+4}{n+1}\\
x_2 &= \frac{n+3}{n+1}\\
x_1 &= \frac{n+2}{n+1}
\end{align*} f Hf(\mathbf{x}) A det(B_k) B_k k \times k A det(B_1) = 4, det(B_2) = 8, det(B_3) = 16, \ldots, det(B_n)=2^{n+1} \mathbf{x} f \begin{align}
\min \{f(\mathbf{x}):\mathbf{x}\in \mathbf{R}^n \} &=\left(1-\frac{n+2}{n+1}\right)^2 + \left(\frac{1}{n+1}\right)^2 + \ldots +  \left(\frac{2n+1}{n+1}-2\right)^2\\
&= \frac{(n+1)}{(n+1)^2}\\
&= \frac{1}{n+1}
\end{align}","['multivariable-calculus', 'optimization', 'solution-verification', 'maxima-minima']"
44,Global minimum of convex function,Global minimum of convex function,,"Let $E \subset \mathbb R^n$ a closed set, convex and not bounded. Let $f \in C^0 (E, \mathbb R)$ strictly convex, it means that $$\forall (x, y) \in E^2, \forall \alpha \in ]0, 1[, x \neq y \implies f(\alpha x + (1 - \alpha)y) < \alpha f(x) + (1 âˆ’ \alpha)f(y)$$ such that $$\lim_{\lVert ð‘¥ \rVert \to +\infty} f(x) = +\infty ; i.e. âˆ€ð‘ âˆˆ ]0, +âˆž[, âˆƒð‘€ âˆˆ ]0, +âˆž[, \forall x \in ð¸, \lVert x \rVert \geq M \implies f(x) \geq c$$ Show that $f$ has a global minimum and at a unique point. Since the function is defined on $\mathbb R^n$ I cannot use one dimensional mean value theorem to show that there exists a point $a$ on which $\triangledown f(a)=0$ and I also must show that the hessian matrix is defined positive to show that it is a global minimum but I am running out of ideas to do so.","Let a closed set, convex and not bounded. Let strictly convex, it means that such that Show that has a global minimum and at a unique point. Since the function is defined on I cannot use one dimensional mean value theorem to show that there exists a point on which and I also must show that the hessian matrix is defined positive to show that it is a global minimum but I am running out of ideas to do so.","E \subset \mathbb R^n f \in C^0
(E, \mathbb R) \forall (x, y) \in E^2, \forall \alpha \in ]0, 1[, x \neq y \implies f(\alpha x + (1 - \alpha)y) < \alpha f(x) + (1 âˆ’ \alpha)f(y) \lim_{\lVert ð‘¥ \rVert \to +\infty} f(x) = +\infty ; i.e. âˆ€ð‘ âˆˆ ]0, +âˆž[, âˆƒð‘€ âˆˆ ]0, +âˆž[, \forall x \in ð¸, \lVert x \rVert \geq M \implies f(x) \geq c f \mathbb R^n a \triangledown f(a)=0","['real-analysis', 'multivariable-calculus', 'convex-analysis', 'maxima-minima', 'global-optimization']"
45,"Prove $\int\int_{[0,\pi]\times [0,\pi]} |\cos(x+y)| d(x,y) = 2\pi$",Prove,"\int\int_{[0,\pi]\times [0,\pi]} |\cos(x+y)| d(x,y) = 2\pi","I have come to an exercise in a multivariate calculus book that I am having trouble with. The problem is : Show that $\int\int_{[0,\pi]\times[0,\pi]} |\cos(x+y)| d(x,y) = 2 \pi$ I have attempted solving the problem but I'm not getting the right answer. I know there are probably other ways to solve this problem, but here I am seeking help from others to see where I am going wrong in my solution. My solution is below : Let : \begin{equation} R = [0,\pi] \times [0,\pi] \end{equation} We see : \begin{equation} \{ x + y \; : \; (x,y) \in R \} = [0,2\pi] = T \end{equation} We see : \begin{equation} | \cos(x+y) | =  	\begin{cases} 	\cos(x+y)  \; & \forall \; x + y \in \left[ 0 , \frac{\pi}{2} \right] \bigcup \left[ \frac{3\pi}{2} , 2 \pi \right] \\ 	-\cos(x+y) \; & \forall \; x + y \in \left[ \frac{\pi}{2} , \frac{3\pi}{2} \right] 	\end{cases} \end{equation} Define : \begin{align} D_{1} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ 0 , \frac{\pi}{2} \right] \right\} 	\\ D_{2} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ \frac{3\pi}{2} , 2\pi \right] \right\} \\ D_{3} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ \frac{\pi}{2} , \frac{3\pi}{2} \right] \right\}  \end{align} We see : \begin{equation} T = D_{1} \bigcup D_{2} \bigcup D_{3} \end{equation} and : \begin{align} D_{1} \bigcap D_{2}	& = \emptyset \\ D_{1} \bigcap D_{3}	& = \left\{ (x,y) \in R \; : \; x + y = \frac{\pi}{2} \right\}\\ D_{2} \bigcap D_{3}	& = \left\{ (x,y) \in R \; : \; x + y = \frac{3\pi}{2} \right\} \end{align} We see : \begin{align} (x,y) \in D_{1} \bigcap D_{3} 	& \Rightarrow \cos(x+y) = 0 \\ (x,y) \in D_{2} \bigcap D_{3} 	& \Rightarrow \cos(x+y) = 0 \end{align} So  : \begin{equation} \int\int_{R} |\cos(x+y)| d(x,y) = \int\int_{D_{1}} \cos(x+y)d(x,y) + \int\int_{D_{2}} \cos(x+y)d(x,y) - \int\int_{D_{3}} \cos(x+y)d(x,y) \end{equation} We see : \begin{align} (x,y) \in D_{1} & \Rightarrow x \in \left[ 0 , \frac{\pi}{2} \right] \text{ and } y \in \left[ 0 , \frac{\pi}{2} - x \right] \\ (x,y) \in D_{2} & \Rightarrow x \in \left[ \frac{\pi}{2} , \pi \right] \text{ and } y \in \left[ \frac{3\pi}{2} - x , \pi \right] \end{align} Let's say : \begin{equation} D_{3} = D_{3}^{(a)} \bigcup D_{3}^{(b)} \end{equation} where : \begin{align} D_{3}^{(a)} 	& = \left\{ (x,y) \in D_{3} \; : \; x \in \left[ 0 , \frac{\pi}{2} \right] \right\}\\ D_{3}^{(b)} 	& = \left\{ (x,y) \in D_{3} \; : \; x \in \left[ \frac{\pi}{2} , \pi \right] \right\} \end{align} So : \begin{align} (x,y) \in D_{3}^{(a)} & \Rightarrow x \in \left[ 0 , \frac{\pi}{2} \right] \text{ and } y \in \left[ \frac{\pi}{2} - x , \pi \right] \\ (x,y) \in D_{3}^{(b)} & \Rightarrow x \in \left[ \frac{\pi}{2} , \pi \right] \text{ and } y \in \left[ 0 , \frac{3\pi}{2} - x \right] \end{align} and : \begin{equation} (x,y) \in D_{3}^{(a)} \bigcap D_{3}^{(b)} \Leftrightarrow x = \frac{\pi}{2} \text{ and } y \in \left[ 0, \pi \right] \end{equation} So : \begin{equation} \int\int_{D_{3}} \cos(x+y)d(x,y) = \int\int_{D_{3}^{(a)}} \cos(x+y)d(x,y) + \int\int_{D_{3}^{(b)}} \cos(x+y)d(x,y) - \int\int_{D_{3}^{(a)}\bigcap D_{3}^{(b)}} \cos(x+y)d(x,y) \end{equation} We can see that $D_{1},D_{2},D_{3}^{(a)},D_{3}^{(b)}$ , and $D_{3}^{(a)} \bigcap D_{3}^{(b)}$ are all elementary regions. So we can use Fubini's theorm to evaluate each. Let : \begin{align} \phi_{1}(x) & = 0 \\ \phi_{2}(x) & = \frac{\pi}{2} - x \end{align} So : \begin{align} \require{cancel} \int\int_{D_{1}} \cos(x+y)d(x,y) 	 	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right] dx\\ 	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{0}^{\frac{\pi}{2} - x} \cos(x+y) dy \right] dx\\ 	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x+y) \Bigr|_{0}^{\frac{\pi}{2} - x} \right) dx \\ 	& = \int_{0}^{\frac{\pi}{2}} \left( \sin\left( \cancel{x} + \frac{\pi}{2} - \cancel{x} \right) - \sin(x) \right) dx\\ 	& = \int_{0}^{\frac{\pi}{2}} \sin\left( \frac{\pi}{2} \right) dx - \int_{0}^{\frac{\pi}{2}} \sin(x) dx \\ 	& = \int_{0}^{\frac{\pi}{2}} 1 dx + \left( \cos(x) \Bigr|_{0}^{\frac{\pi}{2}} \right)\\ 	& = \frac{\pi}{2} + \cos\left( \frac{\pi}{2} \right) - \cos(0) \\ 	& = \frac{\pi}{2} + 0 - 1 \\ 	& = \frac{\pi}{2} - 1 \end{align} Let : \begin{align} \phi_{1} & = \frac{3\pi}{2} - x \\ \phi_{2} & = \pi \end{align} So : \begin{align} \int\int_{D_{2}} \cos(x+y)d(x,y)  	& = \int_{\frac{\pi}{2}}^{\pi} \left( \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right) dx\\ 	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+y)\Bigr|_{\frac{3\pi}{2} - x}^{\pi} \right) dx\\ 	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+\pi) - \sin\left( \cancel{x} + \frac{3\pi}{2} - \cancel{x} \right) \right)dx\\ 	& = \int_{\frac{\pi}{2}}^{\pi} \sin(x+\pi) dx - \int_{\frac{\pi}{2}}^{\pi} \sin\left( \frac{3\pi}{2} \right) dx\\ 	& = \int_{\frac{\pi}{2}}^{\pi} \left[ \sin(x)\cos(\pi) + \cancel{\sin(\pi)\cos(x)} \right] dx + \int_{\frac{\pi}{2}}^{\pi} dx\\ 	& = \left( - \int_{\frac{\pi}{2}}^{\pi} \sin(x) dx \right) + \frac{\pi}{2} \\ 	& = \cos(x) \Bigr|_{\frac{\pi}{2}}^{\pi} + \frac{\pi}{2} \\ 	& = \cos(\pi) - \cancel{\cos\left( \frac{\pi}{2} \right)} + \frac{\pi}{2} \\ 	& = -1 + \frac{\pi}{2} \\ 	& = \frac{\pi}{2} - 1  \end{align} Now let : \begin{align} \phi_{1}(x) & = \frac{\pi}{2} - x \\ \phi_{2}(x) & = \pi \end{align} So : \begin{align} \int\int_{D_{3}^{(a)}} \cos(x+y) d(x,y)  	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right] dx\\ 	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x+y) \Bigr|_{\frac{\pi}{2} - x}^{\pi} \right) dx\\ 	& = \int_{0}^{\frac{\pi}{2}} \sin(x+\pi) dx - \int_{0}^{\frac{\pi}{2}} \sin\left(\cancel{x} + \frac{\pi}{2} - \cancel{x} \right) dx\\ 	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x)\cos(\pi) + \cancel{\sin(\pi)\cos(x)} \right) dx - \int_{0}^{\frac{\pi}{2}} \sin\left( \frac{\pi}{2} \right) dx\\ 	& = -\int_{0}^{\frac{\pi}{2}} \sin(x) dx + 0 - \frac{\pi}{2}\\ 	& = \cos(x) \Bigr|_{0}^{\frac{\pi}{2}} - \frac{\pi}{2} \\ 	& = \cancel{\cos\left( \frac{\pi}{2} \right)} - \cos(0) - \frac{\pi}{2}\\ 	& = -1 - \frac{\pi}{2}  \end{align} Now let : \begin{align} \phi_{1}(x) & = 0\\ \phi_{2}(x) & = \frac{3\pi}{2} - x \end{align} So : \begin{align} \int\int_{D_{3}^{(b)}} \cos(x+y)d(x,y)  	& = \int_{\frac{\pi}{2}}^{\pi} \left( \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right) dx\\ 	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+y) \Bigr|_{0}^{\frac{3\pi}{2} - x} \right) dx\\ 	& = \int_{\frac{\pi}{2}}^{\pi} \left[ \sin\left( \cancel{x} + \frac{3\pi}{2} - \cancel{x} \right) - \sin(x) \right] dx\\ 	& = - \int_{\frac{\pi}{2}}^{\pi} 1 dx - \int_{\frac{\pi}{2}}^{\pi} \sin(x) dx \\ 	& = -\frac{\pi}{2} + \left( \cos(\pi) - \cos\left( \frac{\pi}{2} \right) \right)\\ 	& = -\frac{\pi}{2} - 1  \end{align} We see : \begin{align} \int\int_{D_{3}^{(a)} \bigcap D_{3}^{(b)}} \cos(x+y) d(x,y)  	& = \int_{0}^{\pi} \cos\left( \frac{\pi}{2} + y \right) dy \\ 	& = \int_{0}^{\pi} \left[ \cancel{\cos\left(\frac{\pi}{2}\right)\cos(y)} - \sin\left(\frac{\pi}{2}\right) \sin(y) \right] dy\\ 	& = -\int_{0}^{\pi} \sin(y) dy\\ 	& = \cos(y) \Bigr|_{0}^{\pi}\\ 	& = \cos(\pi) - \cos(0)\\ 	& = -1 - 1 = -2 \end{align} So : \begin{align} \int\int_{D_{3}} \cos(x+y)d(x,y) 	 	& = \left( -1 - \frac{\pi}{2} \right) + \left( -\frac{\pi}{2} - 1 \right) - (-2)\\ 	& = \cancel{-2} - \pi + \cancel{2} \\ 	& = -\pi \end{align} So : \begin{align} \int\int_{R} |\cos(x+y)|d(x,y) 	& = \left( \frac{\pi}{2} - 1 \right) + \left( \frac{\pi}{2} - 1 \right) - (-\pi)\\ 				& = \pi - 2 + \pi\\ 				& = 2 \pi - 2  \neq 2\pi \end{align} So I made a mistake somewhere. Can anyone help me see where I made the mistakes that lead to the incorrect result.","I have come to an exercise in a multivariate calculus book that I am having trouble with. The problem is : Show that I have attempted solving the problem but I'm not getting the right answer. I know there are probably other ways to solve this problem, but here I am seeking help from others to see where I am going wrong in my solution. My solution is below : Let : We see : We see : Define : We see : and : We see : So  : We see : Let's say : where : So : and : So : We can see that , and are all elementary regions. So we can use Fubini's theorm to evaluate each. Let : So : Let : So : Now let : So : Now let : So : We see : So : So : So I made a mistake somewhere. Can anyone help me see where I made the mistakes that lead to the incorrect result.","\int\int_{[0,\pi]\times[0,\pi]} |\cos(x+y)| d(x,y) = 2 \pi \begin{equation}
R = [0,\pi] \times [0,\pi]
\end{equation} \begin{equation}
\{ x + y \; : \; (x,y) \in R \} = [0,2\pi] = T
\end{equation} \begin{equation}
| \cos(x+y) | = 
	\begin{cases}
	\cos(x+y)  \; & \forall \; x + y \in \left[ 0 , \frac{\pi}{2} \right] \bigcup \left[ \frac{3\pi}{2} , 2 \pi \right] \\
	-\cos(x+y) \; & \forall \; x + y \in \left[ \frac{\pi}{2} , \frac{3\pi}{2} \right]
	\end{cases}
\end{equation} \begin{align}
D_{1} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ 0 , \frac{\pi}{2} \right] \right\} 	\\
D_{2} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ \frac{3\pi}{2} , 2\pi \right] \right\} \\
D_{3} 	& = \left\{ (x,y) \in R \; : \; x + y \in \left[ \frac{\pi}{2} , \frac{3\pi}{2} \right] \right\} 
\end{align} \begin{equation}
T = D_{1} \bigcup D_{2} \bigcup D_{3}
\end{equation} \begin{align}
D_{1} \bigcap D_{2}	& = \emptyset \\
D_{1} \bigcap D_{3}	& = \left\{ (x,y) \in R \; : \; x + y = \frac{\pi}{2} \right\}\\
D_{2} \bigcap D_{3}	& = \left\{ (x,y) \in R \; : \; x + y = \frac{3\pi}{2} \right\}
\end{align} \begin{align}
(x,y) \in D_{1} \bigcap D_{3} 	& \Rightarrow \cos(x+y) = 0 \\
(x,y) \in D_{2} \bigcap D_{3} 	& \Rightarrow \cos(x+y) = 0
\end{align} \begin{equation}
\int\int_{R} |\cos(x+y)| d(x,y) = \int\int_{D_{1}} \cos(x+y)d(x,y) + \int\int_{D_{2}} \cos(x+y)d(x,y) - \int\int_{D_{3}} \cos(x+y)d(x,y)
\end{equation} \begin{align}
(x,y) \in D_{1} & \Rightarrow x \in \left[ 0 , \frac{\pi}{2} \right] \text{ and } y \in \left[ 0 , \frac{\pi}{2} - x \right] \\
(x,y) \in D_{2} & \Rightarrow x \in \left[ \frac{\pi}{2} , \pi \right] \text{ and } y \in \left[ \frac{3\pi}{2} - x , \pi \right]
\end{align} \begin{equation}
D_{3} = D_{3}^{(a)} \bigcup D_{3}^{(b)}
\end{equation} \begin{align}
D_{3}^{(a)} 	& = \left\{ (x,y) \in D_{3} \; : \; x \in \left[ 0 , \frac{\pi}{2} \right] \right\}\\
D_{3}^{(b)} 	& = \left\{ (x,y) \in D_{3} \; : \; x \in \left[ \frac{\pi}{2} , \pi \right] \right\}
\end{align} \begin{align}
(x,y) \in D_{3}^{(a)} & \Rightarrow x \in \left[ 0 , \frac{\pi}{2} \right] \text{ and } y \in \left[ \frac{\pi}{2} - x , \pi \right] \\
(x,y) \in D_{3}^{(b)} & \Rightarrow x \in \left[ \frac{\pi}{2} , \pi \right] \text{ and } y \in \left[ 0 , \frac{3\pi}{2} - x \right]
\end{align} \begin{equation}
(x,y) \in D_{3}^{(a)} \bigcap D_{3}^{(b)} \Leftrightarrow x = \frac{\pi}{2} \text{ and } y \in \left[ 0, \pi \right]
\end{equation} \begin{equation}
\int\int_{D_{3}} \cos(x+y)d(x,y) = \int\int_{D_{3}^{(a)}} \cos(x+y)d(x,y) + \int\int_{D_{3}^{(b)}} \cos(x+y)d(x,y) - \int\int_{D_{3}^{(a)}\bigcap D_{3}^{(b)}} \cos(x+y)d(x,y)
\end{equation} D_{1},D_{2},D_{3}^{(a)},D_{3}^{(b)} D_{3}^{(a)} \bigcap D_{3}^{(b)} \begin{align}
\phi_{1}(x) & = 0 \\
\phi_{2}(x) & = \frac{\pi}{2} - x
\end{align} \begin{align}
\require{cancel}
\int\int_{D_{1}} \cos(x+y)d(x,y) 	
	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right] dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{0}^{\frac{\pi}{2} - x} \cos(x+y) dy \right] dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x+y) \Bigr|_{0}^{\frac{\pi}{2} - x} \right) dx \\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin\left( \cancel{x} + \frac{\pi}{2} - \cancel{x} \right) - \sin(x) \right) dx\\
	& = \int_{0}^{\frac{\pi}{2}} \sin\left( \frac{\pi}{2} \right) dx - \int_{0}^{\frac{\pi}{2}} \sin(x) dx \\
	& = \int_{0}^{\frac{\pi}{2}} 1 dx + \left( \cos(x) \Bigr|_{0}^{\frac{\pi}{2}} \right)\\
	& = \frac{\pi}{2} + \cos\left( \frac{\pi}{2} \right) - \cos(0) \\
	& = \frac{\pi}{2} + 0 - 1 \\
	& = \frac{\pi}{2} - 1
\end{align} \begin{align}
\phi_{1} & = \frac{3\pi}{2} - x \\
\phi_{2} & = \pi
\end{align} \begin{align}
\int\int_{D_{2}} \cos(x+y)d(x,y) 
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+y)\Bigr|_{\frac{3\pi}{2} - x}^{\pi} \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+\pi) - \sin\left( \cancel{x} + \frac{3\pi}{2} - \cancel{x} \right) \right)dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \sin(x+\pi) dx - \int_{\frac{\pi}{2}}^{\pi} \sin\left( \frac{3\pi}{2} \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left[ \sin(x)\cos(\pi) + \cancel{\sin(\pi)\cos(x)} \right] dx + \int_{\frac{\pi}{2}}^{\pi} dx\\
	& = \left( - \int_{\frac{\pi}{2}}^{\pi} \sin(x) dx \right) + \frac{\pi}{2} \\
	& = \cos(x) \Bigr|_{\frac{\pi}{2}}^{\pi} + \frac{\pi}{2} \\
	& = \cos(\pi) - \cancel{\cos\left( \frac{\pi}{2} \right)} + \frac{\pi}{2} \\
	& = -1 + \frac{\pi}{2} \\
	& = \frac{\pi}{2} - 1 
\end{align} \begin{align}
\phi_{1}(x) & = \frac{\pi}{2} - x \\
\phi_{2}(x) & = \pi
\end{align} \begin{align}
\int\int_{D_{3}^{(a)}} \cos(x+y) d(x,y) 
	& = \int_{0}^{\frac{\pi}{2}} \left[ \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right] dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x+y) \Bigr|_{\frac{\pi}{2} - x}^{\pi} \right) dx\\
	& = \int_{0}^{\frac{\pi}{2}} \sin(x+\pi) dx - \int_{0}^{\frac{\pi}{2}} \sin\left(\cancel{x} + \frac{\pi}{2} - \cancel{x} \right) dx\\
	& = \int_{0}^{\frac{\pi}{2}} \left( \sin(x)\cos(\pi) + \cancel{\sin(\pi)\cos(x)} \right) dx - \int_{0}^{\frac{\pi}{2}} \sin\left( \frac{\pi}{2} \right) dx\\
	& = -\int_{0}^{\frac{\pi}{2}} \sin(x) dx + 0 - \frac{\pi}{2}\\
	& = \cos(x) \Bigr|_{0}^{\frac{\pi}{2}} - \frac{\pi}{2} \\
	& = \cancel{\cos\left( \frac{\pi}{2} \right)} - \cos(0) - \frac{\pi}{2}\\
	& = -1 - \frac{\pi}{2} 
\end{align} \begin{align}
\phi_{1}(x) & = 0\\
\phi_{2}(x) & = \frac{3\pi}{2} - x
\end{align} \begin{align}
\int\int_{D_{3}^{(b)}} \cos(x+y)d(x,y) 
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \int_{\phi_{1}(x)}^{\phi_{2}(x)} \cos(x+y) dy \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left( \sin(x+y) \Bigr|_{0}^{\frac{3\pi}{2} - x} \right) dx\\
	& = \int_{\frac{\pi}{2}}^{\pi} \left[ \sin\left( \cancel{x} + \frac{3\pi}{2} - \cancel{x} \right) - \sin(x) \right] dx\\
	& = - \int_{\frac{\pi}{2}}^{\pi} 1 dx - \int_{\frac{\pi}{2}}^{\pi} \sin(x) dx \\
	& = -\frac{\pi}{2} + \left( \cos(\pi) - \cos\left( \frac{\pi}{2} \right) \right)\\
	& = -\frac{\pi}{2} - 1 
\end{align} \begin{align}
\int\int_{D_{3}^{(a)} \bigcap D_{3}^{(b)}} \cos(x+y) d(x,y) 
	& = \int_{0}^{\pi} \cos\left( \frac{\pi}{2} + y \right) dy \\
	& = \int_{0}^{\pi} \left[ \cancel{\cos\left(\frac{\pi}{2}\right)\cos(y)} - \sin\left(\frac{\pi}{2}\right) \sin(y) \right] dy\\
	& = -\int_{0}^{\pi} \sin(y) dy\\
	& = \cos(y) \Bigr|_{0}^{\pi}\\
	& = \cos(\pi) - \cos(0)\\
	& = -1 - 1 = -2
\end{align} \begin{align}
\int\int_{D_{3}} \cos(x+y)d(x,y) 	
	& = \left( -1 - \frac{\pi}{2} \right) + \left( -\frac{\pi}{2} - 1 \right) - (-2)\\
	& = \cancel{-2} - \pi + \cancel{2} \\
	& = -\pi
\end{align} \begin{align}
\int\int_{R} |\cos(x+y)|d(x,y) 	& = \left( \frac{\pi}{2} - 1 \right) + \left( \frac{\pi}{2} - 1 \right) - (-\pi)\\
				& = \pi - 2 + \pi\\
				& = 2 \pi - 2  \neq 2\pi
\end{align}",['multivariable-calculus']
46,Show that the tangent plane to the surface $\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1$,Show that the tangent plane to the surface,\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1,"Show that the tangent plane to the surface $$\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1$$ At $(x_0,y_0,z_0)$ is given by $$\frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}=1$$ Then assume $a=b=c=1$ and show the tangent lines to the surface in the intersection with $x=x_0$ construct a cone. ( $x_0 >1$ ). Define $$F(x,y,z)=\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}-1$$ Then the tangent plane to the the surface at the given point is: $$\frac{2x_{0}}{a^{2}}\left(x-x_{0}\right)-\frac{2y_{0}}{b^{2}}\left(y-y_{0}\right)-\frac{2z_{0}}{c^{2}}\left(z-z_{0}\right)=0$$ On the other hand the point is also on the surface so the equation is: $$\frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}-\left(\frac{2x_{0}^{2}}{a^{2}}-\frac{2y_{0}^{2}}{b^{2}}-\frac{2z_{0}^{2}}{c^{2}}\right)=0=0$$ $$\frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}=1$$ Also if $a=b=c=1$ then the points on the intersection of the surface with $x=x_0$ satisfy the following relation: $$x_0^2-y^2-z^2=1$$ $$z^2+y^2=x_0^2-1\tag{1}$$ Now should I find the tangent lines to $(1)$ ? And show that they construct a cone?",Show that the tangent plane to the surface At is given by Then assume and show the tangent lines to the surface in the intersection with construct a cone. ( ). Define Then the tangent plane to the the surface at the given point is: On the other hand the point is also on the surface so the equation is: Also if then the points on the intersection of the surface with satisfy the following relation: Now should I find the tangent lines to ? And show that they construct a cone?,"\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1 (x_0,y_0,z_0) \frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}=1 a=b=c=1 x=x_0 x_0 >1 F(x,y,z)=\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}-1 \frac{2x_{0}}{a^{2}}\left(x-x_{0}\right)-\frac{2y_{0}}{b^{2}}\left(y-y_{0}\right)-\frac{2z_{0}}{c^{2}}\left(z-z_{0}\right)=0 \frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}-\left(\frac{2x_{0}^{2}}{a^{2}}-\frac{2y_{0}^{2}}{b^{2}}-\frac{2z_{0}^{2}}{c^{2}}\right)=0=0 \frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}=1 a=b=c=1 x=x_0 x_0^2-y^2-z^2=1 z^2+y^2=x_0^2-1\tag{1} (1)","['multivariable-calculus', 'surfaces']"
47,Multivariable Calculus and Differentiability,Multivariable Calculus and Differentiability,,"$$f(x,y)=\begin{cases}\dfrac{y^3}{x^2+y^2} &(x,y) \neq \ \mathbb{(0,0)}\\ 0 &  (x,y)=(0,0) \\ \end{cases}$$ Evaluate $f_x(0,0)$ and $f_y(0,0)$ and $D_\overrightarrow{u}f(0,0)$ I tried directly taking the derivative to no avail (obviously) so then I tried to use the definition of partial derivative which also left me without a correct solution. Also I have proved that it is continuous (Sertoz Theorem) but how would I prove that it is also differentiable at $(0,0)$ ?",Evaluate and and I tried directly taking the derivative to no avail (obviously) so then I tried to use the definition of partial derivative which also left me without a correct solution. Also I have proved that it is continuous (Sertoz Theorem) but how would I prove that it is also differentiable at ?,"f(x,y)=\begin{cases}\dfrac{y^3}{x^2+y^2} &(x,y) \neq \ \mathbb{(0,0)}\\ 0 &  (x,y)=(0,0) \\ \end{cases} f_x(0,0) f_y(0,0) D_\overrightarrow{u}f(0,0) (0,0)","['multivariable-calculus', 'derivatives', 'piecewise-continuity']"
48,How does one find the moment of inertia about a line passing through the centroid of a cone?,How does one find the moment of inertia about a line passing through the centroid of a cone?,,"Image 1 detailing the question Image 2 showing the part of the working I do not understand In image 2 part d, I'm unsure why the moment of inertia with respect to the y axis, Iy, is equal to Ic plus the expression shown. Could someone explain to me the intuition behind this equation? Can this be extended to lines other than that passing through the centroid?","Image 1 detailing the question Image 2 showing the part of the working I do not understand In image 2 part d, I'm unsure why the moment of inertia with respect to the y axis, Iy, is equal to Ic plus the expression shown. Could someone explain to me the intuition behind this equation? Can this be extended to lines other than that passing through the centroid?",,"['calculus', 'multivariable-calculus']"
49,Solving this double integral,Solving this double integral,,"I have the following double integral, and I am struggling to solve it. $$\int_{0}^{1} y dy \int_{y}^{1}\frac{dx}{x}\left[f(x)+g(x)\right]h \left(\frac{y}{x}\right)$$ Both $f(x)$ and $g(x)$ are known in their full analytic form. However, we do not have any information about the analytic form of the function $h\left(\frac{y}{x}\right)$ . Could someone give me a hint as to how I can go about solving this? Also, let me know if my question is missing any information.","I have the following double integral, and I am struggling to solve it. Both and are known in their full analytic form. However, we do not have any information about the analytic form of the function . Could someone give me a hint as to how I can go about solving this? Also, let me know if my question is missing any information.",\int_{0}^{1} y dy \int_{y}^{1}\frac{dx}{x}\left[f(x)+g(x)\right]h \left(\frac{y}{x}\right) f(x) g(x) h\left(\frac{y}{x}\right),"['multivariable-calculus', 'definite-integrals', 'multiple-integral']"
50,Question about Divergence formula derivation posted a while ago,Question about Divergence formula derivation posted a while ago,,"As a new user, I am not allowed to comment on someone else's answer to a question. My only choice was to ask a new question about an old answer to a question. User @Kcronix mentioned, in this question , that the divergence of an arbitrary vector field can be derived as the net flux through the boundary of a region $R$ : $$\text{net flux}=\oint_{\partial \text{Rect}} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{R} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{L} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{B} \vec{V}\cdot \hat{n}\;\mathbb{d}s$$ I have tried to define the same situation in a notebook and I figured that going counter-clockwise through the sides of the rectangle would be a good idea. So it would be simply rearranged in my case: $$\text{net flux}=\oint_{\partial \text{Rect}} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{B} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{R} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{L} \vec{V}\cdot \hat{n}\;\mathbb{d}s$$ Thus, I defined my top flux as: $$\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{x+\Delta x}^{x} \left(M(X,y+\Delta y)\hat   i+N(X,y+\Delta y)\hat j\right)\cdot \hat j\;\mathbb{d}X=\int_{x+\Delta x}^{x}N(X,y+\Delta y)\;\mathbb{d}X$$ Please, notice how the integral goes from $x+\Delta x$ to $x$ , instead of going from $x$ to $x+\Delta x$ . This also happens to occur with the left side of the rectangle, going from $y+\Delta y$ to $y$ , instead of $y$ to $y+\Delta y$ . This is my main question with this derivation. I know I can switch the boundaries of the integral by bringing a negative sign outside of it (from the Fundamental Theorem of Calculus), but still, I would get: $$\text{net flux}=\int_{y}^{y+\Delta y}M(x+\Delta x,Y)\;\mathbb{d}Y+\int_{y}^{y+\Delta y}M(x,Y)\;\mathbb{d}Y\\-\int_{x}^{x+\Delta x}N(X,y+\Delta y)\;\mathbb{d}X-\int_{x}^{x+\Delta x}N(X,y)\;\mathbb{d}X\tag{1}$$ and I do not think I can reduce to the partial derivative definition with the above. Is there anything I am missing?","As a new user, I am not allowed to comment on someone else's answer to a question. My only choice was to ask a new question about an old answer to a question. User @Kcronix mentioned, in this question , that the divergence of an arbitrary vector field can be derived as the net flux through the boundary of a region : I have tried to define the same situation in a notebook and I figured that going counter-clockwise through the sides of the rectangle would be a good idea. So it would be simply rearranged in my case: Thus, I defined my top flux as: Please, notice how the integral goes from to , instead of going from to . This also happens to occur with the left side of the rectangle, going from to , instead of to . This is my main question with this derivation. I know I can switch the boundaries of the integral by bringing a negative sign outside of it (from the Fundamental Theorem of Calculus), but still, I would get: and I do not think I can reduce to the partial derivative definition with the above. Is there anything I am missing?","R \text{net flux}=\oint_{\partial \text{Rect}} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{R} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{L} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{B} \vec{V}\cdot \hat{n}\;\mathbb{d}s \text{net flux}=\oint_{\partial \text{Rect}} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{B} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{R} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s+\int_{L} \vec{V}\cdot \hat{n}\;\mathbb{d}s \int_{T} \vec{V}\cdot \hat{n}\;\mathbb{d}s=\int_{x+\Delta x}^{x} \left(M(X,y+\Delta y)\hat 
 i+N(X,y+\Delta y)\hat j\right)\cdot \hat j\;\mathbb{d}X=\int_{x+\Delta x}^{x}N(X,y+\Delta y)\;\mathbb{d}X x+\Delta x x x x+\Delta x y+\Delta y y y y+\Delta y \text{net flux}=\int_{y}^{y+\Delta y}M(x+\Delta x,Y)\;\mathbb{d}Y+\int_{y}^{y+\Delta y}M(x,Y)\;\mathbb{d}Y\\-\int_{x}^{x+\Delta x}N(X,y+\Delta y)\;\mathbb{d}X-\int_{x}^{x+\Delta x}N(X,y)\;\mathbb{d}X\tag{1}","['multivariable-calculus', 'partial-derivative', 'line-integrals', 'divergence-operator', 'divergence-theorem']"
51,"Is the integral identity $\int d x \, f(x) \, \, g(f(x))' \, = \, 0$ true?",Is the integral identity  true?,"\int d x \, f(x) \, \, g(f(x))' \, = \, 0","We have a positive function $f$ on a certain N-dimensional domain $E \subset  \mathbb{R}^N$ , namely $f(x)\geq 0$ for every $x\in E$ . The function is such that $f(x)=0$ on the boundary, when $x \in \partial E$ . We also require that $$ \int_E d^Nx \, f(x)  \, = \, F >0 $$ is finite. Given a ""well behaved"" function $g: \mathbb{R} \rightarrow \mathbb{R}$ , how to prove that  (feel free to assume all the nice continuity properties you want for both $f$ and $g$ ) $$ \int_E d^Nx \, f(x) \, \nabla \, g(f(x)) \, = \, 0 \quad ? $$ Attempt: probably the N-dimensional domain is a complication and it is possible to just consider the 1D case. If $g$ is the identity, by using the integration by parts we have $$ \int_E dx \, f \, \partial_x \, f \,=-\int_E dx \, (\partial_x f) \,  \, f  $$ so in this simple case the statement is true. A similar argument holds also when $g(f) = f^a$ for some real power $a\neq -1$ such that the integrals are finite.","We have a positive function on a certain N-dimensional domain , namely for every . The function is such that on the boundary, when . We also require that is finite. Given a ""well behaved"" function , how to prove that  (feel free to assume all the nice continuity properties you want for both and ) Attempt: probably the N-dimensional domain is a complication and it is possible to just consider the 1D case. If is the identity, by using the integration by parts we have so in this simple case the statement is true. A similar argument holds also when for some real power such that the integrals are finite.","f E \subset  \mathbb{R}^N f(x)\geq 0 x\in E f(x)=0 x \in \partial E 
\int_E d^Nx \, f(x)  \, = \, F >0
 g: \mathbb{R} \rightarrow \mathbb{R} f g 
\int_E d^Nx \, f(x) \, \nabla \, g(f(x)) \, = \, 0 \quad ?
 g 
\int_E dx \, f \, \partial_x \, f \,=-\int_E dx \, (\partial_x f) \,  \, f 
 g(f) = f^a a\neq -1","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
52,Calculating the derivative of the map $T\to T^{-1}$,Calculating the derivative of the map,T\to T^{-1},"Let $E$ be a banach space and $U$ be the set of all bounded invertible linear operator on $E$ with bounded inverse. Consider the map, $f:U\to U$ by $T\to T^{-1}$ Now we are interested in calculating the 2nd order  derivatives of this map. For the first order derivative, I got that $D(f(T))(S)=-T^{-1}ST^{-1}=-M(S),$ where $M:BL(E)\to BL(E)$ given by, $M(S)=T^{-1}ST^{-1}$ Let, $A: BL(BL(E),BL(E))\to BL(E)$ denoted by, $A(T)=T(S)$ be the evaluation map at $S.$ So , we have, $ A(D(f(T)))=-M(S).$ Therefore by chain rule, we get, $$D^2(f(T))(S,W)=-DM(S)(W)=T^{-1}WT^{-1}.$$ It seems I am doing something wrong. Can anyone point it out? Edit: Calculating it simply using definition, I found that the answer will be, $D^2(f(T))(S,W)=T^{-1}ST^{-1}WT^{-1}+T^{-1}WT^{-1}ST^{-1}$ which is correct,I guess.","Let be a banach space and be the set of all bounded invertible linear operator on with bounded inverse. Consider the map, by Now we are interested in calculating the 2nd order  derivatives of this map. For the first order derivative, I got that where given by, Let, denoted by, be the evaluation map at So , we have, Therefore by chain rule, we get, It seems I am doing something wrong. Can anyone point it out? Edit: Calculating it simply using definition, I found that the answer will be, which is correct,I guess.","E U E f:U\to U T\to T^{-1} D(f(T))(S)=-T^{-1}ST^{-1}=-M(S), M:BL(E)\to BL(E) M(S)=T^{-1}ST^{-1} A: BL(BL(E),BL(E))\to BL(E) A(T)=T(S) S.  A(D(f(T)))=-M(S). D^2(f(T))(S,W)=-DM(S)(W)=T^{-1}WT^{-1}. D^2(f(T))(S,W)=T^{-1}ST^{-1}WT^{-1}+T^{-1}WT^{-1}ST^{-1}","['real-analysis', 'multivariable-calculus']"
53,Show that the set $X_s = \{A\in M_{m\times n}(\Bbb {R}) | rk(A) \leq s\}$ is closed,Show that the set  is closed,X_s = \{A\in M_{m\times n}(\Bbb {R}) | rk(A) \leq s\},"A bit of background: I am trying to show that if $f\in C^1(\Bbb {R}^n,\Bbb{R}^m)$ then for every $a\in \Bbb{R}^n$ there exists a neighborrhood $U$ of $a$ s.t. for every $x\in U$ we have $rk(Df(x))\geq rk(Df(a))$ . I am trying to prove this by contradition,  suppose there isn't such a neighborrhood of $a$ with this property, then there is a sequence $x_k\to a$ s.t. $rk(Df(x_k))<rk(Df(a))$ for every $k$ . Since the degree only takes on finitely any values there is a sub-sequence $x_{k_l}\to a$ with $rk(Df(x_{k_l})) = s<rk(Df(a))$ Hence if $X_s = \{A\in M_{m\times n}(\Bbb {R}) | rk(A) \leq s\}$ is closed then we get a contradiction since then $Df(a)\in X_s$ which we assumed isent true. Here are a few of my attempts : I will denote $V = M_{m\times n}(\Bbb R)$ throughout this post. $\mathbf {Attempt}$ $\mathbf{one:}$ Induction on $s$ , for $s=0$ the statment is obvious since $X_0 = \{0\}$ which is closed. now lets assume the statment is true for $s$ , every matrix in $X_{s+1}$ can be written as $A+B$ where $A\in X_s$ and $B\in X_1$ so: $$X_{s+1}\subseteq X_{s}+X_1$$ This is a sum of two closed sets in $V$ so we only need to show it is closed (which is not always true - there are closed subsets whose sum isn't). Now if $A\in \partial ( X_{s}+X_1)$ then there is a sequence $B_k+C_k\to A$ s.t. $B_k\in  X_s$ and $C_k\in X_1$ $\mathbf {IF}$ $B_k\to B$ and $C_k\to C$ converege themselves then the statment is obvius since: $$A = lim_{k\to \infty} B_k+C_k = lim_{k\to \infty} B_k +lim_{k\to \infty} C_k =B+C\in X_s+X_1$$ since $B\in X_s$ and $C \in  X_1$ since they are closed by the induction hypothesis and so $A\in  X_s+X_1$ and we are done. However, it might be that $B_k$ and $C_k$ dont converge so we have a problem. In this case, my qeustions are: can we choose a two convergeing sequences $B_k,C_k$ s.t. $B_k\in X_s$ and $C_k \in X_1$ and $B_k+C_k\to A$ If not, can we show the sum is closed by some other method? $\mathbf{Attempt}$ $\mathbf{two:}$ Notice that Gaussian elimination is a linear (hence continuous) and preserves rank so if $A_k\to A$ where $A_k \in  X_s$ then by applying Gaussian elimination to $A$ we can bring it to the form $\varphi(A)=diag(a_1,...,a_u,0,...,0)$ and since $\varphi$ is continuous we have $\varphi(A_k)\to diag(a_1,...,a_u,0,...,0)$ hence $[\varphi(A_k)]_{ij}\to \delta_{i,j,\leq u} a_i$ where $\delta_{i,j,\leq u} = 1$ iff $i=j\leq u$ . Let us write: $$\varphi(A_k) = \begin{pmatrix} (v_1)_k \\ \vdots \\ (v_s)_k \\ \sum_{i=0}^{s} x^k_{s+1,i} (v_i)_k \\ \vdots \\ \sum_{i=0}^{s} x^k_{m,i} (v_i)_k\\ \end{pmatrix}$$ So $(v_i)_k\to a_i\cdot e_i$ for each $1\leq i\leq s$ and $\sum_{i=0}^{s} x^k_{j,i} (v_i)_k \to 0 $ for each $s+1\leq j\leq m$ . Here I am pretty much stuck. any help would be much appreciated.","A bit of background: I am trying to show that if then for every there exists a neighborrhood of s.t. for every we have . I am trying to prove this by contradition,  suppose there isn't such a neighborrhood of with this property, then there is a sequence s.t. for every . Since the degree only takes on finitely any values there is a sub-sequence with Hence if is closed then we get a contradiction since then which we assumed isent true. Here are a few of my attempts : I will denote throughout this post. Induction on , for the statment is obvious since which is closed. now lets assume the statment is true for , every matrix in can be written as where and so: This is a sum of two closed sets in so we only need to show it is closed (which is not always true - there are closed subsets whose sum isn't). Now if then there is a sequence s.t. and and converege themselves then the statment is obvius since: since and since they are closed by the induction hypothesis and so and we are done. However, it might be that and dont converge so we have a problem. In this case, my qeustions are: can we choose a two convergeing sequences s.t. and and If not, can we show the sum is closed by some other method? Notice that Gaussian elimination is a linear (hence continuous) and preserves rank so if where then by applying Gaussian elimination to we can bring it to the form and since is continuous we have hence where iff . Let us write: So for each and for each . Here I am pretty much stuck. any help would be much appreciated.","f\in C^1(\Bbb {R}^n,\Bbb{R}^m) a\in \Bbb{R}^n U a x\in U rk(Df(x))\geq rk(Df(a)) a x_k\to a rk(Df(x_k))<rk(Df(a)) k x_{k_l}\to a rk(Df(x_{k_l})) = s<rk(Df(a)) X_s = \{A\in M_{m\times n}(\Bbb {R}) | rk(A) \leq s\} Df(a)\in X_s V = M_{m\times n}(\Bbb R) \mathbf {Attempt} \mathbf{one:} s s=0 X_0 = \{0\} s X_{s+1} A+B A\in X_s B\in X_1 X_{s+1}\subseteq X_{s}+X_1 V A\in \partial ( X_{s}+X_1) B_k+C_k\to A B_k\in  X_s C_k\in X_1 \mathbf {IF} B_k\to B C_k\to C A = lim_{k\to \infty} B_k+C_k = lim_{k\to \infty} B_k +lim_{k\to \infty} C_k =B+C\in X_s+X_1 B\in X_s C \in  X_1 A\in  X_s+X_1 B_k C_k B_k,C_k B_k\in X_s C_k \in X_1 B_k+C_k\to A \mathbf{Attempt} \mathbf{two:} A_k\to A A_k \in  X_s A \varphi(A)=diag(a_1,...,a_u,0,...,0) \varphi \varphi(A_k)\to diag(a_1,...,a_u,0,...,0) [\varphi(A_k)]_{ij}\to \delta_{i,j,\leq u} a_i \delta_{i,j,\leq u} = 1 i=j\leq u \varphi(A_k) = \begin{pmatrix}
(v_1)_k \\
\vdots \\
(v_s)_k \\
\sum_{i=0}^{s} x^k_{s+1,i} (v_i)_k \\
\vdots \\
\sum_{i=0}^{s} x^k_{m,i} (v_i)_k\\
\end{pmatrix} (v_i)_k\to a_i\cdot e_i 1\leq i\leq s \sum_{i=0}^{s} x^k_{j,i} (v_i)_k \to 0  s+1\leq j\leq m","['linear-algebra', 'general-topology', 'multivariable-calculus', 'metric-spaces']"
54,Showing that a divergence free vector field is curl of a vector potential,Showing that a divergence free vector field is curl of a vector potential,,"(This must have been asked somewhere before but for some reason I could not find an answer) In physics one often meets the criterion that a vector field has zero divergence, e.g. $$\nabla\cdot\mathbf{v}= 0 \quad (1)$$ Based on this one often claim that the vector field can be written as the curl of another field, $$\mathbf{v}=\nabla\times\mathbf{A} \quad (2)$$ since $\nabla\cdot (\nabla \times \mathbf{f})= 0$ for any (smooth) $\mathbf{f}$ . But the implication here is only obvious from (2) -> (1) and not the other way around. I tried the Helmholtz decomposition theorem where the vector field can be decomposed into curl-free and divergence-free components $$\mathbf{v} = \nabla \phi + \nabla\times \mathbf{A}$$ Taking the divergence of this and combining with (1) gives $$\nabla\cdot\mathbf{v} = \nabla \cdot\nabla \phi = 0$$ But I can't see how this would imply $\nabla \phi = 0$ which I guess would thus imply (2). Am I missing something or completely on the wrong track?","(This must have been asked somewhere before but for some reason I could not find an answer) In physics one often meets the criterion that a vector field has zero divergence, e.g. Based on this one often claim that the vector field can be written as the curl of another field, since for any (smooth) . But the implication here is only obvious from (2) -> (1) and not the other way around. I tried the Helmholtz decomposition theorem where the vector field can be decomposed into curl-free and divergence-free components Taking the divergence of this and combining with (1) gives But I can't see how this would imply which I guess would thus imply (2). Am I missing something or completely on the wrong track?",\nabla\cdot\mathbf{v}= 0 \quad (1) \mathbf{v}=\nabla\times\mathbf{A} \quad (2) \nabla\cdot (\nabla \times \mathbf{f})= 0 \mathbf{f} \mathbf{v} = \nabla \phi + \nabla\times \mathbf{A} \nabla\cdot\mathbf{v} = \nabla \cdot\nabla \phi = 0 \nabla \phi = 0,['multivariable-calculus']
55,"Is $f(x,y,z)=x^2y^2z^2\sin(\frac{1}{xyz})$ when $xyz\neq 0$ and $0$ else differentiable everywhere?",Is  when  and  else differentiable everywhere?,"f(x,y,z)=x^2y^2z^2\sin(\frac{1}{xyz}) xyz\neq 0 0","Is $$f(x,y,z)=\begin{cases} x^2y^2z^2\sin\left(\frac{1}{xyz}\right), & xyz\neq 0\\ 0, &\text{else}\end{cases}$$ differentiable everywhere? Here is my proof. If $x,y,z\neq0$ then $f$ is differentiable as a composition of differentiable functions. if $x=0$ and $y=u,z=v$ then: \begin{align*} \lim_{(x,y,z)\to(0,u,v)} \left|\frac{x^2y^2z^2\sin(\frac{1}{xyz})}{\sqrt{x^2+(y-u)^2+(z-v)^2}}\right|&\leq \lim_{(x,y,z)\to(0,u,v)} |y^2z^2|\left|\frac{x^2}{\sqrt{x^2+(y-u)^2+(z-v)^2}}\right|\\[5pt] &\leq u^2v^2 \lim_{(x,y,z)\to(0,u,v)} \left|\frac{x^2}{\sqrt{x^2}}\right|\\ &=0 \end{align*} and so $f=0+o(|(x,y,z)-(0,u,v)|)$ so $Df=0$ and $f$ is differentiable. In the same way we can show that if $x,y=0,z=u\neq0$ then $f$ is differentiable at $(0,0,u)$ with differential $0$ . Also at $(x,y,z)=(0,0,0)$ the function over $|(x,y,z)-(0,0,0)|$ is bounded by $C\cdot r^5$ and so its differentiable at $(0,0,0)$ ans all in all its differentiable everywhere. Is my solution or am I missing something? I think I am correct, although my multivariable calculus is a bit rusty hence the post.","Is differentiable everywhere? Here is my proof. If then is differentiable as a composition of differentiable functions. if and then: and so so and is differentiable. In the same way we can show that if then is differentiable at with differential . Also at the function over is bounded by and so its differentiable at ans all in all its differentiable everywhere. Is my solution or am I missing something? I think I am correct, although my multivariable calculus is a bit rusty hence the post.","f(x,y,z)=\begin{cases}
x^2y^2z^2\sin\left(\frac{1}{xyz}\right), & xyz\neq 0\\
0, &\text{else}\end{cases} x,y,z\neq0 f x=0 y=u,z=v \begin{align*}
\lim_{(x,y,z)\to(0,u,v)} \left|\frac{x^2y^2z^2\sin(\frac{1}{xyz})}{\sqrt{x^2+(y-u)^2+(z-v)^2}}\right|&\leq \lim_{(x,y,z)\to(0,u,v)} |y^2z^2|\left|\frac{x^2}{\sqrt{x^2+(y-u)^2+(z-v)^2}}\right|\\[5pt]
&\leq u^2v^2 \lim_{(x,y,z)\to(0,u,v)} \left|\frac{x^2}{\sqrt{x^2}}\right|\\
&=0
\end{align*} f=0+o(|(x,y,z)-(0,u,v)|) Df=0 f x,y=0,z=u\neq0 f (0,0,u) 0 (x,y,z)=(0,0,0) |(x,y,z)-(0,0,0)| C\cdot r^5 (0,0,0)","['limits', 'multivariable-calculus', 'derivatives', 'solution-verification']"
56,partial derivative with respect to a vector?,partial derivative with respect to a vector?,,"I encountered what I can only understand as a partial derivative with respect to a vector, used in a taylor series expansion, and would like to understand it better. In the book I'm reading, $x(t)$ is an $n$ -dimensional real vector, that is $x: \mathbb{R} \to \mathbb{R}^n$ , and $u(t)$ is a real valued function, $t\in \mathbb{R}$ . We have the equation $$ \dot{x} = f(x, u) $$ The author writes that $f$ can be linearized around a stationary point, that is a point $(x_0, u_0)$ such that $f(x_0, u_0) = 0$ : By looking at small deviations from $x_0$ and $u_0$ $$ x = x_0 + \Delta x\\   u = u_0 + \Delta u  $$ we get with taylor expansion $$ \dot{x} = \Delta \dot{x} = f(x_0 + \Delta x, u_0 + \Delta u) \approx \\ f(x_0, u_0) + f_x(x_0, u_0)\Delta x + f_u(x_0, u_0)\Delta u $$ where higher order terms in $\Delta x$ and $\Delta u$ are discarded. $f_x, f_u, h_x$ and $h_u$ denotes partial derivatives with respect to $x$ and $u$ . $f_x$ is a $n \times n$ matrix who's $i, j$ element is $$ \dfrac{\partial}{\partial x_j}f_i(x, u) $$ where $f_i$ is the i:th row in $f$ , and the corresponding is true for $f_u$ . I have never seen this type of partial derivative before, $f_x$ is a partial derivative with respect to $x$ and $x$ is a vector. I've only seen partial derivatives with respect to a real variable before, and the wikipedia article on partial derivatives does not seem to mention it. Not surprisingly I've also never seen this kind of taylor expansion, the wikipedia article does not seem to use partial derivatives with respect to vectors, but I've never even worked with taylor series of several variables so I might be misunderstanding something. I can't find much information about this, am I right that this is a special kind of partial derivatives and taylor series, and in that case does anyone know of some relatively easy to understand source of information?","I encountered what I can only understand as a partial derivative with respect to a vector, used in a taylor series expansion, and would like to understand it better. In the book I'm reading, is an -dimensional real vector, that is , and is a real valued function, . We have the equation The author writes that can be linearized around a stationary point, that is a point such that : By looking at small deviations from and we get with taylor expansion where higher order terms in and are discarded. and denotes partial derivatives with respect to and . is a matrix who's element is where is the i:th row in , and the corresponding is true for . I have never seen this type of partial derivative before, is a partial derivative with respect to and is a vector. I've only seen partial derivatives with respect to a real variable before, and the wikipedia article on partial derivatives does not seem to mention it. Not surprisingly I've also never seen this kind of taylor expansion, the wikipedia article does not seem to use partial derivatives with respect to vectors, but I've never even worked with taylor series of several variables so I might be misunderstanding something. I can't find much information about this, am I right that this is a special kind of partial derivatives and taylor series, and in that case does anyone know of some relatively easy to understand source of information?","x(t) n x: \mathbb{R} \to \mathbb{R}^n u(t) t\in \mathbb{R} 
\dot{x} = f(x, u)
 f (x_0, u_0) f(x_0, u_0) = 0 x_0 u_0 
x = x_0 + \Delta x\\  
u = u_0 + \Delta u
  
\dot{x} = \Delta \dot{x} = f(x_0 + \Delta x, u_0 + \Delta u) \approx \\
f(x_0, u_0) + f_x(x_0, u_0)\Delta x + f_u(x_0, u_0)\Delta u
 \Delta x \Delta u f_x, f_u, h_x h_u x u f_x n \times n i, j 
\dfrac{\partial}{\partial x_j}f_i(x, u)
 f_i f f_u f_x x x",['multivariable-calculus']
57,Is maximum of sum exists if each function has a maximum?,Is maximum of sum exists if each function has a maximum?,,"Let $f,g:D \rightarrow \mathbb R$ be multivariable functions over the open set $D$ . Assume that $f,g$ both have a global maximum point inside $D$ . Can we be sure that $f+g$ also has a global maximum point inside $D$ ? I know that the maximum point if exists must satisfying $\max(f+g) \leq \max f + \max g$ . But will the maximum point exist in general? If not, is there any condition that is needed for the existence of the global maximum?","Let be multivariable functions over the open set . Assume that both have a global maximum point inside . Can we be sure that also has a global maximum point inside ? I know that the maximum point if exists must satisfying . But will the maximum point exist in general? If not, is there any condition that is needed for the existence of the global maximum?","f,g:D \rightarrow \mathbb R D f,g D f+g D \max(f+g) \leq \max f + \max g","['analysis', 'multivariable-calculus', 'optimization', 'maxima-minima']"
58,For matrices (why) is it true that $\left\Vert P^{-1}MP\right\Vert \le\left\Vert P\right\Vert ^{-1}\left\Vert M\right\Vert \left\Vert P\right\Vert ?$,For matrices (why) is it true that,\left\Vert P^{-1}MP\right\Vert \le\left\Vert P\right\Vert ^{-1}\left\Vert M\right\Vert \left\Vert P\right\Vert ?,"I'm assuming the answer is embarrassingly obvious, but I'm not seeing it.  In this context the linear mapping and matrix norms are treated as effectively synonymous.  The discussion pertains to real numbers. Is it the case in general that, given a non-singular matrix $M$ and a conforming diagonal matrix $P$ with all diagonal components positive, we have $$ \left\Vert P^{-1}MP\right\Vert \le\left\Vert P\right\Vert ^{-1}\left\Vert M\right\Vert \left\Vert P\right\Vert ? $$ Notice that I have written $\left\Vert P\right\Vert ^{-1},$ and not $\left\Vert P^{-1}\right\Vert .$ This is what I have so far: Denote by $p_{i}$ the diagonal components of $P.$ Evidently we have $$ \left\Vert P\right\Vert =max\left(p_{i}\right), $$ and $$ \left\Vert P^{-1}\right\Vert =\frac{1}{min\left(p_{i}\right)}. $$ Context: From https://www.scribd.com/read/282634061/Advanced-Calculus-of-Several-Variables In the proof of Theorem IV 5.4, near the end we encounter the following: Write the origin-centered n-cube of radius $1$ as $\mathcal{C}_{1}\subset\mathbb{R}^{n};$ the volume function as $\mathit{v}:\mathbb{R}^{n}\to\mathbb{R};$ the identity mapping as $I;$ and the linear mapping and matrix norms as $\left\Vert \dots\right\Vert .$ Let there be a non-negligible interval $\mathcal{Q}\subset\mathbb{R}^{n}$ centered on $\mathbf{a}\in\mathbb{R}^{n};$ a mapping $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ which is $\mathscr{C}^{1}$ -invertible on a neighborhood containing $\mathcal{Q};$ a linear transformation $\rho:\mathbb{R}^{n}\to\mathbb{R}^{n}$ such that $\rho\left(\mathcal{C}_{1}\right)$ is an origin-centered interval congruent with $\mathcal{Q};$ a translation $\tau_{\mathbf{a}}\circ\rho\left(\mathcal{C}_{1}\right)=\mathcal{Q};$ and a composite mapping $S=T\circ\tau_{\mathbf{a}}\circ\rho.$ In addition, it is given that $$ \mathbf{y}=\tau_{\mathbf{a}}\left(\mathbf{x}\right)\in\mathcal{Q}\implies\left\Vert dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}-I\right\Vert \le\epsilon\in\left(0,1\right). $$ So, by the chain rule and the linearity of $\rho$ we have $$ dS_{\mathbf{0}}^{-1}\circ dS_{\mathbf{x}}=\left(dT_{\mathbf{a}}\circ\rho\right)^{-1}\circ\left(dT_{\mathbf{y}}\circ\rho\right)=\rho^{-1}\circ dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}\circ\rho. $$ Therefore $$ \left\Vert dS_{\mathbf{0}}^{-1}\circ dS_{\mathbf{x}}-I\right\Vert \le\left\Vert \rho\right\Vert ^{-1}\left\Vert dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}-I\right\Vert \left\Vert \rho\right\Vert . $$ It's the last part that I'm not following. The part I do (believe I) understand is $$ dS_{\mathbf{0}}^{-1}\circ dS_{\mathbf{x}}-I=\rho^{-1}\circ dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}\circ\rho-I $$ $$ =\rho^{-1}\circ\left(dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}-I\right)\circ\rho. $$ The norms I am using: Just for fun, this is a partial depiction of what is going on in the proof.","I'm assuming the answer is embarrassingly obvious, but I'm not seeing it.  In this context the linear mapping and matrix norms are treated as effectively synonymous.  The discussion pertains to real numbers. Is it the case in general that, given a non-singular matrix and a conforming diagonal matrix with all diagonal components positive, we have Notice that I have written and not This is what I have so far: Denote by the diagonal components of Evidently we have and Context: From https://www.scribd.com/read/282634061/Advanced-Calculus-of-Several-Variables In the proof of Theorem IV 5.4, near the end we encounter the following: Write the origin-centered n-cube of radius as the volume function as the identity mapping as and the linear mapping and matrix norms as Let there be a non-negligible interval centered on a mapping which is -invertible on a neighborhood containing a linear transformation such that is an origin-centered interval congruent with a translation and a composite mapping In addition, it is given that So, by the chain rule and the linearity of we have Therefore It's the last part that I'm not following. The part I do (believe I) understand is The norms I am using: Just for fun, this is a partial depiction of what is going on in the proof.","M P 
\left\Vert P^{-1}MP\right\Vert \le\left\Vert P\right\Vert ^{-1}\left\Vert M\right\Vert \left\Vert P\right\Vert ?
 \left\Vert P\right\Vert ^{-1}, \left\Vert P^{-1}\right\Vert . p_{i} P. 
\left\Vert P\right\Vert =max\left(p_{i}\right),
 
\left\Vert P^{-1}\right\Vert =\frac{1}{min\left(p_{i}\right)}.
 1 \mathcal{C}_{1}\subset\mathbb{R}^{n}; \mathit{v}:\mathbb{R}^{n}\to\mathbb{R}; I; \left\Vert \dots\right\Vert . \mathcal{Q}\subset\mathbb{R}^{n} \mathbf{a}\in\mathbb{R}^{n}; T:\mathbb{R}^{n}\to\mathbb{R}^{n} \mathscr{C}^{1} \mathcal{Q}; \rho:\mathbb{R}^{n}\to\mathbb{R}^{n} \rho\left(\mathcal{C}_{1}\right) \mathcal{Q}; \tau_{\mathbf{a}}\circ\rho\left(\mathcal{C}_{1}\right)=\mathcal{Q}; S=T\circ\tau_{\mathbf{a}}\circ\rho. 
\mathbf{y}=\tau_{\mathbf{a}}\left(\mathbf{x}\right)\in\mathcal{Q}\implies\left\Vert dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}-I\right\Vert \le\epsilon\in\left(0,1\right).
 \rho 
dS_{\mathbf{0}}^{-1}\circ dS_{\mathbf{x}}=\left(dT_{\mathbf{a}}\circ\rho\right)^{-1}\circ\left(dT_{\mathbf{y}}\circ\rho\right)=\rho^{-1}\circ dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}\circ\rho.
 
\left\Vert dS_{\mathbf{0}}^{-1}\circ dS_{\mathbf{x}}-I\right\Vert \le\left\Vert \rho\right\Vert ^{-1}\left\Vert dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}-I\right\Vert \left\Vert \rho\right\Vert .
 
dS_{\mathbf{0}}^{-1}\circ dS_{\mathbf{x}}-I=\rho^{-1}\circ dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}\circ\rho-I
 
=\rho^{-1}\circ\left(dT_{\mathbf{a}}^{-1}\circ dT_{\mathbf{y}}-I\right)\circ\rho.
","['linear-algebra', 'multivariable-calculus', 'proof-explanation', 'normed-spaces']"
59,Area of the section enclosed by the curve,Area of the section enclosed by the curve,,"Curve is formed by the intersection of the surface: $\displaystyle \frac{1}{x} + \frac{1}{y} + \frac{1}{z} = 0$ with the plane $2x + 2y + z = 1$ What is the area enclosed by this curve? Eliminating z from both equations, I get an implicit equation relating $x$ and y, with terms in $x^2, y^2, xy, x,$ and $y$ . But I don't know where to go from there.","Curve is formed by the intersection of the surface: with the plane What is the area enclosed by this curve? Eliminating z from both equations, I get an implicit equation relating and y, with terms in and . But I don't know where to go from there.","\displaystyle \frac{1}{x} + \frac{1}{y} + \frac{1}{z} = 0 2x + 2y + z = 1 x x^2, y^2, xy, x, y","['calculus', 'integration', 'multivariable-calculus']"
60,Show that $x^{2}\frac{\partial^{2}u}{\partial x^{2}}+2xy\frac{\partial^{2}u}{\partial x\partial y}+y^{2}\frac{\partial^{2}u}{\partial y^{2}}=0$,Show that,x^{2}\frac{\partial^{2}u}{\partial x^{2}}+2xy\frac{\partial^{2}u}{\partial x\partial y}+y^{2}\frac{\partial^{2}u}{\partial y^{2}}=0,"I want to prove that $x^{2}\frac{\partial^{2}u}{\partial x^{2}}+2xy\frac{\partial^{2}u}{\partial x\partial y}+y^{2}\frac{\partial^{2}u}{\partial y^{2}}=0$ where $u(x,y)=\frac{xy}{x+y}$ . I know that it could be done by calculating each partial derivative but I realized that $x^{2}\frac{\partial^{2}u}{\partial x^{2}}+2xy\frac{\partial^{2}u}{\partial x\partial y}+y^{2}\frac{\partial^{2}u}{\partial y^{2}}=[(x,y)\cdot \nabla]^{2}u$ . So my question is if there exists some result that I can use to prove in an easy way that the equality holds.",I want to prove that where . I know that it could be done by calculating each partial derivative but I realized that . So my question is if there exists some result that I can use to prove in an easy way that the equality holds.,"x^{2}\frac{\partial^{2}u}{\partial x^{2}}+2xy\frac{\partial^{2}u}{\partial x\partial y}+y^{2}\frac{\partial^{2}u}{\partial y^{2}}=0 u(x,y)=\frac{xy}{x+y} x^{2}\frac{\partial^{2}u}{\partial x^{2}}+2xy\frac{\partial^{2}u}{\partial x\partial y}+y^{2}\frac{\partial^{2}u}{\partial y^{2}}=[(x,y)\cdot \nabla]^{2}u",['multivariable-calculus']
61,"Find the volume of the solid in the first octant bounded by the three surfaces $z = 1-y^2$, $y=2x$, and $x=3$","Find the volume of the solid in the first octant bounded by the three surfaces , , and",z = 1-y^2 y=2x x=3,"I want to find the volume of the solid in the first octant bounded by the three surfaces $z = 1-y^2$ , $y=2x$ , and $x=3$ . It seems that would simply be to calculate the following triple integral: $\int_0^3 \int_0^{2x} \int_0^{1-y^2} z\,dz\,dy\,dx$ This is pretty straight-forward to do without any variable substitutions etc. which makes me think it's almost too simple (for a home assignment). Am I missing something or is the above correct?","I want to find the volume of the solid in the first octant bounded by the three surfaces , , and . It seems that would simply be to calculate the following triple integral: This is pretty straight-forward to do without any variable substitutions etc. which makes me think it's almost too simple (for a home assignment). Am I missing something or is the above correct?","z = 1-y^2 y=2x x=3 \int_0^3 \int_0^{2x} \int_0^{1-y^2} z\,dz\,dy\,dx","['integration', 'multivariable-calculus']"
62,Making sense of matrix calculus involving matrices,Making sense of matrix calculus involving matrices,,"In matrix calculus, we can calculate scalar by matrix derivative or matrix by scalar derivatives, according to here . Where do these formulae come from? What I'm confused about is how this is related to ordinary multi-variable differentiation? For example, given a matrix to scalar function $f: \mathbb{R}^{n \times m} \to \mathbb{R}$ , in what sense is the result a $m \times n$ matrix? Is this related to treating matrices as vectors of dimension $n \times m$ , using matrix norm to measure distance on the vector space of matrices? However, shouldn't the resulting derivative linear map of type $\mathbb{R}^{n \times m} \to \mathbb{R}$ ? Why is it $\mathbb{R}^m \to \mathbb{R}^n$ ? A detailed derivation of the formulae would be helpful!","In matrix calculus, we can calculate scalar by matrix derivative or matrix by scalar derivatives, according to here . Where do these formulae come from? What I'm confused about is how this is related to ordinary multi-variable differentiation? For example, given a matrix to scalar function , in what sense is the result a matrix? Is this related to treating matrices as vectors of dimension , using matrix norm to measure distance on the vector space of matrices? However, shouldn't the resulting derivative linear map of type ? Why is it ? A detailed derivation of the formulae would be helpful!",f: \mathbb{R}^{n \times m} \to \mathbb{R} m \times n n \times m \mathbb{R}^{n \times m} \to \mathbb{R} \mathbb{R}^m \to \mathbb{R}^n,"['calculus', 'multivariable-calculus', 'matrix-calculus']"
63,Show that the image of a cube is almost a cube,Show that the image of a cube is almost a cube,,"Let $C_r = \left \{ x \in \mathbb{R}^n : |x^i| < r \forall 1 \leq i \leq n \right \}$ and $ g \in C^1(U, \mathbb{R}^n)$ for some open $\left \{ 0 \right \} \subset U$ s.t $dg(\vec{0}) = I, g(\vec{0})=\vec{0}$ . and let us choose some $0 < \varepsilon < 1$ . Show that there exists $\delta > 0$ s.t $\forall r < \delta, C_{(1-\varepsilon)r} \subset g(C_{r}) \subset C_{(1+\varepsilon)r}$ I couldn't think of a better title, edits are welcome. This is what I tried: $g(0+\triangle x) - g(0) = dg(0)(\triangle x) +o(\triangle x) \implies g(x) = x+o(x) \implies \frac{||g(x) - x||}{||x||} \xrightarrow[x \to 0]{} 0$ So we can choose some $\delta > 0$ s.t if $||x|| < \delta$ then $||g(x)-x|| < ||x||_{\infty} \varepsilon$ Now $x \in C_r \implies |x^i|<r \implies |g(x)^i| < r+ ||x||_{\infty}\varepsilon \leq r+r \varepsilon \implies g(C_{r}) \subset C_{(1+\varepsilon)r}$ But i'm not sure how to show $C_{(1-\varepsilon)r} \subset g(C_{r})$ . Hints appreciated. Also, does what I did so far seem correct?","Let and for some open s.t . and let us choose some . Show that there exists s.t I couldn't think of a better title, edits are welcome. This is what I tried: So we can choose some s.t if then Now But i'm not sure how to show . Hints appreciated. Also, does what I did so far seem correct?","C_r = \left \{ x \in \mathbb{R}^n : |x^i| < r \forall 1 \leq i \leq n \right \}  g \in C^1(U, \mathbb{R}^n) \left \{ 0 \right \} \subset U dg(\vec{0}) = I, g(\vec{0})=\vec{0} 0 < \varepsilon < 1 \delta > 0 \forall r < \delta, C_{(1-\varepsilon)r} \subset g(C_{r}) \subset C_{(1+\varepsilon)r} g(0+\triangle x) - g(0) = dg(0)(\triangle x) +o(\triangle x) \implies g(x) = x+o(x) \implies \frac{||g(x) - x||}{||x||} \xrightarrow[x \to 0]{} 0 \delta > 0 ||x|| < \delta ||g(x)-x|| < ||x||_{\infty} \varepsilon x \in C_r \implies |x^i|<r \implies |g(x)^i| < r+ ||x||_{\infty}\varepsilon \leq r+r \varepsilon \implies g(C_{r}) \subset C_{(1+\varepsilon)r} C_{(1-\varepsilon)r} \subset g(C_{r})",['multivariable-calculus']
64,Composite function gradient,Composite function gradient,,"Suppose I have smooth maps $g:R^m \rightarrow R^n$ and $f:R^n \rightarrow R$ . Then I think $$\nabla (f \circ g) = (\nabla g)^T (\nabla f) \circ g,$$ where $\nabla g$ is the $n \times m$ Jacobian matrix. This is the only plausible answer that makes the dimension work out, and if I use row vectors instead of columns, it nicely resembles the univariate chain rule: $$\nabla (f \circ g) = ((\nabla f) \circ g) (\nabla g).$$ But can anyone prove it? And is there some way this can work with column vectors that doesn't involve transposing the Jacobian? Thanks!","Suppose I have smooth maps and . Then I think where is the Jacobian matrix. This is the only plausible answer that makes the dimension work out, and if I use row vectors instead of columns, it nicely resembles the univariate chain rule: But can anyone prove it? And is there some way this can work with column vectors that doesn't involve transposing the Jacobian? Thanks!","g:R^m \rightarrow R^n f:R^n \rightarrow R \nabla (f \circ g) = (\nabla g)^T (\nabla f) \circ g, \nabla g n \times m \nabla (f \circ g) = ((\nabla f) \circ g) (\nabla g).","['multivariable-calculus', 'vector-analysis']"
65,"What is meant by ""a curve $r(t)$ of constant length $|r(t)|$""?","What is meant by ""a curve  of constant length ""?",r(t) |r(t)|,"I am working on an assignment for Calculus III. This particular one has to do with some vector calculus, but I am caught on some wording by my professor and he is not holding office hours at the moment. In a few questions he asks us to show certain properties of ""a curve $r(t)$ with constant length $|r(t)|$ ."" I have been operating under the assumption that this means any curve with a scalar as its length, such as $\langle 0,0,t\rangle$ over $0\le t\le 1$ , but am now questioning that assumption, as he asks us to show a property is true ""for all of $t$ ."" Does anyone have any ideas what he means here? I have been working these by creating examples with small bounds such as the one above for simple integration for the lengths.","I am working on an assignment for Calculus III. This particular one has to do with some vector calculus, but I am caught on some wording by my professor and he is not holding office hours at the moment. In a few questions he asks us to show certain properties of ""a curve with constant length ."" I have been operating under the assumption that this means any curve with a scalar as its length, such as over , but am now questioning that assumption, as he asks us to show a property is true ""for all of ."" Does anyone have any ideas what he means here? I have been working these by creating examples with small bounds such as the one above for simple integration for the lengths.","r(t) |r(t)| \langle 0,0,t\rangle 0\le t\le 1 t","['multivariable-calculus', 'vectors']"
66,compute line integral using Greens theorem,compute line integral using Greens theorem,,"The question is: $$\int_\gamma \frac{(x^2+y^2-2)\ dx+(4y-x^2-y^2-2)\ dy}{x^2+y^2-2x-2y+2}$$ $$\gamma:y=2\sin\frac{\pi x}{2} \quad \text{from}\  (2,0)\ \text{to}\ (0,0)$$ Here how i have tried to solve it: I thought that due to the singularity at $(1,1)$ it would be best to use Greens theorem with one circle around the singularity call it $\sigma$ and the line segment between $(0,0)$ to $(2,0)$ and call it $\varphi$ and then $\int_\gamma=-\int_\sigma-\int_\varphi$ but the problem is that the first integral (circle around the point $(1,1)$ ) is very complicated with $x=1+Rcost$ and $y=1+Rsint$ . Do any of you guys have any suggestion or am i on the right track at all?",The question is: Here how i have tried to solve it: I thought that due to the singularity at it would be best to use Greens theorem with one circle around the singularity call it and the line segment between to and call it and then but the problem is that the first integral (circle around the point ) is very complicated with and . Do any of you guys have any suggestion or am i on the right track at all?,"\int_\gamma \frac{(x^2+y^2-2)\ dx+(4y-x^2-y^2-2)\ dy}{x^2+y^2-2x-2y+2} \gamma:y=2\sin\frac{\pi x}{2} \quad \text{from}\  (2,0)\ \text{to}\ (0,0) (1,1) \sigma (0,0) (2,0) \varphi \int_\gamma=-\int_\sigma-\int_\varphi (1,1) x=1+Rcost y=1+Rsint","['integration', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
67,Application of implicit theorem,Application of implicit theorem,,"Find conditions on the function $f$ and $g$ which permit you to solve the equations $$f(xy)+g(yz)=0\ \ \textrm{and} \ \ g(xy)+f(yz)=0 $$ for $y$ and $z$ as functions of $x$ , near the point $x=y=z=1$ and $f(1)=g(1)=0$ . Attempt: This problem seems to be an application of the implicit function theorem. Usually on this kind of problems we define special transformations, but I don't now how to define it in this case since I have several variables $x,y,z$ and two functions $f$ and $g$ . Any idea on how to start?","Find conditions on the function and which permit you to solve the equations for and as functions of , near the point and . Attempt: This problem seems to be an application of the implicit function theorem. Usually on this kind of problems we define special transformations, but I don't now how to define it in this case since I have several variables and two functions and . Any idea on how to start?","f g f(xy)+g(yz)=0\ \ \textrm{and} \ \ g(xy)+f(yz)=0  y z x x=y=z=1 f(1)=g(1)=0 x,y,z f g",['multivariable-calculus']
68,Computing a difficult integral,Computing a difficult integral,,"This is actually a follow-up question to what I posted here: Computing double integral for expected value . I think that the result in the previous post may be helpful in computing this integral. The result in the previous post was: $$\frac{\int_{\mathbb{R}^{2}}\frac{p^2}{2} e^{-\beta (V(x) + p^2/2)}\mathop{dx}\mathop{dp}}{\int_{\mathbb{R}^{2}} e^{-\beta (V(x) + p^2/2)} \mathop{dx}\mathop{dp}} = (k_BT)^{-1},$$ where $\beta = (k_BT)^{-1}$ . So we can use this result if it makes the following integral easier. I would now like to compute the following: $$\frac{\int_{\mathbb{R}^{6n}} \frac{p^2}{2} e^{-\beta(V(x) + \frac{1}{2} \sum_{i = 1}^{3n} p_i^2)} \mathop{dx dp}}{\int_{\mathbb{R}^{6n}} e^{-\beta(V(x) + \frac{1}{2}\sum_{i = 1}^{3n} p_i^2)} \mathop{dx dp}},$$ where $V$ is some unknown function of $x$ . The answer should be $3nk_BT/2 = \frac{3n}{2\beta}$ , but I'm really not sure about how to show this result. Any help is appreciated","This is actually a follow-up question to what I posted here: Computing double integral for expected value . I think that the result in the previous post may be helpful in computing this integral. The result in the previous post was: where . So we can use this result if it makes the following integral easier. I would now like to compute the following: where is some unknown function of . The answer should be , but I'm really not sure about how to show this result. Any help is appreciated","\frac{\int_{\mathbb{R}^{2}}\frac{p^2}{2} e^{-\beta (V(x) + p^2/2)}\mathop{dx}\mathop{dp}}{\int_{\mathbb{R}^{2}} e^{-\beta (V(x) + p^2/2)} \mathop{dx}\mathop{dp}} = (k_BT)^{-1}, \beta = (k_BT)^{-1} \frac{\int_{\mathbb{R}^{6n}} \frac{p^2}{2} e^{-\beta(V(x) + \frac{1}{2} \sum_{i = 1}^{3n} p_i^2)} \mathop{dx dp}}{\int_{\mathbb{R}^{6n}} e^{-\beta(V(x) + \frac{1}{2}\sum_{i = 1}^{3n} p_i^2)} \mathop{dx dp}}, V x 3nk_BT/2 = \frac{3n}{2\beta}","['calculus', 'integration', 'multivariable-calculus']"
69,How to show that $D$ is a Borel measurable set and $D_{f}$ is a Borel function.,How to show that  is a Borel measurable set and  is a Borel function.,D D_{f},"How to show that $D_{f}$ is a Borel measurable function. Well I have one Lipschitz function $f:\Bbb{R}^{n}\to \Bbb{R}$ and I want to proof that $D_{f}:D\to L(\Bbb{R}^{n},\Bbb{R})$ is Borel function, where $D=\{ x\in \Bbb{R}^{n}: f'(x)\quad \text{exist }\}$ I try with the definition to show that $\forall U$ Borel set in $ L(\Bbb{R}^{n},\Bbb{R})$ imply $D_{f}^{-1}(U)$ is Borel set. Then let $U$ Borel set in $L(\Bbb{R}^{n},\Bbb{R})$ hence $D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: D_{f}(x)\in U\}$ but $D_{f}(x)$ is one linear transformation  hence $D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: T(x)\in U \}$ can to say : Like $T$ is continuous because is linear transformation then $D_{f}^{-1}(U)$ is measurable imply is Borel set?, can somebody help me please or give me one hint...thank you","How to show that is a Borel measurable function. Well I have one Lipschitz function and I want to proof that is Borel function, where I try with the definition to show that Borel set in imply is Borel set. Then let Borel set in hence but is one linear transformation  hence can to say : Like is continuous because is linear transformation then is measurable imply is Borel set?, can somebody help me please or give me one hint...thank you","D_{f} f:\Bbb{R}^{n}\to \Bbb{R} D_{f}:D\to L(\Bbb{R}^{n},\Bbb{R}) D=\{ x\in \Bbb{R}^{n}: f'(x)\quad \text{exist }\} \forall U  L(\Bbb{R}^{n},\Bbb{R}) D_{f}^{-1}(U) U L(\Bbb{R}^{n},\Bbb{R}) D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: D_{f}(x)\in U\} D_{f}(x) D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: T(x)\in U \} T D_{f}^{-1}(U)","['linear-algebra', 'measure-theory', 'multivariable-calculus', 'geometric-measure-theory', 'measurable-functions']"
70,"Show that $f(x,y) = \frac{x^2y^2}{x^2+y^2}$ is (totally) differentiable",Show that  is (totally) differentiable,"f(x,y) = \frac{x^2y^2}{x^2+y^2}","I want to prove that $f(x,y) = \frac{x^2y^2}{x^2+y^2} , (f(0,0) = 0)$ is (totally) differentiable at $ \begin{pmatrix}  0 \\ 0 \end{pmatrix}$ . I want to use the criterion that a function which is continuously partially differentiable is differentiable. So I compute $\frac{\partial f}{\partial x}=\frac{2xy^4}{(x^2+y^2)^2}$ (The partial derivative $\frac{\partial f}{\partial y}$ works analogously). Going back to the definition of partial deriviative with respect to the $x$ direction, we see that $\frac{\partial f}{\partial x}(0,0)=0$ . So it remains to show that $\lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial x}(x,y) = 0.$ I use the $\epsilon-\delta$ criterion: Let $\epsilon >0 $ . Let $\delta := \frac{1}{2} \epsilon$ . So let $d((x,y),(0,0)) = \sqrt{x^2+y^2}< \delta$ , say $\sqrt{x^2+y^2} = \delta'<\delta.$ This implies $|x|\leq\delta'<\delta, |y|\leq\delta'<\delta$ . Therefore: $|\frac{\partial f}{\partial x}(x,y)| \leq \frac{2(\delta')^5}{(\delta')^4} = 2\delta'<2\delta=\epsilon$ , as desired. Is this analysis correct?","I want to prove that is (totally) differentiable at . I want to use the criterion that a function which is continuously partially differentiable is differentiable. So I compute (The partial derivative works analogously). Going back to the definition of partial deriviative with respect to the direction, we see that . So it remains to show that I use the criterion: Let . Let . So let , say This implies . Therefore: , as desired. Is this analysis correct?","f(x,y) = \frac{x^2y^2}{x^2+y^2} , (f(0,0) = 0)  \begin{pmatrix}  0 \\ 0 \end{pmatrix} \frac{\partial f}{\partial x}=\frac{2xy^4}{(x^2+y^2)^2} \frac{\partial f}{\partial y} x \frac{\partial f}{\partial x}(0,0)=0 \lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial x}(x,y) = 0. \epsilon-\delta \epsilon >0  \delta := \frac{1}{2} \epsilon d((x,y),(0,0)) = \sqrt{x^2+y^2}< \delta \sqrt{x^2+y^2} = \delta'<\delta. |x|\leq\delta'<\delta, |y|\leq\delta'<\delta |\frac{\partial f}{\partial x}(x,y)| \leq \frac{2(\delta')^5}{(\delta')^4} = 2\delta'<2\delta=\epsilon","['multivariable-calculus', 'derivatives', 'partial-derivative', 'rational-functions']"
71,How to prove differentiability of the following unknown function,How to prove differentiability of the following unknown function,,"We have $f:\Bbb{R}^n\rightarrow \Bbb{R}^m$ s.t $f(x)=o(\|x-x_0\|)$ , and I have to prove $f$ is differentiable at $x_0$ . So by definition, I know that $f$ is differentiable if there exists a linear transformation $L_{x_0}$ s.t $\lim_{x\rightarrow x_0}\frac{f(x)-f(x_0)-L_{x_0}(x-x_0) }{\|x-x_0\|}=0$ . Also I know that $L_{x_0}(x-x_0)$ satisfies $L_{x_0}(x-x_0)\in O(\|x-x_0\|)$ . Any help would be appreciated.","We have s.t , and I have to prove is differentiable at . So by definition, I know that is differentiable if there exists a linear transformation s.t . Also I know that satisfies . Any help would be appreciated.",f:\Bbb{R}^n\rightarrow \Bbb{R}^m f(x)=o(\|x-x_0\|) f x_0 f L_{x_0} \lim_{x\rightarrow x_0}\frac{f(x)-f(x_0)-L_{x_0}(x-x_0) }{\|x-x_0\|}=0 L_{x_0}(x-x_0) L_{x_0}(x-x_0)\in O(\|x-x_0\|),"['real-analysis', 'multivariable-calculus', 'derivatives']"
72,"How to evaluate $\int_{0}^{1}\int_{0}^{1} \sqrt{1 + 4(x^2 + y^2)}\,dx\, dy$?",How to evaluate ?,"\int_{0}^{1}\int_{0}^{1} \sqrt{1 + 4(x^2 + y^2)}\,dx\, dy","I need to solve the integral $$\int_{0}^{1}\int_{0}^{1} \sqrt{1 + 4(x^2 + y^2)}\,dx\,dy$$ I am using polar coordinates here to get : $$ \int_{0} ^{\pi/4}\int_{0}^{\sec \theta} \sqrt{(1 + 4r^2)} r \,dr \,d\theta +  \int_{\pi/4} ^{\pi/2}\int_{0}^{\operatorname{cosec}\theta} \sqrt{(1 + 4r^2)} r \,dr \,d\theta$$ After this integral becomes too complex to solve further . For eg : the first integral gives : $$\int_{0}^{\pi/4}\frac1{12}{((1 + 4\sec^2\theta)^{3/2} - 1)}\, d\theta$$ After this I am stuck how to proceed further, Please help. Thank You.","I need to solve the integral I am using polar coordinates here to get : After this integral becomes too complex to solve further . For eg : the first integral gives : After this I am stuck how to proceed further, Please help. Thank You.","\int_{0}^{1}\int_{0}^{1} \sqrt{1 + 4(x^2 + y^2)}\,dx\,dy  \int_{0} ^{\pi/4}\int_{0}^{\sec \theta} \sqrt{(1 + 4r^2)} r \,dr \,d\theta +  \int_{\pi/4} ^{\pi/2}\int_{0}^{\operatorname{cosec}\theta} \sqrt{(1 + 4r^2)} r \,dr \,d\theta \int_{0}^{\pi/4}\frac1{12}{((1 + 4\sec^2\theta)^{3/2} - 1)}\, d\theta","['integration', 'multivariable-calculus', 'multiple-integral']"
73,Finding Force from acceleration at a point given constant speed,Finding Force from acceleration at a point given constant speed,,"Prompt: An object of mass $m$ travels along a parabola $y = x^2$ with constant speed $5$ units/sec. What is the force on the object due to acceleration at point $(\sqrt{2}, 2)$ ? Recall $\vec{F} = m\vec{a}$ . This was an ""applied theory"" problem I got incorrect on an generated exam I recently took. This question seems to be a similar idea, excepting that I need to work backwards from speed rather than velocity and that speed is constant. Here is what I understand: Since we're working in 2-D, $|\vec{v}(t)| = \sqrt{v_x^2 + v_y^2} = 5 \implies |\vec{v}(t)|^2 = 25 = v_x^2 + v_y^2$ . I don't know if this helps me. The parabola can be reparameterized as a trajectory/displacement curve: $\vec{r}(t) = \langle t, t^2 \rangle$ From the reparameterization, we can differentiate $\frac{d \vec{r}}{dt} = \vec{v}(t) = \langle 1, 2t \rangle$ and once more $\frac{d \vec{v}}{dt} = \vec{a}(t) = \langle 0, 2 \rangle$ . If I compute the supposed magnitude of $\vec{v}$ at point $(\sqrt{2}, 2) \Longleftrightarrow t = \sqrt{2}$ , I find $$ |\vec{v}(t=\sqrt{2})| = \sqrt{1^2 + (2\sqrt{2})^2} = \sqrt{1 + 8} = \sqrt{9} = 3 \neq 5 $$ A contradiction of sorts. Point ( $3$ ) above would lead one to naively assume that acceleration is $2$ units/sec $^2$ upwards for all times $t$ . From there, one could conclude $\vec{F} = m \langle 0, 2 \rangle = \langle 0, 2m \rangle$ . Alas the automated grader reports the correct answer is: $$\displaystyle \vec{F} = \langle-\frac{-100\sqrt{2}}{81}m, \frac{50}{81}m\rangle$$ The connection between 3 (or 9) and 81 is not lost on me ( $3^4 = 9^2 = 81$ ), but I don't see how to bridge the gap. Any help would be greatly appreciated. I've already got my grade and the ""answer"". What I seek now is understanding. Many thanks in advance.","Prompt: An object of mass travels along a parabola with constant speed units/sec. What is the force on the object due to acceleration at point ? Recall . This was an ""applied theory"" problem I got incorrect on an generated exam I recently took. This question seems to be a similar idea, excepting that I need to work backwards from speed rather than velocity and that speed is constant. Here is what I understand: Since we're working in 2-D, . I don't know if this helps me. The parabola can be reparameterized as a trajectory/displacement curve: From the reparameterization, we can differentiate and once more . If I compute the supposed magnitude of at point , I find A contradiction of sorts. Point ( ) above would lead one to naively assume that acceleration is units/sec upwards for all times . From there, one could conclude . Alas the automated grader reports the correct answer is: The connection between 3 (or 9) and 81 is not lost on me ( ), but I don't see how to bridge the gap. Any help would be greatly appreciated. I've already got my grade and the ""answer"". What I seek now is understanding. Many thanks in advance.","m y = x^2 5 (\sqrt{2}, 2) \vec{F} = m\vec{a} |\vec{v}(t)| = \sqrt{v_x^2 + v_y^2} = 5 \implies |\vec{v}(t)|^2 = 25 = v_x^2 + v_y^2 \vec{r}(t) = \langle t, t^2 \rangle \frac{d \vec{r}}{dt} = \vec{v}(t) = \langle 1, 2t \rangle \frac{d \vec{v}}{dt} = \vec{a}(t) = \langle 0, 2 \rangle \vec{v} (\sqrt{2}, 2) \Longleftrightarrow t = \sqrt{2} 
|\vec{v}(t=\sqrt{2})| = \sqrt{1^2 + (2\sqrt{2})^2} = \sqrt{1 + 8} = \sqrt{9} = 3 \neq 5
 3 2 ^2 t \vec{F} = m \langle 0, 2 \rangle = \langle 0, 2m \rangle \displaystyle \vec{F} = \langle-\frac{-100\sqrt{2}}{81}m, \frac{50}{81}m\rangle 3^4 = 9^2 = 81","['multivariable-calculus', 'vectors', 'vector-analysis']"
74,"Finding $\int_{S}^{} x^{4} \sin (x^{3}z^{5})\,dx\,dy\,dz$ where $S$ is part of a sphere",Finding  where  is part of a sphere,"\int_{S}^{} x^{4} \sin (x^{3}z^{5})\,dx\,dy\,dz S","Let $S$ be the subset of the sphere $x^{2} + y^{2} + z^{2} = 1,  z > 0$ . Calculate the integral $$\int_{S}^{} x^{4} \sin (x^{3}z^{5})\,dx\,dy\,dz$$ So I know that this is a surface integral. I used these parameters: $$\boldsymbol{\mathbf{}\Phi} (\varphi ,\theta )=(\sin \varphi \cos \theta, \sin \varphi \sin \theta, \cos \varphi) , 0<\varphi < \frac{\pi}{2}, 0<\theta<2\pi$$ I also found $$\left \| \Phi_{\phi} \times \Phi_{\theta} \right \| = \sin \varphi$$ So I got the double integral $$\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi} \sin^{4}\varphi\cos^{4}\theta \sin(\sin^{3}\varphi \cos^{3}\theta \cos^{5}\varphi)\sin\varphi \,d\varphi \,d\theta $$ but I don't think that it's a good idea.",Let be the subset of the sphere . Calculate the integral So I know that this is a surface integral. I used these parameters: I also found So I got the double integral but I don't think that it's a good idea.,"S x^{2} + y^{2} + z^{2} = 1,  z > 0 \int_{S}^{} x^{4} \sin (x^{3}z^{5})\,dx\,dy\,dz \boldsymbol{\mathbf{}\Phi} (\varphi ,\theta )=(\sin \varphi \cos \theta, \sin \varphi \sin \theta, \cos \varphi) , 0<\varphi < \frac{\pi}{2}, 0<\theta<2\pi \left \| \Phi_{\phi} \times \Phi_{\theta} \right \| = \sin \varphi \int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi} \sin^{4}\varphi\cos^{4}\theta \sin(\sin^{3}\varphi \cos^{3}\theta \cos^{5}\varphi)\sin\varphi \,d\varphi \,d\theta ","['integration', 'multivariable-calculus', 'multiple-integral', 'surface-integrals']"
75,Shortest time to travel around quicksand,Shortest time to travel around quicksand,,"You want to travel from one side of a quicksand $(1,0)$ pit to another side of the quicksand pit $(-1,0)$ . The speed you can run is determined by how far you are away from the quicksand $|v(x,y)| = \sqrt{x^2+y^2}$ . What is the optimal path you should travel such that the time taken to travel is minimized? In my attempted solution: Use cylindrical coordinates $(r,\theta)$ and find an expression for the total time taken $T$ Attempt to minimize $T$ using Beltrami's identity Solve the resulting differential equation to find $r(\theta)$ However, when I do this, I find $r = c_2 e^{c_1 \theta}$ . This is strange because I would have expected a symmetrical path around the y axis. Also, when I plug in the start and end points I get nonsensical values for the constants $c_1$ and $c_2$ . Can someone help me figure out what I'm doing wrong?","You want to travel from one side of a quicksand pit to another side of the quicksand pit . The speed you can run is determined by how far you are away from the quicksand . What is the optimal path you should travel such that the time taken to travel is minimized? In my attempted solution: Use cylindrical coordinates and find an expression for the total time taken Attempt to minimize using Beltrami's identity Solve the resulting differential equation to find However, when I do this, I find . This is strange because I would have expected a symmetrical path around the y axis. Also, when I plug in the start and end points I get nonsensical values for the constants and . Can someone help me figure out what I'm doing wrong?","(1,0) (-1,0) |v(x,y)| = \sqrt{x^2+y^2} (r,\theta) T T r(\theta) r = c_2 e^{c_1 \theta} c_1 c_2","['multivariable-calculus', 'optimization', 'euler-lagrange-equation']"
76,Length of the intersection between a sphere and a cylinder,Length of the intersection between a sphere and a cylinder,,"Note: while writing this question I realized what I was missing so there is no question here, but I thought it is a nice exercise to share, and I'd like to see more solutions. I am trying to calculate the length of the curve given by $$x^2+y^2+z^2=1, x^2+y^2=x$$ The curve is the intersection between a sphere and a cylinder, and we can  notice is is symmetric around the $xy$ and $xz$ planes, so I tried to calculate only the part where $y>0, z>0$ and multiply by 4. For this part I used the parameterization: $\gamma(t)=(t, \sqrt{t-t^2}, \sqrt{1-t}), t \in [0, 1]$ And then the length should be $l=4\int_0^1 ||\gamma'(t)|| dt = 4\int_0^1 \sqrt{1+\frac{(1-2t)^2}{4(t-t^2)} +\frac{1}{4-4t}} dt = 4\int_0^1 \sqrt{\frac{t+1}{4t-4t^2}} dt$ And I'm not sure how to solve this integral but using an online calculator we can see the length is $\approx 7.64$ Different solutions, or suggestions for ways to solve this integral are welcome!","Note: while writing this question I realized what I was missing so there is no question here, but I thought it is a nice exercise to share, and I'd like to see more solutions. I am trying to calculate the length of the curve given by The curve is the intersection between a sphere and a cylinder, and we can  notice is is symmetric around the and planes, so I tried to calculate only the part where and multiply by 4. For this part I used the parameterization: And then the length should be And I'm not sure how to solve this integral but using an online calculator we can see the length is Different solutions, or suggestions for ways to solve this integral are welcome!","x^2+y^2+z^2=1, x^2+y^2=x xy xz y>0, z>0 \gamma(t)=(t, \sqrt{t-t^2}, \sqrt{1-t}), t \in [0, 1] l=4\int_0^1 ||\gamma'(t)|| dt = 4\int_0^1 \sqrt{1+\frac{(1-2t)^2}{4(t-t^2)} +\frac{1}{4-4t}} dt = 4\int_0^1 \sqrt{\frac{t+1}{4t-4t^2}} dt \approx 7.64",['multivariable-calculus']
77,show that the set $ A'=\left\{ x:x\in\partial(A\setminus\{x\}\right\} $is closed,show that the set is closed, A'=\left\{ x:x\in\partial(A\setminus\{x\}\right\} ,"I'm studying Calculus 3, and our teacher tends to give us extremely difficult questions. one of those questions was this: ""let $A \subseteq \Bbb R^k $ . show that the set $ A' = \{ x \in \Bbb R^k\mid x \in \partial(A \setminus \{ x \})\}$ is closed"". I've tried to show using sequence characterization or showing that $(A')^c$ is open, but I don't have a clue of how to start, or even if I'm in the right direction. I could really use some help or a guiding hand with this.","I'm studying Calculus 3, and our teacher tends to give us extremely difficult questions. one of those questions was this: ""let . show that the set is closed"". I've tried to show using sequence characterization or showing that is open, but I don't have a clue of how to start, or even if I'm in the right direction. I could really use some help or a guiding hand with this.",A \subseteq \Bbb R^k   A' = \{ x \in \Bbb R^k\mid x \in \partial(A \setminus \{ x \})\} (A')^c,"['calculus', 'general-topology', 'multivariable-calculus']"
78,"Are the following function 1st derivatives continuous at (0,0)?","Are the following function 1st derivatives continuous at (0,0)?",,"Apologies: I've had to rephrase this question quite a few times Suppose the following function: $$f(x,y)=\frac{2x^3y^2}{x^4+y^2}$$ $$f(0,0)=0$$ Show it is differentiable/not differentiable at $(0,0)$ I attempted this question first by applying the definition of differentiability which yielded the following expression: $$\lim_{(x,y)\to(0,0)}\frac{f(x,y)}{\sqrt{x^2+y^2}} = \lim_{(x,y)\to(0,0)}\frac{2x^3y^2}{(x^4+y^2)\sqrt{x^2+y^2}}$$ If the above equals 0, then the function is infact differentiable at $(0,0)$ . I can't seem to find a counterexample which proves that it is discontinuous so I attempted to prove its continuity by applying the $\epsilon-\delta$ definition of limits. Unfortunately, no matter whether I express the function in terms of polarcoordinates or not, I cannot bound the following $$|\frac{2x^3y^2}{(x^4+y^2)\sqrt{x^2+y^2}}| < \epsilon$$ for all $\epsilon$ . I'd be greatly appreciative for a solution to this kind of problem!","Apologies: I've had to rephrase this question quite a few times Suppose the following function: Show it is differentiable/not differentiable at I attempted this question first by applying the definition of differentiability which yielded the following expression: If the above equals 0, then the function is infact differentiable at . I can't seem to find a counterexample which proves that it is discontinuous so I attempted to prove its continuity by applying the definition of limits. Unfortunately, no matter whether I express the function in terms of polarcoordinates or not, I cannot bound the following for all . I'd be greatly appreciative for a solution to this kind of problem!","f(x,y)=\frac{2x^3y^2}{x^4+y^2} f(0,0)=0 (0,0) \lim_{(x,y)\to(0,0)}\frac{f(x,y)}{\sqrt{x^2+y^2}} = \lim_{(x,y)\to(0,0)}\frac{2x^3y^2}{(x^4+y^2)\sqrt{x^2+y^2}} (0,0) \epsilon-\delta |\frac{2x^3y^2}{(x^4+y^2)\sqrt{x^2+y^2}}| < \epsilon \epsilon","['real-analysis', 'calculus', 'complex-analysis', 'analysis', 'multivariable-calculus']"
79,"Continuous $f : \Bbb{R}^n \rightarrow \Bbb{R}$, can always find $\delta$ with $\delta < \epsilon$?","Continuous , can always find  with ?",f : \Bbb{R}^n \rightarrow \Bbb{R} \delta \delta < \epsilon,"I'm still new to rigorous proofs in multivariable calculus, does my attempt at the following work? Let $f : \Bbb{R}^n \rightarrow \Bbb{R}$ be a continuous function with $f(\boldsymbol{0}) = 0$ and $f(\boldsymbol{x}) > 0$ otherwise. Let $\epsilon > 0$ be given. Let $$m = \min_{{\Vert\boldsymbol{x}\Vert = \epsilon}} f(\boldsymbol{x})$$ Show that it is always possible to find $\delta$ with $0 < \delta < \epsilon$ such that when $\Vert \boldsymbol{x} \Vert < \delta$ , we have $\Vert f(\boldsymbol{x})\Vert < m$ . My attempt: The existence of a $\delta$ such that when $\Vert \boldsymbol{x} \Vert < \delta$ , we have $\Vert f(\boldsymbol{x})\Vert < m$ follows trivially from the definition of continuity of $f$ at $0$ . All that remains is to show that is possible to pick a $\delta$ with $\delta < \epsilon$ . Assume we are given such a $\delta_1$ . We have $\delta_1 \leq \epsilon$ , because $\Vert f(x)\Vert \geq m$ for $\Vert \boldsymbol{x} \Vert = \epsilon$ . If $\delta_1 < \epsilon$ , let $\delta = \delta_1$ and we are done. Otherwise, $\delta_1 = \epsilon$ , so choose $\delta = \frac{\delta_1}{2}$ and we are done. Does that proof work? Is there a simpler way to express the ""idea"" I'm trying to get at (that we can always shrink our potential $\delta$ down until the strict inequality $\delta < \epsilon$ holds)?","I'm still new to rigorous proofs in multivariable calculus, does my attempt at the following work? Let be a continuous function with and otherwise. Let be given. Let Show that it is always possible to find with such that when , we have . My attempt: The existence of a such that when , we have follows trivially from the definition of continuity of at . All that remains is to show that is possible to pick a with . Assume we are given such a . We have , because for . If , let and we are done. Otherwise, , so choose and we are done. Does that proof work? Is there a simpler way to express the ""idea"" I'm trying to get at (that we can always shrink our potential down until the strict inequality holds)?",f : \Bbb{R}^n \rightarrow \Bbb{R} f(\boldsymbol{0}) = 0 f(\boldsymbol{x}) > 0 \epsilon > 0 m = \min_{{\Vert\boldsymbol{x}\Vert = \epsilon}} f(\boldsymbol{x}) \delta 0 < \delta < \epsilon \Vert \boldsymbol{x} \Vert < \delta \Vert f(\boldsymbol{x})\Vert < m \delta \Vert \boldsymbol{x} \Vert < \delta \Vert f(\boldsymbol{x})\Vert < m f 0 \delta \delta < \epsilon \delta_1 \delta_1 \leq \epsilon \Vert f(x)\Vert \geq m \Vert \boldsymbol{x} \Vert = \epsilon \delta_1 < \epsilon \delta = \delta_1 \delta_1 = \epsilon \delta = \frac{\delta_1}{2} \delta \delta < \epsilon,"['limits', 'multivariable-calculus', 'continuity', 'manifolds']"
80,Confused about one Concept,Confused about one Concept,,"Today in my differential equation lecture, the teacher begin to write some examples in which differential equations appear, first begin with the classic example of a rocket, the example of the mass and the resort, and then he request to us give to he a example of a family of lines, and I tell to he $y=mx+b$ where $m,b$ are parameters and then he put $\frac{dy}{dx}=m$ and then he remplaze it in the original equation $y=\frac{dy}{dx}x+b$ and put that this are the ""Equivalent differential form of the family of curves. Then he  put an exercise and exactlly say the following Consider $y=A \sin x+ B \cos x$ familly of curves with two parameters $A,B$ .Find itÂ´s equivalent differential form. I Never watch these kind of substitution in my life, and since he say that is a differential form IÂ´m confused because the differential form of one equation is an expression like a $dz=f(x,y)dx+g(x,y)dy$ Is correct the teacher or itÂ´s wrong, in the second case, can someone explain me or put the name of the concept. I think that is wrong with the word differential form because if I donÂ´t make a mistake the differential of the equation $y=mx+b$ is $dy=mdx$","Today in my differential equation lecture, the teacher begin to write some examples in which differential equations appear, first begin with the classic example of a rocket, the example of the mass and the resort, and then he request to us give to he a example of a family of lines, and I tell to he where are parameters and then he put and then he remplaze it in the original equation and put that this are the ""Equivalent differential form of the family of curves. Then he  put an exercise and exactlly say the following Consider familly of curves with two parameters .Find itÂ´s equivalent differential form. I Never watch these kind of substitution in my life, and since he say that is a differential form IÂ´m confused because the differential form of one equation is an expression like a Is correct the teacher or itÂ´s wrong, in the second case, can someone explain me or put the name of the concept. I think that is wrong with the word differential form because if I donÂ´t make a mistake the differential of the equation is","y=mx+b m,b \frac{dy}{dx}=m y=\frac{dy}{dx}x+b y=A \sin x+ B \cos x A,B dz=f(x,y)dx+g(x,y)dy y=mx+b dy=mdx","['calculus', 'ordinary-differential-equations']"
81,deriving Stokes' theorem from substituting this identity,deriving Stokes' theorem from substituting this identity,,"As per requested: deriving Stokes' theorem from substituting this identity Making substitution with $ âˆ‡ \times(\phi a) $ where $\phi$ is a scalar field, and $a$ is a vector field. Given that the identity is equal to: $ âˆ‡ \times(\phi a) = \phi (âˆ‡ \times a) + (âˆ‡\phi)\times a$ then derive stokes theorem: $\int_{C}\phi dr = \iint_{S}n \times (âˆ‡\phi )dS$ for a surface $S$ with unit normal $n$ that is bounded by the closed curve $C$ , where the direction of $n$ obeys the usual convention. What I've tried: following from the hint $$\nabla \times (\phi \mathbf{a}) \cdot \mathbf{n} = (\nabla \phi \times \mathbf{a})\cdot \mathbf{n} = (\mathbf{n} \times \nabla \phi) \cdot \mathbf{a}$$ substituting this into the theorem above gives, $\int_{C} \phi \cdot \mathbf{a} dr=\iint_{S} (\mathbf{\hat{n}}\times\nabla\phi)\cdot\mathbf{a}dS$ Then taking out the constant a $\mathbf{a}\cdot(\int_{C}\phi dr)=\mathbf{a}\cdot(\iint_{S}\mathbf{\hat{n}}\times(\nabla\phi)dS) $ since a is arbitrary we deduce that: $\int_{C}\phi dr = \iint_{S}n \times (âˆ‡\phi )dS$","As per requested: deriving Stokes' theorem from substituting this identity Making substitution with where is a scalar field, and is a vector field. Given that the identity is equal to: then derive stokes theorem: for a surface with unit normal that is bounded by the closed curve , where the direction of obeys the usual convention. What I've tried: following from the hint substituting this into the theorem above gives, Then taking out the constant a since a is arbitrary we deduce that:", âˆ‡ \times(\phi a)  \phi a  âˆ‡ \times(\phi a) = \phi (âˆ‡ \times a) + (âˆ‡\phi)\times a \int_{C}\phi dr = \iint_{S}n \times (âˆ‡\phi )dS S n C n \nabla \times (\phi \mathbf{a}) \cdot \mathbf{n} = (\nabla \phi \times \mathbf{a})\cdot \mathbf{n} = (\mathbf{n} \times \nabla \phi) \cdot \mathbf{a} \int_{C} \phi \cdot \mathbf{a} dr=\iint_{S} (\mathbf{\hat{n}}\times\nabla\phi)\cdot\mathbf{a}dS \mathbf{a}\cdot(\int_{C}\phi dr)=\mathbf{a}\cdot(\iint_{S}\mathbf{\hat{n}}\times(\nabla\phi)dS)  \int_{C}\phi dr = \iint_{S}n \times (âˆ‡\phi )dS,"['multivariable-calculus', 'vector-analysis', 'stokes-theorem']"
82,finding the limit $\lim_{x \to 0}[ \lim_{ y \to 0}x\sin\frac{1}{y}]$,finding the limit,\lim_{x \to 0}[ \lim_{ y \to 0}x\sin\frac{1}{y}],"limit $$\lim_{x \to 0}[ \lim_{ y \to 0}f(x,y)]$$ Given : $$f(x,y) = \left\{ 	\begin{array}{ll} 		x\sin\frac{1}{y}  & \mbox{if } y \ne 0 \\ 		0 & \mbox{if } y = 0 	\end{array} \right.$$ this would mean since: $y\neq0$ we need to find : $$ \lim_{x\to 0}[\lim_{y\to0}x\sin\frac{1}{y}]$$ My approach was let the limit be L then it's clear that : $$ \lim_{x\to 0} -x \geq L \geq \lim_{x\to 0} x$$ So the limit must be 0. But it's given to be not defined.",limit Given : this would mean since: we need to find : My approach was let the limit be L then it's clear that : So the limit must be 0. But it's given to be not defined.,"\lim_{x \to 0}[ \lim_{ y \to 0}f(x,y)] f(x,y) =
\left\{
	\begin{array}{ll}
		x\sin\frac{1}{y}  & \mbox{if } y \ne 0 \\
		0 & \mbox{if } y = 0
	\end{array}
\right. y\neq0  \lim_{x\to 0}[\lim_{y\to0}x\sin\frac{1}{y}]  \lim_{x\to 0} -x \geq L \geq \lim_{x\to 0} x","['calculus', 'multivariable-calculus']"
83,Proving the Leibniz Integral Rule,Proving the Leibniz Integral Rule,,"I've been wondering if there is a fairly simple proof or derivation for the following (called the Leibniz Rule): $$ \frac{d}{dt} \int_{a(t)}^{b(t)}f(x,t)dx = \int_{a(t)}^{b(t)}\frac{\partial{f}}{\partial{t}}dx+ f(b(t),t)\frac{d}{dt}b(t)-f(a(t),t)\frac{d}{dt}a(t)$$ What I tried doing is integrating by parts the leftmost integral, and then applying the time derivative to what it's equal to. However, it seems the other side is very far from what the result above. A small hint would suffice as I like trying to prove things on my own.","I've been wondering if there is a fairly simple proof or derivation for the following (called the Leibniz Rule): What I tried doing is integrating by parts the leftmost integral, and then applying the time derivative to what it's equal to. However, it seems the other side is very far from what the result above. A small hint would suffice as I like trying to prove things on my own."," \frac{d}{dt} \int_{a(t)}^{b(t)}f(x,t)dx = \int_{a(t)}^{b(t)}\frac{\partial{f}}{\partial{t}}dx+ f(b(t),t)\frac{d}{dt}b(t)-f(a(t),t)\frac{d}{dt}a(t)","['calculus', 'integration', 'multivariable-calculus', 'leibniz-integral-rule']"
84,Gauss theorem and potential,Gauss theorem and potential,,"Let $V(r) = qe^{-\mu r}/r$ if R is any region containig the origin, show that: $\int_{dR} \nabla V dS = \mu^2 \int_{R} Vd^3r - 4\pi q$ dr is the boundary of R I applied Gauss theorem here, so that $\int_{dR} \nabla V dS = \int \Delta V dÂ³r = \muÂ² \int_{R} VdÂ³r$ Now i can guess that the difference between my answer and the book answer is due the singularity at origin, but how to deal with this? In case like that i can't apply Gauss theorem?","Let if R is any region containig the origin, show that: dr is the boundary of R I applied Gauss theorem here, so that Now i can guess that the difference between my answer and the book answer is due the singularity at origin, but how to deal with this? In case like that i can't apply Gauss theorem?",V(r) = qe^{-\mu r}/r \int_{dR} \nabla V dS = \mu^2 \int_{R} Vd^3r - 4\pi q \int_{dR} \nabla V dS = \int \Delta V dÂ³r = \muÂ² \int_{R} VdÂ³r,"['calculus', 'multivariable-calculus', 'vector-fields']"
85,Calculate the limit of an integration domain,Calculate the limit of an integration domain,,"Consider the set $D(\varepsilon)=\{(x,y)\in\mathbb{R}^2|0<x^2+y^2\leq\varepsilon^2\}$ , which is the filled disk of radius $\varepsilon$ with a hole at the origin. Calculate the limit $$\lim_{\varepsilon\rightarrow0}\dfrac{1}{\varepsilon}\int_{D(\varepsilon)}\dfrac{1+\sin x+\sin y}{\sqrt{x^2+y^2}}d(x,y).$$ My first instinct was to do a change of variables into polar coordinates which would give $D(\varepsilon)=\{(x,y)\in\mathbb{R}^2|0<r^2\leq\varepsilon^2; 0\leq\theta\leq2\pi\}$ and $$\lim_{\varepsilon\rightarrow0}\dfrac{1}{\varepsilon}\int_{D(\varepsilon)}[1+\sin(r\cos\theta)+\sin(r\sin\theta)]d(r,\theta),$$ But this does need seem any simpler than before. Any hints are welcome.","Consider the set , which is the filled disk of radius with a hole at the origin. Calculate the limit My first instinct was to do a change of variables into polar coordinates which would give and But this does need seem any simpler than before. Any hints are welcome.","D(\varepsilon)=\{(x,y)\in\mathbb{R}^2|0<x^2+y^2\leq\varepsilon^2\} \varepsilon \lim_{\varepsilon\rightarrow0}\dfrac{1}{\varepsilon}\int_{D(\varepsilon)}\dfrac{1+\sin x+\sin y}{\sqrt{x^2+y^2}}d(x,y). D(\varepsilon)=\{(x,y)\in\mathbb{R}^2|0<r^2\leq\varepsilon^2; 0\leq\theta\leq2\pi\} \lim_{\varepsilon\rightarrow0}\dfrac{1}{\varepsilon}\int_{D(\varepsilon)}[1+\sin(r\cos\theta)+\sin(r\sin\theta)]d(r,\theta),","['integration', 'limits', 'analysis', 'multivariable-calculus']"
86,Compute this integral (multivar. calc.),Compute this integral (multivar. calc.),,"So i have the following question: Compute $$\int_{C}(x^2+y)dx + (z+x)dy + (x+2y)dz$$ where $C$ is the intersection of the cylinder $x^2+y^2=4 $ and the plane $x+y=z$ So my thoughts are to parametrise it to make get $$r(t) = 2cos(t) i + 2sin(t)j + (2cos(t) + 2sin(t) ) k$$ and then use this along with $r'(t)$ to calculate $$\int_{0}^{2\pi} F(r(t)) \cdot r'(t) dt $$ Which when I computed it all, came out to be $$-8\pi$$ I was hoping someone could either verify this method, along with the answer or help me to find out the solution (through tips) Thank you","So i have the following question: Compute where is the intersection of the cylinder and the plane So my thoughts are to parametrise it to make get and then use this along with to calculate Which when I computed it all, came out to be I was hoping someone could either verify this method, along with the answer or help me to find out the solution (through tips) Thank you",\int_{C}(x^2+y)dx + (z+x)dy + (x+2y)dz C x^2+y^2=4  x+y=z r(t) = 2cos(t) i + 2sin(t)j + (2cos(t) + 2sin(t) ) k r'(t) \int_{0}^{2\pi} F(r(t)) \cdot r'(t) dt  -8\pi,['multivariable-calculus']
87,How to show frechet characterization differentiability 2,How to show frechet characterization differentiability 2,,"Let $U\subset R^{n} $ and $f:U\to R$ . Show that: $f$ is differetiable in $x_0\in U$ iff exist $A\in L(R^{n},R)$ and exist $r>0$ : $\lim_{t\rightarrow 0^{+}}\frac{f(x_0 +tv)-f(x_0)}{t}=Av$ uniformly in $v\in r\mathcal{S}^{n-1}$ with $\mathcal{S}^{n-1}$ is unitary sphere. Attempt $\Rightarrow$ Suppose that $f$ is differentiable in $x_0\in U$ , then exist $A_{x_0}:R^{n}\to R$ linear transformation such that $f(x_0 +y)=f(x_0)+A_{x_0}(y)+||y||E_{x_0}(y)$ where $E_{x_0}(y)\rightarrow 0$ when $y\rightarrow 0$ , equivalently $\frac{f(x_{0} + y)-f(x_{0})}{||y||}-\frac{A_{x_0}(y)}{||y||}=E_{x_0}(y)$ , now if i take $z=r\frac{y}{||y||}$ then $\frac{y}{||y||}\in \mathcal{S}^{n-1}$ and $y=\frac{||y||}{r}z$ , if we will call $t=\frac{||y||}{r}$ imply $\frac{f(x_{0} + tz)-f(x_{0})}{tr}-A_{x_0}(\frac{z}{r})=E_{x_0}(tz)$ and when i take limit when $t\rightarrow 0$ tehn $\lim_{t\rightarrow 0}\left( \frac{f(x_{0} + tz)-f(x_{0})}{tr}-A_{x_0}(\frac{z}{r}) \right)=\frac{1}{r}\left[\lim_{t\rightarrow 0}\frac{f(x_{0} + tz)-f(x_{0})}{t}-A_{x_0}(z)\right]=\lim_{t\rightarrow 0}E_{x_0}(tz)=0$ hence $\lim_{t\rightarrow 0}\frac{f(x_{0} + tz)-f(x_{0})}{t}=A_{x_0}(z)$ with $z\in r\mathcal{S}^{n-1}$ . $\Leftarrow$ Now by hypothesis $\lim_{t\rightarrow 0}\frac{f(x_{0} + tv)-f(x_{0})}{t}=A_{x_0}(v)$ with $v\in r\mathcal{S}^{n-1}$ uniformly, then $v=rw$ with $r>0$ and $w\in \mathcal{S}^{n-1}$ so $\lim_{t\rightarrow 0}\frac{f(x_{0} + trw)-f(x_{0})}{t}=A_{x_0}(rw)=rA_{x_0}(w)$ or the same way $\lim_{t\rightarrow 0}\frac{f(x_{0} + trw)-f(x_{0})}{rt}=A_{x_0}(w)$ , but if we choose $h=tr$ when $t\rightarrow 0$ hence $h\rightarrow 0$ imply $\lim_{h\rightarrow 0}\frac{f(x_{0} + hw)-f(x_{0})}{h}=A_{x_0}(w)$ and this is a definition of differentiablility in $x_0$ . My profesor say that i m not using the condition of uniformly no anywhere and my proof is confuse, therefore i have to proof that exist $r$ , i just take one but i dont know how to show that exist,  can somebody to help me please, thank you","Let and . Show that: is differetiable in iff exist and exist : uniformly in with is unitary sphere. Attempt Suppose that is differentiable in , then exist linear transformation such that where when , equivalently , now if i take then and , if we will call imply and when i take limit when tehn hence with . Now by hypothesis with uniformly, then with and so or the same way , but if we choose when hence imply and this is a definition of differentiablility in . My profesor say that i m not using the condition of uniformly no anywhere and my proof is confuse, therefore i have to proof that exist , i just take one but i dont know how to show that exist,  can somebody to help me please, thank you","U\subset R^{n}  f:U\to R f x_0\in U A\in L(R^{n},R) r>0 \lim_{t\rightarrow 0^{+}}\frac{f(x_0 +tv)-f(x_0)}{t}=Av v\in r\mathcal{S}^{n-1} \mathcal{S}^{n-1} \Rightarrow f x_0\in U A_{x_0}:R^{n}\to R f(x_0 +y)=f(x_0)+A_{x_0}(y)+||y||E_{x_0}(y) E_{x_0}(y)\rightarrow 0 y\rightarrow 0 \frac{f(x_{0} + y)-f(x_{0})}{||y||}-\frac{A_{x_0}(y)}{||y||}=E_{x_0}(y) z=r\frac{y}{||y||} \frac{y}{||y||}\in \mathcal{S}^{n-1} y=\frac{||y||}{r}z t=\frac{||y||}{r} \frac{f(x_{0} + tz)-f(x_{0})}{tr}-A_{x_0}(\frac{z}{r})=E_{x_0}(tz) t\rightarrow 0 \lim_{t\rightarrow 0}\left( \frac{f(x_{0} + tz)-f(x_{0})}{tr}-A_{x_0}(\frac{z}{r}) \right)=\frac{1}{r}\left[\lim_{t\rightarrow 0}\frac{f(x_{0} + tz)-f(x_{0})}{t}-A_{x_0}(z)\right]=\lim_{t\rightarrow 0}E_{x_0}(tz)=0 \lim_{t\rightarrow 0}\frac{f(x_{0} + tz)-f(x_{0})}{t}=A_{x_0}(z) z\in r\mathcal{S}^{n-1} \Leftarrow \lim_{t\rightarrow 0}\frac{f(x_{0} + tv)-f(x_{0})}{t}=A_{x_0}(v) v\in r\mathcal{S}^{n-1} v=rw r>0 w\in \mathcal{S}^{n-1} \lim_{t\rightarrow 0}\frac{f(x_{0} + trw)-f(x_{0})}{t}=A_{x_0}(rw)=rA_{x_0}(w) \lim_{t\rightarrow 0}\frac{f(x_{0} + trw)-f(x_{0})}{rt}=A_{x_0}(w) h=tr t\rightarrow 0 h\rightarrow 0 \lim_{h\rightarrow 0}\frac{f(x_{0} + hw)-f(x_{0})}{h}=A_{x_0}(w) x_0 r","['multivariable-calculus', 'frechet-derivative']"
88,Change of Variable with Jacobian,Change of Variable with Jacobian,,"Let $\Omega \subset  \mathbb{R}^m$ be some compactly contained domain in $\mathbb{R}^m.$ Let $Q_t(x) = x+t\zeta$ where $\zeta\in C^{\infty}_c(\Omega, \mathbb{R}^m)$ and $t$ is small enough so that $Q_t$ is a diffeomorphism on $\Omega.$ My goal is to compute, $$\frac{d}{dt}\left(\int_{\Omega} |D(u(x+t\zeta))|^2dx\right)_{|t=0} = \int_{\Omega}\left(\frac{|Du|^2}{2}\operatorname{div}(\zeta) - \partial_{\alpha}u^i \partial_{\beta} u^i \partial_{\beta}\zeta^\alpha\right) dx$$ where $u\in H^{1}(\Omega, \mathbb{R}^n),$ and we implicitly sum over indices $1\leq \alpha, \beta \leq m$ and $1\leq i \leq n.$ I am reading a proof where the author considers the map, $u_t(x)=u(Q_t^{-1}(x))$ and argues that, $$\int_{\Omega} |D(u(x+t\zeta))|^2dx =\int_{\Omega} |Du_t(x)|^2dx.$$ I am not sure how to prove this, I tried change of variable, by setting $y=Q_t(x)$ in the first integral, to get $x=Q_t^{-1}(y)\implies dx = |\det DQ_t^{-1}(y)| dy$ and so, $$\int_{\Omega} |D(u(x+t\zeta))|^2dx = \int_{\Omega} |Du(x+t\zeta) DQ_t(x)|^2 dx \\ =\int_{\Omega} |Du(y)|^2 |DQ_t(Q_t^{-1}(y))|^2 |\det DQ_t^{-1}(y)| dy\\ =\int_{\Omega} |Du(y)|^2 |DQ_t(Q_t^{-1}(y))|^2 |\det DQ_t^{-1}(y)| dy.$$ On the other hand, the author claims that, $$\int_{\Omega} |Du_t(x)|^2dx = \int_{\Omega} |Du(x)|^2 DQ_t^{-1}(Q_t(x))|^2 |\det DQ_t(x)| dx.$$ How do I show that the two expressions are the same?","Let be some compactly contained domain in Let where and is small enough so that is a diffeomorphism on My goal is to compute, where and we implicitly sum over indices and I am reading a proof where the author considers the map, and argues that, I am not sure how to prove this, I tried change of variable, by setting in the first integral, to get and so, On the other hand, the author claims that, How do I show that the two expressions are the same?","\Omega \subset  \mathbb{R}^m \mathbb{R}^m. Q_t(x) = x+t\zeta \zeta\in C^{\infty}_c(\Omega, \mathbb{R}^m) t Q_t \Omega. \frac{d}{dt}\left(\int_{\Omega} |D(u(x+t\zeta))|^2dx\right)_{|t=0} = \int_{\Omega}\left(\frac{|Du|^2}{2}\operatorname{div}(\zeta) - \partial_{\alpha}u^i \partial_{\beta} u^i \partial_{\beta}\zeta^\alpha\right) dx u\in H^{1}(\Omega, \mathbb{R}^n), 1\leq \alpha, \beta \leq m 1\leq i \leq n. u_t(x)=u(Q_t^{-1}(x)) \int_{\Omega} |D(u(x+t\zeta))|^2dx =\int_{\Omega} |Du_t(x)|^2dx. y=Q_t(x) x=Q_t^{-1}(y)\implies dx = |\det DQ_t^{-1}(y)| dy \int_{\Omega} |D(u(x+t\zeta))|^2dx = \int_{\Omega} |Du(x+t\zeta) DQ_t(x)|^2 dx \\
=\int_{\Omega} |Du(y)|^2 |DQ_t(Q_t^{-1}(y))|^2 |\det DQ_t^{-1}(y)| dy\\
=\int_{\Omega} |Du(y)|^2 |DQ_t(Q_t^{-1}(y))|^2 |\det DQ_t^{-1}(y)| dy. \int_{\Omega} |Du_t(x)|^2dx = \int_{\Omega} |Du(x)|^2 DQ_t^{-1}(Q_t(x))|^2 |\det DQ_t(x)| dx.","['multivariable-calculus', 'vector-analysis', 'change-of-variable']"
89,"How to prove case 1 of Theorem 4.3 in ""Calculus of Several Variables"" by Serge Lang?","How to prove case 1 of Theorem 4.3 in ""Calculus of Several Variables"" by Serge Lang?",,"I am reading ""Calculus of Several Variables 3rd Edition"" by Serge Lang. How to prove case 1 of Theorem 4.3 in ""Calculus of Several Variables"" by Serge Lang? In Theorem 4.1, for any two points $P, Q$ in $U$ , $\int_{P,C}^{Q} F$ was independent of the path $C$ in $U$ joining $P$ and $Q$ . But in Theorem 4.3, Lang specifies the path from $(1,0)$ to $X$ for $\int_{(1,0)}^{X} F$ . So I cannot mimic the proof of Theorem 4.1. Theorem 4.3 Theorem 4.1","I am reading ""Calculus of Several Variables 3rd Edition"" by Serge Lang. How to prove case 1 of Theorem 4.3 in ""Calculus of Several Variables"" by Serge Lang? In Theorem 4.1, for any two points in , was independent of the path in joining and . But in Theorem 4.3, Lang specifies the path from to for . So I cannot mimic the proof of Theorem 4.1. Theorem 4.3 Theorem 4.1","P, Q U \int_{P,C}^{Q} F C U P Q (1,0) X \int_{(1,0)}^{X} F","['multivariable-calculus', 'vector-analysis', 'potential-theory']"
90,Non continuous function with directional derivatives zero in all directions,Non continuous function with directional derivatives zero in all directions,,"I was asked the following question: A function f(x,y) has directional derivatives in all directions, and they are 0 in all directions. Does f have to be continous? I thought I proved it by: \begin{align}\lim_{(x,y)\to (0,0)}f(x,y)-f(0,0)&=\lim_{r\to0}f(r\cos\theta,r\sin\theta)-f(0,0)\\&=\lim_{r\to0}r\cdot (f(r\cos\theta,r\sin\theta)-f(0,0))/r\\&=\lim_{r\to0}r\cdot \lim_{r\to0}(f(r\cos\theta,r\sin\theta)-f(0,0))/r\end{align} The right limit is the directional derivative in direction $\theta$ which is said to be zero, and the left limit is zero and we get: $$\lim_{(x,y)\to (0,0)}f(x,y)=f(0,0)$$ Is there a mistake in this proof? Because the solution said that f does not have to be continuous. Thank you!","I was asked the following question: A function f(x,y) has directional derivatives in all directions, and they are 0 in all directions. Does f have to be continous? I thought I proved it by: The right limit is the directional derivative in direction which is said to be zero, and the left limit is zero and we get: Is there a mistake in this proof? Because the solution said that f does not have to be continuous. Thank you!","\begin{align}\lim_{(x,y)\to (0,0)}f(x,y)-f(0,0)&=\lim_{r\to0}f(r\cos\theta,r\sin\theta)-f(0,0)\\&=\lim_{r\to0}r\cdot (f(r\cos\theta,r\sin\theta)-f(0,0))/r\\&=\lim_{r\to0}r\cdot \lim_{r\to0}(f(r\cos\theta,r\sin\theta)-f(0,0))/r\end{align} \theta \lim_{(x,y)\to (0,0)}f(x,y)=f(0,0)","['multivariable-calculus', 'derivatives', 'continuity']"
91,Inverse of Multivariable Functions on Manifolds,Inverse of Multivariable Functions on Manifolds,,"Consider the unit circle, $S=$ { $(x,y) | x^2 + y^2 =1$ } with the stereographic charts $(U_N,\phi_N)$ , $(U_S,\phi_S)$ i.e. $U_N=S$ \ { $(0,1)$ }, $\phi_N((x,y))=\frac{x}{1-y}$ $U_S=S$ \ { $(0,-1)$ }, $\phi_S((x,y))=\frac{x}{1+y}$ How would I find ${\phi_N}^{-1}$ and ${\phi_S}^{-1}$ ? These functions are bijective on their domains ( $U_N$ and $U_S$ respectively) so their inverses exist. For ${\phi_N}^{-1}$ , I have considered a point $z$ in $\phi_N(U_N)$ and set $\frac{xy}{1-y}=z$ . I believe I need to write both x and y in terms of $z$ to get something like ${\phi_N}^{-1}(z)=(f(z), g(z))$ where I need to find $f(z)$ and $g(z)$ . And then similarly for $\phi_S$ . How would I do this? Thank you for any help!","Consider the unit circle, { } with the stereographic charts , i.e. \ { }, \ { }, How would I find and ? These functions are bijective on their domains ( and respectively) so their inverses exist. For , I have considered a point in and set . I believe I need to write both x and y in terms of to get something like where I need to find and . And then similarly for . How would I do this? Thank you for any help!","S= (x,y) | x^2 + y^2 =1 (U_N,\phi_N) (U_S,\phi_S) U_N=S (0,1) \phi_N((x,y))=\frac{x}{1-y} U_S=S (0,-1) \phi_S((x,y))=\frac{x}{1+y} {\phi_N}^{-1} {\phi_S}^{-1} U_N U_S {\phi_N}^{-1} z \phi_N(U_N) \frac{xy}{1-y}=z z {\phi_N}^{-1}(z)=(f(z), g(z)) f(z) g(z) \phi_S","['multivariable-calculus', 'manifolds', 'inverse-function', 'stereographic-projections']"
92,"Mistake when substituting constraint $4x^2+y^2=1$ into a function $f(x,y)=x^2+y^2$ in extrema problem",Mistake when substituting constraint  into a function  in extrema problem,"4x^2+y^2=1 f(x,y)=x^2+y^2","Consider the function $f(x,y)=x^2+y^2$ under the constraint $4x^2+y^2=1$ . The extrema of $f$ under that constraint can be easily found with Lagrange multipliers, and they are attained for $(0,1)$ , $(0,-1)$ for maximum and $(1/2,0)$ , $(-1/2,0)$ for minimum; however, if we isolate $y^2=1-4x^2$ and we substitute it in the function, we get a wrong result (in particular, the one variable function $g(x)=1-3x^2$ obtained has only a maximum for $x=0$ and Weierstrass theorem assures that the are both maximum and minimum for $f$ ). Can someone explain me why this fails?","Consider the function under the constraint . The extrema of under that constraint can be easily found with Lagrange multipliers, and they are attained for , for maximum and , for minimum; however, if we isolate and we substitute it in the function, we get a wrong result (in particular, the one variable function obtained has only a maximum for and Weierstrass theorem assures that the are both maximum and minimum for ). Can someone explain me why this fails?","f(x,y)=x^2+y^2 4x^2+y^2=1 f (0,1) (0,-1) (1/2,0) (-1/2,0) y^2=1-4x^2 g(x)=1-3x^2 x=0 f","['analysis', 'multivariable-calculus', 'optimization']"
93,Characterization of the image of the Laplacian,Characterization of the image of the Laplacian,,"I'm trying to understand the set of functions $\mathbb R^3 \rightarrow \mathbb R$ and how the Laplacian interacts with them. Recall that the laplacian $\Delta: (\mathbb R^3 \rightarrow \mathbb R)\rightarrow (\mathbb R^3 \rightarrow \mathbb R)$ is defined as $$ \Delta(f) \equiv \frac{\partial^2 f}{\partial x^2} + \frac{\partial ^2 f}{\partial y^2} + \frac{\partial ^2 f}{\partial z^2} $$ Now, is there some convenient characterization of the image of the Laplacian, the set: $$ Im(\Delta) \equiv \{ \Delta(f) \mid  f:   \mathbb R^3 \rightarrow \mathbb R \} $$ Searching for ""image of Laplacian"" is not useful since it gives me results about image processing that I don't care about! Is there a nice characterization of the set $Im(\Delta)$ ?","I'm trying to understand the set of functions and how the Laplacian interacts with them. Recall that the laplacian is defined as Now, is there some convenient characterization of the image of the Laplacian, the set: Searching for ""image of Laplacian"" is not useful since it gives me results about image processing that I don't care about! Is there a nice characterization of the set ?","\mathbb R^3 \rightarrow \mathbb R \Delta: (\mathbb R^3 \rightarrow \mathbb R)\rightarrow (\mathbb R^3 \rightarrow \mathbb R) 
\Delta(f) \equiv \frac{\partial^2 f}{\partial x^2} + \frac{\partial ^2 f}{\partial y^2} + \frac{\partial ^2 f}{\partial z^2}
 
Im(\Delta) \equiv \{ \Delta(f) \mid  f:   \mathbb R^3 \rightarrow \mathbb R \}
 Im(\Delta)","['multivariable-calculus', 'vector-analysis', 'laplacian']"
94,Applying simple boundary conditions to a multi-variable function.,Applying simple boundary conditions to a multi-variable function.,,"If I have the following general differential equation solution (to Laplace equation): $$ \phi(r,\theta)= A+B\ln(r)+\sum_{n=1}^{\infty}\left(C_nr^n+\frac{D_n}{r^n}\right)\left( E_n \cos(n\theta) + F_n \sin(n\theta) \right) \quad \ \quad  (*) $$ subject to $\phi(r=0,\theta=0)=0$ , why is the particular solution $$ \phi(r,\theta)= \sum_{n=1}^{\infty}C_nF_n r^n \sin(n\theta) \quad ? $$ A priori, why $A=B=D_n=0 \ \forall \ n$ is clear. With these conditions $(*)$ becomes $$ \phi(r,\theta)=\sum_{n=1}^{\infty}C_nr^n\left( E_n \cos(n\theta) + F_n \sin(n\theta) \right) $$ However, I dont see why $E_n =0$ , the above already satisfies the boundary conditions. Moreover couldnâ€™t one keep $D_n\neq0$ provided $E_n=0$ ? To me it seems as if there is not enough information to uniquely determine $\phi(r,\theta)$ .","If I have the following general differential equation solution (to Laplace equation): subject to , why is the particular solution A priori, why is clear. With these conditions becomes However, I dont see why , the above already satisfies the boundary conditions. Moreover couldnâ€™t one keep provided ? To me it seems as if there is not enough information to uniquely determine .","
\phi(r,\theta)= A+B\ln(r)+\sum_{n=1}^{\infty}\left(C_nr^n+\frac{D_n}{r^n}\right)\left( E_n \cos(n\theta) + F_n \sin(n\theta) \right) \quad \ \quad  (*)
 \phi(r=0,\theta=0)=0 
\phi(r,\theta)= \sum_{n=1}^{\infty}C_nF_n r^n \sin(n\theta) \quad ?
 A=B=D_n=0 \ \forall \ n (*) 
\phi(r,\theta)=\sum_{n=1}^{\infty}C_nr^n\left( E_n \cos(n\theta) + F_n \sin(n\theta) \right)
 E_n =0 D_n\neq0 E_n=0 \phi(r,\theta)","['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'boundary-value-problem']"
95,Cannot Solve Tangent Plane Equation to Parametric Surface,Cannot Solve Tangent Plane Equation to Parametric Surface,,"I have a problem while finding the equation of a tangent plane to a parametric surface. The surface is given by $$\pmb r = \begin{bmatrix} s^2 + t^2 \cr 2s+2t \cr 2 \end{bmatrix}$$ The point at which plane is tangent $$ \pmb r_o = \begin{bmatrix} 2 \cr 4 \cr 2 \end{bmatrix} (s=1, t=1)$$ I know that for tangent-plane equation, I need to find a normal vector $\pmb n$ by doing a cross-product of $\frac{\partial \pmb r}{\partial t}$ at $r_o$ and $\frac{\partial \pmb r}{\partial s}$ at $r_o$ . My problem is that both $\frac{\partial \pmb r}{\partial t}$ and $\frac{\partial \pmb r}{\partial s}$ are being computed as the same vector $\begin{bmatrix} 2 \cr 2 \cr 0 \end{bmatrix}$ . So my normal vector will always be zero, and I can't use it to find the equation of the tangent. How to get around this problem? Also, in general, how do we solve tangent-plane equations to parametric surfaces in such cases?","I have a problem while finding the equation of a tangent plane to a parametric surface. The surface is given by The point at which plane is tangent I know that for tangent-plane equation, I need to find a normal vector by doing a cross-product of at and at . My problem is that both and are being computed as the same vector . So my normal vector will always be zero, and I can't use it to find the equation of the tangent. How to get around this problem? Also, in general, how do we solve tangent-plane equations to parametric surfaces in such cases?","\pmb r = \begin{bmatrix} s^2 + t^2 \cr 2s+2t \cr 2 \end{bmatrix}  \pmb r_o = \begin{bmatrix} 2 \cr 4 \cr 2 \end{bmatrix} (s=1, t=1) \pmb n \frac{\partial \pmb r}{\partial t} r_o \frac{\partial \pmb r}{\partial s} r_o \frac{\partial \pmb r}{\partial t} \frac{\partial \pmb r}{\partial s} \begin{bmatrix} 2 \cr 2 \cr 0 \end{bmatrix}","['calculus', 'geometry', 'multivariable-calculus', 'vectors', 'parametric']"
96,Find curve at the intersection of two level surfaces.,Find curve at the intersection of two level surfaces.,,"Let C be the curve at the intersection of two level surfaces M(x, y, z) = 5 and N(x,y,z) = 0, passing through a point P (1,1,1). Let $M(x, y, z) = 2x^2 + y^2 + 2z^2$ and $N(x, y, z) = xy-z$ . Find curve C in the parametric form <x(t), y(t), z(t)>. I am quite stuck on this one. I could let $z = xy$ and then plugging that into M I'd have $2x^2 + y^2 + 2x^2y^2 = 5$ , and I have no idea what would be good ways to parametrize that. Another way is to let $y=z/x$ , in which case I end up with $2x^4 + (2z^2-5)x^2+z^2 = 0$ , but again, I'm quite stuck. Any help would be really appreciated! Or, if it is impossible to find the curve C, the question I'm working on actually asks for the tangent line to curve C at P if x'(0) = 3. If this makes things easier, how exactly is using this tangent more useful?","Let C be the curve at the intersection of two level surfaces M(x, y, z) = 5 and N(x,y,z) = 0, passing through a point P (1,1,1). Let and . Find curve C in the parametric form <x(t), y(t), z(t)>. I am quite stuck on this one. I could let and then plugging that into M I'd have , and I have no idea what would be good ways to parametrize that. Another way is to let , in which case I end up with , but again, I'm quite stuck. Any help would be really appreciated! Or, if it is impossible to find the curve C, the question I'm working on actually asks for the tangent line to curve C at P if x'(0) = 3. If this makes things easier, how exactly is using this tangent more useful?","M(x, y, z) = 2x^2 + y^2 + 2z^2 N(x, y, z) = xy-z z = xy 2x^2 + y^2 + 2x^2y^2 = 5 y=z/x 2x^4 + (2z^2-5)x^2+z^2 = 0","['multivariable-calculus', 'vectors', 'parametric', 'tangent-line']"
97,Problem on Stokes' Theorem,Problem on Stokes' Theorem,,"I'm really struggling to understand Stokes' Theorem. I tried this exercise: Let D be the portion of $z=1-x^2-y^2$ above the xy-plane, oriented up, and let $\vec{F}=\langle xy^2,-x^2y,xyz\rangle$ . Compute $$\iint_{D}^{}(\nabla\times \vec{F})\cdot \hat{n}dS$$ Here is my work: $$\nabla\times \vec{F}=\langle xz,-yz,-4xy\rangle$$ $$\vec{f}(r,\theta) = \bigl\langle r\cos\theta ,r\sin\theta ,1-r^2 \bigr\rangle$$ $$\frac{\partial\vec{f} }{\partial r}= \langle\cos\theta,\sin\theta,-2r\rangle$$ $$\frac{\partial \vec{f}}{\partial\theta }= \langle -r\sin\theta ,r\cos\theta ,0 \rangle$$ $$\frac{\partial\vec{f} }{\partial r}\times \frac{\partial \vec{f}}{\partial \theta}=\left \langle 2r^2\cos\theta ,2r^2\sin\theta ,r\right \rangle$$ $$\left \|\frac{\partial\vec{f} }{\partial r}\times \frac{\partial \vec{f}}{\partial \theta } \right \|=r\sqrt{3}$$ $$\widehat{n}= \biggl\langle \frac{2r\cos\theta}{\sqrt{3}} ,\frac{2r\sin\theta }{\sqrt{3}},\frac{1}{\sqrt{3}}\biggr\rangle$$ Integrating, I have $$\frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}2r^2\cos^2\theta (1-r^2)-2r^2\sin^2\theta (1-r\cos\theta )-4r\sin\theta\cos\theta\,dr\,d\theta $$ $$=\frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}-2r^2\sin^2\theta (1-r\cos\theta )+2r^2\cos\theta\, \theta (1-r^2)-2r\sin(2\theta )\,dr\,d\theta$$ After splitting the integral into three integrals, I have $$\frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}-2r^2\sin^2\theta (1-r\cos\theta )\,dr\,d\theta=-\frac{2\pi }{3\sqrt{3}}$$ $$\frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}2r^2\cos\theta (1-r^2)\,dr\,d\theta =0$$ $$\frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}-2r\sin(2\theta )\,dr\,d\theta =0$$ $$=-\frac{2\pi }{3\sqrt{3}}+0+0$$ But the answer is zero. What am I doing wrong?","I'm really struggling to understand Stokes' Theorem. I tried this exercise: Let D be the portion of above the xy-plane, oriented up, and let . Compute Here is my work: Integrating, I have After splitting the integral into three integrals, I have But the answer is zero. What am I doing wrong?","z=1-x^2-y^2 \vec{F}=\langle xy^2,-x^2y,xyz\rangle \iint_{D}^{}(\nabla\times \vec{F})\cdot \hat{n}dS \nabla\times \vec{F}=\langle xz,-yz,-4xy\rangle \vec{f}(r,\theta) = \bigl\langle r\cos\theta ,r\sin\theta ,1-r^2 \bigr\rangle \frac{\partial\vec{f} }{\partial r}= \langle\cos\theta,\sin\theta,-2r\rangle \frac{\partial \vec{f}}{\partial\theta }= \langle -r\sin\theta ,r\cos\theta ,0 \rangle \frac{\partial\vec{f} }{\partial r}\times \frac{\partial \vec{f}}{\partial \theta}=\left \langle 2r^2\cos\theta ,2r^2\sin\theta ,r\right \rangle \left \|\frac{\partial\vec{f} }{\partial r}\times \frac{\partial \vec{f}}{\partial \theta } \right \|=r\sqrt{3} \widehat{n}= \biggl\langle \frac{2r\cos\theta}{\sqrt{3}} ,\frac{2r\sin\theta }{\sqrt{3}},\frac{1}{\sqrt{3}}\biggr\rangle \frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}2r^2\cos^2\theta (1-r^2)-2r^2\sin^2\theta (1-r\cos\theta )-4r\sin\theta\cos\theta\,dr\,d\theta  =\frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}-2r^2\sin^2\theta (1-r\cos\theta )+2r^2\cos\theta\, \theta (1-r^2)-2r\sin(2\theta )\,dr\,d\theta \frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}-2r^2\sin^2\theta (1-r\cos\theta )\,dr\,d\theta=-\frac{2\pi }{3\sqrt{3}} \frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}2r^2\cos\theta (1-r^2)\,dr\,d\theta =0 \frac{1}{\sqrt{3}}\int_{0}^{2\pi }\int_{0}^{1}-2r\sin(2\theta )\,dr\,d\theta =0 =-\frac{2\pi }{3\sqrt{3}}+0+0","['multivariable-calculus', 'vector-analysis', 'stokes-theorem']"
98,"Showing the set $\{x: x = 2^{-k} \ k\in \mathbb{N}\ \text{or}\ x = 0 \}$ is open, closed, or neither - solution feedback","Showing the set  is open, closed, or neither - solution feedback",\{x: x = 2^{-k} \ k\in \mathbb{N}\ \text{or}\ x = 0 \},"This question comes from Shifrin's  Multivariable Mathermatics - Sec 2.2 - 1(b) , it asks: Show that the set $B = \{x: x = 2^{-k} \ k\in \mathbb{N}\ \text{or}\ x = 0 \}$ is open, closed, or neither and prove your answer. I've come to the conclusion that the set is closed and I wanted some assistance on tightening up my solution. Solution The tools I have up to this point to show that a set is closed are: i) show all convergent sequences in the set converge to a point in the set. ii) Show that the complement of the set is open. I chose to approach this problem using (ii), but I do have a question about how I could use (i) I'll ask at the end. So first I have to figure out what it means to be in $C = \mathbb{R} - B$ . To me this meant $\mathbb{R} - B = \{x: x \neq 2^{-k}\ \text{and} x \neq 0\}$ . To build off of this idea I drew a picture of the number line with the points belonging to $B$ and observed how the points of $C$ would have to be to satisfy my argument. So after some fiddling around I defined the points of $C$ as follows: $$C = \mathbb{R} - B = \{y: y = x\pm \delta\ \text{depending on which side of an $x$ from $B$ that the point $y$ is in relation to}\}$$ . So how do I choose my $\delta$ ? So keep in mind that each $x \in B$ is actually a function of a particular $k$ , so with this in mind I defined $\delta = \min(|2^{-k}-y|, |2^{-(k+1)}-y|)$ . In this set up it means I'm taking a $\delta$ that will take into consideration the possibility of having an open ball over a value $x \in B$ which we don't want. So with this setup I now need to show $C$ is open. Define a point $z \in C$ as $z = x \pm \frac{\delta}{2}$ . By this definition we see that $z \in B(y, \delta)$ . It's here where I'm having trouble completing the proof. So to show the ball is open is dependent on where my $y$ value is, so for a concrete argument let's suppose that $y < 2^{-k}$ . To show $B(y, \delta) \subset C$ means I have to show for all $z \in B(y, \delta)$ that: $$|z - y| < |2^{-k} - y| \\ \Rightarrow\ |z - y| < 2^{-k} - y\ \text{(since assuming $y < 2^{-k}$ case)}$$ I'm having trouble arriving at the conclusion...What I envision happening after some algebra is: $$2^{-(k+1)} < z < 2^{-k}$$ Which would then give me my open ball. So I have two questions: My idea seems to be leading me in the right direction, but needs some fine-tuning. What do I need to do to tighten it up? With regards to the other way of showing closed. In this set it seems like the only sequence is the set itself since the $x = 2^{-k}$ and that will converge to $0$ , but can I actually say that this is true? Couldn't there be some rare sequences that I have not found that are in the set, but since I have not explicitly stated all convergent sequences then I can't use the definition of closed explicitly?","This question comes from Shifrin's  Multivariable Mathermatics - Sec 2.2 - 1(b) , it asks: Show that the set is open, closed, or neither and prove your answer. I've come to the conclusion that the set is closed and I wanted some assistance on tightening up my solution. Solution The tools I have up to this point to show that a set is closed are: i) show all convergent sequences in the set converge to a point in the set. ii) Show that the complement of the set is open. I chose to approach this problem using (ii), but I do have a question about how I could use (i) I'll ask at the end. So first I have to figure out what it means to be in . To me this meant . To build off of this idea I drew a picture of the number line with the points belonging to and observed how the points of would have to be to satisfy my argument. So after some fiddling around I defined the points of as follows: . So how do I choose my ? So keep in mind that each is actually a function of a particular , so with this in mind I defined . In this set up it means I'm taking a that will take into consideration the possibility of having an open ball over a value which we don't want. So with this setup I now need to show is open. Define a point as . By this definition we see that . It's here where I'm having trouble completing the proof. So to show the ball is open is dependent on where my value is, so for a concrete argument let's suppose that . To show means I have to show for all that: I'm having trouble arriving at the conclusion...What I envision happening after some algebra is: Which would then give me my open ball. So I have two questions: My idea seems to be leading me in the right direction, but needs some fine-tuning. What do I need to do to tighten it up? With regards to the other way of showing closed. In this set it seems like the only sequence is the set itself since the and that will converge to , but can I actually say that this is true? Couldn't there be some rare sequences that I have not found that are in the set, but since I have not explicitly stated all convergent sequences then I can't use the definition of closed explicitly?","B = \{x: x = 2^{-k} \ k\in \mathbb{N}\ \text{or}\ x = 0 \} C = \mathbb{R} - B \mathbb{R} - B = \{x: x \neq 2^{-k}\ \text{and} x \neq 0\} B C C C = \mathbb{R} - B = \{y: y = x\pm \delta\ \text{depending on which side of an x from B that the point y is in relation to}\} \delta x \in B k \delta = \min(|2^{-k}-y|, |2^{-(k+1)}-y|) \delta x \in B C z \in C z = x \pm \frac{\delta}{2} z \in B(y, \delta) y y < 2^{-k} B(y, \delta) \subset C z \in B(y, \delta) |z - y| < |2^{-k} - y| \\
\Rightarrow\ |z - y| < 2^{-k} - y\ \text{(since assuming y < 2^{-k} case)} 2^{-(k+1)} < z < 2^{-k} x = 2^{-k} 0","['real-analysis', 'calculus', 'sequences-and-series', 'multivariable-calculus', 'elementary-set-theory']"
99,"Compute the volume of the solid bounded by $x=0, x=\frac{\pi}{2}, z=0,z=y, y=\cos(x).$",Compute the volume of the solid bounded by,"x=0, x=\frac{\pi}{2}, z=0,z=y, y=\cos(x).","Compute the volume of the solid bounded by $x=0, x=\frac{\pi}{2}, z=0,z=y, y=\cos(x).$ I'm not sure if I'm mistaken here, but isn't this just $$\int_{0}^{\frac{\pi}{2}}\int_{0}^{y}\int_{0}^{\cos(x)} \ dy \ dz \ dx = \int_{0}^{\frac{\pi}{2}}\int_{0}^{y} \cos(x) \ dz \ dx = \int_{0}^{\frac{\pi}{2}}y\cos(x) \ dx = y$$ this doesn't seem right to me. Is there a problem regarding the limits of $y$ ? The picture of the $xy$ -plane looks like a $\cos(x)$ bounded between $0$ and $\frac{\pi}{2}$","Compute the volume of the solid bounded by I'm not sure if I'm mistaken here, but isn't this just this doesn't seem right to me. Is there a problem regarding the limits of ? The picture of the -plane looks like a bounded between and","x=0, x=\frac{\pi}{2}, z=0,z=y, y=\cos(x). \int_{0}^{\frac{\pi}{2}}\int_{0}^{y}\int_{0}^{\cos(x)} \ dy \ dz \ dx = \int_{0}^{\frac{\pi}{2}}\int_{0}^{y} \cos(x) \ dz \ dx = \int_{0}^{\frac{\pi}{2}}y\cos(x) \ dx = y y xy \cos(x) 0 \frac{\pi}{2}",['integration']
