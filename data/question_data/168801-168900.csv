,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Average amount of money received from pulling three random coins,Average amount of money received from pulling three random coins,,"Let's say I have a bag of coins, which contains $1$ quarter, $2$ dimes, $3$ nickles and $4$ pennies.  If I were to randomly pull out $3$ coins, on average, how much money would I get?","Let's say I have a bag of coins, which contains $1$ quarter, $2$ dimes, $3$ nickles and $4$ pennies.  If I were to randomly pull out $3$ coins, on average, how much money would I get?",,['probability']
1,"partition set {1,2,...,9} into subsets of size 2 and 5?","partition set {1,2,...,9} into subsets of size 2 and 5?",,"I do some ex for preparing discrete mathematics exam, i get stuck in one problem, anyone could help me? How many ways we can partition set {1,2,...,9} into subsets of size 2   and 5? anyway, some tutorials for solving such a question... Edit: Like always Scott is Right...","I do some ex for preparing discrete mathematics exam, i get stuck in one problem, anyone could help me? How many ways we can partition set {1,2,...,9} into subsets of size 2   and 5? anyway, some tutorials for solving such a question... Edit: Like always Scott is Right...",,"['combinatorics', 'statistics', 'discrete-mathematics', 'combinations']"
2,New to generating functions - how do I get the function from the sequence defined by $a_n= n$ for $n\geqslant 0$?,New to generating functions - how do I get the function from the sequence defined by  for ?,a_n= n n\geqslant 0,"I'm given: $a_n= n$ for $n \geqslant 0$. I'm quite good at recursive generating functions, but I haven't came across a simpler one like this, so I'm sure I'm just overlooking something really basic.","I'm given: $a_n= n$ for $n \geqslant 0$. I'm quite good at recursive generating functions, but I haven't came across a simpler one like this, so I'm sure I'm just overlooking something really basic.",,"['combinatorics', 'statistics', 'discrete-mathematics', 'generating-functions']"
3,"The average of 10 scores is $25,$ and the lowest score is $20.$ So, the highest score must be...","The average of 10 scores is  and the lowest score is  So, the highest score must be...","25, 20.","The average score of $10$ students in a test is $25.$ The lowest score is $20.$ Then the highest score is: $$A.100,$$ $$B.70,$$ $$C.30,$$ $$D.75$$ The answer key suggests option $B$ as the answer. I don't understand why this differs from my solution: Let the marks obtained by each student be $x_1,x_2,...,x_{10}$ and $S=\sum_{i=1}^{10} x_i$ .  Then, we have $\frac{S}{10}=\frac{\sum_{i=1}^{10} x_i}{10}=20\implies S=200.$ If say, only one student obtained $25$ , i.e let $x_k=25$ , then, $$S-x_k=200-25=175.$$ Considering $S'=\sum_{i\neq k}x_i=175$ . Now, if $\exists x_p=100$ , then $100+(S'-x_p)=175\implies (S'-75)=75$ , thus we are getting a situation, where it's possible for a student to score hundred, then, the rest students, will have to score, $75$ marks in total. This is the highest option given. Hence, the highest possible  score can be $100.$ According to the information given, this can be a possible case, when 1 student scores 20, 8 students have a total score of  75 and one student scores 100, i.e then, average of this will still be 20. So, option $A$ is correct.","The average score of students in a test is The lowest score is Then the highest score is: The answer key suggests option as the answer. I don't understand why this differs from my solution: Let the marks obtained by each student be and .  Then, we have If say, only one student obtained , i.e let , then, Considering . Now, if , then , thus we are getting a situation, where it's possible for a student to score hundred, then, the rest students, will have to score, marks in total. This is the highest option given. Hence, the highest possible  score can be According to the information given, this can be a possible case, when 1 student scores 20, 8 students have a total score of  75 and one student scores 100, i.e then, average of this will still be 20. So, option is correct.","10 25. 20. A.100, B.70, C.30, D.75 B x_1,x_2,...,x_{10} S=\sum_{i=1}^{10} x_i \frac{S}{10}=\frac{\sum_{i=1}^{10} x_i}{10}=20\implies S=200. 25 x_k=25 S-x_k=200-25=175. S'=\sum_{i\neq k}x_i=175 \exists x_p=100 100+(S'-x_p)=175\implies (S'-75)=75 75 100. A","['algebra-precalculus', 'statistics', 'solution-verification', 'average', 'means']"
4,quant interview: (mathematical modelling) linear regression and statistical significance,quant interview: (mathematical modelling) linear regression and statistical significance,,"I am preparing a quantitative finance interview and I am struggling with this exercise: Consider two data series, X = (x1, x2, . . . , xn) and Y = (y1, y2, . . . , yn), both with mean zero. We use linear regression (ordinary least squares) to regress Y against X (without fitting any intercept), as in Y = aX + $\epsilon$ where $\epsilon$ denotes a series of error terms. Suppose that ρXY = 0.01. Is the resulting value of a statistically significantly different from 0 at the 95% level if: i. $n = 10^2$ \ ii. $n = 10^3$ \ iii. $n = 10^4$ \ I already know the relation between a and $\rho$ is given by $$a = \frac{\rho_{XY}}{\sigma_X}$$ But I am struggling with the confidence level part. Any help would be appreciated. Thank you!","I am preparing a quantitative finance interview and I am struggling with this exercise: Consider two data series, X = (x1, x2, . . . , xn) and Y = (y1, y2, . . . , yn), both with mean zero. We use linear regression (ordinary least squares) to regress Y against X (without fitting any intercept), as in Y = aX + where denotes a series of error terms. Suppose that ρXY = 0.01. Is the resulting value of a statistically significantly different from 0 at the 95% level if: i. \ ii. \ iii. \ I already know the relation between a and is given by But I am struggling with the confidence level part. Any help would be appreciated. Thank you!",\epsilon \epsilon n = 10^2 n = 10^3 n = 10^4 \rho a = \frac{\rho_{XY}}{\sigma_X},"['statistics', 'linear-regression', 'confidence-interval']"
5,Random Variable,Random Variable,,"I was wondering if I correctly understand what a random variable is. Is a random variable's domain the set of numbers that are reasonable, when considering how the random variable is defined. For instance, $X =$ number of broken eggs in a dozen-egg cartoon. Would the domain be $\{0,1,2,3,4,....12\}$? And when you apply a function to the random variable, it will associate a number from the domain, to an element in the sample same, thereby giving some output? Taking our example, $(X=12)$ would take the domain value 12, and associate to it the element, from the sample space, $(B,~B,~B,~B,~B,~B,~B,~B,~B,~B,~B,~B)$? If I am using any notation incorrectly, please be prompt to remonstrate.","I was wondering if I correctly understand what a random variable is. Is a random variable's domain the set of numbers that are reasonable, when considering how the random variable is defined. For instance, $X =$ number of broken eggs in a dozen-egg cartoon. Would the domain be $\{0,1,2,3,4,....12\}$? And when you apply a function to the random variable, it will associate a number from the domain, to an element in the sample same, thereby giving some output? Taking our example, $(X=12)$ would take the domain value 12, and associate to it the element, from the sample space, $(B,~B,~B,~B,~B,~B,~B,~B,~B,~B,~B,~B)$? If I am using any notation incorrectly, please be prompt to remonstrate.",,"['probability', 'statistics', 'random-variables']"
6,How to find the expectation of $\log x$？,How to find the expectation of ？,\log x,"Let $X_1,...,X_n$ be a random sample. The pdf is $f(x\mid\theta)=\theta x^{\theta-1},0<x<1,\theta>0$ . I want to know $\mathbf{E}(\log x)$ . $$\int_0^1 \theta x^{\theta-1} \log x \;dx$$ I don't know how to solve this integral.",Let be a random sample. The pdf is . I want to know . I don't know how to solve this integral.,"X_1,...,X_n f(x\mid\theta)=\theta x^{\theta-1},0<x<1,\theta>0 \mathbf{E}(\log x) \int_0^1 \theta x^{\theta-1} \log x \;dx","['probability', 'integration', 'statistics']"
7,How to calculate expected value for piecewise constant distribution function?,How to calculate expected value for piecewise constant distribution function?,,"The distribution function of a discrete random variable X is given $F_X(x)=\begin{cases} 0, &x<1\\ \frac{5}{13},& 1\leq x< 2  \\ \frac{10}{13}, & 2\leq x<3 \\ \frac{11}{13}, & 3\leq x<4 \\ 1, & 4\leq x   \end{cases} $ $A=(X=2)\cup  (X=4)$ Calculate: $P(A)$ and $E(X)$ I was thinking to solve $P(A)$ with formula: $P(a)=\begin{pmatrix} n \\ a  \end{pmatrix} p^a (1-p)^{n-a} $, but I dont $p$ and $n$. Which formula I should use?","The distribution function of a discrete random variable X is given $F_X(x)=\begin{cases} 0, &x<1\\ \frac{5}{13},& 1\leq x< 2  \\ \frac{10}{13}, & 2\leq x<3 \\ \frac{11}{13}, & 3\leq x<4 \\ 1, & 4\leq x   \end{cases} $ $A=(X=2)\cup  (X=4)$ Calculate: $P(A)$ and $E(X)$ I was thinking to solve $P(A)$ with formula: $P(a)=\begin{pmatrix} n \\ a  \end{pmatrix} p^a (1-p)^{n-a} $, but I dont $p$ and $n$. Which formula I should use?",,"['probability', 'statistics']"
8,Statistics: Conditional Probability,Statistics: Conditional Probability,,"$P(A│B)=\frac25$  ,$P(B)=\frac14$, $P(A)=\frac13$. Find $P(A\land B)$ $P(B|A)$ Here is what I did: Part 1. $$P(A\land B) = P(A) \cdot P(B)\\ = \frac13\cdot\frac14=\frac{1}{12}$$ Part 2. $$P(B|A) = \frac{P(B\land A)}{P(A)} = \frac{\frac1{12}}{\frac13}=\frac14$$ I'm not too sure if my answers are right and I will like them checked over. Thanks","$P(A│B)=\frac25$  ,$P(B)=\frac14$, $P(A)=\frac13$. Find $P(A\land B)$ $P(B|A)$ Here is what I did: Part 1. $$P(A\land B) = P(A) \cdot P(B)\\ = \frac13\cdot\frac14=\frac{1}{12}$$ Part 2. $$P(B|A) = \frac{P(B\land A)}{P(A)} = \frac{\frac1{12}}{\frac13}=\frac14$$ I'm not too sure if my answers are right and I will like them checked over. Thanks",,"['probability', 'statistics']"
9,Existence of complete sufficient statistics,Existence of complete sufficient statistics,,"Suppose $X_1,\ldots,X_n$ are iid r.v.'s, each with pdf $f_{\theta}(x)=\frac{1}{\theta}I\{\theta<x<2\theta\}$. I find the minimal sufficient statistics $(X_{(1)},X_{(n)})$. I am trying to prove it is complete. Can someone give me hint? Also are there any complete sufficient statistics in this model?","Suppose $X_1,\ldots,X_n$ are iid r.v.'s, each with pdf $f_{\theta}(x)=\frac{1}{\theta}I\{\theta<x<2\theta\}$. I find the minimal sufficient statistics $(X_{(1)},X_{(n)})$. I am trying to prove it is complete. Can someone give me hint? Also are there any complete sufficient statistics in this model?",,"['statistics', 'statistical-inference']"
10,Probability of $x<y^2$ where $x$ and $y$ are uniformly distributed.,Probability of  where  and  are uniformly distributed.,x<y^2 x y,"Suppose $x$ and $y$ are uniformly distiruted between $[-L;L]$, what is the probability that $x<y^2$? I found this 5-year old discussion: http://answers.yahoo.com/question/index?qid=20080308204357AAEdyRP&r=w but it is long and lot very illustrative. Edit: OK, I understand the idea. I just don't understand why there is need to seperate $L<1$? Why can't you just take and integrate $y^2$ in interval $[-L,L]$ then add $2L^2$ (area below 0) and divide everything with $4L^2$ (all area)?","Suppose $x$ and $y$ are uniformly distiruted between $[-L;L]$, what is the probability that $x<y^2$? I found this 5-year old discussion: http://answers.yahoo.com/question/index?qid=20080308204357AAEdyRP&r=w but it is long and lot very illustrative. Edit: OK, I understand the idea. I just don't understand why there is need to seperate $L<1$? Why can't you just take and integrate $y^2$ in interval $[-L,L]$ then add $2L^2$ (area below 0) and divide everything with $4L^2$ (all area)?",,['statistics']
11,Scaling a uniform distribution - Probability,Scaling a uniform distribution - Probability,,"I just have a simple question on scaling a uniform distribution.  I know that uniform distribution has probability density of $1/(b-a)$ defined on the interval a to b. My textbook says that we can scale the distribution to be between (0,1) and have a constant density of 1 by doing the following: Suppose X is a random variable. Then $U=(X-a)/(b-a)$ so $X=a+(b-a)U$. Thus the expected value E(X) = E($a + (b -a)U$) which equals $(a+b)/2$. I don't understand why we subtract a from X and then divide by b - a. The intuition just doesn't make sense to me. -How does this make it so that the distribution is defined from 0 to 1 with density 1 instead of the original definition on (a,b) with density $1/(b-a)$? -Also, what is the mathematically correct way to derive E(U)?","I just have a simple question on scaling a uniform distribution.  I know that uniform distribution has probability density of $1/(b-a)$ defined on the interval a to b. My textbook says that we can scale the distribution to be between (0,1) and have a constant density of 1 by doing the following: Suppose X is a random variable. Then $U=(X-a)/(b-a)$ so $X=a+(b-a)U$. Thus the expected value E(X) = E($a + (b -a)U$) which equals $(a+b)/2$. I don't understand why we subtract a from X and then divide by b - a. The intuition just doesn't make sense to me. -How does this make it so that the distribution is defined from 0 to 1 with density 1 instead of the original definition on (a,b) with density $1/(b-a)$? -Also, what is the mathematically correct way to derive E(U)?",,['statistics']
12,Proportion of Values,Proportion of Values,,"I'm not quite sure how to do this problem: Calculate the the proportion of values that is 3 in the following data set: 2, 3, 3, 6, 9","I'm not quite sure how to do this problem: Calculate the the proportion of values that is 3 in the following data set: 2, 3, 3, 6, 9",,['statistics']
13,How can I solve the expected number of frog jumps problem?,How can I solve the expected number of frog jumps problem?,,"A frog sits on the real number line at 0. It makes repeated jumps of random distance forward. For every jump, the frog advances by a random amount, drawn (independently) from the uniform distribution over $U([0, 1])$ . The frog stops once it has reached or surpassed 1. How many jumps does the frog make on average? What’s the standard deviation? Here is my answer: Let $N$ be a random variable with possible values 2, 3, 4, ... which represents the number of jumps the frog makes immediately after it has reached or surpassed 1. We will neglect the possibility of only one jump being required. Let $X_1$ , $X_2$ , ... $X_n$ be a set of random variates taken from $U([0,1])$ . Let $$S_n = \sum_{i=1}^n X_i.$$ For $n\ge2$ , the probability that $N=n$ is given by $$ \begin{aligned} P(N=n) &= P[(S_n \ge 1) \cap (S_{n-1}<1)] \\ &= P(S_n \ge 1)P(S_{n-1}<1). \end{aligned} $$ From the CDF of the Irwin-Hall distribution we know that $$P(S_n\le x)=\sum_{k=0}^n\frac{(-1)^k}{n!(n-k)!}(x-k)^n_+.$$ Hence, $$P(S_n\le 1)=\frac{1}{n!}.$$ Similarly, $$P(S_{n-1}\le 1)=\frac{1}{(n-1)!},$$ $$P(S_n > 1)=1 - \frac{1}{n!},$$ $$\implies P(N=n)=\frac{1}{(n-1)!} - \frac{1}{n!(n-1)!}.$$ Hence the expected value of $N$ (i.e. the average number of jumps) is given by (see WolframAlpha ), $$\begin{aligned} E(N)&=\sum_{n=2}^\infty nP(N=n) \\ &=\sum_{n=2}^\infty \frac{n}{(n-1)!}\left(1-\frac{1}{n!}\right) \\ &= 2e - I_0(2) \\ &\approx 3.1570. \end{aligned}$$ Let $\mu = E(N)$ . Now we need to calculate, $$E[(N-\mu)^2] = E(N^2) - \mu^2.$$ The first term is given by ( see WolframAlpha ): $$\begin{aligned} E(N^2) &= \sum_{n=2}^\infty n^2P(N^2=n^2) \\ &= \sum_{n=2}^\infty n^2P(N=n) \\\ &=\sum_{n=2}^\infty \frac{n^2}{(n-1)!}\left(1-\frac{1}{n!}\right) \\ &=5e-I_0(2) - I_1(2)\\ &\approx 9.7212. \end{aligned}$$ Hence the standard deviation, $\sigma$ is approximately (see WolframAlpha ), $$ \begin{aligned} \sigma \approx 0.2185. \end{aligned} $$ However, when I check these results using the code below, it seems that $$\mu\approx 2.7222 \approx e?,$$ $$\sigma \approx 0.8752.$$ Can you see where I went wrong? import numpy as np  num_trials = int(1e5) N = np.zeros(num_trials) for n in range(num_trials):     X = 0     while X < 1:         N[n] += 1         X += np.random.uniform()  print(np.mean(N)) print(np.std(N))","A frog sits on the real number line at 0. It makes repeated jumps of random distance forward. For every jump, the frog advances by a random amount, drawn (independently) from the uniform distribution over . The frog stops once it has reached or surpassed 1. How many jumps does the frog make on average? What’s the standard deviation? Here is my answer: Let be a random variable with possible values 2, 3, 4, ... which represents the number of jumps the frog makes immediately after it has reached or surpassed 1. We will neglect the possibility of only one jump being required. Let , , ... be a set of random variates taken from . Let For , the probability that is given by From the CDF of the Irwin-Hall distribution we know that Hence, Similarly, Hence the expected value of (i.e. the average number of jumps) is given by (see WolframAlpha ), Let . Now we need to calculate, The first term is given by ( see WolframAlpha ): Hence the standard deviation, is approximately (see WolframAlpha ), However, when I check these results using the code below, it seems that Can you see where I went wrong? import numpy as np  num_trials = int(1e5) N = np.zeros(num_trials) for n in range(num_trials):     X = 0     while X < 1:         N[n] += 1         X += np.random.uniform()  print(np.mean(N)) print(np.std(N))","U([0, 1]) N X_1 X_2 X_n U([0,1]) S_n = \sum_{i=1}^n X_i. n\ge2 N=n 
\begin{aligned}
P(N=n) &= P[(S_n \ge 1) \cap (S_{n-1}<1)] \\
&= P(S_n \ge 1)P(S_{n-1}<1).
\end{aligned}
 P(S_n\le x)=\sum_{k=0}^n\frac{(-1)^k}{n!(n-k)!}(x-k)^n_+. P(S_n\le 1)=\frac{1}{n!}. P(S_{n-1}\le 1)=\frac{1}{(n-1)!}, P(S_n > 1)=1 - \frac{1}{n!}, \implies P(N=n)=\frac{1}{(n-1)!} - \frac{1}{n!(n-1)!}. N \begin{aligned}
E(N)&=\sum_{n=2}^\infty nP(N=n) \\
&=\sum_{n=2}^\infty \frac{n}{(n-1)!}\left(1-\frac{1}{n!}\right) \\
&= 2e - I_0(2) \\
&\approx 3.1570.
\end{aligned} \mu = E(N) E[(N-\mu)^2] = E(N^2) - \mu^2. \begin{aligned}
E(N^2) &= \sum_{n=2}^\infty n^2P(N^2=n^2) \\
&= \sum_{n=2}^\infty n^2P(N=n) \\\
&=\sum_{n=2}^\infty \frac{n^2}{(n-1)!}\left(1-\frac{1}{n!}\right) \\
&=5e-I_0(2) - I_1(2)\\
&\approx 9.7212.
\end{aligned} \sigma 
\begin{aligned}
\sigma \approx 0.2185.
\end{aligned}
 \mu\approx 2.7222 \approx e?, \sigma \approx 0.8752.","['probability', 'statistics', 'expected-value', 'standard-deviation']"
14,Absorbing Markov chain but the absorbing states don't absorb right away,Absorbing Markov chain but the absorbing states don't absorb right away,,"What's the name of a Markov chain where there are states which absorb if they return to themselves twice in a row? Say I have $5$ states, and state $5$ absorbs if it gets hit twice in a row. For example, the sequence $1\rightarrow 5\rightarrow 3\rightarrow \cdots$ is possible but if we have $1\rightarrow 3\rightarrow 5\rightarrow 5 \rightarrow \cdots$ then all subsequent states are necessarily $5$ . I've seen some rough work with these where it seems that some sort of ""exit matrix"" is subtracted from the transition matrix in order to calculate average number of steps and whatnot but I can't find any information about these particular kinds of Markov chains as I don't know what they're called.","What's the name of a Markov chain where there are states which absorb if they return to themselves twice in a row? Say I have states, and state absorbs if it gets hit twice in a row. For example, the sequence is possible but if we have then all subsequent states are necessarily . I've seen some rough work with these where it seems that some sort of ""exit matrix"" is subtracted from the transition matrix in order to calculate average number of steps and whatnot but I can't find any information about these particular kinds of Markov chains as I don't know what they're called.",5 5 1\rightarrow 5\rightarrow 3\rightarrow \cdots 1\rightarrow 3\rightarrow 5\rightarrow 5 \rightarrow \cdots 5,"['statistics', 'reference-request', 'markov-chains']"
15,"$f(x,y)=\frac{1}{2x^2y}$ , $1\le x<\infty $ , $\frac{1}{x}\le y<x$ marginal of X and Y is?",",  ,  marginal of X and Y is?","f(x,y)=\frac{1}{2x^2y} 1\le x<\infty  \frac{1}{x}\le y<x","$f(x,y)=\dfrac{1}{2x^2y}$ , $1\le x<\infty $ , $\dfrac{1}{x}\le y<x$ Derive marginal probability density function of $X$ and $Y$ I have a problem in calculating marginal of Y. We have to calculate $\int_{x}f(x,y)dx$ Usually, I draw graphs to get my limits of integration first before calculating marginal distribution but this time I am unable to figure out limits. Graph is Function changes. I have $y<x$ and $y<1/x$ as well . So how do I calculate marginal in this case?",", , Derive marginal probability density function of and I have a problem in calculating marginal of Y. We have to calculate Usually, I draw graphs to get my limits of integration first before calculating marginal distribution but this time I am unable to figure out limits. Graph is Function changes. I have and as well . So how do I calculate marginal in this case?","f(x,y)=\dfrac{1}{2x^2y} 1\le x<\infty  \dfrac{1}{x}\le y<x X Y \int_{x}f(x,y)dx y<x y<1/x","['probability', 'statistics', 'probability-distributions']"
16,Let $X$ and $Y$ be independent and identically distributed random variables with moment generating function then $E(\dfrac{e^{tX}}{e^{tY}})$,Let  and  be independent and identically distributed random variables with moment generating function then,X Y E(\dfrac{e^{tX}}{e^{tY}}),Let $X$ and $Y$ be independent and identically distributed random variables with moment generating function $M(t)=E(e^{tX});\ \ -\infty<t<\infty$  then $E(\dfrac{e^{tX}}{e^{tY}})$ equals ? $(A)=M(t)M(-t)$ $(B)=1$ $(C)=(M(t))^2$ $(D)=\frac{M(t)}{M(-t)}$ My input: Since its given that random variables are i.i.d so  $E(\dfrac{\require{\cancel}\cancel{e^{tX}}}{\cancel{e^{tY}}})=E(1)=1 $ can I do that?,Let $X$ and $Y$ be independent and identically distributed random variables with moment generating function $M(t)=E(e^{tX});\ \ -\infty<t<\infty$  then $E(\dfrac{e^{tX}}{e^{tY}})$ equals ? $(A)=M(t)M(-t)$ $(B)=1$ $(C)=(M(t))^2$ $(D)=\frac{M(t)}{M(-t)}$ My input: Since its given that random variables are i.i.d so  $E(\dfrac{\require{\cancel}\cancel{e^{tX}}}{\cancel{e^{tY}}})=E(1)=1 $ can I do that?,,"['probability', 'statistics', 'moment-generating-functions']"
17,Is conditional probability transitive?,Is conditional probability transitive?,,"Is conditional probability ""transitive"" (or how else this is called - please, explicate) in the meaning that $P(a \mid c)=P(a \mid b)P(b \mid c)$ ? Intuitively this seems so, but could you comment/proove on how to understand this (or refute)?","Is conditional probability ""transitive"" (or how else this is called - please, explicate) in the meaning that $P(a \mid c)=P(a \mid b)P(b \mid c)$ ? Intuitively this seems so, but could you comment/proove on how to understand this (or refute)?",,"['probability', 'statistics', 'proof-explanation']"
18,Probability that my roll on a die will be higher than yours: Why divide by 6?,Probability that my roll on a die will be higher than yours: Why divide by 6?,,"I have to work out a question where on two fair, $6$ sided dice, what is the probability that the second die gives me a higher number than the first die. So I broke the question down the long way and said ""If you roll a $1$, I have a $\frac{5}{6}$ chance of beating you, if you roll a $2$, I have a $\frac{4}{6}$ chance of beating you, etc."" Then I added all these up but got the answer to be $\frac{5}{2}$. More research showed that I was actually supposed to divide this number by $6$ to get my probability to be $\frac{15}{36} = \frac{5}{12}$, but I can't see why you divide it by $6$. Can someone explain this to me please? EDIT: The bit I am struggling with is the fact that why do we take the draw into consideration. Thank you to everyone who has commented and answered, I now get that the division by 6 is because we include the probability of drawing. But in my specific question, there was nothing about a tie, I simply have to beat you, or I lose. So why do we still include the possibility of a draw, when that's not part of the game?","I have to work out a question where on two fair, $6$ sided dice, what is the probability that the second die gives me a higher number than the first die. So I broke the question down the long way and said ""If you roll a $1$, I have a $\frac{5}{6}$ chance of beating you, if you roll a $2$, I have a $\frac{4}{6}$ chance of beating you, etc."" Then I added all these up but got the answer to be $\frac{5}{2}$. More research showed that I was actually supposed to divide this number by $6$ to get my probability to be $\frac{15}{36} = \frac{5}{12}$, but I can't see why you divide it by $6$. Can someone explain this to me please? EDIT: The bit I am struggling with is the fact that why do we take the draw into consideration. Thank you to everyone who has commented and answered, I now get that the division by 6 is because we include the probability of drawing. But in my specific question, there was nothing about a tie, I simply have to beat you, or I lose. So why do we still include the possibility of a draw, when that's not part of the game?",,"['probability', 'algebra-precalculus', 'statistics', 'dice']"
19,Density of points uniformly distributed in a sphere,Density of points uniformly distributed in a sphere,,"Suppose we have a set of points in $R^3$ distributed uniformly in a sphere. Suppose we cut up the sphere in spherical shells of uniform thickness $\Delta r=r_n-r_{n-1}$. I want to find the density of points in each spherical shell. Since outer shells are larger than inner shells, a proper measure of density would have to be somehow normalized. The straightforward answer would be to divide the number of points in each spherical shell by its volume, $\frac{4}{3}\pi (r_n^3 - r_{n-1}^3)$ and that would give a normalized density. Here's some plots. X-axis: radial distance from center of sphere. Y-axis: number of points in corresponding spherical shell This is density before normalizing for volume: This is density after normalizing for volume: Perhaps, refining this process will give a straight line for the normalized density and indeed this is the correct way of measuring what I want. However, taking alot more points and cutting up the sphere in much finer partitions, the results don't get drastically better. Perhaps it's an instance of slow convergence. But perhaps this method really has trouble as you approach the origin. Any ideas? I have no stat background, is this the way to go normalizing this set of points, are there other ways I'm not aware of that might deal better with points closer to the center of the sphere? Thanks","Suppose we have a set of points in $R^3$ distributed uniformly in a sphere. Suppose we cut up the sphere in spherical shells of uniform thickness $\Delta r=r_n-r_{n-1}$. I want to find the density of points in each spherical shell. Since outer shells are larger than inner shells, a proper measure of density would have to be somehow normalized. The straightforward answer would be to divide the number of points in each spherical shell by its volume, $\frac{4}{3}\pi (r_n^3 - r_{n-1}^3)$ and that would give a normalized density. Here's some plots. X-axis: radial distance from center of sphere. Y-axis: number of points in corresponding spherical shell This is density before normalizing for volume: This is density after normalizing for volume: Perhaps, refining this process will give a straight line for the normalized density and indeed this is the correct way of measuring what I want. However, taking alot more points and cutting up the sphere in much finer partitions, the results don't get drastically better. Perhaps it's an instance of slow convergence. But perhaps this method really has trouble as you approach the origin. Any ideas? I have no stat background, is this the way to go normalizing this set of points, are there other ways I'm not aware of that might deal better with points closer to the center of the sphere? Thanks",,"['statistics', 'statistical-inference', 'uniform-distribution']"
20,A Game of Coin and Die,A Game of Coin and Die,,"This game is played with a fair coin and a die. First player flips a coin. If it turns out head(H), the player proceeds with tossing a die. If it turns out tail(T), the player proceeds with flipping a coin for the second time. The player wins if it gets head on the first tossing and 6 on the second or tails on both flips of coin. What is the probability of winning a game?","This game is played with a fair coin and a die. First player flips a coin. If it turns out head(H), the player proceeds with tossing a die. If it turns out tail(T), the player proceeds with flipping a coin for the second time. The player wins if it gets head on the first tossing and 6 on the second or tails on both flips of coin. What is the probability of winning a game?",,"['probability', 'statistics']"
21,Escaping Prisoner Probability Question,Escaping Prisoner Probability Question,,"Question: A prisoner is trapped in a cell containing three doors. The first door leads to a tunnel which returns him to his cell after two days travel. The second door leads to a tunnel that returns him to his cell after three days travel. The third door leads immediately to freedom. (a) Assuming the prisoner will always select doors 1, 2 and 3 with probabilities 0.5, 0.3 and 0.2 respectively, what is the expected number of days until he reaches freedom? (b) Calculate the variance of the number of days until the prisoner reaches freedom. My Attempt so far: I've used first step decomposition to get an answer of $9.5$ days to the first part of the question, but I have no idea how to find the variance for part (b). I've tried using the Law of Total Variance but got nowhere. I don't know whether you're supposed to use $Var(X) = E(X^2) - (E(X))^2$, where $X$ is the number of days taken to reach freedom. If so, how do you go about finding $E(X^2)$? Any help would be greatly appreciated.","Question: A prisoner is trapped in a cell containing three doors. The first door leads to a tunnel which returns him to his cell after two days travel. The second door leads to a tunnel that returns him to his cell after three days travel. The third door leads immediately to freedom. (a) Assuming the prisoner will always select doors 1, 2 and 3 with probabilities 0.5, 0.3 and 0.2 respectively, what is the expected number of days until he reaches freedom? (b) Calculate the variance of the number of days until the prisoner reaches freedom. My Attempt so far: I've used first step decomposition to get an answer of $9.5$ days to the first part of the question, but I have no idea how to find the variance for part (b). I've tried using the Law of Total Variance but got nowhere. I don't know whether you're supposed to use $Var(X) = E(X^2) - (E(X))^2$, where $X$ is the number of days taken to reach freedom. If so, how do you go about finding $E(X^2)$? Any help would be greatly appreciated.",,"['probability', 'statistics']"
22,Entropy for three random variables [duplicate],Entropy for three random variables [duplicate],,"This question already has answers here : Calculating conditional entropy given two random variables (2 answers) Closed 9 years ago . I'm just working through some information theory and entropy, and I've come into a bit of a problem. In many texts, it's easy to find the ""chain rule"" for entropy in two variables, and the ""conditional chain rule"" for three variables, respectively; $$H(Y|X) = H(X,Y) - H(X)$$ $$H(X,Y|Z) = H(Y|Z) + H(X|Y,Z) = H(X|Z) + H(Y|X,Z)$$ However, I'm trying to determine the entropy of three random variables: $H(X,Y,Z)$. I haven't done a lot of probability/statistics before, and googling hasn't really turned up anything too fruitful. Can anyone help me derive this result??","This question already has answers here : Calculating conditional entropy given two random variables (2 answers) Closed 9 years ago . I'm just working through some information theory and entropy, and I've come into a bit of a problem. In many texts, it's easy to find the ""chain rule"" for entropy in two variables, and the ""conditional chain rule"" for three variables, respectively; $$H(Y|X) = H(X,Y) - H(X)$$ $$H(X,Y|Z) = H(Y|Z) + H(X|Y,Z) = H(X|Z) + H(Y|X,Z)$$ However, I'm trying to determine the entropy of three random variables: $H(X,Y,Z)$. I haven't done a lot of probability/statistics before, and googling hasn't really turned up anything too fruitful. Can anyone help me derive this result??",,"['probability', 'statistics', 'information-theory', 'entropy']"
23,Finding the ACF of AR(1) process,Finding the ACF of AR(1) process,,"For an AR(1) process: $X_{t} = \phi X_{t-1} + w_{t}$ with $w_{t} \sim N(0,\sigma^{2})$ How do you derive the ACF of the process? Since $E[X_{t}] = 0$, would you just calculate $cov(\phi X_{t-1} + w_{t},\phi X_{t+h-1} + w_{t+h}) = \phi^{2} E[(X_{t-1}*X_{t-1+h})] + \sigma^{2}$. I am having trouble simplifying this expression specifically the $E[(X_{t-1}*X_{t-1+h})$ term.","For an AR(1) process: $X_{t} = \phi X_{t-1} + w_{t}$ with $w_{t} \sim N(0,\sigma^{2})$ How do you derive the ACF of the process? Since $E[X_{t}] = 0$, would you just calculate $cov(\phi X_{t-1} + w_{t},\phi X_{t+h-1} + w_{t+h}) = \phi^{2} E[(X_{t-1}*X_{t-1+h})] + \sigma^{2}$. I am having trouble simplifying this expression specifically the $E[(X_{t-1}*X_{t-1+h})$ term.",,"['probability', 'statistics']"
24,The sum of $n$ independent normal random variables.,The sum of  independent normal random variables.,n,"How can I prove that the sum of $X_1, X_2, \ldots,X_n$ random variables, all of which have normal distributions $N(\mu_i, \sigma_i)$, is a random variable that is itself normally distributed with mean $$\mu =\sum_{i=1}^n \mu_i$$ and variance $$\sigma^2 = \sum_{i=1}^n \sigma_i^2$$ Edit: I forgot to add that this was with the assumption that all $X_1, X_2,\ldots,X_n$ are independent.","How can I prove that the sum of $X_1, X_2, \ldots,X_n$ random variables, all of which have normal distributions $N(\mu_i, \sigma_i)$, is a random variable that is itself normally distributed with mean $$\mu =\sum_{i=1}^n \mu_i$$ and variance $$\sigma^2 = \sum_{i=1}^n \sigma_i^2$$ Edit: I forgot to add that this was with the assumption that all $X_1, X_2,\ldots,X_n$ are independent.",,"['probability', 'statistics', 'normal-distribution', 'random-variables']"
25,$P(B|A)=1-P(\sim B|A)$ are these equivalent?,are these equivalent?,P(B|A)=1-P(\sim B|A),"The probability of $B$ given $A$ and one minus the probablity of $\sim B$ given $A$. So, $$P(B|A)=1-P(\sim B|A)$$ The events are not necessarily mutually exclusive, I just wanted to know if the statements are equivalent? I just started statistics so I don't know how much more detail I can give, please let me know in your comments.","The probability of $B$ given $A$ and one minus the probablity of $\sim B$ given $A$. So, $$P(B|A)=1-P(\sim B|A)$$ The events are not necessarily mutually exclusive, I just wanted to know if the statements are equivalent? I just started statistics so I don't know how much more detail I can give, please let me know in your comments.",,"['probability', 'statistics']"
26,Expectation of Random Walk,Expectation of Random Walk,,"At each time step, I have 1/2 probability of walking one step to the right, and  the same probability of walking one step to the left. Let X be the random variable corresponding to the final position of the $n$ step I walk. Compute a) $E[X^4]$ for this random variable b) Show that $P(|X|>c) \le \dfrac{E[X^4]}{c^4}$ My thought: I tried to use the definition of expectation to compute but the polynomial of degree 4 got really messy. I was wondering if there is an elegant way to approach this problem. And I also tried to use generating function but how to write the generating function for this random variable.","At each time step, I have 1/2 probability of walking one step to the right, and  the same probability of walking one step to the left. Let X be the random variable corresponding to the final position of the $n$ step I walk. Compute a) $E[X^4]$ for this random variable b) Show that $P(|X|>c) \le \dfrac{E[X^4]}{c^4}$ My thought: I tried to use the definition of expectation to compute but the polynomial of degree 4 got really messy. I was wondering if there is an elegant way to approach this problem. And I also tried to use generating function but how to write the generating function for this random variable.",,"['probability', 'statistics', 'random-walk']"
27,Assumption of Normality in Central Limit Theorem,Assumption of Normality in Central Limit Theorem,,"Regarding the distributions of random variables, I understand the following points: The distribution of the expected value (i.e. ""mean"") of any random variable is (asymptotically) normally distributed - this is stated within the Central Limit Theorem The sums and differences of normally distributed random variables are also normally distributed This being said, now consider the popular ""T-Test"" . The T-Test can be used to determine if the difference between the ""mean value"" of some random variable from two samples are equal or not - in this case, we can consider this ""value"" as a random variable. And since we know that the difference in the ""mean values"" of any two random variables follows a normal distribution, the T-Test exploits this fact and thereby uses the normal distribution to determine the mean differences between two samples is statistically significant or not. Something I have never quite been able to understand: In a T-Test, we are told that we  require the underlying distribution of both samples to be normally distributed - yet, the difference between the mean values of ANY two random variables is normally distributed . My Question: Thus, why is the assumption of normality required in a T-Test when we know that the difference between the mean values of any two random variables is always normally distributed (provided there are enough observations)? I can understand why this might not be the case when we have very few observations in each sample - but when we have many observations in both samples, why is the assumption of bormality still required for the T-Test? Thanks!","Regarding the distributions of random variables, I understand the following points: The distribution of the expected value (i.e. ""mean"") of any random variable is (asymptotically) normally distributed - this is stated within the Central Limit Theorem The sums and differences of normally distributed random variables are also normally distributed This being said, now consider the popular ""T-Test"" . The T-Test can be used to determine if the difference between the ""mean value"" of some random variable from two samples are equal or not - in this case, we can consider this ""value"" as a random variable. And since we know that the difference in the ""mean values"" of any two random variables follows a normal distribution, the T-Test exploits this fact and thereby uses the normal distribution to determine the mean differences between two samples is statistically significant or not. Something I have never quite been able to understand: In a T-Test, we are told that we  require the underlying distribution of both samples to be normally distributed - yet, the difference between the mean values of ANY two random variables is normally distributed . My Question: Thus, why is the assumption of normality required in a T-Test when we know that the difference between the mean values of any two random variables is always normally distributed (provided there are enough observations)? I can understand why this might not be the case when we have very few observations in each sample - but when we have many observations in both samples, why is the assumption of bormality still required for the T-Test? Thanks!",,"['probability', 'statistics', 'statistical-inference', 'hypothesis-testing', 'central-limit-theorem']"
28,What does pairwise disjoint mean?,What does pairwise disjoint mean?,,"Say I have a set, $\{a,mc,d\}$ . Would $\{a,mc\}$ , $\{a,d\}$ , and $\{mc,d\}$ be pairwise disjoint? And what about $\{b,g\}$ , is $\{b,b\}$ , $\{g,g\}$ pairwise disjoint?","Say I have a set, . Would , , and be pairwise disjoint? And what about , is , pairwise disjoint?","\{a,mc,d\} \{a,mc\} \{a,d\} \{mc,d\} \{b,g\} \{b,b\} \{g,g\}","['probability', 'statistics', 'discrete-mathematics']"
29,"Maximum likelihood estimator for uniform distribution $U(-\theta, 0)$",Maximum likelihood estimator for uniform distribution,"U(-\theta, 0)","Consider $X_1,X_2,...,X_n$ i.i.d $U(-\theta,0)$. I want to find the maximum likelihood estimator of $\theta$. I know that $f(x,\theta)=\frac{1}{\theta}$ for $-\theta < x < 0$ and that $L_n(\theta, x)= \frac{1}{\theta^n}$. If we were looking at $U(0,\theta)$, then the MLE of $\theta$ would be $x_{(n)}$ because $L_n(\theta, x)= \frac{1}{\theta^n}$ is decreasing from $0 < x < \theta$ and would thus be maximized at the max $x_i$, which is $x_{(n)}$. For my case, since $L_n(\theta, x)= \frac{1}{\theta^n}$ is an increasing function for $-\theta < x < 0$, then $L_n(\theta, x)= \frac{1}{\theta^n}$ will be maximized at the max $x_i$, and thus the MLE of $\theta$ will be $x_{(n)}$ as well. I think this is correct, but it seems very silly to me that for both cases you can just say that it will be maximized at the max $x_i$. Could someone better explain this to me?","Consider $X_1,X_2,...,X_n$ i.i.d $U(-\theta,0)$. I want to find the maximum likelihood estimator of $\theta$. I know that $f(x,\theta)=\frac{1}{\theta}$ for $-\theta < x < 0$ and that $L_n(\theta, x)= \frac{1}{\theta^n}$. If we were looking at $U(0,\theta)$, then the MLE of $\theta$ would be $x_{(n)}$ because $L_n(\theta, x)= \frac{1}{\theta^n}$ is decreasing from $0 < x < \theta$ and would thus be maximized at the max $x_i$, which is $x_{(n)}$. For my case, since $L_n(\theta, x)= \frac{1}{\theta^n}$ is an increasing function for $-\theta < x < 0$, then $L_n(\theta, x)= \frac{1}{\theta^n}$ will be maximized at the max $x_i$, and thus the MLE of $\theta$ will be $x_{(n)}$ as well. I think this is correct, but it seems very silly to me that for both cases you can just say that it will be maximized at the max $x_i$. Could someone better explain this to me?",,"['statistics', 'maximum-likelihood']"
30,What's the chance for the closest whole number to $\frac{A}{B}$ is even?,What's the chance for the closest whole number to  is even?,\frac{A}{B},"Both $A$ and $B$ are a random number from the $\left [ 0;1 \right ]$ interval. I don't know how to calculate it, so i've made an estimation with excel and 1 million test, and i've got $0.214633$. But i would need the exact number.","Both $A$ and $B$ are a random number from the $\left [ 0;1 \right ]$ interval. I don't know how to calculate it, so i've made an estimation with excel and 1 million test, and i've got $0.214633$. But i would need the exact number.",,"['probability', 'statistics', 'contest-math']"
31,Why arrival time in Poisson process has uniform distribution given that a single arrival occurred in an interval [closed],Why arrival time in Poisson process has uniform distribution given that a single arrival occurred in an interval [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Consider a Poisson process. Given that a single arrival occurred in a given interval [0,t], why is the resulting distribution for the arrival time uniform?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Consider a Poisson process. Given that a single arrival occurred in a given interval [0,t], why is the resulting distribution for the arrival time uniform?",,"['probability', 'statistics', 'poisson-distribution', 'poisson-process']"
32,Prove that $E (\overline{X} - \mu)^2 = \frac{1}{n}\sigma^2$,Prove that,E (\overline{X} - \mu)^2 = \frac{1}{n}\sigma^2,"How to prove $E (\overline{X} - \mu)^2  = \frac{1}{n}\sigma^2$ (from wiki ), where $\overline{X}$ - is the sample mean ? What I have so far: \begin{align} E (\overline{X} - \mu)^2  = \frac{1}{n}\sigma^2 &=\\  &= E (\overline{X}^2 + \mu^2 - 2\mu\overline{X}) = E(\overline{X}^2) - \mu^2 =\\ &= \frac{\sum_{i=1}^n\sum_{j=1}^n E (X_iX_j)}{n^2} - \mu^2 \end{align} Could you give me some tips how to proceed?","How to prove $E (\overline{X} - \mu)^2  = \frac{1}{n}\sigma^2$ (from wiki ), where $\overline{X}$ - is the sample mean ? What I have so far: \begin{align} E (\overline{X} - \mu)^2  = \frac{1}{n}\sigma^2 &=\\  &= E (\overline{X}^2 + \mu^2 - 2\mu\overline{X}) = E(\overline{X}^2) - \mu^2 =\\ &= \frac{\sum_{i=1}^n\sum_{j=1}^n E (X_iX_j)}{n^2} - \mu^2 \end{align} Could you give me some tips how to proceed?",,"['statistics', 'means']"
33,Probability - marbles without replacement,Probability - marbles without replacement,,"Math is my weakest subject and I'm having a hard time trying to figure out what equation to use in this problem: A jar contains 5 purple balls, 10 pink balls, and 7 blue balls. If 3 balls are to be drawn successively without replacement. What is the probability of getting 2 purple balls and 1 pink ball? Ans: I tried doing (5/22)(4/21) & (10/22). But I don't think it's right... Any help is appreciated.","Math is my weakest subject and I'm having a hard time trying to figure out what equation to use in this problem: A jar contains 5 purple balls, 10 pink balls, and 7 blue balls. If 3 balls are to be drawn successively without replacement. What is the probability of getting 2 purple balls and 1 pink ball? Ans: I tried doing (5/22)(4/21) & (10/22). But I don't think it's right... Any help is appreciated.",,"['probability', 'statistics']"
34,What Percentile Do I fall in with this test score vs average?,What Percentile Do I fall in with this test score vs average?,,I just took an online test and I was notified that the average score for the test was 45% (out of 100%) and I received a 72%. Using these 2 pieces of information would it be possible to calculate under what percentile I fall for test scores?,I just took an online test and I was notified that the average score for the test was 45% (out of 100%) and I received a 72%. Using these 2 pieces of information would it be possible to calculate under what percentile I fall for test scores?,,['statistics']
35,"A bowl contains 10 red balls and 10 blue balls, A women selects ball at random without looking?","A bowl contains 10 red balls and 10 blue balls, A women selects ball at random without looking?",,"How can we solve this question ? A bowl contains $10$ red balls and $10$ blue balls, and a women picks up  balls from the bowl, at random, without looking. A) How many balls must she pickup in order for her to be sure she is holding at least $3$ balls of the same color? B) How many balls must she pickup in order for her to be sure she is holding at least $3$ blue balls ?","How can we solve this question ? A bowl contains $10$ red balls and $10$ blue balls, and a women picks up  balls from the bowl, at random, without looking. A) How many balls must she pickup in order for her to be sure she is holding at least $3$ balls of the same color? B) How many balls must she pickup in order for her to be sure she is holding at least $3$ blue balls ?",,"['combinatorics', 'statistics', 'discrete-mathematics', 'permutations']"
36,Probability of Coin Game,Probability of Coin Game,,"Suppose we play the following game. We take turns tossing a fair coin, and whoever is the first to reach $2$ heads (not necessarily in a row) wins. You go first. Draws are not allowed, so for example, if you flip heads, then I flip heads, and then you flip heads again, you win (I don't get a chance to toss the coin again). What is the probability that you will win the game? Enter your answer as a fraction, such as $\frac23$. I'm not sure how to approach this. I understand the probability of getting a head on the first two tries is $\frac14$. I also understand that the first person who goes should have better odds.  Just not sure how to approach this with infinite turns until you get two heads.","Suppose we play the following game. We take turns tossing a fair coin, and whoever is the first to reach $2$ heads (not necessarily in a row) wins. You go first. Draws are not allowed, so for example, if you flip heads, then I flip heads, and then you flip heads again, you win (I don't get a chance to toss the coin again). What is the probability that you will win the game? Enter your answer as a fraction, such as $\frac23$. I'm not sure how to approach this. I understand the probability of getting a head on the first two tries is $\frac14$. I also understand that the first person who goes should have better odds.  Just not sure how to approach this with infinite turns until you get two heads.",,"['probability', 'statistics']"
37,What about without replacement,What about without replacement,,Cards are drawn at random and with replacement from an ordinary deck of 52 cards until a spade appears. what is the probability that at least 4 draws are necessary. Is this idea correct. The probability of picking a all non spades on 1 consecutive draw with replacement is 1/4.  2 consecutive is $(1/2)^2$  and 3 consecutive draw is $ (3/4)^3$. So $ 1 - (3/4)^3$ is the final answer. Is it okay to reason that way? What about without replacement?,Cards are drawn at random and with replacement from an ordinary deck of 52 cards until a spade appears. what is the probability that at least 4 draws are necessary. Is this idea correct. The probability of picking a all non spades on 1 consecutive draw with replacement is 1/4.  2 consecutive is $(1/2)^2$  and 3 consecutive draw is $ (3/4)^3$. So $ 1 - (3/4)^3$ is the final answer. Is it okay to reason that way? What about without replacement?,,"['probability', 'statistics']"
38,Why the Kullback-Leibler Divergence is never negative,Why the Kullback-Leibler Divergence is never negative,,"The Kullback-Leibler Divergence for the continuous case for two probability densities $p$ and $q$ is $$D_\text{kl}\left(p,q\right) = \int_{x\in\chi}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x$$ . Then, how come that for any choice of $q$ , $D_\text{kl}\left(p,q\right)\geq0$ ? I will attempt to prove by absurd. Suppose that there exists a probability distribution $q$ in which $D_\text{kl}\left(p,q\right)<0$ . An initial guess would be to make $\forall x\in\chi: q(x)>p(x)$ But that doesn't hold since both $p$ and $q$ must have integrals adding up to one: $$\int_{x\in\chi}q(x)\text{d}x>\int_{x\in\chi}p(x)\text{d}x = 1 \unicode{x21af}$$ . So, $p$ and $q$ must intersect at least once, meaning that we can divide the integral into two parts, one in which $p\geq q$ and another with $p<q$ : $$D_\text{kl}\left(p,q\right) = \int_{x\in\chi_{q\geq p}}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x + \int_{x\in\chi_{q< p}}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x$$ . On the second term, choose $q$ so that it gets arbitrarily close to $p$ as possible $$\forall x\in\chi_{q<p} : q(x) = p(x)-\epsilon(x)$$ . Whereas in the first term, we want $q\geq p$ . However, if we choose $q>p$ : $$\int_{x\in\chi}q(x)\text{d}x = \int_{x\in\chi_{q\geq p}}q(x)\text{d}x + \int_{x\in\chi_{q<p}}q(x)\text{d}x>\int_{x\in\chi_{q\geq p}}p(x)\text{d}x + \int_{x\in\chi_{q<p}}p(x)\text{d}x - \int_{x\in\chi_{q<p}}\epsilon(x)\text{d}x$$ $$\int_{x\in\chi}q(x)\text{d}x>1- \int_{x\in\chi_{q<p}}\epsilon(x)\text{d}x$$ . We have an absurd on the limit $\epsilon(x)\to0$ . So, the only suitable choice for $q$ is $q=p$ .","The Kullback-Leibler Divergence for the continuous case for two probability densities and is . Then, how come that for any choice of , ? I will attempt to prove by absurd. Suppose that there exists a probability distribution in which . An initial guess would be to make But that doesn't hold since both and must have integrals adding up to one: . So, and must intersect at least once, meaning that we can divide the integral into two parts, one in which and another with : . On the second term, choose so that it gets arbitrarily close to as possible . Whereas in the first term, we want . However, if we choose : . We have an absurd on the limit . So, the only suitable choice for is .","p q D_\text{kl}\left(p,q\right) = \int_{x\in\chi}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x q D_\text{kl}\left(p,q\right)\geq0 q D_\text{kl}\left(p,q\right)<0 \forall x\in\chi: q(x)>p(x) p q \int_{x\in\chi}q(x)\text{d}x>\int_{x\in\chi}p(x)\text{d}x = 1 \unicode{x21af} p q p\geq q p<q D_\text{kl}\left(p,q\right) = \int_{x\in\chi_{q\geq p}}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x + \int_{x\in\chi_{q< p}}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x q p \forall x\in\chi_{q<p} : q(x) = p(x)-\epsilon(x) q\geq p q>p \int_{x\in\chi}q(x)\text{d}x = \int_{x\in\chi_{q\geq p}}q(x)\text{d}x + \int_{x\in\chi_{q<p}}q(x)\text{d}x>\int_{x\in\chi_{q\geq p}}p(x)\text{d}x + \int_{x\in\chi_{q<p}}p(x)\text{d}x - \int_{x\in\chi_{q<p}}\epsilon(x)\text{d}x \int_{x\in\chi}q(x)\text{d}x>1- \int_{x\in\chi_{q<p}}\epsilon(x)\text{d}x \epsilon(x)\to0 q q=p","['measure-theory', 'statistics']"
39,Switch being red knowing that it worked? (conditional probability),Switch being red knowing that it worked? (conditional probability),,"A spaceship has switches inside its cabinet control. 30% of them are red, 70% are blue. You've learned that the probability of a switch not to work is 0.12 if it is red , and 0.2 if it is blue . A switch is randomly pressed. What is the probability of it being a red switch knowing that it worked? I'm really stuck in what to think here. Probability is not really intuitive to me. This is what I've tried, being $R = ""Red""$ , $W = ""Worked""$ : $P(R | W) = \frac{P(R \cap W)}{P(W)}$ , now I have to find each the numerator and denominator, but I can't get there with the info that I have.","A spaceship has switches inside its cabinet control. 30% of them are red, 70% are blue. You've learned that the probability of a switch not to work is 0.12 if it is red , and 0.2 if it is blue . A switch is randomly pressed. What is the probability of it being a red switch knowing that it worked? I'm really stuck in what to think here. Probability is not really intuitive to me. This is what I've tried, being , : , now I have to find each the numerator and denominator, but I can't get there with the info that I have.","R = ""Red"" W = ""Worked"" P(R | W) = \frac{P(R \cap W)}{P(W)}","['probability', 'statistics', 'conditional-probability', 'bayes-theorem']"
40,"Why have we defined ""average"" as $a_1 + a_2 + \cdots + a_N\over N$ only?","Why have we defined ""average"" as  only?",a_1 + a_2 + \cdots + a_N\over N,"I am graduate student in Physics and I have taken a graduate level course called Probability and Statistics as an elective. The professor told me that in this course will take a Measure theoretic perspective to the theory of Probability and Statistics. In the first lecture, he posed a problem (among others) that why do we define ""averages"" as $a_1 + a_2 + \cdots + a_N\over N$ ? One may say that is our intuition, but is it really the average ? By an Average , we tend to find a number that roughly tells us the ""general"" picture about the data we have at hand. With averages, of course, we need to have another information about the variance , that is how spread our data is? But does our formal definition of ""sum divided by the number of data points"" really does what it is intended to do? Why don't we have any of the following as the definition of averages? $\sqrt{a_1^2 + a_2^2 + \cdots + a_N^2\over N}$ (we use this in physics a lot) $\left(\frac{a_1^p + a_2^p + \dots + a_N^p}{N}\right)^{1/p}$ $(a_1a_2\cdots a_N)^{1/N}$ $\exp{(\ln{a_1}\ln{a_2}\cdots\ln{a_N})^{1/N}}$ I assume that there must be a way to quantify the effectiveness of these averages and there must be a more effective measure of our intutive average than just $a_1 + a_2 + \dots + a_N\over N$ . Can someone help me out with this? Thank you in advance!","I am graduate student in Physics and I have taken a graduate level course called Probability and Statistics as an elective. The professor told me that in this course will take a Measure theoretic perspective to the theory of Probability and Statistics. In the first lecture, he posed a problem (among others) that why do we define ""averages"" as ? One may say that is our intuition, but is it really the average ? By an Average , we tend to find a number that roughly tells us the ""general"" picture about the data we have at hand. With averages, of course, we need to have another information about the variance , that is how spread our data is? But does our formal definition of ""sum divided by the number of data points"" really does what it is intended to do? Why don't we have any of the following as the definition of averages? (we use this in physics a lot) I assume that there must be a way to quantify the effectiveness of these averages and there must be a more effective measure of our intutive average than just . Can someone help me out with this? Thank you in advance!",a_1 + a_2 + \cdots + a_N\over N \sqrt{a_1^2 + a_2^2 + \cdots + a_N^2\over N} \left(\frac{a_1^p + a_2^p + \dots + a_N^p}{N}\right)^{1/p} (a_1a_2\cdots a_N)^{1/N} \exp{(\ln{a_1}\ln{a_2}\cdots\ln{a_N})^{1/N}} a_1 + a_2 + \dots + a_N\over N,"['probability', 'statistics', 'measure-theory']"
41,Monotonic operations and integrals,Monotonic operations and integrals,,"If I have a monotonic function, say ln, can I bring it inside an integral? in other words, is $$\ln\left[ \int f(x)\, dx\right] = \int \ln(f(x))\, dx.$$ My limits of integration don't depend on $x,$ so I think I can. Could I move expectation inside the integral? (I am working on something with the Cramer-Rao inequality, which requires that I take the natural log and then the expectation of a function.)","If I have a monotonic function, say ln, can I bring it inside an integral? in other words, is My limits of integration don't depend on so I think I can. Could I move expectation inside the integral? (I am working on something with the Cramer-Rao inequality, which requires that I take the natural log and then the expectation of a function.)","\ln\left[ \int f(x)\, dx\right] = \int \ln(f(x))\, dx. x,","['calculus', 'integration', 'statistics', 'expected-value']"
42,Why is $y$ separated into two intervals?,Why is  separated into two intervals?,y,"So, here's a question and a solution to part b). I do not understand why they make $y^{1/2}$ belong to interval $[0,1)$ and then separately to the interval $[1,3)$ .","So, here's a question and a solution to part b). I do not understand why they make belong to interval and then separately to the interval .","y^{1/2} [0,1) [1,3)","['probability', 'statistics']"
43,Computation of a limit involving a series (related to Poisson distribution),Computation of a limit involving a series (related to Poisson distribution),,Consider $\lambda >0.$ I am reading a paper and the author states that $$ \displaystyle\lim_{v \rightarrow +\infty} \sum_{n=0}^{+\infty} \frac{\lambda^{n}}{(n !)^v}  = 1 + \lambda$$ I tried to compute such limit but I am getting anywhere. Someone could help me? Thanks in advance!,Consider I am reading a paper and the author states that I tried to compute such limit but I am getting anywhere. Someone could help me? Thanks in advance!,\lambda >0.  \displaystyle\lim_{v \rightarrow +\infty} \sum_{n=0}^{+\infty} \frac{\lambda^{n}}{(n !)^v}  = 1 + \lambda,"['analysis', 'statistics']"
44,Addition of $2$ Events,Addition of  Events,2,"Let $X$ and $Y$ be independent, each uniformly distributed on $\{1, 2, ..., n\}$. Find $P(X + Y = k)$ for $2 \le k \le 2n$. \begin{align}P(X + Y = k) &= \sum_{(x,y)\,:\,x+y=k} P(x, y) \\ &= \sum_{(x,y)\,:\,x+y=k} \frac{1}{n^2} \\ &= (k - 1)\frac{1}{n^2}  \\ &= \frac{k-1}{n^2} \end{align} When $k = 2: (1, 1)$ When $k = 3: (1, 2), (2, 1)$ When $k = 4: (1, 3), (3, 1), (2, 2)$ When $k = 5: (1, 4), (4, 1), (2, 3), (3, 2)$ $$\#(x, y) = k - 1$$ Textbook Answer: $\frac{k-1}{n^2}\,\,\,$ for $\,\,\,2 \le k \le n+1$ $\frac{2n-k+1}{n^2}\,\,\,$ for $\,\,\,n+2 \le k \le 2n$ Why are there $2$ intervals being considered?","Let $X$ and $Y$ be independent, each uniformly distributed on $\{1, 2, ..., n\}$. Find $P(X + Y = k)$ for $2 \le k \le 2n$. \begin{align}P(X + Y = k) &= \sum_{(x,y)\,:\,x+y=k} P(x, y) \\ &= \sum_{(x,y)\,:\,x+y=k} \frac{1}{n^2} \\ &= (k - 1)\frac{1}{n^2}  \\ &= \frac{k-1}{n^2} \end{align} When $k = 2: (1, 1)$ When $k = 3: (1, 2), (2, 1)$ When $k = 4: (1, 3), (3, 1), (2, 2)$ When $k = 5: (1, 4), (4, 1), (2, 3), (3, 2)$ $$\#(x, y) = k - 1$$ Textbook Answer: $\frac{k-1}{n^2}\,\,\,$ for $\,\,\,2 \le k \le n+1$ $\frac{2n-k+1}{n^2}\,\,\,$ for $\,\,\,n+2 \le k \le 2n$ Why are there $2$ intervals being considered?",,"['probability', 'statistics', 'uniform-distribution']"
45,Monty Hall Problem With Uneven Door Probabilities,Monty Hall Problem With Uneven Door Probabilities,,"In the conventional Monty Hall problem it is assumed that the probability of the car being placed behind any of the three doors is the same (namely, $\frac{1}{3}$). (In other words, the host has no inherent bias of placing the car behind a door versus the other two). But what if the probabilities of the car being behind doors 1, 2, or 3, with probabilities $p_1, p_2$ and $p_3$ respectively, are such that the assumption $p_1 = p_2 = p_3$ is not necessarily true. To test whether switching would be beneficial, I decided to experiment using python. I tested switching every time, not switching any time, and randomly deciding whether to switch one million times each. For each option, I used python to generate three random (technically pseudo-random) probabilities, then carried out the experiment. (I will spare you the details of the programming). The results for each option I got were very close to $0.5$. This means that it does not matter whether we switch, do not switch or randomly decide to switch. But more surprising is that given three doors with uneven probabilities of having the car behind, the probability of winning the car is $0.5$ (assuming the contestant picks randomly). Is there any mathematical justification to this? Is my interpretation of the results correct? Edit-- Here is the code snippet for generating random probabilities that I used: def prob_gen(): #Generates random probabilities     prob_a = round(random.uniform(0, 1), 4)     prob_b = round(random.uniform(0, 1 - prob_a), 4)     prob_c = round(1 - (prob_a + prob_b), 4)     return [prob_a, prob_b, prob_c] Full code is here: https://www.pastiebin.com/5a4d75a3e880e","In the conventional Monty Hall problem it is assumed that the probability of the car being placed behind any of the three doors is the same (namely, $\frac{1}{3}$). (In other words, the host has no inherent bias of placing the car behind a door versus the other two). But what if the probabilities of the car being behind doors 1, 2, or 3, with probabilities $p_1, p_2$ and $p_3$ respectively, are such that the assumption $p_1 = p_2 = p_3$ is not necessarily true. To test whether switching would be beneficial, I decided to experiment using python. I tested switching every time, not switching any time, and randomly deciding whether to switch one million times each. For each option, I used python to generate three random (technically pseudo-random) probabilities, then carried out the experiment. (I will spare you the details of the programming). The results for each option I got were very close to $0.5$. This means that it does not matter whether we switch, do not switch or randomly decide to switch. But more surprising is that given three doors with uneven probabilities of having the car behind, the probability of winning the car is $0.5$ (assuming the contestant picks randomly). Is there any mathematical justification to this? Is my interpretation of the results correct? Edit-- Here is the code snippet for generating random probabilities that I used: def prob_gen(): #Generates random probabilities     prob_a = round(random.uniform(0, 1), 4)     prob_b = round(random.uniform(0, 1 - prob_a), 4)     prob_c = round(1 - (prob_a + prob_b), 4)     return [prob_a, prob_b, prob_c] Full code is here: https://www.pastiebin.com/5a4d75a3e880e",,"['probability', 'statistics', 'mathematical-modeling', 'monty-hall']"
46,Minimum mean squared error of uniform distribution,Minimum mean squared error of uniform distribution,,"Let $X_1,X_2,\ldots,X_n$ be i.i.d $\operatorname{Uniform}(-\theta,0)$. Now consider all of the estimates of the form $S_\rho=\rho \hat{\theta}_\text{MLE}$. I have to find which of these estimates has the minimum mean squared error. I found that the MLE of $\theta$ is $\hat{\theta}_\text{MLE}=-x_{(1)}$. I know that the mean squared error is $\operatorname{MSE}(\hat{\theta})=\operatorname E_{\hat{\theta}}[(\hat{\theta}-\theta)^2]$ and that the minimum (best) mean squared error is $\operatorname{MMSE} = \operatorname E[X\mid Y]$. However I wouldn't know how to apply these to $S_\rho=\rho \hat{\theta}_\text{MLE}$. Is there a better (more intuitive) way to see which estimate is the best without using these? Is the best estimator simply the MLE? This is from a past exam of 5 years ago and seems very foreign to me, so it is possible that this was never covered in class. Any hint is appreciated.","Let $X_1,X_2,\ldots,X_n$ be i.i.d $\operatorname{Uniform}(-\theta,0)$. Now consider all of the estimates of the form $S_\rho=\rho \hat{\theta}_\text{MLE}$. I have to find which of these estimates has the minimum mean squared error. I found that the MLE of $\theta$ is $\hat{\theta}_\text{MLE}=-x_{(1)}$. I know that the mean squared error is $\operatorname{MSE}(\hat{\theta})=\operatorname E_{\hat{\theta}}[(\hat{\theta}-\theta)^2]$ and that the minimum (best) mean squared error is $\operatorname{MMSE} = \operatorname E[X\mid Y]$. However I wouldn't know how to apply these to $S_\rho=\rho \hat{\theta}_\text{MLE}$. Is there a better (more intuitive) way to see which estimate is the best without using these? Is the best estimator simply the MLE? This is from a past exam of 5 years ago and seems very foreign to me, so it is possible that this was never covered in class. Any hint is appreciated.",,"['statistics', 'maximum-likelihood', 'mean-square-error']"
47,Finding mean and standard deviation of normal distribution given 2 points.,Finding mean and standard deviation of normal distribution given 2 points.,,"In general, how do do you calculate the mean and standard deviation of a normal distribution given 2 values on the distribution with their respective probabilities? For Example: Suppose that the ages of students in an intro to statistics class are normally distributed. We know that 5% of the students are older than 19.76 years. We also know that 10% of students are younger than 18.3 years. What are the mean and standard deviation of the ages? In my attempts to solve a similar problem I can't see how to calculate the mean or standard deviation without first knowing one of the two. I can find the z-score for 95% and 10%, and if I could somehow derive the values for 5% or 90% I could then average the 5% and 95% or 10% and 90% values to then find the mean, but I don't see a way to do so. Is it even possible to solve this problem or is there not enough information?","In general, how do do you calculate the mean and standard deviation of a normal distribution given 2 values on the distribution with their respective probabilities? For Example: Suppose that the ages of students in an intro to statistics class are normally distributed. We know that 5% of the students are older than 19.76 years. We also know that 10% of students are younger than 18.3 years. What are the mean and standard deviation of the ages? In my attempts to solve a similar problem I can't see how to calculate the mean or standard deviation without first knowing one of the two. I can find the z-score for 95% and 10%, and if I could somehow derive the values for 5% or 90% I could then average the 5% and 95% or 10% and 90% values to then find the mean, but I don't see a way to do so. Is it even possible to solve this problem or is there not enough information?",,"['statistics', 'normal-distribution', 'standard-deviation', 'means']"
48,Find E(X) from Moment Generating Function,Find E(X) from Moment Generating Function,,Let $$M(t)=\frac{5}{1-8t}$$ for $t<1/8$ be the mgf of random variable $X$. Find $E(X)$ and $Var(x)$. I am not sure how to use the mgf to find the $E(X)$. Once I have the expected value I can find the variance. Thank you in advance!,Let $$M(t)=\frac{5}{1-8t}$$ for $t<1/8$ be the mgf of random variable $X$. Find $E(X)$ and $Var(x)$. I am not sure how to use the mgf to find the $E(X)$. Once I have the expected value I can find the variance. Thank you in advance!,,"['probability', 'statistics', 'moment-generating-functions']"
49,How do I make an exponential regression on data with noise?,How do I make an exponential regression on data with noise?,,"I have some measurements that should, logically, be fit to an exponential formula. Problem is, there is some uncertainty in the measurements, so some of them are negative. Since both negative and 0 are illegal in exponential models, I can't just do a headless regression on Excel, say; even if the fit is quite obvious. Let's just say my data looks like this: The blue dots are exponential decay, the orange are exponential decay plus/minus up to 0.1. That's not a lot initially, but when the numbers drop low enough, I get negative values quite randomly; so no exponential regression for me. I could of course delete the negative values, which would give a sampling bias. Not a good solution. Any obvious solutions I'm missing?","I have some measurements that should, logically, be fit to an exponential formula. Problem is, there is some uncertainty in the measurements, so some of them are negative. Since both negative and 0 are illegal in exponential models, I can't just do a headless regression on Excel, say; even if the fit is quite obvious. Let's just say my data looks like this: The blue dots are exponential decay, the orange are exponential decay plus/minus up to 0.1. That's not a lot initially, but when the numbers drop low enough, I get negative values quite randomly; so no exponential regression for me. I could of course delete the negative values, which would give a sampling bias. Not a good solution. Any obvious solutions I'm missing?",,"['probability', 'statistics', 'exponential-function', 'regression']"
50,Probability with biased coin problem,Probability with biased coin problem,,"Jules César gives Astérix a biased coin which produces heads 70% of the time, and asks him to play one of the following games: Game A : Toss the biased coin 99 times. If there are more than 49 heads, he will be sent to feed crocodiles. Game B : Toss the biased coin 100 times. If there are more than 50 heads, he will be sent to feed crocodiles. If there are exactly 50 heads, he is granted a fair coin. If the fair coin produces a head, he will be sent to feed crocodiles. Which game will Astérix choose to play? My hunch is that it will be game A, but I want to calculate the probability. For Game A, I wanted to compute the probability that he would get more than 49 heads. Using the binomial distribution, I found: $$\sum_{i=49}^{99}{99 \choose i} (0.70)^i (0.30)^{ 99-i}  $$ I tried calculating this through Wolfram Alpha but got 0.999999 which doesn't seem right. Is there an easier way to compute this sum, or probability for that matter?","Jules César gives Astérix a biased coin which produces heads 70% of the time, and asks him to play one of the following games: Game A : Toss the biased coin 99 times. If there are more than 49 heads, he will be sent to feed crocodiles. Game B : Toss the biased coin 100 times. If there are more than 50 heads, he will be sent to feed crocodiles. If there are exactly 50 heads, he is granted a fair coin. If the fair coin produces a head, he will be sent to feed crocodiles. Which game will Astérix choose to play? My hunch is that it will be game A, but I want to calculate the probability. For Game A, I wanted to compute the probability that he would get more than 49 heads. Using the binomial distribution, I found: $$\sum_{i=49}^{99}{99 \choose i} (0.70)^i (0.30)^{ 99-i}  $$ I tried calculating this through Wolfram Alpha but got 0.999999 which doesn't seem right. Is there an easier way to compute this sum, or probability for that matter?",,"['probability', 'combinatorics', 'statistics', 'binomial-distribution']"
51,"Why is $P(X\in[a,b])=P(X\in[a,b))=P(X\in(a,b])=P(X\in(a,b))$",Why is,"P(X\in[a,b])=P(X\in[a,b))=P(X\in(a,b])=P(X\in(a,b))","I saw, for any continuous random variable $X$, $P(X\in[a,b])=P(X\in[a,b))=P(X\in(a,b])=P(X\in(a,b))$, where $a,b\in\mathbb{R}$, in my textbook. I don't quite understand why the openness/closeness of an interval doesn't matter in continuous random variable? Is it because prob. density function $f(x)=0$ whenever $x\in\{a,b\}$ Update: I realize some fundamental mistakes I have made after checking out the comments and answer from all of you. PDF $f(x)=0$ whenever $x = a, b$ is so wrong. It should be, WLOG, CDF instead of PDF, $ P(X=a) = F(a) = \int_{a}^{a}f(x) = 0$, which is why the equality holds regardless of the openness of interval.","I saw, for any continuous random variable $X$, $P(X\in[a,b])=P(X\in[a,b))=P(X\in(a,b])=P(X\in(a,b))$, where $a,b\in\mathbb{R}$, in my textbook. I don't quite understand why the openness/closeness of an interval doesn't matter in continuous random variable? Is it because prob. density function $f(x)=0$ whenever $x\in\{a,b\}$ Update: I realize some fundamental mistakes I have made after checking out the comments and answer from all of you. PDF $f(x)=0$ whenever $x = a, b$ is so wrong. It should be, WLOG, CDF instead of PDF, $ P(X=a) = F(a) = \int_{a}^{a}f(x) = 0$, which is why the equality holds regardless of the openness of interval.",,"['probability', 'statistics', 'probability-distributions', 'random-variables']"
52,The weighted mean results in smaller value than the simple mean.,The weighted mean results in smaller value than the simple mean.,,"If weights attached to larger items are smaller and those attached to smaller items are larger, then the weighted mean results in smaller value than the simple mean. My try: Let the weights be $w_1<w_2<\dots<w_n $ and let the items be $x_1>x_2>\dots>x_n$, Weighted mean: $$\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}$$ and mean :$$\frac{\sum_{i=1}^nx_i}{n}.$$ How to show that $$\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}<\frac{\sum_{i=1}^nx_i}{n}?$$","If weights attached to larger items are smaller and those attached to smaller items are larger, then the weighted mean results in smaller value than the simple mean. My try: Let the weights be $w_1<w_2<\dots<w_n $ and let the items be $x_1>x_2>\dots>x_n$, Weighted mean: $$\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}$$ and mean :$$\frac{\sum_{i=1}^nx_i}{n}.$$ How to show that $$\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}<\frac{\sum_{i=1}^nx_i}{n}?$$",,['statistics']
53,How to show the normal density integrates to 1?,How to show the normal density integrates to 1?,,How could you show that the normal density integrates to 1? $$ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma^2}} e^{-(x+\mu)^2 / \sigma^2} dx = 1 $$,How could you show that the normal density integrates to 1? $$ \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma^2}} e^{-(x+\mu)^2 / \sigma^2} dx = 1 $$,,"['calculus', 'probability', 'statistics', 'polar-coordinates']"
54,Is a variable transformed from a random variable a random variable?,Is a variable transformed from a random variable a random variable?,,"Let a random variable $X$ be uniform distribution in $(0,1)$ and another variable $Y=X^2$. I calculate the sample space of $Y$ is also $(0,1)$, and the cdf and pdf of $Y$ are $F_Y(y)=\sqrt{y}$ and $f_Y(y) = \frac{1}{2\sqrt{y}}$. I think $Y$ should also be a random variable and the integration of pdf of $Y$ should give 1 but it does not seems right. I don't expect the following integration gives me 1. $$ \int^1_0{\frac{1}{2\sqrt{y}}}dy = -\frac1 4 y^{-\frac 3 2}|^1_0 $$ Did I calculate the sample space and pdf and the integration correctly? Or a variable derived from a random variable is not necessarily a random variable.","Let a random variable $X$ be uniform distribution in $(0,1)$ and another variable $Y=X^2$. I calculate the sample space of $Y$ is also $(0,1)$, and the cdf and pdf of $Y$ are $F_Y(y)=\sqrt{y}$ and $f_Y(y) = \frac{1}{2\sqrt{y}}$. I think $Y$ should also be a random variable and the integration of pdf of $Y$ should give 1 but it does not seems right. I don't expect the following integration gives me 1. $$ \int^1_0{\frac{1}{2\sqrt{y}}}dy = -\frac1 4 y^{-\frac 3 2}|^1_0 $$ Did I calculate the sample space and pdf and the integration correctly? Or a variable derived from a random variable is not necessarily a random variable.",,"['probability', 'statistics', 'random-variables']"
55,least squares regression in 3space,least squares regression in 3space,,"robjohn is giving me a hand with this, but in case anybody else knows... I need to do a least-squares regression for linearity on a set of coordinates in 3space. If the dataset is linear, I need to see if it is close to vertical or horizontal. How could I do this? Many thanks in advance Joe Stavitsky","robjohn is giving me a hand with this, but in case anybody else knows... I need to do a least-squares regression for linearity on a set of coordinates in 3space. If the dataset is linear, I need to see if it is close to vertical or horizontal. How could I do this? Many thanks in advance Joe Stavitsky",,"['statistics', 'regression']"
56,Order statistic,Order statistic,,"I've two questions (1) How do I determine the distribution of the first order and the highest order statistic for sample of random size N is taken from the continuous uniform(0, $\theta$) and \begin{equation} \rm{P} (N = n) = \frac{1}{n! (e − 1)} \text{for n = 1, 2, 3, . . . .} \end{equation} (2) In general without any distribution of any random variables given, the distribution of highest order statistic is $F_{X(n)}(a) = [F_{X}(a)]^n$ and the distribution of the lowest order statistic is $F_{X(1)}(a) = 1 - [1 - F_X(a)]^n$. I understand this derivation. But what does these values say about the distribution ?","I've two questions (1) How do I determine the distribution of the first order and the highest order statistic for sample of random size N is taken from the continuous uniform(0, $\theta$) and \begin{equation} \rm{P} (N = n) = \frac{1}{n! (e − 1)} \text{for n = 1, 2, 3, . . . .} \end{equation} (2) In general without any distribution of any random variables given, the distribution of highest order statistic is $F_{X(n)}(a) = [F_{X}(a)]^n$ and the distribution of the lowest order statistic is $F_{X(1)}(a) = 1 - [1 - F_X(a)]^n$. I understand this derivation. But what does these values say about the distribution ?",,"['statistics', 'probability-distributions']"
57,Probability of two specific cards to be in your hand in a game of bridge,Probability of two specific cards to be in your hand in a game of bridge,,"Assume that a 52-card deck is distributed among 4 players, each with 13. What is the probability that two specific cards, say the Ace of Hearts and Ace of Diamonds to be in my hand? I have two approach in solving this, each giving a different answer. The first approach to consider the sample space to be the position of the two aces among the four players. There is a 1/4 chance that the Ace of hearts to be in your hand and a 1/4 chance that the Ace of diamonds to be in your hand, so the chance that both Aces to be in your hand is 1/16 The second approach is using a combination approach, the sample size being all possibilites of a 13 card hand from 52 cards. Then, we can calculate as follows: $\frac{{50}\choose{11}}{{52}\choose{13}} = \frac{1}{17} $ Which one is the correct answer and where did one of the working went wrong?","Assume that a 52-card deck is distributed among 4 players, each with 13. What is the probability that two specific cards, say the Ace of Hearts and Ace of Diamonds to be in my hand? I have two approach in solving this, each giving a different answer. The first approach to consider the sample space to be the position of the two aces among the four players. There is a 1/4 chance that the Ace of hearts to be in your hand and a 1/4 chance that the Ace of diamonds to be in your hand, so the chance that both Aces to be in your hand is 1/16 The second approach is using a combination approach, the sample size being all possibilites of a 13 card hand from 52 cards. Then, we can calculate as follows: Which one is the correct answer and where did one of the working went wrong?",\frac{{50}\choose{11}}{{52}\choose{13}} = \frac{1}{17} ,"['probability', 'combinatorics', 'statistics', 'conditional-probability']"
58,"Prove this $\sum_{S \subseteq\{1, \ldots, n\}}(n-|S|) \pi(S)=(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+1}\right)$",Prove this,"\sum_{S \subseteq\{1, \ldots, n\}}(n-|S|) \pi(S)=(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+1}\right)","Set the natural number $n$ . For each $S \subseteq\{1, \ldots, n\}$ define $\pi(S)$ as the product of the members of $S$ , with the agreement $\pi(\emptyset)=1$ . Prove that $$ \sum_{S \subseteq\{1, \ldots, n\}}(n-|S|) \pi(S)=(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+1}\right) $$ My working: I've worked on the right side so that the result is like this $$(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+ 1}\right) $$ $$(n+1)!$$ $$ \sum_{S \subseteq\{1, \ldots, n\}}(n+1) !\left(\frac{1}{n+1}\right) $$ $$ \sum_{S \subseteq\{1, \ldots, n\}}(n+1)n !\left(\frac{1}{n+1}\right) $$ $$ \sum_{S \subseteq\{1, \ldots, n\}}n ! $$ I don't know if my method is correct or not, please help and correct it","Set the natural number . For each define as the product of the members of , with the agreement . Prove that My working: I've worked on the right side so that the result is like this I don't know if my method is correct or not, please help and correct it","n S \subseteq\{1, \ldots, n\} \pi(S) S \pi(\emptyset)=1 
\sum_{S \subseteq\{1, \ldots, n\}}(n-|S|) \pi(S)=(n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+1}\right)
 (n+1) !\left(\frac{1}{2}+\frac {1}{3}+\ldots+\frac{1}{n+ 1}\right)
 (n+1)! 
\sum_{S \subseteq\{1, \ldots, n\}}(n+1) !\left(\frac{1}{n+1}\right)
 
\sum_{S \subseteq\{1, \ldots, n\}}(n+1)n !\left(\frac{1}{n+1}\right)
 
\sum_{S \subseteq\{1, \ldots, n\}}n !
","['probability', 'combinatorics', 'statistics', 'stochastic-calculus', 'problem-solving']"
59,Probability of One Geometric Random Variable when Sum of Two is given,Probability of One Geometric Random Variable when Sum of Two is given,,"This problem comes directly from MIT OCW 6.041 assignment #4, question #3 .  The solution is given .  I think the solution is wrong.  The question is as follows: Suppose that X and Y are independent, identically distributed, geometric random variables with parameter p. Show that $P(X=i|X+Y=n)=\frac{1}{n-1}$ , for $i$ =1,2,..., $n$ −1. The solution given makes use of the fact that $X$ and $Y$ are independent, specifically the solution writes [Call this equation A] $P(X=i∩X+Y=n)=P(X=i∩Y=n−i)=P(X=i)P(Y=n−i).$ This seems incorrect to me because if n and i are given, then $Y$ is completely dependent on those two values. $Y$ has been implicitly specified.  So the distributions are not independent.  Further, the supposed solution to the problem is that all the $X=i$ are equally likely if we specify the sum of $X$ and $Y$ , whereas prior to that sum $X$ follows a geometric distribution.  That sure seems like dependence to me.  Just using the law of multiplication the solution should have $$ P(X=i∩X+Y=n)=P(X=i∩Y=n−i)=P(X=i)P(Y=n−i | X=i). $$ $X$ is specified, one can't just throw away that dependence.  In other words, the quantity $n-i$ is not independent of $X$ , so $Y$ cannot be independent of $X$ either, given the conditioning. Here is my alternative solution.  I'm going to use Bayes' Rule: $$ P(X=i|X+Y=n)= \frac{P(X+Y=n|X=i) P(X=i)}{\sum_{j=1}^{n-1} P(X+Y=n|X=j) P(X=j)}. $$ The key, I think, is to recognize that $P(X+Y=n|X=j) = P(Y=n-j)$ .  This is to say, that if $X$ is given to be $j$ , then the distribution of $X+Y$ is just the distribution of $Y$ .  This makes sense to me because I am adding a geometric random variable, $Y$ , to a constant $j$ , so the distribution will also be a geometric random variable.  So, the conditional probability comes out of the sum and cancels, leaving $$ P(X=i|X+Y=n)= \frac{P(X=i)}{\sum_{j=1}^{n-1} P(X=j)} = \frac{p(1-p)^{i-1}}{1-(1-p)^{n-1}},\; \mathrm{for} \; i=1,2,...,n−1. $$ All the conditional probabilities sum to 1, they are all non-negative, they are exclusive, and they are exhaustive.  This looks like a valid probability distribution to me. Even the content of that final equation looks correct to me. $X+Y=n$ is given, and we want to know what the probability of seeing any particular $X=i$ value is.  We know that $X$ is now limited to the interval $[1,n-1]$ , because it is a geometric variable ( $X \ge 1$ ) and we know that it is added to another geometric variable to get $n$ , so $X \le n-1$ .  So all I have to do is re-weight the probabilities I had before conditioning such that the now-allowed values sum to 1 and I should be done. My question is: why is equation A used in the solution when every time I look at it, it appears wrong and I have a perfectly sensible solution that appears better in every way?  What am I missing about equation A? Edit: I'm going to answer the opposite of my question.  The mistake I made was pulling $P(X+Y=n|X=j) = P(Y=n-j)$ out of the sum in the denominator.  The identity is correct, but that probability literally depends on $j$ and I can't pull it out of the sum.  That was silly.  Plugging in the probabilities for the geometric variables I get the same answer as the answer key. Furthermore, the identity I argue is true actually proves the identity I thought was untrue.  If I multiply both sides of my identity by $P(X=i)$ and use the multiplication rule to combine the conditional and marginal probabilities, I find $P(X=j∩X+Y=n) = P(Y=n-j)P(X=i)$ , which I spent so many hours thinking was wrong. It appears I made a mathematical error that led me to a specious answer which got me to believing a prior logical error.  What a ride.","This problem comes directly from MIT OCW 6.041 assignment #4, question #3 .  The solution is given .  I think the solution is wrong.  The question is as follows: Suppose that X and Y are independent, identically distributed, geometric random variables with parameter p. Show that , for =1,2,..., −1. The solution given makes use of the fact that and are independent, specifically the solution writes [Call this equation A] This seems incorrect to me because if n and i are given, then is completely dependent on those two values. has been implicitly specified.  So the distributions are not independent.  Further, the supposed solution to the problem is that all the are equally likely if we specify the sum of and , whereas prior to that sum follows a geometric distribution.  That sure seems like dependence to me.  Just using the law of multiplication the solution should have is specified, one can't just throw away that dependence.  In other words, the quantity is not independent of , so cannot be independent of either, given the conditioning. Here is my alternative solution.  I'm going to use Bayes' Rule: The key, I think, is to recognize that .  This is to say, that if is given to be , then the distribution of is just the distribution of .  This makes sense to me because I am adding a geometric random variable, , to a constant , so the distribution will also be a geometric random variable.  So, the conditional probability comes out of the sum and cancels, leaving All the conditional probabilities sum to 1, they are all non-negative, they are exclusive, and they are exhaustive.  This looks like a valid probability distribution to me. Even the content of that final equation looks correct to me. is given, and we want to know what the probability of seeing any particular value is.  We know that is now limited to the interval , because it is a geometric variable ( ) and we know that it is added to another geometric variable to get , so .  So all I have to do is re-weight the probabilities I had before conditioning such that the now-allowed values sum to 1 and I should be done. My question is: why is equation A used in the solution when every time I look at it, it appears wrong and I have a perfectly sensible solution that appears better in every way?  What am I missing about equation A? Edit: I'm going to answer the opposite of my question.  The mistake I made was pulling out of the sum in the denominator.  The identity is correct, but that probability literally depends on and I can't pull it out of the sum.  That was silly.  Plugging in the probabilities for the geometric variables I get the same answer as the answer key. Furthermore, the identity I argue is true actually proves the identity I thought was untrue.  If I multiply both sides of my identity by and use the multiplication rule to combine the conditional and marginal probabilities, I find , which I spent so many hours thinking was wrong. It appears I made a mathematical error that led me to a specious answer which got me to believing a prior logical error.  What a ride.","P(X=i|X+Y=n)=\frac{1}{n-1} i n X Y P(X=i∩X+Y=n)=P(X=i∩Y=n−i)=P(X=i)P(Y=n−i). Y Y X=i X Y X 
P(X=i∩X+Y=n)=P(X=i∩Y=n−i)=P(X=i)P(Y=n−i | X=i).
 X n-i X Y X 
P(X=i|X+Y=n)= \frac{P(X+Y=n|X=i) P(X=i)}{\sum_{j=1}^{n-1} P(X+Y=n|X=j) P(X=j)}.
 P(X+Y=n|X=j) = P(Y=n-j) X j X+Y Y Y j 
P(X=i|X+Y=n)= \frac{P(X=i)}{\sum_{j=1}^{n-1} P(X=j)} = \frac{p(1-p)^{i-1}}{1-(1-p)^{n-1}},\; \mathrm{for} \; i=1,2,...,n−1.
 X+Y=n X=i X [1,n-1] X \ge 1 n X \le n-1 P(X+Y=n|X=j) = P(Y=n-j) j P(X=i) P(X=j∩X+Y=n) = P(Y=n-j)P(X=i)","['probability', 'statistics']"
60,Cambridge Admissions Exam Statistics 2006,Cambridge Admissions Exam Statistics 2006,,"The question I am trying to answer is My work: I understand that there are three cases. One where the two points are on the diameter, one where they are both on the circumference of the half circle, and one where one is on the diameter and one is on the circumference. The area in the first case is obviously $0$ , however, I do not know how to proceed from here. I have tried a couple of methods non of which seemed to work. The very first thing I tried was to assign angles to the two points, say $\theta$ and $\alpha$ . The area is then given by $$\frac{\sin (\theta - \alpha)}{2}$$ I thought of now defining a new angle $\phi=\theta - \alpha$ and then to take the expectancy of the area, however, this seems to go nowhere as I first don't know the distribution of this angle, and moreover, I am not sure you can take expectancy over $\sin$ . Then I thought about using Heron's Formula for the area of a triangle and consider the distribution of the distance between the two points. Again, that led me to nowhere as I do not the distribution of the distance between them. There are published answers for this question : https://www.thestudentroom.co.uk/showthread.php?t=894936&page=4#post32939428 https://pmt.physicsandmathstutor.com/download/Maths/STEP/Solutions-and-Reports/2006%20Hints%20and%20Answers.pdf (page 28) however I do not understand them. They assume that the points are in some regions and then they assume that the distance between them is just the distance between these regions. I am not following the logic of the whole argument. If anybody could help me understand this problem and help me answer it I would very greatful.","The question I am trying to answer is My work: I understand that there are three cases. One where the two points are on the diameter, one where they are both on the circumference of the half circle, and one where one is on the diameter and one is on the circumference. The area in the first case is obviously , however, I do not know how to proceed from here. I have tried a couple of methods non of which seemed to work. The very first thing I tried was to assign angles to the two points, say and . The area is then given by I thought of now defining a new angle and then to take the expectancy of the area, however, this seems to go nowhere as I first don't know the distribution of this angle, and moreover, I am not sure you can take expectancy over . Then I thought about using Heron's Formula for the area of a triangle and consider the distribution of the distance between the two points. Again, that led me to nowhere as I do not the distribution of the distance between them. There are published answers for this question : https://www.thestudentroom.co.uk/showthread.php?t=894936&page=4#post32939428 https://pmt.physicsandmathstutor.com/download/Maths/STEP/Solutions-and-Reports/2006%20Hints%20and%20Answers.pdf (page 28) however I do not understand them. They assume that the points are in some regions and then they assume that the distance between them is just the distance between these regions. I am not following the logic of the whole argument. If anybody could help me understand this problem and help me answer it I would very greatful.",0 \theta \alpha \frac{\sin (\theta - \alpha)}{2} \phi=\theta - \alpha \sin,"['probability', 'statistics', 'probability-distributions']"
61,Find CDF of quotient of uniform RVs,Find CDF of quotient of uniform RVs,,"Let $U_1,U_2 = \operatorname{Uniform}[0,1]$ and independent. Find the CDF of $Z=\frac{U_1}{U_2+1}$ . My attempt: $P(Z\le z)=P(U_1\le (U_2+1)z, U_2+1> 0)+P(U_1\ge (U_2+1)z, U_2+1<0)$ . The last term will be equal to $0$ , since $U_2$ is never less than $-1$ . This results in: $$ F_Z(z)=\int_0^1du_2\int_0^{(u_2+1)z}du_1=z\int_0^1(u_2+1)du_2=\frac32z.$$ This holds only if $0\le (u_2+1)z\le 1$ and since $2\ge u_2+1>0$ , we get $0\le z\le \frac12.$ So, my answer would be $$ F_Z(z)=\begin{cases} 0 & \text{if }\quad z<0 \\ \frac32z&\text{if }\quad 0\le z\le\frac12\\1&\text{if }\quad z>\frac12\end{cases}.$$ However, in the solutions, there is an extra component: $2-\frac{1}{2z}-\frac{z}2$ if $\frac12\le z\le 1$ . I don't see where this one comes from. Thanks.","Let and independent. Find the CDF of . My attempt: . The last term will be equal to , since is never less than . This results in: This holds only if and since , we get So, my answer would be However, in the solutions, there is an extra component: if . I don't see where this one comes from. Thanks.","U_1,U_2 = \operatorname{Uniform}[0,1] Z=\frac{U_1}{U_2+1} P(Z\le z)=P(U_1\le (U_2+1)z, U_2+1> 0)+P(U_1\ge (U_2+1)z, U_2+1<0) 0 U_2 -1  F_Z(z)=\int_0^1du_2\int_0^{(u_2+1)z}du_1=z\int_0^1(u_2+1)du_2=\frac32z. 0\le (u_2+1)z\le 1 2\ge u_2+1>0 0\le z\le \frac12.  F_Z(z)=\begin{cases} 0 & \text{if }\quad z<0 \\ \frac32z&\text{if }\quad 0\le z\le\frac12\\1&\text{if }\quad z>\frac12\end{cases}. 2-\frac{1}{2z}-\frac{z}2 \frac12\le z\le 1","['probability', 'statistics', 'solution-verification']"
62,Proof that $\rho = \pm1$ implies linear combination,Proof that  implies linear combination,\rho = \pm1,"I need to prove that $$\rho(X,Y) = \pm 1 \implies Y = aX+b,$$ for some constants $a, b$ . With the help of this thread and this document here's what I've got so far: Let $X, Y$ be random variables and $a$ some constant. $aX + Y$ is also a random variable whose variance by definition is non-negative $$V(aX+Y) \ge 0.$$ From variance properties: $$a^2 V(X)+2a\text{Cov}{X,Y}+V(Y)\ge 0$$ This is a quadratic as a function of $a$ , which has a maximum of 1 root. It has a root only for $a$ that satisfies $$a^2 V(X)+2a\text{Cov}{X,Y}+V(Y)=0$$ let it be $a_0$ . That means that $$V(a_0X+Y)=0$$ How can I continue from here? Thank you.","I need to prove that for some constants . With the help of this thread and this document here's what I've got so far: Let be random variables and some constant. is also a random variable whose variance by definition is non-negative From variance properties: This is a quadratic as a function of , which has a maximum of 1 root. It has a root only for that satisfies let it be . That means that How can I continue from here? Thank you.","\rho(X,Y) = \pm 1 \implies Y = aX+b, a, b X, Y a aX + Y V(aX+Y) \ge 0. a^2 V(X)+2a\text{Cov}{X,Y}+V(Y)\ge 0 a a a^2 V(X)+2a\text{Cov}{X,Y}+V(Y)=0 a_0 V(a_0X+Y)=0","['statistics', 'correlation']"
63,Conditional Expectation in Poisson Distribution,Conditional Expectation in Poisson Distribution,,"So I am currently in the process of learning basic distributions of random variables and I have been trying to understand the following scenario (distilled for sake of brevity): The number of people who enter an elevator on the ground floor is a Poisson random variable with mean $10$ . If there are $N$ floors above the ground floor and if each person is equally likely to get off at any one of these $N$ floors, independently of where the others get off, compute the expected number of stops that the elevator will make before discharging all of its passengers. So, my thought process is as follows. Let $X=$ number of people who enter the elevator. $X \sim Poisson(10)$ . Let $Y=$ the number of stops it takes in total. I define the indicator variable $I_n$ such that it equals $1$ if the elevator stops at a given floor and $0$ otherwise. Then, $Y = I_1 + ... + I_N$ . From what I understand, the goal is to find $E[Y]$ since this essentially gives the expected number of times the elevator will stop. Now, $E[Y] = \sum_{n=1}^N E[I_n] = NE[I_n]$ . Thus, I need to find $I_n$ but this is where I am stuck. I have attempted to condition on $X$ (the number of people who entered the elevator) since naturally, the number of stops it takes depends on the number of people that entered to begin with, but then I am not sure how to go after defining $I_n$ in terms of the conditional expectation: $E[I_n] = E[E[I_n | X = m]]$ . I know that I need to find $E[I_n | X = m]$ somehow, but I am not sure how to do so. This is not a homework question and I am really trying to understand the properties of expectation and how to work with distributions on a deeper level rather than just memorizing a strategy for solving these problems and I would greatly appreciate any help.","So I am currently in the process of learning basic distributions of random variables and I have been trying to understand the following scenario (distilled for sake of brevity): The number of people who enter an elevator on the ground floor is a Poisson random variable with mean . If there are floors above the ground floor and if each person is equally likely to get off at any one of these floors, independently of where the others get off, compute the expected number of stops that the elevator will make before discharging all of its passengers. So, my thought process is as follows. Let number of people who enter the elevator. . Let the number of stops it takes in total. I define the indicator variable such that it equals if the elevator stops at a given floor and otherwise. Then, . From what I understand, the goal is to find since this essentially gives the expected number of times the elevator will stop. Now, . Thus, I need to find but this is where I am stuck. I have attempted to condition on (the number of people who entered the elevator) since naturally, the number of stops it takes depends on the number of people that entered to begin with, but then I am not sure how to go after defining in terms of the conditional expectation: . I know that I need to find somehow, but I am not sure how to do so. This is not a homework question and I am really trying to understand the properties of expectation and how to work with distributions on a deeper level rather than just memorizing a strategy for solving these problems and I would greatly appreciate any help.",10 N N X= X \sim Poisson(10) Y= I_n 1 0 Y = I_1 + ... + I_N E[Y] E[Y] = \sum_{n=1}^N E[I_n] = NE[I_n] I_n X I_n E[I_n] = E[E[I_n | X = m]] E[I_n | X = m],"['probability', 'statistics', 'probability-distributions', 'conditional-expectation']"
64,Need some help figuring out $E(X^2)$,Need some help figuring out,E(X^2),"This is my first question and I hate making this a new question, as I found an answer to a similar question, but it's not quite clear and I don't have enough rep to leave a comment asking for more info. So I got quite stumped trying to figure out $E(X^2)$ in statistics. I found this answer while googling: --Quote-- Actually, if $EX=\mu$ and $E(X-\mu)^2=\sigma^2$ $$ EX^2 = E[X-\mu+\mu]^2=\\ =E(X-\mu)^2-2E[(X-\mu)\mu]+E(\mu^2)=\\=\sigma^2-2\mu E(X-\mu)+\mu^2=\\ =\sigma^2+\mu^2 $$ So $EX^2 =\sigma^2+\mu^2$ , no matter the distribution, and $EX^2\ne(EX)^2$ unless the variance equals zero. --End quote-- Here's a link to said answer: https://math.stackexchange.com/a/737227/693253 Now, if that answer is correct it certainly solves most of my problems but I'm having trouble figuring out this step: $$ \sigma^2-2\mu E(X-\mu)+\mu^2=\\ =\sigma^2+\mu^2 $$ I don't get how he gets rid of the $-2\mu E(X-\mu)$ I guess the question is both ""is this answer correct?"" and some help figuring out that step, if it is. If it's not correct, I'd like some help figuring out $E(X^2)$ Thanks in advance!","This is my first question and I hate making this a new question, as I found an answer to a similar question, but it's not quite clear and I don't have enough rep to leave a comment asking for more info. So I got quite stumped trying to figure out in statistics. I found this answer while googling: --Quote-- Actually, if and So , no matter the distribution, and unless the variance equals zero. --End quote-- Here's a link to said answer: https://math.stackexchange.com/a/737227/693253 Now, if that answer is correct it certainly solves most of my problems but I'm having trouble figuring out this step: I don't get how he gets rid of the I guess the question is both ""is this answer correct?"" and some help figuring out that step, if it is. If it's not correct, I'd like some help figuring out Thanks in advance!","E(X^2) EX=\mu E(X-\mu)^2=\sigma^2 
EX^2 = E[X-\mu+\mu]^2=\\
=E(X-\mu)^2-2E[(X-\mu)\mu]+E(\mu^2)=\\=\sigma^2-2\mu E(X-\mu)+\mu^2=\\
=\sigma^2+\mu^2
 EX^2 =\sigma^2+\mu^2 EX^2\ne(EX)^2 
\sigma^2-2\mu E(X-\mu)+\mu^2=\\
=\sigma^2+\mu^2
 -2\mu E(X-\mu) E(X^2)",['statistics']
65,"$10$ people ($6$ male, $4$ female) divided into $2$ equal groups: what is the probability that all females are in the same group?","people ( male,  female) divided into  equal groups: what is the probability that all females are in the same group?",10 6 4 2,"This question comes from a completed, marked, and returned exam. It will not likely be reused. Problem As stated in the question above Work First, I note that there are $\binom{10}{5}$ possible groupings. Second, I note that, if all $4$ females are in the same group, then the remaining fifth member is one of the boys: there are $\binom{6}{1} = 6$ ways to choose the fifth member. So I conclude $\Pr = \frac{6}{\binom{10}{5}} = \frac{1}{42}$ . Question I was marked incorrect: the given answer is $\frac{1}{21}$ , or exactly twice my answer. What reasoning led to this conclusion? Why does it seem like some sort of symmetry argument allows us to conclude there are $12$ ways to choose the fifth member?","This question comes from a completed, marked, and returned exam. It will not likely be reused. Problem As stated in the question above Work First, I note that there are possible groupings. Second, I note that, if all females are in the same group, then the remaining fifth member is one of the boys: there are ways to choose the fifth member. So I conclude . Question I was marked incorrect: the given answer is , or exactly twice my answer. What reasoning led to this conclusion? Why does it seem like some sort of symmetry argument allows us to conclude there are ways to choose the fifth member?",\binom{10}{5} 4 \binom{6}{1} = 6 \Pr = \frac{6}{\binom{10}{5}} = \frac{1}{42} \frac{1}{21} 12,"['probability', 'combinatorics', 'statistics']"
66,"There is a bag of 8 candies, and 3 are chocolates. You eat candy until the chocolates are gone. What is the probability you will have eaten 7 candies? [closed]","There is a bag of 8 candies, and 3 are chocolates. You eat candy until the chocolates are gone. What is the probability you will have eaten 7 candies? [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question You buy a bag of $8$ candies, of which $3$ are chocolates, but all candies look alike. You eat candies from the bag until you have eaten all three chocolates. What is the probability you will have eaten exactly $7$ of the candies in the bag?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question You buy a bag of candies, of which are chocolates, but all candies look alike. You eat candies from the bag until you have eaten all three chocolates. What is the probability you will have eaten exactly of the candies in the bag?",8 3 7,"['probability', 'statistics']"
67,Find the standard deviation of $|X−Y|$,Find the standard deviation of,|X−Y|,"Let $X$ and $Y$ be independent random variables with a Bernoulli Distribution $Ber(1/3)$ . Find the standard deviation of $|X−Y|$ . The Standard Deviation in the square root of the variance. For a $Ber(1/3)$ the $Var(X)=Var(Y)=1/3(1-1/3)=2/9$ , now how can I calculate $\sigma=\sqrt{Var(|X−Y|)}$ ? Because my idea was to subtract the two $Var$ but then the result will be $0$ , but it should be $\frac{2\sqrt{5}}{9}$ , how can I solve it?","Let and be independent random variables with a Bernoulli Distribution . Find the standard deviation of . The Standard Deviation in the square root of the variance. For a the , now how can I calculate ? Because my idea was to subtract the two but then the result will be , but it should be , how can I solve it?",X Y Ber(1/3) |X−Y| Ber(1/3) Var(X)=Var(Y)=1/3(1-1/3)=2/9 \sigma=\sqrt{Var(|X−Y|)} Var 0 \frac{2\sqrt{5}}{9},"['probability', 'statistics']"
68,Conditional probability- what is wrong with my understanding,Conditional probability- what is wrong with my understanding,,"From Ostaszewski, 2007: An insurance policy covers two employees of a company. The policy will reimburse no more than one loss per employee per year. It reimburses the full amount of the loss up to a company wide maximum of $8000$ . The probability of an employee incurring a loss is $40$ %, and is independent of the other employee's losses. The amount of each loss is uniformly distributed on $(1000, 5000)$ . Given that one of the employees has incurred a loss in excess of $2000$ , determine the probability that losses will exceed reimbursements. The way to solve this is to make a square with vertices $(1000,1000), (5000,1000), (5000,5000),$ and $(1000,5000)$ . Then, to find the condition that one of the employees incurred $\text {loss} >2000$ , one must find the area that goes from $2000$ to $5000$ on one of the axes, and from $1000$ to $5000$ on the other, for a total area of $3000\cdot 4000$ in the denominator. The probability that losses exceed reimbursement is $P(X+Y>8000)$ , so the area is the triangle with vertices $(3000,5000), (5000,5000),$ and $(5000,3000)$ , and so the numerator will be ${1\over2}\cdot 2000^2$ . What is confusing me is how to account for fact that the probability of an employee incurring a loss is $40$ %. I would have thought that the total probability of at least one employee incurring a loss is $.40\cdot.60 +.60\cdot.40 + .40\cdot.40$ , and the probability that both employees incur a loss (which is necessary for losses to exceed reimbursements) is $.40 \cdot .40$ , and therefore the solution should be $$.40 \cdot .40\cdot {1\over2} \cdot 2000^2 \over (.40\cdot.60 +.60\cdot.40 + .40\cdot.40)\cdot 3000 \cdot 4000$$ However the solution says to just do: $$.40\cdot {1\over2} \cdot 2000^2 \over 3000 \cdot 4000$$ What is wrong with how I wanted to do it?","From Ostaszewski, 2007: An insurance policy covers two employees of a company. The policy will reimburse no more than one loss per employee per year. It reimburses the full amount of the loss up to a company wide maximum of . The probability of an employee incurring a loss is %, and is independent of the other employee's losses. The amount of each loss is uniformly distributed on . Given that one of the employees has incurred a loss in excess of , determine the probability that losses will exceed reimbursements. The way to solve this is to make a square with vertices and . Then, to find the condition that one of the employees incurred , one must find the area that goes from to on one of the axes, and from to on the other, for a total area of in the denominator. The probability that losses exceed reimbursement is , so the area is the triangle with vertices and , and so the numerator will be . What is confusing me is how to account for fact that the probability of an employee incurring a loss is %. I would have thought that the total probability of at least one employee incurring a loss is , and the probability that both employees incur a loss (which is necessary for losses to exceed reimbursements) is , and therefore the solution should be However the solution says to just do: What is wrong with how I wanted to do it?","8000 40 (1000, 5000) 2000 (1000,1000), (5000,1000), (5000,5000), (1000,5000) \text {loss} >2000 2000 5000 1000 5000 3000\cdot 4000 P(X+Y>8000) (3000,5000), (5000,5000), (5000,3000) {1\over2}\cdot 2000^2 40 .40\cdot.60 +.60\cdot.40 + .40\cdot.40 .40 \cdot .40 .40 \cdot .40\cdot {1\over2} \cdot 2000^2 \over (.40\cdot.60 +.60\cdot.40 + .40\cdot.40)\cdot 3000 \cdot 4000 .40\cdot {1\over2} \cdot 2000^2 \over 3000 \cdot 4000","['probability', 'statistics']"
69,How to get the combinatory of the numbers?,How to get the combinatory of the numbers?,,"I have a statement that says: Let the digits be: $1, 2, 3, 4, 5, 6$ So, how many even numbers of   three figures  different can be formed? So, the number should be like: $\overline{abc}$, where $a$ can be $1, 3 , 5$ that is $3$ options, $b$ can be $2$ options, because an odd number has already been used for $a$ and can not be repeated, and $c$ can be $3$ options, that are {$2, 4, 6$} because it must be a even number. So, they should be formed: $3 * 2 * 3 = 18$, according to the multiplicative principle. But my answer isn't correct, i would like to know why it is not correct, and how it should be done","I have a statement that says: Let the digits be: $1, 2, 3, 4, 5, 6$ So, how many even numbers of   three figures  different can be formed? So, the number should be like: $\overline{abc}$, where $a$ can be $1, 3 , 5$ that is $3$ options, $b$ can be $2$ options, because an odd number has already been used for $a$ and can not be repeated, and $c$ can be $3$ options, that are {$2, 4, 6$} because it must be a even number. So, they should be formed: $3 * 2 * 3 = 18$, according to the multiplicative principle. But my answer isn't correct, i would like to know why it is not correct, and how it should be done",,"['probability', 'statistics', 'combinations']"
70,Why is the sample variance unbiased? [duplicate],Why is the sample variance unbiased? [duplicate],,"This question already has answers here : Mean of $ \sum (X_i - \bar{X})^2$ (3 answers) Closed 4 years ago . Let $X_1,X_2,...,X_n$ be independent normal distributions with a common variance $\sigma^2$. The usual definition of sample variance is $S^2:=\frac{\Sigma_{i=1}^{n} (X_i-\bar{X})^2}{n-1}$. I want to show that $E[S^2]=\sigma^2$, i.e. the population variance. My attempt at proof: $\frac{n-1}{\sigma^2}E[S^2] = E[\frac{(n-1)S^2}{\sigma^2}] = E[\frac{\Sigma_{i=1}^{n}(X_i-\bar{X})^2}{\sigma^2}]=n$, This is because for each $i$, $\frac{(X_i-\bar{X})}{\sigma}$ has a $N(0,1)$ distribution, so $\frac{(X_i-\bar{X})^2}{\sigma^2}$ has a Chi-squared distribution, each has an expected value of $1$. But then $E[S^2]=\frac{n}{n-1}\sigma^2\neq 1$.","This question already has answers here : Mean of $ \sum (X_i - \bar{X})^2$ (3 answers) Closed 4 years ago . Let $X_1,X_2,...,X_n$ be independent normal distributions with a common variance $\sigma^2$. The usual definition of sample variance is $S^2:=\frac{\Sigma_{i=1}^{n} (X_i-\bar{X})^2}{n-1}$. I want to show that $E[S^2]=\sigma^2$, i.e. the population variance. My attempt at proof: $\frac{n-1}{\sigma^2}E[S^2] = E[\frac{(n-1)S^2}{\sigma^2}] = E[\frac{\Sigma_{i=1}^{n}(X_i-\bar{X})^2}{\sigma^2}]=n$, This is because for each $i$, $\frac{(X_i-\bar{X})}{\sigma}$ has a $N(0,1)$ distribution, so $\frac{(X_i-\bar{X})^2}{\sigma^2}$ has a Chi-squared distribution, each has an expected value of $1$. But then $E[S^2]=\frac{n}{n-1}\sigma^2\neq 1$.",,"['probability', 'statistics', 'statistical-inference']"
71,Finding the height and probability of a probability density function,Finding the height and probability of a probability density function,,"A continuous random variable X has a probability density function f(x) represented on the diagram below (0’A’B’C’). A) FIND THE VALUE OF H We know that the total area must be equal to 1. We can divide the probability density function into two different sections that can be easily calculated (rectangle and a triangle). Given this, we know that we know that the total area is the sum of the area of the rectangle and the area of the triangle. We have both the height, and the width needed to calculate the area of the rectangle Area rectangle = 1/4 · 3 = 3/4 We don’t have the height for the triangle, so we solve with reference to total area 1 =  3/4 + Area triangle Area triangle = 1/4 Using the area of the triangle, we solve for height h 1/4 = ((h-1/4)2)/2 = h - 1/4 h = 1/4 + 1/4 = 1/2 B) COMPUTE P (0 < X ≤ 1) The probability of P (0 < X ≤ 1) is equivalent to the area under the curve from 0 to 1. This area is a rectangle so we use the formula A = hb to find the area. A= (0.25)(1) = 1/4 C)COMPUTE P (1 < X < 2) I'm really not sure how to answer this one, any help would be appreciated.","A continuous random variable X has a probability density function f(x) represented on the diagram below (0’A’B’C’). A) FIND THE VALUE OF H We know that the total area must be equal to 1. We can divide the probability density function into two different sections that can be easily calculated (rectangle and a triangle). Given this, we know that we know that the total area is the sum of the area of the rectangle and the area of the triangle. We have both the height, and the width needed to calculate the area of the rectangle Area rectangle = 1/4 · 3 = 3/4 We don’t have the height for the triangle, so we solve with reference to total area 1 =  3/4 + Area triangle Area triangle = 1/4 Using the area of the triangle, we solve for height h 1/4 = ((h-1/4)2)/2 = h - 1/4 h = 1/4 + 1/4 = 1/2 B) COMPUTE P (0 < X ≤ 1) The probability of P (0 < X ≤ 1) is equivalent to the area under the curve from 0 to 1. This area is a rectangle so we use the formula A = hb to find the area. A= (0.25)(1) = 1/4 C)COMPUTE P (1 < X < 2) I'm really not sure how to answer this one, any help would be appreciated.",,"['probability', 'statistics', 'functions', 'area']"
72,Relationship between chi-square distribution and Poisson distribution,Relationship between chi-square distribution and Poisson distribution,,Random variable $X \sim {{\chi}_{2n}}^2$ and $Y \sim P(\lambda)$. Prove that $P(X < 2\lambda) = P(Y \ge n)$. I have found that $P(X < 2\lambda) = \frac{\int_{0}^{\lambda} e^{-x}x^{n/2-1}dx}{\int_{0}^{\infty} e^{-x}x^{n/2-1}dx}$ and $P(Y \ge n) = \frac{\sum_{k=n}^{\infty} \frac{\lambda^k}{k!}}{\sum_{k=0}^{\infty} \frac{\lambda^k}{k!}}$. But I'm still having difficulty equating the two probability.,Random variable $X \sim {{\chi}_{2n}}^2$ and $Y \sim P(\lambda)$. Prove that $P(X < 2\lambda) = P(Y \ge n)$. I have found that $P(X < 2\lambda) = \frac{\int_{0}^{\lambda} e^{-x}x^{n/2-1}dx}{\int_{0}^{\infty} e^{-x}x^{n/2-1}dx}$ and $P(Y \ge n) = \frac{\sum_{k=n}^{\infty} \frac{\lambda^k}{k!}}{\sum_{k=0}^{\infty} \frac{\lambda^k}{k!}}$. But I'm still having difficulty equating the two probability.,,"['probability', 'statistics']"
73,"For a 3 sets' tennis game, would you bet on it finishing in 2 sets or 3 sets, assuming each player has an equal probability of winning a set?","For a 3 sets' tennis game, would you bet on it finishing in 2 sets or 3 sets, assuming each player has an equal probability of winning a set?",,"Obviously the various permutations are: AA BB ABB BAB ABA BAA.  Nevertheless, I'm still confused because I can reason to myself both ways so any help would be much appreciated.","Obviously the various permutations are: AA BB ABB BAB ABA BAA.  Nevertheless, I'm still confused because I can reason to myself both ways so any help would be much appreciated.",,"['probability', 'statistics', 'permutations']"
74,The Poker Hand or Don't Call Out the Professor,The Poker Hand or Don't Call Out the Professor,,"This goes back to a problem given to my statistics class, 50 years ago. You are dealt 5 cards from a standard deck of 52. What are the odds of you having the Queen of Spades? The Professor said to work on it and be prepared to answer the following day. I said I could give the correct answer right then, which I  did. We took most of the next class discussing/arguing about the answer. I may have won the argument but lost the war (grade). What is the correct answer?","This goes back to a problem given to my statistics class, 50 years ago. You are dealt 5 cards from a standard deck of 52. What are the odds of you having the Queen of Spades? The Professor said to work on it and be prepared to answer the following day. I said I could give the correct answer right then, which I  did. We took most of the next class discussing/arguing about the answer. I may have won the argument but lost the war (grade). What is the correct answer?",,['statistics']
75,Two squares are chosen at random on a chessboard. What is the probability that they have a side in common?,Two squares are chosen at random on a chessboard. What is the probability that they have a side in common?,,Two squares are chosen at random on a chessboard. What is the probability that they have a side in common? I have got the total no of events by using 64 C 2. But I am unable to find the numerator(no. of favorable events).,Two squares are chosen at random on a chessboard. What is the probability that they have a side in common? I have got the total no of events by using 64 C 2. But I am unable to find the numerator(no. of favorable events).,,"['probability', 'statistics', 'permutations', 'combinations']"
76,Fit plane to 3D data using least squares,Fit plane to 3D data using least squares,,"I have some samples of data of the form $x,y$ and $z=f(x,y)$ . I wish to fit a plane (i.e. $z = Ax + By + C$ ) to the data with the smallest mean square errors. I have found an ""answer"" in section 3 of this document, and several other locations too but the answers always end with variations of ""now solve these equations and you can find $A$ , $B$ and $C$ "" I just about have the ability to solve these equations, but the process gets so messy that the likelihood of me making a mistake is quite high (and I need a guaranteed-correct answer). Surely someone has written out the full solution longhand somewhere - i.e. in the form $A=\dots$ , $B=\dots$ and $C=\dots$ anyone know where this has been done? EDIT: I see two answers already which leave out the last step as being trivial... and indeed they don't require any advanced maths... but you do need quite a big whiteboard to work it all out and the scope for making a mistake along the way is quite large. Indeed I noticed that assorted tutorials going through worked examples always have rather neat whole numbers. I find it hard to believe that there is nowhere online where someone has worked out the general case. I.e. find $A$ , $B$ and $C$ in the following set of equations... $$Ad+Be+Cf=g$$ $$Ah+Bi+Cj=j$$ $$Al+Bm+Cn=p$$ ...where $d$ to $p$ are all constants.","I have some samples of data of the form and . I wish to fit a plane (i.e. ) to the data with the smallest mean square errors. I have found an ""answer"" in section 3 of this document, and several other locations too but the answers always end with variations of ""now solve these equations and you can find , and "" I just about have the ability to solve these equations, but the process gets so messy that the likelihood of me making a mistake is quite high (and I need a guaranteed-correct answer). Surely someone has written out the full solution longhand somewhere - i.e. in the form , and anyone know where this has been done? EDIT: I see two answers already which leave out the last step as being trivial... and indeed they don't require any advanced maths... but you do need quite a big whiteboard to work it all out and the scope for making a mistake along the way is quite large. Indeed I noticed that assorted tutorials going through worked examples always have rather neat whole numbers. I find it hard to believe that there is nowhere online where someone has worked out the general case. I.e. find , and in the following set of equations... ...where to are all constants.","x,y z=f(x,y) z = Ax + By + C A B C A=\dots B=\dots C=\dots A B C Ad+Be+Cf=g Ah+Bi+Cj=j Al+Bm+Cn=p d p","['statistics', 'linear-regression']"
77,Expectation of $\bar X^2$,Expectation of,\bar X^2,If all enumerated $X$s are observations from a population with a population $\mu$ and variance $\sigma^2$: Why is this true? $$\mathbb E\!\left(\bar X ^2\right) = \frac{\sigma^2}{n}+\mu^2$$ Source.,If all enumerated $X$s are observations from a population with a population $\mu$ and variance $\sigma^2$: Why is this true? $$\mathbb E\!\left(\bar X ^2\right) = \frac{\sigma^2}{n}+\mu^2$$ Source.,,['statistics']
78,Correlation of Proportions,Correlation of Proportions,,"To introduce my question, here is a small simplification for consideration: Let $X,Y$ be independent random variates, each with finite mean and variance. Interestingly, $$\text{Corr}\big(\frac{X}{X+Y},\frac{Y}{X+Y} \big) = \text{Corr}\big(\frac{X}{X+Y},1-\frac{X}{X+Y} \big) = -1$$ The question becomes, if I have a set of $X_i$ which are independent with finite mean and variance, is there a possible simplification (much like the relationship above) which can be made to the following for some selection of $j,k$? $$\text{Corr}\big(\frac{X_j}{\sum_i{X_i}},\frac{X_k}{\sum_i{X_i}} \big)$$ If the variates are 'iid' this problem is likely simplified considerably. And while I would like to see something of that form, the case I'm most interested in is when $X_i$ are all from the same 'family' of distribution, but with different parameters, $\mu_i$ as a varying mean, for example. I attempted looking at the fact that $$\frac{X_j}{\sum_i{X_i}} = 1 - \frac{\sum_{i\neq j}X_i}{\sum_i{X_i}} = 1 - \frac{X_k}{\sum_i X_i} - \frac{\sum_{i\notin\{j,k\}}X_i}{\sum_i X_i}, \ k\neq j$$ But using this just gives us $$\text{Corr}\big(\frac{X_j}{\sum_i{X_i}},\frac{X_k}{\sum_i{X_i}} \big) = $$ $$\text{Corr}\big(1 - \frac{X_k}{\sum_i X_i} - \frac{\sum_{i\notin\{j,k\}}X_i}{\sum_i X_i}, 1 - \frac{X_j}{\sum_i X_i} - \frac{\sum_{i\notin\{j,k\}}X_i}{\sum_i X_i}\big)$$ Which if I understand correctly is very similar to: $$\text{Corr}\big(1 - \frac{X_k}{\sum_i X_i},1 - \frac{X_j}{\sum_i X_i}\big)$$ So it didn't really take me anywhere","To introduce my question, here is a small simplification for consideration: Let $X,Y$ be independent random variates, each with finite mean and variance. Interestingly, $$\text{Corr}\big(\frac{X}{X+Y},\frac{Y}{X+Y} \big) = \text{Corr}\big(\frac{X}{X+Y},1-\frac{X}{X+Y} \big) = -1$$ The question becomes, if I have a set of $X_i$ which are independent with finite mean and variance, is there a possible simplification (much like the relationship above) which can be made to the following for some selection of $j,k$? $$\text{Corr}\big(\frac{X_j}{\sum_i{X_i}},\frac{X_k}{\sum_i{X_i}} \big)$$ If the variates are 'iid' this problem is likely simplified considerably. And while I would like to see something of that form, the case I'm most interested in is when $X_i$ are all from the same 'family' of distribution, but with different parameters, $\mu_i$ as a varying mean, for example. I attempted looking at the fact that $$\frac{X_j}{\sum_i{X_i}} = 1 - \frac{\sum_{i\neq j}X_i}{\sum_i{X_i}} = 1 - \frac{X_k}{\sum_i X_i} - \frac{\sum_{i\notin\{j,k\}}X_i}{\sum_i X_i}, \ k\neq j$$ But using this just gives us $$\text{Corr}\big(\frac{X_j}{\sum_i{X_i}},\frac{X_k}{\sum_i{X_i}} \big) = $$ $$\text{Corr}\big(1 - \frac{X_k}{\sum_i X_i} - \frac{\sum_{i\notin\{j,k\}}X_i}{\sum_i X_i}, 1 - \frac{X_j}{\sum_i X_i} - \frac{\sum_{i\notin\{j,k\}}X_i}{\sum_i X_i}\big)$$ Which if I understand correctly is very similar to: $$\text{Corr}\big(1 - \frac{X_k}{\sum_i X_i},1 - \frac{X_j}{\sum_i X_i}\big)$$ So it didn't really take me anywhere",,"['probability', 'statistics', 'correlation']"
79,Suppose 5 red and 7 green balls are in a bag. Three balls are removed without replacement.,Suppose 5 red and 7 green balls are in a bag. Three balls are removed without replacement.,,"Suppose $5$ red and $7$ green balls are in a bag. Three balls are removed without replacement. What is the probability that the second and third balls are both green? I'm having trouble figuring out how to go about this problem. The way I see it is, that the first ball can be either red or green so P(R or G)? Then the second and third ball must be green. So does this depend on the first choice. IS it a conditional probability? Can I think of it this way: $P(G)P(G)P(G) + P(R)P(G)P(G)$, i.e the probability of choosing green first then the other two being green plus the probability of choosing red first then the other two green?","Suppose $5$ red and $7$ green balls are in a bag. Three balls are removed without replacement. What is the probability that the second and third balls are both green? I'm having trouble figuring out how to go about this problem. The way I see it is, that the first ball can be either red or green so P(R or G)? Then the second and third ball must be green. So does this depend on the first choice. IS it a conditional probability? Can I think of it this way: $P(G)P(G)P(G) + P(R)P(G)P(G)$, i.e the probability of choosing green first then the other two being green plus the probability of choosing red first then the other two green?",,"['probability', 'statistics', 'probability-distributions']"
80,Proving a sample mean converges in probability to the true mean,Proving a sample mean converges in probability to the true mean,,"I have an i.i.d. sequence of observations $\{x_1, x_2,\ldots, x_n\}$ and I need to prove that $\frac{1}{n}\sum x_i$ converges in probability the ""true mean"", that it is consistent estimate of the mean. But seems to me that I already have the true mean defined... and have nothing to show.","I have an i.i.d. sequence of observations $\{x_1, x_2,\ldots, x_n\}$ and I need to prove that $\frac{1}{n}\sum x_i$ converges in probability the ""true mean"", that it is consistent estimate of the mean. But seems to me that I already have the true mean defined... and have nothing to show.",,['statistics']
81,Why square the result of $x_1 - \bar{x}$ in the standard deviation? [duplicate],Why square the result of  in the standard deviation? [duplicate],x_1 - \bar{x},This question already has answers here : Why is there not a simpler way to calculate the standard deviation? (4 answers) Closed 10 years ago . I don't understand the necessity of square the result of  $x_1 - \bar{x}$ in  $$\sqrt{\frac{\sum_{i=1}^{N} (x_i - \bar{x})^2}{N-1}}$$. In fact I don't understand even why is $N - 1$ on the denominator instead of just $N$. Someone could explain it or recommend a good text about it? All books about Errors Theory or even Statistics that I found are either too much abstract or too much simplist. Thanks in advance.,This question already has answers here : Why is there not a simpler way to calculate the standard deviation? (4 answers) Closed 10 years ago . I don't understand the necessity of square the result of  $x_1 - \bar{x}$ in  $$\sqrt{\frac{\sum_{i=1}^{N} (x_i - \bar{x})^2}{N-1}}$$. In fact I don't understand even why is $N - 1$ on the denominator instead of just $N$. Someone could explain it or recommend a good text about it? All books about Errors Theory or even Statistics that I found are either too much abstract or too much simplist. Thanks in advance.,,"['statistics', 'standard-deviation']"
82,Binomial distributions: Probability that player $A$ wins $x$ games,Binomial distributions: Probability that player  wins  games,A x,"I think this is a binomial question because it involves success/failure (win/lose) but it doesn't give that information. I'm finding this question difficult because it asks for the probability of winning $x$ games. I'm more comfortable with examples using actual numbers rather than ""$x$ games"". Q: There is a $7$ game tournament between player $A$ and $B$. Player $A$ has $0.6$ chance of winning each game. Find the probability player $A$ wins the tournament in $x$ games. So if its binomial then I'll have $p=0.6$ and $q=0.4$. Player $A$ would need to win $4$ games before s/he has won the tournament so $x$ must be $x=4,5,6,7$. So what is $n$? How do I write out my pdf for this problem? Thanks very much.","I think this is a binomial question because it involves success/failure (win/lose) but it doesn't give that information. I'm finding this question difficult because it asks for the probability of winning $x$ games. I'm more comfortable with examples using actual numbers rather than ""$x$ games"". Q: There is a $7$ game tournament between player $A$ and $B$. Player $A$ has $0.6$ chance of winning each game. Find the probability player $A$ wins the tournament in $x$ games. So if its binomial then I'll have $p=0.6$ and $q=0.4$. Player $A$ would need to win $4$ games before s/he has won the tournament so $x$ must be $x=4,5,6,7$. So what is $n$? How do I write out my pdf for this problem? Thanks very much.",,"['statistics', 'probability-distributions']"
83,What does the (1-alpha) mean in confidence interval?,What does the (1-alpha) mean in confidence interval?,,"same as title, what does it represents? I know alpha is the percentile we are trying to reach. such as if we are trying to get to 95% CI , then 0.05 would be 1-alpha . the question is :explain what is meant by the“1-ɑ”part of a confidence interval","same as title, what does it represents? I know alpha is the percentile we are trying to reach. such as if we are trying to get to 95% CI , then 0.05 would be 1-alpha . the question is :explain what is meant by the“1-ɑ”part of a confidence interval",,['statistics']
84,Probabilities associated with negatively marked questions,Probabilities associated with negatively marked questions,,"First of all: not a native english speaker, and not a mathematician. Please explain as you would to your 10 years old son. I have 120 questions to answer True or False For each right answer, i score 1 point For each wrong answer, i lose 1 point Not answering a question does not affect the score (=0) I don't know if the T/F is evenly distributed, so it could be 50/50, 85/15, etc (%) I don't know the answer for any of them, so i'll just guess them all Given the above, can i say that: I have 50% chance of getting each question right? I have 50% chance of getting all questions right? Answering all is better than answering just some of the questions? I have to score a minimum of 70 points, but only know the correct answer for 50 questions; how many others would be ""safe"" to guess? Only 20? The remaining 70? Or something in between? Feel free to edit the question (or the title) if i wrote something wrong.","First of all: not a native english speaker, and not a mathematician. Please explain as you would to your 10 years old son. I have 120 questions to answer True or False For each right answer, i score 1 point For each wrong answer, i lose 1 point Not answering a question does not affect the score (=0) I don't know if the T/F is evenly distributed, so it could be 50/50, 85/15, etc (%) I don't know the answer for any of them, so i'll just guess them all Given the above, can i say that: I have 50% chance of getting each question right? I have 50% chance of getting all questions right? Answering all is better than answering just some of the questions? I have to score a minimum of 70 points, but only know the correct answer for 50 questions; how many others would be ""safe"" to guess? Only 20? The remaining 70? Or something in between? Feel free to edit the question (or the title) if i wrote something wrong.",,"['probability', 'statistics', 'recreational-mathematics', 'education']"
85,Covariance of sample central moments,Covariance of sample central moments,,"What is the covariance matrix of the first four sample central moments ? Assuming a zero mean and possibly without assuming normality. The covariance matrix of the first four sample raw moments for a normal random variable of zero mean is: $$ \left( \begin{array}{cccc}  s^2 & 0 & 3 s^4 & 0 \\  0 & 2 s^4 & 0 & 12 s^6 \\  3 s^4 & 0 & 15 s^6 & 0 \\  0 & 12 s^6 & 0 & 96 s^8 \\ \end{array} \right) $$ and can be obtained e.g. in Mathematica using the command Covariance[  TransformedDistribution[{x, x^2, x^3, x^4},    Distributed[x, NormalDistribution[0, s]]]] The variance of the sample variance was discussed here .","What is the covariance matrix of the first four sample central moments ? Assuming a zero mean and possibly without assuming normality. The covariance matrix of the first four sample raw moments for a normal random variable of zero mean is: $$ \left( \begin{array}{cccc}  s^2 & 0 & 3 s^4 & 0 \\  0 & 2 s^4 & 0 & 12 s^6 \\  3 s^4 & 0 & 15 s^6 & 0 \\  0 & 12 s^6 & 0 & 96 s^8 \\ \end{array} \right) $$ and can be obtained e.g. in Mathematica using the command Covariance[  TransformedDistribution[{x, x^2, x^3, x^4},    Distributed[x, NormalDistribution[0, s]]]] The variance of the sample variance was discussed here .",,"['probability', 'statistics', 'random-variables']"
86,how to tell whether x and y are independent or not,how to tell whether x and y are independent or not,,"Suppose that $f_{x,y}(x,y) = \lambda^2 e^{\displaystyle-\lambda(x+y)}, 0\leq x , 0\leq y.$ Find $\operatorname{Var(X+Y)}$. I'm having trouble with this problem the way to find $\operatorname{Var(X+Y)} = \operatorname{Var(X)}+\operatorname{Var(Y)}+2\operatorname{Cov(X,Y)}$, however if $X$ and $Y$ are independent, then $\operatorname{Cov(X, Y)}=0$, the answers indicated that $X$ and $Y$ are independent since they just used $\operatorname{Var(X+Y)} = \operatorname{Var(X)}+\operatorname{Var(Y)}+0$, my question is how do I tell whether $X$ and $Y$ are independent or not, based on looking only at $f_{x,y}(x,y) = \lambda^2 e^{\displaystyle-\lambda(x+y)}, 0\leq x , 0\leq y.$","Suppose that $f_{x,y}(x,y) = \lambda^2 e^{\displaystyle-\lambda(x+y)}, 0\leq x , 0\leq y.$ Find $\operatorname{Var(X+Y)}$. I'm having trouble with this problem the way to find $\operatorname{Var(X+Y)} = \operatorname{Var(X)}+\operatorname{Var(Y)}+2\operatorname{Cov(X,Y)}$, however if $X$ and $Y$ are independent, then $\operatorname{Cov(X, Y)}=0$, the answers indicated that $X$ and $Y$ are independent since they just used $\operatorname{Var(X+Y)} = \operatorname{Var(X)}+\operatorname{Var(Y)}+0$, my question is how do I tell whether $X$ and $Y$ are independent or not, based on looking only at $f_{x,y}(x,y) = \lambda^2 e^{\displaystyle-\lambda(x+y)}, 0\leq x , 0\leq y.$",,['statistics']
87,Can a Covariance matrix have negative elements?,Can a Covariance matrix have negative elements?,,I have a $N \times N$ covariance matrix $C$ of a multivariate Normal distribution.  Can any of the elements of the Covariance matrix $C$ be negative for a real-valued distributions ?,I have a $N \times N$ covariance matrix $C$ of a multivariate Normal distribution.  Can any of the elements of the Covariance matrix $C$ be negative for a real-valued distributions ?,,"['statistics', 'multivariable-calculus', 'random-variables', 'standard-deviation']"
88,Chauvenet's Criterion - All my data points are outliers?,Chauvenet's Criterion - All my data points are outliers?,,"I have runtimes for requests on a webserver. Sometimes events occur that cause the runtimes to skyrocket (we've all seen the occasionaly slow web page before). Sometimes, they plummet, due to terminated connections and other events. I am trying to come up with a consistent method to throw away spurious events so that I can evaluate performance more consistently. I am trying Chauvenet's Criterion, and I am finding that, in some cases, it claims that all of my data points are outliers. How can this be? Take the following numbers for instance: [30.0, 38.0, 40.0, 43.0, 45.0, 48.0, 48.0, 51.0, 60.0, 62.0, 69.0, 74.0, 78.0, 80.0, 83.0, 84.0, 86.0, 86.0, 86.0, 87.0, 92.0, 101.0, 103.0, 108.0, 108.0, 109.0, 113.0, 113.0, 114.0, 119.0, 123.0, 127.0, 128.0, 130.0, 131.0, 133.0, 138.0, 139.0, 140.0, 148.0, 149.0, 150.0, 150.0, 164.0, 171.0, 177.0, 180.0, 182.0, 191.0, 200.0, 204.0, 205.0, 208.0, 210.0, 227.0, 238.0, 244.0, 249.0, 279.0, 360.0, 378.0, 394.0, 403.0, 489.0, 532.0, 533.0, 545.0, 569.0, 589.0, 761.0, 794.0, 1014.0, 1393.0] 73 values. A mean of 222.29 , and a standard deviation of 236.87 . Chauvenet's criterion for the value 227 would have me calculate the probability according to a normal distribution ( 0.001684 if my math is correct). That number times 73 is .123 , less than .5 and thus an outlier. What am I doing wrong here? Is there a better approach that I should be taking?","I have runtimes for requests on a webserver. Sometimes events occur that cause the runtimes to skyrocket (we've all seen the occasionaly slow web page before). Sometimes, they plummet, due to terminated connections and other events. I am trying to come up with a consistent method to throw away spurious events so that I can evaluate performance more consistently. I am trying Chauvenet's Criterion, and I am finding that, in some cases, it claims that all of my data points are outliers. How can this be? Take the following numbers for instance: [30.0, 38.0, 40.0, 43.0, 45.0, 48.0, 48.0, 51.0, 60.0, 62.0, 69.0, 74.0, 78.0, 80.0, 83.0, 84.0, 86.0, 86.0, 86.0, 87.0, 92.0, 101.0, 103.0, 108.0, 108.0, 109.0, 113.0, 113.0, 114.0, 119.0, 123.0, 127.0, 128.0, 130.0, 131.0, 133.0, 138.0, 139.0, 140.0, 148.0, 149.0, 150.0, 150.0, 164.0, 171.0, 177.0, 180.0, 182.0, 191.0, 200.0, 204.0, 205.0, 208.0, 210.0, 227.0, 238.0, 244.0, 249.0, 279.0, 360.0, 378.0, 394.0, 403.0, 489.0, 532.0, 533.0, 545.0, 569.0, 589.0, 761.0, 794.0, 1014.0, 1393.0] 73 values. A mean of 222.29 , and a standard deviation of 236.87 . Chauvenet's criterion for the value 227 would have me calculate the probability according to a normal distribution ( 0.001684 if my math is correct). That number times 73 is .123 , less than .5 and thus an outlier. What am I doing wrong here? Is there a better approach that I should be taking?",,['statistics']
89,conditional probability that both answers are correct given that at least one is correct,conditional probability that both answers are correct given that at least one is correct,,"I came across this question whiles doing conditional probability and need a vivid explanation. A person answers each of two multiple choice questions at random. if there are four possible choices on each of the question, what is the conditional probability that both answers are correct given that at least one is correct","I came across this question whiles doing conditional probability and need a vivid explanation. A person answers each of two multiple choice questions at random. if there are four possible choices on each of the question, what is the conditional probability that both answers are correct given that at least one is correct",,"['probability', 'statistics']"
90,Solving this pie-chart,Solving this pie-chart,,"Hi could anyone please guide me on how I would go about calculating the percentage of a specific sector from this Pi-chart. I think I am suppose to use data from the bar chart and apply it to the pie-chart but I don't really know how. The question is According to the data provided, in 2005 the total expenditure (outgoings) on Advertising and Sales and Distribution was closest to which of the following amounts a)50,000,000 b)35,000,000 c)30,000,000 d)$5,000,000 e)3,000,000  Ans is a","Hi could anyone please guide me on how I would go about calculating the percentage of a specific sector from this Pi-chart. I think I am suppose to use data from the bar chart and apply it to the pie-chart but I don't really know how. The question is According to the data provided, in 2005 the total expenditure (outgoings) on Advertising and Sales and Distribution was closest to which of the following amounts a)50,000,000 b)35,000,000 c)30,000,000 d)$5,000,000 e)3,000,000  Ans is a",,"['statistics', 'graphing-functions']"
91,When standard deviation is unknown?,When standard deviation is unknown?,,"I am reading a book about Statistics and I have encountered a text line: ...except in the case where standard deviation of the basic set is unknown... I am not really sure what it means, could you please help me. For what types of data can't we define the standard deviation?","I am reading a book about Statistics and I have encountered a text line: ...except in the case where standard deviation of the basic set is unknown... I am not really sure what it means, could you please help me. For what types of data can't we define the standard deviation?",,"['probability', 'statistics', 'standard-deviation']"
92,Deriving the exponential distribution from a shift property of its expectation (equivalent to memorylessness).,Deriving the exponential distribution from a shift property of its expectation (equivalent to memorylessness).,,"Suppose $X$ is a continuous, nonnegative random variable with distribution function $F$ and probability density function $f$. If for $a>0,\ E(X|X>a)=a+E(X)$, find the distribution $F$ of $X$.","Suppose $X$ is a continuous, nonnegative random variable with distribution function $F$ and probability density function $f$. If for $a>0,\ E(X|X>a)=a+E(X)$, find the distribution $F$ of $X$.",,"['statistics', 'probability-distributions']"
93,summation of x * (y choose x) binomial coefficients,summation of x * (y choose x) binomial coefficients,,"What does this summation simplify to? $$ \sum_{x=0}^{y} \frac{x}{x!(y-x)!} $$ I was able to realize that it is equivalent to the summation of $x\dbinom{y}{x}$ if you divide and multiply by $y!$, but I am unsure of how to further simplify. Thanks for the help!","What does this summation simplify to? $$ \sum_{x=0}^{y} \frac{x}{x!(y-x)!} $$ I was able to realize that it is equivalent to the summation of $x\dbinom{y}{x}$ if you divide and multiply by $y!$, but I am unsure of how to further simplify. Thanks for the help!",,"['statistics', 'binomial-coefficients']"
94,Can I calculate a percentile given only quartile values?,Can I calculate a percentile given only quartile values?,,"It's been a while since I took a statistics course, but this question came to mind the other day. Let's suppose that I am looking at Salary data, but the only data provided is the quartiles. For example: Q1 = 25 percentile = 40 000 Q2 = 50 percentile = 70 000 Q3 = 75 percentile = 100 000 Assuming that we have a normal distribution and the above information, is it possible to calculate any given percentile? If so, how? Any help would be appreciated. Thanks!","It's been a while since I took a statistics course, but this question came to mind the other day. Let's suppose that I am looking at Salary data, but the only data provided is the quartiles. For example: Q1 = 25 percentile = 40 000 Q2 = 50 percentile = 70 000 Q3 = 75 percentile = 100 000 Assuming that we have a normal distribution and the above information, is it possible to calculate any given percentile? If so, how? Any help would be appreciated. Thanks!",,['statistics']
95,Probability of Union of 4 or More Elements,Probability of Union of 4 or More Elements,,"I have the following problem: Given $P(A)=0.2$, $P(B)=0.4$, $P(C)=0.8$, $P(D)=0.5$, find $P(A\cup B\cup C\cup D)$ And the final answer should be 0.952 I know how to find the union of two and three elements (for 2, its: $A+B-AB$), but the formula becomes clumsy after 3. The best things I've found says that to find the union for n elements, I add as follows $$0.2-(0.2\times0.4)+(0.2\times0.4\times0.8)-(0.2\times0.4\times0.8\times0.5) = 0.152$$ which is wrong. What is a good general rule for n events?","I have the following problem: Given $P(A)=0.2$, $P(B)=0.4$, $P(C)=0.8$, $P(D)=0.5$, find $P(A\cup B\cup C\cup D)$ And the final answer should be 0.952 I know how to find the union of two and three elements (for 2, its: $A+B-AB$), but the formula becomes clumsy after 3. The best things I've found says that to find the union for n elements, I add as follows $$0.2-(0.2\times0.4)+(0.2\times0.4\times0.8)-(0.2\times0.4\times0.8\times0.5) = 0.152$$ which is wrong. What is a good general rule for n events?",,"['probability', 'statistics']"
96,"How to calculate a confidence interval for $p$, the parameter of the binomial distribution?","How to calculate a confidence interval for , the parameter of the binomial distribution?",p,"Suppose I have a random variable X that is known to follow the binomial distribution B, but whose parameter $p$ is unknown. I have observed 100 samples of X, and they all came out true. How can I calculate a 95% confidence interval for the value of $p$ (which stands for P(true)) of the underlying distribution B?","Suppose I have a random variable X that is known to follow the binomial distribution B, but whose parameter $p$ is unknown. I have observed 100 samples of X, and they all came out true. How can I calculate a 95% confidence interval for the value of $p$ (which stands for P(true)) of the underlying distribution B?",,"['probability', 'statistics']"
97,Set vs Multiset,Set vs Multiset,,"A set is a collection of distinct objects. Suppose I sample some parameter of some population and the results are $\{35,35,36,36,37 \}$. Do I need to say (i) the multiset of the samples is $\{35,35, \dots \}$ instead of (ii) the set of samples is $\{35,35, \dots \}$?","A set is a collection of distinct objects. Suppose I sample some parameter of some population and the results are $\{35,35,36,36,37 \}$. Do I need to say (i) the multiset of the samples is $\{35,35, \dots \}$ instead of (ii) the set of samples is $\{35,35, \dots \}$?",,"['statistics', 'multisets']"
98,"What could the notation $l^\infty(\mathcal{F})$ mean, where $\mathcal{F}$ is a set of measurable functions?","What could the notation  mean, where  is a set of measurable functions?",l^\infty(\mathcal{F}) \mathcal{F},"In the book Weak convergence and Empirical Processes , by Aad W. van der Vaart and Jon A. Wellner, on page 81, the notation $l^\infty(\mathcal{F})$ appears, where $\mathcal{F}$ is a set of measurable functions, $f \in \mathcal{F}$ then $f \colon \mathcal{X} \rightarrow \mathbb{R}$. I am not sure what $l^\infty(\mathcal{F})$ is. I know what $l^\infty$ is by itself, the set of bounded sequences (Wikipedia), but not sure about this one.","In the book Weak convergence and Empirical Processes , by Aad W. van der Vaart and Jon A. Wellner, on page 81, the notation $l^\infty(\mathcal{F})$ appears, where $\mathcal{F}$ is a set of measurable functions, $f \in \mathcal{F}$ then $f \colon \mathcal{X} \rightarrow \mathbb{R}$. I am not sure what $l^\infty(\mathcal{F})$ is. I know what $l^\infty$ is by itself, the set of bounded sequences (Wikipedia), but not sure about this one.",,"['real-analysis', 'probability', 'statistics', 'notation']"
99,"When $\mathbb {P} (X \ge Y)\ge \frac{1}{2}$, $\mathbb {P} (Y \ge Z)\ge \frac{1}{2}$ $\Rightarrow $ $\mathbb {P} (X \ge Z)\ge \frac{1}{2}$?","When ,   ?",\mathbb {P} (X \ge Y)\ge \frac{1}{2} \mathbb {P} (Y \ge Z)\ge \frac{1}{2} \Rightarrow  \mathbb {P} (X \ge Z)\ge \frac{1}{2},"Let $X, Y, Z$ be random variables. When does the following hold? $$\mathbb {P} (X \ge Y)\ge  \frac{1}{2} \text{ and } \mathbb {P} (Y \ge Z)\ge  \frac{1}{2} \Rightarrow \mathbb {P} (X \ge Z)\ge  \frac{1}{2}.$$ Could you specify some necessary or sufficient conditions under which the above holds when $X, Y, Z$ are independent? Updates 1: Considering the counterexample in the answer given by @RyszardSzwarc, I updated my findings as follows: A- When $X, Y, Z$ are independent and have symmetric continuous distributions with unique medians , the above holds (still not sure about that this holds when $X, Y, Z$ are dependent). B- When the distributions of $X, Y, Z$ do not have unique medians, even if they are symmetric, the above may not hold. Specific questions: 1- If $X, Y, Z$ are independent and have distributions with unique medians , does the above hold? This can be written as follows: $$\int_{-\infty}^{+\infty} F_Y(x)dF_X(x) \ge  \frac{1}{2}, \int_{-\infty}^{+\infty} F_Z(y)dF_Y(y) \ge  \frac{1}{2} \Rightarrow \int_{-\infty}^{+\infty} F_Z(x)dF_X(x) \ge  \frac{1}{2}.$$ Please provide a proof or counterexample. 2- What happens if the analysis of the above question is restricted to the class of distributions whose cdfs are continuous and strictly monotone over their supports (implying that the median is unique)? For a subclass of this class including those distributions with densities, the claim is equivalent to the following $$\int_{-\infty}^{+\infty} F_Y(x)F'_X(x)dx \ge  \frac{1}{2}, \int_{-\infty}^{+\infty} F_Z(y)F'_Y(y)dy \ge  \frac{1}{2} \Rightarrow \int_{-\infty}^{+\infty} F_Z(x)F'_X(x)dx \ge  \frac{1}{2}.$$ Here, $F_X, F_Y, F_Z$ denote the cdfs of $X, Y, Z$ , respectively. $X, Y, Z$ have unique medians when $F_X(m_X)=F_X(m_Y)=F_X(m_Z)=0.5$ have unique solutions $m_X, m_Y, m_Z$ . Updates 2: Based on the counterexample provided by @stochasticboy321, I realized that the set of independent RVs with symmetric continuous distributions whose medians are unique is the only set of independent RVs over which the proposed probabilistic order is transitive. Note that based on the counterexample provided by @RyszardSzwarc we can design symmetric continuous distributions with non-unique medians for which the implication does not hold (the continuous version of $Y$ has a symmetric bimodal continuous distribution   with multiple medians). $\color{red}{\text{Prove, refine, or provide a counterexample:}}$ I guess this can be generalized to the set of all RVs with symmetric distributions whose medians are unique (including constants and symmetric distributions with unique medians) as The union of the set of real numbers and the set of RVs with symmetric continuous distributions whose medians are unique is the largest set of RVs over which the proposed probabilistic order is transitive. Here, $(X,Y, Z)$ is called to have a symmetric distribution if for some $m_1, m_2, m_3$ : $$(X-m_1,Y-m_2, Z-m_3) \sim - (X-m_1,Y-m_2, Z-m_3).$$ Updates 3: By the Bonferroni's inequality and $$\mathbb {P} (X \ge Z)\ge \mathbb {P} (X \ge Y , Y \ge Z)$$ we have $$\mathbb {P} (X \ge Y)\ge  p, \mathbb {P} (Y \ge Z)\ge  q \Rightarrow \mathbb {P} (X \ge Z)\ge  p+q-1.$$ Hence, to have a probabilistic order over the set of all RVs , $p=q=1$ seems to be the only choice for which $p=q=p+q-1$ .","Let be random variables. When does the following hold? Could you specify some necessary or sufficient conditions under which the above holds when are independent? Updates 1: Considering the counterexample in the answer given by @RyszardSzwarc, I updated my findings as follows: A- When are independent and have symmetric continuous distributions with unique medians , the above holds (still not sure about that this holds when are dependent). B- When the distributions of do not have unique medians, even if they are symmetric, the above may not hold. Specific questions: 1- If are independent and have distributions with unique medians , does the above hold? This can be written as follows: Please provide a proof or counterexample. 2- What happens if the analysis of the above question is restricted to the class of distributions whose cdfs are continuous and strictly monotone over their supports (implying that the median is unique)? For a subclass of this class including those distributions with densities, the claim is equivalent to the following Here, denote the cdfs of , respectively. have unique medians when have unique solutions . Updates 2: Based on the counterexample provided by @stochasticboy321, I realized that the set of independent RVs with symmetric continuous distributions whose medians are unique is the only set of independent RVs over which the proposed probabilistic order is transitive. Note that based on the counterexample provided by @RyszardSzwarc we can design symmetric continuous distributions with non-unique medians for which the implication does not hold (the continuous version of has a symmetric bimodal continuous distribution   with multiple medians). I guess this can be generalized to the set of all RVs with symmetric distributions whose medians are unique (including constants and symmetric distributions with unique medians) as The union of the set of real numbers and the set of RVs with symmetric continuous distributions whose medians are unique is the largest set of RVs over which the proposed probabilistic order is transitive. Here, is called to have a symmetric distribution if for some : Updates 3: By the Bonferroni's inequality and we have Hence, to have a probabilistic order over the set of all RVs , seems to be the only choice for which .","X, Y, Z \mathbb {P} (X \ge Y)\ge  \frac{1}{2} \text{ and } \mathbb {P} (Y \ge Z)\ge  \frac{1}{2} \Rightarrow \mathbb {P} (X \ge Z)\ge  \frac{1}{2}. X, Y, Z X, Y, Z X, Y, Z X, Y, Z X, Y, Z \int_{-\infty}^{+\infty} F_Y(x)dF_X(x) \ge  \frac{1}{2}, \int_{-\infty}^{+\infty} F_Z(y)dF_Y(y) \ge  \frac{1}{2} \Rightarrow \int_{-\infty}^{+\infty} F_Z(x)dF_X(x) \ge  \frac{1}{2}. \int_{-\infty}^{+\infty} F_Y(x)F'_X(x)dx \ge  \frac{1}{2}, \int_{-\infty}^{+\infty} F_Z(y)F'_Y(y)dy \ge  \frac{1}{2} \Rightarrow \int_{-\infty}^{+\infty} F_Z(x)F'_X(x)dx \ge  \frac{1}{2}. F_X, F_Y, F_Z X, Y, Z X, Y, Z F_X(m_X)=F_X(m_Y)=F_X(m_Z)=0.5 m_X, m_Y, m_Z Y \color{red}{\text{Prove, refine, or provide a counterexample:}} (X,Y, Z) m_1, m_2, m_3 (X-m_1,Y-m_2, Z-m_3) \sim - (X-m_1,Y-m_2, Z-m_3). \mathbb {P} (X \ge Z)\ge \mathbb {P} (X \ge Y , Y \ge Z) \mathbb {P} (X \ge Y)\ge  p, \mathbb {P} (Y \ge Z)\ge  q \Rightarrow \mathbb {P} (X \ge Z)\ge  p+q-1. p=q=1 p=q=p+q-1","['real-analysis', 'probability', 'integration', 'statistics', 'order-theory']"
