,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Integral over the surface of a paraboloid,Integral over the surface of a paraboloid,,"The integral I'm trying to solve is the following: $$ \iint_S xyz \,d\sigma $$ Where $z=x^2+y^2$ and $0<z<1$ . So I transform the integral to a double integral: $$\iint_Sxy(x^2+y^2)\sqrt{1+4(x^2+y^2)}\,dx\,dy$$ After that I transform it to polar coordinates with jacobian $J=\rho$ and get the following: $$\int_{0}^{1}\int_{0}^{2\pi}\rho^5\cos(\phi)\sin(\phi)\sqrt{1+4\rho^2}\,d\phi\,d\rho$$ However the $\phi$ part of the integral evaluates to zero and I can't figure where's my mistake. The answer should be: $\frac{125\sqrt5-1}{420}$",The integral I'm trying to solve is the following: Where and . So I transform the integral to a double integral: After that I transform it to polar coordinates with jacobian and get the following: However the part of the integral evaluates to zero and I can't figure where's my mistake. The answer should be:," \iint_S xyz \,d\sigma  z=x^2+y^2 0<z<1 \iint_Sxy(x^2+y^2)\sqrt{1+4(x^2+y^2)}\,dx\,dy J=\rho \int_{0}^{1}\int_{0}^{2\pi}\rho^5\cos(\phi)\sin(\phi)\sqrt{1+4\rho^2}\,d\phi\,d\rho \phi \frac{125\sqrt5-1}{420}","['calculus', 'integration', 'multivariable-calculus', 'surface-integrals']"
1,About global and local extrema for this function,About global and local extrema for this function,,"I have the function $$f(x, y) = \ln(1 + xy) - \frac{1}{3}(x^2+y^2)$$ I found its critical points, one saddle and two maxima, the latter of which are $$A = (1/\sqrt{2}, 1/\sqrt{2}) \qquad \qquad B = -A$$ How can I say that those points are global maxima and not just local, without plotting the function? I thought about analysing what happens when $(x, y) \to (\pm \infty,\ \pm\infty)$ but there are times in which it's a mess. In this case for example, I thought about studying what happens along the straight lines $x = y$ , getting $$f(x) = \ln(1+x^2) - \frac{2}{3}x^2 \sim 2\ln|x| - x^2$$ as $x \to \pm \infty$ , and this either cases goes to $-\infty$ . Hence there is an upper limit and my points are global max. Is this reasoning correct? Or is it wrong and why? Are there better / ""righter"" ways to proceed in general, when dealing with functions in two variables in the case of a non compact domain?","I have the function I found its critical points, one saddle and two maxima, the latter of which are How can I say that those points are global maxima and not just local, without plotting the function? I thought about analysing what happens when but there are times in which it's a mess. In this case for example, I thought about studying what happens along the straight lines , getting as , and this either cases goes to . Hence there is an upper limit and my points are global max. Is this reasoning correct? Or is it wrong and why? Are there better / ""righter"" ways to proceed in general, when dealing with functions in two variables in the case of a non compact domain?","f(x, y) = \ln(1 + xy) - \frac{1}{3}(x^2+y^2) A = (1/\sqrt{2}, 1/\sqrt{2}) \qquad \qquad B = -A (x, y) \to (\pm \infty,\ \pm\infty) x = y f(x) = \ln(1+x^2) - \frac{2}{3}x^2 \sim 2\ln|x| - x^2 x \to \pm \infty -\infty","['limits', 'multivariable-calculus', 'optimization', 'maxima-minima']"
2,"Finding the minimum of $\int 1/|Du \cdot v|^2$ over $[0,1]^2$ for some $v \in \mathbb{R}^2$ when $\int Du = I$",Finding the minimum of  over  for some  when,"\int 1/|Du \cdot v|^2 [0,1]^2 v \in \mathbb{R}^2 \int Du = I","I've run into the question of finding the minimum (or infimum) of this integral over $C^2([0,1]^2)$ , subject to the conditions that... $$\det (Du) \neq 0 \,\,\text{everywhere} \qquad \text{and} \qquad \int_{[0,1]^2} Du(x)dx = I_{2\times2} $$ Let $u: \mathbb{R}^2 \to \mathbb{R}^2$ with $u\in C^2([0,1]^2)$ . Let $v \in \mathbb{R}^2\setminus \{0\}$ . Find the infimum of $J$ with respect to $u$ . $$J(u) = \int_{[0,1]^2} \frac{1}{|Du(x) \cdot v|^2}dx$$ The idea is that $u$ is some deformation of the unit square. My conjecture is that the minimisers $u^*$ will be linear, and so I attempted to try and use some sort of convexity of $f(y) = \frac{1}{|y|^2}$ , but I struggled to get anywhere with that strategy. Is there a standard method for problems like this? Examples of similar solved problems would be helpful too if you have them. Context: I'm using a continuum approximation of a system of point particles with some interaction and this is the integral which describes their total interaction energies up to some asymptotically small error term.","I've run into the question of finding the minimum (or infimum) of this integral over , subject to the conditions that... Let with . Let . Find the infimum of with respect to . The idea is that is some deformation of the unit square. My conjecture is that the minimisers will be linear, and so I attempted to try and use some sort of convexity of , but I struggled to get anywhere with that strategy. Is there a standard method for problems like this? Examples of similar solved problems would be helpful too if you have them. Context: I'm using a continuum approximation of a system of point particles with some interaction and this is the integral which describes their total interaction energies up to some asymptotically small error term.","C^2([0,1]^2) \det (Du) \neq 0 \,\,\text{everywhere} \qquad \text{and} \qquad \int_{[0,1]^2} Du(x)dx = I_{2\times2}  u: \mathbb{R}^2 \to \mathbb{R}^2 u\in C^2([0,1]^2) v \in \mathbb{R}^2\setminus \{0\} J u J(u) = \int_{[0,1]^2} \frac{1}{|Du(x) \cdot v|^2}dx u u^* f(y) = \frac{1}{|y|^2}","['integration', 'multivariable-calculus', 'calculus-of-variations']"
3,"Show that the function $f(x, y) = x^2 + y^2$ is convex",Show that the function  is convex,"f(x, y) = x^2 + y^2","I have to show that the function $f(x, y) = x^2 + y^2$ is convex so I am thinking to demonstrate that the graph of the function lies above any of its tangent planes.But I want to prove it with linear algebra perspective. To do this, i used the definition of a convex function in terms of its Hessian matrix. The Hessian matrix of a function with two variables is a $2 \times 2$ matrix that contains its second partial derivatives. If the Hessian matrix is positive semidefinite then the function is convex.Am I correct ? To find the Hessian matrix of $f(x, y) = x^2 + y^2$ , i need to compute its second partial derivatives: \begin{align*} \frac{\partial^2 f}{\partial x^2} &= \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right) \\ &= \frac{\partial}{\partial x}\left(2x\right) \\ &= 2 \end{align*} \begin{align*} \frac{\partial^2 f}{\partial y^2} &= \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right) \\ &= \frac{\partial}{\partial y}\left(2y\right) \\ &= 2 \end{align*} \begin{align*} \frac{\partial^2 f}{\partial x\partial y} &= \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right) \\ &= \frac{\partial}{\partial x}\left(0\right) \\ &= 0 \end{align*} \begin{align*} \frac{\partial^2 f}{\partial y\partial x} &= \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right) \\ &= \frac{\partial}{\partial y}\left(0\right) \\ &= 0 \end{align*} Therefore, the Hessian matrix of $$f(x, y) = x^2 + y^2$$ is: $$H = \begin{pmatrix} 2 & 0 \\ 0 & 2\\ \end{pmatrix}$$ To show that the Hessian matrix of $f(x, y) = x^2 + y^2$ is positive semidefinite, i need to demonstrate that for any vector $x = [u, v]^T$ , the quadratic form $x^T H x$ is non-negative: \begin{align*} x^T H x &= \begin{bmatrix} u & v \end{bmatrix}^T \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \begin{bmatrix} u \ v \end{bmatrix} \\ &= \begin{bmatrix} 2u & 2v \end{bmatrix} \begin{bmatrix} u \ v \end{bmatrix} \\ &= 2u^2 + 2v^2 \\ &= 2(x_1^2 + x_2^2) \end{align*} Since $x^T H x$ is equal to 2 times the sum of squares of $x$ 's components, it is non-negative for any choice of $x$ . Therefore, the Hessian matrix of $f(x, y)$ is positive semidefinite and therefore the function is convex.Am I correct ?","I have to show that the function is convex so I am thinking to demonstrate that the graph of the function lies above any of its tangent planes.But I want to prove it with linear algebra perspective. To do this, i used the definition of a convex function in terms of its Hessian matrix. The Hessian matrix of a function with two variables is a matrix that contains its second partial derivatives. If the Hessian matrix is positive semidefinite then the function is convex.Am I correct ? To find the Hessian matrix of , i need to compute its second partial derivatives: Therefore, the Hessian matrix of is: To show that the Hessian matrix of is positive semidefinite, i need to demonstrate that for any vector , the quadratic form is non-negative: Since is equal to 2 times the sum of squares of 's components, it is non-negative for any choice of . Therefore, the Hessian matrix of is positive semidefinite and therefore the function is convex.Am I correct ?","f(x, y) = x^2 + y^2 2 \times 2 f(x, y) = x^2 + y^2 \begin{align*}
\frac{\partial^2 f}{\partial x^2} &= \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right) \\
&= \frac{\partial}{\partial x}\left(2x\right) \\
&= 2
\end{align*} \begin{align*}
\frac{\partial^2 f}{\partial y^2} &= \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right) \\
&= \frac{\partial}{\partial y}\left(2y\right) \\
&= 2
\end{align*} \begin{align*}
\frac{\partial^2 f}{\partial x\partial y} &= \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right) \\
&= \frac{\partial}{\partial x}\left(0\right) \\
&= 0
\end{align*} \begin{align*}
\frac{\partial^2 f}{\partial y\partial x} &= \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right) \\
&= \frac{\partial}{\partial y}\left(0\right) \\
&= 0
\end{align*} f(x, y) = x^2 + y^2 H = \begin{pmatrix}
2 & 0 \\
0 & 2\\
\end{pmatrix} f(x, y) = x^2 + y^2 x = [u, v]^T x^T H x \begin{align*}
x^T H x &= \begin{bmatrix} u & v \end{bmatrix}^T \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \begin{bmatrix} u \ v \end{bmatrix} \\
&= \begin{bmatrix} 2u & 2v \end{bmatrix} \begin{bmatrix} u \ v \end{bmatrix} \\
&= 2u^2 + 2v^2 \\
&= 2(x_1^2 + x_2^2)
\end{align*} x^T H x x x f(x, y)","['multivariable-calculus', 'functions', 'solution-verification', 'convex-analysis']"
4,Surface area over a scalar field,Surface area over a scalar field,,"Let S be the sphere of radius $r$ centered at the origin. Define $f:S\to\Bbb R$ as $f(x)=\frac{1}{||x -x_0||}$ , where $x_0=(a,b,c)\in\Bbb R^3\setminus S$ . Compute $\int_S fdS$ (Scalar surface integral). My solution is to first parametrize $S$ by $\sigma:D:=[0,\pi]\times[0,2\pi]\to\Bbb R^3$ where $\sigma(\varphi, \theta) = (r\cos\theta\sin\varphi, r\sin\theta\sin\varphi, r\cos\varphi)$ . Hence, after some computation, $$\int_S fdS = \int_D \frac{r^2\sin\varphi}{\sqrt{(r\cos\theta\sin\varphi-x_0)^2 + (r\sin\theta\sin\varphi-y_0)^2 + (r\cos\varphi-z_0)^2}}d\varphi d\theta = \int_D \frac{r^2\sin\varphi \;d\varphi\;d\theta}{ \sqrt {r^2 - 2r\cos\theta \sin\varphi x_0 - 2r\sin\theta\sin\varphi y_0 -2r\cos\varphi z_0 + x_0^2 +y_0^2 + z_0^2} }$$ But from there I don't know how to compute it. In fact, after researching more about this problem, I found that it has already been solved here: https://math.stackexchange.com/a/3225231/1174522 But I don't understand how $$\sqrt{r^2+\lVert x_0\rVert^2-2r\,\lVert x_0\rVert\cos\varphi} = \sqrt{(r\cos\theta\sin\varphi-x_0)^2 + (r\sin\theta\sin\varphi-y_0)^2 + (r\cos\varphi-z_0)^2}$$ and then how he solves it.","Let S be the sphere of radius centered at the origin. Define as , where . Compute (Scalar surface integral). My solution is to first parametrize by where . Hence, after some computation, But from there I don't know how to compute it. In fact, after researching more about this problem, I found that it has already been solved here: https://math.stackexchange.com/a/3225231/1174522 But I don't understand how and then how he solves it.","r f:S\to\Bbb R f(x)=\frac{1}{||x -x_0||} x_0=(a,b,c)\in\Bbb R^3\setminus S \int_S fdS S \sigma:D:=[0,\pi]\times[0,2\pi]\to\Bbb R^3 \sigma(\varphi, \theta) = (r\cos\theta\sin\varphi, r\sin\theta\sin\varphi, r\cos\varphi) \int_S fdS = \int_D \frac{r^2\sin\varphi}{\sqrt{(r\cos\theta\sin\varphi-x_0)^2 + (r\sin\theta\sin\varphi-y_0)^2 + (r\cos\varphi-z_0)^2}}d\varphi d\theta = \int_D \frac{r^2\sin\varphi \;d\varphi\;d\theta}{ \sqrt {r^2 - 2r\cos\theta \sin\varphi x_0 - 2r\sin\theta\sin\varphi y_0 -2r\cos\varphi z_0 + x_0^2 +y_0^2 + z_0^2} } \sqrt{r^2+\lVert x_0\rVert^2-2r\,\lVert x_0\rVert\cos\varphi} = \sqrt{(r\cos\theta\sin\varphi-x_0)^2 + (r\sin\theta\sin\varphi-y_0)^2 + (r\cos\varphi-z_0)^2}","['integration', 'multivariable-calculus', 'proof-explanation']"
5,Dealing with variables in multivariable limit questions,Dealing with variables in multivariable limit questions,,"I am trying to solve the question below. My initial approach was to look at the limit along the x and y axis which both yield 0. Now, I moved on to look at the limit along the path of y = mx. When I did this, i noticed that I had to fix my x and substitute y for mx instead of saying x = y/m and y = mx. When I did the latter, I ended up going in circles and it produced the original equation. My question is, why do I need to fix the x value? What is the mathematical reasoning?","I am trying to solve the question below. My initial approach was to look at the limit along the x and y axis which both yield 0. Now, I moved on to look at the limit along the path of y = mx. When I did this, i noticed that I had to fix my x and substitute y for mx instead of saying x = y/m and y = mx. When I did the latter, I ended up going in circles and it produced the original equation. My question is, why do I need to fix the x value? What is the mathematical reasoning?",,"['limits', 'multivariable-calculus', 'problem-solving']"
6,"Apostol: Find $\lim_{(x,y)\to (0,0)} \sqrt{\frac{x^2+y^2}{(x+3x^3+3xy^2)^2+(y+3y^3+3yx^2)^2+x^2+y^2}}$?",Apostol: Find ?,"\lim_{(x,y)\to (0,0)} \sqrt{\frac{x^2+y^2}{(x+3x^3+3xy^2)^2+(y+3y^3+3yx^2)^2+x^2+y^2}}","The following problem is from Chapter 8 of Apostol's Calculus , Vol II (a) Find a vector $V(x,y,z)$ normal to the surface $$z(x,y)=\sqrt{x^2+y^2}+(x^2+y^2)^{3/2}$$ at a general point $(x,y,z)$ of the surface, $(x,y,z)\neq (0,0,0)$ . (b) Find the cosine of the angle $\theta$ between $V(x,y,z)$ and the z-axis and determine the limit of $\cos{\theta}$ as $(x,y,z)\to > (0,0,0)$ . I used the fundamental product of the vector equation $$\vec{r}(x,y)=\langle x, y, z(x,y)\rangle$$ which is $$\frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y}=\left \langle \frac{x}{\sqrt{x^2+y^2}}-\frac{3x^3+3xy^2}{\sqrt{x^2+y^2}}, -\frac{y}{\sqrt{x^2+y^2}}-\frac{3x^3+3xy^2}{\sqrt{x^2+y^2}}, 1 \right\rangle$$ Then $$\frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y}\cdot \hat{k}=\left\lVert \frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y} \right\rVert \cos{\theta}$$ $$\cos{\theta}=\sqrt{\frac{x^2+y^2}{(x+3x^3+3xy^2)^2+(y+3y^3+3yx^2)^2+x^2+y^2}}$$ My question is how to find the limit of this expression as $(x,y)\to (0,0)$ ?","The following problem is from Chapter 8 of Apostol's Calculus , Vol II (a) Find a vector normal to the surface at a general point of the surface, . (b) Find the cosine of the angle between and the z-axis and determine the limit of as . I used the fundamental product of the vector equation which is Then My question is how to find the limit of this expression as ?","V(x,y,z) z(x,y)=\sqrt{x^2+y^2}+(x^2+y^2)^{3/2} (x,y,z) (x,y,z)\neq (0,0,0) \theta V(x,y,z) \cos{\theta} (x,y,z)\to
> (0,0,0) \vec{r}(x,y)=\langle x, y, z(x,y)\rangle \frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y}=\left \langle \frac{x}{\sqrt{x^2+y^2}}-\frac{3x^3+3xy^2}{\sqrt{x^2+y^2}}, -\frac{y}{\sqrt{x^2+y^2}}-\frac{3x^3+3xy^2}{\sqrt{x^2+y^2}}, 1 \right\rangle \frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y}\cdot \hat{k}=\left\lVert \frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y} \right\rVert \cos{\theta} \cos{\theta}=\sqrt{\frac{x^2+y^2}{(x+3x^3+3xy^2)^2+(y+3y^3+3yx^2)^2+x^2+y^2}} (x,y)\to (0,0)","['limits', 'multivariable-calculus']"
7,Integral over circular disk region to Integral over rectangular region.,Integral over circular disk region to Integral over rectangular region.,,"I was going through the double integral over the general region in the plane and had a question. When we integrate over a circular disk : $$ D=\{(x, y) \mid \delta(x) \leq y \leq \gamma(x), a \leq x \leq b\} $$ We do, $$ \int_a^b \int_{\delta(x)}^{\gamma(x)} f(x, y) d y d x $$ I would like to integrate over the semi circular disk defined by, $\gamma(x)=\sqrt{1-(x-2)^2}$ and $\delta(x)=0$ . Instead of this if I wanted to make my work easier and instead of dealing with a case where the elementary region is a circle or semi-circle, I want to make it a rectangle. Because I need the same volume, I would need the base area to be equal at every height, which essentially boils down to making the base area the same. As in both cases, the base area won't decrease as a function of height.(i) So, now let's make the area equal. $\operatorname{Radius}(r)=\frac{|b-a|}{2}$ Area of the circular disk $=$ Area of the rectangle. $$ [D=\{(x, y) \mid d \leq y \leq c, a \leq x \leq b\} $$ $$ \frac{\pi|b-a|^2}{4}=|b-a| \times l $$ $l=\frac{\pi|b-a|}{4}$ $l$ is $|d-c|$ I can make $c=0$ , and that would mean $d=\frac{\pi|b-a|}{4}$ Now, I do $$ \int_{\mathrm{a}}^{\frac{\mathrm{a+b}}{2}} \int_0^{\frac{\pi|b-a|}{4}} f(x, y) d y d x $$ We do $\frac{a+b}{2}$ because I want to find the volume which has the elementary region as a semi-circle, so I should make the rectangle's area equal to the semi-circle which we can do by halving the length of the rectangle. (ii) My question is what is the conceptually wrong thing I am doing here? There must be some conceptual gap that I am not seeing. Are the statements in (i) and (ii) wrong?","I was going through the double integral over the general region in the plane and had a question. When we integrate over a circular disk : We do, I would like to integrate over the semi circular disk defined by, and . Instead of this if I wanted to make my work easier and instead of dealing with a case where the elementary region is a circle or semi-circle, I want to make it a rectangle. Because I need the same volume, I would need the base area to be equal at every height, which essentially boils down to making the base area the same. As in both cases, the base area won't decrease as a function of height.(i) So, now let's make the area equal. Area of the circular disk Area of the rectangle. is I can make , and that would mean Now, I do We do because I want to find the volume which has the elementary region as a semi-circle, so I should make the rectangle's area equal to the semi-circle which we can do by halving the length of the rectangle. (ii) My question is what is the conceptually wrong thing I am doing here? There must be some conceptual gap that I am not seeing. Are the statements in (i) and (ii) wrong?","
D=\{(x, y) \mid \delta(x) \leq y \leq \gamma(x), a \leq x \leq b\}
 
\int_a^b \int_{\delta(x)}^{\gamma(x)} f(x, y) d y d x
 \gamma(x)=\sqrt{1-(x-2)^2} \delta(x)=0 \operatorname{Radius}(r)=\frac{|b-a|}{2} = 
[D=\{(x, y) \mid d \leq y \leq c, a \leq x \leq b\}
 
\frac{\pi|b-a|^2}{4}=|b-a| \times l
 l=\frac{\pi|b-a|}{4} l |d-c| c=0 d=\frac{\pi|b-a|}{4} 
\int_{\mathrm{a}}^{\frac{\mathrm{a+b}}{2}} \int_0^{\frac{\pi|b-a|}{4}} f(x, y) d y d x
 \frac{a+b}{2}","['integration', 'multivariable-calculus']"
8,"Instantiating single-variable ""ordinary"" derivatives from total derivatives","Instantiating single-variable ""ordinary"" derivatives from total derivatives",,"We've been learning total derivatives in class and I had some questions about how our ""ordinary"" single-variable derivatives are instantiated from all this. Let's say we have a differentiable real-to-real function $f: \mathbb{R} \rightarrow \mathbb{R}$ . Just to see how everything relates, I will try to compute every ""derivative-related object"" that we've learned (e.g. total, ordinary, directional etc.) By definition, the total derivative $Df(x)$ is the linear function $T : \mathbb{R} \rightarrow \mathbb{R}$ satisfying the following limit: $\lim_{h \to 0} \frac{|f(x+h)-f(x)-A(h)|}{|h|} = \lim_{h \to 0} |\frac{f(x+h)-f(x)}{h} - \frac{A(h)}{h}| = |\lim_{h \to 0}[\frac{f(x+h)-f(x)}{h}-\frac{A(h)}{h}]|$ $= |f'(x) - lim_{h \to 0}\frac{A(h)}{h}|$ (I believe it can be shown that for all x, $Df(x)$ exists iff $f'(x)$ exists?) This makes it clear that, if $Df(x)$ exists, then it must be $(Df(x))(v) = f'(x)v$ The directional derivative $D_{v}f(x)$ is simply the total derivative evaluated at v i.e. $D_{v}f(x)=(Df(x))(v)=f'(x)v$ Thus we get that: $$(1) f'(x) = Df(x)(1) = D_{1}f(x)$$ or equivalently $$ (2) f'(x) = Df(x)(e_{1}) = D_{e_{1}}f(x)$$ where $D_{e_{1}}f(x)$ is the partial derivative of f wrt x (the only possible direction). My questions are: Is it only accidental that (1) and (2) holds or is there some special reason behind it? Given that (1) and (2), is it fair to say that partial derivatives (each of which is the total derivative evaluated on vectors of the standard basis) is a true generalization of a ""ordinary derivative"" like $f'(x)$ in higher dimensions?","We've been learning total derivatives in class and I had some questions about how our ""ordinary"" single-variable derivatives are instantiated from all this. Let's say we have a differentiable real-to-real function . Just to see how everything relates, I will try to compute every ""derivative-related object"" that we've learned (e.g. total, ordinary, directional etc.) By definition, the total derivative is the linear function satisfying the following limit: (I believe it can be shown that for all x, exists iff exists?) This makes it clear that, if exists, then it must be The directional derivative is simply the total derivative evaluated at v i.e. Thus we get that: or equivalently where is the partial derivative of f wrt x (the only possible direction). My questions are: Is it only accidental that (1) and (2) holds or is there some special reason behind it? Given that (1) and (2), is it fair to say that partial derivatives (each of which is the total derivative evaluated on vectors of the standard basis) is a true generalization of a ""ordinary derivative"" like in higher dimensions?",f: \mathbb{R} \rightarrow \mathbb{R} Df(x) T : \mathbb{R} \rightarrow \mathbb{R} \lim_{h \to 0} \frac{|f(x+h)-f(x)-A(h)|}{|h|} = \lim_{h \to 0} |\frac{f(x+h)-f(x)}{h} - \frac{A(h)}{h}| = |\lim_{h \to 0}[\frac{f(x+h)-f(x)}{h}-\frac{A(h)}{h}]| = |f'(x) - lim_{h \to 0}\frac{A(h)}{h}| Df(x) f'(x) Df(x) (Df(x))(v) = f'(x)v D_{v}f(x) D_{v}f(x)=(Df(x))(v)=f'(x)v (1) f'(x) = Df(x)(1) = D_{1}f(x)  (2) f'(x) = Df(x)(e_{1}) = D_{e_{1}}f(x) D_{e_{1}}f(x) f'(x),"['analysis', 'multivariable-calculus', 'derivatives']"
9,"Prove triangle inequality on $\hat{N}(x,y) = \sqrt{x^2 + xy + y^2}$",Prove triangle inequality on,"\hat{N}(x,y) = \sqrt{x^2 + xy + y^2}","I'm having problems to demonstrate triangle inequality on the above function. So far I've tried: \begin{align} \hat{N}(u+v) &= \hat{N}((u_1, u_2)+ (v_1,v_2))\\ &= \hat{N}((u_1+v_1, u_2+v_2))\\ &= \sqrt{(u_1+v_1)^2+(u_1+v_1)(u_2+v_2)+(u_2+v_2)^2}\\ &= \sqrt{(u_1^2+u_1u_2+u_2^2) + (v_1^2+v_1v_2+v_2^2) + 2(u_1v_1+u_2v_2) + u_1v_2 + u_2v_1} \end{align} For the restant terms inside the square root, using Cauchy-Schwarz inequality we have that: \begin{align} |2(u_1v_1 + u_2v_2)| &\leq 2(u_1^2+u_2^2)(v_1^2+v_2^2)\\ |u_1v_2 + u_2v_1| &\leq (u_1^2+u_2^2)(v_1^2+v_2^2) \end{align} Summing up the inequalities: \begin{align} |2(u_1v_1 + u_2v_2)| + |u_1v_2 + u_2v_1| &\leq 3(u_1^2+u_2^2)(v_1^2+v_2^2) \end{align} Using that $ |a+b| \leq |a| + |b|$ : \begin{align} |2(u_1v_1 + u_2v_2) + u_1v_2 + u_2v_1| &\leq 3(u_1^2+u_2^2)(v_1^2+v_2^2) \end{align} Maybe it could be of some help, but I'm stuck from this point.","I'm having problems to demonstrate triangle inequality on the above function. So far I've tried: For the restant terms inside the square root, using Cauchy-Schwarz inequality we have that: Summing up the inequalities: Using that : Maybe it could be of some help, but I'm stuck from this point.","\begin{align}
\hat{N}(u+v) &= \hat{N}((u_1, u_2)+ (v_1,v_2))\\
&= \hat{N}((u_1+v_1, u_2+v_2))\\
&= \sqrt{(u_1+v_1)^2+(u_1+v_1)(u_2+v_2)+(u_2+v_2)^2}\\
&= \sqrt{(u_1^2+u_1u_2+u_2^2) + (v_1^2+v_1v_2+v_2^2) + 2(u_1v_1+u_2v_2) + u_1v_2 + u_2v_1}
\end{align} \begin{align}
|2(u_1v_1 + u_2v_2)| &\leq 2(u_1^2+u_2^2)(v_1^2+v_2^2)\\
|u_1v_2 + u_2v_1| &\leq (u_1^2+u_2^2)(v_1^2+v_2^2)
\end{align} \begin{align}
|2(u_1v_1 + u_2v_2)| + |u_1v_2 + u_2v_1| &\leq 3(u_1^2+u_2^2)(v_1^2+v_2^2)
\end{align}  |a+b| \leq |a| + |b| \begin{align}
|2(u_1v_1 + u_2v_2) + u_1v_2 + u_2v_1| &\leq 3(u_1^2+u_2^2)(v_1^2+v_2^2)
\end{align}","['multivariable-calculus', 'vector-spaces', 'normed-spaces']"
10,Solve $\left(3x^2y-xy\right)dx+\left(2x^3y^2+x^3y^4\right)dy=0$,Solve,\left(3x^2y-xy\right)dx+\left(2x^3y^2+x^3y^4\right)dy=0,"$\left(3x^2y-xy\right)dx+\left(2x^3y^2+x^3y^4\right)dy=0$ I'm trying to solve this first-order differential equation. I know it's not an exact equation so I'm trying to use the method taught in class to solve it. I get stuck trying to do the integrating factor. Below is what I have: $\frac{\partial \:}{\partial \:x}\left(M\right)=\frac{\partial }{\partial x}\left(3x^2y-xy\right) = 6xy-y$ $\frac{\partial \:}{\partial y}\left(N\right)=\frac{\partial }{\partial y}\left(2x^3y^2+x^3y^4\right)dy = 4x^3y+4x^3y^3$ And since $\frac{\partial \:}{\partial \:y}\left(N\right)\neq\frac{\partial }{\partial \:x}\left(M\right)$ , have that it is not exact. So we apply the formula to get an integrating factor: $\xi =\frac{\left(\:\frac{\partial \:}{\partial \:y}-\frac{\partial }{\partial x}\right)}{N}$ to get a function $\xi(x)$ , or the formula $\xi =\frac{\left(\:\frac{\partial \:}{\partial \:y}-\frac{\partial }{\partial x}\right)}{-M}\:$ to get a function $\xi(y)$ . We use $\xi$ to get an integrating factor $\mu(x)=e^{\int \:\xi(x) dx}$ or $\mu(y)=e^{\int \:\xi(x) dy}$ . Now, when I apply either one of the formulas for $\xi$ , I always get a result dependent on both $x$ and $y$ , so I'm unable to get the integrating factor. Is there supposed to be a simpler way to solve this? I'm using this method because it's what was taught in class, but is there another simple way to solve this that I'm not seeing?","I'm trying to solve this first-order differential equation. I know it's not an exact equation so I'm trying to use the method taught in class to solve it. I get stuck trying to do the integrating factor. Below is what I have: And since , have that it is not exact. So we apply the formula to get an integrating factor: to get a function , or the formula to get a function . We use to get an integrating factor or . Now, when I apply either one of the formulas for , I always get a result dependent on both and , so I'm unable to get the integrating factor. Is there supposed to be a simpler way to solve this? I'm using this method because it's what was taught in class, but is there another simple way to solve this that I'm not seeing?",\left(3x^2y-xy\right)dx+\left(2x^3y^2+x^3y^4\right)dy=0 \frac{\partial \:}{\partial \:x}\left(M\right)=\frac{\partial }{\partial x}\left(3x^2y-xy\right) = 6xy-y \frac{\partial \:}{\partial y}\left(N\right)=\frac{\partial }{\partial y}\left(2x^3y^2+x^3y^4\right)dy = 4x^3y+4x^3y^3 \frac{\partial \:}{\partial \:y}\left(N\right)\neq\frac{\partial }{\partial \:x}\left(M\right) \xi =\frac{\left(\:\frac{\partial \:}{\partial \:y}-\frac{\partial }{\partial x}\right)}{N} \xi(x) \xi =\frac{\left(\:\frac{\partial \:}{\partial \:y}-\frac{\partial }{\partial x}\right)}{-M}\: \xi(y) \xi \mu(x)=e^{\int \:\xi(x) dx} \mu(y)=e^{\int \:\xi(x) dy} \xi x y,"['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'homogeneous-equation']"
11,"Analyzing whether the unit circle $S=\{(x,y)\in \mathbb{R}^2|x^2+y^2=1\}$ is Jordan measurable - 3 different ways",Analyzing whether the unit circle  is Jordan measurable - 3 different ways,"S=\{(x,y)\in \mathbb{R}^2|x^2+y^2=1\}","Let $D$ be a bounded set and $C$ a rectangular cuboid with $D\subset C$ . Let $P=\{C_i|i\in I\}$ be a partition of $C$ . In our script $D$ is Jordan-measurable if: $D$ is Jordan-measurable  if and only if the indicator function $\chi_D$ is integrable on $C$ . Then the Volume of $D$ is given by $F(D)=\int \limits_{C}^{}\chi_D$ . Its inner $F_i$ and outer $F_o$ volume are equal, with $F_i(D):=\underset{P}{\text{sup}}\sum \limits_{\underset{i\in I}{C_i\subset D}} F(C_i)$ $F_o(D):=\underset{P}{\text{inf}}\sum \limits_{\underset{i\in I}{C_i\cap D\neq \emptyset}}F(C_i)$ A set $D\subset \mathbb{R}^n$ is a Jordan null set if it is Jordan-measurable and $F(D)=0$ . $D$ is Jordan-measurable if and only if the boundary $\partial D$ is a Jordan null set. Is the unit circle $S=\{(x,y)\in\mathbb{R}^2|x^2+y^2=1\}$ Jordan-measurable? 1) To calculte $\chi_S$ over a bounded set, one needs to continue the function over a cuboid $C$ with $S\subset C$ . In our case $C=[-1,1]^2$ ensures that $S\subset C$ . Using the Fubini theorem: $$F(S)=\int \limits_C \chi_S=\int \limits_{[-1,1]\times [-1,1]}^{}\chi_S=\int \limits_{-1}^{1}\int \limits_{-1}^{1}\chi_S \, dy \, dx$$ The condition is that $(x,y)$ is on the unit-circle, so $y=\pm\sqrt{1-x^2}$ . I don't know how to continue? 2) I didn't want to just write ""no idea"" so here are some thoughts: I know that $S$ has no inner points ( i proved that in another exercise), so I think that $C_i\not \subset S$ for every partition $P=\{C_i|i\in I\}$ of $C$ . So I'm not sure what $F_i(S)=\underset{P}{\text{sup}}\sum \limits_{\underset{i\in I}{C_i\subset S}} F(C_i)$ even means in this case. A partition with $C_i\cap S\neq \emptyset$ is possible so $F_o(S)=\underset{P}{\text{inf}}\sum \limits_{\underset{i\in I}{C_i\cap S\neq \emptyset}}F(C_i)$ would be a meaningful expression. The boundary of the unit circle is $\partial S=S$ . So I need to use the definition $1)$ or $2)$ .","Let be a bounded set and a rectangular cuboid with . Let be a partition of . In our script is Jordan-measurable if: is Jordan-measurable  if and only if the indicator function is integrable on . Then the Volume of is given by . Its inner and outer volume are equal, with A set is a Jordan null set if it is Jordan-measurable and . is Jordan-measurable if and only if the boundary is a Jordan null set. Is the unit circle Jordan-measurable? 1) To calculte over a bounded set, one needs to continue the function over a cuboid with . In our case ensures that . Using the Fubini theorem: The condition is that is on the unit-circle, so . I don't know how to continue? 2) I didn't want to just write ""no idea"" so here are some thoughts: I know that has no inner points ( i proved that in another exercise), so I think that for every partition of . So I'm not sure what even means in this case. A partition with is possible so would be a meaningful expression. The boundary of the unit circle is . So I need to use the definition or .","D C D\subset C P=\{C_i|i\in I\} C D D \chi_D C D F(D)=\int \limits_{C}^{}\chi_D F_i F_o F_i(D):=\underset{P}{\text{sup}}\sum \limits_{\underset{i\in I}{C_i\subset D}} F(C_i) F_o(D):=\underset{P}{\text{inf}}\sum \limits_{\underset{i\in I}{C_i\cap D\neq \emptyset}}F(C_i) D\subset \mathbb{R}^n F(D)=0 D \partial D S=\{(x,y)\in\mathbb{R}^2|x^2+y^2=1\} \chi_S C S\subset C C=[-1,1]^2 S\subset C F(S)=\int \limits_C \chi_S=\int \limits_{[-1,1]\times [-1,1]}^{}\chi_S=\int \limits_{-1}^{1}\int \limits_{-1}^{1}\chi_S \, dy \, dx (x,y) y=\pm\sqrt{1-x^2} S C_i\not \subset S P=\{C_i|i\in I\} C F_i(S)=\underset{P}{\text{sup}}\sum \limits_{\underset{i\in I}{C_i\subset S}} F(C_i) C_i\cap S\neq \emptyset F_o(S)=\underset{P}{\text{inf}}\sum \limits_{\underset{i\in I}{C_i\cap S\neq \emptyset}}F(C_i) \partial S=S 1) 2)","['real-analysis', 'measure-theory', 'multivariable-calculus']"
12,"Check an $\epsilon-\delta$ proof of $\lim_{ (x,y) \to (3,1)} \frac{x}{y} = 3$",Check an  proof of,"\epsilon-\delta \lim_{ (x,y) \to (3,1)} \frac{x}{y} = 3","I am going to prove $$\lim_{ (x,y) \to (3,1)} \frac{x}{y} = 3$$ by using $\epsilon-\delta$ argument. My Attempts Preliminary Analysis For every $\epsilon >0$ , we need to find a suitable $\delta$ such that: If $\Vert (x,y)-(3,1)\Vert _2 < \delta$ then $\left| \frac{x}{y} - 3\right| < \epsilon.$ First note that $$\left| \frac{x}{y} - 3\right| = \left| \frac{x}{y} - 3y + 3y -3\right| \le \left|\frac{1}{y}\right||x-3| + 3|y-1|.$$ It is obvious that $$|x-3| \le \Vert (x,y)-(3,1)\Vert_2\; \text{and} \; |y-1| \le \Vert (x,y)-(3,1)\Vert_2.$$ Suppose $\Vert (x,y)-(3,1)\Vert_2 < \frac{1}{2}$ then $|y-1| <\frac{1}{2}.$ As a result we have $$\frac{1}{2} < y < \frac{3}{2} \Longrightarrow \frac{1}{|y|} < 2.$$ Hence, $$\left| \frac{x}{y} - 3\right| \le \left|\frac{1}{y}\right||x-3| + 3|y-1|< 2\delta+3\delta = 5\delta.$$ Formal Proof For every $\epsilon > 0$ , we must choose $$\delta= \min\left\{\frac{1}{2}, \frac{\epsilon}{5}\right\}.$$ If $\Vert (x,y)-(3,1)\Vert _2 < \delta$ then $\left| \frac{x}{y} - 3\right| < \epsilon.$ Is my proof correct?","I am going to prove by using argument. My Attempts Preliminary Analysis For every , we need to find a suitable such that: If then First note that It is obvious that Suppose then As a result we have Hence, Formal Proof For every , we must choose If then Is my proof correct?","\lim_{ (x,y) \to (3,1)} \frac{x}{y} = 3 \epsilon-\delta \epsilon >0 \delta \Vert (x,y)-(3,1)\Vert _2 < \delta \left| \frac{x}{y} - 3\right| < \epsilon. \left| \frac{x}{y} - 3\right| = \left| \frac{x}{y} - 3y + 3y -3\right| \le \left|\frac{1}{y}\right||x-3| + 3|y-1|. |x-3| \le \Vert (x,y)-(3,1)\Vert_2\; \text{and} \; |y-1| \le \Vert (x,y)-(3,1)\Vert_2. \Vert (x,y)-(3,1)\Vert_2 < \frac{1}{2} |y-1| <\frac{1}{2}. \frac{1}{2} < y < \frac{3}{2} \Longrightarrow \frac{1}{|y|} < 2. \left| \frac{x}{y} - 3\right| \le \left|\frac{1}{y}\right||x-3| + 3|y-1|< 2\delta+3\delta = 5\delta. \epsilon > 0 \delta= \min\left\{\frac{1}{2}, \frac{\epsilon}{5}\right\}. \Vert (x,y)-(3,1)\Vert _2 < \delta \left| \frac{x}{y} - 3\right| < \epsilon.","['multivariable-calculus', 'solution-verification', 'epsilon-delta']"
13,During constrained optimization with inequalities why must the gradient of the objective be in the same direction as the gradient of the constraint?,During constrained optimization with inequalities why must the gradient of the objective be in the same direction as the gradient of the constraint?,,"I have been reading about constrained optimization and understood when there are just equality constraints but am having trouble understanding when there are inequality constraints. I was initially following KhanAcademy's multivariable calculus course but wanted to expand from the simple Lagrange Multipliers treatment so I began looking at pdfs online from different college courses. I am thinking of things in 2 variables for now and let's just assume one inequality constraint for simplicity. I understand that during an optimization of the form: $$ \begin{gathered} \max _{x, y} f\left(x, y\right) \text { subject to : } \\ g\left(x, y\right) \leq b . \end{gathered} $$ that there are two possible cases. One is where the candidate point is in the interior of the boundary (i.e. $g(x, y) \lt b$ ) and that case makes sense to me that we are basically looking for unconstrainted local extrema (maxima here) by checking where $\nabla f(x, y)=\mathbf{0}$ . For the other case, the candidate points are on the boundary. I understand that for candidate point $(x^*, y^*)$ , $\nabla g(x^*, y^*)$ will point outwards from the boundary and that $\nabla f(x^*, y^*)$ will be parallel to it. All the resources I am looking at say that when looking for maxima that you only consider the point if the gradient $\nabla f(x^*, y^*)$ points outward/in the same direction as $\nabla g(x^*, y^*)$ . The explanation is that if $\nabla f(x^*, y^*)$ pointed inward then we know that there are some feasible points such that when $f$ is evaluated there that it will be greater than $f(x^*, y^*)$ . Let's say we find such a point $(x^*, y^*)$ where the $\nabla f(x^*, y^*)$ is pointed inward. I agree with the above reasoning that there will be some feasible points in the interior that will have higher values of $f$ . Let's say one of these feasible points in the interior with a higher value of $f$ is $(x^1, y^1)$ . What I don't follow is how we can safely ignore the point $(x^*, y^*)$ without being 100% sure that the local unconstrained maximum case above (so setting $\nabla f(x, y)=\mathbf{0}$ ) will find the point $(x^1, y^1)$ . Couldn't it be the case that although $f$ 'dips' down on the interior of the boundary near $(x^*, y^*)$ that it will rise back up in some way that it won't be a local maximum and therefore the KKT conditions won't find it? In that case wouldn't we want to consider $(x^*, y^*)$ since at the very least we know it is an extrema of the boundary? Apologies in advance if any of the math above feels hand wavey. I am trying to get an intuitive sense of things rather than something super rigorous. My basic question is that if we know there is some point in the interior ( $g(x, y) \lt b$ ) such that $f$ will be greater than $f(x^*, y^*)$ are we guaranteed (or guaranteed under some conditions) to find it by just looking for unconstrained maxima? There might be some theorem I am missing here that'd help. In one dimension I can picture that if $f$ decreases/stays equal within the feasible region (after initially increasing right at the boundary near $(x^*, y^*)$ ) that we immediately have a critical point by Rolle's theorem. And that if $f$ only increases then the other end of the boundary will have the max point. But I can't seem to make that same leap for 2+ dimensions. If it helps I am thinking of a set up like the one in the picture below where the red region is the set of feasible points. Thanks in advance for the help!","I have been reading about constrained optimization and understood when there are just equality constraints but am having trouble understanding when there are inequality constraints. I was initially following KhanAcademy's multivariable calculus course but wanted to expand from the simple Lagrange Multipliers treatment so I began looking at pdfs online from different college courses. I am thinking of things in 2 variables for now and let's just assume one inequality constraint for simplicity. I understand that during an optimization of the form: that there are two possible cases. One is where the candidate point is in the interior of the boundary (i.e. ) and that case makes sense to me that we are basically looking for unconstrainted local extrema (maxima here) by checking where . For the other case, the candidate points are on the boundary. I understand that for candidate point , will point outwards from the boundary and that will be parallel to it. All the resources I am looking at say that when looking for maxima that you only consider the point if the gradient points outward/in the same direction as . The explanation is that if pointed inward then we know that there are some feasible points such that when is evaluated there that it will be greater than . Let's say we find such a point where the is pointed inward. I agree with the above reasoning that there will be some feasible points in the interior that will have higher values of . Let's say one of these feasible points in the interior with a higher value of is . What I don't follow is how we can safely ignore the point without being 100% sure that the local unconstrained maximum case above (so setting ) will find the point . Couldn't it be the case that although 'dips' down on the interior of the boundary near that it will rise back up in some way that it won't be a local maximum and therefore the KKT conditions won't find it? In that case wouldn't we want to consider since at the very least we know it is an extrema of the boundary? Apologies in advance if any of the math above feels hand wavey. I am trying to get an intuitive sense of things rather than something super rigorous. My basic question is that if we know there is some point in the interior ( ) such that will be greater than are we guaranteed (or guaranteed under some conditions) to find it by just looking for unconstrained maxima? There might be some theorem I am missing here that'd help. In one dimension I can picture that if decreases/stays equal within the feasible region (after initially increasing right at the boundary near ) that we immediately have a critical point by Rolle's theorem. And that if only increases then the other end of the boundary will have the max point. But I can't seem to make that same leap for 2+ dimensions. If it helps I am thinking of a set up like the one in the picture below where the red region is the set of feasible points. Thanks in advance for the help!","
\begin{gathered}
\max _{x, y} f\left(x, y\right) \text { subject to : } \\
g\left(x, y\right) \leq b .
\end{gathered}
 g(x, y) \lt b \nabla f(x, y)=\mathbf{0} (x^*, y^*) \nabla g(x^*, y^*) \nabla f(x^*, y^*) \nabla f(x^*, y^*) \nabla g(x^*, y^*) \nabla f(x^*, y^*) f f(x^*, y^*) (x^*, y^*) \nabla f(x^*, y^*) f f (x^1, y^1) (x^*, y^*) \nabla f(x, y)=\mathbf{0} (x^1, y^1) f (x^*, y^*) (x^*, y^*) g(x, y) \lt b f f(x^*, y^*) f (x^*, y^*) f","['real-analysis', 'multivariable-calculus', 'optimization', 'lagrange-multiplier', 'karush-kuhn-tucker']"
14,Differentiable Functions Multivariate Definition,Differentiable Functions Multivariate Definition,,"The book ""Nonlinear Programming"" by Bazaraa, Sherali, and Shetty has the following definition in its appendix: Let $S$ be a nonempty set in $\mathbb{R}^n$ , $\bar{x} \in \operatorname{int} S$ and let $f:S\to \mathbb{R}$ . Then $f$ is said to be differentiable at $\bar{x}$ if there is a vector $\nabla f (\bar{x})$ in $\mathbb{R}^n$ called the gradient of $f$ at $\bar{x}$ and a function $\beta$ satisfying $\beta (\bar{x};x) \to 0$ as $x \to \bar{x}$ such that \begin{align*} f(x) = f(\bar{x}) + \nabla f(\bar{x})^t(x-\bar{x}) + \|x-\bar{x}\|\beta (\bar{x}; x) \quad \forall x \in S. \end{align*} The gradient vector consists of the partial derivatives, that is, \begin{align*} \nabla f (\bar{x})^t = \left(\frac{\partial f(\bar{x})}{\partial x_1}, \frac{\partial f (\bar{x})}{\partial x_2}, \ldots, \frac{\partial f(\bar{x})}{\partial x_n} \right). \end{align*} Can someone please explain why this makes sense and where this definition comes from? I've looked in two analysis books and did not see this definition. And what does the semicolon mean in this case in "" $\beta (\bar{x}; x)$ ""?","The book ""Nonlinear Programming"" by Bazaraa, Sherali, and Shetty has the following definition in its appendix: Let be a nonempty set in , and let . Then is said to be differentiable at if there is a vector in called the gradient of at and a function satisfying as such that The gradient vector consists of the partial derivatives, that is, Can someone please explain why this makes sense and where this definition comes from? I've looked in two analysis books and did not see this definition. And what does the semicolon mean in this case in "" ""?","S \mathbb{R}^n \bar{x} \in \operatorname{int} S f:S\to \mathbb{R} f \bar{x} \nabla f (\bar{x}) \mathbb{R}^n f \bar{x} \beta \beta (\bar{x};x) \to 0 x \to \bar{x} \begin{align*}
f(x) = f(\bar{x}) + \nabla f(\bar{x})^t(x-\bar{x}) + \|x-\bar{x}\|\beta (\bar{x}; x) \quad \forall x \in S.
\end{align*} \begin{align*}
\nabla f (\bar{x})^t = \left(\frac{\partial f(\bar{x})}{\partial x_1}, \frac{\partial f (\bar{x})}{\partial x_2}, \ldots, \frac{\partial f(\bar{x})}{\partial x_n} \right).
\end{align*} \beta (\bar{x}; x)","['analysis', 'multivariable-calculus', 'optimization']"
15,"$\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)=\min_{r\ge0}(-r\lVert z\rVert+r^2)$",,"\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)=\min_{r\ge0}(-r\lVert z\rVert+r^2)","Let $y,z\in\mathbb{R}^n$ . Then $\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)=\min_{r\ge0}(-r\lVert z\rVert+r^2)$ . Proof) $-r\lVert z\rVert+r^2$ has the minimum at $r=\frac{\lVert z\rVert}{2}$ . Take $x_1$ such that $\lVert x_1-y\rVert=\frac{\lVert z\rVert}{2}$ . Then for any $x\in\mathbb{R}^n$ , $$ -\lVert x_1-y\rVert\lVert z\rVert+\lVert x_1-y\rVert^2\le -\lVert x-y\rVert\lVert z\rVert+\lVert x-y\rVert^2\le \langle z,x-y\rangle+\lVert x-y\rVert^2.$$ So $\min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)\ge\min_{r\ge0}(-r\lVert z\rVert+r^2)$ . How do I prove the converse?","Let . Then . Proof) has the minimum at . Take such that . Then for any , So . How do I prove the converse?","y,z\in\mathbb{R}^n \min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)=\min_{r\ge0}(-r\lVert z\rVert+r^2) -r\lVert z\rVert+r^2 r=\frac{\lVert z\rVert}{2} x_1 \lVert x_1-y\rVert=\frac{\lVert z\rVert}{2} x\in\mathbb{R}^n  -\lVert x_1-y\rVert\lVert z\rVert+\lVert x_1-y\rVert^2\le -\lVert x-y\rVert\lVert z\rVert+\lVert x-y\rVert^2\le \langle z,x-y\rangle+\lVert x-y\rVert^2. \min_{x\in\mathbb{R}^n}(\langle z,x-y\rangle+\lVert x-y\rVert^2)\ge\min_{r\ge0}(-r\lVert z\rVert+r^2)","['real-analysis', 'linear-algebra']"
16,Does existence and continuity of partial derivatives imply differentiability in Normed Vector Spaces?,Does existence and continuity of partial derivatives imply differentiability in Normed Vector Spaces?,,"I'm wondering if the 'Normed Vector Space version' of the following theorem holds: Theorem: let $A\subseteq \mathbb{R}^n$ be open and let $f:A\to \mathbb{R}^m$ have continuous partial derivatives $\partial f_i/\partial x_j$ on $A$ . Then $f$ is differentiable on $A$ . In particular, the total derivative can be extended to NVS by considering the Fréchet Derivative . If we extend the definition of partial derivatives as follows: Throughout the post let $V$ and $W$ be NVSs, with $V'\subseteq V$ open and $f$ a function $V'\to W$ . Definition: given $a\in V'$ and a vector $v\in V$ , we define (assuming the limit exists) $$\frac{\partial f}{\partial v}(a) := \lim_{t\rightarrow 0}\frac{f(a+tv)-f(a)}{t},$$ and, in the case $v=e_i$ for a basis vector $e_i$ , $$\frac{\partial f}{\partial x_i}(a) := \frac{\partial f}{\partial e_i}(a).$$ then, is either of the following statements true? Theorem (?): if $f:V'\to W$ has continuous partial derivatives $\partial f/\partial v$ (for any $v\in V$ ) on $A$ , then $f$ is differentiable on $A$ . Theorem (?): let $\dim V = n$ and $\dim W = m$ . If $f:V'\to W$ have continuous partial derivatives $\partial f_i/\partial x_j$ on $A$ , then $f$ is differentiable on $A$ .","I'm wondering if the 'Normed Vector Space version' of the following theorem holds: Theorem: let be open and let have continuous partial derivatives on . Then is differentiable on . In particular, the total derivative can be extended to NVS by considering the Fréchet Derivative . If we extend the definition of partial derivatives as follows: Throughout the post let and be NVSs, with open and a function . Definition: given and a vector , we define (assuming the limit exists) and, in the case for a basis vector , then, is either of the following statements true? Theorem (?): if has continuous partial derivatives (for any ) on , then is differentiable on . Theorem (?): let and . If have continuous partial derivatives on , then is differentiable on .","A\subseteq \mathbb{R}^n f:A\to \mathbb{R}^m \partial f_i/\partial x_j A f A V W V'\subseteq V f V'\to W a\in V' v\in V \frac{\partial f}{\partial v}(a) := \lim_{t\rightarrow 0}\frac{f(a+tv)-f(a)}{t}, v=e_i e_i \frac{\partial f}{\partial x_i}(a) := \frac{\partial f}{\partial e_i}(a). f:V'\to W \partial f/\partial v v\in V A f A \dim V = n \dim W = m f:V'\to W \partial f_i/\partial x_j A f A","['multivariable-calculus', 'normed-spaces', 'partial-derivative', 'frechet-derivative']"
17,"If $\partial_2 f\equiv 0$ on domain, then $f$ is independent of that variable?","If  on domain, then  is independent of that variable?",\partial_2 f\equiv 0 f,"Let $z=f(x,y)$ be a function of class $C^1(G;\mathbb{R})$ . a) If $\frac{\partial f}{\partial y}(x,y)\equiv 0$ in $G$ , can one assert that $f$ is independent of $y$ in $G$ ? b) Under what condition on the domain $G$ does the preceding question have an affirmative answer? Sorry if this question has been asked many times but my question is a bit different and it is about part b). But I did not find my question in existing topics. Part a) The answer is NO because one can consider the following function $f:G\to \mathbb{R}$ , where $G=\{(x,y)\in \mathbb{R}^2: 1<x^2+y^2<4,\ x>0\}$ and $$f(x,y) = \begin{cases} 0, & \text{if }(x,y)\in G, x\in [1,2), \\ (x-1)^2, & \text{if }(x,y)\in G,\ x\in (0,1),\ y>0, \\ -(x-1)^2, & \text{if }(x,y)\in G,\ x\in (0,1),\ y<0. \end{cases}$$ It is easy to see that $f$ depends on $y$ since $f(\frac{1}{2},1)\neq f(\frac{1}{2},-1)$ and $\frac{\partial f}{\partial y}\equiv 0$ on $G$ . Part b) I can prove that if $G$ is convex open set in $\mathbb{R}^2$ then the answer to part a) is YES. But I believe that it is true for larger family of sets in $\mathbb{R}^2$ . More precisely, if $G\subset \mathbb{R}^2$ is an open nonempty set with connected first projections then the answer to part a) is still YES. Here by connected first projections I mean that if $x\in \pi_1(G)$ , then for any $y_1, y_2$ such that $z_1:=(x,y_1)\in G$ and $z_2:=(x,y_2)\in G$ the line segment $[z_1,z_2]:=\{(x,\theta y_1+(1-\theta)y_2):\theta\in [0,1]\}\subset G$ . This is a larger class of sets since every convex set has this property. The proof is relatively easy. Indeed, let $x_0\in \pi_1(G)$ , then $\exists y_0: (x_0,y_0)\in G$ . Let $y$ be such that $(x_0,y)\in G$ and WLOG $y_0<y$ . Consider a function $\varphi:[y_0,y]\to \mathbb{R}$ defined as $t\mapsto f(x_0,t)$ . One can check that $\varphi$ is continuous and differentiable on $[y_0,y]$ and by MVT, we have: $\varphi(y)-\varphi(y_0)=\varphi'(c)(y-y_0)=\frac{\partial f}{\partial y}(x_0,c)(y-y_0)=0.$ Therefore, $\varphi(y)=\varphi(y_0)$ which is equivalent to $f(x_0,y)=f(x_0,y_0)$ . We have shown that for any $(x,y)\in G$ , we have $f(x,y)=f(x,F(x))$ , where $F:\pi_1(G)\to \mathbb{R}$ which is defined as: $x_0\mapsto y_0$ , where $(x_0,y_0)\in G$ . For example, this function $F$ can be constructed by Axiom of Choice. We are done since we have shown that $f(x,y)$ is independent of $y$ . So far I do not see any mistake. Thank you!","Let be a function of class . a) If in , can one assert that is independent of in ? b) Under what condition on the domain does the preceding question have an affirmative answer? Sorry if this question has been asked many times but my question is a bit different and it is about part b). But I did not find my question in existing topics. Part a) The answer is NO because one can consider the following function , where and It is easy to see that depends on since and on . Part b) I can prove that if is convex open set in then the answer to part a) is YES. But I believe that it is true for larger family of sets in . More precisely, if is an open nonempty set with connected first projections then the answer to part a) is still YES. Here by connected first projections I mean that if , then for any such that and the line segment . This is a larger class of sets since every convex set has this property. The proof is relatively easy. Indeed, let , then . Let be such that and WLOG . Consider a function defined as . One can check that is continuous and differentiable on and by MVT, we have: Therefore, which is equivalent to . We have shown that for any , we have , where which is defined as: , where . For example, this function can be constructed by Axiom of Choice. We are done since we have shown that is independent of . So far I do not see any mistake. Thank you!","z=f(x,y) C^1(G;\mathbb{R}) \frac{\partial f}{\partial y}(x,y)\equiv 0 G f y G G f:G\to \mathbb{R} G=\{(x,y)\in \mathbb{R}^2: 1<x^2+y^2<4,\ x>0\} f(x,y) =
\begin{cases}
0, & \text{if }(x,y)\in G, x\in [1,2), \\
(x-1)^2, & \text{if }(x,y)\in G,\ x\in (0,1),\ y>0, \\
-(x-1)^2, & \text{if }(x,y)\in G,\ x\in (0,1),\ y<0.
\end{cases} f y f(\frac{1}{2},1)\neq f(\frac{1}{2},-1) \frac{\partial f}{\partial y}\equiv 0 G G \mathbb{R}^2 \mathbb{R}^2 G\subset \mathbb{R}^2 x\in \pi_1(G) y_1, y_2 z_1:=(x,y_1)\in G z_2:=(x,y_2)\in G [z_1,z_2]:=\{(x,\theta y_1+(1-\theta)y_2):\theta\in [0,1]\}\subset G x_0\in \pi_1(G) \exists y_0: (x_0,y_0)\in G y (x_0,y)\in G y_0<y \varphi:[y_0,y]\to \mathbb{R} t\mapsto f(x_0,t) \varphi [y_0,y] \varphi(y)-\varphi(y_0)=\varphi'(c)(y-y_0)=\frac{\partial f}{\partial y}(x_0,c)(y-y_0)=0. \varphi(y)=\varphi(y_0) f(x_0,y)=f(x_0,y_0) (x,y)\in G f(x,y)=f(x,F(x)) F:\pi_1(G)\to \mathbb{R} x_0\mapsto y_0 (x_0,y_0)\in G F f(x,y) y","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'smooth-functions']"
18,What does the norm of the Jacobian represent?,What does the norm of the Jacobian represent?,,"Let $f ∈ C^1(R^n, R)$ . If $||Df(x_0)||≠0$ then f increases most in the direction $Df(x_0)$ at $x_0$ and if it is =0, the derivative of f is 0 in any direction. Could someone try to explain what this means? How can we evaluate the norm of the Jacobian? Is it Euclidean norm? I would really appreciate any help in visualizing what it actually means as well. It's my first time studying this and I would appreciate it if it is explained in the simplest terms possible since I do not have much knowledge on topology and multivariable calculus. Thank you!","Let . If then f increases most in the direction at and if it is =0, the derivative of f is 0 in any direction. Could someone try to explain what this means? How can we evaluate the norm of the Jacobian? Is it Euclidean norm? I would really appreciate any help in visualizing what it actually means as well. It's my first time studying this and I would appreciate it if it is explained in the simplest terms possible since I do not have much knowledge on topology and multivariable calculus. Thank you!","f ∈ C^1(R^n, R) ||Df(x_0)||≠0 Df(x_0) x_0","['multivariable-calculus', 'differential-topology', 'jacobian']"
19,Optimizing a nonlinear function with both equality and inequality contraints,Optimizing a nonlinear function with both equality and inequality contraints,,"I have the non-linear optimization problem $$\min f(a,b,c,x)=\int_0^R\frac{y}{1+(2ay+b)^2}dy =\frac{\log(\frac{(2aR+b)^2+1}{b^2+1})-2b(\arctan(2aR+b)-\arctan(b))}{8a^2}$$ subject to the constraints $$\begin{align} g_1(a,b,c,x)&=2\pi (\frac{aR^4}{4}+\frac{bR^3}{3}+\frac{cR^2}{2})-V=0,\\ g_2(a,b,c,x)&=-(ax^2+bx+c)\leq 0,\ \ \ \ (0 \leq x \leq R) \end{align}$$ where $R$ and $V$ are positive constants. I am ultimately looking for a triplet $(a,b,c)$ that minimizes $f$ and satisfies the constraints for all $x \in [0,R]$ . First of all, I noticed that the $x \in [0,R]$ part can be handled by replacing $g_2$ with a new function $g_2^\ast$ defined by $$g_2^\ast(a,b,c,x)=-(ax^2+bx+c)\cdot x(x-R)\leq 0.$$ It seems pretty clear that this would most likely not have a clean closed-form solution, and after some calculations, Sage seems to struggle to symbolically solve the system of equations given by even just forming the Lagrangian $$\mathcal{L}(a,b,c,\lambda)=f(a,b,c,x)+\lambda g_1(a,b,c,x)$$ for the equality constraint and taking $$\nabla \mathcal{L}(a,b,c,\lambda)=0.$$ From what I've learned so far, I'd assume that to solve the full problem with all the constraints implemented, I would form the (new, disregard the above example) Lagrangian $$\mathcal{L}(a,b,c,x,\lambda,\mu)=f(a,b,c,x)+\lambda g_1(a,b,c,x)+\mu g_2^\ast(a,b,c,x)$$ and solve the system given by $$\nabla \mathcal{L}(a,b,c,x,\lambda,\mu)=0.$$ I have concluded that this problem could probably only be solved numerically, and am looking for some method to do so. After some google searches, I have stumbled upon Sequential Quadratic Programming which (from what I've read off of Wikipedia) would allow one to numerically estimate solutions to nonlinear programming problems. Furthermore, I have found that since this problem involves both inequality and equality constraints, the usual method of Lagrange multipliers may not be sufficient, and that after an optimal solution is found numerically, checking it with the KKT conditions would verify its optimality(though not a necessary condition since my function is non-convex). As a result, it would be great if someone could direct me to any programs which could carry out these calculations or implement these algorithms for me, and also explain how to apply the KKT conditions here to verify the calculations after such numerical estimates have been made. I am very new to optimization, only just familiar with Lagrange multipliers, so please let me know if I have made mistakes or misunderstood something. Thanks in advance!","I have the non-linear optimization problem subject to the constraints where and are positive constants. I am ultimately looking for a triplet that minimizes and satisfies the constraints for all . First of all, I noticed that the part can be handled by replacing with a new function defined by It seems pretty clear that this would most likely not have a clean closed-form solution, and after some calculations, Sage seems to struggle to symbolically solve the system of equations given by even just forming the Lagrangian for the equality constraint and taking From what I've learned so far, I'd assume that to solve the full problem with all the constraints implemented, I would form the (new, disregard the above example) Lagrangian and solve the system given by I have concluded that this problem could probably only be solved numerically, and am looking for some method to do so. After some google searches, I have stumbled upon Sequential Quadratic Programming which (from what I've read off of Wikipedia) would allow one to numerically estimate solutions to nonlinear programming problems. Furthermore, I have found that since this problem involves both inequality and equality constraints, the usual method of Lagrange multipliers may not be sufficient, and that after an optimal solution is found numerically, checking it with the KKT conditions would verify its optimality(though not a necessary condition since my function is non-convex). As a result, it would be great if someone could direct me to any programs which could carry out these calculations or implement these algorithms for me, and also explain how to apply the KKT conditions here to verify the calculations after such numerical estimates have been made. I am very new to optimization, only just familiar with Lagrange multipliers, so please let me know if I have made mistakes or misunderstood something. Thanks in advance!","\min f(a,b,c,x)=\int_0^R\frac{y}{1+(2ay+b)^2}dy =\frac{\log(\frac{(2aR+b)^2+1}{b^2+1})-2b(\arctan(2aR+b)-\arctan(b))}{8a^2} \begin{align}
g_1(a,b,c,x)&=2\pi (\frac{aR^4}{4}+\frac{bR^3}{3}+\frac{cR^2}{2})-V=0,\\
g_2(a,b,c,x)&=-(ax^2+bx+c)\leq 0,\ \ \ \ (0 \leq x \leq R)
\end{align} R V (a,b,c) f x \in [0,R] x \in [0,R] g_2 g_2^\ast g_2^\ast(a,b,c,x)=-(ax^2+bx+c)\cdot x(x-R)\leq 0. \mathcal{L}(a,b,c,\lambda)=f(a,b,c,x)+\lambda g_1(a,b,c,x) \nabla \mathcal{L}(a,b,c,\lambda)=0. \mathcal{L}(a,b,c,x,\lambda,\mu)=f(a,b,c,x)+\lambda g_1(a,b,c,x)+\mu g_2^\ast(a,b,c,x) \nabla \mathcal{L}(a,b,c,x,\lambda,\mu)=0.","['multivariable-calculus', 'nonlinear-optimization', 'numerical-optimization']"
20,A proof of a sufficient condition to have that $f : A\subset\mathbb{R}^n\to\mathbb{R}$ is differentiable,A proof of a sufficient condition to have that  is differentiable,f : A\subset\mathbb{R}^n\to\mathbb{R},"Consider $f : O\subset\mathbb{R}^{n}\to\mathbb{R}$ a function continuously differentiable on $O$ an open set of $\mathbb{R}^n$ that is all its partial derivatives exist on $O$ and they are continuous. We want to show that it is a sufficient condition to have that $f$ is differentiable on $O$ . To do this, we consider the open set $N(X_0, r)$ where $X_0\in O$ . The idea is to use the theorem discussed here : Kind of Taylor expansion for functions of several variables? This theorem says the following : Consider $f:A\subset\mathbb{R}^n\to\mathbb{R}$ where all its partial derivatives exists on the open ball $N(X_o, r)$ with $X_0\in A$ . Consider $Z$ a vector of $\mathbb{R}^n$ with $\lVert Z\rVert\leq r$ . Then we have $f(X_0 + Z) = f(X_0) + \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i$ with $V_i=(z_1, ..., z_{i-1}, \theta z_{i}, 0, ..., 0),\quad 0<\theta<1$ Now, consider $Z\in\mathbb{R}^n$ such that $\lVert Z\rVert < r$ . Then, we can use the theorem  above at the point $X_0$ , it follows that $f(X_0 + Z) = f(X_0) + \sum_{i=1}^{n}f_{x_i}^{'}(X_0 + V_i)z_i$ . If we consider the approximation $f_{x_i}^{'}(X_0)$ instead of $f_{x_i}^{'}(X_0 + V_i)$ we can write the following : $\epsilon(X_0, Z) = f(X_0 + Z) - f(X_0) - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i $ And then we make appear the equality of interest for the differentiability that is : $\epsilon_1(X_0, Z) =\frac{1}{\lVert Z\rVert}\left[ f(X_0 + Z) - f(X_0) - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i\right] $ Using the fact that $f(X_0 + Z) - f(X_0) = \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i$ we have : $\lvert\epsilon_1(X_0, Z)\rvert =\left\lvert\frac{1}{\lVert Z\rVert}\left[ \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i\right]\right\rvert$ $\quad\quad\quad\quad\quad = \left\lvert\sum_{i=1}^{n}\frac{z_i}{\lVert Z\rVert}\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert $ $\quad\quad\quad\quad\;\;\;\leq\sum_{i=1}^{n}\left\lvert\frac{z_i}{\lVert Z\rVert}\right\rvert\left\lvert\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert\leq\sum_{i=1}^{n}\left\lvert\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert $ But $\forall 1\leq i\leq n : \lVert V_i\rVert\leq\lVert Z\rVert\implies\lim_{Z\to 0_{\mathbb{R}^n}} V_i =0$ Thus using the continuity of the absolute value and of $f_{x_i}^{'}$ at $X_0$ we get $\lim_{Z\to 0_{\mathbb{R}^n}}\lvert\epsilon_1(X_0, Z)\rvert = 0 $ And this holds for all $X\in O$ , which concludes the proof. Is this seems correct or do you see some improvement possible for this proof ? Thank you a lot !","Consider a function continuously differentiable on an open set of that is all its partial derivatives exist on and they are continuous. We want to show that it is a sufficient condition to have that is differentiable on . To do this, we consider the open set where . The idea is to use the theorem discussed here : Kind of Taylor expansion for functions of several variables? This theorem says the following : Consider where all its partial derivatives exists on the open ball with . Consider a vector of with . Then we have with Now, consider such that . Then, we can use the theorem  above at the point , it follows that . If we consider the approximation instead of we can write the following : And then we make appear the equality of interest for the differentiability that is : Using the fact that we have : But Thus using the continuity of the absolute value and of at we get And this holds for all , which concludes the proof. Is this seems correct or do you see some improvement possible for this proof ? Thank you a lot !","f : O\subset\mathbb{R}^{n}\to\mathbb{R} O \mathbb{R}^n O f O N(X_0, r) X_0\in O f:A\subset\mathbb{R}^n\to\mathbb{R} N(X_o, r) X_0\in A Z \mathbb{R}^n \lVert Z\rVert\leq r f(X_0 + Z) = f(X_0) + \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i V_i=(z_1, ..., z_{i-1}, \theta z_{i}, 0, ..., 0),\quad 0<\theta<1 Z\in\mathbb{R}^n \lVert Z\rVert < r X_0 f(X_0 + Z) = f(X_0) + \sum_{i=1}^{n}f_{x_i}^{'}(X_0 + V_i)z_i f_{x_i}^{'}(X_0) f_{x_i}^{'}(X_0 + V_i) \epsilon(X_0, Z) = f(X_0 + Z) - f(X_0) - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i  \epsilon_1(X_0, Z) =\frac{1}{\lVert Z\rVert}\left[ f(X_0 + Z) - f(X_0) - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i\right]  f(X_0 + Z) - f(X_0) = \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i \lvert\epsilon_1(X_0, Z)\rvert =\left\lvert\frac{1}{\lVert Z\rVert}\left[ \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i\right]\right\rvert \quad\quad\quad\quad\quad = \left\lvert\sum_{i=1}^{n}\frac{z_i}{\lVert Z\rVert}\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert  \quad\quad\quad\quad\;\;\;\leq\sum_{i=1}^{n}\left\lvert\frac{z_i}{\lVert Z\rVert}\right\rvert\left\lvert\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert\leq\sum_{i=1}^{n}\left\lvert\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert  \forall 1\leq i\leq n : \lVert V_i\rVert\leq\lVert Z\rVert\implies\lim_{Z\to 0_{\mathbb{R}^n}} V_i =0 f_{x_i}^{'} X_0 \lim_{Z\to 0_{\mathbb{R}^n}}\lvert\epsilon_1(X_0, Z)\rvert = 0  X\in O","['calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
21,Gradient of a homogenous function - proof with a help function ; Unsure about a derivative and about using the multidimensional chain rule,Gradient of a homogenous function - proof with a help function ; Unsure about a derivative and about using the multidimensional chain rule,,"Let $k$ be an integer. A function $f : \mathbb{R^n} \to \mathbb{R}$ is called homogenous of degree $k$ if $f(\lambda x) = \lambda^k f(x)$ for all $\lambda \in \mathbb{R}$ and $x \in \mathbb{R^n}$ . Prove that if $f$ is homogenous of degree $k$ then $x \cdot \nabla f(x) = kf(x)$ . Task: Prove this statement using the function $g:\mathbb{R}^+\rightarrow \mathbb{R}, \lambda \mapsto f(\lambda x)$ for a fixed $x\in \mathbb{R^n}$ Proof: Let $x\in \mathbb{R^n}$ be arbitrary but fixed. i) $g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)=\frac{d}{d\lambda}(\lambda^kf(x))=k \lambda^{k-1}f(x)$ $\Rightarrow g'(1)=kf(x)$ ii) $g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)=x\cdot f'(\lambda x)\Rightarrow g'(1)=x\cdot f'(x)$ $(i),(ii)\Rightarrow g'(1)=x\cdot f'(x)=kf(x)$ almost q.e.d. I have problems with $\frac{d}{d\lambda}f(\lambda x)=x\cdot f'(\lambda x)$ in (ii) because it should probably be $\frac{d}{d\lambda}f(\lambda x)=x\cdot \nabla f(\lambda x)$ but I cannot justify it. Is the notation $g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)$ wrong? I used this notation  because $g$ is a onedimensional function. I tried to use the multivariable chain rule with $h:\mathbb{R}^+\rightarrow \mathbb{R^n}, \lambda \mapsto \lambda x$ $g(\lambda)=f(h(\lambda))\Rightarrow g'(\lambda)=Dg(\lambda)=D(f\circ h)(\lambda)=Df(\lambda x)\cdot Dh(\lambda)=\nabla f(\lambda x)\cdot h'(\lambda)=\nabla f(\lambda x)\cdot x= x\cdot \nabla f(\lambda x)=0$ because $\nabla f(\lambda x)=0$ for a fixed $x$ . Apart from the point mentioned, is the proof correct?","Let be an integer. A function is called homogenous of degree if for all and . Prove that if is homogenous of degree then . Task: Prove this statement using the function for a fixed Proof: Let be arbitrary but fixed. i) ii) almost q.e.d. I have problems with in (ii) because it should probably be but I cannot justify it. Is the notation wrong? I used this notation  because is a onedimensional function. I tried to use the multivariable chain rule with because for a fixed . Apart from the point mentioned, is the proof correct?","k f : \mathbb{R^n} \to \mathbb{R} k f(\lambda x) = \lambda^k f(x) \lambda \in \mathbb{R} x \in \mathbb{R^n} f k x \cdot \nabla f(x) = kf(x) g:\mathbb{R}^+\rightarrow \mathbb{R}, \lambda \mapsto f(\lambda x) x\in \mathbb{R^n} x\in \mathbb{R^n} g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)=\frac{d}{d\lambda}(\lambda^kf(x))=k \lambda^{k-1}f(x) \Rightarrow g'(1)=kf(x) g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)=x\cdot f'(\lambda x)\Rightarrow g'(1)=x\cdot f'(x) (i),(ii)\Rightarrow g'(1)=x\cdot f'(x)=kf(x) \frac{d}{d\lambda}f(\lambda x)=x\cdot f'(\lambda x) \frac{d}{d\lambda}f(\lambda x)=x\cdot \nabla f(\lambda x) g'(\lambda)=\frac{d}{d\lambda}f(\lambda x) g h:\mathbb{R}^+\rightarrow \mathbb{R^n}, \lambda \mapsto \lambda x g(\lambda)=f(h(\lambda))\Rightarrow g'(\lambda)=Dg(\lambda)=D(f\circ h)(\lambda)=Df(\lambda x)\cdot Dh(\lambda)=\nabla f(\lambda x)\cdot h'(\lambda)=\nabla f(\lambda x)\cdot x= x\cdot \nabla f(\lambda x)=0 \nabla f(\lambda x)=0 x","['real-analysis', 'multivariable-calculus', 'solution-verification']"
22,Divergence Theorem,Divergence Theorem,,"I'm looking for an alternate proof of a result. Let $A,B,C\in\mathbb{S}^d$ . Let $T$ be the intersection of $\mathbb{S}^d$ with the cone generated by $A,B,C$ . Then, $T$ is a spherical triangle. The vertices $A,B,C$ are opposite arcs with lengths $a,b,c$ . Call $[T]$ the area of $T$ . The centroid of $T$ is $$g:=\frac{1}{[T]}\int_T xd\mu$$ where $\mu$ is uniform on the sphere. JE Brock found the centroid of a spherical triangle $T=\triangle ABC$ to be (paraphrased, but it checks out for me) $$g=\frac{1}{2[T]}\left(\frac{A\times B}{|A\times B|}c+\frac{B\times C}{|B\times C|}a+\frac{C\times A}{|C\times A|}b\right)$$ Thinking of $A\times B/|A\times B|$ as the unit vector perpendicular to side $c$ , we can write this as $$\int_T xd\mu=g*[T]=\frac{1}{2}\int_{\partial T} \vec{n} ds$$ where $\vec{n}$ is the inward pointing unit vector. This formulation looks like the divergence theorem. So, my question is how to prove it with the divergence or Stokes Theorem. $$\int_T\mathrm{div}F=\int_{\partial T} F\cdot\vec{n}$$ I've tried separating components setting the vector field $F$ to $(x_1^2,0,\dots,0)$ to get a linear term in the divergence, but that doesn't match the right side where I expect $F=(1,0,\dots,0)$ to get just the normal around the boundary. So, I suspect I need to use Stoke's Theorem on manifolds. The form can be the unit tangent so that the derivative points toward the center of the sphere. That would fit the form I'm looking for, I just need some details. Does that seem like the right course? Thanks! Happy Holidays!","I'm looking for an alternate proof of a result. Let . Let be the intersection of with the cone generated by . Then, is a spherical triangle. The vertices are opposite arcs with lengths . Call the area of . The centroid of is where is uniform on the sphere. JE Brock found the centroid of a spherical triangle to be (paraphrased, but it checks out for me) Thinking of as the unit vector perpendicular to side , we can write this as where is the inward pointing unit vector. This formulation looks like the divergence theorem. So, my question is how to prove it with the divergence or Stokes Theorem. I've tried separating components setting the vector field to to get a linear term in the divergence, but that doesn't match the right side where I expect to get just the normal around the boundary. So, I suspect I need to use Stoke's Theorem on manifolds. The form can be the unit tangent so that the derivative points toward the center of the sphere. That would fit the form I'm looking for, I just need some details. Does that seem like the right course? Thanks! Happy Holidays!","A,B,C\in\mathbb{S}^d T \mathbb{S}^d A,B,C T A,B,C a,b,c [T] T T g:=\frac{1}{[T]}\int_T xd\mu \mu T=\triangle ABC g=\frac{1}{2[T]}\left(\frac{A\times B}{|A\times B|}c+\frac{B\times C}{|B\times C|}a+\frac{C\times A}{|C\times A|}b\right) A\times B/|A\times B| c \int_T xd\mu=g*[T]=\frac{1}{2}\int_{\partial T} \vec{n} ds \vec{n} \int_T\mathrm{div}F=\int_{\partial T} F\cdot\vec{n} F (x_1^2,0,\dots,0) F=(1,0,\dots,0)","['geometry', 'multivariable-calculus', 'differential-geometry', 'multiple-integral', 'stokes-theorem']"
23,How does a divergence-free field $\vec F$ imply $\vec F=\vec\nabla\times\vec G$?,How does a divergence-free field  imply ?,\vec F \vec F=\vec\nabla\times\vec G,"I am experiencing a slight dilemma here. Starting from the Divergence theorem, I am trying to show how a divergence-free field $\vec F$ implies that $\vec F$ can be written as the curl of another vector, say $\vec G$ . Below I have written some steps which outline my current chain of logic. Note that I'm assuming we don't already know that $\vec F =\vec\nabla\times \vec G$ , since this is what I am trying to prove. Also, I am not using anything like the Helmholtz theorem either. Here is goes: Step $0$ : Start with the divergence theorem in $\mathbb R^3$ : $$\iint_S\vec F\cdot d\vec S=\iiint _V\text{div}(\vec F)\;dV,$$ where $S$ is of course a closed surface. Step $1$ : If $\vec F$ is a $C^1$ vector field on an open region containing the volume V and $\text{div}(\vec F)=0$ everywhere, then $$\iint_S\vec F\cdot d\vec S=0,$$ which says the flux through any closed surface must be zero. Step $2$ : Suppose we break the closed surface $S$ into two orientable surfaces $S_1$ and $S_2$ (note that $S_1$ and $S_2$ share the same boundary curve). Let one surface assume the orientation of the other. E.g, if $S_2$ assumes the orientation of $S_1$ , then $S_2$ starts off with ""negative"" orientation, which we will denote by $-S_2$ . Since $\iint_{-S_2}\vec F\cdot d\vec S =-\iint_{S_2}\vec F\cdot d\vec S$ , hence $$0=\iint_S\vec F\cdot d\vec S=\iint_{S_1} \vec F\cdot d\vec S + \iint_{-S_2} \vec F\cdot d\vec S=\iint_{S_1} \vec F\cdot d\vec S - \iint_{S_2} \vec F\cdot d\vec S.$$ Therefore, $$\iint_{S_1}\vec F \cdot d\vec S=\iint_{S_2}\vec F \cdot d\vec S.$$ This says that the flux of a divergence-free vector field through any two open surfaces is surface independent and only depends on the surface boundary. In other words, for any fixed non-empty boundary, we may deform the surface $S$ . You may view this thread for more details regarding this step: Let $F$ be a vector field in $\mathbb{R}^3$. If $F$ is divergence free, we may deform the surface. Why? Step $3$ : This is where I am stuck. I've seen several articles somehow relate this surface independence property back to Stokes' theorem. In particular, I've seen: If $\text{div}(\vec F)=0$ everywhere, then $$\tag{$\star$}\iint_{\text{any open surface}}\!\!\!\!\vec F\cdot d\vec S=\text{constant}=\oint_{\partial S}\vec G\cdot d\vec r=\iint_S\text{curl}(\vec G)\cdot d\vec S,$$ which implies $\vec F=\vec\nabla\times\vec G$ . Here's the link to the article where that's from (on page 3): https://physics56.files.wordpress.com/2016/01/tutorial-7-scalar-and-vector-potential3.pdf My confusion lies with equation $(\star)$ . I just don't see how one simply relates the LHS to Stokes' theorem. I know this is analogous to the case when $\text{curl}(\vec F)=0$ , which by Stokes' theorem implies path independence and moreover, the existence of a scalar function $\psi$ , such that $\vec F=\vec\nabla \psi$ . We can show that this is indeed the case for curl-free fields by using the fundament theorem of calculus. Again, I'm trying to come to this conclusion using vector calculus but not Helmholtz theorem or calculus on manifolds. Just following my outlined chain of logic starting from the Divergence theorem. I appreciate any input I can get. :)","I am experiencing a slight dilemma here. Starting from the Divergence theorem, I am trying to show how a divergence-free field implies that can be written as the curl of another vector, say . Below I have written some steps which outline my current chain of logic. Note that I'm assuming we don't already know that , since this is what I am trying to prove. Also, I am not using anything like the Helmholtz theorem either. Here is goes: Step : Start with the divergence theorem in : where is of course a closed surface. Step : If is a vector field on an open region containing the volume V and everywhere, then which says the flux through any closed surface must be zero. Step : Suppose we break the closed surface into two orientable surfaces and (note that and share the same boundary curve). Let one surface assume the orientation of the other. E.g, if assumes the orientation of , then starts off with ""negative"" orientation, which we will denote by . Since , hence Therefore, This says that the flux of a divergence-free vector field through any two open surfaces is surface independent and only depends on the surface boundary. In other words, for any fixed non-empty boundary, we may deform the surface . You may view this thread for more details regarding this step: Let $F$ be a vector field in $\mathbb{R}^3$. If $F$ is divergence free, we may deform the surface. Why? Step : This is where I am stuck. I've seen several articles somehow relate this surface independence property back to Stokes' theorem. In particular, I've seen: If everywhere, then which implies . Here's the link to the article where that's from (on page 3): https://physics56.files.wordpress.com/2016/01/tutorial-7-scalar-and-vector-potential3.pdf My confusion lies with equation . I just don't see how one simply relates the LHS to Stokes' theorem. I know this is analogous to the case when , which by Stokes' theorem implies path independence and moreover, the existence of a scalar function , such that . We can show that this is indeed the case for curl-free fields by using the fundament theorem of calculus. Again, I'm trying to come to this conclusion using vector calculus but not Helmholtz theorem or calculus on manifolds. Just following my outlined chain of logic starting from the Divergence theorem. I appreciate any input I can get. :)","\vec F \vec F \vec G \vec F =\vec\nabla\times \vec G 0 \mathbb R^3 \iint_S\vec F\cdot d\vec S=\iiint _V\text{div}(\vec F)\;dV, S 1 \vec F C^1 \text{div}(\vec F)=0 \iint_S\vec F\cdot d\vec S=0, 2 S S_1 S_2 S_1 S_2 S_2 S_1 S_2 -S_2 \iint_{-S_2}\vec F\cdot d\vec S =-\iint_{S_2}\vec F\cdot d\vec S 0=\iint_S\vec F\cdot d\vec S=\iint_{S_1} \vec F\cdot d\vec S + \iint_{-S_2} \vec F\cdot d\vec S=\iint_{S_1} \vec F\cdot d\vec S - \iint_{S_2} \vec F\cdot d\vec S. \iint_{S_1}\vec F \cdot d\vec S=\iint_{S_2}\vec F \cdot d\vec S. S 3 \text{div}(\vec F)=0 \tag{\star}\iint_{\text{any open surface}}\!\!\!\!\vec F\cdot d\vec S=\text{constant}=\oint_{\partial S}\vec G\cdot d\vec r=\iint_S\text{curl}(\vec G)\cdot d\vec S, \vec F=\vec\nabla\times\vec G (\star) \text{curl}(\vec F)=0 \psi \vec F=\vec\nabla \psi","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'stokes-theorem', 'divergence-theorem']"
24,Find the closest point to the origin of $\{\alpha x + \beta y + \gamma z = c\}\cap\{x+y+z=1\}.$,Find the closest point to the origin of,\{\alpha x + \beta y + \gamma z = c\}\cap\{x+y+z=1\}.,"On the line given by this intersection: $$\{\alpha x + \beta y + \gamma z = c\}\cap\{x+y+z=1\}.$$ We need to find the closest point to the origin. My attempt: Let's look at $f(x,y,z)=x^2 + y^2 + z^2$ , this is the euclidean norm squared. Now, setting the constraints $g_1(x,y,z)=x+y+z-1$ and $g_2(x,y,z)=\alpha x + \beta y + \gamma z - c$ . We can denote that the intersection is a compact set since it's closed and blocked. Hence, by Weierstrass theorem we can guarantee that $f$ has minimum and maximum in the intersection above. Note that $f$ , $g_1$ and $g_2$ are $C^1$ functions. In addition, $\nabla g_1=(1,1,1)$ and $\nabla g_2=(\alpha,\beta,\gamma)$ are linearly dependent if and only if $\alpha = \beta = \gamma$ . If they're equal we have two cases: One is $\alpha=c$ , in this case the intersection isn't empty iff $\alpha=c=1$ and then we'll get that $(x,y,z)=(\frac{1}{3}, \frac{1}{3},\frac{1}{3})$ . The other is that $\alpha \ne c$ and then the intersect is empty and there isn't a point on which we can say it is the closest to the origin since there're no points at all. Under the assumption that at least one of then is different from the rest, we can use Lagrange multiplier theorem and obtain the following: $\nabla f=\lambda_1\nabla$$g_1+\lambda_2\nabla$$g_2$ . Denote this equations system: \begin{cases}2x=\lambda_1+\alpha\lambda_2 \\ 2y=\lambda_1+\beta\lambda_2 \\ 2z=\lambda_1+\gamma\lambda_2 \\ x+y+z-1=0 \\ \alpha x + \beta y + \gamma z - c=0\end{cases} . I got stuck here, thanks in advanced!","On the line given by this intersection: We need to find the closest point to the origin. My attempt: Let's look at , this is the euclidean norm squared. Now, setting the constraints and . We can denote that the intersection is a compact set since it's closed and blocked. Hence, by Weierstrass theorem we can guarantee that has minimum and maximum in the intersection above. Note that , and are functions. In addition, and are linearly dependent if and only if . If they're equal we have two cases: One is , in this case the intersection isn't empty iff and then we'll get that . The other is that and then the intersect is empty and there isn't a point on which we can say it is the closest to the origin since there're no points at all. Under the assumption that at least one of then is different from the rest, we can use Lagrange multiplier theorem and obtain the following: . Denote this equations system: . I got stuck here, thanks in advanced!","\{\alpha x + \beta y + \gamma z = c\}\cap\{x+y+z=1\}. f(x,y,z)=x^2 + y^2 + z^2 g_1(x,y,z)=x+y+z-1 g_2(x,y,z)=\alpha x + \beta y + \gamma z - c f f g_1 g_2 C^1 \nabla g_1=(1,1,1) \nabla g_2=(\alpha,\beta,\gamma) \alpha = \beta = \gamma \alpha=c \alpha=c=1 (x,y,z)=(\frac{1}{3}, \frac{1}{3},\frac{1}{3}) \alpha \ne c \nabla f=\lambda_1\nablag_1+\lambda_2\nablag_2 \begin{cases}2x=\lambda_1+\alpha\lambda_2 \\ 2y=\lambda_1+\beta\lambda_2 \\ 2z=\lambda_1+\gamma\lambda_2 \\ x+y+z-1=0 \\ \alpha x + \beta y + \gamma z - c=0\end{cases}","['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
25,"If $f(x,y)$ is concave such that $f_1 < 0, f_2 > 0$, how are the level curves supposed to look like?","If  is concave such that , how are the level curves supposed to look like?","f(x,y) f_1 < 0, f_2 > 0","Suppose $f(x,y)$ is a concave function such that $\frac{\partial f}{\partial x} < 0$ and $\frac{\partial f}{\partial y} > 0$ . How are the level curves supposed to look like? Can I get an example of such a concave function? Just to be clear, this isn't a homework question. I am learning multivariable calculus and I am stuck with convex and concave functions, especially visualizing the various types of such functions. My attempt: To answer (1) , I have drawn two possible images below. The curves are essentially $L_c = \{(x,y) : f(x,y) = c\}$ or the level sets of $f$ . In each figure, the arrow represents the direction in which the level curves are attaining higher values (of $c$ ). $\hskip2in$ We know that $tf(x) + (1-t)f(y) \leq f(tx + (1-t)y)$ $(t \in (0,1))$ for a concave function. If we pick two points $p$ and $q$ on a level curve, then it's easy to notice that $f(x(r), y(r)) > \max\{f(x(p), y(p)), f(x(q), y(q))\}$ . This is only true for the first image and not the second image. Is there a better way than this, just for visualization? $\hskip2in$ I don't have an answer to (2).","Suppose is a concave function such that and . How are the level curves supposed to look like? Can I get an example of such a concave function? Just to be clear, this isn't a homework question. I am learning multivariable calculus and I am stuck with convex and concave functions, especially visualizing the various types of such functions. My attempt: To answer (1) , I have drawn two possible images below. The curves are essentially or the level sets of . In each figure, the arrow represents the direction in which the level curves are attaining higher values (of ). We know that for a concave function. If we pick two points and on a level curve, then it's easy to notice that . This is only true for the first image and not the second image. Is there a better way than this, just for visualization? I don't have an answer to (2).","f(x,y) \frac{\partial f}{\partial x} < 0 \frac{\partial f}{\partial y} > 0 L_c = \{(x,y) : f(x,y) = c\} f c \hskip2in tf(x) + (1-t)f(y) \leq f(tx + (1-t)y) (t \in (0,1)) p q f(x(r), y(r)) > \max\{f(x(p), y(p)), f(x(q), y(q))\} \hskip2in","['multivariable-calculus', 'convex-analysis']"
26,Doubt in finding volume and setting up the limits?,Doubt in finding volume and setting up the limits?,,"I am currently confused in how to find volume enclosed between two surfaces $z_1=f_1(x,y),z_2(x,y)=f_2(x,y)$ After going through some online resources, I thought the general way of doing this is: $\iint_{D} (z_2-z_1) dxdy$ where it will be $(z_2-z_1)$ or $(z_1-z_2)$ depending on the surfaces. $D=\{(x,y)\in \mathbb{R}^2:  f_1(x,y)=f_2(x,y)$ (eliminating $z$ from the two equation) $\}$ However I got stuck while solving the problem . Find the volume between the surfaces $x+y+2z=2,2x+y+z=4$ in the first octent. I could figure out that it would be of the form $$\iint _D \left\{(4-(2x+y))-\frac{(2-(x+y))}{2}\right\}\,dxdy$$ However how do I figure out $D$ ? Should I take the projection of the two surfaces on the $xy$ plane and then find out the limits as we did in double integration? Or should I equate the two surfaces and then find out the limit?","I am currently confused in how to find volume enclosed between two surfaces After going through some online resources, I thought the general way of doing this is: where it will be or depending on the surfaces. (eliminating from the two equation) However I got stuck while solving the problem . Find the volume between the surfaces in the first octent. I could figure out that it would be of the form However how do I figure out ? Should I take the projection of the two surfaces on the plane and then find out the limits as we did in double integration? Or should I equate the two surfaces and then find out the limit?","z_1=f_1(x,y),z_2(x,y)=f_2(x,y) \iint_{D} (z_2-z_1) dxdy (z_2-z_1) (z_1-z_2) D=\{(x,y)\in \mathbb{R}^2:  f_1(x,y)=f_2(x,y) z \} x+y+2z=2,2x+y+z=4 \iint _D \left\{(4-(2x+y))-\frac{(2-(x+y))}{2}\right\}\,dxdy D xy","['multivariable-calculus', 'volume', 'multiple-integral']"
27,Tangency points of an ellipsoid with the coordinate planes,Tangency points of an ellipsoid with the coordinate planes,,"Suppose you have an ellipsoid with known semi-axes, say $a,b,c$ .  The ellipsoid is then rotated so that the three axes of the ellipsoid are aligned with the three columns of a known rotation matrix $R$ .  Keeping this orientation, you place the ellipsoid on the $xy$ plane, in the first octant (where $ x \ge 0, y \ge 0, z \ge 0$ ).  Next, you drag the ellipsoid, while maintaining its spatial orientation, along the $xy$ plane, in a direction parallel to the $x$ axis, till it touches the $yz$ plane, then you drag it parallel to the $y$ axis, till it touches the $xz$ plane.  Now the ellipsoid is tangent to the $3$ coordinate planes.  The question is find the tangency points of the ellipsoid with the three coordinate planes . As for context, this question can be considered a extension of $2D$ ellipses in general orientation, to the $3D$ case involving an ellipsoid instead of an ellipse. My Progress: The algebraic equation of an ellipsoid is $ (r - r_0)^T Q (r - r_0) = 1 $ where $ r = [x , y, z ]^T $ and $ r_0 = [r_{0x}, r_{0y}, r_{0z}]^T $ is the center of the ellipsoid (unknown).  And $Q$ is known and is of the form $ Q = R D R^T $ with $ D = \text{diag}\bigg(\  \dfrac{1}{a^2},\  \dfrac{1}{b^2}, \ \dfrac{1}{c^2} \bigg) $ , and $R $ is a rotation matrix, whose columns (mutually orthogonal unit vectors) give the direction of the three axes of the ellipsoid. From tangency to the $xy$ plane, let $r_1$ be the tangency point, then $ (r_1 - r_0)^T Q (r_1 - r_0) = 1 $ and the gradient of the ellipsoid (which is a vector normal to its surface) points in a direction of $- \mathbf{k} $ (where $\mathbf{k} $ is the unit vector pointing in the positive $z$ -direction).  Hence, $ Q (r_1 - r_0) = -\alpha \mathbf{k} $ for some $\alpha \gt 0 $ Similar equations can written for $r_2$ and $r_3$ , the tangency points with the $xz$ plane and the $ y z $ plane respectively. Continuing with $r_1$ , and from the above equation, $ r_1 - r_0 = - \alpha Q^{-1} \mathbf{k} $ Plugging this into the equation of the ellipsoid, yields $ \alpha = \dfrac{1}{\sqrt{ \mathbf{k}^T Q^{-1} \mathbf{k} } } $ Everything on the right hand side is known, so $\alpha$ is now known. Recall that $r_1$ is the tangency point with $xy$ plane, so its $z$ -coordinate is $0$ .  Premultiplying the above equation for $r_1 - r_0$ by $ \mathbf{k}^T$ therefore, will give $ r_{0z} = \sqrt{ \mathbf{k}^T Q^{-1} \mathbf{k} } $ Similarly, we will get $ r_{0y} = \sqrt{ \mathbf{j}^T Q^{-1} \mathbf{j} } $ and $ r_{0x} = \sqrt{ \mathbf{i}^T Q^{-1} \mathbf{i} } $ where $\mathbf{i}$ and $\mathbf{j} $ are the unit vectors in the positive $ x $ and $y$ directions, respectively. Having found $r_0$ completely (the center of the ellipsoid).  Then now we can use the equations above for $(r_1 - r_0)$ , $(r_2 - r_0)$ , $(r_3 - r_0) $ to solve for $r_1, r_2, r_3$ .","Suppose you have an ellipsoid with known semi-axes, say .  The ellipsoid is then rotated so that the three axes of the ellipsoid are aligned with the three columns of a known rotation matrix .  Keeping this orientation, you place the ellipsoid on the plane, in the first octant (where ).  Next, you drag the ellipsoid, while maintaining its spatial orientation, along the plane, in a direction parallel to the axis, till it touches the plane, then you drag it parallel to the axis, till it touches the plane.  Now the ellipsoid is tangent to the coordinate planes.  The question is find the tangency points of the ellipsoid with the three coordinate planes . As for context, this question can be considered a extension of ellipses in general orientation, to the case involving an ellipsoid instead of an ellipse. My Progress: The algebraic equation of an ellipsoid is where and is the center of the ellipsoid (unknown).  And is known and is of the form with , and is a rotation matrix, whose columns (mutually orthogonal unit vectors) give the direction of the three axes of the ellipsoid. From tangency to the plane, let be the tangency point, then and the gradient of the ellipsoid (which is a vector normal to its surface) points in a direction of (where is the unit vector pointing in the positive -direction).  Hence, for some Similar equations can written for and , the tangency points with the plane and the plane respectively. Continuing with , and from the above equation, Plugging this into the equation of the ellipsoid, yields Everything on the right hand side is known, so is now known. Recall that is the tangency point with plane, so its -coordinate is .  Premultiplying the above equation for by therefore, will give Similarly, we will get and where and are the unit vectors in the positive and directions, respectively. Having found completely (the center of the ellipsoid).  Then now we can use the equations above for , , to solve for .","a,b,c R xy  x \ge 0, y \ge 0, z \ge 0 xy x yz y xz 3 2D 3D  (r - r_0)^T Q (r - r_0) = 1   r = [x , y, z ]^T   r_0 = [r_{0x}, r_{0y}, r_{0z}]^T  Q  Q = R D R^T   D = \text{diag}\bigg(\  \dfrac{1}{a^2},\  \dfrac{1}{b^2}, \ \dfrac{1}{c^2} \bigg)  R  xy r_1  (r_1 - r_0)^T Q (r_1 - r_0) = 1  - \mathbf{k}  \mathbf{k}  z  Q (r_1 - r_0) = -\alpha \mathbf{k}  \alpha \gt 0  r_2 r_3 xz  y z  r_1  r_1 - r_0 = - \alpha Q^{-1} \mathbf{k}   \alpha = \dfrac{1}{\sqrt{ \mathbf{k}^T Q^{-1} \mathbf{k} } }  \alpha r_1 xy z 0 r_1 - r_0  \mathbf{k}^T  r_{0z} = \sqrt{ \mathbf{k}^T Q^{-1} \mathbf{k} }   r_{0y} = \sqrt{ \mathbf{j}^T Q^{-1} \mathbf{j} }   r_{0x} = \sqrt{ \mathbf{i}^T Q^{-1} \mathbf{i} }  \mathbf{i} \mathbf{j}   x  y r_0 (r_1 - r_0) (r_2 - r_0) (r_3 - r_0)  r_1, r_2, r_3","['linear-algebra', 'geometry', 'multivariable-calculus', 'quadrics', 'ellipsoids']"
28,Why isn't there an extra term in the jacobian to account for how much du and dv are perpendicular?,Why isn't there an extra term in the jacobian to account for how much du and dv are perpendicular?,,"I wanted to derive the formula for the multivariable change of basis in an integral on my own (for the 2 by 2 case). What I did was: $$x=f(u,v)$$ $$y=g(u,v)$$ so $$dx = \frac{\partial f}{\partial u}du + \frac{\partial f}{\partial v}dv$$ $$dy = \frac{\partial g}{\partial u}du + \frac{\partial g}{\partial v}dv$$ Then, $$dx \wedge dy = (\frac{\partial f}{\partial u}du + \frac{\partial f}{\partial v}dv) \wedge (\frac{\partial g}{\partial u}du + \frac{\partial g}{\partial v}dv) = (\frac{\partial f}{\partial u}\frac{\partial g}{\partial v} - \frac{\partial g}{\partial u}\frac{\partial f}{\partial v}) du \wedge dv$$ I recognize that term as the Jacobian. Then: $$dx\wedge dy = J(u,v) du \wedge dv$$ but I don't want to be working with bivectors, I want to work with scalars. I take the absolute value on both sides and since dx is perpendicular to dy: $$dx dy = J(u,v) \sin(\theta) du dv $$ where $\theta$ is the angle between the two vectors. This is not the formula I learned in my undergraduate studies. How did the original formula work even if dx and dy were scalars?","I wanted to derive the formula for the multivariable change of basis in an integral on my own (for the 2 by 2 case). What I did was: so Then, I recognize that term as the Jacobian. Then: but I don't want to be working with bivectors, I want to work with scalars. I take the absolute value on both sides and since dx is perpendicular to dy: where is the angle between the two vectors. This is not the formula I learned in my undergraduate studies. How did the original formula work even if dx and dy were scalars?","x=f(u,v) y=g(u,v) dx = \frac{\partial f}{\partial u}du + \frac{\partial f}{\partial v}dv dy = \frac{\partial g}{\partial u}du + \frac{\partial g}{\partial v}dv dx \wedge dy = (\frac{\partial f}{\partial u}du + \frac{\partial f}{\partial v}dv) \wedge (\frac{\partial g}{\partial u}du + \frac{\partial g}{\partial v}dv) = (\frac{\partial f}{\partial u}\frac{\partial g}{\partial v} - \frac{\partial g}{\partial u}\frac{\partial f}{\partial v}) du \wedge dv dx\wedge dy = J(u,v) du \wedge dv dx dy = J(u,v) \sin(\theta) du dv  \theta","['multivariable-calculus', 'vector-analysis', 'jacobian']"
29,Time derivative of a composite function,Time derivative of a composite function,,"Let $x(t)\in R^n$ be a time-dependent variable and consider two vector-valued functions $g : R^n \mapsto R^m$ and $f : R^m \mapsto R^p$ . What is the time derivative $\frac{d}{dt}f(g(x(t)))$ ? As I understand, applying the chain rule we have that \begin{equation} \frac{d}{dt}f(g(x(t))) = \frac{\partial }{\partial x} f(g(x)) \frac{dx}{dt}. \end{equation} The partial derivative (a Jacobian matrix, actually) is given by \begin{equation} \frac{\partial }{\partial x} f(g(x)) = D_f(g(x)) D_g(x), \end{equation} where $D_f(a)$ is the Jacobian matrix of a function $f$ evaluated at $a$ . Is this correct? Or should it be $(D_f(g(x)))^T$ in the equation above?","Let be a time-dependent variable and consider two vector-valued functions and . What is the time derivative ? As I understand, applying the chain rule we have that The partial derivative (a Jacobian matrix, actually) is given by where is the Jacobian matrix of a function evaluated at . Is this correct? Or should it be in the equation above?","x(t)\in R^n g : R^n \mapsto R^m f : R^m \mapsto R^p \frac{d}{dt}f(g(x(t))) \begin{equation}
\frac{d}{dt}f(g(x(t))) = \frac{\partial }{\partial x} f(g(x)) \frac{dx}{dt}.
\end{equation} \begin{equation}
\frac{\partial }{\partial x} f(g(x)) = D_f(g(x)) D_g(x),
\end{equation} D_f(a) f a (D_f(g(x)))^T","['multivariable-calculus', 'chain-rule']"
30,Find the volume between the regions $x^2 + y^2+ z^2 = 4$ and $x = 4-y^2$,Find the volume between the regions  and,x^2 + y^2+ z^2 = 4 x = 4-y^2,"I want the volume of the sphere $x^2 + y^2 + z^2 = 4$ from $x = 0$ to $x = 4-y^2$ .  The integral that gives this volume is $$\int\limits_{-2}^2 \int\limits_0^{4-y^2} \int\limits_{-\sqrt{4-x^2-y^2}}^{\sqrt{4-x^2-y^2}}\ 1\ dz\ dx\ dy$$ I don't find $T$ such that $T\bar{u} = \bar{x}$ for every $\bar{x}\in D$ where $D$ is our desired volume. I mean, I'm trying to find $D^*$ such that for every $\bar{u} \in D^*$ , $T$ is a change of variables for our problem. So, I'm trying the find the volume between the regions $x^2 + y^2 + z^2 = 4$ and $x = 4-y^2$ but I can neither find that scalar. Any ideas please.","I want the volume of the sphere from to .  The integral that gives this volume is I don't find such that for every where is our desired volume. I mean, I'm trying to find such that for every , is a change of variables for our problem. So, I'm trying the find the volume between the regions and but I can neither find that scalar. Any ideas please.",x^2 + y^2 + z^2 = 4 x = 0 x = 4-y^2 \int\limits_{-2}^2 \int\limits_0^{4-y^2} \int\limits_{-\sqrt{4-x^2-y^2}}^{\sqrt{4-x^2-y^2}}\ 1\ dz\ dx\ dy T T\bar{u} = \bar{x} \bar{x}\in D D D^* \bar{u} \in D^* T x^2 + y^2 + z^2 = 4 x = 4-y^2,"['calculus', 'multivariable-calculus']"
31,Showing that a constant composition implies a constant input,Showing that a constant composition implies a constant input,,"Regarding the problem: Consider the differentiable (continuously) functions $f$ and $g$ where $f: \mathbb{R}^k \rightarrow \mathbb{R}$ and $g: (a,d) \rightarrow \mathbb{R}^k$ solving the system of equations $$\frac{dg}{dx}=- \nabla f(g(x)) \space \text{ for } x \in (a,d)$$ Within the frameworks of the above setup, prove that for $[b,c] \subset (a,d)$ we have the following implication: $$f(g(b)) = f(g(c)) \implies \nabla f(g(b))=0 \space \text{ and } g(x) = g(b) \space \space \forall x \in [b,c]$$ I know that we can apply the chain rule here which gives us the following result $$ (f \circ g)'(x) =  \nabla f(g(x)) \space \cdot \space g'(x)$$ And from the system of equations presented in the question, this should allow us to make a direct substitution to find that: $$ (f \circ g)'(x) = - \Big{(} \frac{dg}{dx} \Big{)}^2 $$ This tells us that $f \circ g$ is decreasing. It is also clear the converse of the desired result holds (trivially). It feels intuitively clear, that if $f(g(b)) = f(g(c))$ for any open subset $[b,c]$ over the domain, then $g$ must be constant on this interval (with zero gradient), but I'm not clear on how to formalise this. I’m unsure if / how the progress I have made helps us with this particular problem, and would be grateful for any guidance.","Regarding the problem: Consider the differentiable (continuously) functions and where and solving the system of equations Within the frameworks of the above setup, prove that for we have the following implication: I know that we can apply the chain rule here which gives us the following result And from the system of equations presented in the question, this should allow us to make a direct substitution to find that: This tells us that is decreasing. It is also clear the converse of the desired result holds (trivially). It feels intuitively clear, that if for any open subset over the domain, then must be constant on this interval (with zero gradient), but I'm not clear on how to formalise this. I’m unsure if / how the progress I have made helps us with this particular problem, and would be grateful for any guidance.","f g f: \mathbb{R}^k \rightarrow \mathbb{R} g: (a,d) \rightarrow \mathbb{R}^k \frac{dg}{dx}=- \nabla f(g(x)) \space \text{ for } x \in (a,d) [b,c] \subset (a,d) f(g(b)) = f(g(c)) \implies \nabla f(g(b))=0 \space \text{ and } g(x) = g(b) \space \space \forall x \in [b,c]  (f \circ g)'(x) =  \nabla f(g(x)) \space \cdot \space g'(x)  (f \circ g)'(x) = - \Big{(} \frac{dg}{dx} \Big{)}^2  f \circ g f(g(b)) = f(g(c)) [b,c] g","['real-analysis', 'calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'derivatives']"
32,Is there a way to find the values of 3 variables with just one equation?,Is there a way to find the values of 3 variables with just one equation?,,"I was doing a personal project until I came upon this equation that i need to solve to continue, the thing is I don’t thing I have been thought this in math ever so i wonder if this even possible to solve at all if why or why not 2850=2x+4y+6z If i can get more specific, it needs to be real numbers, and cannot be negatives, and must be only integers, no fractions. Can values aside from all real numbers be obtain with those limitations, or is it impossible? And Thank you for thank you time to answer.","I was doing a personal project until I came upon this equation that i need to solve to continue, the thing is I don’t thing I have been thought this in math ever so i wonder if this even possible to solve at all if why or why not 2850=2x+4y+6z If i can get more specific, it needs to be real numbers, and cannot be negatives, and must be only integers, no fractions. Can values aside from all real numbers be obtain with those limitations, or is it impossible? And Thank you for thank you time to answer.",,"['multivariable-calculus', 'diophantine-equations', 'numerical-linear-algebra', 'integer-partitions', 'linear-diophantine-equations']"
33,The existence of a partial derivative at a point,The existence of a partial derivative at a point,,"Let $f(x,y) = \frac{xy}{x^2+y^2}$ if $(x,y)\neq(0,0)$ and $f(x,y) = 0$ if $(x,y)=(0,0)$ . By definition, we have $f_x(0,0) = \lim_{h\rightarrow0} \frac{f(h,0)-f(0,0)}{h}=0$ . But we also have that $f_x(x,y) = \frac{y(y^2-x^2)}{(x^2+y^2)^2}$ , and from this equation it seems that $f_x(x,y)$ is not defined at $(0,0)$ . Why is this the case? I can see that $f_x(x,y) = \frac{y(y^2-x^2)}{(x^2+y^2)^2}$ doesn't take into account that "" $f(x,y) = 0$ if $(x,y)=(0,0)$ "", whereas by the definition, $f_x(0,0)$ is defined $\iff$ $f(0,0)$ is defined. But I still am not sure to fully understand why we cannot use the equation of $f_x$ to determine whether $f_x$ is defined at $(0,0)$ or not.","Let if and if . By definition, we have . But we also have that , and from this equation it seems that is not defined at . Why is this the case? I can see that doesn't take into account that "" if "", whereas by the definition, is defined is defined. But I still am not sure to fully understand why we cannot use the equation of to determine whether is defined at or not.","f(x,y) = \frac{xy}{x^2+y^2} (x,y)\neq(0,0) f(x,y) = 0 (x,y)=(0,0) f_x(0,0) = \lim_{h\rightarrow0} \frac{f(h,0)-f(0,0)}{h}=0 f_x(x,y) = \frac{y(y^2-x^2)}{(x^2+y^2)^2} f_x(x,y) (0,0) f_x(x,y) = \frac{y(y^2-x^2)}{(x^2+y^2)^2} f(x,y) = 0 (x,y)=(0,0) f_x(0,0) \iff f(0,0) f_x f_x (0,0)","['multivariable-calculus', 'partial-derivative']"
34,Problem with partial derivative with multi-variable,Problem with partial derivative with multi-variable,,"Here, $z=z(u,v)$ where $u=u(x,y)$ and $v=v(x,y)$ $$p=\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial x} \implies \frac{\partial}{\partial x}=\frac{\partial u}{\partial x} \frac{\partial}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial}{\partial v}$$ $$q=\frac{\partial z}{\partial y}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial y}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial y}\implies \frac{\partial}{\partial y}=\frac{\partial u}{\partial y} \frac{\partial}{\partial u}+\frac{\partial v}{\partial y} \frac{\partial}{\partial v}$$ $$ \begin{align} r&=\frac{\partial^2 z}{\partial x^2}\\\\ &=\frac{\partial}{\partial x}\left(\frac{\partial z}{\partial x}\right)\\\\ &=\left(\frac{\partial u}{\partial x} \frac{\partial}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial}{\partial v}\right)\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)\\\\ &\stackrel{{}^3}{=}\frac{\partial u}{\partial x} \frac{\partial}{\partial u}\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)+\frac{\partial v}{\partial x} \frac{\partial}{\partial v}\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)\\\\ &\stackrel{{}^4}{=}\frac{\partial^2 z}{\partial u^2}\left(\frac{\partial u}{\partial x}\right)^2+2 \frac{\partial^2 z}{\partial u \partial v} \frac{\partial u}{\partial x} \frac{\partial v}{\partial x}+\frac{\partial^2 z}{\partial v^2}\left(\frac{\partial v}{\partial x}\right)^2+\frac{\partial z}{\partial u} \frac{\partial^2 u}{\partial x^2}+\frac{\partial z}{\partial v} \frac{\partial^2 v}{\partial x^2} \end{align} $$ I couldn't understand how line $(4)$ came from line $(3)$ , Like what should be $$\frac{\partial u}{\partial x} \frac{\partial}{\partial u}\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)=?$$ I guess $$\frac{\partial^2 z}{\partial u^2}\left(\frac{\partial u}{\partial x}\right)^2+ \frac{\partial^2 z}{\partial u \partial v} \frac{\partial u}{\partial x} \frac{\partial v}{\partial x}+\frac{\partial u}{\partial x}\frac{\partial z}{\partial u} \frac{\partial^2 u}{\partial x^2}+0$$ But that seems not correct. Any help will be appreciated.","Here, where and I couldn't understand how line came from line , Like what should be I guess But that seems not correct. Any help will be appreciated.","z=z(u,v) u=u(x,y) v=v(x,y) p=\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial x} \implies \frac{\partial}{\partial x}=\frac{\partial u}{\partial x} \frac{\partial}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial}{\partial v} q=\frac{\partial z}{\partial y}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial y}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial y}\implies \frac{\partial}{\partial y}=\frac{\partial u}{\partial y} \frac{\partial}{\partial u}+\frac{\partial v}{\partial y} \frac{\partial}{\partial v} 
\begin{align}
r&=\frac{\partial^2 z}{\partial x^2}\\\\
&=\frac{\partial}{\partial x}\left(\frac{\partial z}{\partial x}\right)\\\\
&=\left(\frac{\partial u}{\partial x} \frac{\partial}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial}{\partial v}\right)\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)\\\\
&\stackrel{{}^3}{=}\frac{\partial u}{\partial x} \frac{\partial}{\partial u}\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)+\frac{\partial v}{\partial x} \frac{\partial}{\partial v}\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)\\\\
&\stackrel{{}^4}{=}\frac{\partial^2 z}{\partial u^2}\left(\frac{\partial u}{\partial x}\right)^2+2 \frac{\partial^2 z}{\partial u \partial v} \frac{\partial u}{\partial x} \frac{\partial v}{\partial x}+\frac{\partial^2 z}{\partial v^2}\left(\frac{\partial v}{\partial x}\right)^2+\frac{\partial z}{\partial u} \frac{\partial^2 u}{\partial x^2}+\frac{\partial z}{\partial v} \frac{\partial^2 v}{\partial x^2}
\end{align}
 (4) (3) \frac{\partial u}{\partial x} \frac{\partial}{\partial u}\left(\frac{\partial u}{\partial x} \frac{\partial z}{\partial u}+\frac{\partial v}{\partial x} \frac{\partial z}{\partial v}\right)=? \frac{\partial^2 z}{\partial u^2}\left(\frac{\partial u}{\partial x}\right)^2+ \frac{\partial^2 z}{\partial u \partial v} \frac{\partial u}{\partial x} \frac{\partial v}{\partial x}+\frac{\partial u}{\partial x}\frac{\partial z}{\partial u} \frac{\partial^2 u}{\partial x^2}+0","['multivariable-calculus', 'chain-rule']"
35,Doubts on the setting of PMA Rudin's proof of the 9.24 (Inverse Function Theorem),Doubts on the setting of PMA Rudin's proof of the 9.24 (Inverse Function Theorem),,"PMA Rudin Theorem 9.24 (Inverse Function Theorem) Suppose $\textbf{f}$ is a $\mathscr{C '}$ -mapping of an open set $E \subset R^n$ into $ R^n. \textbf{f '(a)}$ is invertible for some $\textbf{a} \in E$ , and $\textbf{b = f (a)}$ . Then: (a) There exist open sets $U$ and $V$ in $R^n$ such that $\textbf{a} \in U, \textbf{b} \in V$ . $\textbf{f}$ is one-to-one on U, and $\textbf{f}(U)=V$ ; (b) If $\textbf{g}$ is the inverse of $\textbf{f}$ [which exists, by (a)], defined in $V$ by $\textbf{g(f(x))} = \textbf{x}$ , (for $\textbf{x} \in U)$ . Then $\textbf{g} \in \mathscr{C '}(V)$ For part b of his proof on the book, the first line states: ""Pick $\textbf{y} \in V, \; \textbf{y} + \textbf{k} \in V$ . Then there exist $\textbf{x} \in U, \; \textbf{x + h} \in U,$ so that $\textbf{y} = \textbf{f(x)}, \; \textbf{y + k = f(x + h)}.$ "" Question 1: Is it okay to assume that open sets $U$ and $V$ as vector spaces. Is this assumption arbitrary or common sense ?","PMA Rudin Theorem 9.24 (Inverse Function Theorem) Suppose is a -mapping of an open set into is invertible for some , and . Then: (a) There exist open sets and in such that . is one-to-one on U, and ; (b) If is the inverse of [which exists, by (a)], defined in by , (for . Then For part b of his proof on the book, the first line states: ""Pick . Then there exist so that "" Question 1: Is it okay to assume that open sets and as vector spaces. Is this assumption arbitrary or common sense ?","\textbf{f} \mathscr{C '} E \subset R^n  R^n. \textbf{f '(a)} \textbf{a} \in E \textbf{b = f (a)} U V R^n \textbf{a} \in U, \textbf{b} \in V \textbf{f} \textbf{f}(U)=V \textbf{g} \textbf{f} V \textbf{g(f(x))} = \textbf{x} \textbf{x} \in U) \textbf{g} \in \mathscr{C '}(V) \textbf{y} \in V, \; \textbf{y} + \textbf{k} \in V \textbf{x} \in U, \; \textbf{x + h} \in U, \textbf{y} = \textbf{f(x)}, \; \textbf{y + k = f(x + h)}. U V","['real-analysis', 'general-topology', 'multivariable-calculus', 'linear-transformations']"
36,Langrange Multiplier to Function $x^2-y^2-z^2$,Langrange Multiplier to Function,x^2-y^2-z^2,"Im Trying To Find The min and max values of function $$f(x, y, z) = x^2-y^2-z^2$$ At constraint $g(x, y, z) = x^2+y^2+z^2-1$ . My Solution : I'm using gradient to find the partial derivatives of each variables so : $\nabla f'(x, y, z) = <2x, -2y, -2z>$ $\nabla g'(x, y, z) = <2x, 2y, 2z>$ Set up : $$\nabla (f) = \lambda \nabla(g)$$ $$ 2x = \lambda(2x)$$ $$-2y = \lambda(2y)$$ $$-2z = \lambda(2z)$$ At $x^2+y^2+z^2-1$ Solving for $\lambda$ : $$\lambda = \frac{2x}{2x} = 1$$ $$\lambda = -\frac{2y}{2y} = -1$$ $$\lambda = -\frac{2z}{2z} = -1$$ Im basically stuck at here because the lambdas are not giving any variable and only constant to sub into the constraint (which is confusing). Anyone can tell me what i did wrong here? Ps: i'm writing on a touch device.",Im Trying To Find The min and max values of function At constraint . My Solution : I'm using gradient to find the partial derivatives of each variables so : Set up : At Solving for : Im basically stuck at here because the lambdas are not giving any variable and only constant to sub into the constraint (which is confusing). Anyone can tell me what i did wrong here? Ps: i'm writing on a touch device.,"f(x, y, z) = x^2-y^2-z^2 g(x, y, z) = x^2+y^2+z^2-1 \nabla f'(x, y, z) = <2x, -2y, -2z> \nabla g'(x, y, z) = <2x, 2y, 2z> \nabla (f) = \lambda \nabla(g)  2x = \lambda(2x) -2y = \lambda(2y) -2z = \lambda(2z) x^2+y^2+z^2-1 \lambda \lambda = \frac{2x}{2x} = 1 \lambda = -\frac{2y}{2y} = -1 \lambda = -\frac{2z}{2z} = -1","['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
37,How to apply Fubini's theorem here?,How to apply Fubini's theorem here?,,"Given $f:\mathbb{R}^2\times \mathbb{R}^2 \to \mathbb{R}$ , my classmate and I are discussing how to apply Fubini's theorem in the next integral with $x=(x_1,x_2), y=(y_1,y_2)$ $$\int_{x\in C}\int_{y\in C-x}{f(x,y)}dydx,$$ where , $$C=\{x=(x_1,x_2)\in \mathbb{R}^2:2<x_1^2+x_2^2<3\}$$ and $$ C-x=\{y-x:y\in C\}.$$ Our first attempt was $$\int_{x\in C}\int_{y\in C-x}{f(x,y)}dydx=\int_{y\in C}\int_{x\in C+y}{f(x,y)}dxdy,$$ we discarded this option by means of the simpler 1D example: $$\int_{x=-1}^{x=1}\int_{y= -1-x}^{y=1-x}f(x,y)dydx\neq\int_{y=-1}^{y=1}\int_{x= -1-y}^{x=1-y}{f(x,y)}dxdy.$$ Does anyone have any hint that can help us?","Given , my classmate and I are discussing how to apply Fubini's theorem in the next integral with where , and Our first attempt was we discarded this option by means of the simpler 1D example: Does anyone have any hint that can help us?","f:\mathbb{R}^2\times \mathbb{R}^2 \to \mathbb{R} x=(x_1,x_2), y=(y_1,y_2) \int_{x\in C}\int_{y\in C-x}{f(x,y)}dydx, C=\{x=(x_1,x_2)\in \mathbb{R}^2:2<x_1^2+x_2^2<3\}  C-x=\{y-x:y\in C\}. \int_{x\in C}\int_{y\in C-x}{f(x,y)}dydx=\int_{y\in C}\int_{x\in C+y}{f(x,y)}dxdy, \int_{x=-1}^{x=1}\int_{y= -1-x}^{y=1-x}f(x,y)dydx\neq\int_{y=-1}^{y=1}\int_{x= -1-y}^{x=1-y}{f(x,y)}dxdy.","['real-analysis', 'integration', 'multivariable-calculus', 'lebesgue-integral']"
38,Factor of 4 appears in Jacobian coordinate transformation,Factor of 4 appears in Jacobian coordinate transformation,,"I am was reading the wikipedia page on metric tensors , when I saw something that was hard to grasp in the coordinate transformation section. This topic is a little bit uncomfortable to me, so maybe I have missed something, but there appears to be a factor of 4 that appears when working everything out by hand? With r being a vector valued function $\vec{r}(u,\,v) = \bigl( x(u,\,v),\, y(u,\,v),\, z(u,\,v) \bigr)$ , and with the following identity $$ \begin{bmatrix} \frac{\partial r}{\partial u}\frac{\partial r}{\partial u} & \frac{\partial r}{\partial u}\frac{\partial r}{\partial v} \\ \frac{\partial r}{\partial u}\frac{\partial r}{\partial v} & \frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \end{bmatrix} = \begin{bmatrix} E  & F  \\ F  & G \end{bmatrix} $$ the coordinate transformation is given by, $$ \begin{aligned} \begin{bmatrix} E^\prime & F^\prime \\ F^\prime & G^\prime \end{bmatrix} = \begin{bmatrix} \frac{\partial u}{\partial u^\prime} & \frac{\partial u}{\partial v^\prime} \\ \frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \\ \end{bmatrix}^\top \begin{bmatrix} E & F \\ F & G \end{bmatrix} \begin{bmatrix} \frac{\partial u}{\partial u^\prime} & \frac{\partial u}{\partial v^\prime} \\ \frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \\ \end{bmatrix} \end{aligned} $$ With the following substitution in the coordinate transformation matrix, $$ \begin{bmatrix} \frac{\partial u}{\partial u^\prime} &  \frac{\partial u}{\partial v^\prime} \\ \frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \end{bmatrix} = \begin{bmatrix} A &  B \\ C & D \end{bmatrix} $$ The transformation then becomes, $$ \begin{aligned} \begin{bmatrix} A & B \\ C & D \end{bmatrix}^\top \begin{bmatrix} E & F \\ F & G \end{bmatrix} \begin{bmatrix} A & B \\ C & D \end{bmatrix} =  \begin{bmatrix} \underbrace{A^2 E + 2 ACF + C^2G}_{E^\prime} & \underbrace{ABE + BCF + AFD + CDG}_{F^\prime} \\ \underbrace{ABE + BCF + AFD + CDG}_{F^\prime} & \underbrace{B^2E + 2BFD + D^2G}_{G^\prime} \end{bmatrix} \end{aligned} $$ Plugging the values into the variables give the following expressions, $$ \begin{aligned} E^\prime &= \frac{\partial u}{\partial u^\prime}\frac{\partial u}{\partial u^\prime}  \frac{\partial r}{\partial u}\frac{\partial r}{\partial u}  + 2 \frac{\partial u}{\partial u^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} + \frac{\partial v}{\partial u^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \\ F^\prime &= \frac{\partial u}{\partial u^\prime}\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} + \frac{\partial u}{\partial v^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} + \frac{\partial u}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}\frac{\partial v}{\partial v^\prime} + \frac{\partial v}{\partial u^\prime}\frac{\partial v}{\partial v^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \\ G^\prime &= \frac{\partial u}{\partial v^\prime}\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} + 2\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}\frac{\partial v}{\partial v^\prime} + \frac{\partial v}{\partial v^\prime}\frac{\partial v}{\partial v^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \end{aligned} $$ after simplifying by cancelling out similar factors in the numerator and denominator and summing the result, everything comes out to, $$ \require{cancel} \begin{aligned} E^\prime &= \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial u}}{\partial u^\prime}  \frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}}  + 2 \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}} + \frac{\cancel{\partial v}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \\ F^\prime &= \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} + \frac{\cancel{\partial u}}{\partial v^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}} + \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}\frac{\cancel{\partial v}}{\partial v^\prime} + \frac{\cancel{\partial v}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \\ G^\prime &= \frac{\cancel{\partial u}}{\partial v^\prime}\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} + 2\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}\frac{\cancel{\partial v}}{\partial v^\prime} + \frac{\cancel{\partial v}}{\partial v^\prime}\frac{\cancel{\partial v}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \end{aligned} $$ which then reduces to the following by adding the remaining terms, $$ \begin{bmatrix} E^\prime & F^\prime \\ F^\prime & G^\prime \end{bmatrix}  = 4\begin{bmatrix} \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial u^\prime} & \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime} \\ \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime} & \frac{\partial r}{\partial v^\prime}\frac{\partial r}{\partial v^\prime} \end{bmatrix} $$ The Wikipedia article states (above equation 2') that the values of $E^\prime, F^\prime, G^\prime$ are in fact $ E^\prime = \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial u^\prime}, \;\;  F^\prime = \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime}, \;\; G^\prime = \frac{\partial r}{\partial v^\prime}\frac{\partial r}{\partial v^\prime} $ which then means that I get the following expression after simplifying my manual transformation above, and comparing it with the definition of $E^\prime, F^\prime, G^\prime$ from Wikipedia. $$ \begin{aligned} \begin{bmatrix} E^\prime & F^\prime \\ F^\prime & G^\prime \end{bmatrix} \neq 4\begin{bmatrix} E^\prime & F^\prime \\ F^\prime & G^\prime \end{bmatrix} \end{aligned} $$ Questions How can I reconcile that this factor of 4 comes out? Is it just because the factor of 4 becomes irrelevant for an infinitesimal difference? Or have I made a terrible error somewhere? Generally, how can I understand coordinate transformations when dealing with a Jacobian matrix, are they the same thing as a change of basis in linear algebra when we see the form $P^{-1}AP$ ? What is the significance here that this form is $P^\top AP$ with a transpose instead of an inverse? The Jacobian is highly unlikely to be orthonormal (right?), so the transpose is definitely not the inverse.","I am was reading the wikipedia page on metric tensors , when I saw something that was hard to grasp in the coordinate transformation section. This topic is a little bit uncomfortable to me, so maybe I have missed something, but there appears to be a factor of 4 that appears when working everything out by hand? With r being a vector valued function , and with the following identity the coordinate transformation is given by, With the following substitution in the coordinate transformation matrix, The transformation then becomes, Plugging the values into the variables give the following expressions, after simplifying by cancelling out similar factors in the numerator and denominator and summing the result, everything comes out to, which then reduces to the following by adding the remaining terms, The Wikipedia article states (above equation 2') that the values of are in fact which then means that I get the following expression after simplifying my manual transformation above, and comparing it with the definition of from Wikipedia. Questions How can I reconcile that this factor of 4 comes out? Is it just because the factor of 4 becomes irrelevant for an infinitesimal difference? Or have I made a terrible error somewhere? Generally, how can I understand coordinate transformations when dealing with a Jacobian matrix, are they the same thing as a change of basis in linear algebra when we see the form ? What is the significance here that this form is with a transpose instead of an inverse? The Jacobian is highly unlikely to be orthonormal (right?), so the transpose is definitely not the inverse.","\vec{r}(u,\,v) = \bigl( x(u,\,v),\, y(u,\,v),\, z(u,\,v) \bigr) 
\begin{bmatrix}
\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} &
\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} \\
\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} &
\frac{\partial r}{\partial v}\frac{\partial r}{\partial v}
\end{bmatrix}
=
\begin{bmatrix}
E  &
F  \\
F  &
G
\end{bmatrix}
 
\begin{aligned}
\begin{bmatrix}
E^\prime & F^\prime \\ F^\prime & G^\prime
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial u}{\partial u^\prime} & \frac{\partial u}{\partial v^\prime} \\
\frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \\
\end{bmatrix}^\top
\begin{bmatrix}
E & F \\ F & G
\end{bmatrix}
\begin{bmatrix}
\frac{\partial u}{\partial u^\prime} & \frac{\partial u}{\partial v^\prime} \\
\frac{\partial v}{\partial u^\prime} & \frac{\partial v}{\partial v^\prime} \\
\end{bmatrix}
\end{aligned}
 
\begin{bmatrix}
\frac{\partial u}{\partial u^\prime} & 
\frac{\partial u}{\partial v^\prime} \\
\frac{\partial v}{\partial u^\prime} &
\frac{\partial v}{\partial v^\prime}
\end{bmatrix}
=
\begin{bmatrix}
A & 
B \\
C &
D
\end{bmatrix}
 
\begin{aligned}
\begin{bmatrix}
A & B \\ C & D
\end{bmatrix}^\top
\begin{bmatrix}
E & F \\ F & G
\end{bmatrix}
\begin{bmatrix}
A & B \\ C & D
\end{bmatrix}
= 
\begin{bmatrix}
\underbrace{A^2 E + 2 ACF + C^2G}_{E^\prime} & \underbrace{ABE + BCF + AFD + CDG}_{F^\prime} \\
\underbrace{ABE + BCF + AFD + CDG}_{F^\prime} & \underbrace{B^2E + 2BFD + D^2G}_{G^\prime}
\end{bmatrix}
\end{aligned}
 
\begin{aligned}
E^\prime &= \frac{\partial u}{\partial u^\prime}\frac{\partial u}{\partial u^\prime}
 \frac{\partial r}{\partial u}\frac{\partial r}{\partial u} 
+ 2 \frac{\partial u}{\partial u^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}
+ \frac{\partial v}{\partial u^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \\
F^\prime &= \frac{\partial u}{\partial u^\prime}\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} + \frac{\partial u}{\partial v^\prime}\frac{\partial v}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v} + \frac{\partial u}{\partial u^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}\frac{\partial v}{\partial v^\prime} + \frac{\partial v}{\partial u^\prime}\frac{\partial v}{\partial v^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v} \\
G^\prime &= \frac{\partial u}{\partial v^\prime}\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial u} + 2\frac{\partial u}{\partial v^\prime}\frac{\partial r}{\partial u}\frac{\partial r}{\partial v}\frac{\partial v}{\partial v^\prime} + \frac{\partial v}{\partial v^\prime}\frac{\partial v}{\partial v^\prime}\frac{\partial r}{\partial v}\frac{\partial r}{\partial v}
\end{aligned}
 
\require{cancel}
\begin{aligned}
E^\prime &= \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial u}}{\partial u^\prime}
 \frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} 
+ 2 \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}
+ \frac{\cancel{\partial v}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \\
F^\prime &= \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} + \frac{\cancel{\partial u}}{\partial v^\prime}\frac{\cancel{\partial v}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}} + \frac{\cancel{\partial u}}{\partial u^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}\frac{\cancel{\partial v}}{\partial v^\prime} + \frac{\cancel{\partial v}}{\partial u^\prime}\frac{\cancel{\partial v}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}} \\
G^\prime &= \frac{\cancel{\partial u}}{\partial v^\prime}\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial u}} + 2\frac{\cancel{\partial u}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial u}}\frac{\partial r}{\cancel{\partial v}}\frac{\cancel{\partial v}}{\partial v^\prime} + \frac{\cancel{\partial v}}{\partial v^\prime}\frac{\cancel{\partial v}}{\partial v^\prime}\frac{\partial r}{\cancel{\partial v}}\frac{\partial r}{\cancel{\partial v}}
\end{aligned}
 
\begin{bmatrix}
E^\prime &
F^\prime \\
F^\prime &
G^\prime
\end{bmatrix} 
=
4\begin{bmatrix}
\frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial u^\prime} &
\frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime} \\
\frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime} &
\frac{\partial r}{\partial v^\prime}\frac{\partial r}{\partial v^\prime}
\end{bmatrix}
 E^\prime, F^\prime, G^\prime 
E^\prime = \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial u^\prime}, \;\; 
F^\prime = \frac{\partial r}{\partial u^\prime}\frac{\partial r}{\partial v^\prime}, \;\;
G^\prime = \frac{\partial r}{\partial v^\prime}\frac{\partial r}{\partial v^\prime}
 E^\prime, F^\prime, G^\prime 
\begin{aligned}
\begin{bmatrix}
E^\prime & F^\prime \\ F^\prime & G^\prime
\end{bmatrix}
\neq
4\begin{bmatrix}
E^\prime & F^\prime \\ F^\prime & G^\prime
\end{bmatrix}
\end{aligned}
 P^{-1}AP P^\top AP","['calculus', 'linear-algebra', 'multivariable-calculus', 'metric-spaces', 'jacobian']"
39,How to prove $|f(x)| \leq C \epsilon |x|^N$?,How to prove ?,|f(x)| \leq C \epsilon |x|^N,"Suppose that $f\in C^{\infty}(\mathbb R^d)$ and $D^{\alpha}f(0)=0$ for all $0\leq |\alpha|\leq N.$ We may assume that $$|D^{\alpha}f(x)| \leq \epsilon \quad \text{for all} \quad |\alpha|=N$$ in small neighbourhood of origin. Question: How to show that $$|f(x)| \leq C \epsilon |x|^N$$ ? My thought: Maybe I've to invoke mean value theorem.  But I do not know how when $N\neq 1.$ Edit : Notation: $D^{\alpha}= \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}\cdots \partial x_N^{\alpha_N}},  \quad |\alpha|= \alpha_1+ \cdots + \alpha_n, \alpha_i \in \mathbb N_0$",Suppose that and for all We may assume that in small neighbourhood of origin. Question: How to show that ? My thought: Maybe I've to invoke mean value theorem.  But I do not know how when Edit : Notation:,"f\in C^{\infty}(\mathbb R^d) D^{\alpha}f(0)=0 0\leq |\alpha|\leq N. |D^{\alpha}f(x)| \leq \epsilon \quad \text{for all} \quad |\alpha|=N |f(x)| \leq C \epsilon |x|^N N\neq 1. D^{\alpha}= \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}\cdots \partial x_N^{\alpha_N}},  \quad |\alpha|= \alpha_1+ \cdots + \alpha_n, \alpha_i \in \mathbb N_0","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'inequality']"
40,Polar coordinates question (integration),Polar coordinates question (integration),,"Coming to the end of a question on Polar Coordinates and I have come across this integral which I cannot seem to evaluate. Upon expanding the entire expression, I'm not sure how to use the formula $cos^2(\theta) = \frac{1 +cos(2 \theta)}{2}$ in order to integrate the powers without everything becoming absurdly messy. Happy to accept hints. Book is Calculus of Several Variables by Serge Lang","Coming to the end of a question on Polar Coordinates and I have come across this integral which I cannot seem to evaluate. Upon expanding the entire expression, I'm not sure how to use the formula in order to integrate the powers without everything becoming absurdly messy. Happy to accept hints. Book is Calculus of Several Variables by Serge Lang",cos^2(\theta) = \frac{1 +cos(2 \theta)}{2},['multivariable-calculus']
41,Greatest and smallest value of a function in $\mathbb{R}^2$,Greatest and smallest value of a function in,\mathbb{R}^2,"Does the function $$ f(x,y) = 2x+2y $$ have a greatest or smallest value in $\mathbb{R}^2$ ? I thought that since $$ \lim_{x \to \infty} f(x,0) = \infty $$ $$\lim_{x \to -\infty} f(x,0) = -\infty $$ then the function can be infinite great and small, am I thinking right?","Does the function have a greatest or smallest value in ? I thought that since then the function can be infinite great and small, am I thinking right?"," f(x,y) = 2x+2y  \mathbb{R}^2  \lim_{x \to \infty} f(x,0) = \infty  \lim_{x \to -\infty} f(x,0) = -\infty ","['limits', 'multivariable-calculus', 'optimization', 'maxima-minima']"
42,The continuity of this two variable function. Is this continuous?,The continuity of this two variable function. Is this continuous?,,"I have come across with this problem in studying partial differential equation. Let $f$ be a continuous and periodic function on $\mathbb R$ with period $2\pi$ , and denote $D=\{(x,y)\mid x^2+y^2<1\}.$ And let $u:D\to \mathbb R$ be continuous and $C^2$ class on $D$ , and satisfy $$\lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0,$$ where I use the polar coodinates $x=r\cos a, y=r\sin a$ for $u(x,y)$ . Then, if I define $v: \overline D \to \mathbb R$ by $$v(x,y)=v(r\cos a, r\sin a):=\begin{cases}u(r\cos a, r\sin a) & \mathrm{if} \ (x,y)\in D\\ f(a) &\mathrm{if} \ (x,y)\in \overline D \setminus D=\partial D\end{cases}$$ , is $v$ continuous on $\overline D$ ? On $D$ , $v$ is continuous from the supposition of continuity of $u$ , so the problem is the continuity at $\partial D.$ I expect this holds because $$\lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0$$ means that if $a\fallingdotseq b$ and $r\fallingdotseq 1$ , then $u(r\cos a, r\sin a)\fallingdotseq f(b)\fallingdotseq f(a)$ . ( $f(b)\fallingdotseq f(a)$ follows from the continuity of $f$ .) So I tried to prove but it doesn't seem to work. Let $(c,d)\in \overline D$ . If $(c,d)\in D$ , $v$ is continuous at $(c,d)$ due to the continuity of $u$ . So I'll consider the case $(c,d)\in \partial D.$ Let $\epsilon>0.$ $(c,d)$ is on the unit circle so I can write $$c=\cos \xi, d=\sin \xi.$$ From $$\lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0,$$ there is $\delta_1$ s.t. $$|\delta|\leqq \delta_1\Rightarrow \underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)| <\epsilon.$$ And from the continuity of $f$ , there is $\delta_2>0$ s.t. $$|\eta-\xi|<\delta_2 \Rightarrow |f(\eta)-f(\xi)|<\epsilon.$$ Let $\delta_3:=\min\{\delta_1, \delta_2\}$ , and let $(x,y)\in \overline D$ satisfy $|(x,y)-(c,d)|<\delta_3.$ Denote $x=r\cos a, y=r\sin a.$ Then, if I could show $|v(x,y)-v(c,d)|<\epsilon$ , the proof will finish. If $(x,y)\in \partial D$ , then $|v(x,y)-v(c,d)|=|f(a)-f(\xi)|$ , so $|a-\xi|<\delta_3$ is desired, but this doesn't seem to work because $|a-\xi|<|(x,y)-(c,d)|$ doesn't hold. (Actually, the opposite inequality holds.) If $(x,y)\in D$ , I want to do \begin{align} |v(x,y)-v(c,d)|&=|u(x,y)-f(\xi)|\\ &\leqq\underset{1-\delta_3\leqq r<1}{\sup_{|a-b|\leqq \delta_3}}|u(r\cos a,r\sin a)-f(b)| \end{align} , so I have to check $|a-\xi|\leqq\delta_3$ and $1-\delta_3\leqq r<1$ I get $1-\delta_3\leqq r<1$ since $r<1$ follows from $(x,y)\in D$ and $1-\delta_3\leqq 1-|(x,y)-(c,d)|\leqq r.$ Thus, in both cases $(x,y)\in \partial D$ and $(x,y)\in D$ , $|a-\xi|<\delta_3$ is desired. I'd like you to share the idea for showing $|a-\xi|<\delta_3$ . Another proof for the continuity of $v$ is also welcomed.","I have come across with this problem in studying partial differential equation. Let be a continuous and periodic function on with period , and denote And let be continuous and class on , and satisfy where I use the polar coodinates for . Then, if I define by , is continuous on ? On , is continuous from the supposition of continuity of , so the problem is the continuity at I expect this holds because means that if and , then . ( follows from the continuity of .) So I tried to prove but it doesn't seem to work. Let . If , is continuous at due to the continuity of . So I'll consider the case Let is on the unit circle so I can write From there is s.t. And from the continuity of , there is s.t. Let , and let satisfy Denote Then, if I could show , the proof will finish. If , then , so is desired, but this doesn't seem to work because doesn't hold. (Actually, the opposite inequality holds.) If , I want to do , so I have to check and I get since follows from and Thus, in both cases and , is desired. I'd like you to share the idea for showing . Another proof for the continuity of is also welcomed.","f \mathbb R 2\pi D=\{(x,y)\mid x^2+y^2<1\}. u:D\to \mathbb R C^2 D \lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0, x=r\cos a, y=r\sin a u(x,y) v: \overline D \to \mathbb R v(x,y)=v(r\cos a, r\sin a):=\begin{cases}u(r\cos a, r\sin a) & \mathrm{if} \ (x,y)\in D\\ f(a) &\mathrm{if} \ (x,y)\in \overline D \setminus D=\partial D\end{cases} v \overline D D v u \partial D. \lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0 a\fallingdotseq b r\fallingdotseq 1 u(r\cos a, r\sin a)\fallingdotseq f(b)\fallingdotseq f(a) f(b)\fallingdotseq f(a) f (c,d)\in \overline D (c,d)\in D v (c,d) u (c,d)\in \partial D. \epsilon>0. (c,d) c=\cos \xi, d=\sin \xi. \lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0, \delta_1 |\delta|\leqq \delta_1\Rightarrow \underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)| <\epsilon. f \delta_2>0 |\eta-\xi|<\delta_2 \Rightarrow |f(\eta)-f(\xi)|<\epsilon. \delta_3:=\min\{\delta_1, \delta_2\} (x,y)\in \overline D |(x,y)-(c,d)|<\delta_3. x=r\cos a, y=r\sin a. |v(x,y)-v(c,d)|<\epsilon (x,y)\in \partial D |v(x,y)-v(c,d)|=|f(a)-f(\xi)| |a-\xi|<\delta_3 |a-\xi|<|(x,y)-(c,d)| (x,y)\in D \begin{align}
|v(x,y)-v(c,d)|&=|u(x,y)-f(\xi)|\\
&\leqq\underset{1-\delta_3\leqq r<1}{\sup_{|a-b|\leqq \delta_3}}|u(r\cos a,r\sin a)-f(b)|
\end{align} |a-\xi|\leqq\delta_3 1-\delta_3\leqq r<1 1-\delta_3\leqq r<1 r<1 (x,y)\in D 1-\delta_3\leqq 1-|(x,y)-(c,d)|\leqq r. (x,y)\in \partial D (x,y)\in D |a-\xi|<\delta_3 |a-\xi|<\delta_3 v","['real-analysis', 'multivariable-calculus', 'continuity']"
43,"Integration of vector field $(y,-y,1)$ over paraboloid $z=1-x^2-y^2$",Integration of vector field  over paraboloid,"(y,-y,1) z=1-x^2-y^2","The question asks to integrate the vector field $$ F(x,y,z)={1\over\sqrt{x^2+y^2}}(y,-y,1) $$ over the paraboloid $$z=1-x^2-y^2.$$ over the area defined by $0\leq z\leq1$ which is $x^2+y^2\leq1$ . Using $X=(x,y,1-x^2-y^2)$ as the surface I obtained $$ \begin{align} {\partial X\over\partial x}&=(1,0,-2x)\\ {\partial X\over\partial y}&=(0,1,-2y)\\ N={\partial X\over\partial x}\times {\partial X\over\partial y}&=(2x,2y,1)\\ \end{align} $$ and the integral of $F$ is then $$ \iint_R F(x,y,z)\cdot N dx\;dy = \int_0^{2\pi}\int_0^1 {2xy-2y^2+1\over r} r\;dr\;d\theta\\ =\frac13\int_0^{2\pi}(2\sin\theta\cos\theta-2\sin^2\theta+1) \;d\theta\\ =\frac13\int_0^{2\pi}(\sin2\theta+\cos2\theta)\;d\theta=0.\\ $$ However, the book gives the answer $4\pi/3$ . I've been over the calculations repeatedly without avail. 🥴 Can anyone point out my error? This is exercise 12 of XII §3 of ""Calculus of Several Variables"" by Serge Lang, third edition, on page 341.","The question asks to integrate the vector field over the paraboloid over the area defined by which is . Using as the surface I obtained and the integral of is then However, the book gives the answer . I've been over the calculations repeatedly without avail. 🥴 Can anyone point out my error? This is exercise 12 of XII §3 of ""Calculus of Several Variables"" by Serge Lang, third edition, on page 341.","
F(x,y,z)={1\over\sqrt{x^2+y^2}}(y,-y,1)
 z=1-x^2-y^2. 0\leq z\leq1 x^2+y^2\leq1 X=(x,y,1-x^2-y^2) 
\begin{align}
{\partial X\over\partial x}&=(1,0,-2x)\\
{\partial X\over\partial y}&=(0,1,-2y)\\
N={\partial X\over\partial x}\times
{\partial X\over\partial y}&=(2x,2y,1)\\
\end{align}
 F 
\iint_R F(x,y,z)\cdot N dx\;dy
=
\int_0^{2\pi}\int_0^1
{2xy-2y^2+1\over r} r\;dr\;d\theta\\
=\frac13\int_0^{2\pi}(2\sin\theta\cos\theta-2\sin^2\theta+1) \;d\theta\\
=\frac13\int_0^{2\pi}(\sin2\theta+\cos2\theta)\;d\theta=0.\\
 4\pi/3",['multivariable-calculus']
44,How do I find the points of tangency given a 2 variable function and a normal vector?,How do I find the points of tangency given a 2 variable function and a normal vector?,,"I'm given the two variable function $f(x,y) = 4x^2+7y^2+5xy+14$ and asked to find the (2) points on the surface where the vector $6\hat i + 69\hat j + 3\hat k$ is normal to the tangent plane.  So far I have written a generalized linearization using the points $f(a,b)$ : $$f(a,b) = 4a^2+7b^2+5ab+14$$ $$\Rightarrow z_0 = 4a^2+7b^2+5ab+14$$ $$ $$ $$\frac{\partial f}{\partial x} = 8x+5y$$ $$\Rightarrow x_0 = 8a+5b$$ $$ $$ $$\frac{\partial f}{\partial y} = 14y+5x$$ $$\Rightarrow y_0 = 14b+5a$$ For the following linearization: $$ $$ $$L(x,y) = 4a^2+7b^2+5ab+14+(8a+5b)(x-a)+(14b+5a)(y-b)$$ $$ $$ Rearranging this to be in the standard form of a plane: $$ $$ $$x(8a+5b)+y(14b+5a)-z = 4a^2+7b^2+5ab-14$$ $$ $$ If the goal is to find the two points where the given vector is normal to this plane I am at a loss. Any help from this point would be greatly appreciated. Edit: Fixed an error in the partial derivatives.",I'm given the two variable function and asked to find the (2) points on the surface where the vector is normal to the tangent plane.  So far I have written a generalized linearization using the points : For the following linearization: Rearranging this to be in the standard form of a plane: If the goal is to find the two points where the given vector is normal to this plane I am at a loss. Any help from this point would be greatly appreciated. Edit: Fixed an error in the partial derivatives.,"f(x,y) = 4x^2+7y^2+5xy+14 6\hat i + 69\hat j + 3\hat k f(a,b) f(a,b) = 4a^2+7b^2+5ab+14 \Rightarrow z_0 = 4a^2+7b^2+5ab+14   \frac{\partial f}{\partial x} = 8x+5y \Rightarrow x_0 = 8a+5b   \frac{\partial f}{\partial y} = 14y+5x \Rightarrow y_0 = 14b+5a   L(x,y) = 4a^2+7b^2+5ab+14+(8a+5b)(x-a)+(14b+5a)(y-b)     x(8a+5b)+y(14b+5a)-z = 4a^2+7b^2+5ab-14  ","['calculus', 'multivariable-calculus', 'vectors', 'tangent-spaces', 'linearization']"
45,Can someone help me understand the difference between gradient vector and directional derivative?,Can someone help me understand the difference between gradient vector and directional derivative?,,"Okay so here's what I understand: If we have a surface, then the directional derivate in the direction of a unit vector $\vec{u}$ at a point $P_{0}$ is the slope of the curve on the surface going through $P_{0}$ in the direction of $\vec{u}$ And $D_{\vec{u}} = f_{x}(x,y)a + f_{y}(x,y)b$ where $\vec{u} = \langle a, b \rangle$ and $\nabla f(x,y) = \langle f_{x}(x,y), f_{y}(x,y) \rangle $ So $D_{\vec{u}} = \nabla f(x,y) \cdot u$ But then $\nabla f(x,y)$ is in the direction of maximum ascent? how does that come to be? what is happening here?","Okay so here's what I understand: If we have a surface, then the directional derivate in the direction of a unit vector at a point is the slope of the curve on the surface going through in the direction of And where and So But then is in the direction of maximum ascent? how does that come to be? what is happening here?","\vec{u} P_{0} P_{0} \vec{u} D_{\vec{u}} = f_{x}(x,y)a + f_{y}(x,y)b \vec{u} = \langle a, b \rangle \nabla f(x,y) = \langle f_{x}(x,y), f_{y}(x,y) \rangle  D_{\vec{u}} = \nabla f(x,y) \cdot u \nabla f(x,y)",['multivariable-calculus']
46,"Lagrange multiplier $f(x,y)=x^2+2xy^2+y^2$. Show that $(-1,1)$ is the minimal point of $f$.",Lagrange multiplier . Show that  is the minimal point of .,"f(x,y)=x^2+2xy^2+y^2 (-1,1) f","Given the function $f(x,y)=x^2+2xy^2+y^2$ with the restriction $x+2y-1=0$ I can't justify why the point (-1,1) is the minimum point of $f$ . I used the lagrange multiplier and found the following maximum and minimum candidates: $(-1,1), (-1/2,2/3)$ , and $(0,2/3)$ .","Given the function with the restriction I can't justify why the point (-1,1) is the minimum point of . I used the lagrange multiplier and found the following maximum and minimum candidates: , and .","f(x,y)=x^2+2xy^2+y^2 x+2y-1=0 f (-1,1), (-1/2,2/3) (0,2/3)","['functional-analysis', 'analysis', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
47,"Integral bounds for a square with vertices $(\pm2,0), (0,\pm2)$",Integral bounds for a square with vertices,"(\pm2,0), (0,\pm2)","In exercise X §1 1(e) from ""Calculus of several variables"" by Serge Lang (Third Edition), we are asked to Use Green's theorem to find the integral $\int_C y^2 \;dx+x\;dy$ When C is the following curve (taken counterclockwise) ... (e) The square with vertices $(\pm2,0), (0, \pm2)$ . I took this to mean $$ \int_{-2}^0\int_{-2-x}^{2+x}(1-2y)dy\;dx+ \int_{0}^2\int_{x-2}^{2-x}(1-2y)dy\;dx $$ which comes out to 0 according to my calculations. Checking in the back the answer was given as 8. I checked my work again and didn't find any error, but according to the explanation for the answer, see photograph below, the integral is just $$ \int_{-2}^{2}\int_{-2}^{2}(1-2y)dy\;dx $$ I think the book's answer is wrong, but I am asking for your opinion because I would like to check that I haven't missed some obvious thing.","In exercise X §1 1(e) from ""Calculus of several variables"" by Serge Lang (Third Edition), we are asked to Use Green's theorem to find the integral When C is the following curve (taken counterclockwise) ... (e) The square with vertices . I took this to mean which comes out to 0 according to my calculations. Checking in the back the answer was given as 8. I checked my work again and didn't find any error, but according to the explanation for the answer, see photograph below, the integral is just I think the book's answer is wrong, but I am asking for your opinion because I would like to check that I haven't missed some obvious thing.","\int_C y^2 \;dx+x\;dy (\pm2,0), (0, \pm2) 
\int_{-2}^0\int_{-2-x}^{2+x}(1-2y)dy\;dx+
\int_{0}^2\int_{x-2}^{2-x}(1-2y)dy\;dx
 
\int_{-2}^{2}\int_{-2}^{2}(1-2y)dy\;dx
","['multivariable-calculus', 'solution-verification']"
48,I'm having trouble computing a double integral in a probability question.,I'm having trouble computing a double integral in a probability question.,,"The Question: Two points are chosen randomly and independently on two adjacent sides of a square, connecting them splits the square into two parts, one of which is a right-angled triangle. What is the probability the resulting triangle's area will be less than $\tfrac16$ th the area of the square? My Attempted Solution I started by defining k as the length of the side of the x And I decided the sides will be on the x and y axis respectively. $$X = \text{The first point's x value} \quad X \sim \mathrm{U}_{[0,k]}$$ $$Y = \text{The second point's y value} \quad Y \sim \mathrm{U}_{[0,k]}$$ since they're independent their shared pdf is $$f_{X,Y}(x,y) = \frac{1}{k^2}$$ So solving then becomes: $$P\left(0.5XY<\frac{k^2}{6}\right) = \iint_{xy< \frac{k^2}{3} } f_{X,Y}(x,y) dxdy $$ this is the point where I'm getting confused, my current attempt is to then do $$\int_{\frac{k}{3}}^{k} \int_{\frac{k}{3}}^{\frac{k^2}{3x}} \frac{1}{k^2} dy dx = \frac{\ln(3)}{3} -\frac{2}{9}$$ Based on a drawing of the region I'm integrating, but what I'm getting is not the right answer. I'd love to get help with whether my approach is okay, and where did I go wrong","The Question: Two points are chosen randomly and independently on two adjacent sides of a square, connecting them splits the square into two parts, one of which is a right-angled triangle. What is the probability the resulting triangle's area will be less than th the area of the square? My Attempted Solution I started by defining k as the length of the side of the x And I decided the sides will be on the x and y axis respectively. since they're independent their shared pdf is So solving then becomes: this is the point where I'm getting confused, my current attempt is to then do Based on a drawing of the region I'm integrating, but what I'm getting is not the right answer. I'd love to get help with whether my approach is okay, and where did I go wrong","\tfrac16 X = \text{The first point's x value} \quad X \sim \mathrm{U}_{[0,k]} Y = \text{The second point's y value} \quad Y \sim \mathrm{U}_{[0,k]} f_{X,Y}(x,y) = \frac{1}{k^2} P\left(0.5XY<\frac{k^2}{6}\right) = \iint_{xy< \frac{k^2}{3} } f_{X,Y}(x,y) dxdy  \int_{\frac{k}{3}}^{k} \int_{\frac{k}{3}}^{\frac{k^2}{3x}} \frac{1}{k^2} dy dx = \frac{\ln(3)}{3} -\frac{2}{9}","['probability', 'multivariable-calculus', 'probability-distributions']"
49,Verify stokes theorem in spherical coordinates,Verify stokes theorem in spherical coordinates,,"I got the vector field $F= r \sin^2 (\theta) \hat{r} + r\sin(\theta)\cos(\phi) \hat{\theta}+ r\cos^2(\theta)\sin(\phi) \hat{\phi}$ . I’m trying to solve the integral $ \int_C F \cdot dl$ over a close path. The idea is to use the volume stored in the upper hemisphere of radius $R$ $(x^2 + y^2 + z^2 =R^2, z>0)$ but I cannot use the stokes theorem, since the idea is to verify both integrals have the same result. I don’t know how to parametrize the surface.  I know $dl= dr \hat{r} + rd\theta \hat{\theta} + r\sin(\theta) d\phi \hat{\phi}$ but I cannot integrate over a closed path with this. Thank you!","I got the vector field . I’m trying to solve the integral over a close path. The idea is to use the volume stored in the upper hemisphere of radius but I cannot use the stokes theorem, since the idea is to verify both integrals have the same result. I don’t know how to parametrize the surface.  I know but I cannot integrate over a closed path with this. Thank you!","F= r \sin^2 (\theta) \hat{r} + r\sin(\theta)\cos(\phi) \hat{\theta}+ r\cos^2(\theta)\sin(\phi) \hat{\phi}  \int_C F \cdot dl R (x^2 + y^2 + z^2 =R^2, z>0) dl= dr \hat{r} + rd\theta \hat{\theta} + r\sin(\theta) d\phi \hat{\phi}","['integration', 'multivariable-calculus', 'spherical-coordinates', 'stokes-theorem']"
50,How to construct the unity partition on $\mathbb{S}^1$?,How to construct the unity partition on ?,\mathbb{S}^1,"How to construct the unity partition on $\mathbb{S}^1$ ? I was reading the construction of unity partition on $\mathbb{S}^1$ given in https://en.wikipedia.org/wiki/Partition_of_unity . I would like to understand the steps please. First, I know that $\mathbb{S}^1=\left \{(x,y)\in \mathbb{R}^2:x^2+y^2=1\right \}$ and I need to find two functions $\psi _i:\mathbb{S}^1\to [0,1]$ , $i=1,2$ and for every $p\in \mathbb{S}^1$ we need that $\psi _1(p)+\psi _2(p)=1$ . Wikipedia used the function $\Phi (t)=e^\frac{1}{t^2-1}$ , for $t\in (-1,1)$ . Now i want to connect that function with the functions $\psi _i$ . They choose $\psi _1=\Phi$ and $\psi _2=1-\Phi$ , but I do not understand why it works since the domain of $\psi _i$ need to be $\mathbb{S}^1$ instead of $(-1,1)$ . Can somebody help me to understand what is the idea behind of this or I do not get it?  Thank you.","How to construct the unity partition on ? I was reading the construction of unity partition on given in https://en.wikipedia.org/wiki/Partition_of_unity . I would like to understand the steps please. First, I know that and I need to find two functions , and for every we need that . Wikipedia used the function , for . Now i want to connect that function with the functions . They choose and , but I do not understand why it works since the domain of need to be instead of . Can somebody help me to understand what is the idea behind of this or I do not get it?  Thank you.","\mathbb{S}^1 \mathbb{S}^1 \mathbb{S}^1=\left \{(x,y)\in \mathbb{R}^2:x^2+y^2=1\right \} \psi _i:\mathbb{S}^1\to [0,1] i=1,2 p\in \mathbb{S}^1 \psi _1(p)+\psi _2(p)=1 \Phi (t)=e^\frac{1}{t^2-1} t\in (-1,1) \psi _i \psi _1=\Phi \psi _2=1-\Phi \psi _i \mathbb{S}^1 (-1,1)","['calculus', 'integration', 'multivariable-calculus']"
51,integration with respect to time,integration with respect to time,,"I am pretty sure this is a really basic question, but after the summer, not using integrals / derivatives at all, I just can not remember how to do math anymore. I have a function for acceleration that I need to integrate with respect to time $t$ in order to obtain speed. My acceleration function is (for angle $\phi$ ) $$\ddot \phi = -2 \frac{\dot r}{r} \dot \phi - \dot \phi \dot \theta \frac{cos(\theta)}{sin(\theta)}$$ where $$\ddot \phi = \frac{d^2}{dt^2}\phi $$ and $$\dot \theta = \frac{d}{dt}\theta $$ and $$\dot \phi = \frac{d}{dt}\phi$$ and so on. so I would need to obtain the $\dot \phi$ by integrating the acceleration once. $$\dot \phi = \int \ddot \phi dt= \int \Big(-2 \frac{\dot r}{r} \dot \phi - \dot \phi \dot \theta \frac{cos(\theta)}{sin(\theta)} \Big) dt$$ I just do not remember at all how to start to process this problem. I know it's almost a bit shameful but indeed could use a helping hand here. (I have 2 similar equations to be solved for $\ddot \theta$ and $\ddot r$ but I am pretty sure I get these done when I once remember how to calculate things) Thank you so much if you could help me out. Those 2 other equations are following : (if someone is interested) $$\ddot{r} = r \dot \theta ^2 + r\dot\phi^2 sin^2(\theta) -\frac{GM}{r^2}$$ $$\ddot \theta = \dot \phi^2 sin(\theta)cos(\theta) - 2 \frac{\dot r}{r} \theta$$","I am pretty sure this is a really basic question, but after the summer, not using integrals / derivatives at all, I just can not remember how to do math anymore. I have a function for acceleration that I need to integrate with respect to time in order to obtain speed. My acceleration function is (for angle ) where and and and so on. so I would need to obtain the by integrating the acceleration once. I just do not remember at all how to start to process this problem. I know it's almost a bit shameful but indeed could use a helping hand here. (I have 2 similar equations to be solved for and but I am pretty sure I get these done when I once remember how to calculate things) Thank you so much if you could help me out. Those 2 other equations are following : (if someone is interested)",t \phi \ddot \phi = -2 \frac{\dot r}{r} \dot \phi - \dot \phi \dot \theta \frac{cos(\theta)}{sin(\theta)} \ddot \phi = \frac{d^2}{dt^2}\phi  \dot \theta = \frac{d}{dt}\theta  \dot \phi = \frac{d}{dt}\phi \dot \phi \dot \phi = \int \ddot \phi dt= \int \Big(-2 \frac{\dot r}{r} \dot \phi - \dot \phi \dot \theta \frac{cos(\theta)}{sin(\theta)} \Big) dt \ddot \theta \ddot r \ddot{r} = r \dot \theta ^2 + r\dot\phi^2 sin^2(\theta) -\frac{GM}{r^2} \ddot \theta = \dot \phi^2 sin(\theta)cos(\theta) - 2 \frac{\dot r}{r} \theta,"['calculus', 'integration', 'multivariable-calculus', 'derivatives']"
52,An equality about an infimum over a vector: $\inf_y = \inf_{\|v\|=1}\inf_t$,An equality about an infimum over a vector:,\inf_y = \inf_{\|v\|=1}\inf_t,"I am reading convex optimization lecture notes , where I saw the equality as a part of the derivation: Here $f$ is a function and $\nabla f$ is the gradient. What seems to happen here is that the author replaces $(y-z) = t v$ with $\|v \| = 1$ and converts infimum over $y$ to double infimum (is this trivial? i.e. infimum over multiplication equals taking a double infimum?). What confuses me further here is, with that, now $z$ depends on $v$ and I don't understand how this works, or I'm overthinking. Could anyone provide a proof for this? Is this a well-known technique?","I am reading convex optimization lecture notes , where I saw the equality as a part of the derivation: Here is a function and is the gradient. What seems to happen here is that the author replaces with and converts infimum over to double infimum (is this trivial? i.e. infimum over multiplication equals taking a double infimum?). What confuses me further here is, with that, now depends on and I don't understand how this works, or I'm overthinking. Could anyone provide a proof for this? Is this a well-known technique?",f \nabla f (y-z) = t v \|v \| = 1 y z v,"['multivariable-calculus', 'convex-optimization', 'supremum-and-infimum']"
53,Continuity of specific bivariate function,Continuity of specific bivariate function,,"Let $f:\Bbb R^2 \to \Bbb R$ be defined by the argument-value description: $$f(x, y) = (x+1)^y $$ then I should be able to show that $f$ is continuous on the unit square $[0,1]^2$ . However, so far, I did not manage to do it. I tried using the $\epsilon - \delta$ definition but it becomes very difficult very quickly. Therefore, I would like to ask if there is any easier way of showing it. As always any comment or answer is welcome and let me know if I can explain myself clearer!","Let be defined by the argument-value description: then I should be able to show that is continuous on the unit square . However, so far, I did not manage to do it. I tried using the definition but it becomes very difficult very quickly. Therefore, I would like to ask if there is any easier way of showing it. As always any comment or answer is welcome and let me know if I can explain myself clearer!","f:\Bbb R^2 \to \Bbb R f(x, y) = (x+1)^y  f [0,1]^2 \epsilon - \delta","['real-analysis', 'calculus', 'multivariable-calculus', 'continuity']"
54,Dot product between a vector and a matrix,Dot product between a vector and a matrix,,"Suppose $v$ is a vector valued function. Then $\nabla v$ is a rank 2 tensor, i.e. a matrix. In the Navier-Stokes equations we see a term of the form $$(v \cdot \nabla) v.$$ At first I thought that the dot would indicate matrix multiplication, but as it's written, this does not work due to dimensions. How is one supposed to interpret/evaluate this term? Also, what does the divergence of this term look like, i.e. $\nabla\cdot\big((v \cdot \nabla) v\big)$ ? Does it follow the usual product rule?","Suppose is a vector valued function. Then is a rank 2 tensor, i.e. a matrix. In the Navier-Stokes equations we see a term of the form At first I thought that the dot would indicate matrix multiplication, but as it's written, this does not work due to dimensions. How is one supposed to interpret/evaluate this term? Also, what does the divergence of this term look like, i.e. ? Does it follow the usual product rule?",v \nabla v (v \cdot \nabla) v. \nabla\cdot\big((v \cdot \nabla) v\big),"['linear-algebra', 'multivariable-calculus', 'tensor-products', 'tensors']"
55,Properness of the map $\psi$ in the proof of Normal bundle is a manifold,Properness of the map  in the proof of Normal bundle is a manifold,\psi,"In Guillemin and Pollack's Differential Topology book while proving that the Normal bundle is a manifold they used the following argument: Given a manifold $Y$ embedded in $\mathbb{R}^M$ choose locally an open set $\tilde{U}$ of $\mathbb{R}^M$ and a submersion $\phi: \tilde{U} \to \mathbb{R}^k$ such that $U = Y \cap \tilde{U} = \phi^{-1}(0)$ . Then $N(U)=N(Y)\cap U\times \mathbb{R}^M$ , is open is $N(Y)$ . Now they took the map $\psi: U\times \mathbb{R}^k \to N(U), \psi(y,v)=(y,d\phi_y^tv)$ and claimed this to be an embedding from $U\times \mathbb{R}^k \to U \times \mathbb{R}^M$ . In their context embedding is a proper, injective, immersion. I am unable to prove that the map is proper. I tried with sequential approach but could not proceed. Any help in this regard will be appreciated. In this regard I found the following solution by Leandro Caniglia ( https://math.stackexchange.com/users/269050/leandro-caniglia ), Normal bundle of a submanifold is again a manifold, URL (version: 2022-04-30): https://math.stackexchange.com/q/4439740 In this case I also want to prove that the projection onto the first coordinate from $\sigma: N(Y)\to Y$ is a submersion. For that purpose I just chose coordinates for $\tilde{U},U$ using the slice chart as in the above solution. Similarly I get a coordinate chart for $N(U)$ . Then the map $\sigma$ commutes with the charts to give the canonical submersion and hence is itself a submersion. Is this method correct? Although this is a different approach to solve the main problem. I will still like to know the answer of my first question which I am not able to solve so far.","In Guillemin and Pollack's Differential Topology book while proving that the Normal bundle is a manifold they used the following argument: Given a manifold embedded in choose locally an open set of and a submersion such that . Then , is open is . Now they took the map and claimed this to be an embedding from . In their context embedding is a proper, injective, immersion. I am unable to prove that the map is proper. I tried with sequential approach but could not proceed. Any help in this regard will be appreciated. In this regard I found the following solution by Leandro Caniglia ( https://math.stackexchange.com/users/269050/leandro-caniglia ), Normal bundle of a submanifold is again a manifold, URL (version: 2022-04-30): https://math.stackexchange.com/q/4439740 In this case I also want to prove that the projection onto the first coordinate from is a submersion. For that purpose I just chose coordinates for using the slice chart as in the above solution. Similarly I get a coordinate chart for . Then the map commutes with the charts to give the canonical submersion and hence is itself a submersion. Is this method correct? Although this is a different approach to solve the main problem. I will still like to know the answer of my first question which I am not able to solve so far.","Y \mathbb{R}^M \tilde{U} \mathbb{R}^M \phi: \tilde{U} \to \mathbb{R}^k U = Y \cap \tilde{U} = \phi^{-1}(0) N(U)=N(Y)\cap U\times \mathbb{R}^M N(Y) \psi: U\times \mathbb{R}^k \to N(U), \psi(y,v)=(y,d\phi_y^tv) U\times \mathbb{R}^k \to U \times \mathbb{R}^M \sigma: N(Y)\to Y \tilde{U},U N(U) \sigma","['multivariable-calculus', 'differential-geometry', 'differential-topology']"
56,Can A Dependent Variable be Factored Out of Integral?,Can A Dependent Variable be Factored Out of Integral?,,"say I have a the equation $$y = xw + z$$ And I am trying to compute $$\begin{aligned}\int \text{exp}((y - xw)^2 - w^2)dw &= \int\text{exp}(y^2 - 2xwy + x^2w^2 - w^2)dw \\&= \int\text{exp}(y^2)exp(-2xwy + x^2 -w^2)dw \end{aligned}$$ Am I allowed to factor out the $\text{exp}(y^2)$ term considering $y = xw + z$ to get $$\text{exp}(y^2)\int\text{exp}(-2xwy + x^2 - w^2)dw$$ I appreciate all of the swift replies. I did believe that was not correct, but needed a sanity check. Thank you all. To answer some comments, x and z are constants here.","say I have a the equation And I am trying to compute Am I allowed to factor out the term considering to get I appreciate all of the swift replies. I did believe that was not correct, but needed a sanity check. Thank you all. To answer some comments, x and z are constants here.","y = xw + z \begin{aligned}\int \text{exp}((y - xw)^2 - w^2)dw &= \int\text{exp}(y^2 - 2xwy + x^2w^2 - w^2)dw
\\&= \int\text{exp}(y^2)exp(-2xwy + x^2 -w^2)dw \end{aligned} \text{exp}(y^2) y = xw + z \text{exp}(y^2)\int\text{exp}(-2xwy + x^2 - w^2)dw",['multivariable-calculus']
57,Evaluating curl of $\hat{\textbf{r}}$ in cartesian coordinates,Evaluating curl of  in cartesian coordinates,\hat{\textbf{r}},"I am reading book about Classical Mechanics, which states that a spherically symmetric central force is always conservative, and I want to prove it. Spherical symmetry of a function $f$ means that in spherical coordinates the partial derivatives wrt the polar and azimuthal angles are zero: $$\frac{\partial f}{\partial \phi}=\frac{\partial f}{\partial \theta}=0$$ (I am using the mathematician's convention that $\phi$ is the polar and $\theta$ is the azimuth.) A central force is just a force of the form $\textbf{F}=f(\textbf{r})\hat{\textbf{r}}$ , where $\textbf{r}$ is the position. Thus, a spherically symmetric central force is of the form $\textbf{F}=f(r)\hat{\textbf{r}}$ . The task to show this to be conservative, which is the same as showing that the curl vanishes (a result of Stokes' theorem). So the task is: $\nabla\times f(r)\hat{\textbf{r}}=0$ Now, I can easily evaluate this curl in spherical coordinates without a problem, but I do want to evaluate this in cartesian coordinates just to see that it works, despite the fact that it's probably a bad idea. Anyway, $\hat{\textbf{r}}$ in cartesian coordinates is $$\hat{\textbf{r}}=\sin(\phi)\cos(\theta)\hat{\textbf{x}}+\sin(\phi)\sin(\theta)\hat{\textbf{y}}+\cos(\phi)\hat{\textbf{z}}$$ For simplicity, I ignore the $f(r)$ factor. The curl is $$\nabla\times \hat{\textbf{r}}= \begin{bmatrix} \frac{\partial \cos(\phi)}{\partial y}-\frac{\partial \sin(\phi)\sin(\theta)}{\partial z}\\ \frac{\partial \sin(\phi)\cos(\theta)}{\partial z}-\frac{\partial \cos(\phi)}{\partial x}\\ \frac{\partial \sin(\phi)\sin(\theta)}{\partial x}-\frac{\partial \sin(\phi)\cos(\theta)}{\partial y} \end{bmatrix}$$ I thought that I could use the chain rule on $\frac{\partial\cos(\phi)}{\partial y}$ in a perhaps hand-wavy manner: $$\frac{\partial\cos(\phi)}{\partial y}=\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial \phi}{\partial y}\right)=\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial y}{\partial \phi}\right)^{-1}$$ But I tried it on the $x$ coordinate: $$\frac{\partial \cos(\phi)}{\partial y}-\frac{\partial \sin(\phi)\sin(\theta)}{\partial z}=$$ $$\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial y}{\partial \phi}\right)^{-1}-\frac{\partial \sin(\phi)}{\partial \phi}\left(\frac{\partial z}{\partial \phi}\right)^{-1}\sin(\theta) -\frac{\partial \sin(\theta)}{\partial \theta}\left(\frac{\partial z}{\partial \theta}\right)^{-1}\sin(\phi)=$$ $$-\sin(\phi)\left(\frac{\partial y}{\partial \phi}\right)^{-1}-\cos(\phi)\left(\frac{\partial z}{\partial \phi}\right)^{-1}\sin(\theta) -\cos(\theta)\left(\frac{\partial z}{\partial \theta}\right)^{-1}\sin(\phi)= (*)$$ We know: $y=r\sin(\phi)\sin(\theta)$ , $z=r\cos(\phi)$ , so we obtain $$\frac{\partial z}{\partial \phi}=-r\sin(\phi)\qquad \frac{\partial y}{\partial \phi}=r\cos(\phi)\sin(\theta)$$ $$(*)=-\sin(\phi)\left(r\cos(\phi)\sin(\theta)\right)^{-1}-\cos(\phi)\left(-r\sin(\phi)\right)^{-1}\sin(\theta) -\cos(\theta)\left(-r\sin(\phi)\right)^{-1}\sin(\phi)$$ $$=-\tan(\phi)/r\sin(\theta)+\sin(\theta)/r\tan(\phi) +\cos(\theta)/r$$ $$=\frac{\sin^{2}(\theta)+\cos(\theta)\sin(\theta)\tan(\phi)-\tan(\phi)\sin(\theta)}{r\sin(\theta)\tan(\phi)}$$ $$\neq 0$$ I suspect something went wrong during the application of the chain rule. So how would this curl be evaluated correctly? How does this generalize to the function $f(r)\hat{\textbf{r}}$ ?","I am reading book about Classical Mechanics, which states that a spherically symmetric central force is always conservative, and I want to prove it. Spherical symmetry of a function means that in spherical coordinates the partial derivatives wrt the polar and azimuthal angles are zero: (I am using the mathematician's convention that is the polar and is the azimuth.) A central force is just a force of the form , where is the position. Thus, a spherically symmetric central force is of the form . The task to show this to be conservative, which is the same as showing that the curl vanishes (a result of Stokes' theorem). So the task is: Now, I can easily evaluate this curl in spherical coordinates without a problem, but I do want to evaluate this in cartesian coordinates just to see that it works, despite the fact that it's probably a bad idea. Anyway, in cartesian coordinates is For simplicity, I ignore the factor. The curl is I thought that I could use the chain rule on in a perhaps hand-wavy manner: But I tried it on the coordinate: We know: , , so we obtain I suspect something went wrong during the application of the chain rule. So how would this curl be evaluated correctly? How does this generalize to the function ?","f \frac{\partial f}{\partial \phi}=\frac{\partial f}{\partial \theta}=0 \phi \theta \textbf{F}=f(\textbf{r})\hat{\textbf{r}} \textbf{r} \textbf{F}=f(r)\hat{\textbf{r}} \nabla\times f(r)\hat{\textbf{r}}=0 \hat{\textbf{r}} \hat{\textbf{r}}=\sin(\phi)\cos(\theta)\hat{\textbf{x}}+\sin(\phi)\sin(\theta)\hat{\textbf{y}}+\cos(\phi)\hat{\textbf{z}} f(r) \nabla\times \hat{\textbf{r}}=
\begin{bmatrix}
\frac{\partial \cos(\phi)}{\partial y}-\frac{\partial \sin(\phi)\sin(\theta)}{\partial z}\\
\frac{\partial \sin(\phi)\cos(\theta)}{\partial z}-\frac{\partial \cos(\phi)}{\partial x}\\
\frac{\partial \sin(\phi)\sin(\theta)}{\partial x}-\frac{\partial \sin(\phi)\cos(\theta)}{\partial y}
\end{bmatrix} \frac{\partial\cos(\phi)}{\partial y} \frac{\partial\cos(\phi)}{\partial y}=\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial \phi}{\partial y}\right)=\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial y}{\partial \phi}\right)^{-1} x \frac{\partial \cos(\phi)}{\partial y}-\frac{\partial \sin(\phi)\sin(\theta)}{\partial z}= \frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial y}{\partial \phi}\right)^{-1}-\frac{\partial \sin(\phi)}{\partial \phi}\left(\frac{\partial z}{\partial \phi}\right)^{-1}\sin(\theta) -\frac{\partial \sin(\theta)}{\partial \theta}\left(\frac{\partial z}{\partial \theta}\right)^{-1}\sin(\phi)= -\sin(\phi)\left(\frac{\partial y}{\partial \phi}\right)^{-1}-\cos(\phi)\left(\frac{\partial z}{\partial \phi}\right)^{-1}\sin(\theta) -\cos(\theta)\left(\frac{\partial z}{\partial \theta}\right)^{-1}\sin(\phi)= (*) y=r\sin(\phi)\sin(\theta) z=r\cos(\phi) \frac{\partial z}{\partial \phi}=-r\sin(\phi)\qquad \frac{\partial y}{\partial \phi}=r\cos(\phi)\sin(\theta) (*)=-\sin(\phi)\left(r\cos(\phi)\sin(\theta)\right)^{-1}-\cos(\phi)\left(-r\sin(\phi)\right)^{-1}\sin(\theta) -\cos(\theta)\left(-r\sin(\phi)\right)^{-1}\sin(\phi) =-\tan(\phi)/r\sin(\theta)+\sin(\theta)/r\tan(\phi) +\cos(\theta)/r =\frac{\sin^{2}(\theta)+\cos(\theta)\sin(\theta)\tan(\phi)-\tan(\phi)\sin(\theta)}{r\sin(\theta)\tan(\phi)} \neq 0 f(r)\hat{\textbf{r}}","['multivariable-calculus', 'vector-analysis']"
58,Folland Advanced Calculus Ex. 2.5.7,Folland Advanced Calculus Ex. 2.5.7,,"Suppose that the variables $E$ , $T$ , $V$ , and $P$ are related by a pair of equations, $f(E,T,V,P)=0$ and $g(E,T,V,P)=0$ , that can be solved for any two of the variables in terms of the other two, and suppose that the differential equation $\partial_VE-T\partial_TP+P=0$ is satisfied when $V$ and $T$ are taken as the independent variables. Show that $\partial_PE+T\partial_TV+P\partial_PV=0$ when $P$ and $T$ are taken as the independent variables. If we put $\phi(V,T)=(E(V,T),T,V,P(V,T))$ and $F=(f,g)$ , then $(F\circ\phi)(V,T)=0$ and hence $F'(\phi(V,T))\phi'(V,T)=0$ . How to proceed any further?","Suppose that the variables , , , and are related by a pair of equations, and , that can be solved for any two of the variables in terms of the other two, and suppose that the differential equation is satisfied when and are taken as the independent variables. Show that when and are taken as the independent variables. If we put and , then and hence . How to proceed any further?","E T V P f(E,T,V,P)=0 g(E,T,V,P)=0 \partial_VE-T\partial_TP+P=0 V T \partial_PE+T\partial_TV+P\partial_PV=0 P T \phi(V,T)=(E(V,T),T,V,P(V,T)) F=(f,g) (F\circ\phi)(V,T)=0 F'(\phi(V,T))\phi'(V,T)=0",[]
59,Find that of $\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}$ equals $= \frac{\partial^2 f}{\partial u^2} + ...$,Find that of  equals,\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = \frac{\partial^2 f}{\partial u^2} + ...,"Given: $$\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = \frac{\partial^2 f}{\partial u^2} + \frac{\partial^2 f}{\partial v^2}$$ where $$u = x \cos \theta + y \sin \theta$$ $$v = -x \sin \theta + y \cos \theta$$ I am trying to show how we can get: $$ \frac{\partial f}{\partial y} = \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} = \frac{\partial f}{\partial u} \sin \theta+ \frac{\partial f}{\partial v} \cos \theta$$ Now, we can see that if we take the derivative of $u = x \cos \theta + y \sin \theta$ with respect to $y$ we can get why $\frac{\partial v}{\partial y} = sin(\theta)$ and same for $\frac{\partial u}{\partial y} = sin(\theta)$ . Question : how we can get that $ \frac{\partial f}{\partial y} = \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} $ in the first place please as if we summed $ \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} $ we will get $ 2 \frac{\partial f}{\partial y}  $ and not $ \frac{\partial f}{\partial y}  $ ?","Given: where I am trying to show how we can get: Now, we can see that if we take the derivative of with respect to we can get why and same for . Question : how we can get that in the first place please as if we summed we will get and not ?",\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = \frac{\partial^2 f}{\partial u^2} + \frac{\partial^2 f}{\partial v^2} u = x \cos \theta + y \sin \theta v = -x \sin \theta + y \cos \theta  \frac{\partial f}{\partial y} = \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y} = \frac{\partial f}{\partial u} \sin \theta+ \frac{\partial f}{\partial v} \cos \theta u = x \cos \theta + y \sin \theta y \frac{\partial v}{\partial y} = sin(\theta) \frac{\partial u}{\partial y} = sin(\theta)  \frac{\partial f}{\partial y} = \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y}   \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y}   2 \frac{\partial f}{\partial y}    \frac{\partial f}{\partial y}  ,"['multivariable-calculus', 'partial-derivative', 'chain-rule', 'laplacian']"
60,Demonstrate that the limit does not exist.,Demonstrate that the limit does not exist.,,"Is the following reasoning sound for demonstrating that the limit does not exist. The limit is: $$\lim_{(x,y) \to (0,0)}{x^y}$$ I show that the restriction gives me two different values of the limit as follows: $$\lim_{(0,y) \to (0,0)}{x^y}=\lim_{(0,y) \to (0,0)}0^y=0$$ $$\lim_{(x,0) \to (0,0)}{x^y}=\lim_{(x,0) \to (0,0)}x^0=1$$ Therefore the limit does not exist.",Is the following reasoning sound for demonstrating that the limit does not exist. The limit is: I show that the restriction gives me two different values of the limit as follows: Therefore the limit does not exist.,"\lim_{(x,y) \to (0,0)}{x^y} \lim_{(0,y) \to (0,0)}{x^y}=\lim_{(0,y) \to (0,0)}0^y=0 \lim_{(x,0) \to (0,0)}{x^y}=\lim_{(x,0) \to (0,0)}x^0=1","['limits', 'multivariable-calculus']"
61,Computing surface area of a plane,Computing surface area of a plane,,"I am learning about surface integrals and flux integrals and I have done the following exercise for practice. Since I am still not entirely sure if I have understood correctly I would appreciate some feedback on my solution below (whether it is correct, if/what can be improved), thanks. Let $S$ be that portion of the plane $x+2y+2z=4$ lying in the first octant, oriented with outward normal pointing upward. Find (a) the area of $S$ ; (b) $\int_{S} (x-y+3z)\sigma$ ; (c) $\int_{S} zdx\wedge dy+ydz\wedge dx+xdy\wedge dz$ where $\sigma=n_1dy\wedge dz+n_2dz\wedge dx+n_3dx\wedge dy$ is the area $2$ -form (and $\mathbf{n}=(n_1,n_2,n_3)$ is the outward-pointing unit normal to the surface $S$ .) What I have done: We parametrize the surface by $\mathbf{g}:\Omega\to\mathbb{R}^3,\ \mathbf{g}\begin{pmatrix}u\\v\end{pmatrix}=\begin{bmatrix} u\\v  \\ \frac{4-u-2v}{2} \end{bmatrix}$ and note that $D\mathbf{g}=\begin{bmatrix}1 & 0\\0 & 1\\-\frac{1}{2} & -1\end{bmatrix}$ and that $\mathbf{n}=\frac{\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}}{\lVert\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}\rVert}=\frac{\left(\frac{1}{2},1,1\right)}{\frac{3}{2}}=\begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{2}{3}\end{bmatrix}$ is a viable outward pointing unit normal vector so (a) $\text{area}(S)=\int_{S}\sigma=\int_{\Omega}\mathbf{g}^*\sigma=\int_{\Omega}\left(\frac{1}{2}\cdot \frac{1}{3}+\frac{2}{3}+\frac{2}{3}\right)dudv=\int_{\Omega}\frac{3}{2}dudv\overset{*}{=}\int_{u=0}^{u=4}\left(\int_{v=0}^{v=2-\frac{1}{2}u}dv\right)du=\frac{3}{2}\int_{u=0}^{u=4}\left(2-\frac{1}{2}u\right)du=6.$ ( $^*$ To be in the first octant it must be $u\geq 0,\ v\geq 0$ and $z=\frac{4-u-2v}{2}\geq 0\Leftrightarrow v\leq 2-\frac{1}{2}u$ .) (b) $\int_{S}(x-y+3z)\sigma=\int_{\Omega}\mathbf{g}^* \left( (x-y+3z)\sigma \right)=\int_{\Omega}\left( \left(u-v+\frac{3}{2}(4-u-2v)\right)\left(\frac{3}{2}\right) \right)dudv=\int_{u=0}^{u=4}\left(\int_{v=0}^{v=4}\left(-\frac{3}{4}u-6v+9\right)dv\right)du=\int_{u=0}^{u=4}\left(6-\frac{3}{8}u^2\right)du=16.$ (c) $\int_{S} zdx\wedge dy+ydz\wedge dx+xdy\wedge dz=\int_{\Omega}\mathbf{g}^*\left(zdx\wedge dy+ydz\wedge dx+xdy\wedge dz\right)=\int_{\Omega}\left( \left(\frac{4-u-2v}{2}\right)+v+u\cdot\frac{1}{2} \right)dudv=\int_{\Omega}2dudv=2\int_{u=0}^{u=4}\left(\int_{v=0}^{v=2-\frac{1}{2}u}dv\right)du=2\int_{u=0}^{u=2}\left(2-\frac{1}{2}u\right)du=8.$","I am learning about surface integrals and flux integrals and I have done the following exercise for practice. Since I am still not entirely sure if I have understood correctly I would appreciate some feedback on my solution below (whether it is correct, if/what can be improved), thanks. Let be that portion of the plane lying in the first octant, oriented with outward normal pointing upward. Find (a) the area of ; (b) ; (c) where is the area -form (and is the outward-pointing unit normal to the surface .) What I have done: We parametrize the surface by and note that and that is a viable outward pointing unit normal vector so (a) ( To be in the first octant it must be and .) (b) (c)","S x+2y+2z=4 S \int_{S} (x-y+3z)\sigma \int_{S} zdx\wedge dy+ydz\wedge dx+xdy\wedge dz \sigma=n_1dy\wedge dz+n_2dz\wedge dx+n_3dx\wedge dy 2 \mathbf{n}=(n_1,n_2,n_3) S \mathbf{g}:\Omega\to\mathbb{R}^3,\ \mathbf{g}\begin{pmatrix}u\\v\end{pmatrix}=\begin{bmatrix} u\\v 
\\ \frac{4-u-2v}{2} \end{bmatrix} D\mathbf{g}=\begin{bmatrix}1 & 0\\0 & 1\\-\frac{1}{2} & -1\end{bmatrix} \mathbf{n}=\frac{\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}}{\lVert\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}\rVert}=\frac{\left(\frac{1}{2},1,1\right)}{\frac{3}{2}}=\begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{2}{3}\end{bmatrix} \text{area}(S)=\int_{S}\sigma=\int_{\Omega}\mathbf{g}^*\sigma=\int_{\Omega}\left(\frac{1}{2}\cdot \frac{1}{3}+\frac{2}{3}+\frac{2}{3}\right)dudv=\int_{\Omega}\frac{3}{2}dudv\overset{*}{=}\int_{u=0}^{u=4}\left(\int_{v=0}^{v=2-\frac{1}{2}u}dv\right)du=\frac{3}{2}\int_{u=0}^{u=4}\left(2-\frac{1}{2}u\right)du=6. ^* u\geq 0,\ v\geq 0 z=\frac{4-u-2v}{2}\geq 0\Leftrightarrow v\leq 2-\frac{1}{2}u \int_{S}(x-y+3z)\sigma=\int_{\Omega}\mathbf{g}^* \left( (x-y+3z)\sigma \right)=\int_{\Omega}\left( \left(u-v+\frac{3}{2}(4-u-2v)\right)\left(\frac{3}{2}\right) \right)dudv=\int_{u=0}^{u=4}\left(\int_{v=0}^{v=4}\left(-\frac{3}{4}u-6v+9\right)dv\right)du=\int_{u=0}^{u=4}\left(6-\frac{3}{8}u^2\right)du=16. \int_{S} zdx\wedge dy+ydz\wedge dx+xdy\wedge dz=\int_{\Omega}\mathbf{g}^*\left(zdx\wedge dy+ydz\wedge dx+xdy\wedge dz\right)=\int_{\Omega}\left( \left(\frac{4-u-2v}{2}\right)+v+u\cdot\frac{1}{2} \right)dudv=\int_{\Omega}2dudv=2\int_{u=0}^{u=4}\left(\int_{v=0}^{v=2-\frac{1}{2}u}dv\right)du=2\int_{u=0}^{u=2}\left(2-\frac{1}{2}u\right)du=8.","['multivariable-calculus', 'solution-verification', 'differential-forms', 'multiple-integral', 'surface-integrals']"
62,Special case of multi variable integration with natural log result?,Special case of multi variable integration with natural log result?,,"We know the famous equation $PV = nRT$ but in thermodynamics we typically deal with differentials $P\,dV$ and even $V\,dP$ at times. Given: $$ d(PV) = V\,dP + P\,dV $$ integrating both sides $$ PV = \int \frac{nRT}{P}\,dP + \int \frac{nRT}{V}\,dV  $$ Typically I'm assuming a decoupling constant can be used to decouple the factors particularly $P$ and $V$ . In fact physically the system is not defined for the first 2 variables which we can assume are independent. So the number of moles can't change and the temperature can also be assumed independent. After integration assuming $n$ and $T$ are independent: $$ PV = nRT \cdot \ln(PV) $$ Is this a special result that should be memorized in multivariable calculus regarding a natural log of the same compound variable on the left?",We know the famous equation but in thermodynamics we typically deal with differentials and even at times. Given: integrating both sides Typically I'm assuming a decoupling constant can be used to decouple the factors particularly and . In fact physically the system is not defined for the first 2 variables which we can assume are independent. So the number of moles can't change and the temperature can also be assumed independent. After integration assuming and are independent: Is this a special result that should be memorized in multivariable calculus regarding a natural log of the same compound variable on the left?,"PV = nRT P\,dV V\,dP 
d(PV) = V\,dP + P\,dV
 
PV = \int \frac{nRT}{P}\,dP + \int \frac{nRT}{V}\,dV 
 P V n T 
PV = nRT \cdot \ln(PV)
","['calculus', 'integration', 'multivariable-calculus', 'derivatives']"
63,Show that there is a point $x_0\in\Bbb{R}^n$ such that $Df(x_0)=0$,Show that there is a point  such that,x_0\in\Bbb{R}^n Df(x_0)=0,"Let $f : \Bbb{R}^n \to \Bbb{R}$ be a $C^1$ function such that $$\lim\limits_{\|x\|\to\infty}f(x)=0$$ Show that there is a point $x_0 \in \Bbb{R}^n$ such that $Df(x_0)=0$ . I'm struggling to finish the proof. My idea was to distinguish some case. Let for example suppose that there is $z\in \Bbb{R}^n$ such that $f(z)>0$ . Let $\epsilon>0$ . By limit hypothesis at infinity we have the existence of $c>0$ s.t $\|x\|\ge c \implies |f(x)|<f(z)$ . The idea now is to look what's going on in the set $\overline{B(0,c)}$ . As $f$ is continuous and $\overline{B(0,c)}$ is a compact set, then $f$ has it's max and min there. But, I'm struggling to show that max cannot occur on the board of my close ball. If someone could help I would appreciate it. Thank you","Let be a function such that Show that there is a point such that . I'm struggling to finish the proof. My idea was to distinguish some case. Let for example suppose that there is such that . Let . By limit hypothesis at infinity we have the existence of s.t . The idea now is to look what's going on in the set . As is continuous and is a compact set, then has it's max and min there. But, I'm struggling to show that max cannot occur on the board of my close ball. If someone could help I would appreciate it. Thank you","f : \Bbb{R}^n \to \Bbb{R} C^1 \lim\limits_{\|x\|\to\infty}f(x)=0 x_0 \in \Bbb{R}^n Df(x_0)=0 z\in \Bbb{R}^n f(z)>0 \epsilon>0 c>0 \|x\|\ge c \implies |f(x)|<f(z) \overline{B(0,c)} f \overline{B(0,c)} f","['real-analysis', 'multivariable-calculus', 'derivatives', 'scalar-fields']"
64,"Derivatives of a function $y(x)$ which is implicitly defined by $F(x,y)=0$",Derivatives of a function  which is implicitly defined by,"y(x) F(x,y)=0","I have a function $F(x,y)$ for which I have the analytic expression, and I also know all the partial derivatives $\frac{\partial^{(m+n)}F}{\partial x^m \partial y^n}$ . For each $x$ , the equation $F(x,y) = 0$ has a unique solution for $y$ , i.e. $F(x,y) = 0$ implicitly defines a function $y(x)$ which is continuous and differentiable. I would like to express the derivatives $\frac{d^py}{dx^p}$ of the function $y(x)$ in terms of the known partial derivatives $\frac{\partial^{(m+n)}F}{\partial x^m \partial y^n}$ . How should I proceed?","I have a function for which I have the analytic expression, and I also know all the partial derivatives . For each , the equation has a unique solution for , i.e. implicitly defines a function which is continuous and differentiable. I would like to express the derivatives of the function in terms of the known partial derivatives . How should I proceed?","F(x,y) \frac{\partial^{(m+n)}F}{\partial x^m \partial y^n} x F(x,y) = 0 y F(x,y) = 0 y(x) \frac{d^py}{dx^p} y(x) \frac{\partial^{(m+n)}F}{\partial x^m \partial y^n}","['calculus', 'multivariable-calculus']"
65,How to use Stokes' theorem?,How to use Stokes' theorem?,,"$\int_{K}\left(x^{2}+y z\right) d x+\left(y^{2}+x z\right) d y+\left(z^{2}-x y\right) d z$ where K is a closed curve, oriented positive, consisting of an arc defined by the parametric equation $x=a\cos t, y=a\sin t, z=\frac{1}{2\pi}t$ and segment $BA,A=(a,0,0),B=(a,0,1)$ I drew this K-curve for the parameter a = 2 to see what area we would integrate over. That is, looking from above, it will be a circle of radius: $$ S=\left\{(x, y) \in \mathbb{R}^{2} \mid x^{2}+y^{2}=a^{2}\right\} $$ In polar coordinate system $$ \begin{aligned} &0 \leqslant r \leqslant a \\ &0 \leqslant \vartheta \leqslant 2 \pi \end{aligned} $$ So I use Stokes' theorem because the assumptions are satisfied (see below). Stokes’ theorem: Let $D \subset \mathbb{R}^{2}$ be a regular area and let $\sigma: D \rightarrow S \subset \mathbb{R}^{3}$ be a regular oriented surface with the edge of $\partial S$ being a piecewise smooth curve. Suppose that the surface $S$ is oriented according to its parameterization, that is, in such a way that according to the right-handed screw rule, the circulation around the curve determines the return of the vector normal to the surface. Let $G \subset \mathbb{R}^{3}$ be an open set such that $G \supset S \cup \partial S$ . Suppose that the function $f=(P, Q, R): G \rightarrow \mathbb{R}^{3}$ is a function of the class $C^{(1)}$ in $G$ . Then: $$\int_{\partial S} P d x+Q d y+R d z=\iint_{S}\left(\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z}\right) d y d z+\left(\frac{\partial P}{\partial z}-\frac{\partial R}{\partial x}\right) d z d x+\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y$$ Auxiliary calculation: $$ \frac{\partial R}{\partial y}=-x\,\,  \frac{\partial Q}{\partial z}=x\quad \,\,\frac{\partial P}{\partial z}=y\,\, \frac{\partial R}{\partial x}=-y\,\,  \frac{\partial R}{\partial x}=z\quad \frac{\partial P}{\partial y}=z $$ Then $$ \int_K{\underset{P}{\underbrace{\left( x^2+yz \right) }}dx+\underset{Q}{\underbrace{\left( y^2+xz \right) }}dy+\underset{R}{\underbrace{\left( z^2-xy \right) }}dz}=\iint_S{-2x\,\,dydz}+2y\,\,dxdz\,\, +\,\,0 \cdot dxdy = \ldots \star (?) $$ Question: How to calculate this double integral ( $\star$ ) ? Did I make good use of Stokes' theorem? Is there a faster way to solve this problem? Thanks in advance.","where K is a closed curve, oriented positive, consisting of an arc defined by the parametric equation and segment I drew this K-curve for the parameter a = 2 to see what area we would integrate over. That is, looking from above, it will be a circle of radius: In polar coordinate system So I use Stokes' theorem because the assumptions are satisfied (see below). Stokes’ theorem: Let be a regular area and let be a regular oriented surface with the edge of being a piecewise smooth curve. Suppose that the surface is oriented according to its parameterization, that is, in such a way that according to the right-handed screw rule, the circulation around the curve determines the return of the vector normal to the surface. Let be an open set such that . Suppose that the function is a function of the class in . Then: Auxiliary calculation: Then Question: How to calculate this double integral ( ) ? Did I make good use of Stokes' theorem? Is there a faster way to solve this problem? Thanks in advance.","\int_{K}\left(x^{2}+y z\right) d x+\left(y^{2}+x z\right) d y+\left(z^{2}-x y\right) d z x=a\cos t, y=a\sin t, z=\frac{1}{2\pi}t BA,A=(a,0,0),B=(a,0,1) 
S=\left\{(x, y) \in \mathbb{R}^{2} \mid x^{2}+y^{2}=a^{2}\right\}
 
\begin{aligned}
&0 \leqslant r \leqslant a \\
&0 \leqslant \vartheta \leqslant 2 \pi
\end{aligned}
 D \subset \mathbb{R}^{2} \sigma: D \rightarrow S \subset \mathbb{R}^{3} \partial S S G \subset \mathbb{R}^{3} G \supset S \cup \partial S f=(P, Q, R): G \rightarrow \mathbb{R}^{3} C^{(1)} G \int_{\partial S} P d x+Q d y+R d z=\iint_{S}\left(\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z}\right) d y d z+\left(\frac{\partial P}{\partial z}-\frac{\partial R}{\partial x}\right) d z d x+\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y 
\frac{\partial R}{\partial y}=-x\,\,  \frac{\partial Q}{\partial z}=x\quad \,\,\frac{\partial P}{\partial z}=y\,\, \frac{\partial R}{\partial x}=-y\,\,  \frac{\partial R}{\partial x}=z\quad \frac{\partial P}{\partial y}=z
 
\int_K{\underset{P}{\underbrace{\left( x^2+yz \right) }}dx+\underset{Q}{\underbrace{\left( y^2+xz \right) }}dy+\underset{R}{\underbrace{\left( z^2-xy \right) }}dz}=\iint_S{-2x\,\,dydz}+2y\,\,dxdz\,\, +\,\,0 \cdot dxdy = \ldots \star (?)
 \star","['multivariable-calculus', 'vector-analysis', 'multiple-integral', 'stokes-theorem']"
66,Is a planar square on the equator a locally energy minimizing configuration of electrons on $\mathbb{S}^2$?,Is a planar square on the equator a locally energy minimizing configuration of electrons on ?,\mathbb{S}^2,"$\newcommand{\S}{\mathbb{S}^2}$ Let $$M=\{(x_1,x_2,x_3,x_4) \in  \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \, |\,\, \text{ all the } x_i \, \text{ are distinct}\} $$ Let $E:M \to \mathbb{R}$ be defined by $$E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|},$$ where $\| x_i - x_j \|$ denotes the Euclidean distance in $\mathbb{R}^3$ . Question: Let $p:=(x_1,x_2,x_3,x_4)$ be the configuration of a planar square lying on the equator of $\S$ . Is $p$ a local minimum of $E$ ? (Of course, it is not a strict minima since one can rotate). Here is an attempt: Let $\beta_i(t)$ be a path in $\mathbb{S}^2$ , $\beta_i(0)=x_i, \dot \beta(0)=w_i \in T_{x_i}\S$ . Consider the path $$\alpha(t)=(\beta_1(t),\beta_2(t),\beta_3(t),\beta_4(t)).$$ Using $$ \begin{align} &\frac{d}{dt}| \beta_i(t) - \beta_j(t) |^{-1}=\frac{d}{dt}(| \beta_i(t) - \beta_j(t) |^2)^{-\frac{1}{2}}\\&=| \beta_i(t) - \beta_j(t)  |^{-3}\big(\langle \dot \beta_i(t), \beta_j(t)\rangle+\langle \beta_i(t), \dot \beta_j(t)\rangle\big), \tag{1} \end{align} $$ we get $$ \frac{d}{dt}E(\alpha(t))=\sum_{i<j}| \beta_i(t) - \beta_j(t)  |^{-3}\big(\langle \dot \beta_i(t), \beta_j(t)\rangle+\langle \beta_i(t), \dot \beta_j(t)\rangle\big). \tag{2} $$ In particular, denoting the length of the square's edge by $a$ , and assuming that $x_1,x_2,x_3,x_4$ are the square's vertices arranged in a cyclic order we have $$ dE_p(0,0,0,w)=\sum_{i=1}^3 | x_i - x_4 |^{-3}\langle x_i,w\rangle=a^{-3}  \langle x_1+x_3,w\rangle+(a\sqrt 2)^{-3} \langle x_2,w \rangle=0, $$ where we used the fact that $x_3=-x_1$ , and $x_2=-x_4$ , so $\langle x_2,w \rangle=-\langle x_4,w \rangle=0$ as $w \in T_{x_4}\S$ . Differentiating equation $(2)$ again, we get $$ \frac{d^2}{dt^2}| \beta_i - \beta_j |^{-1}=| \beta_i - \beta_j |^{-3}\bigg(3| \beta_i - \beta_j |^{-2}\big(\langle \dot \beta_i, \beta_j\rangle+\langle \beta_i, \dot \beta_j\rangle\big)^2+\langle \ddot \beta_i, \beta_j\rangle+\langle \beta_i, \ddot \beta_j\rangle+2 \langle \dot\beta_i, \dot \beta_j\rangle\bigg)\tag{2}. $$ Now, consider first all the $i<j$ such that $j=i+1 \text{mod} 4$ , i.e. $i-j$ is an edge of the square, or equivalently $d_{ij}=|x_i-x_j|=a$ . If we choose e.g. $j=4$ , then the two neighbors are $i=1,3$ , and so combining terms $1-4,3-4$ we get $$ \langle x_1+x_3, \ddot \beta_4(0)\rangle=0, $$ and similarly for the other two edges $1-2,2-3$ . Thus $$ \frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=a^{-3}\bigg( 3a^{-2}\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle\bigg)+A, $$ where $A$ is the part correspondong to $i-j$ equal $1-3$ , $2-4$ (the diagonals). Consider the pair $2-4$ : $\langle x_2,\dot \beta_{4}\rangle=-\langle x_4,\dot \beta_{4}\rangle=0$ , so the first summand $3(a\sqrt 2)^{-2}...$ vanishes. Thus we are left with $$A=(a\sqrt 2)^{-3}\bigg( \langle \ddot \beta_2, \beta_4\rangle+\langle \beta_2, \ddot \beta_4\rangle+2 \langle \dot\beta_2, \dot \beta_4\rangle \bigg). $$ Since $$ \langle \ddot \beta_2, \beta_4\rangle=-\langle \ddot \beta_2, \beta_2\rangle=|\dot \beta_2|^2, $$ we get $$ A=(a\sqrt 2)^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg). $$ Thus, up to a factor of $a^{-3}$ , we have $$ \frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3a^{-2}\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle+ $$ $$ \sqrt 2^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg). $$ Since $a=\sqrt 2$ , we get $$ \frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3/2\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle+ $$ $$ \sqrt 2^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg). $$ Is the last quantity $\ge 0$ ? (If I am not mistaken in my computations so far...).","Let Let be defined by where denotes the Euclidean distance in . Question: Let be the configuration of a planar square lying on the equator of . Is a local minimum of ? (Of course, it is not a strict minima since one can rotate). Here is an attempt: Let be a path in , . Consider the path Using we get In particular, denoting the length of the square's edge by , and assuming that are the square's vertices arranged in a cyclic order we have where we used the fact that , and , so as . Differentiating equation again, we get Now, consider first all the such that , i.e. is an edge of the square, or equivalently . If we choose e.g. , then the two neighbors are , and so combining terms we get and similarly for the other two edges . Thus where is the part correspondong to equal , (the diagonals). Consider the pair : , so the first summand vanishes. Thus we are left with Since we get Thus, up to a factor of , we have Since , we get Is the last quantity ? (If I am not mistaken in my computations so far...).","\newcommand{\S}{\mathbb{S}^2} M=\{(x_1,x_2,x_3,x_4) \in  \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \, |\,\, \text{ all the } x_i \, \text{ are distinct}\}  E:M \to \mathbb{R} E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|}, \| x_i - x_j \| \mathbb{R}^3 p:=(x_1,x_2,x_3,x_4) \S p E \beta_i(t) \mathbb{S}^2 \beta_i(0)=x_i, \dot \beta(0)=w_i \in T_{x_i}\S \alpha(t)=(\beta_1(t),\beta_2(t),\beta_3(t),\beta_4(t)). 
\begin{align}
&\frac{d}{dt}| \beta_i(t) - \beta_j(t) |^{-1}=\frac{d}{dt}(| \beta_i(t) - \beta_j(t) |^2)^{-\frac{1}{2}}\\&=| \beta_i(t) - \beta_j(t)  |^{-3}\big(\langle \dot \beta_i(t), \beta_j(t)\rangle+\langle \beta_i(t), \dot \beta_j(t)\rangle\big), \tag{1}
\end{align}
 
\frac{d}{dt}E(\alpha(t))=\sum_{i<j}| \beta_i(t) - \beta_j(t)  |^{-3}\big(\langle \dot \beta_i(t), \beta_j(t)\rangle+\langle \beta_i(t), \dot \beta_j(t)\rangle\big). \tag{2}
 a x_1,x_2,x_3,x_4 
dE_p(0,0,0,w)=\sum_{i=1}^3 | x_i - x_4 |^{-3}\langle x_i,w\rangle=a^{-3}  \langle x_1+x_3,w\rangle+(a\sqrt 2)^{-3}
\langle x_2,w \rangle=0,
 x_3=-x_1 x_2=-x_4 \langle x_2,w \rangle=-\langle x_4,w \rangle=0 w \in T_{x_4}\S (2) 
\frac{d^2}{dt^2}| \beta_i - \beta_j |^{-1}=| \beta_i - \beta_j |^{-3}\bigg(3| \beta_i - \beta_j |^{-2}\big(\langle \dot \beta_i, \beta_j\rangle+\langle \beta_i, \dot \beta_j\rangle\big)^2+\langle \ddot \beta_i, \beta_j\rangle+\langle \beta_i, \ddot \beta_j\rangle+2 \langle \dot\beta_i, \dot \beta_j\rangle\bigg)\tag{2}.
 i<j j=i+1 \text{mod} 4 i-j d_{ij}=|x_i-x_j|=a j=4 i=1,3 1-4,3-4 
\langle x_1+x_3, \ddot \beta_4(0)\rangle=0,
 1-2,2-3 
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=a^{-3}\bigg( 3a^{-2}\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle\bigg)+A,
 A i-j 1-3 2-4 2-4 \langle x_2,\dot \beta_{4}\rangle=-\langle x_4,\dot \beta_{4}\rangle=0 3(a\sqrt 2)^{-2}... A=(a\sqrt 2)^{-3}\bigg( \langle \ddot \beta_2, \beta_4\rangle+\langle \beta_2, \ddot \beta_4\rangle+2 \langle \dot\beta_2, \dot \beta_4\rangle \bigg).
 
\langle \ddot \beta_2, \beta_4\rangle=-\langle \ddot \beta_2, \beta_2\rangle=|\dot \beta_2|^2,
 
A=(a\sqrt 2)^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg).
 a^{-3} 
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3a^{-2}\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle+
 
\sqrt 2^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg).
 a=\sqrt 2 
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3/2\sum_{i=1}^4\big(\langle x_i,\dot \beta_{i+1}\rangle+\langle x_{i+1},\dot \beta_{i}\rangle\big)^2  +2\langle \dot \beta_{i}, \dot \beta_{i+1}\rangle+
 
\sqrt 2^{-3}\bigg( |\dot \beta_2+\dot \beta_4|^2+ |\dot \beta_1+\dot \beta_3|^2 \bigg).
 \ge 0","['multivariable-calculus', 'differential-geometry', 'optimization', 'maxima-minima', 'symmetry']"
67,How to find the average distance between two points in a square of length $1$ by using Average = sum of all distances / number of pair of points?,How to find the average distance between two points in a square of length  by using Average = sum of all distances / number of pair of points?,1,"Lets consider a line of length $l$ , let $X_1$ and $Y_1$ be two points and their distance from a common end point of the line be $x$ and $y$ . Then, the distance between $X_1$ and $Y_1$ is $$ \vert x-y \vert$$ Now I will consider a function $$ f(x,y)=\vert x-y\vert$$ If I find the volume under the graph by the integral $$ \int_0^l \int_0^l \vert x-y \vert  dxdxy = \frac {l^3}{3}$$ My interpretation of this $\frac {l^3}{3} $ is that it is the sum of all possible lengths that can be marked by the two points $X_1$ and $Y_1$ . Now, to find the number of points by which I mean the number of pairs of ( $X_1,Y_1$ ) I find the base area of the graph $$\int_0^l\int_0^l dxdy = l^2$$ I think $l^2$ is the infinite sum of the number of points. Dividing these two also does give the correct result of the average distance between two points on a line $\frac l3$ . $$ $$ My idea is to consider a line that is formed by two points on the perimeter the, the sum of all the distances that could be marked by two points will be $\frac {l^3}{3}$ so if I integrate this expression for the entire perimeter I would get all the possible distance that could be marked by two points on a square. For this I divide the perimeter into $6$ pairs, $2$ pairs of opposite sides and $4$ pairs of adjacent sides. $$ $$ Its fairly simple to arrive at the expression for their lengths. Now, I integrate $\frac {l^3}{3}$ for the perimeter $$\int_0^1\int_0^1 (x^2+y^2)^\frac 32dxdy= 0.20906$$ $$\int_0^1\int_0^1 ((x-y)^2+1)^\frac 32dxdy= 0.42438$$ Now, similarly I integrate $l^2$ over the perimter to get the number of points $$\int_0^1\int_0^1 (x^2+y^2)dxdy= \frac 23$$ $$\int_0^1\int_0^1 ((x-y)^2+1)dxdy= \frac 76$$ Now to calculate the average $$ \frac {0.20906*4+0.42438*2}{\frac 23 *4+\frac 76 *2} =0.337 $$ I have described my entire process and I do  not know where I am wrong. The correct answer is close to 0.52. I would appreciate any help on this.","Lets consider a line of length , let and be two points and their distance from a common end point of the line be and . Then, the distance between and is Now I will consider a function If I find the volume under the graph by the integral My interpretation of this is that it is the sum of all possible lengths that can be marked by the two points and . Now, to find the number of points by which I mean the number of pairs of ( ) I find the base area of the graph I think is the infinite sum of the number of points. Dividing these two also does give the correct result of the average distance between two points on a line . My idea is to consider a line that is formed by two points on the perimeter the, the sum of all the distances that could be marked by two points will be so if I integrate this expression for the entire perimeter I would get all the possible distance that could be marked by two points on a square. For this I divide the perimeter into pairs, pairs of opposite sides and pairs of adjacent sides. Its fairly simple to arrive at the expression for their lengths. Now, I integrate for the perimeter Now, similarly I integrate over the perimter to get the number of points Now to calculate the average I have described my entire process and I do  not know where I am wrong. The correct answer is close to 0.52. I would appreciate any help on this.","l X_1 Y_1 x y X_1 Y_1  \vert x-y \vert  f(x,y)=\vert x-y\vert  \int_0^l \int_0^l \vert x-y \vert  dxdxy = \frac {l^3}{3} \frac {l^3}{3}  X_1 Y_1 X_1,Y_1 \int_0^l\int_0^l dxdy = l^2 l^2 \frac l3   \frac {l^3}{3} 6 2 4   \frac {l^3}{3} \int_0^1\int_0^1 (x^2+y^2)^\frac 32dxdy= 0.20906 \int_0^1\int_0^1 ((x-y)^2+1)^\frac 32dxdy= 0.42438 l^2 \int_0^1\int_0^1 (x^2+y^2)dxdy= \frac 23 \int_0^1\int_0^1 ((x-y)^2+1)dxdy= \frac 76  \frac {0.20906*4+0.42438*2}{\frac 23 *4+\frac 76 *2} =0.337 ","['calculus', 'multivariable-calculus']"
68,Is the function $x^2\sin(y/x^2)$ differentiable?,Is the function  differentiable?,x^2\sin(y/x^2),"Let $f:\mathbb{R}^2 \to \mathbb{R}$ defined by: $f(x,y) = \begin{cases} \left( x^2 \right) \sin \left(\frac{y}{x^2} \right) & \text { if } x \neq 0\\ 0 & \text{ if } x = 0 \end{cases}$ Is the function $f$ differentiable or not? I don’t know whether my solution is right, so I’ll post it here. My solution: If $x \neq 0$ , than $f(x,y) = x^2  \sin(y/x^2)$ is differentiable as it is the product of differentiable functions. Now for $x=0$ , let's consider the limit: $$\begin{align*} \lim_{h \to 0} \frac{f(h,y) - f(0,y)}{h} &= \lim_{h \to 0} h^2\sin(\frac{y}{h^2})\\ &= \lim_{h \to 0} h^2\sin(yh^{-2})\\ &= \lim_{h \to 0} h^{2} = 0  \end{align*}$$ Therefore the function $f$ is differentiable. MY NEW SOLUTION : I first calculated the partial derivatives with respect to x and y with limits at the point $(0,y)$ . $\frac{\partial f}{\partial x}(0,y)= \lim_{h \to 0} \frac{f(0+h,y)-f(0,y)}{h} = 0$ $\frac{\partial f}{\partial y}(0,y) = \lim_{k \to 0} \frac{f(0,y+k)-f(0,y)}{k} = \lim_{k \to 0} \frac{0-0}{k} = 0$ Consequently, since $f(x, 0) = 0$ and the only candidate for a derivative matrix is $f'(x, 0) = (0, 0)$ , we have: $E(h,k):= f(0+h,y+k) = f(h,y+k) = h^2 \sin(\frac{y+k}{h^2})$ Then: $0 \leq \frac{|E(h,k)|}{||(h,k)||} \leq \frac{h^2\sin(\frac{y+k}{h^2})}{\sqrt{h^2+k^2}} \leq |h| \cdot \frac{|h|}{\sqrt{h^2+k^2}} \leq |h|$ This converge to zero when $(h,k) \to 0$ , therefore $f$ is differentiable at the point $(0,y)$ .","Let defined by: Is the function differentiable or not? I don’t know whether my solution is right, so I’ll post it here. My solution: If , than is differentiable as it is the product of differentiable functions. Now for , let's consider the limit: Therefore the function is differentiable. MY NEW SOLUTION : I first calculated the partial derivatives with respect to x and y with limits at the point . Consequently, since and the only candidate for a derivative matrix is , we have: Then: This converge to zero when , therefore is differentiable at the point .","f:\mathbb{R}^2 \to \mathbb{R} f(x,y) = \begin{cases} \left( x^2 \right) \sin \left(\frac{y}{x^2} \right) & \text { if } x \neq 0\\ 0 & \text{ if } x = 0 \end{cases} f x \neq 0 f(x,y) = x^2  \sin(y/x^2) x=0 \begin{align*}
\lim_{h \to 0} \frac{f(h,y) - f(0,y)}{h} &= \lim_{h \to 0} h^2\sin(\frac{y}{h^2})\\
&= \lim_{h \to 0} h^2\sin(yh^{-2})\\
&= \lim_{h \to 0} h^{2} = 0 
\end{align*} f (0,y) \frac{\partial f}{\partial x}(0,y)= \lim_{h \to 0} \frac{f(0+h,y)-f(0,y)}{h} = 0 \frac{\partial f}{\partial y}(0,y) = \lim_{k \to 0} \frac{f(0,y+k)-f(0,y)}{k} = \lim_{k \to 0} \frac{0-0}{k} = 0 f(x, 0) = 0 f'(x, 0) = (0, 0) E(h,k):= f(0+h,y+k) = f(h,y+k) = h^2 \sin(\frac{y+k}{h^2}) 0 \leq \frac{|E(h,k)|}{||(h,k)||} \leq \frac{h^2\sin(\frac{y+k}{h^2})}{\sqrt{h^2+k^2}} \leq |h| \cdot \frac{|h|}{\sqrt{h^2+k^2}} \leq |h| (h,k) \to 0 f (0,y)",['multivariable-calculus']
69,Is there any hope to prove that $g(x)>-4$ if $f(x)<0$?,Is there any hope to prove that  if ?,g(x)>-4 f(x)<0,"I have these two functions for $x>0$ , $\beta>0$ and $\alpha$ (all reals) $$ f(x)= \frac{\alpha \; \sin (\beta \; x)}x+4    \cos (\beta\;  x) ,\qquad\qquad\qquad\qquad\qquad\qquad\\ g(x)=\frac{\alpha  \cos (\beta  \;x)}{\beta \; x^2}+\frac{\alpha  \cos (2 \beta \; x)}{\beta \; x^2}-\frac{\alpha\; \cot  (\frac{\beta \; x}{2})}x    -\frac{4\;\sin (2 \beta \; x)}{\beta\;x}    $$ Numerically, I am sure that if $f(x)<0$ , then $g(x)>-4$ (an example is attached); any hints to prove this analytically?","I have these two functions for , and (all reals) Numerically, I am sure that if , then (an example is attached); any hints to prove this analytically?","x>0 \beta>0 \alpha  f(x)= \frac{\alpha \; \sin (\beta \; x)}x+4    \cos (\beta\;  x) ,\qquad\qquad\qquad\qquad\qquad\qquad\\
g(x)=\frac{\alpha  \cos (\beta  \;x)}{\beta \; x^2}+\frac{\alpha  \cos (2 \beta \; x)}{\beta \; x^2}-\frac{\alpha\; \cot  (\frac{\beta \; x}{2})}x    -\frac{4\;\sin (2 \beta \; x)}{\beta\;x}   
 f(x)<0 g(x)>-4","['real-analysis', 'calculus', 'multivariable-calculus', 'solution-verification', 'maxima-minima']"
70,Determining the sign of the directional derivative and the partial derivatives on a surface,Determining the sign of the directional derivative and the partial derivatives on a surface,,"This is the question: The solution says: a) The surface is given by $z=f(x,y)$ If we see in graph as we move towards $\vec{u}=<5,0>$ , $z$ increases, thus $D_{\vec{u}}f(2,-4)$ is positive. I'm not sure what it means to 'move toward <5,0>'. Does that just mean move from the point (2,-4) towards the point $(5,0)$ in the xy plane? And how do we know that z increases? In a contour plot we can normally see numbers but in this case I don't see anything. Similarly, for c) the solution says: z increases rapidly, thus $D_{\vec{u}}f(2,-4)$ is positive. Again, how do I know it's increasing 'rapidly'? Also the solution for d) is: At $f(2,-4)$ z is neither minima nor maxima as we can see in the graph thus the answer is NEI. How do I know that it's not a minima or maxima? And what does that mean in terms if $f_{xx}$ ? I.e. how do I solve this versus finding the answer for $f_{yy}$ . The only thing the solution says for e) is: Same reason as d) Lastly, Are these the correct answers: a) $D_{\vec{u}}f(2,-4)$ is positive b) $D_{\vec{u}}f(2,-4)$ is positive c) $D_{\vec{u}}f(2,-4)$ is positive d) NEI e) NEI","This is the question: The solution says: a) The surface is given by If we see in graph as we move towards , increases, thus is positive. I'm not sure what it means to 'move toward <5,0>'. Does that just mean move from the point (2,-4) towards the point in the xy plane? And how do we know that z increases? In a contour plot we can normally see numbers but in this case I don't see anything. Similarly, for c) the solution says: z increases rapidly, thus is positive. Again, how do I know it's increasing 'rapidly'? Also the solution for d) is: At z is neither minima nor maxima as we can see in the graph thus the answer is NEI. How do I know that it's not a minima or maxima? And what does that mean in terms if ? I.e. how do I solve this versus finding the answer for . The only thing the solution says for e) is: Same reason as d) Lastly, Are these the correct answers: a) is positive b) is positive c) is positive d) NEI e) NEI","z=f(x,y) \vec{u}=<5,0> z D_{\vec{u}}f(2,-4) (5,0) D_{\vec{u}}f(2,-4) f(2,-4) f_{xx} f_{yy} D_{\vec{u}}f(2,-4) D_{\vec{u}}f(2,-4) D_{\vec{u}}f(2,-4)","['multivariable-calculus', 'partial-derivative']"
71,"Is the tangent plane to the surface $x^4+y^4-z^2=0$ at $(0,0,0)$ undefined?",Is the tangent plane to the surface  at  undefined?,"x^4+y^4-z^2=0 (0,0,0)","In calculus, the tangent plane to a surface $f(x,y,z)=0$ at $(x_0,y_0,z_0)$ is defined as the plane passing through $(x_0,y_0,z_0)$ with the normal vector $\mathrm{grad} f(x_0,y_0,z_0)$ . According to this description, the tangent plane to the surface $x^4+y^4-z^2=0$ at $(0,0,0)$ should be undefined as the gradient of $x^4+y^4-z^2$ is the zero vector at $(0,0,0)$ . However, intuitively speaking, it seems that the plane $z=0$ should be the tangent plane to this surface at $(0,0,0)$ : So in this case, is the plane $z=0$ the tangent plane to the surface $x^4+y^4-z^2=0$ at $(0,0,0)$ , or is it undefined since the gradient at $(0,0,0)$ is the zero vector?","In calculus, the tangent plane to a surface at is defined as the plane passing through with the normal vector . According to this description, the tangent plane to the surface at should be undefined as the gradient of is the zero vector at . However, intuitively speaking, it seems that the plane should be the tangent plane to this surface at : So in this case, is the plane the tangent plane to the surface at , or is it undefined since the gradient at is the zero vector?","f(x,y,z)=0 (x_0,y_0,z_0) (x_0,y_0,z_0) \mathrm{grad} f(x_0,y_0,z_0) x^4+y^4-z^2=0 (0,0,0) x^4+y^4-z^2 (0,0,0) z=0 (0,0,0) z=0 x^4+y^4-z^2=0 (0,0,0) (0,0,0)","['calculus', 'multivariable-calculus', 'tangent-spaces']"
72,Trouble with a triple integral on a region bounded by a sphere and two planes,Trouble with a triple integral on a region bounded by a sphere and two planes,,"I would like to compute the integral $\int_A zdzdydx,$ where $A$ is the region bounded by the sphere $x^2+y^2+z^2=R^2,$ plane $\frac{x}a+\frac{y}b=1$ and coordinate planes (which doesn't contain the origin on its boundary and is in the first quadrant). I considered switching to either spherical or cylindrical coordinates, but I don't see any symmetry or pattern. In spherical coordinates, I tried expressing the lower bound for radius $r$ in terms of $\theta\in[0,\pi/2]$ $$r(\theta, \varphi)=\frac{ab}{a\sin\varphi\sin\theta+b\cos\varphi\sin\theta}$$ and the integral becomes \begin{aligned}&\color{white}=\int_0^{\pi/2}\int_0^{\pi/2}\int_{\frac{ab}{a\sin\theta\sin\varphi+b\sin\theta\cos\varphi}}^Rr^2\sin\theta r\cos\theta drd\theta d\varphi\\&=\int_0^{\pi/2}\int_0^{\pi/2}\sin\theta\int_{\frac{ab}{a\sin\theta\sin\varphi+b\sin\theta\cos\varphi}}^Rr^3drd\theta d\varphi\end{aligned} In cylindrical coordinates $$r(\varphi)=\frac{ab}{a\sin\varphi+b\cos\varphi}$$ and the integral is $$\int_0^{\pi/2}\int_{\frac{ab}{a\sin\varphi+b\cos\varphi}}^R\int_0^{\sqrt{R^2-r^2}}zrdzdrd\varphi$$ However, I have to deal with a powers of $a\sin\theta\sin\varphi+b\sin\theta\cos\varphi$ and $a\sin\varphi+b\cos\varphi$ too early, and if I swap the order of integration, I need to express the angles in terms of $r$ which seems worse. I saw the substitution in this answer, but it isn't so smooth here and I've seen the reduction formula . How should one attack this task?","I would like to compute the integral where is the region bounded by the sphere plane and coordinate planes (which doesn't contain the origin on its boundary and is in the first quadrant). I considered switching to either spherical or cylindrical coordinates, but I don't see any symmetry or pattern. In spherical coordinates, I tried expressing the lower bound for radius in terms of and the integral becomes In cylindrical coordinates and the integral is However, I have to deal with a powers of and too early, and if I swap the order of integration, I need to express the angles in terms of which seems worse. I saw the substitution in this answer, but it isn't so smooth here and I've seen the reduction formula . How should one attack this task?","\int_A zdzdydx, A x^2+y^2+z^2=R^2, \frac{x}a+\frac{y}b=1 r \theta\in[0,\pi/2] r(\theta, \varphi)=\frac{ab}{a\sin\varphi\sin\theta+b\cos\varphi\sin\theta} \begin{aligned}&\color{white}=\int_0^{\pi/2}\int_0^{\pi/2}\int_{\frac{ab}{a\sin\theta\sin\varphi+b\sin\theta\cos\varphi}}^Rr^2\sin\theta r\cos\theta drd\theta d\varphi\\&=\int_0^{\pi/2}\int_0^{\pi/2}\sin\theta\int_{\frac{ab}{a\sin\theta\sin\varphi+b\sin\theta\cos\varphi}}^Rr^3drd\theta d\varphi\end{aligned} r(\varphi)=\frac{ab}{a\sin\varphi+b\cos\varphi} \int_0^{\pi/2}\int_{\frac{ab}{a\sin\varphi+b\cos\varphi}}^R\int_0^{\sqrt{R^2-r^2}}zrdzdrd\varphi a\sin\theta\sin\varphi+b\sin\theta\cos\varphi a\sin\varphi+b\cos\varphi r","['real-analysis', 'integration', 'multivariable-calculus', 'multiple-integral']"
73,"Question on an improper integral of the form $\int_0^af(x,a)dx$",Question on an improper integral of the form,"\int_0^af(x,a)dx","The question I'm about to ask isn't covered in our lectures, so pardon my ignorance. Suppose $f:(0,a]\times [c,d]$ . How is uniform convergence of the improper integral $\int_0^af(x,a)dx$ defined? Just to give an insight of what I've (hope so) learnt so far, I'm going to write down some definitions and results (and proofs) I think are relevant for functions $f:[a,+\infty)\times [c,d]$ that I found in the script by prof. Šime Ungar from 2004. It might be available here . $\underline{\boldsymbol{\text{ definition 1: }}}$ An improper integral $\int_a^\infty f(x)dx$ is defined as $\lim_{b\to\infty}\int_a^b f(x)dx$ under the condition that $f$ is integrable on $[a,b],\forall b>a$ and that the limit exists. In this case we also say the improper integral $\int_a^\infty f(x)dx$ converges. That is equivalent to the following condition: $f$ is integrable on $[a,b],\forall b>a$ and $$(\forall\varepsilon>0)(\exists a_0>a), \left|\int_b^c f(x)dx\right|<\varepsilon,\forall c>b>a_0.$$ Now, suppose we have a function $f:[a,+\infty)\times S\to\Bbb R,$ where $S\subseteq\Bbb R$ is an arbitrary set. Here we can observe convergence of the integral $\displaystyle\int_a^b f(x,y)dx,\forall y\in S.$ In this situation, the following definition makes sense: $\underline{\boldsymbol{\text{definition 2:}}}$ We say the improper integral $\int_a^\infty f(x,y)dx$ converges uniformly on $S$ if the integral converges $\forall y\in S$ and if $$\lim_{b\to\infty}\left(\sup_{y\in S}\left|\int_b^\infty f(x,y)dx\right|\right)=0.$$ $\underline{\boldsymbol{\text{result 1:}}}$ Suppose $\int_a^b f(x,y)dx$ exists $\forall b>a$ and $\color{red}{\forall y\in S}.$ Then, the improper integral $\int_a^\infty f(x,y)dx$ converges uniformly on $S$ if and only if $$(\forall\varepsilon>0)(\exists a_0>a), \color{red}{\sup_{y\in S}}\left|\int_b^c f(x,y)dx\right|<\varepsilon,\forall c>b\ge a_0.$$ $\boldsymbol{\text{ proof: }}$ $\boxed{\Rightarrow}$ Necessity of the conditions in the theorem is obvious. $\boxed{\Leftarrow}$ Let's prove thr sufficiency. From the conditions of the theorem, it first follows that, $\forall y\in S,$ the integral $\int_a^\infty f(x,y)dx$ converges. Furthermore, for $y\in S,$ since $\left|\int_b^c\right|<\varepsilon$ whenever $c>b>a_0,$ letting $c\to\infty,$ we have $\left|\int_b^\infty f(x,y)dx\right|\color{red}\le\varepsilon.$ But, as this holds $\color{red}{\forall y\in S},$ it is also true that $\color{red}{\sup_{y\in S}}\left|\int_b^\infty f(x,y)dx\right|\le\varepsilon$ and the claim follows. Something I believe is important (I'll write down my motivation in the end): $\underline{\boldsymbol{\text{Weierstrass M-test:}}}$ Suppose $\int_a^b f(x,y)dx$ exists $\forall b>a.$ If there is a function $M:[a,+\infty)\to\Bbb R$ s. t. $|f(x,y)|\le M(x),\forall x\in[a,+\infty)$ and $\forall y\in S$ and if $\int_a^\infty M(x)dx$ converges, then $\int_a^\infty f(x,y)dx$ converges (absolutely and) uniformly on $S$ . I've gone through the proof of the discrete version . If needed, I'll analyze this more carefully. Last result with the proof, I promise. $\underline{\boldsymbol{\text{result 2: }}}$ Suppose $f:[a,+\infty)\times [c,d]$ is continuous and that the following statements are true: $\exists y\in [c,d]$ s. t. the improper integral $\int_a^\infty f(x,y)dx$ converges $\partial_2f(x,y)$ exists and is continuous on $[a,+\infty)\times [c,d]$ $\int_a^\infty\partial_2f(x,y)dx$ converges uniformly on $[c,d].$ Then, the improper integral $\int_a^\infty f(x,y)dx$ exists and converges uniformly on $[c,d],$ and the function $F(y):=\int_a^\infty f(x,y)dx$ is differentiable on $[c,d]$ and $\color{purple}{F'(y)=\int_a^\infty\partial_2 f(x,y)dx}\forall y\in [c,d].$ $\boldsymbol{\text{ proof: }}$ Suppose that $\int_a^\infty f(x,y_0)dx$ converges. Apart from that, we also know that $\forall b>a$ and $\forall y\in [c,d]$ the integral $\int_a^b f(x,y)dx.$ Because of $2,\forall\varepsilon>0,\exists a_0$ s. t. $a_0<b<g\implies \left|\int_b^g\partial_2f(x,t)dx\right|<\varepsilon,\forall t\in [c,d]$ and at the same time $\left|\int_b^g f(x,y_0)dx\right|<\varepsilon.$ $$\begin{aligned}\left|\int_b^g(f(x,y)-f(x,y_0))dx\right|&=\left|\int_b^g\int_{y_0}^y\partial_2f(x,t)dtdx\right|\\&=\left|\int_{y_0}^y\int_b^g\partial_2f(x,t)dxdt\right|\\&\le\int_{y_0}^y\left|\int_b^g\partial_2f(x,t)dx\right|dt\\&\le (d-c)\varepsilon,\end{aligned}$$ whenever $a_0<b<g.$ $$\begin{aligned}\implies\left|\int_b^gf(x,y)dx\right|&\le\left|\int_b^g(f(x,y)-f(x,y_0))dx\right|+\left|\int_b^g f(x,y_0)dx\right|\\&\le(d-c)\varepsilon+\varepsilon\\&=(1+d-c)\varepsilon,\forall y\in [c,d]\end{aligned}$$ it follows that $\int_a^\infty f(x,y)dx$ exists and converges uniformly on $[c,d].$ Now, let's define a function $\color{purple}{G(y):=\int_a^\infty\partial_2f(x,y)dx}.$ Then $G$ is continuous (a result proven in the script priorly). It remains to notice that $$\int_c^ydt\int_a^\infty\partial_2f(x,t)dx=\int_a^\infty dx\int_c^y\partial_2f(x,t)dt.$$ In all the results, we used the fact the set was unbounded. In old materials, I've come across the following task: Prove that the  function $F(a)=\int_0^{1/a}\frac{e^{ax}-1}xdx$ is constant. I might be wrong, but I tried to use the substitution $x=\frac1t$ in order to get to an unbounded interval and then use the results above. This got a bit messy, so I was wondering if it makes sense to even consider the $\boldsymbol{\text{ result 2}}$ and write something as $F'(a)=\int_0^{1/a}\partial_2 f(x,a)dx.$ I'm primarily interested in justification for that. I apologize for the length of my post and thank you for reading in advance!","The question I'm about to ask isn't covered in our lectures, so pardon my ignorance. Suppose . How is uniform convergence of the improper integral defined? Just to give an insight of what I've (hope so) learnt so far, I'm going to write down some definitions and results (and proofs) I think are relevant for functions that I found in the script by prof. Šime Ungar from 2004. It might be available here . An improper integral is defined as under the condition that is integrable on and that the limit exists. In this case we also say the improper integral converges. That is equivalent to the following condition: is integrable on and Now, suppose we have a function where is an arbitrary set. Here we can observe convergence of the integral In this situation, the following definition makes sense: We say the improper integral converges uniformly on if the integral converges and if Suppose exists and Then, the improper integral converges uniformly on if and only if Necessity of the conditions in the theorem is obvious. Let's prove thr sufficiency. From the conditions of the theorem, it first follows that, the integral converges. Furthermore, for since whenever letting we have But, as this holds it is also true that and the claim follows. Something I believe is important (I'll write down my motivation in the end): Suppose exists If there is a function s. t. and and if converges, then converges (absolutely and) uniformly on . I've gone through the proof of the discrete version . If needed, I'll analyze this more carefully. Last result with the proof, I promise. Suppose is continuous and that the following statements are true: s. t. the improper integral converges exists and is continuous on converges uniformly on Then, the improper integral exists and converges uniformly on and the function is differentiable on and Suppose that converges. Apart from that, we also know that and the integral Because of s. t. and at the same time whenever it follows that exists and converges uniformly on Now, let's define a function Then is continuous (a result proven in the script priorly). It remains to notice that In all the results, we used the fact the set was unbounded. In old materials, I've come across the following task: Prove that the  function is constant. I might be wrong, but I tried to use the substitution in order to get to an unbounded interval and then use the results above. This got a bit messy, so I was wondering if it makes sense to even consider the and write something as I'm primarily interested in justification for that. I apologize for the length of my post and thank you for reading in advance!","f:(0,a]\times [c,d] \int_0^af(x,a)dx f:[a,+\infty)\times [c,d] \underline{\boldsymbol{\text{ definition 1: }}} \int_a^\infty f(x)dx \lim_{b\to\infty}\int_a^b f(x)dx f [a,b],\forall b>a \int_a^\infty f(x)dx f [a,b],\forall b>a (\forall\varepsilon>0)(\exists a_0>a), \left|\int_b^c f(x)dx\right|<\varepsilon,\forall c>b>a_0. f:[a,+\infty)\times S\to\Bbb R, S\subseteq\Bbb R \displaystyle\int_a^b f(x,y)dx,\forall y\in S. \underline{\boldsymbol{\text{definition 2:}}} \int_a^\infty f(x,y)dx S \forall y\in S \lim_{b\to\infty}\left(\sup_{y\in S}\left|\int_b^\infty f(x,y)dx\right|\right)=0. \underline{\boldsymbol{\text{result 1:}}} \int_a^b f(x,y)dx \forall b>a \color{red}{\forall y\in S}. \int_a^\infty f(x,y)dx S (\forall\varepsilon>0)(\exists a_0>a), \color{red}{\sup_{y\in S}}\left|\int_b^c f(x,y)dx\right|<\varepsilon,\forall c>b\ge a_0. \boldsymbol{\text{ proof: }} \boxed{\Rightarrow} \boxed{\Leftarrow} \forall y\in S, \int_a^\infty f(x,y)dx y\in S, \left|\int_b^c\right|<\varepsilon c>b>a_0, c\to\infty, \left|\int_b^\infty f(x,y)dx\right|\color{red}\le\varepsilon. \color{red}{\forall y\in S}, \color{red}{\sup_{y\in S}}\left|\int_b^\infty f(x,y)dx\right|\le\varepsilon \underline{\boldsymbol{\text{Weierstrass M-test:}}} \int_a^b f(x,y)dx \forall b>a. M:[a,+\infty)\to\Bbb R |f(x,y)|\le M(x),\forall x\in[a,+\infty) \forall y\in S \int_a^\infty M(x)dx \int_a^\infty f(x,y)dx S \underline{\boldsymbol{\text{result 2: }}} f:[a,+\infty)\times [c,d] \exists y\in [c,d] \int_a^\infty f(x,y)dx \partial_2f(x,y) [a,+\infty)\times [c,d] \int_a^\infty\partial_2f(x,y)dx [c,d]. \int_a^\infty f(x,y)dx [c,d], F(y):=\int_a^\infty f(x,y)dx [c,d] \color{purple}{F'(y)=\int_a^\infty\partial_2 f(x,y)dx}\forall y\in [c,d]. \boldsymbol{\text{ proof: }} \int_a^\infty f(x,y_0)dx \forall b>a \forall y\in [c,d] \int_a^b f(x,y)dx. 2,\forall\varepsilon>0,\exists a_0 a_0<b<g\implies \left|\int_b^g\partial_2f(x,t)dx\right|<\varepsilon,\forall t\in [c,d] \left|\int_b^g f(x,y_0)dx\right|<\varepsilon. \begin{aligned}\left|\int_b^g(f(x,y)-f(x,y_0))dx\right|&=\left|\int_b^g\int_{y_0}^y\partial_2f(x,t)dtdx\right|\\&=\left|\int_{y_0}^y\int_b^g\partial_2f(x,t)dxdt\right|\\&\le\int_{y_0}^y\left|\int_b^g\partial_2f(x,t)dx\right|dt\\&\le (d-c)\varepsilon,\end{aligned} a_0<b<g. \begin{aligned}\implies\left|\int_b^gf(x,y)dx\right|&\le\left|\int_b^g(f(x,y)-f(x,y_0))dx\right|+\left|\int_b^g f(x,y_0)dx\right|\\&\le(d-c)\varepsilon+\varepsilon\\&=(1+d-c)\varepsilon,\forall y\in [c,d]\end{aligned} \int_a^\infty f(x,y)dx [c,d]. \color{purple}{G(y):=\int_a^\infty\partial_2f(x,y)dx}. G \int_c^ydt\int_a^\infty\partial_2f(x,t)dx=\int_a^\infty dx\int_c^y\partial_2f(x,t)dt. F(a)=\int_0^{1/a}\frac{e^{ax}-1}xdx x=\frac1t \boldsymbol{\text{ result 2}} F'(a)=\int_0^{1/a}\partial_2 f(x,a)dx.","['real-analysis', 'multivariable-calculus', 'improper-integrals']"
74,How to evaluate $\int^{}_{c} y dx + z dy + x dz$,How to evaluate,\int^{}_{c} y dx + z dy + x dz,"How to calculate $$\int^{}_{c} y dx + z dy + x dz$$ Where C consists of the segment $C_{1}$ joining $(2,0,0)$ to $(3,4,5)$ followed by the vertical line segment C2 from $(3,4,5)$ to $(3,4,0)$ I'm thinking of parameterizing C1, starting parameterization $(2,0,0)$ to $(3,4,5)$ : Let $$A(2,0,0)$$ and $$B  (3,4,5)$$ Parameterization formula: $$(x,y,z)=B.t+(1-t).A$$ $$(x,y,z)=(3,4,5).t+(1-t).(2,0,0)$$ $$(x,y,z)=(3t,4t,5t)+(2-2t,0,0)$$ $$(x,y,z)=(2+t,4t,5t)$$ So our vector $$r(t)=(2+t)\hat{i}+(4t)\hat{j}+(5t)\hat{k}$$ $$r'(t)=(1,4,5)$$ $$ |r'(t) |=\sqrt{1^{2}+4^{2}+5^{2}}=\sqrt{42}$$ I don't know how to continue.the function $f(x,y,z)$ is $x+y+z$ ? then I would apply Line integral","How to calculate Where C consists of the segment joining to followed by the vertical line segment C2 from to I'm thinking of parameterizing C1, starting parameterization to : Let and Parameterization formula: So our vector I don't know how to continue.the function is ? then I would apply Line integral","\int^{}_{c} y dx + z dy + x dz C_{1} (2,0,0) (3,4,5) (3,4,5) (3,4,0) (2,0,0) (3,4,5) A(2,0,0) B 
(3,4,5) (x,y,z)=B.t+(1-t).A (x,y,z)=(3,4,5).t+(1-t).(2,0,0) (x,y,z)=(3t,4t,5t)+(2-2t,0,0) (x,y,z)=(2+t,4t,5t) r(t)=(2+t)\hat{i}+(4t)\hat{j}+(5t)\hat{k} r'(t)=(1,4,5)  |r'(t) |=\sqrt{1^{2}+4^{2}+5^{2}}=\sqrt{42} f(x,y,z) x+y+z","['integration', 'multivariable-calculus']"
75,"Calculate area bounded by $\frac{x^{2}}{16}+\frac{y^{2}}{25}=1$, $y=\pm 3$ and $y=x+4$","Calculate area bounded by ,  and",\frac{x^{2}}{16}+\frac{y^{2}}{25}=1 y=\pm 3 y=x+4,"In my exam I was asked to find out the area bounded by the ellipse $x^{2}/16+y^{2}/25=1$ , and the lines $y=\pm 3$ and $y=x+4$ . Here is what I did. $$\begin{aligned}A=\iint_{R}\mathrm dA&=\int_{0}^{3}\int_{y-4}^{0}\mathrm dx\mathrm dy+3\int_{0}^{3}\int_{0}^{\frac{4}{5}\sqrt{25-y^{2}}}\mathrm dx\mathrm dy \\ &=\int_{0}^{3}(4-y)\mathrm dy+\int_{0}^{3}\frac{12}{5}\sqrt{25-y^{2}}\mathrm dy \\ &=\frac{15}{2}+\frac{72}{5}+30\arcsin\left(\frac{3}{5}\right)\end{aligned}$$ Could someone check whether I have done it correctly. Any inputs are appreciated. Thanks.","In my exam I was asked to find out the area bounded by the ellipse , and the lines and . Here is what I did. Could someone check whether I have done it correctly. Any inputs are appreciated. Thanks.",x^{2}/16+y^{2}/25=1 y=\pm 3 y=x+4 \begin{aligned}A=\iint_{R}\mathrm dA&=\int_{0}^{3}\int_{y-4}^{0}\mathrm dx\mathrm dy+3\int_{0}^{3}\int_{0}^{\frac{4}{5}\sqrt{25-y^{2}}}\mathrm dx\mathrm dy \\ &=\int_{0}^{3}(4-y)\mathrm dy+\int_{0}^{3}\frac{12}{5}\sqrt{25-y^{2}}\mathrm dy \\ &=\frac{15}{2}+\frac{72}{5}+30\arcsin\left(\frac{3}{5}\right)\end{aligned},"['integration', 'multivariable-calculus']"
76,Directional derivative definition versus gradient,Directional derivative definition versus gradient,,"Given the following scalar field $$f(x,y) = \begin{cases}            \frac{y^3}{x^2+y^2} & (x,y)\ne(0,0) \\             0 & (x,y)=(0,0)            \end{cases}$$ find its directional derivative in the direction of $(3,2)$ at the point $(0,0)$ . First way I wanted to do this was with the gradient. However, as neither partial derivative exists at $(0,0)$ , I need to use their limit form instead: $$\frac{\partial}{\partial x}f(x,y)=\lim_{h\to 0}\frac{f(h,0)-f(0)}{h}=\lim_{h\to 0}\frac{f(h,0)}{h}=\lim_{h\to 0}\frac{0}{h^3}=0$$ $$\frac{\partial}{\partial y}f(x,y)=\lim_{k\to 0}\frac{f(0,k)-f(0)}{k}=\lim_{k\to 0}\frac{f(0,k)}{k}=\lim_{k\to 0}\frac{k^3}{k^3}=1$$ Yielding my gradient at $(0,0)$ : $$\nabla f = \vec{(0, 1)}$$ Using this to calculate the directional derivative with $u = (3,2)$ , I get $$\nabla_{u} f(0,0)=\nabla f_{(0,0)}\frac{u}{|u|}=\frac{1}{\sqrt{13}}(3,2)\cdot{}(0,1)=\frac{2}{\sqrt{13}}$$ However, if I use the directional derivative definition here: $$\lim_{h\to 0}\frac{f(\vec{a}+h\vec{u})-f(\vec{a})}{h|u|}=\lim_{h\to 0}\frac{f(h(3,2))}{h\sqrt{13}}=\lim_{h\to 0}\frac{\frac{(2h)^3}{(3h)^2+(2h)^2}}{h\sqrt{13}}=\frac{8}{13\sqrt{13}}$$ which was the gradient answer but cubed. What should I expect from these answers? And what have I done wrong that yields these different but similar answers?","Given the following scalar field find its directional derivative in the direction of at the point . First way I wanted to do this was with the gradient. However, as neither partial derivative exists at , I need to use their limit form instead: Yielding my gradient at : Using this to calculate the directional derivative with , I get However, if I use the directional derivative definition here: which was the gradient answer but cubed. What should I expect from these answers? And what have I done wrong that yields these different but similar answers?","f(x,y) = \begin{cases}
           \frac{y^3}{x^2+y^2} & (x,y)\ne(0,0) \\
            0 & (x,y)=(0,0)
           \end{cases} (3,2) (0,0) (0,0) \frac{\partial}{\partial x}f(x,y)=\lim_{h\to 0}\frac{f(h,0)-f(0)}{h}=\lim_{h\to 0}\frac{f(h,0)}{h}=\lim_{h\to 0}\frac{0}{h^3}=0 \frac{\partial}{\partial y}f(x,y)=\lim_{k\to 0}\frac{f(0,k)-f(0)}{k}=\lim_{k\to 0}\frac{f(0,k)}{k}=\lim_{k\to 0}\frac{k^3}{k^3}=1 (0,0) \nabla f = \vec{(0, 1)} u = (3,2) \nabla_{u} f(0,0)=\nabla f_{(0,0)}\frac{u}{|u|}=\frac{1}{\sqrt{13}}(3,2)\cdot{}(0,1)=\frac{2}{\sqrt{13}} \lim_{h\to 0}\frac{f(\vec{a}+h\vec{u})-f(\vec{a})}{h|u|}=\lim_{h\to 0}\frac{f(h(3,2))}{h\sqrt{13}}=\lim_{h\to 0}\frac{\frac{(2h)^3}{(3h)^2+(2h)^2}}{h\sqrt{13}}=\frac{8}{13\sqrt{13}}","['limits', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'scalar-fields']"
77,Double integral of $1/(x^2+y^2)$ restricted to $x^2+y^2\leq2$ and $x\leq1$,Double integral of  restricted to  and,1/(x^2+y^2) x^2+y^2\leq2 x\leq1,"Find $$\iint_D \frac{1}{(x^2+y^2)^2}dA$$ where $$D = \left\{ (x,y): x^2 + y^2 \leq 2 \right\} \cap \left\{ (x,y): x \geq 1 \right\}$$ Because of the prevalence of $x^2+y^2$ terms here, I figured we would be using a change of variables to polar coordinates with $dA = rdrd\theta$ . However, I ran into trouble when finding the bounds of the integral. I know $r$ goes from $0$ to $\sqrt 2$ , but I got stumped when considering $\theta$ . Solving for $\theta$ using the substitution $x=r\cos\theta$ into $x\leq 1$ , I got $\theta =\arccos(\frac{1}{r})$ . This seems ok, but it resulted in an integral that is impossible to solve by hand (maybe not technically impossible, but clearly I did something wrong here). I also don't think using Cartesian coordinates would be the right approach, since the polar coordinate substitution results in very nice cancelling and easy integration.","Find where Because of the prevalence of terms here, I figured we would be using a change of variables to polar coordinates with . However, I ran into trouble when finding the bounds of the integral. I know goes from to , but I got stumped when considering . Solving for using the substitution into , I got . This seems ok, but it resulted in an integral that is impossible to solve by hand (maybe not technically impossible, but clearly I did something wrong here). I also don't think using Cartesian coordinates would be the right approach, since the polar coordinate substitution results in very nice cancelling and easy integration.","\iint_D \frac{1}{(x^2+y^2)^2}dA D = \left\{ (x,y): x^2 + y^2 \leq 2 \right\} \cap \left\{ (x,y): x \geq 1 \right\} x^2+y^2 dA = rdrd\theta r 0 \sqrt 2 \theta \theta x=r\cos\theta x\leq 1 \theta =\arccos(\frac{1}{r})","['integration', 'multivariable-calculus', 'multiple-integral', 'bounds-of-integration']"
78,Would you recommend Advanced Calculus: A geometric View by Callahan as a self-study book for multivariable analysis?,Would you recommend Advanced Calculus: A geometric View by Callahan as a self-study book for multivariable analysis?,,"I have background in calculus, linear algebra, single variable analysis, topology, ode and some abstract algebra. So I've decided to study multivariable analysis before/alongside Lee's smooth manifolds. But since I have had trouble with Rudin and Spivak, I have done some research to find different textbook. After some researching, I have narrowed down to 3 books: Zorich, Hubbard, and Callahan's book. However, I have some question regarding Callahan's book. Is the book rigorous enough to be used in a multivariable analysis course or should it be used as a supplement instead of main textbook? Thanks in advance.","I have background in calculus, linear algebra, single variable analysis, topology, ode and some abstract algebra. So I've decided to study multivariable analysis before/alongside Lee's smooth manifolds. But since I have had trouble with Rudin and Spivak, I have done some research to find different textbook. After some researching, I have narrowed down to 3 books: Zorich, Hubbard, and Callahan's book. However, I have some question regarding Callahan's book. Is the book rigorous enough to be used in a multivariable analysis course or should it be used as a supplement instead of main textbook? Thanks in advance.",,"['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'reference-request']"
79,"Range of $\phi, \theta$ in $\int_0^{\pi/4} \int_0^{\pi/2} \int_0^{2\sin\phi \sin\theta} \rho^3\sin\phi \sin\theta d\rho d\theta d\phi$",Range of  in,"\phi, \theta \int_0^{\pi/4} \int_0^{\pi/2} \int_0^{2\sin\phi \sin\theta} \rho^3\sin\phi \sin\theta d\rho d\theta d\phi","The question: A solid bounded by the (y,z)-plane, the (x,y)-plane, the cone $x^2 + y^2 = z^2$ , and the surface $x^2 + y^2 + z^2 - 2y = 0$ . Suppose a density of a chunk of metal of the shape of this solid at the point $(x, y, z)$ is $\sqrt{ x^2 + y^2 + z^2 }$ . Find the mass of the chunk of metal. So far I have $$\int_0^{\pi/4} \int_0^{\pi/2} \int_0^{2\sin\phi \sin\theta} \rho^3\sin\phi  d\rho d\theta d\phi  $$ but I'm unsure about the range for $\phi$ and $\theta$ ?","The question: A solid bounded by the (y,z)-plane, the (x,y)-plane, the cone , and the surface . Suppose a density of a chunk of metal of the shape of this solid at the point is . Find the mass of the chunk of metal. So far I have but I'm unsure about the range for and ?","x^2 + y^2 = z^2 x^2 + y^2 + z^2 - 2y = 0 (x, y, z) \sqrt{ x^2 + y^2 + z^2 } \int_0^{\pi/4} \int_0^{\pi/2} \int_0^{2\sin\phi \sin\theta} \rho^3\sin\phi  d\rho d\theta d\phi   \phi \theta","['integration', 'multivariable-calculus', 'multiple-integral', 'spherical-coordinates']"
80,Regarding verifying Gauss-Divergence theorem,Regarding verifying Gauss-Divergence theorem,,"If $\vec{F}=4x \hat{i}-2y^{2}\hat{j}+z^{2}\hat{k}$ taken over the region bounded by the cylinder $x^{2}+y^{2}=4$ , $z=0$ and $z=3$ . Verify Divergence theorem. Attempted Solution : Here is what I've done as yet. To verify the theorem, we have to compute $\iiint_{V}\nabla\cdot \vec{F}\mathrm dV$ and $\iint_{S}\vec{F}\cdot \hat{n}\mathrm dS$ and show that they're equal. $$\iiint_{V}\nabla\cdot\vec{F}\mathrm dV =\int_{0}^{3}\int_{-2}^{+2}\int_{-\sqrt{4-x^{2}}}^{+\sqrt{4-x^{2}}}(4-4y+2z)\mathrm dy\mathrm dx\mathrm dz$$ As for the surface integral, we need to consider the three surfaces, viz. , two disks and a curved surface. I'm having slight trouble computing the integral over the curved surface. $$\begin{aligned} x^{2}+y^{2}&=4 \\  \phi(x,y)&=x^{2}+y^{2}-4 \\ \hat{n}&=\frac{\nabla \phi}{|\nabla\phi|}=\frac{x\hat{i}+y\hat{j}}{2} \\ I&=\iint_{C}(2x^{2}-y^{3})\mathrm dS \end{aligned}$$ I don't know how to proceed. I'm having trouble writing $\mathrm dS$ in terms of Cartesian coordinates and that is the source of confusion. If I were to project it onto the plane, would I make separate cases for each, or is there some neat way to solve this compactly. Thanks in advance.","If taken over the region bounded by the cylinder , and . Verify Divergence theorem. Attempted Solution : Here is what I've done as yet. To verify the theorem, we have to compute and and show that they're equal. As for the surface integral, we need to consider the three surfaces, viz. , two disks and a curved surface. I'm having slight trouble computing the integral over the curved surface. I don't know how to proceed. I'm having trouble writing in terms of Cartesian coordinates and that is the source of confusion. If I were to project it onto the plane, would I make separate cases for each, or is there some neat way to solve this compactly. Thanks in advance.","\vec{F}=4x \hat{i}-2y^{2}\hat{j}+z^{2}\hat{k} x^{2}+y^{2}=4 z=0 z=3 \iiint_{V}\nabla\cdot \vec{F}\mathrm dV \iint_{S}\vec{F}\cdot \hat{n}\mathrm dS \iiint_{V}\nabla\cdot\vec{F}\mathrm dV =\int_{0}^{3}\int_{-2}^{+2}\int_{-\sqrt{4-x^{2}}}^{+\sqrt{4-x^{2}}}(4-4y+2z)\mathrm dy\mathrm dx\mathrm dz \begin{aligned}
x^{2}+y^{2}&=4 \\ 
\phi(x,y)&=x^{2}+y^{2}-4 \\
\hat{n}&=\frac{\nabla \phi}{|\nabla\phi|}=\frac{x\hat{i}+y\hat{j}}{2} \\
I&=\iint_{C}(2x^{2}-y^{3})\mathrm dS
\end{aligned} \mathrm dS","['integration', 'multivariable-calculus', 'divergence-theorem']"
81,Geometric/vector explanation of $\det(A)=0\iff$ unique solution doesn't exist to system of linear equation,Geometric/vector explanation of  unique solution doesn't exist to system of linear equation,\det(A)=0\iff,"Currently, I am self-studying Multivariable Calculus. I have prior knowledge about Vectors, Matrices and System of Linear Equations. However, the linkages between the three are not explicitly covered by my prior education and curriculum. I reckon it is beneficial for me to build a correct understanding about these linkages. I am not most familiar with the language of Mathematics. I apologize for any inaccuracies or intracies throughout my question in advance. TL;DR: Explain the following geometrically / from ""vector view"": $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\impliedby&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align}$$ Suppose there is a system of linear equations in the forms $$x\overrightarrow{a}+y\overrightarrow{b}+z\overrightarrow{c}=\overrightarrow{d}$$ and $$\mathbf{A}\overrightarrow{x}=\overrightarrow{d}$$ From my understanding, solving this system of linear equations is conceptually equivalent to: in a ""vector view"", expressing $\overrightarrow{d}$ in terms of linear combination(s) of $\overrightarrow{a}$ , $\overrightarrow{b}$ and $\overrightarrow{c}$ ; in a ""matrix view"", solving $\overrightarrow{x}=\mathbf{A}\overrightarrow{d}$ . My goal is to understand "" $\det(\mathbf{A}^{-1})=0\iff\text{unique solution doesn't exist}$ "" from the ""vector view"". I believe there is a logical explanation to my question: $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\iff&\det\mathbf{A}=0\\\iff&\text{unique solution doesn't exist}\\\iff&\text{no solution or infinitely many solutions}\\\iff&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\\&\text{ or there exist infinitely many linear combinations of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ expressing }\overrightarrow{d}\end{align}$$ To put it simply, the logical explanation is $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\iff&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\\&\text{ or there exist infinitely many linear combinations of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ expressing }\overrightarrow{d}\end{align}$$ From the definition of basis vectors, I know that $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly independent}\\\iff&\overrightarrow{d}\text{ can be expressed in }\mathbf{unique }\text{ linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align}$$ and I understand that the negation of the above statement would give the previous statement. **My understanding is stuck in the fact that ** $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\impliedby&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align}$$ $\det{A}=0\impliedby\text{no solution}$ is the reason why the above statement is true. I lack the intuition that it is true. Could anyone fill me in with the ""vector view"" of such statement? Thank you.","Currently, I am self-studying Multivariable Calculus. I have prior knowledge about Vectors, Matrices and System of Linear Equations. However, the linkages between the three are not explicitly covered by my prior education and curriculum. I reckon it is beneficial for me to build a correct understanding about these linkages. I am not most familiar with the language of Mathematics. I apologize for any inaccuracies or intracies throughout my question in advance. TL;DR: Explain the following geometrically / from ""vector view"": Suppose there is a system of linear equations in the forms and From my understanding, solving this system of linear equations is conceptually equivalent to: in a ""vector view"", expressing in terms of linear combination(s) of , and ; in a ""matrix view"", solving . My goal is to understand "" "" from the ""vector view"". I believe there is a logical explanation to my question: To put it simply, the logical explanation is From the definition of basis vectors, I know that and I understand that the negation of the above statement would give the previous statement. **My understanding is stuck in the fact that ** is the reason why the above statement is true. I lack the intuition that it is true. Could anyone fill me in with the ""vector view"" of such statement? Thank you.","\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\impliedby&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align} x\overrightarrow{a}+y\overrightarrow{b}+z\overrightarrow{c}=\overrightarrow{d} \mathbf{A}\overrightarrow{x}=\overrightarrow{d} \overrightarrow{d} \overrightarrow{a} \overrightarrow{b} \overrightarrow{c} \overrightarrow{x}=\mathbf{A}\overrightarrow{d} \det(\mathbf{A}^{-1})=0\iff\text{unique solution doesn't exist} \begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\iff&\det\mathbf{A}=0\\\iff&\text{unique solution doesn't exist}\\\iff&\text{no solution or infinitely many solutions}\\\iff&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\\&\text{ or there exist infinitely many linear combinations of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ expressing }\overrightarrow{d}\end{align} \begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\iff&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\\&\text{ or there exist infinitely many linear combinations of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ expressing }\overrightarrow{d}\end{align} \begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly independent}\\\iff&\overrightarrow{d}\text{ can be expressed in }\mathbf{unique }\text{ linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align} \begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\impliedby&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align} \det{A}=0\impliedby\text{no solution}","['linear-algebra', 'matrices', 'multivariable-calculus', 'vectors', 'systems-of-equations']"
82,Showing that the derivative of a vector-valued function is the the derivative of it's components using linear operators,Showing that the derivative of a vector-valued function is the the derivative of it's components using linear operators,,"Recently in class we had shown the following to be true using a proof that's equivalent to this ProofWiki article : Let $\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k}$ be a vector-valued function on $\left(a,b\right) \rightarrow \mathbb{V}^3$ whose components $x(t), y(t), z(t)$ are differentiable real functions. Then $\vec{r}$ is differentiable and: $$\vec{r}'(t) = x'(t)\vec{i} + y'(t)\vec{j} + z'(t)\vec{k}$$ Now while the linked proof is straightforward enough, I was thinking about differentiation in the context of a linear operator. My argument was: since differentiable real functions form a vector space and differentiation was a linear operator, then the theorem above immediately follows from the linearity property. Like this: Let $x(t), y(t), z(t)$ be vectors from the vector space of real differentiable functions and let $$\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k}$$ Apply linear operator of differenitation and use linearity: $$\vec{r}'(t) = \left(x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k}\right)' = x'(t)\vec{i} + y'(t)\vec{j} + z'(t)\vec{k}$$ When I asked my professor about this, he said that this cannot be done because I've applied linearity incorrectly, but upon further questioning he was unable to answer why. It seems valid to me, so I want to ask, is this a valid way to show the theorem? It seems much more elegant than using parametric limits. The only possible problem I can see is the multiplication of vectors $x(t), y(t), z(t)$ with $\vec{i}, \vec{j}, \vec{k}$ since they are from different vector spaces. But since $x,y,z$ are real valued, it is, in a way, equivalent to scalar-vector multiplication.","Recently in class we had shown the following to be true using a proof that's equivalent to this ProofWiki article : Let be a vector-valued function on whose components are differentiable real functions. Then is differentiable and: Now while the linked proof is straightforward enough, I was thinking about differentiation in the context of a linear operator. My argument was: since differentiable real functions form a vector space and differentiation was a linear operator, then the theorem above immediately follows from the linearity property. Like this: Let be vectors from the vector space of real differentiable functions and let Apply linear operator of differenitation and use linearity: When I asked my professor about this, he said that this cannot be done because I've applied linearity incorrectly, but upon further questioning he was unable to answer why. It seems valid to me, so I want to ask, is this a valid way to show the theorem? It seems much more elegant than using parametric limits. The only possible problem I can see is the multiplication of vectors with since they are from different vector spaces. But since are real valued, it is, in a way, equivalent to scalar-vector multiplication.","\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k} \left(a,b\right) \rightarrow \mathbb{V}^3 x(t), y(t), z(t) \vec{r} \vec{r}'(t) = x'(t)\vec{i} + y'(t)\vec{j} + z'(t)\vec{k} x(t), y(t), z(t) \vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k} \vec{r}'(t) = \left(x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k}\right)' = x'(t)\vec{i} + y'(t)\vec{j} + z'(t)\vec{k} x(t), y(t), z(t) \vec{i}, \vec{j}, \vec{k} x,y,z","['linear-algebra', 'multivariable-calculus']"
83,"Differentiability of the functions $\mathbf{f}:\mathbb{M}_{n\times n}\to \mathbb{M}_{n\times n},\ \mathbf{f}(A)=A^2,\ \mathbf{f}(A)=A^T A$",Differentiability of the functions,"\mathbf{f}:\mathbb{M}_{n\times n}\to \mathbb{M}_{n\times n},\ \mathbf{f}(A)=A^2,\ \mathbf{f}(A)=A^T A","I am trying to prove that the functions $\mathbf{f}:\mathbb{M}_{n\times n}\to \mathbb{M}_{n\times n},\ \mathbf{f}(A)=A^2,\ \mathbf{f}(A)=A^T A$ are differentiable. What I have done: We consider $\mathbf{D}(f_1)(A):H\mapsto AH+HA$ and $\mathbf{D}(f_2)(A):H\mapsto A^TH+H^TA$ . These are linear transformations (*) from $\mathbb{M}_{n\times n}$ to $\mathbb{M}_{n\times n}$ and we want to prove that $$\lim\limits_{H\to [0]}\frac{1}{|H|}\left( (f_1(A+H)-f_1(A)) - (AH+HA) \right)=[0]$$ and $$\lim\limits_{H\to [0]}\frac{1}{|H|}\left( (f_2(A+H)-f_2(A)) - (A^TH+H^TA) \right)=[0].$$ Since $f_1(A)=A^2,$ we have $$|f_1(A+H)-f_1(A) - (AH+HA)|=|A^2+AH+HA+H^2-A^2-AH-HA|=|H^2|;$$ this gives $$\lim\limits_{H\to [0]}\frac{|H^2|}{|H|}\leq\lim\limits_{H\to [0]}\frac{|H||H|}{|H|}=0$$ so $f_1$ is differentiable with derivative $AH+HA$ .\ Similarly, since $f_2(A)=A^TH+H^TA,$ we have $|f_2(A+H)-f_2(A)-(A^TH+H^TA)|=|(A+H)^T(A+H)-A^TA-A^TH-H^TA|=|(A^T+H^T)(A+H)-A^TA-A^TH-H^TA|=|A^TA+A^TH+H^TA+H^TH-A^TA-A^TH-H^TA|=|H^TH|$ ; this gives $$\lim\limits_{H\to [0]}\frac{|H^TH|}{|H|}\leq\lim\limits_{H\to [0]}\frac{|H^T||H|}{|H|}=\lim\limits_{H\to [0]} |H^T|=0$$ so $f_2$ is differentiable with derivative $A^TH+H^TA.$ $f_1(H_1+H_2)=A(H_1+H_2)+(H_1+H_2)A=AH_1+AH_2+H_1A+H_2A=(AH_1+H_1A)+(AH_2+H_2A)=f_1(H_1)+f_2(H_2),$ $f_1(cH))=A(cH)+(cH)A=cAH+cHA=c(AH+HA)=cf_1(H),$ $f_2(H_1+H_2)=A^T(H_1+H_2)+(H_1+H_2)^TA=A^TH_1+A^TH_2+H_1^TA+H_2^TA=(A^TH_1+H_1^TA)+(A^TH_2+H_2^TA)=f_2(H_1)+f_2(H_2),$ $f_2(cH)=A^T(cH)+(cH)^TA=cA^TH+cH^TA=c(A^TH+H^TA)=cf_2(H)$ for all $H,H_1,H_2\in\mathbb{M}_{n\times n},\ c\in\mathbb{R}$ Is this correct? Thanks for the feedback.","I am trying to prove that the functions are differentiable. What I have done: We consider and . These are linear transformations (*) from to and we want to prove that and Since we have this gives so is differentiable with derivative .\ Similarly, since we have ; this gives so is differentiable with derivative for all Is this correct? Thanks for the feedback.","\mathbf{f}:\mathbb{M}_{n\times n}\to \mathbb{M}_{n\times n},\ \mathbf{f}(A)=A^2,\ \mathbf{f}(A)=A^T A \mathbf{D}(f_1)(A):H\mapsto AH+HA \mathbf{D}(f_2)(A):H\mapsto A^TH+H^TA \mathbb{M}_{n\times n} \mathbb{M}_{n\times n} \lim\limits_{H\to [0]}\frac{1}{|H|}\left( (f_1(A+H)-f_1(A)) - (AH+HA) \right)=[0] \lim\limits_{H\to [0]}\frac{1}{|H|}\left( (f_2(A+H)-f_2(A)) - (A^TH+H^TA) \right)=[0]. f_1(A)=A^2, |f_1(A+H)-f_1(A) - (AH+HA)|=|A^2+AH+HA+H^2-A^2-AH-HA|=|H^2|; \lim\limits_{H\to [0]}\frac{|H^2|}{|H|}\leq\lim\limits_{H\to [0]}\frac{|H||H|}{|H|}=0 f_1 AH+HA f_2(A)=A^TH+H^TA, |f_2(A+H)-f_2(A)-(A^TH+H^TA)|=|(A+H)^T(A+H)-A^TA-A^TH-H^TA|=|(A^T+H^T)(A+H)-A^TA-A^TH-H^TA|=|A^TA+A^TH+H^TA+H^TH-A^TA-A^TH-H^TA|=|H^TH| \lim\limits_{H\to [0]}\frac{|H^TH|}{|H|}\leq\lim\limits_{H\to [0]}\frac{|H^T||H|}{|H|}=\lim\limits_{H\to [0]} |H^T|=0 f_2 A^TH+H^TA. f_1(H_1+H_2)=A(H_1+H_2)+(H_1+H_2)A=AH_1+AH_2+H_1A+H_2A=(AH_1+H_1A)+(AH_2+H_2A)=f_1(H_1)+f_2(H_2), f_1(cH))=A(cH)+(cH)A=cAH+cHA=c(AH+HA)=cf_1(H), f_2(H_1+H_2)=A^T(H_1+H_2)+(H_1+H_2)^TA=A^TH_1+A^TH_2+H_1^TA+H_2^TA=(A^TH_1+H_1^TA)+(A^TH_2+H_2^TA)=f_2(H_1)+f_2(H_2), f_2(cH)=A^T(cH)+(cH)^TA=cA^TH+cH^TA=c(A^TH+H^TA)=cf_2(H) H,H_1,H_2\in\mathbb{M}_{n\times n},\ c\in\mathbb{R}","['multivariable-calculus', 'derivatives', 'differential']"
84,Maximizing the volume of a cuboid with constraints (lagrange) fails?,Maximizing the volume of a cuboid with constraints (lagrange) fails?,,"Given is the following (translated) problem: You have $12$ meters of wire. Try to build a wireframe model of a cuboid with sidelengths $x, y, z$ and maximize volume $V(x,y,z)=xyz$ . Show that this is the case iff all sidelengths are equal. My attempt: Using Lagrange with the constraint $g(x,y,z)=x+y+z-3$ leads to: $$\nabla L(x,y,z,\lambda)=\begin{pmatrix}            yz+\lambda \\            xz+\lambda \\            xy +\lambda \\            x+y+z-3          \end{pmatrix} \stackrel{!}{=}0$$ After solving I get: $$x=y=z=1$$ $$\lambda = -1$$ I calculate the Hessian matrix afterwards and get: $$H_L=\begin{pmatrix}            0 & z & y & 1\\            z & 0 & x & 1 \\            y & x & 0 & 1  \\            1 & 1 & 1 & 0          \end{pmatrix}$$ Inserting previous values produces this matrix: $$H_L=\begin{pmatrix}            0 & 1 & 1& 1\\            1 & 0 & 1 & 1 \\            1 & 1 & 0 & 1  \\            1 & 1 & 1 & 0          \end{pmatrix}$$ Now the problem : This matrix has eigenvalues $3$ and $-1$ , which should mean it's indefinite and therefore has as saddle point/ no minimum or maximum. Did I do something wrong or is it not possible to solve this problem with lagrange?","Given is the following (translated) problem: You have meters of wire. Try to build a wireframe model of a cuboid with sidelengths and maximize volume . Show that this is the case iff all sidelengths are equal. My attempt: Using Lagrange with the constraint leads to: After solving I get: I calculate the Hessian matrix afterwards and get: Inserting previous values produces this matrix: Now the problem : This matrix has eigenvalues and , which should mean it's indefinite and therefore has as saddle point/ no minimum or maximum. Did I do something wrong or is it not possible to solve this problem with lagrange?","12 x, y, z V(x,y,z)=xyz g(x,y,z)=x+y+z-3 \nabla L(x,y,z,\lambda)=\begin{pmatrix}
           yz+\lambda \\
           xz+\lambda \\
           xy +\lambda \\
           x+y+z-3
         \end{pmatrix} \stackrel{!}{=}0 x=y=z=1 \lambda = -1 H_L=\begin{pmatrix}
           0 & z & y & 1\\
           z & 0 & x & 1 \\
           y & x & 0 & 1  \\
           1 & 1 & 1 & 0
         \end{pmatrix} H_L=\begin{pmatrix}
           0 & 1 & 1& 1\\
           1 & 0 & 1 & 1 \\
           1 & 1 & 0 & 1  \\
           1 & 1 & 1 & 0
         \end{pmatrix} 3 -1","['calculus', 'matrices', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
85,Let $h:S\rightarrow \mathbb{R}$ be a differentiable function. Show there exists a unique tangent field $F$ with $D_qh(v)=F\cdot v$.,Let  be a differentiable function. Show there exists a unique tangent field  with .,h:S\rightarrow \mathbb{R} F D_qh(v)=F\cdot v,"The following is an exercise from a set of notes on Differential Geometry asking us to prove a result. I have a few questions about the proof. Theorem: Let $S$ be a regular surface and let $h:S\rightarrow \mathbb{R}$ be a differentiable function. Show there exists a unique tangent field $F:S\rightarrow \mathbb{R}^3$ (that is, a unique differentiable function $F$ tangent to the surface at every point) such that $D_qh(v)=F\cdot v$ (where $\cdot$ denotes the dot producut on $\mathbb{R}^3$ ). Proof: Since the restriction of the dot product to $T_qS$ (the tangent plane to $S$ at $q$ ) is non-degenerate, there exists a unique vector $F(q)\in T_qS$ such that $D_qh(v)=F(q)\cdot v$ for every $v\in T_qS$ . $ \ \ \ $ Q1 . Why is there such a unique vector? To show that $F$ is a tangent field, it remains to be shown that $F$ is differentiable. Let $\phi : \Omega \rightarrow \mathbb{R}^3$ be a parametrization of $S$ .  We'll show the function $\textbf{x}:=F\circ \phi$ is differentiable. Since the partial derivatives $\phi _u, \phi _v$ define a basis for each tangent plane to the surface $S$ , there exist functions $\lambda, \mu:\Omega \rightarrow \mathbb{R}$ such that $\textbf{x}=\lambda \phi_u+\mu\phi_v$ . Therefore to show that $F$ is differentiable it is sufficient to show that (for every parametrization $\phi:\Omega \rightarrow \mathbb{R}^3)$ the functions $\lambda, \mu$ are differentiable. $ \ \ \ $ Q2 . Why does the differentiability of $\lambda$ and $\mu$ imply the differentiability of $F$ ? Let $E, F, G$ be the coefficients of the first fundamental form of $\phi$ . We have that $\textbf{x}\cdot \phi_u=\lambda E+\mu F$ and $\textbf{x}\cdot \phi_v = \lambda F + \mu G$ . On the other hand, we have that $\textbf{x}\cdot \phi _u = (h\circ \phi)_u$ and $\textbf{x}\cdot \phi _v = (h\circ \phi)_v$ . $ \ \ \ $ Q3 . Where do these last two equalities come from? Since $h:S\rightarrow \mathbb{R}$ is differentiable, both functions $(h\circ \phi)_u$ , $(h \circ \phi)_v$ are differentiable. Thus we have $$ \begin{pmatrix} \lambda \\ \mu \end{pmatrix} = \frac{1}{\Delta ^2} \begin{pmatrix} G & -F \\ -F & E \end{pmatrix} \begin{pmatrix} \lambda E + \mu F \\ \lambda F + \mu G \end{pmatrix} =  \frac{1}{\Delta ^2} \begin{pmatrix} G & -F \\ -F & E \end{pmatrix} \begin{pmatrix} (h\circ \phi)_u \\ (h\circ \phi)_v \end{pmatrix} $$ where $\Delta ^2 = EG-F^2$ . Therefore both functions $\lambda, \mu$ are differentiable.","The following is an exercise from a set of notes on Differential Geometry asking us to prove a result. I have a few questions about the proof. Theorem: Let be a regular surface and let be a differentiable function. Show there exists a unique tangent field (that is, a unique differentiable function tangent to the surface at every point) such that (where denotes the dot producut on ). Proof: Since the restriction of the dot product to (the tangent plane to at ) is non-degenerate, there exists a unique vector such that for every . Q1 . Why is there such a unique vector? To show that is a tangent field, it remains to be shown that is differentiable. Let be a parametrization of .  We'll show the function is differentiable. Since the partial derivatives define a basis for each tangent plane to the surface , there exist functions such that . Therefore to show that is differentiable it is sufficient to show that (for every parametrization the functions are differentiable. Q2 . Why does the differentiability of and imply the differentiability of ? Let be the coefficients of the first fundamental form of . We have that and . On the other hand, we have that and . Q3 . Where do these last two equalities come from? Since is differentiable, both functions , are differentiable. Thus we have where . Therefore both functions are differentiable.","S h:S\rightarrow \mathbb{R} F:S\rightarrow \mathbb{R}^3 F D_qh(v)=F\cdot v \cdot \mathbb{R}^3 T_qS S q F(q)\in T_qS D_qh(v)=F(q)\cdot v v\in T_qS  \ \ \  F F \phi : \Omega \rightarrow \mathbb{R}^3 S \textbf{x}:=F\circ \phi \phi _u, \phi _v S \lambda, \mu:\Omega \rightarrow \mathbb{R} \textbf{x}=\lambda \phi_u+\mu\phi_v F \phi:\Omega \rightarrow \mathbb{R}^3) \lambda, \mu  \ \ \  \lambda \mu F E, F, G \phi \textbf{x}\cdot \phi_u=\lambda E+\mu F \textbf{x}\cdot \phi_v = \lambda F + \mu G \textbf{x}\cdot \phi _u = (h\circ \phi)_u \textbf{x}\cdot \phi _v = (h\circ \phi)_v  \ \ \  h:S\rightarrow \mathbb{R} (h\circ \phi)_u (h \circ \phi)_v 
\begin{pmatrix}
\lambda \\
\mu
\end{pmatrix}
=
\frac{1}{\Delta ^2}
\begin{pmatrix}
G & -F \\
-F & E
\end{pmatrix}
\begin{pmatrix}
\lambda E + \mu F \\
\lambda F + \mu G
\end{pmatrix}
= 
\frac{1}{\Delta ^2}
\begin{pmatrix}
G & -F \\
-F & E
\end{pmatrix}
\begin{pmatrix}
(h\circ \phi)_u \\
(h\circ \phi)_v
\end{pmatrix}
 \Delta ^2 = EG-F^2 \lambda, \mu","['geometry', 'multivariable-calculus', 'differential-geometry', 'proof-explanation']"
86,Is a differentiable multivariable function with continuous derivatives on analytic paths continuously differentiable?,Is a differentiable multivariable function with continuous derivatives on analytic paths continuously differentiable?,,"Let $f: \mathbb R^n \rightarrow \mathbb R$ be an everywhere differentiable function; and continuously differentiable when restricted to any analytic path. Is then $f$ continuously differentiable? I would bet on no, but I failed to construct a counter-example in a reasonable time. But maybe I am overthinking, and there should be a way to show the continuous differentiability? This problem came to me from my research: I found myself in a company of a functional, which is smooth outside of some codimension 1 piecewise-analytic set. I was able to show that over this set the functional is differentiable. It was also clear that its restriction to any analytic path was nice. So I started wondering whether I can rely on that my functional friend is $C^1$ .","Let be an everywhere differentiable function; and continuously differentiable when restricted to any analytic path. Is then continuously differentiable? I would bet on no, but I failed to construct a counter-example in a reasonable time. But maybe I am overthinking, and there should be a way to show the continuous differentiability? This problem came to me from my research: I found myself in a company of a functional, which is smooth outside of some codimension 1 piecewise-analytic set. I was able to show that over this set the functional is differentiable. It was also clear that its restriction to any analytic path was nice. So I started wondering whether I can rely on that my functional friend is .",f: \mathbb R^n \rightarrow \mathbb R f C^1,"['real-analysis', 'multivariable-calculus', 'derivatives']"
87,"If $D_1 f$ exists and is bounded, and $y \mapsto f(x,y)$ is continuous, then $f$ is continuous","If  exists and is bounded, and  is continuous, then  is continuous","D_1 f y \mapsto f(x,y) f","Suppose that $U$ is an open subset of $\mathbb{R}^2$ and $f:U \to \mathbb{R}$ is such that the map $y \mapsto f(x,y)$ is continuous. Moreover, assume that the partial derivative in the first coordinate, $D_1 f$ , exists and is bounded. I'm trying to show that $f$ must be continuous. What I have so far: The condition that $D_1f$ exists and is bounded is equivalent to saying that for all $(a_1,a_2) \in U$ , $$\lim_{t \to 0} \frac{1}{t} (f(a_1+t,a_2)-f(a_1,a_2))$$ exists and is bounded. The condition that $y \to f(x,y)$ is continuous is equivalent to $$\lim_{(x,y) \to (x_0,y_0)} f(x,y) = f(x_0,y_0)$$ but I can't really see a sensible way of linking these together straight away. (I know the $D_1f$ condition must be important because it's not necessarily true that $x \mapsto f(x,y)$ and $y \mapsto f(x,y)$ both being componentwise continuous implies $f$ continuous). How do you go about showing the result?","Suppose that is an open subset of and is such that the map is continuous. Moreover, assume that the partial derivative in the first coordinate, , exists and is bounded. I'm trying to show that must be continuous. What I have so far: The condition that exists and is bounded is equivalent to saying that for all , exists and is bounded. The condition that is continuous is equivalent to but I can't really see a sensible way of linking these together straight away. (I know the condition must be important because it's not necessarily true that and both being componentwise continuous implies continuous). How do you go about showing the result?","U \mathbb{R}^2 f:U \to \mathbb{R} y \mapsto f(x,y) D_1 f f D_1f (a_1,a_2) \in U \lim_{t \to 0} \frac{1}{t} (f(a_1+t,a_2)-f(a_1,a_2)) y \to f(x,y) \lim_{(x,y) \to (x_0,y_0)} f(x,y) = f(x_0,y_0) D_1f x \mapsto f(x,y) y \mapsto f(x,y) f","['real-analysis', 'multivariable-calculus']"
88,Construct $f$ satisfying certain given conditions,Construct  satisfying certain given conditions,f,"Given $\textbf x\in \mathbb R^n$ and $r>0$ , construct a function $f:\mathbb R^n\to [0,1]$ of class $C^\infty$ such that $$f^{-1}(1)=\overline{B(\textbf{x},r/2)}\\f^{-1}(0)=\mathbb R^n\setminus B(\textbf{x},r)$$ Being helpless about the original question (apart from the geometrical realization that it looks like a peninsula), I was trying the $n=1$ variant out. This translates to Given $a<b$ , find a function $g:\mathbb R\to [0,1]$ of class $C^\infty$ such that $$g|_{(-\infty,a]}\equiv1\\g|_{[b,\infty)}\equiv0$$ Even this one seems quite out of my reach. The first idea I had was to use the Sigmoid function. But, that never gives a zero derivative except at $x\to \pm \infty$ . So, my next idea was to use the $C^\infty$ property of the function $y=e^{-\frac 1x}$ and construct $g$ locally at $x=a$ using the function $$\alpha(x)=1-e^{-\frac 1{x-a}}$$ and locally at $x=b$ using the function $$\beta(x)=e^{\frac 1{x-b}}$$ But, I couldn't proceed with this as well.","Given and , construct a function of class such that Being helpless about the original question (apart from the geometrical realization that it looks like a peninsula), I was trying the variant out. This translates to Given , find a function of class such that Even this one seems quite out of my reach. The first idea I had was to use the Sigmoid function. But, that never gives a zero derivative except at . So, my next idea was to use the property of the function and construct locally at using the function and locally at using the function But, I couldn't proceed with this as well.","\textbf x\in \mathbb R^n r>0 f:\mathbb R^n\to [0,1] C^\infty f^{-1}(1)=\overline{B(\textbf{x},r/2)}\\f^{-1}(0)=\mathbb R^n\setminus B(\textbf{x},r) n=1 a<b g:\mathbb R\to [0,1] C^\infty g|_{(-\infty,a]}\equiv1\\g|_{[b,\infty)}\equiv0 x\to \pm \infty C^\infty y=e^{-\frac 1x} g x=a \alpha(x)=1-e^{-\frac 1{x-a}} x=b \beta(x)=e^{\frac 1{x-b}}","['calculus', 'analysis', 'multivariable-calculus', 'functions', 'derivatives']"
89,Deterministic Policy Gradient Theorem Proof,Deterministic Policy Gradient Theorem Proof,,"This paper states the deterministic policy gradient theorem. The proof of the theorem is provided in a supplement . This question is about the very first step in the proof. It begins with $$ \nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))  = \nabla_\theta \left( r(s, \mu_\theta(s)) + \int_{\mathcal{S}} \gamma p(s' | s, \mu_\theta(s)) V^{\mu_\theta} (s')ds' \right)  $$ and then more steps follow. The first equality is clear. This follows from the relation between the state value function $V$ and the state-action value function $Q$ . In the more general stochastic case, with a stochastic policy $\pi$ we have this relation: $$ V^\pi (s) = \sum_a \pi(a|s) Q^\pi(s, a). $$ When the policy $\pi$ is changed to $\mu_\theta $ and is deterministic, only 1 term in the sum remains and we get $$ V^{\mu_\theta}(s) = Q^{\mu_\theta}(s, \mu_\theta(s)) . $$ Now we apply $\nabla_\theta$ on each side. This is where the confusion starts. On the right side we can use the chain rule and we would get $$ \nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))  = \nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)}  $$ and be done. Doesn't this directly prove the theorem? Surely I must have missed something. Can you please explain why this simple differentiation rule is not used in the proof and why the long substitution in the next step is needed, and where does that come from? Also, it would be helpful to see what do the Bellman equations look like for a deterministic policy? Many thanks","This paper states the deterministic policy gradient theorem. The proof of the theorem is provided in a supplement . This question is about the very first step in the proof. It begins with and then more steps follow. The first equality is clear. This follows from the relation between the state value function and the state-action value function . In the more general stochastic case, with a stochastic policy we have this relation: When the policy is changed to and is deterministic, only 1 term in the sum remains and we get Now we apply on each side. This is where the confusion starts. On the right side we can use the chain rule and we would get and be done. Doesn't this directly prove the theorem? Surely I must have missed something. Can you please explain why this simple differentiation rule is not used in the proof and why the long substitution in the next step is needed, and where does that come from? Also, it would be helpful to see what do the Bellman equations look like for a deterministic policy? Many thanks"," \nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))
 = \nabla_\theta \left( r(s, \mu_\theta(s)) + \int_{\mathcal{S}} \gamma p(s' | s, \mu_\theta(s)) V^{\mu_\theta} (s')ds' \right)   V Q \pi  V^\pi (s) = \sum_a \pi(a|s) Q^\pi(s, a).  \pi \mu_\theta   V^{\mu_\theta}(s) = Q^{\mu_\theta}(s, \mu_\theta(s))
.  \nabla_\theta  \nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))
 = \nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)}  ",['multivariable-calculus']
90,A problem about the area element in the Stokes' Theorem,A problem about the area element in the Stokes' Theorem,,"Given a vector field $F(x,y,z) = x^2 \hat i + 2x  \hat{j} + z^2 \hat{k}  $ and a curve $C: \text{the ellipse } 4x^2 + y^2 = 4 \text{ in the } xy- \text{plane}$ , I want to find $$\oint_{C} \vec F \cdot dr = \int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma $$ via Stokes' Theorem. Here, $S$ is the surface on the $xy$ -plane bounded by the curve $C$ above. I found $$\nabla \times \vec F = 2 \hat{k}$$ and I take $\hat n = \hat k$ as the surface is lying on the $xy$ -plane. Thus, I end up with $$\oint_{C} \vec F \cdot dr = \int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma   =\int\int_{S} 2 d\sigma = 2(\text{area of the ellipse}) = 4 \pi.   $$ However, for the surface area element, we should have $$d \sigma = \frac{\mid \nabla f|}{|\nabla f \cdot \hat k |} $$ where $f$ is the level surface that comes from the the surface $4x^2 + y^2 = 4 $ in the $xy$ -plane. So, if I let $f(x,y,z)=4x^2 +y^2 -4$ for example, I end up with $\nabla f \cdot \hat k = 0$ such that I cannot write $d\sigma$ . What do I do wrong?","Given a vector field and a curve , I want to find via Stokes' Theorem. Here, is the surface on the -plane bounded by the curve above. I found and I take as the surface is lying on the -plane. Thus, I end up with However, for the surface area element, we should have where is the level surface that comes from the the surface in the -plane. So, if I let for example, I end up with such that I cannot write . What do I do wrong?","F(x,y,z) = x^2 \hat i + 2x  \hat{j} + z^2 \hat{k}   C: \text{the ellipse } 4x^2 + y^2 = 4 \text{ in the } xy- \text{plane} \oint_{C} \vec F \cdot dr = \int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma  S xy C \nabla \times \vec F = 2 \hat{k} \hat n = \hat k xy \oint_{C} \vec F \cdot dr = \int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma 
 =\int\int_{S} 2 d\sigma = 2(\text{area of the ellipse}) = 4 \pi.
   d \sigma = \frac{\mid \nabla f|}{|\nabla f \cdot \hat k |}  f 4x^2 + y^2 = 4  xy f(x,y,z)=4x^2 +y^2 -4 \nabla f \cdot \hat k = 0 d\sigma","['calculus', 'multivariable-calculus', 'surface-integrals', 'stokes-theorem']"
91,"Theorem 9.41 Rudin, Principles of Mathematical Analysis","Theorem 9.41 Rudin, Principles of Mathematical Analysis",,"Rudin pp. 235-6 I am having trouble following Rudin’s proof of theorem 9.41.  The first inequality is clear to me: for any $(x,y)$ sufficiently close to $(a,b)$ , the difference in $D_{21}$ at these points can be made arbitrarily small.  However I do not understand how Rudin gets the next inequality.  The identity $\frac{\Delta(f,Q)}{hk}=hk(D_{21}f)(x,y)$ can only be said to apply for a point in the rectangle, $Q$ , not for every $(x,y)$ in $Q$ . Along the same line of reasoning it seems to me to be erroneous for Rudin to use the variable $b$ in (97), again, because the identity applies only to a specific $(x,y)$ . Can anyone lend clarification to these issues? EDIT: I believe that I have answered my own question and there is only one other problem that remains for me. Final question: Why, in equation (97), is there an $\leq$ sign instead of $<$ ? My answer: Rudin uses theorem 9.40, which guarantees the existence of an $(x,y)$ in the interior of $Q$ that satisfies (95), to rewrite $D_{21}f(x,y)$ in terms of $a,b,h,k,$ and the function, $f$ . He then applies limits twice in one inequality to get the desired derivatives.  Specifically, $$|D_{21}f(x,y)-A|<\epsilon$$ for all $(x,y)$ in $Q$ .  Now apply theorem 9.40 and hence write $$\bigg | \frac{\Delta(f,Q)}{hk}-A \bigg |=\bigg | \frac{1}{h} \cdot\frac{f(a+h,b+k)-f(a+h,b)-[f(a,b+k)-f(a,b)]}{k}-A \bigg |< \epsilon$$ for some $(x,y)$ in $Q$ . Now take the limit of the first two and last two terms in the large numerator with $k$ approaching zero.  This gives equation (97).  This time it seems that we cannot take a limit because we are not told explicitly that $D_{12}$ exists. So instead Rudin says that equation (97) holds for all small $h$ , and this is, incidentally, the derivative we want, $D_{12}$ . Theorem 9.41, Rudin, PMA and Theorem 9.40, Rudin, PMA","Rudin pp. 235-6 I am having trouble following Rudin’s proof of theorem 9.41.  The first inequality is clear to me: for any sufficiently close to , the difference in at these points can be made arbitrarily small.  However I do not understand how Rudin gets the next inequality.  The identity can only be said to apply for a point in the rectangle, , not for every in . Along the same line of reasoning it seems to me to be erroneous for Rudin to use the variable in (97), again, because the identity applies only to a specific . Can anyone lend clarification to these issues? EDIT: I believe that I have answered my own question and there is only one other problem that remains for me. Final question: Why, in equation (97), is there an sign instead of ? My answer: Rudin uses theorem 9.40, which guarantees the existence of an in the interior of that satisfies (95), to rewrite in terms of and the function, . He then applies limits twice in one inequality to get the desired derivatives.  Specifically, for all in .  Now apply theorem 9.40 and hence write for some in . Now take the limit of the first two and last two terms in the large numerator with approaching zero.  This gives equation (97).  This time it seems that we cannot take a limit because we are not told explicitly that exists. So instead Rudin says that equation (97) holds for all small , and this is, incidentally, the derivative we want, . Theorem 9.41, Rudin, PMA and Theorem 9.40, Rudin, PMA","(x,y) (a,b) D_{21} \frac{\Delta(f,Q)}{hk}=hk(D_{21}f)(x,y) Q (x,y) Q b (x,y) \leq < (x,y) Q D_{21}f(x,y) a,b,h,k, f |D_{21}f(x,y)-A|<\epsilon (x,y) Q \bigg | \frac{\Delta(f,Q)}{hk}-A \bigg |=\bigg | \frac{1}{h} \cdot\frac{f(a+h,b+k)-f(a+h,b)-[f(a,b+k)-f(a,b)]}{k}-A \bigg |< \epsilon (x,y) Q k D_{12} h D_{12}","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
92,Dilemma in a proof of the Implicit Function Theorem,Dilemma in a proof of the Implicit Function Theorem,,"I was studying the Implicit Function Theorem (source Diferencijalni račun funkcija više varijabli by I. Gogić, P. Pandžić and J. Tambača, pages 91-92) and I stumbled across something. First, I'm going to write the statement and proof as given in the script verbatim from Croatian: Let $A\subseteq\Bbb R^n\times\Bbb R^m$ be an open set, $F:A\to\Bbb R^m$ of the class $C^p,p\ge 1.$ Suppose $(x^0,y^0)\in A$ satisfies $F(x^0,y^0)=0_{\Bbb R^m}$ and let $\frac{\partial F}{\partial y}(x^0,y^0)\in M_m(\Bbb R)$ be a regular matrix. Then, there is an open neighbourhood $U\subseteq\Bbb R^n$ of $x^0$ and an open neighbourhood $V\subseteq\Bbb R^m$ of $y^0$ and a unique function $f:U\to V$ of the class $C^p$ s. t. $F(x,f(x))=0_{\Bbb R^m},x\in U.$ Note $$\frac{\partial F}{\partial y}(x,y)=\begin{bmatrix}\frac{\partial F_1}{\partial y_1}(x,y)&\ldots&\frac{\partial F_1}{\partial y_m}(x,y)\\\vdots&\ddots&\vdots\\\frac{\partial F_m}{\partial y_1}(x,y)&\ldots&\frac{\partial F_m}{\partial y_m}(x,y)\end{bmatrix}$$ proof : The idea of the proof is to apply the Inverse Function Theorem so, let's define a function $G:A\to\Bbb R^n\times\Bbb R^m$ by formula $G(x,y)=(x,F(x,y)).$ Since $F$ is of the class $C^p,G$ is also of the class $C^p$ . The Jacobian matrix of the function $G$ is given by $\nabla G(x,y)=\begin{bmatrix}1&\ldots&0&0&\ldots&0\\\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\0&\ldots&1&0&\ldots&0\\\frac{\partial F_1}{\partial x_1}(x,y)&\ldots&\frac{\partial F_1}{\partial x_n}(x,y)&\frac{\partial F_1}{\partial y_1}&\ldots&\frac{\partial F_1}{\partial y_m}(x,y)\\\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\\frac{\partial F_m}{\partial x_1}(x,y)&\ldots&\frac{\partial F_m}{\partial x_n}(x,y)&\frac{\partial F_m}{\partial y_1}(x,y)&\ldots&\frac{\partial F_m}{\partial y_m}(x,y)\end{bmatrix}$ Due to the structure of this matrix, $J_G(x,y)=\det\frac{\partial F}{\partial y}(x,y),$ hence $J_G(x^0,y^0)\ne 0$ so we can apply the Inverse Function Theorem: there is an open neighbourhood $S$ of the point $(x^0,y^0),$ and open neighbourhood $W$ of $G(x^0,y^0)=(x^0,0)$ and the inverse function $G^{-1}:W\to S$ of the class $C^p$ . Since $S$ is open, there is an open neighbourhood $U$ of $x^0$ and an open neighbourhood $V$ of $y^0$ s. t. $U\times V\subset S$ . Since $G^{-1}$ is continuous, the set $Y=G(U\times V)\subseteq W$ is open. Therefore, $G:U\times V\to Y$ is a $C^p$ -diffeomorphism. It holds: $(x,y)=G^{-1}(G(x,y))=G^{-1}(x,F(x,y)), (x,y)\in U\times V.$ For $(x,y)\in U\times V,$ which satisfy $F(x,y)=0\tag 1,$ it follows they are dependent through $(x,y)=G^{-1}(x,0).$ Therefore, we define a function $f:U\to V$ by $$f(x)=\pi_2\circ G^{-1}(x,0),$$ where $\pi_1(x,y)=x,\pi_2(x,y)=y.$ As $\pi_2$ and $G^{-1}$ are of the class $C^p,$ the function $f$ is, too, of the class $C^p$ . We now use the other composition $\begin{aligned}(x,0)&=G(G^{-1}(x,0))\\&=G(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0))\\&=(\pi_1\circ G^{-1}(x,0), F(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0)))\end{aligned}$ so that we conclude $x=\pi_1\circ G^{-1}(x,0),0=F(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0)).$ From the definition of the function $f,$ it follows that $$0=F(x,f(x)),x\in U.\boxed{}$$ I have a question regarding $(1)$ : It says : For $(x,y)\in U\times V$ which satisfy $F(x,y)=0,$ ... we define $f:U\to V$ by $f(x)=\pi_2\circ G^{-1}(x,0).$ I know that, since $Y=G(U\times V)$ is open and $G(x^0,y^0)=(x^0,F(x^0,y^0))=(x^0,0)\in Y,$ there certainly must be some other points of the form $(x,0)\in Y,$ where $x\in U,$ but how do we know $G^{-1}(x,0)$ isn't empty for an arbitrary $x\in U,$ i. e., that $f$ is well-defined $\forall x\in U$ ?","I was studying the Implicit Function Theorem (source Diferencijalni račun funkcija više varijabli by I. Gogić, P. Pandžić and J. Tambača, pages 91-92) and I stumbled across something. First, I'm going to write the statement and proof as given in the script verbatim from Croatian: Let be an open set, of the class Suppose satisfies and let be a regular matrix. Then, there is an open neighbourhood of and an open neighbourhood of and a unique function of the class s. t. Note proof : The idea of the proof is to apply the Inverse Function Theorem so, let's define a function by formula Since is of the class is also of the class . The Jacobian matrix of the function is given by Due to the structure of this matrix, hence so we can apply the Inverse Function Theorem: there is an open neighbourhood of the point and open neighbourhood of and the inverse function of the class . Since is open, there is an open neighbourhood of and an open neighbourhood of s. t. . Since is continuous, the set is open. Therefore, is a -diffeomorphism. It holds: For which satisfy it follows they are dependent through Therefore, we define a function by where As and are of the class the function is, too, of the class . We now use the other composition so that we conclude From the definition of the function it follows that I have a question regarding : It says : For which satisfy ... we define by I know that, since is open and there certainly must be some other points of the form where but how do we know isn't empty for an arbitrary i. e., that is well-defined ?","A\subseteq\Bbb R^n\times\Bbb R^m F:A\to\Bbb R^m C^p,p\ge 1. (x^0,y^0)\in A F(x^0,y^0)=0_{\Bbb R^m} \frac{\partial F}{\partial y}(x^0,y^0)\in M_m(\Bbb R) U\subseteq\Bbb R^n x^0 V\subseteq\Bbb R^m y^0 f:U\to V C^p F(x,f(x))=0_{\Bbb R^m},x\in U. \frac{\partial F}{\partial y}(x,y)=\begin{bmatrix}\frac{\partial F_1}{\partial y_1}(x,y)&\ldots&\frac{\partial F_1}{\partial y_m}(x,y)\\\vdots&\ddots&\vdots\\\frac{\partial F_m}{\partial y_1}(x,y)&\ldots&\frac{\partial F_m}{\partial y_m}(x,y)\end{bmatrix} G:A\to\Bbb R^n\times\Bbb R^m G(x,y)=(x,F(x,y)). F C^p,G C^p G \nabla G(x,y)=\begin{bmatrix}1&\ldots&0&0&\ldots&0\\\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\0&\ldots&1&0&\ldots&0\\\frac{\partial F_1}{\partial x_1}(x,y)&\ldots&\frac{\partial F_1}{\partial x_n}(x,y)&\frac{\partial F_1}{\partial y_1}&\ldots&\frac{\partial F_1}{\partial y_m}(x,y)\\\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\\frac{\partial F_m}{\partial x_1}(x,y)&\ldots&\frac{\partial F_m}{\partial x_n}(x,y)&\frac{\partial F_m}{\partial y_1}(x,y)&\ldots&\frac{\partial F_m}{\partial y_m}(x,y)\end{bmatrix} J_G(x,y)=\det\frac{\partial F}{\partial y}(x,y), J_G(x^0,y^0)\ne 0 S (x^0,y^0), W G(x^0,y^0)=(x^0,0) G^{-1}:W\to S C^p S U x^0 V y^0 U\times V\subset S G^{-1} Y=G(U\times V)\subseteq W G:U\times V\to Y C^p (x,y)=G^{-1}(G(x,y))=G^{-1}(x,F(x,y)), (x,y)\in U\times V. (x,y)\in U\times V, F(x,y)=0\tag 1, (x,y)=G^{-1}(x,0). f:U\to V f(x)=\pi_2\circ G^{-1}(x,0), \pi_1(x,y)=x,\pi_2(x,y)=y. \pi_2 G^{-1} C^p, f C^p \begin{aligned}(x,0)&=G(G^{-1}(x,0))\\&=G(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0))\\&=(\pi_1\circ G^{-1}(x,0), F(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0)))\end{aligned} x=\pi_1\circ G^{-1}(x,0),0=F(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0)). f, 0=F(x,f(x)),x\in U.\boxed{} (1) (x,y)\in U\times V F(x,y)=0, f:U\to V f(x)=\pi_2\circ G^{-1}(x,0). Y=G(U\times V) G(x^0,y^0)=(x^0,F(x^0,y^0))=(x^0,0)\in Y, (x,0)\in Y, x\in U, G^{-1}(x,0) x\in U, f \forall x\in U","['real-analysis', 'multivariable-calculus', 'proof-explanation', 'implicit-function-theorem']"
93,show that $f$ is constant over the level sets of $\phi$ if $G = \nabla \phi $ and $fG$ is conservative,show that  is constant over the level sets of  if  and  is conservative,f \phi G = \nabla \phi  fG,"As the title suggests, i'm having trouble to show this. By premise, $G : \mathbb{R}^3 \rightarrow \mathbb{R}^3$ , $\phi: \mathbb{R}^3 \rightarrow \mathbb{R}$ twice continously differentiable and $f : \mathbb{R}^3 \to \mathbb{R}$ . The problem suggests using the following identity: $\nabla \times (fG) = \nabla f \times G \ + f(\nabla \times G)$ . By replacing $G = \nabla \phi$ , using that $fG$ is conservative, and that since $\phi \in C^2 \implies \nabla \times (\nabla \phi) = 0$ , it can be concluded that $\nabla f \times \nabla \phi = 0$ . Now, how can I use this fact to prove that $f$ is constant over the level sets of $\phi$ ?","As the title suggests, i'm having trouble to show this. By premise, , twice continously differentiable and . The problem suggests using the following identity: . By replacing , using that is conservative, and that since , it can be concluded that . Now, how can I use this fact to prove that is constant over the level sets of ?",G : \mathbb{R}^3 \rightarrow \mathbb{R}^3 \phi: \mathbb{R}^3 \rightarrow \mathbb{R} f : \mathbb{R}^3 \to \mathbb{R} \nabla \times (fG) = \nabla f \times G \ + f(\nabla \times G) G = \nabla \phi fG \phi \in C^2 \implies \nabla \times (\nabla \phi) = 0 \nabla f \times \nabla \phi = 0 f \phi,"['multivariable-calculus', 'vector-analysis']"
94,"Is $\frac{\partial}{\partial x} \cos{\sqrt{x^2+y^2}}$ indeterminate or zero at (0,0)?","Is  indeterminate or zero at (0,0)?",\frac{\partial}{\partial x} \cos{\sqrt{x^2+y^2}},"I'd like to know what's the derivative of $f(x,y)=\cos{\sqrt{x^2+y^2}}$ at $(0, 0)$ . WolframAlpha says it is indeterminate . However, if we apply the definition, we can actually evaluate it to zero: $$ f_x(0,0)   = \lim_{h\to 0} \dfrac{f(h,0) - f(0,0)}{h}   = \lim_{h\to0} \dfrac{\cos{|h|}-1}{h}   = \lim_{h\to0} \dfrac{\cos{h}-1}{h}   = 0 \\ f_y(0,0)   = \lim_{k\to 0} \dfrac{f(0,k) - f(0,0)}{k}   = \lim_{k\to0} \dfrac{\cos{|k|}-1}{k}   = \lim_{k\to0} \dfrac{\cos{k}-1}{k}   = 0 $$ Aside question: if Wolfram is wrong, how often it happens in your experience? Does it ever happen?","I'd like to know what's the derivative of at . WolframAlpha says it is indeterminate . However, if we apply the definition, we can actually evaluate it to zero: Aside question: if Wolfram is wrong, how often it happens in your experience? Does it ever happen?","f(x,y)=\cos{\sqrt{x^2+y^2}} (0, 0) 
f_x(0,0)
  = \lim_{h\to 0} \dfrac{f(h,0) - f(0,0)}{h}
  = \lim_{h\to0} \dfrac{\cos{|h|}-1}{h}
  = \lim_{h\to0} \dfrac{\cos{h}-1}{h}
  = 0
\\
f_y(0,0)
  = \lim_{k\to 0} \dfrac{f(0,k) - f(0,0)}{k}
  = \lim_{k\to0} \dfrac{\cos{|k|}-1}{k}
  = \lim_{k\to0} \dfrac{\cos{k}-1}{k}
  = 0
","['calculus', 'multivariable-calculus', 'wolfram-alpha']"
95,Maximizing linear objective subject to quadratic equality constraint,Maximizing linear objective subject to quadratic equality constraint,,"I've been trying to get through some practice questions on the Karush-Kuhn-Tucker (KKT) theorem but I can't seem to answer the following. Given $f, g : \mathbb{R}^2 \to \mathbb{R}$ defined by $f(x) := x_1 + x_2$ and $g(x) := x_1^2+3x_1x_2+3x_2^2-3$ , respectively, $$\begin{array}{ll} \underset{x \in \mathbb{R}^n}{\text{maximize}} & f(x)\\ \text{subject to} & g(x) = 0\end{array}$$ My attempt: $$\nabla f(x)=\begin{pmatrix} 1\\ 1\\ \end{pmatrix}$$ $$\nabla g(x)=\begin{pmatrix} 2x_1+3x_2\\ 3x_1+6x_2\\ \end{pmatrix}$$ and by complementary slackness $\lambda[x_1^2+3x_1x_2+3x_2^2-3]=0$ and $\lambda\geq0$ By first order conditions, I get $\lambda[2x_1+3x_2]=1$ and $\lambda[3x_1+6x_2]=1$ I checked WolframAlpha and the answer should be (3,-1) but I can't seem to figure out the right steps to solve this optimization.","I've been trying to get through some practice questions on the Karush-Kuhn-Tucker (KKT) theorem but I can't seem to answer the following. Given defined by and , respectively, My attempt: and by complementary slackness and By first order conditions, I get and I checked WolframAlpha and the answer should be (3,-1) but I can't seem to figure out the right steps to solve this optimization.","f, g : \mathbb{R}^2 \to \mathbb{R} f(x) := x_1 + x_2 g(x) := x_1^2+3x_1x_2+3x_2^2-3 \begin{array}{ll} \underset{x \in \mathbb{R}^n}{\text{maximize}} & f(x)\\ \text{subject to} & g(x) = 0\end{array} \nabla f(x)=\begin{pmatrix}
1\\
1\\
\end{pmatrix} \nabla g(x)=\begin{pmatrix}
2x_1+3x_2\\
3x_1+6x_2\\
\end{pmatrix} \lambda[x_1^2+3x_1x_2+3x_2^2-3]=0 \lambda\geq0 \lambda[2x_1+3x_2]=1 \lambda[3x_1+6x_2]=1","['multivariable-calculus', 'optimization', 'qclp']"
96,multivariable calculus divergence theorem help,multivariable calculus divergence theorem help,,"I am stuck on a problem: Use the Divergence Theorem to evaluate $\iint \mathbf{F} \cdot d\mathbf{S}$ , where $$\mathbf{F}(x,y,z)=z^2x\mathbf{i}+(\frac{1}{3}y^3+\tan(z))\mathbf{j}+(x^2z+y^2)\mathbf{k}$$ and $S$ is the top half of the sphere $x^2+y^2+z^2=1$ . [Hint: Note that $S$ is not a closed surface. First compute integrals over $S_1$ and $S_2$ , where $S_1$ is the disk $x^2+y^2\le 1$ , oriented downward, and $S_2 = S \cup S_1$ .] my working process: $\iint \mathbf{F} \cdot d\mathbf{S} = \iiint\limits_E div\mathbf{F} \cdot d\mathbf{V}  = \iiint\limits_E (x^2+y^2+z^2) d\mathbf{V}$ . for $S_2$ , parametrise $x=r\sin(\phi)\cos(\theta)$ , $y=r\sin(\phi)\sin(\theta)$ , $z=r\cos(\theta)$ and $0\le r \le 1,\;\; 0\le \theta \le 2\pi,\;\; 0\le \phi \le \pi.$ = $\iiint\limits_{S_2} (r^2\cos^2(\theta)+r^2\sin^2(\phi)) dV+\iint\limits_{S_1} \mathbf{F} \cdot d\mathbf{S}$ = $\frac{14\pi}{15}$ + $\iint\limits_{S_1} \mathbf{F} \cdot d\mathbf{S}$ (the correct answer is $\frac{13\pi}{20}$ ) could you point out the mistake? Thank you very much!!!","I am stuck on a problem: Use the Divergence Theorem to evaluate , where and is the top half of the sphere . [Hint: Note that is not a closed surface. First compute integrals over and , where is the disk , oriented downward, and .] my working process: . for , parametrise , , and = = + (the correct answer is ) could you point out the mistake? Thank you very much!!!","\iint \mathbf{F} \cdot d\mathbf{S} \mathbf{F}(x,y,z)=z^2x\mathbf{i}+(\frac{1}{3}y^3+\tan(z))\mathbf{j}+(x^2z+y^2)\mathbf{k} S x^2+y^2+z^2=1 S S_1 S_2 S_1 x^2+y^2\le 1 S_2 = S \cup S_1 \iint \mathbf{F} \cdot d\mathbf{S} = \iiint\limits_E div\mathbf{F} \cdot d\mathbf{V} 
= \iiint\limits_E (x^2+y^2+z^2) d\mathbf{V} S_2 x=r\sin(\phi)\cos(\theta) y=r\sin(\phi)\sin(\theta) z=r\cos(\theta) 0\le r \le 1,\;\; 0\le \theta \le 2\pi,\;\; 0\le \phi \le \pi. \iiint\limits_{S_2} (r^2\cos^2(\theta)+r^2\sin^2(\phi)) dV+\iint\limits_{S_1} \mathbf{F} \cdot d\mathbf{S} \frac{14\pi}{15} \iint\limits_{S_1} \mathbf{F} \cdot d\mathbf{S} \frac{13\pi}{20}","['multivariable-calculus', 'divergence-theorem']"
97,Changing the bounds in an improper double integral,Changing the bounds in an improper double integral,,"Suppose we have a double integral of the form $$ \int_0^{\infty} \int_0^{\infty} f(xy) g(x,y) \, dx \, dy $$ Let's suppose the functions behave well so that the integrals converge, etc. I'm trying to perform a substitution to change this to an iterated integral. If we set $y=\frac{t}{x}$ so that $dy=\frac{dt}{x}$ , would the bounds on the new variable $t$ also be zero and infinity, so we could write $$ \int_0^{\infty} \int_0^{\infty} f(xy) g(x,y) \, dx \, dy = \int_0^{\infty} \int_0^{\infty} f(t) g\left(x,\frac{t}{x}\right) \, dx \, \frac{dt}{x} = \int_0^{\infty} f(t) \int_0^{\infty} \frac{g \left(x,\frac{t}{x} \right)}{x} \, dx \, dt $$ or would the bounds on $t$ be something different? How should I think about this to determine what the correct bounds should be?","Suppose we have a double integral of the form Let's suppose the functions behave well so that the integrals converge, etc. I'm trying to perform a substitution to change this to an iterated integral. If we set so that , would the bounds on the new variable also be zero and infinity, so we could write or would the bounds on be something different? How should I think about this to determine what the correct bounds should be?"," \int_0^{\infty} \int_0^{\infty} f(xy) g(x,y) \, dx \, dy  y=\frac{t}{x} dy=\frac{dt}{x} t  \int_0^{\infty} \int_0^{\infty} f(xy) g(x,y) \, dx \, dy = \int_0^{\infty} \int_0^{\infty} f(t) g\left(x,\frac{t}{x}\right) \, dx \, \frac{dt}{x} = \int_0^{\infty} f(t) \int_0^{\infty} \frac{g \left(x,\frac{t}{x} \right)}{x} \, dx \, dt  t","['multivariable-calculus', 'improper-integrals']"
98,Volume between two cones with intersecting axes,Volume between two cones with intersecting axes,,"I have two cones with given vertex coordinates, axis direction and aperture angle. I know that the axes of the two cones are intersecting in a point (see the figure attached), which coordinates are straightforward to find. I'd like to find the volume of the intersection (green volume in the figure). Two intersecting cones sketch I made some lit search, but I found nothing and all the approaches I am using seems to fail. Is there a closed form or iterative method to compute that volume? Additionally, what if the cones are more than two?","I have two cones with given vertex coordinates, axis direction and aperture angle. I know that the axes of the two cones are intersecting in a point (see the figure attached), which coordinates are straightforward to find. I'd like to find the volume of the intersection (green volume in the figure). Two intersecting cones sketch I made some lit search, but I found nothing and all the approaches I am using seems to fail. Is there a closed form or iterative method to compute that volume? Additionally, what if the cones are more than two?",,"['multivariable-calculus', 'euclidean-geometry', 'volume']"
99,"volume of the solid which is the intersection of the solid sphere $x^2+y^2+z^2\leqq1 \; , 2x^2+y^2-2x=0$",volume of the solid which is the intersection of the solid sphere,"x^2+y^2+z^2\leqq1 \; , 2x^2+y^2-2x=0","i'm trying to find the volume of the solid which is the intersection of the solid sphere $x^2+y^2+z^2\leqq1 \; , 2x^2+y^2-2x=0$ My attempt: I tried with cylindrical coordinates $$x=rcos(\phi)\;,y=rsin(\phi)\;,z=z$$ $$(rcos(\phi))^2+(rsin(\phi))^2+z^2=r^2+z^2 \leqq 1 \Rightarrow -\sqrt{(1-z^2)} \leqq r \leqq \sqrt{(1-z^2)} $$ I'm stuck here, because i don't know how to use $2x^2+y^2-2x=0$ to find the other limits of integration.","i'm trying to find the volume of the solid which is the intersection of the solid sphere My attempt: I tried with cylindrical coordinates I'm stuck here, because i don't know how to use to find the other limits of integration.","x^2+y^2+z^2\leqq1 \; , 2x^2+y^2-2x=0 x=rcos(\phi)\;,y=rsin(\phi)\;,z=z (rcos(\phi))^2+(rsin(\phi))^2+z^2=r^2+z^2 \leqq 1 \Rightarrow -\sqrt{(1-z^2)} \leqq r \leqq \sqrt{(1-z^2)}  2x^2+y^2-2x=0","['multivariable-calculus', 'volume']"
