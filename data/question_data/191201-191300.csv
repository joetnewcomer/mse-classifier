,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Need help understanding if a function is increasing or decreasing,Need help understanding if a function is increasing or decreasing,,"I am working on understanding the following function: $$g(x) = \ln\Gamma\left(\frac{x}{4}\right) - \ln\Gamma\left(\frac{x}{5}+\frac{1}{2}\right) - \ln\Gamma\left(\frac{x}{20}+\frac{1}{2}\right) - 2(1.03883)\left(\sqrt{\frac{x}{4}}\right) - (1.03883)\left(\frac{x}{8}\right) +  \ln\Gamma \left(\frac{x}{10}\right) - \ln\Gamma\left(\frac{x}{12}+\frac{1}{2}\right) - \ln\Gamma \left(\frac{x}{60}+\frac{1}{2}\right)$$ Using this series for the digamma, I am using: $$\frac{d}{dx}(\ln\Gamma(x)) = \psi(x) = -\gamma + \sum_{k=0}^{\infty}\left(\frac{1}{k+1} - \frac{1}{k+x}\right) $$ So that: $$g'(x) = 2\gamma + \sum_{k=0}^{\infty}(\frac{-2}{k+1} + \frac{1}{k+\frac{x}{5}+\frac{1}{2}} + \frac{1}{k + \frac{x}{20} + \frac{1}{2}} - \frac{1}{k+\frac{x}{4}} + \frac{1}{k+\frac{x}{12} + \frac{1}{2}} + \frac{1}{k + \frac{x}{60} + \frac{1}{2}} - \frac{1}{k+\frac{x}{10}}) - \frac{0.519415}{\sqrt{x}} -0.12985375$$ On the surface, I can't see how I would prove this is increasing for $x \ge 214$?  Did I make a mistake in the derivative?  Is there some trick to show that this function increases for $x \ge 214$? Yet, when I look at Wolfram Alpha at $214 \le x \le 1000$, the graph , the function is increasing. Does it continue to increase for $x > 1000$? One thought that occurs to me is that I can analyze the following function: $$h(k) = \frac{-2}{k+1} + \frac{1}{k+\frac{x}{5}+\frac{1}{2}} + \frac{1}{k + \frac{x}{20} + \frac{1}{2}} - \frac{1}{k+\frac{x}{4}} + \frac{1}{k+\frac{x}{12} + \frac{1}{2}} + \frac{1}{k + \frac{x}{60} + \frac{1}{2}} - \frac{1}{k+\frac{x}{10}}$$ I will see if I can prove that at a certain point, the sum is increasing.  If I make progress with this approach, I will post my thinking as an answer. Thanks very much, -Larry Edit: I added a link to the graph based on a comment. The function starts increasing before $x$ gets to $214$.","I am working on understanding the following function: $$g(x) = \ln\Gamma\left(\frac{x}{4}\right) - \ln\Gamma\left(\frac{x}{5}+\frac{1}{2}\right) - \ln\Gamma\left(\frac{x}{20}+\frac{1}{2}\right) - 2(1.03883)\left(\sqrt{\frac{x}{4}}\right) - (1.03883)\left(\frac{x}{8}\right) +  \ln\Gamma \left(\frac{x}{10}\right) - \ln\Gamma\left(\frac{x}{12}+\frac{1}{2}\right) - \ln\Gamma \left(\frac{x}{60}+\frac{1}{2}\right)$$ Using this series for the digamma, I am using: $$\frac{d}{dx}(\ln\Gamma(x)) = \psi(x) = -\gamma + \sum_{k=0}^{\infty}\left(\frac{1}{k+1} - \frac{1}{k+x}\right) $$ So that: $$g'(x) = 2\gamma + \sum_{k=0}^{\infty}(\frac{-2}{k+1} + \frac{1}{k+\frac{x}{5}+\frac{1}{2}} + \frac{1}{k + \frac{x}{20} + \frac{1}{2}} - \frac{1}{k+\frac{x}{4}} + \frac{1}{k+\frac{x}{12} + \frac{1}{2}} + \frac{1}{k + \frac{x}{60} + \frac{1}{2}} - \frac{1}{k+\frac{x}{10}}) - \frac{0.519415}{\sqrt{x}} -0.12985375$$ On the surface, I can't see how I would prove this is increasing for $x \ge 214$?  Did I make a mistake in the derivative?  Is there some trick to show that this function increases for $x \ge 214$? Yet, when I look at Wolfram Alpha at $214 \le x \le 1000$, the graph , the function is increasing. Does it continue to increase for $x > 1000$? One thought that occurs to me is that I can analyze the following function: $$h(k) = \frac{-2}{k+1} + \frac{1}{k+\frac{x}{5}+\frac{1}{2}} + \frac{1}{k + \frac{x}{20} + \frac{1}{2}} - \frac{1}{k+\frac{x}{4}} + \frac{1}{k+\frac{x}{12} + \frac{1}{2}} + \frac{1}{k + \frac{x}{60} + \frac{1}{2}} - \frac{1}{k+\frac{x}{10}}$$ I will see if I can prove that at a certain point, the sum is increasing.  If I make progress with this approach, I will post my thinking as an answer. Thanks very much, -Larry Edit: I added a link to the graph based on a comment. The function starts increasing before $x$ gets to $214$.",,"['derivatives', 'logarithms', 'gamma-function']"
1,Polylogarithm - derivative with respect to order,Polylogarithm - derivative with respect to order,,Does anybody know where I could find the expression for $$\frac{\partial}{\partial s}\mathrm{Li}_s(z)\bigg|_{s=0}$$ or something similar?,Does anybody know where I could find the expression for $$\frac{\partial}{\partial s}\mathrm{Li}_s(z)\bigg|_{s=0}$$ or something similar?,,['derivatives']
2,Step in derivation of Euler-Lagrange equations of motion,Step in derivation of Euler-Lagrange equations of motion,,"From http://www.mathpages.com/home/kmath523/kmath523.htm Variations in $x,y,z$ and $X$ at constant $t$ are independent of $t$ (since each of these variables is strictly a function of $t$), so we have   $${\frac{\partial x}{\partial X}=\frac{\partial{\dot x}}{\partial{\dot X}}}$$ (This is just after equation (5) on the page.) I'm having trouble making sense of this.  If each of these variables is strictly a function of $t$, and $t$ is held constant, how does a time derivative (${\dot x}$) make sense? If it were to mean that $t$ is replaced with a constant after the differentiation, then I could take the example of $$ x(y)=X^{3/4}=t^{3}$$ $$ X(t)=t^{4}$$ I could then calculate $$ {\frac{\partial x}{\partial X}=\frac{\dot x}{\dot X} = \frac{3}{4t}}$$ and $$ {\frac{\partial {\dot x}}{\partial {\dot X}}=\frac{\ddot x}{\ddot X}=\frac{1}{2t}}$$ ...and those are not equal for any $t$. What is the justification for this step? Note: This is a follow up to my first question about this, but that error was resolved. Rates of change for functions dependent on same variable","From http://www.mathpages.com/home/kmath523/kmath523.htm Variations in $x,y,z$ and $X$ at constant $t$ are independent of $t$ (since each of these variables is strictly a function of $t$), so we have   $${\frac{\partial x}{\partial X}=\frac{\partial{\dot x}}{\partial{\dot X}}}$$ (This is just after equation (5) on the page.) I'm having trouble making sense of this.  If each of these variables is strictly a function of $t$, and $t$ is held constant, how does a time derivative (${\dot x}$) make sense? If it were to mean that $t$ is replaced with a constant after the differentiation, then I could take the example of $$ x(y)=X^{3/4}=t^{3}$$ $$ X(t)=t^{4}$$ I could then calculate $$ {\frac{\partial x}{\partial X}=\frac{\dot x}{\dot X} = \frac{3}{4t}}$$ and $$ {\frac{\partial {\dot x}}{\partial {\dot X}}=\frac{\ddot x}{\ddot X}=\frac{1}{2t}}$$ ...and those are not equal for any $t$. What is the justification for this step? Note: This is a follow up to my first question about this, but that error was resolved. Rates of change for functions dependent on same variable",,"['partial-differential-equations', 'derivatives', 'calculus-of-variations']"
3,Evaluate the derivative of an inverse function by using a table of values?,Evaluate the derivative of an inverse function by using a table of values?,,"Given the function and derivative values in the table below, evaluate  $\frac{d}{dx}f^{-1}(3)$ x: 1   2   3   4   5  f(x): 4   1   5   2   3 df/dx: 3  -1   4   0  -2 All I know is that the derivative of an inverse is $\frac{1}{f^\prime(f^{-1}(x))}$. Could anyone at least give me hints on how to use the table to my advantage? Thank you!","Given the function and derivative values in the table below, evaluate  $\frac{d}{dx}f^{-1}(3)$ x: 1   2   3   4   5  f(x): 4   1   5   2   3 df/dx: 3  -1   4   0  -2 All I know is that the derivative of an inverse is $\frac{1}{f^\prime(f^{-1}(x))}$. Could anyone at least give me hints on how to use the table to my advantage? Thank you!",,"['calculus', 'derivatives', 'inverse']"
4,Differentiate then solve or vice versa. Why is there a difference?,Differentiate then solve or vice versa. Why is there a difference?,,"The other day I stumbled upon the following problem. Start with a rectangular piece of card an integer number wide, by an   integer number long, with one of those values being prime.  Then cut   four identical squares from each corner of the card. Fold up the four   ""flaps"" to create an open topped box, where the corners have been cut   in such a way to allow the maximum volume possible for the box. Once   the box is made, it turns out that the base length is 4 times larger   than the base width. What are the dimensions of the original card? I did solve the problem. But I still have a question. Let $L$ and $W$ be the length and width of the original paper and $x$ the length of the cut-off square. The volume of the box is a function $V(L, W, x)$. $$V(L,W,x) = x(L-2x)(W-2x)\tag{1}$$ We also have that $$L-2x = 4(W-2x)\tag{2}$$ I solved the problem by first finding the derivative of $V(L,W,x)$ with respect to $x$ and then making use of the second formula, finding a relationship between $W$ and $L$, and considering they're both whole and one of them is prime, I got the correct answers for $W$ and $L$ ($W = 3$, $L = 8$). However, if I first substituted the second formula into $V(L,W,x)$ and then differentiate, I arrived at wrong results. I mean, if I first rewrite $V$ as $$V(W,x) = 4x(W-2x)^2$$ Differentiate, find $x$ expressed with $W$, and substitute in $(2)$, I get wrong results. Why?","The other day I stumbled upon the following problem. Start with a rectangular piece of card an integer number wide, by an   integer number long, with one of those values being prime.  Then cut   four identical squares from each corner of the card. Fold up the four   ""flaps"" to create an open topped box, where the corners have been cut   in such a way to allow the maximum volume possible for the box. Once   the box is made, it turns out that the base length is 4 times larger   than the base width. What are the dimensions of the original card? I did solve the problem. But I still have a question. Let $L$ and $W$ be the length and width of the original paper and $x$ the length of the cut-off square. The volume of the box is a function $V(L, W, x)$. $$V(L,W,x) = x(L-2x)(W-2x)\tag{1}$$ We also have that $$L-2x = 4(W-2x)\tag{2}$$ I solved the problem by first finding the derivative of $V(L,W,x)$ with respect to $x$ and then making use of the second formula, finding a relationship between $W$ and $L$, and considering they're both whole and one of them is prime, I got the correct answers for $W$ and $L$ ($W = 3$, $L = 8$). However, if I first substituted the second formula into $V(L,W,x)$ and then differentiate, I arrived at wrong results. I mean, if I first rewrite $V$ as $$V(W,x) = 4x(W-2x)^2$$ Differentiate, find $x$ expressed with $W$, and substitute in $(2)$, I get wrong results. Why?",,['derivatives']
5,Find coordinates where the tangent to the curve is horizontal.,Find coordinates where the tangent to the curve is horizontal.,,"I've been trying to figure out how can I solve this exercise but I haven't had much luck so far. Do you think you can help me out a bit? Pointing out what might I possibly be doing wrong? The exercise is as follows: Find the coordinates where the tangent to the curve is horizontal $$x^3+3xy+2y^2+4y=1$$ Given that it's difficult to solve for either x or y. I decided to differentiate implicitly. And here's what I got: $$- {3x^2+3y\over 4y+3x+4}=0 $$ In order to find the horizontal tangents, the first order differential must be zero, and for this case particularly: $$ 3x^2+3y=0 $$ Now, solving for x: $$x=\sqrt{-y}$$        $$x=-\sqrt{-y}$$ Which tells me that y must be positive. (Real field) But now I'm stuck there. Just looking at the answers I can't think of anything else but some numbers that might satisfy the equation; $(1,-1)$, $(-1,-1)$,$(0,0)$ But I wouldn't know how to get there, nor I know if those are the right coordinates. Can  you help me out? Thanks in advance.","I've been trying to figure out how can I solve this exercise but I haven't had much luck so far. Do you think you can help me out a bit? Pointing out what might I possibly be doing wrong? The exercise is as follows: Find the coordinates where the tangent to the curve is horizontal $$x^3+3xy+2y^2+4y=1$$ Given that it's difficult to solve for either x or y. I decided to differentiate implicitly. And here's what I got: $$- {3x^2+3y\over 4y+3x+4}=0 $$ In order to find the horizontal tangents, the first order differential must be zero, and for this case particularly: $$ 3x^2+3y=0 $$ Now, solving for x: $$x=\sqrt{-y}$$        $$x=-\sqrt{-y}$$ Which tells me that y must be positive. (Real field) But now I'm stuck there. Just looking at the answers I can't think of anything else but some numbers that might satisfy the equation; $(1,-1)$, $(-1,-1)$,$(0,0)$ But I wouldn't know how to get there, nor I know if those are the right coordinates. Can  you help me out? Thanks in advance.",,"['calculus', 'derivatives']"
6,Numerical differentiation issues,Numerical differentiation issues,,I've been using this to compute the first order derivative's value of a function $f$ in a given point: $$f'(x) = \frac{f(x+\epsilon) - f(x-\epsilon)}{2\epsilon}$$ For some $\epsilon = 0.0001$ or so. But when I try to use the same formula for higher order derivatives it gives odd results $$f''(x) = \frac{f'(x+\epsilon) - f'(x-\epsilon)}{2\epsilon}$$ $$f''(x) = \frac{\frac{f(x+\epsilon+\epsilon) - f(x-\epsilon+\epsilon)}{2\epsilon} - \frac{f(x+\epsilon-\epsilon) - f(x-\epsilon-\epsilon)}{2\epsilon}}{2\epsilon}$$ What am I doing wrong here?,I've been using this to compute the first order derivative's value of a function $f$ in a given point: $$f'(x) = \frac{f(x+\epsilon) - f(x-\epsilon)}{2\epsilon}$$ For some $\epsilon = 0.0001$ or so. But when I try to use the same formula for higher order derivatives it gives odd results $$f''(x) = \frac{f'(x+\epsilon) - f'(x-\epsilon)}{2\epsilon}$$ $$f''(x) = \frac{\frac{f(x+\epsilon+\epsilon) - f(x-\epsilon+\epsilon)}{2\epsilon} - \frac{f(x+\epsilon-\epsilon) - f(x-\epsilon-\epsilon)}{2\epsilon}}{2\epsilon}$$ What am I doing wrong here?,,"['numerical-methods', 'approximation', 'derivatives']"
7,Under which conditions the Leibniz rule is equivalent to defining the derivative as a limit?,Under which conditions the Leibniz rule is equivalent to defining the derivative as a limit?,,"Let $A$ and $B$ be two $\mathbb{R}$ -vector spaces, and $F$ the $\mathbb{R}$ -vector space of ""smooth enough"" functions between them, where: sum is defined pointwise, and by ""smooth enough"" I mean that all needed limits exist (I don't want to deal with weird cases). With these assumptions, I think you can define the set $D$ of all operators from $F$ to $F$ , of this form: $ d(f)(x) = \lim_{\epsilon \rightarrow 0} ((f(x+\epsilon v) - f(x)) / \epsilon ) $ .  (clearly there's one for each $v$ ) (Is this ill-defined somehow?) Now, take $B$ to be an algebra over $\mathbb{R}$ , with some vector product. Now $F$ is an algebra over $\mathbb{R}$ too, with the product defined pointwise. Clearly the definition of $D$ still applies. But, you can now define a new set of operators, $L$ , defined to be the set of all linear operators from $F$ to $F$ that satisfy the Leibniz rule, $l(fg) = l(f)g + fl(g)$ . My question is: Are $L$ and $D$ the same set? If yes, why? (For context: it looks like they are the same in the case of $A=\mathbb{R}^d$ , $B=\mathbb{R}$ .) If no (as it's more likely in general- since the definition of $D$ doesn't use the product at all!): Can you provide a counterexample? And most importantly: which additional conditions on $B$ do you have to impose for $D$ and $L$ to be the same? For example: Is it only true on $B=\mathbb{R}$ ? What about $B=\mathbb{C}$ ? What if $B$ has finite dimension? And finally (which is of course my real question, although it is only a soft question): why does $L$ turn out to be a more interesting way than $D$ to extend the concept of derivative to more abstract structures? What's interesting about derivatives over algebras? Any insight on this would be appreciated.","Let and be two -vector spaces, and the -vector space of ""smooth enough"" functions between them, where: sum is defined pointwise, and by ""smooth enough"" I mean that all needed limits exist (I don't want to deal with weird cases). With these assumptions, I think you can define the set of all operators from to , of this form: .  (clearly there's one for each ) (Is this ill-defined somehow?) Now, take to be an algebra over , with some vector product. Now is an algebra over too, with the product defined pointwise. Clearly the definition of still applies. But, you can now define a new set of operators, , defined to be the set of all linear operators from to that satisfy the Leibniz rule, . My question is: Are and the same set? If yes, why? (For context: it looks like they are the same in the case of , .) If no (as it's more likely in general- since the definition of doesn't use the product at all!): Can you provide a counterexample? And most importantly: which additional conditions on do you have to impose for and to be the same? For example: Is it only true on ? What about ? What if has finite dimension? And finally (which is of course my real question, although it is only a soft question): why does turn out to be a more interesting way than to extend the concept of derivative to more abstract structures? What's interesting about derivatives over algebras? Any insight on this would be appreciated.",A B \mathbb{R} F \mathbb{R} D F F  d(f)(x) = \lim_{\epsilon \rightarrow 0} ((f(x+\epsilon v) - f(x)) / \epsilon )  v B \mathbb{R} F \mathbb{R} D L F F l(fg) = l(f)g + fl(g) L D A=\mathbb{R}^d B=\mathbb{R} D B D L B=\mathbb{R} B=\mathbb{C} B L D,"['derivatives', 'differential-geometry', 'differential-algebra']"
8,Showing that a function is exponential by real analysis.,Showing that a function is exponential by real analysis.,,"Let $f:\left( 0,\infty \right) \rightarrow \mathbb{R} $ be a differentiable function strictly increasing. For every given $a\in\mathbb{R}$ the tangent line of $f$ at the point $(a,f(a))$ cuts the $x$ axis at the point $a-b$ , where $b$ is a constant. Show that $f$ is an exponential function. I can only get that $f(x)=bf'(x)$ for every $x\in(0,\infty)$ . One could think to resolve like in differential equations, but I need to use theorems from real analysis on derivatives. I think I need to use that $f$ is strictly increasing, but I don't know how. I would appreciate any help.","Let be a differentiable function strictly increasing. For every given the tangent line of at the point cuts the axis at the point , where is a constant. Show that is an exponential function. I can only get that for every . One could think to resolve like in differential equations, but I need to use theorems from real analysis on derivatives. I think I need to use that is strictly increasing, but I don't know how. I would appreciate any help.","f:\left( 0,\infty \right) \rightarrow \mathbb{R}  a\in\mathbb{R} f (a,f(a)) x a-b b f f(x)=bf'(x) x\in(0,\infty) f","['real-analysis', 'derivatives']"
9,Gauge integral on infinite-dimensional Banach space and differentiability,Gauge integral on infinite-dimensional Banach space and differentiability,,"Call $f:I\to F$ gauge integrable where $I = [a, b]$ is a compact interval and $F$ is a Banach space, if the usual definition holds like if $F = \mathbb{R}$ , just replace absolute value by norm. How can one prove the following? Theorem 1. If $f:I\to F$ is gauge integrable, $I = [a, b]$ then its indefinite integral $F(x) := \int_a^x f$ is differentiable a.e. Above theorem might not be true, it should be true when $F$ is finite-dimensional. Lemma 1 (Saks-Henstock). Let $f:I\to F$ be gauge integrable, $\varepsilon > 0$ and $\delta$ be a gauge such that for all $\delta$ -fine paritions $\mathcal{P}$ $$\left\|\int_I f - S(f, \mathcal{P})\right\|\leq \varepsilon.$$ If $\mathcal{Q} = \{(I_i, t_i)\}$ is a $\delta$ -fine subpartition of $I$ , then $$\left\| \sum_i \int_{I_i} f - S(f, \mathcal{Q})\right\| \leq \varepsilon.$$ Above lemma is fine as it is for any Banach space $F$ . Here $S(f, \mathcal{P})$ is the Riemann sum of $f$ with respect to (sub)partion $\mathcal{P}$ . Lemma 2. If $f, \varepsilon$ and $\delta$ are like above, $F = \mathbb{R}$ then $$\sum_i \left\|\int_{I_i} f - f(t_i)\ell(I_i)\right\| \leq 2\varepsilon.$$ Here $\ell(J)$ is length of $J$ , and for $F = \mathbb{C}$ lemma 2 holds with $2$ replaced by $4$ . The proof of theorem 1 for $F = \mathbb{R}$ uses Vitali's covering lemma and lemma 2, but as this seems to be unavailable when $F$ is an infinite-dimensional Banach space, how can I prove theorem 1 in this case, and is it even true?","Call gauge integrable where is a compact interval and is a Banach space, if the usual definition holds like if , just replace absolute value by norm. How can one prove the following? Theorem 1. If is gauge integrable, then its indefinite integral is differentiable a.e. Above theorem might not be true, it should be true when is finite-dimensional. Lemma 1 (Saks-Henstock). Let be gauge integrable, and be a gauge such that for all -fine paritions If is a -fine subpartition of , then Above lemma is fine as it is for any Banach space . Here is the Riemann sum of with respect to (sub)partion . Lemma 2. If and are like above, then Here is length of , and for lemma 2 holds with replaced by . The proof of theorem 1 for uses Vitali's covering lemma and lemma 2, but as this seems to be unavailable when is an infinite-dimensional Banach space, how can I prove theorem 1 in this case, and is it even true?","f:I\to F I = [a, b] F F = \mathbb{R} f:I\to F I = [a, b] F(x) := \int_a^x f F f:I\to F \varepsilon > 0 \delta \delta \mathcal{P} \left\|\int_I f - S(f, \mathcal{P})\right\|\leq \varepsilon. \mathcal{Q} = \{(I_i, t_i)\} \delta I \left\| \sum_i \int_{I_i} f - S(f, \mathcal{Q})\right\| \leq \varepsilon. F S(f, \mathcal{P}) f \mathcal{P} f, \varepsilon \delta F = \mathbb{R} \sum_i \left\|\int_{I_i} f - f(t_i)\ell(I_i)\right\| \leq 2\varepsilon. \ell(J) J F = \mathbb{C} 2 4 F = \mathbb{R} F","['integration', 'derivatives', 'banach-spaces', 'indefinite-integrals', 'gauge-integral']"
10,"Sufficient condition to apply integration by parts ""infinitely many times""","Sufficient condition to apply integration by parts ""infinitely many times""",,"My question is related to this one . Suppose we are trying to solve the following integral $$ \int f(x) g(x) dx,$$ where we know $f(x)$ is smooth, all of its derivatives are positive, and the sum of its derivatives $\sum_{n=0}^{\infty} f^{(n)}(x)$ converges. For the sake of simplicity, let's say $g(x)=e^x$ for now. Then if we naively apply integration by parts ""infinitely times"" with $u=f(x)$ and $v'=g(x)=e^x$ we get $$ \sum_{n=0}^{\infty} (-1)^n e^x f^{(n)}(x) = e^x \sum_{n=0}^{\infty} (-1)^n f^{(n)}(x). $$ Since the above series converges (by absolute value test and our assumption), were we justified in performing integration by parts infinitely times? In other words, is the condition that a series of a function's derivatives (which are all positive) converges, sufficient to performing integration by parts infinitely times (with $g(x)=e^x$ )? Now what about for a more general function $g(x)$ ?  What conditions are needed on $g(x)$ to allow infinite applications of integration by parts (with the same assumptions on $f(x)$ )?  Thanks to all in advance.","My question is related to this one . Suppose we are trying to solve the following integral where we know is smooth, all of its derivatives are positive, and the sum of its derivatives converges. For the sake of simplicity, let's say for now. Then if we naively apply integration by parts ""infinitely times"" with and we get Since the above series converges (by absolute value test and our assumption), were we justified in performing integration by parts infinitely times? In other words, is the condition that a series of a function's derivatives (which are all positive) converges, sufficient to performing integration by parts infinitely times (with )? Now what about for a more general function ?  What conditions are needed on to allow infinite applications of integration by parts (with the same assumptions on )?  Thanks to all in advance."," \int f(x) g(x) dx, f(x) \sum_{n=0}^{\infty} f^{(n)}(x) g(x)=e^x u=f(x) v'=g(x)=e^x  \sum_{n=0}^{\infty} (-1)^n e^x f^{(n)}(x) = e^x \sum_{n=0}^{\infty} (-1)^n f^{(n)}(x).  g(x)=e^x g(x) g(x) f(x)","['real-analysis', 'integration', 'sequences-and-series', 'derivatives', 'absolute-convergence']"
11,Undertanding rank-deficiencies of the differential of matrix function $f(A)=A^3$ in the space of $3 \times 3$ matrices,Undertanding rank-deficiencies of the differential of matrix function  in the space of  matrices,f(A)=A^3 3 \times 3,"This a follow-on of this question ""Q"" asked some days ago on Maths SE to which I had given an answer that will be denoted ""A"". This question ""Q"" was dealing with the mapping : $$f_{A_0}(X)=A_0^2X+A_0XA_0+XA_0^2$$ In fact (it wasn't mentionned in ""Q""), this mapping is, in the 9-dimensional space $\frak{M}_{3 \times 3}$ of $3 \times 3$ matrices, the differential of function $f:A \to A^3$ in ""point"" $A_0$ . As such $f_{A_0}$ is a linear function having a certain rank $r(f_{A_0})$ which can be maximal, i.e., equal to $9$ (for most $A_0$ ) or have a rank deficiency (equivalent naming : ""rank drop""), like $8$ for some $A_0$ or even less in very rare cases (see statistical study below). It will be easier to work on the complementary value of the rank, i.e., the dimension of the kernel : $$\underbrace{\dim(\ker(f_{A_0}))}_k = 9 - \underbrace{\operatorname{rank}(f_{A_0})}_r$$ Taking a pragmatic approach, I have done a large scale simulation on matrices $A$ having integer entries between $-3$ and $3$ which has given these approximate proportion of values for the dimension $k$ of the kernel of $f_{A_0}$ : $$\begin{array}{c|ccccccc}k&0&1&2&3&4&5&6 \\\hline &97 \% &2.6 \%&0.2\%&0.1 \%&0.0 \%&0.0 \%&0.1 \%\end{array}$$ Side remark : In fact the relative percentages aren't important ; the important fact is the ""holes"" in the spectrum  : no matrices for $k=4,5,7,8$ . I know that this approach doesn't meet the approval of some readers : this question has been downvoted probably for this reason, but I don't bother, I think that all the tools we now have, in particular simulation methods, are good to use in order to establish conjectures . Question 1 : Set apart the fact that in a large majority of cases this differential is injective, how is it possible to explain the presence of these particular cases ( $k=1, 2,3,6$ ) and the absence of other cases ? Let us enter into the details. Case $k=1$ : for matrices $A$ having a one-dimensional kernel, I observe on my simulations that this kernel is always with rank-1 matrices $X$ . Why ? Moreover, one doesn't see the rationale existing between $A$ and $X$ (btw, it was the main question in question ""Q""). Two examples of a matrix $A$ and an element $X$ of the kernel of $f_A$ : $$A=\begin{pmatrix}0&-1&1\\1&-2&2\\-1&2&2\end{pmatrix} \text{ and } X=\begin{pmatrix}0&0&0\\0&1&1\\0&1&1\end{pmatrix}$$ $$A=\begin{pmatrix}1&2&0\\2&1&0\\0&-2&0\end{pmatrix} \text{ and } X=\begin{pmatrix}0&0&0\\0&0&0\\4&-2&3\end{pmatrix}$$ Let us turn now to other cases $k=2,3,6$ where $X$ has (necessarily) rank $1$ : Case k=2 : examples $$A=\begin{pmatrix}0&1&0\\0&0&1\\1&0&0\end{pmatrix} \text{ and } X_1=\begin{pmatrix}0&-1&0\\0&0&1\\0&0&0\end{pmatrix}, X_2=\begin{pmatrix}-2&2&0\\0&1&2\\-4&0&1\end{pmatrix}$$ Case k=3 : example $$A=\begin{pmatrix}0&1&1\\1&0&0\\-1&1&0\end{pmatrix}$$ We are here in a particular case of a matrix where rank $(A)=2$ , rank $(A^2)=1$ and $A^3=A^2$ . Case k=6 : example $$A=\begin{pmatrix}-1&1&-2\\2&0&1\\2&1&1\end{pmatrix}.$$ This matrix has a very specific property $A^3=3I$ . In fact, all matrices of this category ( $k=6$ ) are such that $A^3=\mu A$ for a certain real number $A$ . Why that ? (Another example of this special case $k=6$ can be seen in my former answer ""A""). Question 2 (the main question in fact) How can the disparate observations seen above be understood and proved under a common globalizing explanation ?","This a follow-on of this question ""Q"" asked some days ago on Maths SE to which I had given an answer that will be denoted ""A"". This question ""Q"" was dealing with the mapping : In fact (it wasn't mentionned in ""Q""), this mapping is, in the 9-dimensional space of matrices, the differential of function in ""point"" . As such is a linear function having a certain rank which can be maximal, i.e., equal to (for most ) or have a rank deficiency (equivalent naming : ""rank drop""), like for some or even less in very rare cases (see statistical study below). It will be easier to work on the complementary value of the rank, i.e., the dimension of the kernel : Taking a pragmatic approach, I have done a large scale simulation on matrices having integer entries between and which has given these approximate proportion of values for the dimension of the kernel of : Side remark : In fact the relative percentages aren't important ; the important fact is the ""holes"" in the spectrum  : no matrices for . I know that this approach doesn't meet the approval of some readers : this question has been downvoted probably for this reason, but I don't bother, I think that all the tools we now have, in particular simulation methods, are good to use in order to establish conjectures . Question 1 : Set apart the fact that in a large majority of cases this differential is injective, how is it possible to explain the presence of these particular cases ( ) and the absence of other cases ? Let us enter into the details. Case : for matrices having a one-dimensional kernel, I observe on my simulations that this kernel is always with rank-1 matrices . Why ? Moreover, one doesn't see the rationale existing between and (btw, it was the main question in question ""Q""). Two examples of a matrix and an element of the kernel of : Let us turn now to other cases where has (necessarily) rank : Case k=2 : examples Case k=3 : example We are here in a particular case of a matrix where rank , rank and . Case k=6 : example This matrix has a very specific property . In fact, all matrices of this category ( ) are such that for a certain real number . Why that ? (Another example of this special case can be seen in my former answer ""A""). Question 2 (the main question in fact) How can the disparate observations seen above be understood and proved under a common globalizing explanation ?","f_{A_0}(X)=A_0^2X+A_0XA_0+XA_0^2 \frak{M}_{3 \times 3} 3 \times 3 f:A \to A^3 A_0 f_{A_0} r(f_{A_0}) 9 A_0 8 A_0 \underbrace{\dim(\ker(f_{A_0}))}_k = 9 - \underbrace{\operatorname{rank}(f_{A_0})}_r A -3 3 k f_{A_0} \begin{array}{c|ccccccc}k&0&1&2&3&4&5&6 \\\hline &97 \% &2.6 \%&0.2\%&0.1 \%&0.0 \%&0.0 \%&0.1 \%\end{array} k=4,5,7,8 k=1, 2,3,6 k=1 A X A X A X f_A A=\begin{pmatrix}0&-1&1\\1&-2&2\\-1&2&2\end{pmatrix} \text{ and } X=\begin{pmatrix}0&0&0\\0&1&1\\0&1&1\end{pmatrix} A=\begin{pmatrix}1&2&0\\2&1&0\\0&-2&0\end{pmatrix} \text{ and } X=\begin{pmatrix}0&0&0\\0&0&0\\4&-2&3\end{pmatrix} k=2,3,6 X 1 A=\begin{pmatrix}0&1&0\\0&0&1\\1&0&0\end{pmatrix} \text{ and } X_1=\begin{pmatrix}0&-1&0\\0&0&1\\0&0&0\end{pmatrix}, X_2=\begin{pmatrix}-2&2&0\\0&1&2\\-4&0&1\end{pmatrix} A=\begin{pmatrix}0&1&1\\1&0&0\\-1&1&0\end{pmatrix} (A)=2 (A^2)=1 A^3=A^2 A=\begin{pmatrix}-1&1&-2\\2&0&1\\2&1&1\end{pmatrix}. A^3=3I k=6 A^3=\mu A A k=6","['linear-algebra', 'derivatives', 'matrix-rank']"
12,Derivative of a pointwise limit of a sequence of functions,Derivative of a pointwise limit of a sequence of functions,,"It is easy to construct a sequence of differentiable functions $f_n(x)$ converging pointwise to a function that is not differentiable (a simple example is $\tan^{-1}(nx)$ ). And it is a theorem that if in addition the derivatives $f_n'(x)$ converge uniformly (on an interval $[a,b]$ for example), then  the limit function is differentiable with derivative the limit of the derivatives. I am looking for an example (if any exist) of a sequence of differentiable functions $f_n(x)$ that converges pointwise to a differentiable function $f(x)$ , and such that the sequence of derivatives $f_n'(x)$ also converges pointwise to a differentiable function $g(x)$ , but $f'\neq g$ . If such an example does not exist, I would like to see the proof of the corresponding theorem.","It is easy to construct a sequence of differentiable functions converging pointwise to a function that is not differentiable (a simple example is ). And it is a theorem that if in addition the derivatives converge uniformly (on an interval for example), then  the limit function is differentiable with derivative the limit of the derivatives. I am looking for an example (if any exist) of a sequence of differentiable functions that converges pointwise to a differentiable function , and such that the sequence of derivatives also converges pointwise to a differentiable function , but . If such an example does not exist, I would like to see the proof of the corresponding theorem.","f_n(x) \tan^{-1}(nx) f_n'(x) [a,b] f_n(x) f(x) f_n'(x) g(x) f'\neq g","['derivatives', 'uniform-convergence', 'sequence-of-function', 'pointwise-convergence']"
13,"If $f$ is continuous on $[0,1]$, $a\in (0,1)$, the rational derivative of $f$ at $a: \lim_{\Bbb Q\ni r_n\to a}\frac{f(a+r_n)-f(a)}{r_n}=A$.","If  is continuous on , , the rational derivative of  at .","f [0,1] a\in (0,1) f a: \lim_{\Bbb Q\ni r_n\to a}\frac{f(a+r_n)-f(a)}{r_n}=A","If $f$ is continuous on $[0,1]$ , $a\in (0,1)$ , the rational derivative of $f$ at $a: \lim\limits_{\Bbb Q\ni r_n\to0}\frac{f(a+r_n)-f(a)}{r_n}=A$ . Show $f$ is differentiable at $a$ . My attempt: For any $x_n\searrow 0$ , find $0<r_n<x_n<s_n$ such that $s_n\to 0$ . But $\dfrac{f(a+r_n)-f(a)}{r_n}, \dfrac{f(a+x_n)-f(a)}{x_n}, \dfrac{f(a+s_n)-f(a)}{s_n}$ is not compatible. How to use the continuity ?","If is continuous on , , the rational derivative of at . Show is differentiable at . My attempt: For any , find such that . But is not compatible. How to use the continuity ?","f [0,1] a\in (0,1) f a: \lim\limits_{\Bbb Q\ni r_n\to0}\frac{f(a+r_n)-f(a)}{r_n}=A f a x_n\searrow 0 0<r_n<x_n<s_n s_n\to 0 \dfrac{f(a+r_n)-f(a)}{r_n}, \dfrac{f(a+x_n)-f(a)}{x_n}, \dfrac{f(a+s_n)-f(a)}{s_n}","['calculus', 'derivatives', 'continuity']"
14,Rudin Exercise 18 Chapter 8 - Elegant solution?,Rudin Exercise 18 Chapter 8 - Elegant solution?,,"Exercise : Define $$f(x) = x^3 - \sin^2(x)\tan(x)$$ $$g(x) = 2x^2 - \sin^2(x) -x\tan(x)$$ Find out, for each of these two functions, whether it is positive or negative for all $x \in (0, \pi/2)$ , or whether it changes sign. Prove your answer. The problem The standard solution to this exercise requires to compute the derivatives of $f$ and $g$ as many times as six and five respectively, however I think that such an answer is neither illuminating nor typical for Rudin's exercises. While trying to solve it by myself I would have guessed the solution to rely on some clever inequality involving sine and tangent. In particular, I have noticed that the presence of $x^3$ and $2x^2$ is not random and this is something that the iterated derivatives do not highlight. Indeed, by the Mean Value Theorem, one can link the behavior of sine and tangent of $x$ to that of $x$ multiplied by some constant, so that, for instance, $\sin^2x\tan x$ behaves like a multiple of $x^3$ . I would like to know whether there is an actual elegant solution or if this exercise were mere computation. As always, any comment or answer is much appreciated and let me know if I can explain myself clearer!","Exercise : Define Find out, for each of these two functions, whether it is positive or negative for all , or whether it changes sign. Prove your answer. The problem The standard solution to this exercise requires to compute the derivatives of and as many times as six and five respectively, however I think that such an answer is neither illuminating nor typical for Rudin's exercises. While trying to solve it by myself I would have guessed the solution to rely on some clever inequality involving sine and tangent. In particular, I have noticed that the presence of and is not random and this is something that the iterated derivatives do not highlight. Indeed, by the Mean Value Theorem, one can link the behavior of sine and tangent of to that of multiplied by some constant, so that, for instance, behaves like a multiple of . I would like to know whether there is an actual elegant solution or if this exercise were mere computation. As always, any comment or answer is much appreciated and let me know if I can explain myself clearer!","f(x) = x^3 - \sin^2(x)\tan(x) g(x) = 2x^2 - \sin^2(x) -x\tan(x) x \in (0, \pi/2) f g x^3 2x^2 x x \sin^2x\tan x x^3","['real-analysis', 'calculus', 'derivatives', 'trigonometry', 'inequality']"
15,"Derivative of $f(r\cos\theta,r\sin\theta)$ at $(0,0)$",Derivative of  at,"f(r\cos\theta,r\sin\theta) (0,0)","Can someone help with any hints or a proof to this question? Question. Suppose $n>0$ , $r>0$ , and $0<\theta<2\pi$ . Define $f:\mathbb{R^2}\rightarrow \mathbb{R}$ by $$f(r\cos(\theta),r\sin(\theta))=r^n \cos(\theta) \ \text{ and } \ f(0,0)=0.$$ Determine for which $n$ , $f$ is partially differentiable with respect to $x$ and $y$ at $(0,0)$ . For which $n$ is it true that f is differentiable at $(0,0)$ ? Find the value of derivative at $(0,0)$ ？ Since $x=r\cos(\theta), y=r\sin(\theta$ ), I tried to replace $f(r\cos(\theta),rsin(\theta))=r^n \cos(\theta)$ by $\require{enclose}     \enclose{horizontalstrike}{f(x,y)=x(x^2+y^2)^{n-3}}$ $\color{red}{(f(x,y)=x(x^2+y^2)^{\frac{n-1}{2}}),}$ then computed the partial derivative by definition directly. I'm not sure if this process is right or not.","Can someone help with any hints or a proof to this question? Question. Suppose , , and . Define by Determine for which , is partially differentiable with respect to and at . For which is it true that f is differentiable at ? Find the value of derivative at ？ Since ), I tried to replace by then computed the partial derivative by definition directly. I'm not sure if this process is right or not.","n>0 r>0 0<\theta<2\pi f:\mathbb{R^2}\rightarrow \mathbb{R} f(r\cos(\theta),r\sin(\theta))=r^n \cos(\theta) \ \text{ and } \ f(0,0)=0. n f x y (0,0) n (0,0) (0,0) x=r\cos(\theta), y=r\sin(\theta f(r\cos(\theta),rsin(\theta))=r^n \cos(\theta) \require{enclose}
    \enclose{horizontalstrike}{f(x,y)=x(x^2+y^2)^{n-3}} \color{red}{(f(x,y)=x(x^2+y^2)^{\frac{n-1}{2}}),}","['real-analysis', 'calculus', 'derivatives', 'partial-derivative']"
16,Mapping from dual numbers to real numbers,Mapping from dual numbers to real numbers,,"Background I was naively playing around with some interpolation ideas once again and came across the dual numbers as a way to perform differentiation implicitly. Naturally, I thought, okay, perhaps there's some way for which we can map some dual numbers to the real numbers using a polynomial function. As it turns out in general, this is not the case, and I'm struggling to understand why. Symptoms For example, if we consider the function $f$ such that $f(0)=0$ and $f(\varepsilon)=1$ we know that if $f$ is an analytic function, then: $$ f(\varepsilon)=f(0)+f'(0)\cdot\varepsilon=1\implies f'(0)\cdot\varepsilon=1 $$ Obviously there is no solution to this. So perhaps we could employ the use of say, a Lagrange polynomial, which when simplified would give: $$ f(x) = \frac{x}{\varepsilon} $$ Again, this makes absolutely no sense; multiply numerator and denominator by $\varepsilon$ and we have division by zero. Under the real numbers this expression might 'work' (using $\frac{a}{a}=1$ , $\frac{0}{a}=0$ for non-zero $a$ ), but in this context, not so. As a last ditch, I thought we could use the matrix representation of a dual number; so we could reframe our problem as something like taking a function $F$ such that: \begin{align*} F\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \\ F\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}=\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \\ \end{align*} However, now the problem becomes that not all functions which would interpolate these values termwise (i.e. $A_{1,1}\to B_{1,1}$ ) are well-defined. Question Obviously up until now I've listed off a couple of things that I've tried, but I suspect they're merely symptoms of a same underlying problem with attempting to perform a mapping like this. Putting aside the feasibility of the outcome of something like this, is there any particular reason that this idea fails from the very beginning?","Background I was naively playing around with some interpolation ideas once again and came across the dual numbers as a way to perform differentiation implicitly. Naturally, I thought, okay, perhaps there's some way for which we can map some dual numbers to the real numbers using a polynomial function. As it turns out in general, this is not the case, and I'm struggling to understand why. Symptoms For example, if we consider the function such that and we know that if is an analytic function, then: Obviously there is no solution to this. So perhaps we could employ the use of say, a Lagrange polynomial, which when simplified would give: Again, this makes absolutely no sense; multiply numerator and denominator by and we have division by zero. Under the real numbers this expression might 'work' (using , for non-zero ), but in this context, not so. As a last ditch, I thought we could use the matrix representation of a dual number; so we could reframe our problem as something like taking a function such that: However, now the problem becomes that not all functions which would interpolate these values termwise (i.e. ) are well-defined. Question Obviously up until now I've listed off a couple of things that I've tried, but I suspect they're merely symptoms of a same underlying problem with attempting to perform a mapping like this. Putting aside the feasibility of the outcome of something like this, is there any particular reason that this idea fails from the very beginning?","f f(0)=0 f(\varepsilon)=1 f 
f(\varepsilon)=f(0)+f'(0)\cdot\varepsilon=1\implies f'(0)\cdot\varepsilon=1
 
f(x) = \frac{x}{\varepsilon}
 \varepsilon \frac{a}{a}=1 \frac{0}{a}=0 a F \begin{align*}
F\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \\
F\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}=\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \\
\end{align*} A_{1,1}\to B_{1,1}","['derivatives', 'interpolation', 'hypercomplex-numbers']"
17,"Formal derivative, any problems for non-commutative rings?","Formal derivative, any problems for non-commutative rings?",,"Let $R$ be a ring and $R\left[ x \right]$ be the ring of polynomials over $R$ . If $f=a_nx^n+\cdots+a_0 \in R\left[ x \right]$ , we define the formal derivative $f^{'}=na_nx^{n-1}+\cdots+a_1$ . Let $f$ , $g$ $\in R\left[ x \right]$ and $c \in R$ . Using the definition, it can be verified that: $(f+g)^{'}=f^{'}+g^{'}$ $(cf)^{'}=cf^{'}$ $(f+g)^{'}=f^{'}+g^{'}$ However, I came across this passage on Wikipedia : There is a problem with this definition for noncommutative rings. The formula itself is correct, but there is no standard form of a polynomial. Therefore using this definition it is difficult to prove that $(f(x)\cdot b)^{'}=f^{'}(x)\cdot b$ . This makes me confused. I don't know what it is talking about. I don't see any problems here. The proofs of 1,2,3 for commutative rings and noncommutative rings are all the same. For example, I can easily prove that $(f\cdot b)^{'}=f^{'}\cdot b$ : Let $f_k$ denote the k-th coefficient of the polynomial $f$ , $k=0,1,2\ldots$ . By definition we know $(f^{'})_k=(k+1)f_{k+1}$ . So $(LHS)_k=(k+1)(f\cdot b)_{k+1}=(k+1)(f_{k+1}\cdot b)=((k+1)f_{k+1})\cdot b=(f^{'})_k\cdot b=(f^{'}\cdot b)_k=(RHS)_k$ . QED. Where did I go wrong? What does the paragraph on Wikipedia mean? Any insights are much appreciated.","Let be a ring and be the ring of polynomials over . If , we define the formal derivative . Let , and . Using the definition, it can be verified that: However, I came across this passage on Wikipedia : There is a problem with this definition for noncommutative rings. The formula itself is correct, but there is no standard form of a polynomial. Therefore using this definition it is difficult to prove that . This makes me confused. I don't know what it is talking about. I don't see any problems here. The proofs of 1,2,3 for commutative rings and noncommutative rings are all the same. For example, I can easily prove that : Let denote the k-th coefficient of the polynomial , . By definition we know . So . QED. Where did I go wrong? What does the paragraph on Wikipedia mean? Any insights are much appreciated.","R R\left[ x \right] R f=a_nx^n+\cdots+a_0 \in R\left[ x \right] f^{'}=na_nx^{n-1}+\cdots+a_1 f g \in R\left[ x \right] c \in R (f+g)^{'}=f^{'}+g^{'} (cf)^{'}=cf^{'} (f+g)^{'}=f^{'}+g^{'} (f(x)\cdot b)^{'}=f^{'}(x)\cdot b (f\cdot b)^{'}=f^{'}\cdot b f_k f k=0,1,2\ldots (f^{'})_k=(k+1)f_{k+1} (LHS)_k=(k+1)(f\cdot b)_{k+1}=(k+1)(f_{k+1}\cdot b)=((k+1)f_{k+1})\cdot b=(f^{'})_k\cdot b=(f^{'}\cdot b)_k=(RHS)_k","['abstract-algebra', 'derivatives', 'polynomials', 'ring-theory', 'noncommutative-algebra']"
18,Is the multivector derivative invariant under a change of coordinates?,Is the multivector derivative invariant under a change of coordinates?,,"The multivector derivative $\partial _X \equiv e^I \partial_I \equiv \partial_{\langle X \rangle_0} + e^1\partial_{X_1} + e^2 \partial_{X_2}+\dots + e^{1,2,3} \partial_{X_{1,2,3}}$ with respect to some multivector variable is defined as the linear combination of all of the basis k-vectors with the components being the partial derivative operators with respect to the variable’s components. My question is if the operator would give the same results if you changed the coordinates, or in other words, if the following statement is true: $\partial_{X}\equiv e^I \partial_I \equiv \partial_{\langle X \rangle_0} + e^1\partial_{X_1} + e^2 \partial_{X_2}+\dots + e^{1,2,3} \partial_{X_{1,2,3}} = \partial_{\langle X \rangle_0}+ e’^1\partial_{X’_1} + e’^2 \partial_{X’_2}+\dots + e’^{1,2,3} \partial_{X’_{1,2,3}}$","The multivector derivative with respect to some multivector variable is defined as the linear combination of all of the basis k-vectors with the components being the partial derivative operators with respect to the variable’s components. My question is if the operator would give the same results if you changed the coordinates, or in other words, if the following statement is true:","\partial _X \equiv e^I \partial_I \equiv \partial_{\langle X \rangle_0} + e^1\partial_{X_1} + e^2 \partial_{X_2}+\dots + e^{1,2,3} \partial_{X_{1,2,3}} \partial_{X}\equiv e^I \partial_I \equiv \partial_{\langle X \rangle_0} + e^1\partial_{X_1} + e^2 \partial_{X_2}+\dots + e^{1,2,3} \partial_{X_{1,2,3}} = \partial_{\langle X \rangle_0}+ e’^1\partial_{X’_1} + e’^2 \partial_{X’_2}+\dots + e’^{1,2,3} \partial_{X’_{1,2,3}}","['derivatives', 'clifford-algebras', 'geometric-algebras']"
19,Common ways Leibniz notation is abused,Common ways Leibniz notation is abused,,"I've listed out a couple of instances where Leibniz's notation is abused, sometimes with nasty consequences. What are some other situations where Leibniz's notation is abused? Chain Rule Let $u = g(x)$ and $y = f(u)$ , then: $$(f(g(x)))' = \frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}$$ Even with differential forms, it is still a bit awkward. Let $u(x) = g(x)$ and $y(x) = f(u)$ , then: $$\frac{dy}{dx} = \frac{ \frac{df}{du} du}{dx} = \frac{ \frac{df}{du} \frac{du}{dx} dx}{dx} = \frac{df}{du} \frac{du}{dx}$$ There's plenty of good answers which discuss its problems and alternatives. But just to add on to it, here's a common example of its abuse: $$\frac{dy}{dx} = \frac{dy}{dt} \frac{dt}{dx} = \frac{dy}{dt} \frac{1}{ \frac{dx}{dt} } = \frac{y'(t)}{x'(t)}$$ This method is commonly shown in multivariable calculus courses when ""deriving"" curvature. Often called the dummy variable approach to ""deriving"" curvature. It's featured in LibreTexts and plenty of educational YouTube videos ""deriving"" curvature. An actual derivation using Lagrange's notation is significantly more enlightening. Multivariable Chain Rule for implicit functions In Leibniz's notation, we can obtain the partial derivative for the implicit function $F(x, y, z) = 0$ as: $$\frac{\partial z}{\partial x} = - \frac{\partial F / \partial x}{\partial F / \partial z}$$ Which can cause the contradiction: \begin{aligned} \frac{\partial z}{\partial x} = - \frac{\partial F / \partial x}{\partial F / \partial z} \\ \frac{\partial z}{\partial x} = - \frac{\partial F}{\partial x} \frac{\partial z}{\partial F} \\ \frac{\partial z}{\partial x} = - \frac{\partial z}{\partial x} \end{aligned} In Lagrange's notation, this is a non problem: $(z)'_x = - \frac{F'_x}{F'_z}$ . This kind of contradiction has been discussed in, other answers . The contradiction appears due to a misunderstanding/abuse of Leibniz notation which is very easy to make. Integration by substitution Integration by substitution, or u-substitution, often includes a similar process. As an example: $$\int (x+1)^2\ dx$$ Let $u=x+1$ , then $\frac{du}{dx} = 1 \implies dx = \frac{du}{1}$ $$\int (x+1)^2\ dx = \int (u)^2\ \frac{du}{1}$$ Which is a helpful mnemonic, yet the same rule can be derived quite nicely from the  chain rule, as it's simply dividing out the $g'(x)$ produced by the chain rule. Again, this has been discussed in other questions . Separation of variables This one is named after the abuse of notation. Unfortunately, it rather obscures the fact that it's simply integration by substitution: $$\frac{dy}{dx} = g(x) \cdot h(y)$$ $$dy = g(x) \cdot h(y) \cdot dx$$ $$\frac{1}{h(y)} dy = g(x) \ dx$$ $$\int \frac{1}{h(y)} dy = \int g(x) \ dx$$ This whole splitting step does almost seem mystical , there are plenty of great answers and alternatives to this method. It is unfortunate however that the name ""Separation of variable"" has stuck, which goes to show how prevalent the abuse of notation is. Green's theorem $$\int_C Pdx + Qdy = \int_C (Pdx + Qdy) \frac{dt}{dt} = \int_C \left(P\frac{dx}{dt} + Q\frac{dy}{dt}\right) dt$$ While a bit unfair, since it is simply a differential form, the reasoning offered to students can only be characterised as an abuse of notation. Personally, I feel that hinting/suggesting differential forms is unnecessary and unhelpful in most multivariable calculus course.","I've listed out a couple of instances where Leibniz's notation is abused, sometimes with nasty consequences. What are some other situations where Leibniz's notation is abused? Chain Rule Let and , then: Even with differential forms, it is still a bit awkward. Let and , then: There's plenty of good answers which discuss its problems and alternatives. But just to add on to it, here's a common example of its abuse: This method is commonly shown in multivariable calculus courses when ""deriving"" curvature. Often called the dummy variable approach to ""deriving"" curvature. It's featured in LibreTexts and plenty of educational YouTube videos ""deriving"" curvature. An actual derivation using Lagrange's notation is significantly more enlightening. Multivariable Chain Rule for implicit functions In Leibniz's notation, we can obtain the partial derivative for the implicit function as: Which can cause the contradiction: In Lagrange's notation, this is a non problem: . This kind of contradiction has been discussed in, other answers . The contradiction appears due to a misunderstanding/abuse of Leibniz notation which is very easy to make. Integration by substitution Integration by substitution, or u-substitution, often includes a similar process. As an example: Let , then Which is a helpful mnemonic, yet the same rule can be derived quite nicely from the  chain rule, as it's simply dividing out the produced by the chain rule. Again, this has been discussed in other questions . Separation of variables This one is named after the abuse of notation. Unfortunately, it rather obscures the fact that it's simply integration by substitution: This whole splitting step does almost seem mystical , there are plenty of great answers and alternatives to this method. It is unfortunate however that the name ""Separation of variable"" has stuck, which goes to show how prevalent the abuse of notation is. Green's theorem While a bit unfair, since it is simply a differential form, the reasoning offered to students can only be characterised as an abuse of notation. Personally, I feel that hinting/suggesting differential forms is unnecessary and unhelpful in most multivariable calculus course.","u = g(x) y = f(u) (f(g(x)))' = \frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx} u(x) = g(x) y(x) = f(u) \frac{dy}{dx} = \frac{ \frac{df}{du} du}{dx} = \frac{ \frac{df}{du} \frac{du}{dx} dx}{dx} = \frac{df}{du} \frac{du}{dx} \frac{dy}{dx} = \frac{dy}{dt} \frac{dt}{dx} = \frac{dy}{dt} \frac{1}{ \frac{dx}{dt} } = \frac{y'(t)}{x'(t)} F(x, y, z) = 0 \frac{\partial z}{\partial x} = - \frac{\partial F / \partial x}{\partial F / \partial z} \begin{aligned}
\frac{\partial z}{\partial x} = - \frac{\partial F / \partial x}{\partial F / \partial z} \\
\frac{\partial z}{\partial x} = - \frac{\partial F}{\partial x} \frac{\partial z}{\partial F} \\
\frac{\partial z}{\partial x} = - \frac{\partial z}{\partial x}
\end{aligned} (z)'_x = - \frac{F'_x}{F'_z} \int (x+1)^2\ dx u=x+1 \frac{du}{dx} = 1 \implies dx = \frac{du}{1} \int (x+1)^2\ dx = \int (u)^2\ \frac{du}{1} g'(x) \frac{dy}{dx} = g(x) \cdot h(y) dy = g(x) \cdot h(y) \cdot dx \frac{1}{h(y)} dy = g(x) \ dx \int \frac{1}{h(y)} dy = \int g(x) \ dx \int_C Pdx + Qdy = \int_C (Pdx + Qdy) \frac{dt}{dt} = \int_C \left(P\frac{dx}{dt} + Q\frac{dy}{dt}\right) dt","['calculus', 'derivatives', 'notation']"
20,"Inverse function theorem, Tao, Analysis II","Inverse function theorem, Tao, Analysis II",,"In Analysis II by Tao, he wrote: Theorem 6.7.2 (Inverse function theorem). Let $E$ be an open subset of $\mathbf{R}^n$ , and let $f : E \to \mathbf{R}^n$ be a function which is continuously differentiable on $E$ . Suppose $x_0 \in E$ is such that linear transformation $f'(x_0) : \mathbf{R}^n \to \mathbf{R}^n$ is invertible. Then there exists an open set $U$ in $E$ containing $x_0$ , and an open set $V$ in $\mathbf{R}^n$ containing $f(x_0)$ , such that $f$ is a bijection from $U$ to $V$ . In particular, there is an inverse map $f^{-1} : V \to U$ . Furthermore, this inverse map is differentiable at $f(x_0)$ , and $$ (f^{-1})'(f(x_0)) = (f'(x_0))^{-1}. $$ From errta of this book on his blog, he wrote: Exercise 6.7.4. Let the notation and hypotheses be as in Theorem 6.7.2. Show that, after shrinking the open sets U, V if necessary (while still having $x_0 \in U$ , $f(x_0) \in V$ of course), the derivative map $f'(x)$ is invertible for all $x \in U$ , and that the inverse map $f^{-1}$ is differentiable at every point of $V$ with $(f^{-1})'(f(x)) = (f'(x))^{-1}$ for all $x \in U$ . Finally, show that $f^{-1}$ is continuously differentiable on $V$ . I try to solve this exercise by looking for the relation between $f'(x)$ and $f'(x_0)$ for $x$ near $x_0$ , but so far I got nothing. I also try to google proofs, turn out the exercise I'm tring to proof is given as a part of the hypothesis . To conclude, I am looking for the answers of the following questions: How to proof the exercise for functions $f : \mathbf{R}^n \to \mathbf{R}^n$ ? (Tao already explained the case $f : \mathbf{R} \to \mathbf{R}$ .) More specifically, proof the exercise without using some prior knowledge in linear algebra just like the way Tao proof the inverse function theorem (like determinants, etc. I did not learn linear algebra before). How to interpret the result of the exercises (with graph) ? Which parts of the given hypotheses are necessary ?","In Analysis II by Tao, he wrote: Theorem 6.7.2 (Inverse function theorem). Let be an open subset of , and let be a function which is continuously differentiable on . Suppose is such that linear transformation is invertible. Then there exists an open set in containing , and an open set in containing , such that is a bijection from to . In particular, there is an inverse map . Furthermore, this inverse map is differentiable at , and From errta of this book on his blog, he wrote: Exercise 6.7.4. Let the notation and hypotheses be as in Theorem 6.7.2. Show that, after shrinking the open sets U, V if necessary (while still having , of course), the derivative map is invertible for all , and that the inverse map is differentiable at every point of with for all . Finally, show that is continuously differentiable on . I try to solve this exercise by looking for the relation between and for near , but so far I got nothing. I also try to google proofs, turn out the exercise I'm tring to proof is given as a part of the hypothesis . To conclude, I am looking for the answers of the following questions: How to proof the exercise for functions ? (Tao already explained the case .) More specifically, proof the exercise without using some prior knowledge in linear algebra just like the way Tao proof the inverse function theorem (like determinants, etc. I did not learn linear algebra before). How to interpret the result of the exercises (with graph) ? Which parts of the given hypotheses are necessary ?","E \mathbf{R}^n f : E \to \mathbf{R}^n E x_0 \in E f'(x_0) : \mathbf{R}^n \to \mathbf{R}^n U E x_0 V \mathbf{R}^n f(x_0) f U V f^{-1} : V \to U f(x_0) 
(f^{-1})'(f(x_0)) = (f'(x_0))^{-1}.
 x_0 \in U f(x_0) \in V f'(x) x \in U f^{-1} V (f^{-1})'(f(x)) = (f'(x))^{-1} x \in U f^{-1} V f'(x) f'(x_0) x x_0 f : \mathbf{R}^n \to \mathbf{R}^n f : \mathbf{R} \to \mathbf{R}","['real-analysis', 'derivatives', 'differential-geometry', 'inverse-function-theorem']"
21,Exercise about Schwarz derivative,Exercise about Schwarz derivative,,"If $f$ is three times differentiable and $f'(x)\neq 0$ , the Schwarz derivative of $f$ at $x$ is defined by $$\mathscr{D}f(x)=\dfrac{f'''(x)}{f'(x)}-\dfrac32\left( \dfrac{f''(x)}{f'(x)}\right)^2.$$ (a) Demonstrate that $$\mathscr{D}(f\circ g)=[\mathscr{D}f\circ g]\cdot g'^2+\mathscr{D}g.$$ My solution: using the chain rule and the product rule and, we obtain $$(f\circ g)'(x)=f'(g(x))\cdot g'(x)$$ $$(f\circ g)''(x)=f''(g(x))\cdot (g'(x))^2+f'(g(x))\cdot g''(x)$$ $$(f\circ g)'''(x)=f'''(g(x))\cdot (g'(x))^3+2f''(g(x))\cdot g'(x)g''(x)+f''(g(x))\cdot g''(x)g'(x)+f'(g(x))\cdot g'''(x)=f'''(g(x))\cdot (g'(x))^3+3f''(g(x))\cdot g'(x)g''(x)+f'(g(x))\cdot g'''(x)$$ \begin{align*} 	\mathscr{D}(f\circ g) &= \dfrac{(f\circ g)'''(x)}{(f\circ g)'(x)}-\dfrac32\left( \dfrac{(f\circ g)''(x)}{(f\circ g)'(x)}\right)^2=\\ &= \dfrac{f'''(g(x))\cdot (g'(x))^3+3f''(g(x))\cdot g'(x)g''(x)+f'(g(x))\cdot g'''(x)}{f'(g(x))\cdot g'(x)}-\dfrac32 \left( \dfrac{f''(g(x))\cdot (g'(x))^2+f'(g(x))\cdot g''(x)}{f'(g(x))\cdot g'(x)}\right)^2 =\\ &= \dfrac{f'''(g(x))}{f'(g(x))}(g'(x))^2+3\dfrac{f''(g(x))}{f'(g(x))}g''(x)+\dfrac{g'''(x)}{g'(x)}-\dfrac32 \left( \left( \dfrac{f''(g(x))}{f'(g(x))}\right)^2(g'(x))^2+2\dfrac{f''(g(x))}{f'(g(x))}g''(x)+\left( \dfrac{g''(x)}{g'(x)}\right)^2\right)=\\ &= \left( \dfrac{f'''(g(x))}{f'(g(x))}-\dfrac32 \left( \dfrac{f''(g(x))}{f'(g(x))}\right) \right) (g'(x))^2+\left( \dfrac{g'''(x)}{g'(x)}-\dfrac32 \left( \dfrac{g''(x)}{g'(x)}\right)^2 \right) =[\mathscr{D}f\circ g]\cdot (g')^2+\mathscr{D}g \end{align*} (b) Show that if $f(x)=\dfrac{ax+b}{cx+d}$ , with $ad-bc\neq 0$ , then $\mathscr{D}f=0$ . Therefore, $\mathscr{D}(f\circ g)=\mathscr{D}g$ . My solution: $f(x)=\dfrac{ax+b}{cx+d}$ , where $ad-bc\neq 0$ . We have $f(x)=\dfrac{a}{c}-\dfrac{ad-bc}{c(cx+d)}$ . We can find $f'$ , $f''$ , and $f'''$ : $$f'(x)=\dfrac{ad-bc}{(cx+d)^2}$$ $$f''(x)=-2c\cdot \dfrac{ad-bc}{(cx+d)^3}$$ $$f'''(x)=6c^2\cdot \dfrac{ad-bc}{(cx+d)^4}$$ Then we have $\dfrac{f'''(x)}{f'(x)}=\dfrac{6c^2}{(cx+d)^2}$ and $\dfrac{f''(x)}{f'(x)}=-\dfrac{2c}{cx+d}$ . $$\mathscr{D}f(x)=\dfrac{f'''(x)}{f'(x)}-\dfrac32\left( \dfrac{f''(x)}{f'(x)}\right)^2 =\dfrac{6c^2}{(cx+d)^2}-\dfrac32\dfrac{4c^2}{(cx+d)^2}=0$$ Therefore, $\mathscr{D}(f\circ g)=[\mathscr{D}f\circ g]\cdot (g')^2+\mathscr{D}g=0\cdot (g')^2+\mathscr{D}g=\mathscr{D}g$ . Is my solution correct? Anything to improve or comment on?","If is three times differentiable and , the Schwarz derivative of at is defined by (a) Demonstrate that My solution: using the chain rule and the product rule and, we obtain (b) Show that if , with , then . Therefore, . My solution: , where . We have . We can find , , and : Then we have and . Therefore, . Is my solution correct? Anything to improve or comment on?","f f'(x)\neq 0 f x \mathscr{D}f(x)=\dfrac{f'''(x)}{f'(x)}-\dfrac32\left( \dfrac{f''(x)}{f'(x)}\right)^2. \mathscr{D}(f\circ g)=[\mathscr{D}f\circ g]\cdot g'^2+\mathscr{D}g. (f\circ g)'(x)=f'(g(x))\cdot g'(x) (f\circ g)''(x)=f''(g(x))\cdot (g'(x))^2+f'(g(x))\cdot g''(x) (f\circ g)'''(x)=f'''(g(x))\cdot (g'(x))^3+2f''(g(x))\cdot g'(x)g''(x)+f''(g(x))\cdot g''(x)g'(x)+f'(g(x))\cdot g'''(x)=f'''(g(x))\cdot (g'(x))^3+3f''(g(x))\cdot g'(x)g''(x)+f'(g(x))\cdot g'''(x) \begin{align*}
	\mathscr{D}(f\circ g) &= \dfrac{(f\circ g)'''(x)}{(f\circ g)'(x)}-\dfrac32\left( \dfrac{(f\circ g)''(x)}{(f\circ g)'(x)}\right)^2=\\
&= \dfrac{f'''(g(x))\cdot (g'(x))^3+3f''(g(x))\cdot g'(x)g''(x)+f'(g(x))\cdot g'''(x)}{f'(g(x))\cdot g'(x)}-\dfrac32 \left( \dfrac{f''(g(x))\cdot (g'(x))^2+f'(g(x))\cdot g''(x)}{f'(g(x))\cdot g'(x)}\right)^2 =\\
&= \dfrac{f'''(g(x))}{f'(g(x))}(g'(x))^2+3\dfrac{f''(g(x))}{f'(g(x))}g''(x)+\dfrac{g'''(x)}{g'(x)}-\dfrac32 \left( \left( \dfrac{f''(g(x))}{f'(g(x))}\right)^2(g'(x))^2+2\dfrac{f''(g(x))}{f'(g(x))}g''(x)+\left( \dfrac{g''(x)}{g'(x)}\right)^2\right)=\\
&= \left( \dfrac{f'''(g(x))}{f'(g(x))}-\dfrac32 \left( \dfrac{f''(g(x))}{f'(g(x))}\right) \right) (g'(x))^2+\left( \dfrac{g'''(x)}{g'(x)}-\dfrac32 \left( \dfrac{g''(x)}{g'(x)}\right)^2 \right) =[\mathscr{D}f\circ g]\cdot (g')^2+\mathscr{D}g
\end{align*} f(x)=\dfrac{ax+b}{cx+d} ad-bc\neq 0 \mathscr{D}f=0 \mathscr{D}(f\circ g)=\mathscr{D}g f(x)=\dfrac{ax+b}{cx+d} ad-bc\neq 0 f(x)=\dfrac{a}{c}-\dfrac{ad-bc}{c(cx+d)} f' f'' f''' f'(x)=\dfrac{ad-bc}{(cx+d)^2} f''(x)=-2c\cdot \dfrac{ad-bc}{(cx+d)^3} f'''(x)=6c^2\cdot \dfrac{ad-bc}{(cx+d)^4} \dfrac{f'''(x)}{f'(x)}=\dfrac{6c^2}{(cx+d)^2} \dfrac{f''(x)}{f'(x)}=-\dfrac{2c}{cx+d} \mathscr{D}f(x)=\dfrac{f'''(x)}{f'(x)}-\dfrac32\left( \dfrac{f''(x)}{f'(x)}\right)^2 =\dfrac{6c^2}{(cx+d)^2}-\dfrac32\dfrac{4c^2}{(cx+d)^2}=0 \mathscr{D}(f\circ g)=[\mathscr{D}f\circ g]\cdot (g')^2+\mathscr{D}g=0\cdot (g')^2+\mathscr{D}g=\mathscr{D}g","['real-analysis', 'calculus', 'derivatives', 'differential']"
22,Derivatives of a recursively and implicitly defined polynomial,Derivatives of a recursively and implicitly defined polynomial,,"I'm studying Frobenius Manifolds associated with $A_n$ -type singularities and in order to prove some results about their potentials I need to calculate the following thing. Assume that $n$ is a fixed natural number and there is a finite family of $n-1$ polynomials with rational coefficients on $n-1$ variables $f_q(x_1,\dots,x_{n-1})$ for $1\leqslant q \leqslant n-1$ , whose coefficients and the polynomials themselves are defined recursively. Moreover, each polynomial is defined as a sum over all tuples of natural numbers which subject to a condition on their sum. The main question is the following: how to calculate partial derivatives (actually I need only second ones, i.e. a Hessian matrix) of these objects? Maybe some more explicit details could help to calculate derivatives, so let me define the family which I am interested in. Firstly, let me define recursively the numbers $P_n(s_1,\dots,s_m)$ for a fixed natural number $n$ and an $m$ -tuple of natural numbers $(s_1,\dots,s_m)$ as follows: $$P_n(s_1)=s_1$$ $$P_n(s_1,\dots,s_m) = \binom{n}{m} - \sum\limits_{q=1}^{m-1} P_n(s_1,\dots,s_q) \binom{n-q-(s_1+\cdots+s_q)}{m-q}$$ where I assume that $\binom{p}{t} = 0$ are defined as usual for $p\geqslant t \geqslant 0$ and $0$ otherwise. For instance, $P_n(s_1,\dots,s_m) = 0$ for $m\geqslant n+1$ . Now I define the main object, namely a family of polynomials with rational coefficients $x_{-q}(x_1,\dots,x_{n-1})$ ( $n$ is still a fixed natural number) for $q \geqslant 0$ as follows: $x_{0} = 0$ and $x_{-q}(x_1,\dots,x_{n-1})$ for $q > 0$ is defined recursively via $$x_{-q} = -\frac{1}{n} \sum\limits_{m=2}^{n}\sum P_n(s_1,\dots,s_m) x_{n-s_1}\cdots x_{n-s_m}$$ where the second sum is taken over all natural numbers $(s_1,\dots,s_m)$ such that $\sum\limits_{i=1}^m s_i = q+n+1-m$ . For example, here is the list of them for $n=3$ (one can also prove that $x_{-n}$ are always equal to zero): $$x_{-1} = -x_1x_2$$ $$x_{-2} = -x_1^2-\frac{x_2^3}{3}$$ Then I need to compute all second partial derivatives of $x_{-q}$ (i.e. Hessian matrix $H_{n,q}$ of $x_{-q}$ ) for any $n$ and $1\leqslant q \leqslant n-1$ and it is the point where I've been completely stuck. I need these derivatives in order to check some equalities involving them and I   am able to found them via direct calculations for some small numbers $n$ , but I can't cope with the general case because the coefficients $P_n$ and the polynomials $x_{-q}$ are defined recursively and moreover $x_{-q}$ is defined via the implicit summation condition, that's why I do not understand how to write down a closed formula for any $n$ . Actually I can prove it, but only for $q=1$ . In this case it follows from some general theory of Frobenius Manifolds that $-H_{n,1}$ is equal to the pairing matrix for $A_{n-1}$ -type Frobenius Manifold which is just an antidiagonal matrix I'd appreciate any help and thoughts on the computation or even on some properties of the coefficients $P_n$ , polynomials $x_{-q}$ or their Hessian matrix.","I'm studying Frobenius Manifolds associated with -type singularities and in order to prove some results about their potentials I need to calculate the following thing. Assume that is a fixed natural number and there is a finite family of polynomials with rational coefficients on variables for , whose coefficients and the polynomials themselves are defined recursively. Moreover, each polynomial is defined as a sum over all tuples of natural numbers which subject to a condition on their sum. The main question is the following: how to calculate partial derivatives (actually I need only second ones, i.e. a Hessian matrix) of these objects? Maybe some more explicit details could help to calculate derivatives, so let me define the family which I am interested in. Firstly, let me define recursively the numbers for a fixed natural number and an -tuple of natural numbers as follows: where I assume that are defined as usual for and otherwise. For instance, for . Now I define the main object, namely a family of polynomials with rational coefficients ( is still a fixed natural number) for as follows: and for is defined recursively via where the second sum is taken over all natural numbers such that . For example, here is the list of them for (one can also prove that are always equal to zero): Then I need to compute all second partial derivatives of (i.e. Hessian matrix of ) for any and and it is the point where I've been completely stuck. I need these derivatives in order to check some equalities involving them and I   am able to found them via direct calculations for some small numbers , but I can't cope with the general case because the coefficients and the polynomials are defined recursively and moreover is defined via the implicit summation condition, that's why I do not understand how to write down a closed formula for any . Actually I can prove it, but only for . In this case it follows from some general theory of Frobenius Manifolds that is equal to the pairing matrix for -type Frobenius Manifold which is just an antidiagonal matrix I'd appreciate any help and thoughts on the computation or even on some properties of the coefficients , polynomials or their Hessian matrix.","A_n n n-1 n-1 f_q(x_1,\dots,x_{n-1}) 1\leqslant q \leqslant n-1 P_n(s_1,\dots,s_m) n m (s_1,\dots,s_m) P_n(s_1)=s_1 P_n(s_1,\dots,s_m) = \binom{n}{m} - \sum\limits_{q=1}^{m-1} P_n(s_1,\dots,s_q) \binom{n-q-(s_1+\cdots+s_q)}{m-q} \binom{p}{t} = 0 p\geqslant t \geqslant 0 0 P_n(s_1,\dots,s_m) = 0 m\geqslant n+1 x_{-q}(x_1,\dots,x_{n-1}) n q \geqslant 0 x_{0} = 0 x_{-q}(x_1,\dots,x_{n-1}) q > 0 x_{-q} = -\frac{1}{n} \sum\limits_{m=2}^{n}\sum P_n(s_1,\dots,s_m) x_{n-s_1}\cdots x_{n-s_m} (s_1,\dots,s_m) \sum\limits_{i=1}^m s_i = q+n+1-m n=3 x_{-n} x_{-1} = -x_1x_2 x_{-2} = -x_1^2-\frac{x_2^3}{3} x_{-q} H_{n,q} x_{-q} n 1\leqslant q \leqslant n-1 n P_n x_{-q} x_{-q} n q=1 -H_{n,1} A_{n-1} P_n x_{-q}","['derivatives', 'polynomials', 'recurrence-relations', 'partial-derivative', 'singularity-theory']"
23,Reference for Multivariate Taylors theorem on closed domains,Reference for Multivariate Taylors theorem on closed domains,,"I'm currently looking for a source (preferably a book) that provides a proof of the multivariate Taylors Theorem in the setting that a function $f$ is defined on a closed domain $D$ and is $m$ -times continuously differentiable in the interior of the domain such that all partial derivatives extend continuously to $D$ . The univariate case can be found here , but I am particularly interested in a quality reference (book) of the multivariate case.","I'm currently looking for a source (preferably a book) that provides a proof of the multivariate Taylors Theorem in the setting that a function is defined on a closed domain and is -times continuously differentiable in the interior of the domain such that all partial derivatives extend continuously to . The univariate case can be found here , but I am particularly interested in a quality reference (book) of the multivariate case.",f D m D,"['derivatives', 'reference-request', 'taylor-expansion']"
24,Finding the derivative of $x\uparrow\uparrow n$ [duplicate],Finding the derivative of  [duplicate],x\uparrow\uparrow n,"This question already has answers here : $n^{th}$ derivative of a tetration function (6 answers) Closed 5 years ago . I am trying to find a general derivative for the function: $f(x)=x^{x^{x^{...^{x}}}}$however to do that I must find $f^{\prime }$ and $f^{\prime \prime}$...etc. I am now trying to write down a general expression for $f^{\prime \prime}$ and I have stumble opon the series in the picture below, I wonder, do anyone have an idea of what sort of expantion it migth be ? I have tried breaking it down into three different sums, but I wasn't very successful! EDIT So since this post has gotten some attention and the title has been change I will tell you what I am on about, although I already have a post just for the question in the title from before. I am as I said earlier trying to find a general derivative for the function $f(x)=x^{x^{x^{...^{x}}}}$ where $n$ is an arbitrary number, natural number and greater or equal to 2. I have already made success when I tried to find the first derivative, and I have also found some interesting pattern emerge when the derivatives as well as n incresses. But my question here was if anyone knew what this expation might be or look similar to be, since I have yet to be lernt series etc. I did not know that the question needed such background since it's straight a forward question.","This question already has answers here : $n^{th}$ derivative of a tetration function (6 answers) Closed 5 years ago . I am trying to find a general derivative for the function: $f(x)=x^{x^{x^{...^{x}}}}$however to do that I must find $f^{\prime }$ and $f^{\prime \prime}$...etc. I am now trying to write down a general expression for $f^{\prime \prime}$ and I have stumble opon the series in the picture below, I wonder, do anyone have an idea of what sort of expantion it migth be ? I have tried breaking it down into three different sums, but I wasn't very successful! EDIT So since this post has gotten some attention and the title has been change I will tell you what I am on about, although I already have a post just for the question in the title from before. I am as I said earlier trying to find a general derivative for the function $f(x)=x^{x^{x^{...^{x}}}}$ where $n$ is an arbitrary number, natural number and greater or equal to 2. I have already made success when I tried to find the first derivative, and I have also found some interesting pattern emerge when the derivatives as well as n incresses. But my question here was if anyone knew what this expation might be or look similar to be, since I have yet to be lernt series etc. I did not know that the question needed such background since it's straight a forward question.",,"['calculus', 'sequences-and-series', 'derivatives', 'power-series']"
25,Optimization with strictly increasing continuous function with zero derivative (a.e.),Optimization with strictly increasing continuous function with zero derivative (a.e.),,"Suppose we have a strictly increasing, continuous scalar variable function $f(x)$ such that $f'(x) = 0 \text{ a.e.}$ The problem is $$\underset{x \in [a, b]}{\max}\ f(x) - cx$$ for some positive $c$ . Since we have a sum of two continuous functions, it follows from the Weirstrass theorem that there exists a solution. If solution is interior, we cannot find it from the first order condition since $f'(x) = 0 \text{ a.e}$ . My question is: are there ways to characterize the interior solution analytically in general (without specifying $f(x)$ )? I am also interested in a more general setting, where instead of $-cx$ there could be any continuous function. If you have references to papers which study this, would be very grateful if you share.","Suppose we have a strictly increasing, continuous scalar variable function such that The problem is for some positive . Since we have a sum of two continuous functions, it follows from the Weirstrass theorem that there exists a solution. If solution is interior, we cannot find it from the first order condition since . My question is: are there ways to characterize the interior solution analytically in general (without specifying )? I am also interested in a more general setting, where instead of there could be any continuous function. If you have references to papers which study this, would be very grateful if you share.","f(x) f'(x) = 0 \text{ a.e.} \underset{x \in [a, b]}{\max}\ f(x) - cx c f'(x) = 0 \text{ a.e} f(x) -cx","['real-analysis', 'derivatives', 'optimization']"
26,"Does $\exists$ a differentiable function $f:\mathbb{R}\to \mathbb{R}, f(x) \neq x+c$ s.t. every interval $(a,b)$ contains a point $p$ with gradient 1?",Does  a differentiable function  s.t. every interval  contains a point  with gradient 1?,"\exists f:\mathbb{R}\to \mathbb{R}, f(x) \neq x+c (a,b) p","Does there exist a differentiable function $f:\mathbb{R} \to \mathbb{R}, f(x) \neq x+c$ such that every interval $(a,b)$ contains a point $p$ with gradient $1$ ? I would guess no, but I've no idea how to prove it. Now that I think about it, isn't this question similar to: Does there exist a non-constant differentiable function $f:\mathbb{R} \to \mathbb{R}$ , such that every interval $(a,b)$ contains a point $p$ with gradient $0$ ? Which I may or may not have seen on the site somewhere - can't remember. I guess the disproof would be something like: $f(x)$ has gradient $0$ almost everywhere $\implies f(x)$ has unbounded variation somewhere (e.g. in some interval) $\implies$ f(x) is not everywhere differentiable. I'm not that familiar with bounded variation other than having skim-read this thread once, but bounded variation may in fact not be necessary to answer this question, I've no idea. Maybe the mean value theorem is more relevant here.","Does there exist a differentiable function such that every interval contains a point with gradient ? I would guess no, but I've no idea how to prove it. Now that I think about it, isn't this question similar to: Does there exist a non-constant differentiable function , such that every interval contains a point with gradient ? Which I may or may not have seen on the site somewhere - can't remember. I guess the disproof would be something like: has gradient almost everywhere has unbounded variation somewhere (e.g. in some interval) f(x) is not everywhere differentiable. I'm not that familiar with bounded variation other than having skim-read this thread once, but bounded variation may in fact not be necessary to answer this question, I've no idea. Maybe the mean value theorem is more relevant here.","f:\mathbb{R} \to \mathbb{R}, f(x) \neq x+c (a,b) p 1 f:\mathbb{R} \to \mathbb{R} (a,b) p 0 f(x) 0 \implies f(x) \implies","['real-analysis', 'calculus', 'derivatives']"
27,Is Grönwall's Lemma true only for non-negative functions?,Is Grönwall's Lemma true only for non-negative functions?,,"For instance, in this article of Wikipédia a statement is made for Differential form of the Grönwall's Lema for a $\beta, u:I:=[a,b]\subset \mathbb{R} \longrightarrow \mathbb{R}$ continuous functions and $u$ differentiable in the interior $I^{\circ}$ of $I$ . That is, $ u $ and $ \beta $ are not necessarily non-negatives. Where, $$u'(t) \leq \beta(t)u(t) , \; \forall \;  t \in I^{\circ}\Rightarrow u(t) \leq u(a)e^{\int_{a}^{t}\beta(s)}ds, \; \forall \; t \in I.$$ The versions of the Grönwall's Lema that I know of require that the functions be non-negative. Is that what's on Wikipedia right? If so, is there a reference that contains this result? In the statement of the article, quoted above, an observation is made: There are no assumptions on the signs of the functions $\beta$ and $u$ . I did not see error in the proof presented there.","For instance, in this article of Wikipédia a statement is made for Differential form of the Grönwall's Lema for a continuous functions and differentiable in the interior of . That is, and are not necessarily non-negatives. Where, The versions of the Grönwall's Lema that I know of require that the functions be non-negative. Is that what's on Wikipedia right? If so, is there a reference that contains this result? In the statement of the article, quoted above, an observation is made: There are no assumptions on the signs of the functions and . I did not see error in the proof presented there.","\beta, u:I:=[a,b]\subset \mathbb{R} \longrightarrow \mathbb{R} u I^{\circ} I  u   \beta  u'(t) \leq \beta(t)u(t) , \; \forall \;  t \in I^{\circ}\Rightarrow u(t) \leq u(a)e^{\int_{a}^{t}\beta(s)}ds, \; \forall \; t \in I. \beta u","['real-analysis', 'derivatives', 'inequality', 'continuity']"
28,Prove vector-valued function is differentiable,Prove vector-valued function is differentiable,,"Give $D$ is an open set in $\mathbb{R}^n$ , $f: D\to \mathbb{R}^p$ is differentiable on $D$ . Supposing that $f$ has second derivative at $x_0\in D$ . For every $u\in \mathbb{R}^n$ , give $g: D\to \mathbb{R}^p$ define by $$g(x)=f'(x)(u), \forall x\in D.$$ Prove that $g$ is differentiable at $x_0$ and $g'(x_0)(v)=f^{(2)}(x_0)(u,v)$ . My attempt: Give $h\in \mathbb{R}^n$ that $x+h \in D$ . Firstly, we have $$g(x_0+h)-g(x_0)=f'(x_0+h)(u)-f'(x_0)(u)=\left[f'(x_0+h)-f'(x_0)\right](u) \quad (1)$$ Secondly, since $f$ have second dervative at $x_0$ that exist linear mapping $A:\mathbb{R}^n\to L(\mathbb{R}^n,\mathbb{R}^p)$ that $$f'(x_0+h)-f'(x_0)=A(h)+\vert h \vert_2\varphi(h) \quad (2)$$ with $\lim\limits_{h \to 0_{\mathbb{R}^n}}\varphi(h)=0_{L(\mathbb{R}^n,\mathbb{R}^p)}$ . From (1) and (2), we infer that $$g(x_0+h)-g(x_0)=A(h)(u)+\vert h \vert_2\varphi(h)(u)$$ I stuck here now. I wonder that there exists linear mapping $A_1: \mathbb{R}^n \to \mathbb{R}^p$ and function $\varphi_1:\mathbb{R}^n \to \mathbb{R}^p$ that $A_1(u)(h)=A(h)(u)$ and $\varphi_1(u)(h)=\varphi(h)(u)$ ?","Give is an open set in , is differentiable on . Supposing that has second derivative at . For every , give define by Prove that is differentiable at and . My attempt: Give that . Firstly, we have Secondly, since have second dervative at that exist linear mapping that with . From (1) and (2), we infer that I stuck here now. I wonder that there exists linear mapping and function that and ?","D \mathbb{R}^n f: D\to \mathbb{R}^p D f x_0\in D u\in \mathbb{R}^n g: D\to \mathbb{R}^p g(x)=f'(x)(u), \forall x\in D. g x_0 g'(x_0)(v)=f^{(2)}(x_0)(u,v) h\in \mathbb{R}^n x+h \in D g(x_0+h)-g(x_0)=f'(x_0+h)(u)-f'(x_0)(u)=\left[f'(x_0+h)-f'(x_0)\right](u) \quad (1) f x_0 A:\mathbb{R}^n\to L(\mathbb{R}^n,\mathbb{R}^p) f'(x_0+h)-f'(x_0)=A(h)+\vert h \vert_2\varphi(h) \quad (2) \lim\limits_{h \to 0_{\mathbb{R}^n}}\varphi(h)=0_{L(\mathbb{R}^n,\mathbb{R}^p)} g(x_0+h)-g(x_0)=A(h)(u)+\vert h \vert_2\varphi(h)(u) A_1: \mathbb{R}^n \to \mathbb{R}^p \varphi_1:\mathbb{R}^n \to \mathbb{R}^p A_1(u)(h)=A(h)(u) \varphi_1(u)(h)=\varphi(h)(u)","['derivatives', 'vector-analysis']"
29,An alternative concept of a 'derivative',An alternative concept of a 'derivative',,"The definition of the derivative of a function $f$ at $x$ $$f'(x) = \lim_{h \to 0} \frac{f(x+h) -f(x)}{x+h-x}$$ arises from considering its gradient , given by $\frac{\Delta y}{\Delta x}$ . This yields a gradient function from which the gradient at any suitable point $(x, f(x))$ can be calculated. This process is obviously very useful as the gradient is connected to concepts such as tangents and stationary points, and sometimes represents a useful physical quantity such as acceleration. However, what if we were interested in finding normals to a graph, rather than tangents? Considering the definition of the derivative from first principles, we could introduce an analogous definition for finding a 'normal function' rather than a gradient function; the gradient of the normal is given by $-\frac{\Delta x}{\Delta y}$ , so we could define this alternative 'derivative' by $$f\star(x) = \lim_{h \to 0} \frac{h}{f(x) -f(x+h)}$$ using $f\star(x)$ to denote this idea. My question is, has this process been explored, and does it have any applications? Of course, anything that can be accomplished with this can (presumably) be accomplished with the usual derivative and the negative reciprocal relationship $f'(x) = -\frac{1}{f\star(x)}$ so it may well be obselete, but I'm wondering if there is use for it elsewhere. In particular, the inverse of differentiation (in the standard sense) is definitely useful - integrals have geometrical significance and antiderivatives can also represent useful quantities. If we introduce an analogous 'integration' as the inverse of differentiation - so the process by which $f \star (x)$ becomes $f(x)$ - then what, if any, is the geometrical significance of these 'integrals'? Experimenting with this definition, I found analogies to the usual chain rule, quotient rule and product rule; for instance if $f(x) = g(h(x))$ , then $$f \star (x) = -\left[ g\star (h(x)) \right] * \left[h\star (x) \right]$$ although the others were more unwieldy. Are there any other properties of differentiating in the usual sense that can be translated into the analogous definitions? One consequence that is particularly apparent when 'differentiating' trigonometric functions is that the alternative derivative is undefined at stationary points; for instance if $f(x) = \sin x$ then $f \star (x) = -\sec x$ which is undefined at every $\frac \pi 2 + n \pi$ with integer $n$ . I couldn't find any examples of this idea but I wasn't sure what to search for, as clearly this isn't a derivative in the usual sense of the word and 'normal function' is not a very insightful search query - the proper terminology and notation would be very helpful, if it exists.","The definition of the derivative of a function at arises from considering its gradient , given by . This yields a gradient function from which the gradient at any suitable point can be calculated. This process is obviously very useful as the gradient is connected to concepts such as tangents and stationary points, and sometimes represents a useful physical quantity such as acceleration. However, what if we were interested in finding normals to a graph, rather than tangents? Considering the definition of the derivative from first principles, we could introduce an analogous definition for finding a 'normal function' rather than a gradient function; the gradient of the normal is given by , so we could define this alternative 'derivative' by using to denote this idea. My question is, has this process been explored, and does it have any applications? Of course, anything that can be accomplished with this can (presumably) be accomplished with the usual derivative and the negative reciprocal relationship so it may well be obselete, but I'm wondering if there is use for it elsewhere. In particular, the inverse of differentiation (in the standard sense) is definitely useful - integrals have geometrical significance and antiderivatives can also represent useful quantities. If we introduce an analogous 'integration' as the inverse of differentiation - so the process by which becomes - then what, if any, is the geometrical significance of these 'integrals'? Experimenting with this definition, I found analogies to the usual chain rule, quotient rule and product rule; for instance if , then although the others were more unwieldy. Are there any other properties of differentiating in the usual sense that can be translated into the analogous definitions? One consequence that is particularly apparent when 'differentiating' trigonometric functions is that the alternative derivative is undefined at stationary points; for instance if then which is undefined at every with integer . I couldn't find any examples of this idea but I wasn't sure what to search for, as clearly this isn't a derivative in the usual sense of the word and 'normal function' is not a very insightful search query - the proper terminology and notation would be very helpful, if it exists.","f x f'(x) = \lim_{h \to 0} \frac{f(x+h) -f(x)}{x+h-x} \frac{\Delta y}{\Delta x} (x, f(x)) -\frac{\Delta x}{\Delta y} f\star(x) = \lim_{h \to 0} \frac{h}{f(x) -f(x+h)} f\star(x) f'(x) = -\frac{1}{f\star(x)} f \star (x) f(x) f(x) = g(h(x)) f \star (x) = -\left[ g\star (h(x)) \right] * \left[h\star (x) \right] f(x) = \sin x f \star (x) = -\sec x \frac \pi 2 + n \pi n","['calculus', 'derivatives']"
30,How to prove that this generalized polynomial has exactly one root?,How to prove that this generalized polynomial has exactly one root?,,"I am studying the function $U: \mathbb{R}_{\ge 0} \to \mathbb{R}$ given by \begin{align} U(l) &= (1-r^l) \left[ T - \tau n^l \left( 1 - \left( 1 - \frac{1}{\tau n^l} \right)^T \right)  \right] \\ &=(1-r^l)\sum_{i=2}^T \binom{T}{i}(-1)^i\frac{1}{(\tau n^l)^{i-1}} \end{align} where $0 < r < 1$ and $\tau \in\mathbb{Z}_{\ge 1}$ and $n, T \in \mathbb{Z}_{\ge 2}$ are fixed, with $T>\tau$ . This is a utility function in an application. It seems that, regardless of the values of the parameters $r,l,T$ , and $\tau$ (within their respective bounds), $U(l)$ starts at zero (for $l=0$ ), and for some $l_0>0$ increases in $[0,l_0]$ , has a unique local (and global) maximum at $l=l_0$ and is decreasing in $[l_0,\infty]$ and tends to zero as $l\to\infty$ . (See example graph in link below.) Example graph of U I want to prove the part that $U'(l)=0$ has exactly one root (at  some $l_0>0$ ). I have not gotten anywhere with the equation $U'(l)=0$ as is. Neither with $U'(l_0)=U'(l_1)=0$ for $l_0 \ne l_1$ leading to a contradiction. For $T=2$ it can be solved analytically. Yet another idea is therefore a proof by induction over $T$ , but I cannot see how $U'(l)|_{T=t}$ having exactly one zero leads to $U'(l)|_{T=t+1}$ having exactly one zero. However, I have been able to prove that $U'(0)>0$ and that there exists an $L>0$ such that $U'(l)<0$ for all $l\ge L$ . By the intermediate value theorem, $U'$ (which is continuous) must then have a zero in $(0,L)$ . The question is how to prove that it is only one. If one could establish $U'(l_0)=0 \Rightarrow U''(l_0)<0$ we would be done, since $U'$ is continuous, but I cannot see how to use $U'(l_0)=0$ to prove $U''(l_0)<0$ . First try: The variable substitution $\tau n^l=x$ induces the new function $u: \mathbb{R}_{\ge\tau} \to \mathbb{R}$ given by \begin{align} u(x) &= (1-\tau^B x^{-B}) \left[ T - x \left( 1 - \left( 1 - \frac{1}{x} \right)^T \right)  \right] \\ &= \sum_{i=2}^T(-1)^i\binom{T}{i} x^{-i+1} - \sum_{i=2}^T(-1)^i\binom{T}{i} \tau^B x^{-i+1-B} \end{align} (where $B = -\frac{\log r}{\log n} > 0$ ) whose derivative $u'(x)$ (after a binomial expansion) can be written $$ u'(x)=p(x)+q(x) $$ where $p(x)=\sum_{i=2}^T p_i x^{-i}$ and $q(x)=\sum_{i=2}^T q_i x^{-i-B}$ are generalized polynomials with $$ p_i=(-1)^i (-i+1) \binom{T}{i} \quad \text{and} \quad q_i=-(-1)^i (-i+1-B) \binom{T}{i} \tau^B. $$ For example, if $\tau = 2, n = 10, r = 0.7$ and $T = 5$ , then (with rounded off values) $B=0.15$ and $$ u'(x) = -10x^{-2} + 12.86x^{-2.15} + 20x^{-3} - 23.99x^{-3.15} - 15x^{-4} + 17.56x^{-4.15} + 4x^{-5} - 4.63x^{-5.15}. $$ Since $u'(x)$ is a generalized polynomial, I thought first of using  [Graham J.O. Jameson, Counting zeros of generalised polynomials: Descartes’ rule of signs and Laguerre’s extensions , The Mathematical Gazette 90 (2006), no. 518, 223–234]. Unfortunately, neither the generalized Decartes' rule of signs (Theorem 3.1) nor the theorem about sign changes in partial coefficient sums for zeros in $(1,\infty)$ (Theorem 4.7) is of any use as $u'(x)$ in general has more than one (cumulative) sign change. Second try: Use the same substitution as above and write the equation $u'(x)=0$ as $g(x)=x$ and check if $g:\mathbb{R}_{\ge\tau}\to\mathbb{R}_{\ge\tau}$ is a contraction (i.e. there is some real number $0\le k<1$ such that $|g(x)-g(y)|<k|x-y|$ for all $x,y\in\mathbb{R}_{\ge\tau}$ ). If so, it would follow from Banach's fixed point theorem that $u'(x)=0$ has exactly one root. Unfortunately $g'(\tau)$ is not $<1$ in general, so $g$ is not a contraction. Are there any other ways or perhaps different approaches altogether to prove that $U'(l)$ (or $u'(x)$ ) has exactly one zero? The second challenge is to prove that $l_0$ increases with $T$ (while keeping all the other parameters fixed). From simulations, this seems to be the case, but I currently have no idea how to prove it.","I am studying the function given by where and and are fixed, with . This is a utility function in an application. It seems that, regardless of the values of the parameters , and (within their respective bounds), starts at zero (for ), and for some increases in , has a unique local (and global) maximum at and is decreasing in and tends to zero as . (See example graph in link below.) Example graph of U I want to prove the part that has exactly one root (at  some ). I have not gotten anywhere with the equation as is. Neither with for leading to a contradiction. For it can be solved analytically. Yet another idea is therefore a proof by induction over , but I cannot see how having exactly one zero leads to having exactly one zero. However, I have been able to prove that and that there exists an such that for all . By the intermediate value theorem, (which is continuous) must then have a zero in . The question is how to prove that it is only one. If one could establish we would be done, since is continuous, but I cannot see how to use to prove . First try: The variable substitution induces the new function given by (where ) whose derivative (after a binomial expansion) can be written where and are generalized polynomials with For example, if and , then (with rounded off values) and Since is a generalized polynomial, I thought first of using  [Graham J.O. Jameson, Counting zeros of generalised polynomials: Descartes’ rule of signs and Laguerre’s extensions , The Mathematical Gazette 90 (2006), no. 518, 223–234]. Unfortunately, neither the generalized Decartes' rule of signs (Theorem 3.1) nor the theorem about sign changes in partial coefficient sums for zeros in (Theorem 4.7) is of any use as in general has more than one (cumulative) sign change. Second try: Use the same substitution as above and write the equation as and check if is a contraction (i.e. there is some real number such that for all ). If so, it would follow from Banach's fixed point theorem that has exactly one root. Unfortunately is not in general, so is not a contraction. Are there any other ways or perhaps different approaches altogether to prove that (or ) has exactly one zero? The second challenge is to prove that increases with (while keeping all the other parameters fixed). From simulations, this seems to be the case, but I currently have no idea how to prove it.","U: \mathbb{R}_{\ge 0} \to \mathbb{R} \begin{align}
U(l) &= (1-r^l) \left[ T - \tau n^l \left( 1 - \left( 1 - \frac{1}{\tau n^l} \right)^T \right)  \right] \\
&=(1-r^l)\sum_{i=2}^T \binom{T}{i}(-1)^i\frac{1}{(\tau n^l)^{i-1}}
\end{align} 0 < r < 1 \tau \in\mathbb{Z}_{\ge 1} n, T \in \mathbb{Z}_{\ge 2} T>\tau r,l,T \tau U(l) l=0 l_0>0 [0,l_0] l=l_0 [l_0,\infty] l\to\infty U'(l)=0 l_0>0 U'(l)=0 U'(l_0)=U'(l_1)=0 l_0 \ne l_1 T=2 T U'(l)|_{T=t} U'(l)|_{T=t+1} U'(0)>0 L>0 U'(l)<0 l\ge L U' (0,L) U'(l_0)=0 \Rightarrow U''(l_0)<0 U' U'(l_0)=0 U''(l_0)<0 \tau n^l=x u: \mathbb{R}_{\ge\tau} \to \mathbb{R} \begin{align}
u(x) &= (1-\tau^B x^{-B}) \left[ T - x \left( 1 - \left( 1 - \frac{1}{x} \right)^T \right)  \right] \\
&= \sum_{i=2}^T(-1)^i\binom{T}{i} x^{-i+1} - \sum_{i=2}^T(-1)^i\binom{T}{i} \tau^B x^{-i+1-B}
\end{align} B = -\frac{\log r}{\log n} > 0 u'(x) 
u'(x)=p(x)+q(x)
 p(x)=\sum_{i=2}^T p_i x^{-i} q(x)=\sum_{i=2}^T q_i x^{-i-B} 
p_i=(-1)^i (-i+1) \binom{T}{i} \quad \text{and} \quad q_i=-(-1)^i (-i+1-B) \binom{T}{i} \tau^B.
 \tau = 2, n = 10,
r = 0.7 T = 5 B=0.15 
u'(x) = -10x^{-2} + 12.86x^{-2.15} + 20x^{-3} - 23.99x^{-3.15} - 15x^{-4} + 17.56x^{-4.15} + 4x^{-5} - 4.63x^{-5.15}.
 u'(x) (1,\infty) u'(x) u'(x)=0 g(x)=x g:\mathbb{R}_{\ge\tau}\to\mathbb{R}_{\ge\tau} 0\le k<1 |g(x)-g(y)|<k|x-y| x,y\in\mathbb{R}_{\ge\tau} u'(x)=0 g'(\tau) <1 g U'(l) u'(x) l_0 T","['calculus', 'derivatives', 'polynomials', 'roots']"
31,Definition of differentiability using sequence notations,Definition of differentiability using sequence notations,,"Let $f$ be differentiable at $c$ and let $\{a_n\}$ and $\{b_n\}$ be sequences such that $a_n<c<b_n$ , and $a_n$ and $b_n$ converges to $c.$ Prove that $$\lim_{n\to\infty}\frac{f(a_n)-f(b_n)}{a_n-b_n}=f'(c).$$ What I have so far is that \begin{align} & \frac{f(a_n)-f(b_n)}{a_n-b_n}-f'(c) \\[8pt] = {} & \left[\frac{f(b_n)-f(c)}{b_n-c}-f'(c)\right]\cdot\frac{b_n-c}{b_n-a_n} \\[8pt] & {} + \left[\frac{f(c)-f(a_n)}{a_n-c}-f'(c)\right]\cdot\frac{c-a_n}{b_n-a_n} \end{align} All is left to prove is that $\frac{b_n-c}{b_n-a_n}$ and $\frac{c-a_n}{b_n-a_n}$ is bounded. Then, I am planning to use the fact that $f$ is differentiable at c to show that the brackets goes to 0 and thus the whole expression goes to 0. Thus, the equation. But I am having hard time formally proving that $\frac{b_n-c}{b_n-a_n}$ and $\frac{c-a_n}{b_n-a_n}$ is bounded. Can you guys help me?","Let be differentiable at and let and be sequences such that , and and converges to Prove that What I have so far is that All is left to prove is that and is bounded. Then, I am planning to use the fact that is differentiable at c to show that the brackets goes to 0 and thus the whole expression goes to 0. Thus, the equation. But I am having hard time formally proving that and is bounded. Can you guys help me?","f c \{a_n\} \{b_n\} a_n<c<b_n a_n b_n c. \lim_{n\to\infty}\frac{f(a_n)-f(b_n)}{a_n-b_n}=f'(c). \begin{align}
& \frac{f(a_n)-f(b_n)}{a_n-b_n}-f'(c) \\[8pt]
= {} & \left[\frac{f(b_n)-f(c)}{b_n-c}-f'(c)\right]\cdot\frac{b_n-c}{b_n-a_n} \\[8pt]
& {} + \left[\frac{f(c)-f(a_n)}{a_n-c}-f'(c)\right]\cdot\frac{c-a_n}{b_n-a_n}
\end{align} \frac{b_n-c}{b_n-a_n} \frac{c-a_n}{b_n-a_n} f \frac{b_n-c}{b_n-a_n} \frac{c-a_n}{b_n-a_n}","['real-analysis', 'sequences-and-series', 'derivatives']"
32,Alternative proofs for the higher product rule (differentiation),Alternative proofs for the higher product rule (differentiation),,"Let $f,g$ be smooth functions. I'm looking for proofs for the formula $$ \left(\frac d {dx}\right)^n f(x)\cdot g(x) =  \sum_{i=0}^n \binom{n}{ i} f^{(i)}(x)\cdot g^{(n-i)}(x)  $$ , where $f^{(i)}$ denotes the $i$ -th derivation of $f$ . The classical proof uses induction, and runs analogue to the proof of the binomial theorem. It can be found here: Proof 1 The second proof that is easily possible is by creating a bijection between $$ \binom{\{1,..,n\}}{k} = \{M\subseteq \{1,..,n\}\mid |M| =k\} $$ and the ways to create a single term $f^{(k)}(x)\cdot g^{(n-k)}(x)$ . The argument is roughly this: We can view each set $M\in \binom{\{1,..,n\}}{k} $ as list of all the times we derived $f$ instead of $g$ : Say e.g. $M =\{1,2,4\}\in \binom{\{1,..,4\}}{3}$ , i.e. especially $n=4, k=3$ . Then that 'traces' the path $\begin{align*}  &f^{(0)}(x)\cdot g^{(0)}(x)\\ \to &f^{(1)}(x)\cdot g^{(0)}(x)\\ \to &f^{(2)}(x)\cdot g^{(0)}(x)\\ \to &f^{(2)}(x)\cdot g^{(1)}(x)\\ \to &f^{(3)}(x)\cdot g^{(1)}(x) \end{align*}$ Each such path tracks one summand in the expanding of $\left(\frac d {dx}\right)^n f(x)\cdot g(x)$ if we don't simplify the terms (i.e. add up terms of the same type). One then has only to show that the paths are injective and surjective. Proof 3: We define $M:=\{f:\mathbb R \to \mathbb R \mid f \text{ smooth}\}$ . Using this set, we now define the polynomial ring $\mathbb R[M]$ . In other words, we look at the functions now as nothing more than formal objects, equipped with a commutative and associative addition. The idea is now this: Instead of looking at the product $f\cdot g$ , we look at formal pairs $(f,g)$ , which we interpret as a multiplication of its elements. On these we then define a differentiation, construct a recursion, and then solve it. First, we formalize the differentiation-operator to a function $\partial:\mathbb R[M]^2 \to \mathbb R[M]^2$ (we'll only formalize the parts of it that we need). For this, we define: $$ \partial_l:\mathbb R[M]^2\to \mathbb R[M]^2,\qquad (f^{(i)},g^{(j)})\mapsto  (f^{(i+1)},g^{(j)}) \\ \partial_r: \mathbb R[M]^2\to \mathbb R[M]^2 ,\qquad (f^{(i)},g^{(j)})\mapsto  (f^{(i)},g^{(j+1)}) $$ I.e. $\partial_l$ is the differentiates the left element of the tuple, and $\partial_r$ the right element of the tuple. On the pairs $\mathbb R[M]^2$ we define a commutative & associate addition as well, which though can only be simplified if both summands are identical: $$ (f^{(i)},g^{(j)}) +(f^{(i)},g^{(j)}) = 2 (f^{(i)},g^{(j)}) \\ (f^{(i)},g^{(j+1)}) + (f^{(i)},g^{(j)}) = (f^{(i)},g^{(j+1)}) + (f^{(i)},g^{(j)}) $$ Now $\partial$ is simply: $$ \partial = \partial_l + \partial_r $$ One can show, that $\partial$ is linear, and that $\partial_l,\partial_r,\partial$ all are commuting with each other. If we define $\partial^n$ as applying $\partial$ $n$ times, the original problem now is: $$ \partial^n (f,g) =     \sum_{i=0}^n \binom{n}{ i} \partial_l^i \partial_r^{n-i} (f,g) $$ We can use the definition of $\partial$ to formulate the recursion: $$ \begin{align*} \partial^n (f,g) &= \partial^{n-1} (\partial_l(f,g)+\partial_r(f,g)) =  \\ &= \partial_l\partial^{n-1}(f,g) + \partial_r\partial^{n-1}(f,g) \\ &=(\partial_l + \partial_r)\partial^{n-1}(f,g) \end{align*} $$ This is equivalent to $$ \partial^n (f,g) -(\partial_l + \partial_r)\partial^{n-1}(f,g) = 0 $$ This leads to the closed formula of the generating function $\sum_{n\ge 0} a_n x^n$ : $$ \sum_{n\ge 0} a_n x^n = \frac{1}{1-(\partial_l + \partial_r)x} $$ It is now easy to obtain $a_n = (\partial_l + \partial_r)^n (f,g)$ , which is what we wanted to show, albeit in a totally different notation. (This proof is still far from perfect, and any improvements/suggestions are welcome) What are other proofs to show the higher product rule?","Let be smooth functions. I'm looking for proofs for the formula , where denotes the -th derivation of . The classical proof uses induction, and runs analogue to the proof of the binomial theorem. It can be found here: Proof 1 The second proof that is easily possible is by creating a bijection between and the ways to create a single term . The argument is roughly this: We can view each set as list of all the times we derived instead of : Say e.g. , i.e. especially . Then that 'traces' the path Each such path tracks one summand in the expanding of if we don't simplify the terms (i.e. add up terms of the same type). One then has only to show that the paths are injective and surjective. Proof 3: We define . Using this set, we now define the polynomial ring . In other words, we look at the functions now as nothing more than formal objects, equipped with a commutative and associative addition. The idea is now this: Instead of looking at the product , we look at formal pairs , which we interpret as a multiplication of its elements. On these we then define a differentiation, construct a recursion, and then solve it. First, we formalize the differentiation-operator to a function (we'll only formalize the parts of it that we need). For this, we define: I.e. is the differentiates the left element of the tuple, and the right element of the tuple. On the pairs we define a commutative & associate addition as well, which though can only be simplified if both summands are identical: Now is simply: One can show, that is linear, and that all are commuting with each other. If we define as applying times, the original problem now is: We can use the definition of to formulate the recursion: This is equivalent to This leads to the closed formula of the generating function : It is now easy to obtain , which is what we wanted to show, albeit in a totally different notation. (This proof is still far from perfect, and any improvements/suggestions are welcome) What are other proofs to show the higher product rule?","f,g 
\left(\frac d {dx}\right)^n f(x)\cdot g(x) =
 \sum_{i=0}^n \binom{n}{ i} f^{(i)}(x)\cdot g^{(n-i)}(x) 
 f^{(i)} i f 
\binom{\{1,..,n\}}{k} = \{M\subseteq \{1,..,n\}\mid |M| =k\}
 f^{(k)}(x)\cdot g^{(n-k)}(x) M\in \binom{\{1,..,n\}}{k}  f g M =\{1,2,4\}\in \binom{\{1,..,4\}}{3} n=4, k=3 \begin{align*}
 &f^{(0)}(x)\cdot g^{(0)}(x)\\
\to &f^{(1)}(x)\cdot g^{(0)}(x)\\
\to &f^{(2)}(x)\cdot g^{(0)}(x)\\
\to &f^{(2)}(x)\cdot g^{(1)}(x)\\
\to &f^{(3)}(x)\cdot g^{(1)}(x)
\end{align*} \left(\frac d {dx}\right)^n f(x)\cdot g(x) M:=\{f:\mathbb R \to \mathbb R \mid f \text{ smooth}\} \mathbb R[M] f\cdot g (f,g) \partial:\mathbb R[M]^2 \to \mathbb R[M]^2 
\partial_l:\mathbb R[M]^2\to \mathbb R[M]^2,\qquad (f^{(i)},g^{(j)})\mapsto 
(f^{(i+1)},g^{(j)})
\\
\partial_r: \mathbb R[M]^2\to \mathbb R[M]^2
,\qquad (f^{(i)},g^{(j)})\mapsto 
(f^{(i)},g^{(j+1)})
 \partial_l \partial_r \mathbb R[M]^2 
(f^{(i)},g^{(j)}) +(f^{(i)},g^{(j)}) = 2 (f^{(i)},g^{(j)})
\\
(f^{(i)},g^{(j+1)}) + (f^{(i)},g^{(j)}) = (f^{(i)},g^{(j+1)}) + (f^{(i)},g^{(j)})
 \partial 
\partial = \partial_l + \partial_r
 \partial \partial_l,\partial_r,\partial \partial^n \partial n 
\partial^n (f,g) =   
 \sum_{i=0}^n \binom{n}{ i} \partial_l^i \partial_r^{n-i} (f,g)
 \partial 
\begin{align*}
\partial^n (f,g) &= \partial^{n-1} (\partial_l(f,g)+\partial_r(f,g)) = 
\\ &=
\partial_l\partial^{n-1}(f,g) + \partial_r\partial^{n-1}(f,g)
\\
&=(\partial_l + \partial_r)\partial^{n-1}(f,g)
\end{align*}
 
\partial^n (f,g) -(\partial_l + \partial_r)\partial^{n-1}(f,g) = 0
 \sum_{n\ge 0} a_n x^n 
\sum_{n\ge 0} a_n x^n = \frac{1}{1-(\partial_l + \partial_r)x}
 a_n = (\partial_l + \partial_r)^n (f,g)","['derivatives', 'alternative-proof']"
33,Differentiability and integrability of a function composed with itself,Differentiability and integrability of a function composed with itself,,"I am reviewing for an exam and came across a multi-part question that I am having a hard time with. We are asked to prove or disprove the following statements. If a statement is false, what additional hypothesis would make it true? (Note: $f^{\circ 2}=f\circ f$ ). Let $f:[0,1]\rightarrow [0,1]$ be a continuous function. If $f$ is differentiable, then so is $f^{\circ 2}$ . If $f^{\circ 2}$ is differentiable, then so is $f$ . Let $f:[0,1]\rightarrow [0,1]$ be a function (not necessarily continuous). If $f$ is Riemann integrable, then so is $f^{\circ 2}$ . If $f^{\circ 2}$ is Riemann integrable, then so is $f$ . I think statement 1 is as simple as saying that if $f$ is differentiable, then $f^{\circ 2}$ is the composition of differentiable functions and is thus differentiable. Any guidance would be much appreciated!","I am reviewing for an exam and came across a multi-part question that I am having a hard time with. We are asked to prove or disprove the following statements. If a statement is false, what additional hypothesis would make it true? (Note: ). Let be a continuous function. If is differentiable, then so is . If is differentiable, then so is . Let be a function (not necessarily continuous). If is Riemann integrable, then so is . If is Riemann integrable, then so is . I think statement 1 is as simple as saying that if is differentiable, then is the composition of differentiable functions and is thus differentiable. Any guidance would be much appreciated!","f^{\circ 2}=f\circ f f:[0,1]\rightarrow [0,1] f f^{\circ 2} f^{\circ 2} f f:[0,1]\rightarrow [0,1] f f^{\circ 2} f^{\circ 2} f f f^{\circ 2}","['real-analysis', 'derivatives', 'riemann-integration']"
34,Does there always exist $x_0 \in \mathbb{R}$ such that $f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x - x_0)^2 \ge 0$ for all $x \in \mathbb{R}$? [closed],Does there always exist  such that  for all ? [closed],x_0 \in \mathbb{R} f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x - x_0)^2 \ge 0 x \in \mathbb{R},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $f: \mathbb{R} \to (0, \infty)$ be two times differentiable function. Does there always exist $x_0 \in \mathbb{R}$ such that $f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x - x_0)^2 \ge 0$ for all $x \in \mathbb{R}$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let be two times differentiable function. Does there always exist such that for all ?","f: \mathbb{R} \to (0, \infty) x_0 \in \mathbb{R} f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x - x_0)^2 \ge 0 x \in \mathbb{R}","['real-analysis', 'derivatives', 'taylor-expansion']"
35,$\sum_{k=0}^n a_kx^k$ splits $\Rightarrow \sum_{k=0}^n \frac {a_k} {k!}x^k$ splits over reals,splits  splits over reals,\sum_{k=0}^n a_kx^k \Rightarrow \sum_{k=0}^n \frac {a_k} {k!}x^k,"Suppose that $a_0, a_1, \ldots a_n \in \mathbb R$ and the polynomial $P(x) = \sum_{k=0}^n a_kx^k$ has all real roots. I'm supposed to show that $$ Q(x) = \sum_{k=0}^n \frac {a_k} {k!}x^k $$ also has this property, i.e. it has all $n$ (possibly non-distinct) real roots. I know that I'm supposed to show my progress, but I can't find any useful path. Perhaps a proof by induction on $n$ would be helpful - differentiating $Q$ leaves a similar polynomial to work with. I wasn't able to progress though. I would appreciate some clue (not full solution!).","Suppose that and the polynomial has all real roots. I'm supposed to show that also has this property, i.e. it has all (possibly non-distinct) real roots. I know that I'm supposed to show my progress, but I can't find any useful path. Perhaps a proof by induction on would be helpful - differentiating leaves a similar polynomial to work with. I wasn't able to progress though. I would appreciate some clue (not full solution!).","a_0, a_1, \ldots a_n \in \mathbb R P(x) = \sum_{k=0}^n a_kx^k 
Q(x) = \sum_{k=0}^n \frac {a_k} {k!}x^k
 n n Q","['real-analysis', 'derivatives', 'polynomials']"
36,Properties of a continuous function that satisfies $f(tx)=t^2 f(x)$,Properties of a continuous function that satisfies,f(tx)=t^2 f(x),"Let $f:\mathbb{R}^{n} \to \mathbb{R}$ be a continuous function such that $f(x) > 0$ for each $x \neq 0$ e, moreover, $$f(tx)=t^2 f(x)$$ for any $x \in \mathbb{R}^{n}$ and $t \in \mathbb{R}$ . (i) Determine $f(\mathbb{R}^{n})$ . (ii) Show that there is $C,D>0$ such that $$C\Vert x \Vert^2 \leq f(x) \leq D\Vert x \Vert^2$$ for any $x \in \mathbb{R}^{n}$ . (iii) Is $f$ differentiable at $0$ ? My attempt. (i) Fix $x\neq 0$ and define $\varphi(t) = f(tx) =t^2f(x) = \alpha t^2$ . So, $\varphi$ is a continuous function and $\varphi(\mathbb{R}) = [0,\infty)$ by Intermediate Value Theorem. Thus, $f(\mathbb{R}^n) = [0,\infty)$ . (ii) We have $f(x) = f\left(|x|\frac{x}{|x|}\right) = |x|^2f\left(\frac{x}{|x|}\right)$ . We know that $S^{n-1}$ is compact, $f(S^{n-1})$ has a maximum and minimum, respectively, $D$ and $C$ . Therefore, $$C|x|^2 \leq f(x) \leq D|x|^2.$$ (iii) Since $C|x|^2 \leq f(x) \leq D|x|^2$ , $$C|x| \leq \frac{f(x)}{|x|} \leq D|x|,$$ then $$\begin{eqnarray*} \lim_{h \to 0}\frac{f(0+h)-f(0)}{|h|} & = & \lim_{h\to 0}\frac{f(h)}{|h|}\\ & = & 0\\ & = & Df_{0}(h)  \end{eqnarray*},$$ that is, $f$ is differentiable and $Df_{0} \equiv 0$ . Is correct? Im not sure about (i).","Let be a continuous function such that for each e, moreover, for any and . (i) Determine . (ii) Show that there is such that for any . (iii) Is differentiable at ? My attempt. (i) Fix and define . So, is a continuous function and by Intermediate Value Theorem. Thus, . (ii) We have . We know that is compact, has a maximum and minimum, respectively, and . Therefore, (iii) Since , then that is, is differentiable and . Is correct? Im not sure about (i).","f:\mathbb{R}^{n} \to \mathbb{R} f(x) > 0 x \neq 0 f(tx)=t^2 f(x) x \in \mathbb{R}^{n} t \in \mathbb{R} f(\mathbb{R}^{n}) C,D>0 C\Vert x \Vert^2 \leq f(x) \leq D\Vert x \Vert^2 x \in \mathbb{R}^{n} f 0 x\neq 0 \varphi(t) = f(tx) =t^2f(x) = \alpha t^2 \varphi \varphi(\mathbb{R}) = [0,\infty) f(\mathbb{R}^n) = [0,\infty) f(x) = f\left(|x|\frac{x}{|x|}\right) = |x|^2f\left(\frac{x}{|x|}\right) S^{n-1} f(S^{n-1}) D C C|x|^2 \leq f(x) \leq D|x|^2. C|x|^2 \leq f(x) \leq D|x|^2 C|x| \leq \frac{f(x)}{|x|} \leq D|x|, \begin{eqnarray*}
\lim_{h \to 0}\frac{f(0+h)-f(0)}{|h|} & = & \lim_{h\to 0}\frac{f(h)}{|h|}\\
& = & 0\\
& = & Df_{0}(h) 
\end{eqnarray*}, f Df_{0} \equiv 0","['real-analysis', 'derivatives']"
37,How can I graph the derivative of 1/4th of a circle or a semicircle in a piecewise function? (Also other kinds of piecewise functions),How can I graph the derivative of 1/4th of a circle or a semicircle in a piecewise function? (Also other kinds of piecewise functions),,"I'm having trouble with questions like these. In the first image, the original function is what is the two sharp lines and a semicircle in between. I understand how to find and graph the derivative of the parts that are straight lines however i don't understand how to find the derivative of the semicircle. I can see that the radius is 2 so $x^2+y^2 = 2^2 = 4$ Do I implicitly differentiate with respect to $x$ in order to get $y'$ and the graph that? If i do that I get $2x+2yy' = 0 \implies y' = \frac{-x}{y}$ Now what? Also this is a semicircle so must I differentiate $\frac{x^2+y^2}{2} = 2$ ? Similarly, in the second question I have 1/4th of a circle, how can I take the derivative of that and graph it. Finally, in the second question, I understand that the derivative of a parabola would be a linear function because (for example) the derivative of $x^2$ is $2x$ but how can I graph this with just the information that it is a parabola and I'm not given the function itself. I know that if the function is decreasing then the derivative must be negative. I understand intuitively why it must be linear but is there any graphing ""rule"" which would make me know this? EDIT: is it as simple as (for the first question) that the function is decreasing so the derivative will be below the x-axis and for the second question that the inflection point is at $x=2$ so it is decreasing from 0 to 2 so function will be negative and increasing from 2 to 4 so it will be positive there? But how does it get that shape that makes it look like an $x^3$ graph? it could look like a line, or a parabola or anything, why does it have that specific shape?","I'm having trouble with questions like these. In the first image, the original function is what is the two sharp lines and a semicircle in between. I understand how to find and graph the derivative of the parts that are straight lines however i don't understand how to find the derivative of the semicircle. I can see that the radius is 2 so Do I implicitly differentiate with respect to in order to get and the graph that? If i do that I get Now what? Also this is a semicircle so must I differentiate ? Similarly, in the second question I have 1/4th of a circle, how can I take the derivative of that and graph it. Finally, in the second question, I understand that the derivative of a parabola would be a linear function because (for example) the derivative of is but how can I graph this with just the information that it is a parabola and I'm not given the function itself. I know that if the function is decreasing then the derivative must be negative. I understand intuitively why it must be linear but is there any graphing ""rule"" which would make me know this? EDIT: is it as simple as (for the first question) that the function is decreasing so the derivative will be below the x-axis and for the second question that the inflection point is at so it is decreasing from 0 to 2 so function will be negative and increasing from 2 to 4 so it will be positive there? But how does it get that shape that makes it look like an graph? it could look like a line, or a parabola or anything, why does it have that specific shape?",x^2+y^2 = 2^2 = 4 x y' 2x+2yy' = 0 \implies y' = \frac{-x}{y} \frac{x^2+y^2}{2} = 2 x^2 2x x=2 x^3,"['calculus', 'derivatives', 'piecewise-continuity']"
38,Solving an equation that contains a variable which brings 0=0 issue,Solving an equation that contains a variable which brings 0=0 issue,,"So I encountered this question: Given that the relationship between distance (m) and velocity (v) of an object is $$v^2 = 1 - m^3$$ Find the acceleration of the object when $m=1$ By taking the derivative of each side with respect to $t$ $2v \frac{dv}{dt} = -3m^2 \frac{dm}{dt} $ and we know that $\frac{dv}{dt}$ = acceleration, $\frac{dm}{dt}$ is equal to $v$ , then: $2v  a = -3m^2 v$ and by solving for $a$ without dividing by $v$ since $v$ can be zero, $a = -3/2$ But hold on... we've just substituted $m$ with 1, this means that we have to substitute $v$ with zero, but if we do this we would turn out with 0 = 0, without solving for $a$ That's how our teacher solved it. My questions are: is it permissible to substitute the value for a variable and keep the other, even if i know its value? And why did we treat $v$ here as any other number other than zero?","So I encountered this question: Given that the relationship between distance (m) and velocity (v) of an object is Find the acceleration of the object when By taking the derivative of each side with respect to and we know that = acceleration, is equal to , then: and by solving for without dividing by since can be zero, But hold on... we've just substituted with 1, this means that we have to substitute with zero, but if we do this we would turn out with 0 = 0, without solving for That's how our teacher solved it. My questions are: is it permissible to substitute the value for a variable and keep the other, even if i know its value? And why did we treat here as any other number other than zero?",v^2 = 1 - m^3 m=1 t 2v \frac{dv}{dt} = -3m^2 \frac{dm}{dt}  \frac{dv}{dt} \frac{dm}{dt} v 2v  a = -3m^2 v a v v a = -3/2 m v a v,"['derivatives', 'chain-rule']"
39,What can you do if the higher-order derivative test is inconclusive?,What can you do if the higher-order derivative test is inconclusive?,,"The second-derivative test states that if $x$ is a real number such that $f'(x)=0$, then: If $f''(x)>0$, then $f$ has a local minimum at $x$. If $f''(x)<0$, then $f$ has a local maximum at $x$. If $f''(x)=0$, then the text is inconclusive. But there's no need to despair if the second-derivative test is inconclusive, because there is the higher-order derivative test.  It states that if $x$ is a real number such that $f'(x)=0$, and $n$ is the smallest natural number such that $f^{(n)}(x)\neq 0$, then: If $n$ is even and $f^{(n)}>0$, then $f$ has a local minimum at $x$. If $n$ is even and $f^{(n)}<0$, then $f$ has a local manimum at $x$. If $n$ is odd, then $f$ has an inflection point at $x$. But the higher-order derivative test can also be inconclusive, if $f^{(n)}(x)=0$ for all $n$.  My question, what can you do if the higher-order derivative test is inconclusive? Is the first-derivative test the only option at that point, or are there other options? EDIT: I’m interested in finding a method that depends only on the germ of $f$. EDIT 2: Let me explain more precisely what I’m saying regarding the germ.  Let $X$ be the set of all functions $f$ infinitely differentiable at $a$ where $f^{(n)}(a)=0$ for all $n$.  Two functions $f$ and $g$ in $X$ belong to the same germ if there is an open interval $I$ containing $a$ such that $f(x) = g(x)$ for all $x$ in $I$.  Now let $Y$ be the set of germs of $X$.  I want to know if there exists a nontrivial function $F:Y\rightarrow\mathbb{R}$ such that if $F$ evaluated at a particular germ yields a positive number, then all the functions in the germ have a local minimum at $a$, and if it yields a negative number then all the functions in the germ have a local maximum at $a$.","The second-derivative test states that if $x$ is a real number such that $f'(x)=0$, then: If $f''(x)>0$, then $f$ has a local minimum at $x$. If $f''(x)<0$, then $f$ has a local maximum at $x$. If $f''(x)=0$, then the text is inconclusive. But there's no need to despair if the second-derivative test is inconclusive, because there is the higher-order derivative test.  It states that if $x$ is a real number such that $f'(x)=0$, and $n$ is the smallest natural number such that $f^{(n)}(x)\neq 0$, then: If $n$ is even and $f^{(n)}>0$, then $f$ has a local minimum at $x$. If $n$ is even and $f^{(n)}<0$, then $f$ has a local manimum at $x$. If $n$ is odd, then $f$ has an inflection point at $x$. But the higher-order derivative test can also be inconclusive, if $f^{(n)}(x)=0$ for all $n$.  My question, what can you do if the higher-order derivative test is inconclusive? Is the first-derivative test the only option at that point, or are there other options? EDIT: I’m interested in finding a method that depends only on the germ of $f$. EDIT 2: Let me explain more precisely what I’m saying regarding the germ.  Let $X$ be the set of all functions $f$ infinitely differentiable at $a$ where $f^{(n)}(a)=0$ for all $n$.  Two functions $f$ and $g$ in $X$ belong to the same germ if there is an open interval $I$ containing $a$ such that $f(x) = g(x)$ for all $x$ in $I$.  Now let $Y$ be the set of germs of $X$.  I want to know if there exists a nontrivial function $F:Y\rightarrow\mathbb{R}$ such that if $F$ evaluated at a particular germ yields a positive number, then all the functions in the germ have a local minimum at $a$, and if it yields a negative number then all the functions in the germ have a local maximum at $a$.",,"['calculus', 'real-analysis', 'derivatives', 'optimization', 'maxima-minima']"
40,Asymptotic Stability Criterion,Asymptotic Stability Criterion,,"I'm currently reading Saber Elaydi's Introduction to Difference Equations. I am confused with the proof of theorem 1.15 which states the following: Let $x^*$ be an equilibrium point of $x(n+1)=f(x(n))$ where $f$ is continuously differentiable at $x^*$ . Then it follows that: If $f''(x^*)\neq 0$ , then $x^*$ is unstable. If $f''(x^*)=0$ and $f'''(x^*)>0$ , then $x^*$ is unstable. If $f''(x^*)=0$ and $f'''(x^*)<0$ , then $x^*$ is asymptotically stable. The proof of the first item is given in the book (pp. 30), while the other two are not. Those are left as exercises. The proof of the first item starts off by stating that $f$ must be either concave upward or downward depending on the sign of $f''(x^*)$ . Then we assume $f''(x^*)> 0$ , since this is the case, it follows that $f'$ is increasing on a neighborhood of $x^*$ . My first question comes from this part. The author states: If $f''(x^*)>0$ , then $f'(x)>1$ for all $x$ in a small interval $\rbrack x^*,x^*+\varepsilon\lbrack$ . Then it would follow by another stability criterion that $f$ is unstable at $x$ . Does that hold for the whole neighborhood of $x$ ? Including $x^*$ , even if it is in the boundary of the neighborhood? I don't know how to justifiy this. How does $f''(x^*)>0$ , imply $f'(x)>1$ ? My first attempt to answer this question was to say: Since $f'$ grows near $x^*$ then in some neighborhood it must grow around 1. I know it sounds wrong, but it didn't when I thought of it the first time. The problem with my idea is that $f'$ can be bounded and I don't know if $\varepsilon $ can be made as large as I want. This also threw me off when reading the next part. (...) if $f''(x^*)<0$ , then $f'(x)>1$ for $x\in\rbrack x^*-\varepsilon,x^*\lbrack$ . Once again by a previous criterion, $x^*$ is unstable. How does $f'(x)>1$ follow from $f''(x^*)<0$ ? If the same conclusion followed from $f''(x^*)>0$ , in which way is that hypothesis being used? That $f''(x^*)<0$ implies that $f'$ is decreasing in a neighborhood of $x^*$ . I know that from the left, the values of $f'$ must be bigger than $f'(x^*)$ . But what if $f'$ is bounded, or always negative? I can't wrap my head around this. These are all my questions for the first part of the proof. I tried gathering some ideas for the second statement. If $f'''(x^*)>0$ then by the same argument as before $f''(x)>1$ in a neighborhood of $x^*$ , in particular $f''(x)>0$ Then I think that the argument should run as before. For the third statement, I am trying to get to the condition $|f'(x^*)|<1$ to inmediately use the stability criterion and show that $x^*$ is asymptotically stable. However I can also try to get to the definition of being asymptotically stable, but the function with its third derivative at this point seems too convoluted to actuall try to use the definition. Any lead for the second and third part is greatly appreciated.","I'm currently reading Saber Elaydi's Introduction to Difference Equations. I am confused with the proof of theorem 1.15 which states the following: Let be an equilibrium point of where is continuously differentiable at . Then it follows that: If , then is unstable. If and , then is unstable. If and , then is asymptotically stable. The proof of the first item is given in the book (pp. 30), while the other two are not. Those are left as exercises. The proof of the first item starts off by stating that must be either concave upward or downward depending on the sign of . Then we assume , since this is the case, it follows that is increasing on a neighborhood of . My first question comes from this part. The author states: If , then for all in a small interval . Then it would follow by another stability criterion that is unstable at . Does that hold for the whole neighborhood of ? Including , even if it is in the boundary of the neighborhood? I don't know how to justifiy this. How does , imply ? My first attempt to answer this question was to say: Since grows near then in some neighborhood it must grow around 1. I know it sounds wrong, but it didn't when I thought of it the first time. The problem with my idea is that can be bounded and I don't know if can be made as large as I want. This also threw me off when reading the next part. (...) if , then for . Once again by a previous criterion, is unstable. How does follow from ? If the same conclusion followed from , in which way is that hypothesis being used? That implies that is decreasing in a neighborhood of . I know that from the left, the values of must be bigger than . But what if is bounded, or always negative? I can't wrap my head around this. These are all my questions for the first part of the proof. I tried gathering some ideas for the second statement. If then by the same argument as before in a neighborhood of , in particular Then I think that the argument should run as before. For the third statement, I am trying to get to the condition to inmediately use the stability criterion and show that is asymptotically stable. However I can also try to get to the definition of being asymptotically stable, but the function with its third derivative at this point seems too convoluted to actuall try to use the definition. Any lead for the second and third part is greatly appreciated.","x^* x(n+1)=f(x(n)) f x^* f''(x^*)\neq 0 x^* f''(x^*)=0 f'''(x^*)>0 x^* f''(x^*)=0 f'''(x^*)<0 x^* f f''(x^*) f''(x^*)> 0 f' x^* f''(x^*)>0 f'(x)>1 x \rbrack x^*,x^*+\varepsilon\lbrack f x x x^* f''(x^*)>0 f'(x)>1 f' x^* f' \varepsilon  f''(x^*)<0 f'(x)>1 x\in\rbrack x^*-\varepsilon,x^*\lbrack x^* f'(x)>1 f''(x^*)<0 f''(x^*)>0 f''(x^*)<0 f' x^* f' f'(x^*) f' f'''(x^*)>0 f''(x)>1 x^* f''(x)>0 |f'(x^*)|<1 x^*","['derivatives', 'recurrence-relations', 'proof-explanation', 'stability-theory']"
41,"$f : \mathbb R \to [-2,2]$ be a twice differentiable with $f(0)^2+ f'(0)^2=85$",be a twice differentiable with,"f : \mathbb R \to [-2,2] f(0)^2+ f'(0)^2=85","Let $f : \mathbb R \to [-2,2]$ be a twice differentiable with $f(0)^2+ f'(0)^2=85$.  Is it true that $\exists a \in (-4,4)$ such that $f'(a) \ne 0$ and $f(a)+f''(a)=0$ ? I think I have to use IVT / MVT to some nicely constructed new function, but I am unable to see what. Please help.","Let $f : \mathbb R \to [-2,2]$ be a twice differentiable with $f(0)^2+ f'(0)^2=85$.  Is it true that $\exists a \in (-4,4)$ such that $f'(a) \ne 0$ and $f(a)+f''(a)=0$ ? I think I have to use IVT / MVT to some nicely constructed new function, but I am unable to see what. Please help.",,['real-analysis']
42,Show that the following space curve $c$ is $C^{\infty}$ and Compute binormal.,Show that the following space curve  is  and Compute binormal.,c C^{\infty},"Consider the space curve $c : (-\frac{1}{2},\frac{1}{2}) \rightarrow \mathbb{R^3}$ , $$c(x)= \begin{cases} (x,e^{-\frac{1}{x^2}},0)  &\ x<0\\ \ \\ (0,0,0)&\ x=0 \\ \ \\ (x,0,e^{-\frac{1}{x^2}}) &\ x>0 \end{cases} $$ Show: $\cdot$ $c$ is a regular $C^{\infty}$  curve and $ \kappa: (-\frac{1}{2},\frac{1}{2}) $ $\rightarrow \mathbb{R_{\geq 0}}$ a $C^{\infty}$ function. $\cdot$ $B(x) =(0,0,1)$ for $x < 0$ and $B(x) = (0,-1,0)$ for $x>0$ . $\kappa$ = curvature with $\kappa(x)$ := $\frac{|| c' \times c''||(x)}{||c'(x)||^3} $. $B$ = binormal with $B(x)$ := $T(x) \times N(x)$ $\tau$ = torsion with $\tau(x)$ := $\frac{det(c',c'',c''')(x)}{|| c' \times c''||^2(x)} $ $T(x)$ := $\frac{1}{||c'(x)||} \cdot c'(x) $. $N(x)$ := $\frac{1}{||T'(x)||} \cdot T'(x) $. My thoughts : a curve is regular, if $|| c'(x) || \neq 0$. $$c'(x)= \begin{cases} (1,\frac{2}{x^3}e^{-\frac{1}{x^2}},0)  &\ x<0\\ \ \\ (1,0,0)&\ x=0 \\ \ \\ (1,0,\frac{2}{x^3}e^{-\frac{1}{x^2}}) &\ x>0 \end{cases} $$  Do I have a mistake here? Next point: It's clear to me that $c$ is a $C^{\infty}$ function for $x \neq 0$, but I have problems with $x=0$. Next point: for the curvature we need $c''$.$$c''(x)= \begin{cases} (0,-\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}},0)  &\ x<0\\ \ \\ (0,0,0)&\ x=0 \\ \ \\ (0,0,-\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}}) &\ x>0 \end{cases} $$ $$\kappa(x) \begin{cases} \frac{|\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}}|}{(1+(\frac{2}{x^3} e^{-\frac{1}{x^2}})^2)^{\frac{3}{2}}} &\ x<0\\ \ \\ 0&\ x=0 \\ \ \\ \frac{|\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}}|}{(1+(\frac{2}{x^3} e^{-\frac{1}{x^2}})^2)^{\frac{3}{2}}} &\ x>0 \end{cases} $$ I know that my result looks terrible. I really need help here. Is there a trick?  How can I solve the other claims? I mean how can I solve them, if my results looks so terrible.","Consider the space curve $c : (-\frac{1}{2},\frac{1}{2}) \rightarrow \mathbb{R^3}$ , $$c(x)= \begin{cases} (x,e^{-\frac{1}{x^2}},0)  &\ x<0\\ \ \\ (0,0,0)&\ x=0 \\ \ \\ (x,0,e^{-\frac{1}{x^2}}) &\ x>0 \end{cases} $$ Show: $\cdot$ $c$ is a regular $C^{\infty}$  curve and $ \kappa: (-\frac{1}{2},\frac{1}{2}) $ $\rightarrow \mathbb{R_{\geq 0}}$ a $C^{\infty}$ function. $\cdot$ $B(x) =(0,0,1)$ for $x < 0$ and $B(x) = (0,-1,0)$ for $x>0$ . $\kappa$ = curvature with $\kappa(x)$ := $\frac{|| c' \times c''||(x)}{||c'(x)||^3} $. $B$ = binormal with $B(x)$ := $T(x) \times N(x)$ $\tau$ = torsion with $\tau(x)$ := $\frac{det(c',c'',c''')(x)}{|| c' \times c''||^2(x)} $ $T(x)$ := $\frac{1}{||c'(x)||} \cdot c'(x) $. $N(x)$ := $\frac{1}{||T'(x)||} \cdot T'(x) $. My thoughts : a curve is regular, if $|| c'(x) || \neq 0$. $$c'(x)= \begin{cases} (1,\frac{2}{x^3}e^{-\frac{1}{x^2}},0)  &\ x<0\\ \ \\ (1,0,0)&\ x=0 \\ \ \\ (1,0,\frac{2}{x^3}e^{-\frac{1}{x^2}}) &\ x>0 \end{cases} $$  Do I have a mistake here? Next point: It's clear to me that $c$ is a $C^{\infty}$ function for $x \neq 0$, but I have problems with $x=0$. Next point: for the curvature we need $c''$.$$c''(x)= \begin{cases} (0,-\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}},0)  &\ x<0\\ \ \\ (0,0,0)&\ x=0 \\ \ \\ (0,0,-\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}}) &\ x>0 \end{cases} $$ $$\kappa(x) \begin{cases} \frac{|\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}}|}{(1+(\frac{2}{x^3} e^{-\frac{1}{x^2}})^2)^{\frac{3}{2}}} &\ x<0\\ \ \\ 0&\ x=0 \\ \ \\ \frac{|\frac{(6x^2-4)}{x^6}e^{-\frac{1}{x^2}}|}{(1+(\frac{2}{x^3} e^{-\frac{1}{x^2}})^2)^{\frac{3}{2}}} &\ x>0 \end{cases} $$ I know that my result looks terrible. I really need help here. Is there a trick?  How can I solve the other claims? I mean how can I solve them, if my results looks so terrible.",,"['derivatives', 'differential-geometry', 'curves', 'curvature']"
43,Finding the Weingarten Map of a simple surface,Finding the Weingarten Map of a simple surface,,"I'm trying to find the Weingarten map of the surface parametrized by $\sigma(u, v) = (u, v, u^2 + v^2)$. Now, one can easily find that the Gauss map (which is just the surface normal $\bf N$) is given by $$ {\bf N}(\sigma(u, v)) = -\frac{(u,v,1/2)}{\sqrt{u^2+v^2+1/4}} $$ The Weingarten map is the negative derivative of this, $\mathcal{W} = -D_{\vec p}{\bf N}$. To calculate the derivative of a smooth function $f$ from a surface $S$ parametrized by $\sigma$, onto a surface $\tilde{S}$ parametrized by $\tilde{\sigma}$, one first needs to find functions $\alpha$ and $\beta$ such that $$ f(\sigma(u, v)) = \tilde{\sigma}(\alpha(u, v), \beta(u, v)) $$ so that, for a curve on $S$, $\gamma(t) = \sigma(u(t), v(t))$, the tangent vector $(f\circ\gamma)'$ is given by $$ \tilde{\sigma}_\alpha(\alpha_uu'+\alpha_vv') + \tilde{\sigma}_\beta(\beta_uu'+\beta_vv') $$ In this case, since the Gauss map maps from the tangent space to the surface, $T_{\vec p}S$, onto itself, we need to find $\alpha$ and $\beta$ such that $$ {\bf N}(\sigma(u,v)) = \sigma(\alpha(u,v),\beta(u,v)) $$ but $$ \sigma(\alpha(u,v),\beta(u,v)) = (\alpha(u,v),\beta(u,v),\alpha^2(u,v)+\beta^2(u,v)) $$ and so clearly, we must have $$ \alpha(u,v) = -\frac{u}{\sqrt{u^2+v^2+1/4}} \\ \beta(u,v) = -\frac{v}{\sqrt{u^2+v^2+1/4}} $$ but then $\alpha^2(u,v) + \beta^2(u,v)$ doesn't equal the last component, so there aren't functions $\alpha$ and $\beta$ that satifsy the above equality. What is happening? I can't figure this out. From this, it seems like the Weingarten map shouldn't exist , but clearly it does. Any help here would be greatly appreciated!","I'm trying to find the Weingarten map of the surface parametrized by $\sigma(u, v) = (u, v, u^2 + v^2)$. Now, one can easily find that the Gauss map (which is just the surface normal $\bf N$) is given by $$ {\bf N}(\sigma(u, v)) = -\frac{(u,v,1/2)}{\sqrt{u^2+v^2+1/4}} $$ The Weingarten map is the negative derivative of this, $\mathcal{W} = -D_{\vec p}{\bf N}$. To calculate the derivative of a smooth function $f$ from a surface $S$ parametrized by $\sigma$, onto a surface $\tilde{S}$ parametrized by $\tilde{\sigma}$, one first needs to find functions $\alpha$ and $\beta$ such that $$ f(\sigma(u, v)) = \tilde{\sigma}(\alpha(u, v), \beta(u, v)) $$ so that, for a curve on $S$, $\gamma(t) = \sigma(u(t), v(t))$, the tangent vector $(f\circ\gamma)'$ is given by $$ \tilde{\sigma}_\alpha(\alpha_uu'+\alpha_vv') + \tilde{\sigma}_\beta(\beta_uu'+\beta_vv') $$ In this case, since the Gauss map maps from the tangent space to the surface, $T_{\vec p}S$, onto itself, we need to find $\alpha$ and $\beta$ such that $$ {\bf N}(\sigma(u,v)) = \sigma(\alpha(u,v),\beta(u,v)) $$ but $$ \sigma(\alpha(u,v),\beta(u,v)) = (\alpha(u,v),\beta(u,v),\alpha^2(u,v)+\beta^2(u,v)) $$ and so clearly, we must have $$ \alpha(u,v) = -\frac{u}{\sqrt{u^2+v^2+1/4}} \\ \beta(u,v) = -\frac{v}{\sqrt{u^2+v^2+1/4}} $$ but then $\alpha^2(u,v) + \beta^2(u,v)$ doesn't equal the last component, so there aren't functions $\alpha$ and $\beta$ that satifsy the above equality. What is happening? I can't figure this out. From this, it seems like the Weingarten map shouldn't exist , but clearly it does. Any help here would be greatly appreciated!",,"['derivatives', 'differential-geometry']"
44,"Prove that $f'$ is continuous, knowing that it has lateral limits","Prove that  is continuous, knowing that it has lateral limits",f',"Let $f:\mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(x)-f'(x)$ is monotonic. Prove that $f'$ is continuous $f(x)-f'(x)$ is monotonic, so it has finite left/right-hand limits at any point. The same can be said about $f$, because it is continuous, so $f'$ has also finite left/right-hand limits at any point $a\in \mathbb{R}$, call them $l=\lim_{x\nearrow a}f'(x)$ and $r=\lim_{x \searrow a}f'(x).$ But $f'(a)=\lim_{x \to a}\frac{f(x)-f(a)}{x-a}$, so $$f'(a)=\lim_{x \nearrow a}\frac{f(x)-f(a)}{x-a}=\lim_{x \nearrow a} f'(x)=l$$ and $$f'(a)=\lim_{x \searrow a}\frac{f(x)-f(a)}{x-a}=\lim_{x \searrow a} f'(x)=r$$ from l'Hospital rule. But this means that $f'(a)=l=r$, so $f'$ is continuous at $a$. Is this proof ok?","Let $f:\mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(x)-f'(x)$ is monotonic. Prove that $f'$ is continuous $f(x)-f'(x)$ is monotonic, so it has finite left/right-hand limits at any point. The same can be said about $f$, because it is continuous, so $f'$ has also finite left/right-hand limits at any point $a\in \mathbb{R}$, call them $l=\lim_{x\nearrow a}f'(x)$ and $r=\lim_{x \searrow a}f'(x).$ But $f'(a)=\lim_{x \to a}\frac{f(x)-f(a)}{x-a}$, so $$f'(a)=\lim_{x \nearrow a}\frac{f(x)-f(a)}{x-a}=\lim_{x \nearrow a} f'(x)=l$$ and $$f'(a)=\lim_{x \searrow a}\frac{f(x)-f(a)}{x-a}=\lim_{x \searrow a} f'(x)=r$$ from l'Hospital rule. But this means that $f'(a)=l=r$, so $f'$ is continuous at $a$. Is this proof ok?",,"['calculus', 'real-analysis', 'derivatives', 'proof-verification']"
45,Derivative of an expansion of a function in spherical harmonics,Derivative of an expansion of a function in spherical harmonics,,"Recall that, for fixed $k \in Z_+\cup \{0\}$, a spherical harmonic of degree $k$ is the restriction to $S^{n-1}$ of a harmonic polynomial on $R^n$ that is homogeneous of degree $k$. Let $Y_{k,1}\, ,Y_{k,2}\,\ldots, Y_{k,d_{n-1}(k)}$, with $\displaystyle{d_{n-1}(k):={n+k-1\choose k}}$, be an orthonormal basis of the subspace in $L^{2}(S^{n-1})$ of degree $k$ of spherical harmonics. Then, $\{Y_{k,m}\}$ is an orthonormal basis for $L^2(S^{n-1})$, i.e. for any $g\in L^2(S^{n-1})$, one has: $$ g=\sum_{k,m}g_{k,m}Y_{k,m}, $$ with $$ g_{k,m}:=\int_{S^{n-1}}g(\omega)Y_{k,m}(\omega) d\omega, \quad k=0,1, \ldots; m=1,2, \ldots, d_{n-1}(k), $$ This so-called spherical harmonic expansion of $g$ converging to it in $L^2(S^{n-1})$. QUESTION: How to find high order derivative $g^{(\ell)}$.","Recall that, for fixed $k \in Z_+\cup \{0\}$, a spherical harmonic of degree $k$ is the restriction to $S^{n-1}$ of a harmonic polynomial on $R^n$ that is homogeneous of degree $k$. Let $Y_{k,1}\, ,Y_{k,2}\,\ldots, Y_{k,d_{n-1}(k)}$, with $\displaystyle{d_{n-1}(k):={n+k-1\choose k}}$, be an orthonormal basis of the subspace in $L^{2}(S^{n-1})$ of degree $k$ of spherical harmonics. Then, $\{Y_{k,m}\}$ is an orthonormal basis for $L^2(S^{n-1})$, i.e. for any $g\in L^2(S^{n-1})$, one has: $$ g=\sum_{k,m}g_{k,m}Y_{k,m}, $$ with $$ g_{k,m}:=\int_{S^{n-1}}g(\omega)Y_{k,m}(\omega) d\omega, \quad k=0,1, \ldots; m=1,2, \ldots, d_{n-1}(k), $$ This so-called spherical harmonic expansion of $g$ converging to it in $L^2(S^{n-1})$. QUESTION: How to find high order derivative $g^{(\ell)}$.",,"['derivatives', 'partial-derivative', 'harmonic-analysis', 'spherical-geometry', 'spherical-harmonics']"
46,Questions about $g(x)= \inf_{h \in \mathbb{R}} \frac{f(x+h)-f(x)}{h}$.,Questions about .,g(x)= \inf_{h \in \mathbb{R}} \frac{f(x+h)-f(x)}{h},I have several question about the following functions \begin{align} g_1(x)= \inf_{h \in \mathbb{R}} \frac{f(x+h)-f(x)}{h} \\ g_2(x)= \inf_{h \in \mathbb{R}} \left| \frac{f(x+h)-f(x)}{h} \right| \end{align} Note that if $f$ is differentiable at $x$ then \begin{align} g_1(x) \le f^\prime(x).  \end{align} Do $g_1(x)$ and $g_2(x)$  have names? Do $g_1(x)$ and $g_2(x)$ have any applications? What is required for $g_1(x)=f^\prime(x)$  for some given $x$? Are there better bounds on $g_1(x)$ than  $g_1(x) \le f^\prime(x)$?,I have several question about the following functions \begin{align} g_1(x)= \inf_{h \in \mathbb{R}} \frac{f(x+h)-f(x)}{h} \\ g_2(x)= \inf_{h \in \mathbb{R}} \left| \frac{f(x+h)-f(x)}{h} \right| \end{align} Note that if $f$ is differentiable at $x$ then \begin{align} g_1(x) \le f^\prime(x).  \end{align} Do $g_1(x)$ and $g_2(x)$  have names? Do $g_1(x)$ and $g_2(x)$ have any applications? What is required for $g_1(x)=f^\prime(x)$  for some given $x$? Are there better bounds on $g_1(x)$ than  $g_1(x) \le f^\prime(x)$?,,"['calculus', 'real-analysis', 'derivatives']"
47,Continuous right derivative implies differentiability,Continuous right derivative implies differentiability,,"A book of mine says the following is true, and I am having some trouble proving it. (I've considered using the Lebesgue differentiation theorem and absolute continuity, as well as elementary analysis methods.) Let $f: [0, \infty) \rightarrow \mathbb{R}$ be continuous and have right derivatives at each point in the domain, with the right derivative function being continuous.  Then $f$ is differentiable.","A book of mine says the following is true, and I am having some trouble proving it. (I've considered using the Lebesgue differentiation theorem and absolute continuity, as well as elementary analysis methods.) Let $f: [0, \infty) \rightarrow \mathbb{R}$ be continuous and have right derivatives at each point in the domain, with the right derivative function being continuous.  Then $f$ is differentiable.",,"['real-analysis', 'derivatives']"
48,Find the derivative of integral $f(x)/(x^2(x-5)^7)$ when $f(x)$ is a quadratic function.,Find the derivative of integral  when  is a quadratic function.,f(x)/(x^2(x-5)^7) f(x),"This question is quite tricky. It's for my Calculus 2 assignment and I can't seem to figure out how to integrate this function in order to get its derivative. I tried partial fractions, u-sub with x-5 and x^2, but nothing seems to work. All I conluded is that the quadratic function y-coordinate is -4. Can someone help me with this? Regards, You Xiao Ruan.","This question is quite tricky. It's for my Calculus 2 assignment and I can't seem to figure out how to integrate this function in order to get its derivative. I tried partial fractions, u-sub with x-5 and x^2, but nothing seems to work. All I conluded is that the quadratic function y-coordinate is -4. Can someone help me with this? Regards, You Xiao Ruan.",,"['calculus', 'integration', 'derivatives', 'partial-fractions']"
49,Are these relations possible to prove without defining this new kind of derivative?,Are these relations possible to prove without defining this new kind of derivative?,,"I'm using these notations: 1.$log^n_xy$: For log with the base $x$ applied $n$ times to $y$. For example, $log^3y=log(log(log(y))$ all with the same base. 2.$^{n[x]}a$: For the power tower or repeated exponentiation of $a$ evaluated from right to left such that the number of $a$'s in the operation is $n$ and it has $x$ on the top. For example, $$^{3[5]}2=2^{2^{2^5}}$$, $$^{0[8]}2=8$$ The power tower contains no $2$'s in the second case. I've defined this new kind of derivative: $$\lim_{h\rightarrow0}\log_{\frac{log^nx+h}{log^nx}}\frac{log^nf(x+h)}{log^nf(x)}=\frac{dy}{dx}\frac{x}{y}\prod_{k=1}^n\frac{log^kx}{log^ky}$$ where $\frac{dy}{dx}$ is the normal derivative of $f(x)$ and all the logarithms whose base is unspecified should be calculated to the the same base $a$. I've not found any use of this derivative except in approximations, in which it is better than the normal derivative. I've got these approximation results: By taking the derivative at $n=0$, $$e^{x_2}\approx e^{x_1}(\frac{x_2}{x_1})^{x_1}$$, $$x_2^2+x_2\approx (x_1^2+x_1)(\frac{x_2}{x_1})^{\frac{2x_1+1}{x_1+1}}$$ $$ln(x_2)\approx ln(x_1)(\frac{x_2}{x_1})^{\frac{1}{ln(x_1)}}$$ when $\frac{x_2}{x_1}\approx 1$ $$^{(n)[kx]}e\approx ^{(n)[x]}e+\log_{e}k\prod_{i=0}^n{^{i[x]}}e$$, when $k\approx 1$ $$\frac{\log_ax_2-\log_ax_1}{\log^{n+1}_ax_2-\log^{n+1}_ax_1}*\frac{1}{((ln(a))^n}\approx \prod_{i=1}^nlog_a^ix_1$$, when $\frac{\log_{a}^{n}x_2}{\log_{a}^{n}x_1}\approx 1$ Can these results be proved by using existing maths concepts without using this derivative of mine?","I'm using these notations: 1.$log^n_xy$: For log with the base $x$ applied $n$ times to $y$. For example, $log^3y=log(log(log(y))$ all with the same base. 2.$^{n[x]}a$: For the power tower or repeated exponentiation of $a$ evaluated from right to left such that the number of $a$'s in the operation is $n$ and it has $x$ on the top. For example, $$^{3[5]}2=2^{2^{2^5}}$$, $$^{0[8]}2=8$$ The power tower contains no $2$'s in the second case. I've defined this new kind of derivative: $$\lim_{h\rightarrow0}\log_{\frac{log^nx+h}{log^nx}}\frac{log^nf(x+h)}{log^nf(x)}=\frac{dy}{dx}\frac{x}{y}\prod_{k=1}^n\frac{log^kx}{log^ky}$$ where $\frac{dy}{dx}$ is the normal derivative of $f(x)$ and all the logarithms whose base is unspecified should be calculated to the the same base $a$. I've not found any use of this derivative except in approximations, in which it is better than the normal derivative. I've got these approximation results: By taking the derivative at $n=0$, $$e^{x_2}\approx e^{x_1}(\frac{x_2}{x_1})^{x_1}$$, $$x_2^2+x_2\approx (x_1^2+x_1)(\frac{x_2}{x_1})^{\frac{2x_1+1}{x_1+1}}$$ $$ln(x_2)\approx ln(x_1)(\frac{x_2}{x_1})^{\frac{1}{ln(x_1)}}$$ when $\frac{x_2}{x_1}\approx 1$ $$^{(n)[kx]}e\approx ^{(n)[x]}e+\log_{e}k\prod_{i=0}^n{^{i[x]}}e$$, when $k\approx 1$ $$\frac{\log_ax_2-\log_ax_1}{\log^{n+1}_ax_2-\log^{n+1}_ax_1}*\frac{1}{((ln(a))^n}\approx \prod_{i=1}^nlog_a^ix_1$$, when $\frac{\log_{a}^{n}x_2}{\log_{a}^{n}x_1}\approx 1$ Can these results be proved by using existing maths concepts without using this derivative of mine?",,['derivatives']
50,Differentiability of an exotic function,Differentiability of an exotic function,,"I saw a question on stackexchange earlier today about the function $f(x)=\begin{cases} 1&\text{if }x\in\mathbb{Q}\\ 0&\text{otherwise.} \end{cases}$ It reminded me of an old problem (that I don't think I solved), and I thought it was worth sharing with others. Let \begin{equation*} g(x)=\begin{cases} \frac{1}{q^j}&\text{if }x\in\mathbb{Q}\\ 0&\text{otherwise.} \end{cases} \end{equation*} For rational values of $x$, we have $x=\frac{p}{q}$ is in lowest terms. Additionally, $j> 1$. 1) One of the questions that was asked was to show the following: $g$ is differentiable for $x\in\mathbb{R}\setminus\mathbb{Q}$. I don't think this is very difficult; however, I had trouble with the next one: 2) Show that $g$ is $k$-times differentiable for $x\in\mathbb{R}\setminus\mathbb{Q}$ and $k<j$. How do we prove (2)? Is it even true? For the second derivative, we take  \begin{equation*} f''(x)=\lim_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2} \end{equation*} I'm not totally sure how one might extend to higher derivatives, and a quick google search did not yield what I was looking for! I would be happy if there was a proof for $k=2$ in my question.","I saw a question on stackexchange earlier today about the function $f(x)=\begin{cases} 1&\text{if }x\in\mathbb{Q}\\ 0&\text{otherwise.} \end{cases}$ It reminded me of an old problem (that I don't think I solved), and I thought it was worth sharing with others. Let \begin{equation*} g(x)=\begin{cases} \frac{1}{q^j}&\text{if }x\in\mathbb{Q}\\ 0&\text{otherwise.} \end{cases} \end{equation*} For rational values of $x$, we have $x=\frac{p}{q}$ is in lowest terms. Additionally, $j> 1$. 1) One of the questions that was asked was to show the following: $g$ is differentiable for $x\in\mathbb{R}\setminus\mathbb{Q}$. I don't think this is very difficult; however, I had trouble with the next one: 2) Show that $g$ is $k$-times differentiable for $x\in\mathbb{R}\setminus\mathbb{Q}$ and $k<j$. How do we prove (2)? Is it even true? For the second derivative, we take  \begin{equation*} f''(x)=\lim_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2} \end{equation*} I'm not totally sure how one might extend to higher derivatives, and a quick google search did not yield what I was looking for! I would be happy if there was a proof for $k=2$ in my question.",,"['calculus', 'real-analysis', 'derivatives']"
51,Finding the slope of an hypotenuse of the trigonometic circle,Finding the slope of an hypotenuse of the trigonometic circle,,"I've been learning about derivatives and I've learnt that the derivative of a circunference for $x$ is given by $-\frac{x}{y}$ with the formula for the circunference being $y^2+x^2=1$ Knowing this, I have tried to calculate the slope of a line that goes through center of this circle and a point in the circunference. To make things simpler, I decided to use the trigonometic circle. I did the following: Let this line be $f(x)$ and the tangent on the point where $f(x) = 1$ be $g(x)$ I know that $f(x) = mx$ because it goes through through the point $(0,0)$ and so $b=0$ Because the derivative of the circunference is $-\frac{x}{y}$, I know that $g(x) = -\frac{x}{y}x+b = -\frac{x^2}{y}+b$ I think that the tangent is perpendicular to $g(x)$, which means that $g(x)$ is its normal line. I don't know this for sure nor do I know how to prove it, it just looks that way for me so I am going explore that possibility and see what happens. Assuming that $g(x)$ is perpendicular to $f(x)$: If the slope of $g(x)$ is $-\frac{x}{y}$, then the slope of $f(x)$ is $\frac{y}{x}$, and so $f(x) = \frac{y}{x}x = y$, which doesn't really bring me anywhere... I think. Another way of calculating the slope of $f(x)$ would be the traditional way, replacing $x$ and $y$ with $(\cos(\theta);\sin(\theta))$ and so: $$f(x) = mx \Leftrightarrow \sin(\theta) = m\cos(\theta) \Leftrightarrow m = \frac{\sin(\theta)}{\cos(\theta)} \Leftrightarrow m = \tan(\theta)$$ And so $f(x) = \tan(\theta)x$ Another thing that I tried to do was to calculate $b$ for $g(x)$, using the point $(\cos(\theta);\sin(\theta))$: $$g(x) = -\frac{x^2}{y}+b \Leftrightarrow \sin(\theta) = -\frac{\cos^2(\theta)}{\sin(\theta)} \Leftrightarrow \sin(\theta)+\cos(\theta)\cot(\theta) = b \Leftrightarrow b = \csc(\theta)$$ And so $g(x) = -\frac{x^2}{y}+ \csc(\theta)$ EDIT: So, to recap: $$f(x) =  \tan(\theta)x  \\ f'(x) = \tan(\theta) \\ g(x) = -\frac{x^2}{y}+ \csc(\theta) \\ g'(x) = -\frac{x}{y}$$ Now my questions: Did I do everything correctly? Are $f(x)$ and $g(x)$ perpendicular? Did I prove it above when I got $f(x) = y$? If not, could you show me how to prove whether or not if they are perpendicular? I could never really understand the purpose of $\tan$. Is it the slope of the line that contains the hypotenuse? Feel free to expand on this subject if you want or point out any detail I might have missed.","I've been learning about derivatives and I've learnt that the derivative of a circunference for $x$ is given by $-\frac{x}{y}$ with the formula for the circunference being $y^2+x^2=1$ Knowing this, I have tried to calculate the slope of a line that goes through center of this circle and a point in the circunference. To make things simpler, I decided to use the trigonometic circle. I did the following: Let this line be $f(x)$ and the tangent on the point where $f(x) = 1$ be $g(x)$ I know that $f(x) = mx$ because it goes through through the point $(0,0)$ and so $b=0$ Because the derivative of the circunference is $-\frac{x}{y}$, I know that $g(x) = -\frac{x}{y}x+b = -\frac{x^2}{y}+b$ I think that the tangent is perpendicular to $g(x)$, which means that $g(x)$ is its normal line. I don't know this for sure nor do I know how to prove it, it just looks that way for me so I am going explore that possibility and see what happens. Assuming that $g(x)$ is perpendicular to $f(x)$: If the slope of $g(x)$ is $-\frac{x}{y}$, then the slope of $f(x)$ is $\frac{y}{x}$, and so $f(x) = \frac{y}{x}x = y$, which doesn't really bring me anywhere... I think. Another way of calculating the slope of $f(x)$ would be the traditional way, replacing $x$ and $y$ with $(\cos(\theta);\sin(\theta))$ and so: $$f(x) = mx \Leftrightarrow \sin(\theta) = m\cos(\theta) \Leftrightarrow m = \frac{\sin(\theta)}{\cos(\theta)} \Leftrightarrow m = \tan(\theta)$$ And so $f(x) = \tan(\theta)x$ Another thing that I tried to do was to calculate $b$ for $g(x)$, using the point $(\cos(\theta);\sin(\theta))$: $$g(x) = -\frac{x^2}{y}+b \Leftrightarrow \sin(\theta) = -\frac{\cos^2(\theta)}{\sin(\theta)} \Leftrightarrow \sin(\theta)+\cos(\theta)\cot(\theta) = b \Leftrightarrow b = \csc(\theta)$$ And so $g(x) = -\frac{x^2}{y}+ \csc(\theta)$ EDIT: So, to recap: $$f(x) =  \tan(\theta)x  \\ f'(x) = \tan(\theta) \\ g(x) = -\frac{x^2}{y}+ \csc(\theta) \\ g'(x) = -\frac{x}{y}$$ Now my questions: Did I do everything correctly? Are $f(x)$ and $g(x)$ perpendicular? Did I prove it above when I got $f(x) = y$? If not, could you show me how to prove whether or not if they are perpendicular? I could never really understand the purpose of $\tan$. Is it the slope of the line that contains the hypotenuse? Feel free to expand on this subject if you want or point out any detail I might have missed.",,"['calculus', 'trigonometry', 'derivatives', 'tangent-line']"
52,How to prove with Lagrange theorem,How to prove with Lagrange theorem,,"How can I prove that: if $f: \Bbb R \rightarrow \Bbb R$ is convex and differentiable, such that $y = 0$ is an asymptote for $x \rightarrow \infty$, then $f(x) \ge 0, \forall x \in \Bbb R$ How can I prove that $f(x) \ge 0$ with the Lagrange theorem?","How can I prove that: if $f: \Bbb R \rightarrow \Bbb R$ is convex and differentiable, such that $y = 0$ is an asymptote for $x \rightarrow \infty$, then $f(x) \ge 0, \forall x \in \Bbb R$ How can I prove that $f(x) \ge 0$ with the Lagrange theorem?",,"['calculus', 'derivatives']"
53,"Show that $\sum\limits_{k=1}^{n-1} (n-k) x^k$ is non-decreasing for $x \in ]-1,1[$.",Show that  is non-decreasing for .,"\sum\limits_{k=1}^{n-1} (n-k) x^k x \in ]-1,1[","Question I would like to show for arbitrary $n \in \mathbb{N}$, that the polynomial: $$ p(x) := \sum_{k=1}^{n-1} (n-k) x^k $$ is non-decreasing. Start Of Solution We can write its derivative by: $$ p'(x) = \frac{(-(x + 1) (x^n-1) + n (x - 1) (x^n + 1))}{(x - 1)^3}, $$ as $(-1 + x)^3 < 0$ for $x \in ]-1,1[$ it suffices to show for arbitrary $n$ that: $$ n(x-1) (x^n + 1) \leq (x+1)(x^n - 1), $$ for $n = 1$ this is trivial (as both sides become equal). Thus we assume the inequality to hold for $n$ and show it for $n+1$, here I use $x^{n+1} + 1 = x(x^n+1) + (1-x)$ to use the induction hypothesis but I don't achieve the inequality through this method. No positive roots By Descartes' Sign rule we see that $p'(x) = \sum\limits_{k=1}^{n-1}(n-k)k x^{k-1}$ has no positive roots, thus it suffices to show","Question I would like to show for arbitrary $n \in \mathbb{N}$, that the polynomial: $$ p(x) := \sum_{k=1}^{n-1} (n-k) x^k $$ is non-decreasing. Start Of Solution We can write its derivative by: $$ p'(x) = \frac{(-(x + 1) (x^n-1) + n (x - 1) (x^n + 1))}{(x - 1)^3}, $$ as $(-1 + x)^3 < 0$ for $x \in ]-1,1[$ it suffices to show for arbitrary $n$ that: $$ n(x-1) (x^n + 1) \leq (x+1)(x^n - 1), $$ for $n = 1$ this is trivial (as both sides become equal). Thus we assume the inequality to hold for $n$ and show it for $n+1$, here I use $x^{n+1} + 1 = x(x^n+1) + (1-x)$ to use the induction hypothesis but I don't achieve the inequality through this method. No positive roots By Descartes' Sign rule we see that $p'(x) = \sum\limits_{k=1}^{n-1}(n-k)k x^{k-1}$ has no positive roots, thus it suffices to show",,"['derivatives', 'inequality', 'polynomials', 'induction', 'roots']"
54,Taylor expansions of order $n+1$ of $C^n$ functions,Taylor expansions of order  of  functions,n+1 C^n,"Main Question. Suppose $f:I\to\mathbb{R}$ is of class $C^n$, and suppose $f$ has a Taylor expansion of order $n+1$ at $a$ : does it follow that $f^{(n)}$ have a derivative at $a$? If not, what are some further assumptions one can impose on $f$ to make such a statement true? Motivation. Let $I$ be an interval, and let $a\in I$ be one of its points. Recall that $f:I\to\mathbb{R}$ is said to have a Taylor expansion of order $n$ at $a$ if there are real numbers $c_0,\dots,c_n$ such that $$f(x)=\sum_{k=0}^nc_k(x-a)^k+o_a\Big((x-a)^n\Big)$$ The following is a basic result about Taylor expansions : Theorem 1a. Let $f:I\to\mathbb{R}$ be a function, then $f$ has a Taylor expansion of order $0$ at $a$ iff $f$ is continuous at $a$, $f$ has a Taylor expansion of order $1$ at $a$ iff $f$ has a derivative at $a$. Analoguous statements are false for higher order Taylor expansions : even if $f$ has a Taylor expansion of order $n\geq 1$ at $a$, $a$ may be the only point in $I$ where $f$ is continuous, let alone differentiable. Recall that Taylor expansions can be integrated : Theorem 2. Let $F:I\to\mathbb{R}$ be differentiable, and let $f=F'$ be its derivative. Suppose $f$ has a Taylor expansion of order $n$ at $a$. Then $F$ has a Taylor expansion of order $n+1$ at $a$ which is given, as one might expect, by formally integrating the Taylor expansion of $f$ (and adding $F(a)$). Let us define $$D^n(I)=\lbrace f:I\to\mathbb{R}\quad\text{ s.t. }\quad f',f'',\dots,f^{(n)}\text{ exist everywhere on }I\rbrace\,,$$ and two subsets $$D_aD^n(I)\subset C_aD^n(I)\subset D^n(I)$$ where $f\in D^n(I)$ belongs to $C_aD^n(I)$ iff $f^{(n)}$ is continuous at $a$, and to $D_aD^n(I)$ iff $f^{(n)}$ is differentiable at $a$. Note that $D^0(I)$ is the set of all functions $I\to\mathbb{R}$, $C_aD^0(I)$ the subset of those functions that are continuous at $a$, and $C_aD^0(I)$ the subset of those functions that are differentiable at $a$. From Theorem 1a and Theorem 2 it easily follows, by successive integrations, that Theorem 1b. Let $f\in D^n(I)$ be a function with derivatives up to order $n$ if $f\in C_aD^n(I)$, then $f$ has a Taylor expansion of order $n$ at $a$, ( EDIT : this, while true, follows from the next point, and continuity plays no role) if $f\in D_aD^n(I)$, then $f$ has a Taylor expansion of order $n+1$ at $a$. And the coefficients in the Taylor expansion are, up to some factorials, $f(a),\dots,f^{(n)}(a)$ and $f^{(n+1)}(a)$. For $n=0$, the converse holds : this is Theorem 1a , but for $n\geq 1$, he converse to both statments is false : there are differentiable functions such that $f(x)=o(x^2)$ at $0$, yet $f'$ isn't even continuous at $0$; something like $f(x)=x^3\sin(\frac1{x^{2}})$ will do. Some calculations Let us suppose $f$ is $C^n$ and has a Taylor expansion of order $n+1$, so that, for some $c_{n+1}\in\mathbb{R}$, \begin{array}{rcl} f(x) & = & \sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k     +     \overbrace{\int_{a}^x\frac{(x-t)^{n-1}}{(n-1)!}\left[f^{(n)}(t)-f^{(n)}(a)\right]dt}^{=o((x-a)^n)}\\     & = & \sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+\frac{c_{n+1}}{(n+1)!}(x-a)^{n+1}+o_a\Big((x-a)^{n+1}\Big) \end{array} We take the difference and obtain $$\frac1{x-a}\int_{a}^x\left(\frac{x-t}{x-a}\right)^{n-1}\left[\frac{f^{(n)}(t)-f^{(n)}(a)-c_{n+1}(t-a)}{x-a}\right]dt=o_a(1)$$ which we can rewrite as $$\frac1{x-a}\int_{a}^x\left(\frac{x-t}{x-a}\right)^{n-1}\left[\frac{f^{(n)}(t)-f^{(n)}(a)-c_{n+1}(t-a)}{t-a}\right]\frac{t-a}{x-a}dt=o_a(1)$$ which can be rewritten as $$\int_{0}^1(1-u)^{n-1}\left[\frac{f^{(n)}((x-a)u+a)-f^{(n)}(a)-c_{n+1}(x-a)u}{x-a}\right]du=o_a(1)$$ Without loss of generality we may assume $a=0$, and setting $g(t)=f^{(n)}(t)-f^{(n)}(0)$, $g$ is may be any continuous function under the sun vanishing at $0$, and the hypothesis becomes $$\int_{0}^1(1-u)^{n-1}\left[\frac{g(xu)}{x}-c_{n+1}u\right]du=o_a(1)$$ and the question becomes Suppose $c$ is a real number, and $g:\mathbb{R}\to\mathbb{R}$ is continuous, vanishes at $0$ and satisfies   $$\lim_{x\to 0}\int_{0}^1(1-u)^{n-1}\left[\frac{g(xu)}{x}-cu\right]du=0$$   Can anything be deduced about differentiability of $g$ at $0$? Such as ""$g$ is differentiable at $0$ and $g'(0)=c$? Presumably the $(1-u)^{n-1}$ factor has no incidence on the result, and replacing $g$ by $h(t)=g(t)-ct$ we can further reduce the problem to the following Suppose $h:\mathbb{R}\to\mathbb{R}$ is continuous, vanishes at $0$ and satisfies   $$\lim_{x\to 0}\int_{0}^1\frac{h(xu)}{x}du=0$$   Is $h$ necessarily differentiable at $0$ with $h'(0)=0$? The answer to this question is no : consider $h(x)=x(1-\delta(x))$ where $\delta(x)=\sum_{k=0}^\infty T_k(x)$, where $T_k$ is the piecwise affine tent function that is constant equal to zero outside of $[2^{-n}-\frac1{4^n},2^{-n}1\frac1{4^n}]$ and takes the value $1$ at $2^{-n}$. Likely conclusion and new question It seems likely that the answer to the question in this generality (that is : $f$ $C^n$ admitting a Taylor expansion of order $n+1$) is no , and the above should provide a counter example. However, it seems likely that the answer becomes yes if we assume $f^{(n)}$ to be $K$-lipschitz on some neighborhood of $0$ : Suppose $h:\mathbb{R}\to\mathbb{R}$ is $K$-lipschitz, vanishes at $0$ and satisfies   $$\lim_{x\to 0}\int_{0}^1\frac{h(xu)}{x}du=0$$   Is $h$ necessarily differentiable at $0$ with $h'(0)=0$?","Main Question. Suppose $f:I\to\mathbb{R}$ is of class $C^n$, and suppose $f$ has a Taylor expansion of order $n+1$ at $a$ : does it follow that $f^{(n)}$ have a derivative at $a$? If not, what are some further assumptions one can impose on $f$ to make such a statement true? Motivation. Let $I$ be an interval, and let $a\in I$ be one of its points. Recall that $f:I\to\mathbb{R}$ is said to have a Taylor expansion of order $n$ at $a$ if there are real numbers $c_0,\dots,c_n$ such that $$f(x)=\sum_{k=0}^nc_k(x-a)^k+o_a\Big((x-a)^n\Big)$$ The following is a basic result about Taylor expansions : Theorem 1a. Let $f:I\to\mathbb{R}$ be a function, then $f$ has a Taylor expansion of order $0$ at $a$ iff $f$ is continuous at $a$, $f$ has a Taylor expansion of order $1$ at $a$ iff $f$ has a derivative at $a$. Analoguous statements are false for higher order Taylor expansions : even if $f$ has a Taylor expansion of order $n\geq 1$ at $a$, $a$ may be the only point in $I$ where $f$ is continuous, let alone differentiable. Recall that Taylor expansions can be integrated : Theorem 2. Let $F:I\to\mathbb{R}$ be differentiable, and let $f=F'$ be its derivative. Suppose $f$ has a Taylor expansion of order $n$ at $a$. Then $F$ has a Taylor expansion of order $n+1$ at $a$ which is given, as one might expect, by formally integrating the Taylor expansion of $f$ (and adding $F(a)$). Let us define $$D^n(I)=\lbrace f:I\to\mathbb{R}\quad\text{ s.t. }\quad f',f'',\dots,f^{(n)}\text{ exist everywhere on }I\rbrace\,,$$ and two subsets $$D_aD^n(I)\subset C_aD^n(I)\subset D^n(I)$$ where $f\in D^n(I)$ belongs to $C_aD^n(I)$ iff $f^{(n)}$ is continuous at $a$, and to $D_aD^n(I)$ iff $f^{(n)}$ is differentiable at $a$. Note that $D^0(I)$ is the set of all functions $I\to\mathbb{R}$, $C_aD^0(I)$ the subset of those functions that are continuous at $a$, and $C_aD^0(I)$ the subset of those functions that are differentiable at $a$. From Theorem 1a and Theorem 2 it easily follows, by successive integrations, that Theorem 1b. Let $f\in D^n(I)$ be a function with derivatives up to order $n$ if $f\in C_aD^n(I)$, then $f$ has a Taylor expansion of order $n$ at $a$, ( EDIT : this, while true, follows from the next point, and continuity plays no role) if $f\in D_aD^n(I)$, then $f$ has a Taylor expansion of order $n+1$ at $a$. And the coefficients in the Taylor expansion are, up to some factorials, $f(a),\dots,f^{(n)}(a)$ and $f^{(n+1)}(a)$. For $n=0$, the converse holds : this is Theorem 1a , but for $n\geq 1$, he converse to both statments is false : there are differentiable functions such that $f(x)=o(x^2)$ at $0$, yet $f'$ isn't even continuous at $0$; something like $f(x)=x^3\sin(\frac1{x^{2}})$ will do. Some calculations Let us suppose $f$ is $C^n$ and has a Taylor expansion of order $n+1$, so that, for some $c_{n+1}\in\mathbb{R}$, \begin{array}{rcl} f(x) & = & \sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k     +     \overbrace{\int_{a}^x\frac{(x-t)^{n-1}}{(n-1)!}\left[f^{(n)}(t)-f^{(n)}(a)\right]dt}^{=o((x-a)^n)}\\     & = & \sum_{k=0}^n\frac{f^{(k)}(a)}{k!}(x-a)^k+\frac{c_{n+1}}{(n+1)!}(x-a)^{n+1}+o_a\Big((x-a)^{n+1}\Big) \end{array} We take the difference and obtain $$\frac1{x-a}\int_{a}^x\left(\frac{x-t}{x-a}\right)^{n-1}\left[\frac{f^{(n)}(t)-f^{(n)}(a)-c_{n+1}(t-a)}{x-a}\right]dt=o_a(1)$$ which we can rewrite as $$\frac1{x-a}\int_{a}^x\left(\frac{x-t}{x-a}\right)^{n-1}\left[\frac{f^{(n)}(t)-f^{(n)}(a)-c_{n+1}(t-a)}{t-a}\right]\frac{t-a}{x-a}dt=o_a(1)$$ which can be rewritten as $$\int_{0}^1(1-u)^{n-1}\left[\frac{f^{(n)}((x-a)u+a)-f^{(n)}(a)-c_{n+1}(x-a)u}{x-a}\right]du=o_a(1)$$ Without loss of generality we may assume $a=0$, and setting $g(t)=f^{(n)}(t)-f^{(n)}(0)$, $g$ is may be any continuous function under the sun vanishing at $0$, and the hypothesis becomes $$\int_{0}^1(1-u)^{n-1}\left[\frac{g(xu)}{x}-c_{n+1}u\right]du=o_a(1)$$ and the question becomes Suppose $c$ is a real number, and $g:\mathbb{R}\to\mathbb{R}$ is continuous, vanishes at $0$ and satisfies   $$\lim_{x\to 0}\int_{0}^1(1-u)^{n-1}\left[\frac{g(xu)}{x}-cu\right]du=0$$   Can anything be deduced about differentiability of $g$ at $0$? Such as ""$g$ is differentiable at $0$ and $g'(0)=c$? Presumably the $(1-u)^{n-1}$ factor has no incidence on the result, and replacing $g$ by $h(t)=g(t)-ct$ we can further reduce the problem to the following Suppose $h:\mathbb{R}\to\mathbb{R}$ is continuous, vanishes at $0$ and satisfies   $$\lim_{x\to 0}\int_{0}^1\frac{h(xu)}{x}du=0$$   Is $h$ necessarily differentiable at $0$ with $h'(0)=0$? The answer to this question is no : consider $h(x)=x(1-\delta(x))$ where $\delta(x)=\sum_{k=0}^\infty T_k(x)$, where $T_k$ is the piecwise affine tent function that is constant equal to zero outside of $[2^{-n}-\frac1{4^n},2^{-n}1\frac1{4^n}]$ and takes the value $1$ at $2^{-n}$. Likely conclusion and new question It seems likely that the answer to the question in this generality (that is : $f$ $C^n$ admitting a Taylor expansion of order $n+1$) is no , and the above should provide a counter example. However, it seems likely that the answer becomes yes if we assume $f^{(n)}$ to be $K$-lipschitz on some neighborhood of $0$ : Suppose $h:\mathbb{R}\to\mathbb{R}$ is $K$-lipschitz, vanishes at $0$ and satisfies   $$\lim_{x\to 0}\int_{0}^1\frac{h(xu)}{x}du=0$$   Is $h$ necessarily differentiable at $0$ with $h'(0)=0$?",,"['derivatives', 'taylor-expansion']"
55,Misconception in applying Leibniz' rule,Misconception in applying Leibniz' rule,,"One can show using Leibniz' rule that  $$\int_0^1 \frac{x^n-1}{\ln x} dx = \ln|n+1|.$$ To be specific, if we set $g(n) := \int_0^1 \frac{x^n-1}{\ln x} dx$, then  \begin{align} g'(n) &= \frac d{dn} \int_0^1 \frac{x^n-1}{\ln x} dx \\ &= \int_0^1 \frac{\partial}{\partial n} \frac{x^n-1}{\ln x} \, dx \\ &= \int_0^1 \frac{x^n \ln x}{\ln x} \, dx \\ &= \int_0^1x^n \, dx \\ &= \frac 1{n+1}, \end{align} Upon integrating in $n$, we conclude that $$ g(n) = \ln |n+1|+C. $$ But we notice that $g(0)=0$, which implies that $C=0$. Hence, $$ g(n)=\ln|n+1|. $$ Question. If $g_a(n) := \int_0^1 \frac{x^n-a}{\ln x} \, dx$ for $a \not= 1$, then it seems like using Leibniz' rule would also give us $g_a(n) = \ln|n+1|$. But it cannot be, because $g_a(n)$ is a divergent integral. So the same justification using Leibniz' rule must not work for $f_a(n)$ somehow. But I'm not sure how. The hypothesis for Leiniz' rule is that $\frac{x^n-a}{\ln x}$ and $\frac{\partial}{\partial x} \frac{x^n-a}{\ln x}$ both be continuous over $(0,1)$, which they are.","One can show using Leibniz' rule that  $$\int_0^1 \frac{x^n-1}{\ln x} dx = \ln|n+1|.$$ To be specific, if we set $g(n) := \int_0^1 \frac{x^n-1}{\ln x} dx$, then  \begin{align} g'(n) &= \frac d{dn} \int_0^1 \frac{x^n-1}{\ln x} dx \\ &= \int_0^1 \frac{\partial}{\partial n} \frac{x^n-1}{\ln x} \, dx \\ &= \int_0^1 \frac{x^n \ln x}{\ln x} \, dx \\ &= \int_0^1x^n \, dx \\ &= \frac 1{n+1}, \end{align} Upon integrating in $n$, we conclude that $$ g(n) = \ln |n+1|+C. $$ But we notice that $g(0)=0$, which implies that $C=0$. Hence, $$ g(n)=\ln|n+1|. $$ Question. If $g_a(n) := \int_0^1 \frac{x^n-a}{\ln x} \, dx$ for $a \not= 1$, then it seems like using Leibniz' rule would also give us $g_a(n) = \ln|n+1|$. But it cannot be, because $g_a(n)$ is a divergent integral. So the same justification using Leibniz' rule must not work for $f_a(n)$ somehow. But I'm not sure how. The hypothesis for Leiniz' rule is that $\frac{x^n-a}{\ln x}$ and $\frac{\partial}{\partial x} \frac{x^n-a}{\ln x}$ both be continuous over $(0,1)$, which they are.",,"['calculus', 'integration', 'derivatives', 'definite-integrals', 'leibniz-integral-rule']"
56,Show that $ f(t) = \begin{cases} e^{-1/t} & x > 0 \\ 0 & x \le 0 \end{cases}$ is in $C^{\infty}$.,Show that  is in ., f(t) = \begin{cases} e^{-1/t} & x > 0 \\ 0 & x \le 0 \end{cases} C^{\infty},"Show that \begin{equation}    f(t) =    \begin{cases}       e^{-1/t} & x > 0 \\       0        & x \le 0    \end{cases} \end{equation} is in $C^{\infty}$. To show that this is true, I think we have to show that $f^{(k)}$ is continuous for all $k \in \mathbb{N}$, where $f^{(k)}$ is the $k$-th derivative of $f$ ($f^{(0)} := f$). Since the only point where these could be discontinuous is at $x = 0$, I think I need to show that \begin{equation}    f^{(k)}(0) = 0 \end{equation} for all $k$. I've calculated the first few derivatives in order to see a pattern emerge and create a closed form, but I can't do it. Here are the derivatives. \begin{align} f^{(0)} & = e^{-1/t}\\ f^{(1)} & = {\frac {1}{{t}^{2}}{{e}^{-1/t}}}\\ f^{(2)} & = -2\,{\frac {1}{{t}^{3}}{{e}^{-1/t}}}+{\frac {1}{{t}^{4}}{{e}^{-1/t}}}\\ f^{(3)} & = 6\,{\frac {1}{{t}^{4}}{{e}^{-1/t}}}-6\,{\frac {1}{{t}^{5}}{{e}^{-1/t}}}+{\frac {1}{{t}^{6}}{{e}^{-1/t}}}\\ f^{(4)} & = -24\,{\frac {1}{{t}^{5}}{{e}^{-1/t}}}+36\,{\frac {1}{{t}^{6}}{{e}^{-1/t}}}-12\,{\frac {1}{{t}^{7}}{{e}^{-1/t}}}+{\frac {1}{{t}^{8}}{{e}^{-1/t}}}\\ \end{align} I can see that  \begin{equation}    f^{(k)}(t) = f^{(0)}(t) \sum_{i = k+1}^{2k} (-1)^{i}t^{-i} c_i \end{equation} where $c_i \stackrel{?}{=} k!$ or something like that. So in fact, I just need to find an expression for $c_i$. Any ideas?","Show that \begin{equation}    f(t) =    \begin{cases}       e^{-1/t} & x > 0 \\       0        & x \le 0    \end{cases} \end{equation} is in $C^{\infty}$. To show that this is true, I think we have to show that $f^{(k)}$ is continuous for all $k \in \mathbb{N}$, where $f^{(k)}$ is the $k$-th derivative of $f$ ($f^{(0)} := f$). Since the only point where these could be discontinuous is at $x = 0$, I think I need to show that \begin{equation}    f^{(k)}(0) = 0 \end{equation} for all $k$. I've calculated the first few derivatives in order to see a pattern emerge and create a closed form, but I can't do it. Here are the derivatives. \begin{align} f^{(0)} & = e^{-1/t}\\ f^{(1)} & = {\frac {1}{{t}^{2}}{{e}^{-1/t}}}\\ f^{(2)} & = -2\,{\frac {1}{{t}^{3}}{{e}^{-1/t}}}+{\frac {1}{{t}^{4}}{{e}^{-1/t}}}\\ f^{(3)} & = 6\,{\frac {1}{{t}^{4}}{{e}^{-1/t}}}-6\,{\frac {1}{{t}^{5}}{{e}^{-1/t}}}+{\frac {1}{{t}^{6}}{{e}^{-1/t}}}\\ f^{(4)} & = -24\,{\frac {1}{{t}^{5}}{{e}^{-1/t}}}+36\,{\frac {1}{{t}^{6}}{{e}^{-1/t}}}-12\,{\frac {1}{{t}^{7}}{{e}^{-1/t}}}+{\frac {1}{{t}^{8}}{{e}^{-1/t}}}\\ \end{align} I can see that  \begin{equation}    f^{(k)}(t) = f^{(0)}(t) \sum_{i = k+1}^{2k} (-1)^{i}t^{-i} c_i \end{equation} where $c_i \stackrel{?}{=} k!$ or something like that. So in fact, I just need to find an expression for $c_i$. Any ideas?",,"['sequences-and-series', 'derivatives', 'closed-form']"
57,More convenient form of derivative of $\mathrm{sinc}(x)$,More convenient form of derivative of,\mathrm{sinc}(x),"$\mathrm{sinc}(x)$ is defined as $\frac{\sin(x)}{x}$ except continuous at $x=0$ (insert the removable singularity). The derivative of $\mathrm{sinc}(x)$ is usually given as the derivative of $\frac{\sin(x)}{x}$, namely $$\frac{\cos(x)}{x} - \frac{\sin(x)}{x^2},$$ but this has the same problem as $\frac{\sin(x)}{x}$. You have to re-insert the removable singularity at $x=0$. Is there a more convenient form of the derivative of $\mathrm{sinc}(x)$, say perhaps using $\mathrm{sinc}(x)$ itself, that doesn't have this issue? (don't say piecewise; if piecewise was convenient we wouldn't have $\mathrm{sinc}$ in the first place.)","$\mathrm{sinc}(x)$ is defined as $\frac{\sin(x)}{x}$ except continuous at $x=0$ (insert the removable singularity). The derivative of $\mathrm{sinc}(x)$ is usually given as the derivative of $\frac{\sin(x)}{x}$, namely $$\frac{\cos(x)}{x} - \frac{\sin(x)}{x^2},$$ but this has the same problem as $\frac{\sin(x)}{x}$. You have to re-insert the removable singularity at $x=0$. Is there a more convenient form of the derivative of $\mathrm{sinc}(x)$, say perhaps using $\mathrm{sinc}(x)$ itself, that doesn't have this issue? (don't say piecewise; if piecewise was convenient we wouldn't have $\mathrm{sinc}$ in the first place.)",,['derivatives']
58,Simplify an integral formula,Simplify an integral formula,,"I stuck on a statement in the book Mixed Boundary Value Problems - Dean G. Duffy . In page 107, he gives $$ h(t) = \dfrac{2}{\pi} \dfrac{d}{dt}\left\lbrace \int_0^t \dfrac{\cos(x/2)}{\sqrt{\cos(x) - \cos(t)}} \left[\int_0^x f(\xi) d\xi \right]dx\right\rbrace $$ then he claims $$ h(t) = \frac{2}{\pi}\cot\left(\frac{t}{2}\right) \int_0^t \frac{\sin(x/2)f(x)}{\sqrt{\cos(x) - \cos(t)}}dx $$ Here we assume $f$ is continuous on $\mathbb{R}$. I don't understand how he simplified $h(t)$ from the first equation to the second equation. Can anyone help me out?","I stuck on a statement in the book Mixed Boundary Value Problems - Dean G. Duffy . In page 107, he gives $$ h(t) = \dfrac{2}{\pi} \dfrac{d}{dt}\left\lbrace \int_0^t \dfrac{\cos(x/2)}{\sqrt{\cos(x) - \cos(t)}} \left[\int_0^x f(\xi) d\xi \right]dx\right\rbrace $$ then he claims $$ h(t) = \frac{2}{\pi}\cot\left(\frac{t}{2}\right) \int_0^t \frac{\sin(x/2)f(x)}{\sqrt{\cos(x) - \cos(t)}}dx $$ Here we assume $f$ is continuous on $\mathbb{R}$. I don't understand how he simplified $h(t)$ from the first equation to the second equation. Can anyone help me out?",,['integration']
59,$41$-th derivative of $\sin(x^2-2x+2)$ at point $1$,-th derivative of  at point,41 \sin(x^2-2x+2) 1,Problem:  $$\frac{\mathrm d^{41}}{\mathrm dx^{41}}\sin(x^2-2x+2)$$ Can anyone help me with this because I tried to use General Leibniz rule(after I differentiate $\sin(x^2-2x+2)$) and I didn't get much better information and I also tried some proof by induction but also no success  Without using Taylor.,Problem:  $$\frac{\mathrm d^{41}}{\mathrm dx^{41}}\sin(x^2-2x+2)$$ Can anyone help me with this because I tried to use General Leibniz rule(after I differentiate $\sin(x^2-2x+2)$) and I didn't get much better information and I also tried some proof by induction but also no success  Without using Taylor.,,"['calculus', 'real-analysis', 'derivatives']"
60,Problem on integration: $\int\frac{\log_{\ e}[\sec x]}{\sqrt{1-x^2}}dx$,Problem on integration:,\int\frac{\log_{\ e}[\sec x]}{\sqrt{1-x^2}}dx,"Well, Today I am very confused over a integral problem and I tried  wolfram and many websites they did not help me. Compute: $$\int\frac{\log_{\ e}[\sec x]}{\sqrt{1-x^2}}dx$$  this I have to solve to find integral of arcsinx and tanx The Whole Equation: $$ \int \sin^{-1}x \tan x=\sin^{-1}x\log_{\ e}|\sec x|-\int\frac{\log_{\ e}[\sec x]}{\sqrt{1-x^2}}dx$$ Can Any one give solution fully yo Me and Thanks","Well, Today I am very confused over a integral problem and I tried  wolfram and many websites they did not help me. Compute: $$\int\frac{\log_{\ e}[\sec x]}{\sqrt{1-x^2}}dx$$  this I have to solve to find integral of arcsinx and tanx The Whole Equation: $$ \int \sin^{-1}x \tan x=\sin^{-1}x\log_{\ e}|\sec x|-\int\frac{\log_{\ e}[\sec x]}{\sqrt{1-x^2}}dx$$ Can Any one give solution fully yo Me and Thanks",,"['calculus', 'integration', 'derivatives']"
61,Find the derivative of the function $ y= x|\cos{\frac{\pi}{x}}|$,Find the derivative of the function, y= x|\cos{\frac{\pi}{x}}|,Function is defined as it follows : $x \neq 0$ and $f(0)=0$ My work is: $\frac{d}{dx}(x|\cos{\frac{\pi}{x}}|)$ = $|\cos{\frac{\pi}{x}}|$ + $x(\frac{d}{dx}|\cos{\frac{\pi}{x}}|)$ = $|cos{\frac{\pi}{x}}|$ + $\frac{\pi}{x}(\operatorname{sgn}(\cos{\frac{\pi}{x}}))(\sin{\frac{\pi}{x}})$ = $\operatorname{sgn}(\cos{\frac{\pi}{x}})[(\cos{\frac{\pi}{x}}) + \frac{\pi}{x}(\sin{\frac{\pi}{x}})]$ Should I consider any points on the domain and find the derivative of the function on those points? Thank you,Function is defined as it follows : $x \neq 0$ and $f(0)=0$ My work is: $\frac{d}{dx}(x|\cos{\frac{\pi}{x}}|)$ = $|\cos{\frac{\pi}{x}}|$ + $x(\frac{d}{dx}|\cos{\frac{\pi}{x}}|)$ = $|cos{\frac{\pi}{x}}|$ + $\frac{\pi}{x}(\operatorname{sgn}(\cos{\frac{\pi}{x}}))(\sin{\frac{\pi}{x}})$ = $\operatorname{sgn}(\cos{\frac{\pi}{x}})[(\cos{\frac{\pi}{x}}) + \frac{\pi}{x}(\sin{\frac{\pi}{x}})]$ Should I consider any points on the domain and find the derivative of the function on those points? Thank you,,['derivatives']
62,Derivative of product tensor (or matrix product ),Derivative of product tensor (or matrix product ),,"if I define a ""product tensor"" of two second-order tensors as the second-order tensor: $\boldsymbol{C}=\boldsymbol{AB}$ such that $C_{ij}=A_{ik}B_{kj}$ does the product rule applies to $\dfrac{\partial\boldsymbol{C}}{\partial\boldsymbol{X}}$ with $\boldsymbol{X}$ another second-order tensor ? If so, how and how do I prove it? The question can be more specific: I have a tensor valued function of tensor argument $\boldsymbol{n(x)}$. Both $\boldsymbol{n}$ and $\boldsymbol{x}$ are symmetric. I've found that $\dfrac{\partial\boldsymbol{n}^2}{\partial\boldsymbol{x}}=2\boldsymbol{n}\dfrac{\partial\boldsymbol{n}}{\partial\boldsymbol{x}}$ with $\boldsymbol{n}^2$ the product tensor $\boldsymbol{nn}$ and the last term is the left multiplication of $\boldsymbol{n}$ for the fourth-order tensor $\dfrac{\partial\boldsymbol{n}}{\partial\boldsymbol{x}}$. How can I prove it?","if I define a ""product tensor"" of two second-order tensors as the second-order tensor: $\boldsymbol{C}=\boldsymbol{AB}$ such that $C_{ij}=A_{ik}B_{kj}$ does the product rule applies to $\dfrac{\partial\boldsymbol{C}}{\partial\boldsymbol{X}}$ with $\boldsymbol{X}$ another second-order tensor ? If so, how and how do I prove it? The question can be more specific: I have a tensor valued function of tensor argument $\boldsymbol{n(x)}$. Both $\boldsymbol{n}$ and $\boldsymbol{x}$ are symmetric. I've found that $\dfrac{\partial\boldsymbol{n}^2}{\partial\boldsymbol{x}}=2\boldsymbol{n}\dfrac{\partial\boldsymbol{n}}{\partial\boldsymbol{x}}$ with $\boldsymbol{n}^2$ the product tensor $\boldsymbol{nn}$ and the last term is the left multiplication of $\boldsymbol{n}$ for the fourth-order tensor $\dfrac{\partial\boldsymbol{n}}{\partial\boldsymbol{x}}$. How can I prove it?",,"['derivatives', 'tensors']"
63,integral form of special function,integral form of special function,,"Do you have any idea  to present   integral form of this function? $f(x)=\frac{1}{x^2}+\frac{1}{x}(\psi(x)-2\ln x-2)+2(1+\ln x)\psi^{'}(x)+x\ln x\psi^{''}(x).$ Where $\psi^{(n)}(x)$ is polygamma function and $x>0.$ Note: The function $f$ is Completely Monotonic function in $(0,\infty).$ Then, by Bernstein's theorem we obtain $f(x) = \int_0^\infty e^{-tx} \,dg(t).$ https://en.wikipedia.org/wiki/Bernstein%27s_theorem_on_monotone_functions","Do you have any idea  to present   integral form of this function? $f(x)=\frac{1}{x^2}+\frac{1}{x}(\psi(x)-2\ln x-2)+2(1+\ln x)\psi^{'}(x)+x\ln x\psi^{''}(x).$ Where $\psi^{(n)}(x)$ is polygamma function and $x>0.$ Note: The function $f$ is Completely Monotonic function in $(0,\infty).$ Then, by Bernstein's theorem we obtain $f(x) = \int_0^\infty e^{-tx} \,dg(t).$ https://en.wikipedia.org/wiki/Bernstein%27s_theorem_on_monotone_functions",,"['derivatives', 'special-functions']"
64,Derivative of Legendre Polynomial,Derivative of Legendre Polynomial,,"I am given with a relation \begin{equation} \frac{d^2}{dx^2}(P_l(x))=\frac{1}{2}\sum_{n=(0,1),2}^{l-2}(l-n)(l+n+1)(2n+1)P_n(x). \end{equation} The above sum starts with 0 for even end point, and 1 for odd end point, and proceeds in a step of 2. Using the above equation, I get  \begin{equation} \frac{d^4}{dx^4}(P_l(x))=\frac{1}{4}\sum_{n=(0,1),2}^{l-2}\sum_{m=(0,1),2}^{n-2}(l-n)(l+n+1)(2n+1)(n-m)(n+m+1)(2m+1)P_m(x). \end{equation} The problem is that I want the double sum in the above as a single sum, just in terms of $n, l$. Is there any idea how to simplify the above?","I am given with a relation \begin{equation} \frac{d^2}{dx^2}(P_l(x))=\frac{1}{2}\sum_{n=(0,1),2}^{l-2}(l-n)(l+n+1)(2n+1)P_n(x). \end{equation} The above sum starts with 0 for even end point, and 1 for odd end point, and proceeds in a step of 2. Using the above equation, I get  \begin{equation} \frac{d^4}{dx^4}(P_l(x))=\frac{1}{4}\sum_{n=(0,1),2}^{l-2}\sum_{m=(0,1),2}^{n-2}(l-n)(l+n+1)(2n+1)(n-m)(n+m+1)(2m+1)P_m(x). \end{equation} The problem is that I want the double sum in the above as a single sum, just in terms of $n, l$. Is there any idea how to simplify the above?",,"['calculus', 'derivatives', 'special-functions', 'legendre-polynomials']"
65,What do the Stirling numbers of the first kind have to do with polylogarithms?,What do the Stirling numbers of the first kind have to do with polylogarithms?,,"On a whim, I had decided to look into ways of evaluating series of the form $$\sum_{n\ge1}\frac{1}{n^k2^n}$$ which I learned has a more general form in terms of polylogarithms : $$\text{Li}_k(x)=\sum_{n\ge1}\frac{x^n}{n^k}$$ Without prior knowledge of polylogarithms, I started working the sum where $k=1$. It's easy enough to evaluate by considering the derivative: $$F(x)=\sum_{n\ge1}\frac{x^n}{n}~~\implies~~F'(x)=\frac{1}{x}\sum_{n\ge1}x^n$$ and so on, which tells me the sum is $\ln2$. I noticed that I could keep taking the derivative in this fashion for larger values of $k$ and came up with the recursive derivative relation for the polylog (equation (18) in the MathWorld link). If I denote $f_k\equiv\text{Li}_k(x)$, then $$\frac{df_k}{dx}=\frac{f_{k-1}}{x}$$ I considered successive orders of the derivative: $$\frac{d^2f_k}{dx^2}=\frac{-f_{k-1}+f_{k-2}}{x^2}\\ \frac{d^3f_k}{dx^3}=\frac{2f_{k-1}-3f_{k-2}+f_{k-3}}{x^3}\\ \frac{d^4f_k}{dx^4}=\frac{-6f_{k-1}+11f_{k-2}-6f_{k-3}+f_{k-4}}{x^4}$$ I hadn't known about the Stirling numbers before this. As it turns out, the coefficients of the numerator's terms belong to this sequence . More generally, it would appear that $$\frac{d^mf_k}{dx^m}=\frac{1}{x^m}\sum_{i=1}^ms(m,i)f_{k-i}$$ Is this result at all surprising? Edit: For another thing, is it at all surprising that $$\sum_{i=1}^m s(m,i)=0~~?$$","On a whim, I had decided to look into ways of evaluating series of the form $$\sum_{n\ge1}\frac{1}{n^k2^n}$$ which I learned has a more general form in terms of polylogarithms : $$\text{Li}_k(x)=\sum_{n\ge1}\frac{x^n}{n^k}$$ Without prior knowledge of polylogarithms, I started working the sum where $k=1$. It's easy enough to evaluate by considering the derivative: $$F(x)=\sum_{n\ge1}\frac{x^n}{n}~~\implies~~F'(x)=\frac{1}{x}\sum_{n\ge1}x^n$$ and so on, which tells me the sum is $\ln2$. I noticed that I could keep taking the derivative in this fashion for larger values of $k$ and came up with the recursive derivative relation for the polylog (equation (18) in the MathWorld link). If I denote $f_k\equiv\text{Li}_k(x)$, then $$\frac{df_k}{dx}=\frac{f_{k-1}}{x}$$ I considered successive orders of the derivative: $$\frac{d^2f_k}{dx^2}=\frac{-f_{k-1}+f_{k-2}}{x^2}\\ \frac{d^3f_k}{dx^3}=\frac{2f_{k-1}-3f_{k-2}+f_{k-3}}{x^3}\\ \frac{d^4f_k}{dx^4}=\frac{-6f_{k-1}+11f_{k-2}-6f_{k-3}+f_{k-4}}{x^4}$$ I hadn't known about the Stirling numbers before this. As it turns out, the coefficients of the numerator's terms belong to this sequence . More generally, it would appear that $$\frac{d^mf_k}{dx^m}=\frac{1}{x^m}\sum_{i=1}^ms(m,i)f_{k-i}$$ Is this result at all surprising? Edit: For another thing, is it at all surprising that $$\sum_{i=1}^m s(m,i)=0~~?$$",,"['sequences-and-series', 'derivatives', 'special-functions', 'stirling-numbers']"
66,"Is this a ""locally surjective"" function?","Is this a ""locally surjective"" function?",,"I quote the ""locally surjective"" part because I haven't found any reference of that concept, but it kind of fits what I mean. Let $f:\mathbb{R}^N \to \mathbb{R}^M, f \in C^1, x_0 \in \mathbb{R}^N : rank(Df(x_0)) = M$. Show that $\exists U \subset \mathbb{R}^M$ neighbourhood of $f(x_0)$ such that $U \subset Im(f)$. I'm struggling to get an intuition of why should this happen, but haven't made any progress. Could any of you please provide me some hints about it? Thanks in advance.","I quote the ""locally surjective"" part because I haven't found any reference of that concept, but it kind of fits what I mean. Let $f:\mathbb{R}^N \to \mathbb{R}^M, f \in C^1, x_0 \in \mathbb{R}^N : rank(Df(x_0)) = M$. Show that $\exists U \subset \mathbb{R}^M$ neighbourhood of $f(x_0)$ such that $U \subset Im(f)$. I'm struggling to get an intuition of why should this happen, but haven't made any progress. Could any of you please provide me some hints about it? Thanks in advance.",,"['real-analysis', 'derivatives']"
67,Problem about $\lim \limits_{x \to c} f'(x) = l $ implies $f'(c) = l$,Problem about  implies,\lim \limits_{x \to c} f'(x) = l  f'(c) = l,"I found this problem in a paper. Let $f$ be a function differentiable on $(a, b)$ except possibly at $c \in (a, b)$. Suppose that $\lim \limits_{x \to c} f'(x) = l \in \Bbb  R$. Prove that $f$ is differentiable at $c$ and $f'(c) = l$. But I think I have a counterexample for why the claimed result need not hold. Define, $$f(x) = \begin{cases} x + 1  & \text{if $-1\lt x \lt $ 0 } \\ x & \text{if $ \; 0 \le x \lt 1$} \end{cases}$$ Then $f$ is differentiable at each point of $(-1, 1) \setminus \{0\}$ with $f'(t) = 1$ for each non-zero point $t$ in the interval $(-1,1)$. And this also tells us that $\lim \limits_{x \to 0} f'(x) = 1$. But $f$ is not differentiable at $0$ since $f$ is not continuous there. Where am I going wrong? P. S. : I found a similar problem in Apostol's Mathematical Analysis which has the additional stipulation that $f$ is continuous on $(a, b)$ which can be used to apply the Mean Value Theorem to prove what is asked.","I found this problem in a paper. Let $f$ be a function differentiable on $(a, b)$ except possibly at $c \in (a, b)$. Suppose that $\lim \limits_{x \to c} f'(x) = l \in \Bbb  R$. Prove that $f$ is differentiable at $c$ and $f'(c) = l$. But I think I have a counterexample for why the claimed result need not hold. Define, $$f(x) = \begin{cases} x + 1  & \text{if $-1\lt x \lt $ 0 } \\ x & \text{if $ \; 0 \le x \lt 1$} \end{cases}$$ Then $f$ is differentiable at each point of $(-1, 1) \setminus \{0\}$ with $f'(t) = 1$ for each non-zero point $t$ in the interval $(-1,1)$. And this also tells us that $\lim \limits_{x \to 0} f'(x) = 1$. But $f$ is not differentiable at $0$ since $f$ is not continuous there. Where am I going wrong? P. S. : I found a similar problem in Apostol's Mathematical Analysis which has the additional stipulation that $f$ is continuous on $(a, b)$ which can be used to apply the Mean Value Theorem to prove what is asked.",,"['real-analysis', 'derivatives']"
68,Differentiating an endomorphism,Differentiating an endomorphism,,"Let $(M,\rho)$ be a symplectic $2$-dimensional manifold, and let $J$ be a compatible complex structure on $M$, i.e. the symmetric $(0,2)$-tensor $$g(*,*) = \rho(*,J*)$$ is a Riemannian metric. Denote by $\nabla$ the Levi-Civita connection. Now let $V\in\Gamma(TM)$ be a vector field (with some further assumptions which are probably not needed). I would like to compute $$\left.\frac{d}{dt}\right|_{t=0}\left((d\varphi_V^{-t})_{\varphi_V^t(m)}J(\varphi_V^t(m))(d\varphi_V^t)_m\right),$$ where the map in the brackets is a map $T_mM\to T_mM$. Here $\varphi_V^t$ denotes the flow of $V$. If my computations are correct, I get the expression $$-\nabla_{J*}V + (\nabla_VJ)(*) + J(\nabla_*V)\tag{1},$$ which is again an endomorphism of $TM$. I would like to further simplify this expression. This would be easy if we had $$\nabla_{J*}V = J(\nabla_*V),$$ but I'm afraid that this is not true (at least without some more assumptions/structure). The only results I have about relations between $\nabla$ and $J$ are $$J(\nabla_*J) = (\nabla_*J)J\quad\mathrm{and}\quad\nabla_{J*}J = -J(\nabla_*J)$$ (see e.g. lemma 4.1.14 in this book ), but I haven't been able to get anywhere using them. My questions are: Is my computation $(1)$ correct? Is it possible to further simplify the expression $(1)$? If you think that further assumptions are needed to solve the problem, please let me know.","Let $(M,\rho)$ be a symplectic $2$-dimensional manifold, and let $J$ be a compatible complex structure on $M$, i.e. the symmetric $(0,2)$-tensor $$g(*,*) = \rho(*,J*)$$ is a Riemannian metric. Denote by $\nabla$ the Levi-Civita connection. Now let $V\in\Gamma(TM)$ be a vector field (with some further assumptions which are probably not needed). I would like to compute $$\left.\frac{d}{dt}\right|_{t=0}\left((d\varphi_V^{-t})_{\varphi_V^t(m)}J(\varphi_V^t(m))(d\varphi_V^t)_m\right),$$ where the map in the brackets is a map $T_mM\to T_mM$. Here $\varphi_V^t$ denotes the flow of $V$. If my computations are correct, I get the expression $$-\nabla_{J*}V + (\nabla_VJ)(*) + J(\nabla_*V)\tag{1},$$ which is again an endomorphism of $TM$. I would like to further simplify this expression. This would be easy if we had $$\nabla_{J*}V = J(\nabla_*V),$$ but I'm afraid that this is not true (at least without some more assumptions/structure). The only results I have about relations between $\nabla$ and $J$ are $$J(\nabla_*J) = (\nabla_*J)J\quad\mathrm{and}\quad\nabla_{J*}J = -J(\nabla_*J)$$ (see e.g. lemma 4.1.14 in this book ), but I haven't been able to get anywhere using them. My questions are: Is my computation $(1)$ correct? Is it possible to further simplify the expression $(1)$? If you think that further assumptions are needed to solve the problem, please let me know.",,"['derivatives', 'manifolds', 'complex-geometry', 'symplectic-geometry', 'kahler-manifolds']"
69,Derivative of a linear basis function over a moving mesh,Derivative of a linear basis function over a moving mesh,,"Given a moving mesh $0=x_0(t)<x_1(t)<\cdots<x_N(t)<x_{N+1}(t)=1,$ where $t$ denotes the current time so that the mesh is moving with time. The linear basis function is then defined as $\phi_j(x,t)=\dfrac{x-x_{j-1}(t)}{x_j(t)-x_{j-1}(t)},$ for $x\in[x_{j-1}(t),x_j(t)]$ and $\phi_j(x,t)=\dfrac{x_{j+1}(t)-x}{x_{j+1}(t)-x_j(t)},$ for $x\in[x_j(t),x_{j+1}(t)]$ and $0$ otherwise, for all $j=1,\cdots,N.$ Then it comes with the conclusion that a direct calculation shows that  $$ \dfrac{\partial\phi_j}{\partial t}(x,t)=-\dfrac{\partial\phi_j}{\partial x}(x,t)X_t(x,t), $$ where $X_t(x,t)$ is the linear interpolant of the nodal mesh speeds, i.e, $$X_t(x,t)=\sum\limits_{j=1}^N\dfrac{dx_j}{dt}(t)\phi_j(x,t).$$ However, I am not able to deduce this equation. Is anybody able to do that?","Given a moving mesh $0=x_0(t)<x_1(t)<\cdots<x_N(t)<x_{N+1}(t)=1,$ where $t$ denotes the current time so that the mesh is moving with time. The linear basis function is then defined as $\phi_j(x,t)=\dfrac{x-x_{j-1}(t)}{x_j(t)-x_{j-1}(t)},$ for $x\in[x_{j-1}(t),x_j(t)]$ and $\phi_j(x,t)=\dfrac{x_{j+1}(t)-x}{x_{j+1}(t)-x_j(t)},$ for $x\in[x_j(t),x_{j+1}(t)]$ and $0$ otherwise, for all $j=1,\cdots,N.$ Then it comes with the conclusion that a direct calculation shows that  $$ \dfrac{\partial\phi_j}{\partial t}(x,t)=-\dfrac{\partial\phi_j}{\partial x}(x,t)X_t(x,t), $$ where $X_t(x,t)$ is the linear interpolant of the nodal mesh speeds, i.e, $$X_t(x,t)=\sum\limits_{j=1}^N\dfrac{dx_j}{dt}(t)\phi_j(x,t).$$ However, I am not able to deduce this equation. Is anybody able to do that?",,"['derivatives', 'finite-element-method']"
70,Find the slope at $t=16$ for $s(t) = $arctan$(\sqrt{t})$,Find the slope at  for arctan,t=16 s(t) =  (\sqrt{t}),"A particle moves along the x axis so that its position at any time when t is greater than or equals zero is $s(t) = $arctan$(\sqrt{t})$. Find the velocity of the particle at $t=16$. The point of this question isn't to find the velocity, but to solve for the derivative and then find the slope of the line at a certain point. The velocity is the slope of the line of a position time graph, so I took the derivative: $$s'(t) = {\frac{1}{\sqrt{t}(2t+2)}}$$ I then made $t=16$, to get: $$s'(16) = 7.4 \cdot 10^{-3} \text{m/s}$$ Did I do this correctly? Any hints or advice would be appreciated.","A particle moves along the x axis so that its position at any time when t is greater than or equals zero is $s(t) = $arctan$(\sqrt{t})$. Find the velocity of the particle at $t=16$. The point of this question isn't to find the velocity, but to solve for the derivative and then find the slope of the line at a certain point. The velocity is the slope of the line of a position time graph, so I took the derivative: $$s'(t) = {\frac{1}{\sqrt{t}(2t+2)}}$$ I then made $t=16$, to get: $$s'(16) = 7.4 \cdot 10^{-3} \text{m/s}$$ Did I do this correctly? Any hints or advice would be appreciated.",,"['calculus', 'derivatives', 'solution-verification']"
71,Then is $f_a$ continious?,Then is  continious?,f_a,"Excuse me for the bad title, here's the question Given a differentiable function defined on R. For a given number $a$, $\forall x\in \mathbb R, x\neq a$, by mean value theorem,  there exists a  $\xi$ between $a$ and $x$ such that $f'(\xi)=\frac {f(x)-f(a)}{x-a}$. Suppose it's unique for all $x\neq a$. Therefore $\forall x\neq a$ we define $f_a$ as $f_a(x)=\xi$. Then is $f_a$ continious?","Excuse me for the bad title, here's the question Given a differentiable function defined on R. For a given number $a$, $\forall x\in \mathbb R, x\neq a$, by mean value theorem,  there exists a  $\xi$ between $a$ and $x$ such that $f'(\xi)=\frac {f(x)-f(a)}{x-a}$. Suppose it's unique for all $x\neq a$. Therefore $\forall x\neq a$ we define $f_a$ as $f_a(x)=\xi$. Then is $f_a$ continious?",,"['calculus', 'derivatives']"
72,Will antiderivative always be differentiable?,Will antiderivative always be differentiable?,,"Suppose f(x) is continuous on [0,1]. Obviously, such a function will be integrable. Will antiderivative be always differentiable on (0,1)? The answer is ""Yes"" by the Fundamental Theorem of Calculus. But what if f(x) is not continuous, can it still be integrable but have non-differentiable antiderivative? What confuses me is that I always thought about integrand as the derivative of an antiderivative. So if we find a non-differentiable antiderivative, how can we relate it back to the integrand?","Suppose f(x) is continuous on [0,1]. Obviously, such a function will be integrable. Will antiderivative be always differentiable on (0,1)? The answer is ""Yes"" by the Fundamental Theorem of Calculus. But what if f(x) is not continuous, can it still be integrable but have non-differentiable antiderivative? What confuses me is that I always thought about integrand as the derivative of an antiderivative. So if we find a non-differentiable antiderivative, how can we relate it back to the integrand?",,"['real-analysis', 'integration', 'derivatives']"
73,Roots of derivative of q-expontial function,Roots of derivative of q-expontial function,,"Let the q-deformation of the exponential function be defined by $$ e_q(z)=\sum_{n=0}^\infty{\frac{z^n}{[n]_q!}}. $$ Eq. (1.8) of this paper provides the product representation $$ e_q(z)=\prod_{k=0}^\infty{\left[1+z q^{-k}\left(1-\frac1q\right)\right]}.\tag{*} $$ This formula shows that the roots of $z\mapsto e_q(-z)$ are given by $$ z_k=\frac{q^{k+1}}{q-1}. $$ Question : Are there similarly simple formulas for the roots of the derivative(s)   of the q-exponential function? Many thanks. Edit 1 : Computing the logarithmic derivative of the product (*) we find that the zeros of the derivative of $e_q(-z)$ are the same as the zeros of $$ \sum_{n=1}^\infty{\frac{1}{z-\frac{q^n}{q-1}}},\tag{**} $$ which can be evaluated in terms of the q-digamma function. Taking $q=2$, which is indeed the case I am most interested in, this becomes $$ \sum_{n=1}^\infty{\frac{1}{z-2^n}}. $$ The numerical value of the first root $z_0\approx2.83643\; 10564\; 48581\; 92032\; 21153\; 61015$ (thank you @AntonioVargas) is not recognized by any of the inverse symbolic calculators. Edit 2 : Inspired by this paper I was able to compute that, for $|z|<q/(q-1)$, $({}^{**})$ can be written as $$ \frac{1}{z(q-1)}\log_q\left(1-z(q-1)\right), $$ where $$ \log_q(1-z)=-\sum_{n=1}^\infty{\frac{z^n}{[n]_q}} $$ is a series representation of the the q-logarithm. Unfortunately, the smallest zero of $({}^{**})$ is larger than $q/(q-1)$, so this is not very helpful. Question 2 : Does there exist a similar reformulation of $({^{**}})$ that is valid for $q^k/(q-1)<z<q^{k+1}/(q-1)$ for   $k\geqslant 2$? The answer to this question is 'yes'. More precisely, for each integer $K\geq 1$ and $q>1$ we have $$ \sum_{n=1}^\infty{\frac{1}{z-\frac{q^n}{q-1}}}=\sum_{k=1}^K{\frac{1}{z-\frac{q^k}{q-1}}}+\frac{1}{z(q-1)}\log_q\left(1-z\frac{q-1}{q^K}\right),\quad |z|<\frac{q^{K+1}}{q-1}. $$ In particular, setting $K=1$ and $q=2$, the zero $z_0\in[2,4]$ that I am after satisfies $$ \frac{1}{1-\frac{2}{z_0}}+\log_2\left(1-\frac {z_0}{2}\right)=0. $$","Let the q-deformation of the exponential function be defined by $$ e_q(z)=\sum_{n=0}^\infty{\frac{z^n}{[n]_q!}}. $$ Eq. (1.8) of this paper provides the product representation $$ e_q(z)=\prod_{k=0}^\infty{\left[1+z q^{-k}\left(1-\frac1q\right)\right]}.\tag{*} $$ This formula shows that the roots of $z\mapsto e_q(-z)$ are given by $$ z_k=\frac{q^{k+1}}{q-1}. $$ Question : Are there similarly simple formulas for the roots of the derivative(s)   of the q-exponential function? Many thanks. Edit 1 : Computing the logarithmic derivative of the product (*) we find that the zeros of the derivative of $e_q(-z)$ are the same as the zeros of $$ \sum_{n=1}^\infty{\frac{1}{z-\frac{q^n}{q-1}}},\tag{**} $$ which can be evaluated in terms of the q-digamma function. Taking $q=2$, which is indeed the case I am most interested in, this becomes $$ \sum_{n=1}^\infty{\frac{1}{z-2^n}}. $$ The numerical value of the first root $z_0\approx2.83643\; 10564\; 48581\; 92032\; 21153\; 61015$ (thank you @AntonioVargas) is not recognized by any of the inverse symbolic calculators. Edit 2 : Inspired by this paper I was able to compute that, for $|z|<q/(q-1)$, $({}^{**})$ can be written as $$ \frac{1}{z(q-1)}\log_q\left(1-z(q-1)\right), $$ where $$ \log_q(1-z)=-\sum_{n=1}^\infty{\frac{z^n}{[n]_q}} $$ is a series representation of the the q-logarithm. Unfortunately, the smallest zero of $({}^{**})$ is larger than $q/(q-1)$, so this is not very helpful. Question 2 : Does there exist a similar reformulation of $({^{**}})$ that is valid for $q^k/(q-1)<z<q^{k+1}/(q-1)$ for   $k\geqslant 2$? The answer to this question is 'yes'. More precisely, for each integer $K\geq 1$ and $q>1$ we have $$ \sum_{n=1}^\infty{\frac{1}{z-\frac{q^n}{q-1}}}=\sum_{k=1}^K{\frac{1}{z-\frac{q^k}{q-1}}}+\frac{1}{z(q-1)}\log_q\left(1-z\frac{q-1}{q^K}\right),\quad |z|<\frac{q^{K+1}}{q-1}. $$ In particular, setting $K=1$ and $q=2$, the zero $z_0\in[2,4]$ that I am after satisfies $$ \frac{1}{1-\frac{2}{z_0}}+\log_2\left(1-\frac {z_0}{2}\right)=0. $$",,"['sequences-and-series', 'derivatives', 'exponential-function', 'roots', 'q-analogs']"
74,Numerical computation of the $n^{\mathrm {th}}$ derivative of a multivariate function,Numerical computation of the  derivative of a multivariate function,n^{\mathrm {th}},"From a multivariate function $f$, depending on $n\geq 1$ variables, which can be computed numerically, but which does not admit simple analytic expression, I would like to approximate numerically the quantity: $$ \frac{\partial^n f}{\partial x_1...\partial x_n}(x_1,...,x_n)$$ using, e.g., finite differences. Intuitively, using finite differences, I would proceed like this: Let ($e_1,...,e_n$) be the canonical base of $\mathbb{R}^n$, and let $h\in\mathbb{R}_+^*$ be a small number. If $n = 1$ (univariate function); I would compute: $$ \frac{\partial f}{\partial x_1}(x) \approx \frac{f(x + h e_1) - f(x - h e_1)}{2h}$$ Now, if $n = 2$ (multivariate function with 2 variables), I would compute: $$ \frac{\partial^2 f}{\partial x_1\partial x_2}(x) \approx \frac{(f(x + h e_1 + h e_2) - f(x + h e_1 - h e_2)) - (f(x - h e_1 + h e_2) - f(x - h e_1 - h e_2))}{(2h)^2}$$ and so on for larger $n$. My problem is that this approximation involves $2^n$ terms, which is cumbersome for large $n$. Is anyone aware of a procedure / reference to obtain a good approximation without computing as much as $2^n$ evaluations of $f$ , or is this hopeless ?","From a multivariate function $f$, depending on $n\geq 1$ variables, which can be computed numerically, but which does not admit simple analytic expression, I would like to approximate numerically the quantity: $$ \frac{\partial^n f}{\partial x_1...\partial x_n}(x_1,...,x_n)$$ using, e.g., finite differences. Intuitively, using finite differences, I would proceed like this: Let ($e_1,...,e_n$) be the canonical base of $\mathbb{R}^n$, and let $h\in\mathbb{R}_+^*$ be a small number. If $n = 1$ (univariate function); I would compute: $$ \frac{\partial f}{\partial x_1}(x) \approx \frac{f(x + h e_1) - f(x - h e_1)}{2h}$$ Now, if $n = 2$ (multivariate function with 2 variables), I would compute: $$ \frac{\partial^2 f}{\partial x_1\partial x_2}(x) \approx \frac{(f(x + h e_1 + h e_2) - f(x + h e_1 - h e_2)) - (f(x - h e_1 + h e_2) - f(x - h e_1 - h e_2))}{(2h)^2}$$ and so on for larger $n$. My problem is that this approximation involves $2^n$ terms, which is cumbersome for large $n$. Is anyone aware of a procedure / reference to obtain a good approximation without computing as much as $2^n$ evaluations of $f$ , or is this hopeless ?",,"['derivatives', 'numerical-methods', 'finite-differences']"
75,Derivative of trig function,Derivative of trig function,,Find the second derivative of $ \arcsin(2x^3) $ The solution says for the first derivative : $ \dfrac{1}{\sqrt{1-(2x^3)^2}} \cdot 6x^2 = \dfrac{6x^2}{\sqrt{1-4x^6}}  $ When i answered the first derivative i got to : $ \dfrac{\cos(2x^3) \cdot 6x^2}{\sin^2(2x^3)} $ So what am i missing ?,Find the second derivative of $ \arcsin(2x^3) $ The solution says for the first derivative : $ \dfrac{1}{\sqrt{1-(2x^3)^2}} \cdot 6x^2 = \dfrac{6x^2}{\sqrt{1-4x^6}}  $ When i answered the first derivative i got to : $ \dfrac{\cos(2x^3) \cdot 6x^2}{\sin^2(2x^3)} $ So what am i missing ?,,"['calculus', 'trigonometry', 'derivatives']"
76,Is my calculation of $\frac{\partial}{\partial x}\frac{x+y}{\sqrt{y^2-x^2}}$ correct?,Is my calculation of  correct?,\frac{\partial}{\partial x}\frac{x+y}{\sqrt{y^2-x^2}},"Is my calculation of the partial derivative (with respect to $x$) of the function $$f(x,y)=\frac{x+y}{\sqrt{y^2-x^2}}$$ correct? $$f'_x=\frac{\sqrt{y^2-x^2}-(x+y)(\frac{-x}{\sqrt{y^2-x^2}})}{|y^2-x^2|}\\=\frac{\sqrt{y^2-x^2}+\frac{x^2+xy}{\sqrt{y^2-x^2}}}{|y^2-x^2|}\\=\frac{\frac{|y^2-x^2|+x^2+xy}{\sqrt{y^2-x^2}}}{|y^2-x^2|}\\=|y^2-x^2|\frac{|y^2-x^2|+x^2+xy}{\sqrt{y^2-x^2}}$$","Is my calculation of the partial derivative (with respect to $x$) of the function $$f(x,y)=\frac{x+y}{\sqrt{y^2-x^2}}$$ correct? $$f'_x=\frac{\sqrt{y^2-x^2}-(x+y)(\frac{-x}{\sqrt{y^2-x^2}})}{|y^2-x^2|}\\=\frac{\sqrt{y^2-x^2}+\frac{x^2+xy}{\sqrt{y^2-x^2}}}{|y^2-x^2|}\\=\frac{\frac{|y^2-x^2|+x^2+xy}{\sqrt{y^2-x^2}}}{|y^2-x^2|}\\=|y^2-x^2|\frac{|y^2-x^2|+x^2+xy}{\sqrt{y^2-x^2}}$$",,"['calculus', 'derivatives', 'partial-derivative']"
77,Derivative of this formula?,Derivative of this formula?,,"I'm studying Solid State Electronics and at one point my book says: $$\dfrac{\text{d}x_n}{\text{d}V_a}= \dfrac{1}{N_d} \left(\dfrac{\varepsilon_s}{2q(\frac{1}{N_a}+\frac{1}{N_d})(\phi_i -V_a)}\right)^{1/2}$$ where  $$ \left\{\begin{align} x_d &= x_n \left( 1+ \frac{N_d}{N_a} \right) \\ x_d &=\sqrt{ \frac{2\varepsilon}{q}\left(\frac{1}{N_a}+\frac{1}{N_d}\right)(\phi_i -V_a)} \end{align}\right. $$ So I tried calculating that derivative by ""hand"". I actually get the result of the book only, there's a minus sign that doesn't tally with what the books says. Is it me or is the book mistaken?","I'm studying Solid State Electronics and at one point my book says: $$\dfrac{\text{d}x_n}{\text{d}V_a}= \dfrac{1}{N_d} \left(\dfrac{\varepsilon_s}{2q(\frac{1}{N_a}+\frac{1}{N_d})(\phi_i -V_a)}\right)^{1/2}$$ where  $$ \left\{\begin{align} x_d &= x_n \left( 1+ \frac{N_d}{N_a} \right) \\ x_d &=\sqrt{ \frac{2\varepsilon}{q}\left(\frac{1}{N_a}+\frac{1}{N_d}\right)(\phi_i -V_a)} \end{align}\right. $$ So I tried calculating that derivative by ""hand"". I actually get the result of the book only, there's a minus sign that doesn't tally with what the books says. Is it me or is the book mistaken?",,['derivatives']
78,"If $D:A\to A$ is a derivation, what can be said about the range of $D$?","If  is a derivation, what can be said about the range of ?",D:A\to A D,"What can be said about the relation between the domain and range of a derivation as a function? If $A$ is the domain, any space of functions, what does $D(A)$ look like, where $D$ is a derivation? Is $D(A)=A$? I guess the function space should always be closed under multiplication and addition and then the Leibniz rule $$D(ab)=D(a)b+aD(b)$$ should be unproblematic. It comes down to the definition of $D$ acting on the functions. In case that the domain is a priori ""not as big"" as the range, can one complete a function space by adding the closure w.r.t. a derivative? And are there any categorical results? After all the cohomology business deals with mappings given by differential operators.","What can be said about the relation between the domain and range of a derivation as a function? If $A$ is the domain, any space of functions, what does $D(A)$ look like, where $D$ is a derivation? Is $D(A)=A$? I guess the function space should always be closed under multiplication and addition and then the Leibniz rule $$D(ab)=D(a)b+aD(b)$$ should be unproblematic. It comes down to the definition of $D$ acting on the functions. In case that the domain is a priori ""not as big"" as the range, can one complete a function space by adding the closure w.r.t. a derivative? And are there any categorical results? After all the cohomology business deals with mappings given by differential operators.",,"['derivatives', 'differential-algebra']"
79,"Is $H^{\alpha}_{ij,k}=H^{\alpha}_{ik,j}$ for submanifolds in $R^n$?",Is  for submanifolds in ?,"H^{\alpha}_{ij,k}=H^{\alpha}_{ik,j} R^n","Consider $\Sigma^n\subset {\bf R}^{n+p}$  a submanifold and  $\{e_i, e_\alpha\}$ an orthonormal frame where the Greek indexes stand for the normal vectors. Then define $H^{\alpha}_{ij,k}=e_k\langle e_{\alpha},\nabla _{e_i} e_j\rangle $ Can anyone provide a proof that $H^{\alpha}_{ij,k}=H^{\alpha}_{ik,j}$ for submanifolds in ${\bf R}^n$? PS: The $e_k$ vector is deriving the function.","Consider $\Sigma^n\subset {\bf R}^{n+p}$  a submanifold and  $\{e_i, e_\alpha\}$ an orthonormal frame where the Greek indexes stand for the normal vectors. Then define $H^{\alpha}_{ij,k}=e_k\langle e_{\alpha},\nabla _{e_i} e_j\rangle $ Can anyone provide a proof that $H^{\alpha}_{ij,k}=H^{\alpha}_{ik,j}$ for submanifolds in ${\bf R}^n$? PS: The $e_k$ vector is deriving the function.",,"['differential-geometry', 'derivatives', 'tensors', 'curvature']"
80,Sign of derivative of a complicated function,Sign of derivative of a complicated function,,"EDIT (for bounty): Consider the differential equation $G(p;x,\lambda)p \left[1-\lambda-x(1+\lambda)\right] + x(1+\lambda)p + (1-x)(1-\lambda) \int_{p}^{1} z G'(z;x,\lambda) dz - (1-\lambda) = 0$, where $G(p;x,\lambda)$ is a c.d.f. with parameters $x \in (0,1)$ and $\lambda \in (0,1)$. The boundary condition is $G(1,x,\lambda) = 1$. This equation has the solution $G(p;x,\lambda) = \frac{x(1+\lambda) - (1-\lambda)p^{-\frac{x(1+\lambda)-(1-\lambda)}{2x \lambda}}}{x(1+\lambda)-(1-\lambda)}.$ Solving $G(p;x,\lambda) = 0$, one can find its lower bound $\underline{p} = \left(\frac{1-\lambda}{x(1+\lambda)}\right)^{\frac{2\lambda x}{(1+\lambda)-(1-\lambda)}}$. Define $C_1(x) := \int_{\underline{p}}^{1} (1+\lambda) G(p;x,\lambda) - G(p;x,\lambda)^2 dp$ and $C_2(x) := \lambda + (1-\lambda) \int_{\underline{p}}^{1} G(p;x,\lambda)^2 dp$. One can show that both $C_1(x)$ and $C_2(x)$ are increasing in $x$. Also, $G(p;x,\lambda)$ is increasing in $x$ (over the relevant range). Moreover, I know that $x C_1(x) + (1-x)C_2(x) = \lambda$. OBJECTIVE: Show that $C_2(x)-C_1(x)$ is decreasing in $x$ over the relevant parameter range. Bonus objective: Show that it is convex (both of these features can be shown numerically). Possibly helpful hint: $C_2(x)-C_1(x)$ can also be expressed as $\frac{1-\lambda}{x} \int_{\underline{p}}^{1} G(p;x,\lambda)^2 dp$. Many thanks in advance for any suggestions that might lead to a proof! EDIT: Actually, the problem doesn't look too hard, but I still cannot solve it. Any clever ideas what to do here? Let $x \in (0,1), \lambda \in (0,1)$. Let $p(x,\lambda) = \left(\frac{(1+\lambda) x}{1-\lambda}\right)^{-\frac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}}$. Show that $\frac{(1-\lambda)(\lambda + x) - x(1+\lambda)^2 p(x,\lambda)}{(1-x)(1-x-\lambda)}$ is strictly decreasing in $x$ [for $x \in (0,1), \lambda \in (0,1)$]. Original post For a couple of days I've been struggling with confirming that $\frac{\partial S(x)}{\partial x}$, with $$S(x) =  v \dfrac{(1+\lambda)^2 x \left(\dfrac{1-\lambda}{(1+\lambda) x}\right)^{\tfrac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}}-(1-\lambda) (\lambda+x)}{(1-x) [x-(1-\lambda)]}\; \;,$$ is strictly negative for $\lambda \in (0,1)$, $x \in (0,1)$ [$v>0$ is irrelevant]. I can show this numerically, but a general proof would be desirable. For what it's worth, the derivative of this function can be shown to have the same sign as $(1-\lambda)\left[1+(1-\lambda) \lambda-2 \lambda x-x^{2^{\vphantom{a}}}\right]+  \frac{\Large\left(\frac{(1+\lambda) x}{1-\lambda}\right)^{-\frac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}} \left(\left(1-\lambda^2\right)^2-\left(1-\lambda^2\right)^2 x+(1+\lambda)^2 (-1+\lambda (-3+2 \lambda)) x^2+(1+\lambda)^2 (1+3 \lambda) x^3\right)}{\Large x(1+\lambda)-(1-\lambda)}$ $+\dfrac{2 (1-\lambda) \lambda (1+\lambda)^2 (1-x) x \left(\frac{(1+\lambda) x}{1-\lambda}\right)^{-\tfrac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}} [x-(1-\lambda)] \text{Log}\left[\frac{(1+\lambda) x}{1-\lambda}\right]}{[x(1+\lambda)-(1-\lambda)]^2},$ provided that $x \neq 1-\lambda.$ The expression gets much simpler by using the upper bound $Log[z] \leq z-1$ (for the case that $x > 1-\lambda$), but then the sign of the simplified expression unfortunately becomes ambiguous. A possibly simpler representation of $\frac{\partial S(x)}{\partial x}$ can be given by $$(1-\lambda)\int_{\underline{p_G}(x)}^{v} 2 x G(p;x) \frac{\partial G(p;x)}{\partial x} - G(p;x)^2 dp,$$ where  $$G(p;x) = \frac{x(1+\lambda) - (1-\lambda)\left(\frac{v}{p}\right)^{\tfrac{x(1+\lambda)-(1-\lambda)}{2x \lambda}}}{x(1+\lambda)-(1-\lambda)},\quad\underline{p_G}(x) = v \left(\frac{1-\lambda}{x(1+\lambda)}\right)^{\tfrac{2\lambda x}{x(1+\lambda)-(1-\lambda)}},$$ $$G(\underline{p_G}(x);x) = 0,\quad G(v) =1.$$ Moreover, I can show that $\frac{\partial G(p;x)}{\partial x} \geq 0.$ Any help would be welcome! I will happily include a thanking note to my (economics) research article for suggestions that lead to a solution. EDIT: Two other things that might help: $G(p;x)$ is actually the solution to the differential equation $G(p;x)p \left[1-\lambda-x(1+\lambda)\right] + x(1+\lambda)p + (1-x)(1-\lambda) \int_{p}^{v} z G'(z;x) dz - (1-\lambda)v = 0$, and $S(x) = \left(1-\lambda \right)\frac{\int_{\underline{p_G}(x)}^{v} G(p;x)^2dp}{x}$.","EDIT (for bounty): Consider the differential equation $G(p;x,\lambda)p \left[1-\lambda-x(1+\lambda)\right] + x(1+\lambda)p + (1-x)(1-\lambda) \int_{p}^{1} z G'(z;x,\lambda) dz - (1-\lambda) = 0$, where $G(p;x,\lambda)$ is a c.d.f. with parameters $x \in (0,1)$ and $\lambda \in (0,1)$. The boundary condition is $G(1,x,\lambda) = 1$. This equation has the solution $G(p;x,\lambda) = \frac{x(1+\lambda) - (1-\lambda)p^{-\frac{x(1+\lambda)-(1-\lambda)}{2x \lambda}}}{x(1+\lambda)-(1-\lambda)}.$ Solving $G(p;x,\lambda) = 0$, one can find its lower bound $\underline{p} = \left(\frac{1-\lambda}{x(1+\lambda)}\right)^{\frac{2\lambda x}{(1+\lambda)-(1-\lambda)}}$. Define $C_1(x) := \int_{\underline{p}}^{1} (1+\lambda) G(p;x,\lambda) - G(p;x,\lambda)^2 dp$ and $C_2(x) := \lambda + (1-\lambda) \int_{\underline{p}}^{1} G(p;x,\lambda)^2 dp$. One can show that both $C_1(x)$ and $C_2(x)$ are increasing in $x$. Also, $G(p;x,\lambda)$ is increasing in $x$ (over the relevant range). Moreover, I know that $x C_1(x) + (1-x)C_2(x) = \lambda$. OBJECTIVE: Show that $C_2(x)-C_1(x)$ is decreasing in $x$ over the relevant parameter range. Bonus objective: Show that it is convex (both of these features can be shown numerically). Possibly helpful hint: $C_2(x)-C_1(x)$ can also be expressed as $\frac{1-\lambda}{x} \int_{\underline{p}}^{1} G(p;x,\lambda)^2 dp$. Many thanks in advance for any suggestions that might lead to a proof! EDIT: Actually, the problem doesn't look too hard, but I still cannot solve it. Any clever ideas what to do here? Let $x \in (0,1), \lambda \in (0,1)$. Let $p(x,\lambda) = \left(\frac{(1+\lambda) x}{1-\lambda}\right)^{-\frac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}}$. Show that $\frac{(1-\lambda)(\lambda + x) - x(1+\lambda)^2 p(x,\lambda)}{(1-x)(1-x-\lambda)}$ is strictly decreasing in $x$ [for $x \in (0,1), \lambda \in (0,1)$]. Original post For a couple of days I've been struggling with confirming that $\frac{\partial S(x)}{\partial x}$, with $$S(x) =  v \dfrac{(1+\lambda)^2 x \left(\dfrac{1-\lambda}{(1+\lambda) x}\right)^{\tfrac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}}-(1-\lambda) (\lambda+x)}{(1-x) [x-(1-\lambda)]}\; \;,$$ is strictly negative for $\lambda \in (0,1)$, $x \in (0,1)$ [$v>0$ is irrelevant]. I can show this numerically, but a general proof would be desirable. For what it's worth, the derivative of this function can be shown to have the same sign as $(1-\lambda)\left[1+(1-\lambda) \lambda-2 \lambda x-x^{2^{\vphantom{a}}}\right]+  \frac{\Large\left(\frac{(1+\lambda) x}{1-\lambda}\right)^{-\frac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}} \left(\left(1-\lambda^2\right)^2-\left(1-\lambda^2\right)^2 x+(1+\lambda)^2 (-1+\lambda (-3+2 \lambda)) x^2+(1+\lambda)^2 (1+3 \lambda) x^3\right)}{\Large x(1+\lambda)-(1-\lambda)}$ $+\dfrac{2 (1-\lambda) \lambda (1+\lambda)^2 (1-x) x \left(\frac{(1+\lambda) x}{1-\lambda}\right)^{-\tfrac{2 \lambda x}{x(1+\lambda)-(1-\lambda)}} [x-(1-\lambda)] \text{Log}\left[\frac{(1+\lambda) x}{1-\lambda}\right]}{[x(1+\lambda)-(1-\lambda)]^2},$ provided that $x \neq 1-\lambda.$ The expression gets much simpler by using the upper bound $Log[z] \leq z-1$ (for the case that $x > 1-\lambda$), but then the sign of the simplified expression unfortunately becomes ambiguous. A possibly simpler representation of $\frac{\partial S(x)}{\partial x}$ can be given by $$(1-\lambda)\int_{\underline{p_G}(x)}^{v} 2 x G(p;x) \frac{\partial G(p;x)}{\partial x} - G(p;x)^2 dp,$$ where  $$G(p;x) = \frac{x(1+\lambda) - (1-\lambda)\left(\frac{v}{p}\right)^{\tfrac{x(1+\lambda)-(1-\lambda)}{2x \lambda}}}{x(1+\lambda)-(1-\lambda)},\quad\underline{p_G}(x) = v \left(\frac{1-\lambda}{x(1+\lambda)}\right)^{\tfrac{2\lambda x}{x(1+\lambda)-(1-\lambda)}},$$ $$G(\underline{p_G}(x);x) = 0,\quad G(v) =1.$$ Moreover, I can show that $\frac{\partial G(p;x)}{\partial x} \geq 0.$ Any help would be welcome! I will happily include a thanking note to my (economics) research article for suggestions that lead to a solution. EDIT: Two other things that might help: $G(p;x)$ is actually the solution to the differential equation $G(p;x)p \left[1-\lambda-x(1+\lambda)\right] + x(1+\lambda)p + (1-x)(1-\lambda) \int_{p}^{v} z G'(z;x) dz - (1-\lambda)v = 0$, and $S(x) = \left(1-\lambda \right)\frac{\int_{\underline{p_G}(x)}^{v} G(p;x)^2dp}{x}$.",,"['calculus', 'derivatives']"
81,Differential function problem,Differential function problem,,Let $f: \mathbb{R} → \mathbb{R} $ be a function such that $f(x)$ is differentiable on all $\mathbb{R}$ and $\lim_{x\to \infty}(f(x)-f(-x))=0$. Prove there exists $x_{0} \in \mathbb{R}$ such that $f'(x_{0})=0$ I tried proving it by contradiction using that the limits at infinity and minus infinity are equal I'm having hard time formalizing it. Any help appreciated.,Let $f: \mathbb{R} → \mathbb{R} $ be a function such that $f(x)$ is differentiable on all $\mathbb{R}$ and $\lim_{x\to \infty}(f(x)-f(-x))=0$. Prove there exists $x_{0} \in \mathbb{R}$ such that $f'(x_{0})=0$ I tried proving it by contradiction using that the limits at infinity and minus infinity are equal I'm having hard time formalizing it. Any help appreciated.,,['derivatives']
82,Equivalent definitions of $\beta$-smoothness,Equivalent definitions of -smoothness,\beta,"Suppose that the gradient of $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ is $\beta$ -Lipschitz, for some $\beta \geq 0$ , i.e. \begin{equation}       \|\nabla f(x)-\nabla f(y)\|_2\leq \beta\|x-y\|_2, \end{equation} for all $x,y\in\mathbb{R}^n$ . Show that this is equivalent to \begin{align*}         f(\theta x+(1-\theta)y)&\geq \theta f(x)+(1-\theta)f(y)-\tfrac{\beta}{2}\theta(1-\theta)\|x-y\|^2,\\         f(\theta x+(1-\theta)y)&\leq \theta f(x)+(1-\theta)f(y)+\tfrac{\beta}{2}\theta(1-\theta)\|x-y\|^2. \end{align*} for all $x,y\in\mathbb{R}^n$ and all $\theta \in [0,1]$ , Moreover, show that it is equivalent to \begin{align*}         f(y)&\geq f(x)+ \nabla f(x)^{T}(y-x) - \tfrac{\beta}{2}\|x-y\|^2,\\         f(y)&\leq f(x) + \nabla f(x)^{T}(y-x) + \tfrac{\beta}{2}\|x-y\|^2, \end{align*} for all $x,y\in\mathbb{R}^n$ .","Suppose that the gradient of is -Lipschitz, for some , i.e. for all . Show that this is equivalent to for all and all , Moreover, show that it is equivalent to for all .","f:\mathbb{R}^{n}\rightarrow\mathbb{R} \beta \beta \geq 0 \begin{equation}
      \|\nabla f(x)-\nabla f(y)\|_2\leq \beta\|x-y\|_2,
\end{equation} x,y\in\mathbb{R}^n \begin{align*}
        f(\theta x+(1-\theta)y)&\geq \theta f(x)+(1-\theta)f(y)-\tfrac{\beta}{2}\theta(1-\theta)\|x-y\|^2,\\
        f(\theta x+(1-\theta)y)&\leq \theta f(x)+(1-\theta)f(y)+\tfrac{\beta}{2}\theta(1-\theta)\|x-y\|^2.
\end{align*} x,y\in\mathbb{R}^n \theta \in [0,1] \begin{align*}
        f(y)&\geq f(x)+ \nabla f(x)^{T}(y-x) - \tfrac{\beta}{2}\|x-y\|^2,\\
        f(y)&\leq f(x) + \nabla f(x)^{T}(y-x) + \tfrac{\beta}{2}\|x-y\|^2,
\end{align*} x,y\in\mathbb{R}^n","['real-analysis', 'calculus', 'derivatives', 'optimization']"
83,Does there exist a function that is differentiable everywhere with everywhere discontinuous partial derivatives?,Does there exist a function that is differentiable everywhere with everywhere discontinuous partial derivatives?,,"Does there exist a function that is differentiable everywhere with everywhere discontinuous  partial derivatives? I just had this doubt, talking about first order partials.","Does there exist a function that is differentiable everywhere with everywhere discontinuous  partial derivatives? I just had this doubt, talking about first order partials.",,"['real-analysis', 'derivatives']"
84,What is the difference between gradient of divergence and Laplacian?,What is the difference between gradient of divergence and Laplacian?,,$\nabla^2\mathbf{V}$ versus $\nabla (\nabla \cdot \mathbf{V})$. They seem pretty much the same to me. Both give back a vector.,$\nabla^2\mathbf{V}$ versus $\nabla (\nabla \cdot \mathbf{V})$. They seem pretty much the same to me. Both give back a vector.,,"['derivatives', 'calculus']"
85,Minimum value of $\frac{b+1}{a+b-2}$,Minimum value of,\frac{b+1}{a+b-2},"If $a^2 + b^2= 1 $ and $u$ is the minimum value of the $\dfrac{b+1}{a+b-2}$, then find the value of $u^2$. Attempt: Then I tried this way: Let $a= bk$ for some real $k$. Then I got $f(b)$  in terms of b and k which is minmum when $b = \dfrac{2-k}{2(k+1)}$ ... then again I got an equation in $k$ which didn't simplify. Please suggest an efficient  way to solve it.","If $a^2 + b^2= 1 $ and $u$ is the minimum value of the $\dfrac{b+1}{a+b-2}$, then find the value of $u^2$. Attempt: Then I tried this way: Let $a= bk$ for some real $k$. Then I got $f(b)$  in terms of b and k which is minmum when $b = \dfrac{2-k}{2(k+1)}$ ... then again I got an equation in $k$ which didn't simplify. Please suggest an efficient  way to solve it.",,"['calculus', 'derivatives', 'trigonometry', 'optimization', 'quadratics']"
86,A function that is not a derivative [closed],A function that is not a derivative [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question If this is possible, could somebody give me an example of a function $f:[a,b]\to \Bbb{R}$ continuous in $[a,b]$ but that THERE IS NOT a function, namely G, such that its derivative is the funciont f? i.e., such that $$G'=f.$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question If this is possible, could somebody give me an example of a function $f:[a,b]\to \Bbb{R}$ continuous in $[a,b]$ but that THERE IS NOT a function, namely G, such that its derivative is the funciont f? i.e., such that $$G'=f.$$",,"['real-analysis', 'calculus', 'integration', 'derivatives']"
87,What is an example of a continuous function that doesn't have a derivative at any point? [closed],What is an example of a continuous function that doesn't have a derivative at any point? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Please give an example of a continuous function $f:[0,1]\to\mathbb R$ which doesn't have a derivative at any point. I can't think of anything, can someone help please?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Please give an example of a continuous function $f:[0,1]\to\mathbb R$ which doesn't have a derivative at any point. I can't think of anything, can someone help please?",,"['calculus', 'derivatives', 'continuity']"
88,If we have two functions that have composition differentiable does it mean both are differentiable?,If we have two functions that have composition differentiable does it mean both are differentiable?,,If $f : \mathbb{R}^{n} \rightarrow \mathbb{R}^m$ and $g : \mathbb{R}^m \rightarrow \mathbb{R}^p$ are functions such that $g \circ f$ differentiable does it mean both are differentiable ? I just need this for differential geometry. Suppose I know the following fact $proj : \mathbb{R}^m \rightarrow \mathbb{R}$ and $u : \mathbb{R}^n \rightarrow \mathbb{R}^m$ where $proj \circ u$ is differentiable does it mean that u is differentiable ? Here proj is the regular projection function.,If $f : \mathbb{R}^{n} \rightarrow \mathbb{R}^m$ and $g : \mathbb{R}^m \rightarrow \mathbb{R}^p$ are functions such that $g \circ f$ differentiable does it mean both are differentiable ? I just need this for differential geometry. Suppose I know the following fact $proj : \mathbb{R}^m \rightarrow \mathbb{R}$ and $u : \mathbb{R}^n \rightarrow \mathbb{R}^m$ where $proj \circ u$ is differentiable does it mean that u is differentiable ? Here proj is the regular projection function.,,['calculus']
89,Differentiate the squared dot product,Differentiate the squared dot product,,"I am new to Mathematics stack exchange community and has no experience in asking question so please bear with me. I am watching deep learning course from Coursera and encounter a question during the video. $$\left|\frac d{d\vec x}(\vec x\cdot\vec x)^2\right|=?$$ $$\vec x=\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}3\\4\end{bmatrix}$$ I am not sure how to perform dot product on 2 vectors since they are 2x1 and 2x1 dimension. Please guide me! Thanks Note: After some try and error, I got the answer, but I don't understand the solution.","I am new to Mathematics stack exchange community and has no experience in asking question so please bear with me. I am watching deep learning course from Coursera and encounter a question during the video. $$\left|\frac d{d\vec x}(\vec x\cdot\vec x)^2\right|=?$$ $$\vec x=\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}3\\4\end{bmatrix}$$ I am not sure how to perform dot product on 2 vectors since they are 2x1 and 2x1 dimension. Please guide me! Thanks Note: After some try and error, I got the answer, but I don't understand the solution.",,"['linear-algebra', 'derivatives', 'vectors']"
90,How do I use the power rule with $4x^2 - 2$?,How do I use the power rule with ?,4x^2 - 2,Every website I look at only shows the power use with a single term example or polynomial. Could somebody please give me a step by step breakdown of using the power rule to find $f'(x)$ where $f(x) = 4x^2 - 2$?,Every website I look at only shows the power use with a single term example or polynomial. Could somebody please give me a step by step breakdown of using the power rule to find $f'(x)$ where $f(x) = 4x^2 - 2$?,,"['calculus', 'derivatives']"
91,How do I prove this $\frac{dx^n}{dx}=nx^{n-1}$ is true for every $n\geq 1$ to convince my students?,How do I prove this  is true for every  to convince my students?,\frac{dx^n}{dx}=nx^{n-1} n\geq 1,let  $p_n(x)=x^n$ be a polynomial of degree $n$. I need help to be able to explain to my students why the derivative of $p$ is defined as follows: $$ p_n'(x)=\frac{dx^n}{dx}=nx^{n-1} $$ for every $n\geq1$. Note: I'd prefer geometric proofs if any exist. Edit :I edited the question as it is related to the recent question Thank you for any help,let  $p_n(x)=x^n$ be a polynomial of degree $n$. I need help to be able to explain to my students why the derivative of $p$ is defined as follows: $$ p_n'(x)=\frac{dx^n}{dx}=nx^{n-1} $$ for every $n\geq1$. Note: I'd prefer geometric proofs if any exist. Edit :I edited the question as it is related to the recent question Thank you for any help,,"['derivatives', 'polynomials', 'proof-verification']"
92,The sum of the series by derivatives,The sum of the series by derivatives,,We have a series $$\sum_{k=3}^\infty k \frac{\bigl(\frac{1+\sqrt5}{2}\big)^{k-1}}{2^{k-1}}$$ and I want to compute it like that. Let's say that $a = \bigl(\frac{1+\sqrt5}{2}\bigr)^{k-1}$ . Then we have $$\sum_{k=3}^\infty k \frac{a^{k-1}}{2^{k-1}} = \sum_{k=3}^\infty k (\frac{a}{2})^{k-1}$$ I tried to rewrite it as a derivative: $$\begin{align} \sum_{k=3}^\infty k \Bigl(\frac{a}{2}\Bigr)^{k-1} &= \sum_{k=3}^\infty \frac{d(\frac{a}{2})^{k}}{da} \\ &= \frac{d\frac{(\frac{a}{2})^{3}}{(1 - \frac{a}{2})}}{da} \\ &= \frac{3 * (\frac{a}{2})^2 * (1 - \frac{a}{2}) + (\frac{a}{2})^3 * \frac{1}{2}}{(1 - \frac{a}{2})^{2}} \end{align}$$ But the answer is wrong. What did I do incorrectly?,We have a series and I want to compute it like that. Let's say that . Then we have I tried to rewrite it as a derivative: But the answer is wrong. What did I do incorrectly?,"\sum_{k=3}^\infty k \frac{\bigl(\frac{1+\sqrt5}{2}\big)^{k-1}}{2^{k-1}} a = \bigl(\frac{1+\sqrt5}{2}\bigr)^{k-1} \sum_{k=3}^\infty k \frac{a^{k-1}}{2^{k-1}} = \sum_{k=3}^\infty k (\frac{a}{2})^{k-1} \begin{align}
\sum_{k=3}^\infty k \Bigl(\frac{a}{2}\Bigr)^{k-1}
&= \sum_{k=3}^\infty \frac{d(\frac{a}{2})^{k}}{da} \\
&= \frac{d\frac{(\frac{a}{2})^{3}}{(1 - \frac{a}{2})}}{da} \\
&= \frac{3 * (\frac{a}{2})^2 * (1 - \frac{a}{2}) + (\frac{a}{2})^3 * \frac{1}{2}}{(1 - \frac{a}{2})^{2}}
\end{align}","['calculus', 'integration', 'sequences-and-series', 'derivatives']"
93,How to take second derivative implicitly,How to take second derivative implicitly,,"Let $$y^4 + 5x = 21.$$ What is the value of $d^2y/dx^2$ at the point $(2, 1)$? I’m stuck at trying to work out the second implicit derivative of this function. As far as I can work out, the first derivative implicitly is $$\dfrac {-5}{4y^{3}}$$ How do you take the second derivative implicitly with respect to $x$ when $x$ has vanished? There would be nowhere to plug in $x=2$. What am I missing here?","Let $$y^4 + 5x = 21.$$ What is the value of $d^2y/dx^2$ at the point $(2, 1)$? I’m stuck at trying to work out the second implicit derivative of this function. As far as I can work out, the first derivative implicitly is $$\dfrac {-5}{4y^{3}}$$ How do you take the second derivative implicitly with respect to $x$ when $x$ has vanished? There would be nowhere to plug in $x=2$. What am I missing here?",,"['calculus', 'derivatives', 'implicit-differentiation']"
94,What is $\frac{d(\arctan(x))}{dx}$?,What is ?,\frac{d(\arctan(x))}{dx},Let $v= \arctan{x}$. Now I want to find $\frac{dv}{dx}$. My method is this: Rearranging yields $\tan(v) = x$ and so $dx = \sec^2(v)dv$. How do I simplify from here? Of course I could do something like $dx = \sec^2(\arctan(x))dv$ so that $\frac{dv}{dx} = \cos^2(\arctan(x))$ but I am sure a better expression exists. I am probably just missing some crucial step where we convert one of the trigonometric expressions into an expression involving $x$. Thanks in advance for any help or tips!,Let $v= \arctan{x}$. Now I want to find $\frac{dv}{dx}$. My method is this: Rearranging yields $\tan(v) = x$ and so $dx = \sec^2(v)dv$. How do I simplify from here? Of course I could do something like $dx = \sec^2(\arctan(x))dv$ so that $\frac{dv}{dx} = \cos^2(\arctan(x))$ but I am sure a better expression exists. I am probably just missing some crucial step where we convert one of the trigonometric expressions into an expression involving $x$. Thanks in advance for any help or tips!,,"['calculus', 'trigonometry', 'derivatives']"
95,"If $f(x^3 + x) = x^3 + x^2 + 1$, then what is $f'(2)$?","If , then what is ?",f(x^3 + x) = x^3 + x^2 + 1 f'(2),"If $f(x^3 + x) = x^3 + x^2 + 1$, then what is $f'(2)$? I don't even have an idea of how to solve this problem. I solved every single problem in my text book until this question so I thought I'm either missing some critical information about derivatives or just couldn't find the way to solve. Thanks in advance.","If $f(x^3 + x) = x^3 + x^2 + 1$, then what is $f'(2)$? I don't even have an idea of how to solve this problem. I solved every single problem in my text book until this question so I thought I'm either missing some critical information about derivatives or just couldn't find the way to solve. Thanks in advance.",,['derivatives']
96,If $f(x) =\sin^{1200}(x) \ln(1+x)^{500}\arctan^{300}(x)$ how to find $f^{(2000)}(0)$ without Taylor series?,If  how to find  without Taylor series?,f(x) =\sin^{1200}(x) \ln(1+x)^{500}\arctan^{300}(x) f^{(2000)}(0),"I saw this challenging problem: If $f(x) =\sin^{1200}(x) \ln(1+x)^{500}\arctan^{300}(x)$ how to find $f^{(2000)}(0)$ . Applying Leibniz rule seems to be the only way to solve this, but  it quickly became a nightmare finding a pattern: $$f'(x)=1200 \sin^{1199}(x) \ln(1+x)^{500}\arctan^{300}(x)\cos(x)+\sin^{1200}(x) 500\frac{\ln(1+x)^{499}}{1+x}\arctan^{300}(x)+300\sin^{1200}(x) \ln(1+x)^{500}\frac{\arctan^{299}(x)}{1+x^2}$$ This function becomes very messy and ugly in the first derivative so there is no way Leibniz rule is applied here. There must be some trick since $1200+500+300=2000$ . My friend (who gave me this problem) gave a very strange proof to this problem with  the answer $2000!$ but I didn't understand his proof. There is an easy way to solve this problem with Taylor's series like user170231's answer, But I want to ask is other way to solve it without Taylor series ?","I saw this challenging problem: If how to find . Applying Leibniz rule seems to be the only way to solve this, but  it quickly became a nightmare finding a pattern: This function becomes very messy and ugly in the first derivative so there is no way Leibniz rule is applied here. There must be some trick since . My friend (who gave me this problem) gave a very strange proof to this problem with  the answer but I didn't understand his proof. There is an easy way to solve this problem with Taylor's series like user170231's answer, But I want to ask is other way to solve it without Taylor series ?",f(x) =\sin^{1200}(x) \ln(1+x)^{500}\arctan^{300}(x) f^{(2000)}(0) f'(x)=1200 \sin^{1199}(x) \ln(1+x)^{500}\arctan^{300}(x)\cos(x)+\sin^{1200}(x) 500\frac{\ln(1+x)^{499}}{1+x}\arctan^{300}(x)+300\sin^{1200}(x) \ln(1+x)^{500}\frac{\arctan^{299}(x)}{1+x^2} 1200+500+300=2000 2000!,"['real-analysis', 'calculus', 'derivatives']"
97,$f'$ odd implies that $f$ is even,odd implies that  is even,f' f,"I can prove that if $f$ is even then $f'$ is odd (and $f$ odd implying $f'$ is even). However, I'm not sure how to prove it the other way around by using limits. To prove that $f'$ being odd implies that $f$ is even, I tried $f'(a) = -f'(-a).$ So, $$f'(a) = \lim_{h \rightarrow 0} \frac{f(a+h) - f(a)}{h} = -\lim_{h \rightarrow 0} \frac{f(-a+h) - f(-a)}{h} = -f'(-a) $$ Is this correct, and if so, where do I go from here?","I can prove that if $f$ is even then $f'$ is odd (and $f$ odd implying $f'$ is even). However, I'm not sure how to prove it the other way around by using limits. To prove that $f'$ being odd implies that $f$ is even, I tried $f'(a) = -f'(-a).$ So, $$f'(a) = \lim_{h \rightarrow 0} \frac{f(a+h) - f(a)}{h} = -\lim_{h \rightarrow 0} \frac{f(-a+h) - f(-a)}{h} = -f'(-a) $$ Is this correct, and if so, where do I go from here?",,"['calculus', 'real-analysis', 'derivatives']"
98,"Prove that $f: \mathbb{R} \to \mathbb{R}, f(x) = x^\frac{1}{9}$ is not a differentiable function",Prove that  is not a differentiable function,"f: \mathbb{R} \to \mathbb{R}, f(x) = x^\frac{1}{9}","Prove that the following function is not a differentiable function: $$f: \mathbb{R} \to \mathbb{R}, f(x) = x^\frac{1}{9}$$ I believe all I have to show is one point in the domain where the function is not differentiable: Hence I have the following proof: $$ f'(x) =\frac{1}{9x^\frac{8}{9}} $$ At $ x = 0$ , $f'(x)$ is undefined. Hence, $f(x)$ is not a differentiable function. Is this enough or do I need to show that the $\lim\limits_{h\to0} \frac{f(x_0 + h) - x_0}{h}$ does not exist in some other way.","Prove that the following function is not a differentiable function: I believe all I have to show is one point in the domain where the function is not differentiable: Hence I have the following proof: At , is undefined. Hence, is not a differentiable function. Is this enough or do I need to show that the does not exist in some other way.","f: \mathbb{R} \to \mathbb{R}, f(x) = x^\frac{1}{9}  f'(x) =\frac{1}{9x^\frac{8}{9}}   x = 0 f'(x) f(x) \lim\limits_{h\to0} \frac{f(x_0 + h) - x_0}{h}","['calculus', 'derivatives', 'solution-verification']"
99,How come $\frac{1}{1-x}$ and $\frac{x}{1-x}$ have the same derivative?,How come  and  have the same derivative?,\frac{1}{1-x} \frac{x}{1-x},"Maybe I'm just having a brain breakdown moment, but it seems weird to me that both functions have exactly the same derivative, namely $\frac{1}{(1-x)^2}$. Obviously I'm not disputing whether or not it's correct, but I'm looking for some sort of intuition/justification (geometric or otherwise) as to why this is indeed the case. I can take the derivatives manually so this is really not a question of not understanding the algebra, rather maybe having poor geometric intuition as to why the changing numerator as a function of $x$ doesn't impact the rate of change of the overall function.","Maybe I'm just having a brain breakdown moment, but it seems weird to me that both functions have exactly the same derivative, namely $\frac{1}{(1-x)^2}$. Obviously I'm not disputing whether or not it's correct, but I'm looking for some sort of intuition/justification (geometric or otherwise) as to why this is indeed the case. I can take the derivatives manually so this is really not a question of not understanding the algebra, rather maybe having poor geometric intuition as to why the changing numerator as a function of $x$ doesn't impact the rate of change of the overall function.",,['derivatives']
