,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differentiability of the operator norm,Differentiability of the operator norm,,"My question is simple. Given finite-dimensional real Banach spaces $V, W$ , is the operator norm on $\mathcal{L}(V, W) \setminus \{ 0 \}$ differentiable? I know the standard Euclidean norm would be, but I don’t know what to do with this.","My question is simple. Given finite-dimensional real Banach spaces , is the operator norm on differentiable? I know the standard Euclidean norm would be, but I don’t know what to do with this.","V, W \mathcal{L}(V, W) \setminus \{ 0 \}","['linear-algebra', 'derivatives', 'normed-spaces']"
1,An easy proof that an isometry preserving the zero vector is linear,An easy proof that an isometry preserving the zero vector is linear,,"I want to show that for real inner product spaces $V$ and $W$ , if $L:V\to W$ satisfies the following properties: $$\parallel L(\vec{x})-L(\vec{y})\parallel=\parallel \vec{x} -\vec{y}\parallel\\$$ and $$L(\vec{0})=\vec{0},$$ then this map is linear. I am aware of the existence of the (more general) theorem of Mazur-Ulam, but I was wondering if there is more accessible proof, which is suitable for beginners in linear algebra. Thanks in advance!","I want to show that for real inner product spaces and , if satisfies the following properties: and then this map is linear. I am aware of the existence of the (more general) theorem of Mazur-Ulam, but I was wondering if there is more accessible proof, which is suitable for beginners in linear algebra. Thanks in advance!","V W L:V\to W \parallel L(\vec{x})-L(\vec{y})\parallel=\parallel \vec{x} -\vec{y}\parallel\\ L(\vec{0})=\vec{0},","['linear-algebra', 'linear-transformations', 'isometry']"
2,Intuition for the invariance of the determinant under change of basis,Intuition for the invariance of the determinant under change of basis,,"$$A' = PAP^{-1}$$ $$\det(A')=\det(P)\det(A)\det(P^{-1})=\det(A)$$ Now, that makes sense algebraically, but consider the below diagram: This a geometric representation of the two 'normal' basis vectors $\bf i$ and $\bf j$ (I will denote this set by $B$ ) in $\Bbb R^2$ , and my choice of two new basis vectors $\bf i'$ and $\bf j'$ (I will denote this set by $B'$ ). The determinant preserves the area of of the unit square, which is determined by our choice of basis vectors. The unit square area in the basis $B$ is different to the unit square area in basis $B'$ . The determinant gives the area of the image of the unit square. The image of the black B unit square will likely be different to the image of the red $B'$ unit square, so why is $\det(A)=\det(A')$ ?","Now, that makes sense algebraically, but consider the below diagram: This a geometric representation of the two 'normal' basis vectors and (I will denote this set by ) in , and my choice of two new basis vectors and (I will denote this set by ). The determinant preserves the area of of the unit square, which is determined by our choice of basis vectors. The unit square area in the basis is different to the unit square area in basis . The determinant gives the area of the image of the unit square. The image of the black B unit square will likely be different to the image of the red unit square, so why is ?",A' = PAP^{-1} \det(A')=\det(P)\det(A)\det(P^{-1})=\det(A) \bf i \bf j B \Bbb R^2 \bf i' \bf j' B' B B' B' \det(A)=\det(A'),"['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'change-of-basis']"
3,Proof that cross product is orthogonal,Proof that cross product is orthogonal,,"I'm trying to prove that (u x v) is orthogonal to both u and v. Is it a sufficient proof to simply demonstrate that the dot product of u and (u x v) is equal to zero because due to the properties of the cross product, the previous expression is equivalent to the dot product of (u x u) and v. Since the cross product of u with itself is obviously 0, we can see that u is orthogonal to (u x v). I would repeat this same process for v. Is that a sufficient proof?","I'm trying to prove that (u x v) is orthogonal to both u and v. Is it a sufficient proof to simply demonstrate that the dot product of u and (u x v) is equal to zero because due to the properties of the cross product, the previous expression is equivalent to the dot product of (u x u) and v. Since the cross product of u with itself is obviously 0, we can see that u is orthogonal to (u x v). I would repeat this same process for v. Is that a sufficient proof?",,"['linear-algebra', 'proof-verification', 'orthogonality', 'cross-product']"
4,Is there a natural category in which the morphisms are derivative operators?,Is there a natural category in which the morphisms are derivative operators?,,"I'm studying general relativity. As I currently understand the theory, there's a part where we have a (differentiable) manifold $M$, and define a vector field on $M$ to be a function $v : (M \to \Re) \to (M \to \Re)$ satisfying: (linearity) $v(\alpha f + \beta g) = \alpha v(f) + \beta v(g)$ (Leibniz) $v(f \cdot g) = f \cdot v(g) + g \cdot v(f)$ we might also need to further restrict our attention to $v$ that are continuous. We can then show that the set of functions $v$ that satisfy these properties form a vector space, at which point we can generalize from vector fields to tensor fields. (Then, given a metric on $M$ we can derive a natural notion of differentiation on tensor fields, at which point we're ready to state some properties that the spacetime metric and the stress-energy tensor obey.) My question is, is there a (natural) category in which vector fields are just endomorphisms on $(M \to \Re)$? For example, condition (1) above arises automatically if we require that $v$ be an endomorphism of $(M \to \Re)$ in $\Re$-Vect; is there a well-known category such that conditions (1) and (2) arise automatically if we require $v$ to be an endomorphism of $(M \to \Re)$? Obviously I could simply define a category where the objects are vector spaces and the morphisms are linear maps that happen to satisfy the Leibniz rule [EDIT: This is wrong, as pointed out by Eric below -- given two $v$ that satisfy the property above, their composition does not in general satisfy the Leibniz property]; my question is, is this a well-known category (or, is there a simple variation on my question that allows me to see vector fields in a categorical light)? As an example of the genre of question that I'm asking, recall that a vector space over a field $K$ with vectors $V$ can be viewed either as a function $* : K \to V \to V$ satisfying a handful of axioms, or simply as a ring homomorphism between $K$ and the ring of group endomorphisms on $V$. In other words, we can either specify a vector space as a function obeying a bunch of axioms, or we can choose a morphism between the right objects in the right category (in this case, any $\phi : K \to_\text{Ring} (V \to_\text{Group} V)$) at which point the axioms come free. In the case of vector spaces, I could have defined a category in which all morphisms are vector spaces, but I probably wouldn't have noticed that I was trying to ask for a ring homomorphism between the scalar field and the ring of endomorphisms of the vector group. In my question here about a category where morphisms correspond to derivative operators, I'm hoping for an answer analogous to ""you're looking for a ring homomorphism between $K$ and $V \to_\text{Group} V$"".","I'm studying general relativity. As I currently understand the theory, there's a part where we have a (differentiable) manifold $M$, and define a vector field on $M$ to be a function $v : (M \to \Re) \to (M \to \Re)$ satisfying: (linearity) $v(\alpha f + \beta g) = \alpha v(f) + \beta v(g)$ (Leibniz) $v(f \cdot g) = f \cdot v(g) + g \cdot v(f)$ we might also need to further restrict our attention to $v$ that are continuous. We can then show that the set of functions $v$ that satisfy these properties form a vector space, at which point we can generalize from vector fields to tensor fields. (Then, given a metric on $M$ we can derive a natural notion of differentiation on tensor fields, at which point we're ready to state some properties that the spacetime metric and the stress-energy tensor obey.) My question is, is there a (natural) category in which vector fields are just endomorphisms on $(M \to \Re)$? For example, condition (1) above arises automatically if we require that $v$ be an endomorphism of $(M \to \Re)$ in $\Re$-Vect; is there a well-known category such that conditions (1) and (2) arise automatically if we require $v$ to be an endomorphism of $(M \to \Re)$? Obviously I could simply define a category where the objects are vector spaces and the morphisms are linear maps that happen to satisfy the Leibniz rule [EDIT: This is wrong, as pointed out by Eric below -- given two $v$ that satisfy the property above, their composition does not in general satisfy the Leibniz property]; my question is, is this a well-known category (or, is there a simple variation on my question that allows me to see vector fields in a categorical light)? As an example of the genre of question that I'm asking, recall that a vector space over a field $K$ with vectors $V$ can be viewed either as a function $* : K \to V \to V$ satisfying a handful of axioms, or simply as a ring homomorphism between $K$ and the ring of group endomorphisms on $V$. In other words, we can either specify a vector space as a function obeying a bunch of axioms, or we can choose a morphism between the right objects in the right category (in this case, any $\phi : K \to_\text{Ring} (V \to_\text{Group} V)$) at which point the axioms come free. In the case of vector spaces, I could have defined a category in which all morphisms are vector spaces, but I probably wouldn't have noticed that I was trying to ask for a ring homomorphism between the scalar field and the ring of endomorphisms of the vector group. In my question here about a category where morphisms correspond to derivative operators, I'm hoping for an answer analogous to ""you're looking for a ring homomorphism between $K$ and $V \to_\text{Group} V$"".",,"['linear-algebra', 'vector-spaces', 'category-theory', 'general-relativity']"
5,Every affine set can be expressed as the solution set of a system of linear equations,Every affine set can be expressed as the solution set of a system of linear equations,,"This is not a duplicate of How to prove: Every affine set can be expressed as the solution set of a system of linear equations . I have already read the provided answer , but I don't understand the logic of the answer. Here are the definitions and theorems I have proved. Definition . A set $C \subseteq \mathbb{R}^n$ is affine if $$x_1, x_2 \in C \text{ and }\theta \in \mathbb{R} \implies \theta x_1+(1-\theta)x_2\in C\text{.}$$ Theorem . If $C$ is affine and $x_1, \dots, x_k \in C$, $\sum_{i=1}^{k}\theta_i = 1$, then $\sum_{i=1}^{k}\theta_i x_i \in C$. Theorem . Let $C$ be affine and $x_0 \in C$. Then $$V = C - x_0 = \{x-x_0:x \in C\}$$   is a subspace of $\mathbb{R}^n$. Theorem . The set $C = \{x : Ax = b\}$ is affine, with $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. Theorem . With $C = \{x : Ax = b\}$, the set $V$ (defined above) is the nullspace of $A$. What I want to prove : Every affine set can be expressed as the solution set of a system of linear equations. What I think this means : If $C \subseteq \mathbb{R}^n$ is affine, then $C = \{x: Ax = b\}$ for some $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. This means that we need to show that $C \subseteq \{x: Ax = b\}$ and $\{x: Ax = b\} \subseteq C$. How does this imply the answer in the link above ? Fix an element $x_0 \in C$. Claim: The set $K = \{x-x_0: x \in C\}$ is a linear subspace of $\Bbb R^n$. Claim: There exists a linear map $T$ whose kernel is precisely $K$. Claim: $C$ is the solution to the linear system $Tx = Tx_0$ on $x$. Please note that I'm very rusty on linear transformations and the associated terminology.","This is not a duplicate of How to prove: Every affine set can be expressed as the solution set of a system of linear equations . I have already read the provided answer , but I don't understand the logic of the answer. Here are the definitions and theorems I have proved. Definition . A set $C \subseteq \mathbb{R}^n$ is affine if $$x_1, x_2 \in C \text{ and }\theta \in \mathbb{R} \implies \theta x_1+(1-\theta)x_2\in C\text{.}$$ Theorem . If $C$ is affine and $x_1, \dots, x_k \in C$, $\sum_{i=1}^{k}\theta_i = 1$, then $\sum_{i=1}^{k}\theta_i x_i \in C$. Theorem . Let $C$ be affine and $x_0 \in C$. Then $$V = C - x_0 = \{x-x_0:x \in C\}$$   is a subspace of $\mathbb{R}^n$. Theorem . The set $C = \{x : Ax = b\}$ is affine, with $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. Theorem . With $C = \{x : Ax = b\}$, the set $V$ (defined above) is the nullspace of $A$. What I want to prove : Every affine set can be expressed as the solution set of a system of linear equations. What I think this means : If $C \subseteq \mathbb{R}^n$ is affine, then $C = \{x: Ax = b\}$ for some $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. This means that we need to show that $C \subseteq \{x: Ax = b\}$ and $\{x: Ax = b\} \subseteq C$. How does this imply the answer in the link above ? Fix an element $x_0 \in C$. Claim: The set $K = \{x-x_0: x \in C\}$ is a linear subspace of $\Bbb R^n$. Claim: There exists a linear map $T$ whose kernel is precisely $K$. Claim: $C$ is the solution to the linear system $Tx = Tx_0$ on $x$. Please note that I'm very rusty on linear transformations and the associated terminology.",,"['linear-algebra', 'systems-of-equations']"
6,The length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.,The length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.,,"I am referring to Theorem 2.23 of the book is Linear Algebra Done Right by Axler. It mentions Theorem: In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. Proof Suppose $u_1, u_2,.....,u_m$ is linearly independent in V. Suppose also that   $w_1,w_2,...,w_n$ spans V. We need to prove that $m \leq n$. We do so through the   multi-step process described below; note that in each step we add one of the $u$’s and remove one of the $w$’s. Step 1 Let B be the list $w_1,w_2,...,w_n$, which spans V. Thus adjoining any vector   in V to this list produces a linearly dependent list (because the newly   adjoined vector can be written as a linear combination of the other   vectors). In particular, the list   $u_1,w_1,...,w_n$ is linearly dependent. Thus by the Linear Dependence Lemma (2.21),   we can remove one of the $w$’s so that the new list B (of length $n$)   consisting of $u_1$ and the remaining $w$’s spans V. Step j The list B (of length $n$) from step $j-1$ spans V. Thus adjoining any   vector to this list produces a linearly dependent list. In particular, the   list of length $n+1$ obtained by adjoining $u_j$ to B, placing it just after   $u_1,u_2,...,u_{j-1}$, is linearly dependent. By the Linear Dependence Lemma   (2.21), one of the vectors in this list is in the span of the previous ones,   and because $u_1,u_2,...,u_j$ is linearly independent, this vector is one of   the $w$’s, not one of the $u$’s. We can remove that $w$ from B so that the   new list B (of length $n$) consisting of $u_1,u_2,...,u_j$ and the remaining $w$’s   spans V. I have problem with the part that states By the Linear Dependence Lemma (2.21), one of the vectors in this list is in the span of the previous ones, and because $u_1,u_2,...,u_j$ is linearly independent, this vector is one of the $w$’s, not one of the $u$’s. Why does linear independence of the list $u_1,u_2,...,u_j$ imply that one of $u$'s cannot be written as a linear combination of the rest of $u$'s and $w$'s in the list? What I can understand is that if the author said it must be possible to select one of $w$'s as otherwise, the $u$'s will end up being linearly dependent, then he'd be right. If you cannot choose any of the $w$'s and the list is known to be linearly dependent, then one of the $u$'s will end up being in the span of the rest of the $u$'s. This is not what he states though. He states it has to be one of $w$'s. I think that statement is wrong. If I am making a mistake in the way I have understood the proof, please help me understand it correctly.","I am referring to Theorem 2.23 of the book is Linear Algebra Done Right by Axler. It mentions Theorem: In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. Proof Suppose $u_1, u_2,.....,u_m$ is linearly independent in V. Suppose also that   $w_1,w_2,...,w_n$ spans V. We need to prove that $m \leq n$. We do so through the   multi-step process described below; note that in each step we add one of the $u$’s and remove one of the $w$’s. Step 1 Let B be the list $w_1,w_2,...,w_n$, which spans V. Thus adjoining any vector   in V to this list produces a linearly dependent list (because the newly   adjoined vector can be written as a linear combination of the other   vectors). In particular, the list   $u_1,w_1,...,w_n$ is linearly dependent. Thus by the Linear Dependence Lemma (2.21),   we can remove one of the $w$’s so that the new list B (of length $n$)   consisting of $u_1$ and the remaining $w$’s spans V. Step j The list B (of length $n$) from step $j-1$ spans V. Thus adjoining any   vector to this list produces a linearly dependent list. In particular, the   list of length $n+1$ obtained by adjoining $u_j$ to B, placing it just after   $u_1,u_2,...,u_{j-1}$, is linearly dependent. By the Linear Dependence Lemma   (2.21), one of the vectors in this list is in the span of the previous ones,   and because $u_1,u_2,...,u_j$ is linearly independent, this vector is one of   the $w$’s, not one of the $u$’s. We can remove that $w$ from B so that the   new list B (of length $n$) consisting of $u_1,u_2,...,u_j$ and the remaining $w$’s   spans V. I have problem with the part that states By the Linear Dependence Lemma (2.21), one of the vectors in this list is in the span of the previous ones, and because $u_1,u_2,...,u_j$ is linearly independent, this vector is one of the $w$’s, not one of the $u$’s. Why does linear independence of the list $u_1,u_2,...,u_j$ imply that one of $u$'s cannot be written as a linear combination of the rest of $u$'s and $w$'s in the list? What I can understand is that if the author said it must be possible to select one of $w$'s as otherwise, the $u$'s will end up being linearly dependent, then he'd be right. If you cannot choose any of the $w$'s and the list is known to be linearly dependent, then one of the $u$'s will end up being in the span of the rest of the $u$'s. This is not what he states though. He states it has to be one of $w$'s. I think that statement is wrong. If I am making a mistake in the way I have understood the proof, please help me understand it correctly.",,['linear-algebra']
7,Determinant of 9 Consecutive Integers,Determinant of 9 Consecutive Integers,,"Let a $3 \times 3$ matrix have the elements $1,2,\dots,9$. What is the maximum value the determinant may have? I have found the desired value and an intuitive feeling/approach as to why that must be optimal. I struggle to really prove that claim, though.","Let a $3 \times 3$ matrix have the elements $1,2,\dots,9$. What is the maximum value the determinant may have? I have found the desired value and an intuitive feeling/approach as to why that must be optimal. I struggle to really prove that claim, though.",,['linear-algebra']
8,Finding matrices with $A_1^{-1}+A_2^{-1}+\dots+A_k^{-1}=(A_1+A_2+\dots+A_k)^{-1}$,Finding matrices with,A_1^{-1}+A_2^{-1}+\dots+A_k^{-1}=(A_1+A_2+\dots+A_k)^{-1},"Prove that for any $n, k\geq2$ there exist nonsingular nondiagonal matrices $A_1, A_2, \dots, A_k \in M_n(\mathbb{R})$ such that $$A_1^{-1} + A_2^{-1} + \dots + A_k^{-1} = \left( A_1 + A_2 + \dots + A_k \right)^{-1}$$ For $n=2$ , if we have $A^{-1} + B^{-1} = (A+B)^{-1}$ , if I am not mistaken, we can prove that $$\det(A)=\det(B)=\det(A+B)$$ if $A,B\in M_n(\mathbb{R})$ , so it would be natural to consider that $\det(A_1)=\det(A_2)=\dots=\det(A_k)$ , but from here I don't have any idea what should I do. What should we do?","Prove that for any there exist nonsingular nondiagonal matrices such that For , if we have , if I am not mistaken, we can prove that if , so it would be natural to consider that , but from here I don't have any idea what should I do. What should we do?","n, k\geq2 A_1, A_2, \dots, A_k \in M_n(\mathbb{R}) A_1^{-1} + A_2^{-1} + \dots + A_k^{-1} = \left( A_1 + A_2 + \dots + A_k \right)^{-1} n=2 A^{-1} + B^{-1} = (A+B)^{-1} \det(A)=\det(B)=\det(A+B) A,B\in M_n(\mathbb{R}) \det(A_1)=\det(A_2)=\dots=\det(A_k)","['linear-algebra', 'matrices', 'inverse']"
9,"Suppose M is a module over integral domain R. If M is generated by k elements, then $rank(M) \leq k$ (Confirm my proof ?)","Suppose M is a module over integral domain R. If M is generated by k elements, then  (Confirm my proof ?)",rank(M) \leq k,"I am self studying algebra and am wondering if the following proof works. By rank we mean the supremum of cardinalities of linearly independent sets. Now this is equivalent to the supremum of cardinalities of maximal linearly independent set which is equal to the cardinality of any maximal independent set as any two maximal independent sets have the same cardinality. Here is my argument: I said let $B=\{a_1,...a_n\}$ be a maximal linearly independent subset of M of cardinality rank(M). (We can always choose such a B whose cardinality is the rank of M). Then consider passing to the field of fractions F. B is easily seen to be a linearly independent set over F by clearing denominators as R is an integral domain: Namely if $(c_1/d_1)a_1 + ... (c_n/d_n) a_n=0$ we have by multiplying through by $d_1 d_2... d_n$   that $(c_1 d_2... d_n) a_1 + ... + (c_n d_1...d_{n-1}) a_n =0$ but now these coefficients are in R and hence they must all equal $0$ by linear independece over R. But R is integral domain and the $d_i$ are non zero being denominators, so each $c_i=0$ and hence we see that B is an independent subset over F as well. Now M is finitely generated over R, so by embedding M in its quotient field, M is also finitely generated over F obviously. Now if this finite set that generates M has cardinality k, then the basis of M as a vector space over F has cardinality less or equal to k being a minimal spanning set. Finally B being a independent subset of a vector space M has cardinality less than the cardinality of the basis which has cardinality less than k. Hence B has cardinality less than k. But B is a maximal linearly independent set, hence $rank(M)\leq k$. Is my proof correct? I realise I have assumed that the rank is finite, but assuming it is finite, is it a correct proof? Thanks in advance. Edit: Oh wait, I guess I can't really do this as M is not free.. I was thinking M is isomorphic to R^d for some d hence pass to F^d but M is not necessarily free.","I am self studying algebra and am wondering if the following proof works. By rank we mean the supremum of cardinalities of linearly independent sets. Now this is equivalent to the supremum of cardinalities of maximal linearly independent set which is equal to the cardinality of any maximal independent set as any two maximal independent sets have the same cardinality. Here is my argument: I said let $B=\{a_1,...a_n\}$ be a maximal linearly independent subset of M of cardinality rank(M). (We can always choose such a B whose cardinality is the rank of M). Then consider passing to the field of fractions F. B is easily seen to be a linearly independent set over F by clearing denominators as R is an integral domain: Namely if $(c_1/d_1)a_1 + ... (c_n/d_n) a_n=0$ we have by multiplying through by $d_1 d_2... d_n$   that $(c_1 d_2... d_n) a_1 + ... + (c_n d_1...d_{n-1}) a_n =0$ but now these coefficients are in R and hence they must all equal $0$ by linear independece over R. But R is integral domain and the $d_i$ are non zero being denominators, so each $c_i=0$ and hence we see that B is an independent subset over F as well. Now M is finitely generated over R, so by embedding M in its quotient field, M is also finitely generated over F obviously. Now if this finite set that generates M has cardinality k, then the basis of M as a vector space over F has cardinality less or equal to k being a minimal spanning set. Finally B being a independent subset of a vector space M has cardinality less than the cardinality of the basis which has cardinality less than k. Hence B has cardinality less than k. But B is a maximal linearly independent set, hence $rank(M)\leq k$. Is my proof correct? I realise I have assumed that the rank is finite, but assuming it is finite, is it a correct proof? Thanks in advance. Edit: Oh wait, I guess I can't really do this as M is not free.. I was thinking M is isomorphic to R^d for some d hence pass to F^d but M is not necessarily free.",,"['linear-algebra', 'abstract-algebra', 'modules']"
10,Why is $BB^T$ always invertible?,Why is  always invertible?,BB^T,"In Karmarkar’s method, we use $$[I - B^T(BB^T)^{-1}B]v$$ Why does $BB^T$ always have an inverse? Karmarkar’s method is applied to an LP in the following form: $\min z = cx$ subject to $AX=0$ $x_1 +x_2 +......+ x_n =1$ $X\ge0$ $x =[x_1 ,x_2,.....,x_n]^T$, $A$ is an $m \times n$ matrix, $c = [c_1, c_2, .....			 ,c_n]$ ,and 0 is an n-dimensional column vector of zeros. The LP must also satisfy $[\frac{1}{n},\frac{1}{n},.....,\frac{1}{n}]^T$ is feasible , Optimal $z-$value $=0$ B is the $(m * 1) * n$ matrix whose first m rows are A and whose last row is a vector of $1’$s. $B = \begin{bmatrix}A\\1 \end{bmatrix}$","In Karmarkar’s method, we use $$[I - B^T(BB^T)^{-1}B]v$$ Why does $BB^T$ always have an inverse? Karmarkar’s method is applied to an LP in the following form: $\min z = cx$ subject to $AX=0$ $x_1 +x_2 +......+ x_n =1$ $X\ge0$ $x =[x_1 ,x_2,.....,x_n]^T$, $A$ is an $m \times n$ matrix, $c = [c_1, c_2, .....			 ,c_n]$ ,and 0 is an n-dimensional column vector of zeros. The LP must also satisfy $[\frac{1}{n},\frac{1}{n},.....,\frac{1}{n}]^T$ is feasible , Optimal $z-$value $=0$ B is the $(m * 1) * n$ matrix whose first m rows are A and whose last row is a vector of $1’$s. $B = \begin{bmatrix}A\\1 \end{bmatrix}$",,"['linear-algebra', 'matrices', 'optimization', 'inverse', 'linear-programming']"
11,Proof of Theorem 7 (Chapter 5) in Hoffman and Kunze's *Linear Algebra* is unclear,Proof of Theorem 7 (Chapter 5) in Hoffman and Kunze's *Linear Algebra* is unclear,,"Let $V$ be a free module of rank $n$ over a commutative ring $K$ with identity. We denote the space of all $r$-linear forms on $V$ by $M^r(V)$ and the space of all alternating $r$-linear forms by $\Lambda^r(V)$. For $L \in M^r(V)$ and any permutation $\sigma$ of $\{1,\dots,r\}$, we obtain another $r$-linear function $L_\sigma$ by defining $$L_\sigma(\alpha_1,\dots,\alpha_r) = L(\alpha_{\sigma 1},\dots,\alpha_{\sigma r})$$ for all $(\alpha_1,\dots,\alpha_r) \in V^r$. For each $L \in M^r(V)$, we define the alternating $r$-linear function $\pi_r L$ by $$\pi_r L = \sum_\sigma (\operatorname{sgn} \sigma) L_\sigma$$ where the sum is over all permutations $\sigma$ of $\{1,\dots,r\}$. Now, Theorem 7 of Chapter 5 in Hoffman and Kunze's Linear Algebra states the following: Theorem $7$. Let $K$ be a commutative ring with identity and let $V$ be a free $K$-module of rank $n$. If $r > n$, then $\Lambda^r(V) = \{0\}$. If $1 \leq r \leq n$, then $\Lambda^r(V)$ is a free $K$-module of rank $\binom{n}{r}$. Proof. Suppose $\{ \beta_1,\dots,\beta_n \}$ is an ordered basis for $V$ with dual basis $\{ f_1,\dots,f_n\}$. If $L \in M^r(V)$, then $$L = \sum_H L(\beta_{h_1},\dots,\beta_{h_r})\ f_{h_1}\! \otimes \dots \otimes f_{h_r} \tag{5-37}$$ where the sum extends over all $r$-tuples $H = (h_1,\dots,h_r)$ of integers between $1$ and $n$. If $L \in \Lambda^r(V)$, this sum need be extended only over the $r$-tuples $H$ for which $h_1,\dots,h_r$ are distinct because if $L$ is alternating then $$L(\beta_{h_1},\dots,\beta_{h_r}) = 0$$ whenever two subscripts $h_i$ are the same. If $r > n$ then in each $r$-tuple some integer must be repeated. Thus $\Lambda^r(V) = \{0\}$ if $r > n$. Now, suppose $1 \leq r \leq n$. We define an $r$-shuffle of $\{ 1,\dots, n\}$ to be an $r$-tuple $J = (j_1,\dots,j_r)$ such that $1 \leq j_1 < \dots < j_r \leq n$. There are $$\binom{n}{r} = \frac{n!}{r!(n-r)!}$$ such shuffles. Suppose we fix an $r$-shuffle $J$. Let $L_J$ be the sum of all the terms in $(5\text{-}37)$ for which the indexing $r$-tuple $H$ is a permutation of the $r$-shuffle $J$. If $\sigma$ is a permutation of $\{1,\dots,r\}$, then $$L(\beta_{j_{\sigma 1}},\dots,\beta_{j_{\sigma r}}) = (\operatorname{sgn} \sigma) L(\beta_{j_1},\dots,\beta_{j_r}).$$ Thus, $$L_J = L(\beta_{j_1},\dots,\beta_{j_r}) D_J \tag{5-38}$$ where $$\begin{align} D_J &= \sum_\sigma (\operatorname{sgn} \sigma)\ f_{j_{\sigma 1}}\! \otimes \dots \otimes f_{j_{\sigma r}} \tag{5-39}\\ &= \pi_r(f_{j_1}\! \otimes \dots \otimes f_{j_r}). \end{align}$$ We see from $(5\text{-}39)$ that each $D_J$ is alternating and that $$L = \sum_{\text{shuffles $J$}} L(\beta_{j_1},\dots,\beta_{j_r}) D_J \tag{5-40}$$ for every $L$ in $\Lambda^r(V)$. The assertion is that the $\binom{n}{r}$ forms $D_J$ constitute a basis for $\Lambda^r(V)$. We have seen that they span $\Lambda^r(V)$. It is easy to see that they are independent. Hence, proved. My doubt is how in Equation $(5\text{-}39)$ we can go from the first line to the second line. It does not seem to follow directly from the definition of $\pi_r L$, because $$\pi_r(f_{j_1}\! \otimes \dots \otimes f_{j_r}) := \sum_\sigma (\operatorname{sgn} \sigma) (f_{j_1}\! \otimes \dots \otimes f_{j_r} )_\sigma \stackrel{?}{=} \sum_{\sigma} (\operatorname{sgn} \sigma)\ f_{j_{\sigma 1}}\! \otimes \dots \otimes f_{j_{\sigma r}}.$$ If someone can give me a step-by-step proof of the equality it would be really helpful.","Let $V$ be a free module of rank $n$ over a commutative ring $K$ with identity. We denote the space of all $r$-linear forms on $V$ by $M^r(V)$ and the space of all alternating $r$-linear forms by $\Lambda^r(V)$. For $L \in M^r(V)$ and any permutation $\sigma$ of $\{1,\dots,r\}$, we obtain another $r$-linear function $L_\sigma$ by defining $$L_\sigma(\alpha_1,\dots,\alpha_r) = L(\alpha_{\sigma 1},\dots,\alpha_{\sigma r})$$ for all $(\alpha_1,\dots,\alpha_r) \in V^r$. For each $L \in M^r(V)$, we define the alternating $r$-linear function $\pi_r L$ by $$\pi_r L = \sum_\sigma (\operatorname{sgn} \sigma) L_\sigma$$ where the sum is over all permutations $\sigma$ of $\{1,\dots,r\}$. Now, Theorem 7 of Chapter 5 in Hoffman and Kunze's Linear Algebra states the following: Theorem $7$. Let $K$ be a commutative ring with identity and let $V$ be a free $K$-module of rank $n$. If $r > n$, then $\Lambda^r(V) = \{0\}$. If $1 \leq r \leq n$, then $\Lambda^r(V)$ is a free $K$-module of rank $\binom{n}{r}$. Proof. Suppose $\{ \beta_1,\dots,\beta_n \}$ is an ordered basis for $V$ with dual basis $\{ f_1,\dots,f_n\}$. If $L \in M^r(V)$, then $$L = \sum_H L(\beta_{h_1},\dots,\beta_{h_r})\ f_{h_1}\! \otimes \dots \otimes f_{h_r} \tag{5-37}$$ where the sum extends over all $r$-tuples $H = (h_1,\dots,h_r)$ of integers between $1$ and $n$. If $L \in \Lambda^r(V)$, this sum need be extended only over the $r$-tuples $H$ for which $h_1,\dots,h_r$ are distinct because if $L$ is alternating then $$L(\beta_{h_1},\dots,\beta_{h_r}) = 0$$ whenever two subscripts $h_i$ are the same. If $r > n$ then in each $r$-tuple some integer must be repeated. Thus $\Lambda^r(V) = \{0\}$ if $r > n$. Now, suppose $1 \leq r \leq n$. We define an $r$-shuffle of $\{ 1,\dots, n\}$ to be an $r$-tuple $J = (j_1,\dots,j_r)$ such that $1 \leq j_1 < \dots < j_r \leq n$. There are $$\binom{n}{r} = \frac{n!}{r!(n-r)!}$$ such shuffles. Suppose we fix an $r$-shuffle $J$. Let $L_J$ be the sum of all the terms in $(5\text{-}37)$ for which the indexing $r$-tuple $H$ is a permutation of the $r$-shuffle $J$. If $\sigma$ is a permutation of $\{1,\dots,r\}$, then $$L(\beta_{j_{\sigma 1}},\dots,\beta_{j_{\sigma r}}) = (\operatorname{sgn} \sigma) L(\beta_{j_1},\dots,\beta_{j_r}).$$ Thus, $$L_J = L(\beta_{j_1},\dots,\beta_{j_r}) D_J \tag{5-38}$$ where $$\begin{align} D_J &= \sum_\sigma (\operatorname{sgn} \sigma)\ f_{j_{\sigma 1}}\! \otimes \dots \otimes f_{j_{\sigma r}} \tag{5-39}\\ &= \pi_r(f_{j_1}\! \otimes \dots \otimes f_{j_r}). \end{align}$$ We see from $(5\text{-}39)$ that each $D_J$ is alternating and that $$L = \sum_{\text{shuffles $J$}} L(\beta_{j_1},\dots,\beta_{j_r}) D_J \tag{5-40}$$ for every $L$ in $\Lambda^r(V)$. The assertion is that the $\binom{n}{r}$ forms $D_J$ constitute a basis for $\Lambda^r(V)$. We have seen that they span $\Lambda^r(V)$. It is easy to see that they are independent. Hence, proved. My doubt is how in Equation $(5\text{-}39)$ we can go from the first line to the second line. It does not seem to follow directly from the definition of $\pi_r L$, because $$\pi_r(f_{j_1}\! \otimes \dots \otimes f_{j_r}) := \sum_\sigma (\operatorname{sgn} \sigma) (f_{j_1}\! \otimes \dots \otimes f_{j_r} )_\sigma \stackrel{?}{=} \sum_{\sigma} (\operatorname{sgn} \sigma)\ f_{j_{\sigma 1}}\! \otimes \dots \otimes f_{j_{\sigma r}}.$$ If someone can give me a step-by-step proof of the equality it would be really helpful.",,['linear-algebra']
12,What is the geometrical meaning of symmetric matrix [closed],What is the geometrical meaning of symmetric matrix [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Could anyone help explain what is the geometric meaning of symmetric matrix in context of hypercube or parallelepiped  in Euclidean coordinate?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Could anyone help explain what is the geometric meaning of symmetric matrix in context of hypercube or parallelepiped  in Euclidean coordinate?,,"['linear-algebra', 'matrices', 'euclidean-geometry', 'symmetric-matrices']"
13,The inverse of a Kac-Murdock-Szegő matrix,The inverse of a Kac-Murdock-Szegő matrix,,I want to calculate the inverse of a matrix resembling the following form: \begin{bmatrix} 1 &\rho &\rho^2 &\cdots &\rho^{n-1}\\ \rho& 1& \rho& \cdots &\rho^{n-2}\\ \rho^2& \rho& 1 &\cdots&\rho^{n-3}\\ \vdots&\vdots &\vdots &\ddots &\vdots\\ \rho^{n-1} & \rho^{n-2}&\rho^{n-3} &\cdots&1 \end{bmatrix} each line of the off-diagonal is a polynomial of $\rho$ and the exponential is increasing towards the boundary. I wonder how to solve this problem.,I want to calculate the inverse of a matrix resembling the following form: each line of the off-diagonal is a polynomial of and the exponential is increasing towards the boundary. I wonder how to solve this problem.,"\begin{bmatrix}
1 &\rho &\rho^2 &\cdots &\rho^{n-1}\\
\rho& 1& \rho& \cdots &\rho^{n-2}\\
\rho^2& \rho& 1 &\cdots&\rho^{n-3}\\
\vdots&\vdots &\vdots &\ddots &\vdots\\
\rho^{n-1} & \rho^{n-2}&\rho^{n-3} &\cdots&1
\end{bmatrix} \rho","['linear-algebra', 'matrices', 'inverse', 'toeplitz-matrices']"
14,Determinants of square matrices filled with consecutive squares,Determinants of square matrices filled with consecutive squares,,"This is my first question, please forgive me if I mistake something, since I don't think I will be allowed to edit the question later. So, let me explain the kind of matrices I'm talking about. Think of a $n\times n$ square matrix, and think of a sequence of $n^2$ consecutive squares starting at $k^2$ (the most natural choice being $k=1$). Now place those squares in the matrix as if you were writing, row by row. I'll dare to invent a notation for these ""Square Matrice filled with Consecutive Squares"": $SMCS(n,k)$. So we have, for example: $$ SMCS(3,4)=         \begin{bmatrix}         4^2 & 5^2 & 6^2 \\         7^2 & 8^2 & 9^2 \\         10^2 & 11^2 & 12^2 \\         \end{bmatrix}= \begin{bmatrix}         16 & 25 & 36 \\         49 & 64 & 81 \\         100 & 121 & 144 \\         \end{bmatrix} $$ I wasn't able to find anything about matrices like these neither here nor elsewhere. Probably there are of no mathematical interest. My interest starts from a lecture, years ago, when the professor made us notice that the ""Square Matrice filled with Consecutive Integers"" $1$ to $9$ —it is the cell phone keypad! I'll call it $SMCI(3,1)$— is singular. That is: $$ \det \begin{bmatrix}         1 & 2 & 3 \\         4 & 5 & 6 \\         7 & 8 & 9 \\         \end{bmatrix}=0 $$ I find very intuitive that this elegant property holds for matrices of higher order and even if starting from a different integer, that is  $$ \det \left[SMCI(n,k)\right]=0 \qquad n\geqslant 3, \;\forall k $$ I can recognize a clear pattern (where the middle culumns are an ""average"" of the ones at their sides) so that I feel relieved from the need to provide an explicit proof for this result, that shouldn't be too hard anyway. Now I'm looking at the determinants of the matrices filled with squares. With no surprise I find nonsingular matrices of order $2$. Then, with some hope, I look at $SMCS(3,1)$ (the cell phone keypad with each number squared) but I find that the determinant is $-216$. So I find myself admitting that, understandably, the property doesn't hold for matrices filled with powers. Yet something curious happens: the result is the same even if I take a different starting point, that is $$ \det\left[SMCS(3,k)\right]=-216\qquad \forall k $$ This was a surprise, but it was pretty straightforward proving it with some direct algebra calculations. Here comes the big deal. I went on looking at higher orders, wondering if there were a characteristic constant for each $n$. Instead... $0$'s began to appear again! Me unbeliever! I've grown convinced that $$ \det \left[SMCS(n,k)\right]=0 \qquad n\geqslant 4, \;\forall k $$ but attempting a direct proof, if only for $n=4$, was out of the question. Let alone a general proof for any $n$, which is what I would really be interested in, but I wouldn't know even where to start from. I don't really need the rigorous proof, what I'd love is to be sure of the validity of the property, and possibly a way to ""understand"" it in a manner similar to what I could do with the $SMCI$ matrices, to be ""convinced"" of the result. Can anyone help my with this? Thank you! Wow, great, I got it! It took me some effort, but I got it all, thank you JeanMarie! Forgive me for coming back with some delay, but I saw you constantly improving your answer, and considering my time zone disadvantage I had to give up for the day. Understanding your ""trick"" allows to easily extend the property to even higher orders; for example rows (or columns) of consecutive cubes give singular matrices from $5 \times 5$ on. If I extend the notation to ""Square Matrix filled with Consecutive Powers"", I shall write $$ \det \left[SMCP(p,n,k)\right]=0 \qquad n\geqslant p+2, \;\forall k $$ I couldn't resist exploring the case just before the first singularity occurence, when the matrix order is greater than the power only by $1$. I mean, in the general case, because it is easy to algebraically verify that $$ \det \left[SMCP(1,2,k)\right]=-2 \qquad \forall k $$ $$ \det \left[SMCP(2,3,k)\right]=-216 \qquad \forall k $$ I made a few direct attempts to find that (probably $\forall k$, but I don't dare writing it) $$ \det \left[SMCP(3,4,k)\right]=5308416 $$ $$ \det \left[SMCP(4,5,k)\right]=7776 \cdot 10^{10} $$ I hoped to infer a rule from this sequence start, and try a proof at a later stage, but I cannot see anything. Nor I think that the kernel trick can be helpful at this. So the question now would be $$ \det \left[SMCP(p,n,k)\right]=\;? \qquad \forall k \;\textrm{  when  }\; n=p+1 $$ but I don't know how hard it can be to annswer it, and I won't ask anyone an eccessive effort about something that now has gone well beyond what I was looking for in the first place. I leave it here just in case someone is able to easily see something that doesn't occur to me. Again thank you!","This is my first question, please forgive me if I mistake something, since I don't think I will be allowed to edit the question later. So, let me explain the kind of matrices I'm talking about. Think of a $n\times n$ square matrix, and think of a sequence of $n^2$ consecutive squares starting at $k^2$ (the most natural choice being $k=1$). Now place those squares in the matrix as if you were writing, row by row. I'll dare to invent a notation for these ""Square Matrice filled with Consecutive Squares"": $SMCS(n,k)$. So we have, for example: $$ SMCS(3,4)=         \begin{bmatrix}         4^2 & 5^2 & 6^2 \\         7^2 & 8^2 & 9^2 \\         10^2 & 11^2 & 12^2 \\         \end{bmatrix}= \begin{bmatrix}         16 & 25 & 36 \\         49 & 64 & 81 \\         100 & 121 & 144 \\         \end{bmatrix} $$ I wasn't able to find anything about matrices like these neither here nor elsewhere. Probably there are of no mathematical interest. My interest starts from a lecture, years ago, when the professor made us notice that the ""Square Matrice filled with Consecutive Integers"" $1$ to $9$ —it is the cell phone keypad! I'll call it $SMCI(3,1)$— is singular. That is: $$ \det \begin{bmatrix}         1 & 2 & 3 \\         4 & 5 & 6 \\         7 & 8 & 9 \\         \end{bmatrix}=0 $$ I find very intuitive that this elegant property holds for matrices of higher order and even if starting from a different integer, that is  $$ \det \left[SMCI(n,k)\right]=0 \qquad n\geqslant 3, \;\forall k $$ I can recognize a clear pattern (where the middle culumns are an ""average"" of the ones at their sides) so that I feel relieved from the need to provide an explicit proof for this result, that shouldn't be too hard anyway. Now I'm looking at the determinants of the matrices filled with squares. With no surprise I find nonsingular matrices of order $2$. Then, with some hope, I look at $SMCS(3,1)$ (the cell phone keypad with each number squared) but I find that the determinant is $-216$. So I find myself admitting that, understandably, the property doesn't hold for matrices filled with powers. Yet something curious happens: the result is the same even if I take a different starting point, that is $$ \det\left[SMCS(3,k)\right]=-216\qquad \forall k $$ This was a surprise, but it was pretty straightforward proving it with some direct algebra calculations. Here comes the big deal. I went on looking at higher orders, wondering if there were a characteristic constant for each $n$. Instead... $0$'s began to appear again! Me unbeliever! I've grown convinced that $$ \det \left[SMCS(n,k)\right]=0 \qquad n\geqslant 4, \;\forall k $$ but attempting a direct proof, if only for $n=4$, was out of the question. Let alone a general proof for any $n$, which is what I would really be interested in, but I wouldn't know even where to start from. I don't really need the rigorous proof, what I'd love is to be sure of the validity of the property, and possibly a way to ""understand"" it in a manner similar to what I could do with the $SMCI$ matrices, to be ""convinced"" of the result. Can anyone help my with this? Thank you! Wow, great, I got it! It took me some effort, but I got it all, thank you JeanMarie! Forgive me for coming back with some delay, but I saw you constantly improving your answer, and considering my time zone disadvantage I had to give up for the day. Understanding your ""trick"" allows to easily extend the property to even higher orders; for example rows (or columns) of consecutive cubes give singular matrices from $5 \times 5$ on. If I extend the notation to ""Square Matrix filled with Consecutive Powers"", I shall write $$ \det \left[SMCP(p,n,k)\right]=0 \qquad n\geqslant p+2, \;\forall k $$ I couldn't resist exploring the case just before the first singularity occurence, when the matrix order is greater than the power only by $1$. I mean, in the general case, because it is easy to algebraically verify that $$ \det \left[SMCP(1,2,k)\right]=-2 \qquad \forall k $$ $$ \det \left[SMCP(2,3,k)\right]=-216 \qquad \forall k $$ I made a few direct attempts to find that (probably $\forall k$, but I don't dare writing it) $$ \det \left[SMCP(3,4,k)\right]=5308416 $$ $$ \det \left[SMCP(4,5,k)\right]=7776 \cdot 10^{10} $$ I hoped to infer a rule from this sequence start, and try a proof at a later stage, but I cannot see anything. Nor I think that the kernel trick can be helpful at this. So the question now would be $$ \det \left[SMCP(p,n,k)\right]=\;? \qquad \forall k \;\textrm{  when  }\; n=p+1 $$ but I don't know how hard it can be to annswer it, and I won't ask anyone an eccessive effort about something that now has gone well beyond what I was looking for in the first place. I leave it here just in case someone is able to easily see something that doesn't occur to me. Again thank you!",,"['linear-algebra', 'matrices', 'determinant', 'recreational-mathematics']"
15,$ n$-dimensional rotation matrix,-dimensional rotation matrix, n,"I want to find $n$ dimensional rotation matrix which corresponds rotation of an angle  $\theta$ around the  $(n−2)$-dimensional subspace. There is  the n-dimensional rotation matrix formula . (see equation $15$) $$I+(n_2n_1^T-n_1n_2^T)\sin(a)+(n_1n_1^T+n_2n_2^T)(\cos(a)-1)$$ where $n_1$ and $n_2$  are $n$-dimensional orthogonal unit vectors. Can anybody explain how can I use this formula,  for $n=6$?","I want to find $n$ dimensional rotation matrix which corresponds rotation of an angle  $\theta$ around the  $(n−2)$-dimensional subspace. There is  the n-dimensional rotation matrix formula . (see equation $15$) $$I+(n_2n_1^T-n_1n_2^T)\sin(a)+(n_1n_1^T+n_2n_2^T)(\cos(a)-1)$$ where $n_1$ and $n_2$  are $n$-dimensional orthogonal unit vectors. Can anybody explain how can I use this formula,  for $n=6$?",,"['linear-algebra', 'matrices']"
16,What's the relation between basis for a vector space and coordinate systems?,What's the relation between basis for a vector space and coordinate systems?,,I know what's a basis for some vector space $V$: a set of objects from that space that span the whole space. We can change between basis by using the change of basis matrix. Basically this matrix transforms a vector representation with respect to a basis to another representation with respect to a new basis. I'm now wondering what's the relation between a basis of a vector space and a coordinate system for that same vector space?,I know what's a basis for some vector space $V$: a set of objects from that space that span the whole space. We can change between basis by using the change of basis matrix. Basically this matrix transforms a vector representation with respect to a basis to another representation with respect to a new basis. I'm now wondering what's the relation between a basis of a vector space and a coordinate system for that same vector space?,,"['linear-algebra', 'vector-spaces']"
17,lower bound on norm of matrix vector product,lower bound on norm of matrix vector product,,"I'm wondering if the following inequality holds $ \sigma_{min}\|v\|_2 \leq \|Av\|_2$ , where $ \sigma_{min}$ is the smallest singular value of A. Furthermore, assuming that A is positive definite and $v \in \mathbb{R^n}$. Thank you :)","I'm wondering if the following inequality holds $ \sigma_{min}\|v\|_2 \leq \|Av\|_2$ , where $ \sigma_{min}$ is the smallest singular value of A. Furthermore, assuming that A is positive definite and $v \in \mathbb{R^n}$. Thank you :)",,"['linear-algebra', 'matrices', 'vectors', 'normed-spaces']"
18,How to represent matrix multiplication in tensor algebra?,How to represent matrix multiplication in tensor algebra?,,"How can we represent matrix multiplication in tensor algebra? Even if we assume all matrices represent contravariant tensors only, clearly matrix multiplication does not correspond to the multiplication operation of the tensor algebra (the tensor product), since the former is grade-preserving or grade-reducing, whereas the latter is always grade-increasing. And then if we allow matrices to represent either contravariant or covariant or mixed variance tensors, then things get even more confusing. For instance, a quadratic form then can be represented by the same matrix as the bilinear form it generates via polarization. Seemingly we must implicitly be using the universal property relating $V \otimes V$ (tensor product) and $V \times V$ (Cartesian product). But we can define the same type of (matrix) multiplication for $V \otimes V^*, V^* \otimes V,$ or $V^* \otimes V^*$ or between elements of any two. Thus now even the claim that matrices represent linear transformations and that matrix multiplication is the composition of linear maps seems suspect to me. Is this just a result of the fact that linear algebra was invented before multilinear algebra/tensor analysis, and thus people were abusing notation when using matrices without realizing it but then the convention stuck? Or is there something more to this which I am missing? Related but more abstract and slightly different question: How do we describe standard matrix multiplication using tensor products? Relevant wikipedia articles: https://en.wikipedia.org/wiki/Outer_product#Tensor_multiplication , https://en.wikipedia.org/wiki/Kronecker_product","How can we represent matrix multiplication in tensor algebra? Even if we assume all matrices represent contravariant tensors only, clearly matrix multiplication does not correspond to the multiplication operation of the tensor algebra (the tensor product), since the former is grade-preserving or grade-reducing, whereas the latter is always grade-increasing. And then if we allow matrices to represent either contravariant or covariant or mixed variance tensors, then things get even more confusing. For instance, a quadratic form then can be represented by the same matrix as the bilinear form it generates via polarization. Seemingly we must implicitly be using the universal property relating $V \otimes V$ (tensor product) and $V \times V$ (Cartesian product). But we can define the same type of (matrix) multiplication for $V \otimes V^*, V^* \otimes V,$ or $V^* \otimes V^*$ or between elements of any two. Thus now even the claim that matrices represent linear transformations and that matrix multiplication is the composition of linear maps seems suspect to me. Is this just a result of the fact that linear algebra was invented before multilinear algebra/tensor analysis, and thus people were abusing notation when using matrices without realizing it but then the convention stuck? Or is there something more to this which I am missing? Related but more abstract and slightly different question: How do we describe standard matrix multiplication using tensor products? Relevant wikipedia articles: https://en.wikipedia.org/wiki/Outer_product#Tensor_multiplication , https://en.wikipedia.org/wiki/Kronecker_product",,"['linear-algebra', 'tensor-products', 'tensors', 'multilinear-algebra']"
19,We have matrix $A\in M_{n-1\times n}(\mathbb Z)$ so that the sum of entries in each row is zero. Prove that $\det(AA^T)=nk^2.$,We have matrix  so that the sum of entries in each row is zero. Prove that,A\in M_{n-1\times n}(\mathbb Z) \det(AA^T)=nk^2.,"Problem: We have matrix $A\in M_{n-1\times n}(\mathbb Z)$ so that the sum of elements in each row is zero. Prove that $\det(AA^T)=nk^2$, where $k\in \mathbb Z$. What have I considered so far: First I thought, since sum of all elements in each row is zero, zero is eigenvalue of $A$ but $A\in M_{(n-1)\times n}(\mathbb Z)$ confused me. I see that $AA^T$ will be $(n-1)$ by $(n-1)$, so I tried calculating $AA^T$ but I failed to see any connection. I did some research and found this question but we had no mention of it, which makes me believe I am on the wrong track. Thank you all for your help.","Problem: We have matrix $A\in M_{n-1\times n}(\mathbb Z)$ so that the sum of elements in each row is zero. Prove that $\det(AA^T)=nk^2$, where $k\in \mathbb Z$. What have I considered so far: First I thought, since sum of all elements in each row is zero, zero is eigenvalue of $A$ but $A\in M_{(n-1)\times n}(\mathbb Z)$ confused me. I see that $AA^T$ will be $(n-1)$ by $(n-1)$, so I tried calculating $AA^T$ but I failed to see any connection. I did some research and found this question but we had no mention of it, which makes me believe I am on the wrong track. Thank you all for your help.",,"['linear-algebra', 'matrices']"
20,"If $\dim(V) $ is infinite, show that $V\oplus V$ is isomorphic to $V$","If  is infinite, show that  is isomorphic to",\dim(V)  V\oplus V V,"For a vector space $V$ of infinite dimension, to show that $V\oplus V$ is isomorphic to $V$ is to show that there exists an invertible linear transformation between $V \oplus V $ and $V$. Every vector space have a basis. If $B$ is the infinite set of basis for $V$, then the set $B\oplus0 \cup 0\oplus B$ is the basis for $V \oplus V$.  Using axiom of choice the cardinality $|B\oplus0 \cup 0\oplus B|= |B\oplus0|+|0 \oplus B|=|B|+ |B| = \max\{|B|,|B|\}= |B|$ and then I was trying to argue that this implies that there is an isomorphism between the bases of $V\oplus V$ and $V$, so they are isomorphic. But I'm not sure how $|B|+|B|=\max\{|B|,|B|\}= |B|$ is true by the axiom of choice. How could we show that if $V$ is infinite dimensional vector space, then $V\oplus V \cong V$?","For a vector space $V$ of infinite dimension, to show that $V\oplus V$ is isomorphic to $V$ is to show that there exists an invertible linear transformation between $V \oplus V $ and $V$. Every vector space have a basis. If $B$ is the infinite set of basis for $V$, then the set $B\oplus0 \cup 0\oplus B$ is the basis for $V \oplus V$.  Using axiom of choice the cardinality $|B\oplus0 \cup 0\oplus B|= |B\oplus0|+|0 \oplus B|=|B|+ |B| = \max\{|B|,|B|\}= |B|$ and then I was trying to argue that this implies that there is an isomorphism between the bases of $V\oplus V$ and $V$, so they are isomorphic. But I'm not sure how $|B|+|B|=\max\{|B|,|B|\}= |B|$ is true by the axiom of choice. How could we show that if $V$ is infinite dimensional vector space, then $V\oplus V \cong V$?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
21,Existence and uniqueness of the eigen decomposition of a square matrix,Existence and uniqueness of the eigen decomposition of a square matrix,,"I'm confused on the sufficient conditions for the existence and uniqueness of the eigen decomposition of a square matrix. Consider a matrix $A$ of dimension $m\times m$, a matrix $B$ of dimension $m\times m$ and a matrix $D$ diagonal of dimension $m\times m$. Assumption 1 : $B$ invertible Assumption 2 : The diagonal elements of $D$ are all distinct Assumption 3 : $A=BDB^{-1}$ where $B^{-1}$ exists by Assumption 1 Questions : (1) Does Assumption 3 mean that $BDB^{-1}$ is the eigen decomposition of $A$? In other words, does Assumption 3 is equivalent to say that the columns of $B$ are the eigenvectors of $A$ and the diagonal elements of $D$ are the eigenvalues of $A$? Or do we need other assumptions to state that? My doubt is that: if Assumption 3 means that the columns of $B$ are the eigenvectors of $A$ and the diagonal elements of $D$ are the eigenvalues of $A$, then, since $B$ is invertible, it should be that the eigenvectors of $A$ are linearly independent and, hence, that $A$ is invertible (which is not among my assumptions). (2) From what I have read in some sources, Assumptions 2 and 3 imply that the the eigen decomposition of $A$ is unique [up to a left multiplication of $B$ by a invertible diagonal matrix and up to an order for the eigenvalues]. What does ""unique"" exactly mean? My thought was that it means that there are no other matrices $E,F$ with $F$ diagonal such that $A=EFE^{-1}$? But if that is right, the  uniqueness would be necessary to guarantee that the columns of $B$ are the eigenvectors of $A$ and the diagonal elements of $D$ are the eigenvalues of $A$; in other words would be ""embedded"" in saying that $BDB^{-1}$ is the eigen decomposition of $A$. Could you clarify this point?","I'm confused on the sufficient conditions for the existence and uniqueness of the eigen decomposition of a square matrix. Consider a matrix $A$ of dimension $m\times m$, a matrix $B$ of dimension $m\times m$ and a matrix $D$ diagonal of dimension $m\times m$. Assumption 1 : $B$ invertible Assumption 2 : The diagonal elements of $D$ are all distinct Assumption 3 : $A=BDB^{-1}$ where $B^{-1}$ exists by Assumption 1 Questions : (1) Does Assumption 3 mean that $BDB^{-1}$ is the eigen decomposition of $A$? In other words, does Assumption 3 is equivalent to say that the columns of $B$ are the eigenvectors of $A$ and the diagonal elements of $D$ are the eigenvalues of $A$? Or do we need other assumptions to state that? My doubt is that: if Assumption 3 means that the columns of $B$ are the eigenvectors of $A$ and the diagonal elements of $D$ are the eigenvalues of $A$, then, since $B$ is invertible, it should be that the eigenvectors of $A$ are linearly independent and, hence, that $A$ is invertible (which is not among my assumptions). (2) From what I have read in some sources, Assumptions 2 and 3 imply that the the eigen decomposition of $A$ is unique [up to a left multiplication of $B$ by a invertible diagonal matrix and up to an order for the eigenvalues]. What does ""unique"" exactly mean? My thought was that it means that there are no other matrices $E,F$ with $F$ diagonal such that $A=EFE^{-1}$? But if that is right, the  uniqueness would be necessary to guarantee that the columns of $B$ are the eigenvectors of $A$ and the diagonal elements of $D$ are the eigenvalues of $A$; in other words would be ""embedded"" in saying that $BDB^{-1}$ is the eigen decomposition of $A$. Could you clarify this point?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
22,Positivity of the alternating sum associated to at most five subspaces,Positivity of the alternating sum associated to at most five subspaces,,"Let $V_1 , V_2 , \dots , V_n $ be vector subspaces of $ \mathbb{C}^m$ and let  $$\alpha = \sum_{r=1}^n (-1)^{r+1} \sum_{  \ i_1 < i_2 < \cdots < i_r } \dim(V_{i_1} \cap \cdots \cap V_{i_r})$$ For $n = 2$ we have the equality $ \alpha =  \dim(\sum_{i = 1}^{n} V_i) $; it's false for $n>2$, see this answer . For $n=3$, we have only the inequality $ \alpha \ge  \dim(\sum_{i = 1}^{n} V_i) $; it's false for $n>3$, see this post . For $n>5$, the inequality $\alpha  \ge 0$ is false in general, see the comment of Darij Grinberg below. Question : Is it true that $\alpha \ge 0$, in the case $n \le 5$? Remark : I think this question interesting for itself; it admits also applications in the interaction between representations theory and subgroups lattice.","Let $V_1 , V_2 , \dots , V_n $ be vector subspaces of $ \mathbb{C}^m$ and let  $$\alpha = \sum_{r=1}^n (-1)^{r+1} \sum_{  \ i_1 < i_2 < \cdots < i_r } \dim(V_{i_1} \cap \cdots \cap V_{i_r})$$ For $n = 2$ we have the equality $ \alpha =  \dim(\sum_{i = 1}^{n} V_i) $; it's false for $n>2$, see this answer . For $n=3$, we have only the inequality $ \alpha \ge  \dim(\sum_{i = 1}^{n} V_i) $; it's false for $n>3$, see this post . For $n>5$, the inequality $\alpha  \ge 0$ is false in general, see the comment of Darij Grinberg below. Question : Is it true that $\alpha \ge 0$, in the case $n \le 5$? Remark : I think this question interesting for itself; it admits also applications in the interaction between representations theory and subgroups lattice.",,"['linear-algebra', 'combinatorics', 'vector-spaces', 'homological-algebra', 'combinatorial-geometry']"
23,Is matrix-vector product a dot or cross product,Is matrix-vector product a dot or cross product,,"Going through linear algebra tutorials on khanacademy I've found that matrix-vector products are not defined clearly as dot or cross products. Am I missing something? Is matrix-vector product a dot or cross product? At first I thought it's a cross product, because result is a vector, not a scalar. But cross-product is not defined in R2, however, matrix-vector product is allowed in R2. A bit confused.","Going through linear algebra tutorials on khanacademy I've found that matrix-vector products are not defined clearly as dot or cross products. Am I missing something? Is matrix-vector product a dot or cross product? At first I thought it's a cross product, because result is a vector, not a scalar. But cross-product is not defined in R2, however, matrix-vector product is allowed in R2. A bit confused.",,"['linear-algebra', 'matrices', 'vectors']"
24,Image of unit sphere being hyper ellipse proof (SVD),Image of unit sphere being hyper ellipse proof (SVD),,"When I check for the proof of singular value decomposition, they all assume the following is true: The image of the unit sphere under any $m * n$ matrix is a hyper   ellipse. However I could not find a decent proof for this, even though I googled for hours. I keep seeing notes like: ""This geometric fact is not obvious. We shall restate it in the language of linear algebra and prove it later. For the moment, assume it is true."" Maybe I am using wrong keywords. Could you please give me a link, text book name, etc. (a reference) for this proof?","When I check for the proof of singular value decomposition, they all assume the following is true: The image of the unit sphere under any $m * n$ matrix is a hyper   ellipse. However I could not find a decent proof for this, even though I googled for hours. I keep seeing notes like: ""This geometric fact is not obvious. We shall restate it in the language of linear algebra and prove it later. For the moment, assume it is true."" Maybe I am using wrong keywords. Could you please give me a link, text book name, etc. (a reference) for this proof?",,"['linear-algebra', 'geometry', 'svd']"
25,For which matrices $A \in \mathscr{M}_n(\mathbb C)$ is the similarity class of $A$ closed?,For which matrices  is the similarity class of  closed?,A \in \mathscr{M}_n(\mathbb C) A,What are the matrices $A \in \mathscr{M}_n(\mathbb C)$ for which the similarity class is closed? What about the same question if we replace $\mathbb C$ by $\mathbb R$?,What are the matrices $A \in \mathscr{M}_n(\mathbb C)$ for which the similarity class is closed? What about the same question if we replace $\mathbb C$ by $\mathbb R$?,,"['linear-algebra', 'general-topology', 'matrices']"
26,"Show that the polynomial $p_0, p_1, \ldots, p_m $ are linearly dependent.",Show that the polynomial  are linearly dependent.,"p_0, p_1, \ldots, p_m ","Let $p_0, p_1, \ldots, p_m $ polynomials in $\mathbb{P}_m$ with the property that $p_j(1)=0\ \forall j$. Show that the polynomial $p_0, p_1, \ldots,p_m $ are linearly dependent. My approach for this problem is the following. Consider $$ \sum_{i=0}^m\alpha_ip_i(x)=0\ \ \forall x. \tag{1} $$ Put $x=1 $ in $(1)$ $ \implies \sum_{i=0}^m\alpha_ip_i(1)=0 \implies \alpha_i  $ can be nonzero also. Hence the given set of polynomial is linearly dependent.","Let $p_0, p_1, \ldots, p_m $ polynomials in $\mathbb{P}_m$ with the property that $p_j(1)=0\ \forall j$. Show that the polynomial $p_0, p_1, \ldots,p_m $ are linearly dependent. My approach for this problem is the following. Consider $$ \sum_{i=0}^m\alpha_ip_i(x)=0\ \ \forall x. \tag{1} $$ Put $x=1 $ in $(1)$ $ \implies \sum_{i=0}^m\alpha_ip_i(1)=0 \implies \alpha_i  $ can be nonzero also. Hence the given set of polynomial is linearly dependent.",,"['linear-algebra', 'proof-verification']"
27,Computationally efficient form to evaluate multivariate polynomials?,Computationally efficient form to evaluate multivariate polynomials?,,"It is well known the Horner's method to transform a univariate polynomial into a computationally efficient form to evaluate it. Instead of $\sum_{i=0}^na_ix^i$ you compute $(((a_nx+a_{n-1})x)+a_{n-2})\dots$ and the number of operations needed to evaluate the polynomial is reduced. Particularly, there are less multiplications (which are more expensive than sums) My question is: Is there any method, algorithm to transform a multivariate polynomial into a computationally efficient form to evaluate it?  Let's define the ""computationally efficient form"" as the one which requires the minimum multiplications","It is well known the Horner's method to transform a univariate polynomial into a computationally efficient form to evaluate it. Instead of $\sum_{i=0}^na_ix^i$ you compute $(((a_nx+a_{n-1})x)+a_{n-2})\dots$ and the number of operations needed to evaluate the polynomial is reduced. Particularly, there are less multiplications (which are more expensive than sums) My question is: Is there any method, algorithm to transform a multivariate polynomial into a computationally efficient form to evaluate it?  Let's define the ""computationally efficient form"" as the one which requires the minimum multiplications",,"['linear-algebra', 'algebra-precalculus', 'algorithms']"
28,"For any unitary matrix $U$, there exists a Hermitian matrix $H$ such that $U=e^{iH}$","For any unitary matrix , there exists a Hermitian matrix  such that",U H U=e^{iH},"For any unitary matrix $U$ , there exists a Hermitian matrix $H$ such that $U=e^{iH}$ . Is the above statement true? (I guess not) If not, how ""many"" unitary matrixes  can the expression $U=e^{iH}$ cover?","For any unitary matrix , there exists a Hermitian matrix such that . Is the above statement true? (I guess not) If not, how ""many"" unitary matrixes  can the expression cover?",U H U=e^{iH} U=e^{iH},"['linear-algebra', 'matrices', 'matrix-exponential', 'unitary-matrices']"
29,Invertible matrix over a ring and its eigenvalues,Invertible matrix over a ring and its eigenvalues,,"Eigenvalues and invertible matrices for fields and vector spaces: Let $K$ be a field (so $K^n$ is a $K$-vector space) and let $A \in K^{n\times n}$ be an $n\times n$-matrix. Then we have the following definition and theorem: Definition: An element $\lambda\in K$ is called an eigenvalue of $A$ iff there is a $v\in K^n\setminus\{0\}$ such that $Av=\lambda v$. Theorem: The matrix $A$ is invertible iff all of its eigenvalues are invertible (i. e. 0 isn't an eigenvalue). ""Eigenvalues"" and invertible matrices for rings and modules: Let $R$ be a commutative ring with a multiplicative identity (so $R^n$ is an $R$-module) and let $B\in R^{n\times n}$ be an $n\times n$-matrix. Then let's define what an eigenvalue of $B$ is: Definition: An element $\mu\in R$ is called an eigenvalue of $B$ iff there is a $v\in R^n\setminus\{0\}$ such that $Bv=\mu v$. Clearly, it is possible for an $R^{n\times n}$-matrix to have only invertible eigenvalues and not to be invertible itself. Take for example $\begin{pmatrix} 3 & -13 \\ 1 & -3\end{pmatrix}\in \mathbb{Z}^{2\times 2}$. All of its eigenvalues are invertible (simply because it doesn't have any eigenvalues in $\mathbb{Z}$); however the matrix is not invertible (because its determinant is 4 which is not a unit in $\mathbb{Z}$). So the following is for sure false: Not a theorem! The matrix $B$ is invertible if all of its eigenvalues are invertible (i. e. are units in $R$). Now, I wonder if the following is true: Theorem? If the matrix $B$ is invertible , then all of its eigenvalues are invertible (i. e. are units in $R$). If it is true, how can it be proved? If it is false, what would be a counter-example?","Eigenvalues and invertible matrices for fields and vector spaces: Let $K$ be a field (so $K^n$ is a $K$-vector space) and let $A \in K^{n\times n}$ be an $n\times n$-matrix. Then we have the following definition and theorem: Definition: An element $\lambda\in K$ is called an eigenvalue of $A$ iff there is a $v\in K^n\setminus\{0\}$ such that $Av=\lambda v$. Theorem: The matrix $A$ is invertible iff all of its eigenvalues are invertible (i. e. 0 isn't an eigenvalue). ""Eigenvalues"" and invertible matrices for rings and modules: Let $R$ be a commutative ring with a multiplicative identity (so $R^n$ is an $R$-module) and let $B\in R^{n\times n}$ be an $n\times n$-matrix. Then let's define what an eigenvalue of $B$ is: Definition: An element $\mu\in R$ is called an eigenvalue of $B$ iff there is a $v\in R^n\setminus\{0\}$ such that $Bv=\mu v$. Clearly, it is possible for an $R^{n\times n}$-matrix to have only invertible eigenvalues and not to be invertible itself. Take for example $\begin{pmatrix} 3 & -13 \\ 1 & -3\end{pmatrix}\in \mathbb{Z}^{2\times 2}$. All of its eigenvalues are invertible (simply because it doesn't have any eigenvalues in $\mathbb{Z}$); however the matrix is not invertible (because its determinant is 4 which is not a unit in $\mathbb{Z}$). So the following is for sure false: Not a theorem! The matrix $B$ is invertible if all of its eigenvalues are invertible (i. e. are units in $R$). Now, I wonder if the following is true: Theorem? If the matrix $B$ is invertible , then all of its eigenvalues are invertible (i. e. are units in $R$). If it is true, how can it be proved? If it is false, what would be a counter-example?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'modules', 'inverse']"
30,Plücker Relation: misunderstanding?,Plücker Relation: misunderstanding?,,"I'm trying to understand exterior algebra better by gaining some ""bare hands"" understanding of the exterior powers $\Lambda^k(X)$ in more detail when $\dim(X)$ is small. I think so far I understand the cases $\dim(X) = 1,2,3$ quite well. The next case $\dim(X) =4$ is giving me more trouble. So, let us take $X = \mathbb{R}^4$. We have, as usual $\Lambda^1(\mathbb{R}^4) = \mathbb{R}$ and $\Lambda^1(\mathbb{R}^4) = \mathbb{R}^4$. The exterior square $\Lambda^2(\mathbb{R}^4)$ is more interesting because $4$ is the smallest dimension such that there exist elements in $\Lambda^2(\mathbb{R}^4)$ besides those of the form $u \wedge v$ for $u,v \in \mathbb{R}^4$. Notably, there are the ""symplectic elements"" like $e_1 \wedge e_2 + e_3 \wedge e_4$. I can see that they cannot be obtained as wedges of two vectors. Also, I think I can see that they are all congugate under the action of $GL(4)$ on $\Lambda^2(\mathbb{R}^4)$. Initial Question : It seems to me that there are precisely two types of elements in $\Lambda^2(\mathbb{R}^2)$, each of which constitutes an orbit of the $GL(4)$-action. The ""symplectic elements"" and the ones which are wedges of two vectors. Is this correct? Or is there some intermediate type of element I am missing? In trying to confirm the answer to the above question was ""yes"" I spend some time browsing through wikipedia articles like these ones on Plücker coordinates. A lot of what is there is superfluous to my needs, but I think I correctly understood that there is a mapping $$ \{ \text{planes in }\mathbb{R}^4 \} \longrightarrow \{ \text{lines in }\Lambda^2(\mathbb{R}^4)\}$$ which sends $$ \mathrm{span}\{u,v\} \to \mathrm{span}( u \wedge v).$$ In terms of coordinates, this map sends the column space of a rank $2$ matrix  $$ A =  \begin{bmatrix} u_1 & v_1 \\ u_2 & v_2 \\ u_3 & v_3 \\ u_4 & v_4 \\ \end{bmatrix} $$ to  $$ X_{12} e_1 \wedge e_2 + X_{13} e_1 \wedge e_3  + X_{14} e_1 \wedge e_4 + X_{23} e_2 \wedge e_3  + X_{24} e_2 \wedge e_4  + X_{34} e_3 \wedge e_4 $$ where $X_{ij}$ is the determinant of the $2 \times 2$ submatrix of $A$ consisting of Rows $i$ and $j$. It's not too hard to see this map is well defined. It suffices to check performing elementary operations on the matrix can only scale the Plücker coordinates. Indeed: Switching the  columns of $A$ changes the sign of each $X_{ij}$. Adding a multiple of one column of $A$ to the other leaves each $X_{ij}$ invariant. Scaling a column of $A$ results in scaling each $X_{ij}$ by the same amount. Now from this section , I gathered that you are supposed to be able to detect precisely which elements of $\Lambda^2(\mathbb{R}^4)$ are wedges of two vectors by checking whether they satisfy the Plücker Relation: $$X_{12}X_{34} − X_{13}X_{24}+ X_{23}X_{14} = 0$$ So, what I tried to do next was verify directly that, given an element $w =  \sum_{i < j} X_{ij} e_i \wedge e_j \in \Lambda^2(\mathbb{R}^4)$: If $w$ is a wedge of two vectors, then $w$  satisfies the Plücker Relation. If $w$ does not satisfy the Plücker Relation, then $w$ is a ""symplectic element"". However, I was confused to discover that (1) does not seem to hold Main Question: Am I going crazy? Or does the Plücker relation   $$X_{12}X_{34} − X_{13}X_{24} + X_{23}X_{14} = 0$$ not generally hold where   $$ X_{ij} =  \det \begin{bmatrix} u_i & v_i \\ u_j & v_j \\  \end{bmatrix}$$   and $u,v \in \mathbb{R^4}$ are linearly independent? I have computed this about 4 times now. Possibly I am making some mistake, but at this point I think it is more likely that I am not understanding what the Plücker relations are supposed to do properly. Sorry for the long question. I could have been more concise, but I also wanted to record my thought process so that I could recall it later.","I'm trying to understand exterior algebra better by gaining some ""bare hands"" understanding of the exterior powers $\Lambda^k(X)$ in more detail when $\dim(X)$ is small. I think so far I understand the cases $\dim(X) = 1,2,3$ quite well. The next case $\dim(X) =4$ is giving me more trouble. So, let us take $X = \mathbb{R}^4$. We have, as usual $\Lambda^1(\mathbb{R}^4) = \mathbb{R}$ and $\Lambda^1(\mathbb{R}^4) = \mathbb{R}^4$. The exterior square $\Lambda^2(\mathbb{R}^4)$ is more interesting because $4$ is the smallest dimension such that there exist elements in $\Lambda^2(\mathbb{R}^4)$ besides those of the form $u \wedge v$ for $u,v \in \mathbb{R}^4$. Notably, there are the ""symplectic elements"" like $e_1 \wedge e_2 + e_3 \wedge e_4$. I can see that they cannot be obtained as wedges of two vectors. Also, I think I can see that they are all congugate under the action of $GL(4)$ on $\Lambda^2(\mathbb{R}^4)$. Initial Question : It seems to me that there are precisely two types of elements in $\Lambda^2(\mathbb{R}^2)$, each of which constitutes an orbit of the $GL(4)$-action. The ""symplectic elements"" and the ones which are wedges of two vectors. Is this correct? Or is there some intermediate type of element I am missing? In trying to confirm the answer to the above question was ""yes"" I spend some time browsing through wikipedia articles like these ones on Plücker coordinates. A lot of what is there is superfluous to my needs, but I think I correctly understood that there is a mapping $$ \{ \text{planes in }\mathbb{R}^4 \} \longrightarrow \{ \text{lines in }\Lambda^2(\mathbb{R}^4)\}$$ which sends $$ \mathrm{span}\{u,v\} \to \mathrm{span}( u \wedge v).$$ In terms of coordinates, this map sends the column space of a rank $2$ matrix  $$ A =  \begin{bmatrix} u_1 & v_1 \\ u_2 & v_2 \\ u_3 & v_3 \\ u_4 & v_4 \\ \end{bmatrix} $$ to  $$ X_{12} e_1 \wedge e_2 + X_{13} e_1 \wedge e_3  + X_{14} e_1 \wedge e_4 + X_{23} e_2 \wedge e_3  + X_{24} e_2 \wedge e_4  + X_{34} e_3 \wedge e_4 $$ where $X_{ij}$ is the determinant of the $2 \times 2$ submatrix of $A$ consisting of Rows $i$ and $j$. It's not too hard to see this map is well defined. It suffices to check performing elementary operations on the matrix can only scale the Plücker coordinates. Indeed: Switching the  columns of $A$ changes the sign of each $X_{ij}$. Adding a multiple of one column of $A$ to the other leaves each $X_{ij}$ invariant. Scaling a column of $A$ results in scaling each $X_{ij}$ by the same amount. Now from this section , I gathered that you are supposed to be able to detect precisely which elements of $\Lambda^2(\mathbb{R}^4)$ are wedges of two vectors by checking whether they satisfy the Plücker Relation: $$X_{12}X_{34} − X_{13}X_{24}+ X_{23}X_{14} = 0$$ So, what I tried to do next was verify directly that, given an element $w =  \sum_{i < j} X_{ij} e_i \wedge e_j \in \Lambda^2(\mathbb{R}^4)$: If $w$ is a wedge of two vectors, then $w$  satisfies the Plücker Relation. If $w$ does not satisfy the Plücker Relation, then $w$ is a ""symplectic element"". However, I was confused to discover that (1) does not seem to hold Main Question: Am I going crazy? Or does the Plücker relation   $$X_{12}X_{34} − X_{13}X_{24} + X_{23}X_{14} = 0$$ not generally hold where   $$ X_{ij} =  \det \begin{bmatrix} u_i & v_i \\ u_j & v_j \\  \end{bmatrix}$$   and $u,v \in \mathbb{R^4}$ are linearly independent? I have computed this about 4 times now. Possibly I am making some mistake, but at this point I think it is more likely that I am not understanding what the Plücker relations are supposed to do properly. Sorry for the long question. I could have been more concise, but I also wanted to record my thought process so that I could recall it later.",,"['linear-algebra', 'abstract-algebra', 'determinant', 'multilinear-algebra', 'exterior-algebra']"
31,"Is it true that $A \geq B$ implies $\|A\|_2 \geq \|B\|_2$ for $A,B \geq 0$?",Is it true that  implies  for ?,"A \geq B \|A\|_2 \geq \|B\|_2 A,B \geq 0","All matrices are real and not necessarily symmetric. Denote by $A \geq B$ the condition that $(A-B)$ has eigenvalues with non-negative real parts. Denote by $\| \cdot \|_2$ the $L_2$ matrix norm. Is it true that $A \geq B$ implies $\|A\|_2 \geq \|B\|_2$ for $A,B \geq 0$? Edit: Now I see it doesn't hold in general, I would also be grateful if someone could provide additional conditions on $A$ and $B$ for which $\|A\|_2 \geq \|B\|_2$ would hold. I am particularly interested in whether the statement can still be rescued the following cases: $A,B$ symmetric. $A = \Xi$, $B = \Xi P$ where $P$ is a stochastic matrix (rows sum to 1) and $\Xi$ a diagonal matrix with the principal left eigenvector of $P$ (i.e. stationary distribution) on the diagonal.","All matrices are real and not necessarily symmetric. Denote by $A \geq B$ the condition that $(A-B)$ has eigenvalues with non-negative real parts. Denote by $\| \cdot \|_2$ the $L_2$ matrix norm. Is it true that $A \geq B$ implies $\|A\|_2 \geq \|B\|_2$ for $A,B \geq 0$? Edit: Now I see it doesn't hold in general, I would also be grateful if someone could provide additional conditions on $A$ and $B$ for which $\|A\|_2 \geq \|B\|_2$ would hold. I am particularly interested in whether the statement can still be rescued the following cases: $A,B$ symmetric. $A = \Xi$, $B = \Xi P$ where $P$ is a stochastic matrix (rows sum to 1) and $\Xi$ a diagonal matrix with the principal left eigenvector of $P$ (i.e. stationary distribution) on the diagonal.",,"['linear-algebra', 'matrices', 'convex-analysis', 'normed-spaces']"
32,The inverse of $(I-A)$ and the spectral radius of a nonnegative $A$ matrix,The inverse of  and the spectral radius of a nonnegative  matrix,(I-A) A,"Suppost that $A$ is a nonnegative matrix , and let denote the identitiy matrix with $I$ and the spectral radius of $A$ with $\rho(A)$. Note that because $A$ is nonnegative according to the Perron–Frobenius theorem $\rho(A) = \lambda_\max(A)$. Statement. The following two are equivalent $I-A$ matrix is invertible and $(I-A)^{-1}$ is a nonnegative matrix $\lambda_\max(A) < 1$. Note that because of the statement we also know that the Neumann series of $A$ convergent. Question. How could we prove this statement?","Suppost that $A$ is a nonnegative matrix , and let denote the identitiy matrix with $I$ and the spectral radius of $A$ with $\rho(A)$. Note that because $A$ is nonnegative according to the Perron–Frobenius theorem $\rho(A) = \lambda_\max(A)$. Statement. The following two are equivalent $I-A$ matrix is invertible and $(I-A)^{-1}$ is a nonnegative matrix $\lambda_\max(A) < 1$. Note that because of the statement we also know that the Neumann series of $A$ convergent. Question. How could we prove this statement?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse']"
33,Equation for a plane perpendicular to a line through two given points [duplicate],Equation for a plane perpendicular to a line through two given points [duplicate],,"This question already has answers here : Equation of a plane containing a point and perpendicular to a line (2 answers) Closed 7 years ago . The following type of question is quite popular with examiners at the institution where I study. Find an equation of the plane containing the point $(0, 1, 1)$ and perpendicular to the line passing through the points $(2, 1, 0)$ and $(1, -1, 0)$ I start by calculating the the parametric equation of the line passing through $(2, 1, 0)$ and $(1, -1, 0)$, but after that I am lost. I realize that I have to find the equation for the second line, and that the first plane will be perpendicular to it if the dot product of the vectors equal zero, but I cannot seem to put the pieces together. Can someone please guide me in the correct direction for solving this type of problem? Much appreciated.","This question already has answers here : Equation of a plane containing a point and perpendicular to a line (2 answers) Closed 7 years ago . The following type of question is quite popular with examiners at the institution where I study. Find an equation of the plane containing the point $(0, 1, 1)$ and perpendicular to the line passing through the points $(2, 1, 0)$ and $(1, -1, 0)$ I start by calculating the the parametric equation of the line passing through $(2, 1, 0)$ and $(1, -1, 0)$, but after that I am lost. I realize that I have to find the equation for the second line, and that the first plane will be perpendicular to it if the dot product of the vectors equal zero, but I cannot seem to put the pieces together. Can someone please guide me in the correct direction for solving this type of problem? Much appreciated.",,"['linear-algebra', 'geometry', 'vectors']"
34,Sum of squares of maximal minors of a rectangular matrix with orthonormal rows,Sum of squares of maximal minors of a rectangular matrix with orthonormal rows,,"A matrix $A$ has $m$ rows and $n$ columns, such that $m \leq n$. We know that each row of $A$ has norm $1$ (the norm of an element $x=(x_1,x_2,...,x_n) \in \mathbb{R}^n$ is $||x||=\sqrt{x_1^2+x_2^2+...+x_n^2}$) and any two rows are  orthogonal (if $y=(y_1,y_2,...,y_n) \in \mathbb{R}^n$ then $x$ and $y$ are orthogonal if their dot product is $0$, i.e. $x_1y_1+x_2y_2+\cdots+x_ny_n=0$. Is the sum of the squares of the minors of order $m$ of $A$ equal $1$? I have already asked this question before, but in a cheeky way, so nobody gave it a try. I think that this is true. What I did, was to find out that $A^tA=I_n$ and I know that the eigenvalues in case $m=n$ have absolute value 1 (they are not necessarily real). Observation: The matrix is not a square matrix. We have $A^tA=I_n$ but $^tAA=I_n$ is not equivalent to the first (if this relation holds, then $n\leq m$ which is not given). Any ideas?","A matrix $A$ has $m$ rows and $n$ columns, such that $m \leq n$. We know that each row of $A$ has norm $1$ (the norm of an element $x=(x_1,x_2,...,x_n) \in \mathbb{R}^n$ is $||x||=\sqrt{x_1^2+x_2^2+...+x_n^2}$) and any two rows are  orthogonal (if $y=(y_1,y_2,...,y_n) \in \mathbb{R}^n$ then $x$ and $y$ are orthogonal if their dot product is $0$, i.e. $x_1y_1+x_2y_2+\cdots+x_ny_n=0$. Is the sum of the squares of the minors of order $m$ of $A$ equal $1$? I have already asked this question before, but in a cheeky way, so nobody gave it a try. I think that this is true. What I did, was to find out that $A^tA=I_n$ and I know that the eigenvalues in case $m=n$ have absolute value 1 (they are not necessarily real). Observation: The matrix is not a square matrix. We have $A^tA=I_n$ but $^tAA=I_n$ is not equivalent to the first (if this relation holds, then $n\leq m$ which is not given). Any ideas?",,"['linear-algebra', 'matrices']"
35,Relationship between the singular value decomposition (SVD) and the principal component analysis (PCA). A radical result(?),Relationship between the singular value decomposition (SVD) and the principal component analysis (PCA). A radical result(?),,"I was wondering if I could get a mathematical description of the relationship between the singular value decomposition (SVD) and the principal component analysis (PCA). To be more specific I have some point which I don't understand very well, at least from a mathematical point of view. What are the principal components (PCs), bearing in mind that we are using the SVD to compute the PCA? Which part of the SVD becomes the PCs? What is the relationship between the orthogonal matrices from the SVD plus the diagonal matrix with the scores and loadings of the PCA? I have read that the principal components can describe big data sets with very few loadings vectors, how is this (mathematically) possible? In which way can these few principal components tell me something about the variance of a big data set and what does the SVD has to do with this process? I have tried to be very specific making my questions, if something is not so clear I apologize. Thanks in advance for your help! PS I have made my homework and look very close to : What is the intuitive relationship between SVD and PCA? but I could not fine the answers I am looking for. I believe mine are more related to the mathematical concepts than the practical ones. Anyways, if you believe this questions is unnecessary (duplicate) I will remove it.","I was wondering if I could get a mathematical description of the relationship between the singular value decomposition (SVD) and the principal component analysis (PCA). To be more specific I have some point which I don't understand very well, at least from a mathematical point of view. What are the principal components (PCs), bearing in mind that we are using the SVD to compute the PCA? Which part of the SVD becomes the PCs? What is the relationship between the orthogonal matrices from the SVD plus the diagonal matrix with the scores and loadings of the PCA? I have read that the principal components can describe big data sets with very few loadings vectors, how is this (mathematically) possible? In which way can these few principal components tell me something about the variance of a big data set and what does the SVD has to do with this process? I have tried to be very specific making my questions, if something is not so clear I apologize. Thanks in advance for your help! PS I have made my homework and look very close to : What is the intuitive relationship between SVD and PCA? but I could not fine the answers I am looking for. I believe mine are more related to the mathematical concepts than the practical ones. Anyways, if you believe this questions is unnecessary (duplicate) I will remove it.",,"['linear-algebra', 'matrices', 'statistics', 'svd', 'principal-component-analysis']"
36,How to find determinant of this matrix?,How to find determinant of this matrix?,,"Is there a manual method to find  $\det\left(XY^{-1}\right)$ ? Let $$X=\left[ {\begin{array}{cc} 1 & 2 & 2^2 & \cdots & 2^{2012} \\  1 & 3 & 3^2 & \cdots & 3^{2012} \\ 1 & 4 & 4^2 & \cdots & 4^{2012} \\   \vdots  & \vdots  & \vdots  & \cdots & \vdots  \\  1 & 2014 & 2014^2 & \cdots & 2014^{2012} \\ \end{array} } \right], $$ $$Y=\left[ {\begin{array}{cc}\frac{2^2}{4} & \frac{3^2}{5} & \dfrac{4^2}{6} & \cdots & \dfrac{2014^2}{2016} \\  2 & 3 & 4 & \cdots & 2014 \\ 2^2 & 3^2 & 4^2 & \cdots & 2014^{2} \\   \vdots  & \vdots  & \vdots  & \cdots & \vdots  \\  2^{2012} & 3^{2012} & 4^{2012} & \cdots & 2014^{2012} \\ \end{array} } \right] $$. Thanks in advance.","Is there a manual method to find  $\det\left(XY^{-1}\right)$ ? Let $$X=\left[ {\begin{array}{cc} 1 & 2 & 2^2 & \cdots & 2^{2012} \\  1 & 3 & 3^2 & \cdots & 3^{2012} \\ 1 & 4 & 4^2 & \cdots & 4^{2012} \\   \vdots  & \vdots  & \vdots  & \cdots & \vdots  \\  1 & 2014 & 2014^2 & \cdots & 2014^{2012} \\ \end{array} } \right], $$ $$Y=\left[ {\begin{array}{cc}\frac{2^2}{4} & \frac{3^2}{5} & \dfrac{4^2}{6} & \cdots & \dfrac{2014^2}{2016} \\  2 & 3 & 4 & \cdots & 2014 \\ 2^2 & 3^2 & 4^2 & \cdots & 2014^{2} \\   \vdots  & \vdots  & \vdots  & \cdots & \vdots  \\  2^{2012} & 3^{2012} & 4^{2012} & \cdots & 2014^{2012} \\ \end{array} } \right] $$. Thanks in advance.",,"['linear-algebra', 'matrices', 'determinant']"
37,What can be computed by axiomatic summation?,What can be computed by axiomatic summation?,,"Here are three simple properties one might require of a summation method for divergent series: A stable summation scheme is one in which (assuming also each sums are defined iff the other is) $$\sum_{n=1}^\infty a_n = a_1 + \sum_{n=1}^\infty a_{n+1}$$ A linear one satisfies (whenever the RHS is defined) $$\sum_{n=1}^\infty (\kappa a_n+b_n) = \kappa\sum_{n=1}^\infty a_n + \sum_{n=1}^\infty b_n$$( Edit: Added $\kappa$, that was forgetfulness.) A regular one agrees with normal summation when both are defined. (That is, if one has a convergent series, it is given its normal value by the summation method.) If one assumes these rules, what class of series may be computed? It sounds like it should be the set of series which differ by a convergent series from another which is periodic up to rescaling; i.e. $a_n = c_n + k^n p_n$ where $\sum c_n$ is convergent and $p_{n+m} = p_n$ for some $m$. Is that correct? ( Edit: Clearly this guess should have been $c_n + \sum_{i=1}^M k_{(i)}^n p_{(i) n}$, thanks Karene. Further, I was already aware of the issue whereby stability gives contradictions to e.g. $1+1+\cdots$ so my conjecture should also have reflected that.) Given a specific, arbitrary series not of this form, can one actually/necessarily either (a) find two stable, linear, regular summation schemes for which the sum is defined, but for which we get distinct results; or (b) prove that no such scheme can define a value for the sum? (Motivation: pure curiosity.)","Here are three simple properties one might require of a summation method for divergent series: A stable summation scheme is one in which (assuming also each sums are defined iff the other is) $$\sum_{n=1}^\infty a_n = a_1 + \sum_{n=1}^\infty a_{n+1}$$ A linear one satisfies (whenever the RHS is defined) $$\sum_{n=1}^\infty (\kappa a_n+b_n) = \kappa\sum_{n=1}^\infty a_n + \sum_{n=1}^\infty b_n$$( Edit: Added $\kappa$, that was forgetfulness.) A regular one agrees with normal summation when both are defined. (That is, if one has a convergent series, it is given its normal value by the summation method.) If one assumes these rules, what class of series may be computed? It sounds like it should be the set of series which differ by a convergent series from another which is periodic up to rescaling; i.e. $a_n = c_n + k^n p_n$ where $\sum c_n$ is convergent and $p_{n+m} = p_n$ for some $m$. Is that correct? ( Edit: Clearly this guess should have been $c_n + \sum_{i=1}^M k_{(i)}^n p_{(i) n}$, thanks Karene. Further, I was already aware of the issue whereby stability gives contradictions to e.g. $1+1+\cdots$ so my conjecture should also have reflected that.) Given a specific, arbitrary series not of this form, can one actually/necessarily either (a) find two stable, linear, regular summation schemes for which the sum is defined, but for which we get distinct results; or (b) prove that no such scheme can define a value for the sum? (Motivation: pure curiosity.)",,"['linear-algebra', 'sequences-and-series', 'summation', 'divergent-series']"
38,Matrix with a parameter determinant,Matrix with a parameter determinant,,"$A \in \mathbb R^{n,n}$ $A_n=\begin{bmatrix} 1&2&3&\dots&n\\x&1&2&\dots&n-1\\x&x&1&\dots&n-2\\\vdots&\vdots&\vdots&\ddots&\vdots\\x&x&x&\dots&1\end{bmatrix}$ How to calculate $\det A_n$ for $x\in\mathbb R$?","$A \in \mathbb R^{n,n}$ $A_n=\begin{bmatrix} 1&2&3&\dots&n\\x&1&2&\dots&n-1\\x&x&1&\dots&n-2\\\vdots&\vdots&\vdots&\ddots&\vdots\\x&x&x&\dots&1\end{bmatrix}$ How to calculate $\det A_n$ for $x\in\mathbb R$?",,"['linear-algebra', 'matrices']"
39,determinant calculation,determinant calculation,,"This question is in my assignment. We are not allowed to use any symbol to represent any elementary row and column operations used in the solution. We must solve it step-by-step. Please help me to check my solution word by word including my spelling and grammar. Question: Given that $$\begin{vmatrix}a& b& c\\ d& e& f\\ g& h& i\end{vmatrix}=2$$ find $$\begin{vmatrix}3c-6a& 3b& a\\ 3i-9c+18a-6g& 3h-9b& g-3a\\ 3f-6d& 3e& d\end{vmatrix}.$$ Solution: We interchange the second and third rows of the matrix $\begin{pmatrix}a& b& c\\ d& e& f\\ g& h& i\end{pmatrix}$ to get the matrix $\begin{pmatrix}a& b& c\\ g& h& i\\ d& e& f\end{pmatrix}$ and we have $$\begin{vmatrix}a& b& c\\ g& h& i\\ d& e& f\end{vmatrix}=-\begin{vmatrix}a& b& c\\ d& e& f\\ g& h& i\end{vmatrix}=-2.$$ We interchange the first and third columns of the matrix $\begin{pmatrix}a& b& c\\ g& h& i\\ d& e& f\end{pmatrix}$ to get the matrix $\begin{pmatrix}c& b& a\\ i& h& g\\ f& e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c& b& a\\ i& h& g\\ f& e& d\end{vmatrix}=-\begin{vmatrix}a& b& c\\ g& h& i\\ d& e& f\end{vmatrix}=-(-2)=2.$$ We multiply the second column of the matrix $\begin{pmatrix}c& b& a\\ i& h& g\\ f& e& d\end{pmatrix}$ by 3 to get the matrix $\begin{pmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{vmatrix}=3\begin{vmatrix}c& b& a\\ i& h& g\\ f& e& d\end{vmatrix}=(3)(2)=6.$$ We add $(-2)$ times the third column of the matrix $\begin{pmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{pmatrix}$ to its first column to get the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{vmatrix}=\begin{vmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{vmatrix}=6.$$ We add $(-3)$ times the first row of the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{pmatrix}$ to its second row to get the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{vmatrix}=\begin{vmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{vmatrix}=6.$$ Finally, we multiply the first column of the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{pmatrix}$ by 3 to get the matrix $\begin{pmatrix}3c-6a& 3b& a\\ 3i-9c+18a-6g& 3h-9b& g-3a\\ 3f-6d& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}3c-6a& 3b& a\\ 3i-9c+18a-6g& 3h-9b& g-3a\\ 3f-6d& 3e& d\end{vmatrix}=3\begin{vmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{vmatrix}=(3)(6)=18.$$ Thank you.","This question is in my assignment. We are not allowed to use any symbol to represent any elementary row and column operations used in the solution. We must solve it step-by-step. Please help me to check my solution word by word including my spelling and grammar. Question: Given that $$\begin{vmatrix}a& b& c\\ d& e& f\\ g& h& i\end{vmatrix}=2$$ find $$\begin{vmatrix}3c-6a& 3b& a\\ 3i-9c+18a-6g& 3h-9b& g-3a\\ 3f-6d& 3e& d\end{vmatrix}.$$ Solution: We interchange the second and third rows of the matrix $\begin{pmatrix}a& b& c\\ d& e& f\\ g& h& i\end{pmatrix}$ to get the matrix $\begin{pmatrix}a& b& c\\ g& h& i\\ d& e& f\end{pmatrix}$ and we have $$\begin{vmatrix}a& b& c\\ g& h& i\\ d& e& f\end{vmatrix}=-\begin{vmatrix}a& b& c\\ d& e& f\\ g& h& i\end{vmatrix}=-2.$$ We interchange the first and third columns of the matrix $\begin{pmatrix}a& b& c\\ g& h& i\\ d& e& f\end{pmatrix}$ to get the matrix $\begin{pmatrix}c& b& a\\ i& h& g\\ f& e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c& b& a\\ i& h& g\\ f& e& d\end{vmatrix}=-\begin{vmatrix}a& b& c\\ g& h& i\\ d& e& f\end{vmatrix}=-(-2)=2.$$ We multiply the second column of the matrix $\begin{pmatrix}c& b& a\\ i& h& g\\ f& e& d\end{pmatrix}$ by 3 to get the matrix $\begin{pmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{vmatrix}=3\begin{vmatrix}c& b& a\\ i& h& g\\ f& e& d\end{vmatrix}=(3)(2)=6.$$ We add $(-2)$ times the third column of the matrix $\begin{pmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{pmatrix}$ to its first column to get the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{vmatrix}=\begin{vmatrix}c& 3b& a\\ i& 3h& g\\ f& 3e& d\end{vmatrix}=6.$$ We add $(-3)$ times the first row of the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{pmatrix}$ to its second row to get the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{vmatrix}=\begin{vmatrix}c-2a& 3b& a\\ i-2g& 3h& g\\ f-2d& 3e& d\end{vmatrix}=6.$$ Finally, we multiply the first column of the matrix $\begin{pmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{pmatrix}$ by 3 to get the matrix $\begin{pmatrix}3c-6a& 3b& a\\ 3i-9c+18a-6g& 3h-9b& g-3a\\ 3f-6d& 3e& d\end{pmatrix}$ and we have $$\begin{vmatrix}3c-6a& 3b& a\\ 3i-9c+18a-6g& 3h-9b& g-3a\\ 3f-6d& 3e& d\end{vmatrix}=3\begin{vmatrix}c-2a& 3b& a\\ i-3c+6a-2g& 3h-9b& g-3a\\ f-2d& 3e& d\end{vmatrix}=(3)(6)=18.$$ Thank you.",,"['linear-algebra', 'matrices', 'determinant']"
40,What is a $0\times0$ or $0\times3$ matrix?,What is a  or  matrix?,0\times0 0\times3,"In the comments to another question , the following exchange was noted: ... wait until you see a 0×0 matrix. and ... or worse, a 0×3 matrix! What are these things? Do they have a name or any special proprieties? Where are they used?","In the comments to another question , the following exchange was noted: ... wait until you see a 0×0 matrix. and ... or worse, a 0×3 matrix! What are these things? Do they have a name or any special proprieties? Where are they used?",,"['linear-algebra', 'matrices']"
41,Integer Programming problem,Integer Programming problem,,"I have an integer programming problem with $L$ variables $x_1, x_2, x_{L}$ which all assume integer values and the following constraints must stand: $x_i \geq 0$ $x_1 = 10$ $x_2 + x_3 + ... + x_{L} = 36$ how can I find the max and min of the following quantity? $\displaystyle{\sum_{l = 1}^{L-1} x_{l}\,x_{l + 1}}$ My question is more oriented towards finding out the algorithm used to solve this problem that the exact number of max and min value but obviously if you can come up with an answer without using integer programming that is still more than welcome!","I have an integer programming problem with $L$ variables $x_1, x_2, x_{L}$ which all assume integer values and the following constraints must stand: $x_i \geq 0$ $x_1 = 10$ $x_2 + x_3 + ... + x_{L} = 36$ how can I find the max and min of the following quantity? $\displaystyle{\sum_{l = 1}^{L-1} x_{l}\,x_{l + 1}}$ My question is more oriented towards finding out the algorithm used to solve this problem that the exact number of max and min value but obviously if you can come up with an answer without using integer programming that is still more than welcome!",,"['linear-algebra', 'integer-programming', 'systems-of-equations']"
42,"Given $A^2$ where A is matrix, how find A?","Given  where A is matrix, how find A?",A^2,Problem is simple. Given $$A^2=\begin{bmatrix}13 & 9 &-9 \\ 0 & 4 & 0 \\ 12 & 12 & -8 \end{bmatrix}$$ How find $A$? I think a method using eigenvalues and I find them. But I can't find an actual $A$. Is it right to use eigenvalues?,Problem is simple. Given $$A^2=\begin{bmatrix}13 & 9 &-9 \\ 0 & 4 & 0 \\ 12 & 12 & -8 \end{bmatrix}$$ How find $A$? I think a method using eigenvalues and I find them. But I can't find an actual $A$. Is it right to use eigenvalues?,,"['linear-algebra', 'matrices']"
43,"If $A$ is the adjacency matrix of a graph, why does the $(i,j)$ entry of $A^n$ give the number of $n$-step walks from $i$th vertex to $j$th vertex?","If  is the adjacency matrix of a graph, why does the  entry of  give the number of -step walks from th vertex to th vertex?","A (i,j) A^n n i j","Let $A$ be the adjacency matrix of some directed graph with $m$ vertices labeled as $v_1, v_2, \ldots, v_m$. So here $A_{ij} = 1$ if there is an edge from $v_i$ to $v_j$, and $A_{ij} = 0$ otherwise. By induction it is not too hard to show that $(A^n)_{ij}$ is the number of $n$-step walks from $v_i$ to $v_j$. Why is this the case? Is there some geometric explanation, or some explanation using linear algebra? The definition of matrix multiplication comes from composition of linear maps, and it seems surprising to me to see this connection between linear maps and graphs.","Let $A$ be the adjacency matrix of some directed graph with $m$ vertices labeled as $v_1, v_2, \ldots, v_m$. So here $A_{ij} = 1$ if there is an edge from $v_i$ to $v_j$, and $A_{ij} = 0$ otherwise. By induction it is not too hard to show that $(A^n)_{ij}$ is the number of $n$-step walks from $v_i$ to $v_j$. Why is this the case? Is there some geometric explanation, or some explanation using linear algebra? The definition of matrix multiplication comes from composition of linear maps, and it seems surprising to me to see this connection between linear maps and graphs.",,"['linear-algebra', 'matrices', 'graph-theory', 'intuition']"
44,Why is echelon form important?,Why is echelon form important?,,"My professor gave us this definition for a system of equations in echelon form: A system of m linear equations in n variables is called an echelon system if m ≤ n. Every variable is the leading variable of at most one equation. Every leading variable is to the left of the leading variables of all lower equations. Every equation has a leading variable. He stressed the importance of this form, but didn't actually give us a reason why it was important. Why is this form so important? It seems kinda random.","My professor gave us this definition for a system of equations in echelon form: A system of m linear equations in n variables is called an echelon system if m ≤ n. Every variable is the leading variable of at most one equation. Every leading variable is to the left of the leading variables of all lower equations. Every equation has a leading variable. He stressed the importance of this form, but didn't actually give us a reason why it was important. Why is this form so important? It seems kinda random.",,"['linear-algebra', 'systems-of-equations']"
45,characterize all matrices $X$ such that $BA = X$ whenever $AB = X$,characterize all matrices  such that  whenever,X BA = X AB = X,It is clear that if $A$ and $B$ are $n\times n$ matrices (over a field) with $AB = I$ then $BA = I$. I like to characterize all matrices $X$ such that $BA = X$ whenever $AB = X$.,It is clear that if $A$ and $B$ are $n\times n$ matrices (over a field) with $AB = I$ then $BA = I$. I like to characterize all matrices $X$ such that $BA = X$ whenever $AB = X$.,,['linear-algebra']
46,Congruent and Similar Matrices,Congruent and Similar Matrices,,I tried to solve the following question: Find 2 matrices A and B in M2(C) such that A is similar to B but not congruent. Find 2 matrices A and B in M2(C) such that A is congruent to B but not similar. What is the best strategy to find such matrices? I have tried guessing matrices but it doesn't seem to be a good strategy.,I tried to solve the following question: Find 2 matrices A and B in M2(C) such that A is similar to B but not congruent. Find 2 matrices A and B in M2(C) such that A is congruent to B but not similar. What is the best strategy to find such matrices? I have tried guessing matrices but it doesn't seem to be a good strategy.,,['linear-algebra']
47,"Is a projective space a vector space? If not, what of a basis?","Is a projective space a vector space? If not, what of a basis?",,"A projective space is not a vector space, correct?  At a minimum, there is no additive identity in a projective space. So can you even have a basis of a projective space?  $[1 0 0], [0 1 0], [0 0 1]$ might work for $P^3$.  Or are $[1 0 0]$ and $ [0 1 0]$ not linearly independent since they both lie on the line at infinity? (is there a ""too much"" span problem with $[0 0 0]$ as well?) If not, is there a roughly analogous concept for a projective space?","A projective space is not a vector space, correct?  At a minimum, there is no additive identity in a projective space. So can you even have a basis of a projective space?  $[1 0 0], [0 1 0], [0 0 1]$ might work for $P^3$.  Or are $[1 0 0]$ and $ [0 1 0]$ not linearly independent since they both lie on the line at infinity? (is there a ""too much"" span problem with $[0 0 0]$ as well?) If not, is there a roughly analogous concept for a projective space?",,"['linear-algebra', 'projective-geometry', 'projective-space']"
48,Angle preserving linear maps [duplicate],Angle preserving linear maps [duplicate],,"This question already has answers here : Question about Angle-Preserving Operators (3 answers) Closed 11 years ago . In Spivak's Calculus On Manifolds , in part (c) of question 1-8, he asks the following question: What are all angle preserving $T:\mathbf{R}^n \to \mathbf{R}^n$? I already showed that if $T$ is diagonalizable with a basis $\{x_1,\ldots,x_n\}$ where $Tx_i = \lambda_i$, then $T$ is angle preserving $\iff$ $|\lambda_i| = |\lambda_j|$ for all $i,j$ (this was part (b)). Perhaps this can be used? Thanks.","This question already has answers here : Question about Angle-Preserving Operators (3 answers) Closed 11 years ago . In Spivak's Calculus On Manifolds , in part (c) of question 1-8, he asks the following question: What are all angle preserving $T:\mathbf{R}^n \to \mathbf{R}^n$? I already showed that if $T$ is diagonalizable with a basis $\{x_1,\ldots,x_n\}$ where $Tx_i = \lambda_i$, then $T$ is angle preserving $\iff$ $|\lambda_i| = |\lambda_j|$ for all $i,j$ (this was part (b)). Perhaps this can be used? Thanks.",,['linear-algebra']
49,Prove $p^2=p$ and $qp=0$,Prove  and,p^2=p qp=0,"I am not really aware what's going on in this question. I appreciate your help. Let $U$ be a vector space over a field $F$ and $p, q: U \rightarrow U$ linear maps. Assume $p+q = \text{id}_U$ and $pq=0$. Let $K=\ker(p)$ and $L=\ker(q)$. Prove $p^2=p$ and $qp=0$. $$p(p+q) = p(\text{id}_U) \Rightarrow p^2+pq=p \Rightarrow p^2 =p.$$ I actually first found this by letting $(\text{id}_U) =1$, although probably a wrong to do it. Since $p^2 =p$ and $q$ is also a linear map defined by $q: U \rightarrow U$ then $q^2=q$. Again, I'm not sure if this actually is the right way to do it. Then we have $$q(p+q) = q(\text{id}_U) \Rightarrow q^2+qp=q \Rightarrow q^2+ qp= q^2 \Rightarrow qp=0.$$ Prove $K=\text{im}(q)$. For this question, I honestly do not know how to tackle it from the following definitions. $K = \ker(p)=\left\{u \in U \ |\ p(u)=0_U\right\}$ and $\text{im}(q)=\left\{q(u) \ | \ u \in U \right\}$. What's the correct way of doing these questions? Thank you for your time.","I am not really aware what's going on in this question. I appreciate your help. Let $U$ be a vector space over a field $F$ and $p, q: U \rightarrow U$ linear maps. Assume $p+q = \text{id}_U$ and $pq=0$. Let $K=\ker(p)$ and $L=\ker(q)$. Prove $p^2=p$ and $qp=0$. $$p(p+q) = p(\text{id}_U) \Rightarrow p^2+pq=p \Rightarrow p^2 =p.$$ I actually first found this by letting $(\text{id}_U) =1$, although probably a wrong to do it. Since $p^2 =p$ and $q$ is also a linear map defined by $q: U \rightarrow U$ then $q^2=q$. Again, I'm not sure if this actually is the right way to do it. Then we have $$q(p+q) = q(\text{id}_U) \Rightarrow q^2+qp=q \Rightarrow q^2+ qp= q^2 \Rightarrow qp=0.$$ Prove $K=\text{im}(q)$. For this question, I honestly do not know how to tackle it from the following definitions. $K = \ker(p)=\left\{u \in U \ |\ p(u)=0_U\right\}$ and $\text{im}(q)=\left\{q(u) \ | \ u \in U \right\}$. What's the correct way of doing these questions? Thank you for your time.",,"['linear-algebra', 'vector-spaces', 'operator-theory']"
50,"Differentiate between column space, dimension of column space, and basis of column space.","Differentiate between column space, dimension of column space, and basis of column space.",,Say if there is a matrix A: $$\begin{bmatrix} 1 & 2 & 0 & 2 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$$ What the column space of A? : I am confused whether to exclude NON-pivot columns. What is the dimension of column space? : The dimension of the column space or dimension of the basis of column space? Can be either 4 or 3? What is the basis of column space? This is just the pivot columns. Is this the so called $\operatorname{Col}A$?,Say if there is a matrix A: $$\begin{bmatrix} 1 & 2 & 0 & 2 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$$ What the column space of A? : I am confused whether to exclude NON-pivot columns. What is the dimension of column space? : The dimension of the column space or dimension of the basis of column space? Can be either 4 or 3? What is the basis of column space? This is just the pivot columns. Is this the so called $\operatorname{Col}A$?,,"['linear-algebra', 'matrices']"
51,Prove that a set of matrices is a subspace,Prove that a set of matrices is a subspace,,"I'm self studying linear algebra and now I'm starting with proofs and so on. I found this exercise and this is the way I prove it. I think it's correct but I'm not sure I mean, what do you think? Is the set of matrices      $ \begin{pmatrix} x && x+y \\ x-y && y \end{pmatrix} $ where $x, y \in R $ a subspace of $M_{2\times 2}$ I know that for the set to be a subspace it needs to be closed under vector addition and under scalar multiplication. So: Given $M_1, M_2 \in M_{2\times 2}, x,y, \in R$. Then $M_1 + M_2$ will be: $$\begin{align} \begin{pmatrix} x_1 && x_1+y_1\\ x_1-y_1 && y_1 \end{pmatrix} &+ \begin{pmatrix} x_2 && x_2+y_2\\ x_2-y_2 && y_2 \end{pmatrix} = \\& \begin{pmatrix} (x_1 + x_2) && (x_1+x_2)+(y_1+y_2)\\ (x_1+x_2)-(y_1+y2) && (y_1 + y_2) \end{pmatrix} \end{align}$$ Which has the same structure so it's closed under vector addition. Now, Given $M \in M_{2\times 2}, x,y,r \in R$. Then $rM_1$ will be: $$ r\begin{pmatrix} x && x+y\\ x-y && y \end{pmatrix} = \begin{pmatrix} rx && r(x+y)\\ r(x-y) && ry \end{pmatrix} $$ Which, given that $x,y,r \in R$ is also closed under vector multiplication. So yes. The set is a subspace of $M_{2\times 2}$","I'm self studying linear algebra and now I'm starting with proofs and so on. I found this exercise and this is the way I prove it. I think it's correct but I'm not sure I mean, what do you think? Is the set of matrices      $ \begin{pmatrix} x && x+y \\ x-y && y \end{pmatrix} $ where $x, y \in R $ a subspace of $M_{2\times 2}$ I know that for the set to be a subspace it needs to be closed under vector addition and under scalar multiplication. So: Given $M_1, M_2 \in M_{2\times 2}, x,y, \in R$. Then $M_1 + M_2$ will be: $$\begin{align} \begin{pmatrix} x_1 && x_1+y_1\\ x_1-y_1 && y_1 \end{pmatrix} &+ \begin{pmatrix} x_2 && x_2+y_2\\ x_2-y_2 && y_2 \end{pmatrix} = \\& \begin{pmatrix} (x_1 + x_2) && (x_1+x_2)+(y_1+y_2)\\ (x_1+x_2)-(y_1+y2) && (y_1 + y_2) \end{pmatrix} \end{align}$$ Which has the same structure so it's closed under vector addition. Now, Given $M \in M_{2\times 2}, x,y,r \in R$. Then $rM_1$ will be: $$ r\begin{pmatrix} x && x+y\\ x-y && y \end{pmatrix} = \begin{pmatrix} rx && r(x+y)\\ r(x-y) && ry \end{pmatrix} $$ Which, given that $x,y,r \in R$ is also closed under vector multiplication. So yes. The set is a subspace of $M_{2\times 2}$",,['linear-algebra']
52,Extending a real vector space into a complex vector space using a linear map,Extending a real vector space into a complex vector space using a linear map,,"Let $V$ be real $n$-dimensional vector space, and $T:V\to V$ is a linear map satisfying the condition $T^2(v)=-v$ for all $v \in V$. Then, Show that $n$ is an even integer. Use $T$ to make $V$ into a complex vector space such that the multiplication by complex numbers extends the multiplication by reals. Show that, with respect to the complex vector space structure on $V$ obtained in 2, $T:V\to V $ is a complex linear function. This problem is bugging me for a while. And I have a few questions about it. I did no. 1 using the concept of minimal polynomials. [Another nice proof can be found here .] But the real troubles are question no. 2 and 3. The whole statement of Q.2 looks very vague to me. (For instance, I have doubts that, if I declare a real vector space to be a complex one, how can it be same as the previous one?) The follow up question has an equally dubious statement. I would be glad if somebody takes the time to clarify what these two statements actually mean and exactly what I have to prove. Thank you. [Source: This question can be found here (Question 25b).]","Let $V$ be real $n$-dimensional vector space, and $T:V\to V$ is a linear map satisfying the condition $T^2(v)=-v$ for all $v \in V$. Then, Show that $n$ is an even integer. Use $T$ to make $V$ into a complex vector space such that the multiplication by complex numbers extends the multiplication by reals. Show that, with respect to the complex vector space structure on $V$ obtained in 2, $T:V\to V $ is a complex linear function. This problem is bugging me for a while. And I have a few questions about it. I did no. 1 using the concept of minimal polynomials. [Another nice proof can be found here .] But the real troubles are question no. 2 and 3. The whole statement of Q.2 looks very vague to me. (For instance, I have doubts that, if I declare a real vector space to be a complex one, how can it be same as the previous one?) The follow up question has an equally dubious statement. I would be glad if somebody takes the time to clarify what these two statements actually mean and exactly what I have to prove. Thank you. [Source: This question can be found here (Question 25b).]",,['linear-algebra']
53,Singular covariance matrix,Singular covariance matrix,,"I am looking into the process $\{X_t, t\in\mathbb{Z}\}$, $X_t=A\cos(\lambda t)+B\sin(\lambda t)$, here $\lambda\in(0,\pi)$ is fixed, $A$ and $B$ are uncorrelated random variables with $EA=EB=0$, $EA^2=EB^2=\sigma^2$. I have found the covariance function $r(k)=\sigma^2\cos(\lambda k)$ and now I want to show that process' covariance matrix $$\sigma^2 \begin{pmatrix} 1 & \cos(\lambda) & \cos(2\lambda) & \cdots & \cos(n\lambda) \\ \cos(\lambda) & 1 & \cos(\lambda) & \cdots & \cos((n-1)\lambda) \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ \cos(n\lambda) & \cos((n-1)\lambda) & \cos((n-2)\lambda) & \cdots & 1 \end{pmatrix} $$ is singular when $n\geq 2$. Also the relationship $X_{n+1}=2X_n\cos\lambda-X_{n-1}$, $n\geq 2$ holds, from which I get that $r(k)=\frac{r(k-1)+r(k+1)}{2\cos\lambda}$. But I can not find a fast way to show that the covariance matrix is singular neither using latter relationship nor the matrix above.","I am looking into the process $\{X_t, t\in\mathbb{Z}\}$, $X_t=A\cos(\lambda t)+B\sin(\lambda t)$, here $\lambda\in(0,\pi)$ is fixed, $A$ and $B$ are uncorrelated random variables with $EA=EB=0$, $EA^2=EB^2=\sigma^2$. I have found the covariance function $r(k)=\sigma^2\cos(\lambda k)$ and now I want to show that process' covariance matrix $$\sigma^2 \begin{pmatrix} 1 & \cos(\lambda) & \cos(2\lambda) & \cdots & \cos(n\lambda) \\ \cos(\lambda) & 1 & \cos(\lambda) & \cdots & \cos((n-1)\lambda) \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ \cos(n\lambda) & \cos((n-1)\lambda) & \cos((n-2)\lambda) & \cdots & 1 \end{pmatrix} $$ is singular when $n\geq 2$. Also the relationship $X_{n+1}=2X_n\cos\lambda-X_{n-1}$, $n\geq 2$ holds, from which I get that $r(k)=\frac{r(k-1)+r(k+1)}{2\cos\lambda}$. But I can not find a fast way to show that the covariance matrix is singular neither using latter relationship nor the matrix above.",,"['linear-algebra', 'stochastic-processes', 'determinant']"
54,Generating random linear programming problems,Generating random linear programming problems,,"I've just finished writing a a linear programming problem solver which uses the simplex method.  Now I would like to start optimizing my solver but before I can do this, I need a way of reliably testing it's performance. What is a good algorithm for generating random linear programming problems of arbitrary size?  If possible I would also like to be able to control whether a solution exists or not and I would like to ensure that the origin is a vertex on the simplex.","I've just finished writing a a linear programming problem solver which uses the simplex method.  Now I would like to start optimizing my solver but before I can do this, I need a way of reliably testing it's performance. What is a good algorithm for generating random linear programming problems of arbitrary size?  If possible I would also like to be able to control whether a solution exists or not and I would like to ensure that the origin is a vertex on the simplex.",,"['linear-algebra', 'numerical-methods', 'linear-programming', 'numerical-linear-algebra', 'two-phase-simplex']"
55,Inner product for vector space over arbitrary field,Inner product for vector space over arbitrary field,,"The definition of an inner product in Linear Algebra Done Right by Sheldon Axler assumes that the vector space is over either the real or complex field. PlanetMath makes the same assumption. Is there a definition of an inner product over, for example, finite fields? I sometimes find finite fields easier to reason about, so it would be nice to have a definition of an inner product for vector spaces over them.","The definition of an inner product in Linear Algebra Done Right by Sheldon Axler assumes that the vector space is over either the real or complex field. PlanetMath makes the same assumption. Is there a definition of an inner product over, for example, finite fields? I sometimes find finite fields easier to reason about, so it would be nice to have a definition of an inner product for vector spaces over them.",,['linear-algebra']
56,Circle Least Squares Fit,Circle Least Squares Fit,,"So my question is this: Find the equation of the circle that gives the best least squares circle fit to the points $(-1,-2), (0,2.4), (1.1,-4),$ and $(2.4,-1.6).$ So far I have this general equation: $2xc_1+2yc_2+(r^2-c_1^2-c_2^2)=x^2+y^2$ where $r^2-c_1^2-c_2^2 = c_3$ So then I think I create matrix: $\begin{pmatrix} 2x_1 & 2y_1 & 1 \\ . & . & . \\ 2x_n & 2y_n & 1 \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3\end{pmatrix} - \begin{pmatrix} x_1^2+y_1^2 \\ ... \\ x_n^2+y_n^2\end{pmatrix}$ and after replacing $x_1 = -1$ and $y_1 = -2$ until $x_4$ and $y_4$, I have this matrix: $\begin{pmatrix} -2 & -4 & 1 \\ 0 & 4.8 & 1 \\ 2.2 & -8 & 1 \\ 4.8 & -3.2 & 1 \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3\end{pmatrix} - \begin{pmatrix} 5 \\ 5.76 \\ 17.21 \\ 8.32\end{pmatrix}$ Which i guess I try to solve by setting equal to 0, then moving the last matrix to the other side and make it look like this: $\begin{pmatrix} -2 & -4 & 1 \\ 0 & 4.8 & 1 \\ 2.2 & -8 & 1 \\ 4.8 & -3.2 & 1 \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3\end{pmatrix} = \begin{pmatrix} 5 \\ 5.76 \\ 17.21 \\ 8.32\end{pmatrix}$ And finally solve for $c_1$, $c_2$, $c_3$. But this system has no solution as shown here . Am I missing something huge here or what am I doing wrong?","So my question is this: Find the equation of the circle that gives the best least squares circle fit to the points $(-1,-2), (0,2.4), (1.1,-4),$ and $(2.4,-1.6).$ So far I have this general equation: $2xc_1+2yc_2+(r^2-c_1^2-c_2^2)=x^2+y^2$ where $r^2-c_1^2-c_2^2 = c_3$ So then I think I create matrix: $\begin{pmatrix} 2x_1 & 2y_1 & 1 \\ . & . & . \\ 2x_n & 2y_n & 1 \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3\end{pmatrix} - \begin{pmatrix} x_1^2+y_1^2 \\ ... \\ x_n^2+y_n^2\end{pmatrix}$ and after replacing $x_1 = -1$ and $y_1 = -2$ until $x_4$ and $y_4$, I have this matrix: $\begin{pmatrix} -2 & -4 & 1 \\ 0 & 4.8 & 1 \\ 2.2 & -8 & 1 \\ 4.8 & -3.2 & 1 \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3\end{pmatrix} - \begin{pmatrix} 5 \\ 5.76 \\ 17.21 \\ 8.32\end{pmatrix}$ Which i guess I try to solve by setting equal to 0, then moving the last matrix to the other side and make it look like this: $\begin{pmatrix} -2 & -4 & 1 \\ 0 & 4.8 & 1 \\ 2.2 & -8 & 1 \\ 4.8 & -3.2 & 1 \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3\end{pmatrix} = \begin{pmatrix} 5 \\ 5.76 \\ 17.21 \\ 8.32\end{pmatrix}$ And finally solve for $c_1$, $c_2$, $c_3$. But this system has no solution as shown here . Am I missing something huge here or what am I doing wrong?",,"['linear-algebra', 'matrices']"
57,Find all linearly dependent subsets of this set of vectors,Find all linearly dependent subsets of this set of vectors,,"I have vectors in such form (1 1 1 0 1 0) (0 0 1 0 0 0) (1 0 0 0 0 0)  (0 0 0 1 0 0)  (1 1 0 0 1 0)  (0 0 1 1 0 0)  (1 0 1 1 0 0) I need to find all linear dependent subsets over $Z_2$. For example 1,2,5 and 3,6,7. EDIT (after @rschwieb) The answer for presented vectors: 521 642 763 6541 7432 75431 765321 I did by brute force. I mean i wrote program to iterate through all variants in  $${7 \choose 3} {7 \choose 4} {7 \choose 5} {7 \choose 6} {7 \choose 7}$$ 99 in total. But i just thought what some method exist for such task. For now im trying to implement http://en.wikipedia.org/wiki/Quadratic_sieve . Code incorporated in whole program. I plan to put it here then i organize it well.","I have vectors in such form (1 1 1 0 1 0) (0 0 1 0 0 0) (1 0 0 0 0 0)  (0 0 0 1 0 0)  (1 1 0 0 1 0)  (0 0 1 1 0 0)  (1 0 1 1 0 0) I need to find all linear dependent subsets over $Z_2$. For example 1,2,5 and 3,6,7. EDIT (after @rschwieb) The answer for presented vectors: 521 642 763 6541 7432 75431 765321 I did by brute force. I mean i wrote program to iterate through all variants in  $${7 \choose 3} {7 \choose 4} {7 \choose 5} {7 \choose 6} {7 \choose 7}$$ 99 in total. But i just thought what some method exist for such task. For now im trying to implement http://en.wikipedia.org/wiki/Quadratic_sieve . Code incorporated in whole program. I plan to put it here then i organize it well.",,"['linear-algebra', 'modular-arithmetic']"
58,$\wedge^k(V)^* \cong \mathrm{Alt}^k(V)$,,\wedge^k(V)^* \cong \mathrm{Alt}^k(V),"Let $V$ be a finite dimensional real vector space, let $\mathrm{Alt}^k(V)$ denote the space of  alternating $k$-linear forms on $V$ and let $\wedge^k(V)$ denote the $k^{th}$ exterior power of $V$. I am trying to see why the algebraic dual $\wedge^k(V)^* := (\wedge^k(V))^*$ is isomorphic to  $\mathrm{Alt}^k(V)$. Here are my thoughts: By the universal property of the exterior power, for any alternating $k$-linear form $f$  with domain $V^k$ there exists a unique linear form $\phi$ with domain $\wedge^k(V)$ such that  $$ \phi(v_1 \wedge \cdots \wedge v_k) = f(v_1, \dots, v_k). $$ The universal property thus provides a mechanism to produce elements in $\wedge^k(V)^*$  from elements in $\mathrm{Alt}^k(V)$ and this mechanism of production is unique. Thus, we have an injection $$ \Phi: \mathrm{Alt}^k(V) \longrightarrow \wedge^k(V)^* $$ What I'm not sure about is how to argue surjectivity; what is the best way to approach this?","Let $V$ be a finite dimensional real vector space, let $\mathrm{Alt}^k(V)$ denote the space of  alternating $k$-linear forms on $V$ and let $\wedge^k(V)$ denote the $k^{th}$ exterior power of $V$. I am trying to see why the algebraic dual $\wedge^k(V)^* := (\wedge^k(V))^*$ is isomorphic to  $\mathrm{Alt}^k(V)$. Here are my thoughts: By the universal property of the exterior power, for any alternating $k$-linear form $f$  with domain $V^k$ there exists a unique linear form $\phi$ with domain $\wedge^k(V)$ such that  $$ \phi(v_1 \wedge \cdots \wedge v_k) = f(v_1, \dots, v_k). $$ The universal property thus provides a mechanism to produce elements in $\wedge^k(V)^*$  from elements in $\mathrm{Alt}^k(V)$ and this mechanism of production is unique. Thus, we have an injection $$ \Phi: \mathrm{Alt}^k(V) \longrightarrow \wedge^k(V)^* $$ What I'm not sure about is how to argue surjectivity; what is the best way to approach this?",,"['linear-algebra', 'exterior-algebra']"
59,Eigenvalues of $A+B$,Eigenvalues of,A+B,"$A,B$ are symmetric matrices, $A$ has eigenvalues in $[a,b]$ and $B$ has eigenvalues in $[c,d]$ then  we need to show that eigenvalues of  $A+B$ lie in $[a+c,b+d]$, I am really not getting where to start. What I know $A,B$ have real eigenvalues, they are diagonalizable also.","$A,B$ are symmetric matrices, $A$ has eigenvalues in $[a,b]$ and $B$ has eigenvalues in $[c,d]$ then  we need to show that eigenvalues of  $A+B$ lie in $[a+c,b+d]$, I am really not getting where to start. What I know $A,B$ have real eigenvalues, they are diagonalizable also.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
60,matrix equation $(A-B)CA=B$,matrix equation,(A-B)CA=B,"let $A,B,C$ be $n\times n$ matrices with real entries such that $A$ is invertible. if $(A-B)CA=B$ show that $AC(A-B)=B$. any Ideas??","let $A,B,C$ be $n\times n$ matrices with real entries such that $A$ is invertible. if $(A-B)CA=B$ show that $AC(A-B)=B$. any Ideas??",,"['linear-algebra', 'matrices']"
61,interpretation of dot product of complex vectors,interpretation of dot product of complex vectors,,"given two complex vectors, what is the geometric interpretation of their dot product? $$ \mathbf{x}\mathbf{y} = \sum x_i y_i^*  $$ is there any interpretation similar to the case with real vectors based on projection of one vector onto another or similar?","given two complex vectors, what is the geometric interpretation of their dot product? $$ \mathbf{x}\mathbf{y} = \sum x_i y_i^*  $$ is there any interpretation similar to the case with real vectors based on projection of one vector onto another or similar?",,"['linear-algebra', 'complex-numbers']"
62,Cost of Solving Linear System,Cost of Solving Linear System,,"As most of us are aware the cost for solving a linear system (""exactly"") with Gauss Elimination and other similar methods with a few right hand side and where the matrix has no structure is $\mathcal{O}(N^3)$ where $N$ is the system size. I am wondering about the lower bound for solving a linear system. An obvious lower bound is $\mathcal{\Omega}(N^2)$ (since the information content is $\mathcal{O}(N^2)$). Are there better lower bounds other than $\mathcal{\Omega}(N^2)$ for solving the linear system? Is there a way to prove that the lower bound of $\mathcal{\Omega}(N^2)$ can never be hit for a matrix with no special structure? (assume that we are solving a system with only one right hand side). Also are there other algorithm which solve these system ""exactly"" whose cost is less than $\mathcal{O}(N^3)$?  I am aware of Strassen algorithm which perform matrix multiplications in $\mathcal{O}(N^{\log_27})$. Can this be used to solve a linear system in $\mathcal{O}(N^{\log_27})$? ( Note : The system has no special structure. Say the matrix is just made up of entries drawn out of a random number generator. I am not worried about the stability and other numerical intricacies of the method as of now. I would appreciate if someone could point to some work done in this regard.)","As most of us are aware the cost for solving a linear system (""exactly"") with Gauss Elimination and other similar methods with a few right hand side and where the matrix has no structure is $\mathcal{O}(N^3)$ where $N$ is the system size. I am wondering about the lower bound for solving a linear system. An obvious lower bound is $\mathcal{\Omega}(N^2)$ (since the information content is $\mathcal{O}(N^2)$). Are there better lower bounds other than $\mathcal{\Omega}(N^2)$ for solving the linear system? Is there a way to prove that the lower bound of $\mathcal{\Omega}(N^2)$ can never be hit for a matrix with no special structure? (assume that we are solving a system with only one right hand side). Also are there other algorithm which solve these system ""exactly"" whose cost is less than $\mathcal{O}(N^3)$?  I am aware of Strassen algorithm which perform matrix multiplications in $\mathcal{O}(N^{\log_27})$. Can this be used to solve a linear system in $\mathcal{O}(N^{\log_27})$? ( Note : The system has no special structure. Say the matrix is just made up of entries drawn out of a random number generator. I am not worried about the stability and other numerical intricacies of the method as of now. I would appreciate if someone could point to some work done in this regard.)",,[]
63,On the definition of quadratic forms,On the definition of quadratic forms,,"Let $\mathbb{F}$ be an arbitrary field of characteristic $\mathrm{char}(\mathbb{F})\neq 2$ and $V$ a $\mathbb{F}$ -vector space. The definition of a quadratic form I am used to is a map $\varphi\colon V\to\mathbb{F}$ with the property $\varphi(\lambda v)=\lambda^{2}\varphi(v)$ such that $$\beta_{\varphi}(v,w):=\frac{1}{2}(\varphi(v+w)-\varphi(v)-\varphi(w))$$ is bilinear. Now, clearly, $\beta_{\varphi}$ is symmetric and $\beta_{\varphi}(v,v)=\varphi(v)$ , by definition. In the case $\mathbb{F}=\mathbb{R}$ , I have seen the following claim: Proposition 1 : If $\varphi\colon V\to\mathbb{R}$ with the property $\varphi(\lambda v)=\lambda^{2}\varphi(v)$ satisfies the paralellogram law $$\varphi(v+w)+\varphi(v-w)=2\varphi(v)+2\varphi(w)$$ then it is a quadratic form, i.e. there exists a symmetric bilinear form $\beta_{\varphi}$ such that $\beta_{\varphi}(v,v)=\varphi(v)$ . This is essentially a similar statement as the Jordan-von Neumann theorem, which asserts that a norm $\Vert\cdot\Vert$ on $V$ admits a inner product $\langle\cdot,\cdot\rangle$ inducing it if and only if the paralellogram law $$\Vert v+w\Vert^{2}+\Vert v-w\Vert^{2}=2\Vert v\Vert^{2}+2\Vert w\Vert^{2}$$ holds. Now, my question is the following: Is Proposition 1 also true for arbitrary fields (excluding $\mathrm{char}(\mathbb{F})=2$ of course)? I was not able to find a proof of this statement. I tried to mimik the proof of Jordan-von Neumann, but all the proofs I know of this result are somehow specific to $\mathbb{R}$ . The idea of the proof is usually to define $$\beta_{\varphi}(v,w):=\frac{1}{2}(\varphi(v+w)-\varphi(v)-\varphi(w))$$ Then clearly $\beta_{\varphi}(v,v)=\varphi(v)$ and $\beta_{\varphi}$ is symmetric. The only thing to show is that $\beta_{\varphi}$ actually defines a bilinear form. Any literature on this is appreciated.","Let be an arbitrary field of characteristic and a -vector space. The definition of a quadratic form I am used to is a map with the property such that is bilinear. Now, clearly, is symmetric and , by definition. In the case , I have seen the following claim: Proposition 1 : If with the property satisfies the paralellogram law then it is a quadratic form, i.e. there exists a symmetric bilinear form such that . This is essentially a similar statement as the Jordan-von Neumann theorem, which asserts that a norm on admits a inner product inducing it if and only if the paralellogram law holds. Now, my question is the following: Is Proposition 1 also true for arbitrary fields (excluding of course)? I was not able to find a proof of this statement. I tried to mimik the proof of Jordan-von Neumann, but all the proofs I know of this result are somehow specific to . The idea of the proof is usually to define Then clearly and is symmetric. The only thing to show is that actually defines a bilinear form. Any literature on this is appreciated.","\mathbb{F} \mathrm{char}(\mathbb{F})\neq 2 V \mathbb{F} \varphi\colon V\to\mathbb{F} \varphi(\lambda v)=\lambda^{2}\varphi(v) \beta_{\varphi}(v,w):=\frac{1}{2}(\varphi(v+w)-\varphi(v)-\varphi(w)) \beta_{\varphi} \beta_{\varphi}(v,v)=\varphi(v) \mathbb{F}=\mathbb{R} \varphi\colon V\to\mathbb{R} \varphi(\lambda v)=\lambda^{2}\varphi(v) \varphi(v+w)+\varphi(v-w)=2\varphi(v)+2\varphi(w) \beta_{\varphi} \beta_{\varphi}(v,v)=\varphi(v) \Vert\cdot\Vert V \langle\cdot,\cdot\rangle \Vert v+w\Vert^{2}+\Vert v-w\Vert^{2}=2\Vert v\Vert^{2}+2\Vert w\Vert^{2} \mathrm{char}(\mathbb{F})=2 \mathbb{R} \beta_{\varphi}(v,w):=\frac{1}{2}(\varphi(v+w)-\varphi(v)-\varphi(w)) \beta_{\varphi}(v,v)=\varphi(v) \beta_{\varphi} \beta_{\varphi}","['linear-algebra', 'abstract-algebra', 'reference-request', 'inner-products', 'quadratic-forms']"
64,"If $B = x(xI-A)^{-1}$ for a generator matrix $A$, then $B-B^2$ has positive diagonal elements","If  for a generator matrix , then  has positive diagonal elements",B = x(xI-A)^{-1} A B-B^2,"Let $A$ be the generator matrix of a continuous-time Markov chain. This means that $A$ has positive off-diagonal elements $A_{ij} > 0$ , $i \ne j$ , and row sums $\sum_j A_{ij}$ equal to $0$ . For example, $A$ could be $$ A = \left( \begin{matrix} -7 & 4 & 3 \\ 1 & -2 & 1 \\ 3 & 5 & -8 \end{matrix} \right). $$ I am interested in proving the following claim about the matrix $B = x(x I - A)^{-1}$ for some $x > 0$ . Claim. For the matrix $B = x(x I - A)^{-1}$ , it holds that the diagonal elements of $B - B^2$ are non-negative. Using numerical simulations I have convinced myself that this claim is likely true; however, I have not been able to make much progress toward proving it. It is straightforward to show that the matrix $B$ is stochastic. However, the claim above is not true for all stochastic matrices $B$ ; there is something special about stochastic matrices of this particular form. Any ideas?","Let be the generator matrix of a continuous-time Markov chain. This means that has positive off-diagonal elements , , and row sums equal to . For example, could be I am interested in proving the following claim about the matrix for some . Claim. For the matrix , it holds that the diagonal elements of are non-negative. Using numerical simulations I have convinced myself that this claim is likely true; however, I have not been able to make much progress toward proving it. It is straightforward to show that the matrix is stochastic. However, the claim above is not true for all stochastic matrices ; there is something special about stochastic matrices of this particular form. Any ideas?","A A A_{ij} > 0 i \ne j \sum_j A_{ij} 0 A 
A = \left(
\begin{matrix}
-7 & 4 & 3 \\
1 & -2 & 1 \\
3 & 5 & -8
\end{matrix}
\right).
 B = x(x I - A)^{-1} x > 0 B = x(x I - A)^{-1} B - B^2 B B","['linear-algebra', 'matrices', 'stochastic-processes', 'markov-chains', 'stochastic-matrices']"
65,Can you get a $\mathbb{C}$-basis of $\mathbb{C}^{n}$ from an $\mathbb{R}$-basis by picking one vector out of each of $n$ pairs?,Can you get a -basis of  from an -basis by picking one vector out of each of  pairs?,\mathbb{C} \mathbb{C}^{n} \mathbb{R} n,"Let $T = \{v_1, \dots,  v_{2n}\} \subseteq \mathbb{C}^n$ be a $\mathbb{R}$ -linearly independent set of vectors. Now consider the $2^n$ subsets $S \subseteq T$ of size $n$ which contain exactly one of $v_1$ and $v_{n+1}$ , exactly one of $v_2$ and $v_{n+2}$ etc. Question: Is one of these subsets necessarily a $\mathbb{C}$ -basis? This is equivalent to asking whether there exist $t_1, \dots, t_n \in \mathbb{Z}$ such that the vectors $v_1 + t_1 v_{n+1}, \:v_2 + t_2 v_{n+2}, \dots$ are $\mathbb{C}$ -linearly independent. Proof. Suppose such $t_i$ exist. Then $(v_1 + t_1 v_{n+1}) \wedge \dots \wedge (v_n + t_n v_{n+1})$ is a nonvanishing polynomial in $\underline{t} \in \mathbb{R}^n$ , so one of its coeffcients must be nonzero. But each coefficient is the wedge of one of the sets $S$ as above. Conversely, if you have a subset $S$ corresponding to replacing $v_j$ by $v_{n + j}$ for some $j \in A \subseteq \{1, \dots, n\}$ then you can set $t_j = 0$ for $j \not\in A$ and pick $t_j$ very large for $j \in A$ . Since being linearly independent is an open condition, this will give linearly independent vectors $v_j + t_j v_{n+j}$ . $\square$","Let be a -linearly independent set of vectors. Now consider the subsets of size which contain exactly one of and , exactly one of and etc. Question: Is one of these subsets necessarily a -basis? This is equivalent to asking whether there exist such that the vectors are -linearly independent. Proof. Suppose such exist. Then is a nonvanishing polynomial in , so one of its coeffcients must be nonzero. But each coefficient is the wedge of one of the sets as above. Conversely, if you have a subset corresponding to replacing by for some then you can set for and pick very large for . Since being linearly independent is an open condition, this will give linearly independent vectors .","T = \{v_1, \dots,  v_{2n}\} \subseteq \mathbb{C}^n \mathbb{R} 2^n S \subseteq T n v_1 v_{n+1} v_2 v_{n+2} \mathbb{C} t_1, \dots, t_n \in \mathbb{Z} v_1 + t_1 v_{n+1}, \:v_2 + t_2 v_{n+2}, \dots \mathbb{C} t_i (v_1 + t_1 v_{n+1}) \wedge \dots \wedge (v_n + t_n v_{n+1}) \underline{t} \in \mathbb{R}^n S S v_j v_{n + j} j \in A \subseteq \{1, \dots, n\} t_j = 0 j \not\in A t_j j \in A v_j + t_j v_{n+j} \square","['linear-algebra', 'abelian-varieties']"
66,Taylor series of a matrix exponential,Taylor series of a matrix exponential,,"I am looking to minimize the value of: $$g(t)=\mathrm{Tr}\left[\exp(X+tY)\right]$$ where both $X$ and $Y$ are symmetrical matrices with real coefficients. In general, $X$ and $Y$ do not commute so $\exp(X+tY)\neq\exp(X)\exp(tY)$ . We can further assume that $tY$ is small when compared to $X$ at the minimum. I assume that one of the the simplest approach is to attempt to write $g(t)$ as a Taylor expansion, like: $$g(t)=g_0+tg_1+\frac12t^2g_2+\dots$$ In this case, a good approximation for the minimum is easily obtained with $t\sim-\frac{g_1}{g_2}$ (Newton's method) and the process can be iterated until we meet a convergence criterion. The first two coefficients are quite trivial to find. For instance, $g_0=\mathrm{Tr}\left[\exp(X)\right]$ and $g_1=\mathrm{Tr}\left[Y\exp(X)\right]$ , as explained here . However, I spent some hours on this but I can't find an easy expression for $g_2$ yet. Is there a proper way to express $g_2$ so it can be computed numerically?","I am looking to minimize the value of: where both and are symmetrical matrices with real coefficients. In general, and do not commute so . We can further assume that is small when compared to at the minimum. I assume that one of the the simplest approach is to attempt to write as a Taylor expansion, like: In this case, a good approximation for the minimum is easily obtained with (Newton's method) and the process can be iterated until we meet a convergence criterion. The first two coefficients are quite trivial to find. For instance, and , as explained here . However, I spent some hours on this but I can't find an easy expression for yet. Is there a proper way to express so it can be computed numerically?",g(t)=\mathrm{Tr}\left[\exp(X+tY)\right] X Y X Y \exp(X+tY)\neq\exp(X)\exp(tY) tY X g(t) g(t)=g_0+tg_1+\frac12t^2g_2+\dots t\sim-\frac{g_1}{g_2} g_0=\mathrm{Tr}\left[\exp(X)\right] g_1=\mathrm{Tr}\left[Y\exp(X)\right] g_2 g_2,"['linear-algebra', 'matrices', 'taylor-expansion', 'matrix-calculus', 'matrix-exponential']"
67,Decomposition of symmetric powers of the standard representation of $SO(n)$,Decomposition of symmetric powers of the standard representation of,SO(n),"Let $V$ be the n-dimensional standard-representation of $SO(n)$ . Since $SO(n)$ preserves a bilinear form on $V$ there is a trivial 1-dimensional subrepresentation in $S^2V$ . So, in general, $S^k V$ seems not to be irreducible. Is there any known decomposition of the $k$ -th symmetric power $S^kV$ into irreducible $SO(n)$ -representations? I am coming from a different area and have little knowledge about general representation theory, but any hint or reference is welcome, thanks.","Let be the n-dimensional standard-representation of . Since preserves a bilinear form on there is a trivial 1-dimensional subrepresentation in . So, in general, seems not to be irreducible. Is there any known decomposition of the -th symmetric power into irreducible -representations? I am coming from a different area and have little knowledge about general representation theory, but any hint or reference is welcome, thanks.",V SO(n) SO(n) V S^2V S^k V k S^kV SO(n),"['linear-algebra', 'representation-theory', 'lie-groups', 'algebraic-groups']"
68,Sparse PCA vs Orthogonal Matching Pursuit,Sparse PCA vs Orthogonal Matching Pursuit,,"Can't wrap my head around the difference between Sparse PCA and OMP. Both try to find a sparse linear combination. Of course, the optimization criteria is different. In Sparse PCA we have: \begin{aligned} \max & x^{T} \Sigma x \\ \text { subject to } &\|x\|_{2}=1 \\ &\|x\|_{0} \leq k \end{aligned} In OMP we have: $$ \min _{x}\|f-D x\|_{2}^{2} \text { subject to }\|x\|_{0} \leq k $$ Even though these are different, they resemble one another in my eyes. I'll explain: In PCA we wish to take the projection that counts for the most variance. If we add the ""sparsity"" constraint, than we regularize and get some projection which is sparser, but does not account for the most variance possible (without the constraint). In OMP (the algorithm procedure here ), we pretty much do the same thing, iteratively - we find the ""atom"" that gives the largest inner product - which is the most correlative. Differences I see: Different optimization problem (already said) - however the ""applicative"" view look very similar, therefore I ask this question OMP is an iterative (greedy) procedure, while in Sparse PCA, there are direct solutions? Moreover, how about this minor modification: we assume that $f$ is ""taken out"" of $D$ (which applicatively mean that we include 𝑓 to our sparse vectors) - now we have a ""basis"" $D$ , where the OMP result will give us the ""best"" sparse approximation (variance-wise) of $f$ , which resembles PCA on the covariance matrix, no? Example : $D$ is our data (samples and features). In SPCA we find the projections that count for the most variance of the data (done by defining $\Sigma=DD^T$ and applying the Sparse PCA). In OMP we do something similar, we ""take"" one sample, $f$ , ""out"" of matrix $D$ , and try to approximate it with the other samples. This ""forces"" us to ""use"" $f$ as an atom (with coef = $1.0$ ), but eventually we will get $y = f - D'x' $ with minimum variance, which translates to $Dx$ where there is coefficient $1.0$ in the row of the sample. Thanks!","Can't wrap my head around the difference between Sparse PCA and OMP. Both try to find a sparse linear combination. Of course, the optimization criteria is different. In Sparse PCA we have: In OMP we have: Even though these are different, they resemble one another in my eyes. I'll explain: In PCA we wish to take the projection that counts for the most variance. If we add the ""sparsity"" constraint, than we regularize and get some projection which is sparser, but does not account for the most variance possible (without the constraint). In OMP (the algorithm procedure here ), we pretty much do the same thing, iteratively - we find the ""atom"" that gives the largest inner product - which is the most correlative. Differences I see: Different optimization problem (already said) - however the ""applicative"" view look very similar, therefore I ask this question OMP is an iterative (greedy) procedure, while in Sparse PCA, there are direct solutions? Moreover, how about this minor modification: we assume that is ""taken out"" of (which applicatively mean that we include 𝑓 to our sparse vectors) - now we have a ""basis"" , where the OMP result will give us the ""best"" sparse approximation (variance-wise) of , which resembles PCA on the covariance matrix, no? Example : is our data (samples and features). In SPCA we find the projections that count for the most variance of the data (done by defining and applying the Sparse PCA). In OMP we do something similar, we ""take"" one sample, , ""out"" of matrix , and try to approximate it with the other samples. This ""forces"" us to ""use"" as an atom (with coef = ), but eventually we will get with minimum variance, which translates to where there is coefficient in the row of the sample. Thanks!","\begin{aligned} \max & x^{T} \Sigma x \\ \text { subject to } &\|x\|_{2}=1 \\ &\|x\|_{0} \leq k \end{aligned} 
\min _{x}\|f-D x\|_{2}^{2} \text { subject to }\|x\|_{0} \leq k
 f D D f D \Sigma=DD^T f D f 1.0 y = f - D'x'  Dx 1.0","['linear-algebra', 'numerical-linear-algebra', 'numerical-optimization', 'sparse-matrices', 'sparsity']"
69,Show that $\det(AC) \ge 0$ for real matrices with $(A+iB)^{-1} = C+iD$.,Show that  for real matrices with .,\det(AC) \ge 0 (A+iB)^{-1} = C+iD,"Let $H=A+Bi$ be a complex $n \times n$ invertible matrix where $A,B$ are real matrices, with inverse $H^{-1}=C+Di$ for $C,D$ real. Prove $\det(AC)\geq 0$ . My attempt so far, $$I=HH^{-1}=(A+Bi)(C+Di)=AC-BD+(AD+BC)i$$ So $$AC-BD+(AD+BC)i=I$$ Solving by $AC$ and taking the determinant in both sides, I end up with $$ \det(AC)=\det(I-(AD+BC)I+BD)$$ Since $AC-BD+(AD+BC)i$ is the identity then it is a real matrix so it must be true that $(AD+BC)=O$ , So finally I get $$\det(AC)=\det(I+BD).$$ But this is where I am stuck, is $\det(I+BD)$ non-negative? Or did I take a completely wrong approach?","Let be a complex invertible matrix where are real matrices, with inverse for real. Prove . My attempt so far, So Solving by and taking the determinant in both sides, I end up with Since is the identity then it is a real matrix so it must be true that , So finally I get But this is where I am stuck, is non-negative? Or did I take a completely wrong approach?","H=A+Bi n \times n A,B H^{-1}=C+Di C,D \det(AC)\geq 0 I=HH^{-1}=(A+Bi)(C+Di)=AC-BD+(AD+BC)i AC-BD+(AD+BC)i=I AC 
\det(AC)=\det(I-(AD+BC)I+BD) AC-BD+(AD+BC)i (AD+BC)=O \det(AC)=\det(I+BD). \det(I+BD)","['linear-algebra', 'matrices', 'determinant']"
70,Gram-Schmidt method to get a basis for $P_3$,Gram-Schmidt method to get a basis for,P_3,"If $P_3$ is a vector space of third-degree polynomials. It is known the basis for $P_3$ is ${( 1,x,x^2 , x^3})$ and $\langle p, q\rangle = \int_{0}^{1} p(x)q(x)\, dx.$ is a valid product on $P_3$ I am trying to use the Gram-Schmidt method to get a basis for $P_3$ which is orthonormal with respect to the above inner product. Even though I found partial solutions or similar problems the explanations are limited. PS. I read the rules before posting my first question. Even though I found similar problems  I didn't understand entirely the method and calculations. Additional Sources the below exercise which has a partial solution, but I am not sure how to calculate the remaining values. this question which is similar but in $P_2$ Finding an orthonormal basis for the space $P_2$ with respect to a given inner product I hope I did not violate any rule. It was my last hope to ask here since due to current conditions I can't ask my Teacher face to face.","If is a vector space of third-degree polynomials. It is known the basis for is and is a valid product on I am trying to use the Gram-Schmidt method to get a basis for which is orthonormal with respect to the above inner product. Even though I found partial solutions or similar problems the explanations are limited. PS. I read the rules before posting my first question. Even though I found similar problems  I didn't understand entirely the method and calculations. Additional Sources the below exercise which has a partial solution, but I am not sure how to calculate the remaining values. this question which is similar but in Finding an orthonormal basis for the space $P_2$ with respect to a given inner product I hope I did not violate any rule. It was my last hope to ask here since due to current conditions I can't ask my Teacher face to face.","P_3 P_3 {( 1,x,x^2 , x^3}) \langle p, q\rangle = \int_{0}^{1} p(x)q(x)\, dx. P_3 P_3 P_2","['linear-algebra', 'self-learning', 'gram-schmidt']"
71,Best approximation of sum of unit vectors by a smaller subset,Best approximation of sum of unit vectors by a smaller subset,,"Let $v_1,\ldots,v_N$ be linear independent unit vectors in $\mathbb{R}^N$ and denote their scaled sum by $s_N = \frac{1}{N}\sum_{k=1}^N v_k.$ I would like to find a small subset of size $n$ among those vectors such that their scaled sum approximates $s_N$ well. In other words find $$ J = \underset{J\in\mathscr{J}}{\operatorname{argmin}} \bigg\lVert s_N - \frac{1}{n}\sum_{k=1}^n v_{J_k}\bigg\rVert$$ where $J$ runs over the set $\mathscr{J}$ of all subsets of $\{1,\ldots,N\}$ with size $n$ and $\lVert \cdot \rVert$ is the euclidean norm. The set of vectors can be considered an iid sample drawn uniformly from the sphere. And, of course, in my case $N$ and $n$ are too large ( $N$ will be of the order of 10'000 or 100'000 and $n$ maybe one or two magnitudes smaller) to just try all subsets. So  I am looking for something more clever. My approach so far I tried Repeated random subsampling , i.e. drawing many, many subsets of size $n$ in an iid fashion, calculating the approximation for each instance and retaining the best. Greedy approach, starting with a single vector, and then increasing the set in steps every time by a single vector. The vector is that single vector which gives the best approximation for the enlarged set. Questions Is this a known problem with a proper name? Is it hard (as in NP-hard for example) or are clever solutions known? Are there better heuristic approaches? Are there theoretic results/performance guarantees for the two heuristics I used? Note : I edited the question to include scaling. Some of the answers/comments refer to the older version where vectors were not scaled.","Let be linear independent unit vectors in and denote their scaled sum by I would like to find a small subset of size among those vectors such that their scaled sum approximates well. In other words find where runs over the set of all subsets of with size and is the euclidean norm. The set of vectors can be considered an iid sample drawn uniformly from the sphere. And, of course, in my case and are too large ( will be of the order of 10'000 or 100'000 and maybe one or two magnitudes smaller) to just try all subsets. So  I am looking for something more clever. My approach so far I tried Repeated random subsampling , i.e. drawing many, many subsets of size in an iid fashion, calculating the approximation for each instance and retaining the best. Greedy approach, starting with a single vector, and then increasing the set in steps every time by a single vector. The vector is that single vector which gives the best approximation for the enlarged set. Questions Is this a known problem with a proper name? Is it hard (as in NP-hard for example) or are clever solutions known? Are there better heuristic approaches? Are there theoretic results/performance guarantees for the two heuristics I used? Note : I edited the question to include scaling. Some of the answers/comments refer to the older version where vectors were not scaled.","v_1,\ldots,v_N \mathbb{R}^N s_N = \frac{1}{N}\sum_{k=1}^N v_k. n s_N  J = \underset{J\in\mathscr{J}}{\operatorname{argmin}} \bigg\lVert s_N - \frac{1}{n}\sum_{k=1}^n v_{J_k}\bigg\rVert J \mathscr{J} \{1,\ldots,N\} n \lVert \cdot \rVert N n N n n","['linear-algebra', 'combinatorics', 'optimization', 'integer-programming', 'discrete-optimization']"
72,Basis-free definition of derivative of polynomial functions on a vector space,Basis-free definition of derivative of polynomial functions on a vector space,,"Let $V$ be a finite dimensional vector space over an infinite field $k$ . The ring of polynomial functions on $V$ is the subalgebra of the $k$ -algebra of all functions $V\to k$ generated by the dual space $V^*$ , and is denoted by $k[V]$ . Let $(e_1,\dots,e_n)$ be an ordered basis of $V$ and let $(f_1,\dots,f_n)$ be its dual basis, then an element of $k[V]$ is a polynomial in $f_1,\dots,f_n$ . We can then define a (formal) derivative as follows: First, fix $i\in\{1,\dots,n\}$ and define $$ \partial_{e_i}(f_1^{r_1}\cdots f_{i-1}^{r_{i-1}}f_i^{r_i}f_{i+1}^{r_{i+1}}\cdots f_n^{r_n}) = r_i f_1^{r_1}\cdots f_{i-1}^{r_{i-1}}f_i^{r_i-1}f_{i+1}^{r_{i+1}}\cdots f_n^{r_n}, $$ for all $r_1,\dots,r_n\in \mathbb{Z}_{\geq 0}$ . Extending by linearity we obtain a well defined derivation $\partial_{e_i}:k[V]\to k[V]$ . Then for $v\in V$ , write $$ v = \sum_{i=1}^n a_i e_i, \qquad a_1,\dots,a_n\in k $$ and define $$ \partial_v(f) = \sum_{i=1}^n a_i \partial_{e_i}(f), \qquad \forall f\in k[V]. $$ When we take $V=k^n$ and $(e_1,\dots,e_n)$ as the canonical ordered basis, the $i$ -th vector in the dual basis is the coordinate function $x_i:k^n\to k$ given by $x_i(a_1,\dots,a_n) = a_i$ , and $k[V]$ is precisely the polynomial ring $k[x_1,\dots,x_n]$ and the derivation $\partial_v$ coincides with the known formal directional derivative on that polynomial ring. The main issue with this definition is that it depends on the chosen basis $(e_1,\dots,e_n)$ . I would like to know if there is a basis-free definition of the derivative $\partial_v$ for a ring of polynomial functions $k[V]$ on a finite dimensional vector space $V$ over an infinite field $k$ .","Let be a finite dimensional vector space over an infinite field . The ring of polynomial functions on is the subalgebra of the -algebra of all functions generated by the dual space , and is denoted by . Let be an ordered basis of and let be its dual basis, then an element of is a polynomial in . We can then define a (formal) derivative as follows: First, fix and define for all . Extending by linearity we obtain a well defined derivation . Then for , write and define When we take and as the canonical ordered basis, the -th vector in the dual basis is the coordinate function given by , and is precisely the polynomial ring and the derivation coincides with the known formal directional derivative on that polynomial ring. The main issue with this definition is that it depends on the chosen basis . I would like to know if there is a basis-free definition of the derivative for a ring of polynomial functions on a finite dimensional vector space over an infinite field .","V k V k V\to k V^* k[V] (e_1,\dots,e_n) V (f_1,\dots,f_n) k[V] f_1,\dots,f_n i\in\{1,\dots,n\} 
\partial_{e_i}(f_1^{r_1}\cdots f_{i-1}^{r_{i-1}}f_i^{r_i}f_{i+1}^{r_{i+1}}\cdots f_n^{r_n}) = r_i f_1^{r_1}\cdots f_{i-1}^{r_{i-1}}f_i^{r_i-1}f_{i+1}^{r_{i+1}}\cdots f_n^{r_n},
 r_1,\dots,r_n\in \mathbb{Z}_{\geq 0} \partial_{e_i}:k[V]\to k[V] v\in V 
v = \sum_{i=1}^n a_i e_i, \qquad a_1,\dots,a_n\in k
 
\partial_v(f) = \sum_{i=1}^n a_i \partial_{e_i}(f), \qquad \forall f\in k[V].
 V=k^n (e_1,\dots,e_n) i x_i:k^n\to k x_i(a_1,\dots,a_n) = a_i k[V] k[x_1,\dots,x_n] \partial_v (e_1,\dots,e_n) \partial_v k[V] V k","['linear-algebra', 'abstract-algebra', 'derivatives', 'polynomials']"
73,Is Hamel Basis necessarily uncountable?,Is Hamel Basis necessarily uncountable?,,Let $X$ be a (real or complex) infinite dimensional vector space. (Not Normed or Banach one). Is every Hamel Basis for $X$ necessarily uncountable ?,Let be a (real or complex) infinite dimensional vector space. (Not Normed or Banach one). Is every Hamel Basis for necessarily uncountable ?,X X,"['linear-algebra', 'hamel-basis']"
74,This question appeared in a mate's child's homework and it's broken my brain.,This question appeared in a mate's child's homework and it's broken my brain.,,I could use gaussian elimination if I make some assumptions or does any one have another suggestion?,I could use gaussian elimination if I make some assumptions or does any one have another suggestion?,,['linear-algebra']
75,How would the most general $2 \times 2$ normal matrix look like?,How would the most general  normal matrix look like?,2 \times 2,"How would the most general $2 \times 2$ normal matrix look like? The normal matrix satisfy equation: $A^*A=AA^*$ where $A^*$ denotes   conjugate transpose. I was thinking about the matrix: $$     \begin{pmatrix}     a & -b  \\     b & a  \\     \end{pmatrix} $$ because its columns are orthogonal to each other and it satisfies the given equation: $$     \begin{pmatrix}     a & -b  \\     b & a  \\     \end{pmatrix}     \begin{pmatrix}     a & b  \\     -b & a  \\     \end{pmatrix} =      \begin{pmatrix}     a^2 + b^2 & 0  \\     0 & b^2 + a^2  \\     \end{pmatrix} $$ $$ $$ $$     \begin{pmatrix}     a & b  \\     -b & a  \\     \end{pmatrix}     \begin{pmatrix}     a & -b  \\     b & a  \\     \end{pmatrix} =      \begin{pmatrix}     a^2 + b^2 & 0  \\     0 & b^2 + a^2  \\     \end{pmatrix} $$ $$ $$ It is true for real matrices, and I suppose for the complex one too. But is this the most general case, or is there something else?","How would the most general normal matrix look like? The normal matrix satisfy equation: where denotes   conjugate transpose. I was thinking about the matrix: because its columns are orthogonal to each other and it satisfies the given equation: It is true for real matrices, and I suppose for the complex one too. But is this the most general case, or is there something else?","2 \times 2 A^*A=AA^* A^* 
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix}
 
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix}
    \begin{pmatrix}
    a & b  \\
    -b & a  \\
    \end{pmatrix} = 
    \begin{pmatrix}
    a^2 + b^2 & 0  \\
    0 & b^2 + a^2  \\
    \end{pmatrix}
 
 
    \begin{pmatrix}
    a & b  \\
    -b & a  \\
    \end{pmatrix}
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix} = 
    \begin{pmatrix}
    a^2 + b^2 & 0  \\
    0 & b^2 + a^2  \\
    \end{pmatrix}
 
","['linear-algebra', 'matrices', 'orthonormal']"
76,Prove there exists a matrix $B$ such that $AB=BA$ and $B^2 = -I$.,Prove there exists a matrix  such that  and .,B AB=BA B^2 = -I,Let $A$ be a real $n \times n$ matrix without real eigenvalues. Prove there exists a real matrix $B$ such that $AB=BA$ and $B^2 = -I$ . I understand that $n$ is even and $A$ is a nonsingular  matrix.,Let be a real matrix without real eigenvalues. Prove there exists a real matrix such that and . I understand that is even and is a nonsingular  matrix.,A n \times n B AB=BA B^2 = -I n A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
77,Equality concerning the norm of rows of a resolvent matrix.,Equality concerning the norm of rows of a resolvent matrix.,,"This problem showed up on UCLA's basic exam for Fall 2018: Let $X$ be an $n \times n$ symmetric (real) matrix and $z \in \mathbb{C}$ with $\text{Im } z > 0$ .  Define $G = (X - zI)^{-1}.$ Show that $$\sum_{1 \leq j \leq n} |G_{ij}|^2 = \frac{\text{Im } G_{ii}}{\text{Im }z}.$$ I worked on this for a little bit where I applied the real spectral theorem to $X$ which in turn gives you that $G = Q^T D Q$ where $Q$ is real orthogonal ( $Q^T Q = Q Q^T = I$ ) and $D$ is diagonal satisfying $D_{ii} = (\lambda_i - z)^{-1}$ .  Where $\lambda_1, \ldots, \lambda_n$ are the eigenvalues for $X$ .  Couldn't really see any immediate way out from there.  Would be interested in seeing peoples' solutions to this problem and any connections to the study of matrix resolvents: https://en.wikipedia.org/wiki/Resolvent_formalism",This problem showed up on UCLA's basic exam for Fall 2018: Let be an symmetric (real) matrix and with .  Define Show that I worked on this for a little bit where I applied the real spectral theorem to which in turn gives you that where is real orthogonal ( ) and is diagonal satisfying .  Where are the eigenvalues for .  Couldn't really see any immediate way out from there.  Would be interested in seeing peoples' solutions to this problem and any connections to the study of matrix resolvents: https://en.wikipedia.org/wiki/Resolvent_formalism,"X n \times n z \in \mathbb{C} \text{Im } z > 0 G = (X - zI)^{-1}. \sum_{1 \leq j \leq n} |G_{ij}|^2 = \frac{\text{Im } G_{ii}}{\text{Im }z}. X G = Q^T D Q Q Q^T Q = Q Q^T = I D D_{ii} = (\lambda_i - z)^{-1} \lambda_1, \ldots, \lambda_n X","['linear-algebra', 'complex-analysis']"
78,"Kernel of a zero diagonal, non-negative symmetric matrix","Kernel of a zero diagonal, non-negative symmetric matrix",,"Let $A\in M^n(\mathbb{R})$ a symmetric matrix with zero principal diagonal and with strictly positive off-diagonal entries. What is the highest possible dimension of $\,\mathbf{Ker(A)}$ ? When $n=2,3$ it turns out that $A$ is invertible. If $n=4$ then $A$ can be singular, with $dim(Ker(A))=1$ . I would like to understand if there is a method to study the case of a generic $n$ . Thank you for any suggestion.","Let a symmetric matrix with zero principal diagonal and with strictly positive off-diagonal entries. What is the highest possible dimension of ? When it turns out that is invertible. If then can be singular, with . I would like to understand if there is a method to study the case of a generic . Thank you for any suggestion.","A\in M^n(\mathbb{R}) \,\mathbf{Ker(A)} n=2,3 A n=4 A dim(Ker(A))=1 n","['linear-algebra', 'matrices']"
79,Integers which are squared norm of 3 by 3 integer matrices,Integers which are squared norm of 3 by 3 integer matrices,,"Let $n$ be a positive integer. Let $E_n$ be the set of integers which are the sum of $n$ squares. Let $F_n$ be the set of integers of the form $\Vert A \Vert^2$ with $A \in M_n(\mathbb{Z})$ .  Then $E_n \subseteq F_n$ because: $$\left\| \pmatrix{a_1&0& \cdots\\ \vdots & \vdots&  \\ a_n&0& \cdots} \right\|^2  = \sum_{i=1}^n a_i^2.$$ Note that the case $n=3$ is exceptional, because $E_n= F_n$ $\forall n \neq 3$ , whereas $E_3 \subsetneq F_3$ : obviously $E_1=F_1$ , it is proved here that $E_2=F_2$ , for $n \ge 4$ , $E_n=F_n$ because $E_4 = \mathbb{N}$ , by Lagrange's four square theorem , finally, $E_3 \subsetneq F_3$ because $\forall n \le 2000$ , $n \in F_3$ (by computation below), whereas: Legendre's three-square theorem A natural number can be represented as the sum of three squares of integers if and only if it is not of the form $4^n(8m+7)$ for integers $n,m \ge 0$ . Question : Which integers are contained in $F_3$ ? The computation suggests that $F_3$ contains every natural number, so (if it is true) we are reduced to prove that it contains those of the form $4^n(8m+7)$ , by Legendre's three-square theorem. Computation sage: L=[] ....: for a2 in range(33): ....:     for a4 in range(33): ....:         for a5 in range(33): ....:             for a7 in range(33): ....:                 for a8 in range(33): ....:                     n=numerical_approx(matrix([[0,a2,0],[a4,a5,0],[a7,a8,0]]).norm()^2,digits=10) ....:                     if n.is_integer(): ....:                         L.append(int(n)) ....: l=list(set(L)) ....: l.sort() ....: l[2095] ....: 2095","Let be a positive integer. Let be the set of integers which are the sum of squares. Let be the set of integers of the form with .  Then because: Note that the case is exceptional, because , whereas : obviously , it is proved here that , for , because , by Lagrange's four square theorem , finally, because , (by computation below), whereas: Legendre's three-square theorem A natural number can be represented as the sum of three squares of integers if and only if it is not of the form for integers . Question : Which integers are contained in ? The computation suggests that contains every natural number, so (if it is true) we are reduced to prove that it contains those of the form , by Legendre's three-square theorem. Computation sage: L=[] ....: for a2 in range(33): ....:     for a4 in range(33): ....:         for a5 in range(33): ....:             for a7 in range(33): ....:                 for a8 in range(33): ....:                     n=numerical_approx(matrix([[0,a2,0],[a4,a5,0],[a7,a8,0]]).norm()^2,digits=10) ....:                     if n.is_integer(): ....:                         L.append(int(n)) ....: l=list(set(L)) ....: l.sort() ....: l[2095] ....: 2095","n E_n n F_n \Vert A \Vert^2 A \in M_n(\mathbb{Z}) E_n \subseteq F_n \left\| \pmatrix{a_1&0& \cdots\\ \vdots & \vdots&  \\ a_n&0& \cdots} \right\|^2  = \sum_{i=1}^n a_i^2. n=3 E_n= F_n \forall n \neq 3 E_3 \subsetneq F_3 E_1=F_1 E_2=F_2 n \ge 4 E_n=F_n E_4 = \mathbb{N} E_3 \subsetneq F_3 \forall n \le 2000 n \in F_3 4^n(8m+7) n,m \ge 0 F_3 F_3 4^n(8m+7)","['linear-algebra', 'matrices', 'number-theory', 'elementary-number-theory', 'normed-spaces']"
80,Finding representatives of conjugacy classes in $\text{Mat}_2(\mathbb{Q})$,Finding representatives of conjugacy classes in,\text{Mat}_2(\mathbb{Q}),Let $S= \{A \in \text{Mat}_2(\mathbb{Q}) : A^6 = I$ and $A^n \ne I$ for any $0 < n < 6\}$. I wish to describe the orbits of each of the element in $S$ with respect to conjugation by $GL_2(\mathbb{Q})$ on  $\text{Mat}_2(\mathbb{Q})$. I can't see how to start. Hints are much appreciated!,Let $S= \{A \in \text{Mat}_2(\mathbb{Q}) : A^6 = I$ and $A^n \ne I$ for any $0 < n < 6\}$. I wish to describe the orbits of each of the element in $S$ with respect to conjugation by $GL_2(\mathbb{Q})$ on  $\text{Mat}_2(\mathbb{Q})$. I can't see how to start. Hints are much appreciated!,,"['linear-algebra', 'abstract-algebra', 'group-theory']"
81,Gram-Schmidt over GF$(2)$,Gram-Schmidt over GF,(2),"I am reading the paper The Steganographic File System by Ross Anderson, Roger Needham, and Adi Shamir. On page 4, paragraph 2, the authors write: Finally, we use the Gram-Schmidt method to orthonormalise all the   vectors from $i$ onwards by subtracting from the candidate $K_i$ all   its components along later $K_j$ which the user knows by the chaining   property of the $p_j$’s. This simply means that the authors use the Gram-Schmidt algorithm with the ground field GF$(2)$, and from the context, each of the original vectors the algorithm is applied to has norm $1$. However, in this case the algorithm produces a basis which is not necessarily orthonormal, not even orthogonal. Am I right? Is this a serious flaw?","I am reading the paper The Steganographic File System by Ross Anderson, Roger Needham, and Adi Shamir. On page 4, paragraph 2, the authors write: Finally, we use the Gram-Schmidt method to orthonormalise all the   vectors from $i$ onwards by subtracting from the candidate $K_i$ all   its components along later $K_j$ which the user knows by the chaining   property of the $p_j$’s. This simply means that the authors use the Gram-Schmidt algorithm with the ground field GF$(2)$, and from the context, each of the original vectors the algorithm is applied to has norm $1$. However, in this case the algorithm produces a basis which is not necessarily orthonormal, not even orthogonal. Am I right? Is this a serious flaw?",,"['linear-algebra', 'finite-fields', 'cryptography', 'orthonormal', 'gram-schmidt']"
82,Examples of when the Riesz representation theorem doesn't hold,Examples of when the Riesz representation theorem doesn't hold,,"I was wondering if anyone could give me some interesting ""counter examples"" to the Riesz representation theorem about functionals over Hilbert spaces. When I say counter examples, I'm obviously talking about examples where some of the basic assumptions of the theorem aren't met, so the theorem doesn't hold. In other words - could you show me some non-trivial examples of functionals over inner-product spaces that cannot be expressed as an inner-product with some vector in the vector space? I already have an example from $C [0, 1]$ based on the standard $L^2$ integral inner-product, but I was wondering if anyone could enlighten me with a more interesting example. I don't have much background, but I'm very interested to hear about this topic, and I'd appreciate it if you could give full explanations so I could understand. Thanks in advance","I was wondering if anyone could give me some interesting ""counter examples"" to the Riesz representation theorem about functionals over Hilbert spaces. When I say counter examples, I'm obviously talking about examples where some of the basic assumptions of the theorem aren't met, so the theorem doesn't hold. In other words - could you show me some non-trivial examples of functionals over inner-product spaces that cannot be expressed as an inner-product with some vector in the vector space? I already have an example from $C [0, 1]$ based on the standard $L^2$ integral inner-product, but I was wondering if anyone could enlighten me with a more interesting example. I don't have much background, but I'm very interested to hear about this topic, and I'd appreciate it if you could give full explanations so I could understand. Thanks in advance",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'examples-counterexamples', 'riesz-representation-theorem']"
83,Geometric Proof of Perron-Frobenius II,Geometric Proof of Perron-Frobenius II,,"The following is proved in these lecture notes. Let $A$ be an $n\times n$ real matrix with all entries positive. Then $A$ has a unique positive eigenvector (up to positve scaling), and the eigenvalue of $A$ corresponding to this eigenvector is greater than every other real eigenvalue in absolute value. By a positive vector we mean a vector all of whose entries are positive. (I am alluding the statement made alongside ""Perron-Frobenius Theorem"" in the link provided. What I have stated is not quite what is stated in the PDF). The proof given goes as follows. Let $X$ be the set of all the points $(x_1, \ldots, x_n)\in\mathbf R^n$ such that each $x_i\geq 0$ , and $\sum x_i^2=1$ . Define $T:X\to X$ as $Tv=Av/\|Av\|_2$ . Then $T$ is a contraction (I haven't yet verified this but this seems intuitively reasonable). Therefore, by Banach Fixed Point theorem, there is a unique vector $v\in X$ which is fixed by $T$ . Thus $v$ is an eigenvector of $A$ and say $Av=\lambda v$ . Now let $\mu$ be any other real eigenvalue of $A$ , and let $w=(w_1, \ldots, w_n)$ be a corresponding eigenvector. At least one entry of $w$ is negative. Write $|w|$ to denote the vector whose $i$ -th entry is $|w_i|$ . Here is the statement I am unable to understand Then $A(|w|)$ has smaller length than $\lambda L$ , where $L$ is the length of $w$ . Can somebody explain why is this true? It seems what is really used here is that $\|Au\|\leq \lambda \|u\|_2$ for each $u\in X$ , but I don't see why is this true. A related discussion is here .","The following is proved in these lecture notes. Let be an real matrix with all entries positive. Then has a unique positive eigenvector (up to positve scaling), and the eigenvalue of corresponding to this eigenvector is greater than every other real eigenvalue in absolute value. By a positive vector we mean a vector all of whose entries are positive. (I am alluding the statement made alongside ""Perron-Frobenius Theorem"" in the link provided. What I have stated is not quite what is stated in the PDF). The proof given goes as follows. Let be the set of all the points such that each , and . Define as . Then is a contraction (I haven't yet verified this but this seems intuitively reasonable). Therefore, by Banach Fixed Point theorem, there is a unique vector which is fixed by . Thus is an eigenvector of and say . Now let be any other real eigenvalue of , and let be a corresponding eigenvector. At least one entry of is negative. Write to denote the vector whose -th entry is . Here is the statement I am unable to understand Then has smaller length than , where is the length of . Can somebody explain why is this true? It seems what is really used here is that for each , but I don't see why is this true. A related discussion is here .","A n\times n A A X (x_1, \ldots, x_n)\in\mathbf R^n x_i\geq 0 \sum x_i^2=1 T:X\to X Tv=Av/\|Av\|_2 T v\in X T v A Av=\lambda v \mu A w=(w_1, \ldots, w_n) w |w| i |w_i| A(|w|) \lambda L L w \|Au\|\leq \lambda \|u\|_2 u\in X","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'fixed-point-theorems', 'positive-matrices']"
84,What's the relationship between free variables and nullspaces?,What's the relationship between free variables and nullspaces?,,I know what a free variable and a basic variable is. I understand free variables show up because there is a lack of a pivot. I understand a pivot represents a solution in a subspace that's not a nullspace. So my question is do free variables represent a solution in a nullspace? Is there some connection between that and a reduced row echelon form that would be easy to understand?,I know what a free variable and a basic variable is. I understand free variables show up because there is a lack of a pivot. I understand a pivot represents a solution in a subspace that's not a nullspace. So my question is do free variables represent a solution in a nullspace? Is there some connection between that and a reduced row echelon form that would be easy to understand?,,['linear-algebra']
85,linear independent over $\mathbb{Q}$,linear independent over,\mathbb{Q},"Let $r_1, r_2, \cdots, r_n$ be distinct rational numbers in the interval $(0,1)$. How to prove that in the space $\mathbb{R}$ over $\mathbb{Q}$ the numbers $2^{r_1}, \cdots, 2^{r_n}$ are independent?","Let $r_1, r_2, \cdots, r_n$ be distinct rational numbers in the interval $(0,1)$. How to prove that in the space $\mathbb{R}$ over $\mathbb{Q}$ the numbers $2^{r_1}, \cdots, 2^{r_n}$ are independent?",,['linear-algebra']
86,Explicit relation between dual and adjoint of a linear map,Explicit relation between dual and adjoint of a linear map,,"Let $V$ and $W$ be finite-dimensional vector spaces over some arbitrary field $K$, and let $V^\ast$ and $W^\ast$ be their respective dual spaces. Let $f:V \rightarrow W$ be a linear map. a) Define the dual of $f$ as the map $f^\ast : W^\ast \rightarrow V^\ast$, $e \mapsto e \circ f$. b) Suppose we define two fixed non-degenerate bilinear forms $\langle \cdot,\cdot \rangle_V : V^2 \rightarrow K$, $\langle \cdot,\cdot \rangle_W : W^2 \rightarrow K$. Define the adjoint of $f$ as the map $\bar{f}: W \rightarrow V$ satisfying $\langle v,\bar{f}(w) \rangle_V = \langle f(v),w \rangle_W$. Since the bilinear forms are non-degenerate, the linear maps $\phi_V : V \rightarrow V^\ast$ and $\phi_W : W \rightarrow W^\ast$ given by $$ \phi_V(v)(v_0) = \langle v,v_0 \rangle_V, \qquad \phi_W(w)(w_0) = \langle w,w_0 \rangle_W $$ are isomorphisms. Given this, how can we express $\bar{f}$ explicitly in terms of $f^\ast$ (and if required, $\phi_V$ and $\phi_W$)? Alternatively, how can we prove that the map $\bar{f}$ is guaranteed to exist? EDIT: As pointed out by @levap I have fixed the definition of $\phi_W$.","Let $V$ and $W$ be finite-dimensional vector spaces over some arbitrary field $K$, and let $V^\ast$ and $W^\ast$ be their respective dual spaces. Let $f:V \rightarrow W$ be a linear map. a) Define the dual of $f$ as the map $f^\ast : W^\ast \rightarrow V^\ast$, $e \mapsto e \circ f$. b) Suppose we define two fixed non-degenerate bilinear forms $\langle \cdot,\cdot \rangle_V : V^2 \rightarrow K$, $\langle \cdot,\cdot \rangle_W : W^2 \rightarrow K$. Define the adjoint of $f$ as the map $\bar{f}: W \rightarrow V$ satisfying $\langle v,\bar{f}(w) \rangle_V = \langle f(v),w \rangle_W$. Since the bilinear forms are non-degenerate, the linear maps $\phi_V : V \rightarrow V^\ast$ and $\phi_W : W \rightarrow W^\ast$ given by $$ \phi_V(v)(v_0) = \langle v,v_0 \rangle_V, \qquad \phi_W(w)(w_0) = \langle w,w_0 \rangle_W $$ are isomorphisms. Given this, how can we express $\bar{f}$ explicitly in terms of $f^\ast$ (and if required, $\phi_V$ and $\phi_W$)? Alternatively, how can we prove that the map $\bar{f}$ is guaranteed to exist? EDIT: As pointed out by @levap I have fixed the definition of $\phi_W$.",,"['linear-algebra', 'linear-transformations']"
87,Help Showing that the Adjoint Operator $T^*$ is Surjective if and only if $T$ is Injective,Help Showing that the Adjoint Operator  is Surjective if and only if  is Injective,T^* T,"Let $T\in L(V,W)$,where $L(V,W)$ denotes a linear map from a vector space $V$ to vector space $W$. I want to prove that $T$ is injective iff $T^*$ is surjective, where $T^*$ is the adjoint of $T$. I start with the definition of adjoint: $\langle w,Tv \rangle= \langle T^*w,v \rangle$ for all $w \in W $, $v\in V$. What should I do next? Take $v=0$?","Let $T\in L(V,W)$,where $L(V,W)$ denotes a linear map from a vector space $V$ to vector space $W$. I want to prove that $T$ is injective iff $T^*$ is surjective, where $T^*$ is the adjoint of $T$. I start with the definition of adjoint: $\langle w,Tv \rangle= \langle T^*w,v \rangle$ for all $w \in W $, $v\in V$. What should I do next? Take $v=0$?",,['linear-algebra']
88,"Show that the system $\dot{x}=Ax+Bu$ is controllable if the linear system of equations $XA-AX=0$, $XB=0$ admits only the trivial solution $X = 0$","Show that the system  is controllable if the linear system of equations ,  admits only the trivial solution",\dot{x}=Ax+Bu XA-AX=0 XB=0 X = 0,"Show that the system $\dot{x}=Ax+Bu$ is controllable if the linear system of equations $XA-AX=0$, $XB=0$ admits only the trivial solution $X = 0$ . (The system is controllable iff the controllability matrix $[B, AB, A^{2}B, ..., A^{n-1}B]$ has full row rank)","Show that the system $\dot{x}=Ax+Bu$ is controllable if the linear system of equations $XA-AX=0$, $XB=0$ admits only the trivial solution $X = 0$ . (The system is controllable iff the controllability matrix $[B, AB, A^{2}B, ..., A^{n-1}B]$ has full row rank)",,"['linear-algebra', 'systems-of-equations', 'control-theory']"
89,A question about Cayley-Hamilton's density proof.,A question about Cayley-Hamilton's density proof.,,"The usual Cayley-Hamilton proof using density is something like this: Lemma: Let $f,g:X\to Y$ be two continuous functions in metric spaces $X$ and $Y$. If $f(x)=g(x)$ for all $x\in E$, where $E$ is a dense subset of $X$, then $f=g$. Let $\chi_A$ be the characteristic polynomial of $A$. Since it is trivial to prove Cayley-Hamilton for diagonalizable matrices and the set of all diagonalizable matrices is dense in $M_n(\mathbb{C})$ we can argue as follows.   If $A$ is any matrix, there is a sequence of diagonalizable matrices $A_k$ such that $A_k\to A$. Hence, $\chi_{A_k}(A_k)\to\chi_A(A)$ by our lemma. Since $\chi_{A_k}(A_k)=O_n$ (because CH holds for diagonalizable matrices), it follows that $\chi_A(A)=O_n$. My problem with this proof is the following step: ""Hence, $\chi_{A_k}(A_k)\to\chi_A(A)$ by our lemma."" I do not find this obvious. Why is the function $f(A)=\chi_A(A)$ continuous? It is clear that, for a fixed matrix $B$, the function $g(A)=\chi_B(A)$ is continuous as it is a polynomial. However that does not seem to be enough. Can someone clarify this for me? Thanks. EDIT: I further explained my trouble. Consider the functions $f(A)=\chi_A$ and $g(p)=p(A)$. $f$ maps $A$ to its characteristic polynomial and $g$ gets some polynomial and applies $A$ to it.   Clearly the map $A\mapsto \chi_A(A)$ is the function $g\circ f$. $f$ is continuous since $f(A)=\det(xI-A)$. That is, $f$ is polynomial in the entries of $A$. However, why does $g$ is continuous? How can I prove it?","The usual Cayley-Hamilton proof using density is something like this: Lemma: Let $f,g:X\to Y$ be two continuous functions in metric spaces $X$ and $Y$. If $f(x)=g(x)$ for all $x\in E$, where $E$ is a dense subset of $X$, then $f=g$. Let $\chi_A$ be the characteristic polynomial of $A$. Since it is trivial to prove Cayley-Hamilton for diagonalizable matrices and the set of all diagonalizable matrices is dense in $M_n(\mathbb{C})$ we can argue as follows.   If $A$ is any matrix, there is a sequence of diagonalizable matrices $A_k$ such that $A_k\to A$. Hence, $\chi_{A_k}(A_k)\to\chi_A(A)$ by our lemma. Since $\chi_{A_k}(A_k)=O_n$ (because CH holds for diagonalizable matrices), it follows that $\chi_A(A)=O_n$. My problem with this proof is the following step: ""Hence, $\chi_{A_k}(A_k)\to\chi_A(A)$ by our lemma."" I do not find this obvious. Why is the function $f(A)=\chi_A(A)$ continuous? It is clear that, for a fixed matrix $B$, the function $g(A)=\chi_B(A)$ is continuous as it is a polynomial. However that does not seem to be enough. Can someone clarify this for me? Thanks. EDIT: I further explained my trouble. Consider the functions $f(A)=\chi_A$ and $g(p)=p(A)$. $f$ maps $A$ to its characteristic polynomial and $g$ gets some polynomial and applies $A$ to it.   Clearly the map $A\mapsto \chi_A(A)$ is the function $g\circ f$. $f$ is continuous since $f(A)=\det(xI-A)$. That is, $f$ is polynomial in the entries of $A$. However, why does $g$ is continuous? How can I prove it?",,"['linear-algebra', 'general-topology']"
90,Expressing invertible maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ as $\bigwedge^{d-1}A$ for some $A$,Expressing invertible maps  as  for some,\bigwedge^{d-1} V \to \bigwedge^{d-1} V \bigwedge^{d-1}A A,"Let $V$ be a real $d$-dimensional vector space, let $\bigwedge^{d-1} V$ be its exterior power. Consider the following claim: Proposition: If $d$ is even, then every invertible linear map $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ equals $\bigwedge^{d-1}A$ for some $A \in \text{GL}(V)$. If $d$ is odd, then every orientation-preserving*  invertible map $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ equals $\bigwedge^kA$ for some $A \in \text{GL}(V)$. I found a proof for the above proposition, but it is based on endowing $V$ with an inner product, which I don't like very much.  Since there is no mention of products in the claim, it's natural to expect a metric-free proof. Is there such a proof? Edit: Here is an argument for showing that when $d$ is odd, it is impossible to express orientation-reversing maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ as ""$(d-1)$-wedge"" of a map $V \to V$. Let $A:V \to V$. Since $$\det (\bigwedge^k A)=(\det A)^{\binom{d-1}{k-1}},$$ we get for $k=d-1$ that $$ \det (\bigwedge^{d-1} A)=(\det A)^{\binom{d-1}{d-2}}=(\det A)^{d-1},$$ so if $d$ is odd, we see that $\det (\bigwedge^{d-1} A)$ is always positive, whether or not $A$ was orientation-preserving to begin with. *Note there is no need for a choice of orientation on $\bigwedge^{d-1} V$ to define which maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ are orientation preserving. (If you like you can put the same orientation on ""both sides"", it does not matter which).","Let $V$ be a real $d$-dimensional vector space, let $\bigwedge^{d-1} V$ be its exterior power. Consider the following claim: Proposition: If $d$ is even, then every invertible linear map $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ equals $\bigwedge^{d-1}A$ for some $A \in \text{GL}(V)$. If $d$ is odd, then every orientation-preserving*  invertible map $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ equals $\bigwedge^kA$ for some $A \in \text{GL}(V)$. I found a proof for the above proposition, but it is based on endowing $V$ with an inner product, which I don't like very much.  Since there is no mention of products in the claim, it's natural to expect a metric-free proof. Is there such a proof? Edit: Here is an argument for showing that when $d$ is odd, it is impossible to express orientation-reversing maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ as ""$(d-1)$-wedge"" of a map $V \to V$. Let $A:V \to V$. Since $$\det (\bigwedge^k A)=(\det A)^{\binom{d-1}{k-1}},$$ we get for $k=d-1$ that $$ \det (\bigwedge^{d-1} A)=(\det A)^{\binom{d-1}{d-2}}=(\det A)^{d-1},$$ so if $d$ is odd, we see that $\det (\bigwedge^{d-1} A)$ is always positive, whether or not $A$ was orientation-preserving to begin with. *Note there is no need for a choice of orientation on $\bigwedge^{d-1} V$ to define which maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ are orientation preserving. (If you like you can put the same orientation on ""both sides"", it does not matter which).",,"['linear-algebra', 'differential-geometry', 'exterior-algebra', 'orientation']"
91,Intuition behind row vectors of orthonormal matrix being an orthonormal basis,Intuition behind row vectors of orthonormal matrix being an orthonormal basis,,"By definition, in an orthonormal matrix, all the column vectors are unit vectors and mutually orthogonal. However, the row vectors also turn out to be an orthonormal basis. I know how to prove it mathematically, but is there any intuition or geometric interpretation behind this observation?","By definition, in an orthonormal matrix, all the column vectors are unit vectors and mutually orthogonal. However, the row vectors also turn out to be an orthonormal basis. I know how to prove it mathematically, but is there any intuition or geometric interpretation behind this observation?",,"['linear-algebra', 'matrices', 'vector-spaces']"
92,"How to find an integer matrix $P$ such that $PAP^{-1}=B$ for given two similar, integer matrices $A$ and $B$?","How to find an integer matrix  such that  for given two similar, integer matrices  and ?",P PAP^{-1}=B A B,"An integer matrix is a matrix whose coefficients are integers. Suppose that two given invertible, integer $3\times 3$ matrices $A$ and $B$ are similar to each other, that is, there exists an invertible, integer matrix $P$ such that $PAP^{-1}=B$. Assuming that we know such a matrix $P$ exists, how can we concretely find $P$? I would like to know how to find $P$ for the case that $A=\begin{bmatrix}0&25&37\\ 0&2&3\\1&0&38\end{bmatrix}$ and $B=\begin{bmatrix}0&23&297\\ 0&12&155\\1&0&28\end{bmatrix}$. (One can check that $A$ and $B$ are similar by using a theorem of Latimer and MacDufee.)","An integer matrix is a matrix whose coefficients are integers. Suppose that two given invertible, integer $3\times 3$ matrices $A$ and $B$ are similar to each other, that is, there exists an invertible, integer matrix $P$ such that $PAP^{-1}=B$. Assuming that we know such a matrix $P$ exists, how can we concretely find $P$? I would like to know how to find $P$ for the case that $A=\begin{bmatrix}0&25&37\\ 0&2&3\\1&0&38\end{bmatrix}$ and $B=\begin{bmatrix}0&23&297\\ 0&12&155\\1&0&28\end{bmatrix}$. (One can check that $A$ and $B$ are similar by using a theorem of Latimer and MacDufee.)",,"['linear-algebra', 'matrices']"
93,What relates to multi-linearity in the same way differentiability relates to linearity?,What relates to multi-linearity in the same way differentiability relates to linearity?,,"We know that a differential of a function existing at a point implies that said function can be approximated as a linear function near this point (local linearity). What is the equivalent notion that allows a function to be approximated by a multilinear function locally , does such a notion even exist? Side question that came to mind:   Another way of looking a linearity is as a map that preserves a Vector space structure (maps a vector space into a vector space). Analogously what abstract structure or space, tied with a structure preserving map, is related to multi-linearity. Sorry if my Jargon is not accurate, but i hope the gist of my question is clear.","We know that a differential of a function existing at a point implies that said function can be approximated as a linear function near this point (local linearity). What is the equivalent notion that allows a function to be approximated by a multilinear function locally , does such a notion even exist? Side question that came to mind:   Another way of looking a linearity is as a map that preserves a Vector space structure (maps a vector space into a vector space). Analogously what abstract structure or space, tied with a structure preserving map, is related to multi-linearity. Sorry if my Jargon is not accurate, but i hope the gist of my question is clear.",,"['linear-algebra', 'multivariable-calculus', 'multilinear-algebra']"
94,When is $(BA)^\dagger\neq A^\dagger B^\dagger$ for the Moore Penrose inverse?,When is  for the Moore Penrose inverse?,(BA)^\dagger\neq A^\dagger B^\dagger,"I wanted to find real matrices $A$ and $B$ such that:$$(BA)^\dagger\neq A^\dagger B^\dagger$$ whereas $A^\dagger$ denotes the Moore-Penrose pseudoinverse of a matrix. I tried some things and ended up with: $$ B = \begin{pmatrix} 0 & 0\\ 1 & 2\end{pmatrix} , A=\begin{pmatrix} 1 & 0 \\ 0 & 0\end{pmatrix} $$ for these matrices I get $$A^\dagger=\begin{pmatrix} 1 & 0\\ 0 & 0\end{pmatrix}, B^\dagger = \begin{pmatrix} 0 & \frac 15 \\ 0 & \frac 25\end{pmatrix}, A^\dagger B^\dagger = \begin{pmatrix} 0 & \frac 15 \\ 0 & 0\end{pmatrix}$$ but  $$(BA)^\dagger=\begin{pmatrix}0 & 1\\ 0 & 0\end{pmatrix}$$ so these two are obviously not equal. I found on Wikipedia that the equality stated above holds, when for example $B$ or $A$ has orthonormal columns or rows. What I am trying to find out here is: Is there a criterion such that the equality does $\bf not$ hold? When trying to find such matrices I just stumbled upon these by accident, say I just picked random matrices which did not fulfill the requirements that the equality automatically holds. Any help is greatly appreciated!","I wanted to find real matrices $A$ and $B$ such that:$$(BA)^\dagger\neq A^\dagger B^\dagger$$ whereas $A^\dagger$ denotes the Moore-Penrose pseudoinverse of a matrix. I tried some things and ended up with: $$ B = \begin{pmatrix} 0 & 0\\ 1 & 2\end{pmatrix} , A=\begin{pmatrix} 1 & 0 \\ 0 & 0\end{pmatrix} $$ for these matrices I get $$A^\dagger=\begin{pmatrix} 1 & 0\\ 0 & 0\end{pmatrix}, B^\dagger = \begin{pmatrix} 0 & \frac 15 \\ 0 & \frac 25\end{pmatrix}, A^\dagger B^\dagger = \begin{pmatrix} 0 & \frac 15 \\ 0 & 0\end{pmatrix}$$ but  $$(BA)^\dagger=\begin{pmatrix}0 & 1\\ 0 & 0\end{pmatrix}$$ so these two are obviously not equal. I found on Wikipedia that the equality stated above holds, when for example $B$ or $A$ has orthonormal columns or rows. What I am trying to find out here is: Is there a criterion such that the equality does $\bf not$ hold? When trying to find such matrices I just stumbled upon these by accident, say I just picked random matrices which did not fulfill the requirements that the equality automatically holds. Any help is greatly appreciated!",,"['linear-algebra', 'inverse', 'pseudoinverse']"
95,Find a basis for orthogonal complement in R⁴,Find a basis for orthogonal complement in R⁴,,"How do I approach part 2? I found the projection of 1. to be (6,-2,2,-2) but what do I do now?","How do I approach part 2? I found the projection of 1. to be (6,-2,2,-2) but what do I do now?",,"['linear-algebra', 'orthogonality']"
96,SVD in the language of linear operators?,SVD in the language of linear operators?,,"I noticed that Axler's Linear Algebra Done Right has an explanation of SVD in a matrix free way. The statement of the theorem is the following Given $T\in L(V)$ there are orthonormal basis $(e_1,\ldots,e_n)$ and $(f_1,\ldots,f_n)$ of $V$ such that $Tv = s_1\langle v,e_1\rangle f_1+\ldots+s_n\langle v,e_n\rangle f_n$, where $s_1,\ldots,s_n$ are the singular values of $T$. This is easy to prove by just letting $(e_1,\ldots,e_n)$ the an orthonormal basis for $T^*T$, which is guaranteed to have such a basis being self-adjoint. We then apply the polar composition by writing $T=S\sqrt{T^*T}$, where $S$ is an isometry, so it preserves orthonormality of vectors and we can set $f_i = Sf_i$ and seeing that $Tv$ has the required form is then a trivial computation. My question is the following: In most applications of SVD, where we do some form of dimensionality reduction, we do not have a square matrix, so the assumption $T\in L(V)$ doesn't hold and we would instead need to generalize this to the setting $T\in L(V,W)$, where $V$ and $W$ are vector spaces of possible different dimensions. Is there a nice formulation of the SVD in this setting comparable to the one above?","I noticed that Axler's Linear Algebra Done Right has an explanation of SVD in a matrix free way. The statement of the theorem is the following Given $T\in L(V)$ there are orthonormal basis $(e_1,\ldots,e_n)$ and $(f_1,\ldots,f_n)$ of $V$ such that $Tv = s_1\langle v,e_1\rangle f_1+\ldots+s_n\langle v,e_n\rangle f_n$, where $s_1,\ldots,s_n$ are the singular values of $T$. This is easy to prove by just letting $(e_1,\ldots,e_n)$ the an orthonormal basis for $T^*T$, which is guaranteed to have such a basis being self-adjoint. We then apply the polar composition by writing $T=S\sqrt{T^*T}$, where $S$ is an isometry, so it preserves orthonormality of vectors and we can set $f_i = Sf_i$ and seeing that $Tv$ has the required form is then a trivial computation. My question is the following: In most applications of SVD, where we do some form of dimensionality reduction, we do not have a square matrix, so the assumption $T\in L(V)$ doesn't hold and we would instead need to generalize this to the setting $T\in L(V,W)$, where $V$ and $W$ are vector spaces of possible different dimensions. Is there a nice formulation of the SVD in this setting comparable to the one above?",,"['linear-algebra', 'svd']"
97,Minimal Polynomial of Inverse [duplicate],Minimal Polynomial of Inverse [duplicate],,"This question already has answers here : What is the minimal polynomial of $T^{-1}$? (2 answers) Closed 2 years ago . Suppose $T$ has minimal polynomial $x^m+a_{m-1}x^{m-1}+...+a_1x+a_0$ and $T$ is invertible (hence $a_0\not=0$). Is it true that the minimal polynomial of $T^{-1}$ is $\frac{1}{a_0}(1+a_{m-1}x+...+a_1x^{m-1})+x^m$? My thought was that since $T^m+a_{m-1}T^{m-1}+...+a_1T+a_0I=0$, we have $I+a_{m-1}T^{-1}+...+a_1T^{m-1}+a_0T^{-m}=0\implies\frac{1}{a_0}(I+a_{m-1}T^{-1}+...+a_1T^{m-1})+T^{-m}=0$. So the minimal polynomial of $T^{-1}$ divides $\frac{1}{a_0}(1+a_{m-1}x+...+a_1x^{m-1})+x^m$ and so (degree of minimal polynomial of $T^{-1}$)$\leq m=$(degree of minimal polynomial of $T$). On the other hand, if $x^i+b_{i-1}x^{i-1}+...+b_1x+b_0$ is the minimal polynomial of $T^{-1}$ (so $b_0\not=0$), then $0=T^{-i}+b_{i-1}T^{-i+1}+...+b_1T^{-1}+b_0I\implies 0=I+b_{i-1}T+...+b_1T^{i-1}+b_0T^i$ so the minimal polynomial of $T$ divides $b_0x^i+b_1x^{i-1}+...+b_{i-1}x+1$ and so $m=$(degree of minimal polynomial of $T$)$\leq$(degree of minimal polynomial of $T^{-1}$ Hence, degree of minimal polynomial of $T^{-1}$ is $m$ and $\frac{1}{a_0}(1+a_{m-1}x+...+a_1x^{m-1})+x^m$ is a monic polynomial of degree $m$ annihilating $T^{-1}$, so it is the minimal polynomial of $T^{-1}$. Does this look correct? Thank you","This question already has answers here : What is the minimal polynomial of $T^{-1}$? (2 answers) Closed 2 years ago . Suppose $T$ has minimal polynomial $x^m+a_{m-1}x^{m-1}+...+a_1x+a_0$ and $T$ is invertible (hence $a_0\not=0$). Is it true that the minimal polynomial of $T^{-1}$ is $\frac{1}{a_0}(1+a_{m-1}x+...+a_1x^{m-1})+x^m$? My thought was that since $T^m+a_{m-1}T^{m-1}+...+a_1T+a_0I=0$, we have $I+a_{m-1}T^{-1}+...+a_1T^{m-1}+a_0T^{-m}=0\implies\frac{1}{a_0}(I+a_{m-1}T^{-1}+...+a_1T^{m-1})+T^{-m}=0$. So the minimal polynomial of $T^{-1}$ divides $\frac{1}{a_0}(1+a_{m-1}x+...+a_1x^{m-1})+x^m$ and so (degree of minimal polynomial of $T^{-1}$)$\leq m=$(degree of minimal polynomial of $T$). On the other hand, if $x^i+b_{i-1}x^{i-1}+...+b_1x+b_0$ is the minimal polynomial of $T^{-1}$ (so $b_0\not=0$), then $0=T^{-i}+b_{i-1}T^{-i+1}+...+b_1T^{-1}+b_0I\implies 0=I+b_{i-1}T+...+b_1T^{i-1}+b_0T^i$ so the minimal polynomial of $T$ divides $b_0x^i+b_1x^{i-1}+...+b_{i-1}x+1$ and so $m=$(degree of minimal polynomial of $T$)$\leq$(degree of minimal polynomial of $T^{-1}$ Hence, degree of minimal polynomial of $T^{-1}$ is $m$ and $\frac{1}{a_0}(1+a_{m-1}x+...+a_1x^{m-1})+x^m$ is a monic polynomial of degree $m$ annihilating $T^{-1}$, so it is the minimal polynomial of $T^{-1}$. Does this look correct? Thank you",,"['linear-algebra', 'proof-verification', 'minimal-polynomials']"
98,Help fit a similar claim into Farka's lemma.,Help fit a similar claim into Farka's lemma.,,"I found the following implication of Farka's lemma in Wikipedia , If $Ax \le b$ has not solution, then there exists $y$ such that $y^TA=0,y^T=-1$. The claim we want to show is similar but a little bit different (see ""previously asked"" below for more). If system $A_1x \ge 0, ..., A_Nx \ge 0, f^Tx=1$ has no solution,then there exists vectors $y_1,...,y_N$ and a real number $\mu > 0$ s.t. $y_1^TA_1+...+y_N^TA_N+\mu f=0$ I have trouble to completely fit this claim to above implication of Farka's lemma. Please help. Previously asked : See the following. Can anyone help point out what version of Farka's lemma this is using? I check my textbook and find all mention of Farka's lemma but I have trouble to match them with the claim in the proof.","I found the following implication of Farka's lemma in Wikipedia , If $Ax \le b$ has not solution, then there exists $y$ such that $y^TA=0,y^T=-1$. The claim we want to show is similar but a little bit different (see ""previously asked"" below for more). If system $A_1x \ge 0, ..., A_Nx \ge 0, f^Tx=1$ has no solution,then there exists vectors $y_1,...,y_N$ and a real number $\mu > 0$ s.t. $y_1^TA_1+...+y_N^TA_N+\mu f=0$ I have trouble to completely fit this claim to above implication of Farka's lemma. Please help. Previously asked : See the following. Can anyone help point out what version of Farka's lemma this is using? I check my textbook and find all mention of Farka's lemma but I have trouble to match them with the claim in the proof.",,"['linear-algebra', 'optimization', 'convex-analysis', 'convex-optimization']"
99,Show that the determinant of any outer product is 0,Show that the determinant of any outer product is 0,,"I want to show that for any two vectors $a, b \in \mathbb{R}^n$, and given $M = ab^T$, $\det(M) = 0$. I've come up with the following proof, but I'm not sure if it's sufficient; could someone double-check it? Given $M = ab^T$, the system of equations $Mx = 0$ is equivalent to $ab^Tx = a(b \cdot x) = 0$. Then there exists a nonzero vector $x$ that is orthogonal to $b$, and so $a(b \cdot x) = a \times 0 = 0$. The existence of a nonzero $x$ such that $Mx = 0$ implies that $M$ is singular, and therefore $\det(M) = 0$. Is this proof correct? More interestingly, is there a much simpler proof that I'm missing?","I want to show that for any two vectors $a, b \in \mathbb{R}^n$, and given $M = ab^T$, $\det(M) = 0$. I've come up with the following proof, but I'm not sure if it's sufficient; could someone double-check it? Given $M = ab^T$, the system of equations $Mx = 0$ is equivalent to $ab^Tx = a(b \cdot x) = 0$. Then there exists a nonzero vector $x$ that is orthogonal to $b$, and so $a(b \cdot x) = a \times 0 = 0$. The existence of a nonzero $x$ such that $Mx = 0$ implies that $M$ is singular, and therefore $\det(M) = 0$. Is this proof correct? More interestingly, is there a much simpler proof that I'm missing?",,"['linear-algebra', 'determinant']"
