,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is the following matrix defined by the roots of Chebyshev polynomial invertible?,Is the following matrix defined by the roots of Chebyshev polynomial invertible?,,"Let $x_0, \dots , x_n$ be the roots of the Chebyshev polynomial $T_{n+1}(x)$ . We define: $$A=\begin{pmatrix}  \frac{1}{\sqrt2}T_0(x_0) & \cdots & \frac{1}{\sqrt2}T_0(x_n) \\  T_1(x_0) & \cdots & T_1(x_n) \\  \vdots  & \vdots& \vdots \\  T_n(x_0) & \cdots & T_n(x_n) \\  \end{pmatrix}$$ Is $A$ invertible? If so, calculate $A^{-1}$ . I have tried to solve it for the roots of $T_2(x)$ and I found that the matrix is invertible. How can I generalize this?","Let be the roots of the Chebyshev polynomial . We define: Is invertible? If so, calculate . I have tried to solve it for the roots of and I found that the matrix is invertible. How can I generalize this?","x_0, \dots , x_n T_{n+1}(x) A=\begin{pmatrix}
 \frac{1}{\sqrt2}T_0(x_0) & \cdots & \frac{1}{\sqrt2}T_0(x_n) \\
 T_1(x_0) & \cdots & T_1(x_n) \\
 \vdots  & \vdots& \vdots \\
 T_n(x_0) & \cdots & T_n(x_n) \\
 \end{pmatrix} A A^{-1} T_2(x)","['linear-algebra', 'matrices', 'orthogonal-polynomials', 'chebyshev-polynomials']"
1,Powers of bidiagonal Toeplitz matrix,Powers of bidiagonal Toeplitz matrix,,"Consider the following bidiagonal $n \times n$ Toeplitz matrix $A$ $$A = \begin{bmatrix}   1-p & 0 & 0 & \cdots & 0\\   p & 1-p & 0 && \vdots \\   0 & \ddots & \ddots & \ddots & 0 \\   \vdots && p & 1-p & 0\\   0 & \cdots & 0 & p & 1-p  \end{bmatrix}$$ where $0 < p < 1$ . What is $A^m$ for any $m \ge 2$ ? It's easy to show what the matrix is when $n = 2$ for all $m$ , but not for general $n$ . I have seen several papers on powers of tridiagonal Toeplitz matrices but they assume that the off-by- $1$ diagonals are all nonzero, but the ""upper"" diagonal here is all $0$ .","Consider the following bidiagonal Toeplitz matrix where . What is for any ? It's easy to show what the matrix is when for all , but not for general . I have seen several papers on powers of tridiagonal Toeplitz matrices but they assume that the off-by- diagonals are all nonzero, but the ""upper"" diagonal here is all .","n \times n A A = \begin{bmatrix}
  1-p & 0 & 0 & \cdots & 0\\
  p & 1-p & 0 && \vdots \\
  0 & \ddots & \ddots & \ddots & 0 \\
  \vdots && p & 1-p & 0\\
  0 & \cdots & 0 & p & 1-p
 \end{bmatrix} 0 < p < 1 A^m m \ge 2 n = 2 m n 1 0","['matrices', 'toeplitz-matrices']"
2,The relationship between vector space dualization and matrix transposition.,The relationship between vector space dualization and matrix transposition.,,"Let $\mathbb{F}$ be a field. The category of matrices $\mathbf{Mat}$ has $\mathbb{N}_0$ as class of objects and $\mathrm{hom}(n,m)=\mathbb{F}^{n\times m}$ for $n,m\in\mathbb{N}_0$ , with $\mathrm{id}_n=\mathbf{1}_n$ and $B\circ A=A\cdot B$ with the usual matrix multiplication whenever defined. The category of finite-dimensional vector spaces $\mathbf{FinVect}$ has $\mathbb{F}$ -vector spaces as objects and linear maps as morphisms, where composition is the usual composition of functions and the identity morphism is the identity map. The categories $\mathbf{Mat}$ and $\mathbf{FinVect}$ are equivalent. There is a contravariant duality functor $(-)^\ast\colon\mathbf{FinVect}\rightarrow\mathbf{FinVect}$ with $V^\ast=\mathrm{Hom}(V,\mathbb{F})$ for any vector space $V$ and $f^\ast\colon W^\ast\rightarrow V^\ast,\,g\mapsto g\circ f$ for any linear map $f\colon V\rightarrow W$ . There also is a contravariant transpose functor $(-)^t\colon\mathbf{Mat}\rightarrow\mathbf{Mat}$ where $n^t=n$ for $n\in\mathbb{N}_0$ and $A^t$ is the usual transpose of a matrix. These two contravariant functors on equivalent categories are related by the fact that for finite-dimensional vector spaces $V,W$ with bases $\mathscr{B},\mathscr{C}$ respectively, we have that $$(T^{\mathscr{B}}_{\mathscr{C}}(f))^t=T^{\mathscr{C}^\ast}_{\mathscr{B}^\ast}(f^\ast),$$ where $T$ denotes the transformation matrix for the respective bases. Is there a way to describe this relationship of functors in category-theoretical terms?","Let be a field. The category of matrices has as class of objects and for , with and with the usual matrix multiplication whenever defined. The category of finite-dimensional vector spaces has -vector spaces as objects and linear maps as morphisms, where composition is the usual composition of functions and the identity morphism is the identity map. The categories and are equivalent. There is a contravariant duality functor with for any vector space and for any linear map . There also is a contravariant transpose functor where for and is the usual transpose of a matrix. These two contravariant functors on equivalent categories are related by the fact that for finite-dimensional vector spaces with bases respectively, we have that where denotes the transformation matrix for the respective bases. Is there a way to describe this relationship of functors in category-theoretical terms?","\mathbb{F} \mathbf{Mat} \mathbb{N}_0 \mathrm{hom}(n,m)=\mathbb{F}^{n\times m} n,m\in\mathbb{N}_0 \mathrm{id}_n=\mathbf{1}_n B\circ A=A\cdot B \mathbf{FinVect} \mathbb{F} \mathbf{Mat} \mathbf{FinVect} (-)^\ast\colon\mathbf{FinVect}\rightarrow\mathbf{FinVect} V^\ast=\mathrm{Hom}(V,\mathbb{F}) V f^\ast\colon W^\ast\rightarrow V^\ast,\,g\mapsto g\circ f f\colon V\rightarrow W (-)^t\colon\mathbf{Mat}\rightarrow\mathbf{Mat} n^t=n n\in\mathbb{N}_0 A^t V,W \mathscr{B},\mathscr{C} (T^{\mathscr{B}}_{\mathscr{C}}(f))^t=T^{\mathscr{C}^\ast}_{\mathscr{B}^\ast}(f^\ast), T","['matrices', 'category-theory', 'dual-spaces']"
3,Correct notation for writing array reshape,Correct notation for writing array reshape,,"Suppose I am coding in python. Then one can do e.g. something like this: import numpy as np A = np.random.rand((6,6)) # Lets reshape it A_new = A.reshape(-1,3) So it went from 2D array (matrix) to a 2D array with six 12 rows and three columns. How would one write the re-shape operation formally in mathematical notation? Transpose is obviously easy, but is there a parallel for reshapes? $$\mathbf{A} \in \mathbb{R}^{6\times6} \rightarrow \mathbf{A} \in \mathbb{R}^{12 \times 3}$$ Thx","Suppose I am coding in python. Then one can do e.g. something like this: import numpy as np A = np.random.rand((6,6)) # Lets reshape it A_new = A.reshape(-1,3) So it went from 2D array (matrix) to a 2D array with six 12 rows and three columns. How would one write the re-shape operation formally in mathematical notation? Transpose is obviously easy, but is there a parallel for reshapes? Thx",\mathbf{A} \in \mathbb{R}^{6\times6} \rightarrow \mathbf{A} \in \mathbb{R}^{12 \times 3},"['matrices', 'notation', 'vectors', 'matrix-equations']"
4,"Let $A$ and $B$ be nilpotent matrices that commute with $[A,B]$. If $A$, $B$, and $[A,B]$ are all nilpotent, show that $A+B$ is nilpotent.","Let  and  be nilpotent matrices that commute with . If , , and  are all nilpotent, show that  is nilpotent.","A B [A,B] A B [A,B] A+B","How to prove that $A + B$ is nilpotent, when $A$ , $B$ , $[A, B]$ are nilpotent matrices, and also $A$ and $[A, B]$ , $B$ and $[A, B]$ are two pairs of commuting matrices? Looks like I should use binomial formula for commuting matrices, but if $$A  [A, B] = [A, B]  A \\ B [A, B] = [A, B] B,$$ does it mean that $A$ and $B$ commute? EDIT: Let $C = [A, B]$ , so $AB = BA + C$ , now we need to prove that every product of $A$ and $B$ is $C^kB^mA^n$ , if $k, m, n > 0$ . If it will be done, we can say, that with big $N$ (for example $2(k + m + n)$ ) every of the $2^N$ addends of the $(A + B)^N$ is equal to $0$ . So the matrix will be nilpotent. How can i prove this?","How to prove that is nilpotent, when , , are nilpotent matrices, and also and , and are two pairs of commuting matrices? Looks like I should use binomial formula for commuting matrices, but if does it mean that and commute? EDIT: Let , so , now we need to prove that every product of and is , if . If it will be done, we can say, that with big (for example ) every of the addends of the is equal to . So the matrix will be nilpotent. How can i prove this?","A + B A B [A, B] A [A, B] B [A, B] A  [A, B] = [A, B]  A \\ B [A, B] = [A, B] B, A B C = [A, B] AB = BA + C A B C^kB^mA^n k, m, n > 0 N 2(k + m + n) 2^N (A + B)^N 0","['linear-algebra', 'matrices', 'ring-theory', 'noncommutative-algebra', 'nilpotence']"
5,Show $\| X \| = \sqrt{X^* A X}$ is a norm,Show  is a norm,\| X \| = \sqrt{X^* A X},"I'm trying to show that, given a positive definite matrix $A\in \mathcal{M_n}(\mathbb{C})$, the function $$\begin{array}{cccl} \lVert \cdot \rVert: &\mathbb{C}^n &\longrightarrow &\mathbb{R} \\\ &X &\longmapsto & \lVert X \rVert = \sqrt{X^* A X} \end{array}$$ is a norm. I've proved all properties except $\| X+Y \| \leq \| X \|+\| Y \| \ \forall \ X,Y \in  \mathbb{C}^n$. $$\| X+Y \|= \sqrt{(X+Y)^* A (X+Y)}=\sqrt{(X^*+Y^*) A (X+Y)}=\sqrt{X^*AX+X^*AY+Y^*AX+Y^*AY}$$ $$\| X\| +\|Y \|= \sqrt{X^* A X} + \sqrt{Y^* A Y}$$ I don't know how to arrive to the inequality from here.","I'm trying to show that, given a positive definite matrix $A\in \mathcal{M_n}(\mathbb{C})$, the function $$\begin{array}{cccl} \lVert \cdot \rVert: &\mathbb{C}^n &\longrightarrow &\mathbb{R} \\\ &X &\longmapsto & \lVert X \rVert = \sqrt{X^* A X} \end{array}$$ is a norm. I've proved all properties except $\| X+Y \| \leq \| X \|+\| Y \| \ \forall \ X,Y \in  \mathbb{C}^n$. $$\| X+Y \|= \sqrt{(X+Y)^* A (X+Y)}=\sqrt{(X^*+Y^*) A (X+Y)}=\sqrt{X^*AX+X^*AY+Y^*AX+Y^*AY}$$ $$\| X\| +\|Y \|= \sqrt{X^* A X} + \sqrt{Y^* A Y}$$ I don't know how to arrive to the inequality from here.",,"['linear-algebra', 'matrices', 'normed-spaces']"
6,"If $Q$ is hermitian and unitary, prove that $Q=I-2P$ for some orthogonal projection $P$?","If  is hermitian and unitary, prove that  for some orthogonal projection ?",Q Q=I-2P P,"Good afternoon everyone. I am in a numerical linear algebra class, and have come across this problem: Prove that for $V$ a subspace of $\mathbb{C}^m$, $P$ the orthogonal projector onto $V$, and $Q=I-2P$, $Q=Q^*$ ($Q$ is hermitian) and $Q^*Q=QQ^*=I$ ($Q$ is unitary) and conversely, Prove that $\forall Q \in \mathbb{C}^{m\times m}. ((Q=Q^*)\land (Q^*Q=I) \implies \exists P \in \mathbb{C}^{m\times m}. (P=P^*) \land (Q=I-2P))$ or in other words If $Q$ is both hermitian and unitary then $Q=I-2P$ for some orthogonal projector $P$. I have succeeded in proving the first part of the problem by using some properties of the conjugate transpose (namely $(A+B)^*=A^*+B^*$) and some simple algebra, but the second one puzzles me. My paper literally just looks like this right now: $Q=Q^*$, $Q^*Q=I$ $Q^2=I$ And I'm stuck. Do I proceed by supposing a projection onto $V$? Do I somehow prove that $P$ is an orthogonal projector? Thanks much.","Good afternoon everyone. I am in a numerical linear algebra class, and have come across this problem: Prove that for $V$ a subspace of $\mathbb{C}^m$, $P$ the orthogonal projector onto $V$, and $Q=I-2P$, $Q=Q^*$ ($Q$ is hermitian) and $Q^*Q=QQ^*=I$ ($Q$ is unitary) and conversely, Prove that $\forall Q \in \mathbb{C}^{m\times m}. ((Q=Q^*)\land (Q^*Q=I) \implies \exists P \in \mathbb{C}^{m\times m}. (P=P^*) \land (Q=I-2P))$ or in other words If $Q$ is both hermitian and unitary then $Q=I-2P$ for some orthogonal projector $P$. I have succeeded in proving the first part of the problem by using some properties of the conjugate transpose (namely $(A+B)^*=A^*+B^*$) and some simple algebra, but the second one puzzles me. My paper literally just looks like this right now: $Q=Q^*$, $Q^*Q=I$ $Q^2=I$ And I'm stuck. Do I proceed by supposing a projection onto $V$? Do I somehow prove that $P$ is an orthogonal projector? Thanks much.",,"['linear-algebra', 'matrices', 'projection', 'projection-matrices']"
7,"Adding the constraint ""at least one entry of $x$ is $100$"" to a convex program","Adding the constraint ""at least one entry of  is "" to a convex program",x 100,"I have the following optimization problem in $x \in \mathbb R^p$ $$\begin{array}{ll} \text{minimize} & a^\top x\\ \text{subject to} & x \geq 0\end{array}$$ where $x \geq 0$ means that $x_1, x_2, \dots, x_p \geq 0$. To add the condition ""at least one entry of $x$ is $100$"", one option is to use the following $$(x_1−100) (x_2−100) \cdots (x_p−100)=0$$ But how do I represent this in matrix form? I used the example from the posted link. But looks like $$(x_1−100)(x_2−100) \cdots (x_p−100)=0$$ is difficult to express in matrix multiplication as it is not linear. The alternate solution, does not guarantee that value of 100 will be achieved.","I have the following optimization problem in $x \in \mathbb R^p$ $$\begin{array}{ll} \text{minimize} & a^\top x\\ \text{subject to} & x \geq 0\end{array}$$ where $x \geq 0$ means that $x_1, x_2, \dots, x_p \geq 0$. To add the condition ""at least one entry of $x$ is $100$"", one option is to use the following $$(x_1−100) (x_2−100) \cdots (x_p−100)=0$$ But how do I represent this in matrix form? I used the example from the posted link. But looks like $$(x_1−100)(x_2−100) \cdots (x_p−100)=0$$ is difficult to express in matrix multiplication as it is not linear. The alternate solution, does not guarantee that value of 100 will be achieved.",,"['matrices', 'optimization']"
8,Let $T: \mathbb{R}^4 \to \mathbb{R}^4$ be any linear transformation. Then how can I show that $T$ has a proper non zero invariant subspace. [duplicate],Let  be any linear transformation. Then how can I show that  has a proper non zero invariant subspace. [duplicate],T: \mathbb{R}^4 \to \mathbb{R}^4 T,This question already has an answer here : Invariant subspaces of specific dimension (1 answer) Closed 5 years ago . Let $T: \mathbb{R}^4 \to \mathbb{R}^4$ be any linear transformation. Then how can I show that $T$ has a proper non zero invariant subspace. If $T$ has an eigen value then it is clear but if not then I can't solve it. Please help me. Thanks.,This question already has an answer here : Invariant subspaces of specific dimension (1 answer) Closed 5 years ago . Let $T: \mathbb{R}^4 \to \mathbb{R}^4$ be any linear transformation. Then how can I show that $T$ has a proper non zero invariant subspace. If $T$ has an eigen value then it is clear but if not then I can't solve it. Please help me. Thanks.,,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations']"
9,Calculate matrix powering given one outer product: $(x\cdot{y}^T)^k$,Calculate matrix powering given one outer product:,(x\cdot{y}^T)^k,"It is an exercise on the chapter one of a book. Book: ""Matrix Computations 4th edition"" by Golub and Van Loan. It reads: Give an $O(n^2)$ algorithm for computing $C=(x\cdot{y}^T)^k$ where $x$ and $y$ are $n$-vectors. I did a lot of research because I am not great at linear algebra, I am just starting learning. I know somethings: $x\cdot{y}^T$ is a matrix, so this can be translated to matrix powering. However, I believe knowing one outer product gives more information than just a given matrix. I was trying to figure out if I can use that to my benefit, but I am not sure how. I also know algorithms for fast exponentiation, but they will not help here. I believe there is a solution, because this book seems very good and I read good reviews. I would love hints at it. Can I translate $(x\cdot{y}^T)^k$ to something more useful, for example?","It is an exercise on the chapter one of a book. Book: ""Matrix Computations 4th edition"" by Golub and Van Loan. It reads: Give an $O(n^2)$ algorithm for computing $C=(x\cdot{y}^T)^k$ where $x$ and $y$ are $n$-vectors. I did a lot of research because I am not great at linear algebra, I am just starting learning. I know somethings: $x\cdot{y}^T$ is a matrix, so this can be translated to matrix powering. However, I believe knowing one outer product gives more information than just a given matrix. I was trying to figure out if I can use that to my benefit, but I am not sure how. I also know algorithms for fast exponentiation, but they will not help here. I believe there is a solution, because this book seems very good and I read good reviews. I would love hints at it. Can I translate $(x\cdot{y}^T)^k$ to something more useful, for example?",,"['matrices', 'algorithms', 'outer-product']"
10,Determinant of $I-aa^T+bb^T$,Determinant of,I-aa^T+bb^T,"Are there any nice formulas for exactly computing the determinant of identity - rank 2 matrix, e.g. $$\det(M) = \det(I-aa^T+bb^T)?$$ Here we cannot assume that $a$ and $b$ are orthogonal. ($a^Tb > 0$.) I think it is ok to use projections on range spaces, e.g. $$ proj_{range(a)} = \frac{1}{a^Ta} aa^T, \quad proj_{range(a)^\perp} = I - \frac{1}{a^Ta} aa^T. $$ But the answer is still not clear to me. Thanks!","Are there any nice formulas for exactly computing the determinant of identity - rank 2 matrix, e.g. $$\det(M) = \det(I-aa^T+bb^T)?$$ Here we cannot assume that $a$ and $b$ are orthogonal. ($a^Tb > 0$.) I think it is ok to use projections on range spaces, e.g. $$ proj_{range(a)} = \frac{1}{a^Ta} aa^T, \quad proj_{range(a)^\perp} = I - \frac{1}{a^Ta} aa^T. $$ But the answer is still not clear to me. Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
11,"Decomposing the ""sign flip"" matrix in terms of Pauli matrices","Decomposing the ""sign flip"" matrix in terms of Pauli matrices",,"Can the matrix: $A=\left[\begin{matrix}0 & -1 \\ -1 & 0\end{matrix}\right]$ be somehow expressed as a product of the $3$ standard Pauli matrices? I'm being able to diagonalize $A$, but not sure if it can be expressed in terms of Pauli-X, Y, Z.","Can the matrix: $A=\left[\begin{matrix}0 & -1 \\ -1 & 0\end{matrix}\right]$ be somehow expressed as a product of the $3$ standard Pauli matrices? I'm being able to diagonalize $A$, but not sure if it can be expressed in terms of Pauli-X, Y, Z.",,['linear-algebra']
12,Frobenius Norm of Hadamard Product and Trace,Frobenius Norm of Hadamard Product and Trace,,"I'm trying to relate the Frobenius Norm of a Hadamard Product to a trace that does not include another Hadamard Product, if possible.  In other words, if A and B are (sxr) matrices, with not all positive values, $\left||A\circ B \right||^2_F = $? I'm trying to relate the sum of the squares of all the entries of the Hadamard product.  I know that the sum of all the entries of the Hadamard product are $\sum_i \sum_j (A\circ B)_{ij} = tr(A B^T) $ and also $\left||A\circ B \right||^2_F = tr((A\circ B)^T(A\circ B)) $ But I am trying to get $\left||A\circ B \right||^2_F $ in the form of the trace of some combination of A and B, without a Hadamard product.  Even an inequality would help.    I've tried various identities and inequalities related to Hadamard Product and Frobenius Norm, but I am not having any luck. Any suggestions would be appreciated.","I'm trying to relate the Frobenius Norm of a Hadamard Product to a trace that does not include another Hadamard Product, if possible.  In other words, if A and B are (sxr) matrices, with not all positive values, $\left||A\circ B \right||^2_F = $? I'm trying to relate the sum of the squares of all the entries of the Hadamard product.  I know that the sum of all the entries of the Hadamard product are $\sum_i \sum_j (A\circ B)_{ij} = tr(A B^T) $ and also $\left||A\circ B \right||^2_F = tr((A\circ B)^T(A\circ B)) $ But I am trying to get $\left||A\circ B \right||^2_F $ in the form of the trace of some combination of A and B, without a Hadamard product.  Even an inequality would help.    I've tried various identities and inequalities related to Hadamard Product and Frobenius Norm, but I am not having any luck. Any suggestions would be appreciated.",,"['matrices', 'hadamard-product']"
13,Prove that an $n \times n$ matrix ${\bf A}$ is orthogonally similar to a lower triangular matrix,Prove that an  matrix  is orthogonally similar to a lower triangular matrix,n \times n {\bf A},"Prove that if ${\bf A}$ is an $n \times n$ matrix with real eigenvalues, then ${\bf A}$ is orthogonally similar to a lower triangular matrix ${\bf T}$. I can prove that ${\bf A}$ is similar to an upper triangular matrix (using induction), but I can't find a way to prove it similar to a lower triangular matrix.","Prove that if ${\bf A}$ is an $n \times n$ matrix with real eigenvalues, then ${\bf A}$ is orthogonally similar to a lower triangular matrix ${\bf T}$. I can prove that ${\bf A}$ is similar to an upper triangular matrix (using induction), but I can't find a way to prove it similar to a lower triangular matrix.",,"['linear-algebra', 'matrices']"
14,The determinant of $T: \mathbb C^2\to \mathbb C^2$ as an $\mathbb R$-linear operator,The determinant of  as an -linear operator,T: \mathbb C^2\to \mathbb C^2 \mathbb R,"Suppose the determinant of a $\mathbb C$-linear transformation $T:\mathbb C^2\to \mathbb C^2$ is $a+bi$. I'm trying to prove that when $\mathbb C^2$ is identified with $\mathbb R^4$, the determinant of the $\mathbb R$-linear transformation $T:\mathbb R^4\to \mathbb R^4$ is $a^2+b^2$. I started off with a lower dimensional case: a complex-linear operator $T': \mathbb C\to \mathbb C$ must be multiplication by a complex number $a+bi$, and if I write the matrix of $T'$ w.r.t. the basis $(1,i)$ of $\mathbb C$ over $\mathbb R$, then the determinant of the matrix is $a^2+b^2$. In higher dimensional case, the complex-linear $T$ is multiplication by a $2\times 2$ matrix $$A=\begin{bmatrix}x_1+ix_2&z_1+iz_2\\y_1+iy_2&w_1+iw_2\end{bmatrix}.$$ The set $((1,0)^T,(i,0)^T,(0,1)^T,(0,i)^T)$ would be an $\mathbb R$-basis of $\mathbb C^2$. W.r.t. this basis, the matrix of $T$ is $$A'=\begin{bmatrix}x_1&-x_2&z_1&-z_2\\x_2&x_1&z_2&z_1\\y_1&-y_2&w_1&-w_2\\y_2&y_1&w_2&w_1\end{bmatrix}.$$ Am I supposed to compute the determinant of this last matrix and make sure it equals $a^2+b^2$ provided $\det A=a+bi$? It seems like a lot of calculations.","Suppose the determinant of a $\mathbb C$-linear transformation $T:\mathbb C^2\to \mathbb C^2$ is $a+bi$. I'm trying to prove that when $\mathbb C^2$ is identified with $\mathbb R^4$, the determinant of the $\mathbb R$-linear transformation $T:\mathbb R^4\to \mathbb R^4$ is $a^2+b^2$. I started off with a lower dimensional case: a complex-linear operator $T': \mathbb C\to \mathbb C$ must be multiplication by a complex number $a+bi$, and if I write the matrix of $T'$ w.r.t. the basis $(1,i)$ of $\mathbb C$ over $\mathbb R$, then the determinant of the matrix is $a^2+b^2$. In higher dimensional case, the complex-linear $T$ is multiplication by a $2\times 2$ matrix $$A=\begin{bmatrix}x_1+ix_2&z_1+iz_2\\y_1+iy_2&w_1+iw_2\end{bmatrix}.$$ The set $((1,0)^T,(i,0)^T,(0,1)^T,(0,i)^T)$ would be an $\mathbb R$-basis of $\mathbb C^2$. W.r.t. this basis, the matrix of $T$ is $$A'=\begin{bmatrix}x_1&-x_2&z_1&-z_2\\x_2&x_1&z_2&z_1\\y_1&-y_2&w_1&-w_2\\y_2&y_1&w_2&w_1\end{bmatrix}.$$ Am I supposed to compute the determinant of this last matrix and make sure it equals $a^2+b^2$ provided $\det A=a+bi$? It seems like a lot of calculations.",,"['linear-algebra', 'matrices', 'complex-numbers', 'linear-transformations', 'determinant']"
15,norm of $AUx$ for unitary $U$,norm of  for unitary,AUx U,"Let $U$ be a unitary matrix. Does it mean that for every invertible matrix $A$, $$\Vert AUx\Vert \le \Vert Ax\Vert$$ for every vector $x$?","Let $U$ be a unitary matrix. Does it mean that for every invertible matrix $A$, $$\Vert AUx\Vert \le \Vert Ax\Vert$$ for every vector $x$?",,"['linear-algebra', 'matrices', 'normed-spaces']"
16,Prove the equation $AXB-BXA=0$ has at least one invertible solution in $M_n(F)$.,Prove the equation  has at least one invertible solution in .,AXB-BXA=0 M_n(F),"Let $F$ be a field and $A,B \in M_n(F)$ (i.e., $A$ and $B$ are $n\times n$ matrices with entries in $F$ ). If there exist a linear combination of $A$ and $B$ which is invertible, then prove the equation $$AXB-BXA=0$$ has at least one invertible solution in $M_n(F)$ . Attempt. If $A$ is invertible, then from $AXB=BXA$ , we get $XBA^{-1}=A^{-1}BX$ .  Then, we see that $X=A^{-1}$ is a solution.  Similarly, if $B$ is invertible, then $X=B^{-1}$ is a solution.","Let be a field and (i.e., and are matrices with entries in ). If there exist a linear combination of and which is invertible, then prove the equation has at least one invertible solution in . Attempt. If is invertible, then from , we get .  Then, we see that is a solution.  Similarly, if is invertible, then is a solution.","F A,B \in M_n(F) A B n\times n F A B AXB-BXA=0 M_n(F) A AXB=BXA XBA^{-1}=A^{-1}BX X=A^{-1} B X=B^{-1}","['linear-algebra', 'matrices', 'inverse', 'matrix-equations']"
17,Find a normal matrix which commute with $A$.,Find a normal matrix which commute with .,A,Consider $$A=\begin{pmatrix}1&1&0&0\\-1&1&0&0\\0&0&2&2\\0&0&-2&2\end{pmatrix}.$$ I want to find a matrix $B$ such that $B$ is normal. $AB=BA$ . $B\neq I$ and $B\neq \alpha A$ ( $\alpha \in \mathbb{R}^*$ ).,Consider I want to find a matrix such that is normal. . and ( ).,A=\begin{pmatrix}1&1&0&0\\-1&1&0&0\\0&0&2&2\\0&0&-2&2\end{pmatrix}. B B AB=BA B\neq I B\neq \alpha A \alpha \in \mathbb{R}^*,"['linear-algebra', 'matrices']"
18,Finding points on a line that are closest,Finding points on a line that are closest,,Find the points that give the shortest distance between the lines$$\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}2-t\\-1+2t\\-1+t\end{pmatrix}\\\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}5+3s\\0\\2-s\end{pmatrix}$$ So I subtracted the second line from the first to get these two equations: $\begin{pmatrix}-t-3s-3\\ -1+2t\\ t+s-3\end{pmatrix}\cdot \begin{pmatrix}-1\\ 2\\ 1\end{pmatrix}=0$ $\begin{pmatrix}-t-3s-3\\ -1+2t\\ t+s-3\end{pmatrix}\cdot \begin{pmatrix}3\\ 0\\ -1\end{pmatrix}=0$ I know I am supposed to rearrange them together two get a system of equations but I am not sure how. Any help please?,Find the points that give the shortest distance between the lines$$\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}2-t\\-1+2t\\-1+t\end{pmatrix}\\\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}5+3s\\0\\2-s\end{pmatrix}$$ So I subtracted the second line from the first to get these two equations: $\begin{pmatrix}-t-3s-3\\ -1+2t\\ t+s-3\end{pmatrix}\cdot \begin{pmatrix}-1\\ 2\\ 1\end{pmatrix}=0$ $\begin{pmatrix}-t-3s-3\\ -1+2t\\ t+s-3\end{pmatrix}\cdot \begin{pmatrix}3\\ 0\\ -1\end{pmatrix}=0$ I know I am supposed to rearrange them together two get a system of equations but I am not sure how. Any help please?,,"['linear-algebra', 'matrices', 'vectors', 'systems-of-equations']"
19,How do i find $e^{tA}$?,How do i find ?,e^{tA},"I've been trying to solve the following system of linear differential equations $$\begin{aligned} \dot x_1(t) &= 10 x_1(t) + 5 x_2(t) −  5 x_3(t)\\ \dot x_2(t) &= −5 x_1(t) −   x_2(t) +  6 x_3(t)\\ \dot x_3(t) &= −5 x_1(t) − 6 x_2(t) + 11 x_3(t)\\ \end{aligned}$$ with initial state $x (0) = (1,0,1)$ , and I found out that the solution should be $x(t) = e^{tA}x_0$ , where $$A = \begin{pmatrix}       10 &  5 & -5 \\\       -5 & -1 &  6 \\\       -5 & -6 & 11     \end{pmatrix}$$ How do I calculate the matrix exponential $e^{tA}$ ? I found that $f(A) = C f(J) C^{-1}$ but that still leaves me with the question of how to calculate $f(J)$ ?","I've been trying to solve the following system of linear differential equations with initial state , and I found out that the solution should be , where How do I calculate the matrix exponential ? I found that but that still leaves me with the question of how to calculate ?","\begin{aligned}
\dot x_1(t) &= 10 x_1(t) + 5 x_2(t) −  5 x_3(t)\\
\dot x_2(t) &= −5 x_1(t) −   x_2(t) +  6 x_3(t)\\
\dot x_3(t) &= −5 x_1(t) − 6 x_2(t) + 11 x_3(t)\\
\end{aligned} x (0) = (1,0,1) x(t) = e^{tA}x_0 A = \begin{pmatrix}
      10 &  5 & -5 \\\
      -5 & -1 &  6 \\\
      -5 & -6 & 11
    \end{pmatrix} e^{tA} f(A) = C f(J) C^{-1} f(J)","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'jordan-normal-form', 'matrix-exponential']"
20,If $A$ is idempotent then $A$ is similar to a diagonal matrix with only $0$'s and $1$'s on the diagonal.,If  is idempotent then  is similar to a diagonal matrix with only 's and 's on the diagonal.,A A 0 1,"I am trying to use Jordan normal form to show that if $A^2 = A$ then it is similar to a diagonal matrix with only $0$'s and $1$'s. I've proved that the eigenvalues of $A$ have to be either $0$ or $1$ so we know the diagonal elements of the JNF have to be $0$ or $1$. How do we know that none of the off-diagonals above are $1$? I don't full understand JNF - I've only learned about it in terms of elementary divisors and the minimal and characteristic polynomials, but a lot of resources online talk about it in terms of eigenspaces which is confusing.","I am trying to use Jordan normal form to show that if $A^2 = A$ then it is similar to a diagonal matrix with only $0$'s and $1$'s. I've proved that the eigenvalues of $A$ have to be either $0$ or $1$ so we know the diagonal elements of the JNF have to be $0$ or $1$. How do we know that none of the off-diagonals above are $1$? I don't full understand JNF - I've only learned about it in terms of elementary divisors and the minimal and characteristic polynomials, but a lot of resources online talk about it in terms of eigenspaces which is confusing.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'jordan-normal-form']"
21,Is it possible to write this operation as a (series of) matrix operation(s),Is it possible to write this operation as a (series of) matrix operation(s),,"Consider the symmetric matrix $T\in \mathbb{R}^{n\times n}$. For the sake of simplicty I will use the example for $n=2$. I was wondering if we have the full matrix as $$ T = \begin{pmatrix} a & c\\ c& b\end{pmatrix},$$ it is possible to construct this from a vector $t\in\mathbb{R}^{\frac{1}{2}n(n+1)}$, such that  $$ t = \begin{pmatrix} a\\b\\c\end{pmatrix}.\quad (n=2)$$ I was thinking that this might be possible by writing it as some kind of strange permutation in the form of $$AtB=T,$$ where $A\in \mathbb{R}^{n\times \frac{1}{2}n(n+1)}$ and $B\in \mathbb{R}^{1\times n}$. However, working this out component wise gives contradicting requirements for $a_{ij}$ ($a_{13}=0$ and $1$), so this seems impossible. $\textbf{Question:}$ Is there a way to construct $T$ when $t$ is given, which uses only matrix-vector operations?","Consider the symmetric matrix $T\in \mathbb{R}^{n\times n}$. For the sake of simplicty I will use the example for $n=2$. I was wondering if we have the full matrix as $$ T = \begin{pmatrix} a & c\\ c& b\end{pmatrix},$$ it is possible to construct this from a vector $t\in\mathbb{R}^{\frac{1}{2}n(n+1)}$, such that  $$ t = \begin{pmatrix} a\\b\\c\end{pmatrix}.\quad (n=2)$$ I was thinking that this might be possible by writing it as some kind of strange permutation in the form of $$AtB=T,$$ where $A\in \mathbb{R}^{n\times \frac{1}{2}n(n+1)}$ and $B\in \mathbb{R}^{1\times n}$. However, working this out component wise gives contradicting requirements for $a_{ij}$ ($a_{13}=0$ and $1$), so this seems impossible. $\textbf{Question:}$ Is there a way to construct $T$ when $t$ is given, which uses only matrix-vector operations?",,"['linear-algebra', 'matrices', 'permutations']"
22,"$U$ be the set of all $2×2$ matrices with real entries such that all their eigenvalues belong to $C - R $, and $X = M_2(R)$. Is $U$ open?","be the set of all  matrices with real entries such that all their eigenvalues belong to , and . Is  open?",U 2×2 C - R  X = M_2(R) U,"Let $U$ be the set of all $2×2$ matrices with real entries such that all their eigenvalues belong to $C - R  $, and $X = M_2(R)$. How can I prove $U$ is open in $X$? Can anyone please help me by giving any hint? I know this is not a closed set.","Let $U$ be the set of all $2×2$ matrices with real entries such that all their eigenvalues belong to $C - R  $, and $X = M_2(R)$. How can I prove $U$ is open in $X$? Can anyone please help me by giving any hint? I know this is not a closed set.",,"['general-topology', 'matrices', 'metric-spaces', 'eigenvalues-eigenvectors']"
23,Prove that $B$ is non singular and that $AB^{-1}A=A$,Prove that  is non singular and that,B AB^{-1}A=A,"$$A_{n\times n}=\begin{bmatrix}a & b & b & b &. &.&.&&b\\b & a &b&b&.&.&.&&b\\b & b &a&b&.&.&.&&b\\b & . &.&.&.&.&.&&b\\b & . &.&.&.&.&.&&b\\b & b &b&b&.&.&.&&a\end{bmatrix}\text{ where }  a+(n-1)b =0$$ Define $l^t=\begin{bmatrix}1&1&1&1&1&....1\end{bmatrix}$ Where $l$ is   a $ n\times1$ vector, and: $$B= A+ \frac{l\cdot l^t}{n}$$ Prove that $B$ is non singular and that $AB^{-1}A=A$ What i did: $\text{A has a 0 eigenvalue , so A is a singular matrix}$ $\text{B  has an eigenvalue of 1 with eigenvector} $$\,\, v^{t}= \begin{bmatrix}1&1&1&1&1&....1\end{bmatrix}$ Any idea about how to proceed? Thanks.","$$A_{n\times n}=\begin{bmatrix}a & b & b & b &. &.&.&&b\\b & a &b&b&.&.&.&&b\\b & b &a&b&.&.&.&&b\\b & . &.&.&.&.&.&&b\\b & . &.&.&.&.&.&&b\\b & b &b&b&.&.&.&&a\end{bmatrix}\text{ where }  a+(n-1)b =0$$ Define $l^t=\begin{bmatrix}1&1&1&1&1&....1\end{bmatrix}$ Where $l$ is   a $ n\times1$ vector, and: $$B= A+ \frac{l\cdot l^t}{n}$$ Prove that $B$ is non singular and that $AB^{-1}A=A$ What i did: $\text{A has a 0 eigenvalue , so A is a singular matrix}$ $\text{B  has an eigenvalue of 1 with eigenvector} $$\,\, v^{t}= \begin{bmatrix}1&1&1&1&1&....1\end{bmatrix}$ Any idea about how to proceed? Thanks.",,"['linear-algebra', 'matrices']"
24,How to prove that $AB$ is invertible if and only if $A$ is invertible?,How to prove that  is invertible if and only if  is invertible?,AB A,"Let $A$ be a matrix and $B$ an invertible matrix. Show that $AB$ is invertible if and only if $A$ is invertible. I know how to do this using determinants, but how else could you prove this?","Let $A$ be a matrix and $B$ an invertible matrix. Show that $AB$ is invertible if and only if $A$ is invertible. I know how to do this using determinants, but how else could you prove this?",,"['linear-algebra', 'matrices', 'inverse']"
25,What is the infimal norm of a matrix in a conjugacy class?,What is the infimal norm of a matrix in a conjugacy class?,,"$\newcommand{\tr}{\operatorname{tr}}$Let $A$ be a real $d \times d$ matrix. What is  $$ F(A):=\inf_{M \in \text{GL}(d, \mathbb{R})} |MAM^{-1}|^2?$$ Here $| \cdot |$ is the standard Euclidean (Frobenius) norm. Is the infimum always obtained? I would like to find a simple formula for $F(A)$ in terms of $A$. Cauchy-Schwarts inequality implies that $$( \tr(A))^2 \le d |A|^2$$ with equality if and only if $A$ is a multiple of the identity matrix. Since the trace is an invariant of a conjugacy class, we deduce $$ F(A) \ge \frac{\tr(A)^2}{d},$$ and this value is obtained if  and only if $A$ is a multiple of the identity matrix. Comment: Note that I am referring to conjugacy classes in $\mathrm{GL}(d, \mathbb{R})$, not in $\mathrm{GL}(d, \mathbb{C})$.","$\newcommand{\tr}{\operatorname{tr}}$Let $A$ be a real $d \times d$ matrix. What is  $$ F(A):=\inf_{M \in \text{GL}(d, \mathbb{R})} |MAM^{-1}|^2?$$ Here $| \cdot |$ is the standard Euclidean (Frobenius) norm. Is the infimum always obtained? I would like to find a simple formula for $F(A)$ in terms of $A$. Cauchy-Schwarts inequality implies that $$( \tr(A))^2 \le d |A|^2$$ with equality if and only if $A$ is a multiple of the identity matrix. Since the trace is an invariant of a conjugacy class, we deduce $$ F(A) \ge \frac{\tr(A)^2}{d},$$ and this value is obtained if  and only if $A$ is a multiple of the identity matrix. Comment: Note that I am referring to conjugacy classes in $\mathrm{GL}(d, \mathbb{R})$, not in $\mathrm{GL}(d, \mathbb{C})$.",,"['matrices', 'inequality', 'matrix-calculus', 'geometric-inequalities']"
26,"If $P \in M_7(\mathbb{R})$ has rank 4, rank of $P + aa^T$, where $a$ is a column vector? [duplicate]","If  has rank 4, rank of , where  is a column vector? [duplicate]",P \in M_7(\mathbb{R}) P + aa^T a,This question already has an answer here : Least rank of a matrix (1 answer) Closed 6 years ago . Let $P$ be a $7 \times 7$ matrix of rank $4$ with real entries and let $a \in \mathbb{R}^7$ be a column vector. Then the rank of $P + aa^T$ is at least ______. This question was in the 2017 IIT JAM paper. Any ideas?,This question already has an answer here : Least rank of a matrix (1 answer) Closed 6 years ago . Let $P$ be a $7 \times 7$ matrix of rank $4$ with real entries and let $a \in \mathbb{R}^7$ be a column vector. Then the rank of $P + aa^T$ is at least ______. This question was in the 2017 IIT JAM paper. Any ideas?,,"['linear-algebra', 'matrices', 'matrix-rank']"
27,Is the Perturbed Laplacian Matrix positive Definite?,Is the Perturbed Laplacian Matrix positive Definite?,,"Let $\mathcal{L}$ be the Laplacian matrix of a connected, undirected and unweighted graph $\mathcal{G}$ with $n$ vertices, and let $\Delta \in \mathbb{R}^{n\times n}$ be a diagonal matrix with at least one nonzero entry (for instance $\Delta=\mbox{diag}\{1,0,\cdots,0\}$). Numerically, I can see that $$x^T(L+\Delta)x>0$$ But I cannot prove it since both matrices, $\mathcal{L}$ and $\Delta$ are positive semidefinite. Any thoughts?","Let $\mathcal{L}$ be the Laplacian matrix of a connected, undirected and unweighted graph $\mathcal{G}$ with $n$ vertices, and let $\Delta \in \mathbb{R}^{n\times n}$ be a diagonal matrix with at least one nonzero entry (for instance $\Delta=\mbox{diag}\{1,0,\cdots,0\}$). Numerically, I can see that $$x^T(L+\Delta)x>0$$ But I cannot prove it since both matrices, $\mathcal{L}$ and $\Delta$ are positive semidefinite. Any thoughts?",,"['linear-algebra', 'matrices', 'matrix-equations', 'quadratic-forms', 'graph-laplacian']"
28,Short inequality involving determinants,Short inequality involving determinants,,"Let $A, B \in M_n(\mathbb{C})$ such that $$|\det(A+zB)|\leq1$$ for any $z\in\mathbb{C}$  with $|z|=1$. Prove that $$|\det(A)|\le1$$ I assumed that $|\det A|\ge1$. Then there exists $A^{-1}$ and $|\det(A^{-1})|\le1$. I multiplied the relation and obtained $$|\det(I_n+zBA^{-1})|\le|\det(A^{-1})|\le1$$ so $|\det(I_n+zC)|\le1$ for any $z$ with $|z|=1$, where $C=BA^{-1}$. I don't think that this is true but I don't know how to prove it.","Let $A, B \in M_n(\mathbb{C})$ such that $$|\det(A+zB)|\leq1$$ for any $z\in\mathbb{C}$  with $|z|=1$. Prove that $$|\det(A)|\le1$$ I assumed that $|\det A|\ge1$. Then there exists $A^{-1}$ and $|\det(A^{-1})|\le1$. I multiplied the relation and obtained $$|\det(I_n+zBA^{-1})|\le|\det(A^{-1})|\le1$$ so $|\det(I_n+zC)|\le1$ for any $z$ with $|z|=1$, where $C=BA^{-1}$. I don't think that this is true but I don't know how to prove it.",,"['linear-algebra', 'matrices', 'complex-numbers', 'determinant']"
29,"Show that if $A,B$ are $2\times2$ matrices, then $(AB-BA)^2$ commutes with all $2\times2$ matrices.","Show that if  are  matrices, then  commutes with all  matrices.","A,B 2\times2 (AB-BA)^2 2\times2","I tried to write it all out, but it becomes really messy... Is there a more elegant way to do it? Note that I don't know about dimensions, vector spaces & bases yet","I tried to write it all out, but it becomes really messy... Is there a more elegant way to do it? Note that I don't know about dimensions, vector spaces & bases yet",,"['linear-algebra', 'matrices', 'linear-transformations']"
30,"Let $M$ be the set of all $2\times2$ real matrices, and define $f:M\rightarrow M$ by $f(A)=A^3$. Is $f$ surjective?","Let  be the set of all  real matrices, and define  by . Is  surjective?",M 2\times2 f:M\rightarrow M f(A)=A^3 f,"I know that in a finite field, injectivity implies surjectivity, but we are working in a ring, not a field. Also $f$ is not injective. I'm sure an argument could be made using a system of equations of an arbitrary cube, but there must be a more elegant solution. Something using Cayley-Hamilton, maybe? Any advice on how to approach this is appreciated.","I know that in a finite field, injectivity implies surjectivity, but we are working in a ring, not a field. Also $f$ is not injective. I'm sure an argument could be made using a system of equations of an arbitrary cube, but there must be a more elegant solution. Something using Cayley-Hamilton, maybe? Any advice on how to approach this is appreciated.",,"['linear-algebra', 'matrices']"
31,Diagonalizability of $A \in M_{n}(\Bbb{R})$ (Upper traingular matrix) with all diagonal entries 1 and $A \neq I$?,Diagonalizability of  (Upper traingular matrix) with all diagonal entries 1 and ?,A \in M_{n}(\Bbb{R}) A \neq I,"If $A \in M_{n}(\Bbb{R})$ is an Upper triangular matrix with diagonal entries $1$ such that  $A \neq I$, then what can we say about the diagonalizability of $A$ ? I know that if the matrix has distinct eigenvalues or the set of eigenvectors are linearly independent then the matrix is diagonalizable. And that the eigenvalues of the triangular matrices are given by the diagonal elements like here and here , but they work nocely if we had distinct elements on the main diagonal. But in my case I have same value 1 on the main diagonal, how can I approach about the diagonalizability of the matrix?","If $A \in M_{n}(\Bbb{R})$ is an Upper triangular matrix with diagonal entries $1$ such that  $A \neq I$, then what can we say about the diagonalizability of $A$ ? I know that if the matrix has distinct eigenvalues or the set of eigenvectors are linearly independent then the matrix is diagonalizable. And that the eigenvalues of the triangular matrices are given by the diagonal elements like here and here , but they work nocely if we had distinct elements on the main diagonal. But in my case I have same value 1 on the main diagonal, how can I approach about the diagonalizability of the matrix?",,"['linear-algebra', 'matrices', 'diagonalization']"
32,Question about finding the inverse of matrices,Question about finding the inverse of matrices,,"My professor gave us this formula for finding the inverse of 3x3 matrices; [matrix given|identity] then row reduce matrix given to the identity and you end up with [identity|inverse of matrix given] So basically you take the matrix you are trying the find the inverse of, augment it with the identity, row reduce it until you have the identity for the first 3 columns, and then your last 3 columns will be the inverse. (I hope my wording makes sense.) For example: to find the inverse of \begin{pmatrix}0&1&2\\ 1&0&3\\ 4&-3&8\end{pmatrix} you first do this:  \begin{pmatrix}0&1&2&1&0&0\\ 1&0&3&0&1&0\\ 4&-3&8&0&0&1\end{pmatrix} then row reduce it until you have the identity at the beginning and the last 3 columns will be the inverse: \begin{pmatrix}1&0&0&-\frac{9}{2}&7&-\frac{3}{2}\\ 0&1&0&-2&4&-1\\ 0&0&1&-\frac{3}{2}&-2&\frac{1}{2}\end{pmatrix} so the inverse is:  \begin{pmatrix}-\frac{9}{2}&7&-\frac{3}{2}\\ -2&4&-1\\ \frac{3}{2}&-2&\frac{1}{2}\end{pmatrix} My question is: does this work for other matrices bigger than a 3x3? Thanks!","My professor gave us this formula for finding the inverse of 3x3 matrices; [matrix given|identity] then row reduce matrix given to the identity and you end up with [identity|inverse of matrix given] So basically you take the matrix you are trying the find the inverse of, augment it with the identity, row reduce it until you have the identity for the first 3 columns, and then your last 3 columns will be the inverse. (I hope my wording makes sense.) For example: to find the inverse of \begin{pmatrix}0&1&2\\ 1&0&3\\ 4&-3&8\end{pmatrix} you first do this:  \begin{pmatrix}0&1&2&1&0&0\\ 1&0&3&0&1&0\\ 4&-3&8&0&0&1\end{pmatrix} then row reduce it until you have the identity at the beginning and the last 3 columns will be the inverse: \begin{pmatrix}1&0&0&-\frac{9}{2}&7&-\frac{3}{2}\\ 0&1&0&-2&4&-1\\ 0&0&1&-\frac{3}{2}&-2&\frac{1}{2}\end{pmatrix} so the inverse is:  \begin{pmatrix}-\frac{9}{2}&7&-\frac{3}{2}\\ -2&4&-1\\ \frac{3}{2}&-2&\frac{1}{2}\end{pmatrix} My question is: does this work for other matrices bigger than a 3x3? Thanks!",,"['linear-algebra', 'matrices', 'inverse']"
33,Rank of upper triangular block with Identity matrix,Rank of upper triangular block with Identity matrix,,"Quick question I was hoping I could get some insight on. If I have a block matrix of the following form $$A = \begin{pmatrix} I & A_{12} \\ 0 & A_{22} \end{pmatrix}$$ Where $A_{12}, A_{22}$ are arbitrary block matrices that need not be square (in fact in my case they are not square). Is it correct to say that $$\text{rank} A = \text{rank}I + \text{rank}A_{22}$$ Because the identity matrix ensures that $A$ has the rank of $I$ and then the rank of $A$ should be determined by the rank of $A_{22}$, no?","Quick question I was hoping I could get some insight on. If I have a block matrix of the following form $$A = \begin{pmatrix} I & A_{12} \\ 0 & A_{22} \end{pmatrix}$$ Where $A_{12}, A_{22}$ are arbitrary block matrices that need not be square (in fact in my case they are not square). Is it correct to say that $$\text{rank} A = \text{rank}I + \text{rank}A_{22}$$ Because the identity matrix ensures that $A$ has the rank of $I$ and then the rank of $A$ should be determined by the rank of $A_{22}$, no?",,"['linear-algebra', 'matrices', 'matrix-rank']"
34,"2-Sylow subgroup of $\operatorname{GL}(2,3)$",2-Sylow subgroup of,"\operatorname{GL}(2,3)","Consider the group $\operatorname{GL}(2,3)$, the group of invertible $2 × 2$ matrices over the field of 3 elements. It is of order $48 = 2^4 \cdot 3$. A 3-Sylow subgroup is easily seen to be the Heisenberg group (unitary upper triangular matrices). What about the 2-Sylow subgroups? Is there a nice way to identify one of them?","Consider the group $\operatorname{GL}(2,3)$, the group of invertible $2 × 2$ matrices over the field of 3 elements. It is of order $48 = 2^4 \cdot 3$. A 3-Sylow subgroup is easily seen to be the Heisenberg group (unitary upper triangular matrices). What about the 2-Sylow subgroups? Is there a nice way to identify one of them?",,"['abstract-algebra', 'matrices', 'group-theory', 'finite-groups', 'sylow-theory']"
35,Relationship between Diagonally dominant and Well Conditioned matrices,Relationship between Diagonally dominant and Well Conditioned matrices,,"While solving linear systems by iterative methods is common pay attention to Diagonally dominant matrices, equivalently when studying stability of interpolation methods (and other kind of approximations) we look for well conditioned matrices to avoid huge oscillations with a small  change in the system. I'm looking for any relationships between well-conditioned matrices and diagonally dominant matrices and for bounds in $\kappa$ constant. For sake of completeness I will define the subjects involved in my question Definition 1 (Well conditioned matrix) : Let $A$ be a matrix, and $\|\cdot\|$ a matrix norm. We call $A$ well conditioned if: $$ \kappa_{\| \cdot \| }(A) = \| A \| \cdot \|A^{-1} \| \approx 1 $$ When $\kappa_{\| \cdot \| } \gg 1 $ we call it ill conditioned . And $\kappa_{\| \cdot \| }(A)$ is the condition number related to the matrix $A$ and the norm. Definition 2 (Diagonally dominant matrix) : Let $A$ be a square matrix rank $n$, we  call it diagonally dominant if for each $i$ row: $$ |a_{ii}| \ge \sum_{i \ne j}^n | a_{ij} | \; \forall i $$","While solving linear systems by iterative methods is common pay attention to Diagonally dominant matrices, equivalently when studying stability of interpolation methods (and other kind of approximations) we look for well conditioned matrices to avoid huge oscillations with a small  change in the system. I'm looking for any relationships between well-conditioned matrices and diagonally dominant matrices and for bounds in $\kappa$ constant. For sake of completeness I will define the subjects involved in my question Definition 1 (Well conditioned matrix) : Let $A$ be a matrix, and $\|\cdot\|$ a matrix norm. We call $A$ well conditioned if: $$ \kappa_{\| \cdot \| }(A) = \| A \| \cdot \|A^{-1} \| \approx 1 $$ When $\kappa_{\| \cdot \| } \gg 1 $ we call it ill conditioned . And $\kappa_{\| \cdot \| }(A)$ is the condition number related to the matrix $A$ and the norm. Definition 2 (Diagonally dominant matrix) : Let $A$ be a square matrix rank $n$, we  call it diagonally dominant if for each $i$ row: $$ |a_{ii}| \ge \sum_{i \ne j}^n | a_{ij} | \; \forall i $$",,"['matrices', 'numerical-methods', 'numerical-linear-algebra']"
36,Examples of $e^A e^B \neq e^C $,Examples of,e^A e^B \neq e^C ,"Can anyone give me an example where for matrices $A$ and $B$, there is no matrix $C$ such that $$ e^A e^B  = e^C ? $$","Can anyone give me an example where for matrices $A$ and $B$, there is no matrix $C$ such that $$ e^A e^B  = e^C ? $$",,"['matrices', 'lie-groups']"
37,determinant of the $7\times7$ matrix [closed],determinant of the  matrix [closed],7\times7,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to find determinant of the following $7\times7$ matrix \begin{bmatrix}      a&1&0&0&0&0&1&\\     1&a&1&0&0&0&0&\\     0&1&a&1&0&0&0&\\     0&0&1&a&1&0&0&\\     0&0&0&1&a&1&0&\\     0&0&0&0&1&a&1&\\     1&0&0&0&0&1&a& \end{bmatrix}","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to find determinant of the following $7\times7$ matrix \begin{bmatrix}      a&1&0&0&0&0&1&\\     1&a&1&0&0&0&0&\\     0&1&a&1&0&0&0&\\     0&0&1&a&1&0&0&\\     0&0&0&1&a&1&0&\\     0&0&0&0&1&a&1&\\     1&0&0&0&0&1&a& \end{bmatrix}",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
38,Proving Sylvester's Inequality,Proving Sylvester's Inequality,,"I have been attempting to prove Sylvester's Inequality. $$rank(A)+rank(B)\leq n+rank(AB)$$ I have been referencing other proofs on this site, but all of them involve concepts and theorems that I am not familiar with, so I am still struggling. So far, I have proven that $rank(AB) \le rank(A)$ and $rank(AB) \le rank(B)$. I also have rewritten the inequality as $ker(A)+ker(B) \ge ker(AB)$, but I can't seem to get past this point. Any help would be appreciated.","I have been attempting to prove Sylvester's Inequality. $$rank(A)+rank(B)\leq n+rank(AB)$$ I have been referencing other proofs on this site, but all of them involve concepts and theorems that I am not familiar with, so I am still struggling. So far, I have proven that $rank(AB) \le rank(A)$ and $rank(AB) \le rank(B)$. I also have rewritten the inequality as $ker(A)+ker(B) \ge ker(AB)$, but I can't seem to get past this point. Any help would be appreciated.",,"['linear-algebra', 'matrices', 'proof-writing', 'matrix-rank']"
39,How to calculate the second derivative of the determinant?,How to calculate the second derivative of the determinant?,,"I am working on the following question and have managed to do most of it but unfortunately I am getting stuck on the last bit. I think part of my confusion lies with calculating second determinants (which are linear maps to a space of linear maps) of matrices (which are themselves linear maps). This is the question: Let $V = M_{n×n}(\mathbb{R})$ . By considering $\det(I + A)$ as a polynomial in the entries of $A$ , show that the function $\det : V → \mathbb{R}$ is differentiable at the identity matrix $I$ and that its derivative there is the function $A → \text{tr} A$ . Hence show that $\det$ is differentiable at any invertible matrix $X$ , with derivative $A → \det X \text{tr}(X^{−1}A)$ . Compute the second derivative of det at $I$ as a bilinear map $V × V → \mathbb{R}$ , and verify it is symmetric. I have done everything except for the last bit. However whatever I try for the last bit is not fruitful. I know that the second derivative at $I$ is a bilinear form whose matrix has as its entries the values of the second partial derivatives $D_{ij}(I)$ but I don't see how to compute the second partial derivatives. I know what the first partial derivatives are though as I worked them out for the first part of the question (at $I$ they are the trace of the matrix that represents the $i^{th}$ basis vector). I also see how we can view the partial derivative as a function of $X$ but I don't see how to extend that to work out the limit $D_{ij}f(I) = \lim\limits_{t \to 0} \frac{D_{\mathbf{e}_j}f(I+t\mathbf{e}_i)-D_{\mathbf{e}_j}f(I)}{t}$ I don't think the solution to the first bit is important for the last part so I am not posting it up, however if you wish to see it please let me know and I will add it. Thanks for all your help.","I am working on the following question and have managed to do most of it but unfortunately I am getting stuck on the last bit. I think part of my confusion lies with calculating second determinants (which are linear maps to a space of linear maps) of matrices (which are themselves linear maps). This is the question: Let . By considering as a polynomial in the entries of , show that the function is differentiable at the identity matrix and that its derivative there is the function . Hence show that is differentiable at any invertible matrix , with derivative . Compute the second derivative of det at as a bilinear map , and verify it is symmetric. I have done everything except for the last bit. However whatever I try for the last bit is not fruitful. I know that the second derivative at is a bilinear form whose matrix has as its entries the values of the second partial derivatives but I don't see how to compute the second partial derivatives. I know what the first partial derivatives are though as I worked them out for the first part of the question (at they are the trace of the matrix that represents the basis vector). I also see how we can view the partial derivative as a function of but I don't see how to extend that to work out the limit I don't think the solution to the first bit is important for the last part so I am not posting it up, however if you wish to see it please let me know and I will add it. Thanks for all your help.",V = M_{n×n}(\mathbb{R}) \det(I + A) A \det : V → \mathbb{R} I A → \text{tr} A \det X A → \det X \text{tr}(X^{−1}A) I V × V → \mathbb{R} I D_{ij}(I) I i^{th} X D_{ij}f(I) = \lim\limits_{t \to 0} \frac{D_{\mathbf{e}_j}f(I+t\mathbf{e}_i)-D_{\mathbf{e}_j}f(I)}{t},"['matrices', 'analysis', 'multivariable-calculus', 'derivatives']"
40,Matrices which never map vectors onto orthogonal vectors,Matrices which never map vectors onto orthogonal vectors,,"Clearly, there are matrices $M$ for which there exists a vector $v$ such that $\langle v,Mv \rangle=0$ but $M v\ne 0$. In fact, a $90^{\circ}$ rotation in $2D$ has this property for all $v$: $$v^T \left[\begin{matrix}0 & 1 \\ -1 & 0\end{matrix}\right] v \equiv 0$$ What can we say about matrices that don't have this property? Matrices for which $\langle v,Mv \rangle=0 \implies M v=0$. The unit matrix is an example, as is $$\left[\begin{matrix}1 & 0 \\ 0 & 0\end{matrix}\right] $$ so there are non-invertible examples. How can we check that $\langle v,Mv\rangle=0\implies M v=0$? Does it suffice that $M$ is symmetric/Hermitian?","Clearly, there are matrices $M$ for which there exists a vector $v$ such that $\langle v,Mv \rangle=0$ but $M v\ne 0$. In fact, a $90^{\circ}$ rotation in $2D$ has this property for all $v$: $$v^T \left[\begin{matrix}0 & 1 \\ -1 & 0\end{matrix}\right] v \equiv 0$$ What can we say about matrices that don't have this property? Matrices for which $\langle v,Mv \rangle=0 \implies M v=0$. The unit matrix is an example, as is $$\left[\begin{matrix}1 & 0 \\ 0 & 0\end{matrix}\right] $$ so there are non-invertible examples. How can we check that $\langle v,Mv\rangle=0\implies M v=0$? Does it suffice that $M$ is symmetric/Hermitian?",,['matrices']
41,Find the determinant of the following $4 \times 4$ matrix,Find the determinant of the following  matrix,4 \times 4,Use a cofactor expansion across a row or column to find the determinant of the following matrix $$B=\begin{pmatrix}1 &c&0&0\\0&1&c&0\\0&0&1&c\\c&0&0&1\end{pmatrix}$$ Clearly indicate the steps you take. I have tried $$ \begin{aligned} \det B &= 1 \det \begin{pmatrix}1 &c&0\\0&1&c\\0&0&1\end{pmatrix}+(-c) \det \begin{pmatrix}c &0&0\\1&c&0\\0&1&c\end{pmatrix} \\ &= \det\begin{pmatrix}1 &c\\0&1\end{pmatrix}+c(-c)\det\begin{pmatrix}c &0\\1&c\end{pmatrix} \\ &= 1-c^4 \end{aligned}$$,Use a cofactor expansion across a row or column to find the determinant of the following matrix Clearly indicate the steps you take. I have tried,B=\begin{pmatrix}1 &c&0&0\\0&1&c&0\\0&0&1&c\\c&0&0&1\end{pmatrix}  \begin{aligned} \det B &= 1 \det \begin{pmatrix}1 &c&0\\0&1&c\\0&0&1\end{pmatrix}+(-c) \det \begin{pmatrix}c &0&0\\1&c&0\\0&1&c\end{pmatrix} \\ &= \det\begin{pmatrix}1 &c\\0&1\end{pmatrix}+c(-c)\det\begin{pmatrix}c &0\\1&c\end{pmatrix} \\ &= 1-c^4 \end{aligned},"['linear-algebra', 'matrices', 'determinant', 'circulant-matrices']"
42,Difference between Ritz vectors and Eigenvectors,Difference between Ritz vectors and Eigenvectors,,"This is probably a silly question, as it came from an error in the Eigenvectors I found using ARPACK (Fortran). In this case, the values of the Ritz vectors are identical in value to the theoretical Eigenvectors but different in sign. So what is the real difference between the two? Should I expect this behavior?","This is probably a silly question, as it came from an error in the Eigenvectors I found using ARPACK (Fortran). In this case, the values of the Ritz vectors are identical in value to the theoretical Eigenvectors but different in sign. So what is the real difference between the two? Should I expect this behavior?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
43,How to obtain the square root of the transition matrix of Markov,How to obtain the square root of the transition matrix of Markov,,"Suppose  $$Q = \{q_{ij}\}$$ with $\sum\limits_j q_{ij} = 1,\quad |q_{ij}|\leq1$ is the transition matrix of a Markov chain. Then how to find the square root of $Q,$ namely $$R^2 = Q.$$ One way I used is using the Taylor expansion of $\sqrt{x},$ since every entry of $Q$ is smaller than $1,$ then $Q^n$ should be convergent. But is there any better way to computer the square root of a matrix?","Suppose  $$Q = \{q_{ij}\}$$ with $\sum\limits_j q_{ij} = 1,\quad |q_{ij}|\leq1$ is the transition matrix of a Markov chain. Then how to find the square root of $Q,$ namely $$R^2 = Q.$$ One way I used is using the Taylor expansion of $\sqrt{x},$ since every entry of $Q$ is smaller than $1,$ then $Q^n$ should be convergent. But is there any better way to computer the square root of a matrix?",,"['linear-algebra', 'matrices', 'markov-chains']"
44,Why the two following maps agree on the basis vector for $R^3$?,Why the two following maps agree on the basis vector for ?,R^3,"I cannot manage to understand the proof of the theorem below, could you elaborate why the maps agree on the basis vectors in $R^3$? For $v = (v_x,v_y,v_z)^t \in \mathbb{R}^3$ the matrix $\hat{v}$ is defined as $$ \hat{v} = \left( \begin{array}{lll} 0 & -v_z & v_y \\ v_z & 0 & -v_x \\ -v_y & v_x & 0 \end{array} \right) $$ Lema 5.4. (The hat operator). For a vector $T\in\mathbb R^3$ and a matrix $K\in\mathbb R^{3\times 3}$, if $\det(K)=+1$ and $T'=KT$, then $\widehat T = K^T \widehat{T'} K$. Proof. Since both $K^T\widehat{(\cdot)}K$ and $\widehat{K^{-1}(\cdot)}$ are linear maps from $\mathbb R^3$ to $\mathbb R^{3\times 3}$ one may directly verify that these two linear maps agree on the basis vectors $[1,0,0]^T$, $[0,1,0]^T$ and $[0,0,1]^T$ (using the fact that $\det(K)=1$). This is Lemma 5.4 from An Invitation to 3-D Vision (Yi Ma et al.), page 113 .","I cannot manage to understand the proof of the theorem below, could you elaborate why the maps agree on the basis vectors in $R^3$? For $v = (v_x,v_y,v_z)^t \in \mathbb{R}^3$ the matrix $\hat{v}$ is defined as $$ \hat{v} = \left( \begin{array}{lll} 0 & -v_z & v_y \\ v_z & 0 & -v_x \\ -v_y & v_x & 0 \end{array} \right) $$ Lema 5.4. (The hat operator). For a vector $T\in\mathbb R^3$ and a matrix $K\in\mathbb R^{3\times 3}$, if $\det(K)=+1$ and $T'=KT$, then $\widehat T = K^T \widehat{T'} K$. Proof. Since both $K^T\widehat{(\cdot)}K$ and $\widehat{K^{-1}(\cdot)}$ are linear maps from $\mathbb R^3$ to $\mathbb R^{3\times 3}$ one may directly verify that these two linear maps agree on the basis vectors $[1,0,0]^T$, $[0,1,0]^T$ and $[0,0,1]^T$ (using the fact that $\det(K)=1$). This is Lemma 5.4 from An Invitation to 3-D Vision (Yi Ma et al.), page 113 .",,"['linear-algebra', 'matrices', 'proof-explanation']"
45,"Let $A$, $B$ and $C$ be complex matrices such that $C\neq 0, $ $AC=CB$. Prove that $A$ and $B$ have a common eigenvalue. [duplicate]","Let ,  and  be complex matrices such that  . Prove that  and  have a common eigenvalue. [duplicate]","A B C C\neq 0,  AC=CB A B","This question already has answers here : There exists $C\neq0$ with $CA=BC$ iff $A$ and $B$ have a common eigenvalue (5 answers) Closed 6 years ago . Let $A$, $B$ and $C$ be complex matrices such that $C\neq 0, $ $AC=CB$. Prove that $A$ and $B$ have a common eigenvalue. There is a hint in the question, these facts can be used for the prove: For a complex matrices $A, B$, If $AB = 0$, and $B$ is invertible, $A = 0$. For a complex matrices $A, B$ and $C$, if $AB = BC$ than for each natural number $k$, $\\\\A^kB = BC^k $. Any ideas?","This question already has answers here : There exists $C\neq0$ with $CA=BC$ iff $A$ and $B$ have a common eigenvalue (5 answers) Closed 6 years ago . Let $A$, $B$ and $C$ be complex matrices such that $C\neq 0, $ $AC=CB$. Prove that $A$ and $B$ have a common eigenvalue. There is a hint in the question, these facts can be used for the prove: For a complex matrices $A, B$, If $AB = 0$, and $B$ is invertible, $A = 0$. For a complex matrices $A, B$ and $C$, if $AB = BC$ than for each natural number $k$, $\\\\A^kB = BC^k $. Any ideas?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
46,Is there any sufficient condition under which the hadamard product of two square matrices is invertible?,Is there any sufficient condition under which the hadamard product of two square matrices is invertible?,,Is there any sufficient condition under which the hadamard product of two square matrices is invertible ?,Is there any sufficient condition under which the hadamard product of two square matrices is invertible ?,,"['matrices', 'inverse']"
47,"Given matrix $A$, show that there exists a matrix $B$ such that $A = \exp(B)$","Given matrix , show that there exists a matrix  such that",A B A = \exp(B),"Let $$A = \begin{pmatrix} 2 & 2\\ 2 & 5\end{pmatrix}$$ Show that there exists a matrix $B$ such that $A = \exp(B)$. My thought is to use $\log(A)$, but first I need to make sure this is well-defined. The eigenvalues of $A$ are $6$ and $1$, so $$A=Q^{-1}\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix}Q$$ for some invertible $Q$. My first attempt was to use the fact that $$\log(x)=\sum_{n=1}^{\infty}(-1)^{n+1}\frac{(x-1)^n}{n}$$ for $|x-1| < 1$. However, all the matrix norms I can think of give me $\|A-I\|>1$. So my second approach was to note that $1, 6 \in [1,6]$, which is compact, so Weierstrass approx. $\implies$ there exists polynomials $P_n:[1,6]\to\mathbb{R}$ s.t. $P_n\to log$ (uniformly). Then $$P(Q^{-1}\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix}Q)=Q^{-1}P(\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix})Q=Q^{-1}\begin{pmatrix}P(6) & 0\\0 & P(1)\end{pmatrix}Q$$ for any polynomial $P$. Hence, $$P_n(Q^{-1}\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix}Q)\to Q^{-1}\begin{pmatrix}log(6) & 0\\0 & 0\end{pmatrix}Q$$ so $$\log(A)=Q^{-1}\begin{pmatrix} \log(6) & 0\\ 0 & 0\end{pmatrix}Q$$ is well-defined. Does this look correct? If so, does anyone have a simpler solution? Thank you.","Let $$A = \begin{pmatrix} 2 & 2\\ 2 & 5\end{pmatrix}$$ Show that there exists a matrix $B$ such that $A = \exp(B)$. My thought is to use $\log(A)$, but first I need to make sure this is well-defined. The eigenvalues of $A$ are $6$ and $1$, so $$A=Q^{-1}\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix}Q$$ for some invertible $Q$. My first attempt was to use the fact that $$\log(x)=\sum_{n=1}^{\infty}(-1)^{n+1}\frac{(x-1)^n}{n}$$ for $|x-1| < 1$. However, all the matrix norms I can think of give me $\|A-I\|>1$. So my second approach was to note that $1, 6 \in [1,6]$, which is compact, so Weierstrass approx. $\implies$ there exists polynomials $P_n:[1,6]\to\mathbb{R}$ s.t. $P_n\to log$ (uniformly). Then $$P(Q^{-1}\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix}Q)=Q^{-1}P(\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix})Q=Q^{-1}\begin{pmatrix}P(6) & 0\\0 & P(1)\end{pmatrix}Q$$ for any polynomial $P$. Hence, $$P_n(Q^{-1}\begin{pmatrix}6 & 0\\0 & 1\end{pmatrix}Q)\to Q^{-1}\begin{pmatrix}log(6) & 0\\0 & 0\end{pmatrix}Q$$ so $$\log(A)=Q^{-1}\begin{pmatrix} \log(6) & 0\\ 0 & 0\end{pmatrix}Q$$ is well-defined. Does this look correct? If so, does anyone have a simpler solution? Thank you.",,"['linear-algebra', 'matrices', 'matrix-exponential']"
48,"Show that : $e^{tA}=I+g(t,a)xy^T$",Show that :,"e^{tA}=I+g(t,a)xy^T","Suppose $x,y \in \Bbb R^n$ and let $A = xy^T$. Futher, let $\alpha = x^Ty$. Show That $e^{tA} = I + g(t,\alpha)xy^T$ where  $$g(t,\alpha)=\begin{cases}{1\over\alpha}(e^{\alpha t}-1)&\text{if $\alpha \neq 0$}\\t &\text {if $\alpha=0$}\end{cases}$$ I can proof with condition $\alpha = 0$. But I cant solve when $\alpha \neq 0$ When $\alpha = 0$. We need to show that $$e^{tA}=I+txy^T=I+tA$$ We have :  $$e^{tA} = \sum_{k=0}^\infty{{t^kA^k}\over k!}$$ Note that : $$AA=A^2=xy^Txy^T=yx^Tyx^T=y\alpha x^T $$ $$AAA=A^3=y\alpha x^Tyx^T=y\alpha \alpha x^T = y\alpha^2x^T$$  So : $$A^k=y\alpha^{k-1}x^T$$ then : $\alpha = 0 , A^k = 0 \text { with  $k > 1$}$ $\Rightarrow e^{tA} = \sum_{k=0}^\infty{{t^kA^k}\over k!}=I+tA$","Suppose $x,y \in \Bbb R^n$ and let $A = xy^T$. Futher, let $\alpha = x^Ty$. Show That $e^{tA} = I + g(t,\alpha)xy^T$ where  $$g(t,\alpha)=\begin{cases}{1\over\alpha}(e^{\alpha t}-1)&\text{if $\alpha \neq 0$}\\t &\text {if $\alpha=0$}\end{cases}$$ I can proof with condition $\alpha = 0$. But I cant solve when $\alpha \neq 0$ When $\alpha = 0$. We need to show that $$e^{tA}=I+txy^T=I+tA$$ We have :  $$e^{tA} = \sum_{k=0}^\infty{{t^kA^k}\over k!}$$ Note that : $$AA=A^2=xy^Txy^T=yx^Tyx^T=y\alpha x^T $$ $$AAA=A^3=y\alpha x^Tyx^T=y\alpha \alpha x^T = y\alpha^2x^T$$  So : $$A^k=y\alpha^{k-1}x^T$$ then : $\alpha = 0 , A^k = 0 \text { with  $k > 1$}$ $\Rightarrow e^{tA} = \sum_{k=0}^\infty{{t^kA^k}\over k!}=I+tA$",,"['matrices', 'matrix-exponential']"
49,To prove there are no Matrices $A$ and $B$ such that $AB-BA=kI$,To prove there are no Matrices  and  such that,A B AB-BA=kI,Prove that there are no Matrices $A$ and $B$ such that $AB-BA=kI$ where $k \ne 0$ Now since the products $AB$ and $BA$ are both defined and a subtraction exists between them so obviously both are square matrices of same order. Actually i have proved this by considering generic $2\times 2$ matrices. Letting $$A=\begin{bmatrix} a & b\\   c& d \end{bmatrix}$$ Letting $$B=\begin{bmatrix} p & q\\   r& s \end{bmatrix}$$ Now $$AB-BA=\begin{bmatrix} br-qc & q(a-d)+b(s-p)\\   c(p-s)+r(d-a)& cq-br \end{bmatrix}=\begin{bmatrix} k & o\\   0& k \end{bmatrix}$$ $\implies$ $$br-qc=k$$ and $$br-qc=-k$$ which is not valid unless $k =0$ is there a formal proof?,Prove that there are no Matrices $A$ and $B$ such that $AB-BA=kI$ where $k \ne 0$ Now since the products $AB$ and $BA$ are both defined and a subtraction exists between them so obviously both are square matrices of same order. Actually i have proved this by considering generic $2\times 2$ matrices. Letting $$A=\begin{bmatrix} a & b\\   c& d \end{bmatrix}$$ Letting $$B=\begin{bmatrix} p & q\\   r& s \end{bmatrix}$$ Now $$AB-BA=\begin{bmatrix} br-qc & q(a-d)+b(s-p)\\   c(p-s)+r(d-a)& cq-br \end{bmatrix}=\begin{bmatrix} k & o\\   0& k \end{bmatrix}$$ $\implies$ $$br-qc=k$$ and $$br-qc=-k$$ which is not valid unless $k =0$ is there a formal proof?,,"['linear-algebra', 'matrices', 'systems-of-equations']"
50,Question about two idempotent symmetric matrices,Question about two idempotent symmetric matrices,,"Let $A$ and $B$ be two idempotent symmetric matrices such that the difference matrix $A-B$ is a positive semidefinite matrix. The question is to show that $$AB=BA=B.$$ I first thought that the problem is about simultaneous diagonalization, but I don't know how to proceed.","Let $A$ and $B$ be two idempotent symmetric matrices such that the difference matrix $A-B$ is a positive semidefinite matrix. The question is to show that $$AB=BA=B.$$ I first thought that the problem is about simultaneous diagonalization, but I don't know how to proceed.",,"['linear-algebra', 'matrices', 'statistics', 'positive-definite', 'idempotents']"
51,Are there any matrices satisfying $|A+B|=|A|+|B|$ [duplicate],Are there any matrices satisfying  [duplicate],|A+B|=|A|+|B|,"This question already has answers here : Does $\det(A + B) = \det(A) + \det(B)$ hold? (5 answers) Closed 7 years ago . Are there any $n \times n$ $(n \ne 1)$ matrices satisfying $$Det(A+B)=Det(A)+Det(B)$$ Trivially the above equation is True for either $A=O$ or $B=O$ or $ A=B=O$ For $2 \times 2$ matrix i have tried  as follows: Let $$A=\begin{bmatrix} a &b \\  c & d \end{bmatrix}$$  and $$B=\begin{bmatrix} p &q \\  r &s \end{bmatrix}$$  Then we have $$Det(A+B)=(a+p)(d+s)-(b+q)(c+r)$$ $$Det(A)+Det(B)=ad-bc+ps-rq$$  Equating both we get $$as+pd=br+qc$$  which is satisfied by infinite values of $a,b,c,d,p,q,r,s$ One such pair is $$A=\begin{bmatrix} 2 &10 \\  2 & 3 \end{bmatrix} $$ and $$B=\begin{bmatrix} 4 &1 \\  2 &5 \end{bmatrix}$$ is there any general way to find the matrices of higher order?","This question already has answers here : Does $\det(A + B) = \det(A) + \det(B)$ hold? (5 answers) Closed 7 years ago . Are there any $n \times n$ $(n \ne 1)$ matrices satisfying $$Det(A+B)=Det(A)+Det(B)$$ Trivially the above equation is True for either $A=O$ or $B=O$ or $ A=B=O$ For $2 \times 2$ matrix i have tried  as follows: Let $$A=\begin{bmatrix} a &b \\  c & d \end{bmatrix}$$  and $$B=\begin{bmatrix} p &q \\  r &s \end{bmatrix}$$  Then we have $$Det(A+B)=(a+p)(d+s)-(b+q)(c+r)$$ $$Det(A)+Det(B)=ad-bc+ps-rq$$  Equating both we get $$as+pd=br+qc$$  which is satisfied by infinite values of $a,b,c,d,p,q,r,s$ One such pair is $$A=\begin{bmatrix} 2 &10 \\  2 & 3 \end{bmatrix} $$ and $$B=\begin{bmatrix} 4 &1 \\  2 &5 \end{bmatrix}$$ is there any general way to find the matrices of higher order?",,"['linear-algebra', 'matrices', 'determinant']"
52,Linear transformation and its matrix with respect to unknown bases,Linear transformation and its matrix with respect to unknown bases,,"I am given a linear transformation $$T:\mathbb{R^3} \rightarrow \mathbb{R^2}$$ $$T((x,y,z)) = (x+y,-y+z)$$ The task if to find a basis in $\mathbb{R^3}$, let's call it $B=\{e_1, e_2, e_3\}$ and $\mathbb{R^2}$, let's call it $B'=\{f_1, f_2\}$ such that $A$ is the matrix of this transformation with respect to the found bases. Here is the matrix $A$: $$A = \begin{bmatrix} 1&0&0\\0&2&0\end{bmatrix}$$ I think I am not sure how to interpret the given matrix $A$.","I am given a linear transformation $$T:\mathbb{R^3} \rightarrow \mathbb{R^2}$$ $$T((x,y,z)) = (x+y,-y+z)$$ The task if to find a basis in $\mathbb{R^3}$, let's call it $B=\{e_1, e_2, e_3\}$ and $\mathbb{R^2}$, let's call it $B'=\{f_1, f_2\}$ such that $A$ is the matrix of this transformation with respect to the found bases. Here is the matrix $A$: $$A = \begin{bmatrix} 1&0&0\\0&2&0\end{bmatrix}$$ I think I am not sure how to interpret the given matrix $A$.",,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations']"
53,On the sine of a two by two matrix with integer entries,On the sine of a two by two matrix with integer entries,,"Let $A \in M(2,\mathbb Z)$ ; Let $\sin A :=\sum _{n=0}^\infty \dfrac {(-1)^n}{(2n+1)!}A^{2n+1}$ (This series is known to converge absolutely in the Banach space $\mathcal L(\mathbb R^2)$ under Operator norm ) . If $\sin A \in M(2,\mathbb Z)$ , then is it true that $A^2=O$ ?","Let $A \in M(2,\mathbb Z)$ ; Let $\sin A :=\sum _{n=0}^\infty \dfrac {(-1)^n}{(2n+1)!}A^{2n+1}$ (This series is known to converge absolutely in the Banach space $\mathcal L(\mathbb R^2)$ under Operator norm ) . If $\sin A \in M(2,\mathbb Z)$ , then is it true that $A^2=O$ ?",,"['matrices', 'number-theory']"
54,$S=T\circ T + T+\mathrm{Id}_V$ and there is a $T$-invariant subspace $U\subseteq V$ with $ \dim (U) = 2$ [closed],and there is a -invariant subspace  with  [closed],S=T\circ T + T+\mathrm{Id}_V T U\subseteq V  \dim (U) = 2,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $V$ be a vector space with $\dim V < \infty$. Let $T, S:V\rightarrow V$ be linear operators, $S=T\circ T + T+\mathrm{Id}_V$ and $\dim\mathrm{Im}(S) < \dim (V)$. How to prove that there exists a $T$-invariant subspace, $U \subseteq V$, such that $\dim(U) = 2$ ?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $V$ be a vector space with $\dim V < \infty$. Let $T, S:V\rightarrow V$ be linear operators, $S=T\circ T + T+\mathrm{Id}_V$ and $\dim\mathrm{Im}(S) < \dim (V)$. How to prove that there exists a $T$-invariant subspace, $U \subseteq V$, such that $\dim(U) = 2$ ?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces']"
55,"If $A^2=O$, then prove that $I+A$ is invertible and find $(I+A)^{-1}$","If , then prove that  is invertible and find",A^2=O I+A (I+A)^{-1},"So I'm stuck on this Linear Algebra question. My first (naive) train of thought here was to go through with an implication that $A^2=O$ implies $A=O$. Then this got quickly debunked having read up on nilpotent matrices. So I'm back to square one without a clue as to how to proceed. Any hints will be much appreciated on where to begin with this, has been pestering me for at least one day now. EDIT: Thank you everyone, very happy for all your assistance.","So I'm stuck on this Linear Algebra question. My first (naive) train of thought here was to go through with an implication that $A^2=O$ implies $A=O$. Then this got quickly debunked having read up on nilpotent matrices. So I'm back to square one without a clue as to how to proceed. Any hints will be much appreciated on where to begin with this, has been pestering me for at least one day now. EDIT: Thank you everyone, very happy for all your assistance.",,"['linear-algebra', 'matrices', 'inverse']"
56,Compute matrix determinant for matrix with $\lambda$ on diagonal and $\mu$ elsewhere [duplicate],Compute matrix determinant for matrix with  on diagonal and  elsewhere [duplicate],\lambda \mu,"This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 7 years ago . Given $n\in \mathbb N$, how can I compute the determinant of $(a_{ij})_{1\leq i,j \leq n} \in \mathcal M_{n\times n}(\mathbb R)$ where, for each $1\leq i,j\leq n:$ $$a_{ij}= \begin{cases} \lambda &i=j \\ \mu & i\neq j \end{cases}$$ The only (or the only easy) way to do this I think involves using Laplace's formula, expanding along any row. But doing this everything gets too messy, I can't express it in a good way. I tried to get a recursive relation and prove it via induction but didn't get anywhere. Is there an easier way? Any hints appreciated.","This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 7 years ago . Given $n\in \mathbb N$, how can I compute the determinant of $(a_{ij})_{1\leq i,j \leq n} \in \mathcal M_{n\times n}(\mathbb R)$ where, for each $1\leq i,j\leq n:$ $$a_{ij}= \begin{cases} \lambda &i=j \\ \mu & i\neq j \end{cases}$$ The only (or the only easy) way to do this I think involves using Laplace's formula, expanding along any row. But doing this everything gets too messy, I can't express it in a good way. I tried to get a recursive relation and prove it via induction but didn't get anywhere. Is there an easier way? Any hints appreciated.",,"['linear-algebra', 'matrices', 'determinant']"
57,General form of a shear map,General form of a shear map,,"From Wikipedia In plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction. I'm interested in the matrix representation of a general shear map in the plane.  Every resource I look at either only gives the horizontal and vertical shear matrices $$\begin{bmatrix} 1 & k \\ 0 & 1\end{bmatrix} \quad\text{and}\quad \begin{bmatrix} 1 & 0 \\ k & 1\end{bmatrix}$$ or a couple have said that $$\begin{bmatrix} 1 & a \\ b & 1\end{bmatrix}$$ is also a shear map.  However I don't think that last one is if neither $a$ nor $b$ is zero because, as far as I understand, shear mappings should be area preserving. So then what is the general form of the matrix representing a shear map that displaces all vectors in the direction parallel to an arbitrary vector $(x,y)$?","From Wikipedia In plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction. I'm interested in the matrix representation of a general shear map in the plane.  Every resource I look at either only gives the horizontal and vertical shear matrices $$\begin{bmatrix} 1 & k \\ 0 & 1\end{bmatrix} \quad\text{and}\quad \begin{bmatrix} 1 & 0 \\ k & 1\end{bmatrix}$$ or a couple have said that $$\begin{bmatrix} 1 & a \\ b & 1\end{bmatrix}$$ is also a shear map.  However I don't think that last one is if neither $a$ nor $b$ is zero because, as far as I understand, shear mappings should be area preserving. So then what is the general form of the matrix representing a shear map that displaces all vectors in the direction parallel to an arbitrary vector $(x,y)$?",,"['linear-algebra', 'matrices', 'geometry', 'linear-transformations']"
58,Why can't this matrix have a right inverse?,Why can't this matrix have a right inverse?,,"Let $A$ be an $m \times n$ matrix with m > n. Why can't $A$ have a right inverse. We want $AB = I_m$, why is this impossible if $m > n$?","Let $A$ be an $m \times n$ matrix with m > n. Why can't $A$ have a right inverse. We want $AB = I_m$, why is this impossible if $m > n$?",,"['linear-algebra', 'matrices', 'vector-spaces']"
59,How to prove that sub-level set of spectral radius function is not convex,How to prove that sub-level set of spectral radius function is not convex,,Let $A \in \mathbb{R}^{n \times n}$ and $\rho(A)=\{|\lambda|_\max : \lambda \text{ is an eigen-value of } A\}$. I am trying to prove that the set  \begin{align*} B_\alpha=\{A: \rho(A) < \alpha \} \end{align*} need not be a convex set in general (for any $\alpha>0$). Can anyone provide a useful hint about how to proceed?,Let $A \in \mathbb{R}^{n \times n}$ and $\rho(A)=\{|\lambda|_\max : \lambda \text{ is an eigen-value of } A\}$. I am trying to prove that the set  \begin{align*} B_\alpha=\{A: \rho(A) < \alpha \} \end{align*} need not be a convex set in general (for any $\alpha>0$). Can anyone provide a useful hint about how to proceed?,,"['matrices', 'convex-analysis']"
60,Smallest eigenvalue is supremum for...,Smallest eigenvalue is supremum for...,,Suppose $A \in \mathbb{R}^{nxn}$ symmetric and positive definit. We can show that: $$\exists \epsilon > 0 \in \mathbb{R}: \forall x \in \mathbb{R}^n: x^TAx \ge \epsilon \Vert x \Vert^2$$ My claim is that the smallest eigenvalue of $A$ is the supremum for those $\epsilon$. I'd appreciate to see a prove/disprove!,Suppose $A \in \mathbb{R}^{nxn}$ symmetric and positive definit. We can show that: $$\exists \epsilon > 0 \in \mathbb{R}: \forall x \in \mathbb{R}^n: x^TAx \ge \epsilon \Vert x \Vert^2$$ My claim is that the smallest eigenvalue of $A$ is the supremum for those $\epsilon$. I'd appreciate to see a prove/disprove!,,"['linear-algebra', 'matrices', 'proof-writing', 'positive-definite']"
61,Rank of the $n \times n$ matrix with ones on the main diagonal and $a$ off the main diagonal,Rank of the  matrix with ones on the main diagonal and  off the main diagonal,n \times n a,"I want to find the rank of this $n\times n$ matrix \begin{pmatrix} 1 & a & a & \cdots & \cdots & a \\ a & 1 & a & \cdots & \cdots & a \\ a & a & 1 & a & \cdots & a \\ \vdots & \vdots & a& \ddots & & \vdots\\ \vdots & \vdots & \vdots & & \ddots & \vdots \\ a & a & a & \cdots  &\cdots & 1 \end{pmatrix} that is, the matrix whose diagonals are $1's$ and $a$ otherwise, where $a$ is any real number. My first observation is when $a=0$ the rank is $n$ and when $a=1$ the rank is $1.$ Then I can assume $a\neq 0, 1$ and proceed row reduction to find its pivot rows. I obtain \begin{pmatrix} 1 & a & a & \cdots  & a \\ 0 & 1+a & a & \cdots & a \\ 0 & a & 1+a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & a & a & \cdots  & 1+a \end{pmatrix} by subtracting the first row multiplied $a$ for each row below the first, and then divides the factor $(1-a)$,  and stuck there. Any hints/helps?","I want to find the rank of this $n\times n$ matrix \begin{pmatrix} 1 & a & a & \cdots & \cdots & a \\ a & 1 & a & \cdots & \cdots & a \\ a & a & 1 & a & \cdots & a \\ \vdots & \vdots & a& \ddots & & \vdots\\ \vdots & \vdots & \vdots & & \ddots & \vdots \\ a & a & a & \cdots  &\cdots & 1 \end{pmatrix} that is, the matrix whose diagonals are $1's$ and $a$ otherwise, where $a$ is any real number. My first observation is when $a=0$ the rank is $n$ and when $a=1$ the rank is $1.$ Then I can assume $a\neq 0, 1$ and proceed row reduction to find its pivot rows. I obtain \begin{pmatrix} 1 & a & a & \cdots  & a \\ 0 & 1+a & a & \cdots & a \\ 0 & a & 1+a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & a & a & \cdots  & 1+a \end{pmatrix} by subtracting the first row multiplied $a$ for each row below the first, and then divides the factor $(1-a)$,  and stuck there. Any hints/helps?",,"['linear-algebra', 'matrices', 'matrix-rank', 'symmetric-matrices']"
62,Is every 1x1 matrix a diagonal matrix?,Is every 1x1 matrix a diagonal matrix?,,"By definition of a diagonal matrix, a square matrix is said to be diagonal if all its non-diagonal elements are zero.  So, can a 1x1 matrix be considered diagonal by this definition?","By definition of a diagonal matrix, a square matrix is said to be diagonal if all its non-diagonal elements are zero.  So, can a 1x1 matrix be considered diagonal by this definition?",,['matrices']
63,What are some transformations that leave the eigenvalues of a matrix unchanged?,What are some transformations that leave the eigenvalues of a matrix unchanged?,,I'm looking for transformations (isometries or not) that leave the spectrum of a matrix unchanged. I've only been able to come up with similarity transformations and transposition (is transposition in fact just a similarity transformation?). What are some others? Thanks.,I'm looking for transformations (isometries or not) that leave the spectrum of a matrix unchanged. I've only been able to come up with similarity transformations and transposition (is transposition in fact just a similarity transformation?). What are some others? Thanks.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'transformation', 'invariance']"
64,A matrix is given in echelon form,A matrix is given in echelon form,,"This is my first time posting a question and I'm really stuck on this one. Help would be appreciated.  I'm given this matrix $$\left[ \begin{array}{ccc|c} 2&3&0&9\\0&1&\lambda+6&4\\ 0&0&\lambda^2-5\lambda+6&9-3\lambda \end{array}\right], $$ and I need to find for which  λ ∈ ℝ this matrix has one solution no solutions infinite solutions","This is my first time posting a question and I'm really stuck on this one. Help would be appreciated.  I'm given this matrix $$\left[ \begin{array}{ccc|c} 2&3&0&9\\0&1&\lambda+6&4\\ 0&0&\lambda^2-5\lambda+6&9-3\lambda \end{array}\right], $$ and I need to find for which  λ ∈ ℝ this matrix has one solution no solutions infinite solutions",,"['matrices', 'matrix-equations']"
65,Is the sum of the entries of a symmetric positive semidefinite matrix positive?,Is the sum of the entries of a symmetric positive semidefinite matrix positive?,,"Given that $A$ is an $n\times n$, symmetric and positive semidefinite, how would you prove that the sum of the entries of $A$ is positive?","Given that $A$ is an $n\times n$, symmetric and positive semidefinite, how would you prove that the sum of the entries of $A$ is positive?",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite']"
66,Matrix A multiply A transpose,Matrix A multiply A transpose,,"Suppose $A$ is an $n\times n$ matrix, and wondering when $A A^T$ equals $A^T A$? Only when $A$ is symmetric? regards, Lin","Suppose $A$ is an $n\times n$ matrix, and wondering when $A A^T$ equals $A^T A$? Only when $A$ is symmetric? regards, Lin",,"['matrices', 'transpose']"
67,Prove the solution set to homogeneous equations with this form,Prove the solution set to homogeneous equations with this form,,"Let a homogeneous system be given by: $$\left(\begin{matrix} 0    & \pm1 & \pm1 & \ldots & \pm1 \\ \pm1 & 0    & \pm1 & \ldots & \pm1 \\ \pm1 & \pm1 & 0    & \ldots & \pm1 \\ &&\ldots \\ \pm1 & \pm1 & \pm1    & \ldots & 0 \\ \end{matrix}\right){\bf{x}}={\bf{0}}$$ Where this matrix is $n\times n$, where $n$ is odd, and the sum of each row is zero - that is, there are exactly $(n-1)/2$ ones, $(n-1)/2$ minus ones, and one zero in each row. I wish to prove that the solution set is the set of vectors $\bf{x}$ with all elements of $\bf{x}$ being equal. Is there a nice matrix based way of proving this? I think this may be proved using induction, but a non-inductive method would be preferable.","Let a homogeneous system be given by: $$\left(\begin{matrix} 0    & \pm1 & \pm1 & \ldots & \pm1 \\ \pm1 & 0    & \pm1 & \ldots & \pm1 \\ \pm1 & \pm1 & 0    & \ldots & \pm1 \\ &&\ldots \\ \pm1 & \pm1 & \pm1    & \ldots & 0 \\ \end{matrix}\right){\bf{x}}={\bf{0}}$$ Where this matrix is $n\times n$, where $n$ is odd, and the sum of each row is zero - that is, there are exactly $(n-1)/2$ ones, $(n-1)/2$ minus ones, and one zero in each row. I wish to prove that the solution set is the set of vectors $\bf{x}$ with all elements of $\bf{x}$ being equal. Is there a nice matrix based way of proving this? I think this may be proved using induction, but a non-inductive method would be preferable.",,"['linear-algebra', 'matrices', 'systems-of-equations']"
68,Why isn't the SVD simplified to USV (no transpose)?,Why isn't the SVD simplified to USV (no transpose)?,,"I've looked at a lot of explanations of the SVD, and they all phrase it as $A = USV^T$ (or $V^*$ for complex matrices), and where V is unitary. But the conjugate of a unitary matrix is always unitary, right? Wouldn't a simpler definition be $A = USV$?","I've looked at a lot of explanations of the SVD, and they all phrase it as $A = USV^T$ (or $V^*$ for complex matrices), and where V is unitary. But the conjugate of a unitary matrix is always unitary, right? Wouldn't a simpler definition be $A = USV$?",,"['linear-algebra', 'matrices', 'notation', 'matrix-decomposition', 'svd']"
69,Determinant of a 3x3 matrix with trig.,Determinant of a 3x3 matrix with trig.,,"So I'm not sure if this is a simple question to solve, but I was going through some exam review for my upcoming Linear Algebra exam and I came across this question. What is the determinant of the following matrix? $$         \begin{vmatrix}         1 & 1 & 1 \\         \sin^2(a) & \sin^2(b) & \sin^2(c) \\         \cos^2(a) & \cos^2(b) & \cos^2(c) \\         \end{vmatrix} $$ We never went over anything like this in lecture, so I am at a loss as to where I should even start. I found this example but its not quite the same as this question.","So I'm not sure if this is a simple question to solve, but I was going through some exam review for my upcoming Linear Algebra exam and I came across this question. What is the determinant of the following matrix? $$         \begin{vmatrix}         1 & 1 & 1 \\         \sin^2(a) & \sin^2(b) & \sin^2(c) \\         \cos^2(a) & \cos^2(b) & \cos^2(c) \\         \end{vmatrix} $$ We never went over anything like this in lecture, so I am at a loss as to where I should even start. I found this example but its not quite the same as this question.",,"['linear-algebra', 'matrices', 'determinant']"
70,"$n$ by $n$ matrices such that $Ax=0$ implies $Bx=0$, then what can we say about $A$ and $B$?","by  matrices such that  implies , then what can we say about  and ?",n n Ax=0 Bx=0 A B,"Ok, this is one of the interesting questions I encountered in my GRE Maths exam 2 weeks ago. Fix an $x\in \mathbb{R}^n$ (doesn't have to be $0$), and $A$, $B$ are $n$ by $n$ matrices such that $Ax=0$ implies $Bx=0$, then what can we say about $A$ and $B$? I think the choices were: $\exists C$ an $n$ by $n$ matrices such that:  a) $A=BC$, b) $A=CB$, c) $B=CA$, d) $B=AC$, e) $B=C^{-1}AC$. Well, the set of matrices $D$ such that $Dx=0$ form an left ideal, but I only know what bi-ideals look like in this case. Also the wording the this question is funny, we dont know if $A$ is in the ideal or not, we just know if $A$ is in the ideal then $B$ is in the ideal. Edit: As we discussed below, I think I misunderstood the question in the exam (although I don't have the actually paper now). $x$ is probably not fixed.","Ok, this is one of the interesting questions I encountered in my GRE Maths exam 2 weeks ago. Fix an $x\in \mathbb{R}^n$ (doesn't have to be $0$), and $A$, $B$ are $n$ by $n$ matrices such that $Ax=0$ implies $Bx=0$, then what can we say about $A$ and $B$? I think the choices were: $\exists C$ an $n$ by $n$ matrices such that:  a) $A=BC$, b) $A=CB$, c) $B=CA$, d) $B=AC$, e) $B=C^{-1}AC$. Well, the set of matrices $D$ such that $Dx=0$ form an left ideal, but I only know what bi-ideals look like in this case. Also the wording the this question is funny, we dont know if $A$ is in the ideal or not, we just know if $A$ is in the ideal then $B$ is in the ideal. Edit: As we discussed below, I think I misunderstood the question in the exam (although I don't have the actually paper now). $x$ is probably not fixed.",,"['linear-algebra', 'matrices', 'ideals', 'gre-exam']"
71,Is the norm of a singular matrix necessarily zero?,Is the norm of a singular matrix necessarily zero?,,"If not, can you give example of when it's not?","If not, can you give example of when it's not?",,"['linear-algebra', 'matrices', 'normed-spaces']"
72,The logarithm of a symmetric positive definite matrix as a function,The logarithm of a symmetric positive definite matrix as a function,,"Can the logarithm of a symmetric,positive definite matrix always be expressed locally as a power series? That is: Is the logarithm function analytic on the space of symmetric, positive definite matrices?","Can the logarithm of a symmetric,positive definite matrix always be expressed locally as a power series? That is: Is the logarithm function analytic on the space of symmetric, positive definite matrices?",,"['linear-algebra', 'matrices']"
73,What exactly is a matrix transformation?,What exactly is a matrix transformation?,,"I'm confused on the notation for $\mathbb{R}^{m}$ and $\mathbb{R}^{n}$ and its relationship to linear transformations. We say that $\mathbb{R}^{m}$ is just the space of column vectors since there are n columns, right? If so, then what do we mean when we say T: $\mathbb{R}^{m} \mapsto \mathbb{R}^{n}$ is equivalent to $T_A(X) = AX$? I watched a video on linear algebra (essence of Linear Algebra) that (only if I'm remembering correctly) that all matrix multiplication was is the change of the standard basis vectors such as $\hat{\imath} \space \hat{\jmath}$ landed on different spots in the same dimension. What is exactly going on here? Can you have linear transformations that don't depend on the number of rows or columns to be the same? Geometrically, what is going on? It seems like I have no idea what is going on in a matrix at all.","I'm confused on the notation for $\mathbb{R}^{m}$ and $\mathbb{R}^{n}$ and its relationship to linear transformations. We say that $\mathbb{R}^{m}$ is just the space of column vectors since there are n columns, right? If so, then what do we mean when we say T: $\mathbb{R}^{m} \mapsto \mathbb{R}^{n}$ is equivalent to $T_A(X) = AX$? I watched a video on linear algebra (essence of Linear Algebra) that (only if I'm remembering correctly) that all matrix multiplication was is the change of the standard basis vectors such as $\hat{\imath} \space \hat{\jmath}$ landed on different spots in the same dimension. What is exactly going on here? Can you have linear transformations that don't depend on the number of rows or columns to be the same? Geometrically, what is going on? It seems like I have no idea what is going on in a matrix at all.",,"['linear-algebra', 'matrices', 'linear-transformations']"
74,"Find two $2\times2$ matrices which have the same eigenvalues, but are not similar","Find two  matrices which have the same eigenvalues, but are not similar",2\times2,"Find two matrices $\mathbf{A}\in\mathbb{C}^{2\times2}$ and $\mathbf{B}\in\mathbb{C}^{2\times2}$ such that: they have the same eigenvalues but they are NOT similar MY ATTEMPT The characteristic polynomial of $\mathbf{A}$ is $p_{\mathbf{A}}(\lambda)=\lambda^{2}-\mathrm{tr}(\mathbf{A})+\mathrm{det}(\mathbf{A})$ and the characteristic polynomial of $\mathbf{B}$ is $p_{\mathbf{B}}(\lambda)=\lambda^{2}-\mathrm{tr}(\mathbf{B})+\mathrm{det}(\mathbf{B})$. ""The same eigenvalues"" $\Longleftrightarrow P_{\mathbf{A}}(\lambda)=P_{\mathbf{B}}(\lambda)\Longleftrightarrow$ $\Longleftrightarrow\begin{cases} \begin{array}{c} \mathrm{tr}(\mathbf{A})=\mathrm{tr}(\mathbf{B})\\ \mathrm{det}(\mathbf{A})=\mathrm{det}(\mathbf{B}) \end{array} & \Longleftrightarrow\end{cases}\begin{cases} \begin{array}{c} a_{11}+a_{22}=b_{11}+b_{22}\\ a_{11}a_{22}-a_{21}a_{12}=b_{11}b_{22}-b_{21}b_{12} \end{array}\end{cases}$ And now?","Find two matrices $\mathbf{A}\in\mathbb{C}^{2\times2}$ and $\mathbf{B}\in\mathbb{C}^{2\times2}$ such that: they have the same eigenvalues but they are NOT similar MY ATTEMPT The characteristic polynomial of $\mathbf{A}$ is $p_{\mathbf{A}}(\lambda)=\lambda^{2}-\mathrm{tr}(\mathbf{A})+\mathrm{det}(\mathbf{A})$ and the characteristic polynomial of $\mathbf{B}$ is $p_{\mathbf{B}}(\lambda)=\lambda^{2}-\mathrm{tr}(\mathbf{B})+\mathrm{det}(\mathbf{B})$. ""The same eigenvalues"" $\Longleftrightarrow P_{\mathbf{A}}(\lambda)=P_{\mathbf{B}}(\lambda)\Longleftrightarrow$ $\Longleftrightarrow\begin{cases} \begin{array}{c} \mathrm{tr}(\mathbf{A})=\mathrm{tr}(\mathbf{B})\\ \mathrm{det}(\mathbf{A})=\mathrm{det}(\mathbf{B}) \end{array} & \Longleftrightarrow\end{cases}\begin{cases} \begin{array}{c} a_{11}+a_{22}=b_{11}+b_{22}\\ a_{11}a_{22}-a_{21}a_{12}=b_{11}b_{22}-b_{21}b_{12} \end{array}\end{cases}$ And now?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'similar-matrices']"
75,Using elementary row or column operations to compute a determinant,Using elementary row or column operations to compute a determinant,,How do you use elementary row or column operations to find the determinant of the following matrix? $$\begin{bmatrix} 1 & 7 & -3\\ 1 &3 & 1\\ 4&8&1\end{bmatrix}$$,How do you use elementary row or column operations to find the determinant of the following matrix? $$\begin{bmatrix} 1 & 7 & -3\\ 1 &3 & 1\\ 4&8&1\end{bmatrix}$$,,"['linear-algebra', 'matrices', 'determinant', 'gaussian-elimination']"
76,Schur decomposition to show matrix has $n$ orthonormal eigenvectors,Schur decomposition to show matrix has  orthonormal eigenvectors,n,"From Gilbert Strang's ""Introduction to Linear Algebra."" We are trying to show by Schur decomposition that all symmetric matrices are diagonalizable. We write down the Schur decomposition as $A=QTQ^{-1}$ where $A$ is square, $T$ is upper triangular and $\bar Q^{T}=Q^{-1}$, $\bar Q$ is the complex conjugate of $Q$. The text looks for $AQ=QT$ and argues that the first column of $Q$ has to be an eigenvector. Since $T$ is triangular and not necessarily diagonal, the first proposed step is to use the first column of $Q$ and supplement it with $n-1$ columns to complete an orthonormal matrix $Q_{1}$. Then we write: $\bar Q^{T}_{1}AQ_{1}=         \begin{bmatrix}         \bar q^{T}_{1} \\         \vdots \\         \bar q^{T}_{n} \\         \end{bmatrix}         \begin{bmatrix}         Aq_{1} & \cdots & Aq_{n} \\         \end{bmatrix} =         \begin{bmatrix}         t_{11} & \cdots \\         0 & A_{2} \\         \end{bmatrix} $ (1) Why do we get the right hand side? I am able to reach it by multiplication and from the properties of a complex conjugate. At the same time, shouldn't the right hand side simplify to a triangular matrix? What am I missing? Going on, the book makes and argument by ""induction"" (not a formal one). It assumes a Schur factorization $A_2=Q_{2}T_{2}Q_{2}^{-1}$ is possible for $A_{2}$ of size $n-1$. Then it ""puts"" $Q_2$ and $T_2$ into $Q$ and $T$: $Q=Q_{1}         \begin{bmatrix}         1 & 0 \\         0 & Q_{2} \\         \end{bmatrix}$ and $T=     \begin{bmatrix}         t_{11} & \cdots \\         0 & T_{2} \\         \end{bmatrix}$ and $AQ=QT$ (2) Where do we get this transformation from? In particular how can we see where $Q_{1}, T_{1}, Q_{}2, T_{2}$ fit into $Q$ and $T$? Once this is cleared, I am able to show that for a symmetric matrix $T$ is diagonal and the matrix has the requisite number of eigenvectors. Just to be clear, this is the first time the Schur decomposition is presented in the material so the answer might be obvious if you have mastered the decomposition but it definitely is not at this point in the book.","From Gilbert Strang's ""Introduction to Linear Algebra."" We are trying to show by Schur decomposition that all symmetric matrices are diagonalizable. We write down the Schur decomposition as $A=QTQ^{-1}$ where $A$ is square, $T$ is upper triangular and $\bar Q^{T}=Q^{-1}$, $\bar Q$ is the complex conjugate of $Q$. The text looks for $AQ=QT$ and argues that the first column of $Q$ has to be an eigenvector. Since $T$ is triangular and not necessarily diagonal, the first proposed step is to use the first column of $Q$ and supplement it with $n-1$ columns to complete an orthonormal matrix $Q_{1}$. Then we write: $\bar Q^{T}_{1}AQ_{1}=         \begin{bmatrix}         \bar q^{T}_{1} \\         \vdots \\         \bar q^{T}_{n} \\         \end{bmatrix}         \begin{bmatrix}         Aq_{1} & \cdots & Aq_{n} \\         \end{bmatrix} =         \begin{bmatrix}         t_{11} & \cdots \\         0 & A_{2} \\         \end{bmatrix} $ (1) Why do we get the right hand side? I am able to reach it by multiplication and from the properties of a complex conjugate. At the same time, shouldn't the right hand side simplify to a triangular matrix? What am I missing? Going on, the book makes and argument by ""induction"" (not a formal one). It assumes a Schur factorization $A_2=Q_{2}T_{2}Q_{2}^{-1}$ is possible for $A_{2}$ of size $n-1$. Then it ""puts"" $Q_2$ and $T_2$ into $Q$ and $T$: $Q=Q_{1}         \begin{bmatrix}         1 & 0 \\         0 & Q_{2} \\         \end{bmatrix}$ and $T=     \begin{bmatrix}         t_{11} & \cdots \\         0 & T_{2} \\         \end{bmatrix}$ and $AQ=QT$ (2) Where do we get this transformation from? In particular how can we see where $Q_{1}, T_{1}, Q_{}2, T_{2}$ fit into $Q$ and $T$? Once this is cleared, I am able to show that for a symmetric matrix $T$ is diagonal and the matrix has the requisite number of eigenvectors. Just to be clear, this is the first time the Schur decomposition is presented in the material so the answer might be obvious if you have mastered the decomposition but it definitely is not at this point in the book.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'schur-decomposition']"
77,Proof that linearly dependent rows/columns of $A \implies \det(A) = 0$,Proof that linearly dependent rows/columns of,A \implies \det(A) = 0,"I have seen in a few textbooks of proofs of $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent from the properties of the determinant, but I have yet to see a proof that $\det(A) = 0$ from the definition of the determinant: $$\det(A) = \sum \text{sgn}(P)a_{1\alpha}a_{2\beta}...a_{n\omega}$$ where $(\alpha, \beta, ..., \omega)$ are permutations of the columns of $A$. And since all the properties of $\det$ follow from it's definition, I find that proving $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent from the properties of the determinant, to be somewhat unsatisfactory. Can someone provide a proof from the Leibniz Expansion definition of the determinant, that I provided above, of the result that $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent?","I have seen in a few textbooks of proofs of $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent from the properties of the determinant, but I have yet to see a proof that $\det(A) = 0$ from the definition of the determinant: $$\det(A) = \sum \text{sgn}(P)a_{1\alpha}a_{2\beta}...a_{n\omega}$$ where $(\alpha, \beta, ..., \omega)$ are permutations of the columns of $A$. And since all the properties of $\det$ follow from it's definition, I find that proving $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent from the properties of the determinant, to be somewhat unsatisfactory. Can someone provide a proof from the Leibniz Expansion definition of the determinant, that I provided above, of the result that $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent?",,"['linear-algebra', 'matrices', 'determinant', 'proof-explanation']"
78,Square root of a matrix.,Square root of a matrix.,,"Determine all $A,B \in \mathbf{M}_{2}(\mathbf{R})$ such that $A^2+B^2=\begin{pmatrix} 22 & 44\\   14 & 28 \end{pmatrix}$ and $AB+BA=\begin{pmatrix} 10 & 20\\   2 & 4 \end{pmatrix}$. I have tried to assume that $A=\begin{pmatrix} a & b\\   c & d \end{pmatrix}$ and $B=\begin{pmatrix} e & f\\   g & h \end{pmatrix}$. And we know that $(A+B)^{2}=A^{2}+B^{2}+AB+BA$ and $(A-B)^{2}=A^{2}+B^{2}-(AB+BA)$. Then, I will end up with $8$ equations and $8$ which can be solve, but it is really complicated. I am just wondering if someone could show me how to take the square root of an matrix. Please if you show that please make sure that you will prove it. Would any one have an idea what is the input code for solving that in MATLAB","Determine all $A,B \in \mathbf{M}_{2}(\mathbf{R})$ such that $A^2+B^2=\begin{pmatrix} 22 & 44\\   14 & 28 \end{pmatrix}$ and $AB+BA=\begin{pmatrix} 10 & 20\\   2 & 4 \end{pmatrix}$. I have tried to assume that $A=\begin{pmatrix} a & b\\   c & d \end{pmatrix}$ and $B=\begin{pmatrix} e & f\\   g & h \end{pmatrix}$. And we know that $(A+B)^{2}=A^{2}+B^{2}+AB+BA$ and $(A-B)^{2}=A^{2}+B^{2}-(AB+BA)$. Then, I will end up with $8$ equations and $8$ which can be solve, but it is really complicated. I am just wondering if someone could show me how to take the square root of an matrix. Please if you show that please make sure that you will prove it. Would any one have an idea what is the input code for solving that in MATLAB",,['matrices']
79,Why does the eigenvalue $-1$ have even multiplicity for all the conjugacy classes in these representations?,Why does the eigenvalue  have even multiplicity for all the conjugacy classes in these representations?,-1,"I have been conducting some experiments with GAP and have noticed a pattern which I would like to disregard as a coincidence or explain theoretically. Consider a finite group $G$ and a representation of $G$ over $\mathbb{Q}$. Let $C$ be a conjugacy class in $G$. For any $g,h\in C$, the characteristic polynomials of the associated matrices in the representation are the same, so we can write $E_C$ for the associated list of eigenvalues. For many groups I have tried, with the aid of GAP, the multiplicity $m(E_C)$ of the eigenvalue $-1$ always seems to be even for all conjugacy classes $C$. I cannot see if that is just a coincidence or not. I have used a variety of representations available in the Atlas, for example, http://brauer.maths.qmul.ac.uk/Atlas/v3/clas/U42/ . For the representation of dimension $15$ of the unitary group $U_4(2)$ listed there, $m(E_C)\in\{0,2,4,8\}$. For a representation of dimension $6$ of the alternating group $A_7$, $m(E_C)\in\{0,2\}$. For a representation of dimension $11$ of the linear group $L_2(11)$, $m(E_C)\in\{0,2,6\}$. I have experimented with a number of other groups, with similar results, but cannot explain the persistence of even multiplicities.","I have been conducting some experiments with GAP and have noticed a pattern which I would like to disregard as a coincidence or explain theoretically. Consider a finite group $G$ and a representation of $G$ over $\mathbb{Q}$. Let $C$ be a conjugacy class in $G$. For any $g,h\in C$, the characteristic polynomials of the associated matrices in the representation are the same, so we can write $E_C$ for the associated list of eigenvalues. For many groups I have tried, with the aid of GAP, the multiplicity $m(E_C)$ of the eigenvalue $-1$ always seems to be even for all conjugacy classes $C$. I cannot see if that is just a coincidence or not. I have used a variety of representations available in the Atlas, for example, http://brauer.maths.qmul.ac.uk/Atlas/v3/clas/U42/ . For the representation of dimension $15$ of the unitary group $U_4(2)$ listed there, $m(E_C)\in\{0,2,4,8\}$. For a representation of dimension $6$ of the alternating group $A_7$, $m(E_C)\in\{0,2\}$. For a representation of dimension $11$ of the linear group $L_2(11)$, $m(E_C)\in\{0,2,6\}$. I have experimented with a number of other groups, with similar results, but cannot explain the persistence of even multiplicities.",,"['matrices', 'representation-theory']"
80,Write summation of vector outer products into matrix form,Write summation of vector outer products into matrix form,,"My question is as follows: Given the weighted summation of vector outer products $\sum_i\sum_jh_{ij}{\bf v_i}{\bf u_j}^T$, where $h_{ij}$ is the weight, and ${\bf v_i,u_j}$ are column vectors, I was wondering if we could write it in a more elegant matrix form? For example, a simpler case $\sum_i {\bf u_i}{\bf u_i}^T$, we can write it in the matrix form ${\bf UU}^T$, where ${\bf U}=[{\bf u_1,u_2},\ldots]$. Thanks!","My question is as follows: Given the weighted summation of vector outer products $\sum_i\sum_jh_{ij}{\bf v_i}{\bf u_j}^T$, where $h_{ij}$ is the weight, and ${\bf v_i,u_j}$ are column vectors, I was wondering if we could write it in a more elegant matrix form? For example, a simpler case $\sum_i {\bf u_i}{\bf u_i}^T$, we can write it in the matrix form ${\bf UU}^T$, where ${\bf U}=[{\bf u_1,u_2},\ldots]$. Thanks!",,"['linear-algebra', 'matrices', 'vectors']"
81,If $BNA=N$ it can happen that $B$ and $A$ identities?,If  it can happen that  and  identities?,BNA=N B A,"Consider two matrices $A\in GL_m(K)$, $B\in GL_n(K)$ such that for any $N\in M_{n,m}(K)$ is true that $$BNA=N$$ How can I prove that $B$ and $A$ are identities?","Consider two matrices $A\in GL_m(K)$, $B\in GL_n(K)$ such that for any $N\in M_{n,m}(K)$ is true that $$BNA=N$$ How can I prove that $B$ and $A$ are identities?",,"['matrices', 'matrix-equations']"
82,Finding the matrix of this particular quadratic form,Finding the matrix of this particular quadratic form,,"I have been working on problems related to bilinear and quadratic forms, and I came across an introductory problem that I have been having issues with. Take $$Q(x) = x_1^2 + 2x_1x_2 - 3x_1x_3 - 9x_2^2 + 6x_2x_3 + 13x_3^2$$ I want to find a matrix $A$ such that $Q(x) = \langle Ax,x \rangle$. My initial guess was to simply establish this via a coefficient matrix, i.e., $$A = \begin{bmatrix} 1 & 2 & -3\\ 2 & -9 & 6\\ -3 & 6 & 13\end{bmatrix}$$ However, upon closer inspection, I see that this matrix does not produce our desired outcome. Is there a more reasonable algorithm for generating the matrix $A$ of a quadratic form?","I have been working on problems related to bilinear and quadratic forms, and I came across an introductory problem that I have been having issues with. Take $$Q(x) = x_1^2 + 2x_1x_2 - 3x_1x_3 - 9x_2^2 + 6x_2x_3 + 13x_3^2$$ I want to find a matrix $A$ such that $Q(x) = \langle Ax,x \rangle$. My initial guess was to simply establish this via a coefficient matrix, i.e., $$A = \begin{bmatrix} 1 & 2 & -3\\ 2 & -9 & 6\\ -3 & 6 & 13\end{bmatrix}$$ However, upon closer inspection, I see that this matrix does not produce our desired outcome. Is there a more reasonable algorithm for generating the matrix $A$ of a quadratic form?",,"['linear-algebra', 'matrices', 'quadratic-forms']"
83,Let the column vectors of a $3 × 3$ matrix $A$ form an orthonormal basis. Explain why $A^T = A^{−1}$ .,Let the column vectors of a  matrix  form an orthonormal basis. Explain why  .,3 × 3 A A^T = A^{−1},Let the column vectors of a $3 × 3$ matrix $A$ form an orthonormal basis. Explain why $A^T = A^{−1}$ . My Attempt:  $AA^T=I$ if and only if $A^TA=I$. So $A$ is orthogonal if and only if $A^T$ is orthogonal. Is this a correct explanation/ can it be put in a more better way?,Let the column vectors of a $3 × 3$ matrix $A$ form an orthonormal basis. Explain why $A^T = A^{−1}$ . My Attempt:  $AA^T=I$ if and only if $A^TA=I$. So $A$ is orthogonal if and only if $A^T$ is orthogonal. Is this a correct explanation/ can it be put in a more better way?,,"['linear-algebra', 'matrices', 'orthonormal']"
84,Are matrices 2D by definition?,Are matrices 2D by definition?,,"On the one hand, I read on Wikipedia that [A] matrix (plural matrices) is a rectangular array of numbers,   symbols, or expressions, arranged in rows and columns. However, googling ""3D matrix"" produces a huge number of hits, including many from this website. I also find some people complaining about alleged misuse of terminology, e.g. here Just to gratify a pet peeve, there is no such thing as a ""3D matrix"".   Matrices are 2D by definition. What's the most appropriate terminology here, say for a high school or undergraduate mathematics teaching context?","On the one hand, I read on Wikipedia that [A] matrix (plural matrices) is a rectangular array of numbers,   symbols, or expressions, arranged in rows and columns. However, googling ""3D matrix"" produces a huge number of hits, including many from this website. I also find some people complaining about alleged misuse of terminology, e.g. here Just to gratify a pet peeve, there is no such thing as a ""3D matrix"".   Matrices are 2D by definition. What's the most appropriate terminology here, say for a high school or undergraduate mathematics teaching context?",,"['matrices', 'terminology']"
85,Does a repeated eigenvalue always mean that there is an eigenplane under the transformation matrix?,Does a repeated eigenvalue always mean that there is an eigenplane under the transformation matrix?,,"If you have a 3x3 matrix, if you find that it has repeated eigenvalues, does this mean that there is an invariant plane (or plane of invariant points if eigenvalue=1)? I always thought that there was an invariant plane if all 3 equations were the same when trying to find the eigenvectors, but does this only happen when there is a repeated eigenvalue, or does it happen also when there are 3 distinct eigenvalues?","If you have a 3x3 matrix, if you find that it has repeated eigenvalues, does this mean that there is an invariant plane (or plane of invariant points if eigenvalue=1)? I always thought that there was an invariant plane if all 3 equations were the same when trying to find the eigenvectors, but does this only happen when there is a repeated eigenvalue, or does it happen also when there are 3 distinct eigenvalues?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'invariance']"
86,Show that $\left\| \exp(A)-\mathbf{1} \right\| \leq e^{\left\|A\right\|}-1$,Show that,\left\| \exp(A)-\mathbf{1} \right\| \leq e^{\left\|A\right\|}-1,"Have been attempting this question, just wondering if my answer looks alright. Question: Given $A \in \Bbb{K}^{n\times n}$ show that $\left\| \exp(A)-\mathbf{1} \right\| \leq e^{\left\|A\right\|}-1$ My proof goes as follows if $\left\| . \right\|$ is the matrix norm with the submultiplicative property $\left\|AB\right\| \leq \left\|A\right\| \left\|B\right\|$ $\left\|\exp(A)-\mathbf{1}\right\| = \left\|\mathbf{1} + A + \frac{1}{2!}A^{2} + \cdots - \mathbf{1} \right\| = \left\|A + \frac{1}{2!}A^{2} + \cdots \right\|$ We know by triangle inequality that $ \left\|A + \frac{1}{2!}A^{2} + \cdots \right\| \leq \left\|A\right\| + \frac{1}{2!} \left\|A\right\|^{2} + \cdots = e^{\left\|A\right\|} -1 $ Hence $\left\|\exp(A)-\mathbf{1}\right\| \leq e^{\left\|A\right\|} -1 $ Does this seem alright? Thanks in advance!","Have been attempting this question, just wondering if my answer looks alright. Question: Given $A \in \Bbb{K}^{n\times n}$ show that $\left\| \exp(A)-\mathbf{1} \right\| \leq e^{\left\|A\right\|}-1$ My proof goes as follows if $\left\| . \right\|$ is the matrix norm with the submultiplicative property $\left\|AB\right\| \leq \left\|A\right\| \left\|B\right\|$ $\left\|\exp(A)-\mathbf{1}\right\| = \left\|\mathbf{1} + A + \frac{1}{2!}A^{2} + \cdots - \mathbf{1} \right\| = \left\|A + \frac{1}{2!}A^{2} + \cdots \right\|$ We know by triangle inequality that $ \left\|A + \frac{1}{2!}A^{2} + \cdots \right\| \leq \left\|A\right\| + \frac{1}{2!} \left\|A\right\|^{2} + \cdots = e^{\left\|A\right\|} -1 $ Hence $\left\|\exp(A)-\mathbf{1}\right\| \leq e^{\left\|A\right\|} -1 $ Does this seem alright? Thanks in advance!",,"['abstract-algebra', 'matrices', 'analysis']"
87,Matrix with a certain pattern,Matrix with a certain pattern,,"Consider the following matrix: $$\left[\begin{array}{cccc} 1+x_1y_1 & 1+x_1y_2 & \ldots & 1+x_1y_n  \\ 1+x_2y_1 & 1+x_2y_2 & \ldots & 1+x_2y_n \\ 1+x_3y_1 & 1+x_3y_2 & \ldots & 1+x_3 y_n \\ \vdots & & & \\ 1+x_ny_1 & 1+x_ny_2 & \ldots & 1+x_n y_n \\ \end{array}  \right]$$ with $x_k, y_k$ being real numbers. I suppose that there must be some neat way of computing the determinant of this matrix. Those ones are annoying to me.","Consider the following matrix: $$\left[\begin{array}{cccc} 1+x_1y_1 & 1+x_1y_2 & \ldots & 1+x_1y_n  \\ 1+x_2y_1 & 1+x_2y_2 & \ldots & 1+x_2y_n \\ 1+x_3y_1 & 1+x_3y_2 & \ldots & 1+x_3 y_n \\ \vdots & & & \\ 1+x_ny_1 & 1+x_ny_2 & \ldots & 1+x_n y_n \\ \end{array}  \right]$$ with $x_k, y_k$ being real numbers. I suppose that there must be some neat way of computing the determinant of this matrix. Those ones are annoying to me.",,"['matrices', 'determinant']"
88,Known classes of Hadamard matrices,Known classes of Hadamard matrices,,"In the book Combinatorics: Room Squares, Sum-Free Sets, Hadamard Matrices by Wallis et al., Appendix A of the chapter on Hadamard matrices gives a list of known classes of Hadamard matrices. However, the list is a bit outdated since the book was published in 1972. Can anyone point me in the direction of a more up-to-date list of known orders for which a Hadamard exists? I've tried searching online but I haven't been able to find a large, diverse list like that given in the book by Wallis et al. Thanks in advance for any help. Edit: In Appendix A of the book mentioned above, it compiles a list of known classes of Hadamard matrices and gives a brief justification for the existence of each class. For example, a couple lines from the table are: (where $q \equiv 3 \pmod{4}$ is a prime power) +----------+---------------------------------+ |  Order$\,\,\,\,\,\,$   |          Justification$\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$| +----------+---------------------------------+ | $2^t$$\,\,\,\,\,\,\,\,\,\,\,\,\,\,$    | Sylvester Construction $\,\,\,\,\,\,\,\,\,\,\,\,\,\,$         | | $q(q+1)$ | Williamson; Corollary 8.14 $\,\,\,\,\,\,$| +----------+---------------------------------+ I'm looking for a more up-to-date list that gives known classes similarly to this example","In the book Combinatorics: Room Squares, Sum-Free Sets, Hadamard Matrices by Wallis et al., Appendix A of the chapter on Hadamard matrices gives a list of known classes of Hadamard matrices. However, the list is a bit outdated since the book was published in 1972. Can anyone point me in the direction of a more up-to-date list of known orders for which a Hadamard exists? I've tried searching online but I haven't been able to find a large, diverse list like that given in the book by Wallis et al. Thanks in advance for any help. Edit: In Appendix A of the book mentioned above, it compiles a list of known classes of Hadamard matrices and gives a brief justification for the existence of each class. For example, a couple lines from the table are: (where $q \equiv 3 \pmod{4}$ is a prime power) +----------+---------------------------------+ |  Order$\,\,\,\,\,\,$   |          Justification$\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$| +----------+---------------------------------+ | $2^t$$\,\,\,\,\,\,\,\,\,\,\,\,\,\,$    | Sylvester Construction $\,\,\,\,\,\,\,\,\,\,\,\,\,\,$         | | $q(q+1)$ | Williamson; Corollary 8.14 $\,\,\,\,\,\,$| +----------+---------------------------------+ I'm looking for a more up-to-date list that gives known classes similarly to this example",,"['combinatorics', 'matrices', 'combinatorial-designs', 'hadamard-matrices']"
89,Equivalent matrices represent the same linear map in different bases,Equivalent matrices represent the same linear map in different bases,,"It is clear from the change of basis formula that the matrices $A$ and $B$ representing the same linear map in different bases are equivalent: there exists invertible matrixes $Q$ and $P$ such that $A=Q^{-1}BP$. My question is about the other way, I mean given two equivalent matrices $A$ and $B$ in $M_{n,p}(\mathbb K)$ why they must represent the same linear map in different bases ? thank you for your help!","It is clear from the change of basis formula that the matrices $A$ and $B$ representing the same linear map in different bases are equivalent: there exists invertible matrixes $Q$ and $P$ such that $A=Q^{-1}BP$. My question is about the other way, I mean given two equivalent matrices $A$ and $B$ in $M_{n,p}(\mathbb K)$ why they must represent the same linear map in different bases ? thank you for your help!",,"['linear-algebra', 'matrices', 'linear-transformations']"
90,Let $A\in M_n(\mathbb R)$ is a non-zero symmetric zero-diagonal matrix and its elements are $0$ ore $1$. What we can say about eigenvalue of $A$?,Let  is a non-zero symmetric zero-diagonal matrix and its elements are  ore . What we can say about eigenvalue of ?,A\in M_n(\mathbb R) 0 1 A,"Let $A\in M_n(\mathbb R)$ is a non-zero symmetric zero-diagonal matrix and its elements are $0$ ore $1$. We know that the eigenvalue of $A$ are real. I'm interesting to know the number of distinct eigenvlue of the matrix $A$. How many of them are greater than $1$ and how many of them are lower than $1$? Since $\mathrm{trac}(A)=0$, we get that some eigenvalue of $A$ is negative and some of them are positive. In the case that $A=[a_{ij}]$ and $\Pi_{i\ne j}a_{ij}=1$ (i.e. $a_{ij}=1$ for $i\ne j$ and $a_{ii}=0$) the answer is easy and the eigenvalues are $\lambda_1=n-1$ by repeated order $1$ and $\lambda_2=-1$ by repeated order $n-1$. My question is about the case that: $\Pi_{i\ne j}a_{ij}=0$ (i.e. there is $i\ne j$ which $a_{ij}=a_{ji}=0$).","Let $A\in M_n(\mathbb R)$ is a non-zero symmetric zero-diagonal matrix and its elements are $0$ ore $1$. We know that the eigenvalue of $A$ are real. I'm interesting to know the number of distinct eigenvlue of the matrix $A$. How many of them are greater than $1$ and how many of them are lower than $1$? Since $\mathrm{trac}(A)=0$, we get that some eigenvalue of $A$ is negative and some of them are positive. In the case that $A=[a_{ij}]$ and $\Pi_{i\ne j}a_{ij}=1$ (i.e. $a_{ij}=1$ for $i\ne j$ and $a_{ii}=0$) the answer is easy and the eigenvalues are $\lambda_1=n-1$ by repeated order $1$ and $\lambda_2=-1$ by repeated order $n-1$. My question is about the case that: $\Pi_{i\ne j}a_{ij}=0$ (i.e. there is $i\ne j$ which $a_{ij}=a_{ji}=0$).",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-graph-theory', 'minimal-polynomials']"
91,"For which values of $a,b$ is the matrix invertible?",For which values of  is the matrix invertible?,"a,b","I am trying to figure out the below question: 15 . For which values of the constants $a$ and $b$ is the matrix   $$A = \left[\begin{array}{cc} a & -b \\ b & a \end{array}\right]$$   invertible? What is the inverse in this case? See Exercise 13. My understanding is that a matrix is invertible when the determinant is not zero. In this case, when $a^2 - b^2 = 0$ the matrix is not invertible. Thus, for any values $a,b$ such that $a^2$ does not equal $b^2$, the matrix is invertible. However, the solutions in the back of the book state that the matrix is invertible if $a$ does not equal zero or if $b$ does not equal zero. Can someone explain this to me?","I am trying to figure out the below question: 15 . For which values of the constants $a$ and $b$ is the matrix   $$A = \left[\begin{array}{cc} a & -b \\ b & a \end{array}\right]$$   invertible? What is the inverse in this case? See Exercise 13. My understanding is that a matrix is invertible when the determinant is not zero. In this case, when $a^2 - b^2 = 0$ the matrix is not invertible. Thus, for any values $a,b$ such that $a^2$ does not equal $b^2$, the matrix is invertible. However, the solutions in the back of the book state that the matrix is invertible if $a$ does not equal zero or if $b$ does not equal zero. Can someone explain this to me?",,"['linear-algebra', 'matrices', 'inverse']"
92,When is a matrix similar to a skew symmetric matrix?,When is a matrix similar to a skew symmetric matrix?,,"More precisely. Is there any way one can tell if a matrix is similar to a skew symmetric matrix by looking at its eigenvalues or even better at the coefficients of its characteristic polynomial ? For instance, if a  $2 \times 2$ matrix $A$  is similar to a skew symmetric matrix $S$( that is $A=U^{-1}SU$ ) then $$ tr(A)=0, $$ and $$ tr(A^2)=-2 det(A).$$ These two conditions are necessary but not sufficient. Are there any sufficient conditions?","More precisely. Is there any way one can tell if a matrix is similar to a skew symmetric matrix by looking at its eigenvalues or even better at the coefficients of its characteristic polynomial ? For instance, if a  $2 \times 2$ matrix $A$  is similar to a skew symmetric matrix $S$( that is $A=U^{-1}SU$ ) then $$ tr(A)=0, $$ and $$ tr(A^2)=-2 det(A).$$ These two conditions are necessary but not sufficient. Are there any sufficient conditions?",,"['linear-algebra', 'matrices']"
93,Eigenvalues of inverse matrix to a given matrix,Eigenvalues of inverse matrix to a given matrix,,How to calculate the eigenvalue of the inverse of a matrix given matrix is $A= \begin{bmatrix} 0&1&0\\ 0&0&1 \\4&-17&8\end{bmatrix}$ Is there any fast method?,How to calculate the eigenvalue of the inverse of a matrix given matrix is $A= \begin{bmatrix} 0&1&0\\ 0&0&1 \\4&-17&8\end{bmatrix}$ Is there any fast method?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,If $A = L+D+U $ is symmetry positive definite (s.p.d) then so is $(D+L)D^{-1}(D+U)$,If  is symmetry positive definite (s.p.d) then so is,A = L+D+U  (D+L)D^{-1}(D+U),"Let $A $ be symmetry positive definite (s.p.d). Matrix $A= L+D+U$, where L,D,U denote lower, diagonal, upper matrix of A, respectively. Show that matrix $M= (D+L)D^{-1}(D+U)$ is s.p.d A is s.p.d then $U = L^T$, hence $M^T=M$. But I don't know how to show that M is p.d. Can anyone help me? Thank you so much.","Let $A $ be symmetry positive definite (s.p.d). Matrix $A= L+D+U$, where L,D,U denote lower, diagonal, upper matrix of A, respectively. Show that matrix $M= (D+L)D^{-1}(D+U)$ is s.p.d A is s.p.d then $U = L^T$, hence $M^T=M$. But I don't know how to show that M is p.d. Can anyone help me? Thank you so much.",,"['linear-algebra', 'matrices']"
95,Show that rank of skew-symetric is even number,Show that rank of skew-symetric is even number,,$$A = -A^T$$ I assume that $A$ is not singular. So $$\det{A} \neq 0$$ Then $$ \det(A) = \det(-A^T) = \det(-I_{n} A^T) = (-1)^n\det(A^T) = (-1)^n\det(A)$$ So I get that $n$ must be even. But what about odd $n$ ? I know it has to be singular matrix. Hints?,I assume that is not singular. So Then So I get that must be even. But what about odd ? I know it has to be singular matrix. Hints?,A = -A^T A \det{A} \neq 0  \det(A) = \det(-A^T) = \det(-I_{n} A^T) = (-1)^n\det(A^T) = (-1)^n\det(A) n n,"['matrices', 'matrix-rank']"
96,Matrix Equations,Matrix Equations,,I have worked it out and determined that both equations hold. I am wondering why this is the case. Is there a reason why the equation holds for these two types of matrices?,I have worked it out and determined that both equations hold. I am wondering why this is the case. Is there a reason why the equation holds for these two types of matrices?,,"['linear-algebra', 'matrices']"
97,Matrix determinant lemma with adjugate matrix,Matrix determinant lemma with adjugate matrix,,"I would like a proof of the following result, given on wikipedia . For all square matrices $\mathbf{A}$ and column vectors $\mathbf{u}$ and $\mathbf{v}$ over some field $\mathbb{F}$, $$ \det(\mathbf{A}+\mathbf{uv}^\mathrm{T}) = \det(\mathbf{A}) + \mathbf{v}^\mathrm{T}\mathrm{adj}(\mathbf{A})\mathbf{u}, $$ where $\mathrm{adj}(\mathbf{A})$ is the adjugate matrix of $\mathbf{A}$. Note that $\mathbf{A}$ may be singular. However, the proof given on wikipedia requires that $\mathbf{A}$ is nonsingular.","I would like a proof of the following result, given on wikipedia . For all square matrices $\mathbf{A}$ and column vectors $\mathbf{u}$ and $\mathbf{v}$ over some field $\mathbb{F}$, $$ \det(\mathbf{A}+\mathbf{uv}^\mathrm{T}) = \det(\mathbf{A}) + \mathbf{v}^\mathrm{T}\mathrm{adj}(\mathbf{A})\mathbf{u}, $$ where $\mathrm{adj}(\mathbf{A})$ is the adjugate matrix of $\mathbf{A}$. Note that $\mathbf{A}$ may be singular. However, the proof given on wikipedia requires that $\mathbf{A}$ is nonsingular.",,"['matrices', 'determinant']"
98,How to prove that a matrix is orthogonal given that,How to prove that a matrix is orthogonal given that,,"How to prove that a matrix is orthogonal given that abs(det(Q)) = 1 and its columns have unit norm? my work I tried with Gram matrix  but not correct , please help","How to prove that a matrix is orthogonal given that abs(det(Q)) = 1 and its columns have unit norm? my work I tried with Gram matrix  but not correct , please help",,"['linear-algebra', 'matrices']"
99,"If $v$ is an eigenvalue of $e^{t A}$ for all $t \geq 0$, is it also an eigenvalue of $A$?","If  is an eigenvalue of  for all , is it also an eigenvalue of ?",v e^{t A} t \geq 0 A,"I'm currently writtng a proof and I wont use that, if $v$ is an eigenvector of $e^{tA}$ for all $t \geq 0$ where $A$ is some generating matrix, then $v$ is an eigenvector of $A$ itself. However, I found neither the proof nor a contradiction in available sources. The case of diagonalisable matrices is self-evident as  a popular computation approach utilises this fact. However, it is still unclear for me if  the same is true for non-diagonalisable matrices at least for the case of basic (non generalised) eigenvectors. Recalling Jordan structure  I think that all kinds of eigenvectors are preserved. But are there any better rigorous proof for this conjecture to make me totally confident? Can this fact be assumed as obvious?","I'm currently writtng a proof and I wont use that, if $v$ is an eigenvector of $e^{tA}$ for all $t \geq 0$ where $A$ is some generating matrix, then $v$ is an eigenvector of $A$ itself. However, I found neither the proof nor a contradiction in available sources. The case of diagonalisable matrices is self-evident as  a popular computation approach utilises this fact. However, it is still unclear for me if  the same is true for non-diagonalisable matrices at least for the case of basic (non generalised) eigenvectors. Recalling Jordan structure  I think that all kinds of eigenvectors are preserved. But are there any better rigorous proof for this conjecture to make me totally confident? Can this fact be assumed as obvious?",,"['linear-algebra', 'matrices']"
