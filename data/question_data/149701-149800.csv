,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Are all functions $f:\mathbf{Z}\to\mathbf{Z}$ ""continuous""?","Are all functions  ""continuous""?",f:\mathbf{Z}\to\mathbf{Z},"I read the following definitions in Glen E. Bredon's ""Topology and Geometry"": Let $\mathbf{x},\mathbf{y}\in\mathbf{R}^n$ and $$ \text{dist}\left(\mathbf{x},\mathbf{y}\right)=\left(\sum_{i=1}^n\left(x_i-y_i\right)^2\right)^{1/2}. $$ Moreover, let $f:\mathbf{R}^n\to\mathbf{R}^k$ be continuous at   $\mathbf{x}\in\mathbf{R}^n$ if $$ \forall\epsilon>0,\exists\delta>0\ni\text{dist}\left(\mathbf{x},\mathbf{y}\right)<\delta\implies\text{dist}\left(f\left(\mathbf{x}\right),f\left(\mathbf{y}\right)\right)<\epsilon. $$ Using these definitions, can I prove that all functions $f:\mathbf{Z}\to\mathbf{Z}$ are ""continuous"" by setting $\delta=1$? I.e., $\text{dist}\left(\mathbf{x},\mathbf{y}\right)<1$ implies that $\mathbf{x}=\mathbf{y}$ and therefore $\text{dist}\left(f\left(\mathbf{x}\right),f\left(\mathbf{y}\right)\right)=0<\epsilon$. Am I correct? Although this fact may be useless, I am trying to ensure that my understanding of these concepts is sound.","I read the following definitions in Glen E. Bredon's ""Topology and Geometry"": Let $\mathbf{x},\mathbf{y}\in\mathbf{R}^n$ and $$ \text{dist}\left(\mathbf{x},\mathbf{y}\right)=\left(\sum_{i=1}^n\left(x_i-y_i\right)^2\right)^{1/2}. $$ Moreover, let $f:\mathbf{R}^n\to\mathbf{R}^k$ be continuous at   $\mathbf{x}\in\mathbf{R}^n$ if $$ \forall\epsilon>0,\exists\delta>0\ni\text{dist}\left(\mathbf{x},\mathbf{y}\right)<\delta\implies\text{dist}\left(f\left(\mathbf{x}\right),f\left(\mathbf{y}\right)\right)<\epsilon. $$ Using these definitions, can I prove that all functions $f:\mathbf{Z}\to\mathbf{Z}$ are ""continuous"" by setting $\delta=1$? I.e., $\text{dist}\left(\mathbf{x},\mathbf{y}\right)<1$ implies that $\mathbf{x}=\mathbf{y}$ and therefore $\text{dist}\left(f\left(\mathbf{x}\right),f\left(\mathbf{y}\right)\right)=0<\epsilon$. Am I correct? Although this fact may be useless, I am trying to ensure that my understanding of these concepts is sound.",,"['real-analysis', 'general-topology', 'analysis']"
1,Questions about Proof of Lusin's Theorem,Questions about Proof of Lusin's Theorem,,"I am reviewing my analysis notes, and having trouble understanding certain parts of the proof to Lusin's theorem. $\textbf{Lusin's Theorem}$: Let $F: [0,1] \rightarrow [0,\infty)$ be a nonnegative, measurable function.  Suppose $\epsilon > 0$.  Then $\exists$ a compact set $K \subseteq [0,1]$ such that $m(K) > 1 - \epsilon$ ($K$ fills out most of $[0,1]$), and a continuous function $g$ on $[0,1]$ such that $g(x) = f(x)$ if $x \in K$. Here is the proof given in class: First, find a sequence of simple functions $s_{n}$, such that $\lim \limits_{n \rightarrow \infty} s_{n}(x) = F(x)$ for all $x \in [0,1]$.  Choose $E_{n}$ measurable such that $m(E_{n}) \leq \epsilon*2^{-n-2}$, and continuous functions $f_{n}$ such that $f_{n}(x) = s_{n}(x)$ for $x \in [0,1] \setminus E_{n}$. First question: I know we can find a continuous function that is nearly equal to the characteristic function of a measurable set, and each simple function $s_{n}(x)$ is the sum of characteristic functions, but why does that mean I can find a continuous function nearly equal to the sum of simple functions?  Is it because the sum of the continuous functions for each characteristic function is itself continuous? Now we let $Y = \bigcup \limits_{n = 1}^{\infty} E_{n}$.  Then $m(Y) \leq \frac{\epsilon}{4}$, and $f_{n}(x) = s_{n}(x)$ if $x \in [0,1] \setminus Y$.  This means $\lim \limits_{n \rightarrow \infty} f_{n}(x) = \lim \limits_{n \rightarrow \infty} s_{n}(x) = F(x)$ if $x \in [0,1] \setminus Y$. Using Egoroff's theorem , we can find a set $Z \subseteq [0,1] \setminus Y$ such that $s_{n} \rightarrow F$ uniformly on $Z$, and consequently, $f_{n} \rightarrow F$ uniformly on $Z$. Second question: Uniform convergence of a sequence of continuous functions has a continuous limit, so doesn't this mean $F$ is continuous on $Z$?  My professor does something strange by saying $\exists$ a continuous function $G : Z \rightarrow [0, \infty)$ such that $G(x) = F(x)$ for all $x \in Z$ , which has confused me.  Where did $G$ come from, and why does its codomain not include $\infty$?  What if $F$ did equal $\infty$ on some part of $Z$? Finally, we find a compact set $K \subseteq Z$ such that $m(K) > 1 - \epsilon$, and extend $G\mid_{k}$ to a continuous function on $[0,1]$ by letting the extension be linear on the complementary intervals in $[0,1] \setminus K$. Third question: What complementary intervals of $[0,1] \setminus K$?","I am reviewing my analysis notes, and having trouble understanding certain parts of the proof to Lusin's theorem. $\textbf{Lusin's Theorem}$: Let $F: [0,1] \rightarrow [0,\infty)$ be a nonnegative, measurable function.  Suppose $\epsilon > 0$.  Then $\exists$ a compact set $K \subseteq [0,1]$ such that $m(K) > 1 - \epsilon$ ($K$ fills out most of $[0,1]$), and a continuous function $g$ on $[0,1]$ such that $g(x) = f(x)$ if $x \in K$. Here is the proof given in class: First, find a sequence of simple functions $s_{n}$, such that $\lim \limits_{n \rightarrow \infty} s_{n}(x) = F(x)$ for all $x \in [0,1]$.  Choose $E_{n}$ measurable such that $m(E_{n}) \leq \epsilon*2^{-n-2}$, and continuous functions $f_{n}$ such that $f_{n}(x) = s_{n}(x)$ for $x \in [0,1] \setminus E_{n}$. First question: I know we can find a continuous function that is nearly equal to the characteristic function of a measurable set, and each simple function $s_{n}(x)$ is the sum of characteristic functions, but why does that mean I can find a continuous function nearly equal to the sum of simple functions?  Is it because the sum of the continuous functions for each characteristic function is itself continuous? Now we let $Y = \bigcup \limits_{n = 1}^{\infty} E_{n}$.  Then $m(Y) \leq \frac{\epsilon}{4}$, and $f_{n}(x) = s_{n}(x)$ if $x \in [0,1] \setminus Y$.  This means $\lim \limits_{n \rightarrow \infty} f_{n}(x) = \lim \limits_{n \rightarrow \infty} s_{n}(x) = F(x)$ if $x \in [0,1] \setminus Y$. Using Egoroff's theorem , we can find a set $Z \subseteq [0,1] \setminus Y$ such that $s_{n} \rightarrow F$ uniformly on $Z$, and consequently, $f_{n} \rightarrow F$ uniformly on $Z$. Second question: Uniform convergence of a sequence of continuous functions has a continuous limit, so doesn't this mean $F$ is continuous on $Z$?  My professor does something strange by saying $\exists$ a continuous function $G : Z \rightarrow [0, \infty)$ such that $G(x) = F(x)$ for all $x \in Z$ , which has confused me.  Where did $G$ come from, and why does its codomain not include $\infty$?  What if $F$ did equal $\infty$ on some part of $Z$? Finally, we find a compact set $K \subseteq Z$ such that $m(K) > 1 - \epsilon$, and extend $G\mid_{k}$ to a continuous function on $[0,1]$ by letting the extension be linear on the complementary intervals in $[0,1] \setminus K$. Third question: What complementary intervals of $[0,1] \setminus K$?",,"['analysis', 'proof-writing']"
2,Constructive proof of Euler's formula,Constructive proof of Euler's formula,,"In most textbooks on the subject I have seen, Euler's formula (by which I mean $e^{ix}=\cos(x)+i\sin(x)$) is proved by applying either differential equations or the power series of sine and cosine. However, any of these two approaches would rely on the derivatives of sine and cosine. However, I have never seen a proof of these derivatives that does not somehow use Euler's formula. Is it possible to give such a proof? And if it is, which kind of mathematics does it rely on? Does it rely on the definition of sine and cosine based on the unit circle? And if it does, a new problem arises: If we are to define sine and cosine based on the unit circle, we need the concept of radians and hence the concept of arc length. But this, in return, would require a ---- from a constructive POV --- relatively advanced level of integral calculus. And this suddenly makes the derivation of the formula rely on a large amount of mathematics that has to be constructed as well.","In most textbooks on the subject I have seen, Euler's formula (by which I mean $e^{ix}=\cos(x)+i\sin(x)$) is proved by applying either differential equations or the power series of sine and cosine. However, any of these two approaches would rely on the derivatives of sine and cosine. However, I have never seen a proof of these derivatives that does not somehow use Euler's formula. Is it possible to give such a proof? And if it is, which kind of mathematics does it rely on? Does it rely on the definition of sine and cosine based on the unit circle? And if it does, a new problem arises: If we are to define sine and cosine based on the unit circle, we need the concept of radians and hence the concept of arc length. But this, in return, would require a ---- from a constructive POV --- relatively advanced level of integral calculus. And this suddenly makes the derivation of the formula rely on a large amount of mathematics that has to be constructed as well.",,"['analysis', 'constructive-mathematics']"
3,Show $\lim_{n_\rightarrow \infty}\int_0^\infty ne^{-\frac{2n^2x^2}{x + 1}}dx = \infty$,Show,\lim_{n_\rightarrow \infty}\int_0^\infty ne^{-\frac{2n^2x^2}{x + 1}}dx = \infty,"As part of an analysis qual problem, I am having a hard time showing that $\lim_{n_\rightarrow \infty}\int_0^\infty ne^{-\frac{2n^2x^2}{x + 1}}dx = \infty$.  Any suggestions?  Thanks in advance.  I tried using Fatou.  That didn't work","As part of an analysis qual problem, I am having a hard time showing that $\lim_{n_\rightarrow \infty}\int_0^\infty ne^{-\frac{2n^2x^2}{x + 1}}dx = \infty$.  Any suggestions?  Thanks in advance.  I tried using Fatou.  That didn't work",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
4,Two halls 6 and 9 meters perpendicularly intersect. Optimization,Two halls 6 and 9 meters perpendicularly intersect. Optimization,,Two halls 6 and 9 meters perpendicularly intersect. Find the length of the longest straight bar to be passed horizontally from one aisle to another by a corner without deformation. and this is my try: How to find the equation to maximize in this problem.Please.,Two halls 6 and 9 meters perpendicularly intersect. Find the length of the longest straight bar to be passed horizontally from one aisle to another by a corner without deformation. and this is my try: How to find the equation to maximize in this problem.Please.,,"['calculus', 'analysis', 'derivatives', 'optimization']"
5,"Is it true that, $|e^{x}-e^{y}|\leq C \cdot |x-y|$?","Is it true that, ?",|e^{x}-e^{y}|\leq C \cdot |x-y|,"Define $f:\mathbb R \to \mathbb R$ such that $f(x)= e^{x}-1:= \sum_{n=1}^{\infty} \frac{x^{n}}{n!};$ for $x\in \mathbb R.$ My Question : Can we expect $|f(x)-f(y)|\leq |x-y| \cdot C;$ where $C$ is constant, $x, y \in \mathbb R$; if yes what can we say about $C$ ? Ruff Attempt : $$f(x)-f(y)= \sum_{n=1}^{\infty} \frac{1}{n!} (x^{n}-y^{n})= \sum_{n=1}^{\infty}\frac{1}{n!} (x-y)(x^{n-1}+x^{n-2}y+...+y^{n-1})= (x-y)\sum_{n=1}^{\infty} \frac{1}{n!}(x^{n-1}+x^{n-2}y+...+y^{n-1}) ;$$ I am not sure what I have done so far is legitimate; and also I have question; Is $$\sum_{n=1}^{\infty}\frac{1}{n!} (x^{n-1}+x^{n-2}y+...+y^{n-1})$$ is converges ? Please correct me if I have done some thing wrong here;","Define $f:\mathbb R \to \mathbb R$ such that $f(x)= e^{x}-1:= \sum_{n=1}^{\infty} \frac{x^{n}}{n!};$ for $x\in \mathbb R.$ My Question : Can we expect $|f(x)-f(y)|\leq |x-y| \cdot C;$ where $C$ is constant, $x, y \in \mathbb R$; if yes what can we say about $C$ ? Ruff Attempt : $$f(x)-f(y)= \sum_{n=1}^{\infty} \frac{1}{n!} (x^{n}-y^{n})= \sum_{n=1}^{\infty}\frac{1}{n!} (x-y)(x^{n-1}+x^{n-2}y+...+y^{n-1})= (x-y)\sum_{n=1}^{\infty} \frac{1}{n!}(x^{n-1}+x^{n-2}y+...+y^{n-1}) ;$$ I am not sure what I have done so far is legitimate; and also I have question; Is $$\sum_{n=1}^{\infty}\frac{1}{n!} (x^{n-1}+x^{n-2}y+...+y^{n-1})$$ is converges ? Please correct me if I have done some thing wrong here;",,"['real-analysis', 'sequences-and-series', 'analysis', 'power-series']"
6,Derivatives on Functors,Derivatives on Functors,,I'm not even sure if this question makes pedantic sense but is there any way to rigorously define the notion of taking the derivative of a functor?,I'm not even sure if this question makes pedantic sense but is there any way to rigorously define the notion of taking the derivative of a functor?,,"['general-topology', 'analysis', 'category-theory']"
7,"When is $x^{\alpha}\sin(x^{\beta})$ uniformly continuous for $\alpha, \beta > 0$?",When is  uniformly continuous for ?,"x^{\alpha}\sin(x^{\beta}) \alpha, \beta > 0","Consider a function $f_{\alpha, \beta}\colon (0, \infty) \longrightarrow \mathbb{R}$ defined in the following way: $$f_{\alpha, \beta} = x^{\alpha}\sin(x^{\beta}) \quad \alpha, \beta > 0$$ Then we can pose the questions: For which pairs $\alpha, \beta$ is this function uniformly continuous? For which sets $(\alpha, \beta)$ in $(0, \infty)^2$ is the family equicontinuous? I am baffled as to how to go about answering these questions in a clear and concise way. I think that it is possible to produce an answer by considering many cases and lots of tedious estimates. Is there a better way to approach the problem? Any help will be appreciated.","Consider a function $f_{\alpha, \beta}\colon (0, \infty) \longrightarrow \mathbb{R}$ defined in the following way: $$f_{\alpha, \beta} = x^{\alpha}\sin(x^{\beta}) \quad \alpha, \beta > 0$$ Then we can pose the questions: For which pairs $\alpha, \beta$ is this function uniformly continuous? For which sets $(\alpha, \beta)$ in $(0, \infty)^2$ is the family equicontinuous? I am baffled as to how to go about answering these questions in a clear and concise way. I think that it is possible to produce an answer by considering many cases and lots of tedious estimates. Is there a better way to approach the problem? Any help will be appreciated.",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
8,Convergence of $\int_0^\infty \frac{x^p}{1+x^p}dx$,Convergence of,\int_0^\infty \frac{x^p}{1+x^p}dx,"Let $p\geq -1$. How do I show that $\int_0^\infty \frac{x^p}{1+x^p}dx$   diverges? I thought to break up the integral into $\int_0^1 \frac{x^p}{1+x^p}dx+\int_1^\infty \frac{x^p}{1+x^p}dx$, but I can't find suitable comparisons (a nonnegative expression that doesn't exceed the integrand on each domain of integration such that the expressions are not integrable over their respective domains) to prove that neither term converges.","Let $p\geq -1$. How do I show that $\int_0^\infty \frac{x^p}{1+x^p}dx$   diverges? I thought to break up the integral into $\int_0^1 \frac{x^p}{1+x^p}dx+\int_1^\infty \frac{x^p}{1+x^p}dx$, but I can't find suitable comparisons (a nonnegative expression that doesn't exceed the integrand on each domain of integration such that the expressions are not integrable over their respective domains) to prove that neither term converges.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'improper-integrals']"
9,Can any metric space be completed?,Can any metric space be completed?,,"Completion defined in Real Analysis, Carothers, 1ed has been captured below. Can any metric space be completed?","Completion defined in Real Analysis, Carothers, 1ed has been captured below. Can any metric space be completed?",,"['real-analysis', 'analysis']"
10,"If $|x_{n+1}-x_n| < |x_n-x_{n-1}|$, then $(x_n)$ is a Cauchy sequence","If , then  is a Cauchy sequence",|x_{n+1}-x_n| < |x_n-x_{n-1}| (x_n),"Prove or disprove : If $|x_{n+1}-x_n| < |x_n-x_{n-1}|$ for all $n\geq 2$, then $(x_n)$ is a Cauchy sequence What I understand from this is if the difference between the $n$ and $n+1$ terms in the sequence is getting smaller and smaller, then, the sequence is a Cauchy sequence. I am pretty sure it it is not. I thought of the logarithmic structure, but I don't think I can use that yet (we haven't seen it yet). I then thought of the square root function whose terms are in fact getting smaller and smaller relative to the preceding one. However, I fail to see how to prove that formally... What arguments can I invoke? Can I manipulate the expression and how?","Prove or disprove : If $|x_{n+1}-x_n| < |x_n-x_{n-1}|$ for all $n\geq 2$, then $(x_n)$ is a Cauchy sequence What I understand from this is if the difference between the $n$ and $n+1$ terms in the sequence is getting smaller and smaller, then, the sequence is a Cauchy sequence. I am pretty sure it it is not. I thought of the logarithmic structure, but I don't think I can use that yet (we haven't seen it yet). I then thought of the square root function whose terms are in fact getting smaller and smaller relative to the preceding one. However, I fail to see how to prove that formally... What arguments can I invoke? Can I manipulate the expression and how?",,"['real-analysis', 'analysis']"
11,An extension of a question about no 3 points in $\mathbb{R}^2$ being collinear,An extension of a question about no 3 points in  being collinear,\mathbb{R}^2,"I am interested to see if we can use any of the answers here - or other methods - to answer the following question(s): Does there exist a set $S\subset \mathbb{R}^2$ whose set of limit points in $\mathbb{R}^2$ contains the line $[0,1] =$ {$(x,0):  0 \le x \le 1$} such that no 3 points in S are collinear? Update: I've deleted a silly follow-on question from the OP to simplify things for readers.","I am interested to see if we can use any of the answers here - or other methods - to answer the following question(s): Does there exist a set $S\subset \mathbb{R}^2$ whose set of limit points in $\mathbb{R}^2$ contains the line $[0,1] =$ {$(x,0):  0 \le x \le 1$} such that no 3 points in S are collinear? Update: I've deleted a silly follow-on question from the OP to simplify things for readers.",,"['real-analysis', 'analysis']"
12,Proof that the space of infinite 01-sequences (Cantor-space) is totally disconnected,Proof that the space of infinite 01-sequences (Cantor-space) is totally disconnected,,"I want to proof that the space $\{0,1\}^{\mathbb N}$ of infinite binary sequences with the product topology is totally disconnected. I know that this space has a basis consisting of clopen sets and is $T_2$, so it follows already that it is totally disconnected. But I tried to proof it directly using the definition. Def: A topological space $X$ is totally disconnected iff only the singletons are connected, i.e. if for every set $Y$ with more than two elements there exists two nonempty separated sets $X_1, X_2$ such that $Y = X_1 \cup X_2$. Proof: Let $Y \ne X$ be a set with more than one element. Then if $Y$ is finite, select some $y \in Y$, and because for finite sets $M$ it holds that $cl(M) = M$ it follows that $Y = \{y\} \cup Y\setminus\{y\}$ is a separation. Otherwise $Y$ is infinite if we can select a point $y$ which is not a limit point of $Y$ and we can write again $Y = \{y\} \cup Y\setminus\{y\}$. I am not sure if it is always possible to select a point which is not a limit point of $Y$, but I had sets like $Y = \{ ab^{\mathbb N}, aab^{\mathbb N},aaab^{\mathbb N},\ldots \}$ in mind, which has limit point $a^{\mathbb N}$. Is there a way to proceed along the lines of this proof and construct for every $Y$ such a partition into separated sets?","I want to proof that the space $\{0,1\}^{\mathbb N}$ of infinite binary sequences with the product topology is totally disconnected. I know that this space has a basis consisting of clopen sets and is $T_2$, so it follows already that it is totally disconnected. But I tried to proof it directly using the definition. Def: A topological space $X$ is totally disconnected iff only the singletons are connected, i.e. if for every set $Y$ with more than two elements there exists two nonempty separated sets $X_1, X_2$ such that $Y = X_1 \cup X_2$. Proof: Let $Y \ne X$ be a set with more than one element. Then if $Y$ is finite, select some $y \in Y$, and because for finite sets $M$ it holds that $cl(M) = M$ it follows that $Y = \{y\} \cup Y\setminus\{y\}$ is a separation. Otherwise $Y$ is infinite if we can select a point $y$ which is not a limit point of $Y$ and we can write again $Y = \{y\} \cup Y\setminus\{y\}$. I am not sure if it is always possible to select a point which is not a limit point of $Y$, but I had sets like $Y = \{ ab^{\mathbb N}, aab^{\mathbb N},aaab^{\mathbb N},\ldots \}$ in mind, which has limit point $a^{\mathbb N}$. Is there a way to proceed along the lines of this proof and construct for every $Y$ such a partition into separated sets?",,"['general-topology', 'analysis', 'descriptive-set-theory', 'connectedness']"
13,Prove that a sequence either: 1) converges 2) is unbounded or 3) has more than one limit point,Prove that a sequence either: 1) converges 2) is unbounded or 3) has more than one limit point,,"Prove that a sequence either 1) converges 2) is unbounded 3) has more than one limit point Well i think that this is equivalent to looking at the limit set of the sequence and showing that sequence has either: 1) limsup = liminf 2) at least limsup or liminf is infinity 3) limsup > liminf Just for clarification, a limit point is a limit of some subsequence. The limit set is the set of all limit pts. Not so sure how to show this. Any ideas?","Prove that a sequence either 1) converges 2) is unbounded 3) has more than one limit point Well i think that this is equivalent to looking at the limit set of the sequence and showing that sequence has either: 1) limsup = liminf 2) at least limsup or liminf is infinity 3) limsup > liminf Just for clarification, a limit point is a limit of some subsequence. The limit set is the set of all limit pts. Not so sure how to show this. Any ideas?",,"['real-analysis', 'analysis']"
14,"If $f:[a,b]\to[a,b]$ is increasing, continuous, and $f(a)=a$, how to prove $f(E)=E$ where $E=\{x:a\le x\le b,f(x)\ge x\}$?","If  is increasing, continuous, and , how to prove  where ?","f:[a,b]\to[a,b] f(a)=a f(E)=E E=\{x:a\le x\le b,f(x)\ge x\}","Let $f:[a,b]\to[a,b]$ satisfy: $f$ is   increasing $f$ is  continuous $f(a)=a$ If $E=\{x:a\le x\le b,f(x)\ge x\}$ , then how can we prove that $f(E)=E$ ?","Let satisfy: is   increasing is  continuous If , then how can we prove that ?","f:[a,b]\to[a,b] f f f(a)=a E=\{x:a\le x\le b,f(x)\ge x\} f(E)=E","['real-analysis', 'analysis', 'contest-math']"
15,Proving the existence of a point $a \in \mathbb{R}_+$ s.t. $\cos(a) < 0$,Proving the existence of a point  s.t.,a \in \mathbb{R}_+ \cos(a) < 0,"I am currently working on a challenge problem where I need to show that there is a point $x \in \mathbb{R_+}$ such that $\cos(x) = 0$ using only a few properties of the cosine function. In particular, the only properties of the cosine function that I can use are: $\cos(x)$ is continuous $\cos(x) = Re(\exp(z))$  for $z \in \mathbb{C}$ $\cos^2(x) + \sin^2(x)=1$ $\displaystyle \cos(x) = \sum_{n=0}^\infty \frac{(-1)^n}{(2n!)}x^{2n}$ $\displaystyle \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!}$ My strategy is to use the intermediate value theorem on the interval $[0,a]$ since it's easy to show that $\cos(0) = 1$. If I could show that there is a point $a \in \mathbb{R}_+$ s.t. $\cos(a) < 0$, then the IVT and the continuity of the cosine function would allow me to conclude that there has to be some $x\in [0,a]$ such that $\cos(x) = 0$.","I am currently working on a challenge problem where I need to show that there is a point $x \in \mathbb{R_+}$ such that $\cos(x) = 0$ using only a few properties of the cosine function. In particular, the only properties of the cosine function that I can use are: $\cos(x)$ is continuous $\cos(x) = Re(\exp(z))$  for $z \in \mathbb{C}$ $\cos^2(x) + \sin^2(x)=1$ $\displaystyle \cos(x) = \sum_{n=0}^\infty \frac{(-1)^n}{(2n!)}x^{2n}$ $\displaystyle \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!}$ My strategy is to use the intermediate value theorem on the interval $[0,a]$ since it's easy to show that $\cos(0) = 1$. If I could show that there is a point $a \in \mathbb{R}_+$ s.t. $\cos(a) < 0$, then the IVT and the continuity of the cosine function would allow me to conclude that there has to be some $x\in [0,a]$ such that $\cos(x) = 0$.",,['analysis']
16,Cesaro summable implies that $c_{n}/n$ goes to $0$,Cesaro summable implies that  goes to,c_{n}/n 0,"Theorem. If $\sum_{n=1}^{\infty}c_{n}$ is Cesaro summable, then $c_{n}/n$ tends to $0$. How to prove it?","Theorem. If $\sum_{n=1}^{\infty}c_{n}$ is Cesaro summable, then $c_{n}/n$ tends to $0$. How to prove it?",,"['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
17,Surjective and injective functions,Surjective and injective functions,,"So let's say that we have some function $f:  \mathbb{A} \rightarrow \mathbb{B} $ Is it possible to have some function such that not all elements of A map to some value in B? Like for example, in these pictures for various surjective and injective functions: Would it be possible to have some function that has elements in A that don't map to any values of B? Like in example 1, just have the 3 in A without mapping to the element in B?","So let's say that we have some function $f:  \mathbb{A} \rightarrow \mathbb{B} $ Is it possible to have some function such that not all elements of A map to some value in B? Like for example, in these pictures for various surjective and injective functions: Would it be possible to have some function that has elements in A that don't map to any values of B? Like in example 1, just have the 3 in A without mapping to the element in B?",,"['analysis', 'functions']"
18,Convergence of multiple series $\sum\frac{1}{n^2+m^2}$,Convergence of multiple series,\sum\frac{1}{n^2+m^2},"I have a following series $$ \sum\frac{1}{n^2+m^2} $$ As far as I understand it converges. I tried Cauchy criteria and it showed divergency, but i may be mistaken. When I calculate it in matlab or Maxima it have a good behaviour and converge to finite number about 10.17615092535112. The convergency plot is following:","I have a following series $$ \sum\frac{1}{n^2+m^2} $$ As far as I understand it converges. I tried Cauchy criteria and it showed divergency, but i may be mistaken. When I calculate it in matlab or Maxima it have a good behaviour and converge to finite number about 10.17615092535112. The convergency plot is following:",,"['sequences-and-series', 'analysis', 'divergent-series']"
19,Showing that $\sum_{k=1}^\infty\frac{(\log k)^m}{k^{1+\delta}} < \infty $ ?,Showing that  ?,\sum_{k=1}^\infty\frac{(\log k)^m}{k^{1+\delta}} < \infty ,How would you show that $$\sum_{k=1}^\infty\frac{(\log k)^m}{k^{1+\delta}} < \infty $$ for $\delta >0$ and $m \in \mathbb{N}$?,How would you show that $$\sum_{k=1}^\infty\frac{(\log k)^m}{k^{1+\delta}} < \infty $$ for $\delta >0$ and $m \in \mathbb{N}$?,,['real-analysis']
20,Why is $\sum_{n=1}^{\infty }(-1)^{n+1}\frac{1}{n}.e^{-nx}$ uniformly convergent?,Why is  uniformly convergent?,\sum_{n=1}^{\infty }(-1)^{n+1}\frac{1}{n}.e^{-nx},"Why is the following series uniformly convergent:$$\sum_{n=1}^{\infty }(-1)^{n+1}\frac{1}{n}.e^{-nx}$$? where $ x\geq 0$ I tried the Weierstrass-M test, but it doesn't work here because:$\left | (-1)^{n+1}\frac{1}{n}.e^{-nx} \right |= \frac{1}{n}.e^{-nx}\leq \frac{1}{n}$, and $ \sum_{n=1}^{\infty }\frac{1}{n}$ is divergent.","Why is the following series uniformly convergent:$$\sum_{n=1}^{\infty }(-1)^{n+1}\frac{1}{n}.e^{-nx}$$? where $ x\geq 0$ I tried the Weierstrass-M test, but it doesn't work here because:$\left | (-1)^{n+1}\frac{1}{n}.e^{-nx} \right |= \frac{1}{n}.e^{-nx}\leq \frac{1}{n}$, and $ \sum_{n=1}^{\infty }\frac{1}{n}$ is divergent.",,"['calculus', 'real-analysis', 'analysis']"
21,"$X, Y \subset \mathbb{R}^n$. Define $d(K, F) = \inf\{ d(x,y), x \in K, y \in F\}$, show that $d(a,b) = d(K,F)$ for some $a$, $b$",". Define , show that  for some ,","X, Y \subset \mathbb{R}^n d(K, F) = \inf\{ d(x,y), x \in K, y \in F\} d(a,b) = d(K,F) a b","Some caveats: Let $K$ be non-empty and compact, $F$ be non-empty and closed, $X, Y \subset \mathbb{R}^n$. Define $d(K, F) = \inf\{ d(x,y), x \in K, y \in F\}$,  where $d(x,y)$ is one of the metrics $d_1$, $d_2$, or $d_\infty$ on $\mathbb{R}^n$. Show that $d(a,b) = d(K,F)$ for some $a\in K$, $b\in F$. It seems like the solution would use some properties of compactness and perhaps the Heine-Borel theorem, but I can't figure out where to start. Any help much appreciated.","Some caveats: Let $K$ be non-empty and compact, $F$ be non-empty and closed, $X, Y \subset \mathbb{R}^n$. Define $d(K, F) = \inf\{ d(x,y), x \in K, y \in F\}$,  where $d(x,y)$ is one of the metrics $d_1$, $d_2$, or $d_\infty$ on $\mathbb{R}^n$. Show that $d(a,b) = d(K,F)$ for some $a\in K$, $b\in F$. It seems like the solution would use some properties of compactness and perhaps the Heine-Borel theorem, but I can't figure out where to start. Any help much appreciated.",,"['general-topology', 'analysis', 'metric-spaces']"
22,Finding second derivative of integral,Finding second derivative of integral,,"Here is the problem I'm looking at: Given $f: \mathbb{R} \to \mathbb{R}$ is differentiable, define the function $$ H(x) = \int_{-x}^x [f(t)+f(-t)] dt \text{ } \text{ } \text{  for all x}$$ Find $H''(x)$ Now here's my crack at the solution. Is this right? $H'(x) = \displaystyle\frac{d}{dx} \displaystyle\int_{-x}^x f(t) + f(-t) dt = [x+(-x)] - [(-x) +x] = 0$ $H''(X) = \displaystyle\frac{d}{dx} H'(X) = \displaystyle\frac{d}{dx} 0 = 0$","Here is the problem I'm looking at: Given $f: \mathbb{R} \to \mathbb{R}$ is differentiable, define the function $$ H(x) = \int_{-x}^x [f(t)+f(-t)] dt \text{ } \text{ } \text{  for all x}$$ Find $H''(x)$ Now here's my crack at the solution. Is this right? $H'(x) = \displaystyle\frac{d}{dx} \displaystyle\int_{-x}^x f(t) + f(-t) dt = [x+(-x)] - [(-x) +x] = 0$ $H''(X) = \displaystyle\frac{d}{dx} H'(X) = \displaystyle\frac{d}{dx} 0 = 0$",,"['calculus', 'analysis']"
23,Cauchy sequence and convergence,Cauchy sequence and convergence,,"A sequence is said to be Cauchy sequence if for given any integer n, there exists a positive real number R, such that for any n1, n2 > n, mod{n1th term - n2th term} 1,0,1,0,1,0,1,0........ Now for any integer n,  whenever n1,n2 >n mod{n1th term - n2th term} < or equal to 1. So as per the definition of a Cauchy  sequence we can say that this sequence is a Cauchy sequence, however, this is not a convergent sequence. How come? This implies there is gap in my understanding, can anyone kindly point out where am I wrong?. Edited version: A sequence $(a_n)$ is said to be Cauchy sequence if for given any integer $n$, there exists a positive real number $R$, such that for any $n_1, n_2 > n$, $|a_{n_1}-a_{n_2}|<R$. We can prove that every Cauchy sequence is a convergent sequence. Now let us consider the following sequence, $1,0,1,0,1,0,1,0,\ldots$ Now for any integer $n$,  whenever $n_1,n_2 >n$ we have  $|a_{n_1}-a_{n_2}|\le 1$ . So as per the definition of a Cauchy  sequence we can say that this sequence is a Cauchy sequence, however, this is not a convergent sequence. How come? This implies there is gap in my understanding, can anyone kindly point out where am I wrong?.","A sequence is said to be Cauchy sequence if for given any integer n, there exists a positive real number R, such that for any n1, n2 > n, mod{n1th term - n2th term} 1,0,1,0,1,0,1,0........ Now for any integer n,  whenever n1,n2 >n mod{n1th term - n2th term} < or equal to 1. So as per the definition of a Cauchy  sequence we can say that this sequence is a Cauchy sequence, however, this is not a convergent sequence. How come? This implies there is gap in my understanding, can anyone kindly point out where am I wrong?. Edited version: A sequence $(a_n)$ is said to be Cauchy sequence if for given any integer $n$, there exists a positive real number $R$, such that for any $n_1, n_2 > n$, $|a_{n_1}-a_{n_2}|<R$. We can prove that every Cauchy sequence is a convergent sequence. Now let us consider the following sequence, $1,0,1,0,1,0,1,0,\ldots$ Now for any integer $n$,  whenever $n_1,n_2 >n$ we have  $|a_{n_1}-a_{n_2}|\le 1$ . So as per the definition of a Cauchy  sequence we can say that this sequence is a Cauchy sequence, however, this is not a convergent sequence. How come? This implies there is gap in my understanding, can anyone kindly point out where am I wrong?.",,"['sequences-and-series', 'analysis', 'cauchy-sequences']"
24,Can you provide a lower bound on $|\zeta (s) |$ for fixed $\mathrm{Re}(s) > 1$?,Can you provide a lower bound on  for fixed ?,|\zeta (s) | \mathrm{Re}(s) > 1,"It's easy to prove, for example, that $|\zeta(2 + it)| > 2 - \frac{\pi^2}{6}$.  However, there is some $\sigma > 1$ for which $\zeta ( \sigma ) = 2$, and it is more difficult to obtain a lower bound on $|\zeta (\sigma' + it) |$ for $1<\sigma' \leq \sigma$.  Can you provide a non-trivial lower bound?","It's easy to prove, for example, that $|\zeta(2 + it)| > 2 - \frac{\pi^2}{6}$.  However, there is some $\sigma > 1$ for which $\zeta ( \sigma ) = 2$, and it is more difficult to obtain a lower bound on $|\zeta (\sigma' + it) |$ for $1<\sigma' \leq \sigma$.  Can you provide a non-trivial lower bound?",,"['analysis', 'riemann-zeta']"
25,Is Thomae's function Riemann integrable?,Is Thomae's function Riemann integrable?,,"Let $\displaystyle f: [0,1] \rightarrow \mathbb{R}$ given by $$f(x) = \begin{cases} 0 & x \notin \mathbb{Q} \\ \\ 0 & x = 0 \\ \\ \frac{1}{q_x} & x = \frac{p_x}{q_x} \in \mathbb{Q} \backslash \{0\}, \ p_x \in \mathbb{Z}, \  q_x \in \mathbb{N}, \ \text{gcd}(|p_x|, q_x) = 1 \end{cases}$$ Is $\displaystyle f$ Riemann integrable? I am trying to use the equivalent statements $\displaystyle g:[a,b] \rightarrow \mathbb{R}$ is Riemann integrable and $\displaystyle \forall \epsilon >0 \ \exists$ step functions $\displaystyle \rho, \psi$ with $\displaystyle \rho \leq g \leq \psi$ such that $\displaystyle \int_a^b (\psi - \rho) \leq \epsilon$. I guess that means I would have to somehow show that given $\displaystyle \epsilon$, there exists only a finite amount of $\displaystyle x \in [0,1]$ with $\displaystyle f(x) \geq \epsilon$? Is it recommended that I consider something else instead? If not, how should I do this?","Let $\displaystyle f: [0,1] \rightarrow \mathbb{R}$ given by $$f(x) = \begin{cases} 0 & x \notin \mathbb{Q} \\ \\ 0 & x = 0 \\ \\ \frac{1}{q_x} & x = \frac{p_x}{q_x} \in \mathbb{Q} \backslash \{0\}, \ p_x \in \mathbb{Z}, \  q_x \in \mathbb{N}, \ \text{gcd}(|p_x|, q_x) = 1 \end{cases}$$ Is $\displaystyle f$ Riemann integrable? I am trying to use the equivalent statements $\displaystyle g:[a,b] \rightarrow \mathbb{R}$ is Riemann integrable and $\displaystyle \forall \epsilon >0 \ \exists$ step functions $\displaystyle \rho, \psi$ with $\displaystyle \rho \leq g \leq \psi$ such that $\displaystyle \int_a^b (\psi - \rho) \leq \epsilon$. I guess that means I would have to somehow show that given $\displaystyle \epsilon$, there exists only a finite amount of $\displaystyle x \in [0,1]$ with $\displaystyle f(x) \geq \epsilon$? Is it recommended that I consider something else instead? If not, how should I do this?",,['analysis']
26,Constructing a Cantor-like set by subtracting closed intervals,Constructing a Cantor-like set by subtracting closed intervals,,"I want to construct a nowhere dense set of positive measure by using the Smith-Volterra-Cantor method but subtracting closed sets intead of open sets. Suppose we start with the open interval (0,1).  Let $\alpha=1/2$.  Now remove $U_1=[1/2-\alpha/4,1/2+\alpha/4]$.  The measure $\mu(U_1)=\alpha/2$.  For each of the two remaining open sets, subtract a closed from the center of width $\alpha/8$, thus a total measure of $\alpha/4$.  Repeating this process results in removing a total measure of $\alpha/2+\alpha/4+\dots=\alpha$. Does this give me a set of positive measure just as with subtracting closed intervals?  I suspect not, since I have never heard of this, but what's the exact function of removing a closed set instead of an empty set? Or, if I can do this, can I now consider this to be an open set that I can choose an admissible partition of unity for?","I want to construct a nowhere dense set of positive measure by using the Smith-Volterra-Cantor method but subtracting closed sets intead of open sets. Suppose we start with the open interval (0,1).  Let $\alpha=1/2$.  Now remove $U_1=[1/2-\alpha/4,1/2+\alpha/4]$.  The measure $\mu(U_1)=\alpha/2$.  For each of the two remaining open sets, subtract a closed from the center of width $\alpha/8$, thus a total measure of $\alpha/4$.  Repeating this process results in removing a total measure of $\alpha/2+\alpha/4+\dots=\alpha$. Does this give me a set of positive measure just as with subtracting closed intervals?  I suspect not, since I have never heard of this, but what's the exact function of removing a closed set instead of an empty set? Or, if I can do this, can I now consider this to be an open set that I can choose an admissible partition of unity for?",,"['calculus', 'analysis']"
27,Limit of the sequence $nx_{n}$ where $x_{n+1} = \log (1 +x_{n})$ [duplicate],Limit of the sequence  where  [duplicate],nx_{n} x_{n+1} = \log (1 +x_{n}),"This question already has an answer here : $a_{n+1}=\log(1+a_n),~a_1>0$. Then find $\lim_{n \rightarrow \infty} n \cdot a_n$ (1 answer) Closed 5 years ago . Suppose $x_{1}>0$, and consider the sequence, $\{x_{n}\}$ defined as follows: $$x_{n+1}=\log(1+x_{n}) \quad n\geq 1 $$ Find the value of $\displaystyle \lim_{n \to \infty} nx_{n}$ I am having trouble solving it. One thing is clear, that since $x_{n}>0$ and $x_{n+1} < x_{n}$, we can have a sequence which converges an $f$ which satisfies $f=\log(1+f)$, so that $f=0$. Any way as to how we can proceed from here.","This question already has an answer here : $a_{n+1}=\log(1+a_n),~a_1>0$. Then find $\lim_{n \rightarrow \infty} n \cdot a_n$ (1 answer) Closed 5 years ago . Suppose $x_{1}>0$, and consider the sequence, $\{x_{n}\}$ defined as follows: $$x_{n+1}=\log(1+x_{n}) \quad n\geq 1 $$ Find the value of $\displaystyle \lim_{n \to \infty} nx_{n}$ I am having trouble solving it. One thing is clear, that since $x_{n}>0$ and $x_{n+1} < x_{n}$, we can have a sequence which converges an $f$ which satisfies $f=\log(1+f)$, so that $f=0$. Any way as to how we can proceed from here.",,"['real-analysis', 'analysis']"
28,Examining why the proof for the intersection of (finite) collections of open sets being open in a metric space does not extend to infinite collections,Examining why the proof for the intersection of (finite) collections of open sets being open in a metric space does not extend to infinite collections,,"In a metric space, we have that the intersection of countably many open sets is open: Let $A_1, A_2, \ldots, A_n$ be open sets and $A = \bigcap_{i=1}^n A_i$ . For $x \in A$ , $x \in A_i$ for all $i$ , and since each $A_i$ is open, $\exists r_i > 0$ with $B_{r_i}(x) \subseteq A_i$ . Set $r = \min\{r_1, r_2, \ldots, r_n\}$ , then $B_r(x) \subseteq A$ . Since $x$ was arbitrary, $A$ is open. I am trying to understand why this proof, does not hold for uncountably infinite sets and I believe the problem lies in this step: $r = \min\{r_1, r_2, \ldots, r_n\}$ I think this because: For any collection of elements of any space that I am considering the intersection of, there is a associated $r_i$ with each element. Then since we are considering a metric space $\{r_1, r_2, \ldots, r_i, \ldots\} \subset \mathbb{R}$ . Then it is not necessarily true that there exists a minimum $r_i$ . As say $\{r_1, r_2, \ldots, r_i, \ldots\}$ was the set $(0,1)$ . I am aware that I can consider the many counterexamples for the the uncountable union of open sets, but I want to identify, which will help me examine when the future proofs I do hold, where the proof given falters when I try and make a more general statement. If someone could point out if the reasoning I provided is the correct way to think about things, that would be great!","In a metric space, we have that the intersection of countably many open sets is open: Let be open sets and . For , for all , and since each is open, with . Set , then . Since was arbitrary, is open. I am trying to understand why this proof, does not hold for uncountably infinite sets and I believe the problem lies in this step: I think this because: For any collection of elements of any space that I am considering the intersection of, there is a associated with each element. Then since we are considering a metric space . Then it is not necessarily true that there exists a minimum . As say was the set . I am aware that I can consider the many counterexamples for the the uncountable union of open sets, but I want to identify, which will help me examine when the future proofs I do hold, where the proof given falters when I try and make a more general statement. If someone could point out if the reasoning I provided is the correct way to think about things, that would be great!","A_1, A_2, \ldots, A_n A = \bigcap_{i=1}^n A_i x \in A x \in A_i i A_i \exists r_i > 0 B_{r_i}(x) \subseteq A_i r = \min\{r_1, r_2, \ldots, r_n\} B_r(x) \subseteq A x A r = \min\{r_1, r_2, \ldots, r_n\} r_i \{r_1, r_2, \ldots, r_i, \ldots\} \subset \mathbb{R} r_i \{r_1, r_2, \ldots, r_i, \ldots\} (0,1)","['real-analysis', 'general-topology', 'analysis']"
29,Why are closed balls not a topology in a metric space?,Why are closed balls not a topology in a metric space?,,"Define a closed ball in a metric space $X$ with the metric $\rho$ that has centre a $x$ and radius $r$ as the set $\{y:\rho(x,y)\le r\}$ for $r\ge0$ and $x,y\in X$ . Why are arbitrary unions of closed balls not a topology in $X$ but arbitrary unions of open balls are? An open ball is defined as the set $\{y:\rho(x,y)\lt r\}$ .",Define a closed ball in a metric space with the metric that has centre a and radius as the set for and . Why are arbitrary unions of closed balls not a topology in but arbitrary unions of open balls are? An open ball is defined as the set .,"X \rho x r \{y:\rho(x,y)\le r\} r\ge0 x,y\in X X \{y:\rho(x,y)\lt r\}","['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
30,Showing $\frac{[\sin y \ (x+\frac{1}{x}-2\cos y)]^2}{x^2+6} \leq 1$ for $x>1$,Showing  for,\frac{[\sin y \ (x+\frac{1}{x}-2\cos y)]^2}{x^2+6} \leq 1 x>1,"I was trying to solve Ex 21 Chapter 5 from Stevenhagen, Number Theory and I came up into this trigonometric inequality which would be: $$\frac{[\sin y \ (x+\frac{1}{x}-2\cos y)]^2}{x^2+6} \leq 1  \quad \text{ for } x>1$$ I tried to maximize the multivariate function but couldn't conclude. Does anyone have any ideas/hints on how to solve this?","I was trying to solve Ex 21 Chapter 5 from Stevenhagen, Number Theory and I came up into this trigonometric inequality which would be: I tried to maximize the multivariate function but couldn't conclude. Does anyone have any ideas/hints on how to solve this?","\frac{[\sin y \ (x+\frac{1}{x}-2\cos y)]^2}{x^2+6} \leq 1
 \quad \text{ for } x>1","['analysis', 'multivariable-calculus', 'inequality', 'examples-counterexamples']"
31,Solving a particulary tricky PDE,Solving a particulary tricky PDE,,"I'm currently working on a problem that involves PDEs. I rarely work with them and have never formally learned any solution strategies other than the one I'm going to describe to you. However I somehow fail at the step of comparing the coeffictients: Solving the PDE given by: $$\partial_y u(x,y) = -x^2-y$$ and $$\partial_x u(x,y) = 2x - y$$ Therefore u(x,y) is given by: $$u(x,y) =-x^2y - \frac{1}{2} y^2 + g(x)$$ and by: $$u(x,y) = x^2 - yx + h(y)$$ Normally I'd just compare the results and see the answer, but currently I can't see anything :/","I'm currently working on a problem that involves PDEs. I rarely work with them and have never formally learned any solution strategies other than the one I'm going to describe to you. However I somehow fail at the step of comparing the coeffictients: Solving the PDE given by: and Therefore u(x,y) is given by: and by: Normally I'd just compare the results and see the answer, but currently I can't see anything :/","\partial_y u(x,y) = -x^2-y \partial_x u(x,y) = 2x - y u(x,y) =-x^2y - \frac{1}{2} y^2 + g(x) u(x,y) = x^2 - yx + h(y)","['real-analysis', 'analysis', 'partial-differential-equations', 'linear-pde']"
32,Showing a recursively defined sequence is convergent using Banach fixed-point theorem,Showing a recursively defined sequence is convergent using Banach fixed-point theorem,,"I recently stumbled across an interesting question, in which we need to show a recursively defined sequence $(x_n)_{n\in\mathbb{N}}$ in $\mathbb{R}$ defined as follows $$ x_0 = 4 , \quad x_{n+1}=\ln\left(3-\frac{x_n}{2}\right) \quad \text{for} \ \ \  n \ge 1 $$ is convergent using the Banach Fixed-Point Theorem. I am familiar with how to show a function is convergent using the Banach Fixed-Point Theorem given, but I haven't necessarily seen it be used on a recursively defined sequence. I'd assume you could use generating functions, but is there an easier way? I would be thankful for any help.","I recently stumbled across an interesting question, in which we need to show a recursively defined sequence in defined as follows is convergent using the Banach Fixed-Point Theorem. I am familiar with how to show a function is convergent using the Banach Fixed-Point Theorem given, but I haven't necessarily seen it be used on a recursively defined sequence. I'd assume you could use generating functions, but is there an easier way? I would be thankful for any help.","(x_n)_{n\in\mathbb{N}} \mathbb{R} 
x_0 = 4 , \quad x_{n+1}=\ln\left(3-\frac{x_n}{2}\right) \quad \text{for} \ \ \  n \ge 1
","['real-analysis', 'sequences-and-series', 'analysis', 'fixed-point-theorems']"
33,"Recursive formula for $\int_0^1 x^n (x + a)^{-1} \,dx$",Recursive formula for,"\int_0^1 x^n (x + a)^{-1} \,dx",I'm stuck on a textbook exercise that asks me to define the integral $I_n$ in the title in terms of $I_{n-1}$ . The only technique I'm aware of for determining integral recurrence relations is integration by parts and that does not seem to work out here no matter how I pick $u$ and $v$ . Is there another technique I need to apply or am I missing something?,I'm stuck on a textbook exercise that asks me to define the integral in the title in terms of . The only technique I'm aware of for determining integral recurrence relations is integration by parts and that does not seem to work out here no matter how I pick and . Is there another technique I need to apply or am I missing something?,I_n I_{n-1} u v,"['integration', 'analysis']"
34,"For every non-closed set, there is a continuous function that has no continuous extension [duplicate]","For every non-closed set, there is a continuous function that has no continuous extension [duplicate]",,"This question already has an answer here : If every continuous function on a set can be extended to a continuous function on $\mathbb{R}$ then the set is closed. (1 answer) Closed 1 year ago . I have a question. We know that ""if $A \subset \mathbb{R}$ is a closed set, then every continuous function $f : A \to \mathbb{R}$ has a continuous extension from $\mathbb{R}$ to $\mathbb{R}$ "". But I now I have to prove ""if $A \subset \mathbb{R}$ is a non-closed set, then there is a continuous function $f : A \to \mathbb{R}$ which has no continuous extension from $\mathbb{R}$ to $\mathbb{R}$ "". This means when we omit the assumption about closedness of $A$ , the theorem does not hold. But how can we prove for every non-closed set $A$ ? I find a similar question that we can prove by the contrapositive: If every continuous function on a set can be extended to a continuous function on $\mathbb{R}$ then the set is closed. But my question is ""Is there any way to prove directly?"" Please give me some hints. Any help is highly appreciated.","This question already has an answer here : If every continuous function on a set can be extended to a continuous function on $\mathbb{R}$ then the set is closed. (1 answer) Closed 1 year ago . I have a question. We know that ""if is a closed set, then every continuous function has a continuous extension from to "". But I now I have to prove ""if is a non-closed set, then there is a continuous function which has no continuous extension from to "". This means when we omit the assumption about closedness of , the theorem does not hold. But how can we prove for every non-closed set ? I find a similar question that we can prove by the contrapositive: If every continuous function on a set can be extended to a continuous function on $\mathbb{R}$ then the set is closed. But my question is ""Is there any way to prove directly?"" Please give me some hints. Any help is highly appreciated.",A \subset \mathbb{R} f : A \to \mathbb{R} \mathbb{R} \mathbb{R} A \subset \mathbb{R} f : A \to \mathbb{R} \mathbb{R} \mathbb{R} A A,"['general-topology', 'analysis', 'continuity']"
35,Why I am get complex values in this integral?,Why I am get complex values in this integral?,,"I would like to get the following integral: $$\int -\frac{\log(a^2+x^2)}{(a^2+x^2)}dx \quad \text{or} \quad \int_{t}^{+\infty}-\frac{\log(a^2+x^2)}{(a^2+x^2)}dx$$ where $t>0$ . I used WolframAlpha to compute, and I got following expression: I am just wondering why some complex value i appears? Any ideas of how to get the closed form of integral from t to +inf ? I guess I need to give more specification to wolframalpha to compute ?","I would like to get the following integral: where . I used WolframAlpha to compute, and I got following expression: I am just wondering why some complex value i appears? Any ideas of how to get the closed form of integral from t to +inf ? I guess I need to give more specification to wolframalpha to compute ?",\int -\frac{\log(a^2+x^2)}{(a^2+x^2)}dx \quad \text{or} \quad \int_{t}^{+\infty}-\frac{\log(a^2+x^2)}{(a^2+x^2)}dx t>0,"['analysis', 'definite-integrals', 'indefinite-integrals']"
36,"Closed form for $\sum_{n=2}^{\infty} \big{(}H_{n,n}-1\big{)} $?",Closed form for ?,"\sum_{n=2}^{\infty} \big{(}H_{n,n}-1\big{)} ","It is known that $$\sum_{n=2}^{\infty}\big{(}\zeta(n)-1\big{)}=1 .$$ Many more series like these, called rational zeta series, can be evaluated in closed form. I wonder if we can also obtain similar results for series involving rational sums of generalized harmonic numbers. Such numbers are defined as: $$H_{n,m} := \sum_{k=1}^{n} \frac{1}{k^{m}} .$$ So they form a finite analogy of zeta values, because $\lim_{n \to \infty} H_{n,m} = \zeta(m). $ Question : can a closed form of the series $$\sum_{n=2}^{\infty} \big{(}H_{n,n}-1\big{)} \approx 0.561 $$ be obtained? And are any results on ""rational generalized harmonic series"" known?","It is known that Many more series like these, called rational zeta series, can be evaluated in closed form. I wonder if we can also obtain similar results for series involving rational sums of generalized harmonic numbers. Such numbers are defined as: So they form a finite analogy of zeta values, because Question : can a closed form of the series be obtained? And are any results on ""rational generalized harmonic series"" known?","\sum_{n=2}^{\infty}\big{(}\zeta(n)-1\big{)}=1 . H_{n,m} := \sum_{k=1}^{n} \frac{1}{k^{m}} . \lim_{n \to \infty} H_{n,m} = \zeta(m).  \sum_{n=2}^{\infty} \big{(}H_{n,n}-1\big{)} \approx 0.561 ","['sequences-and-series', 'analysis', 'harmonic-numbers']"
37,Suggestions for proving $ \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0)$,Suggestions for proving, \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0),"The polynomial $p$ has degrees less or equal to n and I'm trying to prove \begin{equation}  \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0) \end{equation} $p^{(n)}(0)=a_n\cdot n!$ The followings are my attempts First is just to expand $\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)$ , but I didn't find any way of solving this. Second I started to consider about using induction. Base case degree p=1 is true and suppose $\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0)$ Prove \begin{equation} \displaystyle \sum^{n+1}_{k=0} \binom{n+1}{k}(-1)^{n+1-k}p(k)=p^{(n+1)}(0) \end{equation} Then I tried to just directly expand this formula, but it seemingly doesn't work and it requires a lot of computation (as so far, the most hopeful way) Then I use the fact $\binom{n+1}{k}=\binom{n}{k}+\binom{n}{k-1}$ to split the formula My attempt was: \begin{equation*} \displaystyle  \sum^{n+1}_{k=0}[\binom{n}{k}+\binom{n}{k-1}](-1)^{n+1-k}p(k)  \end{equation*} and get \begin{equation*} \displaystyle  \sum^{n+1}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k) + \sum^{n+1}_{k=0}\binom{n}{k-1}(-1)^{n+1-k}p(k)  \end{equation*} For the left part, since when k=n+1, $\binom{n}{n+1}=0$ , \begin{equation*} \displaystyle  \sum^{n+1}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k)=\displaystyle  \sum^{n}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k)=(-1)\sum^{n}_{k=0}\binom{n}{k}(-1)^{n-k}p(k)=-p^{(n)}(0) \end{equation*} However, For the right part:when k=0, $\binom{n}{−1}=0$ . Also, let k=j+1 thus it can be \begin{equation} \sum^{n+1}_{k=0}\binom{n}{k-1}(-1)^{n+1-k}p(k)= \sum^{n+1}_{k=1}\binom{n}{k-1}(-1)^{n+1-k}p(k)=\sum^{n}_{j=0}\binom{n}{j}(-1)^{n-j}p(j+1) \end{equation} Then I totally get stuck, since I don't know how to deal with this $p(j+1)$ , and I can't come up with more ideas. Any help, comments, or suggestions are appreciated! Based on the idea from pirahahindu1999, my trial: regard the formula $\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)$ as a map, and all polynomials are vectors. Thus we have $F(p(x))=\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)$ It's true that the map $F$ is a well-defined and linear map. Then $F$ : $ P_n(x) \to \mathbb R$ It's clear that $P_n(x)$ has a basis $\{1,x,x(x-1),x(x-1)(x-2),...,x(x-1)...1\}$ and suppose $1=e_0,x=e_1,x(x-1)=e_2...$ Then it's true that $F$ depends on the degree of the basis vectors, and the fact is that $F_3(e_3)=3!,F_3(e_2)=F_3(e_1)=F_3(e_0)=0$ Similarly for $F_n$ Since any polynomial can be expressed as the linear combination of basis vectors, it's true to have \begin{equation} p(x)=a_0e_0+...+a_me_m \end{equation} When m=n, $F_n(p(x))=F(a_ne_n)=a_n·n!=p^{(n)}(0)$ When m<n, $F_n(p(x))=0$ qed","The polynomial has degrees less or equal to n and I'm trying to prove The followings are my attempts First is just to expand , but I didn't find any way of solving this. Second I started to consider about using induction. Base case degree p=1 is true and suppose Prove Then I tried to just directly expand this formula, but it seemingly doesn't work and it requires a lot of computation (as so far, the most hopeful way) Then I use the fact to split the formula My attempt was: and get For the left part, since when k=n+1, , However, For the right part:when k=0, . Also, let k=j+1 thus it can be Then I totally get stuck, since I don't know how to deal with this , and I can't come up with more ideas. Any help, comments, or suggestions are appreciated! Based on the idea from pirahahindu1999, my trial: regard the formula as a map, and all polynomials are vectors. Thus we have It's true that the map is a well-defined and linear map. Then : It's clear that has a basis and suppose Then it's true that depends on the degree of the basis vectors, and the fact is that Similarly for Since any polynomial can be expressed as the linear combination of basis vectors, it's true to have When m=n, When m<n, qed","p \begin{equation}
 \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0)
\end{equation} p^{(n)}(0)=a_n\cdot n! \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k) \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0) \begin{equation}
\displaystyle \sum^{n+1}_{k=0} \binom{n+1}{k}(-1)^{n+1-k}p(k)=p^{(n+1)}(0)
\end{equation} \binom{n+1}{k}=\binom{n}{k}+\binom{n}{k-1} \begin{equation*}
\displaystyle  \sum^{n+1}_{k=0}[\binom{n}{k}+\binom{n}{k-1}](-1)^{n+1-k}p(k) 
\end{equation*} \begin{equation*}
\displaystyle  \sum^{n+1}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k) + \sum^{n+1}_{k=0}\binom{n}{k-1}(-1)^{n+1-k}p(k) 
\end{equation*} \binom{n}{n+1}=0 \begin{equation*}
\displaystyle  \sum^{n+1}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k)=\displaystyle  \sum^{n}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k)=(-1)\sum^{n}_{k=0}\binom{n}{k}(-1)^{n-k}p(k)=-p^{(n)}(0)
\end{equation*} \binom{n}{−1}=0 \begin{equation}
\sum^{n+1}_{k=0}\binom{n}{k-1}(-1)^{n+1-k}p(k)= \sum^{n+1}_{k=1}\binom{n}{k-1}(-1)^{n+1-k}p(k)=\sum^{n}_{j=0}\binom{n}{j}(-1)^{n-j}p(j+1)
\end{equation} p(j+1) \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k) F(p(x))=\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k) F F  P_n(x) \to \mathbb R P_n(x) \{1,x,x(x-1),x(x-1)(x-2),...,x(x-1)...1\} 1=e_0,x=e_1,x(x-1)=e_2... F F_3(e_3)=3!,F_3(e_2)=F_3(e_1)=F_3(e_0)=0 F_n \begin{equation}
p(x)=a_0e_0+...+a_me_m
\end{equation} F_n(p(x))=F(a_ne_n)=a_n·n!=p^{(n)}(0) F_n(p(x))=0","['analysis', 'solution-verification']"
38,Why doesn’t an X percent increase in speed equal and X percent decrease in travel time?,Why doesn’t an X percent increase in speed equal and X percent decrease in travel time?,,"If I am traveling 60 miles per hour and I increase my speed 10% to 66 miles per hour, why has my travel time only been reduced 9.09% and not 10%? Thanks","If I am traveling 60 miles per hour and I increase my speed 10% to 66 miles per hour, why has my travel time only been reduced 9.09% and not 10%? Thanks",,"['analysis', 'soft-question']"
39,"If $f$ is differentiable on [a,b], must there be a subinterval on which$f’(x)$ be bounded?","If  is differentiable on [a,b], must there be a subinterval on which be bounded?",f f’(x),"If $f(x)$ is differentiable on $[a,b]$ , then there must be $(\alpha,\beta)\subset [a,b]$ , such that $f’(x)$ is bounded on $(\alpha,\beta)$ . To prove it or to give an counter-example. The question may have something to do with the intermediate value theorem. I understand if there is no such $(\alpha,\beta)$ , then for every $x\in[a,b]$ , there exists ${x_n}$ such that $\lim_{n\rightarrow\infty} x_n=x$ , and $\lim_{n\rightarrow\infty} |f’(x_n)|=+\infty$ . But I still cannot work things out.","If is differentiable on , then there must be , such that is bounded on . To prove it or to give an counter-example. The question may have something to do with the intermediate value theorem. I understand if there is no such , then for every , there exists such that , and . But I still cannot work things out.","f(x) [a,b] (\alpha,\beta)\subset [a,b] f’(x) (\alpha,\beta) (\alpha,\beta) x\in[a,b] {x_n} \lim_{n\rightarrow\infty} x_n=x \lim_{n\rightarrow\infty} |f’(x_n)|=+\infty","['analysis', 'derivatives']"
40,"Prove (only with definition) that $f(x)=\frac{x^3+1}{x^2}, x>0$ is not uniformly continuous",Prove (only with definition) that  is not uniformly continuous,"f(x)=\frac{x^3+1}{x^2}, x>0","I want to prove (only with definition) that $f(x)=\dfrac{x^3+1}{x^2}, x>0$ is not uniformly continuous. Obviously, $f$ has the problem near at zero. I suppose that $f$ is uniformly continuous. So, for all $\epsilon >0$ , exists some $\delta>0$ , such that if $x,y>0, |x-y|<\delta\Rightarrow |f(x)-f(y)|<\epsilon$ . I tried to take some $0<x<\delta$ and $y=x/2$ . Therefore, $|x-y|<\delta$ , and I want to prove that $|f(x)-f(y)| $ is bigger than a fix number $\epsilon >0$ . However,it seems that this choice about $x$ and $ y$ did not work. Any ideas, what's a better choice for $x$ and $ y$ ? Thanks","I want to prove (only with definition) that is not uniformly continuous. Obviously, has the problem near at zero. I suppose that is uniformly continuous. So, for all , exists some , such that if . I tried to take some and . Therefore, , and I want to prove that is bigger than a fix number . However,it seems that this choice about and did not work. Any ideas, what's a better choice for and ? Thanks","f(x)=\dfrac{x^3+1}{x^2}, x>0 f f \epsilon >0 \delta>0 x,y>0, |x-y|<\delta\Rightarrow |f(x)-f(y)|<\epsilon 0<x<\delta y=x/2 |x-y|<\delta |f(x)-f(y)|  \epsilon >0 x  y x  y","['calculus', 'analysis', 'epsilon-delta', 'uniform-continuity']"
41,Integrate $\int_{0}^{\infty} \big( |y + x|^{\lambda - 1} - |y|^{\lambda - 1} \big) y^{-\beta} dy$,Integrate,\int_{0}^{\infty} \big( |y + x|^{\lambda - 1} - |y|^{\lambda - 1} \big) y^{-\beta} dy,"Let $0< \lambda < 1$ and $0< \beta < \lambda/2$ . I would like to integrate, $$\int_{0}^{\infty} \big( |y + 1|^{\lambda - 1} - |y|^{\lambda - 1} \big) y^{-\beta} dy,$$ this integral is finite because for $y \rightarrow \infty,~(|x+y|^{\lambda - 1} - |y|^{\lambda - 1})y^{-\beta} \approx y^{\lambda - \beta - 2}.$ Integral of $|y + 1|^{\lambda - 1}y^{-\beta}$ can be calculated and  it's equal to Beta function that is, $$ \int_{0}^{\infty} |y + 1|^{\lambda - 1}y^{-\beta} dy = B(1 - \beta, \beta - \lambda).$$ I would really appreciate any hints or tips.","Let and . I would like to integrate, this integral is finite because for Integral of can be calculated and  it's equal to Beta function that is, I would really appreciate any hints or tips.","0< \lambda < 1 0< \beta < \lambda/2 \int_{0}^{\infty} \big( |y + 1|^{\lambda - 1} - |y|^{\lambda - 1} \big) y^{-\beta} dy, y \rightarrow \infty,~(|x+y|^{\lambda - 1} - |y|^{\lambda - 1})y^{-\beta} \approx y^{\lambda - \beta - 2}. |y + 1|^{\lambda - 1}y^{-\beta}  \int_{0}^{\infty} |y + 1|^{\lambda - 1}y^{-\beta} dy = B(1 - \beta, \beta - \lambda).","['calculus', 'integration', 'analysis', 'definite-integrals']"
42,Existence of $\xi$ s.t. $f'''\left(\xi\right)=\frac{3f''\left(\xi\right)}{1-\xi}$,Existence of  s.t.,\xi f'''\left(\xi\right)=\frac{3f''\left(\xi\right)}{1-\xi},"Given $f(x)\in C^3$ , and $f(0)=f(1/2)=f(1)$ . Prove that there exists at least one point $\xi$ , such that $f'''\left(\xi\right)=\dfrac{3f''\left(\xi\right)}{1-\xi}$ . I tried to apply Mean Value Theorem to $f(x)$ , but anyway I can't construct the denominator $1-\xi$ . Can anyone help?","Given , and . Prove that there exists at least one point , such that . I tried to apply Mean Value Theorem to , but anyway I can't construct the denominator . Can anyone help?",f(x)\in C^3 f(0)=f(1/2)=f(1) \xi f'''\left(\xi\right)=\dfrac{3f''\left(\xi\right)}{1-\xi} f(x) 1-\xi,"['real-analysis', 'calculus', 'analysis', 'derivatives']"
43,How can I find the interval of x? I apply ratio test and get indeterminate. $\sum_{n=1}^\infty({(n+1)(n+2)....(2n)\over n^n})\space (x-2)^n$,How can I find the interval of x? I apply ratio test and get indeterminate.,\sum_{n=1}^\infty({(n+1)(n+2)....(2n)\over n^n})\space (x-2)^n,"$$\sum_{n=1}^\infty({(n+1)(n+2)....(2n)\over n^n})\space (x-2)^n$$ Find the radius and interval of convergence of the power series given above. Hi! I am trying to find the interval of x first; however, whenever I apply ratio test, I get indeterminate. How can I find the interval of x? Here is what I've done: Convergence: $$\lim_{n\to\infty}{\cfrac{\lvert(n+2)(n+3)...(2n+2)\space(x-2)^{n+1}\rvert}{\lvert(n+1)^{(n+1)}\rvert}\over\cfrac{\lvert(n+1)(n+2)...(2n)\space(x-2)^n\rvert}{\lvert n^n\rvert}}<1$$ $=>$ $$\lim_{n\to\infty}\cfrac{\lvert 2(2n+1)(x-2)n^n\rvert}{\lvert(n+1)^{n+1}\rvert}<1$$ $=>$ L'Hospital: $$\lvert x-2\rvert\space\lim_{n\to\infty}\cfrac{\lvert (4n+2)\space n^n\rvert}{\vert(n+1)^{n+1}\rvert}<1$$ $=>$ $$\lvert x-2\rvert\space\lim_{n\to\infty}\cfrac{\lvert 4n^n+(4n+2).n^n.\ln(n)\rvert}{(n+1)^{n+1}.\ln(n).(\ln1)}$$ As you can see, ln1 makes it indeterminate and I am unable to find. How can I find the interval of x, so that I can find the interval of convergence and then the radius of convergence?","Find the radius and interval of convergence of the power series given above. Hi! I am trying to find the interval of x first; however, whenever I apply ratio test, I get indeterminate. How can I find the interval of x? Here is what I've done: Convergence: L'Hospital: As you can see, ln1 makes it indeterminate and I am unable to find. How can I find the interval of x, so that I can find the interval of convergence and then the radius of convergence?",\sum_{n=1}^\infty({(n+1)(n+2)....(2n)\over n^n})\space (x-2)^n \lim_{n\to\infty}{\cfrac{\lvert(n+2)(n+3)...(2n+2)\space(x-2)^{n+1}\rvert}{\lvert(n+1)^{(n+1)}\rvert}\over\cfrac{\lvert(n+1)(n+2)...(2n)\space(x-2)^n\rvert}{\lvert n^n\rvert}}<1 => \lim_{n\to\infty}\cfrac{\lvert 2(2n+1)(x-2)n^n\rvert}{\lvert(n+1)^{n+1}\rvert}<1 => \lvert x-2\rvert\space\lim_{n\to\infty}\cfrac{\lvert (4n+2)\space n^n\rvert}{\vert(n+1)^{n+1}\rvert}<1 => \lvert x-2\rvert\space\lim_{n\to\infty}\cfrac{\lvert 4n^n+(4n+2).n^n.\ln(n)\rvert}{(n+1)^{n+1}.\ln(n).(\ln1)},"['sequences-and-series', 'analysis']"
44,$x-\sin(x) \geq \dfrac{x^3}{(x+\pi)^2}$,,x-\sin(x) \geq \dfrac{x^3}{(x+\pi)^2},"Let $x \geq 0.$ I need to prove that $x-\sin(x)\geq\dfrac{x^3}{(\pi+x)^2}.$ I tried the derivative, of $f(x)=x-\sin(x)-\dfrac{x^3}{(\pi+x)^2}$ which is $1-\cos(x)-\dfrac{x^2(x+3\pi)}{(\pi+x)^3},$ but it has a complicated formula. Any ideas, hints? Edit: sorry, there was a mistake in the derivative, I corrected it.","Let I need to prove that I tried the derivative, of which is but it has a complicated formula. Any ideas, hints? Edit: sorry, there was a mistake in the derivative, I corrected it.","x \geq 0. x-\sin(x)\geq\dfrac{x^3}{(\pi+x)^2}. f(x)=x-\sin(x)-\dfrac{x^3}{(\pi+x)^2} 1-\cos(x)-\dfrac{x^2(x+3\pi)}{(\pi+x)^3},","['real-analysis', 'analysis', 'functions', 'inequality']"
45,Please check my work! Question about cubic polynomials,Please check my work! Question about cubic polynomials,,"I need some help with this problem. Here is the link. Can you please tell me if there is an easier way to show that cubic polynomials have a real root? The question is in an analysis book from the continuity section so it has to use that. Here is the latex: Show that a cubic equation (i.e. one of the form $ax^3 + bx^2 + cx + d = 0$ where $a\neq 0)$ has at least one real root. Solution: The equation has at least one root if for some $x_1<x_2$ , $\enspace f(x_1) < 0$ and $f(x_2) > 0$ . Then by the intermediate value theorem $f(c) = 0$ for some $x_1 < c < x_2$ . $x^3$ outgrows smaller powers of $x$ so the function is negative for some large negative number and positive for some large positive number. If $(x_n)$ is a sequence of positive terms that tends to infinity, then $$f(x_n) = ax_n^3 + bx_n^2 + cx_n + d = x_n^3(a+ \frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3})$$ Now $\frac{b}{x_n}, \frac{c}{x_n^2}, \frac{d}{x_n^3}$ are sequences that tend to zero, so for any $\epsilon$ there is an $N$ such that $$|\frac{b}{x_n}| < \epsilon/3, \quad |\frac{c}{x_n^2}| < \epsilon/3, \quad |\frac{d}{x_n^3}| < \epsilon/3$$ and for $\epsilon = a$ , we have $$|\frac{b}{x_n}| + |\frac{c}{x_n^2}| + |\frac{d}{x_n^3}| < a$$ so that, by the triangle inequality $$|\frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3}| \leq |\frac{b}{x_n}| + |\frac{c}{x_n^2}| + |\frac{d}{x_n^3}| < a$$ which means $$-a <\frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3} < a$$ Then for some $|k|<1$ , it can be written $$a+ \frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3} = a+ ka = (1+k)a$$ and $$f(x_n) = x_n^3(1+k)a$$ for $n\geq N$ . Since $x_n$ is a sequence of positive terms, $f(x_n) = k_na$ for $n\geq N$ where $k_n>0$ . If $x_n$ is instead chosen as a sequence of negative terms that tends to $-\infty$ , then $f(x_n) = (k_n')a$ for $n\geq N$ where $k_n'<0$ . Therefore regardless of the sign of $a$ the function $f$ takes on both positive and negative values. It seems redundant and too many steps. Is there a more simple way to solve this problem? Any feedback is appreciated. Thank you!","I need some help with this problem. Here is the link. Can you please tell me if there is an easier way to show that cubic polynomials have a real root? The question is in an analysis book from the continuity section so it has to use that. Here is the latex: Show that a cubic equation (i.e. one of the form where has at least one real root. Solution: The equation has at least one root if for some , and . Then by the intermediate value theorem for some . outgrows smaller powers of so the function is negative for some large negative number and positive for some large positive number. If is a sequence of positive terms that tends to infinity, then Now are sequences that tend to zero, so for any there is an such that and for , we have so that, by the triangle inequality which means Then for some , it can be written and for . Since is a sequence of positive terms, for where . If is instead chosen as a sequence of negative terms that tends to , then for where . Therefore regardless of the sign of the function takes on both positive and negative values. It seems redundant and too many steps. Is there a more simple way to solve this problem? Any feedback is appreciated. Thank you!","ax^3 + bx^2 + cx + d = 0 a\neq 0) x_1<x_2 \enspace f(x_1) < 0 f(x_2) > 0 f(c) = 0 x_1 < c < x_2 x^3 x (x_n) f(x_n) = ax_n^3 + bx_n^2 + cx_n + d = x_n^3(a+ \frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3}) \frac{b}{x_n}, \frac{c}{x_n^2}, \frac{d}{x_n^3} \epsilon N |\frac{b}{x_n}| < \epsilon/3, \quad |\frac{c}{x_n^2}| < \epsilon/3, \quad |\frac{d}{x_n^3}| < \epsilon/3 \epsilon = a |\frac{b}{x_n}| + |\frac{c}{x_n^2}| + |\frac{d}{x_n^3}| < a |\frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3}| \leq |\frac{b}{x_n}| + |\frac{c}{x_n^2}| + |\frac{d}{x_n^3}| < a -a <\frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3} < a |k|<1 a+ \frac{b}{x_n} + \frac{c}{x_n^2} + \frac{d}{x_n^3} = a+ ka = (1+k)a f(x_n) = x_n^3(1+k)a n\geq N x_n f(x_n) = k_na n\geq N k_n>0 x_n -\infty f(x_n) = (k_n')a n\geq N k_n'<0 a f","['analysis', 'polynomials', 'roots', 'solution-verification', 'alternative-proof']"
46,Real Analysis Inf and Sup question,Real Analysis Inf and Sup question,,"I am hung up on this question for real analysis ( intro to anaylsis ). Find $\inf D$ and $\sup D$ $$\mathrm{D}=\left\{\frac{m+n\sqrt{2}}{m+n\sqrt{3}} :m,n\in\Bbb{N}\right\}$$ I have spent enough time staring at this thing that I know the $\sup D=1$ and $\inf D=\frac{\sqrt{2}}{\sqrt{3}}$ . for $\sup D$ : $$m+n\sqrt{2}<m+n\sqrt{3}\implies\frac{m+n\sqrt{2}}{m+n\sqrt{3}}<1$$ so $1$ is an upper bound for $D$ , and then for the confirmation that 1 is the least upper bound I can prove by contradiction that $\sup D$ cannot be less than $1$ , because I could always find a $d \in D$ such that $$\sup D<d<1$$ , which is the contradiction since no $d \in D$ can be greater than $\sup D$ .(proof omited) So my problem is with $\inf D$ . I am having trouble establishing that $\frac{\sqrt{2}}{\sqrt{3}}$ is a lower bound. I am just not seeing it. The intuition is that if $m$ is small and $n$ is large than the fraction $\frac{\sqrt{2}}{\sqrt{3}}$ dominates the expression, however it will always be slightly greater than $\frac{\sqrt{2}}{\sqrt{3}}$ . Analytically I am just not able to show it. Any help would be greatly appreciated","I am hung up on this question for real analysis ( intro to anaylsis ). Find and I have spent enough time staring at this thing that I know the and . for : so is an upper bound for , and then for the confirmation that 1 is the least upper bound I can prove by contradiction that cannot be less than , because I could always find a such that , which is the contradiction since no can be greater than .(proof omited) So my problem is with . I am having trouble establishing that is a lower bound. I am just not seeing it. The intuition is that if is small and is large than the fraction dominates the expression, however it will always be slightly greater than . Analytically I am just not able to show it. Any help would be greatly appreciated","\inf D \sup D \mathrm{D}=\left\{\frac{m+n\sqrt{2}}{m+n\sqrt{3}} :m,n\in\Bbb{N}\right\} \sup D=1 \inf D=\frac{\sqrt{2}}{\sqrt{3}} \sup D m+n\sqrt{2}<m+n\sqrt{3}\implies\frac{m+n\sqrt{2}}{m+n\sqrt{3}}<1 1 D \sup D 1 d \in D \sup D<d<1 d \in D \sup D \inf D \frac{\sqrt{2}}{\sqrt{3}} m n \frac{\sqrt{2}}{\sqrt{3}} \frac{\sqrt{2}}{\sqrt{3}}","['real-analysis', 'analysis', 'supremum-and-infimum']"
47,Why does the lebesgue differentiation theorem not work for arbitrary measures?,Why does the lebesgue differentiation theorem not work for arbitrary measures?,,"I was looking through the proof for the Lebesgue differentiation theorem which is defined only for a lebesgue measure. If it was any other measure then it may not hold. However, going through the proof I didn't find any places that I felt wouldn't also be true for an arbitrary measure. Firstly, you prove the Lebesgue differentiation theorem is true for continuous functions. This seems to be true even for non lebesgue measures because we are just using the uniform continuity property. Secondly, we used the fact that continuous functions are dense in L2 to pick a continuous function close to our arbitrary L2 function for which we want to show the general case of the theorem. Again, i don't see why the Lebesgue measure is any special. Why would this not work for an arbitrary measure?","I was looking through the proof for the Lebesgue differentiation theorem which is defined only for a lebesgue measure. If it was any other measure then it may not hold. However, going through the proof I didn't find any places that I felt wouldn't also be true for an arbitrary measure. Firstly, you prove the Lebesgue differentiation theorem is true for continuous functions. This seems to be true even for non lebesgue measures because we are just using the uniform continuity property. Secondly, we used the fact that continuous functions are dense in L2 to pick a continuous function close to our arbitrary L2 function for which we want to show the general case of the theorem. Again, i don't see why the Lebesgue measure is any special. Why would this not work for an arbitrary measure?",,"['probability', 'analysis', 'measure-theory', 'lebesgue-integral']"
48,Prove that $f$ is bijective.,Prove that  is bijective.,f,"We have that $f:\mathbb{R} \to \mathbb{R}$ is continuous and that $|f(x) - f(y)| \geq |x-y|$ for all $x,y \in \mathbb{R}$ . How do I show that $f$ is bijective? Injective is easy to show because if $f(x) = f(y)$ then $0 \geq |x-y|$ so $x=y$ . How do I show surjective?",We have that is continuous and that for all . How do I show that is bijective? Injective is easy to show because if then so . How do I show surjective?,"f:\mathbb{R} \to \mathbb{R} |f(x) - f(y)| \geq |x-y| x,y \in \mathbb{R} f f(x) = f(y) 0 \geq |x-y| x=y","['analysis', 'functions']"
49,Walter Rudin's mathematical analysis: theorem 2.43. Why proof can't work under the perfect set is uncountable.,Walter Rudin's mathematical analysis: theorem 2.43. Why proof can't work under the perfect set is uncountable.,,"I found several discussions about this theorem, like this one . I understand the proof adopts contradiction by assuming the perfect set $P$ is countable. My question is if the assumption is $P$ is uncountable, the proof seems remains the same, i.e., the $P$ can't be uncountable either. In other words, I think whatever the assumption is, we can draw the contradiction in any way. I don't understand in which way the uncountable condition could solve the contradiction in the proof.","I found several discussions about this theorem, like this one . I understand the proof adopts contradiction by assuming the perfect set is countable. My question is if the assumption is is uncountable, the proof seems remains the same, i.e., the can't be uncountable either. In other words, I think whatever the assumption is, we can draw the contradiction in any way. I don't understand in which way the uncountable condition could solve the contradiction in the proof.",P P P,"['real-analysis', 'analysis']"
50,"If $f:\Bbb{R}^2\to\Bbb{R}^2$ is smooth and the derivative matrix has non-zero determinant everywhere, is the function injective?","If  is smooth and the derivative matrix has non-zero determinant everywhere, is the function injective?",f:\Bbb{R}^2\to\Bbb{R}^2,"Is the following generalization of the Inverse Function Theorem true: Let $f:\Bbb{R^2}\to\Bbb{R^2}$ be a smooth function. If the determinant of the derivative matrix is non-zero everywhere, then the function is globally one-to-one.","Is the following generalization of the Inverse Function Theorem true: Let be a smooth function. If the determinant of the derivative matrix is non-zero everywhere, then the function is globally one-to-one.",f:\Bbb{R^2}\to\Bbb{R^2},"['analysis', 'multivariable-calculus', 'derivatives']"
51,Surface of an open ball,Surface of an open ball,,"In $\mathbb{R}^3$ , Euclidean space. Suppose each point is either blue or red. Let $R>0$ be the largest number such that an open ball $B(x_0,R)$ contains only blue points. Is it true that there is at least one limit point of the red points on the surface of this ball? This is trivial if we are considering real line, but here I'm not so sure. The converse is that for all points on surface we can find an open blue ball centered around it. But there is no way to find an uniform lower bound of the radius of all such balls for all the points--so that I can claim if the statement is not true then I can find $R'>R$ , i.e. blue ball can be enlarged. This difficulty does not exist for 1D since there  are only two boundary points. What can I do?","In , Euclidean space. Suppose each point is either blue or red. Let be the largest number such that an open ball contains only blue points. Is it true that there is at least one limit point of the red points on the surface of this ball? This is trivial if we are considering real line, but here I'm not so sure. The converse is that for all points on surface we can find an open blue ball centered around it. But there is no way to find an uniform lower bound of the radius of all such balls for all the points--so that I can claim if the statement is not true then I can find , i.e. blue ball can be enlarged. This difficulty does not exist for 1D since there  are only two boundary points. What can I do?","\mathbb{R}^3 R>0 B(x_0,R) R'>R","['real-analysis', 'general-topology', 'analysis']"
52,Prove that $\sqrt[8]5 > \sqrt[9]6 > \sqrt[10]7 > \cdots$,Prove that,\sqrt[8]5 > \sqrt[9]6 > \sqrt[10]7 > \cdots,"Prove that $\sqrt[8]5 > \sqrt[9]6 > \sqrt[10]7 > \cdots$ My friend came up with this and gave this to me as a challenge and I'm totally stuck. I have tried proving this by induction $\root{n+3}\of{n} > \root{n+4} \of {n+1} $ for all integers $n \geq 5$ with no luck. I don't even know how to prove the base case without a calculator. Also, it turns out that this is not true for $n \leq 4$ . Why would this inequality only true from $5$ onwards?","Prove that My friend came up with this and gave this to me as a challenge and I'm totally stuck. I have tried proving this by induction for all integers with no luck. I don't even know how to prove the base case without a calculator. Also, it turns out that this is not true for . Why would this inequality only true from onwards?",\sqrt[8]5 > \sqrt[9]6 > \sqrt[10]7 > \cdots \root{n+3}\of{n} > \root{n+4} \of {n+1}  n \geq 5 n \leq 4 5,"['sequences-and-series', 'analysis', 'inequality', 'roots', 'radicals']"
53,Explain carefully why the equation $3x=2$ has no solution in $\mathbb{Z}$.,Explain carefully why the equation  has no solution in .,3x=2 \mathbb{Z},"My proof is below, but I am not sure if this is ""carefully"" enough. I am sure there are many better proofs out there, but this is the one that came to my mind first. Does this work? Let us assume, for the sake of contradiction, that $3x=2$ has a solution in $\mathbb{Z}$ . This would then imply that $x$ is an element in $Z$ , the set of integers. We can calculate $x$ as follows: $3x=2\Rightarrow \frac{3x}{3}=\frac{2}{3}\Rightarrow x=\frac{2}{3}$ . Any integer can be written as the fraction $z=\frac{a}{b}$ where $a$ and $b$ are any real numbers and, importantly, $a=n\cdot b$ where $n\in\mathbb{Z}$ . This means the numerator must be a multiple of the denominator. There are infinitely many different possibilities for $a,b$ for any $z\in\mathbb{Z}$ . However, it is impossible for this to be true for $\frac{2}{3}$ as the numerator must be strictly less than the denominator. Therefore we have reached a contradiction and $3x=2$ can not have a solution in $\mathbb{Z}$ .","My proof is below, but I am not sure if this is ""carefully"" enough. I am sure there are many better proofs out there, but this is the one that came to my mind first. Does this work? Let us assume, for the sake of contradiction, that has a solution in . This would then imply that is an element in , the set of integers. We can calculate as follows: . Any integer can be written as the fraction where and are any real numbers and, importantly, where . This means the numerator must be a multiple of the denominator. There are infinitely many different possibilities for for any . However, it is impossible for this to be true for as the numerator must be strictly less than the denominator. Therefore we have reached a contradiction and can not have a solution in .","3x=2 \mathbb{Z} x Z x 3x=2\Rightarrow \frac{3x}{3}=\frac{2}{3}\Rightarrow x=\frac{2}{3} z=\frac{a}{b} a b a=n\cdot b n\in\mathbb{Z} a,b z\in\mathbb{Z} \frac{2}{3} 3x=2 \mathbb{Z}","['real-analysis', 'analysis']"
54,Does the integrability of $ f^3 $ imply the integrability of $ f^2 $ and/or $ f $?,Does the integrability of  imply the integrability of  and/or ?, f^3   f^2   f ,"We know that the integrability of $ f $ implies the integrability of $ f^2 $, but the integrability of $ f^2 $ does not imply the integrability of $ f $ (for example, the function $ f(x) = 1 $ when rational and $ -1 $ when irrational). Question : However, does the integrability of $ f^3 $ imply anything about the integrability of $ f $? And what about higher powers?","We know that the integrability of $ f $ implies the integrability of $ f^2 $, but the integrability of $ f^2 $ does not imply the integrability of $ f $ (for example, the function $ f(x) = 1 $ when rational and $ -1 $ when irrational). Question : However, does the integrability of $ f^3 $ imply anything about the integrability of $ f $? And what about higher powers?",,"['real-analysis', 'integration', 'analysis']"
55,How to partially differentiate the integral $\int_{0}^{x/\sqrt{t}}\exp(-\xi^2/4)d\xi$ w.r.t $x$ and $t$?,How to partially differentiate the integral  w.r.t  and ?,\int_{0}^{x/\sqrt{t}}\exp(-\xi^2/4)d\xi x t,"How do I partially differentiate the following integral: $$ u(x,t) := \int_{0}^{x/\sqrt{t}}e^{-\xi^2/4} \,\mathrm d \xi $$ I would like to calculate $\frac{\partial u}{\partial x}(x, t)$ and $\frac{\partial u}{\partial t}(x, t)$. I have tried to solve the indefinite integral: $$ \int e^{-\xi^2/4} \,\mathrm d \xi $$ hoping to get a nicer formula, but even the CAS only spit out $$ \int e^{-\xi^2/4} \,\mathrm d \xi = \sqrt{\pi}\operatorname{erf}\left(\frac{\xi}{2}\right)=2\int_{0}^{\frac{\xi}{2}}e^{-\tau^2}\,\mathrm d\tau $$ which is of not much help, since it is basically just a slight rearrangement of the orginal function. I have also tried to apply the fundamental theorem of calculus, by rewriting $$ u(x,t) := \int_{0}^{g(x,t)}h(\xi)d\xi = H(\xi)\Big|_0^{g(x,t)}=H(g(x,t))-H(0) $$ but I'm also stuck here.","How do I partially differentiate the following integral: $$ u(x,t) := \int_{0}^{x/\sqrt{t}}e^{-\xi^2/4} \,\mathrm d \xi $$ I would like to calculate $\frac{\partial u}{\partial x}(x, t)$ and $\frac{\partial u}{\partial t}(x, t)$. I have tried to solve the indefinite integral: $$ \int e^{-\xi^2/4} \,\mathrm d \xi $$ hoping to get a nicer formula, but even the CAS only spit out $$ \int e^{-\xi^2/4} \,\mathrm d \xi = \sqrt{\pi}\operatorname{erf}\left(\frac{\xi}{2}\right)=2\int_{0}^{\frac{\xi}{2}}e^{-\tau^2}\,\mathrm d\tau $$ which is of not much help, since it is basically just a slight rearrangement of the orginal function. I have also tried to apply the fundamental theorem of calculus, by rewriting $$ u(x,t) := \int_{0}^{g(x,t)}h(\xi)d\xi = H(\xi)\Big|_0^{g(x,t)}=H(g(x,t))-H(0) $$ but I'm also stuck here.",,"['integration', 'analysis', 'partial-derivative']"
56,How to prove that $1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-1}(1+\frac1n))...)))=1+\frac1{1!}+\frac1{2!}+\frac1{3!}+...+\frac1{n!}$?,How to prove that ?,1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-1}(1+\frac1n))...)))=1+\frac1{1!}+\frac1{2!}+\frac1{3!}+...+\frac1{n!},"I'm trying to prove that $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-1}(1+\frac1n))...)))=1+\frac1{1!}+\frac1{2!}+\frac1{3!}+...+\frac1{n!}$$ Using induction, suppose that $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{(n-1)}))...)))=1+\frac1{1!}+\frac1{2!}+...+\frac1{(n-1)!}$$ Then $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{n-1}(1+\frac 1n))...)))\\ =1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{n-1}+\frac1{(n-1)n})...)))\\ =(1+\frac1{1!}+\frac1{2!}+...+\frac1{(n-1)!})+\frac1{n!}$$ But I couldn't completely justify the last equality. Could anyone explain this for me, please? Thanks!","I'm trying to prove that $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-1}(1+\frac1n))...)))=1+\frac1{1!}+\frac1{2!}+\frac1{3!}+...+\frac1{n!}$$ Using induction, suppose that $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{(n-1)}))...)))=1+\frac1{1!}+\frac1{2!}+...+\frac1{(n-1)!}$$ Then $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{n-1}(1+\frac 1n))...)))\\ =1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{n-1}+\frac1{(n-1)n})...)))\\ =(1+\frac1{1!}+\frac1{2!}+...+\frac1{(n-1)!})+\frac1{n!}$$ But I couldn't completely justify the last equality. Could anyone explain this for me, please? Thanks!",,"['calculus', 'real-analysis']"
57,"If $f'''(x)$ exists on an interval $[a,b]$, does that mean $f(x)$ is continuous on $[a,b]$?","If  exists on an interval , does that mean  is continuous on ?","f'''(x) [a,b] f(x) [a,b]","Does this follow trivially from the fact that differentiability implies continuity, and if $f'''(x)$ exists, then $f(x)$ is differentiable and therefore continuous?","Does this follow trivially from the fact that differentiability implies continuity, and if $f'''(x)$ exists, then $f(x)$ is differentiable and therefore continuous?",,['analysis']
58,Proving a sequence converges uniformly,Proving a sequence converges uniformly,,"The problem is: Let $f ∈ C(\mathbb R)$. Prove that the sequence $\{f_n\}^∞_{n=1}$ defined by $f_n(x)=\frac{1}{n}\sum^{n-1}_{k=0}f(x+\frac{k}{n})$ converges uniformly on each finite interval $[a, b]$. How would you prove this? I'm not that good at analysis so any hints or advice would be great. I was thinking maybe comparing sequences and using triangle inequality? However, the summation is really throwing me off.","The problem is: Let $f ∈ C(\mathbb R)$. Prove that the sequence $\{f_n\}^∞_{n=1}$ defined by $f_n(x)=\frac{1}{n}\sum^{n-1}_{k=0}f(x+\frac{k}{n})$ converges uniformly on each finite interval $[a, b]$. How would you prove this? I'm not that good at analysis so any hints or advice would be great. I was thinking maybe comparing sequences and using triangle inequality? However, the summation is really throwing me off.",,"['real-analysis', 'sequences-and-series']"
59,About convex hull and closed sets,About convex hull and closed sets,,"Let S be a closed set. Show with an example that $conv(S)$ is not necessarily   closed. Also show that if S is compact then $conv(S)$ is always closed. Here $conv(S)$ denotes the hull of S. Proof: (I didn't show an example I did the proof) Recall that $conv(S)$ is the intersection of all the sets X such that $S\subset X$. w.l.o.g we can suppose that all the sets X such that $S\subset X$ are open and also suppose their intersection is finite. Thus $conv(S)$ can't be closed, is open. Is my proof correct?","Let S be a closed set. Show with an example that $conv(S)$ is not necessarily   closed. Also show that if S is compact then $conv(S)$ is always closed. Here $conv(S)$ denotes the hull of S. Proof: (I didn't show an example I did the proof) Recall that $conv(S)$ is the intersection of all the sets X such that $S\subset X$. w.l.o.g we can suppose that all the sets X such that $S\subset X$ are open and also suppose their intersection is finite. Thus $conv(S)$ can't be closed, is open. Is my proof correct?",,"['analysis', 'convex-analysis', 'convex-optimization', 'linear-programming']"
60,composition of the derivative of Dirac delta with a function,composition of the derivative of Dirac delta with a function,,"I found this question where a nice formula is given for the composition $\delta(f(x))$. Is there a similar general formula for $\delta'(f(x))$? In other words, is there a nice way to express the integral $$ \displaystyle\int_{\mathbb{R}^n} \delta'(f(x))g(x) dx,$$ where $f,g:\mathbb{R}^n\to \mathbb{R}$?","I found this question where a nice formula is given for the composition $\delta(f(x))$. Is there a similar general formula for $\delta'(f(x))$? In other words, is there a nice way to express the integral $$ \displaystyle\int_{\mathbb{R}^n} \delta'(f(x))g(x) dx,$$ where $f,g:\mathbb{R}^n\to \mathbb{R}$?",,"['analysis', 'distribution-theory', 'dirac-delta']"
61,Theorem 6.19 in Baby Rudin: Do we need the continuity of $\varphi$?,Theorem 6.19 in Baby Rudin: Do we need the continuity of ?,\varphi,"Here is Theorem 6.19 (change of variable) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\varphi$ is a strictly increasing continuous function that maps an interval $[ A, B]$ onto $[ a, b]$. Suppose $\alpha$ is monotonically increasing on $[ a, b]$ and $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Define $\beta$ and $g$ on $[ A, B]$ by    $$ \beta(y) = \alpha \left( \varphi(y) \right), \qquad g(y) = f \left( \varphi(y) \right). \tag{36} $$   Then $g \in \mathscr{R}(\beta)$ and    $$ \int_A^B g \ \mathrm{d} \beta =  \int_a^b f \ \mathrm{d} \alpha. \tag{37} $$ And, here is Rudin's proof: To each partition $P = \{ \ x_0, \ldots, x_n \ \}$ of $[a, b]$ corresponds a partition $Q = \{ \ y_0, \ldots, y_n \ \}$ of $[ A, B]$, so that $x_i = \varphi \left( y_i \right)$. All partitions of $[A, B]$ are obtained in this way. Since the values taken by $f$ on $\left[ x_{i-1}, x_i \right]$ are exactly the same as those taken by $g$ on $\left[ y_{i-1}, y_i \right]$, we see that    $$ \tag{38} U(Q, g, \beta) = U(P, f, \alpha), \qquad L(Q, g, \beta) = L(P, f, \alpha). $$    Since $f \in \mathscr{R}(\alpha)$, $P$ can be chosen so that both $U(P, f, \alpha)$ and $L(P, f, \alpha)$ are close to $\int f \ \mathrm{d} \alpha$. Hence (38), combined with Theorem 6.6, shows that $g \in \mathscr{R}(\beta)$ and that (37) holds. This completes the proof. Let us note the following special case: Take $\alpha(x) = x$. Then $\beta = \varphi$. Assume $\varphi^\prime \in \mathscr{R}$ on $[ A, B]$. If Theorem 6.17 is applied to the left side of (37), we obtain    $$ \tag{39} \int_a^b f(x) \ \mathrm{d} x = \int_A^B f \left( \varphi(y) \right) \varphi^\prime(y) \ \mathrm{d} y. $$ Now here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that    $$ \tag{13} U(P, f, \alpha) - L(P, f, \alpha) < \varepsilon. $$ And, here is Theorem 6.17: Assume $\alpha$ increases monotonically and $\alpha^\prime \in \mathscr{R}$ on $[a, b]$. Let $f$ be a bounded real function on $[a, b]$. Then $f \in \mathscr{R}(\alpha)$ if and only if $f \alpha^\prime \in \mathscr{R}$. In that case    $$ \tag{27} \int_a^b f \ \mathrm{d} \alpha = \int_a^b \ f(x) \alpha^\prime(x) \ \mathrm{d} x. $$ Now my question is, is the continuity of $\varphi$ essential in Theorem 6.19 (or its special case)? As far as I can see it, it suffices to just assume that $\varphi$ is a strictly increasing mapping of $[ A, b]$ onto $[a, b]$. And, can we not state Theorem 6.19 as an if-and-only-if-statement, rather than as just an if-statement as it is in its present form? In short, can we not restate Theorem 6.19 as follows? Suppose $\varphi$ is a strictly increasing (not necessarily continuous) function that maps an interval $[ A, B]$ onto $[ a, b]$. Suppose $\alpha$ is monotonically increasing on $[a, b]$, and $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Define $\beta$ and $g$ on $[A, B]$ by    $$ \beta(y) = \alpha \left( \varphi(y) \right), \qquad g(y) = f \left( \varphi(y) \right). $$ Then $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if $g \in \mathscr{R}(\beta)$ on $[ A, B ]$, and in that case    $$ \int_A^B g \ \mathrm{d} \beta = \int_a^b f \ \mathrm{d} \alpha. $$","Here is Theorem 6.19 (change of variable) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\varphi$ is a strictly increasing continuous function that maps an interval $[ A, B]$ onto $[ a, b]$. Suppose $\alpha$ is monotonically increasing on $[ a, b]$ and $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Define $\beta$ and $g$ on $[ A, B]$ by    $$ \beta(y) = \alpha \left( \varphi(y) \right), \qquad g(y) = f \left( \varphi(y) \right). \tag{36} $$   Then $g \in \mathscr{R}(\beta)$ and    $$ \int_A^B g \ \mathrm{d} \beta =  \int_a^b f \ \mathrm{d} \alpha. \tag{37} $$ And, here is Rudin's proof: To each partition $P = \{ \ x_0, \ldots, x_n \ \}$ of $[a, b]$ corresponds a partition $Q = \{ \ y_0, \ldots, y_n \ \}$ of $[ A, B]$, so that $x_i = \varphi \left( y_i \right)$. All partitions of $[A, B]$ are obtained in this way. Since the values taken by $f$ on $\left[ x_{i-1}, x_i \right]$ are exactly the same as those taken by $g$ on $\left[ y_{i-1}, y_i \right]$, we see that    $$ \tag{38} U(Q, g, \beta) = U(P, f, \alpha), \qquad L(Q, g, \beta) = L(P, f, \alpha). $$    Since $f \in \mathscr{R}(\alpha)$, $P$ can be chosen so that both $U(P, f, \alpha)$ and $L(P, f, \alpha)$ are close to $\int f \ \mathrm{d} \alpha$. Hence (38), combined with Theorem 6.6, shows that $g \in \mathscr{R}(\beta)$ and that (37) holds. This completes the proof. Let us note the following special case: Take $\alpha(x) = x$. Then $\beta = \varphi$. Assume $\varphi^\prime \in \mathscr{R}$ on $[ A, B]$. If Theorem 6.17 is applied to the left side of (37), we obtain    $$ \tag{39} \int_a^b f(x) \ \mathrm{d} x = \int_A^B f \left( \varphi(y) \right) \varphi^\prime(y) \ \mathrm{d} y. $$ Now here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that    $$ \tag{13} U(P, f, \alpha) - L(P, f, \alpha) < \varepsilon. $$ And, here is Theorem 6.17: Assume $\alpha$ increases monotonically and $\alpha^\prime \in \mathscr{R}$ on $[a, b]$. Let $f$ be a bounded real function on $[a, b]$. Then $f \in \mathscr{R}(\alpha)$ if and only if $f \alpha^\prime \in \mathscr{R}$. In that case    $$ \tag{27} \int_a^b f \ \mathrm{d} \alpha = \int_a^b \ f(x) \alpha^\prime(x) \ \mathrm{d} x. $$ Now my question is, is the continuity of $\varphi$ essential in Theorem 6.19 (or its special case)? As far as I can see it, it suffices to just assume that $\varphi$ is a strictly increasing mapping of $[ A, b]$ onto $[a, b]$. And, can we not state Theorem 6.19 as an if-and-only-if-statement, rather than as just an if-statement as it is in its present form? In short, can we not restate Theorem 6.19 as follows? Suppose $\varphi$ is a strictly increasing (not necessarily continuous) function that maps an interval $[ A, B]$ onto $[ a, b]$. Suppose $\alpha$ is monotonically increasing on $[a, b]$, and $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Define $\beta$ and $g$ on $[A, B]$ by    $$ \beta(y) = \alpha \left( \varphi(y) \right), \qquad g(y) = f \left( \varphi(y) \right). $$ Then $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if $g \in \mathscr{R}(\beta)$ on $[ A, B ]$, and in that case    $$ \int_A^B g \ \mathrm{d} \beta = \int_a^b f \ \mathrm{d} \alpha. $$",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'riemann-integration']"
62,Theorem 2.14 in Walter Rudin's Principles of Mathematical Analysis,Theorem 2.14 in Walter Rudin's Principles of Mathematical Analysis,,"I've got some difficult for understanding Theorem 2.14 in baby rudin. Theorem 2.14. Let $A$ be the set of all sequences whose elements are the digits 0 and 1. This set $A$ is uncountable. The elements of $A$ are sequences like 1, 0, 0, 1, 0, 1, 1, 1, ... Rudin gave a proof looks correct. But with Theorem 2.12 , I figure out a proof that gets a opposite conclusion. Theorem 2.12. Let $\{E_n\}, n=1, 2, 3,...,$ be a sequence of countable sets, and put $S=\bigcup_{n=1}^{\infty}E_n$ . Then S is countable. Collary. Suppose $A$ is at most countable, and, for every $\alpha\in{A}$ , $B_\alpha$ is at most countable. Put $T=\bigcup_{\alpha\in{A}}B_\alpha$ . Then $T$ is at most countable. And my proof for the set $A$ in Theorem 2.14 is countable: My proof : Let $A_n$ be the set of length $n$ 's sequences whose elements are digits 0 and 1. $A_n$ is at most countable. So, with Theorem 2.12 , $A=\bigcup_{n=1}^\infty{A_n}$ is at most countable. That means the set of all sequences whose elements are the digits 0 and 1 is countable. I know I definitely made a mistake, but where is it?","I've got some difficult for understanding Theorem 2.14 in baby rudin. Theorem 2.14. Let be the set of all sequences whose elements are the digits 0 and 1. This set is uncountable. The elements of are sequences like 1, 0, 0, 1, 0, 1, 1, 1, ... Rudin gave a proof looks correct. But with Theorem 2.12 , I figure out a proof that gets a opposite conclusion. Theorem 2.12. Let be a sequence of countable sets, and put . Then S is countable. Collary. Suppose is at most countable, and, for every , is at most countable. Put . Then is at most countable. And my proof for the set in Theorem 2.14 is countable: My proof : Let be the set of length 's sequences whose elements are digits 0 and 1. is at most countable. So, with Theorem 2.12 , is at most countable. That means the set of all sequences whose elements are the digits 0 and 1 is countable. I know I definitely made a mistake, but where is it?","A A A \{E_n\}, n=1, 2, 3,..., S=\bigcup_{n=1}^{\infty}E_n A \alpha\in{A} B_\alpha T=\bigcup_{\alpha\in{A}}B_\alpha T A A_n n A_n A=\bigcup_{n=1}^\infty{A_n}","['real-analysis', 'analysis']"
63,"If a compact set and a closed set have no intersection, is it true that the distance between the two set always positive?","If a compact set and a closed set have no intersection, is it true that the distance between the two set always positive?",,"Let $A \subseteq \mathbb{R}^n$ be compact, and $B \subseteq \mathbb{R}^n$ be closed. Assume $A \cap B=\emptyset$.Prove that there is a M>0 such that $||\vec{a}-\vec{b}|| \geq c \quad \forall \vec{a} \in A,\vec{b} \in B$. My solution is as follows: Suppose to the contrary that there exist $\vec{c} \in A,\vec{d} \in B$ such that for all c>0 we have $||\vec{c}-\vec{d}||<M$. But this would imply that $\vec{c}=\vec{d}$ which contradicts the fact that  $A \cap B=\emptyset$. I think my proof is wrong because i did not use the fact that A is compact and B is closed. Which step is wrong?","Let $A \subseteq \mathbb{R}^n$ be compact, and $B \subseteq \mathbb{R}^n$ be closed. Assume $A \cap B=\emptyset$.Prove that there is a M>0 such that $||\vec{a}-\vec{b}|| \geq c \quad \forall \vec{a} \in A,\vec{b} \in B$. My solution is as follows: Suppose to the contrary that there exist $\vec{c} \in A,\vec{d} \in B$ such that for all c>0 we have $||\vec{c}-\vec{d}||<M$. But this would imply that $\vec{c}=\vec{d}$ which contradicts the fact that  $A \cap B=\emptyset$. I think my proof is wrong because i did not use the fact that A is compact and B is closed. Which step is wrong?",,"['calculus', 'analysis']"
64,Which point of the sphere $x^2+y^2+z^2=19$ maximize $2x+3y+5z$?,Which point of the sphere  maximize ?,x^2+y^2+z^2=19 2x+3y+5z,Which point of the sphere $x^2+y^2+z^2=19$ maximize $2x+3y+5z$? So I assume that there is a point maximizing $2x+3y+5z$. How can I calculate the exact value of this point?,Which point of the sphere $x^2+y^2+z^2=19$ maximize $2x+3y+5z$? So I assume that there is a point maximizing $2x+3y+5z$. How can I calculate the exact value of this point?,,"['analysis', 'functions', 'multivariable-calculus', 'maxima-minima']"
65,What is the derivative of $g(x)= \int^{x}_{0}e^{\frac{t^2}{2}}dt$?,What is the derivative of ?,g(x)= \int^{x}_{0}e^{\frac{t^2}{2}}dt,"This must be a simple answer, yet I am unsure because of the fact that it has a precise interval. Would the derivative of $g(x)=\int^{x}_{0}e^{\frac{t^2}{2}}dt$ be simply: $$g'(x)= e^{\frac{x^2}{2}}$$ ?","This must be a simple answer, yet I am unsure because of the fact that it has a precise interval. Would the derivative of $g(x)=\int^{x}_{0}e^{\frac{t^2}{2}}dt$ be simply: $$g'(x)= e^{\frac{x^2}{2}}$$ ?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'derivatives']"
66,Axiom of Choice in Mathematical Analysis,Axiom of Choice in Mathematical Analysis,,"I'm an undergraduate student and I'm looking for a book, suitable for self-study, that ""explains"" the applications of the axiom of choice in mathematical analysis. I'm not familiar with the axiom of choice, so I'd prefer a beginner-level book. If anybody could help me with this, I'd be grateful. I really don't know where to look for something like this.","I'm an undergraduate student and I'm looking for a book, suitable for self-study, that ""explains"" the applications of the axiom of choice in mathematical analysis. I'm not familiar with the axiom of choice, so I'd prefer a beginner-level book. If anybody could help me with this, I'd be grateful. I really don't know where to look for something like this.",,"['analysis', 'reference-request', 'book-recommendation', 'axiom-of-choice']"
67,Proving the Cauchy-Schwarz integral inequality in a different way,Proving the Cauchy-Schwarz integral inequality in a different way,,"Suppose $\alpha$ is monotonically increasing on $[a,b]$. Also suppose $f,g: [a,b] \rightarrow \mathbf{R}$, and both functions are Riemann-Stieltjes integrable with respect to $\alpha$. That is, $\forall \epsilon > 0$ there is a partition $P_\epsilon$ such that for all upper and lower sums: \begin{equation*} U(P_\epsilon,f,\alpha) - L(P_\epsilon,f,\alpha) < \epsilon \end{equation*} Use the Cauchy-Schwarz inequality To prove the following: \begin{equation*} \left|\int_a^b f(x)g(x)d\alpha (x)\right|^2 \leq \left(\int_a^b [f(x)]^2 d\alpha(x)\right) \left(\int_a^b [g(x)]^2 d\alpha(x)\right)  \end{equation*} I've seen this proof using done by looking at $0 \leq \int_a^b (\lambda f(x) + g(x))^2 dx$, and then looking at the resulting quadratic equation. This problem, however, seems to be a more general case. How could I approach this?","Suppose $\alpha$ is monotonically increasing on $[a,b]$. Also suppose $f,g: [a,b] \rightarrow \mathbf{R}$, and both functions are Riemann-Stieltjes integrable with respect to $\alpha$. That is, $\forall \epsilon > 0$ there is a partition $P_\epsilon$ such that for all upper and lower sums: \begin{equation*} U(P_\epsilon,f,\alpha) - L(P_\epsilon,f,\alpha) < \epsilon \end{equation*} Use the Cauchy-Schwarz inequality To prove the following: \begin{equation*} \left|\int_a^b f(x)g(x)d\alpha (x)\right|^2 \leq \left(\int_a^b [f(x)]^2 d\alpha(x)\right) \left(\int_a^b [g(x)]^2 d\alpha(x)\right)  \end{equation*} I've seen this proof using done by looking at $0 \leq \int_a^b (\lambda f(x) + g(x))^2 dx$, and then looking at the resulting quadratic equation. This problem, however, seems to be a more general case. How could I approach this?",,"['real-analysis', 'analysis']"
68,Partial sums of $nx^n$,Partial sums of,nx^n,"WolframAlpha claims: $$\sum_{n=0}^m n x^n = \frac{(m x - m - 1) x^{m + 1} + x}{(1 - x)^2} \tag{1}$$ I know that one can differentiate the geometric series to compute $(1)$ when it is a series, i.e. $m=\infty$. However, I'm wondering how the closed form for the partial sum is obtained. Actually, WolframAlpha gives an explicit formula for $$\sum_{n=0}^m n(n-1)(n-2)\cdots (n-k)x^n \tag{2}$$ where $k$ is an integer between $0$ and $n-1$, so it seems that there are some differentiation involved. But already for $k=5$, the formula becomes very messy . My question: How to prove $(1)$ and how to build a similar formula for $(2)$ assuming $k$ is given?","WolframAlpha claims: $$\sum_{n=0}^m n x^n = \frac{(m x - m - 1) x^{m + 1} + x}{(1 - x)^2} \tag{1}$$ I know that one can differentiate the geometric series to compute $(1)$ when it is a series, i.e. $m=\infty$. However, I'm wondering how the closed form for the partial sum is obtained. Actually, WolframAlpha gives an explicit formula for $$\sum_{n=0}^m n(n-1)(n-2)\cdots (n-k)x^n \tag{2}$$ where $k$ is an integer between $0$ and $n-1$, so it seems that there are some differentiation involved. But already for $k=5$, the formula becomes very messy . My question: How to prove $(1)$ and how to build a similar formula for $(2)$ assuming $k$ is given?",,"['calculus', 'real-analysis', 'analysis', 'summation']"
69,\epsilon - packings in compact metric spaces,\epsilon - packings in compact metric spaces,,"Let $(X,d)$ be a compact metric space. Fix some $\epsilon >0$. Then it is clear that any set $S\subset X$ such that for all $x,y \in S$ one has that $d(x,y) > \epsilon$ is finite. In fact, an infinite one would contain a sequence with no convergent subsequence contradicting the compactness of $X$. But now one can ask, whether there is a constant $N$, such that any subset $S$ of $X$ with that property has at most $N$ elements. Unfortunately I have no clue how to prove this or find a counterexample. I'm thinking of the following (somewhat analogous) question where there is no such $N$: Let $X$ be a noetherian scheme. Then its topological space is noetherian meaning that every descending chain of closed subsets of $X$ stabilizes. However this does not imply that dim$(X)$ is finite. (see for example here: https://mathoverflow.net/questions/21067/noetherian-rings-of-infinite-krull-dimension ) So my question is, if there exists such a bound $N$ in general (i.e. for arbitrary compact metric spaces) or if there are nice conditions under which such a bound exist. Moreover I would be interested if there is a way to build up a dimension theory using the minimal bound. (It seems to be similar to the following: https://en.wikipedia.org/wiki/Equilateral_dimension , but without the assumption that all distances coincide. Here one could also ask, whether the best possible choice is always given by equilateral points, I'm also not sure about that.) To finish I want to add that I'm not familiar with the notions of nets, filters and ultrafilters (and so on), so I would appreciate, if a solution would be more elementary if possible (if not, I'm willing to accept that I have to learn about those things first..)","Let $(X,d)$ be a compact metric space. Fix some $\epsilon >0$. Then it is clear that any set $S\subset X$ such that for all $x,y \in S$ one has that $d(x,y) > \epsilon$ is finite. In fact, an infinite one would contain a sequence with no convergent subsequence contradicting the compactness of $X$. But now one can ask, whether there is a constant $N$, such that any subset $S$ of $X$ with that property has at most $N$ elements. Unfortunately I have no clue how to prove this or find a counterexample. I'm thinking of the following (somewhat analogous) question where there is no such $N$: Let $X$ be a noetherian scheme. Then its topological space is noetherian meaning that every descending chain of closed subsets of $X$ stabilizes. However this does not imply that dim$(X)$ is finite. (see for example here: https://mathoverflow.net/questions/21067/noetherian-rings-of-infinite-krull-dimension ) So my question is, if there exists such a bound $N$ in general (i.e. for arbitrary compact metric spaces) or if there are nice conditions under which such a bound exist. Moreover I would be interested if there is a way to build up a dimension theory using the minimal bound. (It seems to be similar to the following: https://en.wikipedia.org/wiki/Equilateral_dimension , but without the assumption that all distances coincide. Here one could also ask, whether the best possible choice is always given by equilateral points, I'm also not sure about that.) To finish I want to add that I'm not familiar with the notions of nets, filters and ultrafilters (and so on), so I would appreciate, if a solution would be more elementary if possible (if not, I'm willing to accept that I have to learn about those things first..)",,"['general-topology', 'analysis']"
70,"Finitely additive ""measure"" that is not upper-continuous","Finitely additive ""measure"" that is not upper-continuous",,"Let $X$ be a set and $\mathcal{A}\subset 2^X$ an algebra of subsets (i.e closed under finite intersection, union and complements) containing $\emptyset$.  Suppose $$\mu: \mathcal{A}\rightarrow [0,1]$$ is finitely additive (for disjoint sets) with $\mu(X)=1$.  Is it possible that for a nested family $A_1 \supset A_2 \supset... $ of elements of $\mathcal{A}$ with $\cap_n A_n =\emptyset$, we have $\lim_n\mu(A_n)\neq 0$? Remark:  I'm pretty sure there is such a measure.  The reason is that the hypothesis of upper-continuity is part of a well-known extension theorem used to extend $\mu$ to the $\sigma$ algebra generated by $\mathcal{A}$.","Let $X$ be a set and $\mathcal{A}\subset 2^X$ an algebra of subsets (i.e closed under finite intersection, union and complements) containing $\emptyset$.  Suppose $$\mu: \mathcal{A}\rightarrow [0,1]$$ is finitely additive (for disjoint sets) with $\mu(X)=1$.  Is it possible that for a nested family $A_1 \supset A_2 \supset... $ of elements of $\mathcal{A}$ with $\cap_n A_n =\emptyset$, we have $\lim_n\mu(A_n)\neq 0$? Remark:  I'm pretty sure there is such a measure.  The reason is that the hypothesis of upper-continuity is part of a well-known extension theorem used to extend $\mu$ to the $\sigma$ algebra generated by $\mathcal{A}$.",,"['analysis', 'measure-theory']"
71,When is it true that $a^{n}<n!$?,When is it true that ?,a^{n}<n!,"How can I  find the smaller $n\in\mathbb{N}$, which makes the equation true: $$a^{n}<n!$$ For example: If $a=2$ then $\longrightarrow 2^{n}<n!$ when $n\geq 4$ If $a=3$ then $\longrightarrow 3^{n}<n!$ when $n\geq 7$ If $a=4$ then $\longrightarrow 4^{n}<n!$ when $n\geq 9$ If $a=5$ then $\longrightarrow 5^{n}<n!$ when $n\geq 12$ $$a,n\in\mathbb{N}$$ $$a^{n}<n!$$when$$n\geq ?$$","How can I  find the smaller $n\in\mathbb{N}$, which makes the equation true: $$a^{n}<n!$$ For example: If $a=2$ then $\longrightarrow 2^{n}<n!$ when $n\geq 4$ If $a=3$ then $\longrightarrow 3^{n}<n!$ when $n\geq 7$ If $a=4$ then $\longrightarrow 4^{n}<n!$ when $n\geq 9$ If $a=5$ then $\longrightarrow 5^{n}<n!$ when $n\geq 12$ $$a,n\in\mathbb{N}$$ $$a^{n}<n!$$when$$n\geq ?$$",,"['combinatorics', 'analysis', 'number-theory', 'discrete-mathematics', 'axioms']"
72,"two metrics on X such that lim d1(xn,x)=0 <=> lim d2(xn,x)=0, does it imply the identity of the two induced topologies?","two metrics on X such that lim d1(xn,x)=0 <=> lim d2(xn,x)=0, does it imply the identity of the two induced topologies?",,"Two metrics $d_1, d_2$ on $X$ For all $x_n, x$ from $X$ it holds: $$\lim d_1(x_n,x)=0 \iff \lim d_2(x_n,x)=0$$ Does it imply that the topology induced by $d_1$ is the same as the topology induced by $d_2$? For example: I have two definition of metric for compact convergence. $X=C(IR,E)$ all continuous functions from $IR$ to $E$, where $E$ is a metric space with metric $q$ $d_1(f,g)=\sum_{i=1}^\infty 2^{-i} * \sup \{q(f(x),g(x)) :x \in [0, i] \} $ $d_2(f,g)=\sum_{i=0}^\infty \min \{2^{-i}, sup(q(f(x),g(x)) : x\in [0, i]) \} $ $\lim d_1(f_n,f)=0 \iff$ For all compact subset $K$ of $X$: $f_n$ converge uniformly to $f$ on $K \iff \lim d_2(f_n,f)=0$. Do these two metrics induce the same topology?","Two metrics $d_1, d_2$ on $X$ For all $x_n, x$ from $X$ it holds: $$\lim d_1(x_n,x)=0 \iff \lim d_2(x_n,x)=0$$ Does it imply that the topology induced by $d_1$ is the same as the topology induced by $d_2$? For example: I have two definition of metric for compact convergence. $X=C(IR,E)$ all continuous functions from $IR$ to $E$, where $E$ is a metric space with metric $q$ $d_1(f,g)=\sum_{i=1}^\infty 2^{-i} * \sup \{q(f(x),g(x)) :x \in [0, i] \} $ $d_2(f,g)=\sum_{i=0}^\infty \min \{2^{-i}, sup(q(f(x),g(x)) : x\in [0, i]) \} $ $\lim d_1(f_n,f)=0 \iff$ For all compact subset $K$ of $X$: $f_n$ converge uniformly to $f$ on $K \iff \lim d_2(f_n,f)=0$. Do these two metrics induce the same topology?",,"['general-topology', 'analysis']"
73,Integrating against compactly supported functions,Integrating against compactly supported functions,,"Let $\Omega$ be an open bounded set in $\mathbb R^n$ and let $f : \Omega \to \mathbb R$. Assume $f$ is continuous and satisfies $$ \int_\Omega f(x)g(x)\; \mathrm{d}x = 0 $$ for any $g$ compactly supported in $\Omega$. Show that $f$ is identically zero in $\Omega$. I think I have a proof by contradiction, but I would like to do better. Suppose $f(x_0)> 0$ for some $x_0 \in \Omega$. Then $f$ is positive in some neighborhood o $\Omega$ Let $g$ be equal to 1 in some compact subset of this neighborhood, so that  $$\int_\Omega f(x)g(x) > 0,$$ a contradiction. My idea was to somehow force $|f(x)| < \varepsilon$ for all $\varepsilon > 0$, but I can't come up with the right estimates.","Let $\Omega$ be an open bounded set in $\mathbb R^n$ and let $f : \Omega \to \mathbb R$. Assume $f$ is continuous and satisfies $$ \int_\Omega f(x)g(x)\; \mathrm{d}x = 0 $$ for any $g$ compactly supported in $\Omega$. Show that $f$ is identically zero in $\Omega$. I think I have a proof by contradiction, but I would like to do better. Suppose $f(x_0)> 0$ for some $x_0 \in \Omega$. Then $f$ is positive in some neighborhood o $\Omega$ Let $g$ be equal to 1 in some compact subset of this neighborhood, so that  $$\int_\Omega f(x)g(x) > 0,$$ a contradiction. My idea was to somehow force $|f(x)| < \varepsilon$ for all $\varepsilon > 0$, but I can't come up with the right estimates.",,"['integration', 'analysis']"
74,Can limit superior and limit inferior exist for an unbounded sequence?,Can limit superior and limit inferior exist for an unbounded sequence?,,Can limit superior and limit inferior exist for an unbounded sequence? If not then why doesn't text books say limit inferior and superior of a bounded sequence instead of limit superior and inferior of a sequence?,Can limit superior and limit inferior exist for an unbounded sequence? If not then why doesn't text books say limit inferior and superior of a bounded sequence instead of limit superior and inferior of a sequence?,,"['real-analysis', 'analysis']"
75,Prove that $x^y < y^x$,Prove that,x^y < y^x,"Assuming that $e<y<x$, prove that $ x^y < y^x$. I think this must be easy, but I can't work it out. Thanks in advance for any kind of help.","Assuming that $e<y<x$, prove that $ x^y < y^x$. I think this must be easy, but I can't work it out. Thanks in advance for any kind of help.",,"['real-analysis', 'analysis', 'derivatives', 'inequality']"
76,Textbook for Vector Calculus,Textbook for Vector Calculus,,"Can anyone recommend a textbook for studying vector calculus (vector analysis) only, that focuses on the theoretical mathematics behind vector calculus? Currently, I am using vector analysis by Snider. I have also taken a look at vector calculus by Marsden. Both of these books skip a large amount of the theory behind what we are doing and why it matters.","Can anyone recommend a textbook for studying vector calculus (vector analysis) only, that focuses on the theoretical mathematics behind vector calculus? Currently, I am using vector analysis by Snider. I have also taken a look at vector calculus by Marsden. Both of these books skip a large amount of the theory behind what we are doing and why it matters.",,"['analysis', 'reference-request', 'vectors', 'vector-analysis']"
77,Prove that :$\frac{1}{100\pi}>\int_{100 \pi}^{200\pi}\frac{\cos(x)}{x}>0$,Prove that :,\frac{1}{100\pi}>\int_{100 \pi}^{200\pi}\frac{\cos(x)}{x}>0,"Using integration by parts, prove that $$0<\int_{100\pi}^{200\pi}\frac{\cos(x)}{x}<\frac{1}{100\pi}.$$ Using integration by parts prove that $\frac{1}{100\pi}>\int_{100 \pi}^{200\pi}\frac{\cos(x)}{x}>0$. Could anyone give me a help with this problem? I have tried using integration by parts but I don't get what integrating by parts achieves. Intuitively I can see why the integral must be greater than 0 as $\cos(100\pi) = \cos(200\pi) = 1$ and $\frac{1}{x}$ is a decreasing function, so the various areas above and below the $x$ axis and $\frac{\cos(x)}{x}$ would cancel to something positive, but this isn't using integration by parts.","Using integration by parts, prove that $$0<\int_{100\pi}^{200\pi}\frac{\cos(x)}{x}<\frac{1}{100\pi}.$$ Using integration by parts prove that $\frac{1}{100\pi}>\int_{100 \pi}^{200\pi}\frac{\cos(x)}{x}>0$. Could anyone give me a help with this problem? I have tried using integration by parts but I don't get what integrating by parts achieves. Intuitively I can see why the integral must be greater than 0 as $\cos(100\pi) = \cos(200\pi) = 1$ and $\frac{1}{x}$ is a decreasing function, so the various areas above and below the $x$ axis and $\frac{\cos(x)}{x}$ would cancel to something positive, but this isn't using integration by parts.",,"['calculus', 'integration', 'analysis', 'definite-integrals']"
78,When does the limit of derivatives coincide with the derivative of the limit function?,When does the limit of derivatives coincide with the derivative of the limit function?,,"Thinking about the (probably) well-known fallacy about approaching a unit square diagonal with staircase functions and thus concluding the diagonal length be $2$ instead of $\sqrt 2$ led me to an interesting question: Given a sequence $(f_k)_{k\in\mathbb N}$ of differentiable functions converging towards a differentiable limit function $f$, when does the limit of derivatives coincide with the derivative of the limit function, that is, when do we have $f'(x)=\lim_{k\to\infty}f_k'(x)$ for all $x$ in the function's domain? And what about second or $n$-th derivatives, supposing all the functions $f_k$ as well as $f$ are twice or $n$ times differentiable? No need to tell me staircase functions aren't differentiable - this is supposed to be a more general question about necessary and sufficient conditions for the limit of $n$-th derivatives to coincide with the $n$-th derivative of the limit.","Thinking about the (probably) well-known fallacy about approaching a unit square diagonal with staircase functions and thus concluding the diagonal length be $2$ instead of $\sqrt 2$ led me to an interesting question: Given a sequence $(f_k)_{k\in\mathbb N}$ of differentiable functions converging towards a differentiable limit function $f$, when does the limit of derivatives coincide with the derivative of the limit function, that is, when do we have $f'(x)=\lim_{k\to\infty}f_k'(x)$ for all $x$ in the function's domain? And what about second or $n$-th derivatives, supposing all the functions $f_k$ as well as $f$ are twice or $n$ times differentiable? No need to tell me staircase functions aren't differentiable - this is supposed to be a more general question about necessary and sufficient conditions for the limit of $n$-th derivatives to coincide with the $n$-th derivative of the limit.",,"['real-analysis', 'analysis', 'derivatives', 'convergence-divergence', 'uniform-convergence']"
79,Quadratic Sieve,Quadratic Sieve,,Can anyone explain how Quadratic Sieve (factorization algorithm) works? I tried reading relevant articles but they didn't include clear explanation / implementation of it.,Can anyone explain how Quadratic Sieve (factorization algorithm) works? I tried reading relevant articles but they didn't include clear explanation / implementation of it.,,"['calculus', 'combinatorics', 'analysis', 'number-theory', 'factoring']"
80,Double integral $\int\int_A y dx dy$,Double integral,\int\int_A y dx dy,"Calculate Double integral $$\iint_A y dxdy$$  where: $$A=\{(x,y)\in\mathbb{R}^2 : x^2+y^2\le4, y \ge 0 \}$$ I do not know what would be the limit of integration if i change this to polar coordinates. I will manage to do the latter part of the question.","Calculate Double integral $$\iint_A y dxdy$$  where: $$A=\{(x,y)\in\mathbb{R}^2 : x^2+y^2\le4, y \ge 0 \}$$ I do not know what would be the limit of integration if i change this to polar coordinates. I will manage to do the latter part of the question.",,"['calculus', 'integration', 'analysis']"
81,Is every compact set in $\mathbb R^2$ a continuous image of some compact set of $\mathbb R$?,Is every compact set in  a continuous image of some compact set of ?,\mathbb R^2 \mathbb R,"Is it true that for every compact subset $A$ of $\mathbb R^2$ , there exist a compact set $B$ in $\mathbb R$ such that there is a continuous surjection from $B$ to $A$ ?","Is it true that for every compact subset $A$ of $\mathbb R^2$ , there exist a compact set $B$ in $\mathbb R$ such that there is a continuous surjection from $B$ to $A$ ?",,"['analysis', 'multivariable-calculus']"
82,"Inverse function of $f(x,y,z) = (xy-z^2, x+z)$?",Inverse function of ?,"f(x,y,z) = (xy-z^2, x+z)","How do you determine the inverse function $f^{-1}: \mathbb{R}^2 \to \mathbb{R}^3$ of $f: \mathbb{R}^3 \to \mathbb{R}^2 , f(x,y,z) = (xy-z^2, x+z) $          ? Or to put it into a bigger context: I have to show that $M := \{(x,y,z) \in \mathbb{R}^3 | xy-z^2 = 1 \text{ and } x+z = 2 \} $ is a submanifold of $\mathbb{R}^3$. The professor's approach is to show that $(1,2)$ is a regular value of f (from above). Because $M = f^{-1}(1,2) $ and because of the Submersion Theorem, $M$ is a submanifold. Now, how does he know that $M = f^{-1}(1,2) $ ? Thanks in advance for your help!","How do you determine the inverse function $f^{-1}: \mathbb{R}^2 \to \mathbb{R}^3$ of $f: \mathbb{R}^3 \to \mathbb{R}^2 , f(x,y,z) = (xy-z^2, x+z) $          ? Or to put it into a bigger context: I have to show that $M := \{(x,y,z) \in \mathbb{R}^3 | xy-z^2 = 1 \text{ and } x+z = 2 \} $ is a submanifold of $\mathbb{R}^3$. The professor's approach is to show that $(1,2)$ is a regular value of f (from above). Because $M = f^{-1}(1,2) $ and because of the Submersion Theorem, $M$ is a submanifold. Now, how does he know that $M = f^{-1}(1,2) $ ? Thanks in advance for your help!",,"['analysis', 'manifolds', 'inverse']"
83,"Uniform convergence on $[0,1]$",Uniform convergence on,"[0,1]","Let $f_n(x)=x^n$. The sequence $\{f_n(x)\}$ converge pointwise but no uniformly on $[0,1]$. Let $g$ be continuous on $[0,1]$ with $g(1)=0$. Prove that the sequence $\{g(x)x^n\}$ converge uniformly on $[0,1]$. My attempt: Since $g$ is continuous on $[0,1]$ it is uniformly continuous on $[0,1]$. So $$\forall \; \epsilon >0, \exists \; \delta(\epsilon): |x-y|<\delta \, \Rightarrow \, |g(x)-g(y)|<\epsilon$$ It is clear that $\lim_{n\rightarrow\infty}g(x)x^n=0$ for all $x \in [0,1]$. Then, i need to prove that, given $\epsilon > 0 $, there is a positive integer $N$ such that as $n \geq N$, then $|g(x)x^n|<\epsilon$. Since $g$ is continuous at $1$, then $$|x-1|<\delta \, \Rightarrow |g(x)-g(1)|=|g(x)|<\epsilon$$ So, we have $$|g(x)x^n|<|g(x)|<\epsilon,$$ but i'm stuck in the case $x \in [0,1)$","Let $f_n(x)=x^n$. The sequence $\{f_n(x)\}$ converge pointwise but no uniformly on $[0,1]$. Let $g$ be continuous on $[0,1]$ with $g(1)=0$. Prove that the sequence $\{g(x)x^n\}$ converge uniformly on $[0,1]$. My attempt: Since $g$ is continuous on $[0,1]$ it is uniformly continuous on $[0,1]$. So $$\forall \; \epsilon >0, \exists \; \delta(\epsilon): |x-y|<\delta \, \Rightarrow \, |g(x)-g(y)|<\epsilon$$ It is clear that $\lim_{n\rightarrow\infty}g(x)x^n=0$ for all $x \in [0,1]$. Then, i need to prove that, given $\epsilon > 0 $, there is a positive integer $N$ such that as $n \geq N$, then $|g(x)x^n|<\epsilon$. Since $g$ is continuous at $1$, then $$|x-1|<\delta \, \Rightarrow |g(x)-g(1)|=|g(x)|<\epsilon$$ So, we have $$|g(x)x^n|<|g(x)|<\epsilon,$$ but i'm stuck in the case $x \in [0,1)$",,"['real-analysis', 'sequences-and-series', 'analysis', 'uniform-convergence']"
84,"If x and y are both greater than or equal to 1, show that $|\sqrt{x}-\sqrt{y}|$ is less than or equal to $0.5| x-y |$","If x and y are both greater than or equal to 1, show that  is less than or equal to",|\sqrt{x}-\sqrt{y}| 0.5| x-y |,"If x and y are both greater than or equal to 1, show that $|\sqrt{x}-\sqrt{y}|$ is less than or equal to $0.5| x-y |$ Would really appreciate any help! Thanks","If x and y are both greater than or equal to 1, show that $|\sqrt{x}-\sqrt{y}|$ is less than or equal to $0.5| x-y |$ Would really appreciate any help! Thanks",,"['analysis', 'inequality', 'radicals']"
85,"Suppose that f is integrable on $[a,b]$. Prove there is a number $x$ in $[a,b]$ such that $\int_a^x f = \int_x^b f$",Suppose that f is integrable on . Prove there is a number  in  such that,"[a,b] x [a,b] \int_a^x f = \int_x^b f","Also, show by example that it is not always possible to choose $x$ in $(a,b)$ I've proven the first part (in the title), but I can't seem to think of a scenario for the second part.  Perhaps my brain is a bit fuzzy at this point in the night, so I apologize in advance if this is an obvious answer, but any help would be appreciated.","Also, show by example that it is not always possible to choose $x$ in $(a,b)$ I've proven the first part (in the title), but I can't seem to think of a scenario for the second part.  Perhaps my brain is a bit fuzzy at this point in the night, so I apologize in advance if this is an obvious answer, but any help would be appreciated.",,"['real-analysis', 'analysis']"
86,Finding the infinite sum of $e^{-n}$ using integrals,Finding the infinite sum of  using integrals,e^{-n},"I am trying to understand this: $\displaystyle \sum_{n=1}^{\infty} e^{-n}$ using integrals, what I have though: $= \displaystyle \lim_{m\to\infty} \sum_{n=1}^{m} e^{-n}$ $= \displaystyle \lim_{m\to\infty} \frac{1}{m}\sum_{n=1}^{m} me^{-n}$ So, suppose this is an right-hand Riemann sum, with $m$ Equal subintervals. $f(x_i) = me^{-n}$ represents the height of the function, we will have the integral for. $\Delta(x) = \frac{1}{m}$ But, How can this be represented as an integral? Thanks!","I am trying to understand this: $\displaystyle \sum_{n=1}^{\infty} e^{-n}$ using integrals, what I have though: $= \displaystyle \lim_{m\to\infty} \sum_{n=1}^{m} e^{-n}$ $= \displaystyle \lim_{m\to\infty} \frac{1}{m}\sum_{n=1}^{m} me^{-n}$ So, suppose this is an right-hand Riemann sum, with $m$ Equal subintervals. $f(x_i) = me^{-n}$ represents the height of the function, we will have the integral for. $\Delta(x) = \frac{1}{m}$ But, How can this be represented as an integral? Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'summation']"
87,How many roots have a complex number with irrational exponent?,How many roots have a complex number with irrational exponent?,,"If a rational exponent on a complex number $z^q$ is the representation of a finite number of roots, then if the exponent is irrational this mean that there are infinite countable roots? If this is the case... the cardinality of the number of roots is the same for any irrational exponent? Thanks in advance. NOTE: there are different questions about irrational exponents but no one answer what Im searching so please dont mark this as repeated.","If a rational exponent on a complex number is the representation of a finite number of roots, then if the exponent is irrational this mean that there are infinite countable roots? If this is the case... the cardinality of the number of roots is the same for any irrational exponent? Thanks in advance. NOTE: there are different questions about irrational exponents but no one answer what Im searching so please dont mark this as repeated.",z^q,"['calculus', 'analysis', 'complex-numbers']"
88,Theorem 1.20 (b) in Baby Rudin: Can the proof of the theorem be improved?,Theorem 1.20 (b) in Baby Rudin: Can the proof of the theorem be improved?,,"I'm reading Principles of Mathematical Analysis , third edition, and am at Theorem 1.20(b), which is as follows: If $x \in \mathbb{R}$ , $y \in \mathbb{R}$ , and $x < y$ , then there exists a $p \in \mathbb{Q}$ such that $x< p < y$ . Now here is the proof given by Rudin: Since $x<y$ , we have $y-x > 0$ , and Theorem 1.20(a) furnishes a positive integer $n$ such that $$n(y-x) > 1.$$ Apply Theorem 1.20(a) again to obtain positive integers $m_1$ and $m_2$ such that $m_1 > nx$ and $m_2 > -nx$ . Then $$-m_2 < nx < m_1. $$ Hence there is an integer $m$ (with $-m_2 \leq m \leq m_1$ )  such that $$ m-1 \leq nx < m.$$ If we combine these inequalities, we obtain $$nx < m \leq 1+nx < ny.$$ Since $n > 0$ , it follows that $$ x < \frac{m}{n} < y.$$ This proves Theorem 1.20(b) with $p = \frac{m}{n}$ . Now I have the following questions: (1) In the above proof, can we not dispense with the integers $m_1$ and $m_2$ ? (2)  How do we know that the integer $m$ satisfies the inequalities $-m_2 \leq m \leq m_1$ ? (3) As Rudin has not mentioned the well-ordering principle, how do we obtain the inequalities $ m-1 \leq nx < m$ ? (4) Is this proof sound enough logically even without using the well-ordering principle?","I'm reading Principles of Mathematical Analysis , third edition, and am at Theorem 1.20(b), which is as follows: If , , and , then there exists a such that . Now here is the proof given by Rudin: Since , we have , and Theorem 1.20(a) furnishes a positive integer such that Apply Theorem 1.20(a) again to obtain positive integers and such that and . Then Hence there is an integer (with )  such that If we combine these inequalities, we obtain Since , it follows that This proves Theorem 1.20(b) with . Now I have the following questions: (1) In the above proof, can we not dispense with the integers and ? (2)  How do we know that the integer satisfies the inequalities ? (3) As Rudin has not mentioned the well-ordering principle, how do we obtain the inequalities ? (4) Is this proof sound enough logically even without using the well-ordering principle?",x \in \mathbb{R} y \in \mathbb{R} x < y p \in \mathbb{Q} x< p < y x<y y-x > 0 n n(y-x) > 1. m_1 m_2 m_1 > nx m_2 > -nx -m_2 < nx < m_1.  m -m_2 \leq m \leq m_1  m-1 \leq nx < m. nx < m \leq 1+nx < ny. n > 0  x < \frac{m}{n} < y. p = \frac{m}{n} m_1 m_2 m -m_2 \leq m \leq m_1  m-1 \leq nx < m,"['real-analysis', 'analysis', 'proof-writing', 'solution-verification']"
89,"Estimate the arc length of the graph of a particular $\mathcal{C}^1$ function from $[0,1]\to [0,1]$.",Estimate the arc length of the graph of a particular  function from .,"\mathcal{C}^1 [0,1]\to [0,1]","Let $f:[0,1]\to[0,1]$ be $\mathcal{C}^1$ such that $f(0) = f(1) = 0$ and $f'$ is nonincreasing ( $f$ concave). Show that the arc length of the graph is smaller than 3. I have a rather geometric proof. For any $P$ be any (finite) partition of $[0,1]$ , then $$\Lambda(P,(x,f(x)))$$ is nothing but the sum of length of finitely many polygon arcs. Using mean value theorem we can show that these arcs are convex. By certain argument we can show that sum of the lengths of these polygon arcs are always smaller than sum of the boundary three sides, which is 3. Since this is true for any $P$ and $$\Gamma((x,f(x)) = \sup_P\Gamma(P,(x,f(x)))\le 3.$$ We are done. I want to know a purely analytic proof, which just estimates $$\Gamma((x,f(x)) = \int_0^1 \sqrt{1+f'(x)^2}dx$$ using the information given.","Let be such that and is nonincreasing ( concave). Show that the arc length of the graph is smaller than 3. I have a rather geometric proof. For any be any (finite) partition of , then is nothing but the sum of length of finitely many polygon arcs. Using mean value theorem we can show that these arcs are convex. By certain argument we can show that sum of the lengths of these polygon arcs are always smaller than sum of the boundary three sides, which is 3. Since this is true for any and We are done. I want to know a purely analytic proof, which just estimates using the information given.","f:[0,1]\to[0,1] \mathcal{C}^1 f(0) = f(1) = 0 f' f P [0,1] \Lambda(P,(x,f(x))) P \Gamma((x,f(x)) = \sup_P\Gamma(P,(x,f(x)))\le 3. \Gamma((x,f(x)) = \int_0^1 \sqrt{1+f'(x)^2}dx","['real-analysis', 'integration', 'analysis']"
90,A set that is a countable intersection of open and dense sets but not open.,A set that is a countable intersection of open and dense sets but not open.,,"We know that Baire Category Theorem implies that in a complete metric space, the countable intersection of open and dense sets is nonempty and actually dense itself. But it is clear that a countable intersection of open sets need not be open. So, can you find an example of a non-open set that is a countable intersection of open and dense sets in a complete metric space?","We know that Baire Category Theorem implies that in a complete metric space, the countable intersection of open and dense sets is nonempty and actually dense itself. But it is clear that a countable intersection of open sets need not be open. So, can you find an example of a non-open set that is a countable intersection of open and dense sets in a complete metric space?",,"['analysis', 'examples-counterexamples', 'baire-category']"
91,"A function continuously differentiable that is bijection from $\mathbb R$ to $ \mathbb R$, but the continuous inverse is not differentiable.","A function continuously differentiable that is bijection from  to , but the continuous inverse is not differentiable.",\mathbb R  \mathbb R,"Does there exist a function $f: \mathbb R \to \mathbb R$ , $f$ is continuously differentiable and bijection and has a inverse function $g: \mathbb R\to \mathbb R$ , but $g$ is not differentiable everywhere? I compare it with the inverse theorem and found that the difference is that the differentiable function of $f$ doesn't have to be invertible, but I am struggling to find such a function.","Does there exist a function , is continuously differentiable and bijection and has a inverse function , but is not differentiable everywhere? I compare it with the inverse theorem and found that the difference is that the differentiable function of doesn't have to be invertible, but I am struggling to find such a function.",f: \mathbb R \to \mathbb R f g: \mathbb R\to \mathbb R g f,"['calculus', 'analysis']"
92,Isometry is not surjective,Isometry is not surjective,,"According to the definition I am using, an isometry is a mapping $f:X \rightarrow Y$ between two metric spaces $(X,d_{X})$ and $(Y,d_{Y})$: $$ d_{Y}(f(a),f(b)) = d_{X}(a,b) $$ for all $a,b \in X $ I managed to prove that it is injective. I let $f(a)=f(b)$ so $d_{Y}(f(a),f(b))=d_{X}(a,b)=0$, which means $a=b$, by one of the axioms of distance. So $f(a)=f(b) \Rightarrow a=b$ and $f$ is injective. Now, I have to prove (or give a counterexample) that $f$ is surjective. I thought of a very simple counterexample, but I am not sure if it is right. Consider $X,Y \subset \mathbb{R}^{2}$ such that  $X=\left\{ (0,0),(0,1),(1/2,\sqrt{3}/2)\right\} $ and $Y=\left\{ (0,0),(0,1),(1/2,\sqrt{3}/2),(10,10)\right\} $. And the usual euclidean distance. I constructed $X$ so that for any two given points, their distance is always the same (=1). $Y$ contains $X$ and also another point that could be pretty much anything. Now I define $f(\textbf{x})=\textbf{x}$. Surely, $f$ is not surjective, but $d(f(a),f(b)) = d(a,b)$ for any $a,b \in X$. Is my reasoning right?","According to the definition I am using, an isometry is a mapping $f:X \rightarrow Y$ between two metric spaces $(X,d_{X})$ and $(Y,d_{Y})$: $$ d_{Y}(f(a),f(b)) = d_{X}(a,b) $$ for all $a,b \in X $ I managed to prove that it is injective. I let $f(a)=f(b)$ so $d_{Y}(f(a),f(b))=d_{X}(a,b)=0$, which means $a=b$, by one of the axioms of distance. So $f(a)=f(b) \Rightarrow a=b$ and $f$ is injective. Now, I have to prove (or give a counterexample) that $f$ is surjective. I thought of a very simple counterexample, but I am not sure if it is right. Consider $X,Y \subset \mathbb{R}^{2}$ such that  $X=\left\{ (0,0),(0,1),(1/2,\sqrt{3}/2)\right\} $ and $Y=\left\{ (0,0),(0,1),(1/2,\sqrt{3}/2),(10,10)\right\} $. And the usual euclidean distance. I constructed $X$ so that for any two given points, their distance is always the same (=1). $Y$ contains $X$ and also another point that could be pretty much anything. Now I define $f(\textbf{x})=\textbf{x}$. Surely, $f$ is not surjective, but $d(f(a),f(b)) = d(a,b)$ for any $a,b \in X$. Is my reasoning right?",,"['analysis', 'metric-spaces', 'proof-verification']"
93,Absolutely convergent series,Absolutely convergent series,,"I am looking for an easy proof of the following theorem ( I know how to prove this by using Banach-Steinhaus), but I guess there must be something much easier: If for every $(t_n)_n$ such that $t_n \rightarrow  0$ the series $\sum_n s_nt_n$ converges, then $\sum_n s_n$ converges absolutely. Probably one needs to look at particular sequences $(t_n)$, but I do not know which one.","I am looking for an easy proof of the following theorem ( I know how to prove this by using Banach-Steinhaus), but I guess there must be something much easier: If for every $(t_n)_n$ such that $t_n \rightarrow  0$ the series $\sum_n s_nt_n$ converges, then $\sum_n s_n$ converges absolutely. Probably one needs to look at particular sequences $(t_n)$, but I do not know which one.",,"['calculus', 'real-analysis']"
94,How to prove that the series $\sum_{n=1}^\infty \frac{(-1)^n}n$ converges,How to prove that the series  converges,\sum_{n=1}^\infty \frac{(-1)^n}n,"I know by definition that a series $$\sum_{n=1}^\infty a_n$$ converges when the sequence of partial sums $S_N=a_1 + a_2 + .. + a_N$ converges to $S$, so $\lim_{N\rightarrow \infty} S_N=S$. So in particular, I'm given the series $$\sum_{n=1}^\infty \frac{(-1)^n}{n},$$ which converges to $\ln 2$. I'm kinda stuck on how to get started. So far I have, $\forall \epsilon >0$, $\exists N>0$ s.t. if $n>N$ then $|a_n-0|<\epsilon$. Is this ok so far? How do I go from here?","I know by definition that a series $$\sum_{n=1}^\infty a_n$$ converges when the sequence of partial sums $S_N=a_1 + a_2 + .. + a_N$ converges to $S$, so $\lim_{N\rightarrow \infty} S_N=S$. So in particular, I'm given the series $$\sum_{n=1}^\infty \frac{(-1)^n}{n},$$ which converges to $\ln 2$. I'm kinda stuck on how to get started. So far I have, $\forall \epsilon >0$, $\exists N>0$ s.t. if $n>N$ then $|a_n-0|<\epsilon$. Is this ok so far? How do I go from here?",,"['real-analysis', 'sequences-and-series', 'analysis']"
95,"Functions that satisfy $f(x,z) = f(x,y) f(y,z)$",Functions that satisfy,"f(x,z) = f(x,y) f(y,z)","I am specifically looking for solutions that are NOT of the form: $f(x,y) = g(x)/g(y)$ since that is an obvious solution, as is $f(x,y) = 0$. I have a suspicion that there may be answers to this question that do not fall into these categories. The functions don't necessarily have to be continuous or differentiable. Edit: $f$ is not necessarily defined on $\mathbb{R}^2$, it can be defined on any space you like. Basically the equation deals with functions of two variables/arguments. Edit #2: Domains are irrelevant to this question.","I am specifically looking for solutions that are NOT of the form: $f(x,y) = g(x)/g(y)$ since that is an obvious solution, as is $f(x,y) = 0$. I have a suspicion that there may be answers to this question that do not fall into these categories. The functions don't necessarily have to be continuous or differentiable. Edit: $f$ is not necessarily defined on $\mathbb{R}^2$, it can be defined on any space you like. Basically the equation deals with functions of two variables/arguments. Edit #2: Domains are irrelevant to this question.",,"['analysis', 'functions']"
96,Show $S(t) =\sum_{n=-\infty}^\infty\sin{(n^2t^2)}e^{-tn^2}$ is $O(t^p)$ at zero,Show  is  at zero,S(t) =\sum_{n=-\infty}^\infty\sin{(n^2t^2)}e^{-tn^2} O(t^p),"An old qualifying exam problem: For $t>0$, define $$S(t) =\sum_{n=-\infty}^\infty\sin{(n^2t^2)}e^{-tn^2}.$$ Show that $S(t) = C t^p + o(t^p)$ as $t\to 0$ . Find $C$ and $p$. There are a couple of solutions here , both of which are fairly complicated. I wondered if anyone had an alternate, perhaps simpler method. (Perhaps the problem does not admit one).","An old qualifying exam problem: For $t>0$, define $$S(t) =\sum_{n=-\infty}^\infty\sin{(n^2t^2)}e^{-tn^2}.$$ Show that $S(t) = C t^p + o(t^p)$ as $t\to 0$ . Find $C$ and $p$. There are a couple of solutions here , both of which are fairly complicated. I wondered if anyone had an alternate, perhaps simpler method. (Perhaps the problem does not admit one).",,"['sequences-and-series', 'analysis', 'asymptotics']"
97,"Let $a_n$ and $b_n$ be sequences of real numbers. If $b_n$ is bounded and $\lim_{n \to \infty} a_n = 0$, then $\lim_{n\to\infty} a_{n}b_{n} = 0$","Let  and  be sequences of real numbers. If  is bounded and , then",a_n b_n b_n \lim_{n \to \infty} a_n = 0 \lim_{n\to\infty} a_{n}b_{n} = 0,"Prove the below statement: Let $a_n$ and $b_n$ be sequences of real numbers. If $b_n$ is bounded and $\lim_{n \to \infty} a_n = 0$, then $\lim_{n \to \infty} a_n b_n=0$ When I read this question, I read that $b_n$ may or may not converge, so taking the example $\dfrac{1}{n}$ and $3n$, $\lim_{n\to\infty} a_nb_n=3\neq0$. What am I getting wrong? It's a theorem from my book and the only hint is that $b_n$ being bounded is crucial. So I know it must be true but I can't understand why. Any suggestions or further hints in the right direction would be greatly appreciated. This is the last problem I have and have been stumped by it all day.","Prove the below statement: Let $a_n$ and $b_n$ be sequences of real numbers. If $b_n$ is bounded and $\lim_{n \to \infty} a_n = 0$, then $\lim_{n \to \infty} a_n b_n=0$ When I read this question, I read that $b_n$ may or may not converge, so taking the example $\dfrac{1}{n}$ and $3n$, $\lim_{n\to\infty} a_nb_n=3\neq0$. What am I getting wrong? It's a theorem from my book and the only hint is that $b_n$ being bounded is crucial. So I know it must be true but I can't understand why. Any suggestions or further hints in the right direction would be greatly appreciated. This is the last problem I have and have been stumped by it all day.",,['analysis']
98,Convergent Sequence and Cauchy Criterion- Counter Example,Convergent Sequence and Cauchy Criterion- Counter Example,,"Consider the sequence $\left \{ x_{n} \right \}$ that satisfies the condition: $$\left | x_{n+1}-x_{n} \right |< \frac{1}{2^{n}} \ \ \ for\  all\  n=1,2,3,...$$ Part (1): Prove that the sequence $\left \{ x_{n} \right \}$ is convergent. Part (2): Does the result in part (1) hold if we only assume that $\left | x_{n+1}-x_{n} \right |< \frac{1}{n} \ \ \ for\  all\  n=1,2,3,...$? For part (1), I proved that the sequence is Cauchy and hence it is convergent. For part (2), I feel like the sequence is not necessarily convergent. I am trying to come up with a sequence that is divergent, but satisfies the condition given in part (2). Any ideas?","Consider the sequence $\left \{ x_{n} \right \}$ that satisfies the condition: $$\left | x_{n+1}-x_{n} \right |< \frac{1}{2^{n}} \ \ \ for\  all\  n=1,2,3,...$$ Part (1): Prove that the sequence $\left \{ x_{n} \right \}$ is convergent. Part (2): Does the result in part (1) hold if we only assume that $\left | x_{n+1}-x_{n} \right |< \frac{1}{n} \ \ \ for\  all\  n=1,2,3,...$? For part (1), I proved that the sequence is Cauchy and hence it is convergent. For part (2), I feel like the sequence is not necessarily convergent. I am trying to come up with a sequence that is divergent, but satisfies the condition given in part (2). Any ideas?",,"['real-analysis', 'sequences-and-series', 'analysis', 'cauchy-sequences']"
99,Smooth partitions of unity,Smooth partitions of unity,,Let $ M $ be a Riemannian manifold and let $ \{U_i\} $ be a countable covering of $ M $. It is well known that there exists a countable collection of smooth function with compact support $ \{\rho_i\} $ (called smooth partition of unity subordinated to $ \{U_i\} $) such that the collection of supports is locally finite and $$ support(\rho_i) \subset U_i $$ $$ \sum_i \rho_i = 1 \;\;\; on \; M $$ $$ 0 \leq \rho_i \leq 1 $$ My question: is it possible to find a partion of unity subordinated to $ \{U_i\} $ with the following ADDITIONAL condition: there exists a constant independent on $ x \in M $ such that $$ \sum_i |\nabla \rho_i(x)| < C \; \; \textrm{for every} \; x \in M \; ? $$ Thanks,Let $ M $ be a Riemannian manifold and let $ \{U_i\} $ be a countable covering of $ M $. It is well known that there exists a countable collection of smooth function with compact support $ \{\rho_i\} $ (called smooth partition of unity subordinated to $ \{U_i\} $) such that the collection of supports is locally finite and $$ support(\rho_i) \subset U_i $$ $$ \sum_i \rho_i = 1 \;\;\; on \; M $$ $$ 0 \leq \rho_i \leq 1 $$ My question: is it possible to find a partion of unity subordinated to $ \{U_i\} $ with the following ADDITIONAL condition: there exists a constant independent on $ x \in M $ such that $$ \sum_i |\nabla \rho_i(x)| < C \; \; \textrm{for every} \; x \in M \; ? $$ Thanks,,['analysis']
