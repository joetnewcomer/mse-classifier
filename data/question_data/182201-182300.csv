,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How do you calculate average circulation across a möbius band?,How do you calculate average circulation across a möbius band?,,"The surface is given by the parameterization shown in the picture. We are to find the average circulation across the mobius band with the circulation vector <-y,x,0> using stokes theorem if necessary. here's a picture of the problem","The surface is given by the parameterization shown in the picture. We are to find the average circulation across the mobius band with the circulation vector <-y,x,0> using stokes theorem if necessary. here's a picture of the problem",,"['calculus', 'multivariable-calculus', 'stokes-theorem', 'mobius-band']"
1,"If $\vec{v}$ is orthogonal to every vector in $\mathbb{R}^3$, then $\vec{v} = \textbf{0}$","If  is orthogonal to every vector in , then",\vec{v} \mathbb{R}^3 \vec{v} = \textbf{0},"I'm preparing for a test and was wondering if someone could validate my solution to the following problem: If $\vec{v} \in \mathbb{R}^3$ is orthogonal to every vector in $\mathbb{R}^3$ , then $\vec{v} = \textbf{0}$ . I took the following approach. $I.$ If $\vec{v}$ is orthogonal to all vectors in $\mathbb{R}^3$ , then it must be orthogonal to itself. Then $\vec{v} \cdot \vec{v}=v_1^2+v_2^2+v_3^2=0$ , where $v_i$ is the $i$ th component of $\vec{v}$ . $II$ . $||\vec{v}||=\sqrt{v_1^2+v_2^2+v_3^2} = \sqrt{\vec{v}\cdot\vec{v}}= \sqrt{0}=0.$ Then $\vec{v}$ has length $0$ . $III$ . The vector of length $0$ is by definition the zero vector. Then $\vec{v}= \textbf{0}$ . As you can see, a pretty simple problem, but with the test ahead I want to leave no place for doubt. Thanks in advance. Oh, and by the way, alternative proofs are appreciated!","I'm preparing for a test and was wondering if someone could validate my solution to the following problem: If is orthogonal to every vector in , then . I took the following approach. If is orthogonal to all vectors in , then it must be orthogonal to itself. Then , where is the th component of . . Then has length . . The vector of length is by definition the zero vector. Then . As you can see, a pretty simple problem, but with the test ahead I want to leave no place for doubt. Thanks in advance. Oh, and by the way, alternative proofs are appreciated!",\vec{v} \in \mathbb{R}^3 \mathbb{R}^3 \vec{v} = \textbf{0} I. \vec{v} \mathbb{R}^3 \vec{v} \cdot \vec{v}=v_1^2+v_2^2+v_3^2=0 v_i i \vec{v} II ||\vec{v}||=\sqrt{v_1^2+v_2^2+v_3^2} = \sqrt{\vec{v}\cdot\vec{v}}= \sqrt{0}=0. \vec{v} 0 III 0 \vec{v}= \textbf{0},"['linear-algebra', 'multivariable-calculus', 'solution-verification']"
2,Determine for which $p>0$ the double integral converges,Determine for which  the double integral converges,p>0,"I would appreciate help solving the double integral below. I have started the task but cannot complete it fully. Let $D=\{(x,y)\in \mathbb(R)^2:x^2+y^2 < 1\}$ . Determine for which $p>0$ the double integral converges: $$\iint_D\frac{1}{(x^4+2x^3y+3x^2y^2+2xy^3+y^4)^p}.$$ My attempt: My idea is to use polar coordinates, so I started by rewriting the expression in the denominator as follows $$(x^4+2x^3y+3x^2y^2+2xy^3+y^4)^p=(x^2+xy+y^2)^{2p}.$$ Then I can put in polar coordinates and get that $(r^2+r^2\cos\theta\sin\theta)^{2p}=(r^2)^{2p}(1+\cos\theta\sin\theta)^{2p}$ which makes it possible to split the integral into the following $$\int_0^{2\pi}\frac{1}{(1+\cos\theta\sin\theta)^{2p}}d\theta\int_0^1\frac{r}{(r^2)^{2p}}dr.$$ For the second integral I carried out the variable substitution $u=r^2$ , $du=2r dr$ and got the value $\frac{1}{2-4p}$ . But it is now when I have to find the value of the first integral that I get stuck. I started by looking at trigonometric rewrites for $1+\cos\theta\sin\theta$ but found nothing that would facilitate the calculations. I also looked at whether you could employ $u=\cos\theta\sin\theta$ , but then $du=\cos^2\theta-\sin^2\theta$ and that would not simplify the calculations either since I then need to divide with the expression. I would be happy to receive tips on how I could solve this task. Maybe there is something obvious that I have missed but I have been looking at this for a while now so any suggestions are warmly welcomed!","I would appreciate help solving the double integral below. I have started the task but cannot complete it fully. Let . Determine for which the double integral converges: My attempt: My idea is to use polar coordinates, so I started by rewriting the expression in the denominator as follows Then I can put in polar coordinates and get that which makes it possible to split the integral into the following For the second integral I carried out the variable substitution , and got the value . But it is now when I have to find the value of the first integral that I get stuck. I started by looking at trigonometric rewrites for but found nothing that would facilitate the calculations. I also looked at whether you could employ , but then and that would not simplify the calculations either since I then need to divide with the expression. I would be happy to receive tips on how I could solve this task. Maybe there is something obvious that I have missed but I have been looking at this for a while now so any suggestions are warmly welcomed!","D=\{(x,y)\in \mathbb(R)^2:x^2+y^2 < 1\} p>0 \iint_D\frac{1}{(x^4+2x^3y+3x^2y^2+2xy^3+y^4)^p}. (x^4+2x^3y+3x^2y^2+2xy^3+y^4)^p=(x^2+xy+y^2)^{2p}. (r^2+r^2\cos\theta\sin\theta)^{2p}=(r^2)^{2p}(1+\cos\theta\sin\theta)^{2p} \int_0^{2\pi}\frac{1}{(1+\cos\theta\sin\theta)^{2p}}d\theta\int_0^1\frac{r}{(r^2)^{2p}}dr. u=r^2 du=2r dr \frac{1}{2-4p} 1+\cos\theta\sin\theta u=\cos\theta\sin\theta du=\cos^2\theta-\sin^2\theta","['multivariable-calculus', 'trigonometric-integrals']"
3,At what point on the paraboloid $y = x^2 + z^2$ is the tangent plane parallel to the plane $x + 2y + 3z = 1$?,At what point on the paraboloid  is the tangent plane parallel to the plane ?,y = x^2 + z^2 x + 2y + 3z = 1,"I am not sure how to proceed. This is what I think: I know that the gradient is perpendicular to the level curve of a function. does that mean that the gradient at a point will be parallel to the tangent plane at that exact point? If so, that would mean that the normal of the plane $x+2y+3z = 1$ is perpendicular to the gradient. and then from that, I just set the dot product of the gradient and the normal vector to 0 to find where it happens. However, I highly doubt my approach and it made me realize that I don't really understand the geometric meaning of the gradient. Thus, can someone please how should i approach this problem intuitively.","I am not sure how to proceed. This is what I think: I know that the gradient is perpendicular to the level curve of a function. does that mean that the gradient at a point will be parallel to the tangent plane at that exact point? If so, that would mean that the normal of the plane is perpendicular to the gradient. and then from that, I just set the dot product of the gradient and the normal vector to 0 to find where it happens. However, I highly doubt my approach and it made me realize that I don't really understand the geometric meaning of the gradient. Thus, can someone please how should i approach this problem intuitively.",x+2y+3z = 1,"['calculus', 'multivariable-calculus', 'vector-analysis', 'tangent-spaces']"
4,change variable integral over a ball,change variable integral over a ball,,"If I have the integral $$ \int_{\partial B(x_0,r)}f(z)\,d\sigma(z) $$ over a open ball of center $x_0$ and radius $r$ , how can 'move' this integral to the open ball $B(x,\tilde{r})$ , where $\tilde{r}>0$ , I mean $$ \int_{\partial B(x_0,r)}f(z)\,d\sigma(z)\to\int_{\partial B(x,\tilde{r})}f(\tilde{z})\,d\sigma(\tilde{z}) $$ My approach was take the change of variable $z\to z-x_0+x$ , then $$ \int_{\partial B(x_0,r)}f(z)\,d\sigma(z)\to\int_{\partial B(x,r)}f(z-x_0+x)\,d\sigma(z) $$ this change the center of the ball, but I don't know this is correct. Any hint will be appreciated. Thanks!","If I have the integral over a open ball of center and radius , how can 'move' this integral to the open ball , where , I mean My approach was take the change of variable , then this change the center of the ball, but I don't know this is correct. Any hint will be appreciated. Thanks!","
\int_{\partial B(x_0,r)}f(z)\,d\sigma(z)
 x_0 r B(x,\tilde{r}) \tilde{r}>0 
\int_{\partial B(x_0,r)}f(z)\,d\sigma(z)\to\int_{\partial B(x,\tilde{r})}f(\tilde{z})\,d\sigma(\tilde{z})
 z\to z-x_0+x 
\int_{\partial B(x_0,r)}f(z)\,d\sigma(z)\to\int_{\partial B(x,r)}f(z-x_0+x)\,d\sigma(z)
","['calculus', 'multivariable-calculus', 'change-of-variable']"
5,how to compute the de Rham cohomology of the punctured plane just by Calculus?,how to compute the de Rham cohomology of the punctured plane just by Calculus?,,"I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\mathbb{R}^2\setminus\{0\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?","I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\mathbb{R}^2\setminus\{0\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?",,"['calculus', 'differential-geometry', 'algebraic-topology', 'homology-cohomology', 'smooth-manifolds']"
6,Circulation and the flux in a field,Circulation and the flux in a field,,"Problem: Find the circulation and flux of the field $F=x^2i+y^2j$ around and across the closed semicircular path that consists of the semicircular arch $r_1(t)=(a\cos(t))i+(a\sin(t))j$ , $0\le t\le\pi$ , followed by the line segment $r_2(t)=ti$ , $-a\le t\le a$ . Then I compute the followings: $r_1(t)=(a\cos(t))i+(a\sin(t))j\implies r_1'(t)=(-a\sin(t))i+(a\cos(t))j$ $F(r_1(t))=(a^2\cos^2t)i+(a^2\sin^2t)j$ $F(r_1(t))\cdot r_1'(t)=-a^3\sin t\cos^2 t+a^3\sin^2t\cos t$ Circulation along $r_1=\int_{-a}^aa^3(\sin^2 t\cos t-\sin t\cos^2t)dt=\frac{2}{3}a^3\sin^3a$ Flux along $r_1=\int_0^{\pi}(a\cos t)(a\cos t)-(a\sin t)(-a\sin t)dt=a^2\pi$ $r_2(t)=ti\implies r_2'(t)=i$ $F(r_2(t))=t^2i$ $F(r_2(t))\cdot r_2'(t)=t^2$ Circulation along $r_2=\int_{-a}^at^2dt=\frac{2}{3}a^3$ Flux along $r_2=\int_{-a}^a(t(0)-(0)(1))dt=0$ Hence the total circulation $=\frac{2}{3}a^3\sin^3a+\frac{2}{3}a^3=\frac{2}{3}a^3(1+\sin^3a)$ , and the total flux $=a^2\pi+0=a^2\pi$ . My questions are: Are my computations valid? (I mean the steps here, not the arithmetics) It seems that the calculation of the flux is independent of the field. Is that normal?","Problem: Find the circulation and flux of the field around and across the closed semicircular path that consists of the semicircular arch , , followed by the line segment , . Then I compute the followings: Circulation along Flux along Circulation along Flux along Hence the total circulation , and the total flux . My questions are: Are my computations valid? (I mean the steps here, not the arithmetics) It seems that the calculation of the flux is independent of the field. Is that normal?",F=x^2i+y^2j r_1(t)=(a\cos(t))i+(a\sin(t))j 0\le t\le\pi r_2(t)=ti -a\le t\le a r_1(t)=(a\cos(t))i+(a\sin(t))j\implies r_1'(t)=(-a\sin(t))i+(a\cos(t))j F(r_1(t))=(a^2\cos^2t)i+(a^2\sin^2t)j F(r_1(t))\cdot r_1'(t)=-a^3\sin t\cos^2 t+a^3\sin^2t\cos t r_1=\int_{-a}^aa^3(\sin^2 t\cos t-\sin t\cos^2t)dt=\frac{2}{3}a^3\sin^3a r_1=\int_0^{\pi}(a\cos t)(a\cos t)-(a\sin t)(-a\sin t)dt=a^2\pi r_2(t)=ti\implies r_2'(t)=i F(r_2(t))=t^2i F(r_2(t))\cdot r_2'(t)=t^2 r_2=\int_{-a}^at^2dt=\frac{2}{3}a^3 r_2=\int_{-a}^a(t(0)-(0)(1))dt=0 =\frac{2}{3}a^3\sin^3a+\frac{2}{3}a^3=\frac{2}{3}a^3(1+\sin^3a) =a^2\pi+0=a^2\pi,['multivariable-calculus']
7,Troubles on the domain of this two variables function.,Troubles on the domain of this two variables function.,,"What's the domain of ? $$f(x, y) = \sqrt{\ln(x) + \ln(y)}$$ So, first of all I thought: I cannot write the argument of the root as $\ln(xy)$ since this propert only applies if $x >0$ and $y >0$ . In writing it as $\ln(xy)$ I then would be allowed to have both $x$ and $y$ negative. Then: surely $x>0$ and $y>0$ is required. But also $\ln(x) > -\ln(y)$ that is $x \geq 1/y$ . Clearly this bonds with the condition $y > 0$ . So am I right in saying $$\Omega = \{(x, y)\in\mathbb{R}^2: x>0, y>0, x\geq 1/y\}$$ or am I missing something?","What's the domain of ? So, first of all I thought: I cannot write the argument of the root as since this propert only applies if and . In writing it as I then would be allowed to have both and negative. Then: surely and is required. But also that is . Clearly this bonds with the condition . So am I right in saying or am I missing something?","f(x, y) = \sqrt{\ln(x) + \ln(y)} \ln(xy) x >0 y >0 \ln(xy) x y x>0 y>0 \ln(x) > -\ln(y) x \geq 1/y y > 0 \Omega = \{(x, y)\in\mathbb{R}^2: x>0, y>0, x\geq 1/y\}","['calculus', 'analysis', 'multivariable-calculus']"
8,Exercise of homogeneous function of two variables,Exercise of homogeneous function of two variables,,"I am trying to solve the following exercise: ""Find all the class 2 functions $v=v(t)$ of the variable $t=x^2+y^2+z^2$ verifying the equation: $$\frac{\partial f}{\partial x}\left(f\frac{\partial v}{\partial x}\right)+\frac{\partial f}{\partial y}\left(f\frac{\partial v}{\partial y}\right)+\frac{\partial f}{\partial z}\left(f\frac{\partial v}{\partial z}\right)=0$$ where $f=f(x,y,z)$ is a differentiable, not identically zero and homogeneous of degree m function. I have applied the product rule, then the chain rule for the composition of functions and in the end Euler theorem for homogeneous functions. Finally I have obtained the equation: $m\frac{\partial v}{\partial t}+2\frac{\partial v^2}{\partial t^2}(x^2+y^2+z^2)=0$ . But the steps in which I have doubt are when I apply the chain rule. Would this be correct?: $$\frac{\partial v}{\partial x}=\frac{\partial v}{\partial t}\frac{\partial t}{\partial x}=\frac{\partial v}{\partial t}2x$$ . Thanks for your help.","I am trying to solve the following exercise: ""Find all the class 2 functions of the variable verifying the equation: where is a differentiable, not identically zero and homogeneous of degree m function. I have applied the product rule, then the chain rule for the composition of functions and in the end Euler theorem for homogeneous functions. Finally I have obtained the equation: . But the steps in which I have doubt are when I apply the chain rule. Would this be correct?: . Thanks for your help.","v=v(t) t=x^2+y^2+z^2 \frac{\partial f}{\partial x}\left(f\frac{\partial v}{\partial x}\right)+\frac{\partial f}{\partial y}\left(f\frac{\partial v}{\partial y}\right)+\frac{\partial f}{\partial z}\left(f\frac{\partial v}{\partial z}\right)=0 f=f(x,y,z) m\frac{\partial v}{\partial t}+2\frac{\partial v^2}{\partial t^2}(x^2+y^2+z^2)=0 \frac{\partial v}{\partial x}=\frac{\partial v}{\partial t}\frac{\partial t}{\partial x}=\frac{\partial v}{\partial t}2x","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
9,"Semantics of the mean-value formulas for Laplace's equation in Evan's Partial Differential Equations: Do we need to have $\overline{B(x,r})\subset U$?",Semantics of the mean-value formulas for Laplace's equation in Evan's Partial Differential Equations: Do we need to have ?,"\overline{B(x,r})\subset U","Let $U \subset \mathbb{R}^n$ be an open set and suppose that $u$ is harmonic on $U$ . The theorem 2 of Evan's Partial Differential Equations 2nd edition makes a connection between the pointwise and average behaviour of harmonic functions on $U$ . In the theorem itself the closure of the ball in question need not be inside of $U$ . Namely, (Mean-value formulas for the Laplace's equation) If $u \in C^2(U)$ is harmonic, then $$u(x) = \frac{\int_{\partial B(x, r)}udS}{|\partial B(x, r)|} = \frac{\int_{B(x, r)}u dy}{| B(x, r)|}$$ for each ball $B(x, r) \subset U$ . What I am wondering is that if $x \in U$ and $r > 0$ such that $B(x, r)\subset U$ , do we then also need to have that $\overline{B(x, r)}\subset U$ for us to be able so use the said formula? I am wondering this since do we not have some general PDE solutions which are harmonic in a punctured space/set, so that you could take a suitable open set whose boundary crosses such a punctured point? Or could we have a harmonic function which is positive on $U$ but vanishes on $\partial U$ ? Consequently, if we are not interested in the surface integral but only on the averages over the balls, is it then okay if $\overline{B(x, r)}\not\subset U$ , or more concisely $\partial B(x, r)\not\subset U$ ?","Let be an open set and suppose that is harmonic on . The theorem 2 of Evan's Partial Differential Equations 2nd edition makes a connection between the pointwise and average behaviour of harmonic functions on . In the theorem itself the closure of the ball in question need not be inside of . Namely, (Mean-value formulas for the Laplace's equation) If is harmonic, then for each ball . What I am wondering is that if and such that , do we then also need to have that for us to be able so use the said formula? I am wondering this since do we not have some general PDE solutions which are harmonic in a punctured space/set, so that you could take a suitable open set whose boundary crosses such a punctured point? Or could we have a harmonic function which is positive on but vanishes on ? Consequently, if we are not interested in the surface integral but only on the averages over the balls, is it then okay if , or more concisely ?","U \subset \mathbb{R}^n u U U U u \in C^2(U) u(x) = \frac{\int_{\partial B(x, r)}udS}{|\partial B(x, r)|} = \frac{\int_{B(x, r)}u dy}{| B(x, r)|} B(x, r) \subset U x \in U r > 0 B(x, r)\subset U \overline{B(x, r)}\subset U U \partial U \overline{B(x, r)}\not\subset U \partial B(x, r)\not\subset U","['multivariable-calculus', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
10,Chain rule in a Hilbert space.,Chain rule in a Hilbert space.,,"Let $F:H\to \mathbb{R}$ be some functional on a Hilbert space $H$ . Denote its Frechet derivative at $h\in H$ as $\frac{\delta F}{\delta h}(h)$ . Suppose $h_t$ is a curve in $H$ i.e $$h_\cdot : \mathbb{R}\to H,~~~~ t\mapsto h_t$$ how do I use the chain rule in this case to show that $$ \frac{\partial}{\partial t}F(h_t)=\Big\langle \frac{\delta F}{\delta h}(h_t) , \partial_t h_t \Big\rangle ~~~~~?$$",Let be some functional on a Hilbert space . Denote its Frechet derivative at as . Suppose is a curve in i.e how do I use the chain rule in this case to show that,"F:H\to \mathbb{R} H h\in H \frac{\delta F}{\delta h}(h) h_t H h_\cdot : \mathbb{R}\to H,~~~~ t\mapsto h_t  \frac{\partial}{\partial t}F(h_t)=\Big\langle \frac{\delta F}{\delta h}(h_t) , \partial_t h_t \Big\rangle ~~~~~?","['calculus', 'multivariable-calculus', 'hilbert-spaces', 'chain-rule', 'frechet-derivative']"
11,For what $m$ does $\iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy$ converge or diverge?,For what  does  converge or diverge?,m \iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy,"For what $m$ does $\iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy$ converge or diverge? Attempt: We write the double integral as $$\iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy$$ We note that out integrand is symmetric, thus $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy=4\int_{0}^{\infty}\int_{0}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy$$ We now establish polar coordinates $x=r\cos{\theta}$ , $y=r\sin{\theta}$ . Then (this is where I'm slighly unsure), we have integration bounds $0\leq\theta\leq2\pi$ , $0\leq r \leq \infty$ . Recalling that the jacobian of this change of variables is simply $r$ , we are now considering the integral $$4\int_{0}^{\infty}\int_{0}^{2\pi} \frac{r}{(1+r^2)^m}d\theta dr=8\pi\int_{0}^{\infty} \frac{r}{(1+r^2)^m} dr$$ This is trivially integrated using $u=1+r^2$ , which yields $$8\pi\int_{0}^{\infty} \frac{r}{(1+r^2)^m} dr=8\pi \frac{(1+r^2)^{1-m}}{1-m} \Big|_0^\infty$$ Then it should stand to reason that the integral diverges for $m\leq1 $ and converges for $m>1$ . There was no answer provided in the exercise, so I used some python code to check, and it appears to converge for $m=0$ , but otherwise seems to agree with my result (however code can be unshaky at times, so I turned to you guys). Input would be much appriciated!","For what does converge or diverge? Attempt: We write the double integral as We note that out integrand is symmetric, thus We now establish polar coordinates , . Then (this is where I'm slighly unsure), we have integration bounds , . Recalling that the jacobian of this change of variables is simply , we are now considering the integral This is trivially integrated using , which yields Then it should stand to reason that the integral diverges for and converges for . There was no answer provided in the exercise, so I used some python code to check, and it appears to converge for , but otherwise seems to agree with my result (however code can be unshaky at times, so I turned to you guys). Input would be much appriciated!",m \iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy \iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy=4\int_{0}^{\infty}\int_{0}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy x=r\cos{\theta} y=r\sin{\theta} 0\leq\theta\leq2\pi 0\leq r \leq \infty r 4\int_{0}^{\infty}\int_{0}^{2\pi} \frac{r}{(1+r^2)^m}d\theta dr=8\pi\int_{0}^{\infty} \frac{r}{(1+r^2)^m} dr u=1+r^2 8\pi\int_{0}^{\infty} \frac{r}{(1+r^2)^m} dr=8\pi \frac{(1+r^2)^{1-m}}{1-m} \Big|_0^\infty m\leq1  m>1 m=0,"['calculus', 'multivariable-calculus', 'convergence-divergence']"
12,what does triple integral represent geometrically?,what does triple integral represent geometrically?,,"If a single integral represents the area under the curve, double integral represents volume under the curve, then what does triple integral represent geometrically?","If a single integral represents the area under the curve, double integral represents volume under the curve, then what does triple integral represent geometrically?",,['multivariable-calculus']
13,Is analytic continuation the same as solving a PDE boundary problem with Cauchy Riemann,Is analytic continuation the same as solving a PDE boundary problem with Cauchy Riemann,,"I was considering the idea of analytic continuation: Let $ U \subseteq \mathbb{R}$ be a subinterval (perhaps all of R). Then suppose we have some smooth function defined $f: U \rightarrow \mathbb{R}$ and we want to ""analytically continue"" this function to the complex plane. We could look at the power series of the $f$ and set up sheafs and start expanding to find an analytic continuation OR... We could ask ""what is a holomorphic function equal to this function on the interval $U$ ?"" I.E. we want find $u(x,y), v(x,y)$ satisfying $$ \frac{ \partial u}{\partial x} = \frac{\partial v}{\partial y} \\ \frac{ \partial u}{\partial y} = - \frac{\partial v}{\partial x}  \\ u(x,0) = f(x), x \in U $$ Would there be a unique such $u,v$ pair? If there is such a unique pair would it then be THE ""analytic continuation"" of $f$ to the complex plane?","I was considering the idea of analytic continuation: Let be a subinterval (perhaps all of R). Then suppose we have some smooth function defined and we want to ""analytically continue"" this function to the complex plane. We could look at the power series of the and set up sheafs and start expanding to find an analytic continuation OR... We could ask ""what is a holomorphic function equal to this function on the interval ?"" I.E. we want find satisfying Would there be a unique such pair? If there is such a unique pair would it then be THE ""analytic continuation"" of to the complex plane?"," U \subseteq \mathbb{R} f: U \rightarrow \mathbb{R} f U u(x,y), v(x,y)  \frac{ \partial u}{\partial x} = \frac{\partial v}{\partial y} \\ \frac{ \partial u}{\partial y} = - \frac{\partial v}{\partial x}  \\ u(x,0) = f(x), x \in U  u,v f","['complex-analysis', 'multivariable-calculus', 'partial-differential-equations', 'divergent-series', 'analytic-continuation']"
14,Double integral objective question.,Double integral objective question.,,"Let $G(t,x):[0,1]\times [0,1]\to\mathbb R$ be defined as $$ G(t,x)=\begin{cases} t(1-x)& \text{if $t\leq x\leq 1$ }\\ x(1-t) & \text{if $x\leq t\leq 1$} \end{cases} $$ For a continuous function $f$ on $[0,1]$ define $$I(f)=\int_0^1\int_0^1G(t,x)f(t)f(x)dtdx$$ Which of the following is true? $1$ . $I(f)>0$ if $f$ is not identically zero. $2.$ $I(f)=0$ for some non-zero $f$ . $3.$ $I(f)<0$ for some non-zero $f$ . $4$ . $I(sin(\pi x))=1.$ It’s single option correct question. How to solve it ? $G(t,x)$ is clearly non negative as given, but value of $f$ is not given so  value of $f(t)f(x)$ in integrand can be negative? If minimum value of $f$ is positive then I can say that Integral will be positive . One way can be think about this question as the given function $G(t,x)$ is green function corresponding to ODE $$y’’=-f(x), y(0)=y(1)=0$$ Please help . Thank you.","Let be defined as For a continuous function on define Which of the following is true? . if is not identically zero. for some non-zero . for some non-zero . . It’s single option correct question. How to solve it ? is clearly non negative as given, but value of is not given so  value of in integrand can be negative? If minimum value of is positive then I can say that Integral will be positive . One way can be think about this question as the given function is green function corresponding to ODE Please help . Thank you.","G(t,x):[0,1]\times [0,1]\to\mathbb R 
G(t,x)=\begin{cases}
t(1-x)& \text{if t\leq x\leq 1 }\\
x(1-t) & \text{if x\leq t\leq 1}
\end{cases}
 f [0,1] I(f)=\int_0^1\int_0^1G(t,x)f(t)f(x)dtdx 1 I(f)>0 f 2. I(f)=0 f 3. I(f)<0 f 4 I(sin(\pi x))=1. G(t,x) f f(t)f(x) f G(t,x) y’’=-f(x), y(0)=y(1)=0","['multivariable-calculus', 'multiple-integral']"
15,Using dot produt or element-wise as multiplication for vectorized multivariables functions in chain rule?,Using dot produt or element-wise as multiplication for vectorized multivariables functions in chain rule?,,"Should we use dot product or Hadamard product (element-wise) for vectorized mutlivariable functions with the chain rule ? I'm struggling to find the correct operation rule between gradient and Jacobian for the chain rule. I have the following expression : for $ x = \left[ \begin{matrix} x_1 \\ x_2 \\ x_3 \\ \end{matrix} \right] \in \mathbf{R}^3 $ , and for $a, b, c \in \mathbf{R}$ , $w = \left[ \begin{matrix} a \\ b \\ c \\ \end{matrix} \right] \in \mathbf{R}^3 $ $$ v_4 = g(w, x) \in \mathbf{R}^3 $$ $$ v_5 = f(w, v_4) \in \mathbf{R}^3 $$ $$ v_6 = L(v_5) \in \mathbf{R} $$ $$ v_7 = x \times v_6 = x \times L(f(w, g(w, x))) \in \mathbf{R}^3 $$ Where I considered vectorized functions defined as following : for example, if $f: (w \in \mathbf{R}^3, x \in \mathbf{R}) \mapsto a \times_{\mathbf{R}} x + b \times_{\mathbf{R}} x^2 + c$ the vectorized function is $(w \in \mathbf{R}^3, x \in \mathbf{R}^3) \mapsto \left[ \begin{matrix} f'(x_1) \\ f'(x_2) \\ f'(x_3) \\ \end{matrix} \right]$ with $f'$ being the scalar function. Then, for the differentiation we can consider the use of the Jacobian (which is then a diagonal matrix). Here : $$ g(w, x) : \mathbf{R}^3 \times \mathbf{R} \rightarrow \mathbf{R} $$ $$ f(w, x) : \mathbf{R}^3 \times \mathbf{R} \rightarrow \mathbf{R} $$ $$ L(x) : \mathbf{R}^3 \rightarrow \mathbf{R} $$ The chains rule give us : $$ \frac{\partial{v_7}}{\partial{a}} = x \left( \frac{\partial{L(v_5)}}{\partial{v_5}} \left( \frac{\partial{f(w, v_4)}}{\partial{v_4}} \frac{\partial{g(w, x)}}{\partial{a}} + \frac{\partial{f(w, v_4)}}{\partial{a}}  \right) \right) $$ which in term of objects give us : $$ \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot \end{matrix} \right] \times_{1} \left( \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot \end{matrix} \right] \times_{2} \left( \left[ \begin{matrix}  \cdot & 0 & 0 \\ 0 & \cdot & 0 \\ 0 & 0 & \cdot \\ \end{matrix} \right] \times \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot  \end{matrix} \right] + \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot  \end{matrix} \right] \right) \right) $$ The question here is : Should I be using the dot product for $\times_{2}$ or matrix multiplication by using the transpose of the $\frac{\partial{L(v_5)}}{\partial{v_5}}$ (or using the row column convention for the gradient which may also impact the form of $\frac{\partial{g(w, x)}}{\partial{a}} = \left[ \begin{matrix} \frac{\partial{g_1(w, x_1)}}{\partial{a}} \\ \frac{\partial{g_2(w, x_2)}}{\partial{a}} \\ \frac{\partial{g_3(w, x_1)}}{\partial{a}} \end{matrix} \right]$ ) or the element-wise product which may result in having $\left[ \begin{matrix} x_0 & x_0 & x_0 \\ x_1 & x_1 & x_1 \\ x_2 & x_2 & x_2 \end{matrix} \right]$ instead of $x \times_{1} ...$ , by defining for any constant $e \in \mathbf{R}$ and vector $v \in \mathbf{R}^3$ , $v \times e = v \times \left[ \begin{matrix} e \\ e \\ e \end{matrix} \right]$ ?","Should we use dot product or Hadamard product (element-wise) for vectorized mutlivariable functions with the chain rule ? I'm struggling to find the correct operation rule between gradient and Jacobian for the chain rule. I have the following expression : for , and for , Where I considered vectorized functions defined as following : for example, if the vectorized function is with being the scalar function. Then, for the differentiation we can consider the use of the Jacobian (which is then a diagonal matrix). Here : The chains rule give us : which in term of objects give us : The question here is : Should I be using the dot product for or matrix multiplication by using the transpose of the (or using the row column convention for the gradient which may also impact the form of ) or the element-wise product which may result in having instead of , by defining for any constant and vector , ?","
x = \left[ \begin{matrix}
x_1 \\
x_2 \\
x_3 \\
\end{matrix} \right] \in \mathbf{R}^3
 a, b, c \in \mathbf{R} w = \left[ \begin{matrix}
a \\
b \\
c \\
\end{matrix} \right] \in \mathbf{R}^3
 
v_4 = g(w, x) \in \mathbf{R}^3
 
v_5 = f(w, v_4) \in \mathbf{R}^3
 
v_6 = L(v_5) \in \mathbf{R}
 
v_7 = x \times v_6 = x \times L(f(w, g(w, x))) \in \mathbf{R}^3
 f: (w \in \mathbf{R}^3, x \in \mathbf{R}) \mapsto a \times_{\mathbf{R}} x + b \times_{\mathbf{R}} x^2 + c (w \in \mathbf{R}^3, x \in \mathbf{R}^3) \mapsto \left[ \begin{matrix}
f'(x_1) \\
f'(x_2) \\
f'(x_3) \\
\end{matrix} \right] f' 
g(w, x) : \mathbf{R}^3 \times \mathbf{R} \rightarrow \mathbf{R}
 
f(w, x) : \mathbf{R}^3 \times \mathbf{R} \rightarrow \mathbf{R}
 
L(x) : \mathbf{R}^3 \rightarrow \mathbf{R}
 
\frac{\partial{v_7}}{\partial{a}} = x \left( \frac{\partial{L(v_5)}}{\partial{v_5}} \left( \frac{\partial{f(w, v_4)}}{\partial{v_4}} \frac{\partial{g(w, x)}}{\partial{a}} + \frac{\partial{f(w, v_4)}}{\partial{a}}  \right) \right)
 
\left[ \begin{matrix} \cdot \\ \cdot \\ \cdot \end{matrix} \right] \times_{1} \left( \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot \end{matrix} \right] \times_{2} \left(
\left[ \begin{matrix} 
\cdot & 0 & 0 \\
0 & \cdot & 0 \\
0 & 0 & \cdot \\
\end{matrix} \right] \times \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot  \end{matrix} \right] + \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot  \end{matrix} \right] \right) \right)
 \times_{2} \frac{\partial{L(v_5)}}{\partial{v_5}} \frac{\partial{g(w, x)}}{\partial{a}} = \left[ \begin{matrix} \frac{\partial{g_1(w, x_1)}}{\partial{a}} \\ \frac{\partial{g_2(w, x_2)}}{\partial{a}} \\ \frac{\partial{g_3(w, x_1)}}{\partial{a}} \end{matrix} \right] \left[ \begin{matrix} x_0 & x_0 & x_0 \\ x_1 & x_1 & x_1 \\ x_2 & x_2 & x_2 \end{matrix} \right] x \times_{1} ... e \in \mathbf{R} v \in \mathbf{R}^3 v \times e = v \times \left[ \begin{matrix} e \\ e \\ e \end{matrix} \right]","['multivariable-calculus', 'matrix-calculus', 'chain-rule', 'jacobian', 'gradient-descent']"
16,understanding surface element in $n \geq 4$ dimensions in proof of Mean value property,understanding surface element in  dimensions in proof of Mean value property,n \geq 4,"I took a basic multivariable calculus class as an undergrad where I saw Green's theorem, Stokes theorem, and Divergence theorem without proof. I know how to use and make sense of them in dimension $n \leq 3$ . I have recently started learning PDEs from Evans and take the 'Guass-Green' theorem for granted since I haven't learnt about differential forms/manifolds yet. $$\int_{U}{\frac{\partial u}{\partial x_i}}dx=\int_{\partial U}u\nu^{i}dS\;\;\;\;(i=1,\ldots,n),$$ I am trying to understand the proof of MVP for a harmonic function $u:U \to \mathbb{R}$ , where $U \subseteq \mathbb{R}^n$ and $u \in C^2(\overline{U})$ which says $$u(x)=\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma $$ for all balls $\overline{B(x,r)} \subseteq U$ where $d\sigma$ is the surface element. The proof begins by fixing $x$ and defining $$ \phi(r):= \def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma$$ and showing $\phi'(r)=0$ to get $$\phi(r)=\phi(0)=u(x)=\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma$$ . My question is what is the expression for $d \sigma$ ? How does it depend on $y$ in the above integrals? I know when using hyperspeherical coordinates, we get $$dy=dy_1dy_2 \ldots dy_n=r^{n-1}\sin^{n-2}(\theta_1)\sin^{n-3}(\theta_2) \ldots \sin^{n-2}(\theta_{n-2})dr d\theta_1d\theta_2 \ldots d\theta_{n-1}=r^{n-1}dr d\sigma$$ How can I write $d \sigma$ explicitly in terms of $y$ and make sure I am using change of variables correctly?","I took a basic multivariable calculus class as an undergrad where I saw Green's theorem, Stokes theorem, and Divergence theorem without proof. I know how to use and make sense of them in dimension . I have recently started learning PDEs from Evans and take the 'Guass-Green' theorem for granted since I haven't learnt about differential forms/manifolds yet. I am trying to understand the proof of MVP for a harmonic function , where and which says for all balls where is the surface element. The proof begins by fixing and defining and showing to get . My question is what is the expression for ? How does it depend on in the above integrals? I know when using hyperspeherical coordinates, we get How can I write explicitly in terms of and make sure I am using change of variables correctly?","n \leq 3 \int_{U}{\frac{\partial u}{\partial x_i}}dx=\int_{\partial U}u\nu^{i}dS\;\;\;\;(i=1,\ldots,n), u:U \to \mathbb{R} U \subseteq \mathbb{R}^n u \in C^2(\overline{U}) u(x)=\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma  \overline{B(x,r)} \subseteq U d\sigma x  \phi(r):= \def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma \phi'(r)=0 \phi(r)=\phi(0)=u(x)=\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma d \sigma y dy=dy_1dy_2 \ldots dy_n=r^{n-1}\sin^{n-2}(\theta_1)\sin^{n-3}(\theta_2) \ldots \sin^{n-2}(\theta_{n-2})dr d\theta_1d\theta_2 \ldots d\theta_{n-1}=r^{n-1}dr d\sigma d \sigma y","['integration', 'measure-theory', 'multivariable-calculus', 'partial-differential-equations', 'harmonic-analysis']"
17,Limit of 2 variable function,Limit of 2 variable function,,"I'm trying to determine the limit of this function: $$\lim_{(x,y)\to (1,2)} \frac{xy^2-4xy-y^2+4x+4y-4}{x^2+y^2-2x-4y+5}$$ I tried to approach in many different ways, such as $$\lim_{t\to 1} f(t,2t) \quad, \quad\lim_{t\to 1} f(t,2) \quad, \quad \lim_{t\to 2} f(1,t) $$ But i got that the limit is 0 for all of them, tried with polar coordinates but it seems hopeless to get the limit! How should I think there? Thanks in advance!","I'm trying to determine the limit of this function: I tried to approach in many different ways, such as But i got that the limit is 0 for all of them, tried with polar coordinates but it seems hopeless to get the limit! How should I think there? Thanks in advance!","\lim_{(x,y)\to (1,2)} \frac{xy^2-4xy-y^2+4x+4y-4}{x^2+y^2-2x-4y+5} \lim_{t\to 1} f(t,2t) \quad, \quad\lim_{t\to 1} f(t,2) \quad, \quad \lim_{t\to 2} f(1,t) ","['calculus', 'limits', 'multivariable-calculus', 'polar-coordinates']"
18,"Express $T$, $N$ and $B$ unitary vectors in terms of $\sigma$ (not necessary an arc length parametrization).","Express ,  and  unitary vectors in terms of  (not necessary an arc length parametrization).",T N B \sigma,"Let $\sigma:[a, b] \to C \subset \mathbb{R}^n$ a parametrization of a curve $C$ . I want to express $T$ , $N$ and $B$ unitary vectors in terms of $\sigma$ . The main problem is that I cannot find a short way to express these unitary vectors. For example: $$T' = \Big(\frac{1}{|| \sigma' ||}\Big) \sigma''- \Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big) \sigma' = \Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x', \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y', \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)$$ and $$|| T' || = \sqrt{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)^2}$$ Then $$N = \frac{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x', \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y', \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)}{\sqrt{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)^2}}$$ Is it a short way to write $N$ and $B$ in terms of $\sigma$ ?","Let a parametrization of a curve . I want to express , and unitary vectors in terms of . The main problem is that I cannot find a short way to express these unitary vectors. For example: and Then Is it a short way to write and in terms of ?","\sigma:[a, b] \to C \subset \mathbb{R}^n C T N B \sigma T' = \Big(\frac{1}{|| \sigma' ||}\Big) \sigma''- \Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big) \sigma' = \Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x', \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y', \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg) || T' || = \sqrt{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)^2} N = \frac{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x', \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y', \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)}{\sqrt{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)^2}} N B \sigma","['calculus', 'multivariable-calculus']"
19,Finding a vector field given its curl.,Finding a vector field given its curl.,,"This is the question I'm working on Find a vector field with twice differentiable components whose curl is $\langle x,y,z \rangle$ or prove that no such field exists. My Solution Let $\textbf{F}=\langle M,N,P \rangle$ be the vector field which we have to find. Given $$ \nabla \times \textbf{F} = \langle x,y,z \rangle $$ Using the determinant expansion of the curl formula, we get $$ \frac{\partial P}{\partial y}-\frac{\partial N}{\partial z} = x \\ \frac{\partial P}{\partial x}-\frac{\partial M}{\partial z} = y \\ \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} = z $$ Differentiating first equation w.r.t $x$ , second w.r.t $y$ and the third w.r.t $z$ , we get $$ P_{xy}-N_{xz}=1 \\ P_{xy}-M_{yz}=1 \\ N_{xz}-M_{yz}=1 \\ $$ Subtracting the second from first, third from second and then adding the first and third, we get $$ M_{zy}-N_{xz}=0 \\ P_{xy}-N_{xz}=0 \\ P_{xy}-M_{yz}=0 \\ $$ Comparing the last two sets of equations yields $1=0$ , a contradiction. Hence the initial assumption that such a vector field exists is false.","This is the question I'm working on Find a vector field with twice differentiable components whose curl is or prove that no such field exists. My Solution Let be the vector field which we have to find. Given Using the determinant expansion of the curl formula, we get Differentiating first equation w.r.t , second w.r.t and the third w.r.t , we get Subtracting the second from first, third from second and then adding the first and third, we get Comparing the last two sets of equations yields , a contradiction. Hence the initial assumption that such a vector field exists is false.","\langle x,y,z \rangle \textbf{F}=\langle M,N,P \rangle 
\nabla \times \textbf{F} = \langle x,y,z \rangle
 
\frac{\partial P}{\partial y}-\frac{\partial N}{\partial z} = x \\
\frac{\partial P}{\partial x}-\frac{\partial M}{\partial z} = y \\
\frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} = z
 x y z 
P_{xy}-N_{xz}=1 \\
P_{xy}-M_{yz}=1 \\
N_{xz}-M_{yz}=1 \\
 
M_{zy}-N_{xz}=0 \\
P_{xy}-N_{xz}=0 \\
P_{xy}-M_{yz}=0 \\
 1=0","['multivariable-calculus', 'solution-verification', 'vector-analysis', 'vector-fields']"
20,"$\int_W x^2y^2 = \int_V x^2y^2$. Is my proof of this equation right? (""Analysis on Manifolds"" by James R. Munkres)",". Is my proof of this equation right? (""Analysis on Manifolds"" by James R. Munkres)",\int_W x^2y^2 = \int_V x^2y^2,"I am reading ""Analysis on Manifolds"" by James R. Munkres. The following example is EXAMPLE 4 on p.149: EXAMPLE 4. Suppose we wish to integrate the same function $x^2y^2$ over the open set $$W=\{(x,y)\mid x^2+y^2<a^2\}.$$ Here the use of polar coordinates is a bit more tricky. The polar coordinate transformation $g$ does not in this case define a diffeomorphism of an open set in the $(r,\theta)$ plane with $W$ .  However, $g$ does define a diffeomorphism of the open set $U=(0,a)\times (0,2\pi)$ with the open set $$V=\{(x,y)\mid x^2+y^2<a^2\text{ and }x<0\text{   if   }y=0\}$$ of $\mathbb{R}^2$ . See Figure 17.5; the set $V$ consists of $W$ with the non-negative $x$ -axis deleted. Because the non-negative $x$ -axis has measure zero, $$\int_W x^2y^2=\int_V x^2y^2.$$ The latter can be expressed as an integral over $U$ , by use of the polar coordinate transformation. Is my proof of the following equality right? $\int_W x^2y^2=\int_V x^2y^2.$ Theorem 11.3. Let $Q$ be a rectangle in $\mathbb{R}^n$ ; let $f:Q\to\mathbb{R}$ ; assume $f$ is integrable over $Q$ . (a) If $f$ vanishes except on a set of measure zero, then $\int_Q f=0$ . Theorem 13.3 (Properties of the integral). (d) (Additivity). If $S=S_1\cup S_2$ and $f$ is integrable over $S_1$ and $S_2$ , then $f$ is integrable over $S$ and $S_1\cap S_2$ ; furthermore $$\int_S f=\int_{S_1} f+\int_{S_2} f-\int_{S_1\cap S_2} f.$$ My proof is here: Let $T:=\{(x,y)\mid 0\leq x<a\text{ and }y=0\}$ . Then, $W=V\cup T$ and $V\cap T=\emptyset.$ So, by Theorem 13.3(d), $\int_W x^2y^2 = \int_V x^2y^2 + \int_T x^2y^2 - \int_{V\cap T} x^2y^2$ . Since $T$ and $V\cap T$ have measure zero, $\int_T x^2y^2=0$ and $\int_{V\cap T} x^2y^2=0$ by Theorem 11.3. So, $\int_W x^2y^2 = \int_V x^2y^2$ .","I am reading ""Analysis on Manifolds"" by James R. Munkres. The following example is EXAMPLE 4 on p.149: EXAMPLE 4. Suppose we wish to integrate the same function over the open set Here the use of polar coordinates is a bit more tricky. The polar coordinate transformation does not in this case define a diffeomorphism of an open set in the plane with .  However, does define a diffeomorphism of the open set with the open set of . See Figure 17.5; the set consists of with the non-negative -axis deleted. Because the non-negative -axis has measure zero, The latter can be expressed as an integral over , by use of the polar coordinate transformation. Is my proof of the following equality right? Theorem 11.3. Let be a rectangle in ; let ; assume is integrable over . (a) If vanishes except on a set of measure zero, then . Theorem 13.3 (Properties of the integral). (d) (Additivity). If and is integrable over and , then is integrable over and ; furthermore My proof is here: Let . Then, and So, by Theorem 13.3(d), . Since and have measure zero, and by Theorem 11.3. So, .","x^2y^2 W=\{(x,y)\mid x^2+y^2<a^2\}. g (r,\theta) W g U=(0,a)\times (0,2\pi) V=\{(x,y)\mid x^2+y^2<a^2\text{ and }x<0\text{   if   }y=0\} \mathbb{R}^2 V W x x \int_W x^2y^2=\int_V x^2y^2. U \int_W x^2y^2=\int_V x^2y^2. Q \mathbb{R}^n f:Q\to\mathbb{R} f Q f \int_Q f=0 S=S_1\cup S_2 f S_1 S_2 f S S_1\cap S_2 \int_S f=\int_{S_1} f+\int_{S_2} f-\int_{S_1\cap S_2} f. T:=\{(x,y)\mid 0\leq x<a\text{ and }y=0\} W=V\cup T V\cap T=\emptyset. \int_W x^2y^2 = \int_V x^2y^2 + \int_T x^2y^2 - \int_{V\cap T} x^2y^2 T V\cap T \int_T x^2y^2=0 \int_{V\cap T} x^2y^2=0 \int_W x^2y^2 = \int_V x^2y^2","['integration', 'measure-theory', 'multivariable-calculus', 'solution-verification']"
21,Finding the equation of a plane given three points,Finding the equation of a plane given three points,,"Below is a problem I did from a Calculus text book. My answer matches the back of the book and I believe my answer is right. However, the method I used is something I made up. That is, it is not the method described in the text book. Is my method correct? Problem: Find the plane through the points $(1,1,-1)$ , $(2,0,2)$ and $(0,-2,1)$ . Answer: The general form of a plane is: $$ Ax + By + Cz = D$$ Sometimes the following constrain is added: $$ A^2 + B^2 + C^2 = 1$$ By inspection, we can see this plane is not parallel to the x-axis, the y-axis or the z-axis. Hence, we can assume that the plane is of the form: $$ Ax + By + Cz = 1 $$ Now we setup the following system of linear equations. \begin{align*} A + B - C &= 1 \\ 2A + 2C &= 1 \\ -2B + C &= 1 \\ \end{align*} To solve this system of equations, we get rid of $A$ and $B$ in the first equation. \begin{align*} 2A &= 1 - 2C \\ A &= \frac{ 1 - 2C }{2} \\ -2B &= 1 - C \\ B &= \frac{ C - 1 }{2} \\ \left( \frac{ 1 - 2C }{2} \right) + \left( \frac{ C - 1 }{2} \right)  - C &= 1 \\ 1 - 2C + C - 1 - 2C &= 2 \\ - 2C + C - 2C &= 2 \\ -3C &= 2 \\ C &= -\frac{2}{3} \\ B &= \frac{ -\frac{2}{3} - 1 }{2} = -\frac{2}{6} - \frac{1}{2} \\ B &= -\frac{5}{6} \\ A &= \frac{ 1 - 2\left(  -\frac{2}{3} \right)  }{2} = \dfrac{1 + \dfrac{4}{3} }{2} \\ A &= \dfrac{7}{6} \end{align*} Hence the equation is: $$ \left( \dfrac{7}{6} \right) A + \left( -\frac{5}{6} \right) B + \left(  -\frac{2}{3} \right) C = 1  $$ Clearing the fraction, we get the final answer of: $$ 7A - 5B - 4C = 6 $$ As pointed out by Paul, the correct answer is: $$ 7x - 5y - 4z = 6 $$","Below is a problem I did from a Calculus text book. My answer matches the back of the book and I believe my answer is right. However, the method I used is something I made up. That is, it is not the method described in the text book. Is my method correct? Problem: Find the plane through the points , and . Answer: The general form of a plane is: Sometimes the following constrain is added: By inspection, we can see this plane is not parallel to the x-axis, the y-axis or the z-axis. Hence, we can assume that the plane is of the form: Now we setup the following system of linear equations. To solve this system of equations, we get rid of and in the first equation. Hence the equation is: Clearing the fraction, we get the final answer of: As pointed out by Paul, the correct answer is:","(1,1,-1) (2,0,2) (0,-2,1)  Ax + By + Cz = D  A^2 + B^2 + C^2 = 1  Ax + By + Cz = 1  \begin{align*}
A + B - C &= 1 \\
2A + 2C &= 1 \\
-2B + C &= 1 \\
\end{align*} A B \begin{align*}
2A &= 1 - 2C \\
A &= \frac{ 1 - 2C }{2} \\
-2B &= 1 - C \\
B &= \frac{ C - 1 }{2} \\
\left( \frac{ 1 - 2C }{2} \right) + \left( \frac{ C - 1 }{2} \right)  - C &= 1 \\
1 - 2C + C - 1 - 2C &= 2 \\
- 2C + C - 2C &= 2 \\
-3C &= 2 \\
C &= -\frac{2}{3} \\
B &= \frac{ -\frac{2}{3} - 1 }{2} = -\frac{2}{6} - \frac{1}{2} \\
B &= -\frac{5}{6} \\
A &= \frac{ 1 - 2\left(  -\frac{2}{3} \right)  }{2} = \dfrac{1 + \dfrac{4}{3} }{2} \\
A &= \dfrac{7}{6}
\end{align*}  \left( \dfrac{7}{6} \right) A + \left( -\frac{5}{6} \right) B + \left(  -\frac{2}{3} \right) C = 1    7A - 5B - 4C = 6   7x - 5y - 4z = 6 ","['geometry', 'multivariable-calculus', 'solution-verification']"
22,Electric potential due to dipole layer,Electric potential due to dipole layer,,"In Classical Electrodynamics , Jackson derives the electric potential for a surface with a dipole charge. Here is his derivation. I will omit constants for brevity. Letting $D(\textbf{x}) := \lim_{d(\textbf{x}) \to 0} \sigma(\textbf{x}) d(\textbf{x})$ where $d(\textbf{x})$ is the local separation of $S$ and $S'$ with $S$ having charge density $\sigma(x)$ and $S'$ having equal and opposite charge density. The potential due to the two surfaces is: $$ \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \frac{\sigma(\textbf{x'})}{|\textbf{x} - \textbf{x}' + \textbf{n}d|} da'' \tag{1}$$ where $\textbf{n}$ is the unit normal to the surface $S$ pointing away from $S'$ . He uses a Taylor expansion $$ \frac{1}{|\textbf{x} + \textbf{a}|} = \frac{1}{x} + \textbf{a} \cdot \nabla \Big( \frac{1}{x} \Big) \tag{2}$$ He says this is valid when $|\textbf{a}| \ll |\textbf{x}|$ (and I assume $x := |\textbf{x}|$ ). Then as $d \to 0$ (and I believe he redefines $\textbf{x} := \textbf{x} - \textbf{x}'$ and $\textbf{a} := \textbf{n}d$ ) he arrives at $$ \phi(\textbf{x}) = \int_S D(\textbf{x}') \textbf{n} \cdot \nabla'\Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) da' \tag{3}$$ and since $\textbf{p} = \textbf{n}\ D\ da'$ then the potential at $\textbf{x}$ caused by a dipole at $\textbf{x}'$ is $$ \phi(\textbf{x}) = \frac{\textbf{p} \cdot (\textbf{x} - \textbf{x}')}{|\textbf{x} - \textbf{x}'|^3} \tag{4}$$ There are many steps I don't understand: From (1) Jackson used $\sigma(\textbf{x}')$ at $S$ and $- \sigma(\textbf{x}')$ at $S'$ . But, if $\textbf{x}'$ traces out $S$ wouldn't this be starting with the assumption that $S$ and $S'$ are the same surface? Why is $|\textbf{a}| \ll |\textbf{x}|$ a necessary assumption to use the Taylor expansion? The 1D case would be analogous to expanding the function $1/(x+a)$ and I do not see a reason that $a \ll x$ is necessary to do this. After substituting the Taylor expansion into (1) (and using $\textbf{x} := \textbf{x} - \textbf{x}'$ and $\textbf{a} := \textbf{n}d$ ) we get $$ \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \sigma(\textbf{x'}) \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} + \textbf{n}d \cdot \nabla \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) \Big) da''$$ $$ \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \sigma(\textbf{x'})  \frac{1}{|\textbf{x} - \textbf{x}'|} da''  - \int_{S'} \sigma(\textbf{x'}) \textbf{n}d \cdot \nabla \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) da''$$ which somehow reduces to (3). It seems like Jackson cancelled the first two terms but how is this valid when we are integrating over $S$ in one and $S'$ in the other? Also, it seems like Jackson is missing a negative sign from the third term above. Also, the third term above differs from (3) in that he switches from integrating over $S'$ to $S$ . Is his change justified because after we do the limiting process the two surfaces coincide, allowing using swap? EDIT: I just realized these issues (besides the missing negative sign) can be resolved by doing the limiting process first and then expanding the integral into two terms. But this just begs the question; how do we know which order to do these steps when modeling with differentials and limiting processes? Should $\textbf{n}d$ be $\textbf{n}d(\textbf{x}')$ ? I just don't see the jump from (3) to (4) equation.","In Classical Electrodynamics , Jackson derives the electric potential for a surface with a dipole charge. Here is his derivation. I will omit constants for brevity. Letting where is the local separation of and with having charge density and having equal and opposite charge density. The potential due to the two surfaces is: where is the unit normal to the surface pointing away from . He uses a Taylor expansion He says this is valid when (and I assume ). Then as (and I believe he redefines and ) he arrives at and since then the potential at caused by a dipole at is There are many steps I don't understand: From (1) Jackson used at and at . But, if traces out wouldn't this be starting with the assumption that and are the same surface? Why is a necessary assumption to use the Taylor expansion? The 1D case would be analogous to expanding the function and I do not see a reason that is necessary to do this. After substituting the Taylor expansion into (1) (and using and ) we get which somehow reduces to (3). It seems like Jackson cancelled the first two terms but how is this valid when we are integrating over in one and in the other? Also, it seems like Jackson is missing a negative sign from the third term above. Also, the third term above differs from (3) in that he switches from integrating over to . Is his change justified because after we do the limiting process the two surfaces coincide, allowing using swap? EDIT: I just realized these issues (besides the missing negative sign) can be resolved by doing the limiting process first and then expanding the integral into two terms. But this just begs the question; how do we know which order to do these steps when modeling with differentials and limiting processes? Should be ? I just don't see the jump from (3) to (4) equation.","D(\textbf{x}) := \lim_{d(\textbf{x}) \to 0} \sigma(\textbf{x}) d(\textbf{x}) d(\textbf{x}) S S' S \sigma(x) S' 
\phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \frac{\sigma(\textbf{x'})}{|\textbf{x} - \textbf{x}' + \textbf{n}d|} da''
\tag{1} \textbf{n} S S'  \frac{1}{|\textbf{x} + \textbf{a}|} = \frac{1}{x} + \textbf{a} \cdot \nabla \Big( \frac{1}{x} \Big) \tag{2} |\textbf{a}| \ll |\textbf{x}| x := |\textbf{x}| d \to 0 \textbf{x} := \textbf{x} - \textbf{x}' \textbf{a} := \textbf{n}d  \phi(\textbf{x}) = \int_S D(\textbf{x}') \textbf{n} \cdot \nabla'\Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) da' \tag{3} \textbf{p} = \textbf{n}\ D\ da' \textbf{x} \textbf{x}'  \phi(\textbf{x}) = \frac{\textbf{p} \cdot (\textbf{x} - \textbf{x}')}{|\textbf{x} - \textbf{x}'|^3} \tag{4} \sigma(\textbf{x}') S - \sigma(\textbf{x}') S' \textbf{x}' S S S' |\textbf{a}| \ll |\textbf{x}| 1/(x+a) a \ll x \textbf{x} := \textbf{x} - \textbf{x}' \textbf{a} := \textbf{n}d  \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \sigma(\textbf{x'}) \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} + \textbf{n}d \cdot \nabla \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) \Big) da''  \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \sigma(\textbf{x'})  \frac{1}{|\textbf{x} - \textbf{x}'|} da''  - \int_{S'} \sigma(\textbf{x'}) \textbf{n}d \cdot \nabla \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) da'' S S' S' S \textbf{n}d \textbf{n}d(\textbf{x}')","['limits', 'multivariable-calculus', 'taylor-expansion', 'vector-analysis', 'electromagnetism']"
23,"Show that $\int_A f=\underline{\int_Q} (f_{+})_A - \underline{\int_Q} (f_{-})_A.$ Is my solution ok or not? (in ""Analysis on Manifolds"" by Munkres)","Show that  Is my solution ok or not? (in ""Analysis on Manifolds"" by Munkres)",\int_A f=\underline{\int_Q} (f_{+})_A - \underline{\int_Q} (f_{-})_A.,"I am reading ""Analysis on Manifolds"" by James R. Munkres. The following exercise is Exercise 7 on p.133 in Section 15 in this book: Exercise 7. Let $A$ be a bounded open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a bounded continuous function. Let $Q$ be a rectangle containing $A$ . Show that $$\int_A f=\underline{\int_Q} (f_{+})_A - \underline{\int_Q} (f_{-})_A.$$ I solved this exercise, but I am not sure my solution is ok or not. The following is the definition of the extended integral of $f$ over $A$ . Definition. Let $A$ be an open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a continuous function. If $f$ is non-negative on $A$ , we define the (extended) integral of $f$ over $A$ , denoted $\int_A f$ , to be the supremum of the numbers $\int_D f$ , as $D$ ranges over all compact rectifiable subsets of $A$ , provided this supremum exists. In this case, we say that $f$ is integrable over $A$ (in the extended sense). More generally, if $f$ is an arbitrary continuous function on $A$ , set $$f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}.$$ We say that $f$ is integrable over $A$ (in the extended sense) if both $f_{+}$ and $f_{-}$ are; and in this case we set $$\int_A f=\int_A f_{+}-\int_A f_{-},$$ where $\int_A$ denotes the extended integral throughout. The author made the following convention: Convention. If $A$ is an open set in $\mathbb{R}^n$ , then $\int_A f$ will denote the extended integral unless specifically stated otherwise. So, $\int_A f$ in Exercise 7 is an extended integral (an improper integral). $\underline{\int_Q} (f_{+})_A$ and $\underline{\int_Q} (f_{-})_A$ are ordinary lower integrals. The author proved the following theorem on p.127 in Section 15: Theorem 15.4. Let $A$ be a bounded open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a bounded continuous function. Then the extended integral $\int_A f$ exists. If the ordinary integral $\int_A f$ also exists, then these two integrals are equal. My solution is here: By Theorem 15.4, $\int_A f$ exists in the extended sense. Since $(f_{+})_A$ and $(f_{-})_A$ are bounded, $\underline{\int_Q} (f_{+})_A$ and $\underline{\int_Q} (f_{-})_A$ exist. (in the ordinary sense.) By Theorem 15.4, $\int_A f$ exists. So by the definition of the extended integral of $f$ over $A$ , $\int_A f_{+}$ and $\int_A f_{-}$ exist and $$\int_A f=\int_A f_{+}-\int_A f_{-}$$ holds. We prove that $\int_A f_{+}=\underline{\int_Q} (f_{+})_A$ and $\int_A f_{-}=\underline{\int_Q} (f_{-})_A$ hold. To show $\int_A f_{+}=\underline{\int_Q} (f_{+})_A$ and $\int_A f_{-}=\underline{\int_Q} (f_{-})_A$ hold, we prove the following holds: If $f$ is a non-negative bounded continuous function on $A$ , $\int_A f=\underline{\int_Q} f_A$ holds. Let $P$ be a partition of $Q$ . \begin{align*} L(f_A,P) &=\sum_R m_R(f_A)v(R)\\ &=\sum_{R\subset A} m_R(f_A)v(R)\\ &\leq\sum_{R\subset A} \int_R f_A\\ &=\sum_{R\subset A} \int_R f\\ &=\int_{\cup\{R\mid R\subset A\}} f\\ &\leq \int_A f. \end{align*} Therefore, $\underline{\int_Q} f_A\leq\int_A f.$ Let $D$ be a compact rectifiable subsets of $A$ . \begin{align*} L(f_D,P) &=\sum_R m_R(f_D)v(R)\\ &=\sum_{R\subset D} m_R(f_D)v(R)\\ &=\sum_{R\subset D} m_R(f_A)v(R)\\ &\leq\sum_{R\subset A} m_R(f_A)v(R)\\ &\leq\underline{\int_Q} f_A. \end{align*} Therefore $\int_D f_=\int_Q f_D=\underline{\int_Q} f_D\leq\underline{\int_Q} f_A.$ Therefore $\int_A f\leq\underline{\int_Q} f_A.$","I am reading ""Analysis on Manifolds"" by James R. Munkres. The following exercise is Exercise 7 on p.133 in Section 15 in this book: Exercise 7. Let be a bounded open set in ; let be a bounded continuous function. Let be a rectangle containing . Show that I solved this exercise, but I am not sure my solution is ok or not. The following is the definition of the extended integral of over . Definition. Let be an open set in ; let be a continuous function. If is non-negative on , we define the (extended) integral of over , denoted , to be the supremum of the numbers , as ranges over all compact rectifiable subsets of , provided this supremum exists. In this case, we say that is integrable over (in the extended sense). More generally, if is an arbitrary continuous function on , set We say that is integrable over (in the extended sense) if both and are; and in this case we set where denotes the extended integral throughout. The author made the following convention: Convention. If is an open set in , then will denote the extended integral unless specifically stated otherwise. So, in Exercise 7 is an extended integral (an improper integral). and are ordinary lower integrals. The author proved the following theorem on p.127 in Section 15: Theorem 15.4. Let be a bounded open set in ; let be a bounded continuous function. Then the extended integral exists. If the ordinary integral also exists, then these two integrals are equal. My solution is here: By Theorem 15.4, exists in the extended sense. Since and are bounded, and exist. (in the ordinary sense.) By Theorem 15.4, exists. So by the definition of the extended integral of over , and exist and holds. We prove that and hold. To show and hold, we prove the following holds: If is a non-negative bounded continuous function on , holds. Let be a partition of . Therefore, Let be a compact rectifiable subsets of . Therefore Therefore","A \mathbb{R}^n f:A\to\mathbb{R} Q A \int_A f=\underline{\int_Q} (f_{+})_A - \underline{\int_Q} (f_{-})_A. f A A \mathbb{R}^n f:A\to\mathbb{R} f A f A \int_A f \int_D f D A f A f A f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}. f A f_{+} f_{-} \int_A f=\int_A f_{+}-\int_A f_{-}, \int_A A \mathbb{R}^n \int_A f \int_A f \underline{\int_Q} (f_{+})_A \underline{\int_Q} (f_{-})_A A \mathbb{R}^n f:A\to\mathbb{R} \int_A f \int_A f \int_A f (f_{+})_A (f_{-})_A \underline{\int_Q} (f_{+})_A \underline{\int_Q} (f_{-})_A \int_A f f A \int_A f_{+} \int_A f_{-} \int_A f=\int_A f_{+}-\int_A f_{-} \int_A f_{+}=\underline{\int_Q} (f_{+})_A \int_A f_{-}=\underline{\int_Q} (f_{-})_A \int_A f_{+}=\underline{\int_Q} (f_{+})_A \int_A f_{-}=\underline{\int_Q} (f_{-})_A f A \int_A f=\underline{\int_Q} f_A P Q \begin{align*}
L(f_A,P)
&=\sum_R m_R(f_A)v(R)\\
&=\sum_{R\subset A} m_R(f_A)v(R)\\
&\leq\sum_{R\subset A} \int_R f_A\\
&=\sum_{R\subset A} \int_R f\\
&=\int_{\cup\{R\mid R\subset A\}} f\\
&\leq \int_A f.
\end{align*} \underline{\int_Q} f_A\leq\int_A f. D A \begin{align*}
L(f_D,P)
&=\sum_R m_R(f_D)v(R)\\
&=\sum_{R\subset D} m_R(f_D)v(R)\\
&=\sum_{R\subset D} m_R(f_A)v(R)\\
&\leq\sum_{R\subset A} m_R(f_A)v(R)\\
&\leq\underline{\int_Q} f_A.
\end{align*} \int_D f_=\int_Q f_D=\underline{\int_Q} f_D\leq\underline{\int_Q} f_A. \int_A f\leq\underline{\int_Q} f_A.","['multivariable-calculus', 'solution-verification', 'improper-integrals']"
24,multivariable function maximum,multivariable function maximum,,"$max\Bigl\{\sum_{i=1}^{n}x_{i}^{2}:\sum_{i=1}^{n}x_{i}=1,x_{i}>\lambda,i=1,...n\Bigl\}= \\ max\Bigl\{\sum_{i=1}^{n-1}x_{i}^{2}+(1-\sum_{i=1}^{n-1}x_{i})^{2} : \sum_{i=1}^{n-1}x_{i} \leq 1-\lambda,x_{i}\geq\lambda, i=1,...,n-1 \Bigl\}= \\ (n-1)\lambda^{2}+(1-(n-1)\lambda)^{2}$ Any idea how to solve this? I tried Lagrange multipliers in both braces. The first one returns critical point at 1/n while the second at $\frac{1-\lambda}{n-1}$ . edit. $(1-\frac{2}{n})\frac{1}{n-1}<\lambda<\frac{1}{n}$ $\sum_{i=1}^{n}x_{i}^{2}=\sum_{i=1}^{n-1}x_{i}^{2}+x_{n}^{2}=\sum_{i=1}^{n-1}x_{i}^{2}+(1-\sum_{i=1}^{n-1}x_{i})^{2}$",Any idea how to solve this? I tried Lagrange multipliers in both braces. The first one returns critical point at 1/n while the second at . edit.,"max\Bigl\{\sum_{i=1}^{n}x_{i}^{2}:\sum_{i=1}^{n}x_{i}=1,x_{i}>\lambda,i=1,...n\Bigl\}= \\
max\Bigl\{\sum_{i=1}^{n-1}x_{i}^{2}+(1-\sum_{i=1}^{n-1}x_{i})^{2} : \sum_{i=1}^{n-1}x_{i} \leq 1-\lambda,x_{i}\geq\lambda, i=1,...,n-1 \Bigl\}= \\
(n-1)\lambda^{2}+(1-(n-1)\lambda)^{2} \frac{1-\lambda}{n-1} (1-\frac{2}{n})\frac{1}{n-1}<\lambda<\frac{1}{n} \sum_{i=1}^{n}x_{i}^{2}=\sum_{i=1}^{n-1}x_{i}^{2}+x_{n}^{2}=\sum_{i=1}^{n-1}x_{i}^{2}+(1-\sum_{i=1}^{n-1}x_{i})^{2}",['multivariable-calculus']
25,What is the total differential used for?,What is the total differential used for?,,"I was reading through all of these explanation of what a total differential is and how it looks. But what is it used for? Why would someone want to know a small change dz? The book highlights the similarity with one viariable calculus, where dy=f'(x)dx, this for me is just a way to rearrange dy/dx=f'(x) to integrate and find y=f(x), not to know the small change of dy because it is not used for anything. Am I missing some concept? I know is rather a simple question, but I would appreciate some help","I was reading through all of these explanation of what a total differential is and how it looks. But what is it used for? Why would someone want to know a small change dz? The book highlights the similarity with one viariable calculus, where dy=f'(x)dx, this for me is just a way to rearrange dy/dx=f'(x) to integrate and find y=f(x), not to know the small change of dy because it is not used for anything. Am I missing some concept? I know is rather a simple question, but I would appreciate some help",,"['calculus', 'multivariable-calculus']"
26,"A question about an improper integral over an open set $A$. Why does $D$ need to be a closed set? (""Analysis on Manifolds"" by James R. Munkres)","A question about an improper integral over an open set . Why does  need to be a closed set? (""Analysis on Manifolds"" by James R. Munkres)",A D,"I am reading ""Analysis on Manifolds"" by James R. Munkres. The following is the definition of an improper integral in this book. Definition. Let $A$ be an open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a continuous function. If $f$ is non-negative on $A$ , we define the (extended) integral of $f$ over $A$ , denoted $\int_A f$ , to be the supremum of the numbers $\int_D f$ , as $D$ ranges over all compact rectifiable subsets of $A$ , provided this supremum exists. In this case, we say that $f$ is integrable over $A$ (in the extended sense). More generally, if $f$ is an arbitrary continuous function on $A$ , set $$f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}.$$ We say that $f$ is integrable over $A$ (in the extended sense) if both $f_{+}$ and $f_{-}$ are; and in this case we set $$\int_A f=\int_A f_{+}-\int_A f_{-},$$ where $\int_A$ denotes the extended integral throughout. $D$ needs to be a bounded set since we define the (ordinary) integral of $f$ only over a bounded set. The author wrote $D$ is a compact rectifiable subsets of $A$ . Why does $D$ need to be a closed set? If $A=\{(x,y)\mid x^2+y^2<1\}$ , $D:=A$ is a bounded rectifiable subset of $A$ , but there is a continuous function $f$ on $A$ which is not bounded. Can we define an improper integral over $A$ as follows? Definition. Let $A$ be an open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a continuous function. If $f$ is non-negative on $A$ , we define the (extended) integral of $f$ over $A$ , denoted $\int_A f$ , to be the supremum of the numbers $\int_D f$ , as $D$ ranges over all bounded rectifiable subsets of $A$ $\color{red}{\text{on which $f$ is bounded}}$ , provided this supremum exists. In this case, we say that $f$ is integrable over $A$ (in the extended sense). More generally, if $f$ is an arbitrary continuous function on $A$ , set $$f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}.$$ We say that $f$ is integrable over $A$ (in the extended sense) if both $f_{+}$ and $f_{-}$ are; and in this case we set $$\int_A f=\int_A f_{+}-\int_A f_{-},$$ where $\int_A$ denotes the extended integral throughout. Can we modify the definition of an improper integral over $A$ in such a way $D$ doesn't need to be a closed set in a new definition?","I am reading ""Analysis on Manifolds"" by James R. Munkres. The following is the definition of an improper integral in this book. Definition. Let be an open set in ; let be a continuous function. If is non-negative on , we define the (extended) integral of over , denoted , to be the supremum of the numbers , as ranges over all compact rectifiable subsets of , provided this supremum exists. In this case, we say that is integrable over (in the extended sense). More generally, if is an arbitrary continuous function on , set We say that is integrable over (in the extended sense) if both and are; and in this case we set where denotes the extended integral throughout. needs to be a bounded set since we define the (ordinary) integral of only over a bounded set. The author wrote is a compact rectifiable subsets of . Why does need to be a closed set? If , is a bounded rectifiable subset of , but there is a continuous function on which is not bounded. Can we define an improper integral over as follows? Definition. Let be an open set in ; let be a continuous function. If is non-negative on , we define the (extended) integral of over , denoted , to be the supremum of the numbers , as ranges over all bounded rectifiable subsets of , provided this supremum exists. In this case, we say that is integrable over (in the extended sense). More generally, if is an arbitrary continuous function on , set We say that is integrable over (in the extended sense) if both and are; and in this case we set where denotes the extended integral throughout. Can we modify the definition of an improper integral over in such a way doesn't need to be a closed set in a new definition?","A \mathbb{R}^n f:A\to\mathbb{R} f A f A \int_A f \int_D f D A f A f A f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}. f A f_{+} f_{-} \int_A f=\int_A f_{+}-\int_A f_{-}, \int_A D f D A D A=\{(x,y)\mid x^2+y^2<1\} D:=A A f A A A \mathbb{R}^n f:A\to\mathbb{R} f A f A \int_A f \int_D f D A \color{red}{\text{on which f is bounded}} f A f A f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}. f A f_{+} f_{-} \int_A f=\int_A f_{+}-\int_A f_{-}, \int_A A D","['multivariable-calculus', 'improper-integrals', 'definition']"
27,Basis for monomials $x^p_1x^q_2$ using $x^s$,Basis for monomials  using,x^p_1x^q_2 x^s,"Let $s\in\mathbb{N}$ . I have a sum of the form $$     \sum_{p+q\leq s}c_{p,q}x^p_1x^q_2, $$ and want to write it in terms of functions taking the form $$     g(x,(c_0,c_1,c_2)) = (c_0+c_1x_1+c_2x_2)^s. $$ I want to use as few as possible terms $g(x,(c_0,c_1,c_2))$ to describe the entire sum. To understand how to do that I first want to find one set of $g(x,(c_0,c_1,c_2))$ for each $(p,q)$ , i.e. find a set $S\subset \mathbb{R}^3$ such that $$   x_1^px_2^q = \sum_{(c_0,c_1,c_2)\in S}g(x,((c_0,c_1,c_2)). $$ For the simple cases I have (p,q) S (0,0) {(1,0,0)} (s,0) {(0,1,0)} (0,s) {(0,0,1)} and I know that $g$ can also be written as $$    g(x,(c_0,c_1,c_2)) = \sum_{k_0+k_1+k_2=s}\frac{s!}{k_0!k_1!k_2!}c^{k_0}_0c^{k_1}_1c^{k_2}_2x^{k_1}_1x^{k_2}_2 $$ Is there an explicit way of writing $S$ for the other $(p,q)$ ? -- edit -- For $s=2$ the full list is (up to multiplicative constants) (p,q) S (0,0) {(1,0,0)} (1,0) {(1,1,0),(0,-1,0),(-1,0,0)} (0,1) {(1,0,1),(0,0,-1),(-1,0,0)} (1,1) {(0,0,-1),(0,-1,0),(0,1,1)} (2,0) {(0,1,0)} (0,2) {(0,0,1)}","Let . I have a sum of the form and want to write it in terms of functions taking the form I want to use as few as possible terms to describe the entire sum. To understand how to do that I first want to find one set of for each , i.e. find a set such that For the simple cases I have (p,q) S (0,0) {(1,0,0)} (s,0) {(0,1,0)} (0,s) {(0,0,1)} and I know that can also be written as Is there an explicit way of writing for the other ? -- edit -- For the full list is (up to multiplicative constants) (p,q) S (0,0) {(1,0,0)} (1,0) {(1,1,0),(0,-1,0),(-1,0,0)} (0,1) {(1,0,1),(0,0,-1),(-1,0,0)} (1,1) {(0,0,-1),(0,-1,0),(0,1,1)} (2,0) {(0,1,0)} (0,2) {(0,0,1)}","s\in\mathbb{N} 
    \sum_{p+q\leq s}c_{p,q}x^p_1x^q_2,
 
    g(x,(c_0,c_1,c_2)) = (c_0+c_1x_1+c_2x_2)^s.
 g(x,(c_0,c_1,c_2)) g(x,(c_0,c_1,c_2)) (p,q) S\subset \mathbb{R}^3 
  x_1^px_2^q = \sum_{(c_0,c_1,c_2)\in S}g(x,((c_0,c_1,c_2)).
 g 
   g(x,(c_0,c_1,c_2)) = \sum_{k_0+k_1+k_2=s}\frac{s!}{k_0!k_1!k_2!}c^{k_0}_0c^{k_1}_1c^{k_2}_2x^{k_1}_1x^{k_2}_2
 S (p,q) s=2","['linear-algebra', 'multivariable-calculus', 'polynomials', 'systems-of-equations', 'change-of-basis']"
28,"Evaluate $\int_0^{2\pi} \frac{\sin t}{a^2+k^2t^2}\, dt$",Evaluate,"\int_0^{2\pi} \frac{\sin t}{a^2+k^2t^2}\, dt","I need to compute $$ \int_\gamma \frac{dx}{x^2+y^2+z^2},$$ where $\gamma$ is the first turn of the helix $\gamma(t) = (a\cos t, a\sin t, kt),$ that is, $t \in [0, 2\pi].$ I don't see any option other than using the definition, which yields $$ \int_0^{2\pi} \left\langle \left(\frac{1}{a^2\cos^2 t+a^2\sin^2 t+k^2t^2}, 0, 0 \right), \left(-a\sin t, a \cos t, k \right) \right\rangle \,dt = \int_0^{2\pi} \frac{-a\sin t}{a^2+k^2t^2}\,dt.$$ But I don't know how to compute this integral. This exercise is supposed to be analytically solvable. Thanks a lot.","I need to compute where is the first turn of the helix that is, I don't see any option other than using the definition, which yields But I don't know how to compute this integral. This exercise is supposed to be analytically solvable. Thanks a lot."," \int_\gamma \frac{dx}{x^2+y^2+z^2}, \gamma \gamma(t) = (a\cos t, a\sin t, kt), t \in [0, 2\pi].  \int_0^{2\pi} \left\langle \left(\frac{1}{a^2\cos^2 t+a^2\sin^2 t+k^2t^2}, 0, 0 \right), \left(-a\sin t, a \cos t, k \right) \right\rangle \,dt = \int_0^{2\pi} \frac{-a\sin t}{a^2+k^2t^2}\,dt.","['multivariable-calculus', 'vector-analysis']"
29,Lagrange Remainder multivariable functions,Lagrange Remainder multivariable functions,,"Let $f: \mathbb{R^2}\rightarrow \mathbb{R}$ be $f(x,y)=y^2\cdot cos(2\pi x)$ Calculate the 2nd degree taylor polynomial at $(x_0,y_0)=(1,1)$ : $$ T_2[f,(1,1)]=f(x,y)+\langle \nabla f(x,y),\xi\rangle+\frac{1}{2}\langle\xi,Hessf\cdot\xi\rangle=-y^24\pi^2cos(2\pi x)\xi_1^2-4\pi\cdot y\cdot sin(x)\xi_1\xi_2-4\pi\cdot y\cdot sin(2\pi x)\cdot\xi_1\xi_2+2cos(2\pi x)=1+2\xi_2-2\pi^2\xi_1^2+\xi_2^2 $$ I believe my calculated Taylor Polynomial should be correct. Now I am supposed to calculate the error. Let $I=[0,2]x[0,2]$ , show that the following estimate is true: $$ \forall x\in I: \lvert f(x)-T_2[f(1,1)](x-(1,1))\rvert\leq\frac{16}{3}\pi^3+8\pi^2+2\pi $$ There was a tip given to use triangle inequality but I decided to ignore that one for now. I calculated the error: $$ R_2=\lvert \sum_{\lvert \alpha\rvert =3}\frac{D^\alpha f(\xi)}{\alpha !}\cdot(x-(1,1))^\alpha\rvert $$ $$ \frac{\xi_2^2 8\pi^3sin(2\pi\xi_1)}{6}\cdot(x_1-1)^3+4\pi^2cos(2\pi\xi_1)\xi_2(x_1-1)^2\cdot(x_2-1)-2\pi sin(2\pi\xi_1)\cdot(x_1-1)\cdot(x_2-1)^2 $$ $\xi$ is between $1$ and $x_i$ and I want the term to reach its maximum. Let $x_1=2$ and $x_2=2$ We now have: $$ \frac{\xi_2^2\cdot8\pi^3sin(2\pi\xi_1)}{6}-4\pi^2cos(2\pi\xi_1)-2\pi sin(2\pi\xi_1) $$ This can't ever reach $\frac{16}{3}pi^3+8\pi^2+2\pi$ right? So where did I go wrong, is it my derivation or something else? I don't really know how to go further and can't find any examples for the lagrange remainder for more than one variable.","Let be Calculate the 2nd degree taylor polynomial at : I believe my calculated Taylor Polynomial should be correct. Now I am supposed to calculate the error. Let , show that the following estimate is true: There was a tip given to use triangle inequality but I decided to ignore that one for now. I calculated the error: is between and and I want the term to reach its maximum. Let and We now have: This can't ever reach right? So where did I go wrong, is it my derivation or something else? I don't really know how to go further and can't find any examples for the lagrange remainder for more than one variable.","f: \mathbb{R^2}\rightarrow \mathbb{R} f(x,y)=y^2\cdot cos(2\pi x) (x_0,y_0)=(1,1) 
T_2[f,(1,1)]=f(x,y)+\langle \nabla f(x,y),\xi\rangle+\frac{1}{2}\langle\xi,Hessf\cdot\xi\rangle=-y^24\pi^2cos(2\pi x)\xi_1^2-4\pi\cdot y\cdot sin(x)\xi_1\xi_2-4\pi\cdot y\cdot sin(2\pi x)\cdot\xi_1\xi_2+2cos(2\pi x)=1+2\xi_2-2\pi^2\xi_1^2+\xi_2^2
 I=[0,2]x[0,2] 
\forall x\in I: \lvert f(x)-T_2[f(1,1)](x-(1,1))\rvert\leq\frac{16}{3}\pi^3+8\pi^2+2\pi
 
R_2=\lvert \sum_{\lvert \alpha\rvert =3}\frac{D^\alpha f(\xi)}{\alpha !}\cdot(x-(1,1))^\alpha\rvert
 
\frac{\xi_2^2 8\pi^3sin(2\pi\xi_1)}{6}\cdot(x_1-1)^3+4\pi^2cos(2\pi\xi_1)\xi_2(x_1-1)^2\cdot(x_2-1)-2\pi sin(2\pi\xi_1)\cdot(x_1-1)\cdot(x_2-1)^2
 \xi 1 x_i x_1=2 x_2=2 
\frac{\xi_2^2\cdot8\pi^3sin(2\pi\xi_1)}{6}-4\pi^2cos(2\pi\xi_1)-2\pi sin(2\pi\xi_1)
 \frac{16}{3}pi^3+8\pi^2+2\pi","['multivariable-calculus', 'taylor-expansion', 'partial-derivative']"
30,CDF of the distance from origin to the hyperplane passing through $d$ i.i.d. points in $\mathbb{R}^d$,CDF of the distance from origin to the hyperplane passing through  i.i.d. points in,d \mathbb{R}^d,"I am stuck with a problem in multivariable statistics. The problem can be stated as follows: For a spherically symmetric distribution in $\mathbb{R}^d$ , it can be specified completely by the function $F(r)=\Pr(\|X\|>r)$ . For example, 2d standard Gaussian distribution has $F(r)=\exp(-r^2/2)$ . Now $X_1, X_2, \dots, X_d$ i.i.d. follow this distribution. These $d$ random points form a hyperplane in $\mathbb{R}^d$ . What is the probability $H(p)$ that the distance from the origin to this hyperplane is larger than $p$ for $p>0$ ? The answer should be written as integration about $F$ . I found the solution for $d=2$ in a German paper [1], which says $$ H(p)=\frac{2}{\pi} \int_p^{\infty} \arccos\frac{p}{r} |d(F^2(r))| $$ I successfully derive the formula for $d=3$ as: \begin{align} H(p)= \int_p^{+\infty}\frac{2p }{\pi r \sqrt{r^2-p^2}}G_3^{3}(r)dr \end{align} where \begin{align} G_3(r)=\int_r^{+\infty} \sqrt{1-\frac{r^2}{y^2}}     |d F(y)| \end{align} which represents $P(x_1^2+x_2^2>r^2)$ . In $d=3$ , I can write $H(p)$ in a similar form as $d=2$ : \begin{align} H(p)=\frac{2}{\pi} \int_p^{\infty} \textrm{arccos}\frac{p}{r} |d(G_3^{3}(r))| \end{align} Then I encounter problem to obtain $H(p)$ for $d>3$ , since the derivation for $d=2,3$ relies on geometric insights. Any suggestions on this problem? Now I guess for $d\geq 2$ : \begin{align} G_d(r) &= \int_r^{+\infty} (1-\frac{r^2}{y^2})^{\frac{d}{2}-1}|dF(y)|\\ H(p) & =\frac{2}{\pi} \int_p^{\infty} \textrm{arccos}\frac{p}{r} |d(G_d^{d}(r))| \end{align} However, I found difficulty to prove this conjecture. [1] Carnal, Henri. ""Die konvexe Hülle von n rotationssymmetrisch verteilten Punkten."" Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 15.2 (1970): 168-176.","I am stuck with a problem in multivariable statistics. The problem can be stated as follows: For a spherically symmetric distribution in , it can be specified completely by the function . For example, 2d standard Gaussian distribution has . Now i.i.d. follow this distribution. These random points form a hyperplane in . What is the probability that the distance from the origin to this hyperplane is larger than for ? The answer should be written as integration about . I found the solution for in a German paper [1], which says I successfully derive the formula for as: where which represents . In , I can write in a similar form as : Then I encounter problem to obtain for , since the derivation for relies on geometric insights. Any suggestions on this problem? Now I guess for : However, I found difficulty to prove this conjecture. [1] Carnal, Henri. ""Die konvexe Hülle von n rotationssymmetrisch verteilten Punkten."" Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 15.2 (1970): 168-176.","\mathbb{R}^d F(r)=\Pr(\|X\|>r) F(r)=\exp(-r^2/2) X_1, X_2, \dots, X_d d \mathbb{R}^d H(p) p p>0 F d=2 
H(p)=\frac{2}{\pi} \int_p^{\infty} \arccos\frac{p}{r} |d(F^2(r))|
 d=3 \begin{align}
H(p)= \int_p^{+\infty}\frac{2p }{\pi r \sqrt{r^2-p^2}}G_3^{3}(r)dr
\end{align} \begin{align}
G_3(r)=\int_r^{+\infty} \sqrt{1-\frac{r^2}{y^2}}
    |d F(y)|
\end{align} P(x_1^2+x_2^2>r^2) d=3 H(p) d=2 \begin{align}
H(p)=\frac{2}{\pi} \int_p^{\infty} \textrm{arccos}\frac{p}{r} |d(G_3^{3}(r))|
\end{align} H(p) d>3 d=2,3 d\geq 2 \begin{align}
G_d(r) &= \int_r^{+\infty} (1-\frac{r^2}{y^2})^{\frac{d}{2}-1}|dF(y)|\\
H(p) & =\frac{2}{\pi} \int_p^{\infty} \textrm{arccos}\frac{p}{r} |d(G_d^{d}(r))|
\end{align}","['multivariable-calculus', 'multivariate-statistical-analysis', 'integral-geometry']"
31,Total derivative of $e^{xy}(x^2+y^2)$,Total derivative of,e^{xy}(x^2+y^2),"Let $f:\mathbb{R^2} \rightarrow \mathbb{R} \longmapsto e^{xy}\cdot (x^2+y^2)$ Show for which $(x,y)\in \mathbb{R^2}$ the function is totally differentiable. A function is totally differentiable if a) $\lim \limits_{h \to 0}\frac{f(x+h)-f(x)-A\cdot h}{\Vert h \Vert}$ or b) f is continuously partially differentiable I first calculated the partial derivatives for both x and y: $$ \frac{\partial f}{\partial x}=e^{xy}\cdot(yx^2+y^3+2x)$$ $$ \frac{\partial f}{\partial y}=e^{xy}\cdot(xy^2+x^3+2y)$$ Thus the jacobian Matrix is $\begin{array}{rrr}  e^{xy}\cdot(yx^2+y^3+2x) & e^{xy}\cdot(xy^2+x^3+2y) \\ \end{array}$ The partial derivatives are continuous as they are a product of two continuous functions. Therefore according to b) f is totally differentiable $\forall x,y\in \mathbb{R}$ Is this correct or did I make a mistake somewhere and f actually isn't totally differentiable for every point? Should this rather be done via a) or is that unnecessearily complex?",Let Show for which the function is totally differentiable. A function is totally differentiable if a) or b) f is continuously partially differentiable I first calculated the partial derivatives for both x and y: Thus the jacobian Matrix is The partial derivatives are continuous as they are a product of two continuous functions. Therefore according to b) f is totally differentiable Is this correct or did I make a mistake somewhere and f actually isn't totally differentiable for every point? Should this rather be done via a) or is that unnecessearily complex?,"f:\mathbb{R^2} \rightarrow \mathbb{R} \longmapsto e^{xy}\cdot (x^2+y^2) (x,y)\in \mathbb{R^2} \lim \limits_{h \to 0}\frac{f(x+h)-f(x)-A\cdot h}{\Vert h \Vert}  \frac{\partial f}{\partial x}=e^{xy}\cdot(yx^2+y^3+2x)  \frac{\partial f}{\partial y}=e^{xy}\cdot(xy^2+x^3+2y) \begin{array}{rrr} 
e^{xy}\cdot(yx^2+y^3+2x) & e^{xy}\cdot(xy^2+x^3+2y) \\
\end{array} \forall x,y\in \mathbb{R}","['multivariable-calculus', 'partial-derivative']"
32,Equations for geodesics of the plane in polar coordinates,Equations for geodesics of the plane in polar coordinates,,"I'm studying equations for geodesics of the plane in polar coordinates from the book ""gravity by James hartle"" and I am stuck at the last step which involves partial derivative of 3 variable. The metric of the plane in polar coordinates $r$ and $Φ$ is : $$ dS^2 = dr^2 + r^2dΦ^2 $$ A curve between two points $A$ and $B$ can be described parametrically by giving $r$ and $Φ$ as a function of a parameter $σ$ which varies between $σ=0$ at point $A$ and $σ=1$ at point $B$ distance between $A$ and $B$ is given by: $$ \int_A^B dS = \int_A^B (dr^2 + r^2 dΦ^2)^{1/2}                   = \int_0^1 {dσ} \left[ \left(\frac{dr}{dσ}^2\right) + r^2\left(\frac{dΦ}{dσ}\right)^2\right]^{1/2} $$ Solve the above equation by Lagrangian and we will get : $$ \frac d{dσ}\left(\frac1L\frac{dr}{dσ}\right) = \frac rL\left(\frac{dΦ}{dσ}\right)^2 \tag{1} $$ $$ \frac d{dσ}\left(\frac1L r^2 \frac{dΦ}{dσ}\right) = 0 \tag{2} $$ Now writer says that the value of $L$ is just $dS/dσ$ . Therefore, multiplying above equations by $dσ/dS$ , the equations for geodesics using the distance $S$ as the parameter along the curve take the simple form $$ \frac{d^2r}{ds^2} = r\left(\frac{dΦ}{dS}\right)^2 \tag{3} $$ $$ \frac d{dS}\left(r^2 \frac{dΦ}{dS}\right) = 0 \tag{4} $$ Can somebody please explain how do we get equations 3 and 4 from 1 and 2 ?? I understand the rest of the calculation but facing difficulty how to deal with these mix partial derivatives. Thanks","I'm studying equations for geodesics of the plane in polar coordinates from the book ""gravity by James hartle"" and I am stuck at the last step which involves partial derivative of 3 variable. The metric of the plane in polar coordinates and is : A curve between two points and can be described parametrically by giving and as a function of a parameter which varies between at point and at point distance between and is given by: Solve the above equation by Lagrangian and we will get : Now writer says that the value of is just . Therefore, multiplying above equations by , the equations for geodesics using the distance as the parameter along the curve take the simple form Can somebody please explain how do we get equations 3 and 4 from 1 and 2 ?? I understand the rest of the calculation but facing difficulty how to deal with these mix partial derivatives. Thanks","r Φ 
dS^2 = dr^2 + r^2dΦ^2
 A B r Φ σ σ=0 A σ=1 B A B 
\int_A^B dS = \int_A^B (dr^2 + r^2 dΦ^2)^{1/2}
                  = \int_0^1 {dσ} \left[ \left(\frac{dr}{dσ}^2\right) + r^2\left(\frac{dΦ}{dσ}\right)^2\right]^{1/2}
 
\frac d{dσ}\left(\frac1L\frac{dr}{dσ}\right) = \frac rL\left(\frac{dΦ}{dσ}\right)^2
\tag{1}
 
\frac d{dσ}\left(\frac1L r^2 \frac{dΦ}{dσ}\right) = 0
\tag{2}
 L dS/dσ dσ/dS S 
\frac{d^2r}{ds^2} = r\left(\frac{dΦ}{dS}\right)^2
\tag{3}
 
\frac d{dS}\left(r^2 \frac{dΦ}{dS}\right) = 0
\tag{4}
","['multivariable-calculus', 'derivatives', 'partial-differential-equations', 'polar-coordinates', 'geodesic']"
33,Average Energy/Light in a box,Average Energy/Light in a box,,"Imagine there exists a region of completely empty space bounded by a rectangular prism, R, with length, l, width, w, and height, h. There is a star(a simple light/particle emitting object) in the center of one of the faces of the bounding rectangular prism. there is a gradient of light fall off which starts with the lights original brightness and slowly gradually decreases towards zero the farther away you get from the emitter, even outside of the bounded rectangular prism you could imagine it reaches zero at infinity. This light fall off propagates spherically from the emitter . What is the average energy/light of the entire bounding rectangular prism. Another way to visualize this is a box with density which decreases spherically from a point on the side of the box and you must evaluate the average density of the box. The function of light fall off should look something like $$L(x) = \frac{2E}{1+e^{\frac{1}{c}x}}$$ or $$L(x) = \frac{1}{\frac{1}{c}x+E}$$ where x is the distance from the emitter, E is the starting energy/density, and c is some constant which controls the rate of fall off. while the exact function does not matter to me  I have provided two examples for you to use whichever you think will be easier to work with. Feel free to also use any functions you like which follows the same boundary conditions of starting at E at $x=0$ and ending tending towards $0$ as $x$ goes to $\infty$ . I have attempted this by trying to take rectangular cross sections of a perfect sphere/hemisphere to no avail, I also have tried to create functions which capture the x,y, and z inputs for rotation to any given ray/vector from the emitter but I am unaware and could not find how to map a function which draws out the rectangular prism in 3d space, although I figure you could do it with some form of multivariable Fourier series, but i do not feel that is the easiest method to go about generating a approximation. if it is much simpler feel free to generalize to a perfect cube rather then a rectangle but im not sure it would help much either way. Feel free to choose the center of any face on the rectangular prism to place the emitter.","Imagine there exists a region of completely empty space bounded by a rectangular prism, R, with length, l, width, w, and height, h. There is a star(a simple light/particle emitting object) in the center of one of the faces of the bounding rectangular prism. there is a gradient of light fall off which starts with the lights original brightness and slowly gradually decreases towards zero the farther away you get from the emitter, even outside of the bounded rectangular prism you could imagine it reaches zero at infinity. This light fall off propagates spherically from the emitter . What is the average energy/light of the entire bounding rectangular prism. Another way to visualize this is a box with density which decreases spherically from a point on the side of the box and you must evaluate the average density of the box. The function of light fall off should look something like or where x is the distance from the emitter, E is the starting energy/density, and c is some constant which controls the rate of fall off. while the exact function does not matter to me  I have provided two examples for you to use whichever you think will be easier to work with. Feel free to also use any functions you like which follows the same boundary conditions of starting at E at and ending tending towards as goes to . I have attempted this by trying to take rectangular cross sections of a perfect sphere/hemisphere to no avail, I also have tried to create functions which capture the x,y, and z inputs for rotation to any given ray/vector from the emitter but I am unaware and could not find how to map a function which draws out the rectangular prism in 3d space, although I figure you could do it with some form of multivariable Fourier series, but i do not feel that is the easiest method to go about generating a approximation. if it is much simpler feel free to generalize to a perfect cube rather then a rectangle but im not sure it would help much either way. Feel free to choose the center of any face on the rectangular prism to place the emitter.",L(x) = \frac{2E}{1+e^{\frac{1}{c}x}} L(x) = \frac{1}{\frac{1}{c}x+E} x=0 0 x \infty,"['calculus', 'integration', 'multivariable-calculus', 'exponential-function', 'average']"
34,Multivariable Piecewise function with interval defined by the variables,Multivariable Piecewise function with interval defined by the variables,,"I am wondering how you analyse and take the partial derivatives of a multivariable piecewise functions where the intervals are defined by the variables Something like f(x,y)= $\left\{ \begin{array}              (f(x) &   if  & x>h(y) \\              \\ g(x) &  if & x\leq h(y)\\              \end{array}    \right.$ How would you take the partial derivative of f(x,y) in y?","I am wondering how you analyse and take the partial derivatives of a multivariable piecewise functions where the intervals are defined by the variables Something like f(x,y)= How would you take the partial derivative of f(x,y) in y?","\left\{ \begin{array}
             (f(x) &   if  & x>h(y) \\
             \\ g(x) &  if & x\leq h(y)\\
             \end{array}
   \right.",['multivariable-calculus']
35,Plotting the streaklines knowing the pathlines,Plotting the streaklines knowing the pathlines,,"I have a two-dimensional velocity field $$\mathbf{V}=\begin{pmatrix} u \\ v \end{pmatrix}=\begin{pmatrix} 0.5+0.8x \\ 1.5+2.5\sin(\omega t)-0.8y \end{pmatrix}$$ from Example 4-5 in ""Fluid Mechanics: Fundamentals and Applications"" 2nd Edition. I have correctly derived the streamlines from $$\frac{dy}{dx}=\frac{v}{u}$$ at $t=2$ giving $$y=\frac{C}{0.8(0.5+0.8x)}+1.875$$ in agreement with the text. Using a Runge-Kutta numerical integration method I approximated the pathlines from $0<t<2$ s using these equations. $$u=\frac{dx}{dt}=0.5+0.8x$$ $$v=\frac{dy}{dt}=1.5+2.5\sin(\omega t)-0.8y$$ Plotting the equations agrees with the figure in the text. My results are on the left red: starting from $(0.5,0.5)$ blue: starting from $(0.5,2.5)$ green: starting from $(0.5,4.5)$ and the light pink curves in the right figure are from the text. These questions ( here , and here ) deal with methods to find the streaklines . This link , however, has been the most helpful in understanding how to derive the necessary equations. But my results do not agree with the text. The text defines the method for plotting the streaklines this way. Finally, the streaklines are simulated by following the paths of many fluid tracer particles released at the given three locations at times between $t=0$ s and $t=2$ s, and connecting the locus of their positions at $t=2$ s. I want to try and avoid solving the ODEs by hand since $y(t)$ will involve an integrating factor, but more importantly, to arrive at equations for $x=x(x',t,t')$ and $y=y(y',t,t')$ without going through lots of error-prone algebra. From the third link above $(x',y')$ are the future positions of the particle at time $t'$ and $t$ is the current time. To close out the question, I am looking for a way to use my approximate values for the pathlines to plot my streaklines from $0<t<2$ s. EDIT: It has just occurred to me that in the first of the previous links, an ODE that defines a streakline is mentioned. The ODE is from Wikipedia and is given here. $$\left\{\begin{matrix} \frac{d\vec{x}_{str}}{dt}=\vec{u}_P(\vec{x}_{str},t) \\ \vec{x}_{str}(t=\tau_P)=\vec{x}_{P0} \end{matrix}\right.$$ From Wikipedia... $\vec{u}_P(\vec{x},t)$ is the velocity of a particle $P$ at location $\vec{x}$ and time $t$ . The parameter $\tau_P$ , parameterizes the streakline $\vec{x}_{str}(t,\tau_P)$ and $t_0\le\tau_P\le t$ , where $t$ is a time of interest. The second link from above describes this parameterization, but I am struggling to understand how to work it out in general, not just for this velocity field. But if I can make that work then the ODE should be able to be solved numerically with a similar Runge-Kutta method. However, this link suggests that there are errors in the Wikipedia article. The parameterized ODEs from $0<\tau<t$ should be the following. $$\frac{dx_\tau}{d\theta}=0.5+0.8x_\tau(\theta)$$ $$\frac{dy_\tau}{d\theta}=1.5+2.5\sin\left(\omega(\tau+\theta)\right)-0.8y_\tau(\theta)$$ Getting some help setting up the numerical integration is where I'm stuck. I'm unsure how to deal with the $\tau$ and $\theta$ terms; i.e, which term is marching forward and what values to put in for $\tau$ .","I have a two-dimensional velocity field from Example 4-5 in ""Fluid Mechanics: Fundamentals and Applications"" 2nd Edition. I have correctly derived the streamlines from at giving in agreement with the text. Using a Runge-Kutta numerical integration method I approximated the pathlines from s using these equations. Plotting the equations agrees with the figure in the text. My results are on the left red: starting from blue: starting from green: starting from and the light pink curves in the right figure are from the text. These questions ( here , and here ) deal with methods to find the streaklines . This link , however, has been the most helpful in understanding how to derive the necessary equations. But my results do not agree with the text. The text defines the method for plotting the streaklines this way. Finally, the streaklines are simulated by following the paths of many fluid tracer particles released at the given three locations at times between s and s, and connecting the locus of their positions at s. I want to try and avoid solving the ODEs by hand since will involve an integrating factor, but more importantly, to arrive at equations for and without going through lots of error-prone algebra. From the third link above are the future positions of the particle at time and is the current time. To close out the question, I am looking for a way to use my approximate values for the pathlines to plot my streaklines from s. EDIT: It has just occurred to me that in the first of the previous links, an ODE that defines a streakline is mentioned. The ODE is from Wikipedia and is given here. From Wikipedia... is the velocity of a particle at location and time . The parameter , parameterizes the streakline and , where is a time of interest. The second link from above describes this parameterization, but I am struggling to understand how to work it out in general, not just for this velocity field. But if I can make that work then the ODE should be able to be solved numerically with a similar Runge-Kutta method. However, this link suggests that there are errors in the Wikipedia article. The parameterized ODEs from should be the following. Getting some help setting up the numerical integration is where I'm stuck. I'm unsure how to deal with the and terms; i.e, which term is marching forward and what values to put in for .","\mathbf{V}=\begin{pmatrix}
u \\
v
\end{pmatrix}=\begin{pmatrix}
0.5+0.8x \\
1.5+2.5\sin(\omega t)-0.8y
\end{pmatrix} \frac{dy}{dx}=\frac{v}{u} t=2 y=\frac{C}{0.8(0.5+0.8x)}+1.875 0<t<2 u=\frac{dx}{dt}=0.5+0.8x v=\frac{dy}{dt}=1.5+2.5\sin(\omega t)-0.8y (0.5,0.5) (0.5,2.5) (0.5,4.5) t=0 t=2 t=2 y(t) x=x(x',t,t') y=y(y',t,t') (x',y') t' t 0<t<2 \left\{\begin{matrix}
\frac{d\vec{x}_{str}}{dt}=\vec{u}_P(\vec{x}_{str},t) \\
\vec{x}_{str}(t=\tau_P)=\vec{x}_{P0}
\end{matrix}\right. \vec{u}_P(\vec{x},t) P \vec{x} t \tau_P \vec{x}_{str}(t,\tau_P) t_0\le\tau_P\le t t 0<\tau<t \frac{dx_\tau}{d\theta}=0.5+0.8x_\tau(\theta) \frac{dy_\tau}{d\theta}=1.5+2.5\sin\left(\omega(\tau+\theta)\right)-0.8y_\tau(\theta) \tau \theta \tau","['ordinary-differential-equations', 'multivariable-calculus', 'vector-fields', 'fluid-dynamics', 'runge-kutta-methods']"
36,Integrals involving the error function and the gaussian,Integrals involving the error function and the gaussian,,"I would like to know if the following integral has an explicit expression or not. $$F(r)=\int_1^r \frac{1}{t^2}e^{t^2/2} \operatorname{erf}\left(\frac{t}{\sqrt{2}}\right)dt$$ where the error function inside is defined as $$\operatorname{erf}\left(\frac{t}{\sqrt{2}}\right)=\frac{1}{\sqrt{\pi}}\int_0^t e^{-x^2/2}dx.$$ Thus, $$F(r) = \frac{1}{\sqrt{\pi}}\int_{1}^{r}\int_0^{t}\frac{1}{t^2}e^{\frac{t^2-x^2}{2}}dt dx$$ which on making a change of variable takes the form, $$F(r) = \frac{1}{\sqrt{\pi}}\int_1^r\int_{0}^1 \frac{\exp\left(\frac{t^2}{2}\left(1-y^2\right)\right)}{t}dy dt$$ Now $$\int_1^r \frac{\exp\left(\frac{t^2}{2}\left(1-y^2\right)\right)}{t} dt = \frac{1}{2}\left[\Gamma\left(0,\frac{y^2-2}{2}\right)-\Gamma\left(0,\frac{r^2(y^2-2)}{2}\right)\right].$$ To finish to computation one needs to compute, $$\frac{1}{2}\int_{0}^1\left[\Gamma\left(0,\frac{y^2-2}{2}\right)-\Gamma\left(0,\frac{r^2(y^2-2)}{2}\right)\right]dy,$$ but I am not sure how to compute expression involving the incomplete gamma function. I would like to know if there are any explicit expressions of the above integral involving the Exponential Integral or any other special function?","I would like to know if the following integral has an explicit expression or not. where the error function inside is defined as Thus, which on making a change of variable takes the form, Now To finish to computation one needs to compute, but I am not sure how to compute expression involving the incomplete gamma function. I would like to know if there are any explicit expressions of the above integral involving the Exponential Integral or any other special function?","F(r)=\int_1^r \frac{1}{t^2}e^{t^2/2} \operatorname{erf}\left(\frac{t}{\sqrt{2}}\right)dt \operatorname{erf}\left(\frac{t}{\sqrt{2}}\right)=\frac{1}{\sqrt{\pi}}\int_0^t e^{-x^2/2}dx. F(r) = \frac{1}{\sqrt{\pi}}\int_{1}^{r}\int_0^{t}\frac{1}{t^2}e^{\frac{t^2-x^2}{2}}dt dx F(r) = \frac{1}{\sqrt{\pi}}\int_1^r\int_{0}^1 \frac{\exp\left(\frac{t^2}{2}\left(1-y^2\right)\right)}{t}dy dt \int_1^r \frac{\exp\left(\frac{t^2}{2}\left(1-y^2\right)\right)}{t} dt = \frac{1}{2}\left[\Gamma\left(0,\frac{y^2-2}{2}\right)-\Gamma\left(0,\frac{r^2(y^2-2)}{2}\right)\right]. \frac{1}{2}\int_{0}^1\left[\Gamma\left(0,\frac{y^2-2}{2}\right)-\Gamma\left(0,\frac{r^2(y^2-2)}{2}\right)\right]dy,","['real-analysis', 'calculus', 'integration', 'multivariable-calculus']"
37,Verification of an inequality involving $|x|^\alpha$,Verification of an inequality involving,|x|^\alpha,"Let $u:\mathbb{R}^n\to \mathbb{R}$ , $\alpha>0$ and $w(x)=|x|^\alpha.$ Then this paper I am reading has the following claim, $$\nabla^2\log w(\nabla u,\nabla u)+\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}\leq 0.$$ I am trying to check if this inequality indeed holds. I first computed $$\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}=\frac{\alpha}{|x|^4}(x\cdot \nabla u)^2$$ since $$\nabla\log w = \frac{\alpha}{|x|^2} x.$$ I am guessing that (the authors did not define this in the paper) $$\nabla^2\log w(\nabla u,\nabla u)=\sum_{i,j}D_{ij}\log wD_iuD_ju$$ where $D_i u$ is the partial derivative wrt to $x_i$ of $u.$ This gives me $$\nabla^2\log w(\nabla u,\nabla u)=\frac{\alpha |\nabla u|^2}{|x|^2}-\frac{2\alpha (x\cdot \nabla u)^2}{|x|^4}$$ since $$D_{ij}\log w = \frac{\alpha}{|x|^2}\delta_{ij}-\frac{2\alpha}{|x|^4}x_i x_j.$$ Therefore, $$\nabla^2\log w(\nabla u,\nabla u)+\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}=\frac{\alpha |\nabla u|^2}{|x|^2}-\frac{\alpha (x\cdot \nabla u)^2}{|x|^4}\\ =\frac{\alpha |\nabla u|^2}{|x|^2}\left(1-\cos^2(\theta_x)\right)\geq 0,$$ where $\theta_x$ is the angle between $x$ and the gradient of $u$ , however, this contradicts the claim in the paper. Where have I made a mistake?","Let , and Then this paper I am reading has the following claim, I am trying to check if this inequality indeed holds. I first computed since I am guessing that (the authors did not define this in the paper) where is the partial derivative wrt to of This gives me since Therefore, where is the angle between and the gradient of , however, this contradicts the claim in the paper. Where have I made a mistake?","u:\mathbb{R}^n\to \mathbb{R} \alpha>0 w(x)=|x|^\alpha. \nabla^2\log w(\nabla u,\nabla u)+\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}\leq 0. \frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}=\frac{\alpha}{|x|^4}(x\cdot \nabla u)^2 \nabla\log w = \frac{\alpha}{|x|^2} x. \nabla^2\log w(\nabla u,\nabla u)=\sum_{i,j}D_{ij}\log wD_iuD_ju D_i u x_i u. \nabla^2\log w(\nabla u,\nabla u)=\frac{\alpha |\nabla u|^2}{|x|^2}-\frac{2\alpha (x\cdot \nabla u)^2}{|x|^4} D_{ij}\log w = \frac{\alpha}{|x|^2}\delta_{ij}-\frac{2\alpha}{|x|^4}x_i x_j. \nabla^2\log w(\nabla u,\nabla u)+\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}=\frac{\alpha |\nabla u|^2}{|x|^2}-\frac{\alpha (x\cdot \nabla u)^2}{|x|^4}\\
=\frac{\alpha |\nabla u|^2}{|x|^2}\left(1-\cos^2(\theta_x)\right)\geq 0, \theta_x x u","['real-analysis', 'multivariable-calculus', 'inequality', 'partial-differential-equations']"
38,Triple integral over the region bounded by a parabolic cylinder and three planes,Triple integral over the region bounded by a parabolic cylinder and three planes,,"In chapter 13 of Stewart's calculus the integral $$\iiint_A 3y \, dx \,dy \,dz,$$ where $A$ is the region bounded by the parabolic cylinder $z = 1-x^2$ and the planes $z = 0,$ $y = 0$ and $y+z=2,$ is computed by projecting $A$ on the $XZ$ plane, which yields $184/35$ as a result. Now, I've tried to compute the integral by using the projection on the $XY$ plane, but I get a different result. I guess the way I've set up the integral is mistaken, but I can't see why. It seems to me that the projection of $A$ on the $XY$ plane is $\pi_{XY}(A) = [-1, 1] \times [0, 2]$ and that, for each $(x, y) \in \pi_{XY}(A),$ the corresponding section is $$ A_{(x, y)} = \begin{cases} [0, 1-x^2] &\text{ if } 0\leq y \leq 1, \\                               [0, 2-y] &\text{ if } 1< y \leq 2.\end{cases}$$ Hence $$ \iiint_A 3y \, dx \, dy \, dz = \int_{-1}^1 \int_0^1 \int_0^{1-x^2}3y \, dz \, dy \, dx+\int_{-1}^1 \int_1^2 \int_0^{2-y} 3y \, dz \, dy \, dx.$$ Edit: There must definitely be something wrong with the limits of integration because my section is not continuous at $y=1,$ but I still can't picture the situation clearly.","In chapter 13 of Stewart's calculus the integral where is the region bounded by the parabolic cylinder and the planes and is computed by projecting on the plane, which yields as a result. Now, I've tried to compute the integral by using the projection on the plane, but I get a different result. I guess the way I've set up the integral is mistaken, but I can't see why. It seems to me that the projection of on the plane is and that, for each the corresponding section is Hence Edit: There must definitely be something wrong with the limits of integration because my section is not continuous at but I still can't picture the situation clearly.","\iiint_A 3y \, dx \,dy \,dz, A z = 1-x^2 z = 0, y = 0 y+z=2, A XZ 184/35 XY A XY \pi_{XY}(A) = [-1, 1] \times [0, 2] (x, y) \in \pi_{XY}(A),  A_{(x, y)} = \begin{cases} [0, 1-x^2] &\text{ if } 0\leq y \leq 1, \\
                              [0, 2-y] &\text{ if } 1< y \leq 2.\end{cases}  \iiint_A 3y \, dx \, dy \, dz = \int_{-1}^1 \int_0^1 \int_0^{1-x^2}3y \, dz \, dy \, dx+\int_{-1}^1 \int_1^2 \int_0^{2-y} 3y \, dz \, dy \, dx. y=1,","['multivariable-calculus', 'multiple-integral']"
39,"Calculate $D^3f $ if $ f(x,y)= e^x cos(y)$",Calculate  if,"D^3f   f(x,y)= e^x cos(y)","I want to know if my answer is correct, please. Let $f(x,y)=e^x cos(y)$ , calculate $D^3f$ . Solution: We have that $Df = [\frac{\partial f}{\partial x} \space \space \frac{\partial f}{\partial y}]$ , then: \begin{equation} Df= \begin{bmatrix} e^{x}cos(y) & -e^{x}sin(y) \\ \end{bmatrix} \end{equation} \begin{equation} D^2f=D(Df)= \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \\ \end{bmatrix} =  \begin{bmatrix} e^{x}cos(y) & -e^{x}sin(y) \\ -e^{x}sin(y) & -e^{x}cos(y) \\ \end{bmatrix} \end{equation} And finally: \begin{equation} D^3f=D(D(Df))= \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \\ \frac{\partial f_3}{\partial x} & \frac{\partial f_3}{\partial y} \\ \frac{\partial f_4}{\partial x} & \frac{\partial f_4}{\partial y} \\ \end{bmatrix} = \begin{bmatrix} e^{x}cos(y) & -e^{x}sin(y) \\ -e^{x}sin(y) & -e^{x}cos(y) \\ -e^{x}sin(y) & -e^{x}cos(y) \\ -e^{x}cos(y) & e^{x}sin(y) \\ \end{bmatrix} \end{equation} Therefore, \begin{equation} D^3f= \begin{bmatrix} e^{x}cos(y) & -e^{x}sin(y) \\ -e^{x}sin(y) & -e^{x}cos(y) \\ -e^{x}sin(y) & -e^{x}cos(y) \\ -e^{x}cos(y) & e^{x}sin(y) \\ \end{bmatrix} \end{equation}","I want to know if my answer is correct, please. Let , calculate . Solution: We have that , then: And finally: Therefore,","f(x,y)=e^x cos(y) D^3f Df = [\frac{\partial f}{\partial x} \space \space \frac{\partial f}{\partial y}] \begin{equation}
Df=
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
\end{bmatrix}
\end{equation} \begin{equation}
D^2f=D(Df)=
\begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \\
\end{bmatrix} = 
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
\end{bmatrix}
\end{equation} \begin{equation}
D^3f=D(D(Df))=
\begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \\
\frac{\partial f_3}{\partial x} & \frac{\partial f_3}{\partial y} \\
\frac{\partial f_4}{\partial x} & \frac{\partial f_4}{\partial y} \\
\end{bmatrix} =
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}cos(y) & e^{x}sin(y) \\
\end{bmatrix}
\end{equation} \begin{equation}
D^3f=
\begin{bmatrix}
e^{x}cos(y) & -e^{x}sin(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}sin(y) & -e^{x}cos(y) \\
-e^{x}cos(y) & e^{x}sin(y) \\
\end{bmatrix}
\end{equation}","['calculus', 'multivariable-calculus', 'solution-verification']"
40,Transform the partial differential equation $(y-z)\frac{∂z}{∂x}+(y+z)\frac{dz}{dy}=0$ [closed],Transform the partial differential equation  [closed],(y-z)\frac{∂z}{∂x}+(y+z)\frac{dz}{dy}=0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Transform the P.D.E. $(y-z)\frac{\partial z}{\partial x}+(y+z)\frac{\partial z}{\partial y}=0$ so that the new equation contains $x$ as a new function, and $u=y-z, v=y+z$ are new independent variables.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Transform the P.D.E. so that the new equation contains as a new function, and are new independent variables.","(y-z)\frac{\partial z}{\partial x}+(y+z)\frac{\partial z}{\partial y}=0 x u=y-z, v=y+z","['multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'transformation']"
41,Bounded maxima for $3\sqrt{xy}$ on $x^2y+y^2+x=3$,Bounded maxima for  on,3\sqrt{xy} x^2y+y^2+x=3,"Consider the function $f(x,y)=3\sqrt{xy}$ : how can I find maxima/minima of $f$ on the domain $D=\{(x,y)\in \mathbb{R}^2\mid x\geq 0, y\geq 0, x^2y+y^2+x\leq 3\}$ ? Obviously, minima occur on the $x$ and $y$ axis, while maxima (which clearly exist) lie on the curve $x^2y+y^2+x=3$ with $0<x<3$ . First of all, I can consider the function $g(x,y)=xy$ instead of $f$ , as they share (I hope) the same points of bounded minima/maxima in the first quadrant. Using Lagrange's multipliers, I get the system $$\left\{\begin{array}{l} y-\lambda xy-\lambda=0\\ x-\lambda x^2-2\lambda y=0\\ x^2y+y^2+x=3 \end{array}\right.$$ I can see that $x=y=1$ , $\lambda=\frac{1}{3}$ is a solution, but I'm not able to prove that it is unique for $0<x<3$ . Another possibility is to obtain $y$ from $x$ in the equation of the curve $x^2y+y^2+x=3$ , by completing the square I get (for positive $y$ ) $y=\frac{\sqrt{x^4-4x+12}-x^2}{2}$ . Then I tryed to study the sign of the derivative of $h(x)=x\cdot \frac{\sqrt{x^4-4x+12}-x^2}{2}$ , again I see that the derivative is $0$ for $x=1$ , but I have no clue how about to prove that this is the unique solution.","Consider the function : how can I find maxima/minima of on the domain ? Obviously, minima occur on the and axis, while maxima (which clearly exist) lie on the curve with . First of all, I can consider the function instead of , as they share (I hope) the same points of bounded minima/maxima in the first quadrant. Using Lagrange's multipliers, I get the system I can see that , is a solution, but I'm not able to prove that it is unique for . Another possibility is to obtain from in the equation of the curve , by completing the square I get (for positive ) . Then I tryed to study the sign of the derivative of , again I see that the derivative is for , but I have no clue how about to prove that this is the unique solution.","f(x,y)=3\sqrt{xy} f D=\{(x,y)\in \mathbb{R}^2\mid x\geq 0, y\geq 0, x^2y+y^2+x\leq 3\} x y x^2y+y^2+x=3 0<x<3 g(x,y)=xy f \left\{\begin{array}{l} y-\lambda xy-\lambda=0\\ x-\lambda x^2-2\lambda y=0\\ x^2y+y^2+x=3 \end{array}\right. x=y=1 \lambda=\frac{1}{3} 0<x<3 y x x^2y+y^2+x=3 y y=\frac{\sqrt{x^4-4x+12}-x^2}{2} h(x)=x\cdot \frac{\sqrt{x^4-4x+12}-x^2}{2} 0 x=1","['multivariable-calculus', 'maxima-minima']"
42,"Green's theorem and translated angular form on the path $\gamma:[0,3\pi/2]\to\Bbb R^2,\gamma(t)=(t,\pi\cos t)$-strange result",Green's theorem and translated angular form on the path -strange result,"\gamma:[0,3\pi/2]\to\Bbb R^2,\gamma(t)=(t,\pi\cos t)","I would like to compute the following integral $$\int_\gamma-\frac{y}{(x-\pi)^2+y^2}dx+\frac{x-\pi}{(x-\pi)^2+y^2}dy,$$ where $\gamma:\left[0,\frac{3\pi}2\right]\to\Bbb R^2,\gamma(t)=(t,\pi\cos t).$ Here is my answer which I would like to verify. The differential $1$ -form $\omega=-\frac{y}{(x-\pi)^2+y^2}dx+\frac{x-\pi}{(x-\pi)^2+y^2}dy$ is a translated angular form centered at $(\pi,0),$ hence $d\omega=0,$ that is $\omega$ is closed. I would like to apply Green's theorem, so, for that purpose, I considered the following paths: $$\gamma_2:[0,1]\to\Bbb R^2,\gamma_2(t)=\left(\frac{3\pi}2,t\right),\\\gamma_3:[0,1]\to\Bbb R^2,\gamma_3(t)=\left(\frac{3\pi}2-\frac{3\pi}2t,\pi\right),\\\gamma_4:[0,2\pi]\to\Bbb R^2,\gamma_4(t)=(\pi+\cos t,-\sin t).$$ First, $$\int_{\gamma_2}\omega=\int_0^1\frac{\frac{3\pi}2-\pi}{\left(\frac{3\pi}2-\pi\right)^2+t^2}dt=\arctan\frac{t}{\frac{\pi}2}\Big|_0^1=\arctan\frac2\pi\\\int_{\gamma_3}\omega=\int_0^1\frac{\pi}{(t-\pi)^2+\pi^2}dt=\arctan\frac{t-\pi}\pi\Big|_0^1=\arctan\frac{1-\pi}\pi+\frac\pi4$$ and $$\int_{\gamma_4}\omega=-2\pi$$ because $\gamma_4$ is closed and negatively oriented. Now, let $D$ be the set ""inside"" the contour $\gamma+\gamma_2+\gamma_3$ and outside the circle $\gamma_4.$ By Green's theorem, $$\begin{aligned}\int_\gamma\omega+\int_{\gamma_2+\gamma_3+\gamma_4}\omega&=\int_{\partial D}\omega=\int_D d\omega=0\\\implies \int_\gamma\omega&=-\int_{\gamma_2+\gamma_3+\gamma_4}\omega=2\pi-\arctan\frac2\pi-\arctan\frac{1-\pi}\pi-\frac\pi4\\&=\frac{7\pi}4-\arctan\left(\tan\left(\arctan\frac2\pi+\arctan\frac{1-\pi}\pi\right)\right)\\&=\frac{7\pi}4-\arctan\left(\frac{\frac2\pi+\frac{1-\pi}\pi}{1-\frac2\pi\cdot\frac{1-\pi}\pi}\right)\\&=\frac{7\pi}4+\arctan\frac{(\pi-3)\pi}{\pi^2+2\pi-2}\\&=\arctan\left(\tan\left(\frac{7\pi}4+\arctan\frac{(\pi-3)\pi}{\pi^2+2\pi-2}\right)\right)\\&=\arctan\left(\frac{-1+\frac{(\pi-3)\pi}{\pi^2+2\pi-2}}{1+\frac{(\pi-3)\pi}{\pi^2+2\pi-2}}\right)\\&=\arctan\frac{2-5\pi}{2\pi^2-\pi-2}\end{aligned}$$ But I'm unsure as I didn't expect such a result. Did I make any mistakes?","I would like to compute the following integral where Here is my answer which I would like to verify. The differential -form is a translated angular form centered at hence that is is closed. I would like to apply Green's theorem, so, for that purpose, I considered the following paths: First, and because is closed and negatively oriented. Now, let be the set ""inside"" the contour and outside the circle By Green's theorem, But I'm unsure as I didn't expect such a result. Did I make any mistakes?","\int_\gamma-\frac{y}{(x-\pi)^2+y^2}dx+\frac{x-\pi}{(x-\pi)^2+y^2}dy, \gamma:\left[0,\frac{3\pi}2\right]\to\Bbb R^2,\gamma(t)=(t,\pi\cos t). 1 \omega=-\frac{y}{(x-\pi)^2+y^2}dx+\frac{x-\pi}{(x-\pi)^2+y^2}dy (\pi,0), d\omega=0, \omega \gamma_2:[0,1]\to\Bbb R^2,\gamma_2(t)=\left(\frac{3\pi}2,t\right),\\\gamma_3:[0,1]\to\Bbb R^2,\gamma_3(t)=\left(\frac{3\pi}2-\frac{3\pi}2t,\pi\right),\\\gamma_4:[0,2\pi]\to\Bbb R^2,\gamma_4(t)=(\pi+\cos t,-\sin t). \int_{\gamma_2}\omega=\int_0^1\frac{\frac{3\pi}2-\pi}{\left(\frac{3\pi}2-\pi\right)^2+t^2}dt=\arctan\frac{t}{\frac{\pi}2}\Big|_0^1=\arctan\frac2\pi\\\int_{\gamma_3}\omega=\int_0^1\frac{\pi}{(t-\pi)^2+\pi^2}dt=\arctan\frac{t-\pi}\pi\Big|_0^1=\arctan\frac{1-\pi}\pi+\frac\pi4 \int_{\gamma_4}\omega=-2\pi \gamma_4 D \gamma+\gamma_2+\gamma_3 \gamma_4. \begin{aligned}\int_\gamma\omega+\int_{\gamma_2+\gamma_3+\gamma_4}\omega&=\int_{\partial D}\omega=\int_D d\omega=0\\\implies \int_\gamma\omega&=-\int_{\gamma_2+\gamma_3+\gamma_4}\omega=2\pi-\arctan\frac2\pi-\arctan\frac{1-\pi}\pi-\frac\pi4\\&=\frac{7\pi}4-\arctan\left(\tan\left(\arctan\frac2\pi+\arctan\frac{1-\pi}\pi\right)\right)\\&=\frac{7\pi}4-\arctan\left(\frac{\frac2\pi+\frac{1-\pi}\pi}{1-\frac2\pi\cdot\frac{1-\pi}\pi}\right)\\&=\frac{7\pi}4+\arctan\frac{(\pi-3)\pi}{\pi^2+2\pi-2}\\&=\arctan\left(\tan\left(\frac{7\pi}4+\arctan\frac{(\pi-3)\pi}{\pi^2+2\pi-2}\right)\right)\\&=\arctan\left(\frac{-1+\frac{(\pi-3)\pi}{\pi^2+2\pi-2}}{1+\frac{(\pi-3)\pi}{\pi^2+2\pi-2}}\right)\\&=\arctan\frac{2-5\pi}{2\pi^2-\pi-2}\end{aligned}","['integration', 'multivariable-calculus', 'solution-verification', 'greens-theorem']"
43,Is the following relation concerning $2D$ integrals true?,Is the following relation concerning  integrals true?,2D,"Fix $d,t>0$ and let $a \in L^2[-d,0]$ and $Y \in L^2[-d,t]$ . Is it true via some change of variable the following relation? $$\int_{(-t) \vee (-d)}^0 \int_{-d}^s a(\xi)Y(\xi-s)d\xi d s+\int_0^t\int_{-d}^{0} I_{[-(t-s),0]}(\xi)a(\xi)Y(s) d \xi ds=\int_0^t\int_{-d}^{0} a(\xi)Y(s+\xi) d \xi ds$$",Fix and let and . Is it true via some change of variable the following relation?,"d,t>0 a \in L^2[-d,0] Y \in L^2[-d,t] \int_{(-t) \vee (-d)}^0 \int_{-d}^s a(\xi)Y(\xi-s)d\xi d s+\int_0^t\int_{-d}^{0} I_{[-(t-s),0]}(\xi)a(\xi)Y(s) d \xi ds=\int_0^t\int_{-d}^{0} a(\xi)Y(s+\xi) d \xi ds","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
44,Are there functions which take themselves as input? [closed],Are there functions which take themselves as input? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Are there functions which takes itself as an argument? Like: $f(x,f(x_{n\pm1}))$ A functions which value at a given point depends on its value on the next,or previous intake value. Or: $f(x,f(x))$ . A function which value at a given point depends on itself at that given point. This seems like nonsense because it should be: $f(x,f(x,f(x,f(x....))$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Are there functions which takes itself as an argument? Like: A functions which value at a given point depends on its value on the next,or previous intake value. Or: . A function which value at a given point depends on itself at that given point. This seems like nonsense because it should be:","f(x,f(x_{n\pm1})) f(x,f(x)) f(x,f(x,f(x,f(x....))","['calculus', 'multivariable-calculus', 'functions', 'recursion']"
45,Determining which double integral has the largest area,Determining which double integral has the largest area,,"Let $f(x,y)$ be a positive function. If the integrals (A) $\int_{0}^{1} \int_{x^2}^{1} f(x,y) dydx$ (B) $\int_{0}^{1} \int_{x^3}^{1} f(x,y) dydx$ (C) $\int_{0}^{1} \int_{0}^{1} f(x,y) dydx$ are ranked from smallest to largest, then (a) (A) < (C) < (B) (b) (B) < (A) < (C) (c) (A) < (B) < (C) (d) (C) < (A) < (B) (e) (B) < (C) < (A) (f) None of the above. For this question, I got the answer (c), and I just want to verify that my work is correct: I graphed the lines $y=x^2, y=x^3, x=0, x=1, y=0, y=1$ $y=x^2$ is green $y=x^3$ is purple So just by looking at this graph, I can tell that (C) is the largest, (B) is the second largest, and (A) is the smallest. So the order should be (A)<(B)<(C) Thus my answer is choice (c). Is this correct?","Let be a positive function. If the integrals (A) (B) (C) are ranked from smallest to largest, then (a) (A) < (C) < (B) (b) (B) < (A) < (C) (c) (A) < (B) < (C) (d) (C) < (A) < (B) (e) (B) < (C) < (A) (f) None of the above. For this question, I got the answer (c), and I just want to verify that my work is correct: I graphed the lines is green is purple So just by looking at this graph, I can tell that (C) is the largest, (B) is the second largest, and (A) is the smallest. So the order should be (A)<(B)<(C) Thus my answer is choice (c). Is this correct?","f(x,y) \int_{0}^{1} \int_{x^2}^{1} f(x,y) dydx \int_{0}^{1} \int_{x^3}^{1} f(x,y) dydx \int_{0}^{1} \int_{0}^{1} f(x,y) dydx y=x^2, y=x^3, x=0, x=1, y=0, y=1 y=x^2 y=x^3",['multivariable-calculus']
46,Overview of basic results about images and preimages,Overview of basic results about images and preimages,,Are there some good overviews of basic facts about images and inverse images of sets under functions?,Are there some good overviews of basic facts about images and inverse images of sets under functions?,,"['functions', 'elementary-set-theory', 'reference-request', 'online-resources', 'faq']"
47,"How to define a bijection between $(0,1)$ and $(0,1]$?",How to define a bijection between  and ?,"(0,1) (0,1]","How to define a bijection between $(0,1)$ and $(0,1]$?   Or any other open and closed intervals? If the intervals are both open like $(-1,2)\text{ and }(-5,4)$ I do a cheap trick (don't know if that's how you're supposed to do it): I make a function $f : (-1, 2)\rightarrow (-5, 4)$ of the form $f(x)=mx+b$ by \begin{align*} -5 = f(-1) &= m(-1)+b \\ 4 = f(2) &= m(2) + b \end{align*} Solving for $m$ and $b$ I find $m=3\text{ and }b=-2$ so then $f(x)=3x-2.$ Then I show that $f$ is a bijection by showing that it is injective and surjective.","How to define a bijection between $(0,1)$ and $(0,1]$?   Or any other open and closed intervals? If the intervals are both open like $(-1,2)\text{ and }(-5,4)$ I do a cheap trick (don't know if that's how you're supposed to do it): I make a function $f : (-1, 2)\rightarrow (-5, 4)$ of the form $f(x)=mx+b$ by \begin{align*} -5 = f(-1) &= m(-1)+b \\ 4 = f(2) &= m(2) + b \end{align*} Solving for $m$ and $b$ I find $m=3\text{ and }b=-2$ so then $f(x)=3x-2.$ Then I show that $f$ is a bijection by showing that it is injective and surjective.",,"['functions', 'elementary-set-theory']"
48,How do I prove that a function is well defined?,How do I prove that a function is well defined?,,"How do you in general prove that a function is well-defined? $$f:X\to Y:x\mapsto f(x)$$ I learned that I need to prove that every point has exactly one image. Does that mean that I need to prove the following two things: Every element in the domain maps to an element in the codomain: $$x\in X \implies f(x)\in Y$$ The same element in the domain maps to the same element in the codomain: $$x=y\implies f(x)=f(y)$$ At the moment I'm trying to prove this function is well-defined: $$f:(\Bbb Z/12\mathbb Z)^∗→(\Bbb Z/4\Bbb Z)^∗:[x]_{12}↦[x]_4 ,$$ but I'm more interested in the general procedure.","How do you in general prove that a function is well-defined? $$f:X\to Y:x\mapsto f(x)$$ I learned that I need to prove that every point has exactly one image. Does that mean that I need to prove the following two things: Every element in the domain maps to an element in the codomain: $$x\in X \implies f(x)\in Y$$ The same element in the domain maps to the same element in the codomain: $$x=y\implies f(x)=f(y)$$ At the moment I'm trying to prove this function is well-defined: $$f:(\Bbb Z/12\mathbb Z)^∗→(\Bbb Z/4\Bbb Z)^∗:[x]_{12}↦[x]_4 ,$$ but I'm more interested in the general procedure.",,"['functions', 'logic', 'proof-writing']"
49,"Find three non-constant, pairwise unequal functions $f,g,h:\mathbb R\to \mathbb R$...","Find three non-constant, pairwise unequal functions ...","f,g,h:\mathbb R\to \mathbb R","I've been stumped by this problem: Find three non-constant, pairwise unequal functions $f,g,h:\mathbb R\to \mathbb R$ such that   $$f\circ g=h$$   $$g\circ h=f$$   $$h\circ f=g$$   or prove that no three such functions exist. I highly suspect, by now, that no non-trivial triplet of functions satisfying the stated property exists... but I don't know how to prove it. How do I prove this, or how do I find these functions if they do exist? All help is appreciated! The functions should also be continuous.","I've been stumped by this problem: Find three non-constant, pairwise unequal functions $f,g,h:\mathbb R\to \mathbb R$ such that   $$f\circ g=h$$   $$g\circ h=f$$   $$h\circ f=g$$   or prove that no three such functions exist. I highly suspect, by now, that no non-trivial triplet of functions satisfying the stated property exists... but I don't know how to prove it. How do I prove this, or how do I find these functions if they do exist? All help is appreciated! The functions should also be continuous.",,"['functions', 'function-and-relation-composition']"
50,Create unique number from 2 numbers,Create unique number from 2 numbers,,"is there some way to create unique number from 2 positive integer numbers? Result must be unique even for these pairs: 2 and 30, 1 and 15, 4 and 60. In general, if I take 2 random numbers result must be unique(or with very high probability unique) Thanks a lot EDIT: calculation is for computer program,so computational complexity is important","is there some way to create unique number from 2 positive integer numbers? Result must be unique even for these pairs: 2 and 30, 1 and 15, 4 and 60. In general, if I take 2 random numbers result must be unique(or with very high probability unique) Thanks a lot EDIT: calculation is for computer program,so computational complexity is important",,"['functions', 'elementary-set-theory']"
51,Is there a way to get trig functions without a calculator?,Is there a way to get trig functions without a calculator?,,"In school, we just started learning about trigonometry, and I was wondering: is there a way to find the sine, cosine, tangent, cosecant, secant, and cotangent of a single angle without using a calculator? Sometimes I don't feel right when I can't do things out myself and let a machine do it when I can't. Or, if you could redirect me to a place that explains how to do it, please do so. My dad said there isn't, but I just had to make sure. Thanks.","In school, we just started learning about trigonometry, and I was wondering: is there a way to find the sine, cosine, tangent, cosecant, secant, and cotangent of a single angle without using a calculator? Sometimes I don't feel right when I can't do things out myself and let a machine do it when I can't. Or, if you could redirect me to a place that explains how to do it, please do so. My dad said there isn't, but I just had to make sure. Thanks.",,"['functions', 'trigonometry', 'algorithms', 'computational-mathematics', 'calculator']"
52,"Big O Notation ""is element of"" or ""is equal""","Big O Notation ""is element of"" or ""is equal""",,"People are always having trouble with ""big $O$"" notation when it comes to how to write it down in a mathematically correct way. Example: you have two functions $n\mapsto f(n) = n^3$ and $n\mapsto g(n) = n^2$ Obviously $f$ is asymptotically faster than $g$. Is it $f(n) = O (g(n))$ or is it $f(n) \in O(g(n))$? My prof says that the first one is wrong but is a very common practice, therefore it is used very offten in books. Although the second one is the right one. Why is that so?","People are always having trouble with ""big $O$"" notation when it comes to how to write it down in a mathematically correct way. Example: you have two functions $n\mapsto f(n) = n^3$ and $n\mapsto g(n) = n^2$ Obviously $f$ is asymptotically faster than $g$. Is it $f(n) = O (g(n))$ or is it $f(n) \in O(g(n))$? My prof says that the first one is wrong but is a very common practice, therefore it is used very offten in books. Although the second one is the right one. Why is that so?",,"['functions', 'asymptotics', 'math-history']"
53,What is an operator in mathematics?,What is an operator in mathematics?,,Could someone please explain the mathematical difference between an operator (not in the programming sense) and a function? Is an operator a function?,Could someone please explain the mathematical difference between an operator (not in the programming sense) and a function? Is an operator a function?,,"['functions', 'terminology', 'operator-theory']"
54,"What does it mean when two functions are ""orthogonal"", why is it important?","What does it mean when two functions are ""orthogonal"", why is it important?",,"I have often come across the concept of orthogonality and orthogonal functions e.g in fourier series the basis functions are cos and sine, and they are orthogonal. For vectors being orthogonal means that they are actually perpendicular such that their dot product is zero. However, I am not sure how sine and cosine are actually orthogonal. They are 90 out of phase, but there must be a different reason why they are considered orthogonal. What is that reason? Does being orthognal really have something to do with geometry i.e 90 degree angels? Why do we want to have orthogonal things so often in maths? especially with transforms like fourier transform, we want to have orthogonal basis. What does that even mean? Is there something magical about things being orthogonal?","I have often come across the concept of orthogonality and orthogonal functions e.g in fourier series the basis functions are cos and sine, and they are orthogonal. For vectors being orthogonal means that they are actually perpendicular such that their dot product is zero. However, I am not sure how sine and cosine are actually orthogonal. They are 90 out of phase, but there must be a different reason why they are considered orthogonal. What is that reason? Does being orthognal really have something to do with geometry i.e 90 degree angels? Why do we want to have orthogonal things so often in maths? especially with transforms like fourier transform, we want to have orthogonal basis. What does that even mean? Is there something magical about things being orthogonal?",,"['functions', 'orthogonality', 'integral-transforms']"
55,Why is there no function with a nonempty domain and an empty range?,Why is there no function with a nonempty domain and an empty range?,,"Let $A$ to be a nonempty set and $B= \emptyset$; then $ A \times B$ is a set. And let $F$ be a function $A \to B$. Then $F \subseteq A \times B$. By the axiom of specification, $F$ must exists (if I didn't mess up something). But the book I'm reading, Elements of Set Theory by Enderton, says that no function could have a nonempty domain and an empty range, and no more detail is given. So my confusion arises. His statement against my proof. The only axiom as far as I know to prove such a set does not exist is the axiom of regularity. But I can't give such a proof. So I need help. I hope someone could clearly explain why such a function doesn't exist, and why this doesn't contradict the axiom of specification. Let me explain my confusion in more detail: First of all, I know that $A \times \emptyset$ is empty. But $ \emptyset \times A$ is also an empty set, yet there is no problem with functions with an empty domain . The book I'm reading defines a function as: ""A function is a relation such that for each $x$ in $\operatorname{dom} F$ there is only one $y$ such that $x \mathop F y$."" and a relation as: ""A relation is a set of ordered pairs."" Now, let me define the function $F$ as: $$F =  \{\langle x,y \rangle \mid x \in A \text{ and } y \in B \text{ and ... other conditions} \}$$  so it's equal to: $$F = \{ \langle x,y \rangle \in A \times B \mid \text{... other conditions} \}.$$ Doesn't that mean that $F$ meets the conditions of the axiom of specification, and therefore exists? What I would specifically like to ask is: How do you define a function $F$ (as precisely as possible)? Why does the argument above not imply that a function $F: A \to B$ always exists? Does $F \subseteq A \times B$ still hold when $B = \emptyset$? (The answer to #2 cannot simply be that $A \times B = \emptyset$, because $ B \times A = \emptyset$ as well, but a function $B \to A$ does exist.)","Let $A$ to be a nonempty set and $B= \emptyset$; then $ A \times B$ is a set. And let $F$ be a function $A \to B$. Then $F \subseteq A \times B$. By the axiom of specification, $F$ must exists (if I didn't mess up something). But the book I'm reading, Elements of Set Theory by Enderton, says that no function could have a nonempty domain and an empty range, and no more detail is given. So my confusion arises. His statement against my proof. The only axiom as far as I know to prove such a set does not exist is the axiom of regularity. But I can't give such a proof. So I need help. I hope someone could clearly explain why such a function doesn't exist, and why this doesn't contradict the axiom of specification. Let me explain my confusion in more detail: First of all, I know that $A \times \emptyset$ is empty. But $ \emptyset \times A$ is also an empty set, yet there is no problem with functions with an empty domain . The book I'm reading defines a function as: ""A function is a relation such that for each $x$ in $\operatorname{dom} F$ there is only one $y$ such that $x \mathop F y$."" and a relation as: ""A relation is a set of ordered pairs."" Now, let me define the function $F$ as: $$F =  \{\langle x,y \rangle \mid x \in A \text{ and } y \in B \text{ and ... other conditions} \}$$  so it's equal to: $$F = \{ \langle x,y \rangle \in A \times B \mid \text{... other conditions} \}.$$ Doesn't that mean that $F$ meets the conditions of the axiom of specification, and therefore exists? What I would specifically like to ask is: How do you define a function $F$ (as precisely as possible)? Why does the argument above not imply that a function $F: A \to B$ always exists? Does $F \subseteq A \times B$ still hold when $B = \emptyset$? (The answer to #2 cannot simply be that $A \times B = \emptyset$, because $ B \times A = \emptyset$ as well, but a function $B \to A$ does exist.)",,"['elementary-set-theory', 'functions']"
56,What numbers can be created by $1-x^2$ and $\frac{x}{2}$?,What numbers can be created by  and ?,1-x^2 \frac{x}{2},"Suppose I have two functions $$f(x)=1-x^2$$ $$g(x)=\frac{x}{2}$$ and the number $1$. If I am allowed to compose these functions as many times as I like and in any order, what numbers can I get to if I must take $1$ as the input? For example, I can obtain $15/16$ by using $$(f\circ g\circ g)(1)=\frac{15}{16}$$ It is obvious that all obtainable numbers are in the set $\mathbb Q\cap [0,1]$, but some numbers in this set are not obtainable, like $5/8$ (which can be easily verified). Can someone identify a set of all obtainable numbers, or at least a better restriction than $\mathbb Q\cap[0,1]$? Or, perhaps, a very general class of numbers which are obtainable?","Suppose I have two functions $$f(x)=1-x^2$$ $$g(x)=\frac{x}{2}$$ and the number $1$. If I am allowed to compose these functions as many times as I like and in any order, what numbers can I get to if I must take $1$ as the input? For example, I can obtain $15/16$ by using $$(f\circ g\circ g)(1)=\frac{15}{16}$$ It is obvious that all obtainable numbers are in the set $\mathbb Q\cap [0,1]$, but some numbers in this set are not obtainable, like $5/8$ (which can be easily verified). Can someone identify a set of all obtainable numbers, or at least a better restriction than $\mathbb Q\cap[0,1]$? Or, perhaps, a very general class of numbers which are obtainable?",,"['functions', 'dynamical-systems', 'recreational-mathematics', 'function-and-relation-composition']"
57,Why is an empty function considered a function?,Why is an empty function considered a function?,,"A function by definition is a set of ordered pairs, and also according the Kuratowski, an ordered pair $(x,y)$ is defined to be $$\{\{x\}, \{x,y\}\}.$$ Given $A\neq \varnothing$, and $\varnothing\colon \varnothing \rightarrow A$. I know $\varnothing \subseteq \varnothing \times A$, but still an empty set is not an ordered pair. How do you explain that an empty function is a function?","A function by definition is a set of ordered pairs, and also according the Kuratowski, an ordered pair $(x,y)$ is defined to be $$\{\{x\}, \{x,y\}\}.$$ Given $A\neq \varnothing$, and $\varnothing\colon \varnothing \rightarrow A$. I know $\varnothing \subseteq \varnothing \times A$, but still an empty set is not an ordered pair. How do you explain that an empty function is a function?",,"['functions', 'elementary-set-theory']"
58,A function in which addition and multiplication behave the same way,A function in which addition and multiplication behave the same way,,"Exponents have a well-known property: $$x^ax^b = x^{a+b}$$ but $$x^{a} + x^{b} \neq x^{a+b}$$ Similarly, $$\log(a) + \log(b) = \log(ab) $$ But $$\log(a)\log(b) \neq \log(ab)$$ So my question is this: Is there a function $f$ on $\mathbb{R}$ or some infinite subset of $\mathbb{R}$ with the following properties $$(1)\quad f(x)f(y) = f(x+y)$$ $$(2)\quad f(x)+f(y) = f(x+y)$$ ie $$(3)\quad f(x)+f(y) = f(x)f(y)$$ It seems that $(2)$ requires the function to be linear...","Exponents have a well-known property: $$x^ax^b = x^{a+b}$$ but $$x^{a} + x^{b} \neq x^{a+b}$$ Similarly, $$\log(a) + \log(b) = \log(ab) $$ But $$\log(a)\log(b) \neq \log(ab)$$ So my question is this: Is there a function $f$ on $\mathbb{R}$ or some infinite subset of $\mathbb{R}$ with the following properties $$(1)\quad f(x)f(y) = f(x+y)$$ $$(2)\quad f(x)+f(y) = f(x+y)$$ ie $$(3)\quad f(x)+f(y) = f(x)f(y)$$ It seems that $(2)$ requires the function to be linear...",,"['functions', 'functional-equations']"
59,"What is the ""fastest"" increasing function that's useful in some area of math?","What is the ""fastest"" increasing function that's useful in some area of math?",,"Context: I just completed the first quarter of an Intro to Real Analysis class, and while I was thinking about how some functions (like $x^2$) aren't uniformly continuous because they, roughly speaking, ""increase too quickly"" (I'm aware of the actual $\varepsilon$-$\delta$ definition), I wondered if there was a ""fastest increasing"" function as its input goes to infinity (discrete or continuous, it doesn't really matter). My Research: After a moment of thought, I realized there wasn't one, since we could merely compose it with itself or square it or tack a factorial on the end or do any number of other things. But I was still curious, so I took to the internet and was able to find things like the Ackerman Function and the Fast-growing Hierarchy on Wikipedia. These functions are cool and all, but, as far as I can tell, they don't serve a purpose other than being functions that increase really quickly. So this led to my question... Question: What is the fastest increasing function that's useful in some area of math? By ""useful"" I mean something similar to how Graham's Number was used in a proof in a non-arbitrary way. I'm wondering about functions that grow incredibly quickly and exist for reasons other than that they grow incredibly quickly. Based on my research, it looks some kind of computer science comes into play with these functions, and so do large ordinals. I don't know a lot about these areas because I'm just a second year right now, so don't assume too much background knowledge please.","Context: I just completed the first quarter of an Intro to Real Analysis class, and while I was thinking about how some functions (like $x^2$) aren't uniformly continuous because they, roughly speaking, ""increase too quickly"" (I'm aware of the actual $\varepsilon$-$\delta$ definition), I wondered if there was a ""fastest increasing"" function as its input goes to infinity (discrete or continuous, it doesn't really matter). My Research: After a moment of thought, I realized there wasn't one, since we could merely compose it with itself or square it or tack a factorial on the end or do any number of other things. But I was still curious, so I took to the internet and was able to find things like the Ackerman Function and the Fast-growing Hierarchy on Wikipedia. These functions are cool and all, but, as far as I can tell, they don't serve a purpose other than being functions that increase really quickly. So this led to my question... Question: What is the fastest increasing function that's useful in some area of math? By ""useful"" I mean something similar to how Graham's Number was used in a proof in a non-arbitrary way. I'm wondering about functions that grow incredibly quickly and exist for reasons other than that they grow incredibly quickly. Based on my research, it looks some kind of computer science comes into play with these functions, and so do large ordinals. I don't know a lot about these areas because I'm just a second year right now, so don't assume too much background knowledge please.",,"['functions', 'recreational-mathematics', 'big-numbers']"
60,How do I divide a function into even and odd sections?,How do I divide a function into even and odd sections?,,"While working on a proof showing that all functions limited to the domain of real numbers can be expressed as a sum of their odd and even components, I stumbled into a troublesome roadblock; namely, I had no clue how one divides the function into these even and odd parts. Looking up a solution for the proof, I found these general formulas for the even and odd parts of a function $f(n)$: $$\begin{align*} f_e(n)&\overset{\Delta}{=}\frac{f(n)+f(-n)}{2}\\ f_o(n)&\overset{\Delta}{=}\frac{f(n)-f(-n)}{2} \end{align*}$$ While I understand that in an even function $f(n) = f(-n)$ and that in an odd function $f(-n) = -f(n)$, I still don't get how these general formulas for the even and odd parts were obtained. Can someone guide me through the logic?","While working on a proof showing that all functions limited to the domain of real numbers can be expressed as a sum of their odd and even components, I stumbled into a troublesome roadblock; namely, I had no clue how one divides the function into these even and odd parts. Looking up a solution for the proof, I found these general formulas for the even and odd parts of a function $f(n)$: $$\begin{align*} f_e(n)&\overset{\Delta}{=}\frac{f(n)+f(-n)}{2}\\ f_o(n)&\overset{\Delta}{=}\frac{f(n)-f(-n)}{2} \end{align*}$$ While I understand that in an even function $f(n) = f(-n)$ and that in an odd function $f(-n) = -f(n)$, I still don't get how these general formulas for the even and odd parts were obtained. Can someone guide me through the logic?",,"['functions', 'even-and-odd-functions']"
61,"How to obtain $f(x)$, if it is known that $f(f(x))=x^2+x$?","How to obtain , if it is known that ?",f(x) f(f(x))=x^2+x,"How to get $f(x)$, if we know that $f(f(x))=x^2+x$? Is there an elementary function $f(x)$ that satisfies the equation?","How to get $f(x)$, if we know that $f(f(x))=x^2+x$? Is there an elementary function $f(x)$ that satisfies the equation?",,"['functions', 'functional-equations']"
62,Which functions satisfy $f^n(x) = f(x)^n$ for some $n \ge 2$?,Which functions satisfy  for some ?,f^n(x) = f(x)^n n \ge 2,"Let $n$ be an integer greater than $1$. The notation $f^n$ is notoriously ambiguous: it means either the $n$-th iterate of $f$ or its $n$-th power. I was wondering when the two interpretations are in fact the same. In other words, if we write $f^n(x)$ for $f(f(\dotsb f(x) \dotsb))$ and $f(x)^n$ for $f(x) \cdot f(x) \dotsb f(x)$: For which functions $f \colon I \to \mathbb R$ is $f^n(x) = f(x)^n$ for all $x \in I$? So far I have been able to show that: The constant functions $f(x) = 0$ and $f(x) = 1$ satisfy the condition for all $n$ and $f(x) = -1$ satisfies the condition for all odd $n$. Also, if $f$ satisfies the condition for $n$, then the only fixed points of $f$ can be either $0$, $1$, or if $n$ is odd also $-1$. The squaring function $f(x) = x^2$ satisfies the condition for $n = 2$ and  is essentially the only non-constant differentiable function to do so. Indeed, $$f(f(x)) = f(x)^2 \implies f'(f(x)) f'(x) = 2 f(x) f'(x)$$ so if we assume $f'(x) \neq 0$ and let $y = f(x)$, we have that $$f'(y) = 2 y \implies f(y) = y^2 + c$$ and furthermore $$(y^2 + c)^2 + c = (y^2 + c)^2 \implies c = 0.$$ (Of course we could then consider also, e.g., $f(x) = x^2$ for $x \ge 0$ and $f(x) = 0$ for $x < 0$.) More generally, $f(x) = x^\sqrt[n-1]{n}$ satisfies the condition for any $n > 2$. These functions are only defined on $\mathbb R^+$: this is why I chose an interval $I$ instead of all $\mathbb R$ as the domain, but I actually don't care if a solution is defined on any other non-trivial subset. Are there any other solutions? If not, how can we prove so? Remark : The question can be generalized to $n \in \mathbb Z$ if we assume $f$ to be invertible and denote by $f^{-n}$ either the $n$-th iterate of its compositional inverse or the $n$-th power of its multiplicative inverse (the cases $n = 0, 1$ are trivial). But this seems to be an even harder problem. For the case $n = -1$ see this question .","Let $n$ be an integer greater than $1$. The notation $f^n$ is notoriously ambiguous: it means either the $n$-th iterate of $f$ or its $n$-th power. I was wondering when the two interpretations are in fact the same. In other words, if we write $f^n(x)$ for $f(f(\dotsb f(x) \dotsb))$ and $f(x)^n$ for $f(x) \cdot f(x) \dotsb f(x)$: For which functions $f \colon I \to \mathbb R$ is $f^n(x) = f(x)^n$ for all $x \in I$? So far I have been able to show that: The constant functions $f(x) = 0$ and $f(x) = 1$ satisfy the condition for all $n$ and $f(x) = -1$ satisfies the condition for all odd $n$. Also, if $f$ satisfies the condition for $n$, then the only fixed points of $f$ can be either $0$, $1$, or if $n$ is odd also $-1$. The squaring function $f(x) = x^2$ satisfies the condition for $n = 2$ and  is essentially the only non-constant differentiable function to do so. Indeed, $$f(f(x)) = f(x)^2 \implies f'(f(x)) f'(x) = 2 f(x) f'(x)$$ so if we assume $f'(x) \neq 0$ and let $y = f(x)$, we have that $$f'(y) = 2 y \implies f(y) = y^2 + c$$ and furthermore $$(y^2 + c)^2 + c = (y^2 + c)^2 \implies c = 0.$$ (Of course we could then consider also, e.g., $f(x) = x^2$ for $x \ge 0$ and $f(x) = 0$ for $x < 0$.) More generally, $f(x) = x^\sqrt[n-1]{n}$ satisfies the condition for any $n > 2$. These functions are only defined on $\mathbb R^+$: this is why I chose an interval $I$ instead of all $\mathbb R$ as the domain, but I actually don't care if a solution is defined on any other non-trivial subset. Are there any other solutions? If not, how can we prove so? Remark : The question can be generalized to $n \in \mathbb Z$ if we assume $f$ to be invertible and denote by $f^{-n}$ either the $n$-th iterate of its compositional inverse or the $n$-th power of its multiplicative inverse (the cases $n = 0, 1$ are trivial). But this seems to be an even harder problem. For the case $n = -1$ see this question .",,"['functions', 'functional-equations', 'function-and-relation-composition']"
63,"Is there a name for the function $\max(x, 0)$?",Is there a name for the function ?,"\max(x, 0)","Is there a name for the function $ \max(x, 0) $ ? For comparison, the function $ \max(x, -x) $ is known as the absolute value or modulus of x, and has its own notation $ |x| $ .","Is there a name for the function ? For comparison, the function is known as the absolute value or modulus of x, and has its own notation ."," \max(x, 0)   \max(x, -x)   |x| ","['functions', 'terminology']"
64,what is the difference between functor and function?,what is the difference between functor and function?,,"As it is, what is the difference between functor and function? As far as I know, they look really similar. And is functor used in set theory? I know that function is used in set theory. Thanks.","As it is, what is the difference between functor and function? As far as I know, they look really similar. And is functor used in set theory? I know that function is used in set theory. Thanks.",,"['functions', 'elementary-set-theory', 'terminology', 'category-theory']"
65,Is there a Cantor-Schroder-Bernstein statement about surjective maps?,Is there a Cantor-Schroder-Bernstein statement about surjective maps?,,"Let $A,B$ be two sets. The Cantor-Schroder-Bernstein states that if there is an injection $f\colon A\to B$ and an injection $g\colon B\to A$, then there exists a bijection $h\colon A\to B$. I was wondering whether the following statements are true (maybe by using the AC if necessary): Suppose $f \colon A\to B$ and $g\colon B\to A$ are both surjective, does this imply that there is a bijection between $A$ and $B$. Suppose either $f\colon A\to B$ or $g\colon  A\to B$ is surjective and the other one injective, does this imply that there is a bijection between $A$ and $B$.","Let $A,B$ be two sets. The Cantor-Schroder-Bernstein states that if there is an injection $f\colon A\to B$ and an injection $g\colon B\to A$, then there exists a bijection $h\colon A\to B$. I was wondering whether the following statements are true (maybe by using the AC if necessary): Suppose $f \colon A\to B$ and $g\colon B\to A$ are both surjective, does this imply that there is a bijection between $A$ and $B$. Suppose either $f\colon A\to B$ or $g\colon  A\to B$ is surjective and the other one injective, does this imply that there is a bijection between $A$ and $B$.",,"['functions', 'set-theory', 'axiom-of-choice']"
66,Is it true that this function $f(n)=n^{13}$?,Is it true that this function ?,f(n)=n^{13},"Assume strictly monotone increasing function;    such that $f:N^{+}\to N^{+}$ , $h$ for all $n\in N^{+}$ , $$f(f(f(n)))=f(f(n))\cdot f(n)\cdot n^{2015}$$ Prove or disprove: $f(n)=n^{13}$ Put $n=1,f(1)=m$ $$f(f(m))=mf(m)$$ Put $n=m$ , $$f(f(f(m)))=f(f(m))f(m)m^{2015}\Longrightarrow f(mf(m))=m^{2016}(f(m))^2$$ What about following?","Assume strictly monotone increasing function;    such that , for all , Prove or disprove: Put Put , What about following?","f:N^{+}\to N^{+} h n\in N^{+} f(f(f(n)))=f(f(n))\cdot f(n)\cdot n^{2015} f(n)=n^{13} n=1,f(1)=m f(f(m))=mf(m) n=m f(f(f(m)))=f(f(m))f(m)m^{2015}\Longrightarrow f(mf(m))=m^{2016}(f(m))^2",['functions']
67,Notation for repeated application of function,Notation for repeated application of function,,"If I have the function $f(x)$ and I want to apply it $n$ times, what is the notation to use? For example, would $f(f(x))$ be $f_2(x)$, $f^2(x)$, or anything less cumbersome than $f(f(x))$? This is important especially since I am trying to couple this with a limit toward infinity.","If I have the function $f(x)$ and I want to apply it $n$ times, what is the notation to use? For example, would $f(f(x))$ be $f_2(x)$, $f^2(x)$, or anything less cumbersome than $f(f(x))$? This is important especially since I am trying to couple this with a limit toward infinity.",,"['functions', 'notation']"
68,How is  $e^x$ read aloud?,How is   read aloud?,e^x,My current research colleague from New Castle told me that I was reading it wrong. I usually read it as e power x . How do you read aloud $e ^ x$? Is it: e raised to x e power x e powered x or e raised to the power x. What is the correct pronunciation?,My current research colleague from New Castle told me that I was reading it wrong. I usually read it as e power x . How do you read aloud $e ^ x$? Is it: e raised to x e power x e powered x or e raised to the power x. What is the correct pronunciation?,,"['functions', 'exponentiation', 'pronunciation']"
69,Is it possible to describe the Collatz function in one formula?,Is it possible to describe the Collatz function in one formula?,,"This is related to Collatz sequence, which is that $$C(n) = \begin{cases} n/2 &\text{if } n \equiv 0 \pmod{2}\\ 3n+1 & \text{if } n\equiv 1 \pmod{2} .\end{cases}$$ Is it possible to describe the Collatz function in one formula? (without modular conditions)","This is related to Collatz sequence, which is that $$C(n) = \begin{cases} n/2 &\text{if } n \equiv 0 \pmod{2}\\ 3n+1 & \text{if } n\equiv 1 \pmod{2} .\end{cases}$$ Is it possible to describe the Collatz function in one formula? (without modular conditions)",,"['elementary-number-theory', 'functions', 'collatz-conjecture']"
70,Is there an intuitive way of visualising complex roots?,Is there an intuitive way of visualising complex roots?,,"Consider the function $f(x)$ such that $f(x) = x^2-4x+13$. By considering the discriminant, it can immediately be seen that the function has no real roots, since $b^2-4ac = (-4)^2-4(13) = -36$ and $-36 < 0 $, and hence the graph looks as follows With no real roots, and hence the $x$-axis is never intercepted. However, with knowledge of complex numbers, it can be seen that the roots are $$x=\frac{4\pm\sqrt{-36}}{2}$$ $$x_1=2+3i$$ $$x_2=2-3i$$ $$\implies (x-2-3i)(x-2+3i)=0$$ and the question on the table is, are there any means of visualising this fact? I know the closest we generally come to visualising complex numbers is argand diagrams, but I've thus far failed to recognise whether or not this can be used to visualise solutions such as these. An example of a visualisation such as this enquires looking at real solutions, so consider when $$x^2-4x+13=\frac{1}{2}x+10$$ Here, the graph of $0=2x^2-9x+6$ indicates the solutions to the system of equations, but is there any way this example can be applied when the roots are complex? Any help is appreciated. Thank you.","Consider the function $f(x)$ such that $f(x) = x^2-4x+13$. By considering the discriminant, it can immediately be seen that the function has no real roots, since $b^2-4ac = (-4)^2-4(13) = -36$ and $-36 < 0 $, and hence the graph looks as follows With no real roots, and hence the $x$-axis is never intercepted. However, with knowledge of complex numbers, it can be seen that the roots are $$x=\frac{4\pm\sqrt{-36}}{2}$$ $$x_1=2+3i$$ $$x_2=2-3i$$ $$\implies (x-2-3i)(x-2+3i)=0$$ and the question on the table is, are there any means of visualising this fact? I know the closest we generally come to visualising complex numbers is argand diagrams, but I've thus far failed to recognise whether or not this can be used to visualise solutions such as these. An example of a visualisation such as this enquires looking at real solutions, so consider when $$x^2-4x+13=\frac{1}{2}x+10$$ Here, the graph of $0=2x^2-9x+6$ indicates the solutions to the system of equations, but is there any way this example can be applied when the roots are complex? Any help is appreciated. Thank you.",,"['functions', 'complex-numbers', 'roots', 'visualization']"
71,Does this pattern have anything to do with derivatives?,Does this pattern have anything to do with derivatives?,,"In 6th grade I was first introduced to the idea of a function in the form of tables. The input would be ""n"" and the output ""$f_n$"" would be some modification of the input. I remember finding a pattern in the function ""f(n)=n^2"". Here is what the table looked like: \begin{array}{|c|c|} \hline n& f_n\\ \hline 1&1 \\ \hline 2&4\\ \hline 3&9\\ \hline 4&16\\ \hline 5&25\\ \hline ...&...\\ \hline n&n^2\\ \hline \end{array} I would then take the outputs $f_n$ and find the differences between each one: $f_n-f_{n-1}$. This would produce: \begin{array}{|c|c|} \hline n& f(n)-f(n-1)\\ \hline 1&1 \\ \hline 2&3\\ \hline 3&5\\ \hline 4&7\\ \hline 5&9\\ \hline ...&...\\ \hline \end{array} Repeating this process (of finding the differences) for the outputs of  $f_n-f_{n-1}$ would yield a continuous string of $2$s. As a 6th grader I called this process 'breaking down the function' and at the time it was just another pattern I had found. Looking back at my work as a freshman in high school, I realize that the end result of 'breaking down a function' corresponds to the penultimate derivative (before the derivative equals zero). For example: breaking down $y=x^3$ gives a continuous string of $6$s, and the third derivative of $x^3$ is 6 (while the 2nd derivative is 6x). Is there any significance to this pattern found by finding the differences between each output of a function over-and-over again? Does it have anything to do with derivatives? I know my question is naive, but I'm only a high school freshman in algebra II. A non-calculus (or intuitively explained calculus concepts) answer would be very helpful [note that I used an online derivative calculator to find the derivatives of these functions and I apologize for any incorrect calculus terminology].","In 6th grade I was first introduced to the idea of a function in the form of tables. The input would be ""n"" and the output ""$f_n$"" would be some modification of the input. I remember finding a pattern in the function ""f(n)=n^2"". Here is what the table looked like: \begin{array}{|c|c|} \hline n& f_n\\ \hline 1&1 \\ \hline 2&4\\ \hline 3&9\\ \hline 4&16\\ \hline 5&25\\ \hline ...&...\\ \hline n&n^2\\ \hline \end{array} I would then take the outputs $f_n$ and find the differences between each one: $f_n-f_{n-1}$. This would produce: \begin{array}{|c|c|} \hline n& f(n)-f(n-1)\\ \hline 1&1 \\ \hline 2&3\\ \hline 3&5\\ \hline 4&7\\ \hline 5&9\\ \hline ...&...\\ \hline \end{array} Repeating this process (of finding the differences) for the outputs of  $f_n-f_{n-1}$ would yield a continuous string of $2$s. As a 6th grader I called this process 'breaking down the function' and at the time it was just another pattern I had found. Looking back at my work as a freshman in high school, I realize that the end result of 'breaking down a function' corresponds to the penultimate derivative (before the derivative equals zero). For example: breaking down $y=x^3$ gives a continuous string of $6$s, and the third derivative of $x^3$ is 6 (while the 2nd derivative is 6x). Is there any significance to this pattern found by finding the differences between each output of a function over-and-over again? Does it have anything to do with derivatives? I know my question is naive, but I'm only a high school freshman in algebra II. A non-calculus (or intuitively explained calculus concepts) answer would be very helpful [note that I used an online derivative calculator to find the derivatives of these functions and I apologize for any incorrect calculus terminology].",,"['functions', 'derivatives']"
72,"What do people mean by ""canonical""?","What do people mean by ""canonical""?",,"So yes, I hear the term ""canonical"" come up quite often. Examples: ""canonical homomorphism"", ""canonical map,  "" etc. What is really meant by this?","So yes, I hear the term ""canonical"" come up quite often. Examples: ""canonical homomorphism"", ""canonical map,  "" etc. What is really meant by this?",,"['functions', 'terminology']"
73,Derivative of a function with respect to another function. [duplicate],Derivative of a function with respect to another function. [duplicate],,"This question already has answers here : differentiate with respect to a function (3 answers) Closed 6 years ago . I want to calculate the derivative of a function with respect to, not a variable, but respect to another function. For example: $$g(x)=2f(x)+x+\log[f(x)]$$ I want to compute $$\frac{\mathrm dg(x)}{\mathrm df(x)}$$ Can I treat $f(x)$ as a variable and derive ""blindly""? If so, I would get $$\frac{\mathrm dg(x)}{\mathrm df(x)}=2+\frac{1}{f(x)}$$ and treat the simple $x$ as a parameter which derivative is zero. Or I should consider other derivation rules?","This question already has answers here : differentiate with respect to a function (3 answers) Closed 6 years ago . I want to calculate the derivative of a function with respect to, not a variable, but respect to another function. For example: $$g(x)=2f(x)+x+\log[f(x)]$$ I want to compute $$\frac{\mathrm dg(x)}{\mathrm df(x)}$$ Can I treat $f(x)$ as a variable and derive ""blindly""? If so, I would get $$\frac{\mathrm dg(x)}{\mathrm df(x)}=2+\frac{1}{f(x)}$$ and treat the simple $x$ as a parameter which derivative is zero. Or I should consider other derivation rules?",,"['functions', 'derivatives', 'self-learning', 'partial-derivative']"
74,How pathological can a convex function be?,How pathological can a convex function be?,,"Let $f : \mathbb R \to \mathbb R$ be convex. How weird can $f$ be? I know $f$ can easily be non-differentiable at finitely many points, for example $f(x) = \sum_{i=1}^n | x - c_i|$. Can it be non-differentiable at infinitely many points? Or on a set of positive measure? My motivation for this is that when I picture an arbitrary convex function I tend to think of very regular ones like $f(x) = x^2$ but I want to make sure that I'm not building my intuition on examples that aren't rich enough.","Let $f : \mathbb R \to \mathbb R$ be convex. How weird can $f$ be? I know $f$ can easily be non-differentiable at finitely many points, for example $f(x) = \sum_{i=1}^n | x - c_i|$. Can it be non-differentiable at infinitely many points? Or on a set of positive measure? My motivation for this is that when I picture an arbitrary convex function I tend to think of very regular ones like $f(x) = x^2$ but I want to make sure that I'm not building my intuition on examples that aren't rich enough.",,"['functions', 'convex-analysis']"
75,When functions commute under composition,When functions commute under composition,,"Today I was thinking about composition of functions.  It has nice properties, its always associative, there is an identity, and if we restrict to bijective functions then we have an inverse. But then I thought about commutativity.  My first intuition was that bijective self maps of a space should commute but then I saw some counter-examples. The symmetric group is only abelian if $n \le 2$ so clearly there need to be more restrictions on functions than bijectivity for them to commute. The only examples I could think of were boring things like multiplying by a constant or maximal tori of groups like $O(n)$ (maybe less boring). My question: In a euclidean space, what are (edit) some nice characterizations of sets of functions that commute? What about in a more general space? Bonus: Is this notion of commutativity important anywhere in analysis?","Today I was thinking about composition of functions.  It has nice properties, its always associative, there is an identity, and if we restrict to bijective functions then we have an inverse. But then I thought about commutativity.  My first intuition was that bijective self maps of a space should commute but then I saw some counter-examples. The symmetric group is only abelian if $n \le 2$ so clearly there need to be more restrictions on functions than bijectivity for them to commute. The only examples I could think of were boring things like multiplying by a constant or maximal tori of groups like $O(n)$ (maybe less boring). My question: In a euclidean space, what are (edit) some nice characterizations of sets of functions that commute? What about in a more general space? Bonus: Is this notion of commutativity important anywhere in analysis?",,['functions']
76,Looking for a function that approximates a parabola,Looking for a function that approximates a parabola,,"I have a shape that is defined by a parabola in a certain range, and a horizontal line outside of that range (see red in figure). I am looking for a single differentiable, without absolute values, non-piecewise, and continuous function that can approximate that shape. I tried a Gaussian-like function (blue), which works well around the maximum, but is too large at the edges. Is there a way to make the blue function more like the red function? Or, is there another function that can do this?","I have a shape that is defined by a parabola in a certain range, and a horizontal line outside of that range (see red in figure). I am looking for a single differentiable, without absolute values, non-piecewise, and continuous function that can approximate that shape. I tried a Gaussian-like function (blue), which works well around the maximum, but is too large at the edges. Is there a way to make the blue function more like the red function? Or, is there another function that can do this?",,"['functions', 'quadratics', 'conic-sections', 'approximation', 'gaussian']"
77,Proving a function is onto and one to one,Proving a function is onto and one to one,,"I'm reading up on how to prove if a function (represented by a formula) is one-to-one or onto, and I'm having some trouble understanding. To prove if a function is one-to-one, it says that I have to show that for elements $a$ and $b$ in set $A$, if $f(a) = f(b)$, then $a = b$. I understand this to mean that if two elements in a domain map to the the same element in a codomain, then for the function to be one-to-one, they must be the same element because by definition, a one-to-one function has at most one element in the domain mapped to a particular element in the co-domain. Did I understand this correctly? Then to prove that the function is onto, I'm reading an example that says ""let's prove that $f: \mathbb{R} \rightarrow  \mathbb{R}$ defined by $f(x) = 5x+2$ is onto, where $\mathbb{R}$ denotes the real numbers. We let $y$ be a typical element of the codomain and set up the equation $y =f(x)$. then, $y = 5x+2$ and solving for $x$ we get $x ={y-2\over 5}$. Since $y$ is a real number, then ${y-2\over 5}$ is a real number and $f({y-2\over 5})=5({y-2\over 5})+2=y.$ I'm not really seeing how that proves anything, so can anybody explain this to me?","I'm reading up on how to prove if a function (represented by a formula) is one-to-one or onto, and I'm having some trouble understanding. To prove if a function is one-to-one, it says that I have to show that for elements $a$ and $b$ in set $A$, if $f(a) = f(b)$, then $a = b$. I understand this to mean that if two elements in a domain map to the the same element in a codomain, then for the function to be one-to-one, they must be the same element because by definition, a one-to-one function has at most one element in the domain mapped to a particular element in the co-domain. Did I understand this correctly? Then to prove that the function is onto, I'm reading an example that says ""let's prove that $f: \mathbb{R} \rightarrow  \mathbb{R}$ defined by $f(x) = 5x+2$ is onto, where $\mathbb{R}$ denotes the real numbers. We let $y$ be a typical element of the codomain and set up the equation $y =f(x)$. then, $y = 5x+2$ and solving for $x$ we get $x ={y-2\over 5}$. Since $y$ is a real number, then ${y-2\over 5}$ is a real number and $f({y-2\over 5})=5({y-2\over 5})+2=y.$ I'm not really seeing how that proves anything, so can anybody explain this to me?",,['functions']
78,What does the function f: x ↦ y mean?,What does the function f: x ↦ y mean?,,"I am doing IGCSE Maths , and am having a few problems with function notation. I understand the form $f(x)$ . What does the form $f: x ↦ y$ mean? Could you also give one or two examples? And, if possible, state your source. Thank you.","I am doing IGCSE Maths , and am having a few problems with function notation. I understand the form . What does the form mean? Could you also give one or two examples? And, if possible, state your source. Thank you.",f(x) f: x ↦ y,"['functions', 'notation']"
79,"Domain, Co-Domain & Range of a Function","Domain, Co-Domain & Range of a Function",,I'm a little confused between the difference between the range & co-domain of a function. Are they not the same thing (i.e. all possible outputs of the function)?,I'm a little confused between the difference between the range & co-domain of a function. Are they not the same thing (i.e. all possible outputs of the function)?,,"['functions', 'terminology']"
80,Show that the maximum value of this nested radical is $\phi-1$,Show that the maximum value of this nested radical is,\phi-1,"I was experimenting on Desmos (as usual), in particular infinite recursions and series. Here is one that was of interest: What is the maximum value of $$F_\infty=\sqrt{\frac{x}{x+\sqrt{\dfrac{x^2}{x-\sqrt{\dfrac{x^3}{x+ \sqrt{ \dfrac{x^4}{x-\cdots}}}}}}}}$$ where the sign alternates and the power in each numerator increases by one? Some observations follow. Let $F_k$ be the nested radical up to $x^k$ . For large nests, say after $k=10$ , the function monotonically increases from zero onwards. It is hopeless to simply rearrange $F_\infty$ since the powers increase each time - we can no longer write $F_\infty$ as a function of itself to be solved. Here is a plot of $F_{15}$ . What is striking is that the largest value of $x$ in the domain of $F_k$ decreases as $k$ increases. Based on the plot, I think that the domain of $F_\infty$ is $[0,1]$ . This is because for large $x$ , the denominator of the square roots will be larger than its successor, which is absurd as we are working only in $\Bbb R$ . Furthermore, I also conjecture that $$\max F_\infty=\phi-1,$$ where $\phi$ is the golden ratio. This seems right as $\max F_{15}=0.6179$ from the plot. EDIT: The problem can be reduced to proving that for $x\in(0,1]$ , $$\frac d{dx}\sqrt{\frac{x^3}{x+\sqrt{\dfrac{x^4}{x-\sqrt{\dfrac{x^5}{x+ \sqrt{ \dfrac{x^6}{x+\cdots}}}}}}}}<1.$$","I was experimenting on Desmos (as usual), in particular infinite recursions and series. Here is one that was of interest: What is the maximum value of where the sign alternates and the power in each numerator increases by one? Some observations follow. Let be the nested radical up to . For large nests, say after , the function monotonically increases from zero onwards. It is hopeless to simply rearrange since the powers increase each time - we can no longer write as a function of itself to be solved. Here is a plot of . What is striking is that the largest value of in the domain of decreases as increases. Based on the plot, I think that the domain of is . This is because for large , the denominator of the square roots will be larger than its successor, which is absurd as we are working only in . Furthermore, I also conjecture that where is the golden ratio. This seems right as from the plot. EDIT: The problem can be reduced to proving that for ,","F_\infty=\sqrt{\frac{x}{x+\sqrt{\dfrac{x^2}{x-\sqrt{\dfrac{x^3}{x+ \sqrt{ \dfrac{x^4}{x-\cdots}}}}}}}} F_k x^k k=10 F_\infty F_\infty F_{15} x F_k k F_\infty [0,1] x \Bbb R \max F_\infty=\phi-1, \phi \max F_{15}=0.6179 x\in(0,1] \frac d{dx}\sqrt{\frac{x^3}{x+\sqrt{\dfrac{x^4}{x-\sqrt{\dfrac{x^5}{x+ \sqrt{ \dfrac{x^6}{x+\cdots}}}}}}}}<1.","['functions', 'recursion', 'maxima-minima', 'nested-radicals', 'golden-ratio']"
81,What is the purpose of a function being surjective?,What is the purpose of a function being surjective?,,"So as far as I understand, a function $f\colon A \to B$ is surjective if and only if for every $b\in B$ there exists $a\in A$ such that $f(a) = b$. My question is when is this actually relevant? Couldn't you arbitrarily define the set $B$ so that any elements never ""used"" are removed from the set, leaving you with a surjective function?","So as far as I understand, a function $f\colon A \to B$ is surjective if and only if for every $b\in B$ there exists $a\in A$ such that $f(a) = b$. My question is when is this actually relevant? Couldn't you arbitrarily define the set $B$ so that any elements never ""used"" are removed from the set, leaving you with a surjective function?",,"['functions', 'elementary-set-theory']"
82,Punch 2000 holes in 2000 polygons with 1000 needles,Punch 2000 holes in 2000 polygons with 1000 needles,,"You have two identical perfectly square pieces of paper. The area of each paper is 1000 units. On each paper, draw 1000 convex, non-overlapping polygons with all polygons having the same area (exactly 1 unit). Obviously, the polygons are covering both papers completely and edges of paper also serve as edges of some polygons). Polygons may have different shapes and number of sides and the drawing on the first paper is completely different from the drawing on the second paper. Now put the first paper on top of the second and align paper edges perfectly. Prove that it is always possible to punch a hole in all 2000 polygons with 1000 needles (each needle goes through both papers). What have I tried? This problem came from my son who likes to torture his father with difficult problems brought back from his math school. My first try was to steal his clever analysis book while he was sleeping and find the right page in the answer section. Alas, this problem had no solution, which basically means that it's either too simple (and I'm too stupid) or it's too difficult. So I decided to read some theory and discovered that I had some pretty huge gaps in my math education. This problem is definitely about functions. You have a set of 1000 polygons on one side and a set of 1000 polygons on the other side. I have to prove that there is a bijective function between these two sets. Needles are just lines connecting the dots. However, all my attempts to construct such function ended miserably. I guess there has to be some clever theorem than can be applied to problems like this one but I would have to read a pretty thick book to find it. Thanks for the hint.","You have two identical perfectly square pieces of paper. The area of each paper is 1000 units. On each paper, draw 1000 convex, non-overlapping polygons with all polygons having the same area (exactly 1 unit). Obviously, the polygons are covering both papers completely and edges of paper also serve as edges of some polygons). Polygons may have different shapes and number of sides and the drawing on the first paper is completely different from the drawing on the second paper. Now put the first paper on top of the second and align paper edges perfectly. Prove that it is always possible to punch a hole in all 2000 polygons with 1000 needles (each needle goes through both papers). What have I tried? This problem came from my son who likes to torture his father with difficult problems brought back from his math school. My first try was to steal his clever analysis book while he was sleeping and find the right page in the answer section. Alas, this problem had no solution, which basically means that it's either too simple (and I'm too stupid) or it's too difficult. So I decided to read some theory and discovered that I had some pretty huge gaps in my math education. This problem is definitely about functions. You have a set of 1000 polygons on one side and a set of 1000 polygons on the other side. I have to prove that there is a bijective function between these two sets. Needles are just lines connecting the dots. However, all my attempts to construct such function ended miserably. I guess there has to be some clever theorem than can be applied to problems like this one but I would have to read a pretty thick book to find it. Thanks for the hint.",,['functions']
83,Characterising functions $f$ that can be written as $f = g \circ g$?,Characterising functions  that can be written as ?,f f = g \circ g,"I'd like to characterise the functions that ‘have square roots’ in the function composition sense. That is, can a given function $f$ be written as $f = g \circ g$ (where $\circ$ is function composition)? For instance, the function $f(x) = x+10$ has a square root $g(x) = x+5$. Similarly, the function $f(x) = 9x$ has a square root $g(x) = 3x$. I don't know if the function $f(x) = x^2 + 1$ has a square root, but I couldn't think of any. Is there a way to determine which functions have square roots? To keep things simpler, I'd be happy just to consider functions $f: \mathbb R \to \mathbb R$.","I'd like to characterise the functions that ‘have square roots’ in the function composition sense. That is, can a given function $f$ be written as $f = g \circ g$ (where $\circ$ is function composition)? For instance, the function $f(x) = x+10$ has a square root $g(x) = x+5$. Similarly, the function $f(x) = 9x$ has a square root $g(x) = 3x$. I don't know if the function $f(x) = x^2 + 1$ has a square root, but I couldn't think of any. Is there a way to determine which functions have square roots? To keep things simpler, I'd be happy just to consider functions $f: \mathbb R \to \mathbb R$.",,"['functions', 'elementary-set-theory', 'function-and-relation-composition']"
84,"Why isn't the inverse of the function $x\mapsto x+\sin(x)$ expressible in terms of ""the functions one finds on a calculator""?","Why isn't the inverse of the function  expressible in terms of ""the functions one finds on a calculator""?",x\mapsto x+\sin(x),"The function $f(x)=x+\sin(x)$ is easily checked to be a bijection from the reals to itself, and so it has a unique inverse $y\mapsto g(y)$ such that $f\circ g=g\circ f$ are both the identity map. Now $g$ will almost certainly be a function which is not expressible using ""the functions in a high-schooler's toolkit"" (by which I guess I mean $\exp$, $\log$, and, if you like, the usual trigonometric functions and their friends like $\sinh$, although of course these can all be of course built from exponentials anyway). For purely recreational reasons (stemming from conversations I've had whilst teaching undergraduates) I'm interested in how one proves this sort of thing. A few years ago I was interested in a related question, and took the trouble to learn some differential Galois theory. My motivation at the time was learning how to prove things like why $h(t):=\int_0^t e^{x^2} dx$ is not expressible in terms of these calculator-button functions (I'm sure there's a better name for them but I'm afraid I don't know it). I've realised that since then I've forgotten most of what I knew, but furthermore I am also unclear about whether this is the way one is supposed to proceed. Is the idea that I come up with some linear differential equation satisfied by $g$ and then apply some differential Galois theory technique? In fact, one of the many things that I have forgotten is the following: if $F$ is a field equipped with a differential operator $D$, and $E/F$ is the field extension obtained by adding a non-zero root of $Dh=ch$, with $c\in F$, then the Galois group of $E/F$ is solvable, whereas the equation itself might not be, in terms of calculator-button functions, if I can't integrate $c$. Can a more enlightened soul explain to me how one is supposed to proceed? I wonder whether I am somehow conflating two ideas and the differential Galois theory business is a red herring, but it seemed simpler to ask rather than continuing to flounder around.","The function $f(x)=x+\sin(x)$ is easily checked to be a bijection from the reals to itself, and so it has a unique inverse $y\mapsto g(y)$ such that $f\circ g=g\circ f$ are both the identity map. Now $g$ will almost certainly be a function which is not expressible using ""the functions in a high-schooler's toolkit"" (by which I guess I mean $\exp$, $\log$, and, if you like, the usual trigonometric functions and their friends like $\sinh$, although of course these can all be of course built from exponentials anyway). For purely recreational reasons (stemming from conversations I've had whilst teaching undergraduates) I'm interested in how one proves this sort of thing. A few years ago I was interested in a related question, and took the trouble to learn some differential Galois theory. My motivation at the time was learning how to prove things like why $h(t):=\int_0^t e^{x^2} dx$ is not expressible in terms of these calculator-button functions (I'm sure there's a better name for them but I'm afraid I don't know it). I've realised that since then I've forgotten most of what I knew, but furthermore I am also unclear about whether this is the way one is supposed to proceed. Is the idea that I come up with some linear differential equation satisfied by $g$ and then apply some differential Galois theory technique? In fact, one of the many things that I have forgotten is the following: if $F$ is a field equipped with a differential operator $D$, and $E/F$ is the field extension obtained by adding a non-zero root of $Dh=ch$, with $c\in F$, then the Galois group of $E/F$ is solvable, whereas the equation itself might not be, in terms of calculator-button functions, if I can't integrate $c$. Can a more enlightened soul explain to me how one is supposed to proceed? I wonder whether I am somehow conflating two ideas and the differential Galois theory business is a red herring, but it seemed simpler to ask rather than continuing to flounder around.",,"['functions', 'field-theory', 'galois-theory', 'closed-form', 'elementary-functions']"
85,Is a function a set or a rule?,Is a function a set or a rule?,,"My textbook says that a function is a set, and that it is a kind of relation, which is also a set. Now: $$f(x) = x+5$$ is called a function, but the above expression is not a set. This is also true for other functions, like trigonometric functions such as $\sin x$ , etc. I have heard arguments that a function is a rule , and it is expressed as a set. But when we say a function is a kind of relation, this directly implies that it is a set. So, what is a function?","My textbook says that a function is a set, and that it is a kind of relation, which is also a set. Now: is called a function, but the above expression is not a set. This is also true for other functions, like trigonometric functions such as , etc. I have heard arguments that a function is a rule , and it is expressed as a set. But when we say a function is a kind of relation, this directly implies that it is a set. So, what is a function?",f(x) = x+5 \sin x,"['functions', 'elementary-set-theory', 'relations']"
86,"Some confusion about what a function ""really is"".","Some confusion about what a function ""really is"".",,"Despite my username, my background is mostly in functional analysis where (at least to my understanding), a function $f$ is considered as a mathematical object in its own right distinctly different from the values it takes under point evaluation (i.e. $f(x)$). Another way of stating this is that the possible values of a function under evaluation are properties of the function, when considered as its own mathematical object. However, I am reading a book about the foundations of mathematics by Kunen and he refers to a function as being identified with its graph (i.e. a set of ordered pairs) in axiomatic set theory. I was under the impression that this definition of a function as a set of ordered pairs was an oversimplification that teachers used in high school that one grew out of past calculus. So anyways, what is the most fundamental definition of a function? Obviously we all (students of mathematics) know what a function is intuitively but formally, I have a hard time swallowing the idea that a function is the same thing as its graph. I realize that the whole point of axiomatic set theory is to make it possible to denominate every mathematical object in terms of sets but I find this definition to be particularly disappointing. I suspect that this is one of those things that just depends on what area one chooses to work in but I'd love to see what thoughts some of the more experienced mathematicians on here can offer.","Despite my username, my background is mostly in functional analysis where (at least to my understanding), a function $f$ is considered as a mathematical object in its own right distinctly different from the values it takes under point evaluation (i.e. $f(x)$). Another way of stating this is that the possible values of a function under evaluation are properties of the function, when considered as its own mathematical object. However, I am reading a book about the foundations of mathematics by Kunen and he refers to a function as being identified with its graph (i.e. a set of ordered pairs) in axiomatic set theory. I was under the impression that this definition of a function as a set of ordered pairs was an oversimplification that teachers used in high school that one grew out of past calculus. So anyways, what is the most fundamental definition of a function? Obviously we all (students of mathematics) know what a function is intuitively but formally, I have a hard time swallowing the idea that a function is the same thing as its graph. I realize that the whole point of axiomatic set theory is to make it possible to denominate every mathematical object in terms of sets but I find this definition to be particularly disappointing. I suspect that this is one of those things that just depends on what area one chooses to work in but I'd love to see what thoughts some of the more experienced mathematicians on here can offer.",,"['functions', 'elementary-set-theory', 'definition', 'philosophy', 'foundations']"
87,How to prove if a function is bijective?,How to prove if a function is bijective?,,"I am having problems being able to formally demonstrate when a function is bijective (and therefore, surjective and injective). Here's an example: How do I prove that $g(x)$ is bijective? \begin{align} f &: \mathbb R \to\mathbb R \\ g &: \mathbb R \to\mathbb R \\ g(x) &= 2f(x) + 3 \end{align} However, I fear I don't really know how to do such. I realize that the above example implies a composition (which makes things slighty harder?). In any case, I don't understand how to prove such (be it a composition or not). For injective, I believe I need to prove that different elements of the codomain have different preimages in the domain . Alright, but, well, how? As for surjective, I think I have to prove that all the elements of the codomain have one, and only one preimage in the domain , right? I don't know how to prove that either! EDIT f is a bijection. Sorry I forgot to say that.","I am having problems being able to formally demonstrate when a function is bijective (and therefore, surjective and injective). Here's an example: How do I prove that $g(x)$ is bijective? \begin{align} f &: \mathbb R \to\mathbb R \\ g &: \mathbb R \to\mathbb R \\ g(x) &= 2f(x) + 3 \end{align} However, I fear I don't really know how to do such. I realize that the above example implies a composition (which makes things slighty harder?). In any case, I don't understand how to prove such (be it a composition or not). For injective, I believe I need to prove that different elements of the codomain have different preimages in the domain . Alright, but, well, how? As for surjective, I think I have to prove that all the elements of the codomain have one, and only one preimage in the domain , right? I don't know how to prove that either! EDIT f is a bijection. Sorry I forgot to say that.",,['functions']
88,"Is there a bijection between $(0,1)$ and $\mathbb{R}$ that preserves rationality?",Is there a bijection between  and  that preserves rationality?,"(0,1) \mathbb{R}","While reading about cardinality, I've seen a few examples of bijections from the open unit interval $(0,1)$ to $\mathbb{R}$, one example being the function defined by $f(x)=\tan\pi(2x-1)/2$. Another geometric example is found by bending the unit interval into a semicircle with center $P$, and mapping a point to its projection from $P$ onto the real line. My question is, is there a bijection between the open unit interval $(0,1)$ and $\mathbb{R}$ such that rationals are mapped to rationals and irrationals are mapped to irrationals? I played around with mappings similar to $x\mapsto 1/x$, but found that this never really had the right range, and using google didn't yield any examples, at least none which I could find. Any examples would be most appreciated, thanks!","While reading about cardinality, I've seen a few examples of bijections from the open unit interval $(0,1)$ to $\mathbb{R}$, one example being the function defined by $f(x)=\tan\pi(2x-1)/2$. Another geometric example is found by bending the unit interval into a semicircle with center $P$, and mapping a point to its projection from $P$ onto the real line. My question is, is there a bijection between the open unit interval $(0,1)$ and $\mathbb{R}$ such that rationals are mapped to rationals and irrationals are mapped to irrationals? I played around with mappings similar to $x\mapsto 1/x$, but found that this never really had the right range, and using google didn't yield any examples, at least none which I could find. Any examples would be most appreciated, thanks!",,"['functions', 'elementary-set-theory']"
89,IMO 1987 - function such that $f(f(n))=n+1987$ [closed],IMO 1987 - function such that  [closed],f(f(n))=n+1987,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Show that there is no function $f: \mathbb{N} \to \mathbb{N}$ such that $$f(f(n))=n+1987, \ \forall n \in \mathbb{N}$$ This is problem 4 from IMO 1987 - see, for example, AoPS .","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Show that there is no function such that This is problem 4 from IMO 1987 - see, for example, AoPS .","f: \mathbb{N} \to \mathbb{N} f(f(n))=n+1987, \ \forall n \in \mathbb{N}","['elementary-number-theory', 'functions', 'induction', 'contest-math', 'functional-equations']"
90,can any continuous function be represented as a sum of convex and concave function?,can any continuous function be represented as a sum of convex and concave function?,,"I read that any continuous function can be represented as a sum of convex and concave function, meaning for all $f(x)$, $f(x) = g(x) + h(x)$ where $g$ is convex and $h$ is concave. There could be infinitely many decompositions of that sort. Anyone knows where I can see a proof or this, or knows of a proof of this? Thanks.","I read that any continuous function can be represented as a sum of convex and concave function, meaning for all $f(x)$, $f(x) = g(x) + h(x)$ where $g$ is convex and $h$ is concave. There could be infinitely many decompositions of that sort. Anyone knows where I can see a proof or this, or knows of a proof of this? Thanks.",,"['functions', 'convex-analysis']"
91,When do two functions become equal?,When do two functions become equal?,,"When do two functions become equal? I have stumbled over this definition of equality of functions in elementary real analysis. Let $X$ and $Y$ be two sets. Let $f:X\rightarrow Y$ and $g:X\rightarrow Y$ be two functions. $f=g$ iff $f(x)=g(x)$ for all $x\in X$. Of course, this definition is so standard and I have no problem with it. However, as far as I know, this definition is actually a theorem. It can be proved in ZFC set theory. A function is just a set of ordered pairs (with some conditions). Two sets are equal if they have the same elements (the Axiom of Extensionality). With this assumption, one can prove the above definition. However, the proof does not require that the ranges of $f$ and $g$ have to be the same. Suppose that the ranges are different, let's say $f:\mathbb{R}\rightarrow\mathbb{R}$ defined by $f(x)=x$ and $g:\mathbb{R}\rightarrow\mathbb{C}$ defined by $g(x)=x$. If we consider these functions as sets of ordered pairs, then they are just the set $\lbrace (x,x):x\in\mathbb{R}\rbrace$. Thus they are equal by the Axiom of Extensionality. However, the equality also requires that if objects $a$ and $b$ are equal, then any property which is true for $a$ is also true for $b$. In this case, we know that $f$ is a surjective, but $g$ isn't. Thus $f$ should not be equal to $g$ and hence a contradiction. But this is not a proof by contradiction to show that the ranges must be equal, everything assumed here is just axioms of ZFC and what is equality itself. So it looks like it is inconsistent. I have searched similar questions in this website, but there is no question or answer that relate to this. The most related answer would be $f$ and $g$ must have the same range so there would be no problem. But logically, this is just an additional assumption to restrict the ability to compare two functions. If they don't have the same range, then you can't compare it, or there will be a paradox. The problem for this answer is, it doesn't solve the above paradox. It just restricts itself to the situation that the paradox won't arise, but the inconsistency is still there. Lastly, I found another way to solve this problem in a set theory book. In the book, when one considers surjection, one have to specify which set the surjection is over. For example, one has to say whether it's surjection over $\mathbb{R}$ or surjection over $\mathbb{C}$. In this case, the paradox won't arise because $f$ and $g$ are both surjective over $\mathbb{R}$ and not surjective over $\mathbb{C}$. Thus the surjective property are the same for $f$ and $g$. The only problem for this answer is, if one consider the property of 'surjection over its range' instead, then this property is true for $f$ but not for $g$, which implies that $f\neq g$ again. When do two functions become equal in general? Can anyone clarify this for me? Thank you in advance.","When do two functions become equal? I have stumbled over this definition of equality of functions in elementary real analysis. Let $X$ and $Y$ be two sets. Let $f:X\rightarrow Y$ and $g:X\rightarrow Y$ be two functions. $f=g$ iff $f(x)=g(x)$ for all $x\in X$. Of course, this definition is so standard and I have no problem with it. However, as far as I know, this definition is actually a theorem. It can be proved in ZFC set theory. A function is just a set of ordered pairs (with some conditions). Two sets are equal if they have the same elements (the Axiom of Extensionality). With this assumption, one can prove the above definition. However, the proof does not require that the ranges of $f$ and $g$ have to be the same. Suppose that the ranges are different, let's say $f:\mathbb{R}\rightarrow\mathbb{R}$ defined by $f(x)=x$ and $g:\mathbb{R}\rightarrow\mathbb{C}$ defined by $g(x)=x$. If we consider these functions as sets of ordered pairs, then they are just the set $\lbrace (x,x):x\in\mathbb{R}\rbrace$. Thus they are equal by the Axiom of Extensionality. However, the equality also requires that if objects $a$ and $b$ are equal, then any property which is true for $a$ is also true for $b$. In this case, we know that $f$ is a surjective, but $g$ isn't. Thus $f$ should not be equal to $g$ and hence a contradiction. But this is not a proof by contradiction to show that the ranges must be equal, everything assumed here is just axioms of ZFC and what is equality itself. So it looks like it is inconsistent. I have searched similar questions in this website, but there is no question or answer that relate to this. The most related answer would be $f$ and $g$ must have the same range so there would be no problem. But logically, this is just an additional assumption to restrict the ability to compare two functions. If they don't have the same range, then you can't compare it, or there will be a paradox. The problem for this answer is, it doesn't solve the above paradox. It just restricts itself to the situation that the paradox won't arise, but the inconsistency is still there. Lastly, I found another way to solve this problem in a set theory book. In the book, when one considers surjection, one have to specify which set the surjection is over. For example, one has to say whether it's surjection over $\mathbb{R}$ or surjection over $\mathbb{C}$. In this case, the paradox won't arise because $f$ and $g$ are both surjective over $\mathbb{R}$ and not surjective over $\mathbb{C}$. Thus the surjective property are the same for $f$ and $g$. The only problem for this answer is, if one consider the property of 'surjection over its range' instead, then this property is true for $f$ but not for $g$, which implies that $f\neq g$ again. When do two functions become equal in general? Can anyone clarify this for me? Thank you in advance.",,"['functions', 'elementary-set-theory']"
92,How to evaluate fractional tetrations?,How to evaluate fractional tetrations?,,"Recently I've come across 'tetration' in my studies of math, and I've become intrigued how they can be evaluated when the ""tetration number"" is not whole. For those who do not know, tetrations are the next in sequence of iteration functions. (The first three being addition, multiplication, and exponentiation, while the proceeding iteration function is pentation) As an example, 2 with a tetration number of 2 is equal to $$2^2$$ 3 with a tetration number of 3 is equal to $$3^{3^3}$$ and so forth. My question is simply, or maybe not so simply, what is the value of a number ""raised"" to a fractional tetration number. What would the value of 3 with a tetration number of 4/3 be? Thanks for anyone's insight","Recently I've come across 'tetration' in my studies of math, and I've become intrigued how they can be evaluated when the ""tetration number"" is not whole. For those who do not know, tetrations are the next in sequence of iteration functions. (The first three being addition, multiplication, and exponentiation, while the proceeding iteration function is pentation) As an example, 2 with a tetration number of 2 is equal to $$2^2$$ 3 with a tetration number of 3 is equal to $$3^{3^3}$$ and so forth. My question is simply, or maybe not so simply, what is the value of a number ""raised"" to a fractional tetration number. What would the value of 3 with a tetration number of 4/3 be? Thanks for anyone's insight",,"['functions', 'exponentiation', 'tetration']"
93,What is the meaning of expressions of the type $f(\cdot)$ (function (dot))?,What is the meaning of expressions of the type  (function (dot))?,f(\cdot),"Simple question, fully expressed in the Title line. Is the dot within the parenthesis intended to mean, ""any possible function""?","Simple question, fully expressed in the Title line. Is the dot within the parenthesis intended to mean, ""any possible function""?",,"['functions', 'notation']"
94,How to represent the floor function using mathematical notation?,How to represent the floor function using mathematical notation?,,"I'm curious as to how the floor function can be defined using mathematical notation. What I mean by this, is, instead of a word-based explanation (i.e. ""The closest integer that is not greater than x""), I'm curious to see the mathematical equivalent of the definition, if that is even possible. For example, a word-based explanation of the factorial function would be ""multiply the number by all integers below it"", whereas a mathematical equivalent of the definition would be $n! = \prod^n_{i=1}i$. So, in summary: how can I show what $\lfloor x \rfloor$ means without words?","I'm curious as to how the floor function can be defined using mathematical notation. What I mean by this, is, instead of a word-based explanation (i.e. ""The closest integer that is not greater than x""), I'm curious to see the mathematical equivalent of the definition, if that is even possible. For example, a word-based explanation of the factorial function would be ""multiply the number by all integers below it"", whereas a mathematical equivalent of the definition would be $n! = \prod^n_{i=1}i$. So, in summary: how can I show what $\lfloor x \rfloor$ means without words?",,"['functions', 'notation', 'definition']"
95,Show that Function Compositions Are Associative,Show that Function Compositions Are Associative,,"My intent is to show that a composition of bijections is also a bijection by showing the existence of an inverse.  But my approach requires the associativity of function composition. Let $f:  X \rightarrow Y, g:  Y \rightarrow Z, h:  Z \rightarrow W$ be functions. $((f \circ g) \circ h)(x) = h((f \circ g)(x)) = h(g(f(x)))$, and $(f \circ (g \circ h))(x) = (g \circ h)(f(x)) = h(g(f(x)))$. However, I am having problems in justifying that the two compositions, $(f \circ g) \circ h$ and $f \circ (g \circ h)$, have the same domain and range.  When I consulted ProofWiki, whose link is at the bottom, I got even more confused.  Specifically, for $(f \circ g) \circ h = f \circ (g \circ h)$ to be defined, ProofWiki requires that dom$g =$ codom$f$ and dom$h =$ codom$g$. First of all, I think that it should be dom$g =$ range$f$ ....  Moreover, as you can see in the example below, you actually have to adjust domains and ranges of $f, g, h$ for the requirement to hold true. Let $f: \mathbb R \rightarrow \mathbb R$ be $f(x) = 2x$, $g: \mathbb R^+ \rightarrow \mathbb R$ be $g(y) = ln(y)$, $h: \mathbb R \rightarrow \mathbb R$ be $h(z) = z - 10$. Then $((f \circ g) \circ h)(x) = ln(2x) - 10 = (f \circ (g \circ h))(x)$, with dom$((f \circ g) \circ h) = \mathbb R^+$ = dom$(f \circ (g \circ h))$.  As a result, we need to set dom$f = \mathbb R^+$, range$f = \mathbb R^+$; dom$g$, range$g$, dom$h$, and range$h$ remain the same.  Am I allowed to do that? This adjustment implies that when we say dom$f = X$, $f$ must be defined for all elements in $X$, but $X$ may not be the entire set of elements for which $f$ is defined. http://www.proofwiki.org/wiki/Composition_of_Mappings_is_Associative","My intent is to show that a composition of bijections is also a bijection by showing the existence of an inverse.  But my approach requires the associativity of function composition. Let $f:  X \rightarrow Y, g:  Y \rightarrow Z, h:  Z \rightarrow W$ be functions. $((f \circ g) \circ h)(x) = h((f \circ g)(x)) = h(g(f(x)))$, and $(f \circ (g \circ h))(x) = (g \circ h)(f(x)) = h(g(f(x)))$. However, I am having problems in justifying that the two compositions, $(f \circ g) \circ h$ and $f \circ (g \circ h)$, have the same domain and range.  When I consulted ProofWiki, whose link is at the bottom, I got even more confused.  Specifically, for $(f \circ g) \circ h = f \circ (g \circ h)$ to be defined, ProofWiki requires that dom$g =$ codom$f$ and dom$h =$ codom$g$. First of all, I think that it should be dom$g =$ range$f$ ....  Moreover, as you can see in the example below, you actually have to adjust domains and ranges of $f, g, h$ for the requirement to hold true. Let $f: \mathbb R \rightarrow \mathbb R$ be $f(x) = 2x$, $g: \mathbb R^+ \rightarrow \mathbb R$ be $g(y) = ln(y)$, $h: \mathbb R \rightarrow \mathbb R$ be $h(z) = z - 10$. Then $((f \circ g) \circ h)(x) = ln(2x) - 10 = (f \circ (g \circ h))(x)$, with dom$((f \circ g) \circ h) = \mathbb R^+$ = dom$(f \circ (g \circ h))$.  As a result, we need to set dom$f = \mathbb R^+$, range$f = \mathbb R^+$; dom$g$, range$g$, dom$h$, and range$h$ remain the same.  Am I allowed to do that? This adjustment implies that when we say dom$f = X$, $f$ must be defined for all elements in $X$, but $X$ may not be the entire set of elements for which $f$ is defined. http://www.proofwiki.org/wiki/Composition_of_Mappings_is_Associative",,"['functions', 'elementary-set-theory', 'function-and-relation-composition']"
96,Are all functions that have an inverse bijective functions?,Are all functions that have an inverse bijective functions?,,"To have an inverse, a function must be injective i.e one-one. Now, I believe the function must be surjective i.e. onto, to have an inverse, since if it is not surjective, the function's inverse's domain will have some elements left out which are not mapped to any element in the range of the function's inverse. So is it true that all functions that have an inverse must be bijective? Thank you.","To have an inverse, a function must be injective i.e one-one. Now, I believe the function must be surjective i.e. onto, to have an inverse, since if it is not surjective, the function's inverse's domain will have some elements left out which are not mapped to any element in the range of the function's inverse. So is it true that all functions that have an inverse must be bijective? Thank you.",,['functions']
97,Find formula from values,Find formula from values,,"Is there any ""algorithm"" or steps to follow to get a formula from a table of values. Example: Using this values: X                Result 1                3 2                5 3                7 4                9 I'd like to obtain: Result = 2X+1 Edit Maybe using excel? Edit 2 Additional info: It is not going to be always a polynomial and it may have several parameters (I think 2).","Is there any ""algorithm"" or steps to follow to get a formula from a table of values. Example: Using this values: X                Result 1                3 2                5 3                7 4                9 I'd like to obtain: Result = 2X+1 Edit Maybe using excel? Edit 2 Additional info: It is not going to be always a polynomial and it may have several parameters (I think 2).",,"['functions', 'approximation', 'interpolation', 'pattern-recognition']"
98,Why does notation for functions seem to be abused and ambiguous?,Why does notation for functions seem to be abused and ambiguous?,,"I really need to clear up a few things about function notation; I can't seem to grasp how to interpret it. As of right now, I know that a function is roughly a mapping between a set $X$ and a set $Y$, where no element of $X$ is paired with more than one element of $Y$. This seems simple enough. I know that this function is commonly denoted by a single letter, such as $f$, $g$, or $h$. I also that when it comes to ""rules"" for function, $f$ denotes the set of mathematical instructions that tell how to find an output in set $Y$ given an input in set $X$. $x$ is the input, $f$ is the function, and $f(x)$ is the result of applying f to an input $x$, i.e., the output. My main question is, why do many authors say call $f(x)$ the function? This really confuses me, since $f(x)$ is a variable for a real number, and not a mapping between two different sets. Following from this, why do some say that an expression such as $2x + 5$ is a function? As stated before, this seems to just be a variable quantity that varies with $x$, but is not a function itself. Finally, if it's true that $x$ is the input, $f$ is the function, and $f(x)$ is the output, then why do we manipulate functions, like $f$, through the output $f(x)$? For example, we have the image of $x$ under $f$, $f(x) = 2x^2 + 5x$. The only way to find $f'$ (the derivative of $f$) is to manipulate $f(x)$. If we're manipulating functions, then why must we reference an input variable $x$ in the process? Why do we have to have $f(x)$ in order to find the derivative of $f$? One of the most confusing aspects about function notation is the differentiation operator. $dy/dx$ represents the ""infinitesimal"" change in $y$ with respect to the ""infinitesimal"" change in $x$, and since $y = f(x)$, we can write $df(x)/dx$. The confusing aspect of this is, we say ""take the derivative of the function $f(x)$""; however, $f(x)$ can't be a function because it is equal to $y$, which is a variable quantity, not a function. To add to the confusion, we say that the differentiation operator $d/dx$ maps a function, $f$, to its derivative, $f'$. However, as with $df(x)/dx$, we need $f(x)$ in order to transform the function $f$ into $f'$. This seems very confusing, because then it seems that the derivative operator, $d/dx$, actually maps $f(x)$ to $f'(x)$, since we need $f(x)$ to calculate the derivative. The differentiation operator is just an example of a more broad frustration with function notation. To recap, I know that $x$ is the input, $f$ is the function, and $f(x)$ is the image of $x$ under $f$, which can often be given by an algebraic expression. I know that $f$ is a mapping, so $f: x \mapsto f(x)$. This means that $f$ is the function that maps $x$ to an output $f(x)$. I've determined this for myself, but I always stumble when I see authors or other people refer to $f(x) = $ ""some expression"" as the function. It is clear that $x$ is a variable of a real number, and $f(x)$ is a variable of a real number that is dependent on $x$. Then, $f$ is the function, the mapping that links $x$ to $f(x)$; yet , people insist on saying that something like $2x + 1$ is a function. Additionally, I know that differentiation is an operator $d/dx: f \mapsto f'$. However, in order to calculate derivatives, we are not given a function $f$, we are given the image of $x$ under $f$, $f(x)$. This means that it seems that the differentiation operator should be $d/dx: f(x) \mapsto f'(x)$. However, I do not think this is right, and it is one of the main points of my confusion. EDIT: Looking at some of the comments, I have one additional question. When we define a function, we usually do so by writing $f: X \rightarrow Y$, such that $f(x) = 5x^2$, for example. My additional question is, why is it necessary to, in order to define the rule for a function, use a variable x as the input in the function? Why don't we define functions like $f(~)$, with no reference to any variables, since we are specifying the action of the function, not the image of $x$ under $f$...","I really need to clear up a few things about function notation; I can't seem to grasp how to interpret it. As of right now, I know that a function is roughly a mapping between a set $X$ and a set $Y$, where no element of $X$ is paired with more than one element of $Y$. This seems simple enough. I know that this function is commonly denoted by a single letter, such as $f$, $g$, or $h$. I also that when it comes to ""rules"" for function, $f$ denotes the set of mathematical instructions that tell how to find an output in set $Y$ given an input in set $X$. $x$ is the input, $f$ is the function, and $f(x)$ is the result of applying f to an input $x$, i.e., the output. My main question is, why do many authors say call $f(x)$ the function? This really confuses me, since $f(x)$ is a variable for a real number, and not a mapping between two different sets. Following from this, why do some say that an expression such as $2x + 5$ is a function? As stated before, this seems to just be a variable quantity that varies with $x$, but is not a function itself. Finally, if it's true that $x$ is the input, $f$ is the function, and $f(x)$ is the output, then why do we manipulate functions, like $f$, through the output $f(x)$? For example, we have the image of $x$ under $f$, $f(x) = 2x^2 + 5x$. The only way to find $f'$ (the derivative of $f$) is to manipulate $f(x)$. If we're manipulating functions, then why must we reference an input variable $x$ in the process? Why do we have to have $f(x)$ in order to find the derivative of $f$? One of the most confusing aspects about function notation is the differentiation operator. $dy/dx$ represents the ""infinitesimal"" change in $y$ with respect to the ""infinitesimal"" change in $x$, and since $y = f(x)$, we can write $df(x)/dx$. The confusing aspect of this is, we say ""take the derivative of the function $f(x)$""; however, $f(x)$ can't be a function because it is equal to $y$, which is a variable quantity, not a function. To add to the confusion, we say that the differentiation operator $d/dx$ maps a function, $f$, to its derivative, $f'$. However, as with $df(x)/dx$, we need $f(x)$ in order to transform the function $f$ into $f'$. This seems very confusing, because then it seems that the derivative operator, $d/dx$, actually maps $f(x)$ to $f'(x)$, since we need $f(x)$ to calculate the derivative. The differentiation operator is just an example of a more broad frustration with function notation. To recap, I know that $x$ is the input, $f$ is the function, and $f(x)$ is the image of $x$ under $f$, which can often be given by an algebraic expression. I know that $f$ is a mapping, so $f: x \mapsto f(x)$. This means that $f$ is the function that maps $x$ to an output $f(x)$. I've determined this for myself, but I always stumble when I see authors or other people refer to $f(x) = $ ""some expression"" as the function. It is clear that $x$ is a variable of a real number, and $f(x)$ is a variable of a real number that is dependent on $x$. Then, $f$ is the function, the mapping that links $x$ to $f(x)$; yet , people insist on saying that something like $2x + 1$ is a function. Additionally, I know that differentiation is an operator $d/dx: f \mapsto f'$. However, in order to calculate derivatives, we are not given a function $f$, we are given the image of $x$ under $f$, $f(x)$. This means that it seems that the differentiation operator should be $d/dx: f(x) \mapsto f'(x)$. However, I do not think this is right, and it is one of the main points of my confusion. EDIT: Looking at some of the comments, I have one additional question. When we define a function, we usually do so by writing $f: X \rightarrow Y$, such that $f(x) = 5x^2$, for example. My additional question is, why is it necessary to, in order to define the rule for a function, use a variable x as the input in the function? Why don't we define functions like $f(~)$, with no reference to any variables, since we are specifying the action of the function, not the image of $x$ under $f$...",,"['functions', 'notation']"
99,"Functions without complex roots, but with quaternion roots","Functions without complex roots, but with quaternion roots",,"Many introductions to complex numbers begin with the question ""What are the roots of $x^2 + 1 = 0$ ?"" This function does not have real roots, but does have complex roots. Are there functions which, in a similar vein, do not have complex roots but do have roots in the quaternions?","Many introductions to complex numbers begin with the question ""What are the roots of ?"" This function does not have real roots, but does have complex roots. Are there functions which, in a similar vein, do not have complex roots but do have roots in the quaternions?",x^2 + 1 = 0,"['functions', 'complex-numbers', 'quaternions']"
