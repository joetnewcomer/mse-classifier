,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the limit $\lim_{n\to \infty} \cos(n)^{n^2} $,What is the limit,\lim_{n\to \infty} \cos(n)^{n^2} ,"Two years ago I found this question and wasn't able to make the slightest advancement. $\lim_{n\to \infty} \cos(n)^{(n²)} $ The question is does the limit exist, where n is natural only (of course it does not exist for n being real). Up until now, I noticed the limit does not exist only if there are peaks of cosine in natural numbers, that peaks ""more"" than raising to n² power. Since cosine peaks in $2\pi k$ , the question arising is ""how good can we approximate multiples of $\pi$ using a natural number, or asking what is the best rational approximation for $\pi$ .","Two years ago I found this question and wasn't able to make the slightest advancement. The question is does the limit exist, where n is natural only (of course it does not exist for n being real). Up until now, I noticed the limit does not exist only if there are peaks of cosine in natural numbers, that peaks ""more"" than raising to n² power. Since cosine peaks in , the question arising is ""how good can we approximate multiples of using a natural number, or asking what is the best rational approximation for .",\lim_{n\to \infty} \cos(n)^{(n²)}  2\pi k \pi \pi,"['calculus', 'limits', 'trigonometry', 'irrational-numbers', 'pi']"
1,How to evaluate $\displaystyle \int_{-\pi/2}^{\pi/2} f(x) dx$ where $f(x)=\cos(x)+\sin(f(x))$,How to evaluate  where,\displaystyle \int_{-\pi/2}^{\pi/2} f(x) dx f(x)=\cos(x)+\sin(f(x)),"So I want to find the area of this circular looking thing. I had the following thought process in solving it. Consider the implicit derivative of the function. $\begin{align} y &= \cos(x)+\sin(y) \\ \mathrm{d}y &= -\mathrm{d}x \sin(x)+\mathrm{d}y \cos(y) \\ \mathrm{d}y (\cos (y) -1) &= \mathrm{d}x \sin (x) \\ \dfrac{\mathrm{d}y}{\mathrm{d}x} &= \dfrac{\sin (x)}{\cos(y)-1} \end{align}$ Then consider a parametric equation $r(t)=\begin{bmatrix} x(t) \\ y(t) \end{bmatrix}$ that lines up with the implicit differentiation above i.e. $r'(t)=\begin{bmatrix} x'(t) \\ y'(t) \end{bmatrix}=\begin{bmatrix} \cos(y(t))-1 \\ \sin(x(t)) \end{bmatrix}$ . This is what the vector field $\begin{bmatrix} x \\ y \end{bmatrix} \to \begin{bmatrix} \cos(y)-1 \\ \sin(x) \end{bmatrix}$ looks like superimposed on the graph. The derivative vectors go anticlockwise relative to the graph, which gave me an idea. Take Green's theorem: $\displaystyle \oint_C \vec{F} \cdot \vec{dr} = \iint_R \nabla \times \vec{F} dA$ and apply it to this problem in an interesting fashion. If $\vec{F}(x,y) = \begin{bmatrix}0\\x\end{bmatrix}$ , then $\nabla \times \vec{F} = 1$ which means I can use the line integral to calculate the area. I should maybe point out that this is 2D curl, which in 3D would technically be $\hat{k}$ . I got stuck at this assortment of half-ideas. Of course I can write the equation in terms of x like this: $f(y) = \cos^{-1} (y - \sin(y))$ , do  approximations to get the upper y bound at the crest of the thing like this: $a = \cos(0) + \sin(a) \implies a = 1 + \sin a$ then approximate an integral to get the answer like this: $\text{Area} = 2\displaystyle \int_0^a \cos^{-1} (y - \sin(y)) \mathrm{d}y$ , which gives me this answer to several decimal places: $4.9296448613689990829170456909320900725529417147308463240261037884$ . But that's boring. Is there any other way?","So I want to find the area of this circular looking thing. I had the following thought process in solving it. Consider the implicit derivative of the function. Then consider a parametric equation that lines up with the implicit differentiation above i.e. . This is what the vector field looks like superimposed on the graph. The derivative vectors go anticlockwise relative to the graph, which gave me an idea. Take Green's theorem: and apply it to this problem in an interesting fashion. If , then which means I can use the line integral to calculate the area. I should maybe point out that this is 2D curl, which in 3D would technically be . I got stuck at this assortment of half-ideas. Of course I can write the equation in terms of x like this: , do  approximations to get the upper y bound at the crest of the thing like this: then approximate an integral to get the answer like this: , which gives me this answer to several decimal places: . But that's boring. Is there any other way?","\begin{align} y &= \cos(x)+\sin(y) \\ \mathrm{d}y &= -\mathrm{d}x \sin(x)+\mathrm{d}y \cos(y) \\ \mathrm{d}y (\cos (y) -1) &= \mathrm{d}x \sin (x) \\ \dfrac{\mathrm{d}y}{\mathrm{d}x} &= \dfrac{\sin (x)}{\cos(y)-1} \end{align} r(t)=\begin{bmatrix} x(t) \\ y(t) \end{bmatrix} r'(t)=\begin{bmatrix} x'(t) \\ y'(t) \end{bmatrix}=\begin{bmatrix} \cos(y(t))-1 \\ \sin(x(t)) \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \to \begin{bmatrix} \cos(y)-1 \\ \sin(x) \end{bmatrix} \displaystyle \oint_C \vec{F} \cdot \vec{dr} = \iint_R \nabla \times \vec{F} dA \vec{F}(x,y) = \begin{bmatrix}0\\x\end{bmatrix} \nabla \times \vec{F} = 1 \hat{k} f(y) = \cos^{-1} (y - \sin(y)) a = \cos(0) + \sin(a) \implies a = 1 + \sin a \text{Area} = 2\displaystyle \int_0^a \cos^{-1} (y - \sin(y)) \mathrm{d}y 4.9296448613689990829170456909320900725529417147308463240261037884","['calculus', 'multivariable-calculus', 'partial-differential-equations', 'functional-equations', 'greens-theorem']"
2,Closed-Form solution for nested integrals of this polynomial?,Closed-Form solution for nested integrals of this polynomial?,,"I was wondering whether there is a closed-form solution for this (nested) integral: $$ \int_{-1}^{1}\int_{t_{0}}^{1}\int_{t_{1}}^{1}...\int_{t_{a-2}}^{1}\prod_{\begin{array}{c} i<j\\ j=\{0,..,a-1\}\\ i=\{0,..,a-1\} \end{array}}\left(t_{i}-t_{j}\right)^{4}dt_{a-1}dt_{a-2}...dt_{0} $$ These are the results I get for $a=2$ and $a=3$ : $a=2$ : $$ \int _{-1}^1\int _{t_0}^1(t_0-t_1)^4 dt_1 dt_0 = \frac{32}{15} $$ $a=3$ : $$ \int _{-1}^1\int _{t_0}^1\int _{t_1}^1(t_0-t_1)^4 (t_0-t_2)^4 (t_1-t_2)^4dt_2dt_1dt_0 = \frac{8192}{33075} $$ Is there a known closed-form solution $\forall a$ ? Edit : As noted by  @Steven Stadnicki in the comments, the function inside the integrals can also be written as $$ \prod_{\begin{array}{c} i\ne j\\ j=\{0,..,a-1\}\\ i=\{0,..,a-1\} \end{array}}(t_{i}-t_{j})^{2} $$ Thanks!","I was wondering whether there is a closed-form solution for this (nested) integral: These are the results I get for and : : : Is there a known closed-form solution ? Edit : As noted by  @Steven Stadnicki in the comments, the function inside the integrals can also be written as Thanks!","
\int_{-1}^{1}\int_{t_{0}}^{1}\int_{t_{1}}^{1}...\int_{t_{a-2}}^{1}\prod_{\begin{array}{c}
i<j\\
j=\{0,..,a-1\}\\
i=\{0,..,a-1\}
\end{array}}\left(t_{i}-t_{j}\right)^{4}dt_{a-1}dt_{a-2}...dt_{0}
 a=2 a=3 a=2 
\int _{-1}^1\int _{t_0}^1(t_0-t_1)^4 dt_1 dt_0 = \frac{32}{15}
 a=3 
\int _{-1}^1\int _{t_0}^1\int _{t_1}^1(t_0-t_1)^4 (t_0-t_2)^4 (t_1-t_2)^4dt_2dt_1dt_0 = \frac{8192}{33075}
 \forall a 
\prod_{\begin{array}{c}
i\ne j\\
j=\{0,..,a-1\}\\
i=\{0,..,a-1\}
\end{array}}(t_{i}-t_{j})^{2}
","['calculus', 'integration', 'definite-integrals', 'recurrence-relations', 'closed-form']"
3,"Prove that $\forall d > 0$ there exists $x_1, x_2 \in \mathbb{R}$ such that $x_1 - x_2 = d$ and $f(x_1) = f(x_2)$",Prove that  there exists  such that  and,"\forall d > 0 x_1, x_2 \in \mathbb{R} x_1 - x_2 = d f(x_1) = f(x_2)","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function with the property that $\lim_{x \rightarrow \infty} f(x)$ and $\lim_{x \rightarrow -\infty} f(x)$ exist and are equal. Prove that $\forall d > 0$ there exists $x_1, x_2 \in \mathbb{R}$ such that $x_1 - x_2 = d$ and $f(x_1) = f(x_2)$. I applied Rolle's theorem on $\mathbb{R}$ since $\lim_{x \rightarrow \infty} f(x)= \lim_{x \rightarrow -\infty} f(x)$. So, there exists $c \in \mathbb{R}$ such that $f'(c) = 0$. This means that our function has a minimum or a maximum. Now, since the limits at $\infty$ and $-\infty$ are equal we can conclude that $\forall d > 0$ there exists $x_1, x_2 \in \mathbb{R}$ such that $x_1 - x_2 = d$ and $f(x_1) = f(x_2)$. Is this a correct solution? If not, please help me find a good one. Thank you!","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function with the property that $\lim_{x \rightarrow \infty} f(x)$ and $\lim_{x \rightarrow -\infty} f(x)$ exist and are equal. Prove that $\forall d > 0$ there exists $x_1, x_2 \in \mathbb{R}$ such that $x_1 - x_2 = d$ and $f(x_1) = f(x_2)$. I applied Rolle's theorem on $\mathbb{R}$ since $\lim_{x \rightarrow \infty} f(x)= \lim_{x \rightarrow -\infty} f(x)$. So, there exists $c \in \mathbb{R}$ such that $f'(c) = 0$. This means that our function has a minimum or a maximum. Now, since the limits at $\infty$ and $-\infty$ are equal we can conclude that $\forall d > 0$ there exists $x_1, x_2 \in \mathbb{R}$ such that $x_1 - x_2 = d$ and $f(x_1) = f(x_2)$. Is this a correct solution? If not, please help me find a good one. Thank you!",,"['calculus', 'continuity']"
4,Compute multivariate complex Gaussian integral,Compute multivariate complex Gaussian integral,,"I don't know how to work out the homework of Leib&Loss P121, Ex4(b), in which we need to compute the following $$ \int_{\mathbb{R}^n}\exp(-x^tAx)dx=\pi^{n/2}/\sqrt{\det A} $$ where $A=A^t$ is a symmetric (thank Paul, see the comments) complex matrix with positive definite real part. It hints to use something like continuous extension, but I don't know how to do this? UPDATE Since it is easy to show in case $A$ is real, I try to show that $$ F(t)=\int_{\mathbb{R}^n}\exp(-x^t(A+tBi)x)dx-\pi^{n/2}/\sqrt{\det (A+tBi)} , $$ is independent to $t$ , the DCT make us differentiate under the integral, but I can't show that $F'(t)=0$ .","I don't know how to work out the homework of Leib&Loss P121, Ex4(b), in which we need to compute the following where is a symmetric (thank Paul, see the comments) complex matrix with positive definite real part. It hints to use something like continuous extension, but I don't know how to do this? UPDATE Since it is easy to show in case is real, I try to show that is independent to , the DCT make us differentiate under the integral, but I can't show that .","
\int_{\mathbb{R}^n}\exp(-x^tAx)dx=\pi^{n/2}/\sqrt{\det A}
 A=A^t A 
F(t)=\int_{\mathbb{R}^n}\exp(-x^t(A+tBi)x)dx-\pi^{n/2}/\sqrt{\det (A+tBi)}
,
 t F'(t)=0","['calculus', 'complex-analysis', 'contour-integration', 'gaussian-integral']"
5,An $\operatorname{erfi}(x)e^{-x^2}$ integral,An  integral,\operatorname{erfi}(x)e^{-x^2},"I want to find an elementary evaluation of $$I=\int_0^\infty \left(\frac{\sqrt\pi}2\operatorname{erfi}(x)e^{-x^2}-\frac1{1+2x}\right)dx$$   where $\operatorname{erfi}(x)=\frac{2}{\sqrt\pi}\int_0^xe^{t^2}dt$. Rough Solution $$I=\int_0^\infty\left({}_1F_1(1;3/2,-x^2)x-\frac1{1+2x}\right)dx$$ $$=\left(\frac{x^2}2{}_2F_2(1,1;3/2,2,-x^2)-\frac12\ln(1+2x)\right)\Bigg|_0^\infty$$ By using the asymptotic expansion of $_2F_2$ I can get the answer is $\frac{\gamma}4$, where $\gamma$ is the Euler's constant. I wonder if there is a elementary proof without using hypergeometric function.","I want to find an elementary evaluation of $$I=\int_0^\infty \left(\frac{\sqrt\pi}2\operatorname{erfi}(x)e^{-x^2}-\frac1{1+2x}\right)dx$$   where $\operatorname{erfi}(x)=\frac{2}{\sqrt\pi}\int_0^xe^{t^2}dt$. Rough Solution $$I=\int_0^\infty\left({}_1F_1(1;3/2,-x^2)x-\frac1{1+2x}\right)dx$$ $$=\left(\frac{x^2}2{}_2F_2(1,1;3/2,2,-x^2)-\frac12\ln(1+2x)\right)\Bigg|_0^\infty$$ By using the asymptotic expansion of $_2F_2$ I can get the answer is $\frac{\gamma}4$, where $\gamma$ is the Euler's constant. I wonder if there is a elementary proof without using hypergeometric function.",,"['calculus', 'integration', 'definite-integrals', 'special-functions']"
6,Finding the sum $\sum_{k=1}^rk^2\binom {n-k}{r-k}$,Finding the sum,\sum_{k=1}^rk^2\binom {n-k}{r-k},"I was stuck while finding the given summation. $$\sum_{k=1}^rk^2\binom {n-k}{r-k}$$ Since $n$ and $r$ are both constants, so I have first converted the above summation into this: $$\sum_{k=1}^rk^2\binom {n-k}{n-r}$$ I have no idea how to proceed next. Any help will be appreciated.","I was stuck while finding the given summation. $$\sum_{k=1}^rk^2\binom {n-k}{r-k}$$ Since $n$ and $r$ are both constants, so I have first converted the above summation into this: $$\sum_{k=1}^rk^2\binom {n-k}{n-r}$$ I have no idea how to proceed next. Any help will be appreciated.",,"['calculus', 'combinatorics', 'binomial-coefficients']"
7,How to define the inverse of a vector?,How to define the inverse of a vector?,,"Most physical situations in mechanics can be modeled using a combination of derivatives - specifically, derivatives of position: velocity and acceleration. But physical situations can also be modeled other ways. Consider the scalar equation for velocity in one dimension: $v = \frac{dx}{dt}$ it can be modeled just as well by a different quantity, called ""slowness"" which is described as: $s = \frac{dt}{dx} = \frac{1}{v}$ Which is used commonly in day-to-day life. For example, runners usually measure distance in minutes per mile, or minutes per km. Walkers go slow enough such that it doesn't make sense to measure speed in miles/hour or a similar unit, but instead to say they take ~20mins per mile. However, this is rarely, if ever, used in physics. In order to model slowness physically, there are a few basic things to know. Firstly, when we want to add velocities, they add quite nicely. If I stand on top of a flat bed truck and run at 5m/s while the truck is going 10m/s (disregarding special relativity) my total speed is 5+10=15m/s. For a slowness, we have to take advantage of knowing this fact to figure out how to ""add"" slownesses together. We know $v_1+v_2=v_t$ , so if $s_1=\frac{1}{v_1}$ and $s_2=\frac{1}{v_2}$ and $s_t=\frac{1}{v_t}$ then $\frac{1}{s_1}+\frac{1}{s_2}=\frac{1}{s_t}$ and therefore: $\frac{1}{\frac{1}{s_1}+\frac{1}{s_2}}=s_t$ defining an operation associated with this (called ""oplus"" - discussed in detail here ): $x \oplus y := \frac{1}{\frac{1}{x}+\frac{1}{y}}$ and its inverse (""ominus"") $x \ominus y := \frac{1}{\frac{1}{x}-\frac{1}{y}} = x \oplus (-y)$ we have $s_1 \oplus s_2 = s_t$ , so while velocities add, slownesses oplus. It seems slowness is not easily measured in vector form, even though it represents the same physical quantity as velocity and therefore has both magnitude and direction. The vector version of slowness should (I think) fulfill three requirements: Preserve direction (point same direction as velocity vector) Invert magnitude (magnitude of slowness should be 1/speed) Coordinate independence (slowness in the x-direction doesn't effect y-direction, etc) There is only one vector which satisfies the first two is the vector $\frac{\vec{v}}{|\vec{v}|^2}$ which unfortunately doesn't also satisfy the third requirement, because the magnitude of $\vec{v}$ is affected by all coordinates of $\vec{v}$ . For example, if $\vec{v}=\langle1,2\rangle$ then $\frac{\vec{v}}{|\vec{v}|^2}=\langle\frac{1}{\sqrt{5}},\frac{2}{\sqrt{5}}\rangle$ but if $\vec{v}=\langle1,3\rangle$ then $\frac{\vec{v}}{|\vec{v}|^2}=\langle\frac{1}{\sqrt{10}},\frac{3}{\sqrt{10}}\rangle$ which means just changing the y coordinate also changed the x coordinate of the slowness. In order to preserve coordinate independence, as well as stay consistent with the one-dimensional definition of slowness, one can define a ""vector"" for slowness as taking the reciprocal of each component. So if $\vec{v}=\langle x,y,z\rangle$ then $\vec{s}=\langle\frac{1}{x},\frac{1}{y},\frac{1}{z}\rangle$ . The initial problem is that it appears to not preserve direction or invert the magnitude of the velocity vector. However it requires changing a fundamental vector property. It comes down to how we measure distance in a coordinate system, and the operations we use on vectors. We all know that vectors add together, which makes sense since velocity and position do the same, and those things add when they are scalars. One problem with defining slowness as a vector may be that slowness does not satisfy vector properties, even in one dimension! Slownesses do not add, they oplus (as shown above). So instead of defining slowness, which is the reciprocal of slowness, as a vector, why not define it as something else? Something which is like a vector, but more readily taking advantage of its properties. For example, it could be different in how we measure its magnitude, as well as other properties: Given $\vec{v}=\langle x,y,z \rangle$ $\frac{1}{\vec{v}} := [x^{-1},y^{-1},z^{-1}]$ <-- not a vector, instead could be called an ""inverse vector"" or ""invector"", denoted by square brackets $|\frac{1}{\vec{v}}| := \sqrt{(\frac{1}{x})^2\oplus(\frac{1}{y})^2\oplus(\frac{1}{z})^2}$ This appears to behave quite nicely, since $|\frac{1}{\vec{v}}|=\sqrt{(\frac{1}{x})^2\oplus(\frac{1}{y})^2\oplus(\frac{1}{z})^2} = \sqrt{\frac{1}{x^2+y^2+z^2}} = \frac{1}{\sqrt{x^2+y^2+z^2}} = \frac{1}{|\vec{v}|}$ Which satisfies requirement #2. Through this definition, requirement #1 can also be satisfied, provided we change how we measure distance. Most people are familiar with a graph on a logarithmic scale. This helps visualize data which grows exponentially by changing where the numbers are located geometrically on an axis. In our new idea of distance, we will work with reciprocal space (I am aware that term is used to describe a crystal lattice, but I am not using the same thing here). BOTH the x and y axes (we'll start working in 2 dimensions) will be re-scaled such that x=1/x and y=1/y. The origin will be replaced with a single ""point at infinity"" similar to projective geometry. Graphing reciprocal vectors in a space which is measured this way satisfies requirement #1 - preserving direction. So all three requirements are satisfied with this new definition, provided we say that this isn't a vector, and it lives in a different space. The vector $\langle 2,3 \rangle$ (shown right) when graphed in Cartesian space looks the same as the vector $\langle \frac{1}{2},\frac{1}{3} \rangle$ (shown left) graphed in reciprocal space. An amazing thing about this space is that the geometric ""tip to tail"" addition of vectors applies to this new space as well, except it corresponds to oplussing of invectors instead of addition of vectors. And, assuming this represents a slowness, it is entirely consistent with vector addition of velocity! We define  (invectors will be denoted with a *) given $\vec{a}* = [a_1,a_2]$ and $\vec{b}* = [b_1,b_2]$ $\vec{a}* \oplus  \vec{b}* := [a_1 \oplus b_1, a_2 \oplus b_2]$ For instance: $\vec{v_1} = \langle x_1,y_1 \rangle$ and $\vec{v_2} = \langle x_2,y_2 \rangle$ $\vec{v_1}+\vec{v_2} = \vec{v_t} = \langle x_1+x_2,y_1+y_2 \rangle$ as a slowness invector, that would be $\vec{s_1}* = [\frac{1}{x_1},\frac{1}{y_1}] $ and $\vec{s_2}* = [\frac{1}{x_2},\frac{1}{y_2}]$ $\vec{s_1} \oplus \vec{s_2} = \vec{s_t} = [\frac{1}{x_1+x_2},\frac{1}{y_1+y_2}]$ which is consistent because $\frac{1}{[\frac{1}{x_1+x_2},\frac{1}{y_1+y_2}]} = \langle x_1+x_2,y_1+y_2 \rangle = \frac{1}{\vec{v_t}}$ Using the fact that vector addition is the same as invector oplussing, it is possible to prove that since lengths in reciprocal space are 1 divided by their geometric lengths, there is a new Pythagorean theorem for reciprocal space. $c^2=a^2 \oplus b^2$ because we know that $\frac{1}{length(x)}=x$ in reciprocal space (where length(x) is the length you measure normally, if you took out a ruler, for example and x is the ""actual length"") and we know that $length(c)^2 = length(a)^2+length(b)^2$ so $\frac{1}{c^2} =\frac{1}{a^2}+\frac{1}{b^2}$ and $c^2 =\frac{1}{\frac{1}{a^2}+\frac{1}{b^2}} = a^2 \oplus b^2$ which explains the equation above for magnitude of an invector. We can also see that if we use the inverse operation of $\oplus$ , $\ominus$ (o-minus) we can define a linear distance function along the axes in one dimension. We can call this function ""closeness"" because it is how close one object is to another. A small closeness is a large distance and a large closeness is a small distance (because they are reciprocals). $closeness(x,y) = c(x,y) := |x \ominus y|$ and for two dimensions $c((x_1,y_1),(x_2,y_2)) := \sqrt{(x_1 \ominus x_2)^2 \oplus (y_1 \ominus y_2)^2}$ The three dimensional formula is similar. We can see that under this closeness (distance) formula in 1 dimension, the distance between the reciprocal of any integer and the reciprocal of the next is 1. The distance between 1 and 1/2 is $|1\ominus\frac{1}{2}|=1$ . The distance between 1/2 and 1/3 is 1, the distance between 1/3 and 1/4 is 1, and so on. The distance function here is translation-invariant - if we move the axes the lengths of lines do not change. I have skirted along without mentioning the fact that the identity under the oplus operation is $\infty$ . I've found that $\infty$ is an integral part of this system. It effectively works just like zero in the Cartesian system. $a\oplus\infty=a\ominus\infty=a$ for all a and in general, $\infty = -\infty$ . As far as I can tell, this makes sense physically - a body with $0$ velocity has $\infty$ slowness. A body which has $0$ distance between it and something else has $\infty$ closeness to it. The invector of acceleration can be found to have physical meaning as well, and to work perfectly well in reciprocal space. I have not included everything which I have found about this system, including the dot product of two invectors $(\vec{a}*\cdot\vec{b}*=[a_1\cdot b_1,a_2\cdot b_2])$ and how they relate to derivatives describing motion in a reciprocal way. As an amateur with only high school level math training, I'd simply like to ask, does this make any sense? Is anybody aware of a vector way to describe slowness or other inverse vector quantities which is different (or the same) from my own work? I'd like to understand how this relates to mathematics in general, and if my ideas and work are valid. A vector is generally defined as a mathematical object with both magnitude and direction, but it seems to me that even though that suits this type of idea, a vector is unable to describe the type of object I'm dealing with here. Is there another way to do this that is already accepted by the math community? Is my work new or does this exist somewhere? Is defining the inverse of a vector even possible? If vectors are independent of coordinate system, why does changing to reciprocal space change anything? Basically, I would just like to know more details about how to define the inverse of a vector, in a mathematical sense and a physical sense.","Most physical situations in mechanics can be modeled using a combination of derivatives - specifically, derivatives of position: velocity and acceleration. But physical situations can also be modeled other ways. Consider the scalar equation for velocity in one dimension: it can be modeled just as well by a different quantity, called ""slowness"" which is described as: Which is used commonly in day-to-day life. For example, runners usually measure distance in minutes per mile, or minutes per km. Walkers go slow enough such that it doesn't make sense to measure speed in miles/hour or a similar unit, but instead to say they take ~20mins per mile. However, this is rarely, if ever, used in physics. In order to model slowness physically, there are a few basic things to know. Firstly, when we want to add velocities, they add quite nicely. If I stand on top of a flat bed truck and run at 5m/s while the truck is going 10m/s (disregarding special relativity) my total speed is 5+10=15m/s. For a slowness, we have to take advantage of knowing this fact to figure out how to ""add"" slownesses together. We know , so if and and then and therefore: defining an operation associated with this (called ""oplus"" - discussed in detail here ): and its inverse (""ominus"") we have , so while velocities add, slownesses oplus. It seems slowness is not easily measured in vector form, even though it represents the same physical quantity as velocity and therefore has both magnitude and direction. The vector version of slowness should (I think) fulfill three requirements: Preserve direction (point same direction as velocity vector) Invert magnitude (magnitude of slowness should be 1/speed) Coordinate independence (slowness in the x-direction doesn't effect y-direction, etc) There is only one vector which satisfies the first two is the vector which unfortunately doesn't also satisfy the third requirement, because the magnitude of is affected by all coordinates of . For example, if then but if then which means just changing the y coordinate also changed the x coordinate of the slowness. In order to preserve coordinate independence, as well as stay consistent with the one-dimensional definition of slowness, one can define a ""vector"" for slowness as taking the reciprocal of each component. So if then . The initial problem is that it appears to not preserve direction or invert the magnitude of the velocity vector. However it requires changing a fundamental vector property. It comes down to how we measure distance in a coordinate system, and the operations we use on vectors. We all know that vectors add together, which makes sense since velocity and position do the same, and those things add when they are scalars. One problem with defining slowness as a vector may be that slowness does not satisfy vector properties, even in one dimension! Slownesses do not add, they oplus (as shown above). So instead of defining slowness, which is the reciprocal of slowness, as a vector, why not define it as something else? Something which is like a vector, but more readily taking advantage of its properties. For example, it could be different in how we measure its magnitude, as well as other properties: Given <-- not a vector, instead could be called an ""inverse vector"" or ""invector"", denoted by square brackets This appears to behave quite nicely, since Which satisfies requirement #2. Through this definition, requirement #1 can also be satisfied, provided we change how we measure distance. Most people are familiar with a graph on a logarithmic scale. This helps visualize data which grows exponentially by changing where the numbers are located geometrically on an axis. In our new idea of distance, we will work with reciprocal space (I am aware that term is used to describe a crystal lattice, but I am not using the same thing here). BOTH the x and y axes (we'll start working in 2 dimensions) will be re-scaled such that x=1/x and y=1/y. The origin will be replaced with a single ""point at infinity"" similar to projective geometry. Graphing reciprocal vectors in a space which is measured this way satisfies requirement #1 - preserving direction. So all three requirements are satisfied with this new definition, provided we say that this isn't a vector, and it lives in a different space. The vector (shown right) when graphed in Cartesian space looks the same as the vector (shown left) graphed in reciprocal space. An amazing thing about this space is that the geometric ""tip to tail"" addition of vectors applies to this new space as well, except it corresponds to oplussing of invectors instead of addition of vectors. And, assuming this represents a slowness, it is entirely consistent with vector addition of velocity! We define  (invectors will be denoted with a *) given and For instance: and as a slowness invector, that would be and which is consistent because Using the fact that vector addition is the same as invector oplussing, it is possible to prove that since lengths in reciprocal space are 1 divided by their geometric lengths, there is a new Pythagorean theorem for reciprocal space. because we know that in reciprocal space (where length(x) is the length you measure normally, if you took out a ruler, for example and x is the ""actual length"") and we know that so and which explains the equation above for magnitude of an invector. We can also see that if we use the inverse operation of , (o-minus) we can define a linear distance function along the axes in one dimension. We can call this function ""closeness"" because it is how close one object is to another. A small closeness is a large distance and a large closeness is a small distance (because they are reciprocals). and for two dimensions The three dimensional formula is similar. We can see that under this closeness (distance) formula in 1 dimension, the distance between the reciprocal of any integer and the reciprocal of the next is 1. The distance between 1 and 1/2 is . The distance between 1/2 and 1/3 is 1, the distance between 1/3 and 1/4 is 1, and so on. The distance function here is translation-invariant - if we move the axes the lengths of lines do not change. I have skirted along without mentioning the fact that the identity under the oplus operation is . I've found that is an integral part of this system. It effectively works just like zero in the Cartesian system. for all a and in general, . As far as I can tell, this makes sense physically - a body with velocity has slowness. A body which has distance between it and something else has closeness to it. The invector of acceleration can be found to have physical meaning as well, and to work perfectly well in reciprocal space. I have not included everything which I have found about this system, including the dot product of two invectors and how they relate to derivatives describing motion in a reciprocal way. As an amateur with only high school level math training, I'd simply like to ask, does this make any sense? Is anybody aware of a vector way to describe slowness or other inverse vector quantities which is different (or the same) from my own work? I'd like to understand how this relates to mathematics in general, and if my ideas and work are valid. A vector is generally defined as a mathematical object with both magnitude and direction, but it seems to me that even though that suits this type of idea, a vector is unable to describe the type of object I'm dealing with here. Is there another way to do this that is already accepted by the math community? Is my work new or does this exist somewhere? Is defining the inverse of a vector even possible? If vectors are independent of coordinate system, why does changing to reciprocal space change anything? Basically, I would just like to know more details about how to define the inverse of a vector, in a mathematical sense and a physical sense.","v = \frac{dx}{dt} s = \frac{dt}{dx} = \frac{1}{v} v_1+v_2=v_t s_1=\frac{1}{v_1} s_2=\frac{1}{v_2} s_t=\frac{1}{v_t} \frac{1}{s_1}+\frac{1}{s_2}=\frac{1}{s_t} \frac{1}{\frac{1}{s_1}+\frac{1}{s_2}}=s_t x \oplus y := \frac{1}{\frac{1}{x}+\frac{1}{y}} x \ominus y := \frac{1}{\frac{1}{x}-\frac{1}{y}} = x \oplus (-y) s_1 \oplus s_2 = s_t \frac{\vec{v}}{|\vec{v}|^2} \vec{v} \vec{v} \vec{v}=\langle1,2\rangle \frac{\vec{v}}{|\vec{v}|^2}=\langle\frac{1}{\sqrt{5}},\frac{2}{\sqrt{5}}\rangle \vec{v}=\langle1,3\rangle \frac{\vec{v}}{|\vec{v}|^2}=\langle\frac{1}{\sqrt{10}},\frac{3}{\sqrt{10}}\rangle \vec{v}=\langle x,y,z\rangle \vec{s}=\langle\frac{1}{x},\frac{1}{y},\frac{1}{z}\rangle \vec{v}=\langle x,y,z \rangle \frac{1}{\vec{v}} := [x^{-1},y^{-1},z^{-1}] |\frac{1}{\vec{v}}| := \sqrt{(\frac{1}{x})^2\oplus(\frac{1}{y})^2\oplus(\frac{1}{z})^2} |\frac{1}{\vec{v}}|=\sqrt{(\frac{1}{x})^2\oplus(\frac{1}{y})^2\oplus(\frac{1}{z})^2} = \sqrt{\frac{1}{x^2+y^2+z^2}} = \frac{1}{\sqrt{x^2+y^2+z^2}} = \frac{1}{|\vec{v}|} \langle 2,3 \rangle \langle \frac{1}{2},\frac{1}{3} \rangle \vec{a}* = [a_1,a_2] \vec{b}* = [b_1,b_2] \vec{a}* \oplus  \vec{b}* := [a_1 \oplus b_1, a_2 \oplus b_2] \vec{v_1} = \langle x_1,y_1 \rangle \vec{v_2} = \langle x_2,y_2 \rangle \vec{v_1}+\vec{v_2} = \vec{v_t} = \langle x_1+x_2,y_1+y_2 \rangle \vec{s_1}* = [\frac{1}{x_1},\frac{1}{y_1}]  \vec{s_2}* = [\frac{1}{x_2},\frac{1}{y_2}] \vec{s_1} \oplus \vec{s_2} = \vec{s_t} = [\frac{1}{x_1+x_2},\frac{1}{y_1+y_2}] \frac{1}{[\frac{1}{x_1+x_2},\frac{1}{y_1+y_2}]} = \langle x_1+x_2,y_1+y_2 \rangle = \frac{1}{\vec{v_t}} c^2=a^2 \oplus b^2 \frac{1}{length(x)}=x length(c)^2 = length(a)^2+length(b)^2 \frac{1}{c^2} =\frac{1}{a^2}+\frac{1}{b^2} c^2 =\frac{1}{\frac{1}{a^2}+\frac{1}{b^2}} = a^2 \oplus b^2 \oplus \ominus closeness(x,y) = c(x,y) := |x \ominus y| c((x_1,y_1),(x_2,y_2)) := \sqrt{(x_1 \ominus x_2)^2 \oplus (y_1 \ominus y_2)^2} |1\ominus\frac{1}{2}|=1 \infty \infty a\oplus\infty=a\ominus\infty=a \infty = -\infty 0 \infty 0 \infty (\vec{a}*\cdot\vec{b}*=[a_1\cdot b_1,a_2\cdot b_2])","['calculus', 'algebra-precalculus', 'reference-request', 'vectors', 'physics']"
8,Sequences for that $\sum_{n} \frac{1}{x_n}$ is divergent and $\sum_{n} \frac{1}{x_n \ln x_n}$ is convergent,Sequences for that  is divergent and  is convergent,\sum_{n} \frac{1}{x_n} \sum_{n} \frac{1}{x_n \ln x_n},"We will denote with $(x_n)$ a given sequence and we introduce the following two series. $$S^* = \sum_{n} \frac{1}{x_n} \quad \text{and} \quad S_* = \sum_{n} \frac{1}{x_n \ln x_n}.$$ We know that if $(x_n)$ are for example the Fibonacci numbers greater then $1$, then $S^*$ and $S_*$ are convergent. If $(x_n)$ are the prime numbers then $S^*$ is divergent and $S_*$ is convergent . If $(x_n)$ are the natural numbers greater then $2$, then both series are divergent. Question. How could we characterise the $(x_n)$ sequences, for that $S^*$ is divergent and $S_*$ is convergent. I would be also glad to see some reference in this topic. If we cannot characterise $(x_n)$ then is there any special property of such sequences?","We will denote with $(x_n)$ a given sequence and we introduce the following two series. $$S^* = \sum_{n} \frac{1}{x_n} \quad \text{and} \quad S_* = \sum_{n} \frac{1}{x_n \ln x_n}.$$ We know that if $(x_n)$ are for example the Fibonacci numbers greater then $1$, then $S^*$ and $S_*$ are convergent. If $(x_n)$ are the prime numbers then $S^*$ is divergent and $S_*$ is convergent . If $(x_n)$ are the natural numbers greater then $2$, then both series are divergent. Question. How could we characterise the $(x_n)$ sequences, for that $S^*$ is divergent and $S_*$ is convergent. I would be also glad to see some reference in this topic. If we cannot characterise $(x_n)$ then is there any special property of such sequences?",,"['calculus', 'sequences-and-series', 'number-theory', 'convergence-divergence', 'prime-numbers']"
9,Proof that $\sin(x)$ don't have limit to infinity,Proof that  don't have limit to infinity,\sin(x),"I just used the Heine's definition. Let $\alpha,\delta \in \mathbb{R}$ such that $\sin(\alpha)=a$ and $\sin(\delta)=b$. Let $(u_{n})=\alpha+2\pi n$ and $(v_{n})=\delta+2\pi n$ and $f(x)=\sin(x)$. So one have, $$\lim\limits_{n\rightarrow \infty} u_{n}=+\infty$$ $$\lim\limits_{n\rightarrow \infty} v_{n}=+\infty$$ But, $\lim\limits_{n\rightarrow \infty} f(u_{n})=a$ $\lim\limits_{n\rightarrow \infty} f(v_{n})=b$ Because these last limits aren't equal, the sine function don't have limit to infinity. Is this proof correct? Thanks.","I just used the Heine's definition. Let $\alpha,\delta \in \mathbb{R}$ such that $\sin(\alpha)=a$ and $\sin(\delta)=b$. Let $(u_{n})=\alpha+2\pi n$ and $(v_{n})=\delta+2\pi n$ and $f(x)=\sin(x)$. So one have, $$\lim\limits_{n\rightarrow \infty} u_{n}=+\infty$$ $$\lim\limits_{n\rightarrow \infty} v_{n}=+\infty$$ But, $\lim\limits_{n\rightarrow \infty} f(u_{n})=a$ $\lim\limits_{n\rightarrow \infty} f(v_{n})=b$ Because these last limits aren't equal, the sine function don't have limit to infinity. Is this proof correct? Thanks.",,['calculus']
10,"Determine whether $\lim_{(x,y)\to (2,-2)} \frac{\sin(x+y)}{x+y}$ exists.",Determine whether  exists.,"\lim_{(x,y)\to (2,-2)} \frac{\sin(x+y)}{x+y}","I am trying to determine whether $\lim_{(x,y)\to (2,-2)} \dfrac{\sin(x+y)}{x+y}$ exists. I should be able to use the following definition for a limit of a function of two variables: Let $f$ be a function of two variables that is defined on some open disk $B((x_0,y_0),r)$ , except possibly at the point $(x_0,y_0)$ itself. Then $$\lim_{(x,y)\to (x_0,y_0)}f(x,y)=L$$ if for any $\varepsilon>0$ , however small, there exists a $\delta>0$ such that $$\text{if } 0<\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta \text{ then } |f(x,y)-L|<\varepsilon$$ The limit definition above applied to the limit $\lim_{(x,y)\to (2,-2)} \dfrac{\sin(x+y)}{x+y}$ requires that the function $\dfrac{\sin(x+y)}{x+y}$ be defined on every point of some open disk $B((2,-2),r)$ (with the possible exception of the point $(2,-2)$ ). My problem here is that the function $\dfrac{\sin(x+y)}{x+y}$ is undefined for $x+y=0$ , that is, it is undefined for all points in the line $y=-x$ . If I understood it correctly, it seems to mean that I can't find any open disk $B((2,-2),r)$ on which the given function is defined. Therefore, it seems that I can't apply the limit definition. What am I misunderstanding here? The argument above makes me believe that this limit does not exist (because I can't apply the definition), but Wolfram|Alpha says that the value of this limit is $1$ . Why this apparent contradiction? Edit : I should point out that this problem and the limit definition that I'm using are both taken from the same book. So, the book assumes that I will use the given limit definition. Update I have one more doubt concerning this. The same book that I'm using says that the function below is continuous for all points $(x,y)$ in $R_2$ . $$f(x,y)=\begin{cases} \dfrac{\sin(x+y)}{x+y} & \text{ if $x + y \neq 0$} \\ 1 & \text{ if $x + y = 0$}\end{cases}$$ The proof is as follows: Let $h(t)=\begin{cases} \dfrac{\sin(t)}{t} & \text{ if $t \neq 0$} \\ 1 & \text{ if $t = 0$}\end{cases}$ and $g(x,y) = x+y$ ; so, $f(x,y) = h(g(x,y))$ . The domain of $g$ is $R_2$ and it is continuous everywhere in its domain. Since $g(x,y)\to 0$ as $x+y\to 0$ and $h$ is continuous at $0$ , $h(g(x,y))$ is continuous everywhere. The definition of continuity in this book states that, for a two-variable function $g$ to be continuous at $(x_0,y_0)$ , $g(x_0,y_0)$ has to exist and $\lim_{(x,y)\to (x_0,y_0)} g(x,y)$ has to be equal to $g(x_0,y_0)$ . So, by the given definition of continuity, $f(x,y)$ being continuous everywhere means that $\lim_{(x,y)\to (x_0,y_0)} f(x,y) = f(x_0,y_0)$ for all $(x_0,y_0)$ . The proof above seems to imply that $\lim_{(x,y)\to (2,-2)} \dfrac{\sin(x+y)}{x+y}$ should exist. But the problem is that this limit should not exist, considering the definition of limit that I am using in this question. Is there a contradiction here or am I missing something?","I am trying to determine whether exists. I should be able to use the following definition for a limit of a function of two variables: Let be a function of two variables that is defined on some open disk , except possibly at the point itself. Then if for any , however small, there exists a such that The limit definition above applied to the limit requires that the function be defined on every point of some open disk (with the possible exception of the point ). My problem here is that the function is undefined for , that is, it is undefined for all points in the line . If I understood it correctly, it seems to mean that I can't find any open disk on which the given function is defined. Therefore, it seems that I can't apply the limit definition. What am I misunderstanding here? The argument above makes me believe that this limit does not exist (because I can't apply the definition), but Wolfram|Alpha says that the value of this limit is . Why this apparent contradiction? Edit : I should point out that this problem and the limit definition that I'm using are both taken from the same book. So, the book assumes that I will use the given limit definition. Update I have one more doubt concerning this. The same book that I'm using says that the function below is continuous for all points in . The proof is as follows: Let and ; so, . The domain of is and it is continuous everywhere in its domain. Since as and is continuous at , is continuous everywhere. The definition of continuity in this book states that, for a two-variable function to be continuous at , has to exist and has to be equal to . So, by the given definition of continuity, being continuous everywhere means that for all . The proof above seems to imply that should exist. But the problem is that this limit should not exist, considering the definition of limit that I am using in this question. Is there a contradiction here or am I missing something?","\lim_{(x,y)\to (2,-2)} \dfrac{\sin(x+y)}{x+y} f B((x_0,y_0),r) (x_0,y_0) \lim_{(x,y)\to (x_0,y_0)}f(x,y)=L \varepsilon>0 \delta>0 \text{if } 0<\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta \text{ then } |f(x,y)-L|<\varepsilon \lim_{(x,y)\to (2,-2)} \dfrac{\sin(x+y)}{x+y} \dfrac{\sin(x+y)}{x+y} B((2,-2),r) (2,-2) \dfrac{\sin(x+y)}{x+y} x+y=0 y=-x B((2,-2),r) 1 (x,y) R_2 f(x,y)=\begin{cases} \dfrac{\sin(x+y)}{x+y} & \text{ if x + y \neq 0} \\ 1 & \text{ if x + y = 0}\end{cases} h(t)=\begin{cases} \dfrac{\sin(t)}{t} & \text{ if t \neq 0} \\ 1 & \text{ if t = 0}\end{cases} g(x,y) = x+y f(x,y) = h(g(x,y)) g R_2 g(x,y)\to 0 x+y\to 0 h 0 h(g(x,y)) g (x_0,y_0) g(x_0,y_0) \lim_{(x,y)\to (x_0,y_0)} g(x,y) g(x_0,y_0) f(x,y) \lim_{(x,y)\to (x_0,y_0)} f(x,y) = f(x_0,y_0) (x_0,y_0) \lim_{(x,y)\to (2,-2)} \dfrac{\sin(x+y)}{x+y}","['calculus', 'limits', 'multivariable-calculus']"
11,The boundedness of an integral,The boundedness of an integral,,"Is there a constant $C$ which is independent of real numbers $a,b,N$, such that $$\left| {\int_{-N}^N \dfrac{e^{i(ax^2+bx)}-1}{x}dx} \right| \le C?$$","Is there a constant $C$ which is independent of real numbers $a,b,N$, such that $$\left| {\int_{-N}^N \dfrac{e^{i(ax^2+bx)}-1}{x}dx} \right| \le C?$$",,"['calculus', 'integration', 'fourier-analysis', 'asymptotics']"
12,Taylor expansion to show that for Stratonovich stochastic calculus the chain rule takes the form of the classical one,Taylor expansion to show that for Stratonovich stochastic calculus the chain rule takes the form of the classical one,,"How can I show with a heuristic argument based on a Taylor expansion that for Stratonovich stochastic calculus the chain rule takes the form of the classical (Newtonian) one? Concerning Ito calculus the fact that $dX^2 = dt$ results via a Taylor expansion in Ito's lemma -- this fact should stay the same with Stratonovich, but it should somehow cancel out in there -- I just don't know how....","How can I show with a heuristic argument based on a Taylor expansion that for Stratonovich stochastic calculus the chain rule takes the form of the classical (Newtonian) one? Concerning Ito calculus the fact that results via a Taylor expansion in Ito's lemma -- this fact should stay the same with Stratonovich, but it should somehow cancel out in there -- I just don't know how....",dX^2 = dt,"['calculus', 'intuition', 'stochastic-processes']"
13,Integral $\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt$,Integral,\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt,"For $a, b, c \in \text{N}$ : $$I(a,b,c)=\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt$$ Some Results using numerical Evaluation: $$I(4k-2,1,1)=(2k-1)\pi$$ $$I(a,2,1)=\left(\frac{3a-1}{2}\right)\pi$$ Keeping 2 variables constant and changing one, we can see the pattern for many cases. And confirming for many cases, it comes out to be a multiple of $\pi$ . One observation is that answer will be symmetric in terms of $a, b, c$ . Maybe there is an Anti-Derivative too, but I can't seem to find it. Some Examples: $$\int_{0}^{\pi}\cot\left(10t\right)\cot\left(11t\right)\cot\left(12t\right)\left(\sin\left(1320t\right)\right)^{3}dt=\frac{1979}{2}\pi$$ EDIT: Here is a table I made too, Keeping $c=1$ : $$ \begin{array}{c|c|c|c|c|c|c|c|c|c|c|c}  a/b                                       & 1  &2&3&4&5&6&7&8&9&10 \\\hline  1                                       &0&1&0&2&0&3&0&4&0&5 \\\hline  2                                       &1&2.5&4&5.5&7&8.5&10&11.5&13&14.5\\\hline  3                                       &0&4&0&9&0&13&0&17&0&22\\\hline  4 &2&5.5&9&11.5&14&17.5&21&23.5&26&29.5\\\hline  5&0&7&0&14&0&23&0&30&0&37\\\hline  6&3&8.5&13&17.5&23&26.5&30&35.5&40&44.5\\\hline  7&0&10&0&21&0&30&0&43&0&52\\\hline  8&4&11.5&17&23.5&30&35.5&43&47.5&52&59.5\\\hline  9&0&13&0&26&0&40&0&52&0&69\\\hline  10&5&14.5&22&29.5&37&44.5&52&59.5&69&74.5\\\hline \end{array} $$ Now obviously the table will be symmetric about the diagonal but I made it fully anyways. Maybe someone can conjecture a form using these. EDIT 2: Using more data, maybe: For $a, b, c$ : all odd Natural Numbers. $$\boxed{I(a,b,c)=0}$$ Proof: $$I(a,b,c)=\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt$$ $t\to\pi-t$ $$I(a,b,c)=\int_{0}^{\pi}\cot\left(a\left(\pi-t\right)\right)\cot\left(b\left(\pi-t\right)\right)\cot\left(c\left(\pi-t\right)\right)\left(\sin\left(abc\left(\pi-t\right)\right)\right)^{3}dt$$ For $a, b, c$ odd we have $abc$ odd too. Therefore, $$I(a, b, c)=-I(a, b, c)$$ $$I(a,b,c)=0$$","For : Some Results using numerical Evaluation: Keeping 2 variables constant and changing one, we can see the pattern for many cases. And confirming for many cases, it comes out to be a multiple of . One observation is that answer will be symmetric in terms of . Maybe there is an Anti-Derivative too, but I can't seem to find it. Some Examples: EDIT: Here is a table I made too, Keeping : Now obviously the table will be symmetric about the diagonal but I made it fully anyways. Maybe someone can conjecture a form using these. EDIT 2: Using more data, maybe: For : all odd Natural Numbers. Proof: For odd we have odd too. Therefore,","a, b, c \in \text{N} I(a,b,c)=\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt I(4k-2,1,1)=(2k-1)\pi I(a,2,1)=\left(\frac{3a-1}{2}\right)\pi \pi a, b, c \int_{0}^{\pi}\cot\left(10t\right)\cot\left(11t\right)\cot\left(12t\right)\left(\sin\left(1320t\right)\right)^{3}dt=\frac{1979}{2}\pi c=1 
\begin{array}{c|c|c|c|c|c|c|c|c|c|c|c}
 a/b                                       & 1  &2&3&4&5&6&7&8&9&10 \\\hline
 1                                       &0&1&0&2&0&3&0&4&0&5 \\\hline
 2                                       &1&2.5&4&5.5&7&8.5&10&11.5&13&14.5\\\hline
 3                                       &0&4&0&9&0&13&0&17&0&22\\\hline
 4 &2&5.5&9&11.5&14&17.5&21&23.5&26&29.5\\\hline
 5&0&7&0&14&0&23&0&30&0&37\\\hline
 6&3&8.5&13&17.5&23&26.5&30&35.5&40&44.5\\\hline
 7&0&10&0&21&0&30&0&43&0&52\\\hline
 8&4&11.5&17&23.5&30&35.5&43&47.5&52&59.5\\\hline
 9&0&13&0&26&0&40&0&52&0&69\\\hline
 10&5&14.5&22&29.5&37&44.5&52&59.5&69&74.5\\\hline
\end{array}
 a, b, c \boxed{I(a,b,c)=0} I(a,b,c)=\int_{0}^{\pi}\cot\left(at\right)\cot\left(bt\right)\cot\left(ct\right)\left(\sin\left(abct\right)\right)^{3}dt t\to\pi-t I(a,b,c)=\int_{0}^{\pi}\cot\left(a\left(\pi-t\right)\right)\cot\left(b\left(\pi-t\right)\right)\cot\left(c\left(\pi-t\right)\right)\left(\sin\left(abc\left(\pi-t\right)\right)\right)^{3}dt a, b, c abc I(a, b, c)=-I(a, b, c) I(a,b,c)=0","['calculus', 'integration', 'definite-integrals']"
14,Prove $ \lim_{h\rightarrow 0}\frac{1}{h}\int_a^x[f(t+h)-f(t)]\mathrm{d}t=f(x)-f(a). $,Prove, \lim_{h\rightarrow 0}\frac{1}{h}\int_a^x[f(t+h)-f(t)]\mathrm{d}t=f(x)-f(a). ,"Suppose $f(x)$ is continuous on $[a, b]$ . Prove that $\forall x\in (a,b)$ , we have $$ \lim_{h\rightarrow 0}\frac{1}{h}\int_a^x[f(t+h)-f(t)]\mathrm{d}t=f(x)-f(a). $$ If $f$ id diffentiable, the conclution can be obtained by Newton-Leibniz formula easily. When $f$ is just continuous, I tried to use knowledge about integral with parameters. Write $$ F(h,x):=\frac{1}{h}\int_a^x[f(t+h)-f(t)]\mathrm{d}t, $$ but it seems we still need to compute $\lim_{h\rightarrow 0}\frac{f(t+h)-f(t)}{h}$ , and prove uniform convergence. I have no idea how to go on. Is my thoughts workable? If not, please suggest your way. Appreciate any help!","Suppose is continuous on . Prove that , we have If id diffentiable, the conclution can be obtained by Newton-Leibniz formula easily. When is just continuous, I tried to use knowledge about integral with parameters. Write but it seems we still need to compute , and prove uniform convergence. I have no idea how to go on. Is my thoughts workable? If not, please suggest your way. Appreciate any help!","f(x) [a, b] \forall x\in (a,b) 
\lim_{h\rightarrow 0}\frac{1}{h}\int_a^x[f(t+h)-f(t)]\mathrm{d}t=f(x)-f(a).
 f f 
F(h,x):=\frac{1}{h}\int_a^x[f(t+h)-f(t)]\mathrm{d}t,
 \lim_{h\rightarrow 0}\frac{f(t+h)-f(t)}{h}","['calculus', 'integration', 'analysis', 'multivariable-calculus', 'uniform-convergence']"
15,"Prove there exists $\xi \in (a,b)$ such that $f(\xi)=f^{(n+1)}(\xi)$.",Prove there exists  such that .,"\xi \in (a,b) f(\xi)=f^{(n+1)}(\xi)","Problem Let $f(x)$ be $n$ -times differentiable over $[a,b]$ and $n+1$ -times differentiable over $(a,b)$ . $f^{(k)}(a)=f^{(k)}(b)=0$ , where $k=0,1,2,\cdots,n$ . Prove there exists $\xi \in (a,b)$ such that $f(\xi)=f^{(n+1)}(\xi)$ . Attempt Consider applying Taylor's formula expanding at $x=a,b$ . We have \begin{align*} f(x)&=f(a)+f'(a)(x-a)+\cdots+\frac{f^{(n+1)}(\xi_1)}{(n+1)!}(x-a)^{n+1} =\frac{f^{(n+1)}(\xi_1)}{(n+1)!}(x-a)^{n+1}. \end{align*} and \begin{align*} f(x)&=f(b)+f'(b)(x-b)+\cdots+\frac{f^{(n+1)}(\xi_2)}{(n+1)!}(x-b)^{n+1} =\frac{f^{(n+1)}(\xi_2)}{(n+1)!}(x-b)^{n+1}. \end{align*} Can we go on from these? Edit Later I consulted some reference books and find a similar problem in the book named Problems In Real Analysis: Advanced Calculus On The Real Axis . Thus, I wonder whether the conclusion holds or not ,if we are only given that $f(x)$ is $n$ -times differentiable over $[a,b]$ and $n+1$ -times differentiable over $(a,b)$ .","Problem Let be -times differentiable over and -times differentiable over . , where . Prove there exists such that . Attempt Consider applying Taylor's formula expanding at . We have and Can we go on from these? Edit Later I consulted some reference books and find a similar problem in the book named Problems In Real Analysis: Advanced Calculus On The Real Axis . Thus, I wonder whether the conclusion holds or not ,if we are only given that is -times differentiable over and -times differentiable over .","f(x) n [a,b] n+1 (a,b) f^{(k)}(a)=f^{(k)}(b)=0 k=0,1,2,\cdots,n \xi \in (a,b) f(\xi)=f^{(n+1)}(\xi) x=a,b \begin{align*}
f(x)&=f(a)+f'(a)(x-a)+\cdots+\frac{f^{(n+1)}(\xi_1)}{(n+1)!}(x-a)^{n+1}
=\frac{f^{(n+1)}(\xi_1)}{(n+1)!}(x-a)^{n+1}.
\end{align*} \begin{align*}
f(x)&=f(b)+f'(b)(x-b)+\cdots+\frac{f^{(n+1)}(\xi_2)}{(n+1)!}(x-b)^{n+1}
=\frac{f^{(n+1)}(\xi_2)}{(n+1)!}(x-b)^{n+1}.
\end{align*} f(x) n [a,b] n+1 (a,b)",['calculus']
16,How to compute this improper integral?,How to compute this improper integral?,,"Let $n\geq1$ be an integer and let $$I_n=\int\limits_{0}^{\infty}\dfrac{\arctan x}{(1+x^2)^n} \,\mathrm dx$$ Prove that $$\sum\limits_{n=1}^{\infty}\dfrac{I_n}{n}=\dfrac{\pi^2}{6} \tag{1}$$ $$\int\limits_{0}^{\infty} \arctan x\cdot\ln\left(1+\frac{1}{x^2}\right) \,\mathrm d x=\dfrac{\pi^2}{6} \tag{2}$$ I have an idea that dominated convergence theorem may help here. But I am not getting the proper way.",Let be an integer and let Prove that I have an idea that dominated convergence theorem may help here. But I am not getting the proper way.,"n\geq1 I_n=\int\limits_{0}^{\infty}\dfrac{\arctan x}{(1+x^2)^n} \,\mathrm dx \sum\limits_{n=1}^{\infty}\dfrac{I_n}{n}=\dfrac{\pi^2}{6} \tag{1} \int\limits_{0}^{\infty} \arctan x\cdot\ln\left(1+\frac{1}{x^2}\right) \,\mathrm d x=\dfrac{\pi^2}{6} \tag{2}","['calculus', 'integration', 'sequences-and-series', 'improper-integrals']"
17,Integrate $\int \frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}$,Integrate,\int \frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}},"Evaluate $$I=\int \frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}$$ My Try: we have $x^2-x+1=(x+w)(x+w^2)$ where $w$ is complex cube root of unity I have splitted $I$ as $$I=AI_1+BI_2$$ where $A,B$ are some constants $$I_1=\int \frac{dx}{(x+w)\sqrt{x^2+x+1}}$$ By taylor's series $$x^2+x+1=P(x+w)^2+Q(x+w)+R=(x+w)^2 \left(P+\frac{Q}{x+w}+\frac{R}{(x+w)^2}\right)$$ for some constants complex $P,Q,R$ hence $$I_1=\int \frac{\frac{dx}{(x+w)^2}}{\sqrt{ \left(P+\frac{Q}{x+w}+\frac{R}{(x+w)^2}\right)}}=\int \frac{dt}{\sqrt{Rt^2+Qt+P}}$$ which is a standard Integral. Similar analysis for $I_2$ . Any other approach?",Evaluate My Try: we have where is complex cube root of unity I have splitted as where are some constants By taylor's series for some constants complex hence which is a standard Integral. Similar analysis for . Any other approach?,"I=\int \frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}} x^2-x+1=(x+w)(x+w^2) w I I=AI_1+BI_2 A,B I_1=\int \frac{dx}{(x+w)\sqrt{x^2+x+1}} x^2+x+1=P(x+w)^2+Q(x+w)+R=(x+w)^2 \left(P+\frac{Q}{x+w}+\frac{R}{(x+w)^2}\right) P,Q,R I_1=\int \frac{\frac{dx}{(x+w)^2}}{\sqrt{ \left(P+\frac{Q}{x+w}+\frac{R}{(x+w)^2}\right)}}=\int \frac{dt}{\sqrt{Rt^2+Qt+P}} I_2","['calculus', 'integration', 'functions', 'complex-numbers', 'indefinite-integrals']"
18,Definite integral of $\frac{\sin(x)}{x}$,Definite integral of,\frac{\sin(x)}{x},$ $  I'm wondering how to approach the following definite integral: $$\int_{-400}^{400} \frac{\sin(x)}{x}dx = \int_{-400}^{400} \DeclareMathOperator{\sinc}{sinc} \sinc(x) dx$$ I tried taylor expanding and integrating the polynomial but then I get a divergent series.,$ $  I'm wondering how to approach the following definite integral: $$\int_{-400}^{400} \frac{\sin(x)}{x}dx = \int_{-400}^{400} \DeclareMathOperator{\sinc}{sinc} \sinc(x) dx$$ I tried taylor expanding and integrating the polynomial but then I get a divergent series.,,"['calculus', 'integration']"
19,How to evaluate this limit about Bernoulli number?,How to evaluate this limit about Bernoulli number?,,"First,we define $\displaystyle I_{1}\left ( x \right )=\frac{\sin x}{x}$, then $\displaystyle \lim_{x\rightarrow 0^+}I_{1}\left ( x \right )=1$, also we have \begin{align*} I_2\left ( x \right )&=\frac{I_1\left ( x \right )-1}{x^{2}}~,~\lim_{x\rightarrow 0^+}I_2\left ( x \right )=-\frac{1}{6}\\ I_3\left ( x \right )&=\frac{I_2\left ( x \right )+\dfrac{1}{6}}{x^2}~,~\lim_{x\rightarrow 0^+}I_3\left ( x \right )=\frac{1}{120}\\ &\cdots \\ I_n\left ( x \right )&=\frac{I_{n-1}\left ( x \right )-\displaystyle \lim_{x\rightarrow 0^+}I_{n-1}\left ( x \right )}{x^{2}} \end{align*} Now we have the following questions. (1)$I_n(x)$ is related to bernoulli number, but how to find it. (2)Evaluate $\displaystyle \lim_{k\rightarrow +\infty }\left [ \lim_{x\rightarrow 0^{+}}I_{2k}\left ( x \right ) \right ]~,~\lim_{k\rightarrow +\infty }\left [ \lim_{x\rightarrow 0^{+}}I_{2k+1}\left ( x \right ) \right ]~,~k\in \mathbb{Z}.$","First,we define $\displaystyle I_{1}\left ( x \right )=\frac{\sin x}{x}$, then $\displaystyle \lim_{x\rightarrow 0^+}I_{1}\left ( x \right )=1$, also we have \begin{align*} I_2\left ( x \right )&=\frac{I_1\left ( x \right )-1}{x^{2}}~,~\lim_{x\rightarrow 0^+}I_2\left ( x \right )=-\frac{1}{6}\\ I_3\left ( x \right )&=\frac{I_2\left ( x \right )+\dfrac{1}{6}}{x^2}~,~\lim_{x\rightarrow 0^+}I_3\left ( x \right )=\frac{1}{120}\\ &\cdots \\ I_n\left ( x \right )&=\frac{I_{n-1}\left ( x \right )-\displaystyle \lim_{x\rightarrow 0^+}I_{n-1}\left ( x \right )}{x^{2}} \end{align*} Now we have the following questions. (1)$I_n(x)$ is related to bernoulli number, but how to find it. (2)Evaluate $\displaystyle \lim_{k\rightarrow +\infty }\left [ \lim_{x\rightarrow 0^{+}}I_{2k}\left ( x \right ) \right ]~,~\lim_{k\rightarrow +\infty }\left [ \lim_{x\rightarrow 0^{+}}I_{2k+1}\left ( x \right ) \right ]~,~k\in \mathbb{Z}.$",,"['calculus', 'sequences-and-series', 'limits', 'bernoulli-numbers']"
20,Justifying differentiation of infinite product,Justifying differentiation of infinite product,,"Let $(z_n)_{n\in\mathbb{Z}}$ be a sequence of complex numbers s.t. the product $P(z):=\prod_{n=1}^{\infty}{\left(1-\frac{z}{z_{-n}}\right)\left(1-\frac{z}{z_{n}}\right)}$ is absolutely convergent for every $z\in\mathbb{C}$, and hence defines an entire function with zeros at every $z_k$. Is there some nice way to justify $\frac{\mathrm{d}}{\mathrm{d} z}P(z_k)=\frac{-1}{z_k}\prod_{|n|\geq1, n\neq k}{\left(1-\frac{z_k}{z_{n}}\right)}$? I guess one is not allowed to simply use the product rule for differentiation here?","Let $(z_n)_{n\in\mathbb{Z}}$ be a sequence of complex numbers s.t. the product $P(z):=\prod_{n=1}^{\infty}{\left(1-\frac{z}{z_{-n}}\right)\left(1-\frac{z}{z_{n}}\right)}$ is absolutely convergent for every $z\in\mathbb{C}$, and hence defines an entire function with zeros at every $z_k$. Is there some nice way to justify $\frac{\mathrm{d}}{\mathrm{d} z}P(z_k)=\frac{-1}{z_k}\prod_{|n|\geq1, n\neq k}{\left(1-\frac{z_k}{z_{n}}\right)}$? I guess one is not allowed to simply use the product rule for differentiation here?",,"['calculus', 'sequences-and-series', 'complex-analysis', 'analysis']"
21,Substitution for definite integrals,Substitution for definite integrals,,"In my experience, Calculus II students dislike changing bounds in definite integrals involving substitution.  When facing an integral like $$\int_0^{\sqrt{\pi }} x \sin \left(x^2\right)dx,$$ for example, most US Calc II students would introduce $u=x^2$ and compute \begin{align}   \int x \sin \left(x^2\right)dx &= \frac{1}{2}\int \sin(u) \, du \\   &= -\frac{1}{2} \cos(u)+c = -\frac{1}{2} \cos(x^2)+c. \end{align} Afterward, they would conclude that $$\int_0^{\sqrt{\pi }} x \sin \left(x^2\right) \, dx = -\frac{1}{2} \cos(x^2) \big|_0^{\sqrt{\pi}} = 1.$$ I would generally encourage them to write \begin{align}   \int_0^{\sqrt{\pi }} x \sin \left(x^2\right)dx &= \frac{1}{2}\int_0^{\pi} \sin(u) \, du \\   &= -\frac{1}{2} \cos(u) \big|_0^{\pi} = 1. \end{align} This question expresses opinion of a typical such student and this answer correctly expresses the fact that the two step process favored my most calculus students is actually more work . I think there's more to it than this, though.  Specifically, the identity $$\int_a^b f(g(x)) g'(x) \, dx = \int_{g(a)}^{g(b)} f(u) \, du$$ is a relationship between definite integrals which could have applications other than symbolic evaluation of the integral on the left.  In this case, the change of the bounds of integration is important in its own right.  Thus my question: What are some important applications of change of variables in definite integration, other than symbolic evaluation? I have at least one answer but would be happy to hear more, particularly those that are easily understandable by Calc II students, as I think it's an important pedagogical question.","In my experience, Calculus II students dislike changing bounds in definite integrals involving substitution.  When facing an integral like $$\int_0^{\sqrt{\pi }} x \sin \left(x^2\right)dx,$$ for example, most US Calc II students would introduce $u=x^2$ and compute \begin{align}   \int x \sin \left(x^2\right)dx &= \frac{1}{2}\int \sin(u) \, du \\   &= -\frac{1}{2} \cos(u)+c = -\frac{1}{2} \cos(x^2)+c. \end{align} Afterward, they would conclude that $$\int_0^{\sqrt{\pi }} x \sin \left(x^2\right) \, dx = -\frac{1}{2} \cos(x^2) \big|_0^{\sqrt{\pi}} = 1.$$ I would generally encourage them to write \begin{align}   \int_0^{\sqrt{\pi }} x \sin \left(x^2\right)dx &= \frac{1}{2}\int_0^{\pi} \sin(u) \, du \\   &= -\frac{1}{2} \cos(u) \big|_0^{\pi} = 1. \end{align} This question expresses opinion of a typical such student and this answer correctly expresses the fact that the two step process favored my most calculus students is actually more work . I think there's more to it than this, though.  Specifically, the identity $$\int_a^b f(g(x)) g'(x) \, dx = \int_{g(a)}^{g(b)} f(u) \, du$$ is a relationship between definite integrals which could have applications other than symbolic evaluation of the integral on the left.  In this case, the change of the bounds of integration is important in its own right.  Thus my question: What are some important applications of change of variables in definite integration, other than symbolic evaluation? I have at least one answer but would be happy to hear more, particularly those that are easily understandable by Calc II students, as I think it's an important pedagogical question.",,"['calculus', 'integration', 'definite-integrals', 'education']"
22,"Find maximum of $\sum\limits_{k=1}^n \sin^2{\theta_{k}}$ for $\theta_k \ge 0, \forall k$ with $\sum\limits_{k=1}^n \theta_k = \pi$",Find maximum of  for  with,"\sum\limits_{k=1}^n \sin^2{\theta_{k}} \theta_k \ge 0, \forall k \sum\limits_{k=1}^n \theta_k = \pi","Question: let $\theta_{1},\theta_{2},\cdots,\theta_{n}\ge 0$ ,and such $$\theta_{1}+\theta_{2}+\theta_{3}+\cdots+\theta_{n}=\pi$$ find the maximum of value, $P(n)$ , of $$P=\sin^2{\theta_{1}}+\sin^2{\theta_{2}}+\cdots+\sin^2{\theta_{n}}$$ Find the closed $P(n)$ I found this when $n=2$ then $$P=\sin^2{\theta_{1}}+\sin^2{\theta_{2}}=1-\dfrac{1}{2}(\cos{2\theta_{1}}+\cos{2\theta_{2}})=1-\cos{(\theta_{1}+\theta_{2})}\cos{(\theta_{1}-\theta_{2})}=1+\cos{(\theta_{1}-\theta_{2})}\le 2$$ when $\theta_{1}=\theta_{2}=\dfrac{\pi}{2}$ . so $$P(2)=2$$ and for general $n$ ,maybe have use other methods,Thank you I guess we can prove $$P(3)=P(4)=P(5)=\cdots=P(n)?$$","Question: let ,and such find the maximum of value, , of Find the closed I found this when then when . so and for general ,maybe have use other methods,Thank you I guess we can prove","\theta_{1},\theta_{2},\cdots,\theta_{n}\ge 0 \theta_{1}+\theta_{2}+\theta_{3}+\cdots+\theta_{n}=\pi P(n) P=\sin^2{\theta_{1}}+\sin^2{\theta_{2}}+\cdots+\sin^2{\theta_{n}} P(n) n=2 P=\sin^2{\theta_{1}}+\sin^2{\theta_{2}}=1-\dfrac{1}{2}(\cos{2\theta_{1}}+\cos{2\theta_{2}})=1-\cos{(\theta_{1}+\theta_{2})}\cos{(\theta_{1}-\theta_{2})}=1+\cos{(\theta_{1}-\theta_{2})}\le 2 \theta_{1}=\theta_{2}=\dfrac{\pi}{2} P(2)=2 n P(3)=P(4)=P(5)=\cdots=P(n)?","['calculus', 'inequality', 'optimization', 'maxima-minima']"
23,"What is, how do you use, and why do you use differentials? What are their practical uses?","What is, how do you use, and why do you use differentials? What are their practical uses?",,"What is a differential? And how is it useful? What is its practical use? For example, in Electromagnetic Wave Theory as it pertains to diffraction gratings, we have an equation like this one: $$d_s\sin(\theta) = m\lambda.$$ (Not important, but in case you're curious: $d_s$ is the distance between slits in the grating, $\theta$ is an approximate angle at which light bends through each slit of the grating, $\lambda$ is the wavelength of the light passing through the gradient, and $m$ is the number of wavelengths by which distances traveled by one ray from one slit differ from an adjacent slit.) My physics book says that the differential of the above mentioned equation is $$d_s \cos(\theta)d\theta = md\lambda$$ (without confusing the single $d_s$ (distance) with the ones in $d\theta$ and $d\lambda$). What does this mean and how is it useful? I am trying to understand the concept behind the differentials more so than the physics so that I may later make sense of the physics. EDIT: In user6786's question , user6786 states that ""according to the formula $dy=f'(x)dx$ we are able to plug in values for $dx$ and calculate a $dy$ (differential)"". I'm trying to see how that works.","What is a differential? And how is it useful? What is its practical use? For example, in Electromagnetic Wave Theory as it pertains to diffraction gratings, we have an equation like this one: $$d_s\sin(\theta) = m\lambda.$$ (Not important, but in case you're curious: $d_s$ is the distance between slits in the grating, $\theta$ is an approximate angle at which light bends through each slit of the grating, $\lambda$ is the wavelength of the light passing through the gradient, and $m$ is the number of wavelengths by which distances traveled by one ray from one slit differ from an adjacent slit.) My physics book says that the differential of the above mentioned equation is $$d_s \cos(\theta)d\theta = md\lambda$$ (without confusing the single $d_s$ (distance) with the ones in $d\theta$ and $d\lambda$). What does this mean and how is it useful? I am trying to understand the concept behind the differentials more so than the physics so that I may later make sense of the physics. EDIT: In user6786's question , user6786 states that ""according to the formula $dy=f'(x)dx$ we are able to plug in values for $dx$ and calculate a $dy$ (differential)"". I'm trying to see how that works.",,"['calculus', 'ordinary-differential-equations']"
24,How to prove completeness of the Spherical Harmonics,How to prove completeness of the Spherical Harmonics,,"Laplace's spherical harmonics ""form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable functions"" [1] .  I have three related questions about this statement: (1) I can prove their orthonormality, but how do you prove that they form a complete set? (2) What does completeness mean for an set with an infinite number of elements? (3) How does the assertion that the spherical harmonics form an orthonormal basis of the Hilbert space of square-integrable functions follow from their being a complete set of orthornormal functions?","Laplace's spherical harmonics ""form a complete set of orthonormal functions and thus form an orthonormal basis of the Hilbert space of square-integrable functions"" [1] .  I have three related questions about this statement: (1) I can prove their orthonormality, but how do you prove that they form a complete set? (2) What does completeness mean for an set with an infinite number of elements? (3) How does the assertion that the spherical harmonics form an orthonormal basis of the Hilbert space of square-integrable functions follow from their being a complete set of orthornormal functions?",,"['calculus', 'group-theory', 'spherical-harmonics']"
25,A limit and a coordinate trigonometric transformation of the interior points of a square into the interior points of a triangle,A limit and a coordinate trigonometric transformation of the interior points of a square into the interior points of a triangle,,"The coordinate transformation (due to Beukers, Calabi and Kolk) $$x=\frac{\sin u}{\cos v}$$ $$y=\frac{\sin v}{\cos u}$$ transforms the square domain $0\lt x\lt 1$ and $0\lt y\lt 1$ into the triangle domain $u,v>0,u+v<\pi /2$ (in Proofs from the BOOK by M. Aigner and G. Ziegler). Since the invert transformation is $$u=\arccos \sqrt{\dfrac{1-x^{2}}{1-x^{2}y^{2}}}$$ $$v=\arccos \sqrt{\dfrac{1-y^{2}}{1-x^{2}y^{2}}}$$ it is easy to see that three of the vertices (although not belonging to the domain) are transformed as follows: $$(x,y)=(0,0)\mapsto (0,0)=(u,v),$$ $$(x,y)=(1,0)\mapsto (\pi /2,0)=(u,v),$$ $$(x,y)=(0,1)\mapsto (0,\pi /2)=(u,v).$$ Question 1 - But how is the fourth vertex $(x,y)=(1,1)$ transformed? In the plot below of $\dfrac{1-x^{2}}{1-x^{2}y^{2}}$ seems that the following limit does not exist $$\underset{(x,y)\rightarrow (1,1)}{\lim }\sqrt{\dfrac{1-x^{2}}{1-x^{2}y^{2}}}.$$ Question 2 - As a second question I would like to know how can one ""discover"" a transformation of a square into a triangle such as this. Is there any systematic study of this kind of transformations?","The coordinate transformation (due to Beukers, Calabi and Kolk) $$x=\frac{\sin u}{\cos v}$$ $$y=\frac{\sin v}{\cos u}$$ transforms the square domain $0\lt x\lt 1$ and $0\lt y\lt 1$ into the triangle domain $u,v>0,u+v<\pi /2$ (in Proofs from the BOOK by M. Aigner and G. Ziegler). Since the invert transformation is $$u=\arccos \sqrt{\dfrac{1-x^{2}}{1-x^{2}y^{2}}}$$ $$v=\arccos \sqrt{\dfrac{1-y^{2}}{1-x^{2}y^{2}}}$$ it is easy to see that three of the vertices (although not belonging to the domain) are transformed as follows: $$(x,y)=(0,0)\mapsto (0,0)=(u,v),$$ $$(x,y)=(1,0)\mapsto (\pi /2,0)=(u,v),$$ $$(x,y)=(0,1)\mapsto (0,\pi /2)=(u,v).$$ Question 1 - But how is the fourth vertex $(x,y)=(1,1)$ transformed? In the plot below of $\dfrac{1-x^{2}}{1-x^{2}y^{2}}$ seems that the following limit does not exist $$\underset{(x,y)\rightarrow (1,1)}{\lim }\sqrt{\dfrac{1-x^{2}}{1-x^{2}y^{2}}}.$$ Question 2 - As a second question I would like to know how can one ""discover"" a transformation of a square into a triangle such as this. Is there any systematic study of this kind of transformations?",,"['calculus', 'trigonometry', 'limits', 'transformation', 'multivariable-calculus']"
26,Can the second integral of $x^x$ be expressed in terms of the first integral and standard mathatical functions?,Can the second integral of  be expressed in terms of the first integral and standard mathatical functions?,x^x,"Note : by elementary I also mean functions like $\operatorname{Li}(x)$ and $\operatorname{Erfi}(x)$ . Edit: This is not a duplicate. I am not asking if the integral of $x^x$ is elementary. Im asking if the second integral of $x^x$ can be expressed in terms the first + normal mathematical functions. I know the first integral of $x^x$ is not elementary. Please stop directing me to those resources. I decided one day to experiment with inventing a new special function, $\DeclareMathOperator{Ti}{Ti}\Ti_2(x)$ (Tetrational Integral), defined as $$\DeclareMathOperator{Ti}{Ti} \Ti_2(x)=\int_0^x t^tdt. $$ With this new function, any function of the form $f(x)^{f(x)}f'(x)$ can be integrated as $\Ti_2(f(x))$ . In addition, $e^{W(\ln(x))}$ can be trivially integrated, as it is the inverse function of $x^x$ . I have tried finding $$\int \Ti_2(x)dx.$$ But performing integration by parts requires finding $$ \int x^{x+1}dx = \int x\cdot x^xdx, $$ which expands infinitely. By performing an integration by substitution and put $u = \ln(x)$ , (expanding $x\cdot x^x$ to $xe^{x\ln(x)}$ ). I get $\int \Ti_2(x)dx= \int e^{u^2}e^{ue^u}du,$ but I have had no progress after that. It will most likely involve $\operatorname{Erfi}(x)$ due to the presence of the $e^{u^2}$ part, I also know this can be reduced to $x^2x^{x-1}$ which I recall making some more actual progress with, but not a whole bunch. And i don't remember a tone of specifics on that right now. Is it even possible to integrate this in terms of elementary functions (plus common special functions of a single variable like $\operatorname{Li}(x)$ ) and $\Ti_2$ itself? Can someone give me a proof that it is not or is? If not are there any known functions whatsoever this can be done in terms of? (Hyper-geometric functions for example). Finally, as related question can $\Ti_2(x)$ itself (not of its integral) be expressed in terms of generalized hyper-geometric functions or other related functions?","Note : by elementary I also mean functions like and . Edit: This is not a duplicate. I am not asking if the integral of is elementary. Im asking if the second integral of can be expressed in terms the first + normal mathematical functions. I know the first integral of is not elementary. Please stop directing me to those resources. I decided one day to experiment with inventing a new special function, (Tetrational Integral), defined as With this new function, any function of the form can be integrated as . In addition, can be trivially integrated, as it is the inverse function of . I have tried finding But performing integration by parts requires finding which expands infinitely. By performing an integration by substitution and put , (expanding to ). I get but I have had no progress after that. It will most likely involve due to the presence of the part, I also know this can be reduced to which I recall making some more actual progress with, but not a whole bunch. And i don't remember a tone of specifics on that right now. Is it even possible to integrate this in terms of elementary functions (plus common special functions of a single variable like ) and itself? Can someone give me a proof that it is not or is? If not are there any known functions whatsoever this can be done in terms of? (Hyper-geometric functions for example). Finally, as related question can itself (not of its integral) be expressed in terms of generalized hyper-geometric functions or other related functions?","\operatorname{Li}(x) \operatorname{Erfi}(x) x^x x^x x^x \DeclareMathOperator{Ti}{Ti}\Ti_2(x) \DeclareMathOperator{Ti}{Ti}
\Ti_2(x)=\int_0^x t^tdt.
 f(x)^{f(x)}f'(x) \Ti_2(f(x)) e^{W(\ln(x))} x^x \int \Ti_2(x)dx. 
\int x^{x+1}dx = \int x\cdot x^xdx,
 u = \ln(x) x\cdot x^x xe^{x\ln(x)} \int \Ti_2(x)dx= \int e^{u^2}e^{ue^u}du, \operatorname{Erfi}(x) e^{u^2} x^2x^{x-1} \operatorname{Li}(x) \Ti_2 \Ti_2(x)","['calculus', 'integration', 'tetration']"
27,Confusion in finding derivative of $\sqrt{\frac{1-\cos(2x)}{1 + \cos(2x)}}$,Confusion in finding derivative of,\sqrt{\frac{1-\cos(2x)}{1 + \cos(2x)}},"Find $f'(x)$ where $f(x) = \sqrt{\dfrac{1-\cos(2x)}{1 + \cos(2x)}}$ . This question is given in my textbook but I don't agree with the solution given in my book and various sites on the internet. The book shows the following method: $$f(x) = \sqrt{\dfrac{1-\cos(2x)}{1 + \cos(2x)}} =\sqrt{\dfrac{2\sin^2(x)}{2\cos^2(x)}}=\sqrt{\tan^2x}= \tan(x)$$ So the derivative of $\tan(x)$ is $\sec^2(x).$ But my confusion is that, $\sqrt{\tan^2(x)}$ should be $|\tan(x)|$ and so, it's derivative cannot be equal to $\sec^2x$ . Derivative of $|x|$ is $\dfrac{x}{|x|}$ . So, the derivative of $|\tan(x)|$ should be $\dfrac{\tan(x) \cdot \sec^2(x)}{|\tan(x)|}.\bf\qquad\qquad\qquad\qquad....(1)$ Or we can also say that derivative of $|\tan(x)|= \begin{cases}\sec^2{x},\rm If\, tan(x)\ge0\\-\sec^2x, \rm If \tan(x) < 0 \end{cases}.\bf\qquad....(2)$ Am, I right in (1) and (2) ?","Find where . This question is given in my textbook but I don't agree with the solution given in my book and various sites on the internet. The book shows the following method: So the derivative of is But my confusion is that, should be and so, it's derivative cannot be equal to . Derivative of is . So, the derivative of should be Or we can also say that derivative of Am, I right in (1) and (2) ?","f'(x) f(x) = \sqrt{\dfrac{1-\cos(2x)}{1 + \cos(2x)}} f(x) = \sqrt{\dfrac{1-\cos(2x)}{1 + \cos(2x)}} =\sqrt{\dfrac{2\sin^2(x)}{2\cos^2(x)}}=\sqrt{\tan^2x}= \tan(x) \tan(x) \sec^2(x). \sqrt{\tan^2(x)} |\tan(x)| \sec^2x |x| \dfrac{x}{|x|} |\tan(x)| \dfrac{\tan(x) \cdot \sec^2(x)}{|\tan(x)|}.\bf\qquad\qquad\qquad\qquad....(1) |\tan(x)|= \begin{cases}\sec^2{x},\rm If\, tan(x)\ge0\\-\sec^2x, \rm If \tan(x) < 0 \end{cases}.\bf\qquad....(2)","['calculus', 'derivatives']"
28,Compare $\ln(\pi)$ and $\pi-2$ without calculator,Compare  and  without calculator,\ln(\pi) \pi-2,"Unlike the famous question of comparing $e^\pi$ and $\pi^e$ , which I solved almost instantly, I am stuck with this problem.  My thought was the following. Since exponential function is order-preserving, we exponentiate both terms and get $\pi$ and $e^{\pi-2}$ .  Then we study the function $f(x) = e^{x-2} - x$ or the function $g(x) = \frac{e^{x-2}}{x}$ , and compare them with zero and one respectively.  I tried both.  But both involve solving the equation $$e^{x-2} = x.$$ I tried Lagrange error terms and have $$f(x) = -1 + \frac{(x-2)^2}{2!} + R_2(x-2),$$ where $$\frac{(x-2)^3}{3!} \le R_2(x-2) \le \frac{e^{x-2}}{3!} (x-2)^3.$$ It is easy to see that the equation have a root between $3$ and $2 + \sqrt2$ .  But I don't know how close it is to $\pi$ .  It is to provide some lower bounds since we can plug in some values and calculate to show that $f(x) > 0$ for such values.  But for the upper bound, it is hard to calculate by hands since it has the $e^{x-2}$ factor.  At my best attempt by hand, I showed that $f(3.15) > 0$ .  All it entails is that for all $x \ge 3.15$ , $e^{x-2}$ is greater than $x$ .  But it tells nothing about the other side. Then I looked at the calculator and find that $e^{\pi-2} < \pi$ . I also tried Newton-Raphson iteration, but it involves a lot of exponentiation which is hard to calculate by hand and also involves approximation by themselves.  And I don't know how fast and close the iteration converges to the true root of the equation. Any other hint for comparing these two number purely by hand?","Unlike the famous question of comparing and , which I solved almost instantly, I am stuck with this problem.  My thought was the following. Since exponential function is order-preserving, we exponentiate both terms and get and .  Then we study the function or the function , and compare them with zero and one respectively.  I tried both.  But both involve solving the equation I tried Lagrange error terms and have where It is easy to see that the equation have a root between and .  But I don't know how close it is to .  It is to provide some lower bounds since we can plug in some values and calculate to show that for such values.  But for the upper bound, it is hard to calculate by hands since it has the factor.  At my best attempt by hand, I showed that .  All it entails is that for all , is greater than .  But it tells nothing about the other side. Then I looked at the calculator and find that . I also tried Newton-Raphson iteration, but it involves a lot of exponentiation which is hard to calculate by hand and also involves approximation by themselves.  And I don't know how fast and close the iteration converges to the true root of the equation. Any other hint for comparing these two number purely by hand?","e^\pi \pi^e \pi e^{\pi-2} f(x) = e^{x-2} - x g(x) = \frac{e^{x-2}}{x} e^{x-2} = x. f(x) = -1 + \frac{(x-2)^2}{2!} + R_2(x-2), \frac{(x-2)^3}{3!} \le R_2(x-2) \le \frac{e^{x-2}}{3!} (x-2)^3. 3 2 + \sqrt2 \pi f(x) > 0 e^{x-2} f(3.15) > 0 x \ge 3.15 e^{x-2} x e^{\pi-2} < \pi","['calculus', 'numerical-methods', 'approximation']"
29,What is the maximum of $\sum_{k=1}^{\infty} (-1)^k(^kx)$?,What is the maximum of ?,\sum_{k=1}^{\infty} (-1)^k(^kx),"During my testing of the series $\sum\limits_{k=1}^{n} (-1)^k(^kx)$ , I found that the sum converges to two limits when $n \to \infty$ , for $e^{-e} \lt x \le e^{1/e}$ and oscillates between depending on whether $n$ is even or odd. Here, $^kx$ is tetration . The notation $^kx$ is the same as $x^{x^{x^{....}}}$ , which is the application of exponentiation $k-1$ times. Ex. $^3x=x^{x^x}$ . Questions: $(1)$ What is the maximum and minimum of $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for even $n$ ? $(2)$ What is the maximum and minimum of $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for odd $n$ ? Edit 1: Also during my testing in PARI, I observed that the sum seems to converge to two values only in the domain of $e^{-e} \lt x \le e^{1/e}$ . I think the reason for this maybe is that, since $^{\infty}x$ converges only for $e^{-e} \lt x \le e^{1/e}$ , the sum also converges for the same domain. I would appreciate if someone could explain why the sum converges only for $e^{-e} \lt x \le e^{1/e}$ . Edit 2: With the help of user Vepir , I was able to plot $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for both even and odd $n$ . Even $n$ : Odd $n$ : Observations from graphs: $(i.)$ $x=e^{-e}$ is the maximum for $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for both even and odd $n$ when $e^{-e} \lt x \le e^{1/e}$ . $(ii.)$ $x=1$ is the minimum for $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for even $n$ when $e^{-e} \lt x \le e^{1/e}$ . $(iii.)$ $x=e^{1/e}$ is the minimum for $\lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx)$ for odd $n$ when $e^{-e} \lt x \le e^{1/e}$ . Now how can we prove any of the three claims above?","During my testing of the series , I found that the sum converges to two limits when , for and oscillates between depending on whether is even or odd. Here, is tetration . The notation is the same as , which is the application of exponentiation times. Ex. . Questions: What is the maximum and minimum of for even ? What is the maximum and minimum of for odd ? Edit 1: Also during my testing in PARI, I observed that the sum seems to converge to two values only in the domain of . I think the reason for this maybe is that, since converges only for , the sum also converges for the same domain. I would appreciate if someone could explain why the sum converges only for . Edit 2: With the help of user Vepir , I was able to plot for both even and odd . Even : Odd : Observations from graphs: is the maximum for for both even and odd when . is the minimum for for even when . is the minimum for for odd when . Now how can we prove any of the three claims above?",\sum\limits_{k=1}^{n} (-1)^k(^kx) n \to \infty e^{-e} \lt x \le e^{1/e} n ^kx ^kx x^{x^{x^{....}}} k-1 ^3x=x^{x^x} (1) \lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx) n (2) \lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx) n e^{-e} \lt x \le e^{1/e} ^{\infty}x e^{-e} \lt x \le e^{1/e} e^{-e} \lt x \le e^{1/e} \lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx) n n n (i.) x=e^{-e} \lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx) n e^{-e} \lt x \le e^{1/e} (ii.) x=1 \lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx) n e^{-e} \lt x \le e^{1/e} (iii.) x=e^{1/e} \lim\limits_{n\to \infty}\sum\limits_{k=1}^{n} (-1)^k(^kx) n e^{-e} \lt x \le e^{1/e},"['calculus', 'sequences-and-series', 'limits', 'maxima-minima', 'tetration']"
30,Show that $\forall n \in\Bbb N: e < \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right)$,Show that,\forall n \in\Bbb N: e < \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right),"Show that: $$ \forall n \in\Bbb N: e < \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right) $$ Till now i've only worked out a couple of proof sketches, and I don't have an idea how to proceed with them. Please note that this question has already been asked here . The answer there uses derivatives and integrals which i'm not allowed to use . First sketch Consider the sequence: $$ x_n = \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right) $$ One of the ways to show what's required is to show that: $$ x_{n+1} \le x_n $$ namely the sequence is monotonically decreasing. Now given $n\in\Bbb N$ we may calculate $x_1$ : $$ x_1 = \left(1+{1\over 1}\right)^1\left(1+{1\over 2\cdot 1}\right) = 3 $$ Consider the limit: $$ \lim_{n\to\infty}x_n = \lim_{n\to\infty} \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right) = e $$ Now based on the fact that the sequence tends to $e$ and it is monotonically decreasing and $x_1 = 3$ , then it should follow that: $$ \forall n\in\Bbb N: x_n \ge e $$ Here comes the hard part, I could't prove that $x_n$ is monotonically decreasing. I've considered the fraction: $$ {x_{n+1}\over x_n} = \left(1 - {1\over (n+1)^2}\right)^n \cdot \frac{2n(2n+3)(n+2)}{(2n+2)(2n+1)(n+1)} $$ Not sure how to show it is less than $1$ . Second sketch This sketch is based on the idea from my previous question . Namely it has been shown there that: $$ e \le \frac{n+2}{(n+1)(n+1)!} + \sum_{k=0}^n {1\over k!} $$ It looks like : $$ e \le \frac{n+2}{(n+1)(n+1)!} + \sum_{k=0}^n {1\over k!} \le \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right) \tag1 $$ In such case if we prove $(1)$ we are done. I've given it several tries but it gets ugly very soon. The question is: Is it possible to utilize any of those sketches to finish the proof? If not what would be the way to prove the inequality using anything before the definition of a derivative/Taylor series/intergrals Thank you!","Show that: Till now i've only worked out a couple of proof sketches, and I don't have an idea how to proceed with them. Please note that this question has already been asked here . The answer there uses derivatives and integrals which i'm not allowed to use . First sketch Consider the sequence: One of the ways to show what's required is to show that: namely the sequence is monotonically decreasing. Now given we may calculate : Consider the limit: Now based on the fact that the sequence tends to and it is monotonically decreasing and , then it should follow that: Here comes the hard part, I could't prove that is monotonically decreasing. I've considered the fraction: Not sure how to show it is less than . Second sketch This sketch is based on the idea from my previous question . Namely it has been shown there that: It looks like : In such case if we prove we are done. I've given it several tries but it gets ugly very soon. The question is: Is it possible to utilize any of those sketches to finish the proof? If not what would be the way to prove the inequality using anything before the definition of a derivative/Taylor series/intergrals Thank you!","
\forall n \in\Bbb N: e < \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right)
 
x_n = \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right)
 
x_{n+1} \le x_n
 n\in\Bbb N x_1 
x_1 = \left(1+{1\over 1}\right)^1\left(1+{1\over 2\cdot 1}\right) = 3
 
\lim_{n\to\infty}x_n = \lim_{n\to\infty} \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right) = e
 e x_1 = 3 
\forall n\in\Bbb N: x_n \ge e
 x_n 
{x_{n+1}\over x_n} = \left(1 - {1\over (n+1)^2}\right)^n \cdot \frac{2n(2n+3)(n+2)}{(2n+2)(2n+1)(n+1)}
 1 
e \le \frac{n+2}{(n+1)(n+1)!} + \sum_{k=0}^n {1\over k!}
 
e \le \frac{n+2}{(n+1)(n+1)!} + \sum_{k=0}^n {1\over k!} \le \left(1+{1\over n}\right)^n \left(1 + {1\over 2n}\right) \tag1
 (1)","['calculus', 'sequences-and-series', 'limits', 'proof-verification', 'inequality']"
31,When does the limit of $a_n$ exist where $a_{n+1}:=a_n+\frac{a_n^2}{n^2}?$,When does the limit of  exist where,a_n a_{n+1}:=a_n+\frac{a_n^2}{n^2}?,"Consider the recursive relation $a_{n+1}:=a_n+\cfrac{a_n^2}{n^2}$. The existence of $\lim_n a_n$ depends on the  initial value $a_1$. For instance: If $a_1=1$, then $a_n=n$ and the sequence is divergent. If $a_1=0$, then $a_n=0$ and the sequence is convergent. Questions : Numerical calculation shows that if $a_1\in(-2,1)$, then it is convergent. Is that right? How to prove that, and can we find the limit? How about $a_1\in \mathbb{C}$ ? P.S: I found this related to Göbel's Sequence .","Consider the recursive relation $a_{n+1}:=a_n+\cfrac{a_n^2}{n^2}$. The existence of $\lim_n a_n$ depends on the  initial value $a_1$. For instance: If $a_1=1$, then $a_n=n$ and the sequence is divergent. If $a_1=0$, then $a_n=0$ and the sequence is convergent. Questions : Numerical calculation shows that if $a_1\in(-2,1)$, then it is convergent. Is that right? How to prove that, and can we find the limit? How about $a_1\in \mathbb{C}$ ? P.S: I found this related to Göbel's Sequence .",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'telescopic-series']"
32,Does the function $x \mapsto \sqrt{1-x^2}$ have a name?,Does the function  have a name?,x \mapsto \sqrt{1-x^2},"Define a function $$[-1,1] \rightarrow \mathbb{R}$$ $$x \mapsto x^\perp$$ as follows: $$x^\perp = \sqrt{1-x^2}.$$ I find this function to be pedagogically useful, to help students grasp certain facts about the circular functions and relationships between circles and right-angled triangles. Question. Does $x^\perp$ have an accepted name and/or notation? I might as well tell you guys how I use it. I begin by drawing a right-angled triangle with hypotenuse $1$ . One side is labelled $x$ . I ask the student to write find the length of the other side, and to begin by labelling the other side $y$ . So they solve $x^2+y^2=1^2$ for $y$ , and obtain $y = \sqrt{1-x^2}$ . I tell them: lets call $\sqrt{1-x^2}$ the ""Pythagorean complement"" of $x$ , and denote it $x^\perp$ . I get them to compute a few examples until they've internalized the meaning of the formula. Then we graph the whole thing an it ends up looking like a half-circle. Hmmm, why is that? I get them to compute the length of the vector $(x,x^\perp),$ and sure enough it's $1$ . That makes sense; we obtained $x^\perp$ by solving the equation $|(x,y)| = 1$ subject to the constraint $y \geq 0$ , and then defining $y= x^\perp$ . So it stands to reason that $(x,x^\perp)$ is always a point on the upper half of the unit circle. Then we move to trigonometry. I complete the half-circle to a unit circle, and draw a vertical line at $x=\frac{1}{2}$ . What are the $y$ -values of the points of intersection? The student puzzles out that these are $\pm x^\perp$ . I get them to write those two points as $(x,x^\perp)$ and $(x,-x^\perp)$ . To see why this makes sense, we go back to the equation $x^2+y^2 = 1^2$ and solve it properly, with no assumptions on $y$ beyond being a real number and satisfying this equation. We get $$x^2+y^2 = 1^2 \iff y^2=1-x^2 \iff y \in \pm \sqrt{1-x^2} \iff y \in \pm x^\perp.$$ Seems to make sense. I get the student to compute a few examples: $$0^\perp = 1, \qquad \left(\frac{1}{2}\right)^\perp = \frac{\sqrt{3}}{2}, \qquad \left(\frac{1}{\sqrt{2}}\right)^\perp = \frac{1}{\sqrt{2}}, \qquad \mathrm{etc.} $$ I explain that this pairs off each point in $[0,1]$ with another point, such that the ordered pair $(x,x^\perp)$ is always on the unit circle in the first quadrant. We discuss whether or not $x \mapsto x^\perp$ is an involution. On the domain $[0,1]$ , it is. But on $[-1,1]$ , all we can say is that $(x^\perp)^\perp = |x|.$ Finally, we get to solving some trigonometric equations. In order to avoid complex numbers, I define $\mathrm{wrap} \,\theta = (\cos \theta, \sin \theta).$ The student computes $\|\mathrm{wrap}\,\theta\|$ and checks that this is always $1$ . So $\mathrm{wrap}$ lands us on the unit circle; it ""wraps"" the real line around the circle. Finally we try to solve some trigonometric equations. We proceed like so: TFAE $\cos \theta = \frac{1}{2}$ $\mathrm{wrap}\, \theta = \left(\frac{1}{2},\frac{\sqrt{3}}{2}\right) \vee \mathrm{wrap} \,\theta = \left(\frac{1}{2},-\frac{\sqrt{3}}{2}\right)$ $\left(\theta \in \frac{\pi}{3}+2\pi \mathbb{Z}\right) \vee \left(\theta \in \frac{5\pi}{3}+2\pi \mathbb{Z}\right)$ $\theta \in \left(\frac{\pi}{3}+2\pi \mathbb{Z}\right) \cup \left(\frac{5\pi}{3}+2\pi \mathbb{Z}\right)$ I think students appreciate this kind of reasoning, because it helps them understand why there's two families of solutions to each of these problems. For more advanced students, you can show them things like: $$(\cos\theta)^\perp = |\sin \theta|, \qquad (\sin\theta)^\perp = |\cos \theta|$$ $$\cos (\mathrm{arcsin}(x)) = \sin (\mathrm{arccos}(x)) = x^\perp$$ $$\frac{d}{dx} \mathrm{arcsin}(x) = \frac{1}{x^\perp}, \qquad \frac{d}{dx} \mathrm{arccos}(x) = -\frac{1}{x^\perp}, \quad \frac{d}{dx} \mathrm{arcsec} = \frac{1}{|x|x^\perp}$$ Etc. You can also give them some challenge problems related to the geometry of the circle. For instance, ask them to conjecture a value for $\int_{x=-1}^1 x^\perp$ . Addendum. I just learned that this function shows up when sums of inverse trigonometric functions are involved . In particular: $$\sin^{-1}(a) \pm \sin^{-1}(b) = \sin^{-1}(ab^\perp \pm a^\perp b)$$ $$\cos^{-1}(a) \pm cos^{-1}(b) = \cos^{-1}(ab \mp a^\perp b^\perp)$$","Define a function as follows: I find this function to be pedagogically useful, to help students grasp certain facts about the circular functions and relationships between circles and right-angled triangles. Question. Does have an accepted name and/or notation? I might as well tell you guys how I use it. I begin by drawing a right-angled triangle with hypotenuse . One side is labelled . I ask the student to write find the length of the other side, and to begin by labelling the other side . So they solve for , and obtain . I tell them: lets call the ""Pythagorean complement"" of , and denote it . I get them to compute a few examples until they've internalized the meaning of the formula. Then we graph the whole thing an it ends up looking like a half-circle. Hmmm, why is that? I get them to compute the length of the vector and sure enough it's . That makes sense; we obtained by solving the equation subject to the constraint , and then defining . So it stands to reason that is always a point on the upper half of the unit circle. Then we move to trigonometry. I complete the half-circle to a unit circle, and draw a vertical line at . What are the -values of the points of intersection? The student puzzles out that these are . I get them to write those two points as and . To see why this makes sense, we go back to the equation and solve it properly, with no assumptions on beyond being a real number and satisfying this equation. We get Seems to make sense. I get the student to compute a few examples: I explain that this pairs off each point in with another point, such that the ordered pair is always on the unit circle in the first quadrant. We discuss whether or not is an involution. On the domain , it is. But on , all we can say is that Finally, we get to solving some trigonometric equations. In order to avoid complex numbers, I define The student computes and checks that this is always . So lands us on the unit circle; it ""wraps"" the real line around the circle. Finally we try to solve some trigonometric equations. We proceed like so: TFAE I think students appreciate this kind of reasoning, because it helps them understand why there's two families of solutions to each of these problems. For more advanced students, you can show them things like: Etc. You can also give them some challenge problems related to the geometry of the circle. For instance, ask them to conjecture a value for . Addendum. I just learned that this function shows up when sums of inverse trigonometric functions are involved . In particular:","[-1,1] \rightarrow \mathbb{R} x \mapsto x^\perp x^\perp = \sqrt{1-x^2}. x^\perp 1 x y x^2+y^2=1^2 y y = \sqrt{1-x^2} \sqrt{1-x^2} x x^\perp (x,x^\perp), 1 x^\perp |(x,y)| = 1 y \geq 0 y= x^\perp (x,x^\perp) x=\frac{1}{2} y \pm x^\perp (x,x^\perp) (x,-x^\perp) x^2+y^2 = 1^2 y x^2+y^2 = 1^2 \iff y^2=1-x^2 \iff y \in \pm \sqrt{1-x^2} \iff y \in \pm x^\perp. 0^\perp = 1, \qquad \left(\frac{1}{2}\right)^\perp = \frac{\sqrt{3}}{2}, \qquad \left(\frac{1}{\sqrt{2}}\right)^\perp = \frac{1}{\sqrt{2}}, \qquad \mathrm{etc.}  [0,1] (x,x^\perp) x \mapsto x^\perp [0,1] [-1,1] (x^\perp)^\perp = |x|. \mathrm{wrap} \,\theta = (\cos \theta, \sin \theta). \|\mathrm{wrap}\,\theta\| 1 \mathrm{wrap} \cos \theta = \frac{1}{2} \mathrm{wrap}\, \theta = \left(\frac{1}{2},\frac{\sqrt{3}}{2}\right) \vee \mathrm{wrap} \,\theta = \left(\frac{1}{2},-\frac{\sqrt{3}}{2}\right) \left(\theta \in \frac{\pi}{3}+2\pi \mathbb{Z}\right) \vee \left(\theta \in \frac{5\pi}{3}+2\pi \mathbb{Z}\right) \theta \in \left(\frac{\pi}{3}+2\pi \mathbb{Z}\right) \cup \left(\frac{5\pi}{3}+2\pi \mathbb{Z}\right) (\cos\theta)^\perp = |\sin \theta|, \qquad (\sin\theta)^\perp = |\cos \theta| \cos (\mathrm{arcsin}(x)) = \sin (\mathrm{arccos}(x)) = x^\perp \frac{d}{dx} \mathrm{arcsin}(x) = \frac{1}{x^\perp}, \qquad \frac{d}{dx} \mathrm{arccos}(x) = -\frac{1}{x^\perp}, \quad \frac{d}{dx} \mathrm{arcsec} = \frac{1}{|x|x^\perp} \int_{x=-1}^1 x^\perp \sin^{-1}(a) \pm \sin^{-1}(b) = \sin^{-1}(ab^\perp \pm a^\perp b) \cos^{-1}(a) \pm cos^{-1}(b) = \cos^{-1}(ab \mp a^\perp b^\perp)","['calculus', 'geometry', 'terminology', 'triangles', 'education']"
33,Logarithmic Integral II,Logarithmic Integral II,,"While reviewing an old calculus book the following integral was assigned: \begin{align} \int_{0}^{1} \left( x^{a-1} - x^{n-a-1} \right) \, \frac{\ln^{2}x \, dx}{1-x^{n}} = \frac{2 \, \pi^{3} \, \cos\left(\frac{a \pi}{n}\right)}{n^{3} \, \sin^{3}\left(\frac{a \pi}{n}\right)} \end{align} for $a \neq n$. The proposed questions here are: What are some methods to prove the given integral? What are the changes if the power of the logarithm is lowered to one or raised to 3? The old calculus book is: Ralph A Roberts, ""A treatise on the integral calculus; part 1"", 1887. The propoblem presented here is found on page 354 as example 36. The book can be found in the Google Books collection.","While reviewing an old calculus book the following integral was assigned: \begin{align} \int_{0}^{1} \left( x^{a-1} - x^{n-a-1} \right) \, \frac{\ln^{2}x \, dx}{1-x^{n}} = \frac{2 \, \pi^{3} \, \cos\left(\frac{a \pi}{n}\right)}{n^{3} \, \sin^{3}\left(\frac{a \pi}{n}\right)} \end{align} for $a \neq n$. The proposed questions here are: What are some methods to prove the given integral? What are the changes if the power of the logarithm is lowered to one or raised to 3? The old calculus book is: Ralph A Roberts, ""A treatise on the integral calculus; part 1"", 1887. The propoblem presented here is found on page 354 as example 36. The book can be found in the Google Books collection.",,"['calculus', 'integration', 'definite-integrals', 'logarithms']"
34,"Can I interpret the exponential of the derivative operator, $e^D$, as infinite shift operators each shifting ""infinitesimally""?","Can I interpret the exponential of the derivative operator, , as infinite shift operators each shifting ""infinitesimally""?",e^D,"To better explain what I mean, an example can be very useful. Consider $e^{i\theta}$. We could express this using the series definition or the limit definition of $e^x$ instead: $$e^{i\theta} = \lim_{n\to\infty}\left(1+\frac{\theta}{n}i\right)^n$$ If we want to see visually what's going on as $n$ grows we can look at the following GIF from Wikipedia: $\hskip2in$ Essentially each factor in the product above as $n$ grows has its norm shrink to $1$ and its angle get ""infinitesimally"" small. The intuitive idea would be that when you multiply all the factors you add all the (very small) angles to the total $\theta$ and multiply all the norms which in the limit gives $1$. I've considered applying this intuition to the exponential of the derivative operator, which shifts the function it's applied to: $$e^{cD} f(x)=f(x+c)$$ I've considered rewriting the derivative operator as a limit in terms of the shift operator, and then insert it into the product definition of the exponential ($I$ is the identity in what follows), where I take $c=1$ for simplicity: $$Df(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}\equiv\lim_{h\to0}\frac{S(h)-I}{h} f(x)$$ $$e^{D} = \lim_{n\to\infty}\left(1+\frac{1}{n} \lim_{h\to 0}\frac{S(h)-I}{h} \right)^n$$ The only way I see to get to the exponential being the composition of a lot of ""small"" shift operators is if we evaluate the double limit along $h=1/n$ (or $n=1/h$ alternatively) in which case the above simplifies and we get $e^{D}=\lim_{n\to\infty}S^n(1/n)=S(1)$ but I cannot think of any way to justify taking that specific path, $h=1/n$, when evaluating the limits. To sum up, I would like help with two things: Is it conceptually sound to interpret the $e^D$ in the above manner? If it is, can I prove with some rigour the above intuition? Thank you for your time.","To better explain what I mean, an example can be very useful. Consider $e^{i\theta}$. We could express this using the series definition or the limit definition of $e^x$ instead: $$e^{i\theta} = \lim_{n\to\infty}\left(1+\frac{\theta}{n}i\right)^n$$ If we want to see visually what's going on as $n$ grows we can look at the following GIF from Wikipedia: $\hskip2in$ Essentially each factor in the product above as $n$ grows has its norm shrink to $1$ and its angle get ""infinitesimally"" small. The intuitive idea would be that when you multiply all the factors you add all the (very small) angles to the total $\theta$ and multiply all the norms which in the limit gives $1$. I've considered applying this intuition to the exponential of the derivative operator, which shifts the function it's applied to: $$e^{cD} f(x)=f(x+c)$$ I've considered rewriting the derivative operator as a limit in terms of the shift operator, and then insert it into the product definition of the exponential ($I$ is the identity in what follows), where I take $c=1$ for simplicity: $$Df(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}\equiv\lim_{h\to0}\frac{S(h)-I}{h} f(x)$$ $$e^{D} = \lim_{n\to\infty}\left(1+\frac{1}{n} \lim_{h\to 0}\frac{S(h)-I}{h} \right)^n$$ The only way I see to get to the exponential being the composition of a lot of ""small"" shift operators is if we evaluate the double limit along $h=1/n$ (or $n=1/h$ alternatively) in which case the above simplifies and we get $e^{D}=\lim_{n\to\infty}S^n(1/n)=S(1)$ but I cannot think of any way to justify taking that specific path, $h=1/n$, when evaluating the limits. To sum up, I would like help with two things: Is it conceptually sound to interpret the $e^D$ in the above manner? If it is, can I prove with some rigour the above intuition? Thank you for your time.",,"['calculus', 'derivatives', 'exponential-function']"
35,Closed form of $ \int _{ 0 }^{ \pi /2 }{ x\sqrt { \tan { x } } \log { (\cos { x } ) }\ dx }$,Closed form of, \int _{ 0 }^{ \pi /2 }{ x\sqrt { \tan { x } } \log { (\cos { x } ) }\ dx },"Does there exists a closed form of$$ \displaystyle \int _{ 0 }^{ \pi /2 }{ x\sqrt { \tan { x }  } \log { (\cos { x } ) }\ dx }$$ If exists can someone find a way to tackle this integral and provide a closed-form of it. Many similar integrals have closed form and I believe this one, too.","Does there exists a closed form of$$ \displaystyle \int _{ 0 }^{ \pi /2 }{ x\sqrt { \tan { x }  } \log { (\cos { x } ) }\ dx }$$ If exists can someone find a way to tackle this integral and provide a closed-form of it. Many similar integrals have closed form and I believe this one, too.",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'trigonometric-integrals']"
36,Function for which trapezoidal rule outperforms midpoint rule for every $n$,Function for which trapezoidal rule outperforms midpoint rule for every,n,"Is there a continuous elementary function $f:[0,1]\to [0,\infty)$ such that for every $n$ the trapezoidal approximation to $\int_{0}^{1}f(x)\,dx$ with $n$ trapezoids is strictly better than the midpoint approximation with $n$ rectangles? The point of this questions is that even though the midpoint approximation to an integral is generally better than the trapezoidal approximation, there is, for each $n$, a continuous function $f:[0,1]\to \mathbb{R}$ such that the trapezoidal approximation to $\int_{0}^{1}f(x)\,dx$ with $n$ trapezoids is better than the midpoint approximation with $n$ rectangles. See here for an example. I added the restriction that $f$ be elementary so I can talk about the answer with my calculus students. I added the restriction that $f$ be non-negative for simplicity.","Is there a continuous elementary function $f:[0,1]\to [0,\infty)$ such that for every $n$ the trapezoidal approximation to $\int_{0}^{1}f(x)\,dx$ with $n$ trapezoids is strictly better than the midpoint approximation with $n$ rectangles? The point of this questions is that even though the midpoint approximation to an integral is generally better than the trapezoidal approximation, there is, for each $n$, a continuous function $f:[0,1]\to \mathbb{R}$ such that the trapezoidal approximation to $\int_{0}^{1}f(x)\,dx$ with $n$ trapezoids is better than the midpoint approximation with $n$ rectangles. See here for an example. I added the restriction that $f$ be elementary so I can talk about the answer with my calculus students. I added the restriction that $f$ be non-negative for simplicity.",,"['calculus', 'integration', 'approximation', 'approximation-theory']"
37,Evaluation of $\int_{0}^{\frac{\pi}{2}}\frac{\sin (2015x)}{\sin x+\cos x}dx$,Evaluation of,\int_{0}^{\frac{\pi}{2}}\frac{\sin (2015x)}{\sin x+\cos x}dx,Evaluate the definite integral   $$ I=\int_{0}^{\pi/2}\frac{\sin (2015x)}{\sin x+\cos x}\;dx $$ My Attempt: Using the identity $$ \int_{0}^{a}f(x)\;dx = \int_{0}^{a}f(a-x)\;dx $$ Exchange $\left(\frac{\pi}{2}-x\right)$ for $x$ in the integral to get $$ \begin{align} \int_0^{\pi/2}\frac{\sin (2015x)}{\sin x+\cos x}\;dx &= \int_0^{\pi/2}\frac{\sin \left(\frac{2015\pi}{2}-2015 x\right)}{\cos x+\sin x}\;dx\\ &= \int_0^{\pi/2}\frac{\cos (2015x)}{\sin x+\cos x}\;dx \end{align} $$ How can I complete the solution from this point?,Evaluate the definite integral   $$ I=\int_{0}^{\pi/2}\frac{\sin (2015x)}{\sin x+\cos x}\;dx $$ My Attempt: Using the identity $$ \int_{0}^{a}f(x)\;dx = \int_{0}^{a}f(a-x)\;dx $$ Exchange $\left(\frac{\pi}{2}-x\right)$ for $x$ in the integral to get $$ \begin{align} \int_0^{\pi/2}\frac{\sin (2015x)}{\sin x+\cos x}\;dx &= \int_0^{\pi/2}\frac{\sin \left(\frac{2015\pi}{2}-2015 x\right)}{\cos x+\sin x}\;dx\\ &= \int_0^{\pi/2}\frac{\cos (2015x)}{\sin x+\cos x}\;dx \end{align} $$ How can I complete the solution from this point?,,"['calculus', 'definite-integrals']"
38,Integral $\int_{0}^{\frac{\pi}{4}}\ln\left(\sqrt{\sin^3{x}}+\sqrt{\cos^3{x}}\right)dx $,Integral,\int_{0}^{\frac{\pi}{4}}\ln\left(\sqrt{\sin^3{x}}+\sqrt{\cos^3{x}}\right)dx ,"Yesterday my friend emailed me a math question, which perplexed me greatly when I first read it. I've been working on it, but I can't solve it. Show that $$\int_{0}^{\frac{\pi}{4}}\ln\left(\sqrt{\sin^3{x}}+\sqrt{\cos^3{x}}\right)dx=\dfrac{G}{12}-\dfrac{5\pi}{16}\ln{2}+\dfrac{\pi}{8}\ln{(2-\sqrt{2})}-\dfrac{\pi}{8}\ln{(2+\sqrt{2})}-\dfrac{\pi}{3}\ln{(\sqrt{3}-1)}+\dfrac{\pi}{3}\ln{(1+\sqrt{3})}$$ where $G$ is Catalan's constant I hope someone can help me. Thank you.","Yesterday my friend emailed me a math question, which perplexed me greatly when I first read it. I've been working on it, but I can't solve it. Show that where is Catalan's constant I hope someone can help me. Thank you.",\int_{0}^{\frac{\pi}{4}}\ln\left(\sqrt{\sin^3{x}}+\sqrt{\cos^3{x}}\right)dx=\dfrac{G}{12}-\dfrac{5\pi}{16}\ln{2}+\dfrac{\pi}{8}\ln{(2-\sqrt{2})}-\dfrac{\pi}{8}\ln{(2+\sqrt{2})}-\dfrac{\pi}{3}\ln{(\sqrt{3}-1)}+\dfrac{\pi}{3}\ln{(1+\sqrt{3})} G,"['calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
39,General formula or a pattern for the $n$th derivatives of $e^{f(x)}$?,General formula or a pattern for the th derivatives of ?,n e^{f(x)},"I want to find the $nth$ derivatives of the function $e^{f(x)}$ with respect to $x$, the first derivative is $$e^{f(x)}f^{\prime}(x).$$  The second derivative is $$\left( {\frac {d^{2}}{d{x}^{2}}}f \left( x \right)  \right) {{\rm e}^ {f \left( x \right) }}+ \left( {\frac {d}{dx}}f \left( x \right)   \right) ^{2}{{\rm e}^{f \left( x \right) }} .$$ The third derivative is $$\left( {\frac {d^{3}}{d{x}^{3}}}f \left( x \right)  \right) {{\rm e}^ {f \left( x \right) }}+3\, \left( {\frac {d^{2}}{d{x}^{2}}}f \left( x  \right)  \right)  \left( {\frac {d}{dx}}f \left( x \right)  \right) { {\rm e}^{f \left( x \right) }}+ \left( {\frac {d}{dx}}f \left( x  \right)  \right) ^{3}{{\rm e}^{f \left( x \right) }} $$  The fourth derivative is $$ \left( {\frac {d^{4}}{d{x}^{4}}}f \left( x \right)  \right) {{\rm e}^ {f \left( x \right) }}+4\, \left( {\frac {d^{3}}{d{x}^{3}}}f \left( x  \right)  \right)  \left( {\frac {d}{dx}}f \left( x \right)  \right) { {\rm e}^{f \left( x \right) }}+3\, \left( {\frac {d^{2}}{d{x}^{2}}}f  \left( x \right)  \right) ^{2}{{\rm e}^{f \left( x \right) }}+6\,  \left( {\frac {d^{2}}{d{x}^{2}}}f \left( x \right)  \right)  \left( { \frac {d}{dx}}f \left( x \right)  \right) ^{2}{{\rm e}^{f \left( x  \right) }}+ \left( {\frac {d}{dx}}f \left( x \right)  \right) ^{4}{ {\rm e}^{f \left( x \right) }} $$ My question is: Is there a general formula or a pattern for the nth derivative of $e^{f(x)}$. You may use the maple command diff(exp(f(x)), x$5) to do some experiments. Thanks a lot ^^","I want to find the $nth$ derivatives of the function $e^{f(x)}$ with respect to $x$, the first derivative is $$e^{f(x)}f^{\prime}(x).$$  The second derivative is $$\left( {\frac {d^{2}}{d{x}^{2}}}f \left( x \right)  \right) {{\rm e}^ {f \left( x \right) }}+ \left( {\frac {d}{dx}}f \left( x \right)   \right) ^{2}{{\rm e}^{f \left( x \right) }} .$$ The third derivative is $$\left( {\frac {d^{3}}{d{x}^{3}}}f \left( x \right)  \right) {{\rm e}^ {f \left( x \right) }}+3\, \left( {\frac {d^{2}}{d{x}^{2}}}f \left( x  \right)  \right)  \left( {\frac {d}{dx}}f \left( x \right)  \right) { {\rm e}^{f \left( x \right) }}+ \left( {\frac {d}{dx}}f \left( x  \right)  \right) ^{3}{{\rm e}^{f \left( x \right) }} $$  The fourth derivative is $$ \left( {\frac {d^{4}}{d{x}^{4}}}f \left( x \right)  \right) {{\rm e}^ {f \left( x \right) }}+4\, \left( {\frac {d^{3}}{d{x}^{3}}}f \left( x  \right)  \right)  \left( {\frac {d}{dx}}f \left( x \right)  \right) { {\rm e}^{f \left( x \right) }}+3\, \left( {\frac {d^{2}}{d{x}^{2}}}f  \left( x \right)  \right) ^{2}{{\rm e}^{f \left( x \right) }}+6\,  \left( {\frac {d^{2}}{d{x}^{2}}}f \left( x \right)  \right)  \left( { \frac {d}{dx}}f \left( x \right)  \right) ^{2}{{\rm e}^{f \left( x  \right) }}+ \left( {\frac {d}{dx}}f \left( x \right)  \right) ^{4}{ {\rm e}^{f \left( x \right) }} $$ My question is: Is there a general formula or a pattern for the nth derivative of $e^{f(x)}$. You may use the maple command diff(exp(f(x)), x$5) to do some experiments. Thanks a lot ^^",,"['calculus', 'combinatorics']"
40,Derivative of matrix involving trace and log,Derivative of matrix involving trace and log,,"I'm stuck on this problem. Let $X\in\mathbb{R}^{n\times n}$, compute the following matrix derivatives $$\frac{\partial}{\partial X}\mathrm{tr}(\log(XA)\log(XA)^\top),$$ $$\frac{\partial}{\partial X}\mathrm{tr}(B\log(XA)), $$ where $\log(\cdot)$ is the matrix logarithm (not element-wise) and $A,B\in\mathbb{R}^{n\times n}$ are constant matrices. Thanks in advance for your suggestions!","I'm stuck on this problem. Let $X\in\mathbb{R}^{n\times n}$, compute the following matrix derivatives $$\frac{\partial}{\partial X}\mathrm{tr}(\log(XA)\log(XA)^\top),$$ $$\frac{\partial}{\partial X}\mathrm{tr}(B\log(XA)), $$ where $\log(\cdot)$ is the matrix logarithm (not element-wise) and $A,B\in\mathbb{R}^{n\times n}$ are constant matrices. Thanks in advance for your suggestions!",,"['calculus', 'matrices', 'derivatives', 'matrix-calculus']"
41,Evaluating $\int_{0}^{1} x^m \ln^\alpha(x) dx$,Evaluating,\int_{0}^{1} x^m \ln^\alpha(x) dx,"Honestly, I am asked to think about $$\int_{0}^{1} x^m \ln^\alpha(x) dx$$ And I applied all methods I know. I doubt if this integral makes sense either. If it is replicate, plz inform me to omit the question soon. Thanks.","Honestly, I am asked to think about $$\int_{0}^{1} x^m \ln^\alpha(x) dx$$ And I applied all methods I know. I doubt if this integral makes sense either. If it is replicate, plz inform me to omit the question soon. Thanks.",,"['calculus', 'integration', 'definite-integrals']"
42,Can I use L'Hopital's to show $\lim_{x\to1^-}(1-x)[\frac{d}{dx}(1-x)\sum_{n=1}^\infty a_nx^n]=0$ for $a_n$ a bounded sequence of reals?,Can I use L'Hopital's to show  for  a bounded sequence of reals?,\lim_{x\to1^-}(1-x)[\frac{d}{dx}(1-x)\sum_{n=1}^\infty a_nx^n]=0 a_n,"I am attempting to prove that if $a_n$ is a bounded sequence of real numbers then $$\lim_{x\to1^-}(1-x)\left[\frac{d}{dx}(1-x)\sum_{n=1}^{\infty}a_nx^n\right]=0$$ My approach is to first make some algebraic manipulations, namely we see that \begin{align*} 1&=\lim_{x\to1^-}\frac{(1-x)\sum_{n=1}^{\infty}a_nx^n}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\\ &=\lim_{x\to1^-}\frac{1}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\left(\frac{1-x}{\frac{1}{\sum_{n=1}^{\infty}a_nx^n}}\right)\\ \end{align*} The reason I want to do this is that if I were able to apply L'Hopital's rule to $$\frac{1-x}{\frac{1}{\sum_{n=1}^{\infty}a_nx^n}}$$ then I would get that \begin{align*} 1&=\lim_{x\to1^-}\frac{1}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\left(\frac{-1}{-\frac{\sum_{n=1}^{\infty}na_nx^{n-1}}{\left(\sum_{n=1}^{\infty}a_nx^n\right)^2}}\right)\\ &=\lim_{x\to1^-}\frac{\sum_{n=1}^{\infty}a_nx^n}{(1-x)\sum_{n=1}^{\infty}na_nx^{n-1}}\\ \end{align*} From there we can subtract $1$ from both sides and multiply top and bottom by $(1-x)$ to get that $$\lim_{x\to1^-}\frac{\left(1-x\right)\sum_{n=1}^{\infty}a_{n}x^{n}-\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}}{\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}}=0$$ Since $$\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}$$ is bounded, the only way for this quantity to go to zero would be for $$\left(1-x\right)\sum_{n=1}^{\infty}a_{n}x^{n}-\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}=(1-x)\left[\frac{d}{dx}(1-x)\sum_{n=1}^{\infty}a_nx^n\right]$$ to go to $0$ , thus yielding what we want. I am not sure if this use of L'Hopitals is (or can be) justified, since the limit of $$\frac{-1}{-\frac{\sum_{n=1}^{\infty}na_nx^{n-1}}{\left(\sum_{n=1}^{\infty}a_nx^n\right)^2}}$$ as $x\to1^-$ is not required to exist. Is there any way I can make this argument rigorous? EDIT: If I had the pair of inequalities $$\limsup_{x\to 1^-}k(x)\frac{f(x)}{g(x)}\leq \limsup_{x\to 1^-}k(x)\frac{f'(x)}{g'(x)}$$ $$\liminf_{x\to 1^-}k(x)\frac{f'(x)}{g'(x)} \leq \liminf_{x\to 1^-}k(x)\frac{f(x)}{g(x)}$$ for differentiable functions $f$ , $g$ and $k$ on $[0,1)$ then I could resolve my issue. On wikipedia it states that $$\liminf_{x\to1^-}\frac{f'(x)}{g'(x)}\leq \liminf_{x\to1^-}\frac{f(x)}{g(x)} \leq \limsup_{x\to1^-}\frac{f(x)}{g(x)}\leq \limsup_{x\to1^-}\frac{f'(x)}{g'(x)}$$ but I can't complete the argument for when the factor of $k(x)$ is added.","I am attempting to prove that if is a bounded sequence of real numbers then My approach is to first make some algebraic manipulations, namely we see that The reason I want to do this is that if I were able to apply L'Hopital's rule to then I would get that From there we can subtract from both sides and multiply top and bottom by to get that Since is bounded, the only way for this quantity to go to zero would be for to go to , thus yielding what we want. I am not sure if this use of L'Hopitals is (or can be) justified, since the limit of as is not required to exist. Is there any way I can make this argument rigorous? EDIT: If I had the pair of inequalities for differentiable functions , and on then I could resolve my issue. On wikipedia it states that but I can't complete the argument for when the factor of is added.","a_n \lim_{x\to1^-}(1-x)\left[\frac{d}{dx}(1-x)\sum_{n=1}^{\infty}a_nx^n\right]=0 \begin{align*}
1&=\lim_{x\to1^-}\frac{(1-x)\sum_{n=1}^{\infty}a_nx^n}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\\
&=\lim_{x\to1^-}\frac{1}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\left(\frac{1-x}{\frac{1}{\sum_{n=1}^{\infty}a_nx^n}}\right)\\
\end{align*} \frac{1-x}{\frac{1}{\sum_{n=1}^{\infty}a_nx^n}} \begin{align*}
1&=\lim_{x\to1^-}\frac{1}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\left(\frac{-1}{-\frac{\sum_{n=1}^{\infty}na_nx^{n-1}}{\left(\sum_{n=1}^{\infty}a_nx^n\right)^2}}\right)\\
&=\lim_{x\to1^-}\frac{\sum_{n=1}^{\infty}a_nx^n}{(1-x)\sum_{n=1}^{\infty}na_nx^{n-1}}\\
\end{align*} 1 (1-x) \lim_{x\to1^-}\frac{\left(1-x\right)\sum_{n=1}^{\infty}a_{n}x^{n}-\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}}{\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}}=0 \left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1} \left(1-x\right)\sum_{n=1}^{\infty}a_{n}x^{n}-\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}=(1-x)\left[\frac{d}{dx}(1-x)\sum_{n=1}^{\infty}a_nx^n\right] 0 \frac{-1}{-\frac{\sum_{n=1}^{\infty}na_nx^{n-1}}{\left(\sum_{n=1}^{\infty}a_nx^n\right)^2}} x\to1^- \limsup_{x\to 1^-}k(x)\frac{f(x)}{g(x)}\leq \limsup_{x\to 1^-}k(x)\frac{f'(x)}{g'(x)} \liminf_{x\to 1^-}k(x)\frac{f'(x)}{g'(x)} \leq \liminf_{x\to 1^-}k(x)\frac{f(x)}{g(x)} f g k [0,1) \liminf_{x\to1^-}\frac{f'(x)}{g'(x)}\leq \liminf_{x\to1^-}\frac{f(x)}{g(x)} \leq \limsup_{x\to1^-}\frac{f(x)}{g(x)}\leq \limsup_{x\to1^-}\frac{f'(x)}{g'(x)} k(x)","['calculus', 'limits']"
43,"Generalizing Archimedes' ""The Quadrature of the Parabola""","Generalizing Archimedes' ""The Quadrature of the Parabola""",,"In the third century BC Archimedes discovered that The area enclosed by a parabola and a line (left figure) is 4/3 that of a related inscribed triangle (right figure). Consequentially, the area enclosed by a parabola and a line is 2/3 that of a parallelogram which has the chord and its tangential-to-the-parabola-copy as two of its sides. I have tried to derive this result myself using calculus. Suppose $f:I \subseteq \mathbf{R} \to \mathbf{R}$ is a smooth convex function (so that the chord is on a fixed side of the graph), defined over some interval $I$ . For $a < b$ , let $[a,b] \subseteq I$ be a closed subinterval. The area of the segment bounded by the graph of $f$ and the chord $\overline{(a,f(a)) (b,f(b))}$ is given by $$\int_a^b \left( f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - f(t) \right) \mathrm{d}t. \tag{1}$$ For the areas of the inscribed triangle and related parallelogram, we will use the point $c \in (a,b)$ , which has the slope $$f'(c) =\frac{f(b)-f(a)}{b-a}.$$ Such a point exists according to the MVT, and it is unique because of the convexity of $f$ . Thus $$c = c(a,b)= f'^{-1} \left( \frac{f(b)-f(a)}{b-a} \right). $$ The area of the aforementioned parallelogram is then the area between two parallel segments $$\int_a^b \left[ f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - \left( f(c)+(t-c) \frac{f(b)-f(a)}{b-a} \right) \right] \mathrm{d}t.$$ Archimedes' result implies that $$ \begin{align}&\int_a^b \left( f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - f(t) \right) \mathrm{d}t \\ &=  k \int_a^b \left[ f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - \left( f(c)+(t-c) \frac{f(b)-f(a)}{b-a} \right) \right] \mathrm{d}t \end{align} \tag{3}$$ for $f(x)=x^2$ , $a<b$ , $c(a,b)=\frac{a+b}{2}$ and $k=\frac{2}{3}$ . My questions are about going in reverse: Is it possible to systematically arrive at $f(x)=x^2$ (or a similar parabola), starting off with knowledge that $f$ is convex over some interval, $f$ satisfies Equation $(3)$ for all subintervals $[a,b]$ of its domain and $k=\frac{2}{3}$ ? Is it possible to generalize this result by solving Equation $(3)$ for $k \neq \frac{2}{3}$ (clearly, $k \leq 1$ )? I would appreciate guidance on how to do this if possible. Thanks!","In the third century BC Archimedes discovered that The area enclosed by a parabola and a line (left figure) is 4/3 that of a related inscribed triangle (right figure). Consequentially, the area enclosed by a parabola and a line is 2/3 that of a parallelogram which has the chord and its tangential-to-the-parabola-copy as two of its sides. I have tried to derive this result myself using calculus. Suppose is a smooth convex function (so that the chord is on a fixed side of the graph), defined over some interval . For , let be a closed subinterval. The area of the segment bounded by the graph of and the chord is given by For the areas of the inscribed triangle and related parallelogram, we will use the point , which has the slope Such a point exists according to the MVT, and it is unique because of the convexity of . Thus The area of the aforementioned parallelogram is then the area between two parallel segments Archimedes' result implies that for , , and . My questions are about going in reverse: Is it possible to systematically arrive at (or a similar parabola), starting off with knowledge that is convex over some interval, satisfies Equation for all subintervals of its domain and ? Is it possible to generalize this result by solving Equation for (clearly, )? I would appreciate guidance on how to do this if possible. Thanks!","f:I \subseteq \mathbf{R} \to \mathbf{R} I a < b [a,b] \subseteq I f \overline{(a,f(a)) (b,f(b))} \int_a^b \left( f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - f(t) \right) \mathrm{d}t. \tag{1} c \in (a,b) f'(c) =\frac{f(b)-f(a)}{b-a}. f c = c(a,b)= f'^{-1} \left( \frac{f(b)-f(a)}{b-a} \right).  \int_a^b \left[ f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - \left( f(c)+(t-c) \frac{f(b)-f(a)}{b-a} \right) \right] \mathrm{d}t. 
\begin{align}&\int_a^b \left( f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - f(t) \right) \mathrm{d}t \\
&= 
k \int_a^b \left[ f(a)+(t-a) \frac{f(b)-f(a)}{b-a} - \left( f(c)+(t-c) \frac{f(b)-f(a)}{b-a} \right) \right] \mathrm{d}t
\end{align} \tag{3} f(x)=x^2 a<b c(a,b)=\frac{a+b}{2} k=\frac{2}{3} f(x)=x^2 f f (3) [a,b] k=\frac{2}{3} (3) k \neq \frac{2}{3} k \leq 1","['calculus', 'integration', 'geometry', 'ordinary-differential-equations', 'integral-equations']"
44,the sum of two arbitrary different numbers in the sequence is always coprime with the sum of three arbitrary different numbers,the sum of two arbitrary different numbers in the sequence is always coprime with the sum of three arbitrary different numbers,,"Does there exist a sequence of infinite positive integers $a_1,a_2,...$ such that the sum of two arbitrary different numbers in the sequence is always coprime with the sum of three arbitrary different numbers in the sequence? Assume that there exist such sequence. For every prime $p$ , if there exist three numbers from the sequence $a_i,a_j,a_k$ that are divisible by $p$ , then $a_i+a_j$ is not coprime with $a_i+a_j+a_k$ , contradiction. Thus $p$ can divide at most two numbers from the sequence. Therefore there are at most two even numbers from the sequence and there are infinite odd numbers from that sequence. However, if there are two even numbers, then the sum of two odd numbers is not coprime with the sum of two odd numbers and an even one, since both of the sum are greater than $2$ and are both even. Hence there are no even numbers in the sequence. Here I am stuck. How can I progress ? Is there a better way to solve the problem ?","Does there exist a sequence of infinite positive integers such that the sum of two arbitrary different numbers in the sequence is always coprime with the sum of three arbitrary different numbers in the sequence? Assume that there exist such sequence. For every prime , if there exist three numbers from the sequence that are divisible by , then is not coprime with , contradiction. Thus can divide at most two numbers from the sequence. Therefore there are at most two even numbers from the sequence and there are infinite odd numbers from that sequence. However, if there are two even numbers, then the sum of two odd numbers is not coprime with the sum of two odd numbers and an even one, since both of the sum are greater than and are both even. Hence there are no even numbers in the sequence. Here I am stuck. How can I progress ? Is there a better way to solve the problem ?","a_1,a_2,... p a_i,a_j,a_k p a_i+a_j a_i+a_j+a_k p 2","['calculus', 'combinatorics', 'number-theory', 'elementary-number-theory']"
45,Evaluating $\int_0^\infty\frac{\ln(1+x^2)}{e^{\pi x}-1}dx$,Evaluating,\int_0^\infty\frac{\ln(1+x^2)}{e^{\pi x}-1}dx,"I want to evaluate $$\int_0^\infty\frac{\ln(1+x^2)}{e^{\pi x}-1}\,\mathrm dx$$ I tried to let $$I(a)=\int_0^\infty\frac{\ln(1+a^2x^2)}{e^{\pi x}-1}\,\mathrm dx,$$ and then $$ \begin{align} I'(a)&=\int_0^\infty\frac{2ax^2}{(1+a^2x^2)(e^{\pi x}-1)}\,\mathrm dx\\[25pt] &=2a\int_0^\infty\mathscr L^{-1}\Big(\frac{x}{1+a^2x^2}\Big)\mathscr L\Big(\frac{x}{e^{\pi x}-1}\Big)\,\mathrm dx\\[25pt] &=-2a^{-2}\pi^{-2}\int_0^\infty \sin\frac xa \psi^{(1)}\left(1+\frac x\pi\right)\,\mathrm dx \end{align} $$ I can't go further.","I want to evaluate $$\int_0^\infty\frac{\ln(1+x^2)}{e^{\pi x}-1}\,\mathrm dx$$ I tried to let $$I(a)=\int_0^\infty\frac{\ln(1+a^2x^2)}{e^{\pi x}-1}\,\mathrm dx,$$ and then $$ \begin{align} I'(a)&=\int_0^\infty\frac{2ax^2}{(1+a^2x^2)(e^{\pi x}-1)}\,\mathrm dx\\[25pt] &=2a\int_0^\infty\mathscr L^{-1}\Big(\frac{x}{1+a^2x^2}\Big)\mathscr L\Big(\frac{x}{e^{\pi x}-1}\Big)\,\mathrm dx\\[25pt] &=-2a^{-2}\pi^{-2}\int_0^\infty \sin\frac xa \psi^{(1)}\left(1+\frac x\pi\right)\,\mathrm dx \end{align} $$ I can't go further.",,"['calculus', 'integration', 'definite-integrals']"
46,Integration technique of writing $\int_0^{\infty}$ as $\int_0^1 + \int_1^{\infty}$ and using the substitution $\frac{1}{x} \leftrightarrow u$,Integration technique of writing  as  and using the substitution,\int_0^{\infty} \int_0^1 + \int_1^{\infty} \frac{1}{x} \leftrightarrow u,"I've recently seen this idea applied a few times to evaluate otherwise difficult integrals . Suppose you have an integral $\int_0^{\infty} f(x)\,\mathrm{d}x$. You can break up the domain of integration and make the substitution $\frac{1}{x}\leftrightarrow u$ to arrive at an integral which may be easier to evaluate. $$\begin{align}   \int\limits_0^{\infty} f(x)\,\mathrm{d}x \;\;&=\;\;      \int\limits_0^1 f(x)\,\mathrm{d}x +      \int\limits_1^{\infty} f(u)\,\mathrm{d}u     \\\;\;&=\;\;\int\limits_0^1 f(x)\,\mathrm{d}x +      \int\limits_0^1 \frac{f\!\left(\frac{1}{x}\right)}{x^2}\,\mathrm{d}x \;\;=\;\;      \int\limits_0^1 \frac{x^2f(x)+f\!\left(\frac{1}{x}\right)}{x^2}\,\mathrm{d}x \end{align}$$ Does this technique have a name? Are there broad families of functions for which this trick is essential for integration, or does this only help in very specific cases? Can this idea be applied generally enough to be worth showing students?","I've recently seen this idea applied a few times to evaluate otherwise difficult integrals . Suppose you have an integral $\int_0^{\infty} f(x)\,\mathrm{d}x$. You can break up the domain of integration and make the substitution $\frac{1}{x}\leftrightarrow u$ to arrive at an integral which may be easier to evaluate. $$\begin{align}   \int\limits_0^{\infty} f(x)\,\mathrm{d}x \;\;&=\;\;      \int\limits_0^1 f(x)\,\mathrm{d}x +      \int\limits_1^{\infty} f(u)\,\mathrm{d}u     \\\;\;&=\;\;\int\limits_0^1 f(x)\,\mathrm{d}x +      \int\limits_0^1 \frac{f\!\left(\frac{1}{x}\right)}{x^2}\,\mathrm{d}x \;\;=\;\;      \int\limits_0^1 \frac{x^2f(x)+f\!\left(\frac{1}{x}\right)}{x^2}\,\mathrm{d}x \end{align}$$ Does this technique have a name? Are there broad families of functions for which this trick is essential for integration, or does this only help in very specific cases? Can this idea be applied generally enough to be worth showing students?",,"['calculus', 'integration', 'improper-integrals']"
47,Find $ ? = \sqrt[3] {1 + \sqrt[3] {1 + 2 \sqrt[3] {1 + 3 \sqrt[3] \cdots}}} $,Find, ? = \sqrt[3] {1 + \sqrt[3] {1 + 2 \sqrt[3] {1 + 3 \sqrt[3] \cdots}}} ,"I wonder about a closed form for $ ? = \sqrt[3] {1 + \sqrt[3] {1 + 2 \sqrt[3] {1 + 3 \sqrt[3] {1 + 4 \sqrt[3] {1 + 5 \sqrt[3] \cdots}}}}} $ To be clear $$? = \sqrt[3]{ 1 + \color{Red}{1}\sqrt[3]{ 1 + \color{Red}{2} \sqrt[3]{ 1 + \color{Red}{3} \sqrt[3]{\cdots}}}} $$ Where the red coefficients are just the natural numbers. I tried solving the related equation $ f(x) ^3 = 1 + (x+y) f(x+1) $ for various fixed integer values $y,$ but I failed. It appears $$ ? = \sqrt[3] {1 + \sqrt[3] 5} $$ But I am not able to prove it. See also https://en.m.wikipedia.org/wiki/Nested_radical#Ramanujan.27s_infinite_radicals",I wonder about a closed form for To be clear Where the red coefficients are just the natural numbers. I tried solving the related equation for various fixed integer values but I failed. It appears But I am not able to prove it. See also https://en.m.wikipedia.org/wiki/Nested_radical#Ramanujan.27s_infinite_radicals," ? = \sqrt[3] {1 + \sqrt[3] {1 + 2 \sqrt[3] {1 + 3 \sqrt[3] {1 + 4 \sqrt[3] {1 + 5 \sqrt[3] \cdots}}}}}  ? = \sqrt[3]{ 1 + \color{Red}{1}\sqrt[3]{ 1 + \color{Red}{2} \sqrt[3]{ 1 + \color{Red}{3} \sqrt[3]{\cdots}}}}   f(x) ^3 = 1 + (x+y) f(x+1)  y,  ? = \sqrt[3] {1 + \sqrt[3] 5} ","['calculus', 'limits', 'recursion', 'nested-radicals']"
48,Deriving the derivative formula for arcsecant correctly,Deriving the derivative formula for arcsecant correctly,,"I have been trying to derive the derivative of the arcsecant function, but I can't quite get the right answer (the correct answer is the absolute value of what I get). I first get $\frac{d}{dy}\sec(y)=\frac{\cos^2(y)}{\sin(y)}=\frac{\cos^2(\sec^{-1}(x))}{\sin(\sec^{-1}(x))}$. Then, by examining the appropriate right triangle with angle $\theta$, hypotenuse $x$, adjacent side length $1$ and opposite side length $\sqrt{x^2-1}$, we see that $\sin(\sec^{-1}(x))=\sin(\theta)=\frac{\sqrt{x^2-1}}{x}$ and $\cos(\sec^{-1}(x))=\cos(\theta)=\frac{1}{x}$. Substituting these identities into the formula for the derivative, we get $$\frac{1}{x\sqrt{x^2-1}}$$. However, the actual answer is the absolute value of that.  I can't figure out where I am assuming $x$ is positive. It has been a while since I have mucked about with this kind of thing, so my apologies if this is a silly question. Wikipedia wasn't helpful on the matter.","I have been trying to derive the derivative of the arcsecant function, but I can't quite get the right answer (the correct answer is the absolute value of what I get). I first get $\frac{d}{dy}\sec(y)=\frac{\cos^2(y)}{\sin(y)}=\frac{\cos^2(\sec^{-1}(x))}{\sin(\sec^{-1}(x))}$. Then, by examining the appropriate right triangle with angle $\theta$, hypotenuse $x$, adjacent side length $1$ and opposite side length $\sqrt{x^2-1}$, we see that $\sin(\sec^{-1}(x))=\sin(\theta)=\frac{\sqrt{x^2-1}}{x}$ and $\cos(\sec^{-1}(x))=\cos(\theta)=\frac{1}{x}$. Substituting these identities into the formula for the derivative, we get $$\frac{1}{x\sqrt{x^2-1}}$$. However, the actual answer is the absolute value of that.  I can't figure out where I am assuming $x$ is positive. It has been a while since I have mucked about with this kind of thing, so my apologies if this is a silly question. Wikipedia wasn't helpful on the matter.",,"['calculus', 'trigonometry', 'derivatives']"
49,l'Hopital's questionable premise?,l'Hopital's questionable premise?,,"Historians widely report that l'Hopital's 1696 book Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes contains a questionable premise expressed by an equation of type $x+dx=x$ (sometimes written as $y+dy=y$ as in Laugwitz 1997). I used to believe this until I looked in l'Hopital's book and did not find any such equation. What I did find is an axiom right at the beginning of the book to the effect that the $dx$ can be neglected. What l'Hopital wrote, more precisely, was: On demande qu'on puisse prendre indifféremment l'une pour l'autre deux quantités qui ne différent entr'elles que d'une quantité infiniment petite: ou (ce qui est la même chose) qu'une quantité qui n'est augmentée ou diminuée que d'une autre quantité infiniment moindre qu'elle, puisse être considérée comme demeurant la même . l'Hopital did not say that they are equal, but rather that ""qu'on puisse prendre indifféremment l'une pour l'autre"" meaning that ""one can take one for the other"". This viewpoint is close to the one adopted in the hyperreal formalisation of this idea in terms of the standard part function and is not known to entail contradictions like $x+dx=x$. Does the equation perhaps appear elsewhere in the book, or is this simply an inaccuracy? A 17th century scholar I consulted with agreed that the equation is probably not in the book; it would be nice to have a reference to that effect.","Historians widely report that l'Hopital's 1696 book Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes contains a questionable premise expressed by an equation of type $x+dx=x$ (sometimes written as $y+dy=y$ as in Laugwitz 1997). I used to believe this until I looked in l'Hopital's book and did not find any such equation. What I did find is an axiom right at the beginning of the book to the effect that the $dx$ can be neglected. What l'Hopital wrote, more precisely, was: On demande qu'on puisse prendre indifféremment l'une pour l'autre deux quantités qui ne différent entr'elles que d'une quantité infiniment petite: ou (ce qui est la même chose) qu'une quantité qui n'est augmentée ou diminuée que d'une autre quantité infiniment moindre qu'elle, puisse être considérée comme demeurant la même . l'Hopital did not say that they are equal, but rather that ""qu'on puisse prendre indifféremment l'une pour l'autre"" meaning that ""one can take one for the other"". This viewpoint is close to the one adopted in the hyperreal formalisation of this idea in terms of the standard part function and is not known to entail contradictions like $x+dx=x$. Does the equation perhaps appear elsewhere in the book, or is this simply an inaccuracy? A 17th century scholar I consulted with agreed that the equation is probably not in the book; it would be nice to have a reference to that effect.",,"['calculus', 'reference-request', 'soft-question', 'math-history', 'nonstandard-analysis']"
50,Feynman Parameters,Feynman Parameters,,"I'm trying to prove the following identity: $$ \left(\prod_{j=1}^n A_j\right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right) \frac{(n-1)!}{(\sum_{j=1}^nx_iA_i)^{n}}\,\,\,\,\,\,\,\,\forall n\in\mathbb{N}$$ My strategy is induction on $n$. The $n=1$ step is easy to show. Assuming the induction hypothesis, we have: $$ \left(\prod_{j=1}^{n+1} A_j\right)^{-1} = \left(\Pi_{j=1}^{n} A_j\right)^{-1} \cdot \left( A_{n+1} \right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\frac{1}{A_{n+1}(\sum_{j=1}^nx_iA_i)^{n}}$$ Then I use an identity which is easy to prove by differentiation of both sides of the equation repeatedly w.r.t. $B$: $$ \frac{1}{A\cdot B^n} = \int_0^1dx\int_0^1dy\delta\left(x+y-1\right)\frac{n\,y^{n-1}}{\left(xA+yB\right)^{n+1}}\,\,\,\forall n\in\mathbb{N}$$  to obtain: $$ \left(\prod_{j=1}^{n+1} A_j\right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\int_0^1dx\int_0^1dy\delta\left(x+y-1\right)\frac{ny^{n-1}}{\left(xA_{n+1}+y\sum_{i=1}^nx_iA_i\right)^{n+1}}$$ Perform the $x$ integration to get: $$ \left(\prod_{j=1}^{n+1} A_j\right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\int_0^1dy\frac{ny^{n-1}}{\left(\left(1-y\right)A_{n+1}+y\sum_{i=1}^nx_iA_i\right)^{n+1}}\\\ = \int_0^1dy\int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\frac{ny^{n-1}}{\left(\left(1-y\right)A_{n+1}+y\sum_{i=1}^nx_iA_i\right)^{n+1}}$$ Now make the following $n$ substitutions: $$ u_i := y x_i \forall i \in \left\{1,\dots,n \right\} $$ Which results in: $$ = \int_0^1dy\int_0^y\frac{du_1}{y} \dots \int_0^y\frac{du_n}{y} \,\delta\left(\sum_{i=1}^{n}\frac{u_i}{y} -1\right)\left((n-1)!\right)\frac{ny^{n-1}}{\left(\left(1-y\right)A_{n+1}+\sum_{i=1}^n u_iA_i\right)^{n+1}}$$ Use the delta function identity: $$ \delta\left(\sum_{i=1}^n \frac{u_i}{y} -1\right) = \delta\left(\sum_{i=1}^n u_i -y\right)\cdot y$$ To get: $$ = \int_0^1dy\int_0^y du_1 \dots \int_0^y du_n \,\delta\left(\sum_{i=1}^{n}u_i -y\right)\frac{n!}{\left(\left(1-y\right)A_{n+1}+\sum_{i=1}^n u_iA_i\right)^{n+1}}$$ Finally, make the substitution $u_{n+1} := 1-y$  $$ = \int_0^1du_{n+1}\int_0^{1-u_{n+1}} du_1 \dots \int_0^{1-u_{n+1}} du_n \,\delta\left(\sum_{i=1}^{n+1}u_i -1\right)\frac{n!}{\left(\sum_{i=1}^{n+1} u_iA_i\right)^{n+1}}$$ Now I am stuck, because I don't know how to deal with the integration limits. I could stretch them all from $0$ to $1$, but as far as I can tell, that would mean I'd have to divide the whole thing by $n!$, which would not give me the result I'm looking for. What am I doing wrong?","I'm trying to prove the following identity: $$ \left(\prod_{j=1}^n A_j\right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right) \frac{(n-1)!}{(\sum_{j=1}^nx_iA_i)^{n}}\,\,\,\,\,\,\,\,\forall n\in\mathbb{N}$$ My strategy is induction on $n$. The $n=1$ step is easy to show. Assuming the induction hypothesis, we have: $$ \left(\prod_{j=1}^{n+1} A_j\right)^{-1} = \left(\Pi_{j=1}^{n} A_j\right)^{-1} \cdot \left( A_{n+1} \right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\frac{1}{A_{n+1}(\sum_{j=1}^nx_iA_i)^{n}}$$ Then I use an identity which is easy to prove by differentiation of both sides of the equation repeatedly w.r.t. $B$: $$ \frac{1}{A\cdot B^n} = \int_0^1dx\int_0^1dy\delta\left(x+y-1\right)\frac{n\,y^{n-1}}{\left(xA+yB\right)^{n+1}}\,\,\,\forall n\in\mathbb{N}$$  to obtain: $$ \left(\prod_{j=1}^{n+1} A_j\right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\int_0^1dx\int_0^1dy\delta\left(x+y-1\right)\frac{ny^{n-1}}{\left(xA_{n+1}+y\sum_{i=1}^nx_iA_i\right)^{n+1}}$$ Perform the $x$ integration to get: $$ \left(\prod_{j=1}^{n+1} A_j\right)^{-1} = \int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\int_0^1dy\frac{ny^{n-1}}{\left(\left(1-y\right)A_{n+1}+y\sum_{i=1}^nx_iA_i\right)^{n+1}}\\\ = \int_0^1dy\int_0^1dx_1 \dots \int_0^1dx_n \,\delta\left(\sum_{i=1}^{n}x_i -1\right)\left((n-1)!\right)\frac{ny^{n-1}}{\left(\left(1-y\right)A_{n+1}+y\sum_{i=1}^nx_iA_i\right)^{n+1}}$$ Now make the following $n$ substitutions: $$ u_i := y x_i \forall i \in \left\{1,\dots,n \right\} $$ Which results in: $$ = \int_0^1dy\int_0^y\frac{du_1}{y} \dots \int_0^y\frac{du_n}{y} \,\delta\left(\sum_{i=1}^{n}\frac{u_i}{y} -1\right)\left((n-1)!\right)\frac{ny^{n-1}}{\left(\left(1-y\right)A_{n+1}+\sum_{i=1}^n u_iA_i\right)^{n+1}}$$ Use the delta function identity: $$ \delta\left(\sum_{i=1}^n \frac{u_i}{y} -1\right) = \delta\left(\sum_{i=1}^n u_i -y\right)\cdot y$$ To get: $$ = \int_0^1dy\int_0^y du_1 \dots \int_0^y du_n \,\delta\left(\sum_{i=1}^{n}u_i -y\right)\frac{n!}{\left(\left(1-y\right)A_{n+1}+\sum_{i=1}^n u_iA_i\right)^{n+1}}$$ Finally, make the substitution $u_{n+1} := 1-y$  $$ = \int_0^1du_{n+1}\int_0^{1-u_{n+1}} du_1 \dots \int_0^{1-u_{n+1}} du_n \,\delta\left(\sum_{i=1}^{n+1}u_i -1\right)\frac{n!}{\left(\sum_{i=1}^{n+1} u_iA_i\right)^{n+1}}$$ Now I am stuck, because I don't know how to deal with the integration limits. I could stretch them all from $0$ to $1$, but as far as I can tell, that would mean I'd have to divide the whole thing by $n!$, which would not give me the result I'm looking for. What am I doing wrong?",,"['calculus', 'integration', 'definite-integrals']"
51,Probablility of a dart landing closer to the center than the edge of a square dartboard?,Probablility of a dart landing closer to the center than the edge of a square dartboard?,,"The question is: There is a square dart board with a side length of $2$m, and a dart has equal probability to land anywhere on the board. What is the probability that the dart will land closer to the center than to the edges? So I know that if I center the square at the origin, I have to find $f(x)$ such that every point on $f(x)$ is equidistant between the origin and the line $x=1$ or $y=1$ (I'm only working with the first quadrant since by symmetry the probability should be the same). Another reasonable assumption that I made is that $f(x)$ is reflected over $y=x$, so I only have to work with the first $45$ degrees of the first quadrant. So to find $f(x)$ I did the following: $\sqrt{x^2+[f(x)]^2}=1-x$, so $f(x)=\sqrt{1-2x}$. Now I know that I just find the area under this curve and divide it by $1/8$ the area of the square to find the probability. I just wanted to know if my $f(x)$ was okay? Thanks!","The question is: There is a square dart board with a side length of $2$m, and a dart has equal probability to land anywhere on the board. What is the probability that the dart will land closer to the center than to the edges? So I know that if I center the square at the origin, I have to find $f(x)$ such that every point on $f(x)$ is equidistant between the origin and the line $x=1$ or $y=1$ (I'm only working with the first quadrant since by symmetry the probability should be the same). Another reasonable assumption that I made is that $f(x)$ is reflected over $y=x$, so I only have to work with the first $45$ degrees of the first quadrant. So to find $f(x)$ I did the following: $\sqrt{x^2+[f(x)]^2}=1-x$, so $f(x)=\sqrt{1-2x}$. Now I know that I just find the area under this curve and divide it by $1/8$ the area of the square to find the probability. I just wanted to know if my $f(x)$ was okay? Thanks!",,"['calculus', 'probability', 'integration']"
52,Prove inequality $\tan \left( \frac\pi2 \frac{(1+x)^2}{3+x^2}\right) \tan \left( \frac\pi2 \frac{(1-x)^2}{3+x^2}\right)\le\frac13 $,Prove inequality,\tan \left( \frac\pi2 \frac{(1+x)^2}{3+x^2}\right) \tan \left( \frac\pi2 \frac{(1-x)^2}{3+x^2}\right)\le\frac13 ,"How to determine the range of the function $$f(x)=\tan \left( \frac\pi2 \frac{(1+x)^2}{3+x^2}\right) \tan \left( \frac\pi2 \frac{(1-x)^2}{3+x^2}\right) $$ It it is straightforward to verify that $f(x)$ is even and $$f(0)= \frac13, \>\>\>\>\> \lim_{x\to\pm \infty} f(x) \to -\infty$$ which implies $f(x) \in (-\infty,\frac13]$ , i.e. $$\tan \left( \frac\pi2 \frac{(1+x)^2}{3+x^2}\right) \tan \left( \frac\pi2 \frac{(1-x)^2}{3+x^2}\right)\le \frac13 $$ and is visually confirmed below However, it is not obvious algebraically that $f(x)$ monotonically decreases away from $x=0$ . The standard derivative tests are not viable due to their rather complicated functional forms. So, the question is how to prove the inequality $f(x) \le \frac13$ with rigor. Note that it is equivalent to proving $$\cot \left(\frac{\pi(1+x)}{3+x^2}\right) \cot \left(\frac{\pi(1-x)}{3+x^2}\right)\le \frac13 $$","How to determine the range of the function It it is straightforward to verify that is even and which implies , i.e. and is visually confirmed below However, it is not obvious algebraically that monotonically decreases away from . The standard derivative tests are not viable due to their rather complicated functional forms. So, the question is how to prove the inequality with rigor. Note that it is equivalent to proving","f(x)=\tan \left( \frac\pi2 \frac{(1+x)^2}{3+x^2}\right)
\tan \left( \frac\pi2 \frac{(1-x)^2}{3+x^2}\right)
 f(x) f(0)= \frac13, \>\>\>\>\> \lim_{x\to\pm \infty} f(x) \to -\infty f(x) \in (-\infty,\frac13] \tan \left( \frac\pi2 \frac{(1+x)^2}{3+x^2}\right)
\tan \left( \frac\pi2 \frac{(1-x)^2}{3+x^2}\right)\le \frac13
 f(x) x=0 f(x) \le \frac13 \cot \left(\frac{\pi(1+x)}{3+x^2}\right)
\cot \left(\frac{\pi(1-x)}{3+x^2}\right)\le \frac13
","['calculus', 'trigonometry', 'inequality']"
53,Strengthening the notion of 'limit equivalence',Strengthening the notion of 'limit equivalence',,"The notation $f \sim g$ is used to signify that two functions are asymptotically equivalent, either at $± \infty$ or near some real number $c$. For example, near $0$ we have $\sin x \sim x$. The formal definition is $$f \sim g \ \text{at} \ c \ \text{iff} \ \lim_{x \to c} \frac{f(x)}{g(x)} = 1$$ where $c \in \mathbb{R} \cup \{-\infty, +\infty\}$. However, this notion is somewhat imprecise, in my opinion. For 'equivalence' of functions, an important property we ought to have (in my opinion) is for any sufficiently well behaved (e.g., $C^{\infty}$ or even just $C^{1}$) function $h$, $f \sim g$ implies $h \circ f \sim h \circ g$. In other words, our notion of 'equivalence' should be such that composition should preserve 'equivalence'. However, this isn't necessarily true with the standard definition of equivalence. In particular, if $h$ varies or grows rapidly near $c$ (again, $|c|$ may be infinity), then the composition with $h$ may 'exacerbate' the otherwise small (relative) difference between $f$ and $g$. An example of this phenomenon is how $\sin x \sim x$ as $x \to 0$ but $$\lim_{x \to 0}\frac{\exp\left(-\exp\left(\frac{1}{(\sin x)^2}\right)\right)}{\exp\left(-\exp\left(\frac{1}{x^2}\right)\right)} = 0$$ Another example, this time as $x \to \infty$, is how $x^2 \sim x^2 + x$ but $\exp(x^2 + x)$ dominates $\exp(x^2)$. My question is, is there a stronger definition of 'asymptotic equivalence' which does not have this apparent weakness? In other words, can we strengthen the definition of $f \sim g$ such that $f \sim g$ implies $h \circ f \sim h \circ g$ for any smooth $h:\mathbb{R} \to \mathbb{R}$. It could be as strong as needed, but not so strong that the notion is rendered useless. For example, we could 'redefine' $f \sim g$ so that $f \sim g$ iff $f \equiv g$ in some neighbourhood of $c$ [or, in the case that $c=+\infty$ (resp. $-\infty$), for sufficiently large positive (resp. negative) $x$ we have $f \equiv g$] but this would be trivial and uninteresting. A valid answer to this question is that this is impossible. In other words, if $f$ and $g$ have the property that $f \sim g$ and $h \circ f \sim h \circ g$ for all smooth $h$, then $f \equiv g$ near $c$. Note that a related question is here .","The notation $f \sim g$ is used to signify that two functions are asymptotically equivalent, either at $± \infty$ or near some real number $c$. For example, near $0$ we have $\sin x \sim x$. The formal definition is $$f \sim g \ \text{at} \ c \ \text{iff} \ \lim_{x \to c} \frac{f(x)}{g(x)} = 1$$ where $c \in \mathbb{R} \cup \{-\infty, +\infty\}$. However, this notion is somewhat imprecise, in my opinion. For 'equivalence' of functions, an important property we ought to have (in my opinion) is for any sufficiently well behaved (e.g., $C^{\infty}$ or even just $C^{1}$) function $h$, $f \sim g$ implies $h \circ f \sim h \circ g$. In other words, our notion of 'equivalence' should be such that composition should preserve 'equivalence'. However, this isn't necessarily true with the standard definition of equivalence. In particular, if $h$ varies or grows rapidly near $c$ (again, $|c|$ may be infinity), then the composition with $h$ may 'exacerbate' the otherwise small (relative) difference between $f$ and $g$. An example of this phenomenon is how $\sin x \sim x$ as $x \to 0$ but $$\lim_{x \to 0}\frac{\exp\left(-\exp\left(\frac{1}{(\sin x)^2}\right)\right)}{\exp\left(-\exp\left(\frac{1}{x^2}\right)\right)} = 0$$ Another example, this time as $x \to \infty$, is how $x^2 \sim x^2 + x$ but $\exp(x^2 + x)$ dominates $\exp(x^2)$. My question is, is there a stronger definition of 'asymptotic equivalence' which does not have this apparent weakness? In other words, can we strengthen the definition of $f \sim g$ such that $f \sim g$ implies $h \circ f \sim h \circ g$ for any smooth $h:\mathbb{R} \to \mathbb{R}$. It could be as strong as needed, but not so strong that the notion is rendered useless. For example, we could 'redefine' $f \sim g$ so that $f \sim g$ iff $f \equiv g$ in some neighbourhood of $c$ [or, in the case that $c=+\infty$ (resp. $-\infty$), for sufficiently large positive (resp. negative) $x$ we have $f \equiv g$] but this would be trivial and uninteresting. A valid answer to this question is that this is impossible. In other words, if $f$ and $g$ have the property that $f \sim g$ and $h \circ f \sim h \circ g$ for all smooth $h$, then $f \equiv g$ near $c$. Note that a related question is here .",,"['calculus', 'asymptotics']"
54,How to evaluate the integral $\int_0^{2\pi} \theta\exp(x\cos(\theta) + y\sin(\theta))) d\theta$,How to evaluate the integral,\int_0^{2\pi} \theta\exp(x\cos(\theta) + y\sin(\theta))) d\theta,"I found five other related integrals whose proofs I am studying now A , B , C , D , and E $$\int^{2\pi}_0e^{\cos \theta}\cos(a\theta -\sin \theta)\,d \theta = \frac{2\pi}{a!}$$ $$\int_0^{2\pi} \exp(\cos(\theta)) \cos(\theta + \sin(\theta)) = 0$$ $$ \int_0^{2\pi} \exp(\alpha \cos(\theta))\cos(\sin(\theta)) = 2\pi I_0(\sqrt{1 - \alpha^2})$$ $$\int_0^{2\pi} \exp(x\cos(\theta) + y\sin(\theta))) = 2\pi I_0(\sqrt{x^2 + y^2})$$  $$ \int_0^\dfrac{\pi}{2}\beta^\alpha\exp\left(-\beta\cos(\theta)\right)d\theta = \dfrac{1}{2}\beta^\alpha\pi\left(J_0(\beta)-L_0(\beta)\right)$$ I was also able to find a very general statement in Gradshteyn as entry number 3.338.  $$\int_{-\pi}^{\pi} \frac{\exp{\frac{a + b\sin x + c \cos x}{1 + p \sin x + q \cos x}}}{1 + p \sin x + q \cos x} dx = \frac{2\pi e^{-\alpha}I_0(\beta)}{\sqrt{1 - p^2 - q^2}}$$ $$\textrm{where } \alpha = \frac{bp + cq -a}{1 - p^2 - q^2},\; \beta = \sqrt{\alpha^2 - \frac{a^2 - b^2 - c^2}{1 - p^2 - q^2}}$$ But the simplest approach of using integration by parts to reduce my problem to one of these does not work. Background Here's some background into why I am interested in this integral, let $v = [x, y] \in \mathbb{R}^2$ and $r = [\cos(\theta), \sin(\theta)] \in \mathbb{R}^2$, Consider the value of $$\underset{\theta \tilde{} \textrm{Hill}}{E}[\exp(v^Tr)]$$ This is the expected value of exponential of the projection of a random vector chosen using the Hill distribution, where the ""Hill"" is an unnormalized distribution that linearly increases from $0$ at $-\pi$ to $1$ at $0$ and then decreases linearly from $0 \textrm{ to } \pi$. Discarding normalizing factor of Hill, This expectation will become: $$ \int_{-\pi}^{0} (\theta + \pi)\exp(x\cos\theta + y\sin\theta) d\theta + \int_{0}^{\pi} (\pi - \theta) \exp(x\cos\theta + y\sin\theta) d\theta $$ Now, there are simplifying unnormalized distributions I could assume in my model, instead of Hill, such as Uniform from 0 to $2\pi$, or $\exp(\cos(\theta))$ both of these distribution allow analytical calculation of the above expectation just based on the identities written below, but I want to know which distributions I can compute this expectation for (Can I do this for Hill?) I will guess that I can only do it for distributions that have some finite decomposition in terms of spherical harmonics. Unfortunately, my knowledge is lacking in complex analysis and spherical harmonics so I can't quickly assess my options.","I found five other related integrals whose proofs I am studying now A , B , C , D , and E $$\int^{2\pi}_0e^{\cos \theta}\cos(a\theta -\sin \theta)\,d \theta = \frac{2\pi}{a!}$$ $$\int_0^{2\pi} \exp(\cos(\theta)) \cos(\theta + \sin(\theta)) = 0$$ $$ \int_0^{2\pi} \exp(\alpha \cos(\theta))\cos(\sin(\theta)) = 2\pi I_0(\sqrt{1 - \alpha^2})$$ $$\int_0^{2\pi} \exp(x\cos(\theta) + y\sin(\theta))) = 2\pi I_0(\sqrt{x^2 + y^2})$$  $$ \int_0^\dfrac{\pi}{2}\beta^\alpha\exp\left(-\beta\cos(\theta)\right)d\theta = \dfrac{1}{2}\beta^\alpha\pi\left(J_0(\beta)-L_0(\beta)\right)$$ I was also able to find a very general statement in Gradshteyn as entry number 3.338.  $$\int_{-\pi}^{\pi} \frac{\exp{\frac{a + b\sin x + c \cos x}{1 + p \sin x + q \cos x}}}{1 + p \sin x + q \cos x} dx = \frac{2\pi e^{-\alpha}I_0(\beta)}{\sqrt{1 - p^2 - q^2}}$$ $$\textrm{where } \alpha = \frac{bp + cq -a}{1 - p^2 - q^2},\; \beta = \sqrt{\alpha^2 - \frac{a^2 - b^2 - c^2}{1 - p^2 - q^2}}$$ But the simplest approach of using integration by parts to reduce my problem to one of these does not work. Background Here's some background into why I am interested in this integral, let $v = [x, y] \in \mathbb{R}^2$ and $r = [\cos(\theta), \sin(\theta)] \in \mathbb{R}^2$, Consider the value of $$\underset{\theta \tilde{} \textrm{Hill}}{E}[\exp(v^Tr)]$$ This is the expected value of exponential of the projection of a random vector chosen using the Hill distribution, where the ""Hill"" is an unnormalized distribution that linearly increases from $0$ at $-\pi$ to $1$ at $0$ and then decreases linearly from $0 \textrm{ to } \pi$. Discarding normalizing factor of Hill, This expectation will become: $$ \int_{-\pi}^{0} (\theta + \pi)\exp(x\cos\theta + y\sin\theta) d\theta + \int_{0}^{\pi} (\pi - \theta) \exp(x\cos\theta + y\sin\theta) d\theta $$ Now, there are simplifying unnormalized distributions I could assume in my model, instead of Hill, such as Uniform from 0 to $2\pi$, or $\exp(\cos(\theta))$ both of these distribution allow analytical calculation of the above expectation just based on the identities written below, but I want to know which distributions I can compute this expectation for (Can I do this for Hill?) I will guess that I can only do it for distributions that have some finite decomposition in terms of spherical harmonics. Unfortunately, my knowledge is lacking in complex analysis and spherical harmonics so I can't quickly assess my options.",,"['calculus', 'integration', 'definite-integrals', 'special-functions']"
55,Derivative of cot(x),Derivative of cot(x),,"If we rewrite $\displaystyle \frac {d} {dx} \cot(x)$ as $\displaystyle \frac {d} {dx} \frac {1} {\tan(x)}$ and then apply the quotient rule, we get to $\displaystyle \frac {\tan(x)\frac{d}{dx}1-1\frac{d}{dx}\tan(x)} {\tan^2(x)}$ and then $\displaystyle \frac {-\sec^2(x)} {\tan^2(x)} = -\frac {1} {\cos^2(x)} \cdot \frac{\cos^2(x)} {\sin^2(x)} = -\csc^2(x)$ My question is that will this proof be valid for $\displaystyle \frac {\pi} {2}$? The derivative of $\tan(x)$ is $\sec^2(x)$ only for angles for which $\tan(x)$ is defined. $\tan(x)$ is undefined for $\frac {\pi} {2}$, so in the above quotient rule, when it is claimed that $\frac {d} {dx} \tan(x) = \ sec^2(x)$, that comes with the caveat that $x \neq \frac {\pi} {2}$ (as well as $\frac {3\pi} {2}$ and their co-terminals). Now $\frac {\pi} {2}$ is in the domain of $\cot(x)$, but I don't think that the proof in the opening paragraph holds for $\frac {\pi} {2}$ If I go by the first principle i.e. $$\displaystyle \lim_{h \to 0} \frac  {cot(x+h)-cot(h)} {h}$$, then I get a proof which works for all angles in the domain of $\cot(x)$ including $\frac {\pi} {2}$ Similarly, if I apply the quotient rule on $\displaystyle \frac {d} {dx} \cot(x) = \frac {d} {dx} \frac {\cos(x)} {\sin(x)}$, I don't run into any problems with $\frac {\pi} {2}$ So, I am just curious if the proof outlined in the first para is applicable for $\frac {\pi} {2}$. I am not so convinced, but wherever I have seen that approach on the internet, I haven't seen anyone make a note or caveat that this approach may have some issues. Thanks.","If we rewrite $\displaystyle \frac {d} {dx} \cot(x)$ as $\displaystyle \frac {d} {dx} \frac {1} {\tan(x)}$ and then apply the quotient rule, we get to $\displaystyle \frac {\tan(x)\frac{d}{dx}1-1\frac{d}{dx}\tan(x)} {\tan^2(x)}$ and then $\displaystyle \frac {-\sec^2(x)} {\tan^2(x)} = -\frac {1} {\cos^2(x)} \cdot \frac{\cos^2(x)} {\sin^2(x)} = -\csc^2(x)$ My question is that will this proof be valid for $\displaystyle \frac {\pi} {2}$? The derivative of $\tan(x)$ is $\sec^2(x)$ only for angles for which $\tan(x)$ is defined. $\tan(x)$ is undefined for $\frac {\pi} {2}$, so in the above quotient rule, when it is claimed that $\frac {d} {dx} \tan(x) = \ sec^2(x)$, that comes with the caveat that $x \neq \frac {\pi} {2}$ (as well as $\frac {3\pi} {2}$ and their co-terminals). Now $\frac {\pi} {2}$ is in the domain of $\cot(x)$, but I don't think that the proof in the opening paragraph holds for $\frac {\pi} {2}$ If I go by the first principle i.e. $$\displaystyle \lim_{h \to 0} \frac  {cot(x+h)-cot(h)} {h}$$, then I get a proof which works for all angles in the domain of $\cot(x)$ including $\frac {\pi} {2}$ Similarly, if I apply the quotient rule on $\displaystyle \frac {d} {dx} \cot(x) = \frac {d} {dx} \frac {\cos(x)} {\sin(x)}$, I don't run into any problems with $\frac {\pi} {2}$ So, I am just curious if the proof outlined in the first para is applicable for $\frac {\pi} {2}$. I am not so convinced, but wherever I have seen that approach on the internet, I haven't seen anyone make a note or caveat that this approach may have some issues. Thanks.",,"['calculus', 'derivatives']"
56,Compute the value of the integral $\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x $,Compute the value of the integral,\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x ,"I want to compute the value of the integral $$\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x $$ where $\lfloor \  \rfloor$ denotes the floor function. My try: I split into a series: since $\lfloor x^2 \rfloor = n$ is equivalent to $\sqrt{n} \le x < \sqrt{n+1}$ , I get $$\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x = \sum_{n=1}^{\infty} n \int_{\sqrt{n}}^{\sqrt{n+1}} e^{-x} \ \mathrm d x = \sum_{n=1}^{\infty} n (e^{-\sqrt{n}}-e^{-\sqrt{n+1}})$$ Now I write down the first few terms of the series and write it in another way: $$\sum_{n=1}^{\infty} n (e^{-\sqrt{n}}-e^{-\sqrt{n+1}}) = 1 e^{-\sqrt{1}} - 1 e^{-\sqrt{2}} +2 e^{-\sqrt{2}} - 2 e^{-\sqrt{3}} + 3 e^{-\sqrt{3}} - 3 e^{-\sqrt{4}} + \cdots$$ and note that terms cancel. Thus the series is equal to te following limit: $$\lim_{n \to \infty} ( e^{-\sqrt{1}} +  e^{-\sqrt{2}} +  e^{-\sqrt{3}} + \cdots +e^{-\sqrt{n}}) - n e^{-\sqrt{n+1}}$$ And here I get stuck. Maybe I should use some integration by parts, but as $\lfloor x^2 \rfloor$ is discountinuous I don't know if I'm allowed to use it.","I want to compute the value of the integral where denotes the floor function. My try: I split into a series: since is equivalent to , I get Now I write down the first few terms of the series and write it in another way: and note that terms cancel. Thus the series is equal to te following limit: And here I get stuck. Maybe I should use some integration by parts, but as is discountinuous I don't know if I'm allowed to use it.",\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x  \lfloor \  \rfloor \lfloor x^2 \rfloor = n \sqrt{n} \le x < \sqrt{n+1} \int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x = \sum_{n=1}^{\infty} n \int_{\sqrt{n}}^{\sqrt{n+1}} e^{-x} \ \mathrm d x = \sum_{n=1}^{\infty} n (e^{-\sqrt{n}}-e^{-\sqrt{n+1}}) \sum_{n=1}^{\infty} n (e^{-\sqrt{n}}-e^{-\sqrt{n+1}}) = 1 e^{-\sqrt{1}} - 1 e^{-\sqrt{2}} +2 e^{-\sqrt{2}} - 2 e^{-\sqrt{3}} + 3 e^{-\sqrt{3}} - 3 e^{-\sqrt{4}} + \cdots \lim_{n \to \infty} ( e^{-\sqrt{1}} +  e^{-\sqrt{2}} +  e^{-\sqrt{3}} + \cdots +e^{-\sqrt{n}}) - n e^{-\sqrt{n+1}} \lfloor x^2 \rfloor,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
57,Integral with messy integrand,Integral with messy integrand,,"Let $N\in\mathbb{N}$ , $\theta_{i}>0$ , and $a_{ij}\in\mathbb{R}$ , $\forall i = 1,\ldots, N$ . Is there a somewhat explicit expression for \begin{align} \int_{0}^{\infty} t \left[\prod_{i=1}^{N} \frac{1}{\sqrt{1+2\theta_{i}t}}\right]      \left[\sum_{i,j=1}^{N}\frac{a_{ij}}{(1+2\theta_{i}t)(1+2\theta_{j}t)}\right]dt? \end{align}","Let , , and , . Is there a somewhat explicit expression for","N\in\mathbb{N} \theta_{i}>0 a_{ij}\in\mathbb{R} \forall i = 1,\ldots, N \begin{align}
\int_{0}^{\infty} t \left[\prod_{i=1}^{N} \frac{1}{\sqrt{1+2\theta_{i}t}}\right] 
    \left[\sum_{i,j=1}^{N}\frac{a_{ij}}{(1+2\theta_{i}t)(1+2\theta_{j}t)}\right]dt?
\end{align}","['calculus', 'integration', 'analysis', 'definite-integrals']"
58,Limit of recursive sequence with floor,Limit of recursive sequence with floor,,"Sequences $x_n$ and $y_n$ are defined as   $$x_n=\left\lfloor x_{n-1}\frac{y_{n+1}}{y_{n-1}}\right\rfloor,\\ y_n=y_{n-1}+1,\\ x_0=2015,\ y_0=307.$$   Compute $$\lim_{n\to\infty}\frac{x_n}{y_n^2}$$ My attempt: $y_n=307+n$ so $$x_n> x_{n-1} \frac{308+n}{306+n}>x_0\frac{(308+n)(309+n)}{306\cdot 307}\approx O(n^2),$$ so the limit is approximately $\dfrac{2015}{306\cdot 307}$. However, the answer is given as $\dfrac{2}{101}$. How to obtain this value?","Sequences $x_n$ and $y_n$ are defined as   $$x_n=\left\lfloor x_{n-1}\frac{y_{n+1}}{y_{n-1}}\right\rfloor,\\ y_n=y_{n-1}+1,\\ x_0=2015,\ y_0=307.$$   Compute $$\lim_{n\to\infty}\frac{x_n}{y_n^2}$$ My attempt: $y_n=307+n$ so $$x_n> x_{n-1} \frac{308+n}{306+n}>x_0\frac{(308+n)(309+n)}{306\cdot 307}\approx O(n^2),$$ so the limit is approximately $\dfrac{2015}{306\cdot 307}$. However, the answer is given as $\dfrac{2}{101}$. How to obtain this value?",,"['calculus', 'sequences-and-series', 'limits', 'recurrence-relations', 'contest-math']"
59,"Formula for a sequence defined on $K_1(x,y) := y+0$ if $x \geq y$ and $y-1$ otherwise",Formula for a sequence defined on  if  and  otherwise,"K_1(x,y) := y+0 x \geq y y-1","Define $K_1:[0,1]^2\rightarrow\mathbb{R}$ as $$K_1(x,y) := x - \frac{1}{2} - \begin{cases} \ +(x - y - \frac{1}{2}) & \text{if $x \geq y$},\\ \ -(y - x - \frac{1}{2}) & \text{otherwise} \end{cases}$$ then with $$K_n(x,y) := \int_0^1K_1(x,u)K_{n-1}(u,y)\textrm{d}u$$ cf. equation $(35)$ part $3$ show that for $n\geq 1$ $$ n!K_n(x,y) = B_n(x) - \begin{cases} B_n(x-y) ~~\textrm{ if } x\geq y\\ ~\\ (-1)^nB_n(y-x)~~\textrm{ otherwise }\end{cases} $$ Here $B_n$ are the Bernoulli Polynomials . Verify that \begin{align} +\sin(2\pi k x)=(2\pi k)^1\int_0^1K_1(x,u)\cos(2\pi k u)\textrm{d}u\\ -\cos(2\pi k x)=(2\pi k)^2\int_0^1K_2(x,u)\cos(2\pi k u)\textrm{d}u\\ -\sin(2\pi k x)=(2\pi k)^3\int_0^1K_3(x,u)\cos(2\pi k u)\textrm{d}u\\ +\cos(2\pi k x)=(2\pi k)^4\int_0^1K_4(x,u)\cos(2\pi k u)\textrm{d}u \end{align} for all $x\in [0,1]$ and $k\in \mathbb{Z}$, $k\neq 0$, as well as $$ B_{n+m}(x)=\frac{(n+m)!}{m!}\int_0^1K_n(x,u)B_m(u)\textrm{d}u\\ $$","Define $K_1:[0,1]^2\rightarrow\mathbb{R}$ as $$K_1(x,y) := x - \frac{1}{2} - \begin{cases} \ +(x - y - \frac{1}{2}) & \text{if $x \geq y$},\\ \ -(y - x - \frac{1}{2}) & \text{otherwise} \end{cases}$$ then with $$K_n(x,y) := \int_0^1K_1(x,u)K_{n-1}(u,y)\textrm{d}u$$ cf. equation $(35)$ part $3$ show that for $n\geq 1$ $$ n!K_n(x,y) = B_n(x) - \begin{cases} B_n(x-y) ~~\textrm{ if } x\geq y\\ ~\\ (-1)^nB_n(y-x)~~\textrm{ otherwise }\end{cases} $$ Here $B_n$ are the Bernoulli Polynomials . Verify that \begin{align} +\sin(2\pi k x)=(2\pi k)^1\int_0^1K_1(x,u)\cos(2\pi k u)\textrm{d}u\\ -\cos(2\pi k x)=(2\pi k)^2\int_0^1K_2(x,u)\cos(2\pi k u)\textrm{d}u\\ -\sin(2\pi k x)=(2\pi k)^3\int_0^1K_3(x,u)\cos(2\pi k u)\textrm{d}u\\ +\cos(2\pi k x)=(2\pi k)^4\int_0^1K_4(x,u)\cos(2\pi k u)\textrm{d}u \end{align} for all $x\in [0,1]$ and $k\in \mathbb{Z}$, $k\neq 0$, as well as $$ B_{n+m}(x)=\frac{(n+m)!}{m!}\int_0^1K_n(x,u)B_m(u)\textrm{d}u\\ $$",,"['calculus', 'functional-analysis']"
60,Inequality constraints in calculus of variations,Inequality constraints in calculus of variations,,"$\def\d{\mathrm{d}}$It turns out that Yuri's answer to my earlier question , whilst correct (and I thank him for his effort), was not quite what I desired. I had not posed the question properly, so I have chosen to re-ask as I am still struggling to implement the strict inequality constraints. Let me begin by reposing the question. I wish to extremise  $$Q = \int_0^h u \, \, \d y$$ subject to the constraints$$ B = B_0,\ M = M_0,$$ where $$B = \int_0^h ug \, \d y, \ M = \int_0^h \left( u^2 + \int_0^y g(y^*) \, \d y^*\right) \, \d y.$$ An important new difference is that we have that $h$, the upper limit, can vary, and crucially we need $$u(h) = 0, \ g(h) = 0, \\ \int_0^h u \, \d y> 0, \ \int_0^h g \, \d y = G(h) > 0.$$ Here is my attempt (mostly following @Yuri). We set $$ G(y) = \int_0^y g(y^*) \, \d y^*$$ such that $$B = \int_0^h uG' \, \d y, \ M = \int_0^h u^2 + G \, \d y$$ and hence the Lagrangian becomes $$L(u,G,G',\lambda,\mu) = Q + \lambda(B-B_0) + \mu(M-M_0).$$ The variations of this (using Euler-Lagrange $\frac{\partial L}{\partial F} -  \frac{\d}{\d y}\frac{\partial L}{\partial F'} = 0$) become $$1 + \lambda G' + 2\mu u = 0, \ \mu - \lambda u' = 0, \\ \int_0^h uG' = B_0, \ \int_0^h u^2 + G = M_0.$$ The second equation here tells us $$u' = \frac{\lambda}{\mu} \Longrightarrow u = \frac{\lambda}{\mu}y + u_0.$$ Now since $h$ can vary we have two further transversality conditions $$\left.\frac{\partial L}{\partial G'}\right|_{y=h} = 0, \ \left.L - G' \frac{\partial L}{\partial G'}\right|_{y=h} = 0.$$ The first transversality condition tells us that $$\lambda u(h) = 0 \Longrightarrow u(h) = -\frac{\lambda}{\mu} \Longrightarrow u = -\frac{\mu}{\lambda}\left(h-y\right).$$ This is great as it satisifes $u(h) = 0$. The second transversality condition tells us (using the first variation equation to give us $G$) that $$\mu G(h) = 0 \Longrightarrow h = \frac{\lambda}{\mu^2}.$$ (However I think this condition violates the requirement $G(h) > 0$) We can now calculate everything we need, $$B_0 = -\frac{1}{6\mu^3}, \ M_0 = \frac{\lambda}{2\mu^4} \\ \lambda = \frac{M_0}{3\left(6^{1/3}B_0^{4/3}\right)}, \ \mu = -\frac{1}{6^{1/3}B_0^{1/3}}, \ h = \frac{2^{1/3}M_0}{3^{2/3}B_0^{2/3}} \\ Q = \frac{M_0}{6^{1/3}B_0^{1/3}}.$$ If we now sub in some values $B_0 = 6, M_0 = 36.5$ I am especially happy with the answer for $Q \approx 11.05, h \approx 6.696$ as it verifies a separate result I have reached. The problem comes with the profile for $g$. If look at the profile for $u$ we have everything behaving as expected. $u(h) = 0$ and $\int_0^h u \, \, dy >0$. However, the issue comes when looking at the profile for $g$. We can immediately see that $g(h) \neq 0$ (strangely $g(h/2) = 0$), and also we have $\int_0^h g \, \d y = 0$ which is not strictly bigger than 0. So my question is, how can I build the machinery for the constraints on $g$ into the Lagrangian. I have read a little online about possibly using slack variables, but I am not sure how to implement them. Any pointers would be greatly appreciated. Interestingly, if I were to transform the profile of $g$ to $\hat{g}$ such that $g(h/2) = \hat{g}(h) = 0$ and $g(0) = g^*, \hat{g}(0) = g^* / 2$ the solution for $\hat{g}$ would match the separate result I have reached identically. EDIT So I have a found a paper in which the author uses slack variables to impose a condition $\theta_z \geq 0$, my thinking is that this condition is equivalent to $G' >0$. Though in his derivation, his multiplier becomes a function of $z$. Something that I have never seen before. EDIT 2 I am now not convinced the second transversality condition is correct. This condition $G(h) = \int_0^h g = 0$ is what is forcing the profile to be negative above $h/2$ (to keep the area 0). This actually contradicts the essential requirement that $\int_0^h g > 0$. EDIT 3 I thought it would be helpful to add some further motivation. This method of calculus of variations as described above finds two solution profiles for $u$ and $g$ that are given by $$ u_1 = \frac{3 B_0 (h - y)}{M_0} \\ g_1 = \frac{3 \left(6 B_0^2 (h-y)-6^{1/3} B_0^{4/3} M_0\right)}{M_0^2} $$ which we can see satisfy the equations and maximise $Q$. I believe that the true solution I am seeking is given by profiles $$ u_2 = \frac{3 B_0 (h - y)}{M_0}, \ g_2 = \frac{9 B_0^2 (h-y)}{2 M_0^2}, \ h = \frac{2^{1/3}M_0}{3^{2/3}B_0^{2/3}}.$$ which can be shown to satisfy all the equations for $M,B$ and also give the same maximal $Q$. Furthermore, the profile for $g_2$ in particular satisfies the requirements that $g_2(h) = 0$ and $\int_0^h g_2 > 0$. It would appear to me that the $g_1, g_2$ profiles are equivalent solutions and I would expect that the calculus of variations to find both? I find it confusing that it doesn't. I suppose one could frame the question in the way. What constraints needed to be added to the Lagrangian such that the calculus of variations gives the me the desired $g_2$ profile, rather than the unphysical $g_1$ profile?","$\def\d{\mathrm{d}}$It turns out that Yuri's answer to my earlier question , whilst correct (and I thank him for his effort), was not quite what I desired. I had not posed the question properly, so I have chosen to re-ask as I am still struggling to implement the strict inequality constraints. Let me begin by reposing the question. I wish to extremise  $$Q = \int_0^h u \, \, \d y$$ subject to the constraints$$ B = B_0,\ M = M_0,$$ where $$B = \int_0^h ug \, \d y, \ M = \int_0^h \left( u^2 + \int_0^y g(y^*) \, \d y^*\right) \, \d y.$$ An important new difference is that we have that $h$, the upper limit, can vary, and crucially we need $$u(h) = 0, \ g(h) = 0, \\ \int_0^h u \, \d y> 0, \ \int_0^h g \, \d y = G(h) > 0.$$ Here is my attempt (mostly following @Yuri). We set $$ G(y) = \int_0^y g(y^*) \, \d y^*$$ such that $$B = \int_0^h uG' \, \d y, \ M = \int_0^h u^2 + G \, \d y$$ and hence the Lagrangian becomes $$L(u,G,G',\lambda,\mu) = Q + \lambda(B-B_0) + \mu(M-M_0).$$ The variations of this (using Euler-Lagrange $\frac{\partial L}{\partial F} -  \frac{\d}{\d y}\frac{\partial L}{\partial F'} = 0$) become $$1 + \lambda G' + 2\mu u = 0, \ \mu - \lambda u' = 0, \\ \int_0^h uG' = B_0, \ \int_0^h u^2 + G = M_0.$$ The second equation here tells us $$u' = \frac{\lambda}{\mu} \Longrightarrow u = \frac{\lambda}{\mu}y + u_0.$$ Now since $h$ can vary we have two further transversality conditions $$\left.\frac{\partial L}{\partial G'}\right|_{y=h} = 0, \ \left.L - G' \frac{\partial L}{\partial G'}\right|_{y=h} = 0.$$ The first transversality condition tells us that $$\lambda u(h) = 0 \Longrightarrow u(h) = -\frac{\lambda}{\mu} \Longrightarrow u = -\frac{\mu}{\lambda}\left(h-y\right).$$ This is great as it satisifes $u(h) = 0$. The second transversality condition tells us (using the first variation equation to give us $G$) that $$\mu G(h) = 0 \Longrightarrow h = \frac{\lambda}{\mu^2}.$$ (However I think this condition violates the requirement $G(h) > 0$) We can now calculate everything we need, $$B_0 = -\frac{1}{6\mu^3}, \ M_0 = \frac{\lambda}{2\mu^4} \\ \lambda = \frac{M_0}{3\left(6^{1/3}B_0^{4/3}\right)}, \ \mu = -\frac{1}{6^{1/3}B_0^{1/3}}, \ h = \frac{2^{1/3}M_0}{3^{2/3}B_0^{2/3}} \\ Q = \frac{M_0}{6^{1/3}B_0^{1/3}}.$$ If we now sub in some values $B_0 = 6, M_0 = 36.5$ I am especially happy with the answer for $Q \approx 11.05, h \approx 6.696$ as it verifies a separate result I have reached. The problem comes with the profile for $g$. If look at the profile for $u$ we have everything behaving as expected. $u(h) = 0$ and $\int_0^h u \, \, dy >0$. However, the issue comes when looking at the profile for $g$. We can immediately see that $g(h) \neq 0$ (strangely $g(h/2) = 0$), and also we have $\int_0^h g \, \d y = 0$ which is not strictly bigger than 0. So my question is, how can I build the machinery for the constraints on $g$ into the Lagrangian. I have read a little online about possibly using slack variables, but I am not sure how to implement them. Any pointers would be greatly appreciated. Interestingly, if I were to transform the profile of $g$ to $\hat{g}$ such that $g(h/2) = \hat{g}(h) = 0$ and $g(0) = g^*, \hat{g}(0) = g^* / 2$ the solution for $\hat{g}$ would match the separate result I have reached identically. EDIT So I have a found a paper in which the author uses slack variables to impose a condition $\theta_z \geq 0$, my thinking is that this condition is equivalent to $G' >0$. Though in his derivation, his multiplier becomes a function of $z$. Something that I have never seen before. EDIT 2 I am now not convinced the second transversality condition is correct. This condition $G(h) = \int_0^h g = 0$ is what is forcing the profile to be negative above $h/2$ (to keep the area 0). This actually contradicts the essential requirement that $\int_0^h g > 0$. EDIT 3 I thought it would be helpful to add some further motivation. This method of calculus of variations as described above finds two solution profiles for $u$ and $g$ that are given by $$ u_1 = \frac{3 B_0 (h - y)}{M_0} \\ g_1 = \frac{3 \left(6 B_0^2 (h-y)-6^{1/3} B_0^{4/3} M_0\right)}{M_0^2} $$ which we can see satisfy the equations and maximise $Q$. I believe that the true solution I am seeking is given by profiles $$ u_2 = \frac{3 B_0 (h - y)}{M_0}, \ g_2 = \frac{9 B_0^2 (h-y)}{2 M_0^2}, \ h = \frac{2^{1/3}M_0}{3^{2/3}B_0^{2/3}}.$$ which can be shown to satisfy all the equations for $M,B$ and also give the same maximal $Q$. Furthermore, the profile for $g_2$ in particular satisfies the requirements that $g_2(h) = 0$ and $\int_0^h g_2 > 0$. It would appear to me that the $g_1, g_2$ profiles are equivalent solutions and I would expect that the calculus of variations to find both? I find it confusing that it doesn't. I suppose one could frame the question in the way. What constraints needed to be added to the Lagrangian such that the calculus of variations gives the me the desired $g_2$ profile, rather than the unphysical $g_1$ profile?",,"['calculus', 'calculus-of-variations', 'euler-lagrange-equation']"
61,Deriving the surface area of a sphere from the volume,Deriving the surface area of a sphere from the volume,,"I am a high school student, so I know how to derive the volume $V=\dfrac{4}{3}\pi r^3$ using calculus, but I am unable to derive its surface area. However, I notice that we can approximate the surface area of a sphere by imaging 'the shell' of the sphere as the difference between two similarly sized spheres with width infinitely small. What I mean is that if we have two spheres, one of radius $r$ and another of radius $r+h$, where $h$ is very small, the surface area can be roughly measured by the difference in volume of these two spheres divided by its width $h$. Therefore if $f(r)$ is the volume of a sphere with radius $r$, then the surface area should be $$\lim_{h\to0} \dfrac{f(r+h)-f(r)}{h}$$but this is exactly the derivative $f'(r)$ of the volume. Hence we can say that the surface area of a sphere is $4\pi r^2$. Is this proof even correct? Is it rigorous? How do I improve it and make it more convincing? I noticed how this should similarly work for other objects, for example, the derivative of the area of a circle is its perimeter. Does this phenomenon likewise extend to other examples? Thanks for all your help!","I am a high school student, so I know how to derive the volume $V=\dfrac{4}{3}\pi r^3$ using calculus, but I am unable to derive its surface area. However, I notice that we can approximate the surface area of a sphere by imaging 'the shell' of the sphere as the difference between two similarly sized spheres with width infinitely small. What I mean is that if we have two spheres, one of radius $r$ and another of radius $r+h$, where $h$ is very small, the surface area can be roughly measured by the difference in volume of these two spheres divided by its width $h$. Therefore if $f(r)$ is the volume of a sphere with radius $r$, then the surface area should be $$\lim_{h\to0} \dfrac{f(r+h)-f(r)}{h}$$but this is exactly the derivative $f'(r)$ of the volume. Hence we can say that the surface area of a sphere is $4\pi r^2$. Is this proof even correct? Is it rigorous? How do I improve it and make it more convincing? I noticed how this should similarly work for other objects, for example, the derivative of the area of a circle is its perimeter. Does this phenomenon likewise extend to other examples? Thanks for all your help!",,"['calculus', 'recreational-mathematics', 'puzzle', 'surfaces', 'volume']"
62,Proving the limit of a nested sequence,Proving the limit of a nested sequence,,"I have trouble proving the next sequence limit: $\displaystyle\lim_{n\rightarrow\infty}(x_{n}-\sqrt{n})=\frac{1}{2}$ where $x_{n}=\sqrt{n+\sqrt{n-1 ...\sqrt{2+\sqrt{1}}}}.$ I've had a lot of problems; my try is multiplying by the conjugate but the resulting expression continues with $x_{n-1}.$ Also I tried to bound the expression and apply limit in each side which it was failed. By other hand, I thought that I can operate with the expression $x_{n}=\sqrt{n+x_{n-1}}$ and try to get a quadratic equation; solve it and work with a new expression but its usless. I thank any help to prove this limit.","I have trouble proving the next sequence limit: $\displaystyle\lim_{n\rightarrow\infty}(x_{n}-\sqrt{n})=\frac{1}{2}$ where $x_{n}=\sqrt{n+\sqrt{n-1 ...\sqrt{2+\sqrt{1}}}}.$ I've had a lot of problems; my try is multiplying by the conjugate but the resulting expression continues with $x_{n-1}.$ Also I tried to bound the expression and apply limit in each side which it was failed. By other hand, I thought that I can operate with the expression $x_{n}=\sqrt{n+x_{n-1}}$ and try to get a quadratic equation; solve it and work with a new expression but its usless. I thank any help to prove this limit.",,"['calculus', 'sequences-and-series', 'limits', 'nested-radicals']"
63,Probability and Laplace/Fourier transforms to solve limits/integrals from calculus.,Probability and Laplace/Fourier transforms to solve limits/integrals from calculus.,,"I've seen in some answers in Brilliant.org to some very complicated limits and integrals that uses probabilistic arguments (Let $X$ be a random variable from $[0,1]$... some examples are in those answers , see also this for an example that has to do with evaluation of limit of a series ) or some uses Laplace transforms or even Fourier series (Example see some answers by user Tunk-Fey : 1 , 2 ). I would like to ask about a book that discusses those techniques, i.e. where to learn them? $\bullet$ Edit: There's no problem if those techniques can only be learned from a particular section of one book or more (or even just a paper, as long as it is accessible to an upper-undergrad.). Thanks in advance, that would really help me unlock my box of tools.","I've seen in some answers in Brilliant.org to some very complicated limits and integrals that uses probabilistic arguments (Let $X$ be a random variable from $[0,1]$... some examples are in those answers , see also this for an example that has to do with evaluation of limit of a series ) or some uses Laplace transforms or even Fourier series (Example see some answers by user Tunk-Fey : 1 , 2 ). I would like to ask about a book that discusses those techniques, i.e. where to learn them? $\bullet$ Edit: There's no problem if those techniques can only be learned from a particular section of one book or more (or even just a paper, as long as it is accessible to an upper-undergrad.). Thanks in advance, that would really help me unlock my box of tools.",,"['calculus', 'probability', 'integration', 'limits', 'reference-request']"
64,Find the limit $x\to1$ of : $\vert x^2+x-2\vert /(x^2-1)$,Find the limit  of :,x\to1 \vert x^2+x-2\vert /(x^2-1),"Substitution of $1$ in the equation gives us $ \frac 00$ (indeterminate form), and so we must find the limit some other way. By breaking $\vert x^2+x-2\vert $ up into $\vert (x-1)(x+2)\vert $ and noting that $\vert x-1\vert  = -(x-1)$ when $x < 1 $  and $\vert x-1\vert  = (x-1)$ when $x > 1$ and that $\vert x+2\vert  = x + 2$ when $x > -2$ we deduce the following: $$\lim\limits_{x\to1^-}= \frac{-(x-1)(x+2)}{(x-1)(x+1)}= -(3/2)$$ (as the $(x-1)$ terms cancel. Using a similar argument for the limit as $x\to1^+$ (from the right) we  see that it equals $3/2$ (since $\vert x-1\vert  = (x - 1)$.) We conclude that the limit does not exist as the limits taken from the left side and right side (approaching $1$) do not equate (i.e. $-3/2$ does not equal $3/2$). Limit calculators say the answer is $0$, however, I can't seem to figure out how this would be so.","Substitution of $1$ in the equation gives us $ \frac 00$ (indeterminate form), and so we must find the limit some other way. By breaking $\vert x^2+x-2\vert $ up into $\vert (x-1)(x+2)\vert $ and noting that $\vert x-1\vert  = -(x-1)$ when $x < 1 $  and $\vert x-1\vert  = (x-1)$ when $x > 1$ and that $\vert x+2\vert  = x + 2$ when $x > -2$ we deduce the following: $$\lim\limits_{x\to1^-}= \frac{-(x-1)(x+2)}{(x-1)(x+1)}= -(3/2)$$ (as the $(x-1)$ terms cancel. Using a similar argument for the limit as $x\to1^+$ (from the right) we  see that it equals $3/2$ (since $\vert x-1\vert  = (x - 1)$.) We conclude that the limit does not exist as the limits taken from the left side and right side (approaching $1$) do not equate (i.e. $-3/2$ does not equal $3/2$). Limit calculators say the answer is $0$, however, I can't seem to figure out how this would be so.",,"['calculus', 'limits', 'proof-verification']"
65,"Prove $\frac{1}{b-a}\int_a^b\frac{x}{\sin x}dx\leqslant\frac{a+b}{\sin a+\sin b}$ $a,b\in(0,\frac{\pi}{2}),a<b$",Prove,"\frac{1}{b-a}\int_a^b\frac{x}{\sin x}dx\leqslant\frac{a+b}{\sin a+\sin b} a,b\in(0,\frac{\pi}{2}),a<b","For $a,b\in(0,\frac{\pi}{2}),a<b$, prove $$\frac{1}{b-a}\int_a^b\frac{x}{\sin x}dx\leqslant\frac{a+b}{\sin a+\sin b}.$$ By mean value theorem, there is $c\in(a,b)$ s.t. $$\frac{c}{\sin c}=\frac{1}{b-a}\int_a^b\frac{x}{\sin x}dx$$ I want to prove that $$\frac{c}{\sin c}\leqslant\frac{a+b}{\sin a+\sin b}$$ for any $c\in (a,b)$. I am not sure if I am on the right track","For $a,b\in(0,\frac{\pi}{2}),a<b$, prove $$\frac{1}{b-a}\int_a^b\frac{x}{\sin x}dx\leqslant\frac{a+b}{\sin a+\sin b}.$$ By mean value theorem, there is $c\in(a,b)$ s.t. $$\frac{c}{\sin c}=\frac{1}{b-a}\int_a^b\frac{x}{\sin x}dx$$ I want to prove that $$\frac{c}{\sin c}\leqslant\frac{a+b}{\sin a+\sin b}$$ for any $c\in (a,b)$. I am not sure if I am on the right track",,['calculus']
66,where do exponential and logarithmic functions intersect?,where do exponential and logarithmic functions intersect?,,"If $0<a<1$, then the graphs of $y=a^x$ and $y=\log_a(x)$ intersect at some point $(t(a),t(a))$. Does this function $t(a)$ have any nice expression? How much do we know about this function, except that it is obviously increasing and stays between $0$ and $1$? It is a very natural question, so I guess people thought about it before. EDIT: why do I think it is a natural question? When we teach students calculus, one way to introduce the $\log_a(x)$ function is as the inverse of $a^x=e^{x \ln a}$. And drawing the graphs of $a^x$ and $\log_a(x)$ for $0<a<1$ in one coordinate system pulls attention to their intersection point right away. EDIT 2: Thanks to lhf and Wolfram Alpha, the graph of $a^x=\log_a(x)$ for $x,a$ between $0$ and $1$ can be seen here . It shows that for $a<t(e^{-1})=e^{-e}\approx 0.065988$, we have THREE points of intersection of graphs $a^x$ and $\log_ax$ (see example of $a=0.03$ here ). At $a=e^{-e}$, the two graphs are tangent to each other (see here ). However, one of the points of intersection can be seen as the ""principal branch"", which I denoted $t(a)$ above. It looks like it has an inflection point at $a=e^{-1}\approx 0.367879$. EDIT 3: Interestingly, the graphs of $a^x$ and $\log_a(x)$ do intersect for some values of $a>1$, namely, if $1<a<e^{1/e}\approx 1.44467$: example for $a=1.3$ .","If $0<a<1$, then the graphs of $y=a^x$ and $y=\log_a(x)$ intersect at some point $(t(a),t(a))$. Does this function $t(a)$ have any nice expression? How much do we know about this function, except that it is obviously increasing and stays between $0$ and $1$? It is a very natural question, so I guess people thought about it before. EDIT: why do I think it is a natural question? When we teach students calculus, one way to introduce the $\log_a(x)$ function is as the inverse of $a^x=e^{x \ln a}$. And drawing the graphs of $a^x$ and $\log_a(x)$ for $0<a<1$ in one coordinate system pulls attention to their intersection point right away. EDIT 2: Thanks to lhf and Wolfram Alpha, the graph of $a^x=\log_a(x)$ for $x,a$ between $0$ and $1$ can be seen here . It shows that for $a<t(e^{-1})=e^{-e}\approx 0.065988$, we have THREE points of intersection of graphs $a^x$ and $\log_ax$ (see example of $a=0.03$ here ). At $a=e^{-e}$, the two graphs are tangent to each other (see here ). However, one of the points of intersection can be seen as the ""principal branch"", which I denoted $t(a)$ above. It looks like it has an inflection point at $a=e^{-1}\approx 0.367879$. EDIT 3: Interestingly, the graphs of $a^x$ and $\log_a(x)$ do intersect for some values of $a>1$, namely, if $1<a<e^{1/e}\approx 1.44467$: example for $a=1.3$ .",,"['calculus', 'logarithms', 'exponential-function']"
67,Getting started on first order differential equation,Getting started on first order differential equation,,"I'm trying to solve this differential equation: $$\frac{dy}{dx} = \frac{x-\exp(y)}{y+\exp(y)}$$ I thought I could use separation of variable, but I'm unable to isolate $x$. Could you please help me get started on this differential equation? Thank you","I'm trying to solve this differential equation: $$\frac{dy}{dx} = \frac{x-\exp(y)}{y+\exp(y)}$$ I thought I could use separation of variable, but I'm unable to isolate $x$. Could you please help me get started on this differential equation? Thank you",,"['calculus', 'ordinary-differential-equations']"
68,Prove that $\int_{0}^{1}{f^{2}(x)dx}\leq \frac{4}{3}\left(\int_{0}^{1}{f(x)dx}\right)^2$,Prove that,\int_{0}^{1}{f^{2}(x)dx}\leq \frac{4}{3}\left(\int_{0}^{1}{f(x)dx}\right)^2,"Let $f(x)$ be a concave nonnegative function on $[0,1]$ Prove that $$\displaystyle \int\limits_{0}^{1}{f^{2}(x)dx}\leq \frac{4}{3}\left(\int\limits_{0}^{1}{f(x)dx}\right)^2$$ My friend tian_275461 told me we even have the general result Let $f(x)$ be a concave nonnegative function on $[a,b]$,If $p>1$ $$ \frac{2^{p}}{p+1}\left(\frac{1}{b-a}\int\limits_{a}^{b}{f(x)dx}\right)^{p}\geq \frac{1}{b-a}\int\limits_{a}^{b}{f^{p}(x)dx} $$  If $0<p<1 $,the reverse inequality holds. I don't know how to deal with such function which is concave.","Let $f(x)$ be a concave nonnegative function on $[0,1]$ Prove that $$\displaystyle \int\limits_{0}^{1}{f^{2}(x)dx}\leq \frac{4}{3}\left(\int\limits_{0}^{1}{f(x)dx}\right)^2$$ My friend tian_275461 told me we even have the general result Let $f(x)$ be a concave nonnegative function on $[a,b]$,If $p>1$ $$ \frac{2^{p}}{p+1}\left(\frac{1}{b-a}\int\limits_{a}^{b}{f(x)dx}\right)^{p}\geq \frac{1}{b-a}\int\limits_{a}^{b}{f^{p}(x)dx} $$  If $0<p<1 $,the reverse inequality holds. I don't know how to deal with such function which is concave.",,['calculus']
69,Proof of the Pythagorean Theorem via $\frac{d}{dx}\sin^2 x + \frac{d}{dx}\cos^2 x = 0$,Proof of the Pythagorean Theorem via,\frac{d}{dx}\sin^2 x + \frac{d}{dx}\cos^2 x = 0,"Any one seen this proof before? $$\frac{d}{dx} \sin(x)^2=2\cos(x)\sin(x)$$ $$\frac{d}{dx} \cos(x)^2=-2\cos(x)\sin(x)$$ $$\frac{d}{dx} \sin(x)^2+\frac{d}{dx} \cos(x)^2=0$$ $$\sin(x)^2+\cos(x)^2=c$$ $$\sin(0)^2+\cos(0)^2=c$$ $$1=c,$$ $$\sin(x)^2+\cos(x)^2=1$$ Let C, A, and B be the hypotinuse, opposite, and ajacent sides of a right triangle, then $$((C\sin(x))^2+(C\cos(x))^2=C^2$$ $$A^2+B^2=C^2$$ Is this proof valid, i.e. is the Pythagorean theorem used in defining the above trig relations?","Any one seen this proof before? $$\frac{d}{dx} \sin(x)^2=2\cos(x)\sin(x)$$ $$\frac{d}{dx} \cos(x)^2=-2\cos(x)\sin(x)$$ $$\frac{d}{dx} \sin(x)^2+\frac{d}{dx} \cos(x)^2=0$$ $$\sin(x)^2+\cos(x)^2=c$$ $$\sin(0)^2+\cos(0)^2=c$$ $$1=c,$$ $$\sin(x)^2+\cos(x)^2=1$$ Let C, A, and B be the hypotinuse, opposite, and ajacent sides of a right triangle, then $$((C\sin(x))^2+(C\cos(x))^2=C^2$$ $$A^2+B^2=C^2$$ Is this proof valid, i.e. is the Pythagorean theorem used in defining the above trig relations?",,"['calculus', 'geometry', 'proof-writing']"
70,A problem with minimizing a function,A problem with minimizing a function,,"I have the following cost function: $\mbox{BSP Cost}=\sum_{i=1}^{\frac{n}{G}}G^{2}\left\lceil \frac{i}{p}\right\rceil +g\left(p\right)\sum_{i=1}^{\frac{n}{G}}Gi+l\left(p\right)\frac{n}{G}$ I would like to minimize it by choosing an appropriate G (i.e., G is a function of p and n). I have simplified it to the following form: $\mbox{BSP Cost}=\frac{Gn}{2p}+\frac{n^{2}}{2p}+g\left(p\right)\cdot\left(\frac{n}{2G}+\frac{n^{2}}{2G^{2}}\right)+l\left(p\right)\frac{n}{G}\to\min$ To find the minimization, I used a derivative $\frac{d}{dG}$ on the cost function, and compared to zero. I got this: $\frac{nG^{3}-g\left(p\right)npG-g\left(p\right)n^{2}p-2l\left(p\right)npG}{2pG^{3}}=0$ $nG^{3}-g\left(p\right)npG-g\left(p\right)np-2l\left(p\right)npG=0$ And I'm not sure how to proceed from this point. Can you help me find a function for G(n,p)? If you see any mistakes in the process above, please tell me. EDIT : it might also be important to mention that all the variables and functions (g,n,p,l,G) are positive. EDIT 2 : an rough approximation formula will do! EDIT 3 : Here's an approximation of g,l: $g\left(p\right)=-0.858p^{3}+12.31p^{2}-47.12p+79.67$ $l\left(p\right)=670.9p^{2}+2815p-2763$","I have the following cost function: $\mbox{BSP Cost}=\sum_{i=1}^{\frac{n}{G}}G^{2}\left\lceil \frac{i}{p}\right\rceil +g\left(p\right)\sum_{i=1}^{\frac{n}{G}}Gi+l\left(p\right)\frac{n}{G}$ I would like to minimize it by choosing an appropriate G (i.e., G is a function of p and n). I have simplified it to the following form: $\mbox{BSP Cost}=\frac{Gn}{2p}+\frac{n^{2}}{2p}+g\left(p\right)\cdot\left(\frac{n}{2G}+\frac{n^{2}}{2G^{2}}\right)+l\left(p\right)\frac{n}{G}\to\min$ To find the minimization, I used a derivative $\frac{d}{dG}$ on the cost function, and compared to zero. I got this: $\frac{nG^{3}-g\left(p\right)npG-g\left(p\right)n^{2}p-2l\left(p\right)npG}{2pG^{3}}=0$ $nG^{3}-g\left(p\right)npG-g\left(p\right)np-2l\left(p\right)npG=0$ And I'm not sure how to proceed from this point. Can you help me find a function for G(n,p)? If you see any mistakes in the process above, please tell me. EDIT : it might also be important to mention that all the variables and functions (g,n,p,l,G) are positive. EDIT 2 : an rough approximation formula will do! EDIT 3 : Here's an approximation of g,l: $g\left(p\right)=-0.858p^{3}+12.31p^{2}-47.12p+79.67$ $l\left(p\right)=670.9p^{2}+2815p-2763$",,['calculus']
71,How to prove that $\frac{1}{1^2}+\frac{1}{2^2}+\dots+\frac{1}{n^2}+\dots=\frac{\pi^2}{6}$ using the spiral right angle triangle method?,How to prove that  using the spiral right angle triangle method?,\frac{1}{1^2}+\frac{1}{2^2}+\dots+\frac{1}{n^2}+\dots=\frac{\pi^2}{6},"I see this formula given below on You tube video of mathologer channel and then I try to find some new method to prove it : $$\sum_{n=1}^\infty \frac1{n^2} = \frac{\pi^2}6$$ I tried to prove it geometrically like this Our attempt : (1) First I tried to convert it in inverse trignometric form like this but that doesn't help much: (2) In my second attempt I rotate the length of $1/2$ length from $1$ then I rotate $1/3$ length from remaining $1/2$ but that thing doesn't help us. (3) In my third attempt, I tried to use coordinate geometry but that makes things more complex. My question :How to prove that that summation of $1/n^2$ where $n$ tends to infinity is equal to $π^2/6$ by using spiral right angle triangle method ? EDIT NOTE: Sinc the last line segment Whose length tends to Square root of $π^2/6$ but not exactly equal to Square root of $π^2/6$ so it is probably not possible to solved it by using pure geometry . we understand that there must be needs of theory of Limit to prove it . so we will also accept the solution which take the use of both concept means geometry with slight use of calculus.","I see this formula given below on You tube video of mathologer channel and then I try to find some new method to prove it : I tried to prove it geometrically like this Our attempt : (1) First I tried to convert it in inverse trignometric form like this but that doesn't help much: (2) In my second attempt I rotate the length of length from then I rotate length from remaining but that thing doesn't help us. (3) In my third attempt, I tried to use coordinate geometry but that makes things more complex. My question :How to prove that that summation of where tends to infinity is equal to by using spiral right angle triangle method ? EDIT NOTE: Sinc the last line segment Whose length tends to Square root of but not exactly equal to Square root of so it is probably not possible to solved it by using pure geometry . we understand that there must be needs of theory of Limit to prove it . so we will also accept the solution which take the use of both concept means geometry with slight use of calculus.",\sum_{n=1}^\infty \frac1{n^2} = \frac{\pi^2}6 1/2 1 1/3 1/2 1/n^2 n π^2/6 π^2/6 π^2/6,"['calculus', 'geometry']"
72,How prove this identity wih the sum equal to other sum,How prove this identity wih the sum equal to other sum,,"Question: Given $l\in \mathbb{N^+}$ . $a_1,\cdots,a_l,b_1,\cdots,b_l$ are real numbers. $a_0=b_0=a_{l+1}=b_{l+1}=0$ .Define $$g(m,l)=-\dfrac{\displaystyle\prod _{r=0} ^{l}{(a_m-a_r+b_{r+1}-b_m)}}{\displaystyle\prod _{r=1,r\ne m} ^{l}{(a_m-a_r+b_r-b_m)}}$$ Prove: $$\sum_{m=1}^{l}{g(m,l)}=\sum_{t=1}^{l}{(a_t-a_{t-1})b_t}$$ For example,for $l=1$ , $g(1,1)=-\frac{a_1(-b_1)}{1}=(a_1-a_0)b_1$ Another example:for $l=2$ , $$g(1,2)+g(2,2)=-\frac{a_1(b_2-b_1)(a_1-a_2-b_1)}{a_1-a_2+b_2-b_1}+\frac{b_2(a_2-a_1)(a_2+b_1-b_2)}{a_2-a_1+b_1-b_2}$$ $$=\frac{(a_2-a_1+b_1-b_2)(a_1 b_1+a_2 b_2-a_1 b_2)}{a_2-a_1+b_1-b_2}=\sum_{t=1}^{2}{(a_t-a_{t-1})b_t}$$ But How to prove general case,Maybe use Lagrange interpolation formula?","Question: Given . are real numbers. .Define Prove: For example,for , Another example:for , But How to prove general case,Maybe use Lagrange interpolation formula?","l\in \mathbb{N^+} a_1,\cdots,a_l,b_1,\cdots,b_l a_0=b_0=a_{l+1}=b_{l+1}=0 g(m,l)=-\dfrac{\displaystyle\prod _{r=0} ^{l}{(a_m-a_r+b_{r+1}-b_m)}}{\displaystyle\prod _{r=1,r\ne m} ^{l}{(a_m-a_r+b_r-b_m)}} \sum_{m=1}^{l}{g(m,l)}=\sum_{t=1}^{l}{(a_t-a_{t-1})b_t} l=1 g(1,1)=-\frac{a_1(-b_1)}{1}=(a_1-a_0)b_1 l=2 g(1,2)+g(2,2)=-\frac{a_1(b_2-b_1)(a_1-a_2-b_1)}{a_1-a_2+b_2-b_1}+\frac{b_2(a_2-a_1)(a_2+b_1-b_2)}{a_2-a_1+b_1-b_2} =\frac{(a_2-a_1+b_1-b_2)(a_1 b_1+a_2 b_2-a_1 b_2)}{a_2-a_1+b_1-b_2}=\sum_{t=1}^{2}{(a_t-a_{t-1})b_t}",['calculus']
73,Prove this Generalizing AM-GM inequality,Prove this Generalizing AM-GM inequality,,"Let $n\ge 2$ and $a_{i} \ge 0,i=1,2,\cdots,n$, show that   $$(n-1)^{n-1}(a^n_{1}+a^n_{2}+\cdots+a^n_{n})+n^na_{1}a_{2}\cdots a_{n}\ge (a_{1}+a_{2}+\cdots+a_{n})^n$$ When $n=2$, $$a^2_{2}+a^2_{2}+4a_{1}a_{2}=(a_{1}+a_{2})^2+2a_{1}a_{2}\ge (a_{1}+a_{2})^2$$ When $n=3$, it is $$4(a^3_{1}+a^3_{2}+a^3_{3})+27a_{1}a_{2}a_{3}\ge (a_{1}+a_{2}+a_{3})^3$$ By $$(a_{1}+a_{2}+a_{3})^3=a^3_{1}+a^3_{2}+a^3_{3}+3a_{1}a_{2}(a_{1}+a_{2})+3a_{1}a_{3}(a_{1}+a_{3})+3a_{2}a_{3}(a_{2}+a_{3})+6a_{1}a_{2}a_{3}$$ so it's enough to prove $$a^3_{1}+a^3_{2}+a^3_{3}+7a_{1}a_{2}a_{3}\ge a_{1}a_{2}(a_{1}+a_{2})+a_{1}a_{3}(a_{1}+a_{3})+a_{2}a_{3}(a_{2}+a_{3})$$ which is clear by using Schur inequality : $$a^3+b^3+c^3+3abc\ge ab(a+b)+bc(b+c)+ac(a+c)$$","Let $n\ge 2$ and $a_{i} \ge 0,i=1,2,\cdots,n$, show that   $$(n-1)^{n-1}(a^n_{1}+a^n_{2}+\cdots+a^n_{n})+n^na_{1}a_{2}\cdots a_{n}\ge (a_{1}+a_{2}+\cdots+a_{n})^n$$ When $n=2$, $$a^2_{2}+a^2_{2}+4a_{1}a_{2}=(a_{1}+a_{2})^2+2a_{1}a_{2}\ge (a_{1}+a_{2})^2$$ When $n=3$, it is $$4(a^3_{1}+a^3_{2}+a^3_{3})+27a_{1}a_{2}a_{3}\ge (a_{1}+a_{2}+a_{3})^3$$ By $$(a_{1}+a_{2}+a_{3})^3=a^3_{1}+a^3_{2}+a^3_{3}+3a_{1}a_{2}(a_{1}+a_{2})+3a_{1}a_{3}(a_{1}+a_{3})+3a_{2}a_{3}(a_{2}+a_{3})+6a_{1}a_{2}a_{3}$$ so it's enough to prove $$a^3_{1}+a^3_{2}+a^3_{3}+7a_{1}a_{2}a_{3}\ge a_{1}a_{2}(a_{1}+a_{2})+a_{1}a_{3}(a_{1}+a_{3})+a_{2}a_{3}(a_{2}+a_{3})$$ which is clear by using Schur inequality : $$a^3+b^3+c^3+3abc\ge ab(a+b)+bc(b+c)+ac(a+c)$$",,"['calculus', 'multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality']"
74,Is the given binomial sum almost everywhere negative as $K\to\infty$?,Is the given binomial sum almost everywhere negative as ?,K\to\infty,"The binomial sum is as follows: $$\mathcal {L}^K(\theta)= \sum_{i=\lceil{K/2}\rceil}^K \binom{K}{i}\theta^i\left((1-\theta)^{K-i}-\frac{1}{2}(1-\theta)^{-K}(1-2\theta)^{K-i}\right)$$ which can also be written as $$\mathcal {L}^K(\theta)= B(K/2,K,1-\theta)-\frac{1}{2}B\left(K/2,K,\frac{1-2\theta}{1-\theta}\right)$$ where $B$ is the Binomial c.d.f. It can be found that as $K\to\infty$, \begin{equation} \quad\mathcal {L}^\infty(\theta)= \begin{cases} 0, & \mbox{if } \mbox{ $\theta<\frac{1}{3}$} \\ -\frac{1}{2} & \mbox{if } \mbox{ $\frac{1}{3}<\theta<\frac{1}{2}$} \end{cases}\nonumber \end{equation} My question is however not about it. I want to show that $\mathcal {L}^K(\theta)$ is negative if $K$ is chosen large enough, for every $\theta\in(0,0.5)$. This question is equivalent to finding that the zero crossing point $\theta_0\in(0,0.5)$ of $\mathcal {L}^K$ tends to $0$ as $K\to\infty$ and for every $\theta>\theta_0$, $\mathcal {L}^K$ is negative. Notice that $K$ is an odd number for all cases. A mathematica plot confirms that it must be the case. Here is the plot: But I dont have any idea on how to show it. Does anyone have?","The binomial sum is as follows: $$\mathcal {L}^K(\theta)= \sum_{i=\lceil{K/2}\rceil}^K \binom{K}{i}\theta^i\left((1-\theta)^{K-i}-\frac{1}{2}(1-\theta)^{-K}(1-2\theta)^{K-i}\right)$$ which can also be written as $$\mathcal {L}^K(\theta)= B(K/2,K,1-\theta)-\frac{1}{2}B\left(K/2,K,\frac{1-2\theta}{1-\theta}\right)$$ where $B$ is the Binomial c.d.f. It can be found that as $K\to\infty$, \begin{equation} \quad\mathcal {L}^\infty(\theta)= \begin{cases} 0, & \mbox{if } \mbox{ $\theta<\frac{1}{3}$} \\ -\frac{1}{2} & \mbox{if } \mbox{ $\frac{1}{3}<\theta<\frac{1}{2}$} \end{cases}\nonumber \end{equation} My question is however not about it. I want to show that $\mathcal {L}^K(\theta)$ is negative if $K$ is chosen large enough, for every $\theta\in(0,0.5)$. This question is equivalent to finding that the zero crossing point $\theta_0\in(0,0.5)$ of $\mathcal {L}^K$ tends to $0$ as $K\to\infty$ and for every $\theta>\theta_0$, $\mathcal {L}^K$ is negative. Notice that $K$ is an odd number for all cases. A mathematica plot confirms that it must be the case. Here is the plot: But I dont have any idea on how to show it. Does anyone have?",,"['calculus', 'limits', 'polynomials', 'binomial-coefficients']"
75,Algorithm for obtaining the surface of a mirror,Algorithm for obtaining the surface of a mirror,,"My colleague and I have been trying to implement an algorithm described in the paper "" Recovering local shape of a mirror surface from reflection of a regular grid "", primary author of which being Silvio Savarese. I believe we have come a way in our understanding of it, but I wondered if any one else has attempted it's implementation, or succeeded, and would be willing to provide some insight? More specifically, the algorithm is suppose to be able to obtain the position, normal, and curvature of a reflective surface at a point using only 2 intersecting lines at that point, and their vertices. I have already successfully implemented an algorithm similar to that described in "" 3D Acquisition of Mirroring Objects "" in c++ and Objective-C using OpenCV, but that unfortunately requires an initial depth value (at least it does for us because the solution they provide for obtaining the initial depth in sec 5.4 doesn't seem to work, probably because we have much lower resolution as we can't use calibrated color as a cue as they do). Which, since having to know an initial point is not very ideal in our application, we're exploring Silvio's method as a possible alternative. Heres what we have gleaned of his method so far (keeping in mind that all I care about, at the moment, is the value 's' and it's corresponding normal 'nr' which can be easily obtained once s is known): Calibrate the camera and scene (already done and tested and used in our prior algorithm) Obtain qDot for the 2 lines by Eq 32 and project them onto the camera sensor plane 'l'    units along the view direction 'v' Obtain the corresponding deltaP's (basic math since the scene is already calibrated) Calculate the tanPhi's by normalizing qDot and taking the tangent of the angle between    that and the image plane's x axis. for(double s=startValue, s<=endValue; s+=stepSize) {   Solve for the vector position 'r' on the mirror surface (Eq 1)   Solve for the matrix 'R' via solving for 'np' and 'nr' to obtain conversion between XYZ      and UVW space (Page 4of14, second paragraph)   Solve cosTheta (shown with Eq3) and sinTheta (obtained by equation for cosTheta      via trig rules)   Solve Bu and Bv (shown with Eq3)   Solve for matrix 'T' (Eq 12)   Solve for h1, h2, and h3 (described right below Eq 4 on page 6of14)   Solve for V (Described half way down on page 8of14)   Check to see if Eq 17 is true (or within some epsilon), if is, return s,      else continue. } I realize that is not the most effective numerical solver for s (we're actually using a bit better solution), but understand that this is just a example and I've already graphed all chosen values for s (1in to 1000in with a 0.1in step, 40in or so being the correct value) at several regions on our part and it only approaches solving for Eq 17 one time, and diverges all others. So something is wrong, because it never actually reaches a solution. Furthermore, if you run this algorithm on all the points we have on our part, and project their corresponding s values (the ones that came closest to solving E17) into 3 space, it looks like noise and has no resemblance to our part. I've tried several different methods for calculating tanPhi, and am not totally sure what is the correct way. Calculating it in image space, or calculating it by the angle between rDot and the U axis (obtained by Eq11 and the cross of np and nr)? The paper doesn't seem totally clear on that, or how to do it. We don't actually have intersecting lines, just dots, so I know tanPhi will be a little off just because it's changing as deltaT varies (which our deltaT is relatively large) but it's not varying that much, the surface we're looking at is only about 1200mm-1400mm radius, and I would think if we had it just a little off, the part would look noisy but still generally correct... instead it is totally off. 'l' I am still a little confused on as well, in another stackoverflow post ' Camera Properties for 3d using specular reflection ', 'l' was stated to be the focal length, but there is an x and y component to the focal length in a pinhole camera model intrinsic matrix... Can we just average the 2? They are fairly close in our case, but what if they weren't would that still be sufficient? I'm also not totally sure what needs to be converted to UVW space (""the principle reference system""), and what doesn't. I know it states on page 4, just before sec 2.1, that all calculations from here are in UVW (unless otherwise stated), but some things seem to make more sense to calculate in XYZ (like cosTheta and 'T') and he never mentions they should be. In Silvios phd thesis , for example, he doesn't say that until after obtaining cosTheta. So I'm assuming cosTeta can be obtained in XYZ, as it's just the cos of an angle that can be obtained in any space? But can't 'T' be obtained in any space as well, by the same logic, since it's just a rotation matrix? And, with the converting (XYZ to UVW) does it really make sense to subtract r in every instance? I know for a point like 'p0' it makes sense... your just centering it on r, but for directions does it really make any sense (for calculating something like 'T')? For that you'd have to convert 'd' to UVW as well (if 'T' is in fact suppose to be calculated in UVW), but 'd' is a direction... centering it on r makes no sense. He's not very explicit on that. Last thing I can think of, and this probably can wait until the math is all right, but can s be solved for in a closed form of the sort ""s=f(measured and calibrated values)""? Silvio eludes to the fact that rDot can be solved for in closed form, after knowing s, but never really seems to say where s can be or not. I just wonder if this is going to forever require a numerical solver or not? The one I quick wrote up in c++ seems to work well, and converges quickly, but it'd obviously be nice if this was a closed function, plug and chug sort of speak! My colleague has attempted to contact Silvio, as well as a few of the co-authors, but so far no luck. I would be greatly appreciative if someone could provide some insight here! It would be nice to see an algorithm like this out and in peoples hands, perhaps even implemented in OpenCV one day! I know obtaining the surface of a specular reflector is still very much an open problem, but it would be nice if this algorithm, and others like it, were available for testing!","My colleague and I have been trying to implement an algorithm described in the paper "" Recovering local shape of a mirror surface from reflection of a regular grid "", primary author of which being Silvio Savarese. I believe we have come a way in our understanding of it, but I wondered if any one else has attempted it's implementation, or succeeded, and would be willing to provide some insight? More specifically, the algorithm is suppose to be able to obtain the position, normal, and curvature of a reflective surface at a point using only 2 intersecting lines at that point, and their vertices. I have already successfully implemented an algorithm similar to that described in "" 3D Acquisition of Mirroring Objects "" in c++ and Objective-C using OpenCV, but that unfortunately requires an initial depth value (at least it does for us because the solution they provide for obtaining the initial depth in sec 5.4 doesn't seem to work, probably because we have much lower resolution as we can't use calibrated color as a cue as they do). Which, since having to know an initial point is not very ideal in our application, we're exploring Silvio's method as a possible alternative. Heres what we have gleaned of his method so far (keeping in mind that all I care about, at the moment, is the value 's' and it's corresponding normal 'nr' which can be easily obtained once s is known): Calibrate the camera and scene (already done and tested and used in our prior algorithm) Obtain qDot for the 2 lines by Eq 32 and project them onto the camera sensor plane 'l'    units along the view direction 'v' Obtain the corresponding deltaP's (basic math since the scene is already calibrated) Calculate the tanPhi's by normalizing qDot and taking the tangent of the angle between    that and the image plane's x axis. for(double s=startValue, s<=endValue; s+=stepSize) {   Solve for the vector position 'r' on the mirror surface (Eq 1)   Solve for the matrix 'R' via solving for 'np' and 'nr' to obtain conversion between XYZ      and UVW space (Page 4of14, second paragraph)   Solve cosTheta (shown with Eq3) and sinTheta (obtained by equation for cosTheta      via trig rules)   Solve Bu and Bv (shown with Eq3)   Solve for matrix 'T' (Eq 12)   Solve for h1, h2, and h3 (described right below Eq 4 on page 6of14)   Solve for V (Described half way down on page 8of14)   Check to see if Eq 17 is true (or within some epsilon), if is, return s,      else continue. } I realize that is not the most effective numerical solver for s (we're actually using a bit better solution), but understand that this is just a example and I've already graphed all chosen values for s (1in to 1000in with a 0.1in step, 40in or so being the correct value) at several regions on our part and it only approaches solving for Eq 17 one time, and diverges all others. So something is wrong, because it never actually reaches a solution. Furthermore, if you run this algorithm on all the points we have on our part, and project their corresponding s values (the ones that came closest to solving E17) into 3 space, it looks like noise and has no resemblance to our part. I've tried several different methods for calculating tanPhi, and am not totally sure what is the correct way. Calculating it in image space, or calculating it by the angle between rDot and the U axis (obtained by Eq11 and the cross of np and nr)? The paper doesn't seem totally clear on that, or how to do it. We don't actually have intersecting lines, just dots, so I know tanPhi will be a little off just because it's changing as deltaT varies (which our deltaT is relatively large) but it's not varying that much, the surface we're looking at is only about 1200mm-1400mm radius, and I would think if we had it just a little off, the part would look noisy but still generally correct... instead it is totally off. 'l' I am still a little confused on as well, in another stackoverflow post ' Camera Properties for 3d using specular reflection ', 'l' was stated to be the focal length, but there is an x and y component to the focal length in a pinhole camera model intrinsic matrix... Can we just average the 2? They are fairly close in our case, but what if they weren't would that still be sufficient? I'm also not totally sure what needs to be converted to UVW space (""the principle reference system""), and what doesn't. I know it states on page 4, just before sec 2.1, that all calculations from here are in UVW (unless otherwise stated), but some things seem to make more sense to calculate in XYZ (like cosTheta and 'T') and he never mentions they should be. In Silvios phd thesis , for example, he doesn't say that until after obtaining cosTheta. So I'm assuming cosTeta can be obtained in XYZ, as it's just the cos of an angle that can be obtained in any space? But can't 'T' be obtained in any space as well, by the same logic, since it's just a rotation matrix? And, with the converting (XYZ to UVW) does it really make sense to subtract r in every instance? I know for a point like 'p0' it makes sense... your just centering it on r, but for directions does it really make any sense (for calculating something like 'T')? For that you'd have to convert 'd' to UVW as well (if 'T' is in fact suppose to be calculated in UVW), but 'd' is a direction... centering it on r makes no sense. He's not very explicit on that. Last thing I can think of, and this probably can wait until the math is all right, but can s be solved for in a closed form of the sort ""s=f(measured and calibrated values)""? Silvio eludes to the fact that rDot can be solved for in closed form, after knowing s, but never really seems to say where s can be or not. I just wonder if this is going to forever require a numerical solver or not? The one I quick wrote up in c++ seems to work well, and converges quickly, but it'd obviously be nice if this was a closed function, plug and chug sort of speak! My colleague has attempted to contact Silvio, as well as a few of the co-authors, but so far no luck. I would be greatly appreciative if someone could provide some insight here! It would be nice to see an algorithm like this out and in peoples hands, perhaps even implemented in OpenCV one day! I know obtaining the surface of a specular reflector is still very much an open problem, but it would be nice if this algorithm, and others like it, were available for testing!",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'algorithms', 'computer-science']"
76,Find $\lim\limits_{n\to\infty}\sqrt{1+\sqrt{1/2+\dots+\sqrt{1/n}}}$ [closed],Find  [closed],\lim\limits_{n\to\infty}\sqrt{1+\sqrt{1/2+\dots+\sqrt{1/n}}},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find    $$\lim_{n\to\infty}\sqrt{1+\sqrt{\frac{1}{2}+\sqrt{\frac{1}{3}+\dots+\sqrt{\frac{1}{n}}}}}$$ How to evaluate this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find    $$\lim_{n\to\infty}\sqrt{1+\sqrt{\frac{1}{2}+\sqrt{\frac{1}{3}+\dots+\sqrt{\frac{1}{n}}}}}$$ How to evaluate this?",,"['calculus', 'limits']"
77,Is it possible to do calculus on any field with a topology?,Is it possible to do calculus on any field with a topology?,,"I'll try to make my point clear: when we consider the field of complex numbers $\mathbb{C}$ we can do calculus there because we have properties of a field and in the same time we have a topology to define limits. So, we have a metric given by the norm and so we can construct a basis for a topology using this metric and everything works fine. Since $\mathbb{C}$ is a field, then we can define algebraic functions like $f : \mathbb{C}\to \mathbb{C}$ given by $f(z)=z^k$ and the properties of the field together with the definition of limit implies that we can define a derivative $f'(z)=kz^{k-1}$. Integration seems more complicated and requiring more structure because we integrate differential forms, so we must be able to talk about them. My question is: if $F$ is any field and if $\tau$ is a topology on $F$, can we define a differential calculus exactly as we do with $\mathbb{R}$ and $\mathbb{C}$? My thought was that the only true requirement was that the we need a metric and that $(F,d)$ should be complete, but I can be totally wrong. Is just possible to do differential calculus like we do with $\mathbb{C}$ in $\mathbb{C}$ and $\mathbb{R}$ in any field endowed with a metric or this just works in those two specific fields? Thanks very much in advance!","I'll try to make my point clear: when we consider the field of complex numbers $\mathbb{C}$ we can do calculus there because we have properties of a field and in the same time we have a topology to define limits. So, we have a metric given by the norm and so we can construct a basis for a topology using this metric and everything works fine. Since $\mathbb{C}$ is a field, then we can define algebraic functions like $f : \mathbb{C}\to \mathbb{C}$ given by $f(z)=z^k$ and the properties of the field together with the definition of limit implies that we can define a derivative $f'(z)=kz^{k-1}$. Integration seems more complicated and requiring more structure because we integrate differential forms, so we must be able to talk about them. My question is: if $F$ is any field and if $\tau$ is a topology on $F$, can we define a differential calculus exactly as we do with $\mathbb{R}$ and $\mathbb{C}$? My thought was that the only true requirement was that the we need a metric and that $(F,d)$ should be complete, but I can be totally wrong. Is just possible to do differential calculus like we do with $\mathbb{C}$ in $\mathbb{C}$ and $\mathbb{R}$ in any field endowed with a metric or this just works in those two specific fields? Thanks very much in advance!",,"['calculus', 'metric-spaces', 'field-theory']"
78,"If $f<1$, $f(0)^2 + f'(0)^2=4$, exists $x_0$ s.t. $f''(x_0) + f(x_0)=0$","If , , exists  s.t.",f<1 f(0)^2 + f'(0)^2=4 x_0 f''(x_0) + f(x_0)=0,"Suppose $f:\mathbb{R}\to\mathbb{R}$ is $C^2$, $f < 1$ for all $x$, and $f(0)^2 + f'(0)^2=4$. Show that $\exists x_0$ s.t. $f''(x_0) + f(x_0)=0$. So far, I have let $\phi(x) = f(x)^2 + f'(x)^2$. Then $$\phi'(x) = 2f(x)f'(x) + 2f'(x)f''(x) = 2f'(x)(f(x) + f''(x)).$$ So we need to show that there's a critical point of $\phi$ that is not a critical point of $f$. I believe this is supposed to be an exercise in the mean value theorem, but I don't know where to find another value of $\phi(x)$. Any ideas? (This is a problem from a teacher's set of notes, so of course there could be a typo. Could it be that $|f|<1$ is what he meant, for instance?)","Suppose $f:\mathbb{R}\to\mathbb{R}$ is $C^2$, $f < 1$ for all $x$, and $f(0)^2 + f'(0)^2=4$. Show that $\exists x_0$ s.t. $f''(x_0) + f(x_0)=0$. So far, I have let $\phi(x) = f(x)^2 + f'(x)^2$. Then $$\phi'(x) = 2f(x)f'(x) + 2f'(x)f''(x) = 2f'(x)(f(x) + f''(x)).$$ So we need to show that there's a critical point of $\phi$ that is not a critical point of $f$. I believe this is supposed to be an exercise in the mean value theorem, but I don't know where to find another value of $\phi(x)$. Any ideas? (This is a problem from a teacher's set of notes, so of course there could be a typo. Could it be that $|f|<1$ is what he meant, for instance?)",,"['calculus', 'derivatives']"
79,How to calculate this volume?,How to calculate this volume?,,"Be the sets: $$C:= \lbrace (x,y,0)\in\mathbb{R}^{3}: (x-1)^2+y^2=1\rbrace$$ $$C':= \lbrace (x,0,z)\in\mathbb{R}^{3}: (x+1)^2+z^2=1\rbrace $$ $$\overline{C}= \lbrace tx+(1-t)x': x\in C, x' \in C', t\in [0,1]\rbrace$$ Calculate the volume of $\overline{C}$.  I drawed the sets $C$ and $C'$, but I can't see how is the set $\overline{C}$","Be the sets: $$C:= \lbrace (x,y,0)\in\mathbb{R}^{3}: (x-1)^2+y^2=1\rbrace$$ $$C':= \lbrace (x,0,z)\in\mathbb{R}^{3}: (x+1)^2+z^2=1\rbrace $$ $$\overline{C}= \lbrace tx+(1-t)x': x\in C, x' \in C', t\in [0,1]\rbrace$$ Calculate the volume of $\overline{C}$.  I drawed the sets $C$ and $C'$, but I can't see how is the set $\overline{C}$",,"['calculus', 'multivariable-calculus', 'volume']"
80,Prove that $f(1)-f(1/e)\le \int_0^1 \sqrt{x} f'(x) dx$,Prove that,f(1)-f(1/e)\le \int_0^1 \sqrt{x} f'(x) dx,"Let $f:[0,1]\rightarrow \mathbb{R}$ be a differentiable function such that $$f(x^2)+f(y^2)\le2 f(\sqrt{x y}), \space x,y\ge0 $$ Prove that $$f(1)-f(1/e)\le \int_0^1 \sqrt{x} f'(x) dx$$ Where should I start from?","Let $f:[0,1]\rightarrow \mathbb{R}$ be a differentiable function such that $$f(x^2)+f(y^2)\le2 f(\sqrt{x y}), \space x,y\ge0 $$ Prove that $$f(1)-f(1/e)\le \int_0^1 \sqrt{x} f'(x) dx$$ Where should I start from?",,"['calculus', 'integration', 'inequality', 'definite-integrals', 'integral-inequality']"
81,L'Hôpital's rule in topological vector spaces,L'Hôpital's rule in topological vector spaces,,"Let $E$ be a (separated) topological vector space over $\mathbb{R}$ , $f\colon [0,1]\to E$ continuous. Assume that for every $t \in (0,1)$ we have a derivative $$f'(t) = \lim_{h\to 0} \frac{f(t+h)- f(h)}{h}$$ Moreover, assume that there exists the limit $\lim_{t \to 0_{+}} f'(t)$ Is it true that $$\lim_{h\to 0_{+}} \frac{f(h)-f(0)}{h}=\lim_{t \to 0_{+}} f'(t) $$ ? Notes: This is true for $E$ finite dimensional topological vector space ( since it's isomorphic to $\mathbb{R}^n$ , and we can work component-wise, using Lagrange intermediate value theorem). If $E$ is arbitrary, we can still conclude $$ \frac{f(h) - f(0)}{h} \to \lim_{t\to 0} f'(t)$$ weakly. If $f'$ has a continuous extension to $[0,1]$ , and $E$ is locally convex and complete ( say a Banach space) then we have $$f(h) - f(0) = \int_{0}^h f'(t) dt = h \cdot \int_{0}^1 f'(h t) d t$$ so again we get the desired equality. It may be that the equality ( L'Hospital rule ) is not true if $E$ is not locally convex. It may have to do with the fact that the average of a sequence convergent to $0$ may not converge to $0$ . Or it may not work in the general form even for locally convex spaces, or even Banach spaces. $\bf{Added:}$ I think it will work for locally convex spaces.  We use a bit of compactness too.  But I haven't written down details. Anyways, maybe it's a very basic fact, for which references are available. $\bf{Added:}$ The following lemma is useful, and not hard to prove (use the compactness of $[a,b]$ ): Let $[a,b]$ an interval, $f\colon [a,b]\to E$ , with a derivative at every point. Let $U$ be an open convex set such that $f'(t) \in U$ for all $t \in [a,b]$ . Then $$\frac{f(b)-f(a)}{b-a} \in U$$ $\bf{Added:}$ Another lemma : Let $[a,b]$ an interval, $f\colon [a,b]\to E$ , with a derivative at every point except a finite set $A$ .  Let $U$ be an open convex set such that $f'(t) \in U$ for all $t \in [a,b]\backslash A$ . Then $$\frac{f(b)-f(a)}{b-a} \in \bar U$$ ( similar to Dieudonne lemma here... perhaps it's also valid with the proper modifications). Note: this will take care of the case $E$ locally convex. $\bf{Added:}$ It turns out that the statement is related to the mean value theorem ""inequality"", as seen from the previous lemma. What I have learned and is somehow related is that the inequality might not be true if the topological vector space $E$ does not have any linear functionals $\ne 0$ . There exist functions $f\colon [0,1]\to E$ , $f'(t) = 0$ for all $t \in [0,1]$ , but $f$ not constant. See "" functions with $0$ derivative"". $\bf{Added:}$ This is the example of Rolewicz of a nonconstant function with $0$ derivative from $[0,1]$ to $L^p[0,1]$ ( $0<p<1$ ). Take $f(t) = \chi_{[0,t]}$ (!). Following this idea, here is an example of a continuous function $f\colon [0,1] \to L^p[0,1]$ such that $f'(t) = 0$ for all $t \in (0,1]$ , but $\frac{f(h)-f(0)}{h}$ does not converge to $0$ as $h\to 0_{+}$ . Just take $$f(t) = \chi_{[0, t^p]}$$ This provides a counterexample. $\bf{Added:}$ Now that we know it may not work for spaces what are not locally convex, let's see what more can be said. With the same methods we can show: if $f\colon [0,1]\to E$ , $g\colon [0,1] \to \mathbb{R}$ continuous functions, with derivatives on $(0,1]$ , and moreover: $E$ is locally convex and $g$ is monotonous around $0$ (I cannot avoid this extra hypothesis for general l.c. $E$ ), and moreover $\lim_{t\to 0_{+}} \frac{f'(t)}{g'(t)}$ exists then $$\lim_{t\to 0_{+}} \frac {f(t)-f(0)}{g(t)-g(0)} = \lim_{t\to 0_+} \frac{f'(t)}{g'(t)}$$","Let be a (separated) topological vector space over , continuous. Assume that for every we have a derivative Moreover, assume that there exists the limit Is it true that ? Notes: This is true for finite dimensional topological vector space ( since it's isomorphic to , and we can work component-wise, using Lagrange intermediate value theorem). If is arbitrary, we can still conclude weakly. If has a continuous extension to , and is locally convex and complete ( say a Banach space) then we have so again we get the desired equality. It may be that the equality ( L'Hospital rule ) is not true if is not locally convex. It may have to do with the fact that the average of a sequence convergent to may not converge to . Or it may not work in the general form even for locally convex spaces, or even Banach spaces. I think it will work for locally convex spaces.  We use a bit of compactness too.  But I haven't written down details. Anyways, maybe it's a very basic fact, for which references are available. The following lemma is useful, and not hard to prove (use the compactness of ): Let an interval, , with a derivative at every point. Let be an open convex set such that for all . Then Another lemma : Let an interval, , with a derivative at every point except a finite set .  Let be an open convex set such that for all . Then ( similar to Dieudonne lemma here... perhaps it's also valid with the proper modifications). Note: this will take care of the case locally convex. It turns out that the statement is related to the mean value theorem ""inequality"", as seen from the previous lemma. What I have learned and is somehow related is that the inequality might not be true if the topological vector space does not have any linear functionals . There exist functions , for all , but not constant. See "" functions with derivative"". This is the example of Rolewicz of a nonconstant function with derivative from to ( ). Take (!). Following this idea, here is an example of a continuous function such that for all , but does not converge to as . Just take This provides a counterexample. Now that we know it may not work for spaces what are not locally convex, let's see what more can be said. With the same methods we can show: if , continuous functions, with derivatives on , and moreover: is locally convex and is monotonous around (I cannot avoid this extra hypothesis for general l.c. ), and moreover exists then","E \mathbb{R} f\colon [0,1]\to E t \in (0,1) f'(t) = \lim_{h\to 0} \frac{f(t+h)- f(h)}{h} \lim_{t \to 0_{+}} f'(t) \lim_{h\to 0_{+}} \frac{f(h)-f(0)}{h}=\lim_{t \to 0_{+}} f'(t)  E \mathbb{R}^n E  \frac{f(h) - f(0)}{h} \to \lim_{t\to 0} f'(t) f' [0,1] E f(h) - f(0) = \int_{0}^h f'(t) dt = h \cdot \int_{0}^1 f'(h t) d t E 0 0 \bf{Added:} \bf{Added:} [a,b] [a,b] f\colon [a,b]\to E U f'(t) \in U t \in [a,b] \frac{f(b)-f(a)}{b-a} \in U \bf{Added:} [a,b] f\colon [a,b]\to E A U f'(t) \in U t \in [a,b]\backslash A \frac{f(b)-f(a)}{b-a} \in \bar U E \bf{Added:} E \ne 0 f\colon [0,1]\to E f'(t) = 0 t \in [0,1] f 0 \bf{Added:} 0 [0,1] L^p[0,1] 0<p<1 f(t) = \chi_{[0,t]} f\colon [0,1] \to L^p[0,1] f'(t) = 0 t \in (0,1] \frac{f(h)-f(0)}{h} 0 h\to 0_{+} f(t) = \chi_{[0, t^p]} \bf{Added:} f\colon [0,1]\to E g\colon [0,1] \to \mathbb{R} (0,1] E g 0 E \lim_{t\to 0_{+}} \frac{f'(t)}{g'(t)} \lim_{t\to 0_{+}} \frac {f(t)-f(0)}{g(t)-g(0)} = \lim_{t\to 0_+} \frac{f'(t)}{g'(t)}","['calculus', 'analysis', 'convex-analysis', 'topological-vector-spaces', 'differential']"
82,Why are these integrals equivalent to the Riemann Hypothesis?,Why are these integrals equivalent to the Riemann Hypothesis?,,"Riemann Hypothesis is equivalent to the integral equation $$\int_{-\infty}^{\infty} \frac{\log \mid \zeta (1/2+it)\mid }{1+4t^2} \ dt =0$$ Many other integral equations exist that are equivalent. How to show that they are equivalent ? They usually include absolute value of a function. Why is that ? I assume it came from a contour integral on the riemann sphere. The growth rate of the zeta function on the critical line also probably relates to all those integrals , not ? Another example is Establishing the exact value $$\int_{0}^{\infty}\frac{(1-12t^2)}{(1+4t^2)^3}\int_{1/2}^{\infty}\log|\zeta(\sigma+it)|~d\sigma ~dt=\frac{\pi(3-\gamma)}{32}$$ is equivalent to the Riemann Hypothesis. More examples : Riemann's Hypothesis is true if and only if $$\frac{1}{\pi}\int_0^{\infty} \log\left|\frac{\zeta(\frac{1}{2}+it)}{\zeta(\frac{1}{2})}\right|\ \frac{dt}{t^2}=\frac{\pi}{8}+\frac{\gamma}{4}+\frac{\log 8\pi}{4}-2$$ Take $a\in R$ with $\frac{1}{2}\leq a<1$ . Riemann's $\zeta$ -function has no zeros in $\Re(s)>a$ if and only if $$\frac{1}{\pi}\int_0^{\infty} \log\left|\frac{\zeta(a+it)}{\zeta(a)}\right|\ \frac{dt}{t^2}=\frac{\zeta'(a)}{2\zeta(a)}-\frac{1}{1-a}$$ And many more exist. I have no idea how to get to such conclusions or prove them. Im not even sure how to prove integrals for ""slightly easier"" cases , meaning not famous open problems but zero's of other nontrivial functions that are not on a half-plane.","Riemann Hypothesis is equivalent to the integral equation Many other integral equations exist that are equivalent. How to show that they are equivalent ? They usually include absolute value of a function. Why is that ? I assume it came from a contour integral on the riemann sphere. The growth rate of the zeta function on the critical line also probably relates to all those integrals , not ? Another example is Establishing the exact value is equivalent to the Riemann Hypothesis. More examples : Riemann's Hypothesis is true if and only if Take with . Riemann's -function has no zeros in if and only if And many more exist. I have no idea how to get to such conclusions or prove them. Im not even sure how to prove integrals for ""slightly easier"" cases , meaning not famous open problems but zero's of other nontrivial functions that are not on a half-plane.",\int_{-\infty}^{\infty} \frac{\log \mid \zeta (1/2+it)\mid }{1+4t^2} \ dt =0 \int_{0}^{\infty}\frac{(1-12t^2)}{(1+4t^2)^3}\int_{1/2}^{\infty}\log|\zeta(\sigma+it)|~d\sigma ~dt=\frac{\pi(3-\gamma)}{32} \frac{1}{\pi}\int_0^{\infty} \log\left|\frac{\zeta(\frac{1}{2}+it)}{\zeta(\frac{1}{2})}\right|\ \frac{dt}{t^2}=\frac{\pi}{8}+\frac{\gamma}{4}+\frac{\log 8\pi}{4}-2 a\in R \frac{1}{2}\leq a<1 \zeta \Re(s)>a \frac{1}{\pi}\int_0^{\infty} \log\left|\frac{\zeta(a+it)}{\zeta(a)}\right|\ \frac{dt}{t^2}=\frac{\zeta'(a)}{2\zeta(a)}-\frac{1}{1-a},"['calculus', 'definite-integrals', 'riemann-hypothesis']"
83,Mean value theorem with integrals?,Mean value theorem with integrals?,,"Here is the question: Let $f:[0, 1]\rightarrow \mathbb{R}$ be a continuous function satisfying $$\int_0^1 (1-x)f(x) \,dx = 0$$ Show that there exists $c\in (0, 1)$ such that $$\int_0^c xf(x)\,dx = cf(c)$$ I'm pretty sure that the problem wants me to use the mean value theorem of some type. So I tried to consider a function $F(t)$ that would give the form $$F'(t) = \int_0^t xf(x)\,dx -tf(t)$$ so that I would be able to say $F'(c) = 0$ for some $c\in (0, 1)$ , using Rolle's theorem. But this gave me $$F(t) = \int_0^t ((t-1)x-x^2)f(x)\,dx$$ which didn't really help me proceed any further. I have also tried setting $$F(t) = \int_0^t (t-x)f(x)\,dx$$ in hopes of using Rolle's theorem, since $F(0)=F(1)=0$ . But $F'(t)$ wasn't really the required form. I've also tried other different forms so that I could apply mean value theorem for integrals, or Cauchy's mean value theorem. But I couldn't derive the correct form to solve the problem. Maybe I have missed something? Or can someone provide me a different approach to this problem? Thanks in advance.","Here is the question: Let be a continuous function satisfying Show that there exists such that I'm pretty sure that the problem wants me to use the mean value theorem of some type. So I tried to consider a function that would give the form so that I would be able to say for some , using Rolle's theorem. But this gave me which didn't really help me proceed any further. I have also tried setting in hopes of using Rolle's theorem, since . But wasn't really the required form. I've also tried other different forms so that I could apply mean value theorem for integrals, or Cauchy's mean value theorem. But I couldn't derive the correct form to solve the problem. Maybe I have missed something? Or can someone provide me a different approach to this problem? Thanks in advance.","f:[0, 1]\rightarrow \mathbb{R} \int_0^1 (1-x)f(x) \,dx = 0 c\in (0, 1) \int_0^c xf(x)\,dx = cf(c) F(t) F'(t) = \int_0^t xf(x)\,dx -tf(t) F'(c) = 0 c\in (0, 1) F(t) = \int_0^t ((t-1)x-x^2)f(x)\,dx F(t) = \int_0^t (t-x)f(x)\,dx F(0)=F(1)=0 F'(t)",['calculus']
84,The gradient of a function has constant Euclidean length $1$,The gradient of a function has constant Euclidean length,1,"Consider a function $f : \mathbb R^{2} \to \mathbb R$ that is defined on every point and is differentiable. Then it has a gradient $\nabla f$ . Now, suppose that $|\nabla f(x,y)| = 1$ for all $x,y \in \mathbb R^{2}$ . Then must the function be a linear function (and hence $\nabla f$ is constant)? That was the question. So I was thinking that the answer should be no, but coming up with an example is hard. We can draw the field lines for the gradient, and it should be orthogonal to the level curves. And since the gradient is of length 1, it seem intuitive that for any level curves $f(x,y)=c$ and $f(x,y)=d$ if you draw any field lines between them the length along the field line connecting these curve should be $|d-c|$ . So an easy idea here is to take concentric circle to be level curve, but this one can't be made into a function differentiable at the center of these circles. Or perhaps I should start with field lines instead? Start with a family of field lines, cut a single level curve through it, declare that to be $0$ , then going along each field lines and find the value of the function depending on the length. I could go with a family of parabola because that's the only thing that perhaps the length can be computed. But it's going to be a mess, so I don't know if I should keep going with this. So is there a simpler way? Or am I heading in a completely wrong direction here? EDIT: found an older question that answered this: $|\nabla f (x)| =1$ implies $f$ linear? . Thanks to the user below for providing me with the technical phrase to search for.","Consider a function that is defined on every point and is differentiable. Then it has a gradient . Now, suppose that for all . Then must the function be a linear function (and hence is constant)? That was the question. So I was thinking that the answer should be no, but coming up with an example is hard. We can draw the field lines for the gradient, and it should be orthogonal to the level curves. And since the gradient is of length 1, it seem intuitive that for any level curves and if you draw any field lines between them the length along the field line connecting these curve should be . So an easy idea here is to take concentric circle to be level curve, but this one can't be made into a function differentiable at the center of these circles. Or perhaps I should start with field lines instead? Start with a family of field lines, cut a single level curve through it, declare that to be , then going along each field lines and find the value of the function depending on the length. I could go with a family of parabola because that's the only thing that perhaps the length can be computed. But it's going to be a mess, so I don't know if I should keep going with this. So is there a simpler way? Or am I heading in a completely wrong direction here? EDIT: found an older question that answered this: $|\nabla f (x)| =1$ implies $f$ linear? . Thanks to the user below for providing me with the technical phrase to search for.","f : \mathbb R^{2} \to \mathbb R \nabla f |\nabla f(x,y)| = 1 x,y \in \mathbb R^{2} \nabla f f(x,y)=c f(x,y)=d |d-c| 0","['calculus', 'multivariable-calculus', 'vector-fields']"
85,"Complete this table of formulas for $_2F_1\big(a,b;c;u) =v $ with algebraic numbers $u,v$ and $a+b=c$?",Complete this table of formulas for  with algebraic numbers  and ?,"_2F_1\big(a,b;c;u) =v  u,v a+b=c","( This extends this post .) Given fixed rationals $a,b,c,$ the problem of determining, $$_2F_1\big(a,b;c;u) =v $$ such that both $u,v$ are algebraic numbers may be solved by appealing to modular functions/forms like the j-function $j(\tau)$ and Dedekind eta function $\eta(\tau)$ . In the tables below, I derived the formulas empirically using data from Zucker and Joyce in ""Special values of the hypergeometric series II, III"" . However, I'm missing five examples and their formulas. I. Type $a+b=c=\color{blue}{\tfrac12}$ . $$\begin{aligned} &\,_2F_1\big(\tfrac14,\tfrac14;\tfrac12;(1-2\alpha_1)^2\big),\quad \frac1{\alpha_1}-1=\frac1{16}\Big(\tfrac{\eta(\tau/4)}{\eta(\tau)}\Big)^8,\quad \tau_1=N\sqrt{-4}\\[2mm] &\,_2F_1\big(\tfrac16,\tfrac13;\tfrac12;(1-2\alpha_2)^2\big),\quad \frac1{\alpha_2}-1=\frac1{27}\Big(\tfrac{\eta(\tau/3)}{\eta(\tau)}\Big)^{12},\quad \tau_2=N\sqrt{-3}\\[2mm] &\,_2F_1\big(\tfrac18,\tfrac38;\tfrac12;(1-2\alpha_3)^2\big),\quad \frac1{\alpha_3}-1=\frac1{64}\Big(\tfrac{\eta(\tau/2)}{\eta(\tau)}\Big)^{24},\quad \tau_3=N\sqrt{-2}\\[2mm] &\,_2F_1\big(\tfrac1{12},\tfrac5{12};\tfrac12;(1-2\alpha_4)^2\big),\quad \frac{1}{\alpha_4(1-\alpha_4)}=\frac1{432}\,j(\tau),\quad\tau_4=N\sqrt{-1}\end{aligned}$$ though the argument $z_4$ of the fourth can be found more simply as $z_4 = (1-2\alpha_4)^2 = \frac{j(\tau)-1728}{j(\tau)}$ . Examples: $$_2F_1\left(\tfrac14,\tfrac14;\tfrac12;\,\tfrac{(3-6\sqrt2)^2}{(1+\sqrt2)^4}\right)=\tfrac{3}{4\sqrt2}(1+\sqrt2)$$ $$_2F_1\left(\tfrac16,\tfrac13;\tfrac12;\,\tfrac{25}{27}\right)=\tfrac{3\sqrt3}{4}$$ $$_2F_1\left(\tfrac18,\tfrac38;\tfrac12;\tfrac{2400}{2401}\right)=\tfrac{2\sqrt7}{3}$$ $$_2F_1\left(\tfrac1{12},\tfrac5{12};\tfrac12;\tfrac{1323}{1331}\right)=\tfrac{3\,\sqrt[4]{11}}{4}$$ using $\tau_1=2\sqrt{-4},\;\tau_2=2\sqrt{-3},\;\tau_3=3\sqrt{-2},\;\tau_4=2\sqrt{-1}$ , respectively. And so on for any integer $N>1$ . II. Type $a+b=c=\color{blue}{\tfrac23}$ . $$\begin{aligned} &\,_2F_1\big(\tfrac14,\tfrac5{12};\tfrac23;(1-2\beta_1)^2\big),\quad \color{red}{\beta_1 =\,?} \\[2mm] &\,_2F_1\big(\tfrac16,\tfrac12;\tfrac23;(1-2\beta_2)^2\big),\quad \frac{1}{\beta_2}-1=\sqrt{\frac{-1}{1728}\big(2k+\sqrt{4k^2-1728^2}\big)}\\[2mm] &\,_2F_1\big(\tfrac18,\tfrac{13}{24};\tfrac23;(1-2\beta_3)^2\big),\quad \color{red}{\beta_3 =\,?} \\[2mm] &\,_2F_1\big(\tfrac1{12},\tfrac7{12};\tfrac23;(1-2\beta_4)^2\big),\quad \frac{1}{\beta_4}-1=\frac{-1}{1728}\big(2k+\sqrt{4k^2-1728^2}\big)\end{aligned}$$ and where $k=j(\tau)-864$ . Examples: Use $\tau = \frac{1+N\sqrt{-3}}2$ , like $\tau = \frac{1+3\sqrt{-3}}2$ , $$_2F_1\big(\tfrac16,\tfrac12;\tfrac23;\tfrac{125}{128}\big) =\tfrac43\,2^{1/6}$$ $$_2F_1\big(\tfrac1{12},\tfrac7{12};\tfrac23;\tfrac{64000}{64009}\big) =\tfrac23\,253^{1/6}$$ III. Type $a+b=c=\color{blue}{\tfrac34}$ . $$\begin{aligned} &\,_2F_1\big(\tfrac14,\tfrac12;\tfrac34;(1-2\gamma_1)^2\big), \quad\frac1{\gamma_1}-1=\sqrt{\frac1{64}\Big(\tfrac{\sqrt2\,\eta(2\tau)}{\zeta_{48}\,\eta(\tau)}\Big)^{24}}\\[2mm] &\,_2F_1\big(\tfrac16,\tfrac7{12};\tfrac34;(1-2\gamma_2)^2\big),\quad\color{red}{\gamma_2 =\,?} \\[2mm] &\,_2F_1\big(\tfrac18,\tfrac58;\tfrac34;(1-2\gamma_3)^2\big),\quad\frac1{\gamma_3}-1=\frac1{64}\Big(\tfrac{\sqrt2\,\eta(2\tau)}{\zeta_{48}\,\eta(\tau)}\Big)^{24}\\[2mm] &\,_2F_1\big(\tfrac1{12},\tfrac23;\tfrac34;(1-2\gamma_4)^2\big),\quad\color{red}{\gamma_4 =\,?} \end{aligned}$$ where $\zeta_{48} = e^{2\pi i/48}$ . Examples: Use $\tau = \frac{1+N\sqrt{-1}}2$ , like $\tau = \frac{1+5\sqrt{-1}}2$ , $$_2F_1\big(\tfrac14,\tfrac12;\tfrac34;\tfrac{80}{81}\big)=\tfrac95$$ $$_2F_1\big(\tfrac18,\tfrac58;\tfrac34;\tfrac{25920}{25921}\big)=\tfrac35\,161^{1/4}$$ IV. Type $a+b=c=\color{blue}{\tfrac56}$ . $$\begin{aligned} &\,_2F_1\big(\tfrac12,\tfrac13;\tfrac56;(1-2\delta_1)^2\big),\quad\;\delta_1 = \delta_2\\[2mm] &\,_2F_1\big(\tfrac13,\tfrac12;\tfrac56;(1-2\delta_2)^2\big),\quad\;\frac1{\delta_2}-1=\sqrt{\frac1{27}\left(\tfrac{\eta\big(\frac{\tau+1}{3}\big)}{\eta(\tau)}\right)^{12}}\\[2mm] &\,_2F_1\big(\tfrac14,\tfrac7{12};\tfrac56;(1-2\delta_3)^2\big),\quad\color{red}{\delta_3 =\,?} \\[2mm] &\,_2F_1\big(\tfrac16,\tfrac23;\tfrac56;(1-2\delta_4)^2\big),\quad\;\frac1{\delta_4}-1=\frac1{27}\left(\tfrac{\eta\big(\frac{\tau+1}{3}\big)}{\eta(\tau)}\right)^{12} \end{aligned}$$ Note: Of course, $_2F_1\big(\tfrac12,\tfrac13;\tfrac56;z\big) =\,_2F_1\big(\tfrac13,\tfrac12;\tfrac56;z\big)\,$ so the first form is superfluous. Examples: Use $\tau = \frac{1+N\sqrt{-3}}2$ , like $\tau = \frac{1+5\sqrt{-3}}2$ , $$\begin{aligned} &\,_2F_1\big(\tfrac13,\tfrac12;\tfrac56;\tfrac45\big)=\;\tfrac35\sqrt5 \\[2mm] &\,_2F_1\big(\tfrac16,\tfrac23;\tfrac56;\tfrac{80}{81}\big)=\tfrac35\,(9\sqrt5)^{1/3}\end{aligned}$$ with the last one discussed in this post . Q: How do we find the five missing formulas (?) for the variables in red?","( This extends this post .) Given fixed rationals the problem of determining, such that both are algebraic numbers may be solved by appealing to modular functions/forms like the j-function and Dedekind eta function . In the tables below, I derived the formulas empirically using data from Zucker and Joyce in ""Special values of the hypergeometric series II, III"" . However, I'm missing five examples and their formulas. I. Type . though the argument of the fourth can be found more simply as . Examples: using , respectively. And so on for any integer . II. Type . and where . Examples: Use , like , III. Type . where . Examples: Use , like , IV. Type . Note: Of course, so the first form is superfluous. Examples: Use , like , with the last one discussed in this post . Q: How do we find the five missing formulas (?) for the variables in red?","a,b,c, _2F_1\big(a,b;c;u) =v  u,v j(\tau) \eta(\tau) a+b=c=\color{blue}{\tfrac12} \begin{aligned}
&\,_2F_1\big(\tfrac14,\tfrac14;\tfrac12;(1-2\alpha_1)^2\big),\quad \frac1{\alpha_1}-1=\frac1{16}\Big(\tfrac{\eta(\tau/4)}{\eta(\tau)}\Big)^8,\quad \tau_1=N\sqrt{-4}\\[2mm]
&\,_2F_1\big(\tfrac16,\tfrac13;\tfrac12;(1-2\alpha_2)^2\big),\quad \frac1{\alpha_2}-1=\frac1{27}\Big(\tfrac{\eta(\tau/3)}{\eta(\tau)}\Big)^{12},\quad \tau_2=N\sqrt{-3}\\[2mm]
&\,_2F_1\big(\tfrac18,\tfrac38;\tfrac12;(1-2\alpha_3)^2\big),\quad \frac1{\alpha_3}-1=\frac1{64}\Big(\tfrac{\eta(\tau/2)}{\eta(\tau)}\Big)^{24},\quad \tau_3=N\sqrt{-2}\\[2mm]
&\,_2F_1\big(\tfrac1{12},\tfrac5{12};\tfrac12;(1-2\alpha_4)^2\big),\quad \frac{1}{\alpha_4(1-\alpha_4)}=\frac1{432}\,j(\tau),\quad\tau_4=N\sqrt{-1}\end{aligned} z_4 z_4 = (1-2\alpha_4)^2 = \frac{j(\tau)-1728}{j(\tau)} _2F_1\left(\tfrac14,\tfrac14;\tfrac12;\,\tfrac{(3-6\sqrt2)^2}{(1+\sqrt2)^4}\right)=\tfrac{3}{4\sqrt2}(1+\sqrt2) _2F_1\left(\tfrac16,\tfrac13;\tfrac12;\,\tfrac{25}{27}\right)=\tfrac{3\sqrt3}{4} _2F_1\left(\tfrac18,\tfrac38;\tfrac12;\tfrac{2400}{2401}\right)=\tfrac{2\sqrt7}{3} _2F_1\left(\tfrac1{12},\tfrac5{12};\tfrac12;\tfrac{1323}{1331}\right)=\tfrac{3\,\sqrt[4]{11}}{4} \tau_1=2\sqrt{-4},\;\tau_2=2\sqrt{-3},\;\tau_3=3\sqrt{-2},\;\tau_4=2\sqrt{-1} N>1 a+b=c=\color{blue}{\tfrac23} \begin{aligned}
&\,_2F_1\big(\tfrac14,\tfrac5{12};\tfrac23;(1-2\beta_1)^2\big),\quad \color{red}{\beta_1 =\,?} \\[2mm]
&\,_2F_1\big(\tfrac16,\tfrac12;\tfrac23;(1-2\beta_2)^2\big),\quad \frac{1}{\beta_2}-1=\sqrt{\frac{-1}{1728}\big(2k+\sqrt{4k^2-1728^2}\big)}\\[2mm]
&\,_2F_1\big(\tfrac18,\tfrac{13}{24};\tfrac23;(1-2\beta_3)^2\big),\quad \color{red}{\beta_3 =\,?} \\[2mm]
&\,_2F_1\big(\tfrac1{12},\tfrac7{12};\tfrac23;(1-2\beta_4)^2\big),\quad \frac{1}{\beta_4}-1=\frac{-1}{1728}\big(2k+\sqrt{4k^2-1728^2}\big)\end{aligned} k=j(\tau)-864 \tau = \frac{1+N\sqrt{-3}}2 \tau = \frac{1+3\sqrt{-3}}2 _2F_1\big(\tfrac16,\tfrac12;\tfrac23;\tfrac{125}{128}\big) =\tfrac43\,2^{1/6} _2F_1\big(\tfrac1{12},\tfrac7{12};\tfrac23;\tfrac{64000}{64009}\big) =\tfrac23\,253^{1/6} a+b=c=\color{blue}{\tfrac34} \begin{aligned}
&\,_2F_1\big(\tfrac14,\tfrac12;\tfrac34;(1-2\gamma_1)^2\big), \quad\frac1{\gamma_1}-1=\sqrt{\frac1{64}\Big(\tfrac{\sqrt2\,\eta(2\tau)}{\zeta_{48}\,\eta(\tau)}\Big)^{24}}\\[2mm]
&\,_2F_1\big(\tfrac16,\tfrac7{12};\tfrac34;(1-2\gamma_2)^2\big),\quad\color{red}{\gamma_2 =\,?} \\[2mm]
&\,_2F_1\big(\tfrac18,\tfrac58;\tfrac34;(1-2\gamma_3)^2\big),\quad\frac1{\gamma_3}-1=\frac1{64}\Big(\tfrac{\sqrt2\,\eta(2\tau)}{\zeta_{48}\,\eta(\tau)}\Big)^{24}\\[2mm]
&\,_2F_1\big(\tfrac1{12},\tfrac23;\tfrac34;(1-2\gamma_4)^2\big),\quad\color{red}{\gamma_4 =\,?}
\end{aligned} \zeta_{48} = e^{2\pi i/48} \tau = \frac{1+N\sqrt{-1}}2 \tau = \frac{1+5\sqrt{-1}}2 _2F_1\big(\tfrac14,\tfrac12;\tfrac34;\tfrac{80}{81}\big)=\tfrac95 _2F_1\big(\tfrac18,\tfrac58;\tfrac34;\tfrac{25920}{25921}\big)=\tfrac35\,161^{1/4} a+b=c=\color{blue}{\tfrac56} \begin{aligned}
&\,_2F_1\big(\tfrac12,\tfrac13;\tfrac56;(1-2\delta_1)^2\big),\quad\;\delta_1 = \delta_2\\[2mm]
&\,_2F_1\big(\tfrac13,\tfrac12;\tfrac56;(1-2\delta_2)^2\big),\quad\;\frac1{\delta_2}-1=\sqrt{\frac1{27}\left(\tfrac{\eta\big(\frac{\tau+1}{3}\big)}{\eta(\tau)}\right)^{12}}\\[2mm]
&\,_2F_1\big(\tfrac14,\tfrac7{12};\tfrac56;(1-2\delta_3)^2\big),\quad\color{red}{\delta_3 =\,?} \\[2mm]
&\,_2F_1\big(\tfrac16,\tfrac23;\tfrac56;(1-2\delta_4)^2\big),\quad\;\frac1{\delta_4}-1=\frac1{27}\left(\tfrac{\eta\big(\frac{\tau+1}{3}\big)}{\eta(\tau)}\right)^{12}
\end{aligned} _2F_1\big(\tfrac12,\tfrac13;\tfrac56;z\big) =\,_2F_1\big(\tfrac13,\tfrac12;\tfrac56;z\big)\, \tau = \frac{1+N\sqrt{-3}}2 \tau = \frac{1+5\sqrt{-3}}2 \begin{aligned}
&\,_2F_1\big(\tfrac13,\tfrac12;\tfrac56;\tfrac45\big)=\;\tfrac35\sqrt5 \\[2mm]
&\,_2F_1\big(\tfrac16,\tfrac23;\tfrac56;\tfrac{80}{81}\big)=\tfrac35\,(9\sqrt5)^{1/3}\end{aligned}","['calculus', 'radicals', 'modular-forms', 'hypergeometric-function', 'conjectures']"
86,Is $\sqrt{\left(\operatorname{Si}(x)-\frac\pi2\right)^2+\operatorname{Ci}(x)^2}$ completely monotone?,Is  completely monotone?,\sqrt{\left(\operatorname{Si}(x)-\frac\pi2\right)^2+\operatorname{Ci}(x)^2},"Recall the definitions of the sine and cosine integrals : $$\operatorname{Si}(x)=\int_0^x\frac{\sin t}t dt,\quad\operatorname{Ci}(x)=-\int_x^\infty\frac{\cos t}t dt.$$ Both functions are oscillating, with a countably infinite number of minima and maxima. Note that $$\lim_{x\to\infty}\operatorname{Si}(x)=\frac\pi2,\quad\lim_{x\to\infty}\operatorname{Ci}(x)=0.$$ Consider the following function: $$f(x)=\sqrt{\left(\operatorname{Si}(x)-\frac\pi2\right)^2+\operatorname{Ci}(x)^2}.$$ It appears that the function $f(x)$ and all its derivatives are monotonic  for $x>0$. Specifically, the function itself and all its derivatives of an even order are strictly decreasing, and all its derivative of an odd order are strictly increasing. Is it actually true? If so, then how can we prove it?","Recall the definitions of the sine and cosine integrals : $$\operatorname{Si}(x)=\int_0^x\frac{\sin t}t dt,\quad\operatorname{Ci}(x)=-\int_x^\infty\frac{\cos t}t dt.$$ Both functions are oscillating, with a countably infinite number of minima and maxima. Note that $$\lim_{x\to\infty}\operatorname{Si}(x)=\frac\pi2,\quad\lim_{x\to\infty}\operatorname{Ci}(x)=0.$$ Consider the following function: $$f(x)=\sqrt{\left(\operatorname{Si}(x)-\frac\pi2\right)^2+\operatorname{Ci}(x)^2}.$$ It appears that the function $f(x)$ and all its derivatives are monotonic  for $x>0$. Specifically, the function itself and all its derivatives of an even order are strictly decreasing, and all its derivative of an odd order are strictly increasing. Is it actually true? If so, then how can we prove it?",,"['calculus', 'trigonometry', 'derivatives', 'definite-integrals', 'special-functions']"
87,An Additional Rule for Calculus,An Additional Rule for Calculus,,"Background The rules for differentiating elementary functions (arithmetic, exponential, trigonometric, etc.) together with the chain rule for differentiating compositions of functions are often considered the only basic inference tools needed to explicitly compute the derivatives of typical functions thrown at a beginning calculus student. Yet, an easy counterexample is $f(x) = x^x$ , exploiting the fact that exponentiation is really a function of two variables. Then implicit / logarithmic differentiation techniques are typically introduced. I had been curious in my calculus days about an explicit method to compute the derivative of $f$ ; what I came up with I haphazardly dubbed the independence trick : group all appearances of the independent variable into several partitions. Then, the derivative of the function is a sum, each term being a ""partial"" derivative of the function with respect to one partition, treating each instance outside of the partition as a constant. Rigorously, this is an easy consequence of the multivariate chain rule: $$ \left.\frac{d}{dt} f(x_1,\ldots,x_n)\right|_{(x_1,\ldots,x_n) = (x,\ldots,x)} = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x,\ldots,x) \frac{\partial x_i}{\partial t}~~, $$ yielding the mentioned rule because each $\partial x_i / \partial t = 1$ , as the variables are identical. There are a few results nontrivial to a student without multivariate calculus: Directly, $\frac{d}{dx} x^x = x x^{x-1} + x^x \ln x = x^x (\ln x + 1)$ . A one-line proof extending the Leibniz integral rule to its general form with variable limits. Questions Are there other nice applications for the ""independence trick"" in single-variable calculus? Does anybody know if this method has been written / used / taught specifically as a trick for undergrads, and if so, where?","Background The rules for differentiating elementary functions (arithmetic, exponential, trigonometric, etc.) together with the chain rule for differentiating compositions of functions are often considered the only basic inference tools needed to explicitly compute the derivatives of typical functions thrown at a beginning calculus student. Yet, an easy counterexample is , exploiting the fact that exponentiation is really a function of two variables. Then implicit / logarithmic differentiation techniques are typically introduced. I had been curious in my calculus days about an explicit method to compute the derivative of ; what I came up with I haphazardly dubbed the independence trick : group all appearances of the independent variable into several partitions. Then, the derivative of the function is a sum, each term being a ""partial"" derivative of the function with respect to one partition, treating each instance outside of the partition as a constant. Rigorously, this is an easy consequence of the multivariate chain rule: yielding the mentioned rule because each , as the variables are identical. There are a few results nontrivial to a student without multivariate calculus: Directly, . A one-line proof extending the Leibniz integral rule to its general form with variable limits. Questions Are there other nice applications for the ""independence trick"" in single-variable calculus? Does anybody know if this method has been written / used / taught specifically as a trick for undergrads, and if so, where?","f(x) = x^x f  \left.\frac{d}{dt} f(x_1,\ldots,x_n)\right|_{(x_1,\ldots,x_n) = (x,\ldots,x)} = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x,\ldots,x) \frac{\partial x_i}{\partial t}~~,  \partial x_i / \partial t = 1 \frac{d}{dx} x^x = x x^{x-1} + x^x \ln x = x^x (\ln x + 1)","['calculus', 'reference-request']"
88,How to compute product integrals?,How to compute product integrals?,,"From the wikipedia article about product integrals I can see that if our function is scalar, then to compute type I product integral we can just take exponential of a usual integral: $$\prod_a^b f(x)^{dx}=\exp\left(\int_a^b\ln f(x)dx\right).$$ This works if $f(a)\cdot f(b)=f(b)\cdot f(a)$, i.e. product of $f$'s is commutative. But what if $f$ is e.g. matrix-valued or quaternion-valued? We then no longer can as easily take logarithm of the product, find the sum and exponentiate back. So, we have to use some techniques of computing product integrals directly, without resorting to usual integrals. Are there any such techniques known — like analogs of integration by parts etc.? Are there any tables of product integrals of standard functions? Or maybe there are some nice tricks to work around the uncommutativity of $f$?","From the wikipedia article about product integrals I can see that if our function is scalar, then to compute type I product integral we can just take exponential of a usual integral: $$\prod_a^b f(x)^{dx}=\exp\left(\int_a^b\ln f(x)dx\right).$$ This works if $f(a)\cdot f(b)=f(b)\cdot f(a)$, i.e. product of $f$'s is commutative. But what if $f$ is e.g. matrix-valued or quaternion-valued? We then no longer can as easily take logarithm of the product, find the sum and exponentiate back. So, we have to use some techniques of computing product integrals directly, without resorting to usual integrals. Are there any such techniques known — like analogs of integration by parts etc.? Are there any tables of product integrals of standard functions? Or maybe there are some nice tricks to work around the uncommutativity of $f$?",,"['calculus', 'integration', 'products']"
89,Separable non-linear ODE (with radicals),Separable non-linear ODE (with radicals),,"I am trying to solve the equation $$ \frac{dy}{dt}=\sqrt{\left(\gamma-1+\frac{2\alpha\beta}{2\alpha-1}\right)e^{-2\alpha y}-\frac{2\alpha\beta}{2\alpha-1}e^{-y}+1}\tag{1} $$ $y(0) =  0$ ; $t_{0}=0$ ; $\alpha$ , $\beta$ and $\gamma$ are positive constants ( $\beta$ is also less than 1). There is also a special form for $\alpha=\frac{1}{2}$ but I will not dwell on this right now. Provided I am not mistaken, letting $x = e^{-y}$ and rearranging leads to $$ \int_{e^{-y(t)}}^{1}{\frac{dx}{x\sqrt{\left(\gamma-1+\frac{2 \alpha\beta}{2\alpha-1}\right)x^{2\alpha}-\frac{2 \alpha\beta}{2\alpha-1}x+1}}}=t\tag{2} $$ This integral is solvable for particular values of $\alpha$ (ones that allow a partial fraction decomposition of the integrand(?)) My questions are: 1) Is there a relatively compact way that may determine if the  integral $(2)$ can be expressed in closed form? 2) Can $x(t)$ be expressed in terms of relatively well-studied special functions? 3) Even if a general method for solving this equation (which I believe does not exist), is there a way of obtaining an approximate solution for $x(t)$ in closed form? I have considered taking the Taylor series of the radical, but unfortunately this method does not work very well because the integrand does contain a region of rapid change within the integration region. Any ideas on how to treat this problem will be greatly appreciated!","I am trying to solve the equation ; ; , and are positive constants ( is also less than 1). There is also a special form for but I will not dwell on this right now. Provided I am not mistaken, letting and rearranging leads to This integral is solvable for particular values of (ones that allow a partial fraction decomposition of the integrand(?)) My questions are: 1) Is there a relatively compact way that may determine if the  integral can be expressed in closed form? 2) Can be expressed in terms of relatively well-studied special functions? 3) Even if a general method for solving this equation (which I believe does not exist), is there a way of obtaining an approximate solution for in closed form? I have considered taking the Taylor series of the radical, but unfortunately this method does not work very well because the integrand does contain a region of rapid change within the integration region. Any ideas on how to treat this problem will be greatly appreciated!","
\frac{dy}{dt}=\sqrt{\left(\gamma-1+\frac{2\alpha\beta}{2\alpha-1}\right)e^{-2\alpha y}-\frac{2\alpha\beta}{2\alpha-1}e^{-y}+1}\tag{1}
 y(0) =  0 t_{0}=0 \alpha \beta \gamma \beta \alpha=\frac{1}{2} x = e^{-y} 
\int_{e^{-y(t)}}^{1}{\frac{dx}{x\sqrt{\left(\gamma-1+\frac{2 \alpha\beta}{2\alpha-1}\right)x^{2\alpha}-\frac{2 \alpha\beta}{2\alpha-1}x+1}}}=t\tag{2}
 \alpha (2) x(t) x(t)","['calculus', 'integration', 'ordinary-differential-equations', 'dynamical-systems']"
90,What makes a limit 'go away'?,What makes a limit 'go away'?,,"According to this video $$\lim\limits_{x\to\infty} \frac{11 - e^{-x}}{7} = \frac{11}{7}$$ I understand how this works, I don't understand the limit part though. I know $\lim\limits_{x\rightarrow \infty}{e^{-x}} = 0$, but what makes the limit disappear? I conclude that it disappears either because: A . There are no more variables in the equation so no point to having a limit so we just start ignoring it B . Is has been used once so it goes away. Option B seems more likely, but it also it confusing because only one term seemed to be effected by the limit.","According to this video $$\lim\limits_{x\to\infty} \frac{11 - e^{-x}}{7} = \frac{11}{7}$$ I understand how this works, I don't understand the limit part though. I know $\lim\limits_{x\rightarrow \infty}{e^{-x}} = 0$, but what makes the limit disappear? I conclude that it disappears either because: A . There are no more variables in the equation so no point to having a limit so we just start ignoring it B . Is has been used once so it goes away. Option B seems more likely, but it also it confusing because only one term seemed to be effected by the limit.",,"['calculus', 'limits']"
91,Convergence of the sequence $a_n=\int_0^1{nx^{n-1}\over 1+x}dx$ [duplicate],Convergence of the sequence  [duplicate],a_n=\int_0^1{nx^{n-1}\over 1+x}dx,"This question already has answers here : Limit of $s_n = \int\limits_0^1 \frac{nx^{n-1}}{1+x} dx$ as $n \to \infty$ (5 answers) Closed 9 years ago . How to prove the following sequence converges to $0.5$ ? $$a_n=\int_0^1{nx^{n-1}\over 1+x}dx$$ What I have tried: I calculated the integral $$a_n=1-n\left(-1\right)^n\left[\ln2-\sum_{i=1}^n {\left(-1\right)^{i+1}\over i}\right]$$ I also noticed ${1\over2}<a_n<1$ $\forall n \in \mathbb{N}$. Then I wrote a C program and verified that $a_n\to 0.5$ (I didn't know the answer before) by calculating $a_n$ upto $n=9990002$ (starting from $n=2$ and each time increasing $n$ by $10^4$). I can't think of how to prove $\{a_n\}$ is monotone decreasing, which is clear from direct calculation.","This question already has answers here : Limit of $s_n = \int\limits_0^1 \frac{nx^{n-1}}{1+x} dx$ as $n \to \infty$ (5 answers) Closed 9 years ago . How to prove the following sequence converges to $0.5$ ? $$a_n=\int_0^1{nx^{n-1}\over 1+x}dx$$ What I have tried: I calculated the integral $$a_n=1-n\left(-1\right)^n\left[\ln2-\sum_{i=1}^n {\left(-1\right)^{i+1}\over i}\right]$$ I also noticed ${1\over2}<a_n<1$ $\forall n \in \mathbb{N}$. Then I wrote a C program and verified that $a_n\to 0.5$ (I didn't know the answer before) by calculating $a_n$ upto $n=9990002$ (starting from $n=2$ and each time increasing $n$ by $10^4$). I can't think of how to prove $\{a_n\}$ is monotone decreasing, which is clear from direct calculation.",,"['calculus', 'integration', 'sequences-and-series', 'convergence-divergence', 'definite-integrals']"
92,"What is the value of the integral$\int_{0}^{+\infty} \frac{1-\cos t}{t} \, e^{-t} \, \mathrm{d}t$?",What is the value of the integral?,"\int_{0}^{+\infty} \frac{1-\cos t}{t} \, e^{-t} \, \mathrm{d}t","I have a question about evaluating $$\int_{0}^{\infty} \frac{1-\cos t}{t}  \, e^{-t} \, \mathrm{d} t$$ Since $\lim_{t \to 0} \frac{1-\cos(t)}{t}  =0$, we know that the integrand is integrable near $t=0$. But when evaluating the integral, how do you deal with the $t$ in the denominator of the integrand?","I have a question about evaluating $$\int_{0}^{\infty} \frac{1-\cos t}{t}  \, e^{-t} \, \mathrm{d} t$$ Since $\lim_{t \to 0} \frac{1-\cos(t)}{t}  =0$, we know that the integrand is integrable near $t=0$. But when evaluating the integral, how do you deal with the $t$ in the denominator of the integrand?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
93,Why don't I get $e$ when I solve $\lim_{n\to \infty}(1 + \frac{1}{n})^n$? [duplicate],Why don't I get  when I solve ? [duplicate],e \lim_{n\to \infty}(1 + \frac{1}{n})^n,"This question already has answers here : Why $\lim\limits_{n\to \infty}\left(1+\frac{1}{n}\right)^n$ doesn't evaluate to 1? (6 answers) Closed 6 years ago . If I were given $\lim_{n\to \infty}(1 + \frac{1}{n})^n$, and asked to solve, I would do so as follows: $$\lim_{n\to \infty}(1 + \frac{1}{n})^n$$ $$=(1 + \frac{1}{\infty})^\infty$$ $$=(1 + 0)^\infty$$ $$=1^\infty$$ $$=1$$ I'm aware that this limit is meant to equal to $e$, and so I ask: why don't I get $e$ when I solve $\lim_{n\to \infty}(1 + \frac{1}{n})^n$?","This question already has answers here : Why $\lim\limits_{n\to \infty}\left(1+\frac{1}{n}\right)^n$ doesn't evaluate to 1? (6 answers) Closed 6 years ago . If I were given $\lim_{n\to \infty}(1 + \frac{1}{n})^n$, and asked to solve, I would do so as follows: $$\lim_{n\to \infty}(1 + \frac{1}{n})^n$$ $$=(1 + \frac{1}{\infty})^\infty$$ $$=(1 + 0)^\infty$$ $$=1^\infty$$ $$=1$$ I'm aware that this limit is meant to equal to $e$, and so I ask: why don't I get $e$ when I solve $\lim_{n\to \infty}(1 + \frac{1}{n})^n$?",,"['calculus', 'limits', 'exponential-function', 'indeterminate-forms']"
94,Why the differentiation of $e^x$ is $e^x?$,Why the differentiation of  is,e^x e^x?,"$$\frac{d}{dx} e^x=e^x$$ Please explain simply as I haven't studied the first principle of differentiation yet, but I know the basics of differentiation.","$$\frac{d}{dx} e^x=e^x$$ Please explain simply as I haven't studied the first principle of differentiation yet, but I know the basics of differentiation.",,"['calculus', 'derivatives', 'exponential-function']"
95,"Given $a_{1}=1, \ a_{n+1}=a_{n}+\frac{1}{a_{n}}$, find $\lim \limits_{n\to\infty}\frac{a_{n}}{n}$","Given , find","a_{1}=1, \ a_{n+1}=a_{n}+\frac{1}{a_{n}} \lim \limits_{n\to\infty}\frac{a_{n}}{n}",I started by showing that $1\leq a_{n} \leq n$ (by induction) and then $\frac{1}{n}\leq \frac{a_{n}}{n} \leq 1$ which doesn't really get me anywhere. On a different path I showed that $a_{n} \to \infty$ but can't see how that helps me.,I started by showing that $1\leq a_{n} \leq n$ (by induction) and then $\frac{1}{n}\leq \frac{a_{n}}{n} \leq 1$ which doesn't really get me anywhere. On a different path I showed that $a_{n} \to \infty$ but can't see how that helps me.,,"['calculus', 'sequences-and-series']"
96,Finding $\int_{-2}^8xf(x)dx$ given $\int_{-2}^8f(x)dx$,Finding  given,\int_{-2}^8xf(x)dx \int_{-2}^8f(x)dx,"I have a continuous function $f:[-2,8]\rightarrow\mathbb{R}$ for which is true that $f(6-x)=f(x)\forall x\in[-2,8]$. Let: $$\int_{-2}^8f(x)dx=10$$ Now, I want to find the:  $$\int_{-2}^8xf(x)dx$$ I am thinking of using both the methods of u-substitution and integration by parts, but I need some help. Any ideas?","I have a continuous function $f:[-2,8]\rightarrow\mathbb{R}$ for which is true that $f(6-x)=f(x)\forall x\in[-2,8]$. Let: $$\int_{-2}^8f(x)dx=10$$ Now, I want to find the:  $$\int_{-2}^8xf(x)dx$$ I am thinking of using both the methods of u-substitution and integration by parts, but I need some help. Any ideas?",,"['calculus', 'integration', 'definite-integrals']"
97,Shortest distance between parabola and point,Shortest distance between parabola and point,,"Find the shortest distance between the parabola defined by $y^2 = 2x$ and a point $ E:= (1.5, 0)$. I can't use the distance formula because I'm missing a set of points $(x, y)$ to plug into. So, instead, I have a normal that passes through the point $E$ from the parabola. Which is the definition of the shortest distance to a point. $$y - y_1 = m(x - x_1)$$ The slope of the normal is $\frac{1}{y_1}$ by using implicit differentiation and that's where I'm stuck, because I plug the point E into it and I get $$y_1^2=x_1-1.5$$ How do I prove the shortest distance is $\sqrt{2}$?","Find the shortest distance between the parabola defined by $y^2 = 2x$ and a point $ E:= (1.5, 0)$. I can't use the distance formula because I'm missing a set of points $(x, y)$ to plug into. So, instead, I have a normal that passes through the point $E$ from the parabola. Which is the definition of the shortest distance to a point. $$y - y_1 = m(x - x_1)$$ The slope of the normal is $\frac{1}{y_1}$ by using implicit differentiation and that's where I'm stuck, because I plug the point E into it and I get $$y_1^2=x_1-1.5$$ How do I prove the shortest distance is $\sqrt{2}$?",,"['calculus', 'optimization', 'conic-sections', 'maxima-minima']"
98,Formula for computing integrals,Formula for computing integrals,,"For computing derivative of a function, we can use the definition of a derivative, i.e. $$\lim\limits_{h \to 0}\frac{f(x+h)-f(x)}{h}.$$ Is there some for computing integrals too?","For computing derivative of a function, we can use the definition of a derivative, i.e. $$\lim\limits_{h \to 0}\frac{f(x+h)-f(x)}{h}.$$ Is there some for computing integrals too?",,"['calculus', 'integration']"
99,Limit of $(\sin\circ\sin\circ\cdots\circ\sin)(x)$,Limit of,(\sin\circ\sin\circ\cdots\circ\sin)(x),I'm trying to find this limit: $$\lim_{n \to \infty} \underbrace{\sin \sin \ldots \sin }_{\text{$n$ times}}x$$ Thank you,I'm trying to find this limit: $$\lim_{n \to \infty} \underbrace{\sin \sin \ldots \sin }_{\text{$n$ times}}x$$ Thank you,,['calculus']
