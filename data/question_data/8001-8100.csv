,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why does$ \int_{0}^{\pi} \frac{dx}{\sqrt{\sin(x)}}$ converge while $\int_{0}^{\pi} \frac{dx}{\sin(x)}$ diverges?,Why does converge while  diverges?, \int_{0}^{\pi} \frac{dx}{\sqrt{\sin(x)}} \int_{0}^{\pi} \frac{dx}{\sin(x)},"Essentially for me, the question boils to down finding a convergent majorant for $$\left|\frac{1}{\sqrt{\sin(x)}}\right|$$ in the interval $(0, \pi)$ thereby proving convergence of  $$ \int_{0}^{\pi} \frac{dx}{\sqrt{\sin(x)}}. $$ I can't think of a good majorant though.","Essentially for me, the question boils to down finding a convergent majorant for $$\left|\frac{1}{\sqrt{\sin(x)}}\right|$$ in the interval $(0, \pi)$ thereby proving convergence of  $$ \int_{0}^{\pi} \frac{dx}{\sqrt{\sin(x)}}. $$ I can't think of a good majorant though.",,"['real-analysis', 'convergence-divergence', 'improper-integrals', 'trigonometric-integrals']"
1,Prove that $\sum \limits_{n=0}^{\infty} \frac{n!}{(n+1)!+(n+2)!} = \frac{3}{4}$,Prove that,\sum \limits_{n=0}^{\infty} \frac{n!}{(n+1)!+(n+2)!} = \frac{3}{4},"I was playing around with factorials on Wolfram|Alpha , when I got this amazing result : $$\sum \limits_{n=0}^{\infty} \dfrac{n!}{(n+1)!+(n+2)!} = \dfrac{3}{4}.$$ Evaluating the first few partial sums makes it obvious that the sum converges to $\approx 0.7$. But I am not able to prove this result algebraically. I tried manipulating the terms and introducing Gamma Function, but without success. Can anyone help me with this infinite sum ? Is there some well-known method of evaluating infinite sums similar to this ? Any help will be gratefully acknowledged. Thanks in advance ! :-) EDIT : I realized that $(n!)$ can be cancelled out from the fraction and the limit of the remaining fraction as $n \to \infty$ can be calculated very easily to be equal to $0.75$. Very silly of me to ask such a question !!! Anyways you can check out the comment by @Did if this ""Edit"" section does not help.","I was playing around with factorials on Wolfram|Alpha , when I got this amazing result : $$\sum \limits_{n=0}^{\infty} \dfrac{n!}{(n+1)!+(n+2)!} = \dfrac{3}{4}.$$ Evaluating the first few partial sums makes it obvious that the sum converges to $\approx 0.7$. But I am not able to prove this result algebraically. I tried manipulating the terms and introducing Gamma Function, but without success. Can anyone help me with this infinite sum ? Is there some well-known method of evaluating infinite sums similar to this ? Any help will be gratefully acknowledged. Thanks in advance ! :-) EDIT : I realized that $(n!)$ can be cancelled out from the fraction and the limit of the remaining fraction as $n \to \infty$ can be calculated very easily to be equal to $0.75$. Very silly of me to ask such a question !!! Anyways you can check out the comment by @Did if this ""Edit"" section does not help.",,"['real-analysis', 'sequences-and-series']"
2,Proving that the set of all increasing sequences is uncountable,Proving that the set of all increasing sequences is uncountable,,"Claim: Prove that the set of all increasing sequences $n_1<n_2<...<n_n<...$ is uncountable. Here's my attempt: Let $X=\{(x_n)_{n\in N};(x_n)_{n\in N}$ is increasing$\}$. Consider $f:N\rightarrow X$. We claim that no $f$ can be surjective. Denote $(u_n)_{n\in N}$ the image of $f$ at the point $u\in N$, ie, $f(u) =(u_n)_{n\in N}$ . We construct a sequence $(s_n)_{n\in N}$ such that $(s_n)_{n\in N}$$\neq f(s)$ for all $s\in N$. For, construct $(s_n)_{n\in N}$ taking $s_1=\prod_{k=1}^{\infty} f(k)_1$, $s_2=\prod_{k=1}^{\infty} f(k)_2$, ... , $s_n = \prod_{k=1}^{\infty} f(k)_n$, ... By the monoticity of multiplication, we guarantee that $(s_n)_{n\in N} \in X$. But by the way we built it, it is such that $(s_n)_{n\in N} \neq f(s)$ for all s natural. Hence, $f$ cannot be surjective and therefore X is not countable. Is it correct? And if yes, i find pretty intuitive that $(s_n)_{n\in N} \neq f(s)$ for all s natural. But can someone turn it out more rigorous and clear? EDIT: I came up with a new solution and better notation. Here it is: Let $X=\{(x_n)_{n\in N};(x_n)_{n\in N}$ is increasing$\}$. Consider $f:N\rightarrow X$. Denote $(u_n)_{n\in N}$ the image of $f$ at the point $u\in N$, ie, $f(u) =(u_n)_{n\in N}$ . We may write this sequence as $f(u)= (f(u)_1,...,f(u)_n,...)$. Also define $A_u=\{f(u)_1,...,f(u)_n,...\}$ as the set of all elements of the sequence $f(u)$. We define this set for every $u \in N$. Now we construct a new sequence $(s_n)_{n\in N}$ inductively given by: Define $f(s)_1 $= the smallest element of $A_1$. Supposing defined $f(s)_1,...,f(s)_n$, let $S_{n+1} = \{x \in A_{n+1};  x>y$, where $y$ is the smallest element of $A_{n}\}$. Then take $f(s)_{n+1}$= smallest element of $S_{n+1}$. Therefore the sequence $(s_n)_{n\in N}$ is now defined inductively, and by construction $(s_n)_{n\in N}$ is increasing. Therefore, $(s_n)_{n\in N} \in X$ and $(s_n)_{n\in N}$ is such that $(s_n)_{n\in N} \neq f(s)$, for all $s \in N$. Therefore $f$ is not surjective and hence X is not countable.","Claim: Prove that the set of all increasing sequences $n_1<n_2<...<n_n<...$ is uncountable. Here's my attempt: Let $X=\{(x_n)_{n\in N};(x_n)_{n\in N}$ is increasing$\}$. Consider $f:N\rightarrow X$. We claim that no $f$ can be surjective. Denote $(u_n)_{n\in N}$ the image of $f$ at the point $u\in N$, ie, $f(u) =(u_n)_{n\in N}$ . We construct a sequence $(s_n)_{n\in N}$ such that $(s_n)_{n\in N}$$\neq f(s)$ for all $s\in N$. For, construct $(s_n)_{n\in N}$ taking $s_1=\prod_{k=1}^{\infty} f(k)_1$, $s_2=\prod_{k=1}^{\infty} f(k)_2$, ... , $s_n = \prod_{k=1}^{\infty} f(k)_n$, ... By the monoticity of multiplication, we guarantee that $(s_n)_{n\in N} \in X$. But by the way we built it, it is such that $(s_n)_{n\in N} \neq f(s)$ for all s natural. Hence, $f$ cannot be surjective and therefore X is not countable. Is it correct? And if yes, i find pretty intuitive that $(s_n)_{n\in N} \neq f(s)$ for all s natural. But can someone turn it out more rigorous and clear? EDIT: I came up with a new solution and better notation. Here it is: Let $X=\{(x_n)_{n\in N};(x_n)_{n\in N}$ is increasing$\}$. Consider $f:N\rightarrow X$. Denote $(u_n)_{n\in N}$ the image of $f$ at the point $u\in N$, ie, $f(u) =(u_n)_{n\in N}$ . We may write this sequence as $f(u)= (f(u)_1,...,f(u)_n,...)$. Also define $A_u=\{f(u)_1,...,f(u)_n,...\}$ as the set of all elements of the sequence $f(u)$. We define this set for every $u \in N$. Now we construct a new sequence $(s_n)_{n\in N}$ inductively given by: Define $f(s)_1 $= the smallest element of $A_1$. Supposing defined $f(s)_1,...,f(s)_n$, let $S_{n+1} = \{x \in A_{n+1};  x>y$, where $y$ is the smallest element of $A_{n}\}$. Then take $f(s)_{n+1}$= smallest element of $S_{n+1}$. Therefore the sequence $(s_n)_{n\in N}$ is now defined inductively, and by construction $(s_n)_{n\in N}$ is increasing. Therefore, $(s_n)_{n\in N} \in X$ and $(s_n)_{n\in N}$ is such that $(s_n)_{n\in N} \neq f(s)$, for all $s \in N$. Therefore $f$ is not surjective and hence X is not countable.",,"['real-analysis', 'proof-verification']"
3,$\displaystyle\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n(n+1)}=2\ln 2-1$,,\displaystyle\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n(n+1)}=2\ln 2-1,"I have evaluated this sum and found that it is equal to $2\log2-1$. However, my friend found $\log2-1$. When we expanded, we got both the answers are same. So I am confused which one is correct answer. Thank you.","I have evaluated this sum and found that it is equal to $2\log2-1$. However, my friend found $\log2-1$. When we expanded, we got both the answers are same. So I am confused which one is correct answer. Thank you.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'summation']"
4,"Prob. 3, Chap. 3 in Baby Rudin: If $s_1 = \sqrt{2}$, and $s_{n+1} = \sqrt{2 + \sqrt{s_n}}$, what is the limit of this sequence?","Prob. 3, Chap. 3 in Baby Rudin: If , and , what is the limit of this sequence?",s_1 = \sqrt{2} s_{n+1} = \sqrt{2 + \sqrt{s_n}},"Here's Prob. 3, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $s_1 = \sqrt{2}$, and $$s_{n+1} = \sqrt{2 + \sqrt{s_n}} \ \ (n = 1, 2, 3, \ldots),$$ prove that $\left\{ s_n \right\}$ converges, and that $s_n < 2$ for $n = 1, 2, 3, \ldots$. My effort: We can show that $\sqrt{2} \leq s_n \leq 2$ for all $n = 1, 2, 3, \ldots$. [Am I right?] Then we can also show that $s_n < s_{n+1}$ for all $n = 1, 2, 3, \ldots$. [Am I right?] But how to calculate the exact value of the limit? Where does this sequence occur in applications?","Here's Prob. 3, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $s_1 = \sqrt{2}$, and $$s_{n+1} = \sqrt{2 + \sqrt{s_n}} \ \ (n = 1, 2, 3, \ldots),$$ prove that $\left\{ s_n \right\}$ converges, and that $s_n < 2$ for $n = 1, 2, 3, \ldots$. My effort: We can show that $\sqrt{2} \leq s_n \leq 2$ for all $n = 1, 2, 3, \ldots$. [Am I right?] Then we can also show that $s_n < s_{n+1}$ for all $n = 1, 2, 3, \ldots$. [Am I right?] But how to calculate the exact value of the limit? Where does this sequence occur in applications?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'metric-spaces']"
5,Why do sequences need to converge with respect to a norm?,Why do sequences need to converge with respect to a norm?,,"Suppose there is a sequence $x_n$ in $\mathbb{R}^n$, which converges to $x$, with respect to $\lVert\cdot\rVert_2$, say. Then we can write down vectors $x_1, x_2, x_3, ...$, and so on, and, with each $x_i$, we will be closer and closer to $x$. Now, suppose there is another sequence, $y_n$, which converges to the same point $x$, but now with respect to $\lVert\cdot\rVert_\infty$. Then we can write this sequence down as numbers $y_1, y_2, ...$, and with each $y_i$ we will be closer and closer to $x$. My point is that why do we need sequences to converge with respect to certain norms if we can just write them down as numbers, one by one, without any norms, and the numbers (or vectors) will just be closer and closer to the point of convergence? I understand that convergence with respect to a norm $\lVert\cdot\rVert$ means that $\lim\limits_{n\to\infty}\lVert x_n-x \rVert=0$. But if we can write down approaching vectors one by one, without any norms, and the vectors will be approaching a certain point, then shouldn't this mean that convergence must be satisfied with respect to any norm, and the norm should not be relevant to convergence at all?","Suppose there is a sequence $x_n$ in $\mathbb{R}^n$, which converges to $x$, with respect to $\lVert\cdot\rVert_2$, say. Then we can write down vectors $x_1, x_2, x_3, ...$, and so on, and, with each $x_i$, we will be closer and closer to $x$. Now, suppose there is another sequence, $y_n$, which converges to the same point $x$, but now with respect to $\lVert\cdot\rVert_\infty$. Then we can write this sequence down as numbers $y_1, y_2, ...$, and with each $y_i$ we will be closer and closer to $x$. My point is that why do we need sequences to converge with respect to certain norms if we can just write them down as numbers, one by one, without any norms, and the numbers (or vectors) will just be closer and closer to the point of convergence? I understand that convergence with respect to a norm $\lVert\cdot\rVert$ means that $\lim\limits_{n\to\infty}\lVert x_n-x \rVert=0$. But if we can write down approaching vectors one by one, without any norms, and the vectors will be approaching a certain point, then shouldn't this mean that convergence must be satisfied with respect to any norm, and the norm should not be relevant to convergence at all?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'normed-spaces']"
6,$\sum_\limits{n=0}^{\infty} a_n$ converges $\implies \sum_\limits{n=0}^{\infty} a_n^2$ converges [duplicate],converges  converges [duplicate],\sum_\limits{n=0}^{\infty} a_n \implies \sum_\limits{n=0}^{\infty} a_n^2,"This question already has answers here : Prove that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ converges absolutely (3 answers) Closed 7 years ago . Let $(a_n)$ be a sequence of positive terms and suppose that $\sum_\limits{n=0}^{\infty} a_n$ converges. Show that $\sum_\limits{n=0}^{\infty} a_n^2$ converges. This is in the section on the Comparison Test so that must be what I'm supposed to use.  But I don't see how.  $(a_n)^2$ might be smaller or larger than $a_n$ depending on $a_n$.  And I can't use the Comparison Test with some other series because there's no info here about how fast $\sum a_n$ converges.  Hmm.  Any hints?","This question already has answers here : Prove that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ converges absolutely (3 answers) Closed 7 years ago . Let $(a_n)$ be a sequence of positive terms and suppose that $\sum_\limits{n=0}^{\infty} a_n$ converges. Show that $\sum_\limits{n=0}^{\infty} a_n^2$ converges. This is in the section on the Comparison Test so that must be what I'm supposed to use.  But I don't see how.  $(a_n)^2$ might be smaller or larger than $a_n$ depending on $a_n$.  And I can't use the Comparison Test with some other series because there's no info here about how fast $\sum a_n$ converges.  Hmm.  Any hints?",,"['calculus', 'real-analysis', 'sequences-and-series']"
7,A proof of the fact that the Fourier transform is not surjective from $\mathcal{L}^1(\mathbb{R})$ to $C_0( \mathbb{R})$,A proof of the fact that the Fourier transform is not surjective from  to,\mathcal{L}^1(\mathbb{R}) C_0( \mathbb{R}),"Let $f_n = \mathbb 1_{[-n,n]}$ for all $n \in \mathbb{N}$ Compute explicitly $f_n \star f_1$ for all $n \in \mathbb{N}$ . Show that $f_n \star f_1$ is the Fourier transform of $g_n = \frac{  \sin{(2\pi x)} \sin{(2 \pi nx)}}{\pi ^2 x^2}$ Show that $\| g_n \|_1 \to \infty$ Deduce that the Fourier transform is not surjective from $\mathcal{L}^1(\mathbb{R})$ to $C_0( \mathbb{R})$ Here, $C_0(\mathbb{R})$ is the space of continuous functions $f : \mathbb{R} \to \mathbb{R}$ that tends to $0$ at $-\infty$ and $+\infty$ . I managed to compute 1) and proved 2). But I don't know how to prove 3). Moreover, I don't see how can 1), 2) and 3) prove 4). Can anyone help?","Let for all Compute explicitly for all . Show that is the Fourier transform of Show that Deduce that the Fourier transform is not surjective from to Here, is the space of continuous functions that tends to at and . I managed to compute 1) and proved 2). But I don't know how to prove 3). Moreover, I don't see how can 1), 2) and 3) prove 4). Can anyone help?","f_n = \mathbb 1_{[-n,n]} n \in \mathbb{N} f_n \star f_1 n \in \mathbb{N} f_n \star f_1 g_n = \frac{
 \sin{(2\pi x)} \sin{(2 \pi nx)}}{\pi ^2 x^2} \| g_n \|_1 \to \infty \mathcal{L}^1(\mathbb{R}) C_0( \mathbb{R}) C_0(\mathbb{R}) f : \mathbb{R} \to \mathbb{R} 0 -\infty +\infty","['real-analysis', 'analysis', 'fourier-analysis', 'convolution', 'fourier-transform']"
8,Finding roots of $\sin(x)=\sin(ax)$ without resorting to complex analysis,Finding roots of  without resorting to complex analysis,\sin(x)=\sin(ax),"If you were given an equation $\sin(x)=\sin(ax)$ (say $a$ is a natural number), how would you go about finding all the roots on $[0,2\pi)$ without delving into complex numbers? From a simple geometric analysis it is obvious that solving for $\pi-x=ax$ would yield 4 solutions, and $x=0$ and $x=\pi$ are another 2 obvious solutions. From complex analysis though we know that there could be many roots in this interval depending on the $a$. Is there any way to find all these roots using only techniques from real analysis?","If you were given an equation $\sin(x)=\sin(ax)$ (say $a$ is a natural number), how would you go about finding all the roots on $[0,2\pi)$ without delving into complex numbers? From a simple geometric analysis it is obvious that solving for $\pi-x=ax$ would yield 4 solutions, and $x=0$ and $x=\pi$ are another 2 obvious solutions. From complex analysis though we know that there could be many roots in this interval depending on the $a$. Is there any way to find all these roots using only techniques from real analysis?",,"['real-analysis', 'trigonometry']"
9,Computation of an iterated integral,Computation of an iterated integral,,"I want to prove $$\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty\frac{\sin(x^2+y^2)}{x^2+y^2}dxdy=\frac{\pi^2}{2}.$$ Since the function $(x,y)\mapsto\sin(x^2+y^2)/(x^2+y^2)$ is not integrable, I can't use the Theorem of Change of Variable. So, I'm trying to use residue formulae for some suitable holomorphic function to compute the inner integral, but I can't continue. Can someone suggest me a hint to solve this problem? Addendum: I may be wrong, but I suspect Theorem of Change of Variable (TCV) is not the answer. The reason is the following: the number $\pi^2/2$ is gotten if we apply polar coordinates, but TCV guarantees that if we apply any other change of variable we can get the same number, $\pi^2/2$. If this function were integrable, this invariance property would be guaranteed, but it is not the case. Thus we may have strange solutions to this integral.","I want to prove $$\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty\frac{\sin(x^2+y^2)}{x^2+y^2}dxdy=\frac{\pi^2}{2}.$$ Since the function $(x,y)\mapsto\sin(x^2+y^2)/(x^2+y^2)$ is not integrable, I can't use the Theorem of Change of Variable. So, I'm trying to use residue formulae for some suitable holomorphic function to compute the inner integral, but I can't continue. Can someone suggest me a hint to solve this problem? Addendum: I may be wrong, but I suspect Theorem of Change of Variable (TCV) is not the answer. The reason is the following: the number $\pi^2/2$ is gotten if we apply polar coordinates, but TCV guarantees that if we apply any other change of variable we can get the same number, $\pi^2/2$. If this function were integrable, this invariance property would be guaranteed, but it is not the case. Thus we may have strange solutions to this integral.",,"['real-analysis', 'complex-analysis', 'contour-integration']"
10,"How do we conclude that $f(x)=0, \forall x\in \mathbb{R}$ ?",How do we conclude that  ?,"f(x)=0, \forall x\in \mathbb{R}","Suppose that $f:\mathbb{R}\rightarrow \mathbb{R}$ is a periodic function such that $\displaystyle{\lim_{x\rightarrow +\infty}f(x)=0}$. I want to show that $f(x)=0$ for all $x\in \mathbb{R}$. $$$$ Let $T$ be the period of $f$, then $f(x)=f(x+T)$. Therefore, we have that $$0=\lim_{x\rightarrow +\infty}f(x)=\lim_{x\rightarrow +\infty}f(x+T)=\lim_{x\rightarrow +\infty}f(x+2T)=\dots =\lim_{x\rightarrow +\infty}f(x+nT), \ \forall n\in \mathbb{Z}$$ So, $$|f(x)|=|f(x+T)|=|f(x+2T)|=\dots =|f(x+nT)|\leq \epsilon$$ $$$$ But how exactly do we conclude that $f(x)=0, \forall x\in \mathbb{R}$ ?","Suppose that $f:\mathbb{R}\rightarrow \mathbb{R}$ is a periodic function such that $\displaystyle{\lim_{x\rightarrow +\infty}f(x)=0}$. I want to show that $f(x)=0$ for all $x\in \mathbb{R}$. $$$$ Let $T$ be the period of $f$, then $f(x)=f(x+T)$. Therefore, we have that $$0=\lim_{x\rightarrow +\infty}f(x)=\lim_{x\rightarrow +\infty}f(x+T)=\lim_{x\rightarrow +\infty}f(x+2T)=\dots =\lim_{x\rightarrow +\infty}f(x+nT), \ \forall n\in \mathbb{Z}$$ So, $$|f(x)|=|f(x+T)|=|f(x+2T)|=\dots =|f(x+nT)|\leq \epsilon$$ $$$$ But how exactly do we conclude that $f(x)=0, \forall x\in \mathbb{R}$ ?",,"['real-analysis', 'analysis', 'limits', 'periodic-functions']"
11,Give another proof of intermediate value theorem,Give another proof of intermediate value theorem,,"Give another proof of intermediate value theorem by completing the   following argument: If $f$ is a continuous real-valued function on the   closed interval $[a,b]$ in $\mathbb{R}$ and $f(a)<\gamma < f(b)$ then   $$f(\sup \{x\in [a,b]: f(x)\le \gamma\})=\gamma \mbox{.}$$ Denote $S=\{x\in [a,b]: f(x)\le \gamma\}$. First note that $\sup S$ exists since $a\in S$ and $S$ is bounded from above by $b$. Since $S$ is closed (because of the continuity of $f$) it must be the case that $s=\sup S \in S$. If $f(s)<\gamma$ then there would exist some small $\epsilon>0$ such that $f(s + \epsilon)<\gamma$ and that would mean $s$ is not the supremum of $S$, contradiction. Thus $f(s)=\gamma$. Is my reasoning correct?","Give another proof of intermediate value theorem by completing the   following argument: If $f$ is a continuous real-valued function on the   closed interval $[a,b]$ in $\mathbb{R}$ and $f(a)<\gamma < f(b)$ then   $$f(\sup \{x\in [a,b]: f(x)\le \gamma\})=\gamma \mbox{.}$$ Denote $S=\{x\in [a,b]: f(x)\le \gamma\}$. First note that $\sup S$ exists since $a\in S$ and $S$ is bounded from above by $b$. Since $S$ is closed (because of the continuity of $f$) it must be the case that $s=\sup S \in S$. If $f(s)<\gamma$ then there would exist some small $\epsilon>0$ such that $f(s + \epsilon)<\gamma$ and that would mean $s$ is not the supremum of $S$, contradiction. Thus $f(s)=\gamma$. Is my reasoning correct?",,"['calculus', 'real-analysis', 'proof-verification', 'continuity']"
12,Are there functions that are Holder continuous but whose variation is unbounded?,Are there functions that are Holder continuous but whose variation is unbounded?,,"I have recently been introduced to the concept of Holder condition and I was told that there are functions that are Holder continuous but whose variation in unbounded. Can anyone present an example, with explanation of both unboundedness of variation and Holder condition? If possible some example that's not too complicated and doesn't require advanced math - Let's say, I looked up the Weierstrass function and that's quite out of my reach at the moment.","I have recently been introduced to the concept of Holder condition and I was told that there are functions that are Holder continuous but whose variation in unbounded. Can anyone present an example, with explanation of both unboundedness of variation and Holder condition? If possible some example that's not too complicated and doesn't require advanced math - Let's say, I looked up the Weierstrass function and that's quite out of my reach at the moment.",,"['real-analysis', 'examples-counterexamples', 'bounded-variation', 'holder-spaces']"
13,"How can I prove the integral $ \int_{1}^{x} \frac{1}{t} \, dt $ is $\ln x $ with this approach?",How can I prove the integral  is  with this approach?," \int_{1}^{x} \frac{1}{t} \, dt  \ln x ","I have been trying to find a proof for the integral of $ \int_1^x \dfrac{1}{t} \,dt $ being equal to $ \ln \left|x \right| $ from an approach similar to that of the squeeze theorem. Is it possible to calculate the area under the curve $ f(x) = \dfrac{1}{x} $ as in the picture shown below? You may notice that both sums of the areas should converge to $\ln(x)$ as the base of the rectangles gets smaller and smaller. We approach from above and from below to get a limiting argument of the form: Area from below the curve as in the second graph $ \leq \int_a^b \dfrac{1}{x} \,dx \leq$ Area from above the curve as in the first graph The limiting argument would be to keep calculating and adding the areas of the rectangles which bases get smaller and thus showing that this amount is $\ln(x)$. How could I proceed in this way?","I have been trying to find a proof for the integral of $ \int_1^x \dfrac{1}{t} \,dt $ being equal to $ \ln \left|x \right| $ from an approach similar to that of the squeeze theorem. Is it possible to calculate the area under the curve $ f(x) = \dfrac{1}{x} $ as in the picture shown below? You may notice that both sums of the areas should converge to $\ln(x)$ as the base of the rectangles gets smaller and smaller. We approach from above and from below to get a limiting argument of the form: Area from below the curve as in the second graph $ \leq \int_a^b \dfrac{1}{x} \,dx \leq$ Area from above the curve as in the first graph The limiting argument would be to keep calculating and adding the areas of the rectangles which bases get smaller and thus showing that this amount is $\ln(x)$. How could I proceed in this way?",,"['calculus', 'real-analysis', 'integration']"
14,"Let $X \subseteq \mathbb R$ and $X$ has same cardinality as $\mathbb R$ , does there always exist a continuous surjection from $\mathbb R$ onto $X$ ?","Let  and  has same cardinality as  , does there always exist a continuous surjection from  onto  ?",X \subseteq \mathbb R X \mathbb R \mathbb R X,"Let $X \subseteq \mathbb R$ and $X$ has same cardinality as $\mathbb R$ , does there always exist a continuous surjection from $\mathbb R$ onto $X$ ? ( I know that there need not always be a continuous surjection from $X$ onto $\mathbb R$ , for example when $X$ is closed bounded interval )","Let $X \subseteq \mathbb R$ and $X$ has same cardinality as $\mathbb R$ , does there always exist a continuous surjection from $\mathbb R$ onto $X$ ? ( I know that there need not always be a continuous surjection from $X$ onto $\mathbb R$ , for example when $X$ is closed bounded interval )",,['real-analysis']
15,How to prove $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0 $.,How to prove ., \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0 ,"I am trying to prove that $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0  $. My attempt is as follows. Since for each $n\in \mathbb{N}$, $n>0$ by arithmetic geometric inequality $$ \sqrt[n]{n}=\sqrt[n]{n.1...1}=\le \dfrac{n+1+...+1}{n}=\dfrac{n+n-1}{n}=2-\dfrac{1}{n} $$. Hence $$0<\sqrt[n]{n}-1\le 1-\dfrac{1}{n}<1.$$ So $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0  $. Is this correct ? If not how to show that $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0  $ ? Please help me. Thanks.","I am trying to prove that $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0  $. My attempt is as follows. Since for each $n\in \mathbb{N}$, $n>0$ by arithmetic geometric inequality $$ \sqrt[n]{n}=\sqrt[n]{n.1...1}=\le \dfrac{n+1+...+1}{n}=\dfrac{n+n-1}{n}=2-\dfrac{1}{n} $$. Hence $$0<\sqrt[n]{n}-1\le 1-\dfrac{1}{n}<1.$$ So $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0  $. Is this correct ? If not how to show that $ \lim_{n\rightarrow \infty}\left( \sqrt[n]{n}-1 \right)^{n}=0  $ ? Please help me. Thanks.",,"['real-analysis', 'sequences-and-series', 'analysis']"
16,How can one check if a Cauchy-sequence converges in the rationals?,How can one check if a Cauchy-sequence converges in the rationals?,,Let $(x_k)$ be a sequence in $\mathbb Q$ such that $x_k=\sum\limits_{n=1}^{k}\frac{1}{10^{n^2}}$ for all $k\geq 1$. It can be easily seen that this sequence is bounded and Cauchy. But does it converge in $\mathbb Q$? I could not find any way to verify that. Please help!,Let $(x_k)$ be a sequence in $\mathbb Q$ such that $x_k=\sum\limits_{n=1}^{k}\frac{1}{10^{n^2}}$ for all $k\geq 1$. It can be easily seen that this sequence is bounded and Cauchy. But does it converge in $\mathbb Q$? I could not find any way to verify that. Please help!,,"['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
17,"Prob. 8, Sec. 2.10 in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: The dual space of $c_0$ is $\ell^1$?","Prob. 8, Sec. 2.10 in Erwine Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: The dual space of  is ?",c_0 \ell^1,"Let $c_0$ be the subspace of $\ell^\infty$ consisting of all sequences of (real or complex ) numbers converging to $0$ . How to prove that the dual space of $c_0$ is (isomorphic to) $\ell^1$ ? My effort: Let $f \in c_0^\prime$ . Then $f$ is a bounded linear functional with domain $c_0$ . Then, for any $x \colon= (\xi_j)_{j\in \mathbb{N}} \in c_0$ , we have $$x = \sum_{j=1}^\infty \xi_j e_j,$$ where $e_j = (\delta_{ij})$ for each $j \in \mathbb{N}$ . Each $e_j \in c_0$ . Then using the boundedness (and the consequent continuity) of $f$ , we can conclude that $$f(x) = \sum_{j=1}^\infty \xi_j f(e_j).$$ So $$\vert f(x) \vert \leq \sum_{j=1}^\infty \vert \xi_j \vert \ \vert f(e_j) \vert \leq \Vert x \Vert_\infty \sum_{j=1}^\infty \vert f(e_j)\vert, $$ showing that $$\Vert f \Vert \leq \sum_{j=1}^\infty \vert f(e_j)\vert. $$ Is what I've stated so far correct? How to proceed?","Let be the subspace of consisting of all sequences of (real or complex ) numbers converging to . How to prove that the dual space of is (isomorphic to) ? My effort: Let . Then is a bounded linear functional with domain . Then, for any , we have where for each . Each . Then using the boundedness (and the consequent continuity) of , we can conclude that So showing that Is what I've stated so far correct? How to proceed?","c_0 \ell^\infty 0 c_0 \ell^1 f \in c_0^\prime f c_0 x \colon= (\xi_j)_{j\in \mathbb{N}} \in c_0 x = \sum_{j=1}^\infty \xi_j e_j, e_j = (\delta_{ij}) j \in \mathbb{N} e_j \in c_0 f f(x) = \sum_{j=1}^\infty \xi_j f(e_j). \vert f(x) \vert \leq \sum_{j=1}^\infty \vert \xi_j \vert \ \vert f(e_j) \vert \leq \Vert x \Vert_\infty \sum_{j=1}^\infty \vert f(e_j)\vert,  \Vert f \Vert \leq \sum_{j=1}^\infty \vert f(e_j)\vert. ","['real-analysis', 'functional-analysis', 'normed-spaces', 'duality-theorems']"
18,"For which values of $\alpha \in \mathbb{R}$, does the series $\sum_{n=1}^\infty n^\alpha(\sqrt{n+1} - 2 \sqrt{n} + \sqrt{n-1})$ converge?","For which values of , does the series  converge?",\alpha \in \mathbb{R} \sum_{n=1}^\infty n^\alpha(\sqrt{n+1} - 2 \sqrt{n} + \sqrt{n-1}),How do I study for which values of $\alpha \in \mathbb{R}$ the following series converges? (I have some troubles because of the form [$\infty - \infty$] that arises when taking the limit.) $$\sum_{n=1}^\infty n^\alpha(\sqrt{n+1} - 2 \sqrt{n} + \sqrt{n-1})$$,How do I study for which values of $\alpha \in \mathbb{R}$ the following series converges? (I have some troubles because of the form [$\infty - \infty$] that arises when taking the limit.) $$\sum_{n=1}^\infty n^\alpha(\sqrt{n+1} - 2 \sqrt{n} + \sqrt{n-1})$$,,"['calculus', 'real-analysis', 'sequences-and-series']"
19,"Is there an intuitive, not-too-mathematical way of thinking about limit points? [duplicate]","Is there an intuitive, not-too-mathematical way of thinking about limit points? [duplicate]",,"This question already has answers here : Why the definition of limit have a strict inequality rather than not? (3 answers) Closed 9 years ago . so I know this question has been asked sooo many times. But I just have a few questions in particular, which despite searching, I haven't found an answer to. I appreciate any help. Book's definition: A point $x$ is a limit point of a set $A$ if every $\epsilon$-neighborhood $V_{\epsilon}(x)$ of $x$ intersects the set $A$ in some point other than $x$. Particularly, wanted clarification with the bolded part of this sentence. If we take our set $A = [1,4), A \in \mathbb{R}$, then would the point $\{2\}$ be a limit point of the set $A$? Since every epsilon neighborhood of $2$ intersects $A$ in a point other than $2$. Or does that bolded sentence mean to imply that, the limit point itself, cannot belong to the set $A$? So in this case, that would mean to imply, only $4$ is a limit point of the set $A$ ? Is it possible for someone to give a diagrammatic example of what is and isn't a limit point? Edit: I am trying to get a ""non-mathematical"" idea of a limit point, and thus I will opt to not mark this question as a duplicate. I mean, technically one could envision and understand this definition with the $\epsilon$ definition, but for people like me who are mathematically challenged, it's nice to have a different way of thinking about it (they are the exact same idea, I know, but this helps me to link and understand the concept). MY question has been solved, now, however.","This question already has answers here : Why the definition of limit have a strict inequality rather than not? (3 answers) Closed 9 years ago . so I know this question has been asked sooo many times. But I just have a few questions in particular, which despite searching, I haven't found an answer to. I appreciate any help. Book's definition: A point $x$ is a limit point of a set $A$ if every $\epsilon$-neighborhood $V_{\epsilon}(x)$ of $x$ intersects the set $A$ in some point other than $x$. Particularly, wanted clarification with the bolded part of this sentence. If we take our set $A = [1,4), A \in \mathbb{R}$, then would the point $\{2\}$ be a limit point of the set $A$? Since every epsilon neighborhood of $2$ intersects $A$ in a point other than $2$. Or does that bolded sentence mean to imply that, the limit point itself, cannot belong to the set $A$? So in this case, that would mean to imply, only $4$ is a limit point of the set $A$ ? Is it possible for someone to give a diagrammatic example of what is and isn't a limit point? Edit: I am trying to get a ""non-mathematical"" idea of a limit point, and thus I will opt to not mark this question as a duplicate. I mean, technically one could envision and understand this definition with the $\epsilon$ definition, but for people like me who are mathematically challenged, it's nice to have a different way of thinking about it (they are the exact same idea, I know, but this helps me to link and understand the concept). MY question has been solved, now, however.",,"['real-analysis', 'analysis', 'terminology', 'definition']"
20,"On the existence of a continuous bijection $f\colon [0,1]\to [0,1]\times [0,1]$",On the existence of a continuous bijection,"f\colon [0,1]\to [0,1]\times [0,1]","Let $f$ be a continuous function on $[0,1]$ such that $f([0,1])=[0,1]\times[0,1].$ Then show that $f$ is not one-one. Hints will be appreciated.","Let $f$ be a continuous function on $[0,1]$ such that $f([0,1])=[0,1]\times[0,1].$ Then show that $f$ is not one-one. Hints will be appreciated.",,['real-analysis']
21,$x \perp y$ if and only if $\Vert x + \alpha y \Vert \ge \Vert x \Vert$ for all scalars $\alpha$,if and only if  for all scalars,x \perp y \Vert x + \alpha y \Vert \ge \Vert x \Vert \alpha,"Here's Prob. 8 in the Problems after Sec. 3.2 in Introductory Functional Analysis With Applications by Erwine Kreyszig: Show that in an inner product space, $x \perp y$ if and only if $\Vert x + \alpha y \Vert \ge \Vert x \Vert$ for all scalars $\alpha$. If $x \perp y$, then $\langle x, y \rangle = 0$; so for any scalar $\alpha$, we have  $$ \begin{align*} \Vert x + \alpha y \Vert^2 &= \langle x + \alpha y, x + \alpha y \rangle \\ &= \Vert x \Vert^2 + 2 \Re \bar{\alpha} \langle x, y \rangle + \vert \alpha \vert^2 \ \Vert y \Vert^2 \\  &= \Vert x \Vert^2 +  \vert \alpha \vert^2 \ \Vert y \Vert^2 \\  &\ge \Vert x \Vert^2.  \end{align*} $$ So  $$ \Vert x + \alpha y \Vert \geq \Vert x \Vert. $$ Am I right? Now how to prove the converse? It is my guess that we will have to put in a particular value for $\alpha$.","Here's Prob. 8 in the Problems after Sec. 3.2 in Introductory Functional Analysis With Applications by Erwine Kreyszig: Show that in an inner product space, $x \perp y$ if and only if $\Vert x + \alpha y \Vert \ge \Vert x \Vert$ for all scalars $\alpha$. If $x \perp y$, then $\langle x, y \rangle = 0$; so for any scalar $\alpha$, we have  $$ \begin{align*} \Vert x + \alpha y \Vert^2 &= \langle x + \alpha y, x + \alpha y \rangle \\ &= \Vert x \Vert^2 + 2 \Re \bar{\alpha} \langle x, y \rangle + \vert \alpha \vert^2 \ \Vert y \Vert^2 \\  &= \Vert x \Vert^2 +  \vert \alpha \vert^2 \ \Vert y \Vert^2 \\  &\ge \Vert x \Vert^2.  \end{align*} $$ So  $$ \Vert x + \alpha y \Vert \geq \Vert x \Vert. $$ Am I right? Now how to prove the converse? It is my guess that we will have to put in a particular value for $\alpha$.",,"['real-analysis', 'analysis', 'functional-analysis', 'inner-products', 'orthogonality']"
22,Convex function with non-symmetric Hessian,Convex function with non-symmetric Hessian,,"Let $U$ be an open convex subset of $\mathbb R^n$ and $f:U\to\mathbb R$ a convex function on it. It is a well-known fact that if the second partial derivatives exist everywhere on $U$ and are all continuous ( i.e., if $f\in\mathcal C^2$), then the Hessian of $f$ is symmetric, that is, $\partial^2 f/(\partial x_i\partial x_j)=\partial^2 f/(\partial x_j\partial x_i)$ for any $i,j\in\{1,\ldots,n\}$. (Actually, $f$ needn't even be convex for this result.) In fact, Alexandroff's theorem states that the Hessian exists and is symmetric almost everywhere with respect to the $n$-dimensional Lebesgue measure, without any additional assumptions beyond convexity. Question : It is possible for $f$ to be twice differentiable (and thus have, not necessarily everywhere-continuous, second-order partial derivatives) everywhere on $U$ but a Hessian that is not symmetric at some $x\in U$? Update : Dudley (1977) gives an example of a convex function with an existent and asymmetric Hessian at the origin. This counterexample doesn't settle my question, however, because Dudley's function doesn't have a second-order (Fréchet) derivative ( i.e. , not twice differentiable) at the origin (even though the second-order partial derivatives exist). I would like to see a convex function with both an existent second-order Fréchet derivative and with asymmetric Hessian at some point (which necessarily implies that some of the second-order partial derivatives are discontinuous at that point).","Let $U$ be an open convex subset of $\mathbb R^n$ and $f:U\to\mathbb R$ a convex function on it. It is a well-known fact that if the second partial derivatives exist everywhere on $U$ and are all continuous ( i.e., if $f\in\mathcal C^2$), then the Hessian of $f$ is symmetric, that is, $\partial^2 f/(\partial x_i\partial x_j)=\partial^2 f/(\partial x_j\partial x_i)$ for any $i,j\in\{1,\ldots,n\}$. (Actually, $f$ needn't even be convex for this result.) In fact, Alexandroff's theorem states that the Hessian exists and is symmetric almost everywhere with respect to the $n$-dimensional Lebesgue measure, without any additional assumptions beyond convexity. Question : It is possible for $f$ to be twice differentiable (and thus have, not necessarily everywhere-continuous, second-order partial derivatives) everywhere on $U$ but a Hessian that is not symmetric at some $x\in U$? Update : Dudley (1977) gives an example of a convex function with an existent and asymmetric Hessian at the origin. This counterexample doesn't settle my question, however, because Dudley's function doesn't have a second-order (Fréchet) derivative ( i.e. , not twice differentiable) at the origin (even though the second-order partial derivatives exist). I would like to see a convex function with both an existent second-order Fréchet derivative and with asymmetric Hessian at some point (which necessarily implies that some of the second-order partial derivatives are discontinuous at that point).",,"['real-analysis', 'convex-analysis', 'hessian-matrix']"
23,"If $f: \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function, $f(0)=0$ and $f' = f^2$, then $f = 0$.","If  is a differentiable function,  and , then .",f: \mathbb{R} \rightarrow \mathbb{R} f(0)=0 f' = f^2 f = 0,"If $f: \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function, $f(0)=0$ and $f' = f^2$, then $f = 0$. Any help?","If $f: \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function, $f(0)=0$ and $f' = f^2$, then $f = 0$. Any help?",,['real-analysis']
24,"Continuity on $[a,b]$ implies uniform continuity on $[a,b]$",Continuity on  implies uniform continuity on,"[a,b] [a,b]","I don't understand the step underlined in green. I understand that for any $n$ , $|f(x_n)-f(y_n)|\geq \varepsilon$ where $x_n, y_n$ satisfy the conditions given regarding a function being not uniformly continuous.  However if say $x_n=\frac{1}{2n}, y_n=\frac{1}{3n} $ then $|x_n-y_n|=\frac{1}{6n}$ and $|f(\frac{1}{2n})-f(\frac{1}{3n})|\geq \varepsilon$ so if we take $n=1$ , $|f(\frac{1}{2})-f(\frac{1}{3})|\geq \varepsilon$ but what if  $|x_{k_n}-y_{k_n}|\geq\frac{1}{n}?$","I don't understand the step underlined in green. I understand that for any $n$ , $|f(x_n)-f(y_n)|\geq \varepsilon$ where $x_n, y_n$ satisfy the conditions given regarding a function being not uniformly continuous.  However if say $x_n=\frac{1}{2n}, y_n=\frac{1}{3n} $ then $|x_n-y_n|=\frac{1}{6n}$ and $|f(\frac{1}{2n})-f(\frac{1}{3n})|\geq \varepsilon$ so if we take $n=1$ , $|f(\frac{1}{2})-f(\frac{1}{3})|\geq \varepsilon$ but what if  $|x_{k_n}-y_{k_n}|\geq\frac{1}{n}?$",,"['real-analysis', 'sequences-and-series', 'uniform-continuity']"
25,Does $f~'$ have to be continuous for application of Mean Value Theorem?,Does  have to be continuous for application of Mean Value Theorem?,f~',"If $f$ is a continuous function in $[a,b]$, then for any $x,y \in [a,b];~y>x$, we say that $\exists~c \in [x,y]$ such that $f~'(c)= \dfrac {f(y)-f(x)}{y-x}$. Does $f~'$ have to be necessarily continuous to apply the mean value theorem for derivatives? If not, why is $f~''$ needed to be continuous in this problem here Thank you very much for your help.","If $f$ is a continuous function in $[a,b]$, then for any $x,y \in [a,b];~y>x$, we say that $\exists~c \in [x,y]$ such that $f~'(c)= \dfrac {f(y)-f(x)}{y-x}$. Does $f~'$ have to be necessarily continuous to apply the mean value theorem for derivatives? If not, why is $f~''$ needed to be continuous in this problem here Thank you very much for your help.",,"['calculus', 'real-analysis']"
26,"Lemma 2.4-1 in Erwin Kreyszig's ""Introductory Functional Analysis with Applications"": Is there an easier proof?","Lemma 2.4-1 in Erwin Kreyszig's ""Introductory Functional Analysis with Applications"": Is there an easier proof?",,"Here's the statement: Let $\{x_1, \ldots, x_n \}$ be a linearly independent set of vectors in a normed space $X$ (of any dimension). Then there is a real number $c > 0$ such that for every choice of scalars $\alpha_1, \ldots, \alpha_n$, we have  $$\Vert \alpha_1 x_1 + \ldots + \alpha_n x_n \Vert \geq c (\lvert\alpha_1\rvert + \ldots + \lvert\alpha_n\rvert).$$ Now although I understand Kreyszig's proof, I find it to be not-so-clean at least as far as notation goes. So I wonder if there could be a cleaner and easier proof not requiring too many pre-requisites? Or, by suitably modifying the notation, is there a better way of presenting Kreyszig's proof?","Here's the statement: Let $\{x_1, \ldots, x_n \}$ be a linearly independent set of vectors in a normed space $X$ (of any dimension). Then there is a real number $c > 0$ such that for every choice of scalars $\alpha_1, \ldots, \alpha_n$, we have  $$\Vert \alpha_1 x_1 + \ldots + \alpha_n x_n \Vert \geq c (\lvert\alpha_1\rvert + \ldots + \lvert\alpha_n\rvert).$$ Now although I understand Kreyszig's proof, I find it to be not-so-clean at least as far as notation goes. So I wonder if there could be a cleaner and easier proof not requiring too many pre-requisites? Or, by suitably modifying the notation, is there a better way of presenting Kreyszig's proof?",,"['real-analysis', 'functional-analysis', 'normed-spaces']"
27,"$\forall\ x,y,z\in \mathbb{R}$ Show that: $|x+y|+|y+z|+|x+z|\leq |x+y+z|+|x|+|y|+|z|$",Show that:,"\forall\ x,y,z\in \mathbb{R} |x+y|+|y+z|+|x+z|\leq |x+y+z|+|x|+|y|+|z|","$\forall\ x,y,z\in \mathbb{R}$ Show that: $$|x+y|+|y+z|+|x+z|\leq |x+y+z|+|x|+|y|+|z|$$ i tired, i notice that $x,y,z$ plays a symmetrical role in the inequality notice also that \begin{align*} |x+y|+|y+z|+|x+z|\leq |x+y+z|+|x|+|y|+|z| & \Longleftrightarrow \\  (|x+y|-|x|)+(|y+z|-|y|)+(|x+z|-|z|) \leq |x+y+z|  \end{align*} note that $\forall a,b\in \mathbb{R}\quad |a|-|b|\leq |a+b| $ then  $$(|x+y|-|x|)+(|y+z|-|y|)+(|x+z|-|z|) \leq |x|+|y|+|z|$$ i'm stuck here any help would be appreciated!","$\forall\ x,y,z\in \mathbb{R}$ Show that: $$|x+y|+|y+z|+|x+z|\leq |x+y+z|+|x|+|y|+|z|$$ i tired, i notice that $x,y,z$ plays a symmetrical role in the inequality notice also that \begin{align*} |x+y|+|y+z|+|x+z|\leq |x+y+z|+|x|+|y|+|z| & \Longleftrightarrow \\  (|x+y|-|x|)+(|y+z|-|y|)+(|x+z|-|z|) \leq |x+y+z|  \end{align*} note that $\forall a,b\in \mathbb{R}\quad |a|-|b|\leq |a+b| $ then  $$(|x+y|-|x|)+(|y+z|-|y|)+(|x+z|-|z|) \leq |x|+|y|+|z|$$ i'm stuck here any help would be appreciated!",,"['calculus', 'real-analysis', 'analysis', 'inequality', 'absolute-value']"
28,How to find the gradient of norm square,How to find the gradient of norm square,,How to find the gradient of the function $f(x) = ||g(x)||^2$ where $x \in \Bbb R^d$ and $g: \Bbb R^d \to \Bbb R$.,How to find the gradient of the function $f(x) = ||g(x)||^2$ where $x \in \Bbb R^d$ and $g: \Bbb R^d \to \Bbb R$.,,"['real-analysis', 'derivatives']"
29,"Monotonicity of the sequences $\left(1+\frac1n\right)^n$, $\left(1-\frac1n\right)^n$ and $\left(1+\frac1n\right)^{n+1}$","Monotonicity of the sequences ,  and",\left(1+\frac1n\right)^n \left(1-\frac1n\right)^n \left(1+\frac1n\right)^{n+1},"I am working on the following sequences. $$x_n=\left(1+\frac1n\right)^n \qquad z_n=\left(1-\frac1n\right)^n \qquad y_n=\left(1+\frac1n\right)^{n+1}$$ I am trying to prove that $x_n$ and $z_n$ are increasing sequences and $y_n$ is a decreasing sequence. I proved that $x_n$ is increasing using the binomial theorem: as $n$ increases, the number of terms in my sum increases. Also, for $k\ge1$ and $n\ge k$ the $(k+1)th$ term in our sum is $$\frac{n(n-1)(n-2)\cdots(n-k+1)}{k!}\left(\frac{1}{n}\right)^k$$ which can be rewritten as $$\frac{1}{k!}\left[\left(1-\frac1n\right)\left(1-\frac2n\right)\cdots\left(1-\frac{k-1}{n}\right)\right]\qquad(1)$$ Here, the product in $(1)$ increases as $n$ increases. I am in trouble proving that $z_n$ and $y_n$ are decreasing. Could you suggest me an approach to this problem please? Thank you. Edit Since I obtained an answer for $y_n$, I kept working on $z_n$. I took the continuous function $z(x)=(1-\frac1x)^x=\exp \left\{x\log(1-\frac1x)\right\}$. Then I derived it to obtain $$z'(x)=\exp\left\{x\log(1-\frac1x)\right\}\left[\log(1-\frac1x)+\frac{x}{(x-1)/x}\cdot\frac{1}{x^2}\right]$$ But $$z'(x)=\exp\left\{x\log(1-\frac1x)\right\}\left[\log(1-\frac1x)+\frac{1}{x-1}\right]$$ $e^a$ is always positive. Hence the sign of $z'(x)$ depends on what we have between square brackets, which is greater than zero (I don't know how to prove it; I used a plot) for $x\ge1$.  Hence $z_n$ is increasing.","I am working on the following sequences. $$x_n=\left(1+\frac1n\right)^n \qquad z_n=\left(1-\frac1n\right)^n \qquad y_n=\left(1+\frac1n\right)^{n+1}$$ I am trying to prove that $x_n$ and $z_n$ are increasing sequences and $y_n$ is a decreasing sequence. I proved that $x_n$ is increasing using the binomial theorem: as $n$ increases, the number of terms in my sum increases. Also, for $k\ge1$ and $n\ge k$ the $(k+1)th$ term in our sum is $$\frac{n(n-1)(n-2)\cdots(n-k+1)}{k!}\left(\frac{1}{n}\right)^k$$ which can be rewritten as $$\frac{1}{k!}\left[\left(1-\frac1n\right)\left(1-\frac2n\right)\cdots\left(1-\frac{k-1}{n}\right)\right]\qquad(1)$$ Here, the product in $(1)$ increases as $n$ increases. I am in trouble proving that $z_n$ and $y_n$ are decreasing. Could you suggest me an approach to this problem please? Thank you. Edit Since I obtained an answer for $y_n$, I kept working on $z_n$. I took the continuous function $z(x)=(1-\frac1x)^x=\exp \left\{x\log(1-\frac1x)\right\}$. Then I derived it to obtain $$z'(x)=\exp\left\{x\log(1-\frac1x)\right\}\left[\log(1-\frac1x)+\frac{x}{(x-1)/x}\cdot\frac{1}{x^2}\right]$$ But $$z'(x)=\exp\left\{x\log(1-\frac1x)\right\}\left[\log(1-\frac1x)+\frac{1}{x-1}\right]$$ $e^a$ is always positive. Hence the sign of $z'(x)$ depends on what we have between square brackets, which is greater than zero (I don't know how to prove it; I used a plot) for $x\ge1$.  Hence $z_n$ is increasing.",,"['real-analysis', 'sequences-and-series']"
30,Can every open set of $\mathbb{R}$ be written as countable union of disjoint open bounded intervals?,Can every open set of  be written as countable union of disjoint open bounded intervals?,\mathbb{R},"I know that the following two statements are correct. Every open set of $\mathbb{R}$ be written as countable union of disjoint    open intervals ( including open rays and $\mathbb{R}$). Every open set of $\mathbb{R}$ be written as countable union of open bounded intervals. Then is it true that every open set of $\mathbb{R}$ be written as countable union of disjoint open bounded intervals? I think following a similar argument as statement 1, we only need to show open rays and $\mathbb{R}$ satisfy this property.","I know that the following two statements are correct. Every open set of $\mathbb{R}$ be written as countable union of disjoint    open intervals ( including open rays and $\mathbb{R}$). Every open set of $\mathbb{R}$ be written as countable union of open bounded intervals. Then is it true that every open set of $\mathbb{R}$ be written as countable union of disjoint open bounded intervals? I think following a similar argument as statement 1, we only need to show open rays and $\mathbb{R}$ satisfy this property.",,"['real-analysis', 'general-topology']"
31,"Show $(x+y)^a > x^a + y^a$ for $x,y>0$ and $a>1$",Show  for  and,"(x+y)^a > x^a + y^a x,y>0 a>1","This is a pretty straightforward question. I want to show $(x+y)^a > x^a + y^a$ for $x,y>0$ and $a>1$. One way would be this. WLOG, suppose $x \leq y$. Then: $(1+\frac{x}{y})^a >1+\frac{x}{y} \geq 1+(\frac{x}{y})^a$. Noting that $\frac{1}{y^a}t_1>\frac{1}{y^a}t_2 \implies t_1 > t_2$, it follows that $(x+y)^a > x^a + y^a$. But what other ways are there to show this inequality?","This is a pretty straightforward question. I want to show $(x+y)^a > x^a + y^a$ for $x,y>0$ and $a>1$. One way would be this. WLOG, suppose $x \leq y$. Then: $(1+\frac{x}{y})^a >1+\frac{x}{y} \geq 1+(\frac{x}{y})^a$. Noting that $\frac{1}{y^a}t_1>\frac{1}{y^a}t_2 \implies t_1 > t_2$, it follows that $(x+y)^a > x^a + y^a$. But what other ways are there to show this inequality?",,"['calculus', 'real-analysis', 'inequality']"
32,"$f,g$ such that $\int fg = \int f \int g$",such that,"f,g \int fg = \int f \int g","Suppose $f,g$ are real valued on $\mathbb{R}$ (and no further restrictions apart from the obvious requirement that the integrals exist), then when does $\displaystyle\int f(x)g(x)\,dx = \int f(x) \, dx \int g(x) \, dx$? (all integrals indefinite) This question was posed on another site, and I was wondering whether there are any large classes of functions $f,g$ that work. Aside from the trivial solution, things like $f(x) = e^{nx}, g(x) = e^{\frac{n}{n-1}x}$ $(n\not= 1)$ work by inspection. I've tried re-casting the problem as: $$\int F^2 - \left(\int F\right)^2 = \int G^2 - \left(\int G\right)^2$$ With $F=f+g, G=f-g$, to introduce some symmetry. This gives rise to similarities with the variance formulae but nothing more than that. Using power series $F(x) = \displaystyle\sum_{n=0}^{\infty} a_nx^n, G(x) = \sum_{n=0}^{\infty} b_nx^n$, and the cauchy product, this equation boils down to: $$\sum_{m=0}^n \frac{(n-m)(a_ma_{n-m} - b_mb_{n-m})}{(n+1)(n-m+1)}= 0\qquad \forall n\geq 0$$ But again, progress is limited.","Suppose $f,g$ are real valued on $\mathbb{R}$ (and no further restrictions apart from the obvious requirement that the integrals exist), then when does $\displaystyle\int f(x)g(x)\,dx = \int f(x) \, dx \int g(x) \, dx$? (all integrals indefinite) This question was posed on another site, and I was wondering whether there are any large classes of functions $f,g$ that work. Aside from the trivial solution, things like $f(x) = e^{nx}, g(x) = e^{\frac{n}{n-1}x}$ $(n\not= 1)$ work by inspection. I've tried re-casting the problem as: $$\int F^2 - \left(\int F\right)^2 = \int G^2 - \left(\int G\right)^2$$ With $F=f+g, G=f-g$, to introduce some symmetry. This gives rise to similarities with the variance formulae but nothing more than that. Using power series $F(x) = \displaystyle\sum_{n=0}^{\infty} a_nx^n, G(x) = \sum_{n=0}^{\infty} b_nx^n$, and the cauchy product, this equation boils down to: $$\sum_{m=0}^n \frac{(n-m)(a_ma_{n-m} - b_mb_{n-m})}{(n+1)(n-m+1)}= 0\qquad \forall n\geq 0$$ But again, progress is limited.",,"['real-analysis', 'integration', 'functional-equations']"
33,How to figure whether it is a compact operator,How to figure whether it is a compact operator,,"How to figure whether it is a compact operator: $$T:C[0,1]\rightarrow C[0,1] $$ $C[0,1]$:the space of all continous function on [0,1] with supremum norm $$(Tx)(t)=\int^t_0 x(s)ds, \ \ \forall t\in[0,1]$$ Could you please help with this question.","How to figure whether it is a compact operator: $$T:C[0,1]\rightarrow C[0,1] $$ $C[0,1]$:the space of all continous function on [0,1] with supremum norm $$(Tx)(t)=\int^t_0 x(s)ds, \ \ \forall t\in[0,1]$$ Could you please help with this question.",,"['real-analysis', 'functional-analysis', 'compact-operators']"
34,Prime number Stone-Weierstrass-looking problem,Prime number Stone-Weierstrass-looking problem,,"Can you show that if $f \in C[0,1]$, and $\int_{0}^{1} f x^p dx =0$ for all primes $p$, that $f \equiv 0$?","Can you show that if $f \in C[0,1]$, and $\int_{0}^{1} f x^p dx =0$ for all primes $p$, that $f \equiv 0$?",,"['real-analysis', 'integration', 'measure-theory']"
35,Take 2: When/Why are these equal?,Take 2: When/Why are these equal?,,"This didn't go right the first time, so I'm going to drastically rephrase the query. As per this previous question , I am wondering if the two series $$\frac{f(a)+f(b)}{2}\frac{(b-a)}{1!}+\frac{f'(a)-f'(b)}{2}\frac{(b-a)^2}{2!}+\frac{f''(a)+f''(b)}{2}\frac{(b-a)^3}{3!}+\cdots$$ and $$\frac{f(a)+f(b)}{2}\frac{(b-a)}{1!}+\frac{f'(a)-f'(b)}{2^2}\frac{(b-a)^2}{2!}+\frac{f''(a)+f''(b)}{2^3}\frac{(b-a)^3}{3!}+\cdots$$ could possibly be equal. The only difference is the powers of $2$ in the denominator. NOTE: the numerator is $f^{(k)}(a)+(-1)^kf^{(k)}(b)$ in general.","This didn't go right the first time, so I'm going to drastically rephrase the query. As per this previous question , I am wondering if the two series $$\frac{f(a)+f(b)}{2}\frac{(b-a)}{1!}+\frac{f'(a)-f'(b)}{2}\frac{(b-a)^2}{2!}+\frac{f''(a)+f''(b)}{2}\frac{(b-a)^3}{3!}+\cdots$$ and $$\frac{f(a)+f(b)}{2}\frac{(b-a)}{1!}+\frac{f'(a)-f'(b)}{2^2}\frac{(b-a)^2}{2!}+\frac{f''(a)+f''(b)}{2^3}\frac{(b-a)^3}{3!}+\cdots$$ could possibly be equal. The only difference is the powers of $2$ in the denominator. NOTE: the numerator is $f^{(k)}(a)+(-1)^kf^{(k)}(b)$ in general.",,['real-analysis']
36,"Prove that $\forall x>0, \frac {x-1}{\ln(x)} \geq \sqrt{x} $.",Prove that .,"\forall x>0, \frac {x-1}{\ln(x)} \geq \sqrt{x} ","This inequality arose in this question Prove that : $|f(b)-f(a)|\geqslant (b-a) \sqrt{f'(a) f'(b)}$ with $(a,b) \in \mathbb{R}^{2}$ : $$\forall x>0, \frac {x-1}{\ln(x)} \geq \sqrt{x} $$ Can anybody find a way to prove it without calculus? Like using only inequalities like AM-GM,  Jensen or Cauchy - Schwarz? CLARIFICATION I want a proof that does not involve setting $g(x)=\frac {x-1}{\ln(x)}- \sqrt{x}$, studying the derivative of $g$ and proving it is $\geq 0$ with this method. The upvoted answers comply with this criterion.","This inequality arose in this question Prove that : $|f(b)-f(a)|\geqslant (b-a) \sqrt{f'(a) f'(b)}$ with $(a,b) \in \mathbb{R}^{2}$ : $$\forall x>0, \frac {x-1}{\ln(x)} \geq \sqrt{x} $$ Can anybody find a way to prove it without calculus? Like using only inequalities like AM-GM,  Jensen or Cauchy - Schwarz? CLARIFICATION I want a proof that does not involve setting $g(x)=\frac {x-1}{\ln(x)}- \sqrt{x}$, studying the derivative of $g$ and proving it is $\geq 0$ with this method. The upvoted answers comply with this criterion.",,"['real-analysis', 'functions', 'inequality', 'logarithms', 'radicals']"
37,Evaluate these infinite products $\prod_{n\geq 2}(1-\frac{1}{n^3})$ and $\prod_{n\geq 1}(1+\frac{1}{n^3})$,Evaluate these infinite products  and,\prod_{n\geq 2}(1-\frac{1}{n^3}) \prod_{n\geq 1}(1+\frac{1}{n^3}),"What is $\prod\limits_{n\geq 2}(1-\frac{1}{n^3})=?$ $\prod\limits_{n\geq 1}(1+\frac{1}{n^3})=?$ I am sure about their convergence. But don't know about exact values. Know some bounds as well. For example first one is in interval (2/3,1) and second one is in (2,3).","What is $\prod\limits_{n\geq 2}(1-\frac{1}{n^3})=?$ $\prod\limits_{n\geq 1}(1+\frac{1}{n^3})=?$ I am sure about their convergence. But don't know about exact values. Know some bounds as well. For example first one is in interval (2/3,1) and second one is in (2,3).",,"['real-analysis', 'complex-analysis', 'infinite-product']"
38,What is an example of a lower semicontinuous function that is not continuous?,What is an example of a lower semicontinuous function that is not continuous?,,I couldn't find an example anywhere. Does anyone know such example? Thanks.,I couldn't find an example anywhere. Does anyone know such example? Thanks.,,"['real-analysis', 'analysis']"
39,Question from Folland real analysis 6.38,Question from Folland real analysis 6.38,,"I have been staring at this for hours. I cannot figure out how to prove the following from Folland, problem 6.38. Show that: $$f \in L^p \iff \sum_{k=-\infty}^{+\infty}2^{kp}\lambda_f(2^k)<\infty$$ Where $\lambda_f(2^k)=\mu \left\{x: |f(x)|>2^k \right\}$. Some guidance would be greatly appreciated. Thank you.","I have been staring at this for hours. I cannot figure out how to prove the following from Folland, problem 6.38. Show that: $$f \in L^p \iff \sum_{k=-\infty}^{+\infty}2^{kp}\lambda_f(2^k)<\infty$$ Where $\lambda_f(2^k)=\mu \left\{x: |f(x)|>2^k \right\}$. Some guidance would be greatly appreciated. Thank you.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
40,Monotone convergence theorem (almost everywhere concept),Monotone convergence theorem (almost everywhere concept),,"Could someone enlighten me. Monotone convergence theorem says: if $f_{n},f\in L^{+}$ i.e measurable functions, such that $f_{n}\uparrow f$ pointwise then $\int f_{n}\uparrow \int f$. But then my teacher says we can relax the condition, using almost everywhere. We can assume that $f_{n}\uparrow f$ a.e $\Rightarrow \int f_{n}\uparrow \int f$ and then he does something I don't get. He asks the question: ''Is it true $f_{n}\to f$ a.e $\Rightarrow \int f_{n}\to \int f$?'' what does the horizontal arrow mean in this case? (does he also mean converge to but almost everywhere?) Here is his counterexample  $(X,\mathcal{M},\mu)=(\mathbb{R},\mathcal{L},m)$ and we let $f_{n}=\chi_{[n,n+1]}$ then $\int \chi_{[n,n+1]}dm=m([n,n+1])=1$ but $f_{n}\to 0$ pointwise Is the horizontal and the vertical arrow meaning the same (''converge to'')? Why do the characteristic functions converge pointwise??","Could someone enlighten me. Monotone convergence theorem says: if $f_{n},f\in L^{+}$ i.e measurable functions, such that $f_{n}\uparrow f$ pointwise then $\int f_{n}\uparrow \int f$. But then my teacher says we can relax the condition, using almost everywhere. We can assume that $f_{n}\uparrow f$ a.e $\Rightarrow \int f_{n}\uparrow \int f$ and then he does something I don't get. He asks the question: ''Is it true $f_{n}\to f$ a.e $\Rightarrow \int f_{n}\to \int f$?'' what does the horizontal arrow mean in this case? (does he also mean converge to but almost everywhere?) Here is his counterexample  $(X,\mathcal{M},\mu)=(\mathbb{R},\mathcal{L},m)$ and we let $f_{n}=\chi_{[n,n+1]}$ then $\int \chi_{[n,n+1]}dm=m([n,n+1])=1$ but $f_{n}\to 0$ pointwise Is the horizontal and the vertical arrow meaning the same (''converge to'')? Why do the characteristic functions converge pointwise??",,"['real-analysis', 'measure-theory']"
41,An identity involving the Beta function,An identity involving the Beta function,,"I'm trying to show that $$ \int _0^1 \frac{x^{a-1}(1-x)^{b-1}}{(x+c)^{a+b}}dx = \frac{B(a,b)}{(1+c)^ac^b}$$ Where $$B(a,b) :=  \int _0^1 x^{a-1}(1-x)^{b-1}dx $$ is the ""Beta function"".  I am supposed to use a substitution but I'm pretty much stuck.  I am familiar with the basic properties of the Beta function, its relation to the gamma function etc.     Any hints or advice you care to offer would be super cool.","I'm trying to show that $$ \int _0^1 \frac{x^{a-1}(1-x)^{b-1}}{(x+c)^{a+b}}dx = \frac{B(a,b)}{(1+c)^ac^b}$$ Where $$B(a,b) :=  \int _0^1 x^{a-1}(1-x)^{b-1}dx $$ is the ""Beta function"".  I am supposed to use a substitution but I'm pretty much stuck.  I am familiar with the basic properties of the Beta function, its relation to the gamma function etc.     Any hints or advice you care to offer would be super cool.",,"['calculus', 'real-analysis', 'analysis', 'integration']"
42,Every function is the sum of an even function and an odd function in a unique way,Every function is the sum of an even function and an odd function in a unique way,,"It is known that every function $f(x)$ defined on the interval $(-a,a)$ can be represented as the sum of an even function and an odd function. However How do you prove that this representation is unique? Thanks for your help.","It is known that every function $f(x)$ defined on the interval $(-a,a)$ can be represented as the sum of an even function and an odd function. However How do you prove that this representation is unique? Thanks for your help.",,"['real-analysis', 'functions']"
43,$\sum a_n$ divergent with $\lim(n a_n)=0$,divergent with,\sum a_n \lim(n a_n)=0,"Can we find an example of a divergent series  $$\sum_{n=1}^\infty a_n$$ where the sequence $(a_n)$ is a decreasing sequence of real numbers, but such that $$\lim_{n\to\infty}(n a_n)=0$$","Can we find an example of a divergent series  $$\sum_{n=1}^\infty a_n$$ where the sequence $(a_n)$ is a decreasing sequence of real numbers, but such that $$\lim_{n\to\infty}(n a_n)=0$$",,"['real-analysis', 'sequences-and-series', 'examples-counterexamples']"
44,How to compute $\int_a^b (b-x)^{\frac{n-1}{2}}(x-a)^{-1/2}dx$?,How to compute ?,\int_a^b (b-x)^{\frac{n-1}{2}}(x-a)^{-1/2}dx,I wonder how to compute the following integral (for any natural number $n\geq 0$): $$\int_a^b (b-x)^{\frac{n-1}{2}}(x-a)^{-1/2}dx.$$ Does anyone know the final answer and how to get there? Is there a trick? Thank you very much!,I wonder how to compute the following integral (for any natural number $n\geq 0$): $$\int_a^b (b-x)^{\frac{n-1}{2}}(x-a)^{-1/2}dx.$$ Does anyone know the final answer and how to get there? Is there a trick? Thank you very much!,,"['calculus', 'real-analysis', 'integration']"
45,"Prove that the orbit of an iterated rotation of 0 (by (A)(Pi), A irrational) around a circle centered at the origin is dense in the circle.","Prove that the orbit of an iterated rotation of 0 (by (A)(Pi), A irrational) around a circle centered at the origin is dense in the circle.",,"I think the title of the question says it all. I unfortunately did not seem to conclude anything. Some ideas I had: It is easy to show that (given $T$ is the rotation) $\{T^n(\theta)\}$ is a set of distinct points. Furthermore, given that the circle is a compact metric, it must have a limit point $x$. By continuity of the rotation function, $T^n(x)$ is a limit as well since taking $T$ of every term yields the same sequence (with only the first term removed). By induction, we have infinitely many distinct limit points $\{T^n(x)\}$. That's all I could come up with! It was also easy to show that the orbit is infinite. I still don't seem to be able to get close to the required result however.","I think the title of the question says it all. I unfortunately did not seem to conclude anything. Some ideas I had: It is easy to show that (given $T$ is the rotation) $\{T^n(\theta)\}$ is a set of distinct points. Furthermore, given that the circle is a compact metric, it must have a limit point $x$. By continuity of the rotation function, $T^n(x)$ is a limit as well since taking $T$ of every term yields the same sequence (with only the first term removed). By induction, we have infinitely many distinct limit points $\{T^n(x)\}$. That's all I could come up with! It was also easy to show that the orbit is infinite. I still don't seem to be able to get close to the required result however.",,"['real-analysis', 'general-topology', 'dynamical-systems']"
46,"For every real $x>0$ and every integer $n>0$, there is one and only one real $y>0$ such that $y^n=x$","For every real  and every integer , there is one and only one real  such that",x>0 n>0 y>0 y^n=x,"I am unable to prove that For every real $x>0$ and every integer $n>0$, there is one and only one real $y>0$ such that $y^n=x$. Can anyone please help me here? It is clear that there is at most one such real $y$. But how do I go about the existence of such a real?","I am unable to prove that For every real $x>0$ and every integer $n>0$, there is one and only one real $y>0$ such that $y^n=x$. Can anyone please help me here? It is clear that there is at most one such real $y$. But how do I go about the existence of such a real?",,['real-analysis']
47,Does non-decreasing sequence of this form converge?,Does non-decreasing sequence of this form converge?,,"Given a non-decreasing sequence $(a_n)$: $$a_1 \leq a_2 \leq a_3 \leq a_4 \ldots$$ and $$\displaystyle\lim_{n\to\infty}(a_n - a_{n-1}) = 0$$ Does it have to converge? For strictly than sequence $a_1 < a_2 < a_3 < a_4 < \ldots$ with the limit property, it's easy to show that it doesn't converge, for example take $a_n = \sqrt{n}$. In this case, however I couldn't find a counter example sequence, and I have a feeling this sequence might converge but again I'm not so sure. Any hint would be greatly appreciated.","Given a non-decreasing sequence $(a_n)$: $$a_1 \leq a_2 \leq a_3 \leq a_4 \ldots$$ and $$\displaystyle\lim_{n\to\infty}(a_n - a_{n-1}) = 0$$ Does it have to converge? For strictly than sequence $a_1 < a_2 < a_3 < a_4 < \ldots$ with the limit property, it's easy to show that it doesn't converge, for example take $a_n = \sqrt{n}$. In this case, however I couldn't find a counter example sequence, and I have a feeling this sequence might converge but again I'm not so sure. Any hint would be greatly appreciated.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
48,"If $f:(M,d )\to (M,p)$ is a homeomorphism, are $d,p$ equivalent?","If  is a homeomorphism, are  equivalent?","f:(M,d )\to (M,p) d,p","Let $M$ be a set and $\delta$, $\rho$ metrics on $M$. If $f:(M,\delta)\to(M,\rho)$ is a homeomorphism, are $\delta$, $\rho$ equivalent metrics? Not necessarly $f=\operatorname{id}_M$ (since result is obvious).","Let $M$ be a set and $\delta$, $\rho$ metrics on $M$. If $f:(M,\delta)\to(M,\rho)$ is a homeomorphism, are $\delta$, $\rho$ equivalent metrics? Not necessarly $f=\operatorname{id}_M$ (since result is obvious).",,"['real-analysis', 'metric-spaces']"
49,"Extending functions from $(a,b)$ to $[a,b]$.",Extending functions from  to .,"(a,b) [a,b]","Let's say $f$ is continuous on $(a,b)$.  Is it true that it can always be extended to a continuous function on $[a,b]$?  What if it's a uniformly continuous function; can it be extended to a uniformly continuous function on $[a,b]$? I'm thinking that, if it's continuous on $(a,b)$, then $\lim_{x\to a^+} f(x)$ and $\lim_{x\to b^-} f(x)$ exist, and then one can just define $f(a)$ to be $\lim_{x\to a^+} f(x)$, and similarly for $f(b)$.  I'm not too sure about that, though. And if that's the case, then the other one (uniform continuity) follows easily, since $[a,b]$ is compact.","Let's say $f$ is continuous on $(a,b)$.  Is it true that it can always be extended to a continuous function on $[a,b]$?  What if it's a uniformly continuous function; can it be extended to a uniformly continuous function on $[a,b]$? I'm thinking that, if it's continuous on $(a,b)$, then $\lim_{x\to a^+} f(x)$ and $\lim_{x\to b^-} f(x)$ exist, and then one can just define $f(a)$ to be $\lim_{x\to a^+} f(x)$, and similarly for $f(b)$.  I'm not too sure about that, though. And if that's the case, then the other one (uniform continuity) follows easily, since $[a,b]$ is compact.",,"['real-analysis', 'continuity', 'uniform-continuity']"
50,When are complex numbers insufficient?,When are complex numbers insufficient?,,"Rationals can't solve $x^2=2$, and reals can't solve $x^2=-1$. Is there any problem that cannot be solved by complex numbers but can be solved by non-standard numbers? Every polynomial with coefficients in $C$ can be solved by numbers in $C$, can every equation* be solved by numbers in $C$? *(that can not be simplified to $1=0$)","Rationals can't solve $x^2=2$, and reals can't solve $x^2=-1$. Is there any problem that cannot be solved by complex numbers but can be solved by non-standard numbers? Every polynomial with coefficients in $C$ can be solved by numbers in $C$, can every equation* be solved by numbers in $C$? *(that can not be simplified to $1=0$)",,"['real-analysis', 'analysis', 'complex-analysis', 'nonstandard-analysis']"
51,An explicit example of a differentiable function taking rational values at rational points but whose derivative is irrational at rational points,An explicit example of a differentiable function taking rational values at rational points but whose derivative is irrational at rational points,,"Construct an example of a differentiable function such that $$ \forall r \in {\Bbb Q}\quad f(r)  \in {\Bbb Q}\text{ but } f'(r) \notin {\Bbb Q} $$ this example is not trivial, in a paper they prove the existence, but they don't give an explicit example, but they said that other paper as an explicit example of that, but i could not find it. EDITED: This is the paper: Walter Rudin, Restrictions on the Values of Derivatives , The American Mathematical Monthly, Vol. 84 , No. 9 (1977), pp. 722-723, MR480908 .","Construct an example of a differentiable function such that $$ \forall r \in {\Bbb Q}\quad f(r)  \in {\Bbb Q}\text{ but } f'(r) \notin {\Bbb Q} $$ this example is not trivial, in a paper they prove the existence, but they don't give an explicit example, but they said that other paper as an explicit example of that, but i could not find it. EDITED: This is the paper: Walter Rudin, Restrictions on the Values of Derivatives , The American Mathematical Monthly, Vol. 84 , No. 9 (1977), pp. 722-723, MR480908 .",,"['calculus', 'real-analysis', 'examples-counterexamples']"
52,an $L^\infty$ norm equal to a supremum,an  norm equal to a supremum,L^\infty,"My question arose while studying an article which finds the $K$-functional for the pair of spaces $L^1,L^\infty$, so it's related to interpolation theory, but I think it can be solved with some $\inf,\sup$ manipulations. I'm sorry if the tags aren't correct. Consider $(X,\Sigma, \mu)$ an arbitrary measure space. For each measurable function $f: X \to \Bbb{C}$ and $\alpha \geq 0$ define  $$ f_*(\alpha)=\mu(\{ x \in X : |f(x)|>\alpha \}),$$ the distribution function of $f$. For any measurable function $f:X \to \Bbb{C}$ for which there exists $\alpha>0$ with $f_*(\alpha)<\infty$, define $f^* :(0,\infty) \to [0,\infty)$ by $$ f^*(t)=\inf \{ y>0 :f_*(y) \leq t\}.$$ It can be proved from the definitions that for every $t>0$ we have $$ f^*(f_*(t))\leq t,\ f_*(f^*(t))\leq t.$$ Moreover, the function $f^*$ is continuous from the right. Both of the functions $f_*,f^*$ are non increasing. What I need to prove is that $$\sup_{t>0} f^*(t)= \| f\|_\infty,$$ for every function $f \in L^\infty$. The inequality $\leq $ is straight from the definition. The other one I can't seem to get, and the text says that it's pretty hard, but with ""a little more effort"" it can be done. I haven't succeeded.","My question arose while studying an article which finds the $K$-functional for the pair of spaces $L^1,L^\infty$, so it's related to interpolation theory, but I think it can be solved with some $\inf,\sup$ manipulations. I'm sorry if the tags aren't correct. Consider $(X,\Sigma, \mu)$ an arbitrary measure space. For each measurable function $f: X \to \Bbb{C}$ and $\alpha \geq 0$ define  $$ f_*(\alpha)=\mu(\{ x \in X : |f(x)|>\alpha \}),$$ the distribution function of $f$. For any measurable function $f:X \to \Bbb{C}$ for which there exists $\alpha>0$ with $f_*(\alpha)<\infty$, define $f^* :(0,\infty) \to [0,\infty)$ by $$ f^*(t)=\inf \{ y>0 :f_*(y) \leq t\}.$$ It can be proved from the definitions that for every $t>0$ we have $$ f^*(f_*(t))\leq t,\ f_*(f^*(t))\leq t.$$ Moreover, the function $f^*$ is continuous from the right. Both of the functions $f_*,f^*$ are non increasing. What I need to prove is that $$\sup_{t>0} f^*(t)= \| f\|_\infty,$$ for every function $f \in L^\infty$. The inequality $\leq $ is straight from the definition. The other one I can't seem to get, and the text says that it's pretty hard, but with ""a little more effort"" it can be done. I haven't succeeded.",,"['real-analysis', 'analysis']"
53,Sums and products of bounded functions,Sums and products of bounded functions,,"I have been on this one for hours, cant figure out how to write this in the proper format/wording. Let $f$ and $g$ be functions from $\mathbb{R}$ to $\mathbb{R}$. For the sum and product of $f$ and $g$, determine which statements are true. If true provide a proof; if false, provide a counterexample. If $f$ and $g$ are bounded, then $f + g$ is bounded. If $f$ and $g$ are bounded, then $fg$ is bounded. If both $f + g$ and $fg$ are bounded, then $f$ and $g$ are bounded. Any help is appreciated, thanks :)","I have been on this one for hours, cant figure out how to write this in the proper format/wording. Let $f$ and $g$ be functions from $\mathbb{R}$ to $\mathbb{R}$. For the sum and product of $f$ and $g$, determine which statements are true. If true provide a proof; if false, provide a counterexample. If $f$ and $g$ are bounded, then $f + g$ is bounded. If $f$ and $g$ are bounded, then $fg$ is bounded. If both $f + g$ and $fg$ are bounded, then $f$ and $g$ are bounded. Any help is appreciated, thanks :)",,"['real-analysis', 'functions']"
54,What does a compact set look like?,What does a compact set look like?,,"So for a writing assignment in one of my classes we are asked to discuss and prove some basic results about compact sets in general topological spaces. I like proving these things, but they dont help me understand what a compact( locally compact, paracompact,...) set in a topological space ""looks like."" That said I'm asking for some examples of compact (locally compact, ...) sets in a variety of topological spaces. I'm also interested in some explanation of what extra ""benefits"" are brought about by singling out these compact sets. For example the p-adic numbers are locally compact and locally compact things (abelian groups to be precise) are a good setting in which to carry out fourier analyis.","So for a writing assignment in one of my classes we are asked to discuss and prove some basic results about compact sets in general topological spaces. I like proving these things, but they dont help me understand what a compact( locally compact, paracompact,...) set in a topological space ""looks like."" That said I'm asking for some examples of compact (locally compact, ...) sets in a variety of topological spaces. I'm also interested in some explanation of what extra ""benefits"" are brought about by singling out these compact sets. For example the p-adic numbers are locally compact and locally compact things (abelian groups to be precise) are a good setting in which to carry out fourier analyis.",,"['real-analysis', 'general-topology', 'compactness']"
55,Does this function grow slower than $e^{x}$ but faster than $e^{kx}$ for all $0<k<1$,Does this function grow slower than  but faster than  for all,e^{x} e^{kx} 0<k<1,"$f\left(x\right)=\lim_{a\to\infty}\sum_{n=1}^{a}\frac{x^{n}}{n\cdot n!}$ I spontaneously thought of this function while messing with the power series for exp(x). It's fairly obvious that this function grows slower than exp(x) and evaluating further derivatives at x=0 seems to indicate that it grows faster than all $e^{kx}$ functions (for 0<k<1), as their derivatives follow $k$ , $k^2$ , $k^3$ , etc, while derivatives of f(x) are 1, 1/2, 1/3 etc. The former derivatives shrink faster than the latter. I think this is not really a valid argument (since it's all at zero), but my guess seems to be true from my observations. Is it, and how would you prove it in that case? edit: by ""grows faster"" I guess I mean as x→∞, the ratio of f(x) / $e^{kx}$ approaches infinity. P.S. I apologise if this is a well-known function or if I screwed up the latex formatting.","I spontaneously thought of this function while messing with the power series for exp(x). It's fairly obvious that this function grows slower than exp(x) and evaluating further derivatives at x=0 seems to indicate that it grows faster than all functions (for 0<k<1), as their derivatives follow , , , etc, while derivatives of f(x) are 1, 1/2, 1/3 etc. The former derivatives shrink faster than the latter. I think this is not really a valid argument (since it's all at zero), but my guess seems to be true from my observations. Is it, and how would you prove it in that case? edit: by ""grows faster"" I guess I mean as x→∞, the ratio of f(x) / approaches infinity. P.S. I apologise if this is a well-known function or if I screwed up the latex formatting.",f\left(x\right)=\lim_{a\to\infty}\sum_{n=1}^{a}\frac{x^{n}}{n\cdot n!} e^{kx} k k^2 k^3 e^{kx},"['real-analysis', 'calculus', 'limits', 'exponential-function']"
56,If $\int_a^b f(x) \ dx > 0$ then $f(x) > 0$ a.e?,If  then  a.e?,\int_a^b f(x) \ dx > 0 f(x) > 0,"Let $f$ be a measurable nonnegative function with domain $\mathbb{R}$ , such that \begin{equation} \int_a^b f(x) \ dx > 0 \end{equation} when $a < b$ . Can we say that $f>0$ almost everywhere? (or $f\neq 0$ almost everywhere, because is a nonnegative function) We work with Lebesgue measure and the integral above is a Lebesgue integral. I'm thinking that the answer is yes, but I can't prove it. I tried to get a contradiction if I suppose that $m(\{ f=0 \}) > 0$ , or maybe a subset of $\{ f=0 \}$ with measure greater than $0$ , but I got stuck. I already showed that for any open set $G$ we have \begin{equation} \int_G f(x) \ dx > 0 \end{equation} So maybe I could find an open set $G$ such that $m(G) = m(\{ f=0 \})$ , but I can't. And that's it, I have no more ideas :( Thanks","Let be a measurable nonnegative function with domain , such that when . Can we say that almost everywhere? (or almost everywhere, because is a nonnegative function) We work with Lebesgue measure and the integral above is a Lebesgue integral. I'm thinking that the answer is yes, but I can't prove it. I tried to get a contradiction if I suppose that , or maybe a subset of with measure greater than , but I got stuck. I already showed that for any open set we have So maybe I could find an open set such that , but I can't. And that's it, I have no more ideas :( Thanks","f \mathbb{R} \begin{equation}
\int_a^b f(x) \ dx > 0
\end{equation} a < b f>0 f\neq 0 m(\{ f=0 \}) > 0 \{ f=0 \} 0 G \begin{equation}
\int_G f(x) \ dx > 0
\end{equation} G m(G) = m(\{ f=0 \})","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
57,Is uniform convergence a necessary condition for interchangeability of limits,Is uniform convergence a necessary condition for interchangeability of limits,,"I am aware of the standard result (namely, please see Terence Tao's Analysis II, Page 54, Proposition 3.3.3 - Interchange of limits & uniform limits ) that uniform convergence implies the interchangeability of limits, however I am wondering if the converse holds. i.e. If we can interchange the limits of a function, does it imply the uniform convergence of this function please? I don’t think it holds but I am struggling to construct a counterexample. Many thanks in advance!","I am aware of the standard result (namely, please see Terence Tao's Analysis II, Page 54, Proposition 3.3.3 - Interchange of limits & uniform limits ) that uniform convergence implies the interchangeability of limits, however I am wondering if the converse holds. i.e. If we can interchange the limits of a function, does it imply the uniform convergence of this function please? I don’t think it holds but I am struggling to construct a counterexample. Many thanks in advance!",,"['real-analysis', 'uniform-convergence']"
58,How are the following integrals equivalent?,How are the following integrals equivalent?,,"Several textbooks I've read make the claim that for any $f: \mathbb{R} \to \mathbb{R}$ $$ \frac{1}{k!}\int_{0}^{\infty} x^k f(x) dx =\int_{[0,\infty)^{k+1}} f(x_1+x_2+\dots + x_{k+1}) \ dx_1 \dots dx_{k+1}  $$ where we assume both sides of the equality are finite (e.g $f$ is continuous and compactly supported). I would preferably like to start with the left hand side (LHS) and obtain the right hand side (RHS) because the LHS is the ""natural"" integral to look at in the context of these textbooks. However, given that this equality is true, it seems easier to start with the right hand side. (If you have a motivated proof that starts with the LHS and gets to the RHS I would love to see that). Starting with the RHS, we can substitute $ x = x_1 + \dots x_k + x_{k+1}$ and have $dx = dx_k $ . Therefore, $$\int_{[0,\infty)^{k+1}} f(x_1+x_2+\dots + x_{k+1}) \ dx_1 \dots dx_{k+1} = \int_{[0,\infty)^k} \int_{x_1+\dots + x_k}^{\infty} f(x) \ dx   \ dx_1 \dots dx_k$$ We are integrating over the set $\{ x \in \mathbb{R} \ \text{and}\  x_i \in \mathbb{R}| 0 \leq x_i \leq \infty \ \text{and} \ (x_1 + \dots + x_k) \leq x \leq \infty \} $ I was thinking of changing the order of integration because that worked in the case $k=1$ , but I'm running into trouble with doing that for general $k$ . Any ideas?","Several textbooks I've read make the claim that for any where we assume both sides of the equality are finite (e.g is continuous and compactly supported). I would preferably like to start with the left hand side (LHS) and obtain the right hand side (RHS) because the LHS is the ""natural"" integral to look at in the context of these textbooks. However, given that this equality is true, it seems easier to start with the right hand side. (If you have a motivated proof that starts with the LHS and gets to the RHS I would love to see that). Starting with the RHS, we can substitute and have . Therefore, We are integrating over the set I was thinking of changing the order of integration because that worked in the case , but I'm running into trouble with doing that for general . Any ideas?","f: \mathbb{R} \to \mathbb{R}  \frac{1}{k!}\int_{0}^{\infty} x^k f(x) dx =\int_{[0,\infty)^{k+1}} f(x_1+x_2+\dots + x_{k+1}) \ dx_1 \dots dx_{k+1}   f  x = x_1 + \dots x_k + x_{k+1} dx = dx_k  \int_{[0,\infty)^{k+1}} f(x_1+x_2+\dots + x_{k+1}) \ dx_1 \dots dx_{k+1} = \int_{[0,\infty)^k} \int_{x_1+\dots + x_k}^{\infty} f(x) \ dx   \ dx_1 \dots dx_k \{ x \in \mathbb{R} \ \text{and}\  x_i \in \mathbb{R}| 0 \leq x_i \leq \infty \ \text{and} \ (x_1 + \dots + x_k) \leq x \leq \infty \}  k=1 k","['real-analysis', 'linear-algebra', 'integration', 'multivariable-calculus', 'multiple-integral']"
59,"If $f(x)$ is analytic and non-negative, does convergence of $\int_a^{\infty} f(x)\,dx$ imply $\lim_{x\to\infty} f(x)=0$?","If  is analytic and non-negative, does convergence of  imply ?","f(x) \int_a^{\infty} f(x)\,dx \lim_{x\to\infty} f(x)=0","It is well known that improper integrals don't have to satisfy $\lim_{x\to\infty} f(x)=0$ in order for $\int_a^{\infty} f(x)\,dx$ to converge, for instance $f(x)=\sin(x^2)$ . It is also possible to construct such non-negative $f(x)$ , and even continuous and non-negative, by taking $f(x)$ to be $0$ everywhere except for triangular spikes with appropriate converging areas. I was wondering whether we can get such $f(x)$ which is smooth or even analytic. It seems clear that it should be possible to modify the triangular spikes example and get such a smooth $f(x)$ by using bump functions. However, what if we wish for $f(x)$ to be analytic? Bump functions no longer come to the rescue. To summarize: does there exist a non-negative $f(x)$ which is analytic in $[a,\infty)$ and such that $\int_a^{\infty} f(x)\,dx$ converges, but such that $f(x)$ does not converge to $0$ at infinity?","It is well known that improper integrals don't have to satisfy in order for to converge, for instance . It is also possible to construct such non-negative , and even continuous and non-negative, by taking to be everywhere except for triangular spikes with appropriate converging areas. I was wondering whether we can get such which is smooth or even analytic. It seems clear that it should be possible to modify the triangular spikes example and get such a smooth by using bump functions. However, what if we wish for to be analytic? Bump functions no longer come to the rescue. To summarize: does there exist a non-negative which is analytic in and such that converges, but such that does not converge to at infinity?","\lim_{x\to\infty} f(x)=0 \int_a^{\infty} f(x)\,dx f(x)=\sin(x^2) f(x) f(x) 0 f(x) f(x) f(x) f(x) [a,\infty) \int_a^{\infty} f(x)\,dx f(x) 0","['real-analysis', 'calculus', 'improper-integrals', 'riemann-integration', 'analytic-functions']"
60,Uniform continuity and derivatives [closed],Uniform continuity and derivatives [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Is there any example of a three times differentiable and uniformly continuous function $f: \Bbb R \rightarrow \Bbb R$ such that $f^{(3)}$ is bounded but $f^{(2)}$ isn't?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Is there any example of a three times differentiable and uniformly continuous function such that is bounded but isn't?",f: \Bbb R \rightarrow \Bbb R f^{(3)} f^{(2)},"['real-analysis', 'functions', 'derivatives', 'continuity', 'examples-counterexamples']"
61,"Proof verification: Given $\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=0$ and $\lim_{x\to 0} f(x)=0$, show that $\lim _{x\to 0}\frac {f(x)}{x}=0$.","Proof verification: Given  and , show that .",\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=0 \lim_{x\to 0} f(x)=0 \lim _{x\to 0}\frac {f(x)}{x}=0,"Given that $\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=0$ and $\lim_{x\to 0} f(x)=0$ , it is to be proven that $\lim _{x\to 0}\frac {f(x)}{x}=0$ . Proof: Let $\epsilon\gt 0$ be fixed. \begin{align*} \frac {f(x)}x&=\sum_{k=0}^n\color{blue}{\frac{f(x/2^k)-f(x/2^{k+1})}{x/2^{k+1}}}(1/2^{k+1})+\frac{\color{green}{f(x/2^{n+1})}}{x}\\ &=\sum_{k=0}^n\color{blue}{g_k(x)}(1/2^{k+1})+\frac{\color{green}{h_n(x)}}{x}\\ \text{There exists $N$ such that }\\ \left|\frac {f(x)}x\right|&\le \sum_{k=0}^N|g_k(x)|\frac 1{2^{k+1}}+\epsilon\sum_{k=N+1}^\infty\frac 1{2^{k+1}}+\frac 1{|x|}\epsilon|x|\\ \text{It follows that } \\&0\le\liminf \left|\frac {f(x)}x\right|\le \limsup\left|\frac {f(x)}x\right|\le \epsilon\sum_{k=N+1}^\infty\frac 1{2^{k+1}}+\epsilon\\ \text{Taking $N\to \infty$,it follows that}\\&0\le\liminf \left|\frac {f(x)}x\right|\le \limsup\left|\frac {f(x)}x\right|\le \epsilon\tag 1 \end{align*} Since $\epsilon\gt 0$ is arbitrary, it follows by $(1)$ that $\liminf \left|\frac {f(x)}x\right|= \limsup\left|\frac {f(x)}x\right|=0=\lim_{x\to 0} \frac{f(x)}x. \;\;\; \blacksquare$ Is my proof correct? Thanks.","Given that and , it is to be proven that . Proof: Let be fixed. Since is arbitrary, it follows by that Is my proof correct? Thanks.","\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=0 \lim_{x\to 0} f(x)=0 \lim _{x\to 0}\frac {f(x)}{x}=0 \epsilon\gt 0 \begin{align*}
\frac {f(x)}x&=\sum_{k=0}^n\color{blue}{\frac{f(x/2^k)-f(x/2^{k+1})}{x/2^{k+1}}}(1/2^{k+1})+\frac{\color{green}{f(x/2^{n+1})}}{x}\\
&=\sum_{k=0}^n\color{blue}{g_k(x)}(1/2^{k+1})+\frac{\color{green}{h_n(x)}}{x}\\
\text{There exists N such that }\\ \left|\frac {f(x)}x\right|&\le \sum_{k=0}^N|g_k(x)|\frac 1{2^{k+1}}+\epsilon\sum_{k=N+1}^\infty\frac 1{2^{k+1}}+\frac 1{|x|}\epsilon|x|\\
\text{It follows that } \\&0\le\liminf \left|\frac {f(x)}x\right|\le \limsup\left|\frac {f(x)}x\right|\le \epsilon\sum_{k=N+1}^\infty\frac 1{2^{k+1}}+\epsilon\\
\text{Taking N\to \infty,it follows that}\\&0\le\liminf \left|\frac {f(x)}x\right|\le \limsup\left|\frac {f(x)}x\right|\le \epsilon\tag 1
\end{align*} \epsilon\gt 0 (1) \liminf \left|\frac {f(x)}x\right|= \limsup\left|\frac {f(x)}x\right|=0=\lim_{x\to 0} \frac{f(x)}x. \;\;\; \blacksquare","['real-analysis', 'calculus', 'limits', 'solution-verification']"
62,Is there a direct proof of the intermediate value theorem?,Is there a direct proof of the intermediate value theorem?,,"I am interested in proving a weak version of the intermediate value theorem, specifically: Suppose that $f$ is continuous on $[a,b]$ , and $f(a)<0<f(b)$ . Then, there exists a number $x\in(a,b)$ such that $f(x)=0$ . The usual proof of this uses contradiction. I am curious as to whether there is a proof that doesn't use contradiction, and also doesn't require too much machinery. For reference, here is the argument presented in Michael Spivak's Calculus . We need the following lemma, which can be easily proven using the epsilon-delta definition of the limit: Suppose that $\lim_{x \to \lambda}f(x)=f(\lambda)$ , and $f(\lambda)<0$ . Then, there is a $\delta>0$ such that if $|x-\lambda|<\delta$ , then $f(x)<0$ . Similarly, if $\lim_{x\to\lambda}f(x)=f(\lambda)$ and $f(\lambda)>0$ , then there is an open interval containing $\lambda$ for which $f$ is positive. This lemma also applies to ""one-sided continuity"": if $\lim_{x \to a^+}f(x)=f(a)$ and $f(a)<0$ , then there is a $\delta>0$ such that if $0\le x-a<\delta$ , then $f(x)<0$ ; likewise, if $\lim_{x \to b^-}f(x)=f(b)$ and $f(b)>0$ , then there is a $\delta>0$ such that if $0\le b-x<\delta$ , then $f(x)>0$ . Consider the set $$E=\{x\in[a,b]:\text{$f$ is negative on $[a,x]$}\} \, .$$ From the above lemma we know that $E$ contains values greater than $a$ , and that all points sufficiently close to $b$ are not in $E$ . Since the real numbers satisfy the least upper bound property, we know that $\sup E$ exists, and we will denote it as $\alpha$ . Note that $\alpha$ satisfies $a<\alpha<b$ . We can prove that $f(\alpha)=0$ by contradiction. If $f(\alpha)$ were smaller than $0$ , then because $f$ is continuous at $\alpha$ , there would be a $\delta>0$ such that if $|x-\alpha|<\delta$ then $f(x)<0$ . This means that there would be numbers $x>\alpha$ such that $x\in A$ , contradicting the fact that $\alpha$ is an upper bound of $E$ . Similarly, if $f(\alpha)$ were greater than $0$ , then there would be a $\delta>0$ such that if $|x-\alpha|<\delta$ , then $f(x)>0$ . This means that there would be numbers $x<\alpha$ that are not in $E$ , contradicting the fact that $\alpha$ is the least upper bound of $E$ . So $f(\alpha)$ must be equal to $0$ , and since $a<\alpha<b$ , the theorem is proven. As you can see, this proof functions by ruling out the possibilities $f(\alpha)>0$ and $f(\alpha)<0$ using contradiction. Is there a way of directly showing that $f(\alpha)=0$ instead?","I am interested in proving a weak version of the intermediate value theorem, specifically: Suppose that is continuous on , and . Then, there exists a number such that . The usual proof of this uses contradiction. I am curious as to whether there is a proof that doesn't use contradiction, and also doesn't require too much machinery. For reference, here is the argument presented in Michael Spivak's Calculus . We need the following lemma, which can be easily proven using the epsilon-delta definition of the limit: Suppose that , and . Then, there is a such that if , then . Similarly, if and , then there is an open interval containing for which is positive. This lemma also applies to ""one-sided continuity"": if and , then there is a such that if , then ; likewise, if and , then there is a such that if , then . Consider the set From the above lemma we know that contains values greater than , and that all points sufficiently close to are not in . Since the real numbers satisfy the least upper bound property, we know that exists, and we will denote it as . Note that satisfies . We can prove that by contradiction. If were smaller than , then because is continuous at , there would be a such that if then . This means that there would be numbers such that , contradicting the fact that is an upper bound of . Similarly, if were greater than , then there would be a such that if , then . This means that there would be numbers that are not in , contradicting the fact that is the least upper bound of . So must be equal to , and since , the theorem is proven. As you can see, this proof functions by ruling out the possibilities and using contradiction. Is there a way of directly showing that instead?","f [a,b] f(a)<0<f(b) x\in(a,b) f(x)=0 \lim_{x \to \lambda}f(x)=f(\lambda) f(\lambda)<0 \delta>0 |x-\lambda|<\delta f(x)<0 \lim_{x\to\lambda}f(x)=f(\lambda) f(\lambda)>0 \lambda f \lim_{x \to a^+}f(x)=f(a) f(a)<0 \delta>0 0\le x-a<\delta f(x)<0 \lim_{x \to b^-}f(x)=f(b) f(b)>0 \delta>0 0\le b-x<\delta f(x)>0 E=\{x\in[a,b]:\text{f is negative on [a,x]}\} \, . E a b E \sup E \alpha \alpha a<\alpha<b f(\alpha)=0 f(\alpha) 0 f \alpha \delta>0 |x-\alpha|<\delta f(x)<0 x>\alpha x\in A \alpha E f(\alpha) 0 \delta>0 |x-\alpha|<\delta f(x)>0 x<\alpha E \alpha E f(\alpha) 0 a<\alpha<b f(\alpha)>0 f(\alpha)<0 f(\alpha)=0","['real-analysis', 'calculus', 'continuity', 'complete-spaces']"
63,Calculating a limit. Is WolframAlpha wrong or am I wrong?,Calculating a limit. Is WolframAlpha wrong or am I wrong?,,"What I'm trying to solve: $$\lim _{x\to -\infty \:}\frac{\left(\sqrt{\left(x^2+14\right)}+x\right)}{\left(\sqrt{\left(x^2-2\right)}+x\right)}$$ What I put into WolframAlpha: (sqrt(x^2+14)+x)/(sqrt(x^2-2)+x) My result: $1$ , which I get by simply dividing bot the numerator and the denominator by $x$ , then letting it go towards $-\infty$ $$\frac{\frac{\left(\sqrt{x^2+14}+x\right)}{x}}{\frac{\left(\sqrt{x^2-2}+x\right)}{x}}=\frac{\left(\sqrt{\frac{x^2}{x^2}+\frac{14}{x^2}}+\frac{x}{x}\right)}{\left(\sqrt{\frac{x^2}{x^2}-\frac{2}{x^2}}+\frac{x}{x}\right)}=\frac{\left(\sqrt{1+\frac{14}{x^2}}+1\right)}{\left(\sqrt{1-\frac{2}{x^2}}+1\right)}\:=\:\frac{\left(\sqrt{1}+1\right)}{\left(\sqrt{1}+1\right)}\:=\:\frac22 \ = \ 1$$ WolframAlpha's result: $-7$ . It has a long, complicated 25 step solution. Is the solution $1$ , $-7$ or neither? Edit: of course, I set it $x$ to go towards $-\infty$ in WolframAlpha too","What I'm trying to solve: What I put into WolframAlpha: (sqrt(x^2+14)+x)/(sqrt(x^2-2)+x) My result: , which I get by simply dividing bot the numerator and the denominator by , then letting it go towards WolframAlpha's result: . It has a long, complicated 25 step solution. Is the solution , or neither? Edit: of course, I set it to go towards in WolframAlpha too",\lim _{x\to -\infty \:}\frac{\left(\sqrt{\left(x^2+14\right)}+x\right)}{\left(\sqrt{\left(x^2-2\right)}+x\right)} 1 x -\infty \frac{\frac{\left(\sqrt{x^2+14}+x\right)}{x}}{\frac{\left(\sqrt{x^2-2}+x\right)}{x}}=\frac{\left(\sqrt{\frac{x^2}{x^2}+\frac{14}{x^2}}+\frac{x}{x}\right)}{\left(\sqrt{\frac{x^2}{x^2}-\frac{2}{x^2}}+\frac{x}{x}\right)}=\frac{\left(\sqrt{1+\frac{14}{x^2}}+1\right)}{\left(\sqrt{1-\frac{2}{x^2}}+1\right)}\:=\:\frac{\left(\sqrt{1}+1\right)}{\left(\sqrt{1}+1\right)}\:=\:\frac22 \ = \ 1 -7 1 -7 x -\infty,"['real-analysis', 'calculus', 'limits', 'wolfram-alpha']"
64,Applications of Riemann Series Theorem,Applications of Riemann Series Theorem,,"I recently came across Riemann Series Theorem. The theorem seems to be quite general and powerful, making strong statements on the limsup and liminf of rearrangements of conditionally convergent series (specifically that the limsup and liminf can take any arbitrary value). Consequently, I would imagine that it has lots of applications to interesting problems, in proving theorems etc. A cursory search of this site revealed very few such answers and my book does not list any interesting problems. So, I am looking for problems/theorems which can be easily answered/proved using Riemann series theorem.","I recently came across Riemann Series Theorem. The theorem seems to be quite general and powerful, making strong statements on the limsup and liminf of rearrangements of conditionally convergent series (specifically that the limsup and liminf can take any arbitrary value). Consequently, I would imagine that it has lots of applications to interesting problems, in proving theorems etc. A cursory search of this site revealed very few such answers and my book does not list any interesting problems. So, I am looking for problems/theorems which can be easily answered/proved using Riemann series theorem.",,"['real-analysis', 'sequences-and-series', 'examples-counterexamples', 'big-list']"
65,Every set in a sigma-algebra not measurable?,Every set in a sigma-algebra not measurable?,,"I'm studying Axler's measure theory book and there a set is said to be measurable if it belongs to a $\sigma$ -algebra. A power set is always a $\sigma$ -algebra, and combining these, it would seem that every set in $\mathcal{P}(\mathbb R)$ is measurable, which is not true. What am i not understanding, why don't these definitions contradict each other?","I'm studying Axler's measure theory book and there a set is said to be measurable if it belongs to a -algebra. A power set is always a -algebra, and combining these, it would seem that every set in is measurable, which is not true. What am i not understanding, why don't these definitions contradict each other?",\sigma \sigma \mathcal{P}(\mathbb R),"['real-analysis', 'measure-theory', 'real-numbers']"
66,An extension of Baire's category theorem,An extension of Baire's category theorem,,"In a topological space, a set is said to be rare if its closure has empty interior, and a set is said to be meager if it is a countable union of rare sets. If meager sets all have empty interior, then the topological space is said to be a Baire space. The following fact is known as Baire's category theorem. Theorem. Complete metric spaces and locally compact Hausdorff spaces are Baire spaces. [Zălinescu 2002] provides the following extension of this theorem for complete metric spaces, attributing it to C. Ursescu . Theorem. ([Zălinescu 2002, Theorem 1.4.5]) Let $X$ be a complete metric space, and $\{S_n\}$ be a sequence of open sets in $X$ . Then $\mathrm{cl}(\cap_{n=1}^\infty S_n)$ and $\cap_{n=1}^\infty \mathrm{cl}(S_n)$ have the same interior. It is easy to check that a topological space is a Baire space if and only if any countable intersection of dense open sets is still dense in this space. Consequently, the theorem above implies that complete metric spaces are Baire spaces. Thus it is considered as an extension. Question. Does the extension hold for locally compact Hausdorff spaces? Update: As pointed out by @Alex Kruckman, the property in Ursescu's theorem is indeed equivalent to the fact that $X$ is a Baire space. Theorem. A topological space is a Baire space if and only if $\mathrm{cl}(\cap_{n=1}^\infty S_n)$ and $\cap_{n=1}^\infty \mathrm{cl}(S_n)$ have the same interior for any sequence of open sets $\{S_n\}$ . Recall that a space is a Baire space if and only if any countable intersection of dense open sets is still dense. The theorem is essentially another way to state that a space is a Baire space if and only if all its open subspaces are Baire spaces. Combined with Baire's category theorem, this observation by Alex proves the desired theorem. My answer below proves it from scratch, the proof being essentially the same as that of Baire's category theorem for locally compact Hausdorff spaces. I will accept Alex's answer a few days later if nobody else has anything to add. Thanks.","In a topological space, a set is said to be rare if its closure has empty interior, and a set is said to be meager if it is a countable union of rare sets. If meager sets all have empty interior, then the topological space is said to be a Baire space. The following fact is known as Baire's category theorem. Theorem. Complete metric spaces and locally compact Hausdorff spaces are Baire spaces. [Zălinescu 2002] provides the following extension of this theorem for complete metric spaces, attributing it to C. Ursescu . Theorem. ([Zălinescu 2002, Theorem 1.4.5]) Let be a complete metric space, and be a sequence of open sets in . Then and have the same interior. It is easy to check that a topological space is a Baire space if and only if any countable intersection of dense open sets is still dense in this space. Consequently, the theorem above implies that complete metric spaces are Baire spaces. Thus it is considered as an extension. Question. Does the extension hold for locally compact Hausdorff spaces? Update: As pointed out by @Alex Kruckman, the property in Ursescu's theorem is indeed equivalent to the fact that is a Baire space. Theorem. A topological space is a Baire space if and only if and have the same interior for any sequence of open sets . Recall that a space is a Baire space if and only if any countable intersection of dense open sets is still dense. The theorem is essentially another way to state that a space is a Baire space if and only if all its open subspaces are Baire spaces. Combined with Baire's category theorem, this observation by Alex proves the desired theorem. My answer below proves it from scratch, the proof being essentially the same as that of Baire's category theorem for locally compact Hausdorff spaces. I will accept Alex's answer a few days later if nobody else has anything to add. Thanks.",X \{S_n\} X \mathrm{cl}(\cap_{n=1}^\infty S_n) \cap_{n=1}^\infty \mathrm{cl}(S_n) X \mathrm{cl}(\cap_{n=1}^\infty S_n) \cap_{n=1}^\infty \mathrm{cl}(S_n) \{S_n\},"['real-analysis', 'general-topology', 'functional-analysis', 'baire-category']"
67,How to find $\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}$ and $\sum_{n=1}^\infty\frac{(-1)^nH_{2n}^{(2)}}{n^2}$ using real methods?,How to find  and  using real methods?,\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3} \sum_{n=1}^\infty\frac{(-1)^nH_{2n}^{(2)}}{n^2},"How to calculate $$\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}$$ and $$\sum_{n=1}^\infty\frac{(-1)^nH_{2n}^{(2)}}{n^2}$$ by means of real methods? This question was suggested by Cornel the author of the book, Almost Impossible Integrals, Sums and Series . The way I would approach the problem is to use the series property: $$\sum_{n=1}^\infty (-1)^n f(2n)=\Re \sum_{n=1}^\infty i^n f(n),$$ namely $$\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}=8\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{(2n)^3}=8\Re\sum_{n=1}^\infty\frac{i^n H_n}{n^3}$$ then we use the well-known generating function $\sum_{n=1}^\infty\frac{x^nH_n}{n^3}$ . But this method is really tedious as we will need to use $\Re\{\text{Li}_2(1+i), \text{Li}_3(1+i),\text{Li}_4(1+i),\text{Li}_4(\frac{1+i}{2})\}.$ By the way, I have not seen a rigorous proof of the following equality : $$\operatorname{Re} \operatorname{Li}_4 (1 + i)= -\frac{5}{16} \operatorname{Li}_4 \left (\frac{1}{2} \right ) + \frac{97}{9216} \pi^4 + \frac{\pi^2}{48} \ln^2 2 - \frac{5}{384} \ln^4 2\tag1$$ So solving this sum in a different way would be considered a new rigorous proof of $(1)$ . For the second series, I would follow the same approach. Any idea by real methods? Thanks","How to calculate and by means of real methods? This question was suggested by Cornel the author of the book, Almost Impossible Integrals, Sums and Series . The way I would approach the problem is to use the series property: namely then we use the well-known generating function . But this method is really tedious as we will need to use By the way, I have not seen a rigorous proof of the following equality : So solving this sum in a different way would be considered a new rigorous proof of . For the second series, I would follow the same approach. Any idea by real methods? Thanks","\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3} \sum_{n=1}^\infty\frac{(-1)^nH_{2n}^{(2)}}{n^2} \sum_{n=1}^\infty (-1)^n f(2n)=\Re \sum_{n=1}^\infty i^n f(n), \sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{n^3}=8\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{(2n)^3}=8\Re\sum_{n=1}^\infty\frac{i^n H_n}{n^3} \sum_{n=1}^\infty\frac{x^nH_n}{n^3} \Re\{\text{Li}_2(1+i), \text{Li}_3(1+i),\text{Li}_4(1+i),\text{Li}_4(\frac{1+i}{2})\}. \operatorname{Re} \operatorname{Li}_4 (1 + i)= -\frac{5}{16} \operatorname{Li}_4 \left (\frac{1}{2} \right ) + \frac{97}{9216} \pi^4 + \frac{\pi^2}{48} \ln^2 2 - \frac{5}{384} \ln^4 2\tag1 (1)","['real-analysis', 'integration', 'sequences-and-series', 'harmonic-numbers', 'polylogarithm']"
68,If $\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0$ then $E[X] < \infty$?,If  then ?,\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0 E[X] < \infty,"Let $X$ be a positive random variable. Suppose that $\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0$ Does this implies that $X$ has finite expectation? that is $E[X] < \infty $ I know that if $E[X] < \infty$ $\Rightarrow$ $\lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0$ (For any positive random variable see: Expected value as integral of survival function ) , so I was wondering if the converse is true. I have also tried to think in a counterexample but unfortunately I have not been successfull. I would really appreciate any hints or suggestions with this problem.","Let be a positive random variable. Suppose that Does this implies that has finite expectation? that is I know that if (For any positive random variable see: Expected value as integral of survival function ) , so I was wondering if the converse is true. I have also tried to think in a counterexample but unfortunately I have not been successfull. I would really appreciate any hints or suggestions with this problem.",X \lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0 X E[X] < \infty  E[X] < \infty \Rightarrow \lim_{\alpha \to \infty}\alpha P[X > \alpha] = 0,"['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'probability-distributions']"
69,Fourier transform with different measure,Fourier transform with different measure,,"I am interested in the following question: can we generalize Fourier theory to different $L^2(\mu)$ spaces, not just the Lebesgue measure? For example on $\mathbb{R}$ , defining the Fourier transform of $f$ as $$ \mathcal{F}[f](\omega)=\int_\mathbb{R}f(x)e^{-i\omega x}d\mu(x). $$ Is there literature on this subject? Do any properties hold with this (in particular Plancherel’s theorem)?  I have tried to look this idea up online but found nothing. I am a beginner when it comes to Fourier theory, I apologize if it is a simple question.","I am interested in the following question: can we generalize Fourier theory to different spaces, not just the Lebesgue measure? For example on , defining the Fourier transform of as Is there literature on this subject? Do any properties hold with this (in particular Plancherel’s theorem)?  I have tried to look this idea up online but found nothing. I am a beginner when it comes to Fourier theory, I apologize if it is a simple question.",L^2(\mu) \mathbb{R} f  \mathcal{F}[f](\omega)=\int_\mathbb{R}f(x)e^{-i\omega x}d\mu(x). ,"['real-analysis', 'measure-theory', 'fourier-analysis', 'harmonic-analysis']"
70,Simple proof that the Airy function decays faster than any polynomial,Simple proof that the Airy function decays faster than any polynomial,,"My question concerns the Airy function $\mbox{Ai} (\cdot)$ , which is the solution of the Airy equation $$y'' = x y$$ that decays exponentially as $x \to \infty$ . I have read about how the asymptotic expression for $\mbox{Ai} (\cdot)$ is obtained. Comparing the Airy equation above with the much simpler $y'' = Ay$ (for constant $A>0$ ), and its decreasing solution - namely $y(x) = e^{-\sqrt{A}x}$ , I have an intuition for why Ai $(\cdot)$ should decay exponentially as $x \rightarrow \infty$ . So my question is: Without getting into the asymptotic analysis, is there a more direct way to say that Ai $(\cdot) \rightarrow 0$ as $x \rightarrow \infty$ faster than any polynomial? I am expecting that the role of $f(x) = x$ in the Airy equation is just that of a function which increases to $\infty$ as $x \rightarrow \infty$ . So a related question is: Can we make a similar conclusion for a decreasing solution of \begin{align*} y''(x) = f(x) y(x)\text{ and }y(0)=1, \end{align*} where $f>0$ is an increasing function such that $\lim_{x \to \infty} f(x) = \infty$ ? Thanks!","My question concerns the Airy function , which is the solution of the Airy equation that decays exponentially as . I have read about how the asymptotic expression for is obtained. Comparing the Airy equation above with the much simpler (for constant ), and its decreasing solution - namely , I have an intuition for why Ai should decay exponentially as . So my question is: Without getting into the asymptotic analysis, is there a more direct way to say that Ai as faster than any polynomial? I am expecting that the role of in the Airy equation is just that of a function which increases to as . So a related question is: Can we make a similar conclusion for a decreasing solution of where is an increasing function such that ? Thanks!","\mbox{Ai} (\cdot) y'' = x y x \to \infty \mbox{Ai} (\cdot) y'' = Ay A>0 y(x) = e^{-\sqrt{A}x} (\cdot) x \rightarrow \infty (\cdot) \rightarrow 0 x \rightarrow \infty f(x) = x \infty x \rightarrow \infty \begin{align*}
y''(x) = f(x) y(x)\text{ and }y(0)=1,
\end{align*} f>0 \lim_{x \to \infty} f(x) = \infty","['real-analysis', 'ordinary-differential-equations', 'analysis']"
71,"Inverse Function Theorem for functions $f(x,y)$ and $\int\limits_0^1\frac{\partial f}{\partial x}(tx,y)dt$",Inverse Function Theorem for functions  and,"f(x,y) \int\limits_0^1\frac{\partial f}{\partial x}(tx,y)dt","I'm struggling with the following problem: Let $f\colon\mathbb{R}^2\to\mathbb{R}$ be a twice continuously   differentiable function satisfying $$f(0,y)=0\mbox{ for  all }y\in\mathbb{R}$$ (a) Show that $f(x,y) = xg(x,y)$ for all pairs $(x,y)\in\mathbb{R}^2$ ,    where $g$ is the function given by $$g(x,y) =  \int\limits_0^1\frac{\partial f}{\partial x}(tx,y)dt$$ (b) Show that $g$ is continuously differentiable and that for all $x\in\mathbb{R}$ $$g(0,y) = \frac{\partial f}{\partial  x}(0,y),~\frac{\partial g}{\partial y}(0,y) = \frac{\partial^2  f}{\partial x\partial y}(0,y)$$ (c) If $\frac{\partial f}{\partial x}(0,0)\neq0$ there is a   neighborhood $V$ of $(0,0)$ in $\mathbb{R}^2$ such that $f^{-1}(0)\cap  V = V\cap \{x=0\}$ (d) If $\frac{\partial f}{\partial x}(0,0) = 0$ and $\frac{\partial^2  f}{\partial x\partial y}(0,0)\neq0$ there is a neighborhood $V$ of $(0,0)$ in $\mathbb{R}^2$ such that $f^{-1}(0)\cap V$ consists of the   union of the set $V\cap\{x=0\}$ with a curve through $(0,0)$ , whose   tangent at $(0,0)$ is not vertical (not parallel to the $y$ -axis) Here are my attempts: (a) Note that $\frac{\partial f}{\partial x}(tx,y) = \frac{t}{x}\cdot\frac{\partial f}{\partial t}(tx,y)$ and so using the integration by parts $$g(x,y) = \int\limits_0^1\frac{\partial f}{\partial x}(tx,y)dt = \int\limits_0^1 \frac{t}{x}\cdot\frac{\partial f}{\partial t}(tx,y)dt = \frac{1}{x}\left(f(x,y) - \int\limits_0^1 f(tx,y)dt\right)$$ it suffices to show that $\int\limits_0^1 f(tx,y)dt = 0$ , however, I can't see how to proceed with it. (b) Expressions for $g(0,y)$ and $\frac{\partial g}{\partial y}(0,y)$ is the result of a straightforward calculation using the definiton of $g(x,y)$ , but I have troubles showing that $g$ is continuously differentiable. What am I supposed to do -- prove that the partial derivatives exist and they are continuous? If so, can someone write down the details? (c) One can fix $y=0$ and consider a function $f(x,0)\colon\mathbb{R}\to\mathbb{R}$ . Then, since $\frac{\partial f}{\partial x}(0,0)\neq0$ , the Inverse Function Theorem says that there exists a neighborhood $U$ of $0\in\mathbb{R}$ such that $f\colon U\to f(U)$ is the bijection. Denote $V = U\times\{0\}$ , so $V\subset\mathbb{R}^2$ and we are done. Am I right here? (d) It seems like using the result of part (b) we can do the same trick with the function $g(0,y)\colon\mathbb{R}\to\mathbb{R}$ , but I can't finish the proof. Any help will be really appreciated. Thanks a lot in advance.","I'm struggling with the following problem: Let be a twice continuously   differentiable function satisfying (a) Show that for all pairs ,    where is the function given by (b) Show that is continuously differentiable and that for all (c) If there is a   neighborhood of in such that (d) If and there is a neighborhood of in such that consists of the   union of the set with a curve through , whose   tangent at is not vertical (not parallel to the -axis) Here are my attempts: (a) Note that and so using the integration by parts it suffices to show that , however, I can't see how to proceed with it. (b) Expressions for and is the result of a straightforward calculation using the definiton of , but I have troubles showing that is continuously differentiable. What am I supposed to do -- prove that the partial derivatives exist and they are continuous? If so, can someone write down the details? (c) One can fix and consider a function . Then, since , the Inverse Function Theorem says that there exists a neighborhood of such that is the bijection. Denote , so and we are done. Am I right here? (d) It seems like using the result of part (b) we can do the same trick with the function , but I can't finish the proof. Any help will be really appreciated. Thanks a lot in advance.","f\colon\mathbb{R}^2\to\mathbb{R} f(0,y)=0\mbox{ for
 all }y\in\mathbb{R} f(x,y) = xg(x,y) (x,y)\in\mathbb{R}^2 g g(x,y) =
 \int\limits_0^1\frac{\partial f}{\partial x}(tx,y)dt g x\in\mathbb{R} g(0,y) = \frac{\partial f}{\partial
 x}(0,y),~\frac{\partial g}{\partial y}(0,y) = \frac{\partial^2
 f}{\partial x\partial y}(0,y) \frac{\partial f}{\partial x}(0,0)\neq0 V (0,0) \mathbb{R}^2 f^{-1}(0)\cap
 V = V\cap \{x=0\} \frac{\partial f}{\partial x}(0,0) = 0 \frac{\partial^2
 f}{\partial x\partial y}(0,0)\neq0 V (0,0) \mathbb{R}^2 f^{-1}(0)\cap V V\cap\{x=0\} (0,0) (0,0) y \frac{\partial f}{\partial x}(tx,y) = \frac{t}{x}\cdot\frac{\partial f}{\partial t}(tx,y) g(x,y) = \int\limits_0^1\frac{\partial f}{\partial x}(tx,y)dt = \int\limits_0^1 \frac{t}{x}\cdot\frac{\partial f}{\partial t}(tx,y)dt = \frac{1}{x}\left(f(x,y) - \int\limits_0^1 f(tx,y)dt\right) \int\limits_0^1 f(tx,y)dt = 0 g(0,y) \frac{\partial g}{\partial y}(0,y) g(x,y) g y=0 f(x,0)\colon\mathbb{R}\to\mathbb{R} \frac{\partial f}{\partial x}(0,0)\neq0 U 0\in\mathbb{R} f\colon U\to f(U) V = U\times\{0\} V\subset\mathbb{R}^2 g(0,y)\colon\mathbb{R}\to\mathbb{R}","['real-analysis', 'analysis', 'derivatives', 'inverse-function-theorem']"
72,Equality of limits or counterexample [duplicate],Equality of limits or counterexample [duplicate],,"This question already has answers here : $f(x+1)-f(x)$ converges $\Rightarrow\frac{f(x)}x$ converges (2 answers) Closed 4 years ago . Let $f: \mathbb{R} \to \mathbb{R}$ such that $\lim\limits_{x \to \infty} \frac{f(x)}{x} = c$ where $c\neq 0$ or $\infty$ . Is it true that for all such functions $f$ , if $\lim\limits_{x \to \infty}(f(x+1)-f(x))$ exists, then it is equal to $c$ ? Note : Say if $f(x) = x+\sin(x)$ , then the condition is clearly false because the second limit does not exist. But say, we impose another condition that the second limit should exist, then is it necessary that it goes to $c$ as well?","This question already has answers here : $f(x+1)-f(x)$ converges $\Rightarrow\frac{f(x)}x$ converges (2 answers) Closed 4 years ago . Let such that where or . Is it true that for all such functions , if exists, then it is equal to ? Note : Say if , then the condition is clearly false because the second limit does not exist. But say, we impose another condition that the second limit should exist, then is it necessary that it goes to as well?",f: \mathbb{R} \to \mathbb{R} \lim\limits_{x \to \infty} \frac{f(x)}{x} = c c\neq 0 \infty f \lim\limits_{x \to \infty}(f(x+1)-f(x)) c f(x) = x+\sin(x) c,"['real-analysis', 'calculus', 'limits']"
73,Explanation of specific parts of Urysohn's Lemma proof,Explanation of specific parts of Urysohn's Lemma proof,,"While reading and trying to understand the proof I got stuck in some parts of the proof. Why can we take all the $V_{r_i}$ in such manner? Shall we say first w.l.o.g ?: Suppose $n\ge2$ and $V_{r_1},...,V_{r_n}$ have been chosen in such a manner that $r_i<r_j$ implies $\overline V_{r_j}\subset V_{r_i}$ . How can we know that $\overline V_j$ are all compact sets? We need this hypothesis so that Theorem 2.7 can be applied. Why is it enought to mention that $f$ is lower-semicontinuous and $g$ is upper-semicontinuous to have continuity of $f$ ? The author wrote it's clear that $f$ has it's support in $\overline V_0$ . Actually we need the support to be in $V$ ; by (2) it's done. Here is how I proved, please verify. Let's see that $\{x:f(x)\neq 0\}\subset V_0$ (we then take closures on both sides). Let $x\in\{x:f(x)\neq 0\}$ and let´s see that $x\in V_0.$ We have $f(x)\neq 0$ . By definition $f=\sup_r f_r.$ Thus $\sup_r f_r\neq 0$ . Now we must have $f_r=r$ if $x\in V_r$ otherwise $f_r=0,\sup_r f_r=0=f$ , contradiction. If $x\in V_r,f_r=r$ . We have $r>0,$ thus $\overline V_r\subset V_0.$ Thus $x\in V_0$ Hence $\{x:f(x)\neq 0\}\subset V_0$ . Taking closures, we have the result.","While reading and trying to understand the proof I got stuck in some parts of the proof. Why can we take all the in such manner? Shall we say first w.l.o.g ?: Suppose and have been chosen in such a manner that implies . How can we know that are all compact sets? We need this hypothesis so that Theorem 2.7 can be applied. Why is it enought to mention that is lower-semicontinuous and is upper-semicontinuous to have continuity of ? The author wrote it's clear that has it's support in . Actually we need the support to be in ; by (2) it's done. Here is how I proved, please verify. Let's see that (we then take closures on both sides). Let and let´s see that We have . By definition Thus . Now we must have if otherwise , contradiction. If . We have thus Thus Hence . Taking closures, we have the result.","V_{r_i} n\ge2 V_{r_1},...,V_{r_n} r_i<r_j \overline V_{r_j}\subset V_{r_i} \overline V_j f g f f \overline V_0 V \{x:f(x)\neq 0\}\subset V_0 x\in\{x:f(x)\neq 0\} x\in V_0. f(x)\neq 0 f=\sup_r f_r. \sup_r f_r\neq 0 f_r=r x\in V_r f_r=0,\sup_r f_r=0=f x\in V_r,f_r=r r>0, \overline V_r\subset V_0. x\in V_0 \{x:f(x)\neq 0\}\subset V_0","['real-analysis', 'general-topology', 'analysis', 'proof-explanation']"
74,Integral $\int_{-\infty}^{\infty}\ln(2-2\cos(x^2))dx=-\sqrt{2\pi}\zeta(3/2)$,Integral,\int_{-\infty}^{\infty}\ln(2-2\cos(x^2))dx=-\sqrt{2\pi}\zeta(3/2),"Prove that $$\int_{-\infty}^{\infty}\ln(2-2\cos(x^2))dx=-\sqrt{2\pi}\zeta(3/2)$$ I was given this integral in my post Request for crazy integrals . I have never seen an integral like this before and I need help evaluating it. Here's what I've tried. Setting $$J=\int_{-\infty}^{\infty}\ln(2-2\cos(x^2))dx$$ We have that $$I=-\frac{J}{2\ln2}=\sum_{n\geq1}\frac1{n}\int_0^\infty \cos^n(x^2)dx$$ from the series $$-\ln(1-x)=\sum_{n\geq1}\frac{x^n}{n}$$ Then set $$p_n=\int_0^{\infty}\cos^n(x^2)dx$$ so that $$I=p_1+\sum_{k\geq1}\frac{p_{2k+1}}{2k+1}+\frac12\sum_{k\geq1}\frac{p_{2k}}{k}$$ We have from Wikipedia that if $n$ is odd then $$\cos^n x=2^{1-n}\sum_{k=0}^{(n-1)/2}{n\choose k}\cos[(n-2k)x]$$ And for even $n$ , $$\cos^n x=\frac1{2^n}{n\choose n/2}+2^{1-n}\sum_{k=0}^{n/2-1}{n\choose k}\cos[(n-2k)x]$$ So $$p_{2k+1}=\frac1{4^k}\sum_{\ell=0}^{k} {2k+1\choose \ell}\int_0^\infty \cos[(2k-2\ell+1)x^2]dx$$ Then wolfram provides $$\int_0^\infty \cos(ax^2)dx=\frac{\sqrt{\pi}}{2\sqrt{2|a|}}$$ Which I know how to prove. Anyway, $$p_{2k+1}=\frac{\sqrt\pi}{2^{2k+3/2}}\sum_{\ell=0}^{k}\frac{{2k+1\choose \ell}}{\sqrt{2k-2\ell+1}}$$ Thus $$I=\frac12\sqrt{\frac\pi2}\left[1+\sum_{k\geq1}\frac1{4^k(2k+1)}\sum_{\ell=0}^{k}\frac{{2k+1\choose \ell}}{\sqrt{2k-2\ell+1}}\right]+\frac12\sum_{k\geq1}\frac{p_{2k}}{k}$$ The evaluation of $p_{2k}$ may be significantly more difficult from potential convergence issues. In any case, this doesn't seem to getting me anywhere close to $\zeta(3/2)$ , so I would like to see how it's done. Thanks :) Edit: Okay, from @ComplexYetTrivial's comment, we have that essentially everything I have done so far (apart from exploiting symmetry) is wrong. So yeah, I'm happy someone caught that.","Prove that I was given this integral in my post Request for crazy integrals . I have never seen an integral like this before and I need help evaluating it. Here's what I've tried. Setting We have that from the series Then set so that We have from Wikipedia that if is odd then And for even , So Then wolfram provides Which I know how to prove. Anyway, Thus The evaluation of may be significantly more difficult from potential convergence issues. In any case, this doesn't seem to getting me anywhere close to , so I would like to see how it's done. Thanks :) Edit: Okay, from @ComplexYetTrivial's comment, we have that essentially everything I have done so far (apart from exploiting symmetry) is wrong. So yeah, I'm happy someone caught that.",\int_{-\infty}^{\infty}\ln(2-2\cos(x^2))dx=-\sqrt{2\pi}\zeta(3/2) J=\int_{-\infty}^{\infty}\ln(2-2\cos(x^2))dx I=-\frac{J}{2\ln2}=\sum_{n\geq1}\frac1{n}\int_0^\infty \cos^n(x^2)dx -\ln(1-x)=\sum_{n\geq1}\frac{x^n}{n} p_n=\int_0^{\infty}\cos^n(x^2)dx I=p_1+\sum_{k\geq1}\frac{p_{2k+1}}{2k+1}+\frac12\sum_{k\geq1}\frac{p_{2k}}{k} n \cos^n x=2^{1-n}\sum_{k=0}^{(n-1)/2}{n\choose k}\cos[(n-2k)x] n \cos^n x=\frac1{2^n}{n\choose n/2}+2^{1-n}\sum_{k=0}^{n/2-1}{n\choose k}\cos[(n-2k)x] p_{2k+1}=\frac1{4^k}\sum_{\ell=0}^{k} {2k+1\choose \ell}\int_0^\infty \cos[(2k-2\ell+1)x^2]dx \int_0^\infty \cos(ax^2)dx=\frac{\sqrt{\pi}}{2\sqrt{2|a|}} p_{2k+1}=\frac{\sqrt\pi}{2^{2k+3/2}}\sum_{\ell=0}^{k}\frac{{2k+1\choose \ell}}{\sqrt{2k-2\ell+1}} I=\frac12\sqrt{\frac\pi2}\left[1+\sum_{k\geq1}\frac1{4^k(2k+1)}\sum_{\ell=0}^{k}\frac{{2k+1\choose \ell}}{\sqrt{2k-2\ell+1}}\right]+\frac12\sum_{k\geq1}\frac{p_{2k}}{k} p_{2k} \zeta(3/2),"['real-analysis', 'integration', 'special-functions', 'closed-form', 'riemann-zeta']"
75,"If $f,g: [a,b] \to \mathbb{R}$ are bounded with $g$ continuous and $f>g$, is there a continuous $h:[a,b] \to \mathbb{R}$ with $f>h>g$?","If  are bounded with  continuous and , is there a continuous  with ?","f,g: [a,b] \to \mathbb{R} g f>g h:[a,b] \to \mathbb{R} f>h>g","Is the following claim true: Let $f,g :[a,b]  \to  \mathbb{R}$ be bounded functions with $g$ continuous and $f>g$ . Then there exists a continuous function $h: [a,b]  \to  \mathbb{R}$ , such that $f>h>g$ . I am only interested in whether or not the claim is true or not, so please leave the proof to me if the claim is true. However, if the claim is false, I welcome counter examples! Any help is greatly appreciated!","Is the following claim true: Let be bounded functions with continuous and . Then there exists a continuous function , such that . I am only interested in whether or not the claim is true or not, so please leave the proof to me if the claim is true. However, if the claim is false, I welcome counter examples! Any help is greatly appreciated!","f,g :[a,b]  \to  \mathbb{R} g f>g h: [a,b]  \to  \mathbb{R} f>h>g","['real-analysis', 'calculus']"
76,How to find functional square root of $\sin(x)$.,How to find functional square root of .,\sin(x),"Maybe I am overlooking something, but is there some easy way to find a function $x\to f(x)$ so that $$(f\circ f)(x) = f(f(x)) = \sin(x)$$ on some interval, say $x\in [-\pi,\pi]\subset \mathbb R$ of real line? (Analytically or otherwise).","Maybe I am overlooking something, but is there some easy way to find a function so that on some interval, say of real line? (Analytically or otherwise).","x\to f(x) (f\circ f)(x) = f(f(x)) = \sin(x) x\in [-\pi,\pi]\subset \mathbb R","['real-analysis', 'linear-algebra', 'functional-analysis', 'trigonometry', 'fourier-analysis']"
77,Norms on the reals,Norms on the reals,,"On the real numbers the absolute value is a norm on this vector space. We can also define the norm of $x$ to be $c|x|$ , where $c>0$ is a constant. Are they the only norms on the real numbers? If not, what are other norms on the real numbers?","On the real numbers the absolute value is a norm on this vector space. We can also define the norm of to be , where is a constant. Are they the only norms on the real numbers? If not, what are other norms on the real numbers?",x c|x| c>0,['real-analysis']
78,"If $f(x)=\int_{0}^{x}\sqrt{f(t)}\,dt$ then $f(6)$ is?",If  then  is?,"f(x)=\int_{0}^{x}\sqrt{f(t)}\,dt f(6)","let $f:[0,\infty) \rightarrow[0,\infty]$ be continuous on $[0,\infty]$ and differentiable on $(0,\infty)$ . if $f(x)=\int_{0}^{x}\sqrt{f(t)}dt$ then $f(6)$ is ? $f(0)=0$ $f'(x)=\sqrt{f(x)}\implies f'(6)=\sqrt{f(6)}\implies f'(6)^2=f(6)$ Now I am trying to find out $f'(6) $ somehow $f'(6)=\lim_{x \rightarrow 6}\dfrac{f(x)-f(6)}{x-6}=\lim_{x \rightarrow 6}\dfrac{f(x)-f'(6)^2}{x-6}$ $f'(0)=\lim_{x \rightarrow 0}\dfrac{f(x)-f(0)}{x-0}$ I am missing something and I am not able to solve this problem by above steps.  Any hint?",let be continuous on and differentiable on . if then is ? Now I am trying to find out somehow I am missing something and I am not able to solve this problem by above steps.  Any hint?,"f:[0,\infty) \rightarrow[0,\infty] [0,\infty] (0,\infty) f(x)=\int_{0}^{x}\sqrt{f(t)}dt f(6) f(0)=0 f'(x)=\sqrt{f(x)}\implies f'(6)=\sqrt{f(6)}\implies f'(6)^2=f(6) f'(6)  f'(6)=\lim_{x \rightarrow 6}\dfrac{f(x)-f(6)}{x-6}=\lim_{x \rightarrow 6}\dfrac{f(x)-f'(6)^2}{x-6} f'(0)=\lim_{x \rightarrow 0}\dfrac{f(x)-f(0)}{x-0}","['real-analysis', 'calculus']"
79,An Irresistible integral: $\int_0^\infty \frac{x^{2m}}{(ax^2+b)^n}\mathrm dx=\frac{\pi}{2a^mb^{n-m-1}\sqrt{ab}}\frac{(2m-1)!!(2n-2m-3)!!}{(2m-2)!!}$,An Irresistible integral:,\int_0^\infty \frac{x^{2m}}{(ax^2+b)^n}\mathrm dx=\frac{\pi}{2a^mb^{n-m-1}\sqrt{ab}}\frac{(2m-1)!!(2n-2m-3)!!}{(2m-2)!!},"According to the author of Irresistible Integrals , $$I(n,m;a,b)=\int_0^\infty \frac{x^{2m}\mathrm dx}{(ax^2+b)^n}=\frac{\pi}{2a^mb^{n-m-1}\sqrt{ab}}\frac{(2m-1)!!(2n-2m-3)!!}{(2m-2)!!}$$ Which I am attempting to prove. My progress: $$J(n;a,b)=I(n,0;a,b)=\int_0^\infty \frac{\mathrm dx}{(ax^2+b)^n}$$ As I have shown in other posts of mine, this integral satisfies the recurrence $$J(n;a,b)=\frac{2n-3}{2bn}J(n-1;a,b)$$ And has the base case $$J(1;a,b)=\frac\pi{2\sqrt{ab}}$$ So $$J(n;a,b)=\frac{\pi}{2^{2n-1}b^{n-1}\sqrt{ab}}{2n-2\choose n-1}$$ Then I noticed that $$\frac{\partial}{\partial a}J(n;a,b)=-nI(n+1,1;a,b)$$ $$\frac{\partial^2}{\partial a^2}J(n;a,b)=n(n+1)I(n+2,2;a,b)$$ $$...$$ $$\frac{\partial^k}{\partial a^k}J(n;a,b)=(-1)^k(n)_kI(n+k,k;a,b)=\frac{\pi\sqrt\pi}{2^{2n-1}b^{n-1}a^{k}\sqrt{ab}\,\Gamma(\frac{1-2k}2)}{2n-2\choose n-1}$$ Here $(n)_k=\frac{\Gamma(n+k)}{\Gamma(n)}$ . Yet I am confused as to how to proceed. Could I have some help?","According to the author of Irresistible Integrals , Which I am attempting to prove. My progress: As I have shown in other posts of mine, this integral satisfies the recurrence And has the base case So Then I noticed that Here . Yet I am confused as to how to proceed. Could I have some help?","I(n,m;a,b)=\int_0^\infty \frac{x^{2m}\mathrm dx}{(ax^2+b)^n}=\frac{\pi}{2a^mb^{n-m-1}\sqrt{ab}}\frac{(2m-1)!!(2n-2m-3)!!}{(2m-2)!!} J(n;a,b)=I(n,0;a,b)=\int_0^\infty \frac{\mathrm dx}{(ax^2+b)^n} J(n;a,b)=\frac{2n-3}{2bn}J(n-1;a,b) J(1;a,b)=\frac\pi{2\sqrt{ab}} J(n;a,b)=\frac{\pi}{2^{2n-1}b^{n-1}\sqrt{ab}}{2n-2\choose n-1} \frac{\partial}{\partial a}J(n;a,b)=-nI(n+1,1;a,b) \frac{\partial^2}{\partial a^2}J(n;a,b)=n(n+1)I(n+2,2;a,b) ... \frac{\partial^k}{\partial a^k}J(n;a,b)=(-1)^k(n)_kI(n+k,k;a,b)=\frac{\pi\sqrt\pi}{2^{2n-1}b^{n-1}a^{k}\sqrt{ab}\,\Gamma(\frac{1-2k}2)}{2n-2\choose n-1} (n)_k=\frac{\Gamma(n+k)}{\Gamma(n)}","['real-analysis', 'calculus', 'integration']"
80,Why are polynomials tangential to the $x$ axis at real double roots?,Why are polynomials tangential to the  axis at real double roots?,x,"If $(x-a)^2$ is a root of a polynomial, then the graph will be tangent to the $x$ axis at $x=a$ but why? I know this is always the case for real double roots however I do not know the explanation for this behavior. Does this behavior apply to all real polynomial roots with even index multiplicities?","If is a root of a polynomial, then the graph will be tangent to the axis at but why? I know this is always the case for real double roots however I do not know the explanation for this behavior. Does this behavior apply to all real polynomial roots with even index multiplicities?",(x-a)^2 x x=a,"['real-analysis', 'calculus', 'polynomials', 'roots']"
81,"If $f(x+a)-f(x)$ is differentiable for each $a$, then $f$ is differentiable","If  is differentiable for each , then  is differentiable",f(x+a)-f(x) a f,"$ f \in C(\mathbf{R})$ , if for each real $a$ , $f(x+a)-f(x)$ is differentiable, then $f$ is differentiable. It seems hard to convert difference to the original function",", if for each real , is differentiable, then is differentiable. It seems hard to convert difference to the original function", f \in C(\mathbf{R}) a f(x+a)-f(x) f,"['real-analysis', 'derivatives']"
82,"If a function on a product space is continuous in each variable, is it locally bounded?","If a function on a product space is continuous in each variable, is it locally bounded?",,"Let $f : \mathbb R \times \mathbb R \to \mathbb R$ be continuous when we fix one variable. Then $f$ need not be continuous (see e.g. Functions continuous in each variable ). Does it imply that $f$ is locally bounded? I would be surprised, but couldn't immediately think of a counterexample. Context: I'm interested in $f : \mathbb R \times \mathbb C \to \mathbb C$ that are continuous in the first and analytic in the second variable. In this case, Cauchy's integral formula + Dominated convergence tells us that locally bounded implies jointly continuous.","Let $f : \mathbb R \times \mathbb R \to \mathbb R$ be continuous when we fix one variable. Then $f$ need not be continuous (see e.g. Functions continuous in each variable ). Does it imply that $f$ is locally bounded? I would be surprised, but couldn't immediately think of a counterexample. Context: I'm interested in $f : \mathbb R \times \mathbb C \to \mathbb C$ that are continuous in the first and analytic in the second variable. In this case, Cauchy's integral formula + Dominated convergence tells us that locally bounded implies jointly continuous.",,"['real-analysis', 'continuity', 'product-space']"
83,"If $f(f(x))+f(x)=x^4+3x^2+3$, prove that $f$ is even","If , prove that  is even",f(f(x))+f(x)=x^4+3x^2+3 f,"Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous function such that for all $x$ we have $$f(f(x))+f(x)=x^4+3x^2+3,$$   prove that for all $x \in \mathbb{R}$, $f(-x)=f(x)$. I noticed that $f$ cannot have any fixed points. If it had one, say $t$, we would have $$0=t^4+3t^2-2t+3=(t^2+1)^2+(t-1)^2+1$$ which is impossible. So, since $f$ is continuous, we either have $f(x)<x, \: \forall x \in \mathbb{R}$ or $f(x)>x, \: \forall x \in \mathbb{R}$. If the first were true, then we would get  $$x^4+3x^2+3=f(f(x))+f(x)<f(x)+x<2x, \quad \forall x \in \mathbb{R}$$ which is absurd. So $f(x)>x, \: \forall x \in \mathbb{R}$. Using this and the fact that $x^4+3x^2+3$ is strictly increasing on $[0, \infty)$ and strictly decreasing on $(-\infty,0]$, I managed to prove that $f$ is  strictly increasing on $[0, \infty)$ and strictly decreasing on $(-\infty, 0]$. This is where I got stuck. Edit : I think I made some progess. Suppose that there is $x_0$ such that $f(x_0)<0$. Then we get $0>f(x_0)>x_0$ and since $f$ is strictly decreasing on $(-\infty,0]$ it means that $f(0)<f(x_0)<0$, which contradicts $f(0)>0$. So $f(x) \geq 0, \: \forall x \in \mathbb{R}$ Now, suppose $f(x)>f(-x)>0$, for $x \neq 0$. Then $f(f(x))>f(f(-x))$ and summing these yields a contradiction with the hypothesis. The same happens if we suppose $0<f(x)<f(-x)$. So $f(x)=f(-x), \: \forall x \in \mathbb{R}$.","Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous function such that for all $x$ we have $$f(f(x))+f(x)=x^4+3x^2+3,$$   prove that for all $x \in \mathbb{R}$, $f(-x)=f(x)$. I noticed that $f$ cannot have any fixed points. If it had one, say $t$, we would have $$0=t^4+3t^2-2t+3=(t^2+1)^2+(t-1)^2+1$$ which is impossible. So, since $f$ is continuous, we either have $f(x)<x, \: \forall x \in \mathbb{R}$ or $f(x)>x, \: \forall x \in \mathbb{R}$. If the first were true, then we would get  $$x^4+3x^2+3=f(f(x))+f(x)<f(x)+x<2x, \quad \forall x \in \mathbb{R}$$ which is absurd. So $f(x)>x, \: \forall x \in \mathbb{R}$. Using this and the fact that $x^4+3x^2+3$ is strictly increasing on $[0, \infty)$ and strictly decreasing on $(-\infty,0]$, I managed to prove that $f$ is  strictly increasing on $[0, \infty)$ and strictly decreasing on $(-\infty, 0]$. This is where I got stuck. Edit : I think I made some progess. Suppose that there is $x_0$ such that $f(x_0)<0$. Then we get $0>f(x_0)>x_0$ and since $f$ is strictly decreasing on $(-\infty,0]$ it means that $f(0)<f(x_0)<0$, which contradicts $f(0)>0$. So $f(x) \geq 0, \: \forall x \in \mathbb{R}$ Now, suppose $f(x)>f(-x)>0$, for $x \neq 0$. Then $f(f(x))>f(f(-x))$ and summing these yields a contradiction with the hypothesis. The same happens if we suppose $0<f(x)<f(-x)$. So $f(x)=f(-x), \: \forall x \in \mathbb{R}$.",,"['calculus', 'real-analysis']"
84,Trigonometric series convergent to $0$ a.e.?,Trigonometric series convergent to  a.e.?,0,"I was wondering whether there exist $(c_k)_{k\in\mathbb{Z}}$ in $\mathbb{C},$ not all $0,$ so that $\lim_{K \to \infty} \sum_{|k| \le K} c_ke^{2\pi i kx} = 0$ for a.e. $x \in [0,1]$ . Of course the answer is ""no"" provided that $\sum_k |c_k|^2 < \infty$ . This is all I got.","I was wondering whether there exist in not all so that for a.e. . Of course the answer is ""no"" provided that . This is all I got.","(c_k)_{k\in\mathbb{Z}} \mathbb{C}, 0, \lim_{K \to \infty} \sum_{|k| \le K} c_ke^{2\pi i kx} = 0 x \in [0,1] \sum_k |c_k|^2 < \infty","['real-analysis', 'analysis', 'fourier-series']"
85,Understanding the proof to Egorov's Theorem,Understanding the proof to Egorov's Theorem,,"Theorem (Egorov). Let $\{f_n\}$ be a sequence of measurable functions converging almost   everywhere on a measurable set $E$ to a function $f$. Then, given any   $\delta > 0$, there exists a measurable set $E_{\delta} \subset E$   such that $\mu(E_{\delta}) > \mu(E) - \delta$ $\{f_{n}\}$ converges uniformly to f on $E_{\delta}$ proof (partial) In the above link is a picture of (partially) the proof for the theorem in my book. They begin the proof by considering the following set $$E_{n}^{m} = \bigcap_{i > n} \left \{ x \; : \; |f_{i}(x) - f(x)| < \frac{1}{m}\right \}$$ I do not understand the motivation in considering this set. To me, here is what I see. I know that this theorem is meant to show the relationship between convergence a.e. and uniform convergence. The definition of uniform convergence is A sequence of function $\{f_n\}$ (with domain $D$) converges uniformly to $f$ if $$\forall \epsilon > 0 \; \exists N \in \mathbb{N}\; \forall n \geq N\; \forall x \in D, \; |f_n(x) - f(x)| < \epsilon$$ A sequence of functions converge to the function a.e. if the set of points for which the convergence fails to hold is of measure zero. The set $E_{n}^{m}$ looks like the definition of uniform convergence I think. The $\frac{1}{m}$ is the $``\epsilon""$ and the $i > n$ is the $``n > N""$. But I don't see the idea of what we want to do with this. Could someone explain the motivation to me please?","Theorem (Egorov). Let $\{f_n\}$ be a sequence of measurable functions converging almost   everywhere on a measurable set $E$ to a function $f$. Then, given any   $\delta > 0$, there exists a measurable set $E_{\delta} \subset E$   such that $\mu(E_{\delta}) > \mu(E) - \delta$ $\{f_{n}\}$ converges uniformly to f on $E_{\delta}$ proof (partial) In the above link is a picture of (partially) the proof for the theorem in my book. They begin the proof by considering the following set $$E_{n}^{m} = \bigcap_{i > n} \left \{ x \; : \; |f_{i}(x) - f(x)| < \frac{1}{m}\right \}$$ I do not understand the motivation in considering this set. To me, here is what I see. I know that this theorem is meant to show the relationship between convergence a.e. and uniform convergence. The definition of uniform convergence is A sequence of function $\{f_n\}$ (with domain $D$) converges uniformly to $f$ if $$\forall \epsilon > 0 \; \exists N \in \mathbb{N}\; \forall n \geq N\; \forall x \in D, \; |f_n(x) - f(x)| < \epsilon$$ A sequence of functions converge to the function a.e. if the set of points for which the convergence fails to hold is of measure zero. The set $E_{n}^{m}$ looks like the definition of uniform convergence I think. The $\frac{1}{m}$ is the $``\epsilon""$ and the $i > n$ is the $``n > N""$. But I don't see the idea of what we want to do with this. Could someone explain the motivation to me please?",,"['real-analysis', 'measure-theory']"
86,$\sum_{n=1}^{\infty}\frac{1}{n^2} <\frac{33}{20}$ using elementary inequalities,using elementary inequalities,\sum_{n=1}^{\infty}\frac{1}{n^2} <\frac{33}{20},There are many ingenious ways for proving $$\zeta(2)=\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6} \approx 1.6449$$ Using the inequality $$\frac{1}{n^2} < \frac{1}{n^{2}-\frac{1}{4}} =\frac{1}{n-\frac{1}{2}}-\frac{1}{n+\frac{1}{2}}$$ we can see that $$\sum_{n=2}^{\infty}\frac{1}{n^2} <\frac{2}{3} \Rightarrow \zeta(2)<\frac{5}{3}=1.6666$$ Can we improve upon these bounds using elementary inequalities? Like is it possible to show (of course without assuming $\zeta(2)\approx 1.64449$) that $\zeta(2)<\frac{33}{20}$? If there are much nicer bounds which follow using elementary inequalities I would be happy to see them.,There are many ingenious ways for proving $$\zeta(2)=\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6} \approx 1.6449$$ Using the inequality $$\frac{1}{n^2} < \frac{1}{n^{2}-\frac{1}{4}} =\frac{1}{n-\frac{1}{2}}-\frac{1}{n+\frac{1}{2}}$$ we can see that $$\sum_{n=2}^{\infty}\frac{1}{n^2} <\frac{2}{3} \Rightarrow \zeta(2)<\frac{5}{3}=1.6666$$ Can we improve upon these bounds using elementary inequalities? Like is it possible to show (of course without assuming $\zeta(2)\approx 1.64449$) that $\zeta(2)<\frac{33}{20}$? If there are much nicer bounds which follow using elementary inequalities I would be happy to see them.,,"['real-analysis', 'upper-lower-bounds']"
87,Showing surjectivity of $f(x) = \frac{x - \sin x}{1 - \cos x}$,Showing surjectivity of,f(x) = \frac{x - \sin x}{1 - \cos x},"After having shown that $$f(x) = \frac{x - \sin (x)}{1 - \cos (x)}$$ is strictly monotonically increasing on $x \in (0, 2\pi)$, I'd like to show the surjectivity of this function [to $(0,\infty)$]. However, I fail solving the equation $$y =\frac{x - \sin (x)}{1 - \cos (x)}$$ with respect to $x$. Can I somehow use/apply the fact that $f(x) = \frac{u(x)}{u'(x)}$ ? I recognized this, even though I could not use it proving the monotonicity, so maybe I can use it here. For any hints I am very thankful.","After having shown that $$f(x) = \frac{x - \sin (x)}{1 - \cos (x)}$$ is strictly monotonically increasing on $x \in (0, 2\pi)$, I'd like to show the surjectivity of this function [to $(0,\infty)$]. However, I fail solving the equation $$y =\frac{x - \sin (x)}{1 - \cos (x)}$$ with respect to $x$. Can I somehow use/apply the fact that $f(x) = \frac{u(x)}{u'(x)}$ ? I recognized this, even though I could not use it proving the monotonicity, so maybe I can use it here. For any hints I am very thankful.",,"['real-analysis', 'functions', 'trigonometry']"
88,Prove the following sequence always results in a perfect square. [duplicate],Prove the following sequence always results in a perfect square. [duplicate],,"This question already has answers here : Prove that the expression is a perfect square (2 answers) Closed 6 years ago . There's a problem statement: For each $m \in \mathbb{N}$, we construct a sequence $m_0$, $m_1$, $m_2,\dots$ denoted $S_m$, recursively via $m_0=m$ and $$m_{i+1} = m_i + \left\lfloor \sqrt{m_i} \right\rfloor$$ for all $i \ge 0$.   Here, $\lfloor x \rfloor$ is the floor of $x$, the greatest integer less than or equal to $x$.   Hence, we have $\left\lfloor \sqrt{10} \right\rfloor=3$ and $\left\lfloor \sqrt{29} \right\rfloor=5$. Show that for each positive integer $m$, the sequence $S_m$ contains the square of some integer. I'm pretty certain that this can be proved with induction. I am just not quite sure what to induct on. Examining examples shows that $S_m$ always results in a perfect square eventually, though I'm not sure how to prove it.","This question already has answers here : Prove that the expression is a perfect square (2 answers) Closed 6 years ago . There's a problem statement: For each $m \in \mathbb{N}$, we construct a sequence $m_0$, $m_1$, $m_2,\dots$ denoted $S_m$, recursively via $m_0=m$ and $$m_{i+1} = m_i + \left\lfloor \sqrt{m_i} \right\rfloor$$ for all $i \ge 0$.   Here, $\lfloor x \rfloor$ is the floor of $x$, the greatest integer less than or equal to $x$.   Hence, we have $\left\lfloor \sqrt{10} \right\rfloor=3$ and $\left\lfloor \sqrt{29} \right\rfloor=5$. Show that for each positive integer $m$, the sequence $S_m$ contains the square of some integer. I'm pretty certain that this can be proved with induction. I am just not quite sure what to induct on. Examining examples shows that $S_m$ always results in a perfect square eventually, though I'm not sure how to prove it.",,"['real-analysis', 'sequences-and-series', 'number-theory', 'induction', 'ceiling-and-floor-functions']"
89,How to show that $e$ is irrational by studying $\sum^{n}_{k=1}\frac{1}{k!}$ and $\sum^{n}_{k=0}\frac{1}{k} + \frac{1}{n \cdot n!}$? [duplicate],How to show that  is irrational by studying  and ? [duplicate],e \sum^{n}_{k=1}\frac{1}{k!} \sum^{n}_{k=0}\frac{1}{k} + \frac{1}{n \cdot n!},"This question already has an answer here : proving ""e"" irrational using convergent series [closed] (1 answer) Closed 6 years ago . Suppose you have $$a_n = \sum^{n}_{k=0}\frac{1}{k!}$$ and $$b_n = a_n + \frac{1}{n \cdot n!}. $$ By using the fact that $a_n <e <b_n$, which is true $\forall n \in \mathbb{N}$, conclude that $e \not \in \mathbb{Q}$. And I am unable to do anything. I tried to get something absurd by supposing that $e = \frac{a}{b}$ with $a,b \in \mathbb{Z}$, but didn't get anywhere. A hint would be highly appreciated.","This question already has an answer here : proving ""e"" irrational using convergent series [closed] (1 answer) Closed 6 years ago . Suppose you have $$a_n = \sum^{n}_{k=0}\frac{1}{k!}$$ and $$b_n = a_n + \frac{1}{n \cdot n!}. $$ By using the fact that $a_n <e <b_n$, which is true $\forall n \in \mathbb{N}$, conclude that $e \not \in \mathbb{Q}$. And I am unable to do anything. I tried to get something absurd by supposing that $e = \frac{a}{b}$ with $a,b \in \mathbb{Z}$, but didn't get anywhere. A hint would be highly appreciated.",,"['real-analysis', 'sequences-and-series', 'exponential-function', 'irrational-numbers']"
90,Does there exist some strictly positive function such that both the derivative and second derivative are strictly negative?,Does there exist some strictly positive function such that both the derivative and second derivative are strictly negative?,,If $f(x) > 0$ over the reals is it possible to have $f'(x) < 0$ and $f''(x)< 0 $ over the reals? Assuming $f$ can be differentiated twice.,If $f(x) > 0$ over the reals is it possible to have $f'(x) < 0$ and $f''(x)< 0 $ over the reals? Assuming $f$ can be differentiated twice.,,"['calculus', 'real-analysis', 'derivatives']"
91,How to show that $f(x)=|x|/x$ does not have any limit as $x\to0$?,How to show that  does not have any limit as ?,f(x)=|x|/x x\to0,"$f(x)$ does not converge to any $L$ as $x\to a$ if for every $L$ there is $\epsilon>0$ such that for all $\delta>0$ there is $x$ such that $0<|x-a|<\delta$ and $|f(x)-L|\geq\epsilon$. I wish to prove that $$ f(x)=\frac{|x|}{x}=\begin{cases} 1,&x>0\\ -1,&x<0 \end{cases} $$ does not converge to any $L$ as $x\to0$ using the above definition. This is what I did: Fix $L$ and take $\epsilon=\frac{1}{2}$. For any $\delta>0$ there is $x$ such that $0<|x-0|<\delta$ and $$ |f(x)-L|=\begin{cases} |1-L|,&x>0\\ |-1-L|,&x<0 \end{cases}= \begin{cases} |1-L|,&x>0\\ |1+L|,&x<0 \end{cases} $$ but I am not sure how I should show that $|f(x)-L|\geq\frac{1}{2}=\epsilon$.","$f(x)$ does not converge to any $L$ as $x\to a$ if for every $L$ there is $\epsilon>0$ such that for all $\delta>0$ there is $x$ such that $0<|x-a|<\delta$ and $|f(x)-L|\geq\epsilon$. I wish to prove that $$ f(x)=\frac{|x|}{x}=\begin{cases} 1,&x>0\\ -1,&x<0 \end{cases} $$ does not converge to any $L$ as $x\to0$ using the above definition. This is what I did: Fix $L$ and take $\epsilon=\frac{1}{2}$. For any $\delta>0$ there is $x$ such that $0<|x-0|<\delta$ and $$ |f(x)-L|=\begin{cases} |1-L|,&x>0\\ |-1-L|,&x<0 \end{cases}= \begin{cases} |1-L|,&x>0\\ |1+L|,&x<0 \end{cases} $$ but I am not sure how I should show that $|f(x)-L|\geq\frac{1}{2}=\epsilon$.",,"['real-analysis', 'limits', 'proof-writing']"
92,$f(x)$ not continous at 0 but $f(x) + \frac{1}{f(x)}$ continuous at 0: Is my example valid?,not continous at 0 but  continuous at 0: Is my example valid?,f(x) f(x) + \frac{1}{f(x)},"I'm but a lowly electrical engineering student interested in mathematics. Recently, I've been working through the second edition of Stephen Abbott's Understanding Analysis and I encountered a problem that I'm not entirely sure about, specifically problem 4.3.6, part d, which states: Provide an example or explain why the request is impossible: A function $f(x)$ which is not continuous at 0 such that $f(x) + \frac{1}{f(x)}$ is continuous at 0. Now, being supremely lazy, my instinct at first was to say, ""sure, consider the function $f:\mathbb{R} \to \mathbb{R}$ defined by  $$f(x) = \begin{cases} 1,  & \text{if $x=0$} \\ 0, & \text{if $x \neq 0$} \end{cases}""$$ Then, my reasoning was that the only way we can define $g(x) = f(x) + \frac{1}{f(x)}$ is via the restriction of $f$ onto the domain $\{0\}$. So, since 0 is clearly an isolated point in the domain of $g$, it follows that $g(x)$ must be continuous at 0. Does this answer fit the ""spirit"" of the question? Or am I only allowed to choose an $f$ such that $g$ can have the same domain as $f$? Thanks for your help!","I'm but a lowly electrical engineering student interested in mathematics. Recently, I've been working through the second edition of Stephen Abbott's Understanding Analysis and I encountered a problem that I'm not entirely sure about, specifically problem 4.3.6, part d, which states: Provide an example or explain why the request is impossible: A function $f(x)$ which is not continuous at 0 such that $f(x) + \frac{1}{f(x)}$ is continuous at 0. Now, being supremely lazy, my instinct at first was to say, ""sure, consider the function $f:\mathbb{R} \to \mathbb{R}$ defined by  $$f(x) = \begin{cases} 1,  & \text{if $x=0$} \\ 0, & \text{if $x \neq 0$} \end{cases}""$$ Then, my reasoning was that the only way we can define $g(x) = f(x) + \frac{1}{f(x)}$ is via the restriction of $f$ onto the domain $\{0\}$. So, since 0 is clearly an isolated point in the domain of $g$, it follows that $g(x)$ must be continuous at 0. Does this answer fit the ""spirit"" of the question? Or am I only allowed to choose an $f$ such that $g$ can have the same domain as $f$? Thanks for your help!",,"['real-analysis', 'functions', 'continuity']"
93,"If $A$ has measure zero, then is there $x$ such that $A \cap (A-x)=\emptyset$?","If  has measure zero, then is there  such that ?",A x A \cap (A-x)=\emptyset,"If $A\subseteq \mathbb{R}^d$ has Lebesgue measure zero, is it always possible to find an $x$ such that $A \cap (A-x)=\emptyset$? This is one of those things that seems reasonable , but I'm worried about pathologies. For instance, if $A$ were a group, then the result is obvious. One might consider the group generated by $A$, but this may be too large. E.g. if $A$ is the standard Cantor set on $[0,1]$, then the subgroup generated by $A$ is all of $\mathbb{R}$.","If $A\subseteq \mathbb{R}^d$ has Lebesgue measure zero, is it always possible to find an $x$ such that $A \cap (A-x)=\emptyset$? This is one of those things that seems reasonable , but I'm worried about pathologies. For instance, if $A$ were a group, then the result is obvious. One might consider the group generated by $A$, but this may be too large. E.g. if $A$ is the standard Cantor set on $[0,1]$, then the subgroup generated by $A$ is all of $\mathbb{R}$.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
94,Frequently in Nets,Frequently in Nets,,"Let $(x_\alpha)_{\alpha \in I}$ be a net with elements $x_{\alpha}$ in some countable set $A$. Suppose that no element of $A$ is frequently in $(x_\alpha)_{\alpha \in I}$. Is it true that there is some infinite subset $B \subseteq A$ so that for any infinite subset $C \subseteq B$, $C$ is frequently in $(x_\alpha)_{\alpha \in I}$? That is, $\forall \alpha \in I$, $\exists \beta \gtrsim \alpha$ so that $x_\beta \in C$. I think this is true but can't find a proof. It's hard because nets are very different from sequences.","Let $(x_\alpha)_{\alpha \in I}$ be a net with elements $x_{\alpha}$ in some countable set $A$. Suppose that no element of $A$ is frequently in $(x_\alpha)_{\alpha \in I}$. Is it true that there is some infinite subset $B \subseteq A$ so that for any infinite subset $C \subseteq B$, $C$ is frequently in $(x_\alpha)_{\alpha \in I}$? That is, $\forall \alpha \in I$, $\exists \beta \gtrsim \alpha$ so that $x_\beta \in C$. I think this is true but can't find a proof. It's hard because nets are very different from sequences.",,"['real-analysis', 'general-topology', 'analysis', 'nets']"
95,Help for series calculation $\sum_{n\ge1} \frac{1}{4n^3-n}$,Help for series calculation,\sum_{n\ge1} \frac{1}{4n^3-n},"I want to find the following series. $$\sum_{n\ge1} \frac{1}{4n^3-n}$$ So, fisrt of all, patial fraction : $$\frac{1}{4n^3-n}=\frac{1}{2n+1}+\frac{1}{2n-1}-\frac{1}{n}.$$ Next, I consider the geometric series $$\sum_{n\ge1} x^{2n-2}+x^{2n}-x^{n-1}=\frac{1}{1-x^2}+\frac{x^2}{1-x^2}-\frac{1}{1-x}\tag{1}$$ where ${-1\lt x\lt1}$. Then, I integrate both side of (1) from $0$ to $1$: $$\sum_{n\ge1}\frac{1}{2n-1}+\frac{1}{2n+1}-\frac{1}{n}=\int_0^1 \frac{x^2-x}{1-x^2}dx$$ Finally, I get the value that is equal to $ln(2)-1$  by improper integral but it's less than $0$. What did I do wrong? All help is appreciated.","I want to find the following series. $$\sum_{n\ge1} \frac{1}{4n^3-n}$$ So, fisrt of all, patial fraction : $$\frac{1}{4n^3-n}=\frac{1}{2n+1}+\frac{1}{2n-1}-\frac{1}{n}.$$ Next, I consider the geometric series $$\sum_{n\ge1} x^{2n-2}+x^{2n}-x^{n-1}=\frac{1}{1-x^2}+\frac{x^2}{1-x^2}-\frac{1}{1-x}\tag{1}$$ where ${-1\lt x\lt1}$. Then, I integrate both side of (1) from $0$ to $1$: $$\sum_{n\ge1}\frac{1}{2n-1}+\frac{1}{2n+1}-\frac{1}{n}=\int_0^1 \frac{x^2-x}{1-x^2}dx$$ Finally, I get the value that is equal to $ln(2)-1$  by improper integral but it's less than $0$. What did I do wrong? All help is appreciated.",,"['calculus', 'real-analysis', 'sequences-and-series']"
96,"Prove $\int_{B(x,r)}|\nabla u|^2\leq \frac{C}{r^2}\int_{B(x,2r)}|u|^2$ if $-\Delta u(x)+f(x)u(x)=0.$",Prove  if,"\int_{B(x,r)}|\nabla u|^2\leq \frac{C}{r^2}\int_{B(x,2r)}|u|^2 -\Delta u(x)+f(x)u(x)=0.","Let $\Omega $ a smooth domain of $\mathbb R^d$ ($d\geq 2$), $f\in \mathcal C(\overline{\Omega })$. Let $u\in \mathcal C^2(\overline{\Omega })$ solution of $$-\Delta u(x)+f(x)u(x)=0\ \ in\ \ \Omega .$$ Assume that $f(x)\geq 0$ for $x\in \Omega $. Prove that $$\int_{B(x,r)}|\nabla u|^2\leq \frac{C}{r^2}\int_{B(x,2r)}|u|^2,$$ for all $x\in \Omega $ and $r>0$, $B(x,2r)\subset \subset \Omega $ for somme $C\geq 0$ independant of $u,f,x$ and $r$. My attempts Using divergence theorem and that $\Delta u=fu$ in $\Omega $, I have that $$\int_{B(x,r)}|\nabla u|^2=\int_{B(x,r)}\text{div}(u\nabla u)-\int_{B(x,r)}u\Delta u=\int_{\partial B(x,r)}u\nabla u\cdot \nu-\int_{B(x,r)}fu^2.$$ But I can't do better. Any help would be welcome.","Let $\Omega $ a smooth domain of $\mathbb R^d$ ($d\geq 2$), $f\in \mathcal C(\overline{\Omega })$. Let $u\in \mathcal C^2(\overline{\Omega })$ solution of $$-\Delta u(x)+f(x)u(x)=0\ \ in\ \ \Omega .$$ Assume that $f(x)\geq 0$ for $x\in \Omega $. Prove that $$\int_{B(x,r)}|\nabla u|^2\leq \frac{C}{r^2}\int_{B(x,2r)}|u|^2,$$ for all $x\in \Omega $ and $r>0$, $B(x,2r)\subset \subset \Omega $ for somme $C\geq 0$ independant of $u,f,x$ and $r$. My attempts Using divergence theorem and that $\Delta u=fu$ in $\Omega $, I have that $$\int_{B(x,r)}|\nabla u|^2=\int_{B(x,r)}\text{div}(u\nabla u)-\int_{B(x,r)}u\Delta u=\int_{\partial B(x,r)}u\nabla u\cdot \nu-\int_{B(x,r)}fu^2.$$ But I can't do better. Any help would be welcome.",,"['real-analysis', 'partial-differential-equations', 'harmonic-functions']"
97,Series Converging Uniformly,Series Converging Uniformly,,"Given $$\sum\limits_{n=1}^\mathbb{∞}\frac1{ne^{nx}}$$ why does this series not converge uniformly for $x$ in $(0,∞)$ but converges uniformly for $x$ in $[𝛿,∞)$ for $𝛿 > 0$?","Given $$\sum\limits_{n=1}^\mathbb{∞}\frac1{ne^{nx}}$$ why does this series not converge uniformly for $x$ in $(0,∞)$ but converges uniformly for $x$ in $[𝛿,∞)$ for $𝛿 > 0$?",,"['calculus', 'real-analysis']"
98,Showing the set of real values for which the pre-image has measure greater than zero is measure zero,Showing the set of real values for which the pre-image has measure greater than zero is measure zero,,"The question is stated as follows: Show that if $f: \mathbb{R} \rightarrow \mathbb{R}$ is measurable, then the set $E = \{x \in \mathbb{R} \ | \ m(f^{-1}(x)) > 0 \}$ has measure zero. This problem seems simple enough at first glance, and I feel like I had a solution. We begin by showing that $E$ is measurable using a definition from Royden. That is, since $E \subset \mathbb{R}$, and we have \begin{align} m^*(\mathbb{R}) = m^*(\mathbb{R} \cup E) + m^*(\mathbb{R} \cup E^C)  \end{align} it follows that $E$ is a measurable set. Traditionally, if the inverse of $f$ is defined, then f is bijective and for each $y \in \mathbb{R}$, there should be one $x \in \mathbb{R}$ such that $f(x) = y$. But then for each $y \in \mathbb{R}$, $m(f^{-1}(y)) = m(\{x\}) = 0$. From this, it should follow not only that $E$ is empty, but that $m(E) = 0$. Here is where I'm not sure. If we just consider $f^{-1}$ to return the pre-image of a point $y \in \mathbb{R}$, and say that $f$ is a constant function defined $f(x) = c$, then $m(f^{-1}(c)) = m(\mathbb{R}) = \infty > 0$. In this case, however, $E = \{c\}$, which has measure zero. With this new interpretation of $f^{-1}$, I'm not exactly sure how to proceed. If the measure of $E$ was not zero, there would necessarily be an interval $(y_1,y_2) \subset E$ for which each point in the interval $y \in (y_1,y_2)$, we have $f^{-1}(y) = (x_1,x_2)$. I'm not sure what implications this might have, if any. Should I work at obtaining a contradiction here, or should I attempt to prove that $m(E) = 0$ directly? Edit: As was pointed out in the comments, my argument for the measurability of $E$ is insufficient. Further, my conclusion that an interval lived in $E$ is equally false (thanks Cantor)","The question is stated as follows: Show that if $f: \mathbb{R} \rightarrow \mathbb{R}$ is measurable, then the set $E = \{x \in \mathbb{R} \ | \ m(f^{-1}(x)) > 0 \}$ has measure zero. This problem seems simple enough at first glance, and I feel like I had a solution. We begin by showing that $E$ is measurable using a definition from Royden. That is, since $E \subset \mathbb{R}$, and we have \begin{align} m^*(\mathbb{R}) = m^*(\mathbb{R} \cup E) + m^*(\mathbb{R} \cup E^C)  \end{align} it follows that $E$ is a measurable set. Traditionally, if the inverse of $f$ is defined, then f is bijective and for each $y \in \mathbb{R}$, there should be one $x \in \mathbb{R}$ such that $f(x) = y$. But then for each $y \in \mathbb{R}$, $m(f^{-1}(y)) = m(\{x\}) = 0$. From this, it should follow not only that $E$ is empty, but that $m(E) = 0$. Here is where I'm not sure. If we just consider $f^{-1}$ to return the pre-image of a point $y \in \mathbb{R}$, and say that $f$ is a constant function defined $f(x) = c$, then $m(f^{-1}(c)) = m(\mathbb{R}) = \infty > 0$. In this case, however, $E = \{c\}$, which has measure zero. With this new interpretation of $f^{-1}$, I'm not exactly sure how to proceed. If the measure of $E$ was not zero, there would necessarily be an interval $(y_1,y_2) \subset E$ for which each point in the interval $y \in (y_1,y_2)$, we have $f^{-1}(y) = (x_1,x_2)$. I'm not sure what implications this might have, if any. Should I work at obtaining a contradiction here, or should I attempt to prove that $m(E) = 0$ directly? Edit: As was pointed out in the comments, my argument for the measurability of $E$ is insufficient. Further, my conclusion that an interval lived in $E$ is equally false (thanks Cantor)",,"['real-analysis', 'analysis', 'measure-theory']"
99,Uniformly continuous functions sequence $f_n(x)$ converges uniformly to a uniformly continuous function $f(x)$? [duplicate],Uniformly continuous functions sequence  converges uniformly to a uniformly continuous function ? [duplicate],f_n(x) f(x),"This question already has an answer here : Uniform continuous maps that converge uniformly to a function f. What can we say about f? (1 answer) Closed 8 years ago . We know that if continuous functions sequence $g_n(x)$ converges uniformly to $g(x)$, then $g(x)$ is continuous function. But what if uniformly continuous functions sequence $f_n(x)$ converges uniformly to $f(x)$? Does that imply $f(x)$ is uniformly continuous function? If so - how do we prove it? what $\delta$ should we take for $f(x)$? We can't simply choose $\min\{ \delta_n \}$. And what if $f_n(x)$ converges pointwise to $f(x)$. Will $f(x)$ be continuous? uniformly continuous? Or pointwise convergence is not enough?","This question already has an answer here : Uniform continuous maps that converge uniformly to a function f. What can we say about f? (1 answer) Closed 8 years ago . We know that if continuous functions sequence $g_n(x)$ converges uniformly to $g(x)$, then $g(x)$ is continuous function. But what if uniformly continuous functions sequence $f_n(x)$ converges uniformly to $f(x)$? Does that imply $f(x)$ is uniformly continuous function? If so - how do we prove it? what $\delta$ should we take for $f(x)$? We can't simply choose $\min\{ \delta_n \}$. And what if $f_n(x)$ converges pointwise to $f(x)$. Will $f(x)$ be continuous? uniformly continuous? Or pointwise convergence is not enough?",,"['calculus', 'real-analysis', 'sequences-and-series', 'uniform-convergence', 'uniform-continuity']"
