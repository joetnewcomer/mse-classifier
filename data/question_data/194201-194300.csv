,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to prove derivative of $\cos x$ is $-\sin x$ using power series?,How to prove derivative of  is  using power series?,\cos x -\sin x,"How to prove derivative of $\cos x$ is $-\sin x$ using power series? So $\sin x=\sum \limits_{n=0}^\infty\dfrac{(-1)^nx^{2n+1}}{(2n+1)!}$ and $\cos x=\sum \limits_{n=0}^\infty\dfrac{(-1)^nx^{2n}}{(2n)!}$ Then $\cos'(x)=\sum \limits_{n=0}^\infty\left(\dfrac{(-1)^nx^{2n}}{(2n)!}\right)'=\sum \limits_{n=0}^\infty\dfrac{(-1)^nx^{2n-1}}{(2n-1)!}=...$ Then I don't to how to go to $\sin x$, could someone help?","How to prove derivative of $\cos x$ is $-\sin x$ using power series? So $\sin x=\sum \limits_{n=0}^\infty\dfrac{(-1)^nx^{2n+1}}{(2n+1)!}$ and $\cos x=\sum \limits_{n=0}^\infty\dfrac{(-1)^nx^{2n}}{(2n)!}$ Then $\cos'(x)=\sum \limits_{n=0}^\infty\left(\dfrac{(-1)^nx^{2n}}{(2n)!}\right)'=\sum \limits_{n=0}^\infty\dfrac{(-1)^nx^{2n-1}}{(2n-1)!}=...$ Then I don't to how to go to $\sin x$, could someone help?",,"['calculus', 'derivatives', 'power-series']"
1,Finding tangent lines to Devil's Curve,Finding tangent lines to Devil's Curve,,"This is a fairly interesting question and I look forward to being able to solve it. The curve's equation is given by $ y^4-96y^2 = x^4-100x^2$. I have differentiated this implicitly to get $y^1= \frac{(x^3-50x)}{(y^3-48y)}$ Now the questions asks that I find all the tangent lines at where $x = 10$. I drew the graph out and noticed that there should be several tangent lines existing at this point. Therefore, the next step I took was to find different $y$ values that satisfied the equation. I ended up with: $ y= 0, y= \sqrt 96, y=-\sqrt 96 $ I tried subbing these points back into both the original equation and derivative to try and find the slope, but I don't believe I ended up with the right answer. I got, for the point $(10, \sqrt 96)$, a tangent equation of $y = 1.063x - 10.63$, which when plotted with the curve itself, did not look like a proper tangent. Any help would be much appreciated! Please excuse my poor formatting.","This is a fairly interesting question and I look forward to being able to solve it. The curve's equation is given by $ y^4-96y^2 = x^4-100x^2$. I have differentiated this implicitly to get $y^1= \frac{(x^3-50x)}{(y^3-48y)}$ Now the questions asks that I find all the tangent lines at where $x = 10$. I drew the graph out and noticed that there should be several tangent lines existing at this point. Therefore, the next step I took was to find different $y$ values that satisfied the equation. I ended up with: $ y= 0, y= \sqrt 96, y=-\sqrt 96 $ I tried subbing these points back into both the original equation and derivative to try and find the slope, but I don't believe I ended up with the right answer. I got, for the point $(10, \sqrt 96)$, a tangent equation of $y = 1.063x - 10.63$, which when plotted with the curve itself, did not look like a proper tangent. Any help would be much appreciated! Please excuse my poor formatting.",,"['derivatives', 'implicit-differentiation']"
2,"The values of $g'\left(\pm\frac\pi2\right)$ for $g(x)=\int_{\sin x}^{\sin 2x} \sin^{-1}(t) \,dt$",The values of  for,"g'\left(\pm\frac\pi2\right) g(x)=\int_{\sin x}^{\sin 2x} \sin^{-1}(t) \,dt","Question : If $g(x)=\displaystyle\int_{\sin x}^{\sin 2x} \sin^{-1}(t)  \,dt$, then : $\text A) \, g'\left( -\dfrac{\pi}{2}\right)=2\pi$ $\text B) \, g'\left( -\dfrac{\pi}{2}\right)=-2\pi$ $\text C) \, g'\left( \dfrac{\pi}{2}\right)=2\pi$ $\text D) \, g'\left( \dfrac{\pi}{2}\right)=-2\pi$ Now, I used Newton-Leibniz rule and took derivative of $g(x)$ w.r.t $x$. $$g(x)= \int_{\sin x}^{\sin 2x} \sin^{-1}(t)  \,dt$$ \begin{align} \implies g'(x) &=\sin^{-1}(\sin 2x) \cdot (\sin 2x)'-\sin^{-1}(\sin x) \cdot (\sin x)'\\ &=\sin^{-1}(\sin 2x) \cdot 2 \cos 2x-\sin^{-1}(\sin x) \cdot \cos x \end{align} Hence, \begin{align} g'\left( -\frac{\pi}{2}\right)&=\sin^{-1}(\sin (-\pi)) \cdot 2 \cos (-\pi)-\sin^{-1}\left(\sin \left(-\frac{\pi}{2}\right) \right) \cdot \cos \left(-\frac{\pi}{2}\right)\\ &=\sin^{-1}(0) \cdot 2 \cdot (-1)-\sin^{-1}(-1) \cdot 0\\ &=0 \end{align} and \begin{align} g'\left( \frac{\pi}{2}\right)&=\sin^{-1}(\sin (\pi)) \cdot 2 \cos (\pi)-\sin^{-1}\left(\sin \left(\frac{\pi}{2}\right) \right) \cdot \cos \left(\frac{\pi}{2}\right)\\ &=\sin^{-1}(0) \cdot 2 \cdot (-1)-\sin^{-1}(1) \cdot 0\\ &=0 \end{align} Where did I go wrong, because none of the options matches my answer. Most probably this question is wrong. I am asking this question here because this is a question from a  very highly reputated engineering entrance examination. All coaching institutes are saying that this question is bonus (i.e. since none of the options given is correct, its marks will be awarded to all the students, irrespective of what they marked). You can see it here (In case you are tired of searching, it's Q.49) But, I am doubtful for this because a similar scenerio once happened in year $2012$, all the coaching institutes uploaded their un-official answer key and answer given to a particular question was $0$ (in unofficial answer key) , and when the official answer key was out, the answer for the same question was given to be $2$, which was the correct one. All the coaching institutes solved that question wrong. Therefore, I request all mathematicians here to please verify my solution.","Question : If $g(x)=\displaystyle\int_{\sin x}^{\sin 2x} \sin^{-1}(t)  \,dt$, then : $\text A) \, g'\left( -\dfrac{\pi}{2}\right)=2\pi$ $\text B) \, g'\left( -\dfrac{\pi}{2}\right)=-2\pi$ $\text C) \, g'\left( \dfrac{\pi}{2}\right)=2\pi$ $\text D) \, g'\left( \dfrac{\pi}{2}\right)=-2\pi$ Now, I used Newton-Leibniz rule and took derivative of $g(x)$ w.r.t $x$. $$g(x)= \int_{\sin x}^{\sin 2x} \sin^{-1}(t)  \,dt$$ \begin{align} \implies g'(x) &=\sin^{-1}(\sin 2x) \cdot (\sin 2x)'-\sin^{-1}(\sin x) \cdot (\sin x)'\\ &=\sin^{-1}(\sin 2x) \cdot 2 \cos 2x-\sin^{-1}(\sin x) \cdot \cos x \end{align} Hence, \begin{align} g'\left( -\frac{\pi}{2}\right)&=\sin^{-1}(\sin (-\pi)) \cdot 2 \cos (-\pi)-\sin^{-1}\left(\sin \left(-\frac{\pi}{2}\right) \right) \cdot \cos \left(-\frac{\pi}{2}\right)\\ &=\sin^{-1}(0) \cdot 2 \cdot (-1)-\sin^{-1}(-1) \cdot 0\\ &=0 \end{align} and \begin{align} g'\left( \frac{\pi}{2}\right)&=\sin^{-1}(\sin (\pi)) \cdot 2 \cos (\pi)-\sin^{-1}\left(\sin \left(\frac{\pi}{2}\right) \right) \cdot \cos \left(\frac{\pi}{2}\right)\\ &=\sin^{-1}(0) \cdot 2 \cdot (-1)-\sin^{-1}(1) \cdot 0\\ &=0 \end{align} Where did I go wrong, because none of the options matches my answer. Most probably this question is wrong. I am asking this question here because this is a question from a  very highly reputated engineering entrance examination. All coaching institutes are saying that this question is bonus (i.e. since none of the options given is correct, its marks will be awarded to all the students, irrespective of what they marked). You can see it here (In case you are tired of searching, it's Q.49) But, I am doubtful for this because a similar scenerio once happened in year $2012$, all the coaching institutes uploaded their un-official answer key and answer given to a particular question was $0$ (in unofficial answer key) , and when the official answer key was out, the answer for the same question was given to be $2$, which was the correct one. All the coaching institutes solved that question wrong. Therefore, I request all mathematicians here to please verify my solution.",,"['integration', 'derivatives', 'proof-verification']"
3,Differentiation - Power rule,Differentiation - Power rule,,"Find derivative of this function $$g(t)=-3t(6t^4-1)^{4/3}$$ I have tried it till the answer: $ -3t.\frac{4}{3} (6t^4 - 1)^{\frac{4}{3}-1} .\frac{d}{dt}(6t^4 -1)  $ $-3t x \frac{4}{3} (6t^4-1)^\frac{1}{3} ( 6X4 t^3 -0 ) $ $ -4t (6t^4 -1)^\frac{4}{3} (24t^3) $ $-96t(6t^4-1)^\frac{1}{3} $ However , I checked and saw that the answer is $$-3(6t^4-1)^{1/3}(38t^4-1)$$ I just started learning differentiation, and I don't understand my mistake on why I can't achieve the answer. Thanks!","Find derivative of this function $$g(t)=-3t(6t^4-1)^{4/3}$$ I have tried it till the answer: $ -3t.\frac{4}{3} (6t^4 - 1)^{\frac{4}{3}-1} .\frac{d}{dt}(6t^4 -1)  $ $-3t x \frac{4}{3} (6t^4-1)^\frac{1}{3} ( 6X4 t^3 -0 ) $ $ -4t (6t^4 -1)^\frac{4}{3} (24t^3) $ $-96t(6t^4-1)^\frac{1}{3} $ However , I checked and saw that the answer is $$-3(6t^4-1)^{1/3}(38t^4-1)$$ I just started learning differentiation, and I don't understand my mistake on why I can't achieve the answer. Thanks!",,"['calculus', 'derivatives']"
4,"Prove that even if $f'$ is not continuous, there must exist a number $c, a<c<b$, with $f'(c)=0$ [duplicate]","Prove that even if  is not continuous, there must exist a number , with  [duplicate]","f' c, a<c<b f'(c)=0","This question already has answers here : How to prove that derivatives have the Intermediate Value Property (5 answers) Closed 7 years ago . Let $f'$ exist for all $x$ on $[a,b]$, and suppose that $f'(a)=-1, f'(b)=1$. Prove that even if $f'$ is not continuous, there must exist a number $c, a<c<b$, with $f'(c)=0$ I'm not sure how to approach this problem. I thought about Rolle's Theorem, but can't really show it.","This question already has answers here : How to prove that derivatives have the Intermediate Value Property (5 answers) Closed 7 years ago . Let $f'$ exist for all $x$ on $[a,b]$, and suppose that $f'(a)=-1, f'(b)=1$. Prove that even if $f'$ is not continuous, there must exist a number $c, a<c<b$, with $f'(c)=0$ I'm not sure how to approach this problem. I thought about Rolle's Theorem, but can't really show it.",,"['real-analysis', 'derivatives', 'continuity']"
5,Step-by-step derivative of $\left ( \frac{c_1 x}{c_2 x + c_3 + c_4 \sqrt{c_5 x}} \right)^{c_6x + c_7 + c_8 \sqrt{c_9 x}}$,Step-by-step derivative of,\left ( \frac{c_1 x}{c_2 x + c_3 + c_4 \sqrt{c_5 x}} \right)^{c_6x + c_7 + c_8 \sqrt{c_9 x}},"Can someone please walk step by step on how to calculate the derivative $\left ( \frac{c_1 x}{c_2 x + c_3 + c_4 \sqrt{c_5 x}} \right)^{c_6x + c_7 + c_8 \sqrt{c_9 x}}$ Where the $c_i$ are constants, which are positive.","Can someone please walk step by step on how to calculate the derivative $\left ( \frac{c_1 x}{c_2 x + c_3 + c_4 \sqrt{c_5 x}} \right)^{c_6x + c_7 + c_8 \sqrt{c_9 x}}$ Where the $c_i$ are constants, which are positive.",,['derivatives']
6,Is this a necessary and sufficient condition for the derivative to exist at $C$?,Is this a necessary and sufficient condition for the derivative to exist at ?,C,"Suppose we want to prove that the derivative of a function across an interval exists at $C$, but the derivative at $C$ cannot be found. We know the function must be continuous. Can we take the limit of derivative from the negative and positive direction of $C$ and show that if they are equal, the derivative at $C$ exists and is equal to the limit obtained? Is this a necessary and sufficient condition? EDIT: Sufficiency - If a function is a derivative along some interval, it does not have a removable singularity at $C$. Necessity - There is no interval of a derivative of some function in which a jump or essential discontinuity occurs. There are two  cases in which the condition is met if this is a necessary and sufficient condition. One is where the derivative is continuous, the other is where there is a removable discontinuity in the derivative. Is the latter possible?","Suppose we want to prove that the derivative of a function across an interval exists at $C$, but the derivative at $C$ cannot be found. We know the function must be continuous. Can we take the limit of derivative from the negative and positive direction of $C$ and show that if they are equal, the derivative at $C$ exists and is equal to the limit obtained? Is this a necessary and sufficient condition? EDIT: Sufficiency - If a function is a derivative along some interval, it does not have a removable singularity at $C$. Necessity - There is no interval of a derivative of some function in which a jump or essential discontinuity occurs. There are two  cases in which the condition is met if this is a necessary and sufficient condition. One is where the derivative is continuous, the other is where there is a removable discontinuity in the derivative. Is the latter possible?",,"['real-analysis', 'functional-analysis', 'analysis', 'derivatives']"
7,How to show that this function is differentiable in $x=0$ but not in $x=1$,How to show that this function is differentiable in  but not in,x=0 x=1,"I am stuck with showing that this function is differentiable at $x=0$ but not at $x=1$. $f(x) = \begin{cases} x^2, & \text{if x}\in{\mathbb{Q}}\\ x^3, & \text{if x}\notin{\mathbb{Q}}\\ \end{cases}$ So for $x=0$: Case 1: $h \in \mathbb{Q}$ $\lim\limits_{h \to 0}{\frac{f(0+h)-f(0)}{h}} = \lim\limits_{h \to 0}{\frac{f(h)}{h}} = \lim\limits_{h \to 0}{\frac{h^2}{h} =  \lim\limits_{h \to 0}{ h} = 0} $ Case 2: $h \notin \mathbb{Q}$ $\lim\limits_{h \to 0}{\frac{f(0+h)-f(0)}{h}} = \lim\limits_{h \to 0}{\frac{f(h)}{h}} = \lim\limits_{h \to 0}{\frac{h^3}{h} = \lim\limits_{h \to 0}{ h^2} = 0} $ I am unsure on how to continue from here. I thought I need to show that for $x=0$ the right and left hand side limits are the same, and for $x=1$ they are different.","I am stuck with showing that this function is differentiable at $x=0$ but not at $x=1$. $f(x) = \begin{cases} x^2, & \text{if x}\in{\mathbb{Q}}\\ x^3, & \text{if x}\notin{\mathbb{Q}}\\ \end{cases}$ So for $x=0$: Case 1: $h \in \mathbb{Q}$ $\lim\limits_{h \to 0}{\frac{f(0+h)-f(0)}{h}} = \lim\limits_{h \to 0}{\frac{f(h)}{h}} = \lim\limits_{h \to 0}{\frac{h^2}{h} =  \lim\limits_{h \to 0}{ h} = 0} $ Case 2: $h \notin \mathbb{Q}$ $\lim\limits_{h \to 0}{\frac{f(0+h)-f(0)}{h}} = \lim\limits_{h \to 0}{\frac{f(h)}{h}} = \lim\limits_{h \to 0}{\frac{h^3}{h} = \lim\limits_{h \to 0}{ h^2} = 0} $ I am unsure on how to continue from here. I thought I need to show that for $x=0$ the right and left hand side limits are the same, and for $x=1$ they are different.",,['derivatives']
8,One-sided limits of $f'(x)$ at a point vs. one-sided limits of the difference quotient at that point,One-sided limits of  at a point vs. one-sided limits of the difference quotient at that point,f'(x),"I know that if a function is piecewise, then when differentiating it one needs to deal separately with the points of intersection of the pieces, because the derivative may not be defined there. Also, if a one-sided limit of $f'(x)$ at these ""crucial points"" is infinite or doesn't exist, it doesn't preclude the limit of the difference quotient from existing and being finite. For example, take $f(x)=x^2\sin\frac1x, f(0)=0$: $f'(x)= 2x\sin\frac1x-\sin\frac1x $ has no limit as $x\to0$ but $\lim\limits_{h\to0}\frac{f(h)-f(0)}{h}=0=f'(0)$. On the contrary, looking at $f(x)=\sin x +\lvert \sin x\rvert $ on $[0,2\pi]$ one has $f'(x)=2\cos x$ on $(0,\pi) $ and $f'(x)=0$ on $(\pi,2\pi)$ so $\lim\limits_{h\to0^+}f'(\pi+h)\ne\lim\limits_{h\to0^-}f'(\pi+h)$, but they are both finite. And in this case it's easy to find that they agree with the one-sided limits of the difference quotient. Why? Does the finiteness of the former imply that they agree with the latter? Is this also true if the one-sided limits of $f'$ at the crucial points agree with each other?","I know that if a function is piecewise, then when differentiating it one needs to deal separately with the points of intersection of the pieces, because the derivative may not be defined there. Also, if a one-sided limit of $f'(x)$ at these ""crucial points"" is infinite or doesn't exist, it doesn't preclude the limit of the difference quotient from existing and being finite. For example, take $f(x)=x^2\sin\frac1x, f(0)=0$: $f'(x)= 2x\sin\frac1x-\sin\frac1x $ has no limit as $x\to0$ but $\lim\limits_{h\to0}\frac{f(h)-f(0)}{h}=0=f'(0)$. On the contrary, looking at $f(x)=\sin x +\lvert \sin x\rvert $ on $[0,2\pi]$ one has $f'(x)=2\cos x$ on $(0,\pi) $ and $f'(x)=0$ on $(\pi,2\pi)$ so $\lim\limits_{h\to0^+}f'(\pi+h)\ne\lim\limits_{h\to0^-}f'(\pi+h)$, but they are both finite. And in this case it's easy to find that they agree with the one-sided limits of the difference quotient. Why? Does the finiteness of the former imply that they agree with the latter? Is this also true if the one-sided limits of $f'$ at the crucial points agree with each other?",,"['calculus', 'real-analysis', 'derivatives', 'piecewise-continuity']"
9,Entire functions problem,Entire functions problem,,"Suppose that $f:\Bbb C\to \Bbb C$ is entire, and denote by $u,v$ two functions $\Bbb R^2 \to \Bbb R$ such that $$f(x+iy)=u(x,y)+iv(x,y),$$ $\forall x,y \in \Bbb R$ Part (a) Show that if u is constant, then f is constant. My attempt  If $u(x,y)$ is constant then $\frac{\partial u}{\partial x}=0$ and $\frac{\partial u}{\partial y}=0$ therefore the riemann cauchy equations $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}=0$ and $\frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x}$ therefore $v(x,y)$ must also be constant which in turn means that f has to be constant, since $u(x,y),v(x,y)$ are constant Can anyone confirm if my approach for part (a) is correct? Part (b) Show that the function $z\mapsto \overline {f(\bar{z})}$ is entire. My attempt   First calculating what $\overline {f(\bar{z})}$ looks like, $f(\bar {z})=u(x,y)-iv(x,-y)$ therefore $$\overline {f(\bar{z})}=-u(x,y)+iv(x,-y)$$  i tried next to check if this satisfies the riemann cauchy equations but they dont so maybe i've made a mistake in my working of calculating $\overline {f(\bar{z})}$ Part (c) Let $D(0;1):=\{(x,y)\in \Bbb R^2:x+iy\in D(0;1)\}$ denote the open unit disk. Consider the function $g:D(0;1)\to \Bbb C$ defined by $$g(z):=\frac{1}{1-z}$$ $\forall z \in \Bbb C$ , we also denote $$E:=\{(x,y)\in \Bbb R^2:x+iy\in D(0;1)\}$$ The question is to find the functions $u,v:E\to \Bbb R$ such that $g(x+iy)=u(x,y)+iv(x,y)$ $\forall (x,y)\in E$  My attempt:  $$g(z)=\frac{1}{1-z}=\frac{1}{1-(x+iy)}=\frac{1}{1-x-iy}$$ I have gotten stuck here can i split this fraction up to calculate $u(x,y)$,$v(x,y)$. Then show that $u(x,y)$ and $v(x,y)$ satisfies the riemann cauchy equations? would this be the end of the question or is this step not needed as it just asks to calculate $u(x,y)$ and $v(x,y)$ If anyone could go through this and tell me if I've made mistakes and if so where I've made the mistakes i would be extremely thankful.","Suppose that $f:\Bbb C\to \Bbb C$ is entire, and denote by $u,v$ two functions $\Bbb R^2 \to \Bbb R$ such that $$f(x+iy)=u(x,y)+iv(x,y),$$ $\forall x,y \in \Bbb R$ Part (a) Show that if u is constant, then f is constant. My attempt  If $u(x,y)$ is constant then $\frac{\partial u}{\partial x}=0$ and $\frac{\partial u}{\partial y}=0$ therefore the riemann cauchy equations $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}=0$ and $\frac{\partial u}{\partial y}=-\frac{\partial v}{\partial x}$ therefore $v(x,y)$ must also be constant which in turn means that f has to be constant, since $u(x,y),v(x,y)$ are constant Can anyone confirm if my approach for part (a) is correct? Part (b) Show that the function $z\mapsto \overline {f(\bar{z})}$ is entire. My attempt   First calculating what $\overline {f(\bar{z})}$ looks like, $f(\bar {z})=u(x,y)-iv(x,-y)$ therefore $$\overline {f(\bar{z})}=-u(x,y)+iv(x,-y)$$  i tried next to check if this satisfies the riemann cauchy equations but they dont so maybe i've made a mistake in my working of calculating $\overline {f(\bar{z})}$ Part (c) Let $D(0;1):=\{(x,y)\in \Bbb R^2:x+iy\in D(0;1)\}$ denote the open unit disk. Consider the function $g:D(0;1)\to \Bbb C$ defined by $$g(z):=\frac{1}{1-z}$$ $\forall z \in \Bbb C$ , we also denote $$E:=\{(x,y)\in \Bbb R^2:x+iy\in D(0;1)\}$$ The question is to find the functions $u,v:E\to \Bbb R$ such that $g(x+iy)=u(x,y)+iv(x,y)$ $\forall (x,y)\in E$  My attempt:  $$g(z)=\frac{1}{1-z}=\frac{1}{1-(x+iy)}=\frac{1}{1-x-iy}$$ I have gotten stuck here can i split this fraction up to calculate $u(x,y)$,$v(x,y)$. Then show that $u(x,y)$ and $v(x,y)$ satisfies the riemann cauchy equations? would this be the end of the question or is this step not needed as it just asks to calculate $u(x,y)$ and $v(x,y)$ If anyone could go through this and tell me if I've made mistakes and if so where I've made the mistakes i would be extremely thankful.",,['complex-analysis']
10,How to determine if $\ f(x)=|x-2| $ is differentiable at 2,How to determine if  is differentiable at 2,\ f(x)=|x-2| ,I need to determine if $\ f(x)=|x-2| $ is differentiable at 2. I was thinking I could use the definition of a derivative $\ \frac{(f(x+h)-f(x))}{h} $ but am kind of at a loss.,I need to determine if $\ f(x)=|x-2| $ is differentiable at 2. I was thinking I could use the definition of a derivative $\ \frac{(f(x+h)-f(x))}{h} $ but am kind of at a loss.,,"['analysis', 'derivatives']"
11,Relationship between Frechet derivative and this one,Relationship between Frechet derivative and this one,,"My book presents this differenciability definition and says that it's due to Frechet and Stols: Given $f:U\to \mathbb{R}$, with $u\subset \mathbb{R}^n$, let $a\in U$.   We say that our functions is differentiable at point $a$ when there   are constants $A_1,\cdots, A_n$ such that, for every vector $v =  (\alpha_1,\cdots,\alpha_n)\in\mathbb{R}^n$, with $a+v\in U$, we have: $$f(a+v) = f(a) + A_1\alpha_1 + \cdots + A_n\cdot \alpha_n + r(v)$$ when $\lim_{v\to 0}\frac{r(v)}{|v|} = 0$ However, in this question , the derivative is different, it's a limit. Which one is the right one? If they're different, then what's the explanation for this one in my book? I liked the explanation given in that question.","My book presents this differenciability definition and says that it's due to Frechet and Stols: Given $f:U\to \mathbb{R}$, with $u\subset \mathbb{R}^n$, let $a\in U$.   We say that our functions is differentiable at point $a$ when there   are constants $A_1,\cdots, A_n$ such that, for every vector $v =  (\alpha_1,\cdots,\alpha_n)\in\mathbb{R}^n$, with $a+v\in U$, we have: $$f(a+v) = f(a) + A_1\alpha_1 + \cdots + A_n\cdot \alpha_n + r(v)$$ when $\lim_{v\to 0}\frac{r(v)}{|v|} = 0$ However, in this question , the derivative is different, it's a limit. Which one is the right one? If they're different, then what's the explanation for this one in my book? I liked the explanation given in that question.",,"['calculus', 'real-analysis', 'derivatives']"
12,"Is the derivative of $ \ln f(x) $, where $f(x)$ is linear, the same for all $f(x)$?","Is the derivative of , where  is linear, the same for all ?", \ln f(x)  f(x) f(x),"$\frac{d}{dx}\ln f(x)=\frac{{f(x)}'}{f(x)}$ Taking $\ f(x) $ as a linear $\ ax $ the result will always be $\frac{a}{ax} =\frac{1}{x}$ I found this confusion causing in the case of integrating, how would I know  what function to get back? $\ln$ here stands for the natural logarithm.","$\frac{d}{dx}\ln f(x)=\frac{{f(x)}'}{f(x)}$ Taking $\ f(x) $ as a linear $\ ax $ the result will always be $\frac{a}{ax} =\frac{1}{x}$ I found this confusion causing in the case of integrating, how would I know  what function to get back? $\ln$ here stands for the natural logarithm.",,"['derivatives', 'logarithms']"
13,"Integrating Derivatives of Inverse Trigonometric Functions When They Are in Similar, Yet Different Forms","Integrating Derivatives of Inverse Trigonometric Functions When They Are in Similar, Yet Different Forms",,"I am familiar with the derivatives of the three trigonometric functions: $\dfrac{d}{dx} \arcsin(x) = \dfrac{1}{\sqrt{1-x^2}}$ $\dfrac{d}{dx} \arccos(x) = \dfrac{-1}{\sqrt{1-x^2}}$ $\dfrac{d}{dx} \arctan(x) = \dfrac{1}{1+x^2}$ And I understand how to find the antiderivatives/integrate them IF they are in the same form as above. However, I often encounter them in similar, yet different forms: $\dfrac{1}{\sqrt{9-x^2}}$ $\dfrac{-1}{\sqrt{5-x^2}}$ $\dfrac{1}{3+x^2}$ In this form, I am unable to use any method to integrate them -- not even substitution (since there will always be an $x$ left over). If I encounter these forms, how do I take the antiderivative/integrate them? If there is a method, what is the reasoning behind it (why does it work)? I would greatly appreciate it if people could please take the time to help me with this.","I am familiar with the derivatives of the three trigonometric functions: $\dfrac{d}{dx} \arcsin(x) = \dfrac{1}{\sqrt{1-x^2}}$ $\dfrac{d}{dx} \arccos(x) = \dfrac{-1}{\sqrt{1-x^2}}$ $\dfrac{d}{dx} \arctan(x) = \dfrac{1}{1+x^2}$ And I understand how to find the antiderivatives/integrate them IF they are in the same form as above. However, I often encounter them in similar, yet different forms: $\dfrac{1}{\sqrt{9-x^2}}$ $\dfrac{-1}{\sqrt{5-x^2}}$ $\dfrac{1}{3+x^2}$ In this form, I am unable to use any method to integrate them -- not even substitution (since there will always be an $x$ left over). If I encounter these forms, how do I take the antiderivative/integrate them? If there is a method, what is the reasoning behind it (why does it work)? I would greatly appreciate it if people could please take the time to help me with this.",,"['calculus', 'integration', 'derivatives', 'trigonometric-integrals']"
14,Let $f(x)=x^{2}$ show that f is differentiable at $0$,Let  show that f is differentiable at,f(x)=x^{2} 0,"The book that I use has no example for proving the function is diffentiable. It's just give defination of differentiable .  I don't how exacty prove differentiable at some point.this problem I work by check it has limit and conclude if limit is true and exist it is differentiable Ok,Let check my proof we found derivative by using elementary calculus that is  $f'(x)=2x$ and plug in $0$ in $f'$ [$f'(0)=0$] if it's differentiable $f'(0)=0$ must be true let $\epsilon >0$ the defination of differentiable is $$|\frac{f(x)-f(c))}{x-c}-L|<\epsilon $$ in this case $$|\frac{x^{2}-0}{x-0}-0|<\epsilon $$ $$\frac{(x)(x)}{x}<\epsilon $$ $x< \epsilon$ Choose $\epsilon =\delta $ then $x<\epsilon$ since of f' exist and it's true by definition of limit. therefore it's differentiable at $0 $","The book that I use has no example for proving the function is diffentiable. It's just give defination of differentiable .  I don't how exacty prove differentiable at some point.this problem I work by check it has limit and conclude if limit is true and exist it is differentiable Ok,Let check my proof we found derivative by using elementary calculus that is  $f'(x)=2x$ and plug in $0$ in $f'$ [$f'(0)=0$] if it's differentiable $f'(0)=0$ must be true let $\epsilon >0$ the defination of differentiable is $$|\frac{f(x)-f(c))}{x-c}-L|<\epsilon $$ in this case $$|\frac{x^{2}-0}{x-0}-0|<\epsilon $$ $$\frac{(x)(x)}{x}<\epsilon $$ $x< \epsilon$ Choose $\epsilon =\delta $ then $x<\epsilon$ since of f' exist and it's true by definition of limit. therefore it's differentiable at $0 $",,"['real-analysis', 'derivatives', 'proof-verification']"
15,Using Mean Value Theorem x,Using Mean Value Theorem x,,"Let $f:\left[ 0,1\right] \rightarrow \mathbb{R}$ be a continuous function such that $f\left( 0\right) =0$ and $\left| f'\left( x\right) \right| \leq 1$ for all $x$ in $\left( 0,1\right)$. Prove that $-x\leq f\left( x\right) \leq x$ for all $x$ in $\left[ 0,1\right]$. Proof-trying. We will use the mean value theorem. By the mean value theorem and assumption, there is a $c$ in $\left( a,b\right)$ such that $\left| f'\left( c\right) \right| =\left| \dfrac {f\left( 1\right) -f\left( 0\right) } {1-0}\right| =\left| f\left( 1\right) \right| \leq 1 $. Hence, $-1\leq f\left( 1\right) \leq 1$. So, what should I do?","Let $f:\left[ 0,1\right] \rightarrow \mathbb{R}$ be a continuous function such that $f\left( 0\right) =0$ and $\left| f'\left( x\right) \right| \leq 1$ for all $x$ in $\left( 0,1\right)$. Prove that $-x\leq f\left( x\right) \leq x$ for all $x$ in $\left[ 0,1\right]$. Proof-trying. We will use the mean value theorem. By the mean value theorem and assumption, there is a $c$ in $\left( a,b\right)$ such that $\left| f'\left( c\right) \right| =\left| \dfrac {f\left( 1\right) -f\left( 0\right) } {1-0}\right| =\left| f\left( 1\right) \right| \leq 1 $. Hence, $-1\leq f\left( 1\right) \leq 1$. So, what should I do?",,['real-analysis']
16,How to find limit of $\lim_{x\to 1} \frac{f(x) - f(1)}{x-1}$ if $f(x)=-\sqrt{25-x^2}$,How to find limit of  if,\lim_{x\to 1} \frac{f(x) - f(1)}{x-1} f(x)=-\sqrt{25-x^2},"I have a  question , if  then find  $$\lim_{x\to 1} \frac{f(x) - f(1)}{x-1}$$ I got $f(1)=-\sqrt {24}$. Should I get limit = $\frac{\sqrt{24} - \sqrt{25-x^2}}{x-1}$ but answer is $\frac{1}{\sqrt{24}}$ . Should I use $f'(x)$? If I use $f'(x)$ then I got $-\frac{1}{\sqrt{24}}$ .I should not get minus in solution. Please help, Thanks in advance.","I have a  question , if  then find  $$\lim_{x\to 1} \frac{f(x) - f(1)}{x-1}$$ I got $f(1)=-\sqrt {24}$. Should I get limit = $\frac{\sqrt{24} - \sqrt{25-x^2}}{x-1}$ but answer is $\frac{1}{\sqrt{24}}$ . Should I use $f'(x)$? If I use $f'(x)$ then I got $-\frac{1}{\sqrt{24}}$ .I should not get minus in solution. Please help, Thanks in advance.",,"['limits', 'derivatives']"
17,Derivative of Softmax,Derivative of Softmax,,"I'm new to deep learning and am attempting to calculate the derivative of the following function with respect to the matrix w : $$p(a) = \frac{e^{w_a^Tx}}{\Sigma_{d} e^{w_d^Tx}}$$ Using quotient rule, I get: $$\frac{\partial p(a)}{\partial w} = \frac{xe^{w_a^Tx}\Sigma_{d} e^{w_d^Tx} - e^{w_a^Tx}\Sigma_{d} xe^{w_d^Tx}}{[\Sigma_{d} e^{w_d^Tx}]^2} = 0$$ I believe I'm doing something wrong, since the softmax function is commonly used as an activation function in deep learning (and thus cannot always have a derivative of 0). I've gone over similar questions , but they seem to gloss over this part of the calculation. I'd appreciate any pointers towards the right direction.","I'm new to deep learning and am attempting to calculate the derivative of the following function with respect to the matrix w : $$p(a) = \frac{e^{w_a^Tx}}{\Sigma_{d} e^{w_d^Tx}}$$ Using quotient rule, I get: $$\frac{\partial p(a)}{\partial w} = \frac{xe^{w_a^Tx}\Sigma_{d} e^{w_d^Tx} - e^{w_a^Tx}\Sigma_{d} xe^{w_d^Tx}}{[\Sigma_{d} e^{w_d^Tx}]^2} = 0$$ I believe I'm doing something wrong, since the softmax function is commonly used as an activation function in deep learning (and thus cannot always have a derivative of 0). I've gone over similar questions , but they seem to gloss over this part of the calculation. I'd appreciate any pointers towards the right direction.",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'machine-learning']"
18,Finding derivative given values of $f(x)$ and $g(x)$,Finding derivative given values of  and,f(x) g(x),"I am given this function to calculate the derivatitve. $\frac{d}{dx}\frac{f(x)g(x)}{x}$ at $x=5$ They also gave me a table of values: $$\begin{array}{c|ccccc} x&1&2&3&4&5\\ \hline f(x) & 2 & 4 & 1 & 5 & 3\\ f'(x) & 3 & 1 & 5 & 2 & 4\\ g(x) & 4 & 5 & 3 & 2 & 1\\ g'(x) & 1 & 3 & 2 & 4 & 5 \end{array}$$ I have tried using the quotient rule with no luck. I have also tried using the product rule on $f(x)g(x)$ then applying the quotient rule afterwords, but no luck. Can anyone show me how to tackle this bad boy? Quotient rule answer: $$h'(x)=\frac{f'(x)g'(x)(x)-1f(x)g(x)}{x^2}=\frac{97}{25}$$ Product rule answer: $$\frac{d}{dx}f(x)g(x)=f'(x)g(x)+f(x)g'(x)=19$$ so we have, $$\frac{19}{x}$$ Then, $$\frac{d}{dx}19x^{-1}=f'(x)g(x)+f(x)g'(x)=19/x^2=\frac{19}{25}$$","I am given this function to calculate the derivatitve. $\frac{d}{dx}\frac{f(x)g(x)}{x}$ at $x=5$ They also gave me a table of values: $$\begin{array}{c|ccccc} x&1&2&3&4&5\\ \hline f(x) & 2 & 4 & 1 & 5 & 3\\ f'(x) & 3 & 1 & 5 & 2 & 4\\ g(x) & 4 & 5 & 3 & 2 & 1\\ g'(x) & 1 & 3 & 2 & 4 & 5 \end{array}$$ I have tried using the quotient rule with no luck. I have also tried using the product rule on $f(x)g(x)$ then applying the quotient rule afterwords, but no luck. Can anyone show me how to tackle this bad boy? Quotient rule answer: $$h'(x)=\frac{f'(x)g'(x)(x)-1f(x)g(x)}{x^2}=\frac{97}{25}$$ Product rule answer: $$\frac{d}{dx}f(x)g(x)=f'(x)g(x)+f(x)g'(x)=19$$ so we have, $$\frac{19}{x}$$ Then, $$\frac{d}{dx}19x^{-1}=f'(x)g(x)+f(x)g'(x)=19/x^2=\frac{19}{25}$$",,"['calculus', 'derivatives']"
19,For a given $u(z)$ are there any $v(z)$ which make $f(z) = u(z)+iv(z)$ differentiable?,For a given  are there any  which make  differentiable?,u(z) v(z) f(z) = u(z)+iv(z),"I got the answer ""no such $v$ exists"" for both exercises. However, I am very surprised, since the $u's$ I have been given seem very nice and arbitrary. I would like to know if I solved them correctly, and if so, what underlying reason makes these seemingly nice functions so hostile? $1) u(z) = x^3$ Using the Cauchy Riemann equations, we need $3x^2 = v_y$ and $0 = v_x$. The first equation gives us $v(z) = 3x^2y + g_1 (y)$, and the second gives us $v(z) = g_2(y)$. There is no $v(z)$ which satisfies these requirements. $2) u(z) = x^2+y$ Using the Cauchy Riemann equations, we need $2x = v_y$ and $-1 = v_x$. The first equation gives us $v(z) = 2xy + g_1 (y)$, and the second gives us $v(z) = -x + g_2(y)$. There is no $v(z)$ which satisfies these requirements.","I got the answer ""no such $v$ exists"" for both exercises. However, I am very surprised, since the $u's$ I have been given seem very nice and arbitrary. I would like to know if I solved them correctly, and if so, what underlying reason makes these seemingly nice functions so hostile? $1) u(z) = x^3$ Using the Cauchy Riemann equations, we need $3x^2 = v_y$ and $0 = v_x$. The first equation gives us $v(z) = 3x^2y + g_1 (y)$, and the second gives us $v(z) = g_2(y)$. There is no $v(z)$ which satisfies these requirements. $2) u(z) = x^2+y$ Using the Cauchy Riemann equations, we need $2x = v_y$ and $-1 = v_x$. The first equation gives us $v(z) = 2xy + g_1 (y)$, and the second gives us $v(z) = -x + g_2(y)$. There is no $v(z)$ which satisfies these requirements.",,"['complex-analysis', 'derivatives']"
20,Why does the summation dissapear when taking the derivative of the sum of squares?,Why does the summation dissapear when taking the derivative of the sum of squares?,,"Why is it that the derivative of the sum of squares of a vector, w : \begin{eqnarray}  \frac{\lambda}{2n} \sum_w w^2, \end{eqnarray} turns out to be \begin{eqnarray}    \frac{\lambda}{n} w  \end{eqnarray} and not \begin{eqnarray}  \frac{\lambda}{n} \sum_w w \;? \end{eqnarray} Basically as I see it, we've got \begin{eqnarray}    w = [w_1, w_2, w_3 ...] \end{eqnarray} \begin{eqnarray}    \frac{d}{dw} \frac{\lambda}{2n} \sum_w w^2 = \frac{\lambda}{2n}(\frac{\partial}{\partial w_1} \sum_w w^2 + \frac{\partial}{\partial w_2} \sum_w w^2 + \frac{\partial}{\partial w_3} \sum_w w^2 ...) \end{eqnarray} \begin{eqnarray}    = \frac{\lambda}{n} (w_1 + w_2 + w_3 ...) \end{eqnarray} \begin{eqnarray}    = \frac{\lambda}{n} \sum_w w \end{eqnarray} I'm following this ebook here (equations 87/88, which are basically the same as what I've written above).  The main thing I don't understand is why we can eliminate the summation.  Any math books or writeups on the subject would also be helpful.","Why is it that the derivative of the sum of squares of a vector, w : \begin{eqnarray}  \frac{\lambda}{2n} \sum_w w^2, \end{eqnarray} turns out to be \begin{eqnarray}    \frac{\lambda}{n} w  \end{eqnarray} and not \begin{eqnarray}  \frac{\lambda}{n} \sum_w w \;? \end{eqnarray} Basically as I see it, we've got \begin{eqnarray}    w = [w_1, w_2, w_3 ...] \end{eqnarray} \begin{eqnarray}    \frac{d}{dw} \frac{\lambda}{2n} \sum_w w^2 = \frac{\lambda}{2n}(\frac{\partial}{\partial w_1} \sum_w w^2 + \frac{\partial}{\partial w_2} \sum_w w^2 + \frac{\partial}{\partial w_3} \sum_w w^2 ...) \end{eqnarray} \begin{eqnarray}    = \frac{\lambda}{n} (w_1 + w_2 + w_3 ...) \end{eqnarray} \begin{eqnarray}    = \frac{\lambda}{n} \sum_w w \end{eqnarray} I'm following this ebook here (equations 87/88, which are basically the same as what I've written above).  The main thing I don't understand is why we can eliminate the summation.  Any math books or writeups on the subject would also be helpful.",,"['derivatives', 'partial-differential-equations', 'partial-derivative']"
21,Proving $\lim _{ x\to\infty }{ f(x) }=\infty$ if $f'(x)>c$ for every $x$,Proving  if  for every,\lim _{ x\to\infty }{ f(x) }=\infty f'(x)>c x,"Given a differentiable function $f: (0,\infty) \rightarrow \mathbb R $ and $c>0$ such that $f'(x)>c$ for every $x$. Prove: $\lim _{ x\rightarrow\infty  }{ f(x) }=\infty$ Using the MVT, I got to $f(x)>c(x-x_0)+f(x_0)$, and I think I should proceed using the definition of limit, but I got stuck. Any help appreciated.","Given a differentiable function $f: (0,\infty) \rightarrow \mathbb R $ and $c>0$ such that $f'(x)>c$ for every $x$. Prove: $\lim _{ x\rightarrow\infty  }{ f(x) }=\infty$ Using the MVT, I got to $f(x)>c(x-x_0)+f(x_0)$, and I think I should proceed using the definition of limit, but I got stuck. Any help appreciated.",,"['calculus', 'derivatives']"
22,The derivative and extremum of a matrix function,The derivative and extremum of a matrix function,,"$$f(W)=(Ax-b)^TW(Ax-b)=x^TA^TWAx-2b^TWAx+b^TWb$$ where $f(W)$ is a function of $W$, $A$ is a known matrix, $x$ and $b$ are vectors ($b$ is known). How to get $\frac{\partial f}{\partial W}$?","$$f(W)=(Ax-b)^TW(Ax-b)=x^TA^TWAx-2b^TWAx+b^TWb$$ where $f(W)$ is a function of $W$, $A$ is a known matrix, $x$ and $b$ are vectors ($b$ is known). How to get $\frac{\partial f}{\partial W}$?",,"['derivatives', 'matrix-calculus', 'least-squares', 'scalar-fields', 'weighted-least-squares']"
23,Is $\nabla^{n}$ a correct terminology for a partial derivative?,Is  a correct terminology for a partial derivative?,\nabla^{n},"I'm learning optimization techniques and came accross the gradient ( nabla ) operator : $\nabla$. If I'm right, the $\nabla$ operator of a function means the vector of all its partial derivatives. Then, if for example I'm talking about this specific partial derivative $\left(\frac{\partial}{\partial y}\right) F(x,y,z)$ is it a right terminology to write something like : $\nabla^{y}$ (since we can't use the standard i-th -element-in-vector notation like $\nabla^{2}$ as it would mean the Laplacian). If it is not a correct terminology, is there a correct terminlogy to specify a specific component of the partial derivatives vector?","I'm learning optimization techniques and came accross the gradient ( nabla ) operator : $\nabla$. If I'm right, the $\nabla$ operator of a function means the vector of all its partial derivatives. Then, if for example I'm talking about this specific partial derivative $\left(\frac{\partial}{\partial y}\right) F(x,y,z)$ is it a right terminology to write something like : $\nabla^{y}$ (since we can't use the standard i-th -element-in-vector notation like $\nabla^{2}$ as it would mean the Laplacian). If it is not a correct terminology, is there a correct terminlogy to specify a specific component of the partial derivatives vector?",,"['calculus', 'derivatives', 'optimization', 'terminology']"
24,Name for functions whose derivative is expressed in terms of the function value.,Name for functions whose derivative is expressed in terms of the function value.,,"Is there a name for the class of functions whose derivative is expressed only in terms of the function value? One example is the exponential, another example is \begin{align} s_1(t) = \frac{1}{1 + e^{-t}} \end{align} with derivative \begin{align} s'_1(t) = s_1(t)[1-s_1(t)]. \end{align} Clarification My question is related to writing about Neural Networks (NN). In neural networks you calculate the derivative of the output relative to the input by means of an algorithm called backpropagation , or backprop (which is really nothing but the chain rule expressed in a computationally-efficient manner). An important computational advantage while doing backprop is to store the function value when propagating forward, and using that function value to compute the derivative when propagating backward. This is only possible if the derivative only depends on the function value (and not on, say, the variable value). For example. Suppose that you have a working vector w : # w is currently storing the value of t, the independent variable w = [1, 2, 3] in the first step you calculate the function value (you won't need the value of the independent variable $t$ anymore, so you overwrite the contents in memory) # w is currently storing the value of s(t) = 1 / [1 + exp(-t)] w = [0.7310585786300049, 0.8807970779778823, 0.9525741268224334] in the next step you calculate the derivative value (you won't need the value of $s(t)$ anymore, so you overwrite the contents in memory) # w is currently storing the value of s'(t) = s(t)[1-s(t)] w = [0.19661193324148185, 0.10499358540350662, 0.045176659730912] Notice that if $s'$ had a dependency on the value of $t$ (as opposed to only the value of $s(t)$) I would not be able to reuse the memory in w . The specific paragraph I'm trying to improve reads as follows: Pick an activation function whose derivative depends only on the   function value, and not on the value of the independent variable. Such   activation functions enable reusing the working memory during the   backprop stage. And I'd like it to know if this could be expressed more precisely: pick a ??? activation function .","Is there a name for the class of functions whose derivative is expressed only in terms of the function value? One example is the exponential, another example is \begin{align} s_1(t) = \frac{1}{1 + e^{-t}} \end{align} with derivative \begin{align} s'_1(t) = s_1(t)[1-s_1(t)]. \end{align} Clarification My question is related to writing about Neural Networks (NN). In neural networks you calculate the derivative of the output relative to the input by means of an algorithm called backpropagation , or backprop (which is really nothing but the chain rule expressed in a computationally-efficient manner). An important computational advantage while doing backprop is to store the function value when propagating forward, and using that function value to compute the derivative when propagating backward. This is only possible if the derivative only depends on the function value (and not on, say, the variable value). For example. Suppose that you have a working vector w : # w is currently storing the value of t, the independent variable w = [1, 2, 3] in the first step you calculate the function value (you won't need the value of the independent variable $t$ anymore, so you overwrite the contents in memory) # w is currently storing the value of s(t) = 1 / [1 + exp(-t)] w = [0.7310585786300049, 0.8807970779778823, 0.9525741268224334] in the next step you calculate the derivative value (you won't need the value of $s(t)$ anymore, so you overwrite the contents in memory) # w is currently storing the value of s'(t) = s(t)[1-s(t)] w = [0.19661193324148185, 0.10499358540350662, 0.045176659730912] Notice that if $s'$ had a dependency on the value of $t$ (as opposed to only the value of $s(t)$) I would not be able to reuse the memory in w . The specific paragraph I'm trying to improve reads as follows: Pick an activation function whose derivative depends only on the   function value, and not on the value of the independent variable. Such   activation functions enable reusing the working memory during the   backprop stage. And I'd like it to know if this could be expressed more precisely: pick a ??? activation function .",,['derivatives']
25,Are these parametrizations of the same curve?,Are these parametrizations of the same curve?,,"I've a doubt concerning parametrization of $f(x) = x^2$.  I'm studying curves can be parametrized in different ways, changing the speed with which they travel their path. I'm pretty sure that if I'll write,                        $$ f(t)=(t,t^2)$$  and $$  f(t)=(2t,4t^2)$$ they represents the same curve travelled with different speed. I'm not sure about it if we write something like, $$ f(t)=(t^2,t^4)$$ Does the fact that the components of the latter aren't anymore multiples of the previous ones change something? I know the question can seem a bit ambiguous and wide, but any explanation or advice on the topic is welcomed with enthusiasm!","I've a doubt concerning parametrization of $f(x) = x^2$.  I'm studying curves can be parametrized in different ways, changing the speed with which they travel their path. I'm pretty sure that if I'll write,                        $$ f(t)=(t,t^2)$$  and $$  f(t)=(2t,4t^2)$$ they represents the same curve travelled with different speed. I'm not sure about it if we write something like, $$ f(t)=(t^2,t^4)$$ Does the fact that the components of the latter aren't anymore multiples of the previous ones change something? I know the question can seem a bit ambiguous and wide, but any explanation or advice on the topic is welcomed with enthusiasm!",,"['derivatives', 'parametric', 'parametrization']"
26,Is it possible to take infinite derivates on a finite interval?,Is it possible to take infinite derivates on a finite interval?,,"This question comes from physics but I think it fits better here. On page 488 of Sakurai's Modern Quantum Mechanics he says that in order to find an infinite series of higher order derivatives of a function, i.e. $$ i\frac{\partial}{\partial t}\psi(x,t) = \left[m - \frac{1}{2m}\frac{\partial^2}{\partial x^2} + \frac{1}{8m^3}\frac{\partial^4}{\partial x^4}-\cdots\right]\psi(x,t), $$  you would have to specify the function at ""farther and farther away from the origin"" (or whatever point you are evaluating at). The problem (in the physics context) is that this implies nonlocality. Ideally, you should be able to specify the time derivative and spacial derivatives at a point, solving the equation. However, I think Sakurai means to imply that knowing all derivatives at a point is equivalent to knowing the value of the function everywhere. My question is is it possible to take all derivatives $\frac{\partial^n}{\partial x^n}\psi$ for $n \in \mathbb{N}$ only knowing the value of the function on a finite interval? (thanks to Roland for the wording). If so, can't we do the same thing we do with the first derivative (making the interval smaller and smaller) in order to make the interval vanishingly small? Could we then recover locality?","This question comes from physics but I think it fits better here. On page 488 of Sakurai's Modern Quantum Mechanics he says that in order to find an infinite series of higher order derivatives of a function, i.e. $$ i\frac{\partial}{\partial t}\psi(x,t) = \left[m - \frac{1}{2m}\frac{\partial^2}{\partial x^2} + \frac{1}{8m^3}\frac{\partial^4}{\partial x^4}-\cdots\right]\psi(x,t), $$  you would have to specify the function at ""farther and farther away from the origin"" (or whatever point you are evaluating at). The problem (in the physics context) is that this implies nonlocality. Ideally, you should be able to specify the time derivative and spacial derivatives at a point, solving the equation. However, I think Sakurai means to imply that knowing all derivatives at a point is equivalent to knowing the value of the function everywhere. My question is is it possible to take all derivatives $\frac{\partial^n}{\partial x^n}\psi$ for $n \in \mathbb{N}$ only knowing the value of the function on a finite interval? (thanks to Roland for the wording). If so, can't we do the same thing we do with the first derivative (making the interval smaller and smaller) in order to make the interval vanishingly small? Could we then recover locality?",,"['derivatives', 'quantum-mechanics']"
27,Prove the function $f(x)$ have an extreme on $a$,Prove the function  have an extreme on,f(x) a,"If $$f'(a)=f''(a)=f'''(a)=0$$$$ f^{(4)}\ne0$$ Will the function have an extreme on $a$? I have the solution "" Yes because the taylor function will be:""$$f(x) = f(a) + \frac{1}{4!}f^{(4)}(a)(x − a)^4 + o((x − a)^4)$$ But I can't see why that proves there is an extreme.","If $$f'(a)=f''(a)=f'''(a)=0$$$$ f^{(4)}\ne0$$ Will the function have an extreme on $a$? I have the solution "" Yes because the taylor function will be:""$$f(x) = f(a) + \frac{1}{4!}f^{(4)}(a)(x − a)^4 + o((x − a)^4)$$ But I can't see why that proves there is an extreme.",,"['derivatives', 'taylor-expansion', 'maxima-minima']"
28,Does $(x+1)\log{(x+1)}-2x\log{x}+(x-1)\log{(x-1)}\geq\frac{1}{x}$ hold for $x\geq 1$?,Does  hold for ?,(x+1)\log{(x+1)}-2x\log{x}+(x-1)\log{(x-1)}\geq\frac{1}{x} x\geq 1,"While calculating some integrals I happened to face the following estimate: $$\int_m^{m+1}\int_n^{n+1}\frac{dy dx}{x+y}\geq\frac{1}{m+n+1}.$$ After some tedious calculations, I figured that this estimate follows from the inequality $$ (x+1)\log{(x+1)}-2x\log{x}+(x-1)\log{(x-1)}\geq\frac{1}{x}\quad\text{for }x>1.$$ (If we interpret $0\log0$ as $\lim_{\epsilon\rightarrow 0} \epsilon\log{\epsilon}=0$, then the inequality also holds for $x=1$.) But how do we prove this inequality? What I have tried : Let $f(x)=x\log{x}-(x-1)\log{(x-1)}$ for $x>1$. Then the RHS equals $f(x+1)-f(x)$ so by the Mean Value Theorem there exist some $\xi$ between $x$ and $x+1$ such that $$f(x+1)-f(x)=f'(\xi)=\log{\left(1+\frac{1}{\xi-1}\right)},$$ and it suffices to show that $\log{(1+\frac{1}{x})}\geq\frac{1}{x}$ ... which is unfortunately not valid ! I think some clever use of the MVT can solve this problem, but I don't see how I should proceed. Please enlighten me.","While calculating some integrals I happened to face the following estimate: $$\int_m^{m+1}\int_n^{n+1}\frac{dy dx}{x+y}\geq\frac{1}{m+n+1}.$$ After some tedious calculations, I figured that this estimate follows from the inequality $$ (x+1)\log{(x+1)}-2x\log{x}+(x-1)\log{(x-1)}\geq\frac{1}{x}\quad\text{for }x>1.$$ (If we interpret $0\log0$ as $\lim_{\epsilon\rightarrow 0} \epsilon\log{\epsilon}=0$, then the inequality also holds for $x=1$.) But how do we prove this inequality? What I have tried : Let $f(x)=x\log{x}-(x-1)\log{(x-1)}$ for $x>1$. Then the RHS equals $f(x+1)-f(x)$ so by the Mean Value Theorem there exist some $\xi$ between $x$ and $x+1$ such that $$f(x+1)-f(x)=f'(\xi)=\log{\left(1+\frac{1}{\xi-1}\right)},$$ and it suffices to show that $\log{(1+\frac{1}{x})}\geq\frac{1}{x}$ ... which is unfortunately not valid ! I think some clever use of the MVT can solve this problem, but I don't see how I should proceed. Please enlighten me.",,"['calculus', 'real-analysis', 'derivatives', 'inequality', 'fractions']"
29,Smoothness of continuous bilinear maps,Smoothness of continuous bilinear maps,,"$E_1,E_2,F$ are Banach spaces. There is one thing I don't understand if the following proof: I don't understand how he can say ""This proves the first assertion."" I'm assuming that $E_1 \times E_2$ is provided the sum norm $\|(h_1,h_2)\| = \|h_1\| + \|h_2\|$, so if we call $\lambda : E_1 \times E_2 \to \,\, ; \,\, (h_1,h_2) \mapsto \omega(x_1,h_2) + \omega(h_1,x_2)$, what he shows is that $\frac{\omega((x_1,x_2) + (h_1,h_2)) - \omega(x_1,x_2) - \lambda(h_1,h_2)}{\| (h_1,h_2) \|} \\ = \frac{ \omega(x_1+h_1,x_2+h_2) - \omega(x_1,x_2) - (\omega(x_1,h_2) + \omega(h_1,x_2))}{ \|(h_1,h_2) \| } \\ = \frac{ \omega(h_1,h_2) }{ \| (h_1,h_2) \|} = \frac{ \omega(h_1,h_2)}{ \|h_1\| + \|h_2\|}$ due to bilinearity. I know that $\| \omega(h_1,h_2) \| \leqslant \| \omega \| \|h_1\| \|h_2 \|$, where $\| \omega \| = sup_{\|h_1\|=1,\|h_2\|=1} \omega(h_1,h_2)$ but I don't think that is enough to prove that $\omega(h_1,h_2)/(\|h_1\|+\|h_2\|) \to 0$ as $(h_1,h_2) \to 0$, since I don't think $\frac{\|h_1\| \|h_2\| }{\|h_1\| + \|h_2\|}$ tends to zero as $(h_1,h_2) \to 0$. What am I missing?","$E_1,E_2,F$ are Banach spaces. There is one thing I don't understand if the following proof: I don't understand how he can say ""This proves the first assertion."" I'm assuming that $E_1 \times E_2$ is provided the sum norm $\|(h_1,h_2)\| = \|h_1\| + \|h_2\|$, so if we call $\lambda : E_1 \times E_2 \to \,\, ; \,\, (h_1,h_2) \mapsto \omega(x_1,h_2) + \omega(h_1,x_2)$, what he shows is that $\frac{\omega((x_1,x_2) + (h_1,h_2)) - \omega(x_1,x_2) - \lambda(h_1,h_2)}{\| (h_1,h_2) \|} \\ = \frac{ \omega(x_1+h_1,x_2+h_2) - \omega(x_1,x_2) - (\omega(x_1,h_2) + \omega(h_1,x_2))}{ \|(h_1,h_2) \| } \\ = \frac{ \omega(h_1,h_2) }{ \| (h_1,h_2) \|} = \frac{ \omega(h_1,h_2)}{ \|h_1\| + \|h_2\|}$ due to bilinearity. I know that $\| \omega(h_1,h_2) \| \leqslant \| \omega \| \|h_1\| \|h_2 \|$, where $\| \omega \| = sup_{\|h_1\|=1,\|h_2\|=1} \omega(h_1,h_2)$ but I don't think that is enough to prove that $\omega(h_1,h_2)/(\|h_1\|+\|h_2\|) \to 0$ as $(h_1,h_2) \to 0$, since I don't think $\frac{\|h_1\| \|h_2\| }{\|h_1\| + \|h_2\|}$ tends to zero as $(h_1,h_2) \to 0$. What am I missing?",,"['calculus', 'linear-algebra', 'derivatives']"
30,At what point the function $f(z)=|z|^2+i\bar z+1$ is differentiable?,At what point the function  is differentiable?,f(z)=|z|^2+i\bar z+1,"The function $f(z)=|z|^2+i\bar z+1$ is differentiable at $i$ $1$ $-i$ no point in $\mathbb{C}$ So we need to use the definition $f'(z)=\lim_{\Delta z\to 0}\frac{f(z+\Delta z)-f(z)}{\Delta z}$, right? I can write it as $f(z)=z\bar z+i\bar z+1$. How should I proceed next? Any hint ? Thanks.","The function $f(z)=|z|^2+i\bar z+1$ is differentiable at $i$ $1$ $-i$ no point in $\mathbb{C}$ So we need to use the definition $f'(z)=\lim_{\Delta z\to 0}\frac{f(z+\Delta z)-f(z)}{\Delta z}$, right? I can write it as $f(z)=z\bar z+i\bar z+1$. How should I proceed next? Any hint ? Thanks.",,"['complex-analysis', 'derivatives']"
31,Find the partial derivative of a function whih is the integral of another function,Find the partial derivative of a function whih is the integral of another function,,"I've got an exercise and I am not really sure which is the correct answer. Exercise $f$ is R-integrable. Find the partial derivatives for function $g(x,y) = \int_0^{xy} \! f(t) \, \mathrm{d}t.$ End I've found out that I should use The Second Fundamental Theorem of Calculus (from Michael Spivak - Calculus) which says: If $f$ is integrable on $[a,b]$ and $f=g'$ for some function $g$ then $\int_a^b \ = g(b) - g(a)$ I didn't found out how to apply it for my function $g(x,y)$. I wanted to use The First Fundamental Theorem of Calculus , but function $f$ needs to be continuous and I don't have this information. My Solution $f = g'$ which means that: $f(x) = \frac{\partial g}{\partial x} (x,y) = \frac{\partial g}{\partial x} \int_0^{xy} \! f(t) \, \mathrm{d}t.$ $f(y) = \frac{\partial g}{\partial y} (x,y) = \frac{\partial g}{\partial y} \int_0^{xy} \! f(t) \, \mathrm{d}t.$ What now? In my opinion the partial derivatives are $f(x)$ and $f(y)$, but I have no proof for that.","I've got an exercise and I am not really sure which is the correct answer. Exercise $f$ is R-integrable. Find the partial derivatives for function $g(x,y) = \int_0^{xy} \! f(t) \, \mathrm{d}t.$ End I've found out that I should use The Second Fundamental Theorem of Calculus (from Michael Spivak - Calculus) which says: If $f$ is integrable on $[a,b]$ and $f=g'$ for some function $g$ then $\int_a^b \ = g(b) - g(a)$ I didn't found out how to apply it for my function $g(x,y)$. I wanted to use The First Fundamental Theorem of Calculus , but function $f$ needs to be continuous and I don't have this information. My Solution $f = g'$ which means that: $f(x) = \frac{\partial g}{\partial x} (x,y) = \frac{\partial g}{\partial x} \int_0^{xy} \! f(t) \, \mathrm{d}t.$ $f(y) = \frac{\partial g}{\partial y} (x,y) = \frac{\partial g}{\partial y} \int_0^{xy} \! f(t) \, \mathrm{d}t.$ What now? In my opinion the partial derivatives are $f(x)$ and $f(y)$, but I have no proof for that.",,"['calculus', 'integration', 'derivatives', 'partial-derivative']"
32,Showing that for a continuously thrice differentiable function $f(x)$ the following is true,Showing that for a continuously thrice differentiable function  the following is true,f(x),"Show that for a continuously thrice differentiable function $f(x)$ : $$f(x)=f(0)+xf'(0)+\frac{f''(0)x^2}{2}+\frac{1}{2}\int_{0}^{x}f'''(t)(x-t)^2dt$$ I tried by assuming that $f(x)$ can be represented as a sum of powers of $x$. Say, $$f(x)=a_0+a_1x+a_2x^2+a_3x^3+...$$ By simple differentiation (twice) we conclude that $$a_0=f(0),a_1=f'(0)  ,a_2=\frac{f''(0)}{2}$$ So first three terms have been proved equal. But how to prove that the sum of the rest of the terms is $\frac{1}{2}\int_{0}^{x}f'''(t)(x-t)^2dt$ ? Also, is assuming that $f(x)$ can be represented as a sum of powers of $x$ correct ? Why or why not ? P.S: I don't know Taylor series or Maclaurin series. Please don't use them. I have just started learning integral calculus a few days back. So do not use very advanced concepts other than basic differential and integral calculus.","Show that for a continuously thrice differentiable function $f(x)$ : $$f(x)=f(0)+xf'(0)+\frac{f''(0)x^2}{2}+\frac{1}{2}\int_{0}^{x}f'''(t)(x-t)^2dt$$ I tried by assuming that $f(x)$ can be represented as a sum of powers of $x$. Say, $$f(x)=a_0+a_1x+a_2x^2+a_3x^3+...$$ By simple differentiation (twice) we conclude that $$a_0=f(0),a_1=f'(0)  ,a_2=\frac{f''(0)}{2}$$ So first three terms have been proved equal. But how to prove that the sum of the rest of the terms is $\frac{1}{2}\int_{0}^{x}f'''(t)(x-t)^2dt$ ? Also, is assuming that $f(x)$ can be represented as a sum of powers of $x$ correct ? Why or why not ? P.S: I don't know Taylor series or Maclaurin series. Please don't use them. I have just started learning integral calculus a few days back. So do not use very advanced concepts other than basic differential and integral calculus.",,"['calculus', 'sequences-and-series']"
33,If $\phi(x)=\cos(x)-\int_0^x(x-t)\phi(t)dt$ then what is the value of $\phi(x)+\phi''(x)$?,If  then what is the value of ?,\phi(x)=\cos(x)-\int_0^x(x-t)\phi(t)dt \phi(x)+\phi''(x),"If $$\phi(x)=\cos(x)-\int_0^x(x-t)\phi(t)dt$$ then what is the value of $\phi(x)+\phi''(x)$ ? My Attempt: $$\phi'(x)=-\sin(x)-(x-x)\phi(x)\dfrac{d(x)}{dx}+(x-0)\dfrac{d(0)}{dx}=-\sin(x)$$ (using Leibniz Rule) $$\implies \phi''(x)=-\cos(x)$$ So, $$\phi(x)+\phi''(x)=-\int_0^x(x-t)\phi(t)dt$$ How to simplify this? The final answer is given as $-\cos(x)$ but I am not getting it. Please help.","If $$\phi(x)=\cos(x)-\int_0^x(x-t)\phi(t)dt$$ then what is the value of $\phi(x)+\phi''(x)$ ? My Attempt: $$\phi'(x)=-\sin(x)-(x-x)\phi(x)\dfrac{d(x)}{dx}+(x-0)\dfrac{d(0)}{dx}=-\sin(x)$$ (using Leibniz Rule) $$\implies \phi''(x)=-\cos(x)$$ So, $$\phi(x)+\phi''(x)=-\int_0^x(x-t)\phi(t)dt$$ How to simplify this? The final answer is given as $-\cos(x)$ but I am not getting it. Please help.",,"['calculus', 'integration']"
34,Is $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ continuous? or differentiable anywhere?,Is  continuous? or differentiable anywhere?,\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n},"Does it make sense to call $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ continuous? For any finite n it is easy to declare that the function is continuous. Now when we let $n \to \infty$ we get a function that would seem pretty continuous, with each value very very close to its neighbors. However when we attempt to take the derivative anywhere on the function (except for a few key points) we find due the infinitesimal and ever present oscillations inherent in the function it doesn't really make sense to take the derivative. My hunch is to say that $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ is continuous but not differentiable. But how can one justify that? PS: if you want to see an interactive, zoomable version of the function PPS: Could the function possibly be differentiable at $f(2\pi k)$ for $k =$ some integer. At $2\pi k $ the derivative of $ \frac{\sin(2^nx)}{2^n}$ is $\frac{1}{2^n}$ for all n. Does that imply diferentiablitity at $2 \pi k$?","Does it make sense to call $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ continuous? For any finite n it is easy to declare that the function is continuous. Now when we let $n \to \infty$ we get a function that would seem pretty continuous, with each value very very close to its neighbors. However when we attempt to take the derivative anywhere on the function (except for a few key points) we find due the infinitesimal and ever present oscillations inherent in the function it doesn't really make sense to take the derivative. My hunch is to say that $\sum_{n=1}^{\infty} \frac{\sin(2^nx)}{2^n}$ is continuous but not differentiable. But how can one justify that? PS: if you want to see an interactive, zoomable version of the function PPS: Could the function possibly be differentiable at $f(2\pi k)$ for $k =$ some integer. At $2\pi k $ the derivative of $ \frac{\sin(2^nx)}{2^n}$ is $\frac{1}{2^n}$ for all n. Does that imply diferentiablitity at $2 \pi k$?",,"['derivatives', 'continuity', 'fourier-series']"
35,Decrease rate of the shadow,Decrease rate of the shadow,,"If a man of height $6ft$ moves with $5ft/sec$ velocity towards a lamp hanging at $15ft$ height, then at what rate his shadow will decrease? My Work : Let the initial distance between the man and the lamp post be $x$ and the length of shadow be $y$. Then, $\frac{x}{9} = \frac{y}{6} \implies y = \frac{2x}{3}$ Now how do I handle the $5ft/sec$ velocity and the decrease rate of the shadow? I think the rest part is so easy. But I am missing somewhere. Note : This is a problem from BdPhO 2016 but completely math related. So I posted it here.","If a man of height $6ft$ moves with $5ft/sec$ velocity towards a lamp hanging at $15ft$ height, then at what rate his shadow will decrease? My Work : Let the initial distance between the man and the lamp post be $x$ and the length of shadow be $y$. Then, $\frac{x}{9} = \frac{y}{6} \implies y = \frac{2x}{3}$ Now how do I handle the $5ft/sec$ velocity and the decrease rate of the shadow? I think the rest part is so easy. But I am missing somewhere. Note : This is a problem from BdPhO 2016 but completely math related. So I posted it here.",,"['calculus', 'linear-algebra', 'derivatives', 'contest-math']"
36,Differentiation of a scalar function w.r.t. a vector,Differentiation of a scalar function w.r.t. a vector,,"I'd like to know how to take a differentiation w.r.t. a vector as follows: $$\frac{\partial \log f({\bf x}-{\bf y})}{\partial {\bf y}}$$  where $f:{\mathbb R}^p \to {\mathbb R}$ and ${\bf x},{\bf y}\in{\mathbb R}^p$. My calculation is  $$\frac{\partial \log f({\bf x}-{\bf y})}{\partial {\bf y}} =-\frac{f'({\bf x}-{\bf y})}{f({\bf x}-{\bf y})}{\bf I_p}$$ But it should be a vector because this is a differentiation of a scalar function $f(\cdot)$ with respect to a vector ${\bf y}$.  Any comments would be appreciated.","I'd like to know how to take a differentiation w.r.t. a vector as follows: $$\frac{\partial \log f({\bf x}-{\bf y})}{\partial {\bf y}}$$  where $f:{\mathbb R}^p \to {\mathbb R}$ and ${\bf x},{\bf y}\in{\mathbb R}^p$. My calculation is  $$\frac{\partial \log f({\bf x}-{\bf y})}{\partial {\bf y}} =-\frac{f'({\bf x}-{\bf y})}{f({\bf x}-{\bf y})}{\bf I_p}$$ But it should be a vector because this is a differentiation of a scalar function $f(\cdot)$ with respect to a vector ${\bf y}$.  Any comments would be appreciated.",,"['derivatives', 'vector-analysis', 'matrix-calculus', 'chain-rule']"
37,Derivative of the determinant of the right stretch tensor,Derivative of the determinant of the right stretch tensor,,"I have to evaluate the derivative $$ \frac{\partial\det\mathcal{U}}{\partial F} $$ where $\mathcal{U}=\sqrt{F^TF}$ and $F$ is a $m\times n$ real matrix. Any suggestion would be appreciated. Thank you all, guys!! You helped me a lot.","I have to evaluate the derivative $$ \frac{\partial\det\mathcal{U}}{\partial F} $$ where $\mathcal{U}=\sqrt{F^TF}$ and $F$ is a $m\times n$ real matrix. Any suggestion would be appreciated. Thank you all, guys!! You helped me a lot.",,"['linear-algebra', 'matrices', 'derivatives', 'determinant', 'matrix-calculus']"
38,Basic geometry question and proof that $dT/d\Theta = 1 + T^2$ [duplicate],Basic geometry question and proof that  [duplicate],dT/d\Theta = 1 + T^2,"This question already has answers here : Finding $\frac {d(\tan \theta)}{d\theta}$ (2 answers) Closed 3 years ago . I started to flick through Needham's Visual Complex Analysis and pretty much fell on my face in the first exercise. On page ix, he shows a proof that $dT/d\Theta = 1 + T^2$ if $T = tan(\Theta)$. He does so by comparing the black triangle with the initial triangle (grey). What I don't understand is why the length of the one segment is $L*d\Theta$. I'm sure I'm lacking some basic math here..... Shouldn't the length be $L*tan(d\Theta)$ (which obviously would be less helpful)?","This question already has answers here : Finding $\frac {d(\tan \theta)}{d\theta}$ (2 answers) Closed 3 years ago . I started to flick through Needham's Visual Complex Analysis and pretty much fell on my face in the first exercise. On page ix, he shows a proof that $dT/d\Theta = 1 + T^2$ if $T = tan(\Theta)$. He does so by comparing the black triangle with the initial triangle (grey). What I don't understand is why the length of the one segment is $L*d\Theta$. I'm sure I'm lacking some basic math here..... Shouldn't the length be $L*tan(d\Theta)$ (which obviously would be less helpful)?",,"['geometry', 'derivatives']"
39,Showing a function is absolutely continuous and finding its derivative,Showing a function is absolutely continuous and finding its derivative,,"Let $A$ be a measurable subset of $[0,2]$ and define $f:\mathbb{R}\to\mathbb{R}$ by letting $f(x) = \mu((-\infty,x]\cap A)$ for every $x\in\mathbb{R}$. (a) Show that $f$ is absolutely continuous on $\mathbb{R}$, calculate $f'$ and $\int_0^3f'(x)dx$ (b) Show that for every $0<b<\mu(A)$ there exists $x_0\in\mathbb{R}$ such that $b=\mu((-\infty,x_0]\cap A)$ For (a), I was able to show that $f$ is absolutely continuous by applying the definition that for every $\varepsilon>0$ there is a $\delta>0$ such that for every finite disjoint collection $\{(a_k,b_k) \}_{k=1}^n$ of open intervals then $\sum b_k-a_k < \delta \Rightarrow \sum|f(b_k)-f(a_k)|<\varepsilon$. Taking $\delta=\varepsilon$, it's pretty easy to see that this definition is satisfied. I also know that since $f$ is absolutely continuous, then $\int_a^bf'(x)dx$=f(b)-f(a), so $\int_0^3f'(x)dx= f(3)-f(0) = \mu(A)$. I'm having trouble actually calculating what $f'$ is though. I tried using the difference quotient, and I simplify it down to $$f'=\lim_{h\to0}\frac{\mu((t,t+h]\cap A)}{h} $$ I'm not sure where to proceed from here though, if this is even the right way to go. Any help would be much appreciated. I think part (b) is simply the Intermediate Value Theorem, but if it's not I would appreciate a hint.","Let $A$ be a measurable subset of $[0,2]$ and define $f:\mathbb{R}\to\mathbb{R}$ by letting $f(x) = \mu((-\infty,x]\cap A)$ for every $x\in\mathbb{R}$. (a) Show that $f$ is absolutely continuous on $\mathbb{R}$, calculate $f'$ and $\int_0^3f'(x)dx$ (b) Show that for every $0<b<\mu(A)$ there exists $x_0\in\mathbb{R}$ such that $b=\mu((-\infty,x_0]\cap A)$ For (a), I was able to show that $f$ is absolutely continuous by applying the definition that for every $\varepsilon>0$ there is a $\delta>0$ such that for every finite disjoint collection $\{(a_k,b_k) \}_{k=1}^n$ of open intervals then $\sum b_k-a_k < \delta \Rightarrow \sum|f(b_k)-f(a_k)|<\varepsilon$. Taking $\delta=\varepsilon$, it's pretty easy to see that this definition is satisfied. I also know that since $f$ is absolutely continuous, then $\int_a^bf'(x)dx$=f(b)-f(a), so $\int_0^3f'(x)dx= f(3)-f(0) = \mu(A)$. I'm having trouble actually calculating what $f'$ is though. I tried using the difference quotient, and I simplify it down to $$f'=\lim_{h\to0}\frac{\mu((t,t+h]\cap A)}{h} $$ I'm not sure where to proceed from here though, if this is even the right way to go. Any help would be much appreciated. I think part (b) is simply the Intermediate Value Theorem, but if it's not I would appreciate a hint.",,"['real-analysis', 'measure-theory', 'derivatives']"
40,Continuity of $f^{(n)}$ in one point implies n-th differentiability of $f$ in that point?,Continuity of  in one point implies n-th differentiability of  in that point?,f^{(n)} f,"Is the following implication true? Let $f:\mathbb{R} \to \mathbb{R}$ be a continous function,   differentiable in all $\mathbb{R}$, besides at most one point $x_0$. $$f^{(n)}(x) \,\,\mathrm{is \,\,\, continous \,\,\, in \,\,\,} x_0  \implies f \,\,\,\mathrm{is \,\,\, differentiable \,\,\, in }\,\,\,  x_0  \,\,\, \mathrm{and} \,\,\, f^{n}(x_0)=\lim_{x \to x_0}f^{n}(x)$$ Where $f^{(n)}$ denotes the n-th derivative of $f$. I know that such theorem is valid for the first derivative $f'$, but is it valid (as stated) in the case of the n-th derivative?","Is the following implication true? Let $f:\mathbb{R} \to \mathbb{R}$ be a continous function,   differentiable in all $\mathbb{R}$, besides at most one point $x_0$. $$f^{(n)}(x) \,\,\mathrm{is \,\,\, continous \,\,\, in \,\,\,} x_0  \implies f \,\,\,\mathrm{is \,\,\, differentiable \,\,\, in }\,\,\,  x_0  \,\,\, \mathrm{and} \,\,\, f^{n}(x_0)=\lim_{x \to x_0}f^{n}(x)$$ Where $f^{(n)}$ denotes the n-th derivative of $f$. I know that such theorem is valid for the first derivative $f'$, but is it valid (as stated) in the case of the n-th derivative?",,"['calculus', 'real-analysis', 'integration', 'derivatives', 'continuity']"
41,Find the derivative of $f^{-1}(x)$ at $x=2$ if $f(x)=x^2 + x + \ln x$,Find the derivative of  at  if,f^{-1}(x) x=2 f(x)=x^2 + x + \ln x,I'm fairly confused with this question (or I guess the concept of inverse functions and taking their derivative). I know that the general rule for taking the derivative of an inverse function is: $$f^{-1}{'}(x) = \frac{1}{f'(f^{-1}(x))}$$ But I'm not really sure where to go from here. A nudge in the right direction would be greatly appreciated.,I'm fairly confused with this question (or I guess the concept of inverse functions and taking their derivative). I know that the general rule for taking the derivative of an inverse function is: $$f^{-1}{'}(x) = \frac{1}{f'(f^{-1}(x))}$$ But I'm not really sure where to go from here. A nudge in the right direction would be greatly appreciated.,,"['calculus', 'derivatives', 'inverse-function']"
42,First and second derivative of $|x|^3$,First and second derivative of,|x|^3,"I need to prove that $|x|^3$ is twice differentiable, by showing that the first and second derivatives exist using the definition. I've tried several ways, this is what I've got: $$\lim_{h\to 0} \frac{|x+h|^3 - |x|^3}{h} = \lim_{h\to 0}\frac{\sqrt{(x+h)^2}^3 - \sqrt{x^2}^3}{h}  = \lim_{h\to 0} \frac{\sqrt{(x+h)^6} - \sqrt{x^6}}{h}$$ then I rationalized the numerator: $$= \lim_{h\to 0} \frac{(x+h)^6 - x^6}{h\left(\sqrt{(x+h)^6} + \sqrt{x^6}\right)}$$ and I'm stuck on what to do next, I'm skeptic that this is the right way, but I was not able to reach any answer using other ways either. Any help would be great. Thanks.","I need to prove that $|x|^3$ is twice differentiable, by showing that the first and second derivatives exist using the definition. I've tried several ways, this is what I've got: $$\lim_{h\to 0} \frac{|x+h|^3 - |x|^3}{h} = \lim_{h\to 0}\frac{\sqrt{(x+h)^2}^3 - \sqrt{x^2}^3}{h}  = \lim_{h\to 0} \frac{\sqrt{(x+h)^6} - \sqrt{x^6}}{h}$$ then I rationalized the numerator: $$= \lim_{h\to 0} \frac{(x+h)^6 - x^6}{h\left(\sqrt{(x+h)^6} + \sqrt{x^6}\right)}$$ and I'm stuck on what to do next, I'm skeptic that this is the right way, but I was not able to reach any answer using other ways either. Any help would be great. Thanks.",,['limits']
43,If $f$ vanishes sufficiently fast at $x_0$ does this imply that all derivatives vanish as well?,If  vanishes sufficiently fast at  does this imply that all derivatives vanish as well?,f x_0,"Let $f: U \rightarrow \mathbb{R} $, with $U \subset \mathbb{R}^n$ open, be infinitely differentiable. Suppose there is a $x_0 \in U$ such that $$ \lim_{x \rightarrow x_0} \frac{f(x)}{|x-x_0|^k}=0 $$ for all $k \in \mathbb{N}$. How can we prove that every derivative $D^{\alpha} u(x_0)$, $\alpha \in \mathbb{N}^n$, is zero? For $|\alpha|=1$ we can just use the definition of the partial derivative. But in the case $n=2$ this already gets messy.","Let $f: U \rightarrow \mathbb{R} $, with $U \subset \mathbb{R}^n$ open, be infinitely differentiable. Suppose there is a $x_0 \in U$ such that $$ \lim_{x \rightarrow x_0} \frac{f(x)}{|x-x_0|^k}=0 $$ for all $k \in \mathbb{N}$. How can we prove that every derivative $D^{\alpha} u(x_0)$, $\alpha \in \mathbb{N}^n$, is zero? For $|\alpha|=1$ we can just use the definition of the partial derivative. But in the case $n=2$ this already gets messy.",,"['real-analysis', 'derivatives']"
44,Is $f(x)/x$ differentiable at $x=0$?,Is  differentiable at ?,f(x)/x x=0,"Let $\varepsilon> 0$, $I:=(-\varepsilon,\varepsilon)$ and $f\in C^\infty(I,\mathbb R)$ with $f(0)=0$. By the definition of the derivative we know that $$h:I\setminus\{0\}\to \mathbb R, \qquad x \mapsto \frac{f(x)}{x}$$ can be continuously extended to $I$ with $h(0):=f'(0)$. But what do we know about differentiability of $h$ at $0$? Is $h\in C^\infty(I,\mathbb R)$? EDIT: For analytic functions this is clearly the case. But what do we have in general?","Let $\varepsilon> 0$, $I:=(-\varepsilon,\varepsilon)$ and $f\in C^\infty(I,\mathbb R)$ with $f(0)=0$. By the definition of the derivative we know that $$h:I\setminus\{0\}\to \mathbb R, \qquad x \mapsto \frac{f(x)}{x}$$ can be continuously extended to $I$ with $h(0):=f'(0)$. But what do we know about differentiability of $h$ at $0$? Is $h\in C^\infty(I,\mathbb R)$? EDIT: For analytic functions this is clearly the case. But what do we have in general?",,"['real-analysis', 'derivatives', 'continuity']"
45,Investigating the differentiability of $f(x) = |\sin x |$,Investigating the differentiability of,f(x) = |\sin x |,"I'm given the function $$f(x) = |\sin x |$$ and I'm asked to investigate its differentiability. Well, from the graphic of $\sin x$ is clear that the function is differentiable at every point, except at $x = \pi k$ , where $k \in \mathbb{Z}$, since at that point the limit with $h \to 0^-$ and $h \to 0^+$ are not equal. I got this conclusion just by analyzing the graph of the function, but I'd appreciate is some of you can point out a way to reach this conclusion by applying the definition either of derivative or that of differential or by a more ''formal'' approach. This is my attempt: Since $x \in [-\pi/2, \pi/2]$ then the function becomes $$f(x)=\begin{cases}\sin x &\mbox{if, }0\leq x\leq\frac{\pi}{2}\\ -\sin x &\mbox{if, }-\frac{\pi}{2} \leq x<0 \end{cases}$$ Consider the left-hand limit and right-hand limit at $x_0 = 0$. we have $$\lim_{h\to 0^{-}} \frac{f(0+h)-f(0)}{h}=\lim_{h\to 0^{+}} \frac{f(0+h)-f(0)}{h}$$ $$\lim_{h\to 0^{-}} -\frac{\sin(0+h)-\sin(0)}{h}=\lim_{h\to 0^{+}} \frac{\sin(0+h)-\sin(0)}{h}$$ $$-\lim_{h\to 0^{-}} \frac{\sin(h)}{h}=\lim_{h\to 0^{+}} \frac{\sin(h)}{h}$$ Here I don't know how to consider the sign the left-hand limit (I'm somewhat rusty honestly). I'm not sure but if $$\lim_{h\to 0^{-}} \frac{\sin(h)}{h}=1$$ We're done since the LH limit would not be equal to the RH limit (we'd have $-1=1$)","I'm given the function $$f(x) = |\sin x |$$ and I'm asked to investigate its differentiability. Well, from the graphic of $\sin x$ is clear that the function is differentiable at every point, except at $x = \pi k$ , where $k \in \mathbb{Z}$, since at that point the limit with $h \to 0^-$ and $h \to 0^+$ are not equal. I got this conclusion just by analyzing the graph of the function, but I'd appreciate is some of you can point out a way to reach this conclusion by applying the definition either of derivative or that of differential or by a more ''formal'' approach. This is my attempt: Since $x \in [-\pi/2, \pi/2]$ then the function becomes $$f(x)=\begin{cases}\sin x &\mbox{if, }0\leq x\leq\frac{\pi}{2}\\ -\sin x &\mbox{if, }-\frac{\pi}{2} \leq x<0 \end{cases}$$ Consider the left-hand limit and right-hand limit at $x_0 = 0$. we have $$\lim_{h\to 0^{-}} \frac{f(0+h)-f(0)}{h}=\lim_{h\to 0^{+}} \frac{f(0+h)-f(0)}{h}$$ $$\lim_{h\to 0^{-}} -\frac{\sin(0+h)-\sin(0)}{h}=\lim_{h\to 0^{+}} \frac{\sin(0+h)-\sin(0)}{h}$$ $$-\lim_{h\to 0^{-}} \frac{\sin(h)}{h}=\lim_{h\to 0^{+}} \frac{\sin(h)}{h}$$ Here I don't know how to consider the sign the left-hand limit (I'm somewhat rusty honestly). I'm not sure but if $$\lim_{h\to 0^{-}} \frac{\sin(h)}{h}=1$$ We're done since the LH limit would not be equal to the RH limit (we'd have $-1=1$)",,"['calculus', 'real-analysis', 'derivatives']"
46,Calculate the indefinite integral of a given ration,Calculate the indefinite integral of a given ration,,"Let $f : \mathbb{R} \rightarrow (0, \infty)$ be a differentiable function such that its derivative is continuous. Calculate: $$\int \frac{f(x) + f'(x)}{f(x) + e^{-x}} dx$$ I need a solution which involves only the method of integration by parts. I've tried several ways to calculate the integral, but got nothing good. Thank you in advance!","Let $f : \mathbb{R} \rightarrow (0, \infty)$ be a differentiable function such that its derivative is continuous. Calculate: $$\int \frac{f(x) + f'(x)}{f(x) + e^{-x}} dx$$ I need a solution which involves only the method of integration by parts. I've tried several ways to calculate the integral, but got nothing good. Thank you in advance!",,"['derivatives', 'indefinite-integrals']"
47,Find the $n$th derivative of $f(x)=(\sqrt{x^2-1}+\sqrt{x-1})^2$,Find the th derivative of,n f(x)=(\sqrt{x^2-1}+\sqrt{x-1})^2,"In this case $n=16$ and the point is $x=1$. I know $\sqrt{x}$ is not differentiable in zero, but this function actually has left derivatives. Also, I know that $f(x)=(\sqrt{x^2-1}+\sqrt{x-1})^2=x^2+x-2+2\sqrt{(x^2-1)(x-1)}$, so it is enogth to compute the derivative of $2\sqrt{(x^2-1)(x-1)}$. Also, I tried to find the $n$th derivative of $\sqrt{g(x)}$ but its expresion was too complicated.","In this case $n=16$ and the point is $x=1$. I know $\sqrt{x}$ is not differentiable in zero, but this function actually has left derivatives. Also, I know that $f(x)=(\sqrt{x^2-1}+\sqrt{x-1})^2=x^2+x-2+2\sqrt{(x^2-1)(x-1)}$, so it is enogth to compute the derivative of $2\sqrt{(x^2-1)(x-1)}$. Also, I tried to find the $n$th derivative of $\sqrt{g(x)}$ but its expresion was too complicated.",,['derivatives']
48,Why does this derivative equation hold?,Why does this derivative equation hold?,,"$$\frac{d(Q/x)}{dx} = \frac{x(\frac{dQ}{dx})-Q(\frac{dx}{dx})}{x^2}$$ Assume $Q$ is a function of $x$. This equation is in my microeconomics textbook, but I don't know how we can get from the left-hand side to the right-hand side. Can someone please explain?","$$\frac{d(Q/x)}{dx} = \frac{x(\frac{dQ}{dx})-Q(\frac{dx}{dx})}{x^2}$$ Assume $Q$ is a function of $x$. This equation is in my microeconomics textbook, but I don't know how we can get from the left-hand side to the right-hand side. Can someone please explain?",,"['calculus', 'derivatives']"
49,Differentiating $\sqrt{\frac{1-x}{1+x}}$,Differentiating,\sqrt{\frac{1-x}{1+x}},I've differentiated the $\frac{1-x}{1+x}$ which is $\frac{-2}{(1+x)^2}$ I don't know how to proceed to it. Hope someone can show it. Thanks in advance.,I've differentiated the $\frac{1-x}{1+x}$ which is $\frac{-2}{(1+x)^2}$ I don't know how to proceed to it. Hope someone can show it. Thanks in advance.,,"['calculus', 'derivatives']"
50,"$f(z)$ analytic, then $|f(z)|$ or $\arg(z)$ constant, then $f(z)$ constant","analytic, then  or  constant, then  constant",f(z) |f(z)| \arg(z) f(z),"Ir order to show that $f(z)$ is constant I need to show that its partial derivatives are all $0$, that is: $$\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = \frac{\partial v}{\partial x} = -\frac{\partial u}{\partial y} = 0$$ I tried this: If $|f(z)|$ is constant, then $\sqrt{u^2+v^2} = c \implies$ $$\frac{2u}{2\sqrt{u^2+v^2}}=0\implies u = 0$$ and $$\frac{-2v}{2\sqrt{u^2+v^2}}=0 \implies v = 0$$ That's strange because I didn't even use Cauchy-Riemann and I tought it would be necessary. Also, how to do the part of the argument?","Ir order to show that $f(z)$ is constant I need to show that its partial derivatives are all $0$, that is: $$\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = \frac{\partial v}{\partial x} = -\frac{\partial u}{\partial y} = 0$$ I tried this: If $|f(z)|$ is constant, then $\sqrt{u^2+v^2} = c \implies$ $$\frac{2u}{2\sqrt{u^2+v^2}}=0\implies u = 0$$ and $$\frac{-2v}{2\sqrt{u^2+v^2}}=0 \implies v = 0$$ That's strange because I didn't even use Cauchy-Riemann and I tought it would be necessary. Also, how to do the part of the argument?",,"['calculus', 'complex-analysis', 'derivatives']"
51,"Chain rule for a least-square regression, where does the transpose come from?","Chain rule for a least-square regression, where does the transpose come from?",,"I am reading a machine learning book, and I can't understand a couple of things from the following explanation: Suppose we want to ﬁnd the value of $x$ that minimizes $$f(x) = \frac{1}{2} \left \| Ax - b \right \|^2_2$$ There are specialized linear algebra algorithms that can solve this problem eﬃciently. However, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work. First, we need to obtain the gradient: $$\triangledown_xf(x) = A^{T}(Ax-b)$$ ... Am I correct with my understanding that by "" specialized linear algebra algorithms "" they mean finding inverse (or pseudoinverse) and calculating $A^{-1}b$. But my main question is why do they have $A^{T}$ in the derivative? As far as I understood they apply chain rule, but I still can't understand where the transpose comes from.","I am reading a machine learning book, and I can't understand a couple of things from the following explanation: Suppose we want to ﬁnd the value of $x$ that minimizes $$f(x) = \frac{1}{2} \left \| Ax - b \right \|^2_2$$ There are specialized linear algebra algorithms that can solve this problem eﬃciently. However, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work. First, we need to obtain the gradient: $$\triangledown_xf(x) = A^{T}(Ax-b)$$ ... Am I correct with my understanding that by "" specialized linear algebra algorithms "" they mean finding inverse (or pseudoinverse) and calculating $A^{-1}b$. But my main question is why do they have $A^{T}$ in the derivative? As far as I understood they apply chain rule, but I still can't understand where the transpose comes from.",,"['linear-algebra', 'derivatives']"
52,Derivatives Theorems for functions from $\mathbb{R}$ to a Banach Space,Derivatives Theorems for functions from  to a Banach Space,\mathbb{R},"The following are true for functions $f$ from $\mathbb{R}$ to $\mathbb{R^n}$: Suppose f is continuous, $f'$ exists in $(x-\delta,x) \cup (x,x+\delta)$ and  $\lim\limits_{u \to x}{f'(u)} = l$ then $f'(x)=l$ Suppose $f'$ is continuous on $[a,b]$ then $\forall{\epsilon >0}$ $\exists \delta>0$ s.t. $\forall x,t \in [a,b] $, $|x-t|<\delta \implies  |{\frac{f(t)-f(x)}{t-x} - f'(x)}| < \epsilon$ Are they true for functions from $\mathbb{R}$ to a Banach space as well? These are from Baby Rudin Ch 5 Q 8,9, but he doesn't dicuss any generalizations. I'm not very familiar with theorems on Banach spaces, so I apologize if this is standard.","The following are true for functions $f$ from $\mathbb{R}$ to $\mathbb{R^n}$: Suppose f is continuous, $f'$ exists in $(x-\delta,x) \cup (x,x+\delta)$ and  $\lim\limits_{u \to x}{f'(u)} = l$ then $f'(x)=l$ Suppose $f'$ is continuous on $[a,b]$ then $\forall{\epsilon >0}$ $\exists \delta>0$ s.t. $\forall x,t \in [a,b] $, $|x-t|<\delta \implies  |{\frac{f(t)-f(x)}{t-x} - f'(x)}| < \epsilon$ Are they true for functions from $\mathbb{R}$ to a Banach space as well? These are from Baby Rudin Ch 5 Q 8,9, but he doesn't dicuss any generalizations. I'm not very familiar with theorems on Banach spaces, so I apologize if this is standard.",,"['real-analysis', 'functional-analysis', 'derivatives', 'banach-spaces']"
53,Can somebody show me a proof for this?,Can somebody show me a proof for this?,,"Equation of the Volume of a Cone: $$\mathbf{V}=\frac{1}{3}\pi r^2 h$$ Taking the Derivative in perspective to time because the radii, and height are either decreasing or increasing: $$\frac{dV}{dt}=\frac{\pi}{3}[2rh\frac{dr}{dt}+r^2\frac{dh}{dt}]$$ I understand how to take the derivative of it and all, moreover, I am lost on how to prove this a different way because I have seen it done one way substituting $r$ and $h$ for $r(t)$ and $h(t)$ but do not know any other to prove it. Is there another way to do it, and can somebody please show me?","Equation of the Volume of a Cone: $$\mathbf{V}=\frac{1}{3}\pi r^2 h$$ Taking the Derivative in perspective to time because the radii, and height are either decreasing or increasing: $$\frac{dV}{dt}=\frac{\pi}{3}[2rh\frac{dr}{dt}+r^2\frac{dh}{dt}]$$ I understand how to take the derivative of it and all, moreover, I am lost on how to prove this a different way because I have seen it done one way substituting $r$ and $h$ for $r(t)$ and $h(t)$ but do not know any other to prove it. Is there another way to do it, and can somebody please show me?",,"['calculus', 'derivatives']"
54,The vector $\mathbf{x}$ is the derivative of $\mathbf{Ax}$ with respect to what?,The vector  is the derivative of  with respect to what?,\mathbf{x} \mathbf{Ax},"Consider the linear equation $$\mathbf{A}\mathbf{x}=\mathbf{b},$$ where $\mathbf{A}=\begin{bmatrix}\mathbf{a}_1 \\ \vdots \\ \mathbf{a}_n\end{bmatrix}$ is an $N \times K$ random matrix $\mathbf{x}=\begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}$ is a $K \times 1$ vector $\mathbf{b}=\begin{bmatrix}b_1 \\ \vdots \\ b_n\end{bmatrix}$ is an $N \times 1$ random vector For each $K \times 1$ row $\mathbf{a}_n$ of the matrix $\mathbf{A}$, observe that $$\frac{\partial \mathbf{a}_n \mathbf{x}}{\partial a_k}=\frac{\partial (a_{n1} x_1 + a_{n2} x_2+\cdots+a_{nK} x_K)}{\partial a_{nk}}=x_k$$ Is there some way to generalize this statement for all columns $k$ simultaneously? In other words, The vector $\mathbf{x}$ is the derivative of the system of equations $\mathbf{Ax}$, taken with respect to what? Taking a quick look at this list of identities , I see that $\mathbf{A}$ is the derivative of $\mathbf{A}\mathbf{x}$ with respect to $\mathbf{x}$: $$\mathbf{A}=\frac{\partial \mathbf{Ax}}{\partial{\mathbf{x}}}$$ However, I don't see what derivative would give us $\mathbf{x}$ as an answer: $$\mathbf{x}=\frac{\partial \mathbf{Ax}}{\partial (?)}$$ I'm not particularly good with the rules of matrix differentiation, so a proof would be especially helpful here. Thank you!","Consider the linear equation $$\mathbf{A}\mathbf{x}=\mathbf{b},$$ where $\mathbf{A}=\begin{bmatrix}\mathbf{a}_1 \\ \vdots \\ \mathbf{a}_n\end{bmatrix}$ is an $N \times K$ random matrix $\mathbf{x}=\begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}$ is a $K \times 1$ vector $\mathbf{b}=\begin{bmatrix}b_1 \\ \vdots \\ b_n\end{bmatrix}$ is an $N \times 1$ random vector For each $K \times 1$ row $\mathbf{a}_n$ of the matrix $\mathbf{A}$, observe that $$\frac{\partial \mathbf{a}_n \mathbf{x}}{\partial a_k}=\frac{\partial (a_{n1} x_1 + a_{n2} x_2+\cdots+a_{nK} x_K)}{\partial a_{nk}}=x_k$$ Is there some way to generalize this statement for all columns $k$ simultaneously? In other words, The vector $\mathbf{x}$ is the derivative of the system of equations $\mathbf{Ax}$, taken with respect to what? Taking a quick look at this list of identities , I see that $\mathbf{A}$ is the derivative of $\mathbf{A}\mathbf{x}$ with respect to $\mathbf{x}$: $$\mathbf{A}=\frac{\partial \mathbf{Ax}}{\partial{\mathbf{x}}}$$ However, I don't see what derivative would give us $\mathbf{x}$ as an answer: $$\mathbf{x}=\frac{\partial \mathbf{Ax}}{\partial (?)}$$ I'm not particularly good with the rules of matrix differentiation, so a proof would be especially helpful here. Thank you!",,"['calculus', 'linear-algebra', 'matrices', 'derivatives']"
55,Show $f$ is differentiable at $x=1$.,Show  is differentiable at .,f x=1,"Let $f$ be a real valued continuous function on the interval $[0,2]$ which is differentiable at all points except $1$.Also $\lim f^{'}(x)=5$ .Show $f$ is differentiable at $x=1$. My effort : Consider $h\neq 0$ very small.Then $f$ is differentiable at $1+h$. By definition of derivative $f^{'}(1+h)=\lim _{h_1\to 0}\dfrac{f(1+h+h_1)-f(1+h)}{h_1}\rightarrow (1)$ Also $\lim _{x\to 1}f^{'}(x)=5\implies \lim_{h\to 0}f^{'}(1+h)=5\rightarrow (2)$ Now putting $(2)$ in $(1)$ we have $\lim_{h\to 0}\lim _{h_1\to 0}\dfrac{f(1+h+h_1)-f(1+h)}{h_1}=5$ $\hspace{45mm}\implies $$\lim_{h_1\to 0}\lim _{h\to 0}\dfrac{f(1+h+h_1)-f(1+h)}{h_1}=5$ As $f$ is continuous ;$\lim_{h_1\to 0}\dfrac{f(1+h_1)-f(1)}{h_1}=5$ Similarly we have $\lim_{h_1\to 0}\dfrac{f(1)-f(1-h_1)}{h_1}=5$ Thus $f$ is differentiable at $x=1$ Is it right?Please check and suggest required edits","Let $f$ be a real valued continuous function on the interval $[0,2]$ which is differentiable at all points except $1$.Also $\lim f^{'}(x)=5$ .Show $f$ is differentiable at $x=1$. My effort : Consider $h\neq 0$ very small.Then $f$ is differentiable at $1+h$. By definition of derivative $f^{'}(1+h)=\lim _{h_1\to 0}\dfrac{f(1+h+h_1)-f(1+h)}{h_1}\rightarrow (1)$ Also $\lim _{x\to 1}f^{'}(x)=5\implies \lim_{h\to 0}f^{'}(1+h)=5\rightarrow (2)$ Now putting $(2)$ in $(1)$ we have $\lim_{h\to 0}\lim _{h_1\to 0}\dfrac{f(1+h+h_1)-f(1+h)}{h_1}=5$ $\hspace{45mm}\implies $$\lim_{h_1\to 0}\lim _{h\to 0}\dfrac{f(1+h+h_1)-f(1+h)}{h_1}=5$ As $f$ is continuous ;$\lim_{h_1\to 0}\dfrac{f(1+h_1)-f(1)}{h_1}=5$ Similarly we have $\lim_{h_1\to 0}\dfrac{f(1)-f(1-h_1)}{h_1}=5$ Thus $f$ is differentiable at $x=1$ Is it right?Please check and suggest required edits",,"['real-analysis', 'limits', 'derivatives']"
56,Can the endpoints of the interval considered satisfy the mean value theorem?,Can the endpoints of the interval considered satisfy the mean value theorem?,,"For example, if you have a graph $y=x$ and you want to find the values of $c$ that satisfy the mean value theorem for $x\in[1, 3]$, do the points $c=1$ and $c=3$ count as valid? I only ask because for a homework problem $y=-x^3+4x^2-3; [0, 4]$ find values of $c$ that satisfy the MVT, the answer was $0$ and $8/3$ but $0$ was not considered a valid answer because it was on the interval endpoint (to the best of my knowledge).","For example, if you have a graph $y=x$ and you want to find the values of $c$ that satisfy the mean value theorem for $x\in[1, 3]$, do the points $c=1$ and $c=3$ count as valid? I only ask because for a homework problem $y=-x^3+4x^2-3; [0, 4]$ find values of $c$ that satisfy the MVT, the answer was $0$ and $8/3$ but $0$ was not considered a valid answer because it was on the interval endpoint (to the best of my knowledge).",,['derivatives']
57,Fifth differentiation of a function,Fifth differentiation of a function,,Let a function y= $x/(x^2-1)$ And we have to find $y^{\left(\mathtt{V}\right)}(0) $ I wrote $y= {1\over 2}\left[{1\over (x-1)} + {1\over (x+1)}\right]$ But  I am now stuck please help me to proceed .,Let a function y= $x/(x^2-1)$ And we have to find $y^{\left(\mathtt{V}\right)}(0) $ I wrote $y= {1\over 2}\left[{1\over (x-1)} + {1\over (x+1)}\right]$ But  I am now stuck please help me to proceed .,,"['calculus', 'derivatives']"
58,Simplifying derivative result,Simplifying derivative result,,"I am doing the derivative of $$f(x) = \frac{x^2 -4x +3}{x^2-1}$$ So my result is the following $$f'(x) = \frac{4x^2 -8x +4}{(x^2-1)^2}$$ I am sure the answer is correct, but in my solutions book and In Wolfram Alpha they simplify until $$f'(x) = \frac{4}{(x+1)^2}$$ And I don't know why, which steps are they doing?","I am doing the derivative of $$f(x) = \frac{x^2 -4x +3}{x^2-1}$$ So my result is the following $$f'(x) = \frac{4x^2 -8x +4}{(x^2-1)^2}$$ I am sure the answer is correct, but in my solutions book and In Wolfram Alpha they simplify until $$f'(x) = \frac{4}{(x+1)^2}$$ And I don't know why, which steps are they doing?",,['derivatives']
59,Relationship between two-equation constrained optimization and one-equation version,Relationship between two-equation constrained optimization and one-equation version,,"I am learning about the Lagrange multiplier. Here's what I understand so far. Suppose a point $P$ is a minimizer of $f(x)$ subject to $g(x)=0$. Then any movement along that level-curve of $g$ must leave $f$ unaffected, because otherwise (assuming $f$ smooth or something) there would be some point higher than that at $P$. So the level-curve of $g$ must be perpendicular to the gradient of $f$, denoted $\nabla f$. I think the gradient of $g$ is perpendicular to the level curve of $g$ for a similar reason. So the gradients of both $g$ and $f$ are perpendicular to the level curve of $g$. Therefore we can express this situation as $$\nabla f(p) = -\lambda \nabla g(P)$$ with a conventional minus sign, and also keep in mind that $$g(P) = 0.$$ These constitute our two equations for minimizing $f$ subject to $g(x)=0$. I think this should be enough information to just solve it from here. But I often see a single-equation formulation written something like  $$L(x,\lambda) = f(x) - \lambda g(x).$$ Questions (or clusters of questions): What is the relationship between these two formulations? It seems like the first is just some derivative of the second? How would we arrive at the single-equation formulation? What is the point of the second if the first (i.e. the pair of equations) suffice to solve it? In an applied problem would we be trying to get from the first to the second? It seems like the first is a manageable problem, and I don't really understand where the second comes from.","I am learning about the Lagrange multiplier. Here's what I understand so far. Suppose a point $P$ is a minimizer of $f(x)$ subject to $g(x)=0$. Then any movement along that level-curve of $g$ must leave $f$ unaffected, because otherwise (assuming $f$ smooth or something) there would be some point higher than that at $P$. So the level-curve of $g$ must be perpendicular to the gradient of $f$, denoted $\nabla f$. I think the gradient of $g$ is perpendicular to the level curve of $g$ for a similar reason. So the gradients of both $g$ and $f$ are perpendicular to the level curve of $g$. Therefore we can express this situation as $$\nabla f(p) = -\lambda \nabla g(P)$$ with a conventional minus sign, and also keep in mind that $$g(P) = 0.$$ These constitute our two equations for minimizing $f$ subject to $g(x)=0$. I think this should be enough information to just solve it from here. But I often see a single-equation formulation written something like  $$L(x,\lambda) = f(x) - \lambda g(x).$$ Questions (or clusters of questions): What is the relationship between these two formulations? It seems like the first is just some derivative of the second? How would we arrive at the single-equation formulation? What is the point of the second if the first (i.e. the pair of equations) suffice to solve it? In an applied problem would we be trying to get from the first to the second? It seems like the first is a manageable problem, and I don't really understand where the second comes from.",,"['derivatives', 'optimization', 'partial-derivative', 'lagrange-multiplier', 'constraints']"
60,"Find the equation of the tangent to the parabola $y=x^2$, if the x-intercept of the tangent is 2","Find the equation of the tangent to the parabola , if the x-intercept of the tangent is 2",y=x^2,"I'm trying to solve this problem: Find the equation of the tangent to the parabola $y=x^2$. If the x-intercept of the tangent is 2. All what I can think of is finding the slope which is $dy/dx = 2x$ so the tangent line equation would be $y-y_0 = 2x(x-x_0)$ I don't know exactly where to go from here, should I plug in intercept points? $y-0 = 2x(x-2)$","I'm trying to solve this problem: Find the equation of the tangent to the parabola $y=x^2$. If the x-intercept of the tangent is 2. All what I can think of is finding the slope which is $dy/dx = 2x$ so the tangent line equation would be $y-y_0 = 2x(x-x_0)$ I don't know exactly where to go from here, should I plug in intercept points? $y-0 = 2x(x-2)$",,"['calculus', 'derivatives', 'tangent-line', 'slope']"
61,What's wrong with my differentiation (help finding a derivative)?,What's wrong with my differentiation (help finding a derivative)?,,"So the equation looks a bit complicated, but the derivation itself should be straightforward. But I'm evidently getting mixed up somewhere, because my answer is wrong. $$ \frac{\partial ({-k_{b}T \ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} $$ (where V is kept constant, hence the partial derivative) So according to the product rule: $$ {-k_b T} \frac{\partial ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} + {-k_b} ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}}))) $$ Then the chain rule: $$\frac{\partial ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T}  = \frac{\partial {(2\cosh(\frac{\epsilon}{k_{b}T}}))}{\partial T} \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))} = \frac{\partial {(\frac{\epsilon}{k_{b}T}})}{\partial T} 2\sinh(\frac{\epsilon}{k_b T}) \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))} = \frac {\epsilon} {k_b} 2\sinh(\frac{\epsilon}{k_b T}) \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))} = \frac {\epsilon} {k_b} \tanh(\frac{\epsilon}{k_b T})$$ So the final answer I'm getting is:  $$ {-k_b T} \frac {\epsilon} {k_b} \tanh(\frac{\epsilon}{k_b T}) + {-k_b} ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}}))) =  -{\epsilon}T \tanh(\frac{\epsilon}{k_b T})-{k_b} {\ln(2\cosh(\frac{\epsilon}{k_{b}T}})) $$ But apparently this is incorrect, and the correct answer is:  $$ \frac {\epsilon}{T} \tanh(\frac{\epsilon}{k_b T})-{k_b} {\ln(2\cosh(\frac{\epsilon}{k_{b}T}})) $$ I'm probably making a stupid mistake somewhere, but I can't seem to spot it.","So the equation looks a bit complicated, but the derivation itself should be straightforward. But I'm evidently getting mixed up somewhere, because my answer is wrong. $$ \frac{\partial ({-k_{b}T \ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} $$ (where V is kept constant, hence the partial derivative) So according to the product rule: $$ {-k_b T} \frac{\partial ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} + {-k_b} ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}}))) $$ Then the chain rule: $$\frac{\partial ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T}  = \frac{\partial {(2\cosh(\frac{\epsilon}{k_{b}T}}))}{\partial T} \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))} = \frac{\partial {(\frac{\epsilon}{k_{b}T}})}{\partial T} 2\sinh(\frac{\epsilon}{k_b T}) \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))} = \frac {\epsilon} {k_b} 2\sinh(\frac{\epsilon}{k_b T}) \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))} = \frac {\epsilon} {k_b} \tanh(\frac{\epsilon}{k_b T})$$ So the final answer I'm getting is:  $$ {-k_b T} \frac {\epsilon} {k_b} \tanh(\frac{\epsilon}{k_b T}) + {-k_b} ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}}))) =  -{\epsilon}T \tanh(\frac{\epsilon}{k_b T})-{k_b} {\ln(2\cosh(\frac{\epsilon}{k_{b}T}})) $$ But apparently this is incorrect, and the correct answer is:  $$ \frac {\epsilon}{T} \tanh(\frac{\epsilon}{k_b T})-{k_b} {\ln(2\cosh(\frac{\epsilon}{k_{b}T}})) $$ I'm probably making a stupid mistake somewhere, but I can't seem to spot it.",,"['calculus', 'derivatives']"
62,Where i am going wrong in finding normal to curve?,Where i am going wrong in finding normal to curve?,,"The question is Find the perpendicular distance between the normal to the curve $$x=a\cos t+at\sin t, y=a\sin t-at\cos t$$ and the origin. Equation is given in parameterized form. My attempt finding slope of tangent to the curve at point $\theta$  $$\dfrac{dy}{dx}=\dfrac{dy/dt}{dx/dt}|t=\theta;\implies tant| t=\theta$$ therefore slope at $\theta$ is equal to $\tan\theta$ slope of normal :$$\implies m=-\cot \theta$$ finding equation of normal at point $(acos\theta+a\theta sin\theta, asin\theta-a\theta cos\theta)$ by using $y-y_1 =m(x-x_1)$ $$y-(asin\theta -a\theta cos\theta)=-cot\theta(x-(acos\theta+a\theta cos\theta))$$ on finding distance of this line from origin we get the answer $a$ but the answer in my book is $a/2$ can anyone please tell me, why i am going wrong?","The question is Find the perpendicular distance between the normal to the curve $$x=a\cos t+at\sin t, y=a\sin t-at\cos t$$ and the origin. Equation is given in parameterized form. My attempt finding slope of tangent to the curve at point $\theta$  $$\dfrac{dy}{dx}=\dfrac{dy/dt}{dx/dt}|t=\theta;\implies tant| t=\theta$$ therefore slope at $\theta$ is equal to $\tan\theta$ slope of normal :$$\implies m=-\cot \theta$$ finding equation of normal at point $(acos\theta+a\theta sin\theta, asin\theta-a\theta cos\theta)$ by using $y-y_1 =m(x-x_1)$ $$y-(asin\theta -a\theta cos\theta)=-cot\theta(x-(acos\theta+a\theta cos\theta))$$ on finding distance of this line from origin we get the answer $a$ but the answer in my book is $a/2$ can anyone please tell me, why i am going wrong?",,"['derivatives', 'analytic-geometry', 'applications']"
63,How to prove derivative of logarithm with base $b$?,How to prove derivative of logarithm with base ?,b,I learned how to derive a logarithm with any base. This is the formula: $$\frac{d}{dx}\log_bx=\frac{1}{x\ln b}$$ How can it be proved?,I learned how to derive a logarithm with any base. This is the formula: $$\frac{d}{dx}\log_bx=\frac{1}{x\ln b}$$ How can it be proved?,,"['calculus', 'derivatives', 'logarithms']"
64,"Find real parametar $a,b,c$ such that function $f$ become convex function $f(x) = \begin{cases}ax^2+bx+c,& x<0\\1 ,& x \ge 0\end{cases}$",Find real parametar  such that function  become convex function,"a,b,c f f(x) = \begin{cases}ax^2+bx+c,& x<0\\1 ,& x \ge 0\end{cases}","Find real parametar $a,b,c$ such that function $f$ become convex function $$f(x) = \begin{cases}ax^2+bx+c,& x<0\\1 ,& x \ge 0\end{cases}$$ My work: If $f(x)$ is convex function that means that $f'(x)$ must be incersing function. $$f'(x) = \begin{cases}2ax+b,& x<0\\0 ,& x \ge 0\end{cases}$$ We  know that $a\ge0$  and $b\le0$. And if $f$ is convex function that means that $f''(0)\ge0$ which only say that $a\ge0$. This maybe work if function $f$ is twice differentiable. How to find for case when f isn't twice differentiable. Use $\frac{f(x+y)}{2}\le \frac{f(x)}{2}+\frac{f(y)}{2}$?","Find real parametar $a,b,c$ such that function $f$ become convex function $$f(x) = \begin{cases}ax^2+bx+c,& x<0\\1 ,& x \ge 0\end{cases}$$ My work: If $f(x)$ is convex function that means that $f'(x)$ must be incersing function. $$f'(x) = \begin{cases}2ax+b,& x<0\\0 ,& x \ge 0\end{cases}$$ We  know that $a\ge0$  and $b\le0$. And if $f$ is convex function that means that $f''(0)\ge0$ which only say that $a\ge0$. This maybe work if function $f$ is twice differentiable. How to find for case when f isn't twice differentiable. Use $\frac{f(x+y)}{2}\le \frac{f(x)}{2}+\frac{f(y)}{2}$?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'contest-math']"
65,Matrix Calculus Question: a scalar-by-matrix derivative,Matrix Calculus Question: a scalar-by-matrix derivative,,"I have the following scalar-by-matrix derivative that I have completely no clue how to solve: $f(\mathbf{R},\mathbf{S}) = \mathbf{y}^{\top}\bigg(\mathbf{1}\otimes\mathbf{R}+\mathbf{\Phi}\otimes\mathbf{S}\bigg)^{-1}\mathbf{y}$ where $f(\mathbf{R},\mathbf{S})$ is a scalar function, $\mathbf{y}$ is column vector. Is there a closed form solution to $\dfrac{df(\mathbf{R},\mathbf{S})} {d\mathbf{R}}$ and $\dfrac{df(\mathbf{R},\mathbf{S})} {d\mathbf{S}}$? Any guidance will be helpful. Many thanks!","I have the following scalar-by-matrix derivative that I have completely no clue how to solve: $f(\mathbf{R},\mathbf{S}) = \mathbf{y}^{\top}\bigg(\mathbf{1}\otimes\mathbf{R}+\mathbf{\Phi}\otimes\mathbf{S}\bigg)^{-1}\mathbf{y}$ where $f(\mathbf{R},\mathbf{S})$ is a scalar function, $\mathbf{y}$ is column vector. Is there a closed form solution to $\dfrac{df(\mathbf{R},\mathbf{S})} {d\mathbf{R}}$ and $\dfrac{df(\mathbf{R},\mathbf{S})} {d\mathbf{S}}$? Any guidance will be helpful. Many thanks!",,"['matrices', 'derivatives', 'matrix-calculus']"
66,Why can't I solve for this second derivative?,Why can't I solve for this second derivative?,,Here's the equation I have to find the second derivative point for. $$f(x)=\frac{x+2}{x^{\frac{1}{2}}}$$ $$f'(x) = \frac{x-2}{2x^{\frac{3}{2}}}$$ From here I then calculate the second derivative and set it equal to 0. But it doesn't work.. Take a look: $$f''(x)=\frac{-x^\frac{3}{2} + 6x^{\frac{1}{2}}}{4x^{3}} = 0$$ FROM first DERIVATIVE TO second: $$\frac{2x^{\frac{3}{2}}[x-2]'-((x-2)[2x^{\frac{3}{2}}]'}{4x^{3}}$$ $$\frac{2x^{\frac{3}{2}} -3x^{}\frac{3}{2}+6x^{\frac{1}{2}}}{4x^{3}}=0$$ $$-x^{\frac{3}{2}} + 6x^{\frac{1}{2}} = 0$$ $$(-x^{\frac{3}{2}})^{2} + (6x^{\frac{1}{2}})^{2}$$ $$x^{3} + 36x = 0$$ $$x (x^{2} + 36) = 0$$ $$x = 0  \text{   or    } x^{2} = - 36 \text{      no solution..}$$ This doesn't seem right.. Yet I have no idea why. It's easier to write the function as a product but I want to solve it using the quotient rule.. What's going on?,Here's the equation I have to find the second derivative point for. $$f(x)=\frac{x+2}{x^{\frac{1}{2}}}$$ $$f'(x) = \frac{x-2}{2x^{\frac{3}{2}}}$$ From here I then calculate the second derivative and set it equal to 0. But it doesn't work.. Take a look: $$f''(x)=\frac{-x^\frac{3}{2} + 6x^{\frac{1}{2}}}{4x^{3}} = 0$$ FROM first DERIVATIVE TO second: $$\frac{2x^{\frac{3}{2}}[x-2]'-((x-2)[2x^{\frac{3}{2}}]'}{4x^{3}}$$ $$\frac{2x^{\frac{3}{2}} -3x^{}\frac{3}{2}+6x^{\frac{1}{2}}}{4x^{3}}=0$$ $$-x^{\frac{3}{2}} + 6x^{\frac{1}{2}} = 0$$ $$(-x^{\frac{3}{2}})^{2} + (6x^{\frac{1}{2}})^{2}$$ $$x^{3} + 36x = 0$$ $$x (x^{2} + 36) = 0$$ $$x = 0  \text{   or    } x^{2} = - 36 \text{      no solution..}$$ This doesn't seem right.. Yet I have no idea why. It's easier to write the function as a product but I want to solve it using the quotient rule.. What's going on?,,"['calculus', 'derivatives']"
67,How to approximate the derivative of a stock price over time?,How to approximate the derivative of a stock price over time?,,"My high school marketing class is about to do a unit on stocks. We're going to make ""pretend"" investments over the next month or so, and have a competition to see who has the highest gains. These are relatively short term investments, so I'm thinking that looking at trends will be key to success. It occurred to me, that trying to approximate the derivative of a stock price over time could be useful to some extent. I'm wondering how I could do that though, given stock price is kind of jerky and irregular, and is also technically discontinuous? Sorry if this is a kind of stupid question. I have some ideas, but I'm not sure if they're the best.","My high school marketing class is about to do a unit on stocks. We're going to make ""pretend"" investments over the next month or so, and have a competition to see who has the highest gains. These are relatively short term investments, so I'm thinking that looking at trends will be key to success. It occurred to me, that trying to approximate the derivative of a stock price over time could be useful to some extent. I'm wondering how I could do that though, given stock price is kind of jerky and irregular, and is also technically discontinuous? Sorry if this is a kind of stupid question. I have some ideas, but I'm not sure if they're the best.",,['derivatives']
68,Is $f(x)= \frac{1}{x} - \frac{1}{e^x-1}$ monotonic?,Is  monotonic?,f(x)= \frac{1}{x} - \frac{1}{e^x-1},"We have $\displaystyle f(x)= \frac{1}{x} - \frac{1}{e^x-1}$, additionally $f(0)=\frac{1}{2}$. Determine whether $f(x)$ is monotonic. I tried to do this by checking if $f'(x)<0$, however it does not look very helpful. I don't know if it's tricky, or I am just blind to something obvious here. Thanks for any hints in advance!","We have $\displaystyle f(x)= \frac{1}{x} - \frac{1}{e^x-1}$, additionally $f(0)=\frac{1}{2}$. Determine whether $f(x)$ is monotonic. I tried to do this by checking if $f'(x)<0$, however it does not look very helpful. I don't know if it's tricky, or I am just blind to something obvious here. Thanks for any hints in advance!",,"['real-analysis', 'analysis', 'derivatives', 'monotone-functions']"
69,Concave up and concave down?,Concave up and concave down?,,"I am unable to find out how to find concave up and concave down just by looking at a graph of function $f(x)$. I was told that if the tangent line of the slope can be visualized under the graph, it's concave up, and if it's visualized above, it's concave down, but it doesn't appear to be so. I put concave up as $(0,3)$ and concave down as $(3,6)$. How would you identify concave up and down?","I am unable to find out how to find concave up and concave down just by looking at a graph of function $f(x)$. I was told that if the tangent line of the slope can be visualized under the graph, it's concave up, and if it's visualized above, it's concave down, but it doesn't appear to be so. I put concave up as $(0,3)$ and concave down as $(3,6)$. How would you identify concave up and down?",,"['calculus', 'derivatives']"
70,Is there a function that $f^{-1}(x)=f'(x) $?,Is there a function that ?,f^{-1}(x)=f'(x) ,$$f^{-1}(x)=f'(x)   \forall x \in R$$ $R$ is a set of real  numbers. I try to find it  or disprove that fact but I didn't make it.,$$f^{-1}(x)=f'(x)   \forall x \in R$$ $R$ is a set of real  numbers. I try to find it  or disprove that fact but I didn't make it.,,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
71,What is wrong with my logic and derivatives?,What is wrong with my logic and derivatives?,,"Since the derivative of a function is analogous to a description on how fast the function grows, I thought that $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=0$ for the following reason. Assume $f(x)$ is monotone increasing on an interval $(a,\infty]$.  If $f'(x)$ is large, that means that $f(x)$ is getting larger and larger.  From this, I've assumed that $f'(x)$ must eventually be outdone by $f(x)$ simply because $f'(x)$ cannot get large without making $f(x)$ even larger. But that's obviously not the case because if $f(x)=\Gamma(x)$, then $\lim_{x\to\infty}\frac{f'(x)}{f(x)}\ne0$. I'm just wondering what is wrong with my logic, that's all.","Since the derivative of a function is analogous to a description on how fast the function grows, I thought that $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=0$ for the following reason. Assume $f(x)$ is monotone increasing on an interval $(a,\infty]$.  If $f'(x)$ is large, that means that $f(x)$ is getting larger and larger.  From this, I've assumed that $f'(x)$ must eventually be outdone by $f(x)$ simply because $f'(x)$ cannot get large without making $f(x)$ even larger. But that's obviously not the case because if $f(x)=\Gamma(x)$, then $\lim_{x\to\infty}\frac{f'(x)}{f(x)}\ne0$. I'm just wondering what is wrong with my logic, that's all.",,"['derivatives', 'soft-question']"
72,Uniqueness of the derivative operator,Uniqueness of the derivative operator,,"Let $\mathcal{D}(\mathbb{R})$ be the set of all functions $f:\mathbb{R}\to\mathbb{R}$ which are differentiable at all points, and let $\mathcal{F}(\mathbb{R})$ be the set of all functions $f:\mathbb{R}\to\mathbb{R}$. Let's denote by $D$ the derivative operator, i.e. $$D:\mathcal{D}(\mathbb{R})\to\mathcal{F}(\mathbb{R})$$ is such that $$D(f)=f^{\prime}$$ It's clear that $\mathcal{F}(\mathbb{R})$ is a real vector space and that $\mathcal{D}(\mathbb{R})$ is a subspace of it. We also know that $D$ is linear and that it obeys the product rule $$D(f\cdot g)=f\cdot D(g)+g\cdot D(f)$$ The question is: is $D$ the only such linear transformation, i.e., the only linear transformation that obeys product rule? Of course, the $0$ operator and any real multiple of $D$, $\alpha\cdot D$ are trivial examples. But I'm interested in non-trivial examples.","Let $\mathcal{D}(\mathbb{R})$ be the set of all functions $f:\mathbb{R}\to\mathbb{R}$ which are differentiable at all points, and let $\mathcal{F}(\mathbb{R})$ be the set of all functions $f:\mathbb{R}\to\mathbb{R}$. Let's denote by $D$ the derivative operator, i.e. $$D:\mathcal{D}(\mathbb{R})\to\mathcal{F}(\mathbb{R})$$ is such that $$D(f)=f^{\prime}$$ It's clear that $\mathcal{F}(\mathbb{R})$ is a real vector space and that $\mathcal{D}(\mathbb{R})$ is a subspace of it. We also know that $D$ is linear and that it obeys the product rule $$D(f\cdot g)=f\cdot D(g)+g\cdot D(f)$$ The question is: is $D$ the only such linear transformation, i.e., the only linear transformation that obeys product rule? Of course, the $0$ operator and any real multiple of $D$, $\alpha\cdot D$ are trivial examples. But I'm interested in non-trivial examples.",,"['calculus', 'real-analysis', 'linear-algebra', 'derivatives']"
73,Interchangeability of derivative and max operator,Interchangeability of derivative and max operator,,"Consider the function \begin{align} g(y) = \max_{x\in X} f(x,y) \end{align} where $y\in\mathbb{R}$, $x\in\mathbb{R}^n$. Now consider taking the $n^{th}$ derivative of $g(y)$ with respect to $y$ (assuming that it exists). Can I always interchange the derivative and the max? That is, \begin{align} g^{(n)}(y) = \frac{d^{n}}{dy^{n}}\max_{x\in X} f(x,y)\stackrel{?}{=}\max_{x\in X} \frac{d^{n}}{dy^{n}}f(x,y) \end{align} If not, are there conditions under which this is allowed?","Consider the function \begin{align} g(y) = \max_{x\in X} f(x,y) \end{align} where $y\in\mathbb{R}$, $x\in\mathbb{R}^n$. Now consider taking the $n^{th}$ derivative of $g(y)$ with respect to $y$ (assuming that it exists). Can I always interchange the derivative and the max? That is, \begin{align} g^{(n)}(y) = \frac{d^{n}}{dy^{n}}\max_{x\in X} f(x,y)\stackrel{?}{=}\max_{x\in X} \frac{d^{n}}{dy^{n}}f(x,y) \end{align} If not, are there conditions under which this is allowed?",,"['derivatives', 'optimization']"
74,$k-1$st derivative of a degree $k$ polynomial,st derivative of a degree  polynomial,k-1 k,"I know this is going to come across as a very strange question, but it's important that I know the answer. Say I have a degree $k$ polynomial (for my case, I need it to be a complex-valued polynomial with real coefficients, but I think the idea would work even if it were real-valued), $f(z) = a_{0}+a_{1}z+a_{2}z^{2}+\cdots + a_{k-2}z^{k-2}+a_{k-1}z^{k-1}+a_{k}z^{k}$, then what is the formula for the $k-1$st derivative? Obviously, the first few derivatives are: $f^{\prime}=a_{1}+2a_{2}z+\cdots (k-2)a_{k-2}z^{k-3}+(k-1)a_{k-1}z^{k-2}+ka_{k}z^{k-1}$ $f^{\prime\prime}=2a_{2} + 3\cdot 2 a_{3}z + 4 \cdot 3 a_{4}z^{2} + \cdots + (k-3)(k-2)a_{k-2}z^{k-4}+(k-2)(k-1)z^{k-3}+(k-1)ka_{k}z^{k-2}$ $f^{\prime\prime\prime}=3\cdot 2 a_{3} + 4(3)(2) a_{4}z + \cdots + (k-4)(k-3)(k-2)a_{k-2}z^{k-5}+(k-3)(k-2)(k-1)z^{k-4}+(k-2)(k-1)ka_{k}z^{k-3}$ I'm thinking the $k-1$st derivative would definitely contain only two terms, a constant term and a linear term, and the coefficient of the linear term would be $(k-(k-1))(k-(k-2))\cdots (k-2)(k-1)k$, but I can't figure out the pattern for what the constant term should be. Could somebody please help me?!","I know this is going to come across as a very strange question, but it's important that I know the answer. Say I have a degree $k$ polynomial (for my case, I need it to be a complex-valued polynomial with real coefficients, but I think the idea would work even if it were real-valued), $f(z) = a_{0}+a_{1}z+a_{2}z^{2}+\cdots + a_{k-2}z^{k-2}+a_{k-1}z^{k-1}+a_{k}z^{k}$, then what is the formula for the $k-1$st derivative? Obviously, the first few derivatives are: $f^{\prime}=a_{1}+2a_{2}z+\cdots (k-2)a_{k-2}z^{k-3}+(k-1)a_{k-1}z^{k-2}+ka_{k}z^{k-1}$ $f^{\prime\prime}=2a_{2} + 3\cdot 2 a_{3}z + 4 \cdot 3 a_{4}z^{2} + \cdots + (k-3)(k-2)a_{k-2}z^{k-4}+(k-2)(k-1)z^{k-3}+(k-1)ka_{k}z^{k-2}$ $f^{\prime\prime\prime}=3\cdot 2 a_{3} + 4(3)(2) a_{4}z + \cdots + (k-4)(k-3)(k-2)a_{k-2}z^{k-5}+(k-3)(k-2)(k-1)z^{k-4}+(k-2)(k-1)ka_{k}z^{k-3}$ I'm thinking the $k-1$st derivative would definitely contain only two terms, a constant term and a linear term, and the coefficient of the linear term would be $(k-(k-1))(k-(k-2))\cdots (k-2)(k-1)k$, but I can't figure out the pattern for what the constant term should be. Could somebody please help me?!",,['derivatives']
75,"How to compute $[\dot c, X]$ on a manifold?",How to compute  on a manifold?,"[\dot c, X]","Consider a smooth curve $c : [0,1] \to M$ and $X \in \mathcal X (M)$. How can one obtain an explicit formula for $[\dot c, X]$? I know the theoretical approach: for every $t \in [0,1]$ there exist a neighbourhood $U_t$ of $c(t)$ and a local tangent field $C_t \in \mathcal X (U_t)$ that extends $\dot c$, so one computes $[C_t, X]$ and then evaluates it at $c(t)$. Theory is nice, but putting it into practice does not look obvious; how should I do it? Concretely, the problem is that in local coordinates and choosing $X = \partial _i$ I would obtain $- \sum \limits _j (\partial _i C_t ^j) \partial _j$. How would I evaluate $\partial _i C_t ^j$ in $c(t)$? In other words, how do I compute $\partial _i \dot c ^j$?","Consider a smooth curve $c : [0,1] \to M$ and $X \in \mathcal X (M)$. How can one obtain an explicit formula for $[\dot c, X]$? I know the theoretical approach: for every $t \in [0,1]$ there exist a neighbourhood $U_t$ of $c(t)$ and a local tangent field $C_t \in \mathcal X (U_t)$ that extends $\dot c$, so one computes $[C_t, X]$ and then evaluates it at $c(t)$. Theory is nice, but putting it into practice does not look obvious; how should I do it? Concretely, the problem is that in local coordinates and choosing $X = \partial _i$ I would obtain $- \sum \limits _j (\partial _i C_t ^j) \partial _j$. How would I evaluate $\partial _i C_t ^j$ in $c(t)$? In other words, how do I compute $\partial _i \dot c ^j$?",,"['differential-geometry', 'derivatives', 'smooth-manifolds', 'curves', 'lie-derivative']"
76,Derivative of matrix logarithm with respect to matrix,Derivative of matrix logarithm with respect to matrix,,I saw in this post that $\frac{d}{dt}\text{logm}(Z(t)) = \frac{dZ(t)}{dt}(Z(t))^{-1}$ Is this true to say: $\frac{d}{{dU}}{\mathop{\rm logm}\nolimits} (A) = {A^{ - 1}}\frac{d}{{dU}}A$ where U is an m by n matrix and A is an m by m matrix which is a function of U ? EDIT: A is not Symmetric and positive definite.,I saw in this post that $\frac{d}{dt}\text{logm}(Z(t)) = \frac{dZ(t)}{dt}(Z(t))^{-1}$ Is this true to say: $\frac{d}{{dU}}{\mathop{\rm logm}\nolimits} (A) = {A^{ - 1}}\frac{d}{{dU}}A$ where U is an m by n matrix and A is an m by m matrix which is a function of U ? EDIT: A is not Symmetric and positive definite.,,"['matrices', 'derivatives', 'logarithms', 'matrix-calculus']"
77,How to differentiate both sides with an independent variable if one doesn't have a formula?,How to differentiate both sides with an independent variable if one doesn't have a formula?,,"I have an equation similar to the following: $$\frac{a}{b} = c$$ Now I want to differentiate both sides with respect to an independent variable: $$\frac{\mathrm{d} }{\mathrm{d} x} (\frac{a}{b}) = \frac{\mathrm{d} }{\mathrm{d} x}(c)$$ I have the data that represents $a/b$ and $c$ and how it changes with $x$ but I do not have a formula. Now if I try to differentiate like that I would get $0$. Right? Because the derivative of a constant is equals to zero. Is there a way to calculate the derivatives when you do not have an explicit formula? Note: If this question sounds stupid, I am sorry because I am very new to calculus.","I have an equation similar to the following: $$\frac{a}{b} = c$$ Now I want to differentiate both sides with respect to an independent variable: $$\frac{\mathrm{d} }{\mathrm{d} x} (\frac{a}{b}) = \frac{\mathrm{d} }{\mathrm{d} x}(c)$$ I have the data that represents $a/b$ and $c$ and how it changes with $x$ but I do not have a formula. Now if I try to differentiate like that I would get $0$. Right? Because the derivative of a constant is equals to zero. Is there a way to calculate the derivatives when you do not have an explicit formula? Note: If this question sounds stupid, I am sorry because I am very new to calculus.",,"['calculus', 'derivatives']"
78,I would like to get the derivative of this function: $ f(x)=(x-a)^2(x-b)^2 $ and $f(x) = \frac{1}{e}$,I would like to get the derivative of this function:  and, f(x)=(x-a)^2(x-b)^2  f(x) = \frac{1}{e},"I want to get the derivative of this function:  $$ f(x)=(x-a)^2(x-b)^2 $$ for $x ∈< a, b >$, $$f(x) = \frac{1}{e}$$ for all other $x$. Now I know the result is: $$ f'(x) = 2(x − a)(x − b)(2x − a − b)$$ for $$x ∈ (a, b);$$ and$$ f'_{+}(a) = f'_{−}(a) = f'_{+}(b) = f'_{−}(b) = 0; f'(x) = 0 $$ for   $$x ∈ (−∞, a) ∪ (b, ∞) $$ But I don't know how to get to it. Any help would be greatly appreciated.","I want to get the derivative of this function:  $$ f(x)=(x-a)^2(x-b)^2 $$ for $x ∈< a, b >$, $$f(x) = \frac{1}{e}$$ for all other $x$. Now I know the result is: $$ f'(x) = 2(x − a)(x − b)(2x − a − b)$$ for $$x ∈ (a, b);$$ and$$ f'_{+}(a) = f'_{−}(a) = f'_{+}(b) = f'_{−}(b) = 0; f'(x) = 0 $$ for   $$x ∈ (−∞, a) ∪ (b, ∞) $$ But I don't know how to get to it. Any help would be greatly appreciated.",,"['real-analysis', 'functional-analysis', 'derivatives']"
79,Does Darboux theorem imply that $f'$ cannot have jump discontinuity?,Does Darboux theorem imply that  cannot have jump discontinuity?,f',"Does the Darboux theorem for derivatives imply that a derivative on a interval $I$ cannot have jump discontinuity? Darboux theorem states that the derivative function follow the intermediate value theorem on a interval $I$. My doubts are about a function like this $f(x)=\begin{cases} x-1, 0\leq x\leq 2\\ x+1, -2\leq x < 0\end{cases}$ It has a jump discontinuity nevertheless it seems to follow the intermediate value theorem Am I missing something? Thanks for your help","Does the Darboux theorem for derivatives imply that a derivative on a interval $I$ cannot have jump discontinuity? Darboux theorem states that the derivative function follow the intermediate value theorem on a interval $I$. My doubts are about a function like this $f(x)=\begin{cases} x-1, 0\leq x\leq 2\\ x+1, -2\leq x < 0\end{cases}$ It has a jump discontinuity nevertheless it seems to follow the intermediate value theorem Am I missing something? Thanks for your help",,"['calculus', 'derivatives', 'continuity']"
80,"On extending a function $f:\mathbb N \to [-1,1]$ to $\bar f:\mathbb R \to [-1,1]$ such that $\bar f$ is differentiable at every point of $\mathbb N$?",On extending a function  to  such that  is differentiable at every point of ?,"f:\mathbb N \to [-1,1] \bar f:\mathbb R \to [-1,1] \bar f \mathbb N","For which type of functions $f:\mathbb N \to [-1,1]$ , can we extend it to a continuous function $\bar f:\mathbb R \to [-1,1]$ such that $\bar f$ is differentiable at every point of $\mathbb N$ ? And for which type of functions $f:\mathbb N \to [-1,1]$ , can we extend it to a function $\bar f:\mathbb R \to [-1,1]$ such that $\bar f$ is differentiable at every point of $\mathbb N$ ( this time , not requiring $\bar f$ to be continuous everywhere )  ? By extension of $f$ , I obviously do mean $f(\mathbb N)=\bar f(\mathbb N)$","For which type of functions $f:\mathbb N \to [-1,1]$ , can we extend it to a continuous function $\bar f:\mathbb R \to [-1,1]$ such that $\bar f$ is differentiable at every point of $\mathbb N$ ? And for which type of functions $f:\mathbb N \to [-1,1]$ , can we extend it to a function $\bar f:\mathbb R \to [-1,1]$ such that $\bar f$ is differentiable at every point of $\mathbb N$ ( this time , not requiring $\bar f$ to be continuous everywhere )  ? By extension of $f$ , I obviously do mean $f(\mathbb N)=\bar f(\mathbb N)$",,"['real-analysis', 'general-topology']"
81,How to 'get rid of' limit so I can finish proof?,How to 'get rid of' limit so I can finish proof?,,"Suppose $\sup_{x \in \mathbb{R}} f'(x) \le M$. I am trying to show that this is true if and only if $$\frac{f(x) - f(y)}{x - y} \le M$$ for all $x, y \in \mathbb{R}$. Proof $\text{sup}_{x \in \mathbb{R}} f'(x) \le M$ $f'(x) \le M$ for all $x \in \mathbb{R}$ $\lim_{y \to x} \frac{f(y) - f(x)}{y - x} \le M$ $\lim_{y \to x} \frac{f(x) - f(y)}{x - y} \le M$ I can see geometrically why this property holds, but how do I get rid of the limit here? Or am I approaching it wrong in general?","Suppose $\sup_{x \in \mathbb{R}} f'(x) \le M$. I am trying to show that this is true if and only if $$\frac{f(x) - f(y)}{x - y} \le M$$ for all $x, y \in \mathbb{R}$. Proof $\text{sup}_{x \in \mathbb{R}} f'(x) \le M$ $f'(x) \le M$ for all $x \in \mathbb{R}$ $\lim_{y \to x} \frac{f(y) - f(x)}{y - x} \le M$ $\lim_{y \to x} \frac{f(x) - f(y)}{x - y} \le M$ I can see geometrically why this property holds, but how do I get rid of the limit here? Or am I approaching it wrong in general?",,"['calculus', 'limits', 'derivatives', 'metric-spaces']"
82,"If all derivative are uniformly bounded, there is a subsequence that converges uniformly to an infinitely differentiable function","If all derivative are uniformly bounded, there is a subsequence that converges uniformly to an infinitely differentiable function",,"Let $\{f_n\}, n \in\mathbb N$, be a sequence of infinitely differentiable functions (smooth function) on $[a,b]$ such that for all integer $k \ge 0$, there exist a real $M_k$ such that $|f_n^{(k)} (x)| \le M_k$ for all $x \in [a,b]$. Show that there exist a subsequence that converges uniformly with all it's derivatives to a infinitely differentiable function. Since $\{f_n\}$ is bounded I think I could use the Arzela-Ascoli Theorem to show that $\{f_n\}$ has a subsequence that converges uniformly but I'm not sure how to prove equicontinuity.","Let $\{f_n\}, n \in\mathbb N$, be a sequence of infinitely differentiable functions (smooth function) on $[a,b]$ such that for all integer $k \ge 0$, there exist a real $M_k$ such that $|f_n^{(k)} (x)| \le M_k$ for all $x \in [a,b]$. Show that there exist a subsequence that converges uniformly with all it's derivatives to a infinitely differentiable function. Since $\{f_n\}$ is bounded I think I could use the Arzela-Ascoli Theorem to show that $\{f_n\}$ has a subsequence that converges uniformly but I'm not sure how to prove equicontinuity.",,"['real-analysis', 'derivatives', 'convergence-divergence']"
83,What is the derivative of matrix vector product $(A^Tx)$ with respect to A?,What is the derivative of matrix vector product  with respect to A?,(A^Tx),"What is the derivative of a vector with respect to a matrix?  Specifically, $\frac{d(A^Tx)}{dA} = ? $, where $ A \in R^{n \times m}$ and $x \in R^n$.","What is the derivative of a vector with respect to a matrix?  Specifically, $\frac{d(A^Tx)}{dA} = ? $, where $ A \in R^{n \times m}$ and $x \in R^n$.",,"['matrices', 'derivatives', 'matrix-calculus']"
84,Proving the chain rule of a given function,Proving the chain rule of a given function,,"Suppose that $f'(2)=3$, $f'(5)=4$, and let $h(x)$ be the composite function $h(x) = f(x^2+1)$. Find $h'(2)$ I get how to prove the $f'g(x)*g'(x)$ part, which leads to $4*g'(2)$ but how do I prove $g'(2)$ with the information given? Or was I given $f'(2)=3$ to throw me off?","Suppose that $f'(2)=3$, $f'(5)=4$, and let $h(x)$ be the composite function $h(x) = f(x^2+1)$. Find $h'(2)$ I get how to prove the $f'g(x)*g'(x)$ part, which leads to $4*g'(2)$ but how do I prove $g'(2)$ with the information given? Or was I given $f'(2)=3$ to throw me off?",,"['real-analysis', 'derivatives', 'function-and-relation-composition']"
85,Derivative of a nonsingular matrix,Derivative of a nonsingular matrix,,Show that : $$\frac{d}{dt} A^{-1}(t) = -A^{-1}(t) (\frac{d}{dt} A(t) ) A^{-1}(t) $$ A(t) is a matrix.,Show that : $$\frac{d}{dt} A^{-1}(t) = -A^{-1}(t) (\frac{d}{dt} A(t) ) A^{-1}(t) $$ A(t) is a matrix.,,"['linear-algebra', 'matrices', 'derivatives', 'normed-spaces', 'matrix-decomposition']"
86,show that a functions derivative is bounded below by $e^x$,show that a functions derivative is bounded below by,e^x,"I have a differentiable function: $f:[0,\infty) \rightarrow \mathbb{R}$ with $f(0)=1$ and $f'(x) \geq f(x)$ for all $x$. I need to show $f'(x)\geq e^x$ for all $x$. At first this appeared to be obvious but i am having trouble showing it rigorously. It seemed obvious because of $(e^x)'=e^x$ and we are assuming that $f'(x)\geq f(x)$. a thought i had was to define another function $h(x)=e^{-x} f(x)$ and then differentiate and try to obtain the inequality but its getting me nowhere. i.e. $h'(x) = e^{-x}f'(x) - e^{-x}f(x)$ then using the assumptions to try and make the inequality true, by saying that $e^xh'(x) = f'(x)-f(x)$ but I'm not coming out with the required result. is there ant theorems or results i can apply or am i being silly and missing something? EDIT: could this just be deduced by proving the derivative of the exponential function and then saying that since $f'(x)\geq f(x)$ then $f'(x)\geq e^x$ because the derivative of the $e^x$ is $e^x$? Thanks in advance","I have a differentiable function: $f:[0,\infty) \rightarrow \mathbb{R}$ with $f(0)=1$ and $f'(x) \geq f(x)$ for all $x$. I need to show $f'(x)\geq e^x$ for all $x$. At first this appeared to be obvious but i am having trouble showing it rigorously. It seemed obvious because of $(e^x)'=e^x$ and we are assuming that $f'(x)\geq f(x)$. a thought i had was to define another function $h(x)=e^{-x} f(x)$ and then differentiate and try to obtain the inequality but its getting me nowhere. i.e. $h'(x) = e^{-x}f'(x) - e^{-x}f(x)$ then using the assumptions to try and make the inequality true, by saying that $e^xh'(x) = f'(x)-f(x)$ but I'm not coming out with the required result. is there ant theorems or results i can apply or am i being silly and missing something? EDIT: could this just be deduced by proving the derivative of the exponential function and then saying that since $f'(x)\geq f(x)$ then $f'(x)\geq e^x$ because the derivative of the $e^x$ is $e^x$? Thanks in advance",,"['real-analysis', 'derivatives']"
87,inverse laplace transform of $\frac{s^3}{(s^2+4)^2}$,inverse laplace transform of,\frac{s^3}{(s^2+4)^2},Using partial fractions gives $\frac{s}{s^2+4}$ - $\frac{4s}{(s^2+4)^2}$ Inverse laplace transform of the first member ($\frac{s}{s^2+4}$) is cos(2t).  Can't figure out how to transform $\frac{4s}{(s^2+4)^2}$ . Help needed.,Using partial fractions gives $\frac{s}{s^2+4}$ - $\frac{4s}{(s^2+4)^2}$ Inverse laplace transform of the first member ($\frac{s}{s^2+4}$) is cos(2t).  Can't figure out how to transform $\frac{4s}{(s^2+4)^2}$ . Help needed.,,"['integration', 'trigonometry', 'derivatives', 'fourier-analysis', 'laplace-transform']"
88,surface area of a cube related rates problem,surface area of a cube related rates problem,,"The volume of a cube is expanding at a rate of $4cm^3/s$. What is the rate the surface area is changing when the area is $24cm^2$. What I do is: $$ V=h^3 \\A=6h^2={6V\over{h}} \\h=\sqrt{A\over6}=2 $$ Then... $$ {dA\over{dt}}={6{dV\over{dt}}h\over{h^2}}={6\bullet4\bullet2\over{4}}=12cm^2/s $$ But it says the answer is not 12. Not sure what I did wrong, havent done related rates in a long time. Any help appreciated thanks.","The volume of a cube is expanding at a rate of $4cm^3/s$. What is the rate the surface area is changing when the area is $24cm^2$. What I do is: $$ V=h^3 \\A=6h^2={6V\over{h}} \\h=\sqrt{A\over6}=2 $$ Then... $$ {dA\over{dt}}={6{dV\over{dt}}h\over{h^2}}={6\bullet4\bullet2\over{4}}=12cm^2/s $$ But it says the answer is not 12. Not sure what I did wrong, havent done related rates in a long time. Any help appreciated thanks.",,['derivatives']
89,Optimization of location to find where 2 particles are closest.,Optimization of location to find where 2 particles are closest.,,"A particle moves west at a speed of $25km/hr$ from the origin when $t=0$. Another particle moves north at a speed $20km/hr$ and stops at the origin when $t=1$, where $t$ is in hours. What will the minimum distance be between the $2$ particles and at what time $t$ does such minimum occur. I have studied these questions for a while but i need help deriving an equation relating the two particle's movements to minimize them. Thanks for any help. Note: The particles are co-planar.","A particle moves west at a speed of $25km/hr$ from the origin when $t=0$. Another particle moves north at a speed $20km/hr$ and stops at the origin when $t=1$, where $t$ is in hours. What will the minimum distance be between the $2$ particles and at what time $t$ does such minimum occur. I have studied these questions for a while but i need help deriving an equation relating the two particle's movements to minimize them. Thanks for any help. Note: The particles are co-planar.",,"['calculus', 'geometry', 'derivatives', 'optimization', 'physics']"
90,Can someone explain the integration of $\sqrt{v²+\tfrac14}$ to me?,Can someone explain the integration of  to me?,\sqrt{v²+\tfrac14},"I am currently trying to integrate this root: $$\sqrt{v^2+\frac{1}{4}}$$ According to several integration calculators on the web it is: $$\frac{\operatorname{arsinh}(2v)}{8} +\frac{v\sqrt{v^2+\tfrac{1}{4}}}{2}$$ However, I just can't get my head around it. I have absolutely no idea where that $\operatorname{arsinh}$ is coming from. My take on this would have been: $$f = \left(v^2+\frac{1}{4}\right)^{\frac{1}{2}}\implies F = \frac{2}{3}\cdot\frac{1}{2v}\left(v^2+\frac{1}{4}\right)^{\frac{3}{2}}$$","I am currently trying to integrate this root: $$\sqrt{v^2+\frac{1}{4}}$$ According to several integration calculators on the web it is: $$\frac{\operatorname{arsinh}(2v)}{8} +\frac{v\sqrt{v^2+\tfrac{1}{4}}}{2}$$ However, I just can't get my head around it. I have absolutely no idea where that $\operatorname{arsinh}$ is coming from. My take on this would have been: $$f = \left(v^2+\frac{1}{4}\right)^{\frac{1}{2}}\implies F = \frac{2}{3}\cdot\frac{1}{2v}\left(v^2+\frac{1}{4}\right)^{\frac{3}{2}}$$",,"['integration', 'derivatives']"
91,Calculating the derivative of $\csc^2(4x)$,Calculating the derivative of,\csc^2(4x),"I am having problems calculating derivatives of squared trigonometric functions. $$f(x) = \csc^2(4x)$$ Let's see... This is equivalent to $$f(x) = \csc(4x)\cdot \csc(4x)$$ Now, to get the derivative, we use the product rule: $$f'(x) = 2 \cdot (-\csc(4x)\cdot\cot(4x)\cdot\csc(4x))$$ But this appears to be wrong when I evaluate this in a calculator. What is the problem with this procedure?","I am having problems calculating derivatives of squared trigonometric functions. $$f(x) = \csc^2(4x)$$ Let's see... This is equivalent to $$f(x) = \csc(4x)\cdot \csc(4x)$$ Now, to get the derivative, we use the product rule: $$f'(x) = 2 \cdot (-\csc(4x)\cdot\cot(4x)\cdot\csc(4x))$$ But this appears to be wrong when I evaluate this in a calculator. What is the problem with this procedure?",,"['calculus', 'derivatives']"
92,Find all twice-differentiable functions,Find all twice-differentiable functions,,"Find all twice- differentiable functions $f$ such that the average value of $f$ on each closed subinterval of $[a,b],$ $a < b,$ is the mean of $f$ at the endpoints of the subinterval. Please give me a hint how to start.","Find all twice- differentiable functions $f$ such that the average value of $f$ on each closed subinterval of $[a,b],$ $a < b,$ is the mean of $f$ at the endpoints of the subinterval. Please give me a hint how to start.",,"['calculus', 'derivatives']"
93,How to write down proof that if $\lim_{x\to \infty}f(x)=\alpha$ then $\lim_{x\to \infty}f'(x)=0$?,How to write down proof that if  then ?,\lim_{x\to \infty}f(x)=\alpha \lim_{x\to \infty}f'(x)=0,"Let $a, \alpha \in \Bbb{R}$; let $f: (a,+\infty)\to \mathbb{R}$ be differentiable; let $\lim_{x\to \infty}f(x)=\alpha$; let $\beta := \lim_{x\to \infty}f'(x)$. I want to show that $\beta = 0$. Now, the idea is quite clear to me. If $\lim_{x\to \infty}f(x)=\alpha$ then there is one horizontal assymptote and thus $f$ will get closer to it as $x$ grows. In that case, since it is horizontal, the derivative of $f$ should vanish. Although the idea is clear I'm not being able to write down this proof. I've tried by contradiction: suppose $\beta > 0$ first. Then since $\lim_{x\to \infty}f(x)=\alpha$, given $\epsilon > 0$ we have $|f(x)-\alpha|<\epsilon$ as long as $x > M$ for some suitable $M\in \mathbb{R}$, $M > 0$. Now, if we pick $x > M$ and focus on the interval $(M,x)$ there should be $c$, by the mean value theorem, such that $$f'(c) = \dfrac{f(x)-f(M)}{x-M}.$$ In this way I can introduce the derivative there, but it is computed at a fixed point. Of course if I vary $x$ the $c$ would vary and I believe I could get a contradiction from this, but I'm not finding how to do this properly. So how can I write down this idea for this proof? How can I get one contradiction from this?","Let $a, \alpha \in \Bbb{R}$; let $f: (a,+\infty)\to \mathbb{R}$ be differentiable; let $\lim_{x\to \infty}f(x)=\alpha$; let $\beta := \lim_{x\to \infty}f'(x)$. I want to show that $\beta = 0$. Now, the idea is quite clear to me. If $\lim_{x\to \infty}f(x)=\alpha$ then there is one horizontal assymptote and thus $f$ will get closer to it as $x$ grows. In that case, since it is horizontal, the derivative of $f$ should vanish. Although the idea is clear I'm not being able to write down this proof. I've tried by contradiction: suppose $\beta > 0$ first. Then since $\lim_{x\to \infty}f(x)=\alpha$, given $\epsilon > 0$ we have $|f(x)-\alpha|<\epsilon$ as long as $x > M$ for some suitable $M\in \mathbb{R}$, $M > 0$. Now, if we pick $x > M$ and focus on the interval $(M,x)$ there should be $c$, by the mean value theorem, such that $$f'(c) = \dfrac{f(x)-f(M)}{x-M}.$$ In this way I can introduce the derivative there, but it is computed at a fixed point. Of course if I vary $x$ the $c$ would vary and I believe I could get a contradiction from this, but I'm not finding how to do this properly. So how can I write down this idea for this proof? How can I get one contradiction from this?",,"['real-analysis', 'limits', 'derivatives', 'proof-writing', 'infinity']"
94,soft thresholding derivation with two terms,soft thresholding derivation with two terms,,"I am trying to minimize the following function $$argmin_x||x-y||^2_2 + ||x-z||_2^2 + \lambda || \frac{x- w}{c}||_1$$ I have been doing it for while but i am not sure how to do it. I have already been through few of the examples for simple cases such as this one ( Derivation of soft thresholding operator ). Taking the derivative w.r.t $x$ is given by $$\frac{x{-w}}{\left|c\right|\left|x{-w}\right|}+4x{-2z}{-2y}$$, i would like to know that how i would be able to go from here to the soft thresholding function described in the above link. Any help regarding this would be quite useful","I am trying to minimize the following function $$argmin_x||x-y||^2_2 + ||x-z||_2^2 + \lambda || \frac{x- w}{c}||_1$$ I have been doing it for while but i am not sure how to do it. I have already been through few of the examples for simple cases such as this one ( Derivation of soft thresholding operator ). Taking the derivative w.r.t $x$ is given by $$\frac{x{-w}}{\left|c\right|\left|x{-w}\right|}+4x{-2z}{-2y}$$, i would like to know that how i would be able to go from here to the soft thresholding function described in the above link. Any help regarding this would be quite useful",,"['derivatives', 'optimization', 'normed-spaces', 'convex-optimization']"
95,$F(F(x)+x)^k)=(F(x)+x)^2-x$,,F(F(x)+x)^k)=(F(x)+x)^2-x,"I have no idea about this problem. But I feel we have to use chain rule of differentiation here. The Function $F(x)$ is defined by the following identity: $F(F(x)+x)^k)=(F(x)+x)^2-x$ The value of $F(1)$ is such that a finite number of possible values of $F'(1)$ can be determined solely from the above information.The maximum value of $k$ such that  $F'(1)$ is an integer can be expressed as $\frac{a}{b}$, where $a , b$ are co-prime integers. What is the value of $a+b$","I have no idea about this problem. But I feel we have to use chain rule of differentiation here. The Function $F(x)$ is defined by the following identity: $F(F(x)+x)^k)=(F(x)+x)^2-x$ The value of $F(1)$ is such that a finite number of possible values of $F'(1)$ can be determined solely from the above information.The maximum value of $k$ such that  $F'(1)$ is an integer can be expressed as $\frac{a}{b}$, where $a , b$ are co-prime integers. What is the value of $a+b$",,"['calculus', 'real-analysis']"
96,Continuity and Differentiability Q,Continuity and Differentiability Q,,"We have f = e^(-1/|x|) if x is not equal to 0 and f(0) = p. Question 1: for what value(s) of p is f differentiable at 0? Question 2: is f' continuous for the values found in question 1? What I tried for question 1: f is differentiable for the values at which the limit as h goes to 0 of (f(h) - f(o))/h exist. If I continue down this route I get to f'(0) = lim h->0 1/(h*e^(-1/|h|)) - lim h->0 p/h. How should I continue? And for question 2: if we can differentiate a function at some point then that function is by definition also continuous at that point, because continuity is a condition for differentiability. Appreciating all input.","We have f = e^(-1/|x|) if x is not equal to 0 and f(0) = p. Question 1: for what value(s) of p is f differentiable at 0? Question 2: is f' continuous for the values found in question 1? What I tried for question 1: f is differentiable for the values at which the limit as h goes to 0 of (f(h) - f(o))/h exist. If I continue down this route I get to f'(0) = lim h->0 1/(h*e^(-1/|h|)) - lim h->0 p/h. How should I continue? And for question 2: if we can differentiate a function at some point then that function is by definition also continuous at that point, because continuity is a condition for differentiability. Appreciating all input.",,"['derivatives', 'continuity']"
97,Calculate a cubic curve from two slopes and two points,Calculate a cubic curve from two slopes and two points,,"I have a question that is supposed to be solved using derivatives. The question asks to find the cubic polynomial function $f(x)$ where $y=13(x-1)+4$ is tangent to $f(x)$ at $(1,4)$ and $y=(x+1)+6$ is tangent to $f(x)$ at $(-1,6)$ How do I go about finding the function $f(x)$ using this information and the fact that this is a question specifically about derivatives?","I have a question that is supposed to be solved using derivatives. The question asks to find the cubic polynomial function $f(x)$ where $y=13(x-1)+4$ is tangent to $f(x)$ at $(1,4)$ and $y=(x+1)+6$ is tangent to $f(x)$ at $(-1,6)$ How do I go about finding the function $f(x)$ using this information and the fact that this is a question specifically about derivatives?",,"['derivatives', 'polynomials']"
98,What do I do next when trying to find the derivative of this fraction?,What do I do next when trying to find the derivative of this fraction?,,"I'm trying to find the derivative of this equation: $-\frac{3(x-6)}{2\sqrt{9-x}}$ The quotient rule: $\frac{d}{dx}[\frac{f(x)}{g(x)}]=\frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2}$ where $g(x)$ and $f(x)$ are functions. So I take out the constants and I'm left with $\frac{-3}{2}\frac{d}{dx}[\frac{(x-6)}{\sqrt{9-x}}]$. $f(x)$ is $(x-6)$ and $g(x)$ is $(9-x)^{1/2}$. This is what it expands to: $$\frac{-3}{2} \left[\frac{(9-x)^{1/2}\frac{d}{dx}[x-6]-(x-6)\frac{d}{dx}[(9-x)^{1/2}]}{((9-x)^{1/2})^2}\right]$$ After simplifying, I get: $$\frac{-3}{2}\left[\frac{(9-x)^{1/2}(1)-(x-6)[(\frac{1}{2})(9-x)^{-1/2}(-1)]}{9-x}\right]$$ $$\frac{-3}{2}\left[\frac{\sqrt{9-x}-(x-6)(-1)}{2(9-x)\sqrt{9-x}}\right]$$ $$\frac{-3}{2}\left[\frac{\sqrt{9-x}+x-6}{2(9-x)\sqrt{9-x}}\right]$$ $$\frac{-3\sqrt{9-x}-3x+18}{4(9-x)\sqrt{9-x}} \text{ ??}$$ What did I do wrong? Wolfram Alpha says the answer is: $\frac{3(x-12)}{4(9-x)^{3/2}}$","I'm trying to find the derivative of this equation: $-\frac{3(x-6)}{2\sqrt{9-x}}$ The quotient rule: $\frac{d}{dx}[\frac{f(x)}{g(x)}]=\frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2}$ where $g(x)$ and $f(x)$ are functions. So I take out the constants and I'm left with $\frac{-3}{2}\frac{d}{dx}[\frac{(x-6)}{\sqrt{9-x}}]$. $f(x)$ is $(x-6)$ and $g(x)$ is $(9-x)^{1/2}$. This is what it expands to: $$\frac{-3}{2} \left[\frac{(9-x)^{1/2}\frac{d}{dx}[x-6]-(x-6)\frac{d}{dx}[(9-x)^{1/2}]}{((9-x)^{1/2})^2}\right]$$ After simplifying, I get: $$\frac{-3}{2}\left[\frac{(9-x)^{1/2}(1)-(x-6)[(\frac{1}{2})(9-x)^{-1/2}(-1)]}{9-x}\right]$$ $$\frac{-3}{2}\left[\frac{\sqrt{9-x}-(x-6)(-1)}{2(9-x)\sqrt{9-x}}\right]$$ $$\frac{-3}{2}\left[\frac{\sqrt{9-x}+x-6}{2(9-x)\sqrt{9-x}}\right]$$ $$\frac{-3\sqrt{9-x}-3x+18}{4(9-x)\sqrt{9-x}} \text{ ??}$$ What did I do wrong? Wolfram Alpha says the answer is: $\frac{3(x-12)}{4(9-x)^{3/2}}$",,"['calculus', 'derivatives', 'solution-verification']"
99,"Where does ""$f(x_0+h)-f(x_0)=f'(x_0)h+E(h)$"" come from?","Where does """" come from?",f(x_0+h)-f(x_0)=f'(x_0)h+E(h),In my text book it says that if a function $f$ is differentiable at $x_0$ if $f(x_0+h)-f(x_0)=f'(x_0)h+E(h)$ but I don't understand where this has come from. I thought for a function $f$ to be differentiable at $x_0$ we need only ensure that $$\lim_{\delta x \rightarrow 0} \frac{f(x_0+\delta x)-f(x_0)}{\delta x}$$ exists. How are the two equivalent? Also I don't see why we need the error function tacked on why doesn't $f(x_0+h)-f(x_0)=f'(x_0)h$ hold? In my mind I think $f(x_0+h)-f(x_0)=f'(x_0)h \implies f'(x_0)=\frac{f(x_0+h)-f(x_0)}{h}$ which says that the gradient at the point is the derivative but this is the case so where am I wrong?,In my text book it says that if a function $f$ is differentiable at $x_0$ if $f(x_0+h)-f(x_0)=f'(x_0)h+E(h)$ but I don't understand where this has come from. I thought for a function $f$ to be differentiable at $x_0$ we need only ensure that $$\lim_{\delta x \rightarrow 0} \frac{f(x_0+\delta x)-f(x_0)}{\delta x}$$ exists. How are the two equivalent? Also I don't see why we need the error function tacked on why doesn't $f(x_0+h)-f(x_0)=f'(x_0)h$ hold? In my mind I think $f(x_0+h)-f(x_0)=f'(x_0)h \implies f'(x_0)=\frac{f(x_0+h)-f(x_0)}{h}$ which says that the gradient at the point is the derivative but this is the case so where am I wrong?,,"['calculus', 'derivatives']"
