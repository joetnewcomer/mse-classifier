,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find $S = a + b$ such that for $\forall m \in \left[a\sqrt{\frac{15}{7}} + b\sqrt{\frac{7}{15}}; 2\right)$ then $2x^2 + 2x - mf(x) + 5 = 0$ has root,Find  such that for  then  has root,S = a + b \forall m \in \left[a\sqrt{\frac{15}{7}} + b\sqrt{\frac{7}{15}}; 2\right) 2x^2 + 2x - mf(x) + 5 = 0,"Let $f(x)$ be continuos on $\mathbb R$ satisfy $f(0) = 2\sqrt{2}$ and $f(x) > 0, \forall x \in \mathbb R$ and $f(x) f'(x) = (2x+1)\sqrt{1+f^2(x)}$ . For all $m \in \left[a\sqrt{\dfrac{15}{7}} + b\sqrt{\dfrac{7}{15}}; 2\right)$ , then $2x^2 + 2x - mf(x) + 5 = 0$ has at least a root. Find $S = a + b$ Here is what I did so far $$f(x) f'(x) = (2x+1)\sqrt{1+f^2(x)}$$ $$\implies \dfrac{f(x) f'(x)}{\sqrt{1+f^2(x)}} = 2x+1$$ $$\implies \int \dfrac{f(x) f'(x)}{\sqrt{1+f^2(x)}} \,dx = \int2x+1 \,dx = x^2 + x + C$$ Let $t = f^2(x) + 1 \implies dt = 2f'(x)f(x) dx$ . Therefore: $$\int \dfrac{1}{2\sqrt{t}} \,dx = x^2 + x + C$$ $$\implies \sqrt{t} = x^2 + x + C$$ $$\implies \sqrt{1 + f^2(x)} = x^2 + x + C \, (2)$$ Since $f(0) = 2\sqrt{2}$ , plug in $(2)$ we get $C = 3 \implies f(x) = \sqrt{(x^2+x+3)^2 - 1}$ I can't proceed further from here","Let be continuos on satisfy and and . For all , then has at least a root. Find Here is what I did so far Let . Therefore: Since , plug in we get I can't proceed further from here","f(x) \mathbb R f(0) = 2\sqrt{2} f(x) > 0, \forall x \in \mathbb R f(x) f'(x) = (2x+1)\sqrt{1+f^2(x)} m \in \left[a\sqrt{\dfrac{15}{7}} + b\sqrt{\dfrac{7}{15}}; 2\right) 2x^2 + 2x - mf(x) + 5 = 0 S = a + b f(x) f'(x) = (2x+1)\sqrt{1+f^2(x)} \implies \dfrac{f(x) f'(x)}{\sqrt{1+f^2(x)}} = 2x+1 \implies \int \dfrac{f(x) f'(x)}{\sqrt{1+f^2(x)}} \,dx = \int2x+1 \,dx = x^2 + x + C t = f^2(x) + 1 \implies dt = 2f'(x)f(x) dx \int \dfrac{1}{2\sqrt{t}} \,dx = x^2 + x + C \implies \sqrt{t} = x^2 + x + C \implies \sqrt{1 + f^2(x)} = x^2 + x + C \, (2) f(0) = 2\sqrt{2} (2) C = 3 \implies f(x) = \sqrt{(x^2+x+3)^2 - 1}","['calculus', 'integration', 'derivatives', 'polynomials']"
1,Derivative of squared frobenius norm of hadamard product of outer product of vector with itself and matrix w.r.t. vector,Derivative of squared frobenius norm of hadamard product of outer product of vector with itself and matrix w.r.t. vector,,"I know the title is a mouth full, and there have been many similar (and probably more complicated) questions/answers on this site, but I'm stuck on this specific problem. I am working with the following function: $$f(x) = \frac{1}{2}||xx^{T} \circ Y||_{2}^{2}$$ where $x \in \mathbb{R}^{n}$ , $Y \in \mathbb{R}^{n \times n}$ and $\circ$ denotes the hadamard (element wise) product. I would like to compute the following gradient: $$\frac{\partial f(x)}{\partial x} = \frac{\partial }{\partial x} \frac{1}{2}||xx^{T} \circ Y||_{2}^{2}$$ The furthest I've gotten is w.r.t. $xx^{T}$ , following this : $$\frac{\partial f(x)}{\partial xx^{T}} = xx^{T} \circ Y$$ but I do not know how to actually compute this w.r.t. $x$ . I am pretty inexperienced with this kind of thing, so any input would be really helpful. Thank you!","I know the title is a mouth full, and there have been many similar (and probably more complicated) questions/answers on this site, but I'm stuck on this specific problem. I am working with the following function: where , and denotes the hadamard (element wise) product. I would like to compute the following gradient: The furthest I've gotten is w.r.t. , following this : but I do not know how to actually compute this w.r.t. . I am pretty inexperienced with this kind of thing, so any input would be really helpful. Thank you!",f(x) = \frac{1}{2}||xx^{T} \circ Y||_{2}^{2} x \in \mathbb{R}^{n} Y \in \mathbb{R}^{n \times n} \circ \frac{\partial f(x)}{\partial x} = \frac{\partial }{\partial x} \frac{1}{2}||xx^{T} \circ Y||_{2}^{2} xx^{T} \frac{\partial f(x)}{\partial xx^{T}} = xx^{T} \circ Y x,"['matrices', 'derivatives', 'matrix-calculus', 'hadamard-product']"
2,Common notation of differentials,Common notation of differentials,,I have a doubt which confuses me a lot. If f(x) is a function in x then does f’(2x) mean $d f(2x)/d 2x $ or $d f(2x)/ d x $ as when we integrate it the result is f(2x)/2 and the same for $ f’(x^2)$ . Is it $d f(x^2)/d x^2 $ or $d f(x^2)/d x $ and what do we get on integrating it? Sorry for the very petty doubt but it really confuses me. Any help would be greatly appreciated ! Thanks in advance!,I have a doubt which confuses me a lot. If f(x) is a function in x then does f’(2x) mean or as when we integrate it the result is f(2x)/2 and the same for . Is it or and what do we get on integrating it? Sorry for the very petty doubt but it really confuses me. Any help would be greatly appreciated ! Thanks in advance!,d f(2x)/d 2x  d f(2x)/ d x   f’(x^2) d f(x^2)/d x^2  d f(x^2)/d x ,['derivatives']
3,"Derivative, slope or, the tangent of a graph with a shape which has corner like tips as in the letter V [closed]","Derivative, slope or, the tangent of a graph with a shape which has corner like tips as in the letter V [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Consider the graph of a function in the shape of  the letter ‘V’ , how would we be finding the derivative, slope or, the tangent of the function at the value of the function that corresponds to the tip of ‘V’ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Consider the graph of a function in the shape of  the letter ‘V’ , how would we be finding the derivative, slope or, the tangent of the function at the value of the function that corresponds to the tip of ‘V’ ?",,"['derivatives', 'graph-theory', 'tangent-line', 'slope']"
4,Determine continuous function from piecewise derivative,Determine continuous function from piecewise derivative,,"I'm having trouble solving this issue. Determine a continuous function $f$ on the interval $[-2,2]$ whose derived function on $[-2,2] \setminus \{0\}$ is known to be the function: $$ f(x)= \left\lbrace \begin{array}{lll} \dfrac{x^2+4x+7}{2x^3-x^2+18x-9} & \text{ if} -2 \leq x < 0 \\  & \\ x^2 \sin^2 (x) & \text{ if } 0 < x \leq 2 \end{array} \right. $$ I tried to calculate the integral of the function in each definition interval, add a constant and impose conditions to determine the constant. I have the problem in the first interval. I can't integrate the function. How do I solve this problem? Thanks!","I'm having trouble solving this issue. Determine a continuous function on the interval whose derived function on is known to be the function: I tried to calculate the integral of the function in each definition interval, add a constant and impose conditions to determine the constant. I have the problem in the first interval. I can't integrate the function. How do I solve this problem? Thanks!","f [-2,2] [-2,2] \setminus \{0\} 
f(x)=
\left\lbrace
\begin{array}{lll}
\dfrac{x^2+4x+7}{2x^3-x^2+18x-9} & \text{ if} -2 \leq x < 0 \\ 
& \\
x^2 \sin^2 (x) & \text{ if } 0 < x \leq 2
\end{array}
\right.
","['calculus', 'integration', 'derivatives', 'piecewise-continuity']"
5,Prove $f'(x_n) \to f'(x)$,Prove,f'(x_n) \to f'(x),"Let $f: I \to R $ differentiable and $x \in I$ . Prove that there exists a sequence { $x_n$ } in $I$ different from $x$ such that $f'(x_n) \to f'(x)$ So below is my approach to this problem: Take an arbitrary sequence { $y_n$ } and apply the MVT, we have there exists a sequence { $x_n$ }  between { $y_n$ } and $x$ such that $\frac{f(y_n) - f(x)}{y_n - x} = f'(x_n)$ Then taking $\lim_{y_n\to x}$ and since $y_n$ is arbitrary, we have the desired result. Am I correct here? Thank you!","Let differentiable and . Prove that there exists a sequence { } in different from such that So below is my approach to this problem: Take an arbitrary sequence { } and apply the MVT, we have there exists a sequence { }  between { } and such that Then taking and since is arbitrary, we have the desired result. Am I correct here? Thank you!",f: I \to R  x \in I x_n I x f'(x_n) \to f'(x) y_n x_n y_n x \frac{f(y_n) - f(x)}{y_n - x} = f'(x_n) \lim_{y_n\to x} y_n,"['real-analysis', 'derivatives']"
6,Unusual Constant appearing for definite integral,Unusual Constant appearing for definite integral,,"I’m learning integration and have come across a question in a textbook. I arrived to the same answer as the textbook itself, but I know it is not correct. It starts as part a) show that $$\frac{d}{dx} tan^{-1} \left(\frac{3tan(x)}{2} \right)= \frac{6}{5sin^2(x)+4}$$ Hence, find area bounded by curve from $ \dfrac{1}{5sin^2(x) + 4}$ from $x=0$ to $x=7$ . Clearly using part A to integrate and substituting 7 and 0, I arrive to 0.153 square units, in line with the textbook. Yet, by using desmos to graph the curve, using lower rectangles, minimum area has to be $0.111111 \times 7 = 0.77777$ units squared. I put the integral on integral calculator and it yielded $0.153 + \dfrac{\pi}{3}$ , without explaining where this constant came from. I am unfamiliar with constants in definite integrals, and struggled to find anything online to answer this. Thanks. (Note: I am not asking for homework help, Im just asking about how to find constants in these definite integrals, particularly in a question like this, which the textbook itself overlooked. Thank you. Any help is appreciated.)","I’m learning integration and have come across a question in a textbook. I arrived to the same answer as the textbook itself, but I know it is not correct. It starts as part a) show that Hence, find area bounded by curve from from to . Clearly using part A to integrate and substituting 7 and 0, I arrive to 0.153 square units, in line with the textbook. Yet, by using desmos to graph the curve, using lower rectangles, minimum area has to be units squared. I put the integral on integral calculator and it yielded , without explaining where this constant came from. I am unfamiliar with constants in definite integrals, and struggled to find anything online to answer this. Thanks. (Note: I am not asking for homework help, Im just asking about how to find constants in these definite integrals, particularly in a question like this, which the textbook itself overlooked. Thank you. Any help is appreciated.)",\frac{d}{dx} tan^{-1} \left(\frac{3tan(x)}{2} \right)= \frac{6}{5sin^2(x)+4}  \dfrac{1}{5sin^2(x) + 4} x=0 x=7 0.111111 \times 7 = 0.77777 0.153 + \dfrac{\pi}{3},"['calculus', 'integration', 'derivatives', 'definite-integrals']"
7,"Is $\Vert \max(0, M-x)\Vert^2$ differentiable?",Is  differentiable?,"\Vert \max(0, M-x)\Vert^2","I think $\max(0, M-x)^2$ is differentiable everywhere, but what if $x\in \mathbb{R}^n$ , can we prove that $f(x) = \Vert \max(0, M-x)\Vert^2, x\in \mathbb{R}^n$ is differentiable? $M \in \mathbb{R}^n$ is a constant, when $$x \in \mathbb{R}^n,  \max(0, M-x) = \begin{bmatrix}\max(0, M_1 - x_1) \\ \vdots \\ \max(0, M_n - x_n)\end{bmatrix} \in \mathbb{R}^n.$$","I think is differentiable everywhere, but what if , can we prove that is differentiable? is a constant, when","\max(0, M-x)^2 x\in \mathbb{R}^n f(x) = \Vert \max(0, M-x)\Vert^2, x\in \mathbb{R}^n M \in \mathbb{R}^n x \in \mathbb{R}^n,  \max(0, M-x) = \begin{bmatrix}\max(0, M_1 - x_1) \\ \vdots \\ \max(0, M_n - x_n)\end{bmatrix} \in \mathbb{R}^n.","['real-analysis', 'functional-analysis', 'derivatives']"
8,Is the derivative of the Riemann integral continuous wherever it exists?,Is the derivative of the Riemann integral continuous wherever it exists?,,"Let $f$ be Riemann integrable on $[a,b]$ and let $F(x)$ be its integral on $[a,x]$ . (Note the minimal assumption of integrability, not continuity.) Must $F’$ be continuous wherever it’s defined? ( Nota bene : here I mean continuous in the general sense of continuous on its domain. On this definition, it’s possible for a function to be continuous at a point without there being a deleted interval of the point on which the function is defined.) Context The claim is plausible on these heuristic grounds: $F$ ignores trouble at discontinuities (which happen on a set of measure zero), so even if $f$ fails to be continuous somewhere, $F’$ should be continuous there (if it exists). In other words, $F’$ should be “at least as nice” as $f$ (again, given that it exists). As an example, take an $f$ continuous except at $c$ , where it has a removable discontinuity. Let $g$ be the everywhere continuous function that removes the discontinuity. Since $f=g$ almost everywhere, their integrals $F$ and $G$ are everywhere the same. By the FTC, $G’=g$ ; combining with the previous sentence we have $F’=g$ . Thus $F’$ will be continuous at $c$ even though $f$ isn’t: it plugs the hole. This is just intuition. Any proof will need to deal with two cases: one at points where $f$ is continuous and the other at points where $f$ is discontinuous. In the former case we know by the FTC that $F’$ exists at $c$ (and moreover $F’(c)=f(c)$ ). Can we prove that it’s continuous there? In the latter case, we could break into subcases depending on the type of discontinuity. If $f$ has a removable discontinuity we can reduce to the previous case. If $f$ has a jump discontinuity, I think we can show $F’$ doesn’t exist. And if $f$ has an essential (oscillatory) discontinuity, I also think we can show $F’$ doesn’t exist. (I haven’t produced a rigorous argument but can’t find counterexamples. I’m stuck with $f(x)=\sin(1/x)$ on $[-1,0)\cup(0,1]$ and $0$ at the origin. Will $F’$ exist at the origin?) If all that’s right, the proof depends essentially on whether the following implication holds: $f$ continuous at $c$ implies $F’$ continuous at $c$ . If we strengthen the hypothesis by assuming $f$ is not just continuous but differentiable at $c$ , it does indeed follow that $F’$ is continuous at $c$ . The question is whether we can relax the hypothesis to bare continuity of $f$ at $c$ . EDIT Paramanand Singh points out in comments that $F'$ can indeed exist and be discontinuous where $f$ has an oscillatory discontinuity. (The right example, as I guessed, was $f(x)=\sin(1/x)$ on $[-1,0)\cup(0,1]$ and $0$ at the origin, but it turns out that $F'$ exists here, contrary to my hunch.) So the answer to the question in the title is no. But this question is still open: if $f$ is continuous somewhere, must $F'$ be continuous there?","Let be Riemann integrable on and let be its integral on . (Note the minimal assumption of integrability, not continuity.) Must be continuous wherever it’s defined? ( Nota bene : here I mean continuous in the general sense of continuous on its domain. On this definition, it’s possible for a function to be continuous at a point without there being a deleted interval of the point on which the function is defined.) Context The claim is plausible on these heuristic grounds: ignores trouble at discontinuities (which happen on a set of measure zero), so even if fails to be continuous somewhere, should be continuous there (if it exists). In other words, should be “at least as nice” as (again, given that it exists). As an example, take an continuous except at , where it has a removable discontinuity. Let be the everywhere continuous function that removes the discontinuity. Since almost everywhere, their integrals and are everywhere the same. By the FTC, ; combining with the previous sentence we have . Thus will be continuous at even though isn’t: it plugs the hole. This is just intuition. Any proof will need to deal with two cases: one at points where is continuous and the other at points where is discontinuous. In the former case we know by the FTC that exists at (and moreover ). Can we prove that it’s continuous there? In the latter case, we could break into subcases depending on the type of discontinuity. If has a removable discontinuity we can reduce to the previous case. If has a jump discontinuity, I think we can show doesn’t exist. And if has an essential (oscillatory) discontinuity, I also think we can show doesn’t exist. (I haven’t produced a rigorous argument but can’t find counterexamples. I’m stuck with on and at the origin. Will exist at the origin?) If all that’s right, the proof depends essentially on whether the following implication holds: continuous at implies continuous at . If we strengthen the hypothesis by assuming is not just continuous but differentiable at , it does indeed follow that is continuous at . The question is whether we can relax the hypothesis to bare continuity of at . EDIT Paramanand Singh points out in comments that can indeed exist and be discontinuous where has an oscillatory discontinuity. (The right example, as I guessed, was on and at the origin, but it turns out that exists here, contrary to my hunch.) So the answer to the question in the title is no. But this question is still open: if is continuous somewhere, must be continuous there?","f [a,b] F(x) [a,x] F’ F f F’ F’ f f c g f=g F G G’=g F’=g F’ c f f f F’ c F’(c)=f(c) f f F’ f F’ f(x)=\sin(1/x) [-1,0)\cup(0,1] 0 F’ f c F’ c f c F’ c f c F' f f(x)=\sin(1/x) [-1,0)\cup(0,1] 0 F' f F'","['real-analysis', 'calculus', 'integration', 'derivatives']"
9,Do functions with vertical asymptotes necessarily have infinite derivatives for all degrees?,Do functions with vertical asymptotes necessarily have infinite derivatives for all degrees?,,"Suppose I have a function $f(x)$ where $f:\mathbb R^n\to\mathbb R$ . First take $n = 1$ . Now suppose that there exists a point $\bar x$ where the $d$ th derivative $f^d(\bar x)$ is defined and is finite for some $d \geq 1$ . Does this necessarily imply that $f(\bar x)$ is defined and is finite, or is it possible that it is undefined or infinite? (The example I am trying to construct is something like: $f(0) = 0$ , $\lim_{x\to 1} f(x) = +\infty$ . (So it is defined, it just isn't finite.) We can also assume that $f(x) \geq 0$ for $0 \leq x < 1$ , to help at least with the $d=1$ case. Now $f^d(x)$ is defined and finite for all $x < 1$ . But, is it necessary that $f^d(x)\overset{x\to 1}{\to} +\infty$ for all $d$ as well?) Now suppose $n > 1$ , and suppose $\mathcal C\subset \mathbb R^n$ is a closed connected set. I know that for some $\bar x\in \mathcal C$ , $f(\bar x) = 0$ , and I also know that for all $x\in \mathcal C$ ,  for some $u$ , $u^T\nabla f(x) u$ is finite. Does this necessarily mean that $f(x)$ is finite for all $x\in \mathcal C$ ? This seems like a basic result, but I'm fearful of potential counterexamples. Edit : the more that I think about it, the more I think that it is true in $n = 1$ , but can't be true in $n > 1$ . For $n = 1$ , it seems like if $f(x)$ is finite-valued for all $x \in [0,1)$ , then the only way that $\lim_{x\to 1}f(x) = +\infty$ is if all its derivatives go to $+\infty$ at $x\to 1$ . For $n > 1$ , the simple counterexample of $$ \nabla^2 f(x) = \left[\begin{matrix} 1 & 0 \\ 0 & 1/(1-x_2) \end{matrix}\right] $$ seems to satisfy the bill.","Suppose I have a function where . First take . Now suppose that there exists a point where the th derivative is defined and is finite for some . Does this necessarily imply that is defined and is finite, or is it possible that it is undefined or infinite? (The example I am trying to construct is something like: , . (So it is defined, it just isn't finite.) We can also assume that for , to help at least with the case. Now is defined and finite for all . But, is it necessary that for all as well?) Now suppose , and suppose is a closed connected set. I know that for some , , and I also know that for all ,  for some , is finite. Does this necessarily mean that is finite for all ? This seems like a basic result, but I'm fearful of potential counterexamples. Edit : the more that I think about it, the more I think that it is true in , but can't be true in . For , it seems like if is finite-valued for all , then the only way that is if all its derivatives go to at . For , the simple counterexample of seems to satisfy the bill.","f(x) f:\mathbb R^n\to\mathbb R n = 1 \bar x d f^d(\bar x) d \geq 1 f(\bar x) f(0) = 0 \lim_{x\to 1} f(x) = +\infty f(x) \geq 0 0 \leq x < 1 d=1 f^d(x) x < 1 f^d(x)\overset{x\to 1}{\to} +\infty d n > 1 \mathcal C\subset \mathbb R^n \bar x\in \mathcal C f(\bar x) = 0 x\in \mathcal C u u^T\nabla f(x) u f(x) x\in \mathcal C n = 1 n > 1 n = 1 f(x) x \in [0,1) \lim_{x\to 1}f(x) = +\infty +\infty x\to 1 n > 1 
\nabla^2 f(x) = \left[\begin{matrix} 1 & 0 \\ 0 & 1/(1-x_2) \end{matrix}\right]
","['real-analysis', 'calculus', 'derivatives']"
10,"Given $V= \pi \int_{1}^{c}-y \sqrt{1-y^2}\,\mathrm{d}y$, Find $f^\prime\left(x\right)$ [closed]","Given , Find  [closed]","V= \pi \int_{1}^{c}-y \sqrt{1-y^2}\,\mathrm{d}y f^\prime\left(x\right)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose that I have a function, $f\left(x\right)$ s.t. $f\left(x\right) > 0$ on $\left[0,a\right]$ and $f\left(0\right)=1$ and $f\left(a\right) = c$ . The volume $V$ of the function is $$V= \pi \int_{1}^{c}-y \sqrt{1-y^2}\,\mathrm{d}y$$ (and $y=f\left(x\right)$ ). How do I obtain $f^\prime\left(x\right)$ ? On the surface, this looks like a question where I should integrate the volume. However, I have no way of finding out $f^\prime\left(x\right)$ using the fundamental theorem since the bound $c$ is unknown. Help would be appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose that I have a function, s.t. on and and . The volume of the function is (and ). How do I obtain ? On the surface, this looks like a question where I should integrate the volume. However, I have no way of finding out using the fundamental theorem since the bound is unknown. Help would be appreciated.","f\left(x\right) f\left(x\right) > 0 \left[0,a\right] f\left(0\right)=1 f\left(a\right) = c V V= \pi \int_{1}^{c}-y \sqrt{1-y^2}\,\mathrm{d}y y=f\left(x\right) f^\prime\left(x\right) f^\prime\left(x\right) c","['calculus', 'derivatives', 'volume']"
11,"Let $f\in C([a,b])$, $\int_{a}^{b}f(x)\phi'(x) \ dx=0$ for all continuous and differentiable functions $\phi$. Show $f$ is constant","Let ,  for all continuous and differentiable functions . Show  is constant","f\in C([a,b]) \int_{a}^{b}f(x)\phi'(x) \ dx=0 \phi f","Let $f\in C([a,b])$ and $\int_{a}^{b}f(x)\phi'(x) \ dx=0$ for all continuous and differentiable functions $\phi$ on $[a,b]$ with $\phi(a)=\phi(b)=0$ . Show that $f$ is constant on $[a,b]$ . I would like to know if my proof holds, please. My attempt is to pass by primitive of $f$ . First of all, as $f\in C([a,b])$ , $f$ has a primitive $F\in C^1([a,b])$ such that $F'(x)=f(x) \ \forall x \in [a,b]$ . Then, as $\int_{a}^{b}f(x)\phi'(x) \ dx=0$ holds for all continuous and differentiable functions $\phi$ on $[a,b]$ , we can choose $\phi=F$ . So we have to show that $\int_{a}^{b}f(x)F'(x) \ dx=\int_{a}^{b}f^2(x)=0 \implies f$ constant. Let $G$ be a primitive of $f^2$ . We have that $G'(x)=f^2(x)\ge 0 \ \forall x \in [a,b]$ . So, $G$ is increasing on $[a,b]$ . Moreover, $G(b)-G(a)=0 \iff G(a)=G(b)$ . So, we conclude that $G$ is constant as $a\le x\le b\iff G(a)\le G(x)\le G(b)=G(a)$ . Thus, $G'=f^2=0 \Rightarrow f=0$ . So we showed that $f$ is constant. I'm curious as well if there is a maneer to prove the statement using upper and lower Darboux sum considering by absurd that $f\neq 0$ so there exists $c\in[a,b]: f(c)\neq0$ . I tried to prove the statement using it, but I feel lost in the inequalities. I tried to pass by integrability of th functions implies integrability of absolute value of functions and by Upper Darboux sum.","Let and for all continuous and differentiable functions on with . Show that is constant on . I would like to know if my proof holds, please. My attempt is to pass by primitive of . First of all, as , has a primitive such that . Then, as holds for all continuous and differentiable functions on , we can choose . So we have to show that constant. Let be a primitive of . We have that . So, is increasing on . Moreover, . So, we conclude that is constant as . Thus, . So we showed that is constant. I'm curious as well if there is a maneer to prove the statement using upper and lower Darboux sum considering by absurd that so there exists . I tried to prove the statement using it, but I feel lost in the inequalities. I tried to pass by integrability of th functions implies integrability of absolute value of functions and by Upper Darboux sum.","f\in C([a,b]) \int_{a}^{b}f(x)\phi'(x) \ dx=0 \phi [a,b] \phi(a)=\phi(b)=0 f [a,b] f f\in C([a,b]) f F\in C^1([a,b]) F'(x)=f(x) \ \forall x \in [a,b] \int_{a}^{b}f(x)\phi'(x) \ dx=0 \phi [a,b] \phi=F \int_{a}^{b}f(x)F'(x) \ dx=\int_{a}^{b}f^2(x)=0 \implies f G f^2 G'(x)=f^2(x)\ge 0 \ \forall x \in [a,b] G [a,b] G(b)-G(a)=0 \iff G(a)=G(b) G a\le x\le b\iff G(a)\le G(x)\le G(b)=G(a) G'=f^2=0 \Rightarrow f=0 f f\neq 0 c\in[a,b]: f(c)\neq0","['real-analysis', 'integration', 'derivatives', 'solution-verification', 'riemann-sum']"
12,Bounded 2nd derivative and finding a bound for the 1st derivative,Bounded 2nd derivative and finding a bound for the 1st derivative,,"Let $f:[0,1]\xrightarrow{}\mathbb{R}$ be a function such that $f(0)=f(1)=0$ the $f''$ exists and it's continuos on $[0,1]$ , suppose that $|f''(x)|\leq A$ for all $x\in [0,1]$ . Show that $|f'(x)|\leq A/2$ for all $x\in[0,1]$ and $|f'(1/2)| \leq A/4$ . I tried using the Taylor's theorem with Lagrange remainder by plugging 0 and 1 in order to make some terms vanish, however I wasn't able to isolate $A$ and $f'(x)$ . $$ f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2!}f'(c)(x-x_0)^2 \\ f(x) = f(0)   + f'(0)x + \frac{1}{2!}f'(c)x^2 f(1/2) = f(0)   + \frac{f'(0)}{2} + \frac{1}{2! 4}f''(c) = \frac{f'(0)}{2} + \frac{1}{2! 4}f''(c)\\ \leq \frac{f'(0)}{2} + \frac{1}{2! 4}A = \frac{1}{2}\left(f'(0)+\frac{1}{4}A \right) $$ I also thought of applying Rolle's theorem in some way, but I don't think it fits anywhere. If someone could point me in a useful direction, I would be glad.","Let be a function such that the exists and it's continuos on , suppose that for all . Show that for all and . I tried using the Taylor's theorem with Lagrange remainder by plugging 0 and 1 in order to make some terms vanish, however I wasn't able to isolate and . I also thought of applying Rolle's theorem in some way, but I don't think it fits anywhere. If someone could point me in a useful direction, I would be glad.","f:[0,1]\xrightarrow{}\mathbb{R} f(0)=f(1)=0 f'' [0,1] |f''(x)|\leq A x\in [0,1] |f'(x)|\leq A/2 x\in[0,1] |f'(1/2)| \leq A/4 A f'(x) 
f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2!}f'(c)(x-x_0)^2 \\
f(x) = f(0)   + f'(0)x + \frac{1}{2!}f'(c)x^2
f(1/2) = f(0)   + \frac{f'(0)}{2} + \frac{1}{2! 4}f''(c) = \frac{f'(0)}{2} + \frac{1}{2! 4}f''(c)\\
\leq \frac{f'(0)}{2} + \frac{1}{2! 4}A = \frac{1}{2}\left(f'(0)+\frac{1}{4}A \right)
","['real-analysis', 'derivatives', 'taylor-expansion']"
13,A question about the derivative of trace involving Hadamard product,A question about the derivative of trace involving Hadamard product,,"Assume that $X\in\mathbb{R}^{n\times n}\geq 0$ , $Y\in\mathbb{R}^{n\times k}\geq 0$ and $Z\in\mathbb{R}^{k\times n}\geq 0$ . Let us define the function $f(Y,Z)$ as follows: $$f(Y,Z)=\Vert X-YZ\Vert_W^2:=\mathrm{trace}\left((X-YZ)^T(W\circ(X-YZ))\right),$$ where $\circ$ is the Hadamard product and $W\in\mathbb{R}^{n\times n}\geq 0$ is symmetric. It can be seen that \begin{align*} f(Y,Z)&=\mathrm{trace}\left((X^T-Z^TY^T)(W\circ(X-YZ))\right)\\ &=\mathrm{trace}(X^T(W\circ X))-\mathrm{trace}(X^T(W\circ (YZ)))-\mathrm{trace}(Z^TY^T(W\circ X)) +\mathrm{trace}(Z^TY^T(W\circ (YZ))). \end{align*} Now, according to the fact that $\mathrm{trace}(AB)=\mathrm{trace}(BA)$ , and $\mathrm{trace}(A^T(B\circ C)=trace((A\circ B)^TC)$ , it follows that $$f(Y,Z)=\mathrm{trace}(X^T(W\circ X))-2\mathrm{trace}(Z^TY^T(W\circ X)) +\mathrm{trace}(Z^TY^T(W\circ (YZ))).$$ I have a question about the derivative of $f$ with respect to $Y$ or $Z$ . To this end, we have $$\frac{\partial \mathrm{trace}(Z^TY^T(W\circ X))}{\partial Y}=(W\circ X)Z^T,\quad and\quad \frac{\partial \mathrm{trace}(Z^TY^T(W\circ X))}{\partial Z}=Y^T(W\circ X).$$ What can it be said about the following items? $$\frac{\partial \mathrm{trace}(Z^TY^T(W\circ (YZ)))}{\partial Y},\quad and\quad\frac{\partial \mathrm{trace}(Z^TY^T(W\circ (YZ)))}{\partial Z}.$$ I would be appreciate if someone can help me.","Assume that , and . Let us define the function as follows: where is the Hadamard product and is symmetric. It can be seen that Now, according to the fact that , and , it follows that I have a question about the derivative of with respect to or . To this end, we have What can it be said about the following items? I would be appreciate if someone can help me.","X\in\mathbb{R}^{n\times n}\geq 0 Y\in\mathbb{R}^{n\times k}\geq 0 Z\in\mathbb{R}^{k\times n}\geq 0 f(Y,Z) f(Y,Z)=\Vert X-YZ\Vert_W^2:=\mathrm{trace}\left((X-YZ)^T(W\circ(X-YZ))\right), \circ W\in\mathbb{R}^{n\times n}\geq 0 \begin{align*}
f(Y,Z)&=\mathrm{trace}\left((X^T-Z^TY^T)(W\circ(X-YZ))\right)\\
&=\mathrm{trace}(X^T(W\circ X))-\mathrm{trace}(X^T(W\circ (YZ)))-\mathrm{trace}(Z^TY^T(W\circ X))
+\mathrm{trace}(Z^TY^T(W\circ (YZ))).
\end{align*} \mathrm{trace}(AB)=\mathrm{trace}(BA) \mathrm{trace}(A^T(B\circ C)=trace((A\circ B)^TC) f(Y,Z)=\mathrm{trace}(X^T(W\circ X))-2\mathrm{trace}(Z^TY^T(W\circ X))
+\mathrm{trace}(Z^TY^T(W\circ (YZ))). f Y Z \frac{\partial \mathrm{trace}(Z^TY^T(W\circ X))}{\partial Y}=(W\circ X)Z^T,\quad and\quad \frac{\partial \mathrm{trace}(Z^TY^T(W\circ X))}{\partial Z}=Y^T(W\circ X). \frac{\partial \mathrm{trace}(Z^TY^T(W\circ (YZ)))}{\partial Y},\quad and\quad\frac{\partial \mathrm{trace}(Z^TY^T(W\circ (YZ)))}{\partial Z}.","['matrices', 'derivatives', 'hadamard-product']"
14,Derivative of trace involving Hadamard product,Derivative of trace involving Hadamard product,,"Let us assume that $A, S\in\mathbb{R}^{n\times n}$ , $U\in\mathbb{R}^{n\times k}$ , and $V\in\mathbb{R}^{n\times k}$ . I am trying to differentiate the following expression: $$\Phi(U,V)=\mathrm{trace}\left((S\circ A)(S^T\circ(VU^T))\right),$$ with respect to $U$ and $V$ , respectively, i.e.: $$\frac{\partial \Phi}{\partial U}\quad and\quad \frac{\partial \Phi}{\partial V}.$$ in which $\mathrm{trace}(\cdot)$ is the trace of a matrix, and $\circ$ is the Hadamard product. I appreciate any help.","Let us assume that , , and . I am trying to differentiate the following expression: with respect to and , respectively, i.e.: in which is the trace of a matrix, and is the Hadamard product. I appreciate any help.","A, S\in\mathbb{R}^{n\times n} U\in\mathbb{R}^{n\times k} V\in\mathbb{R}^{n\times k} \Phi(U,V)=\mathrm{trace}\left((S\circ A)(S^T\circ(VU^T))\right), U V \frac{\partial \Phi}{\partial U}\quad and\quad \frac{\partial \Phi}{\partial V}. \mathrm{trace}(\cdot) \circ","['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields', 'hadamard-product']"
15,what is the derivative of $\sec^2x$,what is the derivative of,\sec^2x,"I'm using the quotient rule and so I rewrite $\sec^2x$ as $\frac{1}{\cos^2x}$ Then set $u = 1,$ $u'= 0$ then $v = \cos^2x$ and $v'= -2\sin x \cos x$ I then get $\frac{2\sin x \cos x}{\cos^4x}$ but when I simplify that I don't get $2\sec^2x\tan x$ which is the correct answer? I'm not really sure what I'm doing wrong",I'm using the quotient rule and so I rewrite as Then set then and I then get but when I simplify that I don't get which is the correct answer? I'm not really sure what I'm doing wrong,"\sec^2x \frac{1}{\cos^2x} u = 1, u'= 0 v = \cos^2x v'= -2\sin x \cos x \frac{2\sin x \cos x}{\cos^4x} 2\sec^2x\tan x","['derivatives', 'trigonometry']"
16,About the definition of directional derivative in the clousure of an open set,About the definition of directional derivative in the clousure of an open set,,"Let be $X$ a normed space and thus we prove that if $x_0\in\overset{\circ}Y\subseteq X$ then for any $\vec v\in X$ such that $\lVert\vec v\lVert=1$ the set $$ V:=\{h\in\Bbb R:(x_0+h\vec v)\in\overset{\circ}Y\} $$ is a not empty open set that has $0$ as a cluster point. So first of all we observe that if $x_0\in\overset{\circ}Y$ then $0\in V$ and thus it is not empty. Anyway if $x_0\in\overset{\circ}Y$ then  there exists $\delta>0$ such that $B(x_0,\delta)\subseteq\overset{\circ} Y$ and thus $(x_0+h\vec v)\in \overset{\circ}Y$ for any $h\in(-\delta,\delta)$ so that $0$ is a cluster point of $V$ . Then if $(x_0+h\vec v)\in\overset{\circ}Y$ there exists $\delta>0$ such that $B(x_0+h\vec v,\delta)\subseteq\overset{\circ}Y$ and thus $(x_0+t\vec v)\in\overset{\circ}Y$ for any $t\in(h-\delta,h+\delta)$ so that $(h-\delta,h+\delta)\subseteq V$ and thus this it is open. Now if $\overset{\circ}Y$ is not empty and so that for any $x_0\in\overset{\circ}Y$ there exists $\delta>0$ such that $B(x_0,\delta)\subseteq\overset{\circ}Y$ then $V\neq\emptyset$ for any $\vec v\in V$ such that $\lVert\vec v\lVert=1$ (indeed an open ball is a convex set) and so there exist at least a net in $V$ converging to $0$ so that if $f:Y\rightarrow\Bbb R^n$ is a function then the limit $$ \lim_{h\rightarrow 0}\frac{f(x_0+h\vec v)-f(x)}h $$ is well defined and we call it the deriviative of $f$ in the direction $\vec v$ . Anyway if $x_0\notin\overset{\circ}Y$ it could happen that one of the limits $$ \lim_{h\rightarrow 0^+}\frac{f(x_0+h\vec v)-f(x)}h\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\lim_{h\rightarrow 0^-}\frac{f(x_0+h\vec v)-f(x)}h $$ could be defined and we can think to exted the notion of directional derivative: e.g. the upper half space $H^k:=\{x\in\Bbb R^k:x_k\ge0\}$ is closed and if $x_0\in\partial H^k=\{x\in\Bbb R^k:x_k=0\}$ the limit $$ \lim_{h\rightarrow 0^+}\frac{f(x_0+h\vec e_k)-f(x)}h $$ is well defined for any function $f$ whose domain is the upper half space. So I ask if $x_0$ is a cluster point of $\overset{\circ}Y$ $\Biggl($ and thus $x_0\in\overline{\overset{\circ\,}Y}\Biggl)$ then at least for one $\vec v\in X$ such that $\lVert\vec v\lVert=1$ the set $V$ as above defined is a not empty open set that has $0$ as cluster point so that it is possible to exted the notion of directional derivative in the clousure of an open set. So could someone help me, please?","Let be a normed space and thus we prove that if then for any such that the set is a not empty open set that has as a cluster point. So first of all we observe that if then and thus it is not empty. Anyway if then  there exists such that and thus for any so that is a cluster point of . Then if there exists such that and thus for any so that and thus this it is open. Now if is not empty and so that for any there exists such that then for any such that (indeed an open ball is a convex set) and so there exist at least a net in converging to so that if is a function then the limit is well defined and we call it the deriviative of in the direction . Anyway if it could happen that one of the limits could be defined and we can think to exted the notion of directional derivative: e.g. the upper half space is closed and if the limit is well defined for any function whose domain is the upper half space. So I ask if is a cluster point of and thus then at least for one such that the set as above defined is a not empty open set that has as cluster point so that it is possible to exted the notion of directional derivative in the clousure of an open set. So could someone help me, please?","X x_0\in\overset{\circ}Y\subseteq X \vec v\in X \lVert\vec v\lVert=1 
V:=\{h\in\Bbb R:(x_0+h\vec v)\in\overset{\circ}Y\}
 0 x_0\in\overset{\circ}Y 0\in V x_0\in\overset{\circ}Y \delta>0 B(x_0,\delta)\subseteq\overset{\circ} Y (x_0+h\vec v)\in \overset{\circ}Y h\in(-\delta,\delta) 0 V (x_0+h\vec v)\in\overset{\circ}Y \delta>0 B(x_0+h\vec v,\delta)\subseteq\overset{\circ}Y (x_0+t\vec v)\in\overset{\circ}Y t\in(h-\delta,h+\delta) (h-\delta,h+\delta)\subseteq V \overset{\circ}Y x_0\in\overset{\circ}Y \delta>0 B(x_0,\delta)\subseteq\overset{\circ}Y V\neq\emptyset \vec v\in V \lVert\vec v\lVert=1 V 0 f:Y\rightarrow\Bbb R^n 
\lim_{h\rightarrow 0}\frac{f(x_0+h\vec v)-f(x)}h
 f \vec v x_0\notin\overset{\circ}Y 
\lim_{h\rightarrow 0^+}\frac{f(x_0+h\vec v)-f(x)}h\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\lim_{h\rightarrow 0^-}\frac{f(x_0+h\vec v)-f(x)}h
 H^k:=\{x\in\Bbb R^k:x_k\ge0\} x_0\in\partial H^k=\{x\in\Bbb R^k:x_k=0\} 
\lim_{h\rightarrow 0^+}\frac{f(x_0+h\vec e_k)-f(x)}h
 f x_0 \overset{\circ}Y \Biggl( x_0\in\overline{\overset{\circ\,}Y}\Biggl) \vec v\in X \lVert\vec v\lVert=1 V 0","['calculus', 'general-topology', 'derivatives', 'vector-analysis', 'normed-spaces']"
17,Expressing a indefinite integral using a definite integral with the same function,Expressing a indefinite integral using a definite integral with the same function,,"I was wondering how we can express a indefinite integral $\int f(x) \, dx$ with the function $F(x) = \int_a^x f(t)\,dt$ . I was experimenting with some functions, for example, if $f(x) = 3x^2$ , then \begin{equation} \int f(x) \, dx = \int 3x^2 \, dx = x^3 + C. \end{equation} On the other hand, \begin{equation} F(x) = \int_a^x f(t) \, dt = \int_a^x 3t^2 \, dt = [t^3]^x_a = x^3 - a^3. \end{equation} That means what $F(x)$ and $\int f(x) \,dx$ differs is only by a constant. I was wondering if there is a proof for $F(x) + C = \int f(t)\,dt$ and the proof works for integrals that do not cannot be expressed as an elementary function (e.g. the error function (maybe)) P.S. I was thinking if this has to do with the derivatives of $F$ and integral of $f$ , but if there is no upper and lower limit in the indefinite integral, we cannot use FTC. That's why I do not know how to proceed. Thanks you! Edit: Yes. I have forgotten to mention that $f$ should be a continuous function, in order to make things more neat and to apply FTC. However, I very much thank you all for the responses, especially those who talked about discontinuous functions as well. Cheers.","I was wondering how we can express a indefinite integral with the function . I was experimenting with some functions, for example, if , then On the other hand, That means what and differs is only by a constant. I was wondering if there is a proof for and the proof works for integrals that do not cannot be expressed as an elementary function (e.g. the error function (maybe)) P.S. I was thinking if this has to do with the derivatives of and integral of , but if there is no upper and lower limit in the indefinite integral, we cannot use FTC. That's why I do not know how to proceed. Thanks you! Edit: Yes. I have forgotten to mention that should be a continuous function, in order to make things more neat and to apply FTC. However, I very much thank you all for the responses, especially those who talked about discontinuous functions as well. Cheers.","\int f(x) \, dx F(x) = \int_a^x f(t)\,dt f(x) = 3x^2 \begin{equation}
\int f(x) \, dx = \int 3x^2 \, dx = x^3 + C.
\end{equation} \begin{equation}
F(x) = \int_a^x f(t) \, dt = \int_a^x 3t^2 \, dt = [t^3]^x_a = x^3 - a^3.
\end{equation} F(x) \int f(x) \,dx F(x) + C = \int f(t)\,dt F f f","['calculus', 'integration', 'derivatives', 'definite-integrals', 'indefinite-integrals']"
18,On the generalized Leibniz rule,On the generalized Leibniz rule,,"problem definition I have to evaluate in $z=0$ the $n$ -th derivative with respect $z$ of the product $f(z)\cdot z^k$ , where $f(\cdot)$ is a generic smooth function and $k$ is a given integer. I will use the short notation $F^{(n)}(z)$ to denote the $n$ -th derivative of the generic function $F(z)$ , so what I want is to compute for any $n\in\mathbb{N}$ \begin{equation} [(f(z)\cdot z^k)^{(n)}]_{z=0}\triangleq \frac{\text{d}^n\left[f(z)\cdot z^k\right]}{\text{d}z^n}\Bigg|_{z=0} \tag{1} \end{equation} my attempt For the generalized Leibniz rule , holds for any $n$ \begin{equation}(f(z)\cdot z^k)^{(n)}=\sum_{i=0}^n \binom{n}{i}\cdot f^{(n-i)}(z) \cdot \left(z^k\right)^{(i)}\end{equation} so the problem consist into compute the $i$ -th derivative of the power $z^k$ . If I'm not wrong, \begin{equation} \left(z^k\right)^{(i)}=\begin{cases} \frac{k!}{(k-i)!}z^{k-i} & \text{if } i\leq k\\ 0 & \text{otherwise} \end{cases} \end{equation} This formula says that the index $i$ of the previous summation cannot exceed the value $k$ . Anyway, $k$ is an external parameter and can be greater than $n$ : in this case the summation stops at the value $n$ , otherwise the summation stops at value $k$ . So I would write \begin{equation}\begin{aligned}(f(z)\cdot z^k)^{(n)}&=\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot f^{(n-i)}(z) \cdot  \frac{k!}{(k-i)!} z^{k-i}\\ &=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(z) \cdot z^{k-i}\\ \end{aligned}\end{equation} Now comes the problems. By setting $z=0$ turns out \begin{equation}\begin{aligned} % [(f(z)\cdot z^k)^{(n)}]_{z=0} &=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(0) \cdot 0^{k-i}\\ \end{aligned}\end{equation} from my prospective this expression is quite tricky because of the term $0^{k-i}$ . I'm tempted to write \begin{equation} 0^{k-i}=\begin{cases}1 & \text{if } i=k\\ 0 & \text{otherwise} \end{cases} \end{equation} and consequently simplify the last summation as \begin{equation}\begin{aligned} % [(f(z)\cdot z^k)^{(n)}]_{z=0} &=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(0) \cdot 0^{k-i}\\ &=\begin{cases} k!\cdot\binom{n}{k}\cdot \frac{1}{(k-k)!}\cdot f^{(n-k)}(0) \cdot 0^{k-k} & \text{if } n\geq k \\ 0 & \text{otherwise} \end{cases}\\ &=\begin{cases} \frac{n!}{(n-k)!}\cdot f^{(n-k)}(0) & \text{if } n\geq k \\ 0 & \text{otherwise} \end{cases}\\ \end{aligned}\end{equation} question I don't have a precise question about my problem. Essentially I'm doubtful about my derivation because of the undefined power $0^0$ .","problem definition I have to evaluate in the -th derivative with respect of the product , where is a generic smooth function and is a given integer. I will use the short notation to denote the -th derivative of the generic function , so what I want is to compute for any my attempt For the generalized Leibniz rule , holds for any so the problem consist into compute the -th derivative of the power . If I'm not wrong, This formula says that the index of the previous summation cannot exceed the value . Anyway, is an external parameter and can be greater than : in this case the summation stops at the value , otherwise the summation stops at value . So I would write Now comes the problems. By setting turns out from my prospective this expression is quite tricky because of the term . I'm tempted to write and consequently simplify the last summation as question I don't have a precise question about my problem. Essentially I'm doubtful about my derivation because of the undefined power .","z=0 n z f(z)\cdot z^k f(\cdot) k F^{(n)}(z) n F(z) n\in\mathbb{N} \begin{equation}
[(f(z)\cdot z^k)^{(n)}]_{z=0}\triangleq \frac{\text{d}^n\left[f(z)\cdot z^k\right]}{\text{d}z^n}\Bigg|_{z=0}
\tag{1}
\end{equation} n \begin{equation}(f(z)\cdot z^k)^{(n)}=\sum_{i=0}^n \binom{n}{i}\cdot f^{(n-i)}(z) \cdot \left(z^k\right)^{(i)}\end{equation} i z^k \begin{equation}
\left(z^k\right)^{(i)}=\begin{cases}
\frac{k!}{(k-i)!}z^{k-i} & \text{if } i\leq k\\
0 & \text{otherwise}
\end{cases}
\end{equation} i k k n n k \begin{equation}\begin{aligned}(f(z)\cdot z^k)^{(n)}&=\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot f^{(n-i)}(z) \cdot  \frac{k!}{(k-i)!} z^{k-i}\\
&=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(z) \cdot z^{k-i}\\
\end{aligned}\end{equation} z=0 \begin{equation}\begin{aligned}
%
[(f(z)\cdot z^k)^{(n)}]_{z=0}
&=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(0) \cdot 0^{k-i}\\
\end{aligned}\end{equation} 0^{k-i} \begin{equation}
0^{k-i}=\begin{cases}1 & \text{if } i=k\\
0 & \text{otherwise}
\end{cases}
\end{equation} \begin{equation}\begin{aligned}
%
[(f(z)\cdot z^k)^{(n)}]_{z=0}
&=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(0) \cdot 0^{k-i}\\
&=\begin{cases}
k!\cdot\binom{n}{k}\cdot \frac{1}{(k-k)!}\cdot f^{(n-k)}(0) \cdot 0^{k-k} & \text{if } n\geq k \\
0 & \text{otherwise}
\end{cases}\\
&=\begin{cases}
\frac{n!}{(n-k)!}\cdot f^{(n-k)}(0) & \text{if } n\geq k \\
0 & \text{otherwise}
\end{cases}\\
\end{aligned}\end{equation} 0^0","['calculus', 'derivatives', 'summation', 'binomial-coefficients', 'factorial']"
19,Minimizing costs to make a cylinder,Minimizing costs to make a cylinder,,"A cylinder must have a volume $V=4000$ and its made from just one rectangle sheet, so its top and bottom must be cut from this sheet. I have to find the dimensions of this sheet such that the parts I lost (when I cutoff the top and the bottom) are the lowest. I thought about minimize the difference between the area of the original sheet and the parts I'll use to make my cylinder: (I imagine this is the best way to don't waste sheet - the original problem doesn't have any image). So the area of the original sheet is $$(h+2R)\cdot4R=4hR+8R^2$$ and the area I'll use to make the cylinder is $$4hR+2\cdot\pi R^2$$ so the difference between them is $$8R^2-2\pi R^2=0$$ and derivating it: $$16R-4\pi R$$ which the only root is $R=0$ . What is wrong?","A cylinder must have a volume and its made from just one rectangle sheet, so its top and bottom must be cut from this sheet. I have to find the dimensions of this sheet such that the parts I lost (when I cutoff the top and the bottom) are the lowest. I thought about minimize the difference between the area of the original sheet and the parts I'll use to make my cylinder: (I imagine this is the best way to don't waste sheet - the original problem doesn't have any image). So the area of the original sheet is and the area I'll use to make the cylinder is so the difference between them is and derivating it: which the only root is . What is wrong?",V=4000 (h+2R)\cdot4R=4hR+8R^2 4hR+2\cdot\pi R^2 8R^2-2\pi R^2=0 16R-4\pi R R=0,"['calculus', 'derivatives']"
20,Calculate the gradient and Hessian of $x_0^T(X\operatorname{Diag}(w)X^T)^{-1}x_0$ w.r.t. vector $w$ in matrix calculus?,Calculate the gradient and Hessian of  w.r.t. vector  in matrix calculus?,x_0^T(X\operatorname{Diag}(w)X^T)^{-1}x_0 w,"I'm trying to calculate the Hessian matrix of the formula $$ f(w) = x_0^T(XWX^T)^{-1}x_0 $$ where $w=(w_1,\cdots ,w_n)^T$ , $W=\operatorname{Diag}(w)$ is a diagonal matrix that has $w$ as diagonal elements, $x_0$ is a column vector, $X$ is a matrix. To calculate the first-order derivative, do I need to first transform $W$ into $$\sum_i w^T e_i E_i,$$ where $e_i$ is a column vector whose $i$ -th element is $1$ and others $0$ , and $E_i$ is a matrix whose $(i,i)$ element is $1$ and others $0$ ? Any reference is appreciated.","I'm trying to calculate the Hessian matrix of the formula where , is a diagonal matrix that has as diagonal elements, is a column vector, is a matrix. To calculate the first-order derivative, do I need to first transform into where is a column vector whose -th element is and others , and is a matrix whose element is and others ? Any reference is appreciated.","
f(w) = x_0^T(XWX^T)^{-1}x_0
 w=(w_1,\cdots ,w_n)^T W=\operatorname{Diag}(w) w x_0 X W \sum_i w^T e_i E_i, e_i i 1 0 E_i (i,i) 1 0","['derivatives', 'matrix-calculus']"
21,Derivative of Trig Function with exponent and chain rule logic,Derivative of Trig Function with exponent and chain rule logic,,"I'm new to calculus and self-learning it, I am having trouble grasping why $\sin^2 5x$ can be rewritten as $(\sin 5x)^2$ I fumble when I try to understand the logic behind it. For example, if I plug in a number for 'sin' I would get a different answer when I plugged it in the original equation and when I plugged it into the rewritten equation. Just trying to understand the logic and the 'why', so I don't go through calculus mindlessly solving equations. Thanks.","I'm new to calculus and self-learning it, I am having trouble grasping why can be rewritten as I fumble when I try to understand the logic behind it. For example, if I plug in a number for 'sin' I would get a different answer when I plugged it in the original equation and when I plugged it into the rewritten equation. Just trying to understand the logic and the 'why', so I don't go through calculus mindlessly solving equations. Thanks.",\sin^2 5x (\sin 5x)^2,"['calculus', 'derivatives', 'trigonometry', 'exponential-function']"
22,"Prove that $g(x)=(x^2−1)^x$ is increasing on $(1,+\infty)$",Prove that  is increasing on,"g(x)=(x^2−1)^x (1,+\infty)","Let $A = \mathbb{R}\setminus[−1,1]$ . Let $g : A\to\mathbb{R}$ be defined by $g(x)=(x^2-1)^x$ for all $x\in A$ . Prove that $g(x)=(x^2-1)^x$ is increasing on $(1,\infty)$ I have currently attempted to prove this by showing $g'(x)\geq 0 $ for all $ x\in A $ which gives $g'(x)= (x^2-1)^{x-1}(2x^2+(x^2-1)ln(x^2-1))$ which should be greater than or equal to $0$ however I am unsure how to show this or whether I have gone about this the right way. Any Help will be grateful.",Let . Let be defined by for all . Prove that is increasing on I have currently attempted to prove this by showing for all which gives which should be greater than or equal to however I am unsure how to show this or whether I have gone about this the right way. Any Help will be grateful.,"A = \mathbb{R}\setminus[−1,1] g : A\to\mathbb{R} g(x)=(x^2-1)^x x\in A g(x)=(x^2-1)^x (1,\infty) g'(x)\geq 0   x\in A  g'(x)= (x^2-1)^{x-1}(2x^2+(x^2-1)ln(x^2-1)) 0",['real-analysis']
23,Find the derivative of $f(x)=\text{sin}(x^2)\text{ln}(x)$.,Find the derivative of .,f(x)=\text{sin}(x^2)\text{ln}(x),I have the following task: Find the derivative and state your answer in the simplest form: $$f(x) =\text{sin}(x^2)\text{ln}(x)$$ Here's my attempt: \begin{align} f'(x)&=\frac{d}{dx}\left(\sin \left(x^2\right)\right)\ln \left(x\right)+\frac{d}{dx}\left(\ln \left(x\right)\right)\sin \left(x^2\right) \\ &= \cos \left(x^2\right)\cdot \:2x\ln \left(x\right)+\frac{1}{x}\sin \left(x^2\right) \\ &= 2x\cos \left(x^2\right)\ln \left(x\right)+\frac{\sin \left(x^2\right)}{x} \end{align} I'm pretty sure that the answer is correct but I'm not sure if this is the simplest form. From a glance it does look like it can't be simplified anymore but obviously I could be wrong.,I have the following task: Find the derivative and state your answer in the simplest form: Here's my attempt: I'm pretty sure that the answer is correct but I'm not sure if this is the simplest form. From a glance it does look like it can't be simplified anymore but obviously I could be wrong.,"f(x) =\text{sin}(x^2)\text{ln}(x) \begin{align}
f'(x)&=\frac{d}{dx}\left(\sin \left(x^2\right)\right)\ln \left(x\right)+\frac{d}{dx}\left(\ln \left(x\right)\right)\sin \left(x^2\right) \\
&= \cos \left(x^2\right)\cdot \:2x\ln \left(x\right)+\frac{1}{x}\sin \left(x^2\right) \\
&= 2x\cos \left(x^2\right)\ln \left(x\right)+\frac{\sin \left(x^2\right)}{x}
\end{align}","['real-analysis', 'calculus', 'derivatives']"
24,"Difficulties solving this integral: $ \int_0^1 \frac{\ln(x+1)} {x^2 + 1} \, \mathrm{d}x $ by differentiation under the integral sign",Difficulties solving this integral:  by differentiation under the integral sign," \int_0^1 \frac{\ln(x+1)} {x^2 + 1} \, \mathrm{d}x ","So in the book Advanced Calculus Explored, by Hamza E. Asamraee. The next integral appears as an exercise to solve by differentiating under the integral sign: $$ \int_0^1 \frac{\ln(x+1)} {x^2 + 1} \, \mathrm{d}x $$ I have solved this integral before by substitution and change in the limits of integration, but in this chapter the book asks to solve it by differentiation under the integral sign. I have tried several ways of solving this, but the only one that i thought it was leading me somewhere was: $$f(a) = \int_0^1 \frac{\ln(x+a)} {x^2 + 1} \, \mathrm{d}x $$ So that: $$f'(a) = \int_0^1 \frac{1} {(x+a)(x^2 + 1)} \, \mathrm{d}x $$ Then i tried to separate this last integral by partial fractions, my result on this was: $$\frac {1} {(x+a)(x^2 + 1)} = \frac{1} {a^2 + 1} \left(\frac {1} {x+a} - \frac{x-a} {x^2+1}\right)$$ And the integral reduces to: $$f'(a) = \int_0^1 \frac{1} {a^2 + 1} \left(\frac {1} {x+a} - \frac{x-a} {x^2+1} \right) \, \mathrm{d}x $$ Then this last expression evaluates to: $$f'(a) = \frac{1} {a^2 + 1} (\ln(a+1) - \ln(a) - \ln(4)+ \frac{π}{4} a)$$ Then integrating from 0 to 1 with respect to $a$ we will get: $$f(1) - f(0) = \int_0^1 \frac{\ln(a+1)} {a^2 + 1} \, \mathrm{d}a - \int_0^1 \frac{\ln(a)} {a^2 + 1} \, \mathrm{d}a $$ (The last two terms of $f'(a)$ cancel each other after the integration so i didn't wrote them) But then the two integrals on the right hand side are equal to $f(1) - f(0)$ so the differentiation under the integral led nowhere. Do i need some other approach? Or did i made any mistake? Any help is appreciated.","So in the book Advanced Calculus Explored, by Hamza E. Asamraee. The next integral appears as an exercise to solve by differentiating under the integral sign: I have solved this integral before by substitution and change in the limits of integration, but in this chapter the book asks to solve it by differentiation under the integral sign. I have tried several ways of solving this, but the only one that i thought it was leading me somewhere was: So that: Then i tried to separate this last integral by partial fractions, my result on this was: And the integral reduces to: Then this last expression evaluates to: Then integrating from 0 to 1 with respect to we will get: (The last two terms of cancel each other after the integration so i didn't wrote them) But then the two integrals on the right hand side are equal to so the differentiation under the integral led nowhere. Do i need some other approach? Or did i made any mistake? Any help is appreciated."," \int_0^1 \frac{\ln(x+1)} {x^2 + 1} \, \mathrm{d}x  f(a) = \int_0^1 \frac{\ln(x+a)} {x^2 + 1} \, \mathrm{d}x  f'(a) = \int_0^1 \frac{1} {(x+a)(x^2 + 1)} \, \mathrm{d}x  \frac {1} {(x+a)(x^2 + 1)} = \frac{1} {a^2 + 1} \left(\frac {1} {x+a} - \frac{x-a} {x^2+1}\right) f'(a) = \int_0^1 \frac{1} {a^2 + 1} \left(\frac {1} {x+a} - \frac{x-a} {x^2+1} \right) \, \mathrm{d}x  f'(a) = \frac{1} {a^2 + 1} (\ln(a+1) - \ln(a) - \ln(4)+ \frac{π}{4} a) a f(1) - f(0) = \int_0^1 \frac{\ln(a+1)} {a^2 + 1} \, \mathrm{d}a - \int_0^1 \frac{\ln(a)} {a^2 + 1} \, \mathrm{d}a  f'(a) f(1) - f(0)","['calculus', 'integration']"
25,Behavior of Legendre polynomials $P_\ell(\cos\theta)$ under $\theta\to\pi-\theta$,Behavior of Legendre polynomials  under,P_\ell(\cos\theta) \theta\to\pi-\theta,"I'm studying some notes on the hydrogen atom which define the Legendre polynomials via Rodrigues's formula, $$P_\ell(z)=\frac{1}{2^\ell \ell!}\frac{d^\ell}{dz^\ell}(z^2-1)^\ell,\quad z=\cos\theta.$$ I'm trying to figure out what happens when $\theta\to\pi-\theta$ or equivalently, $z\to-z$ . By computing some of the lowest order Legendre polynomials I believe I've convinced myself that $P_\ell(-z)=(-1)^\ell P_\ell(z)$ , but I don't see how I can prove this given the above formula, i.e. I don't see what exactly will happen to the $\ell$ -th order derivative. Is it mathematically meaningful to write $$P_\ell(-z)=\frac{1}{2^\ell \ell!}\frac{d^\ell}{d(-z)^\ell}[(-z)^2-1]^\ell?$$","I'm studying some notes on the hydrogen atom which define the Legendre polynomials via Rodrigues's formula, I'm trying to figure out what happens when or equivalently, . By computing some of the lowest order Legendre polynomials I believe I've convinced myself that , but I don't see how I can prove this given the above formula, i.e. I don't see what exactly will happen to the -th order derivative. Is it mathematically meaningful to write","P_\ell(z)=\frac{1}{2^\ell \ell!}\frac{d^\ell}{dz^\ell}(z^2-1)^\ell,\quad z=\cos\theta. \theta\to\pi-\theta z\to-z P_\ell(-z)=(-1)^\ell P_\ell(z) \ell P_\ell(-z)=\frac{1}{2^\ell \ell!}\frac{d^\ell}{d(-z)^\ell}[(-z)^2-1]^\ell?","['calculus', 'derivatives', 'legendre-polynomials']"
26,Stronger version of Taylor's Theorem,Stronger version of Taylor's Theorem,,"Show that Taylor's Theorem may be strengthened as follows: Let $f$ be a continuous real-valued function on the closed interval in R of extremities $a$ and $b$ . That is, (n + 1) times differentiable on the open interval with these same extremities and suppose that $\lim_{x \to a} f'(x), \lim_{x \to a} f''(x), \dots, \lim_{x \to a} f^{(n)}(x)$ exist and that $f',f'',\dots,f^{(n)}$ are bounded. Then, $$f(b) = f(a) + (\lim_{x \to a} f'(x)) \frac{(b - a)}{1!} + \dots | (\lim_{x \to a} f^{(n)}(x)) \frac{(b - a)^n}{n!} + f^{(n + 1)}(c) \frac{(b - a)^{n + 1}}{(n + 1)!}$$ for some c between a and b. I'm not too familiar with Taylor's Theorem in an Analysis sense. We briefly talked about it in class, but we sort of moved on quickly. This was said to be a ""cool problem"" to complete, so I would like to see it. Can anyone help me with this one? Thank you!","Show that Taylor's Theorem may be strengthened as follows: Let be a continuous real-valued function on the closed interval in R of extremities and . That is, (n + 1) times differentiable on the open interval with these same extremities and suppose that exist and that are bounded. Then, for some c between a and b. I'm not too familiar with Taylor's Theorem in an Analysis sense. We briefly talked about it in class, but we sort of moved on quickly. This was said to be a ""cool problem"" to complete, so I would like to see it. Can anyone help me with this one? Thank you!","f a b \lim_{x \to a} f'(x), \lim_{x \to a} f''(x), \dots, \lim_{x \to a} f^{(n)}(x) f',f'',\dots,f^{(n)} f(b) = f(a) + (\lim_{x \to a} f'(x)) \frac{(b - a)}{1!} + \dots | (\lim_{x \to a} f^{(n)}(x)) \frac{(b - a)^n}{n!} + f^{(n + 1)}(c) \frac{(b - a)^{n + 1}}{(n + 1)!}","['real-analysis', 'derivatives', 'taylor-expansion']"
27,Minimization of iron costs: financial mathematics for an high school,Minimization of iron costs: financial mathematics for an high school,,"Meanwhile I apologize to everyone. I have never treated financial mathematics. A colleague of mine asked me for help but I can't give an answer. I have two exercise. This is the first. A company operates in the iron sector, and sustains the following fixed costs equal to $ 648 \, €$ for month. A variable cost of $4$ € euros per quintal produced. An additional cost determined by the use of a particular ironworking machinery, equal to half of the square of the quintals produced. What are the function of the total monthly cost and the function of the average cost? What is the production at which you have the lowest average cost and what is the total monthly cost at this quantity? The results are: $$a) \quad C=0.5q^2+4q+648, \quad C_m=0.5q+\frac{648}q+4$$ $$b)\quad  q=36\, \frac{\text{quintals}}{\text{month}};\quad  C(36)=€\, 1440$$ My observation : with the first informations of the problem I have a parabola. Why? For the rest of the questions I think that there are some derivative to be done. Never studied financial mathematics. Thank you all for your understanding.","Meanwhile I apologize to everyone. I have never treated financial mathematics. A colleague of mine asked me for help but I can't give an answer. I have two exercise. This is the first. A company operates in the iron sector, and sustains the following fixed costs equal to for month. A variable cost of € euros per quintal produced. An additional cost determined by the use of a particular ironworking machinery, equal to half of the square of the quintals produced. What are the function of the total monthly cost and the function of the average cost? What is the production at which you have the lowest average cost and what is the total monthly cost at this quantity? The results are: My observation : with the first informations of the problem I have a parabola. Why? For the rest of the questions I think that there are some derivative to be done. Never studied financial mathematics. Thank you all for your understanding."," 648 \, € 4 a) \quad C=0.5q^2+4q+648, \quad C_m=0.5q+\frac{648}q+4 b)\quad  q=36\, \frac{\text{quintals}}{\text{month}};\quad  C(36)=€\, 1440","['derivatives', 'optimization', 'finance', 'economics']"
28,Differentiablity of a function $f(z)$ in the complex vs reals,Differentiablity of a function  in the complex vs reals,f(z),"I am quite confused about the differentiability of a function in the complex plane vs the real numbers. Consider the function $f(z) = |z|^2$ for $z \in \mathbb{C}$ . Using the definition of a derivative, and taking the limit as $\Delta z \rightarrow 0$ , we can see that it is only differentiable at $z = 0$ . Elsewhere, the limits are not unique, so do not exist. However, if we consider a similar function $F(x) = |x|^2$ for $x \in \mathbb{R}$ , then $F'(x) = 2x$ and exists everywhere in $\mathbb{R}$ . We know that $\mathbb{C}$ is a field extension of $\mathbb{R}$ , so $\mathbb{R} \subset \mathbb{C}$ . The domain of $f(x)$ is then a subset of the domain of $f(z)$ . Why isn't $f(z)$ instead differentiable for all $z = x + i0$ ? Likewise for a similar function, $g(z) = \operatorname{Re}(z)$ . $g(z)$ is nowhere differentiable in $\mathbb{C}$ , yet $G(x) = x$ is differentiable everywhere in $\mathbb{R}$ . Why is this the case? Is it due to the limit being used in the definition of a derivative? That the limit requires that $|f(z) - f(z_0)| < \epsilon$ , which means that the limit of $f(z)$ must approach $f(z_0)$ , as $z \rightarrow z_0$ , independent of the direction for the limit to exist. If we were only working with the reals, the limit as $x \rightarrow x_0^+$ must equal that of $x \rightarrow x_0^-$ for the limit to exist, but in the complex plane, the limit must be equal as $z \rightarrow z_0$ from all sides of $z_0$ , which is why the derivative for a function in $\mathbb{R}$ can exist, but not a similar one in $\mathbb{C}$ ? Does this mean that given two similar functions $f(z)$ and $F(x)$ acting on both $\mathbb{C}$ and $\mathbb{R}$ , respectively. Let $f(z=x+iy) = u(x,y) + iv(x,y)$ , $u(x,y) = F(x)$ , and $z_0 = x_0 + iy_0$ , then $$f(z) \text{ differentiable at point } z_0 \in \mathbb{C} \text{ and } u'(z) \text{ exists at } z_0 \implies F(x) \text{ differentiable at }  x_0 \in \mathbb{R}?$$","I am quite confused about the differentiability of a function in the complex plane vs the real numbers. Consider the function for . Using the definition of a derivative, and taking the limit as , we can see that it is only differentiable at . Elsewhere, the limits are not unique, so do not exist. However, if we consider a similar function for , then and exists everywhere in . We know that is a field extension of , so . The domain of is then a subset of the domain of . Why isn't instead differentiable for all ? Likewise for a similar function, . is nowhere differentiable in , yet is differentiable everywhere in . Why is this the case? Is it due to the limit being used in the definition of a derivative? That the limit requires that , which means that the limit of must approach , as , independent of the direction for the limit to exist. If we were only working with the reals, the limit as must equal that of for the limit to exist, but in the complex plane, the limit must be equal as from all sides of , which is why the derivative for a function in can exist, but not a similar one in ? Does this mean that given two similar functions and acting on both and , respectively. Let , , and , then","f(z) = |z|^2 z \in \mathbb{C} \Delta z \rightarrow 0 z = 0 F(x) = |x|^2 x \in \mathbb{R} F'(x) = 2x \mathbb{R} \mathbb{C} \mathbb{R} \mathbb{R} \subset \mathbb{C} f(x) f(z) f(z) z = x + i0 g(z) = \operatorname{Re}(z) g(z) \mathbb{C} G(x) = x \mathbb{R} |f(z) - f(z_0)| < \epsilon f(z) f(z_0) z \rightarrow z_0 x \rightarrow x_0^+ x \rightarrow x_0^- z \rightarrow z_0 z_0 \mathbb{R} \mathbb{C} f(z) F(x) \mathbb{C} \mathbb{R} f(z=x+iy) = u(x,y) + iv(x,y) u(x,y) = F(x) z_0 = x_0 + iy_0 f(z) \text{ differentiable at point } z_0 \in \mathbb{C} \text{ and } u'(z) \text{ exists at } z_0 \implies F(x) \text{ differentiable at } 
x_0 \in \mathbb{R}?","['real-analysis', 'complex-analysis', 'derivatives']"
29,"f is continuous on [0,1]. f(0) = f(1). Show that f'(1 - c) = f'(c)","f is continuous on [0,1]. f(0) = f(1). Show that f'(1 - c) = f'(c)",,"Let $f$ be continuous on $[0,1]$ and differentiable in $(0,1)$ s.t. $f(0) = f(1)$ . Prove that $\exists c \ where \ 0 < c < 1 \ s.t. \ f'(1 - c) = f'(c)$ . Here is my attempt Since $f(0) = f(1)$ , Rolle's Theorem applies. This means $\exists c \in (0,1) \ s.t. \ f'(c) = 0$ Since $c \in (0,1)$ , we can see that $1 - c \in (0,1)$ . To show this is also $0$ , examine the original function. If $0 = 1 - c$ , then we see $c = 1$ . This will satisfy the inequality giving us $f(0) = f(1)$ . If $1 = 1 - c$ , then $c = 0$ . This will also satisfy the inequality giving us $f(1) = f(0)$ . So whether we use $c$ or $1 - c$ , Rolle's Theorem will be satisfied at both points. I was told this is incorrect, and I can see why. Where I try to show that $1 - c$ works as well, I am only given information about the function (f(0) = f(1)). Not the derivative. So I can see why this doesn't work. I was told as a hint to examine the function $g(x) = f(x) + f(1 - x)$ . However, I am not so sure how to apply this to the problem and proceed. Can anyone give me some guidance?","Let be continuous on and differentiable in s.t. . Prove that . Here is my attempt Since , Rolle's Theorem applies. This means Since , we can see that . To show this is also , examine the original function. If , then we see . This will satisfy the inequality giving us . If , then . This will also satisfy the inequality giving us . So whether we use or , Rolle's Theorem will be satisfied at both points. I was told this is incorrect, and I can see why. Where I try to show that works as well, I am only given information about the function (f(0) = f(1)). Not the derivative. So I can see why this doesn't work. I was told as a hint to examine the function . However, I am not so sure how to apply this to the problem and proceed. Can anyone give me some guidance?","f [0,1] (0,1) f(0) = f(1) \exists c \ where \ 0 < c < 1 \ s.t. \ f'(1 - c) = f'(c) f(0) = f(1) \exists c \in (0,1) \ s.t. \ f'(c) = 0 c \in (0,1) 1 - c \in (0,1) 0 0 = 1 - c c = 1 f(0) = f(1) 1 = 1 - c c = 0 f(1) = f(0) c 1 - c 1 - c g(x) = f(x) + f(1 - x)","['real-analysis', 'derivatives', 'rolles-theorem']"
30,Solving for a differential equation Gompertz growth equation,Solving for a differential equation Gompertz growth equation,,"What is the general solution of this differential equation? $$ \frac{dy}{dt} = k \enspace  y \enspace \ln(\frac{a}{y})$$ where $a$ and $k$ are positive constants. So far, my solution is: $$ \frac{dy}{y \enspace \ln(\frac{a}{y})} = k \enspace dt$$ When I let $u=lny$ , $$ \int \frac{1}{y(\ln a-\ln y)} \,dy =\int k dt $$ $$ \int \frac{1}{\ln a-u} \,du= kt + C $$ How to continue this?","What is the general solution of this differential equation? where and are positive constants. So far, my solution is: When I let , How to continue this?"," \frac{dy}{dt} = k \enspace  y \enspace \ln(\frac{a}{y}) a k  \frac{dy}{y \enspace \ln(\frac{a}{y})} = k \enspace dt u=lny  \int \frac{1}{y(\ln a-\ln y)} \,dy =\int k dt   \int \frac{1}{\ln a-u} \,du= kt + C ","['calculus', 'integration']"
31,If $f$ is a function differentiable at $a$ find: $\underset{h\rightarrow 0}{\lim} \frac{f(a+7h)-f(a-9h^2)}{h}$,If  is a function differentiable at  find:,f a \underset{h\rightarrow 0}{\lim} \frac{f(a+7h)-f(a-9h^2)}{h},If $f$ is a function differentiable at $a$ find: $\underset{h\rightarrow 0}{\lim} \frac{f(a+7h)-f(a-9h^2)}{h}$ I am struggling to understand what to do. I tried brute forcing this question and I get $\frac{f(a)-f(a)}{0}$ which makes no sense to me. Any clarification would be appreciated.,If is a function differentiable at find: I am struggling to understand what to do. I tried brute forcing this question and I get which makes no sense to me. Any clarification would be appreciated.,f a \underset{h\rightarrow 0}{\lim} \frac{f(a+7h)-f(a-9h^2)}{h} \frac{f(a)-f(a)}{0},['derivatives']
32,"Can a function be differentiable everywhere on its domain, but not Lipschitz on its domain?","Can a function be differentiable everywhere on its domain, but not Lipschitz on its domain?",,"If we suppose that a function is differentiable at every point on its domain (but place no more restrictions than that), does it follow that the function is Lipschitz on its domain? I think that it does not, and I suspect that it is because we do not require that the function is continuously differentiable. But I'm not sure how to prove my thought - specifically, I can't come up with a counterexample. If I am right that it is not Lipschitz, can we say if it will be locally Lipschitz at every point?","If we suppose that a function is differentiable at every point on its domain (but place no more restrictions than that), does it follow that the function is Lipschitz on its domain? I think that it does not, and I suspect that it is because we do not require that the function is continuously differentiable. But I'm not sure how to prove my thought - specifically, I can't come up with a counterexample. If I am right that it is not Lipschitz, can we say if it will be locally Lipschitz at every point?",,"['real-analysis', 'derivatives', 'lipschitz-functions']"
33,How to properly work with Leibniz notation,How to properly work with Leibniz notation,,"This question regards the manipulation of derivatives as if they were fractions. But more generally it also regards doing calculus in a ""Leibnizian"" way. Before asking I checked out the current poll of question regarding this topic: link , link and another one for good measure; my objective with this question is to fill in the holes that I think are left open in the discussion of this topic on this site. In the cited questions is clarified why is incorrect to define the derivate not as a limit but as a fraction, and the consequent importance of knowing the proper definition of the derivative as a limit. This is clear. But when working whit derivatives and integrals I think the usefulness of working with Leibniz notation, and interpreting derivatives as fractions, is beyond any doubt, as this answer points out. Problem is: when working with imprecise assumptions things can go wrong really fast, and for this reason interpreting the derivative as a fraction has the nomea of a pretty dangerous gamble. Here are some examples on how this can go terribly wrong: Example 1: $$\int \nabla \phi \cdot d\vec{x}=\int \frac{d\phi}{dx}dx+\frac{d\phi}{dy}dy+\frac{d\phi}{dz}dz=\int \frac{d\phi}{dx}dx+\int\frac{d\phi}{dy}dy+\int\frac{d\phi}{dz}dz=\phi+\phi+\phi=3\phi$$ but on the other hand if we state that: $d\phi+d\phi+d\phi=d\phi$ we get the correct result: $$\int \nabla \phi \cdot d\vec{x}=\int \frac{d\phi}{dx}dx+\frac{d\phi}{dy}dy+\frac{d\phi}{dz}dz=\int d\phi+d\phi+d\phi=\int d\phi=\phi$$ But if we write it with the proper notation of partial derivative then we should elide $\partial x$ with $dx$ , even more chaos. Example 2: $$\frac{\partial \phi}{\partial x}/\frac{\partial \phi}{\partial y}=\frac{\partial y}{\partial x}$$ by ""eliding the $\partial \phi$ "". And I am sure I could go on with more example, but I think there is no need. Question is: is there a ruleset to follow to ensure to not fall for mistakes like this when working with Leibniz notation? I feel like this question hasn't been properly answered in the previous discussion on this topic. And if there is such ruleset, why it works? Also it's often said that this way of handling derivatives and integrals no longer works when dealing with multivariable calculus, but from my experience it seems to give the correct answer even in this case, what is up with this exactly? Why should it no longer work? For example let's take the integral: $$\int \frac{\partial \vec{A}}{\partial t} \cdot d\vec{x}$$ It would seem a bit difficult to solve this integral ""rigorously"", but by interpreting it in a somewhat Leibnizian way we get: $$\int \frac{\partial \vec{A}}{\partial t} \cdot d\vec{x}=\int \frac{\partial A_x}{\partial t}dx+\frac{\partial A_y}{\partial t}dy+\frac{\partial A_z}{\partial t}dz=\int dA_x \frac{dx}{dt}+dA_y\frac{dy}{dt}+dA_z\frac{dz}{dt}=\int \vec{v} \cdot d\vec{A}=\vec{v}\cdot \vec{A}$$ This seems like a miracle, should we not use this way of working with things like this?","This question regards the manipulation of derivatives as if they were fractions. But more generally it also regards doing calculus in a ""Leibnizian"" way. Before asking I checked out the current poll of question regarding this topic: link , link and another one for good measure; my objective with this question is to fill in the holes that I think are left open in the discussion of this topic on this site. In the cited questions is clarified why is incorrect to define the derivate not as a limit but as a fraction, and the consequent importance of knowing the proper definition of the derivative as a limit. This is clear. But when working whit derivatives and integrals I think the usefulness of working with Leibniz notation, and interpreting derivatives as fractions, is beyond any doubt, as this answer points out. Problem is: when working with imprecise assumptions things can go wrong really fast, and for this reason interpreting the derivative as a fraction has the nomea of a pretty dangerous gamble. Here are some examples on how this can go terribly wrong: Example 1: but on the other hand if we state that: we get the correct result: But if we write it with the proper notation of partial derivative then we should elide with , even more chaos. Example 2: by ""eliding the "". And I am sure I could go on with more example, but I think there is no need. Question is: is there a ruleset to follow to ensure to not fall for mistakes like this when working with Leibniz notation? I feel like this question hasn't been properly answered in the previous discussion on this topic. And if there is such ruleset, why it works? Also it's often said that this way of handling derivatives and integrals no longer works when dealing with multivariable calculus, but from my experience it seems to give the correct answer even in this case, what is up with this exactly? Why should it no longer work? For example let's take the integral: It would seem a bit difficult to solve this integral ""rigorously"", but by interpreting it in a somewhat Leibnizian way we get: This seems like a miracle, should we not use this way of working with things like this?",\int \nabla \phi \cdot d\vec{x}=\int \frac{d\phi}{dx}dx+\frac{d\phi}{dy}dy+\frac{d\phi}{dz}dz=\int \frac{d\phi}{dx}dx+\int\frac{d\phi}{dy}dy+\int\frac{d\phi}{dz}dz=\phi+\phi+\phi=3\phi d\phi+d\phi+d\phi=d\phi \int \nabla \phi \cdot d\vec{x}=\int \frac{d\phi}{dx}dx+\frac{d\phi}{dy}dy+\frac{d\phi}{dz}dz=\int d\phi+d\phi+d\phi=\int d\phi=\phi \partial x dx \frac{\partial \phi}{\partial x}/\frac{\partial \phi}{\partial y}=\frac{\partial y}{\partial x} \partial \phi \int \frac{\partial \vec{A}}{\partial t} \cdot d\vec{x} \int \frac{\partial \vec{A}}{\partial t} \cdot d\vec{x}=\int \frac{\partial A_x}{\partial t}dx+\frac{\partial A_y}{\partial t}dy+\frac{\partial A_z}{\partial t}dz=\int dA_x \frac{dx}{dt}+dA_y\frac{dy}{dt}+dA_z\frac{dz}{dt}=\int \vec{v} \cdot d\vec{A}=\vec{v}\cdot \vec{A},"['calculus', 'integration', 'derivatives', 'notation', 'partial-derivative']"
34,Definition of $k$-times differentiable on $S$?,Definition of -times differentiable on ?,k S,"I'm trying to come up with a good definition of being $k$ -times differentiable on a subset $S$ . Here's the definitions I'm working with. Let $f$ be a function from $X\subseteq\mathbb{R}$ to $\mathbb{R}$ . Let $L\in\mathbb{R}$ . If $x_0\in X$ is a limit point of $X$ , we say that $f$ has derivative $L$ at $x_0$ iff $\lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}=L$ , i.e. iff $$\forall\epsilon>0,\exists\delta>0,0<|x-x_0|<\delta\implies\left\lvert\frac{f(x)-f(x_0)}{x-x_0}-L\right\rvert<\epsilon.$$ If $S$ is a subset of $X$ , we say that $f$ is differentiable on $S$ iff the derivative exists at $x_0$ for all $x_0\in S$ (in particular, every point of $S$ must be a limit point of $X$ ). We say that $f$ is differentiable iff it is differentiable on its whole domain. Are these definitions standard so far? Now the definition I've come upon for "" $k$ -times differentiable"" is as follows. Let $f$ be a function from $X\subseteq\mathbb{R}$ to $\mathbb{R}$ . We say that $f$ is $1$ -times differentiable iff $f$ is differentiable, and the first derivative of $f$ is the function $f^{'}:X\to\mathbb{R}$ , also denoted $f^{(1)}$ , defined in the obvious way. For $k\geq 1$ , we say that $f$ is $(k+1)$ -times differentiable iff $f$ is $k$ -times differentiable and $f^{(k)}$ is differentiable, and in this case the $(k+1)$ th derivative is the function $f^{(k+1)}:= (f^{(k)})': X\to\mathbb{R}$ . A function is said to be infinitely differentiable iff it is $k$ -times differentiable for all $k$ . Seem fine so far? But then I thought, how would I say that a function $f$ is $k$ -times differentiable on some subset of its domain? Well, since we want "" $f$ is $1$ -times differentiable"" to mean the same as "" $f$ is differentiable"", we might want "" $f$ is $1$ -times differentiable on $S$ "" to mean the same as "" $f$ is differentiable on $S$ "". How would you define "" $f$ is $k$ -times differentiable on $S$ "" ?","I'm trying to come up with a good definition of being -times differentiable on a subset . Here's the definitions I'm working with. Let be a function from to . Let . If is a limit point of , we say that has derivative at iff , i.e. iff If is a subset of , we say that is differentiable on iff the derivative exists at for all (in particular, every point of must be a limit point of ). We say that is differentiable iff it is differentiable on its whole domain. Are these definitions standard so far? Now the definition I've come upon for "" -times differentiable"" is as follows. Let be a function from to . We say that is -times differentiable iff is differentiable, and the first derivative of is the function , also denoted , defined in the obvious way. For , we say that is -times differentiable iff is -times differentiable and is differentiable, and in this case the th derivative is the function . A function is said to be infinitely differentiable iff it is -times differentiable for all . Seem fine so far? But then I thought, how would I say that a function is -times differentiable on some subset of its domain? Well, since we want "" is -times differentiable"" to mean the same as "" is differentiable"", we might want "" is -times differentiable on "" to mean the same as "" is differentiable on "". How would you define "" is -times differentiable on "" ?","k S f X\subseteq\mathbb{R} \mathbb{R} L\in\mathbb{R} x_0\in X X f L x_0 \lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}=L \forall\epsilon>0,\exists\delta>0,0<|x-x_0|<\delta\implies\left\lvert\frac{f(x)-f(x_0)}{x-x_0}-L\right\rvert<\epsilon. S X f S x_0 x_0\in S S X f k f X\subseteq\mathbb{R} \mathbb{R} f 1 f f f^{'}:X\to\mathbb{R} f^{(1)} k\geq 1 f (k+1) f k f^{(k)} (k+1) f^{(k+1)}:= (f^{(k)})': X\to\mathbb{R} k k f k f 1 f f 1 S f S f k S","['real-analysis', 'derivatives', 'definition']"
35,Is this condition equivalent to being differentiable?,Is this condition equivalent to being differentiable?,,"Consider a function $f : U \subset \mathbb R^n \to \mathbb R$ and the following two assertions: $f$ is differentiable at $p \in U$ , i.e., there exists a linear map $df_p : \mathbb R^n \to \mathbb R$ such that $$\lim_{v \to 0} \frac {f(p + v) - f(p) - df_p(v)} {\Vert v \Vert} = 0$$ $f$ is continuous at $p$ , and there exists a linear map $df_p : \mathbb R^n \to \mathbb R$ such that $$df_p(v) = \lim_{t \to 0} \frac {f(p + tv) - f(p)} t$$ Clearly, the first assertion implies the second. Does the converse also hold?","Consider a function and the following two assertions: is differentiable at , i.e., there exists a linear map such that is continuous at , and there exists a linear map such that Clearly, the first assertion implies the second. Does the converse also hold?",f : U \subset \mathbb R^n \to \mathbb R f p \in U df_p : \mathbb R^n \to \mathbb R \lim_{v \to 0} \frac {f(p + v) - f(p) - df_p(v)} {\Vert v \Vert} = 0 f p df_p : \mathbb R^n \to \mathbb R df_p(v) = \lim_{t \to 0} \frac {f(p + tv) - f(p)} t,"['real-analysis', 'derivatives']"
36,Integration as inverse of differentiation,Integration as inverse of differentiation,,"I have been reading ""Mathematical Methods of Physics and Engineering"" By K. F. Riley, M. P. Hobson and S. J. Bence. In chapter $2$ of this book (Preliminary Calculus) Under the sub-topic $2.2.2$ ""Integration as the inverse of differentiation"", I'm having trouble understanding the following proof: $$ \text{Let } F(x)=\int_a^xf(u) \mathrm{d}u \ .\text{We then have:}$$ \begin{align}  F(x+\Delta x)&=\int_a^{x+\Delta x}\ f(u) \mathrm{d}u\\[1ex]               &=\int_a^x f(u)\ \mathrm{d}u\ + \ \int_x^{x+\Delta x} f(u)\ \mathrm{d}u\\[1ex]  F(x+\Delta x)&= F(x)+\int_x^{x+\Delta x} f(u)\  \mathrm{d}u\\[1ex] \implies\frac{F(x+\Delta x)-F(x)}{\Delta x}&=\frac{1}{\Delta x}\int_x^{x+\Delta x}f(u) \ \mathrm{d}u \end{align} Letting $\Delta x \to 0$ the LHS becomes $\frac{\mathrm{d}F}{\mathrm{d}x}$ whereas the RHS becomes $f(x)$ . The latter conclusion follows because when $\Delta x$ is small the value of the integral on the RHS approximately becomes $f(x)\Delta x$ , and in the limit $\Delta x \to 0$ no approximation is involved. I didn't understand the last statement. How did we approximate the integral?","I have been reading ""Mathematical Methods of Physics and Engineering"" By K. F. Riley, M. P. Hobson and S. J. Bence. In chapter of this book (Preliminary Calculus) Under the sub-topic ""Integration as the inverse of differentiation"", I'm having trouble understanding the following proof: Letting the LHS becomes whereas the RHS becomes . The latter conclusion follows because when is small the value of the integral on the RHS approximately becomes , and in the limit no approximation is involved. I didn't understand the last statement. How did we approximate the integral?","2 2.2.2  \text{Let } F(x)=\int_a^xf(u) \mathrm{d}u \ .\text{We then have:} \begin{align}
 F(x+\Delta x)&=\int_a^{x+\Delta x}\ f(u) \mathrm{d}u\\[1ex]
              &=\int_a^x f(u)\ \mathrm{d}u\ + \ \int_x^{x+\Delta x} f(u)\ \mathrm{d}u\\[1ex]
 F(x+\Delta x)&= F(x)+\int_x^{x+\Delta x} f(u)\  \mathrm{d}u\\[1ex]
\implies\frac{F(x+\Delta x)-F(x)}{\Delta x}&=\frac{1}{\Delta x}\int_x^{x+\Delta x}f(u) \ \mathrm{d}u
\end{align} \Delta x \to 0 \frac{\mathrm{d}F}{\mathrm{d}x} f(x) \Delta x f(x)\Delta x \Delta x \to 0","['calculus', 'integration', 'derivatives']"
37,Question about differential notation ($\partial$ and $d$),Question about differential notation ( and ),\partial d,"My question is if, given $f(x)$ , it's correct to write: $$\frac{\partial f(x)}{\partial x}$$ instead of $$\frac{d f(x)}{d x}.$$ Is it incorrect to use $\partial$ in one-variable expressions? Or it just doesn't really matter.","My question is if, given , it's correct to write: instead of Is it incorrect to use in one-variable expressions? Or it just doesn't really matter.",f(x) \frac{\partial f(x)}{\partial x} \frac{d f(x)}{d x}. \partial,"['real-analysis', 'derivatives', 'notation', 'partial-derivative']"
38,Find the maximum area of an isosceles triangle inscribed in the ellipse $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1.$,Find the maximum area of an isosceles triangle inscribed in the ellipse,\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1.,"Here's the question- Find the maximum area of an isosceles triangle inscribed in the ellipse $x^2/a^2 + y^2/b^2 = 1$ . My teacher solved it by considering two arbitrary points on the ellipse to be vertices of the triangle, being $(a\cos\theta, b\sin \theta)$ and $(a\cos\theta, -b\sin \theta)$ . (Let's just say $\theta$ is theta) and then proceeded with the derivative tests(which i understood) But, he didn't indicate what our $\theta$ was,and declared that these points always lie on an ellipse. Why so? And even if they do, what's the guarantee that points of such a form will be our required vertices? One more thing, I'd appreciate it if you could suggest another way of solving this problem. Thank you!","Here's the question- Find the maximum area of an isosceles triangle inscribed in the ellipse . My teacher solved it by considering two arbitrary points on the ellipse to be vertices of the triangle, being and . (Let's just say is theta) and then proceeded with the derivative tests(which i understood) But, he didn't indicate what our was,and declared that these points always lie on an ellipse. Why so? And even if they do, what's the guarantee that points of such a form will be our required vertices? One more thing, I'd appreciate it if you could suggest another way of solving this problem. Thank you!","x^2/a^2 + y^2/b^2 = 1 (a\cos\theta, b\sin \theta) (a\cos\theta, -b\sin \theta) \theta \theta","['calculus', 'derivatives']"
39,Reason for repeated root while solving tangent and curve.,Reason for repeated root while solving tangent and curve.,,"I was solving a question from a book which is stated as: Let a tangent be drawn to $y=x^4-16x^2$ at $x = 1$ . I need to find all point of intersection between curve and the tangent. I started by finding equation of tangent. Using slope point form , equation of tangent: $y=-28x+13$ . Now equating $y$ of the line in the curve, I get a biquadratic equation in $x$ , which is: $x^4-16x^2+28x-13=0$ Now the solution in the book states that $x=1$ is a repeated root of the above equation,  since it satisfies both curve and the tangent. I don't understand this, by this logic won't we get a repeated root in every time when we solve equation of tangent and curve since the point of tangency satisfies both curve and tangent? Can anybody provide a better reason why do I get a repeated root?","I was solving a question from a book which is stated as: Let a tangent be drawn to at . I need to find all point of intersection between curve and the tangent. I started by finding equation of tangent. Using slope point form , equation of tangent: . Now equating of the line in the curve, I get a biquadratic equation in , which is: Now the solution in the book states that is a repeated root of the above equation,  since it satisfies both curve and the tangent. I don't understand this, by this logic won't we get a repeated root in every time when we solve equation of tangent and curve since the point of tangency satisfies both curve and tangent? Can anybody provide a better reason why do I get a repeated root?",y=x^4-16x^2 x = 1 y=-28x+13 y x x^4-16x^2+28x-13=0 x=1,"['derivatives', 'roots', 'tangent-line']"
40,Derivative of Renyi entropy,Derivative of Renyi entropy,,"Let $\log$ denote the logarithm with base $2$ . It is claimed that for $0\leq p_i\leq 1$ , $\sum_i p_i = 1$ and for any $0\leq \alpha \leq \infty, \alpha\neq 1$ $$\frac{d}{d\alpha}\left(\frac{1}{1-\alpha}\log\sum_i p_i^\alpha\right) = \frac{1}{(1-\alpha)^2}\sum_i\frac{p_i^\alpha}{\sum_jp_j^\alpha}\log\frac{p_i^{\alpha-1}}{\sum_k p_k^\alpha}$$ I am not able to show this result. Taking the derivative with respect to $\alpha$ , using the product rule and noting that $\frac{d}{dx}a^x = \ln(a) a^x$ , I get $$\frac{d}{d\alpha}\left(\frac{1}{1-\alpha}\log\sum_i p_i^\alpha\right) = \frac{1}{(1-\alpha)^2}\log\sum_i p_i^\alpha + \frac{1}{1-\alpha}\frac{1}{\sum_j p_j^\alpha}\sum_ip_i^\alpha \ln(p_i)$$ How does one proceed to get the desired result?","Let denote the logarithm with base . It is claimed that for , and for any I am not able to show this result. Taking the derivative with respect to , using the product rule and noting that , I get How does one proceed to get the desired result?","\log 2 0\leq p_i\leq 1 \sum_i p_i = 1 0\leq \alpha \leq \infty, \alpha\neq 1 \frac{d}{d\alpha}\left(\frac{1}{1-\alpha}\log\sum_i p_i^\alpha\right) = \frac{1}{(1-\alpha)^2}\sum_i\frac{p_i^\alpha}{\sum_jp_j^\alpha}\log\frac{p_i^{\alpha-1}}{\sum_k p_k^\alpha} \alpha \frac{d}{dx}a^x = \ln(a) a^x \frac{d}{d\alpha}\left(\frac{1}{1-\alpha}\log\sum_i p_i^\alpha\right) = \frac{1}{(1-\alpha)^2}\log\sum_i p_i^\alpha + \frac{1}{1-\alpha}\frac{1}{\sum_j p_j^\alpha}\sum_ip_i^\alpha \ln(p_i)","['calculus', 'derivatives', 'entropy']"
41,What is the derivative of the function and the slope at the given two points?,What is the derivative of the function and the slope at the given two points?,,"I was asked to find the derivative of the function $f(x)=x^3+2x$ using the limit definition of derivatives and find the slope at $(1,3)$ and $(-1,-3)$ . $$\lim_{h\to 0}\frac{(x+h)^3+2(x+h)-x^3-2x}{h}$$ $$\implies \lim_{h\to 0}\frac{x^3+3x^2h+3xh^2+h^3+2x+2h-x^3-2x}{h}$$ $$\implies \lim_{h\to 0}\frac{h(3x^2+3xh+h^2+2)}{h}$$ $$\implies \lim_{h\to 0}3x^2+3xh+h^2+2$$ $$\implies \lim_{h\to 0}3x^2+2$$ $$\implies \boxed{f'(x)=3x^2+2}$$ Above is my answer for the derivative of the function but I have no idea how to find the slope at the points $(1,3)$ and $(-1,-3)$ .",I was asked to find the derivative of the function using the limit definition of derivatives and find the slope at and . Above is my answer for the derivative of the function but I have no idea how to find the slope at the points and .,"f(x)=x^3+2x (1,3) (-1,-3) \lim_{h\to 0}\frac{(x+h)^3+2(x+h)-x^3-2x}{h} \implies \lim_{h\to 0}\frac{x^3+3x^2h+3xh^2+h^3+2x+2h-x^3-2x}{h} \implies \lim_{h\to 0}\frac{h(3x^2+3xh+h^2+2)}{h} \implies \lim_{h\to 0}3x^2+3xh+h^2+2 \implies \lim_{h\to 0}3x^2+2 \implies \boxed{f'(x)=3x^2+2} (1,3) (-1,-3)","['calculus', 'limits', 'derivatives']"
42,What is the theoretical mathematical justification for differential arithmetic?,What is the theoretical mathematical justification for differential arithmetic?,,"Throughout undergraduate physics textbooks, you will see informal math with differentials where elements like $dx$ and $dy$ are multiplied around like scalar constants, and differentiation in terms of a variable is treated as analogous to division. What is the theoretical justification for this? I have never seen a formal mathematical argument to say why this can be done, especially not in the textbooks that use it. When I mean formal, I mean an argument from the point of view of rigorous mathematics, not just saying that $\Delta x/\Delta y$ approximates $dx/dy$ so we can treat $dx$ like we would $\Delta x$ . Are there any formal proofs available? An example of the type of differential mathematics I am talking about is used in thermodynamics. https://en.wikipedia.org/wiki/Fundamental_thermodynamic_relation I have never seen the formal justification that undergirds this way of talking about infinitesimal changes and using the differentials like constants.","Throughout undergraduate physics textbooks, you will see informal math with differentials where elements like and are multiplied around like scalar constants, and differentiation in terms of a variable is treated as analogous to division. What is the theoretical justification for this? I have never seen a formal mathematical argument to say why this can be done, especially not in the textbooks that use it. When I mean formal, I mean an argument from the point of view of rigorous mathematics, not just saying that approximates so we can treat like we would . Are there any formal proofs available? An example of the type of differential mathematics I am talking about is used in thermodynamics. https://en.wikipedia.org/wiki/Fundamental_thermodynamic_relation I have never seen the formal justification that undergirds this way of talking about infinitesimal changes and using the differentials like constants.",dx dy \Delta x/\Delta y dx/dy dx \Delta x,['derivatives']
43,"Q: Prove that if $f(a)g(b) = f(b)g(a),$ then there exists $x\in (a,b)$ such that $f'(x)/f(x)=g'(x)/g(x)$.",Q: Prove that if  then there exists  such that .,"f(a)g(b) = f(b)g(a), x\in (a,b) f'(x)/f(x)=g'(x)/g(x)","I have to prove the following statement holds: Suppose $a,b\in\mathbb{R}$ and $a<b$ . Let $f,g$ be continuous on $[a,b]$ and differentiable on $(a,b)$ , and $f(x)\ne0$ , $g(x)\ne0$ for all $x\in[a,b]$ . If $f(a)g(b)=f(b)g(a)$ , then there exists some $x_{0}\in(a,b)$ such that $\frac{f'(x_{0})}{f(x_{0})}=\frac{g'(x_{0})}{g(x_{0})}$ . I have thought about using both the intermediate value theorem and the mean value theorem, since the functions $f$ and $g$ are continuous and differentiable. For example, we have by the mean value theorem that $\exists x_{0}\in(a,b)$ such that $$ f(b)-f(a)=f'(x_{0})(b-a) $$ and $$ g(b)-g(a)=g'(x_{0})(b-a) $$ whereby $$ \frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(x_{0})}{g'(x_{0})} $$ I'm not entirely sure how to use the hypothesis that $f(a)g(b)=f(b)g(a)$ from here. Any suggestions would be very helpful! Edit: This is the proof I have come up with: Let $a,b\in\mathbb{R}$ and $a<b$ . Let $f$ and $g$ be continuous on $[a,b]$ , differentiable on $(a,b)$ , and $f(x)\ne 0$ , $g(x)\ne0$ for all $x\in[a,b]$ . Suppose $f(a)g(b)=f(b)g(a)$ . Then $\frac{f(a)}{f(b)}=\frac{g(a)}{g(b)}$ . Let $p(x)=\ln(|f(x)|)$ and $q(x)=\ln(|g(x)|)$ , then $$ p'(x) = \frac{f'(x)}{f(x)} \text{ for all } x\in(a,b) $$ while $$ q'(x)=\frac{g'(x)}{g(x)} \text{ for all } x\in(a,b) $$ Furthermore, given some $x_{0}\in(a,b)$ , according to the mean value theorem we obtain $$ p(b)-p(a)=p'(x_{0})(b-a)\Longrightarrow \ln(\left|\frac{f(b)}{f(a)}\right|)=\frac{f'(x_{0})}{f(x_{0})}(b-a) $$ similarly, if $x_{0}\in(a,b)$ then $$ q(b)-q(a)=q'(x_{0})(b-a)\Longrightarrow \ln(\left|\frac{g(b)}{g(a)}\right|)=\frac{g'(x_{0})}{g(x_{0})}(b-a) $$ By hypothesis, we have $$ \ln(\left|\frac{g(b)}{g(a)}\right|)=\ln(\left|\frac{f(b)}{f(a)}\right|) $$ and thus $$ \frac{f'(x_{0})}{f(x_{0})}(b-a)=\frac{g'(x_{0})}{g(x_{0})}(b-a)\Longrightarrow \frac{f'(x_{0})}{f(x_{0})}=\frac{g'(x_{0})}{g(x_{0})} $$ From the comments below there is an issue with choosing $x_{0}\in(a,b)$ for both functions using the mean-value theorem. This is the only part of the proof I am struggling with now.","I have to prove the following statement holds: Suppose and . Let be continuous on and differentiable on , and , for all . If , then there exists some such that . I have thought about using both the intermediate value theorem and the mean value theorem, since the functions and are continuous and differentiable. For example, we have by the mean value theorem that such that and whereby I'm not entirely sure how to use the hypothesis that from here. Any suggestions would be very helpful! Edit: This is the proof I have come up with: Let and . Let and be continuous on , differentiable on , and , for all . Suppose . Then . Let and , then while Furthermore, given some , according to the mean value theorem we obtain similarly, if then By hypothesis, we have and thus From the comments below there is an issue with choosing for both functions using the mean-value theorem. This is the only part of the proof I am struggling with now.","a,b\in\mathbb{R} a<b f,g [a,b] (a,b) f(x)\ne0 g(x)\ne0 x\in[a,b] f(a)g(b)=f(b)g(a) x_{0}\in(a,b) \frac{f'(x_{0})}{f(x_{0})}=\frac{g'(x_{0})}{g(x_{0})} f g \exists x_{0}\in(a,b) 
f(b)-f(a)=f'(x_{0})(b-a)
 
g(b)-g(a)=g'(x_{0})(b-a)
 
\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(x_{0})}{g'(x_{0})}
 f(a)g(b)=f(b)g(a) a,b\in\mathbb{R} a<b f g [a,b] (a,b) f(x)\ne 0 g(x)\ne0 x\in[a,b] f(a)g(b)=f(b)g(a) \frac{f(a)}{f(b)}=\frac{g(a)}{g(b)} p(x)=\ln(|f(x)|) q(x)=\ln(|g(x)|) 
p'(x) = \frac{f'(x)}{f(x)} \text{ for all } x\in(a,b)
 
q'(x)=\frac{g'(x)}{g(x)} \text{ for all } x\in(a,b)
 x_{0}\in(a,b) 
p(b)-p(a)=p'(x_{0})(b-a)\Longrightarrow \ln(\left|\frac{f(b)}{f(a)}\right|)=\frac{f'(x_{0})}{f(x_{0})}(b-a)
 x_{0}\in(a,b) 
q(b)-q(a)=q'(x_{0})(b-a)\Longrightarrow \ln(\left|\frac{g(b)}{g(a)}\right|)=\frac{g'(x_{0})}{g(x_{0})}(b-a)
 
\ln(\left|\frac{g(b)}{g(a)}\right|)=\ln(\left|\frac{f(b)}{f(a)}\right|)
 
\frac{f'(x_{0})}{f(x_{0})}(b-a)=\frac{g'(x_{0})}{g(x_{0})}(b-a)\Longrightarrow
\frac{f'(x_{0})}{f(x_{0})}=\frac{g'(x_{0})}{g(x_{0})}
 x_{0}\in(a,b)","['real-analysis', 'derivatives', 'continuity']"
44,Differentiate with Kronecker product,Differentiate with Kronecker product,,"Acutually, I have a function that : $$\operatorname{tr}(\mathbf{M}(\mathbf{B}\otimes\mathbf{A}))$$ where $M$ and $B$ are constant matrix while $A$ is my variable. I want to have this : $$d \operatorname{tr}(\mathbf{M}(\mathbf{B}\otimes\mathbf{A})) = \operatorname{tr}(\mathbf{G}d\mathbf{A})$$ So, How to solve $\mathbf{G}$ ?","Acutually, I have a function that : where and are constant matrix while is my variable. I want to have this : So, How to solve ?",\operatorname{tr}(\mathbf{M}(\mathbf{B}\otimes\mathbf{A})) M B A d \operatorname{tr}(\mathbf{M}(\mathbf{B}\otimes\mathbf{A})) = \operatorname{tr}(\mathbf{G}d\mathbf{A}) \mathbf{G},"['derivatives', 'differential-geometry', 'kronecker-product']"
45,If $f(x)=\frac{x^3}{2}+1-x\int_0^x g(t) dt$ and $g(x)=x-\int_0^1f(u) du $ then the minimum distance between $f(x)$ and $g(x)$ is?,If  and  then the minimum distance between  and  is?,f(x)=\frac{x^3}{2}+1-x\int_0^x g(t) dt g(x)=x-\int_0^1f(u) du  f(x) g(x),The way I thought to solve this problem is to find $f(x)$ and $g(x)$ . So $$f(x)=\frac{x^3}{2}+1-x\left[\int_0 ^x\left(x-\int_0^1 f(u) du\right)\right]$$ This gives $$f(x)=1-x^2\int_0^1 f(u) du$$ Now dividing both sides by $x^2$ and differentiating we get $$f'(x)-\frac2 x f(x)=-\frac 2 x$$ Solving this we get $$f(x)= 1+Cx^2$$ where $C$ is a constant. Now my question is how do I find $C$ ? I do not have a initial condition. or maybe $C$ isn't required after all? It seems I cannot solve for minimum distance without $C$,The way I thought to solve this problem is to find and . So This gives Now dividing both sides by and differentiating we get Solving this we get where is a constant. Now my question is how do I find ? I do not have a initial condition. or maybe isn't required after all? It seems I cannot solve for minimum distance without,f(x) g(x) f(x)=\frac{x^3}{2}+1-x\left[\int_0 ^x\left(x-\int_0^1 f(u) du\right)\right] f(x)=1-x^2\int_0^1 f(u) du x^2 f'(x)-\frac2 x f(x)=-\frac 2 x f(x)= 1+Cx^2 C C C C,"['real-analysis', 'integration', 'derivatives']"
46,what is derivative of $\exp(X\beta)$ w.r.t $\beta$,what is derivative of  w.r.t,\exp(X\beta) \beta,"I am using the Denominator layout, i.e. $$\frac{\partial X\beta}{\partial \beta} = X^T,$$ where $X$ is $n\times p$ and $\beta$ is $p\times 1$ . What is the result of $$\frac{\partial \exp(X\beta)}{\partial \beta} \text{ ?}$$ Since $\exp(X\beta)$ is $n\times1$ and $\beta$ is $p\times 1$ , the derivative should be a $p\times n$ matrix. However, this is what I derived: $$\frac{\partial \exp(X\beta)}{\partial \beta} = \frac{\partial X\beta}{\partial \beta}\frac{\partial \exp(X\beta)}{\partial X\beta} = X^T\exp(X\beta),$$ which is a $p\times1$ . Where did I make a mistake?","I am using the Denominator layout, i.e. where is and is . What is the result of Since is and is , the derivative should be a matrix. However, this is what I derived: which is a . Where did I make a mistake?","\frac{\partial X\beta}{\partial \beta} = X^T, X n\times p \beta p\times 1 \frac{\partial \exp(X\beta)}{\partial \beta} \text{ ?} \exp(X\beta) n\times1 \beta p\times 1 p\times n \frac{\partial \exp(X\beta)}{\partial \beta} = \frac{\partial X\beta}{\partial \beta}\frac{\partial \exp(X\beta)}{\partial X\beta} = X^T\exp(X\beta), p\times1","['derivatives', 'self-learning', 'matrix-calculus']"
47,"How do I interpret the ""d"" in derivative notation? [duplicate]","How do I interpret the ""d"" in derivative notation? [duplicate]",,"This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) What do the symbols d/dx and dy/dx mean? (5 answers) Closed 3 years ago . I recently decided to declare as an applied math major and I've realized that I don't think I fully understand what derivative notation actually means. I'm kinda worried that I might have missed something and it may affect my progress in more advanced math classes like ODE which I'm taking right now. I have always assumed that $\frac{dy}{{dx}}$ is just notation for a specific operation: $$ \frac{dy}{{dx}} = \mathop {\lim }\limits_{h \to 0} \frac{{y\left( {x + h } \right) - y\left( x \right)}}{h} $$ I understand that this is the definition of a derivative but this doesn't seem to be the full story. For example what does $dx$ mean by itself? In practice it seems like $\frac{dy}{{dx}}$ is a ratio between two values $dy$ and $dx$ that have a mathematical meaning on their own. For example, its common to multiply both sides of an equation by $dx$ . People say that $dx$ is just a small change in $x$ but what does that actually mean? One possible answer that I've just sorta assumed without thinking about it too much was $$dx = \mathop {\lim }\limits_{a \to 0} (x + a) - x $$ But this seems wrong because it just evaluates to $dx =  \mathop {\lim }\limits_{a \to 0} a = 0$ . Reading wikipedia articles about differential notation isn't helping much but it sounds like the meaning of $dx$ has changed over time? The specific thing that prompted my question was a problem in my ODE class. Take a linear differential equation that looks something like this: $$ \frac{dy}{{dx}} + y = x^2 $$ I know what to do in order to solve this but the solution feels hand wavey. If you multiply both sides by the integrating factor $e^{x}dx$ and then integrate both sides you get $$ \int e^{x}dy + e^{x}ydx = \int e^{x}x^2 dx $$ I just don't know how to interpret the RHS of the equation. Do I integrate with respect to x or y? Do I just integrate the $dx$ and $dy$ terms separately? Doing that doesn't give you $e^{x}y$ which is the actual answer. Edit: some people are suggesting that in integral notation $dx$ is just sort of syntactical, it just shows where the end of the integration operation is. I am unsatisfied with this answer. If its just syntactical, then you would evaluate the ODE like this $$ \int (e^{x}\frac{dy}{{dx}} + e^{x}y) dx \\ $$ If I am understanding the syntax argument correctly, you should be able to split this integral into two parts like this: $$ \int e^{x}\frac{dy}{{dx}}dx + \int e^{x}y dx =  e^xy + e^xy = 2e^xy $$ But this doesn't appear to be consistent with my ODE textbook, which says the actual answer is $e^xy$ . They justified that by saying that the derivative of $e^yx$ is $e^{x}\frac{dy}{{dx}} + e^{x}y$ by the chain rule. This feels handwavey and leads me to believe that $ \int e^{x}\frac{dy}{{dx}}dx + \int e^{x}y dx $ is not notationally equivalent to $\int (e^{x}\frac{dy}{{dx}} + e^{x}y) dx$","This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) What do the symbols d/dx and dy/dx mean? (5 answers) Closed 3 years ago . I recently decided to declare as an applied math major and I've realized that I don't think I fully understand what derivative notation actually means. I'm kinda worried that I might have missed something and it may affect my progress in more advanced math classes like ODE which I'm taking right now. I have always assumed that is just notation for a specific operation: I understand that this is the definition of a derivative but this doesn't seem to be the full story. For example what does mean by itself? In practice it seems like is a ratio between two values and that have a mathematical meaning on their own. For example, its common to multiply both sides of an equation by . People say that is just a small change in but what does that actually mean? One possible answer that I've just sorta assumed without thinking about it too much was But this seems wrong because it just evaluates to . Reading wikipedia articles about differential notation isn't helping much but it sounds like the meaning of has changed over time? The specific thing that prompted my question was a problem in my ODE class. Take a linear differential equation that looks something like this: I know what to do in order to solve this but the solution feels hand wavey. If you multiply both sides by the integrating factor and then integrate both sides you get I just don't know how to interpret the RHS of the equation. Do I integrate with respect to x or y? Do I just integrate the and terms separately? Doing that doesn't give you which is the actual answer. Edit: some people are suggesting that in integral notation is just sort of syntactical, it just shows where the end of the integration operation is. I am unsatisfied with this answer. If its just syntactical, then you would evaluate the ODE like this If I am understanding the syntax argument correctly, you should be able to split this integral into two parts like this: But this doesn't appear to be consistent with my ODE textbook, which says the actual answer is . They justified that by saying that the derivative of is by the chain rule. This feels handwavey and leads me to believe that is not notationally equivalent to","\frac{dy}{{dx}} 
\frac{dy}{{dx}} = \mathop {\lim }\limits_{h \to 0} \frac{{y\left( {x + h } \right) - y\left( x \right)}}{h}
 dx \frac{dy}{{dx}} dy dx dx dx x dx = \mathop {\lim }\limits_{a \to 0} (x + a) - x  dx =  \mathop {\lim }\limits_{a \to 0} a = 0 dx 
\frac{dy}{{dx}} + y = x^2
 e^{x}dx 
\int e^{x}dy + e^{x}ydx = \int e^{x}x^2 dx
 dx dy e^{x}y dx 
\int (e^{x}\frac{dy}{{dx}} + e^{x}y) dx \\
  \int e^{x}\frac{dy}{{dx}}dx + \int e^{x}y dx =  e^xy + e^xy = 2e^xy
 e^xy e^yx e^{x}\frac{dy}{{dx}} + e^{x}y  \int e^{x}\frac{dy}{{dx}}dx + \int e^{x}y dx  \int (e^{x}\frac{dy}{{dx}} + e^{x}y) dx",['calculus']
48,Real Analysis objective question.,Real Analysis objective question.,,I don’t know exactly how to solve it but my attempt is as follows $$f’(x)=\sin^4(2g(x))$$ So function is differentiable as composition of two differentiable function is differentiablity . Now it has infinitely many zeroes because of sin(x) has infinity many zero . Please suggest. Thank you .,I don’t know exactly how to solve it but my attempt is as follows So function is differentiable as composition of two differentiable function is differentiablity . Now it has infinitely many zeroes because of sin(x) has infinity many zero . Please suggest. Thank you .,f’(x)=\sin^4(2g(x)),"['real-analysis', 'derivatives']"
49,Finding $f(x)$ for any $x$ by assuming $a_{n+1}$,Finding  for any  by assuming,f(x) x a_{n+1},"I'm trying to solve this problem in the implicit differentiation section of the book I'm going through: The equation that implicitly defines $f$ can be written as $y = \dfrac{2 \sin x + \cos y}{3}$ In this problem we will compute $f(\pi/6)$ . The same method could be used to compute $f(x)$ for any value of $x$ . Let $a_1 = \dfrac{2\sin(\pi/6)}{3} = \dfrac{1}{3}$ and for every positive integer $n$ let $a_{n+1} = \dfrac{2\sin(\pi/6) + \cos a_n}{3} = \dfrac{1 + \cos a_n}{3}$ (a) Prove that for every positive integer $n$ , $|a_n - f(\pi/6)| \leq 1/3^n$ (Hint: Use mathematical induction) (b) Prove that $\lim_{n \to \infty} a_n = f(\pi/6)$ Now, I'm not sure how to solve $f(\pi/6)$ in the base case of the induction proof. Also, does this above pattern of assuming $a_{n+1}$ and using that to compute $f(x)$ for any value of $x$ has any name ? I would like to read more about it.","I'm trying to solve this problem in the implicit differentiation section of the book I'm going through: The equation that implicitly defines can be written as In this problem we will compute . The same method could be used to compute for any value of . Let and for every positive integer let (a) Prove that for every positive integer , (Hint: Use mathematical induction) (b) Prove that Now, I'm not sure how to solve in the base case of the induction proof. Also, does this above pattern of assuming and using that to compute for any value of has any name ? I would like to read more about it.",f y = \dfrac{2 \sin x + \cos y}{3} f(\pi/6) f(x) x a_1 = \dfrac{2\sin(\pi/6)}{3} = \dfrac{1}{3} n a_{n+1} = \dfrac{2\sin(\pi/6) + \cos a_n}{3} = \dfrac{1 + \cos a_n}{3} n |a_n - f(\pi/6)| \leq 1/3^n \lim_{n \to \infty} a_n = f(\pi/6) f(\pi/6) a_{n+1} f(x) x,"['calculus', 'derivatives', 'implicit-differentiation']"
50,Values such that piecewise function is differentiable everywhere,Values such that piecewise function is differentiable everywhere,,"A small discussion on calculus. I have a function $$f(x)=\begin{cases}3-x, & \text{$x<1$}\\ ax^2 +bx, &\text{$x\geq1$}\end{cases}$$ I need to find $a,b$ such that this function is differentiable everywhere. Usually we proceed like this (correct me if I am wrong): We must have continuity, so that $2=a+b$ . We also must have equal left and right derivative at 1, so by calculating each one-sided derivative at 1, we have $-1=2a+b$ . Solving the system, we get $a,b$ . By the way, I am not studying this as new student in Calculus. This is just something that bothers me even though I have passed Calculus class. My two questions here are: What is the theory behind equating both one-sided derivatives here? Are we arguing this way: at any $x<1$ , the derivative is $-1$ , and at any $x>1$ , the derivative is $2ax + b$ . Limiting to $1$ , they have to be equal, hence $-1=2a+b$ . However, aren't we limiting $f'(x)$ (for $x\neq 1$ ) here? Is one-sided limit of $f'(x)$ the definition of one-sided derivative? Still related, I thought one-sided derivative comes from the definition $\frac{f(x)-f(1)}{x-1}\rightarrow \text{(some number)}$ as $x\rightarrow 1^-$ (for left side, and similarly for right side) instead? If it is the case, then I tried to calculate by definition and wanted to equate the result, but it does not look easy at all, and I could not obtain $-1=2a+b$ . Thanks a lot for clearing my misunderstanding.","A small discussion on calculus. I have a function I need to find such that this function is differentiable everywhere. Usually we proceed like this (correct me if I am wrong): We must have continuity, so that . We also must have equal left and right derivative at 1, so by calculating each one-sided derivative at 1, we have . Solving the system, we get . By the way, I am not studying this as new student in Calculus. This is just something that bothers me even though I have passed Calculus class. My two questions here are: What is the theory behind equating both one-sided derivatives here? Are we arguing this way: at any , the derivative is , and at any , the derivative is . Limiting to , they have to be equal, hence . However, aren't we limiting (for ) here? Is one-sided limit of the definition of one-sided derivative? Still related, I thought one-sided derivative comes from the definition as (for left side, and similarly for right side) instead? If it is the case, then I tried to calculate by definition and wanted to equate the result, but it does not look easy at all, and I could not obtain . Thanks a lot for clearing my misunderstanding.","f(x)=\begin{cases}3-x, & \text{x<1}\\ ax^2 +bx, &\text{x\geq1}\end{cases} a,b 2=a+b -1=2a+b a,b x<1 -1 x>1 2ax + b 1 -1=2a+b f'(x) x\neq 1 f'(x) \frac{f(x)-f(1)}{x-1}\rightarrow \text{(some number)} x\rightarrow 1^- -1=2a+b","['calculus', 'limits', 'derivatives']"
51,Show that $f$ is infinitely differentiable,Show that  is infinitely differentiable,f,"Let $f:(0,\infty)\to\Bbb{R}$ satisfy $f(xy)=f(x)+f(y)$ for all $x,y\in(0,\infty)$ . If $f$ is differentiable at $x=1$ , show that $f$ is differentiable on $(0,\infty)$ and $f'(x)=\frac{f'(1)}{x}$ . Show that $f$ is in fact infinitely differentiable. What I did: Let $y=1,$ then $f(x)=f(x)+f(1)$ $\therefore f(1)=0$ is differentiable. Let $\mid h\mid<x$ and $y=1+\frac{h}{x}$ $f(xy)=f(x(1+\frac{h}{x}))=f(x+h)=f(x)+f(1+\frac{h}{x})=f(x)+f(1)+f'(1)\frac{}{}\frac{h}{x}+\text{o}(\frac{h}{x})$ How do I show that $\frac{\frac{h}{x}}{x-h}\to0$ as $x\to h$ ? And how do I show that $f$ is infinitely differentibale? I have to prove another function is infinitely differentiable as well so I hope I can apply the same technique there. Thank you guys.","Let satisfy for all . If is differentiable at , show that is differentiable on and . Show that is in fact infinitely differentiable. What I did: Let then is differentiable. Let and How do I show that as ? And how do I show that is infinitely differentibale? I have to prove another function is infinitely differentiable as well so I hope I can apply the same technique there. Thank you guys.","f:(0,\infty)\to\Bbb{R} f(xy)=f(x)+f(y) x,y\in(0,\infty) f x=1 f (0,\infty) f'(x)=\frac{f'(1)}{x} f y=1, f(x)=f(x)+f(1) \therefore f(1)=0 \mid h\mid<x y=1+\frac{h}{x} f(xy)=f(x(1+\frac{h}{x}))=f(x+h)=f(x)+f(1+\frac{h}{x})=f(x)+f(1)+f'(1)\frac{}{}\frac{h}{x}+\text{o}(\frac{h}{x}) \frac{\frac{h}{x}}{x-h}\to0 x\to h f",['derivatives']
52,"Find the angle between two tangents drawn from point $(0, -2)$ to the curve $y=x^2$",Find the angle between two tangents drawn from point  to the curve,"(0, -2) y=x^2","Find the angle between the two tangents drawn from point $(0, -2)$ to the curve $y=x^2$ . This is my attempt: Let $P(\alpha, \beta)$ be a point on the curve. $$\therefore \beta = \alpha^2$$ $$\frac{dy}{dx}\quad \text{at}\quad P(\alpha,\beta) = 2\alpha$$ Equation of tangent at P: $2\alpha x-y=2\alpha^2-\beta \Rightarrow2\alpha x-y = 2\alpha^2 -\alpha^2$ $$\Rightarrow2\alpha x -y -\alpha^2 =0$$ A/Q $(0, -2)$ should satify this equation. $\therefore 2\alpha\times0 - (-2) - \alpha^2 = 0\Rightarrow\alpha^2=2$ $$\therefore\alpha=\pm\sqrt2$$ $$\Rightarrow\beta=2$$ Now putting these values to find slope $(m)=\frac{dy}{dx}=2\times\pm\sqrt2$ $$\therefore m = +2\sqrt2\quad and\quad -2\sqrt2$$ We know that for $\theta$ =angle between the lines and $m_1\quad\&\quad m_2$ be slope of lines: $$\tan{\theta} =\big|\frac{m_1-m_2}{1+m_1m_2}\big|$$ $$=\big|\frac{2\sqrt2-(-2\sqrt2)}{1+2\sqrt2\times-2\sqrt2}\big|= \big|\frac{4\sqrt2}{1-8}\big|=\frac{4\sqrt2}7$$ My answer does not match the book. The book is a very appreciated one, so it can't be wrong. I cannot find an error in my solution. The book states the answer as $$\pi - 2\arctan\sqrt8$$ Edit: The book actually states its answer as $\pi - 2\arctan\sqrt8$ . I was the blind who couldn't see the 2 .","Find the angle between the two tangents drawn from point to the curve . This is my attempt: Let be a point on the curve. Equation of tangent at P: A/Q should satify this equation. Now putting these values to find slope We know that for =angle between the lines and be slope of lines: My answer does not match the book. The book is a very appreciated one, so it can't be wrong. I cannot find an error in my solution. The book states the answer as Edit: The book actually states its answer as . I was the blind who couldn't see the 2 .","(0, -2) y=x^2 P(\alpha, \beta) \therefore \beta = \alpha^2 \frac{dy}{dx}\quad \text{at}\quad P(\alpha,\beta) = 2\alpha 2\alpha x-y=2\alpha^2-\beta
\Rightarrow2\alpha x-y = 2\alpha^2 -\alpha^2 \Rightarrow2\alpha x -y -\alpha^2 =0 (0, -2) \therefore 2\alpha\times0 - (-2) - \alpha^2 = 0\Rightarrow\alpha^2=2 \therefore\alpha=\pm\sqrt2 \Rightarrow\beta=2 (m)=\frac{dy}{dx}=2\times\pm\sqrt2 \therefore m = +2\sqrt2\quad and\quad -2\sqrt2 \theta m_1\quad\&\quad m_2 \tan{\theta} =\big|\frac{m_1-m_2}{1+m_1m_2}\big| =\big|\frac{2\sqrt2-(-2\sqrt2)}{1+2\sqrt2\times-2\sqrt2}\big|= \big|\frac{4\sqrt2}{1-8}\big|=\frac{4\sqrt2}7 \pi - 2\arctan\sqrt8 \pi - 2\arctan\sqrt8","['geometry', 'derivatives', 'analytic-geometry', 'tangent-line', 'slope']"
53,Finding n-th derivative,Finding n-th derivative,,"How can I find the n-th derivative of $\frac{1}{x(x+1)}$ ? I tried expressing it as such, but I'm not sure about it. $\frac{1}{x(x+1)} = \frac{1}{x}-\frac{1}{x+1}$ edit: I think I found it, can anyone confirm? Let $f_n$ be the n-th derivative of $f$ $f_1\left(x\right)=-\frac{1}{x^2}+\frac{1}{\left(x+1\right)^2}$ $f_2\left(x\right)=\frac{2}{x^{\text{3}}}-\frac{2}{\left(x+1\right)^3}$ $f_3\left(x\right)=-\frac{6}{x^4}+\frac{6}{\left(x+1\right)^4}$ $f_4\left(x\right)=\frac{24}{x^5}-\frac{24}{\left(x+1\right)^5}$ We can then guess that the n-th derivative would be: $f_n\left(x\right)=\left(-1\right)^n\left(-\frac{n!}{x^{n+1}}+\frac{n!}{\left(x+1\right)^{n+1}}\right)$ However is this enough to ""proof"" the result? Like is it acceptable if I was in, say, an exam? $f_n$ is simply based on an assumption?... Also, how can I find x such as $f_n(x) = 0$ ?","How can I find the n-th derivative of ? I tried expressing it as such, but I'm not sure about it. edit: I think I found it, can anyone confirm? Let be the n-th derivative of We can then guess that the n-th derivative would be: However is this enough to ""proof"" the result? Like is it acceptable if I was in, say, an exam? is simply based on an assumption?... Also, how can I find x such as ?",\frac{1}{x(x+1)} \frac{1}{x(x+1)} = \frac{1}{x}-\frac{1}{x+1} f_n f f_1\left(x\right)=-\frac{1}{x^2}+\frac{1}{\left(x+1\right)^2} f_2\left(x\right)=\frac{2}{x^{\text{3}}}-\frac{2}{\left(x+1\right)^3} f_3\left(x\right)=-\frac{6}{x^4}+\frac{6}{\left(x+1\right)^4} f_4\left(x\right)=\frac{24}{x^5}-\frac{24}{\left(x+1\right)^5} f_n\left(x\right)=\left(-1\right)^n\left(-\frac{n!}{x^{n+1}}+\frac{n!}{\left(x+1\right)^{n+1}}\right) f_n f_n(x) = 0,['derivatives']
54,confusion regarding treatment of $dx$ in a physics problem,confusion regarding treatment of  in a physics problem,dx,"Consider a fixed, positive Point charge $q1$ , kept at the origin. Another (positive) charge, $q2$ , is being brought from $\infty$ to the point $(r,0)$ , by an external agent slowly . We wish to calculate the work done by the external agent (and thus derive the ""potential"" of the point charge $q1$ , being defined as $w_{ext}/q2$ (or as $-w_{electric}/q2$ )). Suppose we consider a position $(x,0)$ : The magnitude of force is going to be $kq1q2/x^2$ . We will thus have, $\vec{F}=\dfrac{-kq1q2}{x^2}\hat{i}$ . When we displace it from a position $(x,0)$ to $(x-dx,0)$ , the displacement vector $(\vec{ds})$ will be $(x-dx)\hat{i}-x\hat(i)=-dx\hat{i}$ . Using $dw$ = $\vec{F}.\vec{ds}$ , we will get $dw=\dfrac{kq1q2}{x^2}dx$ . Upon integrating from $\infty$ (initial position) to $r$ (final position), we get : $$w_{ext}=-\dfrac{kq1q2}{r}$$ and thus $$V(r)= w_{ext}/q2 =-\dfrac{kq1}{r}$$ which is completely absurd. I tried to be as rigorous as possible with the definitions, vectors etc and yet a -ve sign has crept in somewhere. The only issue seems to be with the treatment of $dx$ . Although , I took $dx$ to be the magnitude of displacement, and accounted for the direction by using $-\hat{i}$ .A possible argument seems to be "" $x$ decreases, so $dx$ is a negative quantity. So the ""magnitude"" should be $-dx$ . My two concerns: What is then, the issue with displacement= $\vec{r_{final}} - \vec{r_{initial}}$ that simply yields $-dx\hat{i}$ ? Simply ""putting"" a - sign before $dx$ after claiming "" $dx$ is negative"" seems to be arbitrary. There should be an argument (like I presented in the previous bullet point) that will produce the - sign for the magnitude, and thus making the vector $(-dx)(-\hat{i})$ . The main essence of this problem seems to be rigorously defining what $dx$ actually represents, for a quantity $x$ . I believe the entire thing can be summarized by one question: What is wrong in writing displacement= $\vec{r_{final}} - \vec{r_{initial}}$ that simply yields $-dx\hat{i}$ ? If I had $(x+dx)$ instead of $(x-dx)$ ,then the derivation would be correct. But why is this the Case?","Consider a fixed, positive Point charge , kept at the origin. Another (positive) charge, , is being brought from to the point , by an external agent slowly . We wish to calculate the work done by the external agent (and thus derive the ""potential"" of the point charge , being defined as (or as )). Suppose we consider a position : The magnitude of force is going to be . We will thus have, . When we displace it from a position to , the displacement vector will be . Using = , we will get . Upon integrating from (initial position) to (final position), we get : and thus which is completely absurd. I tried to be as rigorous as possible with the definitions, vectors etc and yet a -ve sign has crept in somewhere. The only issue seems to be with the treatment of . Although , I took to be the magnitude of displacement, and accounted for the direction by using .A possible argument seems to be "" decreases, so is a negative quantity. So the ""magnitude"" should be . My two concerns: What is then, the issue with displacement= that simply yields ? Simply ""putting"" a - sign before after claiming "" is negative"" seems to be arbitrary. There should be an argument (like I presented in the previous bullet point) that will produce the - sign for the magnitude, and thus making the vector . The main essence of this problem seems to be rigorously defining what actually represents, for a quantity . I believe the entire thing can be summarized by one question: What is wrong in writing displacement= that simply yields ? If I had instead of ,then the derivation would be correct. But why is this the Case?","q1 q2 \infty (r,0) q1 w_{ext}/q2 -w_{electric}/q2 (x,0) kq1q2/x^2 \vec{F}=\dfrac{-kq1q2}{x^2}\hat{i} (x,0) (x-dx,0) (\vec{ds}) (x-dx)\hat{i}-x\hat(i)=-dx\hat{i} dw \vec{F}.\vec{ds} dw=\dfrac{kq1q2}{x^2}dx \infty r w_{ext}=-\dfrac{kq1q2}{r} V(r)= w_{ext}/q2 =-\dfrac{kq1}{r} dx dx -\hat{i} x dx -dx \vec{r_{final}} - \vec{r_{initial}} -dx\hat{i} dx dx (-dx)(-\hat{i}) dx x \vec{r_{final}} - \vec{r_{initial}} -dx\hat{i} (x+dx) (x-dx)","['derivatives', 'physics', 'convention']"
55,"If $f(x)$ is differentiable for all real numbers, then what is the value of $\frac{a+b+c}{2}$?","If  is differentiable for all real numbers, then what is the value of ?",f(x) \frac{a+b+c}{2},"If $f(x)=\begin {cases} a^2 + e^x & -\infty <x<0 \\ x+2 & 0\le x \le 3 \\ c -\frac{b^2}{x} & 3<x<\infty \end{cases}$ , where $a,b,c$ are positive quantities. If $f(x)$ is differentiable for all real numbers, then value of $\frac{a+b+c}{2}$ is Left hand derivative at $x=0$ $$Lf’(0) =\lim_{h\to 0} \frac{2 - (a^2 +e^{-h})}{h}$$ For limit to exist, $2-a^2=0 \implies a=\pm \sqrt 2$ $$L f’(0)=1$$ Right hand derivative at $x=0$ $$R f’(0) =\lim_{h\to 0} \frac{ h+2 -2}{h} =1$$ Left hand derivative at $x=3$ $$Lf’(3) =\lim_{h\to 0} \frac{5- (3-h+2)}{h}=1$$ And $$Rf’(3) =\lim_{h\to 0} \frac{ c -\frac{b^2}{3+h}-5}{h}$$ For limit to exist, $c=h$ $$Rf’(3) =\lim_{h\to 0} \frac{b^2}{(3+h)(h)}=\infty$$ Where am I going wrong?","If , where are positive quantities. If is differentiable for all real numbers, then value of is Left hand derivative at For limit to exist, Right hand derivative at Left hand derivative at And For limit to exist, Where am I going wrong?","f(x)=\begin {cases} a^2 + e^x & -\infty <x<0 \\ x+2 & 0\le x \le 3 \\ c -\frac{b^2}{x} & 3<x<\infty \end{cases} a,b,c f(x) \frac{a+b+c}{2} x=0 Lf’(0) =\lim_{h\to 0} \frac{2 - (a^2 +e^{-h})}{h} 2-a^2=0 \implies a=\pm \sqrt 2 L f’(0)=1 x=0 R f’(0) =\lim_{h\to 0} \frac{ h+2 -2}{h} =1 x=3 Lf’(3) =\lim_{h\to 0} \frac{5- (3-h+2)}{h}=1 Rf’(3) =\lim_{h\to 0} \frac{ c -\frac{b^2}{3+h}-5}{h} c=h Rf’(3) =\lim_{h\to 0} \frac{b^2}{(3+h)(h)}=\infty","['calculus', 'limits', 'derivatives']"
56,"How to solve for a specific gradient in an implicit relationship, when no points are known?","How to solve for a specific gradient in an implicit relationship, when no points are known?",,"The problem Consider the following relation: $$x^2-3xy+y^2=7$$ I'm struggling with what is essentially the following task: Find all coordinates of all points where the gradient of the tangent of the curve is ${2\over3}$ . Using implicit differentiation, I arrive at the following derivative of $y$ wrt $x$ : $${\text dy\over\text dx}=\frac{3y-2x}{2y-3x}$$ This is somewhat of a mindbender for me, since it looks like the derivative itself depends on both $x$ and $y$ . How can this be? How does one work with this? Even more daunting is the task of solving $\frac{\text dy}{\text dx}=\frac{2}{3}$ . The equation cannot be solved as we have a single, two-variable equation: $$2(2y-3x)=3(3y-2x)$$ Sadder yet is that by my logic, there is an infinite number of solutions ( $xy$ pairs) that satisfy this. Unfortunately, graphing this relation gives the solution set $(x,0)$ where $x\inℝ$ . The correct answer is $(\sqrt7,0)\cup(-\sqrt7,0)$ . My questions How do I work with a derivative which depends on both $x$ and $y$ , and conceptually, how is that even possible? Why is it that graphing the solutions of $2(2y-3x)=3(3y-2x)$ did not work and gave erroneous solutions? How is the answer $(\sqrt7,0)\cup(-\sqrt7,0)$ obtained? Thank you very much :)","The problem Consider the following relation: I'm struggling with what is essentially the following task: Find all coordinates of all points where the gradient of the tangent of the curve is . Using implicit differentiation, I arrive at the following derivative of wrt : This is somewhat of a mindbender for me, since it looks like the derivative itself depends on both and . How can this be? How does one work with this? Even more daunting is the task of solving . The equation cannot be solved as we have a single, two-variable equation: Sadder yet is that by my logic, there is an infinite number of solutions ( pairs) that satisfy this. Unfortunately, graphing this relation gives the solution set where . The correct answer is . My questions How do I work with a derivative which depends on both and , and conceptually, how is that even possible? Why is it that graphing the solutions of did not work and gave erroneous solutions? How is the answer obtained? Thank you very much :)","x^2-3xy+y^2=7 {2\over3} y x {\text dy\over\text dx}=\frac{3y-2x}{2y-3x} x y \frac{\text dy}{\text dx}=\frac{2}{3} 2(2y-3x)=3(3y-2x) xy (x,0) x\inℝ (\sqrt7,0)\cup(-\sqrt7,0) x y 2(2y-3x)=3(3y-2x) (\sqrt7,0)\cup(-\sqrt7,0)","['calculus', 'derivatives', 'implicit-differentiation']"
57,Does a function which is oscillating have to have not-continuous derivative?,Does a function which is oscillating have to have not-continuous derivative?,,"If $f(x)$ is a differentiable function  which is not $0$ everywhere  and has the property that around any interval around $0$ , $f$ is neither fully positive or negative. Then it can be proven that $f(0)=0$ . An example of such a function is $$\begin{cases}x^2\sin({1\over x}) & \text{ for }x\neq 0,\\ 0 &\text{ for }x=0.\end{cases}$$ All such functions I have seen so far satisfy this. Q: Is it true that the derivative of such a function cannot be continuous or is there a counter-example? I feel that such a function exists and have tried a few examples but have been unable to find one.","If is a differentiable function  which is not everywhere  and has the property that around any interval around , is neither fully positive or negative. Then it can be proven that . An example of such a function is All such functions I have seen so far satisfy this. Q: Is it true that the derivative of such a function cannot be continuous or is there a counter-example? I feel that such a function exists and have tried a few examples but have been unable to find one.","f(x) 0 0 f f(0)=0 \begin{cases}x^2\sin({1\over x}) & \text{ for }x\neq 0,\\ 0 &\text{ for }x=0.\end{cases}","['real-analysis', 'calculus', 'derivatives', 'examples-counterexamples']"
58,Gradient of trace of a product with a matrix logarithm and Kronecker product,Gradient of trace of a product with a matrix logarithm and Kronecker product,,"I'm looking to compute the gradient of a reasonably complicated matrix function, which I can mostly reduce to the following problem. I'm not entirely sure if a closed-form analytic solution is possible. I want to find $\nabla_X f$ , where $$f(X) = \text{tr}\left[X\cdot \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m}\right]$$ Here: $X$ is an $n\times n$ complex, full-rank, positive semi-definite matrix, $\{A_i\}$ is a set of real, $m\times n$ matrices (specifically, this sum is to compute the partial trace of $X$ ), $\mathbb{I}$ is the $n/m \times n/m$ identity matrix, and $\otimes$ is the regular Kronecker product. I don't have much experience with matrix calculus, but it seems at face value like most literature on the topic are cheat-sheet type rules about how to compute different basic derivatives but I don't have a good feel for how to tackle more difficult problems. For example, here, I have seen that $\frac{\partial \text{tr}(F(X))}{\partial X} = f(X)^\dagger $ , where $f$ is the scalar derivative of $F$ but it's not clear to me exactly what this scalar derivative means and I can't seem to find more information or build a ground-up approach. From this, my best guess is that $$\frac{\partial f}{\partial X}^\dagger = \frac{\partial}{\partial X}\left[\text{tr}(X)\right]\cdot\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + X\cdot \frac{\partial}{\partial X}\left[\text{tr}\left(\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m}\right)\right]$$ which simplifies to $$\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + X\cdot \frac{\partial}{\partial X}\left[\log\left|\sum_i A_i X A_i^T\right|\right]\cdot\text{Tr}(\mathbb{I}_{n/m})$$ $$= \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + (n/m)\cdot X \cdot \left(\sum_i A_i^T P A_i\right)$$ with $P = \left(\sum_i A_i X A_i^T\right)^{-1}$ . I don't think you can just take the derivatives out of the trace like that, but I don't really know how to proceed with taking the derivative of what's inside the trace and then using the chain rule. Is anybody able to help with this? Am I on the right-track or is there a more systematic way to compute this? Is it even possible to find a closed form expression or should I be resorting to numerics? I know some of the non-commuting aspects to the problem can be tamed by the trace but I'm actually not totally sure which elements should be required to commute in this sense. Many thanks in advance.","I'm looking to compute the gradient of a reasonably complicated matrix function, which I can mostly reduce to the following problem. I'm not entirely sure if a closed-form analytic solution is possible. I want to find , where Here: is an complex, full-rank, positive semi-definite matrix, is a set of real, matrices (specifically, this sum is to compute the partial trace of ), is the identity matrix, and is the regular Kronecker product. I don't have much experience with matrix calculus, but it seems at face value like most literature on the topic are cheat-sheet type rules about how to compute different basic derivatives but I don't have a good feel for how to tackle more difficult problems. For example, here, I have seen that , where is the scalar derivative of but it's not clear to me exactly what this scalar derivative means and I can't seem to find more information or build a ground-up approach. From this, my best guess is that which simplifies to with . I don't think you can just take the derivatives out of the trace like that, but I don't really know how to proceed with taking the derivative of what's inside the trace and then using the chain rule. Is anybody able to help with this? Am I on the right-track or is there a more systematic way to compute this? Is it even possible to find a closed form expression or should I be resorting to numerics? I know some of the non-commuting aspects to the problem can be tamed by the trace but I'm actually not totally sure which elements should be required to commute in this sense. Many thanks in advance.",\nabla_X f f(X) = \text{tr}\left[X\cdot \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m}\right] X n\times n \{A_i\} m\times n X \mathbb{I} n/m \times n/m \otimes \frac{\partial \text{tr}(F(X))}{\partial X} = f(X)^\dagger  f F \frac{\partial f}{\partial X}^\dagger = \frac{\partial}{\partial X}\left[\text{tr}(X)\right]\cdot\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + X\cdot \frac{\partial}{\partial X}\left[\text{tr}\left(\log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m}\right)\right] \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + X\cdot \frac{\partial}{\partial X}\left[\log\left|\sum_i A_i X A_i^T\right|\right]\cdot\text{Tr}(\mathbb{I}_{n/m}) = \log\left(\sum_i A_i X A_i^T\right)\otimes\mathbb{I}_{n/m} + (n/m)\cdot X \cdot \left(\sum_i A_i^T P A_i\right) P = \left(\sum_i A_i X A_i^T\right)^{-1},"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
59,Sign inconsistency in finding higher order partial derivates of $y=f(x-vt)$,Sign inconsistency in finding higher order partial derivates of,y=f(x-vt),"For the function $y=f(x-vt)$ where $x$ and $t$ are variables and $v$ is a constant, I was given the following options where I'm allowed to choose more than one: (a) $\frac{\partial y}{\partial t}=-v\frac{\partial y}{\partial x}$ (b) $\frac{\partial y}{\partial t}=-v\frac{\partial^2 y}{\partial x^2}$ (c) $\frac{\partial^2 y}{\partial t^2}=-v^2\frac{\partial^2 y}{\partial x^2}$ (d) $\frac{\partial^2 y}{\partial t^2}=v^2\frac{\partial^2 y}{\partial x^2}$ This is how I approached: We're given that $y=f(x-vt)$ . So, \begin{align} \frac{\partial y}{\partial t}&=-v f'(x-vt)\tag 1\\ \frac{\partial y}{\partial x}&=f'(x-vt)\tag 2 \end{align} Using $(2)$ in $(1)$ , we get: $$\frac{\partial y}{\partial t}=-v\frac{\partial y}{\partial x}\tag 3$$ which is same as option (a) and according to my book it's one of the correct options. Next for $\frac{\partial^2 y}{\partial t^2}$ , I took the corresponding partial derivatives of $(1)$ and $(2)$ to arrive at $(4)$ and $(5)$ as follows: \begin{align} \frac{\partial^2 y}{\partial t^2}&=v^2 f''(x-vt)\tag 4\\ \frac{\partial^2 y}{\partial x^2}&=f''(x-vt)\tag 5 \end{align} Using $(5)$ in $(4)$ , I got: $$\frac{\partial^2 y}{\partial t^2}=v^2\frac{\partial^2 y}{\partial x^2}\tag 6$$ which is same as option (d). However, my book says the answer is (c) $\frac{\partial^2 y}{\partial t^2}=\color{red}{-}v^2\frac{\partial^2 y}{\partial x^2}$ (with a negative sign). I haven't spotted any errors in my method. It would be helpful if you could explain where I went wrong and how to arrive at the correct answer.","For the function where and are variables and is a constant, I was given the following options where I'm allowed to choose more than one: (a) (b) (c) (d) This is how I approached: We're given that . So, Using in , we get: which is same as option (a) and according to my book it's one of the correct options. Next for , I took the corresponding partial derivatives of and to arrive at and as follows: Using in , I got: which is same as option (d). However, my book says the answer is (c) (with a negative sign). I haven't spotted any errors in my method. It would be helpful if you could explain where I went wrong and how to arrive at the correct answer.","y=f(x-vt) x t v \frac{\partial y}{\partial t}=-v\frac{\partial y}{\partial x} \frac{\partial y}{\partial t}=-v\frac{\partial^2 y}{\partial x^2} \frac{\partial^2 y}{\partial t^2}=-v^2\frac{\partial^2 y}{\partial x^2} \frac{\partial^2 y}{\partial t^2}=v^2\frac{\partial^2 y}{\partial x^2} y=f(x-vt) \begin{align}
\frac{\partial y}{\partial t}&=-v f'(x-vt)\tag 1\\
\frac{\partial y}{\partial x}&=f'(x-vt)\tag 2
\end{align} (2) (1) \frac{\partial y}{\partial t}=-v\frac{\partial y}{\partial x}\tag 3 \frac{\partial^2 y}{\partial t^2} (1) (2) (4) (5) \begin{align}
\frac{\partial^2 y}{\partial t^2}&=v^2 f''(x-vt)\tag 4\\
\frac{\partial^2 y}{\partial x^2}&=f''(x-vt)\tag 5
\end{align} (5) (4) \frac{\partial^2 y}{\partial t^2}=v^2\frac{\partial^2 y}{\partial x^2}\tag 6 \frac{\partial^2 y}{\partial t^2}=\color{red}{-}v^2\frac{\partial^2 y}{\partial x^2}","['calculus', 'derivatives', 'partial-derivative']"
60,"Denote that expression is differentiated, without differentiating it","Denote that expression is differentiated, without differentiating it",,"I'm trying to indicate that I'm working with the derivative of an expression, without differentiating it. This is how it would be done, if I differentiate both sides now: \begin{align} y &= 5x^2\\ y' &= 10x \end{align} However, I want to differentiate the right side later, but keep working with the expression. How can I denote that I'm referring to the derivative? I suppose this won't work, but I would like to do something in this fashion: $$ y = 5x^2\\ y'= (5x^2)' $$ Is there any good way to do this?","I'm trying to indicate that I'm working with the derivative of an expression, without differentiating it. This is how it would be done, if I differentiate both sides now: However, I want to differentiate the right side later, but keep working with the expression. How can I denote that I'm referring to the derivative? I suppose this won't work, but I would like to do something in this fashion: Is there any good way to do this?","\begin{align}
y &= 5x^2\\
y' &= 10x
\end{align} 
y = 5x^2\\
y'= (5x^2)'
","['calculus', 'derivatives', 'notation']"
61,Differentiate $\mathrm{e}^{x\arctan\left(x\right)}$,Differentiate,\mathrm{e}^{x\arctan\left(x\right)},"I was trying to differentiate the question and I did it in the following 2 ways: METHOD $1:$ Using the cain rule, we get, $$\frac{\,d}{\,dx}\mathrm{e}^{x\arctan\left(x\right)}=\mathrm{e}^{x\arctan\left(x\right)}\frac{\,d}{\,dx}x\arctan\left(x\right)=\mathrm{e}^{x\arctan\left(x\right)}\left\{\arctan\left(x\right)+\frac{x}{x^2+1}\right\}\tag1$$ METHOD $2:$ But, when we try to do implicit differentiation, we get $$y=\mathrm{e}^{x\arctan\left(x\right)}$$ $$\ln y=x\arctan\left(x\right)$$ $$\frac{\,d}{\,dx}\tan\left(\frac{\ln\left(y\right)}{x}\right)=\frac{\,d}{\,dx}x$$ $$\frac{1}{\cos^2\left(\frac{\ln\left(y\right)}{x}\right)}\frac{1}{xy}y’=1$$ $$y’=\cos^2\left(\tan^{-1}\left(x\right)\right)x\mathrm{e}^{x\arctan\left(x\right)}$$ $$y’=\frac{x}{x^2+1}\mathrm{e}^{x\arctan\left(x\right)}$$ But these give different answers, please help me where have I gone wrong.","I was trying to differentiate the question and I did it in the following 2 ways: METHOD Using the cain rule, we get, METHOD But, when we try to do implicit differentiation, we get But these give different answers, please help me where have I gone wrong.","1: \frac{\,d}{\,dx}\mathrm{e}^{x\arctan\left(x\right)}=\mathrm{e}^{x\arctan\left(x\right)}\frac{\,d}{\,dx}x\arctan\left(x\right)=\mathrm{e}^{x\arctan\left(x\right)}\left\{\arctan\left(x\right)+\frac{x}{x^2+1}\right\}\tag1 2: y=\mathrm{e}^{x\arctan\left(x\right)} \ln y=x\arctan\left(x\right) \frac{\,d}{\,dx}\tan\left(\frac{\ln\left(y\right)}{x}\right)=\frac{\,d}{\,dx}x \frac{1}{\cos^2\left(\frac{\ln\left(y\right)}{x}\right)}\frac{1}{xy}y’=1 y’=\cos^2\left(\tan^{-1}\left(x\right)\right)x\mathrm{e}^{x\arctan\left(x\right)} y’=\frac{x}{x^2+1}\mathrm{e}^{x\arctan\left(x\right)}",['derivatives']
62,How to find geometrical figures areas?,How to find geometrical figures areas?,,"Find the area of the region that lies inside the circle $r = 1$ and outside the cardioid $r=1-cos\alpha$ We know that area can not be negative value(at least basic calculus). I wonder where I made a mistake,I tried to show equations. $$ 1=1-cos\alpha $$ $$\alpha = \pi/2,3\pi/2$$ \begin{equation} \frac{1}{2}\int_{3\pi/2}^{\pi/2}f(x)^2d\alpha \end{equation} \begin{equation} \frac{1}{2}(\int_{\pi/2}^{3\pi/2}1^2d\alpha-\int_{\pi/2}^{3\pi/2}(1-cos\alpha)^2d\alpha)   \end{equation} $$ \int_{3\pi/2}^{\pi/2}1^2d\alpha = $$ $$ \frac{3\pi}{2} -\frac{\pi}{2}=\pi $$ $$ \int_{\pi/2}^{3\pi/2}(1-cos\alpha)^2d\alpha = $$ $$ 9\pi/2 - 3\pi/2 + 2 - (-2) + 0 - 0 $$ $$ 2\frac{1}{2}(\pi - 3\pi - 4)$$ $$ Answer: (-2\pi-4) $$ symmterical no required multiple by 2.","Find the area of the region that lies inside the circle and outside the cardioid We know that area can not be negative value(at least basic calculus). I wonder where I made a mistake,I tried to show equations. symmterical no required multiple by 2.","r = 1 r=1-cos\alpha  1=1-cos\alpha  \alpha = \pi/2,3\pi/2 \begin{equation}
\frac{1}{2}\int_{3\pi/2}^{\pi/2}f(x)^2d\alpha
\end{equation} \begin{equation}
\frac{1}{2}(\int_{\pi/2}^{3\pi/2}1^2d\alpha-\int_{\pi/2}^{3\pi/2}(1-cos\alpha)^2d\alpha)  
\end{equation}  \int_{3\pi/2}^{\pi/2}1^2d\alpha =   \frac{3\pi}{2} -\frac{\pi}{2}=\pi   \int_{\pi/2}^{3\pi/2}(1-cos\alpha)^2d\alpha =   9\pi/2 - 3\pi/2 + 2 - (-2) + 0 - 0   2\frac{1}{2}(\pi - 3\pi - 4)  Answer: (-2\pi-4) ","['calculus', 'integration', 'derivatives', 'area']"
63,Prove that quadratic form is differentiable from the definition,Prove that quadratic form is differentiable from the definition,,"I am trying to show that a quadratic form $Q: \mathbb{R}^n \to \mathbb{R}, Q(x)=x^T A x$ is differentiable from the definition of the differential. I started by considering $Q(x+h)=(x+h)^T A (x+h)=x^T A x + x^T A h + h^T A x + h^T A h$ . We need to find a linear map $L_Q: \mathbb{R}^n \to \mathbb{R}$ s.t. $\lim \limits_{h \to 0} \frac{Q(x+h)-Q(x)-L_Q(h)}{\|h\|}$ . Note that $x^T (A + A^T) h = \langle x, (A + A^T) h \rangle$ is a linear map, so this is my candidate for the differential of $Q(x)$ , but I am struggling to show that the error term $r_Q(x)=h^T A h$ decays sublinearly. What I tried was to use the CS inequality to show that $\frac{|r_Q(x)|}{\|h\|} = \frac{|\langle h, Ah\rangle|}{\|h\|} \leq \frac{\|h\| \|Ah\|}{\|h\|} = \|Ah\|$ , but I don't see how I can show that the right-hand side has a limit of 0. Can someone please tell me if I am on the right track and give me some guidance? This is a problem in my lecture notes right after the definiton of the differential, so it should be able to solve this without much more knowdledge. Thanks a lot!","I am trying to show that a quadratic form is differentiable from the definition of the differential. I started by considering . We need to find a linear map s.t. . Note that is a linear map, so this is my candidate for the differential of , but I am struggling to show that the error term decays sublinearly. What I tried was to use the CS inequality to show that , but I don't see how I can show that the right-hand side has a limit of 0. Can someone please tell me if I am on the right track and give me some guidance? This is a problem in my lecture notes right after the definiton of the differential, so it should be able to solve this without much more knowdledge. Thanks a lot!","Q: \mathbb{R}^n \to \mathbb{R}, Q(x)=x^T A x Q(x+h)=(x+h)^T A (x+h)=x^T A x + x^T A h + h^T A x + h^T A h L_Q: \mathbb{R}^n \to \mathbb{R} \lim \limits_{h \to 0} \frac{Q(x+h)-Q(x)-L_Q(h)}{\|h\|} x^T (A + A^T) h = \langle x, (A + A^T) h \rangle Q(x) r_Q(x)=h^T A h \frac{|r_Q(x)|}{\|h\|} = \frac{|\langle h, Ah\rangle|}{\|h\|} \leq \frac{\|h\| \|Ah\|}{\|h\|} = \|Ah\|","['real-analysis', 'derivatives', 'linear-transformations']"
64,Conceptual meaning of a differential,Conceptual meaning of a differential,,"When we find the derivative of $z^2$ with respect to $z$ it means the slope of the graph,Which comes out to be $2z$ . $$ \frac{dz^2}{dz}=2z $$ if we take $dz$ on the other side it becomes $dz^2=2zdz$ which is known as the differnetial of $z^2$ . I am not sure what this means, does it mean if we change $z^2 $ with $dz$ then $z$ changes by $2zdz$ ? If so, then for example I have an equation $z^2+\cos\theta+26$ . Then can I differentiate the sides and get $$2zdz-\sin\theta d\theta=0$$ What does this even mean?","When we find the derivative of with respect to it means the slope of the graph,Which comes out to be . if we take on the other side it becomes which is known as the differnetial of . I am not sure what this means, does it mean if we change with then changes by ? If so, then for example I have an equation . Then can I differentiate the sides and get What does this even mean?",z^2 z 2z  \frac{dz^2}{dz}=2z  dz dz^2=2zdz z^2 z^2  dz z 2zdz z^2+\cos\theta+26 2zdz-\sin\theta d\theta=0,['derivatives']
65,Uniqueness of the Frechet Derivative: the role of $x \in int_X(T)$,Uniqueness of the Frechet Derivative: the role of,x \in int_X(T),"I'm currently trying to learn some functional analysis as a way to improve my ability to read economic theory papers. I've come across what I thought was a simple proof but on reflection I don't think I'm grasping it. I'm not a mathematician so I apologise if this question is rather trivial! My problem lies in the proof of the uniqueness of the Frechet derivative. Here is the definition that I'm using (From Efe OK's book Real Analysis with Economic Applications). Definition Let $X$ and $Y$ be two normed linear spaces and $T$ a subset of $X$ . For any $x \in int_X(T)$ , a map $\Phi : T \rightarrow Y$ is said to be Frechet differentiable at $x$ if there is a continuous linear operator $D_{\Phi,x}\in \mathcal{B}(X,Y)$ such that \begin{equation} \lim_{\omega \rightarrow x} \frac{\Phi(\omega)-\Phi(x)-D_{\Phi,x}(\omega-x)}{\left\lVert \omega-x \right\rVert} = \mathbf{0} \end{equation} The linear operator $D_{\Phi,x}$ is called the Frechet derivative of $\Phi$ at $x$ . The proof proceeds by taking any two $K,L \in \mathcal{B}(X,Y)$ that satisfy the definition of the Frechet derivative, with $D_{\Phi,x} = K$ and $D_{\Phi,x} = L$ . We must then have \begin{equation} \lim_{\omega \rightarrow x} \frac{(K-L)(\omega-x)}{\left\lVert \omega-x \right\rVert} = \mathbf{0} \end{equation} The next step is where I'm confused. Since $int_X(T)$ is open, this is equivalent to saying that \begin{equation} \lim_{v \rightarrow \mathbf{0}} \frac{(K-L)(v)}{\left\lVert v \right\rVert} = \mathbf{0} \end{equation} The rest of the proof is reasonably straightforward. The author provides a warning in the footnotes that if $x \notin int_X(T)$ the final two displayed equations are not equivalent, and the Frechet derivative in this case is not unique. It seems intuitively reasonable that the final two expressions are equivalent but I'm not sure how to show it. My initial thought is that, with $x$ in the boundary, it limits the directions from which one can converge to it.","I'm currently trying to learn some functional analysis as a way to improve my ability to read economic theory papers. I've come across what I thought was a simple proof but on reflection I don't think I'm grasping it. I'm not a mathematician so I apologise if this question is rather trivial! My problem lies in the proof of the uniqueness of the Frechet derivative. Here is the definition that I'm using (From Efe OK's book Real Analysis with Economic Applications). Definition Let and be two normed linear spaces and a subset of . For any , a map is said to be Frechet differentiable at if there is a continuous linear operator such that The linear operator is called the Frechet derivative of at . The proof proceeds by taking any two that satisfy the definition of the Frechet derivative, with and . We must then have The next step is where I'm confused. Since is open, this is equivalent to saying that The rest of the proof is reasonably straightforward. The author provides a warning in the footnotes that if the final two displayed equations are not equivalent, and the Frechet derivative in this case is not unique. It seems intuitively reasonable that the final two expressions are equivalent but I'm not sure how to show it. My initial thought is that, with in the boundary, it limits the directions from which one can converge to it.","X Y T X x \in int_X(T) \Phi : T \rightarrow Y x D_{\Phi,x}\in \mathcal{B}(X,Y) \begin{equation}
\lim_{\omega \rightarrow x} \frac{\Phi(\omega)-\Phi(x)-D_{\Phi,x}(\omega-x)}{\left\lVert \omega-x \right\rVert} = \mathbf{0}
\end{equation} D_{\Phi,x} \Phi x K,L \in \mathcal{B}(X,Y) D_{\Phi,x} = K D_{\Phi,x} = L \begin{equation}
\lim_{\omega \rightarrow x} \frac{(K-L)(\omega-x)}{\left\lVert \omega-x \right\rVert} = \mathbf{0}
\end{equation} int_X(T) \begin{equation}
\lim_{v \rightarrow \mathbf{0}} \frac{(K-L)(v)}{\left\lVert v \right\rVert} = \mathbf{0}
\end{equation} x \notin int_X(T) x","['functional-analysis', 'derivatives', 'frechet-derivative']"
66,Derivatives and definition,Derivatives and definition,,"I’m currently doing a course in Mathematical Analysis at University level. I ask myself a simple question; when you’re finding the derivative of a function, you’re essentially finding the rate at which the output is changing (dy) over the rate at which the input is changing (dx). However, whenever my lecturer says the graph is not differentiable at a certain point (eg |x| not differentiable at the origin), they say that there’s isn’t a unique tangent at the origin for that graph. Could someone help me understand what exactly is meant by unique tangent and derivative at a point? From what I gather: If a function is differentiable at a point, there exists a unique tangent at that point of the graph.","I’m currently doing a course in Mathematical Analysis at University level. I ask myself a simple question; when you’re finding the derivative of a function, you’re essentially finding the rate at which the output is changing (dy) over the rate at which the input is changing (dx). However, whenever my lecturer says the graph is not differentiable at a certain point (eg |x| not differentiable at the origin), they say that there’s isn’t a unique tangent at the origin for that graph. Could someone help me understand what exactly is meant by unique tangent and derivative at a point? From what I gather: If a function is differentiable at a point, there exists a unique tangent at that point of the graph.",,"['real-analysis', 'derivatives', 'tangent-line']"
67,why do we call the constant of integration a constant when really its an unknown variable??,why do we call the constant of integration a constant when really its an unknown variable??,,"when I started learning integration and my teacher called ""C"" a constant I questioned her and asked why we call it a constant when the value is never constant, hoping someone can shed some light on this?","when I started learning integration and my teacher called ""C"" a constant I questioned her and asked why we call it a constant when the value is never constant, hoping someone can shed some light on this?",,"['integration', 'derivatives', 'philosophy']"
68,forward - backward differencing = central differencing,forward - backward differencing = central differencing,,"From Taylor series, we can derive: Forward Differencing Formula: $$ f'(x_{i}) = \frac{f(x_{i+1}) - f(x_{i})}{h} - \frac{f''(x_{i})h}{2!} $$ $$\tag 1 f'(x) \approx \dfrac{f(x+h)-f(x)}{h}$$ (1) Backward Differencing Formula: $$ f'(x_{i}) = \frac{f(x_{i}) - f(x_{i-1})}{h} + \frac{f''(x_{i})h}{2!} $$ $$\tag 1 f'(x) \approx \dfrac{f(x)-f(x-h)}{h}$$ (2) I know that in order to calculate the central differencing formula, we subtract (2) from (1). However, I am unable to do the subtraction properly. I must obtain an error term consisting of 1/6 after the subtraction, along with the central differencing formula. However, I just get 2f(x+h).... which is wrong. A step by step subtraction would be really helpful.","From Taylor series, we can derive: Forward Differencing Formula: (1) Backward Differencing Formula: (2) I know that in order to calculate the central differencing formula, we subtract (2) from (1). However, I am unable to do the subtraction properly. I must obtain an error term consisting of 1/6 after the subtraction, along with the central differencing formula. However, I just get 2f(x+h).... which is wrong. A step by step subtraction would be really helpful.", f'(x_{i}) = \frac{f(x_{i+1}) - f(x_{i})}{h} - \frac{f''(x_{i})h}{2!}  \tag 1 f'(x) \approx \dfrac{f(x+h)-f(x)}{h}  f'(x_{i}) = \frac{f(x_{i}) - f(x_{i-1})}{h} + \frac{f''(x_{i})h}{2!}  \tag 1 f'(x) \approx \dfrac{f(x)-f(x-h)}{h},"['derivatives', 'numerical-methods', 'taylor-expansion', 'numerical-calculus', 'truncation-error']"
69,Derivatives of exponential functions and number $e$,Derivatives of exponential functions and number,e,"How to prove that this thing $ e = (1 + h)^\frac{1}{h}, h \rightarrow 0 \iff (1 + \frac n)^n, n \rightarrow \infty$ goes to some exact value? Is there a proof of this, and if possible, intuition? (#) If we want to find solution of equation $\frac{d}{dx} [a^x] = a^x$ we would easly see that solution is limit above, namely $e.$ But why is that? Is there intuitive reason why that golden value is, on a first sight, jut random irrational number? It's obvious that this 1 in limit is base-value, when time equals 0 ( $e^0 = 1$ ). But I don't see conection in the rest of the formula (limit) :( Also, I looked why $e^x = (1 + \frac{x}{n})^n, n \rightarrow \infty,$ (##) and here goes reasoning (I will always suppose that n goest to infinity): Wee see that $e^{\frac{x}{n}} = 1.$ But also $1 + \frac{x}{n} = 1.$ Therefore, we get (##). Of course, this is just wrong: same ""reasoning"" can be done with any positive base. I must say that now I am confused: for very small $h$ we would have, for example when base is 3, $3^h = 1 + h$ ?? (###) Can you prove (explain) questions above: (#), (###) and can you give me intuitive and clear picture of why we got that strange limit. I can get that number with algebra , but just can't with imagination and logic.","How to prove that this thing goes to some exact value? Is there a proof of this, and if possible, intuition? (#) If we want to find solution of equation we would easly see that solution is limit above, namely But why is that? Is there intuitive reason why that golden value is, on a first sight, jut random irrational number? It's obvious that this 1 in limit is base-value, when time equals 0 ( ). But I don't see conection in the rest of the formula (limit) :( Also, I looked why (##) and here goes reasoning (I will always suppose that n goest to infinity): Wee see that But also Therefore, we get (##). Of course, this is just wrong: same ""reasoning"" can be done with any positive base. I must say that now I am confused: for very small we would have, for example when base is 3, ?? (###) Can you prove (explain) questions above: (#), (###) and can you give me intuitive and clear picture of why we got that strange limit. I can get that number with algebra , but just can't with imagination and logic."," e = (1 + h)^\frac{1}{h}, h \rightarrow 0 \iff (1 + \frac n)^n, n \rightarrow \infty \frac{d}{dx} [a^x] = a^x e. e^0 = 1 e^x = (1 + \frac{x}{n})^n, n \rightarrow \infty, e^{\frac{x}{n}} = 1. 1 + \frac{x}{n} = 1. h 3^h = 1 + h","['real-analysis', 'calculus', 'algebra-precalculus', 'limits', 'derivatives']"
70,Can a time derivative be taken?,Can a time derivative be taken?,,Let $f(t)$ and $g(t)$ both be smooth functions of time and $dh(t)/dt = af(t) - bh(t)$ . At the turning point in $h(t)$ we have $$ af(t) - bh(t) = 0 $$ Can I re-arrange this and take the time derivative of both sides as follows? $$ \frac{a}{b}\frac{df(t)}{dt} = \frac{dh(t)}{dt} = 0 $$ thus generating a condition for the turning point of $h$ to be $$ \frac{df(t)}{dt}  $$,Let and both be smooth functions of time and . At the turning point in we have Can I re-arrange this and take the time derivative of both sides as follows? thus generating a condition for the turning point of to be,"f(t) g(t) dh(t)/dt = af(t) - bh(t) h(t) 
af(t) - bh(t) = 0
 
\frac{a}{b}\frac{df(t)}{dt} = \frac{dh(t)}{dt} = 0
 h 
\frac{df(t)}{dt} 
","['calculus', 'derivatives']"
71,"$g$ not continuous in $(0,0)$, differentiable in every direction AND $|D_vg(x)| \leq |v|$","not continuous in , differentiable in every direction AND","g (0,0) |D_vg(x)| \leq |v|","I have found plenty of simliar questions to mine, but in this case there is one more condition that needs to be satisfied, this is the problem: ""Find a function $g:\mathbb{R}^2 \rightarrow\mathbb{R}$ , so that all directional derivatives $D_v g(x)$ exist ( $v\in\mathbb{R}^2$ ) but $g$ isn't continuous in $(0,0)$ AND $|D_vg(x)| \leq |v|$ ."" It's easy to find a function that satisfies the first two conditions, but I just don't know how to use the third one. I also have a problem in understanding it. For example, I can have 2 vectors which point in the same direction but have different length, for example: $\left( \begin{array}{c} 1\\ 0\\ \end{array} \right)$ and $\left( \begin{array}{c} 0.01\\ 0\\ \end{array} \right)$ , both point into the same direction, but their length isn't the same. With this in mind, $|v|$ can become arbitrary small, meaning that $|D_vg(x)|$ has to be $0$ , so $g$ has to be a constant function. But that doesn't really help, since there is no constant function that is uncontinuous in $(0,0)$ (or is there?), so I guess that I didn't understand the $|D_vg(x)| \leq |v|$ condition properly. Could you give me some advice?","I have found plenty of simliar questions to mine, but in this case there is one more condition that needs to be satisfied, this is the problem: ""Find a function , so that all directional derivatives exist ( ) but isn't continuous in AND ."" It's easy to find a function that satisfies the first two conditions, but I just don't know how to use the third one. I also have a problem in understanding it. For example, I can have 2 vectors which point in the same direction but have different length, for example: and , both point into the same direction, but their length isn't the same. With this in mind, can become arbitrary small, meaning that has to be , so has to be a constant function. But that doesn't really help, since there is no constant function that is uncontinuous in (or is there?), so I guess that I didn't understand the condition properly. Could you give me some advice?","g:\mathbb{R}^2 \rightarrow\mathbb{R} D_v g(x) v\in\mathbb{R}^2 g (0,0) |D_vg(x)| \leq |v| \left(
\begin{array}{c}
1\\
0\\
\end{array}
\right) \left(
\begin{array}{c}
0.01\\
0\\
\end{array}
\right) |v| |D_vg(x)| 0 g (0,0) |D_vg(x)| \leq |v|","['real-analysis', 'derivatives']"
72,Proof of the derivative of a matrix exponential $\frac d {dt} e^{tA} = Ae^{tA}$,Proof of the derivative of a matrix exponential,\frac d {dt} e^{tA} = Ae^{tA},"I have the proposition in my book that $$\frac d {dt} e^{tA} = Ae^{tA}$$ The proof provided is somewhat terse. I think I've proved it using games with indices, but the book's preferred proof uses the defn of the derivative. In particular, I'm annoyed by not understanding this line. $$\frac d {dt} e^{tA}=e^{tA} \lim_{h\rightarrow 0} \frac {e^{hA} -I}{h}$$ (we're fine and good) $$= e^{tA}A$$ (huh?)  This would imply that the numerator of the limit is equal to $hA$ , which I am uncertain of how to prove. I have tried expanding $e^{hA}$ according to the defn of the matrix exponential, but see no further steps.","I have the proposition in my book that The proof provided is somewhat terse. I think I've proved it using games with indices, but the book's preferred proof uses the defn of the derivative. In particular, I'm annoyed by not understanding this line. (we're fine and good) (huh?)  This would imply that the numerator of the limit is equal to , which I am uncertain of how to prove. I have tried expanding according to the defn of the matrix exponential, but see no further steps.",\frac d {dt} e^{tA} = Ae^{tA} \frac d {dt} e^{tA}=e^{tA} \lim_{h\rightarrow 0} \frac {e^{hA} -I}{h} = e^{tA}A hA e^{hA},"['matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential']"
73,Term-wise Differentiation of an Infinite Series,Term-wise Differentiation of an Infinite Series,,"One way to prove if an infinite sum is to use the following theorem Theorem Suppose $\{f_n\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $f_n(x_0)$ converges for some point $x_0$ on $[a,b]$ . If $f_n^\prime$ converges uniformly on $[a,b]$ , then $f_n$ converges uniformly on $[a, b]$ , to a function $f$ , and $$ f^\prime(x) = \lim_{n\to\infty} f_n^\prime(x) \qquad (a \leq x \leq b) $$ Does this theorem have a converse? An example of what I mean: In Elementary Classical Analysis (Marsden) there is the following exercise (p. 316, #52): Can the series $$x = \sum_{k=1}^\infty \frac{x^k}{k} - \frac{x^{k+1}}{k+1} \quad 0 \leq x \leq 1 $$ be diﬀerentiated term by term? Letting $S_n =  \sum_{k=1}^n \frac{x^k}{k} - \frac{x^{k+1}}{k+1} = x-\frac{x^{n+1}}{n+1} $ , then $S_n' = 1  - x^{n+1}$ . This converges uniformly to $1$ on $[0,r], r < 1$ . So, on $[0,r], r < 1$ , we can differentiate term by term by the theorem. (term-by-term is an odd way of expressing the point of this exercise I feel. I think it is phrased this way because $S_n$ are differentiated term by term.) In this case, we know that the sum is equal to $x$ , so the derivative is equal to $1$ . The term-wise differentiation yields $0$ at $x=1$ , so term-wise differentiation is not possible at $x=1$ . But is there a general method to disproving this? (i.e., if the series was not equal to something that is easy to differentiate, can we conclude anything about it's differentiability from this theorem?","One way to prove if an infinite sum is to use the following theorem Theorem Suppose is a sequence of functions, differentiable on and such that converges for some point on . If converges uniformly on , then converges uniformly on , to a function , and Does this theorem have a converse? An example of what I mean: In Elementary Classical Analysis (Marsden) there is the following exercise (p. 316, #52): Can the series be diﬀerentiated term by term? Letting , then . This converges uniformly to on . So, on , we can differentiate term by term by the theorem. (term-by-term is an odd way of expressing the point of this exercise I feel. I think it is phrased this way because are differentiated term by term.) In this case, we know that the sum is equal to , so the derivative is equal to . The term-wise differentiation yields at , so term-wise differentiation is not possible at . But is there a general method to disproving this? (i.e., if the series was not equal to something that is easy to differentiate, can we conclude anything about it's differentiability from this theorem?","\{f_n\} [a, b] f_n(x_0) x_0 [a,b] f_n^\prime [a,b] f_n [a, b] f 
f^\prime(x) = \lim_{n\to\infty} f_n^\prime(x) \qquad (a \leq x \leq b)
 x = \sum_{k=1}^\infty \frac{x^k}{k} - \frac{x^{k+1}}{k+1} \quad 0 \leq x \leq 1  S_n =  \sum_{k=1}^n \frac{x^k}{k} - \frac{x^{k+1}}{k+1} = x-\frac{x^{n+1}}{n+1}  S_n' = 1  - x^{n+1} 1 [0,r], r < 1 [0,r], r < 1 S_n x 1 0 x=1 x=1","['real-analysis', 'calculus', 'sequences-and-series', 'derivatives', 'uniform-convergence']"
74,Question about the proof of Fubini's 'other' theorem on the term by term differentiation of infinite sum of increasing functions,Question about the proof of Fubini's 'other' theorem on the term by term differentiation of infinite sum of increasing functions,,"This is problem 25.15 Fubini's 'other' theorem from Rene Schilling's Measures, Integrals, and Martingales. Let $(f_n)_n$ be a sequence of monotone increasing functions $f_n: [a,b] \to \mathbb{R}$ . If the series $s(x):= \sum_{n=1}^\infty f_n(x)$ converges, then $s'(x)$ exists a.e. and is given by $s'(x)=\sum_{n=1}^\infty f_n'(x)$ a.e. Below is the solution to this problem. I can follow all steps of the proof except the last, where it says the first part of the proof applied to this series implies that we can differentiate this series term by term and $\sum_k (s'(x)-s_{n_k}'(x)) $ converges. I can't figure out which part of the proof justifies term by term differentiation here. I would greatly appreciate any help.","This is problem 25.15 Fubini's 'other' theorem from Rene Schilling's Measures, Integrals, and Martingales. Let be a sequence of monotone increasing functions . If the series converges, then exists a.e. and is given by a.e. Below is the solution to this problem. I can follow all steps of the proof except the last, where it says the first part of the proof applied to this series implies that we can differentiate this series term by term and converges. I can't figure out which part of the proof justifies term by term differentiation here. I would greatly appreciate any help.","(f_n)_n f_n: [a,b] \to \mathbb{R} s(x):= \sum_{n=1}^\infty f_n(x) s'(x) s'(x)=\sum_{n=1}^\infty f_n'(x) \sum_k (s'(x)-s_{n_k}'(x)) ","['real-analysis', 'analysis', 'measure-theory', 'derivatives']"
75,Big O notation and derivative,Big O notation and derivative,,"Given a function $f(x) = x^2 + g(x)$ such that $g(x) = O(x)$ and that $f'(x)$ is monotonic non-decreasing for all $x \geq x_0$ I need to prove that $g'(x) = O(\sqrt{x})$ , and also that without the monotonic condtion such implication is false For the latter question I have $g(x) = \sin(x^2) = O( x)$ but $g'(x) = 2x \cos(x^2) = O(x)$ . Which means that the solution to the first question $g'(x) = O(\sqrt{x})$ depends on the fact that $f'$ is monotonic but I couldn't prove it. How to prove it then?","Given a function such that and that is monotonic non-decreasing for all I need to prove that , and also that without the monotonic condtion such implication is false For the latter question I have but . Which means that the solution to the first question depends on the fact that is monotonic but I couldn't prove it. How to prove it then?",f(x) = x^2 + g(x) g(x) = O(x) f'(x) x \geq x_0 g'(x) = O(\sqrt{x}) g(x) = \sin(x^2) = O( x) g'(x) = 2x \cos(x^2) = O(x) g'(x) = O(\sqrt{x}) f',"['calculus', 'number-theory', 'derivatives', 'asymptotics']"
76,Differentiation using product rule?,Differentiation using product rule?,,"I'm stuck on differentiating this: $$f(x) = \frac{4\sin(2x)}{e^\sqrt{2x-1}}$$ I thought about using the product rule here, but when I do that I get an expression that is hard to simplify, and I need to solve for when $f(x) = 0$ . Is there a simpler way of doing this? Help here would be much appreciated!","I'm stuck on differentiating this: I thought about using the product rule here, but when I do that I get an expression that is hard to simplify, and I need to solve for when . Is there a simpler way of doing this? Help here would be much appreciated!",f(x) = \frac{4\sin(2x)}{e^\sqrt{2x-1}} f(x) = 0,['derivatives']
77,How do I establish the differentiability of this function by definition?,How do I establish the differentiability of this function by definition?,,"Given this function $f(x) = (x^2 + 1)^{\sin(x)} $ How would I establish its differentiability over the entire function? I understand how to establish differentiability at a point $c$ , by assessing the limit of the different quotient at $x = c$ . But how would I establish the differentiability of the entire function?","Given this function How would I establish its differentiability over the entire function? I understand how to establish differentiability at a point , by assessing the limit of the different quotient at . But how would I establish the differentiability of the entire function?",f(x) = (x^2 + 1)^{\sin(x)}  c x = c,"['calculus', 'derivatives', 'continuity']"
78,Make sense of norm notations and why is $\int_\Omega \nabla\theta\cdot\nabla\theta_t \ d\mathbf{x} = \frac{1}{2}\frac{d}{dt}|\theta|_1^2$?,Make sense of norm notations and why is ?,\int_\Omega \nabla\theta\cdot\nabla\theta_t \ d\mathbf{x} = \frac{1}{2}\frac{d}{dt}|\theta|_1^2,"I have the heat equation \begin{align} \dot{u}(x,t) -\Delta u(x,t) = f(x,t),& \quad x\in\Omega\subset\mathbb{R}^2 \\ u(x,t) = 0, & \quad x\in\Gamma = \partial\Omega, 0<t\leq T \\ u(x,0) = u_0(x),& \quad x\in\Omega \end{align} In my book there is a proof about a stability estimate. In one of the lines they state that $$a(u,u_t)=\int_\Omega \nabla u\cdot\nabla u_t \ d\mathbf{x} = \frac{1}{2}\frac{d}{dt}|u|_1^2.$$ How does the last equality follow? Need help getting through the arithmetic there. What does the sub-index $1$ mean in the absolute value? EDIT: Adding an attempt. Attempt: $$\int_\Omega \nabla\theta\cdot\nabla\theta_t \ dx =  \int_\Omega\nabla\theta\cdot \nabla\left(\frac{d}{dt}\theta\right) \ dx$$ $$=\int_\Omega \nabla\theta\cdot\frac{d}{dt}\nabla\theta \ dx =\frac{1}{2}\frac{d}{dt}\int_\Omega(\nabla\theta)^2 \ dx =\frac{1}{2}\frac{d}{dt}||\nabla\theta||^2.$$ But why does the book have $\frac{1}{2}\frac{d}{dt}|\theta|_1^2$ instead?",I have the heat equation In my book there is a proof about a stability estimate. In one of the lines they state that How does the last equality follow? Need help getting through the arithmetic there. What does the sub-index mean in the absolute value? EDIT: Adding an attempt. Attempt: But why does the book have instead?,"\begin{align}
\dot{u}(x,t) -\Delta u(x,t) = f(x,t),& \quad x\in\Omega\subset\mathbb{R}^2 \\
u(x,t) = 0, & \quad x\in\Gamma = \partial\Omega, 0<t\leq T \\
u(x,0) = u_0(x),& \quad x\in\Omega
\end{align} a(u,u_t)=\int_\Omega \nabla u\cdot\nabla u_t \ d\mathbf{x} = \frac{1}{2}\frac{d}{dt}|u|_1^2. 1 \int_\Omega \nabla\theta\cdot\nabla\theta_t \ dx =  \int_\Omega\nabla\theta\cdot \nabla\left(\frac{d}{dt}\theta\right) \ dx =\int_\Omega \nabla\theta\cdot\frac{d}{dt}\nabla\theta \ dx =\frac{1}{2}\frac{d}{dt}\int_\Omega(\nabla\theta)^2 \ dx =\frac{1}{2}\frac{d}{dt}||\nabla\theta||^2. \frac{1}{2}\frac{d}{dt}|\theta|_1^2","['integration', 'derivatives', 'normed-spaces']"
79,Differentiating secant inverse,Differentiating secant inverse,,"I want to show that $\cfrac{d}{dx}[\operatorname{arcsec} x]=\cfrac{1}{\vert x \vert\sqrt{x^2-1}}$ . Here is my attempt: \begin{align*} y=\sec^{-1}x &\iff x=\sec y \\ \frac{d}{dx}[x]&=\frac{d}{dx}[\sec y] \\ 1 &=\frac{\cos^2y}{\sin y} \frac{dy}{dx} \\ \frac{dy}{dx} &= \frac{\sin y}{\cos^2y} \end{align*} I tried using the Pythagorean identity for 1 on the lefthand side, but couldn't see what else that would bring. I'm starting to think implicit differentiation isn't going to let me derive the expression I'm looking for. Any ideas?","I want to show that . Here is my attempt: I tried using the Pythagorean identity for 1 on the lefthand side, but couldn't see what else that would bring. I'm starting to think implicit differentiation isn't going to let me derive the expression I'm looking for. Any ideas?",\cfrac{d}{dx}[\operatorname{arcsec} x]=\cfrac{1}{\vert x \vert\sqrt{x^2-1}} \begin{align*} y=\sec^{-1}x &\iff x=\sec y \\ \frac{d}{dx}[x]&=\frac{d}{dx}[\sec y] \\ 1 &=\frac{\cos^2y}{\sin y} \frac{dy}{dx} \\ \frac{dy}{dx} &= \frac{\sin y}{\cos^2y} \end{align*},"['calculus', 'derivatives', 'trigonometry']"
80,Derivative of dot product formula proof.,Derivative of dot product formula proof.,,"Let $f: \mathbb{R}^n \to \mathbb{R}^m$ and $g: \mathbb{R}^n \to \mathbb{R}^m$ Define $h : \mathbb{R}^n \to \mathbb{R}$ as $h = f.g$ Suppose $f, g$ are differentiable at $y$ which is defined as: $\forall \epsilon > 0$ , $\exists \delta > 0$ such that $|x - y| < \delta$ $\implies $ $$\frac{\Big | f(x) - f(y) - df(y)(x - y) \Big |}{|x - y|} < \epsilon  $$ for some $m \times n$ matrix $df(y)$ . Now, I want to prove $h$ is differentiable at $y$ , i.e. there exists matrix $M(y) \in \mathbb{R}^{1 \times n}$ such that $$\frac{\Big |h(x) - h(y) - M(y)(x - y) \Big |}{|x - y|} < \epsilon \, \, \,\, \,\, \,\, \,\, \,\, \,\, \, (1)$$ Fix some $\epsilon> 0$ Let's consider $M(y) = g(y)^T df(y) + f(y)^T dg(y)$ . Further define: $F(x, y) = f(x) - f(y) - df(y)(x - y)$ $G(x, y) = g(x) - g(y) - dg(y)(x - y)$ Then, after a lot of simplification, I got that $|h(x) - h(y) - M(y)(x - y)|$ is equal to: $$\big |g(y)^T F(x, y) + f(y)^T G(x, y) + \left( g(x) - g(y) \right).\left( f(x)-f(y) \right) \big|$$ Then we have: \begin{align*} \frac{\big |g(y)^T F(x, y) + f(y)^T G(x, y) + \left( g(x) - g(y) \right).\left( f(x)-f(y) \right) \big|}{|x - y|} \end{align*} \begin{align*} &\leq |g(y)| \frac{|F(x, y)|}{|x - y|} + |f(y)| \frac{|G(x, y)|}{|x - y|} + \frac{|g(x) - g(y)| |f(x) - f(y)|}{|x - y|}   \end{align*} Since $f$ is differentiable at $y$ , there exists $\delta_1 > 0$ such that $|x - y| < \delta_1$ $\implies $ $\frac{|F(x, y)|}{|x - y|} < \frac{\epsilon}{3 \, |g(y)|}$ . Since $g$ is differentiable at $y$ , there exists $\delta_2 > 0$ such that $|x - y| < \delta_2$ $\implies $ $\frac{|G(x, y)|}{|x - y|} < \frac{\epsilon}{3 \,|f(y)|}$ . As $x \to y$ , we have $\frac{|f(x) - f(y)|}{|x - y|} \to |f'(y)|$ Since $g$ is continuous at $y$ , there exists $\delta_3 > 0$ such that $|x - y| < \delta_3$ $\implies $ $|g(x) - g(y)| < \frac{\epsilon}{3 |f'(y)|}$ Note that $y$ is fixed so the above 4 points make sense. Finally, choosing $\delta = \min \left(\delta_1, \delta_2, \delta_3 \right) $ and considering $|x - y| < \delta$ and further letting $x \to y$ , we have that \begin{align*}     |g(y)| \frac{|F(x, y)|}{|x - y|} + |f(y)| \frac{|G(x, y)|}{|x - y|} + \frac{|g(x) - g(y)| |f(x) - f(y)|}{|x - y|} < \epsilon \end{align*}","Let and Define as Suppose are differentiable at which is defined as: , such that for some matrix . Now, I want to prove is differentiable at , i.e. there exists matrix such that Fix some Let's consider . Further define: Then, after a lot of simplification, I got that is equal to: Then we have: Since is differentiable at , there exists such that . Since is differentiable at , there exists such that . As , we have Since is continuous at , there exists such that Note that is fixed so the above 4 points make sense. Finally, choosing and considering and further letting , we have that","f: \mathbb{R}^n \to \mathbb{R}^m g: \mathbb{R}^n \to \mathbb{R}^m h : \mathbb{R}^n \to \mathbb{R} h = f.g f, g y \forall \epsilon > 0 \exists \delta > 0 |x - y| < \delta \implies  \frac{\Big | f(x) - f(y) - df(y)(x - y) \Big |}{|x - y|} < \epsilon   m \times n df(y) h y M(y) \in \mathbb{R}^{1 \times n} \frac{\Big |h(x) - h(y) - M(y)(x - y) \Big |}{|x - y|} < \epsilon \, \, \,\, \,\, \,\, \,\, \,\, \,\, \, (1) \epsilon> 0 M(y) = g(y)^T df(y) + f(y)^T dg(y) F(x, y) = f(x) - f(y) - df(y)(x - y) G(x, y) = g(x) - g(y) - dg(y)(x - y) |h(x) - h(y) - M(y)(x - y)| \big |g(y)^T F(x, y) + f(y)^T G(x, y) + \left( g(x) - g(y) \right).\left( f(x)-f(y) \right) \big| \begin{align*}
\frac{\big |g(y)^T F(x, y) + f(y)^T G(x, y) + \left( g(x) - g(y) \right).\left( f(x)-f(y) \right) \big|}{|x - y|}
\end{align*} \begin{align*}
&\leq |g(y)| \frac{|F(x, y)|}{|x - y|} + |f(y)| \frac{|G(x, y)|}{|x - y|} + \frac{|g(x) - g(y)| |f(x) - f(y)|}{|x - y|}  
\end{align*} f y \delta_1 > 0 |x - y| < \delta_1 \implies  \frac{|F(x, y)|}{|x - y|} < \frac{\epsilon}{3 \, |g(y)|} g y \delta_2 > 0 |x - y| < \delta_2 \implies  \frac{|G(x, y)|}{|x - y|} < \frac{\epsilon}{3 \,|f(y)|} x \to y \frac{|f(x) - f(y)|}{|x - y|} \to |f'(y)| g y \delta_3 > 0 |x - y| < \delta_3 \implies  |g(x) - g(y)| < \frac{\epsilon}{3 |f'(y)|} y \delta = \min \left(\delta_1, \delta_2, \delta_3 \right)  |x - y| < \delta x \to y \begin{align*}
    |g(y)| \frac{|F(x, y)|}{|x - y|} + |f(y)| \frac{|G(x, y)|}{|x - y|} + \frac{|g(x) - g(y)| |f(x) - f(y)|}{|x - y|} < \epsilon
\end{align*}","['real-analysis', 'limits', 'derivatives', 'continuity', 'solution-verification']"
81,"If $y = e^{5x} + e^{-5y}$, prove that $y'' = 25y$","If , prove that",y = e^{5x} + e^{-5y} y'' = 25y,"Question: $y = e^{5x} + e^{-5y}$ ,  prove  that $y'' = 25y$ what i have done: $y - e^{-5y}= e^{5x} $ $$(1 + 5e^{-5y})y'= 5e^{5x} $$ $$y'= \frac{5e^{5x}}{(1 + 5e^{-5y})} $$ $$y'' = 5\frac{\bigg[ 5e^{5x}(1+5e^{-5y}) - e^{5x}(-25e^{-5y})y' \bigg]}{(1+5e^{-5y})^2}$$ $$y'' = 25\frac{\bigg[ e^{5x}(1+5e^{-5y}) + e^{5x}(5e^{-5y})\frac{5e^{5x}}{(1 + 5e^{-5y})}) \bigg]}{(1+5e^{-5y})^2}$$ $$y'' = 25{\bigg[ e^{5x}\frac{1}{(1+5e^{-5y})} + e^{-5y}\frac{25e^{5x}e^{5x}}{(1+5e^{-5y})^3} \bigg]}$$ This last step here tells me I have gone wrong somewhere, somehow. But I just can't figure out where. Any hints to get the correct answer??","Question: ,  prove  that what i have done: This last step here tells me I have gone wrong somewhere, somehow. But I just can't figure out where. Any hints to get the correct answer??",y = e^{5x} + e^{-5y} y'' = 25y y - e^{-5y}= e^{5x}  (1 + 5e^{-5y})y'= 5e^{5x}  y'= \frac{5e^{5x}}{(1 + 5e^{-5y})}  y'' = 5\frac{\bigg[ 5e^{5x}(1+5e^{-5y}) - e^{5x}(-25e^{-5y})y' \bigg]}{(1+5e^{-5y})^2} y'' = 25\frac{\bigg[ e^{5x}(1+5e^{-5y}) + e^{5x}(5e^{-5y})\frac{5e^{5x}}{(1 + 5e^{-5y})}) \bigg]}{(1+5e^{-5y})^2} y'' = 25{\bigg[ e^{5x}\frac{1}{(1+5e^{-5y})} + e^{-5y}\frac{25e^{5x}e^{5x}}{(1+5e^{-5y})^3} \bigg]},"['calculus', 'derivatives', 'implicit-differentiation']"
82,Function being differentiable vs. derivative expression being undefined,Function being differentiable vs. derivative expression being undefined,,"Here's my confusion: My teacher, as well as some online sources such as Khan Academy, seem to assume that the derivative expression being undefined at a point implies that the function being differentiated is not differentiable at that point. Consider, for example, $g(x)=x^{1/3}$ . The second derivative is $g''(x)=-2/9*x^{-5/3}$ . In this Khan Academy video , the speaker concludes that the second derivative doesn't exist at $x=0$ because if you plug zero into the $g''$ expression you end up dividing by zero. But why does that conclusion follow? How can we be sure the expression is defined wherever the function is twice differentiable? Along similar lines, when doing implicit differentiation in class, we were taught that a function $y$ fails to be differentiable when the expression we get for $dy/dx$ is undefined. For example, if $x^2+2xy+2y^2=1$ , then we found $\frac{dy}{dx}=\frac{(-2x-2y)}{(2x+4y)}$ . We were told that to find where $y$ fails to be differentiable, we should set $2x+4y = 0$ , because that is the denominator of our derivative expression and we can't divide by zero. But again, as asked above, why can we be certain that having the derivative expression undefined implies that $y$ isn't differentiable? Finally, I'll note that there's at least one example where I've noticed disconnect between where the expression is defined and where the derivative exists . That example is $f(x)=\ln(x)$ . That is obviously not differentiable for $x<0$ , yet the derivative expression, $f'(x)=1/x$ , is defined for $x<0$ (it's only undefined if $x=0$ ). How do we reconcile that with what I've written in the preceding paragraphs? I hope my question is clear. I can clarify if necessary - I realize it's a bit complicated and lengthy.","Here's my confusion: My teacher, as well as some online sources such as Khan Academy, seem to assume that the derivative expression being undefined at a point implies that the function being differentiated is not differentiable at that point. Consider, for example, . The second derivative is . In this Khan Academy video , the speaker concludes that the second derivative doesn't exist at because if you plug zero into the expression you end up dividing by zero. But why does that conclusion follow? How can we be sure the expression is defined wherever the function is twice differentiable? Along similar lines, when doing implicit differentiation in class, we were taught that a function fails to be differentiable when the expression we get for is undefined. For example, if , then we found . We were told that to find where fails to be differentiable, we should set , because that is the denominator of our derivative expression and we can't divide by zero. But again, as asked above, why can we be certain that having the derivative expression undefined implies that isn't differentiable? Finally, I'll note that there's at least one example where I've noticed disconnect between where the expression is defined and where the derivative exists . That example is . That is obviously not differentiable for , yet the derivative expression, , is defined for (it's only undefined if ). How do we reconcile that with what I've written in the preceding paragraphs? I hope my question is clear. I can clarify if necessary - I realize it's a bit complicated and lengthy.",g(x)=x^{1/3} g''(x)=-2/9*x^{-5/3} x=0 g'' y dy/dx x^2+2xy+2y^2=1 \frac{dy}{dx}=\frac{(-2x-2y)}{(2x+4y)} y 2x+4y = 0 y f(x)=\ln(x) x<0 f'(x)=1/x x<0 x=0,"['calculus', 'derivatives']"
83,"Show: $e^{y^2}-e^{x^2} \le e(y-x)(y+x)$ for $x,y \in [0,1], 0 \lt x \lt y$.",Show:  for .,"e^{y^2}-e^{x^2} \le e(y-x)(y+x) x,y \in [0,1], 0 \lt x \lt y","Let $x,y \in [0,1], 0 \lt x \lt y$ . Show: $e^{y^2}-e^{x^2} \le e(y-x)(y+x)$ . I tried to solve it with the mean value theorem: Let $f: [0,1] \rightarrow \mathbb{R}, \ f(x):=e^{x^2}$ , thus $f'(x)=2xe^{x^2}$ . By the mean value theorem, it holds that: $\exists \lambda \in (0,1): f(y)-f(x) = f'(\lambda)(y-x)$ . It follows: $$\begin{align} f(y)-f(x) &= e^{y^2}-e^{y^2} \\ &= f'(\lambda)(y-x) = (2\lambda e^{\lambda^2})(y-x) \\ &\le (2e)(y-x) \\ &= e(2y-2x) \\ &= e(\sqrt{2y}-\sqrt{2x})(\sqrt{2y}+\sqrt{2x}). \end{align}$$ I don't know how to go on from there on. I had the idea to split up $e(2y-2x)$ to $e(\sqrt{2y}-\sqrt{2x})(\sqrt{2y}+\sqrt{2x})$ so that I would maybe get to $e(y-x)(y+x)$ , but that didn't work out for me. Any help is appreciated.","Let . Show: . I tried to solve it with the mean value theorem: Let , thus . By the mean value theorem, it holds that: . It follows: I don't know how to go on from there on. I had the idea to split up to so that I would maybe get to , but that didn't work out for me. Any help is appreciated.","x,y \in [0,1], 0 \lt x \lt y e^{y^2}-e^{x^2} \le e(y-x)(y+x) f: [0,1] \rightarrow \mathbb{R}, \ f(x):=e^{x^2} f'(x)=2xe^{x^2} \exists \lambda \in (0,1): f(y)-f(x) = f'(\lambda)(y-x) \begin{align} f(y)-f(x) &= e^{y^2}-e^{y^2} \\ &= f'(\lambda)(y-x) = (2\lambda e^{\lambda^2})(y-x) \\ &\le (2e)(y-x) \\ &= e(2y-2x) \\ &= e(\sqrt{2y}-\sqrt{2x})(\sqrt{2y}+\sqrt{2x}). \end{align} e(2y-2x) e(\sqrt{2y}-\sqrt{2x})(\sqrt{2y}+\sqrt{2x}) e(y-x)(y+x)","['real-analysis', 'analysis', 'derivatives', 'inequality', 'exponential-function']"
84,Fundamental theorem of calculus with chain rule,Fundamental theorem of calculus with chain rule,,"I'm trying to prove that if we define $$F(x) =\int_{g(x)}^{h(x)} f(t) dt$$ If $f$ is continous in $x_0$ , $g(x)$ , $h(x)$ are differentiable in $x_0$ , and the range of a neighbourhood of $h(x_0)$ and $g(x_0)$ is included in the domain of $f$ then $F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)$ I tried to divide the proof in $g(x_0)<h(x_0)$ , $h(x_0)<g(x_0)$ and $g(x_0)=h(x_0)$ When $g(x_0)<h(x_0)$ , what I did is define $$M(y) = \int _a^y f(t) dt$$ $$N(y) = \int_y^a f(t) dt$$ And I have proven that if $f$ is continous in $x_0$ : $$M'(x_0)=f(x_0)$$ $$N'(x_0)=-f(x_0)$$ As $F(x) = M(h(x)) + N(g(x)) $ when $g(x)<h(x)$ and this holds in a neighbourhood of $x_0$ , $(x_0-ε,x_0+ε)$ as $g$ and $h$ are continous in $x_0$ , it follows that $$F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)$$ Similarly we prove the case when $h(x_0)<g(x_0)$ , using that $$F(x) =\int_{g(x)}^{h(x)} f(t) dt =-\int_{h(x)}^{g(x)} f(t) dt$$ The problem comes when $g(x_0)=h(x_0)$ , using $$F(x_0)= \int_{g(x_0)}^{g(x_0)} f(t) dt = 0$$ as I don't know what happen in a neighbourhood of $x_0$ I tried to do $$F'(x_0)=\lim_{h\to 0}\frac{F(x_0+h)-F(x_0)}{h}=\lim_{h\to 0}\frac{F(x_0+h)}{h}$$ but I didn't get anything and I don't know what else should I do. Edit: I change the way to prove it and, with the same inicial conditions, I define $$M(y) = \int _a^y f(t) dt$$ We also have that if $f$ is continous in $x_0$ , then: $$M'(x_0)=f(x_0)$$ Now we have $F(x_0)=M(h(x_0))-M(g(x_0))$ and this holds for all 3 cases, so: $$F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)$$","I'm trying to prove that if we define If is continous in , , are differentiable in , and the range of a neighbourhood of and is included in the domain of then I tried to divide the proof in , and When , what I did is define And I have proven that if is continous in : As when and this holds in a neighbourhood of , as and are continous in , it follows that Similarly we prove the case when , using that The problem comes when , using as I don't know what happen in a neighbourhood of I tried to do but I didn't get anything and I don't know what else should I do. Edit: I change the way to prove it and, with the same inicial conditions, I define We also have that if is continous in , then: Now we have and this holds for all 3 cases, so:","F(x) =\int_{g(x)}^{h(x)} f(t) dt f x_0 g(x) h(x) x_0 h(x_0) g(x_0) f F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0) g(x_0)<h(x_0) h(x_0)<g(x_0) g(x_0)=h(x_0) g(x_0)<h(x_0) M(y) = \int _a^y f(t) dt N(y) = \int_y^a f(t) dt f x_0 M'(x_0)=f(x_0) N'(x_0)=-f(x_0) F(x) = M(h(x)) + N(g(x))  g(x)<h(x) x_0 (x_0-ε,x_0+ε) g h x_0 F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0) h(x_0)<g(x_0) F(x) =\int_{g(x)}^{h(x)} f(t) dt =-\int_{h(x)}^{g(x)} f(t) dt g(x_0)=h(x_0) F(x_0)= \int_{g(x_0)}^{g(x_0)} f(t) dt = 0 x_0 F'(x_0)=\lim_{h\to 0}\frac{F(x_0+h)-F(x_0)}{h}=\lim_{h\to 0}\frac{F(x_0+h)}{h} M(y) = \int _a^y f(t) dt f x_0 M'(x_0)=f(x_0) F(x_0)=M(h(x_0))-M(g(x_0)) F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)","['calculus', 'integration', 'derivatives', 'chain-rule']"
85,Differentiation/ implicit differentiation question,Differentiation/ implicit differentiation question,,"Confused about how these two equate to each other: $$\frac{d}{dx}\left(\frac{dy}{dx}\right)=\frac{dy}{dx}\frac{d}{dy}\left(\frac{dy}{dx}\right)$$ Would anyone kindly explain how to get from the left side to the right? (The left side being the second derivative with respect to x, $\frac{d^2y}{dx^2}$ ) I'm unsure what exactly differentiating with respect to y does for the equation. Many thanks","Confused about how these two equate to each other: Would anyone kindly explain how to get from the left side to the right? (The left side being the second derivative with respect to x, ) I'm unsure what exactly differentiating with respect to y does for the equation. Many thanks",\frac{d}{dx}\left(\frac{dy}{dx}\right)=\frac{dy}{dx}\frac{d}{dy}\left(\frac{dy}{dx}\right) \frac{d^2y}{dx^2},"['derivatives', 'implicit-differentiation']"
86,Find $m$ if $f(x)=x^m\sin\frac{1}{x}$ is continuous and is not differentiable,Find  if  is continuous and is not differentiable,m f(x)=x^m\sin\frac{1}{x},"If $f(x)=\begin{cases} x^m\sin\dfrac{1}{x}, & x\ne 0 \\ 0, & x=0		  \end{cases}$ . Find $m$ if $f(x)$ is continuous and is not differentiable My attempt is as follows:- Let's find the condition of continuity $$\lim_{x\to0^{+}}x^m\sin\dfrac{1}{x}$$ As $x\rightarrow 0^{+}, \dfrac{1}{x}\rightarrow \infty,\sin\dfrac{1}{x} \text { oscillates in }  [-1,1]$ $$m>0$$ $$\lim_{x\to0^{-}}x^m\sin\dfrac{1}{x}$$ As we have the negative base $$m>0 \cap m\notin \left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\}\tag{1}$$ Let's find the condition of non-differentiability $\lim_{h\to 0}\dfrac{h^m\sin\dfrac{1}{h}}{h}$ should not exist $$\lim_{h\to 0^{+}}h^{m-1}\sin\dfrac{1}{h}$$ $$m\le0$$ $$\lim_{h\to 0^{-}}h^{m-1}\sin\dfrac{1}{h}$$ $$m-1\le 0 \cup m-1\in\left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\}$$ $$m\le 1 \cup m\in\left\{\dfrac{p+q}{q} | p,q \text { are coprime and } q \text { is even }\right\}\tag{2}$$ Taking intersection of equations $(1)$ and $(2)$ $$\left(m\in(0,1] \cap m\notin \left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\}\right) \cup  \left(m>0 \cap m\notin \left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\} \cap m\in\left\{\dfrac{p+q}{q} | p,q \text { are coprime and } q \text { is even }\right\}\right)$$ But actual answer is simply $m\in(0,1]$",If . Find if is continuous and is not differentiable My attempt is as follows:- Let's find the condition of continuity As As we have the negative base Let's find the condition of non-differentiability should not exist Taking intersection of equations and But actual answer is simply,"f(x)=\begin{cases}
x^m\sin\dfrac{1}{x}, & x\ne 0 \\
0, & x=0		 
\end{cases} m f(x) \lim_{x\to0^{+}}x^m\sin\dfrac{1}{x} x\rightarrow 0^{+}, \dfrac{1}{x}\rightarrow \infty,\sin\dfrac{1}{x} \text { oscillates in }  [-1,1] m>0 \lim_{x\to0^{-}}x^m\sin\dfrac{1}{x} m>0 \cap m\notin \left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\}\tag{1} \lim_{h\to 0}\dfrac{h^m\sin\dfrac{1}{h}}{h} \lim_{h\to 0^{+}}h^{m-1}\sin\dfrac{1}{h} m\le0 \lim_{h\to 0^{-}}h^{m-1}\sin\dfrac{1}{h} m-1\le 0 \cup m-1\in\left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\} m\le 1 \cup m\in\left\{\dfrac{p+q}{q} | p,q \text { are coprime and } q \text { is even }\right\}\tag{2} (1) (2) \left(m\in(0,1] \cap m\notin \left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\}\right) \cup  \left(m>0 \cap m\notin \left\{\dfrac{p}{q} | p,q \text { are coprime and } q \text { is even }\right\} \cap m\in\left\{\dfrac{p+q}{q} | p,q \text { are coprime and } q \text { is even }\right\}\right) m\in(0,1]","['calculus', 'limits', 'derivatives']"
87,"Verify $JJ' = 1$, given $x = e^u \cos v$, $y = e^u \sin v$.","Verify , given , .",JJ' = 1 x = e^u \cos v y = e^u \sin v,"I was trying to solve this jacobian problem but I'm not getting the required solution i.e $JJ' = 1$ I'm getting $J = e^{2u}$ and $J' = \frac{x^2 - y^2}{x^2 + y^2}$ So multiplying both $J*J'$ will not return 1 in my case. What am I doing wrong? Here's my solution,","I was trying to solve this jacobian problem but I'm not getting the required solution i.e I'm getting and So multiplying both will not return 1 in my case. What am I doing wrong? Here's my solution,",JJ' = 1 J = e^{2u} J' = \frac{x^2 - y^2}{x^2 + y^2} J*J',"['calculus', 'derivatives', 'partial-derivative', 'jacobian']"
88,Numerical Differentation for Complex Functions,Numerical Differentation for Complex Functions,,"Given a function $f(x)$ , there is a way to approximate $f'(x)$ : the finite-step formula. ${\displaystyle {\frac {f(x+\epsilon)-f(x)}{\epsilon}}}$ , for $\epsilon$ arbitrarily small. The smaller $\epsilon$ is, the more accurate the answer would be. However, I don't believe that carries over to $f(z)$ . So, how would it be achieved?","Given a function , there is a way to approximate : the finite-step formula. , for arbitrarily small. The smaller is, the more accurate the answer would be. However, I don't believe that carries over to . So, how would it be achieved?",f(x) f'(x) {\displaystyle {\frac {f(x+\epsilon)-f(x)}{\epsilon}}} \epsilon \epsilon f(z),"['derivatives', 'numerical-methods']"
89,How to find the second derivative of y in $y^2 = x^2 + 2x$?,How to find the second derivative of y in ?,y^2 = x^2 + 2x,"I have a problem to solve: use implicit differentiation to find $\frac{dy}{dx}$ and then $\frac{d^2y}{dx^2}$ . Write the solutions in terms of x and y only It means that I need to differentiate the equation one time to find $y'$ and then once more to find $y''$ . The correct answer from the textbook is $y' = \frac{x + 1}{y}$ and $y'' = \frac{x^2 + 2x}{y^3}$ . I got the first derivative right, but I can't understand how did they get the second one, or is it a typo (unlikely), since I have $y'' = \frac{1}{y} - \frac{(x + 1)^2}{y^3}$ I did this: $$ y^2 = x^2 + 2x\\ 2yy' = 2x + 2\\ yy' = x + 1\\ y' = \frac{x + 1}{y}\\ $$ I tried to get to the second derivative from both $yy' = x + 1$ , $y' = \frac{x + 1}{y}$ and $2yy' = 2x + 2$ . But every time I had that dangling constant (1 or 2), which lead to the dangling $\frac{1}{y}$ in my answer. Like here: $$ yy' = x + 1\\ y'y' + yy'' = 1\\ yy'' = 1 - (y')^2\\ y'' = \frac{1 - (y')^2}{y}\\ y'' = \frac{1}{y} - \frac{(y')^2}{y}\\ y'' = \frac{1}{y} - \frac{(\frac{x + 1}{y})^2}{y}\\ y'' = \frac{1}{y} - \frac{(x + 1)^2}{y^3} $$ I don't see any way to get from my answer to the textbook's one with a transformation, no way to get rid from y in the numerator. And the correct answer doesn't have a ""y"" there. Could someone either point to an error in my solution, or corroborate the suspicion that it indeed may be a typo.","I have a problem to solve: use implicit differentiation to find and then . Write the solutions in terms of x and y only It means that I need to differentiate the equation one time to find and then once more to find . The correct answer from the textbook is and . I got the first derivative right, but I can't understand how did they get the second one, or is it a typo (unlikely), since I have I did this: I tried to get to the second derivative from both , and . But every time I had that dangling constant (1 or 2), which lead to the dangling in my answer. Like here: I don't see any way to get from my answer to the textbook's one with a transformation, no way to get rid from y in the numerator. And the correct answer doesn't have a ""y"" there. Could someone either point to an error in my solution, or corroborate the suspicion that it indeed may be a typo.","\frac{dy}{dx} \frac{d^2y}{dx^2} y' y'' y' = \frac{x + 1}{y} y'' = \frac{x^2 + 2x}{y^3} y'' = \frac{1}{y} - \frac{(x + 1)^2}{y^3} 
y^2 = x^2 + 2x\\
2yy' = 2x + 2\\
yy' = x + 1\\
y' = \frac{x + 1}{y}\\
 yy' = x + 1 y' = \frac{x + 1}{y} 2yy' = 2x + 2 \frac{1}{y} 
yy' = x + 1\\
y'y' + yy'' = 1\\
yy'' = 1 - (y')^2\\
y'' = \frac{1 - (y')^2}{y}\\
y'' = \frac{1}{y} - \frac{(y')^2}{y}\\
y'' = \frac{1}{y} - \frac{(\frac{x + 1}{y})^2}{y}\\
y'' = \frac{1}{y} - \frac{(x + 1)^2}{y^3}
","['calculus', 'derivatives', 'implicit-differentiation']"
90,Differentiability of complex line integral,Differentiability of complex line integral,,"Take $f:D\subseteq\mathbb{C}\rightarrow \mathbb{C}$ to be holomorphic on some path connected $D$ . Suppose there exists a function $F:D\rightarrow \mathbb{C}$ , such that for all $z,z'\in D$ and any path $\gamma:[0;1]\rightarrow D$ with $\gamma(0)=z$ and $\gamma(1)=z'$ we have: $$\int_\gamma f(z)\, dz = F(z')-F(z)$$ Can we show that $F$ is differentiable and $F'=f$ on $D$ (so $F$ is a complex antiderivative of $f$ ) without using Cauchy's integral formula or equivalence between the notions holomorphic and analytic? I already tried fixing one point of the path and then splitting it up (or alternatively the whole integral) into real and imaginary part to be able to use some real analysis but I didn't find something useful.Any ideas? Regards","Take to be holomorphic on some path connected . Suppose there exists a function , such that for all and any path with and we have: Can we show that is differentiable and on (so is a complex antiderivative of ) without using Cauchy's integral formula or equivalence between the notions holomorphic and analytic? I already tried fixing one point of the path and then splitting it up (or alternatively the whole integral) into real and imaginary part to be able to use some real analysis but I didn't find something useful.Any ideas? Regards","f:D\subseteq\mathbb{C}\rightarrow \mathbb{C} D F:D\rightarrow \mathbb{C} z,z'\in D \gamma:[0;1]\rightarrow D \gamma(0)=z \gamma(1)=z' \int_\gamma f(z)\, dz = F(z')-F(z) F F'=f D F f","['integration', 'complex-analysis', 'derivatives', 'line-integrals', 'path-connected']"
91,Existence of an open interval on which $f'$ is bounded,Existence of an open interval on which  is bounded,f',"Recently we've just begun to learn derivative, and our teacher left the following problem. Let $f:\mathbb{R}\to \mathbb{R}$ be differentiable on $\mathbb{R}$ . Show that there is an open interval $(a,b)$ on which $f'$ is bounded. Since we've only come to the definition of derivative, the problem seems remote. Yet our teacher encouraged us to try. I've tried to disprove the opposite, but it didn't work. Please help. A mere hint will do.","Recently we've just begun to learn derivative, and our teacher left the following problem. Let be differentiable on . Show that there is an open interval on which is bounded. Since we've only come to the definition of derivative, the problem seems remote. Yet our teacher encouraged us to try. I've tried to disprove the opposite, but it didn't work. Please help. A mere hint will do.","f:\mathbb{R}\to \mathbb{R} \mathbb{R} (a,b) f'","['analysis', 'derivatives']"
92,If f is twice differenetiable at a and g is twice differentiable at $f(a)$ then prove..,If f is twice differenetiable at a and g is twice differentiable at  then prove..,f(a),$(f\circ g)''(a)= g'((f(a))(f''(a)+ g''(f(a))(f'(a))^2$ This looks like a product rule but I am not sure how to justify it and I don't know where the $(f'(a))^2$ came from,This looks like a product rule but I am not sure how to justify it and I don't know where the came from,(f\circ g)''(a)= g'((f(a))(f''(a)+ g''(f(a))(f'(a))^2 (f'(a))^2,"['real-analysis', 'calculus', 'derivatives']"
93,Linearity of a function on multivariable calculus,Linearity of a function on multivariable calculus,,"Let $f \colon \mathbb {R}^n  \to  \mathbb {R} $ be a $C^1$ class function such that $f (\frac {x}{2}) = \frac {1}{2} f(x)$ . Show that $f$ is linear. Hint : show that $\nabla f (x) = \nabla f (0)$ , for all x, and conclude that that $f (x) = \nabla f (x) \bullet x $ (dot product or scalar product). I have no idea how can the hint help me. Can anyone give me some help?","Let be a class function such that . Show that is linear. Hint : show that , for all x, and conclude that that (dot product or scalar product). I have no idea how can the hint help me. Can anyone give me some help?",f \colon \mathbb {R}^n  \to  \mathbb {R}  C^1 f (\frac {x}{2}) = \frac {1}{2} f(x) f \nabla f (x) = \nabla f (0) f (x) = \nabla f (x) \bullet x ,"['real-analysis', 'derivatives']"
94,What does a twice differentiable function mean?,What does a twice differentiable function mean?,,"In many contexts, I have seen phrases like ""... function $f$ is twice differentiable ..."". I can understand that the function $f$ can be differentiated twice or in other words its second derivative exists. But does this mean third derivative (or higher) does not exist or is equal to $0$ ? In the same way, what would we call a function like $e^x, \sin x, \dots$ which can be differentiated any number of times?","In many contexts, I have seen phrases like ""... function is twice differentiable ..."". I can understand that the function can be differentiated twice or in other words its second derivative exists. But does this mean third derivative (or higher) does not exist or is equal to ? In the same way, what would we call a function like which can be differentiated any number of times?","f f 0 e^x, \sin x, \dots","['calculus', 'derivatives', 'soft-question']"
95,"If for every $h\in\mathbb{R}^n$ $d^k f ( x ) ( h )^k = 0 $, then $d^k f ( x ) = 0?$","If for every  , then",h\in\mathbb{R}^n d^k f ( x ) ( h )^k = 0  d^k f ( x ) = 0?,"Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be a infinitely differentiable real function . Let ${d}^k f ( \mathbf{x} ) $ denotes the $k-$ th diferential of the function $f$ at $\mathbf{x}$ . Is it true that if ${d}^k f ( \mathbf{x} )(\mathbf{h})^{k}=0$ for all $\mathbf{h}\in\mathbb{R}^n,$ then ${d}^k f ( \mathbf{x} ) = \mathbf{0}$ ? For which values of $k$ this is true?",Let be a infinitely differentiable real function . Let denotes the th diferential of the function at . Is it true that if for all then ? For which values of this is true?,"f:\mathbb{R}^n \rightarrow \mathbb{R} {d}^k f ( \mathbf{x} )  k- f \mathbf{x} {d}^k f ( \mathbf{x} )(\mathbf{h})^{k}=0 \mathbf{h}\in\mathbb{R}^n, {d}^k f ( \mathbf{x} ) = \mathbf{0} k","['calculus', 'derivatives']"
96,Limit question concerning the ratio of two functions.,Limit question concerning the ratio of two functions.,,"Let $f(x)$ and $g(x)$ be two continuous functions such that: $f(a) = g(a) = 0$ . In the limit as $x \to a$ , $f'(x)/g'(x) \to 1$ .  (By L'Hôpital's rule, this of course implies that $\lim_{x \to a} f(x)/g(x) = 1$ .) $f'(x) > g'(x)$ for all $x > a$ . Is it then true that $f(x)/g(x) > 1$ for all $x > a$ ?  If not, what would be sufficient additional conditions that this is true?  I feel like there should be a simple proof of this fact, but I've been working in circles on this problem and I can't quite put my finger on it. (For the record, the specific problem that I'm interested in is $a = 1$ , $f(x) = x^\gamma - 1$ , and $g(x) = \gamma(x - 1)$ , with $\gamma$ any real number strictly greater than 1.)","Let and be two continuous functions such that: . In the limit as , .  (By L'Hôpital's rule, this of course implies that .) for all . Is it then true that for all ?  If not, what would be sufficient additional conditions that this is true?  I feel like there should be a simple proof of this fact, but I've been working in circles on this problem and I can't quite put my finger on it. (For the record, the specific problem that I'm interested in is , , and , with any real number strictly greater than 1.)",f(x) g(x) f(a) = g(a) = 0 x \to a f'(x)/g'(x) \to 1 \lim_{x \to a} f(x)/g(x) = 1 f'(x) > g'(x) x > a f(x)/g(x) > 1 x > a a = 1 f(x) = x^\gamma - 1 g(x) = \gamma(x - 1) \gamma,"['limits', 'derivatives']"
97,sup-norm bound by second derivative,sup-norm bound by second derivative,,"If $f\in C[0,1]$ , twice differentiable function, and $f(0)=0=f(1),$ then $$ \sup_{x\in[0,1]} |f(x)| \leq \frac{1}{8} \sup_{x\in[0,1]} |f''(x)|.$$ I am not able to get the constant ${1}/{8}$ , here is my try: For any $x\in[0,1]$ , $$ |f(x)| = |f(x)-f(0)| \leq \int_0^x \int_0^t |f''(y)| \,dy\,dt \leq \sup_{x\in[0,1]} |f''(x)| \int_0^x t\,dt \leq \frac{1}{2} \sup_{x\in[0,1]} |f''(x)|. $$ Thanks for any help regarding this!","If , twice differentiable function, and then I am not able to get the constant , here is my try: For any , Thanks for any help regarding this!","f\in C[0,1] f(0)=0=f(1),  \sup_{x\in[0,1]} |f(x)| \leq \frac{1}{8} \sup_{x\in[0,1]} |f''(x)|. {1}/{8} x\in[0,1]  |f(x)| = |f(x)-f(0)| \leq \int_0^x \int_0^t |f''(y)| \,dy\,dt \leq \sup_{x\in[0,1]} |f''(x)| \int_0^x t\,dt \leq \frac{1}{2} \sup_{x\in[0,1]} |f''(x)|. ","['functional-analysis', 'analysis', 'derivatives']"
98,Derivation with polar coordinate [closed],Derivation with polar coordinate [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f(x)$ be a defined function $\mathbb{R}^2$ using polar coordinate by $\frac{r^6}{(\log(r))^3}(1+\cos(\theta))$ when $r>1$ . Please help me to calculate $\partial_x^{\alpha}f(x)$ for all $x\in\mathbb{R}^2$ and all $\alpha\in\mathbb{N}^2$ with $1\le |\alpha|\le 3$ . Merci de m'aider","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let be a defined function using polar coordinate by when . Please help me to calculate for all and all with . Merci de m'aider",f(x) \mathbb{R}^2 \frac{r^6}{(\log(r))^3}(1+\cos(\theta)) r>1 \partial_x^{\alpha}f(x) x\in\mathbb{R}^2 \alpha\in\mathbb{N}^2 1\le |\alpha|\le 3,"['calculus', 'derivatives', 'partial-differential-equations', 'polar-coordinates']"
99,Example of differentiating through Lebesgue integral,Example of differentiating through Lebesgue integral,,"I am having trouble showing that I can differentiate this specific example through the integral $$\frac{d}{dt} \int_\mathbb{R} e^{-tx^2} dx = - \int_\mathbb{R} x^2 e^{-tx^2} dx$$ The way I tried was using the Dominated convergence theorem, but I am stuck. I was able to prove that the sequence defined by $$f_n = \left(\frac{x^2}{n} +1\right)^{-tn}$$ converges to $e^{-tx^2}$ and counts with all the required properties for the theorem. That is, measurability, and that it is absolutely bounded (e.g. by the constant function $1$ in this case). This also applies to the sequence defined by the derivatives $\frac{d f_n}{dt}$ . My problem is that, at the end I have the same problem, the limit is still outside the integral. At this point, I have shown that $$\int_\mathbb{R} \frac{df_n}{dt} dx \to - \int_\mathbb{R} x^2 e^{-tx^2} dx$$ $$\int_\mathbb{R} f_n dx \to \int_\mathbb{R} e^{-tx^2} dx$$ But still not the crucial fact that $$\frac{d}{dt} \int_\mathbb{R} f_n dx = \int_\mathbb{R} \frac{d f_n}{dt} dx$$","I am having trouble showing that I can differentiate this specific example through the integral The way I tried was using the Dominated convergence theorem, but I am stuck. I was able to prove that the sequence defined by converges to and counts with all the required properties for the theorem. That is, measurability, and that it is absolutely bounded (e.g. by the constant function in this case). This also applies to the sequence defined by the derivatives . My problem is that, at the end I have the same problem, the limit is still outside the integral. At this point, I have shown that But still not the crucial fact that",\frac{d}{dt} \int_\mathbb{R} e^{-tx^2} dx = - \int_\mathbb{R} x^2 e^{-tx^2} dx f_n = \left(\frac{x^2}{n} +1\right)^{-tn} e^{-tx^2} 1 \frac{d f_n}{dt} \int_\mathbb{R} \frac{df_n}{dt} dx \to - \int_\mathbb{R} x^2 e^{-tx^2} dx \int_\mathbb{R} f_n dx \to \int_\mathbb{R} e^{-tx^2} dx \frac{d}{dt} \int_\mathbb{R} f_n dx = \int_\mathbb{R} \frac{d f_n}{dt} dx,"['real-analysis', 'measure-theory', 'derivatives', 'convergence-divergence', 'lebesgue-integral']"
