,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding the inverse Laplace transform of $\frac{s^2-4s-4}{s^4+8s^2+16}$,Finding the inverse Laplace transform of,\frac{s^2-4s-4}{s^4+8s^2+16},"$$F(s) = \frac{s^2-4s-4}{s^4+8s^2+16}$$ My work is as follows, $$\frac{s^2-4s-4}{(s^2+4)^2}=\frac{s^2+4}{(s^2+4)^2}-\frac{8}{(s^2+4)^2}-\frac{4s}{(s^2+4)^2}$$ The inverse laplace of the first term is, $\frac{1}{2} \sin(2t)$ The second one has no direct transform, perhaps using the convolution theorem would do. The third and final term is -$t\sin(2t)$. Is there a faster way to solve this instead of using the convolution theorem for the second term.","$$F(s) = \frac{s^2-4s-4}{s^4+8s^2+16}$$ My work is as follows, $$\frac{s^2-4s-4}{(s^2+4)^2}=\frac{s^2+4}{(s^2+4)^2}-\frac{8}{(s^2+4)^2}-\frac{4s}{(s^2+4)^2}$$ The inverse laplace of the first term is, $\frac{1}{2} \sin(2t)$ The second one has no direct transform, perhaps using the convolution theorem would do. The third and final term is -$t\sin(2t)$. Is there a faster way to solve this instead of using the convolution theorem for the second term.",,"['calculus', 'laplace-transform']"
1,Calculus Question: $\int_0^1\frac{(x-x^2)^4}{1+x^2}dx$,Calculus Question:,\int_0^1\frac{(x-x^2)^4}{1+x^2}dx,I have no idea how to find this integral: $$\int_0^1\frac{(x-x^2)^4}{1+x^2}\ dx\ ?$$ Any help would be appreciated. Thanks in advance.,I have no idea how to find this integral: $$\int_0^1\frac{(x-x^2)^4}{1+x^2}\ dx\ ?$$ Any help would be appreciated. Thanks in advance.,,"['calculus', 'integration', 'definite-integrals']"
2,Dual Integral Problems: $\int\left(\pi^{\sin^2x}+e^{\sin^2x}\right)^2\cos2x~dx$ and $ \int\frac{\sin x}{1+\cos x+e^x}dx$,Dual Integral Problems:  and,\int\left(\pi^{\sin^2x}+e^{\sin^2x}\right)^2\cos2x~dx  \int\frac{\sin x}{1+\cos x+e^x}dx,"Recently, I saw these two calculus problems showed up on internet. I tried to answer those problems but I couldn't. I've already used WolframAlpha to figure it out, but the result was nothing. These are the problems: $$ \int\left(\pi^{\sin^2x}+e^{\sin^2x}\right)^2\cos2x~dx $$ and $$ \int\frac{\sin x}{1+\cos x+e^x}dx $$ Could anyone here help me out how to answer these problems? I'd be grateful for any help you are able to provide.","Recently, I saw these two calculus problems showed up on internet. I tried to answer those problems but I couldn't. I've already used WolframAlpha to figure it out, but the result was nothing. These are the problems: $$ \int\left(\pi^{\sin^2x}+e^{\sin^2x}\right)^2\cos2x~dx $$ and $$ \int\frac{\sin x}{1+\cos x+e^x}dx $$ Could anyone here help me out how to answer these problems? I'd be grateful for any help you are able to provide.",,"['calculus', 'integration', 'indefinite-integrals']"
3,Does the power rule for irrational exponents hold for x<0?,Does the power rule for irrational exponents hold for x<0?,,"Given the power rule $D(x^n) = n \cdot x^{n-1}$: My calculus book proves that for irrational $n$, it holds for $x > 0$. Wikipedia’s aricle on the Power Rule is confusing me because it makes no mention of this limitation. And given a problem such as $D(x^π)$, my book does not give an answer with the limit of $x>0$. (Is that because $x^π$ is itself not defined if $x\not>0$, so it is “by definition” limited?) So is there some extension I don’t know of that extends it to $x\leq0$?","Given the power rule $D(x^n) = n \cdot x^{n-1}$: My calculus book proves that for irrational $n$, it holds for $x > 0$. Wikipedia’s aricle on the Power Rule is confusing me because it makes no mention of this limitation. And given a problem such as $D(x^π)$, my book does not give an answer with the limit of $x>0$. (Is that because $x^π$ is itself not defined if $x\not>0$, so it is “by definition” limited?) So is there some extension I don’t know of that extends it to $x\leq0$?",,"['calculus', 'derivatives']"
4,Prove that limit goes to infinity if a convex function's derivative > 0,Prove that limit goes to infinity if a convex function's derivative > 0,,"I do understand what a convex function is and I can see geometrically that following statement is true: $(1)$ If for a given convex function $f$ which is differentiable over $\mathbb{R}$, if $f'(x_0)>0$ for some $x_0$ then $\lim_{x \to \infty}{f(x)} = \infty$. I see it like this: if the derivative $f'(x_0)>0$ then the slope is increasing all the time as long as you approach infinity from $x_0$ due to the definition of convex function: $$f(tx_0 + (1-t)x_1 \leq tf(x_0) + (1-t)f(x_1)$$ In other words, the secant is above the curve all the time. But how do I prove the statement $(1)$?","I do understand what a convex function is and I can see geometrically that following statement is true: $(1)$ If for a given convex function $f$ which is differentiable over $\mathbb{R}$, if $f'(x_0)>0$ for some $x_0$ then $\lim_{x \to \infty}{f(x)} = \infty$. I see it like this: if the derivative $f'(x_0)>0$ then the slope is increasing all the time as long as you approach infinity from $x_0$ due to the definition of convex function: $$f(tx_0 + (1-t)x_1 \leq tf(x_0) + (1-t)f(x_1)$$ In other words, the secant is above the curve all the time. But how do I prove the statement $(1)$?",,"['calculus', 'functions']"
5,Prerequisites of general calculus,Prerequisites of general calculus,,"I have a unique situation in which I need to know the names of the concepts taught in Pre-Calculus and those taught at the beginning of the school year in Calculus (or just general concepts used in entry-level Calculus courses).  It would be nice if somebody could explain Calculus; from my understanding it is the study of curves. Please use plain English because I am only a High School student and particularly bad at understanding ""math speak"" (even more so than most HS students).  I have a physics course which requires me to use Calculus right away (and it is listed as being perfectly recommendable to Pre-Calc students, I can't figure out why).  In short, I took the class and am now overwhelmed: I am supposed to be finding the slope of a curve based on a tangent line (the derivative, I believe) and the area under the curve (I've heard this called ""integration"" before). Could somebody please explain to me exactly what I need to know in order to understand these concepts?  Of course I have talked to my teacher, asked about it myself, searched around the internet and made a visit to Youtube, but everything assumes too much (for example, I had a number of them use the capital Sigma on me and I only very recently learned what it's used for).","I have a unique situation in which I need to know the names of the concepts taught in Pre-Calculus and those taught at the beginning of the school year in Calculus (or just general concepts used in entry-level Calculus courses).  It would be nice if somebody could explain Calculus; from my understanding it is the study of curves. Please use plain English because I am only a High School student and particularly bad at understanding ""math speak"" (even more so than most HS students).  I have a physics course which requires me to use Calculus right away (and it is listed as being perfectly recommendable to Pre-Calc students, I can't figure out why).  In short, I took the class and am now overwhelmed: I am supposed to be finding the slope of a curve based on a tangent line (the derivative, I believe) and the area under the curve (I've heard this called ""integration"" before). Could somebody please explain to me exactly what I need to know in order to understand these concepts?  Of course I have talked to my teacher, asked about it myself, searched around the internet and made a visit to Youtube, but everything assumes too much (for example, I had a number of them use the capital Sigma on me and I only very recently learned what it's used for).",,"['calculus', 'algebra-precalculus', 'soft-question', 'advice']"
6,Continuity on open interval,Continuity on open interval,,"A function is said to be continuous on an open interval if and only if it is continuous at every point in this interval. But an open interval $(a,b)$ doesn't contain $a$ and $b$, so we never actually reach $a$ or $b$, and therefore they're not defined, and points that are not defined are not continuous, in other words $f(a)$ and $f(b)$ don't exist which makes the interval $(a,b)$ discontinuous. So what is this definition saying, because I thought that it can't be continuous at $a$ or $b$ since they are not defined (an open circle on the graph), but everywhere in between $a$ and $b$ it can still be continuous... So is it just continuous between these points $a$ and $b$, and a jump discontinuity occurs at these two points? Why then does it say that it's continuous at every point in $(a,b)$, if we are not including $a$ and $b$? Points on an open interval can be approached from both right and left, correct? why is it required to be continuous on open $(a,b)$ in order to be continuous on closed $[a,b]$, I don't understand this because $a$ and $b$ are not defined in $(a,b)$. please help to understand","A function is said to be continuous on an open interval if and only if it is continuous at every point in this interval. But an open interval $(a,b)$ doesn't contain $a$ and $b$, so we never actually reach $a$ or $b$, and therefore they're not defined, and points that are not defined are not continuous, in other words $f(a)$ and $f(b)$ don't exist which makes the interval $(a,b)$ discontinuous. So what is this definition saying, because I thought that it can't be continuous at $a$ or $b$ since they are not defined (an open circle on the graph), but everywhere in between $a$ and $b$ it can still be continuous... So is it just continuous between these points $a$ and $b$, and a jump discontinuity occurs at these two points? Why then does it say that it's continuous at every point in $(a,b)$, if we are not including $a$ and $b$? Points on an open interval can be approached from both right and left, correct? why is it required to be continuous on open $(a,b)$ in order to be continuous on closed $[a,b]$, I don't understand this because $a$ and $b$ are not defined in $(a,b)$. please help to understand",,"['calculus', 'limits', 'functions', 'continuity', 'definition']"
7,Prove a limit using the epsilon-delta definition,Prove a limit using the epsilon-delta definition,,"$\lim_{x\to-2}\frac{4x-1}{x+1} = 9$ Given $\epsilon>0$ , $$(\exists \delta(\epsilon)>0) \left( |x+2|<\delta \implies \left|{\frac{4x-1}{x+1} - 9}\right| < \epsilon \right)$$ So, if $|x+2|<\delta$ , then: $$\left|{\frac{4x-1}{x+1} - 9}\right| = \left|{\frac{4x-1-9(x+1)}{x+1}}\right| = \left|{\frac{-5x-10}{x+1}}\right| = \left|{\frac{-1 (5x+10)}{x+1}}\right| = {\frac{|-1||5x+10|}{|x+1|}} = {\frac{5|x+2|}{|x+1|}}$$ I've found the expression $5|x+2|$ in the numerator: $|x+2| < \delta = 1/2$ $-1/2<x+2<1/2$ $-5/2<x<-3/2$ $-3/2<x+1<-1/2 \implies$ ? A colleague told me to leave both members of this inequality as positives. Why do we need both sides to be positives?","Given , So, if , then: I've found the expression in the numerator: ? A colleague told me to leave both members of this inequality as positives. Why do we need both sides to be positives?",\lim_{x\to-2}\frac{4x-1}{x+1} = 9 \epsilon>0 (\exists \delta(\epsilon)>0) \left( |x+2|<\delta \implies \left|{\frac{4x-1}{x+1} - 9}\right| < \epsilon \right) |x+2|<\delta \left|{\frac{4x-1}{x+1} - 9}\right| = \left|{\frac{4x-1-9(x+1)}{x+1}}\right| = \left|{\frac{-5x-10}{x+1}}\right| = \left|{\frac{-1 (5x+10)}{x+1}}\right| = {\frac{|-1||5x+10|}{|x+1|}} = {\frac{5|x+2|}{|x+1|}} 5|x+2| |x+2| < \delta = 1/2 -1/2<x+2<1/2 -5/2<x<-3/2 -3/2<x+1<-1/2 \implies,"['calculus', 'limits']"
8,Nonzero derivative implies function is strictly increasing or decreasing on some interval,Nonzero derivative implies function is strictly increasing or decreasing on some interval,,"Let $f$ be a differentiable function on open interval $(a,b)$. Suppose $f'(x)$ is not identically zero. Show that there exists an subinterval $(c,d)$ such that $f(x)$ is strictly increasing or strictly decreasing on $(c,d)$. How to prove this? I think this statement is wrong...","Let $f$ be a differentiable function on open interval $(a,b)$. Suppose $f'(x)$ is not identically zero. Show that there exists an subinterval $(c,d)$ such that $f(x)$ is strictly increasing or strictly decreasing on $(c,d)$. How to prove this? I think this statement is wrong...",,['calculus']
9,when is the particle speeding up and when is it slowing down,when is the particle speeding up and when is it slowing down,,"Based on this graph i have to figuere out when the particle is speeding up and when it is slowing down.  My understanding is that when velocity and accelaration have the same sign then we are speeding up. however if velocity and accelaration have different signs then we are slowing down. Using this concept i applied it to this problem and i came up speeding up using interval notation. $(0,1)and(3,4) $ I'm getting the answer wrong can someone please help me understand the missing piece to this puzzle. Thanks Miguel","Based on this graph i have to figuere out when the particle is speeding up and when it is slowing down.  My understanding is that when velocity and accelaration have the same sign then we are speeding up. however if velocity and accelaration have different signs then we are slowing down. Using this concept i applied it to this problem and i came up speeding up using interval notation. $(0,1)and(3,4) $ I'm getting the answer wrong can someone please help me understand the missing piece to this puzzle. Thanks Miguel",,['calculus']
10,Representing Functions as Power Series,Representing Functions as Power Series,,"Rewrite $$f(x)=(1+x)/(1-x)^2$$ as a power series. Work thus far: I separated it into two parts: $$1/(1-x)^2 + x/(1-x)^2$$ I realize that the first expression is the derivative of $1/(1-x)$ and come up with this sum of series: $$\sum_{n=0}^\infty x^n$$ Since a derivative was involved, we must derive the series: $$\displaystyle\sum_{n=0}^\infty nx^{n-1} + x \displaystyle\sum_{n=0}^\infty nx^{n-1}$$ $$=\displaystyle\sum_{n=0}^\infty nx^{n-1} + \displaystyle\sum_{n=0}^\infty nx^n$$ These series need to be added, but how? The final answer is $\displaystyle\sum_{n=0}^\infty (2n+1)x^n$","Rewrite $$f(x)=(1+x)/(1-x)^2$$ as a power series. Work thus far: I separated it into two parts: $$1/(1-x)^2 + x/(1-x)^2$$ I realize that the first expression is the derivative of $1/(1-x)$ and come up with this sum of series: $$\sum_{n=0}^\infty x^n$$ Since a derivative was involved, we must derive the series: $$\displaystyle\sum_{n=0}^\infty nx^{n-1} + x \displaystyle\sum_{n=0}^\infty nx^{n-1}$$ $$=\displaystyle\sum_{n=0}^\infty nx^{n-1} + \displaystyle\sum_{n=0}^\infty nx^n$$ These series need to be added, but how? The final answer is $\displaystyle\sum_{n=0}^\infty (2n+1)x^n$",,"['calculus', 'sequences-and-series', 'convergence-divergence', 'power-series']"
11,Integrating $\sum_{n=1}^{\infty} \frac{\sin nx }{n^4}$,Integrating,\sum_{n=1}^{\infty} \frac{\sin nx }{n^4},Consider : $\displaystyle f(x)= \sum_{n=1}^{\infty} \frac{\sin nx }{n^4}$ Find : $\displaystyle \int_0^{x} f(t)\ \mathrm{d}t$.,Consider : $\displaystyle f(x)= \sum_{n=1}^{\infty} \frac{\sin nx }{n^4}$ Find : $\displaystyle \int_0^{x} f(t)\ \mathrm{d}t$.,,"['calculus', 'summation', 'exponential-sum']"
12,What is the probability that the resulting four line segments are the sides of a quadrilateral?,What is the probability that the resulting four line segments are the sides of a quadrilateral?,,"Question: Divide a given line segment into two other line segments. Then, cut each of these new line segments into two more line segments. What is the probability that the resulting line segments are the sides of a quadrilateral? I am stuck on this problem. I think I am close, but I am not sure if it is correct. Any help or conformation on this would be helpful. My thoughts on the problem: Let us say that the line segment is of length 1. The only restriction for these for line segments to form a quadrilateral is that no one side > .5 (correct me if I am wrong). With our first cut we have two smaller line segments, one larger than the other. We only need to look at the longer on of these two line segments. Let us call $y$ the smaller line segment and $x$ the larger one. $x$ will be between 0.5 and 1. When we cut each of these new line segments we need to find where it does not work for it to be a quadrilateral. We only have to look at $x$. Let us call $a$ the length that we cut. If we use the example $x=0.6$ we can see that $a$ cannot be less than 0.1 or greater than 0.5. We can generalize this for any $x$. $a$ cannot be less than $(x-1/2)$ or greater than $1/2$. This is where I get stuck. I believe that the probability for any $x$ value that these four line segments will not be a quadrilateral is $$\frac{2(x-1/2)}{x}$$ If this is correct is the total probability that it cannot be a quadrilateral $$\int\limits_{1/2}^1\frac{2(x-1/2)}{x}\mathrm{d}x?$$ Any help is much appreciated. Thank you","Question: Divide a given line segment into two other line segments. Then, cut each of these new line segments into two more line segments. What is the probability that the resulting line segments are the sides of a quadrilateral? I am stuck on this problem. I think I am close, but I am not sure if it is correct. Any help or conformation on this would be helpful. My thoughts on the problem: Let us say that the line segment is of length 1. The only restriction for these for line segments to form a quadrilateral is that no one side > .5 (correct me if I am wrong). With our first cut we have two smaller line segments, one larger than the other. We only need to look at the longer on of these two line segments. Let us call $y$ the smaller line segment and $x$ the larger one. $x$ will be between 0.5 and 1. When we cut each of these new line segments we need to find where it does not work for it to be a quadrilateral. We only have to look at $x$. Let us call $a$ the length that we cut. If we use the example $x=0.6$ we can see that $a$ cannot be less than 0.1 or greater than 0.5. We can generalize this for any $x$. $a$ cannot be less than $(x-1/2)$ or greater than $1/2$. This is where I get stuck. I believe that the probability for any $x$ value that these four line segments will not be a quadrilateral is $$\frac{2(x-1/2)}{x}$$ If this is correct is the total probability that it cannot be a quadrilateral $$\int\limits_{1/2}^1\frac{2(x-1/2)}{x}\mathrm{d}x?$$ Any help is much appreciated. Thank you",,"['calculus', 'probability', 'geometry']"
13,Limit of $( n\sin^2(x\pi)-n\sin^2(\sqrt{n}\pi))/(x-\sqrt{n})$ without using L'Hopital,Limit of  without using L'Hopital,( n\sin^2(x\pi)-n\sin^2(\sqrt{n}\pi))/(x-\sqrt{n}),"I was asked to prove this , without using L'Hopital... tried out some trig. identities with no big use $(\sin(\alpha)-\sin(\beta))(\sin(\alpha)+\sin(\beta))=\sin^2(\alpha)-\sin^2(\beta)$ for example, and from there to the $\sin(\alpha)-\sin(\beta)$ identity... but with no real success. And tried also multiplying num.and denum. by the conjugate. the question is: Prove (without using L'Hopital) that: $$ \lim_{x\to \sqrt{n}^+} \frac{n\sin^2(x\pi)-n\sin^2(\sqrt{n}\pi)}{x-\sqrt{n}} = n\pi\sin(2\pi\sqrt{n})$$","I was asked to prove this , without using L'Hopital... tried out some trig. identities with no big use $(\sin(\alpha)-\sin(\beta))(\sin(\alpha)+\sin(\beta))=\sin^2(\alpha)-\sin^2(\beta)$ for example, and from there to the $\sin(\alpha)-\sin(\beta)$ identity... but with no real success. And tried also multiplying num.and denum. by the conjugate. the question is: Prove (without using L'Hopital) that: $$ \lim_{x\to \sqrt{n}^+} \frac{n\sin^2(x\pi)-n\sin^2(\sqrt{n}\pi)}{x-\sqrt{n}} = n\pi\sin(2\pi\sqrt{n})$$",,"['calculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
14,Area of surface of revolution of $e^x$,Area of surface of revolution of,e^x,"An at first easy looking question has been giving me problems. Given is the function $f(x)=e^x$ on the interval $[0,1]$, asked are the areas of its surfaces of revolution about the $x$-axis and $y$-axis. $x$-axis: We have $A=2\pi\int\limits_0^1f(x)\sqrt{1+(f'(x))^2}dx=2\pi\int\limits_0^1e^x\sqrt{1+e^{2x}}dx=2\pi\int\limits_1^e\sqrt{1+u^2}du$ Here we can substitute $u=\textrm{tan}(\theta)$ and we get $A=2\pi\int\limits_{\textrm{atan}(1)}^{\textrm{atan}(e)}\frac{1}{\textrm{cos}^3(\theta)}d\theta$. I don't really know how to approach this integral and would appreciate help. Wolfram Alpha suggests using a reduction formula but we haven't learnt this formula and I don't think we're meant to use it. $y$-axis: For this one we have $A=2\pi\int\limits_0^1x\sqrt{1+(f'(x))^2}dx=2\pi\int\limits_0^1x\sqrt{1+e^{2x}}dx$. I have tried many things and not gotten anywhere. Furthermore Wolfram Alpha suggests a complicated function as the indefinite integral of the integrand which leads me to think I have done something else wrong as well. Any help is appreciated","An at first easy looking question has been giving me problems. Given is the function $f(x)=e^x$ on the interval $[0,1]$, asked are the areas of its surfaces of revolution about the $x$-axis and $y$-axis. $x$-axis: We have $A=2\pi\int\limits_0^1f(x)\sqrt{1+(f'(x))^2}dx=2\pi\int\limits_0^1e^x\sqrt{1+e^{2x}}dx=2\pi\int\limits_1^e\sqrt{1+u^2}du$ Here we can substitute $u=\textrm{tan}(\theta)$ and we get $A=2\pi\int\limits_{\textrm{atan}(1)}^{\textrm{atan}(e)}\frac{1}{\textrm{cos}^3(\theta)}d\theta$. I don't really know how to approach this integral and would appreciate help. Wolfram Alpha suggests using a reduction formula but we haven't learnt this formula and I don't think we're meant to use it. $y$-axis: For this one we have $A=2\pi\int\limits_0^1x\sqrt{1+(f'(x))^2}dx=2\pi\int\limits_0^1x\sqrt{1+e^{2x}}dx$. I have tried many things and not gotten anywhere. Furthermore Wolfram Alpha suggests a complicated function as the indefinite integral of the integrand which leads me to think I have done something else wrong as well. Any help is appreciated",,[]
15,Resilient L'Hospital's rule question,Resilient L'Hospital's rule question,,"I'm trying to show that $$\lim_{x\to 0} \frac{(1+x)^{\frac{1}{x}}-e}{x} = -\frac{e}{2}$$ At first it seemed like a routine application of L'Hospital's rule, but my standard bag of tricks isn't working. The $e$ in the numerator prevents any log trickery from separating nicely, and the limit being negative seems to also preclude analyzing the limit of the log. I tried to interpret this as the derivative of some function at a point, say, $g(u) = u^{\frac{1}{u-1}}$ and the point $u = 1$, but evaluating $g'(1)$ just got worse, and I had concerns about differentiability of $g$ there. Would choosing a different function work out better? I tried fiddling with one of the limit definitions for $e$ because the first term in the numerator tends to $e$ as $x\to 0$, but the function we're taking the limit of is not continuous at $x=0$ and so moving the limit in was a no-go. Edit: the $e$ in the numerator seems critical, as the limit diverges without it. I have a feeling I'm missing something simple. Any hints/solutions would be appreciated.","I'm trying to show that $$\lim_{x\to 0} \frac{(1+x)^{\frac{1}{x}}-e}{x} = -\frac{e}{2}$$ At first it seemed like a routine application of L'Hospital's rule, but my standard bag of tricks isn't working. The $e$ in the numerator prevents any log trickery from separating nicely, and the limit being negative seems to also preclude analyzing the limit of the log. I tried to interpret this as the derivative of some function at a point, say, $g(u) = u^{\frac{1}{u-1}}$ and the point $u = 1$, but evaluating $g'(1)$ just got worse, and I had concerns about differentiability of $g$ there. Would choosing a different function work out better? I tried fiddling with one of the limit definitions for $e$ because the first term in the numerator tends to $e$ as $x\to 0$, but the function we're taking the limit of is not continuous at $x=0$ and so moving the limit in was a no-go. Edit: the $e$ in the numerator seems critical, as the limit diverges without it. I have a feeling I'm missing something simple. Any hints/solutions would be appreciated.",,"['calculus', 'limits']"
16,Thermodynamic relation,Thermodynamic relation,,"$$\left(\frac{dU}{dT}\right)_p= \left( \frac{\partial U}{\partial T} \right)_v + \left( \frac{\partial U}{\partial V}\right)_T \left( \frac{\partial V}{\partial T}\right)_p$$ Using maxwell relations etc, can you show me how to prove above thermodynamic relation.","$$\left(\frac{dU}{dT}\right)_p= \left( \frac{\partial U}{\partial T} \right)_v + \left( \frac{\partial U}{\partial V}\right)_T \left( \frac{\partial V}{\partial T}\right)_p$$ Using maxwell relations etc, can you show me how to prove above thermodynamic relation.",,"['calculus', 'physics']"
17,Who came up with the Euler-Lagrange equation first?,Who came up with the Euler-Lagrange equation first?,,"Could someone explain who came up with the specific equation first? http://en.wikipedia.org/wiki/Euler-Lagrange makes it sound like Lagrange got it first, in 1755, then sent it to Euler. but: http://en.wikipedia.org/wiki/Calculus_of_variations sort of makes it sound like Euler got it first in the 1730s. It seems like a straightforward question, but I can't find an answer anywhere. Who came up with the equation, Euler or Lagrange? And what precisely did the other man contribute to get his name on there?","Could someone explain who came up with the specific equation first? http://en.wikipedia.org/wiki/Euler-Lagrange makes it sound like Lagrange got it first, in 1755, then sent it to Euler. but: http://en.wikipedia.org/wiki/Calculus_of_variations sort of makes it sound like Euler got it first in the 1730s. It seems like a straightforward question, but I can't find an answer anywhere. Who came up with the equation, Euler or Lagrange? And what precisely did the other man contribute to get his name on there?",,"['calculus', 'math-history', 'calculus-of-variations']"
18,Prove that $2 < e < 4$ using upper and lower Riemann sums and the definition of $\ln{x}$,Prove that  using upper and lower Riemann sums and the definition of,2 < e < 4 \ln{x},"Prove that $2 < e < 4$ using upper and lower Riemann sums and the   definition of $\ln{x}$ I think I understand the concept of what I need to do, but I am having some trouble implementing a solution. I guess this would be equivalent to showing that $\ln(2) < 1 < \ln(4)$ since the $\ln$ function is increasing. What I'm not sure about is how I use the definition of $\ln(x)$ in the Riemann sum. I tried this: $$\lim_{n\to\infty} \sum_{k=1}^n \frac{1}{n}\int_1^{\frac{k}{n}} \frac{1}{t}dt$$ I wasn't sure how to check the value at each point in order to prove my inequalities. How am I supposed to be doing this?","Prove that $2 < e < 4$ using upper and lower Riemann sums and the   definition of $\ln{x}$ I think I understand the concept of what I need to do, but I am having some trouble implementing a solution. I guess this would be equivalent to showing that $\ln(2) < 1 < \ln(4)$ since the $\ln$ function is increasing. What I'm not sure about is how I use the definition of $\ln(x)$ in the Riemann sum. I tried this: $$\lim_{n\to\infty} \sum_{k=1}^n \frac{1}{n}\int_1^{\frac{k}{n}} \frac{1}{t}dt$$ I wasn't sure how to check the value at each point in order to prove my inequalities. How am I supposed to be doing this?",,['calculus']
19,Prove $\int_a^b f(x)dx \leq \frac{e^{2L\beta}-1}{2L\alpha}\int_c^d f(x)dx$,Prove,\int_a^b f(x)dx \leq \frac{e^{2L\beta}-1}{2L\alpha}\int_c^d f(x)dx,"Assume $f(x)>0$ defined in $[a,b]$, and for a certain $L>0$, $f(x)$ satisfies the Lipschitz condition $|f(x_1)-f(x_2)|\leq L|x_1-x_2|$. Assume that for $a\leq c\leq d\leq b$,$$\int_c^d \frac{1}{f(x)}dx=\alpha,\int_a^b\frac{1}{f(x)}dx=\beta$$Try to prove$$\int_a^b f(x)dx \leq \frac{e^{2L\beta}-1}{2L\alpha}\int_c^d f(x)dx$$","Assume $f(x)>0$ defined in $[a,b]$, and for a certain $L>0$, $f(x)$ satisfies the Lipschitz condition $|f(x_1)-f(x_2)|\leq L|x_1-x_2|$. Assume that for $a\leq c\leq d\leq b$,$$\int_c^d \frac{1}{f(x)}dx=\alpha,\int_a^b\frac{1}{f(x)}dx=\beta$$Try to prove$$\int_a^b f(x)dx \leq \frac{e^{2L\beta}-1}{2L\alpha}\int_c^d f(x)dx$$",,"['calculus', 'inequality', 'integral-inequality', 'lipschitz-functions']"
20,"Apostol Calculus, Volume I, Chapter 8.7, Exercises 9 & 10 (p. 320)","Apostol Calculus, Volume I, Chapter 8.7, Exercises 9 & 10 (p. 320)",,"I'm actually only having trouble with #10, but #10 relies on the statement and results of #9, so I'll include both questions for completeness. #9.   In a tank are $100$ gallons of brine containing $50$ pounds of dissolved salt.  Water runs into the tank at the rate of $3$ gallons per minute, and the concentration is kept uniform by stirring.  How much salt is in the tank at the end of one hour if the mixture runs out at a rate of $2$ gallons per minute? #10.  Refer to Exercise 9.  Suppose the bottom of the tank is covered with a mixture of salt and insoluble material.  Assume that the salt dissolves at a rate proportional to the difference between the concentration of the solution and that of a saturated solution (3 pounds of salt per gallon), and that if the water were fresh 1 pound of salt would dissolve per minute.  How much salt will be in solution at the end of one hour? First, I define $y = f(t) =$ number of pounds of salt in solution at time $t$.  To write an equation for $y'$ we want to determine the amount of salt being added by the dissolving substance and the amount being removed by the solution running off.  The amount added by the dissolving substance is given by $$ k \left(3 - \dfrac{f(t)}{100+t}\right)$$ Since we are told that the rate is proportional to the difference between $3$ pounds of salt per gallon and the current concentration of salt.  Further, we are told that if the concentration of salt is $0$ then the rate is $1$ pound per minute; hence, $3k = 1 \implies k = \frac{1}{3}$. Then, from #9 we have the rate at which salt is exiting the solution given by $$-2 \left(\dfrac{f(t)}{100+t}\right)$$ Putting this together we have $$y' = -2\left(\dfrac{f(t)}{100+t}\right) + 1 - \dfrac{f(t)}{3(100+t)}$$ Thus, giving us the first-order linear differential equation: $$y' + \dfrac{7}{3(100+t)}y = 1$$ From this we have the unique solution $y = f(t)$ with $f(0) = 50$ given by $$\begin{align*} y &= 50 \dfrac{100^{7/3}}{(100+t)^{7/3}} + \dfrac{1}{(100+t)^{7/3}} \int_0^t (100+x)^{7/3} dx\\ &= 50 \dfrac{100^{7/3}}{(100+t)^{7/3}} + \dfrac{3t}{10}\\ \implies f(60) &= 34.7 \text{ pounds of salt} \end{align*}$$ Unfortunately, the solution Apostol gives is $54.7$ pounds of salt.  I cannot seem to find the error that is causing me to be off.  The answers have an obvious similarity, but I don't know if that is just coincidence or if it means there is a small(ish) error in there somewhere. Thanks for your help.","I'm actually only having trouble with #10, but #10 relies on the statement and results of #9, so I'll include both questions for completeness. #9.   In a tank are $100$ gallons of brine containing $50$ pounds of dissolved salt.  Water runs into the tank at the rate of $3$ gallons per minute, and the concentration is kept uniform by stirring.  How much salt is in the tank at the end of one hour if the mixture runs out at a rate of $2$ gallons per minute? #10.  Refer to Exercise 9.  Suppose the bottom of the tank is covered with a mixture of salt and insoluble material.  Assume that the salt dissolves at a rate proportional to the difference between the concentration of the solution and that of a saturated solution (3 pounds of salt per gallon), and that if the water were fresh 1 pound of salt would dissolve per minute.  How much salt will be in solution at the end of one hour? First, I define $y = f(t) =$ number of pounds of salt in solution at time $t$.  To write an equation for $y'$ we want to determine the amount of salt being added by the dissolving substance and the amount being removed by the solution running off.  The amount added by the dissolving substance is given by $$ k \left(3 - \dfrac{f(t)}{100+t}\right)$$ Since we are told that the rate is proportional to the difference between $3$ pounds of salt per gallon and the current concentration of salt.  Further, we are told that if the concentration of salt is $0$ then the rate is $1$ pound per minute; hence, $3k = 1 \implies k = \frac{1}{3}$. Then, from #9 we have the rate at which salt is exiting the solution given by $$-2 \left(\dfrac{f(t)}{100+t}\right)$$ Putting this together we have $$y' = -2\left(\dfrac{f(t)}{100+t}\right) + 1 - \dfrac{f(t)}{3(100+t)}$$ Thus, giving us the first-order linear differential equation: $$y' + \dfrac{7}{3(100+t)}y = 1$$ From this we have the unique solution $y = f(t)$ with $f(0) = 50$ given by $$\begin{align*} y &= 50 \dfrac{100^{7/3}}{(100+t)^{7/3}} + \dfrac{1}{(100+t)^{7/3}} \int_0^t (100+x)^{7/3} dx\\ &= 50 \dfrac{100^{7/3}}{(100+t)^{7/3}} + \dfrac{3t}{10}\\ \implies f(60) &= 34.7 \text{ pounds of salt} \end{align*}$$ Unfortunately, the solution Apostol gives is $54.7$ pounds of salt.  I cannot seem to find the error that is causing me to be off.  The answers have an obvious similarity, but I don't know if that is just coincidence or if it means there is a small(ish) error in there somewhere. Thanks for your help.",,['calculus']
21,How to prove $|F(x)|\leq\frac{M(b-a)^2}{8}$,How to prove,|F(x)|\leq\frac{M(b-a)^2}{8},"$f(x)$ is derivable in $[a,b]$, $|f^{'}(x)|\leq M$. $\int_a^b f(x)dx=0$. Let $F(x)=\int_a^x f(t)dt$. Try to prove $|F(x)|\leq\frac{M(b-a)^2}{8}$ I want to use Taylor expansion at $f(\xi)=0$, but I can't continue.","$f(x)$ is derivable in $[a,b]$, $|f^{'}(x)|\leq M$. $\int_a^b f(x)dx=0$. Let $F(x)=\int_a^x f(t)dt$. Try to prove $|F(x)|\leq\frac{M(b-a)^2}{8}$ I want to use Taylor expansion at $f(\xi)=0$, but I can't continue.",,['calculus']
22,Limit $\frac{0}{0}$ which tends to $\frac{\pi}{2}$,Limit  which tends to,\frac{0}{0} \frac{\pi}{2},"I'm trying to evaluate the following limit: $$\lim_{x\rightarrow\pi/2}\frac{\cos(x)}{(1-\sin(x))^{2/3}}$$ The limit has the form $\frac{0}{0}$, I've tried using L'Hopital's rule but I can't resolve it. Any idea?","I'm trying to evaluate the following limit: $$\lim_{x\rightarrow\pi/2}\frac{\cos(x)}{(1-\sin(x))^{2/3}}$$ The limit has the form $\frac{0}{0}$, I've tried using L'Hopital's rule but I can't resolve it. Any idea?",,['calculus']
23,Forced wave equation,Forced wave equation,,"If a system satisfies the equation $$\nu^2 {\partial^2 \psi\over \partial x^2}={\partial^2 \psi\over \partial t^2}+a{\partial \psi\over \partial t}-b\sin\left({\pi x \over L}\right)\cos\left({\pi \nu t\over L}\right)$$ subjected to conditions: $\psi(0,t)=\psi(L,t)={\partial \psi(x,0)\over \partial t}=0$ and $\psi(x,0)=c\sin\left({\pi x\over L}\right)$, how might I solve this? I can solve the equation $$\nu^2 {\partial^2 \psi\over \partial x^2}={\partial^2 \psi\over \partial t^2}+a{\partial \psi\over \partial t}$$ by separation of variables. But I don't know how to deal with the $$b\sin\left({\pi x \over L}\right)\cos\left({\pi \nu t\over L}\right)$$ term. Also, what is the ""forced component"" of $\psi(x,t)$? Thanks.","If a system satisfies the equation $$\nu^2 {\partial^2 \psi\over \partial x^2}={\partial^2 \psi\over \partial t^2}+a{\partial \psi\over \partial t}-b\sin\left({\pi x \over L}\right)\cos\left({\pi \nu t\over L}\right)$$ subjected to conditions: $\psi(0,t)=\psi(L,t)={\partial \psi(x,0)\over \partial t}=0$ and $\psi(x,0)=c\sin\left({\pi x\over L}\right)$, how might I solve this? I can solve the equation $$\nu^2 {\partial^2 \psi\over \partial x^2}={\partial^2 \psi\over \partial t^2}+a{\partial \psi\over \partial t}$$ by separation of variables. But I don't know how to deal with the $$b\sin\left({\pi x \over L}\right)\cos\left({\pi \nu t\over L}\right)$$ term. Also, what is the ""forced component"" of $\psi(x,t)$? Thanks.",,"['calculus', 'partial-differential-equations']"
24,Probabilistic proof of existence of an integer,Probabilistic proof of existence of an integer,,"The prime number theorem (PNT) says that an integer $n$ is prime with probability $\frac{1}{\ln n}$. Using only PNT, it's conceivable that each integer upto $10^{10^{10}}$ is non-prime. However using other methods this is found not to be the case. Is there a theorem analogous to PNT such that each integer is assigned a definite positive probability to satisfy a property $P(n)$, but no integer that satisfy $P(n)$ is known? However if we sum the probabilities, and it sums to $1$, does it prove the existence of such an integer? And also such that there are no other (then probabilistic) methods to prove the existence of an integer that satisfy $P(n)$. And must it necessarily prove the existence of an infinite such integers (that satisfy $P(n)$)?","The prime number theorem (PNT) says that an integer $n$ is prime with probability $\frac{1}{\ln n}$. Using only PNT, it's conceivable that each integer upto $10^{10^{10}}$ is non-prime. However using other methods this is found not to be the case. Is there a theorem analogous to PNT such that each integer is assigned a definite positive probability to satisfy a property $P(n)$, but no integer that satisfy $P(n)$ is known? However if we sum the probabilities, and it sums to $1$, does it prove the existence of such an integer? And also such that there are no other (then probabilistic) methods to prove the existence of an integer that satisfy $P(n)$. And must it necessarily prove the existence of an infinite such integers (that satisfy $P(n)$)?",,"['calculus', 'probability', 'sequences-and-series', 'set-theory', 'analytic-number-theory']"
25,Simple Trig Equation - Blank Mind,Simple Trig Equation - Blank Mind,,"I'm trying to set a simple trig equation equal to zero, for use with the first derivative test, etc. I have: $-2(\sin\theta+\sin2\theta)$ So, I need to get $\sin\theta=-\sin2\theta$ I'm drawing a complete blank here. If it matters, but it shouldn't the original equation I've derived from is: $f(\theta)=2\cos\theta+\cos2\theta$ I guess what I'm really asking for is a refresher in solving trig equations, if someone doesn't mind. :)","I'm trying to set a simple trig equation equal to zero, for use with the first derivative test, etc. I have: $-2(\sin\theta+\sin2\theta)$ So, I need to get $\sin\theta=-\sin2\theta$ I'm drawing a complete blank here. If it matters, but it shouldn't the original equation I've derived from is: $f(\theta)=2\cos\theta+\cos2\theta$ I guess what I'm really asking for is a refresher in solving trig equations, if someone doesn't mind. :)",,"['calculus', 'trigonometry']"
26,Asymptotic behavior of $\sum_{i>0} x^{p_i}$ as $x \to 1^-$,Asymptotic behavior of  as,\sum_{i>0} x^{p_i} x \to 1^-,"The sum of natural numbers $ \sum_{n>0} x^n = \frac{x}{1-x}$, so as $x\to1^-$ it diverges as $(1-x)^{-1}$. So I wondered what would happen if we make the summation set thinner, i.e. $\sum_{n \in A} x^n$ for $A \subset \mathbb{N}$. EDIT After receiving helpful suggestions,  I guess I understand it a little better. If $A$ is a finite subset, than $ \lim_{x\to 1^-} \sum_{n \in A} x^n = \vert A \vert$. Thus $$    \lim_{x\to 1^-} \frac{\sum_{k \in (A \cap (1,n))} x^k}{\sum_{k=1}^n x^k} = \frac{\vert A \vert}{n} $$ Taking the limit $n\to \infty$ the ratio gives the asymptotic density of $A$ in $\mathbb{N}$ if it exists. Indeed $$  \sum_{n=1}^\infty x^{2n} = \frac{x^2}{1-x^2}  \;\;\; \text{ therefore } \;\;\; \lim_{x->1^-} (1-x) \frac{x^2}{1-x^2} = \frac{1}{2} $$ What I still do not understand is how would I find the asymptotic behavior of the sum in case when the limit $\frac{\vert A\vert}{n}$ goes to zero, like in the case of $A$ being the set of prime numbers $A = \mathbb{P}$. I guess the technique to be used here is to apply Mellin transform, and related the divergence rate to properties of zeta function. $$    \sum_{n>=1} \int_0^\infty t^{s-1} e^{-t n} dt = \Gamma(s) \sum_{n>=1} n^{-s} = \Gamma(s) \zeta(s) $$ In the case of prime set  this give $\Gamma(s) \zeta_\mathbb{P}(s)$ which goes as $log(s-1)$ for $s\to 1^+$, but I do not see it through yet..","The sum of natural numbers $ \sum_{n>0} x^n = \frac{x}{1-x}$, so as $x\to1^-$ it diverges as $(1-x)^{-1}$. So I wondered what would happen if we make the summation set thinner, i.e. $\sum_{n \in A} x^n$ for $A \subset \mathbb{N}$. EDIT After receiving helpful suggestions,  I guess I understand it a little better. If $A$ is a finite subset, than $ \lim_{x\to 1^-} \sum_{n \in A} x^n = \vert A \vert$. Thus $$    \lim_{x\to 1^-} \frac{\sum_{k \in (A \cap (1,n))} x^k}{\sum_{k=1}^n x^k} = \frac{\vert A \vert}{n} $$ Taking the limit $n\to \infty$ the ratio gives the asymptotic density of $A$ in $\mathbb{N}$ if it exists. Indeed $$  \sum_{n=1}^\infty x^{2n} = \frac{x^2}{1-x^2}  \;\;\; \text{ therefore } \;\;\; \lim_{x->1^-} (1-x) \frac{x^2}{1-x^2} = \frac{1}{2} $$ What I still do not understand is how would I find the asymptotic behavior of the sum in case when the limit $\frac{\vert A\vert}{n}$ goes to zero, like in the case of $A$ being the set of prime numbers $A = \mathbb{P}$. I guess the technique to be used here is to apply Mellin transform, and related the divergence rate to properties of zeta function. $$    \sum_{n>=1} \int_0^\infty t^{s-1} e^{-t n} dt = \Gamma(s) \sum_{n>=1} n^{-s} = \Gamma(s) \zeta(s) $$ In the case of prime set  this give $\Gamma(s) \zeta_\mathbb{P}(s)$ which goes as $log(s-1)$ for $s\to 1^+$, but I do not see it through yet..",,"['calculus', 'prime-numbers']"
27,How to integrate by parts in spherical coordinates,How to integrate by parts in spherical coordinates,,"I'm running into some troubles with the integration of a spherically symmetric 3D function.  I'm having the following expression to evaluate : $$ I=\int_0^{2\pi} d\phi \int_0^\pi \sin\theta d\theta\int_0^\infty \frac{\partial f}{\partial r} r^2 dr $$ where the generic function $f$ only depends on the radius $r$. If I where in Cartesian coordinates, there wouldn't be any $r^2$ term and the integral would be 0 provided that $f$ vanishes at infinity (which is ok). Now, in spherical coordinates, this does not seem to be true. If I integrate first over the angles, I get a factor $4\pi$ in front of the radial integral. I can then integrate by parts to remove the partial derivative and I obtain : $$ I=-8\pi\int_0^\infty f(r)r dr $$ as the boundary terms vanish if $f(r) \rightarrow 0$. At first sight, this seems to be correct. However, when looking at the definition of integration by parts in $R^n$: $$ \int_{\Omega} \frac{\partial u}{\partial x_i} v \,d\Omega = \int_{\Gamma} u v \, \nu_i \,d\Gamma - \int_{\Omega} u \frac{\partial v}{\partial x_i} \, d\Omega $$ Where $\nu_i$ is a unit vector on $\Gamma$ the boundary of $\Omega$. My previous computation does not seem to be right. The $r^2$ term is part of the integration measure $d\Omega$ and should thus not be taken into account for the ""derivative-swap"". This would mean that my first integral is $0$ for all functions $f$ that behave nicely and vanish sufficiently fast as $\rightarrow\infty$. $$ I= \left| f(r) r^2 \right|_{r=0}^{r=\infty} = 0 $$ Which (if any) of the two version is correct ? Do you know any good reference on integration by parts in non-cartesian coordinates.","I'm running into some troubles with the integration of a spherically symmetric 3D function.  I'm having the following expression to evaluate : $$ I=\int_0^{2\pi} d\phi \int_0^\pi \sin\theta d\theta\int_0^\infty \frac{\partial f}{\partial r} r^2 dr $$ where the generic function $f$ only depends on the radius $r$. If I where in Cartesian coordinates, there wouldn't be any $r^2$ term and the integral would be 0 provided that $f$ vanishes at infinity (which is ok). Now, in spherical coordinates, this does not seem to be true. If I integrate first over the angles, I get a factor $4\pi$ in front of the radial integral. I can then integrate by parts to remove the partial derivative and I obtain : $$ I=-8\pi\int_0^\infty f(r)r dr $$ as the boundary terms vanish if $f(r) \rightarrow 0$. At first sight, this seems to be correct. However, when looking at the definition of integration by parts in $R^n$: $$ \int_{\Omega} \frac{\partial u}{\partial x_i} v \,d\Omega = \int_{\Gamma} u v \, \nu_i \,d\Gamma - \int_{\Omega} u \frac{\partial v}{\partial x_i} \, d\Omega $$ Where $\nu_i$ is a unit vector on $\Gamma$ the boundary of $\Omega$. My previous computation does not seem to be right. The $r^2$ term is part of the integration measure $d\Omega$ and should thus not be taken into account for the ""derivative-swap"". This would mean that my first integral is $0$ for all functions $f$ that behave nicely and vanish sufficiently fast as $\rightarrow\infty$. $$ I= \left| f(r) r^2 \right|_{r=0}^{r=\infty} = 0 $$ Which (if any) of the two version is correct ? Do you know any good reference on integration by parts in non-cartesian coordinates.",,"['calculus', 'spherical-coordinates', 'integration']"
28,Computing a surface area using cylindrical coordinates,Computing a surface area using cylindrical coordinates,,"$z = f(r,\theta)$, where $(r,\theta)$ varies through a region $D$ on the $r\theta$-plane, $r$ non-negative. I need to show that the surface area of the surface is given by  $$\underset{D}{\iint} \sqrt{1 + \left(\frac{\partial f}{\partial r}\right)^2 + \frac{1}{r^2}\left(\frac{\partial f}{\partial\theta}\right)^2}r\,\mathrm dr\,\mathrm d\theta.$$ I can solve this problem except for the part including $(1 / r^2 )$.  I have no idea where that component comes from.  Any help is appreciated.","$z = f(r,\theta)$, where $(r,\theta)$ varies through a region $D$ on the $r\theta$-plane, $r$ non-negative. I need to show that the surface area of the surface is given by  $$\underset{D}{\iint} \sqrt{1 + \left(\frac{\partial f}{\partial r}\right)^2 + \frac{1}{r^2}\left(\frac{\partial f}{\partial\theta}\right)^2}r\,\mathrm dr\,\mathrm d\theta.$$ I can solve this problem except for the part including $(1 / r^2 )$.  I have no idea where that component comes from.  Any help is appreciated.",,"['calculus', 'multivariable-calculus']"
29,Applications of cross product in sciences other than physics,Applications of cross product in sciences other than physics,,"I am familiar with using cross products in physics to answer questions about force and torque, but are there applications to other scientific fields?  The examples that I've seen in most calculus textbooks deal only with physics or engineering questions.","I am familiar with using cross products in physics to answer questions about force and torque, but are there applications to other scientific fields?  The examples that I've seen in most calculus textbooks deal only with physics or engineering questions.",,"['calculus', 'multivariable-calculus']"
30,"Convergence of $a_{0} = 0, a_{n}=f(a_{n-1})$ when $|f'(x)|\leq \frac{5}{6}$",Convergence of  when,"a_{0} = 0, a_{n}=f(a_{n-1}) |f'(x)|\leq \frac{5}{6}","By the mean value theorem it's easy to show that $|a_{n+1}-a_{n}| \leq \frac{5}{6}|a_{n}-a_{n-1}|$ for every n. Next, I thought of saying $|a_{n+1}-a_{n}| \leq ... \leq (\frac{5}{6})^{n}|a_{1}| \to 0$ and somehow show that ** if $M_{n}$ is the closed interval whose end points are $a_{n}$ and $a_{n-1}$ then $a_{n+1} \in M_{n}$ which implies $M_{n+1} \subseteq M_{n}$ and then to finish with Cantor's intersection theorem that gives us convergence of $a_{n}$. But I'm not even sure if ** is correct and I haven't even used the fact that $a_{0} = 0$. EDIT: Following the tip and some more thought I've come up with the following: For every $m\gt n$: $|a_{m}-a_{n}|\leq|a_{m}-a_{m-1}+a_{m-1}-...+a_{n-1}-a_{n}|\leq$ $\leq\sum_{k=n}^{m-1}|a_{k+1}-a_{k}|\leq|a_{1}|\sum_{k=n}^{m-1}(\frac{5}{6})^{k}\le$ $\le|a_{1}|\sum_{k=n}^{\infty}(\frac{5}{6})^{k}=|a_{1}|\frac{(\frac{5}{6})^{n}}{\frac{1}{6}}=6|a_{1}|(\frac{5}{6})^{n} \to 0$ and from here it's easy to show that the sequence is Cauchy. Please correct me if I made an error.","By the mean value theorem it's easy to show that $|a_{n+1}-a_{n}| \leq \frac{5}{6}|a_{n}-a_{n-1}|$ for every n. Next, I thought of saying $|a_{n+1}-a_{n}| \leq ... \leq (\frac{5}{6})^{n}|a_{1}| \to 0$ and somehow show that ** if $M_{n}$ is the closed interval whose end points are $a_{n}$ and $a_{n-1}$ then $a_{n+1} \in M_{n}$ which implies $M_{n+1} \subseteq M_{n}$ and then to finish with Cantor's intersection theorem that gives us convergence of $a_{n}$. But I'm not even sure if ** is correct and I haven't even used the fact that $a_{0} = 0$. EDIT: Following the tip and some more thought I've come up with the following: For every $m\gt n$: $|a_{m}-a_{n}|\leq|a_{m}-a_{m-1}+a_{m-1}-...+a_{n-1}-a_{n}|\leq$ $\leq\sum_{k=n}^{m-1}|a_{k+1}-a_{k}|\leq|a_{1}|\sum_{k=n}^{m-1}(\frac{5}{6})^{k}\le$ $\le|a_{1}|\sum_{k=n}^{\infty}(\frac{5}{6})^{k}=|a_{1}|\frac{(\frac{5}{6})^{n}}{\frac{1}{6}}=6|a_{1}|(\frac{5}{6})^{n} \to 0$ and from here it's easy to show that the sequence is Cauchy. Please correct me if I made an error.",,"['calculus', 'sequences-and-series']"
31,Meaning of Lagrange multiplier always being 0?,Meaning of Lagrange multiplier always being 0?,,"I'm optimizing a family of objectives over $x_1,\ldots,x_n$ with a single constraint that $x$'s add up to $1$, and when using method of Lagrange multipliers, the single multiplier always ends up $0$ for members of this family... does this tell us anything interesting about these objectives? Edit: the answer was -- it means the constraint it inactive, ie, removing the constraint doesn't change the answer of optimization problem","I'm optimizing a family of objectives over $x_1,\ldots,x_n$ with a single constraint that $x$'s add up to $1$, and when using method of Lagrange multipliers, the single multiplier always ends up $0$ for members of this family... does this tell us anything interesting about these objectives? Edit: the answer was -- it means the constraint it inactive, ie, removing the constraint doesn't change the answer of optimization problem",,"['calculus', 'optimization']"
32,Validating a mathematical model (Lagrange formulation and geometry),Validating a mathematical model (Lagrange formulation and geometry),,"I am working on computing phase diagrams for alloys.  These are blueprints for a material that show what phase, or combination of phases, a material will exist in for a range of concentrations and temperatures (see this pdf presentation ). The crucial step in drawing the boundaries that separate one phase from another on these diagrams involves minimizing a free energy function subject to basic physical conservation constraints.  I am going to leave out the chemistry/physics and hope that we can move forward with the minimization using Lagrange multipliers. The free energy that is to be minimized is this: $\widetilde{G}(x_1, x_2) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2),$ subject to: $f^{(1)}x_1 + f^{(2)}x_2 = c_1,$ $f^{(1)} + f^{(2)} = 1. $ (and also that the $x_{i} > 0$ and $f^{(i)} > 0$, for $i=1,2$.) The Lagrange formulation is: $L(x_1,x_2,f^{(1)},f^{(2)},\lambda_1, \lambda_2, \lambda_3) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2)$ $- \lambda_{1}(f^{(1)}x_1 + f^{(2)}x_2 - c_1)$ $- \lambda_{2}(f^{(1)} + f^{(2)} - 1) $ The minimization of $\widetilde{G}$ follows from finding the $x_{i}$'s  that satisfy $\nabla L = 0:$ $\frac{\partial L}{\partial x_{1}}   = f^{(1)}G_{1}'(x_1) - \lambda_{1}f^{(1)} = 0$ $\frac{\partial L}{\partial x_2}     = f^{(2)}G_{2}'(x_2) - \lambda_{1}f^{(2)} = 0$ $\frac{\partial L}{\partial f^{(1)}} = G_{1}(x_1) - \lambda_{1}x_{1} - \lambda_2 = 0$ $\frac{\partial L}{\partial f^{(2)}} = G_{2}(x_2) - \lambda_{1}x_{2} - \lambda_2 = 0$ which yields: $(*) f^{(1)}\left[G_{1}'(x_1) - \lambda_1 \right] = 0$ $(**) f^{(2)}\left[G_{2}'(x_2) - \lambda_1 \right]= 0 $ $(\***) G_{1}(x_1) - G_{2}(x_2) = \lambda_1 \left[ x_1 - x_2\right]$ Because $f^{(1)}$ and $f^{(2)}$ are not to be zero, from (*) and (**) we have that $G_{1}'(x_1) = G_{2}'(x_2) = \lambda_{1}.$ And, a manipulation of equation (***) looks like $\frac{G_{1}(x_1) -G_{2}(x_2)}{x_1 - x_2} = \lambda_{1}.$ Now, think of $G_{i}$ as an even degree polynomial (which it isn't, but it's graph sometimes resembles one) in the plane.  Let the points $x_1$ and $x_2$ be locations along the x-axis that lie roughly below the minima of this curve.  The constraints (*),(**), and (***) describe the condition that the line drawn between $(x_1,G_{1}(x_1))$ and $(x_2,G_{2}(x_2))$ form a common tangent to the ""wells"" of the curve.  It is these points $x_1$ and $x_2$, which represent concentrations of pure components in our alloy, that become mapped onto a phase diagram.  It is essentially by repeating this procedure for many temperatures that we can trace out the boundaries in the desired phase diagram. The question is: Looking at this from a purely analytic geometry perspective, how would one derive the ""variational"" approach to find a common tangent line that we seem to have found using the above Lagrangian? (warning: I don't really know how to model things using variational methods.) And, secondly: I have presented a model of a binary alloy, meaning two variables to keep track of representing concentrations.  I have been working on ternary alloys, where this free energy $\widetilde{G}$ is a function of three variables (two independent: $x_1,x_2,x_3$, where $x_3 = 1- x_1 - x_2$) and is therefore a surface over a Gibbs triangle.  Then $\nabla L = 0$ produces partial derivatives that no longer ""speak geometry"" to me, although the solution is a common tangent plane.  (I have attempted to characterize a common tangent plane based purely in analytic geometry - completely disregarding the Lagrangian - and have come up with several relations between directional derivatives... How might directional derivatives relate to the optimality conditions set forth by the Lagrangian? ) EDIT: Thank you Greg Graviton for wading through this sub-optimal notation and pointing out several mistakes in the statement of the problem.  (Also, thank you for the excellent discussion below .)","I am working on computing phase diagrams for alloys.  These are blueprints for a material that show what phase, or combination of phases, a material will exist in for a range of concentrations and temperatures (see this pdf presentation ). The crucial step in drawing the boundaries that separate one phase from another on these diagrams involves minimizing a free energy function subject to basic physical conservation constraints.  I am going to leave out the chemistry/physics and hope that we can move forward with the minimization using Lagrange multipliers. The free energy that is to be minimized is this: $\widetilde{G}(x_1, x_2) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2),$ subject to: $f^{(1)}x_1 + f^{(2)}x_2 = c_1,$ $f^{(1)} + f^{(2)} = 1. $ (and also that the $x_{i} > 0$ and $f^{(i)} > 0$, for $i=1,2$.) The Lagrange formulation is: $L(x_1,x_2,f^{(1)},f^{(2)},\lambda_1, \lambda_2, \lambda_3) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2)$ $- \lambda_{1}(f^{(1)}x_1 + f^{(2)}x_2 - c_1)$ $- \lambda_{2}(f^{(1)} + f^{(2)} - 1) $ The minimization of $\widetilde{G}$ follows from finding the $x_{i}$'s  that satisfy $\nabla L = 0:$ $\frac{\partial L}{\partial x_{1}}   = f^{(1)}G_{1}'(x_1) - \lambda_{1}f^{(1)} = 0$ $\frac{\partial L}{\partial x_2}     = f^{(2)}G_{2}'(x_2) - \lambda_{1}f^{(2)} = 0$ $\frac{\partial L}{\partial f^{(1)}} = G_{1}(x_1) - \lambda_{1}x_{1} - \lambda_2 = 0$ $\frac{\partial L}{\partial f^{(2)}} = G_{2}(x_2) - \lambda_{1}x_{2} - \lambda_2 = 0$ which yields: $(*) f^{(1)}\left[G_{1}'(x_1) - \lambda_1 \right] = 0$ $(**) f^{(2)}\left[G_{2}'(x_2) - \lambda_1 \right]= 0 $ $(\***) G_{1}(x_1) - G_{2}(x_2) = \lambda_1 \left[ x_1 - x_2\right]$ Because $f^{(1)}$ and $f^{(2)}$ are not to be zero, from (*) and (**) we have that $G_{1}'(x_1) = G_{2}'(x_2) = \lambda_{1}.$ And, a manipulation of equation (***) looks like $\frac{G_{1}(x_1) -G_{2}(x_2)}{x_1 - x_2} = \lambda_{1}.$ Now, think of $G_{i}$ as an even degree polynomial (which it isn't, but it's graph sometimes resembles one) in the plane.  Let the points $x_1$ and $x_2$ be locations along the x-axis that lie roughly below the minima of this curve.  The constraints (*),(**), and (***) describe the condition that the line drawn between $(x_1,G_{1}(x_1))$ and $(x_2,G_{2}(x_2))$ form a common tangent to the ""wells"" of the curve.  It is these points $x_1$ and $x_2$, which represent concentrations of pure components in our alloy, that become mapped onto a phase diagram.  It is essentially by repeating this procedure for many temperatures that we can trace out the boundaries in the desired phase diagram. The question is: Looking at this from a purely analytic geometry perspective, how would one derive the ""variational"" approach to find a common tangent line that we seem to have found using the above Lagrangian? (warning: I don't really know how to model things using variational methods.) And, secondly: I have presented a model of a binary alloy, meaning two variables to keep track of representing concentrations.  I have been working on ternary alloys, where this free energy $\widetilde{G}$ is a function of three variables (two independent: $x_1,x_2,x_3$, where $x_3 = 1- x_1 - x_2$) and is therefore a surface over a Gibbs triangle.  Then $\nabla L = 0$ produces partial derivatives that no longer ""speak geometry"" to me, although the solution is a common tangent plane.  (I have attempted to characterize a common tangent plane based purely in analytic geometry - completely disregarding the Lagrangian - and have come up with several relations between directional derivatives... How might directional derivatives relate to the optimality conditions set forth by the Lagrangian? ) EDIT: Thank you Greg Graviton for wading through this sub-optimal notation and pointing out several mistakes in the statement of the problem.  (Also, thank you for the excellent discussion below .)",,"['calculus', 'geometry', 'applications']"
33,"Improper Integral $\int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t$",Improper Integral,"\int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t","The integral I'm trying to solve is \begin{align*}     I = \int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t. \end{align*} Using Frullani's integral, note that \begin{align*}     \log(t) = - \, \int_{0}^{\infty} \frac{e^{-xt}-e^{-x}}{x} \, d x. \end{align*} Hence, \begin{align*}     I &= \int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t\\     &= \int_{0}^{\infty} \left[ \int_{0}^{\infty} \frac{e^{-xt}-e^{-x}}{x} \, d x \right] \left[ \int_{0}^{\infty} \frac{e^{-yt}-e^{-y}}{y} \, d y \right] t^{\frac{1}{2}} e^{-t} \, d t\\     &= \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t} \left(e^{-xt}-e^{-x}\right) \left(e^{-yt}-e^{-y}\right) \, d t \right] \, d x \, d y\\     &= \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+x+y)} \, d t - \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+x)-y} \, d t \right.\\     &\qquad \left. - \, \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+y)-x} \, d t + \int_{0}^{\infty} t^{\frac{1}{2}} e^{-x-y-t} \, d t \right] \, d x \, d y\\     &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \frac{1}{(1+x+y)^{3/2}} - \frac{e^{-y}}{(1+x)^{3/2}} - \frac{e^{-x}}{(1+y)^{3/2}} + e^{-x-y} \right] \, d x \, d y. \end{align*} Integrating by parts with $\ d v=\frac{1}{x} \, d x$ , we have \begin{align*}     I &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ \left[ \log(x) \left( \frac{1}{(1+x+y)^{3/2}} - \frac{e^{-y}}{(1+x)^{3/2}} - \frac{e^{-x}}{(1+y)^{3/2}} + e^{-x-y} \right) \right]_{x=0}^{x=\infty} \right.\\     &\qquad \left. - \, \int_{0}^{\infty} \left[ \frac{3e^{-y} \log(x)}{2(1+x)^{5/2}} + \frac{e^{-x} \log(x)}{(1+y)^{3/2}} - \frac{3\log(x)}{2(1+x+y)^{5/2}} - e^{-x-y} \log(x) \right] \, d x \right\} \, d y\\     &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \left[ \frac{3e^{-y} \log(x)}{2(1+x)^{5/2}} + \frac{e^{-x} \log(x)}{(1+y)^{3/2}} - \frac{3\log(x)}{2(1+x+y)^{5/2}} - e^{-x-y} \log(x) \right] \, d x \, d y\\     &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ \frac{3e^{-y}}{2} \int_{0}^{\infty} \frac{\log(x)}{(1+x)^{5/2}} - \frac{1}{(1+y)^{3/2}} \left[ - \int_{0}^{\infty} e^{-x} \log(x) \, d x \right] \right.\\     &\qquad \left. - \, \frac{3}{2} \int_{0}^{\infty} \frac{\log(x)}{(1+x+y)^{5/2}} \, d x + e^{-y} \left[ - \, \int_{0}^{\infty} e^{-x} \log(x) \, d x \right] \right\} \, d y\\     &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ 2e^{-y} [\log(2)-1] - \frac{1}{(1+y)^{3/2}} \gamma - \frac{\log(y+1)}{(y+1)^{3/2}} - \frac{2[\log(2)-1]}{(y+1)^{3/2}} + e^{-y} \gamma \right\} \, d y. \end{align*} Integrating by parts once again with $\ d v=\frac{1}{y} \, d y$ , we have \begin{align}     \nonumber I &= - \, \frac{\sqrt{\pi}}{2} \left\{ \left[ \log(y) \left( 2e^{-y} [\log(2)-1] - \frac{1}{(1+y)^{3/2}} \gamma - \frac{\log(y+1)}{(y+1)^{3/2}} \right. \right. \right.\\     \nonumber &\qquad \left. \left. \left. - \, \frac{2[\log(2)-1]}{(y+1)^{3/2}} + e^{-y} \gamma \right) \right]_{0}^{\infty} - \int_{0}^{\infty} \left[ \frac{3\log(y)}{2(1+y)^{5/2}} + \frac{3[\log(2)-1] \log(y)}{(y+1)^{5/2}} \right. \right.\\     \nonumber &\qquad \left. \left. - \, 2e^{-y} \log(y) [\log(2)-1] - \frac{\log(y)}{(y+1)^{5/2}} + \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} - e^{-y} \log(y) \gamma \right] \, d y \right\}\\     &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \left[ \frac{3\log(y)}{2(1+y)^{5/2}} + \frac{3[\log(2)-1] \log(y)}{(y+1)^{5/2}} - 2e^{-y} \log(y) [\log(2)-1] \right. \\     \nonumber &\qquad \left. - \, \frac{\log(y)}{(y+1)^{5/2}} + \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} - e^{-y} \log(y) \gamma \right] \, d y \\     \nonumber &= \ldots\\     \nonumber &= \frac{\sqrt{\pi}}{2} \left[4\log^2(2) + (4\gamma-8)\log(2) + \frac{\pi^2}{2} + \gamma^2 - 4\gamma\right]. \end{align} I could almost solve the integral in the third-to-last line. However, in the said integral, I haven't found the answer to \begin{align*}     \int_{0}^{\infty} \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} \, d y. \end{align*} Firstly, I have no idea how to solve this particular integral. Secondly, I'm also not sure if my whole calculation is already correct. Can anyone help me with this problem? I am open to any suggestions/solutions for solving the very last integral, or even for the very first integral in the question. Any help/feedback is much appreciated. Thank you!","The integral I'm trying to solve is Using Frullani's integral, note that Hence, Integrating by parts with , we have Integrating by parts once again with , we have I could almost solve the integral in the third-to-last line. However, in the said integral, I haven't found the answer to Firstly, I have no idea how to solve this particular integral. Secondly, I'm also not sure if my whole calculation is already correct. Can anyone help me with this problem? I am open to any suggestions/solutions for solving the very last integral, or even for the very first integral in the question. Any help/feedback is much appreciated. Thank you!","\begin{align*}
    I = \int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t.
\end{align*} \begin{align*}
    \log(t) = - \, \int_{0}^{\infty} \frac{e^{-xt}-e^{-x}}{x} \, d x.
\end{align*} \begin{align*}
    I &= \int_{0}^{\infty} \log^2(t) t^{\frac{1}{2}} e^{-t} \, d t\\
    &= \int_{0}^{\infty} \left[ \int_{0}^{\infty} \frac{e^{-xt}-e^{-x}}{x} \, d x \right] \left[ \int_{0}^{\infty} \frac{e^{-yt}-e^{-y}}{y} \, d y \right] t^{\frac{1}{2}} e^{-t} \, d t\\
    &= \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t} \left(e^{-xt}-e^{-x}\right) \left(e^{-yt}-e^{-y}\right) \, d t \right] \, d x \, d y\\
    &= \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+x+y)} \, d t - \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+x)-y} \, d t \right.\\
    &\qquad \left. - \, \int_{0}^{\infty} t^{\frac{1}{2}} e^{-t(1+y)-x} \, d t + \int_{0}^{\infty} t^{\frac{1}{2}} e^{-x-y-t} \, d t \right] \, d x \, d y\\
    &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \frac{1}{x} \left[ \frac{1}{(1+x+y)^{3/2}} - \frac{e^{-y}}{(1+x)^{3/2}} - \frac{e^{-x}}{(1+y)^{3/2}} + e^{-x-y} \right] \, d x \, d y.
\end{align*} \ d v=\frac{1}{x} \, d x \begin{align*}
    I &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ \left[ \log(x) \left( \frac{1}{(1+x+y)^{3/2}} - \frac{e^{-y}}{(1+x)^{3/2}} - \frac{e^{-x}}{(1+y)^{3/2}} + e^{-x-y} \right) \right]_{x=0}^{x=\infty} \right.\\
    &\qquad \left. - \, \int_{0}^{\infty} \left[ \frac{3e^{-y} \log(x)}{2(1+x)^{5/2}} + \frac{e^{-x} \log(x)}{(1+y)^{3/2}} - \frac{3\log(x)}{2(1+x+y)^{5/2}} - e^{-x-y} \log(x) \right] \, d x \right\} \, d y\\
    &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \int_{0}^{\infty} \left[ \frac{3e^{-y} \log(x)}{2(1+x)^{5/2}} + \frac{e^{-x} \log(x)}{(1+y)^{3/2}} - \frac{3\log(x)}{2(1+x+y)^{5/2}} - e^{-x-y} \log(x) \right] \, d x \, d y\\
    &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ \frac{3e^{-y}}{2} \int_{0}^{\infty} \frac{\log(x)}{(1+x)^{5/2}} - \frac{1}{(1+y)^{3/2}} \left[ - \int_{0}^{\infty} e^{-x} \log(x) \, d x \right] \right.\\
    &\qquad \left. - \, \frac{3}{2} \int_{0}^{\infty} \frac{\log(x)}{(1+x+y)^{5/2}} \, d x + e^{-y} \left[ - \, \int_{0}^{\infty} e^{-x} \log(x) \, d x \right] \right\} \, d y\\
    &= - \, \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \frac{1}{y} \left\{ 2e^{-y} [\log(2)-1] - \frac{1}{(1+y)^{3/2}} \gamma - \frac{\log(y+1)}{(y+1)^{3/2}} - \frac{2[\log(2)-1]}{(y+1)^{3/2}} + e^{-y} \gamma \right\} \, d y.
\end{align*} \ d v=\frac{1}{y} \, d y \begin{align}
    \nonumber I &= - \, \frac{\sqrt{\pi}}{2} \left\{ \left[ \log(y) \left( 2e^{-y} [\log(2)-1] - \frac{1}{(1+y)^{3/2}} \gamma - \frac{\log(y+1)}{(y+1)^{3/2}} \right. \right. \right.\\
    \nonumber &\qquad \left. \left. \left. - \, \frac{2[\log(2)-1]}{(y+1)^{3/2}} + e^{-y} \gamma \right) \right]_{0}^{\infty} - \int_{0}^{\infty} \left[ \frac{3\log(y)}{2(1+y)^{5/2}} + \frac{3[\log(2)-1] \log(y)}{(y+1)^{5/2}} \right. \right.\\
    \nonumber &\qquad \left. \left. - \, 2e^{-y} \log(y) [\log(2)-1] - \frac{\log(y)}{(y+1)^{5/2}} + \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} - e^{-y} \log(y) \gamma \right] \, d y \right\}\\
    &= \frac{\sqrt{\pi}}{2} \int_{0}^{\infty} \left[ \frac{3\log(y)}{2(1+y)^{5/2}} + \frac{3[\log(2)-1] \log(y)}{(y+1)^{5/2}} - 2e^{-y} \log(y) [\log(2)-1] \right. \\
    \nonumber &\qquad \left. - \, \frac{\log(y)}{(y+1)^{5/2}} + \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} - e^{-y} \log(y) \gamma \right] \, d y \\
    \nonumber &= \ldots\\
    \nonumber &= \frac{\sqrt{\pi}}{2} \left[4\log^2(2) + (4\gamma-8)\log(2) + \frac{\pi^2}{2} + \gamma^2 - 4\gamma\right].
\end{align} \begin{align*}
    \int_{0}^{\infty} \frac{3 \log(y+1) \log(y)}{2(y+1)^{5/2}} \, d y.
\end{align*}","['calculus', 'integration', 'improper-integrals']"
34,Computing $ I_n=\int_0^1 \frac{x^n}{6+x-x^2} d x $,Computing, I_n=\int_0^1 \frac{x^n}{6+x-x^2} d x ,"After reading the reduction formula in the post , I am curious about the closed form of the integral $$ I_n=\int_0^1 \frac{x^n}{6+x-x^2} d x $$ I first resolve the integrand into two partial fractions as $$ I_n=\frac{1}{5}  \left[\underbrace{\int_0^1 \frac{x^n}{3-x} d x}_{J_n} +\underbrace{\int_0^1 \frac{x^n}{2+x} d x}_{K_n} \right] $$ Then I tackle the two integrals in general, $$ \begin{aligned} \int_0^1 \frac{x^n}{a-x} d x & =\int_{a-1}^a \frac{(a-x)^n}{x} d x \\ \\ & =\int_{a-1}^a\left(\frac{a^n}{x}-\sum_{k=1}^n\left(\begin{array}{c} n \\ k \end{array}\right) a^{n-k} x^{k-1}\right) dx \\ & =a^n \ln \left(\frac{a}{a-1}\right)-\sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right) \frac{a^{n}-a^{n-k}(a-1)^k}{k} \end{aligned} $$ Hence $$J_n =3^n \ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right) \frac{3^{n}-3^{n-k}2^k}{k} $$ and $$K_n= -\left[(-2)^n \ln \left(\frac{2}{3}\right)-\sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right) \frac{(-2)^{n}-(-1)^n2^{n-k}3^k}{k} \right]$$ We can now conclude that the closed form of the integral is $$ \begin{aligned}I_n=  &\ \frac{1}{5} \left[ 3^n\left[\ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right) \frac{1-\left(\frac{2}{3}\right)^k}{k}\right]+ (-2)^{n}\left[\ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right) \frac{1-\left(\frac{3}{2}\right)^k}{k}\right]\right]\\=& \frac{3^n+(-2)^n}{5} \ln \left(\frac{3}{2}\right) -\frac{1}{5}\left[\sum_{k=1}^n\left(\begin{array}{l} n \\ k \end{array}\right) \frac{1}{k}\left(3^n\left(1-\left(\frac{2}{3}\right)^k\right)+(-2)^n\left(1-\left(\frac{3}{2}\right)^k\right)\right]\right. \end{aligned} $$ My question: Is there any alternative method? Your comments and alternative methods are highly appreciated.","After reading the reduction formula in the post , I am curious about the closed form of the integral I first resolve the integrand into two partial fractions as Then I tackle the two integrals in general, Hence and We can now conclude that the closed form of the integral is My question: Is there any alternative method? Your comments and alternative methods are highly appreciated.","
I_n=\int_0^1 \frac{x^n}{6+x-x^2} d x
 
I_n=\frac{1}{5}  \left[\underbrace{\int_0^1 \frac{x^n}{3-x} d x}_{J_n} +\underbrace{\int_0^1 \frac{x^n}{2+x} d x}_{K_n} \right]
 
\begin{aligned}
\int_0^1 \frac{x^n}{a-x} d x & =\int_{a-1}^a \frac{(a-x)^n}{x} d x \\
\\
& =\int_{a-1}^a\left(\frac{a^n}{x}-\sum_{k=1}^n\left(\begin{array}{c}
n \\
k
\end{array}\right) a^{n-k} x^{k-1}\right) dx \\
& =a^n \ln \left(\frac{a}{a-1}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{a^{n}-a^{n-k}(a-1)^k}{k}
\end{aligned}
 J_n =3^n \ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{3^{n}-3^{n-k}2^k}{k}  K_n= -\left[(-2)^n \ln \left(\frac{2}{3}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{(-2)^{n}-(-1)^n2^{n-k}3^k}{k} \right] 
\begin{aligned}I_n=  &\ \frac{1}{5} \left[ 3^n\left[\ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{1-\left(\frac{2}{3}\right)^k}{k}\right]+ (-2)^{n}\left[\ln \left(\frac{3}{2}\right)-\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{1-\left(\frac{3}{2}\right)^k}{k}\right]\right]\\=& \frac{3^n+(-2)^n}{5} \ln \left(\frac{3}{2}\right) -\frac{1}{5}\left[\sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) \frac{1}{k}\left(3^n\left(1-\left(\frac{2}{3}\right)^k\right)+(-2)^n\left(1-\left(\frac{3}{2}\right)^k\right)\right]\right.
\end{aligned}
","['calculus', 'integration', 'partial-fractions', 'reduction-formula']"
35,Prove that $\lim_{x\to 0^+} 1/x^a \int_0^x t^{a-1} f(t) dt$ exists if and only if $\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt$,Prove that  exists if and only if,\lim_{x\to 0^+} 1/x^a \int_0^x t^{a-1} f(t) dt \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt,"Suppose $f:[0,1]\to\mathbb{R}$ is a function that is continuous on $(0,1].$ Let $a,b>0$ . Prove that $\lim_{x\to 0^+} 1/x^a \int_0^x t^{a-1} f(t) dt$ exists if and only if $\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt$ exists. It seems useful to define $F(x) = \int_0^x t^{a-1} f(t) dt$ . Then $t^{b-1} f(t) = t^{b-a} F'(t)$ . The idea is now to integrate by parts. Clearly by symmetry it is enough to show that if $\lim_{x\to 0^+} 1/x^a F(x)$ exists, then $\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt$ exists. We see that $$\lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt = \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-a} F'(t) dt = \lim_{x\to 0^+} x^{-b} \left[(t^{b-a} F(t))_0^x - (b-a) \int_0^x t^{b-a-1} F(t) dt\right] = \left[\lim_{x\to0^+} 1/x^a F(x) - 1/x^b (b-a) \int_0^x t^{b-a-1} F(t) dt\right].$$ Of course, even though $f$ is continuous on $(0,1]$ , it may not be bounded; take $f(x)=1/x$ for $x>0$ and $f(x)=1$ for $x=0$ . I'm not sure if one can place a bound on $F(x)$ . From above, we just need to show that $- 1/x^b (b-a)\int_0^x t^{b-a-1} F(t) dt$ exists to conclude the proof.","Suppose is a function that is continuous on Let . Prove that exists if and only if exists. It seems useful to define . Then . The idea is now to integrate by parts. Clearly by symmetry it is enough to show that if exists, then exists. We see that Of course, even though is continuous on , it may not be bounded; take for and for . I'm not sure if one can place a bound on . From above, we just need to show that exists to conclude the proof.","f:[0,1]\to\mathbb{R} (0,1]. a,b>0 \lim_{x\to 0^+} 1/x^a \int_0^x t^{a-1} f(t) dt \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt F(x) = \int_0^x t^{a-1} f(t) dt t^{b-1} f(t) = t^{b-a} F'(t) \lim_{x\to 0^+} 1/x^a F(x) \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-1} f(t) dt = \lim_{x\to 0^+} 1/x^b \int_0^x t^{b-a} F'(t) dt = \lim_{x\to 0^+} x^{-b} \left[(t^{b-a} F(t))_0^x - (b-a) \int_0^x t^{b-a-1} F(t) dt\right] = \left[\lim_{x\to0^+} 1/x^a F(x) - 1/x^b (b-a) \int_0^x t^{b-a-1} F(t) dt\right]. f (0,1] f(x)=1/x x>0 f(x)=1 x=0 F(x) - 1/x^b (b-a)\int_0^x t^{b-a-1} F(t) dt","['calculus', 'integration', 'continuity', 'contest-math']"
36,Chance of meeting probability,Chance of meeting probability,,"Two people arrive at a train station some random time between $5$ and $6$ a.m.. Arrival times are uniformly distributed. They stay for exactly five minutes and leave. What's the chance that they meet on a particular day? My given answer is $23/144$ . Equivalently, given $X, Y \sim U(0, 60)$ , we want $P(|X - Y| \leq 5)$ . I know that there's a geometric approach, but I want to know what's wrong with my following approach: I will compute $P(X \leq Y + 5)$ and adjust the answer to account for both cases. $$P(X \leq Y + 5) = \int P(X \leq Y + 5 \mid Y = k)P(Y = k) = \int P(X \leq k + 5)f_{Y}(k) \mathop{dk} $$ $$= \frac{1}{60} \int P(X \leq k + 5).$$ For $0 \leq k \leq 55$ , we have $P(X \leq k + 5) = (k + 5)/60$ . For $55 < k \leq 60$ , we have $P(X \leq k + 5) = 1$ . Thus, $$P(X \leq Y + 5) = \frac{1}{60} \left[\int_{0}^{55} \frac{k + 5}{60} \mathop{dk} + \int_{55}^{60} 1 \mathop{dk}\right]= \frac{1}{60}\left[\frac{715}{24} + 5\right] = \frac{167}{288}$$ This must be wrong because there's no clear way to go from this to $23/144$ . Can someone please point out my mistake?","Two people arrive at a train station some random time between and a.m.. Arrival times are uniformly distributed. They stay for exactly five minutes and leave. What's the chance that they meet on a particular day? My given answer is . Equivalently, given , we want . I know that there's a geometric approach, but I want to know what's wrong with my following approach: I will compute and adjust the answer to account for both cases. For , we have . For , we have . Thus, This must be wrong because there's no clear way to go from this to . Can someone please point out my mistake?","5 6 23/144 X, Y \sim U(0, 60) P(|X - Y| \leq 5) P(X \leq Y + 5) P(X \leq Y + 5) = \int P(X \leq Y + 5 \mid Y = k)P(Y = k) = \int P(X \leq k + 5)f_{Y}(k) \mathop{dk}  = \frac{1}{60} \int P(X \leq k + 5). 0 \leq k \leq 55 P(X \leq k + 5) = (k + 5)/60 55 < k \leq 60 P(X \leq k + 5) = 1 P(X \leq Y + 5) = \frac{1}{60} \left[\int_{0}^{55} \frac{k + 5}{60} \mathop{dk} + \int_{55}^{60} 1 \mathop{dk}\right]= \frac{1}{60}\left[\frac{715}{24} + 5\right] = \frac{167}{288} 23/144","['calculus', 'probability']"
37,Solving $\lim_{n\to\infty}(\frac{(2n)!}{n^n\cdot n!})^{1/n}$,Solving,\lim_{n\to\infty}(\frac{(2n)!}{n^n\cdot n!})^{1/n},"Find the following limit: $$\lim_{n\to\infty}\left(\dfrac{(2n)!}{n^n\cdot n!}\right)^{1/n}$$ My work: Lets assume the given limit be $y$ $$\begin{align}y& = \lim_{n\to\infty}\left(\dfrac{(2n)!}{n^n\cdot n!}\right)^{1/n}\\\\&= \lim_{n\to\infty}\left(\dfrac{(2n)(2n-1)(2n-2)...2.1}{n^n\cdot n!}\right)^{1/n}\\\\&= \lim_{n\to\infty}\left(\dfrac{\Big[(2n)(2n-2)(2n-4)...4.2\Big]\Big[(2n-1)(2n-3)(2n-5)...3.1\Big]}{n^n\cdot n!}\right)^{1/n} \\\\&= \lim_{n\to\infty}\left(\dfrac{\Big[2^n(n!)\Big]\Big[(2n-1)(2n-3)(2n-5)...3.1\Big]}{n^n\cdot n!}\right)^{1/n}\end{align}$$ Taking $\log$ both sides, $$\begin{align}\log(y)&= \lim_{n\to\infty}\left(\dfrac{(4n-2)(4n-6)(4n-8)...(10)(6)(2)}{n^n}\right)^{1/n}\\\\&= \lim_{n\to\infty}\dfrac1n\log\left(\dfrac{(4n-2)(4n-6)(4n-8)...(10)(6)(2)}{n^n}\right)\\\\&\overset{({\Large*})}= \lim_{n\to\infty}\dfrac1n\sum_{r=1}^n\log\left(\dfrac{4n - 2(2r-1)}{n}\right)\\\\&\overset{({\Large*})}= \lim_{n\to\infty}\dfrac1n\sum_{r=1}^n\log\left(4 - \dfrac{4r}{n} + \dfrac2n\right)\\\\&\overset{({\Large*})}= \int_0^1\log(4 - 4x ) dx\end{align}$$ I'm  not sure if the steps (*) are correct or not. Can anyone guide me please.","Find the following limit: My work: Lets assume the given limit be Taking both sides, I'm  not sure if the steps (*) are correct or not. Can anyone guide me please.",\lim_{n\to\infty}\left(\dfrac{(2n)!}{n^n\cdot n!}\right)^{1/n} y \begin{align}y& = \lim_{n\to\infty}\left(\dfrac{(2n)!}{n^n\cdot n!}\right)^{1/n}\\\\&= \lim_{n\to\infty}\left(\dfrac{(2n)(2n-1)(2n-2)...2.1}{n^n\cdot n!}\right)^{1/n}\\\\&= \lim_{n\to\infty}\left(\dfrac{\Big[(2n)(2n-2)(2n-4)...4.2\Big]\Big[(2n-1)(2n-3)(2n-5)...3.1\Big]}{n^n\cdot n!}\right)^{1/n} \\\\&= \lim_{n\to\infty}\left(\dfrac{\Big[2^n(n!)\Big]\Big[(2n-1)(2n-3)(2n-5)...3.1\Big]}{n^n\cdot n!}\right)^{1/n}\end{align} \log \begin{align}\log(y)&= \lim_{n\to\infty}\left(\dfrac{(4n-2)(4n-6)(4n-8)...(10)(6)(2)}{n^n}\right)^{1/n}\\\\&= \lim_{n\to\infty}\dfrac1n\log\left(\dfrac{(4n-2)(4n-6)(4n-8)...(10)(6)(2)}{n^n}\right)\\\\&\overset{({\Large*})}= \lim_{n\to\infty}\dfrac1n\sum_{r=1}^n\log\left(\dfrac{4n - 2(2r-1)}{n}\right)\\\\&\overset{({\Large*})}= \lim_{n\to\infty}\dfrac1n\sum_{r=1}^n\log\left(4 - \dfrac{4r}{n} + \dfrac2n\right)\\\\&\overset{({\Large*})}= \int_0^1\log(4 - 4x ) dx\end{align},"['calculus', 'limits']"
38,Multivariable Calculus Exam Mistake?,Multivariable Calculus Exam Mistake?,,"This question was from an exam taken in January 2022 on a course on introductory multivariable calculus and was worded exactly as follows: ""For a general surface $S$ bounded by a closed curve $C$ show using Stokes theorem that for a vector field, $\mathbf{F}(\mathbf{r})$ $$\int_S\nabla\times(\mathbf{F}\times\mathrm{d}\mathbf{S})=\alpha\int_C\mathbf{F}\times\mathrm{d}\mathbf{r}$$ and identify the constant $\alpha$ ."" The "" $\mathrm{d}\mathbf{S}$ "" is used to denote a surface integral and the "" $\mathrm{d}\mathbf{r}$ "" to denote a line integral. I have asked a couple of mathematicians who work in applied mathematics and they have not been able to prove this either. The LHS is apparently the area of concern - having "" $\nabla\times(\mathbf{F}\times\mathrm{d}\mathbf{S})$ "" seems to be what's throwing people off. I have been told that the answer should result in a vector even though, in the course, vector-valued integrals were never defined and so the notions of "" $\times\mathrm{d}\mathbf{S}$ "" and "" $\times\mathrm{d}\mathbf{r}$ "" were also not defined, so while I personally believe this question was unfair, I'm asking more about whether or not it's possible. If this is a mistake, could you explain why? And if it isn't, can you prove it?","This question was from an exam taken in January 2022 on a course on introductory multivariable calculus and was worded exactly as follows: ""For a general surface bounded by a closed curve show using Stokes theorem that for a vector field, and identify the constant ."" The "" "" is used to denote a surface integral and the "" "" to denote a line integral. I have asked a couple of mathematicians who work in applied mathematics and they have not been able to prove this either. The LHS is apparently the area of concern - having "" "" seems to be what's throwing people off. I have been told that the answer should result in a vector even though, in the course, vector-valued integrals were never defined and so the notions of "" "" and "" "" were also not defined, so while I personally believe this question was unfair, I'm asking more about whether or not it's possible. If this is a mistake, could you explain why? And if it isn't, can you prove it?",S C \mathbf{F}(\mathbf{r}) \int_S\nabla\times(\mathbf{F}\times\mathrm{d}\mathbf{S})=\alpha\int_C\mathbf{F}\times\mathrm{d}\mathbf{r} \alpha \mathrm{d}\mathbf{S} \mathrm{d}\mathbf{r} \nabla\times(\mathbf{F}\times\mathrm{d}\mathbf{S}) \times\mathrm{d}\mathbf{S} \times\mathrm{d}\mathbf{r},"['calculus', 'integration', 'multivariable-calculus', 'vector-fields', 'stokes-theorem']"
39,Ramanujan's q function,Ramanujan's q function,,I stuck at the following problem: Let \begin{equation} Q(n) := \sum_{k \geq 0}\frac{(n-1)_k}{n^k} \end{equation} where $(n)_k = n (n-1) \ldots (n-k + 1)$ . I want to show the following equation: \begin{equation} 1 + Q(n) = \int^{\infty}_{0}{e^{-x}\left(1 + \frac{x}{n} \right)^n dx} \end{equation} By induction for $n \geq 1$ .  For $n = 1$ this is clear. For the induction step I got \begin{equation} Q(n+1) = \sum_{k \geq 0}{\frac{(n)_k}{(n+1)^k}} = \sum_{k \geq 0}{\frac{(n-1)_k}{(n+1)^k}\frac{n}{n-k}}   \end{equation} and \begin{equation} \int^{\infty}_{0}{e^{-x}\left(1 + \frac{x}{n+1} \right)^{n+1} dx} = 1 + \int^{\infty}_{0}{e^{-x} \left( 1 + \frac{x}{n+1}\right)^n dx} \end{equation} I want to connect $Q(n+1)$ and the integral expression above by the induction requirement. I used substitution for the integral but this wasn't helpful. I would be thankful if anyone can give me a hint.,I stuck at the following problem: Let where . I want to show the following equation: By induction for .  For this is clear. For the induction step I got and I want to connect and the integral expression above by the induction requirement. I used substitution for the integral but this wasn't helpful. I would be thankful if anyone can give me a hint.,"\begin{equation}
Q(n) := \sum_{k \geq 0}\frac{(n-1)_k}{n^k}
\end{equation} (n)_k = n (n-1) \ldots (n-k + 1) \begin{equation}
1 + Q(n) = \int^{\infty}_{0}{e^{-x}\left(1 + \frac{x}{n} \right)^n dx}
\end{equation} n \geq 1 n = 1 \begin{equation}
Q(n+1) = \sum_{k \geq 0}{\frac{(n)_k}{(n+1)^k}} = \sum_{k \geq 0}{\frac{(n-1)_k}{(n+1)^k}\frac{n}{n-k}}  
\end{equation} \begin{equation}
\int^{\infty}_{0}{e^{-x}\left(1 + \frac{x}{n+1} \right)^{n+1} dx} = 1 + \int^{\infty}_{0}{e^{-x} \left( 1 + \frac{x}{n+1}\right)^n dx}
\end{equation} Q(n+1)","['calculus', 'integration', 'summation', 'gamma-function', 'binomial-theorem']"
40,For what interval of $a$ is $\int_0^{\frac\pi2}\frac{\cos a\sin x}{1+\sin a\sin x}dx =a\csc a-\frac\pi2\tan \frac a2$ valid?,For what interval of  is  valid?,a \int_0^{\frac\pi2}\frac{\cos a\sin x}{1+\sin a\sin x}dx =a\csc a-\frac\pi2\tan \frac a2,"In my previous Question [ 1 ], @Quanto has defined $$J(a) = \int_0^{\frac\pi2}\ln(1+\sin a\sin x)\,dx$$ and stated $$J'(a) =\int_0^{\frac\pi2}\frac{\cos a\sin x}{1+\sin a\sin x}\,dx =a\csc a-\frac\pi2\tan \frac a2 $$ But, I was wondering if this is valid $\forall a\in \Bbb R-{{n\pi}}$ . Using Desmos, it seems that this is valid for $a\in \Big(-\frac{3\pi}{2},\frac{\pi}{2}\Big)$ . Is there any explanation can we offer? On desmos, I run $a$ from $-1000$ to $1000$ and I see that the the Integral $\Big(J'(a)\Big)$ achieves value only from $-\pi$ to $\pi$ .","In my previous Question [ 1 ], @Quanto has defined and stated But, I was wondering if this is valid . Using Desmos, it seems that this is valid for . Is there any explanation can we offer? On desmos, I run from to and I see that the the Integral achieves value only from to .","J(a) = \int_0^{\frac\pi2}\ln(1+\sin a\sin x)\,dx J'(a)
=\int_0^{\frac\pi2}\frac{\cos a\sin x}{1+\sin a\sin x}\,dx
=a\csc a-\frac\pi2\tan \frac a2
 \forall a\in \Bbb R-{{n\pi}} a\in \Big(-\frac{3\pi}{2},\frac{\pi}{2}\Big) a -1000 1000 \Big(J'(a)\Big) -\pi \pi","['calculus', 'integration', 'convergence-divergence', 'definite-integrals']"
41,"Evaluating $\int_{0}^{\infty} \left( \text{coth} (x) - x \text{csch}^2 (x) \right) \left( \ln \left( \frac{4 \pi^2}{x^2} + 1 \right) \right) \, dx$",Evaluating,"\int_{0}^{\infty} \left( \text{coth} (x) - x \text{csch}^2 (x) \right) \left( \ln \left( \frac{4 \pi^2}{x^2} + 1 \right) \right) \, dx","How can the following improper integral be evaluated? $$\int_{0}^{\infty} \left( \text{coth} (x) - x \text{csch}^2 (x) \right) \left( \ln \left( \frac{4 \pi^2}{x^2} + 1 \right) \right) \, dx$$ or alternatively: $$\int_{0}^{\infty} \frac{x \text{coth} (x) - 1}{x^2 (2 \pi + i x)} \, dx$$ Note: I am only really interested in the imaginary component of the second integral. I've attempted multiple methods, all of which seeming unsuccessful, however, I believe contour integration may be the solution to the second integral above, which would also easily allow me to get the integral I'm interested in.","How can the following improper integral be evaluated? or alternatively: Note: I am only really interested in the imaginary component of the second integral. I've attempted multiple methods, all of which seeming unsuccessful, however, I believe contour integration may be the solution to the second integral above, which would also easily allow me to get the integral I'm interested in.","\int_{0}^{\infty} \left( \text{coth} (x) - x \text{csch}^2 (x) \right) \left( \ln \left( \frac{4 \pi^2}{x^2} + 1 \right) \right) \, dx \int_{0}^{\infty} \frac{x \text{coth} (x) - 1}{x^2 (2 \pi + i x)} \, dx","['calculus', 'integration', 'improper-integrals', 'contour-integration', 'closed-form']"
42,Two form surface integral over sphere,Two form surface integral over sphere,,"I'm trying to compute $\int_M \omega$ with \begin{align*} \omega &= x^4 dy \wedge dz + y^4 dz \wedge dx + z^4 dx \wedge dy, \\ M &: x^2 + y^2 + z^2 = R^2.  \end{align*} I have done this in two ways: Stokes' theorem and direct computation of the wedge product via a spherical coordinate transformation. Using $\phi$ as azimuthal and $\theta$ as polar angle and applying Stokes I obtain \begin{align*} 4 \int_0^{2 \pi}\int_0^{\pi} (x^3 + y^3 + z^3) R^2 \sin \theta d\theta d\phi = 0, \end{align*} where I used the standard \begin{align*} x &= R \sin \theta \cos \phi \\ y &= R \sin \theta \sin \phi \\ z &= R \cos \theta. \end{align*} By directly computing the wedge products and plugging them in I obtain \begin{align*} dy \wedge dz &= R^2 \sin^2 \theta \cos \phi d\theta \wedge d\phi \\ dz \wedge dx &= R^2 \sin^2 \theta \sin \phi d\theta \wedge d\phi \\ dx \wedge dy &= R^2 \cos \theta \sin \theta d\theta \wedge d\phi \\ \int_M x^4 dy &\wedge dz + y^4 dz \wedge dx + z^4 dx \wedge dy = 0. \end{align*} The solution given by my professor was $\frac{12}{5} \pi^2 R^5$ , which neither method agrees with. Then I repeat this procedure but instead of taking a sphere, I take a hemisphere. Changing the limits in the $\theta$ integrals to $[0, \frac{\pi}{2}]$ , the Stokes' method yields $2 \pi R^5$ and the direct wedge computation yields $\frac{\pi R^6}{3}$ . The solution given then is just $\frac{6}{5} \pi^2 R^5$ , which also does not agree with my solutions. Any help with my mistakes would be greatly appreciated.","I'm trying to compute with I have done this in two ways: Stokes' theorem and direct computation of the wedge product via a spherical coordinate transformation. Using as azimuthal and as polar angle and applying Stokes I obtain where I used the standard By directly computing the wedge products and plugging them in I obtain The solution given by my professor was , which neither method agrees with. Then I repeat this procedure but instead of taking a sphere, I take a hemisphere. Changing the limits in the integrals to , the Stokes' method yields and the direct wedge computation yields . The solution given then is just , which also does not agree with my solutions. Any help with my mistakes would be greatly appreciated.","\int_M \omega \begin{align*}
\omega &= x^4 dy \wedge dz + y^4 dz \wedge dx + z^4 dx \wedge dy, \\
M &: x^2 + y^2 + z^2 = R^2. 
\end{align*} \phi \theta \begin{align*}
4 \int_0^{2 \pi}\int_0^{\pi} (x^3 + y^3 + z^3) R^2 \sin \theta d\theta d\phi = 0,
\end{align*} \begin{align*}
x &= R \sin \theta \cos \phi \\
y &= R \sin \theta \sin \phi \\
z &= R \cos \theta.
\end{align*} \begin{align*}
dy \wedge dz &= R^2 \sin^2 \theta \cos \phi d\theta \wedge d\phi \\
dz \wedge dx &= R^2 \sin^2 \theta \sin \phi d\theta \wedge d\phi \\
dx \wedge dy &= R^2 \cos \theta \sin \theta d\theta \wedge d\phi \\
\int_M x^4 dy &\wedge dz + y^4 dz \wedge dx + z^4 dx \wedge dy = 0.
\end{align*} \frac{12}{5} \pi^2 R^5 \theta [0, \frac{\pi}{2}] 2 \pi R^5 \frac{\pi R^6}{3} \frac{6}{5} \pi^2 R^5","['calculus', 'integration', 'differential-forms', 'spherical-coordinates']"
43,How to prove this problem about the power series expanded of $C^\infty$ function?,How to prove this problem about the power series expanded of  function?,C^\infty,"Problem: let $f∈C^\infty [-1,1]$ , with $f^{(n)}(x)\ge0$ for all $n\in\Bbb N$ and all $x∈[-1,1]$ . Show that $f$ can be expanded as a power series on $[-1,1]$ My attempt . I use the Taylor formula: $$ f(x)=\sum_{k=0}^n \frac{f^{(k)}(0)}{k!} x^k +\frac{1}{n!} \int _{0}^{x} (x-t)^n f^{(n+1)}(t) dt.  $$ Let $$ R_n(x)=\frac{1}{n!} \int _{0}^{x} (x-t)^n f^{(n+1)}(t) dt. $$ We must  prove that $R_n(x)$ tends to $0$ as $n$ tends to infinity for all $x \in[-1,1]$ . By a change of variables $x-t=u$ and $u=vx$ we get $$ \begin{split} |R_n(x)| & =\frac{|x|^n}{n!} \int _{0}^{1}(1-v)^n f^{(n+1)}(xv) dv \\ &\le \frac{|x|^n}{n!} \int _{0}^{1}(1-v)^n f^{(n+1)}(v) dv \\ &=|x|^{n+1}\left[f(1) - \sum_{k=0}^n \frac{f^{(k)}(0)}{k!} \right] \\ &\le |x|^{n+1} f(1). \end{split} $$ So we have $R_n(x)$ tend to $0$ as $n$ tend to intfy for all $x ∈(-1,1)$ . So $f$ can be expanded as a power series on $(-1,1)$ . But how to deal with the remaining points? (Indeed, I think these conditions are not sufficient to derive the remain results.) All helps: I will thank you.","Problem: let , with for all and all . Show that can be expanded as a power series on My attempt . I use the Taylor formula: Let We must  prove that tends to as tends to infinity for all . By a change of variables and we get So we have tend to as tend to intfy for all . So can be expanded as a power series on . But how to deal with the remaining points? (Indeed, I think these conditions are not sufficient to derive the remain results.) All helps: I will thank you.","f∈C^\infty [-1,1] f^{(n)}(x)\ge0 n\in\Bbb N x∈[-1,1] f [-1,1] 
f(x)=\sum_{k=0}^n \frac{f^{(k)}(0)}{k!} x^k +\frac{1}{n!} \int _{0}^{x} (x-t)^n f^{(n+1)}(t) dt. 
 
R_n(x)=\frac{1}{n!} \int _{0}^{x} (x-t)^n f^{(n+1)}(t) dt.
 R_n(x) 0 n x \in[-1,1] x-t=u u=vx 
\begin{split}
|R_n(x)| & =\frac{|x|^n}{n!} \int _{0}^{1}(1-v)^n f^{(n+1)}(xv) dv \\
&\le \frac{|x|^n}{n!} \int _{0}^{1}(1-v)^n f^{(n+1)}(v) dv \\
&=|x|^{n+1}\left[f(1) - \sum_{k=0}^n \frac{f^{(k)}(0)}{k!} \right] \\
&\le |x|^{n+1} f(1).
\end{split}
 R_n(x) 0 n x ∈(-1,1) f (-1,1)","['calculus', 'power-series', 'taylor-expansion']"
44,Solving a PDE by method of characteristics,Solving a PDE by method of characteristics,,I solved the following: \begin{align} \frac{dy}{y}&=\frac{dy'}{2y'+wy}\\ \ln y + \ln C &= \frac{1}{2} \ln \left(2y'+wy\right)\\ 2\ln y + \ln C &= \ln \left(2y'+wy\right)\\ C &= \frac{2y'}{y^2} + \frac{w}{y}\\ \end{align} I isolated the constant since the purpose of solving is to find the differential invariant. However the answer given is: $$ \frac{y'}{y^2}+\frac{w}{y} $$ where am I going awry?,I solved the following: I isolated the constant since the purpose of solving is to find the differential invariant. However the answer given is: where am I going awry?,"\begin{align}
\frac{dy}{y}&=\frac{dy'}{2y'+wy}\\
\ln y + \ln C &= \frac{1}{2} \ln \left(2y'+wy\right)\\
2\ln y + \ln C &= \ln \left(2y'+wy\right)\\
C &= \frac{2y'}{y^2} + \frac{w}{y}\\
\end{align} 
\frac{y'}{y^2}+\frac{w}{y}
","['calculus', 'ordinary-differential-equations']"
45,Evaluating $\int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x$,Evaluating,\int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x,"I was generalizing the following integral: $$\int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x \hspace{40pt} n\geq 0$$ We can start by noting that $\sin x =\dfrac{e^{ix}-e^{-ix}}{2i}$ and thus $\displaystyle \sin^{2n+1}x=\frac{(-1)^n}{2^{2n}}\sum_{r=0}^n (-1)^r \binom{2n+1}{r}\sin(2r+1)x$ . This implies $$\begin{aligned}\displaystyle \int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x &=\frac{(-1)^n}{2^{2n}}\sum_{r=0}^n (-1)^r \binom{2n+1}{r}\int_{0}^{\infty}\frac{\sin(2r+1)x}{x}\mathrm{d}x \\ &=\frac{(-1)^n\pi}{2^{2n+1}}\sum_{r=0}^n(-1)^r\binom{2n+1}{r} \\ &=\frac{(-1)^n\pi}{2^{2n+1}}\sum_{r=0}^n\left((-1)^r\binom{2n}{r}-(-1)^{r-1}\binom{2n}{r-1}\right)\end{aligned}$$ Where I have used the well known result $\displaystyle \int_{0}^{\infty} \frac{\sin(2r+1)x}{x}\mathrm{d}x=\int_{0}^{\infty}\frac{\sin x}{x}\mathrm{d}x=\frac{\pi}{2}$ and the property of binomial coefficients that $\displaystyle \binom{n}{r}+\binom{n}{r-1}=\binom{n+1}{r}$ Since the above sum telescopes, we have $$\displaystyle \int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x=\frac{(-1)^n\pi}{2^{2n+1}}(-1)^n\binom{2n}{n}=\frac{\pi}{2^{2n+1}}\binom{2n}{n} ~\forall ~ n\in \mathbb{Z^{+}}$$ I would like to know other methods for evaluating this integral.","I was generalizing the following integral: We can start by noting that and thus . This implies Where I have used the well known result and the property of binomial coefficients that Since the above sum telescopes, we have I would like to know other methods for evaluating this integral.",\int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x \hspace{40pt} n\geq 0 \sin x =\dfrac{e^{ix}-e^{-ix}}{2i} \displaystyle \sin^{2n+1}x=\frac{(-1)^n}{2^{2n}}\sum_{r=0}^n (-1)^r \binom{2n+1}{r}\sin(2r+1)x \begin{aligned}\displaystyle \int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x &=\frac{(-1)^n}{2^{2n}}\sum_{r=0}^n (-1)^r \binom{2n+1}{r}\int_{0}^{\infty}\frac{\sin(2r+1)x}{x}\mathrm{d}x \\ &=\frac{(-1)^n\pi}{2^{2n+1}}\sum_{r=0}^n(-1)^r\binom{2n+1}{r} \\ &=\frac{(-1)^n\pi}{2^{2n+1}}\sum_{r=0}^n\left((-1)^r\binom{2n}{r}-(-1)^{r-1}\binom{2n}{r-1}\right)\end{aligned} \displaystyle \int_{0}^{\infty} \frac{\sin(2r+1)x}{x}\mathrm{d}x=\int_{0}^{\infty}\frac{\sin x}{x}\mathrm{d}x=\frac{\pi}{2} \displaystyle \binom{n}{r}+\binom{n}{r-1}=\binom{n+1}{r} \displaystyle \int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x=\frac{(-1)^n\pi}{2^{2n+1}}(-1)^n\binom{2n}{n}=\frac{\pi}{2^{2n+1}}\binom{2n}{n} ~\forall ~ n\in \mathbb{Z^{+}},"['calculus', 'integration', 'definite-integrals']"
46,Solve second order DE: $\sin^2 x\frac{d^2y}{dx^2} = 2y$,Solve second order DE:,\sin^2 x\frac{d^2y}{dx^2} = 2y,"Solve: $$\sin^2 x \frac{d^2y}{dx^2} = 2y $$ So what I did was separation of variables, it got me $$\frac{y''} {2y}= \csc^2 x$$ and integrating both sides will give $$\frac{y'}{2y}= -\cot x+ C $$ On another integration we will get $$\frac{\ln y}{2} = -\ln(\sin x) + Cx + K$$ then we get $$\ln y = -2\ln(\sin x) + 2Cx + 2K$$ and so my answer is $$y = e^{-2\ln(\sin x) + 2Cx}$$ but I believe it is wrong.","Solve: So what I did was separation of variables, it got me and integrating both sides will give On another integration we will get then we get and so my answer is but I believe it is wrong.",\sin^2 x \frac{d^2y}{dx^2} = 2y  \frac{y''} {2y}= \csc^2 x \frac{y'}{2y}= -\cot x+ C  \frac{\ln y}{2} = -\ln(\sin x) + Cx + K \ln y = -2\ln(\sin x) + 2Cx + 2K y = e^{-2\ln(\sin x) + 2Cx},"['calculus', 'ordinary-differential-equations']"
47,Integrate a weighted Bessel function over the unit disk,Integrate a weighted Bessel function over the unit disk,,"I would like to evaluate a complex-valued integral of the form $$ I_e = \int_0^1 x e^{iax} J_0(b \sqrt{1-x^2}) dx $$ where $a$ and $b$ are real numbers (not necessarily positive) and $J_0(z)$ is the Bessel function of the first kind. An alternative statement of the problem can be considered by making a change of variables $z=b\sqrt{1-x^2}, c = a/b$ , so that $$ I_e = \frac{1}{b^2} \int_0^b z e^{ic\sqrt{b^2-z^2}} J_0(z) dz. $$ I am particularly interested in the special case of $0\leq b \leq 100$ with $c = 1$ or $-4 \leq c \leq -1$ . The task boils down to evaluating two real-valued integrals $$ I_s = \frac{1}{b^2} \int_0^b z \sin(c\sqrt{b^2-z^2}) J_0(z) dz $$ $$ I_c = \frac{1}{b^2} \int_0^b z \cos(c\sqrt{b^2-z^2}) J_0(z) dz $$ The integral with the sine has a simple form given by Gradshteyn and Ryzhik (6.738.1) which, after simplification, becomes $$ I_s = c \frac{j_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}} = a \frac{j_1(\sqrt{a^2 + b^2})}{\sqrt{a^2 + b^2}} $$ where $j_1(z)$ is the spherical Bessel function of the first kind. I am not exactly sure how this expression was derived. Perhaps it holds a clue. I tried substituting the integral form of the Bessel function and integrating analytically but did not get very far. By symmetry, I naively expected the integral involving the cosine to be proportional to the spherical Bessel function of the second kind $y_1(z)$ (and thus, the complex-valued integral to be proportional to the spherical Hankel function of the second kind), but that does not appear to be the case. $$ I_c \neq -c \frac{y_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}} $$ Particularly, because of the cosine term inside the Bessel function, $$ \lim_{b\to0} I_c \neq \lim_{b\to0} -c \frac{y_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}}$$ A better approximation can be achieved by dropping the cosine part of the spherical Bessel $$ I_c \approx c \frac{\sin(b\sqrt{c^2 + 1})}{b(c^2 + 1)} $$ Indeed, if we plot $I_c \times b$ , we can see that it is a regular sine wave for $b \geq 4$ . My current goal is to find a correction term (via a series expansion, perhaps) that would improve the approximation for $b < 4$ . I've only found a single identity connected to the approximation given above. Tables of Integral Transforms , Vol. 2, p. 337, eq. 29 gives $$ \int_0^b \frac{z}{\sqrt{b^2-z^2}} \cos(c\sqrt{b^2-z^2}) J_0(z) dz =  \frac{\sin(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}} $$ I am not sure what's the best way to connect this identity to $ I_c$ . The two integrals only differ by the first term (and the constant $ 1/b^2$ ). One way is to perform a Taylor expansion around the origin: $$\frac{z}{\sqrt{b^2-z^2}} = \frac{z}{b} + \frac{1}{2} \frac{z^3}{b^3} + O(z^5) $$ The left part is from the identity, and the first term on the right is from $I_c$ . The cubic term does not appear to help, so perhaps this is not the right expansion to use in this case (and perhaps I should expand at infinity rather than at the origin). I would appreciate any tips or guidance. Thank you!","I would like to evaluate a complex-valued integral of the form where and are real numbers (not necessarily positive) and is the Bessel function of the first kind. An alternative statement of the problem can be considered by making a change of variables , so that I am particularly interested in the special case of with or . The task boils down to evaluating two real-valued integrals The integral with the sine has a simple form given by Gradshteyn and Ryzhik (6.738.1) which, after simplification, becomes where is the spherical Bessel function of the first kind. I am not exactly sure how this expression was derived. Perhaps it holds a clue. I tried substituting the integral form of the Bessel function and integrating analytically but did not get very far. By symmetry, I naively expected the integral involving the cosine to be proportional to the spherical Bessel function of the second kind (and thus, the complex-valued integral to be proportional to the spherical Hankel function of the second kind), but that does not appear to be the case. Particularly, because of the cosine term inside the Bessel function, A better approximation can be achieved by dropping the cosine part of the spherical Bessel Indeed, if we plot , we can see that it is a regular sine wave for . My current goal is to find a correction term (via a series expansion, perhaps) that would improve the approximation for . I've only found a single identity connected to the approximation given above. Tables of Integral Transforms , Vol. 2, p. 337, eq. 29 gives I am not sure what's the best way to connect this identity to . The two integrals only differ by the first term (and the constant ). One way is to perform a Taylor expansion around the origin: The left part is from the identity, and the first term on the right is from . The cubic term does not appear to help, so perhaps this is not the right expansion to use in this case (and perhaps I should expand at infinity rather than at the origin). I would appreciate any tips or guidance. Thank you!","
I_e = \int_0^1 x e^{iax} J_0(b \sqrt{1-x^2}) dx
 a b J_0(z) z=b\sqrt{1-x^2}, c = a/b 
I_e = \frac{1}{b^2} \int_0^b z e^{ic\sqrt{b^2-z^2}} J_0(z) dz.
 0\leq b \leq 100 c = 1 -4 \leq c \leq -1 
I_s = \frac{1}{b^2} \int_0^b z \sin(c\sqrt{b^2-z^2}) J_0(z) dz
 
I_c = \frac{1}{b^2} \int_0^b z \cos(c\sqrt{b^2-z^2}) J_0(z) dz
 
I_s = c \frac{j_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}} = a \frac{j_1(\sqrt{a^2 + b^2})}{\sqrt{a^2 + b^2}}
 j_1(z) y_1(z) 
I_c \neq -c \frac{y_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}}
  \lim_{b\to0} I_c \neq \lim_{b\to0} -c \frac{y_1(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}} 
I_c \approx c \frac{\sin(b\sqrt{c^2 + 1})}{b(c^2 + 1)}
 I_c \times b b \geq 4 b < 4 
\int_0^b \frac{z}{\sqrt{b^2-z^2}} \cos(c\sqrt{b^2-z^2}) J_0(z) dz =  \frac{\sin(b\sqrt{c^2 + 1})}{\sqrt{c^2 + 1}}
 
I_c 
1/b^2 \frac{z}{\sqrt{b^2-z^2}} = \frac{z}{b} + \frac{1}{2} \frac{z^3}{b^3} + O(z^5)  I_c","['calculus', 'integration']"
48,"I wrote a proof of L'Hospital's rule, am I right?","I wrote a proof of L'Hospital's rule, am I right?",,"Suppose $f$ and $g$ are differentiable and $g'(x)\neq 0$ on an open interval $I$ that contains $a$ . Suppose that $$\lim_{x\to a}f(x)=L=\lim_{x\to a}g(x)\quad L=0\,or\,\infty\,or-\infty$$ , then $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}$$ I proved the case when $L=0$ ,and I wrote a proof about the case when $L=\infty$ ,I found it is different from the proof on the internet, so I don't know whether it is right. My proof: $$\lim_{x\to a}f(x)=\infty=\lim_{x\to a}g(x)$$ $$\lim_{x\to a}\frac{1}{f(x)}=0=\lim_{x\to a}\frac{1}{g(x)}$$ $$\lim_{x\to a}\frac{\frac{1}{g(x)}}{\frac{1}{f(x)}}=\lim_{x\to a}\frac{[\frac{1}{g(x)}]'}{[\frac{1}{f(x)}]'}$$ . $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{g'(x)[f(x)]^2}{f'(x)[(g(x)]^2}$$ $$\lim_{x\to a}\frac{f(x)}{g(x)}[1-\frac{f(x)g'(x)}{g(x)f'(x)}]=0$$ . I think L'Hospital assumes that $\lim_{x\to a}\frac{f(x)}{g(x)}$ exist, or is $\infty$ , or $-\infty$ , and so $$\lim_{x\to a}\frac{f(x)g'(x)}{g(x)f'(x)}=1$$ $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}$$ . If it is wrong, please tell me why. Thank you very much!","Suppose and are differentiable and on an open interval that contains . Suppose that , then I proved the case when ,and I wrote a proof about the case when ,I found it is different from the proof on the internet, so I don't know whether it is right. My proof: . . I think L'Hospital assumes that exist, or is , or , and so . If it is wrong, please tell me why. Thank you very much!","f g g'(x)\neq 0 I a \lim_{x\to a}f(x)=L=\lim_{x\to a}g(x)\quad L=0\,or\,\infty\,or-\infty \lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)} L=0 L=\infty \lim_{x\to a}f(x)=\infty=\lim_{x\to a}g(x) \lim_{x\to a}\frac{1}{f(x)}=0=\lim_{x\to a}\frac{1}{g(x)} \lim_{x\to a}\frac{\frac{1}{g(x)}}{\frac{1}{f(x)}}=\lim_{x\to a}\frac{[\frac{1}{g(x)}]'}{[\frac{1}{f(x)}]'} \lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{g'(x)[f(x)]^2}{f'(x)[(g(x)]^2} \lim_{x\to a}\frac{f(x)}{g(x)}[1-\frac{f(x)g'(x)}{g(x)f'(x)}]=0 \lim_{x\to a}\frac{f(x)}{g(x)} \infty -\infty \lim_{x\to a}\frac{f(x)g'(x)}{g(x)f'(x)}=1 \lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}",['calculus']
49,Evaluate $\lim_{n \to \infty}\left(\frac{1^{1/x}+2^{1/x}+\ldots+n^{1/x}}{n}\right)^{nx}$,Evaluate,\lim_{n \to \infty}\left(\frac{1^{1/x}+2^{1/x}+\ldots+n^{1/x}}{n}\right)^{nx},"$$\lim_{n \to \infty}\left(\frac{1^{1/x}+2^{1/x}+\ldots+n^{1/x}}{n}\right)^{nx}$$ I don't know any format or can't think of anything to solve this limit. It looks like it is Riemann's Sum Form but there is an x, so I am confused. Please help out. Thank You!","I don't know any format or can't think of anything to solve this limit. It looks like it is Riemann's Sum Form but there is an x, so I am confused. Please help out. Thank You!",\lim_{n \to \infty}\left(\frac{1^{1/x}+2^{1/x}+\ldots+n^{1/x}}{n}\right)^{nx},"['calculus', 'integration', 'limits']"
50,Conditions for a vector field to be a gradient,Conditions for a vector field to be a gradient,,"Consider a vector field $F : \mathbf{R}^d \to \mathbf{R}^d$ . I want to know standard conditions on $F$ which guarantee that $F$ is a gradient, i.e. that $F = \nabla V$ for some function $V$ . I know that gradient fields (sufficiently smooth, etc. etc.) all satisfy $$\frac{ \partial F_i }{ \partial x_j} = \frac{ \partial F_j }{ \partial x_i} \quad \text{for all } i, j$$ and I have it in my mind that this is also a sufficient condition (or pretty close to it), but I don't really trust myself on this, and have no reference to this effect. Is this the case? Is there another sufficient condition? What is a good reference for this sort of thing?","Consider a vector field . I want to know standard conditions on which guarantee that is a gradient, i.e. that for some function . I know that gradient fields (sufficiently smooth, etc. etc.) all satisfy and I have it in my mind that this is also a sufficient condition (or pretty close to it), but I don't really trust myself on this, and have no reference to this effect. Is this the case? Is there another sufficient condition? What is a good reference for this sort of thing?","F : \mathbf{R}^d \to \mathbf{R}^d F F F = \nabla V V \frac{ \partial F_i }{ \partial x_j} = \frac{ \partial F_j }{ \partial x_i} \quad \text{for all } i, j","['calculus', 'differential-geometry', 'reference-request', 'vector-fields']"
51,Show $\int_{0}^{1}\frac {x^2\ln x }{{(1-x^2)}{(1+x^4)}}dx=\frac{-π^2}{16(2+\sqrt{2})}$,Show,\int_{0}^{1}\frac {x^2\ln x }{{(1-x^2)}{(1+x^4)}}dx=\frac{-π^2}{16(2+\sqrt{2})},"Question: Prove that $$\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-π^2}{16(2+\sqrt{2})}$$ Using partial fraction,we get \begin{align} &\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx\\ = &\frac{1}{4}\int_{0}^{1}\frac{\ln x}{1-x}dx+\frac{1}{4}\int_{0}^{1}\frac{\ln x}{1+x}dx+\frac{1}{2}\int_{0}^{1}\frac{(x^2-1)\ln x}{1+x^4}dx \end{align} I got first integral as $\frac{-\pi^2}{6}$ and second integral as $\frac{-\pi^2}{12}$ $$\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-\pi^2}{16}+\frac{1}{2}\int_{0}^{1}\frac{(x^2-1)\ln x}{1+x^4}dx$$ I got stuck with third integral, which seems difficult to evaluate. ${}{}{}$ A note is also written saying that:- The reader should evaluate the family of integrals ${I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N}$ The computation of the first few special values indicates an interesting arithmetic structure of the answer. How to evaluate integral for $n$ ?","Question: Prove that Using partial fraction,we get I got first integral as and second integral as I got stuck with third integral, which seems difficult to evaluate. A note is also written saying that:- The reader should evaluate the family of integrals The computation of the first few special values indicates an interesting arithmetic structure of the answer. How to evaluate integral for ?","\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-π^2}{16(2+\sqrt{2})} \begin{align}
&\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx\\
= &\frac{1}{4}\int_{0}^{1}\frac{\ln x}{1-x}dx+\frac{1}{4}\int_{0}^{1}\frac{\ln x}{1+x}dx+\frac{1}{2}\int_{0}^{1}\frac{(x^2-1)\ln x}{1+x^4}dx
\end{align} \frac{-\pi^2}{6} \frac{-\pi^2}{12} \int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-\pi^2}{16}+\frac{1}{2}\int_{0}^{1}\frac{(x^2-1)\ln x}{1+x^4}dx {}{}{} {I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N} n","['calculus', 'integration', 'definite-integrals']"
52,"The outside of a $180$-sheet roll of toilet paper is covered by two sheets; the inner cylinder, by one. What's wrong with how I counted the layers?","The outside of a -sheet roll of toilet paper is covered by two sheets; the inner cylinder, by one. What's wrong with how I counted the layers?",180,"Puzzle: A roll of toilet paper has 180 sheets on it. The outside is covered with exactly two sheets. The inside around the cardboard cylinder is covered by exactly one. Question of the puzzle: how many layers of toilet paper are on the roll of toilet paper? The given solution: One way to solve this is by saying that the average round is covered by 1.5 sheets, so therefore the answer is $180\times\frac{2}{3}=120$ I tried a similar (but wrong) reasoning: ""the average sheet makes an average of $\frac{3}{4}$ rounds (first sheet makes one round and the last sheet makes $\frac{1}{2}$ rounds), so the answer is $180\times\frac{3}{4}=$ 135"" QUESTION: Apparently my answer is wrong. But since it seems analogical to the given solution I don't understand what error I made. Possibly the growth of sheets per round is constant? While the (negative) growth of rounds per sheet is not constant? What are the related functions? Put in another way: if $\frac{dSheets}{dRounds}=Constant$ isn't also $\frac{dRounds}{dSheets}=Constant$ ? This question is linked to this question: Using differential equations to determine the number of rolls on a roll of toilet paper","Puzzle: A roll of toilet paper has 180 sheets on it. The outside is covered with exactly two sheets. The inside around the cardboard cylinder is covered by exactly one. Question of the puzzle: how many layers of toilet paper are on the roll of toilet paper? The given solution: One way to solve this is by saying that the average round is covered by 1.5 sheets, so therefore the answer is I tried a similar (but wrong) reasoning: ""the average sheet makes an average of rounds (first sheet makes one round and the last sheet makes rounds), so the answer is 135"" QUESTION: Apparently my answer is wrong. But since it seems analogical to the given solution I don't understand what error I made. Possibly the growth of sheets per round is constant? While the (negative) growth of rounds per sheet is not constant? What are the related functions? Put in another way: if isn't also ? This question is linked to this question: Using differential equations to determine the number of rolls on a roll of toilet paper",180\times\frac{2}{3}=120 \frac{3}{4} \frac{1}{2} 180\times\frac{3}{4}= \frac{dSheets}{dRounds}=Constant \frac{dRounds}{dSheets}=Constant,"['calculus', 'puzzle']"
53,Lebesgue Integral - Self Learning,Lebesgue Integral - Self Learning,,"I finished some courses on Calculus at college sometime ago, then it goes without saying I learned Riemann Integrals. However, I realized that there is another type of integrating functions: Lebesgue Integrals. Because of this, I really would like to understand them by my own. Which books would you recommend for me, who wants to learn by my on during this quarantine. It would be nice if the book contained exercises or even were like Stewart's Calculus. Thank you","I finished some courses on Calculus at college sometime ago, then it goes without saying I learned Riemann Integrals. However, I realized that there is another type of integrating functions: Lebesgue Integrals. Because of this, I really would like to understand them by my own. Which books would you recommend for me, who wants to learn by my on during this quarantine. It would be nice if the book contained exercises or even were like Stewart's Calculus. Thank you",,"['calculus', 'integration', 'reference-request', 'lebesgue-integral', 'book-recommendation']"
54,Umbral calculus - eigenfunctions of operator,Umbral calculus - eigenfunctions of operator,,"I'm very new to umbral caluclus and I have come across a paper that makes use of some results in this area, which I do not quite understand. The problem I have is the following. Consider the following operator \begin{equation} \mathcal{S} = a^{-1}(bI\Delta_{-1} + c\Delta_{1}), \quad I \in \mathbb{Z^+} \end{equation} where $\Delta_h[f(I)] =f(I+h)-f(I), h \in \mathbb{Z}$ It is claimed by the paper that given that $\Delta_{1}(I)_m=m(I)_{m-1}$ and $I\Delta_{-1}(I)_m = -m(I)_m$ [where with $(I)_m$ we denote the falling factorial] the eigenfunctions $\psi(I)$ of the operator are computable as: \begin{equation} \psi_n(I) = \sum_{m=0}^{n} {n \choose m} \left(-\frac{c}{b}\right)^m(I)_{n-m} \end{equation} and the eigenvalues \begin{equation} \lambda_n=-n\frac{b}{a} \end{equation} Note that $\mathcal{S}(I)_m = -m\frac{b}{a}[(I)_m - \frac{c}{b}(I)_{m-1}]$ How is this caluculation performed? I have looked at Rota's book but I see no reference to the computation of the eigenfunctions. Can anybody point me out in the right direction? Thank you all in advance","I'm very new to umbral caluclus and I have come across a paper that makes use of some results in this area, which I do not quite understand. The problem I have is the following. Consider the following operator where It is claimed by the paper that given that and [where with we denote the falling factorial] the eigenfunctions of the operator are computable as: and the eigenvalues Note that How is this caluculation performed? I have looked at Rota's book but I see no reference to the computation of the eigenfunctions. Can anybody point me out in the right direction? Thank you all in advance","\begin{equation}
\mathcal{S} = a^{-1}(bI\Delta_{-1} + c\Delta_{1}), \quad I \in \mathbb{Z^+}
\end{equation} \Delta_h[f(I)] =f(I+h)-f(I), h \in \mathbb{Z} \Delta_{1}(I)_m=m(I)_{m-1} I\Delta_{-1}(I)_m = -m(I)_m (I)_m \psi(I) \begin{equation}
\psi_n(I) = \sum_{m=0}^{n} {n \choose m} \left(-\frac{c}{b}\right)^m(I)_{n-m}
\end{equation} \begin{equation}
\lambda_n=-n\frac{b}{a}
\end{equation} \mathcal{S}(I)_m = -m\frac{b}{a}[(I)_m - \frac{c}{b}(I)_{m-1}]","['calculus', 'operator-theory', 'eigenfunctions', 'umbral-calculus']"
55,Measurability of intermediate variable in the application of the mean-value theorem,Measurability of intermediate variable in the application of the mean-value theorem,,"To prove the differentiability of a function of the form $$g(x) = \int f(t,x)dt$$ where $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is $C^1$ in the second variable I sometimes see applications of the mean-value theorem as follows: Let $h\in \mathbb{R}$ and define $\varphi_t(s) = f(t,x+sh)$ . Then by the mean-value theorem $\varphi_t(1)-\varphi_t(0) = \varphi_t'(\xi(t)) = \partial_xf(t,x+\xi(t) h)h$ where $\xi(t)$ lies between $0$ and 1. Therefore \begin{align*} \frac{g(x+h)-g(x)}{h} = \int \frac{f(t,x+h)-f(t,x)}{h}dt = \int \partial_xf(t,x+\xi(t)h)dt. \end{align*} So for instance if we assume som integrability condition on $\partial_x f(t,x)$ (for instance suppose we can find an integrable majorant) then by Lebesgue's dominated convergence theorem $g$ is differentiable with derivative $$g'(x) = \int \partial_x f(t,x)dt.$$ Now what I wonder is if the function $\xi(t)$ appearing in the application of the mean-value theorem is actually measurable? As far as I understand its existence is only a consequence of the fact that a continuous function on a compact set has a maximum and a minimum, since this is how one shows Rolle's theorem which one can then use to prove the mean-value theorem. If $\xi$ is not uniquely defined one must apply the axiom of choice when defining the function right? The measurability of $\xi(t)$ is not vital for the differentiation: Since $$\partial_xf(t,x+\xi(t)h) = \frac{f(t,x+h)-f(t,x)}{h}$$ and since the right-hand side is measurable we find that $t\mapsto \partial_xf(t,x+\xi(t)h)$ is measurable. Supposing there exists some integrable function $F(t)\geq 0$ such that $\partial_xf(t,x)\leq F(t)$ for all $x$ then we can take the limit.","To prove the differentiability of a function of the form where is in the second variable I sometimes see applications of the mean-value theorem as follows: Let and define . Then by the mean-value theorem where lies between and 1. Therefore So for instance if we assume som integrability condition on (for instance suppose we can find an integrable majorant) then by Lebesgue's dominated convergence theorem is differentiable with derivative Now what I wonder is if the function appearing in the application of the mean-value theorem is actually measurable? As far as I understand its existence is only a consequence of the fact that a continuous function on a compact set has a maximum and a minimum, since this is how one shows Rolle's theorem which one can then use to prove the mean-value theorem. If is not uniquely defined one must apply the axiom of choice when defining the function right? The measurability of is not vital for the differentiation: Since and since the right-hand side is measurable we find that is measurable. Supposing there exists some integrable function such that for all then we can take the limit.","g(x) = \int f(t,x)dt f:\mathbb{R}^2\rightarrow \mathbb{R} C^1 h\in \mathbb{R} \varphi_t(s) = f(t,x+sh) \varphi_t(1)-\varphi_t(0) = \varphi_t'(\xi(t)) = \partial_xf(t,x+\xi(t) h)h \xi(t) 0 \begin{align*}
\frac{g(x+h)-g(x)}{h} = \int \frac{f(t,x+h)-f(t,x)}{h}dt = \int \partial_xf(t,x+\xi(t)h)dt.
\end{align*} \partial_x f(t,x) g g'(x) = \int \partial_x f(t,x)dt. \xi(t) \xi \xi(t) \partial_xf(t,x+\xi(t)h) = \frac{f(t,x+h)-f(t,x)}{h} t\mapsto \partial_xf(t,x+\xi(t)h) F(t)\geq 0 \partial_xf(t,x)\leq F(t) x","['calculus', 'measure-theory', 'derivatives', 'lebesgue-integral']"
56,"Volume integral zoo: Geometric calculus, Riemannian volumes, Lagrangian densities","Volume integral zoo: Geometric calculus, Riemannian volumes, Lagrangian densities",,"What is the connection between these volume integrals? The Riemannian volume-form: $$ V=\int_M \sqrt{|g|}dx^1\wedge...\wedge dx^n \tag{1} $$ Where $|g|$ is the determinant of the matrix representation of the metric tensor of the manifold. The integral of geometric calculus: $$ V=\int_M  (e_1 dx^1 \wedge ... \wedge e_n dx^n)=\int_M  (e_1 \wedge ... \wedge e_n ) d^nx  \tag{2} $$ The integral used in Lagrangian densities found in physics, such as: $$ S=\int \sqrt{-|g|} d^n x \tag{3} $$ Comparing all three, one notices the following differences: For (1), the wedge product is linking the differential terms For (2), the wedge product is linking the basis elements of the pseudoscalar. For (3), the Lagrangian density has no wedge products, yet the same terms are used $\sqrt{-|g|}$ and the differential terms. Are they all equal in some subtle way, or if they are different, how do they connect to one another? To make sense of this zoo, it seems I am missing an equality, or at least a connection, between $dx\wedge dy$ and $dxdy$ , and between $(e_1\wedge e_2)dxdy$ and $\sqrt{|g|}dxdy$ ? Edit: From  Giuseppe's comment, it appears that $dxdy$ is simply a sloppy way to write $dx\wedge dy$ . So what is left is to explain how to connect: $$ (e_0\wedge ... e_n)dx^1 ... dx^n \to \sqrt{|g|}dx^1...dx^n $$ I am assuming that an intermediary step involves diagonalizing the basis. For instance, it is possibly the case that : $$ (e_0\wedge ... \wedge e_n) \stackrel{?}{=} \sqrt{|g|} (\gamma_0 \wedge ... \wedge \gamma_n) \tag{A.1} $$ On can them eliminate the orthogonal basis $(\gamma_0 \wedge ... \wedge \gamma_n)$ by replacing it by $I$ , the unit pseudoscalar: $$ \sqrt{|g|} (\gamma_0 \wedge ... \wedge \gamma_n)=I\sqrt{|g|} $$ Thus, the connection is $I$ , as follows: $$ \int_M (e_1 \wedge ... \wedge e_n) dx^1...dx^n = I \int_M \sqrt{|g|} dx^1...dx^n $$ or even absorbing $I$ into the square root: $$ \int_M (e_1 \wedge ... \wedge e_n) dx^1...dx^n =    \int_M \sqrt{-|g|} dx^1...dx^n $$ So, the two notations are related by the unit pseudoscalar? Can  A.1 be proven?","What is the connection between these volume integrals? The Riemannian volume-form: Where is the determinant of the matrix representation of the metric tensor of the manifold. The integral of geometric calculus: The integral used in Lagrangian densities found in physics, such as: Comparing all three, one notices the following differences: For (1), the wedge product is linking the differential terms For (2), the wedge product is linking the basis elements of the pseudoscalar. For (3), the Lagrangian density has no wedge products, yet the same terms are used and the differential terms. Are they all equal in some subtle way, or if they are different, how do they connect to one another? To make sense of this zoo, it seems I am missing an equality, or at least a connection, between and , and between and ? Edit: From  Giuseppe's comment, it appears that is simply a sloppy way to write . So what is left is to explain how to connect: I am assuming that an intermediary step involves diagonalizing the basis. For instance, it is possibly the case that : On can them eliminate the orthogonal basis by replacing it by , the unit pseudoscalar: Thus, the connection is , as follows: or even absorbing into the square root: So, the two notations are related by the unit pseudoscalar? Can  A.1 be proven?","
V=\int_M \sqrt{|g|}dx^1\wedge...\wedge dx^n \tag{1}
 |g| 
V=\int_M  (e_1 dx^1 \wedge ... \wedge e_n dx^n)=\int_M  (e_1 \wedge ... \wedge e_n ) d^nx  \tag{2}
 
S=\int \sqrt{-|g|} d^n x \tag{3}
 \sqrt{-|g|} dx\wedge dy dxdy (e_1\wedge e_2)dxdy \sqrt{|g|}dxdy dxdy dx\wedge dy 
(e_0\wedge ... e_n)dx^1 ... dx^n \to \sqrt{|g|}dx^1...dx^n
 
(e_0\wedge ... \wedge e_n) \stackrel{?}{=} \sqrt{|g|} (\gamma_0 \wedge ... \wedge \gamma_n) \tag{A.1}
 (\gamma_0 \wedge ... \wedge \gamma_n) I 
\sqrt{|g|} (\gamma_0 \wedge ... \wedge \gamma_n)=I\sqrt{|g|}
 I 
\int_M (e_1 \wedge ... \wedge e_n) dx^1...dx^n = I \int_M \sqrt{|g|} dx^1...dx^n
 I 
\int_M (e_1 \wedge ... \wedge e_n) dx^1...dx^n =    \int_M \sqrt{-|g|} dx^1...dx^n
","['calculus', 'integration', 'definite-integrals', 'manifolds', 'clifford-algebras']"
57,"Verify the following limit using epsilon-delta definition: $ \lim_{(x,y)\to(0,0)}\frac{x^2y^2}{x^2+y^2}=0$",Verify the following limit using epsilon-delta definition:," \lim_{(x,y)\to(0,0)}\frac{x^2y^2}{x^2+y^2}=0","Show that $$ \lim\limits_{(x,y)\to(0,0)}\dfrac{x^2y^2}{x^2+y^2}=0$$ My try: We know that, $$ x^2\leq x^2+y^2 \implies x^2y^2\leq (x^2+y^2)y^2 \implies x^2y^2\leq (x^2+y^2)^2$$ Then, $$\dfrac{x^2y^2}{x^2+y^2}\leq x^2+y^2 $$ So we chose $\delta=\sqrt{\epsilon}$","Show that My try: We know that, Then, So we chose"," \lim\limits_{(x,y)\to(0,0)}\dfrac{x^2y^2}{x^2+y^2}=0  x^2\leq x^2+y^2 \implies x^2y^2\leq (x^2+y^2)y^2 \implies x^2y^2\leq (x^2+y^2)^2 \dfrac{x^2y^2}{x^2+y^2}\leq x^2+y^2  \delta=\sqrt{\epsilon}","['calculus', 'limits', 'epsilon-delta']"
58,Calculating $H'(x)$ given $H(x) = \int_{x^3 + 1}^{x^2 + 2x} e^{-t^2} dt$,Calculating  given,H'(x) H(x) = \int_{x^3 + 1}^{x^2 + 2x} e^{-t^2} dt,"Given $\displaystyle H(x) = \int_{x^3 + 1}^{x^2 + 2x} e^{-t^2} dt$ , we want to find $H'(x)$ . First, we rewrite $H(x)$ as follows: $$\begin{align} &= \int_0^{x^2 + 2x} e^{-t^2} dt + \int_{x^3 + 1}^0 e^{-t^2} dt \qquad &\text{Properties of integrals} \\ &= \int_0^{x^2 + 2x} e^{-t^2} dt - \int_{0}^{x^3 + 1} e^{-t^2} dt \qquad &\text{Definition of backwards integrals} \tag{1} \end{align} $$ Next, we'll define $\displaystyle F(x) = \int_0^x e^{-t^2} dt$ . We know its derivative is $F'(x) = e^{-x^2}$ , by the Fundamental Theorem of Calculus. Next, we'll define new functions for the two integrals in $(1)$ : $$\begin{align*} H_1(x) &= \int_0^{x^2 + 2x} e^{-t^2} dt  &\qquad H_2(x) &= \displaystyle\int_{0}^{x^3 + 1} e^{-t^2} dt \\ &= F(x^2 + 2x)& &=F(x^3 + 1) \end{align*}$$ We use the chain rule to find their derivatives: $$ H_1'(x) = e^{-(x^2 + 2x)^2} (2x + 2) \qquad H_2'(x) = e^{-(x^3 + 1)^2} (3x) $$ Therefore, $$H'(x) = e^{-(x^2 + 2x)^2} (2x + 2) - e^{-(x^3 + 1)^2} (3x)$$ Is my calculation correct?","Given , we want to find . First, we rewrite as follows: Next, we'll define . We know its derivative is , by the Fundamental Theorem of Calculus. Next, we'll define new functions for the two integrals in : We use the chain rule to find their derivatives: Therefore, Is my calculation correct?","\displaystyle H(x) = \int_{x^3 + 1}^{x^2 + 2x} e^{-t^2} dt H'(x) H(x) \begin{align}
&= \int_0^{x^2 + 2x} e^{-t^2} dt + \int_{x^3 + 1}^0 e^{-t^2} dt \qquad &\text{Properties of integrals} \\
&= \int_0^{x^2 + 2x} e^{-t^2} dt - \int_{0}^{x^3 + 1} e^{-t^2} dt \qquad &\text{Definition of backwards integrals} \tag{1}
\end{align}
 \displaystyle F(x) = \int_0^x e^{-t^2} dt F'(x) = e^{-x^2} (1) \begin{align*}
H_1(x) &= \int_0^{x^2 + 2x} e^{-t^2} dt  &\qquad H_2(x) &= \displaystyle\int_{0}^{x^3 + 1} e^{-t^2} dt \\
&= F(x^2 + 2x)& &=F(x^3 + 1)
\end{align*}  H_1'(x) = e^{-(x^2 + 2x)^2} (2x + 2) \qquad H_2'(x) = e^{-(x^3 + 1)^2} (3x)  H'(x) = e^{-(x^2 + 2x)^2} (2x + 2) - e^{-(x^3 + 1)^2} (3x)",['calculus']
59,How to find $\lim\limits_{n \to \infty}\int_{0}^{1}(\cos x-\sin x)^ndx$?,How to find ?,\lim\limits_{n \to \infty}\int_{0}^{1}(\cos x-\sin x)^ndx,"I want to compute $$\lim\limits_{n \to \infty}\int_{0}^{1}(\cos x-\sin x)^n dx.$$ I tried the squeeze theorem, tried simplifying the integral, but I eventually got nothing.","I want to compute I tried the squeeze theorem, tried simplifying the integral, but I eventually got nothing.",\lim\limits_{n \to \infty}\int_{0}^{1}(\cos x-\sin x)^n dx.,"['calculus', 'limits']"
60,Typical Calculus BC Separation of Variables Question,Typical Calculus BC Separation of Variables Question,,"I was told that I have a cylindrical water tank $10$ ft tall that can store $5000 $ ft $^3$ of water, and that the water drains from the bottom of the tank at a rate proportional to the instantaneous water level. After $6$ hours, half of the tank has drained out from the bottom. If the drain is opened and water is added to the tank at a constant rate of $100$ $\frac{ft^3}{hr}$ , at what height above the drain will the elevation remain constant? My attempt: We are given that $$\frac{dV}{dt}=kh$$ where $\frac{dV}{dt}$ is the rate at which water leaves the tank, $V$ is the total change in water that has left the tank,  and $h$ is the water's height above the drain. If the elevation must remain constant in the tank, then I am merely finding the height in which the water that enters the tank equals to the water that leaves the tank, and therefore $\frac{dV}{dt}=100\frac{ft^3}{hr}$ . To find $k$ , I must do separation of variables and integrate both sides, so: $$\int{dV}=\int{khdt}$$ $$V=kht+C$$ I know that $C=0$ because at $t=0$ hr, no water has left the tank ( $V=0$ ), and at $t=6$ hr, $V=2500$ ft $^3$ and $h=5$ ft, since half the tank has drained out. Solving for $k$ : $$2500=k(5)(6)+0$$ $$k=\frac{250}{3}$$ And solving for $h$ when $\frac{dV}{dt}=100\frac{ft^3}{hr}$ : $$100=\frac{250}{3}h$$ $$h=1.2ft$$ However, the answer is actually about $1.73ft$ . Where is my mistake?","I was told that I have a cylindrical water tank ft tall that can store ft of water, and that the water drains from the bottom of the tank at a rate proportional to the instantaneous water level. After hours, half of the tank has drained out from the bottom. If the drain is opened and water is added to the tank at a constant rate of , at what height above the drain will the elevation remain constant? My attempt: We are given that where is the rate at which water leaves the tank, is the total change in water that has left the tank,  and is the water's height above the drain. If the elevation must remain constant in the tank, then I am merely finding the height in which the water that enters the tank equals to the water that leaves the tank, and therefore . To find , I must do separation of variables and integrate both sides, so: I know that because at hr, no water has left the tank ( ), and at hr, ft and ft, since half the tank has drained out. Solving for : And solving for when : However, the answer is actually about . Where is my mistake?",10 5000  ^3 6 100 \frac{ft^3}{hr} \frac{dV}{dt}=kh \frac{dV}{dt} V h \frac{dV}{dt}=100\frac{ft^3}{hr} k \int{dV}=\int{khdt} V=kht+C C=0 t=0 V=0 t=6 V=2500 ^3 h=5 k 2500=k(5)(6)+0 k=\frac{250}{3} h \frac{dV}{dt}=100\frac{ft^3}{hr} 100=\frac{250}{3}h h=1.2ft 1.73ft,['calculus']
61,Prove $\sum_k{2k\choose k}^2{2\left(n-k\right)\choose n-k}^2=\sum_k(-1)^k16^k{n-k \choose k}{2\left(n-k\right)\choose n-k}^3$,Prove,\sum_k{2k\choose k}^2{2\left(n-k\right)\choose n-k}^2=\sum_k(-1)^k16^k{n-k \choose k}{2\left(n-k\right)\choose n-k}^3,"So I've come across this one : $$\forall n\in\mathbb{N},\sum\limits_{k=0}^{n}\left[{2k\choose k}{2\left(n-k\right)\choose n-k}\right]^2=\sum\limits_{k=0}^{\left\lfloor{\frac{n}{2}}\right\rfloor}\left(-1\right)^k16^k{n-k \choose k}{2\left(n-k\right)\choose n-k}^3$$ It looks nice, but since I am not really familiar with combinatorics, I have no clue as how to prove it. What would you suggest ?","So I've come across this one : It looks nice, but since I am not really familiar with combinatorics, I have no clue as how to prove it. What would you suggest ?","\forall n\in\mathbb{N},\sum\limits_{k=0}^{n}\left[{2k\choose k}{2\left(n-k\right)\choose n-k}\right]^2=\sum\limits_{k=0}^{\left\lfloor{\frac{n}{2}}\right\rfloor}\left(-1\right)^k16^k{n-k \choose k}{2\left(n-k\right)\choose n-k}^3","['calculus', 'combinatorics', 'summation']"
62,Evaluate the indefinite integral,Evaluate the indefinite integral,,"$ I = \int (x^2 + 2x)\cos(x) dx $ Integration by Parts, choose $u$ : $$\begin{align} u &= \cos(x) \\ dv &= (x^2 + 2x)dx \\ du &= -\sin(x) \\ v &= \frac{1}{3}x^3 + x^2 \end{align} $$ Substitute into formula: $$ \begin{align} \int udu &= uv - \int vdu \\ &= \cos(x)\left(\frac{1}{3}x^3 + x^2\right) - \int\left(\frac{1}{3}x^3 + 2x\right)(-\sin(x)) \\ &= \cos(x)\left(\frac{1}{3}x^3 + x^2\right) + \int\left(\frac{1}{3}x^3 + 2x\right)(\sin(x)) \end{align} $$ At this point, it doesn't look like I can use the substitution rule on the the right hand integral, so I decide to use the substitution rule again. Integration by Parts II, choose $u$ : $$\begin{align} u &= sinx \\ dv &= (\frac{1}{3}x^3 + 2x)dx \\ du &= cosx \\ v &= \frac{1}{12}x^4 + x^2 \end{align} $$ Substitute into formula: $$\begin{align} \int_{}udu &= uv - \int_{}vdu \\ &= (sinx)(\frac{1}{12}x^4 + x^2) - \int_{} (\frac{1}{12}x^4 + x^2)(cosx)dx \end{align} $$ Combining the two integration by parts together and I feel like I am no closer to evaluating the integral than whence I started...The integral is still there and I feel another parts by integration won't work. $$\int(x^2 + 2x)\cos(x) = (\cos(x))\left(\frac{1}{3}x^3 + x^2\right) + (\sin(x))\left(\frac{1}{12}x^4 + x^2\right) - \int\left(\frac{1}{12}x^4 + x^2\right)(\cos(x))dx$$ Did I do the math wrong and make a mistake somewhere? Or am I supposed to approach this differently?","Integration by Parts, choose : Substitute into formula: At this point, it doesn't look like I can use the substitution rule on the the right hand integral, so I decide to use the substitution rule again. Integration by Parts II, choose : Substitute into formula: Combining the two integration by parts together and I feel like I am no closer to evaluating the integral than whence I started...The integral is still there and I feel another parts by integration won't work. Did I do the math wrong and make a mistake somewhere? Or am I supposed to approach this differently?"," I = \int (x^2 + 2x)\cos(x) dx  u \begin{align}
u &= \cos(x) \\
dv &= (x^2 + 2x)dx \\
du &= -\sin(x) \\
v &= \frac{1}{3}x^3 + x^2
\end{align}
 
\begin{align}
\int udu &= uv - \int vdu \\
&= \cos(x)\left(\frac{1}{3}x^3 + x^2\right) - \int\left(\frac{1}{3}x^3 + 2x\right)(-\sin(x)) \\
&= \cos(x)\left(\frac{1}{3}x^3 + x^2\right) + \int\left(\frac{1}{3}x^3 + 2x\right)(\sin(x))
\end{align}
 u \begin{align}
u &= sinx \\
dv &= (\frac{1}{3}x^3 + 2x)dx \\
du &= cosx \\
v &= \frac{1}{12}x^4 + x^2
\end{align}
 \begin{align}
\int_{}udu &= uv - \int_{}vdu \\
&= (sinx)(\frac{1}{12}x^4 + x^2) - \int_{} (\frac{1}{12}x^4 + x^2)(cosx)dx
\end{align}
 \int(x^2 + 2x)\cos(x) = (\cos(x))\left(\frac{1}{3}x^3 + x^2\right) + (\sin(x))\left(\frac{1}{12}x^4 + x^2\right) - \int\left(\frac{1}{12}x^4 + x^2\right)(\cos(x))dx","['calculus', 'indefinite-integrals']"
63,inverse Fourier transform of $\frac{x^3y}{(x^2+y^2)^2}$,inverse Fourier transform of,\frac{x^3y}{(x^2+y^2)^2},"Let $f(x,y)=\frac{x^3y}{(x^2+y^2)^2}$. What is the inverse Fourier transform of $f$? Since $f$ is neither a $L^1$-function nor a Schwartz-function I am looking for an inverse Fourier transform in the sense of tempered distributions. I’ve tried several distributions but every single one failed. Does anyone have a hint?","Let $f(x,y)=\frac{x^3y}{(x^2+y^2)^2}$. What is the inverse Fourier transform of $f$? Since $f$ is neither a $L^1$-function nor a Schwartz-function I am looking for an inverse Fourier transform in the sense of tempered distributions. I’ve tried several distributions but every single one failed. Does anyone have a hint?",,['calculus']
64,How to use dimensional analysis to find these integrals?,How to use dimensional analysis to find these integrals?,,"Use dimensional analysis to find $\displaystyle \int_0^∞ \mathrm{e}^{-ax} \,\mathrm{d}x$ and $\displaystyle \int\frac{\mathrm{d}x}{x^2 + a^2}$.  A useful result is$$ \int \frac{\mathrm{d}x}{x^2 + 1} = \arctan x + C. $$ I am using the the book called street mathematics to learn more about dimensional analysis. I am trying to understand a problem in the book. The question is to use dimensional analysis to find the solutions to two integrals. both are in the attached photo. I tried to understand the question and how to best tackle it but I did not succeed.","Use dimensional analysis to find $\displaystyle \int_0^∞ \mathrm{e}^{-ax} \,\mathrm{d}x$ and $\displaystyle \int\frac{\mathrm{d}x}{x^2 + a^2}$.  A useful result is$$ \int \frac{\mathrm{d}x}{x^2 + 1} = \arctan x + C. $$ I am using the the book called street mathematics to learn more about dimensional analysis. I am trying to understand a problem in the book. The question is to use dimensional analysis to find the solutions to two integrals. both are in the attached photo. I tried to understand the question and how to best tackle it but I did not succeed.",,"['calculus', 'dimensional-analysis']"
65,Find $f(3)$ if $f(f(x))=3+2x$,Find  if,f(3) f(f(x))=3+2x,A function $f\colon \mathbb{R} \to \mathbb{R}$ is defined as $f(f(x))=3+2x$ Find $f(3)$ if $f(0)=3$ My try: Method $1.$ Put $x=0$ we get $f(f(0))=3$ $\implies$ $f(3)=3$ Method $2.$ Replace $x$ with $f(x)$ we get $$f(f(f(x)))=3+2f(x)$$ $\implies$ $$f(3+2x)=3+2f(x)$$ Put $x=0$ $$f(3)=9$$ I feel Method $2.$ is Correct since $f(f(x))=3+2x$ is Injective which means $f(x)$ should be Injective.,A function $f\colon \mathbb{R} \to \mathbb{R}$ is defined as $f(f(x))=3+2x$ Find $f(3)$ if $f(0)=3$ My try: Method $1.$ Put $x=0$ we get $f(f(0))=3$ $\implies$ $f(3)=3$ Method $2.$ Replace $x$ with $f(x)$ we get $$f(f(f(x)))=3+2f(x)$$ $\implies$ $$f(3+2x)=3+2f(x)$$ Put $x=0$ $$f(3)=9$$ I feel Method $2.$ is Correct since $f(f(x))=3+2x$ is Injective which means $f(x)$ should be Injective.,,"['calculus', 'algebra-precalculus', 'functions']"
66,Continuous antiderivative of $\frac{1}{1+\cos^2 x}$ without the floor function.,Continuous antiderivative of  without the floor function.,\frac{1}{1+\cos^2 x},"By letting $u = 2x$ and $t = \tan \frac{u}{2}$, I found the continuous antiderivative of the function to be: $$\int \frac{1}{1+\cos^2 x}dx\\= \int \frac{2}{3+\cos2x} dx\\ = \int \frac{1}{3+\cos u}du \\=\int \frac{\frac{2}{1+t^2}}{3+\frac{1-t^2}{1+t^2}}dt\\= \int\frac{1}{2+t^2}dt  \\= \frac{1}{\sqrt2}\arctan\left(\frac{\tan x}{\sqrt2}\right)  +  \frac{\pi}{\sqrt2} \left\lfloor \frac{x + \frac{\pi}{2} }{\pi} \right\rfloor + C $$ (I graphically deduced the floor function bit as I am not familiar with its algebra.) However, GeoGebra (notably not wolfram) does it better. It states, without the floor function, that the continuous antiderivative is also: $$ \frac{x}{\sqrt2} + \frac{1}{\sqrt2} \arctan\left( \frac{(1-\sqrt2)\sin 2x}{(\sqrt2 -1)\cos2x +\sqrt2 + 1}\right) + C$$ How did GeoGebra accomplish such a feat? And how can I prove and apply such ingenuity ?","By letting $u = 2x$ and $t = \tan \frac{u}{2}$, I found the continuous antiderivative of the function to be: $$\int \frac{1}{1+\cos^2 x}dx\\= \int \frac{2}{3+\cos2x} dx\\ = \int \frac{1}{3+\cos u}du \\=\int \frac{\frac{2}{1+t^2}}{3+\frac{1-t^2}{1+t^2}}dt\\= \int\frac{1}{2+t^2}dt  \\= \frac{1}{\sqrt2}\arctan\left(\frac{\tan x}{\sqrt2}\right)  +  \frac{\pi}{\sqrt2} \left\lfloor \frac{x + \frac{\pi}{2} }{\pi} \right\rfloor + C $$ (I graphically deduced the floor function bit as I am not familiar with its algebra.) However, GeoGebra (notably not wolfram) does it better. It states, without the floor function, that the continuous antiderivative is also: $$ \frac{x}{\sqrt2} + \frac{1}{\sqrt2} \arctan\left( \frac{(1-\sqrt2)\sin 2x}{(\sqrt2 -1)\cos2x +\sqrt2 + 1}\right) + C$$ How did GeoGebra accomplish such a feat? And how can I prove and apply such ingenuity ?",,"['calculus', 'indefinite-integrals', 'ceiling-and-floor-functions']"
67,Computing a derivative from a function that involves an integral,Computing a derivative from a function that involves an integral,,"Let $f$ be a function such that $$f(t)=\int_0^\infty \frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx,\;t>0:$$ find $f'(t)$. My attempt: $$\begin{align} f(t)&=\int_0^\infty\frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx\\ \frac{df}{dt}&=\frac{d}{dt}\int_0^\infty\frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx\\ &=\int_0^\infty\frac{d}{dt}\frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx\\ &=\int_0^\infty\frac{\sin(x^2)}{x^2}\frac{d(e^{-tx^2})}{dt}\,dx\\ &=-\int_0^\infty\sin(x^2)\,e^{-tx^2}\,dx. \end{align}$$ Then I used that $2i\sin(x)=e^{ix}-e^{-ix}$ and $\displaystyle\int_{-\infty}^{+\infty}e^{-ax^2}\,dx=\sqrt{\frac{\pi}{a}}$ to achieve $$\begin{align} \int_0^\infty\sin(x^2)\,e^{-tx^2}\,dx&=\int_0^\infty\frac{e^{ix^2}-e^{-ix^2}}{2i}\,e^{-tx^2}\,dx\\ &=\frac{1}{2i}\int_0^\infty\left[e^{(i-t)x^2}-e^{-(i+t)x^2}\right]\,dx\\ &=\frac{1}{2i}\left[\int_0^\infty e^{(i-t)x^2}\,dx-\int_{0}^{\infty}e^{-(i+t)x^2}\,dx\right]\\ &=\frac{\sqrt{\pi}}{4i}\left(\sqrt{\frac{1}{t-i}}-\sqrt{\frac{1}{t+i}}\right). \end{align}$$ Is my answer correct? And how can I approach this problem?","Let $f$ be a function such that $$f(t)=\int_0^\infty \frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx,\;t>0:$$ find $f'(t)$. My attempt: $$\begin{align} f(t)&=\int_0^\infty\frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx\\ \frac{df}{dt}&=\frac{d}{dt}\int_0^\infty\frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx\\ &=\int_0^\infty\frac{d}{dt}\frac{\sin(x^2)e^{-tx^2}}{x^2}\,dx\\ &=\int_0^\infty\frac{\sin(x^2)}{x^2}\frac{d(e^{-tx^2})}{dt}\,dx\\ &=-\int_0^\infty\sin(x^2)\,e^{-tx^2}\,dx. \end{align}$$ Then I used that $2i\sin(x)=e^{ix}-e^{-ix}$ and $\displaystyle\int_{-\infty}^{+\infty}e^{-ax^2}\,dx=\sqrt{\frac{\pi}{a}}$ to achieve $$\begin{align} \int_0^\infty\sin(x^2)\,e^{-tx^2}\,dx&=\int_0^\infty\frac{e^{ix^2}-e^{-ix^2}}{2i}\,e^{-tx^2}\,dx\\ &=\frac{1}{2i}\int_0^\infty\left[e^{(i-t)x^2}-e^{-(i+t)x^2}\right]\,dx\\ &=\frac{1}{2i}\left[\int_0^\infty e^{(i-t)x^2}\,dx-\int_{0}^{\infty}e^{-(i+t)x^2}\,dx\right]\\ &=\frac{\sqrt{\pi}}{4i}\left(\sqrt{\frac{1}{t-i}}-\sqrt{\frac{1}{t+i}}\right). \end{align}$$ Is my answer correct? And how can I approach this problem?",,"['calculus', 'definite-integrals', 'improper-integrals']"
68,Coordinate-free Calculus of Variations?,Coordinate-free Calculus of Variations?,,"Lately I've been doing a lot of work related to solving variational problems (in the context of surface theory), and I'm getting really tired of going to local coordinates for everything. So, I was wondering: does anyone know a good coordinate-free treatment for the calculus of variations?  I've heard such literature exists, but I can't say I'm familiar with it. Any suggested reading you all may have would be much appreciated. Thanks in advance,","Lately I've been doing a lot of work related to solving variational problems (in the context of surface theory), and I'm getting really tired of going to local coordinates for everything. So, I was wondering: does anyone know a good coordinate-free treatment for the calculus of variations?  I've heard such literature exists, but I can't say I'm familiar with it. Any suggested reading you all may have would be much appreciated. Thanks in advance,",,"['calculus', 'reference-request', 'riemannian-geometry']"
69,Geometric interpretations of the quotient rule and reciprocal rule.,Geometric interpretations of the quotient rule and reciprocal rule.,,"I have been reviewing basic calculus by working through Gilbert Strang's book ""Calculus"". My full time job is as a programmer, but I tutor calculus on the side. Professor Strang has kindly made the pdfs of the book, solutions manual, and study guide available for free . I understand his 'regular' proofs of the reciprocal rule, $$\frac{d}{{dx}}\left( {\frac{1}{{v\left( x \right)}}} \right) = \frac{{ - dv/dx}}{{{v^2}}}$$ and the quotient rule, $$\frac{d}{{dx}}\left( {\frac{{u\left( x \right)}}{{v\left( x \right)}}} \right) = \frac{{v\frac{{du}}{{dx}} - u\frac{{dv}}{{dx}}}}{{{v^2}}}$$ however... I'm struggling to understand what appears to be his informal geometric derivation of both of these rules from figure 2.14, pg  74: I know how to derive that $\frac{1}{{v + \Delta v}} - \frac{1}{v}$ is equal to $\frac{{ - \Delta v}}{{v\left( {v + \Delta v} \right)}}$ , and that $\frac{{u + \Delta u}}{{v + \Delta v}} - \frac{u}{v} = \frac{{v\Delta u - u\Delta v}}{{v\left( {v + \Delta v} \right)}}$ . I also know how to get from those equalities to the reciprocal rule and quotient rule. I just have not been able to figure out an intuitive way to understand the reciprocal rule and quotient rule geometrically from figure 2.14. I found a way to use figure 2.14 and geometry to derive $\frac{1}{{v + \Delta v}} - \frac{1}{v} = \frac{{ - \Delta v}}{{v\left( {v + \Delta v} \right)}}$ and $\frac{{ - \Delta v}}{{v\left( {v + \Delta v} \right)}}$ . I couldn't think of a geometric way to derive $\frac{{u + \Delta u}}{{v + \Delta v}} - \frac{u}{v} = \frac{{v\Delta u - u\Delta v}}{{v\left( {v + \Delta v} \right)}}$ . Basically, the best I was able to do for the numerator of the difference quotient for the reciprocal rule from figure 2.14 was show that a geometric derivation of that equation is consistent with the algebraic definition. Here's the best I could come up with, using a modified version of figure 2.14 that I made in paint: For the reciprocal rule (left side of the and its top equation). $$\text{Area}_{red - triangle} = \frac{{\Delta v}}{2}$$ $$\text{Area}_{blue - triangle} = \frac{v}{2}$$ $$\text{Area}_{red - triangle} + \text{Area}_{blue - triangle} = \frac{{v + \Delta v}}{2}$$ $$v + \Delta v = 2\left( {\text{Area}_{red - triangle}} + \text{Area}_{blue - triangle} \right)$$ $$v = 2\text{Area}_{blue - triangle}$$ $$\frac{1}{{v + \Delta v}} = \frac{1}{{2\left( {Are{a_{red - triangle}} + Are{a_{blue - triangle}}} \right)}}$$ $$\frac{1}{v} = \frac{1}{{2\text{Area}_{blue - triangle}}}$$ $$\frac{1}{{v + \Delta v}} = \frac{1}{{2\left( {Are{a_{red - triangle}} + Are{a_{blue - triangle}}} \right)}}$$ $$\frac{1}{{v + \Delta v}} - \frac{1}{v} = \frac{1}{2}\left( {\frac{{ - Are{a_{red - triangle}}}}{{\left( {Are{a_{red - triangle}} + Are{a_{blue - triangle}}} \right)Are{a_{blue - triangle}}}}} \right)$$ $$\frac{1}{{v + \Delta v}} - \frac{1}{v} = \frac{1}{2}\left[ {\frac{{ - \frac{{\Delta v}}{2}}}{{\left( {\frac{{v + \Delta v}}{2}} \right)\frac{v}{2}}}} \right] = \frac{{ - \Delta v}}{{\left( {v + \Delta v} \right)\frac{v}{2}}}$$ From there... I know how to derive the reciprocal rule... taking the limit of the difference quotient as $\Delta x \rightarrow 0$ Basically seems like circular reasoning... I assume that Professor Strang included the figure to give students an intuitive, geometric feel of the quotient rule and reciprocal rule, considering that Professor Strang's informal geometric derivation of the product rule is intuitive (also... why include figure 2.14 at all if it isn't meant to help readers gain an intuitive grasp of the reciprocal and quotient rules). The pdf of his textbook is in black and white, which makes figure 2.13 on page 72 difficult to read. Here is a re-creation of figure 2.13 that I put together using paint: And the product rule plus an informal proof of it: If $u = u(x)$ and $v = v(x)$ (assuming u and v are differentiable functions of x): $$\frac{d}{{dx}}\left( {uv} \right) = u\frac{{dv}}{{dx}} + v\frac{{du}}{{dx}}$$ Informal 'proof': $$\frac{d}{{dx}}\left( {uv} \right) = \mathop {\lim }\limits_{h \to 0} \frac{{u\left( {x + h} \right)v\left( {x + h} \right) - u\left( x \right)v(x)}}{h}$$ As $h \to 0$ , $\Delta u \to 0{\text{ and }}\Delta v \to h$ From 'Calculus' by Gilbert Strang: The important change in area are the two strips $u \Delta v$ and $v \Delta u$ . The corner area $\Delta u \Delta v$ is much smaller. When we divide by $\Delta x$ (h), the strips give $u \frac{\Delta v}{\Delta x}$ and $v \frac{\Delta u}{\Delta x}$ . The corner gives $\Delta u \frac{Delta v}{\Delta x}$ , which approaches zero. The extra area comes from the whole side strip. I would greatly appreciate it if someone could help me figure out a better way to interpret figure 2.14. I know it isn't really essential for me to have an intuitive feel of the quotient rule or the reciprocal rule, however it drives me pretty crazy if I can't figure something out.","I have been reviewing basic calculus by working through Gilbert Strang's book ""Calculus"". My full time job is as a programmer, but I tutor calculus on the side. Professor Strang has kindly made the pdfs of the book, solutions manual, and study guide available for free . I understand his 'regular' proofs of the reciprocal rule, and the quotient rule, however... I'm struggling to understand what appears to be his informal geometric derivation of both of these rules from figure 2.14, pg  74: I know how to derive that is equal to , and that . I also know how to get from those equalities to the reciprocal rule and quotient rule. I just have not been able to figure out an intuitive way to understand the reciprocal rule and quotient rule geometrically from figure 2.14. I found a way to use figure 2.14 and geometry to derive and . I couldn't think of a geometric way to derive . Basically, the best I was able to do for the numerator of the difference quotient for the reciprocal rule from figure 2.14 was show that a geometric derivation of that equation is consistent with the algebraic definition. Here's the best I could come up with, using a modified version of figure 2.14 that I made in paint: For the reciprocal rule (left side of the and its top equation). From there... I know how to derive the reciprocal rule... taking the limit of the difference quotient as Basically seems like circular reasoning... I assume that Professor Strang included the figure to give students an intuitive, geometric feel of the quotient rule and reciprocal rule, considering that Professor Strang's informal geometric derivation of the product rule is intuitive (also... why include figure 2.14 at all if it isn't meant to help readers gain an intuitive grasp of the reciprocal and quotient rules). The pdf of his textbook is in black and white, which makes figure 2.13 on page 72 difficult to read. Here is a re-creation of figure 2.13 that I put together using paint: And the product rule plus an informal proof of it: If and (assuming u and v are differentiable functions of x): Informal 'proof': As , From 'Calculus' by Gilbert Strang: The important change in area are the two strips and . The corner area is much smaller. When we divide by (h), the strips give and . The corner gives , which approaches zero. The extra area comes from the whole side strip. I would greatly appreciate it if someone could help me figure out a better way to interpret figure 2.14. I know it isn't really essential for me to have an intuitive feel of the quotient rule or the reciprocal rule, however it drives me pretty crazy if I can't figure something out.",\frac{d}{{dx}}\left( {\frac{1}{{v\left( x \right)}}} \right) = \frac{{ - dv/dx}}{{{v^2}}} \frac{d}{{dx}}\left( {\frac{{u\left( x \right)}}{{v\left( x \right)}}} \right) = \frac{{v\frac{{du}}{{dx}} - u\frac{{dv}}{{dx}}}}{{{v^2}}} \frac{1}{{v + \Delta v}} - \frac{1}{v} \frac{{ - \Delta v}}{{v\left( {v + \Delta v} \right)}} \frac{{u + \Delta u}}{{v + \Delta v}} - \frac{u}{v} = \frac{{v\Delta u - u\Delta v}}{{v\left( {v + \Delta v} \right)}} \frac{1}{{v + \Delta v}} - \frac{1}{v} = \frac{{ - \Delta v}}{{v\left( {v + \Delta v} \right)}} \frac{{ - \Delta v}}{{v\left( {v + \Delta v} \right)}} \frac{{u + \Delta u}}{{v + \Delta v}} - \frac{u}{v} = \frac{{v\Delta u - u\Delta v}}{{v\left( {v + \Delta v} \right)}} \text{Area}_{red - triangle} = \frac{{\Delta v}}{2} \text{Area}_{blue - triangle} = \frac{v}{2} \text{Area}_{red - triangle} + \text{Area}_{blue - triangle} = \frac{{v + \Delta v}}{2} v + \Delta v = 2\left( {\text{Area}_{red - triangle}} + \text{Area}_{blue - triangle} \right) v = 2\text{Area}_{blue - triangle} \frac{1}{{v + \Delta v}} = \frac{1}{{2\left( {Are{a_{red - triangle}} + Are{a_{blue - triangle}}} \right)}} \frac{1}{v} = \frac{1}{{2\text{Area}_{blue - triangle}}} \frac{1}{{v + \Delta v}} = \frac{1}{{2\left( {Are{a_{red - triangle}} + Are{a_{blue - triangle}}} \right)}} \frac{1}{{v + \Delta v}} - \frac{1}{v} = \frac{1}{2}\left( {\frac{{ - Are{a_{red - triangle}}}}{{\left( {Are{a_{red - triangle}} + Are{a_{blue - triangle}}} \right)Are{a_{blue - triangle}}}}} \right) \frac{1}{{v + \Delta v}} - \frac{1}{v} = \frac{1}{2}\left[ {\frac{{ - \frac{{\Delta v}}{2}}}{{\left( {\frac{{v + \Delta v}}{2}} \right)\frac{v}{2}}}} \right] = \frac{{ - \Delta v}}{{\left( {v + \Delta v} \right)\frac{v}{2}}} \Delta x \rightarrow 0 u = u(x) v = v(x) \frac{d}{{dx}}\left( {uv} \right) = u\frac{{dv}}{{dx}} + v\frac{{du}}{{dx}} \frac{d}{{dx}}\left( {uv} \right) = \mathop {\lim }\limits_{h \to 0} \frac{{u\left( {x + h} \right)v\left( {x + h} \right) - u\left( x \right)v(x)}}{h} h \to 0 \Delta u \to 0{\text{ and }}\Delta v \to h u \Delta v v \Delta u \Delta u \Delta v \Delta x u \frac{\Delta v}{\Delta x} v \frac{\Delta u}{\Delta x} \Delta u \frac{Delta v}{\Delta x},"['calculus', 'derivatives']"
70,"Why do people say dy/dx is not a fraction, but then use it as one when doing the chain rule?","Why do people say dy/dx is not a fraction, but then use it as one when doing the chain rule?",,"To my knowledge, dy/dx is equal to the limit of (f(x+h) - f(x)) / h as h approaches zero. That is, dy is equal to the difference in the y value (f(x+h) - f(x)) and dx is equal to the difference in the x value (h) and dy/dx is equal to the rate of change of the y function as the x function increases. As well as this, in the chain rule you multiply dy/du * du/dx in order to cancel out the du's like you would with normal fractions. Despite this, all of my teachers insist that dy/dx is not a ratio, it is just a symbol for the gradient or for the derivative of y with respect to x that can't be split up and manipulated like a normal fraction can be. On a side note, the symbol d/dx (y) to mean ""differentiate y with respect to x"" confuses me too, as i thought that dy and dx were atomic symbols, but this notation splits up d and y even further.","To my knowledge, dy/dx is equal to the limit of (f(x+h) - f(x)) / h as h approaches zero. That is, dy is equal to the difference in the y value (f(x+h) - f(x)) and dx is equal to the difference in the x value (h) and dy/dx is equal to the rate of change of the y function as the x function increases. As well as this, in the chain rule you multiply dy/du * du/dx in order to cancel out the du's like you would with normal fractions. Despite this, all of my teachers insist that dy/dx is not a ratio, it is just a symbol for the gradient or for the derivative of y with respect to x that can't be split up and manipulated like a normal fraction can be. On a side note, the symbol d/dx (y) to mean ""differentiate y with respect to x"" confuses me too, as i thought that dy and dx were atomic symbols, but this notation splits up d and y even further.",,"['calculus', 'notation']"
71,irrationality or rationality of $\log(\log(2))$.,irrationality or rationality of .,\log(\log(2)),"I know the standard proof that $\log_{10}(2)$ is irrational. Can we prove irrationality of  $\log_{10}(\log_{10}(2))$ using, somehow, similar methods?","I know the standard proof that $\log_{10}(2)$ is irrational. Can we prove irrationality of  $\log_{10}(\log_{10}(2))$ using, somehow, similar methods?",,"['calculus', 'irrational-numbers']"
72,Inequality AM/HM <= (AM/GM)^n with packing problem interpretation,Inequality AM/HM <= (AM/GM)^n with packing problem interpretation,,"I have stumbled upon the following inequality while exploring ""Hoffman's packing problem"", which I'm pretty convinced is true, but unable to prove. Let $n \geq 2$ be a natural number and let $x_1, \dots, x_n$ be positive real numbers. How do I prove that: $$ \frac{AM}{HM} \leq \left(\frac{AM}{GM}\right)^n, $$ where the harmonic mean $HM$, geometric mean $GM$ and arithmetic mean $AM$ are defined as: $$ HM = \frac{n}{\sum_{i=1}^{n}{\frac{1}{x_i}}}, \quad GM = \sqrt[n]{\prod_{i=1}^{n}{x_i}} \quad \text{and} \quad AM = \frac{1}{n}\sum_{i=1}^{n}{x_i}. $$ What I know so far We have equality if $n=2$ or if $x_1 = \dots = x_n$. I haven't been able to prove the inequality in general, but for $n=3$ (let's use $x_1,x_2,x_3 = a,b,c$ for clarity) we obtain: $$ \frac{\frac{a+b+c}{3}}{\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}} \leq \left(\frac{\frac{a+b+c}{3}}{\sqrt[n]{abc}}\right)^3 \Leftrightarrow\\ \frac{(a+b+c) \cdot (bc + ac + ab)}{9abc} \leq \frac{(a+b+c)^3}{27abc} \Leftrightarrow\\ 3(bc + ac + ab) \leq (a+b+c)^2 \quad (*) \Leftrightarrow\\ 0 \leq a^2 + b^2 + c^2 - bc - ac - ab \Leftrightarrow\\ 0 \leq 2a^2 + 2b^2 + 2c^2 - 2bc - 2ac - 2ab \Leftrightarrow\\ 0 \leq (a - b)^2 + (b - c)^2 + (c - a)^2, $$ which is clearly true. Interpretation as packing problem The inequality can be interpreted as an $n-1$ dimensional packing problem. For instance if $n=3$ the inequality can be rearranged as $3(bc + ac + ab) \leq (a+b+c)^2$, see (*) above. We can look at this like fitting three $a \times b$, three $b \times c$ and three $c \times a$ rectangles inside a square with side-length $a+b+c$. Here is an example of a packing: Caption: Suppose $a < b < c$. This is a packing of 3 red $a \times b$, 3 blue $b \times c$ and 3 green $c \times a$ rectangles inside a square with side-length $a+b+c$.","I have stumbled upon the following inequality while exploring ""Hoffman's packing problem"", which I'm pretty convinced is true, but unable to prove. Let $n \geq 2$ be a natural number and let $x_1, \dots, x_n$ be positive real numbers. How do I prove that: $$ \frac{AM}{HM} \leq \left(\frac{AM}{GM}\right)^n, $$ where the harmonic mean $HM$, geometric mean $GM$ and arithmetic mean $AM$ are defined as: $$ HM = \frac{n}{\sum_{i=1}^{n}{\frac{1}{x_i}}}, \quad GM = \sqrt[n]{\prod_{i=1}^{n}{x_i}} \quad \text{and} \quad AM = \frac{1}{n}\sum_{i=1}^{n}{x_i}. $$ What I know so far We have equality if $n=2$ or if $x_1 = \dots = x_n$. I haven't been able to prove the inequality in general, but for $n=3$ (let's use $x_1,x_2,x_3 = a,b,c$ for clarity) we obtain: $$ \frac{\frac{a+b+c}{3}}{\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}} \leq \left(\frac{\frac{a+b+c}{3}}{\sqrt[n]{abc}}\right)^3 \Leftrightarrow\\ \frac{(a+b+c) \cdot (bc + ac + ab)}{9abc} \leq \frac{(a+b+c)^3}{27abc} \Leftrightarrow\\ 3(bc + ac + ab) \leq (a+b+c)^2 \quad (*) \Leftrightarrow\\ 0 \leq a^2 + b^2 + c^2 - bc - ac - ab \Leftrightarrow\\ 0 \leq 2a^2 + 2b^2 + 2c^2 - 2bc - 2ac - 2ab \Leftrightarrow\\ 0 \leq (a - b)^2 + (b - c)^2 + (c - a)^2, $$ which is clearly true. Interpretation as packing problem The inequality can be interpreted as an $n-1$ dimensional packing problem. For instance if $n=3$ the inequality can be rearranged as $3(bc + ac + ab) \leq (a+b+c)^2$, see (*) above. We can look at this like fitting three $a \times b$, three $b \times c$ and three $c \times a$ rectangles inside a square with side-length $a+b+c$. Here is an example of a packing: Caption: Suppose $a < b < c$. This is a packing of 3 red $a \times b$, 3 blue $b \times c$ and 3 green $c \times a$ rectangles inside a square with side-length $a+b+c$.",,"['calculus', 'inequality', 'puzzle', 'packing-problem']"
73,$n$-th derivative of $y$ in $x^y=e^{x-y}$,-th derivative of  in,n y x^y=e^{x-y},"Question: If $x^y=e^{x-y}$, find a general formula for $\displaystyle\frac{d^ny}{dx^n}$. My Attempt: $$x^y=e^{x-y}$$ $$\Rightarrow\ \ \ \ \ y\ln x=x-y$$ $$\Rightarrow\ \ \ \ \ y=\frac{x}{\ln x+1}$$ So $$\begin{align}\frac{dy}{dx}&=\frac{\ln x+1-1}{(\ln x+1)^2}\\ &=\frac{\ln x}{(\ln x+1)^2} \end{align}$$ Then $$\begin{align}\frac{d^2y}{dx^2}&=\frac{\frac{1}{x}(\ln x+1)^2+\frac{2\ln x}{x}(\ln x+1)}{(\ln x+1)^4} \end{align}$$ Now, I can see that the denominator satisfies a straight-forward pattern $(\ln x+1)^{2^n}$. So the general formula for $\displaystyle\frac{d^ny}{dx^n}$ is $$\frac{d^ny}{dx^n}=\frac{g(n)}{(\ln x+1)^{2^n}}$$But I can't derive any formula for the numerator $g(n)$, nor can I see any regular pattern in the structure. Should I differentiate the initial expression $x^y=e^{x-y}$ implicity? Can anyone help me in that?","Question: If $x^y=e^{x-y}$, find a general formula for $\displaystyle\frac{d^ny}{dx^n}$. My Attempt: $$x^y=e^{x-y}$$ $$\Rightarrow\ \ \ \ \ y\ln x=x-y$$ $$\Rightarrow\ \ \ \ \ y=\frac{x}{\ln x+1}$$ So $$\begin{align}\frac{dy}{dx}&=\frac{\ln x+1-1}{(\ln x+1)^2}\\ &=\frac{\ln x}{(\ln x+1)^2} \end{align}$$ Then $$\begin{align}\frac{d^2y}{dx^2}&=\frac{\frac{1}{x}(\ln x+1)^2+\frac{2\ln x}{x}(\ln x+1)}{(\ln x+1)^4} \end{align}$$ Now, I can see that the denominator satisfies a straight-forward pattern $(\ln x+1)^{2^n}$. So the general formula for $\displaystyle\frac{d^ny}{dx^n}$ is $$\frac{d^ny}{dx^n}=\frac{g(n)}{(\ln x+1)^{2^n}}$$But I can't derive any formula for the numerator $g(n)$, nor can I see any regular pattern in the structure. Should I differentiate the initial expression $x^y=e^{x-y}$ implicity? Can anyone help me in that?",,"['calculus', 'derivatives']"
74,Triangle and Circle maximization problem,Triangle and Circle maximization problem,,"So I was playing around GeoGebra and found this thing out, I don't know if this problem has a name or something. Triangle ABC is inscribed inside a circle, from point D which is located inside the circle, we draw 3 perpendicular lines to each side of the triangle, what is the maximum area of the triangle whose vertices are the intersections of the perpendicular lines and the sides of the triangle? (maximum area of triangle EFG, the red triangle in the picture) Using Geogebra I found out that this area is always maximal when point D is located at the center of the circle, or in other words, when the perpendiculars divide the sides into 2 equal segments. If someone could provide a proof/explain why, I would be grateful. See the diagram below:","So I was playing around GeoGebra and found this thing out, I don't know if this problem has a name or something. Triangle ABC is inscribed inside a circle, from point D which is located inside the circle, we draw 3 perpendicular lines to each side of the triangle, what is the maximum area of the triangle whose vertices are the intersections of the perpendicular lines and the sides of the triangle? (maximum area of triangle EFG, the red triangle in the picture) Using Geogebra I found out that this area is always maximal when point D is located at the center of the circle, or in other words, when the perpendiculars divide the sides into 2 equal segments. If someone could provide a proof/explain why, I would be grateful. See the diagram below:",,"['calculus', 'geometry', 'optimization', 'circles', 'triangles']"
75,How addition will be if we don't define it only on pairs of numbers?,How addition will be if we don't define it only on pairs of numbers?,,"In calculus by Michael spivak book he say in page 4 It might seem reasonable to regard addition as an operation which can be performed on several numbers at once and consider the sum $a_1+...+a_n$ of n number $a_1,...,a_n$ as a basic concept . it is more convenient , however to consider addition of pairs of numbers only and to define other sums in terms of sums of this types My questions : 1- How addition will be if we don't define it only on pairs of numbers ? 2- why it is more convenient , however to consider addition of pairs of numbers only and to define other sums in terms of sums of this type s?","In calculus by Michael spivak book he say in page 4 It might seem reasonable to regard addition as an operation which can be performed on several numbers at once and consider the sum of n number as a basic concept . it is more convenient , however to consider addition of pairs of numbers only and to define other sums in terms of sums of this types My questions : 1- How addition will be if we don't define it only on pairs of numbers ? 2- why it is more convenient , however to consider addition of pairs of numbers only and to define other sums in terms of sums of this type s?","a_1+...+a_n a_1,...,a_n","['calculus', 'arithmetic']"
76,Use the Comparison Test,Use the Comparison Test,,"Use the Comparison Test to determine for what values of $p$ the integral: $\int_{8}^{\infty} \frac{1}{x^p} \ ln(x)  \ dx$ converges. (Use interval notation.) $$\\$$ This is what I have so far: I'm comparing to $\frac{1}{x}$, which diverges. I'm unsure of what to do next. I know it starts to converge from 2. I would really appreciate to your help.","Use the Comparison Test to determine for what values of $p$ the integral: $\int_{8}^{\infty} \frac{1}{x^p} \ ln(x)  \ dx$ converges. (Use interval notation.) $$\\$$ This is what I have so far: I'm comparing to $\frac{1}{x}$, which diverges. I'm unsure of what to do next. I know it starts to converge from 2. I would really appreciate to your help.",,"['calculus', 'integration', 'convergence-divergence']"
77,Limit at endpoints,Limit at endpoints,,"The definition of limit states (Spivak): The function $f$ approaches the limit $l$ near $a$ means : for every   $\varepsilon >0$ there is some $\delta >0$ such that, for all $x$, if   $0< |x-a|<\delta$, then $|f(x)-l|<\varepsilon$. and if its not true that $f$ approaches $l$ near $a$: There is some $\varepsilon>0$ such that for every $\delta>0$ there   is some $x$  which satisfies $0<|x-a|<\delta$ but not   $|f(x)-l|<\varepsilon$. There are some textbooks that provide the definition of limit with the additional condition ""for all $x \in D_{f}\dots$"", but it seems to me that Spivak doesn't find it necessary. So, supposing this condition is neither logically necessary nor implicit, suppose $\lim_{x\to a^{+}}g(x)=L$ for some function $g:[a,b]\to \mathbb{R}$.  In order to verify that actually $\lim_{x\to a}g(x)=L$, we could prove that the negated form of the limit definition above is false for $g$ near $a$. However, when $x<a$, $x$ is outside de domain of $g$, and $f(x)$ is not defined. So, perhaps, it's not the case that we have a vacuously true statement, because testing the veracity of an expression containing $|f(x)-L|$ does not even make logical sense at all. What happens in a logical statement when a free variable takes a particular instance that turns an expression of it into nonsense? The statement should be ignored as a whole? In the example above, when $x<a$, for any $\delta>0$ there are plenty of $x$ which satisfy $0<|x-a|<\delta$. But, is it true that  $|f(x)-L|<\varepsilon$? By which logical means does the definition of limit given still applies when $g$ approaches $a$?","The definition of limit states (Spivak): The function $f$ approaches the limit $l$ near $a$ means : for every   $\varepsilon >0$ there is some $\delta >0$ such that, for all $x$, if   $0< |x-a|<\delta$, then $|f(x)-l|<\varepsilon$. and if its not true that $f$ approaches $l$ near $a$: There is some $\varepsilon>0$ such that for every $\delta>0$ there   is some $x$  which satisfies $0<|x-a|<\delta$ but not   $|f(x)-l|<\varepsilon$. There are some textbooks that provide the definition of limit with the additional condition ""for all $x \in D_{f}\dots$"", but it seems to me that Spivak doesn't find it necessary. So, supposing this condition is neither logically necessary nor implicit, suppose $\lim_{x\to a^{+}}g(x)=L$ for some function $g:[a,b]\to \mathbb{R}$.  In order to verify that actually $\lim_{x\to a}g(x)=L$, we could prove that the negated form of the limit definition above is false for $g$ near $a$. However, when $x<a$, $x$ is outside de domain of $g$, and $f(x)$ is not defined. So, perhaps, it's not the case that we have a vacuously true statement, because testing the veracity of an expression containing $|f(x)-L|$ does not even make logical sense at all. What happens in a logical statement when a free variable takes a particular instance that turns an expression of it into nonsense? The statement should be ignored as a whole? In the example above, when $x<a$, for any $\delta>0$ there are plenty of $x$ which satisfy $0<|x-a|<\delta$. But, is it true that  $|f(x)-L|<\varepsilon$? By which logical means does the definition of limit given still applies when $g$ approaches $a$?",,['calculus']
78,PDE with integral,PDE with integral,,"I came across the following functional equation : $$\partial_t \phi(t,x) = a -(x+a)\phi(t,x)+ x \left[ \int_0^\infty \nu(x,y) \phi(t,y) dy \right]^2 $$ where $\nu(x,y)$ is a log-normal distribution function: $$\nu(x,y)=\frac{1}{\sqrt{2\pi} \sigma y} \exp \left(-\frac{(\log y - \log x)^2}{2\sigma^2} \right)$$ In this, $a$ is positive, $x$ and $t$ are positive, and initial conditions are $\phi(0,x)=\phi_0$ for all $x$, $0 < \phi_0 < 1$. I'm solving this numerically (by discretization of the integral) but I need a lot of values of $x$ and this takes a lot of computing time. I'm wondering if there are some methods to simplify the computation or improve the convergence, or even better if there is a closed form ! I tried to find the limit as $t\rightarrow \infty$ but I cannot solve the integral equation, is there some way to do it ? Edit : I tried to expand in $x$ as suggested below but the series is badly divergent because of $e ^{n^2}$ like terms and does not approximate well the numerical solution. I thought of defining an operator $$A[\phi(x)]=\int_0^\infty \nu(x,y) \phi(t,y) dy$$ and solve the differential equation as an ODE  $$\partial_t \phi = a - B \phi + x[A\phi]^2$$ and then interpret the solution as an analytic series of the operators $B$ and $A^{-1}$. Does this have any chance of succeeding, and if so how can we invert $A$ ? Just to be clear, my main goal is to improve the speed and reliability of the numerical integration of the above equation (the computation is long and I have to cut off the integral at some point). I'll be also happy if some analytic properties of the solution can be found.","I came across the following functional equation : $$\partial_t \phi(t,x) = a -(x+a)\phi(t,x)+ x \left[ \int_0^\infty \nu(x,y) \phi(t,y) dy \right]^2 $$ where $\nu(x,y)$ is a log-normal distribution function: $$\nu(x,y)=\frac{1}{\sqrt{2\pi} \sigma y} \exp \left(-\frac{(\log y - \log x)^2}{2\sigma^2} \right)$$ In this, $a$ is positive, $x$ and $t$ are positive, and initial conditions are $\phi(0,x)=\phi_0$ for all $x$, $0 < \phi_0 < 1$. I'm solving this numerically (by discretization of the integral) but I need a lot of values of $x$ and this takes a lot of computing time. I'm wondering if there are some methods to simplify the computation or improve the convergence, or even better if there is a closed form ! I tried to find the limit as $t\rightarrow \infty$ but I cannot solve the integral equation, is there some way to do it ? Edit : I tried to expand in $x$ as suggested below but the series is badly divergent because of $e ^{n^2}$ like terms and does not approximate well the numerical solution. I thought of defining an operator $$A[\phi(x)]=\int_0^\infty \nu(x,y) \phi(t,y) dy$$ and solve the differential equation as an ODE  $$\partial_t \phi = a - B \phi + x[A\phi]^2$$ and then interpret the solution as an analytic series of the operators $B$ and $A^{-1}$. Does this have any chance of succeeding, and if so how can we invert $A$ ? Just to be clear, my main goal is to improve the speed and reliability of the numerical integration of the above equation (the computation is long and I have to cut off the integral at some point). I'll be also happy if some analytic properties of the solution can be found.",,"['calculus', 'partial-differential-equations', 'numerical-methods', 'integral-equations']"
79,Integral and unit of measurement,Integral and unit of measurement,,"Is there a kind of ""formalism"" which define how unit measures come out from integration? An example: given a point $P(x,y,z) \in \mathbb{R}^3$ there is the concept of mass $m$ associated to this point. Mass is measured in $\text{kg,g,lb,...}$ I indicate the generic unite measure of the mass $[m]$. Now, there is also the concept of density of mass $\rho(x,y,z)$ which is a (scalar) function which represents ""mass per unit volume"", or also $\frac{[m]}{[s]^3}$ (where $[s]$ is the unit measure of space), e.g. if we take a constant density over a volume, to know the mass of the volume it suffices to multiply density $\rho$ with volume $V$. The effect of this multiplication is coherent with unit measures involved. Now, in general for non-constant density function, one needs to integrate the function over the volume to know mass: $m=\int_V \rho(x,y,z)\text{d}\tau$ where $\tau$ is the volume element. Now, integrals are pure mathematical objects, how can I relate the fact that an integral is not only (naively) ""a product of the integrand for the measure of the space of integration"" with the fact that, in the end, there will be $[m]=\frac{[m]}{[s]^3} [s]^3$ Now, naively I can argument something like this $\int_{[s^3]} \frac{[m]}{[s]^3} \text{d}([s^3])$, but the integrand is constant so $\frac{[m]}{[s]^3} \int_{[s^3]} \text{d}([s]^3)=[m]$. But here there is no mathematical formalism, only a naive thought about such an ""integral of unit measures"" sounds to me. Is there actually an ad-hoc formal argument for this problem?","Is there a kind of ""formalism"" which define how unit measures come out from integration? An example: given a point $P(x,y,z) \in \mathbb{R}^3$ there is the concept of mass $m$ associated to this point. Mass is measured in $\text{kg,g,lb,...}$ I indicate the generic unite measure of the mass $[m]$. Now, there is also the concept of density of mass $\rho(x,y,z)$ which is a (scalar) function which represents ""mass per unit volume"", or also $\frac{[m]}{[s]^3}$ (where $[s]$ is the unit measure of space), e.g. if we take a constant density over a volume, to know the mass of the volume it suffices to multiply density $\rho$ with volume $V$. The effect of this multiplication is coherent with unit measures involved. Now, in general for non-constant density function, one needs to integrate the function over the volume to know mass: $m=\int_V \rho(x,y,z)\text{d}\tau$ where $\tau$ is the volume element. Now, integrals are pure mathematical objects, how can I relate the fact that an integral is not only (naively) ""a product of the integrand for the measure of the space of integration"" with the fact that, in the end, there will be $[m]=\frac{[m]}{[s]^3} [s]^3$ Now, naively I can argument something like this $\int_{[s^3]} \frac{[m]}{[s]^3} \text{d}([s^3])$, but the integrand is constant so $\frac{[m]}{[s]^3} \int_{[s^3]} \text{d}([s]^3)=[m]$. But here there is no mathematical formalism, only a naive thought about such an ""integral of unit measures"" sounds to me. Is there actually an ad-hoc formal argument for this problem?",,"['calculus', 'integration', 'measure-theory', 'definite-integrals', 'physics']"
80,"Change of variables for $\iint_D x\sin(y-x^2) \, dA$",Change of variables for,"\iint_D x\sin(y-x^2) \, dA","Let $D$ be the region in the first quadrant of the $x,y$-plane, bounded by the curves $y=x^2$, $y=x^2+1$, $x+y=1$, and $x+y=2$. I want to find a change of variables for $\iint_D x\sin(y-x^2) \, dA$. Since $y-x^2=0$, $y-x^2=1$, my inclination is to let $u=y-x^2$, and since $x+y=1$ and $x+y=2$ I want to let $v=x+y$. However, when solving for $x$ in terms of $u$ and $v$ I get a horrid looking thing. Could someone provide me with a better way of approaching this problem? Thank you!","Let $D$ be the region in the first quadrant of the $x,y$-plane, bounded by the curves $y=x^2$, $y=x^2+1$, $x+y=1$, and $x+y=2$. I want to find a change of variables for $\iint_D x\sin(y-x^2) \, dA$. Since $y-x^2=0$, $y-x^2=1$, my inclination is to let $u=y-x^2$, and since $x+y=1$ and $x+y=2$ I want to let $v=x+y$. However, when solving for $x$ in terms of $u$ and $v$ I get a horrid looking thing. Could someone provide me with a better way of approaching this problem? Thank you!",,"['calculus', 'integration', 'multivariable-calculus', 'multiple-integral', 'change-of-variable']"
81,Why is it legal to consider $\frac{dy}{dx}$ for a parametric curve when y may not be a valid function of x?,Why is it legal to consider  for a parametric curve when y may not be a valid function of x?,\frac{dy}{dx},"When deriving the formula for the derivative of a parametric curve, in the form of $x = x(t)$ and $y = y(t)$, the chain rule is applied to $\frac{dy}{dt}$ to obtain $\frac{dy}{dt} = \frac{dy}{dx}\cdot\frac{dx}{dt}$, from which the slope of $y$ with respect to $x$ can be obtained. My question is: why is it legal to use $\frac{dy}{dx}$ when $y$ is often not a function of $x$? For example, the curve described by the parametric equations $x=6sin(t)$ and $y=t^2+t$ ( image here ) is clearly not a function of $x$, since it fails the vertical line test infinitely many times, and yet $\frac{dy}{dx} = \frac{2t+1}{6cos(t)}$. What is the intuition behind $\frac{dy}{dx}$ in this case? I usually think of $\frac{dy}{dx}$ as the (unique) slope induced from a small change in $x$, but that doesn't make sense here, since a small change in $x$ corresponds to infinitely many changes in $y$.","When deriving the formula for the derivative of a parametric curve, in the form of $x = x(t)$ and $y = y(t)$, the chain rule is applied to $\frac{dy}{dt}$ to obtain $\frac{dy}{dt} = \frac{dy}{dx}\cdot\frac{dx}{dt}$, from which the slope of $y$ with respect to $x$ can be obtained. My question is: why is it legal to use $\frac{dy}{dx}$ when $y$ is often not a function of $x$? For example, the curve described by the parametric equations $x=6sin(t)$ and $y=t^2+t$ ( image here ) is clearly not a function of $x$, since it fails the vertical line test infinitely many times, and yet $\frac{dy}{dx} = \frac{2t+1}{6cos(t)}$. What is the intuition behind $\frac{dy}{dx}$ in this case? I usually think of $\frac{dy}{dx}$ as the (unique) slope induced from a small change in $x$, but that doesn't make sense here, since a small change in $x$ corresponds to infinitely many changes in $y$.",,"['calculus', 'derivatives']"
82,Solve for power series $y'' - 9y = 0$,Solve for power series,y'' - 9y = 0,"I really need help with this, the solution from this equation is $y(x) = c_1 e^{3x} + c_2 e^{-3x}$. But I can't get to it, I obtain the next: $$y(x) = \sum_{n=0}^{\infty}a_nx^n$$ $$y''(x) = \sum_{n=0}^{\infty}n(n-1)a_nx^{n-2}$$ Then the coefficients must be $a_{2m} = \frac{9^m a_0}{(2m)!}$ and $a_{2m+1}= \frac{9^ma_1}{(2m+1)!}$ Substituting in the first equation I have: $$y(x) = a_0 \sum_{m=0}^{\infty} \frac{(3x)^{2m}}{(2m)!} + a_1\sum_{m=0}^{\infty} \frac{3^{2m}x^{2m+1}}{(2m+1)!} $$ Since $e^{x}=\sum_{n=0}^{\infty} \frac{x^{n}}{n!}$, I think its obvious that the first part of the last equation is $a_0e^{3x}$, but in the second part I dont really know how to get $a_1e^{-3}$. I am wrong?","I really need help with this, the solution from this equation is $y(x) = c_1 e^{3x} + c_2 e^{-3x}$. But I can't get to it, I obtain the next: $$y(x) = \sum_{n=0}^{\infty}a_nx^n$$ $$y''(x) = \sum_{n=0}^{\infty}n(n-1)a_nx^{n-2}$$ Then the coefficients must be $a_{2m} = \frac{9^m a_0}{(2m)!}$ and $a_{2m+1}= \frac{9^ma_1}{(2m+1)!}$ Substituting in the first equation I have: $$y(x) = a_0 \sum_{m=0}^{\infty} \frac{(3x)^{2m}}{(2m)!} + a_1\sum_{m=0}^{\infty} \frac{3^{2m}x^{2m+1}}{(2m+1)!} $$ Since $e^{x}=\sum_{n=0}^{\infty} \frac{x^{n}}{n!}$, I think its obvious that the first part of the last equation is $a_0e^{3x}$, but in the second part I dont really know how to get $a_1e^{-3}$. I am wrong?",,"['calculus', 'algebra-precalculus', 'ordinary-differential-equations', 'power-series']"
83,Reconcile two anti-derivatives of $\int \frac{dx}{A + \cos x} $,Reconcile two anti-derivatives of,\int \frac{dx}{A + \cos x} ,"I see in a calculus textbook that, for $|A| \neq 1$ , \begin{align} f(x) &\equiv \int \frac{1}{A + \cos x} \,\text{d}x \\&=\frac{1}{ \sqrt{A^2 - 1} } \left( x - 2 \tan^{-1} \frac{ \sin x }{ A + \sqrt{A^2 - 1} + \cos x } \right) \end{align} which I have verified using Mathematica $$\frac{\text{d}\, f(x)}{\text{d}\, x} = \frac{1}{A + \cos x}$$ However, I have failed to show the equivalence (up to a constant) of $f(x)$ to the more sensible form obtained by Mathematica (as explained here) : $$g(x) \equiv \int \frac{1}{A + \cos x} \,\text{d}x = \frac{-2}{\sqrt{1-A^2}} \tanh ^{-1}\frac{(A-1) \tan \frac{x}{2}}{\sqrt{1-A^2}}$$ With the half angle substitution, one can replace $\tan \frac x2$ in above result. However, what really stumps me is how to get the linear term and arctangent in $f(x)$ . It seems reasonable to try some identities involving inverses like $$\tanh^{-1}(\sin x) = \sinh^{-1}(\tan x) $$ or equivalently $$ \sin^{-1}( \tanh x ) = \tan^{-1}( \sinh x )$$ together with some common ways to combine $\sin x$ and $\cos x$ . But, so far, I have gotten nowhere.","I see in a calculus textbook that, for , which I have verified using Mathematica However, I have failed to show the equivalence (up to a constant) of to the more sensible form obtained by Mathematica (as explained here) : With the half angle substitution, one can replace in above result. However, what really stumps me is how to get the linear term and arctangent in . It seems reasonable to try some identities involving inverses like or equivalently together with some common ways to combine and . But, so far, I have gotten nowhere.","|A| \neq 1 \begin{align}
f(x) &\equiv \int \frac{1}{A + \cos x} \,\text{d}x \\&=\frac{1}{ \sqrt{A^2 - 1} } \left( x - 2 \tan^{-1} \frac{ \sin x }{ A + \sqrt{A^2 - 1} + \cos x } \right)
\end{align} \frac{\text{d}\, f(x)}{\text{d}\, x} = \frac{1}{A + \cos x} f(x) g(x) \equiv \int \frac{1}{A + \cos x} \,\text{d}x = \frac{-2}{\sqrt{1-A^2}} \tanh ^{-1}\frac{(A-1) \tan \frac{x}{2}}{\sqrt{1-A^2}} \tan \frac x2 f(x) \tanh^{-1}(\sin x) = \sinh^{-1}(\tan x)   \sin^{-1}( \tanh x ) = \tan^{-1}( \sinh x ) \sin x \cos x","['calculus', 'integration', 'indefinite-integrals', 'trigonometric-integrals']"
84,Can we use trigonometric idendities to calculate $\cos(x)$ and $\sin(x)$ for extremely large $x$?,Can we use trigonometric idendities to calculate  and  for extremely large ?,\cos(x) \sin(x) x,"If we want to calculate $\sin(x)$ and $\cos(x)$ for very large $x$ , lets say $10^5$ , the usual way is to reduce the number $x$ modulo $2\pi$. If the number is a large power of a small number, for example $2^{200}$, we could also use $$\cos(2x)=\cos^2x-\sin^2x$$ and $$\sin(2x)=2\sin(x)\cos(x)$$ multiple times. Do we have any chance, if the power is too high for both methods ? For example, can we calculate $\sin(x)$ and $\cos(x)$ for $x=10^{10^{10^{10}}}$ ? Note, that the power tower must be calculated from above. So, we have $$x=10^{(10^{(10^{10))}}}$$ In tetration-notation, we can write $x=10\uparrow\uparrow 4$","If we want to calculate $\sin(x)$ and $\cos(x)$ for very large $x$ , lets say $10^5$ , the usual way is to reduce the number $x$ modulo $2\pi$. If the number is a large power of a small number, for example $2^{200}$, we could also use $$\cos(2x)=\cos^2x-\sin^2x$$ and $$\sin(2x)=2\sin(x)\cos(x)$$ multiple times. Do we have any chance, if the power is too high for both methods ? For example, can we calculate $\sin(x)$ and $\cos(x)$ for $x=10^{10^{10^{10}}}$ ? Note, that the power tower must be calculated from above. So, we have $$x=10^{(10^{(10^{10))}}}$$ In tetration-notation, we can write $x=10\uparrow\uparrow 4$",,"['calculus', 'trigonometry', 'tetration']"
85,Number of distinct solutions of $f(f(x))=0$,Number of distinct solutions of,f(f(x))=0,Let $f(x)=x^3-3x+1.$ Then what is the number of different real solutions of the equation $f(f(x))=0$? $f(x)$ has three roots and $f(f(x))$ will be 0 when value of $f(x)$ is equal to its root. But this approach is turning out to be tedious as finding exact values of roots of $f(x)$ is not possible Could someone suggest a better approach?,Let $f(x)=x^3-3x+1.$ Then what is the number of different real solutions of the equation $f(f(x))=0$? $f(x)$ has three roots and $f(f(x))$ will be 0 when value of $f(x)$ is equal to its root. But this approach is turning out to be tedious as finding exact values of roots of $f(x)$ is not possible Could someone suggest a better approach?,,"['calculus', 'derivatives']"
86,Fundamental theorem of calculus when the integrand is just right (or left) continuous,Fundamental theorem of calculus when the integrand is just right (or left) continuous,,"Consider $f:[a,b]\rightarrow R$ and $F(x)=\int_a^xf(t)dt$ for all $x\in[a,b]$. If $f$ is continuous, then the fundamental theorem of calculus says that $F'(x)=f(x)$. I want to ask if $f$ is just right (or left) continuous, is it true that the right (or left) derivative of $F$ equals $f$? i.e. $\partial_+F(x)=f(x)$ (or $\partial_-F(x)=f(x)$)? In addition, if $f$ has right (or left) limit at $x$, is it true that $\partial_+F(x)=\lim_{t\rightarrow x+}f(t)$ (or $\partial_-F(x)=\lim_{t\rightarrow x-}f(t)$)?","Consider $f:[a,b]\rightarrow R$ and $F(x)=\int_a^xf(t)dt$ for all $x\in[a,b]$. If $f$ is continuous, then the fundamental theorem of calculus says that $F'(x)=f(x)$. I want to ask if $f$ is just right (or left) continuous, is it true that the right (or left) derivative of $F$ equals $f$? i.e. $\partial_+F(x)=f(x)$ (or $\partial_-F(x)=f(x)$)? In addition, if $f$ has right (or left) limit at $x$, is it true that $\partial_+F(x)=\lim_{t\rightarrow x+}f(t)$ (or $\partial_-F(x)=\lim_{t\rightarrow x-}f(t)$)?",,"['calculus', 'limits', 'derivatives', 'continuity']"
87,The largest root of a recursively defined polynomial,The largest root of a recursively defined polynomial,,"Suppose that for all $x \in \mathbb{R}$, $f_1(x)=x^2$ and for all $k \in \mathbb{N}$, $$   f_{k+1}(x) = f_k(x) - f_k'(x) x (1-x). $$ Let $\underline{x}_k$ denote the largest root of $f_k(x)=0$. I want to prove the following conjectures: $0=\underline{x}_1 < \underline{x}_2 < \underline{x}_3 < \dots < 1$. $\lim_{n \to \infty} \underline{x}_n = 1$. $f_k'(\underline{x}_k) \geq 0$ for all $k$. These conjectures are the missing part of a larger proof and it seems to hold for any $f_k$ that I can compute by hand or numerically. The first few polynomials: $f_1(x)=x^2$, so that $\underline{x}_1=0$ and $f_1'(\underline{x}_1) = 0$ $f_2(x)=x^2 (2x-1)$, so that $\underline{x}_2=\frac{1}{2} \in (\underline{x}_1,1)$ and $f_2'(1/2)=1/2>0$ $f_3(x)=x^2 (6x^2-6x+1)$, so that $\underline{x}_3 = \frac{1}{2} + \frac{1}{2 \sqrt{3}} \approx 0.7887 \in (\underline{x}_2,1)$ and $f_3'(\underline{x}_3) \approx 2.1547>0$ $f_4(x)=x^2 (2x-1) (12 x^2-12 x+1)$, so that $\underline{x}_4=\frac{1}{2}+\frac{1}{\sqrt{6}} \approx 0.9082 \in (\underline{x}_3,1)$ and $f_4'(\underline{x}_4) \approx 6.5993 > 0$ The polynomials $f_k$ have many properties that should help. For example it is easy to show that $f_k(1)=1$ for all $k$ and $f_k(x) > 1$ for all $x>1$ for all $k$. Therefore all real roots must be strictly below $1$.","Suppose that for all $x \in \mathbb{R}$, $f_1(x)=x^2$ and for all $k \in \mathbb{N}$, $$   f_{k+1}(x) = f_k(x) - f_k'(x) x (1-x). $$ Let $\underline{x}_k$ denote the largest root of $f_k(x)=0$. I want to prove the following conjectures: $0=\underline{x}_1 < \underline{x}_2 < \underline{x}_3 < \dots < 1$. $\lim_{n \to \infty} \underline{x}_n = 1$. $f_k'(\underline{x}_k) \geq 0$ for all $k$. These conjectures are the missing part of a larger proof and it seems to hold for any $f_k$ that I can compute by hand or numerically. The first few polynomials: $f_1(x)=x^2$, so that $\underline{x}_1=0$ and $f_1'(\underline{x}_1) = 0$ $f_2(x)=x^2 (2x-1)$, so that $\underline{x}_2=\frac{1}{2} \in (\underline{x}_1,1)$ and $f_2'(1/2)=1/2>0$ $f_3(x)=x^2 (6x^2-6x+1)$, so that $\underline{x}_3 = \frac{1}{2} + \frac{1}{2 \sqrt{3}} \approx 0.7887 \in (\underline{x}_2,1)$ and $f_3'(\underline{x}_3) \approx 2.1547>0$ $f_4(x)=x^2 (2x-1) (12 x^2-12 x+1)$, so that $\underline{x}_4=\frac{1}{2}+\frac{1}{\sqrt{6}} \approx 0.9082 \in (\underline{x}_3,1)$ and $f_4'(\underline{x}_4) \approx 6.5993 > 0$ The polynomials $f_k$ have many properties that should help. For example it is easy to show that $f_k(1)=1$ for all $k$ and $f_k(x) > 1$ for all $x>1$ for all $k$. Therefore all real roots must be strictly below $1$.",,"['calculus', 'polynomials', 'recursion']"
88,Using Green's Theorem to Express the Integral $I=\int_C (Pdx+Qdy)$ as an expression of $I_i=\int _{C_i} (Pdx+Qdy)$,Using Green's Theorem to Express the Integral  as an expression of,I=\int_C (Pdx+Qdy) I_i=\int _{C_i} (Pdx+Qdy),"Let $p_1,...p_n$ be points in $\mathbb{R}^n$. Let $P(x,y), Q(x,y)$ be functions with continuous derivatives in $ D=\mathbb{R}^2\setminus\{p_1,...p_n\}$ such that $Q_x-P_y=1$ for all $(x,y)\in D$. For all $i$ let $C_i$ be a circle of radius $r_i$ around $p_i$ such that $C_i$ doesn't contain $p_j$ for all $j\neq i$. Let $C$ be a circle of radius $R$ around $(0,0)$, such that $\forall i=1,...n, p_i \in C$. Let $I=\int_C (Pdx+Qdy),  I_i =\int_C(Pdx+Qdy)$. Express $I$ as a function of $I_1,...I_n, r_1,...r_n, R$. I was able to prove this considering that $C_1,...C_n$ are all disjoint and $\cup_{i=1}^nC_i \subseteq C$, but wasn't able to prove this when they were not disjoint. Here's the proof for disjoint $C_1,...C_n$ and $\cup_{i=1}^nC_i \subseteq C$: Let $A$ be $C\setminus \cup_{i=1}^nC_i$. $A$ is a Green region, since it's boundary is $C\cup \cup_{i=1}^nC_i$, and therefore: $I+\sum_{i=1}^nI_i=\int_C(Pdx+Qdy)+\sum_{i=1}^n\int_{C_i}(Pdx+Qdy)=\int_{\partial A}(Pdx+Qdy)=\iint_A(Q_x-P_y)dxdy=\iint_A1dxdy=\mu (A)$ And therefore, $I=\mu(A)-\sum_{i=1}^nI_i$, but since $C_1,..C_n$ are all disjoint and if we write $B$ to be the interior of $C$ and $B_i$ to be the interior of $C_i$, then, we get that $\mu(A)-\mu(B)-\sum_{i=1}^n\mu(B_i)=\pi(R-\sum_{i=1}^nr_i^2)$, such that $I=\pi(R-\sum_{i=1}^nr_i^2)-\sum_{i=1}^nI_i$. In the case that $C,C_1,...C_n$ are not disjoint I looked at the case of only two that intersect. Let's assume that $B_1\cap B_2=E\neq \emptyset$. Since $B_1$ only contains $p_1$ of the set $\{p_1,...p_n\}$ and $B_2$ only contains $p_2$ of $\{p_1,...p_n\}$, then $E$ doesn't contain any of the points $\{p_1,...p_n\}$ and therefore, $P,Q$ and their derivatives are continuous on $E$. Also, if $E$ contains more than one point, then it can be proven that $E$'s boundary is a Jordan curve, and therefore, Green's theorem applies on $E$.  Therefore, I figured out that $\int_{C_1\cup C_2}(Pdx+Qdy)=\int_{C_1}(Pdx+Qdy)+\int_{C_2}(Pdx+Qdy)-\iint_E(Q_x-P_y)dxdy=I_1+I_2-\mu(E)$. From here on, I couldn't find a direction that worked.","Let $p_1,...p_n$ be points in $\mathbb{R}^n$. Let $P(x,y), Q(x,y)$ be functions with continuous derivatives in $ D=\mathbb{R}^2\setminus\{p_1,...p_n\}$ such that $Q_x-P_y=1$ for all $(x,y)\in D$. For all $i$ let $C_i$ be a circle of radius $r_i$ around $p_i$ such that $C_i$ doesn't contain $p_j$ for all $j\neq i$. Let $C$ be a circle of radius $R$ around $(0,0)$, such that $\forall i=1,...n, p_i \in C$. Let $I=\int_C (Pdx+Qdy),  I_i =\int_C(Pdx+Qdy)$. Express $I$ as a function of $I_1,...I_n, r_1,...r_n, R$. I was able to prove this considering that $C_1,...C_n$ are all disjoint and $\cup_{i=1}^nC_i \subseteq C$, but wasn't able to prove this when they were not disjoint. Here's the proof for disjoint $C_1,...C_n$ and $\cup_{i=1}^nC_i \subseteq C$: Let $A$ be $C\setminus \cup_{i=1}^nC_i$. $A$ is a Green region, since it's boundary is $C\cup \cup_{i=1}^nC_i$, and therefore: $I+\sum_{i=1}^nI_i=\int_C(Pdx+Qdy)+\sum_{i=1}^n\int_{C_i}(Pdx+Qdy)=\int_{\partial A}(Pdx+Qdy)=\iint_A(Q_x-P_y)dxdy=\iint_A1dxdy=\mu (A)$ And therefore, $I=\mu(A)-\sum_{i=1}^nI_i$, but since $C_1,..C_n$ are all disjoint and if we write $B$ to be the interior of $C$ and $B_i$ to be the interior of $C_i$, then, we get that $\mu(A)-\mu(B)-\sum_{i=1}^n\mu(B_i)=\pi(R-\sum_{i=1}^nr_i^2)$, such that $I=\pi(R-\sum_{i=1}^nr_i^2)-\sum_{i=1}^nI_i$. In the case that $C,C_1,...C_n$ are not disjoint I looked at the case of only two that intersect. Let's assume that $B_1\cap B_2=E\neq \emptyset$. Since $B_1$ only contains $p_1$ of the set $\{p_1,...p_n\}$ and $B_2$ only contains $p_2$ of $\{p_1,...p_n\}$, then $E$ doesn't contain any of the points $\{p_1,...p_n\}$ and therefore, $P,Q$ and their derivatives are continuous on $E$. Also, if $E$ contains more than one point, then it can be proven that $E$'s boundary is a Jordan curve, and therefore, Green's theorem applies on $E$.  Therefore, I figured out that $\int_{C_1\cup C_2}(Pdx+Qdy)=\int_{C_1}(Pdx+Qdy)+\int_{C_2}(Pdx+Qdy)-\iint_E(Q_x-P_y)dxdy=I_1+I_2-\mu(E)$. From here on, I couldn't find a direction that worked.",,"['calculus', 'integration', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
89,"Let $a,b,c,d$ are non-zero real numbers such that $6a+4b+3c+3d=0$,then the equation $ax^3+bx^2+cx+d=0$ has","Let  are non-zero real numbers such that ,then the equation  has","a,b,c,d 6a+4b+3c+3d=0 ax^3+bx^2+cx+d=0","Let $a,b,c,d$ are non-zero real numbers such that $6a+4b+3c+3d=0$ . Then the equation $ax^3+bx^2+cx+d=0$ has: (A) At least one root in $[-2,0]$ (B) At least one root in $[0,2]$ (C) At least two roots in $[-2,2]$ (D) No root in $[-2,2]$ Let $f(x)=ax^3+bx^2+cx+d$ $f(x)$ has at least one root in [-2,0] if $f(-2)f(0)<0$ : $$(-8a+4b-2c+d)d<0$$ $f(x)$ has at least one root in [0,2] if $f(2)f(0)<0$ : $$(8a+4b+2c+d)d<0$$ $f(x)$ has at least two roots in [-2,2] if $f(2)f(0)>0$ : $$(-8a+4b-2c+d)(8a+4b+2c+d)>0$$ Am I right uptil here? I am stuck from hereon.","Let are non-zero real numbers such that . Then the equation has: (A) At least one root in (B) At least one root in (C) At least two roots in (D) No root in Let has at least one root in [-2,0] if : has at least one root in [0,2] if : has at least two roots in [-2,2] if : Am I right uptil here? I am stuck from hereon.","a,b,c,d 6a+4b+3c+3d=0 ax^3+bx^2+cx+d=0 [-2,0] [0,2] [-2,2] [-2,2] f(x)=ax^3+bx^2+cx+d f(x) f(-2)f(0)<0 (-8a+4b-2c+d)d<0 f(x) f(2)f(0)<0 (8a+4b+2c+d)d<0 f(x) f(2)f(0)>0 (-8a+4b-2c+d)(8a+4b+2c+d)>0",['calculus']
90,"Integral $\int_0^1 \frac{\ln|1-x^a|}{1+x}\, dx$",Integral,"\int_0^1 \frac{\ln|1-x^a|}{1+x}\, dx","Let's consider this integral: $$g(a)=\int_0^1 \frac{\ln|1-x^a|}{1+x}\, dx$$ There is a related integral, which is more widely known. See this question , this question and this paper . $$f(a)=\int_0^1 \frac{\ln(1+x^a)}{1+x}\, dx$$ It has a number of interesting properties, namely: $$f(a)-f(-a)=-\frac{\pi^2}{12} a$$ $$f(a)+f \left( \frac{1}{a} \right)=\ln^2 2$$ We can immediately see (by evaluating the integral as well): $$f(0)=\ln^2 2$$ $$f(1)=\frac{\ln^2 2}{2}$$ In the related paper the way is shown to get a closed form for infinitely many values of $a$. Now we consider $g(a)$. Notice the important limit: $$\lim_{a \to \pm 0} g(a)=-\infty$$ In this answer it is shown that: $$g(1)=\int_0^1 \frac{\ln(1-x)}{1+x}\, dx=\frac{\ln^2 2}{2}-\frac{\pi^2}{12}$$ I have not been able to find references about the general integral yet. However, the most interesting property is this obvious formula: $$g(2a)=g(a)+f(a)$$ Using it, we can find some values without explicitly evaluating the integral. $$g(2)=g(1)+f(1)=\ln^2 2-\frac{\pi^2}{12}$$ This integral also has the same symmetry relation as $f(a)$: $$g(a)-g(-a)=-\frac{\pi^2}{12} a$$ However, I have not been able to find any formula for $g(1/a)$ (the proof used for $f(a)$ fails because of the singularity at $a=0$). Here is a plot of these two integrals and their common asymptote: And here is a list of 'nice' values for $f(a)$ and $g(a)$ for integer $a$: $$         \begin{matrix}         a & f(a) & g(a) \\         -4 &   & \dfrac{11 \pi ^2}{48}+\dfrac{21 \ln ^2 2}{12} \\         -2 & \dfrac{7 \pi ^2}{48}+\dfrac{3 \ln ^2 2}{4} & \dfrac{\pi ^2}{12}+\ln ^2 2  \\         -1 & \dfrac{ \pi ^2}{12}+\dfrac{\ln ^2 2}{2} & -\dfrac{\ln ^2 2}{2} \\         -\dfrac{1}{2} & \dfrac{\pi ^2}{16}+\dfrac{\ln ^2 2}{4} & -\dfrac{ \pi ^2}{16}+\dfrac{\ln ^2 2}{4} \\         0 & \ln ^2 2 & -\infty \\         \dfrac{1}{2} & \dfrac{\pi ^2}{48}+\dfrac{\ln ^2 2}{4} & -\dfrac{5 \pi ^2}{48}+\dfrac{\ln ^2 2}{4} \\         1 & \dfrac{\ln ^2 2}{2} & -\dfrac{ \pi ^2}{12}+\dfrac{\ln ^2 2}{2} \\         2 & -\dfrac{\pi ^2}{48}+\dfrac{3 \ln ^2 2}{4} & -\dfrac{ \pi ^2}{12}+\ln ^2 2 \\         4 &   & -\dfrac{5 \pi ^2}{48}+\dfrac{21 \ln ^2 2}{12} \\         \end{matrix} $$ I was able to find most of these values using the highlighted relations, only checking with Mathematica after the fact. Can we find more closed form expressions for $g(a)$ for some $a$, maybe using the values for $f(a)$ (see the linked paper)? What other properties of $g(a)$ can we obtain? And what is the one real solution of $g(a)=0$ (see the plot)? Also for any $a>0$ we can define $g(a)$ in terms of $f$: $$g(a)=g(2a)-f(a)=g(4a)-f(a)-f(2a)=g(8a)-f(a)-f(2a)-f(4a)$$ $$\lim_{a \to + \infty} g(a)=\lim_{a \to + \infty} f(a)=0$$ $$g(a)=- \sum_{k=0}^{\infty} f(2^k~a), ~~~~~ a>0$$ The last relation also directly follows from the well-known identity: $$\prod_{k=0}^{\infty} \left( 1+x^{2^k} \right)=\frac{1}{1-x}, ~~~~~|x|<1$$ We can also use $f(a)$ and $g(a)$ to define a whole family of integrals: $$\sum_{k=0}^{n-1} x^{ka}=\frac{1-x^{na}}{1-x^a}$$ $$I_n (a)=\int_0^1 \ln \left( \sum_{k=0}^{n-1} x^{ka} \right) \frac{dx}{1+x}=g(na)-g(a)$$ $$J_n (a)=\int_0^1 \ln \left( \sum_{k=0}^{n-1} (-1)^k x^{ka} \right) \frac{dx}{1+x}= \begin{cases}g(na)-f(a) & n=2j \\ f(na)-f(a) & n=2j+1 \end{cases} $$ For example: $$\int_0^1 \frac{\ln(1+x+x^2+x^3)}{1+x}\, dx=g(4)-g(1)=-\dfrac{\pi ^2}{48}+\dfrac{15 \ln ^2 2}{12}$$","Let's consider this integral: $$g(a)=\int_0^1 \frac{\ln|1-x^a|}{1+x}\, dx$$ There is a related integral, which is more widely known. See this question , this question and this paper . $$f(a)=\int_0^1 \frac{\ln(1+x^a)}{1+x}\, dx$$ It has a number of interesting properties, namely: $$f(a)-f(-a)=-\frac{\pi^2}{12} a$$ $$f(a)+f \left( \frac{1}{a} \right)=\ln^2 2$$ We can immediately see (by evaluating the integral as well): $$f(0)=\ln^2 2$$ $$f(1)=\frac{\ln^2 2}{2}$$ In the related paper the way is shown to get a closed form for infinitely many values of $a$. Now we consider $g(a)$. Notice the important limit: $$\lim_{a \to \pm 0} g(a)=-\infty$$ In this answer it is shown that: $$g(1)=\int_0^1 \frac{\ln(1-x)}{1+x}\, dx=\frac{\ln^2 2}{2}-\frac{\pi^2}{12}$$ I have not been able to find references about the general integral yet. However, the most interesting property is this obvious formula: $$g(2a)=g(a)+f(a)$$ Using it, we can find some values without explicitly evaluating the integral. $$g(2)=g(1)+f(1)=\ln^2 2-\frac{\pi^2}{12}$$ This integral also has the same symmetry relation as $f(a)$: $$g(a)-g(-a)=-\frac{\pi^2}{12} a$$ However, I have not been able to find any formula for $g(1/a)$ (the proof used for $f(a)$ fails because of the singularity at $a=0$). Here is a plot of these two integrals and their common asymptote: And here is a list of 'nice' values for $f(a)$ and $g(a)$ for integer $a$: $$         \begin{matrix}         a & f(a) & g(a) \\         -4 &   & \dfrac{11 \pi ^2}{48}+\dfrac{21 \ln ^2 2}{12} \\         -2 & \dfrac{7 \pi ^2}{48}+\dfrac{3 \ln ^2 2}{4} & \dfrac{\pi ^2}{12}+\ln ^2 2  \\         -1 & \dfrac{ \pi ^2}{12}+\dfrac{\ln ^2 2}{2} & -\dfrac{\ln ^2 2}{2} \\         -\dfrac{1}{2} & \dfrac{\pi ^2}{16}+\dfrac{\ln ^2 2}{4} & -\dfrac{ \pi ^2}{16}+\dfrac{\ln ^2 2}{4} \\         0 & \ln ^2 2 & -\infty \\         \dfrac{1}{2} & \dfrac{\pi ^2}{48}+\dfrac{\ln ^2 2}{4} & -\dfrac{5 \pi ^2}{48}+\dfrac{\ln ^2 2}{4} \\         1 & \dfrac{\ln ^2 2}{2} & -\dfrac{ \pi ^2}{12}+\dfrac{\ln ^2 2}{2} \\         2 & -\dfrac{\pi ^2}{48}+\dfrac{3 \ln ^2 2}{4} & -\dfrac{ \pi ^2}{12}+\ln ^2 2 \\         4 &   & -\dfrac{5 \pi ^2}{48}+\dfrac{21 \ln ^2 2}{12} \\         \end{matrix} $$ I was able to find most of these values using the highlighted relations, only checking with Mathematica after the fact. Can we find more closed form expressions for $g(a)$ for some $a$, maybe using the values for $f(a)$ (see the linked paper)? What other properties of $g(a)$ can we obtain? And what is the one real solution of $g(a)=0$ (see the plot)? Also for any $a>0$ we can define $g(a)$ in terms of $f$: $$g(a)=g(2a)-f(a)=g(4a)-f(a)-f(2a)=g(8a)-f(a)-f(2a)-f(4a)$$ $$\lim_{a \to + \infty} g(a)=\lim_{a \to + \infty} f(a)=0$$ $$g(a)=- \sum_{k=0}^{\infty} f(2^k~a), ~~~~~ a>0$$ The last relation also directly follows from the well-known identity: $$\prod_{k=0}^{\infty} \left( 1+x^{2^k} \right)=\frac{1}{1-x}, ~~~~~|x|<1$$ We can also use $f(a)$ and $g(a)$ to define a whole family of integrals: $$\sum_{k=0}^{n-1} x^{ka}=\frac{1-x^{na}}{1-x^a}$$ $$I_n (a)=\int_0^1 \ln \left( \sum_{k=0}^{n-1} x^{ka} \right) \frac{dx}{1+x}=g(na)-g(a)$$ $$J_n (a)=\int_0^1 \ln \left( \sum_{k=0}^{n-1} (-1)^k x^{ka} \right) \frac{dx}{1+x}= \begin{cases}g(na)-f(a) & n=2j \\ f(na)-f(a) & n=2j+1 \end{cases} $$ For example: $$\int_0^1 \frac{\ln(1+x+x^2+x^3)}{1+x}\, dx=g(4)-g(1)=-\dfrac{\pi ^2}{48}+\dfrac{15 \ln ^2 2}{12}$$",,"['calculus', 'reference-request', 'definite-integrals']"
91,Focus of the Parabola,Focus of the Parabola,,"Find the Focus of $$(2x+y-1)^2=5(x-2y-3)$$. Clearly its a Parabola whose axis is $2x+y-1=0$ and since $x-2y-3=0$ is perpendicular to $2x+y-1=0$ Tangent at the vertex is $x-2y-3=0$.Also the Vertex is $(3,-1)$, but now how to find its focus?","Find the Focus of $$(2x+y-1)^2=5(x-2y-3)$$. Clearly its a Parabola whose axis is $2x+y-1=0$ and since $x-2y-3=0$ is perpendicular to $2x+y-1=0$ Tangent at the vertex is $x-2y-3=0$.Also the Vertex is $(3,-1)$, but now how to find its focus?",,"['calculus', 'geometry', 'analytic-geometry']"
92,Solve the Integral $\int \:\frac{3x+1}{\left(x^2-x-6\right)\sqrt{3x^2+4x+7}}dx$,Solve the Integral,\int \:\frac{3x+1}{\left(x^2-x-6\right)\sqrt{3x^2+4x+7}}dx,"$$\int \:\frac{3x+1}{\left(x^2-x-6\right)\sqrt{3x^2+4x+7}}dx$$ Can someone tell me what kind of substitution would work here and if this type of integral belongs to a certain group, that can be solved with a certain type of substitution, also a link to that type would be greatly appreciated. P.S. There is no need to solve the problem for me, getting the right substitution and some explanation behind it is really plenty, thanks in advance.","Can someone tell me what kind of substitution would work here and if this type of integral belongs to a certain group, that can be solved with a certain type of substitution, also a link to that type would be greatly appreciated. P.S. There is no need to solve the problem for me, getting the right substitution and some explanation behind it is really plenty, thanks in advance.",\int \:\frac{3x+1}{\left(x^2-x-6\right)\sqrt{3x^2+4x+7}}dx,"['calculus', 'integration']"
93,Integral $\int \frac{1}{\left(x-2\right)^3\sqrt{3x^2-8x+5}}dx$,Integral,\int \frac{1}{\left(x-2\right)^3\sqrt{3x^2-8x+5}}dx,"How to solve the integral $$\int \frac{1}{\left(x-2\right)^3\sqrt{3x^2-8x+5}}dx$$ I am pretty sure that a certain substitution should work. I tried using $x-2=\frac{1}{t}$ and got $$\int \:\frac{-\frac{dt}{t^2}}{\frac{1}{t^3}\sqrt{3\left(\frac{1}{t}+2\right)^2-8\left(\frac{1}{t}+2\right)^2+5}}$$ After some simplification: $$\int \:\frac{tdt}{\sqrt{3\left(\frac{1}{t}+2\right)^2-8\left(\frac{1}{t}+2\right)^2+5}}$$ Any this point, I'm lost.","How to solve the integral I am pretty sure that a certain substitution should work. I tried using and got After some simplification: Any this point, I'm lost.",\int \frac{1}{\left(x-2\right)^3\sqrt{3x^2-8x+5}}dx x-2=\frac{1}{t} \int \:\frac{-\frac{dt}{t^2}}{\frac{1}{t^3}\sqrt{3\left(\frac{1}{t}+2\right)^2-8\left(\frac{1}{t}+2\right)^2+5}} \int \:\frac{tdt}{\sqrt{3\left(\frac{1}{t}+2\right)^2-8\left(\frac{1}{t}+2\right)^2+5}},"['calculus', 'integration', 'indefinite-integrals']"
94,Is there a proof for L'Hôpital's Rule for limits approaching infinity?,Is there a proof for L'Hôpital's Rule for limits approaching infinity?,,"L'Hopital's Rule states that: For two differentiable functions $f$ and $g$, where $g'(x)\neq 0$, such that $$\lim_{x\to a} f(x)=0$$ $$\lim_{x\to a} g(x)=0$$ We can say that: $$\lim_{x\to a} {f(x)\over g(x)}= \lim_{x\to a} {f'(x)\over g'(x)}\\ $$ NOTE: If you already know the proof and don't want to read all this, skip all the way down to $\blacksquare_{1.1}$ PROOF 1.1: Let $f$ and $g$ be continuous functions on $[a,b]$ and differentiable on $(a,b)$. Also, assume $g'(x)\neq 0$ on $(a,b)$ and $g(b)\neq g(a).\\$ Proposition 1.1.1: There exists some point $c$ within the open interval $(a,b)$ such that: ${f'(c)\over g'(c)} = {{f(b) - f(a)}\over {g(b) - g(a)}}$ Proof 1.1.1 $\quad \triangleright$ Let $$\space h(x)= f(x)-f(a) - {{f(b) - f(a)}\over {g(b) - g(a)}}\cdot (g(x)-g (a))\\$$ By simply substituting values we can clearly see that $h(a)=h(b)=0$.  Now because $f(a)$, $f(b)$, $g(a)$ and $g(b)$ are constants, we can therefore say that much like $f$ and $g$,  $h$ is also continuous on $[a,b]$ and differentiable on $(a,b).\\$ If we differentiate  $h$ w.r.t. $x$, we get the following: $$h'(x) = f'(x) - g'(x)\cdot {{f(b) - f(a)}\over {g(b) - g(a)}}\\$$ Using Rolle's Theorem (which I porpusely won't prove as the question is long enough as it is) we can say that there exists a $c$ in $(a,b)$ such that $h'(c)=0$ Thus we can say that $0 = f'(c) - g'(c)\cdot {{f(b) - f(a)}\over {g(b) - g(a)}} $ Hence showing us that ${f'(c)\over g'(c)} = {{f(b) - f(a)}\over {g(b) - g(a)}}$ $$\blacksquare_{1.1.1}$$ $\triangleleft \\$ Remember that $\lim_{x\to a} f(x)= \lim_{x\to a} g(x)=0$. Where $a$ is finite. We also said $g(x)\neq 0$. Therefore we'll Let $L:= \lim_{x\to a} {f'(x)\over g'(x)} \\$ We're also going to define the functions $F$ and $G$. $F(x) = f(x) \Longrightarrow x\neq a$ $F(x) = 0 \Longrightarrow x = a$ Similarly $G(x) = g(x) \Longrightarrow x\neq a$ $G(x) = 0 \Longrightarrow x = a$ Because $F$ and $G$ are defined at $x=a$, they are continuous at $a$. (Unlike $f$ and $g$) This means that for $x>a$, the functions  $F$ and $G$ are differentiable on the open interval  $(a,x)$ and continuous on the closed interval $[a,x]. \\$ Using what we showed in Proof 1.1.1 ,  we can state the following equality to be true: $${F'(c)\over G'(c)} = {{F(x) - F(a)}\over {G(x) - G(a)}}$$ Due to the fact that $F(a)=0$ and $G(a)=0$, we can thus say $${F'(c)\over G'(c)} = {{F(x)}\over {G(x)}}$$ Now since $c$ is within the interval $(a,x)$, we can say $a<c<x$. This means that $x\rightarrow {a^+} \Longrightarrow c\rightarrow {a^+}$ Therefore because $F$ and $G$ are simply the functions $f$ and $g$ respectively where $x=a$ is defined. It's correct for us to then say $$\lim_{x\to a^+} {f(x)\over g(x)} = \lim_{x\to a^+} {F(x)\over G(x)} = \lim_{c\to a^+} {F'(c)\over G'(c)} $$ Also $$\lim_{c\to a^+} {F'(c)\over G'(c)} = \lim_{c\to a^+} {f'(c)\over g'(c)}\\$$ If we notice... $$L:=\lim_{c\to a^+} {f'(c)\over g'(c)}= \lim_{x\to a^+} {f'(x)\over g'(x)}$$ Hence showing us that $$\lim_{x\to a} {f(x)\over g(x)}= \lim_{x\to a} {f'(x)\over g'(x)} $$ $$\blacksquare_{1.1}\\$$ Isn't this only true when $\lim_{x\to a} {f(x)\over g(x)}={0\over 0}$? How would you prove $\lim_{x\to a} {f(x)\over g(x)}= \lim_{x\to a} {f'(x)\over g'(x)} $ for limits such as: $\lim_{x\to a} {f(x)\over g(x)}={\pm\infty\over \pm\infty}$?","L'Hopital's Rule states that: For two differentiable functions $f$ and $g$, where $g'(x)\neq 0$, such that $$\lim_{x\to a} f(x)=0$$ $$\lim_{x\to a} g(x)=0$$ We can say that: $$\lim_{x\to a} {f(x)\over g(x)}= \lim_{x\to a} {f'(x)\over g'(x)}\\ $$ NOTE: If you already know the proof and don't want to read all this, skip all the way down to $\blacksquare_{1.1}$ PROOF 1.1: Let $f$ and $g$ be continuous functions on $[a,b]$ and differentiable on $(a,b)$. Also, assume $g'(x)\neq 0$ on $(a,b)$ and $g(b)\neq g(a).\\$ Proposition 1.1.1: There exists some point $c$ within the open interval $(a,b)$ such that: ${f'(c)\over g'(c)} = {{f(b) - f(a)}\over {g(b) - g(a)}}$ Proof 1.1.1 $\quad \triangleright$ Let $$\space h(x)= f(x)-f(a) - {{f(b) - f(a)}\over {g(b) - g(a)}}\cdot (g(x)-g (a))\\$$ By simply substituting values we can clearly see that $h(a)=h(b)=0$.  Now because $f(a)$, $f(b)$, $g(a)$ and $g(b)$ are constants, we can therefore say that much like $f$ and $g$,  $h$ is also continuous on $[a,b]$ and differentiable on $(a,b).\\$ If we differentiate  $h$ w.r.t. $x$, we get the following: $$h'(x) = f'(x) - g'(x)\cdot {{f(b) - f(a)}\over {g(b) - g(a)}}\\$$ Using Rolle's Theorem (which I porpusely won't prove as the question is long enough as it is) we can say that there exists a $c$ in $(a,b)$ such that $h'(c)=0$ Thus we can say that $0 = f'(c) - g'(c)\cdot {{f(b) - f(a)}\over {g(b) - g(a)}} $ Hence showing us that ${f'(c)\over g'(c)} = {{f(b) - f(a)}\over {g(b) - g(a)}}$ $$\blacksquare_{1.1.1}$$ $\triangleleft \\$ Remember that $\lim_{x\to a} f(x)= \lim_{x\to a} g(x)=0$. Where $a$ is finite. We also said $g(x)\neq 0$. Therefore we'll Let $L:= \lim_{x\to a} {f'(x)\over g'(x)} \\$ We're also going to define the functions $F$ and $G$. $F(x) = f(x) \Longrightarrow x\neq a$ $F(x) = 0 \Longrightarrow x = a$ Similarly $G(x) = g(x) \Longrightarrow x\neq a$ $G(x) = 0 \Longrightarrow x = a$ Because $F$ and $G$ are defined at $x=a$, they are continuous at $a$. (Unlike $f$ and $g$) This means that for $x>a$, the functions  $F$ and $G$ are differentiable on the open interval  $(a,x)$ and continuous on the closed interval $[a,x]. \\$ Using what we showed in Proof 1.1.1 ,  we can state the following equality to be true: $${F'(c)\over G'(c)} = {{F(x) - F(a)}\over {G(x) - G(a)}}$$ Due to the fact that $F(a)=0$ and $G(a)=0$, we can thus say $${F'(c)\over G'(c)} = {{F(x)}\over {G(x)}}$$ Now since $c$ is within the interval $(a,x)$, we can say $a<c<x$. This means that $x\rightarrow {a^+} \Longrightarrow c\rightarrow {a^+}$ Therefore because $F$ and $G$ are simply the functions $f$ and $g$ respectively where $x=a$ is defined. It's correct for us to then say $$\lim_{x\to a^+} {f(x)\over g(x)} = \lim_{x\to a^+} {F(x)\over G(x)} = \lim_{c\to a^+} {F'(c)\over G'(c)} $$ Also $$\lim_{c\to a^+} {F'(c)\over G'(c)} = \lim_{c\to a^+} {f'(c)\over g'(c)}\\$$ If we notice... $$L:=\lim_{c\to a^+} {f'(c)\over g'(c)}= \lim_{x\to a^+} {f'(x)\over g'(x)}$$ Hence showing us that $$\lim_{x\to a} {f(x)\over g(x)}= \lim_{x\to a} {f'(x)\over g'(x)} $$ $$\blacksquare_{1.1}\\$$ Isn't this only true when $\lim_{x\to a} {f(x)\over g(x)}={0\over 0}$? How would you prove $\lim_{x\to a} {f(x)\over g(x)}= \lim_{x\to a} {f'(x)\over g'(x)} $ for limits such as: $\lim_{x\to a} {f(x)\over g(x)}={\pm\infty\over \pm\infty}$?",,"['calculus', 'limits']"
95,Derivation of $\tanh$ solution to $\frac{1}{2}f''=f^3 - f$,Derivation of  solution to,\tanh \frac{1}{2}f''=f^3 - f,"I am a mechanical engineering student, and I am trying to solve the following ODE: $$\frac{1}{2}f''=f^3 - f$$ where $f=f(x)$ and the boundary conditions are $f(0)=0$ and $f'(\infty)=0$. On the Wolfram Mathworld page for the hyperbolic tangent, it is remarked that the solution to this ODE is given by $f=\tanh(x)$. This can easily be verified by substitution, but I am looking for a step-by-step procedure to get to the solution. I have tried using the substitution $g=f'$, which yields $$ g' = \frac{dg}{dx}=\frac{dg}{df} \frac{df}{dx}=\frac{dg}{df}f'=\frac{dg}{df}g$$ Using $f''=g'$, the substitution of the expresion above in the ODE leads to $$\frac{1}{2} g dg = (f^3-f) df$$ which can be integrated to yield $$\frac{1}{4} (f')^2 = \frac{1}{4}f^4-\frac{1}{2}f^2 + C$$ At this point, I am stuck and I do not know how to proceed. Any help would be greatly appreciated! Best regards, Nick Update 1: Thanks to Mattos' answer, I realised that I can write the final expression as (I already use here $C=0$, although I am not completely sure if that is allowed already): $$\frac{df}{f\sqrt{f^2-2}} = dx$$ Using the following expression I found on sosmath (using different symbols to avoid confusion): $$\int \frac{dh}{h \sqrt{h^2-a^2}}=\frac{1}{a}\sec^{-1}\left|\frac{h}{a}\right|$$ we can integrate to find: $$\frac{1}{\sqrt{2}} \sec^{-1}\left|\frac{f}{\sqrt{2}}\right|=x$$ which is definitely a lot closer to the answer I am looking for. Any help to go from here to $\tanh$ is appreciated. I will make sure to post the answer if I figure it out. Thanks! Update 2: I am not sure if the above approach will lead to the correct answer. However, an extensive derivation is given in the answers by LutzL. Thanks for the help!","I am a mechanical engineering student, and I am trying to solve the following ODE: $$\frac{1}{2}f''=f^3 - f$$ where $f=f(x)$ and the boundary conditions are $f(0)=0$ and $f'(\infty)=0$. On the Wolfram Mathworld page for the hyperbolic tangent, it is remarked that the solution to this ODE is given by $f=\tanh(x)$. This can easily be verified by substitution, but I am looking for a step-by-step procedure to get to the solution. I have tried using the substitution $g=f'$, which yields $$ g' = \frac{dg}{dx}=\frac{dg}{df} \frac{df}{dx}=\frac{dg}{df}f'=\frac{dg}{df}g$$ Using $f''=g'$, the substitution of the expresion above in the ODE leads to $$\frac{1}{2} g dg = (f^3-f) df$$ which can be integrated to yield $$\frac{1}{4} (f')^2 = \frac{1}{4}f^4-\frac{1}{2}f^2 + C$$ At this point, I am stuck and I do not know how to proceed. Any help would be greatly appreciated! Best regards, Nick Update 1: Thanks to Mattos' answer, I realised that I can write the final expression as (I already use here $C=0$, although I am not completely sure if that is allowed already): $$\frac{df}{f\sqrt{f^2-2}} = dx$$ Using the following expression I found on sosmath (using different symbols to avoid confusion): $$\int \frac{dh}{h \sqrt{h^2-a^2}}=\frac{1}{a}\sec^{-1}\left|\frac{h}{a}\right|$$ we can integrate to find: $$\frac{1}{\sqrt{2}} \sec^{-1}\left|\frac{f}{\sqrt{2}}\right|=x$$ which is definitely a lot closer to the answer I am looking for. Any help to go from here to $\tanh$ is appreciated. I will make sure to post the answer if I figure it out. Thanks! Update 2: I am not sure if the above approach will lead to the correct answer. However, an extensive derivation is given in the answers by LutzL. Thanks for the help!",,"['calculus', 'ordinary-differential-equations', 'hyperbolic-functions']"
96,Can I take Linear Algebra without having learned Vector Calculus?,Can I take Linear Algebra without having learned Vector Calculus?,,"I need to take Linear Algebra to progress in my major and the course has Calculus III listed as a prerequisite. I've already taken Calculus III and passed with a C, although I didn't learn the smallest bit of it the entire semester (not a good semester for me). Should I reteach myself Calculus III and hold off on taking Linear Algebra until I do, or would that be a waste of time and should I just go ahead and take the Linear Algebra course? I really don't want to waste time teaching myself something I already took...but if I really need the knowledge, I might.","I need to take Linear Algebra to progress in my major and the course has Calculus III listed as a prerequisite. I've already taken Calculus III and passed with a C, although I didn't learn the smallest bit of it the entire semester (not a good semester for me). Should I reteach myself Calculus III and hold off on taking Linear Algebra until I do, or would that be a waste of time and should I just go ahead and take the Linear Algebra course? I really don't want to waste time teaching myself something I already took...but if I really need the knowledge, I might.",,"['calculus', 'linear-algebra', 'soft-question']"
97,Interval for area bounded by $r = 1 + 3 \sin \theta$,Interval for area bounded by,r = 1 + 3 \sin \theta,"I'm trying to calculate the area of the region bounded by one loop of the graph for the equation $$ r = 1 + 3 \sin \theta $$ I first plot the graph as a limaçon with a maximum outer loop at $(4, \frac{\pi}{2})$ and a minimum inner loop at $(-2, -\frac{3 \pi}{2})$. I then note the graph is symmetric with respect to the $\frac{\pi}{2}$ axis and the zero for the right half is at $\theta = \arcsin(-\frac{1}{3})$. So, I chose the interval $[\arcsin(-\frac{1}{3}),\frac{\pi}{2}]$ to calculate the area which can then be multiplied by $2$ for the other half. The problem is that the answer in the book seems to use $\arcsin(\frac{1}{3})$ instead, note the change of sign. Just to make sure I'm not misunderstanding where I went wrong, I get the answer $$ \frac{11 \pi}{4} - \frac{11}{2} \arcsin(-\frac{1}{3}) + 3 \sqrt 2 $$ Whereas the book gets $$ \frac{11 \pi}{4} - \frac{11}{2} \arcsin(\frac{1}{3}) - 3 \sqrt 2 $$ It's a subtle change of sign but I'd really like to understand where I went wrong.","I'm trying to calculate the area of the region bounded by one loop of the graph for the equation $$ r = 1 + 3 \sin \theta $$ I first plot the graph as a limaçon with a maximum outer loop at $(4, \frac{\pi}{2})$ and a minimum inner loop at $(-2, -\frac{3 \pi}{2})$. I then note the graph is symmetric with respect to the $\frac{\pi}{2}$ axis and the zero for the right half is at $\theta = \arcsin(-\frac{1}{3})$. So, I chose the interval $[\arcsin(-\frac{1}{3}),\frac{\pi}{2}]$ to calculate the area which can then be multiplied by $2$ for the other half. The problem is that the answer in the book seems to use $\arcsin(\frac{1}{3})$ instead, note the change of sign. Just to make sure I'm not misunderstanding where I went wrong, I get the answer $$ \frac{11 \pi}{4} - \frac{11}{2} \arcsin(-\frac{1}{3}) + 3 \sqrt 2 $$ Whereas the book gets $$ \frac{11 \pi}{4} - \frac{11}{2} \arcsin(\frac{1}{3}) - 3 \sqrt 2 $$ It's a subtle change of sign but I'd really like to understand where I went wrong.",,"['calculus', 'polar-coordinates']"
98,Evaluate the integral $\int x^{\frac{-4}{3}}(-x^{\frac{2}{3}}+1)^{\frac{1}{2}}\mathrm dx$,Evaluate the integral,\int x^{\frac{-4}{3}}(-x^{\frac{2}{3}}+1)^{\frac{1}{2}}\mathrm dx,$$x^{\frac{-4}{3}}(-x^{\frac{2}{3}}+1)^{\frac{1}{2}}=\frac{\sqrt{(-\sqrt[3]{x^2}+1)}}{\sqrt[3]{x^4}}$$ Is it necessary to simplify the function further? What substitution is useful? $u=\sqrt[n]{\frac{ax+b}{cx+d}}$ doesn't work.,$$x^{\frac{-4}{3}}(-x^{\frac{2}{3}}+1)^{\frac{1}{2}}=\frac{\sqrt{(-\sqrt[3]{x^2}+1)}}{\sqrt[3]{x^4}}$$ Is it necessary to simplify the function further? What substitution is useful? $u=\sqrt[n]{\frac{ax+b}{cx+d}}$ doesn't work.,,"['calculus', 'integration']"
99,For what values of $k$ does $(1+x)^{500+k}(1-x)^{500-k}$ exceed $10^9$?,For what values of  does  exceed ?,k (1+x)^{500+k}(1-x)^{500-k} 10^9,"Pretty simple question, for what values of $0\leq k \leq 500$ do we have $\max\{(1+x)^{500+k}(1-x)^{500-k}|x\in[0,1]\} \geq 10^9$ ? Some trivial observations: The problem is equivalent to finding the smallest $k$ so that $\max\{(1+x)^{500+k}(1-x)^{500-k}|x\in[0,1]\} < 10^9$ Clearly it is equal to $(1-x^{1000-2k})(1+x)^{2k}$ so we must have $(1+x)^{2k}\geq10^9$, since $1+x\leq 2$ this implies $2k\geq 30\implies k\geq 15$. Of course, this is probably useless. Edit: Using the fact that maximum is reached at $\frac{k}{500}$ we can rephrase as: $(500+k)^{500+k}(500-k)^{500-k}>500^{1000}10^9$ The following java problem solves this nicely in well under a second. import java.math.*; public class eulerbla {     public static void main(String[] args) {         BigInteger potdiez = new BigInteger (""1000000000"");         BigInteger quinmas = new BigInteger(""500"");         BigInteger quinmen = new BigInteger(""500"");         BigInteger num;         BigInteger bound = quinmen.pow(1000);         bound= bound.multiply(potdiez);         for(int k=0;k<=500;k++){             num = (quinmas.pow(500+k));             num = num.multiply(quinmen.pow(500-k));             if(num.compareTo(bound)>=0){                 System.out.println(k);             }             quinmas=quinmas.add(BigInteger.ONE);             quinmen=quinmen.subtract(BigInteger.ONE);         }      }  } Output:102","Pretty simple question, for what values of $0\leq k \leq 500$ do we have $\max\{(1+x)^{500+k}(1-x)^{500-k}|x\in[0,1]\} \geq 10^9$ ? Some trivial observations: The problem is equivalent to finding the smallest $k$ so that $\max\{(1+x)^{500+k}(1-x)^{500-k}|x\in[0,1]\} < 10^9$ Clearly it is equal to $(1-x^{1000-2k})(1+x)^{2k}$ so we must have $(1+x)^{2k}\geq10^9$, since $1+x\leq 2$ this implies $2k\geq 30\implies k\geq 15$. Of course, this is probably useless. Edit: Using the fact that maximum is reached at $\frac{k}{500}$ we can rephrase as: $(500+k)^{500+k}(500-k)^{500-k}>500^{1000}10^9$ The following java problem solves this nicely in well under a second. import java.math.*; public class eulerbla {     public static void main(String[] args) {         BigInteger potdiez = new BigInteger (""1000000000"");         BigInteger quinmas = new BigInteger(""500"");         BigInteger quinmen = new BigInteger(""500"");         BigInteger num;         BigInteger bound = quinmen.pow(1000);         bound= bound.multiply(potdiez);         for(int k=0;k<=500;k++){             num = (quinmas.pow(500+k));             num = num.multiply(quinmen.pow(500-k));             if(num.compareTo(bound)>=0){                 System.out.println(k);             }             quinmas=quinmas.add(BigInteger.ONE);             quinmen=quinmen.subtract(BigInteger.ONE);         }      }  } Output:102",,"['calculus', 'inequality', 'computational-mathematics']"
