,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,if $f'''(x)$ is continuous everywhere and $\lim_{x \to 0}(1+x+ \frac{f(x)}{x})^{1/x}=e^3$ Compute $f''(0)$,if  is continuous everywhere and  Compute,f'''(x) \lim_{x \to 0}(1+x+ \frac{f(x)}{x})^{1/x}=e^3 f''(0),"if $f'''(x)$ is continuous everywhere and $$\lim_{x \to 0}(1+x+ \frac{f(x)}{x})^{1/x}=e^3$$ Compute $f''(0)$ The limit equals to $$\begin{align} \lim_{x \to 0} \frac{\log(1+x+ \frac{f(x)}{x})}{x}-3=0.  \end{align}$$  From $$\frac{\log(1+x+ \frac{f(x)}{x})}{x}-3=o(1)$$ as $x \to 0$, I get $$1+x+\frac{f(x)}{x} = e^{3x+o(x)},$$ and  $$f(x)=x(e^{3x+o(x)}-x-1),\frac{f(x)}{x}=e^{3x+o(x)}-x-1$$ as $x \to 0$. So both $f(0)$ and $f'(0)$ are $0$. Approximating $e^{3x+o(x)}=1+3x+o(x)$ I get $$\begin{align} f(x) &= x(1+3x+o(x)-x-1) =2x^2+o(x^2). \end{align}$$ Now I try to use the definition of derivative to calculate the $f''(x)$  $$f''(x)=\lim_{x \to 0}\frac{f'(x)-f'(0)}{x}=\lim_{x \to 0} \frac{f'(x)}{x}$$ I'm not sure whether I can get $f'(x)$ by differentiating the approximation $2x^2+o(x^2)$ and how to differentiate $o(x^2)$.","if $f'''(x)$ is continuous everywhere and $$\lim_{x \to 0}(1+x+ \frac{f(x)}{x})^{1/x}=e^3$$ Compute $f''(0)$ The limit equals to $$\begin{align} \lim_{x \to 0} \frac{\log(1+x+ \frac{f(x)}{x})}{x}-3=0.  \end{align}$$  From $$\frac{\log(1+x+ \frac{f(x)}{x})}{x}-3=o(1)$$ as $x \to 0$, I get $$1+x+\frac{f(x)}{x} = e^{3x+o(x)},$$ and  $$f(x)=x(e^{3x+o(x)}-x-1),\frac{f(x)}{x}=e^{3x+o(x)}-x-1$$ as $x \to 0$. So both $f(0)$ and $f'(0)$ are $0$. Approximating $e^{3x+o(x)}=1+3x+o(x)$ I get $$\begin{align} f(x) &= x(1+3x+o(x)-x-1) =2x^2+o(x^2). \end{align}$$ Now I try to use the definition of derivative to calculate the $f''(x)$  $$f''(x)=\lim_{x \to 0}\frac{f'(x)-f'(0)}{x}=\lim_{x \to 0} \frac{f'(x)}{x}$$ I'm not sure whether I can get $f'(x)$ by differentiating the approximation $2x^2+o(x^2)$ and how to differentiate $o(x^2)$.",,"['calculus', 'limits', 'derivatives']"
1,Prove that a function is differentiable using the limit definition,Prove that a function is differentiable using the limit definition,,"Use the definition of the derivative to prove that $f(x,y)=xy$ is differentiable. So we have: $$\lim_{h \to 0} \frac{||f(x_0 + h) - f(x_0) - J(h)||}{||h||} = 0$$ We find the partial derivatives which are $f_x = y$ and $f_y = x$. We plug them into the definition: $$\lim_{h \to 0} \frac{||f(x_0 + h) - f(x_0) - yh - xh||}{||h||} = 0$$ I'm not sure what to do from here. So do we calculate the norm of the numerator and denominator now?","Use the definition of the derivative to prove that $f(x,y)=xy$ is differentiable. So we have: $$\lim_{h \to 0} \frac{||f(x_0 + h) - f(x_0) - J(h)||}{||h||} = 0$$ We find the partial derivatives which are $f_x = y$ and $f_y = x$. We plug them into the definition: $$\lim_{h \to 0} \frac{||f(x_0 + h) - f(x_0) - yh - xh||}{||h||} = 0$$ I'm not sure what to do from here. So do we calculate the norm of the numerator and denominator now?",,"['limits', 'multivariable-calculus', 'derivatives']"
2,How do I solve this infinite limit?,How do I solve this infinite limit?,,"I have this limit: $$ \lim_{x\to\infty}\frac{x^3+\cos x+e^{-2x}}{x^2\sqrt{x^2+1}} $$ I tried to solve it by this: $$ \lim_{x\to\infty}\frac{x^3+\cos x+e^{-2x}}{x^2\sqrt{x^2+1}} = \lim_{x\to\infty}\frac{\frac{x^3}{x^3}+\frac{\cos x}{x^3}+\frac{e^{-2x}}{x^3}}{\frac{x^2\sqrt{x^2+1}}{x^3}} = \frac{0+0+0}{\frac{\sqrt{\infty^2+1}}{\infty}}$$ I do not think that I got it right there... Wolfram also says that the answer is $1$, which this does not seems to be. How do I solve this?","I have this limit: $$ \lim_{x\to\infty}\frac{x^3+\cos x+e^{-2x}}{x^2\sqrt{x^2+1}} $$ I tried to solve it by this: $$ \lim_{x\to\infty}\frac{x^3+\cos x+e^{-2x}}{x^2\sqrt{x^2+1}} = \lim_{x\to\infty}\frac{\frac{x^3}{x^3}+\frac{\cos x}{x^3}+\frac{e^{-2x}}{x^3}}{\frac{x^2\sqrt{x^2+1}}{x^3}} = \frac{0+0+0}{\frac{\sqrt{\infty^2+1}}{\infty}}$$ I do not think that I got it right there... Wolfram also says that the answer is $1$, which this does not seems to be. How do I solve this?",,['limits']
3,Is every convergent limit of an iteration a fixed point as well?,Is every convergent limit of an iteration a fixed point as well?,,Let $f(x)$ be a function and suppose $\lim_{n \to \infty}f^n(a)=L$ for some $a$ in the domain of $f$. What are the sufficient conditions for $L$ being a fixed point of $f$? Is the continuity of $f$ enough?,Let $f(x)$ be a function and suppose $\lim_{n \to \infty}f^n(a)=L$ for some $a$ in the domain of $f$. What are the sufficient conditions for $L$ being a fixed point of $f$? Is the continuity of $f$ enough?,,['limits']
4,how do I prove that :$\lim_{x\to 3} x(3-x)= 0 $,how do I prove that :,\lim_{x\to 3} x(3-x)= 0 ,how do I prove that :$\lim_{x\to 3} x(3-x)= 0 $ Using $\epsilon -\delta$ I need to find $\delta>0$ whenever $\epsilon>0 $ such that: $|3x-x^2|<\epsilon $ whenever $0<|x-3|<\delta$ $x|3-x|<\epsilon $ whenever $0<|x-3|<\delta$ $\equiv -x|x-3|<\epsilon $ whenever $0<|x-3|<\delta$ $ |x-3|<\dfrac{\epsilon}{-x} $ whenever $0<|x-3|<\delta$ I choose $\dfrac{\epsilon}{-x}=\delta $ $\equiv -x|x-3|<\epsilon $ whenever $0<|x-3|<\delta$ This implies $-x|x-3|<-x\delta $ whenever $0<|x-3|<\delta$ Which should be proof. By the way I was tempted to just plug in $x=3$ in $x(3-x)= 0 $,how do I prove that :$\lim_{x\to 3} x(3-x)= 0 $ Using $\epsilon -\delta$ I need to find $\delta>0$ whenever $\epsilon>0 $ such that: $|3x-x^2|<\epsilon $ whenever $0<|x-3|<\delta$ $x|3-x|<\epsilon $ whenever $0<|x-3|<\delta$ $\equiv -x|x-3|<\epsilon $ whenever $0<|x-3|<\delta$ $ |x-3|<\dfrac{\epsilon}{-x} $ whenever $0<|x-3|<\delta$ I choose $\dfrac{\epsilon}{-x}=\delta $ $\equiv -x|x-3|<\epsilon $ whenever $0<|x-3|<\delta$ This implies $-x|x-3|<-x\delta $ whenever $0<|x-3|<\delta$ Which should be proof. By the way I was tempted to just plug in $x=3$ in $x(3-x)= 0 $,,"['calculus', 'limits']"
5,Is it obvious that $\sum_n x_n = 0 $ when $x_n = 0 ~ \forall n \in \mathbb N$?,Is it obvious that  when ?,\sum_n x_n = 0  x_n = 0 ~ \forall n \in \mathbb N,"Until recently I used to think that because of induction, a statement $P_n$ which is true $\forall n \in \mathbb N$ was also true when $n \to \infty$. Life was simple, and I was happy. Then someone (on this website) showed me a counterexample with $P_n = $ ""$\sum_1^n$ is finite"". I was shocked and confused. My current understanding is that knowing that a statement is true $\forall n \in \mathbb N$ doesn't automatically imply that it is true when $n \to \infty$. The latter case might also be true, but needs a separate proof. Now please help me understand the following two statements: if $x_n = 0 ~ \forall n \in \mathbb N$, then $\lim_n x_n = 0 $ if $x_n = 0 ~ \forall n \in \mathbb N$, then $\sum_n x_n = 0 $ These two statements seem obvious but I don't trust ""obvious"" anymore. How do you prove them? What is the relationship between (1) and (2)?","Until recently I used to think that because of induction, a statement $P_n$ which is true $\forall n \in \mathbb N$ was also true when $n \to \infty$. Life was simple, and I was happy. Then someone (on this website) showed me a counterexample with $P_n = $ ""$\sum_1^n$ is finite"". I was shocked and confused. My current understanding is that knowing that a statement is true $\forall n \in \mathbb N$ doesn't automatically imply that it is true when $n \to \infty$. The latter case might also be true, but needs a separate proof. Now please help me understand the following two statements: if $x_n = 0 ~ \forall n \in \mathbb N$, then $\lim_n x_n = 0 $ if $x_n = 0 ~ \forall n \in \mathbb N$, then $\sum_n x_n = 0 $ These two statements seem obvious but I don't trust ""obvious"" anymore. How do you prove them? What is the relationship between (1) and (2)?",,"['sequences-and-series', 'limits', 'induction']"
6,Prove $\{x_n\}$ converges,Prove  converges,\{x_n\},"Let $\{x_n\}$, a sequence such that: $\forall n \in \mathbb{N}: \left| {x_{n+2}-x_n} \right| < {1 \over 2^n}; \quad   \lim_{n \to \infty } ({x_{n + 1}} - {x_n}) = 0$ I translated the above to: $$\begin{array}{l}  {x_n} - \frac{1}{{{2^n}}} < {x_{n + 2}} < {x_n} + \frac{1}{{{2^n}}} \\   {x_n} - \varepsilon  < {x_{n + 1}} < {x_n} + \varepsilon  \\   \end{array}$$ But at this point I'm kinda stuck. I guess I need to ""glue"" the two statements somehow. I also considered, treating $X_{odd}$ and $X_{even}$. Would that be helpful?","Let $\{x_n\}$, a sequence such that: $\forall n \in \mathbb{N}: \left| {x_{n+2}-x_n} \right| < {1 \over 2^n}; \quad   \lim_{n \to \infty } ({x_{n + 1}} - {x_n}) = 0$ I translated the above to: $$\begin{array}{l}  {x_n} - \frac{1}{{{2^n}}} < {x_{n + 2}} < {x_n} + \frac{1}{{{2^n}}} \\   {x_n} - \varepsilon  < {x_{n + 1}} < {x_n} + \varepsilon  \\   \end{array}$$ But at this point I'm kinda stuck. I guess I need to ""glue"" the two statements somehow. I also considered, treating $X_{odd}$ and $X_{even}$. Would that be helpful?",,"['calculus', 'sequences-and-series', 'limits']"
7,Radius of Convergence and its application to a Power Series including $x^{2n}$ rather than $x^n$,Radius of Convergence and its application to a Power Series including  rather than,x^{2n} x^n,"(Radius of Convergence) Consider the Power Series $f(x)=\sum_{n=0}^{+ \infty}a_n x^n$, the radius of convergence $\rho$ can be found using $$\rho = \displaystyle \lim_{n \to + \infty} \left| \frac{a_n}{a_{n+1}} \right|$$ I did quite a few exercises already using the above definition and successfully computed the radius of convergence with it. But I stumbled across this example which I find interesting: $$\sum_{n=0}^{+ \infty} \frac{(2n)!!}{n^n}x^{2n}\tag{taken from Michaels Analysis I}$$ For the sake of the argument let $0^0:=1$, otherwise just start from $n=1$. First I note that this, from a narrow point of view, is a power series to me. However, I can see that it  is not quite in the form of $\sum a_n x^n$ but in the form $\sum a_n x^{2n}$. My leading impulse was to ignore that fact and just compute it anyway using the above definition. Here is what came out: $$\displaystyle \lim_{n \to + \infty} \frac{(2n)!!}{n^n}\cdot \frac{(n+1)^{n+1}}{(2n+2)!!}= \lim_{n \to + \infty} \frac{(2n)!!(n+1)}{(2n+2)(2n)!!} \cdot \left(\frac{n+1}{n}\right)^n= \frac{e}{2}=\rho$$ so I would have concluded that $|x|< \frac{e}{2}$ is the Radius of convergence (without having checked the behavior on the endpoints) . This is not the right answer, the correct answer would be $\rho = \sqrt{ e / 2}$. Now as I looked for similar exercises on this website I often saw that a lot of people don't use the above definition and just use the Quotient Criteria (d'Alembert Criteria) to find the Radius of convergence. Indeed, using this criteria here things work out much nicer: $$ \lim_{n \to + \infty} \left|\frac{a_{n+1}}{a_n} \right|=\lim_{n \to + \infty} \frac{(2n+2)!!x^{2n+2}}{(n+1)^{n+1}}\cdot \frac{n^n}{(2n)!!x^{2n}}=\lim_{n \to + \infty} x^2\frac{(2n+2)(2n)!!}{(n+1)(2n)!!}\left( \frac{n}{n+1}\right)^n \\ = \frac{2x^2}{e}\overset{!}<1 \implies x^2 < \frac{e}{2}\implies \rho = \sqrt{\frac{e}{2}}$$ Question : Is there a specific way of route I can go into to decide how to find the radius of convergence? It might be a bit far fetched to say such a thing, but what I have seen on this site, most people use the Quotient Criteria rather then the definition above. Also, is there a way I could alter the above example to use the Radius of Convergence definition I have given above in the header? Or would this be too much trouble in the first place to even consider doing such? Additional : In the same book, C.T. Michaels Analysis 1, he shows that the Radius of Convergence for the $J_0$ Besselfunction is $\infty$ by using the exact definition as in the header above: $$J_0(x)= \sum_{n=0}^{+ \infty} \frac{(-1)^nx^{2n}}{2^{2n}(n!)^2}$$ which might help you to understand or rather highlight my confusion.","(Radius of Convergence) Consider the Power Series $f(x)=\sum_{n=0}^{+ \infty}a_n x^n$, the radius of convergence $\rho$ can be found using $$\rho = \displaystyle \lim_{n \to + \infty} \left| \frac{a_n}{a_{n+1}} \right|$$ I did quite a few exercises already using the above definition and successfully computed the radius of convergence with it. But I stumbled across this example which I find interesting: $$\sum_{n=0}^{+ \infty} \frac{(2n)!!}{n^n}x^{2n}\tag{taken from Michaels Analysis I}$$ For the sake of the argument let $0^0:=1$, otherwise just start from $n=1$. First I note that this, from a narrow point of view, is a power series to me. However, I can see that it  is not quite in the form of $\sum a_n x^n$ but in the form $\sum a_n x^{2n}$. My leading impulse was to ignore that fact and just compute it anyway using the above definition. Here is what came out: $$\displaystyle \lim_{n \to + \infty} \frac{(2n)!!}{n^n}\cdot \frac{(n+1)^{n+1}}{(2n+2)!!}= \lim_{n \to + \infty} \frac{(2n)!!(n+1)}{(2n+2)(2n)!!} \cdot \left(\frac{n+1}{n}\right)^n= \frac{e}{2}=\rho$$ so I would have concluded that $|x|< \frac{e}{2}$ is the Radius of convergence (without having checked the behavior on the endpoints) . This is not the right answer, the correct answer would be $\rho = \sqrt{ e / 2}$. Now as I looked for similar exercises on this website I often saw that a lot of people don't use the above definition and just use the Quotient Criteria (d'Alembert Criteria) to find the Radius of convergence. Indeed, using this criteria here things work out much nicer: $$ \lim_{n \to + \infty} \left|\frac{a_{n+1}}{a_n} \right|=\lim_{n \to + \infty} \frac{(2n+2)!!x^{2n+2}}{(n+1)^{n+1}}\cdot \frac{n^n}{(2n)!!x^{2n}}=\lim_{n \to + \infty} x^2\frac{(2n+2)(2n)!!}{(n+1)(2n)!!}\left( \frac{n}{n+1}\right)^n \\ = \frac{2x^2}{e}\overset{!}<1 \implies x^2 < \frac{e}{2}\implies \rho = \sqrt{\frac{e}{2}}$$ Question : Is there a specific way of route I can go into to decide how to find the radius of convergence? It might be a bit far fetched to say such a thing, but what I have seen on this site, most people use the Quotient Criteria rather then the definition above. Also, is there a way I could alter the above example to use the Radius of Convergence definition I have given above in the header? Or would this be too much trouble in the first place to even consider doing such? Additional : In the same book, C.T. Michaels Analysis 1, he shows that the Radius of Convergence for the $J_0$ Besselfunction is $\infty$ by using the exact definition as in the header above: $$J_0(x)= \sum_{n=0}^{+ \infty} \frac{(-1)^nx^{2n}}{2^{2n}(n!)^2}$$ which might help you to understand or rather highlight my confusion.",,"['calculus', 'analysis', 'limits', 'definition']"
8,How find this limit $\lim_{x\to 0^{+}}\int_{x}^{1}\frac{\ln{(1+t)}}{\sqrt{t}}dt$,How find this limit,\lim_{x\to 0^{+}}\int_{x}^{1}\frac{\ln{(1+t)}}{\sqrt{t}}dt,Find this limit $$\lim_{x\to 0^{+}}\left(\int_{x}^{1}\dfrac{\ln{(1+t)}}{\sqrt{t}}dt+\int_{0}^{x}\dfrac{\sin{2t}}{\sqrt{4+t^2}\int_{0}^{x}(\sqrt{y+1}-1)dy}dt\right)$$ My try: since $$\int_{0}^{x}(\sqrt{y+1}-1)dy=\dfrac{2}{3}(1+x)^{3/2}-x$$ maybe $$\lim_{x\to 0^{+}}\int_{x}^{1}\dfrac{\ln{(1+t)}}{\sqrt{t}}dt$$ exsit? and $$\lim_{x\to 0^{+}}\int_{0}^{x}\dfrac{\sin{2t}}{\sqrt{4+t^2}\int_{0}^{x}(\sqrt{y+1}-1)dy}dt$$ exsit? then I can't,Find this limit $$\lim_{x\to 0^{+}}\left(\int_{x}^{1}\dfrac{\ln{(1+t)}}{\sqrt{t}}dt+\int_{0}^{x}\dfrac{\sin{2t}}{\sqrt{4+t^2}\int_{0}^{x}(\sqrt{y+1}-1)dy}dt\right)$$ My try: since $$\int_{0}^{x}(\sqrt{y+1}-1)dy=\dfrac{2}{3}(1+x)^{3/2}-x$$ maybe $$\lim_{x\to 0^{+}}\int_{x}^{1}\dfrac{\ln{(1+t)}}{\sqrt{t}}dt$$ exsit? and $$\lim_{x\to 0^{+}}\int_{0}^{x}\dfrac{\sin{2t}}{\sqrt{4+t^2}\int_{0}^{x}(\sqrt{y+1}-1)dy}dt$$ exsit? then I can't,,[]
9,Express the mathematical constant $e$ in terms of a limit that goes to zero.,Express the mathematical constant  in terms of a limit that goes to zero.,e,The mathematical expression of the mathematical constant $e$ in terms of a limit that goes to infinity is $$e = \lim\limits_{n\rightarrow \infty} \left(1+\frac{1}{n}\right)^n$$ But can we express the mathematical constant $e$ in terms of a limit that goes to zero; $e=\lim\limits_{n\rightarrow 0} ...$? Thank you. (problem from book of Spivak),The mathematical expression of the mathematical constant $e$ in terms of a limit that goes to infinity is $$e = \lim\limits_{n\rightarrow \infty} \left(1+\frac{1}{n}\right)^n$$ But can we express the mathematical constant $e$ in terms of a limit that goes to zero; $e=\lim\limits_{n\rightarrow 0} ...$? Thank you. (problem from book of Spivak),,"['calculus', 'limits', 'constants']"
10,A trouble-some limit..,A trouble-some limit..,,"For a unique value $r$, the value of $$\lim_{n\rightarrow \infty} \ n^r \times \frac{1}{2} \times \frac{3}{4} \times\dots\times\frac{2n-1}{2n}$$ exists and is non-zero. For this value of $r$, what is the limit? I've tried a few values but I haven't made much progress. Any help would be appreciated.","For a unique value $r$, the value of $$\lim_{n\rightarrow \infty} \ n^r \times \frac{1}{2} \times \frac{3}{4} \times\dots\times\frac{2n-1}{2n}$$ exists and is non-zero. For this value of $r$, what is the limit? I've tried a few values but I haven't made much progress. Any help would be appreciated.",,"['calculus', 'limits']"
11,Selection of $b_n$ in Limit Comparison Test for checking convergence of a series,Selection of  in Limit Comparison Test for checking convergence of a series,b_n,I wonder what the correct criterion for selection of $b_n$ in Limit Comparison Test for checking convergence of a series. Any hint to online material will be highly appreciated. The selection of $b_n$ in first series is easy but in other three is tricky. Is there any universal criterion for selection of $b_n$? Thanks in advance for your help. $\sum_{1}^{\infty}\frac{\sqrt{n}}{1+n}$ then $b_{n}=\frac{\sqrt{n}}{n}=\frac{1}{\sqrt{n}}$ $\sum_{1}^{\infty}\frac{\ln n}{n}$ then $b_{n}=\frac{1}{n}$ $\sum_{1}^{\infty}\sin(\frac{\pi}{n})$ then $b_{n}=\frac{\pi}{n}$ $\sum_{1}^{\infty}\frac{\ln\left(n+1\right)}{n^{2}}$ then $b_{n}=\frac{1}{n^{\frac{3}{2}}}$,I wonder what the correct criterion for selection of $b_n$ in Limit Comparison Test for checking convergence of a series. Any hint to online material will be highly appreciated. The selection of $b_n$ in first series is easy but in other three is tricky. Is there any universal criterion for selection of $b_n$? Thanks in advance for your help. $\sum_{1}^{\infty}\frac{\sqrt{n}}{1+n}$ then $b_{n}=\frac{\sqrt{n}}{n}=\frac{1}{\sqrt{n}}$ $\sum_{1}^{\infty}\frac{\ln n}{n}$ then $b_{n}=\frac{1}{n}$ $\sum_{1}^{\infty}\sin(\frac{\pi}{n})$ then $b_{n}=\frac{\pi}{n}$ $\sum_{1}^{\infty}\frac{\ln\left(n+1\right)}{n^{2}}$ then $b_{n}=\frac{1}{n^{\frac{3}{2}}}$,,"['limits', 'convergence-divergence']"
12,How to calculate the following sum:,How to calculate the following sum:,,How may one calculate  $$\lim_{n\to\infty} \ \left(\left(\sum_{k=1}^{n} \frac{1}{3k-1}\right) - \frac{\ln n}{3}\right) \ ?$$,How may one calculate  $$\lim_{n\to\infty} \ \left(\left(\sum_{k=1}^{n} \frac{1}{3k-1}\right) - \frac{\ln n}{3}\right) \ ?$$,,"['sequences-and-series', 'limits']"
13,Show that: $\displaystyle\lim_{n\rightarrow \infty}f(x_n)=f(\lim_{n \rightarrow \infty} x_n)$ [duplicate],Show that:  [duplicate],\displaystyle\lim_{n\rightarrow \infty}f(x_n)=f(\lim_{n \rightarrow \infty} x_n),"This question already has an answer here : How to show $f$ is continuous at $x$ IFF for any sequence ${x_n}$ in $X$ converging to $x$ the sequence $f(x_n)$ converges in $Y$ to $f(x)$ (1 answer) Closed 8 years ago . Let $f:X \rightarrow Y$ be a function. Prove that if $f$ is continuous, then for every convergent sequence $(x_n)$ $\displaystyle\lim_{n\rightarrow \infty}f(x_n)=f(\lim_{n \rightarrow \infty} x_n)$ My attempt: Assume $f$ is continuous. Then $\forall \epsilon >0, \exists \delta > 0$ such that $\forall p \in X$ and $q \in X: d(p,q) < \delta \implies d(f(q),f(q)) < \epsilon.$ And also assume that $(x_n)$ converges. Then $\exists L$ such that $\forall \epsilon >0, \exists N$ such that $n>N \implies d(x_n, L) < \epsilon.$ I need to use these definitions to show lim $_{n\rightarrow \infty}\,f(x_n)=f(\text{lim}_{n \rightarrow \infty} \, x_n)$ . Any hints? Edit: lim $_{n \rightarrow \infty}(x_n) \in X$","This question already has an answer here : How to show $f$ is continuous at $x$ IFF for any sequence ${x_n}$ in $X$ converging to $x$ the sequence $f(x_n)$ converges in $Y$ to $f(x)$ (1 answer) Closed 8 years ago . Let be a function. Prove that if is continuous, then for every convergent sequence My attempt: Assume is continuous. Then such that and And also assume that converges. Then such that such that I need to use these definitions to show lim . Any hints? Edit: lim","f:X \rightarrow Y f (x_n) \displaystyle\lim_{n\rightarrow \infty}f(x_n)=f(\lim_{n \rightarrow \infty} x_n) f \forall \epsilon >0, \exists \delta > 0 \forall p \in X q \in X: d(p,q) < \delta \implies d(f(q),f(q)) < \epsilon. (x_n) \exists L \forall \epsilon >0, \exists N n>N \implies d(x_n, L) < \epsilon. _{n\rightarrow \infty}\,f(x_n)=f(\text{lim}_{n \rightarrow \infty} \, x_n) _{n \rightarrow \infty}(x_n) \in X","['analysis', 'limits']"
14,A basic real analysis question on limit sup and limit inf,A basic real analysis question on limit sup and limit inf,,Suppose $f$ be a function from $\Bbb R$ to $\Bbb R$. Now $\limsup_{t->x} f(t) - \liminf_{t->x} f(t) < \frac{1}{k}$. Does this imply that there is an open interval $I_x$ containing $x$ such that $$\sup\{f(t):t \in I_x\} - \inf\{f(t):t \in I_x\} < \frac{1}{k}$$. I think we can tell the following : $$\sup\{f(t):t \in I_x(\epsilon)\} - \inf\{f(t):t \in I_x(\epsilon)\} < \frac{1}{k}+\epsilon$$ for any $\epsilon > 0$. But how the above ?,Suppose $f$ be a function from $\Bbb R$ to $\Bbb R$. Now $\limsup_{t->x} f(t) - \liminf_{t->x} f(t) < \frac{1}{k}$. Does this imply that there is an open interval $I_x$ containing $x$ such that $$\sup\{f(t):t \in I_x\} - \inf\{f(t):t \in I_x\} < \frac{1}{k}$$. I think we can tell the following : $$\sup\{f(t):t \in I_x(\epsilon)\} - \inf\{f(t):t \in I_x(\epsilon)\} < \frac{1}{k}+\epsilon$$ for any $\epsilon > 0$. But how the above ?,,"['real-analysis', 'limits']"
15,Problem concerning limit,Problem concerning limit,,"My friend asked me the question while he is preparing the mathematical analysis exams. Let $\left\{a_{n}\right\}$ be a sequence satisfying   $\lim\limits_{n\to\infty}\left(a_{n}\sum\limits_{k=1}^{n}a_{k}^{2}\right)=1$. Prove that $\lim\limits_{n\to\infty}\left(\,\sqrt[3]{3n\,}\,\ a_n\,\right) =1$. Here is my attempt:  let $S_n= \sum_{k=1}^{n} a_k^2$, then we get  $$\lim_{n\to\infty} (S_n-S_{n-1}) S_n^2=1$$ What we need is $$\lim_{n\to\infty} \sqrt[3]{9n^2} (S_n-S_{n-1})=1$$ After trying Stolz theorem, I still cannot get the term $\sqrt[3]{9n^2}$. I wonder how to get this result? Any hints or solutions are welcomed, thanks!","My friend asked me the question while he is preparing the mathematical analysis exams. Let $\left\{a_{n}\right\}$ be a sequence satisfying   $\lim\limits_{n\to\infty}\left(a_{n}\sum\limits_{k=1}^{n}a_{k}^{2}\right)=1$. Prove that $\lim\limits_{n\to\infty}\left(\,\sqrt[3]{3n\,}\,\ a_n\,\right) =1$. Here is my attempt:  let $S_n= \sum_{k=1}^{n} a_k^2$, then we get  $$\lim_{n\to\infty} (S_n-S_{n-1}) S_n^2=1$$ What we need is $$\lim_{n\to\infty} \sqrt[3]{9n^2} (S_n-S_{n-1})=1$$ After trying Stolz theorem, I still cannot get the term $\sqrt[3]{9n^2}$. I wonder how to get this result? Any hints or solutions are welcomed, thanks!",,"['analysis', 'limits']"
16,Limit $\mathop {\lim }\limits_{n \to \infty } n({1 \over {{{(n + 1)}^2}}} + {1 \over {{{(n + 2)}^2}}} + \cdots{1 \over {{{(2n)}^2}}})$,Limit,\mathop {\lim }\limits_{n \to \infty } n({1 \over {{{(n + 1)}^2}}} + {1 \over {{{(n + 2)}^2}}} + \cdots{1 \over {{{(2n)}^2}}}),"Without using integrals, how to find this limit: $$\mathop {\lim }\limits_{n \to \infty } {a_n} = n\cdot\left({1 \over {{{(n + 1)}^2}}} + {1 \over {{{(n + 2)}^2}}} + \cdots{1 \over {{{(2n)}^2}}}\right)$$ I tried squeezing the sequence but it didn't workout. What next should I do?","Without using integrals, how to find this limit: $$\mathop {\lim }\limits_{n \to \infty } {a_n} = n\cdot\left({1 \over {{{(n + 1)}^2}}} + {1 \over {{{(n + 2)}^2}}} + \cdots{1 \over {{{(2n)}^2}}}\right)$$ I tried squeezing the sequence but it didn't workout. What next should I do?",,"['calculus', 'sequences-and-series', 'limits']"
17,A short question about an e identity,A short question about an e identity,,Why is this $\{(1+\frac1{a_n})^{a_n}=e\}$ true when: $a_n \to -\infty$ $a_n$ is a sequence. Thanks.,Why is this $\{(1+\frac1{a_n})^{a_n}=e\}$ true when: $a_n \to -\infty$ $a_n$ is a sequence. Thanks.,,"['calculus', 'limits']"
18,Find the limit with L'Hopital's rule $\lim_{x\to0^+}\left(\frac{2}{\pi}\arcsin x\right)^{1/x} $,Find the limit with L'Hopital's rule,\lim_{x\to0^+}\left(\frac{2}{\pi}\arcsin x\right)^{1/x} ,I was solving this problem and wolfram alpha said it was $0$.But i just can't get it to $0$ using L'Hospital.  Can you please show me how to do it. $$\lim_{x\to0^+}\left(\frac{2}{\pi}\arcsin x\right)^{1/x} $$,I was solving this problem and wolfram alpha said it was $0$.But i just can't get it to $0$ using L'Hospital.  Can you please show me how to do it. $$\lim_{x\to0^+}\left(\frac{2}{\pi}\arcsin x\right)^{1/x} $$,,"['calculus', 'limits']"
19,Limit of subtracting fractions from 1,Limit of subtracting fractions from 1,,"Suppose you have the sequence of fractions $\left\{\frac{1}{a} : a \in \mathbb{N}\right\}$ ($\frac{1}{2},\frac{1}{3}$ and so on). Now you start with $1$ and subtract every item of the sequence as long as the result is larger than $0$. You would start with subtracting $\frac{1}{2}$ and $\frac{1}{3}$, but then skip $4-6$ because the result would be 0 or smaller. You continue with $\frac{1}{7}$ and $\frac{1}{43}$. Is there any lower limit to how small a number you can get or can you get as close to $0$ as you like?","Suppose you have the sequence of fractions $\left\{\frac{1}{a} : a \in \mathbb{N}\right\}$ ($\frac{1}{2},\frac{1}{3}$ and so on). Now you start with $1$ and subtract every item of the sequence as long as the result is larger than $0$. You would start with subtracting $\frac{1}{2}$ and $\frac{1}{3}$, but then skip $4-6$ because the result would be 0 or smaller. You continue with $\frac{1}{7}$ and $\frac{1}{43}$. Is there any lower limit to how small a number you can get or can you get as close to $0$ as you like?",,['limits']
20,$n \approxeq k + 2^{2^k}(k+1)$. How can one get the value of $k(n)$ from this equation?,. How can one get the value of  from this equation?,n \approxeq k + 2^{2^k}(k+1) k(n),I am trying to find approximation for this sum. Asymptotic approximation of sum $\sum_{k=0}^{n}\frac{{n\choose k}}{2^{2^k}}$ Doing following way. Let $a_k(n) = \frac{n\choose k}{2^{2^k}}$. I tried to find $k$ that $\frac{a_{k+1}(n)}{a_k(n)}$ congregates to finite value other than $0$. I think for that $k$ the sum will be approximately the same as $a_k(n)$. So I get $n \approxeq k + 2^{2^k}(k+1)$. How can one get the value of $k(n)$ from this equation?,I am trying to find approximation for this sum. Asymptotic approximation of sum $\sum_{k=0}^{n}\frac{{n\choose k}}{2^{2^k}}$ Doing following way. Let $a_k(n) = \frac{n\choose k}{2^{2^k}}$. I tried to find $k$ that $\frac{a_{k+1}(n)}{a_k(n)}$ congregates to finite value other than $0$. I think for that $k$ the sum will be approximately the same as $a_k(n)$. So I get $n \approxeq k + 2^{2^k}(k+1)$. How can one get the value of $k(n)$ from this equation?,,"['sequences-and-series', 'limits', 'approximation', 'roots']"
21,Can we calculate this limit analytically?,Can we calculate this limit analytically?,,"What will be the limit of the following for $x=0$? $\sin(\cos(\tan(\sin(\cos(\tan(\sin(\cos(\tan( ... \mbox{infinite times} ... \sin(\cos(\tan(x)))))))...)$ On a calculator it seems to approach 0.72063..., but how do I get the answer analytically?","What will be the limit of the following for $x=0$? $\sin(\cos(\tan(\sin(\cos(\tan(\sin(\cos(\tan( ... \mbox{infinite times} ... \sin(\cos(\tan(x)))))))...)$ On a calculator it seems to approach 0.72063..., but how do I get the answer analytically?",,"['calculus', 'limits', 'trigonometry']"
22,Can someone help me solve this limits question?,Can someone help me solve this limits question?,,$$\begin{align}\lim x → ∞\end{align}$$ $$\begin{align} f(x) = {\frac{2^{x+1}+{3^{x+1}}}{2^x + 3^x}} \\ \end{align}$$ I tried using L Hopitable but that gives the same expression. Also tried using substitution but I didn't get anywhere. Help would be appreciated.,$$\begin{align}\lim x → ∞\end{align}$$ $$\begin{align} f(x) = {\frac{2^{x+1}+{3^{x+1}}}{2^x + 3^x}} \\ \end{align}$$ I tried using L Hopitable but that gives the same expression. Also tried using substitution but I didn't get anywhere. Help would be appreciated.,,"['limits', 'infinity']"
23,Prove $\sqrt{s_n+1} = \frac{1}{2}(1+\sqrt{5})$,Prove,\sqrt{s_n+1} = \frac{1}{2}(1+\sqrt{5}),"This is to prove how the limit of $s_n$ converges to $\frac{1}{2}(1+\sqrt{5})$. Assume: $s_1 = 1$; for $n \geq 1$, $s_{n+1} = \sqrt{s_n + 1}$. How to prove this converges to $\frac{1}{2}(1+\sqrt{5})$?","This is to prove how the limit of $s_n$ converges to $\frac{1}{2}(1+\sqrt{5})$. Assume: $s_1 = 1$; for $n \geq 1$, $s_{n+1} = \sqrt{s_n + 1}$. How to prove this converges to $\frac{1}{2}(1+\sqrt{5})$?",,['limits']
24,The limit of arccos(x) is the arccos(x) of the limit?,The limit of arccos(x) is the arccos(x) of the limit?,,"Recently I came across a proof using the apparant fact that $$\lim_{x \rightarrow a} \cos^{-1}(x)=\cos^{-1}(\lim_{x \rightarrow a} x)$$ with justification: because arccos(x) is a continuous function . Is this limit property true for all continuous functions or a class of continuous functions and can it be proved easily? If the property is true, I am not looking for a formal proof rather a general outline or conceptual proof. This seems to make intuitive sense, but I cannot think of any type of specific reason for why this property may be true.","Recently I came across a proof using the apparant fact that $$\lim_{x \rightarrow a} \cos^{-1}(x)=\cos^{-1}(\lim_{x \rightarrow a} x)$$ with justification: because arccos(x) is a continuous function . Is this limit property true for all continuous functions or a class of continuous functions and can it be proved easily? If the property is true, I am not looking for a formal proof rather a general outline or conceptual proof. This seems to make intuitive sense, but I cannot think of any type of specific reason for why this property may be true.",,"['limits', 'trigonometry', 'continuity']"
25,"If $(s_n)$ converges to $s$, then $(\lvert s_n\rvert)$ converges to $\lvert s\rvert$.","If  converges to , then  converges to .",(s_n) s (\lvert s_n\rvert) \lvert s\rvert,"Question: If $(s_n)$ converges to $s$, then $(\lvert s_n\rvert)$ converges to $\lvert s\rvert$. Prove or give a counterexample. Attempt: The statement is true because if $(\lvert s_n\rvert)$ converges to $\lvert s\rvert$, then given any $\epsilon >0$ there exists an $N\in\mathbb{N}$ such that for all $n\in\mathbb{N}$ with $n\geq N$ we have that $\lvert \lvert s_n\rvert-\lvert s\rvert\rvert <\epsilon$, but $\lvert \lvert s_n\rvert-\lvert s\rvert\rvert \leq\lvert s_n-s\rvert$ by the reverse triangle inequality, and by assumption given any $\epsilon >0$ there exists an $N\in\mathbb{N}$ such that for all $n\in\mathbb{N}$ with $n\geq N$ we have that $\lvert s_n-s\rvert <\epsilon$; hence   $$\lvert \lvert s_n\rvert -\lvert s\rvert \rvert\leq \lvert s_n-s\rvert<\epsilon.$$","Question: If $(s_n)$ converges to $s$, then $(\lvert s_n\rvert)$ converges to $\lvert s\rvert$. Prove or give a counterexample. Attempt: The statement is true because if $(\lvert s_n\rvert)$ converges to $\lvert s\rvert$, then given any $\epsilon >0$ there exists an $N\in\mathbb{N}$ such that for all $n\in\mathbb{N}$ with $n\geq N$ we have that $\lvert \lvert s_n\rvert-\lvert s\rvert\rvert <\epsilon$, but $\lvert \lvert s_n\rvert-\lvert s\rvert\rvert \leq\lvert s_n-s\rvert$ by the reverse triangle inequality, and by assumption given any $\epsilon >0$ there exists an $N\in\mathbb{N}$ such that for all $n\in\mathbb{N}$ with $n\geq N$ we have that $\lvert s_n-s\rvert <\epsilon$; hence   $$\lvert \lvert s_n\rvert -\lvert s\rvert \rvert\leq \lvert s_n-s\rvert<\epsilon.$$",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
26,On the Divergence of $s_n=\cos{\frac{\pi}{3}n}$: A Proof,On the Divergence of : A Proof,s_n=\cos{\frac{\pi}{3}n},"Question: Show that $s_n=\cos{\frac{\pi}{3}n}$ is divergent. Attempt: Suppose that $\lim_{n\rightarrow \infty}(\cos{\frac{\pi}{3}n})=s$, then given an $\epsilon$, say $\epsilon=1$, we can find an $N\in\mathbb{N}$ so that $$\begin{vmatrix} (\cos{\frac{\pi}{3}n})-s\end{vmatrix}<1.$$   If $n=6k+1$---for some sufficient $k\in\mathbb{N}$, then we obtain $\lvert \frac{1}{2}-s\rvert<1$, and so $\frac{1}{2}<s<\frac{3}{2}$; however, if $n=6k+3$---likewise for some sufficient $k\in\mathbb{N}$, then we obtain $\lvert -1-s\rvert<1$, and so $-2<s<0$. Therefore, since $s$ cannot satisfy both inequalities, $\lim_{n\rightarrow\infty}(\cos{\frac{\pi}{3}n})$ does not exist","Question: Show that $s_n=\cos{\frac{\pi}{3}n}$ is divergent. Attempt: Suppose that $\lim_{n\rightarrow \infty}(\cos{\frac{\pi}{3}n})=s$, then given an $\epsilon$, say $\epsilon=1$, we can find an $N\in\mathbb{N}$ so that $$\begin{vmatrix} (\cos{\frac{\pi}{3}n})-s\end{vmatrix}<1.$$   If $n=6k+1$---for some sufficient $k\in\mathbb{N}$, then we obtain $\lvert \frac{1}{2}-s\rvert<1$, and so $\frac{1}{2}<s<\frac{3}{2}$; however, if $n=6k+3$---likewise for some sufficient $k\in\mathbb{N}$, then we obtain $\lvert -1-s\rvert<1$, and so $-2<s<0$. Therefore, since $s$ cannot satisfy both inequalities, $\lim_{n\rightarrow\infty}(\cos{\frac{\pi}{3}n})$ does not exist",,"['real-analysis', 'sequences-and-series', 'limits', 'inequality']"
27,Limits and continuity and MVT,Limits and continuity and MVT,,"Okay, so in class today we went over proving continuity by the formal definition of limits, that is $$\forall \epsilon \gt 0, \exists \delta \gt 0 \mid \lvert x-a \rvert \lt \delta \implies \lvert f(x)-f(a)\rvert\lt\epsilon$$ So, that's all fine. But my professor gave an example where $f(x)=\ln(x)$ and he tried to show continuity at $x=2$. Now, he proved it using the MVT: $$\lvert\ln(x)-\ln(2)\rvert = \lvert f'(c) \left(x-2\right)\rvert$$ Am I crazy to think this cannot be used? For the MVT, you are assuming continuity, but we are trying to prove continuity! Also, if MVT cannot be used, can someone show how to prove this?","Okay, so in class today we went over proving continuity by the formal definition of limits, that is $$\forall \epsilon \gt 0, \exists \delta \gt 0 \mid \lvert x-a \rvert \lt \delta \implies \lvert f(x)-f(a)\rvert\lt\epsilon$$ So, that's all fine. But my professor gave an example where $f(x)=\ln(x)$ and he tried to show continuity at $x=2$. Now, he proved it using the MVT: $$\lvert\ln(x)-\ln(2)\rvert = \lvert f'(c) \left(x-2\right)\rvert$$ Am I crazy to think this cannot be used? For the MVT, you are assuming continuity, but we are trying to prove continuity! Also, if MVT cannot be used, can someone show how to prove this?",,"['limits', 'continuity']"
28,Calculate $\lim_{n\to\infty}\left(\prod_{i=3}^n\sec\frac{\pi}{i}\right)$,Calculate,\lim_{n\to\infty}\left(\prod_{i=3}^n\sec\frac{\pi}{i}\right),"This question arises from below: Initially you are given a circle $C_2$ with radius $r=1$. You construct the regualr 3-gon (equilateral triangle) with its incircle as $C_2$, and its circumscribed circle $C_3$. Then use $C_3$ as incircle and repeat this process, so you can construct the regualr $n$-gon with its circumscribed circle $C_n$. Does the limit of the radius of $C_n$ exist as $n\to\infty$ ? If it exists, what is the value? My attempt I have worked out that the radius of $C_n$ is $$r_n=\prod_{i=3}^n\sec\frac{\pi}{i}$$ Since $\lim_{n\to\infty}\sec\frac{\pi}{n}=1$, I concluded that $\lim_{n\to\infty}\frac{r_{n+1}}{r_n}=1$, and the limit of $r_n$ exists. But I cannot find its limiting value. MATHEMATICA says $r_{10^4}\approx8.69574$. Do you have any suggestions on how to proceed? Thank you in advanced!","This question arises from below: Initially you are given a circle $C_2$ with radius $r=1$. You construct the regualr 3-gon (equilateral triangle) with its incircle as $C_2$, and its circumscribed circle $C_3$. Then use $C_3$ as incircle and repeat this process, so you can construct the regualr $n$-gon with its circumscribed circle $C_n$. Does the limit of the radius of $C_n$ exist as $n\to\infty$ ? If it exists, what is the value? My attempt I have worked out that the radius of $C_n$ is $$r_n=\prod_{i=3}^n\sec\frac{\pi}{i}$$ Since $\lim_{n\to\infty}\sec\frac{\pi}{n}=1$, I concluded that $\lim_{n\to\infty}\frac{r_{n+1}}{r_n}=1$, and the limit of $r_n$ exists. But I cannot find its limiting value. MATHEMATICA says $r_{10^4}\approx8.69574$. Do you have any suggestions on how to proceed? Thank you in advanced!",,"['sequences-and-series', 'limits']"
29,Proving that $(ax^n)' = nax^{n-1}$ using the definition of the derivative,Proving that  using the definition of the derivative,(ax^n)' = nax^{n-1},"Given $f(x) = ax^n$, we have that $$f'(x) =  \lim_{h\to 0}  \frac{a(x+h)^n-ax^n}{h}$$ While its easy to prove this by induction by already implying that we know that $(ax^n)' = nax^{n-1}$, is there a simple way to solve the limit given above?","Given $f(x) = ax^n$, we have that $$f'(x) =  \lim_{h\to 0}  \frac{a(x+h)^n-ax^n}{h}$$ While its easy to prove this by induction by already implying that we know that $(ax^n)' = nax^{n-1}$, is there a simple way to solve the limit given above?",,"['calculus', 'limits', 'derivatives']"
30,To prove the limit of given function does not exist.,To prove the limit of given function does not exist.,,"Ques: I want to show that a limit of a function $$f(x,y)=\frac{x^{3}+y^{3}}{x-y}$$ does not exist at point $(0,0)$. My try: I am just taking path $y=x-x^{3}$ then $$\lim _{(x,y)\rightarrow(0,0)}\frac{x^{3}+y^{3}}{x-y}=\lim_{x\rightarrow 0}\frac{x^{3}+(x-x^{3})^{3}}{x^{3}}=2$$ Then i am taking path $y=2(x-x^{3})$ and i got limit $0$. So, in the both case the limit does not remain same. it means limit of given function does not exist. Am i right? please give your valuable suggestions!","Ques: I want to show that a limit of a function $$f(x,y)=\frac{x^{3}+y^{3}}{x-y}$$ does not exist at point $(0,0)$. My try: I am just taking path $y=x-x^{3}$ then $$\lim _{(x,y)\rightarrow(0,0)}\frac{x^{3}+y^{3}}{x-y}=\lim_{x\rightarrow 0}\frac{x^{3}+(x-x^{3})^{3}}{x^{3}}=2$$ Then i am taking path $y=2(x-x^{3})$ and i got limit $0$. So, in the both case the limit does not remain same. it means limit of given function does not exist. Am i right? please give your valuable suggestions!",,"['calculus', 'real-analysis', 'limits', 'functions']"
31,"taking the limit of $f(x)$, questions","taking the limit of , questions",f(x),How do I take the limit of the following functions? I had included some of my thoughts with them. $\lim_{x\to\infty}\dfrac{4x^3 - 2x + 1}{8x^3 + \sin(x^2) - x^{-1}}$; my thoughts: I am not sure about the bottom since there are the sine function and $-1$ power $\lim_{x\to\infty}\dfrac{e^x}{x^{x-1}}$; my thoughts: isn't $e^x$ faster since $x^{x-1}$ is one power lower than the top? $\lim_{x\to\infty}\dfrac{x^x}{x^{x+1}}$; my thoughts: the bottom is faster since is one power higher than the top. Ty!,How do I take the limit of the following functions? I had included some of my thoughts with them. $\lim_{x\to\infty}\dfrac{4x^3 - 2x + 1}{8x^3 + \sin(x^2) - x^{-1}}$; my thoughts: I am not sure about the bottom since there are the sine function and $-1$ power $\lim_{x\to\infty}\dfrac{e^x}{x^{x-1}}$; my thoughts: isn't $e^x$ faster since $x^{x-1}$ is one power lower than the top? $\lim_{x\to\infty}\dfrac{x^x}{x^{x+1}}$; my thoughts: the bottom is faster since is one power higher than the top. Ty!,,"['calculus', 'limits']"
32,How can we prove that $\displaystyle \limsup_{n \to \infty } b_n \le \limsup_{n \to \infty } a_n$,How can we prove that,\displaystyle \limsup_{n \to \infty } b_n \le \limsup_{n \to \infty } a_n,"Let $(a_n)_{n\ge1}$ be a bounded sequence in $\mathbb{R}$ Set $\displaystyle B_n=\left\{\sum_{j=n}^{\infty}(\theta_j\cdot a_j)\;:\theta_j\ge 0 \;,\;\sum_{j=n}^{\infty}\theta_j=1\right\}$ for $n\ge1$ Let $(b_n)_{n\ge 1}$ be a sequence such that $b_n \in B_n$ , $\forall \;n\ge1$ How can we prove that $\displaystyle \limsup_{n \to \infty } b_n \le \limsup_{n \to \infty } a_n$ ? Any hints would be appreciated.","Let $(a_n)_{n\ge1}$ be a bounded sequence in $\mathbb{R}$ Set $\displaystyle B_n=\left\{\sum_{j=n}^{\infty}(\theta_j\cdot a_j)\;:\theta_j\ge 0 \;,\;\sum_{j=n}^{\infty}\theta_j=1\right\}$ for $n\ge1$ Let $(b_n)_{n\ge 1}$ be a sequence such that $b_n \in B_n$ , $\forall \;n\ge1$ How can we prove that $\displaystyle \limsup_{n \to \infty } b_n \le \limsup_{n \to \infty } a_n$ ? Any hints would be appreciated.",,"['real-analysis', 'limits', 'inequality', 'limsup-and-liminf']"
33,Calculate the limit using definite integrals: $\lim_{n\to\infty}2n\sum_{k=1}^n\frac1{(n+2k)^2}$,Calculate the limit using definite integrals:,\lim_{n\to\infty}2n\sum_{k=1}^n\frac1{(n+2k)^2},"Calculate the limit using definite integrals: $\lim_{n\to\infty}2n\sum_{k=1}^n\frac1{(n+2k)^2}$ Well, I started like this: $\lim_{n\to\infty}2n\sum_{k=1}^n\frac1{(n+2k)^2}=\lim_{n\to\infty}2n[\frac1{(n+2)^2}+...+\frac1{(n+2n)^2}]$ From this point and on i'm always getting stuck. I know i should use Riemann integral definition but i don't know how can i recognize which function am i looking at? or how can i recognize the correct $\Delta x$? or at least the ""borders"" of the definite integral? Say i presume $\Delta x=\frac1n$ (It's incorrect, Should be $\frac 2n$ - But why?), Then: $\lim_{n\to\infty}\frac1n[\frac{2n^2}{(n+2)^2}+...+\frac{2n^2}{(n+2n)^2}] $, Using limit on both of the 'edges' of the sum should give me the 'borders?' of the definite integral? As you can see i'm pretty much lost in this kind of questions, BTW: An explanation will be of much more use than only an answer to this one.","Calculate the limit using definite integrals: $\lim_{n\to\infty}2n\sum_{k=1}^n\frac1{(n+2k)^2}$ Well, I started like this: $\lim_{n\to\infty}2n\sum_{k=1}^n\frac1{(n+2k)^2}=\lim_{n\to\infty}2n[\frac1{(n+2)^2}+...+\frac1{(n+2n)^2}]$ From this point and on i'm always getting stuck. I know i should use Riemann integral definition but i don't know how can i recognize which function am i looking at? or how can i recognize the correct $\Delta x$? or at least the ""borders"" of the definite integral? Say i presume $\Delta x=\frac1n$ (It's incorrect, Should be $\frac 2n$ - But why?), Then: $\lim_{n\to\infty}\frac1n[\frac{2n^2}{(n+2)^2}+...+\frac{2n^2}{(n+2n)^2}] $, Using limit on both of the 'edges' of the sum should give me the 'borders?' of the definite integral? As you can see i'm pretty much lost in this kind of questions, BTW: An explanation will be of much more use than only an answer to this one.",,"['sequences-and-series', 'limits', 'definite-integrals']"
34,accumulation point of recursive sequence,accumulation point of recursive sequence,,"Given is a sequence with: $(a_0)=1$, $(a_1=1)$, $a_{n+2}=\frac{1+a_{n+1}}{a_n}$ I now have to show what the accumulation points are: I guess that the sequence is jumping from number to number like this: 1->1->2->3->2->1->1->2.. So the acc.points should indeed be ""1, 2 and 3"". Is this correct? If yes, how can I 'show' this? Furthermore: Is there any way to build some subsequence of $a_n$ that converges against ""1, 2 and 3"" ? Thank you :)","Given is a sequence with: $(a_0)=1$, $(a_1=1)$, $a_{n+2}=\frac{1+a_{n+1}}{a_n}$ I now have to show what the accumulation points are: I guess that the sequence is jumping from number to number like this: 1->1->2->3->2->1->1->2.. So the acc.points should indeed be ""1, 2 and 3"". Is this correct? If yes, how can I 'show' this? Furthermore: Is there any way to build some subsequence of $a_n$ that converges against ""1, 2 and 3"" ? Thank you :)",,['limits']
35,"I need to find the value of $a,b \in \mathbb R$ such that the given limit is true",I need to find the value of  such that the given limit is true,"a,b \in \mathbb R","I am given that $\lim_{x \to \infty} \sqrt[3]{8x^3+ax^2}-bx=1$ need to find the value of $a,b \in \mathbb R$ such  that the given limit is true. I was able to work the whole thing out, but I have a question about one step in my work. There is a lot of rough work because I simplify by using the rule of the difference of cubes, so here is a condensed part of my work: $$\begin{align} \lim_{x \to \infty} \sqrt[3]{8x^3+ax^2}-bx &=\lim_{x \to \infty} \frac{8x^3+ax^2-b^3x^3}{(\sqrt[3]{8x^3+ax^2})^2+bx\sqrt[3]{8x^3+ax^2}+b^2x^2} \\&= \lim_{x \to \infty} \frac{8+a\frac{1}{x}-b^3}{\frac{1}{x^3}(\sqrt[3]{8x^3+ax^2})^2+b\frac{1}{x^2}\sqrt[3]{8x^3+ax^2}+b^2\frac{1}{x}} \\&=\frac{\lim_{x \to \infty}8+\lim_{x \to \infty}a\frac{1}{x}-\lim_{x \to \infty}b^3}{\lim_{x \to \infty}\frac{1}{x^3}(\sqrt[3]{8x^3+ax^2})^2+\lim_{x \to \infty}b\frac{1}{x^2}\sqrt[3]{8x^3+ax^2}+\lim_{x \to \infty}b^2\frac{1}{x}} \\&= \frac{8-b^3}{0+0+0} \\&= \frac{8-b^3}{0}\end{align}$$ Thus $8-n^3$ must also equal $0$ which implies that $b=2$. (This is the part I am unsure about. Is what I said true? If $b=2$ then this would give me an indeterminate form, but other than that I'm not sure if what I said holds, and if it does hold why does it hold?) Regardless of my uncertainty, I went on and using this assumption I found that $a=12$ in a similar manner, and when I check $\lim_{x \to \infty} \sqrt[3]{8x^3+12x^2}-2x$ it does equal $1$ . Any help as to why/why not my assumption is correct?  Thanks in advance! (If anyone wants me to post the method as to how i got 12 for $a$, let me know and then I'll type it up).","I am given that $\lim_{x \to \infty} \sqrt[3]{8x^3+ax^2}-bx=1$ need to find the value of $a,b \in \mathbb R$ such  that the given limit is true. I was able to work the whole thing out, but I have a question about one step in my work. There is a lot of rough work because I simplify by using the rule of the difference of cubes, so here is a condensed part of my work: $$\begin{align} \lim_{x \to \infty} \sqrt[3]{8x^3+ax^2}-bx &=\lim_{x \to \infty} \frac{8x^3+ax^2-b^3x^3}{(\sqrt[3]{8x^3+ax^2})^2+bx\sqrt[3]{8x^3+ax^2}+b^2x^2} \\&= \lim_{x \to \infty} \frac{8+a\frac{1}{x}-b^3}{\frac{1}{x^3}(\sqrt[3]{8x^3+ax^2})^2+b\frac{1}{x^2}\sqrt[3]{8x^3+ax^2}+b^2\frac{1}{x}} \\&=\frac{\lim_{x \to \infty}8+\lim_{x \to \infty}a\frac{1}{x}-\lim_{x \to \infty}b^3}{\lim_{x \to \infty}\frac{1}{x^3}(\sqrt[3]{8x^3+ax^2})^2+\lim_{x \to \infty}b\frac{1}{x^2}\sqrt[3]{8x^3+ax^2}+\lim_{x \to \infty}b^2\frac{1}{x}} \\&= \frac{8-b^3}{0+0+0} \\&= \frac{8-b^3}{0}\end{align}$$ Thus $8-n^3$ must also equal $0$ which implies that $b=2$. (This is the part I am unsure about. Is what I said true? If $b=2$ then this would give me an indeterminate form, but other than that I'm not sure if what I said holds, and if it does hold why does it hold?) Regardless of my uncertainty, I went on and using this assumption I found that $a=12$ in a similar manner, and when I check $\lim_{x \to \infty} \sqrt[3]{8x^3+12x^2}-2x$ it does equal $1$ . Any help as to why/why not my assumption is correct?  Thanks in advance! (If anyone wants me to post the method as to how i got 12 for $a$, let me know and then I'll type it up).",,"['real-analysis', 'limits']"
36,Does the equality $\lim_{x\rightarrow\infty} f(g(x)) = f(\lim_{x\rightarrow\infty} g(x))$ hold?,Does the equality  hold?,\lim_{x\rightarrow\infty} f(g(x)) = f(\lim_{x\rightarrow\infty} g(x)),"Does the equality $\lim_{x\rightarrow\infty} f(g(x)) = f(\lim_{x\rightarrow\infty} g(x))$ hold? If it is not always true, what is the condition that makes the equality hold?","Does the equality $\lim_{x\rightarrow\infty} f(g(x)) = f(\lim_{x\rightarrow\infty} g(x))$ hold? If it is not always true, what is the condition that makes the equality hold?",,['limits']
37,Calculate $\sum_{n=2}^\infty ({n^4+2n^3-3n^2-8n-3\over(n+2)!})$,Calculate,\sum_{n=2}^\infty ({n^4+2n^3-3n^2-8n-3\over(n+2)!}),"Calculate $\sum_{n=2}^\infty ({n^4+2n^3-3n^2-8n-3\over(n+2)!})$ I thought about maybe breaking the polynomial in two different fractions in order to make the sum more manageable and reduce it to something similar to $\lim_{n\to\infty}(1+{1\over1!}+{1\over2!}+...+{1\over n!})$, but didn't manage","Calculate $\sum_{n=2}^\infty ({n^4+2n^3-3n^2-8n-3\over(n+2)!})$ I thought about maybe breaking the polynomial in two different fractions in order to make the sum more manageable and reduce it to something similar to $\lim_{n\to\infty}(1+{1\over1!}+{1\over2!}+...+{1\over n!})$, but didn't manage",,"['calculus', 'limits']"
38,Improper integrals determine if they converge or diverge.,Improper integrals determine if they converge or diverge.,,The question is as follows. Determine if the these 2 improper integrals converge. $\int^{\infty}_{0} ( x^{1/2} +x^{3/2} )^{-1}$  And $\int^{\pi}_{0} (1-\cos(x))/(\sin^{2}(x))$ For the first integral we have problems at post infinity and $0$ clearly $1$ is ok so let's split it up into $\lim_{c\to 0} -\int^{c}_{1} ( x^{1/2} +x^{3/2} )^{-1}$ + $\lim_{d\to\infty}\int^{d}_{1} ( x^{1/2} +x^{3/2} )^{-1}$ now my problem being is I can't even evaluate these limits without integrating them clearly I could bound the first one with $x^{-2}$ when d goes to infinity as it both gets the bound much faster and  secondly $x^{-(1/2)}$ as $c$ does to $0$ as well as so we have $\lim_{c\to 0}$  $1/(c)^{1/2}$ this diverges so we are done? as for the second problem I am not sure how to approach it,The question is as follows. Determine if the these 2 improper integrals converge. $\int^{\infty}_{0} ( x^{1/2} +x^{3/2} )^{-1}$  And $\int^{\pi}_{0} (1-\cos(x))/(\sin^{2}(x))$ For the first integral we have problems at post infinity and $0$ clearly $1$ is ok so let's split it up into $\lim_{c\to 0} -\int^{c}_{1} ( x^{1/2} +x^{3/2} )^{-1}$ + $\lim_{d\to\infty}\int^{d}_{1} ( x^{1/2} +x^{3/2} )^{-1}$ now my problem being is I can't even evaluate these limits without integrating them clearly I could bound the first one with $x^{-2}$ when d goes to infinity as it both gets the bound much faster and  secondly $x^{-(1/2)}$ as $c$ does to $0$ as well as so we have $\lim_{c\to 0}$  $1/(c)^{1/2}$ this diverges so we are done? as for the second problem I am not sure how to approach it,,"['calculus', 'limits', 'integration', 'improper-integrals']"
39,Multivariable limit problem,Multivariable limit problem,,"$$\lim_{(x,y)\to 0,0}\frac{e^{xy}-1}y$$ I'm new to this. I do not think you can use epsilon-delta def. I'm also confident that the limit approaches $0$. I just don't know how to prove it.","$$\lim_{(x,y)\to 0,0}\frac{e^{xy}-1}y$$ I'm new to this. I do not think you can use epsilon-delta def. I'm also confident that the limit approaches $0$. I just don't know how to prove it.",,"['limits', 'multivariable-calculus']"
40,A multiple integral question II,A multiple integral question II,,We know from the previous post that  $$\lim_{n\to\infty}\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text{ times}}\frac{1}{(x_1\cdot x_2\cdots x_n)^2+1} \mathrm{d}x_1\cdot\mathrm{d}x_2\cdots\mathrm{d}x_n=1$$ and now I wonder what is $f(n)$ such that $$\lim_{n\to\infty}f(n)\left(1-\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text{ times}}\frac{1}{(x_1\cdot x_2\cdots x_n)^2+1} \mathrm{d}x_1\cdot\mathrm{d}x_2\cdots\mathrm{d}x_n\right)$$ is finite and then I'd like to find out its precise limit.,We know from the previous post that  $$\lim_{n\to\infty}\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text{ times}}\frac{1}{(x_1\cdot x_2\cdots x_n)^2+1} \mathrm{d}x_1\cdot\mathrm{d}x_2\cdots\mathrm{d}x_n=1$$ and now I wonder what is $f(n)$ such that $$\lim_{n\to\infty}f(n)\left(1-\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text{ times}}\frac{1}{(x_1\cdot x_2\cdots x_n)^2+1} \mathrm{d}x_1\cdot\mathrm{d}x_2\cdots\mathrm{d}x_n\right)$$ is finite and then I'd like to find out its precise limit.,,"['calculus', 'real-analysis', 'limits', 'integration', 'definite-integrals']"
41,Limit question factoring [duplicate],Limit question factoring [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: How do I find the delta analytically for $f(x)$ with a degree other than $1$ There is a question, prove that: $$\displaystyle \lim_{x\to3} x^{2} = 9$$ in linear problems of this type I simplify $0<|f(x)-L| \lt \epsilon$ to look like  $0 \lt |x-a| \lt \delta$. Example:  $$\displaystyle \lim_{x\to3} 4x - 5 = 7$$ ends up $0 \lt |x-3| \lt \delta$ then $|(4x-5)-7| \lt e$ ends up $ 0 \lt |x-3| \lt \delta$ then $|x-3| \lt \epsilon/4$ then $\delta = \epsilon/4$ But with quadratics I have a problem with the factoring/simplification... step 1. $|x^{2}-9| \lt \epsilon$ step 2. $|(x+3)(x-3)| \lt \epsilon$ now I know that I have to make $|(x+3)(x-3)| \lt \epsilon$  look like $0 \lt |x-3| \lt \delta$ but I don't know how to go about it.  Any help would be appreciated!","This question already has answers here : Closed 11 years ago . Possible Duplicate: How do I find the delta analytically for $f(x)$ with a degree other than $1$ There is a question, prove that: $$\displaystyle \lim_{x\to3} x^{2} = 9$$ in linear problems of this type I simplify $0<|f(x)-L| \lt \epsilon$ to look like  $0 \lt |x-a| \lt \delta$. Example:  $$\displaystyle \lim_{x\to3} 4x - 5 = 7$$ ends up $0 \lt |x-3| \lt \delta$ then $|(4x-5)-7| \lt e$ ends up $ 0 \lt |x-3| \lt \delta$ then $|x-3| \lt \epsilon/4$ then $\delta = \epsilon/4$ But with quadratics I have a problem with the factoring/simplification... step 1. $|x^{2}-9| \lt \epsilon$ step 2. $|(x+3)(x-3)| \lt \epsilon$ now I know that I have to make $|(x+3)(x-3)| \lt \epsilon$  look like $0 \lt |x-3| \lt \delta$ but I don't know how to go about it.  Any help would be appreciated!",,"['limits', 'factoring']"
42,"Limit of ""simple"" function","Limit of ""simple"" function",,I am trying to solve: $$\lim_{x\to 0} ({1 - \sin x})^{\cot2x}$$ I know that this could be solved by different methods. Can anyone summarize the methods and give me some references to read? Thanks! PS: This is the way I started first $$\lim_{x\to 0} ({1 - \sin x})^{\cot2x} = \lim_{x\to 0} e^{(1 - \sin x)\ln{\cot2x}} = \lim_{x\to 0} \frac{\ln{\cot{2x}}}{\frac{1}{1 - \sin x}}$$ Then I am trying Leibniz's theorem but I end up nowhere.,I am trying to solve: $$\lim_{x\to 0} ({1 - \sin x})^{\cot2x}$$ I know that this could be solved by different methods. Can anyone summarize the methods and give me some references to read? Thanks! PS: This is the way I started first $$\lim_{x\to 0} ({1 - \sin x})^{\cot2x} = \lim_{x\to 0} e^{(1 - \sin x)\ln{\cot2x}} = \lim_{x\to 0} \frac{\ln{\cot{2x}}}{\frac{1}{1 - \sin x}}$$ Then I am trying Leibniz's theorem but I end up nowhere.,,"['limits', 'functions']"
43,Interpreting a limit as a derivative,Interpreting a limit as a derivative,,"I am being stumped by the following question: Evaluate the limit by interpreting each as a derivative: $$\lim_{x\,\to\tfrac{\pi}{6}} \frac{\cos(2x) - \frac{1}{2}}{x - \frac{\pi}{6}}$$ The only way I can think to solve this is using L'Hopital's rule. I have done that and got the correct answer $-\sqrt{3}$. But, I can not figure out how to do it the was it is described. Any help would be greatly appreciated.","I am being stumped by the following question: Evaluate the limit by interpreting each as a derivative: $$\lim_{x\,\to\tfrac{\pi}{6}} \frac{\cos(2x) - \frac{1}{2}}{x - \frac{\pi}{6}}$$ The only way I can think to solve this is using L'Hopital's rule. I have done that and got the correct answer $-\sqrt{3}$. But, I can not figure out how to do it the was it is described. Any help would be greatly appreciated.",,"['calculus', 'limits']"
44,Limit with square and cube root difference $\lim_{n \to \infty} \left(\sqrt{n^2 + n} - \sqrt[3]{n^3 + n^2}\right)$,Limit with square and cube root difference,\lim_{n \to \infty} \left(\sqrt{n^2 + n} - \sqrt[3]{n^3 + n^2}\right),"I've been banging my head at this one for the last hour and I can't seem to find the solution. I've read through almost all of the limit problems that were asked to be solved here and the tricks used just don't work on this one, or maybe I'm just not seeing it. $\lim_{n \to \infty} \left(\sqrt{n^2 + n} - \sqrt[3]{n^3 + n^2}\right)$ Of course the solution has to be done without using the L'Hospital's rule or series expansion. I've used the $a^3 - b^3 = (a - b)(a^2 + ab + b^2)$ identity and $a^2 - b^2 = (a + b)(a - b)$ and got nowhere near the solution -> $\frac{1}{6}$. I even tried using the $(1 + \frac{1}{n})^n$ and played with exponents but got a solution -> $\dfrac{1 - e^{2/3}}{2}$ which told me I'm definitely doing something wrong.","I've been banging my head at this one for the last hour and I can't seem to find the solution. I've read through almost all of the limit problems that were asked to be solved here and the tricks used just don't work on this one, or maybe I'm just not seeing it. $\lim_{n \to \infty} \left(\sqrt{n^2 + n} - \sqrt[3]{n^3 + n^2}\right)$ Of course the solution has to be done without using the L'Hospital's rule or series expansion. I've used the $a^3 - b^3 = (a - b)(a^2 + ab + b^2)$ identity and $a^2 - b^2 = (a + b)(a - b)$ and got nowhere near the solution -> $\frac{1}{6}$. I even tried using the $(1 + \frac{1}{n})^n$ and played with exponents but got a solution -> $\dfrac{1 - e^{2/3}}{2}$ which told me I'm definitely doing something wrong.",,['calculus']
45,Evaluate: $\lim_{h \to 0} \int_{-1}^{1}\frac{h}{h^2+x^2}~dx$,Evaluate:,\lim_{h \to 0} \int_{-1}^{1}\frac{h}{h^2+x^2}~dx,How can I evaluate: $$\lim_{h \to 0} \int_{-1}^{1}\frac{h}{h^2+x^2}~dx$$ How I proceed: $$\lim_{h \to 0} \int_{-1}^{1}\frac{h}{h^2+x^2}~dx=2\lim_{h \to 0} \frac{1}{h}\int_{0}^{1}\frac{1}{1+(\frac{x}{h})^2}~dx=2\lim_{h \to 0}\frac{1}{h}\arctan\frac{1}{h}$$  Then how can I prooceed. Please help. Thank in advance.,How can I evaluate: $$\lim_{h \to 0} \int_{-1}^{1}\frac{h}{h^2+x^2}~dx$$ How I proceed: $$\lim_{h \to 0} \int_{-1}^{1}\frac{h}{h^2+x^2}~dx=2\lim_{h \to 0} \frac{1}{h}\int_{0}^{1}\frac{1}{1+(\frac{x}{h})^2}~dx=2\lim_{h \to 0}\frac{1}{h}\arctan\frac{1}{h}$$  Then how can I prooceed. Please help. Thank in advance.,,"['real-analysis', 'limits', 'integration']"
46,Exercise about MacLaurin's polynomial and small-o,Exercise about MacLaurin's polynomial and small-o,,"In class the professor wrote the following limit: $\lim_{x\to 0} \frac{\sinh^2 (x) -x^2}{x^4}$ So he  ""expanded"" (sorry for my English) the MacLaurin's formula for $\sinh x$ up to the 3rd power, and got: $x + \frac{x^3}{3!} + o(x^4)$ When he squared the MacLaurin's polynomial, he wrote the following steps: $(\sinh x)^2 = (x + \frac{x^3}{3!} + o(x^4))^2 = x^2 + (\frac{x^3}{3!})^2 + (o(x^4))^2 + 2x\frac{x^3}{3!} + 2xo(x^4) + 2\frac{x^3}{3!}o(x^4) = x^2 + \frac{x^4}{3} + o(x^5)$ Now, my question is: why he used $o(x^5)$? As far as I know, when $x\to0$, you have to consider the ""$o$"" with the highest power, because there is a $o(x^7)$ and an $o(x^8)$ Thank you!","In class the professor wrote the following limit: $\lim_{x\to 0} \frac{\sinh^2 (x) -x^2}{x^4}$ So he  ""expanded"" (sorry for my English) the MacLaurin's formula for $\sinh x$ up to the 3rd power, and got: $x + \frac{x^3}{3!} + o(x^4)$ When he squared the MacLaurin's polynomial, he wrote the following steps: $(\sinh x)^2 = (x + \frac{x^3}{3!} + o(x^4))^2 = x^2 + (\frac{x^3}{3!})^2 + (o(x^4))^2 + 2x\frac{x^3}{3!} + 2xo(x^4) + 2\frac{x^3}{3!}o(x^4) = x^2 + \frac{x^4}{3} + o(x^5)$ Now, my question is: why he used $o(x^5)$? As far as I know, when $x\to0$, you have to consider the ""$o$"" with the highest power, because there is a $o(x^7)$ and an $o(x^8)$ Thank you!",,"['limits', 'asymptotics', 'taylor-expansion']"
47,Proving $\lim\limits_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n}=L$ implies $\lim\limits_{n\to \infty}\frac{a_n}{b_n}=L$,Proving  implies,\lim\limits_{n\to\infty}\frac{a_{n+1}-a_n}{b_{n+1}-b_n}=L \lim\limits_{n\to \infty}\frac{a_n}{b_n}=L,"Suppose $(a_n)$ and $(b_n)$ are sequences where $b_n$ is increasing and approaching positive infinity. Assume that $\lim_{n\to \infty}$ $\frac{a_{n+1}-a_n}{b_{n+1}-b_n}=L$, where $L$ is a real number. Prove that $\lim_{n\to \infty}$ $\frac{a_n}{b_n}=L$.","Suppose $(a_n)$ and $(b_n)$ are sequences where $b_n$ is increasing and approaching positive infinity. Assume that $\lim_{n\to \infty}$ $\frac{a_{n+1}-a_n}{b_{n+1}-b_n}=L$, where $L$ is a real number. Prove that $\lim_{n\to \infty}$ $\frac{a_n}{b_n}=L$.",,"['analysis', 'limits']"
48,calculate $\lim_{n\rightarrow\infty} n\left ( \ln(n^2 +1) -2\ln(n)\sqrt[n]{\ln(n)}\right )$,calculate,\lim_{n\rightarrow\infty} n\left ( \ln(n^2 +1) -2\ln(n)\sqrt[n]{\ln(n)}\right ),"As above, I have to calculate $$\lim_{n\rightarrow\infty} n\left  (  \ln(n^2 +1) -2\ln(n) \sqrt[n]{\ln(n)} \right )$$I tried to multiply it by $$ \frac{\left  (  \ln(n^2 +1) +2\ln(n)\sqrt[n]{\ln(n)}\right )}{\left  (  \ln(n^2 +1) +2\ln(n)\sqrt[n]{\ln(n)}\right )}$$ so that it is $$\lim_{n\rightarrow\infty} n \frac{\left  (  \ln^2(n^2 +1) -2\ln^2 (n) \sqrt[\frac{1}{2}n]{\ln(n)} \right )}{\left  (  \ln(n^2 +1) +2\ln(n)\sqrt[n]{\ln(n)}\right )},$$but it is not so easy to play with (first glance says that it goes to infinity), nevertheless, can somebody take a closer look at this and suggest something, please? I would be very grateful.","As above, I have to calculate $$\lim_{n\rightarrow\infty} n\left  (  \ln(n^2 +1) -2\ln(n) \sqrt[n]{\ln(n)} \right )$$I tried to multiply it by $$ \frac{\left  (  \ln(n^2 +1) +2\ln(n)\sqrt[n]{\ln(n)}\right )}{\left  (  \ln(n^2 +1) +2\ln(n)\sqrt[n]{\ln(n)}\right )}$$ so that it is $$\lim_{n\rightarrow\infty} n \frac{\left  (  \ln^2(n^2 +1) -2\ln^2 (n) \sqrt[\frac{1}{2}n]{\ln(n)} \right )}{\left  (  \ln(n^2 +1) +2\ln(n)\sqrt[n]{\ln(n)}\right )},$$but it is not so easy to play with (first glance says that it goes to infinity), nevertheless, can somebody take a closer look at this and suggest something, please? I would be very grateful.",,"['real-analysis', 'limits']"
49,Unclear step on proof of Laplace transform of a derivative,Unclear step on proof of Laplace transform of a derivative,,"Reading the article on the Laplace Transform in Wolfram MathWorld , I found the proof that $\mathcal{L}[f'(t)] = sF(s) - f(0)$. I understand the first and second steps, but I don't understand the third one. Why is it that $lim_{a \to \infty} [e^{-sa} f(a)] = 0$? $e^{-sa}$ does get closer to 0 when $-sa$ approaches to $\infty$, but why does $f(a)$ get closer to 0? As far as I know, $f(a)$ could be anything, so it could be possible that $lim_{x \to \infty} f(a)$ doesn't exist. What guarantees that the limit of $f(a)$ always exists? I hope I'm not missing some simple property of limits here.","Reading the article on the Laplace Transform in Wolfram MathWorld , I found the proof that $\mathcal{L}[f'(t)] = sF(s) - f(0)$. I understand the first and second steps, but I don't understand the third one. Why is it that $lim_{a \to \infty} [e^{-sa} f(a)] = 0$? $e^{-sa}$ does get closer to 0 when $-sa$ approaches to $\infty$, but why does $f(a)$ get closer to 0? As far as I know, $f(a)$ could be anything, so it could be possible that $lim_{x \to \infty} f(a)$ doesn't exist. What guarantees that the limit of $f(a)$ always exists? I hope I'm not missing some simple property of limits here.",,"['limits', 'proof-writing', 'laplace-transform']"
50,limit of inverse trigonometric function,limit of inverse trigonometric function,,I have the following limit that I am trying to solve but apparently I am stuck in doing l'Hôpital's rule and going nowhere so any help would be appreciated $$\lim_{x\to 0} \frac{\arcsin {x}^2}{(\arcsin x)^2}$$ Thank you,I have the following limit that I am trying to solve but apparently I am stuck in doing l'Hôpital's rule and going nowhere so any help would be appreciated $$\lim_{x\to 0} \frac{\arcsin {x}^2}{(\arcsin x)^2}$$ Thank you,,"['trigonometry', 'limits']"
51,Limit Inf/Sup of Sequence of Set Example,Limit Inf/Sup of Sequence of Set Example,,"In ""A Probability Path"", they have an example that states that the lim inf and lim sup of [0,n/(n+1)) is equal to [0,1). I guess I don't see how [0,1) is in all the sets except a finite number of ties or how it is in an infinite number of sets. Can someone give give a demonstration of why it is so?","In ""A Probability Path"", they have an example that states that the lim inf and lim sup of [0,n/(n+1)) is equal to [0,1). I guess I don't see how [0,1) is in all the sets except a finite number of ties or how it is in an infinite number of sets. Can someone give give a demonstration of why it is so?",,"['measure-theory', 'elementary-set-theory', 'limits']"
52,Calculating: $\lim_{n\to \infty}\int_0^\sqrt{n} {(1-\frac{x^2}{n})^n}dx$ [duplicate],Calculating:  [duplicate],\lim_{n\to \infty}\int_0^\sqrt{n} {(1-\frac{x^2}{n})^n}dx,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Prove: $\lim\limits_{n \to \infty} \int_{0}^{\sqrt n}(1-\frac{x^2}{n})^ndx=\int_{0}^{\infty} e^{-x^2}dx$ I need some help calculating the above limit. What i have observed so far is that: For all x the limit of the sequence inside the integral as n tends to infinity is $e^{-x^2}$ I can use Dini's theorem to show that: $f_n(x) = {(1-\frac{x^2}{n})^n}$ uniformly converges to $e^{-x^2}$ Consequently i can use the theorem regarding ""the integral of the limit is the limit of integrals"" for the function sequence $f_n(x)$ and it's limit function $f(x) = e^{-x^2}$ All this is well and good, but i would be ignoring the fact that the interval integrated upon is [0,$\sqrt{n}$], and so also affected by the limit. What should i do to resolve this?","This question already has answers here : Closed 11 years ago . Possible Duplicate: Prove: $\lim\limits_{n \to \infty} \int_{0}^{\sqrt n}(1-\frac{x^2}{n})^ndx=\int_{0}^{\infty} e^{-x^2}dx$ I need some help calculating the above limit. What i have observed so far is that: For all x the limit of the sequence inside the integral as n tends to infinity is $e^{-x^2}$ I can use Dini's theorem to show that: $f_n(x) = {(1-\frac{x^2}{n})^n}$ uniformly converges to $e^{-x^2}$ Consequently i can use the theorem regarding ""the integral of the limit is the limit of integrals"" for the function sequence $f_n(x)$ and it's limit function $f(x) = e^{-x^2}$ All this is well and good, but i would be ignoring the fact that the interval integrated upon is [0,$\sqrt{n}$], and so also affected by the limit. What should i do to resolve this?",,"['sequences-and-series', 'integration', 'limits', 'convergence-divergence', 'improper-integrals']"
53,Power rule for the limits of convergent sequences,Power rule for the limits of convergent sequences,,"The ""Power Rule"" for null sequences states that If a null sequence of non-negative terms is raised to a positive   power, the resulting sequence is also a null sequence. Ok, can this rule be generalised to the following? If a sequence of non-negative terms that converges to the limit $l$ is   raised to a positive power $p$, the resulting sequence converges to   the limit $l^p$.","The ""Power Rule"" for null sequences states that If a null sequence of non-negative terms is raised to a positive   power, the resulting sequence is also a null sequence. Ok, can this rule be generalised to the following? If a sequence of non-negative terms that converges to the limit $l$ is   raised to a positive power $p$, the resulting sequence converges to   the limit $l^p$.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
54,Inductive Sequence Within Inductive Sequence Limit Question,Inductive Sequence Within Inductive Sequence Limit Question,,"Given $a,b>0$ let $\{a_n\}$ and $\{b_n\}$ be sequences defined as follows: $a_1=a, b_1=b,a_{n+1}=\frac{a_n+b_n}{2},b_{n+1}=\sqrt{a_nb_n}$ Prove that the sequences converge and that their limits are equal. I don't know how to begin to solve this question because it's the first time I encounter with a sequence defined by another inductive sequence. When I see an inductive sequence the tool I use is to show that the sequence is monotonic increasing/decreasing and that it's bounded and then use limit arithmetic to calculate the limit. Thank you very much for your time and help.","Given $a,b>0$ let $\{a_n\}$ and $\{b_n\}$ be sequences defined as follows: $a_1=a, b_1=b,a_{n+1}=\frac{a_n+b_n}{2},b_{n+1}=\sqrt{a_nb_n}$ Prove that the sequences converge and that their limits are equal. I don't know how to begin to solve this question because it's the first time I encounter with a sequence defined by another inductive sequence. When I see an inductive sequence the tool I use is to show that the sequence is monotonic increasing/decreasing and that it's bounded and then use limit arithmetic to calculate the limit. Thank you very much for your time and help.",,"['calculus', 'real-analysis', 'limits']"
55,Basic Questions About Inductive Sequences,Basic Questions About Inductive Sequences,,"Given a sequence $(X_n)$ defined as follows: $X_1>0$ and $\forall n, X_{n+1}=\frac{1}{2}(X_n+\frac{b}{X_n})$ what do I need to think about when I see the notion of $X_{n+1}$? Should I think $X_{n+1}$ as a sequence? or just a way to defined the next element of the sequence $X_n$? I'm asking this because I saw that the limit of $X_{n+1}$ is equal to the limit of $X_n$ and the limit is defined for sequences. Why are the limits equal? Can I talk about the sequence $X_n$ and $X_{n+1}$ interchangeably? If so, why? Furthermore, in a lecture I saw involving the same sequences above, in order to find out if the sequence $X_n$ is decreasing we evaluted the expression $X_{n+1}-X_n$ which is equal to $\frac{-(X_n)^2+b}{2X_n}$ However, because we don't have a formula for $X_n$ he developed $\frac{-(X_{n+1})^2+b}{2X_n}$ instead and then he concluded that  $\forall n, b-(X_{n+1})^2 \le 0 $  from that he concluded that $\forall n \le2$,   $b-(X_n)^2 \le 0$ why is that correct? I'm quite confused - could you please help me to understand this fundamental concept? Thank you very much for your time and help.","Given a sequence $(X_n)$ defined as follows: $X_1>0$ and $\forall n, X_{n+1}=\frac{1}{2}(X_n+\frac{b}{X_n})$ what do I need to think about when I see the notion of $X_{n+1}$? Should I think $X_{n+1}$ as a sequence? or just a way to defined the next element of the sequence $X_n$? I'm asking this because I saw that the limit of $X_{n+1}$ is equal to the limit of $X_n$ and the limit is defined for sequences. Why are the limits equal? Can I talk about the sequence $X_n$ and $X_{n+1}$ interchangeably? If so, why? Furthermore, in a lecture I saw involving the same sequences above, in order to find out if the sequence $X_n$ is decreasing we evaluted the expression $X_{n+1}-X_n$ which is equal to $\frac{-(X_n)^2+b}{2X_n}$ However, because we don't have a formula for $X_n$ he developed $\frac{-(X_{n+1})^2+b}{2X_n}$ instead and then he concluded that  $\forall n, b-(X_{n+1})^2 \le 0 $  from that he concluded that $\forall n \le2$,   $b-(X_n)^2 \le 0$ why is that correct? I'm quite confused - could you please help me to understand this fundamental concept? Thank you very much for your time and help.",,"['real-analysis', 'limits']"
56,"A pair of mutually ""bounding"" sequences.","A pair of mutually ""bounding"" sequences.",,"Given $({a_n})_{n=1}^{\infty}$, $({b_n})_{n=1}^{\infty}$ convergent sequences and where $$\{n\in\mathbb{N}\mid a_n\le b_n\}\quad\text{and}\quad\{n\in\mathbb{N}\mid b_n\le a_n\}$$ are both unbounded, prove that  $$\lim \limits_{n\to \infty}a_n=\lim \limits_{n\to \infty}b_n$$ I would like to know how I can prove it using simple calculus theorem(I only know the definition of limit, arithmetics of limits and the Squeeze Theorem). Thank you very much for your time and help.","Given $({a_n})_{n=1}^{\infty}$, $({b_n})_{n=1}^{\infty}$ convergent sequences and where $$\{n\in\mathbb{N}\mid a_n\le b_n\}\quad\text{and}\quad\{n\in\mathbb{N}\mid b_n\le a_n\}$$ are both unbounded, prove that  $$\lim \limits_{n\to \infty}a_n=\lim \limits_{n\to \infty}b_n$$ I would like to know how I can prove it using simple calculus theorem(I only know the definition of limit, arithmetics of limits and the Squeeze Theorem). Thank you very much for your time and help.",,"['real-analysis', 'limits']"
57,Is it true that $\frac{n}{2}=\Theta(n)$?,Is it true that ?,\frac{n}{2}=\Theta(n),"This is probably a very silly question. If $h(n)=\frac{n}{2}, \ g(n)=n$, so $$ \lim_{n \to \infty} \frac{h(n)}{g(n)} = \lim_{n \to \infty} \frac{n}{2n}=\frac{1}{2} $$ so $h(n) \leq C_1 g(n), h(n)=O(n)$ at the same time , if $g(n)=\frac{n}{10}$, this limit becomes 5, so $h(n) \geq C_2 g(n), h(n)= \Omega(n)$ Is this logic correct enough to say that  $$ \frac{n}{2}=\Theta(n) $$","This is probably a very silly question. If $h(n)=\frac{n}{2}, \ g(n)=n$, so $$ \lim_{n \to \infty} \frac{h(n)}{g(n)} = \lim_{n \to \infty} \frac{n}{2n}=\frac{1}{2} $$ so $h(n) \leq C_1 g(n), h(n)=O(n)$ at the same time , if $g(n)=\frac{n}{10}$, this limit becomes 5, so $h(n) \geq C_2 g(n), h(n)= \Omega(n)$ Is this logic correct enough to say that  $$ \frac{n}{2}=\Theta(n) $$",,"['limits', 'asymptotics']"
58,Limit of a function satisfying an inequality,Limit of a function satisfying an inequality,,"If $f(x)+f(y)\leq f(x+y)$ and $f:\mathbb{R}\to\mathbb{R}$, then can we find $\lim_{x\to 0} \frac {f(x)}{x}$? I am not sure whether the question is correct.Thank you.(I tried this idea: $f(x)=f(x+y-y)\ge f(x+y)+f(-y)\ge f(x)+f(y)+f(-y)\implies f(y)\leq -f(-y)$ but after that I seem to be hitting a roadblock. Thank you in advance.","If $f(x)+f(y)\leq f(x+y)$ and $f:\mathbb{R}\to\mathbb{R}$, then can we find $\lim_{x\to 0} \frac {f(x)}{x}$? I am not sure whether the question is correct.Thank you.(I tried this idea: $f(x)=f(x+y-y)\ge f(x+y)+f(-y)\ge f(x)+f(y)+f(-y)\implies f(y)\leq -f(-y)$ but after that I seem to be hitting a roadblock. Thank you in advance.",,"['calculus', 'limits', 'inequality', 'functional-equations', 'functional-inequalities']"
59,proof of l'Hôpital's rule that minimizes special-casing,proof of l'Hôpital's rule that minimizes special-casing,,"A simple form of l'Hôpital's rule looks like this: If $u$ and $v$ are functions with $u(0)=0$ and $v(0)=0$, the derivatives $\dot{v}(0)$ and $\dot{v}(0)$ are defined, and the derivative $\dot{v}(0)\ne 0$, then \begin{align*}   \lim_{x\rightarrow 0} \frac{u}{v} &= \frac{\dot{u}(0)}{\dot{v}(0)} \qquad . \end{align*} To me, the clearest way to arrive at this result uses a little nonstandard analysis: Since $u(0)=0$, and the derivative $d u/d x$ is defined at $0$, $u(d x)=d u$ is infinitesimal, and likewise for $v$. By the definition of the limit, the limit is the standard part of \begin{equation*}   \frac{u}{v} = \frac{d u}{d v} = \frac{d u/d x}{d v/d x} \qquad , \end{equation*} where by assumption the numerator and denominator are both defined (and finite, because the derivative is defined in terms of the standard part). The standard part of a quotient like $p/q$ equals the quotient of the standard parts, provided that both $p$ and $q$ are finite (which we've established), and $q \ne 0$ (which is true by assumption). But the standard part of $d u/d x$ is the definition of the derivative $\dot{u}$, and likewise for  $d v/d x$, so this establishes the result. The generalizations to $x\rightarrow a$, where $a\ne 0$, and $x\rightarrow \infty$ are pretty trivial with the changes of variable $x\rightarrow x-a$ and $x\rightarrow 1/x$. But there are a bunch of other cases of l'Hôpital's that seem to me to involve toxic doses of case-splitting. There are cases where you have to differentiate more than once, and cases where the indeterminate form is $\infty/\infty$ rather than $0/0$. Is it possible to treat all of this in a unified way, possibly using ideas from projective geometry or inversions with respect to a circle in the complex plane?","A simple form of l'Hôpital's rule looks like this: If $u$ and $v$ are functions with $u(0)=0$ and $v(0)=0$, the derivatives $\dot{v}(0)$ and $\dot{v}(0)$ are defined, and the derivative $\dot{v}(0)\ne 0$, then \begin{align*}   \lim_{x\rightarrow 0} \frac{u}{v} &= \frac{\dot{u}(0)}{\dot{v}(0)} \qquad . \end{align*} To me, the clearest way to arrive at this result uses a little nonstandard analysis: Since $u(0)=0$, and the derivative $d u/d x$ is defined at $0$, $u(d x)=d u$ is infinitesimal, and likewise for $v$. By the definition of the limit, the limit is the standard part of \begin{equation*}   \frac{u}{v} = \frac{d u}{d v} = \frac{d u/d x}{d v/d x} \qquad , \end{equation*} where by assumption the numerator and denominator are both defined (and finite, because the derivative is defined in terms of the standard part). The standard part of a quotient like $p/q$ equals the quotient of the standard parts, provided that both $p$ and $q$ are finite (which we've established), and $q \ne 0$ (which is true by assumption). But the standard part of $d u/d x$ is the definition of the derivative $\dot{u}$, and likewise for  $d v/d x$, so this establishes the result. The generalizations to $x\rightarrow a$, where $a\ne 0$, and $x\rightarrow \infty$ are pretty trivial with the changes of variable $x\rightarrow x-a$ and $x\rightarrow 1/x$. But there are a bunch of other cases of l'Hôpital's that seem to me to involve toxic doses of case-splitting. There are cases where you have to differentiate more than once, and cases where the indeterminate form is $\infty/\infty$ rather than $0/0$. Is it possible to treat all of this in a unified way, possibly using ideas from projective geometry or inversions with respect to a circle in the complex plane?",,['calculus']
60,Does anyone know l'Hôpital's rule for limits?,Does anyone know l'Hôpital's rule for limits?,,I was doing an assignment and the middle 2 questions are l'Hôpital's rule questions and we haven't even done this in class yet. These are 1 or 2 chapters away yet we are expected to do them and I have no idea how to do these because we haven't learned yet so I was hoping someone here can help me. First question: An incorrect use of l'Hôpital's rule is illustrated in the following limit computation. Explain what is wrong and find the correct value of the limit. $$\lim_{x\to\pi/2}   \frac{\sin(x)}{x}    = \lim_{x\to\pi/2}  \frac{\cos(x)}{1}=0$$ Second question is: Find $\lim\limits_{x\to-\infty} xe^x$ using l'Hôpital's rule. Using this information does the function $xe^x$ have a horizontal or vertical asymptote? state the equation of the asymptote. I have done about 15 pages worth of questions and some I knew because I studied extra things on spare time but these are just hard.,I was doing an assignment and the middle 2 questions are l'Hôpital's rule questions and we haven't even done this in class yet. These are 1 or 2 chapters away yet we are expected to do them and I have no idea how to do these because we haven't learned yet so I was hoping someone here can help me. First question: An incorrect use of l'Hôpital's rule is illustrated in the following limit computation. Explain what is wrong and find the correct value of the limit. Second question is: Find using l'Hôpital's rule. Using this information does the function have a horizontal or vertical asymptote? state the equation of the asymptote. I have done about 15 pages worth of questions and some I knew because I studied extra things on spare time but these are just hard.,\lim_{x\to\pi/2}   \frac{\sin(x)}{x}    = \lim_{x\to\pi/2}  \frac{\cos(x)}{1}=0 \lim\limits_{x\to-\infty} xe^x xe^x,"['calculus', 'algebra-precalculus', 'limits']"
61,Limits Involving Trigonometric Functions,Limits Involving Trigonometric Functions,,"I should prove using the limit definition that  $$\lim_{x \rightarrow 0} \, x^{1/3}\cos(1/x) = 0.$$ I have a problem because the second function is  much too complex, so I think I need transformation.  And what form this function could have in case  I will transform it?","I should prove using the limit definition that  $$\lim_{x \rightarrow 0} \, x^{1/3}\cos(1/x) = 0.$$ I have a problem because the second function is  much too complex, so I think I need transformation.  And what form this function could have in case  I will transform it?",,"['calculus', 'limits']"
62,"Convergence of $\lim_{n \to \infty} \frac{5 n^2 +\sin n}{3 (n+2)^2 \cos(\frac{n \pi}{5})},$",Convergence of,"\lim_{n \to \infty} \frac{5 n^2 +\sin n}{3 (n+2)^2 \cos(\frac{n \pi}{5})},","I'm in trouble with this limit. The numerator diverges positively, but I do not understand how to operate on the denominator. $$\lim_{n \to \infty} \frac{5 n^2 +\sin n}{3 (n+2)^2 \cos(\frac{n \pi}{5})},$$ $$\lim_{n \to \infty}  \frac{5 n^2 +\sin n}{3 (n+2)^2 \cos(\frac{n \pi}{5})}= \lim_{x\to\infty}\frac {n^2(5 +\frac{\sin n}{n^2})}{3 (n+2)^2 \cos(\frac{n \pi}{5})} \cdots$$","I'm in trouble with this limit. The numerator diverges positively, but I do not understand how to operate on the denominator. $$\lim_{n \to \infty} \frac{5 n^2 +\sin n}{3 (n+2)^2 \cos(\frac{n \pi}{5})},$$ $$\lim_{n \to \infty}  \frac{5 n^2 +\sin n}{3 (n+2)^2 \cos(\frac{n \pi}{5})}= \lim_{x\to\infty}\frac {n^2(5 +\frac{\sin n}{n^2})}{3 (n+2)^2 \cos(\frac{n \pi}{5})} \cdots$$",,"['calculus', 'sequences-and-series', 'limits']"
63,"Prove that $\sum\limits_{r=1}^n (sr^{-1} -r^{-3}) -s \log n$ has a limit as $n \to \infty$, $\forall s>1$","Prove that  has a limit as ,",\sum\limits_{r=1}^n (sr^{-1} -r^{-3}) -s \log n n \to \infty \forall s>1,"Prove that if $s >1$, then $\displaystyle {\sum_{r=1}^n} \left( {\frac{s}{r}- \frac{1}{r^3}} \right) - s \log n$ tends to a limit as $n \to \infty$ ($s$ being fixed), and that if this limit is $\phi (s)$ then [also show that] $0 \le \left[ {\phi (s) + \frac {1}{s-1}} \right] \le (s-1)$ From A Course of Mathematical Analysis by Shanti Narayan pp.332. Edit: Solved! $\displaystyle {\lim_{n \to \infty} \sum_{r=1}^n} \left( {\frac{s}{r}- \frac{1}{r^3}} \right) - s \log n$ $= \displaystyle {\lim_{n \to \infty} \sum_{r=1}^n} \left( {\frac{s}{r}} \right) - s \log n -\displaystyle {\lim_{n \to \infty} \sum_{r=1}^n} \left( {\frac{1}{r^3}} \right)$  $=s \gamma - \zeta(3)$ [$\gamma$ := Euler-Mascheroni constant]. Now the inequality, which I suppose to be wrong (but actually it is not), was proved using Mathematical Induction. Thanks to all.","Prove that if $s >1$, then $\displaystyle {\sum_{r=1}^n} \left( {\frac{s}{r}- \frac{1}{r^3}} \right) - s \log n$ tends to a limit as $n \to \infty$ ($s$ being fixed), and that if this limit is $\phi (s)$ then [also show that] $0 \le \left[ {\phi (s) + \frac {1}{s-1}} \right] \le (s-1)$ From A Course of Mathematical Analysis by Shanti Narayan pp.332. Edit: Solved! $\displaystyle {\lim_{n \to \infty} \sum_{r=1}^n} \left( {\frac{s}{r}- \frac{1}{r^3}} \right) - s \log n$ $= \displaystyle {\lim_{n \to \infty} \sum_{r=1}^n} \left( {\frac{s}{r}} \right) - s \log n -\displaystyle {\lim_{n \to \infty} \sum_{r=1}^n} \left( {\frac{1}{r^3}} \right)$  $=s \gamma - \zeta(3)$ [$\gamma$ := Euler-Mascheroni constant]. Now the inequality, which I suppose to be wrong (but actually it is not), was proved using Mathematical Induction. Thanks to all.",,"['calculus', 'limits', 'convergence-divergence']"
64,questions about limits and derivatives,questions about limits and derivatives,,"I am trying to solve a set of problems, this one is causing my some troubles. For the first one I tried to use the $\epsilon-\delta$ definition but I couldn't solve it, I would appreciate some hints for it. Let $f:(0,\infty) \to \mathbb{R}$ be a differentiable function such that $f'$ is continuous and $f(x) > 0$ for all $x \in (0,\infty)$. Prove or give a counter example for each of the following statements: if $\displaystyle\lim_{x\to 0^{+}}f(x)=0$ then $\displaystyle\lim_{x\to 0^{+}}f'(x)$ exists. if $\displaystyle\lim_{x\to\infty}f(x)=0$ then $\displaystyle\lim_{x\to\infty}f'(x) = 0$","I am trying to solve a set of problems, this one is causing my some troubles. For the first one I tried to use the $\epsilon-\delta$ definition but I couldn't solve it, I would appreciate some hints for it. Let $f:(0,\infty) \to \mathbb{R}$ be a differentiable function such that $f'$ is continuous and $f(x) > 0$ for all $x \in (0,\infty)$. Prove or give a counter example for each of the following statements: if $\displaystyle\lim_{x\to 0^{+}}f(x)=0$ then $\displaystyle\lim_{x\to 0^{+}}f'(x)$ exists. if $\displaystyle\lim_{x\to\infty}f(x)=0$ then $\displaystyle\lim_{x\to\infty}f'(x) = 0$",,"['calculus', 'limits']"
65,$\lim_{x\to 0}\arcsin\left(\frac{\arccos\left(x\right)}{\pi}+\frac{\arccos\left(x^{2}\right)}{\pi}\right)$,,\lim_{x\to 0}\arcsin\left(\frac{\arccos\left(x\right)}{\pi}+\frac{\arccos\left(x^{2}\right)}{\pi}\right),"I'm trying to evaluate this: $$\lim_{x\to 0}\arcsin\left(\frac{\arccos\left(x\right)}{\pi}+\frac{\arccos\left(x^{2}\right)}{\pi}\right)$$ Now, calculating the for $$\lim_{x\to 0}\frac{\arccos\left(x\right)}{\pi}$$ This should equal to $\frac{1}{2}$ And similarly, $$\lim_{x\to 0}\frac{\arccos\left(x^2\right)}{\pi}$$ should equal to $\frac{1}{2}$ . And if I understand correctly, $\lim_{x\to 0}$ returns the actual value, not approximately, but the actual value that the function is approaching. So, here $\frac{1}{2} + \frac{1}{2} = 1$ And, $$\arcsin\left(1\right) = \frac{\pi}{2}$$ which is my answer. Is this valid? The answer given in my textbook is that the limit is not defined. But, I don't understand that.","I'm trying to evaluate this: Now, calculating the for This should equal to And similarly, should equal to . And if I understand correctly, returns the actual value, not approximately, but the actual value that the function is approaching. So, here And, which is my answer. Is this valid? The answer given in my textbook is that the limit is not defined. But, I don't understand that.",\lim_{x\to 0}\arcsin\left(\frac{\arccos\left(x\right)}{\pi}+\frac{\arccos\left(x^{2}\right)}{\pi}\right) \lim_{x\to 0}\frac{\arccos\left(x\right)}{\pi} \frac{1}{2} \lim_{x\to 0}\frac{\arccos\left(x^2\right)}{\pi} \frac{1}{2} \lim_{x\to 0} \frac{1}{2} + \frac{1}{2} = 1 \arcsin\left(1\right) = \frac{\pi}{2},"['calculus', 'limits']"
66,Explain step in proof of $\lim_{n \to \infty} a_n = b \iff \limsup_{n \to \infty} a_n = \liminf_{n \to \infty} a_n = b$.,Explain step in proof of .,\lim_{n \to \infty} a_n = b \iff \limsup_{n \to \infty} a_n = \liminf_{n \to \infty} a_n = b,"I'm trying to understand a step in the forward direction of the proof of the theorem $\lim_{n \to \infty} a_n = b \iff \limsup_{n \to \infty} a_n =  \liminf_{n \to \infty} a_n = b$ . First, to clarify, let $\{a_n\}_{n = 1}^\infty \subseteq \mathbb{R}$ . We define $M_n := \sup\{a_k : k \geq n\}$ and $m_n := \inf\{a_k: k \geq n\}$ yielding two new sequences $\{M_n\}$ and $\{m_n\}$ which may contain the symbols $\infty$ or $-\infty$ . We then define $\limsup_{n \to \infty}a_n := \lim_{n \to \infty}M_n$ and $\liminf_{n \to \infty}a_n := \lim_{n \to \infty} m_n$ which may again be $\infty$ or $-\infty$ . Proof: ( $\Longrightarrow$ ) [We limit this proof to limits converging to $b \in \mathbb{R}$ for now.] Suppose that $\lim_{n \to \infty}a_n = b$ . For any $\varepsilon > 0$ there exists $N \in \mathbb{Z}^+$ such that $|a_n - b| < \varepsilon$ for all $n \geq N$ . Those, we have $b - \varepsilon < a_n < b + \varepsilon$ . But then, $\mathbf{b - \varepsilon < M_n \leq b + \varepsilon}$ and $\mathbf{b - \varepsilon \leq m_n < b + \varepsilon}$ for all $\mathbf{n \geq N}$ . Since this holds for every $\varepsilon > 0$ , we have $\limsup_{n \to \infty} a_n =  \liminf_{n \to \infty} a_n = b$ . I am not sure how the bold statement follows immediately. Clearly, $b - \varepsilon < M_n$ and $m_n < b + \varepsilon$ , but I don't see why $M_n \leq b + \varepsilon$ and $b - \varepsilon \leq m_n$ . I am sure it's something simple that I'm not seeing (properties of supremum and infimum probably), but any help would be appreciated.","I'm trying to understand a step in the forward direction of the proof of the theorem . First, to clarify, let . We define and yielding two new sequences and which may contain the symbols or . We then define and which may again be or . Proof: ( ) [We limit this proof to limits converging to for now.] Suppose that . For any there exists such that for all . Those, we have . But then, and for all . Since this holds for every , we have . I am not sure how the bold statement follows immediately. Clearly, and , but I don't see why and . I am sure it's something simple that I'm not seeing (properties of supremum and infimum probably), but any help would be appreciated.",\lim_{n \to \infty} a_n = b \iff \limsup_{n \to \infty} a_n =  \liminf_{n \to \infty} a_n = b \{a_n\}_{n = 1}^\infty \subseteq \mathbb{R} M_n := \sup\{a_k : k \geq n\} m_n := \inf\{a_k: k \geq n\} \{M_n\} \{m_n\} \infty -\infty \limsup_{n \to \infty}a_n := \lim_{n \to \infty}M_n \liminf_{n \to \infty}a_n := \lim_{n \to \infty} m_n \infty -\infty \Longrightarrow b \in \mathbb{R} \lim_{n \to \infty}a_n = b \varepsilon > 0 N \in \mathbb{Z}^+ |a_n - b| < \varepsilon n \geq N b - \varepsilon < a_n < b + \varepsilon \mathbf{b - \varepsilon < M_n \leq b + \varepsilon} \mathbf{b - \varepsilon \leq m_n < b + \varepsilon} \mathbf{n \geq N} \varepsilon > 0 \limsup_{n \to \infty} a_n =  \liminf_{n \to \infty} a_n = b b - \varepsilon < M_n m_n < b + \varepsilon M_n \leq b + \varepsilon b - \varepsilon \leq m_n,"['real-analysis', 'sequences-and-series', 'limits', 'supremum-and-infimum', 'limsup-and-liminf']"
67,Distance between two sets different from zero,Distance between two sets different from zero,,"Consider these sets $$ A\equiv \bigcap_{\delta>0} \liminf_{n\rightarrow \infty} \{x \in X: d(p_n, [\ell(x), u(x)])\leq \delta\} $$ $$ A_n\equiv \{x \in X: d(p_n, [\ell(x), u(x)])=0\} $$ where: $A$ is non-empty. $(p_n)_n$ is a sequence of  reals taking values in $[0,1]$ . $\ell(\cdot)$ and $u(\cdot)$ are real  function taking values in $(0,1)$ with $\ell<u$ . $d\big(p_n,  [\ell(x), u(x) ] \big):= \inf \big\{|p_n - y| : y \in [\ell(x), u(x) ] \big\}$ . Let $$ d_H(A, B)\equiv \max\{\sup_{x\in B}d(x,A), \sup_{x\in A}d(x, B)\}, $$ denote the Hausdorff distance. Can you give me a simple example of why $d_H(A,A_n)$ may be different from zero?",Consider these sets where: is non-empty. is a sequence of  reals taking values in . and are real  function taking values in with . . Let denote the Hausdorff distance. Can you give me a simple example of why may be different from zero?,"
A\equiv \bigcap_{\delta>0} \liminf_{n\rightarrow \infty} \{x \in X: d(p_n, [\ell(x), u(x)])\leq \delta\}
 
A_n\equiv \{x \in X: d(p_n, [\ell(x), u(x)])=0\}
 A (p_n)_n [0,1] \ell(\cdot) u(\cdot) (0,1) \ell<u d\big(p_n,  [\ell(x), u(x) ] \big):= \inf \big\{|p_n - y| : y \in [\ell(x), u(x) ] \big\} 
d_H(A, B)\equiv \max\{\sup_{x\in B}d(x,A), \sup_{x\in A}d(x, B)\},
 d_H(A,A_n)","['real-analysis', 'sequences-and-series', 'limits', 'hausdorff-distance']"
68,Find the limit $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$,Find the limit,\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right),"I'm working through Advanced Calculus: Theory and Practice by John S. Petrovic and is currently working on problem 3.5.9, which is as follows: Find the limit and give a strict “ε − δ” proof that the result is correct: $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$ There are other threads which discuss the topic around finding the limit of the function ( $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$ ), but do not supply a epsilon-delta proof that it is correct. Also, I have a particular question regarding the solution to this limit (I'll get to that). The most straight forward solution to $\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right)$ that I can find (and understand) goes like this: Note, $\sin(a) - \sin(b) = 2\cos(\frac{a+b}{2})\sin(\frac{a-b}{2})$ , therefore: $\sin(\sqrt{x+1}) - \sin(\sqrt{x}) = 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+1}-\sqrt{x}}{2})=$ $2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+1}-\sqrt{x}}{2}\frac{\sqrt{x+1}+\sqrt{x}}{\sqrt{x+1}+\sqrt{x}})=$ $2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})$ . At this stage the argument I find is this (which I'm doubtful of), because: $\lim_{{x \to \infty}} \sin\frac{1}{2(\sqrt{x+1}+\sqrt{x})}=0 \implies$ $\lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=0$ , however does that really make sense? Using $\lim_{{x \to \infty}} \sin\frac{1}{2(\sqrt{x+1}+\sqrt{x})}=0$ imply that $\lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=$ $\lim_{{x \to \infty}} 2 \cdot\lim_{{x \to \infty}} \cos(\frac{\sqrt{x+1}+\sqrt{x}}{2}) \cdot \lim_{{x \to \infty}} \sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})$ . But $\lim_{{x \to \infty}} \cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})$ does not exists since $\lim_{{n \to \infty}} \cos(2n\pi)=1 \neq \lim_{{n \to \infty}} \cos(\frac{\pi}{2}+2n\pi)=0$ . So my reasoning is that according to the field axioms $0$ multiplied with any number equals $0$ , but $0$ multiplied with something undefined is not equal to $0$ . Therefore we must rely on the Squeeze theorem. $-2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \leq 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \leq 2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})$ , where: $\lim_{{x \to \infty}} -2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) = 0$ $\lim_{{x \to \infty}} 2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) = 0$ And therefore we have that $\lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=0$ Does my argument make sense or is it unnecessary to use the Squeeze theorem here? Now, could someone please help explain to me how I define $M$ as an expression of $\epsilon$ so that I can make an ""epsilon-delta"" proof of the kind: For each $\epsilon>0$ , there exists a $M=...>0$ such that $|f(x)-L|<\epsilon$ whenever $x>M$ . Thank you.","I'm working through Advanced Calculus: Theory and Practice by John S. Petrovic and is currently working on problem 3.5.9, which is as follows: Find the limit and give a strict “ε − δ” proof that the result is correct: There are other threads which discuss the topic around finding the limit of the function ( ), but do not supply a epsilon-delta proof that it is correct. Also, I have a particular question regarding the solution to this limit (I'll get to that). The most straight forward solution to that I can find (and understand) goes like this: Note, , therefore: . At this stage the argument I find is this (which I'm doubtful of), because: , however does that really make sense? Using imply that . But does not exists since . So my reasoning is that according to the field axioms multiplied with any number equals , but multiplied with something undefined is not equal to . Therefore we must rely on the Squeeze theorem. , where: And therefore we have that Does my argument make sense or is it unnecessary to use the Squeeze theorem here? Now, could someone please help explain to me how I define as an expression of so that I can make an ""epsilon-delta"" proof of the kind: For each , there exists a such that whenever . Thank you.",\lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right) \lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right) \lim_{{x \to \infty}} \left( \sin(\sqrt{x+1}) - \sin(\sqrt{x}) \right) \sin(a) - \sin(b) = 2\cos(\frac{a+b}{2})\sin(\frac{a-b}{2}) \sin(\sqrt{x+1}) - \sin(\sqrt{x}) = 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+1}-\sqrt{x}}{2})= 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+1}-\sqrt{x}}{2}\frac{\sqrt{x+1}+\sqrt{x}}{\sqrt{x+1}+\sqrt{x}})= 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \lim_{{x \to \infty}} \sin\frac{1}{2(\sqrt{x+1}+\sqrt{x})}=0 \implies \lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=0 \lim_{{x \to \infty}} \sin\frac{1}{2(\sqrt{x+1}+\sqrt{x})}=0 \lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})= \lim_{{x \to \infty}} 2 \cdot\lim_{{x \to \infty}} \cos(\frac{\sqrt{x+1}+\sqrt{x}}{2}) \cdot \lim_{{x \to \infty}} \sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \lim_{{x \to \infty}} \cos(\frac{\sqrt{x+1}+\sqrt{x}}{2}) \lim_{{n \to \infty}} \cos(2n\pi)=1 \neq \lim_{{n \to \infty}} \cos(\frac{\pi}{2}+2n\pi)=0 0 0 0 0 -2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \leq 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \leq 2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) \lim_{{x \to \infty}} -2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) = 0 \lim_{{x \to \infty}} 2\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})}) = 0 \lim_{{x \to \infty}} 2\cos(\frac{\sqrt{x+1}+\sqrt{x}}{2})\sin(\frac{1}{2(\sqrt{x+1}+\sqrt{x})})=0 M \epsilon \epsilon>0 M=...>0 |f(x)-L|<\epsilon x>M,"['limits', 'limits-without-lhopital', 'epsilon-delta']"
69,Calculate the limit $\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx$ with limsup and liminf,Calculate the limit  with limsup and liminf,\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx,"I have a question about how to prove the limit using $\limsup$ . On the other day, I was asked the following problem in the exam: Let $m$ and $n$ be positive integers. Calculate the following limit, where $m$ is fixed. $$ \lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx $$ To this problem, I answered in the manner shown below: (My solution) Take any $\varepsilon \in (0, 1)$ . Now $f_n(x)$ denotes the integrand of the integral in question ( $f_n(x)$ depends on $m$ of course, but $m$ is fixed in this problem so no problems may occur even if $m$ is not appear in the notation). First, split the integral into two pieces, $$ \int_1^e f_n(x) dx = \int_1^{e - \varepsilon} f_n(x) dx + \int_{e - \varepsilon}^e f_n(x) dx, $$ and call the first term $I_1$ and the second $I_2$ . Since all of $x^m$ , $e^x$ and $(\log x)^n$ are increasing functions on $[1, e]$ , so is $f_n(x)$ . Then, $I_1$ is evaluated as below: $$ I_1 \le \int_1^{e - \varepsilon} f_n(e - \varepsilon) dx = f_n(e - \varepsilon)(e - \varepsilon - 1). $$ Similarly, $I_2$ is evaluated like $$ I_2 \le \int_{e - \varepsilon}^e f_n(e) = f_n(e)\varepsilon. $$ Since $\log(e - \varepsilon) \in (0, 1)$ , I obtained $$ \lim_{n \to \infty} f_n(e - \varepsilon)(e - \varepsilon - 1) = \lim_{n \to \infty} (e - \varepsilon)^m e^{e - \varepsilon} (\log(e - \varepsilon))^n = 0 $$ and $$ \lim_{n \to \infty} f_n(e)\varepsilon = \lim_{n \to \infty} e^m e^e (\log e)^n \varepsilon = e^{m + e}\varepsilon. $$ Therefore, I conclude that $$ 0 \le \liminf_{n \to \infty} \int_1^e f_n(x) dx \le \limsup_{n \to \infty} \int_1^e f_n(x) \le \limsup_{n \to \infty} (I_1 + I_2) \le \limsup_{n \to \infty} (f_n(e - \varepsilon)(e - \varepsilon - 1) + f_n(e)\varepsilon) = e^{m + e}\varepsilon. $$ Since $\varepsilon$ is arbitrary, both $\liminf$ and $\limsup$ of the integral as $n \to \infty$ must be $0$ , which indicates $$ \lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx = 0. $$ Is my proof valid or invalid? If there are either trivial or nontrivial mistakes, please let me know which part I should modify. Especially, my most worried thing is how to use $\liminf$ and $\limsup$ .","I have a question about how to prove the limit using . On the other day, I was asked the following problem in the exam: Let and be positive integers. Calculate the following limit, where is fixed. To this problem, I answered in the manner shown below: (My solution) Take any . Now denotes the integrand of the integral in question ( depends on of course, but is fixed in this problem so no problems may occur even if is not appear in the notation). First, split the integral into two pieces, and call the first term and the second . Since all of , and are increasing functions on , so is . Then, is evaluated as below: Similarly, is evaluated like Since , I obtained and Therefore, I conclude that Since is arbitrary, both and of the integral as must be , which indicates Is my proof valid or invalid? If there are either trivial or nontrivial mistakes, please let me know which part I should modify. Especially, my most worried thing is how to use and .","\limsup m n m 
\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx
 \varepsilon \in (0, 1) f_n(x) f_n(x) m m m 
\int_1^e f_n(x) dx
= \int_1^{e - \varepsilon} f_n(x) dx
+ \int_{e - \varepsilon}^e f_n(x) dx,
 I_1 I_2 x^m e^x (\log x)^n [1, e] f_n(x) I_1 
I_1
\le \int_1^{e - \varepsilon} f_n(e - \varepsilon) dx
= f_n(e - \varepsilon)(e - \varepsilon - 1).
 I_2 
I_2
\le \int_{e - \varepsilon}^e f_n(e)
= f_n(e)\varepsilon.
 \log(e - \varepsilon) \in (0, 1) 
\lim_{n \to \infty} f_n(e - \varepsilon)(e - \varepsilon - 1)
= \lim_{n \to \infty} (e - \varepsilon)^m e^{e - \varepsilon} (\log(e - \varepsilon))^n
= 0
 
\lim_{n \to \infty} f_n(e)\varepsilon
= \lim_{n \to \infty} e^m e^e (\log e)^n \varepsilon
= e^{m + e}\varepsilon.
 
0 \le \liminf_{n \to \infty} \int_1^e f_n(x) dx
\le \limsup_{n \to \infty} \int_1^e f_n(x)
\le \limsup_{n \to \infty} (I_1 + I_2)
\le \limsup_{n \to \infty} (f_n(e - \varepsilon)(e - \varepsilon - 1) + f_n(e)\varepsilon)
= e^{m + e}\varepsilon.
 \varepsilon \liminf \limsup n \to \infty 0 
\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx = 0.
 \liminf \limsup","['real-analysis', 'integration', 'limits', 'solution-verification', 'limsup-and-liminf']"
70,Limit of a sequence of the real root of a family of polynomials,Limit of a sequence of the real root of a family of polynomials,,"Consider a sequence of polynomials $P(x)=x^n+x^{n-1}+x^{n-2}+...+x^2+x-1, n>2$ . (i) Prove that it has a unique positive real root $x_n$ (ii) Find $$\lim_{n \to ∞} 2^n (x_n - 1/2)$$ The first part was relatively simple; upon differentiating $P(x)$ , it is easy to see that for all $x>0$ , the function is strictly increasing, so it can at most have one root. Secondly, $P(0)=-1$ and $P(1)=n-1$ i.e. the root must lie in $(0,1)$ . The third observation is that, as we apply the limit of n tending to infinity on the polynomial, $x_n$ tends to $1/2$ (it just becomes an infinite geometric series) which explains why the limit is an indeterminate form of $∞.0$ In the concise solution of the book, it was written that $x_n = \dfrac{1}{2}+\dfrac{\theta_n}{2^{n+1}}$ where $\theta_n$ lies in $[0,1]$ and left at that. Firstly, this step seems quite unmotivated to me and I also couldn't use it to solve the limit. Secondly, I failed to prove this equality as well (it resembled the form of LMVT so that was my starting point but it led nowhere). Does anyone have a more intuitively well-motivated answer for the second part of this problem or any observation that I missed?","Consider a sequence of polynomials . (i) Prove that it has a unique positive real root (ii) Find The first part was relatively simple; upon differentiating , it is easy to see that for all , the function is strictly increasing, so it can at most have one root. Secondly, and i.e. the root must lie in . The third observation is that, as we apply the limit of n tending to infinity on the polynomial, tends to (it just becomes an infinite geometric series) which explains why the limit is an indeterminate form of In the concise solution of the book, it was written that where lies in and left at that. Firstly, this step seems quite unmotivated to me and I also couldn't use it to solve the limit. Secondly, I failed to prove this equality as well (it resembled the form of LMVT so that was my starting point but it led nowhere). Does anyone have a more intuitively well-motivated answer for the second part of this problem or any observation that I missed?","P(x)=x^n+x^{n-1}+x^{n-2}+...+x^2+x-1, n>2 x_n \lim_{n \to ∞} 2^n (x_n - 1/2) P(x) x>0 P(0)=-1 P(1)=n-1 (0,1) x_n 1/2 ∞.0 x_n = \dfrac{1}{2}+\dfrac{\theta_n}{2^{n+1}} \theta_n [0,1]","['sequences-and-series', 'limits', 'polynomials', 'mean-value-theorem']"
71,Forgetful functor $Z(C)\rightarrow C$ has Left Adjoint,Forgetful functor  has Left Adjoint,Z(C)\rightarrow C,"The monoidal center $Z(C)$ of a monoidal category $C$ comes with a forgetful functor $F:Z(C)\rightarrow C$ defined $Z(X,\phi)=X.$ Does $F$ always admit a left adjoint? This is known (Section 3.2.) if $C$ is a tensor category. What about the general case? Tried the General Adjoint Functor Theorem but I'm not even sure if $F$ preserves limits in general.",The monoidal center of a monoidal category comes with a forgetful functor defined Does always admit a left adjoint? This is known (Section 3.2.) if is a tensor category. What about the general case? Tried the General Adjoint Functor Theorem but I'm not even sure if preserves limits in general.,"Z(C) C F:Z(C)\rightarrow C Z(X,\phi)=X. F C F","['limits', 'category-theory', 'tensor-products', 'adjoint-functors', 'monoidal-categories']"
72,Reverse L'Hospital rule under certain conditions,Reverse L'Hospital rule under certain conditions,,"Let $p$ be a positive real number, and $f:\mathbb{R}^+\rightarrow\mathbb{R}$ a differentiable function. Suppose that $f'(x)$ is monotonically increasing. Show that $\lim\limits_{x\rightarrow+\infty}\frac{f(x)}{x^p}=l$ if and only if $\lim\limits_{x\rightarrow+\infty}\frac{f'(x)}{px^{p-1}}=l$ . The direction "" $\Leftarrow$ "" follows from L'Hospital rule. By the way, many textbooks only prove the L'Hopsital rule for $\frac{0}{0}$ case, but just say that the proof for $\frac{\infty}{\infty}$ case is similar. I do not think that the proof for $\frac{\infty}{\infty}$ case is as easy as that for $\frac{0}{0}$ case. Come back to our question, how to prove "" $\Rightarrow$ ""?","Let be a positive real number, and a differentiable function. Suppose that is monotonically increasing. Show that if and only if . The direction "" "" follows from L'Hospital rule. By the way, many textbooks only prove the L'Hopsital rule for case, but just say that the proof for case is similar. I do not think that the proof for case is as easy as that for case. Come back to our question, how to prove "" ""?",p f:\mathbb{R}^+\rightarrow\mathbb{R} f'(x) \lim\limits_{x\rightarrow+\infty}\frac{f(x)}{x^p}=l \lim\limits_{x\rightarrow+\infty}\frac{f'(x)}{px^{p-1}}=l \Leftarrow \frac{0}{0} \frac{\infty}{\infty} \frac{\infty}{\infty} \frac{0}{0} \Rightarrow,"['real-analysis', 'calculus', 'limits', 'analysis', 'derivatives']"
73,Let $f$ be a differentiable function satisfying $\log_2(f(3x))=x+\log_2(3f(x))\;\forall x\in\mathbb R$ and $f'(0)=1$. Find the value of $[f(3)]$.,Let  be a differentiable function satisfying  and . Find the value of .,f \log_2(f(3x))=x+\log_2(3f(x))\;\forall x\in\mathbb R f'(0)=1 [f(3)],"Let $f$ be a differentiable function satisfying $\log_2(f(3x))=x+\log_2(3f(x))\;\forall x\in\mathbb R$ and $f'(0)=1$ . Find the value of $[f(3)]$ , where $[k]$ denotes greatest integer less than or equal to $k$ . Solution: $\log_2(f(3x))=x+\log_2(3f(x))$ $\frac{f(3x)}{3f(x)}=2^x$ $\frac{f(x)}{3f(\frac x3)}\cdot\frac{f(\frac x3)}{3f(\frac x{3^2})}\cdot\frac{f(\frac x{3^2})}{3f(\frac x{3^3})}\cdot\cdot\cdot\frac{f(\frac x{3^{n-1}})}{3f(\frac x{3^n})}=2^{\frac x3}\cdot2^{\frac x{3^2}}\cdot\cdot\cdot2^{\frac x{3^n}}$ $\frac{f(x)}{3^n\cdot f(\frac x{3^n})}=2^{\left(\frac x3+\frac x{3^2}+...+\frac x{3^n}\right)}$ $\lim_{n\to\infty}\frac{f(x)\cdot\frac x{3^n}}{x\cdot f(\frac x{3^n})}=\lim_{n\to\infty}2^{\left(\frac x3+\frac x{3^2}+...+\frac x{3^n}\right)}$ $\frac{f(x)}{x\cdot f'(0)}=2^{\left(\frac {\frac x3}{1-\frac13}\right)}$ $f(x)=x\cdot2^{\frac x2}$ $\therefore f(3)=3\cdot2^{\frac32}=6√2≈8.4$ $\implies[f(3)]=8$ My doubt: They seem to have written $\lim_{n\to\infty}\frac{f(\frac x{3^n})}{\frac x{3^n}}=f'(0)$ I don't understand this step. My Attempt: $f'(0)=\lim_{h\to0}\frac{f(0+h)-f(0)}{h}$ Replacing $h$ by $\frac x{3^n}$ $f'(0)=\lim_{n\to\infty}\frac{f(\frac x{3^n})-f(0)}{\frac x{3^n}}$ I don't think $f(0)$ is zero because that's not in the range.","Let be a differentiable function satisfying and . Find the value of , where denotes greatest integer less than or equal to . Solution: My doubt: They seem to have written I don't understand this step. My Attempt: Replacing by I don't think is zero because that's not in the range.",f \log_2(f(3x))=x+\log_2(3f(x))\;\forall x\in\mathbb R f'(0)=1 [f(3)] [k] k \log_2(f(3x))=x+\log_2(3f(x)) \frac{f(3x)}{3f(x)}=2^x \frac{f(x)}{3f(\frac x3)}\cdot\frac{f(\frac x3)}{3f(\frac x{3^2})}\cdot\frac{f(\frac x{3^2})}{3f(\frac x{3^3})}\cdot\cdot\cdot\frac{f(\frac x{3^{n-1}})}{3f(\frac x{3^n})}=2^{\frac x3}\cdot2^{\frac x{3^2}}\cdot\cdot\cdot2^{\frac x{3^n}} \frac{f(x)}{3^n\cdot f(\frac x{3^n})}=2^{\left(\frac x3+\frac x{3^2}+...+\frac x{3^n}\right)} \lim_{n\to\infty}\frac{f(x)\cdot\frac x{3^n}}{x\cdot f(\frac x{3^n})}=\lim_{n\to\infty}2^{\left(\frac x3+\frac x{3^2}+...+\frac x{3^n}\right)} \frac{f(x)}{x\cdot f'(0)}=2^{\left(\frac {\frac x3}{1-\frac13}\right)} f(x)=x\cdot2^{\frac x2} \therefore f(3)=3\cdot2^{\frac32}=6√2≈8.4 \implies[f(3)]=8 \lim_{n\to\infty}\frac{f(\frac x{3^n})}{\frac x{3^n}}=f'(0) f'(0)=\lim_{h\to0}\frac{f(0+h)-f(0)}{h} h \frac x{3^n} f'(0)=\lim_{n\to\infty}\frac{f(\frac x{3^n})-f(0)}{\frac x{3^n}} f(0),"['limits', 'functions', 'derivatives', 'contest-math']"
74,Fake proof that $\lim \limits_{x \to 0} \frac{\sin(3x)}{x^3}=\frac{-9}{2}$,Fake proof that,\lim \limits_{x \to 0} \frac{\sin(3x)}{x^3}=\frac{-9}{2},"In the book Mathematical Fallacies, Flaws, and Flimflam, the following fake proof is provided: The question starts with using trigonometric identities to turn $\sin(3x)\to3\sin(x)-4\sin^3(x)$ . Then divides the limit into two where the limit of $({\frac{\sin(x)}{x}})^3$ is obviously $1$ , and variable substitutes $x=3t$ making $\frac{\sin(3t)}{(3t)^3}$ which can then be manipulated to become the original equation of $\frac{\sin(3x)}{x^3}$ , giving a final result of $-9/2$ . I fail to see where the error is in the calculations, everything seems sound, but I know through the use of L'Hôpital's rule and Desmos that the answer should be $\infty$ at $x=0$ .","In the book Mathematical Fallacies, Flaws, and Flimflam, the following fake proof is provided: The question starts with using trigonometric identities to turn . Then divides the limit into two where the limit of is obviously , and variable substitutes making which can then be manipulated to become the original equation of , giving a final result of . I fail to see where the error is in the calculations, everything seems sound, but I know through the use of L'Hôpital's rule and Desmos that the answer should be at .",\sin(3x)\to3\sin(x)-4\sin^3(x) ({\frac{\sin(x)}{x}})^3 1 x=3t \frac{\sin(3t)}{(3t)^3} \frac{\sin(3x)}{x^3} -9/2 \infty x=0,"['limits', 'limits-without-lhopital', 'fake-proofs']"
75,"Prove that $d(x_n,x_{n+1})→0$ $\iff$ $(x_n)$ ia a Cauchy sequence.",Prove that    ia a Cauchy sequence.,"d(x_n,x_{n+1})→0 \iff (x_n)","Let $X\neq\varnothing$ and let $d:X\times X→\mathbb{R}$ be a function such that $$d(x,y)=0 \iff x=y,$$ $$d(x,y)=d(y,x),$$ $$d(x,z)≤\max⁡\{d(x,y),d(z,y)\}.$$ Let us say that $d$ is a metric. Let $(x_n)$ be a sequence in $X$ . Prove that $$\lim_{n\to\infty}d(x_n,x_{n+1})=0$$ if and only if $(x_n)$ is a Cauchy sequence. I proved that if $(x_n)$ is a Cauchy sequence then $d(x_n,x_{n+1})\to0$ as $n\to\infty$ , but I could not prove that if $d(x_n,x_{n+1})\to0$ as $n\to\infty$ then $(x_n)$ is a Cauchy sequence. I know that it is not true for arbitrary spaces. I tried to prove that just for this special metric.","Let and let be a function such that Let us say that is a metric. Let be a sequence in . Prove that if and only if is a Cauchy sequence. I proved that if is a Cauchy sequence then as , but I could not prove that if as then is a Cauchy sequence. I know that it is not true for arbitrary spaces. I tried to prove that just for this special metric.","X\neq\varnothing d:X\times X→\mathbb{R} d(x,y)=0 \iff x=y, d(x,y)=d(y,x), d(x,z)≤\max⁡\{d(x,y),d(z,y)\}. d (x_n) X \lim_{n\to\infty}d(x_n,x_{n+1})=0 (x_n) (x_n) d(x_n,x_{n+1})\to0 n\to\infty d(x_n,x_{n+1})\to0 n\to\infty (x_n)","['functional-analysis', 'limits', 'analysis', 'metric-spaces', 'cauchy-sequences']"
76,$\frac{x_1+x_2+\cdots+x_n}{n} \to a$ implies $\frac{x_1^p+x_2^p+\cdots+x_n^p}{n^p} \to 0$ for $p > 1$,implies  for,\frac{x_1+x_2+\cdots+x_n}{n} \to a \frac{x_1^p+x_2^p+\cdots+x_n^p}{n^p} \to 0 p > 1,"I'm encountering a mathematical analysis problem involving limits. Problem. Given a non-negative sequence $\{x_n\}$ satisfying $$\lim\limits_{n\to \infty}\frac{x_1+x_2+\cdots+x_n}{n}=a,$$ where $|a|<+\infty$ . Given a real number $p>1$ . Prove that $$\lim\limits_{n\to \infty}\frac{x_1^p+x_2^p+\cdots+x_n^p}{n^p}=0.$$ My Approach. I initially attempted to use Stolz's theorem to prove this problem. However, it became apparent that the converse of Stolz's theorem does not necessarily hold. Subsequently, I tried employing the squeeze theorem, but I couldn't find a suitable bounding inequality. I noticed that the form of this inequality is quite similar to the discrete Hardy's inequality. (Hardy's Inequality) Given a non-negative sequence $\{x_n\}$ and a real number $p>1$ , then $$\sum_{k=1}^{n}\left(\frac{x_1+x_2+\cdots+x_k}{k}\right)^p<\left(\frac{p}{p-1}\right)^p\sum_{k=1}^{n}a_k^p.$$ It is disappointing that the inequality here behaves in the opposite direction compared to the original problem. However, I have a faint intuition that the order of $\sum\limits_{k=1}^{n}a_k^p$ is around $O(n)$ . Can anyone help me? I would be very grateful.","I'm encountering a mathematical analysis problem involving limits. Problem. Given a non-negative sequence satisfying where . Given a real number . Prove that My Approach. I initially attempted to use Stolz's theorem to prove this problem. However, it became apparent that the converse of Stolz's theorem does not necessarily hold. Subsequently, I tried employing the squeeze theorem, but I couldn't find a suitable bounding inequality. I noticed that the form of this inequality is quite similar to the discrete Hardy's inequality. (Hardy's Inequality) Given a non-negative sequence and a real number , then It is disappointing that the inequality here behaves in the opposite direction compared to the original problem. However, I have a faint intuition that the order of is around . Can anyone help me? I would be very grateful.","\{x_n\} \lim\limits_{n\to \infty}\frac{x_1+x_2+\cdots+x_n}{n}=a, |a|<+\infty p>1 \lim\limits_{n\to \infty}\frac{x_1^p+x_2^p+\cdots+x_n^p}{n^p}=0. \{x_n\} p>1 \sum_{k=1}^{n}\left(\frac{x_1+x_2+\cdots+x_k}{k}\right)^p<\left(\frac{p}{p-1}\right)^p\sum_{k=1}^{n}a_k^p. \sum\limits_{k=1}^{n}a_k^p O(n)","['sequences-and-series', 'limits', 'analysis']"
77,Calculus - Evalute limit without using L'Hôpital rule,Calculus - Evalute limit without using L'Hôpital rule,,"I tried to solve the following limit by the L'Hôpital Rule: $$ \lim_{x \to \pi/4} \frac{{\tan(x)^{\cot(x)} - \cot(x)^{\tan(x)}}}{x-\pi/4}$$ Taking $L$ to be the said limit, by L'Hôpital Rule, $$L = \lim_{x \to \pi/4} \tan(x)^{\cot(x)}\ln\tan{x}(-\csc^2x) + {\tan(x)^{\cot(x) - 1}}\cot{x}\sec^2x $$ $$- \cot(x)^{\tan(x)} \ln(\cot x) \sec^2{x} + \cot(x)^{\tan(x)-1}\tan x \csc^2 x$$ $$\Rightarrow L = 4$$ Now, this method is quite involved, as the differentiation of the functions is not very simple. This compels me to think that there could be a better method to solve this question. Any help in that pursuit is appreciated.","I tried to solve the following limit by the L'Hôpital Rule: Taking to be the said limit, by L'Hôpital Rule, Now, this method is quite involved, as the differentiation of the functions is not very simple. This compels me to think that there could be a better method to solve this question. Any help in that pursuit is appreciated.", \lim_{x \to \pi/4} \frac{{\tan(x)^{\cot(x)} - \cot(x)^{\tan(x)}}}{x-\pi/4} L L = \lim_{x \to \pi/4} \tan(x)^{\cot(x)}\ln\tan{x}(-\csc^2x) + {\tan(x)^{\cot(x) - 1}}\cot{x}\sec^2x  - \cot(x)^{\tan(x)} \ln(\cot x) \sec^2{x} + \cot(x)^{\tan(x)-1}\tan x \csc^2 x \Rightarrow L = 4,"['calculus', 'limits', 'limits-without-lhopital']"
78,Continuity of $L^p$ norm in terms of p [duplicate],Continuity of  norm in terms of p [duplicate],L^p,"This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 8 months ago . Can it be shown that $\lim_{p \rightarrow q} \lVert f \rVert_{L_p} = \lVert f \rVert_{L_q}$ ? I don't know where to even begin. I have thought of using the dominated convergence theorem, but I don't know how. I want to prove this because it seems like many proofs involving the $L_p$ norm will be much easier, e.g. it would often be enough to prove that something holds for $p>1$ and it follows easily that it also holds for $p=1$ . Instead the textbooks I have read seem to proof many things separately for $p>1$ and $p=1$ , which seems clumsy to me.","This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 8 months ago . Can it be shown that ? I don't know where to even begin. I have thought of using the dominated convergence theorem, but I don't know how. I want to prove this because it seems like many proofs involving the norm will be much easier, e.g. it would often be enough to prove that something holds for and it follows easily that it also holds for . Instead the textbooks I have read seem to proof many things separately for and , which seems clumsy to me.",\lim_{p \rightarrow q} \lVert f \rVert_{L_p} = \lVert f \rVert_{L_q} L_p p>1 p=1 p>1 p=1,"['limits', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
79,Trying to find a counterexample for a false theorem,Trying to find a counterexample for a false theorem,,"I am trying to find a counter example for the following theorem: Suppose there are three functions $h$ , $f$ , and $g$ that satisfy the following inequalities: $$  h(x) < f(x) < g(x)  $$ for all $x$ such that $0 < |x - c| < p$ for some $p > 0$ , and \begin{align*} \lim_{{x \to c}} h(x) &= H, \\ \lim_{{x \to c}} f(x) &= F, \quad \text{and} \\ \lim_{{x \to c}} g(x) &= G, \end{align*} for some real numbers $H$ , $F$ , and $G$ . We claim that $H < F < G$ . I have tried everything that I can think of to find a counter example but nothing worked. For instance, if I were to choose $h(x) = 2x$ , $f(x) = 4x$ , and $g(x) = 6x$ , then as $x$ approaches $0$ the claim would be violated, but the conditions would also be violated meaning it is not a counter example. I have tried piecewise functions as well and ran into a very similar issue where if the claim is violated, then the conditions were also violated and vice versa. I thought of using a Dirichlet function for $f(x)$ so that $f(x)$ would have no limit, but that would also violate the conditions. I am currently at a loss and would appreciate any sort of feedback/advice/input from anyone. Thank you. By the way, sorry for any possible formatting issues, it is my first time posting here.","I am trying to find a counter example for the following theorem: Suppose there are three functions , , and that satisfy the following inequalities: for all such that for some , and for some real numbers , , and . We claim that . I have tried everything that I can think of to find a counter example but nothing worked. For instance, if I were to choose , , and , then as approaches the claim would be violated, but the conditions would also be violated meaning it is not a counter example. I have tried piecewise functions as well and ran into a very similar issue where if the claim is violated, then the conditions were also violated and vice versa. I thought of using a Dirichlet function for so that would have no limit, but that would also violate the conditions. I am currently at a loss and would appreciate any sort of feedback/advice/input from anyone. Thank you. By the way, sorry for any possible formatting issues, it is my first time posting here.","h f g  
h(x) < f(x) < g(x) 
 x 0 < |x - c| < p p > 0 \begin{align*}
\lim_{{x \to c}} h(x) &= H, \\
\lim_{{x \to c}} f(x) &= F, \quad \text{and} \\
\lim_{{x \to c}} g(x) &= G,
\end{align*} H F G H < F < G h(x) = 2x f(x) = 4x g(x) = 6x x 0 f(x) f(x)","['calculus', 'limits']"
80,"Does the sequence $x_n := n^2/\sqrt{n^6+1} + n^2/\sqrt{n^6+2} + . . . + n^2/\sqrt{n^6+n}$ converge? If it does, what value does it converge to?","Does the sequence  converge? If it does, what value does it converge to?",x_n := n^2/\sqrt{n^6+1} + n^2/\sqrt{n^6+2} + . . . + n^2/\sqrt{n^6+n},"Does the sequence converge? If it does, to what value? In my solution, I have rewritten the expression as $x_n = \frac{1}{\sqrt{n^2+\frac{1}{n^4}}} + \frac{1}{\sqrt{n^2+\frac{2}{n^4}}} + . . . + \frac{1}{\sqrt{n^2+\frac{n}{n^4}}}$ since for any $i$ s.t. $1\le i\le n$ , we can take $x_n = \frac{1}{\sqrt{n^2+\frac{a_1}{n^3}}} + \frac{1}{\sqrt{n^2+\frac{a_2}{n^3}}} + . . . + \frac{1}{\sqrt{n^2+\frac{a_n}{n^3}}}$ s.t $a_n = i/n \implies a_n \le 1$ now we can write $p = n^2$ and $q = \frac{1}{n^3}$ Now, to find the limit can I apply partial limits as given below? $\lim_{p \to \infty} \lim_{q \to 0} \frac{1}{\sqrt{x+a_1y}} + \frac{1}{\sqrt{x+a_2y}} + . . . + \frac{1}{\sqrt{x+a_ny}}$ $\implies \lim_{p \to \infty} \frac{1}{\sqrt{x}} + \frac{1}{\sqrt{x}} + . . . (n times)$ $ \implies \lim_{p \to \infty} \frac{n}{\sqrt{x}} $ $ \implies \implies \lim_{n \to \infty} \frac{n}{\sqrt{n^2}} = 1$","Does the sequence converge? If it does, to what value? In my solution, I have rewritten the expression as since for any s.t. , we can take s.t now we can write and Now, to find the limit can I apply partial limits as given below?",x_n = \frac{1}{\sqrt{n^2+\frac{1}{n^4}}} + \frac{1}{\sqrt{n^2+\frac{2}{n^4}}} + . . . + \frac{1}{\sqrt{n^2+\frac{n}{n^4}}} i 1\le i\le n x_n = \frac{1}{\sqrt{n^2+\frac{a_1}{n^3}}} + \frac{1}{\sqrt{n^2+\frac{a_2}{n^3}}} + . . . + \frac{1}{\sqrt{n^2+\frac{a_n}{n^3}}} a_n = i/n \implies a_n \le 1 p = n^2 q = \frac{1}{n^3} \lim_{p \to \infty} \lim_{q \to 0} \frac{1}{\sqrt{x+a_1y}} + \frac{1}{\sqrt{x+a_2y}} + . . . + \frac{1}{\sqrt{x+a_ny}} \implies \lim_{p \to \infty} \frac{1}{\sqrt{x}} + \frac{1}{\sqrt{x}} + . . . (n times)  \implies \lim_{p \to \infty} \frac{n}{\sqrt{x}}   \implies \implies \lim_{n \to \infty} \frac{n}{\sqrt{n^2}} = 1,"['real-analysis', 'sequences-and-series', 'limits']"
81,Gambler's ruin in the limit (only stopping rule ruin),Gambler's ruin in the limit (only stopping rule ruin),,"Imagine a classical Gambler's ruin with winning probability p and losing probability q = 1-p. You start at 1\$ and lose once you reach 0\$. The only stopping rule is that the game is over when the gambler loses. No restriction on p besides the usual $0\leq p\leq1$ . My question is, what is the probability of the gambler losing. My intuition would be that $P(lose) = 1$ , or at least very close to one. The question, I believe, is whether there are values of $p$ , where even when considering an infinite time horizon, the Gambler never reaches 0.","Imagine a classical Gambler's ruin with winning probability p and losing probability q = 1-p. You start at 1\$ and lose once you reach 0\$. The only stopping rule is that the game is over when the gambler loses. No restriction on p besides the usual . My question is, what is the probability of the gambler losing. My intuition would be that , or at least very close to one. The question, I believe, is whether there are values of , where even when considering an infinite time horizon, the Gambler never reaches 0.",0\leq p\leq1 P(lose) = 1 p,"['probability', 'limits', 'probability-distributions', 'random-walk', 'probability-limit-theorems']"
82,"Let $c_k\ge0$ be a sequence which is bounded above and bounded away from $0$. Prove that $\lim 1/n\sum\limits_{k=1}^n c_kz^k=0$ for $|z|\le1,z\ne1$",Let  be a sequence which is bounded above and bounded away from . Prove that  for,"c_k\ge0 0 \lim 1/n\sum\limits_{k=1}^n c_kz^k=0 |z|\le1,z\ne1","Let $m,M>0$ be such that $m\le c_k\le M$ for all $k$ . If $|z|<1$ , then $|c_kz^k|\le M|z|^k\to 0$ . Then we have $$\lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^n c_k z^k=0$$ But I cannot show for $|z|=1$ . If $c_k=c$ is a constant sequence. Then $\frac{1}{n}\sum\limits_{k=1}^n c_k z^k=c\frac{z(z^n-1)}{n(z-1)}\to0$ as $z\ne1$ and $|z|=1$ . Can anyone help with any idea or hint for the problem? Thanks for your help in advance.","Let be such that for all . If , then . Then we have But I cannot show for . If is a constant sequence. Then as and . Can anyone help with any idea or hint for the problem? Thanks for your help in advance.","m,M>0 m\le c_k\le M k |z|<1 |c_kz^k|\le M|z|^k\to 0 \lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^n c_k z^k=0 |z|=1 c_k=c \frac{1}{n}\sum\limits_{k=1}^n c_k z^k=c\frac{z(z^n-1)}{n(z-1)}\to0 z\ne1 |z|=1","['real-analysis', 'sequences-and-series', 'limits', 'analysis']"
83,Does the mean ratio of the largest prime factor in prime gaps to the lower bound of the gap converge?,Does the mean ratio of the largest prime factor in prime gaps to the lower bound of the gap converge?,,"Note : Posted in MO since (2) is open in MSE Let $p_n$ be the $n$ -th prime and $p_n < c_n < p_{n+1}$ be the composite number such that $c_n$ has the largest prime factor $l_n$ in this prime gap. If the largest prime factors occurs in two or more composites then we take $c_n$ to be the smallest among them. Question 1 : Is it true that $$ \lim_{n \to \infty}\frac{1}{n} \left(\frac{c_2 + c_3 + \cdots c_n}{l_2 + l_3 + \cdots l_n}\right)\left(\frac{l_2}{p_2} + \frac{l_3}{p_3} + \cdots \frac{l_n}{p_n}\right) = 1 \tag 1 $$ Moreover, if we look at the individual components $$ \lim_{n \to \infty}\frac{1}{n} \left(\frac{l_2}{p_2} + \frac{l_3}{p_3} + \cdots \frac{l_n}{p_n}\right) \tag 2 $$ we observe that its value close to $0.2614$ Experimental data for $(2)$ : $n = 10^8$ , mean $\approx 0.27815$ $n = 10^9$ , mean $\approx 0.27578$ $n = 3.5 \times 10^9$ , mean $\approx 0.27470$ Update : 14-Dec-2023 I evaluated the above ratios for consecutive prime gaps for $p_n > 10^{40}, p_n > 10^{50}$ and $p_n > 10^{60}$ respectively. The limiting value in these tests were $0.2592, 0.2551$ and $0.2511$ which show a very slow decreasing trend. Question : Does the above limit exist? If yes, what does it converge to?","Note : Posted in MO since (2) is open in MSE Let be the -th prime and be the composite number such that has the largest prime factor in this prime gap. If the largest prime factors occurs in two or more composites then we take to be the smallest among them. Question 1 : Is it true that Moreover, if we look at the individual components we observe that its value close to Experimental data for : , mean , mean , mean Update : 14-Dec-2023 I evaluated the above ratios for consecutive prime gaps for and respectively. The limiting value in these tests were and which show a very slow decreasing trend. Question : Does the above limit exist? If yes, what does it converge to?","p_n n p_n < c_n < p_{n+1} c_n l_n c_n 
\lim_{n \to \infty}\frac{1}{n}
\left(\frac{c_2 + c_3 + \cdots c_n}{l_2 + l_3 + \cdots l_n}\right)\left(\frac{l_2}{p_2} + \frac{l_3}{p_3} + \cdots \frac{l_n}{p_n}\right) = 1 \tag 1
 
\lim_{n \to \infty}\frac{1}{n}
\left(\frac{l_2}{p_2} + \frac{l_3}{p_3} + \cdots \frac{l_n}{p_n}\right) \tag 2
 0.2614 (2) n = 10^8 \approx 0.27815 n = 10^9 \approx 0.27578 n = 3.5 \times 10^9 \approx 0.27470 p_n > 10^{40}, p_n > 10^{50} p_n > 10^{60} 0.2592, 0.2551 0.2511","['limits', 'number-theory', 'prime-numbers', 'analytic-number-theory']"
84,Does $f=O(g)$ and $g=O(f)$ implies $\lim_{x\to\infty}f(x)/g(x)$ exists?,Does  and  implies  exists?,f=O(g) g=O(f) \lim_{x\to\infty}f(x)/g(x),"Let $f,g:[0,\infty)\to [0,\infty)$ be continuous function, and recall that $f=O(g)$ if there exists $C>0$ such that for sufficiently large $x$ , we have $f(x)\leq Cg(x)$ , and define $g=O(f)$ similarly. I found a statement that says There exists $L\in \mathbb{R}, |L|<\infty$ such that $\lim_{x\to\infty}f(x)/g(x)=L$ if and only if $f=O(g)$ and $g=O(f)$ . Is this statement correct? It is easy to see that $\lim_{x\to\infty}f(x)/g(x)=L$ implies $f=O(g)$ and $g=O(f)$ . What about the other implication? Is it wrong or is it provable? If provable, I would appreciate any comments on how to prive it.","Let be continuous function, and recall that if there exists such that for sufficiently large , we have , and define similarly. I found a statement that says There exists such that if and only if and . Is this statement correct? It is easy to see that implies and . What about the other implication? Is it wrong or is it provable? If provable, I would appreciate any comments on how to prive it.","f,g:[0,\infty)\to [0,\infty) f=O(g) C>0 x f(x)\leq Cg(x) g=O(f) L\in \mathbb{R}, |L|<\infty \lim_{x\to\infty}f(x)/g(x)=L f=O(g) g=O(f) \lim_{x\to\infty}f(x)/g(x)=L f=O(g) g=O(f)","['calculus', 'limits', 'analysis', 'asymptotics']"
85,"Prove: if $\lim \frac{s_n}n=L\neq0$, then the sequence $s_n$ is not bounded.","Prove: if , then the sequence  is not bounded.",\lim \frac{s_n}n=L\neq0 s_n,"If $$\lim_{n\rightarrow \infty} \frac{s_n}{n}= L \ne 0$$ then $\{ s_n \}$ is not bounded. How to prove this generally (i.e. without help of examples)? I've sketched a proof below. Is it right.  If not what is the flaw? My attempt of proof: If $$\lim_{n\rightarrow \infty} \frac{s_n}{n}= L \ne 0$$ then $ |\frac{s_n}{n}- L |< \epsilon ,     (n\ge N)$ $L- \epsilon < \frac{s_n}{n}< L+ \epsilon$ $\frac{s_n}{n}< L+ \epsilon \implies s_n <n (L+\epsilon)$ When $n \rightarrow \infty , s_n < \infty$ Then we can't find an N such that for any $M>0$ , $|s_n |\le M$ i.e. for any $(n\ge N)$ we can't find an N such that $s_n \le -M$ .","If then is not bounded. How to prove this generally (i.e. without help of examples)? I've sketched a proof below. Is it right.  If not what is the flaw? My attempt of proof: If then When Then we can't find an N such that for any , i.e. for any we can't find an N such that .","\lim_{n\rightarrow \infty} \frac{s_n}{n}= L \ne 0 \{ s_n \} \lim_{n\rightarrow \infty} \frac{s_n}{n}= L \ne 0  |\frac{s_n}{n}- L |< \epsilon ,     (n\ge N) L- \epsilon < \frac{s_n}{n}< L+ \epsilon \frac{s_n}{n}< L+ \epsilon \implies s_n <n (L+\epsilon) n \rightarrow \infty , s_n < \infty M>0 |s_n |\le M (n\ge N) s_n \le -M","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'upper-lower-bounds']"
86,Is right-continuity of $c$ necessary for $a(u)=\inf \left\{ t \in \mathbb{R_+} \colon c(t)>u \right\}$ to be right-continuous?,Is right-continuity of  necessary for  to be right-continuous?,c a(u)=\inf \left\{ t \in \mathbb{R_+} \colon c(t)>u \right\},"There are similar posts, such as this one or this one , but I can't see where the right-continuity assumption is required. Let $c:\mathbb{R}_+ \rightarrow \mathbb{R}_+$ be a non-decreasing function, right-continuous function. Let $a:\mathbb{R_+} \rightarrow \mathbb{R_+}$ be defined by $a(u)=\inf \left\{t \in \mathbb{R_+}  \,\colon \; c(t) > u \right\}$ . The claim is that $a$ is increasing and right-continuous. As for right-continuity, one should prove that for every sequence $(u_n)_{n=1}^{\infty} \subset \mathbb{R_+}$ such that $u_n \searrow u \in \mathbb{R_+} $ , $ \; a(u_n) \searrow a(u)$ . So, let $(u_n)_{n=1}^{\infty}$ be such a sequence, and define, for each $n \geq 1$ , $A_{u_n}=\left\{t \in \mathbb{R_+} \; \colon c(t) > u_n \right\}=c^{-1}\big(u_n, +\infty\big)$ . It is immediate to see that $(A_{u_n})_{n=1}^{\infty}$ is an increasing sequence of sets, and that for any $u \in \mathbb{R_+}$ , $ \; a(u)=\inf \left\{ A_{u} \right\}$ . So, $A_{u_n} \nearrow \bigcup_{n=1}^{\infty}A_{u_n}=\bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)=c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))=c^{-1}\big(u, \infty \big)=\left\{t \in \mathbb{R}_+ \colon c(t) > u \right\}=A_u$ . My issue is that I can't see where this list of equalities makes use of the right-continuity assumption on $c \,$ : $\bigcup_{n=1}^{\infty}A_{u_n} = \bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)$ follows by definition of $A_{u_n}$ ; $\bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)=c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))$ follows from measurability of $c$ with respect to the Lebesgue measure on $\mathbb{R_+}$ ; $c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))=c^{-1}(u, \infty)$ follows from the fact that $u_n \searrow u_n$ . Once the equality $\bigcup_{n=1}^{\infty} A_{u_n} = A_u$ is proven, the rest of the claim follows smoothly: By definition, $a(u_n)=\inf\left\{ A_{u_n} \right\}$ , hence \begin{align} \lim_{n \rightarrow \infty}a(u_n)=\lim_{n \rightarrow \infty} \inf A_{u_n}=\inf \left\{ \lim_{n \rightarrow \infty} A_{u_n} \right\} = \inf \left\{ \bigcup_{n=1}^{\infty}A_{u_n} \right\}=\inf \left\{ A_u \right\}=a(u) \end{align} I am just not sure why it is possible to interchange $\inf$ and $\lim$ in \begin{align} \lim_{n \rightarrow \infty} \inf \left\{ A_{u_n} \right\} =\inf \left\{ \lim_{n \rightarrow \infty} A_{u_n} \right\} \end{align} but my main concern is about where the right-continuity of $c$ is required in the proof. Thanks to anyone who can help!","There are similar posts, such as this one or this one , but I can't see where the right-continuity assumption is required. Let be a non-decreasing function, right-continuous function. Let be defined by . The claim is that is increasing and right-continuous. As for right-continuity, one should prove that for every sequence such that , . So, let be such a sequence, and define, for each , . It is immediate to see that is an increasing sequence of sets, and that for any , . So, . My issue is that I can't see where this list of equalities makes use of the right-continuity assumption on : follows by definition of ; follows from measurability of with respect to the Lebesgue measure on ; follows from the fact that . Once the equality is proven, the rest of the claim follows smoothly: By definition, , hence I am just not sure why it is possible to interchange and in but my main concern is about where the right-continuity of is required in the proof. Thanks to anyone who can help!","c:\mathbb{R}_+ \rightarrow \mathbb{R}_+ a:\mathbb{R_+} \rightarrow \mathbb{R_+} a(u)=\inf \left\{t \in \mathbb{R_+}  \,\colon \; c(t) > u \right\} a (u_n)_{n=1}^{\infty} \subset \mathbb{R_+} u_n \searrow u \in \mathbb{R_+}   \; a(u_n) \searrow a(u) (u_n)_{n=1}^{\infty} n \geq 1 A_{u_n}=\left\{t \in \mathbb{R_+} \; \colon c(t) > u_n \right\}=c^{-1}\big(u_n, +\infty\big) (A_{u_n})_{n=1}^{\infty} u \in \mathbb{R_+}  \; a(u)=\inf \left\{ A_{u} \right\} A_{u_n} \nearrow \bigcup_{n=1}^{\infty}A_{u_n}=\bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)=c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))=c^{-1}\big(u, \infty \big)=\left\{t \in \mathbb{R}_+ \colon c(t) > u \right\}=A_u c \, \bigcup_{n=1}^{\infty}A_{u_n} = \bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty) A_{u_n} \bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)=c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty)) c \mathbb{R_+} c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))=c^{-1}(u, \infty) u_n \searrow u_n \bigcup_{n=1}^{\infty} A_{u_n} = A_u a(u_n)=\inf\left\{ A_{u_n} \right\} \begin{align}
\lim_{n \rightarrow \infty}a(u_n)=\lim_{n \rightarrow \infty} \inf A_{u_n}=\inf \left\{ \lim_{n \rightarrow \infty} A_{u_n} \right\} = \inf \left\{ \bigcup_{n=1}^{\infty}A_{u_n} \right\}=\inf \left\{ A_u \right\}=a(u)
\end{align} \inf \lim \begin{align}
\lim_{n \rightarrow \infty} \inf \left\{ A_{u_n} \right\} =\inf \left\{ \lim_{n \rightarrow \infty} A_{u_n} \right\}
\end{align} c","['real-analysis', 'limits', 'continuity', 'cumulative-distribution-functions']"
87,$a_1>0$ and $a_{n+1}=\ln{(a_n+1)}$. Where is the largest term of the sequence $na_n$?,and . Where is the largest term of the sequence ?,a_1>0 a_{n+1}=\ln{(a_n+1)} na_n,"Given $a_1>0$ and $a_{n+1}=\ln{(a_n+1)}$ , the sequence $na_n$ is quite interesting. It turns out that the sequence $na_n$ approaches $2$ ( proof ), and that $\begin{align}\lim_{n\to\infty}\frac{n}{\ln{n}}(na_n-2)\end{align}=2/3$ ( proof ). So $na_n$ has some maximum value greater than $2$ . Let $f(x)=$ { $n$ -value of $(na_n)_\text{max}$ when $a_1=x$ }. An Excel simulation yields: $f(3)=1$ $f(2)=3$ $f(1.5)=11$ $f(1)=118$ $f(0.8)=652$ $f(0.6)=10424$ $f(0.55)=28113$ What is a good approximation for $f(x)$ for small $x$ ? I took the log of $f(x)$ and then used Excel's trendline tool to come up with $f(x)\approx ab^{1/x}$ where $a\approx 0.189$ and $b\approx 700$ , but I doubt this is valid for very small values of $a_1$ . I'm wondering if there is a closed form approximation for $f(x)$ .","Given and , the sequence is quite interesting. It turns out that the sequence approaches ( proof ), and that ( proof ). So has some maximum value greater than . Let { -value of when }. An Excel simulation yields: What is a good approximation for for small ? I took the log of and then used Excel's trendline tool to come up with where and , but I doubt this is valid for very small values of . I'm wondering if there is a closed form approximation for .",a_1>0 a_{n+1}=\ln{(a_n+1)} na_n na_n 2 \begin{align}\lim_{n\to\infty}\frac{n}{\ln{n}}(na_n-2)\end{align}=2/3 na_n 2 f(x)= n (na_n)_\text{max} a_1=x f(3)=1 f(2)=3 f(1.5)=11 f(1)=118 f(0.8)=652 f(0.6)=10424 f(0.55)=28113 f(x) x f(x) f(x)\approx ab^{1/x} a\approx 0.189 b\approx 700 a_1 f(x),"['sequences-and-series', 'limits', 'logarithms', 'asymptotics', 'recurrence-relations']"
88,"Proof that $\,\lim\limits_{x\to 0}\frac{x^2+1}{(3x+1)(5x+1)}=1$",Proof that,"\,\lim\limits_{x\to 0}\frac{x^2+1}{(3x+1)(5x+1)}=1","I want to show that $\;\lim\limits_{x\to 0}\dfrac{x^2+1}{(3x+1)(5x+1)}=1$ Note that $$\left|\frac{x^2+1}{(3x+1)(5x+1)}-1 \right|=\left|\frac{-14x^2-8x}{(3x+1)(5x+1)} \right|=\frac{\left |x \right|\left |14x+8 \right|}{\left |3x+1 \right| \left |5x+1 \right|} $$ It appears that if i take $\delta_{1}=1$ I can easily bound the numerator terms, but it is difficult for me to bound the denominator terms; any suggestions?","I want to show that Note that It appears that if i take I can easily bound the numerator terms, but it is difficult for me to bound the denominator terms; any suggestions?",\;\lim\limits_{x\to 0}\dfrac{x^2+1}{(3x+1)(5x+1)}=1 \left|\frac{x^2+1}{(3x+1)(5x+1)}-1 \right|=\left|\frac{-14x^2-8x}{(3x+1)(5x+1)} \right|=\frac{\left |x \right|\left |14x+8 \right|}{\left |3x+1 \right| \left |5x+1 \right|}  \delta_{1}=1,"['real-analysis', 'limits']"
89,Let a function $f$ be differentiable at $3$ and satisfy $f(3)=3f'(3)>0$. Then find the limit $\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x.$,Let a function  be differentiable at  and satisfy . Then find the limit,f 3 f(3)=3f'(3)>0 \lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x.,"Let $f:(0,\infty)\to(0,\infty)$ be a function differentiable at $3$ and sayisfying $f(3)=3f'(3)>0$ . Then find the limit $$\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x.$$ The thing is, I managed to solve the problem. I will give the detailed steps how I solved it: We observe that $$L=\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x,$$ is of the form $1^\infty.$ So, we try a popular recommended transformation, as $$L=e^{\lim_{x\to\infty}x(\frac{f(3+\frac 3x)}{f(3)}-1)}\implies e^{\frac 1{f(3)}\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))}.$$ We focus on the limit $\lim_{x\to\infty}x(f(3+\frac 3x)-f(3)).$ We assume $\frac 1x=k$ and as $x\to\infty$ we have, $k\to 0.$ So, $$\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}.$$ We note that $f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}$ so, we write, $$\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}=3f'(3).$$ Hence, $$L=e^{\lim_{x\to\infty}x(\frac{f(3+\frac 3x)}{f(3)}-1)}\implies e^{\frac 1{f(3)}\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))}=e^{\frac{3f'(3)}{f(3)}}=e^{\frac{3f'(3)}{3f'(3)}}=e.$$ I hope my solution is correct. But the thing I am confused about is with my  approach. In my solution, I wrote a part $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ and a part $$""f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}"".$$ But the thing is, just like $$""f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}"",$$ if I wrote $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ instead of, $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"",$$ my solution would be erroneous as the transformation $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ is not valid. But my question, is, why this is so? Why could I write, $$""f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}"",$$ and considered this transformation, to be a logical step and $$""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}""$$ to be a faulty transformation (when we are basically, doing the analogous thing in both the cases, i.e as $k\to 0$ then we must have $3k\to 0$ and so, we might replace all the $k$ 's in the limit expression with $3k's$ as both $k$ and $3k$ are approaching the same thing) ? My understanding: My understanding that of why $$""f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}"",$$ is a valid transformation ? We note, here $k\to 0$ and hence $3k\to 0$ and the limit expression i.e $\frac{f(3+k)-f(3)}{k}$ (before transformation), has all the $k's$ approaching to zero, and as $3k$ approaches zero as well, it doesn't really matter if we replace all the $k's$ in $\frac{f(3+k)-f(3)}{k}$ by $3k$ and write, $$f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}},$$ due to which, $f'(3) $ is invariant due to this transformation. By the same preceding logic, why can't I argue about the correctness of this $$\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}$$ ?","Let be a function differentiable at and sayisfying . Then find the limit The thing is, I managed to solve the problem. I will give the detailed steps how I solved it: We observe that is of the form So, we try a popular recommended transformation, as We focus on the limit We assume and as we have, So, We note that so, we write, Hence, I hope my solution is correct. But the thing I am confused about is with my  approach. In my solution, I wrote a part and a part But the thing is, just like if I wrote instead of, my solution would be erroneous as the transformation is not valid. But my question, is, why this is so? Why could I write, and considered this transformation, to be a logical step and to be a faulty transformation (when we are basically, doing the analogous thing in both the cases, i.e as then we must have and so, we might replace all the 's in the limit expression with as both and are approaching the same thing) ? My understanding: My understanding that of why is a valid transformation ? We note, here and hence and the limit expression i.e (before transformation), has all the approaching to zero, and as approaches zero as well, it doesn't really matter if we replace all the in by and write, due to which, is invariant due to this transformation. By the same preceding logic, why can't I argue about the correctness of this ?","f:(0,\infty)\to(0,\infty) 3 f(3)=3f'(3)>0 \lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x. L=\lim_{x\to\infty}(\frac{f(3+\frac 3x)}{f(3)})^x, 1^\infty. L=e^{\lim_{x\to\infty}x(\frac{f(3+\frac 3x)}{f(3)}-1)}\implies e^{\frac 1{f(3)}\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))}. \lim_{x\to\infty}x(f(3+\frac 3x)-f(3)). \frac 1x=k x\to\infty k\to 0. \lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}. f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k} \lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}=3f'(3). L=e^{\lim_{x\to\infty}x(\frac{f(3+\frac 3x)}{f(3)}-1)}\implies e^{\frac 1{f(3)}\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))}=e^{\frac{3f'(3)}{f(3)}}=e^{\frac{3f'(3)}{3f'(3)}}=e. ""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"" ""f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}"". ""f'(3)=\lim_{h\to 0}\frac{f(3+h)-f(3)}{h}\color{blue}{=\lim_{h\to 0}\frac{f(3+3h)-f(3)}{3h}}"", ""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"" ""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{blue}{=3\lim_{k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"", ""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"" ""f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}"", ""\lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}"" k\to 0 3k\to 0 k 3k's k 3k ""f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}"", k\to 0 3k\to 0 \frac{f(3+k)-f(3)}{k} k's 3k k's \frac{f(3+k)-f(3)}{k} 3k f'(3)=\lim_{k\to 0}\frac{f(3+k)-f(3)}{k}\color{blue}{=\lim_{k\to 0}\frac{f(3+3k)-f(3)}{3k}}, f'(3)  \lim_{x\to\infty}x(f(3+\frac 3x)-f(3))=\color{green}{\lim_{k\to 0}\frac 1k(f(3+ 3k)-f(3))}\color{red}{=\lim_{3k\to 0}\frac 1{3k}(f(3+ 3k)-f(3))}","['calculus', 'limits', 'derivatives']"
90,Is the following function continuous? Has partial derivatives?,Is the following function continuous? Has partial derivatives?,,"Let $f(t)$ be a differentiable function for every $t$ such that $f(0)=0$ and $f'(0)=1$ . Define: $$ g(x,y)=\begin{cases} \frac{f(x)^2 + f(y)^2}{x^2+y^2}, &\quad (x,y)\neq(0,0) \\ 1 &\quad (x,y)=(0,0). \end{cases} $$ Is $g$ continuous at $(0,0)$ ? Do the partial derivatives $g_x(0,0),g_y(0,0)$ exist? My attempt: We know that $\lim_{x \to 0} f(x)=0$ and $\lim_{x\to 0}\frac{f(x)}{x}=1$ . Therefore $\lim_{x\to 0}\frac{f(x)^2}{x^2}=1$ . In particular, taking the paths $y=0$ or $x=0$ leads to $\lim_{x\to 0} g(x,0)= \lim_{y\to 0} g(0,y)=0$ . Trying polar coordinates leads to $g(r \cos \theta , r\sin \theta) = \frac{f(r\cos \theta) ^2 + f(r\sin \theta)^2 }{r^2}$ for $r\neq 0$ . If hypothetically I would have known that somehow this limit is zero for any choice of $\theta$ , this would solve the problem, but we don't know that since we don't have $f(r)^2 $ in the numerator. First question: How can I prove the function is continuous at $(0,0)$ ? As for partial derivatives: Using the definition, we need to calculate $$ \lim_{h\to 0} \frac{g(h,0)-g(0,0) }{h } = \lim_{h\to 0} \frac{f(h)^2-h^2 }{h^3 }. $$ My intuition is that this limit doesn't exist, but choosing $f(x)=sin(x)$ or $f(x)=x$ seems to do work. So, second question is: What might be a possible counterexample for the existence of partial derivatives? Thank you","Let be a differentiable function for every such that and . Define: Is continuous at ? Do the partial derivatives exist? My attempt: We know that and . Therefore . In particular, taking the paths or leads to . Trying polar coordinates leads to for . If hypothetically I would have known that somehow this limit is zero for any choice of , this would solve the problem, but we don't know that since we don't have in the numerator. First question: How can I prove the function is continuous at ? As for partial derivatives: Using the definition, we need to calculate My intuition is that this limit doesn't exist, but choosing or seems to do work. So, second question is: What might be a possible counterexample for the existence of partial derivatives? Thank you","f(t) t f(0)=0 f'(0)=1 
g(x,y)=\begin{cases}
\frac{f(x)^2 + f(y)^2}{x^2+y^2}, &\quad (x,y)\neq(0,0) \\
1 &\quad (x,y)=(0,0).
\end{cases}
 g (0,0) g_x(0,0),g_y(0,0) \lim_{x \to 0} f(x)=0 \lim_{x\to 0}\frac{f(x)}{x}=1 \lim_{x\to 0}\frac{f(x)^2}{x^2}=1 y=0 x=0 \lim_{x\to 0} g(x,0)= \lim_{y\to 0} g(0,y)=0 g(r \cos \theta , r\sin \theta) = \frac{f(r\cos \theta) ^2 + f(r\sin \theta)^2 }{r^2} r\neq 0 \theta f(r)^2  (0,0) 
\lim_{h\to 0} \frac{g(h,0)-g(0,0) }{h } = \lim_{h\to 0} \frac{f(h)^2-h^2 }{h^3 }.
 f(x)=sin(x) f(x)=x","['limits', 'multivariable-calculus']"
91,Determining a particular limit and its asymptotic behaviour,Determining a particular limit and its asymptotic behaviour,,"Problem description I have the limit $$ \lim_{n \to \infty} \sum_{k=1}^{n-2}\left( \frac{(n-3)!}{(n-k-2)!} n^{-k+1}a^k\frac{1}{k+2}\right),$$ with $a \in \mathbb{R}_{>0}$ (at Attempt 2 I have typed out the first few terms). I have already shown this limit converges for any $a < 1$ , but I hope to show that it diverges for any $a > 1$ (and hopefully determine its behaviour for $a = 1$ ) as well. I think Attempt 2 could be a way to show this, but I am not sure if the steps I take are actually allowed. Even if they are, I would like a proof more similar to Attempt 1 , since it may show how quickly the sum goes to infinity (so that I could quickly see if e.g. $a^{n/2}$ times this sum would also go to infinity). Any help checking if my proof is correct in Attempt 2 , establishing the behaviour for $a = 1$ or giving a more insightful proof is appreciated. Attempt 1 My first idea was looking at the last term of the sum, which is given by $$(n-3)! n^{3-n} \frac{a^{n-2}}{n}.$$ I know that in general the inequality $$m! \geq e\left( \frac{m}{e} \right)^m $$ holds (I think I could use Stirling for a better estimate, but the reasoning should be the same). Substituting this we see that the last term is at least $$\left(\frac{n}{n-3}\right)^{3-n}e^{4-n}\frac{a^{n-2}}{n}.$$ For $n \to \infty$ the first factor should be finite, which means the last term on itself already clearly converges to infinity for any $a > e$ . I then noted that if we look at earlier terms, we can estimate the fraction of factorials by saying for $k\geq 2$ it is at least $\frac{(n-3)!}{n^{n-2-k}}$ and we can apply the same method as for the last term now which yields something similar for every term, just that we take a lower power of $\frac{a}{e}$ . Even though we have $x$ terms which have at least this value, it is clearly not enough, since for $a < e$ the convergence will be too quick. It seems this method as I applied it right now only works for $a > e$ . Attempt 2 If we just look at the terms of the sum, we get $$\frac{a}{3} + \frac{n-3}{n}\frac{a^2}{4} + \frac{(n-3)(n-4)}{n^2}\frac{a^3}{5} + \frac{(n-3)(n-4)(n-5)}{n^3}\frac{a^4}{6} + \cdots $$ I am very tempted to say that the limit indeed goes to infinity. I know I cannot just swap around sum and limit and say that all fractions containing $n$ go to $1$ , but I think I can do something very similar. For any $N \in \mathbb{R}$ we simply take an l such that $\frac{a^l}{l+2} > N$ , which is clearly possible. Now the term with index $k = l$ on its own will already be at least $N$ , since for $n \to \infty$ the fraction containing $n$ will go to $1$ , so the term will be at least $N$ . Since this is possible for any real $N$ the limit clearly does not converge, and thus diverges. I don't see why this reasoning is wrong, but it feels very weird (and as mentioned, I don't think it's insightful in the behaviour of the function).","Problem description I have the limit with (at Attempt 2 I have typed out the first few terms). I have already shown this limit converges for any , but I hope to show that it diverges for any (and hopefully determine its behaviour for ) as well. I think Attempt 2 could be a way to show this, but I am not sure if the steps I take are actually allowed. Even if they are, I would like a proof more similar to Attempt 1 , since it may show how quickly the sum goes to infinity (so that I could quickly see if e.g. times this sum would also go to infinity). Any help checking if my proof is correct in Attempt 2 , establishing the behaviour for or giving a more insightful proof is appreciated. Attempt 1 My first idea was looking at the last term of the sum, which is given by I know that in general the inequality holds (I think I could use Stirling for a better estimate, but the reasoning should be the same). Substituting this we see that the last term is at least For the first factor should be finite, which means the last term on itself already clearly converges to infinity for any . I then noted that if we look at earlier terms, we can estimate the fraction of factorials by saying for it is at least and we can apply the same method as for the last term now which yields something similar for every term, just that we take a lower power of . Even though we have terms which have at least this value, it is clearly not enough, since for the convergence will be too quick. It seems this method as I applied it right now only works for . Attempt 2 If we just look at the terms of the sum, we get I am very tempted to say that the limit indeed goes to infinity. I know I cannot just swap around sum and limit and say that all fractions containing go to , but I think I can do something very similar. For any we simply take an l such that , which is clearly possible. Now the term with index on its own will already be at least , since for the fraction containing will go to , so the term will be at least . Since this is possible for any real the limit clearly does not converge, and thus diverges. I don't see why this reasoning is wrong, but it feels very weird (and as mentioned, I don't think it's insightful in the behaviour of the function)."," \lim_{n \to \infty} \sum_{k=1}^{n-2}\left( \frac{(n-3)!}{(n-k-2)!} n^{-k+1}a^k\frac{1}{k+2}\right), a \in \mathbb{R}_{>0} a < 1 a > 1 a = 1 a^{n/2} a = 1 (n-3)! n^{3-n} \frac{a^{n-2}}{n}. m! \geq e\left( \frac{m}{e} \right)^m  \left(\frac{n}{n-3}\right)^{3-n}e^{4-n}\frac{a^{n-2}}{n}. n \to \infty a > e k\geq 2 \frac{(n-3)!}{n^{n-2-k}} \frac{a}{e} x a < e a > e \frac{a}{3} + \frac{n-3}{n}\frac{a^2}{4} + \frac{(n-3)(n-4)}{n^2}\frac{a^3}{5} + \frac{(n-3)(n-4)(n-5)}{n^3}\frac{a^4}{6} + \cdots  n 1 N \in \mathbb{R} \frac{a^l}{l+2} > N k = l N n \to \infty n 1 N N","['limits', 'summation']"
92,"Show $ \lim_{x \to 0}\left[\int_0^{1} by+a(1-y)^x\,dy \right]^\frac{1}{x} = \frac{1}{e}\left[\frac{b^b}{a^a}\right]^\frac{1}{b-a}$ for $a < b$",Show  for," \lim_{x \to 0}\left[\int_0^{1} by+a(1-y)^x\,dy \right]^\frac{1}{x} = \frac{1}{e}\left[\frac{b^b}{a^a}\right]^\frac{1}{b-a} a < b","How do we prove; $$L =  \lim_{x \to 0}\left[\int_0^{1} by+a(1-y)^x\,dy \right]^\frac{1}{x} = \frac{1}{e}\left[\frac{b^b}{a^a}\right]^\frac{1}{b-a}, a < b$$ This question is from a test. Let $\displaystyle V = \int_0^{1} by+a(1-y)^x\,dy$ then I used the substitution $by+a(1-y) = t$ to derive $V = \left[\dfrac{b^{x+1}-a^{x+1}}{(x+1)(b-a)}\right]$ , from which, $$L = \lim_{x \to 0}\left[\frac{b^{x+1}-a^{x+1}}{(x+1)(b-a)}\right]^\frac{1}{x}$$ Could anyone suggest how this limit can be evaluated (taking log on both sides did not get me the answer)","How do we prove; This question is from a test. Let then I used the substitution to derive , from which, Could anyone suggest how this limit can be evaluated (taking log on both sides did not get me the answer)","L =  \lim_{x \to 0}\left[\int_0^{1} by+a(1-y)^x\,dy \right]^\frac{1}{x} = \frac{1}{e}\left[\frac{b^b}{a^a}\right]^\frac{1}{b-a}, a < b \displaystyle V = \int_0^{1} by+a(1-y)^x\,dy by+a(1-y) = t V = \left[\dfrac{b^{x+1}-a^{x+1}}{(x+1)(b-a)}\right] L = \lim_{x \to 0}\left[\frac{b^{x+1}-a^{x+1}}{(x+1)(b-a)}\right]^\frac{1}{x}","['calculus', 'limits']"
93,Evaluate $\lim\limits_{x \rightarrow 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x}$,Evaluate,\lim\limits_{x \rightarrow 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x},"$$\lim_{x \to 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x}$$ I was solving this but I got the wrong answer. I want to figure out why it is wrong. I did: $$L=\lim_{x \rightarrow 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x}$$ $$\ln L =\lim_{x \rightarrow 0} \bigg(\frac{4}{\sin x} \frac{2}{x} \  \ln(1+x) -\frac{4}{\sin x}\ln e^2\bigg)$$ $$\ln L =\lim_{x \rightarrow 0} \bigg(\frac{4}{\sin x} \ \frac{2}{x} \  \ln(1+x) -\frac{8}{\sin x}\bigg)$$ Since $\lim\limits_{x\to0} \frac{\ln(1+x)}{x}=1$ , $$\ln L =\lim_{x \rightarrow 0}\bigg(\frac{8}{\sin x}-\frac{8}{\sin x}\bigg)$$ $$\ln L=0$$ $$L=e^0 =1$$ However, the given answer is $e^{-4}$ . Can someone please tell me the mistake I made here. I would be grateful if someone shared the correct solution. Thanks.","I was solving this but I got the wrong answer. I want to figure out why it is wrong. I did: Since , However, the given answer is . Can someone please tell me the mistake I made here. I would be grateful if someone shared the correct solution. Thanks.",\lim_{x \to 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x} L=\lim_{x \rightarrow 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x} \ln L =\lim_{x \rightarrow 0} \bigg(\frac{4}{\sin x} \frac{2}{x} \  \ln(1+x) -\frac{4}{\sin x}\ln e^2\bigg) \ln L =\lim_{x \rightarrow 0} \bigg(\frac{4}{\sin x} \ \frac{2}{x} \  \ln(1+x) -\frac{8}{\sin x}\bigg) \lim\limits_{x\to0} \frac{\ln(1+x)}{x}=1 \ln L =\lim_{x \rightarrow 0}\bigg(\frac{8}{\sin x}-\frac{8}{\sin x}\bigg) \ln L=0 L=e^0 =1 e^{-4},['limits']
94,A curious limit for $-\frac{1}{2}$,A curious limit for,-\frac{1}{2},How to prove this ? $$-\frac{1}{2} = \lim_{x\to+\infty}\sum_{n=1}^{\infty}(-1)^n \frac{x^{2n-1}}{(2n)! \sqrt{\ln 2n}}$$ It reminded me of the fact that $$-\frac\pi2 = \lim_{x\to+\infty}\sum_{n=1}^{\infty}(-1)^n \frac{x^{2n-1}}{(2n)! \ln 2n}$$ that has been proven here : A curious limit for $-\frac{\pi}{2}$,How to prove this ? It reminded me of the fact that that has been proven here : A curious limit for $-\frac{\pi}{2}$,-\frac{1}{2} = \lim_{x\to+\infty}\sum_{n=1}^{\infty}(-1)^n \frac{x^{2n-1}}{(2n)! \sqrt{\ln 2n}} -\frac\pi2 = \lim_{x\to+\infty}\sum_{n=1}^{\infty}(-1)^n \frac{x^{2n-1}}{(2n)! \ln 2n},"['calculus', 'limits', 'alternating-expression']"
95,Doubts in $\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}$,Doubts in,\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x},"Consider the limit: $$\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}$$ Since $\int_{-x}^x \frac{1}{y^2}dy=\int_0^x \frac{1}{y^2}dy-\int_0^{-x} \frac{1}{y^2}dy$ , using Hopital's rule: $$\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}=\lim_{x \to +\infty} \frac{\frac{2}{x^2}}{1}=0$$ However, since $1/y^2$ is unbounded around $y=0$ , using the definition of improper integral: $$\int_{-x}^x \frac{1}{y^2}dy=\int_{-x}^0\frac{1}{y^2}dy+\int_0^x \frac{1}{y^2}dy=\lim_{a\to 0^+}\left(\int_{-x}^{-a}\frac{1}{y^2}dy+\int_a^{x}\frac{1}{y^2}dy\right)=2\lim_{a\to 0^+} \left[\frac{1}{a}-\frac{1}{x}\right]$$ And, for each $x \in\mathbb{R}\setminus\{0\}$ , the limit as $a\to 0^+$ is $+\infty$ . Some questions: (i) What's happening with the definition? After evaluating the integral with the definition I have to take another limit as $x\to+\infty$ , but the integral already diverges to $+\infty$ . Can I conclude that $\lim_{x \to +\infty}\int_{-x}^x \frac{1}{y^2}dy=+\infty$ because (very unprecisely) ""I am taking the limit as $x\to+\infty$ of something that already tends to $+\infty$ ""? More precisely: since the integral is $+\infty$ for each $x \in \mathbb{R}\setminus\{0\}$ , I would say that in particular it remains arbitrary large if $x$ is arbitrarily large. However, the $\delta$ such that $|a|<\delta \implies \int_{-x}^x \frac{1}{y^2}dy>M$ of the limit definition I must choose depends on $x$ too, because it must be $\frac{1}{a}-\frac{1}{x}>M$ . Is this dependence troublesome? (ii) Is the approach with Hopital's rule correct?","Consider the limit: Since , using Hopital's rule: However, since is unbounded around , using the definition of improper integral: And, for each , the limit as is . Some questions: (i) What's happening with the definition? After evaluating the integral with the definition I have to take another limit as , but the integral already diverges to . Can I conclude that because (very unprecisely) ""I am taking the limit as of something that already tends to ""? More precisely: since the integral is for each , I would say that in particular it remains arbitrary large if is arbitrarily large. However, the such that of the limit definition I must choose depends on too, because it must be . Is this dependence troublesome? (ii) Is the approach with Hopital's rule correct?",\lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x} \int_{-x}^x \frac{1}{y^2}dy=\int_0^x \frac{1}{y^2}dy-\int_0^{-x} \frac{1}{y^2}dy \lim_{x \to +\infty} \frac{\int_{-x}^x \frac{1}{y^2}dy}{x}=\lim_{x \to +\infty} \frac{\frac{2}{x^2}}{1}=0 1/y^2 y=0 \int_{-x}^x \frac{1}{y^2}dy=\int_{-x}^0\frac{1}{y^2}dy+\int_0^x \frac{1}{y^2}dy=\lim_{a\to 0^+}\left(\int_{-x}^{-a}\frac{1}{y^2}dy+\int_a^{x}\frac{1}{y^2}dy\right)=2\lim_{a\to 0^+} \left[\frac{1}{a}-\frac{1}{x}\right] x \in\mathbb{R}\setminus\{0\} a\to 0^+ +\infty x\to+\infty +\infty \lim_{x \to +\infty}\int_{-x}^x \frac{1}{y^2}dy=+\infty x\to+\infty +\infty +\infty x \in \mathbb{R}\setminus\{0\} x \delta |a|<\delta \implies \int_{-x}^x \frac{1}{y^2}dy>M x \frac{1}{a}-\frac{1}{x}>M,"['integration', 'limits', 'improper-integrals']"
96,Show that $\lim _{n \rightarrow \infty} \frac{1}{4^n} h\left(2^n P\right)$ exists,Show that  exists,\lim _{n \rightarrow \infty} \frac{1}{4^n} h\left(2^n P\right),"This question comes from Rational Points on Elliptic Curves (Silverman & Tate) exercise 3.3(a). $3.3$ . Let $C$ be a rational cubic curve given by the usual Weierstrass equation. Prove that for any rational point $P \in C(\mathbb{Q})$ , the limit $$\hat{h}(P)=\lim _{n \rightarrow \infty} \frac{1}{4^n} h\left(2^n P\right)$$ exists. The quantity $\hat{h}(P)$ is called the canonical height of $P$ . Note that the height of $x$ is $H(x)=H\left(\frac{m}{n}\right)=\max \{|m|,|n|\}$ and $h(P)=\log H(P)$ It gave a hint to suggest showing the sequence is Cauchy, so here is my attempt: We wish to show that, given $\varepsilon>0$ there exists $N$ such that if $m, n > N$ then $$\left|4^{-n}h(2^nP) - 4^{-m}h(2^mP)\right|<\varepsilon.$$ Write $P = x/y$ so we have $$\left|4^{-n}h(2^n\frac x y) - 4^{-m}h(2^m\frac x y)\right|.$$ Now, I'm not sure how to proceed but my best idea is that because $e^{\varepsilon}$ can be made arbitrarily small just like $\varepsilon$ itself, we can raise everything to the power of $e$ and get rid of small $h$ . Is this the right direction?","This question comes from Rational Points on Elliptic Curves (Silverman & Tate) exercise 3.3(a). . Let be a rational cubic curve given by the usual Weierstrass equation. Prove that for any rational point , the limit exists. The quantity is called the canonical height of . Note that the height of is and It gave a hint to suggest showing the sequence is Cauchy, so here is my attempt: We wish to show that, given there exists such that if then Write so we have Now, I'm not sure how to proceed but my best idea is that because can be made arbitrarily small just like itself, we can raise everything to the power of and get rid of small . Is this the right direction?","3.3 C P \in C(\mathbb{Q}) \hat{h}(P)=\lim _{n \rightarrow \infty} \frac{1}{4^n} h\left(2^n P\right) \hat{h}(P) P x H(x)=H\left(\frac{m}{n}\right)=\max \{|m|,|n|\} h(P)=\log H(P) \varepsilon>0 N m, n > N \left|4^{-n}h(2^nP) - 4^{-m}h(2^mP)\right|<\varepsilon. P = x/y \left|4^{-n}h(2^n\frac x y) - 4^{-m}h(2^m\frac x y)\right|. e^{\varepsilon} \varepsilon e h","['limits', 'number-theory', 'elliptic-curves', 'cauchy-sequences', 'rational-numbers']"
97,Prove that the limit of $\sqrt{x} e^{\sin(\pi/x)}$ as $x$ approaches $0$ from the right,Prove that the limit of  as  approaches  from the right,\sqrt{x} e^{\sin(\pi/x)} x 0,"I don't know what a correct answer is.  Graphing the function with a computer doesn't make it too clear, but it seems to say that it approaches zero from the right.  There's a good chance this problem might be expecting me to use the Squeeze Theorem, but perhaps it might be the Cauchy-Schwartz inequality.  I'm not sure. An attempt . I first look at what happens to $\pi/x$ as $x$ approaches 0 from the right.  I conclude it goes to infinity.  Then I consider what happens to $\sin(\pi/x)$ and I conclude it oscillates around $-1$ and $1$ , that is, $$-1 \leq \sin(\pi/x) \leq 1.$$ Then I raise $e$ to both inequalities and I get $$e^{-1} \leq \sin(\pi/x) \leq e.$$ Now I multiply both inequalities by $\sqrt{x}$ , getting $$\frac{\sqrt{x}}{e} \leq \sqrt{x} e^{\sin(\pi/x)} \leq e \sqrt{x}$$ and by the Squeeze Theorem I would be able to say that it goes to zero. Related question .  What happens if we let $x$ approach $0$ from the left? It seems that the function would also approach zero, but perhaps my book is avoiding the question of what happens when we consider values of the function outside of the domain --- by only asking what happens when the function is not undefined. Another possible solution . I believe the Cauchy-Schwartz inequality could also be of a solution here, but, even if it is, I wonder what an experienced eye would say the book is expecting me to do.  (I have not yet used the Cauchy-Schwartz inequality in this section of the book yet.) Reference .  This exercise is number $38$ in section $2.3$ of Stewart's Calculus 6th edition translated to the Portuguese language.  The original 6th edition seems to ask us to prove $\sqrt{x} [1 + \sin^2(2\pi/x)] = 0$ as $x$ approaches zero from the right.  This problem from the original version seems to be concerned with the Squeeze Theorem and not the Cauchy-Schwartz inequality.","I don't know what a correct answer is.  Graphing the function with a computer doesn't make it too clear, but it seems to say that it approaches zero from the right.  There's a good chance this problem might be expecting me to use the Squeeze Theorem, but perhaps it might be the Cauchy-Schwartz inequality.  I'm not sure. An attempt . I first look at what happens to as approaches 0 from the right.  I conclude it goes to infinity.  Then I consider what happens to and I conclude it oscillates around and , that is, Then I raise to both inequalities and I get Now I multiply both inequalities by , getting and by the Squeeze Theorem I would be able to say that it goes to zero. Related question .  What happens if we let approach from the left? It seems that the function would also approach zero, but perhaps my book is avoiding the question of what happens when we consider values of the function outside of the domain --- by only asking what happens when the function is not undefined. Another possible solution . I believe the Cauchy-Schwartz inequality could also be of a solution here, but, even if it is, I wonder what an experienced eye would say the book is expecting me to do.  (I have not yet used the Cauchy-Schwartz inequality in this section of the book yet.) Reference .  This exercise is number in section of Stewart's Calculus 6th edition translated to the Portuguese language.  The original 6th edition seems to ask us to prove as approaches zero from the right.  This problem from the original version seems to be concerned with the Squeeze Theorem and not the Cauchy-Schwartz inequality.",\pi/x x \sin(\pi/x) -1 1 -1 \leq \sin(\pi/x) \leq 1. e e^{-1} \leq \sin(\pi/x) \leq e. \sqrt{x} \frac{\sqrt{x}}{e} \leq \sqrt{x} e^{\sin(\pi/x)} \leq e \sqrt{x} x 0 38 2.3 \sqrt{x} [1 + \sin^2(2\pi/x)] = 0 x,"['calculus', 'limits']"
98,"Limit of $(|x| + |y|)\ln(x^2 + y^4)$ at $(0,0)$",Limit of  at,"(|x| + |y|)\ln(x^2 + y^4) (0,0)","I want to show that $$\lim\limits_{(x,y) \to (0,0)} (\lvert x \rvert + \lvert y \rvert)\ln(x^2 + y^4) = 0$$ First I let $\lVert (x,y) \rVert = \lvert x \rvert + \lvert y \rvert\ < \delta$ , and assume that $x,y < 1$ so $x^2 + y^4 < \lvert x \rvert + \lvert y \rvert$ . Then $\ln(x^2 + y^4) < \ln(\lvert x \rvert + \lvert y \rvert)$ . However, $\lvert \ln(x^2 + y^4)\rvert > \lvert \ln(\lvert x \rvert + \lvert y \rvert)\rvert$ , which is where I am stuck because I wanted to show that $\lvert(\lvert x \rvert + \lvert y \rvert)\ln(x^2 + y^4)\rvert < \lvert x \rvert + \lvert y \rvert < \delta $ . It does not seem like this approach will work & I am not sure what else I can try.","I want to show that First I let , and assume that so . Then . However, , which is where I am stuck because I wanted to show that . It does not seem like this approach will work & I am not sure what else I can try.","\lim\limits_{(x,y) \to (0,0)} (\lvert x \rvert + \lvert y \rvert)\ln(x^2 + y^4) = 0 \lVert (x,y) \rVert = \lvert x \rvert + \lvert y \rvert\ < \delta x,y < 1 x^2 + y^4 < \lvert x \rvert + \lvert y \rvert \ln(x^2 + y^4) < \ln(\lvert x \rvert + \lvert y \rvert) \lvert \ln(x^2 + y^4)\rvert > \lvert \ln(\lvert x \rvert + \lvert y \rvert)\rvert \lvert(\lvert x \rvert + \lvert y \rvert)\ln(x^2 + y^4)\rvert < \lvert x \rvert + \lvert y \rvert < \delta ","['real-analysis', 'limits', 'multivariable-calculus']"
99,Calculate limits for a function defined by an integral.,Calculate limits for a function defined by an integral.,,"Hello I need help with the second item in the next exercise : The exercise says: Let the function $F : \mathbb{R} \longrightarrow \mathbb{R}$ given by $F(x) = \int_{x}^{2x}e^{-t^{2}}dt$ . (a) Show that $F(x)$ is a odd function. (b) Calculate $\lim_{x \to \infty} F(x)$ and $\lim_{x \to -\infty} F(x)$ . For the first item, I showed that $F'$ is an even function, but for the second item I have no ideas. Some suggestions?","Hello I need help with the second item in the next exercise : The exercise says: Let the function given by . (a) Show that is a odd function. (b) Calculate and . For the first item, I showed that is an even function, but for the second item I have no ideas. Some suggestions?",F : \mathbb{R} \longrightarrow \mathbb{R} F(x) = \int_{x}^{2x}e^{-t^{2}}dt F(x) \lim_{x \to \infty} F(x) \lim_{x \to -\infty} F(x) F',"['calculus', 'integration', 'limits', 'exponential-function']"
