,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Numerically find a potential field from gradient,Numerically find a potential field from gradient,,"I know that the gradient of a potential field/scalar field is a vector field, and given the function of the gradient I know how to integrate each component to get back the original scalar field. But is it possible to do this numerically? For example, if I know the value of the gradient as a vector at a finite number of points in 2 dimensions, how can I determine (or at least approximate) the value of the scalar potential function at each of those points?","I know that the gradient of a potential field/scalar field is a vector field, and given the function of the gradient I know how to integrate each component to get back the original scalar field. But is it possible to do this numerically? For example, if I know the value of the gradient as a vector at a finite number of points in 2 dimensions, how can I determine (or at least approximate) the value of the scalar potential function at each of those points?",,"['multivariable-calculus', 'numerical-methods']"
1,"What is $\Delta\frac{1}{|\mathbf{x}|^2}$, as a distribution?","What is , as a distribution?",\Delta\frac{1}{|\mathbf{x}|^2},"$\newcommand{\x}{\mathbf{x}}$Let $\x$ denote a vector in $\mathbb{R}^3$, $|\x|$ its magintude and $\Delta=\frac{\partial^2}{\partial x 2}+\frac{\partial^2}{\partial y 2}+\frac{\partial^2}{\partial z 2}$ the usual Laplacian. What is $$\Delta\frac{1}{|\x|^2}$$ in the sense of distribution? That is, what is $$\iiint_{\mathbb{R}^3}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x$$ for any test function $\varphi\in C^{\infty}_c(\mathbb{R}^3)$ (the subscript $c$ means compact support). Attempt: I know how to do it for $\Delta\frac{1}{|\x|}$ (we get $\Delta\frac{1}{|\x|}=-4\pi\delta_0$, where $\delta_0$ is the delta distribution at $0$) and some others, and I am trying to reproduce the steps for this case but it does not seem to work. Let $\newcommand{\e}{\varepsilon}\e>0$ and consider the ball $B(0,\e)$ of radius $\e$ centered at $0$. Then,  $$\iiint_{\mathbb{R}^3}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x= \iiint_{\mathbb{R}^3\setminus B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x+\iiint_{B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x$$ Then, we can see that $$\left|\iiint_{B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x\right|\leq \iiint_{B(0,\e)}\frac{1}{|\x|^2}|\Delta\varphi(\x)|d\x \leq C\int_0^\e\frac{1}{\rho^2}\rho^2 d\rho\to0$$ as $\e\to 0$. Now, since $\varphi$ has compact support, there is a compact subset $\Omega\subseteq \mathbb{R}^3$ that includes $B(0,\e)$ such that $\varphi$ and all its derivatives vanish on $\Omega^c$. Thus, $$\iiint_{\mathbb{R}^3\setminus B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x = \iiint_{\Omega\setminus B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x$$ Now by Green's Second Identity, $$\iiint_{\mathbb{R}^3\setminus B(0,\e)}\left(\frac{1}{|\x|^2}\Delta\varphi(\x)-\varphi(\x)\Delta\frac{1}{|\x|^2}\right)d\x = \iint_{\partial\Omega}\left(\frac{1}{|\x|^2}\frac{\partial\varphi(\x)}{\partial\mathbf{n}}-\varphi(\x)\frac{\partial}{\partial\mathbf{n}}\frac{1}{|\x|^2}\right)dS+ \iint_{\partial B(0,\e)}\left(\frac{1}{|\x|^2}\frac{\partial\varphi(\x)}{\partial\mathbf{n}}-\varphi(\x)\frac{\partial}{\partial\mathbf{n}}\frac{1}{|\x|^2}\right)dS$$ Since $\varphi$ and all its derivatives vanish on $\partial \Omega$, we are left with $$\iiint_{\mathbb{R}^3\setminus B(0,\e)}\left(\frac{1}{|\x|^2}\Delta\varphi(\x)-\varphi(\x)\Delta\frac{1}{|\x|^2}\right)d\x= \iint_{\partial B(0,\e)}\left(\frac{1}{|\x|^2}\frac{\partial\varphi(\x)}{\partial\mathbf{n}}-\varphi(\x)\frac{\partial}{\partial\mathbf{n}}\frac{1}{|\x|^2}\right)dS$$ Now usually we can work with the right-hand side to find what it does as $\e\to 0$. But here I don't see how...","$\newcommand{\x}{\mathbf{x}}$Let $\x$ denote a vector in $\mathbb{R}^3$, $|\x|$ its magintude and $\Delta=\frac{\partial^2}{\partial x 2}+\frac{\partial^2}{\partial y 2}+\frac{\partial^2}{\partial z 2}$ the usual Laplacian. What is $$\Delta\frac{1}{|\x|^2}$$ in the sense of distribution? That is, what is $$\iiint_{\mathbb{R}^3}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x$$ for any test function $\varphi\in C^{\infty}_c(\mathbb{R}^3)$ (the subscript $c$ means compact support). Attempt: I know how to do it for $\Delta\frac{1}{|\x|}$ (we get $\Delta\frac{1}{|\x|}=-4\pi\delta_0$, where $\delta_0$ is the delta distribution at $0$) and some others, and I am trying to reproduce the steps for this case but it does not seem to work. Let $\newcommand{\e}{\varepsilon}\e>0$ and consider the ball $B(0,\e)$ of radius $\e$ centered at $0$. Then,  $$\iiint_{\mathbb{R}^3}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x= \iiint_{\mathbb{R}^3\setminus B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x+\iiint_{B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x$$ Then, we can see that $$\left|\iiint_{B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x\right|\leq \iiint_{B(0,\e)}\frac{1}{|\x|^2}|\Delta\varphi(\x)|d\x \leq C\int_0^\e\frac{1}{\rho^2}\rho^2 d\rho\to0$$ as $\e\to 0$. Now, since $\varphi$ has compact support, there is a compact subset $\Omega\subseteq \mathbb{R}^3$ that includes $B(0,\e)$ such that $\varphi$ and all its derivatives vanish on $\Omega^c$. Thus, $$\iiint_{\mathbb{R}^3\setminus B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x = \iiint_{\Omega\setminus B(0,\e)}\frac{1}{|\x|^2}\Delta\varphi(\x)d\x$$ Now by Green's Second Identity, $$\iiint_{\mathbb{R}^3\setminus B(0,\e)}\left(\frac{1}{|\x|^2}\Delta\varphi(\x)-\varphi(\x)\Delta\frac{1}{|\x|^2}\right)d\x = \iint_{\partial\Omega}\left(\frac{1}{|\x|^2}\frac{\partial\varphi(\x)}{\partial\mathbf{n}}-\varphi(\x)\frac{\partial}{\partial\mathbf{n}}\frac{1}{|\x|^2}\right)dS+ \iint_{\partial B(0,\e)}\left(\frac{1}{|\x|^2}\frac{\partial\varphi(\x)}{\partial\mathbf{n}}-\varphi(\x)\frac{\partial}{\partial\mathbf{n}}\frac{1}{|\x|^2}\right)dS$$ Since $\varphi$ and all its derivatives vanish on $\partial \Omega$, we are left with $$\iiint_{\mathbb{R}^3\setminus B(0,\e)}\left(\frac{1}{|\x|^2}\Delta\varphi(\x)-\varphi(\x)\Delta\frac{1}{|\x|^2}\right)d\x= \iint_{\partial B(0,\e)}\left(\frac{1}{|\x|^2}\frac{\partial\varphi(\x)}{\partial\mathbf{n}}-\varphi(\x)\frac{\partial}{\partial\mathbf{n}}\frac{1}{|\x|^2}\right)dS$$ Now usually we can work with the right-hand side to find what it does as $\e\to 0$. But here I don't see how...",,"['multivariable-calculus', 'distribution-theory']"
2,"What is the fastest (most reliable) way to calculate the Frenet frame, curvature and torsion, given r(t)?","What is the fastest (most reliable) way to calculate the Frenet frame, curvature and torsion, given r(t)?",,"Vector calculus, just learned about the Frenet frame and curvature and torsion. Naturally, we have to calculate a lot of these on homework and exams. However, the formulas that we are given for getting curvature, torsion, frame are computationally intensive and usually requires a whole bunch of different calculations (differentiate twice, take a cross product and two absolute values just for $\kappa$ and $\tau$). Being unreliable as a biological computer, I am fairly error prone. It feels like there should be easier and more direct ways of getting these formulae. We have: $\mathrm T=\frac{\mathrm r^\prime}{|r^\prime|}$, $\mathrm B=\frac{\mathrm{r'(\mathit t)\times r''(\mathit t)}}{|\mathrm{r'(\mathit t)\times r''(\mathit t)}|}$, $\mathrm{N=B\times T}$, $\kappa =\frac{|\mathrm{r'(\mathit t)\times r''(\mathit t)}|}{\mathrm |r'(t)|^3}$, $\tau=\frac{(\mathrm{r'(\mathit t)\times r''(\mathit t)})\cdot \mathrm r'''(t)}{|\mathrm{r'(\mathit t)\times r''(\mathit t)}|^2}$ and the Frenet-Serret formulas. I am not sure what strategy I should go for - calculate the unit tangent, differentiate a whole bunch, and take absolute values (go straight for Frennet-Serret) or should I try the given formulas since I have a function with respect to t and not arc length? Is there some other better faster way to do this?","Vector calculus, just learned about the Frenet frame and curvature and torsion. Naturally, we have to calculate a lot of these on homework and exams. However, the formulas that we are given for getting curvature, torsion, frame are computationally intensive and usually requires a whole bunch of different calculations (differentiate twice, take a cross product and two absolute values just for $\kappa$ and $\tau$). Being unreliable as a biological computer, I am fairly error prone. It feels like there should be easier and more direct ways of getting these formulae. We have: $\mathrm T=\frac{\mathrm r^\prime}{|r^\prime|}$, $\mathrm B=\frac{\mathrm{r'(\mathit t)\times r''(\mathit t)}}{|\mathrm{r'(\mathit t)\times r''(\mathit t)}|}$, $\mathrm{N=B\times T}$, $\kappa =\frac{|\mathrm{r'(\mathit t)\times r''(\mathit t)}|}{\mathrm |r'(t)|^3}$, $\tau=\frac{(\mathrm{r'(\mathit t)\times r''(\mathit t)})\cdot \mathrm r'''(t)}{|\mathrm{r'(\mathit t)\times r''(\mathit t)}|^2}$ and the Frenet-Serret formulas. I am not sure what strategy I should go for - calculate the unit tangent, differentiate a whole bunch, and take absolute values (go straight for Frennet-Serret) or should I try the given formulas since I have a function with respect to t and not arc length? Is there some other better faster way to do this?",,['multivariable-calculus']
3,How to prove this inequality with this $\sqrt{n\left(x_{1}^{2}+x_{2}^{2}+\dots+x_{n}^{2}\right)} $,How to prove this inequality with this,\sqrt{n\left(x_{1}^{2}+x_{2}^{2}+\dots+x_{n}^{2}\right)} ,"$x_i \ge 0 \quad (i=1,2,\dots,n)$ and $n \ge 2$ . Prove or disprove: $$ \sqrt{n\left(x_{1}^{2}+x_{2}^{2}+\dots+x_{n}^{2}\right)} \geq \sum_{c y c} \sqrt{x_{1}^{2}+\frac{\left((n-1) x_{1}-x_{2}-\dots-x_{n}\right)^{2}}{4 n(n-1)}} $$ This is a hard problem proposed by Chen Shengli, an expert at mechanical theorem-proving in inequality. His program didn't work on this, so I'd like to share it with you and see if anyone can prove or disprove it (proof by computer is welcomed here).","and . Prove or disprove: This is a hard problem proposed by Chen Shengli, an expert at mechanical theorem-proving in inequality. His program didn't work on this, so I'd like to share it with you and see if anyone can prove or disprove it (proof by computer is welcomed here).","x_i \ge 0 \quad (i=1,2,\dots,n) n \ge 2 
\sqrt{n\left(x_{1}^{2}+x_{2}^{2}+\dots+x_{n}^{2}\right)} \geq \sum_{c y c} \sqrt{x_{1}^{2}+\frac{\left((n-1) x_{1}-x_{2}-\dots-x_{n}\right)^{2}}{4 n(n-1)}}
","['multivariable-calculus', 'inequality', 'summation', 'cauchy-schwarz-inequality']"
4,Reference for proof of Green's theorem,Reference for proof of Green's theorem,,"I'm looking for a rigorous proof of Greens theorem for piecewise smooth jordan curves and would appreciate if someone could link a reference text. The only proof I've seen works for regions which can be bounded by curves $\{(x,y)\in \mathbb{R}^2: a\leq x \leq b,\, \phi(x)\leq y \leq \psi(x)\}$ . The article on wikipedia seems to be lacking several details.",I'm looking for a rigorous proof of Greens theorem for piecewise smooth jordan curves and would appreciate if someone could link a reference text. The only proof I've seen works for regions which can be bounded by curves . The article on wikipedia seems to be lacking several details.,"\{(x,y)\in \mathbb{R}^2: a\leq x \leq b,\, \phi(x)\leq y \leq \psi(x)\}","['multivariable-calculus', 'reference-request', 'greens-theorem']"
5,is function differentiable iff directional derivative is linear,is function differentiable iff directional derivative is linear,,"Original definition A function $f: A \to \mathbb{R}^n$ , $A \subseteq \mathbb{R}^m$ is differentiable at a point $\mathbf a \in \mathbb R^m,$ if there is a linear transformation $T$ such that $$ \lim_{\lVert \mathbf h\rVert \to 0} \frac{f(\mathbf a+\mathbf h)-f(\mathbf a)-T_a(\mathbf h)}{\lVert \mathbf h\rVert} = \mathbf 0. $$ My definition A function $f: A \to \mathbb{R}^n$ , $A \subseteq \mathbb{R}^m$ is differentiable at a point $\mathbf a \in \mathbb R^m,$ if there is a linear transformation $T$ such that for any unit vector $\hat u$ $$ \lim_{t \to 0} \frac{f(\mathbf a+t \hat u)-f(\mathbf a)}{t} = T_a(\hat u). $$ Question: is there any difference between original definition and my definition In the original definition, $\mathbf h$ can approach to $\mathbf 0$ by any trajectory; In my definition it can only be approached from certain direction, so my definition is weaker than the original definition. Substitute $\mathbf h = t \hat u$ to the original definition will get my definition. If my definition is not true, can somebody provide a counter example ? One may already noticed that $$ \partial_{\hat u} f (a) = T_a(\hat u) $$ Since $T$ is linear, we can assume (here $\nabla f$ is just a function, it happens to be equal to the gradient if exist): $$ T_a(\hat u) = \hat u \cdot \nabla f(a) $$ So my definition can be written as: $$ \partial_{\hat u} f (a) = \hat u \cdot \nabla f(a) \Leftrightarrow f\text{ is differentiable at }a $$","Original definition A function , is differentiable at a point if there is a linear transformation such that My definition A function , is differentiable at a point if there is a linear transformation such that for any unit vector Question: is there any difference between original definition and my definition In the original definition, can approach to by any trajectory; In my definition it can only be approached from certain direction, so my definition is weaker than the original definition. Substitute to the original definition will get my definition. If my definition is not true, can somebody provide a counter example ? One may already noticed that Since is linear, we can assume (here is just a function, it happens to be equal to the gradient if exist): So my definition can be written as:","f: A \to \mathbb{R}^n A \subseteq \mathbb{R}^m \mathbf a \in \mathbb R^m, T 
\lim_{\lVert \mathbf h\rVert \to 0}
\frac{f(\mathbf a+\mathbf h)-f(\mathbf a)-T_a(\mathbf h)}{\lVert \mathbf h\rVert} = \mathbf 0.
 f: A \to \mathbb{R}^n A \subseteq \mathbb{R}^m \mathbf a \in \mathbb R^m, T \hat u 
\lim_{t \to 0}
\frac{f(\mathbf a+t \hat u)-f(\mathbf a)}{t} = T_a(\hat u).
 \mathbf h \mathbf 0 \mathbf h = t \hat u 
\partial_{\hat u} f (a) = T_a(\hat u)
 T \nabla f 
T_a(\hat u) = \hat u \cdot \nabla f(a)
 
\partial_{\hat u} f (a) = \hat u \cdot \nabla f(a) \Leftrightarrow f\text{ is differentiable at }a
",['multivariable-calculus']
6,Is there a $\nabla^3 f(x)$?,Is there a ?,\nabla^3 f(x),"In multivariable calculus we have so far seen the gradient, and the hessian, So it is natural to ask whether if $\nabla^3 f(x)$ exists Can anyone let me know what comes after the hessian?","In multivariable calculus we have so far seen the gradient, and the hessian, So it is natural to ask whether if $\nabla^3 f(x)$ exists Can anyone let me know what comes after the hessian?",,"['multivariable-calculus', 'hessian-matrix']"
7,Every immersion is locally an embedding?,Every immersion is locally an embedding?,,Let $\varphi: M \to N$ be an immersion and let $p$ be a point in $M$. How would I go about showing that there exists a neighborhood $V \subset M$ of $p$ such that the restriction $\varphi|V$ of $\varphi$ to $V$ is an embedding?,Let $\varphi: M \to N$ be an immersion and let $p$ be a point in $M$. How would I go about showing that there exists a neighborhood $V \subset M$ of $p$ such that the restriction $\varphi|V$ of $\varphi$ to $V$ is an embedding?,,"['multivariable-calculus', 'differential-geometry']"
8,When are $3$ vectors associative in triple cross products?,When are  vectors associative in triple cross products?,3,"The question I am trying to show under what conditions $$\vec{A}\times(\vec{B}\times\vec{C}) = (\vec{A}\times\vec{B})\times\vec{C}.$$ I have found that right-hand side of the above equation is equal to \begin{align} (\vec{A}\times\vec{B})\times\vec{C} &=-\vec{C}\times(\vec{A}\times\vec{B})\\ &= \vec{C}\times(\vec{B}\times\vec{A}). \end{align} This is similar to the left-hand side of the original equation. The conclusion I arrived at was, in order for the equality to be true, either $\vec{A}, \vec{B}$ or $\vec{C}$ have to be zero, or $\vec{A}$ must be equal to $\vec{C}$. Is this correct? If not, or if I am missing anything, please let me know.","The question I am trying to show under what conditions $$\vec{A}\times(\vec{B}\times\vec{C}) = (\vec{A}\times\vec{B})\times\vec{C}.$$ I have found that right-hand side of the above equation is equal to \begin{align} (\vec{A}\times\vec{B})\times\vec{C} &=-\vec{C}\times(\vec{A}\times\vec{B})\\ &= \vec{C}\times(\vec{B}\times\vec{A}). \end{align} This is similar to the left-hand side of the original equation. The conclusion I arrived at was, in order for the equality to be true, either $\vec{A}, \vec{B}$ or $\vec{C}$ have to be zero, or $\vec{A}$ must be equal to $\vec{C}$. Is this correct? If not, or if I am missing anything, please let me know.",,['multivariable-calculus']
9,"The double integral $\int_0^1 \int_0^1 \frac{1-x}{1-xy}(-\log(xy))^s \, dx \, dy$",The double integral,"\int_0^1 \int_0^1 \frac{1-x}{1-xy}(-\log(xy))^s \, dx \, dy","$$\int_0^1 \int_0^1 \frac{1-x}{1-xy}(-\log(xy))^s \, dx \, dy=\Gamma(s+2)\left(\zeta(s+2) -\frac{1}{s+1}\right) \quad \Re(s)>-2$$ How to prove this identity?","$$\int_0^1 \int_0^1 \frac{1-x}{1-xy}(-\log(xy))^s \, dx \, dy=\Gamma(s+2)\left(\zeta(s+2) -\frac{1}{s+1}\right) \quad \Re(s)>-2$$ How to prove this identity?",,"['multivariable-calculus', 'special-functions']"
10,About the inverse of the Jacobian matrix,About the inverse of the Jacobian matrix,,"I have a doubt on Jacobian matrices. Consider the non linear transformation $$ \left[ \begin{array}{c} x\\ y\\ z \end{array}\right]  = \mathbf{G}\left(  \left[ \begin{array}{c} \hat{x}\\ \hat{y}\\ \hat{z} \end{array}\right] \right) =   \left[ \begin{array}{c} \hat{x}g(\hat{z})\\ \hat{y}g(\hat{z})\\ \hat{z} \end{array}\right] $$ whose Jacobian reads $$ \text{J} =  \left[ \begin{array}{ccc} g & 0 & \hat{x}g'\\ 0 & g & \hat{y}g'\\ 0 & 0 & 1 \end{array} \right] $$ If I invert this matrix I get $$ \text{J}^{-1} =  \left[ \begin{array}{ccc} 1/g & 0 & -\hat{x}g'/g\\ 0 & 1/g & -\hat{y}g'/g\\ 0 & 0 & 1 \end{array} \right] $$ which I thought should be the same as the Jacobian of the inverse transformation. However, solving for $\hat{x}, \hat{y}, \hat{z}$ in the definition of the transformation, I get $$ \left[ \begin{array}{c} \hat{x}\\ \hat{y}\\ \hat{z} \end{array}\right]  = \mathbf{G}^{-1}\left(  \left[ \begin{array}{c} x\\ y\\ z \end{array}\right] \right) =   \left[ \begin{array}{c} x/g(z)\\ y/g(z)\\ z \end{array}\right] $$ whose Jacobian now reads $$ \text{J}^{-1} =  \left[ \begin{array}{ccc} 1/g & 0 & -\hat{x}g'/g^2\\ 0 & 1/g & -\hat{y}g'/g^2\\ 0 & 0 & 1 \end{array} \right] $$ which is slightly different. My question is: which one is the correct Jacobian for the inverse? Weren't they supposed to be the same? If so, where's my mistake? Thank you in advance!","I have a doubt on Jacobian matrices. Consider the non linear transformation $$ \left[ \begin{array}{c} x\\ y\\ z \end{array}\right]  = \mathbf{G}\left(  \left[ \begin{array}{c} \hat{x}\\ \hat{y}\\ \hat{z} \end{array}\right] \right) =   \left[ \begin{array}{c} \hat{x}g(\hat{z})\\ \hat{y}g(\hat{z})\\ \hat{z} \end{array}\right] $$ whose Jacobian reads $$ \text{J} =  \left[ \begin{array}{ccc} g & 0 & \hat{x}g'\\ 0 & g & \hat{y}g'\\ 0 & 0 & 1 \end{array} \right] $$ If I invert this matrix I get $$ \text{J}^{-1} =  \left[ \begin{array}{ccc} 1/g & 0 & -\hat{x}g'/g\\ 0 & 1/g & -\hat{y}g'/g\\ 0 & 0 & 1 \end{array} \right] $$ which I thought should be the same as the Jacobian of the inverse transformation. However, solving for $\hat{x}, \hat{y}, \hat{z}$ in the definition of the transformation, I get $$ \left[ \begin{array}{c} \hat{x}\\ \hat{y}\\ \hat{z} \end{array}\right]  = \mathbf{G}^{-1}\left(  \left[ \begin{array}{c} x\\ y\\ z \end{array}\right] \right) =   \left[ \begin{array}{c} x/g(z)\\ y/g(z)\\ z \end{array}\right] $$ whose Jacobian now reads $$ \text{J}^{-1} =  \left[ \begin{array}{ccc} 1/g & 0 & -\hat{x}g'/g^2\\ 0 & 1/g & -\hat{y}g'/g^2\\ 0 & 0 & 1 \end{array} \right] $$ which is slightly different. My question is: which one is the correct Jacobian for the inverse? Weren't they supposed to be the same? If so, where's my mistake? Thank you in advance!",,['multivariable-calculus']
11,"If  $a+b+c+d=16$, then $(a+\frac{1}{c})^2+(c+\frac{1}{a})^2 + (b+\frac{1}{d})^2 + (d+\frac{1}{b})^2 \geq \frac{289}{4}$","If  , then",a+b+c+d=16 (a+\frac{1}{c})^2+(c+\frac{1}{a})^2 + (b+\frac{1}{d})^2 + (d+\frac{1}{b})^2 \geq \frac{289}{4},"If $a,b,c,d$ are positive integers and $a+b+c+d=16$, prove that    $$\left(a+\frac{1}{c}\right)^2+\left(c+\frac{1}{a}\right)^2+\left(b+\frac{1}{d}\right)^2+\left(d+\frac{1}{b}\right)^2 \geq \frac{289}{4}.$$ I know I have to use some inequality, not sure AM GM will work here or Minkowski inequality.  But I only need hints, not a complete solution.  I want to work on it myself. Please provide only Hints.","If $a,b,c,d$ are positive integers and $a+b+c+d=16$, prove that    $$\left(a+\frac{1}{c}\right)^2+\left(c+\frac{1}{a}\right)^2+\left(b+\frac{1}{d}\right)^2+\left(d+\frac{1}{b}\right)^2 \geq \frac{289}{4}.$$ I know I have to use some inequality, not sure AM GM will work here or Minkowski inequality.  But I only need hints, not a complete solution.  I want to work on it myself. Please provide only Hints.",,"['elementary-number-theory', 'multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality', 'jensen-inequality']"
12,"Confusion with immersions, embeddings, local homeomorphisms, and local diffeomorphisms.","Confusion with immersions, embeddings, local homeomorphisms, and local diffeomorphisms.",,"Definitions. A local homeo/diffemorphism is a continuous/smooth map $f:X\to Y$ such that there's an open cover $(U_i)$ of $X$ for which $f|_{U_i}:U_i\to fU_i$ is a homeo/diffeomorphism. A topological/smooth embedding is a continuous/smooth map $f:X\to Y$ which is a homeo/diffeomorphism onto its image. In other words, the first factor of its image factorization is an isomorphism. If I understand correctly, a local homeo/diffeomorphism is thus precisely a local topological/smooth embedding: there's an open cover of the domain making the restrictions into topological/smooth embeddings. This leads me to two sources of confusion. This answer involves the words ""local diffeomorphism onto its image"". This is strange to me - it seems, at least using the my definition, that a smooth map is a local diffeomorphism iff its a local diffeomorphism onto its image. What am I missing here? This answer proves that any immersion (injective differential) is locally a smooth homeomorphism onto its image with injective derivative, i.e a local immersion which is a local topological embedding. Following my (probably frail) reasoning in the paragraph following the definitions, this would imply any immersion is a local homeomorphism. In fact the answer seems to prove any immersion is locally a smooth embedding (following my definition) since the local section constructed seems smooth. But that would mean it's even a local diffeomorphism! (I am not sure which definition of 'embedding' the asker had in mind). I am confused: (1) makes me think I am corrigibly crazy. (2) makes me think I'm hopelessly crazy, since by the inverse function theorem a smooth map is a local diffeomorphism iff it's an immersion and a submersion, and I also don't think immersions need to be local homeomorphisms. What are my mistakes?","Definitions. A local homeo/diffemorphism is a continuous/smooth map $f:X\to Y$ such that there's an open cover $(U_i)$ of $X$ for which $f|_{U_i}:U_i\to fU_i$ is a homeo/diffeomorphism. A topological/smooth embedding is a continuous/smooth map $f:X\to Y$ which is a homeo/diffeomorphism onto its image. In other words, the first factor of its image factorization is an isomorphism. If I understand correctly, a local homeo/diffeomorphism is thus precisely a local topological/smooth embedding: there's an open cover of the domain making the restrictions into topological/smooth embeddings. This leads me to two sources of confusion. This answer involves the words ""local diffeomorphism onto its image"". This is strange to me - it seems, at least using the my definition, that a smooth map is a local diffeomorphism iff its a local diffeomorphism onto its image. What am I missing here? This answer proves that any immersion (injective differential) is locally a smooth homeomorphism onto its image with injective derivative, i.e a local immersion which is a local topological embedding. Following my (probably frail) reasoning in the paragraph following the definitions, this would imply any immersion is a local homeomorphism. In fact the answer seems to prove any immersion is locally a smooth embedding (following my definition) since the local section constructed seems smooth. But that would mean it's even a local diffeomorphism! (I am not sure which definition of 'embedding' the asker had in mind). I am confused: (1) makes me think I am corrigibly crazy. (2) makes me think I'm hopelessly crazy, since by the inverse function theorem a smooth map is a local diffeomorphism iff it's an immersion and a submersion, and I also don't think immersions need to be local homeomorphisms. What are my mistakes?",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
13,What is the intuition behind a function being differentiable if its partial derivatives are continuous?,What is the intuition behind a function being differentiable if its partial derivatives are continuous?,,"I have seen the proof of it, but I can't quite intuitively understand why continuous partial derivatives imply that a function is differentiable. Thanks! EDIT: The only reason I can think of is that if the partials exist at a point and are continuous at a neighborhood around that point, then it is only logical that the tangent plane to that point exists because the equation for the tangent plane at that point would be a good approximation of the function around that point. If the partials exist at the point but are not continuous at a neighborhood around that point, then I can't see how a tangent plane at the point can exist because would not be a good approximation of the function near the point since the function would have discontinuous slope along certain direction(s)(by the definition of a partial derivative). Is my intuition right?","I have seen the proof of it, but I can't quite intuitively understand why continuous partial derivatives imply that a function is differentiable. Thanks! EDIT: The only reason I can think of is that if the partials exist at a point and are continuous at a neighborhood around that point, then it is only logical that the tangent plane to that point exists because the equation for the tangent plane at that point would be a good approximation of the function around that point. If the partials exist at the point but are not continuous at a neighborhood around that point, then I can't see how a tangent plane at the point can exist because would not be a good approximation of the function near the point since the function would have discontinuous slope along certain direction(s)(by the definition of a partial derivative). Is my intuition right?",,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'vector-analysis']"
14,Using Green's theorem to compute an area of a region,Using Green's theorem to compute an area of a region,,"I want to use Green's theorem for computing the area of the region bounded by the $x$-axis and the arch of the cycloid: $$ x = t- \sin (t),\;\;\; y = 1 - \cos (t),\;\; 0 \leq t \leq 2\pi $$ So basically, I know the radius of this cycloid is 1. And to use Green's theorem, I will need to find $Q$ and $P$. $$\int_C P\,dx + Q\,dy =  \iint_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA$$","I want to use Green's theorem for computing the area of the region bounded by the $x$-axis and the arch of the cycloid: $$ x = t- \sin (t),\;\;\; y = 1 - \cos (t),\;\; 0 \leq t \leq 2\pi $$ So basically, I know the radius of this cycloid is 1. And to use Green's theorem, I will need to find $Q$ and $P$. $$\int_C P\,dx + Q\,dy =  \iint_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA$$",,['multivariable-calculus']
15,Another difficult 2D trigonometric integral,Another difficult 2D trigonometric integral,,"This is a follow-up question to A difficult 2d trigonometric integral . Unfortunately, I had a mistake in my calculations and I need a different (yet similar) seemingly simple integral solved: $$\int_{a}^{b}\int_{a}^{y}\frac{\sin(x-y)}{xy}\mathop{\mathrm{d}x}\mathop{\mathrm{d}y}$$ For some $0<a<b$ . The same methods as before can not be applied here, and I've been sitting for hours trying to solve this. Solutions (also using $\mathop{\mathrm{Si}}$ & $\mathop{\mathrm{Ci}}$ functions) would be very-very-very appreciated. Thanks!","This is a follow-up question to A difficult 2d trigonometric integral . Unfortunately, I had a mistake in my calculations and I need a different (yet similar) seemingly simple integral solved: For some . The same methods as before can not be applied here, and I've been sitting for hours trying to solve this. Solutions (also using & functions) would be very-very-very appreciated. Thanks!",\int_{a}^{b}\int_{a}^{y}\frac{\sin(x-y)}{xy}\mathop{\mathrm{d}x}\mathop{\mathrm{d}y} 0<a<b \mathop{\mathrm{Si}} \mathop{\mathrm{Ci}},"['multivariable-calculus', 'definite-integrals', 'trigonometric-integrals']"
16,"If partial derivatives of $f$ is bounded, show that $f$ is continuous.","If partial derivatives of  is bounded, show that  is continuous.",f f,"Let a function $f(x,y)$ be defined in an open set $D$ of the plane, and suppose that $f_1$ and $f_2$ are defined and bounded everywhere in $D$. Show that $f$ is continuous in $D$. The answer says ""Using the mean value theorem, show that $|f(p)-f(p_0)|\le M|p-p_0|$"" But in order to use the mean value theroem, shouldn't we assume f is a continuous function, which is the aim? How can we use it? Even if I use it, I couldn't quite get the statement answer is saying. Any help is welcomed. Thanks in advance.","Let a function $f(x,y)$ be defined in an open set $D$ of the plane, and suppose that $f_1$ and $f_2$ are defined and bounded everywhere in $D$. Show that $f$ is continuous in $D$. The answer says ""Using the mean value theorem, show that $|f(p)-f(p_0)|\le M|p-p_0|$"" But in order to use the mean value theroem, shouldn't we assume f is a continuous function, which is the aim? How can we use it? Even if I use it, I couldn't quite get the statement answer is saying. Any help is welcomed. Thanks in advance.",,"['multivariable-calculus', 'continuity']"
17,A calculus problem from electrostatics,A calculus problem from electrostatics,,"Since this problem consists of multiple parts and one needs to see all of them to understand the problem i'm going to list out all of them: Consider a uniformly charged spherical shell of radius $R$ with surface charge density $\sigma_0$ . Compute the potential outtside the shell, $r>R$ . Now consider we slighttly squzee the sphere (without changing the surface charge density) along one axis so that the surface is now located at $$r=\frac{R}{\left(b^2\cos^2(\beta)+\sin^2(\beta)\right)^{1/2}}$$ where $b$ is a number such that $0<b^2-1 \ll 1$ and $\beta$ is the angle between a point on the surface and the symmetry axis of the system. Find the potential for $r<R$ neglecting terms $O(\epsilon^2)$ where $\epsilon=b^2-1$ . Hint: Recall the potential in terms of the charge volume density $\rho$ is $$\Phi(\vec{r})=\frac{1}{4\pi\epsilon_0}\sum_{n=0}^{\infty}\frac{1}{r^{n+1}}\int \mathrm d^3r'(r')^n\rho (\vec{r}')P_n(\cos\alpha)$$ where $\alpha$ is the angle between $\vec{r}$ and $\vec{r}'$ . The first question is simple enough, but I don't really understand the second question at all. Appreciate any help! EDIT : I shall provide the (my) solution to the first part of the question: The potential of a uniformly charged spherical shell of radius $R$ with surface charge density $\sigma_0$ outside of $R$ is simply $V=\frac{1}{4\pi\epsilon_0}\frac{Q}{r}$ , for $r > R$ . To formulate my question a bit better: Since the potential in electrostatics is generally given by the equation: $$V(r)=\frac{1}{4\pi\epsilon_0}\int \frac{dq}{r}$$ or in this case (using spherical coordinates): $$V(r)=\frac{\sigma_0}{4\pi\epsilon_0}\int \frac{dA}{r}=\frac{\sigma_0}{4\pi\epsilon_0}\int \frac{1}{r}R^2d\theta d\phi$$ Now if the sphere is ""slightly squeezed"", I can no longer use the above equation, and the shape of the sphere becomes somewhat of two (if I can assume again) U-shaped magnets with their open ends closed together. But then i don't understand the part where it says ""the surface is now located at $$r=\frac{R}{\left(b^2\cos^2(\beta)+\sin^2(\beta)\right)^{1/2}}$$ "" Is this $r$ referring to the center of the object?","Since this problem consists of multiple parts and one needs to see all of them to understand the problem i'm going to list out all of them: Consider a uniformly charged spherical shell of radius with surface charge density . Compute the potential outtside the shell, . Now consider we slighttly squzee the sphere (without changing the surface charge density) along one axis so that the surface is now located at where is a number such that and is the angle between a point on the surface and the symmetry axis of the system. Find the potential for neglecting terms where . Hint: Recall the potential in terms of the charge volume density is where is the angle between and . The first question is simple enough, but I don't really understand the second question at all. Appreciate any help! EDIT : I shall provide the (my) solution to the first part of the question: The potential of a uniformly charged spherical shell of radius with surface charge density outside of is simply , for . To formulate my question a bit better: Since the potential in electrostatics is generally given by the equation: or in this case (using spherical coordinates): Now if the sphere is ""slightly squeezed"", I can no longer use the above equation, and the shape of the sphere becomes somewhat of two (if I can assume again) U-shaped magnets with their open ends closed together. But then i don't understand the part where it says ""the surface is now located at "" Is this referring to the center of the object?",R \sigma_0 r>R r=\frac{R}{\left(b^2\cos^2(\beta)+\sin^2(\beta)\right)^{1/2}} b 0<b^2-1 \ll 1 \beta r<R O(\epsilon^2) \epsilon=b^2-1 \rho \Phi(\vec{r})=\frac{1}{4\pi\epsilon_0}\sum_{n=0}^{\infty}\frac{1}{r^{n+1}}\int \mathrm d^3r'(r')^n\rho (\vec{r}')P_n(\cos\alpha) \alpha \vec{r} \vec{r}' R \sigma_0 R V=\frac{1}{4\pi\epsilon_0}\frac{Q}{r} r > R V(r)=\frac{1}{4\pi\epsilon_0}\int \frac{dq}{r} V(r)=\frac{\sigma_0}{4\pi\epsilon_0}\int \frac{dA}{r}=\frac{\sigma_0}{4\pi\epsilon_0}\int \frac{1}{r}R^2d\theta d\phi r=\frac{R}{\left(b^2\cos^2(\beta)+\sin^2(\beta)\right)^{1/2}} r,"['multivariable-calculus', 'physics', 'electromagnetism']"
18,Stokes' Theorem for cross product with line element,Stokes' Theorem for cross product with line element,,"Let $\partial \mathcal{S}$ be a simple closed curve and $\mathcal{S}$ the region enclosed by it, with $\mathcal{S} \subset \mathbf{R}^3$. Let $\vec{F}$ be a vector field in $\mathbf{R}^3$. If $\mathcal{S}$ is contained in the $x$-$y$ plane and $\vec{F}$ has only $x$ and $y$ components, one can check that \begin{equation} \int_{\partial \mathcal{S}} \vec{F} \times \mathrm{d} \vec{l} = \hat{n}\int_\mathcal{S} \nabla \cdot \vec{F} \, \mathrm{d}x \, \mathrm{d}y \, , \end{equation} where $\hat{n}$ is given by the right-hand-screw rule in relation to the sense of the line integral around $\partial \mathcal{S}$. This is just another application of Stokes' Theorem. I have tried to find the appropriate generalization for $\mathcal{S}$ and $\vec{F}$ not constrained to live in a plane, with $\partial \mathcal{S}$ still a simple closed curve. When I try to work the result using the standard form of Stokes' theorem and vector identities I get \begin{equation} \int_{\partial \mathcal{S}} \vec{F} \times \mathrm{d} \vec{l} = \int_\mathcal{S}\left[ ( \nabla \cdot \vec{F} ) \hat{n}- \left( \nabla F_i \right) n_i \right]\, \mathrm{d}s \, , \end{equation} with implicit summation over $i$. This seems to be consistent with the 2D case and I think it works but I could not find it elsewhere and I wanted to make sure if this is right. I would be very grateful if you could let me know if you agree.","Let $\partial \mathcal{S}$ be a simple closed curve and $\mathcal{S}$ the region enclosed by it, with $\mathcal{S} \subset \mathbf{R}^3$. Let $\vec{F}$ be a vector field in $\mathbf{R}^3$. If $\mathcal{S}$ is contained in the $x$-$y$ plane and $\vec{F}$ has only $x$ and $y$ components, one can check that \begin{equation} \int_{\partial \mathcal{S}} \vec{F} \times \mathrm{d} \vec{l} = \hat{n}\int_\mathcal{S} \nabla \cdot \vec{F} \, \mathrm{d}x \, \mathrm{d}y \, , \end{equation} where $\hat{n}$ is given by the right-hand-screw rule in relation to the sense of the line integral around $\partial \mathcal{S}$. This is just another application of Stokes' Theorem. I have tried to find the appropriate generalization for $\mathcal{S}$ and $\vec{F}$ not constrained to live in a plane, with $\partial \mathcal{S}$ still a simple closed curve. When I try to work the result using the standard form of Stokes' theorem and vector identities I get \begin{equation} \int_{\partial \mathcal{S}} \vec{F} \times \mathrm{d} \vec{l} = \int_\mathcal{S}\left[ ( \nabla \cdot \vec{F} ) \hat{n}- \left( \nabla F_i \right) n_i \right]\, \mathrm{d}s \, , \end{equation} with implicit summation over $i$. This seems to be consistent with the 2D case and I think it works but I could not find it elsewhere and I wanted to make sure if this is right. I would be very grateful if you could let me know if you agree.",,['multivariable-calculus']
19,How prove this inequality generalized from 1969 IMO problem 6,How prove this inequality generalized from 1969 IMO problem 6,,"Let    $x_{1},x_{2},y_{1},y_{2},z_{1},z_{2},w_{1},w_{2} $ are all positive numbers, and such   $$x_{1}y_{1}z_{1}-w^3_{1}>0,\; \text{ and }\;x_{2}y_{2}z_{2}-w^3_{2}>0.$$   show that   $$\dfrac{16}{(x_{1}+x_{2})(y_{1}+y_{2})(z_{1}+z_{2})-(w_{1}+w_{2})^3}\le\dfrac{1}{x_{1}y_{1}z_{1}-w^3_{1}}+\dfrac{1}{x_{2}y_{2}z_{2}-w^3_{2}}.$$ This problem is created by me, and the background is the 1969 IMO problem 6, please see: http://www.artofproblemsolving.com/Forum/viewtopic.php?p=363659&sid=efafc24957d8919546afe4254638aea6#p363659 Thank you everyone for showing a proof, I still can't prove it.","Let    $x_{1},x_{2},y_{1},y_{2},z_{1},z_{2},w_{1},w_{2} $ are all positive numbers, and such   $$x_{1}y_{1}z_{1}-w^3_{1}>0,\; \text{ and }\;x_{2}y_{2}z_{2}-w^3_{2}>0.$$   show that   $$\dfrac{16}{(x_{1}+x_{2})(y_{1}+y_{2})(z_{1}+z_{2})-(w_{1}+w_{2})^3}\le\dfrac{1}{x_{1}y_{1}z_{1}-w^3_{1}}+\dfrac{1}{x_{2}y_{2}z_{2}-w^3_{2}}.$$ This problem is created by me, and the background is the 1969 IMO problem 6, please see: http://www.artofproblemsolving.com/Forum/viewtopic.php?p=363659&sid=efafc24957d8919546afe4254638aea6#p363659 Thank you everyone for showing a proof, I still can't prove it.",,"['multivariable-calculus', 'inequality', 'convex-analysis', 'contest-math']"
20,Lagrange's multiplier not working,Lagrange's multiplier not working,,"Given the function $f(x,y):=xy+x-y$. Let $D:=\{(x,y)\in\mathbb{R}^2:x^2+y^2\leq25\wedge x \geq 0\}$. Find the absolute maximum and minimum of $f$ on $D$. My working is as follows: $\begin{array} & f_x(x,y)=y+1=0 & \qquad \qquad f_y(x,y)=x-1=0 \\ \Rightarrow y=-1 & \qquad \qquad \Rightarrow x=1 \end{array}$ $D(x,y)=\begin{vmatrix} f_{xx}(x,y) & f_{xy}(x,y) \\ f_{xy}(x,y) & f_{yy}(x,y)  \end{vmatrix} = \begin{vmatrix} 0 & 1 \\ 1 & 0  \end{vmatrix} = -1$ $D(x,y) = D(1,-1) < 0 \Rightarrow (1,-1)$ is a saddle point. Also, just for interest, $f(1,-1)=1$ To find the maximum and minimum of $f$ subject to $x^2+y^2=25$ I will use a Lagrange multiplier. $\nabla f(x,y) = \lambda \nabla g(x,y)$ where $g(x,y)=x^2+y^2-25$ $\langle y+1, x-1 \rangle = \lambda \langle 2x, 2y \rangle$ $\left\{\begin{array}{llll} y+1=2\lambda x & \Rightarrow & y=2\lambda x -1 & (1) \\ x-1=2\lambda y & \Rightarrow & x=2\lambda y +1 & (2) \\ x^2+y^2=25 & & & (3) \end{array}\right.$ Putting (1) into (2) and (2) into (1) gives $$x=\frac{1}{1+2\lambda} \qquad \text{and} \qquad y=-\frac{1}{1+2\lambda}\tag{4}$$ Where $\lambda \neq \pm \frac12$ Putting (4) into (3) gives $$\lambda = \frac{-5\pm \sqrt2}{10} \approx -0.64 \quad \text{or} \quad -0.36$$ Subsequently, $$x \approx \pm 3.54 \quad \text{and} \quad y \approx \mp 3.54$$ Note that $x=-y$. So, $$f(3.54,-3.54) \approx -5.43 \quad \text{and} \quad f(-3.54,3.54) \approx -19.57$$ By this calculation, (-3.54, 3.54, -19.57) would be a point of absolute minimum on the circle $x^2+y^2=25$. But $x\geq 0$. Hmm. Let me try evaluating $f(0,5)$. $$f(0,5)=-5 \nless f(3.54,-3.54) \approx -5.43$$ Nope. What should I do now to find the minimum in a procedurally correct way? I also cannot find the absolute maximum. $(1,-1,1)$ is not the absolute maximum because I have found that $(3.54, 3.54, 12.5)$ exists on $D$. Why did my calculation using Lagrange's multiplier not give me this point?","Given the function $f(x,y):=xy+x-y$. Let $D:=\{(x,y)\in\mathbb{R}^2:x^2+y^2\leq25\wedge x \geq 0\}$. Find the absolute maximum and minimum of $f$ on $D$. My working is as follows: $\begin{array} & f_x(x,y)=y+1=0 & \qquad \qquad f_y(x,y)=x-1=0 \\ \Rightarrow y=-1 & \qquad \qquad \Rightarrow x=1 \end{array}$ $D(x,y)=\begin{vmatrix} f_{xx}(x,y) & f_{xy}(x,y) \\ f_{xy}(x,y) & f_{yy}(x,y)  \end{vmatrix} = \begin{vmatrix} 0 & 1 \\ 1 & 0  \end{vmatrix} = -1$ $D(x,y) = D(1,-1) < 0 \Rightarrow (1,-1)$ is a saddle point. Also, just for interest, $f(1,-1)=1$ To find the maximum and minimum of $f$ subject to $x^2+y^2=25$ I will use a Lagrange multiplier. $\nabla f(x,y) = \lambda \nabla g(x,y)$ where $g(x,y)=x^2+y^2-25$ $\langle y+1, x-1 \rangle = \lambda \langle 2x, 2y \rangle$ $\left\{\begin{array}{llll} y+1=2\lambda x & \Rightarrow & y=2\lambda x -1 & (1) \\ x-1=2\lambda y & \Rightarrow & x=2\lambda y +1 & (2) \\ x^2+y^2=25 & & & (3) \end{array}\right.$ Putting (1) into (2) and (2) into (1) gives $$x=\frac{1}{1+2\lambda} \qquad \text{and} \qquad y=-\frac{1}{1+2\lambda}\tag{4}$$ Where $\lambda \neq \pm \frac12$ Putting (4) into (3) gives $$\lambda = \frac{-5\pm \sqrt2}{10} \approx -0.64 \quad \text{or} \quad -0.36$$ Subsequently, $$x \approx \pm 3.54 \quad \text{and} \quad y \approx \mp 3.54$$ Note that $x=-y$. So, $$f(3.54,-3.54) \approx -5.43 \quad \text{and} \quad f(-3.54,3.54) \approx -19.57$$ By this calculation, (-3.54, 3.54, -19.57) would be a point of absolute minimum on the circle $x^2+y^2=25$. But $x\geq 0$. Hmm. Let me try evaluating $f(0,5)$. $$f(0,5)=-5 \nless f(3.54,-3.54) \approx -5.43$$ Nope. What should I do now to find the minimum in a procedurally correct way? I also cannot find the absolute maximum. $(1,-1,1)$ is not the absolute maximum because I have found that $(3.54, 3.54, 12.5)$ exists on $D$. Why did my calculation using Lagrange's multiplier not give me this point?",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
21,How to calculate this area?,How to calculate this area?,,"Calculate the area of $$S:=\lbrace (x,y,z)\in\mathbb{R}^3: y^2+(z-x^2)^2=x^2-1\rbrace$$ Anyone have an idea? I tried using cylindrical coordinates, but nothing. Well, I have something, but I'm not really sure... Ok: We have the surface $$y^2+(z-x^2)^2=x^2-1 \Rightarrow (z-x^2)^2=x^2-1-y^2$$  $$\Rightarrow z-x^2=\sqrt{x^2-1-y^2} \Rightarrow z(x,y) =  \sqrt{x^2-1-y^2}+x^2$$ Now we put some reestriction on z. We have two cases: $$S^{-} = \lbrace C \cap \lbrace z<x^2\rbrace \rbrace$$ $$S^{+} = \lbrace C \cap \lbrace z>x^2\rbrace \rbrace$$ Let's work on $S^{+}$. Using the polar parametrization $x=rcos(\theta)$ e $y=rsin(\theta)$ we have: $$z(\theta, r) = \sqrt{r^2cos(2\theta)-1}+r^2cos^2 (\theta)$$ Where we have 2 restrictions: $r>1$ and $r^2cos(\theta)>1$ (looking the root). So we can calculate the integration limits. And we have the vector: $$\gamma(\theta, r) = (rcos(\theta), rsin(\theta), z(\theta, t))$$ So we only need use the area equation $$\int \int \left| \frac{\partial\gamma}{\partial \theta}\times \frac{\partial\gamma}{\partial r}\right|d\theta dr$$","Calculate the area of $$S:=\lbrace (x,y,z)\in\mathbb{R}^3: y^2+(z-x^2)^2=x^2-1\rbrace$$ Anyone have an idea? I tried using cylindrical coordinates, but nothing. Well, I have something, but I'm not really sure... Ok: We have the surface $$y^2+(z-x^2)^2=x^2-1 \Rightarrow (z-x^2)^2=x^2-1-y^2$$  $$\Rightarrow z-x^2=\sqrt{x^2-1-y^2} \Rightarrow z(x,y) =  \sqrt{x^2-1-y^2}+x^2$$ Now we put some reestriction on z. We have two cases: $$S^{-} = \lbrace C \cap \lbrace z<x^2\rbrace \rbrace$$ $$S^{+} = \lbrace C \cap \lbrace z>x^2\rbrace \rbrace$$ Let's work on $S^{+}$. Using the polar parametrization $x=rcos(\theta)$ e $y=rsin(\theta)$ we have: $$z(\theta, r) = \sqrt{r^2cos(2\theta)-1}+r^2cos^2 (\theta)$$ Where we have 2 restrictions: $r>1$ and $r^2cos(\theta)>1$ (looking the root). So we can calculate the integration limits. And we have the vector: $$\gamma(\theta, r) = (rcos(\theta), rsin(\theta), z(\theta, t))$$ So we only need use the area equation $$\int \int \left| \frac{\partial\gamma}{\partial \theta}\times \frac{\partial\gamma}{\partial r}\right|d\theta dr$$",,['multivariable-calculus']
22,Prove or disprove by counterexample $\sup_{x\in \mathbb{R}^n}\frac{[\nabla (\nabla \cdot f)]\cdot f}{(1+|\nabla\cdot f|)^2}<\infty$ [closed],Prove or disprove by counterexample  [closed],\sup_{x\in \mathbb{R}^n}\frac{[\nabla (\nabla \cdot f)]\cdot f}{(1+|\nabla\cdot f|)^2}<\infty,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $f\colon \mathbb{R}^n\to \mathbb{R}^n$ be twice continuously differentiable. Prove or disprove by counterexample that \begin{equation} \sup_{x\in \mathbb{R}^n}\frac{[\nabla (\nabla \cdot f)]\cdot f}{(1+|\nabla\cdot f|)^2}<\infty, \end{equation} where $``\cdot""$ denotes the dot product, $``\nabla""$ denotes gradient, and $``\nabla \cdot""$ denotes divergence. Any help is appreciated. Here is my attempt: If we know that the Taylor series of $f(x)$ converges to $f(x)$ , then I can show the claim at least for the scalar case. But I do not know how to do this rigorously for a general case.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let be twice continuously differentiable. Prove or disprove by counterexample that where denotes the dot product, denotes gradient, and denotes divergence. Any help is appreciated. Here is my attempt: If we know that the Taylor series of converges to , then I can show the claim at least for the scalar case. But I do not know how to do this rigorously for a general case.","f\colon \mathbb{R}^n\to \mathbb{R}^n \begin{equation}
\sup_{x\in \mathbb{R}^n}\frac{[\nabla (\nabla \cdot f)]\cdot f}{(1+|\nabla\cdot f|)^2}<\infty,
\end{equation} ``\cdot"" ``\nabla"" ``\nabla \cdot"" f(x) f(x)","['multivariable-calculus', 'vector-analysis']"
23,"If the second derivatives $f_{xx}$ and $f_{yy}$ exist, does $f_{xy}$ exist?","If the second derivatives  and  exist, does  exist?",f_{xx} f_{yy} f_{xy},"If the second derivative with respect to to $x$ exists ($f_{xx}$) and the second derivative with respect to $y$ ($f_{yy}$), does it follow that $f_{xy}$ exists?","If the second derivative with respect to to $x$ exists ($f_{xx}$) and the second derivative with respect to $y$ ($f_{yy}$), does it follow that $f_{xy}$ exists?",,"['multivariable-calculus', 'derivatives']"
24,Big-O Notation for remainder terms in Taylor expansion,Big-O Notation for remainder terms in Taylor expansion,,"The Big-O notation is commonly used in Taylor expansions of the form $$f(x+\epsilon)=f(x)+\epsilon f'(x)+O(\epsilon^2)$$ to say that the remainder term grows at least quadratic around $\epsilon=0$. Considering the analogue case of a function $f:\mathbb{R}^2\to\mathbb{R}$: $$f(x+\epsilon_1,y+\epsilon_2)=f(x,y)+\epsilon_1 f_x(x,y)+\epsilon_2 f_y(x,y)+\epsilon_1\epsilon_2 f_{x,y}(x,y)+O(?)$$ my question is: What would be a suitable notation for the remainder term? The notation should say precisely that the term contains only contributions of order $\geq2$ in at least one of the variables $\epsilon_1,\epsilon_2$ around $(\epsilon_1,\epsilon_2)=(0,0)$. I am also not sure if Big-O notation is the right thing to use here, as i read here its generalisation to 2D is quite nontrivial. However, the difficulties of this issue might be unimportant in my context, because they arise when describing the asymptotic behavior $\epsilon\to\infty$, whereas my question is on describing the local behaviour of the function for $\epsilon\to0$.","The Big-O notation is commonly used in Taylor expansions of the form $$f(x+\epsilon)=f(x)+\epsilon f'(x)+O(\epsilon^2)$$ to say that the remainder term grows at least quadratic around $\epsilon=0$. Considering the analogue case of a function $f:\mathbb{R}^2\to\mathbb{R}$: $$f(x+\epsilon_1,y+\epsilon_2)=f(x,y)+\epsilon_1 f_x(x,y)+\epsilon_2 f_y(x,y)+\epsilon_1\epsilon_2 f_{x,y}(x,y)+O(?)$$ my question is: What would be a suitable notation for the remainder term? The notation should say precisely that the term contains only contributions of order $\geq2$ in at least one of the variables $\epsilon_1,\epsilon_2$ around $(\epsilon_1,\epsilon_2)=(0,0)$. I am also not sure if Big-O notation is the right thing to use here, as i read here its generalisation to 2D is quite nontrivial. However, the difficulties of this issue might be unimportant in my context, because they arise when describing the asymptotic behavior $\epsilon\to\infty$, whereas my question is on describing the local behaviour of the function for $\epsilon\to0$.",,"['multivariable-calculus', 'asymptotics', 'taylor-expansion']"
25,The set of critical values of a polynomial $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is finite,The set of critical values of a polynomial  is finite,f: \mathbb{R}^n \rightarrow \mathbb{R},"I have result: measure of the set of critical values of $f$ is zero (by Sard's theorem), where $f: \mathbb{R^n} \rightarrow \mathbb{R}$ are polynomial functions. How do you show that the set of critical values of $f$ is finite?","I have result: measure of the set of critical values of $f$ is zero (by Sard's theorem), where $f: \mathbb{R^n} \rightarrow \mathbb{R}$ are polynomial functions. How do you show that the set of critical values of $f$ is finite?",,"['multivariable-calculus', 'polynomials']"
26,Evaluate the line integral along a parabola,Evaluate the line integral along a parabola,,"Evaluate the line integral of $f(x,y)=-y+x$ along part of the parabola $y=2(x+1)^2$ from the point $(0,2)$ to the point $(-1,0)$ I need help trying to find a good parameterization for this because what I've done just lands me in a mess. My work so far: Let $x=t, y=2(t+1)^2$ $$ \begin{split} r(t)  &= \left<t,2(t+1)^2\right>, \quad -1\leq t \leq 0 \\ r'(t) &= \left<1,4(t+1)\right>,   \quad -1\leq t \leq 0 \\ \|r'(t)\| &= \sqrt{1+16(t+1)^2} \\ -y+x &= -2(t+1)^2+t\\      &=-2t^2-3t-2\\ \end{split} $$ So we have $$ \int_0^{-1}\left(-2t^2-3t-2\right)\sqrt{1+16(t+1)^2}dt $$ This integral is really ugly. so then I tried a different method: $$ \begin{split} y &= 2(x+1)^2 \\ \frac{dy}{dx} &= 4x+4 \\ dS &= \sqrt{1+(4x+4)^2} dx\\    &=\sqrt{16x^2+32x+17} dx \end{split} $$ So we get $$\int_0^{-1} \left(-2(x+1)^2+x\right)\sqrt{16x^2+32x+17}dx$$ Again, very ugly. Can someone please help me solve this?","Evaluate the line integral of along part of the parabola from the point to the point I need help trying to find a good parameterization for this because what I've done just lands me in a mess. My work so far: Let So we have This integral is really ugly. so then I tried a different method: So we get Again, very ugly. Can someone please help me solve this?","f(x,y)=-y+x y=2(x+1)^2 (0,2) (-1,0) x=t, y=2(t+1)^2 
\begin{split}
r(t)  &= \left<t,2(t+1)^2\right>, \quad -1\leq t \leq 0 \\
r'(t) &= \left<1,4(t+1)\right>,   \quad -1\leq t \leq 0 \\
\|r'(t)\| &= \sqrt{1+16(t+1)^2} \\
-y+x &= -2(t+1)^2+t\\
     &=-2t^2-3t-2\\
\end{split}
 
\int_0^{-1}\left(-2t^2-3t-2\right)\sqrt{1+16(t+1)^2}dt
 
\begin{split}
y &= 2(x+1)^2 \\
\frac{dy}{dx} &= 4x+4 \\
dS &= \sqrt{1+(4x+4)^2} dx\\
   &=\sqrt{16x^2+32x+17} dx
\end{split}
 \int_0^{-1} \left(-2(x+1)^2+x\right)\sqrt{16x^2+32x+17}dx","['multivariable-calculus', 'line-integrals']"
27,How to show that $D_{f}$ is a Borel function,How to show that  is a Borel function,D_{f},"How to show that $D_{f}$ is a Borel function. Well I have one Lipschitz function $f:\Bbb{R}^{n}\to \Bbb{R}$ and I want to proof that $D_{f}:D\to L(\Bbb{R}^{n},\Bbb{R})$ is Borel function, where $D=\{ x\in \Bbb{R}^{n}: f'(x) \text{ exists in the Frchet sense }\}$ I try with the definition to show that $\forall U$ Borel set in $ L(\Bbb{R}^{n},\Bbb{R})$ imply $D_{f}^{-1}(U)$ is Borel set. Then let $U$ Borel set in $L(\Bbb{R}^{n},\Bbb{R})$ hence $D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: D_{f}(x)\in U\}$ but $D_{f}(x)$ is one linear transformation (using or not using Frchet sense) so $D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: T(x)\in U \}$ can to say : Like $T$ is continuous because is linear transformation then $D_{f}^{-1}(U)$ is measurable imply is Borel set?, can somebody help me please or give me one hint...thank you","How to show that is a Borel function. Well I have one Lipschitz function and I want to proof that is Borel function, where I try with the definition to show that Borel set in imply is Borel set. Then let Borel set in hence but is one linear transformation (using or not using Frchet sense) so can to say : Like is continuous because is linear transformation then is measurable imply is Borel set?, can somebody help me please or give me one hint...thank you","D_{f} f:\Bbb{R}^{n}\to \Bbb{R} D_{f}:D\to L(\Bbb{R}^{n},\Bbb{R}) D=\{ x\in \Bbb{R}^{n}: f'(x) \text{ exists in the Frchet sense }\} \forall U  L(\Bbb{R}^{n},\Bbb{R}) D_{f}^{-1}(U) U L(\Bbb{R}^{n},\Bbb{R}) D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: D_{f}(x)\in U\} D_{f}(x) D_{f}^{-1}(U)=\{x\in \Bbb{R}^{n}: T(x)\in U \} T D_{f}^{-1}(U)","['multivariable-calculus', 'lebesgue-measure', 'borel-sets', 'frechet-derivative']"
28,How can I deduce a lower hemisphere's boundary's orientation?,How can I deduce a lower hemisphere's boundary's orientation?,,"Source: Stewart. Calculus: Early Transcendentals (6 edn 2007) . p. 1098. 16.8, Exercise #19. Despite reading this , I don't understand how to deduce the red sentence beneath. $\text{19}.$ If $S$ is a sphere and $\mathbf{F}$ satisfies the hypotheses of Stokes' Theorem, show that $\iint_S \text{curl} \, \mathbf{F} \cdot d\mathbf{S} = 0 $ . Author's Solution: Assume $S$ is centered at the origin with radius $a$ . Let $U$ and $L$ be the upper and lower hemispheres, respectively, of $S.$ Then by Stokes' Theorem: $\iint_S (\nabla \times \mathbf{F}) \cdot d\mathbf{S} = \iint_U (\nabla \times \mathbf{F}) \cdot d\mathbf{U} + \iint_L (\nabla \times \mathbf{F}) \cdot d\mathbf{L} = \oint_{\partial U} \! \mathbf{F}\cdot d\mathbf{r} + \oint_{\partial L} \! \mathbf{F}\cdot d\mathbf{r}  $ But ${\partial U}$ is $x^2 + y^2 = a^2$ oriented counterclockwise  and $\color{red}{{\partial L} \text{ is the same circle oriented clockwise}}$ . (Rest of solution omitted) Criterion A for Orientation of Boundary Curve (from Stewart p. 1093): If you walk in the positive direction around $\partial S$ and your head points in the direction of $\mathbf{\hat{n}}$ , then the surface will always be on your left. Criterion B (from Thomas' Calculus , 2009 12 edn, p 963): If the thumb of a right hand points along $\mathbf{\hat{n}}$ , then the fingers curl in the direction of $\partial S$ . I know that the orientation of $S$ determines the positive orientation of $\partial S$ . Also, the outward unit normals determine any closed surface's positive orientation . Here, the outward unit normals have $z$ -component $< 0$ which yields the person in red. $\color{purple}{\text{By Criterion A, the red person's head is pointing downwards (in the $-z$ direction)}}.$ $\color{purple}{\text{The purple direction is compulsory for the surface to be on that person's left.}}$ By Criterion B, my thumb must point in the $-z$ direction. So my fingers curl in the orange direction. Why's there a discrepancy between Criteria A and B? Where's the mistake? $\Large{\text{Supplementary to Prof Shifrin's Answer:}}$ Here's my interpretation: The red person must travel in the purple direction, for the surface to be on her left. Then the purple direction is counterclockwise when viewed from above the $xy$ -plane, BUT when viewed from below (as the red person is doing), the purple direction is CLOCKwise. I had to draw a clock in my updated picture to spot this. Without this clock, I still don't understand why the purple direction can be both clockwise and counterclockwise, depending on from where I look at ahe $xy$ -plane. Is there a more intuitive or natural explanation, without drawing clocks? $\Large{\text{2nd Supplementary to Prof Shifrin's Answer:}}$ I still don't understand how to spot that reflections across the $xy$ -plane reserve orientation (but preserve direction), without looking at a sheet of paper from above and below. $\Large{1.}$ Why did you choose the basis { $j, -i, k$ }, instead of the standard basis for $\mathbb{R}^3$ ? $\Large{2.}$ Doesn't $-i := (-1,0,0)$ imply that the tip of the vector, lying on the $x$ -axis, point towards $-\infty.$ How does it ""still point inward""?","Source: Stewart. Calculus: Early Transcendentals (6 edn 2007) . p. 1098. 16.8, Exercise #19. Despite reading this , I don't understand how to deduce the red sentence beneath. If is a sphere and satisfies the hypotheses of Stokes' Theorem, show that . Author's Solution: Assume is centered at the origin with radius . Let and be the upper and lower hemispheres, respectively, of Then by Stokes' Theorem: But is oriented counterclockwise  and . (Rest of solution omitted) Criterion A for Orientation of Boundary Curve (from Stewart p. 1093): If you walk in the positive direction around and your head points in the direction of , then the surface will always be on your left. Criterion B (from Thomas' Calculus , 2009 12 edn, p 963): If the thumb of a right hand points along , then the fingers curl in the direction of . I know that the orientation of determines the positive orientation of . Also, the outward unit normals determine any closed surface's positive orientation . Here, the outward unit normals have -component which yields the person in red. By Criterion B, my thumb must point in the direction. So my fingers curl in the orange direction. Why's there a discrepancy between Criteria A and B? Where's the mistake? Here's my interpretation: The red person must travel in the purple direction, for the surface to be on her left. Then the purple direction is counterclockwise when viewed from above the -plane, BUT when viewed from below (as the red person is doing), the purple direction is CLOCKwise. I had to draw a clock in my updated picture to spot this. Without this clock, I still don't understand why the purple direction can be both clockwise and counterclockwise, depending on from where I look at ahe -plane. Is there a more intuitive or natural explanation, without drawing clocks? I still don't understand how to spot that reflections across the -plane reserve orientation (but preserve direction), without looking at a sheet of paper from above and below. Why did you choose the basis { }, instead of the standard basis for ? Doesn't imply that the tip of the vector, lying on the -axis, point towards How does it ""still point inward""?","\text{19}. S \mathbf{F} \iint_S \text{curl} \, \mathbf{F} \cdot d\mathbf{S} = 0  S a U L S. \iint_S (\nabla \times \mathbf{F}) \cdot d\mathbf{S} = \iint_U (\nabla \times \mathbf{F}) \cdot d\mathbf{U} + \iint_L (\nabla \times \mathbf{F}) \cdot d\mathbf{L} = \oint_{\partial U} \! \mathbf{F}\cdot d\mathbf{r} + \oint_{\partial L} \! \mathbf{F}\cdot d\mathbf{r}   {\partial U} x^2 + y^2 = a^2 \color{red}{{\partial L} \text{ is the same circle oriented clockwise}} \partial S \mathbf{\hat{n}} \mathbf{\hat{n}} \partial S S \partial S z < 0 \color{purple}{\text{By Criterion A, the red person's head is pointing downwards (in the -z direction)}}. \color{purple}{\text{The purple direction is compulsory for the surface to be on that person's left.}} -z \Large{\text{Supplementary to Prof Shifrin's Answer:}} xy xy \Large{\text{2nd Supplementary to Prof Shifrin's Answer:}} xy \Large{1.} j, -i, k \mathbb{R}^3 \Large{2.} -i := (-1,0,0) x -\infty.",[]
29,Searching for a bound for the integral of a 1-form along a loop.,Searching for a bound for the integral of a 1-form along a loop.,,"Consider the submanifold $M$ of $\mathbb R^8$ , with coordinates $(x_1,y_1,x_2,y_2,x_3,y_3,x_4,y_4)$ , defined by the following equations $$x_1^2+y_1^2+x_2^2+y_2^2=1,$$ $$x_3^2+y_3^2+x_4^2+y_4^2=1,$$ $$x_1x_2+y_1y_2+x_3x_4+y_3y_4 = 0.$$ Consider a loop (closed curve) $\gamma$ and assume that $\gamma$ is contractible in $M$ . What I want to prove is the following: The integral $$\int_\gamma x_1dx_2+y_1dy_2+x_3dx_4+y_3dy_4$$ is bounded by a constant not dependent on $\gamma$ . I believe that such an integral is bounded by $\pi$ . I am not sure if such a constant exists. This integral appeared in a research problem where I was computing the holonomy of certain vector bundles, where it does seem that such a bound exists. I also believe there is some trick using symplectic geometry to find this bound. If $\Gamma$ is a disc with boundary $\gamma$ , then $$\int_\gamma x_1dx_2+y_1dy_2+x_3dx_4+y_3dy_4 =\int_\Gamma dx_1dx_2+dy_1dy_2+dx_3dx_4+dy_3dy_4,$$ by the Stokes theorem, the integral of a symplectic form. Any ideas are welcome. Thank you in advance.","Consider the submanifold of , with coordinates , defined by the following equations Consider a loop (closed curve) and assume that is contractible in . What I want to prove is the following: The integral is bounded by a constant not dependent on . I believe that such an integral is bounded by . I am not sure if such a constant exists. This integral appeared in a research problem where I was computing the holonomy of certain vector bundles, where it does seem that such a bound exists. I also believe there is some trick using symplectic geometry to find this bound. If is a disc with boundary , then by the Stokes theorem, the integral of a symplectic form. Any ideas are welcome. Thank you in advance.","M \mathbb R^8 (x_1,y_1,x_2,y_2,x_3,y_3,x_4,y_4) x_1^2+y_1^2+x_2^2+y_2^2=1, x_3^2+y_3^2+x_4^2+y_4^2=1, x_1x_2+y_1y_2+x_3x_4+y_3y_4 = 0. \gamma \gamma M \int_\gamma x_1dx_2+y_1dy_2+x_3dx_4+y_3dy_4 \gamma \pi \Gamma \gamma \int_\gamma x_1dx_2+y_1dy_2+x_3dx_4+y_3dy_4 =\int_\Gamma dx_1dx_2+dy_1dy_2+dx_3dx_4+dy_3dy_4,","['multivariable-calculus', 'differential-forms', 'symplectic-geometry', 'stokes-theorem', 'submanifold']"
30,Proving that $f:U\to\mathbb{R}^n$ differentiable is an open map when $\det \operatorname{J}f(a) \neq 0$,Proving that  differentiable is an open map when,f:U\to\mathbb{R}^n \det \operatorname{J}f(a) \neq 0,"Let $U\subset\mathbb{R}^n$ be an open set and $f:U\to\mathbb{R}^n$ a differentiable function such that $\det \operatorname{J}f(a)\neq 0\; \forall a\in U$. Prove that if $V$ is an open subset of $U$ then $f(V)$ is an open subset of $\mathbb{R}^n$ I'm not sure how could I do this, I tried to sketch a proof as follows: Let $V$ be an open set as the problem says, I have to prove that if $a\in V$ -therefore $a$ is an interior point- then $f(a)$ is an interior point of $f(V)$. $\det \operatorname{J}f(a)\neq 0$ implies that $f$ is smooth for every $a\in U$ and by the inverse function theorem exists $f^{-1}$ and $Y\subset\mathbb{R}^n $ in which $f^{-1}$ is continuous.  Suppose $f(V)\subset Y$ is not open, therefore for some $y\in f(V)$ we have $B_{\epsilon}(y) \not\subset f(V)$ for  every $\epsilon>0\;$  Since $f$ is invertible we can get  $a$ with $f^{-1}$ such that $f(a) = y$ and since $V$ is open $\exists\delta>0$ such that $B_{\delta}(a)\subset V$. The continuity of $f^{-1}$ implies that because of the openness of $B_{\delta}(a)$ the set $f(B_{\delta}(a))$ must be open in $f(V)$ which contradicts that $B_{\epsilon}(y)\not\subset f(V)$ for  every $\epsilon>0\;$. Is this ok?","Let $U\subset\mathbb{R}^n$ be an open set and $f:U\to\mathbb{R}^n$ a differentiable function such that $\det \operatorname{J}f(a)\neq 0\; \forall a\in U$. Prove that if $V$ is an open subset of $U$ then $f(V)$ is an open subset of $\mathbb{R}^n$ I'm not sure how could I do this, I tried to sketch a proof as follows: Let $V$ be an open set as the problem says, I have to prove that if $a\in V$ -therefore $a$ is an interior point- then $f(a)$ is an interior point of $f(V)$. $\det \operatorname{J}f(a)\neq 0$ implies that $f$ is smooth for every $a\in U$ and by the inverse function theorem exists $f^{-1}$ and $Y\subset\mathbb{R}^n $ in which $f^{-1}$ is continuous.  Suppose $f(V)\subset Y$ is not open, therefore for some $y\in f(V)$ we have $B_{\epsilon}(y) \not\subset f(V)$ for  every $\epsilon>0\;$  Since $f$ is invertible we can get  $a$ with $f^{-1}$ such that $f(a) = y$ and since $V$ is open $\exists\delta>0$ such that $B_{\delta}(a)\subset V$. The continuity of $f^{-1}$ implies that because of the openness of $B_{\delta}(a)$ the set $f(B_{\delta}(a))$ must be open in $f(V)$ which contradicts that $B_{\epsilon}(y)\not\subset f(V)$ for  every $\epsilon>0\;$. Is this ok?",,['multivariable-calculus']
31,How to tell if a limit of a multi-variable function exists?,How to tell if a limit of a multi-variable function exists?,,"Since I began studying limits of multi-variable functions, I have been baffled with this question: how can one tells if a limit exists or not? I don't know if it's the right way to solve this kind of problem but I've always identified if the limit exists first before trying to approve/disprove its existence. For example, from the exercises I have done, I can see that those functions having this form $\frac{{{x^a}{y^b}}}{{{x^{ma}} + n{y^{mb}}}}$ will never reach a limit as $(x,y) \to (0,0)$. This will save me from computing a non-existent limit in vain. Sorry if this question sounds dumb to you, I just don't want to screw up my upcoming test. Any tip or suggestion would be much appreciated, thanks!","Since I began studying limits of multi-variable functions, I have been baffled with this question: how can one tells if a limit exists or not? I don't know if it's the right way to solve this kind of problem but I've always identified if the limit exists first before trying to approve/disprove its existence. For example, from the exercises I have done, I can see that those functions having this form $\frac{{{x^a}{y^b}}}{{{x^{ma}} + n{y^{mb}}}}$ will never reach a limit as $(x,y) \to (0,0)$. This will save me from computing a non-existent limit in vain. Sorry if this question sounds dumb to you, I just don't want to screw up my upcoming test. Any tip or suggestion would be much appreciated, thanks!",,['multivariable-calculus']
32,How to interpret Hessian of a function,How to interpret Hessian of a function,,I know that gradient of a function gives the direction in which the directional derivative of the function is maximum. Is there any similar interpretation of Hessian ?,I know that gradient of a function gives the direction in which the directional derivative of the function is maximum. Is there any similar interpretation of Hessian ?,,"['multivariable-calculus', 'optimization']"
33,if : $abc=8 $ then : $(a+1)(b+1)(c+1)27$,if :  then :,abc=8  (a+1)(b+1)(c+1)27,"if : $$abc=8 :a,b,c\in \mathbb{R}_{> 0}$$ then : $$(a+1)(b+1)(c+1)\ge 27.$$ My try : $$(a+1)(b+1)(c+1)=1+(a+b+c)+(ab+ac+bc)+abc$$ $$(a+1)(b+1)(c+1)=1+(a+b+c)+(ab+ac+bc)+8, $$ then?","if : $$abc=8 :a,b,c\in \mathbb{R}_{> 0}$$ then : $$(a+1)(b+1)(c+1)\ge 27.$$ My try : $$(a+1)(b+1)(c+1)=1+(a+b+c)+(ab+ac+bc)+abc$$ $$(a+1)(b+1)(c+1)=1+(a+b+c)+(ab+ac+bc)+8, $$ then?",,"['multivariable-calculus', 'inequality', 'symmetric-polynomials', 'a.m.-g.m.-inequality', 'holder-inequality']"
34,Minimum value of $x+4z$ subject to the constraint $x^2+y^2+z^2\leq 2$?,Minimum value of  subject to the constraint ?,x+4z x^2+y^2+z^2\leq 2,"Consider the following problem: Let $f:{\mathbb R}^3 \to{\mathbb R}$ be $$f(x,y,z)=x+4z$$ where $$x^2+y^2+z^2\leq 2.$$ Find the minimum of $f$. This is similar to the question here . However, since this is not an analytic function with complex variable, one may not be able to use the ""Maximum modulus principle"". What I think is that one may rewrite the inequality constraint $x^2+y^2+z^2\leq 2$ as  $$x^2+y^2+z^2=2-\delta\qquad \delta\in[0,2]$$ then one can use the ""Lagrange Multiplier Method"" with the parameter $\delta$. Or one can do it on the xOz plane with the geometric meaning of $C$ in $C=x+4z$. Is there any other way better than the idea above to solve this problem?","Consider the following problem: Let $f:{\mathbb R}^3 \to{\mathbb R}$ be $$f(x,y,z)=x+4z$$ where $$x^2+y^2+z^2\leq 2.$$ Find the minimum of $f$. This is similar to the question here . However, since this is not an analytic function with complex variable, one may not be able to use the ""Maximum modulus principle"". What I think is that one may rewrite the inequality constraint $x^2+y^2+z^2\leq 2$ as  $$x^2+y^2+z^2=2-\delta\qquad \delta\in[0,2]$$ then one can use the ""Lagrange Multiplier Method"" with the parameter $\delta$. Or one can do it on the xOz plane with the geometric meaning of $C$ in $C=x+4z$. Is there any other way better than the idea above to solve this problem?",,[]
35,"How to show that $\frac{dy}{dx}=-\frac{F_x(x,y)}{F_y(x,y)}$",How to show that,"\frac{dy}{dx}=-\frac{F_x(x,y)}{F_y(x,y)}","Suppose that $y$ is defined implicitly as a function $y(x)$ by an equation on the form $F(x,y)=0$. I'm trying to show that $$\frac{dy}{dx}=-\frac{F_x(x,y)}{F_y(x,y)},$$ but I don't know where to start. Can someone please give me a hint? Both $y(x)$ and $F(x,y)$ are differentiable and $F_y(x,y)\neq 0$.","Suppose that $y$ is defined implicitly as a function $y(x)$ by an equation on the form $F(x,y)=0$. I'm trying to show that $$\frac{dy}{dx}=-\frac{F_x(x,y)}{F_y(x,y)},$$ but I don't know where to start. Can someone please give me a hint? Both $y(x)$ and $F(x,y)$ are differentiable and $F_y(x,y)\neq 0$.",,['multivariable-calculus']
36,"If $x,y,z>0.$Prove: $(x+y+z) \left(\frac{1}{x}+\frac{1}{y}+\frac{1}{z} \right) \geq9\sqrt[]\frac{x^2+y^2+z^2}{xy+yz+zx}$",If Prove:,"x,y,z>0. (x+y+z) \left(\frac{1}{x}+\frac{1}{y}+\frac{1}{z} \right) \geq9\sqrt[]\frac{x^2+y^2+z^2}{xy+yz+zx}","If $x,y,z>0.$ Prove: $$(x+y+z) \left(\frac{1}{x}+\frac{1}{y}+\frac{1}{z} \right)\geq9\sqrt[]\frac{x^2+y^2+z^2}{xy+yz+zx}$$ I was not able to solve this problem instead I could solve similar inequality when we have two variable.I assumed $y=tx$ and uesd derivative. Can this be generalized as: If ${a_i>0}\quad(i=1,2,...,n)$ $$\sum_{i=1}^n a_{i} \sum_{i=1}^n \frac{1}{a_{i}}\geq n^2\sqrt[]\frac{\sum_{i=1}^n a^2_{i} }{\sum_{i=1}^n a_{i}a_{i+1} }$$ $a_{n+1}=a_{1}$ Question from Jalil Hajimir",If Prove: I was not able to solve this problem instead I could solve similar inequality when we have two variable.I assumed and uesd derivative. Can this be generalized as: If Question from Jalil Hajimir,"x,y,z>0. (x+y+z) \left(\frac{1}{x}+\frac{1}{y}+\frac{1}{z} \right)\geq9\sqrt[]\frac{x^2+y^2+z^2}{xy+yz+zx} y=tx {a_i>0}\quad(i=1,2,...,n) \sum_{i=1}^n a_{i} \sum_{i=1}^n \frac{1}{a_{i}}\geq n^2\sqrt[]\frac{\sum_{i=1}^n a^2_{i} }{\sum_{i=1}^n a_{i}a_{i+1} } a_{n+1}=a_{1}","['multivariable-calculus', 'inequality', 'symmetric-polynomials', 'uvw', 'buffalo-way']"
37,"Difference between $\frac{df}{dx}$, $\frac{\Delta f}{\Delta x}$, and $\frac{\partial f}{\partial x}$","Difference between , , and",\frac{df}{dx} \frac{\Delta f}{\Delta x} \frac{\partial f}{\partial x},"[Beginning calculus question.] I've just been introduced to a number of ways of representing changes in a function value with respect to some variable in multivariable calculus. I don't get the difference between $\frac{df}{dx}$, $\frac{\Delta f}{\Delta x}$, and $\frac{\partial f}{\partial x}$. Are these all the same, or do they fall into different categories of objects, or something else? Does the meaning of these objects depend on the context in which they appear? Does the $\frac{\square}{\square}$ actually mean division in all three cases? As far as I can tell, they all represent the rate at which $f$ changes as $x$ changes. What am I missing?","[Beginning calculus question.] I've just been introduced to a number of ways of representing changes in a function value with respect to some variable in multivariable calculus. I don't get the difference between $\frac{df}{dx}$, $\frac{\Delta f}{\Delta x}$, and $\frac{\partial f}{\partial x}$. Are these all the same, or do they fall into different categories of objects, or something else? Does the meaning of these objects depend on the context in which they appear? Does the $\frac{\square}{\square}$ actually mean division in all three cases? As far as I can tell, they all represent the rate at which $f$ changes as $x$ changes. What am I missing?",,['multivariable-calculus']
38,Show that if $f : \mathbb{R}^{2} \to \mathbb{R}$ continuously differetiable then $f$ is not inyective,Show that if  continuously differetiable then  is not inyective,f : \mathbb{R}^{2} \to \mathbb{R} f,"Well my question this time is: How to show that $f : \mathbb{R}^{2} \to \mathbb{R}$ continuously differetiable then $f$ is not inyective I was trying to consider the function $g(x,y)=(f(x,y),y)$, but I don't know how to proceed with $g$, I know I have to use the implicit function theorem but when I was thinking of it, I remember that nearly $(x_0,y_0)$, $f$ must be zero, well $f(x_0,y_0)=0$, then the only tool I thought it could help I can't apply it. Can someone help me please with this problem?, and once you have it for this case, Can we generalize to $R^{m}$ and $R^{n}$?(of course $m<n$) Thanks a lot in advance My implicit function theorem : Let $A \subset R^{m} \times R^{n}$ open, $(x_0,y_0) \in A$, $f:A \to R^{m}$ continously differentiable nearly $(x_0,y_0)$ and  $f(x_0,y_0)=0$. Let M be the matrix of $m \times m$ given by: $$D_{n+j}f^{i}(x_0,y_0)$$ and suppose that $\det(M) \not= 0$, then there exists an open set $U \subset R^{n}$ $x_0 \in U$, and an open $V \subset R^{m}$, $y_0 \in V $, such that, for each $x$ there exists a unique $g(x) \in V$ such that $f(x,g(x))=0$, $g$ differentiable I don't know Topology, or more than introduction to analysis and a little bit of multivariable analysis, thanks a lot for your help","Well my question this time is: How to show that $f : \mathbb{R}^{2} \to \mathbb{R}$ continuously differetiable then $f$ is not inyective I was trying to consider the function $g(x,y)=(f(x,y),y)$, but I don't know how to proceed with $g$, I know I have to use the implicit function theorem but when I was thinking of it, I remember that nearly $(x_0,y_0)$, $f$ must be zero, well $f(x_0,y_0)=0$, then the only tool I thought it could help I can't apply it. Can someone help me please with this problem?, and once you have it for this case, Can we generalize to $R^{m}$ and $R^{n}$?(of course $m<n$) Thanks a lot in advance My implicit function theorem : Let $A \subset R^{m} \times R^{n}$ open, $(x_0,y_0) \in A$, $f:A \to R^{m}$ continously differentiable nearly $(x_0,y_0)$ and  $f(x_0,y_0)=0$. Let M be the matrix of $m \times m$ given by: $$D_{n+j}f^{i}(x_0,y_0)$$ and suppose that $\det(M) \not= 0$, then there exists an open set $U \subset R^{n}$ $x_0 \in U$, and an open $V \subset R^{m}$, $y_0 \in V $, such that, for each $x$ there exists a unique $g(x) \in V$ such that $f(x,g(x))=0$, $g$ differentiable I don't know Topology, or more than introduction to analysis and a little bit of multivariable analysis, thanks a lot for your help",,"['multivariable-calculus', 'proof-writing']"
39,Tangents at singularities,Tangents at singularities,,Given an implicit polynomial function $f$ with singularity at the origin. How do I find the tangents to the curve at the point? Wikipedia says that ignore all the terms except the ones with lowest degree. Why is this true? Take for example the curve $x^3+x^4+y^3=0$. What are the tangents at the origin?,Given an implicit polynomial function $f$ with singularity at the origin. How do I find the tangents to the curve at the point? Wikipedia says that ignore all the terms except the ones with lowest degree. Why is this true? Take for example the curve $x^3+x^4+y^3=0$. What are the tangents at the origin?,,"['algebraic-geometry', 'multivariable-calculus', 'differential-geometry']"
40,Squeeze theorem question for limits,Squeeze theorem question for limits,,"How can I find the following limit using the Squeeze theorem? $$\lim_{x,y\to 0} \frac{5xy^2}{x^2+y^2}$$ I am not sure what to use for the lower and upper bounds.","How can I find the following limit using the Squeeze theorem? $$\lim_{x,y\to 0} \frac{5xy^2}{x^2+y^2}$$ I am not sure what to use for the lower and upper bounds.",,['multivariable-calculus']
41,Derivative of position [duplicate],Derivative of position [duplicate],,"This question already has answers here : Are Position and Velocity (or Velocity and Acceleration) Vectors Always Parallel? (3 answers) Closed 8 years ago . [Beginning calculus question.] I saw in a calculus lecture online that for a position vector $\boldsymbol{r}$ $$\left|\frac{d\boldsymbol r}{dt}\right| \neq  \frac{d\left| \boldsymbol r \right|}{dt}$$ but I don't understand exactly how to parse this. It's my understanding that: $\frac{d\boldsymbol r}{dt}$ refers to the rate of change in the position over time (speed?) $|\boldsymbol r|$ refers to the magnitude of the position, i.e. the distance (from what to what?) $\frac{d\left| \boldsymbol r \right|}{dt}$ refers to the rate of change in distance traveled over time, (a different kind of speed?) Is there a good way to understand what both of these expressions mean?","This question already has answers here : Are Position and Velocity (or Velocity and Acceleration) Vectors Always Parallel? (3 answers) Closed 8 years ago . [Beginning calculus question.] I saw in a calculus lecture online that for a position vector $\boldsymbol{r}$ $$\left|\frac{d\boldsymbol r}{dt}\right| \neq  \frac{d\left| \boldsymbol r \right|}{dt}$$ but I don't understand exactly how to parse this. It's my understanding that: $\frac{d\boldsymbol r}{dt}$ refers to the rate of change in the position over time (speed?) $|\boldsymbol r|$ refers to the magnitude of the position, i.e. the distance (from what to what?) $\frac{d\left| \boldsymbol r \right|}{dt}$ refers to the rate of change in distance traveled over time, (a different kind of speed?) Is there a good way to understand what both of these expressions mean?",,"['multivariable-calculus', 'vectors']"
42,Lagrange multipliers tangency,Lagrange multipliers tangency,,I'm at the start of a section on Lagrange multipliers and the graphical/intuitive explanation is that one can increase/decrease $c$ until the level curves of some function in 2 vars is tangent to the constraint curve. But what happens when we are already on the min/max level curve and it isn't tangent to the constraint curve? [EDIT] Thx everyone for helping out. My mistake was that I didn't notice that $\nabla f=\vec0$ when we are on the max level curve. So whether there is tangency or not the equation: $$\nabla f=\lambda\nabla g$$ is still true.,I'm at the start of a section on Lagrange multipliers and the graphical/intuitive explanation is that one can increase/decrease $c$ until the level curves of some function in 2 vars is tangent to the constraint curve. But what happens when we are already on the min/max level curve and it isn't tangent to the constraint curve? [EDIT] Thx everyone for helping out. My mistake was that I didn't notice that $\nabla f=\vec0$ when we are on the max level curve. So whether there is tangency or not the equation: $$\nabla f=\lambda\nabla g$$ is still true.,,"['multivariable-calculus', 'lagrange-multiplier']"
43,What is the geometric reason of why is the divergence of the curl of a vector field equal to zero?,What is the geometric reason of why is the divergence of the curl of a vector field equal to zero?,,"What is the geometric reason of why is the divergence of the curl of a vector field equal to zero? I know how to prove it but I can't quite get some intuition behind it. I have seen some arguments that treat the del operator as a vector function, but I think this is not so correct as in some cases this analogy fails. This is described in http://www.feynmanlectures.caltech.edu/II_02.html in sections 2-7 and 2-8 but gives poor explanations on why thinking about the del operator as a normal vector works in some cases while does not work in other cases.","What is the geometric reason of why is the divergence of the curl of a vector field equal to zero? I know how to prove it but I can't quite get some intuition behind it. I have seen some arguments that treat the del operator as a vector function, but I think this is not so correct as in some cases this analogy fails. This is described in http://www.feynmanlectures.caltech.edu/II_02.html in sections 2-7 and 2-8 but gives poor explanations on why thinking about the del operator as a normal vector works in some cases while does not work in other cases.",,"['multivariable-calculus', 'differential-geometry', 'vectors', 'vector-analysis']"
44,why normalize and the definition of directional derivative,why normalize and the definition of directional derivative,,I'm not understanding how to solve this problem. I think the problem lies in the fact that I don't understand why normalize and the definition of directional derivative... It's in Portuguese. The question would be: Determine the directional derivative of f in P in direction of vector u:,I'm not understanding how to solve this problem. I think the problem lies in the fact that I don't understand why normalize and the definition of directional derivative... It's in Portuguese. The question would be: Determine the directional derivative of f in P in direction of vector u:,,['multivariable-calculus']
45,Uniformly continuous homeomorphism from open set to $\mathbb{R}^n$.,Uniformly continuous homeomorphism from open set to .,\mathbb{R}^n,"Let $U \subset \mathbb{R}^n$ be an open set. Suppose that the map $h:U \to \mathbb{R}^n$ is a homeomorphism from $U$ onto $\mathbb{R}^n$, which is uniformly continuous. Prove $U = \mathbb{R}^n$. My first attempt guided by intuition was to look at covering $\mathbb{R}^{n}$ by balls of radius $=\frac{1}{n}$ and conclude something forwarding to contradiction by looking at inverse-image of such covering, but I cannot see it for now.","Let $U \subset \mathbb{R}^n$ be an open set. Suppose that the map $h:U \to \mathbb{R}^n$ is a homeomorphism from $U$ onto $\mathbb{R}^n$, which is uniformly continuous. Prove $U = \mathbb{R}^n$. My first attempt guided by intuition was to look at covering $\mathbb{R}^{n}$ by balls of radius $=\frac{1}{n}$ and conclude something forwarding to contradiction by looking at inverse-image of such covering, but I cannot see it for now.",,"['multivariable-calculus', 'uniform-continuity']"
46,Evaluate $\int_A x^{-1}$,Evaluate,\int_A x^{-1},"We are asked to evaluate $\displaystyle \int_A x^{-1}dV(A)$, with $A=\{ (x,y):2<x+y<4,y>0,x-y>0\}$. From the solutions we know that$\displaystyle \int_A x^{-1}dV(A)=2Log(2)$. The point is to use the change of coordinates theorem to evaluate the integral. After proving that $\mathbf{g}(s,t)=(s^2+t^2,s^2-t^2)$ is regular (it's univalent only in each quadrant), I've tried computing $\displaystyle \int_B (s^2+t^2)^{-1}\cdot |det(D\mathbf{g}(s,t))|\text{ }dV(A)$, with $B=\{ (s,t):2<s^2<4,\text{ }s^2>t^2,\text{ and } |s|>|t| \}$. Well, when trying to compute $\displaystyle \int_B (s^2+t^2)^{-1}\cdot |det(D\mathbf{g}(s,t))|\text{ }dV(A)$, $\displaystyle \int_B (s^2+t^2)^{-1}\cdot |det(D\mathbf{g}(s,t))|\text{ }dV(A)= \int^2_\sqrt{2}\int^s_0\frac{8st}{s^2+t^2}+\int^2_\sqrt{2}\int^0_{-s}\frac{-8st}{s^2+t^2}+\int^{-\sqrt{2}}_{-2}\int^0_s\frac{8st}{s^2+t^2}+\int^{-\sqrt{2}}_{-2}\int^{-s}_0\frac{-8st}{s^2+t^2}=4Log(16)$ Where did I go wrong? Any help will be appreciated.","We are asked to evaluate $\displaystyle \int_A x^{-1}dV(A)$, with $A=\{ (x,y):2<x+y<4,y>0,x-y>0\}$. From the solutions we know that$\displaystyle \int_A x^{-1}dV(A)=2Log(2)$. The point is to use the change of coordinates theorem to evaluate the integral. After proving that $\mathbf{g}(s,t)=(s^2+t^2,s^2-t^2)$ is regular (it's univalent only in each quadrant), I've tried computing $\displaystyle \int_B (s^2+t^2)^{-1}\cdot |det(D\mathbf{g}(s,t))|\text{ }dV(A)$, with $B=\{ (s,t):2<s^2<4,\text{ }s^2>t^2,\text{ and } |s|>|t| \}$. Well, when trying to compute $\displaystyle \int_B (s^2+t^2)^{-1}\cdot |det(D\mathbf{g}(s,t))|\text{ }dV(A)$, $\displaystyle \int_B (s^2+t^2)^{-1}\cdot |det(D\mathbf{g}(s,t))|\text{ }dV(A)= \int^2_\sqrt{2}\int^s_0\frac{8st}{s^2+t^2}+\int^2_\sqrt{2}\int^0_{-s}\frac{-8st}{s^2+t^2}+\int^{-\sqrt{2}}_{-2}\int^0_s\frac{8st}{s^2+t^2}+\int^{-\sqrt{2}}_{-2}\int^{-s}_0\frac{-8st}{s^2+t^2}=4Log(16)$ Where did I go wrong? Any help will be appreciated.",,['multivariable-calculus']
47,"Gradient, tangents, planes, and steepest direction","Gradient, tangents, planes, and steepest direction",,"I know this is a topic covered on the internet frequently, but I still have further questions regarding visualization. I last took calculus some time ago, but am struggling with visualizations. Assuming $f(x,y)$ is some surface in 3D space. Now, let's say we take the gradient at point $(x_1, y_1)$. $\nabla f$ is then some two-dimensional vector $\langle \frac {\partial f}{\partial x},\frac{\partial f}{\partial y}\rangle$ and evaluated at $x_1$ and $y_1$. This two-dimensional vector symbolizes both how much change we have with respect to the x-axis and how much change we have with respect to the y-axis. I now have a couple points that I'm struggling (for whatever reason, to figure out) 1) Is it safe to say that the magnitude of the gradient vector at $(x_1, y_1)$ is the slope of the tangent plane to the surface at $f(x_1, y_1)$? 2) How is it that I read the gradient both is normal to the surface, but also points in the direction of maximum increase of $f$? I guess this is the problem I'm having, visualizing what the gradient vector looks like.","I know this is a topic covered on the internet frequently, but I still have further questions regarding visualization. I last took calculus some time ago, but am struggling with visualizations. Assuming $f(x,y)$ is some surface in 3D space. Now, let's say we take the gradient at point $(x_1, y_1)$. $\nabla f$ is then some two-dimensional vector $\langle \frac {\partial f}{\partial x},\frac{\partial f}{\partial y}\rangle$ and evaluated at $x_1$ and $y_1$. This two-dimensional vector symbolizes both how much change we have with respect to the x-axis and how much change we have with respect to the y-axis. I now have a couple points that I'm struggling (for whatever reason, to figure out) 1) Is it safe to say that the magnitude of the gradient vector at $(x_1, y_1)$ is the slope of the tangent plane to the surface at $f(x_1, y_1)$? 2) How is it that I read the gradient both is normal to the surface, but also points in the direction of maximum increase of $f$? I guess this is the problem I'm having, visualizing what the gradient vector looks like.",,['multivariable-calculus']
48,Divergence Proof,Divergence Proof,,"I'm not sure where to start on this proof...also, my book didn't give any clarification as to what $f$ and $\textbf{F}$ are. Usually $F$ is a scalar function, and $\textbf{f}$ is the vector field $f_1\textbf{i}+f_2\textbf{j}+f_3\textbf{k}$. But in this question the $\textbf{F}$ is a vector field because it's bold? Also, can you take the divergence of a scalar function? I thought it was just vector fields... Thanks in advance.","I'm not sure where to start on this proof...also, my book didn't give any clarification as to what $f$ and $\textbf{F}$ are. Usually $F$ is a scalar function, and $\textbf{f}$ is the vector field $f_1\textbf{i}+f_2\textbf{j}+f_3\textbf{k}$. But in this question the $\textbf{F}$ is a vector field because it's bold? Also, can you take the divergence of a scalar function? I thought it was just vector fields... Thanks in advance.",,['multivariable-calculus']
49,integral of $\cos (x+y+z)$ over a sphere,integral of  over a sphere,\cos (x+y+z),"Let $r>0$ and $S_r=\{(x,y,z)\in\mathbb{R}^3|x^2+y^2+z^2=r^2\}$. Show that $$\int_{S_r}\cos x\cos y\cos z\ dS=\int_{S_r}\cos (x+y+z)\ dS$$ and find the value of the integrals. I know that we can parametrize $S_r$ and try to solve the integrals by hand or try to expand the integrands using trigonometric identities, but I had no success in doing so. I am asking for a hint on how to do this. Moreover, is there some way to handle with integrals over spheres that involve ""cosines and sines"" (before parametrization). What I mean by this is that when the integrand does not involve ""cosines and sines"", parametrization often helps but in a situation like the above I don't know what to do.","Let $r>0$ and $S_r=\{(x,y,z)\in\mathbb{R}^3|x^2+y^2+z^2=r^2\}$. Show that $$\int_{S_r}\cos x\cos y\cos z\ dS=\int_{S_r}\cos (x+y+z)\ dS$$ and find the value of the integrals. I know that we can parametrize $S_r$ and try to solve the integrals by hand or try to expand the integrands using trigonometric identities, but I had no success in doing so. I am asking for a hint on how to do this. Moreover, is there some way to handle with integrals over spheres that involve ""cosines and sines"" (before parametrization). What I mean by this is that when the integrand does not involve ""cosines and sines"", parametrization often helps but in a situation like the above I don't know what to do.",,"['multivariable-calculus', 'multiple-integral']"
50,Can the following triple integral be computed via elementary calculus methods?,Can the following triple integral be computed via elementary calculus methods?,,"Consider the following triple integral: $$\int_0^{2\pi}\int_0^1 \int_0^1 xy\sqrt{x^2 + y^2 -2xy\cos(\theta)} \, dx \, dy \,  d\theta$$ A solution was provided to this integral by Jack D'Aurizio here , but both his solutions required rather sophisticated methods, like elliptic integrals and special function expansions. What I've been struggling with-and would like a second opinion on-is whether or not this integral can be solved in closed form using very simple calculus techniques,like a standard change of variables to plane polar in the xy domain or spherical coordinates in $R^3$. My labor over the last 2 days,multiple false starts and geometric arguments in the domain seem to indicate that the answer is no because there's no way to set up the integral without introducing a term of $\sqrt {\sin (ax)}$ or $\sqrt {\cos(ax)}$ at some point. Therefore, some special functional substitution or numerical method solution is needed. Or am I wrong?","Consider the following triple integral: $$\int_0^{2\pi}\int_0^1 \int_0^1 xy\sqrt{x^2 + y^2 -2xy\cos(\theta)} \, dx \, dy \,  d\theta$$ A solution was provided to this integral by Jack D'Aurizio here , but both his solutions required rather sophisticated methods, like elliptic integrals and special function expansions. What I've been struggling with-and would like a second opinion on-is whether or not this integral can be solved in closed form using very simple calculus techniques,like a standard change of variables to plane polar in the xy domain or spherical coordinates in $R^3$. My labor over the last 2 days,multiple false starts and geometric arguments in the domain seem to indicate that the answer is no because there's no way to set up the integral without introducing a term of $\sqrt {\sin (ax)}$ or $\sqrt {\cos(ax)}$ at some point. Therefore, some special functional substitution or numerical method solution is needed. Or am I wrong?",,"['multivariable-calculus', 'definite-integrals']"
51,Derivative of trace of inverse matrix?,Derivative of trace of inverse matrix?,,"I've been trying to derive the formula for the derivative of $Tr(X^{-1})$ w.r.t. $X$, which I know is $X^{-2T}$. According to the Matrix Cookbook $$\dfrac{\partial g(U)}{\partial X_{ij}} = \operatorname{Tr}\left[\left(\dfrac{\partial g(U)}{\partial U}\right)^T  \dfrac{\partial U}{\partial X_{ij}}\right]$$ So, if we let $U = X^{-1}$ and $g(X) = \operatorname{Tr}[X]$, we get $$\dfrac{\partial \,\operatorname{Tr}(X^{-1})}{\partial X_{ij}} = \operatorname{Tr}\left[\left(\dfrac{\partial \,\operatorname{Tr}(U)}{\partial U}\right)^T  \dfrac{\partial X^{-1}}{\partial X_{ij}}\right] = \operatorname{Tr}\left[ \dfrac{\partial X^{-1}}{\partial X_{ij}}\right]$$ since the derivative of the trace of a matrix w.r.t. that matrix is just the identity. However, I'm not sure how to proceed from here. I know that the derivative of the inverse of a matrix w.r.t. that matrix is $-X^{-2}$, but I don't know what the derivative of the inverse w.r.t. a specific entry would be, nor do I know what the trace of that derivative matrix would be. Is there something I'm missing here?","I've been trying to derive the formula for the derivative of $Tr(X^{-1})$ w.r.t. $X$, which I know is $X^{-2T}$. According to the Matrix Cookbook $$\dfrac{\partial g(U)}{\partial X_{ij}} = \operatorname{Tr}\left[\left(\dfrac{\partial g(U)}{\partial U}\right)^T  \dfrac{\partial U}{\partial X_{ij}}\right]$$ So, if we let $U = X^{-1}$ and $g(X) = \operatorname{Tr}[X]$, we get $$\dfrac{\partial \,\operatorname{Tr}(X^{-1})}{\partial X_{ij}} = \operatorname{Tr}\left[\left(\dfrac{\partial \,\operatorname{Tr}(U)}{\partial U}\right)^T  \dfrac{\partial X^{-1}}{\partial X_{ij}}\right] = \operatorname{Tr}\left[ \dfrac{\partial X^{-1}}{\partial X_{ij}}\right]$$ since the derivative of the trace of a matrix w.r.t. that matrix is just the identity. However, I'm not sure how to proceed from here. I know that the derivative of the inverse of a matrix w.r.t. that matrix is $-X^{-2}$, but I don't know what the derivative of the inverse w.r.t. a specific entry would be, nor do I know what the trace of that derivative matrix would be. Is there something I'm missing here?",,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'matrix-calculus', 'trace']"
52,Stokes and Gauss' Divergence theorem on a closed smooth surface in $\Bbb R^3$,Stokes and Gauss' Divergence theorem on a closed smooth surface in,\Bbb R^3,"I have a problem understanding Stokes' Theorem and Gauss' Divergence Theorem. Suppose the following: Let $F$ be a vector field in $\Bbb R^3$. Let $S$ be an oriented closed smooth Surface enclosing a volume $V$ and let $C$ be a positively-oriented closed curve surrounding $S$ Stokes' Theorem says: $$  \int_C Fdr=\iint_S (\nabla \times F)  dS $$ Then, by the Divergence Theorem: $$ \iint_S (\nabla \times F)dS = \iiint_V \nabla(\nabla \times F)  dV $$ But $ \nabla(\nabla \times F)=0$. So everything is $0$ What is it that I am not seeing?","I have a problem understanding Stokes' Theorem and Gauss' Divergence Theorem. Suppose the following: Let $F$ be a vector field in $\Bbb R^3$. Let $S$ be an oriented closed smooth Surface enclosing a volume $V$ and let $C$ be a positively-oriented closed curve surrounding $S$ Stokes' Theorem says: $$  \int_C Fdr=\iint_S (\nabla \times F)  dS $$ Then, by the Divergence Theorem: $$ \iint_S (\nabla \times F)dS = \iiint_V \nabla(\nabla \times F)  dV $$ But $ \nabla(\nabla \times F)=0$. So everything is $0$ What is it that I am not seeing?",,['multivariable-calculus']
53,What is $\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right )$?,What is ?,\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right ),"I'm trying to rewrite $\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right )$ in some other way. I tried using Levi-Civita symbol and Kronecker delta, but I'm stuck. Here is what I did: $$\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right ) = \left ( \vec{\nabla} \times \vec{A} \right )_i \left (\vec{\nabla} \times \vec{A} \right )_i = \epsilon_{ijk} \frac{\partial A_k}{\partial x_j} \epsilon_{imn} \frac{\partial A_n}{\partial x_m} = \left ( \delta_{jm} \delta_{kn} - \delta_{jn} \delta_{km} \right ) \frac{\partial A_k}{\partial x_j} \frac{\partial A_n}{\partial x_m}$$ $$= \left ( \frac{\partial A_k}{\partial x_m} \right )^{2} - \frac{\partial A_m}{\partial x_j} \frac{\partial A_j}{\partial x_m} = \left ( \frac{\partial A_k}{\partial x_m} \right )^{2} - \frac{\partial A_j}{\partial x_j} \frac{\partial A_m}{\partial x_m} $$ And I'm stuck with both these terms. (I'm sorry for no rigour switching order of partials, but I couldn't come up with anything else). Where I messed up?","I'm trying to rewrite in some other way. I tried using Levi-Civita symbol and Kronecker delta, but I'm stuck. Here is what I did: And I'm stuck with both these terms. (I'm sorry for no rigour switching order of partials, but I couldn't come up with anything else). Where I messed up?",\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right ) \left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right ) = \left ( \vec{\nabla} \times \vec{A} \right )_i \left (\vec{\nabla} \times \vec{A} \right )_i = \epsilon_{ijk} \frac{\partial A_k}{\partial x_j} \epsilon_{imn} \frac{\partial A_n}{\partial x_m} = \left ( \delta_{jm} \delta_{kn} - \delta_{jn} \delta_{km} \right ) \frac{\partial A_k}{\partial x_j} \frac{\partial A_n}{\partial x_m} = \left ( \frac{\partial A_k}{\partial x_m} \right )^{2} - \frac{\partial A_m}{\partial x_j} \frac{\partial A_j}{\partial x_m} = \left ( \frac{\partial A_k}{\partial x_m} \right )^{2} - \frac{\partial A_j}{\partial x_j} \frac{\partial A_m}{\partial x_m} ,"['multivariable-calculus', 'vector-analysis', 'tensors', 'vector-fields']"
54,It is really necessary to be always very rigorous in math?,It is really necessary to be always very rigorous in math?,,"I am an undergraduate student trying to read the text "" Calculus on Manifolds "" by Spivak. I have some doubts, in general, about how to write the exercises in this book (or another books) that involves lots of cumbersome notation. Before giving an example, I want to say that as an undergraduate, some teachers asks to students to write very clear proofs and to prove every assertion we made. I think that sometimes, making that very-clear-proofs are very very inefficient or hard. Now, the example that I would like to share is the following (at the end, I will add some comments that I really want to discuss; thanks in advance for taking your time reading this). I'll share with you the "" $(\Leftarrow)$ "" part of my proof. Suppose that for all subrectangles $S_i$ of $P,$ the function $f\vert_{S_i}$ is integrable over $S_i.$ If $S$ is a subrectangle of $P,$ then for all $\varepsilon>0$ we can find a partition $P_S$ of $S$ s.t. $U(f\vert_S,P_S)-L(f\vert_S,P_S) < \varepsilon/N$ (where $N$ is the number of subrectangles of $P$ ). Lets write $P_S=\{ P_{1,S}, \ldots, P_{n,S} \}.$ Let $Q=\{ \cup_S P_{1,S}, \ldots, \cup_S P_{n,S} \}.$ If we write $A=[a_1,b_1]\times \cdots \times [a_n,b_n],$ is not hard to verify that $\cup_S P_{i,S}$ is a partition of the interval $[a_i,b_i].$ Hence $Q$ is a partition of $A.$ Now, for all subrectangle $S_i$ of $P \quad (1 \leq i \leq N),$ there are subrectangles $S_{1,i}, \ldots, S_{\alpha,i}$ of $Q$ such that $S_i=S_{1,i}\cup \cdots \cup S_{\alpha,i},$ which determine a refinement $Q_{S_i}$ of the partition $P_{S_i}.$ Then we have $U(f\vert_{S_i}, Q_{S_i})-L(f\vert_{S_i},Q_{S_i})<\varepsilon/N.$ Therefore \begin{align*} U(f,Q)-L(f,Q) =& \sum_{R}[M_R(f)-m_R(f)]v(R) \\ =& \sum_{i}\sum_{j}  [M_{S_{j,i}}(f) - m_{S_{j,i}}(f)]v(S_{j,i}) \\ =& \sum_{i} \left[ U(f, Q_{S_i})-L(f,Q_{S_i}) \right] = \sum_{i} \left[ U(f\vert_{S_i}, Q_{S_i})-L(f\vert_{S_i},Q_{S_i}) \right] \\ <& \sum_{i} \varepsilon /N =\varepsilon. \end{align*} Then $f$ is integrable. Note that there are several parts of my proof that are not proper and rigorously proved. For example, in this part: "" is not hard to verify that $\cup_S P_{i,S}$ is a partition of the interval $[a_i,b_i].$ Hence $Q$ is a partition of $A.$ Now, for all subrectangle $S_i$ of $P \quad (1 \leq i \leq N),$ there are subrectangles $S_{1,i}, \ldots, S_{\alpha,i}$ of $Q$ such that $S_i=S_{1,i}\cup \cdots \cup S_{\alpha,i},$ which determine a refinement $Q_{S_i}$ of the partition $P_{S_i}$ "". I tried to write proofs of that assertions (that, I consider, are intuitive if you draw rectangles in the plane), but I spent tons of time and made very cumbersome notation (that make the proof very unclear and really hard to follow) in proving those statements. So, it would be insanely hard to prove every assertion that I made in a proof. As a self-learning student, what do you think that I should do in that kind of situations? It should be enough to me having intuitively clear those ideas that requires lots of notation machinery and time? or it should be better that I prove literally any assertion that I made? What should you recommend? If you have some extra comments, it will be great if you share with me your thoughts. Thanks a lot if you read all the text I wrote.","I am an undergraduate student trying to read the text "" Calculus on Manifolds "" by Spivak. I have some doubts, in general, about how to write the exercises in this book (or another books) that involves lots of cumbersome notation. Before giving an example, I want to say that as an undergraduate, some teachers asks to students to write very clear proofs and to prove every assertion we made. I think that sometimes, making that very-clear-proofs are very very inefficient or hard. Now, the example that I would like to share is the following (at the end, I will add some comments that I really want to discuss; thanks in advance for taking your time reading this). I'll share with you the "" "" part of my proof. Suppose that for all subrectangles of the function is integrable over If is a subrectangle of then for all we can find a partition of s.t. (where is the number of subrectangles of ). Lets write Let If we write is not hard to verify that is a partition of the interval Hence is a partition of Now, for all subrectangle of there are subrectangles of such that which determine a refinement of the partition Then we have Therefore Then is integrable. Note that there are several parts of my proof that are not proper and rigorously proved. For example, in this part: "" is not hard to verify that is a partition of the interval Hence is a partition of Now, for all subrectangle of there are subrectangles of such that which determine a refinement of the partition "". I tried to write proofs of that assertions (that, I consider, are intuitive if you draw rectangles in the plane), but I spent tons of time and made very cumbersome notation (that make the proof very unclear and really hard to follow) in proving those statements. So, it would be insanely hard to prove every assertion that I made in a proof. As a self-learning student, what do you think that I should do in that kind of situations? It should be enough to me having intuitively clear those ideas that requires lots of notation machinery and time? or it should be better that I prove literally any assertion that I made? What should you recommend? If you have some extra comments, it will be great if you share with me your thoughts. Thanks a lot if you read all the text I wrote.","(\Leftarrow) S_i P, f\vert_{S_i} S_i. S P, \varepsilon>0 P_S S U(f\vert_S,P_S)-L(f\vert_S,P_S) < \varepsilon/N N P P_S=\{ P_{1,S}, \ldots, P_{n,S} \}. Q=\{ \cup_S P_{1,S}, \ldots, \cup_S P_{n,S} \}. A=[a_1,b_1]\times \cdots \times [a_n,b_n], \cup_S P_{i,S} [a_i,b_i]. Q A. S_i P \quad (1 \leq i \leq N), S_{1,i}, \ldots, S_{\alpha,i} Q S_i=S_{1,i}\cup \cdots \cup S_{\alpha,i}, Q_{S_i} P_{S_i}. U(f\vert_{S_i}, Q_{S_i})-L(f\vert_{S_i},Q_{S_i})<\varepsilon/N. \begin{align*}
U(f,Q)-L(f,Q) =& \sum_{R}[M_R(f)-m_R(f)]v(R) \\
=& \sum_{i}\sum_{j}  [M_{S_{j,i}}(f) - m_{S_{j,i}}(f)]v(S_{j,i}) \\
=& \sum_{i} \left[ U(f, Q_{S_i})-L(f,Q_{S_i}) \right] = \sum_{i} \left[ U(f\vert_{S_i}, Q_{S_i})-L(f\vert_{S_i},Q_{S_i}) \right] \\
<& \sum_{i} \varepsilon /N =\varepsilon.
\end{align*} f \cup_S P_{i,S} [a_i,b_i]. Q A. S_i P \quad (1 \leq i \leq N), S_{1,i}, \ldots, S_{\alpha,i} Q S_i=S_{1,i}\cup \cdots \cup S_{\alpha,i}, Q_{S_i} P_{S_i}","['multivariable-calculus', 'vector-analysis', 'education']"
55,Recover scalar field from gradient,Recover scalar field from gradient,,"In one dimension, if I have a Riemann-integrable derivative $f'$ of a function $f$ which I don't know, I can (almost) recover $f$ from integrating $f'$. A simple example would be $f'(x)=2x$, then by the Fundamental Theorem of Calculus, I get that $f(x)=x^2 + const,$ where the constant does not depend on $x$. I said 'almost recover' because to determine the constant, we need a point on the graph of $f$. My question is now: Is it possible to recover a scalar field $f:\mathbb{R}^n \rightarrow \mathbb{R}$ when I only know the gradient $\left( \partial_1 f(x), \ldots, \partial_n f(x) \right)$. My thoughts: I could apply the fundamental theorem of calculus in the first component by integrating over $x_1$, but then, I would get \begin{align} \int \partial_1 f(x)d x_1=f(x)+C, \end{align} where $C$ constant only in $x_1$, but may well vary with $x_2,\ldots,x_n$. I write $C=C_{-1}$ to indicate this. I could now proceed and do the same calculation for all the partial derivatives, i.e. \begin{align} \int \partial_i f(x)d x_i=f(x)+C_{-i}. \end{align} I cannot just add them together and divide by $n$ to recover $f$ plus a constant (in all variables). In fact, adding any two distinct $C_{-i}$ and $C_{-j}$ together woud give me a function depending on all $x_i$'s. I thought about considering $f(x)+C_{-1}=f(x)+C_{-2}$ where $f$ would cancel. Because the left hand side does not depend on $x_1$ and the right hand side does not depend on $x_2$, neither side depends on either $x_1$ or $x_2$. It appears to me that this shows that the $C_{-i}$ all are constants in all $x_i$'s, but it cannot be correct, as the follwing example shows: Take $f(x_1,x_2)=x_1+x_2$. Then \begin{align} \left( \partial_1 f,\partial_2 f \right)=(1,1) \end{align} If now $C_{-1}$ would indeed be a constant, I would have  \begin{align} f(x)=\int \partial_1 f(x)dx_1=x_1+c \end{align} which is wrong. I've looked around and found the Gradient Theorem which appears to be the right statement, but I don't see how to use it to find $f$, probably because I don't know much about line integrals.","In one dimension, if I have a Riemann-integrable derivative $f'$ of a function $f$ which I don't know, I can (almost) recover $f$ from integrating $f'$. A simple example would be $f'(x)=2x$, then by the Fundamental Theorem of Calculus, I get that $f(x)=x^2 + const,$ where the constant does not depend on $x$. I said 'almost recover' because to determine the constant, we need a point on the graph of $f$. My question is now: Is it possible to recover a scalar field $f:\mathbb{R}^n \rightarrow \mathbb{R}$ when I only know the gradient $\left( \partial_1 f(x), \ldots, \partial_n f(x) \right)$. My thoughts: I could apply the fundamental theorem of calculus in the first component by integrating over $x_1$, but then, I would get \begin{align} \int \partial_1 f(x)d x_1=f(x)+C, \end{align} where $C$ constant only in $x_1$, but may well vary with $x_2,\ldots,x_n$. I write $C=C_{-1}$ to indicate this. I could now proceed and do the same calculation for all the partial derivatives, i.e. \begin{align} \int \partial_i f(x)d x_i=f(x)+C_{-i}. \end{align} I cannot just add them together and divide by $n$ to recover $f$ plus a constant (in all variables). In fact, adding any two distinct $C_{-i}$ and $C_{-j}$ together woud give me a function depending on all $x_i$'s. I thought about considering $f(x)+C_{-1}=f(x)+C_{-2}$ where $f$ would cancel. Because the left hand side does not depend on $x_1$ and the right hand side does not depend on $x_2$, neither side depends on either $x_1$ or $x_2$. It appears to me that this shows that the $C_{-i}$ all are constants in all $x_i$'s, but it cannot be correct, as the follwing example shows: Take $f(x_1,x_2)=x_1+x_2$. Then \begin{align} \left( \partial_1 f,\partial_2 f \right)=(1,1) \end{align} If now $C_{-1}$ would indeed be a constant, I would have  \begin{align} f(x)=\int \partial_1 f(x)dx_1=x_1+c \end{align} which is wrong. I've looked around and found the Gradient Theorem which appears to be the right statement, but I don't see how to use it to find $f$, probably because I don't know much about line integrals.",,"['multivariable-calculus', 'partial-derivative', 'vector-analysis', 'scalar-fields']"
56,Can we choose values for multivariable functions deliberately?,Can we choose values for multivariable functions deliberately?,,"I'm working through multivariable functions and derivatives of multivariable functions. Since I am not very familiar yet with multivariable functions I wondered about the following: In a function like $f(x,y)=x^2+y$, are x and y independent of each other and are we allowed to pick values for each deliberately? Say for 1 for x and 99 for y? As far as I have understood that topic it seems to me that indeed I can deliberately pick any value and will get an output in a third dimension. Rather than a curve I will receive a surface representing every possible combination of x and y. As long as the function is not limited like the equation for a circle, $x^2+y^2=r^2$. But I am not sure if I concluded this correctly. I guess it's a pretty easy or maybe even silly question but I haven't fully figured that out yet.","I'm working through multivariable functions and derivatives of multivariable functions. Since I am not very familiar yet with multivariable functions I wondered about the following: In a function like $f(x,y)=x^2+y$, are x and y independent of each other and are we allowed to pick values for each deliberately? Say for 1 for x and 99 for y? As far as I have understood that topic it seems to me that indeed I can deliberately pick any value and will get an output in a third dimension. Rather than a curve I will receive a surface representing every possible combination of x and y. As long as the function is not limited like the equation for a circle, $x^2+y^2=r^2$. But I am not sure if I concluded this correctly. I guess it's a pretty easy or maybe even silly question but I haven't fully figured that out yet.",,"['functions', 'multivariable-calculus']"
57,Explain the minus sign in the following formula.,Explain the minus sign in the following formula.,,"I just read that: If $z=f(x,y)=c$, be the equation of a curve, then the slope of the tangent to the curve at any point (x,y), is given by $$m=\frac {dy}{dx}=-\frac{\frac{\partial z}{\partial x}}{\frac {\partial z}{\partial y}}$$ I don't see how the minus sign creeps in here.(Of course I don't have a proof, but the - sign is against intuition). A proof(or a link to a simple proof) would be nice, and an intuitive explanation would be nicer. Thanks for help.","I just read that: If $z=f(x,y)=c$, be the equation of a curve, then the slope of the tangent to the curve at any point (x,y), is given by $$m=\frac {dy}{dx}=-\frac{\frac{\partial z}{\partial x}}{\frac {\partial z}{\partial y}}$$ I don't see how the minus sign creeps in here.(Of course I don't have a proof, but the - sign is against intuition). A proof(or a link to a simple proof) would be nice, and an intuitive explanation would be nicer. Thanks for help.",,"['multivariable-calculus', 'derivatives', 'implicit-differentiation', 'implicit-function-theorem']"
58,"Spivak, Calculus on Manifolds, Problem 2-37 (b)","Spivak, Calculus on Manifolds, Problem 2-37 (b)",,"2-37 (a) Let $f:\mathbb{R^2}\to\mathbb{R}$ be a continuously differentiable function. Show that $f$ is $not\mbox{ 1-1}$ . $Hint:$ If, for example, $D_1f(x,y)\neq0$ for all $(x,y)$ in some open set $A$ , then consider $g:A\to\mathbb{R^2}$ defined by $g(x,y) = (f(x,y),y)$ . (b) Generalize this result to the case of a continuously differentiable function $f:\mathbb{R^n}\to\mathbb{R^m}$ with $m<n$ . I want to solve (b) analogously to (a), so here I first present my solution to (a). If $D_1f(x,y) = 0$ for any $(x,y)\in\mathbb{R^2}$ , then $f(x_1, y) = f(x_2,y)$ for any $x_1, x_2\in\mathbb{R}$ ; hence, $f$ is not $\mbox{1-1}$ . Otherwise, there is $(x,y)$ such that $D_1f(x,y)\neq 0$ . Then $g:\mathbb{R^2}\to\mathbb{R^2}$ defined as in the hint is continuously differentiable and $$\det g'(x,y) = \det\left(\begin{smallmatrix}D_1f(x,y)&D_2f(x,y)\\0&1\end{smallmatrix}\right) = D_1f(x,y)\neq0,$$ so it satisfies the conditions for the inverse function theorem. Thus, there are open $V\ni (x,y)$ and $W\ni g(x,y)$ such that $g:V\to W$ is $\mbox{1-1}$ .  For some unequal $(u,y_1),(u,y_2)\in W$ we get unequal $(x_1,y_1),(x_2,y_2)\in V$ such that $f(x_1,y_1) = f(x_2,y_2) = u$ ; hence, $f$ is not $\mbox{1-1}$ . I have three ideas on how to interpret the $D_1f$ from (a) in (b): as $D_1f^1$ , as $D_1f$ , as $\det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right)$ ; but neither of them seem to work. By assuming $D_1f^1(x)\neq 0$ for some $x$ and defining $g:\mathbb{R^n\to\mathbb{R^n}}$ by $g(x^1,...,x^n) = (f^1(x), x^2,...,x^n)$ I can prove that $f^1$ is not $\mbox{1-1}$ . Similarily, any $f^i$ is not $\mbox{1-1}$ , but it doesn't show that $f$ is not $\mbox{1-1}$ . By  assuming $D_1f(x)\neq 0$ for some $x$ and defining $g:\mathbb{R^n\to\mathbb{R^n}}$ by $g(x^1,...,x^n) = (f(x), x^{m+1},...,x^n)$ I get $$\det g'(x) = \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_nf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_nf^m(x)\\O_{m\times m}&&E_{(n-m)\times (n-m)}  \end{smallmatrix}\right)  = \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right);$$ but I cannot show that it's not equal to $0$ at some point $x$ to use the theorem. I want to define $g$ as in 2. but I cannot show why $f$ is not $\mbox{1-1}$ , when $$\det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right) = 0$$ for all $x\in\mathbb{R^n}$ . The third one seems the most promising to me but I don't how to proceed. Do you have any tips?","2-37 (a) Let be a continuously differentiable function. Show that is . If, for example, for all in some open set , then consider defined by . (b) Generalize this result to the case of a continuously differentiable function with . I want to solve (b) analogously to (a), so here I first present my solution to (a). If for any , then for any ; hence, is not . Otherwise, there is such that . Then defined as in the hint is continuously differentiable and so it satisfies the conditions for the inverse function theorem. Thus, there are open and such that is .  For some unequal we get unequal such that ; hence, is not . I have three ideas on how to interpret the from (a) in (b): as , as , as ; but neither of them seem to work. By assuming for some and defining by I can prove that is not . Similarily, any is not , but it doesn't show that is not . By  assuming for some and defining by I get but I cannot show that it's not equal to at some point to use the theorem. I want to define as in 2. but I cannot show why is not , when for all . The third one seems the most promising to me but I don't how to proceed. Do you have any tips?","f:\mathbb{R^2}\to\mathbb{R} f not\mbox{ 1-1} Hint: D_1f(x,y)\neq0 (x,y) A g:A\to\mathbb{R^2} g(x,y) = (f(x,y),y) f:\mathbb{R^n}\to\mathbb{R^m} m<n D_1f(x,y) = 0 (x,y)\in\mathbb{R^2} f(x_1, y) = f(x_2,y) x_1, x_2\in\mathbb{R} f \mbox{1-1} (x,y) D_1f(x,y)\neq 0 g:\mathbb{R^2}\to\mathbb{R^2} \det g'(x,y) = \det\left(\begin{smallmatrix}D_1f(x,y)&D_2f(x,y)\\0&1\end{smallmatrix}\right) = D_1f(x,y)\neq0, V\ni (x,y) W\ni g(x,y) g:V\to W \mbox{1-1} (u,y_1),(u,y_2)\in W (x_1,y_1),(x_2,y_2)\in V f(x_1,y_1) = f(x_2,y_2) = u f \mbox{1-1} D_1f D_1f^1 D_1f \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right) D_1f^1(x)\neq 0 x g:\mathbb{R^n\to\mathbb{R^n}} g(x^1,...,x^n) = (f^1(x), x^2,...,x^n) f^1 \mbox{1-1} f^i \mbox{1-1} f \mbox{1-1} D_1f(x)\neq 0 x g:\mathbb{R^n\to\mathbb{R^n}} g(x^1,...,x^n) = (f(x), x^{m+1},...,x^n) \det g'(x) = \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_nf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_nf^m(x)\\O_{m\times m}&&E_{(n-m)\times (n-m)}  \end{smallmatrix}\right)  = \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right); 0 x g f \mbox{1-1} \det\left(\begin{smallmatrix}D_1f^1(x)&\dots&D_mf^1(x)\\\vdots&\ddots&\vdots \\D_1f^m(x)&\dots&D_mf^m(x)  \end{smallmatrix}\right) = 0 x\in\mathbb{R^n}","['multivariable-calculus', 'inverse-function-theorem']"
59,Differentiability of a scalar function of two variables --- problems with Stewarts definition.,Differentiability of a scalar function of two variables --- problems with Stewarts definition.,,"Im feeling uncomfortable about how Stewarts Calculus makes the transition from differentiability of functions of one variable to differentiability of functions of two variables. Namely, Stewart gives the following definition: Definition. A function $ f $ of two variables is said to be differentiable at $ (a,b) $ if the first-order partial derivatives of $ f $ at $ (a,b) $ exist and   \begin{align}   & ~ f(a + \Delta x,b + \Delta y) - f(a,b) \\ = & ~ {f_{\operatorname{x}}}(a,b) \Delta x +       {f_{\operatorname{y}}}(a,b) \Delta y +       {\epsilon_{1}}(\Delta x,\Delta y) \Delta x +       {\epsilon_{2}}(\Delta x,\Delta y) \Delta y, \end{align}   where $ \displaystyle \lim_{(\Delta x,\Delta y) \to (0,0)} {\epsilon_{1}}(\Delta x,\Delta y) = \lim_{(\Delta x,\Delta y) \to (0,0)} {\epsilon_{2}}(\Delta x,\Delta y) = 0 $. Note. Later on, he shows that this will follow if the partial derivatives exist and are continuous near $ (a,b) $, and I am fine with this. What first irked me is that I didnt remember learning this definition, so I spent a while trying to understand it. Moreover, the only functions that Ive found to motivate the need for such a definition are those that are not continuous at the point in question. For example, consider the function $ f: \mathbb{R}^{2} \to \mathbb{R} $ defined by $$ \forall (x,y) \in \mathbb{R}^{2}: \qquad f(x,y) \stackrel{\text{df}}{=} \begin{cases} \dfrac{x y}{x^{2} + y^{2}} & \text{if $ (x,y) \neq (0,0) $}; \\ 0                          & \text{if $ (x,y) = (0,0) $}. \end{cases} $$ Then $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ are defined at $ (0,0) $, but $ f $ is not continuous there. Heres my quarrel. I dont think that this is a natural extension from the one-dimensional case. In one dimension, differentiability at a point implies continuity at that point, but two dimensions, the existence of $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ at a point dont imply the continuity of $ f $ at that point. My intuition, then, is that to extend the definition of differentiability to two dimensions, we should say that $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ exist and that $ f $ is continuous at the relevant point. This seems nice and simple, so I cant imagine why Stewart would go with the awkward definition given at the beginning of this post, if this was indeed the case. It therefore seems that I should be wrong. If I am, can you give me an example of a function $ f $ that is continuous at $ (a,b) $, and $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ are defined at $ (a,b) $, but it isnt differentiable there? Additionally, I have a rough outline of a proof in my head of why it should be true. The definition of a directional derivative gives $$   {D_{\mathbf{u}} f}(a,b) = \lim_{h \to 0} \frac{f(a + h \Delta x,b + h \Delta y) - f(a,b)}{h}, $$ but expressing it as a gradient implies that $$   \lim_{h \to 0} \frac{f(a + h \Delta x,b + h \Delta y) - f(a,b)}{h} = {f_{\operatorname{x}}}(a,b) \Delta x + {f_{\operatorname{y}}}(a,b) \Delta y. $$ However, for this limit to be true, it seems like the original awkward definition must follow, i.e., $$   \frac{f(a + h \Delta x,b + h \Delta y) - f(a,b)}{h} = {f_{\operatorname{x}}}(a,b)\Delta x + {f_{\operatorname{y}}}(a,b) \Delta y, $$ which implies that $$   f(a + h \Delta x,b + h \Delta y) - f(a,b) = {f_{\operatorname{x}}}(a,b)h \Delta x + {f_{\operatorname{y}}}(a,b)h \Delta y $$ without even the need for the functions $ \epsilon_{1} $ and $ \epsilon_{2} $. Edit 1 zhw. has given a nice counter example below. However, Id now like to alter my questions. Would an alternative definition be that all the directional derivatives of $ f $ exist and that $ f $ is continuous at a given point? What if I now extend my original approach to say that $ f $ is continuous on a neighborhood of $ (a,b) $ and that $ {f_{\operatorname{x}}}(a,b) $ and $ {f_{\operatorname{y}}}(a,b)$ exist? Could you give me a counterexample now? Edit 2 zhw. has knocked down Question 2 of Edit 1 in his response to my comment to his original answer, so now Im just wondering if Would an alternative definition be that all the directional derivatives of $ f $ exist and that $ f $ is continuous at a given point? Im motivated to ask because (a) zhw.s counter examples all have some directional derivatives that dont work and (b) my attempted proof relied on the existence of directional derivatives.","Im feeling uncomfortable about how Stewarts Calculus makes the transition from differentiability of functions of one variable to differentiability of functions of two variables. Namely, Stewart gives the following definition: Definition. A function $ f $ of two variables is said to be differentiable at $ (a,b) $ if the first-order partial derivatives of $ f $ at $ (a,b) $ exist and   \begin{align}   & ~ f(a + \Delta x,b + \Delta y) - f(a,b) \\ = & ~ {f_{\operatorname{x}}}(a,b) \Delta x +       {f_{\operatorname{y}}}(a,b) \Delta y +       {\epsilon_{1}}(\Delta x,\Delta y) \Delta x +       {\epsilon_{2}}(\Delta x,\Delta y) \Delta y, \end{align}   where $ \displaystyle \lim_{(\Delta x,\Delta y) \to (0,0)} {\epsilon_{1}}(\Delta x,\Delta y) = \lim_{(\Delta x,\Delta y) \to (0,0)} {\epsilon_{2}}(\Delta x,\Delta y) = 0 $. Note. Later on, he shows that this will follow if the partial derivatives exist and are continuous near $ (a,b) $, and I am fine with this. What first irked me is that I didnt remember learning this definition, so I spent a while trying to understand it. Moreover, the only functions that Ive found to motivate the need for such a definition are those that are not continuous at the point in question. For example, consider the function $ f: \mathbb{R}^{2} \to \mathbb{R} $ defined by $$ \forall (x,y) \in \mathbb{R}^{2}: \qquad f(x,y) \stackrel{\text{df}}{=} \begin{cases} \dfrac{x y}{x^{2} + y^{2}} & \text{if $ (x,y) \neq (0,0) $}; \\ 0                          & \text{if $ (x,y) = (0,0) $}. \end{cases} $$ Then $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ are defined at $ (0,0) $, but $ f $ is not continuous there. Heres my quarrel. I dont think that this is a natural extension from the one-dimensional case. In one dimension, differentiability at a point implies continuity at that point, but two dimensions, the existence of $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ at a point dont imply the continuity of $ f $ at that point. My intuition, then, is that to extend the definition of differentiability to two dimensions, we should say that $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ exist and that $ f $ is continuous at the relevant point. This seems nice and simple, so I cant imagine why Stewart would go with the awkward definition given at the beginning of this post, if this was indeed the case. It therefore seems that I should be wrong. If I am, can you give me an example of a function $ f $ that is continuous at $ (a,b) $, and $ f_{\operatorname{x}} $ and $ f_{\operatorname{y}} $ are defined at $ (a,b) $, but it isnt differentiable there? Additionally, I have a rough outline of a proof in my head of why it should be true. The definition of a directional derivative gives $$   {D_{\mathbf{u}} f}(a,b) = \lim_{h \to 0} \frac{f(a + h \Delta x,b + h \Delta y) - f(a,b)}{h}, $$ but expressing it as a gradient implies that $$   \lim_{h \to 0} \frac{f(a + h \Delta x,b + h \Delta y) - f(a,b)}{h} = {f_{\operatorname{x}}}(a,b) \Delta x + {f_{\operatorname{y}}}(a,b) \Delta y. $$ However, for this limit to be true, it seems like the original awkward definition must follow, i.e., $$   \frac{f(a + h \Delta x,b + h \Delta y) - f(a,b)}{h} = {f_{\operatorname{x}}}(a,b)\Delta x + {f_{\operatorname{y}}}(a,b) \Delta y, $$ which implies that $$   f(a + h \Delta x,b + h \Delta y) - f(a,b) = {f_{\operatorname{x}}}(a,b)h \Delta x + {f_{\operatorname{y}}}(a,b)h \Delta y $$ without even the need for the functions $ \epsilon_{1} $ and $ \epsilon_{2} $. Edit 1 zhw. has given a nice counter example below. However, Id now like to alter my questions. Would an alternative definition be that all the directional derivatives of $ f $ exist and that $ f $ is continuous at a given point? What if I now extend my original approach to say that $ f $ is continuous on a neighborhood of $ (a,b) $ and that $ {f_{\operatorname{x}}}(a,b) $ and $ {f_{\operatorname{y}}}(a,b)$ exist? Could you give me a counterexample now? Edit 2 zhw. has knocked down Question 2 of Edit 1 in his response to my comment to his original answer, so now Im just wondering if Would an alternative definition be that all the directional derivatives of $ f $ exist and that $ f $ is continuous at a given point? Im motivated to ask because (a) zhw.s counter examples all have some directional derivatives that dont work and (b) my attempted proof relied on the existence of directional derivatives.",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
60,Definition of a Manifold from Guillemin Pollack,Definition of a Manifold from Guillemin Pollack,,I have been studying differential topology from Guillemin and Pollack (GP). Unlike many other books that define differentiable manifolds using maximal atlases GP starts by saying $ X \subset R^{N}$ for some ambient space $R^{N}$ and then goes on to define a $k$ dimensional manifold. But I know that this containment comes due to a weak version of Whitney's theorem. Later on when they prove Whitney's theorem it is done so by induction on $N >= 2k+1$. But how to I justify that $ X \subset R^{N}$ in the first place? How can it just be assumed in definition like that? I need help getting from the general definition of manifolds using atlases to the weak version of Whitney. Thanks,I have been studying differential topology from Guillemin and Pollack (GP). Unlike many other books that define differentiable manifolds using maximal atlases GP starts by saying $ X \subset R^{N}$ for some ambient space $R^{N}$ and then goes on to define a $k$ dimensional manifold. But I know that this containment comes due to a weak version of Whitney's theorem. Later on when they prove Whitney's theorem it is done so by induction on $N >= 2k+1$. But how to I justify that $ X \subset R^{N}$ in the first place? How can it just be assumed in definition like that? I need help getting from the general definition of manifolds using atlases to the weak version of Whitney. Thanks,,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
61,Prove an analog of Rolle's theorem for several variables,Prove an analog of Rolle's theorem for several variables,,"On p. 135 of Buck's Advanced Calculus , he asks the reader to prove an analog of Rolle's theorem for functions of two variables (I suspect the number two is arbitrary). The hint is to assume that $f=0$ on the boundary of a bounded open set, and I belive what he's looking for is that if $f\in $  $C^1(U)$ for $U$ bounded in $\mathbb{R}^{n}$, and such that $f=0$ on $\partial U$, then $\exists x_0 \in U$ such that $\mathbf{D}f(x_0)= \mathbf{0}$. This occurs in the section after the introduction of the derivative on $\mathbb{R}^n$, which suggests that methods like Taylor's theorem, etc. are not required. My intuition is to draw lines parallel to each axis and note that by Rolle's theorem in one dimension, there must be a point on each line where $\frac{\partial f}{\partial x_i}=0$, and then somehow argue that there must be a point where all are zero. It seems to me that the points where say $\frac{\partial f}{\partial x_1}=0$ must be something like an $(n-1)$-manifold sitting in $U$, since each line parallel to $e_i$ through $U$ must intersect this set in some point. And same for those points where $\frac{\partial f}{\partial x_2}=0$, but I don't necessarily see why these two hypersurfaces would intersect. I think I am overcomplicating it. Can anyone think of a simpler argument?","On p. 135 of Buck's Advanced Calculus , he asks the reader to prove an analog of Rolle's theorem for functions of two variables (I suspect the number two is arbitrary). The hint is to assume that $f=0$ on the boundary of a bounded open set, and I belive what he's looking for is that if $f\in $  $C^1(U)$ for $U$ bounded in $\mathbb{R}^{n}$, and such that $f=0$ on $\partial U$, then $\exists x_0 \in U$ such that $\mathbf{D}f(x_0)= \mathbf{0}$. This occurs in the section after the introduction of the derivative on $\mathbb{R}^n$, which suggests that methods like Taylor's theorem, etc. are not required. My intuition is to draw lines parallel to each axis and note that by Rolle's theorem in one dimension, there must be a point on each line where $\frac{\partial f}{\partial x_i}=0$, and then somehow argue that there must be a point where all are zero. It seems to me that the points where say $\frac{\partial f}{\partial x_1}=0$ must be something like an $(n-1)$-manifold sitting in $U$, since each line parallel to $e_i$ through $U$ must intersect this set in some point. And same for those points where $\frac{\partial f}{\partial x_2}=0$, but I don't necessarily see why these two hypersurfaces would intersect. I think I am overcomplicating it. Can anyone think of a simpler argument?",,"['multivariable-calculus', 'derivatives']"
62,How to examine if multivariable functions are differentiable?,How to examine if multivariable functions are differentiable?,,"How to examine if functions: $f(x,y)=|x+y|$ and $g(x,y)=\sqrt{|xy|}$ are diffirentiable in points: $(0,0)$ for $f(x,y)$ and $(0,1)$ for $g(x,y)$","How to examine if functions: $f(x,y)=|x+y|$ and $g(x,y)=\sqrt{|xy|}$ are diffirentiable in points: $(0,0)$ for $f(x,y)$ and $(0,1)$ for $g(x,y)$",,['multivariable-calculus']
63,Proving that $\iint_S (\nabla \times F) \cdot \hat{n} dS =0$,Proving that,\iint_S (\nabla \times F) \cdot \hat{n} dS =0,"I have the following question: Prove that $$\iint_S (\nabla \times \vec{F}) \cdot \hat{n} dS =0$$  for any closed surface $S$ and twice differentiable vector field $\vec F:\mathbb{R^3} \to \mathbb{R^3} $ . I need to prove this using Stokes' theorem. The only thing I want to verify is whether or not for every closed surface $S$, we have:  $$\iint_S (\nabla \times \vec{F}) \cdot \hat{n} dS =\int_C \vec F \cdot d\vec r$$ and the last term is trivially zero, because $C=\emptyset $ ($S$ is a closed surface). Is this correct? Thanks in advance","I have the following question: Prove that $$\iint_S (\nabla \times \vec{F}) \cdot \hat{n} dS =0$$  for any closed surface $S$ and twice differentiable vector field $\vec F:\mathbb{R^3} \to \mathbb{R^3} $ . I need to prove this using Stokes' theorem. The only thing I want to verify is whether or not for every closed surface $S$, we have:  $$\iint_S (\nabla \times \vec{F}) \cdot \hat{n} dS =\int_C \vec F \cdot d\vec r$$ and the last term is trivially zero, because $C=\emptyset $ ($S$ is a closed surface). Is this correct? Thanks in advance",,['multivariable-calculus']
64,"""And"" symbol? Wedge product in a surface integral?  Is this a typo, or did I miss an important lecture?","""And"" symbol? Wedge product in a surface integral?  Is this a typo, or did I miss an important lecture?",,"This is the question I got on my final assignment (Calculus III): Evaluate the surface integral \begin{equation}   \int \int_S xy \; \; dy\wedge dz - yz \; \; dz\wedge dx + xz \; \; dx\wedge dy    \end{equation} Where $S$ is the part of the plane $x+y+z=1$ lying in the first octant. Use $x$ and $y$ as parameters. I am quite confused. I asked around and someone told me that this is also the symbol for something called the Wedge product, which I've not heard of before and appears in neither my calculus textbooks (Stweart's and Div Grad Curl ) nor any of my Linear Algebra books. From what I saw online, I still don't understand how it would make sense in this equation. Is this a typo? Seems like a strange typo. Should it just read: \begin{equation} \int \int_S xy \; \; dydz - yz \; \; dzdx + xz \; \; dxdy  \end{equation}","This is the question I got on my final assignment (Calculus III): Evaluate the surface integral \begin{equation}   \int \int_S xy \; \; dy\wedge dz - yz \; \; dz\wedge dx + xz \; \; dx\wedge dy    \end{equation} Where $S$ is the part of the plane $x+y+z=1$ lying in the first octant. Use $x$ and $y$ as parameters. I am quite confused. I asked around and someone told me that this is also the symbol for something called the Wedge product, which I've not heard of before and appears in neither my calculus textbooks (Stweart's and Div Grad Curl ) nor any of my Linear Algebra books. From what I saw online, I still don't understand how it would make sense in this equation. Is this a typo? Seems like a strange typo. Should it just read: \begin{equation} \int \int_S xy \; \; dydz - yz \; \; dzdx + xz \; \; dxdy  \end{equation}",,"['multivariable-calculus', 'exterior-algebra']"
65,How often should I compound my Algorand (or other proof of stake crypto assets)?,How often should I compound my Algorand (or other proof of stake crypto assets)?,,"Note to editors/mods: Please help me tag this appropriately! I'm not really a math wiz... is it multivariable calculus? See this note from the Algorand FAQ about how collecting rewards on staked assets works with compounding. The participation rewards are calculated automatically, however the compounding effect is not automatic. This is because the rewards are calculated from the last recorded balance on the blockchain, the easiest way to force rewards compounding is to send a zero Algo payment transaction to the target address on a frequent, recurring basis. This transaction will trigger the commit of all accrued rewards and record them to the on-chain balance of the account. As of this question, the network fee of a 0.00 ALGO transaction to myself is .001 ALGO, and the APR for rewards is ~5.827% (converted from ~6% APY). Let's say I have some amount of ALGO principal, P , and I decide to claim my rewards every d days. It stands to reason then that every time I claim my rewards, the resulting interest, I is: I = P * ( d * .05827 /365)-.001 Every d days, P goes to P + I . How do we find, for a given value P , which value d is the optimal compound interval such that we maximize the rate that P increases? My suspicion is that as P increases sufficiently high, it will make sense to compound much more frequently. It's been 11 years since I took Calculus 1 and 2, and this seems like an interesting problem. It seems like there's some recursive nature to this that makes it challenging.","Note to editors/mods: Please help me tag this appropriately! I'm not really a math wiz... is it multivariable calculus? See this note from the Algorand FAQ about how collecting rewards on staked assets works with compounding. The participation rewards are calculated automatically, however the compounding effect is not automatic. This is because the rewards are calculated from the last recorded balance on the blockchain, the easiest way to force rewards compounding is to send a zero Algo payment transaction to the target address on a frequent, recurring basis. This transaction will trigger the commit of all accrued rewards and record them to the on-chain balance of the account. As of this question, the network fee of a 0.00 ALGO transaction to myself is .001 ALGO, and the APR for rewards is ~5.827% (converted from ~6% APY). Let's say I have some amount of ALGO principal, P , and I decide to claim my rewards every d days. It stands to reason then that every time I claim my rewards, the resulting interest, I is: I = P * ( d * .05827 /365)-.001 Every d days, P goes to P + I . How do we find, for a given value P , which value d is the optimal compound interval such that we maximize the rate that P increases? My suspicion is that as P increases sufficiently high, it will make sense to compound much more frequently. It's been 11 years since I took Calculus 1 and 2, and this seems like an interesting problem. It seems like there's some recursive nature to this that makes it challenging.",,"['multivariable-calculus', 'exponential-function']"
66,Shifrin's Multivariable Mathematics,Shifrin's Multivariable Mathematics,,"I'm taking multivariable calc this fall. I began self-studying on my own a couple months ago, using Salas's calc text. Then I stumbled on Ted Shrifin's MTH3500/10 incredible lecture series on Youtube. His text, Multivariable Mathematics, arrived in the mail yesterday evening! It's a freakin' gorgeous book, and I'm super excited to start. I have three questions. (1) Question for anyone that's worked or taught from this book: In terms of coverage, how does this book compare with something like Munkres, Calculus on Manifolds? Is there significant overlap? Will I be prepared for Munkres after reading Shifrin? (2) I decided I had to have the book after bing-watching the lectures. The ideas there are just so lovely, and so nicely explained. The idea of linear maps is a beautiful one, and I'm amazed at how it generalizes the results of single-variable calc. Matrices, matrix multiplication, and the like can seem so unmotivated and pointless, until one sees that matrix multiplication is the algebra behind the composition of linear maps. Historically, was it the need to put multivariable calc on a sound footing that motivated the development of linear algebra? (3) I'll be taking linear algebra in the fall, too. I wonder: Why (or how!) would anyone successfully teach linear algebra without using multivariable calculus and the geometry of linear maps to reify matrices and their symbol-shunting? I mean, Shifrin strives to show the connection between linear algebra and multivariable calc, and this is an unusual approach, right? But then how else would linear algebra be taught?","I'm taking multivariable calc this fall. I began self-studying on my own a couple months ago, using Salas's calc text. Then I stumbled on Ted Shrifin's MTH3500/10 incredible lecture series on Youtube. His text, Multivariable Mathematics, arrived in the mail yesterday evening! It's a freakin' gorgeous book, and I'm super excited to start. I have three questions. (1) Question for anyone that's worked or taught from this book: In terms of coverage, how does this book compare with something like Munkres, Calculus on Manifolds? Is there significant overlap? Will I be prepared for Munkres after reading Shifrin? (2) I decided I had to have the book after bing-watching the lectures. The ideas there are just so lovely, and so nicely explained. The idea of linear maps is a beautiful one, and I'm amazed at how it generalizes the results of single-variable calc. Matrices, matrix multiplication, and the like can seem so unmotivated and pointless, until one sees that matrix multiplication is the algebra behind the composition of linear maps. Historically, was it the need to put multivariable calc on a sound footing that motivated the development of linear algebra? (3) I'll be taking linear algebra in the fall, too. I wonder: Why (or how!) would anyone successfully teach linear algebra without using multivariable calculus and the geometry of linear maps to reify matrices and their symbol-shunting? I mean, Shifrin strives to show the connection between linear algebra and multivariable calc, and this is an unusual approach, right? But then how else would linear algebra be taught?",,['multivariable-calculus']
67,Why is there a separate symbol for partial derivatives? [duplicate],Why is there a separate symbol for partial derivatives? [duplicate],,"This question already has answers here : The notation for partial derivatives (2 answers) Closed 7 years ago . The concept of a partial derivative is very simple: for a multivariate function $f$, the partial derivative of $f$ with respect to a single variable $x$ is computed by treating the other variables as constants and differentiating $f$ with respect to $x$. As a student of Calculus I, I do not fully understand the need for a $\partial y / \partial x$.  As far as I know, the special partial $\partial$ does not change the process of the computation.  In addition, I have become confused as the calculus of my physics course increases in difficulty. Take a three-dimensional position vector $\vec r = \vec x + \vec y + \vec z$ and an electric field vector $\vec E$ that varies with $\vec r$.  If $V$ denotes electric potential, then $\Delta V = -\int \vec E\cdot d\vec r$. On a review sheet that my teacher created, he expanded this, saying $$\begin{align} E_x &= -\partial V / \partial x \\ E_y &= -\partial V / \partial y \\ E_z &= -\partial V / \partial z \end{align}$$ Given the standard setup of a partial derivative, I see no issue with this.  However, our standard formula chart reads that $$E_x = -\frac{dV}{dx}$$ and this genuinely confuses me since our calculations of field potential are almost always expanded to multiple dimensions. What is the need for a $\partial f / \partial x$ notation, and why are sources (at least in physics, the only application of partial derivatives I encounter during the course of the school day) inconsistent?","This question already has answers here : The notation for partial derivatives (2 answers) Closed 7 years ago . The concept of a partial derivative is very simple: for a multivariate function $f$, the partial derivative of $f$ with respect to a single variable $x$ is computed by treating the other variables as constants and differentiating $f$ with respect to $x$. As a student of Calculus I, I do not fully understand the need for a $\partial y / \partial x$.  As far as I know, the special partial $\partial$ does not change the process of the computation.  In addition, I have become confused as the calculus of my physics course increases in difficulty. Take a three-dimensional position vector $\vec r = \vec x + \vec y + \vec z$ and an electric field vector $\vec E$ that varies with $\vec r$.  If $V$ denotes electric potential, then $\Delta V = -\int \vec E\cdot d\vec r$. On a review sheet that my teacher created, he expanded this, saying $$\begin{align} E_x &= -\partial V / \partial x \\ E_y &= -\partial V / \partial y \\ E_z &= -\partial V / \partial z \end{align}$$ Given the standard setup of a partial derivative, I see no issue with this.  However, our standard formula chart reads that $$E_x = -\frac{dV}{dx}$$ and this genuinely confuses me since our calculations of field potential are almost always expanded to multiple dimensions. What is the need for a $\partial f / \partial x$ notation, and why are sources (at least in physics, the only application of partial derivatives I encounter during the course of the school day) inconsistent?",,"['multivariable-calculus', 'notation', 'partial-derivative']"
68,Gradient of squared distance to a convex set,Gradient of squared distance to a convex set,,"I have the following problem: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$, $f(x)=(\operatorname{dist}(x,D))^2$ where $D$ is a convex, close set in $\mathbb{R}^n$. Prove that $f$ is convex and $f'(x)=2(x-P_D(x))$. Here $P_D(x)$ is the projection of $x$ in $D$. I prove convexity, but I can't demonstrate that differential (gradient) is this. I think that the way to go is to use the definition, proving that $$\lim_{x\rightarrow x_0} \frac{|f(x)-f(x_0)-f'(x_0)^{\top}(x-x_0)|}{\|x-x_0\|}=0$$","I have the following problem: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$, $f(x)=(\operatorname{dist}(x,D))^2$ where $D$ is a convex, close set in $\mathbb{R}^n$. Prove that $f$ is convex and $f'(x)=2(x-P_D(x))$. Here $P_D(x)$ is the projection of $x$ in $D$. I prove convexity, but I can't demonstrate that differential (gradient) is this. I think that the way to go is to use the definition, proving that $$\lim_{x\rightarrow x_0} \frac{|f(x)-f(x_0)-f'(x_0)^{\top}(x-x_0)|}{\|x-x_0\|}=0$$",,"['multivariable-calculus', 'convex-optimization']"
69,Lagrange Multiplier Question and my attempt,Lagrange Multiplier Question and my attempt,,"Question is ""Find the extrema of $xyz$ when $x+y+z=a$ , $a>0$ "". Starting with usual Lagrange Multiplier method, I get $f_x = yz + \lambda = 0$ $f_y = xz + \lambda = 0$ $f_z = yz + \lambda = 0$ Now from three equations above, I multiply first by $x$ and second by $y$ and third by $z$ , I get $f_x = xyz + \lambda x = 0$ $f_y = xyz + \lambda y = 0$ $f_z = xyz + \lambda z = 0$ Clearly from these equating values of $xyz$ . I get $x=y=z$ . And thus I have solved the question and is consistent with my answer with textbook. BUT, if I manipulate equations in a way as if I equate values of $\lambda$ I get $yz=xz=xy$ Now I take $yz=xz$ . This implies either $z=0$ or $x=y$ . If I take $z=0$ and put in other two equations I get $xy=0$ which means either $x=0$ or $y=0$ . Say I take $x=0$ and now putting in constraint equation I get $y=a$ so i get $(0,a,0)$ . Not only this but by solving other equations like this I get $(0,a,0)$ , $(a,0,0)$ , $(0,0,a)$ , $((a-1)/2,(a-1)/2,1)$ . But this is not consistent with textbook. Can anybody help me out from here? Thanks","Question is ""Find the extrema of when , "". Starting with usual Lagrange Multiplier method, I get Now from three equations above, I multiply first by and second by and third by , I get Clearly from these equating values of . I get . And thus I have solved the question and is consistent with my answer with textbook. BUT, if I manipulate equations in a way as if I equate values of I get Now I take . This implies either or . If I take and put in other two equations I get which means either or . Say I take and now putting in constraint equation I get so i get . Not only this but by solving other equations like this I get , , , . But this is not consistent with textbook. Can anybody help me out from here? Thanks","xyz x+y+z=a a>0 f_x = yz + \lambda = 0 f_y = xz + \lambda = 0 f_z = yz + \lambda = 0 x y z f_x = xyz + \lambda x = 0 f_y = xyz + \lambda y = 0 f_z = xyz + \lambda z = 0 xyz x=y=z \lambda yz=xz=xy yz=xz z=0 x=y z=0 xy=0 x=0 y=0 x=0 y=a (0,a,0) (0,a,0) (a,0,0) (0,0,a) ((a-1)/2,(a-1)/2,1)","['multivariable-calculus', 'lagrange-multiplier']"
70,"if the curvature is constant and positive, then it is on the circunference","if the curvature is constant and positive, then it is on the circunference",,"I'm trying to prove that if $\alpha(t)=(x(t),y(t))$ is a $C^2$ regular curve $(\alpha'\neq0)$ with constant and positive curvature, then $\alpha$ is on the circumference and if $\alpha$ is on the circumference, then it's curvature is constant. I solved the second part since $\alpha$ is the circunference, then $\alpha(t)=(r\cos\theta,r\sin\theta)$ , so $|\alpha'(t)|=r$ , I need help in the first part. Thanks a lot.","I'm trying to prove that if is a regular curve with constant and positive curvature, then is on the circumference and if is on the circumference, then it's curvature is constant. I solved the second part since is the circunference, then , so , I need help in the first part. Thanks a lot.","\alpha(t)=(x(t),y(t)) C^2 (\alpha'\neq0) \alpha \alpha \alpha \alpha(t)=(r\cos\theta,r\sin\theta) |\alpha'(t)|=r","['differential-geometry', 'multivariable-calculus']"
71,Spivak's Proof of Inverse function theorem (2nd question),Spivak's Proof of Inverse function theorem (2nd question),,"Suppose $\det f^\prime(a) \neq 0$, $f$ is continuously differentiable in an open set containing $a$. Spivak then show that there exist a closed rectangle $U$ containing $a$ in its interior such that $f(x) \neq f(a)$ for all $x \in U$. Question1: Spivak then asserts that since $f$ is continuously differentiable in an open set containing $a$, therefore $\det f'(x) \neq 0$ for all $x \in U$. Why? Question 2: $|D_j f^i (x)-D_j f^i (a)|<\tfrac{1}{2n^2}.$ How can we prove it?","Suppose $\det f^\prime(a) \neq 0$, $f$ is continuously differentiable in an open set containing $a$. Spivak then show that there exist a closed rectangle $U$ containing $a$ in its interior such that $f(x) \neq f(a)$ for all $x \in U$. Question1: Spivak then asserts that since $f$ is continuously differentiable in an open set containing $a$, therefore $\det f'(x) \neq 0$ for all $x \in U$. Why? Question 2: $|D_j f^i (x)-D_j f^i (a)|<\tfrac{1}{2n^2}.$ How can we prove it?",,['multivariable-calculus']
72,What is the divergence of a matrix valued function?,What is the divergence of a matrix valued function?,,"According to Wikipedia: The divergence of a continuously differentiable tensor field $\underline{\underline{\epsilon}}$ is: $$\overrightarrow{\operatorname{div}}\,(\mathbf{\underline{\underline{\epsilon}}}) =  \begin{bmatrix} \frac{\partial \epsilon_{xx}}{\partial x} +\frac{\partial \epsilon_{xy}}{\partial y} +\frac{\partial \epsilon_{xz}}{\partial z} \\ \frac{\partial \epsilon_{yx}}{\partial x} +\frac{\partial \epsilon_{yy}}{\partial y} +\frac{\partial \epsilon_{yz}}{\partial z} \\ \frac{\partial \epsilon_{zx}}{\partial x} +\frac{\partial \epsilon_{zy}}{\partial y} +\frac{\partial \epsilon_{zz}}{\partial z} \end{bmatrix} $$ How do you get this formula from the definition of divergence? Either formally, or with some abuse of notation?","According to Wikipedia: The divergence of a continuously differentiable tensor field $\underline{\underline{\epsilon}}$ is: $$\overrightarrow{\operatorname{div}}\,(\mathbf{\underline{\underline{\epsilon}}}) =  \begin{bmatrix} \frac{\partial \epsilon_{xx}}{\partial x} +\frac{\partial \epsilon_{xy}}{\partial y} +\frac{\partial \epsilon_{xz}}{\partial z} \\ \frac{\partial \epsilon_{yx}}{\partial x} +\frac{\partial \epsilon_{yy}}{\partial y} +\frac{\partial \epsilon_{yz}}{\partial z} \\ \frac{\partial \epsilon_{zx}}{\partial x} +\frac{\partial \epsilon_{zy}}{\partial y} +\frac{\partial \epsilon_{zz}}{\partial z} \end{bmatrix} $$ How do you get this formula from the definition of divergence? Either formally, or with some abuse of notation?",,"['multivariable-calculus', 'tensors']"
73,Gradient of a Harmonic Function,Gradient of a Harmonic Function,,"I was asked the following vector calculus problem: Let $D$ be the unit ball and let $S$ be the unit sphere in $\mathbb{R}^3$ .  Suppose that $F:\mathbb{R}^3\rightarrow \mathbb{R}^3$ is a $C^1$ vector field on some open neighborhood of $D$ which satisfies: $(i) \nabla\times F=0$ $(ii) \nabla\cdot F=0$ $(iii)$ On $S$ , $F$ is orthogonal to the radial vector. Prove that $F=0$ on all of $D$ . Conditions $(i)$ and $(ii)$ imply that $F=\nabla g$ for some $g:\mathbb{R}^3\rightarrow \mathbb{R}$ where $g$ must be harmonic as well. I know one solution (see end), however my initial instinct was to try to use the max/min property of harmonic functions, and I couldn't get it to work.  Since the gradient is always orthogonal to the sphere, there must be a point on the sphere where it is $0$ .  (Hairy ball)  If that was a local max or min in $\mathbb{R}^3$ we would be done, by taking a small neighborhood around it.  If it is a saddle point this doesn't work.  (We know that it must be a local max/min on $S$ since it is harmonic) My question is: Is there any way to modify this approach, and solve the problem? Thanks! Other Solution: Here is one solution that first uses the fact that the radial vector is orthogonal, and then applies Gauss's Divergence theorem to the function $gF$ . ( $\nabla g=F$ ) That is $$0=\iint_S (gF\cdot n)dS=\iiint_D \nabla\cdot (gF)dV=\iiint_D \|F\|^2dV,$$ and since the integrand on the right hand side is non-negative, continuous and integrates to give zero, it must be zero.","I was asked the following vector calculus problem: Let be the unit ball and let be the unit sphere in .  Suppose that is a vector field on some open neighborhood of which satisfies: On , is orthogonal to the radial vector. Prove that on all of . Conditions and imply that for some where must be harmonic as well. I know one solution (see end), however my initial instinct was to try to use the max/min property of harmonic functions, and I couldn't get it to work.  Since the gradient is always orthogonal to the sphere, there must be a point on the sphere where it is .  (Hairy ball)  If that was a local max or min in we would be done, by taking a small neighborhood around it.  If it is a saddle point this doesn't work.  (We know that it must be a local max/min on since it is harmonic) My question is: Is there any way to modify this approach, and solve the problem? Thanks! Other Solution: Here is one solution that first uses the fact that the radial vector is orthogonal, and then applies Gauss's Divergence theorem to the function . ( ) That is and since the integrand on the right hand side is non-negative, continuous and integrates to give zero, it must be zero.","D S \mathbb{R}^3 F:\mathbb{R}^3\rightarrow \mathbb{R}^3 C^1 D (i) \nabla\times F=0 (ii) \nabla\cdot F=0 (iii) S F F=0 D (i) (ii) F=\nabla g g:\mathbb{R}^3\rightarrow \mathbb{R} g 0 \mathbb{R}^3 S gF \nabla g=F 0=\iint_S (gF\cdot n)dS=\iiint_D \nabla\cdot (gF)dV=\iiint_D \|F\|^2dV,","['multivariable-calculus', 'partial-differential-equations']"
74,Why integrate on cubes that's not injective?,Why integrate on cubes that's not injective?,,"Again, this is a conceptual(soft) problem I had while reading Spivak's calculus on manifold. There, to develop the theory of integration, Spivak chose to integrate k-forms on singular cubes. However, as pointed out here , singular cubes can collapse dimension, so it seems theoretically possible that one pulls back a differential form onto a higher dimension cubical region for evaluating the integral, and I wonder when could that ever be useful? Why don't we just add the assumption that singular cubes are injective? Also, without this injectivity I find the picture of chains even less geometrically intuitive...","Again, this is a conceptual(soft) problem I had while reading Spivak's calculus on manifold. There, to develop the theory of integration, Spivak chose to integrate k-forms on singular cubes. However, as pointed out here , singular cubes can collapse dimension, so it seems theoretically possible that one pulls back a differential form onto a higher dimension cubical region for evaluating the integral, and I wonder when could that ever be useful? Why don't we just add the assumption that singular cubes are injective? Also, without this injectivity I find the picture of chains even less geometrically intuitive...",,"['multivariable-calculus', 'differential-topology', 'differential-forms']"
75,"For positive $a$, $b$, $c$, show $\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a} \ge \frac{a+b+c}{a+b+c+\sqrt[3]{abc}}$","For positive , , , show",a b c \frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a} \ge \frac{a+b+c}{a+b+c+\sqrt[3]{abc}},"Prove that for every three positive numbers $a, b, c$ : $$\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a} \ge \frac{a+b+c}{a+b+c+\sqrt[3]{abc}}$$ I tried using $\sum_{\mathrm{cyc}}$ but I haven't got far. I also tried: $$\sqrt[3]{abc}=\frac{1}{3}\sqrt[3]{3a \cdot 3b \cdot 3c}$$ and according to the HM-GM inequality we can replace the root by: $$\frac13 \frac{3}{\frac1{3a} +\frac1{3b}+\frac1{3c}}=\frac{1}{\frac13\left(\frac1a+\frac1b+\frac1c\right)}=\frac{3}{ \frac1a+\frac1b+\frac1c}$$ And that's it. Can you give me a hint or a solution for the question?",Prove that for every three positive numbers : I tried using but I haven't got far. I also tried: and according to the HM-GM inequality we can replace the root by: And that's it. Can you give me a hint or a solution for the question?,"a, b, c \frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a} \ge \frac{a+b+c}{a+b+c+\sqrt[3]{abc}} \sum_{\mathrm{cyc}} \sqrt[3]{abc}=\frac{1}{3}\sqrt[3]{3a \cdot 3b \cdot 3c} \frac13 \frac{3}{\frac1{3a} +\frac1{3b}+\frac1{3c}}=\frac{1}{\frac13\left(\frac1a+\frac1b+\frac1c\right)}=\frac{3}{ \frac1a+\frac1b+\frac1c}","['multivariable-calculus', 'inequality', 'contest-math', 'cauchy-schwarz-inequality']"
76,How do I show that partial derivatives exist everywhere?,How do I show that partial derivatives exist everywhere?,,"I'm having trouble with a certain multi-variable calculus question. $$  f(x,y) = \begin{cases} \large\frac{2xy^2}{x^2 + y^4},  & \text{$(x,y)\neq 0$} \\[2ex] 0, & \text{$(x,y) = 0$} \end{cases}$$ I need to show that both $\large\frac{f}{x}$ and $\large\frac{f}{y}$ exist everywhere. I can easily manage to find both partial derivatives, but I'm not really sure what the question means when it asks to show that they ""exist everywhere"". Any help would be appreciated, thanks.","I'm having trouble with a certain multi-variable calculus question. $$  f(x,y) = \begin{cases} \large\frac{2xy^2}{x^2 + y^4},  & \text{$(x,y)\neq 0$} \\[2ex] 0, & \text{$(x,y) = 0$} \end{cases}$$ I need to show that both $\large\frac{f}{x}$ and $\large\frac{f}{y}$ exist everywhere. I can easily manage to find both partial derivatives, but I'm not really sure what the question means when it asks to show that they ""exist everywhere"". Any help would be appreciated, thanks.",,"['multivariable-calculus', 'derivatives', 'continuity', 'partial-derivative', 'vector-analysis']"
77,Finding the Gradient of a Tensor Field,Finding the Gradient of a Tensor Field,,"Finding the Gradient of a Scalar Field I understand that you can find the gradient of a scalar field, in an arbitrary number of dimensions like so : $$grad(f) = \vec{\nabla}f = \left<\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{1}},...,\frac{\partial f}{\partial x_{n}}\right> = \begin{bmatrix}     \frac{\partial f}{\partial x_{1}}  \\          \frac{\partial f}{\partial x_{2}}   \\        ... \\     \frac{\partial f}{\partial x_{n}}        \end{bmatrix}$$ where $f$ is a scalar function, $f: \mathbb{R^n} \to \mathbb{R}$. And as you can see this generalizes consistently to higher-dimensional scalar fields. All of this is straight out of Multivariable Calculus. Finding the Gradient of a Vector Field Furthermore finding the gradient of a Vector Field, is given by a Tensor i.e. given $f$ to be a vector function, $f : \mathbb{R^m} \to \mathbb{R^n}$ : $$grad(\vec{f}) = \displaystyle  \nabla \vec{f}  = T = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & ... & \frac{\partial f_1}{\partial x_m} \\ \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & ... & \frac{\partial f_2}{\partial x_m} \\ ... & ... & ... & ... \\ \frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & ... & \frac{\partial f_n}{\partial x_m} \\ \end{bmatrix}$$ with $T$ denoting the tensor (a $n\ $x$\ n$ matrix of partial derivatives of $\vec{f}$'s scalar components, i.e. rank-$0$ tensor components, correct me if what I said in these brackets is wrong )  which tells us how the vector field changes in any direction. How to find the Gradient of a Tensor Field? But how do you find the gradient of a Tensor Field? I understand that to answer this question we may need to generalize the concept of a tensor field a bit further. If I understand correctly, a scalar is a tensor of rank-$0$, a vector is a tensor of rank-$1$. Is it then fine to generalize scalar fields as tensor fields of rank-$0$, and vector fields as tensor fields of rank-$1$? If so then it means we've been finding the gradient of tensor fields (albeit of rank-0 being scalar fields) in our Multivariable courses all along, we just didn't know it. By extending the logic behind the leap between taking the gradient of a Scalar Field, to taking the gradient of a Vector Field, is it then correct to say that : The gradient of a Tensor field of rank-$n$ is a Tensor field of rank-($n+1$) ?","Finding the Gradient of a Scalar Field I understand that you can find the gradient of a scalar field, in an arbitrary number of dimensions like so : $$grad(f) = \vec{\nabla}f = \left<\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{1}},...,\frac{\partial f}{\partial x_{n}}\right> = \begin{bmatrix}     \frac{\partial f}{\partial x_{1}}  \\          \frac{\partial f}{\partial x_{2}}   \\        ... \\     \frac{\partial f}{\partial x_{n}}        \end{bmatrix}$$ where $f$ is a scalar function, $f: \mathbb{R^n} \to \mathbb{R}$. And as you can see this generalizes consistently to higher-dimensional scalar fields. All of this is straight out of Multivariable Calculus. Finding the Gradient of a Vector Field Furthermore finding the gradient of a Vector Field, is given by a Tensor i.e. given $f$ to be a vector function, $f : \mathbb{R^m} \to \mathbb{R^n}$ : $$grad(\vec{f}) = \displaystyle  \nabla \vec{f}  = T = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & ... & \frac{\partial f_1}{\partial x_m} \\ \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & ... & \frac{\partial f_2}{\partial x_m} \\ ... & ... & ... & ... \\ \frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & ... & \frac{\partial f_n}{\partial x_m} \\ \end{bmatrix}$$ with $T$ denoting the tensor (a $n\ $x$\ n$ matrix of partial derivatives of $\vec{f}$'s scalar components, i.e. rank-$0$ tensor components, correct me if what I said in these brackets is wrong )  which tells us how the vector field changes in any direction. How to find the Gradient of a Tensor Field? But how do you find the gradient of a Tensor Field? I understand that to answer this question we may need to generalize the concept of a tensor field a bit further. If I understand correctly, a scalar is a tensor of rank-$0$, a vector is a tensor of rank-$1$. Is it then fine to generalize scalar fields as tensor fields of rank-$0$, and vector fields as tensor fields of rank-$1$? If so then it means we've been finding the gradient of tensor fields (albeit of rank-0 being scalar fields) in our Multivariable courses all along, we just didn't know it. By extending the logic behind the leap between taking the gradient of a Scalar Field, to taking the gradient of a Vector Field, is it then correct to say that : The gradient of a Tensor field of rank-$n$ is a Tensor field of rank-($n+1$) ?",,"['multivariable-calculus', 'tensors']"
78,Visualizing linear transformations on vector fields,Visualizing linear transformations on vector fields,,"I'm trying to figure out what it means to apply a linear transformation to a vector field geometrically.  So I start with the easiest geometrically interesting transformation: a rotation. Using StreamPlot in WolframAlpha I see that applying the transformation $T(x,y) = (-y,x) = \pmatrix{\cos(90^\circ) & -\sin(90^\circ) \\ \sin(90^\circ) & \cos(90^\circ)}\pmatrix{x \\ y}$ yields exactly what I'd expect: circular stream lines: However when I try a different angle, say $\pi / 6$, then I have $T(x,y)=\left(\frac{\sqrt{3}}{2}x-\frac12 y, \frac12 x + \frac{\sqrt{3}}{2}y\right)$.  But the picture no longer looks like circles, but spirals : Clearly something's wrong with my interpretation here.  Why do the stream lines look the way they do?  How can I imagine linear transformations being applied to vector fields?","I'm trying to figure out what it means to apply a linear transformation to a vector field geometrically.  So I start with the easiest geometrically interesting transformation: a rotation. Using StreamPlot in WolframAlpha I see that applying the transformation $T(x,y) = (-y,x) = \pmatrix{\cos(90^\circ) & -\sin(90^\circ) \\ \sin(90^\circ) & \cos(90^\circ)}\pmatrix{x \\ y}$ yields exactly what I'd expect: circular stream lines: However when I try a different angle, say $\pi / 6$, then I have $T(x,y)=\left(\frac{\sqrt{3}}{2}x-\frac12 y, \frac12 x + \frac{\sqrt{3}}{2}y\right)$.  But the picture no longer looks like circles, but spirals : Clearly something's wrong with my interpretation here.  Why do the stream lines look the way they do?  How can I imagine linear transformations being applied to vector fields?",,"['multivariable-calculus', 'vectors', 'vector-analysis', 'visualization']"
79,Multiple integration questions and my attempt,Multiple integration questions and my attempt,,"P1: Find volume of portion of sphere centered at $(0,a,0)$ with radius $a$,  between planes $y=0$ and $y=a$ . I changed to spherical coordinates and I calculated volume in 1st quadrant and multiplied by 4 to  get required volume. The limits for $\theta$ and $\phi$ are $0$ to $\pi/2$ and for $\rho$ are $0$ to $2\sin\theta \sin \phi$, but I couldn't get correct and and is double than what it is. Where am I wrong ? Can please someone help me figure out . P2:  Find volume of portion of sphere which is inside right circular cone having its vertex at origin and making angle alpha with positive z axis .. in this q i dont know bounds..can someone help me with bounds only and explanation of them P3: Find volume of solid which is below plane z=2x+3 and in xy plane bounded by $y^{2}=x$ , $x=0$ , $x=2$ ..i cant figure out region to integrate over in xy plane","P1: Find volume of portion of sphere centered at $(0,a,0)$ with radius $a$,  between planes $y=0$ and $y=a$ . I changed to spherical coordinates and I calculated volume in 1st quadrant and multiplied by 4 to  get required volume. The limits for $\theta$ and $\phi$ are $0$ to $\pi/2$ and for $\rho$ are $0$ to $2\sin\theta \sin \phi$, but I couldn't get correct and and is double than what it is. Where am I wrong ? Can please someone help me figure out . P2:  Find volume of portion of sphere which is inside right circular cone having its vertex at origin and making angle alpha with positive z axis .. in this q i dont know bounds..can someone help me with bounds only and explanation of them P3: Find volume of solid which is below plane z=2x+3 and in xy plane bounded by $y^{2}=x$ , $x=0$ , $x=2$ ..i cant figure out region to integrate over in xy plane",,['multivariable-calculus']
80,Show the Grassmannian is a smooth manifold (using dummy definition of smooth manifold),Show the Grassmannian is a smooth manifold (using dummy definition of smooth manifold),,"We received the following problem in my Differential Geometry class: Suppose $0\leq k \leq n$ are integers. Let $G(k,n)$ be the collection of orthogonal projections $T: \mathbb{R}^n \to \mathbb{R}^n$ with rank $k$. Identifying the collection $L(\mathbb{R}^n , \mathbb{R}^n)$ of linear transformations $\mathbb{R}^n \to \mathbb{R}^n$ with the collection $M_{n \times n}$ of $n \times n$ matrices, show that $G(k,n) \subset M_{n\times n} \simeq \mathbb{R}^2$ is a smooth manifold. Here our (provisional) definition of a $k$-manifold is $M \subset \mathbb{R}^n$ is a $k$-manifold if for each point $p\in M$ there exists a neighborhood $U$ of $p$ and $I = \{i_1, \dotsc, i_k\} \subset \{1,\dotsc,n\}$ such that $U \cap M$ is the graph of a $C^\infty$ function $f: V \to \mathbb{R}^{I^c}$, where $V \subset \mathbb{R}^I$ . Now I know the Grassmannian should have dimension $k(n-k)$ (see this question). That means that out of the conditions $F: \mathbb{R}^{n^2} \to \mathbb{R}^{n^2}$ requiring $\rho$ to be an orthogonal projection of rank $k$, I need to find a submatrix of $DF$ of dimension $n^2 - k(n-k) = n^2 - kn +k^2$ which is non-singular. The requirements are 1. $\rho^2 = \rho \; $ 2. $\rho^T = \rho \;$ 3.  $\operatorname{rk}{(\rho)}=k$. So $$a_{ij} = a_{ji} \tag{1}$$ $$\sum_{k=1}^n a_{ik}a_{kj} = a_{ij} \tag{2}.$$ I need to find a clever way to express the last one as an equation, and to show that at each point there is the necessary submatrix. Any ideas?","We received the following problem in my Differential Geometry class: Suppose $0\leq k \leq n$ are integers. Let $G(k,n)$ be the collection of orthogonal projections $T: \mathbb{R}^n \to \mathbb{R}^n$ with rank $k$. Identifying the collection $L(\mathbb{R}^n , \mathbb{R}^n)$ of linear transformations $\mathbb{R}^n \to \mathbb{R}^n$ with the collection $M_{n \times n}$ of $n \times n$ matrices, show that $G(k,n) \subset M_{n\times n} \simeq \mathbb{R}^2$ is a smooth manifold. Here our (provisional) definition of a $k$-manifold is $M \subset \mathbb{R}^n$ is a $k$-manifold if for each point $p\in M$ there exists a neighborhood $U$ of $p$ and $I = \{i_1, \dotsc, i_k\} \subset \{1,\dotsc,n\}$ such that $U \cap M$ is the graph of a $C^\infty$ function $f: V \to \mathbb{R}^{I^c}$, where $V \subset \mathbb{R}^I$ . Now I know the Grassmannian should have dimension $k(n-k)$ (see this question). That means that out of the conditions $F: \mathbb{R}^{n^2} \to \mathbb{R}^{n^2}$ requiring $\rho$ to be an orthogonal projection of rank $k$, I need to find a submatrix of $DF$ of dimension $n^2 - k(n-k) = n^2 - kn +k^2$ which is non-singular. The requirements are 1. $\rho^2 = \rho \; $ 2. $\rho^T = \rho \;$ 3.  $\operatorname{rk}{(\rho)}=k$. So $$a_{ij} = a_{ji} \tag{1}$$ $$\sum_{k=1}^n a_{ik}a_{kj} = a_{ij} \tag{2}.$$ I need to find a clever way to express the last one as an equation, and to show that at each point there is the necessary submatrix. Any ideas?",,"['multivariable-calculus', 'differential-geometry']"
81,Need help understanding solution for using change of variables to evaluate integral,Need help understanding solution for using change of variables to evaluate integral,,"Use the change of variables $x=u^2-v^2$ , $y=2uv$ to evaluate $$\iint_{R}y dA$$ where $R$ is the region bounded by the x-axis, the parabolas $y^2=4-4x$ and $y^2=4+4x, y\geq0$ I'm following along with this solution: I don't understand why the preimage of the domain is $[0,1]\times [0,1]$ I understand how we get $u=\pm 1$ and $v=\pm 1$ but I don't know how the bounds are from $0$ to $1$ ? Why isn't it $$\int_{-1}^{1}\int_{-1}^{1} 2uv(4u^2+4v^2)dudv$$ EDIT: verify that there is some typo for the Jacobian, Because it should be $\begin{vmatrix} 2u & -2v \\ 2v & 2u  \end{vmatrix}=4(u^2+v^2)$","Use the change of variables , to evaluate where is the region bounded by the x-axis, the parabolas and I'm following along with this solution: I don't understand why the preimage of the domain is I understand how we get and but I don't know how the bounds are from to ? Why isn't it EDIT: verify that there is some typo for the Jacobian, Because it should be","x=u^2-v^2 y=2uv \iint_{R}y dA R y^2=4-4x y^2=4+4x, y\geq0 [0,1]\times [0,1] u=\pm 1 v=\pm 1 0 1 \int_{-1}^{1}\int_{-1}^{1} 2uv(4u^2+4v^2)dudv \begin{vmatrix}
2u & -2v \\
2v & 2u 
\end{vmatrix}=4(u^2+v^2)","['multivariable-calculus', 'change-of-variable']"
82,show this inequality with $\sum_{i=1}^{n}a_{i}=n$,show this inequality with,\sum_{i=1}^{n}a_{i}=n,"Let $n\ge 3$ be postive  number, $a_{i}>0,i=1,2,\cdots,n$ ,and $\displaystyle\sum_{i=1}^{n}a_{i}=n$ ,show that $$a^3_{1}a_{2}+a^3_{2}a_{3}+\cdots+a^3_{n}a_{1}+n\ge 2(a_{1}a_{2}\cdots a_{n-1}+a_{2}a_{3}\cdots a_{n}+a_{n}a_{1}\cdots a_{n-2})$$ it seem can use indution to prove it.when $n=3$ ,it must prove $$a^3_{1}a_{2}+a^3_{2}a_{3}+a^3_{3}a_{1}+3\ge 2(a_{1}a_{2}+a_{2}a_{3}+a_{3}a_{1})$$ it seem  use three shcur inequaliy $$a^3+b^3+c^3+3abc\ge \sum ab(a+b)$$ then we have $$a^2+b^2+c^2+3(abc)^{2/3}\ge 2(ab+bc+ca)$$","Let be postive  number, ,and ,show that it seem can use indution to prove it.when ,it must prove it seem  use three shcur inequaliy then we have","n\ge 3 a_{i}>0,i=1,2,\cdots,n \displaystyle\sum_{i=1}^{n}a_{i}=n a^3_{1}a_{2}+a^3_{2}a_{3}+\cdots+a^3_{n}a_{1}+n\ge 2(a_{1}a_{2}\cdots a_{n-1}+a_{2}a_{3}\cdots a_{n}+a_{n}a_{1}\cdots a_{n-2}) n=3 a^3_{1}a_{2}+a^3_{2}a_{3}+a^3_{3}a_{1}+3\ge 2(a_{1}a_{2}+a_{2}a_{3}+a_{3}a_{1}) a^3+b^3+c^3+3abc\ge \sum ab(a+b) a^2+b^2+c^2+3(abc)^{2/3}\ge 2(ab+bc+ca)","['multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality', 'buffalo-way']"
83,What is the difference between linear approximation and a differential?,What is the difference between linear approximation and a differential?,,"From my understanding, linear approximations and differentials both use the tangent line to a function to estimate the value of the function at a point. I understand that their respective equations are different - but conceptually how are they different?","From my understanding, linear approximations and differentials both use the tangent line to a function to estimate the value of the function at a point. I understand that their respective equations are different - but conceptually how are they different?",,"['multivariable-calculus', 'linear-approximation']"
84,"Inverse Function Theorem, Spivak's Proof","Inverse Function Theorem, Spivak's Proof",,"I'm having a lot of trouble following the proof of the following theorem. This is from Spivak's Calculus on Manifolds. 2-11 Theorem (Inverse Function Theorem) . Suppose that $f: \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable in an open set containing $a$ , and let $\det f'(a) \not= 0$ . Then there is an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f: V\to W$ has a continuous inverse $f^{-1}:W \to V$ which is differentiable and for all $y\in W$ satisfies $$ (f^{-1})'(y) = [f'(f^{-1}(y))]^{-1}.$$ The proof starts off with the following. Proof . Let $\lambda$ be the linear transformation $Df(a)$ . Then $\lambda$ is non-singular, since $\det f'(a) \not= 0$ . Now $D(\lambda^{-1}\circ f)(a) = D(\lambda^{-1})(f(a))\circ Df(a) = \lambda^{-1}\circ Df(a)$ is the identity linear transformation. If the theorem is true for $\lambda^{-1}\circ f$ , it is clearly true for $f$ . Therefore we may assume at the outset that $\lambda$ is the identity. The problem I'm having is that I don't quite see where he's trying to go with this argument. I saw another post ( Spivak's proof of Inverse Function Theorem ) that explained the statement If the theorem is true for $\lambda^{-1}\circ f$ , it is clearly true for $f$ . but exactly why is it necessary for this proof? Also, when it says Therefore we may assume at the outset that $\lambda$ is the identity. why are we assuming that $\lambda$ is the identity?","I'm having a lot of trouble following the proof of the following theorem. This is from Spivak's Calculus on Manifolds. 2-11 Theorem (Inverse Function Theorem) . Suppose that is continuously differentiable in an open set containing , and let . Then there is an open set containing and an open set containing such that has a continuous inverse which is differentiable and for all satisfies The proof starts off with the following. Proof . Let be the linear transformation . Then is non-singular, since . Now is the identity linear transformation. If the theorem is true for , it is clearly true for . Therefore we may assume at the outset that is the identity. The problem I'm having is that I don't quite see where he's trying to go with this argument. I saw another post ( Spivak's proof of Inverse Function Theorem ) that explained the statement If the theorem is true for , it is clearly true for . but exactly why is it necessary for this proof? Also, when it says Therefore we may assume at the outset that is the identity. why are we assuming that is the identity?",f: \mathbb{R}^n \to \mathbb{R}^n a \det f'(a) \not= 0 V a W f(a) f: V\to W f^{-1}:W \to V y\in W  (f^{-1})'(y) = [f'(f^{-1}(y))]^{-1}. \lambda Df(a) \lambda \det f'(a) \not= 0 D(\lambda^{-1}\circ f)(a) = D(\lambda^{-1})(f(a))\circ Df(a) = \lambda^{-1}\circ Df(a) \lambda^{-1}\circ f f \lambda \lambda^{-1}\circ f f \lambda \lambda,['multivariable-calculus']
85,Laplace's Equation with Neumann BC,Laplace's Equation with Neumann BC,,"Hi fellow math enthusiasts, I am currently working on some research to do with the electric field induced within the brain via magnetic stimulation. I am trying to solve the partial differential equation in 2D cartesian co-ordinates \begin{equation} \nabla \cdot \underline{\sigma} \vec{\nabla}\phi = - \nabla \cdot  \left(\underline{\sigma} \frac{\partial \vec{A}}{\partial t}\right) \end{equation} For a magnetic vector potential of $\vec{A}=-\frac y2 \hat{i}+\frac x2\hat{j}$ and the magnetic vector potential can be separated so the function of time does not need to be known. From above this can be written as $$ \sigma_x\frac{\partial^2\phi}{\partial x^2}+\sigma_y\frac{\partial^2\phi}{\partial y^2}=0 $$ and after a change of variables $\gamma=\left(\frac{\sigma_x}{\sigma_y}\right)^{\frac12}y$ revels Laplace's equation $$ \frac{\partial^2\phi}{\partial x^2}+\frac{\partial^2\phi}{\partial \gamma^2}=0 $$ The surface to which Laplace's equation is to be solved in is the rectangle bounded by the lines $x=a,\ x=-a$ and $y=b,\ y=-b$ subject to Neumann boundary conditions  \begin{align} \left.\frac{\partial\phi}{\partial x}\right|_{x=a}&=\frac y2 \quad &\left.\frac{\partial\phi}{\partial x}\right|_{x=-a}&=\frac y2\\ \left.\frac{\partial\phi}{\partial y}\right|_{y=b}&=-\frac x2 \quad &\left.\frac{\partial\phi}{\partial y}\right|_{y=-b}&=-\frac x2  \\ \end{align} Which cannot be solved although my math prof. said that since it is a linear PDE, each boundary condition can be solved independently with the other three Neumann BC set to zero and the final answer is a linear combination of the four solutions. Which I have tried to no avail. Any thoughts? Thank you","Hi fellow math enthusiasts, I am currently working on some research to do with the electric field induced within the brain via magnetic stimulation. I am trying to solve the partial differential equation in 2D cartesian co-ordinates \begin{equation} \nabla \cdot \underline{\sigma} \vec{\nabla}\phi = - \nabla \cdot  \left(\underline{\sigma} \frac{\partial \vec{A}}{\partial t}\right) \end{equation} For a magnetic vector potential of $\vec{A}=-\frac y2 \hat{i}+\frac x2\hat{j}$ and the magnetic vector potential can be separated so the function of time does not need to be known. From above this can be written as $$ \sigma_x\frac{\partial^2\phi}{\partial x^2}+\sigma_y\frac{\partial^2\phi}{\partial y^2}=0 $$ and after a change of variables $\gamma=\left(\frac{\sigma_x}{\sigma_y}\right)^{\frac12}y$ revels Laplace's equation $$ \frac{\partial^2\phi}{\partial x^2}+\frac{\partial^2\phi}{\partial \gamma^2}=0 $$ The surface to which Laplace's equation is to be solved in is the rectangle bounded by the lines $x=a,\ x=-a$ and $y=b,\ y=-b$ subject to Neumann boundary conditions  \begin{align} \left.\frac{\partial\phi}{\partial x}\right|_{x=a}&=\frac y2 \quad &\left.\frac{\partial\phi}{\partial x}\right|_{x=-a}&=\frac y2\\ \left.\frac{\partial\phi}{\partial y}\right|_{y=b}&=-\frac x2 \quad &\left.\frac{\partial\phi}{\partial y}\right|_{y=-b}&=-\frac x2  \\ \end{align} Which cannot be solved although my math prof. said that since it is a linear PDE, each boundary condition can be solved independently with the other three Neumann BC set to zero and the final answer is a linear combination of the four solutions. Which I have tried to no avail. Any thoughts? Thank you",,"['multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'boundary-value-problem', 'harmonic-functions']"
86,Definition for monotonicity of multivariate function,Definition for monotonicity of multivariate function,,"Is there any standard definition for monotonicity of a multivariate function? I suppose it's something like: $\forall i: x_i \leq x_i' \implies f(x_1, \ldots, x_i, \ldots, x_k) \leq f(x_1, \ldots, x_i', \ldots, x_k)$ thanks!","Is there any standard definition for monotonicity of a multivariate function? I suppose it's something like: $\forall i: x_i \leq x_i' \implies f(x_1, \ldots, x_i, \ldots, x_k) \leq f(x_1, \ldots, x_i', \ldots, x_k)$ thanks!",,['multivariable-calculus']
87,A stupid question on Stokes' theorem,A stupid question on Stokes' theorem,,"Suppose I want to do the simple integral  $I=\int_{ R^2} \frac{\mathrm{d}r\wedge\mathrm{d}\phi}{(1+r)^2}$. Just evaluating the integral one quickly gets $\left. -2\pi\frac{1}{1+r} \right |^\infty_0=2\pi$. However, since $ \frac{\mathrm{d}r\wedge\mathrm{d}\phi}{(1+r)^2} =\mathrm{d} \left(- \frac{\mathrm{d}\phi}{(1+r)} \right) $ I could also use Stokes' theorem. The form I am integrating is not compactly supported, but I can integrate over a 2-ball of radius $R$ and then take the limit $R \rightarrow \infty$: $I=\lim_{R \rightarrow \infty} \int_{B^2_R} \mathrm{d} \left(- \frac{\mathrm{d}\phi}{(1+r)} \right)= -\lim_{R \rightarrow \infty} \int_{S^1_R}  \frac{\mathrm{d}\phi}{(1+r)}  =-\lim_{R \rightarrow \infty} 2\pi  \frac{1}{(1+R)}=0 $ which is clearly wrong. So where is the mistake? The function $1/(1+r)$ is regular all the way from $0$ to $\infty$. I think that the problem is the fact that $\mathrm{d}\phi$ is not well defined at $r=0$, is that right? Would it be correct then to integrate over the domain $\{(x,y)\in R^2\;|\; \epsilon\leq x^2+y^2\leq R \}$, and then take both the limit $R\rightarrow \infty $ and $\epsilon\rightarrow 0$?","Suppose I want to do the simple integral  $I=\int_{ R^2} \frac{\mathrm{d}r\wedge\mathrm{d}\phi}{(1+r)^2}$. Just evaluating the integral one quickly gets $\left. -2\pi\frac{1}{1+r} \right |^\infty_0=2\pi$. However, since $ \frac{\mathrm{d}r\wedge\mathrm{d}\phi}{(1+r)^2} =\mathrm{d} \left(- \frac{\mathrm{d}\phi}{(1+r)} \right) $ I could also use Stokes' theorem. The form I am integrating is not compactly supported, but I can integrate over a 2-ball of radius $R$ and then take the limit $R \rightarrow \infty$: $I=\lim_{R \rightarrow \infty} \int_{B^2_R} \mathrm{d} \left(- \frac{\mathrm{d}\phi}{(1+r)} \right)= -\lim_{R \rightarrow \infty} \int_{S^1_R}  \frac{\mathrm{d}\phi}{(1+r)}  =-\lim_{R \rightarrow \infty} 2\pi  \frac{1}{(1+R)}=0 $ which is clearly wrong. So where is the mistake? The function $1/(1+r)$ is regular all the way from $0$ to $\infty$. I think that the problem is the fact that $\mathrm{d}\phi$ is not well defined at $r=0$, is that right? Would it be correct then to integrate over the domain $\{(x,y)\in R^2\;|\; \epsilon\leq x^2+y^2\leq R \}$, and then take both the limit $R\rightarrow \infty $ and $\epsilon\rightarrow 0$?",,"['multivariable-calculus', 'differential-geometry']"
88,Left-Invariant Vector Field of a Lie Group,Left-Invariant Vector Field of a Lie Group,,"How do I tell if a vector field on a Lie Group is left-invariant?  I have the technical definition.  But, I want to understand given a specific vector field what should I do to test if it is left-invariant?  For instance, here is a vector field in $\mathbb R^2$: $V(x, y)=y\partial/\partial x-x\partial/\partial y$ If this vector field (is/is not) left-invariant, can you provide me an example worked out as well of a vector field that (is not/is).","How do I tell if a vector field on a Lie Group is left-invariant?  I have the technical definition.  But, I want to understand given a specific vector field what should I do to test if it is left-invariant?  For instance, here is a vector field in $\mathbb R^2$: $V(x, y)=y\partial/\partial x-x\partial/\partial y$ If this vector field (is/is not) left-invariant, can you provide me an example worked out as well of a vector field that (is not/is).",,"['differential-geometry', 'multivariable-calculus', 'lie-groups']"
89,Double derivative of the composite of functions,Double derivative of the composite of functions,,"Consider $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ and $g:\mathbb{R}^m\rightarrow\mathbb{R}^k$. Then $(g\circ f):\mathbb{R}^n\rightarrow\mathbb{R}^k$ and, if both of them are differentiable, $[D(g\circ f)_p]=(Dg)_{f(p)}\cdot (Df)_p$. If these functions are two times differentiable, then $D(D(g\circ f))_p=(D^2g)_{f(p)}\cdot (Df)^2_p + (Dg)_{f(p)}\cdot (D^2 f)_p$. I'm trying to figure out what $(Df)^2_p$ means. Since it is a $m\times n$ matrix I cannot multiply them. Can someone help me?","Consider $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ and $g:\mathbb{R}^m\rightarrow\mathbb{R}^k$. Then $(g\circ f):\mathbb{R}^n\rightarrow\mathbb{R}^k$ and, if both of them are differentiable, $[D(g\circ f)_p]=(Dg)_{f(p)}\cdot (Df)_p$. If these functions are two times differentiable, then $D(D(g\circ f))_p=(D^2g)_{f(p)}\cdot (Df)^2_p + (Dg)_{f(p)}\cdot (D^2 f)_p$. I'm trying to figure out what $(Df)^2_p$ means. Since it is a $m\times n$ matrix I cannot multiply them. Can someone help me?",,['multivariable-calculus']
90,"Prove $\sqrt{\frac{a+4bc}{4a+bc}}+\sqrt{\frac{b+4ca}{4b+ac}}+\sqrt{\frac{c+4ab}{4c+ab}}\ge 3,$ when $a+b+c=3.$",Prove  when,"\sqrt{\frac{a+4bc}{4a+bc}}+\sqrt{\frac{b+4ca}{4b+ac}}+\sqrt{\frac{c+4ab}{4c+ab}}\ge 3, a+b+c=3.","Problem. If $a,b,c\ge 0: ab+bc+ca>0$ and $a+b+c=3,$ prove that $$\sqrt{\frac{a+4bc}{4a+bc}}+\sqrt{\frac{b+4ca}{4b+ac}}+\sqrt{\frac{c+4ab}{4c+ab}}\ge 3.$$ It was here. Equality holds at $a=b=c=1$ and $abc=0.$ I've tried to use AM-GM but the following inequality is not true $$\sqrt{\frac{a+4bc}{4a+bc}}\sqrt{\frac{b+4ca}{4b+ac}}\sqrt{\frac{c+4ab}{4c+ab}}\le 1.$$ I hope Isolated fudging method helps. Indeed, we'll prove $$\sum_{cyc}\sqrt{\frac{\dfrac{a(a+b+c)}{3}+4bc}{\dfrac{4a(a+b+c)}{3}+bc}}\ge 3.$$ Or $$\sum_{cyc}\sqrt{\frac{a^2+ab+12bc+ca}{4a^2+4ab+3bc+4ca}}\ge 3.$$ I failed to use it. Hope you give me a hint to kill this problem. Thank you.","Problem. If and prove that It was here. Equality holds at and I've tried to use AM-GM but the following inequality is not true I hope Isolated fudging method helps. Indeed, we'll prove Or I failed to use it. Hope you give me a hint to kill this problem. Thank you.","a,b,c\ge 0: ab+bc+ca>0 a+b+c=3, \sqrt{\frac{a+4bc}{4a+bc}}+\sqrt{\frac{b+4ca}{4b+ac}}+\sqrt{\frac{c+4ab}{4c+ab}}\ge 3. a=b=c=1 abc=0. \sqrt{\frac{a+4bc}{4a+bc}}\sqrt{\frac{b+4ca}{4b+ac}}\sqrt{\frac{c+4ab}{4c+ab}}\le 1. \sum_{cyc}\sqrt{\frac{\dfrac{a(a+b+c)}{3}+4bc}{\dfrac{4a(a+b+c)}{3}+bc}}\ge 3. \sum_{cyc}\sqrt{\frac{a^2+ab+12bc+ca}{4a^2+4ab+3bc+4ca}}\ge 3.","['multivariable-calculus', 'inequality', 'substitution', 'cauchy-schwarz-inequality', 'convexity-inequality']"
91,If $\vec{\omega}\cdot\vec{n}=0$ for $t=0$ then it holds for all time,If  for  then it holds for all time,\vec{\omega}\cdot\vec{n}=0 t=0,"I am trying to show that if $\vec{u}$ is a solution of the incompressible Euler equations with zero body force in a bounded domain $D$ , and if $\vec{\omega}.\vec{n}=0$ on the boundary of $D$ for $t=0$ then $\vec{\omega}.\vec{n}=0$ also holds for all time $t\geq0$ . I thought that if I can show $\frac{\omega}{t}=0$ then by time independence of $\omega$ the above is true. Taking the curl of the incompressible Euler equations with $\vec{f}=\vec{0}$ then gives me $\frac{\omega}{t}=\nabla\times(\vec{u}\times\vec{\omega})$ . (Of course after a few lines of vector identities/algebra). I'm not sure where to go from here.","I am trying to show that if is a solution of the incompressible Euler equations with zero body force in a bounded domain , and if on the boundary of for then also holds for all time . I thought that if I can show then by time independence of the above is true. Taking the curl of the incompressible Euler equations with then gives me . (Of course after a few lines of vector identities/algebra). I'm not sure where to go from here.",\vec{u} D \vec{\omega}.\vec{n}=0 D t=0 \vec{\omega}.\vec{n}=0 t\geq0 \frac{\omega}{t}=0 \omega \vec{f}=\vec{0} \frac{\omega}{t}=\nabla\times(\vec{u}\times\vec{\omega}),"['multivariable-calculus', 'vector-analysis', 'mathematical-physics', 'fluid-dynamics']"
92,What would be an example of Neumann boundary conditions on a two dimensional domain,What would be an example of Neumann boundary conditions on a two dimensional domain,,"The Wikipedia page says that it would involve the derivative with respect to some normal vector being constant, but I don't quite understand this. Is the value of the normal vector adjusted across the domain, or does it remain the same? Can I say that the standard partial derivative is the derivative with respect to the normal vector if I have a rectangular domain? Thank you for your help.","The Wikipedia page says that it would involve the derivative with respect to some normal vector being constant, but I don't quite understand this. Is the value of the normal vector adjusted across the domain, or does it remain the same? Can I say that the standard partial derivative is the derivative with respect to the normal vector if I have a rectangular domain? Thank you for your help.",,"['multivariable-calculus', 'partial-differential-equations', 'boundary-value-problem']"
93,What is a conservative field?,What is a conservative field?,,"My understanding of the conservative field is that it is any vector field that satisfies any of these three equivalent conditions: $$\oint_C\vec{F}.d\vec{s}=0$$for any closed path $C$ in the domain,$$\vec{F}=\vec{\nabla}\phi$$for some scalar field $\phi$ defined over the domain, and$$\vec{\nabla}\times\vec{F}=\vec{0}$$ at every point in the domain. However, our teacher told us today that a conservative field and a field derived from a potential are not the same thing. In my research on the issue I found this wolfram page that states that the last condition is not equivalent to the others if the domain $D$ is not simply connected. Can anyone provide me with an example on the case ? And in this case, what becomes the definition of a conservative field ?","My understanding of the conservative field is that it is any vector field that satisfies any of these three equivalent conditions: $$\oint_C\vec{F}.d\vec{s}=0$$for any closed path $C$ in the domain,$$\vec{F}=\vec{\nabla}\phi$$for some scalar field $\phi$ defined over the domain, and$$\vec{\nabla}\times\vec{F}=\vec{0}$$ at every point in the domain. However, our teacher told us today that a conservative field and a field derived from a potential are not the same thing. In my research on the issue I found this wolfram page that states that the last condition is not equivalent to the others if the domain $D$ is not simply connected. Can anyone provide me with an example on the case ? And in this case, what becomes the definition of a conservative field ?",,"['multivariable-calculus', 'vector-analysis']"
94,Jacobian with vanishing determinant everywhere,Jacobian with vanishing determinant everywhere,,"This is from advanced calculus. Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is continuously differentiable. The Jacobian of $f$ has zero determinant over $\mathbb{R}^n$. For example in 2-dimensional case, $f(x,y)=(y,y)$ is such a function. Let's suppose $n \ge 2$ since $n=1$ case gives a constant function. My question is :is it true that $f$ cannot be 1-1? My best attempt is that at each $x \in \mathbb{R}^n$ some directional derivative is zero, and perhaps an integral over some path will work, but I am not sure how to do it. Any idea is welcome. Thank you very much.","This is from advanced calculus. Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is continuously differentiable. The Jacobian of $f$ has zero determinant over $\mathbb{R}^n$. For example in 2-dimensional case, $f(x,y)=(y,y)$ is such a function. Let's suppose $n \ge 2$ since $n=1$ case gives a constant function. My question is :is it true that $f$ cannot be 1-1? My best attempt is that at each $x \in \mathbb{R}^n$ some directional derivative is zero, and perhaps an integral over some path will work, but I am not sure how to do it. Any idea is welcome. Thank you very much.",,"['multivariable-calculus', 'derivatives']"
95,Geometric intuition for directional derivatives,Geometric intuition for directional derivatives,,"What I'm trying to do in this post is to see that the intuition I've built is correct, and, if it's not, I would like someone to share its own intuition on why directional derivates are related with the gradient vector. My intuition: The formal definition of a directional derivative is: $$ \frac{\partial f}{\partial \vec{v}} =\nabla f(a,b) \cdot \vec{v} $$ where $\vec{v}$ is the vector that indicates the direction where we need to compute the rates of change. By the definition of partial derivatives, when we compute $\frac{\partial f}{\partial x}$ , we're fixing a plane in $y$ direction, and just analysing what a tiny change in $x$ effects our output. Same happens in $\frac{\partial f}{\partial y}$ , we fix a plane in $x$ direction, and analyse what a tiny change in $y$ effects our output. Now, when we compute a directional derivate of $\vec{v}$ , what we're doing (in my head) is fixing a plane, $\beta$ , that has $\vec{v}$ as one of it's directional vectors and intersects the surface. Just like the picture below: Because $\beta$ has $\vec{v}$ as one of it's directional vectors, what we're essentially doing is checking what a tiny change in direction of $\vec{v}$ causes to our output (surface). But we already know what a tiny change in $x$ causes and what a tiny change in $y$ causes, respectively, $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ ... Assuming that everything I've said is correct, we can decompose $\vec{v}$ in some linear combination of the basis vector of our space, in this case, the standard basis: $$ \vec{v} = \left[\begin{matrix} a\\ b\\ c\\ \end{matrix}\right] = a \cdot \left[\begin{matrix} 1\\ 0\\ 0\\ \end{matrix}\right] + b \cdot \left[\begin{matrix} 0\\ 1\\ 0\\ \end{matrix}\right] + c \cdot \left[\begin{matrix} 0\\ 0\\ 1\\ \end{matrix}\right] $$ Ignoring the third vector $\left[\begin{matrix} 0\\ 0\\ 1\\ \end{matrix}\right]$ because it deals with our output, we can see $\left[\begin{matrix} 1\\ 0\\ 0\\ \end{matrix}\right]$ and $\left[\begin{matrix} 0\\ 1\\ 0\\ \end{matrix}\right]$ as some change in $x$ and $y$ direction, that are computed by their partial derivatives, and we're looking on what a tiny change in $\vec{v}$ direction causes to our output, hence: $$ \frac{\partial f}{\partial \vec{v}} = a \cdot \frac{\partial f}{\partial x} + b \cdot \frac{\partial f}{\partial y}\\ \frac{\partial f}{\partial \vec{v}} = \nabla f(a,b) \cdot \vec{v} $$ Am I correct?","What I'm trying to do in this post is to see that the intuition I've built is correct, and, if it's not, I would like someone to share its own intuition on why directional derivates are related with the gradient vector. My intuition: The formal definition of a directional derivative is: where is the vector that indicates the direction where we need to compute the rates of change. By the definition of partial derivatives, when we compute , we're fixing a plane in direction, and just analysing what a tiny change in effects our output. Same happens in , we fix a plane in direction, and analyse what a tiny change in effects our output. Now, when we compute a directional derivate of , what we're doing (in my head) is fixing a plane, , that has as one of it's directional vectors and intersects the surface. Just like the picture below: Because has as one of it's directional vectors, what we're essentially doing is checking what a tiny change in direction of causes to our output (surface). But we already know what a tiny change in causes and what a tiny change in causes, respectively, and ... Assuming that everything I've said is correct, we can decompose in some linear combination of the basis vector of our space, in this case, the standard basis: Ignoring the third vector because it deals with our output, we can see and as some change in and direction, that are computed by their partial derivatives, and we're looking on what a tiny change in direction causes to our output, hence: Am I correct?","
\frac{\partial f}{\partial \vec{v}} =\nabla f(a,b) \cdot \vec{v}
 \vec{v} \frac{\partial f}{\partial x} y x \frac{\partial f}{\partial y} x y \vec{v} \beta \vec{v} \beta \vec{v} \vec{v} x y \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} \vec{v} 
\vec{v} = \left[\begin{matrix}
a\\
b\\
c\\
\end{matrix}\right] = a \cdot \left[\begin{matrix}
1\\
0\\
0\\
\end{matrix}\right] + b \cdot \left[\begin{matrix}
0\\
1\\
0\\
\end{matrix}\right] + c \cdot \left[\begin{matrix}
0\\
0\\
1\\
\end{matrix}\right]
 \left[\begin{matrix}
0\\
0\\
1\\
\end{matrix}\right] \left[\begin{matrix}
1\\
0\\
0\\
\end{matrix}\right] \left[\begin{matrix}
0\\
1\\
0\\
\end{matrix}\right] x y \vec{v} 
\frac{\partial f}{\partial \vec{v}} = a \cdot \frac{\partial f}{\partial x} + b \cdot \frac{\partial f}{\partial y}\\
\frac{\partial f}{\partial \vec{v}} = \nabla f(a,b) \cdot \vec{v}
","['multivariable-calculus', 'derivatives', 'partial-derivative']"
96,Area form for $M^2 \subseteq \Bbb R^4$,Area form for,M^2 \subseteq \Bbb R^4,"We know that in general, given a orientable hypersurface $M^{n-1} \subseteq \Bbb R^n$, the volume form on $M$ is given by $$dM = \sum_{i=1}^n(-1)^{i-1}n_i\,dx^1 \wedge\cdots\wedge \widehat{dx^i}\wedge \cdots \wedge dx^n,$$where $(n_1,\cdots,n_n)$ is the unit normal to $M$. Moreover, we have $$n_i\,dM = (-1)^{i-1}dx^1 \wedge\cdots\wedge \widehat{dx^i}\wedge \cdots \wedge dx^n$$for tangent vectors. Another general case is for bi-dimensional surfaces in $\Bbb R^n$, for which in coordinates we can write using $\sqrt{EG-F^2}$. But I wanted to write something like the first expression for surfaces $M^2 \subseteq \Bbb R^4$, specifically. There is no standard choice of orthonormal basis for the normal space as far as I know. Is there a way to get around this, at least in this case? If there is, I'd guess it would adapt for submanifolds of codimension $2$, but that would be the cherry on the cake.","We know that in general, given a orientable hypersurface $M^{n-1} \subseteq \Bbb R^n$, the volume form on $M$ is given by $$dM = \sum_{i=1}^n(-1)^{i-1}n_i\,dx^1 \wedge\cdots\wedge \widehat{dx^i}\wedge \cdots \wedge dx^n,$$where $(n_1,\cdots,n_n)$ is the unit normal to $M$. Moreover, we have $$n_i\,dM = (-1)^{i-1}dx^1 \wedge\cdots\wedge \widehat{dx^i}\wedge \cdots \wedge dx^n$$for tangent vectors. Another general case is for bi-dimensional surfaces in $\Bbb R^n$, for which in coordinates we can write using $\sqrt{EG-F^2}$. But I wanted to write something like the first expression for surfaces $M^2 \subseteq \Bbb R^4$, specifically. There is no standard choice of orthonormal basis for the normal space as far as I know. Is there a way to get around this, at least in this case? If there is, I'd guess it would adapt for submanifolds of codimension $2$, but that would be the cherry on the cake.",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
97,Find Surface Area Via a Line Integral (Stokes' Theorem),Find Surface Area Via a Line Integral (Stokes' Theorem),,"I am trying to use Stokes' Theorem to calculate the surface area of a parametrized surface via a line integral. The surface is the part of $z= x^2+y^2$ below the plane $z=5$. I know this can be done the usual way, without Stokes' Theorem , but there is another surface whose area I'm trying to calculate for which the usual way doesn't work well due to tough limits of integration. As stated in the question Un-Curl Operator , we can find the surface area via $\oint\vec{F}\cdot \vec{ds}$ over the curve of intersection of the paraboloid and plane, provided we can find a field $\vec{F}$ such that $\text{curl}(\vec{F})\cdot \vec{n}=1.$ This would mean that $$\text{curl}(\vec{F}) = \frac{\vec{n}}{{\Vert \vec{n} \Vert}^2},$$ so that, $$\text{curl}(\vec{F})\cdot \vec{n}=\left\Vert\frac{\vec{n}}{{\Vert \vec{n} \Vert}^2}\right\Vert\cdot\Vert \vec{n} \Vert=\frac{{\Vert \vec{n} \Vert}^2}{{\Vert \vec{n} \Vert}^2}=1,$$as required. But finding such a vector field has proven difficult. I first tried using the method described in the question Anti-Curl Operator , but realized that this wouldn't work because $\text{curl}(\vec{F})$ has a nonzero divergence. I then turned to the method described in an answer to the Un-Curl Operator question, known as the Helmholtz decomposition. The idea is that given our vector field, $\text{curl}(\vec{F})$ in this case, we can split it into a curl-free part and a divergence-free part: $$\text{curl}(\mathbf{F})=-\nabla\Phi+\nabla\times \mathbf{A}$$ I believe ( I could be wrong though ) that it remains only to find $\mathbf{A}$, since the first term will not contribute to the line integral around a closed path. However, this is where my confusion begins. According to Wikipedia , $\mathbf{A}$ is found as follows: $$ \mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_V\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left\vert\mathbf{r}-\mathbf{r}'\right\vert}\text{d}V' - \frac{1}{4\pi}\oint_S\mathbf{\hat{n}}'\times\frac{\mathbf{F}(\mathbf{r}')}{\left\vert\mathbf{r}-\mathbf{r}'\right\vert}\text{d}S'$$ I do not know how to use this formula. Specifically, I do not know what most of the symbols represent in the context of this problem. What is the difference between $\mathbf{r}$ and $\mathbf{r}'$? What is the region of integration? Also, the Wikipedia page mentions that the second term can be dropped if the field satisfies certain conditions. Does that apply here? I am also wondering if there is anything else I need to do to specify that the dot product of $\text{curl}(\mathbf{F})$ and $\mathbf{n}$ is $1$. The answer on the Un-Curl Operator question used the following notation: $$\mathbf{G}\vert_{\partial\Omega}\cdot\mathbf{n}=1,$$ where $\mathbf{G}$ = $\text{curl}(\mathbf{F})$. I'm pretty sure that $\partial\Omega$ represents the boundary of a domain $\Omega$, but what is $\Omega$ in the context of this problem? Am I missing something? How would we use the Helmholtz decomposition to find the vector field $\mathbf{A}$ in this problem?","I am trying to use Stokes' Theorem to calculate the surface area of a parametrized surface via a line integral. The surface is the part of $z= x^2+y^2$ below the plane $z=5$. I know this can be done the usual way, without Stokes' Theorem , but there is another surface whose area I'm trying to calculate for which the usual way doesn't work well due to tough limits of integration. As stated in the question Un-Curl Operator , we can find the surface area via $\oint\vec{F}\cdot \vec{ds}$ over the curve of intersection of the paraboloid and plane, provided we can find a field $\vec{F}$ such that $\text{curl}(\vec{F})\cdot \vec{n}=1.$ This would mean that $$\text{curl}(\vec{F}) = \frac{\vec{n}}{{\Vert \vec{n} \Vert}^2},$$ so that, $$\text{curl}(\vec{F})\cdot \vec{n}=\left\Vert\frac{\vec{n}}{{\Vert \vec{n} \Vert}^2}\right\Vert\cdot\Vert \vec{n} \Vert=\frac{{\Vert \vec{n} \Vert}^2}{{\Vert \vec{n} \Vert}^2}=1,$$as required. But finding such a vector field has proven difficult. I first tried using the method described in the question Anti-Curl Operator , but realized that this wouldn't work because $\text{curl}(\vec{F})$ has a nonzero divergence. I then turned to the method described in an answer to the Un-Curl Operator question, known as the Helmholtz decomposition. The idea is that given our vector field, $\text{curl}(\vec{F})$ in this case, we can split it into a curl-free part and a divergence-free part: $$\text{curl}(\mathbf{F})=-\nabla\Phi+\nabla\times \mathbf{A}$$ I believe ( I could be wrong though ) that it remains only to find $\mathbf{A}$, since the first term will not contribute to the line integral around a closed path. However, this is where my confusion begins. According to Wikipedia , $\mathbf{A}$ is found as follows: $$ \mathbf{A}(\mathbf{r})=\frac{1}{4\pi}\int_V\frac{\nabla'\times\mathbf{F}(\mathbf{r}')}{\left\vert\mathbf{r}-\mathbf{r}'\right\vert}\text{d}V' - \frac{1}{4\pi}\oint_S\mathbf{\hat{n}}'\times\frac{\mathbf{F}(\mathbf{r}')}{\left\vert\mathbf{r}-\mathbf{r}'\right\vert}\text{d}S'$$ I do not know how to use this formula. Specifically, I do not know what most of the symbols represent in the context of this problem. What is the difference between $\mathbf{r}$ and $\mathbf{r}'$? What is the region of integration? Also, the Wikipedia page mentions that the second term can be dropped if the field satisfies certain conditions. Does that apply here? I am also wondering if there is anything else I need to do to specify that the dot product of $\text{curl}(\mathbf{F})$ and $\mathbf{n}$ is $1$. The answer on the Un-Curl Operator question used the following notation: $$\mathbf{G}\vert_{\partial\Omega}\cdot\mathbf{n}=1,$$ where $\mathbf{G}$ = $\text{curl}(\mathbf{F})$. I'm pretty sure that $\partial\Omega$ represents the boundary of a domain $\Omega$, but what is $\Omega$ in the context of this problem? Am I missing something? How would we use the Helmholtz decomposition to find the vector field $\mathbf{A}$ in this problem?",,"['multivariable-calculus', 'line-integrals', 'stokes-theorem']"
98,How to calculate the area of a region with a closed plane curve boundary?,How to calculate the area of a region with a closed plane curve boundary?,,"Under the conditions of Greens Theorem, the area of a region $R$ enclosed by a curve $C$ is $$\oint_C x \, dy=-\oint_C y \, dx=\frac{1}{2}\oint_C (x \, dy - y \, dx)$$ I tried to use the result to calculate the area of region defined by the following plane curve: $$ \begin{cases}  x &= -9 \sin (2 t)-5 \sin (3 t) \\[6pt]  y & = 9 \cos (2 t)-5 \cos (3 t) \end{cases} $$ but obtained $87\pi$, which does not seem to be correct from the following simple comparison (the circle has a radius of $\sqrt{87}$). How should I use Green's theorem in this case? What would be a more convenient way to calculate the area enclosed by such a plane curve?","Under the conditions of Greens Theorem, the area of a region $R$ enclosed by a curve $C$ is $$\oint_C x \, dy=-\oint_C y \, dx=\frac{1}{2}\oint_C (x \, dy - y \, dx)$$ I tried to use the result to calculate the area of region defined by the following plane curve: $$ \begin{cases}  x &= -9 \sin (2 t)-5 \sin (3 t) \\[6pt]  y & = 9 \cos (2 t)-5 \cos (3 t) \end{cases} $$ but obtained $87\pi$, which does not seem to be correct from the following simple comparison (the circle has a radius of $\sqrt{87}$). How should I use Green's theorem in this case? What would be a more convenient way to calculate the area enclosed by such a plane curve?",,"['multivariable-calculus', 'area', 'plane-curves', 'greens-theorem']"
99,"Directional, differential and lie derivatives on manifolds intuition?","Directional, differential and lie derivatives on manifolds intuition?",,"Trying to translate elementary multivariable calculus into the language of manifolds: Is the directional derivative on a manifold just a way of finding the rate of change of a vector in a single direction, in a fixed basis, where the vector is expressed in terms of linear combinations of basis vectors (i.e. to find the rate of change of a vector in the tangent space moving along one of the directions the basis points out)? Is the differential then just a way of finding the rate of change of that same vector in a single direction, where the vector is just expressed in terms of it's coordinates in an invariant fashion (i.e. to find the rate of change of an equivalent vector in the cotangent space that is dual to the original vector in one direction directed out by the basis)? If I want to take a second derivative in another direction given by a fixed basis, am I forced to define the lie derivative of a one-form? i.e. is the lie derivative just a fancy way of taking second derivatives of scalar-valued functions in a single direction (while also interpretable as first, second, ... derivatives of vector-valued functions)? In terms of vector-valued functions of vector fields, I've never understood why the second derivative naturally ends up with us having to define a bilinear form, intuitively why does this necessarily arise in taking the second derivative of something like $\vec{F}(x,y) = (x^2+y^2,2xy)$? How does this intuitive example translate into the language of lie derivatives of vector fields? Is the covariant derivative just a way to do all of the above in an arbitrary basis, i.e. in a random direction no matter what basis we're given? What does the commutator actually do, thinking along these lines?","Trying to translate elementary multivariable calculus into the language of manifolds: Is the directional derivative on a manifold just a way of finding the rate of change of a vector in a single direction, in a fixed basis, where the vector is expressed in terms of linear combinations of basis vectors (i.e. to find the rate of change of a vector in the tangent space moving along one of the directions the basis points out)? Is the differential then just a way of finding the rate of change of that same vector in a single direction, where the vector is just expressed in terms of it's coordinates in an invariant fashion (i.e. to find the rate of change of an equivalent vector in the cotangent space that is dual to the original vector in one direction directed out by the basis)? If I want to take a second derivative in another direction given by a fixed basis, am I forced to define the lie derivative of a one-form? i.e. is the lie derivative just a fancy way of taking second derivatives of scalar-valued functions in a single direction (while also interpretable as first, second, ... derivatives of vector-valued functions)? In terms of vector-valued functions of vector fields, I've never understood why the second derivative naturally ends up with us having to define a bilinear form, intuitively why does this necessarily arise in taking the second derivative of something like $\vec{F}(x,y) = (x^2+y^2,2xy)$? How does this intuitive example translate into the language of lie derivatives of vector fields? Is the covariant derivative just a way to do all of the above in an arbitrary basis, i.e. in a random direction no matter what basis we're given? What does the commutator actually do, thinking along these lines?",,"['multivariable-calculus', 'manifolds']"
