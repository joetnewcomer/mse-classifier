,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$f(x)$ is convex $\Leftrightarrow f'$ is monotonically increasing,is convex  is monotonically increasing,f(x) \Leftrightarrow f',"How can I prove that $f(x)$ is convex on an interval if and only if $f'(x)$ is monotonically increasing ? • Let $\lambda \in (0,1)$ and $x,y \in I$. $f(x)$ is convex on an interval, if the statement  $f((1-\lambda)x + \lambda y) \leq (1-\lambda)f(x) + \lambda f(y) $ is true for all $x,y$. • $f'(x)$ is monotonically increasing on an interval $I$, if $0 \leq \frac{d f'(x)}{dx}$ is true for all $x \in I$. I don't have a problem understand this, with a graph it's quite easy to understand. But I can't manage to prove it formally, can someone give me a hint? Note that I do have seen this question , but since I can't use inequality proven previously it doesn't help me.","How can I prove that $f(x)$ is convex on an interval if and only if $f'(x)$ is monotonically increasing ? • Let $\lambda \in (0,1)$ and $x,y \in I$. $f(x)$ is convex on an interval, if the statement  $f((1-\lambda)x + \lambda y) \leq (1-\lambda)f(x) + \lambda f(y) $ is true for all $x,y$. • $f'(x)$ is monotonically increasing on an interval $I$, if $0 \leq \frac{d f'(x)}{dx}$ is true for all $x \in I$. I don't have a problem understand this, with a graph it's quite easy to understand. But I can't manage to prove it formally, can someone give me a hint? Note that I do have seen this question , but since I can't use inequality proven previously it doesn't help me.",,"['real-analysis', 'analysis']"
1,How to prove that the limsup of a sequence is equal to its greatest subsequential limit?,How to prove that the limsup of a sequence is equal to its greatest subsequential limit?,,"I have a very tricky problem that I'm having a hard time figuring out how to start.  Basically, I want to prove that the supremum of the set of subsequential limits of a sequence is equal to the lim sup of the sequence. So I have a sequence $S_n$. I want to show that its greatest subsequential limit (which could either be a real number, infinity, or negative infinity) is equal to the limit (as N goes to infinity) of the supremum of the set $X=\{S_n:n>N\}$,  $$\lim_{N\to\infty} \sup \{S_n:n>N\}$$ which is the definition of limit superior.  I'm having a hard time coming up with a way to go about this.","I have a very tricky problem that I'm having a hard time figuring out how to start.  Basically, I want to prove that the supremum of the set of subsequential limits of a sequence is equal to the lim sup of the sequence. So I have a sequence $S_n$. I want to show that its greatest subsequential limit (which could either be a real number, infinity, or negative infinity) is equal to the limit (as N goes to infinity) of the supremum of the set $X=\{S_n:n>N\}$,  $$\lim_{N\to\infty} \sup \{S_n:n>N\}$$ which is the definition of limit superior.  I'm having a hard time coming up with a way to go about this.",,"['real-analysis', 'sequences-and-series', 'analysis', 'limsup-and-liminf']"
2,Why is a norm a continuous function? (Question about existing proof),Why is a norm a continuous function? (Question about existing proof),,"I'm trying to follow the proof given in this answer: https://math.stackexchange.com/a/265595/188401 I understand the proof in general, but I have a question. It's mentioned that ""In this case it suffices to take δ=ε, for the following reason"", but I don't see a reason provided. So why does this suffice? What happens if delta is bigger than epsilon? Note that I don't have enough reputation to post a comment in the existing thread, hence the new question.","I'm trying to follow the proof given in this answer: https://math.stackexchange.com/a/265595/188401 I understand the proof in general, but I have a question. It's mentioned that ""In this case it suffices to take δ=ε, for the following reason"", but I don't see a reason provided. So why does this suffice? What happens if delta is bigger than epsilon? Note that I don't have enough reputation to post a comment in the existing thread, hence the new question.",,"['real-analysis', 'analysis', 'continuity']"
3,Does $\sum_{n\ge0} \sin (\pi \sqrt{n^2+n+1}) $ converge/diverge?,Does  converge/diverge?,\sum_{n\ge0} \sin (\pi \sqrt{n^2+n+1}) ,"How would you prove convergence/divergence of the following series? $$\sum_{n\ge0} \sin (\pi \sqrt{n^2+n+1}) $$ I'm interested in more ways of proving convergence/divergence for this series. My thoughts Let $$u_{n}= \sin (\pi \sqrt{n^2+n+1})$$ trying to bound $$|u_n|\leq |\sin(\pi(n+1) )| $$ since $n^2+n+1\leq n^2+2n+1$ and $\sin$ is decreasing in $(0,\dfrac{\pi}{2} )$ $$\sum_{n\ge0}|u_n|\leq \sum_{n\ge0}|\sin(\pi(n+1) )|$$ or $|\sin(\pi(n+1) )|=0\quad  \forall n\in \mathbb{N}$ then $\sum_{n\ge0}|\sin(\pi(n+1) )|=0$ thus $\sum_{n\ge0} u_n$ is converge absolutely then is converget any help would be appreciated","How would you prove convergence/divergence of the following series? $$\sum_{n\ge0} \sin (\pi \sqrt{n^2+n+1}) $$ I'm interested in more ways of proving convergence/divergence for this series. My thoughts Let $$u_{n}= \sin (\pi \sqrt{n^2+n+1})$$ trying to bound $$|u_n|\leq |\sin(\pi(n+1) )| $$ since $n^2+n+1\leq n^2+2n+1$ and $\sin$ is decreasing in $(0,\dfrac{\pi}{2} )$ $$\sum_{n\ge0}|u_n|\leq \sum_{n\ge0}|\sin(\pi(n+1) )|$$ or $|\sin(\pi(n+1) )|=0\quad  \forall n\in \mathbb{N}$ then $\sum_{n\ge0}|\sin(\pi(n+1) )|=0$ thus $\sum_{n\ge0} u_n$ is converge absolutely then is converget any help would be appreciated",,"['calculus', 'real-analysis', 'sequences-and-series', 'trigonometry', 'convergence-divergence']"
4,"Erwin Kreyszig's Introductory Functional Analysis With Applications, Section 2.7, Problem 9","Erwin Kreyszig's Introductory Functional Analysis With Applications, Section 2.7, Problem 9",,"Here is Problem 9 in the Problem Set following Section 2.7 in the book Introductory Functional Analysis With Applications by Erwine Kryszeg: Let $C[0,1]$ denote the set of all (real- or complex-valued) functions defined and continuous on the closed interval $[0,1]$ with the norm defined as follows:  $$\Vert x \Vert_{C[0,1]} \colon= \max_{t \in [0,1]} \vert x(t) \vert \, \, \,  \forall x \in C[0,1]. $$ Let $T \colon C[0,1] \to C[0,1]$ be defined by  $$Tx \colon= \int_{0}^t x(\tau) \, d \tau \, \, \,  \forall x \in C[0,1].$$ What is the range of $T$? Is $T$ injective? And if so, then what is the inverse $T^{-1}$? Is $T^{-1}$  bounded? I know that $T$ is bounded, for, given any $x \in C[0,1]$, we have  $$ \Vert Tx \Vert_{C[0,1]} = \max_{t\in[0,1]} \vert \int_0^t x(\tau) \, d \tau \vert \leq \max_{t\in[0,1]} (\int_0^t \vert x(\tau) \vert \, d \tau ) = \int_0^1 \vert x(\tau) \vert \, d \tau  \leq \int_0^1 \max_{\tau\in[0,1]} \vert x(\tau) \vert \, d \tau = \int_0^1 \Vert x \Vert_{C[0,1]} \, d \tau = \Vert x \Vert_{C[0,1]}.$$","Here is Problem 9 in the Problem Set following Section 2.7 in the book Introductory Functional Analysis With Applications by Erwine Kryszeg: Let $C[0,1]$ denote the set of all (real- or complex-valued) functions defined and continuous on the closed interval $[0,1]$ with the norm defined as follows:  $$\Vert x \Vert_{C[0,1]} \colon= \max_{t \in [0,1]} \vert x(t) \vert \, \, \,  \forall x \in C[0,1]. $$ Let $T \colon C[0,1] \to C[0,1]$ be defined by  $$Tx \colon= \int_{0}^t x(\tau) \, d \tau \, \, \,  \forall x \in C[0,1].$$ What is the range of $T$? Is $T$ injective? And if so, then what is the inverse $T^{-1}$? Is $T^{-1}$  bounded? I know that $T$ is bounded, for, given any $x \in C[0,1]$, we have  $$ \Vert Tx \Vert_{C[0,1]} = \max_{t\in[0,1]} \vert \int_0^t x(\tau) \, d \tau \vert \leq \max_{t\in[0,1]} (\int_0^t \vert x(\tau) \vert \, d \tau ) = \int_0^1 \vert x(\tau) \vert \, d \tau  \leq \int_0^1 \max_{\tau\in[0,1]} \vert x(\tau) \vert \, d \tau = \int_0^1 \Vert x \Vert_{C[0,1]} \, d \tau = \Vert x \Vert_{C[0,1]}.$$",,"['real-analysis', 'functional-analysis', 'operator-theory', 'normed-spaces']"
5,Limit of $n(a^{1/n}-1)$ as $n \to \infty$,Limit of  as,n(a^{1/n}-1) n \to \infty,"Show that $\lim(n(a^{1/n}-1)) = \ln(a)$ In the context of sequences, I'm not sure how to prove that this is the limit of the sequence. I was trying to convert the expression as follows: $n(a^{1/n}-1) = \ln(a)\displaystyle\frac{n(e^{\ln(a)/n}-1)}{\ln(a)}$ Then, we can set $\displaystyle\frac{\ln(a)}{n} = 1/x$ to then have: $\lim(n(a^{1/n}-1)) = \ln(a)\lim({x(e^{x}-1)})=\ln(a)$ Using the result that $\lim(n(e^{n}-1)) = 1$ (both $n$ and $x$ will tend to $\infty$). However, I'm not satisfied with this answer, as I don't think that it is possible to simply set $\displaystyle\frac{\ln(a)}{n} = x$, as then it would be a sequence of in which $x \in \mathbb{R}$, not $x \in \mathbb{N}$ as it should be for a sequence. Is there any other way to solve the problem? Thank you for your time and patience.","Show that $\lim(n(a^{1/n}-1)) = \ln(a)$ In the context of sequences, I'm not sure how to prove that this is the limit of the sequence. I was trying to convert the expression as follows: $n(a^{1/n}-1) = \ln(a)\displaystyle\frac{n(e^{\ln(a)/n}-1)}{\ln(a)}$ Then, we can set $\displaystyle\frac{\ln(a)}{n} = 1/x$ to then have: $\lim(n(a^{1/n}-1)) = \ln(a)\lim({x(e^{x}-1)})=\ln(a)$ Using the result that $\lim(n(e^{n}-1)) = 1$ (both $n$ and $x$ will tend to $\infty$). However, I'm not satisfied with this answer, as I don't think that it is possible to simply set $\displaystyle\frac{\ln(a)}{n} = x$, as then it would be a sequence of in which $x \in \mathbb{R}$, not $x \in \mathbb{N}$ as it should be for a sequence. Is there any other way to solve the problem? Thank you for your time and patience.",,"['real-analysis', 'sequences-and-series', 'limits']"
6,Prove that $2 \le \int_0^1 \ \frac{{(1+x)^{1+x}}}{x^x} \ dx \le 3$,Prove that,2 \le \int_0^1 \ \frac{{(1+x)^{1+x}}}{x^x} \ dx \le 3,"I need some starting ideas, hints for proving that $$2 \le \int_0^1 \ \frac{{(1+x)^{1+x}}}{x^x} \ dx \le 3$$ I already checked that with Mathematica that numerically says that $$\int_0^1 \ \frac{{(1+x)^{1+x}}}{x^x} \ dx \approx 2.577632915067858 $$","I need some starting ideas, hints for proving that $$2 \le \int_0^1 \ \frac{{(1+x)^{1+x}}}{x^x} \ dx \le 3$$ I already checked that with Mathematica that numerically says that $$\int_0^1 \ \frac{{(1+x)^{1+x}}}{x^x} \ dx \approx 2.577632915067858 $$",,"['calculus', 'real-analysis', 'inequality', 'definite-integrals', 'integral-inequality']"
7,"Determining the best possible constant $k$, for an Integral Inequality","Determining the best possible constant , for an Integral Inequality",k,"If $f : [0,\infty) \to [0,\infty)$ is an integrable function, then what is the best possible constant $k$, for which the following ineqality holds: $$\int_0^{\infty}f(x)dx \leq k\left(\int_0^{\infty}\sqrt{x}f(x)dx\right)^{1/2}\cdot\left(\int_0^{\infty}f^{2}(x)dx\right)^{1/4}$$ For example, $k=2$ is a bound, since, $$\int_0^{\infty}f(x)dx = \int_0^{y}f(x)dx + \int_y^{\infty}f(x)dx < \sqrt{y}\left(\int_0^{\infty}f^2(x)dx\right)^{1/2} + \frac{1}{\sqrt{y}}\int_0^{\infty}\sqrt{x}f(x)dx$$ and setting, $ y = \dfrac{\displaystyle \int_0^{\infty}\sqrt{x}f(x)dx}{\left(\displaystyle\int_0^{\infty} f^2(x)dx\right)^{1/2}}$, establishes the inequality for the case $k=2$, but it is not strict. How to improve the bound $k$ and also determine the function where equality holds for the best constant $k$ ? Thank you.","If $f : [0,\infty) \to [0,\infty)$ is an integrable function, then what is the best possible constant $k$, for which the following ineqality holds: $$\int_0^{\infty}f(x)dx \leq k\left(\int_0^{\infty}\sqrt{x}f(x)dx\right)^{1/2}\cdot\left(\int_0^{\infty}f^{2}(x)dx\right)^{1/4}$$ For example, $k=2$ is a bound, since, $$\int_0^{\infty}f(x)dx = \int_0^{y}f(x)dx + \int_y^{\infty}f(x)dx < \sqrt{y}\left(\int_0^{\infty}f^2(x)dx\right)^{1/2} + \frac{1}{\sqrt{y}}\int_0^{\infty}\sqrt{x}f(x)dx$$ and setting, $ y = \dfrac{\displaystyle \int_0^{\infty}\sqrt{x}f(x)dx}{\left(\displaystyle\int_0^{\infty} f^2(x)dx\right)^{1/2}}$, establishes the inequality for the case $k=2$, but it is not strict. How to improve the bound $k$ and also determine the function where equality holds for the best constant $k$ ? Thank you.",,"['real-analysis', 'inequality', 'indefinite-integrals', 'integral-inequality']"
8,Failure of differential notation,Failure of differential notation,,"Through the informal use of differentials, the product rule can be ""proved"" by writing $$d(fg) = (f + df)(g + dg) - fg = df\,g + f\,dg + df\,dg.$$ Neglecting the product of two differentials, we conclude that  $$d(fg) = df\,g + f\,dg.$$ However, the accepted answer to this question mentions that manipulations like this are not always justified. In particular, he points out that it is unclear why we should not neglect a single differential (itself an ""infinitesimal"" quantity), but we should neglect their product (presumably since it's ""infintesimal-er""). Can someone produce an example in which a line of reasoning similar to the above argument for the product rule leads to a false conclusion (preferably from single-variable calculus)? Another way to phrase the question is this: What failures of the informal use of differentials led to the development of non-standard analysis?","Through the informal use of differentials, the product rule can be ""proved"" by writing $$d(fg) = (f + df)(g + dg) - fg = df\,g + f\,dg + df\,dg.$$ Neglecting the product of two differentials, we conclude that  $$d(fg) = df\,g + f\,dg.$$ However, the accepted answer to this question mentions that manipulations like this are not always justified. In particular, he points out that it is unclear why we should not neglect a single differential (itself an ""infinitesimal"" quantity), but we should neglect their product (presumably since it's ""infintesimal-er""). Can someone produce an example in which a line of reasoning similar to the above argument for the product rule leads to a false conclusion (preferably from single-variable calculus)? Another way to phrase the question is this: What failures of the informal use of differentials led to the development of non-standard analysis?",,"['calculus', 'real-analysis', 'derivatives']"
9,Real-analytic periodic $f(z)$ that has more than 50 % of the derivatives positive?,Real-analytic periodic  that has more than 50 % of the derivatives positive?,f(z),"Im looking for a real-analytic function $f(z)$ such that for any $z$ $1) $$f(z+p) =f(z)$ With $p$ a nonzero real number and where $z$ is close to , or onto the real line such that  $z$ is in the domain of analyticity. $2)$ $f(z)= 0 + a_1 z + a_2 z^2 + a_3 z^3 + ...$ where more than $50$ % of the nonzero (signs of the) $a_n$ are positive. Thus let $f_n(z)$ be the truncated Taylor expansion of $f(z)$ of degree $n$. Let $T(n)$ be the amount of nonzero (signs in the) coefficients of the polynomial $f_n(z)$. Let $v(n)$ be the amount of strict positive ($>0$) coefficients of $f_n(z)$. Then $\lim_{n ->  +\infty} v(n)/T(n) > 1/2$. $3)$ $f(z)$ is nonconstant. Also I prefer $f(z)$ to be entire if possible. Is such a function $f(z)$ possible ? Related : Real-analytic $f(z)=f(\sqrt z) + f(-\sqrt z)$?","Im looking for a real-analytic function $f(z)$ such that for any $z$ $1) $$f(z+p) =f(z)$ With $p$ a nonzero real number and where $z$ is close to , or onto the real line such that  $z$ is in the domain of analyticity. $2)$ $f(z)= 0 + a_1 z + a_2 z^2 + a_3 z^3 + ...$ where more than $50$ % of the nonzero (signs of the) $a_n$ are positive. Thus let $f_n(z)$ be the truncated Taylor expansion of $f(z)$ of degree $n$. Let $T(n)$ be the amount of nonzero (signs in the) coefficients of the polynomial $f_n(z)$. Let $v(n)$ be the amount of strict positive ($>0$) coefficients of $f_n(z)$. Then $\lim_{n ->  +\infty} v(n)/T(n) > 1/2$. $3)$ $f(z)$ is nonconstant. Also I prefer $f(z)$ to be entire if possible. Is such a function $f(z)$ possible ? Related : Real-analytic $f(z)=f(\sqrt z) + f(-\sqrt z)$?",,"['calculus', 'real-analysis', 'taylor-expansion', 'periodic-functions']"
10,Sum Involving Bernoulli Numbers : $\sum_{r=1}^n \binom{2n}{2r-1}\frac{B_{2r}}{r}=\frac{2n-1}{2n+1}$,Sum Involving Bernoulli Numbers :,\sum_{r=1}^n \binom{2n}{2r-1}\frac{B_{2r}}{r}=\frac{2n-1}{2n+1},How can we prove that $$\sum_{r=1}^n \binom{2n}{2r-1}\frac{B_{2r}}{r}=\frac{2n-1}{2n+1}$$ where $B_{2r}$ are the Bernoulli numbers ? $$\begin{array}{c|c|c|} n & \frac{2n-1}{2n+1} & \sum_{r=1}^n \binom{2n}{2r-1} \frac{B_{2r}}{r} \\ \hline 1 &\frac{1}{3} & \frac{1}{3} \\ 2 &\frac{3}{5} & \frac{3}{5} \\ 3 &\frac{5}{7} &\frac{5}{7} \\ 4 & \frac{7}{9} & \frac{7}{9} \\ 5 & \frac{9}{11} &\frac{9}{11}\end{array}$$ The formula appears to be correct for a lot of values of $n$.,How can we prove that $$\sum_{r=1}^n \binom{2n}{2r-1}\frac{B_{2r}}{r}=\frac{2n-1}{2n+1}$$ where $B_{2r}$ are the Bernoulli numbers ? $$\begin{array}{c|c|c|} n & \frac{2n-1}{2n+1} & \sum_{r=1}^n \binom{2n}{2r-1} \frac{B_{2r}}{r} \\ \hline 1 &\frac{1}{3} & \frac{1}{3} \\ 2 &\frac{3}{5} & \frac{3}{5} \\ 3 &\frac{5}{7} &\frac{5}{7} \\ 4 & \frac{7}{9} & \frac{7}{9} \\ 5 & \frac{9}{11} &\frac{9}{11}\end{array}$$ The formula appears to be correct for a lot of values of $n$.,,"['real-analysis', 'summation', 'bernoulli-numbers']"
11,Eigenvalues of a symmetric matrix with Lagrange multipliers,Eigenvalues of a symmetric matrix with Lagrange multipliers,,"Problem: Using Lagrange multipliers, prove that all symmetric matrices $A \in \mathbb{R}^{n \times n}$ have all real eigenvalues. Proof: Consider $f: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by $f(x) = \langle Ax,x \rangle$, where $\langle \cdot,\cdot \rangle$ is the usual intern product of $\mathbb{R}^n$ and $S^{n-1} = \{ x \in \mathbb{R}^n : \| x \| = 1 \}$. I have found that $f'(x) = 2\langle Ax,h \rangle$, but I'm stuck here. I appreciate all your comments. Thanks!!!","Problem: Using Lagrange multipliers, prove that all symmetric matrices $A \in \mathbb{R}^{n \times n}$ have all real eigenvalues. Proof: Consider $f: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by $f(x) = \langle Ax,x \rangle$, where $\langle \cdot,\cdot \rangle$ is the usual intern product of $\mathbb{R}^n$ and $S^{n-1} = \{ x \in \mathbb{R}^n : \| x \| = 1 \}$. I have found that $f'(x) = 2\langle Ax,h \rangle$, but I'm stuck here. I appreciate all your comments. Thanks!!!",,"['real-analysis', 'linear-algebra', 'matrices', 'lagrange-multiplier']"
12,"A bounded subset in $\mathbb R^2$ which is ""nowhere convex""?","A bounded subset in  which is ""nowhere convex""?",\mathbb R^2,"Let $F : \mathbb S^1 \to \mathbb R^2$ represents a simple closed curve $C$ in $\mathbb R^2$. The Jordan curve theorem says that the curves bounds a interior domain $\Omega$ and $\partial \Omega= C$. Do we always have two points $a, b\in C$ such that the line segment joining $a$ and $b$ lies completely in $\Omega$? What's more, can we have ""For all $\epsilon>0$, there is two points $a ,b\in C$ such that $|a- b| < \epsilon$ and the line segment joining $a$ and $b$ lies completely in $\Omega$"". It seems that it is related to how the $C$ oscillate. If $C$ is some kinds of fractal curves, it might happens that $C$ is nowhere convex. What if we assume that $F$ is of bounded variation? This question is related to the following question :","Let $F : \mathbb S^1 \to \mathbb R^2$ represents a simple closed curve $C$ in $\mathbb R^2$. The Jordan curve theorem says that the curves bounds a interior domain $\Omega$ and $\partial \Omega= C$. Do we always have two points $a, b\in C$ such that the line segment joining $a$ and $b$ lies completely in $\Omega$? What's more, can we have ""For all $\epsilon>0$, there is two points $a ,b\in C$ such that $|a- b| < \epsilon$ and the line segment joining $a$ and $b$ lies completely in $\Omega$"". It seems that it is related to how the $C$ oscillate. If $C$ is some kinds of fractal curves, it might happens that $C$ is nowhere convex. What if we assume that $F$ is of bounded variation? This question is related to the following question :",,['real-analysis']
13,"Show that,$\int_0^\pi \left|\frac{\sin nx}{x}\right|\mathrm{d}x \ge \frac{2}{\pi}\left(1+\frac12+\cdots+\frac{1}{n}\right)$","Show that,",\int_0^\pi \left|\frac{\sin nx}{x}\right|\mathrm{d}x \ge \frac{2}{\pi}\left(1+\frac12+\cdots+\frac{1}{n}\right),"Show that,$$\int_0^\pi \bigg|\dfrac{\sin nx}{x}\bigg|\mathrm{d}x \ge \dfrac{2}{\pi}\bigg(1+\dfrac12+\cdots+\dfrac{1}{n}\bigg)$$ I could not approach the problem at all. Please help.","Show that,$$\int_0^\pi \bigg|\dfrac{\sin nx}{x}\bigg|\mathrm{d}x \ge \dfrac{2}{\pi}\bigg(1+\dfrac12+\cdots+\dfrac{1}{n}\bigg)$$ I could not approach the problem at all. Please help.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
14,"""Ito-Riemann"" integration","""Ito-Riemann"" integration",,"Is there a real-valued function on a closed bounded nonempty interval that is not Riemann integrable, but is ""Ito-Riemann"" integrable, that is: if the sampling point is always the left-end point of the subinterval in the partition?","Is there a real-valued function on a closed bounded nonempty interval that is not Riemann integrable, but is ""Ito-Riemann"" integrable, that is: if the sampling point is always the left-end point of the subinterval in the partition?",,"['real-analysis', 'integration', 'functions']"
15,Inverse Functions and $u$-Substitution,Inverse Functions and -Substitution,u,"Back in my undergrad days I wrote a false proof of the following. Problem. Prove that $\displaystyle\int_0^{2\pi}\frac{dx}{1+e^{\sin{x}}}=\pi$ Proof. Integrating by parts gives $$ \int_0^{2\pi}\frac{dx}{1+e^{\sin{x}}} = \left.\frac{x}{1+e^{\sin{x}}}\right\vert_0^{2\pi}+\int_0^{2\pi} x\cdot\frac{e^{\sin x}\cos{x}}{(e^{\sin{x}}+1)^2}dx  =\pi+\int_0^{2\pi} x\cdot\frac{e^{\sin x}\cos{x}}{(e^{\sin{x}}+1)^2}dx  $$ Taking $u=\sin x$ in the last integral gives $$ \int_0^{2\pi} x\cdot\frac{e^{\sin x}\cos{x}}{(e^{\sin{x}}+1)^2}dx  =\int_0^0\arcsin u\frac{e^u}{(e^u+1)}du=0 $$ and combining the two equations gives the result. $\Box$ Of course, the problem with this proof is that the equation $x=\arcsin u$ is only valid on $[0,\pi/2]$. However, $\sin{x}$ is invertible on the intervals $[\pi/2,3\pi/2]$ and $[3\pi/2,2\pi]$ so it seems that this problem can be circumvented by splitting the integral up into three integrals and individually applying the $u$-substitution. Can this proof be salvaged? Edit: I'm aware that there are other ways to prove this result. I'm mainly concerned with the validity of this proof. Edit: I've voted up both answers because they give correct proofs. I haven't accepted an answer, however, because neither addresses the issue of breaking up a noninvertible function into seperate integrals where the function is invertible, which was my main reason for posting this question.","Back in my undergrad days I wrote a false proof of the following. Problem. Prove that $\displaystyle\int_0^{2\pi}\frac{dx}{1+e^{\sin{x}}}=\pi$ Proof. Integrating by parts gives $$ \int_0^{2\pi}\frac{dx}{1+e^{\sin{x}}} = \left.\frac{x}{1+e^{\sin{x}}}\right\vert_0^{2\pi}+\int_0^{2\pi} x\cdot\frac{e^{\sin x}\cos{x}}{(e^{\sin{x}}+1)^2}dx  =\pi+\int_0^{2\pi} x\cdot\frac{e^{\sin x}\cos{x}}{(e^{\sin{x}}+1)^2}dx  $$ Taking $u=\sin x$ in the last integral gives $$ \int_0^{2\pi} x\cdot\frac{e^{\sin x}\cos{x}}{(e^{\sin{x}}+1)^2}dx  =\int_0^0\arcsin u\frac{e^u}{(e^u+1)}du=0 $$ and combining the two equations gives the result. $\Box$ Of course, the problem with this proof is that the equation $x=\arcsin u$ is only valid on $[0,\pi/2]$. However, $\sin{x}$ is invertible on the intervals $[\pi/2,3\pi/2]$ and $[3\pi/2,2\pi]$ so it seems that this problem can be circumvented by splitting the integral up into three integrals and individually applying the $u$-substitution. Can this proof be salvaged? Edit: I'm aware that there are other ways to prove this result. I'm mainly concerned with the validity of this proof. Edit: I've voted up both answers because they give correct proofs. I haven't accepted an answer, however, because neither addresses the issue of breaking up a noninvertible function into seperate integrals where the function is invertible, which was my main reason for posting this question.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'differential-forms']"
16,Find $\int_0^{+\infty}\cos 2x\prod_{n=1}^{\infty}\cos\frac{x}{n}dx$,Find,\int_0^{+\infty}\cos 2x\prod_{n=1}^{\infty}\cos\frac{x}{n}dx,"Evaluate the following integral $$\int_0^{+\infty}\cos 2x\prod_{n=1}^{\infty}\cos\frac{x}{n}dx$$ I was thinking of a way which do not need to explicitly find the closed form of the infinite product, since I don't have any idea to tackle that. Any hints are welcomed.","Evaluate the following integral $$\int_0^{+\infty}\cos 2x\prod_{n=1}^{\infty}\cos\frac{x}{n}dx$$ I was thinking of a way which do not need to explicitly find the closed form of the infinite product, since I don't have any idea to tackle that. Any hints are welcomed.",,"['real-analysis', 'integration', 'improper-integrals', 'infinite-product']"
17,Precise meaning of assigning numbers to divergent sequences,Precise meaning of assigning numbers to divergent sequences,,"First of all, I'm aware lots of very similar questions have been asked and answered here and on other sites. However, after browsing literally tens of explanations of this fact, I couldn't find a single one that satisfactorily convinced we can really do this. In fact, for some reason, people seem to be looking sideways and not giving straight answers whenever this is asked, which makes me think this might be a big inside joke at the expense of spreading misinformation. If you consider this question to be a duplicate, please point out where I can find an explanation which sheds light to the points below: I'm going to list a few premises. Please bear in mind I've only taken a Real Analysis course. Since I'm talking about a sum of real numbers, I'd expect one could explain it without making reference to complex analysis. -Equality between real numbers means double inclusion between the sets they represent -An infinite sum is the limit of the partial sums, from the $\epsilon-\delta$ defition of limit -Divergent series are not equal to any number, since, by virtue of their divergence and the archimedean property, we can show they are different to any given x Now, every explanation of why $1+2+3+4+5+6+...=-1/12$ seems to violate one of the above, either by manipulating infinite series as something other than the limit of partial sums, or by violating the radius of convergence, or by claiming equality means something else rather than equality. To me those explanations (particularly the ones referring to analytical continuations) seem akin to saying: $f(x)=x^2$ behaves like $y=0$ near the origin. Hence, $f(7)=7^2=0$. With that in mind, I ask the following questions: What does it mean, precisely, to take infinite sums, if we are to accept $1+2+3+...=-1/12$? What does $=$ means, precisely, in this context? (bonus) I know this is relevant to string theory. Has it ever been used to make demonstrable predictions about the real world?","First of all, I'm aware lots of very similar questions have been asked and answered here and on other sites. However, after browsing literally tens of explanations of this fact, I couldn't find a single one that satisfactorily convinced we can really do this. In fact, for some reason, people seem to be looking sideways and not giving straight answers whenever this is asked, which makes me think this might be a big inside joke at the expense of spreading misinformation. If you consider this question to be a duplicate, please point out where I can find an explanation which sheds light to the points below: I'm going to list a few premises. Please bear in mind I've only taken a Real Analysis course. Since I'm talking about a sum of real numbers, I'd expect one could explain it without making reference to complex analysis. -Equality between real numbers means double inclusion between the sets they represent -An infinite sum is the limit of the partial sums, from the $\epsilon-\delta$ defition of limit -Divergent series are not equal to any number, since, by virtue of their divergence and the archimedean property, we can show they are different to any given x Now, every explanation of why $1+2+3+4+5+6+...=-1/12$ seems to violate one of the above, either by manipulating infinite series as something other than the limit of partial sums, or by violating the radius of convergence, or by claiming equality means something else rather than equality. To me those explanations (particularly the ones referring to analytical continuations) seem akin to saying: $f(x)=x^2$ behaves like $y=0$ near the origin. Hence, $f(7)=7^2=0$. With that in mind, I ask the following questions: What does it mean, precisely, to take infinite sums, if we are to accept $1+2+3+...=-1/12$? What does $=$ means, precisely, in this context? (bonus) I know this is relevant to string theory. Has it ever been used to make demonstrable predictions about the real world?",,"['real-analysis', 'sequences-and-series', 'divergent-series']"
18,Continuity and sequential continuity,Continuity and sequential continuity,,"The function $f:(X,d)\rightarrow(Y,\rho)$ is continuous if and only if $f$ is sequentially continuous (that means $x_n\rightarrow x \Rightarrow f(x_n)\rightarrow f(x)$) Proof. First I show that if $f$ is continuous then $f$ is sequentially continuous. I consider the sequence $x_n\rightarrow x_0$ so I can find for $\varepsilon =\delta$ a value $n_\delta$ such that $|x_n-x_0|<\delta$. Doing so I can use the hypothesis that $f$ is continuous so $|f(x_n)-f(x_0)|<\varepsilon$.  Now, I show the opposite implication. I know now that $$ \forall \varepsilon >0 \,\,\,\,\exists n_\varepsilon : \forall n\geq n_\varepsilon \,\,\,\,\,|x_n-x_0|<\varepsilon $$ So for a certain $\varepsilon$: $$ |f(x_n)-f(x_0)|<\varepsilon_1 $$ If I call $\varepsilon=\delta$ and $\varepsilon_1=\varepsilon$ we have the definition of continuity. I'm not really sure, I don't know why. But can this proof be considered acceptable? I mean, it is correct and it is written in a decent way?","The function $f:(X,d)\rightarrow(Y,\rho)$ is continuous if and only if $f$ is sequentially continuous (that means $x_n\rightarrow x \Rightarrow f(x_n)\rightarrow f(x)$) Proof. First I show that if $f$ is continuous then $f$ is sequentially continuous. I consider the sequence $x_n\rightarrow x_0$ so I can find for $\varepsilon =\delta$ a value $n_\delta$ such that $|x_n-x_0|<\delta$. Doing so I can use the hypothesis that $f$ is continuous so $|f(x_n)-f(x_0)|<\varepsilon$.  Now, I show the opposite implication. I know now that $$ \forall \varepsilon >0 \,\,\,\,\exists n_\varepsilon : \forall n\geq n_\varepsilon \,\,\,\,\,|x_n-x_0|<\varepsilon $$ So for a certain $\varepsilon$: $$ |f(x_n)-f(x_0)|<\varepsilon_1 $$ If I call $\varepsilon=\delta$ and $\varepsilon_1=\varepsilon$ we have the definition of continuity. I'm not really sure, I don't know why. But can this proof be considered acceptable? I mean, it is correct and it is written in a decent way?",,"['real-analysis', 'metric-spaces']"
19,"Prove that $f:(a, b) \to \mathbb R$ has at most countably many simple discontinuities",Prove that  has at most countably many simple discontinuities,"f:(a, b) \to \mathbb R","This is problem 17 in baby Rudin's chapter on continuity. He has a hint to use triplets of rationals that bound each simple discontinuity on the left, right, and in between the values of the limits from the left and right. It seems like this can be weakened to just rationals to the left and right. Simple discontinuities are those in which the limit from the left and right exist, so there must be intervals to the left and right of a simple continuity on which no other simple discontinuity can exist. More precisely, let $c$ and $c'$ be simple discontinuities for $f$ on $(a,b)$ and consider the limit, $l$, of $f$ approaching $c$ from the left: $$\forall \epsilon >0, \exists \delta>0 : c-x<\delta \Rightarrow |l-f(x)|<\epsilon$$ but $ \exists \epsilon '>0 \forall \delta ' >0 : |c'-x|<\delta ' \Rightarrow |f(c')-f(x)|>\epsilon ' \\ \therefore \epsilon = \epsilon ' , \delta ' = \delta \rightarrow \leftarrow $ Therefore, you can make an injection from the set of simple discontinuities to a subset of rationals by associating each simple discontinuity with one rational in the aforementioned ""free"" interval to its left; from there compose with the map from rationals to integers to show countable. Is this argument correct?","This is problem 17 in baby Rudin's chapter on continuity. He has a hint to use triplets of rationals that bound each simple discontinuity on the left, right, and in between the values of the limits from the left and right. It seems like this can be weakened to just rationals to the left and right. Simple discontinuities are those in which the limit from the left and right exist, so there must be intervals to the left and right of a simple continuity on which no other simple discontinuity can exist. More precisely, let $c$ and $c'$ be simple discontinuities for $f$ on $(a,b)$ and consider the limit, $l$, of $f$ approaching $c$ from the left: $$\forall \epsilon >0, \exists \delta>0 : c-x<\delta \Rightarrow |l-f(x)|<\epsilon$$ but $ \exists \epsilon '>0 \forall \delta ' >0 : |c'-x|<\delta ' \Rightarrow |f(c')-f(x)|>\epsilon ' \\ \therefore \epsilon = \epsilon ' , \delta ' = \delta \rightarrow \leftarrow $ Therefore, you can make an injection from the set of simple discontinuities to a subset of rationals by associating each simple discontinuity with one rational in the aforementioned ""free"" interval to its left; from there compose with the map from rationals to integers to show countable. Is this argument correct?",,['real-analysis']
20,Every open subset of $\mathbb{R}^{p}$ is the union of a countable collection of closed sets.,Every open subset of  is the union of a countable collection of closed sets.,\mathbb{R}^{p},"I want to prove the following statement which is in The Elements of Real Analysis by Robert G. Bartle, on page 67 exercise H: Every open subset of $\mathbb{R}^{p}$ is the union of a countable collection of closed sets . (Hint: Argue as in the preceding exercise, but this time use closed balls) I have referred to the to the solution given in this question , but the approach is different from what the book is asking for. I first show the preceding exercise and its proof, and then I show my attempt at the statement that I want to prove. Preceding Exercise: A subset of $\mathbb{R}^{p}$   is open iff it is the union of a countable collection of open balls. proof . $(\leftarrow )$ An open ball is an open set, and by property 9.3(c) the union of any collection of open sets in $\mathbb{R}^{p}$   is indeed open in $\mathbb{R}^{p}$. That is that the union of a countable collection of open balls is open in $\mathbb{R}^{p}$, hence it is an open subset of $\mathbb{R}^{p}$. $(\rightarrow)$ Let $G\neq\emptyset$  , $G\subset\mathbb{R}^{p}$   be open, let $\{r_{n}\::\: n\in\mathbb{N}\}$   be an enumeration of all the rational points in $G$   (that is that each of the $p$   components of a point of $G$  , consisting of only rational numbers). Now we let $\mathcal{B}(r_{n},\frac{1}{m_{n}}$)   where $m_{n}$   is the smallest natural number such that $\mathcal{B}(r_{n},\frac{1}{m_{n}})\subset G$  . Clearly,$$\bigcup_{n\in\mathbb{N}}\mathcal{B}(r_{n},\frac{1}{m_{n}})\subset G$$. Now if we let $x\in G$  , since $G$   is open we know that there exists a ball centered at $x$  , choose $m$   such that $\mathcal{B}(x,\frac{2}{m})\subset G$  . Now it follows from Theorem 6.10 that there exists a rational number $y\in\mathcal{B} (x,\frac{1}{m})$  ; thus $y\in G$   and therefore $y=r_{n}\in\{r_{n}\::\: n\in\mathbb{N}\}$   for some $n$  . If $x\notin\mathcal{B}(r_{n},\frac{1}{m_{n}})$   then we must have that $\frac{1}{m_{n}}<\frac{1}{m}$  . [(why?) $y\in\mathcal{B}(x,\frac{1}{m})$   implies that the distance $d(x,y)<\frac{1}{m}$  , while $x\notin\mathcal{B}(r_{n},\frac{1}{m_{n}})=\mathcal{B}(y,\frac{1}{m_{n}})$   implies that the distance $d(x,y)\geqslant\frac{1}{m_{n}}$  ; hence $\frac{1}{m_{n}}<\frac{1}{m}$  ]. Now, clearly $\mathcal{B}(r_{n},\frac{1}{m})\subset\mathcal{B}(x,\frac{2}{m})\subset G$   which contradict our selections on $m_{n}$  , thus $x\in\mathcal{B}(r_{n},\frac{1}{m_{n}})$  . [(again,why?) Let $\alpha\in\mathcal{B}(r_{n},\frac{1}{m})$  , then $d(\alpha,y)<1/m$   it follows from the triangle inequality that $d(x,\alpha)\leqslant d(x,y)+d(y,\alpha)<\frac{2}{m}$  ; hence $\alpha\in\mathcal{B}(x,\frac{2}{m})$   and therefore $\mathcal{B}(r_{n},\frac{1}{m})\subset\mathcal{B}(x,\frac{2}{m})$  ]. Now, since $x$   was arbitrarly in $G$  , it follows that for each $x\in G$   there exists $n$   such that $x\in\mathcal{B}(r_{n},\frac{1}{m_{n}})$  ; hence $G\subset\bigcup_{n\in\mathbb{N}}\mathcal{B}(r_{n},\frac{1}{m_{n}})$  which shows that $G=\bigcup_{n\in\mathbb{N}}\mathcal{B}(r_{n},\frac{1}{m_{n}})$. $\square$ Proposition to be proved: Every open subset of $\mathbb{R}^{p}$ is the union of a countable collection of closed sets. proof. Let $G\ne\emptyset$  , $G\subset\mathbb{R}^{p}$   be open, let $\{r_{n}\::\: n\in\mathbb{N}\}$   be an enumeration of all the rational points in $G$  . Since, $G$   is open for every $r_{n}$   there exists an open ball $\mathcal{B}(r_{n},\varepsilon_{n})$   contained in $G$ . Moreover, for each $r_{n}$   there exists a closed ball $\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset\mathcal{B}(r_{n},\varepsilon_{n})\subset\begin{subarray}{c} G\end{subarray}$  , where $m_{n}$   is the smallest natural number such that $\varepsilon_{n}-\frac{1}{m_{n}}>0$  . (i.e. $\frac{1}{m_{n}}<\varepsilon_{n}$   ). Clearly, $$\bigcup_{n\in\mathbb{N}}\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset G.$$ Now we pick an arbitrary $x\in G$  , since $G$   is open we can find a $\mathcal{B}(x,\varepsilon_{m})\subset G$; choose $m\in\mathbb{N}$   so that $m$   is the smallest number such that $\varepsilon_{m}-\frac{1}{m}>0$   and $\overline{\mathcal{B}}(x,\varepsilon_{m}-\frac{1}{m})\begin{subarray}{c} \subset\end{subarray}\mathcal{B}(x,\varepsilon_{m})$  . Now, since $\varepsilon_{m}-\frac{1}{m}>0$   it follows that there must exist a rational number $y\in\overline{\mathcal{B}}(x,\varepsilon_{m}-\frac{1}{m})$  . [hence $y=r_{n}$   for some $n$   and $\mathcal{\overline{B}}(y,\varepsilon_{n}-\frac{1}{m_{n}})=\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset\mathcal{B}(x,\varepsilon_{m})$  ]. Suppose that $x\notin\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})$  , then $d(x,y)>\varepsilon_{n}-\frac{1}{m_{n}}$  , but we know $d(x,y)\leqslant\varepsilon_{m}-\frac{1}{m}$   so it follows that $0<\varepsilon_{n}-\frac{1}{m_{n}}<\varepsilon_{m}-\frac{1}{m}$  . Now, let $\alpha\in\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})$   it follows that $d(\alpha,y)\leqslant\varepsilon_{n}-\frac{1}{m_{n}}$   and that $d(x,\alpha)\leqslant d(\alpha,y)+d(y,x)\leqslant\left(\varepsilon_{n}-\frac{1}{m_{n}}\right)+\left(\varepsilon_{m}-\frac{1}{m}\right)$. As you might see by now I am spinning my wheels and getting no where! If I could show that $x\in\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})$ then I would be done, or likewise if I could show $\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset\overline{\mathcal{B}}(x,\varepsilon_{m}-\frac{1}{m})$ but I can't do either of these, because I don't have any way to relate $\varepsilon_{n}$ to $\varepsilon_{m}$. Perhaps my approach is entirely wrong. I would greatly appreciate some guidance, thanks!","I want to prove the following statement which is in The Elements of Real Analysis by Robert G. Bartle, on page 67 exercise H: Every open subset of $\mathbb{R}^{p}$ is the union of a countable collection of closed sets . (Hint: Argue as in the preceding exercise, but this time use closed balls) I have referred to the to the solution given in this question , but the approach is different from what the book is asking for. I first show the preceding exercise and its proof, and then I show my attempt at the statement that I want to prove. Preceding Exercise: A subset of $\mathbb{R}^{p}$   is open iff it is the union of a countable collection of open balls. proof . $(\leftarrow )$ An open ball is an open set, and by property 9.3(c) the union of any collection of open sets in $\mathbb{R}^{p}$   is indeed open in $\mathbb{R}^{p}$. That is that the union of a countable collection of open balls is open in $\mathbb{R}^{p}$, hence it is an open subset of $\mathbb{R}^{p}$. $(\rightarrow)$ Let $G\neq\emptyset$  , $G\subset\mathbb{R}^{p}$   be open, let $\{r_{n}\::\: n\in\mathbb{N}\}$   be an enumeration of all the rational points in $G$   (that is that each of the $p$   components of a point of $G$  , consisting of only rational numbers). Now we let $\mathcal{B}(r_{n},\frac{1}{m_{n}}$)   where $m_{n}$   is the smallest natural number such that $\mathcal{B}(r_{n},\frac{1}{m_{n}})\subset G$  . Clearly,$$\bigcup_{n\in\mathbb{N}}\mathcal{B}(r_{n},\frac{1}{m_{n}})\subset G$$. Now if we let $x\in G$  , since $G$   is open we know that there exists a ball centered at $x$  , choose $m$   such that $\mathcal{B}(x,\frac{2}{m})\subset G$  . Now it follows from Theorem 6.10 that there exists a rational number $y\in\mathcal{B} (x,\frac{1}{m})$  ; thus $y\in G$   and therefore $y=r_{n}\in\{r_{n}\::\: n\in\mathbb{N}\}$   for some $n$  . If $x\notin\mathcal{B}(r_{n},\frac{1}{m_{n}})$   then we must have that $\frac{1}{m_{n}}<\frac{1}{m}$  . [(why?) $y\in\mathcal{B}(x,\frac{1}{m})$   implies that the distance $d(x,y)<\frac{1}{m}$  , while $x\notin\mathcal{B}(r_{n},\frac{1}{m_{n}})=\mathcal{B}(y,\frac{1}{m_{n}})$   implies that the distance $d(x,y)\geqslant\frac{1}{m_{n}}$  ; hence $\frac{1}{m_{n}}<\frac{1}{m}$  ]. Now, clearly $\mathcal{B}(r_{n},\frac{1}{m})\subset\mathcal{B}(x,\frac{2}{m})\subset G$   which contradict our selections on $m_{n}$  , thus $x\in\mathcal{B}(r_{n},\frac{1}{m_{n}})$  . [(again,why?) Let $\alpha\in\mathcal{B}(r_{n},\frac{1}{m})$  , then $d(\alpha,y)<1/m$   it follows from the triangle inequality that $d(x,\alpha)\leqslant d(x,y)+d(y,\alpha)<\frac{2}{m}$  ; hence $\alpha\in\mathcal{B}(x,\frac{2}{m})$   and therefore $\mathcal{B}(r_{n},\frac{1}{m})\subset\mathcal{B}(x,\frac{2}{m})$  ]. Now, since $x$   was arbitrarly in $G$  , it follows that for each $x\in G$   there exists $n$   such that $x\in\mathcal{B}(r_{n},\frac{1}{m_{n}})$  ; hence $G\subset\bigcup_{n\in\mathbb{N}}\mathcal{B}(r_{n},\frac{1}{m_{n}})$  which shows that $G=\bigcup_{n\in\mathbb{N}}\mathcal{B}(r_{n},\frac{1}{m_{n}})$. $\square$ Proposition to be proved: Every open subset of $\mathbb{R}^{p}$ is the union of a countable collection of closed sets. proof. Let $G\ne\emptyset$  , $G\subset\mathbb{R}^{p}$   be open, let $\{r_{n}\::\: n\in\mathbb{N}\}$   be an enumeration of all the rational points in $G$  . Since, $G$   is open for every $r_{n}$   there exists an open ball $\mathcal{B}(r_{n},\varepsilon_{n})$   contained in $G$ . Moreover, for each $r_{n}$   there exists a closed ball $\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset\mathcal{B}(r_{n},\varepsilon_{n})\subset\begin{subarray}{c} G\end{subarray}$  , where $m_{n}$   is the smallest natural number such that $\varepsilon_{n}-\frac{1}{m_{n}}>0$  . (i.e. $\frac{1}{m_{n}}<\varepsilon_{n}$   ). Clearly, $$\bigcup_{n\in\mathbb{N}}\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset G.$$ Now we pick an arbitrary $x\in G$  , since $G$   is open we can find a $\mathcal{B}(x,\varepsilon_{m})\subset G$; choose $m\in\mathbb{N}$   so that $m$   is the smallest number such that $\varepsilon_{m}-\frac{1}{m}>0$   and $\overline{\mathcal{B}}(x,\varepsilon_{m}-\frac{1}{m})\begin{subarray}{c} \subset\end{subarray}\mathcal{B}(x,\varepsilon_{m})$  . Now, since $\varepsilon_{m}-\frac{1}{m}>0$   it follows that there must exist a rational number $y\in\overline{\mathcal{B}}(x,\varepsilon_{m}-\frac{1}{m})$  . [hence $y=r_{n}$   for some $n$   and $\mathcal{\overline{B}}(y,\varepsilon_{n}-\frac{1}{m_{n}})=\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset\mathcal{B}(x,\varepsilon_{m})$  ]. Suppose that $x\notin\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})$  , then $d(x,y)>\varepsilon_{n}-\frac{1}{m_{n}}$  , but we know $d(x,y)\leqslant\varepsilon_{m}-\frac{1}{m}$   so it follows that $0<\varepsilon_{n}-\frac{1}{m_{n}}<\varepsilon_{m}-\frac{1}{m}$  . Now, let $\alpha\in\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})$   it follows that $d(\alpha,y)\leqslant\varepsilon_{n}-\frac{1}{m_{n}}$   and that $d(x,\alpha)\leqslant d(\alpha,y)+d(y,x)\leqslant\left(\varepsilon_{n}-\frac{1}{m_{n}}\right)+\left(\varepsilon_{m}-\frac{1}{m}\right)$. As you might see by now I am spinning my wheels and getting no where! If I could show that $x\in\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})$ then I would be done, or likewise if I could show $\overline{\mathcal{B}}(r_{n},\varepsilon_{n}-\frac{1}{m_{n}})\subset\overline{\mathcal{B}}(x,\varepsilon_{m}-\frac{1}{m})$ but I can't do either of these, because I don't have any way to relate $\varepsilon_{n}$ to $\varepsilon_{m}$. Perhaps my approach is entirely wrong. I would greatly appreciate some guidance, thanks!",,"['real-analysis', 'general-topology']"
21,Bound on $|f(x)|^2 + |f'(x)|^2$,Bound on,|f(x)|^2 + |f'(x)|^2,"Let $f\in C^2(\mathbb{R})$ be a twice differentiable function satisfying $$|f(x)|^2\le  a$$ and $$|f'(x)|^2 + |f''(x)|^2\le  b$$ for all real $x$, where $a$ and $b$ are positive constants. Prove that $|f(x)|^2 + |f'(x)|^2\le  \max(a, b)$ for all real $x$.","Let $f\in C^2(\mathbb{R})$ be a twice differentiable function satisfying $$|f(x)|^2\le  a$$ and $$|f'(x)|^2 + |f''(x)|^2\le  b$$ for all real $x$, where $a$ and $b$ are positive constants. Prove that $|f(x)|^2 + |f'(x)|^2\le  \max(a, b)$ for all real $x$.",,"['calculus', 'real-analysis', 'analysis', 'inequality', 'contest-math']"
22,"If $f \in L^1(\mathbb{R})$ and $\hat f \geq 0$, is $f$ continuous?","If  and , is  continuous?",f \in L^1(\mathbb{R}) \hat f \geq 0 f,"Suppose $f \in L^1(\mathbb{R})$.  I am wondering what conditions on $\hat f = \left[ s \mapsto \int e^{its} f(t) \ dt \right] \in C_0(\mathbb{R})$ suffice to make $f$ continuous (or, more accurately, equal almost everywhere to some unique continuous function). Example : If $\hat f \in L^1(\mathbb{R})$, then $f$ is, almost everywhere, equal to the inverse transform of $\hat f$ (Fourier inversion) so $f$ is continuous. However, it is possible for $f$ to be continuous, even to have $f \in C_c(\mathbb{R})$, and still not have $\hat f$ integrable. Examples: In his answer here , robjohn says that function    $$f(t) = \begin{cases} \frac{-1}{\log(t) + \log(1-t)} & \text{ if } 0 < t < 1 \\ 0 & \text{ otherwise } \\ \end{cases}$$ (see this plot ) has $\hat f \notin L^1(\mathbb{R})$. The problem of finding an $f \in C_c(\mathbb{R})$ such that $\hat f \notin L^1(\mathbb{R})$ is also discussed here . So, I'm wondering about other ways in which $\hat f$ can ""see"" continuity of $f$. Specifically: Question: If $\hat f \geq 0$, is $f$ continuous? If this is false, or even if it's true, I am also interested in the more general question: Question: In what ways can continuity of $f$ manifest itself as some property of the Fourier transform $\hat f$? Thanks.","Suppose $f \in L^1(\mathbb{R})$.  I am wondering what conditions on $\hat f = \left[ s \mapsto \int e^{its} f(t) \ dt \right] \in C_0(\mathbb{R})$ suffice to make $f$ continuous (or, more accurately, equal almost everywhere to some unique continuous function). Example : If $\hat f \in L^1(\mathbb{R})$, then $f$ is, almost everywhere, equal to the inverse transform of $\hat f$ (Fourier inversion) so $f$ is continuous. However, it is possible for $f$ to be continuous, even to have $f \in C_c(\mathbb{R})$, and still not have $\hat f$ integrable. Examples: In his answer here , robjohn says that function    $$f(t) = \begin{cases} \frac{-1}{\log(t) + \log(1-t)} & \text{ if } 0 < t < 1 \\ 0 & \text{ otherwise } \\ \end{cases}$$ (see this plot ) has $\hat f \notin L^1(\mathbb{R})$. The problem of finding an $f \in C_c(\mathbb{R})$ such that $\hat f \notin L^1(\mathbb{R})$ is also discussed here . So, I'm wondering about other ways in which $\hat f$ can ""see"" continuity of $f$. Specifically: Question: If $\hat f \geq 0$, is $f$ continuous? If this is false, or even if it's true, I am also interested in the more general question: Question: In what ways can continuity of $f$ manifest itself as some property of the Fourier transform $\hat f$? Thanks.",,"['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
23,Must a uniformly continuous function from $\mathbb{R}$ to $\mathbb{R}$ have a finite modulus of continuity?,Must a uniformly continuous function from  to  have a finite modulus of continuity?,\mathbb{R} \mathbb{R},"I searched for similar questions and I could find a duplicate for this. NOTE: I am using the definition of ""modulus of continuity"" as found in Wikipedia for example ( http://en.wikipedia.org/wiki/Modulus_of_continuity ).  Under this definition, any uniformly continuous function has many ""moduli of continuity"".  Some people use a different definition, under which one refers to "" the modulus of continuity of $f$"", and that modulus of continuity satisfies all the requirements that Wikipedia, for example, requires of a modulus of continuity.  It appears that different people use the term slightly differently, and I can't make everyone 100% happy.  I am not claiming Wikipedia is the ultimate authority for math matters, it is just a convenient reference for me at the moment. The title is the question: lacking a good reference, and unable to find one online, I was proving to myself that a uniformly continuous function $f$ with domain and codomain $\mathbb{R}$ has a modulus of continuity $\omega$.  Using the definition of uniformly continuous, I constructed such an $\omega$, and it seems to me that I can prove that $\omega(t)$ is finite for all $t \geq 0$. If this is indeed true, someone has undoubtedly discovered this before, probably one or two centuries ago.  Can anyone provide a reference, or a counterexample in case I'm wrong? If the answer to the title question is ""yes"", I would like some explanation  why moduli of continuity are allowed to be infinite (finitivity is not part of any definition I have ever seen).  Can anyone cite a good reason?  Perhaps it is useful to allow $\omega$ to be infinite if the domain and/or codomain are infinite-dimensional normed linear spaces (?).   I am not interested in silly, contrived examples, rather examples of uniformly continuous functions between metric spaces (such as familiar function spaces that analysts actually use) with no finite modulus of continuity that are of some interest for some real math problem (not just a counterexample for the sake of a counterexample). EDIT: thinking about it some more, I am pretty sure any uniformly continuous function between any two ""nice"" metric spaces (is connectivity sufficient?) has a finite-valued modulus of continuity, but that leaves open the question of why the definition permits a modulus of continuity to be infinite-valued.  Is it ever convenient to use one that takes infinite values?","I searched for similar questions and I could find a duplicate for this. NOTE: I am using the definition of ""modulus of continuity"" as found in Wikipedia for example ( http://en.wikipedia.org/wiki/Modulus_of_continuity ).  Under this definition, any uniformly continuous function has many ""moduli of continuity"".  Some people use a different definition, under which one refers to "" the modulus of continuity of $f$"", and that modulus of continuity satisfies all the requirements that Wikipedia, for example, requires of a modulus of continuity.  It appears that different people use the term slightly differently, and I can't make everyone 100% happy.  I am not claiming Wikipedia is the ultimate authority for math matters, it is just a convenient reference for me at the moment. The title is the question: lacking a good reference, and unable to find one online, I was proving to myself that a uniformly continuous function $f$ with domain and codomain $\mathbb{R}$ has a modulus of continuity $\omega$.  Using the definition of uniformly continuous, I constructed such an $\omega$, and it seems to me that I can prove that $\omega(t)$ is finite for all $t \geq 0$. If this is indeed true, someone has undoubtedly discovered this before, probably one or two centuries ago.  Can anyone provide a reference, or a counterexample in case I'm wrong? If the answer to the title question is ""yes"", I would like some explanation  why moduli of continuity are allowed to be infinite (finitivity is not part of any definition I have ever seen).  Can anyone cite a good reason?  Perhaps it is useful to allow $\omega$ to be infinite if the domain and/or codomain are infinite-dimensional normed linear spaces (?).   I am not interested in silly, contrived examples, rather examples of uniformly continuous functions between metric spaces (such as familiar function spaces that analysts actually use) with no finite modulus of continuity that are of some interest for some real math problem (not just a counterexample for the sake of a counterexample). EDIT: thinking about it some more, I am pretty sure any uniformly continuous function between any two ""nice"" metric spaces (is connectivity sufficient?) has a finite-valued modulus of continuity, but that leaves open the question of why the definition permits a modulus of continuity to be infinite-valued.  Is it ever convenient to use one that takes infinite values?",,"['real-analysis', 'general-topology', 'metric-spaces']"
24,$f=g$ almost everywhere $\Rightarrow |f|=|g|$ almost everywhere?,almost everywhere  almost everywhere?,f=g \Rightarrow |f|=|g|,"Suppose $(X, \mathcal{M}, \mu)$ is a measure space. Assume $f: X\to\overline{\mathbb{R}}$ and $g=X\to\overline{\mathbb{R}}$ are measurable maps. Here $\overline{\mathbb{R}}$ denotes the set of extended real numbers. My question is: If $f=g$ almost everywhere, does it follow that $|f|=|g|$ almost   everywhere? I know the answer is ""Yes"" if $X$ is a complete measure space: If $f=g$ a.e. then $E=\{x\in X: f(x)\neq g(x)\}$ is a null set, i.e. $\mu(E)=0$. It is clear that $$ F=\{x\in X : |f(x)|\neq |g(x)|\}\subseteq E $$ Since $X$ is complete, all subsets of null sets are in $\mathcal{M}$, and so $\mu(F)=0$, and $|f|=|g|$ a.e. What happens when $X$ is not complete? Thanks for your time :)","Suppose $(X, \mathcal{M}, \mu)$ is a measure space. Assume $f: X\to\overline{\mathbb{R}}$ and $g=X\to\overline{\mathbb{R}}$ are measurable maps. Here $\overline{\mathbb{R}}$ denotes the set of extended real numbers. My question is: If $f=g$ almost everywhere, does it follow that $|f|=|g|$ almost   everywhere? I know the answer is ""Yes"" if $X$ is a complete measure space: If $f=g$ a.e. then $E=\{x\in X: f(x)\neq g(x)\}$ is a null set, i.e. $\mu(E)=0$. It is clear that $$ F=\{x\in X : |f(x)|\neq |g(x)|\}\subseteq E $$ Since $X$ is complete, all subsets of null sets are in $\mathcal{M}$, and so $\mu(F)=0$, and $|f|=|g|$ a.e. What happens when $X$ is not complete? Thanks for your time :)",,"['real-analysis', 'measure-theory']"
25,Diagonal sequence trick: Uniform bound necessary?,Diagonal sequence trick: Uniform bound necessary?,,"There is a treatment of the ""diagonal sequence trick"" in Reed and Simon ( Functional Analysis Vol.1 ) stated there as follows: Let $f_n(m)$ be a sequence of functions on the positive integers which is uniformly bounded, i.e. $|f_n(m)| \le C$ for all $n,m$. Then there is a subsequence $\{f_{\hat n(i)}(m)\}^\infty_{i = 1}$ so that for each fixed $m$,  the sequence $f_{\hat n(i)}(m)$ converges as $i \to \infty$. I was wondering whether the uniform bound is actually necessary. I am not sure where it is essential in the proof, outlined below: Consider the sequence $f_n(1)$. It is a bounded set of numbers, so we can find a subsequence $f_{n(i)}$ such that $f_{n_1(i)}(1) \to f_\infty(1)$, for some number $f_\infty(1)$. Now consider the sequence $f_{n_1(i)}(2)$. We can find a subsequence $f_{n_2(i)}(2) \to f_\infty(2)$ as $i \to \infty$. Proceeding inductively, we find successive subsequences $f_{n_k(i)}$ so that $f_{n_{k+1}(i)}$ is a subsequence of $f_{n_k(i)}$, and $f_{n_k(i)} \to f_\infty(k)$ as $i \to \infty$. Thus, in particular, $f_{n_k(i)}(j) \to f_\infty(j)$ as $i \to \infty$ for $j = 1,2,\dots,k$. To get a subsequence $f_{\hat n(i)}$ converging for each $j$, take the diagonal sequence $\hat n(k) = n_k(k)$. Then $f_{\hat n(k)}, f_{\hat n(k+1)}, \dots$ is a subsequence of $f_{n_k(i)}$ so $f_{\hat n(i)}(k) \to f_\infty(k)$ as $i \to \infty$ for each $k$.","There is a treatment of the ""diagonal sequence trick"" in Reed and Simon ( Functional Analysis Vol.1 ) stated there as follows: Let $f_n(m)$ be a sequence of functions on the positive integers which is uniformly bounded, i.e. $|f_n(m)| \le C$ for all $n,m$. Then there is a subsequence $\{f_{\hat n(i)}(m)\}^\infty_{i = 1}$ so that for each fixed $m$,  the sequence $f_{\hat n(i)}(m)$ converges as $i \to \infty$. I was wondering whether the uniform bound is actually necessary. I am not sure where it is essential in the proof, outlined below: Consider the sequence $f_n(1)$. It is a bounded set of numbers, so we can find a subsequence $f_{n(i)}$ such that $f_{n_1(i)}(1) \to f_\infty(1)$, for some number $f_\infty(1)$. Now consider the sequence $f_{n_1(i)}(2)$. We can find a subsequence $f_{n_2(i)}(2) \to f_\infty(2)$ as $i \to \infty$. Proceeding inductively, we find successive subsequences $f_{n_k(i)}$ so that $f_{n_{k+1}(i)}$ is a subsequence of $f_{n_k(i)}$, and $f_{n_k(i)} \to f_\infty(k)$ as $i \to \infty$. Thus, in particular, $f_{n_k(i)}(j) \to f_\infty(j)$ as $i \to \infty$ for $j = 1,2,\dots,k$. To get a subsequence $f_{\hat n(i)}$ converging for each $j$, take the diagonal sequence $\hat n(k) = n_k(k)$. Then $f_{\hat n(k)}, f_{\hat n(k+1)}, \dots$ is a subsequence of $f_{n_k(i)}$ so $f_{\hat n(i)}(k) \to f_\infty(k)$ as $i \to \infty$ for each $k$.",,"['real-analysis', 'functional-analysis']"
26,How to show that the is a $1-1$ correspondence between real numbers and the set of points of a line in the Euclidean plane?,How to show that the is a  correspondence between real numbers and the set of points of a line in the Euclidean plane?,1-1,"When we sketch a line in the plane , we say that we can label every point on this line by a real number , and for every real number  there exists only a point on the line which can be labeled by this number . This - as I think - is the reason which we consider the plane as $\mathbb{R}^2$. but I have never seen a proof for this fact . so my question is , How to prove this ? I think that this can be proved by constructing  a bijection ( or showing that such bijection exist without giving it explicitly )  from the Reals to the set of points of line . So , Can we show that such bijection exist ? If not , How did mathematicians know that reals express lines in the plane without gaps ?! in the case of the lack of such proof , there is a possibility that there are gaps in our line line when we express it by real numbers. Added :  to make the question clearer , We say the plane is $\mathbb{R}^2$ and so we we assume that there is a correspondence between $\mathbb{R}$ and the set of points of the $x$-axis and two sets have this correspondence if there is a bijective between them , so we have to find such bijective , otherwise , why not to say that there is a correspondence between the line and a proper subset $A$ such that there is not bijective from $A$ into $\mathbb{R}$ ( and so there is no correspondence between the line and $\mathbb{R}$)  ?","When we sketch a line in the plane , we say that we can label every point on this line by a real number , and for every real number  there exists only a point on the line which can be labeled by this number . This - as I think - is the reason which we consider the plane as $\mathbb{R}^2$. but I have never seen a proof for this fact . so my question is , How to prove this ? I think that this can be proved by constructing  a bijection ( or showing that such bijection exist without giving it explicitly )  from the Reals to the set of points of line . So , Can we show that such bijection exist ? If not , How did mathematicians know that reals express lines in the plane without gaps ?! in the case of the lack of such proof , there is a possibility that there are gaps in our line line when we express it by real numbers. Added :  to make the question clearer , We say the plane is $\mathbb{R}^2$ and so we we assume that there is a correspondence between $\mathbb{R}$ and the set of points of the $x$-axis and two sets have this correspondence if there is a bijective between them , so we have to find such bijective , otherwise , why not to say that there is a correspondence between the line and a proper subset $A$ such that there is not bijective from $A$ into $\mathbb{R}$ ( and so there is no correspondence between the line and $\mathbb{R}$)  ?",,"['real-analysis', 'elementary-set-theory', 'foundations']"
27,Limits along what curves suffice to guarantee the existence of a limit?,Limits along what curves suffice to guarantee the existence of a limit?,,"For functions f from $\mathbb{R}^2$ to $\mathbb{R}$, we can define the limit of $f ( x,y)$ as $(x,y)$ goes to $(a,b)$ along the curve $C$ for any continuous curve $C$ passing through (a,b).  And it is a theorem that if the limit of $f(x,y)$ as $(x,y)$ goes to $(a,b)$ along $C$ exists and is the same for all continuous curves $C$ passing through $(a,b)$, then the (unqualified) limit of $f(x,y)$ as $(x,y)$ approaches $(a,b)$ also exists and is equal to the limit along all those curves. My question is, can we weaken those conditions?  In other words, is there some smaller class of curves passing through $(a,b)$ for which $f(x,y)$ having the same limit for them suffices to guarantee the existence of the limit?  We can't make restrict ourselves to just lines passing through $(a,b)$, as there exist examples of functions which have the same limit along all lines passing through a point, but a different limit along some parabola.  (For instance, $f(x,y) = \frac{x^2y}{x^4+y^2}$ for the point $(0,0)$ and the parabola $y=x^2$.) And I assume that similarly, the limit along any parabola could be the same, but the limit along some cubic function could be different, and that more generally for any $n$, there exists a function which has the same limit for all polynomial curves of degree $n$ or lower, but has a different limit along some polynomial curve of degree $n+1$.  (By a polynomial curve I mean either the graph of a polynomial function, or a curve obtained by rotating the graph of such a function about the point $ (a,b) $.) Assuming that I'm right about all that, what if we took the class of polynomial curves of all orders?  Then would the function having a limit along those curves suffice to guarantee that the limit exists in general?  Or if that doesn't suffice, what about the class of smooth or analytic curves? Any help would be greatly appreciated. Thank You in Advance.","For functions f from $\mathbb{R}^2$ to $\mathbb{R}$, we can define the limit of $f ( x,y)$ as $(x,y)$ goes to $(a,b)$ along the curve $C$ for any continuous curve $C$ passing through (a,b).  And it is a theorem that if the limit of $f(x,y)$ as $(x,y)$ goes to $(a,b)$ along $C$ exists and is the same for all continuous curves $C$ passing through $(a,b)$, then the (unqualified) limit of $f(x,y)$ as $(x,y)$ approaches $(a,b)$ also exists and is equal to the limit along all those curves. My question is, can we weaken those conditions?  In other words, is there some smaller class of curves passing through $(a,b)$ for which $f(x,y)$ having the same limit for them suffices to guarantee the existence of the limit?  We can't make restrict ourselves to just lines passing through $(a,b)$, as there exist examples of functions which have the same limit along all lines passing through a point, but a different limit along some parabola.  (For instance, $f(x,y) = \frac{x^2y}{x^4+y^2}$ for the point $(0,0)$ and the parabola $y=x^2$.) And I assume that similarly, the limit along any parabola could be the same, but the limit along some cubic function could be different, and that more generally for any $n$, there exists a function which has the same limit for all polynomial curves of degree $n$ or lower, but has a different limit along some polynomial curve of degree $n+1$.  (By a polynomial curve I mean either the graph of a polynomial function, or a curve obtained by rotating the graph of such a function about the point $ (a,b) $.) Assuming that I'm right about all that, what if we took the class of polynomial curves of all orders?  Then would the function having a limit along those curves suffice to guarantee that the limit exists in general?  Or if that doesn't suffice, what about the class of smooth or analytic curves? Any help would be greatly appreciated. Thank You in Advance.",,"['calculus', 'real-analysis', 'general-topology', 'limits', 'multivariable-calculus']"
28,Hahn Banach theorem with no dominating sublinear functional,Hahn Banach theorem with no dominating sublinear functional,,"Let $V$ be a vector space and $M$ be subspace of it. If $f$ is a linear functional on $M$, is it possible to extend it to the whole space $V$? If we have a sublinear functional $p$ on $V$ dominating $f$ on $M$, then by Hahn Banach we know that there is an extension. I would like to know if the same statement is valid with out the hypothesis of domination sublinear functional. I would like to understand the role of sublinear functional in the proof of Hahn Banach. Thanks","Let $V$ be a vector space and $M$ be subspace of it. If $f$ is a linear functional on $M$, is it possible to extend it to the whole space $V$? If we have a sublinear functional $p$ on $V$ dominating $f$ on $M$, then by Hahn Banach we know that there is an extension. I would like to know if the same statement is valid with out the hypothesis of domination sublinear functional. I would like to understand the role of sublinear functional in the proof of Hahn Banach. Thanks",,"['real-analysis', 'analysis', 'functional-analysis']"
29,"Prove that, $f:\mathbb{R}\rightarrow \mathbb{R} $ is a isomorphism if, only if $f(x)=x $.","Prove that,  is a isomorphism if, only if .",f:\mathbb{R}\rightarrow \mathbb{R}  f(x)=x ,"Prove that, $f:\mathbb{R}\rightarrow \mathbb{R} $ is a isomorphism if, only if $f(x)=x $. For any rational element that is right, but do not know how to prove to the irrational. I would like to prove it using only the properties of complete ordered field.","Prove that, $f:\mathbb{R}\rightarrow \mathbb{R} $ is a isomorphism if, only if $f(x)=x $. For any rational element that is right, but do not know how to prove to the irrational. I would like to prove it using only the properties of complete ordered field.",,"['calculus', 'real-analysis']"
30,Convergence of $\sum_{n=1}^{\infty}\dfrac{(-1)^n}{n^{1+1/n}}$,Convergence of,\sum_{n=1}^{\infty}\dfrac{(-1)^n}{n^{1+1/n}},"Does the series $$\sum_{n=1}^{\infty}\dfrac{(-1)^n}{n^{1+1/n}}$$ converge absolutely, converge conditionally, or diverge? I've tried applying the ratio test and the root test, and in both cases the limit is $1$, so I cannot conclude anything.","Does the series $$\sum_{n=1}^{\infty}\dfrac{(-1)^n}{n^{1+1/n}}$$ converge absolutely, converge conditionally, or diverge? I've tried applying the ratio test and the root test, and in both cases the limit is $1$, so I cannot conclude anything.",,"['real-analysis', 'convergence-divergence']"
31,Circle Chord Sequence,Circle Chord Sequence,,"This is my first post, so be nice! When I was in my first Geometry class in high school, I asked the teacher the following: Given a circle of radius 2a, find the length of the chord running parallel to the diameter of the circle such that the semicircle cut by the chord is divided into two regions of equivalent area. The teacher looked at me as if he knew the answer but then paused, puzzled, and told me that we should work on it after class.  So we did, and it was at this moment that I was introduced to trigonometry.  Yet we did not find a solution and the problem was filed into the back of my memory for some time.  A year and a half later, at the end of Algebra 2/Trig, the problem re-entered my awareness and I took another crack at it.  After maybe an hour of work I found the solution!  I was so excited that I asked my teacher if I could present it to the class on the second to last day of school, and she consented.  So I presented it, but crap I forgot my notes and I screwed up the work leading to the solution and embarrassed myself in front of everybody.  I cleaned it up after class when I had more time, but by then only the real math enthusiasts were left.  Anyway, that was completely tangential, as I would now like to pose the question that I came here with: Skip to here if you don't care about anything but mathematics: Given a circle of radius 2a, find the sequence of real numbers given by the length of successive iterations of slicing the segment resulting from a chord which runs parallel to the diameter of the circle such that the area of the segment is halved with each successive slice. Oh, and the answer to the first question is something like 0.71... or 0.79..., I have forgotten by now.  Bonus points to the most elegant solution of the first problem.","This is my first post, so be nice! When I was in my first Geometry class in high school, I asked the teacher the following: Given a circle of radius 2a, find the length of the chord running parallel to the diameter of the circle such that the semicircle cut by the chord is divided into two regions of equivalent area. The teacher looked at me as if he knew the answer but then paused, puzzled, and told me that we should work on it after class.  So we did, and it was at this moment that I was introduced to trigonometry.  Yet we did not find a solution and the problem was filed into the back of my memory for some time.  A year and a half later, at the end of Algebra 2/Trig, the problem re-entered my awareness and I took another crack at it.  After maybe an hour of work I found the solution!  I was so excited that I asked my teacher if I could present it to the class on the second to last day of school, and she consented.  So I presented it, but crap I forgot my notes and I screwed up the work leading to the solution and embarrassed myself in front of everybody.  I cleaned it up after class when I had more time, but by then only the real math enthusiasts were left.  Anyway, that was completely tangential, as I would now like to pose the question that I came here with: Skip to here if you don't care about anything but mathematics: Given a circle of radius 2a, find the sequence of real numbers given by the length of successive iterations of slicing the segment resulting from a chord which runs parallel to the diameter of the circle such that the area of the segment is halved with each successive slice. Oh, and the answer to the first question is something like 0.71... or 0.79..., I have forgotten by now.  Bonus points to the most elegant solution of the first problem.",,"['real-analysis', 'sequences-and-series', 'geometry', 'euclidean-geometry', 'circles']"
32,"Showing that $f(x)=x\sin (1/x)$ is not absolutely continuous on $[0,1]$",Showing that  is not absolutely continuous on,"f(x)=x\sin (1/x) [0,1]","On the interval [0,1].  Define $f(x)=x\sin(1/x)$ for $x\in(0,1]$ and $f(0)=0$.  I didn't work out the exact details but I'm pretty sure that then $$\Big |\int_0^xf'(t)dt\Big |=\infty,$$ due to a process similar to something of the form $1-2+3-4+5-...$ , as one approaches zero from above. However according to the measure-theoretic definition of absolute continuity, there should in fact be some set of measure zero $E\in[0,1]$ such that $$\Big |\int_Ef'd\mu\Big | > 0.$$ I wasn't under the impression that this was even possible. Edit: Maybe I wasn't clear about what my question is.  What I want is a proof (constructive or not) that there exists a set of measure zero $E$ such that $\Big |\int_Ef'd\mu\Big | > 0.$  Or if that's not possible then for someone to explain to me what my misconception is concerning the measure theoretic definition of absolute continuity: For $v(E)=\int_Efd\mu$. If $\mu(E)=0$ then $v(E)=0$. link to definition definition can also be found in Royden's Real Analysis","On the interval [0,1].  Define $f(x)=x\sin(1/x)$ for $x\in(0,1]$ and $f(0)=0$.  I didn't work out the exact details but I'm pretty sure that then $$\Big |\int_0^xf'(t)dt\Big |=\infty,$$ due to a process similar to something of the form $1-2+3-4+5-...$ , as one approaches zero from above. However according to the measure-theoretic definition of absolute continuity, there should in fact be some set of measure zero $E\in[0,1]$ such that $$\Big |\int_Ef'd\mu\Big | > 0.$$ I wasn't under the impression that this was even possible. Edit: Maybe I wasn't clear about what my question is.  What I want is a proof (constructive or not) that there exists a set of measure zero $E$ such that $\Big |\int_Ef'd\mu\Big | > 0.$  Or if that's not possible then for someone to explain to me what my misconception is concerning the measure theoretic definition of absolute continuity: For $v(E)=\int_Efd\mu$. If $\mu(E)=0$ then $v(E)=0$. link to definition definition can also be found in Royden's Real Analysis",,"['real-analysis', 'measure-theory']"
33,"Working on a generalized Cantor set, with Lebesgue measure, and a certain inequality.","Working on a generalized Cantor set, with Lebesgue measure, and a certain inequality.",,"Let's consider the interval $[0,1]$ in the same way that we constructed the Cantor set, we can use the same idea, but instead of removing in the step $n$ middle open intervals of length $\frac{1}{3^n}$ we remove of length $\frac{1}{5^n}$. Call each step of this construction $C_n$ i.e $C_0 = [0,1]$ $C_1= C_0-\left(\frac{1}{2}-\frac{1}{10},\frac{1}{2}+\frac{1}{10}\right)=\left[0,\frac{2}{5}\right]\cup \left[\frac{3}{5},1\right]$ And then to construct $C_2$ consider the middle point of $\left[0,\frac{2}{5}\right]$ which is $\frac{2}{10}$ , and remove from that interval the open interval $\left(\frac{3}{20}-\frac{1}{2\cdot 5^2},\frac{3}{20}+\frac{1}{2\cdot 5^2}\right)$ , the same with the interval $\left[\frac{7}{10},1\right]$ . (It's the same as the usual Cantor construction) So we define $ C = \bigcap C_n$.  Let's define the function $f_n$ as the characteristic function of the set $C_n$ , and $f$ as the characteristic function of the set $C$. Prove that $$ \mathop {\sup }\limits_{m \geqslant n} \int\limits_0^1 {\left| {f_n \left( x \right) - f_m \left( x \right)} \right|\,\mathrm dm}  \leqslant \frac{1} {3}\left( {\frac{2} {5}} \right)^n $$ and then prove that under any modification of the function $f$ on a set of measure zero, is not Riemann integrable. I'm very scared of this problem, I don't know how to prove the inequality please help me, I'm a little dizzy . Thanks for the EDIT","Let's consider the interval $[0,1]$ in the same way that we constructed the Cantor set, we can use the same idea, but instead of removing in the step $n$ middle open intervals of length $\frac{1}{3^n}$ we remove of length $\frac{1}{5^n}$. Call each step of this construction $C_n$ i.e $C_0 = [0,1]$ $C_1= C_0-\left(\frac{1}{2}-\frac{1}{10},\frac{1}{2}+\frac{1}{10}\right)=\left[0,\frac{2}{5}\right]\cup \left[\frac{3}{5},1\right]$ And then to construct $C_2$ consider the middle point of $\left[0,\frac{2}{5}\right]$ which is $\frac{2}{10}$ , and remove from that interval the open interval $\left(\frac{3}{20}-\frac{1}{2\cdot 5^2},\frac{3}{20}+\frac{1}{2\cdot 5^2}\right)$ , the same with the interval $\left[\frac{7}{10},1\right]$ . (It's the same as the usual Cantor construction) So we define $ C = \bigcap C_n$.  Let's define the function $f_n$ as the characteristic function of the set $C_n$ , and $f$ as the characteristic function of the set $C$. Prove that $$ \mathop {\sup }\limits_{m \geqslant n} \int\limits_0^1 {\left| {f_n \left( x \right) - f_m \left( x \right)} \right|\,\mathrm dm}  \leqslant \frac{1} {3}\left( {\frac{2} {5}} \right)^n $$ and then prove that under any modification of the function $f$ on a set of measure zero, is not Riemann integrable. I'm very scared of this problem, I don't know how to prove the inequality please help me, I'm a little dizzy . Thanks for the EDIT",,"['real-analysis', 'measure-theory']"
34,Is $f$ necessarily measurable?,Is  necessarily measurable?,f,"(1) Suppose a function $f$ has a [Lebesgue] measurable domain and is continuous except at a finite number of points. Is $f$ is necessarily [Lebesgue] measurable? Comments For (1), If $f$ is defined on a [Lebesgue] measurable set $E$ and is continuous except for a finite number of values say $x_1,x_2, ... x_n$ are those points of discontinuity, then can't we describe the pre-images of $f$ just as a finite union of of the pre-images of the collection of continuous functions $\{f_i\}_{i=1}^{n}$, where each $f_i$ is defined up between each point of discontinuity of $f$? Or am I missing something here? (2) Suppose the function $f$ is defined on a measurable set $E$ and has the property that $\{x \in E | f(x) > c\}$ is measurable for each rational number $c$. Is $f$ necessarily [Lebesgue] measurable? Comments I got this one, thanks to everyone who commented. Any hints would be appreciated. The text being used is Royden-Fitzpatrick 4th Edition.","(1) Suppose a function $f$ has a [Lebesgue] measurable domain and is continuous except at a finite number of points. Is $f$ is necessarily [Lebesgue] measurable? Comments For (1), If $f$ is defined on a [Lebesgue] measurable set $E$ and is continuous except for a finite number of values say $x_1,x_2, ... x_n$ are those points of discontinuity, then can't we describe the pre-images of $f$ just as a finite union of of the pre-images of the collection of continuous functions $\{f_i\}_{i=1}^{n}$, where each $f_i$ is defined up between each point of discontinuity of $f$? Or am I missing something here? (2) Suppose the function $f$ is defined on a measurable set $E$ and has the property that $\{x \in E | f(x) > c\}$ is measurable for each rational number $c$. Is $f$ necessarily [Lebesgue] measurable? Comments I got this one, thanks to everyone who commented. Any hints would be appreciated. The text being used is Royden-Fitzpatrick 4th Edition.",,['real-analysis']
35,Composition of a continuous function with functions that converge uniformly,Composition of a continuous function with functions that converge uniformly,,"Here is problem that appeared in one of the past final exams for my introductory real analysis course, that I am having hard time to solve. It is Question 5 in 8 of the following file: http://www.math.ubc.ca/Ugrad/pastExams/Math_321_April_2008.pdf Let $\{f_n\}_{n\in\mathbb{N}}$ be a uniformly convergent sequence of   continuous real–valued functions defined on a metric space $M$ and let   $g$ be a continuous function on $\mathbb{R}.$ Define, for each   $n\in\mathbb{N}$, $h_n(x) = g(f_n(x))$. (a) Let $M = [0, 1]$. Prove that the sequence   $\{h_n\}_{n\in\mathbb{N}}$ converges uniformly on $[0, 1]$. (b) Let $M = \mathbb{R}$. Either prove that the sequence   $\{h_n\}_{n\in\mathbb{N}}$ converges uniformly on $\mathbb{R}$ or   provide a counterexample. I have proved part (a). I am having trouble with part (b). It seems to me that part (b) has a counterexample. This is because, the key point in part (a) is that $g$ is uniformly continuous on $[0,1]$ (because $[0,1]$ is compact), but in part (b) compactness is removed. So my guess is that counter-example will involve something like $g(x)=x^2$ (which is an example of continuous function which is not uniformly continuous on $\mathbb{R}$). Is this guess correct? What would be an example of uniformly convergent sequence $\{f_n\}$ of functions in this case? I would very much appreciate any help!","Here is problem that appeared in one of the past final exams for my introductory real analysis course, that I am having hard time to solve. It is Question 5 in 8 of the following file: http://www.math.ubc.ca/Ugrad/pastExams/Math_321_April_2008.pdf Let $\{f_n\}_{n\in\mathbb{N}}$ be a uniformly convergent sequence of   continuous real–valued functions defined on a metric space $M$ and let   $g$ be a continuous function on $\mathbb{R}.$ Define, for each   $n\in\mathbb{N}$, $h_n(x) = g(f_n(x))$. (a) Let $M = [0, 1]$. Prove that the sequence   $\{h_n\}_{n\in\mathbb{N}}$ converges uniformly on $[0, 1]$. (b) Let $M = \mathbb{R}$. Either prove that the sequence   $\{h_n\}_{n\in\mathbb{N}}$ converges uniformly on $\mathbb{R}$ or   provide a counterexample. I have proved part (a). I am having trouble with part (b). It seems to me that part (b) has a counterexample. This is because, the key point in part (a) is that $g$ is uniformly continuous on $[0,1]$ (because $[0,1]$ is compact), but in part (b) compactness is removed. So my guess is that counter-example will involve something like $g(x)=x^2$ (which is an example of continuous function which is not uniformly continuous on $\mathbb{R}$). Is this guess correct? What would be an example of uniformly convergent sequence $\{f_n\}$ of functions in this case? I would very much appreciate any help!",,"['real-analysis', 'convergence-divergence', 'continuity']"
36,Construction of Monotone function which is differentiable on the given set,Construction of Monotone function which is differentiable on the given set,,"Given a set $A \subset \mathbb{R}$ of measure $0$, is it possible to construct a monotone function whose set of non differentiable points is $A$ ?","Given a set $A \subset \mathbb{R}$ of measure $0$, is it possible to construct a monotone function whose set of non differentiable points is $A$ ?",,"['real-analysis', 'analysis', 'reference-request']"
37,Let $ (x_n) $ be a divergent sequence in a compact subset of $ \mathbb{R}^n $. Prove there are two subsequences that converge to different limits.,Let  be a divergent sequence in a compact subset of . Prove there are two subsequences that converge to different limits., (x_n)   \mathbb{R}^n ,"Let $(x_n)$ be a divergent sequence in a compact subset of $\mathbb R^n$. Prove that there are two subsequences of $(x_n)$ that are convergent to different limit points. Some ideas that might be helpful: Heine-Borel theorem states that a subset of $\mathbb R^n$ is compact if and only if it is closed and bounded. Bolzano-Weierstrass Theorem, every bounded sequence contains a convergent subsequence A number $c$ is a limit point of $(x_n)$ if there exists a subsequence of $(x_n)$ convergening to $c$","Let $(x_n)$ be a divergent sequence in a compact subset of $\mathbb R^n$. Prove that there are two subsequences of $(x_n)$ that are convergent to different limit points. Some ideas that might be helpful: Heine-Borel theorem states that a subset of $\mathbb R^n$ is compact if and only if it is closed and bounded. Bolzano-Weierstrass Theorem, every bounded sequence contains a convergent subsequence A number $c$ is a limit point of $(x_n)$ if there exists a subsequence of $(x_n)$ convergening to $c$",,"['real-analysis', 'analysis', 'functional-analysis']"
38,Difference between $\epsilon-n_0$ and $\epsilon-\delta$ limit definition.,Difference between  and  limit definition.,\epsilon-n_0 \epsilon-\delta,"My Real Analysis course uses the $\epsilon - n_0$ definition of the limit, but I have noticed that the $\epsilon - \delta$ approach seems to be more common. Could someone please explain both formally and informally the difference between the definitions? Is there an advantage to using one over the other? An example of a simple proof using both definitions would be great! (Sorry if this is a silly question. Looking at the definitions, I don't think that $n_0 = \delta$. but I could be wrong. If $n_0 = \delta$ then at least there is a simple answer, though I'll feel pretty embarrassed.)","My Real Analysis course uses the $\epsilon - n_0$ definition of the limit, but I have noticed that the $\epsilon - \delta$ approach seems to be more common. Could someone please explain both formally and informally the difference between the definitions? Is there an advantage to using one over the other? An example of a simple proof using both definitions would be great! (Sorry if this is a silly question. Looking at the definitions, I don't think that $n_0 = \delta$. but I could be wrong. If $n_0 = \delta$ then at least there is a simple answer, though I'll feel pretty embarrassed.)",,['real-analysis']
39,Prove existence of $c$ such that $f(x)\ge c g(x)$,Prove existence of  such that,c f(x)\ge c g(x),"Let $f$ and $g$ be non-negative continuous functions on $[0,1]$ such that $f(x)>g(x)$ for all $x$ in $[0,1]$. Show that there exists a constant $c>1$ such that for all $x$ in $[0,1]$ we have $f(x)\ge c g(x)$. I tried using the fact that since $[0,1]$ is compact and $f$ and $g$ are continuous $f$ and $g$ each attain a maximum and a minimum on $[0,1]$, but it didn't get me anywhere. Anyone know how to prove this?","Let $f$ and $g$ be non-negative continuous functions on $[0,1]$ such that $f(x)>g(x)$ for all $x$ in $[0,1]$. Show that there exists a constant $c>1$ such that for all $x$ in $[0,1]$ we have $f(x)\ge c g(x)$. I tried using the fact that since $[0,1]$ is compact and $f$ and $g$ are continuous $f$ and $g$ each attain a maximum and a minimum on $[0,1]$, but it didn't get me anywhere. Anyone know how to prove this?",,['real-analysis']
40,"limit of Holder norms: $\sup\limits_{x\in [a,b]} f(x) = \lim\limits_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$ [duplicate]",limit of Holder norms:  [duplicate],"\sup\limits_{x\in [a,b]} f(x) = \lim\limits_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 6 years ago . Show that $$\sup_{x\in [a,b]} f(x) = \lim_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$$ for f continuous and positive on [a,b].  I can show that LHS is greater than or equal to RHS but I can't show the other direction.","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 6 years ago . Show that $$\sup_{x\in [a,b]} f(x) = \lim_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$$ for f continuous and positive on [a,b].  I can show that LHS is greater than or equal to RHS but I can't show the other direction.",,"['calculus', 'real-analysis', 'integration']"
41,Counterexample to Fubini?,Counterexample to Fubini?,,"I am trying to come up with a measurable function on $[0,1]^2$ which is not integrable, but such that the iterated integrals are defined and unequal. Any help would be appreciated.","I am trying to come up with a measurable function on $[0,1]^2$ which is not integrable, but such that the iterated integrals are defined and unequal. Any help would be appreciated.",,"['real-analysis', 'analysis']"
42,Maps from sets of measure zero to sets of measure zero [duplicate],Maps from sets of measure zero to sets of measure zero [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: If $E$ has measure zero, then does $E^2$ have measure zero? I'm trying to find a proof for the following question: Suppose $A\subset\mathbb{R}$ is a set of measure zero.  Show that the set $A^2=\{x^2 \in \mathbb{R} \,|\, x \in A\}$ is also a set of measure zero. I feel like this is a very easy proof and I am just missing something.  I don't think stating ""the map $f:A\to A^2$ also maps the collection of intervals covering $A$ to a collection of intervals covering $A^2$"" is complete.  I don't believe this qualifies as a Lipschitz function, since the derivative of the function $f$ is unbounded on $\mathbb{R}$.  Please help, is there something I am missing here?","This question already has answers here : Closed 11 years ago . Possible Duplicate: If $E$ has measure zero, then does $E^2$ have measure zero? I'm trying to find a proof for the following question: Suppose $A\subset\mathbb{R}$ is a set of measure zero.  Show that the set $A^2=\{x^2 \in \mathbb{R} \,|\, x \in A\}$ is also a set of measure zero. I feel like this is a very easy proof and I am just missing something.  I don't think stating ""the map $f:A\to A^2$ also maps the collection of intervals covering $A$ to a collection of intervals covering $A^2$"" is complete.  I don't believe this qualifies as a Lipschitz function, since the derivative of the function $f$ is unbounded on $\mathbb{R}$.  Please help, is there something I am missing here?",,"['real-analysis', 'measure-theory']"
43,Calculate: $\lim_{n\to\infty} \int_{0}^{\pi/2}\frac{1}{1+x\tan^{n} x }dx$,Calculate:,\lim_{n\to\infty} \int_{0}^{\pi/2}\frac{1}{1+x\tan^{n} x }dx,"I'm supposed to work out the following limit: $$\lim_{n\to\infty} \int_{0}^{\pi/2}\frac{1}{1+x \left( \tan x \right)^{n} }dx$$ I'm searching for some resonable solutions. Any hint, suggestion is very welcome. Thanks.","I'm supposed to work out the following limit: $$\lim_{n\to\infty} \int_{0}^{\pi/2}\frac{1}{1+x \left( \tan x \right)^{n} }dx$$ I'm searching for some resonable solutions. Any hint, suggestion is very welcome. Thanks.",,"['real-analysis', 'integration', 'limits']"
44,Are all continuous everywhere but nowhere differentiable functions fractal in nature?,Are all continuous everywhere but nowhere differentiable functions fractal in nature?,,"Functions like the Weierstrass function or van der Waerden's function exhibit self-similar plots. Is this characteristic of continuous everywhere, differentiable nowhere functions? Is there a counterexample to this?","Functions like the Weierstrass function or van der Waerden's function exhibit self-similar plots. Is this characteristic of continuous everywhere, differentiable nowhere functions? Is there a counterexample to this?",,"['real-analysis', 'examples-counterexamples']"
45,"1-Variable Real Analysis True/False questions on Supremum, Infimum, and Inequalities with them","1-Variable Real Analysis True/False questions on Supremum, Infimum, and Inequalities with them",,"(S. Abbott. Understanding Analysis 1 ed. pp 18 question 1.3.9) is asking me to answer the following questions without any formal proofs. I have some intuition for them, but I was hoping to get some external input as well. It would be great if you could maybe give some rigorous explanations to the intuition behind. a) A finite, nonempty set always contains its  supremum. - I think this is True. b) If $a < L$ for every element $a$ in the set $A$, then sup$A < L$. - I think this is False, because it could be $\leq$. c) If $A$ and $B$ are sets with the property that $a < b$ for every $a \in A$, and every $b \in B$, then it follows that sup$A < $ inf$B$. - I think that again, it's $\leq$ and not strictly less. d) If sup$A$ = $s$, and sup$B$ = $t$, then sup($A+B$)=$s+t$. $A+B={a+b | a \in A, b \in B}$. - I think it's True, after thinking of several examples with sets that do and do not contain their suprema, but I am not sure. e) If sup$A \leq$ sup$B$, then there exists an element $b \in B$ that is an upper bound for $A$. - I thought this was False, because we can set $B=A$, and the condition on superma would hold, yet pick such an $A$ that doesn't contain its own supremum. Thank you!","(S. Abbott. Understanding Analysis 1 ed. pp 18 question 1.3.9) is asking me to answer the following questions without any formal proofs. I have some intuition for them, but I was hoping to get some external input as well. It would be great if you could maybe give some rigorous explanations to the intuition behind. a) A finite, nonempty set always contains its  supremum. - I think this is True. b) If $a < L$ for every element $a$ in the set $A$, then sup$A < L$. - I think this is False, because it could be $\leq$. c) If $A$ and $B$ are sets with the property that $a < b$ for every $a \in A$, and every $b \in B$, then it follows that sup$A < $ inf$B$. - I think that again, it's $\leq$ and not strictly less. d) If sup$A$ = $s$, and sup$B$ = $t$, then sup($A+B$)=$s+t$. $A+B={a+b | a \in A, b \in B}$. - I think it's True, after thinking of several examples with sets that do and do not contain their suprema, but I am not sure. e) If sup$A \leq$ sup$B$, then there exists an element $b \in B$ that is an upper bound for $A$. - I thought this was False, because we can set $B=A$, and the condition on superma would hold, yet pick such an $A$ that doesn't contain its own supremum. Thank you!",,['real-analysis']
46,Derivative question,Derivative question,,"I need to find the derivative of: $$ h(x) = \int_{0}^{x^2} (1-t^2)^{1/3} \, dt $$ Would the answer to that just be: $$ (1-x^4)^{1/3}? $$","I need to find the derivative of: $$ h(x) = \int_{0}^{x^2} (1-t^2)^{1/3} \, dt $$ Would the answer to that just be: $$ (1-x^4)^{1/3}? $$",,"['calculus', 'real-analysis', 'integration']"
47,"Finding all linear operators $L: C([0,1]) \to C([0,1])$ which satisfy 2 conditions",Finding all linear operators  which satisfy 2 conditions,"L: C([0,1]) \to C([0,1])","As above, I'm trying to find all linear operators $L: C([0,1]) \to  C([0,1])$ which satisfy the following 2 conditions: I) $Lf \, \geq \, 0$ for all non-negative $f\in C([0,1])$. II) $Lf = f$ for $f(x)= 1$, $f(x)=x$, and $f(x)=x^2$. I'm honestly not sure where to start here - I'm struggling to use these conditions to pare down the class of linear operators which could satisfy the conditions significantly. Could anyone help me get a result out of this? Thank you!","As above, I'm trying to find all linear operators $L: C([0,1]) \to  C([0,1])$ which satisfy the following 2 conditions: I) $Lf \, \geq \, 0$ for all non-negative $f\in C([0,1])$. II) $Lf = f$ for $f(x)= 1$, $f(x)=x$, and $f(x)=x^2$. I'm honestly not sure where to start here - I'm struggling to use these conditions to pare down the class of linear operators which could satisfy the conditions significantly. Could anyone help me get a result out of this? Thank you!",,"['real-analysis', 'functional-analysis']"
48,The asymptotic $\prod_{k=1}^n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t\sim A/(Bn)^n$,The asymptotic,\prod_{k=1}^n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t\sim A/(Bn)^n,"Let $P_f(n)=\prod_{k=1}^n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t$ be the product of integral parts of a function $f$ . For simplicity, assume $f$ is smooth and positive-valued on $(0,1)$ . I noticed some limits in this question and this question can be rephrased as asymptotics: $\displaystyle P_f(n)\sim \frac{2\cos(\frac{\pi}{2\sqrt{3}})}{(en)^n}$ for $f(t)=\sqrt{t(1-t)}$ $\displaystyle P_f(n)\sim \frac{4\cosh^2(\frac{\pi}{2\sqrt{3}})}{(2n)^n}$ for $f(t)=1-\cos(2\pi t)$ $\displaystyle P_f(n)\sim\frac{2}{(e^2n)^n}$ for $f(t)=t(1-t)$ $\displaystyle P_f(n)\sim\frac{2\cosh(\frac{\pi}{2\sqrt{3}})}{(e^2n)^n}$ for $f(t)=t^2$ All of the asymptotics are of the form $A/(Bn)^n$ for some constants $A$ and $B$ . Taking logs, dividing by $n$ , and approximating $n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t\approx f(\frac{2k-1}{2n})$ , we can say $-\ln B$ is approximated by a midpoint Riemann sum yielding, potentially, $$ B=\exp\Big(-\int_0^1\ln f(t)\mathrm{d}t\Big)=\prod_0^1 f(t)^{-\mathrm{d}t}, $$ a product integral . This agrees with the four examples above. But how do we find $A$ ? With the ol' adding-and-subtracting trick, I get $\ln A=X+Y$ where $X$ and $Y$ are given by $$ X=\lim_{n\to\infty} n\Big[-\int_0^1\ln f(t)\mathrm{d}t+\sum_{k=1}^n\frac{1}{n}\ln f\big(\frac{2k-1}{2n}\big)\Big] $$ $$ Y=\lim_{n\to\infty} \sum_{k=1}^n \ln\Big(\frac{\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t}{\frac{1}{n}f(\frac{2k-1}{2n})}\Big) $$ The former seems to be in want of an Euler-Maclaurin formula for product integrals. The latter I assume needs to use Newton-Mercator for $\ln(1+u)$ , but if $f\to 0$ as $t\to0$ or $t\to1$ then I'm not sure $u\to0$ uniformly as $n\to\infty$ . Also possible is expanding $f(t)$ as a Taylor series around $\frac{2k-1}{2n}$ in each integral but I think this will hit the same wall. Ideally it may be possible to find a complete asymptotic series for $\ln P_f(n)$ as $n\to\infty$ with coefficients expressed in terms of $f$ , or at least an algorithm to generate such a series.","Let be the product of integral parts of a function . For simplicity, assume is smooth and positive-valued on . I noticed some limits in this question and this question can be rephrased as asymptotics: for for for for All of the asymptotics are of the form for some constants and . Taking logs, dividing by , and approximating , we can say is approximated by a midpoint Riemann sum yielding, potentially, a product integral . This agrees with the four examples above. But how do we find ? With the ol' adding-and-subtracting trick, I get where and are given by The former seems to be in want of an Euler-Maclaurin formula for product integrals. The latter I assume needs to use Newton-Mercator for , but if as or then I'm not sure uniformly as . Also possible is expanding as a Taylor series around in each integral but I think this will hit the same wall. Ideally it may be possible to find a complete asymptotic series for as with coefficients expressed in terms of , or at least an algorithm to generate such a series.","P_f(n)=\prod_{k=1}^n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t f f (0,1) \displaystyle P_f(n)\sim \frac{2\cos(\frac{\pi}{2\sqrt{3}})}{(en)^n} f(t)=\sqrt{t(1-t)} \displaystyle P_f(n)\sim \frac{4\cosh^2(\frac{\pi}{2\sqrt{3}})}{(2n)^n} f(t)=1-\cos(2\pi t) \displaystyle P_f(n)\sim\frac{2}{(e^2n)^n} f(t)=t(1-t) \displaystyle P_f(n)\sim\frac{2\cosh(\frac{\pi}{2\sqrt{3}})}{(e^2n)^n} f(t)=t^2 A/(Bn)^n A B n n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t\approx f(\frac{2k-1}{2n}) -\ln B  B=\exp\Big(-\int_0^1\ln f(t)\mathrm{d}t\Big)=\prod_0^1 f(t)^{-\mathrm{d}t},  A \ln A=X+Y X Y  X=\lim_{n\to\infty} n\Big[-\int_0^1\ln f(t)\mathrm{d}t+\sum_{k=1}^n\frac{1}{n}\ln f\big(\frac{2k-1}{2n}\big)\Big]   Y=\lim_{n\to\infty} \sum_{k=1}^n \ln\Big(\frac{\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t}{\frac{1}{n}f(\frac{2k-1}{2n})}\Big)  \ln(1+u) f\to 0 t\to0 t\to1 u\to0 n\to\infty f(t) \frac{2k-1}{2n} \ln P_f(n) n\to\infty f","['real-analysis', 'integration', 'numerical-methods', 'asymptotics', 'euler-maclaurin']"
49,Inequality involving $f(x)=e^x-x^2/2$ with $f'(a)=f'(b)$ ($a\ne b$),Inequality involving  with  (),f(x)=e^x-x^2/2 f'(a)=f'(b) a\ne b,"I'm currently working on a high-school level mathematical problem and have developed a solution approach, but I'm stuck at the final step. The problem is as follows: Consider the function $f\left(x\right)=\text{e}^x-\dfrac{x^2}{2}$ . Let $a$ and $b$ be real numbers such that $a\neq b$ and $f^\prime\left(a\right)=f^\prime\left(b\right)$ . Prove that $f\left(a\right)+f\left(b\right)<2$ . And here's my approach (failed): Given that $f'(x) = \text{e}^x - x$ , it implies that $\text{e}^a - a = \text{e}^b - b$ . Let's set $a = \ln m$ and $b = \ln n$ . By symmetry, we can assume $b < 0 < a$ , which leads to $0 < n < 1 < m$ . This results in $m - \ln m = n - \ln n$ . Hence, $m - n = \ln m - \ln n = \ln \dfrac{m}{n}$ . Let's denote $t = \dfrac{m}{n}$ , then $t > 1$ and $m = nt$ . Consequently, $nt - n = \ln t$ , which leads to $n = \dfrac{\ln t}{t - 1}$ and $m = nt = \dfrac{t \ln t}{t - 1}$ . Thus, $f(a) + f(b) = \text{e}^a + \text{e}^b - \dfrac{a^2}{2} - \dfrac{b^2}{2}$ $= m + n - \dfrac{1}{2}\left(\ln^2m + \ln^2n\right)$ $= \dfrac{t + 1}{t - 1} \cdot \ln t - \dfrac{1}{2}\left[\ln^2\left(\dfrac{t \ln t}{t - 1}\right) + \ln^2\left(\dfrac{\ln t}{t - 1}\right)\right]$ Now, the task is to prove that this function of $t$ , denoted as $g(t)$ , is always less than 2. However, I am unable to prove this. Upon graphing, I found that the function is monotonically increasing on the interval $(0, 1)$ , monotonically decreasing on $(1, +\infty)$ , and has a removable discontinuity at the point $(1, 2)$ , with $\lim\limits_{t \to 1} g(t) = 2$ . Here's what GeoGebra gave me on this function I would greatly appreciate any guidance or alternative approaches. This problem has been a challenging puzzle for me, and as a beginner with a strong interest in mathematics, I am eager to learn.","I'm currently working on a high-school level mathematical problem and have developed a solution approach, but I'm stuck at the final step. The problem is as follows: Consider the function . Let and be real numbers such that and . Prove that . And here's my approach (failed): Given that , it implies that . Let's set and . By symmetry, we can assume , which leads to . This results in . Hence, . Let's denote , then and . Consequently, , which leads to and . Thus, Now, the task is to prove that this function of , denoted as , is always less than 2. However, I am unable to prove this. Upon graphing, I found that the function is monotonically increasing on the interval , monotonically decreasing on , and has a removable discontinuity at the point , with . Here's what GeoGebra gave me on this function I would greatly appreciate any guidance or alternative approaches. This problem has been a challenging puzzle for me, and as a beginner with a strong interest in mathematics, I am eager to learn.","f\left(x\right)=\text{e}^x-\dfrac{x^2}{2} a b a\neq b f^\prime\left(a\right)=f^\prime\left(b\right) f\left(a\right)+f\left(b\right)<2 f'(x) = \text{e}^x - x \text{e}^a - a = \text{e}^b - b a = \ln m b = \ln n b < 0 < a 0 < n < 1 < m m - \ln m = n - \ln n m - n = \ln m - \ln n = \ln \dfrac{m}{n} t = \dfrac{m}{n} t > 1 m = nt nt - n = \ln t n = \dfrac{\ln t}{t - 1} m = nt = \dfrac{t \ln t}{t - 1} f(a) + f(b) = \text{e}^a + \text{e}^b - \dfrac{a^2}{2} - \dfrac{b^2}{2} = m + n - \dfrac{1}{2}\left(\ln^2m + \ln^2n\right) = \dfrac{t + 1}{t - 1} \cdot \ln t - \dfrac{1}{2}\left[\ln^2\left(\dfrac{t \ln t}{t - 1}\right) + \ln^2\left(\dfrac{\ln t}{t - 1}\right)\right] t g(t) (0, 1) (1, +\infty) (1, 2) \lim\limits_{t \to 1} g(t) = 2","['real-analysis', 'calculus', 'inequality', 'exponential-function', 'problem-solving']"
50,Estimating $\int_0^1 a^x\ \frac{x(1-x)}{\sin(\pi x)} dx$,Estimating,\int_0^1 a^x\ \frac{x(1-x)}{\sin(\pi x)} dx,"I doubt a closed form exists, so I am trying to approximate the integral: $$I(a)=\int_0^1 a^x\ \frac{x(1-x)}{\sin(\pi x)}  dx$$ I am therefore looking for a function $F(a)$ that provides $$F(a)\simeq I(a)$$ But I want this to be true for every value of $a>0$ , and so something like a truncated Taylor series of $I$ would be useless, since for large $a$ it would require adding more and more terms. Up to this day I found only two promising approximations of $I(a)$ : $\Large{1)}$ Using the inequality (which is a quite good approximation for small values of $x,y$ ) $$\frac{x-y}{\log x-\log y}<\left(\frac{x^{\frac13}+y^{\frac23}}{2} \right)^3 $$ I got $$F_1(a)=\frac{1+a}{8\pi}+\frac{a^{\frac13}+a^{\frac23}}{6\sqrt3}$$ with a percentage error of roughly $1\%$ for small $a$ , growing larger as $a$ grows. $\Large{2)}$ Using the famous ancient approximation for $\sin x$ : $$\sin x \simeq \frac{16(\pi-x)x}{5\pi^2-4(\pi-x)x}$$ I got $$F_2(a)=\frac{a-1}{2\log^3a}-\frac{a+1}{4\log^2a}+\frac{5(a-1)}{16\log a}$$ and this one is tremendously precise, even for very large values of $a$ . However, I am still on the lookout for other, even better, approximations for $I$ . These two are the only ones I was able to get. Regarding the first one, I tried even sharper bounds known for the logarithmic mean, but then I wasn't able to integrate. The generalized mean of exponent $\frac13$ was the best I could get. I like the result because it features only polynomials, no exponentials or logarithms and trig functions. But, since the two means start to differ very fast past around $10$ , the result is good only for small values of $a$ . If someone has an idea to get another approximation I'd be happy to hear it.","I doubt a closed form exists, so I am trying to approximate the integral: I am therefore looking for a function that provides But I want this to be true for every value of , and so something like a truncated Taylor series of would be useless, since for large it would require adding more and more terms. Up to this day I found only two promising approximations of : Using the inequality (which is a quite good approximation for small values of ) I got with a percentage error of roughly for small , growing larger as grows. Using the famous ancient approximation for : I got and this one is tremendously precise, even for very large values of . However, I am still on the lookout for other, even better, approximations for . These two are the only ones I was able to get. Regarding the first one, I tried even sharper bounds known for the logarithmic mean, but then I wasn't able to integrate. The generalized mean of exponent was the best I could get. I like the result because it features only polynomials, no exponentials or logarithms and trig functions. But, since the two means start to differ very fast past around , the result is good only for small values of . If someone has an idea to get another approximation I'd be happy to hear it.","I(a)=\int_0^1 a^x\ \frac{x(1-x)}{\sin(\pi x)}  dx F(a) F(a)\simeq I(a) a>0 I a I(a) \Large{1)} x,y \frac{x-y}{\log x-\log y}<\left(\frac{x^{\frac13}+y^{\frac23}}{2} \right)^3  F_1(a)=\frac{1+a}{8\pi}+\frac{a^{\frac13}+a^{\frac23}}{6\sqrt3} 1\% a a \Large{2)} \sin x \sin x \simeq \frac{16(\pi-x)x}{5\pi^2-4(\pi-x)x} F_2(a)=\frac{a-1}{2\log^3a}-\frac{a+1}{4\log^2a}+\frac{5(a-1)}{16\log a} a I \frac13 10 a","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'improper-integrals']"
51,"Given increasing sequence of numbers, what is guaranteed min length the longest subseq. s.t. differences of terms are either decreasing or increasing?","Given increasing sequence of numbers, what is guaranteed min length the longest subseq. s.t. differences of terms are either decreasing or increasing?",,"It would be better if I could fit ""differences of consecutive terms"" in the title, but I ran out of space. Anyway, here is a more precise version of my question: Given $n,$ for any given distinct real numbers $\ x_1,\ x_2,\ \ldots,\ x_n\ $ such that $\ x_1 < x_2 < \ldots < x_n,\ $ what is the guaranteed minimum length, $\ k,\ $ of the longest (ordered, i.e. increasing) subsequence $\ \left( x_{n_1},\ x_{n_2},\ \ldots,\ x_{n_k} \right)\ $ of $\ (x_1,\ x_2,\ldots,\ x_n),\ $ such that either $\ x_{n_i} - x_{n_{i-1}} \leq x_{n_{i+1}} - x_{n_i}\quad \forall\ 2\leq i\leq k-1,\quad $ or $\quad x_{n_i} - x_{n_{i-1}} \geq x_{n_{i+1}} - x_{n_i}\quad \forall\ 2\leq i\leq k-1?$ For example, if we have $ 2,5,6,9,10,12$ , then the longest subsequence with the desired property has length $4$ . But does every length- $6$ sequence have at least $k=4?$ If so then $k(n=6) = 4.$ The question is, what is the function $k(n)$ like in general?","It would be better if I could fit ""differences of consecutive terms"" in the title, but I ran out of space. Anyway, here is a more precise version of my question: Given for any given distinct real numbers such that what is the guaranteed minimum length, of the longest (ordered, i.e. increasing) subsequence of such that either or For example, if we have , then the longest subsequence with the desired property has length . But does every length- sequence have at least If so then The question is, what is the function like in general?","n, \ x_1,\ x_2,\ \ldots,\ x_n\  \ x_1 < x_2 < \ldots < x_n,\  \ k,\  \ \left( x_{n_1},\ x_{n_2},\ \ldots,\ x_{n_k} \right)\  \ (x_1,\ x_2,\ldots,\ x_n),\  \ x_{n_i} - x_{n_{i-1}} \leq x_{n_{i+1}} - x_{n_i}\quad \forall\ 2\leq i\leq k-1,\quad  \quad x_{n_i} - x_{n_{i-1}} \geq x_{n_{i+1}} - x_{n_i}\quad \forall\ 2\leq i\leq k-1?  2,5,6,9,10,12 4 6 k=4? k(n=6) = 4. k(n)","['real-analysis', 'sequences-and-series', 'inequality', 'pigeonhole-principle', 'ramsey-theory']"
52,How to show $ \|fg\|_{L^2}\leq \|\partial_{x_1}f\|_{L^2}\|g\|_{L^2}+\|f\|_{L^2}\|\partial_{x_2}g\|_{L^2} $?,How to show ?, \|fg\|_{L^2}\leq \|\partial_{x_1}f\|_{L^2}\|g\|_{L^2}+\|f\|_{L^2}\|\partial_{x_2}g\|_{L^2} ,"Let $ f,g\in C_c^1(\mathbb{R}^2) $ , show that $$ \|fg\|_{L^2}\leq \|\partial_{x_1}f\|_{L^2}\|g\|_{L^2}+\|f\|_{L^2}\|\partial_{x_2}g\|_{L^2}. $$ Here is my try. Without loss of generality, we can assume that $ \operatorname{supp}(f,g)\subset\mathbb{R}_+\times\mathbb{R}_+ $ . Then I want to use the fundamental theorem of Calculus to represent $ fg $ by the $ \partial_{x_1}fg $ and $ f\partial_{x_2}g $ . However I have some difficulty in this procedure. Can you give me some references or hints?","Let , show that Here is my try. Without loss of generality, we can assume that . Then I want to use the fundamental theorem of Calculus to represent by the and . However I have some difficulty in this procedure. Can you give me some references or hints?"," f,g\in C_c^1(\mathbb{R}^2)  
\|fg\|_{L^2}\leq \|\partial_{x_1}f\|_{L^2}\|g\|_{L^2}+\|f\|_{L^2}\|\partial_{x_2}g\|_{L^2}.
  \operatorname{supp}(f,g)\subset\mathbb{R}_+\times\mathbb{R}_+   fg   \partial_{x_1}fg   f\partial_{x_2}g ","['real-analysis', 'calculus', 'analysis', 'sobolev-spaces']"
53,"If $f$ is bounded and has bounded derivative on $\mathbb{R}$, then $xf(x)\in L^1\Rightarrow xf(x)\in L^\infty$?","If  is bounded and has bounded derivative on , then ?",f \mathbb{R} xf(x)\in L^1\Rightarrow xf(x)\in L^\infty,"Let $f$ be a differentiable and bounded function on $\mathbb{R}$ which has bounded derivative on $\mathbb{R}$ . Assume that $\int_{\mathbb{R}}|xf(x)|dx<\infty.$ Does this imply that $xf(x)$ is bounded on $\mathbb{R}$ ? I know that in general $g\in L^1$ does not imply $g\in L^\infty$ (e.g., the examples here ), but in this case our function is a product of $2$ ""good"" functions - the function $x$ and a bounded function $f$ which has bounded derivative on $\mathbb{R}$ , so probably in this case the implication $xf(x)\in L^1\Rightarrow xf(x)\in L^\infty$ is correct.","Let be a differentiable and bounded function on which has bounded derivative on . Assume that Does this imply that is bounded on ? I know that in general does not imply (e.g., the examples here ), but in this case our function is a product of ""good"" functions - the function and a bounded function which has bounded derivative on , so probably in this case the implication is correct.",f \mathbb{R} \mathbb{R} \int_{\mathbb{R}}|xf(x)|dx<\infty. xf(x) \mathbb{R} g\in L^1 g\in L^\infty 2 x f \mathbb{R} xf(x)\in L^1\Rightarrow xf(x)\in L^\infty,"['real-analysis', 'lebesgue-integral']"
54,compute the integral $\int_0^1 \int_0^1 \int_0^1 \frac{1}{(1+x^2+y^2+z^2)^2} dxdydz$ [duplicate],compute the integral  [duplicate],\int_0^1 \int_0^1 \int_0^1 \frac{1}{(1+x^2+y^2+z^2)^2} dxdydz,"This question already has answers here : Doubt in solution for evaluating $\int_0^1\int_0^1\int_0^1(1+u^2+v^2+w^2)^{-2}du~dv~dw$. (2 answers) Closed 2 years ago . Determine, with justification, the value of the integral $\int_0^1 \int_0^1 \int_0^1 \frac{1}{(1+x^2+y^2+z^2)^2} dxdydz$ . I tried converting this integral to cylindrical coordinates with $r = \sqrt{x^2 + y^2}$ ranging from $0$ to $\sqrt{2}$ , $0\leq \theta \leq \pi/2, 0\leq z \leq 1,$ where $\theta $ is such that $x= r\cos\theta, y = r\sin\theta.$ However, this seems to lead to an incorrect result. Which bounds have I gotten wrong? Also, it seems that the integral over the unit cube equals twice the integral over the region defined by $0\leq z\leq 1, 0\leq x\leq 1, 0\leq y\leq x,$ but I'm not sure why. The result should be $\frac{\pi^2}{32},$ which is basically what WolframAlpha outputs. Using spherical coordinates seems to make the integration more complicated due to the integration factor.","This question already has answers here : Doubt in solution for evaluating $\int_0^1\int_0^1\int_0^1(1+u^2+v^2+w^2)^{-2}du~dv~dw$. (2 answers) Closed 2 years ago . Determine, with justification, the value of the integral . I tried converting this integral to cylindrical coordinates with ranging from to , where is such that However, this seems to lead to an incorrect result. Which bounds have I gotten wrong? Also, it seems that the integral over the unit cube equals twice the integral over the region defined by but I'm not sure why. The result should be which is basically what WolframAlpha outputs. Using spherical coordinates seems to make the integration more complicated due to the integration factor.","\int_0^1 \int_0^1 \int_0^1 \frac{1}{(1+x^2+y^2+z^2)^2} dxdydz r = \sqrt{x^2 + y^2} 0 \sqrt{2} 0\leq \theta \leq \pi/2, 0\leq z \leq 1, \theta  x= r\cos\theta, y = r\sin\theta. 0\leq z\leq 1, 0\leq x\leq 1, 0\leq y\leq x, \frac{\pi^2}{32},","['real-analysis', 'calculus', 'integration']"
55,Evaluate $\int_0^\infty \sum^{\lfloor x\rfloor}_{n=1}\frac{\sin nx\pi}{x^n}dx$,Evaluate,\int_0^\infty \sum^{\lfloor x\rfloor}_{n=1}\frac{\sin nx\pi}{x^n}dx,"Evaluate $$\int_1^\infty \sum^{\lfloor x\rfloor}_{n=1}\frac{\sin nx\pi}{x^n}dx$$ where $\lfloor \cdot \rfloor$ is the floor function. Okay so this is a problem that really stumbled me. I've had quite a few attempts/thoughts at it though. Tried expanding first to get a sense on what is going on: $$\int_1^\infty\left(\frac{\sin x\pi}{x}+\frac{\sin 2x\pi}{x^2}+\frac{\sin 3x\pi}{x^3}+\dots+\frac{\sin(\lfloor x\rfloor x\pi)}{x^{\lfloor x\rfloor}}\right)dx$$ then I thought I could try individualizing the integral into each? Not sure if that's even possible. And then realised that the solutions to those integrals isn't elementary, they use the $Si$ function. But yeah I'm pretty stuck and unsure if what I've done is even possible.","Evaluate where is the floor function. Okay so this is a problem that really stumbled me. I've had quite a few attempts/thoughts at it though. Tried expanding first to get a sense on what is going on: then I thought I could try individualizing the integral into each? Not sure if that's even possible. And then realised that the solutions to those integrals isn't elementary, they use the function. But yeah I'm pretty stuck and unsure if what I've done is even possible.",\int_1^\infty \sum^{\lfloor x\rfloor}_{n=1}\frac{\sin nx\pi}{x^n}dx \lfloor \cdot \rfloor \int_1^\infty\left(\frac{\sin x\pi}{x}+\frac{\sin 2x\pi}{x^2}+\frac{\sin 3x\pi}{x^3}+\dots+\frac{\sin(\lfloor x\rfloor x\pi)}{x^{\lfloor x\rfloor}}\right)dx Si,"['real-analysis', 'integration', 'sequences-and-series', 'improper-integrals']"
56,Density of a set that is closed under halving and under Euclidean sum,Density of a set that is closed under halving and under Euclidean sum,,"This is a problem that one of my neighbors (a sophomore Physics major) had in her last  Real Analysis test and was not able to solve. She posed it to me and I am also unable to bring and end to it. Suppose $E$ is a non-empty subset of $(0,\infty)$ and that For any $x\in E$ , $\frac{x}{2}\in E$ (i.e. $E$ is closed under halving) For any $x,y\in E$ , $\sqrt{x^2+y^2}\in E$ (i.e. $E$ is closed under Euclidean sum (sorry for the rather pedantic name, that is totally on me.) Problem: Show that that the $\overline{E}=[0,\infty)$ . So far, we can establish the following basic facts: (a) $E$ has arbitrarily small elements ( $x\in E$ implies that $2^{-n}x\in E$ ) (b) For any $x\in E$ and $n\in\mathbb{N}$ , $x\sqrt{n}\in E$ (by induction $x\sqrt{2}=\sqrt{x^2+x^2}\in E$ . Once, we have $x\sqrt{n}\in E$ , then $x\sqrt{n+1}=\sqrt{(\sqrt{n}x)^2+x^2}\in E$ . After this, I can't see how to kill the problem. hopefully other undergraduate students out there (others are of course welcome) may have some hints or a solution to this puppy.","This is a problem that one of my neighbors (a sophomore Physics major) had in her last  Real Analysis test and was not able to solve. She posed it to me and I am also unable to bring and end to it. Suppose is a non-empty subset of and that For any , (i.e. is closed under halving) For any , (i.e. is closed under Euclidean sum (sorry for the rather pedantic name, that is totally on me.) Problem: Show that that the . So far, we can establish the following basic facts: (a) has arbitrarily small elements ( implies that ) (b) For any and , (by induction . Once, we have , then . After this, I can't see how to kill the problem. hopefully other undergraduate students out there (others are of course welcome) may have some hints or a solution to this puppy.","E (0,\infty) x\in E \frac{x}{2}\in E E x,y\in E \sqrt{x^2+y^2}\in E E \overline{E}=[0,\infty) E x\in E 2^{-n}x\in E x\in E n\in\mathbb{N} x\sqrt{n}\in E x\sqrt{2}=\sqrt{x^2+x^2}\in E x\sqrt{n}\in E x\sqrt{n+1}=\sqrt{(\sqrt{n}x)^2+x^2}\in E","['real-analysis', 'general-topology']"
57,Proof that $\mathbb{R}$ is not countable,Proof that  is not countable,\mathbb{R},"I know that this proof may sound ridiculous, but I'm really curious to find out if it's logically correct(and whether there are some circularities). Since $|[0,1]|=|\mathbb{R}|$ , we have  to simply show that $[0,1]$ is not countable. This theorem holds: $f:[a,b]\to \mathbb{R} \\ \text{Let } D_f \text{ be the set of discontinuities of }f \\ |D_f|\leq\aleph_0 \Rightarrow f \text{ is Riemann-integrable in }[a,b] $ Now I can define the Dirichlet function in $[0,1]$ : $\chi(x)=\begin{cases} 1 \ \ \text{if }x\in\mathbb{Q} \\ 0 \ \ \text{otherwise} \end{cases}$ Clearly $D_\chi=[0,1]$ . By contradiction, if $|[0,1]|=\aleph_0$ , then it would mean that $\chi$ is Riemann-integrable in $[0,1]$ , but it's not: this is a contadiction! So $|[0,1]|>\aleph_0$ .","I know that this proof may sound ridiculous, but I'm really curious to find out if it's logically correct(and whether there are some circularities). Since , we have  to simply show that is not countable. This theorem holds: Now I can define the Dirichlet function in : Clearly . By contradiction, if , then it would mean that is Riemann-integrable in , but it's not: this is a contadiction! So .","|[0,1]|=|\mathbb{R}| [0,1] f:[a,b]\to \mathbb{R} \\ \text{Let } D_f \text{ be the set of discontinuities of }f \\ |D_f|\leq\aleph_0 \Rightarrow f \text{ is Riemann-integrable in }[a,b]  [0,1] \chi(x)=\begin{cases} 1 \ \ \text{if }x\in\mathbb{Q} \\ 0 \ \ \text{otherwise} \end{cases} D_\chi=[0,1] |[0,1]|=\aleph_0 \chi [0,1] |[0,1]|>\aleph_0","['real-analysis', 'integration', 'continuity', 'real-numbers', 'cardinals']"
58,Is the space of real analytic functions a freely generated algebra?,Is the space of real analytic functions a freely generated algebra?,,"Let us consider the space $C^{\omega}(\mathbb{R})$ of all the functions $f \colon \mathbb{R} \to \mathbb{R}$ which are analytic on the whole real line. It is clear that $\mathcal{C}^\omega(\mathbb{R})$ is an algebra, because it is closed under addition, multiplication by scalars and inner multiplication. However, is it true that $\mathcal{C}^\omega(\mathbb{R})$ is freely $\mathfrak{c}$ -generated? That is: Is there a set $S \subseteq \mathcal{C}^\omega(\mathbb{R})$ of cardinality $\mathfrak{c}$ such that the algebra generated by $S$ is $\mathcal{C}^\omega(\mathbb{R})$ and whenever a polynomial $P \in \mathbb{R}[X_1, \dotsc, X_p]$ with $P(0, \dotsc, 0) = 0$ satisfies $P(s_1, \dotsc, s_p) = 0$ for some $s_1, \dotsc, s_p \in S,$ then $P = 0?$ If so, how can it be proven?","Let us consider the space of all the functions which are analytic on the whole real line. It is clear that is an algebra, because it is closed under addition, multiplication by scalars and inner multiplication. However, is it true that is freely -generated? That is: Is there a set of cardinality such that the algebra generated by is and whenever a polynomial with satisfies for some then If so, how can it be proven?","C^{\omega}(\mathbb{R}) f \colon \mathbb{R} \to \mathbb{R} \mathcal{C}^\omega(\mathbb{R}) \mathcal{C}^\omega(\mathbb{R}) \mathfrak{c} S \subseteq \mathcal{C}^\omega(\mathbb{R}) \mathfrak{c} S \mathcal{C}^\omega(\mathbb{R}) P \in \mathbb{R}[X_1, \dotsc, X_p] P(0, \dotsc, 0) = 0 P(s_1, \dotsc, s_p) = 0 s_1, \dotsc, s_p \in S, P = 0?",['real-analysis']
59,"$\underset{x\to 1}{\text{lim}}\int_0^x \frac{\sqrt{t} f(t)}{\sqrt{f(x)-f(t)}} \, \mathrm dt=\frac{ \pi }{\sqrt{2}}$",,"\underset{x\to 1}{\text{lim}}\int_0^x \frac{\sqrt{t} f(t)}{\sqrt{f(x)-f(t)}} \, \mathrm dt=\frac{ \pi }{\sqrt{2}}","Define $f(x)=\dfrac{x+1}{(x-1)^2}$ . Prove $\lim\limits_{x \to  1}\displaystyle\int_0^x \dfrac{\sqrt{t} f(t)}{\sqrt{f(x)-f(t)}} \,  {\rm d}t=\dfrac{\pi }{\sqrt{2}}$ . We can obtain $$\lim_{x \to 1}\int_0^x\frac{\sqrt{t}f(t)}{\sqrt{f(x)-f(t)}}{\rm d}t=\lim_{x\to 1}\int_0^1\frac{x\sqrt{xu}f(xu)}{\sqrt{f(x)-f(xu)}}{\rm d}u,$$ but how to go on ?",Define . Prove . We can obtain but how to go on ?,"f(x)=\dfrac{x+1}{(x-1)^2} \lim\limits_{x \to
 1}\displaystyle\int_0^x \dfrac{\sqrt{t} f(t)}{\sqrt{f(x)-f(t)}} \,
 {\rm d}t=\dfrac{\pi }{\sqrt{2}} \lim_{x \to 1}\int_0^x\frac{\sqrt{t}f(t)}{\sqrt{f(x)-f(t)}}{\rm d}t=\lim_{x\to 1}\int_0^1\frac{x\sqrt{xu}f(xu)}{\sqrt{f(x)-f(xu)}}{\rm d}u,","['real-analysis', 'calculus', 'integration', 'limits', 'definite-integrals']"
60,"Measurable function from $[0,1]$ to $[0,1]$, mapping each sub interval onto $[0,1]$","Measurable function from  to , mapping each sub interval onto","[0,1] [0,1] [0,1]","Could anybody give a hint me how to construct a measurable function $f:[0,1]\rightarrow [0,1]$ such that: $$\forall_{0\leq a < b \leq 1}: f((a,b))=[0,1]$$ I tried to define a sequence of linear functions $g_n$ that would ""vertically cover"" more and more of $[0,1]$ like so: $g_0(x)=x$ $g_1(x) = \begin{cases}2x \text{, for } x\in[0,\frac{1}{2}]\\  2x-1 \text{, for } x\in[\frac{1}{2},1]\end{cases}$ $...$ $g_n(x) = \begin{cases}2^nx \text{, for } x\in[0,\frac{1}{2^n}]\\  2^nx-1 \text{, for } x\in[\frac{1}{2^n},\frac{2}{2^n}]\\ ...\\ 2^nx - (2^n-1)\text{, for } x\in[\frac{2^n-1}{2^n},1]\end{cases}$ Then I hoped for such a sequence, of clearly measurable functions, to converge pointwise, so that I could define $f(x)=\lim_{n\rightarrow\infty}g_n(x)$ . It seems however, that $g_n$ will not converge and I am stuck looking for a different sequence. Or perhaps my approach is flawed from the very beginning? I will be gratefull for any inights or suggestions.","Could anybody give a hint me how to construct a measurable function such that: I tried to define a sequence of linear functions that would ""vertically cover"" more and more of like so: Then I hoped for such a sequence, of clearly measurable functions, to converge pointwise, so that I could define . It seems however, that will not converge and I am stuck looking for a different sequence. Or perhaps my approach is flawed from the very beginning? I will be gratefull for any inights or suggestions.","f:[0,1]\rightarrow [0,1] \forall_{0\leq a < b \leq 1}: f((a,b))=[0,1] g_n [0,1] g_0(x)=x g_1(x) = \begin{cases}2x \text{, for } x\in[0,\frac{1}{2}]\\
 2x-1 \text{, for } x\in[\frac{1}{2},1]\end{cases} ... g_n(x) = \begin{cases}2^nx \text{, for } x\in[0,\frac{1}{2^n}]\\
 2^nx-1 \text{, for } x\in[\frac{1}{2^n},\frac{2}{2^n}]\\
...\\
2^nx - (2^n-1)\text{, for } x\in[\frac{2^n-1}{2^n},1]\end{cases} f(x)=\lim_{n\rightarrow\infty}g_n(x) g_n","['real-analysis', 'lebesgue-measure']"
61,Evaluate $\int_0^{\pi} \frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta$,Evaluate,\int_0^{\pi} \frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta,"In the Hardy's book Divergent series is proved the equality $$\frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta=2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \cos n\theta(\cos m\theta-\cos m \phi)$$ With $m\in\mathbb{N}$ . Then integrating that equality $0<\theta<\pi$ (he says ""ignoring any difficulties about the range of $\theta$ over which it may be expected to be valid) we obtain $$\int_0^{\pi} \frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta=\pi \frac{\sin m\phi}{\sin \phi}$$ How that result is reached? I've tried, in a sloppy way (because I didn't proved the possibility to exchange series and integral), this approach with Werner identities $$\int_0^\pi 2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \cos n\theta(\cos m\theta-\cos m \phi) \text{d}\theta=$$ $$2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \int_0^\pi (\cos n\theta \cos m\theta-\cos n\theta\cos m \phi) \text{d}\theta=$$ $$\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \int_0^\pi (\cos (n\theta+m\theta)+\cos (n\theta-m\theta) -\cos (n\theta+m\phi) -\cos (n\theta-m\phi)) \text{d}\theta=$$ $$\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \left[\frac{\sin (n\theta+m\theta)}{n+m}+\frac{\sin(n\theta-m\theta)}{n-m} -\frac{\sin(n\theta+m\phi)}{n} -\frac{\sin(n\theta-m\phi)}{n}\right]_{\theta=0}^{\theta=\pi}=$$ $$\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \left[\frac{\sin (n\pi+m\pi)}{n+m}+\frac{\sin(n\pi-m\pi)}{n-m} -\frac{\sin(n\pi+m\phi)}{n} -\frac{\sin(n\pi-m\phi)}{n}\right]$$ And here I'm stuck. Any help is appreciated.","In the Hardy's book Divergent series is proved the equality With . Then integrating that equality (he says ""ignoring any difficulties about the range of over which it may be expected to be valid) we obtain How that result is reached? I've tried, in a sloppy way (because I didn't proved the possibility to exchange series and integral), this approach with Werner identities And here I'm stuck. Any help is appreciated.",\frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta=2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \cos n\theta(\cos m\theta-\cos m \phi) m\in\mathbb{N} 0<\theta<\pi \theta \int_0^{\pi} \frac{\cos m\theta-\cos m \phi}{\cos \theta - \cos \phi} \text{d}\theta=\pi \frac{\sin m\phi}{\sin \phi} \int_0^\pi 2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \cos n\theta(\cos m\theta-\cos m \phi) \text{d}\theta= 2\sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \int_0^\pi (\cos n\theta \cos m\theta-\cos n\theta\cos m \phi) \text{d}\theta= \sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \int_0^\pi (\cos (n\theta+m\theta)+\cos (n\theta-m\theta) -\cos (n\theta+m\phi) -\cos (n\theta-m\phi)) \text{d}\theta= \sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \left[\frac{\sin (n\theta+m\theta)}{n+m}+\frac{\sin(n\theta-m\theta)}{n-m} -\frac{\sin(n\theta+m\phi)}{n} -\frac{\sin(n\theta-m\phi)}{n}\right]_{\theta=0}^{\theta=\pi}= \sum_{n=1}^{\infty} \frac{\sin n\phi}{\sin \phi} \left[\frac{\sin (n\pi+m\pi)}{n+m}+\frac{\sin(n\pi-m\pi)}{n-m} -\frac{\sin(n\pi+m\phi)}{n} -\frac{\sin(n\pi-m\phi)}{n}\right],"['real-analysis', 'sequences-and-series', 'definite-integrals']"
62,"Is $(X, Y)$ always absolutely continuous with respect to $P_X \otimes P_Y$?",Is  always absolutely continuous with respect to ?,"(X, Y) P_X \otimes P_Y","Definitions: Let $X: (\Omega, \mathcal A) \to (\mathbb R, \mathcal B)$ be a random variable on the probability space $(\Omega, \mathcal A, P)$ and define its distribution as the probability measure $P_X(B) = P(X \in B)$ on $\mathcal B$ . ( $\mathcal B$ is the Borel sigma algebra). A random variable is absolutely continuous with respect to a measure $\mu$ if its distribution is, i.e. $P_X(B)=0$ for all $B \in \mathcal B$ with $\mu(B) = 0.$ $X$ may not be absolutely continuous with respect to the Lebesgue measure $\lambda$ but it is always absolutely continuous with respect to $P_X$ . Now let $(X,Y): (\Omega, \mathcal A) \to (\mathbb R^2, \mathcal B^2)$ be a random vector with distribution $P_{X, Y}((X, Y) \in B)$ , $B \in \mathcal B^2$ . By the same reasoning as before, $(X,Y)$ is abolutely continous with respect to $P_{X, Y}$ even if it is not absolutely continuous with respect to $\lambda^2$ . Question: Will $(X, Y)$ always be absolutely continuous with respect to the product measure $P_X \otimes P_Y$ ? What I did: We need to verify that $P_{X, Y}(B) = 0$ whenever $(P_X \otimes P_Y)(B)=0$ . If $B = B_1 \times B_2$ is a rectangular set then this is clearly true because $(P_X \otimes P_Y)(B)=0$ implies $P_X(B_1) = 0$ or $P_Y(B_2)=0$ ( $P_X(B_1) = 0$ , say) and then $$P_{X, Y}(B) = P((X, Y) \in B_1 \times B_2) = P(X \in B_1, Y \in B_2) \le P(X \in B_1) = 0.$$ But for non-rectangular sets I'm not sure how to proceed.","Definitions: Let be a random variable on the probability space and define its distribution as the probability measure on . ( is the Borel sigma algebra). A random variable is absolutely continuous with respect to a measure if its distribution is, i.e. for all with may not be absolutely continuous with respect to the Lebesgue measure but it is always absolutely continuous with respect to . Now let be a random vector with distribution , . By the same reasoning as before, is abolutely continous with respect to even if it is not absolutely continuous with respect to . Question: Will always be absolutely continuous with respect to the product measure ? What I did: We need to verify that whenever . If is a rectangular set then this is clearly true because implies or ( , say) and then But for non-rectangular sets I'm not sure how to proceed.","X: (\Omega, \mathcal A) \to (\mathbb R, \mathcal B) (\Omega, \mathcal A, P) P_X(B) = P(X \in B) \mathcal B \mathcal B \mu P_X(B)=0 B \in \mathcal B \mu(B) = 0. X \lambda P_X (X,Y): (\Omega, \mathcal A) \to (\mathbb R^2, \mathcal B^2) P_{X, Y}((X, Y) \in B) B \in \mathcal B^2 (X,Y) P_{X, Y} \lambda^2 (X, Y) P_X \otimes P_Y P_{X, Y}(B) = 0 (P_X \otimes P_Y)(B)=0 B = B_1 \times B_2 (P_X \otimes P_Y)(B)=0 P_X(B_1) = 0 P_Y(B_2)=0 P_X(B_1) = 0 P_{X, Y}(B) = P((X, Y) \in B_1 \times B_2) = P(X \in B_1, Y \in B_2) \le P(X \in B_1) = 0.","['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'lebesgue-integral']"
63,Understanding sets with an uncountable number of isolated points in $\mathbb{R}$.,Understanding sets with an uncountable number of isolated points in .,\mathbb{R},"Can someone please verify my proof? Thanks! Prove that a set with an uncountable number of isolated points does not exist in $\mathbb{R}$ . (Added: June 2, 2020) Proof : Let $B = \{x_{1}, x_2, \dots\}$ be the set of isolated points of some set with an uncountable number of isolated points. Since each $x_i$ is an isolated point, $\exists \epsilon_i > 0 \textrm{ s.t. } N(x_i , \epsilon_i) \cap B = \{x_i\}$ which implies $N^*(x_i,  \epsilon_i) \cap B = \emptyset$ We claim that for any two distinct $x_a, x_b \in A$ , $N^{*}(x_a , \frac{\epsilon_a}{2}) \cap N^{*}(x_b , \frac{\epsilon_b}{2}) = \emptyset$ . Assume, to obtain a contradiction, that $\exists z$ satisfying $z \in N^{*}(x_a ; \frac{\epsilon_a}{2}) \cap N^{*}(x_b ; \frac{\epsilon_b}{2})$ . Without loss of generality, suppose $\epsilon_a \geq \epsilon_b$ . Then, \begin{equation*} 	            \left|x_a - x_b\right| = \left|(x_a -z) + (z - x_b)\right| 	             \leq \left|x_a -z \right| + \left|z - x_b\right| < \epsilon_{a/2} + \epsilon_{a/2} = \epsilon_{a} 	        \end{equation*} Thus, $\left|x_a - x_b\right| < \epsilon_{a} \implies x_b \in N^{*}(x_a ; \epsilon_a)$ which contradicts $N^{*}(x_a ; \epsilon_a) \cap B = \emptyset$ . Then, by the density of rationals in reals, we can find a $q_i \in \mathbb{Q}$ s.t. $q_i \in N^*(x_i ; \frac{\epsilon_i}{2})$ . This means that we can draw a $1-1$ correspondence between each $q_{i}$ and $x_{i}$ which is a contradiction since there are only a countable number of $q_{i}$ and an uncountable number of $x_{i}$ .","Can someone please verify my proof? Thanks! Prove that a set with an uncountable number of isolated points does not exist in . (Added: June 2, 2020) Proof : Let be the set of isolated points of some set with an uncountable number of isolated points. Since each is an isolated point, which implies We claim that for any two distinct , . Assume, to obtain a contradiction, that satisfying . Without loss of generality, suppose . Then, Thus, which contradicts . Then, by the density of rationals in reals, we can find a s.t. . This means that we can draw a correspondence between each and which is a contradiction since there are only a countable number of and an uncountable number of .","\mathbb{R} B = \{x_{1}, x_2, \dots\} x_i \exists \epsilon_i > 0 \textrm{ s.t. } N(x_i , \epsilon_i) \cap B = \{x_i\} N^*(x_i,  \epsilon_i) \cap B = \emptyset x_a, x_b \in A N^{*}(x_a , \frac{\epsilon_a}{2}) \cap N^{*}(x_b , \frac{\epsilon_b}{2}) = \emptyset \exists z z \in N^{*}(x_a ; \frac{\epsilon_a}{2}) \cap N^{*}(x_b ; \frac{\epsilon_b}{2}) \epsilon_a \geq \epsilon_b \begin{equation*}
	            \left|x_a - x_b\right| = \left|(x_a -z) + (z - x_b)\right|
	             \leq \left|x_a -z \right| + \left|z - x_b\right| < \epsilon_{a/2} + \epsilon_{a/2} = \epsilon_{a}
	        \end{equation*} \left|x_a - x_b\right| < \epsilon_{a} \implies x_b \in N^{*}(x_a ; \epsilon_a) N^{*}(x_a ; \epsilon_a) \cap B = \emptyset q_i \in \mathbb{Q} q_i \in N^*(x_i ; \frac{\epsilon_i}{2}) 1-1 q_{i} x_{i} q_{i} x_{i}","['real-analysis', 'general-topology', 'solution-verification']"
64,Do the lim sup and lim inf always exist?,Do the lim sup and lim inf always exist?,,"Let's focus on the $\lim \inf$ . While trying to prove that the $\lim \inf$ always exists, [this page] https://mathcs.org/analysis/reals/numseq/proofs/lub_ex.html established that the sequence $A_j = \inf\{a_j, a_{j+1}, \dots\}$ is monotone increasing then the $\lim \inf$ exists even though it may be possibly infinite. My question is that if we define the limit of a sequence as a real number as [the same page] https://mathcs.org/analysis/reals/numseq/sequence.html has done then how do we reconcile this definition to allow for ""a limit that is infinite""?","Let's focus on the . While trying to prove that the always exists, [this page] https://mathcs.org/analysis/reals/numseq/proofs/lub_ex.html established that the sequence is monotone increasing then the exists even though it may be possibly infinite. My question is that if we define the limit of a sequence as a real number as [the same page] https://mathcs.org/analysis/reals/numseq/sequence.html has done then how do we reconcile this definition to allow for ""a limit that is infinite""?","\lim \inf \lim \inf A_j = \inf\{a_j, a_{j+1}, \dots\} \lim \inf","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'limsup-and-liminf']"
65,How fast can the sum of a square-summable sequence grow?,How fast can the sum of a square-summable sequence grow?,,"Suppose $x_t$ is a nonnegative sequence satisfying $$ \sum_{t=1}^{+\infty} x_t^2 < \infty.$$ I am trying to get a precise estimate for how fast $\sum_{t=1}^T x_t$ can grow as a function of $T$ . Application of Cauchy-Schwarz gives that $$\sum_{t=1}^T x_t \leq \sqrt{T} \sqrt{\sum_{t=1}^{+\infty} x_t^2},$$ so $O(\sqrt{T})$ is one upper bound. My question is whether in fact $$ \lim_{T \rightarrow +\infty} \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t = 0.$$ Here is why one might hope that such a thing is true. First, Cauchy-Schwarz is tight when the two vectors are multiples of each other, and since $x_t \rightarrow 0$ , the vector $(x_1, \ldots, x_T)$ is very far from being a multiple of $(1,...,1)$ . Second, if we try to come up with a tight example, the natural guess might be $x_t = 1/(\sqrt{t} \log^c(t))$ for some $c>0$ , since its square is close to being the slowest decaying summable sequence. But in that case $\sum_{t=1}^T x_t = O(\sqrt{T}/\log(T))$ , and the limit is indeed zero.","Suppose is a nonnegative sequence satisfying I am trying to get a precise estimate for how fast can grow as a function of . Application of Cauchy-Schwarz gives that so is one upper bound. My question is whether in fact Here is why one might hope that such a thing is true. First, Cauchy-Schwarz is tight when the two vectors are multiples of each other, and since , the vector is very far from being a multiple of . Second, if we try to come up with a tight example, the natural guess might be for some , since its square is close to being the slowest decaying summable sequence. But in that case , and the limit is indeed zero.","x_t  \sum_{t=1}^{+\infty} x_t^2 < \infty. \sum_{t=1}^T x_t T \sum_{t=1}^T x_t \leq \sqrt{T} \sqrt{\sum_{t=1}^{+\infty} x_t^2}, O(\sqrt{T})  \lim_{T \rightarrow +\infty} \frac{1}{\sqrt{T}} \sum_{t=1}^T x_t = 0. x_t \rightarrow 0 (x_1, \ldots, x_T) (1,...,1) x_t = 1/(\sqrt{t} \log^c(t)) c>0 \sum_{t=1}^T x_t = O(\sqrt{T}/\log(T))","['real-analysis', 'sequences-and-series']"
66,Discontinuities of $\sum_{n=2}^{\infty} \frac{\sin(nx)}{\ln(n)}$,Discontinuities of,\sum_{n=2}^{\infty} \frac{\sin(nx)}{\ln(n)},"Let us consider  the trigonometric series $\phi(x)=\sum_{n=2}^{\infty} \frac{\sin(nx)}{\ln(n)}$ . It is every where convergent. However it is not Lebesgue inntegrable in $[-\pi,\pi]$ . What is the set of continuous points of $\phi$ ?",Let us consider  the trigonometric series . It is every where convergent. However it is not Lebesgue inntegrable in . What is the set of continuous points of ?,"\phi(x)=\sum_{n=2}^{\infty} \frac{\sin(nx)}{\ln(n)} [-\pi,\pi] \phi","['real-analysis', 'fourier-series']"
67,Is there a continuous function such that $f(f(x)) = g(x)$?,Is there a continuous function such that ?,f(f(x)) = g(x),"Let $g:\mathbb{R}\to\mathbb{R}$ be strictly increasing, continuous, and satisfy $g(x) > x$ and $\lim\limits_{x\rightarrow -\infty}g(x) = -\infty $ .  Is there a continuous function $f :\mathbb{R} \rightarrow \mathbb{R} $ such that $f(f(x)) = g(x)$ ? I have noted a few properties about a candidate $f$ ; one must have (1) $f$ is injective (2) $f$ is strictly increasing or strictly decreasing (3) $f$ is surjective We also have that $g$ has a continuous inverse $g^{-1}$ . The following maybe useful; We can define an equivalence relation $\sim$ on $\mathbb{R}$ where $x \sim y$ if $y =g_k(x)$ (here $g_{m+1}(x) = g(g_{m}(x))$ , $g_{m-1}(x) = g^{-1}(g_m(x))$ , $g_{0}(x) = x$ ). One can also observe that for a candidate $f$ that if $f(a) = b$ then $a \nsim b$ as if $f(a) = g_k(a)$ then $f(f(a)) = f(g_k(a))$ or $g_1(a) = f(g_k(a))$ ; hence $f(g_1(a)) = g_{k+1}(a)$ ; thus by two-sided induction we must have for $u \in \mathbb{Z} $ that $f(g_u(a)) = g_{k+u}(a)$ ; hence $f(g_k(a)) = g_{2k}(a)$ but from above $f(g_k(a)) = f(f(a)) = g(a)$ ; hence $g(a) = g_{2k}(a)$ which is impossible as $g$ has no fixed points.","Let be strictly increasing, continuous, and satisfy and .  Is there a continuous function such that ? I have noted a few properties about a candidate ; one must have (1) is injective (2) is strictly increasing or strictly decreasing (3) is surjective We also have that has a continuous inverse . The following maybe useful; We can define an equivalence relation on where if (here , , ). One can also observe that for a candidate that if then as if then or ; hence ; thus by two-sided induction we must have for that ; hence but from above ; hence which is impossible as has no fixed points.",g:\mathbb{R}\to\mathbb{R} g(x) > x \lim\limits_{x\rightarrow -\infty}g(x) = -\infty  f :\mathbb{R} \rightarrow \mathbb{R}  f(f(x)) = g(x) f f f f g g^{-1} \sim \mathbb{R} x \sim y y =g_k(x) g_{m+1}(x) = g(g_{m}(x)) g_{m-1}(x) = g^{-1}(g_m(x)) g_{0}(x) = x f f(a) = b a \nsim b f(a) = g_k(a) f(f(a)) = f(g_k(a)) g_1(a) = f(g_k(a)) f(g_1(a)) = g_{k+1}(a) u \in \mathbb{Z}  f(g_u(a)) = g_{k+u}(a) f(g_k(a)) = g_{2k}(a) f(g_k(a)) = f(f(a)) = g(a) g(a) = g_{2k}(a) g,"['real-analysis', 'continuity', 'functional-equations']"
68,Find an example about the supremum of an uncountable family of real-valued measurable functions need not be measurable,Find an example about the supremum of an uncountable family of real-valued measurable functions need not be measurable,,"Show that by way of an explicit example that the supremum of an uncountable family of real-valued measurable functions need not be measurable. Is there such an example? My solution: Consider Lebesgue measure on $\mathbb{R}$ . If we assume that set $E$ is a non-measurable set(such as Vitali set which is not Lebesgue measurable), then consider the collection of all indicator function of $e\in E$ , that is, A:= $\{1_{e}: e\in E\}$ which is uncountable. In fact, if $E$ is not uncountable, then at most countable set $E$ is measurable. Notice that the the supremum of $A$ is $\mathbb{1}_{E}$ . Indeed, if $\forall x\in E$ , then $\mathbb{1}_{E}(x)=1\geq \mathbb{1}_{e}(x)$ . If $\forall x\notin E$ , then $\mathbb{1}_{E}(x)=0=\mathbb{1}_{e}(x)$ . Also, we claim that if there exists function $f$ such that $f<\mathbb{1}_{E}$ , then $f<\mathbb{1}_{e}$ which means $\mathbb{1}_{E}$ is the supremum of $A$ . Indeed, if $\forall x\in E$ , then $f(x)<\mathbb{1}_{E}(x)=1$ which implies $f(x)<\mathbb{1}_{x}(x)$ . Also, If $\forall x\notin E$ , then $f(x)<\mathbb{1}_{E}(x)=0$ which implies $f(x)\geq 0=\mathbb{1}_{e}$ .","Show that by way of an explicit example that the supremum of an uncountable family of real-valued measurable functions need not be measurable. Is there such an example? My solution: Consider Lebesgue measure on . If we assume that set is a non-measurable set(such as Vitali set which is not Lebesgue measurable), then consider the collection of all indicator function of , that is, A:= which is uncountable. In fact, if is not uncountable, then at most countable set is measurable. Notice that the the supremum of is . Indeed, if , then . If , then . Also, we claim that if there exists function such that , then which means is the supremum of . Indeed, if , then which implies . Also, If , then which implies .",\mathbb{R} E e\in E \{1_{e}: e\in E\} E E A \mathbb{1}_{E} \forall x\in E \mathbb{1}_{E}(x)=1\geq \mathbb{1}_{e}(x) \forall x\notin E \mathbb{1}_{E}(x)=0=\mathbb{1}_{e}(x) f f<\mathbb{1}_{E} f<\mathbb{1}_{e} \mathbb{1}_{E} A \forall x\in E f(x)<\mathbb{1}_{E}(x)=1 f(x)<\mathbb{1}_{x}(x) \forall x\notin E f(x)<\mathbb{1}_{E}(x)=0 f(x)\geq 0=\mathbb{1}_{e},['real-analysis']
69,Is it true that every Banach space has Bolzano Weierstrass property?,Is it true that every Banach space has Bolzano Weierstrass property?,,"Is it true that every bounded sequence in $C[0,1]$ with sup norm has convergent subsequence? I really feel this statement is true and the crux of the proof will lie in that $C[0,1]$ is complete. But I am unable to prove it mathematically by constructing a Cauchy subsequence for any random sequence ( which will eventually be convergent). Also is my observation correct that every Banach space has this property?",Is it true that every bounded sequence in with sup norm has convergent subsequence? I really feel this statement is true and the crux of the proof will lie in that is complete. But I am unable to prove it mathematically by constructing a Cauchy subsequence for any random sequence ( which will eventually be convergent). Also is my observation correct that every Banach space has this property?,"C[0,1] C[0,1]","['real-analysis', 'functional-analysis', 'banach-spaces']"
70,If $f(b)-f(a)=(b-a)f'(\frac{a+b}{2})$ prove that any such function is a polynomial of degree $2$ [duplicate],If  prove that any such function is a polynomial of degree  [duplicate],f(b)-f(a)=(b-a)f'(\frac{a+b}{2}) 2,"This question already has an answer here : Functions satisfying $(b-a)f'(\tfrac{a+b}{2}) = f(b)- f(a)$ (1 answer) Closed 5 years ago . Consider $f:\Bbb R \to \Bbb R$ to be differentiable function. $f(b)-f(a)=(b-a)f'(\frac{a+b}{2})$ for all $a,b \in \Bbb R$ prove that any such function is a polynomial of degree $2$ . I am trying using Taylor's expansion that any point $a$ we have $$f(x)=f(a)+(x-a)f'(a)+(x-a)^2/2f''(a)+\cdots$$ . then I observed that the function is not infinitely differentiable. So we can't use the Taylor Series. So, I don't think that proving that the higher order differentiation vanishes also won't help. We can use Lagrange mean value theorem, but how to proceed? Any hint!!","This question already has an answer here : Functions satisfying $(b-a)f'(\tfrac{a+b}{2}) = f(b)- f(a)$ (1 answer) Closed 5 years ago . Consider to be differentiable function. for all prove that any such function is a polynomial of degree . I am trying using Taylor's expansion that any point we have . then I observed that the function is not infinitely differentiable. So we can't use the Taylor Series. So, I don't think that proving that the higher order differentiation vanishes also won't help. We can use Lagrange mean value theorem, but how to proceed? Any hint!!","f:\Bbb R \to \Bbb R f(b)-f(a)=(b-a)f'(\frac{a+b}{2}) a,b \in \Bbb R 2 a f(x)=f(a)+(x-a)f'(a)+(x-a)^2/2f''(a)+\cdots","['real-analysis', 'calculus', 'analysis', 'functions', 'derivatives']"
71,Will these geometric means always converge to $1/e$?,Will these geometric means always converge to ?,1/e,"Let $p_n$ be the $n$ -th prime and $F_n$ be the $n$ -th Fibonacci number. We have $$ \lim_{n \to \infty}\frac{(p_1 p_2 \ldots p_n)^{1/n}}{p_n}  = \lim_{n \to \infty}\frac{\{\log(F_3)\log(F_4)\ldots \log(F_n)\}^{1/n}}{\log(F_n)} = \frac{1}{e} $$ The first limit was proved by Sandor and Verroken using the prime number theorem and the Chebyshev function. The second limit was proved by Farhadian and Jakimjuk using the Binet's formula for Fibonacci numbers and Stirling's approximation for factorial. Although these two results were proved using different ingredients, their structure is exactly similar which led me to investigate if there is a stronger phenomenon governing such results. My analysis led me to the following. Claim : If $a_n= n^{1+o(1)}$ is increasing then, $ \lim_{n \to \infty}\dfrac{(a_1 a_2\ldots a_n)^{1/n}}{a_n} = \dfrac{1}{e}.$ I believe that I have a proof for this using Weyl's Equidistribution Theorem. I am looking for a simpler or a more elementary proof of this. Also have I got the conditions right for this claim to hold?","Let be the -th prime and be the -th Fibonacci number. We have The first limit was proved by Sandor and Verroken using the prime number theorem and the Chebyshev function. The second limit was proved by Farhadian and Jakimjuk using the Binet's formula for Fibonacci numbers and Stirling's approximation for factorial. Although these two results were proved using different ingredients, their structure is exactly similar which led me to investigate if there is a stronger phenomenon governing such results. My analysis led me to the following. Claim : If is increasing then, I believe that I have a proof for this using Weyl's Equidistribution Theorem. I am looking for a simpler or a more elementary proof of this. Also have I got the conditions right for this claim to hold?","p_n n F_n n 
\lim_{n \to \infty}\frac{(p_1 p_2 \ldots p_n)^{1/n}}{p_n} 
= \lim_{n \to \infty}\frac{\{\log(F_3)\log(F_4)\ldots \log(F_n)\}^{1/n}}{\log(F_n)}
= \frac{1}{e}
 a_n= n^{1+o(1)}  \lim_{n \to \infty}\dfrac{(a_1 a_2\ldots a_n)^{1/n}}{a_n} = \dfrac{1}{e}.","['real-analysis', 'complex-analysis', 'number-theory', 'limits', 'analysis']"
72,compactness of a set where am I going wrong,compactness of a set where am I going wrong,,"I have a proof of the following false fact : Let $E$ be normed vector space. Let $K \subset E$ be a compact set. Then the set $B = \{\lambda x \mid \lambda \in \mathbb{R}^+, x \in K \}$ is closed (where $\mathbb{R}^+$ are the positive real numbers including $0$ ). This fact is true when $0 \not \in K$ yet it can be false when $0 \in K$ . For example by taking the semi-circle in the plane centered at $(1,0)$ of radius $1$ . So I made a proof a of this fact. My proof is thus obviously false yet I don't see where the mistake is : Let $(\lambda_n k_n)$ be a sequence in $B$ which converges to a vector $x \in E$ . We want to prove that $x \in B$ . Since $K$ is compact there is $\phi : \mathbb{N} \to \mathbb{N}$ strictly increasing such that $(k_{\phi(n)})$ converges to a vector $k \in K$ . If $ k = 0$ then the sequence $(\lambda_n k_n)$ converges to $0 \in B$ and we are done. So we can suppose $k \ne 0$ . Since the sequence $(\lambda_n k_n)$ converges to $x$ we must have $k \in span \{ x \}$ . So there is $\mu \in \mathbb{R}^*$ such that $k = \mu x$ . From here we can deduce that the sequence $(\lambda_n)$ necessarily converges to $\frac{1}{\mu}$ . Yet since $\mathbb{R}^+$ is closed the sequence $(\lambda_n)$ converges to a positive real number, so $\frac{1}{\mu} \geq 0$ so $\mu \geq 0$ .  So the sequence $(\lambda_n k_n)$ converges to the vector $\frac{1}{\mu} k \in B$ since $k \in K$ and $\frac{1}{\mu} \geq 0$ . Hence $B$ is closed. So where am I going wrong here ? Thank you !","I have a proof of the following false fact : Let be normed vector space. Let be a compact set. Then the set is closed (where are the positive real numbers including ). This fact is true when yet it can be false when . For example by taking the semi-circle in the plane centered at of radius . So I made a proof a of this fact. My proof is thus obviously false yet I don't see where the mistake is : Let be a sequence in which converges to a vector . We want to prove that . Since is compact there is strictly increasing such that converges to a vector . If then the sequence converges to and we are done. So we can suppose . Since the sequence converges to we must have . So there is such that . From here we can deduce that the sequence necessarily converges to . Yet since is closed the sequence converges to a positive real number, so so .  So the sequence converges to the vector since and . Hence is closed. So where am I going wrong here ? Thank you !","E K \subset E B = \{\lambda x \mid \lambda \in \mathbb{R}^+, x \in K \} \mathbb{R}^+ 0 0 \not \in K 0 \in K (1,0) 1 (\lambda_n k_n) B x \in E x \in B K \phi : \mathbb{N} \to \mathbb{N} (k_{\phi(n)}) k \in K  k = 0 (\lambda_n k_n) 0 \in B k \ne 0 (\lambda_n k_n) x k \in span \{ x \} \mu \in \mathbb{R}^* k = \mu x (\lambda_n) \frac{1}{\mu} \mathbb{R}^+ (\lambda_n) \frac{1}{\mu} \geq 0 \mu \geq 0 (\lambda_n k_n) \frac{1}{\mu} k \in B k \in K \frac{1}{\mu} \geq 0 B","['real-analysis', 'general-topology', 'proof-verification', 'compactness']"
73,$\sum_{n=1}^\infty a_n \cos nx$ unbounded near $0$ if $\sum a_n$ diverges?,unbounded near  if  diverges?,\sum_{n=1}^\infty a_n \cos nx 0 \sum a_n,"If $a_n$ is a decreasing positive sequence and tends to $0$ , and given $$\sum_{n=1}^\infty a_n=+\infty$$ can we prove that $$\lim_{x\rightarrow 0}\sum_{n=1}^{\infty} a_n \cos nx =+\infty$$ or at least prove the series above is unbounded for $x$ in a neighborhood of $0$ ? For the power series $$\sum_{n=1}^\infty a_n \left(1-x\right)^n $$ the conclusion holds, because $\left(1-x\right)^n$ is positive. I wonder if trigonometric series can have the similar conclusion, so I tested several $a_n$ , plotted the graph, and discovered that it is probably true. However, since $\cos nx$ is not identically positive, it is hard to give a rigorous proof, and I cannot give a counter-example either. Anyone has some ideas?","If is a decreasing positive sequence and tends to , and given can we prove that or at least prove the series above is unbounded for in a neighborhood of ? For the power series the conclusion holds, because is positive. I wonder if trigonometric series can have the similar conclusion, so I tested several , plotted the graph, and discovered that it is probably true. However, since is not identically positive, it is hard to give a rigorous proof, and I cannot give a counter-example either. Anyone has some ideas?",a_n 0 \sum_{n=1}^\infty a_n=+\infty \lim_{x\rightarrow 0}\sum_{n=1}^{\infty} a_n \cos nx =+\infty x 0 \sum_{n=1}^\infty a_n \left(1-x\right)^n  \left(1-x\right)^n a_n \cos nx,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'analysis', 'trigonometric-series']"
74,Folland Exercise 3.17,Folland Exercise 3.17,,"Let $(X, \mathcal M, \mu)$ be a finite measure space, $\mathcal N$ a sub- $\sigma$ -algebra of $\mathcal M$ , and $\nu = \mu|\mathcal N$ .  If $f \in L^1(\mu)$ , there exists $g \in L^1(\nu)$ (thus $g$ is $\mathcal N$ -measurable) such that $\int_E f d\mu = \int_E g d\nu$ for all $E \in \mathcal N$ ; if $g'$ is another such function then $g = g'$ $\nu$ -a.e.  (In probability theory, $g$ is called the conditional expectation of $f$ on $\scr{N}$ .) I have managed to prove this statement to be true by defining a measure $\lambda$ such that $d\lambda = gd\nu$ and then using Lebesgue-Radon-Nikodym theorem. Now as an extension of the problem, I want to characterize $g$ in terms of $f$ when $\mathcal N = \{\emptyset, X\}$ , and when $\mathcal N=\{\emptyset, X, E, E^c\}$ for some $E\in\mathcal M$ . Now I'm not sure how to do the last bit, and completely stuck here. I would like to get some help on how to tackle the last part.","Let be a finite measure space, a sub- -algebra of , and .  If , there exists (thus is -measurable) such that for all ; if is another such function then -a.e.  (In probability theory, is called the conditional expectation of on .) I have managed to prove this statement to be true by defining a measure such that and then using Lebesgue-Radon-Nikodym theorem. Now as an extension of the problem, I want to characterize in terms of when , and when for some . Now I'm not sure how to do the last bit, and completely stuck here. I would like to get some help on how to tackle the last part.","(X, \mathcal M, \mu) \mathcal N \sigma \mathcal M \nu = \mu|\mathcal N f \in L^1(\mu) g \in L^1(\nu) g \mathcal N \int_E f d\mu = \int_E g d\nu E \in \mathcal N g' g = g' \nu g f \scr{N} \lambda d\lambda = gd\nu g f \mathcal N = \{\emptyset, X\} \mathcal N=\{\emptyset, X, E, E^c\} E\in\mathcal M","['real-analysis', 'measure-theory']"
75,"Proving that $S=\{ x \in (X,\| \cdot \|) : \|x\| =1 \}$ is a closed set.",Proving that  is a closed set.,"S=\{ x \in (X,\| \cdot \|) : \|x\| =1 \}","Exercise : Show that the unit sphere $$S=\{x \in (X,\|\cdot \|) : \|x\| =1\}$$ of a normed space, is a closed set. Attempt : For a set to be closed, its complement must be an open set. Define the complement of $S$ to be : $$S^c = \{ x \in (X, \|\cdot \|) : \|x\| <1 \}\cup\{x\in (X,\|\cdot\|):\|x\|>1\}$$ Note : We have NOT yet been introduced to handling maps for such proofs in our functional analysis course, so it may not be the best approach for my understanding. Question : How would one proceed with proving the fact stated in the exercise?","Exercise : Show that the unit sphere of a normed space, is a closed set. Attempt : For a set to be closed, its complement must be an open set. Define the complement of to be : Note : We have NOT yet been introduced to handling maps for such proofs in our functional analysis course, so it may not be the best approach for my understanding. Question : How would one proceed with proving the fact stated in the exercise?","S=\{x \in (X,\|\cdot \|) : \|x\| =1\} S S^c = \{ x \in (X, \|\cdot \|) : \|x\| <1 \}\cup\{x\in (X,\|\cdot\|):\|x\|>1\}","['real-analysis', 'general-topology', 'functional-analysis', 'normed-spaces', 'spheres']"
76,"Prove that $\liminf \frac{M_n}{\sqrt{2\log n}}\geq 1$, where $M_n=\max_{1}^n X_i$ and $(X_i)$ is a sequence of i.i.d standard normal random variables","Prove that , where  and  is a sequence of i.i.d standard normal random variables",\liminf \frac{M_n}{\sqrt{2\log n}}\geq 1 M_n=\max_{1}^n X_i (X_i),"Question Let $(X_n)_{n\geq 1}$ be an i.i.d sequence of standard normals. Show that with probability one $\liminf \frac{M_n}{\sqrt{2\log n}}\geq 1$ , where $M_n=\max_{1}^n X_i$ . My attempt Given $\varepsilon >0$ , it suffices to show that $P(\frac{M_n}{\sqrt{2\log n}}<1-\varepsilon \quad \text{i.o.})= 0$ . To this end put $A_n=(\frac{M_n}{\sqrt{2\log n}}<1-\varepsilon)$ and we attempt to use the Borel-Cantelli Lemma. So $$ \sum_1 ^\infty P(A_n)=\sum_{1}^\infty P(X_1<c_n)^n=\sum_{1}^\infty (1-\bar{\Phi} (c_n))^n $$ where $c_n=(1-\varepsilon )(\sqrt{2\log n})$ and $\bar{\Phi}=1-\Phi$ is the survival function. At this point since the normal distribution has no nice form for its cdf I have to use some argument involving asymptotics of this sum. To this end, I know that $$ \bar{\Phi}(x)\sim\frac{\phi (x)}{x}  $$ as $x\to \infty$ where $\phi$ is the density of a standard normal. More precisely, $$ \left(\frac{1}{x}-\frac{1}{x^3}\right)\leq \frac{\bar{\Phi}(x)}{\phi(x)}\leq \frac{1}{x}.\tag{1} $$ We can write $$ \sum_{1}^\infty P(X_1<c_n)^n\leq \sum_{1}^\infty(1-(c_n^{-1}-c_n^{-3})\phi(c_n))^n $$ using $1$ , but I am not sure how to argue that this is finite.","Question Let be an i.i.d sequence of standard normals. Show that with probability one , where . My attempt Given , it suffices to show that . To this end put and we attempt to use the Borel-Cantelli Lemma. So where and is the survival function. At this point since the normal distribution has no nice form for its cdf I have to use some argument involving asymptotics of this sum. To this end, I know that as where is the density of a standard normal. More precisely, We can write using , but I am not sure how to argue that this is finite.","(X_n)_{n\geq 1} \liminf \frac{M_n}{\sqrt{2\log n}}\geq 1 M_n=\max_{1}^n X_i \varepsilon >0 P(\frac{M_n}{\sqrt{2\log n}}<1-\varepsilon \quad \text{i.o.})= 0 A_n=(\frac{M_n}{\sqrt{2\log n}}<1-\varepsilon) 
\sum_1 ^\infty P(A_n)=\sum_{1}^\infty P(X_1<c_n)^n=\sum_{1}^\infty (1-\bar{\Phi} (c_n))^n
 c_n=(1-\varepsilon )(\sqrt{2\log n}) \bar{\Phi}=1-\Phi 
\bar{\Phi}(x)\sim\frac{\phi (x)}{x} 
 x\to \infty \phi 
\left(\frac{1}{x}-\frac{1}{x^3}\right)\leq \frac{\bar{\Phi}(x)}{\phi(x)}\leq \frac{1}{x}.\tag{1}
 
\sum_{1}^\infty P(X_1<c_n)^n\leq \sum_{1}^\infty(1-(c_n^{-1}-c_n^{-3})\phi(c_n))^n
 1","['real-analysis', 'probability', 'probability-theory', 'convergence-divergence', 'borel-cantelli-lemmas']"
77,"Differentiation operator is closed on $L^2[0,1]$?",Differentiation operator is closed on ?,"L^2[0,1]","Proposition. Let $\mathcal{H}=L^2[0,1]$ with a differentiation operator $T=i\frac{d}{dt}$ on it, whose domain $D(T)$ consists of all the absolutely continuous functions in $\mathcal{H}$, and the derivatives of those functions is still $L^2$. Then $T$ is a closed operator. (which means its graph is closed in $\mathcal{H}\times\mathcal{H}$) Here is my try so far. Let $\{u_n\}$ be a sequence of $L^2$ functions in $\mathcal{H}$ converging to $u\in\mathcal{H}$. Also, $Tu_n$ converge to another $v\in\mathcal{H}$. It suffices to show that $Tu=v$, which is to say, $\| Tu-v\|=0$. Since $$\|Tu-v\|\le\|Tu-Tu_n\|+\|Tu_n-v\|$$ and $\|Tu_n-v\|\rightarrow 0$, we only need to prove that $\|Tu-Tu_n\|\rightarrow 0$. $$\lim_{n\rightarrow\infty}\|Tu-Tu_n\|^2=\lim_{n\rightarrow\infty}\int \left|i\frac{d}{dt}(u-u_n)\right|^2dx.$$ Ideally, we “may” swap the “limits” with the “integral” and “derivative” and the “proof” is finished. However, it seems to me not very clear that theorems from real analysis allow me to do so. I’ve been stuck here for a long time. Any hints?","Proposition. Let $\mathcal{H}=L^2[0,1]$ with a differentiation operator $T=i\frac{d}{dt}$ on it, whose domain $D(T)$ consists of all the absolutely continuous functions in $\mathcal{H}$, and the derivatives of those functions is still $L^2$. Then $T$ is a closed operator. (which means its graph is closed in $\mathcal{H}\times\mathcal{H}$) Here is my try so far. Let $\{u_n\}$ be a sequence of $L^2$ functions in $\mathcal{H}$ converging to $u\in\mathcal{H}$. Also, $Tu_n$ converge to another $v\in\mathcal{H}$. It suffices to show that $Tu=v$, which is to say, $\| Tu-v\|=0$. Since $$\|Tu-v\|\le\|Tu-Tu_n\|+\|Tu_n-v\|$$ and $\|Tu_n-v\|\rightarrow 0$, we only need to prove that $\|Tu-Tu_n\|\rightarrow 0$. $$\lim_{n\rightarrow\infty}\|Tu-Tu_n\|^2=\lim_{n\rightarrow\infty}\int \left|i\frac{d}{dt}(u-u_n)\right|^2dx.$$ Ideally, we “may” swap the “limits” with the “integral” and “derivative” and the “proof” is finished. However, it seems to me not very clear that theorems from real analysis allow me to do so. I’ve been stuck here for a long time. Any hints?",,"['real-analysis', 'functional-analysis']"
78,open set is the disjoint union of a countable collection of open intervals,open set is the disjoint union of a countable collection of open intervals,,"I don't understand that $\{I_x\}_{x\in O}$ is disjoint. For example, $O = \{(1 , 2)\}$ . Let $x = 1.5$ . Then, $a_x = 1$ , and $b_x = 2$ . Therefore, $I_x = (1, 2)$ . Let $y = 1.6$ . Then, similarly, $I_y = (1, 2)$ . That is, $I_x$ and $I_y$ are not disjoint, but equal. Could you explain how $\{I_x\}_{x \in O}$ can be disjoint?","I don't understand that is disjoint. For example, . Let . Then, , and . Therefore, . Let . Then, similarly, . That is, and are not disjoint, but equal. Could you explain how can be disjoint?","\{I_x\}_{x\in O} O = \{(1 , 2)\} x = 1.5 a_x = 1 b_x = 2 I_x = (1, 2) y = 1.6 I_y = (1, 2) I_x I_y \{I_x\}_{x \in O}",['real-analysis']
79,The convergence in Lebesgue differentiation theorem,The convergence in Lebesgue differentiation theorem,,"Let $f$ be a function on $\mathbb R$ such that $f$ is locally integrable. It is well known that from the Lebesgue differentiation theorem we have $$ \frac{1}{h}\int_t^{t+h} u(s)\,ds \to u(t) $$ almost everywhere if $h \to 0$. My question is, can we prove that the convergence is in $L^1_{loc}$ ?","Let $f$ be a function on $\mathbb R$ such that $f$ is locally integrable. It is well known that from the Lebesgue differentiation theorem we have $$ \frac{1}{h}\int_t^{t+h} u(s)\,ds \to u(t) $$ almost everywhere if $h \to 0$. My question is, can we prove that the convergence is in $L^1_{loc}$ ?",,['real-analysis']
80,Prove that a power series that is zero on a sequence that converges to zero is the zero function,Prove that a power series that is zero on a sequence that converges to zero is the zero function,,"I've been trying to solve this problem from Abbott's Understanding Analysis for hours and can't seem to get the last piece of the proof. Let $g(x) = \sum_{n=1}^\infty b_n x^n$ be a power series that converges for all $x \in (-R,R)$. Let $x_n \rightarrow 0$, while $x_n \neq 0$, and $g(x_n) = 0$ for every $n$. Show that $g(x)$ must be identically zero on $(-R,R)$. I know that $g(0) = 0$ since $g$ is continuous and the set of zeros of a continuous function is closed. I know that $g'(0) = b_1$, and I need to show that this is zero. I know that if $f(x) = 0$ on an interval, then $f'(x) = 0$ on that interval as well, but I can't seem to find a parallel of that result to this problem.","I've been trying to solve this problem from Abbott's Understanding Analysis for hours and can't seem to get the last piece of the proof. Let $g(x) = \sum_{n=1}^\infty b_n x^n$ be a power series that converges for all $x \in (-R,R)$. Let $x_n \rightarrow 0$, while $x_n \neq 0$, and $g(x_n) = 0$ for every $n$. Show that $g(x)$ must be identically zero on $(-R,R)$. I know that $g(0) = 0$ since $g$ is continuous and the set of zeros of a continuous function is closed. I know that $g'(0) = b_1$, and I need to show that this is zero. I know that if $f(x) = 0$ on an interval, then $f'(x) = 0$ on that interval as well, but I can't seem to find a parallel of that result to this problem.",,"['real-analysis', 'sequences-and-series', 'power-series']"
81,"Which functions can be approximated by Hölder continuous functions in a ""bounded way""?","Which functions can be approximated by Hölder continuous functions in a ""bounded way""?",,"I'm interested in the space of functions $f: \mathbb{R}^d \to \mathbb{R}$ satisfying the following property: There exist a sequence $(\alpha_n)_{n \in \mathbb{N}} \subseteq (0,1]$ and functions $f_n$ which are Hölder continuous with Hölder exponent $\alpha_n$ such that $$f_n(x) \xrightarrow[]{n \to \infty} f(x) \quad \text{for (Lebesgue)almost all $x \in \mathbb{R}^d$} \tag{1}$$ and $$\sup_{n \geq 1} \|f_n\|_{\alpha_n} < \infty. \tag{2}$$ Here, $\|\cdot\|_{\alpha}$ denotes the $\alpha$-Hölder norm, i.e. $$\|g\|_{\alpha} := \sup_x |g(x)| + \sup_{x \neq y} \frac{|g(x)-g(y)|}{|x-y|^{\alpha}}.$$ Edit: Note that $\alpha_n$ might tend to $0$, and therefore $(2)$ does not imply $$\sup_{n \geq 1} \|f_n\|_{\alpha} < \infty$$ for some $\alpha>0$. Obviously, any function $f$ which is Hölder continuous has the above property. The interesting things happen if $\alpha_n \to 0$ for $n \to \infty$. For instance, there are discontinuous functions which can be approximated in the above sense; e.g. the Heaviside function $$f(x) = 1_{(0,\infty)}(x)$$ can be approximated by $$f_n(x) := 1_{(0,\infty)}(x) \cdot \min\{|x|^{1/n},1\}.$$ I'm aware of the fact that Sobolev functions can be approximated by Hölder continuous mappings, but the results, which I know, do not provide a uniform bound $(2)$. I'm wondering how ""big"" the above defined space of functions is, i.e. which known function spaces are (not) contained in it. I would be very happy about references and your thoughts on the problem.","I'm interested in the space of functions $f: \mathbb{R}^d \to \mathbb{R}$ satisfying the following property: There exist a sequence $(\alpha_n)_{n \in \mathbb{N}} \subseteq (0,1]$ and functions $f_n$ which are Hölder continuous with Hölder exponent $\alpha_n$ such that $$f_n(x) \xrightarrow[]{n \to \infty} f(x) \quad \text{for (Lebesgue)almost all $x \in \mathbb{R}^d$} \tag{1}$$ and $$\sup_{n \geq 1} \|f_n\|_{\alpha_n} < \infty. \tag{2}$$ Here, $\|\cdot\|_{\alpha}$ denotes the $\alpha$-Hölder norm, i.e. $$\|g\|_{\alpha} := \sup_x |g(x)| + \sup_{x \neq y} \frac{|g(x)-g(y)|}{|x-y|^{\alpha}}.$$ Edit: Note that $\alpha_n$ might tend to $0$, and therefore $(2)$ does not imply $$\sup_{n \geq 1} \|f_n\|_{\alpha} < \infty$$ for some $\alpha>0$. Obviously, any function $f$ which is Hölder continuous has the above property. The interesting things happen if $\alpha_n \to 0$ for $n \to \infty$. For instance, there are discontinuous functions which can be approximated in the above sense; e.g. the Heaviside function $$f(x) = 1_{(0,\infty)}(x)$$ can be approximated by $$f_n(x) := 1_{(0,\infty)}(x) \cdot \min\{|x|^{1/n},1\}.$$ I'm aware of the fact that Sobolev functions can be approximated by Hölder continuous mappings, but the results, which I know, do not provide a uniform bound $(2)$. I'm wondering how ""big"" the above defined space of functions is, i.e. which known function spaces are (not) contained in it. I would be very happy about references and your thoughts on the problem.",,"['real-analysis', 'functional-analysis', 'holder-spaces']"
82,Riemann-Stieltjes Integral with respect to total variation,Riemann-Stieltjes Integral with respect to total variation,,"In this Inequality for Riemann-Stieltjes integral the following question came up. Suppose functions $f,g:[a,b] \to \mathbb{R}$ are such that $f$ is Riemann-Stieltjes integrable with respect to $g$. Suppose $g$ has bounded variation and $v_a^x(g)$ is the total variation on interval $[a,x]$. Is it true that $f$ is integrable with respect to $v_a^x(g)$ (even if $f$ is not continuous)? If true how could this be proved?","In this Inequality for Riemann-Stieltjes integral the following question came up. Suppose functions $f,g:[a,b] \to \mathbb{R}$ are such that $f$ is Riemann-Stieltjes integrable with respect to $g$. Suppose $g$ has bounded variation and $v_a^x(g)$ is the total variation on interval $[a,x]$. Is it true that $f$ is integrable with respect to $v_a^x(g)$ (even if $f$ is not continuous)? If true how could this be proved?",,"['real-analysis', 'integration', 'bounded-variation', 'stieltjes-integral']"
83,A matter of choice of stating the domain of the function in analysis/topology textbooks - narrower or broader at first?,A matter of choice of stating the domain of the function in analysis/topology textbooks - narrower or broader at first?,,"It is customary to see in mathematical analysis that the domain of the function in definition or theorem would require as ""small"" as possible. For example, recall that the Extreme Value Theorem often phrased like this: Way 1: Let $f:[a,b]\to\Bbb R$ . If $f$ is continuous, then $f$ has a max/min on $[a,b]$ . In contrast, authors usually don't like to state as: Way 2: Let $E\subseteq \Bbb R$ , $f:E\to\Bbb R$ and $[a,b]\subseteq E$ , if $f$ is continuous on $[a,b]$ , then $f$ has a max/min on $[a,b]$ (Notice that in this case, the domain $E$ of the function in the context can be a ""fragile"" set, such like $(1,5)\cup (10,15)\cup (20,24)$ .) Now, if we have a function $f:(1,5)\cup (10,15)\cup (20,24)\to \Bbb R$ at hand, then does it have a max/min on $[2,4]$ ? Yes, but given different description of theorem, we have different argument. If we adopt the first one, we would say: because $f\vert_{[2,4]}$ is a function that defined on a closed bounded set $[a,b]$ , so by the theorem, $f\vert_{[2,4]}$ has a max/min on $[2,4]$ . However, if we adopt the second one, we would say: since $[2,4]\subseteq\text{dom} (f)$ , by the theorem, $f$ has a max/min on $[2,4]$ (we don't need to make the restriction here, since this precisely suite the theorem itself this time). One more example, in defining Riemann integrability, people always stated the hyphothesis as Let $f:[a,b]\to\Bbb R$ , $f$ is called integrable if ... rather than Let $E\subseteq\Bbb R$ , $f:E\to\Bbb R$ , and $[a,b]\subseteq E$ , $f$ is called integrable on $[a,b]$ if ... Next, below is the definition of analyticity of a point of a function from Terrence Tao's Analysis II . Definition: Let $I$ be an open interval, $c\in I$ and $f:I\to\Bbb R$ . We say that $f$ is analytic at $c$ if there exists $\delta>0$ such that $(c-\delta,c+\delta)\subseteq I$ and there exists a power   series $\sum a_n(x-c)^n$ with radius of convergence $R\geq \delta$ ,   such that $f(x)=\sum a_n(x-c)^n$ for all $x\in(c-\delta,c+\delta)$ . If we have a function $f:[0,1)\cup [10,20)\cup [40,50]\to \Bbb R$ , which defines by $f(x)=\sin x$ . Notice that this definition from Tao is similar to the type 1 way, which means he requires the domain of the fuction in interest as minimal as possible. (Though this time it is a definition, rather than EVT, which is a theorem. However, doesn't matter for our discussion here.)  We all know that $f$ is definitely ""very good"" to be ""analytic"" at every point(interior point). However, rigorous speaking, it seems that we cannot simply say that $f$ is analytic at, say, the point $0.5$ , since this does not suit the exact pattern that the definition just said! If we truly want to say something about the analyticity of $f$ at $0.5$ , we must make a restriction by ourselves first, then say something like $f\vert_{(0.3,0.6)}$ is analytic at $0.5$ , which is very burdensome. And that's why I post this. Is their any better way to say about this? Or should we always adopt the second way to phrase the definitions and theorems? Any suggestions?","It is customary to see in mathematical analysis that the domain of the function in definition or theorem would require as ""small"" as possible. For example, recall that the Extreme Value Theorem often phrased like this: Way 1: Let . If is continuous, then has a max/min on . In contrast, authors usually don't like to state as: Way 2: Let , and , if is continuous on , then has a max/min on (Notice that in this case, the domain of the function in the context can be a ""fragile"" set, such like .) Now, if we have a function at hand, then does it have a max/min on ? Yes, but given different description of theorem, we have different argument. If we adopt the first one, we would say: because is a function that defined on a closed bounded set , so by the theorem, has a max/min on . However, if we adopt the second one, we would say: since , by the theorem, has a max/min on (we don't need to make the restriction here, since this precisely suite the theorem itself this time). One more example, in defining Riemann integrability, people always stated the hyphothesis as Let , is called integrable if ... rather than Let , , and , is called integrable on if ... Next, below is the definition of analyticity of a point of a function from Terrence Tao's Analysis II . Definition: Let be an open interval, and . We say that is analytic at if there exists such that and there exists a power   series with radius of convergence ,   such that for all . If we have a function , which defines by . Notice that this definition from Tao is similar to the type 1 way, which means he requires the domain of the fuction in interest as minimal as possible. (Though this time it is a definition, rather than EVT, which is a theorem. However, doesn't matter for our discussion here.)  We all know that is definitely ""very good"" to be ""analytic"" at every point(interior point). However, rigorous speaking, it seems that we cannot simply say that is analytic at, say, the point , since this does not suit the exact pattern that the definition just said! If we truly want to say something about the analyticity of at , we must make a restriction by ourselves first, then say something like is analytic at , which is very burdensome. And that's why I post this. Is their any better way to say about this? Or should we always adopt the second way to phrase the definitions and theorems? Any suggestions?","f:[a,b]\to\Bbb R f f [a,b] E\subseteq \Bbb R f:E\to\Bbb R [a,b]\subseteq E f [a,b] f [a,b] E (1,5)\cup (10,15)\cup (20,24) f:(1,5)\cup (10,15)\cup (20,24)\to \Bbb R [2,4] f\vert_{[2,4]} [a,b] f\vert_{[2,4]} [2,4] [2,4]\subseteq\text{dom} (f) f [2,4] f:[a,b]\to\Bbb R f E\subseteq\Bbb R f:E\to\Bbb R [a,b]\subseteq E f [a,b] I c\in I f:I\to\Bbb R f c \delta>0 (c-\delta,c+\delta)\subseteq I \sum a_n(x-c)^n R\geq \delta f(x)=\sum a_n(x-c)^n x\in(c-\delta,c+\delta) f:[0,1)\cup [10,20)\cup [40,50]\to \Bbb R f(x)=\sin x f f 0.5 f 0.5 f\vert_{(0.3,0.6)} 0.5","['real-analysis', 'analysis', 'functions', 'metric-spaces', 'convention']"
84,Lebesgue Spaces and Integration by parts,Lebesgue Spaces and Integration by parts,,"Suppose there exists a Lebesgue Space, $L_1$ and functions functions $\phi$ , $\phi'$ , $f$ , and $f'$ functions where $$\phi, \phi' \in L_1$$ By rule of integration by parts, $$uv|_a^b = \int_a^b udv + \int_a^b vdu$$ Let $$ u = \phi, du= \phi'$$ $$ v = f, dv = f'$$ Are there any properties of Lebesgue functions that allow $$ uv|_a^b = 0$$ Are $\phi$ and $\phi'$ convergent as integrals? Do the unbounded limits of $\phi$ and $\phi'$ converge?","Suppose there exists a Lebesgue Space, and functions functions , , , and functions where By rule of integration by parts, Let Are there any properties of Lebesgue functions that allow Are and convergent as integrals? Do the unbounded limits of and converge?","L_1 \phi \phi' f f' \phi, \phi' \in L_1 uv|_a^b = \int_a^b udv + \int_a^b vdu  u = \phi, du= \phi'  v = f, dv = f'  uv|_a^b = 0 \phi \phi' \phi \phi'","['real-analysis', 'integration', 'analysis', 'lebesgue-integral']"
85,$f$ Holder continuous with Holder exponent $p>1\implies f \text{ is constant}$,Holder continuous with Holder exponent,f p>1\implies f \text{ is constant},"Say I have a function on a an interval $I$ in $\mathbb{R}$ $f: I \to Y  $ where $Y$ is any metric space.Say $f$ satisfies $d_{Y}(f(y), f(x)) \leq  C\cdot|y-x|^p$, for all $y,x \in I$ where $p \in (1,\infty)$ , i.e. $p$ is bigger than $1$ (so this is much stronger than mere Holder Continuity). I want to show that $f$ is constant on $I$. Here is what I got so far. $f$ is obviously continuous. Also, it makes intuitive sense for $f$ to be constant, since $p >1$ will make $|y-x|^p$ very small for for $|y-x| \ll 1$. I also feel like I have to look at the expression  $$\frac{d_{Y}(f(y), f(x))}{|y-x|} \leq  C\cdot|y-x|^{p-1}$$ (Note: There is no notion of differentiability here). I'm also thinking that splitting up the interval $[x,y]$ (assuming $x<y$) will help me like so: $$d_{Y}(f(y), f(x))\leq\sum_{k=1}^{k=n}d_{Y}(f(x_k), f(x_{k-1})) \leq C\cdot\sum_{k=1}^{n}  |x_k-x_{k-1}|^p = \sum_{k=1}^{n}  (\frac{1}{n})^p = n^{1-p} $$ $ (y = x_n, x = x_0)$. Is this the answer, that last equality chain? Edit:  That last chain of equations, as suggested in the comments below should be $$d_{Y}(f(y), f(x))\leq\sum_{k=1}^{k=n}d_{Y}(f(x_k), f(x_{k-1})) \leq C\cdot\sum_{k=1}^{n}  |x_k-x_{k-1}|^p $$ $$= C\sum_{k=1}^{n} (\frac{|y-x|}{n})^p =|y-x|^p n^{1-p} $$","Say I have a function on a an interval $I$ in $\mathbb{R}$ $f: I \to Y  $ where $Y$ is any metric space.Say $f$ satisfies $d_{Y}(f(y), f(x)) \leq  C\cdot|y-x|^p$, for all $y,x \in I$ where $p \in (1,\infty)$ , i.e. $p$ is bigger than $1$ (so this is much stronger than mere Holder Continuity). I want to show that $f$ is constant on $I$. Here is what I got so far. $f$ is obviously continuous. Also, it makes intuitive sense for $f$ to be constant, since $p >1$ will make $|y-x|^p$ very small for for $|y-x| \ll 1$. I also feel like I have to look at the expression  $$\frac{d_{Y}(f(y), f(x))}{|y-x|} \leq  C\cdot|y-x|^{p-1}$$ (Note: There is no notion of differentiability here). I'm also thinking that splitting up the interval $[x,y]$ (assuming $x<y$) will help me like so: $$d_{Y}(f(y), f(x))\leq\sum_{k=1}^{k=n}d_{Y}(f(x_k), f(x_{k-1})) \leq C\cdot\sum_{k=1}^{n}  |x_k-x_{k-1}|^p = \sum_{k=1}^{n}  (\frac{1}{n})^p = n^{1-p} $$ $ (y = x_n, x = x_0)$. Is this the answer, that last equality chain? Edit:  That last chain of equations, as suggested in the comments below should be $$d_{Y}(f(y), f(x))\leq\sum_{k=1}^{k=n}d_{Y}(f(x_k), f(x_{k-1})) \leq C\cdot\sum_{k=1}^{n}  |x_k-x_{k-1}|^p $$ $$= C\sum_{k=1}^{n} (\frac{|y-x|}{n})^p =|y-x|^p n^{1-p} $$",,"['real-analysis', 'continuity', 'holder-spaces']"
86,If $f(0) = 0$ and $|f'(x)|\leq |f(x)|$ for all $x\in\mathbb{R}$ then $f\equiv 0$ [duplicate],If  and  for all  then  [duplicate],f(0) = 0 |f'(x)|\leq |f(x)| x\in\mathbb{R} f\equiv 0,"This question already has answers here : Prove $f(x) = 0$ for all $x \in [0, \infty)$ when $|f'(x)| \leq |f(x)|$ (3 answers) Closed 6 years ago . Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous and differentiable function in all $\mathbb{R}$. If $f(0)=0$ and $|f'(x)|\leq |f(x)|$ for all  $x\in\mathbb{R}$, then $f\equiv 0$. I've been trying to prove this using the Mean Value Theorem, but I can't get to the result. Can someone help?","This question already has answers here : Prove $f(x) = 0$ for all $x \in [0, \infty)$ when $|f'(x)| \leq |f(x)|$ (3 answers) Closed 6 years ago . Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous and differentiable function in all $\mathbb{R}$. If $f(0)=0$ and $|f'(x)|\leq |f(x)|$ for all  $x\in\mathbb{R}$, then $f\equiv 0$. I've been trying to prove this using the Mean Value Theorem, but I can't get to the result. Can someone help?",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
87,Is this $C^\infty-$function analytic?,Is this function analytic?,C^\infty-,"Consider a $C^\infty-$function $\,f$ on $[a, b]$. All of its derivatives are non-negative. I am trying to prove it is an analytic function. I have tried to calculate its Taylor remainder  $$R_n = \int_a^x \frac{f^{(n+1)}(t)}{(n+1)!} (x-t)^{n+1} d t. $$  But I have difficulty estimating the bound. Thanks for any help in advance.","Consider a $C^\infty-$function $\,f$ on $[a, b]$. All of its derivatives are non-negative. I am trying to prove it is an analytic function. I have tried to calculate its Taylor remainder  $$R_n = \int_a^x \frac{f^{(n+1)}(t)}{(n+1)!} (x-t)^{n+1} d t. $$  But I have difficulty estimating the bound. Thanks for any help in advance.",,"['real-analysis', 'sequences-and-series', 'power-series', 'taylor-expansion', 'analyticity']"
88,Calculate $\lim_{n \to \infty} n(1+\sin(n))$,Calculate,\lim_{n \to \infty} n(1+\sin(n)),I think it doesn't exists but I can't find out how to prove it; note that $n$ is integer. I know that there are infinite many integers $a_n$ s.t. $\sin(a_n)>0$ and then limsup is $+ \infty$. I want to prove that liminf $\ne +\infty$ but I don't know how.,I think it doesn't exists but I can't find out how to prove it; note that $n$ is integer. I know that there are infinite many integers $a_n$ s.t. $\sin(a_n)>0$ and then limsup is $+ \infty$. I want to prove that liminf $\ne +\infty$ but I don't know how.,,"['calculus', 'real-analysis']"
89,Metamathematics of the Banach space consequences of the Baire category theorem,Metamathematics of the Banach space consequences of the Baire category theorem,,"My question is about a comment in Tao's presentation of the Uniform Boundedness Principle, the Open Mapping theorem, and the Closed Graph theorem in his blog post 245B, Notes 9: The Baire category theorem and its Banach space consequences .  He says typically the ""easy"" directions of the theorems are used in practice while the ""hard"" directions provide metamathematical justification for the approach. Strictly speaking, these theorems are not used much directly in practice, because one usually works in the reverse direction (i.e. first proving quantitative bounds, and then deriving qualitative corollaries); but the above three theorems help explain why we usually approach qualitative problems in functional analysis via their quantitative counterparts. Despite being given three examples of this phenomenon, I still don't understand what he means at all.  The examples are below, and I added the boldface to highlight what I don't get.  Could someone please expand on this, maybe with a simpler example?  I'm aware this is not supposed to be a formal logical concept, but I just don't understand what he's saying. In the discussion following Example 1 (Fourier inversion formula by Uniform Boundedness), Tao writes This argument only used the “easy” implication of Corollary 1, namely the deduction of 2. from 3.  The “hard” implication using the Baire category theorem was not directly utilised.  However, from a metamathematical standpoint, that implication is important because it tells us that the above strategy to prove convergence in norm of the Fourier inversion formula on $L^2$ – i.e. to obtain uniform operator norms on the partial sums, and to establish convergence on a dense subclass of “nice” functions – is in some sense the only strategy available to prove such a result. In Remark 5 following the Open Mapping Principle, Tao writes The open mapping theorem provides metamathematical justification for the method of a priori estimates for solving linear equations such as $Lu = f$ for a given datum $f \in Y$ and for an unknown $u \in X$, which is of course a familiar problem in linear PDE.  The a priori method assumes that $f$ is in some dense class of nice functions (e.g. smooth functions) in which solvability of $Lu=f$ is presumably easy, and then proceeds to obtain the a priori estimate $\|u\|_X \leq C \|f\|_Y$ for some constant $C$.  Theorem 3 then assures that $Lu=f$ is solvable for all $f$ in $Y$ (with a similar bound).  As before, this implication does not directly use the Baire category theorem, but that theorem helps explain why this method is “not wasteful.” Finally, following the Closed Graph Theorem Tao writes In practice, one should think of $Z$ as some sort of “low regularity” space with a weak topology, and $Y$ as a “high regularity” subspace with a stronger topology.  Corollary 3 motivates the method of a priori estimates to establish the $Y$-regularity of some linear transform $Tx$ of an arbitrary element $x$ in a Banach space $X$, by first establishing the a priori estimate $\|Tx\|_Y \leq C \|x\|_X$ for a dense subclass of “nice” elements of $X$, and then using the above corollary (and some weak continuity of $T$ in a low regularity space) to conclude.  The closed graph theorem provides the metamathematical explanation as to why this approach is at least as powerful as any other approach to proving regularity.","My question is about a comment in Tao's presentation of the Uniform Boundedness Principle, the Open Mapping theorem, and the Closed Graph theorem in his blog post 245B, Notes 9: The Baire category theorem and its Banach space consequences .  He says typically the ""easy"" directions of the theorems are used in practice while the ""hard"" directions provide metamathematical justification for the approach. Strictly speaking, these theorems are not used much directly in practice, because one usually works in the reverse direction (i.e. first proving quantitative bounds, and then deriving qualitative corollaries); but the above three theorems help explain why we usually approach qualitative problems in functional analysis via their quantitative counterparts. Despite being given three examples of this phenomenon, I still don't understand what he means at all.  The examples are below, and I added the boldface to highlight what I don't get.  Could someone please expand on this, maybe with a simpler example?  I'm aware this is not supposed to be a formal logical concept, but I just don't understand what he's saying. In the discussion following Example 1 (Fourier inversion formula by Uniform Boundedness), Tao writes This argument only used the “easy” implication of Corollary 1, namely the deduction of 2. from 3.  The “hard” implication using the Baire category theorem was not directly utilised.  However, from a metamathematical standpoint, that implication is important because it tells us that the above strategy to prove convergence in norm of the Fourier inversion formula on $L^2$ – i.e. to obtain uniform operator norms on the partial sums, and to establish convergence on a dense subclass of “nice” functions – is in some sense the only strategy available to prove such a result. In Remark 5 following the Open Mapping Principle, Tao writes The open mapping theorem provides metamathematical justification for the method of a priori estimates for solving linear equations such as $Lu = f$ for a given datum $f \in Y$ and for an unknown $u \in X$, which is of course a familiar problem in linear PDE.  The a priori method assumes that $f$ is in some dense class of nice functions (e.g. smooth functions) in which solvability of $Lu=f$ is presumably easy, and then proceeds to obtain the a priori estimate $\|u\|_X \leq C \|f\|_Y$ for some constant $C$.  Theorem 3 then assures that $Lu=f$ is solvable for all $f$ in $Y$ (with a similar bound).  As before, this implication does not directly use the Baire category theorem, but that theorem helps explain why this method is “not wasteful.” Finally, following the Closed Graph Theorem Tao writes In practice, one should think of $Z$ as some sort of “low regularity” space with a weak topology, and $Y$ as a “high regularity” subspace with a stronger topology.  Corollary 3 motivates the method of a priori estimates to establish the $Y$-regularity of some linear transform $Tx$ of an arbitrary element $x$ in a Banach space $X$, by first establishing the a priori estimate $\|Tx\|_Y \leq C \|x\|_X$ for a dense subclass of “nice” elements of $X$, and then using the above corollary (and some weak continuity of $T$ in a low regularity space) to conclude.  The closed graph theorem provides the metamathematical explanation as to why this approach is at least as powerful as any other approach to proving regularity.",,"['real-analysis', 'functional-analysis', 'problem-solving', 'meta-math']"
90,Necessary and sufficient condition for real analyticity,Necessary and sufficient condition for real analyticity,,"I am trying to prove the following result: Let $f:\mathbb{R}\to\mathbb{R}$ be infinitely differentiable. If $f$ is analytic at $a$ , then there is a ball $B_r(a)$ for which: $$\lim_{n\to\infty} M_n\frac{r^n}{n!}<\infty$$ where $M_n:=\sup\{|f^{(n)}(x)|:x\in B_r(a)\}$ . I know that the converse is true. I have the feeling that it is too good to be true, but I really don't know how to prove it. Any ideas? Thanks!","I am trying to prove the following result: Let be infinitely differentiable. If is analytic at , then there is a ball for which: where . I know that the converse is true. I have the feeling that it is too good to be true, but I really don't know how to prove it. Any ideas? Thanks!",f:\mathbb{R}\to\mathbb{R} f a B_r(a) \lim_{n\to\infty} M_n\frac{r^n}{n!}<\infty M_n:=\sup\{|f^{(n)}(x)|:x\in B_r(a)\},"['real-analysis', 'power-series', 'taylor-expansion']"
91,Continuous with compact support implies uniform continuity,Continuous with compact support implies uniform continuity,,"This might be a duplicate but I tried googling the MSE site and could not find a satisfactory answer. Let $(X, d)$ be a metric space and $f$ be a real valued continuous function on $X$. Suppose $f$ has a compact support. Does this imply the uniform continuity of $f$? I tried proving this statement,  but only for locally connected spaces have I succeeded in doing so. Is thus true for general metric spaces? I just couldn't provide a proof (or a counterexample) by myself. Please enlighten me.","This might be a duplicate but I tried googling the MSE site and could not find a satisfactory answer. Let $(X, d)$ be a metric space and $f$ be a real valued continuous function on $X$. Suppose $f$ has a compact support. Does this imply the uniform continuity of $f$? I tried proving this statement,  but only for locally connected spaces have I succeeded in doing so. Is thus true for general metric spaces? I just couldn't provide a proof (or a counterexample) by myself. Please enlighten me.",,"['real-analysis', 'general-topology', 'metric-spaces', 'uniform-continuity']"
92,"$f(0)=0$ ; $f(x):=\int_0^x \cos \frac 1t \cos \frac 3t \cos \frac 5t \cos \frac 7t\,dt$, $\forall x \ne 0$. Is $f$ differentiable at $0$?","; , . Is  differentiable at ?","f(0)=0 f(x):=\int_0^x \cos \frac 1t \cos \frac 3t \cos \frac 5t \cos \frac 7t\,dt \forall x \ne 0 f 0","Let $f:\mathbb R \to \mathbb R$ be defined by $f(0)=0$ and $$ f(x):=\int_0^x \cos \dfrac 1t \cos \dfrac 3t \cos \dfrac 5t \cos \dfrac 7t \,dt ,\quad\forall x \ne 0. $$ Then is $f$ differentiable at $0$?","Let $f:\mathbb R \to \mathbb R$ be defined by $f(0)=0$ and $$ f(x):=\int_0^x \cos \dfrac 1t \cos \dfrac 3t \cos \dfrac 5t \cos \dfrac 7t \,dt ,\quad\forall x \ne 0. $$ Then is $f$ differentiable at $0$?",,"['real-analysis', 'integration']"
93,Is there always a stable position for a rectangular lawn table?,Is there always a stable position for a rectangular lawn table?,,"It is a common calculus / analysis exercice to give a square lawn table, with legs at the corners, and ask to prove that it is always possible to rotate it so that it stands stably (not necessarily in perfect horizontal), as long as the wobblyness comes from the uneven lawn, not from the legs of the table itself. The proof, which assumes the lawn is suitably nice, uses the intermediate value theorem by rotating the table $90^\circ$ while keeping three legs firmly on the ground and noting that since all that's happened is that the diagonals of the table swapped places, the fourth leg must have passed through the ground at some point. What about a rectangular table? This has the complication that the table only has $180^\circ$ rotational symmetry, so there is no way to rotate it so as to have the diagonals swap position nicely. So, how can we prove that if we keep three legs along the ground as we turn the table around, the last leg must touch the ground at some point? Or is there some different approach that works? We are, of course, making the same assumptions as in the square case about the niceness of the lawn and that the table stands stably on an even floor.","It is a common calculus / analysis exercice to give a square lawn table, with legs at the corners, and ask to prove that it is always possible to rotate it so that it stands stably (not necessarily in perfect horizontal), as long as the wobblyness comes from the uneven lawn, not from the legs of the table itself. The proof, which assumes the lawn is suitably nice, uses the intermediate value theorem by rotating the table $90^\circ$ while keeping three legs firmly on the ground and noting that since all that's happened is that the diagonals of the table swapped places, the fourth leg must have passed through the ground at some point. What about a rectangular table? This has the complication that the table only has $180^\circ$ rotational symmetry, so there is no way to rotate it so as to have the diagonals swap position nicely. So, how can we prove that if we keep three legs along the ground as we turn the table around, the last leg must touch the ground at some point? Or is there some different approach that works? We are, of course, making the same assumptions as in the square case about the niceness of the lawn and that the table stands stably on an even floor.",,"['real-analysis', 'continuity', 'recreational-mathematics', 'puzzle']"
94,Does there exist an uncountable number of isolated points?,Does there exist an uncountable number of isolated points?,,"This question arose from my thinking about order-preserving isomorphisms on the Real numbers. These functions must be injections (“one-to-one”) $$a\ne b \implies f(a)\ne f(b)$$ and preserve order $$a\le b \implies f(a)\le f(b)$$ In other words, I knew that these functions must be strictly increasing, perhaps with a finite number of points with zero slope. For if there were any intervals $(a,b)$ on which the function had zero or decreasing slope, it would no longer be an injection or preserve order: two different inputs would have the same output. But then I realized that even an infinite, albeit countable, number of points would suffice, for example, a function that is increasing everywhere except at every integer; something like $f(x)=x+sin(x)$. This function’s graph looks like a smoothed-out staircase, and its derivative $f^{\prime}(x)=1+cos(x)$ is tangent to the $x$-axis at those points. So then taking a step further, I wondered if perhaps an uncountable number of points would suffice, provided that these points weren’t “together in a straight line”, or put more precisely, they didn’t make an interval. My first thought was an uncountable set of irrational numbers within an interval, say, $[1,2]$. This function would be increasing at all rational points, but have zero slope at all irrational points. (There are uncountably many irrational numbers within $[1,2]$.) If such a function exists, and is continuous and differentiable, its derivative would be something like the Dirichlet function: $$ g^{\prime}(x) = \begin{cases} 1 &: &x < 1\\ 1 &: &x \in [1,2]\cap\mathbb{Q}\\ 0 &: &x \in [1,2]-\mathbb{Q}\\ 1 &: &x > 2\\ \end{cases} $$ I'm not sure if that’s possible. If it isn’t, then the points in my uncountable set must all be isolated, that is, there must exist an $\epsilon$-neighborhood around each point that doesn’t contain any other points in the set. Is it possible to select uncountably many isolated points?","This question arose from my thinking about order-preserving isomorphisms on the Real numbers. These functions must be injections (“one-to-one”) $$a\ne b \implies f(a)\ne f(b)$$ and preserve order $$a\le b \implies f(a)\le f(b)$$ In other words, I knew that these functions must be strictly increasing, perhaps with a finite number of points with zero slope. For if there were any intervals $(a,b)$ on which the function had zero or decreasing slope, it would no longer be an injection or preserve order: two different inputs would have the same output. But then I realized that even an infinite, albeit countable, number of points would suffice, for example, a function that is increasing everywhere except at every integer; something like $f(x)=x+sin(x)$. This function’s graph looks like a smoothed-out staircase, and its derivative $f^{\prime}(x)=1+cos(x)$ is tangent to the $x$-axis at those points. So then taking a step further, I wondered if perhaps an uncountable number of points would suffice, provided that these points weren’t “together in a straight line”, or put more precisely, they didn’t make an interval. My first thought was an uncountable set of irrational numbers within an interval, say, $[1,2]$. This function would be increasing at all rational points, but have zero slope at all irrational points. (There are uncountably many irrational numbers within $[1,2]$.) If such a function exists, and is continuous and differentiable, its derivative would be something like the Dirichlet function: $$ g^{\prime}(x) = \begin{cases} 1 &: &x < 1\\ 1 &: &x \in [1,2]\cap\mathbb{Q}\\ 0 &: &x \in [1,2]-\mathbb{Q}\\ 1 &: &x > 2\\ \end{cases} $$ I'm not sure if that’s possible. If it isn’t, then the points in my uncountable set must all be isolated, that is, there must exist an $\epsilon$-neighborhood around each point that doesn’t contain any other points in the set. Is it possible to select uncountably many isolated points?",,"['real-analysis', 'general-topology']"
95,Dirac delta distribution and sin(x) - what can be a test function?,Dirac delta distribution and sin(x) - what can be a test function?,,"I read about the Dirac delta distribution some days ago to better understand distributions (or generalized functions), but I've become a bit confused. I used $\delta$ as a ""function"" ($\delta(x)$) until now, without thinking about its strict definition. I usually used the following formula: \begin{equation} \int_{-\infty}^\infty \delta(x) f(x) dx = f(0), \end{equation} but never thought about the properties of the $f(x)$ functions. Now as I understand, distributions are linear, continous functionals from some vector space of test functions $\mathcal{D}$, where the functions in $\mathcal{D}$ are smooth and have compact support. Also, I've read that the test function space can be extended to the Swartz-functions (~ rapidly decreasing functions (faster than polynomial)). But - if I'm correct -, none of these space include e.g. sin(x), cos(x), or any polynomial, etc., but it looks for me, that people use integrals with Dirac-$\delta$ in the same way ( Understanding Dirac delta integrals? ). So, my question is that, is there any other extension of the test function space to include these ones? And also, how can we deal with complex test functions? These are probably interesting from the Fourier transformation point of view. (P.s.: I'm not a mathematician, so please give me ""relativily"" simple answers, or give me some reference book / text, where I can read about these.)","I read about the Dirac delta distribution some days ago to better understand distributions (or generalized functions), but I've become a bit confused. I used $\delta$ as a ""function"" ($\delta(x)$) until now, without thinking about its strict definition. I usually used the following formula: \begin{equation} \int_{-\infty}^\infty \delta(x) f(x) dx = f(0), \end{equation} but never thought about the properties of the $f(x)$ functions. Now as I understand, distributions are linear, continous functionals from some vector space of test functions $\mathcal{D}$, where the functions in $\mathcal{D}$ are smooth and have compact support. Also, I've read that the test function space can be extended to the Swartz-functions (~ rapidly decreasing functions (faster than polynomial)). But - if I'm correct -, none of these space include e.g. sin(x), cos(x), or any polynomial, etc., but it looks for me, that people use integrals with Dirac-$\delta$ in the same way ( Understanding Dirac delta integrals? ). So, my question is that, is there any other extension of the test function space to include these ones? And also, how can we deal with complex test functions? These are probably interesting from the Fourier transformation point of view. (P.s.: I'm not a mathematician, so please give me ""relativily"" simple answers, or give me some reference book / text, where I can read about these.)",,"['real-analysis', 'functional-analysis', 'definition', 'distribution-theory', 'dirac-delta']"
96,Volume of spherical shell with $dr$ thickness,Volume of spherical shell with  thickness,dr,"Let's consider two spheres in the $(x,y,z)$ 3D-space, both centered in the origin: the inner with radius $r$ and the outer with radius $r + dr$. To compute the volume of the spherical shell between their two surfaces, one should simply proceed as follows: $$dV = \frac{4}{3} \pi (r + dr)^3 - \frac{4}{3} \pi r^3 = \frac{4}{3} \pi (r^3 + 3 r^2 dr + 3 r dr^2 + dr^3 - r^3)$$ $$dV = \frac{4}{3} \pi (3 r^2 dr + 3 r dr^2 + dr^3)$$ but usually the last two terms between the brackets are neglected. I know $dr$ is infinitesimal, but can infinitesimals (raised to $n$-th power, $n>1$) be neglected and still obtain an exact result? How can this be justified?","Let's consider two spheres in the $(x,y,z)$ 3D-space, both centered in the origin: the inner with radius $r$ and the outer with radius $r + dr$. To compute the volume of the spherical shell between their two surfaces, one should simply proceed as follows: $$dV = \frac{4}{3} \pi (r + dr)^3 - \frac{4}{3} \pi r^3 = \frac{4}{3} \pi (r^3 + 3 r^2 dr + 3 r dr^2 + dr^3 - r^3)$$ $$dV = \frac{4}{3} \pi (3 r^2 dr + 3 r dr^2 + dr^3)$$ but usually the last two terms between the brackets are neglected. I know $dr$ is infinitesimal, but can infinitesimals (raised to $n$-th power, $n>1$) be neglected and still obtain an exact result? How can this be justified?",,"['calculus', 'real-analysis', 'differential-geometry']"
97,Using squeeze thorem find $ \lim_{n \to \infty}{\frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}}$ [duplicate],Using squeeze thorem find  [duplicate], \lim_{n \to \infty}{\frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}},"This question already has answers here : To show for following sequence $\lim_{n \to \infty} a_n = 0$ where $a_n$ = $1.3.5 ... (2n-1)\over 2.4.6...(2n)$ (5 answers) Closed 8 years ago . It is already solved here at Math.stackexchange, but we haven't learned Stirling's approximation (at our school), so can it be solved using only squeeze theorem? $$\lim_{n \to \infty}{\frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}}$$ My attempt, Let $y_n = \frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}$, we see that $ \frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6\cdot ...\cdot 2n} > \frac{1 \cdot 2 \cdot 2 \cdot ... \cdot 2}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n} = \frac{1}{1 \cdot 2 \cdots .. \cdot (n-1)\cdot2n} = x_n$ $$\lim_{n \to \infty}{x_n} = 0 $$ Now I just need to find a sequnce $z_n>y_n$, so, $\lim_{n \to \infty} z_n = 0$.","This question already has answers here : To show for following sequence $\lim_{n \to \infty} a_n = 0$ where $a_n$ = $1.3.5 ... (2n-1)\over 2.4.6...(2n)$ (5 answers) Closed 8 years ago . It is already solved here at Math.stackexchange, but we haven't learned Stirling's approximation (at our school), so can it be solved using only squeeze theorem? $$\lim_{n \to \infty}{\frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}}$$ My attempt, Let $y_n = \frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}$, we see that $ \frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6\cdot ...\cdot 2n} > \frac{1 \cdot 2 \cdot 2 \cdot ... \cdot 2}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n} = \frac{1}{1 \cdot 2 \cdots .. \cdot (n-1)\cdot2n} = x_n$ $$\lim_{n \to \infty}{x_n} = 0 $$ Now I just need to find a sequnce $z_n>y_n$, so, $\lim_{n \to \infty} z_n = 0$.",,"['real-analysis', 'sequences-and-series']"
98,$x^y=y^x$. Prove that $x^y>e^e$,. Prove that,x^y=y^x x^y>e^e,"Let $x>1$, $y>1$ and $x\neq y$ such that $x^y=y^x$. Prove that: $$x^y>e^e$$ We can assume $y=kx$, where $k>1$. Hence, $x=k^{\frac{1}{k-1}}$ and $y=k^{\frac{k}{k-1}}$ and we need to prove that $f(k)>e^e$, where $f(k)=k^{\frac{k^{\frac{k}{k-1}}}{k-1}}$. We can show that $f'(k)>0$ for $k>1$ and $\lim\limits_{k\rightarrow1^+}f(k)=e^e$ and we are done. Is there a nice proof for this inequality? Thank you!","Let $x>1$, $y>1$ and $x\neq y$ such that $x^y=y^x$. Prove that: $$x^y>e^e$$ We can assume $y=kx$, where $k>1$. Hence, $x=k^{\frac{1}{k-1}}$ and $y=k^{\frac{k}{k-1}}$ and we need to prove that $f(k)>e^e$, where $f(k)=k^{\frac{k^{\frac{k}{k-1}}}{k-1}}$. We can show that $f'(k)>0$ for $k>1$ and $\lim\limits_{k\rightarrow1^+}f(k)=e^e$ and we are done. Is there a nice proof for this inequality? Thank you!",,"['real-analysis', 'inequality']"
99,Lipschitz continuity is equivalent to absolute continuity with bounded derivative,Lipschitz continuity is equivalent to absolute continuity with bounded derivative,,"I am trying to show that a function is a Lipschitz $M$ continuous if and only if it is absolutely continues and $|f'(x)| \leq M$. I think I am on the right track: Proof: (=>) Let f be Lipschitz M continues  ie if $|f(x)-f(y)| \leq M|x-y|$ for all $x,y \in E=[a,b]$. now we want to show abs cont: $\sum^n_{i=1}|f(x'_i)-f(x_i)|< \epsilon$  if  $\sum^n_{i=1}|x'_i-x_i|< \delta$ for all any finite collection of disjoint intervals $(x'_i,x_i)$. Consider we let $\sum^n_{i=1}|x'_i-x_i|< \delta$, then we observe that  $\sum^n_{i=1}|f(x'_i)-f(x_i)| \leq M \sum^n_{i=1}|x'_i-x_i|$ by the Lipschitz M continues and triangle inequality. Then define $\epsilon = M \delta$ and we are done. To see that $|f'(x)| \leq M$ we can just let $x'_i=x_i+h$ and have the following $|f(x_i+h)-f(x_i)| \leq M|x_i+h-x_i|$ thus we have $\frac{|f(x_i+h)-f(x_i)|}{|h|}\leq M$ and if we take the limit and take it inside the absolute values we are done. (<=) Now assume abs continuity and  $|f'(x)|=M$ now by another theorem we know that f is absolute continues if and only if it is an indefinite integral $f(x)= \int_a ^x f'(t)dt +f(a)$  we can manipulate this to $f(x)-f(a) \leq \int_a ^x Mdt$ . Now we can integrate the RHS to get $\int_a ^x Mdt = M (x-a)$ now we have $f(x)-f(a) \leq M (x-a)$ we can take the absolute calues of both sides and we are done. Does this seem correct?","I am trying to show that a function is a Lipschitz $M$ continuous if and only if it is absolutely continues and $|f'(x)| \leq M$. I think I am on the right track: Proof: (=>) Let f be Lipschitz M continues  ie if $|f(x)-f(y)| \leq M|x-y|$ for all $x,y \in E=[a,b]$. now we want to show abs cont: $\sum^n_{i=1}|f(x'_i)-f(x_i)|< \epsilon$  if  $\sum^n_{i=1}|x'_i-x_i|< \delta$ for all any finite collection of disjoint intervals $(x'_i,x_i)$. Consider we let $\sum^n_{i=1}|x'_i-x_i|< \delta$, then we observe that  $\sum^n_{i=1}|f(x'_i)-f(x_i)| \leq M \sum^n_{i=1}|x'_i-x_i|$ by the Lipschitz M continues and triangle inequality. Then define $\epsilon = M \delta$ and we are done. To see that $|f'(x)| \leq M$ we can just let $x'_i=x_i+h$ and have the following $|f(x_i+h)-f(x_i)| \leq M|x_i+h-x_i|$ thus we have $\frac{|f(x_i+h)-f(x_i)|}{|h|}\leq M$ and if we take the limit and take it inside the absolute values we are done. (<=) Now assume abs continuity and  $|f'(x)|=M$ now by another theorem we know that f is absolute continues if and only if it is an indefinite integral $f(x)= \int_a ^x f'(t)dt +f(a)$  we can manipulate this to $f(x)-f(a) \leq \int_a ^x Mdt$ . Now we can integrate the RHS to get $\int_a ^x Mdt = M (x-a)$ now we have $f(x)-f(a) \leq M (x-a)$ we can take the absolute calues of both sides and we are done. Does this seem correct?",,"['real-analysis', 'lipschitz-functions']"
