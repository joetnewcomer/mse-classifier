,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Looking for a proof for the convergence of matrix geometric series,Looking for a proof for the convergence of matrix geometric series,,"Consider a symmetric matrix $A$ with non-negative integer coefficients. It appears that the geometric series $\sum_{i \geq 0}A^i$ will converge to a matrix $B$ if the spectral radius (the largest eigenvalue in absolute value) is less than 1. I would like to extend this result to a case where the matrix doesn't have integer coefficients but generating functions (with real coefficients) instead and to do so, I'd be interested in having a closer look at the proof of the standard case. The questions I'm trying to figure out are: 1) is there a standard proof in a general case where we're dealing with a ring with a norm, than the geometric series has 1 as radius of convergence? Or what's the most obvious proof in the specific case of symmetric non-negative matrices? 2) what is the good equivalent of the case of matrices with integer coefficients that aren't necessarily symmetric. I'm quite interested in keeping the link to the eigenvalues and not in other matrix norms. 3) when dealing with matrices of generating functions (on one variable for a start), can I think of the eigenvalues as generating functions as well? Would that be the good way to think of this result on geometric series? I'll keep posting as I work on all this (but I needed a place to write things down and possible get some help or hints).","Consider a symmetric matrix $A$ with non-negative integer coefficients. It appears that the geometric series $\sum_{i \geq 0}A^i$ will converge to a matrix $B$ if the spectral radius (the largest eigenvalue in absolute value) is less than 1. I would like to extend this result to a case where the matrix doesn't have integer coefficients but generating functions (with real coefficients) instead and to do so, I'd be interested in having a closer look at the proof of the standard case. The questions I'm trying to figure out are: 1) is there a standard proof in a general case where we're dealing with a ring with a norm, than the geometric series has 1 as radius of convergence? Or what's the most obvious proof in the specific case of symmetric non-negative matrices? 2) what is the good equivalent of the case of matrices with integer coefficients that aren't necessarily symmetric. I'm quite interested in keeping the link to the eigenvalues and not in other matrix norms. 3) when dealing with matrices of generating functions (on one variable for a start), can I think of the eigenvalues as generating functions as well? Would that be the good way to think of this result on geometric series? I'll keep posting as I work on all this (but I needed a place to write things down and possible get some help or hints).",,"['linear-algebra', 'matrices', 'convergence-divergence', 'power-series']"
1,Determine the entries $x$ and $y$ in a matrix so that its only eigenvalue is $1$.,Determine the entries  and  in a matrix so that its only eigenvalue is .,x y 1,"I am doing some self-study in preparation for an exam, and in this problem I am given the following matrix in $R^{3×3}$ : $\begin{pmatrix} 1&0&1\\ 0&1&-1\\ 0&x&y\\ \end{pmatrix}$ I need to determine the set of x and y values so that the only eigenvalue of this matrix is one. At first glance, we have $x = 0$ and $y = 1$ , since that would make this into a unipotent matrix and I know that the only eigenvalue of a unipotent matrix equals 1. Now consider the characteristic polynomial of the matrix, which I have determined to be $p(\lambda) := -\lambda^3 + (y + 2)\lambda^2 - (x + 2x + 1)\lambda + (x+y)$ . We see that $a_2 = (y + 2)$ and $a_0 = (x+y)$ , and that $a_2$ is the sum of eigenvalues and that $a_0$ is the product of eigenvalues, both of which must be one. Solving for x and y gives us x = 2 and y = -1. For the third method, consider the companion matrix of the characteristic polynomial. Transforming $p(\lambda) := -\lambda^3 + (y + 2)\lambda^2 - (x + 2x + 1)\lambda + (x+y)$ to the monic polynomial $p(\lambda) := \lambda^3 - (y + 2)\lambda^2 + (x + 2x + 1)\lambda - (x+y)$ and then determining the companion matrix, we get: C(p) := $\begin{pmatrix} 0&0&x+y\\ 1&0&-(x+2x+1)\\ 0&1&y+2\\ \end{pmatrix}$ If the sum of all columns equals 1, then one of the eigenvalues of this matrix equals 1. Summing the third column gives us a value of 1 for all x and y. Therefore, knowing that the eigenvalues of C(p) are the roots of $p(\lambda)$ , we see that 1 is an eigenvalue of the original matrix for all x and y. To see that it is the only real eigenvalue, I factor $(\lambda - 1)$ out of the original monic polynomial and solve for the remaining quadratic coefficients. I get: $p(\lambda) := (\lambda - 1)(\lambda^2 - (y + 1)\lambda + (x+y))$ Solving for the quadratic coefficients gives us: $\lambda_{2,3} = \frac{(y+1) ± \sqrt{(y^2 - 2y -4x + 1)}}{2}$ So if we find x and y in the real numbers so that the term under the radical is less than zero, we have found x and y where 1 is the only real eigenvalue. From this point I am stuck.","I am doing some self-study in preparation for an exam, and in this problem I am given the following matrix in : I need to determine the set of x and y values so that the only eigenvalue of this matrix is one. At first glance, we have and , since that would make this into a unipotent matrix and I know that the only eigenvalue of a unipotent matrix equals 1. Now consider the characteristic polynomial of the matrix, which I have determined to be . We see that and , and that is the sum of eigenvalues and that is the product of eigenvalues, both of which must be one. Solving for x and y gives us x = 2 and y = -1. For the third method, consider the companion matrix of the characteristic polynomial. Transforming to the monic polynomial and then determining the companion matrix, we get: C(p) := If the sum of all columns equals 1, then one of the eigenvalues of this matrix equals 1. Summing the third column gives us a value of 1 for all x and y. Therefore, knowing that the eigenvalues of C(p) are the roots of , we see that 1 is an eigenvalue of the original matrix for all x and y. To see that it is the only real eigenvalue, I factor out of the original monic polynomial and solve for the remaining quadratic coefficients. I get: Solving for the quadratic coefficients gives us: So if we find x and y in the real numbers so that the term under the radical is less than zero, we have found x and y where 1 is the only real eigenvalue. From this point I am stuck.","R^{3×3} \begin{pmatrix}
1&0&1\\
0&1&-1\\
0&x&y\\
\end{pmatrix} x = 0 y = 1 p(\lambda) := -\lambda^3 + (y + 2)\lambda^2 - (x + 2x + 1)\lambda + (x+y) a_2 = (y + 2) a_0 = (x+y) a_2 a_0 p(\lambda) := -\lambda^3 + (y + 2)\lambda^2 - (x + 2x + 1)\lambda + (x+y) p(\lambda) := \lambda^3 - (y + 2)\lambda^2 + (x + 2x + 1)\lambda - (x+y) \begin{pmatrix}
0&0&x+y\\
1&0&-(x+2x+1)\\
0&1&y+2\\
\end{pmatrix} p(\lambda) (\lambda - 1) p(\lambda) := (\lambda - 1)(\lambda^2 - (y + 1)\lambda + (x+y)) \lambda_{2,3} = \frac{(y+1) ± \sqrt{(y^2 - 2y -4x + 1)}}{2}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'characteristic-polynomial']"
2,Norm upper bound on difference of inverted matrices,Norm upper bound on difference of inverted matrices,,"I feel like I have seen this bound before but no longer can recall its source: Suppose $A, B$ are square matrices with $||A^{-1} (A-B) || < 1$ , i.e. their perturbation is relatively minimal. Then, what is an upper bound on $|| A^{-1} - B^{-1} ||$ ? In my notes, I have: $$ || A^{-1} - B^{-1} || \leq \frac{||A^{-1}||^2 ||A - B||}{1 - ||A^{-1} (A - B)||} $$ Is this true? And is there a source?","I feel like I have seen this bound before but no longer can recall its source: Suppose are square matrices with , i.e. their perturbation is relatively minimal. Then, what is an upper bound on ? In my notes, I have: Is this true? And is there a source?","A, B ||A^{-1} (A-B) || < 1 || A^{-1} - B^{-1} || 
|| A^{-1} - B^{-1} || \leq \frac{||A^{-1}||^2 ||A - B||}{1 - ||A^{-1} (A - B)||}
",['matrices']
3,"If the entries of a matrix are sines of distinct prime numbers, could the determinant equal $0$?","If the entries of a matrix are sines of distinct prime numbers, could the determinant equal ?",0,"I recently learned that if the entries of a matrix are sines of distinct integers, the determinant could equal $0$ . For example, $\det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0$ And I learned that if the entries of a matrix are distinct prime numbers, the determinant could equal $0$ . For example, $\det\begin{bmatrix}3&5&7\\13&17&19\\23&29&31\end{bmatrix}=0$ This leads to my question: If the entries of a matrix are sines of distinct prime numbers , could the determinant equal $0$ ? I have tried to apply the ideas contained in the answers and comments in the links above, to prove that the answer to my question is yes, without success.","I recently learned that if the entries of a matrix are sines of distinct integers, the determinant could equal . For example, And I learned that if the entries of a matrix are distinct prime numbers, the determinant could equal . For example, This leads to my question: If the entries of a matrix are sines of distinct prime numbers , could the determinant equal ? I have tried to apply the ideas contained in the answers and comments in the links above, to prove that the answer to my question is yes, without success.",0 \det\begin{bmatrix}\sin 1 & \sin 2 & \sin 3 \\ \sin 4 & \sin 5 & \sin 6 \\ \sin 7 & \sin 8 & \sin 9\end{bmatrix}=0 0 \det\begin{bmatrix}3&5&7\\13&17&19\\23&29&31\end{bmatrix}=0 0,"['linear-algebra', 'matrices', 'trigonometry', 'prime-numbers', 'determinant']"
4,Estimation of the eigenvalues of a matrix,Estimation of the eigenvalues of a matrix,,"Given the following matrix \begin{pmatrix}0 & 1 & 0 & 0\\ -k & -\lambda_1 & -k & 0\\ 0 & 0 & 0 & 1\\ -k & 0 & -k & -\lambda_2\end{pmatrix} where $k,\lambda_1$ and $\lambda_2$ are strictly positive, is there a way to estimate the eigenvalues, or at least say if they are positive or negative, complex or real? One eigenvalue is  zero, but to find the others you need to solve a parametric cubic equation, which is not easy. Note that this matrix is the representation of a mechanical system, so a physical context may come in handy.","Given the following matrix where and are strictly positive, is there a way to estimate the eigenvalues, or at least say if they are positive or negative, complex or real? One eigenvalue is  zero, but to find the others you need to solve a parametric cubic equation, which is not easy. Note that this matrix is the representation of a mechanical system, so a physical context may come in handy.","\begin{pmatrix}0 & 1 & 0 & 0\\
-k & -\lambda_1 & -k & 0\\
0 & 0 & 0 & 1\\
-k & 0 & -k & -\lambda_2\end{pmatrix} k,\lambda_1 \lambda_2","['matrices', 'eigenvalues-eigenvectors', 'estimation']"
5,"Is $AGL(3,2)$ self-normalizing in $S_8$?",Is  self-normalizing in ?,"AGL(3,2) S_8","Let $AGL(3,2)$ be a group of all affine permutations consisting of maps $$ x \to Ax + b, $$ where $x$ and $b$ are vectors of length $3$ over $GF(2)$ and $A$ is an invertible $3\times 3$ matrix over $GF(2)$ . Using some numeration of vectors we can consider $AGL(3,2)$ as a subgroup of $S_8$ . My experimets with computer algebra system Sage have shown that normaliser of $AGL(3,2)$ in $S_8$ is equal to $AGL(3,2)$ . But I can not prove this. Can somebody help me with hints or full solution.",Let be a group of all affine permutations consisting of maps where and are vectors of length over and is an invertible matrix over . Using some numeration of vectors we can consider as a subgroup of . My experimets with computer algebra system Sage have shown that normaliser of in is equal to . But I can not prove this. Can somebody help me with hints or full solution.,"AGL(3,2) 
x \to Ax + b,
 x b 3 GF(2) A 3\times 3 GF(2) AGL(3,2) S_8 AGL(3,2) S_8 AGL(3,2)","['linear-algebra', 'matrices', 'group-theory', 'permutations']"
6,Why do the composition of relations and the matrix product look so alike?,Why do the composition of relations and the matrix product look so alike?,,"For reference: Matrix product: $A : X\times Y \rightarrow R$ $B : Y\times Z \rightarrow R$ $AB := (x,z) \mapsto \int_{y\in Y} A(x,y)B(y,z) d\mu : X\times Z \rightarrow R$ To make the pattern clearer I generalized matrices $A$ and $B$ over any (semi-)ring $R$ to have indices ranging over arbitrary sets instead of just finite ones. In this case we need to give $Y$ a measure. What you get is the $L^2$ inner product. Relation composition: $S \subseteq X\times Y$ $T \subseteq Y\times Z$ $S \circ T := \{(x,z)\ |\ \exists y \in Y.\ (x,y) \in S\ \land\ (y,z) \in T\} \subseteq X\times Z$ Thinking in terms of monoids might be an easy way out, but we are losing a lot of structure doing that and I believe that if there's anything deep going on here, it lies in the $\ \exists \leftrightarrow \int\ $ , $\ \land \leftrightarrow \cdot\ $ connection. What is the thing that generalizes both of these? What is going on here? Thanks in advance!","For reference: Matrix product: To make the pattern clearer I generalized matrices and over any (semi-)ring to have indices ranging over arbitrary sets instead of just finite ones. In this case we need to give a measure. What you get is the inner product. Relation composition: Thinking in terms of monoids might be an easy way out, but we are losing a lot of structure doing that and I believe that if there's anything deep going on here, it lies in the , connection. What is the thing that generalizes both of these? What is going on here? Thanks in advance!","A : X\times Y \rightarrow R B : Y\times Z \rightarrow R AB := (x,z) \mapsto \int_{y\in Y} A(x,y)B(y,z) d\mu : X\times Z \rightarrow R A B R Y L^2 S \subseteq X\times Y T \subseteq Y\times Z S \circ T := \{(x,z)\ |\ \exists y \in Y.\ (x,y) \in S\ \land\ (y,z) \in T\} \subseteq X\times Z \ \exists \leftrightarrow \int\  \ \land \leftrightarrow \cdot\ ","['abstract-algebra', 'matrices', 'category-theory']"
7,How do I know if a 4x4 unitary matrix is a tensor product of two 2x2 unitary matrices?,How do I know if a 4x4 unitary matrix is a tensor product of two 2x2 unitary matrices?,,"Let's say I have a unitary matrix $U$ , how do I know if $U$ is the resultant of a tensor product of two other unitary matrices $U_1$ and $U_2$ (dim $\geq$ 2), such that $U_1\otimes U_2=U$ ? More specifically I am concerned with the almost trivial case of where the dimensions of $U_1$ and $U_2$ are 2x2 ( $U$ is 4x4). Do I necessarily need to find the basis where $U$ is block diagonal? Or do I need to fully diagonalize $U$ and find common factors? Are there any other strategies to verify without doing these? I tried to find some direction, but I do not know what is the right term here, is it reducibility? separability?","Let's say I have a unitary matrix , how do I know if is the resultant of a tensor product of two other unitary matrices and (dim 2), such that ? More specifically I am concerned with the almost trivial case of where the dimensions of and are 2x2 ( is 4x4). Do I necessarily need to find the basis where is block diagonal? Or do I need to fully diagonalize and find common factors? Are there any other strategies to verify without doing these? I tried to find some direction, but I do not know what is the right term here, is it reducibility? separability?",U U U_1 U_2 \geq U_1\otimes U_2=U U_1 U_2 U U U,"['matrices', 'tensor-products', 'unitary-matrices', 'kronecker-product']"
8,"Does the property $A \operatorname{adj}(A) = \operatorname{adj}(A)\,A = \det(A)\,I$ define the adjugate matrix?",Does the property  define the adjugate matrix?,"A \operatorname{adj}(A) = \operatorname{adj}(A)\,A = \det(A)\,I","Many textbooks define the adjugate matrix $\operatorname{adj}(A)$ as the transpose of the matrix of cofactors of $A$ . Then they state that $A \operatorname{adj}(A) = \operatorname{adj}(A)\,A = \det(A)\,I$ . My question is: if we have a matrix $B$ such that $AB = BA = \det(A)\,I$ , must $B$ be equal to $\operatorname{adj}(A)$ ? In other words, can we define the adjugate of $A$ as the matrix satisfying $A \operatorname{adj}(A) = \operatorname{adj}(A)\,A = \det(A)\,I$ ?","Many textbooks define the adjugate matrix as the transpose of the matrix of cofactors of . Then they state that . My question is: if we have a matrix such that , must be equal to ? In other words, can we define the adjugate of as the matrix satisfying ?","\operatorname{adj}(A) A A \operatorname{adj}(A) = \operatorname{adj}(A)\,A = \det(A)\,I B AB = BA = \det(A)\,I B \operatorname{adj}(A) A A \operatorname{adj}(A) = \operatorname{adj}(A)\,A = \det(A)\,I","['linear-algebra', 'matrices']"
9,How can I prove that $f(x)=\det (A+xB)= \alpha x+\beta $?,How can I prove that ?,f(x)=\det (A+xB)= \alpha x+\beta ,"I have two matrices $A$ and $B$ , such that : $$A=A(a,b,c)=\begin{pmatrix} a & c & c & \dots & c \\ b & a & c & \dots & c\\ b & b & a & \dots & c\\ \vdots &\vdots &\vdots & \ddots &\vdots\\ b & b & b &\dots& a \end{pmatrix} \hspace{1cm} \text{and} \hspace{1cm} B=A(1, 1, 1)$$ And we've : $$f(x)=\det(A+xB)$$ I have to prove the existence of two real numbers $\alpha$ and $\beta$ , such that : $$f(x)=\alpha x +\beta$$ Just prove their existence not their values, because later in the same exercise we have to calculate $f(-c)$ and $f(-b)$ , then deduce their values, then deduce $\det(A)$ . So That's why I think calculating the determinant won't be a good idea I guess. Any Ideas to do so ?","I have two matrices and , such that : And we've : I have to prove the existence of two real numbers and , such that : Just prove their existence not their values, because later in the same exercise we have to calculate and , then deduce their values, then deduce . So That's why I think calculating the determinant won't be a good idea I guess. Any Ideas to do so ?","A B A=A(a,b,c)=\begin{pmatrix} a & c & c & \dots & c \\ b & a & c & \dots & c\\ b & b & a & \dots & c\\ \vdots &\vdots &\vdots & \ddots &\vdots\\ b & b & b &\dots& a \end{pmatrix} \hspace{1cm} \text{and} \hspace{1cm} B=A(1, 1, 1) f(x)=\det(A+xB) \alpha \beta f(x)=\alpha x +\beta f(-c) f(-b) \det(A)","['matrices', 'determinant']"
10,Is there a way to calculate the eigenvalues of $xx^T+yy^T$?,Is there a way to calculate the eigenvalues of ?,xx^T+yy^T,"As the title shows, is there a way to calculate the eigenvalues of $A\equiv \vec x\vec x^T+\vec y \vec y^T$ , where $\vec x$ and $\vec y$ are two linearly independent vectors in $\mathbb{R}^n$ (don't have to be unit vector). Here're some of my thoughts. We can see that $A$ is of rank 2 since it can be seen as a map from $\mathbb{R}^n$ to $\mathbb{R}^n$ and there are only two linearly independent vectors in its range. So we may set the eigenvectors of $A$ as $a\vec x+b\vec y$ , where $a$ and $b$ are two real numbers, then we have chances to get the eigenvalues by solving $$A(a\vec x+b\vec y) =\lambda (a\vec x+b\vec y)\tag{1}.$$ For example, if $\vec{x}=\frac{1}{\sqrt{2}}\left( \begin{array}{c} 	1\\ 	1\\ \end{array} \right) , \vec{y}=\left( \begin{array}{c} 	1\\ 	0\\ \end{array} \right) $ , we can solve eq(1) to get the eigenvalues $1\pm \frac{1}{\sqrt 2}$ . But eq(1) only have two equations while with 3 parameters $a,b,\lambda$ . So my question is : is there a way to calculate the eigenvalues of $xx^T+yy^T$ , such as a formula related to $\vec x$ and $\vec y$ that I don't know?","As the title shows, is there a way to calculate the eigenvalues of , where and are two linearly independent vectors in (don't have to be unit vector). Here're some of my thoughts. We can see that is of rank 2 since it can be seen as a map from to and there are only two linearly independent vectors in its range. So we may set the eigenvectors of as , where and are two real numbers, then we have chances to get the eigenvalues by solving For example, if , we can solve eq(1) to get the eigenvalues . But eq(1) only have two equations while with 3 parameters . So my question is : is there a way to calculate the eigenvalues of , such as a formula related to and that I don't know?","A\equiv \vec x\vec x^T+\vec y \vec y^T \vec x \vec y \mathbb{R}^n A \mathbb{R}^n \mathbb{R}^n A a\vec x+b\vec y a b A(a\vec x+b\vec y) =\lambda (a\vec x+b\vec y)\tag{1}. \vec{x}=\frac{1}{\sqrt{2}}\left( \begin{array}{c}
	1\\
	1\\
\end{array} \right) , \vec{y}=\left( \begin{array}{c}
	1\\
	0\\
\end{array} \right)  1\pm \frac{1}{\sqrt 2} a,b,\lambda xx^T+yy^T \vec x \vec y","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-analysis']"
11,How can we determine easily that the following transition matrix is irreducible?,How can we determine easily that the following transition matrix is irreducible?,,"Let $n\in\Bbb N$ be arbitrary, and define the matrix $A=(a_{ij})_{i,j=0}^n$ via: $$a_{ij}=\begin{cases}0&|i-j|\neq1\\\frac{i}{n}&j=i-1\\\frac{n-i}{n}&j=i+1\end{cases}$$ Which represents the transition matrix of the Markov shift system where $n$ balls numbered $1\to n$ are distributed across two urns, and at every time step an integer value $1\le k\le n$ is chosen equiprobably, and the ball numbered $k$ is moved from the urn it's in to the other, and the state is the number of balls in the first urn. The text I was following claimed without proof that $A$ is always irreducible, but I don't know how one shows that. Computing $\sum_{k=1}^nA^k$ in general is not easy, nor is trying to reason about the general graph represented by $A$ . Indeed, I might try to show it is not able to be permuted into a block-triangular matrix, but I believe it is defacto block-triangular - for $n=4$ : $$A=\begin{bmatrix}0&1&0&0&0\\1/4&0&3/4&0&0\\0&1/4&0&3/4&0\\0&0&1/4&0&3/4\\0&0&0&1&0\end{bmatrix}$$ Is there not a block triangle formed by the four zeroes in the bottom left-hand corner? I feel like I am overlooking something trivial. Many thanks for any clarification.","Let be arbitrary, and define the matrix via: Which represents the transition matrix of the Markov shift system where balls numbered are distributed across two urns, and at every time step an integer value is chosen equiprobably, and the ball numbered is moved from the urn it's in to the other, and the state is the number of balls in the first urn. The text I was following claimed without proof that is always irreducible, but I don't know how one shows that. Computing in general is not easy, nor is trying to reason about the general graph represented by . Indeed, I might try to show it is not able to be permuted into a block-triangular matrix, but I believe it is defacto block-triangular - for : Is there not a block triangle formed by the four zeroes in the bottom left-hand corner? I feel like I am overlooking something trivial. Many thanks for any clarification.","n\in\Bbb N A=(a_{ij})_{i,j=0}^n a_{ij}=\begin{cases}0&|i-j|\neq1\\\frac{i}{n}&j=i-1\\\frac{n-i}{n}&j=i+1\end{cases} n 1\to n 1\le k\le n k A \sum_{k=1}^nA^k A n=4 A=\begin{bmatrix}0&1&0&0&0\\1/4&0&3/4&0&0\\0&1/4&0&3/4&0\\0&0&1/4&0&3/4\\0&0&0&1&0\end{bmatrix}","['linear-algebra', 'matrices']"
12,Unique decomposition of square matrices theorem,Unique decomposition of square matrices theorem,,"My professor mentioned a theorem for decomposing square matrices into a symmetric and anti-symmetric matrix. Paraphrasing, If $A$ is square, then $A$ has the unique decomposition $A=U+V$ , for $U$ symmetric and $V$ anti-symmetric. But I'm not quite sure how to prove this, because it seems like there should be a formula for $U$ and $V$ respectively if they are unique, but I haven't been able to find one in terms of $A$ . Where would I begin to find this?","My professor mentioned a theorem for decomposing square matrices into a symmetric and anti-symmetric matrix. Paraphrasing, If is square, then has the unique decomposition , for symmetric and anti-symmetric. But I'm not quite sure how to prove this, because it seems like there should be a formula for and respectively if they are unique, but I haven't been able to find one in terms of . Where would I begin to find this?",A A A=U+V U V U V A,['linear-algebra']
13,Is there an easy way for a person to compute the determinant of an arbitrary matrix?,Is there an easy way for a person to compute the determinant of an arbitrary matrix?,,"I'm having a tough time proving statements that involve the determinant of an arbitrary matrix and was wondering if there is just an easier way to compute it or some simpler equivalent definition for it. Just to give an example: Let $K$ be a field, let $\lambda,a_0,a_1 \dots a_n \in K$ and $n \in \mathbb{N}$ . Prove that $$ A= \begin {pmatrix} \lambda &0 &0 &\dots  & a_0 \\ -1 & \lambda &0 &  \dots &a_1 \\0 &-1&\lambda &\dots& a_2  \\ \vdots & \dots & \dots & \dots &a_n\end{pmatrix}\in K^{(n+1)\times(n+1)}, \det(A)= a_n\lambda^n+\dots+ a_2\lambda^2+a_1\lambda+a_0$$ I have zero clue how I can go about this and the definition of the determinant is not very intuitive . My best guess was using the Laplace expansion so that $n+1 \times n+1$ matrix is not worrysome so that I can deal with the submatrix but I'm not sure where I can go from there. I would appreciate tips on how I should approach these proofs. (Also I would appreciate some help regarding the example problem as well).","I'm having a tough time proving statements that involve the determinant of an arbitrary matrix and was wondering if there is just an easier way to compute it or some simpler equivalent definition for it. Just to give an example: Let be a field, let and . Prove that I have zero clue how I can go about this and the definition of the determinant is not very intuitive . My best guess was using the Laplace expansion so that matrix is not worrysome so that I can deal with the submatrix but I'm not sure where I can go from there. I would appreciate tips on how I should approach these proofs. (Also I would appreciate some help regarding the example problem as well).","K \lambda,a_0,a_1 \dots a_n \in K n \in \mathbb{N} 
A= \begin {pmatrix} \lambda &0 &0 &\dots  & a_0 \\ -1 & \lambda &0 &  \dots &a_1 \\0 &-1&\lambda &\dots& a_2  \\ \vdots & \dots & \dots & \dots &a_n\end{pmatrix}\in K^{(n+1)\times(n+1)}, \det(A)= a_n\lambda^n+\dots+ a_2\lambda^2+a_1\lambda+a_0 n+1 \times n+1","['linear-algebra', 'matrices', 'determinant', 'laplace-expansion']"
14,Toeplitz tridiagonal matrix with $0$s on main diagonal and $1$s on sub/superdiagonal has distinct eigenvalues [closed],Toeplitz tridiagonal matrix with s on main diagonal and s on sub/superdiagonal has distinct eigenvalues [closed],0 1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $$\begin{pmatrix}0&1&&&\\ 1&\ddots&\ddots&&\\ &\ddots&\ddots&\ddots&\\ &&\ddots&\ddots&1\\ &&&1&0\end{pmatrix}$$ has distinct eigenvalues. Why? Clearly, the eigenvalues should be reals. How to show they are distinct? It seems hard to factor the eigenpolynomial.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question has distinct eigenvalues. Why? Clearly, the eigenvalues should be reals. How to show they are distinct? It seems hard to factor the eigenpolynomial.",\begin{pmatrix}0&1&&&\\ 1&\ddots&\ddots&&\\ &\ddots&\ddots&\ddots&\\ &&\ddots&\ddots&1\\ &&&1&0\end{pmatrix},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices', 'toeplitz-matrices']"
15,Some questions about the concept of underdetermined systems,Some questions about the concept of underdetermined systems,,"I'm reading a linear algebra textbook and I have some confusion about the concept of underdetermined systems $A\mathbf{x}=\mathbf{y}$ : First, we know that for each vector $\mathbf{y}$ in $\mathbb{R}^m$ the underdetermined linear system is either inconsistent or has infinitely many solutions. So are there some theorems that tell us when the system is inconsistent and when it has infinitely many solutions? Second, if an underdetermined linear system has infinitely many solutions, is it guaranteed that a positive solution (all elements in $\mathbf{x}$ are positive) exists? It would be very appreciated if anyone could give some explanation on them.","I'm reading a linear algebra textbook and I have some confusion about the concept of underdetermined systems : First, we know that for each vector in the underdetermined linear system is either inconsistent or has infinitely many solutions. So are there some theorems that tell us when the system is inconsistent and when it has infinitely many solutions? Second, if an underdetermined linear system has infinitely many solutions, is it guaranteed that a positive solution (all elements in are positive) exists? It would be very appreciated if anyone could give some explanation on them.",A\mathbf{x}=\mathbf{y} \mathbf{y} \mathbb{R}^m \mathbf{x},"['linear-algebra', 'matrices', 'determinant']"
16,"If $(Av,Au)=(v,u)$ then matrix $A$ is orthogonal",If  then matrix  is orthogonal,"(Av,Au)=(v,u) A","Let $A\in M_{n \times n}(\Bbb R)$ and suppose that for every $u, v \in \Bbb R^{n}$ $$(Av,Au) = (v,u)$$ where $(\cdot,\cdot)$ is the standard inner product on $\Bbb R^{n}$ . Prove $A$ is an orthogonal matrix. I wasn't able to solve it, I got to the point $\left(Av,Au\right)=\left(Av\right)^{T}Au=v^{T}A^{T}Au$ and $\left(v,u\right)=v^{T}u$ $\left(Av,Au\right)=\left(v,u\right)\ \ \ ➜\ \ \ v^{T}A^{T}Au\ =\ v^{T}u$ and now I'm stuck.. I don't know if I can conclude that $A^{T}A=I_{R^{n}}$ just by the last equation, and if not how to get to the point I can show it.. Now I have two questions, first the official solution is this Let $u = e_{i}$ and $v = e_{j}$ . Then $(a_{i} , a_{j} ) = (Ae_{i} , Ae_{j} ) = (e_{i} , e_{j} ) = δ_{i,j}$ . Thus, the columns of $A$ are orthonormal, so $A$ is orthogonal. This is super unclear.. what is $a_{i},a_{j}$ ? what is $δ_{i,j}$ ? I understand that $(e_{i},e_{j})=0$ if $i \ne j$ and 1 otherwise, but why can we choose $v,u$ if it's a for every claim? If someone can explain to be the logic behind the solution I'd be grateful. the second question is if there is another way to solve it?","Let and suppose that for every where is the standard inner product on . Prove is an orthogonal matrix. I wasn't able to solve it, I got to the point and and now I'm stuck.. I don't know if I can conclude that just by the last equation, and if not how to get to the point I can show it.. Now I have two questions, first the official solution is this Let and . Then . Thus, the columns of are orthonormal, so is orthogonal. This is super unclear.. what is ? what is ? I understand that if and 1 otherwise, but why can we choose if it's a for every claim? If someone can explain to be the logic behind the solution I'd be grateful. the second question is if there is another way to solve it?","A\in M_{n \times n}(\Bbb R) u, v \in \Bbb R^{n} (Av,Au) = (v,u) (\cdot,\cdot) \Bbb R^{n} A \left(Av,Au\right)=\left(Av\right)^{T}Au=v^{T}A^{T}Au \left(v,u\right)=v^{T}u \left(Av,Au\right)=\left(v,u\right)\ \ \ ➜\ \ \ v^{T}A^{T}Au\ =\ v^{T}u A^{T}A=I_{R^{n}} u = e_{i} v = e_{j} (a_{i} , a_{j} ) = (Ae_{i} , Ae_{j} ) = (e_{i} , e_{j} ) = δ_{i,j} A A a_{i},a_{j} δ_{i,j} (e_{i},e_{j})=0 i \ne j v,u","['matrices', 'inner-products', 'orthogonal-matrices']"
17,Find the sum of squares of all eigenvalues of a matrix,Find the sum of squares of all eigenvalues of a matrix,,"I found this question which asks to find the sum of squares of all eigenvalues (possibly complex, not necessarily distinct) of (I used a picture because it was hard to write matrix down without making any mistakes) Now, I used this to find the eigenvalues, and squared them manually to see that the answer is $38$ . But, of course, that's not the way to solve it. There must be some patterns in this matrix that I am missing. All I can find is that this is a $14\times 14$ matrix, and there is an $11$ -triangle of zeroes at the left bottom, and a $10$ -triangle of zeroes at the right up. Also, most of the matrix is full of zeroes and almost all the entries are at the diagonal. But, these are not enough to solve the problem. Also, is there any tricks in finding sum of squares (without finding the exact eigenvalues) that will reduce our effort? As achille hui pointed out, sum of square of eigenvalues = trace of square of matrix. So, now I need to have ideas of squaring this matrix. It doesn't look simple enough to just multiply using traditional methods, there must be some tricks. Thanks in advance","I found this question which asks to find the sum of squares of all eigenvalues (possibly complex, not necessarily distinct) of (I used a picture because it was hard to write matrix down without making any mistakes) Now, I used this to find the eigenvalues, and squared them manually to see that the answer is . But, of course, that's not the way to solve it. There must be some patterns in this matrix that I am missing. All I can find is that this is a matrix, and there is an -triangle of zeroes at the left bottom, and a -triangle of zeroes at the right up. Also, most of the matrix is full of zeroes and almost all the entries are at the diagonal. But, these are not enough to solve the problem. Also, is there any tricks in finding sum of squares (without finding the exact eigenvalues) that will reduce our effort? As achille hui pointed out, sum of square of eigenvalues = trace of square of matrix. So, now I need to have ideas of squaring this matrix. It doesn't look simple enough to just multiply using traditional methods, there must be some tricks. Thanks in advance",38 14\times 14 11 10,"['matrices', 'eigenvalues-eigenvectors', 'contest-math']"
18,Understanding Matousek's proof of Equiangular lines,Understanding Matousek's proof of Equiangular lines,,"In Miniature 9 in 33 Miniatures by Matousek, he proofs that: The largest number of equiangular lines in $\mathbb R^3$ is 6, and in general, there cannot be more than $\binom{d+1}{2}$ equiangular lines in $\mathbb R^d$ . The proof starts with: Let us consider a configuration of $n$ lines, where each pair has the same angle $\vartheta \in (0, \pi/2]$ . Let $v_i$ be a unit vector in the direction of the $i$ th line (we choose one of the two possible orientations of $v_i$ arbitrarily). The condition of equal angles is equivalent to $$ |\langle v_i, v_j\rangle| = \cos \vartheta, \quad \text{for all } i\neq j.$$ Let us regard $v_i$ as a column vector, or a $d\times 1$ matrix. Then $v_i^Tv_j$ is the scalar product $\langle v_i, v_j \rangle$ . On the other hand, $v_iv_j^T$ is a $d\times d$ matrix. We show that the matrices $v_iv_i^T, i = 1,2,\dots, n$ are linearly independent. And I do not see how showing the independence of these matrices shows that we can not have more than the claimed number of equiangular lines? Thanks!","In Miniature 9 in 33 Miniatures by Matousek, he proofs that: The largest number of equiangular lines in is 6, and in general, there cannot be more than equiangular lines in . The proof starts with: Let us consider a configuration of lines, where each pair has the same angle . Let be a unit vector in the direction of the th line (we choose one of the two possible orientations of arbitrarily). The condition of equal angles is equivalent to Let us regard as a column vector, or a matrix. Then is the scalar product . On the other hand, is a matrix. We show that the matrices are linearly independent. And I do not see how showing the independence of these matrices shows that we can not have more than the claimed number of equiangular lines? Thanks!","\mathbb R^3 \binom{d+1}{2} \mathbb R^d n \vartheta \in (0, \pi/2] v_i i v_i  |\langle v_i, v_j\rangle| = \cos \vartheta, \quad \text{for all } i\neq j. v_i d\times 1 v_i^Tv_j \langle v_i, v_j \rangle v_iv_j^T d\times d v_iv_i^T, i = 1,2,\dots, n","['linear-algebra', 'combinatorics', 'matrices', 'geometry', 'dimension-theory-algebra']"
19,Understanding matrix multiplication for visualizing what is happening under the hood,Understanding matrix multiplication for visualizing what is happening under the hood,,"Take the case of this matrix multiplication: $$ A x= \begin{pmatrix}  1 & -1 & 2\\ 0 & -3 & 1\\ \end{pmatrix} \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix} $$ The answer of which is $      \begin{pmatrix}     1 \\     -3     \end{pmatrix} $ . Source: https://mathinsight.org/matrix_vector_multiplication I understand there are three components in $A$ and $x.$ So how can matrix multiplication have two (not sure if the component will be the right term) terms as part of the answer leading to matrix multiplication? What is the way to visualize the result? I think with three components, the matrix multiplication should have the result in three parts. I know I am missing something.","Take the case of this matrix multiplication: The answer of which is . Source: https://mathinsight.org/matrix_vector_multiplication I understand there are three components in and So how can matrix multiplication have two (not sure if the component will be the right term) terms as part of the answer leading to matrix multiplication? What is the way to visualize the result? I think with three components, the matrix multiplication should have the result in three parts. I know I am missing something.","
A x=
\begin{pmatrix} 
1 & -1 & 2\\
0 & -3 & 1\\
\end{pmatrix}
\begin{pmatrix}
2 \\ 1 \\ 0
\end{pmatrix}
  
    \begin{pmatrix}
    1 \\
    -3
    \end{pmatrix}
 A x.","['matrices', 'vectors']"
20,What matrix functionals are invariant under change of basis?,What matrix functionals are invariant under change of basis?,,"Fix some integer $n$ , and consider the linear space $M(n,\mathbb F)$ of square $n\times n$ matrices in some field $\mathbb F$ . Let $f:M(n,\mathbb F)\to\mathbb F$ be a functional that is invariant under change of basis, that is, such that $f(PAP^{-1})=f(A)$ for any $A,P\in M(n,\mathbb F)$ with $P$ invertible. Standard examples are $f(A)=\det(A)$ and $f(A)=\operatorname{tr}(A)$ . More generally, any function defined via the eigenvalues of $A$ is another example of this. Are these the only possible such examples? In other words, can we characterise the set of possible functionals $M(n,\mathbb F)\to\mathbb F$ that are invariant under change of basis as being all and only those functions that can be defined from the eigenvalues of the matrix? I'm mostly interested to the cases $\mathbb F=\mathbb R,\mathbb C$ , but I'm leaving this question general because I don't know if this assumption is relevant to the discussion.","Fix some integer , and consider the linear space of square matrices in some field . Let be a functional that is invariant under change of basis, that is, such that for any with invertible. Standard examples are and . More generally, any function defined via the eigenvalues of is another example of this. Are these the only possible such examples? In other words, can we characterise the set of possible functionals that are invariant under change of basis as being all and only those functions that can be defined from the eigenvalues of the matrix? I'm mostly interested to the cases , but I'm leaving this question general because I don't know if this assumption is relevant to the discussion.","n M(n,\mathbb F) n\times n \mathbb F f:M(n,\mathbb F)\to\mathbb F f(PAP^{-1})=f(A) A,P\in M(n,\mathbb F) P f(A)=\det(A) f(A)=\operatorname{tr}(A) A M(n,\mathbb F)\to\mathbb F \mathbb F=\mathbb R,\mathbb C","['linear-algebra', 'matrices', 'determinant', 'trace', 'similar-matrices']"
21,Finding polynomial to the power of 2020,Finding polynomial to the power of 2020,,"I am trying to solve a homework problem, but I am stuck at a point where I don't know what am I suppose to do next. We're given a $3 \times 3$ matrix $$A = \begin{pmatrix}1& 2& 2\\ 2& 1& 2\\ 2& 2& 1\end{pmatrix}$$ And we have to find a polynomial $p(x)$ such that deg( $p(x)$ ) = 2020, and $p(A) = 0$ (as 0 matrix 3x3) What I did was finding the characteristic polynomial which is $p(x) = (x-5)(x+1)^2$ And I know that if I use Cayley Hamilton I can place A in the characteristic polynomial and get the zero matrix. but what is that part with the degree 2020, I don't understand how do I do that or what do I rely on? This might seem easy but I really can't see it. any help is appreciated :) Thank you","I am trying to solve a homework problem, but I am stuck at a point where I don't know what am I suppose to do next. We're given a matrix And we have to find a polynomial such that deg( ) = 2020, and (as 0 matrix 3x3) What I did was finding the characteristic polynomial which is And I know that if I use Cayley Hamilton I can place A in the characteristic polynomial and get the zero matrix. but what is that part with the degree 2020, I don't understand how do I do that or what do I rely on? This might seem easy but I really can't see it. any help is appreciated :) Thank you","3 \times 3 A =
\begin{pmatrix}1& 2& 2\\
2& 1& 2\\
2& 2& 1\end{pmatrix} p(x) p(x) p(A) = 0 p(x) = (x-5)(x+1)^2","['linear-algebra', 'matrices', 'characteristic-polynomial', 'cayley-hamilton']"
22,"$PSL(2,\mathbb{R})$, $PSO(2)$ and Hyperbolic Distance",",  and Hyperbolic Distance","PSL(2,\mathbb{R}) PSO(2)","Let $PSL(2,\mathbb{R})$ be the Projective Special Linear Group and $PSO(2)$ be the Projective Special Orthogonal Group . It is well-known that $PSL(2,\mathbb{R})/PSO(2)$ can be identified with the upper half-plane $\mathbb{H}$ . Let $g,h$ be two elements of $PSL(2,\mathbb{R})$ , How can the hyperbolic distance between $gPSO(2)$ and $hPSO(2)$ be computed?","Let be the Projective Special Linear Group and be the Projective Special Orthogonal Group . It is well-known that can be identified with the upper half-plane . Let be two elements of , How can the hyperbolic distance between and be computed?","PSL(2,\mathbb{R}) PSO(2) PSL(2,\mathbb{R})/PSO(2) \mathbb{H} g,h PSL(2,\mathbb{R}) gPSO(2) hPSO(2)","['matrices', 'complex-analysis', 'manifolds', 'riemannian-geometry', 'hyperbolic-geometry']"
23,"Questions about SVD, Singular Value Decomposition","Questions about SVD, Singular Value Decomposition",,"I am not a mathematician, so I need to understand what SVD does and WHY more than how it works exactly from the math perspective. (I understand at least what is the decomposition though). This guy on youtube gave the only human explanation of SVD saying, that the U matrix maps ""user to concept correlation"" Sigma matrix defines the strength of each concept, and V maps ""movie to concept correlation"" given that initial matrix M has users in the rows, and movie (ratings) in the columns. He also mentioned two concept specifically ""sci fi"" and ""romance"" movies. See the picture below. My questions are: How SVD knows the number of concepts. He as human mentioned two - sci fi, and romance, but in reality in resulting matrices are 3 concepts. (for example matrix U - that one with blue titles - has 3 columns not 2). How SVD knows what is the concept after all. I mean, what If i shuffle the columns randomly how SVD then knows what is sci fi, what is romance. I mean, I suppose there is no rule, group the concepts together in the column order. What if scifi movie is the first and last one? and not first 3 columns in the initial matrix M? What is the practical usage of either U, Sigma or V matrices? (Except that you can multiply them to get the initial matrix M) Is there also any other possible human explanation of SVD than the guy up provided, or it is the only one possible function? Matrices of correlations.","I am not a mathematician, so I need to understand what SVD does and WHY more than how it works exactly from the math perspective. (I understand at least what is the decomposition though). This guy on youtube gave the only human explanation of SVD saying, that the U matrix maps ""user to concept correlation"" Sigma matrix defines the strength of each concept, and V maps ""movie to concept correlation"" given that initial matrix M has users in the rows, and movie (ratings) in the columns. He also mentioned two concept specifically ""sci fi"" and ""romance"" movies. See the picture below. My questions are: How SVD knows the number of concepts. He as human mentioned two - sci fi, and romance, but in reality in resulting matrices are 3 concepts. (for example matrix U - that one with blue titles - has 3 columns not 2). How SVD knows what is the concept after all. I mean, what If i shuffle the columns randomly how SVD then knows what is sci fi, what is romance. I mean, I suppose there is no rule, group the concepts together in the column order. What if scifi movie is the first and last one? and not first 3 columns in the initial matrix M? What is the practical usage of either U, Sigma or V matrices? (Except that you can multiply them to get the initial matrix M) Is there also any other possible human explanation of SVD than the guy up provided, or it is the only one possible function? Matrices of correlations.",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
24,How to use the trace of matrix to prove this problem?,How to use the trace of matrix to prove this problem?,,"Let $A,B$ be a square matrix of order $n$ and $$A B - B A = A^m$$ where $m \geqslant 1$ . Prove that $|A|=0$ . I got that $$\mathrm{tr}(A^m)=\mathrm{tr}(AB-BA)=\mathrm{tr}(AB)-\mathrm{tr}(BA)=0$$ but I don't know the next steps.",Let be a square matrix of order and where . Prove that . I got that but I don't know the next steps.,"A,B n A B - B A = A^m m \geqslant 1 |A|=0 \mathrm{tr}(A^m)=\mathrm{tr}(AB-BA)=\mathrm{tr}(AB)-\mathrm{tr}(BA)=0","['linear-algebra', 'matrices']"
25,Roots of a matrix equation,Roots of a matrix equation,,"Let $X \in \mathbb{C}^{n \times m} $ be a rectangular matrix of full rank, and $X^*$ its hermitian conjugate, let $A \in \mathbb{C}^{m \times m}$ a square matrix, and let $f: \mathbb{C} \to \mathbb{C}$ be defined by $$ f(z) = \det \left( I_n + X \frac{1}{A - z I_m} X^* \right) $$ where $I_n$ is the $n \times n$ identity matrix. Show that for $n \leq m$ solutions of $f(z)= 0$ are given by $z$ equal to an eigenvalue of $B : = A + X^* X$ . My attempt: for $n = m$ , $f(z)$ can be straightforwardly rearranged to $$ f(z) = \det \left( X \frac{1}{B-A} \big( B - I_m z \big) \frac{1}{A - z I_m} X^* \right) = \frac{\det(X) \det(B - I_m z) \det(X^*)}{\det(A - z I_m) \det(B - A)} $$ where the desired result follows from the factor $\det(B - I_m z)$ . However I cannot factorise the determinant in this way for $n < m$ , and I am unsure how to proceed.","Let be a rectangular matrix of full rank, and its hermitian conjugate, let a square matrix, and let be defined by where is the identity matrix. Show that for solutions of are given by equal to an eigenvalue of . My attempt: for , can be straightforwardly rearranged to where the desired result follows from the factor . However I cannot factorise the determinant in this way for , and I am unsure how to proceed.","X \in \mathbb{C}^{n \times m}  X^* A \in \mathbb{C}^{m \times m} f: \mathbb{C} \to \mathbb{C} 
f(z) = \det \left( I_n + X \frac{1}{A - z I_m} X^* \right)
 I_n n \times n n \leq m f(z)= 0 z B : = A + X^* X n = m f(z) 
f(z) = \det \left( X \frac{1}{B-A} \big( B - I_m z \big) \frac{1}{A - z I_m} X^* \right) = \frac{\det(X) \det(B - I_m z) \det(X^*)}{\det(A - z I_m) \det(B - A)}
 \det(B - I_m z) n < m","['matrices', 'eigenvalues-eigenvectors', 'matrix-equations']"
26,"If A is an $m\times n$ matrix, show that $||A||_2 \le \sqrt{||A||_1*||A||_\infty}$","If A is an  matrix, show that",m\times n ||A||_2 \le \sqrt{||A||_1*||A||_\infty},"If $A$ is an $m\times n$ matrix, show that $$\| A \|_2 \le \sqrt{\|A\|_1 \, \| A \|_\infty}$$ I reduced this to: $$\rho(A^TA) \le||A||1*||A||_\infty$$ I created a matrix for experimenting: $$A = \begin{bmatrix}a&b\\c&d\end{bmatrix} \\ A^T = \begin{bmatrix}a&c\\b&d\end{bmatrix} \\ A^tA = \begin{bmatrix}a^2+b^2&ac+bd\\ac+bc&c^2+d^2\end{bmatrix}$$ I tried to calculate the eigenvalues but that got me nowhere. $$-\lambda^2-\lambda(a^2+b^2+c^2+d^2)+(ad-bc)^2 = 0 \Rightarrow(ad-bc)^2 = \lambda(a^2+b^2+c^2+d^2)+\lambda^2$$ As for $||A||_1$ it can be either $a+c$ or $b+d$ , and $||A|||_\infty$ can be either $a+b$ or $c+d$ . This gives me 4 possible combinations: $$c^2+cd+a(c+d) \\ a^2+a(b+c)+bc \\ b^2+a(b+d)+db \\ d^2+d(b+c)+bc$$ I suppose I could do: $$\lambda = \frac{(a^2+b^2+c^2+d^2)\pm \sqrt{(a^2+b^2+c^2+d^2)^2+4*(ad-bc)^2}}{-2}$$ Since that root will always be bigger than $(a^2+b^2+c^2+d^2)$ , $\lambda$ will be either negative (because of the -2), in which case it's ""proven"" because the right hand side of the initial equation is always positive, or... it will be so big that the denominator will approach zero from the negative side, when the sign is negative, because you'll have $(a^2+b^2+c^2+d^2)-((a^2+b^2+c^2+d^2)+ something)$ . In that case, the result will be a very small negative value, divided by -2, which is a very small positive value. Going from there it's ""intuitive"" that the equation is correct but not rigorous, I suppose. Help?","If is an matrix, show that I reduced this to: I created a matrix for experimenting: I tried to calculate the eigenvalues but that got me nowhere. As for it can be either or , and can be either or . This gives me 4 possible combinations: I suppose I could do: Since that root will always be bigger than , will be either negative (because of the -2), in which case it's ""proven"" because the right hand side of the initial equation is always positive, or... it will be so big that the denominator will approach zero from the negative side, when the sign is negative, because you'll have . In that case, the result will be a very small negative value, divided by -2, which is a very small positive value. Going from there it's ""intuitive"" that the equation is correct but not rigorous, I suppose. Help?","A m\times n \| A \|_2 \le \sqrt{\|A\|_1 \, \| A \|_\infty} \rho(A^TA) \le||A||1*||A||_\infty A = \begin{bmatrix}a&b\\c&d\end{bmatrix} \\
A^T = \begin{bmatrix}a&c\\b&d\end{bmatrix} \\
A^tA = \begin{bmatrix}a^2+b^2&ac+bd\\ac+bc&c^2+d^2\end{bmatrix} -\lambda^2-\lambda(a^2+b^2+c^2+d^2)+(ad-bc)^2 = 0 \Rightarrow(ad-bc)^2 = \lambda(a^2+b^2+c^2+d^2)+\lambda^2 ||A||_1 a+c b+d ||A|||_\infty a+b c+d c^2+cd+a(c+d) \\
a^2+a(b+c)+bc \\
b^2+a(b+d)+db \\
d^2+d(b+c)+bc \lambda = \frac{(a^2+b^2+c^2+d^2)\pm \sqrt{(a^2+b^2+c^2+d^2)^2+4*(ad-bc)^2}}{-2} (a^2+b^2+c^2+d^2) \lambda (a^2+b^2+c^2+d^2)-((a^2+b^2+c^2+d^2)+ something)","['linear-algebra', 'matrices', 'solution-verification', 'matrix-norms', 'spectral-norm']"
27,Deriving solution to matrix equation $AV + VA - tAVA = I$,Deriving solution to matrix equation,AV + VA - tAVA = I,"Consider the matrix equation $$ AV + VA - tAVA = I, $$ with $V$ square. Here we assume that $A$ and scalar $t$ are given, with $A$ symmetric positive definite and $0 < t < \tfrac{2}{\lambda_{\rm max}(A)}$ . Here, $\lambda_{\rm max}(A)$ denotes the largest eigenvalue of $A$ . We consider solutions of this equation in matrix variable $V$ . Apparently, a solution to this equation is $V = (2A - tA^2)^{-1}$ .  And indeed, it is easy to verify this solution is valid: $$ AV + VA -tAVA = 2(2I - tA)^{-1} - t(2I - tA)^{-1}A = (2I - tA)^{-1}(2I - tA) = I.  $$ (This calculation follows since $A$ is nonsingular under the stated hypotheses.) But I wonder if there is a clean/intuitive/straightforward way to derive this. One can see this easily if one knows that $AV = VA$ , i.e., that they commute. It is also clear to see this if the dimension is 1: $$ av + va - tava = 1 \quad \mbox{implies} \quad (2a - ta^2)v = 1. $$ But is there a nice way to see this in general?","Consider the matrix equation with square. Here we assume that and scalar are given, with symmetric positive definite and . Here, denotes the largest eigenvalue of . We consider solutions of this equation in matrix variable . Apparently, a solution to this equation is .  And indeed, it is easy to verify this solution is valid: (This calculation follows since is nonsingular under the stated hypotheses.) But I wonder if there is a clean/intuitive/straightforward way to derive this. One can see this easily if one knows that , i.e., that they commute. It is also clear to see this if the dimension is 1: But is there a nice way to see this in general?","
AV + VA - tAVA = I,
 V A t A 0 < t < \tfrac{2}{\lambda_{\rm max}(A)} \lambda_{\rm max}(A) A V V = (2A - tA^2)^{-1} 
AV + VA -tAVA = 2(2I - tA)^{-1} - t(2I - tA)^{-1}A = (2I - tA)^{-1}(2I - tA) = I. 
 A AV = VA 
av + va - tava = 1 \quad \mbox{implies} \quad (2a - ta^2)v = 1.
","['linear-algebra', 'matrices', 'matrix-equations']"
28,Calculate the gradient of a linear scalar field [duplicate],Calculate the gradient of a linear scalar field [duplicate],,"This question already has answers here : Gradient of $a^T X b$ with respect to $X$ (3 answers) Derivation of $\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$ [duplicate] (2 answers) Closed 3 years ago . I am trying to calculate the following gradient $$\nabla_{\mathbf{X}} \left( \mathbf{a}^{T} \mathbf{X} \mathbf{a} \right)$$ where I am using the convention that $\mathbf{a}$ is a column vector. I am wondering what the steps are to extract the solution from the matrix cookbook, which is: $$\nabla_{\mathbf{X}} \left( \mathbf{a}^{T} \mathbf{X} \mathbf{a} \right) = \mathbf{a}\cdot\mathbf{a}^{T}$$","This question already has answers here : Gradient of $a^T X b$ with respect to $X$ (3 answers) Derivation of $\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$ [duplicate] (2 answers) Closed 3 years ago . I am trying to calculate the following gradient where I am using the convention that is a column vector. I am wondering what the steps are to extract the solution from the matrix cookbook, which is:",\nabla_{\mathbf{X}} \left( \mathbf{a}^{T} \mathbf{X} \mathbf{a} \right) \mathbf{a} \nabla_{\mathbf{X}} \left( \mathbf{a}^{T} \mathbf{X} \mathbf{a} \right) = \mathbf{a}\cdot\mathbf{a}^{T},"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'scalar-fields']"
29,What is the rank of a vector?,What is the rank of a vector?,,"From linear algebra we know that the rank of a matrix is the maximal number of linearly independent columns or rows in a matrix. So, for a matrix , the rank can be determined by simple row reduction, determinant, etc. However, I am wondering how the concept of a rank applies to a single vector, i.e., $\mathbf{v} = [a, \ b, \ c]^{\top}$ . My intuition suggests that the rank must be equal to 1, but I'm not even sure if it is defined for a vector. Can anyone help shed some light on this issue? Thanks in advance.","From linear algebra we know that the rank of a matrix is the maximal number of linearly independent columns or rows in a matrix. So, for a matrix , the rank can be determined by simple row reduction, determinant, etc. However, I am wondering how the concept of a rank applies to a single vector, i.e., . My intuition suggests that the rank must be equal to 1, but I'm not even sure if it is defined for a vector. Can anyone help shed some light on this issue? Thanks in advance.","\mathbf{v} = [a, \ b, \ c]^{\top}","['linear-algebra', 'matrices', 'vectors', 'matrix-rank']"
30,"Let $A, B$ be skew-symmetric matrices such that $AB = -BA$. Show that $AB = 0$",Let  be skew-symmetric matrices such that . Show that,"A, B AB = -BA AB = 0","Let $A, B$ be skew-symmetric matrices such that $AB = -BA$ . Show that $AB = 0$ . I know that $AB$ is skew-symmetric,because $$(AB)^t=B^tA^t=BA=-AB$$ but I don't know how show that $AB=0$ .","Let be skew-symmetric matrices such that . Show that . I know that is skew-symmetric,because but I don't know how show that .","A, B AB = -BA AB = 0 AB (AB)^t=B^tA^t=BA=-AB AB=0","['linear-algebra', 'matrices', 'skew-symmetric-matrices']"
31,Evaluate $\lim_{h\to 0}\frac{1}{h^2}\begin{vmatrix}\tan x&\tan(x+h)&\tan(x+2h)\\\tan(x+2h)&\tan x&\tan(x+h)\\\tan(x+h)&\tan(x+2h)&\tan x\end{vmatrix}$,Evaluate,\lim_{h\to 0}\frac{1}{h^2}\begin{vmatrix}\tan x&\tan(x+h)&\tan(x+2h)\\\tan(x+2h)&\tan x&\tan(x+h)\\\tan(x+h)&\tan(x+2h)&\tan x\end{vmatrix},"Evaluate $$ \lim_{h\to 0}\frac{\Delta}{h^2}=\lim_{h\to 0}\frac{1}{h^2}\begin{vmatrix} \tan x&\tan(x+h)&\tan(x+2h)\\ \tan(x+2h)&\tan x&\tan(x+h)\\ \tan(x+h)&\tan(x+2h)&\tan x \end{vmatrix} $$ Attempt $$ \lim_{h\to 0}\frac{\Delta}{h^2}=\begin{vmatrix} \lim_{h\to 0}\tan x&\lim_{h\to 0}\dfrac{\tan(x+h)-\tan x}{h}&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}\\ \lim_{h\to 0}\tan(x+2h)&\lim_{h\to 0}\dfrac{\tan x-\tan(x+2h)}{h}&\lim_{h\to 0}\dfrac{\tan(x+h)-\tan(x+2h)}{h}\\ \lim_{h\to 0}\tan(x+h)&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}&\lim_{h\to 0}\dfrac{\tan x-\tan(x+2h)}{h} \end{vmatrix}\\ =\begin{vmatrix} \lim_{h\to 0}\tan x&\lim_{h\to 0}\dfrac{\tan(x+h)-\tan x}{h}&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}\\ \lim_{h\to 0}\tan(x+2h)&-2.\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan x}{h}&-1.\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}\\ \lim_{h\to 0}\tan(x+h)&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}&-2.\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan x}{2h} \end{vmatrix}\\ $$ $$ \lim_{h\to 0}\dfrac{\tan(x+h)-\tan x}{h}=\frac{d}{dx}\tan x=\sec^2x\\ \lim_{h\to 0}\dfrac{\tan(x+2h)-\tan x}{2h}=\frac{d}{dx}\tan x=\sec^2x\\ \lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}=\frac{d}{dx}\tan(x+h)=\sec^2(x+h) $$ But my reference gives the solution $9\tan x.\sec^4x$ , I think by taking $\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}=\sec^2x$ . Will that make a difference ? It might be silly but could anyone clarify this confusion in my attempt ?","Evaluate Attempt But my reference gives the solution , I think by taking . Will that make a difference ? It might be silly but could anyone clarify this confusion in my attempt ?","
\lim_{h\to 0}\frac{\Delta}{h^2}=\lim_{h\to 0}\frac{1}{h^2}\begin{vmatrix}
\tan x&\tan(x+h)&\tan(x+2h)\\
\tan(x+2h)&\tan x&\tan(x+h)\\
\tan(x+h)&\tan(x+2h)&\tan x
\end{vmatrix}
 
\lim_{h\to 0}\frac{\Delta}{h^2}=\begin{vmatrix}
\lim_{h\to 0}\tan x&\lim_{h\to 0}\dfrac{\tan(x+h)-\tan x}{h}&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}\\
\lim_{h\to 0}\tan(x+2h)&\lim_{h\to 0}\dfrac{\tan x-\tan(x+2h)}{h}&\lim_{h\to 0}\dfrac{\tan(x+h)-\tan(x+2h)}{h}\\
\lim_{h\to 0}\tan(x+h)&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}&\lim_{h\to 0}\dfrac{\tan x-\tan(x+2h)}{h}
\end{vmatrix}\\
=\begin{vmatrix}
\lim_{h\to 0}\tan x&\lim_{h\to 0}\dfrac{\tan(x+h)-\tan x}{h}&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}\\
\lim_{h\to 0}\tan(x+2h)&-2.\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan x}{h}&-1.\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}\\
\lim_{h\to 0}\tan(x+h)&\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}&-2.\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan x}{2h}
\end{vmatrix}\\
 
\lim_{h\to 0}\dfrac{\tan(x+h)-\tan x}{h}=\frac{d}{dx}\tan x=\sec^2x\\
\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan x}{2h}=\frac{d}{dx}\tan x=\sec^2x\\
\lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}=\frac{d}{dx}\tan(x+h)=\sec^2(x+h)
 9\tan x.\sec^4x \lim_{h\to 0}\dfrac{\tan(x+2h)-\tan(x+h)}{h}=\sec^2x","['matrices', 'limits', 'derivatives', 'trigonometry', 'determinant']"
32,congruent matrices,congruent matrices,,"Show that in $M_3(\mathbb{Z}_7), \begin{pmatrix}3 & 0 & 0 \\ 0 & 3 & 0\\ 0 & 0 & 0\end{pmatrix} \cong \begin{pmatrix}1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0\end{pmatrix}$ but $\begin{pmatrix}3 & 0 & 0 \\ 0 & 1 & 0\\ 0 & 0 & 0\end{pmatrix} \not\cong \begin{pmatrix}1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0\end{pmatrix}.$ I know that to show that two matrices $A$ and $B$ are congruent, it suffices to show that for some invertible matrix $P, A = P^T B P$ . However, I am unable to find such an invertible matrix, so I was wondering if it was easier to arrive at a contradiction if I assume that no such matrix exists? To show that the given two matrices are not congruent, one way (though obviously not very generalizable) is to show that no matrix $P = \begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i\end{pmatrix} \in M_3(\mathbb{Z}_7)$ can satisfy that $P^T AP = B,$ where $A = \begin{pmatrix}3 & 0 & 0 \\ 0 & 1 & 0\\ 0 & 0 & 0\end{pmatrix}$ and $B = \begin{pmatrix}1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0\end{pmatrix}$ (in this case this works, but it may be harder to show this if such a matrix exists, but it just isn't invertible). One can do so by manipulating a system of equations and showing that there is never a solution (e.g. one can fix a variable $d$ and consider when $d\in \{0,1,2,3,4,5,6\}$ ). So I was wondering if there was an easier approach for this?","Show that in but I know that to show that two matrices and are congruent, it suffices to show that for some invertible matrix . However, I am unable to find such an invertible matrix, so I was wondering if it was easier to arrive at a contradiction if I assume that no such matrix exists? To show that the given two matrices are not congruent, one way (though obviously not very generalizable) is to show that no matrix can satisfy that where and (in this case this works, but it may be harder to show this if such a matrix exists, but it just isn't invertible). One can do so by manipulating a system of equations and showing that there is never a solution (e.g. one can fix a variable and consider when ). So I was wondering if there was an easier approach for this?","M_3(\mathbb{Z}_7), \begin{pmatrix}3 & 0 & 0 \\
0 & 3 & 0\\
0 & 0 & 0\end{pmatrix} \cong \begin{pmatrix}1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix} \begin{pmatrix}3 & 0 & 0 \\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix} \not\cong \begin{pmatrix}1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix}. A B P, A = P^T B P P = \begin{pmatrix}a & b & c\\
d & e & f\\
g & h & i\end{pmatrix} \in M_3(\mathbb{Z}_7) P^T AP = B, A = \begin{pmatrix}3 & 0 & 0 \\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix} B = \begin{pmatrix}1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix} d d\in \{0,1,2,3,4,5,6\}","['linear-algebra', 'matrices']"
33,"For a projection $\Pi$, is $\text{tr}(\Pi X)\leq \text{tr}(X)$?","For a projection , is ?",\Pi \text{tr}(\Pi X)\leq \text{tr}(X),"All matrices are finite dimensional symmetric positive semidefinite matrices in this question. Let $\Pi$ be projection i.e. in its eigenbasis, it is the the identity matrix with some diagonal elements replaced by $0$ . Let $X$ be an arbitrary symmetric positive definite matrix. Is it true that $$\text{tr}(\Pi X)\leq \text{tr}(X)$$ Using the answer here , I see that it is indeed true that $\text{tr}(\Pi X)\leq \text{rank}(\Pi)\text{tr}(X)$ but I was hoping the rank term could also be dropped.","All matrices are finite dimensional symmetric positive semidefinite matrices in this question. Let be projection i.e. in its eigenbasis, it is the the identity matrix with some diagonal elements replaced by . Let be an arbitrary symmetric positive definite matrix. Is it true that Using the answer here , I see that it is indeed true that but I was hoping the rank term could also be dropped.",\Pi 0 X \text{tr}(\Pi X)\leq \text{tr}(X) \text{tr}(\Pi X)\leq \text{rank}(\Pi)\text{tr}(X),"['linear-algebra', 'matrices', 'trace', 'projection-matrices']"
34,Show $\det(F_n)=1$ for all $n$,Show  for all,\det(F_n)=1 n,"Consider the $n\times n$ matrix $F_n= (f_{i,j})$ of binomial coefficients $$f_{i,j}=\begin{pmatrix}i-1+j-1\\i-1\end{pmatrix}$$ Prove that $\det(F_n)=1$ for all $n$ . My current idea is to apply Leibniz formula for determinants and induction, but it seems too complicated. Any better ideas and suggestions are welcome.","Consider the matrix of binomial coefficients Prove that for all . My current idea is to apply Leibniz formula for determinants and induction, but it seems too complicated. Any better ideas and suggestions are welcome.","n\times n F_n= (f_{i,j}) f_{i,j}=\begin{pmatrix}i-1+j-1\\i-1\end{pmatrix} \det(F_n)=1 n","['linear-algebra', 'matrices', 'determinant']"
35,Proof that Hadamard matrices of order $4k+2$ don't exist,Proof that Hadamard matrices of order  don't exist,4k+2,"It's known that Hadamard matrices can only exist for orders $1$ , $2$ and $4k$ . It's easy to show that there are no Hadamard matrices of order $2k+1$ . But what is the proof that there are no Hadamard matrices of order $4k+2$ ?","It's known that Hadamard matrices can only exist for orders , and . It's easy to show that there are no Hadamard matrices of order . But what is the proof that there are no Hadamard matrices of order ?",1 2 4k 2k+1 4k+2,"['linear-algebra', 'matrices', 'hadamard-matrices']"
36,"If $(A-\lambda I)x_0=0,~y_0^{T}(A-\lambda I)=0$ and $y_0^{T}x_0=0$, prove that eigenvalue $\lambda$ is not simple.","If  and , prove that eigenvalue  is not simple.","(A-\lambda I)x_0=0,~y_0^{T}(A-\lambda I)=0 y_0^{T}x_0=0 \lambda","Let $A\in M_{n\times n}(\Bbb R), \lambda\in\sigma(A)\cap\Bbb R\setminus\{0\}$ . If $x_0,\,y_0$ are real eigenvectors of $A$ such that $(A-\lambda I)x_0=0$ and $y_0^{T}(A-\lambda I)=0$ and $y_0^{T}x_0=0$ , prove that eigenvalue $\lambda$ is not simple, i.e. has algebraic multiplicity $>1$ . Attempt. We just need to work with the case that geometric multiplicity of lambda is equal to $=1$ . Thanks in advance.","Let . If are real eigenvectors of such that and and , prove that eigenvalue is not simple, i.e. has algebraic multiplicity . Attempt. We just need to work with the case that geometric multiplicity of lambda is equal to . Thanks in advance.","A\in M_{n\times n}(\Bbb R), \lambda\in\sigma(A)\cap\Bbb R\setminus\{0\} x_0,\,y_0 A (A-\lambda I)x_0=0 y_0^{T}(A-\lambda I)=0 y_0^{T}x_0=0 \lambda >1 =1","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
37,Product of a symmetric and anti-symmetric matrix,Product of a symmetric and anti-symmetric matrix,,"I have the following question about matrices, Let $S$ and $A$ be two $n \times n$ matrices which are respectively symmetric and anti-symmetric. Can I conclude anything about the products $SA$ or $AS$ , are they symmetric or anti-symmetric? This is part of a bigger problem where I have already shown, $$ \langle x, Ax \rangle = 0$$ For $A$ antisymmetric, but I require that $$ \langle Mx, Ax \rangle = 0$$ For some matrix M, what conditions could I impose on $M$ to satisfy this, I was hoping symmetry would be sufficient or do I require something stronger such as diagonality?","I have the following question about matrices, Let and be two matrices which are respectively symmetric and anti-symmetric. Can I conclude anything about the products or , are they symmetric or anti-symmetric? This is part of a bigger problem where I have already shown, For antisymmetric, but I require that For some matrix M, what conditions could I impose on to satisfy this, I was hoping symmetry would be sufficient or do I require something stronger such as diagonality?","S A n \times n SA AS  \langle x, Ax \rangle = 0 A  \langle Mx, Ax \rangle = 0 M","['linear-algebra', 'matrices', 'inner-products', 'symmetric-matrices', 'skew-symmetric-matrices']"
38,Using the equation $ A^{-1} + B^{-1} = (A+B)^{-1} $ to find relations between det(A) and det(B),Using the equation  to find relations between det(A) and det(B), A^{-1} + B^{-1} = (A+B)^{-1} ,"We are given a condition, $$ A^{-1} + B^{-1} =  (A+B)^{-1} $$ Further, $|A| =4$ and we are asked to find the value of |B|. I tried to simplify the LHS $$ A^{-1} + B^{-1}= B^{-1}BA^{-1} + B^{-1}AA^{-1} $$ $$ A^{-1} + B^{-1}= B^{-1}(A+B)A^{-1} $$ That only lead me to get $$ |A+B|^2=|A||B| $$ Am stuck after this. Any help would be appreciated! Thanks.","We are given a condition, Further, and we are asked to find the value of |B|. I tried to simplify the LHS That only lead me to get Am stuck after this. Any help would be appreciated! Thanks.","
A^{-1} + B^{-1} =  (A+B)^{-1}
 |A| =4 
A^{-1} + B^{-1}= B^{-1}BA^{-1} + B^{-1}AA^{-1}
 
A^{-1} + B^{-1}= B^{-1}(A+B)A^{-1}
 
|A+B|^2=|A||B|
","['linear-algebra', 'matrices', 'determinant']"
39,What is the square root of the quadratic form $x^T A x$?,What is the square root of the quadratic form ?,x^T A x,"If $x \in \mathbb R^N$ and $A \in \mathbb R^{N \times N}$ , is it possible to find a root of the quadratic form $x^T A x$ of the form $$\sqrt{x^T A x} = b^T x$$ for all $x \in \mathbb R^N$ , where $b \in \mathbb R^N$ ?","If and , is it possible to find a root of the quadratic form of the form for all , where ?",x \in \mathbb R^N A \in \mathbb R^{N \times N} x^T A x \sqrt{x^T A x} = b^T x x \in \mathbb R^N b \in \mathbb R^N,"['matrices', 'radicals', 'quadratic-forms']"
40,"If $A, B$ are positive semi-definite, then $Tr(AB) = 0$ iff $AB = 0.$","If  are positive semi-definite, then  iff","A, B Tr(AB) = 0 AB = 0.","I am trying to prove the statement above, and should note that I am new to linear algebra, especially matrices. Here is my attempt, which has been inspired by this post and Wikipedia readings: If $AB = 0,$ $Tr(AB) = 0$ trivially. Now suppose $Tr(AB) = 0$ . $$ Tr(AB) = Tr(BA) \\ = Tr(B^{1/2}B^{1/2}A) \\ = Tr(B^{1/2}AB^{1/2}) \\ = Tr(B^{1/2}A^{1/2}A^{1/2}B^{1/2}) \\ = Tr(A^{1/2}B^{1/2}A^{1/2}B^{1/2}).$$ The first equality holds by the commutativity of trace, the 2nd since $B$ is PSD $\implies B^{1/2}$ exists, similarly for A. The last equality holds since we can consider $B^{1/2}A^{1/2}A^{1/2}B^{1/2}$ as the product of three symmetric matrices, and thus can permute them however we want. But at this point, I am stuck. Maybe we can bring in the initial assumption now, but I'm not sure how it fits in exactly. I'd appreciate any help/clarification!","I am trying to prove the statement above, and should note that I am new to linear algebra, especially matrices. Here is my attempt, which has been inspired by this post and Wikipedia readings: If trivially. Now suppose . The first equality holds by the commutativity of trace, the 2nd since is PSD exists, similarly for A. The last equality holds since we can consider as the product of three symmetric matrices, and thus can permute them however we want. But at this point, I am stuck. Maybe we can bring in the initial assumption now, but I'm not sure how it fits in exactly. I'd appreciate any help/clarification!","AB = 0, Tr(AB) = 0 Tr(AB) = 0  Tr(AB) = Tr(BA) \\
= Tr(B^{1/2}B^{1/2}A) \\
= Tr(B^{1/2}AB^{1/2}) \\
= Tr(B^{1/2}A^{1/2}A^{1/2}B^{1/2}) \\
= Tr(A^{1/2}B^{1/2}A^{1/2}B^{1/2}). B \implies B^{1/2} B^{1/2}A^{1/2}A^{1/2}B^{1/2}","['linear-algebra', 'matrices', 'proof-explanation', 'trace', 'positive-semidefinite']"
41,"Find the values for which $A^2 = I_2$, A is a matrix, with $A \neq I_2$ and $A \neq -I_2$","Find the values for which , A is a matrix, with  and",A^2 = I_2 A \neq I_2 A \neq -I_2,First I tried to find $A^2$ with $$     A=\begin{bmatrix}     \alpha & \beta\\     \delta & \gamma\\     \end{bmatrix} $$ I multiplied this by itself and got: $$     \begin{bmatrix}     \alpha^2+\beta\delta& \beta(\alpha + \gamma)\\     \delta (\alpha + \gamma) & \delta\beta+\gamma^2\\     \end{bmatrix} $$ I put this in a system: $$ \left\{  \begin{array}{c} \alpha^2+\beta\delta = 1 \\  \beta(\alpha + \gamma) = 0 \\  \delta (\alpha + \gamma) = 0  \\ \delta\beta+\gamma^2 = 1 \\ \end{array} \right.  $$ I tried to solve for $\beta$ first and right away got an issue: $$\beta = \frac{1-\alpha^2}{\delta}$$ One solution given by my book is: $$     \begin{bmatrix}     1& 0\\     0 & -1\\     \end{bmatrix} $$ So $\delta$ can be zero but according to my system it can't. How is this possible?,First I tried to find with I multiplied this by itself and got: I put this in a system: I tried to solve for first and right away got an issue: One solution given by my book is: So can be zero but according to my system it can't. How is this possible?,"A^2 
    A=\begin{bmatrix}
    \alpha & \beta\\
    \delta & \gamma\\
    \end{bmatrix}
 
    \begin{bmatrix}
    \alpha^2+\beta\delta& \beta(\alpha + \gamma)\\
    \delta (\alpha + \gamma) & \delta\beta+\gamma^2\\
    \end{bmatrix}
 
\left\{ 
\begin{array}{c}
\alpha^2+\beta\delta = 1 \\ 
\beta(\alpha + \gamma) = 0 \\ 
\delta (\alpha + \gamma) = 0  \\
\delta\beta+\gamma^2 = 1 \\
\end{array}
\right. 
 \beta \beta = \frac{1-\alpha^2}{\delta} 
    \begin{bmatrix}
    1& 0\\
    0 & -1\\
    \end{bmatrix}
 \delta","['linear-algebra', 'matrices']"
42,Find all real matrices such that $X^{6} + 2X^{4} + 10X = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$,Find all real matrices such that,X^{6} + 2X^{4} + 10X = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix},"Find all matrices in $M_2(\mathbb R)$ such that $$X^{6} + 2X^{4} + 10X = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$ I tried to take the determinant and trace of both side, but it seems like it only works when the determinant of the RHS equals $0$ . Can you guys help me please? Thank you.","Find all matrices in such that I tried to take the determinant and trace of both side, but it seems like it only works when the determinant of the RHS equals . Can you guys help me please? Thank you.",M_2(\mathbb R) X^{6} + 2X^{4} + 10X = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} 0,"['linear-algebra', 'matrices', 'matrix-equations']"
43,Polynomial with no real roots implies that $\det(P(A))\ge 0$,Polynomial with no real roots implies that,\det(P(A))\ge 0,"Let $P \in \mathbb{R}[X]$ be a polynomial of degree $n$ , $n\in \mathbb{N}$ , which has no real roots. If $A\in \mathcal{M_n(\mathbb{R})}$ , then prove that $\det(P(A))\ge 0$ . I don't know how to approach the general case, but for $n=2$ I was able to prove this by using the canonical form of a quadratic. However, I don't think that this can be generalised to any $n\in \mathbb{N}$ and I don't have any other approaches to this question.","Let be a polynomial of degree , , which has no real roots. If , then prove that . I don't know how to approach the general case, but for I was able to prove this by using the canonical form of a quadratic. However, I don't think that this can be generalised to any and I don't have any other approaches to this question.",P \in \mathbb{R}[X] n n\in \mathbb{N} A\in \mathcal{M_n(\mathbb{R})} \det(P(A))\ge 0 n=2 n\in \mathbb{N},"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
44,Higher powers of a matrix's relation with its trace,Higher powers of a matrix's relation with its trace,,"Let $A=[a_{ij}]$ , where $a_{ij}=u_{i}v_{j}, 1 \leq i \leq n$ and $1\leq j \leq n$ and $u_i,v_j$ belong to $R$ satisfies $A^5=16A$ . Find trace(A). I denoted U as a column matrix having values u1,u2,...,un. And V a row matrix having values v1,v2,...,vn. So that A=UV. But I am not able to proceed further. Evaluating A^5 would be very tedious so I think that I am missing the trick in this question. Also backtracking from the answer, I feel $A^5=(trace(A))^4A$ . Is there any easy way to prove this?","Let , where and and belong to satisfies . Find trace(A). I denoted U as a column matrix having values u1,u2,...,un. And V a row matrix having values v1,v2,...,vn. So that A=UV. But I am not able to proceed further. Evaluating A^5 would be very tedious so I think that I am missing the trick in this question. Also backtracking from the answer, I feel . Is there any easy way to prove this?","A=[a_{ij}] a_{ij}=u_{i}v_{j}, 1 \leq i \leq n 1\leq j \leq n u_i,v_j R A^5=16A A^5=(trace(A))^4A","['linear-algebra', 'matrices']"
45,Matrices commuting with a given $3\times 3$ complex matrix.,Matrices commuting with a given  complex matrix.,3\times 3,"Let $A$ be a $3\times 3$ complex matrix. Let $C(A)$ be the vector space of complex matrices that commute with $A$ . Show that the complex dimension of $C(A)$ is at least $3$ . I know that this kind of questions has been asked many times on this site. And there is an explicit formula for the dimension of $C(A)$ given by Frobenius viewing the matrices $B$ that commute with $A$ as endomorphisms of $\mathbb C[\lambda]$ -module. But I am looking for a more elementary way to show the lower bound of the dimension of $C(A)$ is $3$ . For example, I have already found that $\operatorname{Span}\{ I, A \}$ is a two-dimensional subspace of $C(A)$ for $A\notin\operatorname{Span}\{I\}$ , where $I$ is the identity matrix. But how to find another matrix that is linearly independent of $\operatorname{Span}\{I, A\}$ ? Thanks.","Let be a complex matrix. Let be the vector space of complex matrices that commute with . Show that the complex dimension of is at least . I know that this kind of questions has been asked many times on this site. And there is an explicit formula for the dimension of given by Frobenius viewing the matrices that commute with as endomorphisms of -module. But I am looking for a more elementary way to show the lower bound of the dimension of is . For example, I have already found that is a two-dimensional subspace of for , where is the identity matrix. But how to find another matrix that is linearly independent of ? Thanks.","A 3\times 3 C(A) A C(A) 3 C(A) B A \mathbb C[\lambda] C(A) 3 \operatorname{Span}\{ I, A \} C(A) A\notin\operatorname{Span}\{I\} I \operatorname{Span}\{I, A\}","['linear-algebra', 'abstract-algebra', 'matrices']"
46,Prove that $A$ is nonsingular,Prove that  is nonsingular,A,"Problem: Let $M$ be a $n \times n$ nonsingular matrix, and $$M = \begin{bmatrix}     A \quad B \\     C \quad D \end{bmatrix}  \in \mathbb{K}^{n \times n}$$ with $\mathbb{K} = \mathbb{R}$ or $\mathbb{C}$ , $A \in \mathbb{K}^{k \times k}$ , $D \in \mathbb{K}^{q \times q}$ , $k<n$ . Prove that $A$ is nonsingular. My attempt: Since $M$ be a nonsingular matrix so every leading principal submatrices of $M$ nonsingular and $A$ be the leading pricipal submatrix of order $k$ of $M$ . Q.E.D Is that true? Thank all!","Problem: Let be a nonsingular matrix, and with or , , , . Prove that is nonsingular. My attempt: Since be a nonsingular matrix so every leading principal submatrices of nonsingular and be the leading pricipal submatrix of order of . Q.E.D Is that true? Thank all!","M n \times n M = \begin{bmatrix}
    A \quad B \\
    C \quad D
\end{bmatrix}  \in \mathbb{K}^{n \times n} \mathbb{K} = \mathbb{R} \mathbb{C} A \in \mathbb{K}^{k \times k} D \in \mathbb{K}^{q \times q} k<n A M M A k M","['linear-algebra', 'matrices', 'matrix-decomposition']"
47,Matrix complicated equation,Matrix complicated equation,,"Let $$A = \begin{bmatrix}     1 & 3 & 4\\     3 & 6 & 9\\     1 & 6 & 4   \end{bmatrix},$$ $B$ be a $3\times 3$ matrix and $$A \cdot A^{T} \cdot A +3B^{-1} =0$$ What would be the value of $ \det( \operatorname{adj} (A^{-1}(B^{-1}){2B^{T}}))$ ?",Let be a matrix and What would be the value of ?,"A = \begin{bmatrix}
    1 & 3 & 4\\
    3 & 6 & 9\\
    1 & 6 & 4
  \end{bmatrix}, B 3\times 3 A \cdot A^{T} \cdot A +3B^{-1} =0  \det( \operatorname{adj} (A^{-1}(B^{-1}){2B^{T}}))","['linear-algebra', 'matrices']"
48,How To Find The Unit Eigenvectors,How To Find The Unit Eigenvectors,,"I have the matrix $$\begin{pmatrix}3&-9\\-9&27\end{pmatrix}.$$ I found the eigenvalues of $0$ and $30.$ However, when I try to plug in $0$ for the Eigenvalues and row reduce, I get $0,0$ as my solutions for find $x_1$ and $x_2.$ This is not correct and when I tried to do it with the eigenvalue of $30,$ it also came out to $0,0.$ I know to find the unit you have to take the length of $0$ and $30.$ Any help would be appreciated.","I have the matrix I found the eigenvalues of and However, when I try to plug in for the Eigenvalues and row reduce, I get as my solutions for find and This is not correct and when I tried to do it with the eigenvalue of it also came out to I know to find the unit you have to take the length of and Any help would be appreciated.","\begin{pmatrix}3&-9\\-9&27\end{pmatrix}. 0 30. 0 0,0 x_1 x_2. 30, 0,0. 0 30.","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
49,Determine whether this map $\phi$ is an isomorphism,Determine whether this map  is an isomorphism,\phi,"$\langle M_2 (\mathbb{R}), \cdot \rangle$ with $\langle \mathbb{R}, \cdot \rangle$ where $\phi(A)$ is the determinant of matrix $A$ . I don't believe the map is an isomorphism because  I don't think it is injective (one-to-one). I know that if a matrix is not invertible, then the determinant will always be 0 and that the $\det(A) = \det(A^T)$ . However I am having a hard time backing up my answer and explaining what I know to show that the map is not an isomorphism. Here is what I have so far. Let $A$ be a $2 \times 2$ Matrix and let B be the transpose of A. Then $\phi(A) = \det(A) = \det(B) = \phi(B)$ . Is this sufficient enough to show that the map is not injective?","with where is the determinant of matrix . I don't believe the map is an isomorphism because  I don't think it is injective (one-to-one). I know that if a matrix is not invertible, then the determinant will always be 0 and that the . However I am having a hard time backing up my answer and explaining what I know to show that the map is not an isomorphism. Here is what I have so far. Let be a Matrix and let B be the transpose of A. Then . Is this sufficient enough to show that the map is not injective?","\langle M_2 (\mathbb{R}), \cdot \rangle \langle \mathbb{R}, \cdot \rangle \phi(A) A \det(A) = \det(A^T) A 2 \times 2 \phi(A) = \det(A) = \det(B) = \phi(B)","['abstract-algebra', 'matrices']"
50,Real matrix satisfying $A^3=4I_n-3A$,Real matrix satisfying,A^3=4I_n-3A,"Let $A\in M_n(\mathbb{R}) $ so that $A^3=4I_n-3A$ . Prove that $\det(A+I_n) =2^n$ . My work : $A$ 's eigenvalues are the roots of $x^3+3x-4=0$ , so one of the eigenvalues is $1$ and the others are $\lambda_1$ and $\lambda_2$ , the roots of $x^2+x+4=0$ . Hence, $$\det(A+I_n) =2(\lambda_1+1)(\lambda_2+1)=2(\lambda_1  \lambda_2 +\lambda_1+\lambda_2+1) =2(4-1+1)=8$$ What is my mistake?","Let so that . Prove that . My work : 's eigenvalues are the roots of , so one of the eigenvalues is and the others are and , the roots of . Hence, What is my mistake?",A\in M_n(\mathbb{R})  A^3=4I_n-3A \det(A+I_n) =2^n A x^3+3x-4=0 1 \lambda_1 \lambda_2 x^2+x+4=0 \det(A+I_n) =2(\lambda_1+1)(\lambda_2+1)=2(\lambda_1  \lambda_2 +\lambda_1+\lambda_2+1) =2(4-1+1)=8,"['linear-algebra', 'matrices', 'determinant']"
51,Are the eigenvalues of a matrix and just its diagonal related?,Are the eigenvalues of a matrix and just its diagonal related?,,"I have a matrix $\textbf{A}$ and form the diagonal matrix $\bar{\textbf{A}}$ from the diagonal entries of $\textbf{A}$ . Is there a relationship between the eigenvalues of $\textbf{A}$ and $\bar{\textbf{A}}$ , or at least between their spectral norms $||\textbf{A}||_2$ and $||\bar{\textbf{A}}||_2$ ?","I have a matrix and form the diagonal matrix from the diagonal entries of . Is there a relationship between the eigenvalues of and , or at least between their spectral norms and ?",\textbf{A} \bar{\textbf{A}} \textbf{A} \textbf{A} \bar{\textbf{A}} ||\textbf{A}||_2 ||\bar{\textbf{A}}||_2,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-norm']"
52,Let $A$ be $2 \times 2$ nonzero real matrix.which of the following is true?,Let  be  nonzero real matrix.which of the following is true?,A 2 \times 2,"Let $A$ be $2 \times 2$ nonzero real matrix.which of the following is   true? $(A)$ trace of $A^2$ is positive $(B)$ $A$ has non zero eigenvalue. $(C)$ All entries of $A^2$ can't be negative. $(D)$$A^2$ has at least one positive entry. I tried to find examples to counter these statements. I took $A$ as $\begin{pmatrix}1&2\\-3&2\end{pmatrix}$ and $A^2$ is $\begin{pmatrix}-5&6\\-9&-2\end{pmatrix}$ This cancels out option $A,C$ Now I am not sure how to figure out option $(B)$ and $(D)$ I tried to change numbers of $A$ to find the example that counters $(D)$ but it is time-consuming. Is there any fact that I am missing for $(B)$ and $(D)$ ? I think $A$ can have zero eigenvalues because in my experience I never saw any statement saying a matrix must have zero value to have an eigenvalue zero. So (D) is my last option to tick. What could be another way to solve this problem quickly?",Let be nonzero real matrix.which of the following is   true? trace of is positive has non zero eigenvalue. All entries of can't be negative. has at least one positive entry. I tried to find examples to counter these statements. I took as and is This cancels out option Now I am not sure how to figure out option and I tried to change numbers of to find the example that counters but it is time-consuming. Is there any fact that I am missing for and ? I think can have zero eigenvalues because in my experience I never saw any statement saying a matrix must have zero value to have an eigenvalue zero. So (D) is my last option to tick. What could be another way to solve this problem quickly?,"A 2 \times 2 (A) A^2 (B) A (C) A^2 (D)A^2 A \begin{pmatrix}1&2\\-3&2\end{pmatrix} A^2 \begin{pmatrix}-5&6\\-9&-2\end{pmatrix} A,C (B) (D) A (D) (B) (D) A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
53,Sum of two invertible matrices [duplicate],Sum of two invertible matrices [duplicate],,"This question already has answers here : Product or sum of invertible matrix give an invertible matrix? (3 answers) Closed 5 years ago . If A and B are two n x n invertible matrices, would the matrix result from A+B be invertible? I think it would because for a matrix to be invertible its determinant would have to be greater than 0, and if you add the determinants of two matrices greater than 0 you would have to get a non zero answer. But is there any way to prove this?","This question already has answers here : Product or sum of invertible matrix give an invertible matrix? (3 answers) Closed 5 years ago . If A and B are two n x n invertible matrices, would the matrix result from A+B be invertible? I think it would because for a matrix to be invertible its determinant would have to be greater than 0, and if you add the determinants of two matrices greater than 0 you would have to get a non zero answer. But is there any way to prove this?",,"['linear-algebra', 'matrices']"
54,Second derivative of a matrix quartic form,Second derivative of a matrix quartic form,,"I need to compute the second derivative of the following quartic expression: $$x^H A^H x x^H A x$$ where is Hermitian. I have tried to compute the first derivative, and if I am not wrong, it should be: $$(A+A^H) x x^H (A+A^H) x$$ But then, I do not know how to proceed to calculate the second derivative. Could someone sketch the steps I need to follow? Thank you.","I need to compute the second derivative of the following quartic expression: where is Hermitian. I have tried to compute the first derivative, and if I am not wrong, it should be: But then, I do not know how to proceed to calculate the second derivative. Could someone sketch the steps I need to follow? Thank you.",x^H A^H x x^H A x (A+A^H) x x^H (A+A^H) x,"['linear-algebra', 'matrices', 'derivatives', 'optimization']"
55,Find the determinant of $N \times N$ matrix,Find the determinant of  matrix,N \times N,"I have the following $N \times N$ matrix. \begin{vmatrix} 0 & 1 & 1 & \ldots & 1 \\  1 & a_1 & 0 & \ldots & 0 \\  1 & 0 & a_2 & \ldots & 0 \\  \vdots & \vdots& &\ddots& \vdots\\ 1 & 0 & 0 & \ldots & a_n \\  \end{vmatrix} There seems to be a pattern going on for the determinant of the $5 \times 5$ version of this matrix, but I'm not sure how I would find the determinant for the $N \times N$ one.","I have the following matrix. There seems to be a pattern going on for the determinant of the version of this matrix, but I'm not sure how I would find the determinant for the one.","N \times N \begin{vmatrix}
0 & 1 & 1 & \ldots & 1 \\ 
1 & a_1 & 0 & \ldots & 0 \\ 
1 & 0 & a_2 & \ldots & 0 \\ 
\vdots & \vdots& &\ddots& \vdots\\
1 & 0 & 0 & \ldots & a_n \\ 
\end{vmatrix} 5 \times 5 N \times N","['linear-algebra', 'matrices', 'determinant']"
56,Square root of $-I_2$,Square root of,-I_2,"I would like to get all matrices $N \in M_2(\mathbb R)$ such that $N^2 = -I_2$ . To start with, I know that $N_0=\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ works, and we can prove that every matrices $N$ that are similar to $N_0$ work. $i.e.$ Let $N \in M_2(\mathbb R)$ if $\exists P \in \mathrm{GL}_2(\mathbb R)$ such that $N = PN_0P^{-1}$ , then $N^2 = -I_2$ . My question is, is the converse true? Are all matrices $N \in M_2(\mathbb R)$ such that $N^2 = -I_2$ similar to $N_0$ ?","I would like to get all matrices such that . To start with, I know that works, and we can prove that every matrices that are similar to work. Let if such that , then . My question is, is the converse true? Are all matrices such that similar to ?","N \in M_2(\mathbb R) N^2 = -I_2 N_0=\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix} N N_0 i.e. N \in M_2(\mathbb R) \exists P \in \mathrm{GL}_2(\mathbb R) N = PN_0P^{-1} N^2 = -I_2 N \in M_2(\mathbb R) N^2 = -I_2 N_0","['linear-algebra', 'matrices']"
57,"If rank$(A) = 2$, then $A^2 \neq 0_3$","If rank, then",(A) = 2 A^2 \neq 0_3,"Let $A$ be a real $3 \times 3 $ matrix such that rank $(A) = 2$ . Prove that $A^2 \neq 0_3$ . where $0_3$ represents the null matrix of order $3$ . I am looking for a solution involving only basic manipulation using matrices. I already have a better solution using the range and the nullity of $A$ . Thank you in advance! Edit. No Sylvester's inequality, Jordan form or range+nullity / linear transformations. At most use the definition of the rank as the dimension of the column/row space.","Let be a real matrix such that rank . Prove that . where represents the null matrix of order . I am looking for a solution involving only basic manipulation using matrices. I already have a better solution using the range and the nullity of . Thank you in advance! Edit. No Sylvester's inequality, Jordan form or range+nullity / linear transformations. At most use the definition of the rank as the dimension of the column/row space.",A 3 \times 3  (A) = 2 A^2 \neq 0_3 0_3 3 A,['linear-algebra']
58,Rank of an NxN matrix,Rank of an NxN matrix,,"This question very similar to this one . I have this problem I'm working on. I think I have it figured out but I just want to make sure I am not misunderstanding how rank works. The question goes like this, Consider an nXn matrix where the elements go from 1 to $ n^2 $ as you read across and then down.  For instance a 3X3 matrix where n = 3 looks like this: $ \left( \begin{array}_ 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{array} \right)$ What is the rank of the nXn matrix, as a function of n, for n>=2? Drawing out the matrices for n=2 to n=5 I can spot a pretty obvious pattern in the matrix construction and the whole thing can be written in terms of n: $ \left( \begin{array}_ 1 & 2 & 3 & ... & n  \\ n+1 & n+2 & n+3 & ... & 2n  \\ 2n+1 & 2n+2 & 2n+3 & ... & 3n  \\ 3n+1 & 3n+2 & 3n+3 & ... & 4n  \\ ... &... &... & ... & ... \\ n^2-n+1  & n^2-n+2 & n^2-n+3 & ... & n^2 \end{array} \right)$ From this I can spot that as soon as you get past the 3rd row, you can sum the prior rows to cancel out that row. For instance take row 4 from above for the first column. If I take $ row 3 + row 2 - row 1 $ I get this value $ (2n+1) + (n+1) - (1) = 3n+1 $ Which is the next rows value in row 4. And that can be carried out on the rest of the columns to produce the same result. So for any arbitrary row k beyond row 3, I can make that row all zeros by doing a row reduction of: R k -> R k - R k-1 - R k-2 + R k-3 Thus the maximum rank this matrix can have is 3 correct? Is this a reasonable way to solve this problem or there a more direct approach I should be taking?","This question very similar to this one . I have this problem I'm working on. I think I have it figured out but I just want to make sure I am not misunderstanding how rank works. The question goes like this, Consider an nXn matrix where the elements go from 1 to $ n^2 $ as you read across and then down.  For instance a 3X3 matrix where n = 3 looks like this: $ \left( \begin{array}_ 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{array} \right)$ What is the rank of the nXn matrix, as a function of n, for n>=2? Drawing out the matrices for n=2 to n=5 I can spot a pretty obvious pattern in the matrix construction and the whole thing can be written in terms of n: $ \left( \begin{array}_ 1 & 2 & 3 & ... & n  \\ n+1 & n+2 & n+3 & ... & 2n  \\ 2n+1 & 2n+2 & 2n+3 & ... & 3n  \\ 3n+1 & 3n+2 & 3n+3 & ... & 4n  \\ ... &... &... & ... & ... \\ n^2-n+1  & n^2-n+2 & n^2-n+3 & ... & n^2 \end{array} \right)$ From this I can spot that as soon as you get past the 3rd row, you can sum the prior rows to cancel out that row. For instance take row 4 from above for the first column. If I take $ row 3 + row 2 - row 1 $ I get this value $ (2n+1) + (n+1) - (1) = 3n+1 $ Which is the next rows value in row 4. And that can be carried out on the rest of the columns to produce the same result. So for any arbitrary row k beyond row 3, I can make that row all zeros by doing a row reduction of: R k -> R k - R k-1 - R k-2 + R k-3 Thus the maximum rank this matrix can have is 3 correct? Is this a reasonable way to solve this problem or there a more direct approach I should be taking?",,"['linear-algebra', 'matrices', 'matrix-rank']"
59,"Second derivative of f(x,y)","Second derivative of f(x,y)",,"I try do finde the Matrix $D^3f(a)$ where $f(x,y)=x^4-3x^3y^2$ What i tried is $Df(x,y)=\left( \begin{array}{c} 4x^3-9x^2y^2 && -6x^3y\\ \end{array} \right)$ This is a $1\times 2 $ matrix. Then following this post I get: $ D^2f(x,y)= \left( \begin{array}{c} 12x^2-18xy^2 && -18x^2y && -18x^2y && -6x^3\\ \end{array} \right) $ and $ D^3f(x,y)= \left( \begin{array}{c} 24x-18y^2 && -36xy^2 && -36xy && -18x^2 && 36xy && -36x^2 && -18x^2 && -18x^3\\\\ \end{array} \right) $ which is a $8\times1$ matrix.  I have troubles by understanding how to get $D^rf(x,y)$ for arbitrary functions","I try do finde the Matrix $D^3f(a)$ where $f(x,y)=x^4-3x^3y^2$ What i tried is $Df(x,y)=\left( \begin{array}{c} 4x^3-9x^2y^2 && -6x^3y\\ \end{array} \right)$ This is a $1\times 2 $ matrix. Then following this post I get: $ D^2f(x,y)= \left( \begin{array}{c} 12x^2-18xy^2 && -18x^2y && -18x^2y && -6x^3\\ \end{array} \right) $ and $ D^3f(x,y)= \left( \begin{array}{c} 24x-18y^2 && -36xy^2 && -36xy && -18x^2 && 36xy && -36x^2 && -18x^2 && -18x^3\\\\ \end{array} \right) $ which is a $8\times1$ matrix.  I have troubles by understanding how to get $D^rf(x,y)$ for arbitrary functions",,"['matrices', 'ordinary-differential-equations', 'partial-derivative']"
60,Can't figure out how to solve matrix equation Ax=0,Can't figure out how to solve matrix equation Ax=0,,"I tried googling how to solve this matrix through RREF and parametric variables but failed to find something that works similarly to try and solve myself. A is a 2x3 matrix with the values going [ 2 -1 -1 : 1 -2  2 ] (imagine the set after the colon to be under the first set) x is [x_1, x_2, x_3] but obviously a column instead of a row and for lack of subscript key I just used ""_#"" to denote the same thing. 0 is a 2 row x 1 column matrix with two 0's. To start, I wrote out the matrix's into 2 equations. 2x_1 - x_2 - x_3 = 0 x_1 - 2x_2 + 2x_3 = 0 Then I turned it into a simplified coefficient matrix. [ 2 -1 -1 0: 1 -2  2 0] (with the second set of numbers past the colon under the first set) Afterward, I reduced it into RREF form to get: [ 1 0 (-4/3) 0 : 0 1 (-5/3) 0 ] (with the second set of numbers past the colon under the first set) Once I got this, I turned the matrix back to these equations: X_1 - (4/3)X_3 = 0 X_2 - (5/3)X_3 = 0 So then I set X_1 = (4/3)X_3 and X_2 = (5/3)X_3. I then tried setting X_3 to t to try and see if I could finagle an answer by solving for 't'. So far, my answers have been wrong every time in comparison to the answer my book gives. Can someone tell me if I messed up somewhere? I deduced it would have to be from my matrix reduction but I did it over a few times in different ways to make sure but I still can't seem to get the answer...","I tried googling how to solve this matrix through RREF and parametric variables but failed to find something that works similarly to try and solve myself. A is a 2x3 matrix with the values going [ 2 -1 -1 : 1 -2  2 ] (imagine the set after the colon to be under the first set) x is [x_1, x_2, x_3] but obviously a column instead of a row and for lack of subscript key I just used ""_#"" to denote the same thing. 0 is a 2 row x 1 column matrix with two 0's. To start, I wrote out the matrix's into 2 equations. 2x_1 - x_2 - x_3 = 0 x_1 - 2x_2 + 2x_3 = 0 Then I turned it into a simplified coefficient matrix. [ 2 -1 -1 0: 1 -2  2 0] (with the second set of numbers past the colon under the first set) Afterward, I reduced it into RREF form to get: [ 1 0 (-4/3) 0 : 0 1 (-5/3) 0 ] (with the second set of numbers past the colon under the first set) Once I got this, I turned the matrix back to these equations: X_1 - (4/3)X_3 = 0 X_2 - (5/3)X_3 = 0 So then I set X_1 = (4/3)X_3 and X_2 = (5/3)X_3. I then tried setting X_3 to t to try and see if I could finagle an answer by solving for 't'. So far, my answers have been wrong every time in comparison to the answer my book gives. Can someone tell me if I messed up somewhere? I deduced it would have to be from my matrix reduction but I did it over a few times in different ways to make sure but I still can't seem to get the answer...",,"['linear-algebra', 'matrices', 'parametric']"
61,Exponential function of a Hermitian matrix,Exponential function of a Hermitian matrix,,"Given $$H = \begin{pmatrix}\sin \theta & 0 & \cos \theta \\ 0 & 1 & 0 \\ \cos \theta & 0 & -\sin \theta \end{pmatrix}$$ where $\theta=\pi/6$, then what is $\exp{ \left( i \frac{\pi}{2} H \right)}$? I tried to calculate in the following way  $e^{(i\pi H)/2}=[e^{(i\pi/2)}]^H=i^H$. I do not know how to proceed.","Given $$H = \begin{pmatrix}\sin \theta & 0 & \cos \theta \\ 0 & 1 & 0 \\ \cos \theta & 0 & -\sin \theta \end{pmatrix}$$ where $\theta=\pi/6$, then what is $\exp{ \left( i \frac{\pi}{2} H \right)}$? I tried to calculate in the following way  $e^{(i\pi H)/2}=[e^{(i\pi/2)}]^H=i^H$. I do not know how to proceed.",,"['linear-algebra', 'matrices', 'matrix-exponential']"
62,Showing Orthogonal Projection Matrix Multiplied by Full-Rank Matrices is Positive-Definite,Showing Orthogonal Projection Matrix Multiplied by Full-Rank Matrices is Positive-Definite,,"Here is some useful information pertaining to my question: Let $X \in\mathbb{R}^{n \times m}$ and $Z \in\mathbb{R}^{n \times p}$ be full-rank matrices. Define $B = I_n - X(X^{T}X)^{-1}X^{T}$. Assume that the columns of $X$ are linearly independent from the columns of $Z$. I am trying to show that $Z^{T}BZ$ is positive definite. My first plan of attack was to (1) show that $Z^{T}BZ$ is an orthogonal projection matrix, (2) prove that $I$ is the only positive definite orthogonal projection matrix, and (3) prove that $Z^{T}BZ=I$. This fell through because I felt that there was not enough information about $Z$ to prove (1). I am pretty sure that I will ultimately need to show either (i) $z^{T}Z^{T}BZz>0$ $\forall$ $z\in \mathbb{R}^p$ OR (ii) all the eigenvalues of $Z^{T}BZ$ are positive. I think my biggest problem stems from not knowing how to deal with $Z^{T}$ and $Z$... clearly they are important, as it seems that we can only prove that $B$ is positive semi definite. However, I am unsure how this full-rank matrix can ""transform"" $B$ from psd to pd. Any assistance would be greatly appreciated! If it helps, I have already shown that B is an orthogonal projection matrix and that B is psd. :)","Here is some useful information pertaining to my question: Let $X \in\mathbb{R}^{n \times m}$ and $Z \in\mathbb{R}^{n \times p}$ be full-rank matrices. Define $B = I_n - X(X^{T}X)^{-1}X^{T}$. Assume that the columns of $X$ are linearly independent from the columns of $Z$. I am trying to show that $Z^{T}BZ$ is positive definite. My first plan of attack was to (1) show that $Z^{T}BZ$ is an orthogonal projection matrix, (2) prove that $I$ is the only positive definite orthogonal projection matrix, and (3) prove that $Z^{T}BZ=I$. This fell through because I felt that there was not enough information about $Z$ to prove (1). I am pretty sure that I will ultimately need to show either (i) $z^{T}Z^{T}BZz>0$ $\forall$ $z\in \mathbb{R}^p$ OR (ii) all the eigenvalues of $Z^{T}BZ$ are positive. I think my biggest problem stems from not knowing how to deal with $Z^{T}$ and $Z$... clearly they are important, as it seems that we can only prove that $B$ is positive semi definite. However, I am unsure how this full-rank matrix can ""transform"" $B$ from psd to pd. Any assistance would be greatly appreciated! If it helps, I have already shown that B is an orthogonal projection matrix and that B is psd. :)",,"['linear-algebra', 'matrices', 'matrix-rank', 'positive-definite', 'projection-matrices']"
63,Two skew symmetric matrices of same rank are congruent.,Two skew symmetric matrices of same rank are congruent.,,"Let $A$ and $B$ be two skew symmetric matrices of order $n$ over a finite field with characteristic not equal $2.$ Also let $\mathrm{Rank}(A)=\mathrm{Rank}(B)$ (which is even, we know). Then I want to show that there is an invertible matrix $P$ such that $P^tAP=B$ . I need some idea to prove this. Thanks","Let and be two skew symmetric matrices of order over a finite field with characteristic not equal Also let (which is even, we know). Then I want to show that there is an invertible matrix such that . I need some idea to prove this. Thanks",A B n 2. \mathrm{Rank}(A)=\mathrm{Rank}(B) P P^tAP=B,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations', 'skew-symmetric-matrices']"
64,Prove that a matrix $M$ is diagonalizable if $M^3 = M$,Prove that a matrix  is diagonalizable if,M M^3 = M,"Let $M$ be a real square matrix such that $M^3 = M$. Prove that $M$ is diagonalizable. Proof: We have a $M$, a $2\times2$ matrix, $M$ is diagonalizable if $M$ has 2 linearly independent eigenvectors ${v_1,v_2}$, if so, then: $D = P^{-1}MP$ We give a concrete example: Let $$ M = \begin{bmatrix}1&3\\2&2\end{bmatrix} $$ then the eigenvalues $x_1  =-1$ and $x_2 = 4$ with eigenvectors $(3,-2)$ and $(1,1)$, respectively. Hence, $$ P = \begin{bmatrix}3&1\\-2&1\end{bmatrix}, D = \begin{bmatrix}-1&0\\0&4\end{bmatrix} $$ This is my proof but I am not sure about the given condition in the statement $M^3 = M$, how can I use it?","Let $M$ be a real square matrix such that $M^3 = M$. Prove that $M$ is diagonalizable. Proof: We have a $M$, a $2\times2$ matrix, $M$ is diagonalizable if $M$ has 2 linearly independent eigenvectors ${v_1,v_2}$, if so, then: $D = P^{-1}MP$ We give a concrete example: Let $$ M = \begin{bmatrix}1&3\\2&2\end{bmatrix} $$ then the eigenvalues $x_1  =-1$ and $x_2 = 4$ with eigenvectors $(3,-2)$ and $(1,1)$, respectively. Hence, $$ P = \begin{bmatrix}3&1\\-2&1\end{bmatrix}, D = \begin{bmatrix}-1&0\\0&4\end{bmatrix} $$ This is my proof but I am not sure about the given condition in the statement $M^3 = M$, how can I use it?",,"['linear-algebra', 'matrices', 'diagonalization']"
65,Find the determinant of the following $5\times 5$ real matrix:,Find the determinant of the following  real matrix:,5\times 5,"Let $A \in \mathbb{R}^{5 \times 5}$ be the matrix $$\begin{bmatrix} a&a&a&a&b\\a&a&a&b&a\\a&a&b&a&a\\a&b&a&a&a\\b&a&a&a&a\end{bmatrix}$$ Find the determinant of $A$ . What I've done so far : $$\det\left(\begin{array}{l}a&a&a&a&b\\a&a&a&b&a\\a&a&b&a&a\\a&b&a&a&a\\b&a&a&a&a\end{array}\right) = \det\left(\begin{array}{l}b&a&a&a&a\\a&b&a&a&a\\a&a&b&a&a\\a&a&a&b&a\\a&a&a&a&b\end{array}\right)$$ (since switching two pairs of rows does not change the determinant) $$= \det\left(\begin{array}{l}b-a&0&0&0&a-b\\0&b-a&0&0&a-b\\0&0&b-a&0&a-b\\0&0&0&b-a&a-b\\a&a&a&a&b\end{array}\right)$$ (since adding a multiple of one row to another does not change the determinant) for all $1\le i\le 4 \rightarrow R_i-R_5$ Now I am quite stuck. I wanted to obtain a triangular matrix so I can calculate its determinant by the diagonal entries, but I don't know what to do with the last row. I've tried some column operations as well, but have had no success. Would be happy to get your help, thank you :)","Let be the matrix Find the determinant of . What I've done so far : (since switching two pairs of rows does not change the determinant) (since adding a multiple of one row to another does not change the determinant) for all Now I am quite stuck. I wanted to obtain a triangular matrix so I can calculate its determinant by the diagonal entries, but I don't know what to do with the last row. I've tried some column operations as well, but have had no success. Would be happy to get your help, thank you :)",A \in \mathbb{R}^{5 \times 5} \begin{bmatrix} a&a&a&a&b\\a&a&a&b&a\\a&a&b&a&a\\a&b&a&a&a\\b&a&a&a&a\end{bmatrix} A \det\left(\begin{array}{l}a&a&a&a&b\\a&a&a&b&a\\a&a&b&a&a\\a&b&a&a&a\\b&a&a&a&a\end{array}\right) = \det\left(\begin{array}{l}b&a&a&a&a\\a&b&a&a&a\\a&a&b&a&a\\a&a&a&b&a\\a&a&a&a&b\end{array}\right) = \det\left(\begin{array}{l}b-a&0&0&0&a-b\\0&b-a&0&0&a-b\\0&0&b-a&0&a-b\\0&0&0&b-a&a-b\\a&a&a&a&b\end{array}\right) 1\le i\le 4 \rightarrow R_i-R_5,"['linear-algebra', 'matrices', 'determinant', 'hankel-matrices']"
66,What is the order of this subgroup in $GL_2(\mathbb{F}_5)$?,What is the order of this subgroup in ?,GL_2(\mathbb{F}_5),"The following subgroup in $GL_2(\mathbb{F}_5)$ is supposed to be of order 20, but I keep calculating that the order is 25? The subgroup is: $\left\langle\begin{pmatrix}2 & 0\\0 & 1\end{pmatrix}, \begin{pmatrix}1 & 1\\0 & 1\end{pmatrix}\right\rangle$. I am confused because I am getting all the matrices in the format: \begin{pmatrix}a & b\\0 & 1\end{pmatrix} where $0\le a,b \le 4$. So essentially I am getting 25 elements in this matrix when there are supposed to be only 20. Am I doing any calculations wrong, or must there be a typo somewhere in the problem?","The following subgroup in $GL_2(\mathbb{F}_5)$ is supposed to be of order 20, but I keep calculating that the order is 25? The subgroup is: $\left\langle\begin{pmatrix}2 & 0\\0 & 1\end{pmatrix}, \begin{pmatrix}1 & 1\\0 & 1\end{pmatrix}\right\rangle$. I am confused because I am getting all the matrices in the format: \begin{pmatrix}a & b\\0 & 1\end{pmatrix} where $0\le a,b \le 4$. So essentially I am getting 25 elements in this matrix when there are supposed to be only 20. Am I doing any calculations wrong, or must there be a typo somewhere in the problem?",,"['abstract-algebra', 'matrices', 'group-theory']"
67,If $\mathfrak{so}(3)$ is the Lie algebra of $SO(3)$ then why are the matrices of $\mathfrak{so}(3)$ not rotation matrices?,If  is the Lie algebra of  then why are the matrices of  not rotation matrices?,\mathfrak{so}(3) SO(3) \mathfrak{so}(3),If $\mathfrak{so}(3)$ is the Lie algebra of $SO(3)$ then why are the matrices of $\mathfrak{so}(3)$ not rotation matrices? They aren't infinitesimal rotations either. The matrices of $\mathfrak{so}(3)$ are skew-symmetric matrices which are the type used to calculate the cross product. How can $\mathfrak{so}(3)$ be tangent to $SO(3)$ if they're never even in $SO(3)$?,If $\mathfrak{so}(3)$ is the Lie algebra of $SO(3)$ then why are the matrices of $\mathfrak{so}(3)$ not rotation matrices? They aren't infinitesimal rotations either. The matrices of $\mathfrak{so}(3)$ are skew-symmetric matrices which are the type used to calculate the cross product. How can $\mathfrak{so}(3)$ be tangent to $SO(3)$ if they're never even in $SO(3)$?,,"['matrices', 'lie-groups', 'lie-algebras']"
68,Prove that orthogonal matrix $Q$ preserves operator norm i.e. $||AQ||_2 = ||A||_2$,Prove that orthogonal matrix  preserves operator norm i.e.,Q ||AQ||_2 = ||A||_2,This question proves that $||QA||_2 = ||A||_2$: $||QA||_2 = \sup_{||x||=1}{||QAx||}=\sup_{||x||=1}{\sqrt{(QAx)^T(QAx)}} =  \sup_{||x||=1}{\sqrt{x^TA^TQ^TQAx|}} = sup_{||x||=1}{\sqrt{x^TA^TAx}} = ||A||_2$ How to prove it for $||AQ||_2$ too?,This question proves that $||QA||_2 = ||A||_2$: $||QA||_2 = \sup_{||x||=1}{||QAx||}=\sup_{||x||=1}{\sqrt{(QAx)^T(QAx)}} =  \sup_{||x||=1}{\sqrt{x^TA^TQ^TQAx|}} = sup_{||x||=1}{\sqrt{x^TA^TAx}} = ||A||_2$ How to prove it for $||AQ||_2$ too?,,"['matrices', 'normed-spaces', 'orthogonality', 'orthogonal-matrices']"
69,"Rotating a matrix itself, not applying a rotation to a space","Rotating a matrix itself, not applying a rotation to a space",,"How could I notate a matrix rotation? Example:  $ A = \begin{pmatrix} a & b \\ c & d \end{pmatrix},\:\:\: A_{\text{rotated}} = \begin{pmatrix} c & a \\ d & b\end{pmatrix}$. Notice the whole matrix is ""rotated"" clockwise. Is there any notation for this, and anyway to compute it generally via basic matrix operations such as addition and multiplication or other?","How could I notate a matrix rotation? Example:  $ A = \begin{pmatrix} a & b \\ c & d \end{pmatrix},\:\:\: A_{\text{rotated}} = \begin{pmatrix} c & a \\ d & b\end{pmatrix}$. Notice the whole matrix is ""rotated"" clockwise. Is there any notation for this, and anyway to compute it generally via basic matrix operations such as addition and multiplication or other?",,['matrices']
70,"Over a commutative ring, is a non-invertible matrix necessarily a zero divisor?","Over a commutative ring, is a non-invertible matrix necessarily a zero divisor?",,"I know that over a field, every non-invertible matrix is a zero divisor. Does the same hold for matrices over an arbitrary commutative ring?","I know that over a field, every non-invertible matrix is a zero divisor. Does the same hold for matrices over an arbitrary commutative ring?",,"['abstract-algebra', 'matrices', 'ring-theory', 'commutative-algebra']"
71,"$\operatorname{span}\{x_1,\ldots,x_n\} = \mathbb{C}^n$?",?,"\operatorname{span}\{x_1,\ldots,x_n\} = \mathbb{C}^n","If $A$ is a $n \times n$ Hermitian matrix, then is it right that, \begin{equation} \operatorname{span}\{x_1,\ldots,x_n\} = \mathbb{C}^n \text{?} \end{equation} where $x_i \ne 0$ is the eigenvector associated with the eigenvalue $\lambda_i$ of $A$. That is, $A x_i = \lambda_i x_i, i=1,\ldots,n$.","If $A$ is a $n \times n$ Hermitian matrix, then is it right that, \begin{equation} \operatorname{span}\{x_1,\ldots,x_n\} = \mathbb{C}^n \text{?} \end{equation} where $x_i \ne 0$ is the eigenvector associated with the eigenvalue $\lambda_i$ of $A$. That is, $A x_i = \lambda_i x_i, i=1,\ldots,n$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
72,"Solving the matrix equation $AP=PB$, where $A$ and $B$ are similar","Solving the matrix equation , where  and  are similar",AP=PB A B,"Currently I'm stuck with this problem: The following matrices are similar to each other. $$A=\begin{bmatrix}-3&0\\8&4\end{bmatrix},\qquad B=\begin{bmatrix}-4&4\\-2&5\end{bmatrix},\qquad C=\begin{bmatrix}k&1\\m&-2\end{bmatrix}.$$ How do you find the matrix $P$ such that $AP = PB$? And how can you calculate the values of $k$ and $m$? Thank you in advance.","Currently I'm stuck with this problem: The following matrices are similar to each other. $$A=\begin{bmatrix}-3&0\\8&4\end{bmatrix},\qquad B=\begin{bmatrix}-4&4\\-2&5\end{bmatrix},\qquad C=\begin{bmatrix}k&1\\m&-2\end{bmatrix}.$$ How do you find the matrix $P$ such that $AP = PB$? And how can you calculate the values of $k$ and $m$? Thank you in advance.",,"['matrices', 'matrix-equations', 'similar-matrices']"
73,"Find a non-zero integer matrix $X$ such that $XA=0$ where $X,A,0$ are all $4 \times 4$",Find a non-zero integer matrix  such that  where  are all,"X XA=0 X,A,0 4 \times 4",Let $A$ be the following $4 \times 4$ matrix. \begin{bmatrix}1&2&1&3\\1&3&2&4\\2&5&3&7\\1&4&1&5\end{bmatrix} How can we find a non-zero integer matrix $C$ such that $CA = 0_{4 \times 4}$ Note that $0$ is a $4 \times 4$ matrix.,Let $A$ be the following $4 \times 4$ matrix. \begin{bmatrix}1&2&1&3\\1&3&2&4\\2&5&3&7\\1&4&1&5\end{bmatrix} How can we find a non-zero integer matrix $C$ such that $CA = 0_{4 \times 4}$ Note that $0$ is a $4 \times 4$ matrix.,,"['linear-algebra', 'matrices']"
74,Gradient of $a^T X b$ with respect to $X$,Gradient of  with respect to,a^T X b X,"How can I find the gradient of the term $a^TXb$ where $X$ is a $n \times m$ matrix, and $a$ and $b$ are column vectors.  Since the gradient is with respect to a matrix, it should be a matrix.  But I do not have a clue on how to derive this gradient. Any help ?","How can I find the gradient of the term $a^TXb$ where $X$ is a $n \times m$ matrix, and $a$ and $b$ are column vectors.  Since the gradient is with respect to a matrix, it should be a matrix.  But I do not have a clue on how to derive this gradient. Any help ?",,"['matrices', 'derivatives', 'vector-analysis', 'matrix-calculus', 'scalar-fields']"
75,"Given is a linear map.., is zero an eigenvalue?","Given is a linear map.., is zero an eigenvalue?",,"$A$ is the linear mapping $f(x)= Ax,\mathbb{R} \rightarrow \mathbb{R}$ $$f\left( \begin{pmatrix} x_{1}\\  x_{2}\\  x_{3} \end{pmatrix}\right)= \begin{pmatrix} x_{2}-x_{3}\\  x_{1}+3x_{2}-2x_{3}\\  x_{1}-4x_{2}+5x_{3} \end{pmatrix}$$ Is zero an eigenvalue of $A$? I need the matrix $A$. I think we can get it by forming the thing above? So $$A= \begin{pmatrix} 0 &  1  & -1\\  1 &  3  & -2\\  1 & -4  &  5 \end{pmatrix}$$ I hope this is correct, if not everything else coming now will be wrong :s $$\begin{vmatrix} -\lambda &  1 & -1\\ 1 &  3-\lambda & -2\\  1 & -4 &  5-\lambda \end{vmatrix} \begin{matrix} -\lambda &  1 \\  1 &  3-\lambda \\  1 & -4 \end{matrix}$$ Characteristic polynomial $p_{A}(\lambda)= -\lambda(3-\lambda)(5-\lambda)+(-2)+4-(-(3-\lambda))-(-8\lambda)-(5-\lambda)$ $p_{A}(\lambda)= (-3\lambda+\lambda^{2})(5-\lambda)+2-(-3+\lambda)+8\lambda-5+\lambda$ $p_{A}(\lambda)= -15\lambda +3\lambda^{2}+5\lambda^{2}-\lambda^{3}+2+3-\lambda+8\lambda-5+\lambda$ $p_{A}(\lambda)= -\lambda^{3}+8\lambda^{2}-7\lambda$ We quickly see that $\lambda=0$ so zero is really an eigenvalue of $A$? Is it correct? I especially wasn't sure at the beginning if I formed it to a matrix correctly because I didn't work with these.. mappings? in combination with matrix before.","$A$ is the linear mapping $f(x)= Ax,\mathbb{R} \rightarrow \mathbb{R}$ $$f\left( \begin{pmatrix} x_{1}\\  x_{2}\\  x_{3} \end{pmatrix}\right)= \begin{pmatrix} x_{2}-x_{3}\\  x_{1}+3x_{2}-2x_{3}\\  x_{1}-4x_{2}+5x_{3} \end{pmatrix}$$ Is zero an eigenvalue of $A$? I need the matrix $A$. I think we can get it by forming the thing above? So $$A= \begin{pmatrix} 0 &  1  & -1\\  1 &  3  & -2\\  1 & -4  &  5 \end{pmatrix}$$ I hope this is correct, if not everything else coming now will be wrong :s $$\begin{vmatrix} -\lambda &  1 & -1\\ 1 &  3-\lambda & -2\\  1 & -4 &  5-\lambda \end{vmatrix} \begin{matrix} -\lambda &  1 \\  1 &  3-\lambda \\  1 & -4 \end{matrix}$$ Characteristic polynomial $p_{A}(\lambda)= -\lambda(3-\lambda)(5-\lambda)+(-2)+4-(-(3-\lambda))-(-8\lambda)-(5-\lambda)$ $p_{A}(\lambda)= (-3\lambda+\lambda^{2})(5-\lambda)+2-(-3+\lambda)+8\lambda-5+\lambda$ $p_{A}(\lambda)= -15\lambda +3\lambda^{2}+5\lambda^{2}-\lambda^{3}+2+3-\lambda+8\lambda-5+\lambda$ $p_{A}(\lambda)= -\lambda^{3}+8\lambda^{2}-7\lambda$ We quickly see that $\lambda=0$ so zero is really an eigenvalue of $A$? Is it correct? I especially wasn't sure at the beginning if I formed it to a matrix correctly because I didn't work with these.. mappings? in combination with matrix before.",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
76,Find the determinant of order $100$,Find the determinant of order,100,Find the determinant of order $100$: $$D=\begin{vmatrix} 5 &5 &5 &\ldots &5 &5 &-1\\ 5 &5 &5 &\ldots &5 &-1 &5\\ 5 &5 &5 &\ldots &-1 &5 &5\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 5 &5 &-1 &\ldots &5 &5 &5\\ 5 &-1 &5 &\ldots &5 &5 &5\\ -1 &5 &5 &\ldots &5 &5 &5 \end{vmatrix}$$ I think I should be using recurrence relations here but I'm not entirely sure how that method works. I tried this: Multiplying the first row by $(-1)$ and adding it to all rows:  $$D=\begin{vmatrix} 5 &5 &5 &\ldots &5 &5 &-1\\ 5 &5 &5 &\ldots &5 &-1 &5\\ 5 &5 &5 &\ldots &-1 &5 &5\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 5 &5 &-1 &\ldots &5 &5 &5\\ 5 &-1 &5 &\ldots &5 &5 &5\\ -1 &5 &5 &\ldots &5 &5 &5 \end{vmatrix}=\begin{vmatrix} 5 &5 &5 &\ldots &5 &5 &-1\\ 0 &0 &0 &\ldots &0 &-6 &6\\ 0 &0 &0 &\ldots &-6 &0 &6\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 0 &0 &-6 &\ldots &0 &0 &6\\\ 0 &-6 &0 &\ldots &0 &0 &6\\ -6 &0 &0 &\ldots &0 &0 &6 \end{vmatrix}$$ Applying Laplace's method to the first column $$D=5\begin{vmatrix} 0 &0  &\ldots &0 &-6 &6\\ 0 &0 &\ldots &-6 &0 &6\\ \vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 0 &-6 &\ldots &0 &0 &6\\ -6 &0 &\ldots &0 &0 &6 \end{vmatrix}+6\begin{vmatrix} 5 &5 &\ldots &5 &5 &-1\\ 0 &0 &\ldots &0 &-6 &6\\ 0 &0 &\ldots &-6 &0 &6\\ \vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 0 &-6 &\ldots &0 &0 &6\\ -6 &0 &\ldots &0 &0 &6 \end{vmatrix}$$ I can see that this one is $D$ but of order $99$...Is this leading anywhere? How would you solve this?,Find the determinant of order $100$: $$D=\begin{vmatrix} 5 &5 &5 &\ldots &5 &5 &-1\\ 5 &5 &5 &\ldots &5 &-1 &5\\ 5 &5 &5 &\ldots &-1 &5 &5\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 5 &5 &-1 &\ldots &5 &5 &5\\ 5 &-1 &5 &\ldots &5 &5 &5\\ -1 &5 &5 &\ldots &5 &5 &5 \end{vmatrix}$$ I think I should be using recurrence relations here but I'm not entirely sure how that method works. I tried this: Multiplying the first row by $(-1)$ and adding it to all rows:  $$D=\begin{vmatrix} 5 &5 &5 &\ldots &5 &5 &-1\\ 5 &5 &5 &\ldots &5 &-1 &5\\ 5 &5 &5 &\ldots &-1 &5 &5\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 5 &5 &-1 &\ldots &5 &5 &5\\ 5 &-1 &5 &\ldots &5 &5 &5\\ -1 &5 &5 &\ldots &5 &5 &5 \end{vmatrix}=\begin{vmatrix} 5 &5 &5 &\ldots &5 &5 &-1\\ 0 &0 &0 &\ldots &0 &-6 &6\\ 0 &0 &0 &\ldots &-6 &0 &6\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 0 &0 &-6 &\ldots &0 &0 &6\\\ 0 &-6 &0 &\ldots &0 &0 &6\\ -6 &0 &0 &\ldots &0 &0 &6 \end{vmatrix}$$ Applying Laplace's method to the first column $$D=5\begin{vmatrix} 0 &0  &\ldots &0 &-6 &6\\ 0 &0 &\ldots &-6 &0 &6\\ \vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 0 &-6 &\ldots &0 &0 &6\\ -6 &0 &\ldots &0 &0 &6 \end{vmatrix}+6\begin{vmatrix} 5 &5 &\ldots &5 &5 &-1\\ 0 &0 &\ldots &0 &-6 &6\\ 0 &0 &\ldots &-6 &0 &6\\ \vdots &\vdots &\ddots &\vdots &\vdots &\vdots\\ 0 &-6 &\ldots &0 &0 &6\\ -6 &0 &\ldots &0 &0 &6 \end{vmatrix}$$ I can see that this one is $D$ but of order $99$...Is this leading anywhere? How would you solve this?,,"['linear-algebra', 'matrices', 'determinant']"
77,Notation for removal of row / column from matrix,Notation for removal of row / column from matrix,,"Is there some common notation for the result of removing the $i$th row, the $j$th column or both of them from a matrix given $A$?","Is there some common notation for the result of removing the $i$th row, the $j$th column or both of them from a matrix given $A$?",,"['linear-algebra', 'matrices', 'notation']"
78,How should we understand change of the basis matrix?,How should we understand change of the basis matrix?,,"I'm rather new to linear algebra and my professor isn't very clear, so although this question may seem ""too easy"" for MSE it would be of great help to me! I'm trying to understand the concept behind a change of basis matrix. Let's say we have some $m \times n$ matrix $A$, formed by a basis $\alpha$. This means that all the columns of $A$ are a linear combination of elements of $\alpha$, right? And if we want a change of basis to $\beta$, we're essentially finding $[A]_\beta$, right? And, the thought behind this is ""reforming"" the columns of $A$ with respect to the basis $\beta$? And, to do this, we take the elements of $\beta$ as columns of a new matrix $C$, then multiply it by the coordinates of $A$ w.r.t. $\beta$? Am I on the right track to understanding this, or am I mistaken?","I'm rather new to linear algebra and my professor isn't very clear, so although this question may seem ""too easy"" for MSE it would be of great help to me! I'm trying to understand the concept behind a change of basis matrix. Let's say we have some $m \times n$ matrix $A$, formed by a basis $\alpha$. This means that all the columns of $A$ are a linear combination of elements of $\alpha$, right? And if we want a change of basis to $\beta$, we're essentially finding $[A]_\beta$, right? And, the thought behind this is ""reforming"" the columns of $A$ with respect to the basis $\beta$? And, to do this, we take the elements of $\beta$ as columns of a new matrix $C$, then multiply it by the coordinates of $A$ w.r.t. $\beta$? Am I on the right track to understanding this, or am I mistaken?",,"['linear-algebra', 'matrices', 'change-of-basis']"
79,How did Frobenius prove the Cayley-Hamilton theorem for a general matrix?,How did Frobenius prove the Cayley-Hamilton theorem for a general matrix?,,"I heard Cayley did the case $n=2$ and then Hamilton generalized the result. How is this done? Assume it works for an $n\times n$ matrix and show $n+1?$ Edit: Sorry, Cayley showed it for $3\times 3$ , I mean Frobenius.","I heard Cayley did the case and then Hamilton generalized the result. How is this done? Assume it works for an matrix and show Edit: Sorry, Cayley showed it for , I mean Frobenius.",n=2 n\times n n+1? 3\times 3,"['linear-algebra', 'matrices', 'math-history', 'cayley-hamilton']"
80,Matrix Multiplication and Rank?,Matrix Multiplication and Rank?,,"I'm having trouble with this question: Is it possible for $A$ and $B$ to be $3\times3$ rank $2$ matrices with $AB = 0$? Prove your       answer. I searched the internet and found this: Rank (A) = Rank (AB) So since Rank (A) = Rank (B) = 2 it follows that this is false since Rank (AB) has to be 2 as well? And a 0 matrix won't have a rank? Somehow I feel this is not the correct explanation, and the answer to this question talks about some column space and null space of the matrix but I can't figure out what it means. Any help is greatly apreciated!","I'm having trouble with this question: Is it possible for $A$ and $B$ to be $3\times3$ rank $2$ matrices with $AB = 0$? Prove your       answer. I searched the internet and found this: Rank (A) = Rank (AB) So since Rank (A) = Rank (B) = 2 it follows that this is false since Rank (AB) has to be 2 as well? And a 0 matrix won't have a rank? Somehow I feel this is not the correct explanation, and the answer to this question talks about some column space and null space of the matrix but I can't figure out what it means. Any help is greatly apreciated!",,"['linear-algebra', 'matrices']"
81,Is every odd order skew-symmetric matrix singular?,Is every odd order skew-symmetric matrix singular?,,We call a square matrix $A$ a skew-symmetric matrix if $A=-A^T$ . A matrix is said to be singular if its determinant is zero. Is every odd order skew-symmetric matrix with complex entries singular?,We call a square matrix a skew-symmetric matrix if . A matrix is said to be singular if its determinant is zero. Is every odd order skew-symmetric matrix with complex entries singular?,A A=-A^T,"['linear-algebra', 'matrices', 'skew-symmetric-matrices']"
82,How does $(Ax -b)^T(Ax -b)$ reduce to $x^TA^TAx − 2b^TAx + b^Tb$?,How does  reduce to ?,(Ax -b)^T(Ax -b) x^TA^TAx − 2b^TAx + b^Tb,"I am not sure if I can write $(Ax -b)^T$ as $(A^Tx^T -b^T)$. If I can, I can't reproduce the above result. Please help. Also refer to any sources that will give me a good insight in matrix derivatives.","I am not sure if I can write $(Ax -b)^T$ as $(A^Tx^T -b^T)$. If I can, I can't reproduce the above result. Please help. Also refer to any sources that will give me a good insight in matrix derivatives.",,"['matrices', 'derivatives', 'partial-derivative']"
83,Matrix to a power,Matrix to a power,,"Let $k$ be fixed natural number and let $$A=\begin{bmatrix}a_1&*&*&...&*\\0&a_2&*&...&*\\0&0&a_3&...&*\\ \vdots & \vdots & \vdots & \ddots & \vdots\\0&0&0&...&a_n\end{bmatrix}$$ where $a_i^k=1$ for every natural $i$. The $a_i$ are pairwise distinct and $*$ denotes a random complex number. My question is: Is it true that $A^k=E$? I've tried for some matrices and its seems to be true, but I have no idea how to start solving this. Thanks for any idea!","Let $k$ be fixed natural number and let $$A=\begin{bmatrix}a_1&*&*&...&*\\0&a_2&*&...&*\\0&0&a_3&...&*\\ \vdots & \vdots & \vdots & \ddots & \vdots\\0&0&0&...&a_n\end{bmatrix}$$ where $a_i^k=1$ for every natural $i$. The $a_i$ are pairwise distinct and $*$ denotes a random complex number. My question is: Is it true that $A^k=E$? I've tried for some matrices and its seems to be true, but I have no idea how to start solving this. Thanks for any idea!",,"['linear-algebra', 'matrices', 'complex-numbers']"
84,Inequality of skew-symmetric matrix,Inequality of skew-symmetric matrix,,Let $A$ be a $n\times n$ skew-symmetric real matrix.Prove that $\forall v \in \mathbb{R^n}　\ \|(E-A)v\| \geq \|v \| $ where $\|v\|$ is the Euclid norm of $v$ and $E$ is the $n \times n$ identity matrix. I have no idea. How to prove this inequality? Many thanks.,Let be a skew-symmetric real matrix.Prove that where is the Euclid norm of and is the identity matrix. I have no idea. How to prove this inequality? Many thanks.,A n\times n \forall v \in \mathbb{R^n}　\ \|(E-A)v\| \geq \|v \|  \|v\| v E n \times n,"['linear-algebra', 'matrices', 'inequality']"
85,Why can't we add a non-square matrix $A$ to its transpose $A^T$?,Why can't we add a non-square matrix  to its transpose ?,A A^T,"The addition operation is commonly defined as follows: Two matrices must have an equal number of rows and columns to be added But this is a very shallow definition/interpretation. A deeper interpretation of a $n \times n$ matrix, I would believe, would be that it is an element of a vector space, e.g. $A \in \mathbb{R}^{n \times n}$ as you need $n^2$ linear equations to compute each entry in the matrix (a dot product for each entry) Now if you use that interpretation of a matrix, then let's assume we have a column vector $B \in \mathbb{R}^{n \times 1}$ and it's transpose, a row vector, $B^T \in \mathbb{R}^{1 \times n}$ $$\begin{align}B = \begin{bmatrix}  b_1 \\ b_2\\ . \\ .\\ .\\ b_n\end{bmatrix} &&\text{and}&&B^T = \begin{bmatrix}  b_1 & b_2& . & .& .& b_n\end{bmatrix} \end{align}$$ But both $B$ and $B^T$, are both elements of the same vector space $B, B^T \in \mathbb{R}^n$. So why is their addition undefined? Why can't you add a matrix to its transpose, just like you could two vectors that are elements of the same vector space? Is it wrong to interpret matrices as I have done? Is there a more rigorous definition of the addition operation for matrices? Are there better ways to interpret matrices?","The addition operation is commonly defined as follows: Two matrices must have an equal number of rows and columns to be added But this is a very shallow definition/interpretation. A deeper interpretation of a $n \times n$ matrix, I would believe, would be that it is an element of a vector space, e.g. $A \in \mathbb{R}^{n \times n}$ as you need $n^2$ linear equations to compute each entry in the matrix (a dot product for each entry) Now if you use that interpretation of a matrix, then let's assume we have a column vector $B \in \mathbb{R}^{n \times 1}$ and it's transpose, a row vector, $B^T \in \mathbb{R}^{1 \times n}$ $$\begin{align}B = \begin{bmatrix}  b_1 \\ b_2\\ . \\ .\\ .\\ b_n\end{bmatrix} &&\text{and}&&B^T = \begin{bmatrix}  b_1 & b_2& . & .& .& b_n\end{bmatrix} \end{align}$$ But both $B$ and $B^T$, are both elements of the same vector space $B, B^T \in \mathbb{R}^n$. So why is their addition undefined? Why can't you add a matrix to its transpose, just like you could two vectors that are elements of the same vector space? Is it wrong to interpret matrices as I have done? Is there a more rigorous definition of the addition operation for matrices? Are there better ways to interpret matrices?",,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
86,$A+tB \ge 0 $ for small $t \Rightarrow A+B \ge 0$?,for small ?,A+tB \ge 0  t \Rightarrow A+B \ge 0,"$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ Let $\psym$ be the set of symmetric positive semidefinite (real) matrices. Let $A \in \psym$ be on the boundary of $\psym$, i.e $\det A=0$ and let $B$ be a symmetric matrix such that $A+tB \in \psym$ for $t >0$ small enough. Is it true that $A+B \in \psym$? Why I think this might be true: The intution is that if we are on the boundary of $\psym$, and we have some small perturbation which keeps us inside the space, than this perturbation must be in the ""inward"" direction from the boundary into the interior of the space. Thus, taking a larger step $(t >>0)$ in this direction will also keep us in the space. Proof for the case when $A,B$ commute: If $A,B$ commute, then they are simultaneously diagonalisable by an orthogonal matrix, i.e there exists $V \in O_n$ such that: $$V^TAV=\Sigma_A,V^TBV=\Sigma_B , $$ so (remember $t>0$) $$A+tB \ge 0 \Rightarrow V^T(A+tB) V=\Sigma_A+t\Sigma_B \ge 0\Rightarrow  \Sigma_A+\Sigma_B \ge 0 \Rightarrow $$ $$ V (\Sigma_A+\Sigma_B) V^T=A+B \ge 0 $$ as required.","$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ Let $\psym$ be the set of symmetric positive semidefinite (real) matrices. Let $A \in \psym$ be on the boundary of $\psym$, i.e $\det A=0$ and let $B$ be a symmetric matrix such that $A+tB \in \psym$ for $t >0$ small enough. Is it true that $A+B \in \psym$? Why I think this might be true: The intution is that if we are on the boundary of $\psym$, and we have some small perturbation which keeps us inside the space, than this perturbation must be in the ""inward"" direction from the boundary into the interior of the space. Thus, taking a larger step $(t >>0)$ in this direction will also keep us in the space. Proof for the case when $A,B$ commute: If $A,B$ commute, then they are simultaneously diagonalisable by an orthogonal matrix, i.e there exists $V \in O_n$ such that: $$V^TAV=\Sigma_A,V^TBV=\Sigma_B , $$ so (remember $t>0$) $$A+tB \ge 0 \Rightarrow V^T(A+tB) V=\Sigma_A+t\Sigma_B \ge 0\Rightarrow  \Sigma_A+\Sigma_B \ge 0 \Rightarrow $$ $$ V (\Sigma_A+\Sigma_B) V^T=A+B \ge 0 $$ as required.",,"['linear-algebra', 'matrices', 'positive-definite']"
87,For which $a$ and $b$ is this matrix diagonalizable?,For which  and  is this matrix diagonalizable?,a b,"For which $a$ and $b$ is this matrix diagonalizable? $$A=\begin{pmatrix} a & 0 & b \\ 0 & b & 0 \\ b & 0 & a  \end{pmatrix}$$ How to get those $a$ and $b$? I calculated eigenvalues and eigenvectors, but don't know what to do next?","For which $a$ and $b$ is this matrix diagonalizable? $$A=\begin{pmatrix} a & 0 & b \\ 0 & b & 0 \\ b & 0 & a  \end{pmatrix}$$ How to get those $a$ and $b$? I calculated eigenvalues and eigenvectors, but don't know what to do next?",,"['linear-algebra', 'matrices']"
88,Minimal polynomial of block matrix,Minimal polynomial of block matrix,,"I'm trying to prove that the minimal polynomial of a diagonal block matrix i.e. a matrix $A = \begin{bmatrix}        B & 0           \\[0.3em]       0 & C       \\[0.3em]      \end{bmatrix}$ is the less common multiple of the minimal polynomials of matrix B and matrix C. Well I checked down the answer that the user @AndreasCaranti gave here: Minpoly and Charpoly of block diagonal matrix However there is a crucial aspect of the proof that I would like to have some clarification please: ""the minimal polynomial $m(x)$ of $A$ vanishes when computed on each block do the minimal polynomial $m_i(x)$ of the $i$-th block divides $m(x)$"". Why? Is this a property of block matrices? Why does the minimal polynomial of each block divides the matrix. Thanks!","I'm trying to prove that the minimal polynomial of a diagonal block matrix i.e. a matrix $A = \begin{bmatrix}        B & 0           \\[0.3em]       0 & C       \\[0.3em]      \end{bmatrix}$ is the less common multiple of the minimal polynomials of matrix B and matrix C. Well I checked down the answer that the user @AndreasCaranti gave here: Minpoly and Charpoly of block diagonal matrix However there is a crucial aspect of the proof that I would like to have some clarification please: ""the minimal polynomial $m(x)$ of $A$ vanishes when computed on each block do the minimal polynomial $m_i(x)$ of the $i$-th block divides $m(x)$"". Why? Is this a property of block matrices? Why does the minimal polynomial of each block divides the matrix. Thanks!",,"['matrices', 'minimal-polynomials', 'block-matrices']"
89,Projection operator,Projection operator,,This may sound stupid question...I know the definition of projection operator(matrix) $P^2=P$ so $P^2-P=0$ which means $P(P-I)=0$ sooo $P=0$ or $P=I$ but this is not always true so what is wrong here ?,This may sound stupid question...I know the definition of projection operator(matrix) $P^2=P$ so $P^2-P=0$ which means $P(P-I)=0$ sooo $P=0$ or $P=I$ but this is not always true so what is wrong here ?,,"['linear-algebra', 'matrices']"
90,determinant of a very large matrix in MATLAB,determinant of a very large matrix in MATLAB,,"I have a very large random matrix which its elements are either $0$ or $1$ randomly. The size of the matrix is $5000$, however when I want to calculate the determinant of the matrix, it is either $Inf$ or $-Inf$. Why it is the case (as I know thw determinant is a real number and for a finite size matrix with finite elements, it cannot be $Inf$) and how can I remedy any possible mistake?","I have a very large random matrix which its elements are either $0$ or $1$ randomly. The size of the matrix is $5000$, however when I want to calculate the determinant of the matrix, it is either $Inf$ or $-Inf$. Why it is the case (as I know thw determinant is a real number and for a finite size matrix with finite elements, it cannot be $Inf$) and how can I remedy any possible mistake?",,"['matrices', 'determinant', 'matlab']"
91,What does $\bigotimes$ and $X^*$ mean?,What does  and  mean?,\bigotimes X^*,"Can someone explain / link me to a linear algebra worked problem where I can see how these work. I've searched and given their statistics and matrix specialty uses, can't find any ready examples.","Can someone explain / link me to a linear algebra worked problem where I can see how these work. I've searched and given their statistics and matrix specialty uses, can't find any ready examples.",,['matrices']
92,Prove that if $A\ge B$ then $\left[ {\begin{array}{*{20}{c}} A & B \\ B & A \\ \end{array}} \right]\ge0$.,Prove that if  then .,A\ge B \left[ {\begin{array}{*{20}{c}} A & B \\ B & A \\ \end{array}} \right]\ge0,"Let $A$ and $B$ be $n \times n$ matrices, i.e., $A, B \in  M_n$. Also, $A \ge 0$, $B \ge 0$, and $A-B \ge 0$ which mean all these matrices are semi-positive-definite. Why does  $\left[ {\begin{array}{*{20}{c}}    A & B  \\    B & A  \\ \end{array}} \right] \ge 0$?","Let $A$ and $B$ be $n \times n$ matrices, i.e., $A, B \in  M_n$. Also, $A \ge 0$, $B \ge 0$, and $A-B \ge 0$ which mean all these matrices are semi-positive-definite. Why does  $\left[ {\begin{array}{*{20}{c}}    A & B  \\    B & A  \\ \end{array}} \right] \ge 0$?",,"['linear-algebra', 'matrices']"
93,Proof that a matrix can be written as the product of a positive definite matrix and an orthogonal matrix,Proof that a matrix can be written as the product of a positive definite matrix and an orthogonal matrix,,How can I show that for a real invertible matrix $A$ with dimensions $n \times  n$ $A$ can be written: $A = B C$ where $B$ is a positive definite matrix and $C$ is an orthogonal matrix.,How can I show that for a real invertible matrix $A$ with dimensions $n \times  n$ $A$ can be written: $A = B C$ where $B$ is a positive definite matrix and $C$ is an orthogonal matrix.,,"['linear-algebra', 'matrices']"
94,Find $A$ ∈ $M_2$ $(\Bbb{R})$ which satisfies $A^4 = I$ but $A^2 ≠ I$,Find  ∈   which satisfies  but,A M_2 (\Bbb{R}) A^4 = I A^2 ≠ I,"Find $A$ ∈ $M_2$ $(\Bbb{R})$ which satisfies $A^4 = I$ but $A^2 ≠ I$. I had a similar problem to this in my homework that was to find a 2x2 matrix such that $A^2 = -I$ without using any zero entries. That one was relatively easy and I came up with the answer $A = \begin{bmatrix}-1 & -2\\1& 1\end{bmatrix}$. However, I am having some trouble with this problem. Any help is appreciated.","Find $A$ ∈ $M_2$ $(\Bbb{R})$ which satisfies $A^4 = I$ but $A^2 ≠ I$. I had a similar problem to this in my homework that was to find a 2x2 matrix such that $A^2 = -I$ without using any zero entries. That one was relatively easy and I came up with the answer $A = \begin{bmatrix}-1 & -2\\1& 1\end{bmatrix}$. However, I am having some trouble with this problem. Any help is appreciated.",,"['linear-algebra', 'matrices']"
95,Characteristic equation of a linear map,Characteristic equation of a linear map,,"Question: Suppose that for a fixed $A \in M_{3}(\mathbb{R})$, i.e. $3\times 3$ matrix over $\mathbb{R}$, we have linear map $T : M_{3}(\mathbb{R}) \to M_{3}(\mathbb{R})$ defined by $T(B) = AB$ for all $B \in M_{3}(\mathbb{R})$. What is the characteristic polynomial of $T$? I'm familiar with characteristic polynomials over a matrix, but the extension to a mapping in this fashion is a little odd. May I be steered in the right direction to solve this?","Question: Suppose that for a fixed $A \in M_{3}(\mathbb{R})$, i.e. $3\times 3$ matrix over $\mathbb{R}$, we have linear map $T : M_{3}(\mathbb{R}) \to M_{3}(\mathbb{R})$ defined by $T(B) = AB$ for all $B \in M_{3}(\mathbb{R})$. What is the characteristic polynomial of $T$? I'm familiar with characteristic polynomials over a matrix, but the extension to a mapping in this fashion is a little odd. May I be steered in the right direction to solve this?",,"['linear-algebra', 'matrices']"
96,Fibonacci and Matrices [duplicate],Fibonacci and Matrices [duplicate],,"This question already has answers here : Proof of this result related to Fibonacci numbers: $\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$? (4 answers) Closed 8 years ago . Consider Matrix $$ A = \begin{pmatrix} 1 & 1\\ 1 & 0 \end{pmatrix} $$ Investigate the sequence of powers of $A$ (i.e. $A^n$ for $n = 1, 2, 3, 4,\ldots$. Verify that $$A^n = \begin{pmatrix}F_{n+1} &F_n \\ F_n & F_{n−1}\end{pmatrix}$$ for $n \geq 20$, where $F_n$ is the $n^{th}$ Fibonacci number. I don't get it, please help. Thank you!","This question already has answers here : Proof of this result related to Fibonacci numbers: $\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$? (4 answers) Closed 8 years ago . Consider Matrix $$ A = \begin{pmatrix} 1 & 1\\ 1 & 0 \end{pmatrix} $$ Investigate the sequence of powers of $A$ (i.e. $A^n$ for $n = 1, 2, 3, 4,\ldots$. Verify that $$A^n = \begin{pmatrix}F_{n+1} &F_n \\ F_n & F_{n−1}\end{pmatrix}$$ for $n \geq 20$, where $F_n$ is the $n^{th}$ Fibonacci number. I don't get it, please help. Thank you!",,"['linear-algebra', 'matrices', 'fibonacci-numbers']"
97,Algorithms for computing matrix logarithm.,Algorithms for computing matrix logarithm.,,"On my quest to find the holy grail of mathematics become a little bit better at algebra, I have read up on matrix logarithms and exponentials and how useful they can be in investigating groups and algebras. I know that power methods are common for trancendental matrix functions. This question is about fast numerical ways to compute the matrix logarithm. Own work: I know of and have implemented the Taylor expansion: $$\log({\bf I + A}) = \sum_{i=1}^N\frac{(-1)^i}{i}{\bf A}^i$$ or equivalently $$\log({\bf A}) = \sum_{i=1}^N\frac{(-1)^i}{i}({\bf A-I})^i$$ in various ways. Having stored $({\bf A-I})^{i-1}$ at iteration $i$ lets us multiply with $\bf (A-I)$ just once every iteration. I have read about some popular speed-up technique using the fact that $$2\log({\bf A}^{1/2}) = \log({\bf A})$$ and that Taylor series is more accurate close to the point of expansion for the lower exponents. However as I don't know any fast matrix-square root, I have not yet managed to employ this fact. Maybe a Taylor expansion of the square root function would do?","On my quest to find the holy grail of mathematics become a little bit better at algebra, I have read up on matrix logarithms and exponentials and how useful they can be in investigating groups and algebras. I know that power methods are common for trancendental matrix functions. This question is about fast numerical ways to compute the matrix logarithm. Own work: I know of and have implemented the Taylor expansion: or equivalently in various ways. Having stored at iteration lets us multiply with just once every iteration. I have read about some popular speed-up technique using the fact that and that Taylor series is more accurate close to the point of expansion for the lower exponents. However as I don't know any fast matrix-square root, I have not yet managed to employ this fact. Maybe a Taylor expansion of the square root function would do?",\log({\bf I + A}) = \sum_{i=1}^N\frac{(-1)^i}{i}{\bf A}^i \log({\bf A}) = \sum_{i=1}^N\frac{(-1)^i}{i}({\bf A-I})^i ({\bf A-I})^{i-1} i \bf (A-I) 2\log({\bf A}^{1/2}) = \log({\bf A}),"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
98,Computing $\operatorname{Tr} \bigl( \bigl( (A+I )^{-1} \bigr)^2\bigr)$,Computing,\operatorname{Tr} \bigl( \bigl( (A+I )^{-1} \bigr)^2\bigr),"Suppose that $A \in \mathbb{R}^{n \times n}$ is a symmetric positive semi-definite matrix  such that $\operatorname{Tr}(A)\le n$. I want a lower bound on the following quantity  $$\operatorname{Tr} \bigl( \bigl( (A+I )^{-1} \bigr)^2\bigr)$$ where $I$ denotes the identity matrix. My intuition tells me it should be $$ \operatorname{Tr} \bigl( \bigl( (A+I )^{-1} \bigr)^2\bigr) \ge 1/4 $$ However, I'm not sure how to show it. Also my intuition might be wrong?","Suppose that $A \in \mathbb{R}^{n \times n}$ is a symmetric positive semi-definite matrix  such that $\operatorname{Tr}(A)\le n$. I want a lower bound on the following quantity  $$\operatorname{Tr} \bigl( \bigl( (A+I )^{-1} \bigr)^2\bigr)$$ where $I$ denotes the identity matrix. My intuition tells me it should be $$ \operatorname{Tr} \bigl( \bigl( (A+I )^{-1} \bigr)^2\bigr) \ge 1/4 $$ However, I'm not sure how to show it. Also my intuition might be wrong?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'trace']"
99,"Does an $n\times n$ matrix $A$ only have an inverse if $rank(A)=n$? If so, why?","Does an  matrix  only have an inverse if ? If so, why?",n\times n A rank(A)=n,"I'm currently learning about the rank and inverses of matrices, and every time I attempt to find the inverse of a matrix with a rank smaller than it's number of rows, I find I am unable. One example of this is as follows: \begin{bmatrix}1&1&0\\1&0&1\\1&0&1\end{bmatrix} The rank of this matrix is 2 while it has 3 rows, and it doesn't have an inverse. Have I just run into examples where this is true, or is this always true? If it is true, why?","I'm currently learning about the rank and inverses of matrices, and every time I attempt to find the inverse of a matrix with a rank smaller than it's number of rows, I find I am unable. One example of this is as follows: \begin{bmatrix}1&1&0\\1&0&1\\1&0&1\end{bmatrix} The rank of this matrix is 2 while it has 3 rows, and it doesn't have an inverse. Have I just run into examples where this is true, or is this always true? If it is true, why?",,"['linear-algebra', 'matrices']"
