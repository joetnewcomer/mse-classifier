,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A limit involving some binomials,A limit involving some binomials,,"Let $C_k^i=\frac{k!}{i!(k-i)!}$. Show that $$\lim_{k\to\infty}\frac{C_k^i+C_k^{n+i}+\cdots+C^{([k/n]-1)n+i}_k}{2^k}=\frac{1}{n}$$ for any $1\leq i<n$. Here $i,n$ be positive integers. As is well-known, $\sum_{i=0}^k C_k^i=2^k$. But how to prove the above limit? Choose $C_k^i$ after $n$ blocks.","Let $C_k^i=\frac{k!}{i!(k-i)!}$. Show that $$\lim_{k\to\infty}\frac{C_k^i+C_k^{n+i}+\cdots+C^{([k/n]-1)n+i}_k}{2^k}=\frac{1}{n}$$ for any $1\leq i<n$. Here $i,n$ be positive integers. As is well-known, $\sum_{i=0}^k C_k^i=2^k$. But how to prove the above limit? Choose $C_k^i$ after $n$ blocks.",,['limits']
1,Problem on limit of sequences,Problem on limit of sequences,,I started solving the question by taking  $b_{n}=\frac{(1^{1^{^{p}}}2^{2^{p}}...(n+1){^{(n+1)^{p}}})^{\frac{1}{(n+1)^{p+1}}}}{  (1^{1^{^{p}}}2^{2^{p}}...(n){^{(n)^{p}}})^{\frac{1}{(n)^{p+1}}}}$  where  $\lim _{n \to \infty }b_{n}=e^{\frac{-2}{(p+1)^{2}}}$ and $lim_{n \to \infty } x_{n}=(1^{1^{^{p}}}2^{2^{p}}...(n){^{(n)^{p}}})^{\frac{1}{(n)^{p+1}}}(b_{n}-1)$ . But I don't know how to solve it further can someone please  help  me with a hint or suggest some other method for solving this question,I started solving the question by taking  $b_{n}=\frac{(1^{1^{^{p}}}2^{2^{p}}...(n+1){^{(n+1)^{p}}})^{\frac{1}{(n+1)^{p+1}}}}{  (1^{1^{^{p}}}2^{2^{p}}...(n){^{(n)^{p}}})^{\frac{1}{(n)^{p+1}}}}$  where  $\lim _{n \to \infty }b_{n}=e^{\frac{-2}{(p+1)^{2}}}$ and $lim_{n \to \infty } x_{n}=(1^{1^{^{p}}}2^{2^{p}}...(n){^{(n)^{p}}})^{\frac{1}{(n)^{p+1}}}(b_{n}-1)$ . But I don't know how to solve it further can someone please  help  me with a hint or suggest some other method for solving this question,,"['real-analysis', 'sequences-and-series', 'limits']"
2,Proof of a sequence $\{S_n\}$ converging to $0$,Proof of a sequence  converging to,\{S_n\} 0,"First, the question asks Prove that $1+\frac12+\frac13+\cdots+\frac1n<2\sqrt n$ At first glance, I see that this is a proof involving induction, and I arrived at an inequality as shown below where I complete my proof: $$\frac1{\sqrt{k+1}-\sqrt k}<2k+2$$ Then, the question asks further Let $S_n=\frac{1}{n}+\frac{1}{2n}+\frac1{3n}+\cdots+\frac1{n^2}$ for each $n\in\Bbb N$. Prove that $\{S_n\}$ converges to $0.$ The second part of the question is where I am stuck. I don't know how I can make use of the first part of the question to answer the latter part.","First, the question asks Prove that $1+\frac12+\frac13+\cdots+\frac1n<2\sqrt n$ At first glance, I see that this is a proof involving induction, and I arrived at an inequality as shown below where I complete my proof: $$\frac1{\sqrt{k+1}-\sqrt k}<2k+2$$ Then, the question asks further Let $S_n=\frac{1}{n}+\frac{1}{2n}+\frac1{3n}+\cdots+\frac1{n^2}$ for each $n\in\Bbb N$. Prove that $\{S_n\}$ converges to $0.$ The second part of the question is where I am stuck. I don't know how I can make use of the first part of the question to answer the latter part.",,"['sequences-and-series', 'limits', 'proof-verification', 'induction']"
3,Finding the infinite limit using definitions,Finding the infinite limit using definitions,,I would really need help proving the following limit using the definitions. My textbook gives me a solution but with very minimal explanation and I'm completely lost. This is the limit I have to find. $$\lim_{x\to1^+} \frac{x}{x-1}$$,I would really need help proving the following limit using the definitions. My textbook gives me a solution but with very minimal explanation and I'm completely lost. This is the limit I have to find. $$\lim_{x\to1^+} \frac{x}{x-1}$$,,"['analysis', 'limits']"
4,Uniform Lipshitz continuity implies Continuous Differentiability,Uniform Lipshitz continuity implies Continuous Differentiability,,"Mallat in his book on wavelets makes the following definition -  A function $f : [a,b] \to \mathbb{R}$ is $(C, \alpha)$-Lipschitz at $v \in [a, b]$ if there is a polynomial $p_v$ of degree at most $\lfloor \alpha \rfloor$ such that $|f(x) - p_v(x)| \leq C|x - v|^\alpha$ for any $x \in [a, b]$. It is uniformly Lipschitz if it is Lipschitz for all $v \in [a ,b]$ with a constant that is independent of $v$.  Continuous differentiability implies Lipschitz continuity as above, since one can use the Taylor polynomial of the function. Question: Apparently, the converse is also true - uniform $(C, \alpha)$-Lipschitz continuity implies that the function is $\lfloor \alpha \rfloor$ times continuously differentiable. I'm having difficulty proving this. It is clear that if $\alpha > 1$ then the function is differentiable. I'm guessing the derivative is uniformly $\alpha-1$ Lipschitz which would complete the proof by induction, but I'm having trouble showing this. Any hints would be appreciated!","Mallat in his book on wavelets makes the following definition -  A function $f : [a,b] \to \mathbb{R}$ is $(C, \alpha)$-Lipschitz at $v \in [a, b]$ if there is a polynomial $p_v$ of degree at most $\lfloor \alpha \rfloor$ such that $|f(x) - p_v(x)| \leq C|x - v|^\alpha$ for any $x \in [a, b]$. It is uniformly Lipschitz if it is Lipschitz for all $v \in [a ,b]$ with a constant that is independent of $v$.  Continuous differentiability implies Lipschitz continuity as above, since one can use the Taylor polynomial of the function. Question: Apparently, the converse is also true - uniform $(C, \alpha)$-Lipschitz continuity implies that the function is $\lfloor \alpha \rfloor$ times continuously differentiable. I'm having difficulty proving this. It is clear that if $\alpha > 1$ then the function is differentiable. I'm guessing the derivative is uniformly $\alpha-1$ Lipschitz which would complete the proof by induction, but I'm having trouble showing this. Any hints would be appreciated!",,"['real-analysis', 'limits', 'derivatives', 'continuity', 'lipschitz-functions']"
5,Limit of measure of sequence of sets,Limit of measure of sequence of sets,,"Recently in my class there was talk of a sequence of sets $A_1 \supseteq A_2 \supseteq \dots $, where each set has infinite measure. Although I found an example that violated the equality (which was the point of the exercise): $$\mu(\bigcap_{n=1}^\infty A_n) = \lim_{n \rightarrow \infty}\mu(A_n),$$ namely, $$ A_n = [n, \infty[, \text{ where } n \in \mathbb{N}^+,$$ I am still a bit puzzled at how I did it. I can see that the measure of the intersection of the sets is  $0$, as can be shown be contradiction (assume it weren't empty, containing a positive real number $r$; but then $r \notin [\lceil{r}\rceil, \infty[$, so it cannot be an intersection of all the sets). Regarding $\lim_{n\rightarrow \infty}\mu(A_n)$, I'm somewhat conflicted: On the one hand, $\forall  \ n \ \mu(A_n) = \infty$. Hence, $\lim_{n \rightarrow \infty} \infty = \infty$ On the other hand, $\lim_{n \rightarrow \infty}A_n = \varnothing$. Hence, $\mu(\lim_{n \rightarrow \infty}A_n) = 0$. So am I right to conclude that in general $\mu(\lim_{n \rightarrow \infty}A_n) \neq \lim_{n\rightarrow \infty}\mu(A_n)$? Why? Thanks for clarifying these issues.","Recently in my class there was talk of a sequence of sets $A_1 \supseteq A_2 \supseteq \dots $, where each set has infinite measure. Although I found an example that violated the equality (which was the point of the exercise): $$\mu(\bigcap_{n=1}^\infty A_n) = \lim_{n \rightarrow \infty}\mu(A_n),$$ namely, $$ A_n = [n, \infty[, \text{ where } n \in \mathbb{N}^+,$$ I am still a bit puzzled at how I did it. I can see that the measure of the intersection of the sets is  $0$, as can be shown be contradiction (assume it weren't empty, containing a positive real number $r$; but then $r \notin [\lceil{r}\rceil, \infty[$, so it cannot be an intersection of all the sets). Regarding $\lim_{n\rightarrow \infty}\mu(A_n)$, I'm somewhat conflicted: On the one hand, $\forall  \ n \ \mu(A_n) = \infty$. Hence, $\lim_{n \rightarrow \infty} \infty = \infty$ On the other hand, $\lim_{n \rightarrow \infty}A_n = \varnothing$. Hence, $\mu(\lim_{n \rightarrow \infty}A_n) = 0$. So am I right to conclude that in general $\mu(\lim_{n \rightarrow \infty}A_n) \neq \lim_{n\rightarrow \infty}\mu(A_n)$? Why? Thanks for clarifying these issues.",,"['real-analysis', 'limits', 'measure-theory']"
6,Find: $\lim\limits_{n\to \infty}\arcsin \frac {1}{\sqrt{n^2+1}}+\arcsin \frac {1}{\sqrt{n^2+2}}+...+\arcsin \frac {1}{\sqrt{n^2+n}}$,Find:,\lim\limits_{n\to \infty}\arcsin \frac {1}{\sqrt{n^2+1}}+\arcsin \frac {1}{\sqrt{n^2+2}}+...+\arcsin \frac {1}{\sqrt{n^2+n}},Let $$x_n=\arcsin \frac {1}{\sqrt{n^2+1}}+\arcsin \frac {1}{\sqrt{n^2+2}}+...+\arcsin \frac {1}{\sqrt{n^2+n}}$$ I would like to find $\lim\limits_{n\to \infty}x_n$ I attempted to use Riemann rectangle formula but it does not work. Any Hint?,Let $$x_n=\arcsin \frac {1}{\sqrt{n^2+1}}+\arcsin \frac {1}{\sqrt{n^2+2}}+...+\arcsin \frac {1}{\sqrt{n^2+n}}$$ I would like to find $\lim\limits_{n\to \infty}x_n$ I attempted to use Riemann rectangle formula but it does not work. Any Hint?,,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'limits']"
7,Computing $\lim_{\varepsilon\to 0^{+}}\psi(\varepsilon)/\Gamma(\varepsilon)$ with asymptotic expansions,Computing  with asymptotic expansions,\lim_{\varepsilon\to 0^{+}}\psi(\varepsilon)/\Gamma(\varepsilon),"I have the following limit of which I want to compute: \begin{equation} \lim_{\varepsilon\to 0^{+}} \frac{\psi(\varepsilon)}{\Gamma(\varepsilon)}. \end{equation} For $\varepsilon\approx 0$ and $\varepsilon\neq 0$ I have the following limiting forms \begin{equation} \tag{1} \psi(\varepsilon)=-\frac{1}{\varepsilon}-\gamma+O(\varepsilon) \end{equation} and \begin{equation} \tag{2} \frac{1}{\Gamma(\varepsilon)}=\varepsilon+O(\varepsilon^{2}). \end{equation} If I multiply $(1)$ and $(2)$ together we get \begin{align} \tag{3} \frac{\psi(\varepsilon)}{\Gamma(\varepsilon)} &= -1-\frac{O(\varepsilon^{2})}{\varepsilon} -\gamma\varepsilon-\gamma O(\varepsilon^{2}) +\varepsilon O(\varepsilon)+O(\varepsilon)O(\varepsilon^{2})\\ &= -1-O(\varepsilon) -\gamma\varepsilon-\gamma O(\varepsilon^{2}) +O(\varepsilon^{2})+O(\varepsilon^{3}). \end{align} In the limit, all of the terms with $\varepsilon$ approach zero such that we arrive at \begin{equation} \lim_{\varepsilon\to0^{+}} \frac{\psi(\varepsilon)}{\Gamma(\varepsilon)} = -1. \end{equation} I have checked this answer against WolframAlpha which yields the same result. Despite getting the same result, I have doubts as to if this is a sound approach to computing the limit. My question is this: Is the use of asymptotic expansions in this manner proper (i.e. is this a valid method to computing my limit)? Or does it just happen to work out in this example?","I have the following limit of which I want to compute: \begin{equation} \lim_{\varepsilon\to 0^{+}} \frac{\psi(\varepsilon)}{\Gamma(\varepsilon)}. \end{equation} For $\varepsilon\approx 0$ and $\varepsilon\neq 0$ I have the following limiting forms \begin{equation} \tag{1} \psi(\varepsilon)=-\frac{1}{\varepsilon}-\gamma+O(\varepsilon) \end{equation} and \begin{equation} \tag{2} \frac{1}{\Gamma(\varepsilon)}=\varepsilon+O(\varepsilon^{2}). \end{equation} If I multiply $(1)$ and $(2)$ together we get \begin{align} \tag{3} \frac{\psi(\varepsilon)}{\Gamma(\varepsilon)} &= -1-\frac{O(\varepsilon^{2})}{\varepsilon} -\gamma\varepsilon-\gamma O(\varepsilon^{2}) +\varepsilon O(\varepsilon)+O(\varepsilon)O(\varepsilon^{2})\\ &= -1-O(\varepsilon) -\gamma\varepsilon-\gamma O(\varepsilon^{2}) +O(\varepsilon^{2})+O(\varepsilon^{3}). \end{align} In the limit, all of the terms with $\varepsilon$ approach zero such that we arrive at \begin{equation} \lim_{\varepsilon\to0^{+}} \frac{\psi(\varepsilon)}{\Gamma(\varepsilon)} = -1. \end{equation} I have checked this answer against WolframAlpha which yields the same result. Despite getting the same result, I have doubts as to if this is a sound approach to computing the limit. My question is this: Is the use of asymptotic expansions in this manner proper (i.e. is this a valid method to computing my limit)? Or does it just happen to work out in this example?",,"['limits', 'asymptotics', 'gamma-function', 'limits-without-lhopital', 'digamma-function']"
8,"If $x_n$ and $y_n$ are bounded, show that $\lim \inf (x_n + y_n) \leq \lim \inf x_n + \lim \sup y_n$","If  and  are bounded, show that",x_n y_n \lim \inf (x_n + y_n) \leq \lim \inf x_n + \lim \sup y_n,"Suppose you have $x_n$ and $y_n$ that are bounded. Show then that: $$\lim \inf x_n + \lim \inf y_n \leq \lim \inf (x_n + y_n) \leq \lim \inf x_n + \lim \sup y_n  $$ So to show this part: $\lim \inf x_n + \lim \inf y_n \leq \lim \inf (x_n + y_n)$. I defined three sets: $$X_n = \{ x_k | k \geq n \} $$ $$Y_n = \{y_k | k \geq n \} $$ and $$XY_n = \{x_k + y_k | k \geq n \} $$ Let $n\in \mathbb{N}$. We then have $$XY_n \subset X_n + Y_n$$, thus $$ \inf X_n + \inf Y_n \leq \inf XY_n$$ thus $$ \lim\inf (X_n + Y_n) \leq  \lim \inf XY_n  \iff \lim \inf X_n + \lim \inf Y_n \leq \lim \inf (X_n + Y_n)$$. Now all is left is to show that $$ \lim \inf (x_n + y_n) \leq \lim \inf x_n + \lim \sup y_n .$$ Any ideas how to proceed?","Suppose you have $x_n$ and $y_n$ that are bounded. Show then that: $$\lim \inf x_n + \lim \inf y_n \leq \lim \inf (x_n + y_n) \leq \lim \inf x_n + \lim \sup y_n  $$ So to show this part: $\lim \inf x_n + \lim \inf y_n \leq \lim \inf (x_n + y_n)$. I defined three sets: $$X_n = \{ x_k | k \geq n \} $$ $$Y_n = \{y_k | k \geq n \} $$ and $$XY_n = \{x_k + y_k | k \geq n \} $$ Let $n\in \mathbb{N}$. We then have $$XY_n \subset X_n + Y_n$$, thus $$ \inf X_n + \inf Y_n \leq \inf XY_n$$ thus $$ \lim\inf (X_n + Y_n) \leq  \lim \inf XY_n  \iff \lim \inf X_n + \lim \inf Y_n \leq \lim \inf (X_n + Y_n)$$. Now all is left is to show that $$ \lim \inf (x_n + y_n) \leq \lim \inf x_n + \lim \sup y_n .$$ Any ideas how to proceed?",,"['real-analysis', 'sequences-and-series', 'limits', 'limsup-and-liminf']"
9,Would this be classified as a corner or a cusp?,Would this be classified as a corner or a cusp?,,"I am teaching about differentiability in an introductory single-variable calculus course.  We went through the usual classification of points at which functions are non-differentiable into corners, cusps, and vertical tangents.  One of my students then asked a question which I'm not sure of the answer to.  Here's my write-up for the community here (using more formal language than I use in my class, of course). We begin by defining our terms: Let $X \subseteq \mathbf R$ and let $f: X \to \mathbf R$.  Let $c \in X$ be a point at which $f$ is continuous.  We say that $f$ is differentiable at $x=c$ if and only if $\lim_{h \to 0}\frac{f(c+h)-f(c)}{h}$ exists. Suppose now that this limit fails to exist, so $f$ is not differentiable at $x=c$.  The limit may fail to exist for several different reasons. If there exist $a, b \in \mathbf R$ such that $\lim_{h \to 0^+}\frac{f(c+h)-f(c)}{h}=a$ and $\lim_{h \to 0^-}\frac{f(c+h)-f(c)}{h}=b$, but $a \neq b$, then we say that $f$ has a corner at $x=c$. If $\lim_{h \to 0^+}\frac{f(c+h)-f(c)}{h}=\pm\infty$ and $\lim_{h \to 0^-}\frac{f(c+h)-f(c)}{h}=\mp\infty$, then we say that $f$ has a cusp at $x=c$. If $\lim_{h \to 0}\frac{f(c+h)-f(c)}{h}=\pm\infty$, then we say that $f$ has a vertical tangent at $x=c$. Here's the question: Suppose our function $f$ is defined by $f(x)=\begin{cases} -\arctan(x) & ,\; x \leq 0 \\ \sqrt{x} & ,\; x \gt 0 \end{cases}$ It is clear that $\lim_{h \to 0^-}\frac{f(0+h)-f(0)}{h}=-1$ and $\lim_{h \to 0^+}\frac{f(0+h)-f(0)}{h}=\infty$.  So how should we classify the failure of $f$ to be differentiable at $x=0$?","I am teaching about differentiability in an introductory single-variable calculus course.  We went through the usual classification of points at which functions are non-differentiable into corners, cusps, and vertical tangents.  One of my students then asked a question which I'm not sure of the answer to.  Here's my write-up for the community here (using more formal language than I use in my class, of course). We begin by defining our terms: Let $X \subseteq \mathbf R$ and let $f: X \to \mathbf R$.  Let $c \in X$ be a point at which $f$ is continuous.  We say that $f$ is differentiable at $x=c$ if and only if $\lim_{h \to 0}\frac{f(c+h)-f(c)}{h}$ exists. Suppose now that this limit fails to exist, so $f$ is not differentiable at $x=c$.  The limit may fail to exist for several different reasons. If there exist $a, b \in \mathbf R$ such that $\lim_{h \to 0^+}\frac{f(c+h)-f(c)}{h}=a$ and $\lim_{h \to 0^-}\frac{f(c+h)-f(c)}{h}=b$, but $a \neq b$, then we say that $f$ has a corner at $x=c$. If $\lim_{h \to 0^+}\frac{f(c+h)-f(c)}{h}=\pm\infty$ and $\lim_{h \to 0^-}\frac{f(c+h)-f(c)}{h}=\mp\infty$, then we say that $f$ has a cusp at $x=c$. If $\lim_{h \to 0}\frac{f(c+h)-f(c)}{h}=\pm\infty$, then we say that $f$ has a vertical tangent at $x=c$. Here's the question: Suppose our function $f$ is defined by $f(x)=\begin{cases} -\arctan(x) & ,\; x \leq 0 \\ \sqrt{x} & ,\; x \gt 0 \end{cases}$ It is clear that $\lim_{h \to 0^-}\frac{f(0+h)-f(0)}{h}=-1$ and $\lim_{h \to 0^+}\frac{f(0+h)-f(0)}{h}=\infty$.  So how should we classify the failure of $f$ to be differentiable at $x=0$?",,"['calculus', 'limits', 'derivatives']"
10,How to prove $\lim_{x \to 2} x^2 = 4$ and make sure it's correct?,How to prove  and make sure it's correct?,\lim_{x \to 2} x^2 = 4,"I would like to specify that english is not my primary language so I had to do some translation here, I do really hope every mathematical term I used is correct, if not please forgive me :) Let's say I want to prove that $\lim_{x \to 2} x^2 = 4$, from what I've read, in order to do that, the solution to $|f(x)-l|<\epsilon$ must contain a neighborhood of $x_{0}$ such as $x_{0}-\delta<x<x_{0}+\delta$ which proves that $|x-x_{0}|<\delta$ So in this case $|x^2-4|<\epsilon$ must contain a neighborhood of $2$, so I proceed by solving that: $-\epsilon<x^2-4<\epsilon$ $4-\epsilon<x^2<4+\epsilon$ $\sqrt{4-\epsilon}<x<\sqrt{4+\epsilon}$ and this is as far as my book goes The limit is proven true because $\sqrt{4-\epsilon}<x<\sqrt{4+\epsilon}$ is a neighborhood of $x_{0}$ which is 2, so when $x\to2$ then $y\to4$ but what I don't understand is: how do we know that $\sqrt{4-\epsilon}<x<\sqrt{4+\epsilon}$ is really a neighborhood of $2$? That doesn't seem obvious to me. Let's say that instead of a ""true"" limit I wanted to prove (Or prove wrong) a ""false"" limit such as $\lim_{x \to 2} x^2 = 8$ which is not true. How do I do that? I can do the usual steps: $-\epsilon<x^2-8<\epsilon$ $8-\epsilon<x^2<8+\epsilon$ $\sqrt{8-\epsilon}<x<\sqrt{8+\epsilon}$ but how do I check if this is really a neighborhood of 2 so that $|x-x_{0}|<\delta$ or not? How do I know if this limit is really proven or not? PS: This is what I've learned reading my book, if anything I said is wrong, please let me know! And if you know a faster/better/simpler method of proving a limit please tell me!","I would like to specify that english is not my primary language so I had to do some translation here, I do really hope every mathematical term I used is correct, if not please forgive me :) Let's say I want to prove that $\lim_{x \to 2} x^2 = 4$, from what I've read, in order to do that, the solution to $|f(x)-l|<\epsilon$ must contain a neighborhood of $x_{0}$ such as $x_{0}-\delta<x<x_{0}+\delta$ which proves that $|x-x_{0}|<\delta$ So in this case $|x^2-4|<\epsilon$ must contain a neighborhood of $2$, so I proceed by solving that: $-\epsilon<x^2-4<\epsilon$ $4-\epsilon<x^2<4+\epsilon$ $\sqrt{4-\epsilon}<x<\sqrt{4+\epsilon}$ and this is as far as my book goes The limit is proven true because $\sqrt{4-\epsilon}<x<\sqrt{4+\epsilon}$ is a neighborhood of $x_{0}$ which is 2, so when $x\to2$ then $y\to4$ but what I don't understand is: how do we know that $\sqrt{4-\epsilon}<x<\sqrt{4+\epsilon}$ is really a neighborhood of $2$? That doesn't seem obvious to me. Let's say that instead of a ""true"" limit I wanted to prove (Or prove wrong) a ""false"" limit such as $\lim_{x \to 2} x^2 = 8$ which is not true. How do I do that? I can do the usual steps: $-\epsilon<x^2-8<\epsilon$ $8-\epsilon<x^2<8+\epsilon$ $\sqrt{8-\epsilon}<x<\sqrt{8+\epsilon}$ but how do I check if this is really a neighborhood of 2 so that $|x-x_{0}|<\delta$ or not? How do I know if this limit is really proven or not? PS: This is what I've learned reading my book, if anything I said is wrong, please let me know! And if you know a faster/better/simpler method of proving a limit please tell me!",,"['calculus', 'algebra-precalculus', 'functional-analysis', 'limits', 'functions']"
11,Convergence Proof: Difference within Function of Epsilon,Convergence Proof: Difference within Function of Epsilon,,"Here's the problem, and then my proposed proof: Suppose that $\phi : (0, \infty) \mapsto (0, \infty)$ is a function such that $\lim_{\epsilon \to 0}\phi(\epsilon) = 0.$ Let $a_1,a_2,\dots$ be a sequence in $\mathbb{R}^n$. Show that this sequence converges to $a$ if and only if for any $\epsilon > 0$, there exists $N$ such that for $n > N$, we have $|a_n-a|\leq \phi(\epsilon).$ (from Vector Calculus, Linear Algebra, and Differential Forms (Hubbard and Hubbard)) My proof is for the first statement (the if, not the only if). First, to avoid confusion, the argument of the function $\phi$ will be renamed to $\epsilon'$.By the definition of the limit, the statement that $\lim_{\epsilon \to 0}\phi(\epsilon) = 0$ is the same as saying that $$\forall\epsilon >0, \exists \delta > 0 \text{ such that } |\epsilon'| \leq \delta \implies |\phi(\epsilon')| \leq \epsilon $$ By assumption, $|a_n - a| \leq \phi(\epsilon')$, and from the statement above, we get $$|a_n - a| \leq \phi(\epsilon') \leq \epsilon$$ We want $\phi(\epsilon') \leq \epsilon'$. However, since it must be true for all $\epsilon$ and all $\epsilon'$ greater than 0, the two can be taken to be equal. Therefore, $$|a_n-a| \leq \epsilon'$$ I'm unsure about the final step, where I say that the unprimed and primed epsilon can be taken to be equal. Is this correct?","Here's the problem, and then my proposed proof: Suppose that $\phi : (0, \infty) \mapsto (0, \infty)$ is a function such that $\lim_{\epsilon \to 0}\phi(\epsilon) = 0.$ Let $a_1,a_2,\dots$ be a sequence in $\mathbb{R}^n$. Show that this sequence converges to $a$ if and only if for any $\epsilon > 0$, there exists $N$ such that for $n > N$, we have $|a_n-a|\leq \phi(\epsilon).$ (from Vector Calculus, Linear Algebra, and Differential Forms (Hubbard and Hubbard)) My proof is for the first statement (the if, not the only if). First, to avoid confusion, the argument of the function $\phi$ will be renamed to $\epsilon'$.By the definition of the limit, the statement that $\lim_{\epsilon \to 0}\phi(\epsilon) = 0$ is the same as saying that $$\forall\epsilon >0, \exists \delta > 0 \text{ such that } |\epsilon'| \leq \delta \implies |\phi(\epsilon')| \leq \epsilon $$ By assumption, $|a_n - a| \leq \phi(\epsilon')$, and from the statement above, we get $$|a_n - a| \leq \phi(\epsilon') \leq \epsilon$$ We want $\phi(\epsilon') \leq \epsilon'$. However, since it must be true for all $\epsilon$ and all $\epsilon'$ greater than 0, the two can be taken to be equal. Therefore, $$|a_n-a| \leq \epsilon'$$ I'm unsure about the final step, where I say that the unprimed and primed epsilon can be taken to be equal. Is this correct?",,"['limits', 'proof-verification', 'convergence-divergence', 'epsilon-delta']"
12,"Proof about sequences in metric spaces, Hilbert's cube","Proof about sequences in metric spaces, Hilbert's cube",,"I am having trouble convincing myself that the following proof is correct. The question is from the exercies in a real analysis book and the section is on sequences in metric spaces. Background: Let $(H^\infty,d)$ be a metric space such that $H^\infty \subset \mathbb{R}^\infty$ and if $\{a_n\}_{n=1}^{\infty} \in H^\infty$, then $|a_n| \leq 1$ for all $n \in \mathbb{N}$. In other words, $H^\infty$ is ""Hilbert's cube."" The metric $d$ in this case is defined by $d(a,b)=\sum_{n=1}^{\infty}\frac{|a_n-b_n|}{2^n}$. Problem: Prove that if $\{a^k\}_{k=0}^\infty$ is a sequence of points in $H^\infty$ which converges to $a=\{a_n\}_{n=1}^{\infty} \in H^\infty$, then $\lim_{k \to \infty} a_j^k = a_j$ for all $j \in \mathbb{N}$. Proof: Let $j\in \mathbb{N}$ and $\epsilon > 0$. Since $\lim_{k \to \infty} \{a^k\}=a,$ there exists $N \in \mathbb{N}$ such that $k \geq N$ implies that $$d(a^k,a) = \sum_{n=1}^{\infty}\frac{|a^k_n-a_n|}{2^n} < \frac{\epsilon}{2^j}.$$ We also have $$\frac{|a^k_j-a_j|}{2^j} < \sum_{n=1}^{\infty}\frac{|a^k_n-a_n|}{2^n} < \frac{\epsilon}{2^j},$$ which implies that $|a^k_j-a_j| < \epsilon$, so $\lim_{k \to \infty} a_j^k = a_j$ for all $j \in \mathbb{N}$. My confusion: I don't feel good about the fact that $N$ depends on $j$... but maybe this is OK. How does the proof look?","I am having trouble convincing myself that the following proof is correct. The question is from the exercies in a real analysis book and the section is on sequences in metric spaces. Background: Let $(H^\infty,d)$ be a metric space such that $H^\infty \subset \mathbb{R}^\infty$ and if $\{a_n\}_{n=1}^{\infty} \in H^\infty$, then $|a_n| \leq 1$ for all $n \in \mathbb{N}$. In other words, $H^\infty$ is ""Hilbert's cube."" The metric $d$ in this case is defined by $d(a,b)=\sum_{n=1}^{\infty}\frac{|a_n-b_n|}{2^n}$. Problem: Prove that if $\{a^k\}_{k=0}^\infty$ is a sequence of points in $H^\infty$ which converges to $a=\{a_n\}_{n=1}^{\infty} \in H^\infty$, then $\lim_{k \to \infty} a_j^k = a_j$ for all $j \in \mathbb{N}$. Proof: Let $j\in \mathbb{N}$ and $\epsilon > 0$. Since $\lim_{k \to \infty} \{a^k\}=a,$ there exists $N \in \mathbb{N}$ such that $k \geq N$ implies that $$d(a^k,a) = \sum_{n=1}^{\infty}\frac{|a^k_n-a_n|}{2^n} < \frac{\epsilon}{2^j}.$$ We also have $$\frac{|a^k_j-a_j|}{2^j} < \sum_{n=1}^{\infty}\frac{|a^k_n-a_n|}{2^n} < \frac{\epsilon}{2^j},$$ which implies that $|a^k_j-a_j| < \epsilon$, so $\lim_{k \to \infty} a_j^k = a_j$ for all $j \in \mathbb{N}$. My confusion: I don't feel good about the fact that $N$ depends on $j$... but maybe this is OK. How does the proof look?",,"['real-analysis', 'sequences-and-series', 'limits', 'metric-spaces']"
13,Proof verification : $f$ is continuous at $c$ $\Leftrightarrow$ $f$ has zero-jump at $c$ for increasing $f$,Proof verification :  is continuous at    has zero-jump at  for increasing,f c \Leftrightarrow f c f,"The Problem : Let $J \subseteq \mathbb{R}$ be an interval and $f : J \to \mathbb{R}$ be increasing. Define the jump at the point $c$ as follows: $j_f(c)=\lim_{x \to c+}f(x)-\lim_{x \to c-}f(x)$ if $c \in J$ is not an end-point of $J.$ $j_f(c)=\lim_{x \to c+}f(x)-f(c)$ if $c \in J$ is the left end-point of $J.$ $j_f(c)=f(c)-\lim_{x \to c-}f(x)$ if $c \in J$ is the right end-point of $J.$ To show that $f$ is continuous at $c \in J$ iff $j_f(c)=0.$ I'm attaching my solution (A few pieces of argument is marked as informal as they are similar to the previous line of argument and hence not elaborated). Please notify if there's any gap/flaws in the arguments, also whether it could be made shorter by any other technique. Any comment/suggestion regarding this proof, or maybe in general context (style of proof-writing etc), would be greatly appreciated. Thank you. My Solution : $(\Rightarrow)$ part : Suppose $c$ is not an end-point of $J$. Given $f$ is continuous at $c.$ Fix $\epsilon > 0.$ Then $\exists \delta > 0$ such that $x \in (c-\delta,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon.$ Hence $$x \in (c-\delta,c) \cap J \Rightarrow |f(x)-f(c)| < \epsilon \tag 1$$ $$x \in (c,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon \tag 2$$ Since $\epsilon > 0$ is arbitrary, we conclude from $(1)$ that $\lim_{x \to c-}f(x)=f(c)$ and from $(2)$ that $\lim_{x \to c+}f(x)=f(c).$ Hence $j_f(c)=\lim_{x \to c+}f(x)-\lim_{x \to c-}f(x)=f(c)-f(c)=0.$ Now suppose $c$ is the left end-point of $J$. Given $f$ is continuous at $c.$ Then $\exists \delta > 0$ such that $x \in (c-\delta,c+\delta) \cap J = [c,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon.$ So, $x \in (c,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon.$ Thus $\lim_{x \to c+}f(x)=f(c),$ and hence $j_f(c)=0.$ Informal : Similarly, if $c$ is the right-end point of $J,$ we obtain $j_f(c)=0.$ Note : We did not need the fact that $f$ is an increasing function to establish this side of the equivalence. $(\Leftarrow)$ part : Suppose $c$ is not an end-point of $J.$ Now $j_f(c)=0 \Leftrightarrow \lim_{x \to c-}f(x)=\lim_{x \to c+}f(x)=l$ $($let$).$ Thus $\exists \delta_1>0$ and $\delta_2>0$ such that $$x \in (c-\delta_1, c) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon$$ $$x \in (c, c+\delta_2) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon$$ Let $\delta=min\{\delta_1, \delta_2\}>0$. Then $$x \in (c-\delta, c+\delta) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon \tag 3$$ Claim : $l=f(c).$ We prove the claim by contradiction. First we assume $l<f(c).$ Choose $\epsilon = \frac{f(c)-l}{2}>0.$ Then $\exists \delta_3>0$ such that $x \in (c-\delta_3,c+\delta_3) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon,$ in particular $f(c+\frac{\delta_3}{2})<l+\epsilon=\frac{l+f(c)}{2}<f(c),$ which is a contradiction since $f$ is increasing. Informal : If otherwise, let $l>f(c),$ we similarly choose $\epsilon=\frac{l-f(c)}{2}>0$ and a corresponding $\delta_4>0.$ Check that $f(c-\frac{\delta_4}{2})>f(c),$ contradicting the fact that $f$ is increasing. Hence we conclude (by trichotomy law of real numbers) that $l=f(c).$ Now, putting $l=f(c)$ in $(3)$ and noting that $x=c \Rightarrow |f(x)-f(c)|=0<\epsilon,$ we obtain $$x \in (c-\delta, c+\delta) \cap J \Rightarrow |f(x)-f(c)|<\epsilon \tag 4$$ Since $\epsilon>0$ is arbitrary, we conclude that $f$ is continuous at $c.$ Now suppose $c$ is the left end-point of $J.$ Then $j_f(c)=0 \Leftrightarrow \lim_{x \to c+}f(x)=f(c),$ i.e. given $\epsilon>0, \exists \delta > 0,$ such that $x \in (c, c+\delta) \cap J \Rightarrow |f(x)-f(c)|<\epsilon.$ At $x=c,$ obviously $|f(x)-f(c)|=0<\epsilon.$ Thus $x \in (c-\delta, c+\delta) \cap J \Rightarrow |f(x)-f(c)|<\epsilon,$ and hence $f$ is continuous at $c.$ Informal : Similarly if $c$ is the right end-point of $J,$ we have $j_f(c)=0 \Rightarrow$ $f$ is continuous at $c.$ This completes the proof. $\square$","The Problem : Let $J \subseteq \mathbb{R}$ be an interval and $f : J \to \mathbb{R}$ be increasing. Define the jump at the point $c$ as follows: $j_f(c)=\lim_{x \to c+}f(x)-\lim_{x \to c-}f(x)$ if $c \in J$ is not an end-point of $J.$ $j_f(c)=\lim_{x \to c+}f(x)-f(c)$ if $c \in J$ is the left end-point of $J.$ $j_f(c)=f(c)-\lim_{x \to c-}f(x)$ if $c \in J$ is the right end-point of $J.$ To show that $f$ is continuous at $c \in J$ iff $j_f(c)=0.$ I'm attaching my solution (A few pieces of argument is marked as informal as they are similar to the previous line of argument and hence not elaborated). Please notify if there's any gap/flaws in the arguments, also whether it could be made shorter by any other technique. Any comment/suggestion regarding this proof, or maybe in general context (style of proof-writing etc), would be greatly appreciated. Thank you. My Solution : $(\Rightarrow)$ part : Suppose $c$ is not an end-point of $J$. Given $f$ is continuous at $c.$ Fix $\epsilon > 0.$ Then $\exists \delta > 0$ such that $x \in (c-\delta,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon.$ Hence $$x \in (c-\delta,c) \cap J \Rightarrow |f(x)-f(c)| < \epsilon \tag 1$$ $$x \in (c,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon \tag 2$$ Since $\epsilon > 0$ is arbitrary, we conclude from $(1)$ that $\lim_{x \to c-}f(x)=f(c)$ and from $(2)$ that $\lim_{x \to c+}f(x)=f(c).$ Hence $j_f(c)=\lim_{x \to c+}f(x)-\lim_{x \to c-}f(x)=f(c)-f(c)=0.$ Now suppose $c$ is the left end-point of $J$. Given $f$ is continuous at $c.$ Then $\exists \delta > 0$ such that $x \in (c-\delta,c+\delta) \cap J = [c,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon.$ So, $x \in (c,c+\delta) \cap J \Rightarrow |f(x)-f(c)| < \epsilon.$ Thus $\lim_{x \to c+}f(x)=f(c),$ and hence $j_f(c)=0.$ Informal : Similarly, if $c$ is the right-end point of $J,$ we obtain $j_f(c)=0.$ Note : We did not need the fact that $f$ is an increasing function to establish this side of the equivalence. $(\Leftarrow)$ part : Suppose $c$ is not an end-point of $J.$ Now $j_f(c)=0 \Leftrightarrow \lim_{x \to c-}f(x)=\lim_{x \to c+}f(x)=l$ $($let$).$ Thus $\exists \delta_1>0$ and $\delta_2>0$ such that $$x \in (c-\delta_1, c) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon$$ $$x \in (c, c+\delta_2) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon$$ Let $\delta=min\{\delta_1, \delta_2\}>0$. Then $$x \in (c-\delta, c+\delta) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon \tag 3$$ Claim : $l=f(c).$ We prove the claim by contradiction. First we assume $l<f(c).$ Choose $\epsilon = \frac{f(c)-l}{2}>0.$ Then $\exists \delta_3>0$ such that $x \in (c-\delta_3,c+\delta_3) \cap J \setminus \{c\} \Rightarrow |f(x)-l|<\epsilon,$ in particular $f(c+\frac{\delta_3}{2})<l+\epsilon=\frac{l+f(c)}{2}<f(c),$ which is a contradiction since $f$ is increasing. Informal : If otherwise, let $l>f(c),$ we similarly choose $\epsilon=\frac{l-f(c)}{2}>0$ and a corresponding $\delta_4>0.$ Check that $f(c-\frac{\delta_4}{2})>f(c),$ contradicting the fact that $f$ is increasing. Hence we conclude (by trichotomy law of real numbers) that $l=f(c).$ Now, putting $l=f(c)$ in $(3)$ and noting that $x=c \Rightarrow |f(x)-f(c)|=0<\epsilon,$ we obtain $$x \in (c-\delta, c+\delta) \cap J \Rightarrow |f(x)-f(c)|<\epsilon \tag 4$$ Since $\epsilon>0$ is arbitrary, we conclude that $f$ is continuous at $c.$ Now suppose $c$ is the left end-point of $J.$ Then $j_f(c)=0 \Leftrightarrow \lim_{x \to c+}f(x)=f(c),$ i.e. given $\epsilon>0, \exists \delta > 0,$ such that $x \in (c, c+\delta) \cap J \Rightarrow |f(x)-f(c)|<\epsilon.$ At $x=c,$ obviously $|f(x)-f(c)|=0<\epsilon.$ Thus $x \in (c-\delta, c+\delta) \cap J \Rightarrow |f(x)-f(c)|<\epsilon,$ and hence $f$ is continuous at $c.$ Informal : Similarly if $c$ is the right end-point of $J,$ we have $j_f(c)=0 \Rightarrow$ $f$ is continuous at $c.$ This completes the proof. $\square$",,"['real-analysis', 'limits']"
14,Easy way to use monotone convergence theorem on $a_{n+1} = (1-\frac{1}{k})\cdot a_n + \frac{r}{k} \cdot {a_{n}}^{-k+1}$,Easy way to use monotone convergence theorem on,a_{n+1} = (1-\frac{1}{k})\cdot a_n + \frac{r}{k} \cdot {a_{n}}^{-k+1},"I'm looking for an easy way to use the monotone convergence theorem on $(a_n)_{n \in \mathbb{N} \cup \{0\}}$ with $ k ~ \in \left [ ~ 2, \infty \right ) \cap \mathbb{N} \\ r ~ \in \left [ ~ 0, \infty \right ) \\ a_0 ~ \in \left ( ~ 0, \infty \right ) \\ ~ \\ a_{n+1} = (1-\frac{1}{k})\cdot a_n + \frac{r}{k} \cdot {a_{n}}^{-k+1}$ It's bounded below by $0$ because $(1-\frac{1}{k}) \geq 0$, $a_0 \geq 0$, $\frac{r}{k} \geq 0$ and ${a_{0}}^{-k+1} \geq 0$. Therefore $\forall n \in \mathbb{N} \cup \{0\} ~ : ~ a_n \geq 0$. (Easy to prove by induction.) However, I'm still looking for an easy way to show that $(a_n)$ is increasing/decreasing. (The series converges to $r^{\frac{1}{k}}$ by the way...)","I'm looking for an easy way to use the monotone convergence theorem on $(a_n)_{n \in \mathbb{N} \cup \{0\}}$ with $ k ~ \in \left [ ~ 2, \infty \right ) \cap \mathbb{N} \\ r ~ \in \left [ ~ 0, \infty \right ) \\ a_0 ~ \in \left ( ~ 0, \infty \right ) \\ ~ \\ a_{n+1} = (1-\frac{1}{k})\cdot a_n + \frac{r}{k} \cdot {a_{n}}^{-k+1}$ It's bounded below by $0$ because $(1-\frac{1}{k}) \geq 0$, $a_0 \geq 0$, $\frac{r}{k} \geq 0$ and ${a_{0}}^{-k+1} \geq 0$. Therefore $\forall n \in \mathbb{N} \cup \{0\} ~ : ~ a_n \geq 0$. (Easy to prove by induction.) However, I'm still looking for an easy way to show that $(a_n)$ is increasing/decreasing. (The series converges to $r^{\frac{1}{k}}$ by the way...)",,"['sequences-and-series', 'limits', 'convergence-divergence', 'recursion']"
15,Integration involving natural logarithm,Integration involving natural logarithm,,"I tried possible way of getting the integral of the following integrand: $$\int_0^{\infty} \frac{\ln\left(x+\frac{1}{x}\right)}{1+x^2}~dx$$ Firstly, I tried to write $\ln\left(x+\frac{1}{x}\right)$ as $\ln(x^2+1)-\ln(x)$ and tried to apply biparts but couldn't take it to its destination solution. Secondly, I tried to substitute $t=x+\frac{1}{x}$ and proceeded, but couldn't solve further. Nothing seems to work. This was a question from my exam. I don't understand how I'm supposed to solve this question in a short time when I can't solve it at home. I'm guessing there has to be a short but smart solution that is pretty much not striking me.","I tried possible way of getting the integral of the following integrand: $$\int_0^{\infty} \frac{\ln\left(x+\frac{1}{x}\right)}{1+x^2}~dx$$ Firstly, I tried to write $\ln\left(x+\frac{1}{x}\right)$ as $\ln(x^2+1)-\ln(x)$ and tried to apply biparts but couldn't take it to its destination solution. Secondly, I tried to substitute $t=x+\frac{1}{x}$ and proceeded, but couldn't solve further. Nothing seems to work. This was a question from my exam. I don't understand how I'm supposed to solve this question in a short time when I can't solve it at home. I'm guessing there has to be a short but smart solution that is pretty much not striking me.",,"['limits', 'definite-integrals', 'substitution']"
16,Rigorous proof of $f'(c)=0$ where $c$ is a global maximum,Rigorous proof of  where  is a global maximum,f'(c)=0 c,"My question is with regard to whether my following 'proof' is wrong or if it is an acceptable way to show that at a maximum $c$, $f'(c)=0$. A function  $f : [a,b]$ $ \to\Bbb R$ is differentiable at $c\in (a,b)$ and $f$ achieves a global maximum at $c$. Prove $f'(c)=0$. My Proof: Since $f(c)$ is a global maximum then $f(c)\ge f(x) \forall x \in (a,b)$. Using the definition of a derivative $f'(c)=\lim \limits_{x\to c}\frac {f(x)-f(c)}{x-c}$ we see that $f(x)-f(c)$ will always be negative. If we approach the limit from the left then $\lim \limits_{x\to c^-}f'(c)>0$, and if we approach the limit from the right then $\lim \limits_{x\to c^+}f'(c)<0$. But we know that $f'(c)$ exists so $f'(c)=0$.","My question is with regard to whether my following 'proof' is wrong or if it is an acceptable way to show that at a maximum $c$, $f'(c)=0$. A function  $f : [a,b]$ $ \to\Bbb R$ is differentiable at $c\in (a,b)$ and $f$ achieves a global maximum at $c$. Prove $f'(c)=0$. My Proof: Since $f(c)$ is a global maximum then $f(c)\ge f(x) \forall x \in (a,b)$. Using the definition of a derivative $f'(c)=\lim \limits_{x\to c}\frac {f(x)-f(c)}{x-c}$ we see that $f(x)-f(c)$ will always be negative. If we approach the limit from the left then $\lim \limits_{x\to c^-}f'(c)>0$, and if we approach the limit from the right then $\lim \limits_{x\to c^+}f'(c)<0$. But we know that $f'(c)$ exists so $f'(c)=0$.",,"['real-analysis', 'limits', 'derivatives']"
17,Is $n\log(n)+m\log(m)=O(n\log(m)+m\log(n))$? And what about $n\log(m)+m\log(n)=O(n\log(n)+m\log(m))$?,Is ? And what about ?,n\log(n)+m\log(m)=O(n\log(m)+m\log(n)) n\log(m)+m\log(n)=O(n\log(n)+m\log(m)),"I am trying to decide if each of these two statements are true or false: $n\log(n)+m\log(m)=O(n\log(m)+m\log(n))$ $n\log(m)+m\log(n)=O(n\log(n)+m\log(m))$ I've tried using some properties of logarithms so as to take these two statements to these inequalities: For the first one, the statement is true if and only if there exist $c$ and $(n_0,m_0)$ such that for all $(n,m)>(n_0,m_0)$ (which means $n>n_0$ and $m>m_0$) $$n^n* m^m \leq c(m^n* n^m)$$ $${n}^{n-m}\leq cm^{n-m} $$ If I take $m=m_0+1$ and $n=(n_0+1)(m_0+1)c$ then $(n,m)>(n_0,m_0)$ but $$n^{n-m} > cm^{n-m}$$ From here it follows that the first statement is false. As for the second one, the statement is true if and only if there exist $c, (n_0,m_0)$ such that for all $(n,m)>(n_0,m_0)$ $$m^n*n^m \leq c(n^n*m^m)$$ $$n^{m-n} \leq cm^{m-n}$$ I got stuck at this part, I would appreciate some help to prove or disprove the second statement and also to know if I've done the first part correctly. Thanks in advance.","I am trying to decide if each of these two statements are true or false: $n\log(n)+m\log(m)=O(n\log(m)+m\log(n))$ $n\log(m)+m\log(n)=O(n\log(n)+m\log(m))$ I've tried using some properties of logarithms so as to take these two statements to these inequalities: For the first one, the statement is true if and only if there exist $c$ and $(n_0,m_0)$ such that for all $(n,m)>(n_0,m_0)$ (which means $n>n_0$ and $m>m_0$) $$n^n* m^m \leq c(m^n* n^m)$$ $${n}^{n-m}\leq cm^{n-m} $$ If I take $m=m_0+1$ and $n=(n_0+1)(m_0+1)c$ then $(n,m)>(n_0,m_0)$ but $$n^{n-m} > cm^{n-m}$$ From here it follows that the first statement is false. As for the second one, the statement is true if and only if there exist $c, (n_0,m_0)$ such that for all $(n,m)>(n_0,m_0)$ $$m^n*n^m \leq c(n^n*m^m)$$ $$n^{m-n} \leq cm^{m-n}$$ I got stuck at this part, I would appreciate some help to prove or disprove the second statement and also to know if I've done the first part correctly. Thanks in advance.",,"['limits', 'asymptotics']"
18,Second order non linear recurrence relation,Second order non linear recurrence relation,,"$$  G_n= G_{n-1}+ (\frac{3}{4})^n G_{n-2}, \quad with \quad  G_0=1,G_1=1 $$ I am trying to find the limit $ \lim\limits_{n \rightarrow +\infty} G_n$ which converges to $ 5.457177946$. Or to find an upper bound of $g_n$ that still converges to a constant. Fibonacci sequence does not converges to a constant although it is an upper bound. I do not need an exact solution, I just a tip or a method name that I could use. I tried to find the general form of the coefficients, but it seems hard to find.","$$  G_n= G_{n-1}+ (\frac{3}{4})^n G_{n-2}, \quad with \quad  G_0=1,G_1=1 $$ I am trying to find the limit $ \lim\limits_{n \rightarrow +\infty} G_n$ which converges to $ 5.457177946$. Or to find an upper bound of $g_n$ that still converges to a constant. Fibonacci sequence does not converges to a constant although it is an upper bound. I do not need an exact solution, I just a tip or a method name that I could use. I tried to find the general form of the coefficients, but it seems hard to find.",,"['limits', 'recurrence-relations', 'fibonacci-numbers']"
19,Unable to solve this limit,Unable to solve this limit,,"$$ \underset{x\to 0}{\lim} \left(\frac{2+\cos x}{x^3\sin x}-\frac{3}{x^4}\right)$$ What I've tried so far: $2+\cos x=1+(1+\cos x)$ $=1+\sin^2\dfrac{x}{2}$ But, I do not think this step is fruitful as I am getting stuck thereafter. Kindly provide some sort of help or hint. Thanks in advance!","$$ \underset{x\to 0}{\lim} \left(\frac{2+\cos x}{x^3\sin x}-\frac{3}{x^4}\right)$$ What I've tried so far: $2+\cos x=1+(1+\cos x)$ $=1+\sin^2\dfrac{x}{2}$ But, I do not think this step is fruitful as I am getting stuck thereafter. Kindly provide some sort of help or hint. Thanks in advance!",,"['calculus', 'limits', 'limits-without-lhopital']"
20,Generalization of a common die-rolling game; looking for a limit,Generalization of a common die-rolling game; looking for a limit,,"A common interview question (which appears at least once on this board) is the following: You are playing a game in which a single fair 6-sided die is rolled repeatedly, up to a maximum of N rolls. You can stop the process any time you wish, at which point your score is the last value rolled. What is your expected score as a function of N, assuming optimal play? The answer is found recursively: For N=1 the answer is 3.5. For larger N the answer for N-1 determines whether to stop after the first roll, from which we can calculate the answer for N. Now consider just that first roll. Obviously if the first roll is a 6, you stop. Suppose the first roll is 5. Do you stop? The answer depends on N. Clearly for N=2 you stop if 5 is the first roll, and just as clearly you do not stop on an initial 5 if N is very large, because you're so likely to get a 6 eventually. How large must N be so that you will stop on the first roll only if that roll is 6? The answer is 6. That is, if the first roll is 5 you should stop if N<=5 and continue if N>=6. Now generalize to a k-sided die: Let g(k) be the smallest N such that the optimal strategy for a k-sided die is to stop after the first roll only when that roll is k. We've seen that g(6)=6. Is this a coincidence? In fact it is: g(k)=k up to k=9, but g(10) = 11. So consider the ratio g(k)/k. As k increases, this ratio increases (though not monotonically). Numeric calculation suggests that it approaches a limit, and that limit might be e/2. Can we prove this limit, or some other limit, or that the limit doesn't exist?","A common interview question (which appears at least once on this board) is the following: You are playing a game in which a single fair 6-sided die is rolled repeatedly, up to a maximum of N rolls. You can stop the process any time you wish, at which point your score is the last value rolled. What is your expected score as a function of N, assuming optimal play? The answer is found recursively: For N=1 the answer is 3.5. For larger N the answer for N-1 determines whether to stop after the first roll, from which we can calculate the answer for N. Now consider just that first roll. Obviously if the first roll is a 6, you stop. Suppose the first roll is 5. Do you stop? The answer depends on N. Clearly for N=2 you stop if 5 is the first roll, and just as clearly you do not stop on an initial 5 if N is very large, because you're so likely to get a 6 eventually. How large must N be so that you will stop on the first roll only if that roll is 6? The answer is 6. That is, if the first roll is 5 you should stop if N<=5 and continue if N>=6. Now generalize to a k-sided die: Let g(k) be the smallest N such that the optimal strategy for a k-sided die is to stop after the first roll only when that roll is k. We've seen that g(6)=6. Is this a coincidence? In fact it is: g(k)=k up to k=9, but g(10) = 11. So consider the ratio g(k)/k. As k increases, this ratio increases (though not monotonically). Numeric calculation suggests that it approaches a limit, and that limit might be e/2. Can we prove this limit, or some other limit, or that the limit doesn't exist?",,"['probability', 'limits', 'dice']"
21,Is this an alright proof of an $e$ limit?,Is this an alright proof of an  limit?,e,"My goal is to prove that $$f(a) = \lim_{w \to 0} (1+aw)^{\frac{1}{w}} = e^a$$ without being too rigorous (just rigorous enough to convince myself that it really is true). Is the following method alright; or does it have flawed logic? $$\frac{d}{da}f(a) = \frac{d}{da} (\lim_{w \to 0} (1+aw)^{\frac{1}{w}}) = \lim_{w \to 0} (\frac{d}{da} (1+aw)^{\frac{1}{w}})$$ $$= \lim_{w \to 0} (1+aw)^{\frac{1}{w} - 1}$$ $$ = \lim_{w \to 0} \frac{(1+aw)^{\frac{1}{w}}}{(1+aw)}$$ $$ = \frac{\lim_{w \to 0}(1+aw)^{\frac{1}{w}}}{\lim_{w \to 0}(1+aw)}$$ $$ = \lim_{w \to 0} (1+aw)^{\frac{1}{w}} = f(a)$$ Hence $\frac{d}{da}f(a) = f(a)$. Apart from the zero function, the only function that satisfies this property is $e^x$. Can we then conclude that $f(a) = e^x$?","My goal is to prove that $$f(a) = \lim_{w \to 0} (1+aw)^{\frac{1}{w}} = e^a$$ without being too rigorous (just rigorous enough to convince myself that it really is true). Is the following method alright; or does it have flawed logic? $$\frac{d}{da}f(a) = \frac{d}{da} (\lim_{w \to 0} (1+aw)^{\frac{1}{w}}) = \lim_{w \to 0} (\frac{d}{da} (1+aw)^{\frac{1}{w}})$$ $$= \lim_{w \to 0} (1+aw)^{\frac{1}{w} - 1}$$ $$ = \lim_{w \to 0} \frac{(1+aw)^{\frac{1}{w}}}{(1+aw)}$$ $$ = \frac{\lim_{w \to 0}(1+aw)^{\frac{1}{w}}}{\lim_{w \to 0}(1+aw)}$$ $$ = \lim_{w \to 0} (1+aw)^{\frac{1}{w}} = f(a)$$ Hence $\frac{d}{da}f(a) = f(a)$. Apart from the zero function, the only function that satisfies this property is $e^x$. Can we then conclude that $f(a) = e^x$?",,['limits']
22,Proof of the sequential criterion for limits,Proof of the sequential criterion for limits,,"Let $f:D\to\mathbb R$ and let $c$ be an accumulation point of $D$. Then $(i)\lim_{x\to c}f(x)=L$ iff $(ii)$ for every sequence $(s_n)$ in $D$ that converges to $c$ with $s_n\neq c$ the sequence $(f(s_n))$ converges to $L$ I'm okay with one direction. To prove the other direction (taking the contrapositive statement): Suppose $L$ is not a limit of $f$ at $c$. Find a sequence $s_n$ in $D$ such that $s_n$ converges to $c$ but $(f(s_n))$ does not converge to $L$ Since $L$ is not a limit of $f$ at $c$, $\exists\epsilon>0$ such that $\forall\delta>0$ $\exists x\in D$ such that $0<|x-c|<\delta$ implies $|f(x)-L|\ge\epsilon$. Now the book I'm reading, Steven Lay's ""Analysis with an introduction to proof"" goes on as follows: "" In particular, for each $n\in\mathbb N$, there exists $s_n\in D$ with $0<|s_n-c|<1/n$ such that $|f(s_n)-L|\ge\epsilon$"" Thus exhibiting $(s_n)$ as the required sequence. I'm not sure why is it required that $\delta$ must be related to $1/n$ . . . ok, I want to show that there exists a sequence $s_n$ that converges to $c$ such that $(f(s_n))$ does not converge to $L$ Let $s_n$ coverge to $c$. Then $\forall \delta>0 \exists N\in \mathbb N$ such that $n\ge N \to |s_n-c|<\delta$ Now I want to make this statement into: $\forall \delta>0 \exists s_n \in D$ such that $|s_n-c|<\delta$ please detail how that happens.","Let $f:D\to\mathbb R$ and let $c$ be an accumulation point of $D$. Then $(i)\lim_{x\to c}f(x)=L$ iff $(ii)$ for every sequence $(s_n)$ in $D$ that converges to $c$ with $s_n\neq c$ the sequence $(f(s_n))$ converges to $L$ I'm okay with one direction. To prove the other direction (taking the contrapositive statement): Suppose $L$ is not a limit of $f$ at $c$. Find a sequence $s_n$ in $D$ such that $s_n$ converges to $c$ but $(f(s_n))$ does not converge to $L$ Since $L$ is not a limit of $f$ at $c$, $\exists\epsilon>0$ such that $\forall\delta>0$ $\exists x\in D$ such that $0<|x-c|<\delta$ implies $|f(x)-L|\ge\epsilon$. Now the book I'm reading, Steven Lay's ""Analysis with an introduction to proof"" goes on as follows: "" In particular, for each $n\in\mathbb N$, there exists $s_n\in D$ with $0<|s_n-c|<1/n$ such that $|f(s_n)-L|\ge\epsilon$"" Thus exhibiting $(s_n)$ as the required sequence. I'm not sure why is it required that $\delta$ must be related to $1/n$ . . . ok, I want to show that there exists a sequence $s_n$ that converges to $c$ such that $(f(s_n))$ does not converge to $L$ Let $s_n$ coverge to $c$. Then $\forall \delta>0 \exists N\in \mathbb N$ such that $n\ge N \to |s_n-c|<\delta$ Now I want to make this statement into: $\forall \delta>0 \exists s_n \in D$ such that $|s_n-c|<\delta$ please detail how that happens.",,"['real-analysis', 'limits']"
23,Question about Cesro summation,Question about Cesro summation,,"Consider $$S_n = \sum_{i=0}^n a_i$$ and its Cesro sums, defined as $$ C = \lim_{n \to \infty} \frac1n\sum_{k=0}^n S_k$$ Is it always true that $$ C = \lim_{n \to \infty} \frac1{L(n)}\sum_{k= n - L(n)}^n S_k$$ where $L(n)$ is any strictly increasing function such that $ 2 < L(n) < \ln(n)$ for every $n$?","Consider $$S_n = \sum_{i=0}^n a_i$$ and its Cesro sums, defined as $$ C = \lim_{n \to \infty} \frac1n\sum_{k=0}^n S_k$$ Is it always true that $$ C = \lim_{n \to \infty} \frac1{L(n)}\sum_{k= n - L(n)}^n S_k$$ where $L(n)$ is any strictly increasing function such that $ 2 < L(n) < \ln(n)$ for every $n$?",,"['real-analysis', 'limits', 'summation', 'cesaro-summable']"
24,Show that the sequence $H_n - \lfloor H_n \rfloor$ is not convergent,Show that the sequence  is not convergent,H_n - \lfloor H_n \rfloor,Let $x_n = 1 + \frac{1}{2} + \dots + \frac{1}{n} - \left\lfloor 1 + \frac{1}{2} +\dots+\frac{1}{n}\right\rfloor  \    \forall   n \in \mathbb{N} $ be a sequence . Prove that it is not convergent? $\lfloor x\rfloor$ means floor. I have absolutely no ideea how to prove that is not convergent ??,Let $x_n = 1 + \frac{1}{2} + \dots + \frac{1}{n} - \left\lfloor 1 + \frac{1}{2} +\dots+\frac{1}{n}\right\rfloor  \    \forall   n \in \mathbb{N} $ be a sequence . Prove that it is not convergent? $\lfloor x\rfloor$ means floor. I have absolutely no ideea how to prove that is not convergent ??,,"['sequences-and-series', 'limits', 'convergence-divergence', 'harmonic-analysis', 'fractional-part']"
25,find Asymptotes of $f(x) = \arcsin(\frac{2x}{1+x^2})$,find Asymptotes of,f(x) = \arcsin(\frac{2x}{1+x^2}),"I'm trying to find the asymptotes of $f(x) = \arcsin(\frac{2x}{1+x^2})$. I've found that this function has no vertical asymptote, since $f$ is bounded between $[-\pi/2 , \pi/2 ]$, and since $\arcsin x$ is continuous where it is defined - for every $x_0 \in R$, $\lim_{x\to x0^+}|f(x)| = |f(x_0)| \neq \infty $. Hopefully this once is correct, please correct me if it isn't. I think I'm wrong in the calculation of the horizontal asymptotes : if $y=ax+b$ is a horizontal asymptote at $\infty$, then $a = \lim_{x\to\infty}\frac{f(x)}{x} = 0$. Now, $b= \lim_{x\to\infty}(f(x)-ax) = \lim_{x\to\infty}f(x) = 0$ So I'm getting that this function has no vertical asymptotes, which I guess is correct, but I also get $y=0$ as a horizontal asymptotes which I'm pretty sure is wrong.. Where is my mistake?","I'm trying to find the asymptotes of $f(x) = \arcsin(\frac{2x}{1+x^2})$. I've found that this function has no vertical asymptote, since $f$ is bounded between $[-\pi/2 , \pi/2 ]$, and since $\arcsin x$ is continuous where it is defined - for every $x_0 \in R$, $\lim_{x\to x0^+}|f(x)| = |f(x_0)| \neq \infty $. Hopefully this once is correct, please correct me if it isn't. I think I'm wrong in the calculation of the horizontal asymptotes : if $y=ax+b$ is a horizontal asymptote at $\infty$, then $a = \lim_{x\to\infty}\frac{f(x)}{x} = 0$. Now, $b= \lim_{x\to\infty}(f(x)-ax) = \lim_{x\to\infty}f(x) = 0$ So I'm getting that this function has no vertical asymptotes, which I guess is correct, but I also get $y=0$ as a horizontal asymptotes which I'm pretty sure is wrong.. Where is my mistake?",,['limits']
26,Question about the inequality in the proof of $\lim_{x\to 4}{\sqrt{x}}=2$ by $\epsilon$ - $\delta$ method?,Question about the inequality in the proof of  by  -  method?,\lim_{x\to 4}{\sqrt{x}}=2 \epsilon \delta,"What I want to proof is $\lim_{x\to 4}{\sqrt{x}}=2$ The following is from my textbook Finding a $\delta$. Let $\epsilon\gt 0$. We seek a $\delta\gt 0$, s.t. $if\quad 0\lt|x-4|\lt\delta,\quad then\quad|\sqrt{x}-2|\lt\epsilon$ To be able to form $\sqrt{x}$, we need to have $x\ge 0$. To ensure this, we must have $\delta\le 4$. How to derive/translate this statement in inequality?","What I want to proof is $\lim_{x\to 4}{\sqrt{x}}=2$ The following is from my textbook Finding a $\delta$. Let $\epsilon\gt 0$. We seek a $\delta\gt 0$, s.t. $if\quad 0\lt|x-4|\lt\delta,\quad then\quad|\sqrt{x}-2|\lt\epsilon$ To be able to form $\sqrt{x}$, we need to have $x\ge 0$. To ensure this, we must have $\delta\le 4$. How to derive/translate this statement in inequality?",,"['calculus', 'limits', 'inequality', 'proof-explanation', 'epsilon-delta']"
27,$\lim_{n\to\infty}n\sin(2^n \pi \sqrt{e}\mathrm n!)=?$ [closed],[closed],\lim_{n\to\infty}n\sin(2^n \pi \sqrt{e}\mathrm n!)=?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question We know that $$\lim_{n\to\infty}n\sin(2\pi \mathrm en!)=2\pi$$ now : $$\lim_{n\to\infty}n\sin(2^n  \pi \sqrt{e}\mathrm n!)=?$$ I tried:,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question We know that now : I tried:,\lim_{n\to\infty}n\sin(2\pi \mathrm en!)=2\pi \lim_{n\to\infty}n\sin(2^n  \pi \sqrt{e}\mathrm n!)=?,['limits']
28,Limit of an Increasing Sequence of Sets,Limit of an Increasing Sequence of Sets,,"Avner Friendman's book on Analysis has the following theorem Theorem 1.2.1: Let $\mu$ be a measure with domain $\mathcal{A}$. Then if $\{E_n\}$ is a monotone-increasing sequence of sets of $\mathcal{A}$, then    \begin{equation} \lim_n \mu(E_n)=\mu \left(\lim_n E_n\right) \end{equation}   Proof: Note that $\lim_n E_n$ belongs to $\mathcal{A}$. Near, warring $E_0=\varnothing$ we have    \begin{align*}\mu(\lim_n E_n)&=\mu \left[\bigcup_n (E_n-E_{n-1})\right]=\sum_{n=1}^\infty \mu(E_n-E_{n-1})=\lim_m \sum_{n=1}^m \mu(E_n-E_{n-1})\\ &=\lim_m\mu\left[\bigcup_{n=1}^m(E_n-E_{n-1})\right]=\lim_m\mu(E_m) \end{align*} My question is: I know that the limit of an increasing sequence of sets is \begin{equation}\lim_n E_n=\bigcup_n E_n \end{equation} And I don't see any problem with replacing $E_n-E_{n-1}$ throughout the entire proof with just $E_n$. So why do they go through out the entire proof using $E_n-E_{n-1}$? Does something go wrong if we don't do this? I'm just not seeing it. I'm also wondering why we set $E_0=\varnothing$. Is this just to ensure that we don't start out with an infinitely large set as our first set? I know these questions seem a bit trivial, but I'm really trying to understand the intricacies of all of the proofs in this book and these little details go a long way. Thanks!","Avner Friendman's book on Analysis has the following theorem Theorem 1.2.1: Let $\mu$ be a measure with domain $\mathcal{A}$. Then if $\{E_n\}$ is a monotone-increasing sequence of sets of $\mathcal{A}$, then    \begin{equation} \lim_n \mu(E_n)=\mu \left(\lim_n E_n\right) \end{equation}   Proof: Note that $\lim_n E_n$ belongs to $\mathcal{A}$. Near, warring $E_0=\varnothing$ we have    \begin{align*}\mu(\lim_n E_n)&=\mu \left[\bigcup_n (E_n-E_{n-1})\right]=\sum_{n=1}^\infty \mu(E_n-E_{n-1})=\lim_m \sum_{n=1}^m \mu(E_n-E_{n-1})\\ &=\lim_m\mu\left[\bigcup_{n=1}^m(E_n-E_{n-1})\right]=\lim_m\mu(E_m) \end{align*} My question is: I know that the limit of an increasing sequence of sets is \begin{equation}\lim_n E_n=\bigcup_n E_n \end{equation} And I don't see any problem with replacing $E_n-E_{n-1}$ throughout the entire proof with just $E_n$. So why do they go through out the entire proof using $E_n-E_{n-1}$? Does something go wrong if we don't do this? I'm just not seeing it. I'm also wondering why we set $E_0=\varnothing$. Is this just to ensure that we don't start out with an infinitely large set as our first set? I know these questions seem a bit trivial, but I'm really trying to understand the intricacies of all of the proofs in this book and these little details go a long way. Thanks!",,"['real-analysis', 'limits', 'measure-theory']"
29,Is the partial derivative of a sum always the sum of partial derivatives,Is the partial derivative of a sum always the sum of partial derivatives,,"What I mean is, if we want to find the partial deriative $\frac{\partial^2}{\partial x \partial y} (f(x,y)+g(x,y))$ Then is it always true that: $\frac{\partial^2}{\partial x \partial y} (f(x,y)+g(x,y))=\frac{\partial^2}{\partial x \partial y} f(x,y) + \frac{\partial^2}{\partial x \partial y} g(x,y)$ and more over can this be extended to any finite sum?","What I mean is, if we want to find the partial deriative $\frac{\partial^2}{\partial x \partial y} (f(x,y)+g(x,y))$ Then is it always true that: $\frac{\partial^2}{\partial x \partial y} (f(x,y)+g(x,y))=\frac{\partial^2}{\partial x \partial y} f(x,y) + \frac{\partial^2}{\partial x \partial y} g(x,y)$ and more over can this be extended to any finite sum?",,"['calculus', 'limits', 'derivatives', 'partial-derivative']"
30,"Prove that if lim f(x) = L1 and lim g(x) = L2, then lim (f(x))^(g(x)) = L1^L2","Prove that if lim f(x) = L1 and lim g(x) = L2, then lim (f(x))^(g(x)) = L1^L2",,"I am trying to prove that if $$ \lim_{x \to c} (f(x)) = L_1 \\ \lim_{x \to c} (g(x)) = L_2 \\ L_1, L_2 \geq 0 $$ Then $$ \lim_{x \to c} f(x)^{g(x)} = (L_1)^{L_2} $$ I am doing this for fun, and my prof said that it shouldn't be too hard, but all I got so far is $$ \forall \epsilon >0 \ \exists \delta > 0 : \text{if}\ \ |x-c|<\delta\ \ \ \text{then}\ |P(x)-L|<\epsilon \\ |f(x)^{g(x)} - (L_1)^{L_2}| < \epsilon $$ I have no idea how to proceed. Can someone help me out? I started by defining h(x) as $$(f(x))^{(g(x))}$$ but I couldn't go anywhere with that without basically defining the limit of h(x) as x approaches c to be L1^L2","I am trying to prove that if $$ \lim_{x \to c} (f(x)) = L_1 \\ \lim_{x \to c} (g(x)) = L_2 \\ L_1, L_2 \geq 0 $$ Then $$ \lim_{x \to c} f(x)^{g(x)} = (L_1)^{L_2} $$ I am doing this for fun, and my prof said that it shouldn't be too hard, but all I got so far is $$ \forall \epsilon >0 \ \exists \delta > 0 : \text{if}\ \ |x-c|<\delta\ \ \ \text{then}\ |P(x)-L|<\epsilon \\ |f(x)^{g(x)} - (L_1)^{L_2}| < \epsilon $$ I have no idea how to proceed. Can someone help me out? I started by defining h(x) as $$(f(x))^{(g(x))}$$ but I couldn't go anywhere with that without basically defining the limit of h(x) as x approaches c to be L1^L2",,"['limits', 'proof-writing', 'exponential-function']"
31,Proving a sequence converges $\epsilon-N$,Proving a sequence converges,\epsilon-N,"It seems my biggest nightmare has come to haunt me again! I first came across the formal proof of a limit $\epsilon-\delta$ in Calculus 1, I never truly mastered it since at the time it was just racking my brain. I have now begun self-teaching a course in sequences & series and it has already come up. It has also been emphasised to me that being able to understand and do this is fundamental for success in this course. I'm almost 100% certain that I'd have no trouble applying the methods, I've seen in tutorials to prove a sequence converges. However, I don't understand how the $\epsilon-N$ proof actually proves a sequence converges; if that makes much sense. Here are a few questions, I'd like to ask: What does this statement mean: $|a_n - L| < \epsilon $ L refers to the limit. Why does this proof actually prove a sequence converges? Why does $\epsilon>0$? Those are the only questions that currently come to mind, also if anyone can provide any additional advice on how I can wrap my head around these proofs please feel free to post!","It seems my biggest nightmare has come to haunt me again! I first came across the formal proof of a limit $\epsilon-\delta$ in Calculus 1, I never truly mastered it since at the time it was just racking my brain. I have now begun self-teaching a course in sequences & series and it has already come up. It has also been emphasised to me that being able to understand and do this is fundamental for success in this course. I'm almost 100% certain that I'd have no trouble applying the methods, I've seen in tutorials to prove a sequence converges. However, I don't understand how the $\epsilon-N$ proof actually proves a sequence converges; if that makes much sense. Here are a few questions, I'd like to ask: What does this statement mean: $|a_n - L| < \epsilon $ L refers to the limit. Why does this proof actually prove a sequence converges? Why does $\epsilon>0$? Those are the only questions that currently come to mind, also if anyone can provide any additional advice on how I can wrap my head around these proofs please feel free to post!",,"['sequences-and-series', 'limits', 'convergence-divergence', 'epsilon-delta']"
32,Proving limit using precise limit definition,Proving limit using precise limit definition,,"I would like to prove that $$ \lim_{x \rightarrow 2}\frac{1}{x^2}=\frac{1}{4} $$ using the epsilon-delta definition of a limit. I start with the formal definition: For every $\varepsilon$ > 0 there exist a $\delta$ > 0 such that if $|x - a| < \delta$, then $|f(x) - a|$ < $\varepsilon$. So I need to prove that if $|x - 2| < \delta$ then $|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon$ . I start by doing scratch-work to turn the left hand side of $|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon$ into $|x-2|$ $$ |\frac{1}{x^2} - \frac{1}{4}| < \varepsilon $$ $$ |\frac{4-x^2}{4x^2}| < \varepsilon $$ $$ |4-x^2| < 4x^2\varepsilon $$ $$ |(x+2)(x-2)| < 4x^2\varepsilon $$ $$ |x-2| < \frac{4x^2}{|x+2|}\varepsilon $$ I would be able to complete the proof if the right-hand side were simply $\frac{\varepsilon}{|x+2|}$, but the $4x^2$ is throwing me off. Normally I would pick: $\delta \leq 1$ then use $|x-2|<\delta=1$, or $-1<x-2<1$ to turn $x-2$ into the denominator with $\varepsilon$, then pick the lowest bound of the inequality, giving me $\delta = min:\{1, \frac{\varepsilon}{<lowest bound>}\}$ I don't know how to handle the extra $4x^2$ however. Any help would be appreciated.","I would like to prove that $$ \lim_{x \rightarrow 2}\frac{1}{x^2}=\frac{1}{4} $$ using the epsilon-delta definition of a limit. I start with the formal definition: For every $\varepsilon$ > 0 there exist a $\delta$ > 0 such that if $|x - a| < \delta$, then $|f(x) - a|$ < $\varepsilon$. So I need to prove that if $|x - 2| < \delta$ then $|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon$ . I start by doing scratch-work to turn the left hand side of $|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon$ into $|x-2|$ $$ |\frac{1}{x^2} - \frac{1}{4}| < \varepsilon $$ $$ |\frac{4-x^2}{4x^2}| < \varepsilon $$ $$ |4-x^2| < 4x^2\varepsilon $$ $$ |(x+2)(x-2)| < 4x^2\varepsilon $$ $$ |x-2| < \frac{4x^2}{|x+2|}\varepsilon $$ I would be able to complete the proof if the right-hand side were simply $\frac{\varepsilon}{|x+2|}$, but the $4x^2$ is throwing me off. Normally I would pick: $\delta \leq 1$ then use $|x-2|<\delta=1$, or $-1<x-2<1$ to turn $x-2$ into the denominator with $\varepsilon$, then pick the lowest bound of the inequality, giving me $\delta = min:\{1, \frac{\varepsilon}{<lowest bound>}\}$ I don't know how to handle the extra $4x^2$ however. Any help would be appreciated.",,"['calculus', 'limits', 'proof-writing', 'epsilon-delta']"
33,What is the exact value of the series $\sqrt{1+2\times\sqrt{1+3\times\sqrt{1+4\times\sqrt\cdot.....}}}$ [duplicate],What is the exact value of the series  [duplicate],\sqrt{1+2\times\sqrt{1+3\times\sqrt{1+4\times\sqrt\cdot.....}}},"This question already has answers here : Evaluating the nested radical $ \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots}}} $. [closed] (3 answers) Closed 7 years ago . Kindly tell me what is the value of: $\sqrt{1+2\times\sqrt{1+3\times\sqrt{1+4\times\sqrt\cdot.....}}}$ According to ramanujan ,it is equal to 3 I want to know how...","This question already has answers here : Evaluating the nested radical $ \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots}}} $. [closed] (3 answers) Closed 7 years ago . Kindly tell me what is the value of: $\sqrt{1+2\times\sqrt{1+3\times\sqrt{1+4\times\sqrt\cdot.....}}}$ According to ramanujan ,it is equal to 3 I want to know how...",,"['calculus', 'limits', 'nested-radicals']"
34,Find $\lim \limits_{n \to \infty}\left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n}$,Find,\lim \limits_{n \to \infty}\left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n},"I am looking for $$\lim \limits_{n \to \infty}\left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n}$$ We notice $\ln{2n+1 \over n} = \ln\left({1 + {n+1 \over n}}\right)$. We also know that ${x \over 1 + x} \le \ln(1+x)$. From this, we get $${n+1 \over 2n+1} \le \ln\left({1 + {n+1 \over n}}\right)$$ Then we notice $n \le 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n}$, so $$n{n+1 \over 2n+1} \le \left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n}$$ $n{n+1 \over 2n+1} \to + \infty$, so $$\lim \limits_{n \to \infty}\left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n} = + \infty$$ Is this reasoning correct?","I am looking for $$\lim \limits_{n \to \infty}\left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n}$$ We notice $\ln{2n+1 \over n} = \ln\left({1 + {n+1 \over n}}\right)$. We also know that ${x \over 1 + x} \le \ln(1+x)$. From this, we get $${n+1 \over 2n+1} \le \ln\left({1 + {n+1 \over n}}\right)$$ Then we notice $n \le 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n}$, so $$n{n+1 \over 2n+1} \le \left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n}$$ $n{n+1 \over 2n+1} \to + \infty$, so $$\lim \limits_{n \to \infty}\left( 1 + \sqrt{2} + \sqrt[3]{3} + \dots \sqrt[n]{n} \right) \ln{2n+1 \over n} = + \infty$$ Is this reasoning correct?",,"['real-analysis', 'limits']"
35,A Machinist's Imperfect Disk,A Machinist's Imperfect Disk,,"Exercise A machinist is required to manufacture a circular metal disk with area $1000$ cm $^2$ . What radius produces such a disk? If the machinist is allowed an error tolerance of $\pm 5$ cm $^2$ in the area of the disk, how close to the ideal radius in part (1) must the machinist control the radius? In terms of the $\epsilon$ , $\delta$ definition of $\lim \limits_{x \to a}{f(x)} = L$ what is $x$ ? What is $f(x)$ ? What value of $\epsilon$ is given? What is the corresponding value of $\delta$ ? Solution Note: I've decided to solve part (3) before part (2), as it helps set the stage of the solution of (2). 1. What radius produces such a disk? $\pi r_0^2 = 1000 \implies r_0 = \sqrt{\frac{1000}{\pi}} \approx 17.8412$ 3. In terms of the $\epsilon$ , $\delta$ definition of $\lim \limits_{x \to a}{f(x)} = L$ what is $x$ ? What is $f(x)$ ? What value of $\epsilon$ is given? What is the corresponding value of $\delta$ ? $a = r_0 \approx 17.8412$ $L = \pi r_0^2 = 1000$ $x = r$ $f(x) = \pi r^2$ $|r - r_0| < \delta$ $|\pi r^2 - \pi r_0^2| < \epsilon$ 2. If the machinist is allowed an error tolerance of $\pm 5$ cm $^2$ in the area of the disk, how close to the ideal radius in part (1) must the machinist control the radius? $|\pi r^2 - \pi r_0^2| < \epsilon$ $\implies |\pi r^2 - 1000| < 5$ $\implies -5 < \pi r^2 - 1000 < 5$ $\implies 995 < \pi r^2 < 1005$ $\implies \frac{995}{\pi} < r^2 < \frac{1005}{\pi}$ $\implies \sqrt{\frac{995}{\pi}} < r < \sqrt{\frac{1005}{\pi}}$ $\implies 17.7966 < r < 17.8858$ $|r - r_0| < \delta \implies |r - 17.8412| < \delta$ $|17.7966 - 17.8412| < \delta \implies 0.0446 < \delta$ $|17.8858 - 17.8412| < \delta \implies 0.0446 < \delta$ $\delta = \min(0.0446, 0.0446) = 0.0446$ Answer 1. What radius produces such a disk? $$r_0 = 17.8412 \text{ cm}$$ 2. If the machinist is allowed an error tolerance of $\pm 5$ cm $^2$ in the area of the disk, how close to the ideal radius in part (1) must the machinist control the radius? $$\delta = 0.0446 \text{ cm}$$ 3. In terms of the $\epsilon$ , $\delta$ definition of $\lim \limits_{x \to a}{f(x)} = L$ what is $x$ ? What is $f(x)$ ? What value of $\epsilon$ is given? What is the corresponding value of $\delta$ ? $$|r - r_0| < \delta$$ $$|\pi r^2 - \pi r_0^2| < \epsilon$$ Request Is my answer correct? If not, in what part of my solution did I make a mistake?","Exercise A machinist is required to manufacture a circular metal disk with area cm . What radius produces such a disk? If the machinist is allowed an error tolerance of cm in the area of the disk, how close to the ideal radius in part (1) must the machinist control the radius? In terms of the , definition of what is ? What is ? What value of is given? What is the corresponding value of ? Solution Note: I've decided to solve part (3) before part (2), as it helps set the stage of the solution of (2). 1. What radius produces such a disk? 3. In terms of the , definition of what is ? What is ? What value of is given? What is the corresponding value of ? 2. If the machinist is allowed an error tolerance of cm in the area of the disk, how close to the ideal radius in part (1) must the machinist control the radius? Answer 1. What radius produces such a disk? 2. If the machinist is allowed an error tolerance of cm in the area of the disk, how close to the ideal radius in part (1) must the machinist control the radius? 3. In terms of the , definition of what is ? What is ? What value of is given? What is the corresponding value of ? Request Is my answer correct? If not, in what part of my solution did I make a mistake?","1000 ^2 \pm 5 ^2 \epsilon \delta \lim \limits_{x \to a}{f(x)} = L x f(x) \epsilon \delta \pi r_0^2 = 1000 \implies r_0 = \sqrt{\frac{1000}{\pi}} \approx 17.8412 \epsilon \delta \lim \limits_{x \to a}{f(x)} = L x f(x) \epsilon \delta a = r_0 \approx 17.8412 L = \pi r_0^2 = 1000 x = r f(x) = \pi r^2 |r - r_0| < \delta |\pi r^2 - \pi r_0^2| < \epsilon \pm 5 ^2 |\pi r^2 - \pi r_0^2| < \epsilon \implies |\pi r^2 - 1000| < 5 \implies -5 < \pi r^2 - 1000 < 5 \implies 995 < \pi r^2 < 1005 \implies \frac{995}{\pi} < r^2 < \frac{1005}{\pi} \implies \sqrt{\frac{995}{\pi}} < r < \sqrt{\frac{1005}{\pi}} \implies 17.7966 < r < 17.8858 |r - r_0| < \delta \implies |r - 17.8412| < \delta |17.7966 - 17.8412| < \delta \implies 0.0446 < \delta |17.8858 - 17.8412| < \delta \implies 0.0446 < \delta \delta = \min(0.0446, 0.0446) = 0.0446 r_0 = 17.8412 \text{ cm} \pm 5 ^2 \delta = 0.0446 \text{ cm} \epsilon \delta \lim \limits_{x \to a}{f(x)} = L x f(x) \epsilon \delta |r - r_0| < \delta |\pi r^2 - \pi r_0^2| < \epsilon","['calculus', 'limits', 'proof-verification', 'word-problem']"
36,Order of limits of difference quotient,Order of limits of difference quotient,,"Let $f$ be a uniformly continuous real function, and let $a$ be a real number. Assume that $f'$ is also uniformly continuous. I need to prove this: $$\lim_{x_1\rightarrow a}\lim_{x_2\rightarrow x_1}\frac{f(x_1)-f(x_2)}{x_1-x_2} =\lim_{x_2\rightarrow a}\lim_{x_1\rightarrow a}\frac{f(x_1)-f(x_2)}{x_1-x_2}$$ Intuitively, the equation just says that the derivative of $f$ at $a$ is equal to the limit of the derivatives of $f$ close to $a$, and is therefore clearly true. However, I need a proof that proceeds directly from the definitions of limits and uniform continuity (or ""basic"" theorems about limits and uniform continuity), and I don't know how to do that.","Let $f$ be a uniformly continuous real function, and let $a$ be a real number. Assume that $f'$ is also uniformly continuous. I need to prove this: $$\lim_{x_1\rightarrow a}\lim_{x_2\rightarrow x_1}\frac{f(x_1)-f(x_2)}{x_1-x_2} =\lim_{x_2\rightarrow a}\lim_{x_1\rightarrow a}\frac{f(x_1)-f(x_2)}{x_1-x_2}$$ Intuitively, the equation just says that the derivative of $f$ at $a$ is equal to the limit of the derivatives of $f$ close to $a$, and is therefore clearly true. However, I need a proof that proceeds directly from the definitions of limits and uniform continuity (or ""basic"" theorems about limits and uniform continuity), and I don't know how to do that.",,"['real-analysis', 'limits', 'derivatives']"
37,Examples in which it is very difficult to find a $\delta$?,Examples in which it is very difficult to find a ?,\delta,"I'm reading Henle/Kleinenberg's Infinitesimal Calculus . They say on page $8$: Can you provide examples in which it is very difficult to find a $\delta$? The example they provided in the previous page is very elementar, I'd like to see how hard it could be.","I'm reading Henle/Kleinenberg's Infinitesimal Calculus . They say on page $8$: Can you provide examples in which it is very difficult to find a $\delta$? The example they provided in the previous page is very elementar, I'd like to see how hard it could be.",,"['real-analysis', 'limits', 'epsilon-delta']"
38,Q: Limit with odd function,Q: Limit with odd function,,"If $f$ is an odd function, show using the 2-sided limits definitions that $\lim_{x\to 0+} f(x) = L$ if and only if $\lim_{x\to 0-} f(x) = -L$.","If $f$ is an odd function, show using the 2-sided limits definitions that $\lim_{x\to 0+} f(x) = L$ if and only if $\lim_{x\to 0-} f(x) = -L$.",,"['calculus', 'limits']"
39,Telescopic series multiplied by n,Telescopic series multiplied by n,,If I have a convergent monotonic series $\sum_{n \in \mathbb{N}}a_n = S < \infty$ then I now that the corresponding telescoping series $\sum_{n=0}^{\infty}(a_n-a_{n+1})$ converges too. How can I demonstrate that another series \begin{equation}   \sum_{n=0}^{\infty}n(a_n-a_{n+1}) \end{equation} converges and has the same limit as $\sum_{n=0}^{\infty} a_n$? I tried with the method of differences but I end up with \begin{align}   \lim_{N\to \infty} \sum_{n=0}^{N}n(a_n-a_{n+1})   &= -\lim_{N\to \infty}Na_{N+1} + \lim_{N\to \infty} \sum_{n=0}^{N}a_n\\   &= S -\lim_{N\to \infty}Na_{N+1} \end{align} $\lim_{N\to \infty}Na_{N+1}$ is now an indeterminate form $\infty \times 0$ and I have to show that it is equal to $0$.,If I have a convergent monotonic series $\sum_{n \in \mathbb{N}}a_n = S < \infty$ then I now that the corresponding telescoping series $\sum_{n=0}^{\infty}(a_n-a_{n+1})$ converges too. How can I demonstrate that another series \begin{equation}   \sum_{n=0}^{\infty}n(a_n-a_{n+1}) \end{equation} converges and has the same limit as $\sum_{n=0}^{\infty} a_n$? I tried with the method of differences but I end up with \begin{align}   \lim_{N\to \infty} \sum_{n=0}^{N}n(a_n-a_{n+1})   &= -\lim_{N\to \infty}Na_{N+1} + \lim_{N\to \infty} \sum_{n=0}^{N}a_n\\   &= S -\lim_{N\to \infty}Na_{N+1} \end{align} $\lim_{N\to \infty}Na_{N+1}$ is now an indeterminate form $\infty \times 0$ and I have to show that it is equal to $0$.,,"['sequences-and-series', 'limits', 'proof-verification', 'telescopic-series']"
40,How to show that following limit is zero?,How to show that following limit is zero?,,"In one of the research article it is written that the following limit is equal to zero $$\lim_{x \to 0 }\frac{d}{2^{b+c/x}-1}\left[a2^{b+c/x}-a-a\frac{c\ln{(2)}2^{b+c/x}}{2x}-\frac{c\ln{(2)}}{2x^2}\frac{2^{b+c/x}}{\sqrt{2^{b+c/x}-1}}\right]\left(e^{-ax\sqrt{2^{b+c/x}-1}}\right)=0$$ where $a,b,c,d$ are all positive constants. I am unable to solve it. Please help me in getting there. Many thanks in advance.","In one of the research article it is written that the following limit is equal to zero $$\lim_{x \to 0 }\frac{d}{2^{b+c/x}-1}\left[a2^{b+c/x}-a-a\frac{c\ln{(2)}2^{b+c/x}}{2x}-\frac{c\ln{(2)}}{2x^2}\frac{2^{b+c/x}}{\sqrt{2^{b+c/x}-1}}\right]\left(e^{-ax\sqrt{2^{b+c/x}-1}}\right)=0$$ where $a,b,c,d$ are all positive constants. I am unable to solve it. Please help me in getting there. Many thanks in advance.",,"['limits', 'limits-without-lhopital']"
41,Evaluating $\frac1{n^2+1^2}+\frac2{n^2+2^2}+\dots+\frac n{n^2+n^2}$ using Riemann sum [duplicate],Evaluating  using Riemann sum [duplicate],\frac1{n^2+1^2}+\frac2{n^2+2^2}+\dots+\frac n{n^2+n^2},"This question already has answers here : limit $\lim_{n\to \infty }\sum_{k=1}^{n}\frac{k}{k^2+n^2}$ [duplicate] (3 answers) Closed 7 years ago . Not sure how to get started on this question. Do I have to form the summation? If yes, how do I go about doing it? Any help will be really appreciated!","This question already has answers here : limit $\lim_{n\to \infty }\sum_{k=1}^{n}\frac{k}{k^2+n^2}$ [duplicate] (3 answers) Closed 7 years ago . Not sure how to get started on this question. Do I have to form the summation? If yes, how do I go about doing it? Any help will be really appreciated!",,"['calculus', 'sequences-and-series', 'limits', 'riemann-sum']"
42,Limit of a sequence of intervals,Limit of a sequence of intervals,,"Let $\{a_n\}_{n\geqslant 1} $ be a sequence such that $a_n>0\ \forall n$ and $\lim_{n\to\infty} a_n=0$ . What will be $$\bigcup_{n=1}^\infty [a_n,1) \ \ =?$$ Will it be $(0,1)$ or $[0,1)$ ? Edit: This was originally a part of a larger question. Consider $2$ sequences $\{a_n\}\ \&\ \{b_n\}$ with $a_n>0$ and $b_n>1$ for all $n$ and $\lim_{n\to\infty}a_n=0$ and $\lim_{n\to\infty}b_n=1$. Define $A_n=[a_n,b_n)$. Find $\limsup_{n\to\infty} A_n$ and $\liminf_{n\to\infty} A_n$ Can someone confirm me whether both the answers are $(0,1)$ or not?","Let $\{a_n\}_{n\geqslant 1} $ be a sequence such that $a_n>0\ \forall n$ and $\lim_{n\to\infty} a_n=0$ . What will be $$\bigcup_{n=1}^\infty [a_n,1) \ \ =?$$ Will it be $(0,1)$ or $[0,1)$ ? Edit: This was originally a part of a larger question. Consider $2$ sequences $\{a_n\}\ \&\ \{b_n\}$ with $a_n>0$ and $b_n>1$ for all $n$ and $\lim_{n\to\infty}a_n=0$ and $\lim_{n\to\infty}b_n=1$. Define $A_n=[a_n,b_n)$. Find $\limsup_{n\to\infty} A_n$ and $\liminf_{n\to\infty} A_n$ Can someone confirm me whether both the answers are $(0,1)$ or not?",,"['sequences-and-series', 'limits', 'interval-arithmetic']"
43,Limit to infinity using $\delta$-$\epsilon$ language.,Limit to infinity using - language.,\delta \epsilon,"I'm trying to prove: $$\lim_{x \to \infty} f(x) = \lim_{x \to \infty} \frac{x^2 + x - 1}{3x^2 + 2x + 1} = \frac{1}{3}$$ using $\delta$-$\epsilon$ language. Here is my attempt: Proof: We wish to show that as $x$ gets arbitrarily large that the difference between $f(x)$ and $\frac{1}{3}$ gets arbitrarily small. In other words, we show that $\forall \epsilon > 0$ $\exists \delta > 0$ such that $$x > \delta \Rightarrow \bigg| \frac{x^2 + x - 1}{3x^2 + 2x + 1} - \frac{1}{3} \bigg| < \epsilon$$ Now observe for $x > 2$, \begin{align*} \bigg| \frac{x^2 + x - 1}{3x^2 + 2x + 1} - \frac{1}{3} \bigg| &= \bigg| \frac{x-4}{9x^2 + 6x + 3} \bigg| \tag{$\star$} \\ &\leq \bigg| \frac{x-4}{3(x^2 + 2x + 1)} \bigg|\\ &= \bigg| \frac{x-4}{3(x + 1)^2} \bigg| \\ &= \bigg| \frac{1}{3(x+1)} - \frac{5}{3(x+1)^{2}} \bigg|\\ &< \frac{1}{x+1} \end{align*} Let $\delta = \max\left\{ 2, \dfrac{1}{\epsilon} - 1 \right\}$. Then, if $x > \delta$ we have that $$\star < \dfrac{1}{x+1} < \frac{1}{\delta + 1} = \dfrac{1}{\left(\frac{1}{\epsilon} - 1 \right) + 1} = \epsilon$$ I just want to make sure that I did this correctly. Thank you.","I'm trying to prove: $$\lim_{x \to \infty} f(x) = \lim_{x \to \infty} \frac{x^2 + x - 1}{3x^2 + 2x + 1} = \frac{1}{3}$$ using $\delta$-$\epsilon$ language. Here is my attempt: Proof: We wish to show that as $x$ gets arbitrarily large that the difference between $f(x)$ and $\frac{1}{3}$ gets arbitrarily small. In other words, we show that $\forall \epsilon > 0$ $\exists \delta > 0$ such that $$x > \delta \Rightarrow \bigg| \frac{x^2 + x - 1}{3x^2 + 2x + 1} - \frac{1}{3} \bigg| < \epsilon$$ Now observe for $x > 2$, \begin{align*} \bigg| \frac{x^2 + x - 1}{3x^2 + 2x + 1} - \frac{1}{3} \bigg| &= \bigg| \frac{x-4}{9x^2 + 6x + 3} \bigg| \tag{$\star$} \\ &\leq \bigg| \frac{x-4}{3(x^2 + 2x + 1)} \bigg|\\ &= \bigg| \frac{x-4}{3(x + 1)^2} \bigg| \\ &= \bigg| \frac{1}{3(x+1)} - \frac{5}{3(x+1)^{2}} \bigg|\\ &< \frac{1}{x+1} \end{align*} Let $\delta = \max\left\{ 2, \dfrac{1}{\epsilon} - 1 \right\}$. Then, if $x > \delta$ we have that $$\star < \dfrac{1}{x+1} < \frac{1}{\delta + 1} = \dfrac{1}{\left(\frac{1}{\epsilon} - 1 \right) + 1} = \epsilon$$ I just want to make sure that I did this correctly. Thank you.",,"['calculus', 'limits', 'infinity']"
44,Proving the quotient rule for limits,Proving the quotient rule for limits,,"In real analysis, we have been asked to finish a proof of the quotient rule for limits (Given that $f(x)$ approaches $L$ and $g(x)$ approaches M as $x$ approaches a, prove that $\frac{f(x)}{g(x)}$ approaches $\frac{L}{M}$ . I know that I could rewrite the quotient as multiplication and prove it that way but that is not the way the proof we are completing starts off. Here is how the proof in the book starts: We showed in a previous exercise that $|g(x)| > \frac{|M|}{2}$ for all $x$ belonging to $D$ near $a$ . In particular, $g(x)$ does not equal $0$ for all $x$ belonging to $D$ near $a$ . Consequently, $\frac{f(x)}{g(x)}$ makes sense near $a$ . Let $\epsilon > 0$ be given. The proof is based on the triangle inequality: \begin{align*} \left|\frac{f(x)}{g(x)} - \frac{L}{M}\right| &= \left|\frac{f(x)}{g(x)} - \frac{L}{g(x)} + \frac{L}{g(x)} - \frac{L}{M}\right|\\ & = \left|\frac{f(x)}{g(x)} - \frac{L}{g(x)} + L \frac{M - g(x)}{Mg(x)}\right| \\ &\leq \left|\frac{1}{g(x)}\right| |f(x) - L| + \left|\frac{L}{M}\right| \left|\frac{1}{g(x)}\right| |M - g(x)|. \end{align*} We are expected to pick up the proof at this point. I was thinking that since I want everything above $< \frac{\epsilon}{2} + \frac{\epsilon}{2}=\epsilon$ , that I would construct $\epsilon_f$ and $\epsilon_g$ based on $\epsilon$ . So, I said we want $|\frac{1}{g(x)}| |f(x) - L| < \epsilon/2$ and because the limit approaches $L$ end up with $\epsilon_f$ equals $\frac{\epsilon|M|}{4}$ . I intended to do the same for the g part; however, when I go to define $\epsilon_g$ it involves $M$ . Am I allowed to use $M$ in that definition for $\epsilon_g$ ? And if not, any ideas on where to go from here? Thank you!!","In real analysis, we have been asked to finish a proof of the quotient rule for limits (Given that approaches and approaches M as approaches a, prove that approaches . I know that I could rewrite the quotient as multiplication and prove it that way but that is not the way the proof we are completing starts off. Here is how the proof in the book starts: We showed in a previous exercise that for all belonging to near . In particular, does not equal for all belonging to near . Consequently, makes sense near . Let be given. The proof is based on the triangle inequality: We are expected to pick up the proof at this point. I was thinking that since I want everything above , that I would construct and based on . So, I said we want and because the limit approaches end up with equals . I intended to do the same for the g part; however, when I go to define it involves . Am I allowed to use in that definition for ? And if not, any ideas on where to go from here? Thank you!!","f(x) L g(x) x \frac{f(x)}{g(x)} \frac{L}{M} |g(x)| > \frac{|M|}{2} x D a g(x) 0 x D a \frac{f(x)}{g(x)} a \epsilon > 0 \begin{align*}
\left|\frac{f(x)}{g(x)} - \frac{L}{M}\right| &= \left|\frac{f(x)}{g(x)} - \frac{L}{g(x)} + \frac{L}{g(x)} - \frac{L}{M}\right|\\
& = \left|\frac{f(x)}{g(x)} - \frac{L}{g(x)} + L \frac{M - g(x)}{Mg(x)}\right| \\
&\leq \left|\frac{1}{g(x)}\right| |f(x) - L| + \left|\frac{L}{M}\right| \left|\frac{1}{g(x)}\right| |M - g(x)|.
\end{align*} < \frac{\epsilon}{2} + \frac{\epsilon}{2}=\epsilon \epsilon_f \epsilon_g \epsilon |\frac{1}{g(x)}| |f(x) - L| < \epsilon/2 L \epsilon_f \frac{\epsilon|M|}{4} \epsilon_g M M \epsilon_g","['real-analysis', 'limits']"
45,A rigorous proof of L'Hpital's rule (/ case for infinite limit),A rigorous proof of L'Hpital's rule (/ case for infinite limit),,"I am looking for a rigorous proof of the following statement: Fix $f,g : \mathbb{R}\to\mathbb{R}$ with $\lim_{x\to 0^+} f(x) = \lim_{x\to 0^+} g(x) = \infty$. Assume that $f', g'$ are differentiable in the right neighbourhood of $0$ and $g'(x)\neq 0$ everywhere sufficiently close to $0$. Then, if $$\lim_{x\to 0^+} \frac{f'(x)}{g'(x)} = \infty\ ,$$ we also have $$\lim_{x\to 0^+} \frac{f(x)}{g(x)} = \infty\ .$$ All proofs I was able to find seem to either ignore that case or contain mistakes or omissions. One possible route would be to apply the / case of L'Hpital for a finite limit to $g(x)/f(x)$ (which tends to $0$), but in order to show that $f(x)/g(x)$, one would need something stronger, nemaly that $g(x)/f(x)$ tends to $0^+$, which is not something any version of L'Hpital's rule that I know of provides. In fact, what I would ideally like to have is a more unified proof of the / case without all these case distinctions (for the 0/0 case, that is possible and quite easy), but I don't think such a proof exists. UPDATE: Okay, I must have had a momentary blackout. If both $f$ and $g$ tend to infinity, then of course $g(x)/f(x)$ must be positive in a righ neighbourhood of $0$. However, two interesting questions remain: Does the statement still hold if only one of $f$ and $g$ tends to infinity? For the case where the limit of $f'(x)/g'(x)$ is finite, only $g$ needs to tend to infinity, as shown e.g. in Walter Rudin's proof of L'Hpital in ""Principles of Mathematical Analysis"". Is there some alternative unifying approach that avoids all these case distinctions altogether?","I am looking for a rigorous proof of the following statement: Fix $f,g : \mathbb{R}\to\mathbb{R}$ with $\lim_{x\to 0^+} f(x) = \lim_{x\to 0^+} g(x) = \infty$. Assume that $f', g'$ are differentiable in the right neighbourhood of $0$ and $g'(x)\neq 0$ everywhere sufficiently close to $0$. Then, if $$\lim_{x\to 0^+} \frac{f'(x)}{g'(x)} = \infty\ ,$$ we also have $$\lim_{x\to 0^+} \frac{f(x)}{g(x)} = \infty\ .$$ All proofs I was able to find seem to either ignore that case or contain mistakes or omissions. One possible route would be to apply the / case of L'Hpital for a finite limit to $g(x)/f(x)$ (which tends to $0$), but in order to show that $f(x)/g(x)$, one would need something stronger, nemaly that $g(x)/f(x)$ tends to $0^+$, which is not something any version of L'Hpital's rule that I know of provides. In fact, what I would ideally like to have is a more unified proof of the / case without all these case distinctions (for the 0/0 case, that is possible and quite easy), but I don't think such a proof exists. UPDATE: Okay, I must have had a momentary blackout. If both $f$ and $g$ tend to infinity, then of course $g(x)/f(x)$ must be positive in a righ neighbourhood of $0$. However, two interesting questions remain: Does the statement still hold if only one of $f$ and $g$ tends to infinity? For the case where the limit of $f'(x)/g'(x)$ is finite, only $g$ needs to tend to infinity, as shown e.g. in Walter Rudin's proof of L'Hpital in ""Principles of Mathematical Analysis"". Is there some alternative unifying approach that avoids all these case distinctions altogether?",,"['real-analysis', 'limits', 'proof-explanation']"
46,For the following function decide for which numbers $a$ the limit $\lim_{x\to a}f(x)$ exists:,For the following function decide for which numbers  the limit  exists:,a \lim_{x\to a}f(x),"For the following function decide for which numbers $a$ the limit $$\lim_{x\to a}f(x)$$ exists: $$f(x) = \text{1st number in decimal expansion of } x $$ the solution says that all $a$ not of the form $$n + \frac{k}{10}$$ for integers  $n$ and $k$ can work. I'm trying to reconcile this notion. I personally thought that all values of $a$ would be fine, but are we just taking into consideration all values except for $a$ itself? I mean all values that approach $a$? But with that being the idea, every value can have a decimal representation which would make it difficult for the limit to exist....","For the following function decide for which numbers $a$ the limit $$\lim_{x\to a}f(x)$$ exists: $$f(x) = \text{1st number in decimal expansion of } x $$ the solution says that all $a$ not of the form $$n + \frac{k}{10}$$ for integers  $n$ and $k$ can work. I'm trying to reconcile this notion. I personally thought that all values of $a$ would be fine, but are we just taking into consideration all values except for $a$ itself? I mean all values that approach $a$? But with that being the idea, every value can have a decimal representation which would make it difficult for the limit to exist....",,"['calculus', 'limits']"
47,Evaluation of $\lim_{n \to \infty} ((n+1)!\ln (a_n))$,Evaluation of,\lim_{n \to \infty} ((n+1)!\ln (a_n)),Consider the sequence $(a_n)_{n \geq1}$ such that $a_0=2$ and $a_{n-1}-a_n=\frac{n}{(n+1)!}$. Evaluate $$\lim_{n \to \infty} ((n+1)!\ln (a_n))$$ Could someone hint me as how to achieve value of $a_n$ from given information.,Consider the sequence $(a_n)_{n \geq1}$ such that $a_0=2$ and $a_{n-1}-a_n=\frac{n}{(n+1)!}$. Evaluate $$\lim_{n \to \infty} ((n+1)!\ln (a_n))$$ Could someone hint me as how to achieve value of $a_n$ from given information.,,"['calculus', 'limits', 'closed-form']"
48,"If a series converges, then the sequence of terms converges to 0, Proof by contradiction.","If a series converges, then the sequence of terms converges to 0, Proof by contradiction.",,"I know there is an elegant proof of this theorem, But I've been ask to prove it in the test and the only thing I was thinking of was to prove it by contradiction.I want to know if I did right, or maybe I was assuming something wrong: ${S_n} = \sum _{n=0}^{\infty }\:a_n$ converge and assume that $\lim _{x\to \infty }\left(a_n\right)\:=\:L\:\in \mathbb{R}$    (L isn't zero) So there is $N\in \mathbb{N}\:\:\:\:\:\forall \:n>\:N\:\:\rightarrow \:a_n>\:L-\epsilon$ So if  $a_n>0 \Rightarrow \sum _{n=0}^{\infty }\:a_n>\sum _{n=0}^{\infty }\:L-\epsilon =\:\lim _{n\to \infty }\left(n\cdot \left(L-\epsilon \right)\right)=\infty \:$ and that's mean the series diverge. And if $a_{n\:}$ can be positive and negative, let's look at both $\left(S_{n_k}\right)$ the series of the non-negative number $\left(S_{n_r}\right)$ the series of the negative number  that will converge to  $\infty \:,\:-\infty $ So that means ${S_n}$ doesn't have a limit What do you say guys? Is it legit prove  ?","I know there is an elegant proof of this theorem, But I've been ask to prove it in the test and the only thing I was thinking of was to prove it by contradiction.I want to know if I did right, or maybe I was assuming something wrong: ${S_n} = \sum _{n=0}^{\infty }\:a_n$ converge and assume that $\lim _{x\to \infty }\left(a_n\right)\:=\:L\:\in \mathbb{R}$    (L isn't zero) So there is $N\in \mathbb{N}\:\:\:\:\:\forall \:n>\:N\:\:\rightarrow \:a_n>\:L-\epsilon$ So if  $a_n>0 \Rightarrow \sum _{n=0}^{\infty }\:a_n>\sum _{n=0}^{\infty }\:L-\epsilon =\:\lim _{n\to \infty }\left(n\cdot \left(L-\epsilon \right)\right)=\infty \:$ and that's mean the series diverge. And if $a_{n\:}$ can be positive and negative, let's look at both $\left(S_{n_k}\right)$ the series of the non-negative number $\left(S_{n_r}\right)$ the series of the negative number  that will converge to  $\infty \:,\:-\infty $ So that means ${S_n}$ doesn't have a limit What do you say guys? Is it legit prove  ?",,"['calculus', 'limits', 'infinity']"
49,Study of differentiablity of function,Study of differentiablity of function,,"Study the differentiability of the function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ $f(x,y)=\begin{cases}        \frac{x^3+y^3}{x^2+\left|y\right|} & (x,y)\ne(0,0) \\       0 &(x,y)=(0,0) \\    \end{cases}$ in point $(0,0)$. So what I did is I calculated the partial derivatives of the function in point $(0,0)$. I got: $$\frac{f}{x}\left(0,0\right)=\lim_{t\rightarrow 0}\left(\frac{f\left(t,0\right)-f\left(0,0\right)}{t}\right)=\lim_{t\rightarrow 0}\left(\frac{t^3}{t^3}\right)=1$$and $$\frac{f}{y}\left(0,0\right)=\lim_{t\rightarrow 0}\left(\frac{f\left(0,t\right)-f\left(0,0\right)}{t}\right)=\lim_{t\rightarrow 0}\left(\frac{t^3}{t\left|t\right|}\right)=0$$ So he partial derivatives do exist. Next I think I need to calculate: $$l=\lim_{h\to O_m}\left(\frac{f\left(a+h\right)-f\left(a\right)-<h,\Delta f\left(a\right)>}{\left|\left|h\right|\right|}\right)$$ $$\:l=\lim_{\left(h_1,h_2\right)\rightarrow \left(0,0\right)}\:\left(\frac{f\left(h_1,h_2\right)-f\left(0,0\right)-h_1\cdot 1-h_2\cdot 0}{\sqrt{h_1^2+h_2^2}}\right)$$ $$\:l=\lim_{\left(h_1,h_2\right)\rightarrow \left(0,0\right)}\:\left(\frac{\frac{h_1^3+h_2^3}{h_1^2+\left|h_2\right|}-h_1}{\sqrt{h_1^2+h_2^2}}\right)$$ And this is the point at which I get stuck, since I don't really fully understand what I'm doing. Can anyone help me with this a bit?","Study the differentiability of the function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ $f(x,y)=\begin{cases}        \frac{x^3+y^3}{x^2+\left|y\right|} & (x,y)\ne(0,0) \\       0 &(x,y)=(0,0) \\    \end{cases}$ in point $(0,0)$. So what I did is I calculated the partial derivatives of the function in point $(0,0)$. I got: $$\frac{f}{x}\left(0,0\right)=\lim_{t\rightarrow 0}\left(\frac{f\left(t,0\right)-f\left(0,0\right)}{t}\right)=\lim_{t\rightarrow 0}\left(\frac{t^3}{t^3}\right)=1$$and $$\frac{f}{y}\left(0,0\right)=\lim_{t\rightarrow 0}\left(\frac{f\left(0,t\right)-f\left(0,0\right)}{t}\right)=\lim_{t\rightarrow 0}\left(\frac{t^3}{t\left|t\right|}\right)=0$$ So he partial derivatives do exist. Next I think I need to calculate: $$l=\lim_{h\to O_m}\left(\frac{f\left(a+h\right)-f\left(a\right)-<h,\Delta f\left(a\right)>}{\left|\left|h\right|\right|}\right)$$ $$\:l=\lim_{\left(h_1,h_2\right)\rightarrow \left(0,0\right)}\:\left(\frac{f\left(h_1,h_2\right)-f\left(0,0\right)-h_1\cdot 1-h_2\cdot 0}{\sqrt{h_1^2+h_2^2}}\right)$$ $$\:l=\lim_{\left(h_1,h_2\right)\rightarrow \left(0,0\right)}\:\left(\frac{\frac{h_1^3+h_2^3}{h_1^2+\left|h_2\right|}-h_1}{\sqrt{h_1^2+h_2^2}}\right)$$ And this is the point at which I get stuck, since I don't really fully understand what I'm doing. Can anyone help me with this a bit?",,"['limits', 'multivariable-calculus', 'derivatives']"
50,Continuity of a function $f : \mathbb R^n \to \mathbb R^k$,Continuity of a function,f : \mathbb R^n \to \mathbb R^k,"So preparing for the end-term of the semester, strolling trough exercises in our huge book (Th. Rassias Calculus 2) and in the internet, I found one more theoritical one, that as always bothers me (because it's theoritical !!). Show that in the function $f : \mathbb R^n \to \mathbb R^k$ , $f=(f_1,\dots,f_k)$ is continuous at $\vec x_o$, if and only if the function $f_i$ is continuous at $\vec x_o$ for $i = [1,2,\dots,n]$. Now, it's pretty obvious that since the ""big"" function $f$ is formed by the ""smaller"" $f_i$'s , all the sub-functions must be continuous, but that's not a mathematical proof of course. Also, I could say that for $f$ to be continuous, its limit must be defined, which means that $\lim_{x \to x_o}f = f_o$, which would mean that : $\lim_{x \to x_o}f = (\lim_{x \to x_o}f_1^o,\dots,\lim_{x \to x_o}f_k^o) = f_o = (f_1^o,\dots,f_k^o)$. Now, by definition that means that each of the sub-functions must be continuous, which means that they are continuous if and only if $\forall e>0$ $\exists d>0$ : $\forall x \in \mathbb R^n$ with distance $d(x,x_o)<d \Rightarrow d(f(x),f(x_{io})<d$. Is that a kind of correct claim ? I am pretty sure it's not as mathematical it should be. Any tips-hints-corrections or help over this will be really appreciated !","So preparing for the end-term of the semester, strolling trough exercises in our huge book (Th. Rassias Calculus 2) and in the internet, I found one more theoritical one, that as always bothers me (because it's theoritical !!). Show that in the function $f : \mathbb R^n \to \mathbb R^k$ , $f=(f_1,\dots,f_k)$ is continuous at $\vec x_o$, if and only if the function $f_i$ is continuous at $\vec x_o$ for $i = [1,2,\dots,n]$. Now, it's pretty obvious that since the ""big"" function $f$ is formed by the ""smaller"" $f_i$'s , all the sub-functions must be continuous, but that's not a mathematical proof of course. Also, I could say that for $f$ to be continuous, its limit must be defined, which means that $\lim_{x \to x_o}f = f_o$, which would mean that : $\lim_{x \to x_o}f = (\lim_{x \to x_o}f_1^o,\dots,\lim_{x \to x_o}f_k^o) = f_o = (f_1^o,\dots,f_k^o)$. Now, by definition that means that each of the sub-functions must be continuous, which means that they are continuous if and only if $\forall e>0$ $\exists d>0$ : $\forall x \in \mathbb R^n$ with distance $d(x,x_o)<d \Rightarrow d(f(x),f(x_{io})<d$. Is that a kind of correct claim ? I am pretty sure it's not as mathematical it should be. Any tips-hints-corrections or help over this will be really appreciated !",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus']"
51,"$a_n = \frac{1}{n}b_n$, $\lim b_n = L>0, L\in\mathbb{R}$, prove $\sum a_n$ diverges",", , prove  diverges","a_n = \frac{1}{n}b_n \lim b_n = L>0, L\in\mathbb{R} \sum a_n","I have to prove that if $$a_n = \frac{1}{n}b_n$$for $n\ge 1$ and $$\lim_{n\to\infty} b_n = L>0, L\in\mathbb{R}$$ then $$\sum_{n=1}^{\infty} a_n$$ diverges. My idea was to show that it's not true that $a_n\to 0$ but I guess it's true because in $\frac{1}{n}b_n$, $b_n$ is limited because converges, and $\frac{1}{n}$ goes to $0$, and there is a theorem that says that when these two things happen in a product, it goes to $0$. So I cannot affirmate anything with this result. I guess it has something to do with comparsion but I cannot find any good comparsion between $a_n$ and $\frac{1}{n}b_n$","I have to prove that if $$a_n = \frac{1}{n}b_n$$for $n\ge 1$ and $$\lim_{n\to\infty} b_n = L>0, L\in\mathbb{R}$$ then $$\sum_{n=1}^{\infty} a_n$$ diverges. My idea was to show that it's not true that $a_n\to 0$ but I guess it's true because in $\frac{1}{n}b_n$, $b_n$ is limited because converges, and $\frac{1}{n}$ goes to $0$, and there is a theorem that says that when these two things happen in a product, it goes to $0$. So I cannot affirmate anything with this result. I guess it has something to do with comparsion but I cannot find any good comparsion between $a_n$ and $\frac{1}{n}b_n$",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
52,When are the limits of roots of a polynomial identical to the roots of the limit of the polynomial?,When are the limits of roots of a polynomial identical to the roots of the limit of the polynomial?,,"I have a univariate polynomial of degree $n$ (where $n$ is larger than $4$). The real-valued coefficients of the polynomial depend on a parameter $\psi$, i.e. $$p_\psi(x)=a_n(\psi) x^n+a_{n-1}(\psi) x^{n-1}+\ldots+a_1(\psi)x+a_0(\psi).$$ Ideally, I would like to compute the roots of this polynomial, $r_i(\psi)$, $i\in\{1,\ldots,n\}$, and take the limit of $\psi$ to zero $${\hat r}_i=\lim_{\psi\rightarrow 0} r_i(\psi)$$ However , since the degree of the polynomial is too high, I cannot get a solution in closed form. A feasible alternative would be the following: Compute the roots $r^\ast_j$ of the limit of the polynomial $$p^\ast(x)=\lim_{\psi\rightarrow 0} p_\psi(x).$$ In my case, the coefficients are such that a) the leading coefficient $a_n(\psi)$ will converge to zero and b) the limiting polynomial $p^\ast(x)$ will factor into low order polynomials. My question is : under which conditions do the two calculations lead to the same answer, i.e. ${\hat r}_i=r^\ast_j$, for the roots that exist in both cases? Any answers or references would be much appreciated.","I have a univariate polynomial of degree $n$ (where $n$ is larger than $4$). The real-valued coefficients of the polynomial depend on a parameter $\psi$, i.e. $$p_\psi(x)=a_n(\psi) x^n+a_{n-1}(\psi) x^{n-1}+\ldots+a_1(\psi)x+a_0(\psi).$$ Ideally, I would like to compute the roots of this polynomial, $r_i(\psi)$, $i\in\{1,\ldots,n\}$, and take the limit of $\psi$ to zero $${\hat r}_i=\lim_{\psi\rightarrow 0} r_i(\psi)$$ However , since the degree of the polynomial is too high, I cannot get a solution in closed form. A feasible alternative would be the following: Compute the roots $r^\ast_j$ of the limit of the polynomial $$p^\ast(x)=\lim_{\psi\rightarrow 0} p_\psi(x).$$ In my case, the coefficients are such that a) the leading coefficient $a_n(\psi)$ will converge to zero and b) the limiting polynomial $p^\ast(x)$ will factor into low order polynomials. My question is : under which conditions do the two calculations lead to the same answer, i.e. ${\hat r}_i=r^\ast_j$, for the roots that exist in both cases? Any answers or references would be much appreciated.",,"['limits', 'polynomials', 'roots']"
53,Trying to compute a Riemann-sum as an integral where the,Trying to compute a Riemann-sum as an integral where the,,"$\lim_{n\to\infty} a\cdot n +\sum_{n^2}^{4n^2} \dfrac{1}{k+n}$ I have the following problem: I want to figure out , for what value of a does this limit converge and what value does it then converge to. I'm assuming that I'm supposed to rewrite the sum as an integral where I can evaluate the integral in the form of ""n's"". Given the integral $\int_{1}^{4}\dfrac{1}{\sqrt{x}}dx = [2\sqrt{x}]_{1}^{4}$, where $x= k+n$. I think I can finish the problem. However, I am unsure of how to get the integral $\int_{1}^{4}\dfrac{1}{\sqrt{x}}dx$ Any help will be grateful. Thanks for your time.","$\lim_{n\to\infty} a\cdot n +\sum_{n^2}^{4n^2} \dfrac{1}{k+n}$ I have the following problem: I want to figure out , for what value of a does this limit converge and what value does it then converge to. I'm assuming that I'm supposed to rewrite the sum as an integral where I can evaluate the integral in the form of ""n's"". Given the integral $\int_{1}^{4}\dfrac{1}{\sqrt{x}}dx = [2\sqrt{x}]_{1}^{4}$, where $x= k+n$. I think I can finish the problem. However, I am unsure of how to get the integral $\int_{1}^{4}\dfrac{1}{\sqrt{x}}dx$ Any help will be grateful. Thanks for your time.",,"['limits', 'riemann-sum', 'riemann-integration']"
54,Rigor in proving continuity of $f$ over a closed interval $I$,Rigor in proving continuity of  over a closed interval,f I,"Given a function $f$ on a closed interval $I \subset \mathbb{R}$, where $I = [a,b]$, to prove continuity of $f$ over the interval $I$, what is generally done is the following. 1. We prove that $f$ is continuous at endpoint $a$ $$ \lim_{x \ \to \  a^{+}} f(x) = f(a)$$ 2. We prove that $f$ is continuous $\forall c \in (a, b)$ $$ \lim_{x \to c} f(x) = f(c) \ \ \ \ \text{,}\ \ \ \ \forall c \in (a, b)$$ 3. We prove that $f$ is continuousat endpoint $b$ $$ \lim_{x \ \to \  b^{-}} f(x) = f(b)$$ My question revolves around step 2 of this process. Usually all that is done to prove the continuity of $f$ over the open interval $(a, b)$, is (using the Limit Laws) a simple direct substitution of a variable $c$ denoting an arbitrary number in the open interval, which is basically substituting $c \in (a, b)$ into $f(x)$. But I don't see why this is mathematically rigorous? Initially I assumed something along the lines of Mathematical Induction would have been used to prove $f$ to be continuous $\forall c \in (a, b)$, but that doesn't seem to be the case. Why does just choosing an arbitrary $c$ to represent a number in the interval $(a, b)$, and showing that the chosen $c$ satisfies the given definition of continuity, prove that $f$ is continuous over $(a, b)$ ? I will give an example below to highlight what I'm asking. Example : Prove $f(x) = e^{x}$ to be continuous over the open interval $(0, 2)$ Proof : Let $c \in (0, 2)$ $$\begin{equation} \begin{aligned} \lim_{x \to c} f(x) &= \lim_{x \to c} e^{x} \\ &= e^{c} \\ &= f(c)\\ \end{aligned} \end{equation}$$ $$Q.E.D$$ (This is how most proofs for continuity of functions over intervals are treated (at least in first-year Calculus courses) . But this way of proving continuity doesn't seem rigorous to me at all. It does nothing to explicitly state or show that $c$ is an element of the open interval $(0, 2)$ other than the fact that we assume it to be true. I could say 'Let $c \in \mathbb{R}$', and the proof would still hold, even though $f(x)$ is only defined for $c \in \mathbb{R^{+}}, \ \text{where} \ c \neq 0$. Just stating that $c \in (a, b)$ and showing that $lim_{x \to c} f(x) = f(c)$, doesn't seem like it proves anything to me, because $c$ doesn't necessarily need to be bounded within that interval, as we have done nothing to explicitly show $c \in (a, b)$ other than the fact that we stated it or assumed it. I mean using this sort of proof process, I could say let $c$ be an element of any arbitrary interval $R$, and by simply using direct substitution to show that $lim_{x \to c} f(x) = f(c)$, I would show that $f$ would be continuous over $R$, even if $f$ contained discontinuities over $R$, which this sort of proof process doesn't take into account at all. To re-emphazize, what I'm getting at, what I'm trying to say is I could always pick an arbitrary $c$ in any arbitrary open interval $I$, and using the Limit Laws (as used in this way in these sorts of proofs) always prove $f$ to be continuous on $I$ even if  $I \not\subset D$, i.e. the open interval lies outside of the domain of the function. As a last sub-question, in Real Analysis is there a more rigorous way in which continuity of a function $f$ over a closed interval $I$ is proven? Is there a reason in Real Analysis why continuity of functions over closed intervals is proved in this sort of way? Essentially I'm looking for a deeper understanding of proving continuity of functions over intervals, and I'm sure Real Analysis must have the answers to the questions I'm asking.","Given a function $f$ on a closed interval $I \subset \mathbb{R}$, where $I = [a,b]$, to prove continuity of $f$ over the interval $I$, what is generally done is the following. 1. We prove that $f$ is continuous at endpoint $a$ $$ \lim_{x \ \to \  a^{+}} f(x) = f(a)$$ 2. We prove that $f$ is continuous $\forall c \in (a, b)$ $$ \lim_{x \to c} f(x) = f(c) \ \ \ \ \text{,}\ \ \ \ \forall c \in (a, b)$$ 3. We prove that $f$ is continuousat endpoint $b$ $$ \lim_{x \ \to \  b^{-}} f(x) = f(b)$$ My question revolves around step 2 of this process. Usually all that is done to prove the continuity of $f$ over the open interval $(a, b)$, is (using the Limit Laws) a simple direct substitution of a variable $c$ denoting an arbitrary number in the open interval, which is basically substituting $c \in (a, b)$ into $f(x)$. But I don't see why this is mathematically rigorous? Initially I assumed something along the lines of Mathematical Induction would have been used to prove $f$ to be continuous $\forall c \in (a, b)$, but that doesn't seem to be the case. Why does just choosing an arbitrary $c$ to represent a number in the interval $(a, b)$, and showing that the chosen $c$ satisfies the given definition of continuity, prove that $f$ is continuous over $(a, b)$ ? I will give an example below to highlight what I'm asking. Example : Prove $f(x) = e^{x}$ to be continuous over the open interval $(0, 2)$ Proof : Let $c \in (0, 2)$ $$\begin{equation} \begin{aligned} \lim_{x \to c} f(x) &= \lim_{x \to c} e^{x} \\ &= e^{c} \\ &= f(c)\\ \end{aligned} \end{equation}$$ $$Q.E.D$$ (This is how most proofs for continuity of functions over intervals are treated (at least in first-year Calculus courses) . But this way of proving continuity doesn't seem rigorous to me at all. It does nothing to explicitly state or show that $c$ is an element of the open interval $(0, 2)$ other than the fact that we assume it to be true. I could say 'Let $c \in \mathbb{R}$', and the proof would still hold, even though $f(x)$ is only defined for $c \in \mathbb{R^{+}}, \ \text{where} \ c \neq 0$. Just stating that $c \in (a, b)$ and showing that $lim_{x \to c} f(x) = f(c)$, doesn't seem like it proves anything to me, because $c$ doesn't necessarily need to be bounded within that interval, as we have done nothing to explicitly show $c \in (a, b)$ other than the fact that we stated it or assumed it. I mean using this sort of proof process, I could say let $c$ be an element of any arbitrary interval $R$, and by simply using direct substitution to show that $lim_{x \to c} f(x) = f(c)$, I would show that $f$ would be continuous over $R$, even if $f$ contained discontinuities over $R$, which this sort of proof process doesn't take into account at all. To re-emphazize, what I'm getting at, what I'm trying to say is I could always pick an arbitrary $c$ in any arbitrary open interval $I$, and using the Limit Laws (as used in this way in these sorts of proofs) always prove $f$ to be continuous on $I$ even if  $I \not\subset D$, i.e. the open interval lies outside of the domain of the function. As a last sub-question, in Real Analysis is there a more rigorous way in which continuity of a function $f$ over a closed interval $I$ is proven? Is there a reason in Real Analysis why continuity of functions over closed intervals is proved in this sort of way? Essentially I'm looking for a deeper understanding of proving continuity of functions over intervals, and I'm sure Real Analysis must have the answers to the questions I'm asking.",,"['calculus', 'real-analysis', 'limits', 'continuity']"
55,$\displaystyle\lim _{x\to 0}\frac{\sqrt{1+x+x}-1}{\sqrt{1+x}-\sqrt{1-x}}$,,\displaystyle\lim _{x\to 0}\frac{\sqrt{1+x+x}-1}{\sqrt{1+x}-\sqrt{1-x}},"First I need to prove that this limit; $\displaystyle\lim _{x\to 0}\frac{\sqrt{1+x+x^2}-1}{\sqrt{1+x}-\sqrt{1-x}}$ converges, then I have to find its limit. Now I don't know how to prove that it converges (these epsilon proofs are still something that I'm trying to learn). And to actually find the limit; I tried to rewrite it by multiplying by $\displaystyle{\sqrt{1+x}+\sqrt{1-x}\over \sqrt{1+x}+\sqrt{1-x}}$ but I didn't get any further.. Edit: I know that it converges to $1/2$, but that's what wolfram alpha says :|","First I need to prove that this limit; $\displaystyle\lim _{x\to 0}\frac{\sqrt{1+x+x^2}-1}{\sqrt{1+x}-\sqrt{1-x}}$ converges, then I have to find its limit. Now I don't know how to prove that it converges (these epsilon proofs are still something that I'm trying to learn). And to actually find the limit; I tried to rewrite it by multiplying by $\displaystyle{\sqrt{1+x}+\sqrt{1-x}\over \sqrt{1+x}+\sqrt{1-x}}$ but I didn't get any further.. Edit: I know that it converges to $1/2$, but that's what wolfram alpha says :|",,"['real-analysis', 'analysis', 'limits']"
56,Show the following limit does not exist,Show the following limit does not exist,,"Let $f,g$ be two functions of two variables, which satisfies that: i) $f$ and $g$ are homogeneous of degree $n$, ii) $g(1,1)\neq0$ and $g(1,0)\neq0$, iii) $g(1,1)*f(1,0)\neq f(1,1)*g(1,0)$. Show that $Lim_{(x,y)\rightarrow(0,0)}\frac{f(x,y)}{g(x,y)}$ does not exist. I have been trying to proof it by Reductio ad absurdum but none of my attempts have solved the exercise. Any hint what to do?","Let $f,g$ be two functions of two variables, which satisfies that: i) $f$ and $g$ are homogeneous of degree $n$, ii) $g(1,1)\neq0$ and $g(1,0)\neq0$, iii) $g(1,1)*f(1,0)\neq f(1,1)*g(1,0)$. Show that $Lim_{(x,y)\rightarrow(0,0)}\frac{f(x,y)}{g(x,y)}$ does not exist. I have been trying to proof it by Reductio ad absurdum but none of my attempts have solved the exercise. Any hint what to do?",,"['analysis', 'limits']"
57,Finding a function based on its Derivative without Integrating,Finding a function based on its Derivative without Integrating,,"My question revolves around finding a function based on its derivative of the type below : Problem : The limit below represents the derivative of some real-valued function $f$ at some real-number $a$. State such an $f$ and $a$ in this case. $$\lim_{h \ \to \  0} \frac{\sqrt{9+h}-3}{h}$$ Now this type of problem is slightly unique. In general we can always find a function based on it's derivative by taking the indefinite integral of the derivative, however in this case we don't have the derivative in a general form, we only have the value for the derivative function at some point $a$, and there are a large number of $f$'s which can produce the value for the derivative at that point. Am I correct in saying this? This problem above is easily solvable, anyone can see already that the function is obviously going to be $f(x) = \sqrt{x}$, but the way I seem to solve it is a very heuristic method, which bothers me greatly (i.e. similar to guessing a function and working from there). I'm trying to find a methodical way of solving problems of this type, as the way I solve it (shown below) will definitely break down for harder examples. This is my solution : By the definition of a derivative : $$f'(x) = \lim_{h \to\ 0} \frac{f(x+h)-f(x)}{h}$$ In the above case we can see that $\ f(a+h) = \sqrt{9+h}\ $ and $\ f(a)=3 = \sqrt{9}$ This implies that $a = 9$ and $f(x) = \sqrt{x}$ or written more formally ($f : x \to \sqrt{x}, \ \forall x\in\mathbb{R^{+}}$) As you can see that is a very hap-hazard solution, and I would like to find a better way to solve problems of these types, but it eludes me. Is there a more methodical approach (or formal approach) to solving problems of this type, where we are given a limit representing the derivative of some real-valued function $f$ at some real-number $a$, and asked to find $f$ and $a$? If you have any other suggestions for solving these types of problems, please comment below.","My question revolves around finding a function based on its derivative of the type below : Problem : The limit below represents the derivative of some real-valued function $f$ at some real-number $a$. State such an $f$ and $a$ in this case. $$\lim_{h \ \to \  0} \frac{\sqrt{9+h}-3}{h}$$ Now this type of problem is slightly unique. In general we can always find a function based on it's derivative by taking the indefinite integral of the derivative, however in this case we don't have the derivative in a general form, we only have the value for the derivative function at some point $a$, and there are a large number of $f$'s which can produce the value for the derivative at that point. Am I correct in saying this? This problem above is easily solvable, anyone can see already that the function is obviously going to be $f(x) = \sqrt{x}$, but the way I seem to solve it is a very heuristic method, which bothers me greatly (i.e. similar to guessing a function and working from there). I'm trying to find a methodical way of solving problems of this type, as the way I solve it (shown below) will definitely break down for harder examples. This is my solution : By the definition of a derivative : $$f'(x) = \lim_{h \to\ 0} \frac{f(x+h)-f(x)}{h}$$ In the above case we can see that $\ f(a+h) = \sqrt{9+h}\ $ and $\ f(a)=3 = \sqrt{9}$ This implies that $a = 9$ and $f(x) = \sqrt{x}$ or written more formally ($f : x \to \sqrt{x}, \ \forall x\in\mathbb{R^{+}}$) As you can see that is a very hap-hazard solution, and I would like to find a better way to solve problems of these types, but it eludes me. Is there a more methodical approach (or formal approach) to solving problems of this type, where we are given a limit representing the derivative of some real-valued function $f$ at some real-number $a$, and asked to find $f$ and $a$? If you have any other suggestions for solving these types of problems, please comment below.",,"['calculus', 'real-analysis', 'integration', 'limits', 'derivatives']"
58,Considering the power series $\sum_{n=1}^\infty(-1)^{n-1}\frac{x^{2n+1}}{(2n+1)(2n-1)}$.,Considering the power series .,\sum_{n=1}^\infty(-1)^{n-1}\frac{x^{2n+1}}{(2n+1)(2n-1)},"Consider the power series $\sum_{n=1}^\infty(-1)^{n-1}\frac{x^{2n+1}}{(2n+1)(2n-1)}$. Find a closed form expression for all x which converge and hence evaluate $\sum_{n=1}^\infty\frac{(-1)^{n-1}}{(2n+1)(2n-1)}$. Attempt at the solution: The radius of convergence is 1. We can rewrite the summands by: \begin{eqnarray} \sum_{n=1}^\infty(-1)^n\frac{x^{2n+1}}{(2n+1)(2n-1)} &=& \frac{1}{2}\Big[(x^3-\frac{x^3}{3})  - (\frac{x^5}{3} - \frac{x^5}{5}) + (\frac{x^7}{5} - \frac{x^7}{7}) + \dots \Big]\\ &=& \frac{1}{2}\Big[(x^3  -\frac{x^5}{3}+ \frac{x^7}{5} + \dots ) + (\frac{-x^3}{3} + \frac{x^5}{5}-\frac{x^7}{7} +...)\Big]\\ &=& \frac{1}{2}\Big[x^2\int\frac{1}{1+x^2}dx + \int\frac{1}{1+x^2}dx-x\Big]\\ &=& \frac{x^2}{2}\arctan(x)+\frac{1}{2}\arctan(x) -\frac{x}{2} \end{eqnarray} Substituting $x=1$ then gives $\frac{\pi}{4}-\frac{1}{2}$ . The issue I have is two fold. Firstly, when dealing with evaluations at the boundary, term by term differentiation may not be valid. In particular, we used the fact that $\arctan(x) = x-\frac{x^3}{3} + ...$ by integrating power series for $\frac{1}{1+x^2}$, valid for |x|<1. This means that the arctan formula can only be guaranteed to hold within the interior (-1,1). What are the conditions needed to talk about power series validity at boundary points? (Abelian/Tauberian theorems came to mind at first, but the conditions in this problem weren't strong enough. Alternatively, I noted that uniform convergence of the terms meant that the limit function of $x-\frac{x^3}{3} + ...$ had to be continuous. So $\arctan(1) = \frac{\pi}{4}$ by continuous extension. Do correct me if I'm wrong. The other issue that I have not been able to justify is that of conditional convergence. Clearly, the arctan series is conditionally convergent at $x=1$. How do we justify the rearrangments carried out above then?","Consider the power series $\sum_{n=1}^\infty(-1)^{n-1}\frac{x^{2n+1}}{(2n+1)(2n-1)}$. Find a closed form expression for all x which converge and hence evaluate $\sum_{n=1}^\infty\frac{(-1)^{n-1}}{(2n+1)(2n-1)}$. Attempt at the solution: The radius of convergence is 1. We can rewrite the summands by: \begin{eqnarray} \sum_{n=1}^\infty(-1)^n\frac{x^{2n+1}}{(2n+1)(2n-1)} &=& \frac{1}{2}\Big[(x^3-\frac{x^3}{3})  - (\frac{x^5}{3} - \frac{x^5}{5}) + (\frac{x^7}{5} - \frac{x^7}{7}) + \dots \Big]\\ &=& \frac{1}{2}\Big[(x^3  -\frac{x^5}{3}+ \frac{x^7}{5} + \dots ) + (\frac{-x^3}{3} + \frac{x^5}{5}-\frac{x^7}{7} +...)\Big]\\ &=& \frac{1}{2}\Big[x^2\int\frac{1}{1+x^2}dx + \int\frac{1}{1+x^2}dx-x\Big]\\ &=& \frac{x^2}{2}\arctan(x)+\frac{1}{2}\arctan(x) -\frac{x}{2} \end{eqnarray} Substituting $x=1$ then gives $\frac{\pi}{4}-\frac{1}{2}$ . The issue I have is two fold. Firstly, when dealing with evaluations at the boundary, term by term differentiation may not be valid. In particular, we used the fact that $\arctan(x) = x-\frac{x^3}{3} + ...$ by integrating power series for $\frac{1}{1+x^2}$, valid for |x|<1. This means that the arctan formula can only be guaranteed to hold within the interior (-1,1). What are the conditions needed to talk about power series validity at boundary points? (Abelian/Tauberian theorems came to mind at first, but the conditions in this problem weren't strong enough. Alternatively, I noted that uniform convergence of the terms meant that the limit function of $x-\frac{x^3}{3} + ...$ had to be continuous. So $\arctan(1) = \frac{\pi}{4}$ by continuous extension. Do correct me if I'm wrong. The other issue that I have not been able to justify is that of conditional convergence. Clearly, the arctan series is conditionally convergent at $x=1$. How do we justify the rearrangments carried out above then?",,"['sequences-and-series', 'analysis', 'limits', 'power-series']"
59,Occurence of number $1$ in the sequence $a_n=2^n$,Occurence of number  in the sequence,1 a_n=2^n,"So I was just calculating the terms of the sequence $a_n=2^n$ for $n=1,2,...50$ and discovered that among the first fifty terms there is $31$ term that has number $1$ in itself (in base $10$, of course). It seems that as we go further in the sequence that the terms that contain $1$ in itself become more and more common. Let us denote by $m(n)$ the number of numbers in the set $\{2^1,...,2^n\}$ that contain number $1$ when written in base $10$ (so I calculated that $m(50)=31$). Can someone evaluate the limit $\lim_{n \to \infty} \frac {m(n)}{n}$?","So I was just calculating the terms of the sequence $a_n=2^n$ for $n=1,2,...50$ and discovered that among the first fifty terms there is $31$ term that has number $1$ in itself (in base $10$, of course). It seems that as we go further in the sequence that the terms that contain $1$ in itself become more and more common. Let us denote by $m(n)$ the number of numbers in the set $\{2^1,...,2^n\}$ that contain number $1$ when written in base $10$ (so I calculated that $m(50)=31$). Can someone evaluate the limit $\lim_{n \to \infty} \frac {m(n)}{n}$?",,"['limits', 'elementary-number-theory']"
60,limit of product exists and one limit exists,limit of product exists and one limit exists,,"Question is to check : If  $\lim_{n\rightarrow \infty}a_nb_n$ exists and $\lim_{n\rightarrow \infty}a_n$ exists implies $\lim_{n\rightarrow \infty}b_n$ exists. Considering $a_n=\frac{1}{n}$ and $b_n=n$ then we see that  $\lim_{n\rightarrow \infty}a_nb_n$ exists, equals to $1$  and $\lim_{n\rightarrow \infty}a_n$ exists and equals to $0$. In this case $\lim_{n\rightarrow \infty}b_n$  does not exists.. So, the answer to the question is Not always .. Now, what if $\lim_{n\rightarrow \infty}a_n$ exists and is non zero and $(b_n)$ is bounded? Suppose that $\lim_{n\rightarrow \infty}a_nb_n=M$ with  $\lim_{n\rightarrow \infty}a_n=P\neq 0$ and $|b_n|\leq A$ for all $n\in \mathbb{N}$. I claim that $\lim_{n\rightarrow \infty}b_n=\frac{M}{P}$ Consider $|b_n-\frac{M}{P}|$.. We estimate this. Given $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $|a_nb_n-M|<\epsilon$ and $|a_n-P|<\epsilon$ for all $n\geq N$. $$|b_n-\frac{M}{P}|=\frac{1}{P}|Pb_n-M|=\frac{1}{P}|Pb_n-a_nb_n+a_nb_n-M|\leq \frac{1}{P}|b_n||a_n-P|+\frac{1}{P}                                   \epsilon$$ As $(b_n)$ is bounded, we have for all $n\geq N$ $$|b_n-\frac{M}{P}|\leq \frac{1}{P}A\epsilon+\frac{1}{P}                                   \epsilon=\epsilon\left(\frac{1}{P}(A+1)\right)$$ Thus, we are done. I am just wondering if i can relax any of the conditions that i have assumed. Help me to know more about this.","Question is to check : If  $\lim_{n\rightarrow \infty}a_nb_n$ exists and $\lim_{n\rightarrow \infty}a_n$ exists implies $\lim_{n\rightarrow \infty}b_n$ exists. Considering $a_n=\frac{1}{n}$ and $b_n=n$ then we see that  $\lim_{n\rightarrow \infty}a_nb_n$ exists, equals to $1$  and $\lim_{n\rightarrow \infty}a_n$ exists and equals to $0$. In this case $\lim_{n\rightarrow \infty}b_n$  does not exists.. So, the answer to the question is Not always .. Now, what if $\lim_{n\rightarrow \infty}a_n$ exists and is non zero and $(b_n)$ is bounded? Suppose that $\lim_{n\rightarrow \infty}a_nb_n=M$ with  $\lim_{n\rightarrow \infty}a_n=P\neq 0$ and $|b_n|\leq A$ for all $n\in \mathbb{N}$. I claim that $\lim_{n\rightarrow \infty}b_n=\frac{M}{P}$ Consider $|b_n-\frac{M}{P}|$.. We estimate this. Given $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $|a_nb_n-M|<\epsilon$ and $|a_n-P|<\epsilon$ for all $n\geq N$. $$|b_n-\frac{M}{P}|=\frac{1}{P}|Pb_n-M|=\frac{1}{P}|Pb_n-a_nb_n+a_nb_n-M|\leq \frac{1}{P}|b_n||a_n-P|+\frac{1}{P}                                   \epsilon$$ As $(b_n)$ is bounded, we have for all $n\geq N$ $$|b_n-\frac{M}{P}|\leq \frac{1}{P}A\epsilon+\frac{1}{P}                                   \epsilon=\epsilon\left(\frac{1}{P}(A+1)\right)$$ Thus, we are done. I am just wondering if i can relax any of the conditions that i have assumed. Help me to know more about this.",,['real-analysis']
61,"Derivative with a ""mixed"" discontinuity","Derivative with a ""mixed"" discontinuity",,"I read that the derivative of a function can never have a ""jump"" discontinuity, but only essential discontinuity. My question is, can the derivative have a ""half essential and half jump"" discontinuity, where $\lim_{x\to a^-}f(x)$ does NOT exist, $\lim_{x\to a^+}f(x)$ DOES exist, and $\lim_{x\to a^+}f(x)\not= f(a)$ ?","I read that the derivative of a function can never have a ""jump"" discontinuity, but only essential discontinuity. My question is, can the derivative have a ""half essential and half jump"" discontinuity, where $\lim_{x\to a^-}f(x)$ does NOT exist, $\lim_{x\to a^+}f(x)$ DOES exist, and $\lim_{x\to a^+}f(x)\not= f(a)$ ?",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
62,Technical challenge: Limit of von-Mises distribution approximates normal. How to take the limit?,Technical challenge: Limit of von-Mises distribution approximates normal. How to take the limit?,,"Background: In psychophysics or the study of ant navigation it's important to represent random variables on a circle. The most popular distribution for doing so is the von-Mises distribution (the wrapped Gaussian is too messy to work with). This is the von-Mises distribution for a random variable on a circle, $\theta \in [-\pi,\pi)$: \begin{align}  p(\theta;\mu,\kappa) = \frac{e^{\kappa cos(\theta - \mu)}}{\int_{-\pi}^\pi e^{\kappa cos(\theta)} \text{d}\theta} = \frac{e^{\kappa cos(\theta - \mu)}}{2 I_0(\kappa)}  , \end{align} where $\kappa$ is the concentration parameter (have the inverse variance $\kappa \sim \sigma^{-2}$ in mind), $\mu$ the mean and the normalisation can be expressed in terms of the modified Bessel function of the first kind, $I_0(\kappa)$. Problem statement: Wikipedia ( https://en.wikipedia.org/wiki/Von_Mises_distribution ) claims that \begin{align}  \lim_{\kappa \rightarrow \infty} p(\theta;\mu,\kappa) = \text{Normal}(\theta; \mu, \kappa^{-1/2}). \end{align} (The limit is not a rigorous mathematical limit but more a physicist-way of saying: expand the expression and throw higher-order terms away). It makes sense: If the distribution is highly concentrated, we can forget about the circularity and just approximate the von-Mises as normal. Mathematically it makes intuitive sense, too. The argument of the exponential ranges between $\pm \kappa$, being 0 always at $\theta = \pm \pi/2$. At the same time, the normalisation constant grows large such that the whole distribution is essentially divided by a very large number and tends to zero everywhere except around it's mode. Around the mode, the cosine can be approximated by $\cos(\epsilon) = 1- \epsilon^2/2$, yielding a nice normal distribution. Despite this, I find it incredibly hard to show this! Can anyone help me with this technical challenge? My (unsuccessful) approach: I've tried to use Taylor expansion for the cosine and the following series representations ( http://mhtlab.uwaterloo.ca/courses/me755/web_chap4.pdf ) of the Bessel function for large $\kappa$: \begin{align} I_0(\kappa) = \sum_{n}^\infty \frac{(\kappa/2)^{2n}}{n!\Gamma(n+1)} \approx \frac{e^\kappa}{\sqrt{2\pi \kappa}}(1 + \frac{1}{8\kappa} + \mathcal{O}(\kappa^{-2})), \end{align} however, it didn't get me anywhere. Does anyone have any ideas how to attack this problem? Any help is much appreciated!","Background: In psychophysics or the study of ant navigation it's important to represent random variables on a circle. The most popular distribution for doing so is the von-Mises distribution (the wrapped Gaussian is too messy to work with). This is the von-Mises distribution for a random variable on a circle, $\theta \in [-\pi,\pi)$: \begin{align}  p(\theta;\mu,\kappa) = \frac{e^{\kappa cos(\theta - \mu)}}{\int_{-\pi}^\pi e^{\kappa cos(\theta)} \text{d}\theta} = \frac{e^{\kappa cos(\theta - \mu)}}{2 I_0(\kappa)}  , \end{align} where $\kappa$ is the concentration parameter (have the inverse variance $\kappa \sim \sigma^{-2}$ in mind), $\mu$ the mean and the normalisation can be expressed in terms of the modified Bessel function of the first kind, $I_0(\kappa)$. Problem statement: Wikipedia ( https://en.wikipedia.org/wiki/Von_Mises_distribution ) claims that \begin{align}  \lim_{\kappa \rightarrow \infty} p(\theta;\mu,\kappa) = \text{Normal}(\theta; \mu, \kappa^{-1/2}). \end{align} (The limit is not a rigorous mathematical limit but more a physicist-way of saying: expand the expression and throw higher-order terms away). It makes sense: If the distribution is highly concentrated, we can forget about the circularity and just approximate the von-Mises as normal. Mathematically it makes intuitive sense, too. The argument of the exponential ranges between $\pm \kappa$, being 0 always at $\theta = \pm \pi/2$. At the same time, the normalisation constant grows large such that the whole distribution is essentially divided by a very large number and tends to zero everywhere except around it's mode. Around the mode, the cosine can be approximated by $\cos(\epsilon) = 1- \epsilon^2/2$, yielding a nice normal distribution. Despite this, I find it incredibly hard to show this! Can anyone help me with this technical challenge? My (unsuccessful) approach: I've tried to use Taylor expansion for the cosine and the following series representations ( http://mhtlab.uwaterloo.ca/courses/me755/web_chap4.pdf ) of the Bessel function for large $\kappa$: \begin{align} I_0(\kappa) = \sum_{n}^\infty \frac{(\kappa/2)^{2n}}{n!\Gamma(n+1)} \approx \frac{e^\kappa}{\sqrt{2\pi \kappa}}(1 + \frac{1}{8\kappa} + \mathcal{O}(\kappa^{-2})), \end{align} however, it didn't get me anywhere. Does anyone have any ideas how to attack this problem? Any help is much appreciated!",,"['limits', 'taylor-expansion', 'normal-distribution', 'approximation', 'bessel-functions']"
63,The $n$th integral of $\ln(x)$ and fractional derivatives,The th integral of  and fractional derivatives,n \ln(x),"For a related question, I need to know the $n$th integral of $\ln(x)$ and the fractional derivative of $\ln(x)$. A break down of how fractional derivatives may be found on the Wikipedia. In particular, I need to calculate $\frac{d^{1/2}}{dx^{1/2}}\ln(x)$ and $\frac{d^{-n}}{dx^{-n}}\ln(x)$ where that is the $n$th integral of $\ln(x)$. The fractional derivative in this scenario is given by: $$\frac{d^{1/2}}{dx^{1/2}}\ln(x)=\lim_{h\to0}\frac{(-1)^{1/2}}{h^{1/2}}\sum_{0\le m<\infty}\frac{\Gamma(1.5)}{\Gamma(m+1)\Gamma(1.5-m+1)}\ln(x+mh)$$ It is rather difficult to take the limit from my skills, so I was hoping someone could solve it.  (I do accept power series answers or anything the cannot be written in easy closed form) Secondly, I have attempted to find the $n$th integral of $\ln(x)$ and this is what I found $$\frac{d^{-n}}{dx^{-n}}\ln(x)=\frac{x^n[\ln(x)-\sum_{i=1}^{n}\frac1i]}{\Gamma(n+1)}$$ Two problems about this formula, I'm unsure if it works fully, and I need it to work for non whole $n$. Thanks for your time and efforts.","For a related question, I need to know the $n$th integral of $\ln(x)$ and the fractional derivative of $\ln(x)$. A break down of how fractional derivatives may be found on the Wikipedia. In particular, I need to calculate $\frac{d^{1/2}}{dx^{1/2}}\ln(x)$ and $\frac{d^{-n}}{dx^{-n}}\ln(x)$ where that is the $n$th integral of $\ln(x)$. The fractional derivative in this scenario is given by: $$\frac{d^{1/2}}{dx^{1/2}}\ln(x)=\lim_{h\to0}\frac{(-1)^{1/2}}{h^{1/2}}\sum_{0\le m<\infty}\frac{\Gamma(1.5)}{\Gamma(m+1)\Gamma(1.5-m+1)}\ln(x+mh)$$ It is rather difficult to take the limit from my skills, so I was hoping someone could solve it.  (I do accept power series answers or anything the cannot be written in easy closed form) Secondly, I have attempted to find the $n$th integral of $\ln(x)$ and this is what I found $$\frac{d^{-n}}{dx^{-n}}\ln(x)=\frac{x^n[\ln(x)-\sum_{i=1}^{n}\frac1i]}{\Gamma(n+1)}$$ Two problems about this formula, I'm unsure if it works fully, and I need it to work for non whole $n$. Thanks for your time and efforts.",,"['calculus', 'limits', 'summation', 'logarithms', 'fractional-calculus']"
64,Why is this limit not zero?,Why is this limit not zero?,,"$$f(x,y) = \frac{y^3(x^2+2xy-y^2)}{(x^2+y^2)^2}$$ I am to calculate the limit of this function as it approaches the origin along the path $y=x^{2/3}$. I keep getting $0$ for this but that is not supposed to be the answer. Why is  this limit not equal to $0$? Edit 1: I left out a few aspects of the problem which might be more important than I thought. The function I posted is actually the partial derivative with respect to x of another function. And I am to find the limit as those points are plugged into the partial and go to 0. Does that change anything in this question? Edit 2: What happens if we approach the origin from the path y=0?  This would mean it is (x,0) approaching (0,0). Is this limit 0 as well?","$$f(x,y) = \frac{y^3(x^2+2xy-y^2)}{(x^2+y^2)^2}$$ I am to calculate the limit of this function as it approaches the origin along the path $y=x^{2/3}$. I keep getting $0$ for this but that is not supposed to be the answer. Why is  this limit not equal to $0$? Edit 1: I left out a few aspects of the problem which might be more important than I thought. The function I posted is actually the partial derivative with respect to x of another function. And I am to find the limit as those points are plugged into the partial and go to 0. Does that change anything in this question? Edit 2: What happens if we approach the origin from the path y=0?  This would mean it is (x,0) approaching (0,0). Is this limit 0 as well?",,"['limits', 'multivariable-calculus']"
65,Limit of sum of terms containing binomial coefficients,Limit of sum of terms containing binomial coefficients,,"$$\lim_{n \to \infty} \sum_{k=0}^n \frac{n \choose k}{k2^n+n}$$ The result is $0$. The $n$ from the denominator can be ignored. If not for the $k$ at the denominator, the result would be $1$, but I can not find the right inequality.","$$\lim_{n \to \infty} \sum_{k=0}^n \frac{n \choose k}{k2^n+n}$$ The result is $0$. The $n$ from the denominator can be ignored. If not for the $k$ at the denominator, the result would be $1$, but I can not find the right inequality.",,"['limits', 'summation', 'binomial-coefficients']"
66,Show that $f_{n}^2(x)$ does not converge in $D^1({\Omega})$,Show that  does not converge in,f_{n}^2(x) D^1({\Omega}),"Let $$ f_n(x) = \left\{ \begin{array}{ll}          n & \mbox{if $0 \lt x \lt \frac{1}{n}$};\\         0 & \mbox{otherwise}.\end{array} \right. \ $$ I have to show that $\lim_{n \to \infty}f_n(x)$ exists in terms of distributions $\lim_{n \to \infty}f_n^2(x)$ doesn't exist in terms of distributions. $\lim_{n \to \infty}(f_n^2(x)-n\delta)$  exists in terms of distributions For $(1)$ $$n\int_{0}^{\frac{1}{n}}\phi(x) dx \to \phi(0) , \text{for},  n \to \infty$$ For $(2)$ $$n^2\int_{0}^{\frac{1}{n}}\phi(x) dx $$ Now there exists a $C^{\infty}$ cut off function such that $\phi(x) =1$ for $x \in (0,\frac{1}{n+1})$ and $\phi(x)=0, x \gt \frac{1}{n}$. For this particular choice of $\phi$ this integral goes to $\infty$. For $(3)$ This will be  $$\int_{-n}^{\frac{1}{n}} -(n\delta)\phi(x) dx +(n^2-n\delta)\int_{0}^{\frac{1}{n}}\phi(x) dx+\int_{\frac{1}{n}}^{n} -(n\delta)\phi(x) dx $$ I am unable to see where this would converge. Thanks for the help!!","Let $$ f_n(x) = \left\{ \begin{array}{ll}          n & \mbox{if $0 \lt x \lt \frac{1}{n}$};\\         0 & \mbox{otherwise}.\end{array} \right. \ $$ I have to show that $\lim_{n \to \infty}f_n(x)$ exists in terms of distributions $\lim_{n \to \infty}f_n^2(x)$ doesn't exist in terms of distributions. $\lim_{n \to \infty}(f_n^2(x)-n\delta)$  exists in terms of distributions For $(1)$ $$n\int_{0}^{\frac{1}{n}}\phi(x) dx \to \phi(0) , \text{for},  n \to \infty$$ For $(2)$ $$n^2\int_{0}^{\frac{1}{n}}\phi(x) dx $$ Now there exists a $C^{\infty}$ cut off function such that $\phi(x) =1$ for $x \in (0,\frac{1}{n+1})$ and $\phi(x)=0, x \gt \frac{1}{n}$. For this particular choice of $\phi$ this integral goes to $\infty$. For $(3)$ This will be  $$\int_{-n}^{\frac{1}{n}} -(n\delta)\phi(x) dx +(n^2-n\delta)\int_{0}^{\frac{1}{n}}\phi(x) dx+\int_{\frac{1}{n}}^{n} -(n\delta)\phi(x) dx $$ I am unable to see where this would converge. Thanks for the help!!",,"['functional-analysis', 'limits', 'distribution-theory']"
67,Solving a Limit with L'Hospital's Rule with 2 Unknown Constants,Solving a Limit with L'Hospital's Rule with 2 Unknown Constants,,"I am trying to find this limit: $$ \lim_{n \to \infty} \frac{(\log_{2}n)^k}{n^p} $$ where $k$ and $p$ are constants and $k \geq 1$, $p \gt 0$. The limit equates to $0$, but I need someone to help explain this derivation. Thank you.","I am trying to find this limit: $$ \lim_{n \to \infty} \frac{(\log_{2}n)^k}{n^p} $$ where $k$ and $p$ are constants and $k \geq 1$, $p \gt 0$. The limit equates to $0$, but I need someone to help explain this derivation. Thank you.",,"['calculus', 'limits']"
68,Computing the limit of an integral,Computing the limit of an integral,,"Consider the following integral $$ \int_{-\infty}^{\infty}f(t) K(\frac{a-t}{h})dt $$ where (1) $h>0$, $a \in \mathbb{R}$ (2) $f:\mathbb{R}\rightarrow[0,\infty)$ is such that $\int_{-\infty}^{\infty}f(t)dt=1$ (3) For fixed $a$ and $h$, the map $K:\mathbb{R}\rightarrow[0,\infty)$ is such that $\int_{-\infty}^{\infty}K(\frac{a-t}{h})dt<\infty$ (4) For fixed $a$ and $t$, $\lim_{h\rightarrow 0}K(\frac{a-t}{h})= \begin{cases} 1 \text{ if $t=a$}\\ 0 \text{ otherwise} \end{cases}$ Could you help me to show that  $$ \lim_{h\rightarrow 0} \int_{-\infty}^{\infty}f(t) K(\frac{a-t}{h})dt=f(a) $$ also stating under which sufficient conditions? The difficult part for me is bringing the limit inside the integral. Any hint would be really appreciated","Consider the following integral $$ \int_{-\infty}^{\infty}f(t) K(\frac{a-t}{h})dt $$ where (1) $h>0$, $a \in \mathbb{R}$ (2) $f:\mathbb{R}\rightarrow[0,\infty)$ is such that $\int_{-\infty}^{\infty}f(t)dt=1$ (3) For fixed $a$ and $h$, the map $K:\mathbb{R}\rightarrow[0,\infty)$ is such that $\int_{-\infty}^{\infty}K(\frac{a-t}{h})dt<\infty$ (4) For fixed $a$ and $t$, $\lim_{h\rightarrow 0}K(\frac{a-t}{h})= \begin{cases} 1 \text{ if $t=a$}\\ 0 \text{ otherwise} \end{cases}$ Could you help me to show that  $$ \lim_{h\rightarrow 0} \int_{-\infty}^{\infty}f(t) K(\frac{a-t}{h})dt=f(a) $$ also stating under which sufficient conditions? The difficult part for me is bringing the limit inside the integral. Any hint would be really appreciated",,"['integration', 'functional-analysis', 'limits', 'special-functions', 'distribution-theory']"
69,Find the limit of $\lim_n a_n$,Find the limit of,\lim_n a_n,"Let , $\{a_n\}$ be a sequence of real numbers such that $$\lim_n \left|a_n+3\left(\frac{n-2}{n}\right)^n\right|^{1/n}=\frac{3}{5}.$$Compute $\displaystyle \lim_na_n$. I have no idea about this..Please help.","Let , $\{a_n\}$ be a sequence of real numbers such that $$\lim_n \left|a_n+3\left(\frac{n-2}{n}\right)^n\right|^{1/n}=\frac{3}{5}.$$Compute $\displaystyle \lim_na_n$. I have no idea about this..Please help.",,['real-analysis']
70,Limit of the floor function of $\frac{x}{\sin(x)}$,Limit of the floor function of,\frac{x}{\sin(x)},Alright this looks like a very simple problem at the first go. I need to find $$\lim_{x\rightarrow0^+} { \left\lfloor{\frac{x}{\sin (x)}}\right\rfloor}$$ So since I know the inner function's graph beforehand I know the answer will be 1. But now here's my problem.When I'm trying to find which function $x$ or $\sin(x)$ is greater when $x \rightarrow 0^+$ I take a function $g(x)=x-\sin(x)$ and find its derivative.So $g'(x)=1-\cos(x)$.Now when I find $g'(x \rightarrow 0^+)$ I get $0$ ! So I'm not being able to prove that $g(x)$ is increasing when $x$ is slightly greater than $0$.And thus I can't prove that $x>sin(x)$ when $x$ is slighty greater than $0$ and neither can I prove $\frac{x}{\sin (x)}>1$.Where am I going wrong?,Alright this looks like a very simple problem at the first go. I need to find $$\lim_{x\rightarrow0^+} { \left\lfloor{\frac{x}{\sin (x)}}\right\rfloor}$$ So since I know the inner function's graph beforehand I know the answer will be 1. But now here's my problem.When I'm trying to find which function $x$ or $\sin(x)$ is greater when $x \rightarrow 0^+$ I take a function $g(x)=x-\sin(x)$ and find its derivative.So $g'(x)=1-\cos(x)$.Now when I find $g'(x \rightarrow 0^+)$ I get $0$ ! So I'm not being able to prove that $g(x)$ is increasing when $x$ is slightly greater than $0$.And thus I can't prove that $x>sin(x)$ when $x$ is slighty greater than $0$ and neither can I prove $\frac{x}{\sin (x)}>1$.Where am I going wrong?,,"['calculus', 'limits']"
71,Limit $\lim_{x \to 0} \dfrac{\sin(1-\cos x)}{ x} $,Limit,\lim_{x \to 0} \dfrac{\sin(1-\cos x)}{ x} ,"$$\lim_{x \to 0} \dfrac{\sin(1-\cos x)}{ x} $$ What is wrong with this argument: as $x$ approaches zero, both $x$ and $(1-\cos x)$ approaches $0$. So the limit is $1$ . How can we prove that they approaches zero at same rate? This is not about solving the limit because I already solved it but about the rate of both functions going to zero . m referring to $$\lim_{x \to 0} \dfrac{\sin x}{ x} =1 $$","$$\lim_{x \to 0} \dfrac{\sin(1-\cos x)}{ x} $$ What is wrong with this argument: as $x$ approaches zero, both $x$ and $(1-\cos x)$ approaches $0$. So the limit is $1$ . How can we prove that they approaches zero at same rate? This is not about solving the limit because I already solved it but about the rate of both functions going to zero . m referring to $$\lim_{x \to 0} \dfrac{\sin x}{ x} =1 $$",,"['calculus', 'real-analysis']"
72,"Calculating the limit $\lim_{x \to \infty } \left( \frac{a-1+b ^{ \frac{1}{x} } }{a} \right) ^{x}$ for $a>0, b>0$ [closed]",Calculating the limit  for  [closed],"\lim_{x \to \infty } \left( \frac{a-1+b ^{ \frac{1}{x} } }{a} \right) ^{x} a>0, b>0","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If $a>0, b>0$ then what is the limit $$\lim_{x \to \infty  } \left(  \frac{a-1+b ^{ \frac{1}{x} } }{a} \right)  ^{x}$$ I tried putting $y=\frac{1}{x}$ but it's not working.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If $a>0, b>0$ then what is the limit $$\lim_{x \to \infty  } \left(  \frac{a-1+b ^{ \frac{1}{x} } }{a} \right)  ^{x}$$ I tried putting $y=\frac{1}{x}$ but it's not working.",,['limits']
73,What does a triangle become?,What does a triangle become?,,"I'll get to the point. Imagine a right triangle in 2D. If you move one of the points (not the 90 degree vertex) very far away, then the corresponding angle will become smaller as the point moves further away. The 90 degree angle will stay constant. The other angle will become bigger, approaching 90, keeping the total angle at 180, by definition. What happens of the point is at ""infinity,"" will you be left with two 90 degree angles? This is impossible as those sides would be parallel, and therefore ending up with a seemingly impossible shape (it can't be a triangle anymore). This similar problem can be applied to different shapes and things, as well. Thank you very much!","I'll get to the point. Imagine a right triangle in 2D. If you move one of the points (not the 90 degree vertex) very far away, then the corresponding angle will become smaller as the point moves further away. The 90 degree angle will stay constant. The other angle will become bigger, approaching 90, keeping the total angle at 180, by definition. What happens of the point is at ""infinity,"" will you be left with two 90 degree angles? This is impossible as those sides would be parallel, and therefore ending up with a seemingly impossible shape (it can't be a triangle anymore). This similar problem can be applied to different shapes and things, as well. Thank you very much!",,"['geometry', 'limits', 'triangles']"
74,How to determine the limit of a sequence in a metric space,How to determine the limit of a sequence in a metric space,,"If I'm trying to prove that a Metric space $(M,d)$ is not complete I have to find a Cauchy a sequence that doesn't converge in $M$. Using the following Metric Space as an example: $M = \{  x \in \mathbb{R}^2 : ||x||<1 \}$ $d(x,y) = 2-||x||-||y||$ if $x \neq y$, and $0$ otherwise I know that the point $(1,1)$ lies outside this space, as $||(1,1)|| = \sqrt{2}  >1 $ So now I want to see if a sequence exists with this limit point. I figured, in the usual metric the sequence $(\frac{\sqrt{2}}{2}-\frac{1}{n},\frac{\sqrt{2}}{2}-\frac{1}{n}$ would converge to $(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$ How would I check to see if the limit point is the same given the above metric $d$? Or even better, how would I determine exactly what the limit of this sequence is given the above metric $d$.","If I'm trying to prove that a Metric space $(M,d)$ is not complete I have to find a Cauchy a sequence that doesn't converge in $M$. Using the following Metric Space as an example: $M = \{  x \in \mathbb{R}^2 : ||x||<1 \}$ $d(x,y) = 2-||x||-||y||$ if $x \neq y$, and $0$ otherwise I know that the point $(1,1)$ lies outside this space, as $||(1,1)|| = \sqrt{2}  >1 $ So now I want to see if a sequence exists with this limit point. I figured, in the usual metric the sequence $(\frac{\sqrt{2}}{2}-\frac{1}{n},\frac{\sqrt{2}}{2}-\frac{1}{n}$ would converge to $(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$ How would I check to see if the limit point is the same given the above metric $d$? Or even better, how would I determine exactly what the limit of this sequence is given the above metric $d$.",,"['limits', 'convergence-divergence', 'metric-spaces', 'cauchy-sequences', 'complete-spaces']"
75,Limit of the following function,Limit of the following function,,"For a function $g(t)$ with $\lim\limits_{t\rightarrow\infty}g(t)=G$, what is the limit $\lim\limits_{t\rightarrow\infty}e^{-t}\int g(t)e^{3t}dt$? Edit: I've thought about using user26977's tip about L'Hopital and got $\lim\limits_{t\rightarrow\infty}e^{-t}\int g(t)e^{3t}dt=\lim\limits_{t\rightarrow\infty}e^{-t}g(t)e^{3t}=\lim\limits_{t\rightarrow\infty}e^{2t}g(t).$ Is this $\infty$? Is my reasoning here correct?","For a function $g(t)$ with $\lim\limits_{t\rightarrow\infty}g(t)=G$, what is the limit $\lim\limits_{t\rightarrow\infty}e^{-t}\int g(t)e^{3t}dt$? Edit: I've thought about using user26977's tip about L'Hopital and got $\lim\limits_{t\rightarrow\infty}e^{-t}\int g(t)e^{3t}dt=\lim\limits_{t\rightarrow\infty}e^{-t}g(t)e^{3t}=\lim\limits_{t\rightarrow\infty}e^{2t}g(t).$ Is this $\infty$? Is my reasoning here correct?",,"['calculus', 'real-analysis', 'limits']"
76,Is this equality $\lim_{x \to \infty} \int_0^x \frac{t^2}{2(e^t-1)}\mathrm{d}t=\lim_{n \to \infty}\sum_{k=1}^n \frac{1}{k^3}$ true?,Is this equality  true?,\lim_{x \to \infty} \int_0^x \frac{t^2}{2(e^t-1)}\mathrm{d}t=\lim_{n \to \infty}\sum_{k=1}^n \frac{1}{k^3},"Using a little program in Python, it looks true for at least two hundred digits after the comma, but I have absolutely no idea, how to begin. Any hint sould be appreciate. $$\lim_{x \to \infty} \int_0^x \frac{t^2}{2(e^t-1)}\mathrm{d}t=\lim_{n \to \infty}\sum_{k=1}^n \frac{1}{k^3}$$ It looks not very difficult, I tried a integration by part but it looks not to be the better way to compute it. I'm stuck there. I should be glad, thank you in advance!","Using a little program in Python, it looks true for at least two hundred digits after the comma, but I have absolutely no idea, how to begin. Any hint sould be appreciate. $$\lim_{x \to \infty} \int_0^x \frac{t^2}{2(e^t-1)}\mathrm{d}t=\lim_{n \to \infty}\sum_{k=1}^n \frac{1}{k^3}$$ It looks not very difficult, I tried a integration by part but it looks not to be the better way to compute it. I'm stuck there. I should be glad, thank you in advance!",,"['calculus', 'integration', 'limits', 'improper-integrals']"
77,Find $\lim\limits_{x\to 0}\frac{e^{x^2}-\cos x^{\sqrt{2}}}{x^2}$,Find,\lim\limits_{x\to 0}\frac{e^{x^2}-\cos x^{\sqrt{2}}}{x^2},Find $\lim\limits_{x\to 0}\frac{e^{x^2}-\cos x^{\sqrt{2}}}{x^2}$ Using Taylor series: $$\lim\limits_{x\to 0}\frac{e^{x^2}-\cos x^{\sqrt{2}}}{x^2}=\lim\limits_{x\to 0}\frac{(1+x^2+O(x^{2n}))-(1-\frac{x^{2\sqrt{2}}}{2}+O(x^{\sqrt{2}(2n+1)})}{x^2}=\lim\limits_{x\to 0}\frac{\frac{2x^2+x^{2\sqrt{2}}}{2}}{x^2}=\lim\limits_{x\to 0}\frac{2x^2+x^{2\sqrt{2}}}{2x^2}$$ How to evaluate this limit?,Find $\lim\limits_{x\to 0}\frac{e^{x^2}-\cos x^{\sqrt{2}}}{x^2}$ Using Taylor series: $$\lim\limits_{x\to 0}\frac{e^{x^2}-\cos x^{\sqrt{2}}}{x^2}=\lim\limits_{x\to 0}\frac{(1+x^2+O(x^{2n}))-(1-\frac{x^{2\sqrt{2}}}{2}+O(x^{\sqrt{2}(2n+1)})}{x^2}=\lim\limits_{x\to 0}\frac{\frac{2x^2+x^{2\sqrt{2}}}{2}}{x^2}=\lim\limits_{x\to 0}\frac{2x^2+x^{2\sqrt{2}}}{2x^2}$$ How to evaluate this limit?,,"['calculus', 'limits', 'convergence-divergence']"
78,"What about $\lim_{n\to\infty}\frac{\sum_{k=1}^n s_k\mu(k)}{n}$, for the zeros of Dirichlet eta function $s_k=1+\frac{2\pi k}{\log 2}i$ with $k\geq 1$?","What about , for the zeros of Dirichlet eta function  with ?",\lim_{n\to\infty}\frac{\sum_{k=1}^n s_k\mu(k)}{n} s_k=1+\frac{2\pi k}{\log 2}i k\geq 1,"Let for integers $k\geq 1$ the corresponding zeros of Dirichlet eta function of the form  $$s_k=1+\frac{2\pi k}{\log 2}i,$$ then we can consider the following puzzle, when we multiply previous equation by $\mu(k)$, its corresponding values from Mobius function : $$\sum_{k=1}^n s_k\mu(k)=M(n)+\frac{2\pi i}{\log 2}\sum_{k=1}^n\mu(k)k,$$ where $M(n)$ is the evaluation of Mertens function at $x=n$.  Thus  $$\frac{\sum_{k=1}^n s_k\mu(k)}{n}=\frac{M(n)}{n}+\frac{2\pi i}{\log 2}\frac{\sum_{k=1}^n\mu(k)k}{n}.$$ Now I know that Prime Number Theorem is equivalent to $M(x)=o(x)$, thus exists $\lim_{n\to\infty}\frac{M(n)}{n}$, and this value is $0$. But Question. What about $$\lim_{n\to\infty}\frac{\sum_{k=1}^n s_k\mu(k)}{n}?$$ This is, you know how analyse the convergence and/or how compute the third limit involving $\frac{\sum_{k=1}^n\mu(k)k}{n}$. Thanks in advance.","Let for integers $k\geq 1$ the corresponding zeros of Dirichlet eta function of the form  $$s_k=1+\frac{2\pi k}{\log 2}i,$$ then we can consider the following puzzle, when we multiply previous equation by $\mu(k)$, its corresponding values from Mobius function : $$\sum_{k=1}^n s_k\mu(k)=M(n)+\frac{2\pi i}{\log 2}\sum_{k=1}^n\mu(k)k,$$ where $M(n)$ is the evaluation of Mertens function at $x=n$.  Thus  $$\frac{\sum_{k=1}^n s_k\mu(k)}{n}=\frac{M(n)}{n}+\frac{2\pi i}{\log 2}\frac{\sum_{k=1}^n\mu(k)k}{n}.$$ Now I know that Prime Number Theorem is equivalent to $M(x)=o(x)$, thus exists $\lim_{n\to\infty}\frac{M(n)}{n}$, and this value is $0$. But Question. What about $$\lim_{n\to\infty}\frac{\sum_{k=1}^n s_k\mu(k)}{n}?$$ This is, you know how analyse the convergence and/or how compute the third limit involving $\frac{\sum_{k=1}^n\mu(k)k}{n}$. Thanks in advance.",,"['limits', 'roots']"
79,How do we study the limit $\lim_{n\to\infty}\frac{n(n-1)...(n-k_n+1)}{n^{k_n}}$ wrt the relationship of $n$ and $k_n$,How do we study the limit  wrt the relationship of  and,\lim_{n\to\infty}\frac{n(n-1)...(n-k_n+1)}{n^{k_n}} n k_n,"I think that when $k_n$ is big enough the limit will be 0, and when $k_n$ is small enough the limit will be going to 1. How do we make a formal analysis?","I think that when $k_n$ is big enough the limit will be 0, and when $k_n$ is small enough the limit will be going to 1. How do we make a formal analysis?",,"['calculus', 'limits']"
80,Series' convergence - making my ideas formal,Series' convergence - making my ideas formal,,"Find the collection of all $x \in \mathbb{R}$ for which the series $\displaystyle \sum_{n=1}^\infty (3^n + n)\cdot x^n$ converges. My first step was the use the ratio test: $$ \lim_{n \to \infty} \dfrac{(3^{n+1}+n+1) \cdot |x|^{n+1}}{(3n+n) \cdot |x|^n} = \lim_{n \to \infty} \dfrac{3^{n+1}|x|+n|x|+|x| }{3^n+n} = \lim_{n \to \infty} ( \dfrac{3^{n+1}|x|}{3^n+n} + \dfrac{n|x|}{3^n+n} + \dfrac{|x|}{3^n+n}) $$ $$ = \lim_{n \to \infty} (\dfrac{3|x|}{1+\dfrac{n}{3^n}} + \dfrac{|x|}{\dfrac{3^n}{n} + 1} + \dfrac{|x|}{3^n + n}) = 3|x| + 0 + 0 = 3|x| $$ So we want $3|x| < 1$, i.e. $|x| < \dfrac{1}{3}$. But we have to also check the points $x=\dfrac{1}{3}, -\dfrac{1}{3}$, where the limit equals $1$. For $x=\dfrac{1}{3}$ we have $\displaystyle \sum_{n=1}^\infty (3^n + n)\cdot (\dfrac{1}{3})^n = \displaystyle \sum_{n=1}^\infty (1 + n \cdot \dfrac{1}{3}^n)  $ which obviously diverges. For $x=-\dfrac{1}{3}$ we have $\displaystyle \sum_{n=1}^\infty (3^n + n)\cdot (-\dfrac{1}{3})^n = \displaystyle \sum_{n=1}^\infty ((-1)^{n} + n \cdot (-\dfrac{1}{3})^n)$. Now I know this diverges because $(-1)^n$ and $(-\dfrac{1}{3})^n$ have alternating coefficients. But is there a theorem that I can use here? So we conclude that $|x| < \dfrac{1}{3}$, but is the above work enough to show it conclusively?","Find the collection of all $x \in \mathbb{R}$ for which the series $\displaystyle \sum_{n=1}^\infty (3^n + n)\cdot x^n$ converges. My first step was the use the ratio test: $$ \lim_{n \to \infty} \dfrac{(3^{n+1}+n+1) \cdot |x|^{n+1}}{(3n+n) \cdot |x|^n} = \lim_{n \to \infty} \dfrac{3^{n+1}|x|+n|x|+|x| }{3^n+n} = \lim_{n \to \infty} ( \dfrac{3^{n+1}|x|}{3^n+n} + \dfrac{n|x|}{3^n+n} + \dfrac{|x|}{3^n+n}) $$ $$ = \lim_{n \to \infty} (\dfrac{3|x|}{1+\dfrac{n}{3^n}} + \dfrac{|x|}{\dfrac{3^n}{n} + 1} + \dfrac{|x|}{3^n + n}) = 3|x| + 0 + 0 = 3|x| $$ So we want $3|x| < 1$, i.e. $|x| < \dfrac{1}{3}$. But we have to also check the points $x=\dfrac{1}{3}, -\dfrac{1}{3}$, where the limit equals $1$. For $x=\dfrac{1}{3}$ we have $\displaystyle \sum_{n=1}^\infty (3^n + n)\cdot (\dfrac{1}{3})^n = \displaystyle \sum_{n=1}^\infty (1 + n \cdot \dfrac{1}{3}^n)  $ which obviously diverges. For $x=-\dfrac{1}{3}$ we have $\displaystyle \sum_{n=1}^\infty (3^n + n)\cdot (-\dfrac{1}{3})^n = \displaystyle \sum_{n=1}^\infty ((-1)^{n} + n \cdot (-\dfrac{1}{3})^n)$. Now I know this diverges because $(-1)^n$ and $(-\dfrac{1}{3})^n$ have alternating coefficients. But is there a theorem that I can use here? So we conclude that $|x| < \dfrac{1}{3}$, but is the above work enough to show it conclusively?",,"['limits', 'convergence-divergence']"
81,How do I find $\liminf$ and $\limsup$ if $a_{2n}=\frac {a_{2n-1}}2$ and $a_{2n+1}=\frac12+\frac {a_{2n}}2$?,How do I find  and  if  and ?,\liminf \limsup a_{2n}=\frac {a_{2n-1}}2 a_{2n+1}=\frac12+\frac {a_{2n}}2,Its given that $a_1=a>0$ and that for any $n>1$ two things happen: $$a_{2n}=\frac {a_{2n-1}}2$$ $$a_{2n+1}=\frac12+\frac {a_{2n}}2$$ How do I find $\lim\inf$ and $\lim\sup$ I am trying to look at $a_{2n+1}$ and $a_{2n-1}$ $a_{2n}$ and $a_{2n-2}$ But I am unable to prove that they are bounded. NOTE: Look at joeys answer for correct solution.,Its given that $a_1=a>0$ and that for any $n>1$ two things happen: $$a_{2n}=\frac {a_{2n-1}}2$$ $$a_{2n+1}=\frac12+\frac {a_{2n}}2$$ How do I find $\lim\inf$ and $\lim\sup$ I am trying to look at $a_{2n+1}$ and $a_{2n-1}$ $a_{2n}$ and $a_{2n-2}$ But I am unable to prove that they are bounded. NOTE: Look at joeys answer for correct solution.,,"['calculus', 'sequences-and-series', 'limits', 'recurrence-relations', 'limsup-and-liminf']"
82,Limit with fractional part,Limit with fractional part,,Find $$\lim_{x\to 0}\frac{\sin\{x\}}{\{x\}}$$ where $\{x\}=$ fractional part $=x-[x] $. Please help me to solve. I know the defn of $[x] $ and $\lim_{x\to 0-}[x]=-1$ and $\lim_{x\to 0+}[x]=0$.,Find $$\lim_{x\to 0}\frac{\sin\{x\}}{\{x\}}$$ where $\{x\}=$ fractional part $=x-[x] $. Please help me to solve. I know the defn of $[x] $ and $\lim_{x\to 0-}[x]=-1$ and $\lim_{x\to 0+}[x]=0$.,,"['calculus', 'limits']"
83,Find a real number such that the piecewise function is continuous,Find a real number such that the piecewise function is continuous,,"I am reasoning about this problem. The text asks to find a real number $a$ such that the piecewise function $$ f(x) = \begin{cases} 3x+2 \;\;\;\; x < 2 \\ x^2+a \;\;\;\;  x\geq 2 \end{cases}$$ is continuous. It gives the hint to compute the left hand and right hand limits. May I solve this exercise simply by applying a limit property, namely: in order for the total function to be continuous the limit of the sum of the functions has to equal the limits of the individual functions? Thus, $$ \lim_{x \rightarrow 2 }  3x +2 = \lim_{  x \rightarrow 2}   x^2 + a = f(2) = 8$$ Thus, $a = 4$ ?","I am reasoning about this problem. The text asks to find a real number $a$ such that the piecewise function $$ f(x) = \begin{cases} 3x+2 \;\;\;\; x < 2 \\ x^2+a \;\;\;\;  x\geq 2 \end{cases}$$ is continuous. It gives the hint to compute the left hand and right hand limits. May I solve this exercise simply by applying a limit property, namely: in order for the total function to be continuous the limit of the sum of the functions has to equal the limits of the individual functions? Thus, $$ \lim_{x \rightarrow 2 }  3x +2 = \lim_{  x \rightarrow 2}   x^2 + a = f(2) = 8$$ Thus, $a = 4$ ?",,"['limits', 'functions', 'continuity']"
84,"Value of the limit without (or with, but giving rigorous arguments) using the Taylor expansion of sin","Value of the limit without (or with, but giving rigorous arguments) using the Taylor expansion of sin",,"I'm trying to evaluate the limit as $N\to \infty.$ $$\frac{  \left(\dfrac{\sin \frac{1}{N}} {\frac{1}{N}}\right)^{N}   -1 }{\frac{1}{N}}.$$ Note first that, using L'Hpital, one can easily show that the numerator goes to $0.$ Using the Taylor series expansion for $sin$, the value of the actual limit seems to be  $-\frac{1}{6}$. But I'm not fully sure how to justify the infinite series in the numerator is $-\frac{1}{6N}+O(\frac{1}{N^2})$. You could either justify that, if I was right, or may be use some other method to tell me what the limit is? Many thanks in advance!","I'm trying to evaluate the limit as $N\to \infty.$ $$\frac{  \left(\dfrac{\sin \frac{1}{N}} {\frac{1}{N}}\right)^{N}   -1 }{\frac{1}{N}}.$$ Note first that, using L'Hpital, one can easily show that the numerator goes to $0.$ Using the Taylor series expansion for $sin$, the value of the actual limit seems to be  $-\frac{1}{6}$. But I'm not fully sure how to justify the infinite series in the numerator is $-\frac{1}{6N}+O(\frac{1}{N^2})$. You could either justify that, if I was right, or may be use some other method to tell me what the limit is? Many thanks in advance!",,"['calculus', 'limits', 'taylor-expansion']"
85,"recurrence relation, limits","recurrence relation, limits",,"Let $a_1\in(0,1)$ and $a_{n+1}=a_n(1-a_n^2)$ for $n\ge 1$. Prove that $\lim \sqrt{n}a_n = \frac{1}{\sqrt{2}}$ and find $\lim \left(\sqrt{n}a_n - \frac{1}{\sqrt{2}}\right)\frac{n}{\ln n}$. For the first part, I used identity $na_n^2=\frac{n}{\frac{1}{a_n^2}}$ and Stolz-Cesaro theorem. How to deal with the more advanced one? I don't see the use of Stolz-Cesaro here.","Let $a_1\in(0,1)$ and $a_{n+1}=a_n(1-a_n^2)$ for $n\ge 1$. Prove that $\lim \sqrt{n}a_n = \frac{1}{\sqrt{2}}$ and find $\lim \left(\sqrt{n}a_n - \frac{1}{\sqrt{2}}\right)\frac{n}{\ln n}$. For the first part, I used identity $na_n^2=\frac{n}{\frac{1}{a_n^2}}$ and Stolz-Cesaro theorem. How to deal with the more advanced one? I don't see the use of Stolz-Cesaro here.",,"['real-analysis', 'sequences-and-series', 'limits']"
86,How to approach this question (multivariable calculus/limits)?,How to approach this question (multivariable calculus/limits)?,,"The thing is I don't want any direct answer I just need a tip of a trick that I can apply to get the magic. I am not really interested in the answer, so please don't write the answer for the question but rather how to approach and think about the question. I'm given the following $$\lim_{(x,y)\to (2,-2)}\frac{\ln(x^4-y^4+1)}{x^2-y^2}.  $$ I obviously have tried to approach it from the $x$ axis by setting $y=0$ and so from the $y$ axis by setting $x =0$ but it didn't help out because when you set $x=0$ you end up with an undefined answer in $\mathbb R$. Mathematica tho says that the limit converges to $8$ Thank you in advance!","The thing is I don't want any direct answer I just need a tip of a trick that I can apply to get the magic. I am not really interested in the answer, so please don't write the answer for the question but rather how to approach and think about the question. I'm given the following $$\lim_{(x,y)\to (2,-2)}\frac{\ln(x^4-y^4+1)}{x^2-y^2}.  $$ I obviously have tried to approach it from the $x$ axis by setting $y=0$ and so from the $y$ axis by setting $x =0$ but it didn't help out because when you set $x=0$ you end up with an undefined answer in $\mathbb R$. Mathematica tho says that the limit converges to $8$ Thank you in advance!",,"['limits', 'multivariable-calculus']"
87,Closed-form formula to evaluate $\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k$,Closed-form formula to evaluate,\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k,"Inspired by this question I'm trying to prove that $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} \approx e^{\frac{x}{2}}$$ So I needed to find the value of $$\frac{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{e^{\frac{x}{2}}} = \frac{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{\frac{x}{2}^k}{k!}} \\ = \lim_{m \to \infty} \frac{\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\sum_{k = 0}^{m} \frac{\frac{x}{2}^k}{k!}} = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{\frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\frac{\frac{x}{2}^k}{k!}} = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}\cdot\frac{k!}{\frac{x}{2}^k} \\ = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\cdot 2^k $$ Now, since $$\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}\cdot 2^k=\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} \frac{m!}{m!}\cdot 2^k=\sum_{k = 0}^{m} \binom{m}{2m} \cdot \binom{2m-k}{m}\cdot 2^k\\=\binom{m}{2m} \cdot \sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k=\frac{\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k}{\binom{2m}{m}}$$ I only need to prove that $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} = \lim_{m \to \infty} \frac{\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k}{\binom{2m}{m}} \approx e^{\frac{x}{2}}$$ so the question: is there a closed-form formula to evaluate $\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k$ ( Bessel functions are not allowed)?","Inspired by this question I'm trying to prove that $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} \approx e^{\frac{x}{2}}$$ So I needed to find the value of $$\frac{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{e^{\frac{x}{2}}} = \frac{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{\frac{x}{2}^k}{k!}} \\ = \lim_{m \to \infty} \frac{\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\sum_{k = 0}^{m} \frac{\frac{x}{2}^k}{k!}} = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{\frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\frac{\frac{x}{2}^k}{k!}} = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}\cdot\frac{k!}{\frac{x}{2}^k} \\ = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\cdot 2^k $$ Now, since $$\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}\cdot 2^k=\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} \frac{m!}{m!}\cdot 2^k=\sum_{k = 0}^{m} \binom{m}{2m} \cdot \binom{2m-k}{m}\cdot 2^k\\=\binom{m}{2m} \cdot \sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k=\frac{\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k}{\binom{2m}{m}}$$ I only need to prove that $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} = \lim_{m \to \infty} \frac{\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k}{\binom{2m}{m}} \approx e^{\frac{x}{2}}$$ so the question: is there a closed-form formula to evaluate $\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k$ ( Bessel functions are not allowed)?",,"['sequences-and-series', 'limits', 'summation', 'binomial-coefficients', 'bessel-functions']"
88,limit of composite functions- switching order of limit,limit of composite functions- switching order of limit,,"When is it correct to do stuff like this:  $$\lim\limits_{x\to a}f(g(x))=f(\lim\limits_{x\to a}g(x)) \quad (*)$$ I know (*) is true when $g$ is continuous at $a$ and $f$ is continuous at $g(a)$. However, what about the other cases ? For instance,  what can we say about $\lim\limits_{x\to a}f(g(x))$ when $\lim\limits_{x\to a}g(x)$ equals $\pm \infty$ or does not exist ? A Similar question in evaluating limits in the indeterminate forms $1^\infty, 0^0,\infty^\infty$. The technique used here is using $e^x$ and $\ln x$ as following $$\lim\limits_{x\to a}f(x)^{g(x)}= \lim\limits_{x\to a} e^{g(x)\ln f(x)}=e^{\lim\limits_{x\to a}g(x)\ln f(x)},\quad \text{provided the limit exists}$$ Here, the limit is switched from the outside to the inside of the exponential fuction.  Can I conclude the following statements ? why and why not ? If $\lim\limits_{x\to a}g(x)\ln f(x)=\infty$ then $\lim\limits_{x\to     a}f(x)^{g(x)}=\infty$ If $\lim\limits_{x\to a}g(x)\ln f(x)=-\infty$ then $\lim\limits_{x\to a}f(x)^{g(x)}=0$ If $\lim\limits_{x\to a}g(x)\ln f(x)$ does not exist, then $\lim\limits_{x\to a}f(x)^{g(x)}$ does not exist.","When is it correct to do stuff like this:  $$\lim\limits_{x\to a}f(g(x))=f(\lim\limits_{x\to a}g(x)) \quad (*)$$ I know (*) is true when $g$ is continuous at $a$ and $f$ is continuous at $g(a)$. However, what about the other cases ? For instance,  what can we say about $\lim\limits_{x\to a}f(g(x))$ when $\lim\limits_{x\to a}g(x)$ equals $\pm \infty$ or does not exist ? A Similar question in evaluating limits in the indeterminate forms $1^\infty, 0^0,\infty^\infty$. The technique used here is using $e^x$ and $\ln x$ as following $$\lim\limits_{x\to a}f(x)^{g(x)}= \lim\limits_{x\to a} e^{g(x)\ln f(x)}=e^{\lim\limits_{x\to a}g(x)\ln f(x)},\quad \text{provided the limit exists}$$ Here, the limit is switched from the outside to the inside of the exponential fuction.  Can I conclude the following statements ? why and why not ? If $\lim\limits_{x\to a}g(x)\ln f(x)=\infty$ then $\lim\limits_{x\to     a}f(x)^{g(x)}=\infty$ If $\lim\limits_{x\to a}g(x)\ln f(x)=-\infty$ then $\lim\limits_{x\to a}f(x)^{g(x)}=0$ If $\lim\limits_{x\to a}g(x)\ln f(x)$ does not exist, then $\lim\limits_{x\to a}f(x)^{g(x)}$ does not exist.",,"['calculus', 'limits', 'function-and-relation-composition']"
89,Calculate a limit $\lim_{t\to 0} \Big(\frac{a_1^t+a_2^t+...+a_n^t}{n}\Big)^{1/t}$,Calculate a limit,\lim_{t\to 0} \Big(\frac{a_1^t+a_2^t+...+a_n^t}{n}\Big)^{1/t},Calculate a limit $$\lim_{t\to 0} \Big(\frac{a_1^t+a_2^t+...+a_n^t}{n}\Big)^{1/t}$$ $a_i \in \mathbb{R}_{>0}$ I may guess a usage of squeeze theorem like $a_{min}$ and $a_{max}$ exist so $$\lim_{t\to 0} \Big(\frac{na_{min}^t}{n}\Big)^{1/t} = \lim_{t\to 0} a_{min}$$ and $$\lim_{t\to 0} \Big(\frac{na_{max}^t}{n}\Big)^{1/t} = \lim_{t\to 0} a_{max}$$ but $\lim_{t \to 0}  a_{min}$ is not the same as for $a_{max}$. So I stuck there.,Calculate a limit $$\lim_{t\to 0} \Big(\frac{a_1^t+a_2^t+...+a_n^t}{n}\Big)^{1/t}$$ $a_i \in \mathbb{R}_{>0}$ I may guess a usage of squeeze theorem like $a_{min}$ and $a_{max}$ exist so $$\lim_{t\to 0} \Big(\frac{na_{min}^t}{n}\Big)^{1/t} = \lim_{t\to 0} a_{min}$$ and $$\lim_{t\to 0} \Big(\frac{na_{max}^t}{n}\Big)^{1/t} = \lim_{t\to 0} a_{max}$$ but $\lim_{t \to 0}  a_{min}$ is not the same as for $a_{max}$. So I stuck there.,,"['calculus', 'limits']"
90,Limits without L'Hopitals Rule ( as I calculate it?),Limits without L'Hopitals Rule ( as I calculate it?),,Prove that: $\lim z \to \infty \left (z^2 +\sqrt{z^{4}+2z^{3}}-2\sqrt{z^{4}+z^{3}}\right )=\frac{-1}{4}$,Prove that: $\lim z \to \infty \left (z^2 +\sqrt{z^{4}+2z^{3}}-2\sqrt{z^{4}+z^{3}}\right )=\frac{-1}{4}$,,"['calculus', 'limits', 'infinity']"
91,"How to prove that both f(x) and its derivative decay to zero, as x grows to infinity, [duplicate]","How to prove that both f(x) and its derivative decay to zero, as x grows to infinity, [duplicate]",,"This question already has answers here : If $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x)) =L<\infty$, does $\lim\limits_{x\rightarrow\infty} f(x) $ exist? (2 answers) Closed 8 years ago . The problem statement is: Prove: If $f(x)$ is differentiable for x>0 and $$\lim_{x \to \infty}  (f(x)+\frac {df}{dx}(x)) =0$$ and if $$\lim_{x \to \infty} f(x)$$ exists, then $$\lim_{x \to \infty} f(x) = \lim_{x \to \infty} \frac{df}{dx}(x)=0$$ My work: I don't see how the differentiability assumption on f can be used (or is even needed?), but I have that, for every $\epsilon$>0, there exists M = $max(M_1,M_2)$, such that x>M implies that $$|f(x)+ \frac{df}{dx}(x)| < \epsilon$$ $$\implies-\epsilon < f(x) +  \frac{df}{dx}(x)  < \epsilon$$ $$\implies -\frac{df}{dx}(x) -\epsilon < f(x)   < -\frac{df}{dx}(x) +\epsilon$$ and since by assumption we know that the limit at infinity for f(x) exists, then the limit must be $-\frac{df}{dx}$, by the last inequality above. Now I think the goal is to just show that the limit at infinity of $|\frac{df}{dx}|=0.$  Then we would have the desired equality, by the Squeeze Theorem. But what can I say about the limit of $|\frac{df}{dx}|$? Thanks,","This question already has answers here : If $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x)) =L<\infty$, does $\lim\limits_{x\rightarrow\infty} f(x) $ exist? (2 answers) Closed 8 years ago . The problem statement is: Prove: If $f(x)$ is differentiable for x>0 and $$\lim_{x \to \infty}  (f(x)+\frac {df}{dx}(x)) =0$$ and if $$\lim_{x \to \infty} f(x)$$ exists, then $$\lim_{x \to \infty} f(x) = \lim_{x \to \infty} \frac{df}{dx}(x)=0$$ My work: I don't see how the differentiability assumption on f can be used (or is even needed?), but I have that, for every $\epsilon$>0, there exists M = $max(M_1,M_2)$, such that x>M implies that $$|f(x)+ \frac{df}{dx}(x)| < \epsilon$$ $$\implies-\epsilon < f(x) +  \frac{df}{dx}(x)  < \epsilon$$ $$\implies -\frac{df}{dx}(x) -\epsilon < f(x)   < -\frac{df}{dx}(x) +\epsilon$$ and since by assumption we know that the limit at infinity for f(x) exists, then the limit must be $-\frac{df}{dx}$, by the last inequality above. Now I think the goal is to just show that the limit at infinity of $|\frac{df}{dx}|=0.$  Then we would have the desired equality, by the Squeeze Theorem. But what can I say about the limit of $|\frac{df}{dx}|$? Thanks,",,"['real-analysis', 'integration', 'limits', 'derivatives']"
92,Proving the limit of a sequence using definitions,Proving the limit of a sequence using definitions,,"This is a review that my professor posted and I want to make sure I'm on the right path as I study $\cdot \lim \limits_{n \to \infty} n - \sqrt{2n^2+1} = n -\sqrt{2n^2+1}*\frac{n+\sqrt{2n^2+1}}{n+\sqrt{2n^2+1}} = \frac{-n^2-1}{n+\sqrt{2n^2+1}}$ So can I say that $M<\frac{-n^2-1}{n+\sqrt{2n^2+1}} \le \frac{n^2}{2n}\le \frac{n}{2}   $ and let my function be $N(M) = \frac{2}{M}$ when $n>N$ $\cdot \lim \limits_{n \to \infty} \frac{10^n}{n!} $ so can I say for a large n, maybe $n>20$  $\frac{10^n}{n!}\le \frac{1}{n!}$ therefore the function $N(\epsilon) = \max${$20, \epsilon!$} I feel like I'm very off track with this one This is the reverse of 2, but wouldn't it be the same approach but to find $N(M) $ such that $M>n!$ Finally, $\cdot \lim \limits_{n \to \infty} \frac{n}{3n-2000} = \frac{\frac{n}{n}}{\frac{3n}{n}-\frac{2000}{n}} = \frac{1}{3-\frac{2000}{n}}$ so then find $N(M)$ such that $M> \frac{1}{3} - \frac{n}{2000}$ therefore $N(M) = 2000(M-\frac{1}{3})$ or would $N(M) = Max${$\frac{2000}{3}, 2000M$}","This is a review that my professor posted and I want to make sure I'm on the right path as I study $\cdot \lim \limits_{n \to \infty} n - \sqrt{2n^2+1} = n -\sqrt{2n^2+1}*\frac{n+\sqrt{2n^2+1}}{n+\sqrt{2n^2+1}} = \frac{-n^2-1}{n+\sqrt{2n^2+1}}$ So can I say that $M<\frac{-n^2-1}{n+\sqrt{2n^2+1}} \le \frac{n^2}{2n}\le \frac{n}{2}   $ and let my function be $N(M) = \frac{2}{M}$ when $n>N$ $\cdot \lim \limits_{n \to \infty} \frac{10^n}{n!} $ so can I say for a large n, maybe $n>20$  $\frac{10^n}{n!}\le \frac{1}{n!}$ therefore the function $N(\epsilon) = \max${$20, \epsilon!$} I feel like I'm very off track with this one This is the reverse of 2, but wouldn't it be the same approach but to find $N(M) $ such that $M>n!$ Finally, $\cdot \lim \limits_{n \to \infty} \frac{n}{3n-2000} = \frac{\frac{n}{n}}{\frac{3n}{n}-\frac{2000}{n}} = \frac{1}{3-\frac{2000}{n}}$ so then find $N(M)$ such that $M> \frac{1}{3} - \frac{n}{2000}$ therefore $N(M) = 2000(M-\frac{1}{3})$ or would $N(M) = Max${$\frac{2000}{3}, 2000M$}",,"['sequences-and-series', 'limits', 'proof-verification', 'self-learning']"
93,Show $\{ a_n-b_n\} \rightarrow 0$ then $\{b_n\} \rightarrow L$,Show  then,\{ a_n-b_n\} \rightarrow 0 \{b_n\} \rightarrow L,"Suppose ${a_n}$ and ${b_n}$ are sequences with $\{a_n\}\rightarrow L$. Show $\{ a_n-b_n\} \rightarrow 0$ then  $\{b_n\} \rightarrow L$ Not sure if my proof is sufficient. Proof Since $\lim{a_n}=L, \exists N_1 $ for $n>N_1$ $|a_n - L| < \epsilon$ Now suppose $\lim{a_n - b_n} = 0, \exists N_2 $ for $n>N_2$ $|(a_n - b_n) - 0| < \epsilon \Rightarrow |a_n-b_n|<\epsilon$ $\Rightarrow a_n - b_n < \epsilon \Rightarrow a_n<\epsilon + b_n \Rightarrow a_n-L < \epsilon + b_n - L$ Thus $|a_n - L| < \epsilon + b_n - L$ Let $M = max\{N_1,N_2\}$ and $n>M$ Then, $|a_n-L+L-b_n| \leq |a_n-L| - |b_n-L| < \epsilon + b_n - L -|b_n - L| = \epsilon$","Suppose ${a_n}$ and ${b_n}$ are sequences with $\{a_n\}\rightarrow L$. Show $\{ a_n-b_n\} \rightarrow 0$ then  $\{b_n\} \rightarrow L$ Not sure if my proof is sufficient. Proof Since $\lim{a_n}=L, \exists N_1 $ for $n>N_1$ $|a_n - L| < \epsilon$ Now suppose $\lim{a_n - b_n} = 0, \exists N_2 $ for $n>N_2$ $|(a_n - b_n) - 0| < \epsilon \Rightarrow |a_n-b_n|<\epsilon$ $\Rightarrow a_n - b_n < \epsilon \Rightarrow a_n<\epsilon + b_n \Rightarrow a_n-L < \epsilon + b_n - L$ Thus $|a_n - L| < \epsilon + b_n - L$ Let $M = max\{N_1,N_2\}$ and $n>M$ Then, $|a_n-L+L-b_n| \leq |a_n-L| - |b_n-L| < \epsilon + b_n - L -|b_n - L| = \epsilon$",,"['real-analysis', 'limits']"
94,On the exponential function.,On the exponential function.,,I was rereading the prologue to Real and Complex Analysis by Rudin (in image) and I realized I never really understood why we have the first and second equality after$\sum_{k = 0}^{\infty}\frac{a^k}{k!}\sum_{m = 0}^{\infty}\frac{b^m}{m!} $ and where does the absolute convergence of (1) play a role. So could somebody provide some extra steps and maybe I will understand?,I was rereading the prologue to Real and Complex Analysis by Rudin (in image) and I realized I never really understood why we have the first and second equality after$\sum_{k = 0}^{\infty}\frac{a^k}{k!}\sum_{m = 0}^{\infty}\frac{b^m}{m!} $ and where does the absolute convergence of (1) play a role. So could somebody provide some extra steps and maybe I will understand?,,"['real-analysis', 'limits', 'summation', 'exponential-function']"
95,"Given a function $f(x,y)$ find the limit as $(x,y)\to(0,0)$",Given a function  find the limit as,"f(x,y) (x,y)\to(0,0)","If it exists, find $$\lim_{(t,x)\to(0,0)}\frac{t^2\sin^2(x)}{2x^2+t^2}$$ Along the curves $x=mt,t=0,x=at^2,t=ax^2$ the limit approaches 0; the graph also makes $L=0$ seem correct. So assuming that $L=0$, I start the epsilon delta proof: $$0<\sqrt{x^2+t^2}<\delta$$ $$\left|\frac{t^2\sin^2(x)}{2x^2+t^2}\right|<\epsilon$$ All attempts I did trying to find a $\delta$ for every $\epsilon$ have just end up circling around and accomplishing nothing. How am I supposed to complete this proof?","If it exists, find $$\lim_{(t,x)\to(0,0)}\frac{t^2\sin^2(x)}{2x^2+t^2}$$ Along the curves $x=mt,t=0,x=at^2,t=ax^2$ the limit approaches 0; the graph also makes $L=0$ seem correct. So assuming that $L=0$, I start the epsilon delta proof: $$0<\sqrt{x^2+t^2}<\delta$$ $$\left|\frac{t^2\sin^2(x)}{2x^2+t^2}\right|<\epsilon$$ All attempts I did trying to find a $\delta$ for every $\epsilon$ have just end up circling around and accomplishing nothing. How am I supposed to complete this proof?",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
96,How to show this limit exists,How to show this limit exists,,"I know the limit is 0, but how would I prove the limit? $$\lim_{(x,y)\to(0,0)} {\frac{x^3y^2}{x^4+y^6}}$$","I know the limit is 0, but how would I prove the limit? $$\lim_{(x,y)\to(0,0)} {\frac{x^3y^2}{x^4+y^6}}$$",,"['limits', 'multivariable-calculus']"
97,Differentiability of a 2-variable function,Differentiability of a 2-variable function,,"In this morning's Mathematical Analysis 2 exam, students were asked to study the continuity and differentiability of: $$f(x,y)=\left\{\begin{array}{cc} \dfrac{xye^{-\frac{1}{(x+y)^2}}}{x^2+2e^{-\frac{2}{(x+y)^2}}} & x\neq-y \\ {} \\ 0 & x=-y \end{array}\right.$$ This question answered the continuity problem. However, I feel if I try the same things I tried there for the differentiability, I will get stuck on the limit involved. Now, it is easy to verify that the partial derivatives in the origin are both zero, since the function is always 0 on the axes. So to prove differentiability, I would have to prove: $$\lim_{(h,k)\to(0,0)}\frac{f(h,k)}{\sqrt{h^2+k^2}}=0,$$ which is the limit proved zero for continuity except for the denominator, which makes it impossible to use the trick the answer to the linked question used. Polar coordinates give all the same problem as continuity. I can try substituting $k=m|h|^\alpha$, so for $\alpha<\frac12$ I can use the asymptotic in the comments to the linked question, and I'm left with $\frac{m|h|^{\alpha-2}\operatorname{sgn}h}{\sqrt{1+m^2|h|^{2\alpha-2}}}e^{-\frac{1}{m^2|h|^{2\alpha}}}$, and I'm not all too sure that tends to 0. And anyway I would still find problems with $\alpha>\frac12$, and L'Hospital, in this case, is just terrible. Any suggestions? PS The present question and this one are NOT duplicates, because that one focuses on continuity and this one focuses on differentiability. I thought I had made that clear in this question, but evidently it is not that clear since this question has been marked as a possible duplicate of the continuity one.","In this morning's Mathematical Analysis 2 exam, students were asked to study the continuity and differentiability of: $$f(x,y)=\left\{\begin{array}{cc} \dfrac{xye^{-\frac{1}{(x+y)^2}}}{x^2+2e^{-\frac{2}{(x+y)^2}}} & x\neq-y \\ {} \\ 0 & x=-y \end{array}\right.$$ This question answered the continuity problem. However, I feel if I try the same things I tried there for the differentiability, I will get stuck on the limit involved. Now, it is easy to verify that the partial derivatives in the origin are both zero, since the function is always 0 on the axes. So to prove differentiability, I would have to prove: $$\lim_{(h,k)\to(0,0)}\frac{f(h,k)}{\sqrt{h^2+k^2}}=0,$$ which is the limit proved zero for continuity except for the denominator, which makes it impossible to use the trick the answer to the linked question used. Polar coordinates give all the same problem as continuity. I can try substituting $k=m|h|^\alpha$, so for $\alpha<\frac12$ I can use the asymptotic in the comments to the linked question, and I'm left with $\frac{m|h|^{\alpha-2}\operatorname{sgn}h}{\sqrt{1+m^2|h|^{2\alpha-2}}}e^{-\frac{1}{m^2|h|^{2\alpha}}}$, and I'm not all too sure that tends to 0. And anyway I would still find problems with $\alpha>\frac12$, and L'Hospital, in this case, is just terrible. Any suggestions? PS The present question and this one are NOT duplicates, because that one focuses on continuity and this one focuses on differentiability. I thought I had made that clear in this question, but evidently it is not that clear since this question has been marked as a possible duplicate of the continuity one.",,"['limits', 'multivariable-calculus', 'differential']"
98,"How to solve this limit 2 variables $\lim_{(x,y) \to (4,1)} \frac{y \sqrt x - 2y - \sqrt x + 2}{4 - x + x \sqrt y - 4 \sqrt y}$",How to solve this limit 2 variables,"\lim_{(x,y) \to (4,1)} \frac{y \sqrt x - 2y - \sqrt x + 2}{4 - x + x \sqrt y - 4 \sqrt y}","Please anybody can help me solve this? $$\lim \frac{y \sqrt x - 2y - \sqrt x + 2}{4 - x + x \sqrt y - 4 \sqrt y}$$ with $(x,y) \to (4,1)$ Thank you!","Please anybody can help me solve this? $$\lim \frac{y \sqrt x - 2y - \sqrt x + 2}{4 - x + x \sqrt y - 4 \sqrt y}$$ with $(x,y) \to (4,1)$ Thank you!",,"['calculus', 'limits', 'multivariable-calculus']"
99,If $\lim\limits_{x\to b^-} f(x)=\infty$ Then $\lim\limits_{x\to b^-}f'(x)=\infty $,If  Then,\lim\limits_{x\to b^-} f(x)=\infty \lim\limits_{x\to b^-}f'(x)=\infty ,"Here's my question: Let $f$ be a continuous function in $[a,b)$ (semi closed interval) which is differentiable twice in $(a,b)$ , such that $f''(x)>0$ for all $x\in(a,b)$ . Prove that $$\lim_{x\to b^-}f(x)=\infty \Rightarrow \lim_{x\to b^-}f'(x)=\infty$$ In the question I have a hint: With the mean value theorem prove that if $f'$ is bounded then $f$ is bounded. Actually I didn't have much success with the hint. Instead, my claim is that if $f'$ is bounded, then $f$ is uniformly continuous (I can prove that), and then its limit is finite, which is a contradiction to the assumption of infinite limit. I don't know if I'm right. How can I use the hint? can I use my solution? Thanks, Alan","Here's my question: Let be a continuous function in (semi closed interval) which is differentiable twice in , such that for all . Prove that In the question I have a hint: With the mean value theorem prove that if is bounded then is bounded. Actually I didn't have much success with the hint. Instead, my claim is that if is bounded, then is uniformly continuous (I can prove that), and then its limit is finite, which is a contradiction to the assumption of infinite limit. I don't know if I'm right. How can I use the hint? can I use my solution? Thanks, Alan","f [a,b) (a,b) f''(x)>0 x\in(a,b) \lim_{x\to b^-}f(x)=\infty \Rightarrow \lim_{x\to b^-}f'(x)=\infty f' f f' f","['calculus', 'real-analysis', 'limits', 'continuity']"
