,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Proof of $\int_{[0,\infty)}pt^{p-1}\mu(\{x:|f(x)|\geq t\})d\mu(t)=\int_{[0,\infty)}\mu(\{x:|f(x)|^p\geq s\})d\mu(s)$",Proof of,"\int_{[0,\infty)}pt^{p-1}\mu(\{x:|f(x)|\geq t\})d\mu(t)=\int_{[0,\infty)}\mu(\{x:|f(x)|^p\geq s\})d\mu(s)","Let $({\Bbb R},{\mathcal A},\mu)$ be the measure space where ${\mu}$ is the Lebesgue measure.  Assume that $\int_{\Bbb R}|f|^pd\mu<\infty$ ($p\geq1$). There is an exercise for proving that $$\int_{[0,\infty)}pt^{p-1}\mu(\{x:|f(x)|\geq t\})d\mu(t)=\int_{[0,\infty)}\mu(\{x:|f(x)|^p\geq s\})d\mu(s).$$ If one formally let $s=t^p$, then $ds=pt^{p-1}dt$ and one would immediately get the equality above. However, how can I justify it rigorously in the sense of Lebesgue integration? [Edited] What puzzles me is that I have the ""change of variable"" theorem for Riemann integration, but all the integrations here are in the Lebesgue sense. Unless one has shown that the integrands are also Riemann integrable, how can I directly use ""change of variable""?","Let $({\Bbb R},{\mathcal A},\mu)$ be the measure space where ${\mu}$ is the Lebesgue measure.  Assume that $\int_{\Bbb R}|f|^pd\mu<\infty$ ($p\geq1$). There is an exercise for proving that $$\int_{[0,\infty)}pt^{p-1}\mu(\{x:|f(x)|\geq t\})d\mu(t)=\int_{[0,\infty)}\mu(\{x:|f(x)|^p\geq s\})d\mu(s).$$ If one formally let $s=t^p$, then $ds=pt^{p-1}dt$ and one would immediately get the equality above. However, how can I justify it rigorously in the sense of Lebesgue integration? [Edited] What puzzles me is that I have the ""change of variable"" theorem for Riemann integration, but all the integrations here are in the Lebesgue sense. Unless one has shown that the integrands are also Riemann integrable, how can I directly use ""change of variable""?",,['real-analysis']
1,"Let the function $f:[a,b] \to \mathbb R$ be Lipschitz. Show that $f$ maps a set of measure zero onto a set of measure zero",Let the function  be Lipschitz. Show that  maps a set of measure zero onto a set of measure zero,"f:[a,b] \to \mathbb R f","Let the function $f:[a,b] \to \mathbb R$ be Lipschitz, that is, there is a constant $c \geq 0$ such that for all $u,v \in [a,b]$, $|f(u)-f(v)| \leq c|u-v|$. Show that $f$ maps a set of measure zero onto a set of measure zero. Show that $f$ maps an $F_\sigma$ set onto an $F_\sigma$ set. Conclude that $f$ maps a measurable set to a measurable set. I have a question about this. We know that a set of measure zero must either be of the form [0], or it must be a set of countable elements. But for this function, we know that the domain is an interval...and we cannot have an interval with only countable elements, because an interval contains all reals, right? So for the first part of this question, we need to prove that [0] is mapped to [0], right? Thanks in advance","Let the function $f:[a,b] \to \mathbb R$ be Lipschitz, that is, there is a constant $c \geq 0$ such that for all $u,v \in [a,b]$, $|f(u)-f(v)| \leq c|u-v|$. Show that $f$ maps a set of measure zero onto a set of measure zero. Show that $f$ maps an $F_\sigma$ set onto an $F_\sigma$ set. Conclude that $f$ maps a measurable set to a measurable set. I have a question about this. We know that a set of measure zero must either be of the form [0], or it must be a set of countable elements. But for this function, we know that the domain is an interval...and we cannot have an interval with only countable elements, because an interval contains all reals, right? So for the first part of this question, we need to prove that [0] is mapped to [0], right? Thanks in advance",,['real-analysis']
2,Compute $ \int_{0}^{1}\frac{\ln(x) \ln^2 (1-x)}{x} dx $,Compute, \int_{0}^{1}\frac{\ln(x) \ln^2 (1-x)}{x} dx ,Compute $$ \int_{0}^{1}\frac{\ln(x) \ln^2 (1-x)}{x} dx $$ I'm looking for some nice proofs at this problem. One idea would be to use Taylor expansion and then integrating term by term. What else can we do? Thanks.,Compute $$ \int_{0}^{1}\frac{\ln(x) \ln^2 (1-x)}{x} dx $$ I'm looking for some nice proofs at this problem. One idea would be to use Taylor expansion and then integrating term by term. What else can we do? Thanks.,,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
3,A tricky sum to infinity,A tricky sum to infinity,,"I try to solve the following tricky limit: $$\lim_{x\rightarrow\infty} \sum_{k=1}^{\infty} \frac{kx}{(k^2+x)^2} $$ For some large values, W|A shows that its limit tends to $\frac{1}{2}$ but not sure how to prove that.","I try to solve the following tricky limit: $$\lim_{x\rightarrow\infty} \sum_{k=1}^{\infty} \frac{kx}{(k^2+x)^2} $$ For some large values, W|A shows that its limit tends to $\frac{1}{2}$ but not sure how to prove that.",,"['real-analysis', 'sequences-and-series', 'limits']"
4,Convergence of Multidimensional Infinite Series,Convergence of Multidimensional Infinite Series,,"I am trying to analyze the convergence of multidimensional infinite sums such as those in the following form: $$\sum_{n=0}^{\infty}\sum_{m=0}^{\infty}\frac{1}{1+\alpha\exp (\beta n) \exp (\gamma m)}$$ where $\alpha,\beta,\gamma\in(0,\infty)$. I'm quickly realizing though, that I have no experience with analyzing sums like this.  I realize that because $\mathbb{N}^2$ is countable, I could in principle re-write this sum as a related one: $\sum_{n=0}^{\infty} f(n)$ where $f(n)$ could even be guarenteed monotone decreasing.  However, I've been having trouble getting anywhere with this. Is there some well-established method of attacking convergence of multidimensional sums like this one?  If so, I would very much appreciate being pointed in the right direction.","I am trying to analyze the convergence of multidimensional infinite sums such as those in the following form: $$\sum_{n=0}^{\infty}\sum_{m=0}^{\infty}\frac{1}{1+\alpha\exp (\beta n) \exp (\gamma m)}$$ where $\alpha,\beta,\gamma\in(0,\infty)$. I'm quickly realizing though, that I have no experience with analyzing sums like this.  I realize that because $\mathbb{N}^2$ is countable, I could in principle re-write this sum as a related one: $\sum_{n=0}^{\infty} f(n)$ where $f(n)$ could even be guarenteed monotone decreasing.  However, I've been having trouble getting anywhere with this. Is there some well-established method of attacking convergence of multidimensional sums like this one?  If so, I would very much appreciate being pointed in the right direction.",,['real-analysis']
5,find $\int_0^\infty \frac{|\cos (\pi x)|}{4x^2 - 1} dx$,find,\int_0^\infty \frac{|\cos (\pi x)|}{4x^2 - 1} dx,"Find (with proof) $\displaystyle\int_0^\infty \frac{|\cos (\pi x)|}{4x^2 - 1}dx$ It's actually not even clear that the integral converges. If there were only sines/cosines in the integral, a standard technique would be to use the trigonometric identity $\sin \theta = \frac{2t}{1+t^2}$ , $\cos \theta = \frac{1-t^2}{1+t^2}$ , $t = \tan(\theta/2)$ . I know that $\frac{2}{4x^2-1} = \frac{1}{2x-1} -\frac{1}{2x+1}$ , so maybe one could plug this into the given integral to obtain an integral that's easier to evaluate? Edit: from a comment, I think it would be useful to note that $\cos (\pi x) \leq 0$ iff $(2k+1/2)\pi \leq \pi x \leq (2k + 3/2)\pi\iff(2k + 1/2)\leq x\leq 2k+3/2$ (for some integer $k$ ) and $\cos(\pi x) \ge 0$ iff $(2k - 1/2) \leq  x \leq (2k+1/2)$ (for some integer $k$ ). So we can split the integral according to these ranges. Then it might be useful to apply integration by parts. Let $a\in \mathbb{R}$ . Then $$\begin{align}\int \frac{\cos(ax)}{4x^2-1}dx &= \frac{1}2\left(\int \frac{\cos(a x)}{2x-1}dx - \int \frac{\cos(a x)}{2x+1}dx\right)\\ \\ &= \frac{1}2\left(\frac{1}2 \ln(2x-1)\cos (ax) +\frac{1}2a \int \sin(ax)\ln(2x-1)dx\right) \\ \\&-\frac{1}2\left(\frac{1}2 \ln(2x+1)\cos (ax) +\frac{1}2a \int \sin(ax)\ln(2x+1)dx\right)\end{align}$$ but I'm not sure how to simplify the result. For the bounty, I'm looking for formal proofs. In particular, I'd like to see justifications for interchanging an integral and a sum. One can freely interchange an integral and a sum if the terms are nonnegative (as $\sum \int f_n = \int \sum f_n$ for nonnegative Lebesgue measurable functions $f_n$ and the Lebesgue integral equals the Riemann integral for Riemann integrable functions). Also I'd like to see justifications for why $$\int_0^\infty \frac{\cos^2(\pi (2m+1) x)}{4x^2 - 1}dx = 0 = \int_0^\infty \frac{\sin^2(\pi (2m+1)x)}{4x^2-1} dx,$$ where $m$ is any nonnegative integer.","Find (with proof) It's actually not even clear that the integral converges. If there were only sines/cosines in the integral, a standard technique would be to use the trigonometric identity , , . I know that , so maybe one could plug this into the given integral to obtain an integral that's easier to evaluate? Edit: from a comment, I think it would be useful to note that iff (for some integer ) and iff (for some integer ). So we can split the integral according to these ranges. Then it might be useful to apply integration by parts. Let . Then but I'm not sure how to simplify the result. For the bounty, I'm looking for formal proofs. In particular, I'd like to see justifications for interchanging an integral and a sum. One can freely interchange an integral and a sum if the terms are nonnegative (as for nonnegative Lebesgue measurable functions and the Lebesgue integral equals the Riemann integral for Riemann integrable functions). Also I'd like to see justifications for why where is any nonnegative integer.","\displaystyle\int_0^\infty \frac{|\cos (\pi x)|}{4x^2 - 1}dx \sin \theta = \frac{2t}{1+t^2} \cos \theta = \frac{1-t^2}{1+t^2} t = \tan(\theta/2) \frac{2}{4x^2-1} = \frac{1}{2x-1} -\frac{1}{2x+1} \cos (\pi x) \leq 0 (2k+1/2)\pi \leq \pi x \leq (2k + 3/2)\pi\iff(2k + 1/2)\leq x\leq 2k+3/2 k \cos(\pi x) \ge 0 (2k - 1/2) \leq  x \leq (2k+1/2) k a\in \mathbb{R} \begin{align}\int \frac{\cos(ax)}{4x^2-1}dx &= \frac{1}2\left(\int \frac{\cos(a x)}{2x-1}dx - \int \frac{\cos(a x)}{2x+1}dx\right)\\
\\
&= \frac{1}2\left(\frac{1}2 \ln(2x-1)\cos (ax) +\frac{1}2a \int \sin(ax)\ln(2x-1)dx\right) \\
\\&-\frac{1}2\left(\frac{1}2 \ln(2x+1)\cos (ax) +\frac{1}2a \int \sin(ax)\ln(2x+1)dx\right)\end{align} \sum \int f_n = \int \sum f_n f_n \int_0^\infty \frac{\cos^2(\pi (2m+1) x)}{4x^2 - 1}dx = 0 = \int_0^\infty \frac{\sin^2(\pi (2m+1)x)}{4x^2-1} dx, m","['real-analysis', 'calculus', 'integration', 'trigonometry']"
6,Find the gradient and hessian of $f(Ax+b)$ for real value $f$ and matrix $A$,Find the gradient and hessian of  for real value  and matrix,f(Ax+b) f A,"Let $A\in\mathbb{R}^{m\times n}$, $b\in \mathbb{R}^m$. For   $x\in\mathbb{R}^n$, we define $q(x) = f(Ax+b)$ with   $f:\mathbb{R}^m\to\mathbb{R}$. Find the gradient and hessian of the   function $q$. This question is kinda strange. If I were to take the jacobian, I would just compose the jacobian of the outer funciton with the jacobian of the inner function. Now, how do I take the partial derivative of $q$? $$\frac{\partial f(Ax+b)}{\partial x_1} = \lim_{h\to 0}\frac{f(A(x_1+h,x_2,\cdots,x_n) + b) - f(Ax+b)}{h}$$ I don't think it helps in thinking this way. I have no means of finding this limit without using some chain rule or so. Maybe I can apply the chain rule to $q$, but how? UPDATE: By the hint given below, $$q(x_1,\dots, x_n)=f(f_1,\cdots,f_n) = f\left(\sum_{i=1}^n a_{1i}x_i+b_1,\dots,\sum_{i=1}^n a_{mi}x_i+b_m\right)$$ I think the multivariable chain rule can be applied: $$\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial f_1}\frac{\partial f_1}{\partial x_1} + \cdots + \frac{\partial f}{\partial f_n}\frac{\partial f_n}{\partial x_1}$$ And see that $$\frac{\partial f_1}{\partial x_1} = a_{11}\\\cdots\\\frac{\partial f_n}{\partial x_1} = a_{m1}$$ So we get $$\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial f_1}a_{11} + \cdots + \frac{\partial f}{\partial f_n}a_{m1}$$ In general: $$\frac{\partial f}{\partial x_j} = \frac{\partial f}{\partial f_1}a_{1j} + \cdots + \frac{\partial f}{\partial f_n}a_{mj}$$ I think there's still a lot of work to do.","Let $A\in\mathbb{R}^{m\times n}$, $b\in \mathbb{R}^m$. For   $x\in\mathbb{R}^n$, we define $q(x) = f(Ax+b)$ with   $f:\mathbb{R}^m\to\mathbb{R}$. Find the gradient and hessian of the   function $q$. This question is kinda strange. If I were to take the jacobian, I would just compose the jacobian of the outer funciton with the jacobian of the inner function. Now, how do I take the partial derivative of $q$? $$\frac{\partial f(Ax+b)}{\partial x_1} = \lim_{h\to 0}\frac{f(A(x_1+h,x_2,\cdots,x_n) + b) - f(Ax+b)}{h}$$ I don't think it helps in thinking this way. I have no means of finding this limit without using some chain rule or so. Maybe I can apply the chain rule to $q$, but how? UPDATE: By the hint given below, $$q(x_1,\dots, x_n)=f(f_1,\cdots,f_n) = f\left(\sum_{i=1}^n a_{1i}x_i+b_1,\dots,\sum_{i=1}^n a_{mi}x_i+b_m\right)$$ I think the multivariable chain rule can be applied: $$\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial f_1}\frac{\partial f_1}{\partial x_1} + \cdots + \frac{\partial f}{\partial f_n}\frac{\partial f_n}{\partial x_1}$$ And see that $$\frac{\partial f_1}{\partial x_1} = a_{11}\\\cdots\\\frac{\partial f_n}{\partial x_1} = a_{m1}$$ So we get $$\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial f_1}a_{11} + \cdots + \frac{\partial f}{\partial f_n}a_{m1}$$ In general: $$\frac{\partial f}{\partial x_j} = \frac{\partial f}{\partial f_1}a_{1j} + \cdots + \frac{\partial f}{\partial f_n}a_{mj}$$ I think there's still a lot of work to do.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
7,Turning infinite sum into integral,Turning infinite sum into integral,,"Could someone explain how one can replace infinite sum with integral? Examples (or you may use your own, it doesn't matter as I want to understand principles): $$\frac{1}{n} \sum_{i=1}^{n} \sin \frac{i-1}{n} \pi$$ $$\frac{1}{n} \sum_{i=1}^{n} \sqrt{1 + \frac{i}{n}}$$ I see that $\frac{1}{n}$ denote partition, so my problem is that I don't know how to figure out the function, hence, the integrand.","Could someone explain how one can replace infinite sum with integral? Examples (or you may use your own, it doesn't matter as I want to understand principles): $$\frac{1}{n} \sum_{i=1}^{n} \sin \frac{i-1}{n} \pi$$ $$\frac{1}{n} \sum_{i=1}^{n} \sqrt{1 + \frac{i}{n}}$$ I see that $\frac{1}{n}$ denote partition, so my problem is that I don't know how to figure out the function, hence, the integrand.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
8,Convergence power series in boundary,Convergence power series in boundary,,"Say I have a power series $\sum_{k=0}^\infty a_k z^k$ with radius of convergence $0<R<\infty$. What can be said topologically about the set $\{z\in\Bbb C\mid |z|=R\,\mbox{ and }\sum_{k=0}^\infty a_k z^k \mbox{ converges}\}$ ? Is it possible for there to be an isolated point? Is it possible that it converges for only one point in the boundary of the circle? Thanks!","Say I have a power series $\sum_{k=0}^\infty a_k z^k$ with radius of convergence $0<R<\infty$. What can be said topologically about the set $\{z\in\Bbb C\mid |z|=R\,\mbox{ and }\sum_{k=0}^\infty a_k z^k \mbox{ converges}\}$ ? Is it possible for there to be an isolated point? Is it possible that it converges for only one point in the boundary of the circle? Thanks!",,"['real-analysis', 'complex-analysis', 'analysis']"
9,Common Ground between Real Analysis and Measure Theory,Common Ground between Real Analysis and Measure Theory,,"I'm currently taking two introductory classes in Real Analysis (Rudin textbook) and Measure Theory (no textbook - but the material we cover is very standard). It seems as if there is a huge overlap between the material that is covered in both classes. In particular, I believe that Measure Theory is more of a specific application of Real Analysis. That said, I'm having a lot of difficulty seeing how the two fields relate to one another. This is all very broad, so here are some questions that I have: Are $\sigma$-fields a subtype of field? What are the ""real analysis"" type properties of a Borel set? (i.e. is it closed? open?compact?)"" What are the ""real analysis"" type properties of a Random Variable? What are the ""real analysis""-type properties of a Measure? Have I even covered enough material to see the ""common ground"" between these subjects? (in Real Analysis, we've covered Ch. 1-2 of Rudin and in Measure Theory, we've covered probability spaces and random variables). Any other insights are very much appreciated!","I'm currently taking two introductory classes in Real Analysis (Rudin textbook) and Measure Theory (no textbook - but the material we cover is very standard). It seems as if there is a huge overlap between the material that is covered in both classes. In particular, I believe that Measure Theory is more of a specific application of Real Analysis. That said, I'm having a lot of difficulty seeing how the two fields relate to one another. This is all very broad, so here are some questions that I have: Are $\sigma$-fields a subtype of field? What are the ""real analysis"" type properties of a Borel set? (i.e. is it closed? open?compact?)"" What are the ""real analysis"" type properties of a Random Variable? What are the ""real analysis""-type properties of a Measure? Have I even covered enough material to see the ""common ground"" between these subjects? (in Real Analysis, we've covered Ch. 1-2 of Rudin and in Measure Theory, we've covered probability spaces and random variables). Any other insights are very much appreciated!",,"['real-analysis', 'measure-theory']"
10,Any explicit examples of irrationals in the Cantor set?,Any explicit examples of irrationals in the Cantor set?,,"Since the Cantor set is uncountable, it must contain irrationals. I am aware that they can't be normal, so the irrationals in the Cantor set are transcendental. Are there any explicit constructions of such numbers, or can we only indirectly show their existence? Could you please provide the construction for the base $10$ ?","Since the Cantor set is uncountable, it must contain irrationals. I am aware that they can't be normal, so the irrationals in the Cantor set are transcendental. Are there any explicit constructions of such numbers, or can we only indirectly show their existence? Could you please provide the construction for the base ?",10,['real-analysis']
11,Why is the Riemann integral only defined on compact sets?,Why is the Riemann integral only defined on compact sets?,,"Every text I look at says a function must be bounded and be defined on a compact set before one can even think about the Riemann integral. Boundedness makes sense, otherwise the Darboux sums could be undefined. However, I don't see where it becomes important that the integral be taken over a compact set.","Every text I look at says a function must be bounded and be defined on a compact set before one can even think about the Riemann integral. Boundedness makes sense, otherwise the Darboux sums could be undefined. However, I don't see where it becomes important that the integral be taken over a compact set.",,"['real-analysis', 'integration']"
12,$xf(f(f(x)))=1$ continuous,continuous,xf(f(f(x)))=1,Assume $f:\mathbb{R}_{>0}\to\mathbb{R}_{>0}$ is a continuous function such that $xf(f(f(x)))=1$ for all $x>0$ . I found that $f(x)=1/x$ is a solution. Could we find another such function? And why?,Assume is a continuous function such that for all . I found that is a solution. Could we find another such function? And why?,f:\mathbb{R}_{>0}\to\mathbb{R}_{>0} xf(f(f(x)))=1 x>0 f(x)=1/x,"['real-analysis', 'analysis']"
13,"If $‎\lim\limits_{x\to\infty}‎\frac{f(x)}{g(x)} = 1‎$,‎ then $\lim\limits_{x\to\infty}(f(x) - g(x)) = 0‎$.","If ,‎ then .",‎\lim\limits_{x\to\infty}‎\frac{f(x)}{g(x)} = 1‎ \lim\limits_{x\to\infty}(f(x) - g(x)) = 0‎,"‎Suppose ‎ $‎f‎$ ‎and ‎‎ $‎g‎$ ‎ ‎‎are real functions such that $‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{f(x)}{g(x)} = 1‎$ ‎‎. ‎My ‎question ‎is‎: ‎‎‎ ‎‎What other condition is required ‏‎that $‎‎‎\displaystyle{\lim_{x\to\infty}}(f(x) - g(x)) = 0‎$ ‎‎‎‎‎‎‎‎‎?‎ ‎‎  ‎Generall‏‎y,‎ $‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{f(x)}{g(x)} = 1‎\nRightarrow‎‎‎‎‎\displaystyle{\lim_{x\to\infty}}(f(x) - g(x)) = 0‎$ ‎. ‏‎For ‎example, ‎‎ $‎f(x) = x^2 + x‎$ ‎and ‎‎ $‎g(x) =‎ ‎x^2‎$ ‎.‎","‎Suppose ‎ ‎and ‎‎ ‎ ‎‎are real functions such that ‎‎. ‎My ‎question ‎is‎: ‎‎‎ ‎‎What other condition is required ‏‎that ‎‎‎‎‎‎‎‎‎?‎ ‎‎  ‎Generall‏‎y,‎ ‎. ‏‎For ‎example, ‎‎ ‎and ‎‎ ‎.‎",‎f‎ ‎g‎ ‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{f(x)}{g(x)} = 1‎ ‎‎‎\displaystyle{\lim_{x\to\infty}}(f(x) - g(x)) = 0‎ ‎‎‎\displaystyle{\lim_{x\to\infty}}‎\frac{f(x)}{g(x)} = 1‎\nRightarrow‎‎‎‎‎\displaystyle{\lim_{x\to\infty}}(f(x) - g(x)) = 0‎ ‎f(x) = x^2 + x‎ ‎g(x) =‎ ‎x^2‎,"['real-analysis', 'limits']"
14,Does every uncountable subset of $\mathbb{R}$ have an uncountable closed subset?,Does every uncountable subset of  have an uncountable closed subset?,\mathbb{R},"Let $E\subseteq \mathbb{R}^1$ be an uncountable set. Can we obtain   some subset $F\subseteq E$ which is closed and uncountable? Basically, I want to construct some set containing only irrational numbers which is also uncountable and closed, in a sense, I want to know a general process to construct such sets. Thank you!","Let $E\subseteq \mathbb{R}^1$ be an uncountable set. Can we obtain   some subset $F\subseteq E$ which is closed and uncountable? Basically, I want to construct some set containing only irrational numbers which is also uncountable and closed, in a sense, I want to know a general process to construct such sets. Thank you!",,['real-analysis']
15,composition of $L^{p}$ functions,composition of  functions,L^{p},"Suppose $f, g\in L^{p}(\mathbb R), (1\leq p < \infty).$ For simplicity, let us assume that, $g,f:\mathbb R\to \mathbb R$ so that composition of $f$ and $g$, namely,  $f\circ g(x)= f(g(x)); (x\in \mathbb R)$ is well-defined. My Question is : Given real-valued functions $f,g \in L^{p}(\mathbb R).$ Can we expect its composition $f\circ g$ is again in $L^{p}(\mathbb R)$ ? If not, under what condition on $f$ one can expect that, $f\circ g\in L^{p}(\mathbb R)$ for all $g\in L^{p}(\mathbb R).$ ? Thanks,","Suppose $f, g\in L^{p}(\mathbb R), (1\leq p < \infty).$ For simplicity, let us assume that, $g,f:\mathbb R\to \mathbb R$ so that composition of $f$ and $g$, namely,  $f\circ g(x)= f(g(x)); (x\in \mathbb R)$ is well-defined. My Question is : Given real-valued functions $f,g \in L^{p}(\mathbb R).$ Can we expect its composition $f\circ g$ is again in $L^{p}(\mathbb R)$ ? If not, under what condition on $f$ one can expect that, $f\circ g\in L^{p}(\mathbb R)$ for all $g\in L^{p}(\mathbb R).$ ? Thanks,",,"['real-analysis', 'analysis', 'lp-spaces']"
16,A Binet-like integral $\int_{0}^{1} \left(\frac{1}{\ln x} + \frac{1}{1-x} -\frac{1}{2} \right) \frac{x^s }{1-x}\mathrm{d}x$,A Binet-like integral,\int_{0}^{1} \left(\frac{1}{\ln x} + \frac{1}{1-x} -\frac{1}{2} \right) \frac{x^s }{1-x}\mathrm{d}x,"I met this integral $$ \int_{0}^{1} \left(\frac{1}{\ln x} + \frac{1}{1-x} -\frac{1}{2} \right) \frac{ \mathrm{d}x}{1-x} \qquad (*) $$ while evaluating this log-cosine integral . I made several attemps before being successful. Find a closed form for $$ I(s): = \int_{0}^{1} \left(\frac{1}{\ln x} + \frac{1}{1-x} -\frac{1}{2} \right) \frac{x^s }{1-x}\mathrm{d}x, \quad \Re (s)>-1. \qquad (**) $$","I met this integral $$ \int_{0}^{1} \left(\frac{1}{\ln x} + \frac{1}{1-x} -\frac{1}{2} \right) \frac{ \mathrm{d}x}{1-x} \qquad (*) $$ while evaluating this log-cosine integral . I made several attemps before being successful. Find a closed form for $$ I(s): = \int_{0}^{1} \left(\frac{1}{\ln x} + \frac{1}{1-x} -\frac{1}{2} \right) \frac{x^s }{1-x}\mathrm{d}x, \quad \Re (s)>-1. \qquad (**) $$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'logarithms']"
17,"Prove the derivative of $x^2 \sin (1/x^2)$ is not (Lebesgue) integrable on $[0,1]$",Prove the derivative of  is not (Lebesgue) integrable on,"x^2 \sin (1/x^2) [0,1]","Prove the derivative of $x^2 \sin (1/x^2)$ is not Lebesgue integrable on $[0,1]$. Note at $x=0$, the value of the function is defined to be $0$. Here 'not integrable' means that the integral value approximated by simple functions from the above is not the same as that by the ones from the below. Should I use some powerful theorem to prove this? I don't think this question is a hard one but don't know what kind of approach I should take.","Prove the derivative of $x^2 \sin (1/x^2)$ is not Lebesgue integrable on $[0,1]$. Note at $x=0$, the value of the function is defined to be $0$. Here 'not integrable' means that the integral value approximated by simple functions from the above is not the same as that by the ones from the below. Should I use some powerful theorem to prove this? I don't think this question is a hard one but don't know what kind of approach I should take.",,['real-analysis']
18,"Prove that $\int\limits_0^1 x^a(1-x)^{-1}\ln x \,dx = -\sum\limits_{n=1}^\infty \frac{1}{(n+a)^2}$",Prove that,"\int\limits_0^1 x^a(1-x)^{-1}\ln x \,dx = -\sum\limits_{n=1}^\infty \frac{1}{(n+a)^2}","Prove that $$\int_0^1 x^a(1-x)^{-1}\ln x \,dx = -\sum_{n=1}^\infty \frac{1}{(n+a)^2}$$ I know that we have a product of $x^a$, $\displaystyle\sum_{n=0}^\infty x^n$, and $\displaystyle\sum_{n=0}^\infty \frac{(1-x)^n}{n}$, but it hasn't helped me so far. Any tips? We are given that $a>-1$.","Prove that $$\int_0^1 x^a(1-x)^{-1}\ln x \,dx = -\sum_{n=1}^\infty \frac{1}{(n+a)^2}$$ I know that we have a product of $x^a$, $\displaystyle\sum_{n=0}^\infty x^n$, and $\displaystyle\sum_{n=0}^\infty \frac{(1-x)^n}{n}$, but it hasn't helped me so far. Any tips? We are given that $a>-1$.",,"['calculus', 'real-analysis']"
19,Every ordered field has a subfield isomorphic to $\mathbb Q$?,Every ordered field has a subfield isomorphic to ?,\mathbb Q,"I'm going through the first chapter in a text on real analysis, which contains preliminaries on ordered fields, the real numbers, etc. Supposedly I had learned about such things already, in calculus, but I thought it wouldn't hurt to go over it again. Up until the paragraph which is the subject of my question, everything is thoroughly proven and examples are provided. However the following excerpt just goes through facts, whose proof I cannot conceive: Although $\mathbb Q$ is an archimedean field, these properties cannot be used to define $\mathbb Q$ since $\\$ there are many Archimedean ordered fields. What distinguishes $\mathbb Q$ from the other $\\$Archimedean ordered fields is that $(\mathbb Q,<)$ is the $smallest$ ordered field in the following sense: $\\$ if $(X,\prec)$ is an ordered field, then $X$ contains a sub-field which is (field) isomorphic to $\mathbb Q$.$\\$ Furthermore such an isomorphism preserves the order relation. I've concatenated the paragraph a bit, so the text isn't idem from the book (if anyone wishes to know it's Phillips, An Introduction to Analysis and Integration Theory , Dover Publications). Also $\mathbb Q$ contains no proper subfields, and this can be verified by the apparently easy-to-prove fact that, if a field has characteristic zero, then it contains a subfield isomorphic to $\mathbb Q$. Then it remains to prove the preservation of order. I'm guessing that some concepts from abstract algebra or field theory would easily suffice, but at the moment such topics are a bit over my head, so all I can think of doing is actually constructing the isomorphism $\phi:\mathbb Q\to\hat{\mathbb Q}$, where $\hat{\mathbb Q}$ is a certain subfield of  $X$ in such a way that field operations are preserved, but I really can't come up with anything. So the question is: how do I construct this isomorphism, or, if there's a better way of proving this, how is it done? Thanks for any help. Edit: Assume the existence of $(\mathbb Q,<)$ as an archimedean ordered field.","I'm going through the first chapter in a text on real analysis, which contains preliminaries on ordered fields, the real numbers, etc. Supposedly I had learned about such things already, in calculus, but I thought it wouldn't hurt to go over it again. Up until the paragraph which is the subject of my question, everything is thoroughly proven and examples are provided. However the following excerpt just goes through facts, whose proof I cannot conceive: Although $\mathbb Q$ is an archimedean field, these properties cannot be used to define $\mathbb Q$ since $\\$ there are many Archimedean ordered fields. What distinguishes $\mathbb Q$ from the other $\\$Archimedean ordered fields is that $(\mathbb Q,<)$ is the $smallest$ ordered field in the following sense: $\\$ if $(X,\prec)$ is an ordered field, then $X$ contains a sub-field which is (field) isomorphic to $\mathbb Q$.$\\$ Furthermore such an isomorphism preserves the order relation. I've concatenated the paragraph a bit, so the text isn't idem from the book (if anyone wishes to know it's Phillips, An Introduction to Analysis and Integration Theory , Dover Publications). Also $\mathbb Q$ contains no proper subfields, and this can be verified by the apparently easy-to-prove fact that, if a field has characteristic zero, then it contains a subfield isomorphic to $\mathbb Q$. Then it remains to prove the preservation of order. I'm guessing that some concepts from abstract algebra or field theory would easily suffice, but at the moment such topics are a bit over my head, so all I can think of doing is actually constructing the isomorphism $\phi:\mathbb Q\to\hat{\mathbb Q}$, where $\hat{\mathbb Q}$ is a certain subfield of  $X$ in such a way that field operations are preserved, but I really can't come up with anything. So the question is: how do I construct this isomorphism, or, if there's a better way of proving this, how is it done? Thanks for any help. Edit: Assume the existence of $(\mathbb Q,<)$ as an archimedean ordered field.",,"['real-analysis', 'field-theory', 'ordered-fields', 'real-numbers']"
20,Polynomial long division: different answers when reordering terms,Polynomial long division: different answers when reordering terms,,"When I use polynomial long division to divide   $\frac{1}{1-x}$,   I get $\;1 + x + x^2 +x^3 + x^4 + \cdots$ But when I just change the order of terms in the divisor:   $\frac{1}{-x+1}$,   the long division algorithm gives me a very different answer: $-\frac{1}{x} - \frac{1}{x^2} - \frac{1}{x^3} - \frac{1}{x^4} - \cdots$,   which seems somewhat strange to me, because sums are supposed to be commutative, so it looks that these two answers should be equivalent. Am I right? But I cannot see how could these two answers be equivalent. Could it be true that $-\frac{1}{x} - \frac{1}{x^2} - \frac{1}{x^3} - \frac{1}{x^4} - \cdots \;=\; 1 + x + x^2 +x^3 + x^4 + \cdots$   ? If that is the case, how can i ""prove"" it? Or how can I transform one form into the other? I already figured out that I can express   $-\frac{1}{x} - \frac{1}{x^2} - \frac{1}{x^3} - \frac{1}{x^4} - \cdots$   as negative exponents:   $-x^{-1} - x^{-2} - x^{-3} - x^{-4} - \cdots$   and then factor the $-1$'s  inside the exponents:   $-x^{-1\cdot1} - x^{-1\cdot2} - x^{-1\cdot3} - x^{-1\cdot4} - \cdots$   which can be seen as a power of a power:   $-(x^{-1})^1 - (x^{-1})^2 - (x^{-1})^3 - (x^{-1})^4 - \cdots$   and the minus sign can be factored out too:   $-\left[ (x^{-1})^1 + (x^{-1})^2 + (x^{-1})^3 + (x^{-1})^4 + \cdots \right]$   which almost gives me the form of the other series with raising positive exponents. But there's that dangling minus sign in front of it, and the 0 th power term is missing :-/ so I don't know how to massage it any further to get the other form of the expansion. Any ideas? Edit: I know about Taylor series expansions and I can expand  $\frac{1}{1-x}$   this way into   $1 + x + x^2 + x^3 + \cdots$   (I don't know how to get the other form this way, though). Also, I know about the convergence condition for the infinite series,   $|x| < 1$   in the case of the first answer, and   $|x| > 1$   or   $|\frac{1}{x}| < 1$   in the case of the second answer. I just didn't expect that this could matter in my case, when I just reorder the terms in the divisor, which shouldn't produce different answer. I suspected that this could be somehow related to the fact that raising powers of a fraction is the same as falling powers of its inverse. But I cannot figure out why just one of these or the other is given, depending on the order of terms in the long division, and this is what confuses me the most.","When I use polynomial long division to divide   $\frac{1}{1-x}$,   I get $\;1 + x + x^2 +x^3 + x^4 + \cdots$ But when I just change the order of terms in the divisor:   $\frac{1}{-x+1}$,   the long division algorithm gives me a very different answer: $-\frac{1}{x} - \frac{1}{x^2} - \frac{1}{x^3} - \frac{1}{x^4} - \cdots$,   which seems somewhat strange to me, because sums are supposed to be commutative, so it looks that these two answers should be equivalent. Am I right? But I cannot see how could these two answers be equivalent. Could it be true that $-\frac{1}{x} - \frac{1}{x^2} - \frac{1}{x^3} - \frac{1}{x^4} - \cdots \;=\; 1 + x + x^2 +x^3 + x^4 + \cdots$   ? If that is the case, how can i ""prove"" it? Or how can I transform one form into the other? I already figured out that I can express   $-\frac{1}{x} - \frac{1}{x^2} - \frac{1}{x^3} - \frac{1}{x^4} - \cdots$   as negative exponents:   $-x^{-1} - x^{-2} - x^{-3} - x^{-4} - \cdots$   and then factor the $-1$'s  inside the exponents:   $-x^{-1\cdot1} - x^{-1\cdot2} - x^{-1\cdot3} - x^{-1\cdot4} - \cdots$   which can be seen as a power of a power:   $-(x^{-1})^1 - (x^{-1})^2 - (x^{-1})^3 - (x^{-1})^4 - \cdots$   and the minus sign can be factored out too:   $-\left[ (x^{-1})^1 + (x^{-1})^2 + (x^{-1})^3 + (x^{-1})^4 + \cdots \right]$   which almost gives me the form of the other series with raising positive exponents. But there's that dangling minus sign in front of it, and the 0 th power term is missing :-/ so I don't know how to massage it any further to get the other form of the expansion. Any ideas? Edit: I know about Taylor series expansions and I can expand  $\frac{1}{1-x}$   this way into   $1 + x + x^2 + x^3 + \cdots$   (I don't know how to get the other form this way, though). Also, I know about the convergence condition for the infinite series,   $|x| < 1$   in the case of the first answer, and   $|x| > 1$   or   $|\frac{1}{x}| < 1$   in the case of the second answer. I just didn't expect that this could matter in my case, when I just reorder the terms in the divisor, which shouldn't produce different answer. I suspected that this could be somehow related to the fact that raising powers of a fraction is the same as falling powers of its inverse. But I cannot figure out why just one of these or the other is given, depending on the order of terms in the long division, and this is what confuses me the most.",,"['real-analysis', 'sequences-and-series', 'polynomials']"
21,An inequality from the handbook of mathematical functions (by Abramowitz and Stegun),An inequality from the handbook of mathematical functions (by Abramowitz and Stegun),,"Prove that $$\frac{1}{x+\sqrt{x^2+2}}<e^{x^2}\int\limits_x^{\infty}e^{-t^2} \, \text dt \le\frac{1}{x+\sqrt{x^2+\displaystyle\tfrac{4}{\pi}}}, \space (x\ge 0)$$","Prove that $$\frac{1}{x+\sqrt{x^2+2}}<e^{x^2}\int\limits_x^{\infty}e^{-t^2} \, \text dt \le\frac{1}{x+\sqrt{x^2+\displaystyle\tfrac{4}{\pi}}}, \space (x\ge 0)$$",,"['calculus', 'real-analysis', 'inequality', 'special-functions', 'integral-inequality']"
22,Prove that $\int_0^1|f''(x)|dx\ge4.$,Prove that,\int_0^1|f''(x)|dx\ge4.,"Let $f$ be a $C^2$ function on $[0,1]$ . $f(0)=f(1)=f'(0)=0,f'(1)=1.$ Prove that $$\int_0^1|f''(x)| \, dx\ge4.$$ Also determine all possible $f$ when equality occurs.",Let be a function on . Prove that Also determine all possible when equality occurs.,"f C^2 [0,1] f(0)=f(1)=f'(0)=0,f'(1)=1. \int_0^1|f''(x)| \, dx\ge4. f","['calculus', 'real-analysis', 'inequality', 'definite-integrals', 'functional-inequalities']"
23,Find the value of : $\lim_{n\to\infty}\prod_{k=1}^n\cos\left(\frac{ka}{n\sqrt{n}}\right)$,Find the value of :,\lim_{n\to\infty}\prod_{k=1}^n\cos\left(\frac{ka}{n\sqrt{n}}\right),Find the limit (where a is a constant) $\lim_{n\to\infty}\prod_{k=1}^n\cos\left(\frac{ka}{n\sqrt{n}}\right)$ I think the answer is $1-a^2/6$,Find the limit (where a is a constant) $\lim_{n\to\infty}\prod_{k=1}^n\cos\left(\frac{ka}{n\sqrt{n}}\right)$ I think the answer is $1-a^2/6$,,"['calculus', 'real-analysis', 'limits', 'infinite-product']"
24,"Riemann-Stieltjes integral, integration by parts (Rudin)","Riemann-Stieltjes integral, integration by parts (Rudin)",,"Problem 17 of Chapter 6 of Rudin's Principles of Mathematical Analysis asks us to prove the following: Suppose $\alpha$ increases monotonically on $[a,b]$, $g$ is   continuous, and $g(x)=G'(x)$ for $a \leq x \leq b$. Prove that, $$\int_a^b\alpha(x)g(x)\,dx=G(b)\alpha(b)-G(a)\alpha(a)-\int_a^bG\,d\alpha.$$ It seems to me that the continuity of $g$ is not necessary for the result above. It is enough to assume that $g$ is Riemann integrable. Am I right in thinking this? I have thought as follows: $\int_a^bG\,d\alpha$ exists because $G$ is differentiable and hence continuous. $\alpha(x)$ is integrable with respect to $x$ since it is monotonic. If $g(x)$ is also integrable with respect to $x$ then $\int_a^b\alpha(x)g(x)\,dx$ also exists. To prove the given formula, I start from the hint given by Rudin $$\sum_{i=1}^n\alpha(x_i)g(t_i)\Delta x_i=G(b)\alpha(b)-G(a)\alpha(a)-\sum_{i=1}^nG(x_{i-1})\Delta \alpha_i$$ where $g(t_i)\Delta x_i=\Delta G_i$ by the intermediate mean value theorem. Now the sum on the right-hand side converges to $\int_a^bG\,d\alpha$. The sum on the left-hand side would have converged to $\int_a^b\alpha(x)g(x)\,dx$ if it had been $$\sum_{i=1}^n \alpha(x_i)g(x_i)\Delta x$$ The absolute difference between this and what we have is bounded above by $$\max(|\alpha(a)|,|\alpha(b)|)\sum_{i=1}^n |g(x_i)-g(t_i)|\Delta x$$ and this can be made arbitrarily small because $g(x)$ is integrable with respect to $x$.","Problem 17 of Chapter 6 of Rudin's Principles of Mathematical Analysis asks us to prove the following: Suppose $\alpha$ increases monotonically on $[a,b]$, $g$ is   continuous, and $g(x)=G'(x)$ for $a \leq x \leq b$. Prove that, $$\int_a^b\alpha(x)g(x)\,dx=G(b)\alpha(b)-G(a)\alpha(a)-\int_a^bG\,d\alpha.$$ It seems to me that the continuity of $g$ is not necessary for the result above. It is enough to assume that $g$ is Riemann integrable. Am I right in thinking this? I have thought as follows: $\int_a^bG\,d\alpha$ exists because $G$ is differentiable and hence continuous. $\alpha(x)$ is integrable with respect to $x$ since it is monotonic. If $g(x)$ is also integrable with respect to $x$ then $\int_a^b\alpha(x)g(x)\,dx$ also exists. To prove the given formula, I start from the hint given by Rudin $$\sum_{i=1}^n\alpha(x_i)g(t_i)\Delta x_i=G(b)\alpha(b)-G(a)\alpha(a)-\sum_{i=1}^nG(x_{i-1})\Delta \alpha_i$$ where $g(t_i)\Delta x_i=\Delta G_i$ by the intermediate mean value theorem. Now the sum on the right-hand side converges to $\int_a^bG\,d\alpha$. The sum on the left-hand side would have converged to $\int_a^b\alpha(x)g(x)\,dx$ if it had been $$\sum_{i=1}^n \alpha(x_i)g(x_i)\Delta x$$ The absolute difference between this and what we have is bounded above by $$\max(|\alpha(a)|,|\alpha(b)|)\sum_{i=1}^n |g(x_i)-g(t_i)|\Delta x$$ and this can be made arbitrarily small because $g(x)$ is integrable with respect to $x$.",,"['real-analysis', 'integration']"
25,Prove that $ 1.462 \le \int_0^1 e^{{x}^{2}}\le 1.463$,Prove that, 1.462 \le \int_0^1 e^{{x}^{2}}\le 1.463,"Prove the following integral inequality: $$ 1.462 \le \int_0^1 e^{{x}^{2}}\le 1.463$$ This is a high school problem. So far i did manage to prove that the integral is bigger than $1.462$ by using Taylor expansion, namely: $$1.462\le 1.4625=\int_0^1 1+x^2+\frac{x^4}{2}+\frac{x^6}{6}+\frac{x^8}{24}+\frac{x^{10}}{120}\le  \int_0^1 e^{{x}^{2}}$$ For the right bound i'm still looking for a way. However, i wonder if there is an elegant way to solve both sides.","Prove the following integral inequality: $$ 1.462 \le \int_0^1 e^{{x}^{2}}\le 1.463$$ This is a high school problem. So far i did manage to prove that the integral is bigger than $1.462$ by using Taylor expansion, namely: $$1.462\le 1.4625=\int_0^1 1+x^2+\frac{x^4}{2}+\frac{x^6}{6}+\frac{x^8}{24}+\frac{x^{10}}{120}\le  \int_0^1 e^{{x}^{2}}$$ For the right bound i'm still looking for a way. However, i wonder if there is an elegant way to solve both sides.",,"['calculus', 'real-analysis', 'integration', 'inequality', 'definite-integrals']"
26,Is every countable dense subset of $\mathbb R$ ambiently homeomorphic to $\mathbb Q$,Is every countable dense subset of  ambiently homeomorphic to,\mathbb R \mathbb Q,"Let $S$ be a countable dense subset of $\mathbb R$. Must there exist a homeomorphism $f: \mathbb R \rightarrow \mathbb R$ such that $f(S) = \mathbb Q$? More weakly, must $S$ be homeomorphic to $\mathbb Q$?","Let $S$ be a countable dense subset of $\mathbb R$. Must there exist a homeomorphism $f: \mathbb R \rightarrow \mathbb R$ such that $f(S) = \mathbb Q$? More weakly, must $S$ be homeomorphic to $\mathbb Q$?",,"['real-analysis', 'general-topology']"
27,Banach Indicatrix Function,Banach Indicatrix Function,,"Let $f: [a,b] \rightarrow \mathbb R$ be of bounded variation. For $y \in \mathbb R$, define the Banach Indicatrix of $y$ by $N(y) = \# f^{pre} (y)$, ie. $N(y)$ is the cardinality of the pre-image of $y$ under $f$. I seek to prove the following: . (a) N(y) is finite for almost every $y \in \mathbb R$ (b) The function $y \mapsto N(y)$ is measurable. (c) The total variation of $f$ is given by $TV(f) = \int_c ^d N(y) dy$. I have tried partitioning $[a,b]$ and looking at the variation over them in order to bound the size of the set where $N(y)$ is infinite, but this hasn't yielded much success.","Let $f: [a,b] \rightarrow \mathbb R$ be of bounded variation. For $y \in \mathbb R$, define the Banach Indicatrix of $y$ by $N(y) = \# f^{pre} (y)$, ie. $N(y)$ is the cardinality of the pre-image of $y$ under $f$. I seek to prove the following: . (a) N(y) is finite for almost every $y \in \mathbb R$ (b) The function $y \mapsto N(y)$ is measurable. (c) The total variation of $f$ is given by $TV(f) = \int_c ^d N(y) dy$. I have tried partitioning $[a,b]$ and looking at the variation over them in order to bound the size of the set where $N(y)$ is infinite, but this hasn't yielded much success.",,"['real-analysis', 'measure-theory', 'functional-analysis']"
28,Integrating the exponential of distance to a convex bounded set,Integrating the exponential of distance to a convex bounded set,,"I've been nerdsniped by a friend of mine trying to solve a sample analysis qualifying exam, and one of the problems I'm trying to figure out is the following: Let $E \subseteq \mathbb{R}^2$ be a convex bounded set. Determine $$\int_{\mathbb{R}^2} e^{-\mathrm{dist}(\mathbf{x},E)}\, d\mathbf{x}$$ in terms of the Lebesgue measure $\mathcal{L}^2(E)$ and the Hausdorff measure $\mathcal{H}^1(\partial E)$ . I've figured out the first step, breaking apart $\mathbb{R}^2 = \bar{E} \cup \mathbb{R}^2\setminus \bar{E}.$ Then, in $\bar{E}$ , $\mathrm{dist}(\mathbf{x},E) = 0$ so that bit of the integral just becomes $\mathcal{L}^2(\bar{E}) = \mathcal{L}^2(E\cup \partial E) = \mathcal{L}^2(E)$ since $E$ is convex and hence $\mathcal{L}^2(\partial E) = 0$ . It's the integral over the complement that I'm now stuck on. I'm not looking for a complete solution - I'd like to figure as much of this out on my own as I can, but any hints towards the next steps would be very much appreciated. Thanks very much!","I've been nerdsniped by a friend of mine trying to solve a sample analysis qualifying exam, and one of the problems I'm trying to figure out is the following: Let be a convex bounded set. Determine in terms of the Lebesgue measure and the Hausdorff measure . I've figured out the first step, breaking apart Then, in , so that bit of the integral just becomes since is convex and hence . It's the integral over the complement that I'm now stuck on. I'm not looking for a complete solution - I'd like to figure as much of this out on my own as I can, but any hints towards the next steps would be very much appreciated. Thanks very much!","E \subseteq \mathbb{R}^2 \int_{\mathbb{R}^2} e^{-\mathrm{dist}(\mathbf{x},E)}\, d\mathbf{x} \mathcal{L}^2(E) \mathcal{H}^1(\partial E) \mathbb{R}^2 = \bar{E} \cup \mathbb{R}^2\setminus \bar{E}. \bar{E} \mathrm{dist}(\mathbf{x},E) = 0 \mathcal{L}^2(\bar{E}) = \mathcal{L}^2(E\cup \partial E) = \mathcal{L}^2(E) E \mathcal{L}^2(\partial E) = 0","['real-analysis', 'integration', 'measure-theory']"
29,"Prove that $\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1$ for $n$ real numbers $a_i\in(-1,1)$",Prove that  for  real numbers,"\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1 n a_i\in(-1,1)","I'm trying to prove the following inequality: $$\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1\tag{1}$$ for $a_i\in(-1,1)$ . I first tried induction but doesn't seem to work well. Special cases can be proved like $n=2,3$ using brute force but I am trying to find a simpler proof. Some observations: Define the function $f_n(x_1,x_2,\ldots,x_n)=\prod_{1\leq i,j\leq n}\frac{1+x_ix_j}{1-x_ix_j}$ for $(x_1,\ldots,x_n)\in(-1,1)^n$ . Note that if $a_i\geq0$ for all $i$ then all the terms in the product are at least $1$ and hence $(1)$ is trivially true. So, is it possible to prove the following? $$f_n(x_1,x_2,\ldots,x_n)\geq1\iff f_n(-x_1,x_2,\ldots,x_n)\geq1$$ , if yes then by symmetry we can repeat this process and make all $a_i\geq0$ . Also, using Induction we can do the following: Base case $n=1$ is trivial. Assuming for some $n\geq1$ , we see that $$f_n(x_1,x_2,\ldots,x_n)f_n(-x_1,x_2,\ldots,x_n)=f_{n-1}(x_2,\ldots,x_n)^2\geq1$$ and hence at least one of the following is true: $f_n(x_1,x_2,\ldots,x_n)\geq1$ or $f_n(-x_1,x_2,\ldots,x_n)\geq1$ . The case of equality As @RiverLi pointed out and @MartinR wrote an answer, it is evident that equality holds if and only if $$\sum_{k=1}^{\infty}\frac{1}{2k-1}\left( \sum_{i=1}^n a_i^{2k-1}\right)^2=0$$ The partial sums form an increasing sequence of nonnegative reals. So, the series can evaluate to zero if and only if $$\sum_{i=1}^na_i^{2k-1}=0,k\geq1$$ which is a separate and interesting problem and is discussed here.","I'm trying to prove the following inequality: for . I first tried induction but doesn't seem to work well. Special cases can be proved like using brute force but I am trying to find a simpler proof. Some observations: Define the function for . Note that if for all then all the terms in the product are at least and hence is trivially true. So, is it possible to prove the following? , if yes then by symmetry we can repeat this process and make all . Also, using Induction we can do the following: Base case is trivial. Assuming for some , we see that and hence at least one of the following is true: or . The case of equality As @RiverLi pointed out and @MartinR wrote an answer, it is evident that equality holds if and only if The partial sums form an increasing sequence of nonnegative reals. So, the series can evaluate to zero if and only if which is a separate and interesting problem and is discussed here.","\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1\tag{1} a_i\in(-1,1) n=2,3 f_n(x_1,x_2,\ldots,x_n)=\prod_{1\leq i,j\leq n}\frac{1+x_ix_j}{1-x_ix_j} (x_1,\ldots,x_n)\in(-1,1)^n a_i\geq0 i 1 (1) f_n(x_1,x_2,\ldots,x_n)\geq1\iff f_n(-x_1,x_2,\ldots,x_n)\geq1 a_i\geq0 n=1 n\geq1 f_n(x_1,x_2,\ldots,x_n)f_n(-x_1,x_2,\ldots,x_n)=f_{n-1}(x_2,\ldots,x_n)^2\geq1 f_n(x_1,x_2,\ldots,x_n)\geq1 f_n(-x_1,x_2,\ldots,x_n)\geq1 \sum_{k=1}^{\infty}\frac{1}{2k-1}\left( \sum_{i=1}^n a_i^{2k-1}\right)^2=0 \sum_{i=1}^na_i^{2k-1}=0,k\geq1","['real-analysis', 'algebra-precalculus', 'inequality', 'contest-math']"
30,"Dimension for a closed subspace of $C[0,1]$.",Dimension for a closed subspace of .,"C[0,1]","Let $X \subset C^1[0,1]$ be a closed subspace of $C[0,1]$ (with sup norm). Prove that $X$ has to be finite-dimensional.","Let $X \subset C^1[0,1]$ be a closed subspace of $C[0,1]$ (with sup norm). Prove that $X$ has to be finite-dimensional.",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
31,"Is there a construction of a perfect set that is nowhere dense that is not done in a ""Cantor set"" manner?","Is there a construction of a perfect set that is nowhere dense that is not done in a ""Cantor set"" manner?",,"Is it possible to construct a set that is perfect and nowhere dense that is not done in the same way that a ""Cantor like"" set is - by removing a certain fixed middle percentage of the initial interval and then iterating. Cantor used 1/3 and others have used various percentiles. Is it possible to construct a set in a way that is not similar to this process?","Is it possible to construct a set that is perfect and nowhere dense that is not done in the same way that a ""Cantor like"" set is - by removing a certain fixed middle percentage of the initial interval and then iterating. Cantor used 1/3 and others have used various percentiles. Is it possible to construct a set in a way that is not similar to this process?",,"['real-analysis', 'general-topology']"
32,Singular continuous functions,Singular continuous functions,,"A function $f:[0,1]\rightarrow\mathbb{R}$ is called singular continuous, if it is nonconstant, nondecreasing, continuous and $f^\prime(t)=0$ whereever the derivative exists. Let $f$ be a singular continuous function and $T$ the set where $f$ is not differentiable. Question: Is $T$ nowhere dense? Examples: A classical example of such a function is the so-called devil's staircase, obtained as a limit of increasing step functions constructed in a similar way as the classical Cantor set. In that case, the set in question is the Cantor set, which is closed and of measure zero, therefore nowhere dense. Another example, as suggested by David Mitra below, is the Minkowski question mark function. In that case I'm not aware of any characterization of $T$ which allows to decide whether its nowhere dense or not. If you have another (interesting) example for a singular continuous function, please give a comment. Edits/Notes/Progress: It is clear that $T$ has Lebesgue measure $0$ . In view of the correspondence of nondecreasing functions with positive measures, singular continuous functions correspond to singular continuous measures, i.e. an atomless positive Borel measures concentrated on a set of Lebesgue measure zero. The set $T$ corresponds very roughly to that ""set of concentration"". Despite of that I would like an answer avoiding measure theory or probability language . My entire knowledge on singular continuous functions is spanned by the information contained in this question. If you possess any further insights into the concept or references containing such, please don't hesitate too leave a comment. It is highly appreciated!","A function is called singular continuous, if it is nonconstant, nondecreasing, continuous and whereever the derivative exists. Let be a singular continuous function and the set where is not differentiable. Question: Is nowhere dense? Examples: A classical example of such a function is the so-called devil's staircase, obtained as a limit of increasing step functions constructed in a similar way as the classical Cantor set. In that case, the set in question is the Cantor set, which is closed and of measure zero, therefore nowhere dense. Another example, as suggested by David Mitra below, is the Minkowski question mark function. In that case I'm not aware of any characterization of which allows to decide whether its nowhere dense or not. If you have another (interesting) example for a singular continuous function, please give a comment. Edits/Notes/Progress: It is clear that has Lebesgue measure . In view of the correspondence of nondecreasing functions with positive measures, singular continuous functions correspond to singular continuous measures, i.e. an atomless positive Borel measures concentrated on a set of Lebesgue measure zero. The set corresponds very roughly to that ""set of concentration"". Despite of that I would like an answer avoiding measure theory or probability language . My entire knowledge on singular continuous functions is spanned by the information contained in this question. If you possess any further insights into the concept or references containing such, please don't hesitate too leave a comment. It is highly appreciated!","f:[0,1]\rightarrow\mathbb{R} f^\prime(t)=0 f T f T T T 0 T",['real-analysis']
33,What does countable union mean?,What does countable union mean?,,"The book I am reading contains the following two definitions: Two sets $A$ and $B$ have the same cardinality if there exists $f: A \rightarrow B$ that is one to one and onto. In this case, we write $A \sim B$ . A set $A$ is countable if $\mathbb{N} \sim A$ . An infinite set that is not countable is called an uncountable set. Following on, I read the following statement: Every open set is either a finite or countable union of open intervals. Here, what does countable union mean? Clearly it can't mean that the resultant set formed by the union of open intervals is countable (since open intervals are uncountable). But I am not sure how the use of ""countable union"" connects with the definition provided earlier.","The book I am reading contains the following two definitions: Two sets and have the same cardinality if there exists that is one to one and onto. In this case, we write . A set is countable if . An infinite set that is not countable is called an uncountable set. Following on, I read the following statement: Every open set is either a finite or countable union of open intervals. Here, what does countable union mean? Clearly it can't mean that the resultant set formed by the union of open intervals is countable (since open intervals are uncountable). But I am not sure how the use of ""countable union"" connects with the definition provided earlier.",A B f: A \rightarrow B A \sim B A \mathbb{N} \sim A,"['real-analysis', 'self-learning']"
34,Why is the undergraph definition of Lebesgue integral so rare?,Why is the undergraph definition of Lebesgue integral so rare?,,"So in Pugh's Real Mathematical Analysis, the initial definition of the Lebesgue integral is as the Lebesgue measure of the undergraph of the function (where the function is nonnegative, with the usual extension to functions whose sign changes). This is extremely intuitive, and immediately justifies all the abstract work of extending our notion of volume with the concept of measure. Another definition is given in the main exposition, and yet another is given in the exercises. i.e. the undergraph definition is: $\int_Ef = m(Uf)$, where $Uf = \{(x, y) \mid x \in E, \ \ 0 \leq y \leq f(x)$ Now I'm working through the more advanced Real Analysis by Folland, where the integral is defined as the supremum of integrals of simple functions, whose integrals are fairly obvious. Additionally, in a graduate course, we used an equivalent definition as the Riemann integral of the measure of the superlevel sets, i.e. $\int_E f = \int_0^\infty m(\{ x \mid f(x) > t\})dt$ Nowhere in the book does it seem to mention the undergraph definition, and googling around it's very hard to find mention of the undergraph definition at all. I realize why the simple function definition might be better to work with in developing the theory (since it often suffices to prove our theorems for simple functions that approximate much uglier ones), but I'm wondering if there's any particular reason why the undergraph definition seems so neglected, given that the main motivation we're given for the very first time we're introduced to the concept of integration is to compute the area below a curve. Perhaps the definition suffers from a serious limitation?","So in Pugh's Real Mathematical Analysis, the initial definition of the Lebesgue integral is as the Lebesgue measure of the undergraph of the function (where the function is nonnegative, with the usual extension to functions whose sign changes). This is extremely intuitive, and immediately justifies all the abstract work of extending our notion of volume with the concept of measure. Another definition is given in the main exposition, and yet another is given in the exercises. i.e. the undergraph definition is: $\int_Ef = m(Uf)$, where $Uf = \{(x, y) \mid x \in E, \ \ 0 \leq y \leq f(x)$ Now I'm working through the more advanced Real Analysis by Folland, where the integral is defined as the supremum of integrals of simple functions, whose integrals are fairly obvious. Additionally, in a graduate course, we used an equivalent definition as the Riemann integral of the measure of the superlevel sets, i.e. $\int_E f = \int_0^\infty m(\{ x \mid f(x) > t\})dt$ Nowhere in the book does it seem to mention the undergraph definition, and googling around it's very hard to find mention of the undergraph definition at all. I realize why the simple function definition might be better to work with in developing the theory (since it often suffices to prove our theorems for simple functions that approximate much uglier ones), but I'm wondering if there's any particular reason why the undergraph definition seems so neglected, given that the main motivation we're given for the very first time we're introduced to the concept of integration is to compute the area below a curve. Perhaps the definition suffers from a serious limitation?",,"['real-analysis', 'integration', 'soft-question', 'lebesgue-integral']"
35,"$f \in {\mathscr R[a,b]} \implies f $ has infinitely many points of continuity.",has infinitely many points of continuity.,"f \in {\mathscr R[a,b]} \implies f ","Claim: If $f \in {\mathscr R[a,b]}$, then $f$ has infinitely many points of continuity. 1.) I read that it is a corollary of the Lebesgue integrability criterion.  Is it possible to prove the claim without invoking the concept of measure(or using less abstraction) ? 2.) Here is attempt: Given $\epsilon > 0, \exists$ Partition, $P =  \left\{ x_0 =a,...,x_n =b \right\} $ of $[a,b]$ such that $\sum^n_{i=1} (M_i -m_i)\Delta x_i < (b-a)\epsilon$, where  $M_i= \sup \left\{ f(x) : x \in \Delta x_i \right\}$ and $ m_i= \inf \left\{ f(x) : x \in \Delta x_i \right\} $ Let $(M_j -m_j)=\min \left\{ M_i -m_i : i=0,...,n \right\} $.  $ \implies (M_j-m_j)(b-a) \leq \sum^n_{i=1} (M_i -m_i)\Delta x_i <  (b-a)\epsilon$ $ \implies (M_j-m_j)<\epsilon. $ Let $ c\in (x_{j-1},x_j)$ and $\delta$ be any positive number such that $(c-\delta, c+\delta) \subseteq (x_{j-1}, x_j)$. It follows that $ \left| f(x) - f(c) \right| < \epsilon, $ whenever $ \left| x - c \right| < \delta $. Since $c$ is arbitrary and $[x_{j-1},x_j]$ is an interval,  there are infinitely many points of continuity. There is something wrong with my proof since Thomae's function is a counterexample. Could anyone point out the mistakes in my proof? Thank you.","Claim: If $f \in {\mathscr R[a,b]}$, then $f$ has infinitely many points of continuity. 1.) I read that it is a corollary of the Lebesgue integrability criterion.  Is it possible to prove the claim without invoking the concept of measure(or using less abstraction) ? 2.) Here is attempt: Given $\epsilon > 0, \exists$ Partition, $P =  \left\{ x_0 =a,...,x_n =b \right\} $ of $[a,b]$ such that $\sum^n_{i=1} (M_i -m_i)\Delta x_i < (b-a)\epsilon$, where  $M_i= \sup \left\{ f(x) : x \in \Delta x_i \right\}$ and $ m_i= \inf \left\{ f(x) : x \in \Delta x_i \right\} $ Let $(M_j -m_j)=\min \left\{ M_i -m_i : i=0,...,n \right\} $.  $ \implies (M_j-m_j)(b-a) \leq \sum^n_{i=1} (M_i -m_i)\Delta x_i <  (b-a)\epsilon$ $ \implies (M_j-m_j)<\epsilon. $ Let $ c\in (x_{j-1},x_j)$ and $\delta$ be any positive number such that $(c-\delta, c+\delta) \subseteq (x_{j-1}, x_j)$. It follows that $ \left| f(x) - f(c) \right| < \epsilon, $ whenever $ \left| x - c \right| < \delta $. Since $c$ is arbitrary and $[x_{j-1},x_j]$ is an interval,  there are infinitely many points of continuity. There is something wrong with my proof since Thomae's function is a counterexample. Could anyone point out the mistakes in my proof? Thank you.",,['real-analysis']
36,"Dense uncountable proper subgroup of $(\mathbb{R},+)$",Dense uncountable proper subgroup of,"(\mathbb{R},+)","Probably someone had asked this question on StackExchange, but can one construct a dense uncountable proper subgroup of $(\mathbb{R},+)$?","Probably someone had asked this question on StackExchange, but can one construct a dense uncountable proper subgroup of $(\mathbb{R},+)$?",,"['real-analysis', 'general-topology', 'group-theory']"
37,How convergence relates to equivalence of norms,How convergence relates to equivalence of norms,,"Let $X$ be a normed linear space with two norms $||\cdot||_1$ and $||\cdot||_2$. Prove or disprove that this statements are equivalent: $||\cdot||_1$ and $||\cdot||_2$ are equivalent, $\{x_n\}$ converges in $||\cdot||_1$ iff $\{x_n\}$ converges in $||\cdot||_2\;\; $   $\forall \{x_n\}\in X$ The first implies the second trivially because of equivalence of topologies. But it implies something more: sequence converges to the same element in both norms. But I don't have such condition. Is this a necessary condition to converge to the same in both norms, or this is just a consequent of ""if and only if"" in the statement of convergence, or there is a example disproving the equivalence?","Let $X$ be a normed linear space with two norms $||\cdot||_1$ and $||\cdot||_2$. Prove or disprove that this statements are equivalent: $||\cdot||_1$ and $||\cdot||_2$ are equivalent, $\{x_n\}$ converges in $||\cdot||_1$ iff $\{x_n\}$ converges in $||\cdot||_2\;\; $   $\forall \{x_n\}\in X$ The first implies the second trivially because of equivalence of topologies. But it implies something more: sequence converges to the same element in both norms. But I don't have such condition. Is this a necessary condition to converge to the same in both norms, or this is just a consequent of ""if and only if"" in the statement of convergence, or there is a example disproving the equivalence?",,"['real-analysis', 'normed-spaces']"
38,Nonlinear Fubini-Tonelli?,Nonlinear Fubini-Tonelli?,,"A student raised the question today: can one interchange the integrals in $$ \int_0^1 \left(\int_0^x f(y) \, dy \right)^2 \, dx $$ for sufficiently nice functions $f$.  I answered no , with the reasoning that integrals are fundamentally linear operations. However now that I think about it, it is certainly possible that some kind of non-linear analogy to Fubini-Tonelli exists and I'm simply not aware of it.  Indeed, it seems quite plausible that some interchanged expression involving square roots is equal to the expression above, and perhaps in general something involving the inverse of the non-linear function of the inner integral. Is anyone aware of a non-linear Fubini-Tonelli theorem?","A student raised the question today: can one interchange the integrals in $$ \int_0^1 \left(\int_0^x f(y) \, dy \right)^2 \, dx $$ for sufficiently nice functions $f$.  I answered no , with the reasoning that integrals are fundamentally linear operations. However now that I think about it, it is certainly possible that some kind of non-linear analogy to Fubini-Tonelli exists and I'm simply not aware of it.  Indeed, it seems quite plausible that some interchanged expression involving square roots is equal to the expression above, and perhaps in general something involving the inverse of the non-linear function of the inner integral. Is anyone aware of a non-linear Fubini-Tonelli theorem?",,"['real-analysis', 'calculus', 'integration', 'fubini-tonelli-theorems']"
39,the Riemann integrability of inverse function,the Riemann integrability of inverse function,,"If $f \colon [a,b] \rightarrow [c,d]$ is a bijection, $f\in \mathcal{R}$ and $f^{-1}$ exists, then prove or disprove that $f^{-1} \in \mathcal{R} [c,d]$. Remark: I tried to use integration by parts to find $\int_{c}^{d} f^{-1}$ and to prove that was the right limit of Riemann sum, but failed. But I think this idea might be useful.","If $f \colon [a,b] \rightarrow [c,d]$ is a bijection, $f\in \mathcal{R}$ and $f^{-1}$ exists, then prove or disprove that $f^{-1} \in \mathcal{R} [c,d]$. Remark: I tried to use integration by parts to find $\int_{c}^{d} f^{-1}$ and to prove that was the right limit of Riemann sum, but failed. But I think this idea might be useful.",,['real-analysis']
40,"$a_n>0$, $\sum a_n^2<+\infty$, and $\sum a_{n_k}=+\infty$ whenever $\lim\limits_{k\to\infty}\frac{n_{k+1}}{n_k}=1\;?$",", , and  whenever",a_n>0 \sum a_n^2<+\infty \sum a_{n_k}=+\infty \lim\limits_{k\to\infty}\frac{n_{k+1}}{n_k}=1\;?,"Does there exist a sequence $\{a_n\}$ such that: $a_n>0$ for all positive integer $n$ , $\sum\limits_{n=1}^{\infty}a_n^2<+\infty$ , and $\sum\limits_{k=1}^{\infty}a_{n_k}=+\infty$ for any subsequence $\{a_{n_k}\}$ satisfying $\lim\limits_{k\to\infty}\frac{n_{k+1}}{n_k}=1\;?$ Note that common simple sequences like $a_n=\frac{1}{n}$ fail to meet the third condition. For example $n_k=\lfloor k(\log k)^2\rfloor+1$ then $\lim\limits_{k\to\infty}\frac{n_{k+1}}{n_k}=1$ and $\sum\limits_{k=1}^{\infty}\frac{1}{n_k}\leq 1+\sum\limits_{k=2}^{\infty}\frac{1}{k(\log k)^2}<+\infty$ .","Does there exist a sequence such that: for all positive integer , , and for any subsequence satisfying Note that common simple sequences like fail to meet the third condition. For example then and .",\{a_n\} a_n>0 n \sum\limits_{n=1}^{\infty}a_n^2<+\infty \sum\limits_{k=1}^{\infty}a_{n_k}=+\infty \{a_{n_k}\} \lim\limits_{k\to\infty}\frac{n_{k+1}}{n_k}=1\;? a_n=\frac{1}{n} n_k=\lfloor k(\log k)^2\rfloor+1 \lim\limits_{k\to\infty}\frac{n_{k+1}}{n_k}=1 \sum\limits_{k=1}^{\infty}\frac{1}{n_k}\leq 1+\sum\limits_{k=2}^{\infty}\frac{1}{k(\log k)^2}<+\infty,"['real-analysis', 'sequences-and-series', 'limits']"
41,Prove that $\frac{\ln x}{x}+\frac{1}{e^x}<\frac{1}{2}$ for $x > 0$,Prove that  for,\frac{\ln x}{x}+\frac{1}{e^x}<\frac{1}{2} x > 0,"Given $x>0$ , prove that $$\frac{\ln x}{x}+\frac{1}{e^x}<\frac{1}{2}$$ I have tried to construct $F(x)=\frac{\ln x}{x}+\frac{1}{e^x}$ and find the derivative function of $F(x)$ to find the maximun value, but I can't solve the transcendental equation. So I tried another way.I tried to use the inequality $e^{-x}\le \frac{1}{x+1}$ (when $x>0$ ) to prove the inequality $\frac{\ln x}{x}<\frac{1}{2}-\frac{1}{e^x}$ , but I can't connect these two inequalities well, and I did't solve the problem in the end. How can I do?","Given , prove that I have tried to construct and find the derivative function of to find the maximun value, but I can't solve the transcendental equation. So I tried another way.I tried to use the inequality (when ) to prove the inequality , but I can't connect these two inequalities well, and I did't solve the problem in the end. How can I do?",x>0 \frac{\ln x}{x}+\frac{1}{e^x}<\frac{1}{2} F(x)=\frac{\ln x}{x}+\frac{1}{e^x} F(x) e^{-x}\le \frac{1}{x+1} x>0 \frac{\ln x}{x}<\frac{1}{2}-\frac{1}{e^x},"['real-analysis', 'calculus', 'inequality']"
42,Check whether $f \mapsto f+ \frac{df}{dx}$ is injective or surjective,Check whether  is injective or surjective,f \mapsto f+ \frac{df}{dx},Consider maps $C^{\infty} \to C^{\infty}$ s.t $f \mapsto f+ \frac{df}{dx}$ . We have to check whether this map is injective or surjective. My try: The map is clearly not injective as $x$ and $x+e^{-x}$ maps to $x+1$ . Now to check whether the map is surjective. Consider $g \in C^{\infty}$ . Then I was thinking in this way that considering $\int_0^xg$ then $f=g-\int_0^xg$ now $f+\frac{df}{dx}=g-\int_0^xg+\frac{dg}{dx}-g=-\int_0^xg+\frac{dg}{dx}$ still I am not getting a proof whether it is surjective or not.,Consider maps s.t . We have to check whether this map is injective or surjective. My try: The map is clearly not injective as and maps to . Now to check whether the map is surjective. Consider . Then I was thinking in this way that considering then now still I am not getting a proof whether it is surjective or not.,C^{\infty} \to C^{\infty} f \mapsto f+ \frac{df}{dx} x x+e^{-x} x+1 g \in C^{\infty} \int_0^xg f=g-\int_0^xg f+\frac{df}{dx}=g-\int_0^xg+\frac{dg}{dx}-g=-\int_0^xg+\frac{dg}{dx},"['real-analysis', 'calculus', 'integration', 'analysis', 'derivatives']"
43,Is uniform continuity related to the rate of change of the function?,Is uniform continuity related to the rate of change of the function?,,"I have been trying to understand how uniformly continuous functions differ from functions that are continuous but not uniformly continuous. Based on some examples and counter-examples, my feeling is that uniformly continuous functions cannot change ""rapidly"". Hopefully, the following examples will make it clear what I mean by that: $f(x) = 1/x$ is not uniformly continuous on $(0,1)$ because it ""blows up"" at $0$, that is, its slope becomes infinite very quickly. Similar functions would be $\log x$ on $(0,1)$ and $\tan x$ on $(0,\pi/2)$. $f(x) = x^2$ is not uniformly continuous on $(0,\infty)$ because it goes to infinity ""rapidly"" in the sense that the function's rate of change is greater than that of linear functions, for sufficiently large $x$. $f(x) = x^{1/3}$ is uniformly continuous on $(-1,1)$, because even though the slope does become infinite at $0$, it does not happen ""rapidly"" as in the previous two examples. $f(x) = \sin (1/x)$ is not uniformly continuous on $(0,1)$ because it oscillates ""rapidly"" as $x$ approaches $0$. $f(x) = x\sin(1/x)$ is uniformly continuous on $(-1,1)$ because, although the frequency of oscillation remains the same, the amplitude is ""small enough"" to make the function uniformly continuous; in some sense, the function is not changing rapidly enough for it to fail to be uniformly continuous. This is admittedly a very naive way of looking at uniform continuity, but does it have any merit? Is there a way to describe uniform continuity by rigorously formulating the notion of rate of change of the function? Some difficulties I already see are: There are continuous functions that are not differentiable, so the natural idea of rate of change is not applicable to them. There even exist continuous but nowhere differentiable functions on $\mathbb{R}$, and we know that any continuous function on a compact subset of $\mathbb{R}$ is uniformly continuous. So, the Weierstrass function restricted to $[0,1]$ is a uniformly continuous function. Surely, the function changes ""rapidly"" all over the place, but somehow it doesn't do so ""rapidly enough""? I have tagged this as a soft question because I know there is a lot of ambiguity in this post. It would be very helpful if anyone can give me insights about why this method of thinking of uniformly continuous functions (at least in the real one-variable case) can or cannot be fruitful. Also, this is a related post that I found useful, especially the most upvoted answer: Why did mathematicians introduce the concept of uniform continuity?","I have been trying to understand how uniformly continuous functions differ from functions that are continuous but not uniformly continuous. Based on some examples and counter-examples, my feeling is that uniformly continuous functions cannot change ""rapidly"". Hopefully, the following examples will make it clear what I mean by that: $f(x) = 1/x$ is not uniformly continuous on $(0,1)$ because it ""blows up"" at $0$, that is, its slope becomes infinite very quickly. Similar functions would be $\log x$ on $(0,1)$ and $\tan x$ on $(0,\pi/2)$. $f(x) = x^2$ is not uniformly continuous on $(0,\infty)$ because it goes to infinity ""rapidly"" in the sense that the function's rate of change is greater than that of linear functions, for sufficiently large $x$. $f(x) = x^{1/3}$ is uniformly continuous on $(-1,1)$, because even though the slope does become infinite at $0$, it does not happen ""rapidly"" as in the previous two examples. $f(x) = \sin (1/x)$ is not uniformly continuous on $(0,1)$ because it oscillates ""rapidly"" as $x$ approaches $0$. $f(x) = x\sin(1/x)$ is uniformly continuous on $(-1,1)$ because, although the frequency of oscillation remains the same, the amplitude is ""small enough"" to make the function uniformly continuous; in some sense, the function is not changing rapidly enough for it to fail to be uniformly continuous. This is admittedly a very naive way of looking at uniform continuity, but does it have any merit? Is there a way to describe uniform continuity by rigorously formulating the notion of rate of change of the function? Some difficulties I already see are: There are continuous functions that are not differentiable, so the natural idea of rate of change is not applicable to them. There even exist continuous but nowhere differentiable functions on $\mathbb{R}$, and we know that any continuous function on a compact subset of $\mathbb{R}$ is uniformly continuous. So, the Weierstrass function restricted to $[0,1]$ is a uniformly continuous function. Surely, the function changes ""rapidly"" all over the place, but somehow it doesn't do so ""rapidly enough""? I have tagged this as a soft question because I know there is a lot of ambiguity in this post. It would be very helpful if anyone can give me insights about why this method of thinking of uniformly continuous functions (at least in the real one-variable case) can or cannot be fruitful. Also, this is a related post that I found useful, especially the most upvoted answer: Why did mathematicians introduce the concept of uniform continuity?",,"['real-analysis', 'soft-question']"
44,Calculating number of equivalence classes where two points are equivalent if they can be joined by a continuous path.,Calculating number of equivalence classes where two points are equivalent if they can be joined by a continuous path.,,"Q. Let $G$ be an open set in $\Bbb R^n$. Two points $x,y \in G$ are said to be equivalent if they can be joined by a continuous path completely lying inside $G$. Number of equivalence classes is Only one. At most finite. At most countable. Can be finite, countable or uncountable. This question was asked in the NET exam December 2016. We can discard the first option by taking $n=1$ and $G=(-\infty,0) \cup (0,\infty)$. We can reject the second option by taking $n=1$ and $G=\cup_{k \in \Bbb Z} (k,k+1).$ Now fun begins. Can we get an uncountable number of disjoint open path connected subsets of $\Bbb R^n$ for some $n$? If so, then we can take $G$ to be their union. For $n=1$, this method fails because that would give us the contradiction that the set of irrational numbers is countable.","Q. Let $G$ be an open set in $\Bbb R^n$. Two points $x,y \in G$ are said to be equivalent if they can be joined by a continuous path completely lying inside $G$. Number of equivalence classes is Only one. At most finite. At most countable. Can be finite, countable or uncountable. This question was asked in the NET exam December 2016. We can discard the first option by taking $n=1$ and $G=(-\infty,0) \cup (0,\infty)$. We can reject the second option by taking $n=1$ and $G=\cup_{k \in \Bbb Z} (k,k+1).$ Now fun begins. Can we get an uncountable number of disjoint open path connected subsets of $\Bbb R^n$ for some $n$? If so, then we can take $G$ to be their union. For $n=1$, this method fails because that would give us the contradiction that the set of irrational numbers is countable.",,"['real-analysis', 'continuity', 'equivalence-relations', 'path-connected']"
45,"Is there a probability measure on $[0,1]$ with no subsets with measure $\frac{1}{2}$?",Is there a probability measure on  with no subsets with measure ?,"[0,1] \frac{1}{2}","I have a decidedly weird question. Does there exist a probability measure $(\mu, \mathcal{F})$ on $[0,1]$ such that 1) $\mu(x) = 0$ for every $x \in [0,1]$ 2) For every $r \in [0,1] \setminus \lbrace \frac{1}{2} \rbrace$, there exists $A \in \mathcal{F}$ with $\mu(A) = r$, and 3) There is no $A \in \mathcal{F}$ with $\mu(A) = \frac{1}{2}$? The question comes about from a prelim problem in which the existence of a measure-$\frac{1}{2}$ set was assumed, and it made me wonder whether the assumption was for convenience or necessary.  Pigeonhole principle?  Ultrafilters?","I have a decidedly weird question. Does there exist a probability measure $(\mu, \mathcal{F})$ on $[0,1]$ such that 1) $\mu(x) = 0$ for every $x \in [0,1]$ 2) For every $r \in [0,1] \setminus \lbrace \frac{1}{2} \rbrace$, there exists $A \in \mathcal{F}$ with $\mu(A) = r$, and 3) There is no $A \in \mathcal{F}$ with $\mu(A) = \frac{1}{2}$? The question comes about from a prelim problem in which the existence of a measure-$\frac{1}{2}$ set was assumed, and it made me wonder whether the assumption was for convenience or necessary.  Pigeonhole principle?  Ultrafilters?",,"['real-analysis', 'functional-analysis', 'probability-theory', 'measure-theory', 'descriptive-set-theory']"
46,Can a sequence of unbounded functions be uniformly convergent?,Can a sequence of unbounded functions be uniformly convergent?,,If I say have a sequence of functions $\displaystyle x^3 + 5\frac{x}{n}$ . I know that I can say it is point-wise convergent as $n$ goes to infinity to the limit function $x^3$ but can I simply say that as it isn't bounded it CAN NOT be uniformly convergent?,If I say have a sequence of functions . I know that I can say it is point-wise convergent as goes to infinity to the limit function but can I simply say that as it isn't bounded it CAN NOT be uniformly convergent?,\displaystyle x^3 + 5\frac{x}{n} n x^3,"['real-analysis', 'uniform-convergence']"
47,Are convex functions enough to determine a measure?,Are convex functions enough to determine a measure?,,"Suppose we are talking about $\mathbb{R}^n$. We know that if $\mu$, $\nu$ are two finite Borel measures such that $$\int_{\mathbb{R}^n}f(x) \, d\mu(x)=\int_{\mathbb{R}^n}f(x) \, d\nu(x),$$ for all continuous functions $f$, then $\mu$ and $\nu$ are really the same measure. Now, suppose the equation only holds for all convex functions. Is it still true that $\mu$ and $\nu$ are the same measure? Edit: As Emanuele Paolini has pointed out, there is a counterexample to the original question. So, what if we further restrict $\mu$ and $\nu$ to have compact support?","Suppose we are talking about $\mathbb{R}^n$. We know that if $\mu$, $\nu$ are two finite Borel measures such that $$\int_{\mathbb{R}^n}f(x) \, d\mu(x)=\int_{\mathbb{R}^n}f(x) \, d\nu(x),$$ for all continuous functions $f$, then $\mu$ and $\nu$ are really the same measure. Now, suppose the equation only holds for all convex functions. Is it still true that $\mu$ and $\nu$ are the same measure? Edit: As Emanuele Paolini has pointed out, there is a counterexample to the original question. So, what if we further restrict $\mu$ and $\nu$ to have compact support?",,"['real-analysis', 'measure-theory', 'convex-analysis']"
48,Integration by parts for general measure?,Integration by parts for general measure?,,"Let $\mu$ be a general measure, suppose $f,g$ has compact support on $\mathbb{R}$, when does the integration by parts formula hold $$\int f'g d\mu = - \int g'fd\mu?$$ I know in general this is false, we can take $\mu$ to be supported on a point, say $0$, then it is not necessarily true that  $$f'(0)g(0) = -g'(0)f(0).$$ If $\mu$ is absolute continuous w.r.t. Lebesgue measure, we have $\frac{d\mu}{dx} = h$ $$\int f'gd\mu = \int f'gh dx = -\int f(gh)'dx$$ where $(gh)'dx$ might be a measure. but we can not recover the form $\int g'fh dx$. Thank you very much!","Let $\mu$ be a general measure, suppose $f,g$ has compact support on $\mathbb{R}$, when does the integration by parts formula hold $$\int f'g d\mu = - \int g'fd\mu?$$ I know in general this is false, we can take $\mu$ to be supported on a point, say $0$, then it is not necessarily true that  $$f'(0)g(0) = -g'(0)f(0).$$ If $\mu$ is absolute continuous w.r.t. Lebesgue measure, we have $\frac{d\mu}{dx} = h$ $$\int f'gd\mu = \int f'gh dx = -\int f(gh)'dx$$ where $(gh)'dx$ might be a measure. but we can not recover the form $\int g'fh dx$. Thank you very much!",,"['real-analysis', 'integration', 'measure-theory', 'partial-differential-equations']"
49,Stuck on crucial step while computing $\int_{- \infty}^{\infty} e^{-t^2}dt$,Stuck on crucial step while computing,\int_{- \infty}^{\infty} e^{-t^2}dt,"This is a not mandatory exercise I am struggling with from my Analysis II Class, at the very end of it I am supposed to compute $$\int_{-\infty}^\infty e^{-t^2}dt \tag{*}$$ The most famous way to do that is switch to polar coordinates but I am not supposed to do it that way, one of my tutors even called it a 'dirty trick'. Instead the exercise gives us the following two Integrals to work with: $$F(x)= \int_0^1 \frac{e^{-x^2(1+t^2)}}{1+t^2}dt \text{ and } G(x)= \left( \int_0^x e^{-t^2}dt\right)^2, \ \forall x \in \mathbb{R} $$ I yet fail to see how these two integrals relate to (*) but I can naively follow the instructions given by exercise sheet and hope that the result will follow: Problem : Show that $F,G$ are of Class $C^1(\mathbb{R})$, compute $F',G'$ for all $x \in \mathbb{R}$ and show that $F+G$ is constant My approach : There isn't much to do really rather than to quote some Lemmas, clearly the parametric integral $F$ is of class $C^1$ because $f(x,t): \mathbb{R} \times [0,1] \to \mathbb{R}$ given by $f(x,t)=e^{-x^2(1+t^2)}/(1+t^2)$ is of class $C^1$, I can therefore switch the partial derivative with the integral and obtain that  $$F'(x) =-2 x \int_0^1e^{-x^2(1+t^2)}dt$$ I doubt that I can further simplify this integral. With a little help of the fundamental theorem of Calculus one obtains that $$G'(x)= 2 \left( \int_0^x e^{-t^2}dt\right)e^{-x^2} $$ At this point I hope that the exercise (or my computation) is flawed, at least the limit points of the integrals because I wouldn't know how to show that $F'+G'=0$ for all $x\in \mathbb{R}$ to conclude that $F+G$ is constant. The only sense I could make out of $F'$ would be if I integrated it with respect to $x$ rather to $t$. Are there any further simplifications I can do?","This is a not mandatory exercise I am struggling with from my Analysis II Class, at the very end of it I am supposed to compute $$\int_{-\infty}^\infty e^{-t^2}dt \tag{*}$$ The most famous way to do that is switch to polar coordinates but I am not supposed to do it that way, one of my tutors even called it a 'dirty trick'. Instead the exercise gives us the following two Integrals to work with: $$F(x)= \int_0^1 \frac{e^{-x^2(1+t^2)}}{1+t^2}dt \text{ and } G(x)= \left( \int_0^x e^{-t^2}dt\right)^2, \ \forall x \in \mathbb{R} $$ I yet fail to see how these two integrals relate to (*) but I can naively follow the instructions given by exercise sheet and hope that the result will follow: Problem : Show that $F,G$ are of Class $C^1(\mathbb{R})$, compute $F',G'$ for all $x \in \mathbb{R}$ and show that $F+G$ is constant My approach : There isn't much to do really rather than to quote some Lemmas, clearly the parametric integral $F$ is of class $C^1$ because $f(x,t): \mathbb{R} \times [0,1] \to \mathbb{R}$ given by $f(x,t)=e^{-x^2(1+t^2)}/(1+t^2)$ is of class $C^1$, I can therefore switch the partial derivative with the integral and obtain that  $$F'(x) =-2 x \int_0^1e^{-x^2(1+t^2)}dt$$ I doubt that I can further simplify this integral. With a little help of the fundamental theorem of Calculus one obtains that $$G'(x)= 2 \left( \int_0^x e^{-t^2}dt\right)e^{-x^2} $$ At this point I hope that the exercise (or my computation) is flawed, at least the limit points of the integrals because I wouldn't know how to show that $F'+G'=0$ for all $x\in \mathbb{R}$ to conclude that $F+G$ is constant. The only sense I could make out of $F'$ would be if I integrated it with respect to $x$ rather to $t$. Are there any further simplifications I can do?",,"['real-analysis', 'integration', 'analysis']"
50,"Prove that $\{\frac{\phi (n)}{n}\}_{n \in \Bbb N}$ is dense in $[0,1]$",Prove that  is dense in,"\{\frac{\phi (n)}{n}\}_{n \in \Bbb N} [0,1]","suppose that $\phi(n)$ is Euler function. prove that, $\{\frac{\phi (n)}{n}\}_{n \in \Bbb N}$ is dense in $[0,1]$ (if $A_n=\{1 \leq m \leq n \mid m \in \Bbb N ; \gcd(n,m)=1\}$ then $\phi(n)=|A_n|$ ) I think : If $p$ is prime then $\phi(p)=p-1$ . for any $ \varepsilon > 0 $ there is a prime number $p$ such that $1-\varepsilon \leq \frac{p-1}{p} \leq 1$ . for other elements i don't know what can i do.","suppose that is Euler function. prove that, is dense in (if then ) I think : If is prime then . for any there is a prime number such that . for other elements i don't know what can i do.","\phi(n) \{\frac{\phi (n)}{n}\}_{n \in \Bbb N} [0,1] A_n=\{1 \leq m \leq n \mid m \in \Bbb N ; \gcd(n,m)=1\} \phi(n)=|A_n| p \phi(p)=p-1  \varepsilon > 0  p 1-\varepsilon \leq \frac{p-1}{p} \leq 1","['real-analysis', 'number-theory']"
51,How prove this limit $\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{i+j}{i^2+j^2}=\frac{\pi}{2}+\ln{2}$,How prove this limit,\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{i+j}{i^2+j^2}=\frac{\pi}{2}+\ln{2},show that: this limit $$I=\lim_{n\to\infty}\dfrac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\dfrac{i+j}{i^2+j^2}=\dfrac{\pi}{2}+\ln{2}$$ My try:  $$I=\lim_{n\to\infty}\dfrac{1}{n^2}\sum_{i=1}^{n}\sum_{j=1}^{n}\dfrac{\dfrac{i}{n}+\dfrac{j}{n}}{\left(\dfrac{i}{n}\right)^2+\left(\dfrac{j}{n}\right)^2}=\int_{0}^{1}\int_{0}^{1}\dfrac{x+y}{x^2+y^2}dxdy?$$ This idea is true? and have other methods?,show that: this limit $$I=\lim_{n\to\infty}\dfrac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\dfrac{i+j}{i^2+j^2}=\dfrac{\pi}{2}+\ln{2}$$ My try:  $$I=\lim_{n\to\infty}\dfrac{1}{n^2}\sum_{i=1}^{n}\sum_{j=1}^{n}\dfrac{\dfrac{i}{n}+\dfrac{j}{n}}{\left(\dfrac{i}{n}\right)^2+\left(\dfrac{j}{n}\right)^2}=\int_{0}^{1}\int_{0}^{1}\dfrac{x+y}{x^2+y^2}dxdy?$$ This idea is true? and have other methods?,,"['real-analysis', 'limits']"
52,"Is it possible to have $f(x)f''(x) \leq -1$ for all $x \in [0,\infty)$?",Is it possible to have  for all ?,"f(x)f''(x) \leq -1 x \in [0,\infty)","Let $f : [0,\infty) \longrightarrow \Bbb{R}$ and suppose that $f''$ exists. Is it possible to have $f(x)f''(x)  \leq -1$ for all $x$ ?","Let $f : [0,\infty) \longrightarrow \Bbb{R}$ and suppose that $f''$ exists. Is it possible to have $f(x)f''(x)  \leq -1$ for all $x$ ?",,"['calculus', 'real-analysis']"
53,Properties of a continuous map $f : \mathbb{R}^2\rightarrow \mathbb{R}$ with only finitely many zeroes,Properties of a continuous map  with only finitely many zeroes,f : \mathbb{R}^2\rightarrow \mathbb{R},"Let  $f : \mathbb{R}^2\rightarrow \mathbb{R}$ be a continuous map such that $f(x)=0$ only for finitely many values of $x$. Which of the following is true? Either $f(x)\leq 0$ for all $x$ or $f(x)\geq 0$ for all $x$. the map $f$ is onto. the map $f$ is one one. None of the above. What I have done so far is : I would take polynomial in two variables.. This need not be like $f(x)\geq 0$ for all $x$ or $f(x)\geq 0$ for all $x$.So,first option is eliminated. The map is not one one assuming that $f$ has more than one zero. So, third option is wrong. I could not think of an example in which it is not onto.. Only examples i am getting in my mind are polynomials and they are onto.. So, I am having trouble with surjectiveness of the function. Please help me to clear this. Thank you..","Let  $f : \mathbb{R}^2\rightarrow \mathbb{R}$ be a continuous map such that $f(x)=0$ only for finitely many values of $x$. Which of the following is true? Either $f(x)\leq 0$ for all $x$ or $f(x)\geq 0$ for all $x$. the map $f$ is onto. the map $f$ is one one. None of the above. What I have done so far is : I would take polynomial in two variables.. This need not be like $f(x)\geq 0$ for all $x$ or $f(x)\geq 0$ for all $x$.So,first option is eliminated. The map is not one one assuming that $f$ has more than one zero. So, third option is wrong. I could not think of an example in which it is not onto.. Only examples i am getting in my mind are polynomials and they are onto.. So, I am having trouble with surjectiveness of the function. Please help me to clear this. Thank you..",,['real-analysis']
54,"A sequence of continuous functions on $[0,1]$ which converge pointwise a.e. but does not converge uniformly on any interval",A sequence of continuous functions on  which converge pointwise a.e. but does not converge uniformly on any interval,"[0,1]","How to construct a sequence of functions that are defined and continuous on $[0,1]$ and it converges to zero a.e. but on any interval it does not converge uniformly?","How to construct a sequence of functions that are defined and continuous on $[0,1]$ and it converges to zero a.e. but on any interval it does not converge uniformly?",,['real-analysis']
55,"Equivalent bounded metric: Why should one prefer $\frac{d}{1+d}$ over $\min\{d,1\}$?",Equivalent bounded metric: Why should one prefer  over ?,"\frac{d}{1+d} \min\{d,1\}","This is my first question and I hope it is not considered too argumentative. It is often useful to change the metric on a space to an equivalent bounded metric. Traditionally, people use $$ \delta(x,y) = \frac{d(x,y)}{1+d(x,y)}. $$ For example for the metric on product spaces: one equips $$ \prod_{i=1}^\infty X_i $$ with the metric $d(x,y) = \sum_{i=1}^\infty \frac{1}{2^{i+1}} \delta_i(x,y) = \sum_{i=1}^\infty \frac{1}{2^{i+1}} \frac{d_i(x,y)}{1+d_i(x,y)}$. Similarly, when equipping the unit ball of the dual space of a separable normed space with a metric inducing the weak$^\ast$-topology, people tend to use a  construction using $\delta$. Or when metrizing convergence in measure on a probability space. Is there any real advantage of this metric over, say, $$ d'(x,y) = \min\{d(x,y),1\}\: ? $$ It is much easier to check that $d'$ is an equivalent metric on $(X,d)$ than proving that $\delta$ is an equivalent metric. So why do people use $\delta$? The only reason I can imagine (apart from tradition) is that knowing $\delta$ one can recover $d$. But is this reason enough to use this more complicated construction?","This is my first question and I hope it is not considered too argumentative. It is often useful to change the metric on a space to an equivalent bounded metric. Traditionally, people use $$ \delta(x,y) = \frac{d(x,y)}{1+d(x,y)}. $$ For example for the metric on product spaces: one equips $$ \prod_{i=1}^\infty X_i $$ with the metric $d(x,y) = \sum_{i=1}^\infty \frac{1}{2^{i+1}} \delta_i(x,y) = \sum_{i=1}^\infty \frac{1}{2^{i+1}} \frac{d_i(x,y)}{1+d_i(x,y)}$. Similarly, when equipping the unit ball of the dual space of a separable normed space with a metric inducing the weak$^\ast$-topology, people tend to use a  construction using $\delta$. Or when metrizing convergence in measure on a probability space. Is there any real advantage of this metric over, say, $$ d'(x,y) = \min\{d(x,y),1\}\: ? $$ It is much easier to check that $d'$ is an equivalent metric on $(X,d)$ than proving that $\delta$ is an equivalent metric. So why do people use $\delta$? The only reason I can imagine (apart from tradition) is that knowing $\delta$ one can recover $d$. But is this reason enough to use this more complicated construction?",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
56,How does one visualize a function with a discontinuous second derivative?,How does one visualize a function with a discontinuous second derivative?,,"Let us assume that all functions are continuous. I was teaching my calculus students the other day. We were talking about what points of non-differentiability look like. Two ways a function can fail to be differentiable at a point is if it looks like $y=|x|$ or like a Brownian motion (think of $x\sin x$ for instance), where the derivative oscillates too much. However, I do not have an intuition about $C^1$ functions and how they differ from $C^i$ functions for higher $i$. An example that I know is the function $$f(x)=x^2,x\geq 0\mbox{ and }f(x)=-x^2,x\leq 0.$$ The graph of this actually looks smooth to me. So the question rephrased may be: how can one visually tell the difference between $C^1$ functions and $C^2$ functions in a straight forward way. Although this is for undergrads, I wouldn't mind a more advanced answer.","Let us assume that all functions are continuous. I was teaching my calculus students the other day. We were talking about what points of non-differentiability look like. Two ways a function can fail to be differentiable at a point is if it looks like $y=|x|$ or like a Brownian motion (think of $x\sin x$ for instance), where the derivative oscillates too much. However, I do not have an intuition about $C^1$ functions and how they differ from $C^i$ functions for higher $i$. An example that I know is the function $$f(x)=x^2,x\geq 0\mbox{ and }f(x)=-x^2,x\leq 0.$$ The graph of this actually looks smooth to me. So the question rephrased may be: how can one visually tell the difference between $C^1$ functions and $C^2$ functions in a straight forward way. Although this is for undergrads, I wouldn't mind a more advanced answer.",,"['calculus', 'real-analysis', 'education']"
57,Proving the inequality $\arctan\frac{\pi}{2}\ge1$,Proving the inequality,\arctan\frac{\pi}{2}\ge1,Do you see any nice way to prove that $$\arctan\frac{\pi}{2}\ge1 ?$$ Thanks! Sis.,Do you see any nice way to prove that $$\arctan\frac{\pi}{2}\ge1 ?$$ Thanks! Sis.,,"['calculus', 'real-analysis', 'inequality', 'contest-math']"
58,Uniform convergence of a sequence of polynomials,Uniform convergence of a sequence of polynomials,,"If a sequence of polynomials converge uniformly in $\mathbb{R}$ to $f$, is $f$ necessarily a polynomial?","If a sequence of polynomials converge uniformly in $\mathbb{R}$ to $f$, is $f$ necessarily a polynomial?",,"['calculus', 'real-analysis', 'polynomials', 'convergence-divergence', 'uniform-convergence']"
59,A sequence with infinitely many radicals: $a_{n}=\sqrt{1+\sqrt{a+\sqrt{a^2+\cdots+\sqrt{a^n}}}}$ [duplicate],A sequence with infinitely many radicals:  [duplicate],a_{n}=\sqrt{1+\sqrt{a+\sqrt{a^2+\cdots+\sqrt{a^n}}}},"This question already has answers here : Definition of convergence of a nested radical $\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}}$? (2 answers) Closed 1 year ago . Consider the sequence $\{a_{n}\}$, with $n\ge1$ and $a>0$, defined as: $$a_{n}=\sqrt{1+\sqrt{a+\sqrt{a^2+\cdots+\sqrt{a^n}}}}$$ I'm trying to prove here 2 things: a). the sequence is convergent; b). the sequence's limit when n goes to $\infty$. I may suppose that there must be a proof for this general case. I saw this problem with the case $a=2$ (where it was required to prove only the convergence), but this is just a particular case. The generalization seems to be much more interesting.","This question already has answers here : Definition of convergence of a nested radical $\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}}$? (2 answers) Closed 1 year ago . Consider the sequence $\{a_{n}\}$, with $n\ge1$ and $a>0$, defined as: $$a_{n}=\sqrt{1+\sqrt{a+\sqrt{a^2+\cdots+\sqrt{a^n}}}}$$ I'm trying to prove here 2 things: a). the sequence is convergent; b). the sequence's limit when n goes to $\infty$. I may suppose that there must be a proof for this general case. I saw this problem with the case $a=2$ (where it was required to prove only the convergence), but this is just a particular case. The generalization seems to be much more interesting.",,"['calculus', 'real-analysis', 'sequences-and-series']"
60,Integrable derivative implies absolutely continuous,Integrable derivative implies absolutely continuous,,"Consider Lebesgue integrals over the real line. I have the following problem: Problem: Suppose $F(x)$ is a continuous function in $[a,b]$ , and $F'(x)$ exists everywhere in $(a,b)$ and is integrable. Show $F(x)$ is absolutely continuous. The hint in the book suggested showing that $F'(x)\ge 0$ a.e. implies that $F(x)$ is increasing. I did that, but I do not see how it helps with this problem. How can this hint be used to solve the problem? (I am aware other proofs exist.)","Consider Lebesgue integrals over the real line. I have the following problem: Problem: Suppose is a continuous function in , and exists everywhere in and is integrable. Show is absolutely continuous. The hint in the book suggested showing that a.e. implies that is increasing. I did that, but I do not see how it helps with this problem. How can this hint be used to solve the problem? (I am aware other proofs exist.)","F(x) [a,b] F'(x) (a,b) F(x) F'(x)\ge 0 F(x)","['real-analysis', 'measure-theory']"
61,If $E \in \sigma(\mathcal{C})$ then there exists a countable subset $\mathcal{C}_0 \subseteq \mathcal{C}$ with $E \in \sigma(\mathcal{C}_0)$,If  then there exists a countable subset  with,E \in \sigma(\mathcal{C}) \mathcal{C}_0 \subseteq \mathcal{C} E \in \sigma(\mathcal{C}_0),"Given a collection of sets $\mathcal{C}$ and $E$ an element in the $\sigma$-algebra generated by $\mathcal{C}$, how do I show that $\exists$ a countable subcollection $\mathcal{C_0} \subset \mathcal{C}$ such that $E$ is an element of the $\sigma$-algebra, $\mathcal{A}$ generated by $\mathcal{C_0}$? The hint says to let $H$ be the union of all $\sigma$-algebras generated by countable subsets of $\mathcal{C}$....although I don't know why.","Given a collection of sets $\mathcal{C}$ and $E$ an element in the $\sigma$-algebra generated by $\mathcal{C}$, how do I show that $\exists$ a countable subcollection $\mathcal{C_0} \subset \mathcal{C}$ such that $E$ is an element of the $\sigma$-algebra, $\mathcal{A}$ generated by $\mathcal{C_0}$? The hint says to let $H$ be the union of all $\sigma$-algebras generated by countable subsets of $\mathcal{C}$....although I don't know why.",,"['real-analysis', 'measure-theory']"
62,Change of Variable formula for a non-differentiable mapping.,Change of Variable formula for a non-differentiable mapping.,,"Let $\Omega \subset \Bbb R^n$ . For a diffeomorphism (or merely a differentiable bijection) $\varphi:\Omega \to \varphi(\Omega)$ , we have the formula $$ \int_{\Omega} f\circ\varphi^{-1}(x)\, dx = \int_{\varphi^{-1}(\Omega)} f(y)|D\varphi(y)| \,dy. $$ How much can we generalize the class in which $\varphi$ is allowed to lie in? Is it enough that we have, says, a bijection $\varphi\in W_{\text{loc}}^{1,\infty}(\Omega ;\Bbb R^n)$ or even $\varphi\in W_{\text{loc}}^{1,1}(\Omega ;\Bbb R^n)$ ? How much does the result depends on the domain $\Omega$ ? Is there a big difference between a compact and an open domain? Does the regularity of the boundary $\partial \Omega$ play any role? I'd also appreciate if you have a good reference to this kind of result so that I can read further into this interesting issue. Happy New Year to all of you.","Let . For a diffeomorphism (or merely a differentiable bijection) , we have the formula How much can we generalize the class in which is allowed to lie in? Is it enough that we have, says, a bijection or even ? How much does the result depends on the domain ? Is there a big difference between a compact and an open domain? Does the regularity of the boundary play any role? I'd also appreciate if you have a good reference to this kind of result so that I can read further into this interesting issue. Happy New Year to all of you.","\Omega \subset \Bbb R^n \varphi:\Omega \to \varphi(\Omega) 
\int_{\Omega} f\circ\varphi^{-1}(x)\, dx = \int_{\varphi^{-1}(\Omega)} f(y)|D\varphi(y)| \,dy.
 \varphi \varphi\in W_{\text{loc}}^{1,\infty}(\Omega ;\Bbb R^n) \varphi\in W_{\text{loc}}^{1,1}(\Omega ;\Bbb R^n) \Omega \partial \Omega","['real-analysis', 'measure-theory', 'differential-geometry', 'reference-request', 'partial-differential-equations']"
63,Finding a set of continuous functions with a certain property [duplicate],Finding a set of continuous functions with a certain property [duplicate],,"This question already has answers here : Finding a set of continuous functions with a certain property 2 (4 answers) Closed 5 years ago . I need help finding the set of continuous functions $f : \Bbb R \to \Bbb R$ such that for all $x \in \Bbb R$, the following integral converges: $$\int_0^1 \frac {f(x+t) - f(x)} {t^2} \ \mathrm dt$$ I am thinking it could be the set of constant functions but i havent been able to prove it :(   I have also noticed that you can kind of  take any two functions and stick them together (continuously extend one into the other) the resulting function verifies the property in question. I hope you can provide some insight and thank you .","This question already has answers here : Finding a set of continuous functions with a certain property 2 (4 answers) Closed 5 years ago . I need help finding the set of continuous functions $f : \Bbb R \to \Bbb R$ such that for all $x \in \Bbb R$, the following integral converges: $$\int_0^1 \frac {f(x+t) - f(x)} {t^2} \ \mathrm dt$$ I am thinking it could be the set of constant functions but i havent been able to prove it :(   I have also noticed that you can kind of  take any two functions and stick them together (continuously extend one into the other) the resulting function verifies the property in question. I hope you can provide some insight and thank you .",,"['real-analysis', 'general-topology']"
64,Reference: solved problems and exercises on PDEs,Reference: solved problems and exercises on PDEs,,"I'm looking for books/lecture notes that contain solved problems and exercises on PDE. More specifically, I'm interested in basic 'computational' exercises; more theoretical/advanced problems (also with some functional analytical flavor); problems that are more ""numerical"" in nature (maybe based on software like Matlab or Mathematica). Some references can be found  at Supplemental reference request-Graduate level PDE problems and solutions book , but I'm looking for something more both basic and advanced.","I'm looking for books/lecture notes that contain solved problems and exercises on PDE. More specifically, I'm interested in basic 'computational' exercises; more theoretical/advanced problems (also with some functional analytical flavor); problems that are more ""numerical"" in nature (maybe based on software like Matlab or Mathematica). Some references can be found  at Supplemental reference request-Graduate level PDE problems and solutions book , but I'm looking for something more both basic and advanced.",,"['real-analysis', 'ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'soft-question']"
65,Prove that a Cauchy sequence is convergent,Prove that a Cauchy sequence is convergent,,"I need help understanding this proof that a Cauchy sequence is convergent. Let $(a_n)_n$ be a Cauchy sequence. Let's prove that $(a_n)_n$ is bounded. In the definition of Cauchy sequence:    $$(\forall \varepsilon>0) (\exists n_\varepsilon\in\Bbb N)(\forall n,m\in\Bbb N)((n,m>n_\varepsilon)\Rightarrow(|a_n-a_m|<\varepsilon))$$   let $\varepsilon=1$. Then we have $n_1\in\Bbb N$ such that $\forall n,m\in\Bbb N (n,m>n_1)\Rightarrow(|a_n-a_m|<1)$. From there for $n>n_1$ we have $|a_n|\leq |a_n-a_{n1+1}|+|a_{n1+1}|(*).$ Now $M=\max\{|a_1|,...|a_{n1}|,1+|a_{n1+1}|\}$ such that $|a_n|\leq M,\ \forall n\in\Bbb N.$ Bounded sequence $(a_n)_n$ has a convergent subsequence $(a_{p_n})_n$, i.e. there exists $a=\lim_n a_{p_n}$. Let's prove $a=\lim_n a_n$. Let $\varepsilon>0$ be arbitrary. From the convergence of subsequence $(a_{p_n})_n$ we have $n'_\varepsilon\in\Bbb N$ such that    $$(n>n'_\varepsilon)\Rightarrow(|a_{p_n}-a|<\frac{\varepsilon}{2}).$$   Because $(a_n)_n$ is a Cauchy sequence, we have $n''_\varepsilon\in\Bbb N$ such that    $$(n,m>n''_\varepsilon)\Rightarrow(|a_n-a_m|<\frac{\varepsilon}{2}).$$    Let $n_\varepsilon=\max\{n'_\varepsilon, n''_\varepsilon\}$ so for $n>n_\varepsilon$ because $p_n\geq n$ we have $$|a_n-a|\leq|a_n-a_{p_n}|+|a_{p_n}-a|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon \ (**)$$ i.e.  $a=\lim_n a_n$. $(*)$Where did $|a_n|\leq |a_n-a_{n1+1}|+|a_{n1+1}|$ come from? I understand why that inequality is true, but I don't see the point in writing in like that. $(**)$ Why is $|a_n-a_{p_n}|<\frac{\varepsilon}{2}?$","I need help understanding this proof that a Cauchy sequence is convergent. Let $(a_n)_n$ be a Cauchy sequence. Let's prove that $(a_n)_n$ is bounded. In the definition of Cauchy sequence:    $$(\forall \varepsilon>0) (\exists n_\varepsilon\in\Bbb N)(\forall n,m\in\Bbb N)((n,m>n_\varepsilon)\Rightarrow(|a_n-a_m|<\varepsilon))$$   let $\varepsilon=1$. Then we have $n_1\in\Bbb N$ such that $\forall n,m\in\Bbb N (n,m>n_1)\Rightarrow(|a_n-a_m|<1)$. From there for $n>n_1$ we have $|a_n|\leq |a_n-a_{n1+1}|+|a_{n1+1}|(*).$ Now $M=\max\{|a_1|,...|a_{n1}|,1+|a_{n1+1}|\}$ such that $|a_n|\leq M,\ \forall n\in\Bbb N.$ Bounded sequence $(a_n)_n$ has a convergent subsequence $(a_{p_n})_n$, i.e. there exists $a=\lim_n a_{p_n}$. Let's prove $a=\lim_n a_n$. Let $\varepsilon>0$ be arbitrary. From the convergence of subsequence $(a_{p_n})_n$ we have $n'_\varepsilon\in\Bbb N$ such that    $$(n>n'_\varepsilon)\Rightarrow(|a_{p_n}-a|<\frac{\varepsilon}{2}).$$   Because $(a_n)_n$ is a Cauchy sequence, we have $n''_\varepsilon\in\Bbb N$ such that    $$(n,m>n''_\varepsilon)\Rightarrow(|a_n-a_m|<\frac{\varepsilon}{2}).$$    Let $n_\varepsilon=\max\{n'_\varepsilon, n''_\varepsilon\}$ so for $n>n_\varepsilon$ because $p_n\geq n$ we have $$|a_n-a|\leq|a_n-a_{p_n}|+|a_{p_n}-a|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon \ (**)$$ i.e.  $a=\lim_n a_n$. $(*)$Where did $|a_n|\leq |a_n-a_{n1+1}|+|a_{n1+1}|$ come from? I understand why that inequality is true, but I don't see the point in writing in like that. $(**)$ Why is $|a_n-a_{p_n}|<\frac{\varepsilon}{2}?$",,"['real-analysis', 'sequences-and-series', 'proof-explanation', 'cauchy-sequences']"
66,Is a bijective smooth function a diffeomorphism almost everywhere?,Is a bijective smooth function a diffeomorphism almost everywhere?,,"Suppose I have $f: M \rightarrow N \in C^{\infty}$ a smooth bijection between $n$-dimensional smooth manifolds. Does it have to be a diffeomorphism except for a set of measure 0? I think the proof might come from showing that $X = \{p: d_pf \text{ is not an isomorphism}\}$ has measure zero. Using the inverse function theorem you can show that the statement follows from this. By Sard's theorem, we know that $f(X)$ has measure zero, but I don't know how to go from there to $X$ having measure zero (since we don't know, for example, that $f^{-1}$ is locally Lipschitz). You may assume (if you want) that $M$ and/or $N$ are connected and/or compact. Thanks!","Suppose I have $f: M \rightarrow N \in C^{\infty}$ a smooth bijection between $n$-dimensional smooth manifolds. Does it have to be a diffeomorphism except for a set of measure 0? I think the proof might come from showing that $X = \{p: d_pf \text{ is not an isomorphism}\}$ has measure zero. Using the inverse function theorem you can show that the statement follows from this. By Sard's theorem, we know that $f(X)$ has measure zero, but I don't know how to go from there to $X$ having measure zero (since we don't know, for example, that $f^{-1}$ is locally Lipschitz). You may assume (if you want) that $M$ and/or $N$ are connected and/or compact. Thanks!",,"['real-analysis', 'differential-geometry', 'smooth-manifolds', 'inverse-function']"
67,Computation of a limit involving factorial $\lim_{n \to \infty} \sqrt[n+1] {(n+1)!} - \sqrt[n] {(n)!} = \frac{1}{e}$,Computation of a limit involving factorial,\lim_{n \to \infty} \sqrt[n+1] {(n+1)!} - \sqrt[n] {(n)!} = \frac{1}{e},"I want to prove the following limit: $$\lim_{n \to \infty} \sqrt[n+1\;] {(n+1)!} - \sqrt[n] {(n)!} = \frac{1}{e}.$$ I searched the forum & found the link here: If $\frac{p_{n+1}}{np_n} \to p > 0 $, then $\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}} \to \frac{p}{e}$ . But still, there is no way out of the problem. So, please solve it.","I want to prove the following limit: $$\lim_{n \to \infty} \sqrt[n+1\;] {(n+1)!} - \sqrt[n] {(n)!} = \frac{1}{e}.$$ I searched the forum & found the link here: If $\frac{p_{n+1}}{np_n} \to p > 0 $, then $\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}} \to \frac{p}{e}$ . But still, there is no way out of the problem. So, please solve it.",,"['calculus', 'real-analysis', 'limits', 'factorial', 'radicals']"
68,Find all continuous functions satisfying $\int_0^xf=(f(x))^2+C$ for some constant $C \neq 0$.,Find all continuous functions satisfying  for some constant .,\int_0^xf=(f(x))^2+C C \neq 0,"Find all continuous functions $f$ satisfying $$\int_0^xf=(f(x))^2+C$$ for some constant $C \neq 0$, assuming that $f$ has at most one $0$. I have a question about the solution to this problem. It says that Clearly $f^2$ is differentiable everywhere, its derivative at $x$ is $f(x)$ (I'm assuming that this is because the left hand side is differentiable, the right hand side must be differentiable as well). So $f$ is differentiable at $x$ whenever $f(x) \neq 0$, and  $$f(x)=2f(x)f'(x),$$ so $f'(x) = \frac{1}{2}$ at such points. Thus, $f= \frac{1}{2}x+b$ for some $b$ on any interval where it is non-zero; if $f$ has a zero, this gives two possible solutions, with two possible values of $b$, but since $f$ is assumed continuous, they must be the same. So we need $$\int_0^x(\frac{1}{2}t+b)dt=(\frac{1}{2}x+b)^2+C$$ so we must have $b=\sqrt{-C}$ or $-\sqrt{-C}$, leading to two solutions for $C \lt 0$. I don't understand the bolded statements. First, how does the differentiability of $f^2$ guarantee that $f$ is differentiable, and why only at $x$ where $f(x) \neq 0$? Next, why are there two possible solutions if $f$ has a zero, but since $f$ is continuous they must be the same? Finally, does the last statement mean that if $C \gt 0$, then there is no solution, since we're restricted to real numbers here? I would appreciate it if anyone answers my questions.","Find all continuous functions $f$ satisfying $$\int_0^xf=(f(x))^2+C$$ for some constant $C \neq 0$, assuming that $f$ has at most one $0$. I have a question about the solution to this problem. It says that Clearly $f^2$ is differentiable everywhere, its derivative at $x$ is $f(x)$ (I'm assuming that this is because the left hand side is differentiable, the right hand side must be differentiable as well). So $f$ is differentiable at $x$ whenever $f(x) \neq 0$, and  $$f(x)=2f(x)f'(x),$$ so $f'(x) = \frac{1}{2}$ at such points. Thus, $f= \frac{1}{2}x+b$ for some $b$ on any interval where it is non-zero; if $f$ has a zero, this gives two possible solutions, with two possible values of $b$, but since $f$ is assumed continuous, they must be the same. So we need $$\int_0^x(\frac{1}{2}t+b)dt=(\frac{1}{2}x+b)^2+C$$ so we must have $b=\sqrt{-C}$ or $-\sqrt{-C}$, leading to two solutions for $C \lt 0$. I don't understand the bolded statements. First, how does the differentiability of $f^2$ guarantee that $f$ is differentiable, and why only at $x$ where $f(x) \neq 0$? Next, why are there two possible solutions if $f$ has a zero, but since $f$ is continuous they must be the same? Finally, does the last statement mean that if $C \gt 0$, then there is no solution, since we're restricted to real numbers here? I would appreciate it if anyone answers my questions.",,"['calculus', 'real-analysis']"
69,Weak topologies and weak convergence - Looking for feedbacks,Weak topologies and weak convergence - Looking for feedbacks,,"I am currently trying to get exactly what the weak and the weak* topologies are, in particular in connection to the concept of weak convergence in measure, however I am not completely sure on what I have got so far. So here, there is a summary of what I understood. I disseminated the text with some numbered questions, to point out what I am not sure about this summary. Any feedback will be most welcome! Edit: The answers I received have been quite useful and stimulating (e.g. I ended up looking at the Kelley-Namioka text), but I would love to get something more specific. Hence, I am adding a bounty. Thus, I slightly changed the text, because I actually changed my ""opinion"" regarding some issues. Edit 2: I know that editing put a question again on the top, however I did not edit with this purpose. I actually noticed that I did not get any new answer, and, considering I actually changed my view on some issues (I am studying the topic intensively these days), I changed the text improving (IMO) the content, without affecting any new answer. Moreover, I add some straightforward questions that arose out of these hours of study, in order to make clear what I am looking for, and I put the rest of the text as block, for those who want to see what I ended up with. Questions: 1. Isn’t it problematic to use the inner product notation when we deal with dual pairs and weak topologices, considering that we work in this case mainly with Banach spaces, and not all norm norms are associated to an inner product? 2. Does it ever enter in the discussion on weak topologies the concept of algebraic dual ? 3. If by definition $X^*$ is the topological dual of $X$, hence it is the set of all continuous functionals on $X$, isn’t by definition that each $x^* \in X^*$ is actually continuous? Why do we need the weak topology on $X$? [The same problem does not really apply to the weak* topology on $X^*$ that makes each evaluation functional $e_x: X \to \mathbb{R}$ continuous, because those evaluation functionals are not continuous by definition .] 4. I see in which way the fact that the bidual $X^{**}$ can be way bigger than $X$ when we deal with weak topologies can be relevant, namely if $X = X^{**}$ then weak and weak* topologies coincide. However, I don’t see how can be that $X \subset X^{**}$. 5. Related to the previous problem is the fact that when we move from dual pairs and weak topologies to weak convergence in measure, I don’t see why the weak convergence in measure coincides with endowing the reference metric space $(X,d)$ with the weak* topology, and not simply the weak topology. Thus the question should be, how do we decide what is the main space, what is the dual one, and which one is the one endowed with a topology, in order to come up with the concept of weak convergence? Is it through the Riesz Representation theorem ? (...and now I should end up looking exactly what that representation thoerem does...) 6. I read that what is called weak convergence should be called weak* convergence, because it is based on the weak* topology. Why, actually? [True, this question can be considered a repetition of the previous] In the following, there are my thoughts on weak topologies and weak convergence. I hope they are correct or interesting. I put them as a spoiler in order not to scare who would like to answer. In general we have the following definition of initial (or weak ) topology : given a set $X \neq \varnothing$, a family of topological spaces $\{ (Y_i, \tau_i ) \}_{i \in I}$, and for every $i \in I$ a function $f_i : X \to Y_i$, the initial (or weak ) topology is the weakest topology on $X$ that makes all the functions $f_i$ continuous. Notice that, if $\mathcal{F} := \{ f | f: X \to \mathbb{R} \}$, then $X$ can be seen as set of real valued functions on $\mathcal{F}$, where each $x \in X$ is an evaluation functional $e_x : X^* \to \mathbb{R}$ with $e_x (f) = f(x)$. Thus, the weak topology on $\mathcal{F}$, denoted by $\sigma (\mathcal{F}, X)$, is identical to the relative topology on $\mathcal{F}$ as subset of $\mathbb{R}^X$ endowed with the product topology. Now, let $X$ a linear space with $x \in X$. Thus, let $X’$ be the algebraic dual of $X$, i.e. the vector space of all linear functionals on $X$, and let $X^*$ be the topological dual of $X$, i.e. $X^* := \{ x^* | x^*: X \to \mathbb{R}, x^* \text{ continuous} \}$ is, or – in plain english – the vector space of all continuous linear functionals on $X$, with $x^* \in X^*$. Notice that, for the following, for example Rudin points out that $\langle x , x^* \rangle \equiv x^*(x)$, thus we should have that $\langle x^* , x \rangle \equiv e_x (x^*)$. [a. Right?] Thus, $X$ can be seen as a vector subspace of $\mathbb{R}^{X^*}$, in which case $\sigma (X, X^*)$ is the weak topology on $X$ (denoted by $w$), defined as   $$ x_\alpha \overset{w}{\to} x \in X \Longleftrightarrow \forall x^* \in X^* , \langle x_a, x^* \rangle \to \langle x , x^* \rangle. $$ In the same vein, $X^*$ can be seen as a vector subspace of $\mathbb{R}^X$, in which case $\sigma (X^*, X)$ is the weak* topology on $X^*$ (denoted by $w^*$), defined as   $$ x^{*}_\alpha \overset{w^*}{\to} x^* \in X \Longleftrightarrow \forall x \in X , \langle x^{*}_a, x \rangle \to \langle x^* , x \rangle. $$ Notice that in both cases we are just declining in different ways the original definition of initial topology given at the beginning. What we are changing is simply the ""reference"" space that makes all functions continuous.  Indeed, in the case of the weak topology , we have that $(X, \sigma(X, X^*)$ is the topological space that makes all the $x^* \in X^*$ continuous. In the case of the weak* topology, we have that $(X^*, \sigma(X^*, X)$ is the topological space that makes all the evaluation functionals $e_x$ from $X$ to $\mathbb{R}$ continuous. I know (after quite some bookreading) that what can be found in the following text as the old point of view is the correct way of looking at the problem, however I keep on feel that it is the new point of view the correct way. In particular, I find what I wrote in the old point of view misleading. Indeed, I implicitly state that we have as dual pair $\langle X, C(X) \rangle$ and every $\mu$ is on $X$, but this does not make sense. Indeed, we should have that the dual pair is $\langle \Delta (X), C(X) \rangle$ and every $\mu$ is on $\Delta (X)$. But then, if the dual pair is really $\langle \Delta (X), C(X) \rangle$, according to the definition of weak and weak* topology I wrote down, here we are actually dealing with the weak topology! Hence, the dual pair should be $\langle C(X), \Delta(X) \rangle$, but why? Old point of view: Thus, if we take all these definitions and we use them to get what the weak convergence in measures is, we actually have that weak convergence in measure is the same as convergence in the weak* topology. [b. Right?] Indeed, given a metric space $(X,d)$ a sequence of measures $(\mu_n)$ converges weakly to $\mu$  in $X$ if and only if for every $\phi \in C(X)$ (where $C(X)$ is the space of all continuous functions on $X$),   $$ \lim_{n \to \infty} \int_X \phi d\mu_n = \int_X \phi d\mu, $$    and here the $\mu_n$ should make the same job of our $x^{*}_n$ in our definition of the weak* topology. [c. Right?] . New point of view: Thus, if we take all these definitions and we use them to get what the weak convergence in measures is, we actually have that weak convergence in measure is the same as convergence in the weak topology. [d. Right?] Indeed, given a metric space $(X,d)$ a sequence of measures $(\mu_n)$ converges weakly to $\mu$  in $\Delta (X)$, where $\Delta (X)$ denotes the set of all probability measures on $X$, if and only if for every $\phi \in C(X)$ (where $C(X)$ is the space of all continuous functions on $X$),   $$ \lim_{n \to \infty} \int_X \phi d\mu_n = \int_X \phi d\mu. $$   Thus, in this setting the dual pair of spaces is $\langle \Delta (X) , C (X) \rangle$ and indeed the definition of weak convergence in $\Delta (X)$ ends up to be equivalent to endow $\Delta (X)$ with the weak topology (and not the weak* topology!). [e. Right?] . Thank you for your time and for your help!","I am currently trying to get exactly what the weak and the weak* topologies are, in particular in connection to the concept of weak convergence in measure, however I am not completely sure on what I have got so far. So here, there is a summary of what I understood. I disseminated the text with some numbered questions, to point out what I am not sure about this summary. Any feedback will be most welcome! Edit: The answers I received have been quite useful and stimulating (e.g. I ended up looking at the Kelley-Namioka text), but I would love to get something more specific. Hence, I am adding a bounty. Thus, I slightly changed the text, because I actually changed my ""opinion"" regarding some issues. Edit 2: I know that editing put a question again on the top, however I did not edit with this purpose. I actually noticed that I did not get any new answer, and, considering I actually changed my view on some issues (I am studying the topic intensively these days), I changed the text improving (IMO) the content, without affecting any new answer. Moreover, I add some straightforward questions that arose out of these hours of study, in order to make clear what I am looking for, and I put the rest of the text as block, for those who want to see what I ended up with. Questions: 1. Isn’t it problematic to use the inner product notation when we deal with dual pairs and weak topologices, considering that we work in this case mainly with Banach spaces, and not all norm norms are associated to an inner product? 2. Does it ever enter in the discussion on weak topologies the concept of algebraic dual ? 3. If by definition $X^*$ is the topological dual of $X$, hence it is the set of all continuous functionals on $X$, isn’t by definition that each $x^* \in X^*$ is actually continuous? Why do we need the weak topology on $X$? [The same problem does not really apply to the weak* topology on $X^*$ that makes each evaluation functional $e_x: X \to \mathbb{R}$ continuous, because those evaluation functionals are not continuous by definition .] 4. I see in which way the fact that the bidual $X^{**}$ can be way bigger than $X$ when we deal with weak topologies can be relevant, namely if $X = X^{**}$ then weak and weak* topologies coincide. However, I don’t see how can be that $X \subset X^{**}$. 5. Related to the previous problem is the fact that when we move from dual pairs and weak topologies to weak convergence in measure, I don’t see why the weak convergence in measure coincides with endowing the reference metric space $(X,d)$ with the weak* topology, and not simply the weak topology. Thus the question should be, how do we decide what is the main space, what is the dual one, and which one is the one endowed with a topology, in order to come up with the concept of weak convergence? Is it through the Riesz Representation theorem ? (...and now I should end up looking exactly what that representation thoerem does...) 6. I read that what is called weak convergence should be called weak* convergence, because it is based on the weak* topology. Why, actually? [True, this question can be considered a repetition of the previous] In the following, there are my thoughts on weak topologies and weak convergence. I hope they are correct or interesting. I put them as a spoiler in order not to scare who would like to answer. In general we have the following definition of initial (or weak ) topology : given a set $X \neq \varnothing$, a family of topological spaces $\{ (Y_i, \tau_i ) \}_{i \in I}$, and for every $i \in I$ a function $f_i : X \to Y_i$, the initial (or weak ) topology is the weakest topology on $X$ that makes all the functions $f_i$ continuous. Notice that, if $\mathcal{F} := \{ f | f: X \to \mathbb{R} \}$, then $X$ can be seen as set of real valued functions on $\mathcal{F}$, where each $x \in X$ is an evaluation functional $e_x : X^* \to \mathbb{R}$ with $e_x (f) = f(x)$. Thus, the weak topology on $\mathcal{F}$, denoted by $\sigma (\mathcal{F}, X)$, is identical to the relative topology on $\mathcal{F}$ as subset of $\mathbb{R}^X$ endowed with the product topology. Now, let $X$ a linear space with $x \in X$. Thus, let $X’$ be the algebraic dual of $X$, i.e. the vector space of all linear functionals on $X$, and let $X^*$ be the topological dual of $X$, i.e. $X^* := \{ x^* | x^*: X \to \mathbb{R}, x^* \text{ continuous} \}$ is, or – in plain english – the vector space of all continuous linear functionals on $X$, with $x^* \in X^*$. Notice that, for the following, for example Rudin points out that $\langle x , x^* \rangle \equiv x^*(x)$, thus we should have that $\langle x^* , x \rangle \equiv e_x (x^*)$. [a. Right?] Thus, $X$ can be seen as a vector subspace of $\mathbb{R}^{X^*}$, in which case $\sigma (X, X^*)$ is the weak topology on $X$ (denoted by $w$), defined as   $$ x_\alpha \overset{w}{\to} x \in X \Longleftrightarrow \forall x^* \in X^* , \langle x_a, x^* \rangle \to \langle x , x^* \rangle. $$ In the same vein, $X^*$ can be seen as a vector subspace of $\mathbb{R}^X$, in which case $\sigma (X^*, X)$ is the weak* topology on $X^*$ (denoted by $w^*$), defined as   $$ x^{*}_\alpha \overset{w^*}{\to} x^* \in X \Longleftrightarrow \forall x \in X , \langle x^{*}_a, x \rangle \to \langle x^* , x \rangle. $$ Notice that in both cases we are just declining in different ways the original definition of initial topology given at the beginning. What we are changing is simply the ""reference"" space that makes all functions continuous.  Indeed, in the case of the weak topology , we have that $(X, \sigma(X, X^*)$ is the topological space that makes all the $x^* \in X^*$ continuous. In the case of the weak* topology, we have that $(X^*, \sigma(X^*, X)$ is the topological space that makes all the evaluation functionals $e_x$ from $X$ to $\mathbb{R}$ continuous. I know (after quite some bookreading) that what can be found in the following text as the old point of view is the correct way of looking at the problem, however I keep on feel that it is the new point of view the correct way. In particular, I find what I wrote in the old point of view misleading. Indeed, I implicitly state that we have as dual pair $\langle X, C(X) \rangle$ and every $\mu$ is on $X$, but this does not make sense. Indeed, we should have that the dual pair is $\langle \Delta (X), C(X) \rangle$ and every $\mu$ is on $\Delta (X)$. But then, if the dual pair is really $\langle \Delta (X), C(X) \rangle$, according to the definition of weak and weak* topology I wrote down, here we are actually dealing with the weak topology! Hence, the dual pair should be $\langle C(X), \Delta(X) \rangle$, but why? Old point of view: Thus, if we take all these definitions and we use them to get what the weak convergence in measures is, we actually have that weak convergence in measure is the same as convergence in the weak* topology. [b. Right?] Indeed, given a metric space $(X,d)$ a sequence of measures $(\mu_n)$ converges weakly to $\mu$  in $X$ if and only if for every $\phi \in C(X)$ (where $C(X)$ is the space of all continuous functions on $X$),   $$ \lim_{n \to \infty} \int_X \phi d\mu_n = \int_X \phi d\mu, $$    and here the $\mu_n$ should make the same job of our $x^{*}_n$ in our definition of the weak* topology. [c. Right?] . New point of view: Thus, if we take all these definitions and we use them to get what the weak convergence in measures is, we actually have that weak convergence in measure is the same as convergence in the weak topology. [d. Right?] Indeed, given a metric space $(X,d)$ a sequence of measures $(\mu_n)$ converges weakly to $\mu$  in $\Delta (X)$, where $\Delta (X)$ denotes the set of all probability measures on $X$, if and only if for every $\phi \in C(X)$ (where $C(X)$ is the space of all continuous functions on $X$),   $$ \lim_{n \to \infty} \int_X \phi d\mu_n = \int_X \phi d\mu. $$   Thus, in this setting the dual pair of spaces is $\langle \Delta (X) , C (X) \rangle$ and indeed the definition of weak convergence in $\Delta (X)$ ends up to be equivalent to endow $\Delta (X)$ with the weak topology (and not the weak* topology!). [e. Right?] . Thank you for your time and for your help!",,"['real-analysis', 'functional-analysis', 'measure-theory', 'probability-theory', 'self-learning']"
70,Proof of Lusin's Theorem,Proof of Lusin's Theorem,,"$\mathbf{Theorem}$. Let $A \subset \mathbb{R}$ be a measurable set, $\mu(A)<\infty$ and $f$ a measurable function with domain $A$. Then, for every $\varepsilon >0$ there exists a compact set $K \subset A$ with $\mu(A \smallsetminus K) <\varepsilon$, such that the restriction of $f$ to $K$ is continuous. Proof .  Let $\{V_n\}_{n\in\mathbb N}$ be an enumeration of the open intervals with rational endpoints. Fix compact sets $K_n\subset f^{-1}[V_n]$ and $K'_n\subset A \smallsetminus f^{-1}[V_n]$ for each $n$, so that  $\mu\big(A \smallsetminus (K_n \cup K'_n)\big) < \varepsilon/2^n$. Set $$K = \bigcap_{n=1}^{\infty} \big(K_n \cup K'_n\big). $$  Clearly $\mu(A \smallsetminus K) < \varepsilon$. Given $x \in K$ and $n$, such that $f(x) \in V_n$, we can prove continuity of $f$ when restricted on $K$, by choosing a compact neighbourhood $\tilde{K}_n$, such that $x \in \tilde{K}_n$ and $f(\tilde{K}_n \cap K) \subset V_n$.  $\qquad\qquad\square$ $\mathbf{Q1}$: I don't see how $K = \bigcap_{n=1}^{\infty}(K_n \cup K'_n) $ yields $\mu(A \setminus K) < \epsilon$. The way I see it, the intersection should have a really small measure.  \begin{align} K = \bigcap_{n=1}^{\infty}(K_n \cup K'_n) =\Bigg( \bigcup_{n=1}^{\infty} (K_n \cup K'_n)^{c} \Bigg)^{c}\\ \mu(K) = \mu \Bigg (\Bigg( \bigcup_{n=1}^{\infty} (K_n \cup K'_n)^{c} \Bigg)^{c} \Bigg) = \mu(A) - \mu \Bigg( \bigcup_{n=1}^{\infty} (K_n \cup K'_n)^{c} \Bigg) \end{align} the union of $(K_n \cup K'_n)^{c}$ is probably going to cover almost the entire $A$, right? If this is the case then $\mu(K)=\mu(A)-(\mu(A)-\delta) = \delta$. Where $\delta$ is a small number. Did I make a mistake somewhere? $\mathbf{Q2}$: Do I understand it correctly that we're removing all the compacts $K'_n \subseteq A \setminus f^{-1}(V_n)$ from $A$ in order to achieve continuity? Because for $x \in K'_n$ we don't have $f(x) \in V_n$.","$\mathbf{Theorem}$. Let $A \subset \mathbb{R}$ be a measurable set, $\mu(A)<\infty$ and $f$ a measurable function with domain $A$. Then, for every $\varepsilon >0$ there exists a compact set $K \subset A$ with $\mu(A \smallsetminus K) <\varepsilon$, such that the restriction of $f$ to $K$ is continuous. Proof .  Let $\{V_n\}_{n\in\mathbb N}$ be an enumeration of the open intervals with rational endpoints. Fix compact sets $K_n\subset f^{-1}[V_n]$ and $K'_n\subset A \smallsetminus f^{-1}[V_n]$ for each $n$, so that  $\mu\big(A \smallsetminus (K_n \cup K'_n)\big) < \varepsilon/2^n$. Set $$K = \bigcap_{n=1}^{\infty} \big(K_n \cup K'_n\big). $$  Clearly $\mu(A \smallsetminus K) < \varepsilon$. Given $x \in K$ and $n$, such that $f(x) \in V_n$, we can prove continuity of $f$ when restricted on $K$, by choosing a compact neighbourhood $\tilde{K}_n$, such that $x \in \tilde{K}_n$ and $f(\tilde{K}_n \cap K) \subset V_n$.  $\qquad\qquad\square$ $\mathbf{Q1}$: I don't see how $K = \bigcap_{n=1}^{\infty}(K_n \cup K'_n) $ yields $\mu(A \setminus K) < \epsilon$. The way I see it, the intersection should have a really small measure.  \begin{align} K = \bigcap_{n=1}^{\infty}(K_n \cup K'_n) =\Bigg( \bigcup_{n=1}^{\infty} (K_n \cup K'_n)^{c} \Bigg)^{c}\\ \mu(K) = \mu \Bigg (\Bigg( \bigcup_{n=1}^{\infty} (K_n \cup K'_n)^{c} \Bigg)^{c} \Bigg) = \mu(A) - \mu \Bigg( \bigcup_{n=1}^{\infty} (K_n \cup K'_n)^{c} \Bigg) \end{align} the union of $(K_n \cup K'_n)^{c}$ is probably going to cover almost the entire $A$, right? If this is the case then $\mu(K)=\mu(A)-(\mu(A)-\delta) = \delta$. Where $\delta$ is a small number. Did I make a mistake somewhere? $\mathbf{Q2}$: Do I understand it correctly that we're removing all the compacts $K'_n \subseteq A \setminus f^{-1}(V_n)$ from $A$ in order to achieve continuity? Because for $x \in K'_n$ we don't have $f(x) \in V_n$.",,"['real-analysis', 'measure-theory']"
71,Closed unit interval is connected proof,Closed unit interval is connected proof,,"The closed unit interval $\mathbb{I}=[0,1]$ is a connected subset of   $\mathbb{R}$. I am having difficulty understanding the proof in my book, which goes: Suppose that $A,B$ are open sets forming a disconnection of   $\mathbb{I}$. Thus $A\cap \mathbb{I}$ and $B\cap \mathbb{I}$ are   non-empty bounded disjoint sets whose union is $\mathbb{I}$. Since $A$   and $B$ are open, the sets $A\cap \mathbb{I}$ and $B\cap \mathbb{I}$   cannot consist of only one point. (Why?) For the sake of definiteness,   we suppose that there exist points $a\in A$, $b\in B$ such that   $0<a<b<1$. Applying the supremum property, we let $c=\sup\{x\in  A:x<b\}$ so that $0<c<1$; hence $c\in A\cup B$. If $c\in A$ then $c\ne  b$ and since $A$ is open there is a point $a_1\in A$, $c<a_1$, such   that the interval $[c,a_1]$ is contained in $\{x\in A: x<b\}$ contrary   to the definition of $c$. Why is $0<c<1$? They define $c=\sup\{x\in A:x<b\}$ so it is the supremum of $A$ which is less than $b$ so doesn't that mean $c$ must be less than $b$ ? Also, why is $c\in A\cup B$ ? Lastly, since $c$ is the supremum of $A$ how is there a point in $A$ such that $c<a_1$ and why is it contrary that it is contained in $\{x\in A: x<b\}$ ?","The closed unit interval $\mathbb{I}=[0,1]$ is a connected subset of   $\mathbb{R}$. I am having difficulty understanding the proof in my book, which goes: Suppose that $A,B$ are open sets forming a disconnection of   $\mathbb{I}$. Thus $A\cap \mathbb{I}$ and $B\cap \mathbb{I}$ are   non-empty bounded disjoint sets whose union is $\mathbb{I}$. Since $A$   and $B$ are open, the sets $A\cap \mathbb{I}$ and $B\cap \mathbb{I}$   cannot consist of only one point. (Why?) For the sake of definiteness,   we suppose that there exist points $a\in A$, $b\in B$ such that   $0<a<b<1$. Applying the supremum property, we let $c=\sup\{x\in  A:x<b\}$ so that $0<c<1$; hence $c\in A\cup B$. If $c\in A$ then $c\ne  b$ and since $A$ is open there is a point $a_1\in A$, $c<a_1$, such   that the interval $[c,a_1]$ is contained in $\{x\in A: x<b\}$ contrary   to the definition of $c$. Why is $0<c<1$? They define $c=\sup\{x\in A:x<b\}$ so it is the supremum of $A$ which is less than $b$ so doesn't that mean $c$ must be less than $b$ ? Also, why is $c\in A\cup B$ ? Lastly, since $c$ is the supremum of $A$ how is there a point in $A$ such that $c<a_1$ and why is it contrary that it is contained in $\{x\in A: x<b\}$ ?",,"['real-analysis', 'general-topology', 'connectedness']"
72,Prove that $\frac{\pi}{4}\le\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{n+1}\right)$,Prove that,\frac{\pi}{4}\le\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{n+1}\right),Prove that $$\frac{\pi}{4}\le\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{n+1}\right)$$ EDIT: inspired by Michael Hardy's suggestion I got that $$\arcsin \frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{(n+1)(n+2)}}=\arcsin\frac{1}{\sqrt{n+1}}-\arcsin\frac{1}{\sqrt{n+2}}$$ and then $$\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{n+1}\right)\ge\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{(n+1)(n+2)}}\right)\rightarrow\frac{\pi}{4}$$ because $\sum_{n=1}^{\infty} \left(\arcsin\frac{1}{\sqrt{n+1}}-\arcsin\frac{1}{\sqrt{n+2}}\right)=\arcsin \frac{\sqrt{2}}{2}=\frac{\pi}{4}$ Sis & Chris.,Prove that $$\frac{\pi}{4}\le\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{n+1}\right)$$ EDIT: inspired by Michael Hardy's suggestion I got that $$\arcsin \frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{(n+1)(n+2)}}=\arcsin\frac{1}{\sqrt{n+1}}-\arcsin\frac{1}{\sqrt{n+2}}$$ and then $$\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{n+1}\right)\ge\sum_{n=1}^{\infty} \arcsin\left(\frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{(n+1)(n+2)}}\right)\rightarrow\frac{\pi}{4}$$ because $\sum_{n=1}^{\infty} \left(\arcsin\frac{1}{\sqrt{n+1}}-\arcsin\frac{1}{\sqrt{n+2}}\right)=\arcsin \frac{\sqrt{2}}{2}=\frac{\pi}{4}$ Sis & Chris.,,"['calculus', 'real-analysis', 'sequences-and-series', 'inequality', 'contest-math']"
73,Prove the Wallis formula form $\left(4^{\zeta{(0)}} \cdot e^{-\zeta'{(0)}}\right)^2=\frac{\pi}{2}$,Prove the Wallis formula form,\left(4^{\zeta{(0)}} \cdot e^{-\zeta'{(0)}}\right)^2=\frac{\pi}{2},How would you prove the following Wallis formula form $$ \left(4^{\zeta{(0)}} \cdot e^{-\zeta'{(0)}}\right)^2=\frac{\pi}{2}?$$ Thanks in advance!,How would you prove the following Wallis formula form $$ \left(4^{\zeta{(0)}} \cdot e^{-\zeta'{(0)}}\right)^2=\frac{\pi}{2}?$$ Thanks in advance!,,"['calculus', 'real-analysis', 'riemann-zeta']"
74,Show $\lim\limits_{n\to\infty}\int_{0}^{\infty}e^{-x}\sin(\frac{n}{x})~\text{d}x=0$,Show,\lim\limits_{n\to\infty}\int_{0}^{\infty}e^{-x}\sin(\frac{n}{x})~\text{d}x=0,I'm having trouble showing $$\lim_{n\to\infty}\int_{0}^{\infty}e^{-x}\sin\left(\frac{n}{x}\right)~\text{d}x=0$$ The integrand doesn't converge for any $x$ so I don't know how to use the standard Lebesgue convergence theorems. Thank you for any hints.,I'm having trouble showing $$\lim_{n\to\infty}\int_{0}^{\infty}e^{-x}\sin\left(\frac{n}{x}\right)~\text{d}x=0$$ The integrand doesn't converge for any $x$ so I don't know how to use the standard Lebesgue convergence theorems. Thank you for any hints.,,"['real-analysis', 'integration', 'limits', 'measure-theory']"
75,Information captured by differential forms,Information captured by differential forms,,"My advanced calculus class is currently doing differential forms and I have a hard time really understanding what they are all about. I can read the proofs of the theorems given in Rudin's PMA chapter 10 and the proofs and I can follow the logic and verify that they are true. However, I don't see why someone would come up with their definition and what makes them useful for building a theory of integration. To rephrase this: What information exactly is encapsulated by the definition of differential forms and what makes them work out so nicely with respect to wedge products? Why is this the ""right"" formulation for an integration theory?","My advanced calculus class is currently doing differential forms and I have a hard time really understanding what they are all about. I can read the proofs of the theorems given in Rudin's PMA chapter 10 and the proofs and I can follow the logic and verify that they are true. However, I don't see why someone would come up with their definition and what makes them useful for building a theory of integration. To rephrase this: What information exactly is encapsulated by the definition of differential forms and what makes them work out so nicely with respect to wedge products? Why is this the ""right"" formulation for an integration theory?",,"['real-analysis', 'multivariable-calculus', 'intuition', 'differential-forms']"
76,$\epsilon{\rm -}\delta$ proof of the discontinuity of Dirichlet's function,proof of the discontinuity of Dirichlet's function,\epsilon{\rm -}\delta,"How can I prove that the function  $$   f(x) = \left\{\begin{array}{l l}     x &\text{if }x \in \mathbb{Q} \\     -x & \text{if } x \notin \mathbb{Q}    \end{array} \right. $$ is discontinuous for $x \neq 0$, using $\epsilon$'s and $\delta$'s? I see that is truth. But I cannot prove using only $\epsilon$'s and $\delta$'s.","How can I prove that the function  $$   f(x) = \left\{\begin{array}{l l}     x &\text{if }x \in \mathbb{Q} \\     -x & \text{if } x \notin \mathbb{Q}    \end{array} \right. $$ is discontinuous for $x \neq 0$, using $\epsilon$'s and $\delta$'s? I see that is truth. But I cannot prove using only $\epsilon$'s and $\delta$'s.",,"['calculus', 'real-analysis']"
77,Calculus of Variations and Lagrange Multipliers,Calculus of Variations and Lagrange Multipliers,,"A general problem for the Calculus of Variations asks us to minimize the value of a functional $A[f]$, where $f$ is usually a differentiable function defined on $\mathbb{R}^n$. What if, however, the domain of $A$ is not actually all differentiable functions. Suppose there is a constraint equation on $f$, such as (for example): $L[f] = \int_{-1}^1 \sqrt{1 + f'(x)^2} dx = \pi$ and we want to minimize over functions satisfying the above and the property that $f(-1)=f(1)=0$ the functional $A[f] = \int_{-1}^1 f(x) dx $ This sort of problem seems to me to be very similar to the problem in multivariate calculus of minimizing a function $f(x)$ with respect to a constraint equation $g(x) = 0$. In this case we are trying to minimize a functional $A[f]$ with respect to a functional constraint equation $L[f] = \pi$. In the former, one can use Lagrange multipliers to reduce the problem to that of solving a system of equations. Is there such a technique for the variational version?","A general problem for the Calculus of Variations asks us to minimize the value of a functional $A[f]$, where $f$ is usually a differentiable function defined on $\mathbb{R}^n$. What if, however, the domain of $A$ is not actually all differentiable functions. Suppose there is a constraint equation on $f$, such as (for example): $L[f] = \int_{-1}^1 \sqrt{1 + f'(x)^2} dx = \pi$ and we want to minimize over functions satisfying the above and the property that $f(-1)=f(1)=0$ the functional $A[f] = \int_{-1}^1 f(x) dx $ This sort of problem seems to me to be very similar to the problem in multivariate calculus of minimizing a function $f(x)$ with respect to a constraint equation $g(x) = 0$. In this case we are trying to minimize a functional $A[f]$ with respect to a functional constraint equation $L[f] = \pi$. In the former, one can use Lagrange multipliers to reduce the problem to that of solving a system of equations. Is there such a technique for the variational version?",,"['calculus', 'real-analysis', 'calculus-of-variations']"
78,Why doesn't pointwise bounded imply uniform bounded?,Why doesn't pointwise bounded imply uniform bounded?,,"I was reading Rudin's Principles of Mathematical Analysis, and I came across the definition 7.19, where it says that a sequence of functions $f_n(x)$ is pointwise bounded on E if there exists a finite-valued function $\phi$ defined on E such that  $$ |f_n(x)| < \phi(x) $$ for x element of E, n = 1, 2 ,3 ...  While $f_n$ is uniformly bounded on E if there exists a number M s.t.  $|f_n(x)| < M$ for x element of E, n = 1, 2 , 3 ...  But if we define the set U as the values of $\phi(x)$ from our first definition and define the sup of the set as R, then don't we get the second definition. Wouldn't that mean that pointwise bounded implies uniform bounded?","I was reading Rudin's Principles of Mathematical Analysis, and I came across the definition 7.19, where it says that a sequence of functions $f_n(x)$ is pointwise bounded on E if there exists a finite-valued function $\phi$ defined on E such that  $$ |f_n(x)| < \phi(x) $$ for x element of E, n = 1, 2 ,3 ...  While $f_n$ is uniformly bounded on E if there exists a number M s.t.  $|f_n(x)| < M$ for x element of E, n = 1, 2 , 3 ...  But if we define the set U as the values of $\phi(x)$ from our first definition and define the sup of the set as R, then don't we get the second definition. Wouldn't that mean that pointwise bounded implies uniform bounded?",,"['real-analysis', 'analysis']"
79,Computing the limit $\lim_{k \to \infty} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x$ for fixed $n \in \mathbb{N}$,Computing the limit  for fixed,\lim_{k \to \infty} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x n \in \mathbb{N},"I'm working on a problem that asks to compute $$\lim_{k \to \infty} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x$$ for fixed $n \in \mathbb{N}$ . What I've tried so far is to do a $u$ -substitution for $u = \frac{x}{k}$ , so I have $$\int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x = k^{n + 1} \int_0^1 u^n (1 - u)^k \mathrm{d} u .$$ Using the Binomial Theorem to break up the $(1 - u)^k$ term, I get $$k^{n + 1} \int_0^1 u^n (1 - u)^k \mathrm{d} u = k^{n + 1} \int_0^1 \sum_{j = 0}^k \binom{k}{j} \frac{(-1)^j}{n + j + 1} .$$ However, I don't know how to compute the limit of this expression as $k \to \infty$ . I assume that I should recognize it as some kind of Taylor series that's somehow $O \left( k^{-(n + 1)} \right)$ , but I'm not seeing it. Note: When looking at other posts, I found an integral that looked similar to this one, and the only answer on that post involved something called a beta function. I have never heard of a beta function, and would like to find a solution here that doesn't rely on whatever a beta function is. Another idea I considered was to use the Dominated Convergence Theorem, since $e^{-x} = \lim_{k \to \infty} \left( 1 - \frac{x}{k} \right)^k$ , so I figured I could use DCT to say that \begin{align*} \lim_{k \to \infty} \int_0^k x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x & = \lim_{k \to \infty} \int_0^\infty \chi_{[0, k]}(x) x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x \\ & = \int_0^\infty x^n e^{-x} \mathrm{d} x & (\textrm{DCT used here})\\ & = n ! , \end{align*} assuming I didn't mess up any of my integration by parts. However, I couldn't find a choice of dominator that would work on all of $[0, \infty)$ , so I'd also be interested in a solution that uses DCT as well. Perhaps I'm being naive, but the pointwise limit is just so convenient that I have to imagine that DCT can be used here. EDIT: Thanks to some inspiration from a comment by user Mars Plastic, I thought to consider some other integral convergence theorems. I came up with this, which I think works. I'm still interested to see if the Dominated Convergence Theorem argument can be made to work, perhaps a bit more smoothly than this. Let $f_k(x) = \chi_{[0, k]}(x) x^n \left( 1 - \frac{x}{k} \right)^k$ . I claim that the sequence is monotone increasing on $[0, \infty)$ . Fix $x \in (0, \infty)$ , and let $K = \lfloor k \rfloor + 1$ , so that $K = \min \{ k \in \mathbb{N} : f_k(x) \neq 0 \}$ . Obviously if $k < K$ , then $f_k(x) = 0 \leq f_{k + 1}(x)$ . So consider the case where $k \geq K$ . Then $f_k(x) = x^n \left(1 - \frac{x}{k} \right)^k$ , and based on answers to this question , it seems this sequence would be monotone increasing. Therefore, I can apply the Monotone Convergence Theorem to say that $f_k(x) \nearrow x^n e^{-x}$ , so $$\int_0^k x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x = \int_0^\infty f_k(x) \mathrm{d} x = \int_0^\infty x^n e^{-x} = n! .$$","I'm working on a problem that asks to compute for fixed . What I've tried so far is to do a -substitution for , so I have Using the Binomial Theorem to break up the term, I get However, I don't know how to compute the limit of this expression as . I assume that I should recognize it as some kind of Taylor series that's somehow , but I'm not seeing it. Note: When looking at other posts, I found an integral that looked similar to this one, and the only answer on that post involved something called a beta function. I have never heard of a beta function, and would like to find a solution here that doesn't rely on whatever a beta function is. Another idea I considered was to use the Dominated Convergence Theorem, since , so I figured I could use DCT to say that assuming I didn't mess up any of my integration by parts. However, I couldn't find a choice of dominator that would work on all of , so I'd also be interested in a solution that uses DCT as well. Perhaps I'm being naive, but the pointwise limit is just so convenient that I have to imagine that DCT can be used here. EDIT: Thanks to some inspiration from a comment by user Mars Plastic, I thought to consider some other integral convergence theorems. I came up with this, which I think works. I'm still interested to see if the Dominated Convergence Theorem argument can be made to work, perhaps a bit more smoothly than this. Let . I claim that the sequence is monotone increasing on . Fix , and let , so that . Obviously if , then . So consider the case where . Then , and based on answers to this question , it seems this sequence would be monotone increasing. Therefore, I can apply the Monotone Convergence Theorem to say that , so","\lim_{k \to \infty} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x n \in \mathbb{N} u u = \frac{x}{k} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x = k^{n + 1} \int_0^1 u^n (1 - u)^k \mathrm{d} u . (1 - u)^k k^{n + 1} \int_0^1 u^n (1 - u)^k \mathrm{d} u = k^{n + 1} \int_0^1 \sum_{j = 0}^k \binom{k}{j} \frac{(-1)^j}{n + j + 1} . k \to \infty O \left( k^{-(n + 1)} \right) e^{-x} = \lim_{k \to \infty} \left( 1 - \frac{x}{k} \right)^k \begin{align*}
\lim_{k \to \infty} \int_0^k x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x & = \lim_{k \to \infty} \int_0^\infty \chi_{[0, k]}(x) x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x \\
& = \int_0^\infty x^n e^{-x} \mathrm{d} x & (\textrm{DCT used here})\\
& = n ! ,
\end{align*} [0, \infty) f_k(x) = \chi_{[0, k]}(x) x^n \left( 1 - \frac{x}{k} \right)^k [0, \infty) x \in (0, \infty) K = \lfloor k \rfloor + 1 K = \min \{ k \in \mathbb{N} : f_k(x) \neq 0 \} k < K f_k(x) = 0 \leq f_{k + 1}(x) k \geq K f_k(x) = x^n \left(1 - \frac{x}{k} \right)^k f_k(x) \nearrow x^n e^{-x} \int_0^k x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x = \int_0^\infty f_k(x) \mathrm{d} x = \int_0^\infty x^n e^{-x} = n! .","['real-analysis', 'integration', 'lebesgue-integral', 'riemann-integration']"
80,Question about Spivak's proof of how to use u-substitution when the derivative of the inner function does not appear in the integral,Question about Spivak's proof of how to use u-substitution when the derivative of the inner function does not appear in the integral,,"Spivak (3rd edition) proposes solving the integral $$\int \frac{1+e^x}{1-e^x} dx$$ by letting $u=e^x$ , $x=\ln(u)$ , and $dx=\frac{1}{u}du$ . This results in the integral $$\int \frac{1+u}{1-u}\frac{1}{u}du\\=\int \frac{2}{1-u}+\frac{1}{u}du=-2\ln(1-u)+\ln(u)=-2\ln(1-e^x)+x$$ From this example, Spivak argues that a similar method will work on any integral of the form $\int f(g(x))dx$ whenever $g(x)$ is invertible in the appropriate interval. Because this method is not a simple application of the substitution theorem, Spivak provides the following justification for his claim. Consider continuous $f$ and $g$ where $g$ is invertible on the appropriate interval. Applying the above  method to the arbitrary case, we let $u=g(x)$ , $x=g^{−1}(u)$ , and $dx=(g^{−1})′(u)du$ . Thus, we need to show that $$∫f(g(x))dx=∫f(u)(g^{−1})′(u)du$$ To prove this equality Spivak uses a more typical substitution $u=g(x)$ , $du=g′(x)dx$ and applies it by noting that $$∫f(g(x))dx=∫f(g(x))g′(x)\frac{1}{g′(x)}dx$$ Presumably using the substitution theorem, which roughly states that $∫f(g(x))g'(x)dx=∫f(u)du$ , Spivak asserts that $$∫f(g(x))g′(x)\frac{1}{g′(x)}dx=∫f(u)\frac{1}{g′(g^{−1}(u))}du$$ Then, because $(g^{-1})'(u)=\frac{1}{g'(g^{-1}(u))}$ Spivak concludes $$∫f(u)\frac{1}{g′(g^{−1}(u))}du=∫f(u)(g^{−1})′(u)du$$ I lose track of the argument when Spivak argues that $$∫f(g(x))g′(x)\frac{1}{g′(x)}dx=∫f(u)\frac{1}{g′(g^{−1}(u))}du$$ In the original example, it was clear to me how we could apply the substitution theorem to make this equality true because $\frac{1}{g'(x)}$ was in fact a function of $g(x)$ as $g'(x)=g(x)$ . But this is not necessarily true in all cases, or so it seems. How do we know that $f(g(x))\frac{1}{g'x}$ can be written in the form $h(g(x))$ for some continuous function $h$ ? To sum up, my main questions is, how do we use the substitution theorem to justify the equality $$∫f(g(x))g′(x)\frac{1}{g′(x)}dx=∫f(u)\frac{1}{g′(g^{−1}(u))}du$$","Spivak (3rd edition) proposes solving the integral by letting , , and . This results in the integral From this example, Spivak argues that a similar method will work on any integral of the form whenever is invertible in the appropriate interval. Because this method is not a simple application of the substitution theorem, Spivak provides the following justification for his claim. Consider continuous and where is invertible on the appropriate interval. Applying the above  method to the arbitrary case, we let , , and . Thus, we need to show that To prove this equality Spivak uses a more typical substitution , and applies it by noting that Presumably using the substitution theorem, which roughly states that , Spivak asserts that Then, because Spivak concludes I lose track of the argument when Spivak argues that In the original example, it was clear to me how we could apply the substitution theorem to make this equality true because was in fact a function of as . But this is not necessarily true in all cases, or so it seems. How do we know that can be written in the form for some continuous function ? To sum up, my main questions is, how do we use the substitution theorem to justify the equality",\int \frac{1+e^x}{1-e^x} dx u=e^x x=\ln(u) dx=\frac{1}{u}du \int \frac{1+u}{1-u}\frac{1}{u}du\\=\int \frac{2}{1-u}+\frac{1}{u}du=-2\ln(1-u)+\ln(u)=-2\ln(1-e^x)+x \int f(g(x))dx g(x) f g g u=g(x) x=g^{−1}(u) dx=(g^{−1})′(u)du ∫f(g(x))dx=∫f(u)(g^{−1})′(u)du u=g(x) du=g′(x)dx ∫f(g(x))dx=∫f(g(x))g′(x)\frac{1}{g′(x)}dx ∫f(g(x))g'(x)dx=∫f(u)du ∫f(g(x))g′(x)\frac{1}{g′(x)}dx=∫f(u)\frac{1}{g′(g^{−1}(u))}du (g^{-1})'(u)=\frac{1}{g'(g^{-1}(u))} ∫f(u)\frac{1}{g′(g^{−1}(u))}du=∫f(u)(g^{−1})′(u)du ∫f(g(x))g′(x)\frac{1}{g′(x)}dx=∫f(u)\frac{1}{g′(g^{−1}(u))}du \frac{1}{g'(x)} g(x) g'(x)=g(x) f(g(x))\frac{1}{g'x} h(g(x)) h ∫f(g(x))g′(x)\frac{1}{g′(x)}dx=∫f(u)\frac{1}{g′(g^{−1}(u))}du,"['real-analysis', 'calculus']"
81,Characterization of the sinus,Characterization of the sinus,,"I am looking for a simple proof of this result: If  $f\in C^{\infty}(\mathbb{R})$ fulfills $f'(0)=1$ and $|f^{(n)}(x)|\leq 1$  for any $x\in\mathbb{R}$ and any $n\in\{0,1,2,\ldots\}$, then $f(x)=\sin(x)$. A proof (through the Paley-Wiener theorem and complex analysis) can be found here: http://www.math.sciences.univ-nantes.fr/~nicoleau/sinus.pdf","I am looking for a simple proof of this result: If  $f\in C^{\infty}(\mathbb{R})$ fulfills $f'(0)=1$ and $|f^{(n)}(x)|\leq 1$  for any $x\in\mathbb{R}$ and any $n\in\{0,1,2,\ldots\}$, then $f(x)=\sin(x)$. A proof (through the Paley-Wiener theorem and complex analysis) can be found here: http://www.math.sciences.univ-nantes.fr/~nicoleau/sinus.pdf",,"['real-analysis', 'complex-analysis']"
82,"If $C$ is the Cantor set, then $C+C=[0,2]$.","If  is the Cantor set, then .","C C+C=[0,2]","Question : Prove that $C+C=\{x+y\mid x,y\in C\}=[0,2]$, using the following steps: We will show that $C\subseteq [0,2]$ and $[0,2]\subseteq C$. a) Show that for an arbitrary $n\in\mathbb{N}$ we can always find $x_n,y_n\in C_n$, where $$C_n=\left[0,\frac1{3^n}\right]\bigcup \dots \bigcup\left[\frac{3^{n}-1}{3^n},1\right]$$such that for a given $s\in[0,2]$ we have  $x_n+y_n=s$. b) Then we will set $x=\lim x_n$ and $y=\lim y_n$,  then $x+y=s$. My progress : Showing that $C+C\subseteq [0,2]$ is obvious, and I did part a) by showing that if $x_n,y_n$ are in different subintervals then concluding that $x_n+y_n$ covers $[0,2]$ (can be done using induction on $n$). My difficulty is in the second part. The sequence $x_n$ is bounded so it must have a convergent subsequence $(x_{n_{k}})$. If we set $x=\lim x_{n_{k}}$, then we can conclude $\lim y_{n_{k}}=y=s-x$, thus $x+y=s$. First I thought that $x,y$ will be in $C$ as it is closed. However $(x_{n_{k}})$ may not necessarily be in $C$, so $x$ can't be in $C$ for sure. Is this last thought correct? How do I overcome this last gap in my solution? Thanks for your help.","Question : Prove that $C+C=\{x+y\mid x,y\in C\}=[0,2]$, using the following steps: We will show that $C\subseteq [0,2]$ and $[0,2]\subseteq C$. a) Show that for an arbitrary $n\in\mathbb{N}$ we can always find $x_n,y_n\in C_n$, where $$C_n=\left[0,\frac1{3^n}\right]\bigcup \dots \bigcup\left[\frac{3^{n}-1}{3^n},1\right]$$such that for a given $s\in[0,2]$ we have  $x_n+y_n=s$. b) Then we will set $x=\lim x_n$ and $y=\lim y_n$,  then $x+y=s$. My progress : Showing that $C+C\subseteq [0,2]$ is obvious, and I did part a) by showing that if $x_n,y_n$ are in different subintervals then concluding that $x_n+y_n$ covers $[0,2]$ (can be done using induction on $n$). My difficulty is in the second part. The sequence $x_n$ is bounded so it must have a convergent subsequence $(x_{n_{k}})$. If we set $x=\lim x_{n_{k}}$, then we can conclude $\lim y_{n_{k}}=y=s-x$, thus $x+y=s$. First I thought that $x,y$ will be in $C$ as it is closed. However $(x_{n_{k}})$ may not necessarily be in $C$, so $x$ can't be in $C$ for sure. Is this last thought correct? How do I overcome this last gap in my solution? Thanks for your help.",,"['real-analysis', 'elementary-set-theory']"
83,"Not every function on $[0,1]$ is a pointwise limit of continuous functions on $[0,1]$",Not every function on  is a pointwise limit of continuous functions on,"[0,1] [0,1]","How to show that not every $\mathbb{R}$-valued function on $[0,1]$ is a pointwise limit of continuous $\mathbb{R}$-valued functions on $[0,1]$? There is a theorem that states that the set of points of discontinuity of a pointwise limit of continuous $\mathbb{R}$-valued functions is Baire first category set. So we can take, for instance, function $f=\chi([0,1]\cap \mathbb{Q})$ and this would be the function that is not a pointwise limit of continuous functions, according to the theorem. But the theorem is strong, it states more then we need and its proof is rather non-trivial. Is there more straightforward way to show that $f=\chi([0,1]\cap \mathbb{Q})$ (or any other function) is not a pointwise limit of continuous functions?","How to show that not every $\mathbb{R}$-valued function on $[0,1]$ is a pointwise limit of continuous $\mathbb{R}$-valued functions on $[0,1]$? There is a theorem that states that the set of points of discontinuity of a pointwise limit of continuous $\mathbb{R}$-valued functions is Baire first category set. So we can take, for instance, function $f=\chi([0,1]\cap \mathbb{Q})$ and this would be the function that is not a pointwise limit of continuous functions, according to the theorem. But the theorem is strong, it states more then we need and its proof is rather non-trivial. Is there more straightforward way to show that $f=\chi([0,1]\cap \mathbb{Q})$ (or any other function) is not a pointwise limit of continuous functions?",,"['real-analysis', 'functional-analysis', 'limits', 'functions']"
84,To find the minimum of $\int_0^1 (f''(x))^2dx$ [duplicate],To find the minimum of  [duplicate],\int_0^1 (f''(x))^2dx,"This question already has an answer here : Prove that $\int_0^1[f''(x)]^2dx\ge4.$ (1 answer) Closed 6 years ago . I was trying to solve a question of an entrance exam. I am completely stuck in the problem. I am not able to find idea how to proceed. Please help me. Let $A$ be the set of twice continuously differentiable functions on the interval $[0, 1]$ and let $B = \{f \in A : f(0) = f(1) = 0, f'(0) = 2\}$. Find the value of $$\text{min}_{f\in B} \displaystyle \int_0^1 (f''(x))^2dx.$$ I am really sorry for not showing some effort from my side but I can not find ant way to proceed.  Please help me. Thnx in advance.","This question already has an answer here : Prove that $\int_0^1[f''(x)]^2dx\ge4.$ (1 answer) Closed 6 years ago . I was trying to solve a question of an entrance exam. I am completely stuck in the problem. I am not able to find idea how to proceed. Please help me. Let $A$ be the set of twice continuously differentiable functions on the interval $[0, 1]$ and let $B = \{f \in A : f(0) = f(1) = 0, f'(0) = 2\}$. Find the value of $$\text{min}_{f\in B} \displaystyle \int_0^1 (f''(x))^2dx.$$ I am really sorry for not showing some effort from my side but I can not find ant way to proceed.  Please help me. Thnx in advance.",,"['real-analysis', 'calculus-of-variations']"
85,Computing $\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$,Computing,\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right),"I'm sure you can do this easily, but I'm looking for an easy way that only uses series manipulation. Is that possible? $$\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$$ where $\psi^{(0)}(x)$ is digamma function Here is a supplementary question, the alternating version $$\sum_{n=1}^{\infty} (-1)^{n+1} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$$ And this one will take into account our year as a power $$\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)^{2014}$$","I'm sure you can do this easily, but I'm looking for an easy way that only uses series manipulation. Is that possible? $$\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$$ where $\psi^{(0)}(x)$ is digamma function Here is a supplementary question, the alternating version $$\sum_{n=1}^{\infty} (-1)^{n+1} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$$ And this one will take into account our year as a power $$\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)^{2014}$$",,"['calculus', 'real-analysis', 'sequences-and-series']"
86,"Is it worthy to buy Kaczor's ""Problems in Mathematical Analysis"" three volumes?","Is it worthy to buy Kaczor's ""Problems in Mathematical Analysis"" three volumes?",,"I'm looking for a problem book in early math analysis, proof based, one single variable calculus problems, limit, continuity, derivative, integral, Taylor theorem, power series, convergence, divergence, elementary transcendental functions, least upper bounds, intermediate and extreme value theorems. I find Kaczor problems in math analysis three volume, it seems cover same area that I'm looking for. however, this book has very few reviews, and I don't know if it's worthy to buy them. Any advice?","I'm looking for a problem book in early math analysis, proof based, one single variable calculus problems, limit, continuity, derivative, integral, Taylor theorem, power series, convergence, divergence, elementary transcendental functions, least upper bounds, intermediate and extreme value theorems. I find Kaczor problems in math analysis three volume, it seems cover same area that I'm looking for. however, this book has very few reviews, and I don't know if it's worthy to buy them. Any advice?",,"['calculus', 'real-analysis', 'reference-request', 'book-recommendation']"
87,Collecting things that are preserved by (isometric) isomorphisms between normed spaces,Collecting things that are preserved by (isometric) isomorphisms between normed spaces,,"I would like to collect a list of things that are preserved under isomorphisms and isometric isomorphisms. The reason is that I hope to get a better perception of their importance in Functional Analysis. Definition: $(X,||.||)$ and $(Y,||.||)$ be normed spaces, then a continuous linear bijective map whose inverse is also continuous is called an isomorphism between them. If this isomorphism is an isometry we call it an isometric isomorphism. I have three things so far: separability, completeness and Hilbertness(although I am more interested in general Banach spaces) are preserved by isomorphism. What are they also good for and what for do we need isometric isomorphisms? The best answer would be one with many properties that are preserved by isomorphisms and refers to things that somebody would know who has attended a first course on functional analysis. Since most answers kept on saying, that ""all"" properties are preserved, I will list a few and maybe you can tell me whether they are preserved: In the following I will always refer to something is mapped onto something with the same property by an isomorphism : Reflexive spaces, closed subspaces, dense sets, linear independent vectors, compact sets, open sets, closed sets, disjoint sets. If $X,Y$ are isomorphic, then also $X',Y'$ . These were a few properties. Is it correct, that they are always preserved?-But still, I would highly appreciate it if anybody could add a few things more. Probably some of them are also preserved by jut continuous (and injective/surjective) maps, would be interesting to know which ones.","I would like to collect a list of things that are preserved under isomorphisms and isometric isomorphisms. The reason is that I hope to get a better perception of their importance in Functional Analysis. Definition: and be normed spaces, then a continuous linear bijective map whose inverse is also continuous is called an isomorphism between them. If this isomorphism is an isometry we call it an isometric isomorphism. I have three things so far: separability, completeness and Hilbertness(although I am more interested in general Banach spaces) are preserved by isomorphism. What are they also good for and what for do we need isometric isomorphisms? The best answer would be one with many properties that are preserved by isomorphisms and refers to things that somebody would know who has attended a first course on functional analysis. Since most answers kept on saying, that ""all"" properties are preserved, I will list a few and maybe you can tell me whether they are preserved: In the following I will always refer to something is mapped onto something with the same property by an isomorphism : Reflexive spaces, closed subspaces, dense sets, linear independent vectors, compact sets, open sets, closed sets, disjoint sets. If are isomorphic, then also . These were a few properties. Is it correct, that they are always preserved?-But still, I would highly appreciate it if anybody could add a few things more. Probably some of them are also preserved by jut continuous (and injective/surjective) maps, would be interesting to know which ones.","(X,||.||) (Y,||.||) X,Y X',Y'","['real-analysis', 'functional-analysis']"
88,Who was the first to prove $\lim_{x \to 0} \frac{\sin{x}}{x}=1 $?,Who was the first to prove ?,\lim_{x \to 0} \frac{\sin{x}}{x}=1 ,Who was the first to prove $\lim_{x \to 0}\frac{\sin{x}}{x}=1$?,Who was the first to prove $\lim_{x \to 0}\frac{\sin{x}}{x}=1$?,,"['calculus', 'real-analysis', 'limits', 'math-history']"
89,"If $a_1,a_2,\dotsc,a_n>0 $, then $\lim\limits_{x \to \infty} \left[\frac {a_1^{1/x}+a_2^{1/x}+\dotsb+a_n^{1/x}}{n}\right]^{nx}=a_1 a_2 \dotsb a_n$","If , then","a_1,a_2,\dotsc,a_n>0  \lim\limits_{x \to \infty} \left[\frac {a_1^{1/x}+a_2^{1/x}+\dotsb+a_n^{1/x}}{n}\right]^{nx}=a_1 a_2 \dotsb a_n","If $a_1,a_2,\dotsc,a_n $ are positive real numbers, then prove that $$\lim_{x \to \infty} \left[\frac {a_1^{1/x}+a_2^{1/x}+.....+a_n^{1/x}}{n}\right]^{nx}=a_1 a_2 \dotsb a_n.$$ My Attempt: Let $P=\lim_{x \to \infty} \left[\dfrac {a_1^{\frac{1}{x}}+a_2^{\frac {1}{x}}+.....+a_n^{\frac {1}{x}}}{n}\right]^{nx} \implies \ln P=\lim_{x \to \infty} \ln \left[\frac {a_1^{\frac{1}{x}}+a_2^{\frac {1}{x}}+.....+a_n^{\frac {1}{x}}}{n}\right]^{nx} =\lim_{x \to \infty} nx \ln \left[\frac {a_1^{\frac{1}{x}}+a_2^{\frac {1}{x}}+.....+a_n^{\frac {1}{x}}}{n}\right]= \lim_{x \to \infty} n \left[\frac {\ln (a_1^{1/x}+a_2^{1/x}+...+a_n^{1/x})-\ln n}{1/x}\right]$ and this is $0/0$ form and so I have to apply L'Hospital's rule. Now things get a bit complicated during derivative. Can someone point me in the right direction? Thanks in advance for your time.","If $a_1,a_2,\dotsc,a_n $ are positive real numbers, then prove that $$\lim_{x \to \infty} \left[\frac {a_1^{1/x}+a_2^{1/x}+.....+a_n^{1/x}}{n}\right]^{nx}=a_1 a_2 \dotsb a_n.$$ My Attempt: Let $P=\lim_{x \to \infty} \left[\dfrac {a_1^{\frac{1}{x}}+a_2^{\frac {1}{x}}+.....+a_n^{\frac {1}{x}}}{n}\right]^{nx} \implies \ln P=\lim_{x \to \infty} \ln \left[\frac {a_1^{\frac{1}{x}}+a_2^{\frac {1}{x}}+.....+a_n^{\frac {1}{x}}}{n}\right]^{nx} =\lim_{x \to \infty} nx \ln \left[\frac {a_1^{\frac{1}{x}}+a_2^{\frac {1}{x}}+.....+a_n^{\frac {1}{x}}}{n}\right]= \lim_{x \to \infty} n \left[\frac {\ln (a_1^{1/x}+a_2^{1/x}+...+a_n^{1/x})-\ln n}{1/x}\right]$ and this is $0/0$ form and so I have to apply L'Hospital's rule. Now things get a bit complicated during derivative. Can someone point me in the right direction? Thanks in advance for your time.",,"['calculus', 'real-analysis', 'derivatives']"
90,Construct a continuous monotone function $f$ on $\mathbb{R}$ that is not constant on any segment but $f'(x)=0$ a.e.,Construct a continuous monotone function  on  that is not constant on any segment but  a.e.,f \mathbb{R} f'(x)=0,"This is one exercise from Rudin's book, Construct a continuous monotone function $f$ on $\mathbb{R}^1$ that is not constant on any segment but $f'(x)=0$ a.e.","This is one exercise from Rudin's book, Construct a continuous monotone function $f$ on $\mathbb{R}^1$ that is not constant on any segment but $f'(x)=0$ a.e.",,"['real-analysis', 'measure-theory']"
91,"Prove that in $\mathbb{R}$, if $|a-b|>\alpha$ for all $a\in A$ and $b\in B$, then outer measure $m^*(A\cup B)=m^*(A)+ m^*(B)$","Prove that in , if  for all  and , then outer measure",\mathbb{R} |a-b|>\alpha a\in A b\in B m^*(A\cup B)=m^*(A)+ m^*(B),"Prove that for sets $A,B$ bounded in $\mathbb{R}$ : If there exists $\alpha > 0$ such that $|a-b|>\alpha$ for all $a\in A$ and $b\in B$ , then outer measure $m^*(A\cup B)=m^*(A)+m^*(B)$ . This comes out of section 2.2 of Royden's Real Analysis.  I'm really having trouble with this one for some reason.  The only theorem that I can see that might be of some help is that outer measure is preserved under set translation.  But I would have to translate each point of one of these sets a different amount, so that seems hopeless. Because I have so few theorems to work with my hunch is that I need to go back to the very definition of outer measure and do something clever with it, but so far I haven't had any luck.  Can anyone help me?  Thanks.","Prove that for sets bounded in : If there exists such that for all and , then outer measure . This comes out of section 2.2 of Royden's Real Analysis.  I'm really having trouble with this one for some reason.  The only theorem that I can see that might be of some help is that outer measure is preserved under set translation.  But I would have to translate each point of one of these sets a different amount, so that seems hopeless. Because I have so few theorems to work with my hunch is that I need to go back to the very definition of outer measure and do something clever with it, but so far I haven't had any luck.  Can anyone help me?  Thanks.","A,B \mathbb{R} \alpha > 0 |a-b|>\alpha a\in A b\in B m^*(A\cup B)=m^*(A)+m^*(B)","['real-analysis', 'measure-theory']"
92,Differentiability of Norms,Differentiability of Norms,,"I am looking at for which point(s) in $\mathbb{R}^n$ and $p\geq 1$ such that the map $\mathbb{R}^n\rightarrow\mathbb{R}$  given by $x\mapsto ||x||_p=(|x_1|^p+\cdots+|x_n|^p)^{\frac{1}{p}}$ is differentiable. Intuitively, it is not hard to ""guess"" the answer to be for all $p$, the map is not differentiable at origin. for $p=1$, the map is not differentiable everywhere, except for the axis. for $p>1$, the map is differentiable everywhere except at the origin. I come up with these answers by checking continuity of partial derivatives, which is pretty tedious. However, I was even asked to verify these by definition. I am wondering whether there is a very neat and insightful way to see this.","I am looking at for which point(s) in $\mathbb{R}^n$ and $p\geq 1$ such that the map $\mathbb{R}^n\rightarrow\mathbb{R}$  given by $x\mapsto ||x||_p=(|x_1|^p+\cdots+|x_n|^p)^{\frac{1}{p}}$ is differentiable. Intuitively, it is not hard to ""guess"" the answer to be for all $p$, the map is not differentiable at origin. for $p=1$, the map is not differentiable everywhere, except for the axis. for $p>1$, the map is differentiable everywhere except at the origin. I come up with these answers by checking continuity of partial derivatives, which is pretty tedious. However, I was even asked to verify these by definition. I am wondering whether there is a very neat and insightful way to see this.",,['real-analysis']
93,"Is $g(x)= \frac{x_1^2}{r_1^2}+\frac{x_2^2}{r_2^2}- c$ a unique solution of $E[g(Z)\mid M=\mu]=0, \forall \mu \in \text{ellipse } C$ for $Z$ Gaussian",Is  a unique solution of  for  Gaussian,"g(x)= \frac{x_1^2}{r_1^2}+\frac{x_2^2}{r_2^2}- c E[g(Z)\mid M=\mu]=0, \forall \mu \in \text{ellipse } C Z","Let $Z \in \mathbb{R}^2$ be an i.i.d. Gaussian vector with mean $M$ where $P_{Z\mid M}$ is its distribution. Let $g: \mathbb{R}^2 \to \mathbb{R}$ and consider the following equation: $$ E[g(Z)\mid M=\mu]=0,  \forall \mu \in C, $$ where $C=\{\mu: \frac{\mu_1^2}{r_1^2}+\frac{\mu_2^2}{r_2^2}=1 \}$ for some given $r_1,r_2 > 0$ . That is, $C$ is an ellipse. It is not difficult to verify (see this question , see also Edit 3) that a solution to this equation is given by $$ g(x)= \frac{x_1^2}{r_1^2}+\frac{x_2^2}{r_2^2}- c, $$ where $c=\frac{1}{r_1^2}+\frac{1}{r_2^2}+1$ . In fact any function $g_a(x)= a g(x)$ for any $a \in \mathbb{R}$ is a solution. Question: Is $g$ a unique solution up to a multiplicative constant? Edit: If we need to make an assumption on  the class of allowed functions $g$ . Let us assume that $g$ 's are bounded by a quadratic monomial (i.e., for every $g$ there exists $a$ and $b$ such that $g(x) \le a \|x \|^2 +b$ ). Edit 2: If you want to avoid expectation notation. Everything can be alternatively written as $$ \iint g(z)\frac{1}{2 \pi} e^{-\frac{\|z-m\|^2}{2}} \, {\rm d} z=0, \, m\in C. $$ From here, one can see that this question is about a convolution. Edit 3: To see that $g(x)$ is a solution we use that the second moment of Gaussian is given by $E[Z_i^2\mid M_i=\mu_i]=1+\mu_i^2$ , which leads to \begin{align} E\left[\frac{Z_1^2}{r_1^2}+\frac{Z_2^2}{r_2^2}- c\mid M=\mu \right]&=  \frac{E[Z_1^2\mid M_1=\mu_1]}{r_1^2}+\frac{  E[Z_2^2\mid M_2=\mu_2]}{r_2^2}-c\\[6pt] &=\frac{1+\mu_1^2}{r_1^2}+\frac{  1+\mu_1^2}{r_2^2}-c\\[6pt] &=\frac{1}{r_1^2}+\frac 1 {r_2^2}+1-c, \end{align} where in the last step we used that $\mu$ is on the ellipse. Edit 4: The comment below shows that the solution is not unique when an ellipse is a circle.  However, I would still like to know the answer for a general ellipse.","Let be an i.i.d. Gaussian vector with mean where is its distribution. Let and consider the following equation: where for some given . That is, is an ellipse. It is not difficult to verify (see this question , see also Edit 3) that a solution to this equation is given by where . In fact any function for any is a solution. Question: Is a unique solution up to a multiplicative constant? Edit: If we need to make an assumption on  the class of allowed functions . Let us assume that 's are bounded by a quadratic monomial (i.e., for every there exists and such that ). Edit 2: If you want to avoid expectation notation. Everything can be alternatively written as From here, one can see that this question is about a convolution. Edit 3: To see that is a solution we use that the second moment of Gaussian is given by , which leads to where in the last step we used that is on the ellipse. Edit 4: The comment below shows that the solution is not unique when an ellipse is a circle.  However, I would still like to know the answer for a general ellipse.","Z \in \mathbb{R}^2 M P_{Z\mid M} g: \mathbb{R}^2 \to \mathbb{R} 
E[g(Z)\mid M=\mu]=0,  \forall \mu \in C,
 C=\{\mu: \frac{\mu_1^2}{r_1^2}+\frac{\mu_2^2}{r_2^2}=1 \} r_1,r_2 > 0 C 
g(x)= \frac{x_1^2}{r_1^2}+\frac{x_2^2}{r_2^2}- c,
 c=\frac{1}{r_1^2}+\frac{1}{r_2^2}+1 g_a(x)= a g(x) a \in \mathbb{R} g g g g a b g(x) \le a \|x \|^2 +b 
\iint g(z)\frac{1}{2 \pi} e^{-\frac{\|z-m\|^2}{2}} \, {\rm d} z=0, \, m\in C.
 g(x) E[Z_i^2\mid M_i=\mu_i]=1+\mu_i^2 \begin{align}
E\left[\frac{Z_1^2}{r_1^2}+\frac{Z_2^2}{r_2^2}- c\mid M=\mu \right]&=  \frac{E[Z_1^2\mid M_1=\mu_1]}{r_1^2}+\frac{  E[Z_2^2\mid M_2=\mu_2]}{r_2^2}-c\\[6pt]
&=\frac{1+\mu_1^2}{r_1^2}+\frac{  1+\mu_1^2}{r_2^2}-c\\[6pt]
&=\frac{1}{r_1^2}+\frac 1 {r_2^2}+1-c,
\end{align} \mu","['real-analysis', 'functional-analysis', 'probability-theory', 'expected-value', 'gaussian-integral']"
94,Can the idea of a 'function of a variable' be made rigorous?,Can the idea of a 'function of a variable' be made rigorous?,,"Suppose that $y=x^2$ . Very often people describe this relationship by saying that ' $y$ is a function of $x$ '. It seems that there are several logical problems with this statement: A function is simply a set of ordered pairs of numbers. It does not matter how you denote the input of a function, and there is no doubt that $x \mapsto x^2$ and $y \mapsto y^2$ are the same function. Indeed, in both cases $y$ and $x$ are simply dummy variables used to illustrate what happens when you plug in an arbitrary number into the function. $x$ and $y$ are often described as variables that are related in some way; in this case, with $y$ being the square of $x$ . However, in this Math Overflow post  Mike Shulman states that the idea of a variable is 'not a standard part of modern formalizations of mathematics'. Strangely, the idea of a dummy variable makes much more sense to me. For instance, when we say 'consider the function $f$ defined by $f(x)=x^2$ for all $x$ ', it is clear that the only purpose of the letter $x$ is to declare that the second entry of the ordered pair is the square of the first. If $y=x^2$ and $x \geq 0$ , then we just as well might write $x=\sqrt{y}$ . This flexibility is not really allowed when speaking of functions: $x \mapsto x^2$ and $x \mapsto \sqrt{x}$ are certainly not the same function. Perhaps it is possible evade this by treating $x$ as the independent variable and $y$ as the dependent variable. Some authors get around this ambiguity by saying that 'the function $y=x^2$ ' is just a shorthand for 'the function $y(x)=x^2$ ', which in turn is just a shorthand for 'the function $y$ defined by $y(x)=x^2$ for all x'. However, this doesn't seem to align with how people treat the relationship between $y$ and $x$ in practice. Even if we accept that $y=x^2$ is simply a shorthand for $y(x)=x^2$ , it still seems that there is a tendency towards treating $x$ as an independent variable representing the input of $y$ , as opposed to simply a dummy variable that can be replaced by any other letter. So I ask, in formal mathematics, is it possible to interpret $y=f(x)$ in such a way that $x$ and $y$ are variables representing the inputs and outputs of a function? And if so, how should the statement ' $y$ is a function of $x$ ' be understood?","Suppose that . Very often people describe this relationship by saying that ' is a function of '. It seems that there are several logical problems with this statement: A function is simply a set of ordered pairs of numbers. It does not matter how you denote the input of a function, and there is no doubt that and are the same function. Indeed, in both cases and are simply dummy variables used to illustrate what happens when you plug in an arbitrary number into the function. and are often described as variables that are related in some way; in this case, with being the square of . However, in this Math Overflow post  Mike Shulman states that the idea of a variable is 'not a standard part of modern formalizations of mathematics'. Strangely, the idea of a dummy variable makes much more sense to me. For instance, when we say 'consider the function defined by for all ', it is clear that the only purpose of the letter is to declare that the second entry of the ordered pair is the square of the first. If and , then we just as well might write . This flexibility is not really allowed when speaking of functions: and are certainly not the same function. Perhaps it is possible evade this by treating as the independent variable and as the dependent variable. Some authors get around this ambiguity by saying that 'the function ' is just a shorthand for 'the function ', which in turn is just a shorthand for 'the function defined by for all x'. However, this doesn't seem to align with how people treat the relationship between and in practice. Even if we accept that is simply a shorthand for , it still seems that there is a tendency towards treating as an independent variable representing the input of , as opposed to simply a dummy variable that can be replaced by any other letter. So I ask, in formal mathematics, is it possible to interpret in such a way that and are variables representing the inputs and outputs of a function? And if so, how should the statement ' is a function of ' be understood?",y=x^2 y x x \mapsto x^2 y \mapsto y^2 y x x y y x f f(x)=x^2 x x y=x^2 x \geq 0 x=\sqrt{y} x \mapsto x^2 x \mapsto \sqrt{x} x y y=x^2 y(x)=x^2 y y(x)=x^2 y x y=x^2 y(x)=x^2 x y y=f(x) x y y x,"['real-analysis', 'functions', 'notation']"
95,"If there is an into isometry from $(\mathbb{R}^m,\|\cdot\|_p)$ to $(\mathbb{R}^n, \|\cdot\|_q)$ where $m\leq n$, then $p=q$?","If there is an into isometry from  to  where , then ?","(\mathbb{R}^m,\|\cdot\|_p) (\mathbb{R}^n, \|\cdot\|_q) m\leq n p=q","Let $p,q\in [1,\infty)$ . Note that $p,q\neq\infty$ . Let $m\geq 2$ be a natural number. The paper Isometries of Finite-Dimensional Normed Spaces by Felix and Jesus asserts that if $(\mathbb{R}^m,\|\cdot\|_p)$ is isometric to $(\mathbb{R}^m, \|\cdot\|_q)$ , then $p =q$ . I am interested in the case when they have different dimensions. More precisely, Let $m,n\geq 2$ be natural numbers such that $m\leq n$ and $T:(\mathbb{R}^m,\|\cdot\|_p)\to (\mathbb{R}^n, \|\cdot\|_q)$ be a linear operator (Note that the dimension of domain and codomain are different). If $T$ is an isometry (not necessarily onto), does $p = q$ ? By the paper above, if $m=n$ , then we have $p=q$ . However, if $m<n$ , I am not sure whether the same result holds. If there is a reference that cites this result, it would be good if someone can provide it.","Let . Note that . Let be a natural number. The paper Isometries of Finite-Dimensional Normed Spaces by Felix and Jesus asserts that if is isometric to , then . I am interested in the case when they have different dimensions. More precisely, Let be natural numbers such that and be a linear operator (Note that the dimension of domain and codomain are different). If is an isometry (not necessarily onto), does ? By the paper above, if , then we have . However, if , I am not sure whether the same result holds. If there is a reference that cites this result, it would be good if someone can provide it.","p,q\in [1,\infty) p,q\neq\infty m\geq 2 (\mathbb{R}^m,\|\cdot\|_p) (\mathbb{R}^m, \|\cdot\|_q) p =q m,n\geq 2 m\leq n T:(\mathbb{R}^m,\|\cdot\|_p)\to (\mathbb{R}^n, \|\cdot\|_q) T p = q m=n p=q m<n","['real-analysis', 'functional-analysis', 'reference-request', 'isometry']"
96,Could we ever hope to integrate all functions?,Could we ever hope to integrate all functions?,,"The Riemann integral has a weakness, in that it cannot integrate many functions of interest, such as Dirichlet's function $\boldsymbol{1}_\mathbb{Q}$ . The Lebesgue and Henstock-Kurzweil integrals extend the range of functions which can be integrated, but still, there are functions which these integrals cannot deal with. My question is thus. Let $\mathcal{A} \subseteq \mathcal{P}(\mathbb{R})$ be a ""rich enough"" family of ""nice enough"" subsets of $\mathbb{R}$ (e.g. $\mathcal{A}$ could be the collection of all (bounded?) Borel subsets of $\mathbb{R}$ ). Let $\overline{\mathbb{R}} = \mathbb{R} \cup \{ -\infty, +\infty \}$ be the extended real line. Is it possible to define an ""integration functional"" $\mathbf{I}: \mathbb{R}^\mathbb{R} \times \mathcal{A} \to \overline{\mathbb{R}}$ satisfying the following: $\mathbf{I}$ is total (i.e. it gives a value for every function $f: \mathbb{R} \to \mathbb{R}$ and set $B \in \mathcal{A}$ ); $\mathbf{I}$ has the familiar ""nice"" properties we expect of integrals, e.g.: Linearity: $\mathbf{I}(cf+dg,B) = c\mathbf{I}(f,B) + d\mathbf{I}(g,B)$ ; Monotonicity: if $f(x) \leq g(x)$ for all $x \in B$ , then $\mathbf{I}(f,B) \leq \mathbf{I}(g,B)$ ; Translation invariance: if $f_c$ is defined by $f_c(x) = f(x+c)$ , then $\mathbf{I}(f,B) = \mathbf{I}(f_c,B+c)$ ; If $B,C \in \mathcal{A}$ are disjoint, then $\mathbf{I}(f,B \cup C) = \mathbf{I}(f,B) + \mathbf{I}(f,C)$ ; Perhaps there are other properties we might want that I've overlooked. Whenever $f: \mathbb{R} \to \mathbb{R}$ is (Riemann/Lebesgue/HK/etc)-integrable on $B$ , we have $\mathbf{I}(f,B) = \int_B f$ . In other words, is it possible to define a notion of integration which extends standard integration, and can integrate any function? Ideally, we would want $\mathcal{A} = \mathcal{P}(\mathbb{R})$ , but I recognise that we might have to restrict it to get a meaningful/nontrivial answer. My bet is that the answer will be much the same as Vitali's theorem. I'm guessing that over $\mathsf{ZF}$ , $\mathsf{AC}$ would prove there is no such $\mathbf{I}$ , while other ""nice"" axioms such as $\mathsf{AD}$ might allow one to construct such an $\mathbf{I}$ .","The Riemann integral has a weakness, in that it cannot integrate many functions of interest, such as Dirichlet's function . The Lebesgue and Henstock-Kurzweil integrals extend the range of functions which can be integrated, but still, there are functions which these integrals cannot deal with. My question is thus. Let be a ""rich enough"" family of ""nice enough"" subsets of (e.g. could be the collection of all (bounded?) Borel subsets of ). Let be the extended real line. Is it possible to define an ""integration functional"" satisfying the following: is total (i.e. it gives a value for every function and set ); has the familiar ""nice"" properties we expect of integrals, e.g.: Linearity: ; Monotonicity: if for all , then ; Translation invariance: if is defined by , then ; If are disjoint, then ; Perhaps there are other properties we might want that I've overlooked. Whenever is (Riemann/Lebesgue/HK/etc)-integrable on , we have . In other words, is it possible to define a notion of integration which extends standard integration, and can integrate any function? Ideally, we would want , but I recognise that we might have to restrict it to get a meaningful/nontrivial answer. My bet is that the answer will be much the same as Vitali's theorem. I'm guessing that over , would prove there is no such , while other ""nice"" axioms such as might allow one to construct such an .","\boldsymbol{1}_\mathbb{Q} \mathcal{A} \subseteq \mathcal{P}(\mathbb{R}) \mathbb{R} \mathcal{A} \mathbb{R} \overline{\mathbb{R}} = \mathbb{R} \cup \{ -\infty, +\infty \} \mathbf{I}: \mathbb{R}^\mathbb{R} \times \mathcal{A} \to \overline{\mathbb{R}} \mathbf{I} f: \mathbb{R} \to \mathbb{R} B \in \mathcal{A} \mathbf{I} \mathbf{I}(cf+dg,B) = c\mathbf{I}(f,B) + d\mathbf{I}(g,B) f(x) \leq g(x) x \in B \mathbf{I}(f,B) \leq \mathbf{I}(g,B) f_c f_c(x) = f(x+c) \mathbf{I}(f,B) = \mathbf{I}(f_c,B+c) B,C \in \mathcal{A} \mathbf{I}(f,B \cup C) = \mathbf{I}(f,B) + \mathbf{I}(f,C) f: \mathbb{R} \to \mathbb{R} B \mathbf{I}(f,B) = \int_B f \mathcal{A} = \mathcal{P}(\mathbb{R}) \mathsf{ZF} \mathsf{AC} \mathbf{I} \mathsf{AD} \mathbf{I}","['real-analysis', 'integration', 'set-theory', 'axioms', 'descriptive-set-theory']"
97,"If $|g(a)-g(b)| \leq |f(a)-f(b)|,$ for every $a,b$, and $f$ is a Darboux function, then $g$ is a Darboux function.","If  for every , and  is a Darboux function, then  is a Darboux function.","|g(a)-g(b)| \leq |f(a)-f(b)|, a,b f g","Let $f,g : \mathbb R \to \mathbb R$ be two functions. In particular, $f$ is a Darboux function (that is, functions satisfying the intermediate value property). If $|g(a)-g(b)| \leq |f(a)-f(b)|,$ for every real numbers $a,b$ , then I need to prove that $g$ is also a Darboux function. I feel like this is easier than most problems/exercises related to the properties of Darboux functions, however, I'm not sure how to approach this one. Source : I just checked the source again, and this is how the problem is written (a Romanian book with no solutions)","Let be two functions. In particular, is a Darboux function (that is, functions satisfying the intermediate value property). If for every real numbers , then I need to prove that is also a Darboux function. I feel like this is easier than most problems/exercises related to the properties of Darboux functions, however, I'm not sure how to approach this one. Source : I just checked the source again, and this is how the problem is written (a Romanian book with no solutions)","f,g : \mathbb R \to \mathbb R f |g(a)-g(b)| \leq |f(a)-f(b)|, a,b g","['real-analysis', 'continuity']"
98,Lower semi-continuity of one dimensional Hausdorff measure under Hausdorff convergence,Lower semi-continuity of one dimensional Hausdorff measure under Hausdorff convergence,,"Let $\mathcal H^1$ be the one-dimensional Hausdorff measure on $ \mathbb R^n$, and let $d_H$ be the Hausdorff metric on compact subsets of $\mathbb R^n$. If $K_n$ is connected for all $n \in \mathbb N$, and $d(K_n,K) \to 0$, I would like to know if  $$\mathcal H^1 (K) \leq \liminf\limits_{n \to \infty} \mathcal H^1(K_n).$$ If $K_n$ is not connected, this is not true. One can take $K_n = \bigcup\limits_{i=0}^{2^n-1} [{i \over 2^n}, {i + 1/2 \over 2^n}]$. Then $\mathcal H^1(K_n) = 1/2$, but $K_n \to [0,1]$. Also, for $\mathcal H^k$, $k$ an integer greater than one, this fails spectacularly. See this picture from Frank Morgan's book: The thing that I am trying to prove is used implicitly in Peter Jones's paper on the analyst's traveling salesman problem, I believe.","Let $\mathcal H^1$ be the one-dimensional Hausdorff measure on $ \mathbb R^n$, and let $d_H$ be the Hausdorff metric on compact subsets of $\mathbb R^n$. If $K_n$ is connected for all $n \in \mathbb N$, and $d(K_n,K) \to 0$, I would like to know if  $$\mathcal H^1 (K) \leq \liminf\limits_{n \to \infty} \mathcal H^1(K_n).$$ If $K_n$ is not connected, this is not true. One can take $K_n = \bigcup\limits_{i=0}^{2^n-1} [{i \over 2^n}, {i + 1/2 \over 2^n}]$. Then $\mathcal H^1(K_n) = 1/2$, but $K_n \to [0,1]$. Also, for $\mathcal H^k$, $k$ an integer greater than one, this fails spectacularly. See this picture from Frank Morgan's book: The thing that I am trying to prove is used implicitly in Peter Jones's paper on the analyst's traveling salesman problem, I believe.",,"['real-analysis', 'analysis', 'metric-spaces', 'metric-geometry']"
99,"Sequences $(\lambda_n)$ such that for every summable sequence $(a_n)$, $(\lambda_na_n)$ is also summable.","Sequences  such that for every summable sequence ,  is also summable.",(\lambda_n) (a_n) (\lambda_na_n),An oral examination exercise : Find all the sequences $(\lambda _n)_n$ of real numbers such that :   $$\sum a_n \; \text{is convergent} \Longrightarrow \sum \lambda_n a_n \; \text{is convergent}$$ I think the only working sequences are the ones which are bounded and ultimately of constant sign. I've not seen it on the site so if you have seen it please tell me how you would solve it. Thanks.,An oral examination exercise : Find all the sequences $(\lambda _n)_n$ of real numbers such that :   $$\sum a_n \; \text{is convergent} \Longrightarrow \sum \lambda_n a_n \; \text{is convergent}$$ I think the only working sequences are the ones which are bounded and ultimately of constant sign. I've not seen it on the site so if you have seen it please tell me how you would solve it. Thanks.,,"['real-analysis', 'sequences-and-series', 'analysis']"
