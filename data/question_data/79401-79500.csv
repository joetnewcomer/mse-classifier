,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Let $A\in M_{n \times k}(\mathbb{R})$. Show that $\det(A^TA)=\sum \det(B^2)$ where the sum runs through all $k \times k$ submatrices $B$ of $A$.,Let . Show that  where the sum runs through all  submatrices  of .,A\in M_{n \times k}(\mathbb{R}) \det(A^TA)=\sum \det(B^2) k \times k B A,"Let $A\in M_{n \times k}(\mathbb{R})$ . Show that \begin{equation} \det(A^TA)=\sum \det(B^2) \;\;\;\;\;\;\;(*) \end{equation} where the summation runs through all $k \times k$ submatrices $B$ of $A$ that result from deleting rows of $A$ . Proof attempt: If $n<k$ , then $A^TA$ , a $k \times k$ matrix, has rank at most $n<k$ . Thus, it is singular and has determinant 0. Also, since $n<k$ , there are no $k \times k$ submatrices of $A$ to consider. Thus, both sides of (*) are 0. If $n=k$ the only $k \times k$ submatrix of $A$ is itself so the right hand side of (*) becomes $$\det(A^2)=\det(A)\det(A)=\det(A^T)\det(A)$$ which is equal to the left hand side. However, I cannot seem to figure out the case for when $n>k$ . I would appreciate an approach or a reference for a solution.","Let . Show that where the summation runs through all submatrices of that result from deleting rows of . Proof attempt: If , then , a matrix, has rank at most . Thus, it is singular and has determinant 0. Also, since , there are no submatrices of to consider. Thus, both sides of (*) are 0. If the only submatrix of is itself so the right hand side of (*) becomes which is equal to the left hand side. However, I cannot seem to figure out the case for when . I would appreciate an approach or a reference for a solution.","A\in M_{n \times k}(\mathbb{R}) \begin{equation}
\det(A^TA)=\sum \det(B^2) \;\;\;\;\;\;\;(*)
\end{equation} k \times k B A A n<k A^TA k \times k n<k n<k k \times k A n=k k \times k A \det(A^2)=\det(A)\det(A)=\det(A^T)\det(A) n>k","['linear-algebra', 'matrices', 'determinant']"
1,Matrix expression for $\operatorname{vec}(X^{\top}X)$?,Matrix expression for ?,\operatorname{vec}(X^{\top}X),"Let $X$ be an $m$ by $n$ matrix. I would like to find the $n^2$ by $(mn)^2$ matrix $B$ such that $$ \operatorname{vec}\left(X^{\top}X\right) = B \operatorname{vec}\left(\operatorname{vec}\left(X\right)\left(\operatorname{vec}\left(X\right)\right)^{\top}\right).$$ Here $\operatorname{vec}(\cdot)$ is the vector function that unrolls a matrix into a vector columnwise. I would guess $B$ involves the Kronecker products, and some $I_m$ and $I_n$ matrices and some 1 vectors, but I am having a hard time with all the indices.","Let be an by matrix. I would like to find the by matrix such that Here is the vector function that unrolls a matrix into a vector columnwise. I would guess involves the Kronecker products, and some and matrices and some 1 vectors, but I am having a hard time with all the indices.","X m n n^2 (mn)^2 B 
\operatorname{vec}\left(X^{\top}X\right) = B \operatorname{vec}\left(\operatorname{vec}\left(X\right)\left(\operatorname{vec}\left(X\right)\right)^{\top}\right). \operatorname{vec}(\cdot) B I_m I_n","['matrices', 'kronecker-product']"
2,Correct notation for broadcasting operation,Correct notation for broadcasting operation,,"In Python numpy a row vector $u \in \mathbb{R}^n$ and its transposed $u^T$ can be added, multiplied or subtracted, yielding a $\mathbb{R}^{n\times n}$ matrix, where each element $(i,j)$ is the addition, subtraction or multiplication of the element $u_i$ and $u_j$ . For example import numpy as np x = np.array([[0],[1],[2]]) x-x.T yields the matrix [[ 0, -1, -2],  [ 1,  0, -1],  [ 2,  1,  0]] I am struggling however to understand how to write this in a nice mathematical notation. Rank 1 products can be written mathematically as $x x^T$ , but what for other operations, like addition? I don't think $x+x^T$ is a viable one, as in the case of product the dot product is meant.","In Python numpy a row vector and its transposed can be added, multiplied or subtracted, yielding a matrix, where each element is the addition, subtraction or multiplication of the element and . For example import numpy as np x = np.array([[0],[1],[2]]) x-x.T yields the matrix [[ 0, -1, -2],  [ 1,  0, -1],  [ 2,  1,  0]] I am struggling however to understand how to write this in a nice mathematical notation. Rank 1 products can be written mathematically as , but what for other operations, like addition? I don't think is a viable one, as in the case of product the dot product is meant.","u \in \mathbb{R}^n u^T \mathbb{R}^{n\times n} (i,j) u_i u_j x x^T x+x^T","['linear-algebra', 'matrices', 'notation', 'python']"
3,Eigenvalues of a $A^T A$,Eigenvalues of a,A^T A,"Given the matrix of order $1\times{n}$ , $A=(a_1, a_2, ..., a_n)$ , where $a_i$ are real; The question is to find all eigenvalues of $A^T A$ . I have proved that it is a non-invertible matrix, therefore $0$ is one of the values. And the product matrix is an $nxn$ matrix with the diagonal elements being $a_1^2, a_2^2,...,a_n^2$ . I am struggling with finding the other eigenvalues, tried by calculating the det of $A - aI$ , but didn't go anywhere.","Given the matrix of order , , where are real; The question is to find all eigenvalues of . I have proved that it is a non-invertible matrix, therefore is one of the values. And the product matrix is an matrix with the diagonal elements being . I am struggling with finding the other eigenvalues, tried by calculating the det of , but didn't go anywhere.","1\times{n} A=(a_1, a_2, ..., a_n) a_i A^T A 0 nxn a_1^2, a_2^2,...,a_n^2 A - aI","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
4,Commutative subring of matrices iff trivial unit group,Commutative subring of matrices iff trivial unit group,,"Let $R$ be a ring and let $T := \left\{\begin{bmatrix}a & b \\ 0 & c\end{bmatrix} \in \text{Mat}_2(R) \mid a,b,c \in R\right\}$ . I have shown that $T$ is a subring of $M_2(R)$ which is noncommutative for $R ≠ \{0\}$ , and $$\begin{bmatrix}a & b \\ 0 & c\end{bmatrix} \in T^* \iff \{a,c\} \subseteq R^*.$$ Now it should also be true that $T^*$ is abelian if and only if $R^* = \{1\}$ . Now of course you can, assuming $T^*$ is abelian, take two arbitrary elements $\begin{bmatrix}a & b \\ 0 & c\end{bmatrix}, \begin{bmatrix}x & y \\ 0 & z\end{bmatrix} \in T^*$ and derive the necessary equalities $ax = xa, ay+bz = xb+yc$ and $cz = zc$ , but I don't see how that would imply that $R$ has a trivial unit group. Another attempt would be to assume that $R^* \setminus \{1\} ≠ \emptyset$ , let $u \in R^* \setminus \{1\}$ and consider some products like $$\begin{bmatrix}u & 1 \\ 0 & u\end{bmatrix}\begin{bmatrix}u & 0 \\ 0 & u\end{bmatrix}.$$ Then you'd see that $u^2 = 1$ , but not necessarily that $u = 1$ . It seems there should be a smarter argument.","Let be a ring and let . I have shown that is a subring of which is noncommutative for , and Now it should also be true that is abelian if and only if . Now of course you can, assuming is abelian, take two arbitrary elements and derive the necessary equalities and , but I don't see how that would imply that has a trivial unit group. Another attempt would be to assume that , let and consider some products like Then you'd see that , but not necessarily that . It seems there should be a smarter argument.","R T := \left\{\begin{bmatrix}a & b \\ 0 & c\end{bmatrix} \in \text{Mat}_2(R) \mid a,b,c \in R\right\} T M_2(R) R ≠ \{0\} \begin{bmatrix}a & b \\ 0 & c\end{bmatrix} \in T^* \iff \{a,c\} \subseteq R^*. T^* R^* = \{1\} T^* \begin{bmatrix}a & b \\ 0 & c\end{bmatrix}, \begin{bmatrix}x & y \\ 0 & z\end{bmatrix} \in T^* ax = xa, ay+bz = xb+yc cz = zc R R^* \setminus \{1\} ≠ \emptyset u \in R^* \setminus \{1\} \begin{bmatrix}u & 1 \\ 0 & u\end{bmatrix}\begin{bmatrix}u & 0 \\ 0 & u\end{bmatrix}. u^2 = 1 u = 1","['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory']"
5,Is there a name for a square matrix with constant diagonal and off-diagonal elements?,Is there a name for a square matrix with constant diagonal and off-diagonal elements?,,"I am interested in real symmetric matrices of the form: $$\mathbf{M} = \begin{bmatrix} a & t & t & \cdots & t \\ t & a & t & \cdots & t \\ t & t & a & \cdots & t \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ t & t & t & \cdots & a \text{ } \text{ } \\ \end{bmatrix}.$$ This is a simple matrix form where the diagonal elements have a constant value $a \in \mathbb{R}$ and the off-diagonal elements have a constant value $t \in \mathbb{R}$ .  Some useful special cases of this matrix form are the centering matrix and the equicorrelation matrix .  (This matrix form is also a particular case of the Toeplitz matrix , but it is much simpler than that general form.) Question: Does this matrix form have a name?","I am interested in real symmetric matrices of the form: This is a simple matrix form where the diagonal elements have a constant value and the off-diagonal elements have a constant value .  Some useful special cases of this matrix form are the centering matrix and the equicorrelation matrix .  (This matrix form is also a particular case of the Toeplitz matrix , but it is much simpler than that general form.) Question: Does this matrix form have a name?","\mathbf{M} = \begin{bmatrix}
a & t & t & \cdots & t \\
t & a & t & \cdots & t \\
t & t & a & \cdots & t \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
t & t & t & \cdots & a \text{ } \text{ } \\
\end{bmatrix}. a \in \mathbb{R} t \in \mathbb{R}","['matrices', 'terminology', 'symmetric-matrices']"
6,"Difference between the projection matrices arising from 1) the normal equations, and 2) an orthonormal basis of a subspace.","Difference between the projection matrices arising from 1) the normal equations, and 2) an orthonormal basis of a subspace.",,"I'm having a bit of trouble tying two related ideas together, and I think I'm just missing a silly detail somewhere. In the standard formulation of linear least-squares, it can be shown that given a full-rank matrix $A$ , one can form the projection matrix $$ P_A = A(A^TA)^{-1}A^T $$ which acts on vectors by orthogonally projecting them onto the column space of $A$ . On the other hand, given a subspace $S \subseteq V$ of a vector space, where $S = \mathrm{Span}(\mathbf u_1, \mathbf u_2, \cdots \mathbf u_k)$ and the $\mathbf u_i$ form an orthonormal basis of $S$ , then one can construct a matrix with each $\mathbf u_i$ in the $i$ th column: $$  U = [\mathbf u_1, \mathbf u_2, \cdots \mathbf u_k]. $$ One can then form the projection $$ P_S = UU^T $$ which projects vectors in $V$ onto the subspace $S$ . I have two questions: The matrix $U$ seems to be orthogonal, which would seem to mean it satisfies $UU^T = I$ . What exactly am I missing? The column space of $U$ is $S$ by construction, so how would one recover the formula for $P_S$ by letting $A=U$ in the formula for $P_A$ ? If $U$ is orthogonal, I see how $(A^TA)^{-1} = I$ , but by the first question, it would also seem to reduce the remaining $AA^T$ to $I$ as well.","I'm having a bit of trouble tying two related ideas together, and I think I'm just missing a silly detail somewhere. In the standard formulation of linear least-squares, it can be shown that given a full-rank matrix , one can form the projection matrix which acts on vectors by orthogonally projecting them onto the column space of . On the other hand, given a subspace of a vector space, where and the form an orthonormal basis of , then one can construct a matrix with each in the th column: One can then form the projection which projects vectors in onto the subspace . I have two questions: The matrix seems to be orthogonal, which would seem to mean it satisfies . What exactly am I missing? The column space of is by construction, so how would one recover the formula for by letting in the formula for ? If is orthogonal, I see how , but by the first question, it would also seem to reduce the remaining to as well.","A 
P_A = A(A^TA)^{-1}A^T
 A S \subseteq V S = \mathrm{Span}(\mathbf u_1, \mathbf u_2, \cdots \mathbf u_k) \mathbf u_i S \mathbf u_i i  
U = [\mathbf u_1, \mathbf u_2, \cdots \mathbf u_k].
 
P_S = UU^T
 V S U UU^T = I U S P_S A=U P_A U (A^TA)^{-1} = I AA^T I","['linear-algebra', 'matrices', 'matrix-equations', 'projection-matrices']"
7,"Free subgroups of $PSL(2,\mathbb{Z})$ of index 6",Free subgroups of  of index 6,"PSL(2,\mathbb{Z})","There are two ""natural"" subgroups of $PSL(2,\mathbb{Z})\cong C_2\ast C_3$ of index 6. One is the congruence subgroup $\Gamma_0(2)$ which is the kernel of the map $PSL(2,\mathbb{Z})\to PSL(2,\mathbb{Z}/2\mathbb{Z})$ . The other subgroup $H$ is the kernel of the map $C_2\ast C_3\to C_2\times C_3$ . Here are two similarities between these two subgroups: Both $\Gamma_0(2)$ and $H$ are subgroups of $PSL(2,\mathbb{Z})$ of index 6. Both $\Gamma_0(2)$ and $H$ are free groups of rank 2. However, $PSL(2,\mathbb{Z}/2\mathbb{Z})\cong S_3$ and $C_2\times C_3\cong C_6$ so these are different subgroups of $PSL(2,\mathbb{Z})$ . Moreover, $\Gamma_0(2)$ is freely generated by the matrices $\begin{bmatrix}1&2\\0&1\end{bmatrix}$ and $\begin{bmatrix}1&0\\2&1\end{bmatrix}$ whereas $H$ is freely generated by the matrices $\begin{bmatrix}2&1\\1&1\end{bmatrix}$ and $\begin{bmatrix}1&1\\1&2\end{bmatrix}$ . This last statement can be seen by noting that if $a=\begin{bmatrix}0&-1\\0&1\end{bmatrix}$ generates $C_2$ and $b=\begin{bmatrix}-1&-1\\1&0\end{bmatrix}$ generates $C_3$ then $H$ is freely generated by $abab^2$ and $ab^2ab$ . What is going on here? More precisely, Are these two subgroups the largest free subgroups of $PSL(2,\mathbb{Z})$ ? Are there any other free subgroups of $PSL(2,\mathbb{Z})$ of index 6? Is there any reason to expect that $PSL(2,\mathbb{Z})$ contains two free subgroups of rank 2 and index 6 with different quotients?","There are two ""natural"" subgroups of of index 6. One is the congruence subgroup which is the kernel of the map . The other subgroup is the kernel of the map . Here are two similarities between these two subgroups: Both and are subgroups of of index 6. Both and are free groups of rank 2. However, and so these are different subgroups of . Moreover, is freely generated by the matrices and whereas is freely generated by the matrices and . This last statement can be seen by noting that if generates and generates then is freely generated by and . What is going on here? More precisely, Are these two subgroups the largest free subgroups of ? Are there any other free subgroups of of index 6? Is there any reason to expect that contains two free subgroups of rank 2 and index 6 with different quotients?","PSL(2,\mathbb{Z})\cong C_2\ast C_3 \Gamma_0(2) PSL(2,\mathbb{Z})\to PSL(2,\mathbb{Z}/2\mathbb{Z}) H C_2\ast C_3\to C_2\times C_3 \Gamma_0(2) H PSL(2,\mathbb{Z}) \Gamma_0(2) H PSL(2,\mathbb{Z}/2\mathbb{Z})\cong S_3 C_2\times C_3\cong C_6 PSL(2,\mathbb{Z}) \Gamma_0(2) \begin{bmatrix}1&2\\0&1\end{bmatrix} \begin{bmatrix}1&0\\2&1\end{bmatrix} H \begin{bmatrix}2&1\\1&1\end{bmatrix} \begin{bmatrix}1&1\\1&2\end{bmatrix} a=\begin{bmatrix}0&-1\\0&1\end{bmatrix} C_2 b=\begin{bmatrix}-1&-1\\1&0\end{bmatrix} C_3 H abab^2 ab^2ab PSL(2,\mathbb{Z}) PSL(2,\mathbb{Z}) PSL(2,\mathbb{Z})","['matrices', 'group-theory', 'free-groups', 'free-product', 'modular-group']"
8,How can I construct a nilpotent matrix with the property $A^2 \not= 0$ but $A^3=0$,How can I construct a nilpotent matrix with the property  but,A^2 \not= 0 A^3=0,"An example of a matrix $A$ that has the property $A^2=0$ would be $$A= \begin{pmatrix} 0 &1 \\ 0&0\end{pmatrix}$$ However, I can't seem to figure out a ""formula"" to construct a matrix that has the property $A^3=0$ but $A^2 \not = 0$ . Or in general, a formula for a matrix that has the property $A^k=0$ with $A^{k-1} \not=0$ . Does such a formula even exist?","An example of a matrix that has the property would be However, I can't seem to figure out a ""formula"" to construct a matrix that has the property but . Or in general, a formula for a matrix that has the property with . Does such a formula even exist?",A A^2=0 A= \begin{pmatrix} 0 &1 \\ 0&0\end{pmatrix} A^3=0 A^2 \not = 0 A^k=0 A^{k-1} \not=0,"['linear-algebra', 'matrices']"
9,Subspace basis of $\mathbb{R}^n$ only with positive values,Subspace basis of  only with positive values,\mathbb{R}^n,"It seems obvious but I didn't find a proof yet:  Let $U$ be an arbitrary subspace of $\mathbb{R}^n$ . Set $m:=\dim{U}$ . Can $U$ be written as $U=\mathrm{span}\{b_1,\dotsc,b_m\}$ , $b_j=\begin{pmatrix}b_{i,1}\\\vdots\\b_{i,n}\end{pmatrix}$ with $b_{i,j}\ge0\;\forall i\in\{1,\dotsc,m\}\;\forall j\in\{1,\dotsc,n\}$ ? An equivalent formulation for this question: $\exists B=\begin{pmatrix}b_{1,1}&\cdots &b_{1,n}\\\vdots&&\vdots\\b_{m,1}&\cdots &b_{m,n}\end{pmatrix}\in\mathbb{R}^{m\times n}: U=\mathrm{img}(B)$ with $b_{i,j}\ge0\;\forall i\in\{1,\dotsc,m\}\;\forall j\in\{1,\dotsc,n\}$ ? If yes, can you please give a construction algorithm for a given base $\{u_1,\dotsc,u_m\}$ of $U$ ?","It seems obvious but I didn't find a proof yet:  Let be an arbitrary subspace of . Set . Can be written as , with ? An equivalent formulation for this question: with ? If yes, can you please give a construction algorithm for a given base of ?","U \mathbb{R}^n m:=\dim{U} U U=\mathrm{span}\{b_1,\dotsc,b_m\} b_j=\begin{pmatrix}b_{i,1}\\\vdots\\b_{i,n}\end{pmatrix} b_{i,j}\ge0\;\forall i\in\{1,\dotsc,m\}\;\forall j\in\{1,\dotsc,n\} \exists B=\begin{pmatrix}b_{1,1}&\cdots &b_{1,n}\\\vdots&&\vdots\\b_{m,1}&\cdots &b_{m,n}\end{pmatrix}\in\mathbb{R}^{m\times n}: U=\mathrm{img}(B) b_{i,j}\ge0\;\forall i\in\{1,\dotsc,m\}\;\forall j\in\{1,\dotsc,n\} \{u_1,\dotsc,u_m\} U","['matrices', 'vector-spaces']"
10,"Given an n by n matrix of 0s and 1s, find the maximum number of 1s you can remove if you can only remove 1s with another 1 in the same row or column","Given an n by n matrix of 0s and 1s, find the maximum number of 1s you can remove if you can only remove 1s with another 1 in the same row or column",,"This is a computer science interview question I heard from a friend and we can't seem to figure it out. Basically, given a square matrix of 0s and 1s, you can remove any 1 one at a time, but only if at the current time there is another 1 in the same row or column as the 1 to be removed. What is an algorithm to calculate the maximum number of 1s that can be removed for any such matrix? I suspect this has something to do with graph theory (treating the matrix as an adjacency matrix) or possibly linear algebra, but I'm not certain. Any advice is appreciated. We did figure out the following: treating the matrix as a directed graph, where a 1 at (i,j) indicates an edge from i to j, then we can remove an edge from the graph iff after removing the edge there would still be an edge from i to another vertex OR there would still be a vertex from j to another vertex. So we want to remove the greatest number of edges (where we count a bidirectional edge as two different edges, one in each direction).","This is a computer science interview question I heard from a friend and we can't seem to figure it out. Basically, given a square matrix of 0s and 1s, you can remove any 1 one at a time, but only if at the current time there is another 1 in the same row or column as the 1 to be removed. What is an algorithm to calculate the maximum number of 1s that can be removed for any such matrix? I suspect this has something to do with graph theory (treating the matrix as an adjacency matrix) or possibly linear algebra, but I'm not certain. Any advice is appreciated. We did figure out the following: treating the matrix as a directed graph, where a 1 at (i,j) indicates an edge from i to j, then we can remove an edge from the graph iff after removing the edge there would still be an edge from i to another vertex OR there would still be a vertex from j to another vertex. So we want to remove the greatest number of edges (where we count a bidirectional edge as two different edges, one in each direction).",,"['linear-algebra', 'matrices', 'graph-theory', 'algorithms', 'computer-science']"
11,"Proof matrix $A=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}^n , \text{ when } n \in \mathbb{N}$ [duplicate]",Proof matrix  [duplicate],"A=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}^n , \text{ when } n \in \mathbb{N}","This question already has answers here : How can I show that $\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^n = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$? (7 answers) Closed 5 years ago . Problem Find generalitazion for matrix A exponents, when $n\in\{1,2,3,\dots\},n \in \mathbb{N}$ $$A^n=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}^n , \text{ when } n \in \mathbb{N}$$ Proof generalization by induction. Attempt to solve By computing a set of $A$ exponent's $n\in \{\ 1,2,3,4 \}$ . It is possible  to form generalization that is applicable for set $n\in \{1,2,3,4 \}$ $$ A^1=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix},A^2=\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix},A^3=\begin{bmatrix} 1 & 3 \\ 0 & 1 \end{bmatrix},A^4=\begin{bmatrix} 1 & 4 \\ 0 & 1 \end{bmatrix} \dots A^n =\begin{bmatrix} 1 & n \\ 0 & 1 \end{bmatrix} $$ Induction proof Induction hypothesis Assume expression is valid when $n=k$ $$ A^k = \begin{bmatrix} 1 & k \\ 0 & 1\end{bmatrix} $$ Base case When $n=1$ $$ A^1=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} $$ which is valid by definition. Induction step When $n=k+1$ $$ A^{k+1} = \begin{bmatrix} 1 & k+1 \\ 0 & 1\end{bmatrix}$$ $$ A^{k+1}=A^kA^1 $$ By utilizing induction hypothesis we have $$ \implies A^{k+1}=\begin{bmatrix} 1 & k \\ 0 & 1\end{bmatrix}\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} $$ By utilizing matrix multiplication we have $$ \implies A^{k+1} \begin{bmatrix} 1\cdot 1 + k\cdot 0 & 1 \cdot 1 + 1 \cdot k \\ 0 \cdot 1 + 1 \cdot 0 & 0 \cdot 1 + 1 \cdot 1 \end{bmatrix} $$ $$ \implies A^{k+1}=\begin{bmatrix} 1 & k+1 \\ 0 & 1 \end{bmatrix} $$ $$ \tag*{$\square$} $$ EDIT The point of posting this was to have comment on if my solution seems correct or not. If you can notice something that doesn't look right, let me know !","This question already has answers here : How can I show that $\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^n = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$? (7 answers) Closed 5 years ago . Problem Find generalitazion for matrix A exponents, when Proof generalization by induction. Attempt to solve By computing a set of exponent's . It is possible  to form generalization that is applicable for set Induction proof Induction hypothesis Assume expression is valid when Base case When which is valid by definition. Induction step When By utilizing induction hypothesis we have By utilizing matrix multiplication we have EDIT The point of posting this was to have comment on if my solution seems correct or not. If you can notice something that doesn't look right, let me know !","n\in\{1,2,3,\dots\},n \in \mathbb{N} A^n=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}^n , \text{ when } n \in \mathbb{N} A n\in \{\ 1,2,3,4 \} n\in \{1,2,3,4 \}  A^1=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix},A^2=\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix},A^3=\begin{bmatrix} 1 & 3 \\ 0 & 1 \end{bmatrix},A^4=\begin{bmatrix} 1 & 4 \\ 0 & 1 \end{bmatrix} \dots A^n =\begin{bmatrix} 1 & n \\ 0 & 1 \end{bmatrix}  n=k 
A^k = \begin{bmatrix} 1 & k \\ 0 & 1\end{bmatrix}
 n=1 
A^1=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
 n=k+1  A^{k+1} = \begin{bmatrix} 1 & k+1 \\ 0 & 1\end{bmatrix}  A^{k+1}=A^kA^1   \implies A^{k+1}=\begin{bmatrix} 1 & k \\ 0 & 1\end{bmatrix}\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}   \implies A^{k+1} \begin{bmatrix} 1\cdot 1 + k\cdot 0 & 1 \cdot 1 + 1 \cdot k \\ 0 \cdot 1 + 1 \cdot 0 & 0 \cdot 1 + 1 \cdot 1 \end{bmatrix}   \implies A^{k+1}=\begin{bmatrix} 1 & k+1 \\ 0 & 1 \end{bmatrix}   \tag*{\square} ","['matrices', 'proof-verification', 'induction']"
12,Correlation matrix with same pairwise correlation coefficient,Correlation matrix with same pairwise correlation coefficient,,"Question Correlation matrix. Consider 𝑛 random variables with the same pairwise correlation coefficient $\rho_n$ . Find the highest possible value of $\rho_n$ for a) n=3 b) n=4 c) general, n $\geq$ 2 HINT: Correlation matrix must be positive semi-definite. My Workings This is what I infer from ""same pairwise coefficients"": $$     \begin{pmatrix}     1 & \rho_n & \rho_n \\     \rho_n & 1 & \rho_n \\     \rho_n & \rho_n & 1 \\     \end{pmatrix} $$ Because a correlation matrix is positive semi-definite, all principal minors have to be positive For n=3, the principal minors calculations yield $∣H_1∣$ =1 $∣H_2∣$ =1- $\rho_n^2$ $\geq 0$ $\Rightarrow $ $\rho_n$ $\leq 1$ $∣H_3∣=\rho_n^2$ - $\rho_n$ $\geq 0$ $\Rightarrow $ $\rho_n$ $\geq 1$ $∣H_4∣=(1^3$ +2 $\rho_n^3$ )-(3 $\rho_n^2$ ) $\geq 0$ $ . $ I solved for the roots and found 1 Conclusion: For n=3, max $\rho_n$ =1 I did the same method for n=4 and found max $\rho_n$ =1 again My Problem Result looks too simple and false. Method is tedious I have no idea how to do the general case. (by induction ?) Thank you for your help $\\$ INDUCTION ATTEMPT $H_0$ : n=2 $$     \begin{pmatrix}     1 & \rho_2 \\     \rho_2 & 1 \\     \end{pmatrix} $$ 1- $\rho_2^2 \geq 0 $ $\Rightarrow - \frac{1}{2-1} \leq \rho_2 \leq 1$ $H_n$ : Pn: Suppose that for a nxn correlation matrix $ A_n$ with same pairwise coefficients $ \rho_n$ , $ -\frac{1}{n-1} \leq \rho_n \leq 1 $ holds $H_{n+1}$ : $ -\frac{1}{n-1} \leq \rho_n \leq 1 $ $ \Leftarrow \Rightarrow$ $- \frac{1}{(n+1)-1} \leq \rho_{n+1} \leq 1 $ $ \Leftarrow \Rightarrow$ $- \frac{1}{n} \leq \rho_{n+1} \leq 1 $ And because for all n>1, $- \frac{1}{n} \geq -\frac{1}{n-1}$ $ -\frac{1}{n-1} \leq \rho_{n+1} \leq 1 $","Question Correlation matrix. Consider 𝑛 random variables with the same pairwise correlation coefficient . Find the highest possible value of for a) n=3 b) n=4 c) general, n 2 HINT: Correlation matrix must be positive semi-definite. My Workings This is what I infer from ""same pairwise coefficients"": Because a correlation matrix is positive semi-definite, all principal minors have to be positive For n=3, the principal minors calculations yield =1 =1- - +2 )-(3 ) I solved for the roots and found 1 Conclusion: For n=3, max =1 I did the same method for n=4 and found max =1 again My Problem Result looks too simple and false. Method is tedious I have no idea how to do the general case. (by induction ?) Thank you for your help INDUCTION ATTEMPT : n=2 1- : Pn: Suppose that for a nxn correlation matrix with same pairwise coefficients , holds : And because for all n>1,","\rho_n \rho_n \geq 
    \begin{pmatrix}
    1 & \rho_n & \rho_n \\
    \rho_n & 1 & \rho_n \\
    \rho_n & \rho_n & 1 \\
    \end{pmatrix}
 ∣H_1∣ ∣H_2∣ \rho_n^2 \geq 0 \Rightarrow  \rho_n \leq 1 ∣H_3∣=\rho_n^2 \rho_n \geq 0 \Rightarrow  \rho_n \geq 1 ∣H_4∣=(1^3 \rho_n^3 \rho_n^2 \geq 0  .  \rho_n \rho_n \\ H_0 
    \begin{pmatrix}
    1 & \rho_2 \\
    \rho_2 & 1 \\
    \end{pmatrix}
 \rho_2^2 \geq 0  \Rightarrow - \frac{1}{2-1} \leq \rho_2 \leq 1 H_n  A_n  \rho_n  -\frac{1}{n-1} \leq \rho_n \leq 1  H_{n+1}  -\frac{1}{n-1} \leq \rho_n \leq 1   \Leftarrow \Rightarrow - \frac{1}{(n+1)-1} \leq \rho_{n+1} \leq 1   \Leftarrow \Rightarrow - \frac{1}{n} \leq \rho_{n+1} \leq 1  - \frac{1}{n} \geq -\frac{1}{n-1}  -\frac{1}{n-1} \leq \rho_{n+1} \leq 1 ","['matrices', 'statistics', 'finance', 'economics']"
13,We have to show that $n\times n$ matrices $A$ and $B$ are nilpotent.,We have to show that  matrices  and  are nilpotent.,n\times n A B,"Let $A$ and $B$ be $n\times n$ matrices with entries from some field $\mathbb{F}$ . Let $c_1,\ldots,c_{n+1}$ be $n+1$ distinct elements in $\mathbb{F}$ such that $A+c_1B,\ldots,A+c_{n+1}B$ are all nilpotent. Then how can I show that $A$ and $B$ are nilpotent.",Let and be matrices with entries from some field . Let be distinct elements in such that are all nilpotent. Then how can I show that and are nilpotent.,"A B n\times n \mathbb{F} c_1,\ldots,c_{n+1} n+1 \mathbb{F} A+c_1B,\ldots,A+c_{n+1}B A B","['linear-algebra', 'matrices', 'vector-spaces']"
14,What are practical examples of Toeplitz matrices?,What are practical examples of Toeplitz matrices?,,"A Toeplitz matrix is one in which each descending diagonal from left to right is constant. Given that structure, matrix operations are sometimes much faster. Where are Toeplitz matrices likely to occur?","A Toeplitz matrix is one in which each descending diagonal from left to right is constant. Given that structure, matrix operations are sometimes much faster. Where are Toeplitz matrices likely to occur?",,"['linear-algebra', 'matrices', 'big-list', 'applications', 'toeplitz-matrices']"
15,A functional equation of a matrix,A functional equation of a matrix,,"How would one prove the following theorem? $p(A)$ is a nonzero polynomial of the entries of $A$ and satisfies $p(AB)=p(A)p(B)$ , for all square matrices $A$ and $B$ of complex numbers. Prove $p(A)=(\det\,A)^k$ for some nonnegative integer $k$ . My attempt: A matrix $A$ can be Shur-decomposed  into $A=QTQ^\dagger$ where $T$ is a triangular matrix and $Q$ a unitary matrix. Now $p(Q)p(Q^\dagger)=p(QQ^\dagger)=p(I)=1$ where the last equation comes from the second paragraph below. Now $p(A)=p(Q)p(T)p(Q^\dagger)=p(T)$ . If we can show $p(T)=(\Pi_i\ T_{i,i})^k$ for some integer $k$ , then we are done. Now if $T$ is diagonal with its diagonal entries all equal to $1$ except one being a complex variable $x$ , we can show $p(T)=x^k$ for some nonnegative integer $k$ . Let $p(T)=\sum_{i=0}^k a_ix^i$ for some nonnegative integer $k$ and $a_k\ne0$ . $\sum_{i=0}^k a_ix^{2i}=p(T^2)=p(T)^2=\big(\sum_{i=0}^k a_ix^i\big)^2$ . Expand the last expression and collecting coefficients of $\{x^i\}_{i=0}^k$ . By the linear independence of $\{x^i\}_{i=0}^k$ , the coefficient of the left hand side and that of the right hand side of the same order terms have to match. Considering the coefficents of the terms of order no less than $k$ , we draw the following conclusion. The coefficients of the odd order terms have to be zero. $a_k=1$ . Recursively we conclude $a_i=0,\,\forall i<k$ . As $p(T_1T_2)=p(T_1)p(T_2)$ , $p(T)=\Pi_i\ T_{i,i}$ for any diagonal matrix $T$ . But I am unable to proceed further to the triangular matrix.","How would one prove the following theorem? is a nonzero polynomial of the entries of and satisfies , for all square matrices and of complex numbers. Prove for some nonnegative integer . My attempt: A matrix can be Shur-decomposed  into where is a triangular matrix and a unitary matrix. Now where the last equation comes from the second paragraph below. Now . If we can show for some integer , then we are done. Now if is diagonal with its diagonal entries all equal to except one being a complex variable , we can show for some nonnegative integer . Let for some nonnegative integer and . . Expand the last expression and collecting coefficients of . By the linear independence of , the coefficient of the left hand side and that of the right hand side of the same order terms have to match. Considering the coefficents of the terms of order no less than , we draw the following conclusion. The coefficients of the odd order terms have to be zero. . Recursively we conclude . As , for any diagonal matrix . But I am unable to proceed further to the triangular matrix.","p(A) A p(AB)=p(A)p(B) A B p(A)=(\det\,A)^k k A A=QTQ^\dagger T Q p(Q)p(Q^\dagger)=p(QQ^\dagger)=p(I)=1 p(A)=p(Q)p(T)p(Q^\dagger)=p(T) p(T)=(\Pi_i\ T_{i,i})^k k T 1 x p(T)=x^k k p(T)=\sum_{i=0}^k a_ix^i k a_k\ne0 \sum_{i=0}^k a_ix^{2i}=p(T^2)=p(T)^2=\big(\sum_{i=0}^k a_ix^i\big)^2 \{x^i\}_{i=0}^k \{x^i\}_{i=0}^k k a_k=1 a_i=0,\,\forall i<k p(T_1T_2)=p(T_1)p(T_2) p(T)=\Pi_i\ T_{i,i} T","['linear-algebra', 'matrices', 'polynomials', 'determinant', 'functional-equations']"
16,Why is $\left(I - M M^T \right)^{-1} M = M\left(I - M^T M \right)^{-1}$,Why is,\left(I - M M^T \right)^{-1} M = M\left(I - M^T M \right)^{-1},"While trying to generalize a certain formula from the scalar case to the matrix case, I came across the following curious observation: Let $M$ be a real, not necessarily square matrix, then $$\left(I -  M M^T \right)^{-1} M = M\left(I -  M^T M \right)^{-1}$$ which seems to be true in general. It holds, for example, for this randomly chosen case $$M=\left( \begin{array}{ccc}  -1 & 7 & -4 \\  -1 & 5 & 7 \\  -4 & 7 & 4 \\  -2 & -5 & 0 \\  2 & -1 & -6 \\ \end{array} \right)$$ Although numerical examples convinced me that this is true, I do not understand why. Any hints?","While trying to generalize a certain formula from the scalar case to the matrix case, I came across the following curious observation: Let $M$ be a real, not necessarily square matrix, then $$\left(I -  M M^T \right)^{-1} M = M\left(I -  M^T M \right)^{-1}$$ which seems to be true in general. It holds, for example, for this randomly chosen case $$M=\left( \begin{array}{ccc}  -1 & 7 & -4 \\  -1 & 5 & 7 \\  -4 & 7 & 4 \\  -2 & -5 & 0 \\  2 & -1 & -6 \\ \end{array} \right)$$ Although numerical examples convinced me that this is true, I do not understand why. Any hints?",,"['matrices', 'matrix-equations']"
17,Product of binary matrices with binary eigenvalues,Product of binary matrices with binary eigenvalues,,"Consider two binary matrices with obvious patterns: $$ C= \begin{bmatrix} 1 &0 &0 &0 &0 &0 &0\\ 1 &0 &0 &0 &0 &0 &0\\ 0 &1 &0 &0 &0 &0 &0\\ 0 &1 &0 &0 &0 &0 &0\\ 0 &0 &1 &0 &0 &0 &0\\ 0 &0 &1 &0 &0 &0 &0\\ 0 &0 &0 &1 &0 &0 &0 \end{bmatrix} $$ and $$ T= \begin{bmatrix} 1 &1 &0 &0 &0 &0 &0\\ 0 &1 &1 &0 &0 &0 &0\\ 0 &0 &1 &1 &0 &0 &0\\ 0 &0 &0 &1 &1 &0 &0\\ 0 &0 &0 &0 &1 &1 &0\\ 0 &0 &0 &0 &0 &1 &1\\ 0 &0 &0 &0 &0 &0 &1 \end{bmatrix} $$ The eigenvalues of the matrices $T^n C,n=0,1,2,3$ are zeros and consecutive powers of $2$ equal to $0,1,2,4$ . I'd like to have a proof of the generalization of this fact for matrices of larger size with the same patterns. Note, the entries of $T^n C$ in the left upper corner are zeros and binomial coefficients for the power $n+1$ . A motivation for this question is in Binary eigenvalues matrices and continued fractions","Consider two binary matrices with obvious patterns: and The eigenvalues of the matrices are zeros and consecutive powers of equal to . I'd like to have a proof of the generalization of this fact for matrices of larger size with the same patterns. Note, the entries of in the left upper corner are zeros and binomial coefficients for the power . A motivation for this question is in Binary eigenvalues matrices and continued fractions","
C=
\begin{bmatrix}
1 &0 &0 &0 &0 &0 &0\\
1 &0 &0 &0 &0 &0 &0\\
0 &1 &0 &0 &0 &0 &0\\
0 &1 &0 &0 &0 &0 &0\\
0 &0 &1 &0 &0 &0 &0\\
0 &0 &1 &0 &0 &0 &0\\
0 &0 &0 &1 &0 &0 &0
\end{bmatrix}
 
T=
\begin{bmatrix}
1 &1 &0 &0 &0 &0 &0\\
0 &1 &1 &0 &0 &0 &0\\
0 &0 &1 &1 &0 &0 &0\\
0 &0 &0 &1 &1 &0 &0\\
0 &0 &0 &0 &1 &1 &0\\
0 &0 &0 &0 &0 &1 &1\\
0 &0 &0 &0 &0 &0 &1
\end{bmatrix}
 T^n C,n=0,1,2,3 2 0,1,2,4 T^n C n+1","['matrices', 'eigenvalues-eigenvectors', 'binomial-coefficients', 'binary']"
18,How many elements of a 3x3 Rotation matrix are redundant?,How many elements of a 3x3 Rotation matrix are redundant?,,"I read the following and got curious: A rotation matrix is an array of nine numbers. These are subject to   the six norm and orthogonality constraints, so only three degrees of   freedom are left: if three of the numbers are given, the other six can   be computed from these equations. I know that the matrix only has three degrees of freedom, but the last statement (emphasis mine) is obviously not literally true. Given these seven elements: $$  \left[ \begin{array}{rrrr}  . & 0 & 0 \\ 0 & . & 0 \\ 0 & 0 & 1 \end{array}\right] $$ There are at least two possible solutions (1, 1 or -1, -1) which gives me valid but different rotation matrices (a rotation of 180° around the Z axis). So, if I can't choose which elements are revealed to me, I apparently need to know eight elements to be certain about the last. Is there any way to rephrase the quote above which makes it true (are diagonal matrices an exception?). I know for instance, that if I have the following six elements:  $$  \left[ \begin{array}{rrrr}  a & b & c \\ d & e & f \\ . & . & . \end{array}\right] $$ I can deduce the last row by using the cross product of the first two (and choose a sign based on handedness). Can I get away with less than these six elements?","I read the following and got curious: A rotation matrix is an array of nine numbers. These are subject to   the six norm and orthogonality constraints, so only three degrees of   freedom are left: if three of the numbers are given, the other six can   be computed from these equations. I know that the matrix only has three degrees of freedom, but the last statement (emphasis mine) is obviously not literally true. Given these seven elements: $$  \left[ \begin{array}{rrrr}  . & 0 & 0 \\ 0 & . & 0 \\ 0 & 0 & 1 \end{array}\right] $$ There are at least two possible solutions (1, 1 or -1, -1) which gives me valid but different rotation matrices (a rotation of 180° around the Z axis). So, if I can't choose which elements are revealed to me, I apparently need to know eight elements to be certain about the last. Is there any way to rephrase the quote above which makes it true (are diagonal matrices an exception?). I know for instance, that if I have the following six elements:  $$  \left[ \begin{array}{rrrr}  a & b & c \\ d & e & f \\ . & . & . \end{array}\right] $$ I can deduce the last row by using the cross product of the first two (and choose a sign based on handedness). Can I get away with less than these six elements?",,"['matrices', 'rotations']"
19,Derivative of $A^\top A$,Derivative of,A^\top A,"Let $f(A):= A^\top A$ where $A$ is an $m \times n$ matrix.  We want to find the derivative of $f$ with respect to $A$. By derivative we mean to find the Jacobian of all partial derivatives of $f(A)$ with respect to $A$. Here is how I proceed. The Derivative of $f$ is the linear map $D f(A): X \to A^\top X + X^\top A$. Let $K$ be the commutation matrix such that $K\operatorname{vec}(X^\top A) = \operatorname{vec}(A^\top X)$. Then, \begin{align}   \operatorname{vec}(A^\top X + X^\top A)   & = \operatorname{vec}(A^\top X) + \operatorname{vec}(X^\top A) \\   & = (I_n\otimes A^\top) \operatorname{vec}(X) +     \operatorname{vec}(X^\top A) \\   & = I_n (\otimes A^\top) \operatorname{vec}(X) +     K_{n,n} \operatorname{vec}(A^\top X) \\   & = (I_n \otimes A^\top) \operatorname{vec}(X) +     K_{n, n} (I_n \otimes A^\top) \operatorname{vec}(X) \end{align} It now follows that \begin{align}   \frac{\partial f}{\partial A} & =  (I_n \otimes A^\top)                                   + K_{n, n} (I_n \otimes A^\top) \end{align} In here I am using the fact that $\operatorname{vec}(AXB) = (B^\top \otimes A)\operatorname{vec}(X)$ where $\operatorname{vec}$ is the vectorization operator. I was inspired by this answer and the corresponding equation under the section Differentials of Quadratic Products on this webpage My Questions: Is this approach correct?. If not how do I go about finding the desired derivative? Where can I find references regarding this type of manipulation?. (I don't mean this particular manipulation, but a reference for derivatives of matrices in general). I looked on Horn and Johnson Matrix Analysis , but a 'commutation matrix' is nowhere to be found. When I say reference, I mean a rigorous linear algebraic exposition.","Let $f(A):= A^\top A$ where $A$ is an $m \times n$ matrix.  We want to find the derivative of $f$ with respect to $A$. By derivative we mean to find the Jacobian of all partial derivatives of $f(A)$ with respect to $A$. Here is how I proceed. The Derivative of $f$ is the linear map $D f(A): X \to A^\top X + X^\top A$. Let $K$ be the commutation matrix such that $K\operatorname{vec}(X^\top A) = \operatorname{vec}(A^\top X)$. Then, \begin{align}   \operatorname{vec}(A^\top X + X^\top A)   & = \operatorname{vec}(A^\top X) + \operatorname{vec}(X^\top A) \\   & = (I_n\otimes A^\top) \operatorname{vec}(X) +     \operatorname{vec}(X^\top A) \\   & = I_n (\otimes A^\top) \operatorname{vec}(X) +     K_{n,n} \operatorname{vec}(A^\top X) \\   & = (I_n \otimes A^\top) \operatorname{vec}(X) +     K_{n, n} (I_n \otimes A^\top) \operatorname{vec}(X) \end{align} It now follows that \begin{align}   \frac{\partial f}{\partial A} & =  (I_n \otimes A^\top)                                   + K_{n, n} (I_n \otimes A^\top) \end{align} In here I am using the fact that $\operatorname{vec}(AXB) = (B^\top \otimes A)\operatorname{vec}(X)$ where $\operatorname{vec}$ is the vectorization operator. I was inspired by this answer and the corresponding equation under the section Differentials of Quadratic Products on this webpage My Questions: Is this approach correct?. If not how do I go about finding the desired derivative? Where can I find references regarding this type of manipulation?. (I don't mean this particular manipulation, but a reference for derivatives of matrices in general). I looked on Horn and Johnson Matrix Analysis , but a 'commutation matrix' is nowhere to be found. When I say reference, I mean a rigorous linear algebraic exposition.",,"['linear-algebra', 'matrices', 'reference-request', 'matrix-calculus', 'jacobian']"
20,Diagonal elements of a symmetric matrix and positive definiteness,Diagonal elements of a symmetric matrix and positive definiteness,,"Let $A = (a_{ij}) \in \mathbb{R}^{n \times n}$ be a symmetric matrix with $$a_{ii}>\sum\limits_{j\neq i} |a_{ij}|$$ for $i=1,\dots,n$. Show that $A$ is positive definite. I tried to work this out algebraically starting from the definition of a real matrix being positive definite $x^{T}Ax > 0$ for every $x \in \mathbb{K}^{n}$ but this got me stuck in the sense that I couldn't really utilize the given precondition. I would appreciate some hints.","Let $A = (a_{ij}) \in \mathbb{R}^{n \times n}$ be a symmetric matrix with $$a_{ii}>\sum\limits_{j\neq i} |a_{ij}|$$ for $i=1,\dots,n$. Show that $A$ is positive definite. I tried to work this out algebraically starting from the definition of a real matrix being positive definite $x^{T}Ax > 0$ for every $x \in \mathbb{K}^{n}$ but this got me stuck in the sense that I couldn't really utilize the given precondition. I would appreciate some hints.",,"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices']"
21,derivative of the determinant of the sum of two matrices,derivative of the determinant of the sum of two matrices,,"From Eq. 51 of the matrix cookbook we know that $\frac{\partial \log\det (AXB)}{\partial X} = (X^{-1})^\top$, where $\det(X)$ is the determinant of $X$. I was wondering what is the derivative of $\frac{\partial \log\det (AXB + C)}{\partial X}$. Is it still  $ (X^{-1})^\top$? Thanks!","From Eq. 51 of the matrix cookbook we know that $\frac{\partial \log\det (AXB)}{\partial X} = (X^{-1})^\top$, where $\det(X)$ is the determinant of $X$. I was wondering what is the derivative of $\frac{\partial \log\det (AXB + C)}{\partial X}$. Is it still  $ (X^{-1})^\top$? Thanks!",,"['matrices', 'derivatives', 'matrix-calculus', 'trace']"
22,Building a matrix from a wavelet.,Building a matrix from a wavelet.,,"I am currently working with wavelets and computer vision, but I want to understand them in a more mathematical way. I know that the object in my study is defined by $$\Phi_{(s, l)}(x) = \frac{1}{\sqrt[]{s}} \Phi\left( \frac{x-l}{s}\right)$$ where $\Phi$ is a mother wavelet. And I know that mother wavelets should satisfy that they have finite energy and the admissibility condition. I also know that we can compute, for a function $f$, its Continuous Wavelet Transform which goes by the formula $$ W(s, l) = \int_{-\infty}^\infty f(t) \Phi_{(s, l)}(t) dt $$ Now, I'm having trouble with: Finding the exact Discrete Wavelet Transform. I have read lots of papers and every single one of them have different approaches to this, I can't come up with the right formula The mathematical relationship between the Discrete Wavelet Transform and the Wavelet Matrix. I know that you can find lots of articles (example: http://www.whydomath.org/node/wavlets/generalwt.html , http://www.whydomath.org/node/wavlets/hwt.html ) talking about how to generate specific Wavelet Matrix. But they don't explain what has everything to do with the Wavelet formula, and that is not the mathematical explanation that I want. To be precise, I don't want to learn that, if I average data and build that matrix I get a Haar wavelet matrix; I want to be able to build the Haar matrix because I understant the Wavelet theory.","I am currently working with wavelets and computer vision, but I want to understand them in a more mathematical way. I know that the object in my study is defined by $$\Phi_{(s, l)}(x) = \frac{1}{\sqrt[]{s}} \Phi\left( \frac{x-l}{s}\right)$$ where $\Phi$ is a mother wavelet. And I know that mother wavelets should satisfy that they have finite energy and the admissibility condition. I also know that we can compute, for a function $f$, its Continuous Wavelet Transform which goes by the formula $$ W(s, l) = \int_{-\infty}^\infty f(t) \Phi_{(s, l)}(t) dt $$ Now, I'm having trouble with: Finding the exact Discrete Wavelet Transform. I have read lots of papers and every single one of them have different approaches to this, I can't come up with the right formula The mathematical relationship between the Discrete Wavelet Transform and the Wavelet Matrix. I know that you can find lots of articles (example: http://www.whydomath.org/node/wavlets/generalwt.html , http://www.whydomath.org/node/wavlets/hwt.html ) talking about how to generate specific Wavelet Matrix. But they don't explain what has everything to do with the Wavelet formula, and that is not the mathematical explanation that I want. To be precise, I don't want to learn that, if I average data and build that matrix I get a Haar wavelet matrix; I want to be able to build the Haar matrix because I understant the Wavelet theory.",,"['matrices', 'transformation', 'signal-processing', 'wavelets']"
23,How many square roots can a complex matrix with distinct non zero eigenvalues can take?,How many square roots can a complex matrix with distinct non zero eigenvalues can take?,,"I got the above problem in an exam, the problem stated to show there is at most $2^n$ such matrices. What I did was to show the matrix is similar to a matrix with a diagonal matrix with distinct complex numbers, and now if $A$ is such a matrix and $B$ is a square root. Then $B$ is diagonalizable so since $A$ and $B$ commute there is a matrix $P$ such that that $P^{-1}BP$ and $P^{-1}AP$ are both are diagonal. So my argument was that to consider the diagonal case (w.l.g) and show that each diagonal entry of $P^{-1}BP$ should be root of the corresponding diganoal entry of $P^{-1}AP$. So we can have atmost 2 options for each and overall we get $2^n$. Now the problem I just realized is that I just saw that for any invertible  diagonalizable matrix $A^2$, $A$ is also diagonalizable. So then we can apply the same logic as what I just did to any such invertible matrix. But I also know the identity has infinitely many roots. So where did I go wrong? Thank You edit: added non zero","I got the above problem in an exam, the problem stated to show there is at most $2^n$ such matrices. What I did was to show the matrix is similar to a matrix with a diagonal matrix with distinct complex numbers, and now if $A$ is such a matrix and $B$ is a square root. Then $B$ is diagonalizable so since $A$ and $B$ commute there is a matrix $P$ such that that $P^{-1}BP$ and $P^{-1}AP$ are both are diagonal. So my argument was that to consider the diagonal case (w.l.g) and show that each diagonal entry of $P^{-1}BP$ should be root of the corresponding diganoal entry of $P^{-1}AP$. So we can have atmost 2 options for each and overall we get $2^n$. Now the problem I just realized is that I just saw that for any invertible  diagonalizable matrix $A^2$, $A$ is also diagonalizable. So then we can apply the same logic as what I just did to any such invertible matrix. But I also know the identity has infinitely many roots. So where did I go wrong? Thank You edit: added non zero",,"['linear-algebra', 'matrices', 'diagonalization']"
24,Product of a primitive matrix and its transpose.,Product of a primitive matrix and its transpose.,,"Is it true that if $A$ is a nonnegative primitive matrix, then $AA^T$ is also primitive? Obviously $A^T$ is primitive but in general product of primitive matrices is not primitive.  Any hint?","Is it true that if $A$ is a nonnegative primitive matrix, then $AA^T$ is also primitive? Obviously $A^T$ is primitive but in general product of primitive matrices is not primitive.  Any hint?",,"['matrices', 'nonnegative-matrices']"
25,Derivative of squared Frobenius norm of linear combination of rank-$1$ matrices with respect to their weights,Derivative of squared Frobenius norm of linear combination of rank- matrices with respect to their weights,1,"Let $${\bf A} := \sum_{i=1}^K w_i {\bf x}_i {\bf x}_i^\top$$ where $\{w_i\}_{i=1}^K$ are scalars, and ${\bf x}_i \in \mathbb{R}^d$. Let ${\bf w} := [w_1 \cdots w_K]^\top \in \mathbb{R}^K$. What is the derivative of the squared Frobenius norm $\|{\bf A}\|_\mathsf{F}^2$ with respect to ${\bf w}$?","Let $${\bf A} := \sum_{i=1}^K w_i {\bf x}_i {\bf x}_i^\top$$ where $\{w_i\}_{i=1}^K$ are scalars, and ${\bf x}_i \in \mathbb{R}^d$. Let ${\bf w} := [w_1 \cdots w_K]^\top \in \mathbb{R}^K$. What is the derivative of the squared Frobenius norm $\|{\bf A}\|_\mathsf{F}^2$ with respect to ${\bf w}$?",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
26,Can the limit of a sequence of matrices of fixed rank have higher rank?,Can the limit of a sequence of matrices of fixed rank have higher rank?,,"Consider the $n\times n$ matrices given by $A_n=1/n*I_{n\times n}$. For every finite $n$ each $A_n$ has rank $n$ but the limit is the zero matrix, and has rank zero. So the limit of a sequence of rank $n$  matrices need not be of rank $n$.  Now I am considering the reverse question. Consider a sequence $A_k \in \mathbb{R}^{m \times n}$ (or $\mathbb{C}^{m \times n}$). Is it possible to construct a sequence such that each $A_k$ has some rank $d \le \min(m,n)$ but: $\operatorname{rank} \left( \lim \limits _{k \to \infty} A_n \right) >d$? If not how would one go about proving that matrices cannot jump rank?","Consider the $n\times n$ matrices given by $A_n=1/n*I_{n\times n}$. For every finite $n$ each $A_n$ has rank $n$ but the limit is the zero matrix, and has rank zero. So the limit of a sequence of rank $n$  matrices need not be of rank $n$.  Now I am considering the reverse question. Consider a sequence $A_k \in \mathbb{R}^{m \times n}$ (or $\mathbb{C}^{m \times n}$). Is it possible to construct a sequence such that each $A_k$ has some rank $d \le \min(m,n)$ but: $\operatorname{rank} \left( \lim \limits _{k \to \infty} A_n \right) >d$? If not how would one go about proving that matrices cannot jump rank?",,"['real-analysis', 'matrices', 'matrix-rank']"
27,Smallest and Largest Eigenvalues of a PSD Matrix,Smallest and Largest Eigenvalues of a PSD Matrix,,"For $M\succeq 0$ and $M\in \mathbb{R}^{n\times n}$, we know the following facts: $$\lambda_{\min} = \min_{\|x\|=1} x^TMx$$ $$\lambda_{\max} = \max_{\|x\|=1} x^TMx$$ My questions are the following: 1. Do the above facts hold for any square matrices $M\in \mathbb{R}^{n\times n}$, or does this matrix have to be at least symmetric? 2. Are  the above facts equivalent to ""$\|x\|=1$, $x\in \text{span}(M)$""? i.e., we restrict the condition $x\in \mathbb{R}^n$ to $x\in \text{span}(M)$ where $\text{span}(M)$ represents the subspace expanded by the column vectors of $M$.","For $M\succeq 0$ and $M\in \mathbb{R}^{n\times n}$, we know the following facts: $$\lambda_{\min} = \min_{\|x\|=1} x^TMx$$ $$\lambda_{\max} = \max_{\|x\|=1} x^TMx$$ My questions are the following: 1. Do the above facts hold for any square matrices $M\in \mathbb{R}^{n\times n}$, or does this matrix have to be at least symmetric? 2. Are  the above facts equivalent to ""$\|x\|=1$, $x\in \text{span}(M)$""? i.e., we restrict the condition $x\in \mathbb{R}^n$ to $x\in \text{span}(M)$ where $\text{span}(M)$ represents the subspace expanded by the column vectors of $M$.",,"['matrices', 'eigenvalues-eigenvectors']"
28,Show that if $\pm\lambda $ is an eigenvalue of $A$ then $\lambda^2$ is an eigenvalue of $A^TA$ and vice versa.,Show that if  is an eigenvalue of  then  is an eigenvalue of  and vice versa.,\pm\lambda  A \lambda^2 A^TA,"Show that if $\pm\lambda $ is an eigenvalue of $A$ then $\lambda^2$ is an eigenvalue of $A^TA$ and vice versa. If $\pm\lambda $  is an eigenvalue of $A$ then $Av=\pm\lambda v\implies v^TA^T=\pm \lambda v^T\implies v^TA^TAv=a^2v^Tv  $ How to show that $a^2$ is an eigenvalue of $A^TA$ from above? Conversely $A^TAv=a^2v\implies v^TA^TAv=a^2 v^Tv\implies \langle Av,Av \rangle =a^2\langle v,v\rangle\implies ||Av||=a^2||v||\implies ||Av||=\pm a||v||$ How to show that $Av=\pm av$ from here? Please help.","Show that if $\pm\lambda $ is an eigenvalue of $A$ then $\lambda^2$ is an eigenvalue of $A^TA$ and vice versa. If $\pm\lambda $  is an eigenvalue of $A$ then $Av=\pm\lambda v\implies v^TA^T=\pm \lambda v^T\implies v^TA^TAv=a^2v^Tv  $ How to show that $a^2$ is an eigenvalue of $A^TA$ from above? Conversely $A^TAv=a^2v\implies v^TA^TAv=a^2 v^Tv\implies \langle Av,Av \rangle =a^2\langle v,v\rangle\implies ||Av||=a^2||v||\implies ||Av||=\pm a||v||$ How to show that $Av=\pm av$ from here? Please help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
29,All roots $\lambda$ of $\det(A-\lambda B)=0$ are $\ge1$ when $B$ is p.d and $A-B$ is n.n.d.,All roots  of  are  when  is p.d and  is n.n.d.,\lambda \det(A-\lambda B)=0 \ge1 B A-B,"I am trying to prove the following statement: Let $A,B\in M(n,\mathbb{R})$ . If $B$ is positive definite and $(A-B)$ is non-negative definite, then $\det(A-\lambda B)=0$ has all its roots $\lambda\geqslant1$ and conversely, if all roots $\lambda\geqslant 1$ , then $(A-B)$ is non-negative definite. If $(A-B)$ is n.n.d and $B$ is p.d, then I have $x^\top(A-B)x\geqslant0$ for all $x\in\mathbb{R}^n$ . This implies $x^\top Ax\geqslant x^\top Bx>0$ for all $x\ne0$ , so that $A$ is p.d. Moreover, as $B$ is p.d, $B$ is nonsingular. So, $\det(A-\lambda B)=0\implies\det((AB^{-1}-\lambda I)B)=0$ $\qquad\qquad\qquad\qquad\quad\implies\det(AB^{-1}-\lambda I)=0$ , as $\det(B)\ne0$ Thus $\lambda$ is an eigenvalue of the matrix $AB^{-1}$ . Now for the eigenvector $x\ne0$ corresponding to $\lambda$ we have, $(AB^{-1})x=\lambda x\implies(AB^{-1}-I)x=(\lambda-1)x$ . If I can show that $AB^{-1}-I$ is n.n.d given that both $A$ and $B$ are p.d, then that would possibly imply $\lambda-1\geqslant0$ and I am done. But I am not sure if this is true or not. In a different approach using the fact that a p.d matrix can be expressed as $D^\top D$ for some nonsingular matrix $D$ , I was able to show that $AB^{-1}-I=PQ$ for some n.n.d matrix $P$ and p.d matrix $Q$ . Does that help me conclude that $AB^{-1}-I$ is indeed n.n.d? Any simpler or alternate approach is welcome.","I am trying to prove the following statement: Let . If is positive definite and is non-negative definite, then has all its roots and conversely, if all roots , then is non-negative definite. If is n.n.d and is p.d, then I have for all . This implies for all , so that is p.d. Moreover, as is p.d, is nonsingular. So, , as Thus is an eigenvalue of the matrix . Now for the eigenvector corresponding to we have, . If I can show that is n.n.d given that both and are p.d, then that would possibly imply and I am done. But I am not sure if this is true or not. In a different approach using the fact that a p.d matrix can be expressed as for some nonsingular matrix , I was able to show that for some n.n.d matrix and p.d matrix . Does that help me conclude that is indeed n.n.d? Any simpler or alternate approach is welcome.","A,B\in M(n,\mathbb{R}) B (A-B) \det(A-\lambda B)=0 \lambda\geqslant1 \lambda\geqslant 1 (A-B) (A-B) B x^\top(A-B)x\geqslant0 x\in\mathbb{R}^n x^\top Ax\geqslant x^\top Bx>0 x\ne0 A B B \det(A-\lambda B)=0\implies\det((AB^{-1}-\lambda I)B)=0 \qquad\qquad\qquad\qquad\quad\implies\det(AB^{-1}-\lambda I)=0 \det(B)\ne0 \lambda AB^{-1} x\ne0 \lambda (AB^{-1})x=\lambda x\implies(AB^{-1}-I)x=(\lambda-1)x AB^{-1}-I A B \lambda-1\geqslant0 D^\top D D AB^{-1}-I=PQ P Q AB^{-1}-I","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms']"
30,Show that $(A*B*A^{-1})^2 = B^2$,Show that,(A*B*A^{-1})^2 = B^2,Show that $(A*B*A^{-1})^2 = B^2$ I tried: $(A*B*A^{-1})*(A*B*A^{-1})$ $\hspace{5mm}$ (1) $= A*B*A^{-1}*A*B*A{-1}$  $\hspace{5mm}$ (2) $= A*B*B*A^{-1}$ $\hspace{5mm}$ (3) $ = A*B^2*A^{-1}$ $\hspace{5mm}$ (4) In step $(4)$ I do not know how to clear $B ^ 2$,Show that $(A*B*A^{-1})^2 = B^2$ I tried: $(A*B*A^{-1})*(A*B*A^{-1})$ $\hspace{5mm}$ (1) $= A*B*A^{-1}*A*B*A{-1}$  $\hspace{5mm}$ (2) $= A*B*B*A^{-1}$ $\hspace{5mm}$ (3) $ = A*B^2*A^{-1}$ $\hspace{5mm}$ (4) In step $(4)$ I do not know how to clear $B ^ 2$,,"['linear-algebra', 'matrices']"
31,Understanding Karlin's Proof of Perron's Theorem.,Understanding Karlin's Proof of Perron's Theorem.,,"Reading a proof of Perron's Theorem I got stuck at some place: We have proved that spectral radius $\rho$ is a simple eigenvalue. Now we have to check that there are no eigenvalues other than spectral radius of modulus $\rho$. Consider $A − \epsilon I > 0$ for small $\epsilon > 0$. Its largest positive eigenvalue is $\rho − \epsilon$, which we have proved to be its spectral radius. Translating this smaller circle back to the right by $\epsilon$ we see that all remaining eigenvalues of $A$ lie within the open disk $|λ| < ρ$. I can't understand the above line(bold), Help Needed! Ref: The Many Proofs and Applications of Perron's Theorem pg496.","Reading a proof of Perron's Theorem I got stuck at some place: We have proved that spectral radius $\rho$ is a simple eigenvalue. Now we have to check that there are no eigenvalues other than spectral radius of modulus $\rho$. Consider $A − \epsilon I > 0$ for small $\epsilon > 0$. Its largest positive eigenvalue is $\rho − \epsilon$, which we have proved to be its spectral radius. Translating this smaller circle back to the right by $\epsilon$ we see that all remaining eigenvalues of $A$ lie within the open disk $|λ| < ρ$. I can't understand the above line(bold), Help Needed! Ref: The Many Proofs and Applications of Perron's Theorem pg496.",,"['linear-algebra', 'matrices']"
32,What is the average value of the determinant of matrices in this set? [duplicate],What is the average value of the determinant of matrices in this set? [duplicate],,"This question already has an answer here : The average determinant of all integer matrices with coefficients $0,1,2$ [closed] (1 answer) Closed 6 years ago . State true or false. Let $n \geq 2$ be a natural number. Let $S$ be   the set of all $n \times n$ real matrices whose entries are only $0, 1$ or $2$. Then the average of determinants of matrices in $S$ is greater than or equal to $1$. This is how I think the answer should go, but I am not fully convinced. False. The average value is zero. There are only finitely many matrices in $S$, and we can exhibit a one-to-one correspondence between the matrices with non-zero determinants as follows: given $A \in S$ with positive determinant, interchanging the first two columns of $A$ gives us a matrix in $S$ with negative determinant. As this function has an inverse (itself) it is bijective. Thus, the average value of the determinant of matrices in $S$ is zero.","This question already has an answer here : The average determinant of all integer matrices with coefficients $0,1,2$ [closed] (1 answer) Closed 6 years ago . State true or false. Let $n \geq 2$ be a natural number. Let $S$ be   the set of all $n \times n$ real matrices whose entries are only $0, 1$ or $2$. Then the average of determinants of matrices in $S$ is greater than or equal to $1$. This is how I think the answer should go, but I am not fully convinced. False. The average value is zero. There are only finitely many matrices in $S$, and we can exhibit a one-to-one correspondence between the matrices with non-zero determinants as follows: given $A \in S$ with positive determinant, interchanging the first two columns of $A$ gives us a matrix in $S$ with negative determinant. As this function has an inverse (itself) it is bijective. Thus, the average value of the determinant of matrices in $S$ is zero.",,"['linear-algebra', 'matrices', 'proof-verification', 'determinant']"
33,Inequality regarding norm of a positive definite matrix,Inequality regarding norm of a positive definite matrix,,"Prove that $$\sqrt{\frac{a^TA^2a}{a^TAa}}\le \sqrt{\|A\|}$$ where $A$ is a $n \times n$ symmetric positive definite matrix and $a \in \mathbb{R}^n \setminus \{0_n\}$ . The norm of a vector here is the Euclidean norm and norm of a matrix $A$ is $$\|A\|=\max\{\|Ax\|:\|x\|=1\}$$ and if $A$ is symmetric then $$\|A\|=\max\{|x^TAx|:\|x\|=1\}$$ and also equal to the largest eigenvalue of $A$ . We have from this min-max theorem that $$\lambda_{\min}\|x\|^2\leq x^THx \le \lambda_{\max}\|x\|^2$$ where $\lambda_{\min},\lambda_{\max}$ are the smallest and largest eigenvalues of the Hermitian matrix $H$ . So we have $$\sqrt{\frac{a^TA^2a}{a^TAa}}\le \sqrt{\frac{\lambda^2_{\max}}{\lambda_{\min}}}$$ In order to prove the inequality in the question, we need to show that $\frac{\lambda^2_{max}}{\lambda_{\min}}\le \lambda_{\max}$ which implies $\frac{\lambda_{\max}}{\lambda_{\min}}\le 1$ . We have $\|A\|=\lambda_{\max}$ and $\|A^{-1}\|=\frac{1}{\lambda_{\min}}$ but on the contrary it is a fact that $\|A\|\|A^{-1}\|\ge 1$ . Where have I gone wrong and how do we prove this inequality?","Prove that where is a symmetric positive definite matrix and . The norm of a vector here is the Euclidean norm and norm of a matrix is and if is symmetric then and also equal to the largest eigenvalue of . We have from this min-max theorem that where are the smallest and largest eigenvalues of the Hermitian matrix . So we have In order to prove the inequality in the question, we need to show that which implies . We have and but on the contrary it is a fact that . Where have I gone wrong and how do we prove this inequality?","\sqrt{\frac{a^TA^2a}{a^TAa}}\le \sqrt{\|A\|} A n \times n a \in \mathbb{R}^n \setminus \{0_n\} A \|A\|=\max\{\|Ax\|:\|x\|=1\} A \|A\|=\max\{|x^TAx|:\|x\|=1\} A \lambda_{\min}\|x\|^2\leq x^THx \le \lambda_{\max}\|x\|^2 \lambda_{\min},\lambda_{\max} H \sqrt{\frac{a^TA^2a}{a^TAa}}\le \sqrt{\frac{\lambda^2_{\max}}{\lambda_{\min}}} \frac{\lambda^2_{max}}{\lambda_{\min}}\le \lambda_{\max} \frac{\lambda_{\max}}{\lambda_{\min}}\le 1 \|A\|=\lambda_{\max} \|A^{-1}\|=\frac{1}{\lambda_{\min}} \|A\|\|A^{-1}\|\ge 1","['linear-algebra', 'matrices', 'optimization', 'positive-definite', 'matrix-norms']"
34,On determinants and common divisors,On determinants and common divisors,,"Let $n\in\mathbb N$ and let $a_1,\ldots,a_n$ be natural numbers smaller than $10^n$ . Write each $a_k$ in base $10$ and add $0$ 's to the left of each decimal expansion, if needed, so that each $a_k$ is written with $n$ digits. Consider the matrix such that the entries of the $k$ th row are the digits of $a_k$ (written in the same order). Let $d\in\mathbb N$ be such that $d$ divides each $a_k$ . Prove that $d\mid\det A$ . For instance, suppose that $n=4$ and that your numbers are $3876$ , $2784$ , $684$ , and $8388$ , each of which is a multiple of $12$ . Then $$A=\begin{bmatrix}3 & 8 & 7 & 6 \\  2 & 7 & 8 & 4 \\ 0 & 6 & 8 & 4 \\  8 & 3 & 8 & 8\end{bmatrix}$$ and $\det A=-360$ , which is, in fact, a multiple of $12$ . I learned about this problem yesterday. I found it quite cute and I decided to share it with all of you. Note: I know how to prove it.","Let and let be natural numbers smaller than . Write each in base and add 's to the left of each decimal expansion, if needed, so that each is written with digits. Consider the matrix such that the entries of the th row are the digits of (written in the same order). Let be such that divides each . Prove that . For instance, suppose that and that your numbers are , , , and , each of which is a multiple of . Then and , which is, in fact, a multiple of . I learned about this problem yesterday. I found it quite cute and I decided to share it with all of you. Note: I know how to prove it.","n\in\mathbb N a_1,\ldots,a_n 10^n a_k 10 0 a_k n k a_k d\in\mathbb N d a_k d\mid\det A n=4 3876 2784 684 8388 12 A=\begin{bmatrix}3 & 8 & 7 & 6 \\  2 & 7 & 8 & 4 \\ 0 & 6 & 8 & 4 \\
 8 & 3 & 8 & 8\end{bmatrix} \det A=-360 12","['matrices', 'divisibility', 'determinant', 'puzzle', 'decimal-expansion']"
35,Is infinite upper triangular matrix with nonzero entries on the diagonal invertible?,Is infinite upper triangular matrix with nonzero entries on the diagonal invertible?,,"Let's say we have an infinite square matrix where rows and columns are indexed by $\mathbb{N}$. We also know that every entry on the diagonal is nonzero and the matrix is upper triangular. Is it enough to conclude that our matrix is invertible? It is true for finite matrices, but does it also hold for infinite matrices? If not, what if we consider the case where only finite number of entries in each row and column are nonzero? What I want to do is: Let's say we have  $$A(i, j) = \sum_{k \in \mathbb{N}} B(i, k) \cdot C(k, j)$$  for $i, j \in \mathbb{N}$ and we want to prove that there exists $D$ such that $$C(i, j) = \sum_{k \in \mathbb{N}} D(i, k) \cdot A(k, j).$$  Then we can rewrite it as a matrix product: $$A = B \cdot C.$$ Can we use the fact that $B$ is upper triangular and has nonzero diagonal to show that $$C = B^{-1} \cdot A$$ and $$C(i, j) = \sum_{k \in \mathbb{N}} B^{-1}(i, k) \cdot A(k, j)?$$","Let's say we have an infinite square matrix where rows and columns are indexed by $\mathbb{N}$. We also know that every entry on the diagonal is nonzero and the matrix is upper triangular. Is it enough to conclude that our matrix is invertible? It is true for finite matrices, but does it also hold for infinite matrices? If not, what if we consider the case where only finite number of entries in each row and column are nonzero? What I want to do is: Let's say we have  $$A(i, j) = \sum_{k \in \mathbb{N}} B(i, k) \cdot C(k, j)$$  for $i, j \in \mathbb{N}$ and we want to prove that there exists $D$ such that $$C(i, j) = \sum_{k \in \mathbb{N}} D(i, k) \cdot A(k, j).$$  Then we can rewrite it as a matrix product: $$A = B \cdot C.$$ Can we use the fact that $B$ is upper triangular and has nonzero diagonal to show that $$C = B^{-1} \cdot A$$ and $$C(i, j) = \sum_{k \in \mathbb{N}} B^{-1}(i, k) \cdot A(k, j)?$$",,"['matrices', 'inverse', 'infinite-matrices']"
36,Determinant of a matrix with integer power entries,Determinant of a matrix with integer power entries,,"Define $$D(n,k)=\begin{vmatrix} 1^k & 2^k&\cdots& n^k\\2^k&3^k&\cdots &(n+1)^k\\ \vdots&\vdots&\ddots&\vdots\\ n^k&(n+1)^k&\cdots&(2n-1)^k  \end{vmatrix}.$$ I'm asking: Calculate $D(1,1), D(2,1), D(3,1), D(4,1)$. Show that $D(n,2)=0$ for $n>3$. Show that $D(n,k)=0$ for $n>k+1$ First one is not difficult. I thought that maybe this calculation could help me to attack the second and third question. Is not the case. For $n=4$ one can verify that this determinant is truly zero by making zeros on the first row an then calculating a $3\times 3$ determinant. However, it doesn't help for found a generalized method for demonstrate the assertion. It's pretty clear (I think) that induction is the ideal method but I'm not sure on how to proceed. I try to use the fact that the matrix is symmetric, but again without success. Thanks in advance!","Define $$D(n,k)=\begin{vmatrix} 1^k & 2^k&\cdots& n^k\\2^k&3^k&\cdots &(n+1)^k\\ \vdots&\vdots&\ddots&\vdots\\ n^k&(n+1)^k&\cdots&(2n-1)^k  \end{vmatrix}.$$ I'm asking: Calculate $D(1,1), D(2,1), D(3,1), D(4,1)$. Show that $D(n,2)=0$ for $n>3$. Show that $D(n,k)=0$ for $n>k+1$ First one is not difficult. I thought that maybe this calculation could help me to attack the second and third question. Is not the case. For $n=4$ one can verify that this determinant is truly zero by making zeros on the first row an then calculating a $3\times 3$ determinant. However, it doesn't help for found a generalized method for demonstrate the assertion. It's pretty clear (I think) that induction is the ideal method but I'm not sure on how to proceed. I try to use the fact that the matrix is symmetric, but again without success. Thanks in advance!",,"['linear-algebra', 'matrices', 'determinant']"
37,Find all matrices which commute with a given matrix,Find all matrices which commute with a given matrix,,"I wonder how to find all matrices $B$ which satisfy $AB=BA$, where $A=\left( {\begin{array}{*{20}{c}}   1&{}&{}&{}&{} \\    1&1&{}&{}&{} \\    1&1&1&{}&{} \\     \vdots & \vdots & \vdots & \ddots &{} \\    1&1&1&1&1  \end{array}} \right)$ (the coefficients above the diagonal of the matrix $A$ are zero). I tried Jordan form of $A$, but still couldn't see what to do next... Any help will be appreciated :)","I wonder how to find all matrices $B$ which satisfy $AB=BA$, where $A=\left( {\begin{array}{*{20}{c}}   1&{}&{}&{}&{} \\    1&1&{}&{}&{} \\    1&1&1&{}&{} \\     \vdots & \vdots & \vdots & \ddots &{} \\    1&1&1&1&1  \end{array}} \right)$ (the coefficients above the diagonal of the matrix $A$ are zero). I tried Jordan form of $A$, but still couldn't see what to do next... Any help will be appreciated :)",,"['linear-algebra', 'matrices', 'matrix-equations']"
38,Abstract Proof that Exponential Map is Surjective onto $\mathrm{GL}_n(\mathbb{C})$,Abstract Proof that Exponential Map is Surjective onto,\mathrm{GL}_n(\mathbb{C}),"It is well-known that the exponential map associated to any compact connected Lie group is surjective (the proof is a simple application of the Lefschetz fixed point theorem). As it happens, the exponential map associated to $\mathrm{GL}_n(\mathbb{C})$ is also surjective, although $\mathrm{GL}_n(\mathbb{C})$ fails to be compact. The only way I know how to prove this latter claim is by expressing each element of $\mathrm{GL}_n(\mathbb{C})$ in its Jordan canonical form and then showing that any Jordan block is the exponential of some matrix. My question is this: is there a wider class of Lie groups (more general than just compact and connected, and perhaps including examples like $\mathrm{GL}_n(\mathbb{C})$) for which we can say that the exponential map is surjective?","It is well-known that the exponential map associated to any compact connected Lie group is surjective (the proof is a simple application of the Lefschetz fixed point theorem). As it happens, the exponential map associated to $\mathrm{GL}_n(\mathbb{C})$ is also surjective, although $\mathrm{GL}_n(\mathbb{C})$ fails to be compact. The only way I know how to prove this latter claim is by expressing each element of $\mathrm{GL}_n(\mathbb{C})$ in its Jordan canonical form and then showing that any Jordan block is the exponential of some matrix. My question is this: is there a wider class of Lie groups (more general than just compact and connected, and perhaps including examples like $\mathrm{GL}_n(\mathbb{C})$) for which we can say that the exponential map is surjective?",,"['matrices', 'group-theory', 'lie-groups', 'lie-algebras', 'matrix-exponential']"
39,Write formula for matrix in terms of Fibonacci numbers,Write formula for matrix in terms of Fibonacci numbers,,"How could I express this matrix in terms of Fibonacci numbers?  It seems like I'd have to use induction once I have a candidate for a formula but I'm unsure of where to start with expressing the matrix in terms of Fibonacci numbers. Thanks in advance! Let $T:\mathbb{R^2}\rightarrow \mathbb{R^2}$ be a linear map such that $$T\left( \begin{array}{c} x\\ y\\ \end{array} \right)=\left( \begin{array}{c} y\\ x+ y\\ \end{array} \right)$$ using the basis $\beta=\{e_1,e_2\}$ $$e_1=\left( \begin{array}{c} 1\\ 0\\ \end{array} \right),\quad e_2=\left( \begin{array}{c} 0\\ 1\\ \end{array} \right)$$ Write a formula for the matrix$$ [T^n]_\beta, \forall n\in\mathbb{N}$$ in terms of Fibonacci numbers.","How could I express this matrix in terms of Fibonacci numbers?  It seems like I'd have to use induction once I have a candidate for a formula but I'm unsure of where to start with expressing the matrix in terms of Fibonacci numbers. Thanks in advance! Let $T:\mathbb{R^2}\rightarrow \mathbb{R^2}$ be a linear map such that $$T\left( \begin{array}{c} x\\ y\\ \end{array} \right)=\left( \begin{array}{c} y\\ x+ y\\ \end{array} \right)$$ using the basis $\beta=\{e_1,e_2\}$ $$e_1=\left( \begin{array}{c} 1\\ 0\\ \end{array} \right),\quad e_2=\left( \begin{array}{c} 0\\ 1\\ \end{array} \right)$$ Write a formula for the matrix$$ [T^n]_\beta, \forall n\in\mathbb{N}$$ in terms of Fibonacci numbers.",,"['linear-algebra', 'matrices', 'number-theory', 'matrix-equations']"
40,Is $det (A-A^T) = 0$ for a $n \times n$ matrix? ($n$ odd),Is  for a  matrix? ( odd),det (A-A^T) = 0 n \times n n,"I have the following problem: We have a $n \times n$ matrix where $n$ is odd. Is $det (A-A^T) = 0$? I don't know if this is true or not, I met this while working at the following problem: ""Let $A$ be a $n \times n$ matrix where $AA^T=I_n$. Prove that $tr A \leq n$ and for $n$ odd, $det (A^2-I_n)=0$."" (Romanian Olympiad, 2007). The inequality was easy, using the known inequality $tr(AB) \cdot tr(A^TB^T) \leq tr(AA^T) \cdot tr(BB^T)$. At the second point I thought about the following relations: $(A-I_n)(A^T+I_n)=A-A^T$ and $(A+I_n)(A^T-I_n)=A^T-A$. So we have that $det (A^2-I_n) \cdot det((A^T)^2-I_n)=-(det(A-A^T))^2$ and if what I say is true, than the problem is a step closer to be solved. For $n=1$ and $n=3$ it is true.","I have the following problem: We have a $n \times n$ matrix where $n$ is odd. Is $det (A-A^T) = 0$? I don't know if this is true or not, I met this while working at the following problem: ""Let $A$ be a $n \times n$ matrix where $AA^T=I_n$. Prove that $tr A \leq n$ and for $n$ odd, $det (A^2-I_n)=0$."" (Romanian Olympiad, 2007). The inequality was easy, using the known inequality $tr(AB) \cdot tr(A^TB^T) \leq tr(AA^T) \cdot tr(BB^T)$. At the second point I thought about the following relations: $(A-I_n)(A^T+I_n)=A-A^T$ and $(A+I_n)(A^T-I_n)=A^T-A$. So we have that $det (A^2-I_n) \cdot det((A^T)^2-I_n)=-(det(A-A^T))^2$ and if what I say is true, than the problem is a step closer to be solved. For $n=1$ and $n=3$ it is true.",,"['linear-algebra', 'matrices', 'determinant']"
41,Conjecture: A linear transformation is onto if and only if its adjoint is 1-1,Conjecture: A linear transformation is onto if and only if its adjoint is 1-1,,"Suppose $V$ and $W$ are inner product spaces and $T:V\rightarrow W$ is a linear mapping between them. Is $T$ onto if and only if $T^*$ is 1-1? This is either a basic fact or a total fantasy. I can find support for the ""if"" but not the ""only if"", and I am curious about showing that $T$ cannot be onto if $T^*$ has nontrivial kernel. A proof for the matrix case is good enough for my purposes.","Suppose $V$ and $W$ are inner product spaces and $T:V\rightarrow W$ is a linear mapping between them. Is $T$ onto if and only if $T^*$ is 1-1? This is either a basic fact or a total fantasy. I can find support for the ""if"" but not the ""only if"", and I am curious about showing that $T$ cannot be onto if $T^*$ has nontrivial kernel. A proof for the matrix case is good enough for my purposes.",,"['linear-algebra', 'matrices', 'operator-theory']"
42,"Let $A, B$ a $3\times\,3$ matrices that sets $\ A^3+5AB=I, A^3-5BA=2I$, find $\det(A) $","Let  a  matrices that sets , find","A, B 3\times\,3 \ A^3+5AB=I, A^3-5BA=2I \det(A) ","Need help with this question: Let $A, B$ a $3\times\,3$ matrices that sets $\ A^3+5AB=I, A^3-5BA=2I$, find the determinate of $A$. So I know since $\ A(A^2+5B) = I$, $A$ is invertible and $\ A^{-1} = A^2+5B $, but what next? Thanks in advance","Need help with this question: Let $A, B$ a $3\times\,3$ matrices that sets $\ A^3+5AB=I, A^3-5BA=2I$, find the determinate of $A$. So I know since $\ A(A^2+5B) = I$, $A$ is invertible and $\ A^{-1} = A^2+5B $, but what next? Thanks in advance",,"['linear-algebra', 'matrices', 'determinant']"
43,$\exp(A)$ is orthogonal iff $A$ is skew symmetric,is orthogonal iff  is skew symmetric,\exp(A) A,"Let $A \in M_n(\mathbb R)$ and $\exp(A)=\sum_{i=0}^{\infty}\frac{A^i}{i!}$. How to prove $\exp(A)$ is an orthogonal matrix iff $A^t=-A$? I can get that $\exp(A)$ is an orthogonal matrix iff $\exp(A^t)=\exp(-A)$, but how to get $A^t=-A$ from it? What's more, is $\exp(A)$ injective or not?","Let $A \in M_n(\mathbb R)$ and $\exp(A)=\sum_{i=0}^{\infty}\frac{A^i}{i!}$. How to prove $\exp(A)$ is an orthogonal matrix iff $A^t=-A$? I can get that $\exp(A)$ is an orthogonal matrix iff $\exp(A^t)=\exp(-A)$, but how to get $A^t=-A$ from it? What's more, is $\exp(A)$ injective or not?",,"['linear-algebra', 'matrices', 'matrix-exponential']"
44,Representing polynomials as quadratic forms,Representing polynomials as quadratic forms,,"I have the following cubic equation $$x^3-6x^2+11x-6=(x-1)(x-2)(x-3)=0$$ whose solutions are $x=1,2,3$. What if the above equation were represented by quadratic form ? Let $\mathbf x = \begin{bmatrix} x^2 & x & 1\end{bmatrix}^T$. Can the cubic equation above be represented as follows? $$x^3-6x^2+11x-6=\begin{bmatrix}x^2&x&1\end{bmatrix}\begin{bmatrix}0&b&c\\d&e&f\\g&h&-6\end{bmatrix}\begin{bmatrix}x^2\\x\\1\end{bmatrix} = \textbf{x}^T \textbf{A} \textbf{x}$$ There are many possibilities to form matrix $A$. For instance the two matrices below $$\textbf{A}_1 = \begin{bmatrix}0&0.5&0\\0.5&-6&5.5\\0&5.5&-6\end{bmatrix}\quad\quad\quad\textbf{A}_2 = \begin{bmatrix}0&0&0\\1&-3&6\\-3&5&-6\end{bmatrix}$$ Is there any study about finding the homogeneous solution, $\textbf{x}^T \textbf{A} \textbf{x}=0$? Thank you in advance.","I have the following cubic equation $$x^3-6x^2+11x-6=(x-1)(x-2)(x-3)=0$$ whose solutions are $x=1,2,3$. What if the above equation were represented by quadratic form ? Let $\mathbf x = \begin{bmatrix} x^2 & x & 1\end{bmatrix}^T$. Can the cubic equation above be represented as follows? $$x^3-6x^2+11x-6=\begin{bmatrix}x^2&x&1\end{bmatrix}\begin{bmatrix}0&b&c\\d&e&f\\g&h&-6\end{bmatrix}\begin{bmatrix}x^2\\x\\1\end{bmatrix} = \textbf{x}^T \textbf{A} \textbf{x}$$ There are many possibilities to form matrix $A$. For instance the two matrices below $$\textbf{A}_1 = \begin{bmatrix}0&0.5&0\\0.5&-6&5.5\\0&5.5&-6\end{bmatrix}\quad\quad\quad\textbf{A}_2 = \begin{bmatrix}0&0&0\\1&-3&6\\-3&5&-6\end{bmatrix}$$ Is there any study about finding the homogeneous solution, $\textbf{x}^T \textbf{A} \textbf{x}=0$? Thank you in advance.",,"['matrices', 'polynomials', 'factoring', 'quadratic-forms', 'cubics']"
45,Are eigenvectors preserved by conjugating by a diagonal matrix?,Are eigenvectors preserved by conjugating by a diagonal matrix?,,"Let $A$ be a real symmetric matrix and $D$ a real diagonal matrix with non-zero entries along the diagonal. Is it true that the eigenvectors of $D^{-1}AD$ are the same as those of $A$? If not, can anything be said relating the eigenvectors/eigenvalues of these two matrices?","Let $A$ be a real symmetric matrix and $D$ a real diagonal matrix with non-zero entries along the diagonal. Is it true that the eigenvectors of $D^{-1}AD$ are the same as those of $A$? If not, can anything be said relating the eigenvectors/eigenvalues of these two matrices?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
46,Matrix equation including diagonal and orthogonal matrices,Matrix equation including diagonal and orthogonal matrices,,"I want to prove the following estimate (I'm not really sure if it really holds, but I'm interested in proving it): $$(1,...,1)UDU^T\left(\begin{array}{c} 1 \\ \vdots\\1 \end{array}\right) < n$$ where $U\in\mathbb{R}^{n\times n}$ is an orthogonal matrix and $D\in\mathbb{R}^{n\times n}$ is a diagonal matrix whose diagonal elements $d_i$ are all in the interval $\lbrack 0, 1) $.  I began the following way: Let $w:=U^T\left(\begin{array}{c} 1 \\ \vdots\\1 \end{array}\right)$ then we have $$(1,...,1)UDU^T\left(\begin{array}{c} 1 \\ \vdots\\1 \end{array}\right) = \langle w, Dw\rangle = \sum_{i=1}^n d_iw_i^2.$$ Further we have $w_i = \sum_{k=1}^nu_{k,i}$ where $U = (u_{i,j})_{i,j=1,...,n}$. I now want to exploit that the the columns of $U$ are orthonormal. Unfortunately I didn't find the right way to do so.","I want to prove the following estimate (I'm not really sure if it really holds, but I'm interested in proving it): $$(1,...,1)UDU^T\left(\begin{array}{c} 1 \\ \vdots\\1 \end{array}\right) < n$$ where $U\in\mathbb{R}^{n\times n}$ is an orthogonal matrix and $D\in\mathbb{R}^{n\times n}$ is a diagonal matrix whose diagonal elements $d_i$ are all in the interval $\lbrack 0, 1) $.  I began the following way: Let $w:=U^T\left(\begin{array}{c} 1 \\ \vdots\\1 \end{array}\right)$ then we have $$(1,...,1)UDU^T\left(\begin{array}{c} 1 \\ \vdots\\1 \end{array}\right) = \langle w, Dw\rangle = \sum_{i=1}^n d_iw_i^2.$$ Further we have $w_i = \sum_{k=1}^nu_{k,i}$ where $U = (u_{i,j})_{i,j=1,...,n}$. I now want to exploit that the the columns of $U$ are orthonormal. Unfortunately I didn't find the right way to do so.",,"['matrices', 'inner-products', 'orthogonality', 'estimation', 'orthogonal-matrices']"
47,When is a *-derivation inner (in matrix- or C*-algebra)?,When is a *-derivation inner (in matrix- or C*-algebra)?,,"Let $A$ be a C*-algebra (actually, the finite dimensional case of $A=Mat(n\times n,\mathbb{C})$ is my main interest at the moment) and $\delta\colon A\to A$ a *-derivation, i.e. a linear map with $\delta(ab)=\delta(a)b+a\delta(b)$ and $\delta(a^*)=\delta(a)^*$. Clearly, the commutator with a self-adjoint element is an example, that is $\delta(a)= i[h,a] = i(ha-ah)$. My question is: What are the conditions on $\delta$ such that such an $h$ exists (and how can I obtain it)? A bit of background: This is like a non-commutative version of Noether's theorem: If $A$ arises from the algebra of functions on a symplectic space then derivations are vector fields and Hamiltonian ones are those that arise from taking Poisson brackets with the ""Noether charge"". There is a differential condition (making sure the Lie derivative of the symplectic structure vanishes) plus a topological one making sure the closed 1-form obtained from contracting the vector field with the symplectic form is in fact exact. The case of 2x2 matrices, I can figure out myself, but that makes use of Pauli matrices and the fact that traceless 2x2 matrices can be represented by 3-vectors with the vector product (i.e. this might be very special due to low dimensionality).","Let $A$ be a C*-algebra (actually, the finite dimensional case of $A=Mat(n\times n,\mathbb{C})$ is my main interest at the moment) and $\delta\colon A\to A$ a *-derivation, i.e. a linear map with $\delta(ab)=\delta(a)b+a\delta(b)$ and $\delta(a^*)=\delta(a)^*$. Clearly, the commutator with a self-adjoint element is an example, that is $\delta(a)= i[h,a] = i(ha-ah)$. My question is: What are the conditions on $\delta$ such that such an $h$ exists (and how can I obtain it)? A bit of background: This is like a non-commutative version of Noether's theorem: If $A$ arises from the algebra of functions on a symplectic space then derivations are vector fields and Hamiltonian ones are those that arise from taking Poisson brackets with the ""Noether charge"". There is a differential condition (making sure the Lie derivative of the symplectic structure vanishes) plus a topological one making sure the closed 1-form obtained from contracting the vector field with the symplectic form is in fact exact. The case of 2x2 matrices, I can figure out myself, but that makes use of Pauli matrices and the fact that traceless 2x2 matrices can be represented by 3-vectors with the vector product (i.e. this might be very special due to low dimensionality).",,"['matrices', 'c-star-algebras', 'noncommutative-algebra']"
48,Eigenvalues of a $2\times2$ matrix,Eigenvalues of a  matrix,2\times2,"Let $a,b$ be distinct eigen values of a $2\times2$ matrix $A.$Then which of the following statement is true? $A^2$ has distinct eigen values. $A^3=\frac{a^3-b^3}{a-b}A-ab(a+b)I$ Trace of $A^n$ is $a^n+b^n$ for every positive integer n. $A^n$ is not a scalar multiple of identity matrix for any positive integer n. I think first option is wrong because if $1,-1$ are distinct eigenvalues of $A$ but $A^2$ has eigenvalues $1,1$. Third option is correct as we have result. I tried second option. Second is also right. For fourth option, characteristic equation implies $A^n$ cannot be a scalar multiple of identity matrix for any positive integer $n$. Is it correct?","Let $a,b$ be distinct eigen values of a $2\times2$ matrix $A.$Then which of the following statement is true? $A^2$ has distinct eigen values. $A^3=\frac{a^3-b^3}{a-b}A-ab(a+b)I$ Trace of $A^n$ is $a^n+b^n$ for every positive integer n. $A^n$ is not a scalar multiple of identity matrix for any positive integer n. I think first option is wrong because if $1,-1$ are distinct eigenvalues of $A$ but $A^2$ has eigenvalues $1,1$. Third option is correct as we have result. I tried second option. Second is also right. For fourth option, characteristic equation implies $A^n$ cannot be a scalar multiple of identity matrix for any positive integer $n$. Is it correct?",,"['linear-algebra', 'matrices', 'proof-verification']"
49,Product of doubly stochastic matrix,Product of doubly stochastic matrix,,"I have a question: Definitions A matrix $A$ is doubly stochastic if: a) $ 0\leq a_{ij}\leq1. $ b) $ \sum_{j=1}^{n}a_{ij}=1\;\forall i=1,2,\ldots, n. $ c) $ \sum_{i=1}^{n}a_{ij}=1\;\forall j=1,2,\ldots, n. $ Prove that the product of two doubly stochastic matrices is doubly stochastic. Part a) $$0\leq b_{kj}\leq 1,\forall k,j\in\{1,\ldots,n\}$$ $$\implies0\leq a_{ik}\cdot b_{kj}\leq a_{ik},\forall i\in\{1,\ldots,n\}$$ $$0\leq \sum_{k=1}^{n} a_{ik}b_{kj}\leq\sum_{k=1}^{n}a_{ik}$$ $$\implies0\leq c_{ij}\leq1.$$ Part b) I use this property: $$\sum_{p=1}^{m}(\sum_{k=1}^{n}a_{ik}\cdot b_{kp})=\sum_{k=1}^{n}(\sum_{p=1}^{m}a_{ik}\cdot b_{kp})$$ $$\sum_{k=1}^{n} c_{ik}=\sum_{k=1}^{n}(\sum_{p=1}^{n}a_{ip}b_{pk})\mbox{, definition}$$ $$\implies \sum_{p=1}^{n}(\sum_{k=1}^{n}a_{ip}b_{pk}).$$ $$\implies\sum_{p=1}^{n} a_{ip}(\sum_{k=1}^{n}b_{pk}).$$ But $ \sum_{k=1}^{n}b_{pk} $ is the sum of  elements in one row of matrix $B$ and it is equal to 1 (doubly stochastic). $$ \implies\sum_{p=1}^{n}a_{ip}\cdot1=\sum_{p=1}^{n}a_{ip}$$ And $ \sum_{p=1}^{n}a_{ip} $ eis the sum of  elements in one row of matrix $A$ and it is equal to 1 (doubly stochastic). Please help me for the part c) or complete my proof. Thank you so much Quote of today: ""Firstly, it is connected with technology. In order to do numerical analysis, you essentially need a machine."" Jacques-Louis Lions","I have a question: Definitions A matrix is doubly stochastic if: a) b) c) Prove that the product of two doubly stochastic matrices is doubly stochastic. Part a) Part b) I use this property: But is the sum of  elements in one row of matrix and it is equal to 1 (doubly stochastic). And eis the sum of  elements in one row of matrix and it is equal to 1 (doubly stochastic). Please help me for the part c) or complete my proof. Thank you so much Quote of today: ""Firstly, it is connected with technology. In order to do numerical analysis, you essentially need a machine."" Jacques-Louis Lions","A  0\leq a_{ij}\leq1.   \sum_{j=1}^{n}a_{ij}=1\;\forall i=1,2,\ldots, n.   \sum_{i=1}^{n}a_{ij}=1\;\forall j=1,2,\ldots, n.  0\leq b_{kj}\leq 1,\forall k,j\in\{1,\ldots,n\} \implies0\leq a_{ik}\cdot b_{kj}\leq a_{ik},\forall i\in\{1,\ldots,n\} 0\leq \sum_{k=1}^{n} a_{ik}b_{kj}\leq\sum_{k=1}^{n}a_{ik} \implies0\leq c_{ij}\leq1. \sum_{p=1}^{m}(\sum_{k=1}^{n}a_{ik}\cdot b_{kp})=\sum_{k=1}^{n}(\sum_{p=1}^{m}a_{ik}\cdot b_{kp}) \sum_{k=1}^{n} c_{ik}=\sum_{k=1}^{n}(\sum_{p=1}^{n}a_{ip}b_{pk})\mbox{, definition} \implies \sum_{p=1}^{n}(\sum_{k=1}^{n}a_{ip}b_{pk}). \implies\sum_{p=1}^{n} a_{ip}(\sum_{k=1}^{n}b_{pk}).  \sum_{k=1}^{n}b_{pk}  B  \implies\sum_{p=1}^{n}a_{ip}\cdot1=\sum_{p=1}^{n}a_{ip}  \sum_{p=1}^{n}a_{ip}  A","['linear-algebra', 'matrices']"
50,Prove that the determinant over $\mathbb F^{2\times 2}$ is a quadratic form,Prove that the determinant over  is a quadratic form,\mathbb F^{2\times 2},"I am working in the vector space $\mathbb F^{2\times 2}$, which denotes the space of $2\times 2$ matrices over the field $\mathbb F$ (the characteristic of $\mathbb F$ is not equal to $2$) and I would like to proof that: $\det:A\mapsto\det A$ is a quadratic form. Furthermore, I would like to show that $$\det(A+B) - \det(A) - \det(B) = tr(AB^\#)$$ whereas $tr$ denotes the trace of the matrix and $B^\#$ denotes the cofactor matrix of $B$. My definition of a quadratic form $q$ is that $q(cx) = c^2q(x)$ and that the map $$(x,y) \mapsto q(x+y)-q(x)-q(y)$$ is a bilinear form. I could easily prove the first criterion, but I am really stuck with the second. I also tried to first show the equality above (the trace equality), but for some reason, for matrices $$A= \begin{pmatrix} a & b\\c & d \end{pmatrix} \qquad B = \begin{pmatrix} w & x\\y & z  \end{pmatrix}$$ the left side gives me $az+dw-by-cx$ and the right side leaves me with $az+dw-bx-cy$ I am pretty sure that I computed the determinants and the cofactor matrix correctly, so I am genuinely confused where my mistake lies and I also do not know how to prove the bilinear form criterion. Any help is greatly appreciated!","I am working in the vector space $\mathbb F^{2\times 2}$, which denotes the space of $2\times 2$ matrices over the field $\mathbb F$ (the characteristic of $\mathbb F$ is not equal to $2$) and I would like to proof that: $\det:A\mapsto\det A$ is a quadratic form. Furthermore, I would like to show that $$\det(A+B) - \det(A) - \det(B) = tr(AB^\#)$$ whereas $tr$ denotes the trace of the matrix and $B^\#$ denotes the cofactor matrix of $B$. My definition of a quadratic form $q$ is that $q(cx) = c^2q(x)$ and that the map $$(x,y) \mapsto q(x+y)-q(x)-q(y)$$ is a bilinear form. I could easily prove the first criterion, but I am really stuck with the second. I also tried to first show the equality above (the trace equality), but for some reason, for matrices $$A= \begin{pmatrix} a & b\\c & d \end{pmatrix} \qquad B = \begin{pmatrix} w & x\\y & z  \end{pmatrix}$$ the left side gives me $az+dw-by-cx$ and the right side leaves me with $az+dw-bx-cy$ I am pretty sure that I computed the determinants and the cofactor matrix correctly, so I am genuinely confused where my mistake lies and I also do not know how to prove the bilinear form criterion. Any help is greatly appreciated!",,"['linear-algebra', 'matrices', 'quadratic-forms']"
51,Factorizing an 8×8 unitary matrix into tensor product of three 2×2 unitaries,Factorizing an 8×8 unitary matrix into tensor product of three 2×2 unitaries,,"I am an undergraduate physics student currently in the final stages of a BSc project. I am trying to decompose an $8 \times 8$ unitary matrix into a tensor product of three $2 \times 2$ unitaries. This is in the context of quantum information where we need to rotate measurement operators for a 3 qubit system but are limited to performing local (ie $2 \times 2$) unitary rotations. Concretely, the problem we have is as follows: We have an 'unphysical' unitary rotation $U_{8 \times 8}^{unphysical}$ that needs to be performed. It is unphysical as it is not a product of single qubit ($2 \times 2$) unitary matrices. So we have tried to numerically (in python) generate $$U_{8 \times 8}^{physical} = U_{2 \times 2}^{(1)} \otimes U_{2 \times 2}^{(2)} \otimes U_{2 \times 2}^{(3)}$$  where we guess the $4 \times 3$ (since $k = 1,2,3$) parameters $\theta^{(k)}, \phi^{(k)}_1, \phi^{(k)}_2, \phi^{(k)}$ in the single qubit unitaries $$U_{2 \times 2}^{(k)} = e^{i \phi^{(k)}} \left( \begin{array}{cc} 		e^{i \phi^{(k)}_1} \cos(\theta^{(k)})& e^{i \phi^{(k)}_2} \sin(\theta^{(k)})\\ 		-e^{-i \phi^{(k)}_2}\sin(\theta^{(k)}) & e^{-i \phi^{(k)}_1} \cos(\theta^{(k)})\\ 	\end{array} \right)$$ I have a suspicion that it may not be possible to obtain a factorization that produces $U_{8 \times 8}^{physical} = U_{8 \times 8}^{unphysical}$ but I would like to get as close as possible. Our current cost function is simply the Frobenius norm of of $U_{8 \times 8}^{physical} - U_{8 \times 8}^{unphysical}$ where every time the solver guesses the parameters, the cost function builds the corresponding single qubit unitaries, tensors (kronecker products) them together as in the construction above and calculates this 'distance'. The black box solver (scipy.optimize.minimize) stops after a suspiciously short number of iterations (we have used it throughout the project and it has been robust) and finishes with the cost function being about 2 or 3, which is clearly not good enough. We can either come up with a better cost function and be able to decompose $U_{8 \times 8}^{unphysical}$ numerically, or there may be a mathematical theorem/construct that can help us do this analytically. I am happy to provide more detail on the context or clarify the problem we have. Thank you, Alex","I am an undergraduate physics student currently in the final stages of a BSc project. I am trying to decompose an $8 \times 8$ unitary matrix into a tensor product of three $2 \times 2$ unitaries. This is in the context of quantum information where we need to rotate measurement operators for a 3 qubit system but are limited to performing local (ie $2 \times 2$) unitary rotations. Concretely, the problem we have is as follows: We have an 'unphysical' unitary rotation $U_{8 \times 8}^{unphysical}$ that needs to be performed. It is unphysical as it is not a product of single qubit ($2 \times 2$) unitary matrices. So we have tried to numerically (in python) generate $$U_{8 \times 8}^{physical} = U_{2 \times 2}^{(1)} \otimes U_{2 \times 2}^{(2)} \otimes U_{2 \times 2}^{(3)}$$  where we guess the $4 \times 3$ (since $k = 1,2,3$) parameters $\theta^{(k)}, \phi^{(k)}_1, \phi^{(k)}_2, \phi^{(k)}$ in the single qubit unitaries $$U_{2 \times 2}^{(k)} = e^{i \phi^{(k)}} \left( \begin{array}{cc} 		e^{i \phi^{(k)}_1} \cos(\theta^{(k)})& e^{i \phi^{(k)}_2} \sin(\theta^{(k)})\\ 		-e^{-i \phi^{(k)}_2}\sin(\theta^{(k)}) & e^{-i \phi^{(k)}_1} \cos(\theta^{(k)})\\ 	\end{array} \right)$$ I have a suspicion that it may not be possible to obtain a factorization that produces $U_{8 \times 8}^{physical} = U_{8 \times 8}^{unphysical}$ but I would like to get as close as possible. Our current cost function is simply the Frobenius norm of of $U_{8 \times 8}^{physical} - U_{8 \times 8}^{unphysical}$ where every time the solver guesses the parameters, the cost function builds the corresponding single qubit unitaries, tensors (kronecker products) them together as in the construction above and calculates this 'distance'. The black box solver (scipy.optimize.minimize) stops after a suspiciously short number of iterations (we have used it throughout the project and it has been robust) and finishes with the cost function being about 2 or 3, which is clearly not good enough. We can either come up with a better cost function and be able to decompose $U_{8 \times 8}^{unphysical}$ numerically, or there may be a mathematical theorem/construct that can help us do this analytically. I am happy to provide more detail on the context or clarify the problem we have. Thank you, Alex",,"['matrices', 'numerical-linear-algebra', 'kronecker-product', 'quantum-information']"
52,"If $A$ is an $m \times n$ matrix, prove that $Ax=0$ has infinite solutions $\iff$ $\text{rank}(A)<n$","If  is an  matrix, prove that  has infinite solutions",A m \times n Ax=0 \iff \text{rank}(A)<n,"Prove that $Ax=0$ has infinite solutions if and only if $ \text{rank}(A)<n$ Here is one way of my if proof, but I don't know how to proof the other part. Let $r$ be the rank of $A$. Then $ r\leq  n$. If $ r<n$, then there are $n-r$ linearly independent solutions. Furthermore any linear combination of these solutions will also be a solution of $Ax=0$. Hence, in this case, the equation $Ax=0$ has infinite number of solutions","Prove that $Ax=0$ has infinite solutions if and only if $ \text{rank}(A)<n$ Here is one way of my if proof, but I don't know how to proof the other part. Let $r$ be the rank of $A$. Then $ r\leq  n$. If $ r<n$, then there are $n-r$ linearly independent solutions. Furthermore any linear combination of these solutions will also be a solution of $Ax=0$. Hence, in this case, the equation $Ax=0$ has infinite number of solutions",,"['linear-algebra', 'matrices', 'solution-verification', 'matrix-rank']"
53,"Jaccard index, matrix notation","Jaccard index, matrix notation",,"I have a matrix with rows representing events and columns representing users. The elements of the matrix are binary values indicating if a user has attended the event or not. \begin{bmatrix}1&1&0&1&1\\1&1&0&0&1\\ 1&0&0&1&1\end{bmatrix} I need matrix notation to compute the Jaccard distances between users. \begin{align}   J(U_1,U_2)=\frac{|U_1\cap U_2|}{|U_1\cup U_2|} \end{align} To compute the numerator I can use the matrix operation  \begin{align}   A^T\times A \end{align} Now my question is how to get the denominator of Jaccard index using the matrix notation.","I have a matrix with rows representing events and columns representing users. The elements of the matrix are binary values indicating if a user has attended the event or not. \begin{bmatrix}1&1&0&1&1\\1&1&0&0&1\\ 1&0&0&1&1\end{bmatrix} I need matrix notation to compute the Jaccard distances between users. \begin{align}   J(U_1,U_2)=\frac{|U_1\cap U_2|}{|U_1\cup U_2|} \end{align} To compute the numerator I can use the matrix operation  \begin{align}   A^T\times A \end{align} Now my question is how to get the denominator of Jaccard index using the matrix notation.",,['matrices']
54,Schur decomposition for $3 \times 3$ matrix,Schur decomposition for  matrix,3 \times 3,"Suppose $A=\begin{bmatrix} 1 &-2 &2\\-1 &1 &1\\-2 &0 &3 \end{bmatrix}$, what is the Schur decomposition? The eigenvalues of $A$ are $\lambda_1 = 1,\lambda_2 = 2+2i$ and $\lambda_3 = 2-2i$. For $2 \times 2$ matrix, if the eigenvalues are real, then I just pick one normalized eigenvector to construct an orthonormal basis. If the eigenvalues are complex numbers, then do the similar process (the other vector should be orthogonal to the conjugate of the normalized eigenvector); however, for $3 \times 3$, I'm not sure how to find $U$ such that $U^{H}AU=T$, where $U$ is unitary and $T$ is upper triangular. Thanks.","Suppose $A=\begin{bmatrix} 1 &-2 &2\\-1 &1 &1\\-2 &0 &3 \end{bmatrix}$, what is the Schur decomposition? The eigenvalues of $A$ are $\lambda_1 = 1,\lambda_2 = 2+2i$ and $\lambda_3 = 2-2i$. For $2 \times 2$ matrix, if the eigenvalues are real, then I just pick one normalized eigenvector to construct an orthonormal basis. If the eigenvalues are complex numbers, then do the similar process (the other vector should be orthogonal to the conjugate of the normalized eigenvector); however, for $3 \times 3$, I'm not sure how to find $U$ such that $U^{H}AU=T$, where $U$ is unitary and $T$ is upper triangular. Thanks.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'schur-decomposition']"
55,Invertibility of perturbed matrices,Invertibility of perturbed matrices,,"Suppose I have two real symmetric matrices $A$ and $B$, where $B$ is invertible. $A$ may or may not be invertible. Both $A$ and $B$ have positive and negative eigenvalues (i.e., no positive semi-definiteness). I'm interested in the invertibility of $A+\delta B$, where $\delta$ is a very small perturbation parameter. Intuitively, as $\delta$ gets arbitrarily small the fixed matrix $A$ can no longer ""mess up"" the eigenvalues of $A+\delta B$ and the perturbed matrix should be invertible. This leads me to the following question: Question 1: Does there exist a small $\epsilon>0$ (which may depend on $A$ and $B$) such that, for all $\delta$ with $0<|\delta|<\epsilon$, the perturbed matrix $(A+\delta B)$ is invertible? Furthermore, is it true that $\|(A+\delta B)^{-1}\|_{op} \leq C(A,B)\cdot \delta^{-1}$ for small $\delta$, where $C(A,B)$ is some constant that only depends on $A$ and $B$? A more general question involves multiple matrices: Qeustion 2 : Let $A,B_1,\cdots,B_r$ be fixed real symmetric matrices and suppose $B_r$ is invertible. Does there exist a small $\epsilon>0$ such that, for all $\delta$ with $0<|\delta|<\epsilon$, the matrix $M(\delta) = A+\delta B_1+\cdots+\delta^r B_r$ is invertible? Furthermore, is it true that $\|M(\delta)^{-1}\|_{op} \leq C(A, B_1,\cdots,B_r)\delta^{-r}$ for small $\delta$, where $C(\cdots)$ only depends on the fixed matrices?","Suppose I have two real symmetric matrices $A$ and $B$, where $B$ is invertible. $A$ may or may not be invertible. Both $A$ and $B$ have positive and negative eigenvalues (i.e., no positive semi-definiteness). I'm interested in the invertibility of $A+\delta B$, where $\delta$ is a very small perturbation parameter. Intuitively, as $\delta$ gets arbitrarily small the fixed matrix $A$ can no longer ""mess up"" the eigenvalues of $A+\delta B$ and the perturbed matrix should be invertible. This leads me to the following question: Question 1: Does there exist a small $\epsilon>0$ (which may depend on $A$ and $B$) such that, for all $\delta$ with $0<|\delta|<\epsilon$, the perturbed matrix $(A+\delta B)$ is invertible? Furthermore, is it true that $\|(A+\delta B)^{-1}\|_{op} \leq C(A,B)\cdot \delta^{-1}$ for small $\delta$, where $C(A,B)$ is some constant that only depends on $A$ and $B$? A more general question involves multiple matrices: Qeustion 2 : Let $A,B_1,\cdots,B_r$ be fixed real symmetric matrices and suppose $B_r$ is invertible. Does there exist a small $\epsilon>0$ such that, for all $\delta$ with $0<|\delta|<\epsilon$, the matrix $M(\delta) = A+\delta B_1+\cdots+\delta^r B_r$ is invertible? Furthermore, is it true that $\|M(\delta)^{-1}\|_{op} \leq C(A, B_1,\cdots,B_r)\delta^{-r}$ for small $\delta$, where $C(\cdots)$ only depends on the fixed matrices?",,"['matrices', 'limits', 'eigenvalues-eigenvectors', 'perturbation-theory']"
56,Decomposition of an idempotent matrix,Decomposition of an idempotent matrix,,"Let $A$ be an $n\times n$ idempotent matrix i.e. $A^2=A$. Suppose that $A=A_1+A_2$ and $rank(A)=rank(A_1)+rank(A_2)$. Show that $A_1,A_2$ are both idempotent and $A_1A_2=A_2A_1=0$. Without confusion, I'd like to identify the matrices and their corresponding linear transformations. Then we have $\text{Im}A\subset\text{Im}A_1+\text{Im}A_2$. Hence we have $\dim\text{Im}A\leq\dim(\text{Im}A_1+\text{Im}A_2)\leq\dim\text{Im}A_1+\dim\text{Im}A_2$. However, the dimension of the image space is the same as the rank of the corresponding matrix. Hence the equality holds. In other words, we have $\dim\text{Im}A=\dim(\text{Im}A_1+\text{Im}A_2)=\dim\text{Im}A_1+\dim\text{Im}A_2$. In particular, $\text{Im}A=\text{Im}A_1+\text{Im}A_2$. So we deduce that $\text{Im}A=\text{Im}A_1\oplus\text{Im}A_2$. But I don't know what can I do next. Could anyone help me complete the proof?","Let $A$ be an $n\times n$ idempotent matrix i.e. $A^2=A$. Suppose that $A=A_1+A_2$ and $rank(A)=rank(A_1)+rank(A_2)$. Show that $A_1,A_2$ are both idempotent and $A_1A_2=A_2A_1=0$. Without confusion, I'd like to identify the matrices and their corresponding linear transformations. Then we have $\text{Im}A\subset\text{Im}A_1+\text{Im}A_2$. Hence we have $\dim\text{Im}A\leq\dim(\text{Im}A_1+\text{Im}A_2)\leq\dim\text{Im}A_1+\dim\text{Im}A_2$. However, the dimension of the image space is the same as the rank of the corresponding matrix. Hence the equality holds. In other words, we have $\dim\text{Im}A=\dim(\text{Im}A_1+\text{Im}A_2)=\dim\text{Im}A_1+\dim\text{Im}A_2$. In particular, $\text{Im}A=\text{Im}A_1+\text{Im}A_2$. So we deduce that $\text{Im}A=\text{Im}A_1\oplus\text{Im}A_2$. But I don't know what can I do next. Could anyone help me complete the proof?",,"['matrices', 'linear-transformations', 'idempotents']"
57,Math inverse Matrix with variable,Math inverse Matrix with variable,,"So it's just a few days before my math exam and I came across this excercise and I just can't seem to find the right answer. Would really appreciate your help on this one. It's from a german textbook so please excuse any grammar mistakes: d 'is celebrating a party. a, b and c are doing a drinking game. They all drink the same from same size glasses:' I: a and b together drank as much as c. II: a and c together drank 4 times as much as b III: a drank n more glasses then b 1) I need to find out the Linear System of Equations. So I would assume: I: a + b = c II: a + c = 4b III: b + n = a To write the matrix it in the form of: A*X = B: => I: a + b - c = 0 => II: a - 4b + c = 0 => III: -a + b + n = 0 (please correct if I'm wrong) next: 2) Find the inverse Matrix with the Gauß-Jordan-Algorithm This is my solution for the inverse Matrix Is that correct? This is where I had a lot of problems with. Because of that extra variable n in the matrix. 3) From that inverse matrix i have to find universal solution where I just have to put in n to solve. Don't really know what to do here. If I take my inverse matrix and multiply it by B wich is (0 0 0) all my values for a,b,c would get 0... So don't really know how to proceed... 4) I just need to try different values for n and show the Equations for different n values. Would really really appreciate any kind of help. I've been working on this for more than 3 hours and I just can't seem to find the answer. Thank you in advance !","So it's just a few days before my math exam and I came across this excercise and I just can't seem to find the right answer. Would really appreciate your help on this one. It's from a german textbook so please excuse any grammar mistakes: d 'is celebrating a party. a, b and c are doing a drinking game. They all drink the same from same size glasses:' I: a and b together drank as much as c. II: a and c together drank 4 times as much as b III: a drank n more glasses then b 1) I need to find out the Linear System of Equations. So I would assume: I: a + b = c II: a + c = 4b III: b + n = a To write the matrix it in the form of: A*X = B: => I: a + b - c = 0 => II: a - 4b + c = 0 => III: -a + b + n = 0 (please correct if I'm wrong) next: 2) Find the inverse Matrix with the Gauß-Jordan-Algorithm This is my solution for the inverse Matrix Is that correct? This is where I had a lot of problems with. Because of that extra variable n in the matrix. 3) From that inverse matrix i have to find universal solution where I just have to put in n to solve. Don't really know what to do here. If I take my inverse matrix and multiply it by B wich is (0 0 0) all my values for a,b,c would get 0... So don't really know how to proceed... 4) I just need to try different values for n and show the Equations for different n values. Would really really appreciate any kind of help. I've been working on this for more than 3 hours and I just can't seem to find the answer. Thank you in advance !",,"['linear-algebra', 'matrices', 'matrix-equations', 'gaussian-elimination']"
58,"If $\Sigma$ is a covariance matrix, how to obtain the decomposition $\Sigma = \Sigma^{1/2}(\Sigma^{1/2})^T$ from Cholesky Decompositon?","If  is a covariance matrix, how to obtain the decomposition  from Cholesky Decompositon?",\Sigma \Sigma = \Sigma^{1/2}(\Sigma^{1/2})^T,"I read somewhere that if $\Sigma$ is a covariance matrix, one can obtain the decomposition $\Sigma = \Sigma^{1/2}(\Sigma^{1/2})^T$ from Cholesky Decompositon. However, I am confused how because Cholesky decomposition requires an upper and lower triangular matrix. How can the square root here for upper and lower triangular, or can it?","I read somewhere that if $\Sigma$ is a covariance matrix, one can obtain the decomposition $\Sigma = \Sigma^{1/2}(\Sigma^{1/2})^T$ from Cholesky Decompositon. However, I am confused how because Cholesky decomposition requires an upper and lower triangular matrix. How can the square root here for upper and lower triangular, or can it?",,"['linear-algebra', 'matrices', 'statistics', 'matrix-decomposition']"
59,positive elements in $M_n(A^+)$,positive elements in,M_n(A^+),"I have a (perhaps simple) question regarding positive elements in tensor products with matrix algebras. Let $A$ be $C^*$-algebra and $A\otimes M_n(\mathbb{C})$ the (minimal) tensor product of $A$ and $M_n(\mathbb{C})$. Let $A^+$ the unitization of $A$. Why does every positive element in $ A^+\otimes M_n(\mathbb{C})\cong M_n(A^+)$ look like $a+x\otimes 1_{A^+}$ with $a^*=a \in M_n(A)$ and $x\in M_n(\mathbb{C})$ with $x\ge 0$ (or sums of such $a+x\otimes 1_{A^+}$)? I understand if $a+x\otimes 1_{A^+} \in A^+\otimes M_n(\mathbb{C})$ is positive, it must be $a^*=a$ and $x\ge 0$. But I don't understand why every positive element in $M_n(A^+)$ looks like such an element $a+x\otimes 1_{A^+}$ (or sums of it). I appreciate your help.","I have a (perhaps simple) question regarding positive elements in tensor products with matrix algebras. Let $A$ be $C^*$-algebra and $A\otimes M_n(\mathbb{C})$ the (minimal) tensor product of $A$ and $M_n(\mathbb{C})$. Let $A^+$ the unitization of $A$. Why does every positive element in $ A^+\otimes M_n(\mathbb{C})\cong M_n(A^+)$ look like $a+x\otimes 1_{A^+}$ with $a^*=a \in M_n(A)$ and $x\in M_n(\mathbb{C})$ with $x\ge 0$ (or sums of such $a+x\otimes 1_{A^+}$)? I understand if $a+x\otimes 1_{A^+} \in A^+\otimes M_n(\mathbb{C})$ is positive, it must be $a^*=a$ and $x\ge 0$. But I don't understand why every positive element in $M_n(A^+)$ looks like such an element $a+x\otimes 1_{A^+}$ (or sums of it). I appreciate your help.",,"['matrices', 'tensor-products', 'operator-algebras', 'c-star-algebras']"
60,Vector space of $m\times n$ matrix $\Bbb R^{m\times n}$ vs vector space of a vector of size $mn$ $\Bbb R^{mn}$?,Vector space of  matrix  vs vector space of a vector of size  ?,m\times n \Bbb R^{m\times n} mn \Bbb R^{mn},"I have a very basic question in linear algebra. Every vector of size $n$ with each entry from $\Bbb R$ lies in the space $\Bbb R^n$ , where $\Bbb R^n$ is the cartesian product of $n$ copies of the set $\Bbb R$ . But how is it different from the vector space of matrices? It is written in my books that the vector space of matrices is $\Bbb R^{m\times n}$ , but what does that mean? What is $\Bbb R^{m\times n}$ ? Because if it is the same as $\Bbb R^{mn}$ , then how is an $m\times n$ matrix different from an $mn$ dimensional vector?","I have a very basic question in linear algebra. Every vector of size with each entry from lies in the space , where is the cartesian product of copies of the set . But how is it different from the vector space of matrices? It is written in my books that the vector space of matrices is , but what does that mean? What is ? Because if it is the same as , then how is an matrix different from an dimensional vector?",n \Bbb R \Bbb R^n \Bbb R^n n \Bbb R \Bbb R^{m\times n} \Bbb R^{m\times n} \Bbb R^{mn} m\times n mn,"['linear-algebra', 'matrices', 'vector-spaces']"
61,Explaining invertible matrices with linear transformations,Explaining invertible matrices with linear transformations,,"Suppose that $A$ and $B$ are square matrices and that $AB$ is invertible. Using the interpretation of multiplication by $A$ (or $B$) as a linear transformation from $\Bbb{R}^n \to \Bbb{R}^n$, explain why both $A$ and $B$ must be invertible. So I think it has to do with $x \mapsto Ax$ and $x \mapsto Bx$ being onto and one-to-one and then using the invertible matrix theorem, but I don't quite understand how to answer this precisely.","Suppose that $A$ and $B$ are square matrices and that $AB$ is invertible. Using the interpretation of multiplication by $A$ (or $B$) as a linear transformation from $\Bbb{R}^n \to \Bbb{R}^n$, explain why both $A$ and $B$ must be invertible. So I think it has to do with $x \mapsto Ax$ and $x \mapsto Bx$ being onto and one-to-one and then using the invertible matrix theorem, but I don't quite understand how to answer this precisely.",,"['linear-algebra', 'matrices', 'inverse', 'linear-transformations']"
62,integration of a nonnegative definite matrix,integration of a nonnegative definite matrix,,"I wanted to know, whether the integration of a nonnegative definite matrix is again going to be a nonnegative definite matrix? for example, if $A(t)$ is a nonnegative definite  $n\times n$ matrix, then what we can tell about $\int_{a}^{b} A(t) dt,$   where $t\in [a, b]$","I wanted to know, whether the integration of a nonnegative definite matrix is again going to be a nonnegative definite matrix? for example, if $A(t)$ is a nonnegative definite  $n\times n$ matrix, then what we can tell about $\int_{a}^{b} A(t) dt,$   where $t\in [a, b]$",,"['linear-algebra', 'matrices']"
63,Why the adjacency matrix of a tree can be written as $N + N^T$ where $N$ is nilpotent?,Why the adjacency matrix of a tree can be written as  where  is nilpotent?,N + N^T N,"I'm interested in looking for properties of adjacency matrices of trees. On one comment of this question: https://mathoverflow.net/questions/89692/if-graph-is-tree-what-can-be-said-about-its-adjacency-matrix , Some one answered that every adjacency matrix of tree can be written as $N + N^T$ by picking a vertex as root and follow the edges. It seems reasonable. However, I can't think of a more rigorous proof of it.  Can someone help provide some insights?  Thanks!","I'm interested in looking for properties of adjacency matrices of trees. On one comment of this question: https://mathoverflow.net/questions/89692/if-graph-is-tree-what-can-be-said-about-its-adjacency-matrix , Some one answered that every adjacency matrix of tree can be written as $N + N^T$ by picking a vertex as root and follow the edges. It seems reasonable. However, I can't think of a more rigorous proof of it.  Can someone help provide some insights?  Thanks!",,"['matrices', 'graph-theory', 'trees', 'nilpotence']"
64,How to prove vectors are linearly independent based on determinant,How to prove vectors are linearly independent based on determinant,,"I am stuck on the following problem: Prove that the vectors $\vec{\text{u}}_1$, $\vec{\text{u}}_2$, and $\vec{\text{u}}_3$ are linearly independent if and only if   $$ \det \begin{bmatrix} x_1 & y_1 & z_1 \\ x_2 & y_2 & z_2 \\ x_3 & y_3 & z_3 \end{bmatrix} \neq 0 $$ I'm having trouble choosing which direction to go about. I'm aware of the following: The above determinant is equivalent to the triple product of said vectors: $$ \vec{\text{u}}_1 \cdot (\vec{\text{u}}_2 \times \vec{\text{u}}_3) $$ The three above vectors are linearly independent if the equation $$ a_1 \vec{\text{u}}_1 + a_2 \vec{\text{u}}_2 + a_3 \vec{\text{u}}_3 = 0 $$ has only the trivial solution. Any two vectors are linearly independent if their dot product is equal to 0. My first attempt was to use the three combinations of dot products to create the following system : \begin{gather*} a_1 x_1 x_2 + a_1 y_1 y_2 + a_1 z_1 z_2 = 0 \\ a_2 x_1 x_3 + a_2 y_1 y_3 + a_2 z_1 z_3 = 0 \\ a_3 x_2 x_3 + a_3 y_2 y_3 + a_3 z_2 z_3 = 0 \end{gather*} and somehow work it into the determinant, but I feel like this isn't the correct way to do it. Another idea was to use Sarrus' rule on the determinant to obtain a new equation through which I could work my way up to the second bullet point, but that doesn't seem to make any sense. Maybe I can't see the forest for the trees; I'm not really good at linear algebra proofs. I'm just looking for a hint to go in the right direction.","I am stuck on the following problem: Prove that the vectors $\vec{\text{u}}_1$, $\vec{\text{u}}_2$, and $\vec{\text{u}}_3$ are linearly independent if and only if   $$ \det \begin{bmatrix} x_1 & y_1 & z_1 \\ x_2 & y_2 & z_2 \\ x_3 & y_3 & z_3 \end{bmatrix} \neq 0 $$ I'm having trouble choosing which direction to go about. I'm aware of the following: The above determinant is equivalent to the triple product of said vectors: $$ \vec{\text{u}}_1 \cdot (\vec{\text{u}}_2 \times \vec{\text{u}}_3) $$ The three above vectors are linearly independent if the equation $$ a_1 \vec{\text{u}}_1 + a_2 \vec{\text{u}}_2 + a_3 \vec{\text{u}}_3 = 0 $$ has only the trivial solution. Any two vectors are linearly independent if their dot product is equal to 0. My first attempt was to use the three combinations of dot products to create the following system : \begin{gather*} a_1 x_1 x_2 + a_1 y_1 y_2 + a_1 z_1 z_2 = 0 \\ a_2 x_1 x_3 + a_2 y_1 y_3 + a_2 z_1 z_3 = 0 \\ a_3 x_2 x_3 + a_3 y_2 y_3 + a_3 z_2 z_3 = 0 \end{gather*} and somehow work it into the determinant, but I feel like this isn't the correct way to do it. Another idea was to use Sarrus' rule on the determinant to obtain a new equation through which I could work my way up to the second bullet point, but that doesn't seem to make any sense. Maybe I can't see the forest for the trees; I'm not really good at linear algebra proofs. I'm just looking for a hint to go in the right direction.",,"['linear-algebra', 'matrices', 'proof-writing', 'vectors', 'determinant']"
65,Using a 4D transformation matrix to scale a 3D object around a given point,Using a 4D transformation matrix to scale a 3D object around a given point,,"I have a 3D object – let's call it $B$ – and I want to scale it by a given amount – let's say $M$ – around a particular point (maybe the center, a corner, or maybe an arbitrary point) – let's say $P$ – using a 4×4 transformation matrix. (For example, if I want to double a certain sphere in size about its center, I specify $B$ to be a sphere, $P$ its center and $M$ to be 2). My question is, what is this generalized 4×4 matrix? [For reference, I'm writing a program that I need to use this in and can get a bounding box around $B$, the coordinates of the point $P$ and a double $M$.] I'm pretty sure my matrix has to be something like: $$ \left( \begin{array}{cccc} M_{11} & M_{12} & M_{13} & 0 \\ M_{21} & M_{22} & M_{23} & 0 \\ M_{31} & M_{32} & M_{33} & 0 \\ 0 & 0 & 0 & 1 \end{array} \right) $$","I have a 3D object – let's call it $B$ – and I want to scale it by a given amount – let's say $M$ – around a particular point (maybe the center, a corner, or maybe an arbitrary point) – let's say $P$ – using a 4×4 transformation matrix. (For example, if I want to double a certain sphere in size about its center, I specify $B$ to be a sphere, $P$ its center and $M$ to be 2). My question is, what is this generalized 4×4 matrix? [For reference, I'm writing a program that I need to use this in and can get a bounding box around $B$, the coordinates of the point $P$ and a double $M$.] I'm pretty sure my matrix has to be something like: $$ \left( \begin{array}{cccc} M_{11} & M_{12} & M_{13} & 0 \\ M_{21} & M_{22} & M_{23} & 0 \\ M_{31} & M_{32} & M_{33} & 0 \\ 0 & 0 & 0 & 1 \end{array} \right) $$",,"['matrices', 'transformation']"
66,Prove that $\|A\|_2 = \sqrt{\|A^* A \|_2}$,Prove that,\|A\|_2 = \sqrt{\|A^* A \|_2},"This is a homework exercise, so hints are welcome. Prove $\|A\|_2 = \sqrt{\|A^* A \|_2}$. The $\|\cdot\|_2$ is the induced 2-norm, and $A \in M_{n,m}(\mathbb{C})$. $A^*$ is the complex conjugate. Note that the matrix $2$-norm is defined as $$  \sup_x\{\|Ax\|_2 \,\big|\,\|x\|_2 \leq 1 \}\text{,} $$ where $\|x\|_2$ is the Euclidean norm. So far, I tried expanding the norms, resulting in $$ \sup_x\{\sum_i|(Ax)_i|^2\,\big|\, \|x\|_2 \leq 1 \} = \sup_x\{\sqrt{\sum_i |(A^* A x)_i|^2} \,\big|\,\|x\|_2 \leq 1 \}\text{.} $$ Unfortunately, I don't have a clue where to go from here.","This is a homework exercise, so hints are welcome. Prove $\|A\|_2 = \sqrt{\|A^* A \|_2}$. The $\|\cdot\|_2$ is the induced 2-norm, and $A \in M_{n,m}(\mathbb{C})$. $A^*$ is the complex conjugate. Note that the matrix $2$-norm is defined as $$  \sup_x\{\|Ax\|_2 \,\big|\,\|x\|_2 \leq 1 \}\text{,} $$ where $\|x\|_2$ is the Euclidean norm. So far, I tried expanding the norms, resulting in $$ \sup_x\{\sum_i|(Ax)_i|^2\,\big|\, \|x\|_2 \leq 1 \} = \sup_x\{\sqrt{\sum_i |(A^* A x)_i|^2} \,\big|\,\|x\|_2 \leq 1 \}\text{.} $$ Unfortunately, I don't have a clue where to go from here.",,"['linear-algebra', 'matrices', 'normed-spaces', 'numerical-linear-algebra']"
67,"Proving that $\max_{\|M\|_{rs}\leq 1,\ \|b\|_s\leq 1} \|A^{-1}(Mc-b) \|_r \geq \|A^{-1}\|_{sr}(\|c\|_r+1) $",Proving that,"\max_{\|M\|_{rs}\leq 1,\ \|b\|_s\leq 1} \|A^{-1}(Mc-b) \|_r \geq \|A^{-1}\|_{sr}(\|c\|_r+1) ","In fact, what I really want to prove is the equality. The another inequality is already proven, but this one I'm not able to prove. Fix an invertible matrix $A\in\mathbb{R}^{n\times n}$ and a vector $c\in\mathbb{R}^n$, I need to prove the following inequality. $$\max_{\|M\|_{rs}\leq 1,\ \|b\|_s\leq 1} \|A^{-1}(Mc-b) \|_r \geq \|A^{-1}\|_{sr}(\|c\|_r+1) $$ $r,s$ are positive integers and $\|\cdot\|_{rs}$ is the matrix norm induced by the norms $\|\cdot\|_r$ and $\|\cdot\|_s$. Thank you very much for your help. EDIT: Ok, I have an answer and I'm very grateful for it. But what I should have said is that I want a direct proof. Is it possible to exhibit $M$ and $b$ in order to get an equality?","In fact, what I really want to prove is the equality. The another inequality is already proven, but this one I'm not able to prove. Fix an invertible matrix $A\in\mathbb{R}^{n\times n}$ and a vector $c\in\mathbb{R}^n$, I need to prove the following inequality. $$\max_{\|M\|_{rs}\leq 1,\ \|b\|_s\leq 1} \|A^{-1}(Mc-b) \|_r \geq \|A^{-1}\|_{sr}(\|c\|_r+1) $$ $r,s$ are positive integers and $\|\cdot\|_{rs}$ is the matrix norm induced by the norms $\|\cdot\|_r$ and $\|\cdot\|_s$. Thank you very much for your help. EDIT: Ok, I have an answer and I'm very grateful for it. But what I should have said is that I want a direct proof. Is it possible to exhibit $M$ and $b$ in order to get an equality?",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
68,Finding the basis for the intersection of two subspaces,Finding the basis for the intersection of two subspaces,,"Find a basis for the intersection of the subspaces $X_1$ and $X_2$ of $\mathbb{R^4}$ where $$X_1=\text{span}\left\{ (1,1,0,0), (0,1,1,0), (0,0,1,1)\right\}$$ and $$X_2=\text{span}\left\{ (1,0,t,0), (0,1,0,t)\right\},$$ where $t\in \mathbb{R}$ is given. Well, I know that if $v\in X_1\cap X_2$, then $v=a(1,1,0,0)+ b(0,1,1,0)+ c(0,0,1,1)=d(1,0,t,0)+ m(0,1,0,t)$, where $a, b, c, d, m \in \mathbb{R}$. Then this implies that $(0,0,0,0)=v-v=(a-d,a+b-m,b+c-dt,c-mt)$, that is $$a-d=0$$ $$a+b-m=0$$ $$b+c-dt=0$$ $$c-mt=0.$$ Since $t\in \mathbb{R}$ is given, I have 5 unknowns and 4 equations. I can't solve this! Is there any other way to solve this problem?? Thank you!","Find a basis for the intersection of the subspaces $X_1$ and $X_2$ of $\mathbb{R^4}$ where $$X_1=\text{span}\left\{ (1,1,0,0), (0,1,1,0), (0,0,1,1)\right\}$$ and $$X_2=\text{span}\left\{ (1,0,t,0), (0,1,0,t)\right\},$$ where $t\in \mathbb{R}$ is given. Well, I know that if $v\in X_1\cap X_2$, then $v=a(1,1,0,0)+ b(0,1,1,0)+ c(0,0,1,1)=d(1,0,t,0)+ m(0,1,0,t)$, where $a, b, c, d, m \in \mathbb{R}$. Then this implies that $(0,0,0,0)=v-v=(a-d,a+b-m,b+c-dt,c-mt)$, that is $$a-d=0$$ $$a+b-m=0$$ $$b+c-dt=0$$ $$c-mt=0.$$ Since $t\in \mathbb{R}$ is given, I have 5 unknowns and 4 equations. I can't solve this! Is there any other way to solve this problem?? Thank you!",,"['linear-algebra', 'matrices']"
69,A matrix with positive principal minors has positive eigenvalues if it is real.,A matrix with positive principal minors has positive eigenvalues if it is real.,,"Let $A$ be an $n\times n$ matrix, not necessarily symmetric, and suppose all principal minors of $A$ are positive. How to show that any real eigenvalue of $A$ is must be positive?","Let $A$ be an $n\times n$ matrix, not necessarily symmetric, and suppose all principal minors of $A$ are positive. How to show that any real eigenvalue of $A$ is must be positive?",,"['linear-algebra', 'matrices']"
70,Derivation of gradient for non negative matrix factorization,Derivation of gradient for non negative matrix factorization,,"I am looking at a paper for non-negative matrix factorization and can't seem to figure out the derivation for the gradient. The function is as follows: $f(W,H) = \frac{1}{2}||V-WH ||^2_F$ Where V is fixed. The resulting gradients are: $\nabla_W f(W,H) = (WH-V)H^T $ $\nabla_H f(W,H) = W^T(WH-V) $ I tried looking up rules in various reference manuals but never got anything solid. Even a hint / solid resource would be incredibly helpful.","I am looking at a paper for non-negative matrix factorization and can't seem to figure out the derivation for the gradient. The function is as follows: $f(W,H) = \frac{1}{2}||V-WH ||^2_F$ Where V is fixed. The resulting gradients are: $\nabla_W f(W,H) = (WH-V)H^T $ $\nabla_H f(W,H) = W^T(WH-V) $ I tried looking up rules in various reference manuals but never got anything solid. Even a hint / solid resource would be incredibly helpful.",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'partial-derivative']"
71,Combining Column- and Row-wise meanings of a matrix,Combining Column- and Row-wise meanings of a matrix,,A matrix can be thought of in terms of columns (then it represents the basis vectors of an coordinate system) or in terms of rows (then it represents a set of linear equations). How can we combine those two views? Example: Let the matrix $A$ represent a set of linear equations. Then each row is a linear equation. But we can also see the matrix as the basis vectors of a coordinate system. What coordinate system does matrix $A$ represent? What is the link between the column-wise and row-wise viewpoints on matrix?,A matrix can be thought of in terms of columns (then it represents the basis vectors of an coordinate system) or in terms of rows (then it represents a set of linear equations). How can we combine those two views? Example: Let the matrix $A$ represent a set of linear equations. Then each row is a linear equation. But we can also see the matrix as the basis vectors of a coordinate system. What coordinate system does matrix $A$ represent? What is the link between the column-wise and row-wise viewpoints on matrix?,,"['linear-algebra', 'matrices']"
72,Square Root of a matrix without diagonalization,Square Root of a matrix without diagonalization,,"It's well known ( wikipedia: Square root of a matrix ) that we can calculate a square root of a matrix by diagonalization or Jordan decomposition (if it's possible). But I saw a different method which seems to work, although it isn't obvious for me why it works. The method for a diagonalizable matrix $A$: First calculate the minimal polynomial $\mu_A$ of the matrix. Then chose a polynomial $p$ with $p(\lambda)=\sqrt{\lambda}$ for every eigenvalue $\lambda$. Now we get $\sqrt{A}=p(A)$. But does it always work and why? It seems to be possible to use the method in a similar way, if the matrix isn't diagonalisable e.g. if the multiplicity $\lambda$ as a zero of $\mu$  is greater than one. In this case we have to add conditions to the derivates of $p$, like $p'(\lambda)=\frac{1}{2\sqrt{\lambda}}$. Is this also correct?","It's well known ( wikipedia: Square root of a matrix ) that we can calculate a square root of a matrix by diagonalization or Jordan decomposition (if it's possible). But I saw a different method which seems to work, although it isn't obvious for me why it works. The method for a diagonalizable matrix $A$: First calculate the minimal polynomial $\mu_A$ of the matrix. Then chose a polynomial $p$ with $p(\lambda)=\sqrt{\lambda}$ for every eigenvalue $\lambda$. Now we get $\sqrt{A}=p(A)$. But does it always work and why? It seems to be possible to use the method in a similar way, if the matrix isn't diagonalisable e.g. if the multiplicity $\lambda$ as a zero of $\mu$  is greater than one. In this case we have to add conditions to the derivates of $p$, like $p'(\lambda)=\frac{1}{2\sqrt{\lambda}}$. Is this also correct?",,['linear-algebra']
73,Show that the sequence of norms of inverses of a convergent sequence of matrices diverges to infinity.,Show that the sequence of norms of inverses of a convergent sequence of matrices diverges to infinity.,,"This is a question I found while working on the book ""Analysis in Euclidean Spaces"" by Ken Hoffman. Suppose $(A_n)$ is a sequence of invertible matrices from $\mathbb{R}^{k \times k}$ that converges to the matrix $A$. Show that if $A$ is not invertible, then $$\lim_{n \to \infty} \| A_n^{-1} \| = \infty.$$ I can easily show that if the sequence $(A_n^{-1})$ converges to some matrix $B$, then $B = A^{-1}$, but I don't know how to proceed if the sequence is not convergent (I can also prove the case where $A = \textbf{0}$, the zero matrix).","This is a question I found while working on the book ""Analysis in Euclidean Spaces"" by Ken Hoffman. Suppose $(A_n)$ is a sequence of invertible matrices from $\mathbb{R}^{k \times k}$ that converges to the matrix $A$. Show that if $A$ is not invertible, then $$\lim_{n \to \infty} \| A_n^{-1} \| = \infty.$$ I can easily show that if the sequence $(A_n^{-1})$ converges to some matrix $B$, then $B = A^{-1}$, but I don't know how to proceed if the sequence is not convergent (I can also prove the case where $A = \textbf{0}$, the zero matrix).",,"['real-analysis', 'matrices']"
74,Matrix equivalent to linear maps - sanity check,Matrix equivalent to linear maps - sanity check,,"I'm reading some Linear algebra notes I found online, and am a bit confused about the following: If $U,V$ are finite dimensional $\mathbb{C}$-spaces with bases $(\mathbf{u}_1,\dots,\mathbf{u}_m)$ and $(\mathbf{v}_1,\dots,\mathbf{v}_n)$ then there is a correspondence: $$\mathrm{M}_{n\times m}(\mathbb{C})\longleftrightarrow \mathcal{L}(U,V);\;\;A\longleftrightarrow \alpha\;\text{where}\;\alpha(\mathbf{u}_i)=\sum_j A_{ji}\mathbf{v}_j$$ I'm confused about is the definition of $\alpha$. Why is $\mathbf{u}_i$ being sent to $\left(\mathbf{v}^{\text{T}}\cdot A^{\text{T}}\right)_i?$ (If I am reading that right that is). Why is $\alpha$ acting on $\mathbf{u}$ not the same as $A$ acting on $\mathbf{u}$ i.e. so $\left(A\cdot\mathbf{u}\right)_i$ instead? Similarly it says that if $\alpha$ is represented by $A$ w.r.t. bases $\{\mathbf{u}_i\},\{\mathbf{v}_j\}$ and $\tilde A$ w.r.t. bases $\{ \mathbf{\tilde u}_i\},\{\mathbf{\tilde v}_j\}$ where $\mathbf{\tilde u}_i=\sum_k P_{ki}\mathbf{u}_k$ and $\mathbf{\tilde v}_j=\sum_k Q_{kj}\mathbf{v}_k$ then: $$\tilde A=Q^{-1}AP$$ Again it feels like things are the wrong way around. To me it looks like $P$ takes $\{\mathbf{u}_i\}\to\{\mathbf{\tilde u}_i\}$ and $Q$ takes $\{\mathbf{v}_i\}\to\{\mathbf{\tilde v}_i\}$ (both in a weird transpose/row vector way) but then the expression above suggests the opposite. I realise this is really basic but I'm having a brain-freeze moment here.","I'm reading some Linear algebra notes I found online, and am a bit confused about the following: If $U,V$ are finite dimensional $\mathbb{C}$-spaces with bases $(\mathbf{u}_1,\dots,\mathbf{u}_m)$ and $(\mathbf{v}_1,\dots,\mathbf{v}_n)$ then there is a correspondence: $$\mathrm{M}_{n\times m}(\mathbb{C})\longleftrightarrow \mathcal{L}(U,V);\;\;A\longleftrightarrow \alpha\;\text{where}\;\alpha(\mathbf{u}_i)=\sum_j A_{ji}\mathbf{v}_j$$ I'm confused about is the definition of $\alpha$. Why is $\mathbf{u}_i$ being sent to $\left(\mathbf{v}^{\text{T}}\cdot A^{\text{T}}\right)_i?$ (If I am reading that right that is). Why is $\alpha$ acting on $\mathbf{u}$ not the same as $A$ acting on $\mathbf{u}$ i.e. so $\left(A\cdot\mathbf{u}\right)_i$ instead? Similarly it says that if $\alpha$ is represented by $A$ w.r.t. bases $\{\mathbf{u}_i\},\{\mathbf{v}_j\}$ and $\tilde A$ w.r.t. bases $\{ \mathbf{\tilde u}_i\},\{\mathbf{\tilde v}_j\}$ where $\mathbf{\tilde u}_i=\sum_k P_{ki}\mathbf{u}_k$ and $\mathbf{\tilde v}_j=\sum_k Q_{kj}\mathbf{v}_k$ then: $$\tilde A=Q^{-1}AP$$ Again it feels like things are the wrong way around. To me it looks like $P$ takes $\{\mathbf{u}_i\}\to\{\mathbf{\tilde u}_i\}$ and $Q$ takes $\{\mathbf{v}_i\}\to\{\mathbf{\tilde v}_i\}$ (both in a weird transpose/row vector way) but then the expression above suggests the opposite. I realise this is really basic but I'm having a brain-freeze moment here.",,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations']"
75,Characteristic Polynomial and Minimal Polynomial,Characteristic Polynomial and Minimal Polynomial,,(True or false): Suppose $A$ is an $n \times n$ matrix and $A^{k} = 0$ for some k. Then the characteristic polynomial is $x^n$ I am inclined to believe this is true since if $x^{k} = 0$ then the minimal polynomial divides $x^k$ thus implying that the minimal polynomial has the form of $x^a$ for some $a$. Which implies that the characteristic polynomial has the form of $x^b$ for some $b$ since the minimal polynomial divides the characteristic polynomial. I know that the characteristic polynomial for an $n \times n$ matrix will have the form of $x^n -trace(A)x^{n-1} + ....+(-1^n)det(A)$ So if the characteristic polynomial has the form $x^b$ then for an $n \times n$ matrix it will have the form $x^n$ Another question is  (True or false) Suppose $A^k = 0$ for some $k$. Then the minimal polynomial of A is $x^k$ I am inclined to believe this is false since the question doesnt really state weather $k < n$ or $k > n$ or if $k$ is the first number which sets $A$ to zero. Thus there can be an $a < k$ where $A^a = 0$ Is my reasoning sound for both examples?,(True or false): Suppose $A$ is an $n \times n$ matrix and $A^{k} = 0$ for some k. Then the characteristic polynomial is $x^n$ I am inclined to believe this is true since if $x^{k} = 0$ then the minimal polynomial divides $x^k$ thus implying that the minimal polynomial has the form of $x^a$ for some $a$. Which implies that the characteristic polynomial has the form of $x^b$ for some $b$ since the minimal polynomial divides the characteristic polynomial. I know that the characteristic polynomial for an $n \times n$ matrix will have the form of $x^n -trace(A)x^{n-1} + ....+(-1^n)det(A)$ So if the characteristic polynomial has the form $x^b$ then for an $n \times n$ matrix it will have the form $x^n$ Another question is  (True or false) Suppose $A^k = 0$ for some $k$. Then the minimal polynomial of A is $x^k$ I am inclined to believe this is false since the question doesnt really state weather $k < n$ or $k > n$ or if $k$ is the first number which sets $A$ to zero. Thus there can be an $a < k$ where $A^a = 0$ Is my reasoning sound for both examples?,,"['linear-algebra', 'matrices']"
76,Find the cardinality of the set of permutation matrices.,Find the cardinality of the set of permutation matrices.,,"Let $S$ be the set of all $3\times3$ matrices $A$ with integer entries such that $AA^t=I$, where $A^t$ is the transpose of $A$. Then $|S|=?$ $12$ $24$ $48$ $60$ I tried to find the structure of such matrices from an arbitrary matrix and using $AA^t=I$ and found that these are permutation matrices. So how to calculate the cardinality of set of permutation matrices? Isn't it $n!$? But that does not give me anything to relate with the options given. Any hint or help please.","Let $S$ be the set of all $3\times3$ matrices $A$ with integer entries such that $AA^t=I$, where $A^t$ is the transpose of $A$. Then $|S|=?$ $12$ $24$ $48$ $60$ I tried to find the structure of such matrices from an arbitrary matrix and using $AA^t=I$ and found that these are permutation matrices. So how to calculate the cardinality of set of permutation matrices? Isn't it $n!$? But that does not give me anything to relate with the options given. Any hint or help please.",,"['linear-algebra', 'matrices']"
77,Finding a $5\times5$ Matrix such that the sum of it and its inverse is a $5\times 5$ matrix with each entry $1$.,Finding a  Matrix such that the sum of it and its inverse is a  matrix with each entry .,5\times5 5\times 5 1,"How can I do this question: Is it possible to find a $5 \times5$ invertible matrix $B$ over $\mathbb Z_2$ such that: $B+B^{-1}=5\times5$ matrix where every element is $1$ anyway, I assume that the question requires a proof if such a matrix $B$ does not exist and working/solution(maybe?) if $B$ exists Thanks!","How can I do this question: Is it possible to find a $5 \times5$ invertible matrix $B$ over $\mathbb Z_2$ such that: $B+B^{-1}=5\times5$ matrix where every element is $1$ anyway, I assume that the question requires a proof if such a matrix $B$ does not exist and working/solution(maybe?) if $B$ exists Thanks!",,"['matrices', 'matrix-equations']"
78,SVD for augmented matrix,SVD for augmented matrix,,"I'm doing the following problem to prepare for my exam tomorrow: Suppose that $A$ is a $n \times m$ matrix with $n \geq m$ and that the singular values of $A$ are $\sigma_1, \sigma_2, \dots, \sigma_m$ . Now consider the augmented matrix $$B = \left[ \begin{array}{cc} A \\ I \end{array} \right],$$ where $I$ is the $m \times m$ identity matrix. We want to show that the singular values of $B$ are $\tilde{\sigma}_i = \sqrt{1 + \sigma_i^2}$ for each $i$ . I'm currently lost on where to begin. I know that $A = U\Sigma V^T$ for orthogonal matrices $U$ and $V$ with $\Sigma$ containing the singular values on the diagonal, by definition. However, I'm not seeing how to extend this to show the desired result for $B$ . Can someone lend a hand?","I'm doing the following problem to prepare for my exam tomorrow: Suppose that is a matrix with and that the singular values of are . Now consider the augmented matrix where is the identity matrix. We want to show that the singular values of are for each . I'm currently lost on where to begin. I know that for orthogonal matrices and with containing the singular values on the diagonal, by definition. However, I'm not seeing how to extend this to show the desired result for . Can someone lend a hand?","A n \times m n \geq m A \sigma_1, \sigma_2, \dots, \sigma_m B = \left[ \begin{array}{cc} A \\ I \end{array} \right], I m \times m B \tilde{\sigma}_i = \sqrt{1 + \sigma_i^2} i A = U\Sigma V^T U V \Sigma B","['linear-algebra', 'matrices', 'svd']"
79,"For matrices, how to deduce $AB$ using $A^2, B^2, (A+B)^2$ and so on (any matrix squares)?","For matrices, how to deduce  using  and so on (any matrix squares)?","AB A^2, B^2, (A+B)^2","Assume matrix square can be calculated in $O(n^c)$ time, show that any square matrix multiplication can be done in the same $O(n^c)$ time. The problem I got is that $AB$ and $BA$ always occur together with same coefficients. So that I cannot get $AB$ only. Thanks in advance and any suggestion is welcomed.","Assume matrix square can be calculated in $O(n^c)$ time, show that any square matrix multiplication can be done in the same $O(n^c)$ time. The problem I got is that $AB$ and $BA$ always occur together with same coefficients. So that I cannot get $AB$ only. Thanks in advance and any suggestion is welcomed.",,"['linear-algebra', 'matrices']"
80,2-Norm of Non-Square Matrices,2-Norm of Non-Square Matrices,,"So, the 2-norm of an $m \times n$ matrix for $m\geq n$ is defined by the max singular value/square of the max eigenvalue. But, if it's not square, and you're only given a matrix A (no x-vector), what do you do if 1. m>n. Do I have to go through the whole SVD process, since I can't find an eigenvalue? or 2. if n>m, since I can't do SVD then. Please note that I'm talking about if I'm only given a matrix A, so I can't use that $||A||{_2} =max _{ \space \vec x \neq 0}  \frac{||A\vec x||{_2}}{||\vec x||_{2}}  $ .","So, the 2-norm of an $m \times n$ matrix for $m\geq n$ is defined by the max singular value/square of the max eigenvalue. But, if it's not square, and you're only given a matrix A (no x-vector), what do you do if 1. m>n. Do I have to go through the whole SVD process, since I can't find an eigenvalue? or 2. if n>m, since I can't do SVD then. Please note that I'm talking about if I'm only given a matrix A, so I can't use that $||A||{_2} =max _{ \space \vec x \neq 0}  \frac{||A\vec x||{_2}}{||\vec x||_{2}}  $ .",,"['linear-algebra', 'matrices', 'analysis', 'numerical-methods', 'normed-spaces']"
81,Stuck with LDU-factorization of a matrix where D should contain zeros,Stuck with LDU-factorization of a matrix where D should contain zeros,,"I thought that L-D-U- factorization of a square matrix (L=lower triangular factor, D=diagonal factors, U=upper triangular factor) was always possible and meaningful even if I encounter zeros on the diagonal factor D. But my algorithm is not correct in that cases in that the resulting factors do not reproduce the source. Let $$  M = \begin{bmatrix}      1&    2&    3&    4&    5\\     2&    4&    6&    8&    0\\     3&    6&    9&    2&    5\\     4&    8&    2&    6&    0\\     5&    0&    5&    0&    5      \end{bmatrix}$$ which is just the top-left of the basic $10 \times 10$ multiplication-table (modulo $10$). It has full rank; but in the naive LDU-algorithm one would assign zeros to some entries of D because zeros occur in the top-left element of the intermediate matrices of the iteration. If I do that, I get LDU-components $$\small L=\begin{bmatrix}   1 & 0 & 0 & 0 & 0  \\   2 & 1 & 0 & 0 & 0  \\   3 & 0 & 1 & 0 & 0  \\   4 & 0 & 0 & 1 & 0  \\   5 & 0 & 0 & 2 & 1  \\   \end{bmatrix}   D= \begin{bmatrix}   1 & 0 & 0 & 0 & 0  \\   0 & 0 & 0 & 0 & 0  \\   0 & 0 & 0 & 0 & 0  \\   0 & 0 & 0 & -10 & 0  \\   0 & 0 & 0 & 0 & 20  \end{bmatrix}   U=\begin{bmatrix}   1 & 2 & 3 & 4 & 5  \\   0 & 1 & 0 & 0 & 0  \\   0 & 0 & 1 & 0 & 0  \\   0 & 0 & 0 & 1 & 2  \\   0 & 0 & 0 & 0 & 1  \\   \end{bmatrix}$$ and if I put them together they don't reproduce the source M: $$ \text{chk}=L \cdot D \cdot U = \small \begin{bmatrix}   1 & 2 & 3 & 4 & 5 \\   2 & 4 & 6 & 8 & 10 \\   3 & 6 & 9 & 12 & 15 \\   4 & 8 & 12 & 6 & 0 \\   5 & 10 & 15 & 0 & 5  \end{bmatrix}$$ The problem is not, that the matrix-rank of M were not sufficient - Pari/GP gives the inverse and even the diagonalization. Q: Can this be repaired? Can a meaningful L-D-U-decomposition be given? Of course, if a general argument exists why and when invertible/diagonalizable matrices cannot be LU or LDU-decomposed I'd like to learn that, too.","I thought that L-D-U- factorization of a square matrix (L=lower triangular factor, D=diagonal factors, U=upper triangular factor) was always possible and meaningful even if I encounter zeros on the diagonal factor D. But my algorithm is not correct in that cases in that the resulting factors do not reproduce the source. Let $$  M = \begin{bmatrix}      1&    2&    3&    4&    5\\     2&    4&    6&    8&    0\\     3&    6&    9&    2&    5\\     4&    8&    2&    6&    0\\     5&    0&    5&    0&    5      \end{bmatrix}$$ which is just the top-left of the basic $10 \times 10$ multiplication-table (modulo $10$). It has full rank; but in the naive LDU-algorithm one would assign zeros to some entries of D because zeros occur in the top-left element of the intermediate matrices of the iteration. If I do that, I get LDU-components $$\small L=\begin{bmatrix}   1 & 0 & 0 & 0 & 0  \\   2 & 1 & 0 & 0 & 0  \\   3 & 0 & 1 & 0 & 0  \\   4 & 0 & 0 & 1 & 0  \\   5 & 0 & 0 & 2 & 1  \\   \end{bmatrix}   D= \begin{bmatrix}   1 & 0 & 0 & 0 & 0  \\   0 & 0 & 0 & 0 & 0  \\   0 & 0 & 0 & 0 & 0  \\   0 & 0 & 0 & -10 & 0  \\   0 & 0 & 0 & 0 & 20  \end{bmatrix}   U=\begin{bmatrix}   1 & 2 & 3 & 4 & 5  \\   0 & 1 & 0 & 0 & 0  \\   0 & 0 & 1 & 0 & 0  \\   0 & 0 & 0 & 1 & 2  \\   0 & 0 & 0 & 0 & 1  \\   \end{bmatrix}$$ and if I put them together they don't reproduce the source M: $$ \text{chk}=L \cdot D \cdot U = \small \begin{bmatrix}   1 & 2 & 3 & 4 & 5 \\   2 & 4 & 6 & 8 & 10 \\   3 & 6 & 9 & 12 & 15 \\   4 & 8 & 12 & 6 & 0 \\   5 & 10 & 15 & 0 & 5  \end{bmatrix}$$ The problem is not, that the matrix-rank of M were not sufficient - Pari/GP gives the inverse and even the diagonalization. Q: Can this be repaired? Can a meaningful L-D-U-decomposition be given? Of course, if a general argument exists why and when invertible/diagonalizable matrices cannot be LU or LDU-decomposed I'd like to learn that, too.",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
82,Find a matrix $A$ such that $\operatorname{proj}_W(x) = Ax$ for every $x \in \Bbb R^3$,Find a matrix  such that  for every,A \operatorname{proj}_W(x) = Ax x \in \Bbb R^3,"Let $W = \operatorname{Span}\{(1, -1, 0), (1, 1, 0)\}$. Find a matrix $A$ such that $\operatorname{proj}_W(x) = Ax$ for every $x \in \Bbb R^3$. I'm not sure how to solve this. What I tried doing is using y = $A(A^TA)^{-1}$ $A^T$ $\vec x$ where A was the matrix \begin{bmatrix}1&1\\-1&1\\0&0\end{bmatrix} Then, solving, I got  \begin{bmatrix}0&1&0\\0&1&0\\0&0&0\end{bmatrix} Is this the way to go about this question or am I completely off? I don't fully understand it.","Let $W = \operatorname{Span}\{(1, -1, 0), (1, 1, 0)\}$. Find a matrix $A$ such that $\operatorname{proj}_W(x) = Ax$ for every $x \in \Bbb R^3$. I'm not sure how to solve this. What I tried doing is using y = $A(A^TA)^{-1}$ $A^T$ $\vec x$ where A was the matrix \begin{bmatrix}1&1\\-1&1\\0&0\end{bmatrix} Then, solving, I got  \begin{bmatrix}0&1&0\\0&1&0\\0&0&0\end{bmatrix} Is this the way to go about this question or am I completely off? I don't fully understand it.",,"['linear-algebra', 'matrices']"
83,Intuition of the cyclic trace property of matrix products and relation to the determinant?,Intuition of the cyclic trace property of matrix products and relation to the determinant?,,"Let $A_1,\ldots,A_n$ be square matrices (not necessarily symmetric). Taking the trace of a matrix product and a cyclic permutation of the product yields: \begin{align*} \text{trace}(A_nA_{n-1}\cdots A_1) = \text{trace}(A_1A_{n}\cdots A_{2}) \end{align*} The above holds for any cyclic permutation (informally, taking an element off the end and appending it onto the other end), but not, in general, for arbitrary permutations of the matrix product. On the other hand, the determinant of a matrix product satisfies the following property $\text{det}(A_nA_{n-1}\cdots A_1) = \text{det}(A_n)\text{det}(A_{n-1})\cdots \text{det}(A_1)$. Thus, \begin{align*} \text{det}(A_nA_{n-1}\cdots A_1) = \text{det}(A_1A_{2}\cdots A_n) = \text{det}(\text{""any permutation of the product of $A$ matrices""}) \end{align*} The determinant can be thought of as the volume of its matrix argument whereas one can think of the trace as being the derivative of the determinant (near the identity matrix), that is, the infinitesimal change in volume . What is the intuition behind the trace property only holding for cyclic permutations where the determinant is the same for any permutation of the matrix product? Can this be explained in a way that is related to the trace being the determinant's derivative?","Let $A_1,\ldots,A_n$ be square matrices (not necessarily symmetric). Taking the trace of a matrix product and a cyclic permutation of the product yields: \begin{align*} \text{trace}(A_nA_{n-1}\cdots A_1) = \text{trace}(A_1A_{n}\cdots A_{2}) \end{align*} The above holds for any cyclic permutation (informally, taking an element off the end and appending it onto the other end), but not, in general, for arbitrary permutations of the matrix product. On the other hand, the determinant of a matrix product satisfies the following property $\text{det}(A_nA_{n-1}\cdots A_1) = \text{det}(A_n)\text{det}(A_{n-1})\cdots \text{det}(A_1)$. Thus, \begin{align*} \text{det}(A_nA_{n-1}\cdots A_1) = \text{det}(A_1A_{2}\cdots A_n) = \text{det}(\text{""any permutation of the product of $A$ matrices""}) \end{align*} The determinant can be thought of as the volume of its matrix argument whereas one can think of the trace as being the derivative of the determinant (near the identity matrix), that is, the infinitesimal change in volume . What is the intuition behind the trace property only holding for cyclic permutations where the determinant is the same for any permutation of the matrix product? Can this be explained in a way that is related to the trace being the determinant's derivative?",,"['matrices', 'determinant', 'trace']"
84,Show that $A$ is an invertible matrix if $\left(A+I\right)^3=0$ and find $A^{-1}$,Show that  is an invertible matrix if  and find,A \left(A+I\right)^3=0 A^{-1},"If $A\in M_{n\times n}\left(\mathbb{R}\right)$ is such that $ (A+I)^3=0$ , show that $A$ is an invertible matrix and find the inverse of $A$ . My idea was: \begin{eqnarray*} 0&=&\left(A+I\right)^3\\ &=&A^3+3A^2I+3AI^2+I^3\\ &=&A^3+3A^2+3A+I, \end{eqnarray*} then $$I=-A^3-3A^2-3A,$$ so $$I=A\left(-A^2-3A-3I\right).$$ It follows that $$A^{-1}=-A^2-3A-3I.$$ Now I found the inverse matrix, but how does this show that an inverse actually exists?","If is such that , show that is an invertible matrix and find the inverse of . My idea was: then so It follows that Now I found the inverse matrix, but how does this show that an inverse actually exists?","A\in M_{n\times n}\left(\mathbb{R}\right)  (A+I)^3=0 A A \begin{eqnarray*}
0&=&\left(A+I\right)^3\\
&=&A^3+3A^2I+3AI^2+I^3\\
&=&A^3+3A^2+3A+I,
\end{eqnarray*} I=-A^3-3A^2-3A, I=A\left(-A^2-3A-3I\right). A^{-1}=-A^2-3A-3I.","['linear-algebra', 'matrices', 'inverse']"
85,Are there matrices $A$ and $B$ such that $AB = BA \neq I$,Are there matrices  and  such that,A B AB = BA \neq I,"I've been learning about matrices and the identity matrix $I$. It says when $AB = BA = I$, then $A$ and $B$ are inverses of one another. Is it possible for $AB$ to equal $BA$ but not equal $I$?","I've been learning about matrices and the identity matrix $I$. It says when $AB = BA = I$, then $A$ and $B$ are inverses of one another. Is it possible for $AB$ to equal $BA$ but not equal $I$?",,['matrices']
86,Does Column space = range = image of a matrix?,Does Column space = range = image of a matrix?,,"Does computing the column space, range, and image of a matrix all produce the same answer?  And are they all written the same way? for example. Basis of the column space = {v1,v2} Basis of the image =  {v1,v2} Basis of the range = {v1,v2}","Does computing the column space, range, and image of a matrix all produce the same answer?  And are they all written the same way? for example. Basis of the column space = {v1,v2} Basis of the image =  {v1,v2} Basis of the range = {v1,v2}",,"['linear-algebra', 'matrices']"
87,Do unitary matrices commute?,Do unitary matrices commute?,,"Physically, unitary matrices of the same dimension describes a rigid motion, so it feels like the order of the rigid motion doesn't really matter? So do unitary matrices commute?","Physically, unitary matrices of the same dimension describes a rigid motion, so it feels like the order of the rigid motion doesn't really matter? So do unitary matrices commute?",,"['linear-algebra', 'matrices', 'unitary-matrices']"
88,"Given Matrices A and B, find x where Ax = B","Given Matrices A and B, find x where Ax = B",,"I'm given two simple matrices, and I'm told to solve for x. $A =\begin{pmatrix}\ 3 &-1 & 3\\1&0&3\\3&-2&-5&\end{pmatrix}$ $B =\begin{pmatrix}\ 14\\11\\-11\end{pmatrix}$ I'm told to find x, where $Ax = B$ I tried doing this: $A =\begin{pmatrix}\ 3 &-1 & 3\\1&0&3\\3&-2&-5&\end{pmatrix}$ $\begin{pmatrix}\ x\\x\\x\end{pmatrix}$ $B =\begin{pmatrix}\ 14\\11\\-11\end{pmatrix}$ Then I tried to multiply it out, and equate each row to b, however I got x = 14/5 and x = 11/4 , so I'm not even sure if I can do this, let alone don't know if this is correct?","I'm given two simple matrices, and I'm told to solve for x. $A =\begin{pmatrix}\ 3 &-1 & 3\\1&0&3\\3&-2&-5&\end{pmatrix}$ $B =\begin{pmatrix}\ 14\\11\\-11\end{pmatrix}$ I'm told to find x, where $Ax = B$ I tried doing this: $A =\begin{pmatrix}\ 3 &-1 & 3\\1&0&3\\3&-2&-5&\end{pmatrix}$ $\begin{pmatrix}\ x\\x\\x\end{pmatrix}$ $B =\begin{pmatrix}\ 14\\11\\-11\end{pmatrix}$ Then I tried to multiply it out, and equate each row to b, however I got x = 14/5 and x = 11/4 , so I'm not even sure if I can do this, let alone don't know if this is correct?",,"['matrices', 'algebra-precalculus', 'systems-of-equations', 'matrix-equations']"
89,Representing $\{x | Ax = b\}$ as the nullspace plus an offset,Representing  as the nullspace plus an offset,\{x | Ax = b\},"During a class on linear algebra my prof wrote $$\{x | Ax = b\} = x_p + N(A), x\in \mathbb{R}^n, A  \in \mathbb{R}^{m\times n}$$ (exactly as written) For the last two hours I have been trying to understand how you can go from left hand side to right hand side in non-hand wavy style as was demonstrated in the lecture. The prof is a good mathematician so I don't think he made a mistake, so the problem is me. I hope someone can help me out. Here's my confusion: Let $Ax_p = b$, and $Ax_n = 0$, then $\{x \in \mathbb{R}^n  | Ax = b\} = \{x_p \in \mathbb{R}^n | Ax_p = b\}\cup \{x_n \in \mathbb{R}^n | Ax_n = 0\} $ At the final step I just do not feel comfortable writing $\{x_p \in \mathbb{R}^n | Ax_p = b\}\cup \{x_n \in \mathbb{R}^n | Ax_n = 0\} = x_p + N(A)$ First:  $\{x_p \in \mathbb{R}^n | Ax_p = b\} \neq x_p$ Second: $\{x_n \in \mathbb{R}^n | Ax_n = 0\} \neq N(A) = \{x \in \mathbb{R}^n | Ax = 0\}$ Third: $\{x_p \in \mathbb{R}^n | Ax_p = b\}\cup \{x_n \in \mathbb{R}^n | Ax_n = 0\} \neq \{x_p \in \mathbb{R}^n | Ax_p = b\} + \{x_n \in \mathbb{R}^n | Ax_n = 0\}$ Can someone please point me to the right direction and show me how this relation can be made true?","During a class on linear algebra my prof wrote $$\{x | Ax = b\} = x_p + N(A), x\in \mathbb{R}^n, A  \in \mathbb{R}^{m\times n}$$ (exactly as written) For the last two hours I have been trying to understand how you can go from left hand side to right hand side in non-hand wavy style as was demonstrated in the lecture. The prof is a good mathematician so I don't think he made a mistake, so the problem is me. I hope someone can help me out. Here's my confusion: Let $Ax_p = b$, and $Ax_n = 0$, then $\{x \in \mathbb{R}^n  | Ax = b\} = \{x_p \in \mathbb{R}^n | Ax_p = b\}\cup \{x_n \in \mathbb{R}^n | Ax_n = 0\} $ At the final step I just do not feel comfortable writing $\{x_p \in \mathbb{R}^n | Ax_p = b\}\cup \{x_n \in \mathbb{R}^n | Ax_n = 0\} = x_p + N(A)$ First:  $\{x_p \in \mathbb{R}^n | Ax_p = b\} \neq x_p$ Second: $\{x_n \in \mathbb{R}^n | Ax_n = 0\} \neq N(A) = \{x \in \mathbb{R}^n | Ax = 0\}$ Third: $\{x_p \in \mathbb{R}^n | Ax_p = b\}\cup \{x_n \in \mathbb{R}^n | Ax_n = 0\} \neq \{x_p \in \mathbb{R}^n | Ax_p = b\} + \{x_n \in \mathbb{R}^n | Ax_n = 0\}$ Can someone please point me to the right direction and show me how this relation can be made true?",,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations']"
90,Finding right inverse matrix,Finding right inverse matrix,,"Given a $3\times 4$ matrix $A$ such as  $$         \begin{pmatrix}         1 & 1 & 1 & 1 \\         0 & 1 & 1 & 0 \\         0 & 0 & 1 & 1 \\         \end{pmatrix} ,$$  find a matrix $B_{4\times 3}$  such that $$AB =         \begin{pmatrix}         1 & 0 & 0  \\         0 & 1 & 0  \\         0 & 0 & 1  \\         \end{pmatrix} $$ Apart from simply multiplying $A$ with $B$ and generating a $12$ variable system of equations, is there any simpler way of finding $B$ ?","Given a $3\times 4$ matrix $A$ such as  $$         \begin{pmatrix}         1 & 1 & 1 & 1 \\         0 & 1 & 1 & 0 \\         0 & 0 & 1 & 1 \\         \end{pmatrix} ,$$  find a matrix $B_{4\times 3}$  such that $$AB =         \begin{pmatrix}         1 & 0 & 0  \\         0 & 1 & 0  \\         0 & 0 & 1  \\         \end{pmatrix} $$ Apart from simply multiplying $A$ with $B$ and generating a $12$ variable system of equations, is there any simpler way of finding $B$ ?",,"['linear-algebra', 'matrices', 'inverse']"
91,Matrix representation of linear map in this basis,Matrix representation of linear map in this basis,,"I am very stuck with this problem and I need help. I have tried different approaches, but I still don't know how to find the representation of $\mathcal{A}$ with respect to this new basis. Here is the problem: Consider a linear map $\mathcal{A} : (U, F)\to(U, F)$ where $U$ has finite dimension $n$. Assume   there exists a vector $b\in U$ such that the collection $$\{b,\mathcal{A}(b),\mathcal{A} \circ \mathcal{A}(b), ... ,\mathcal{A}^{n-1}(b)\}$$ forms a basis for $U$. Derive the representation of $\mathcal{A}$ and $b$ with respect to this basis. Thank you very much.","I am very stuck with this problem and I need help. I have tried different approaches, but I still don't know how to find the representation of $\mathcal{A}$ with respect to this new basis. Here is the problem: Consider a linear map $\mathcal{A} : (U, F)\to(U, F)$ where $U$ has finite dimension $n$. Assume   there exists a vector $b\in U$ such that the collection $$\{b,\mathcal{A}(b),\mathcal{A} \circ \mathcal{A}(b), ... ,\mathcal{A}^{n-1}(b)\}$$ forms a basis for $U$. Derive the representation of $\mathcal{A}$ and $b$ with respect to this basis. Thank you very much.",,"['linear-algebra', 'matrices']"
92,How can I show that this matrix exponential has norm strictly less than one?,How can I show that this matrix exponential has norm strictly less than one?,,"Let $t,\sigma,R \in \mathbb{R^+}$.  Let  $$ \mathrm{A} = \left\lbrack\begin{array}{cc}0 & -\sigma\\ \sigma &-R \end{array} \right\rbrack$$ want to show that the induced $\mathcal{l}_2$ norm of $\exp(\mathrm{A} t)$ is strictly less than one. This is what I have tried. $$\exp(\mathrm{A} t) =  \exp(-R t/2) \left\lbrack\begin{array}{cc} \cosh (\omega t) + \frac{R}{2 \omega} \sinh( \omega t)& -\frac{\sigma}{\omega} \sinh(\omega t)\\ \frac{\sigma}{\omega} \sinh(\omega t) & \cosh(\omega t) - \frac{R}{2\omega} \sinh(\omega t)  \end{array} \right\rbrack, $$ Where $\omega = \sqrt{R^2-4 \sigma^2}/2$.  So the idea would be to show that $$ \left\lbrack\begin{array}{cc} \cosh (\omega t) + \frac{R}{2 \omega} \sinh( \omega t)& -\frac{\sigma}{\omega} \sinh(\omega t)\\ \frac{\sigma}{\omega} \sinh(\omega t) & \cosh(\omega t) - \frac{R}{2\omega} \sinh(\omega t)  \end{array} \right\rbrack $$ has norm strictly less than $\exp(R t/2)$. This is pretty much where I am stuck. I attempted to consider if $\omega$ is purely rule or purely imaginary separately, but they were both seemingly dead ends.  I also have used the eigen decomposition of $\mathrm{A}$ but that only tells us that the matrix exponential is eventually (for large enough $t$) strictly less than 1. Any ideas are appreciated.","Let $t,\sigma,R \in \mathbb{R^+}$.  Let  $$ \mathrm{A} = \left\lbrack\begin{array}{cc}0 & -\sigma\\ \sigma &-R \end{array} \right\rbrack$$ want to show that the induced $\mathcal{l}_2$ norm of $\exp(\mathrm{A} t)$ is strictly less than one. This is what I have tried. $$\exp(\mathrm{A} t) =  \exp(-R t/2) \left\lbrack\begin{array}{cc} \cosh (\omega t) + \frac{R}{2 \omega} \sinh( \omega t)& -\frac{\sigma}{\omega} \sinh(\omega t)\\ \frac{\sigma}{\omega} \sinh(\omega t) & \cosh(\omega t) - \frac{R}{2\omega} \sinh(\omega t)  \end{array} \right\rbrack, $$ Where $\omega = \sqrt{R^2-4 \sigma^2}/2$.  So the idea would be to show that $$ \left\lbrack\begin{array}{cc} \cosh (\omega t) + \frac{R}{2 \omega} \sinh( \omega t)& -\frac{\sigma}{\omega} \sinh(\omega t)\\ \frac{\sigma}{\omega} \sinh(\omega t) & \cosh(\omega t) - \frac{R}{2\omega} \sinh(\omega t)  \end{array} \right\rbrack $$ has norm strictly less than $\exp(R t/2)$. This is pretty much where I am stuck. I attempted to consider if $\omega$ is purely rule or purely imaginary separately, but they were both seemingly dead ends.  I also have used the eigen decomposition of $\mathrm{A}$ but that only tells us that the matrix exponential is eventually (for large enough $t$) strictly less than 1. Any ideas are appreciated.",,"['matrices', 'normed-spaces', 'matrix-calculus']"
93,Determinant of big matrix,Determinant of big matrix,,"Let: $$M_n:=\left(\begin{array}{ccccc} 1 & 2 & 3 & \dots & n \\ 1^3 & 2^3 & 3^3 & \dots & n^3 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1^{2n-1} & 2^{2n-1} & 3^{2n-1} & \dots & n^{2n-1} \end{array}\right),$$ for $n\in\mathbb N$. How could I compute $\det M_n$? I tried computing the first few and seeing if I saw an apparent pattern to try inducting, but the third one was already too much of a mess and no pattern was evident. This was an exercise in an exam, so there must be a smart way of doing it. Is there? Which is it?","Let: $$M_n:=\left(\begin{array}{ccccc} 1 & 2 & 3 & \dots & n \\ 1^3 & 2^3 & 3^3 & \dots & n^3 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1^{2n-1} & 2^{2n-1} & 3^{2n-1} & \dots & n^{2n-1} \end{array}\right),$$ for $n\in\mathbb N$. How could I compute $\det M_n$? I tried computing the first few and seeing if I saw an apparent pattern to try inducting, but the third one was already too much of a mess and no pattern was evident. This was an exercise in an exam, so there must be a smart way of doing it. Is there? Which is it?",,"['linear-algebra', 'matrices', 'determinant']"
94,Looking for an identity for characteristic polynomial of a matrix to the power of n,Looking for an identity for characteristic polynomial of a matrix to the power of n,,"Is there any identity to help finding the characteristic polynomial of a power of a matrix in terms of its own characteristic polynomial? Let $A\in \mathbb{Z}^{m\times m}$, we have $$p(t)=\text{det}(A-t I)$$ If we define $$q(t)=\text{det}(A^n-t I)\,\,,n\in\mathbb{N}$$ Is there any identity relating $p(t)$ to $q(t)$? If there is no general relation between $p(t)$ and $q(t)$, is there any relation to compute $q(1)$ from $p(1)$?","Is there any identity to help finding the characteristic polynomial of a power of a matrix in terms of its own characteristic polynomial? Let $A\in \mathbb{Z}^{m\times m}$, we have $$p(t)=\text{det}(A-t I)$$ If we define $$q(t)=\text{det}(A^n-t I)\,\,,n\in\mathbb{N}$$ Is there any identity relating $p(t)$ to $q(t)$? If there is no general relation between $p(t)$ and $q(t)$, is there any relation to compute $q(1)$ from $p(1)$?",,"['linear-algebra', 'matrices', 'number-theory', 'determinant']"
95,Tensors in matrix multiplication algorithms,Tensors in matrix multiplication algorithms,,"Fast matrix multiplication algorithms, be it the Winograd and Coppersmith algorithm or any further improvement of it, extensively use tensors. In fact, the entire construction is based on tensor analysis. My question is WHY TENSORS? How do tensors help? The following links provide details of the algorithms: http://www.cs.umd.edu/~gasarch/TOPICS/ramsey/matrixmult.pdf http://issac-symposium.org/2014/tutorials/ISSAC2014_tutorial_handout_LeGall.pdf Thanks!","Fast matrix multiplication algorithms, be it the Winograd and Coppersmith algorithm or any further improvement of it, extensively use tensors. In fact, the entire construction is based on tensor analysis. My question is WHY TENSORS? How do tensors help? The following links provide details of the algorithms: http://www.cs.umd.edu/~gasarch/TOPICS/ramsey/matrixmult.pdf http://issac-symposium.org/2014/tutorials/ISSAC2014_tutorial_handout_LeGall.pdf Thanks!",,"['matrices', 'algorithms', 'computational-complexity', 'tensor-products', 'tensors']"
96,Is the row echelon form of a system of equations unique?,Is the row echelon form of a system of equations unique?,,"I'm talking about the row echelon form, not the reduced row echelon form. If it isn't can you give me some examples?","I'm talking about the row echelon form, not the reduced row echelon form. If it isn't can you give me some examples?",,"['linear-algebra', 'matrices']"
97,Is this matrix invertible?,Is this matrix invertible?,,"I have been working on a proof and am stuck with showing that the below matrix is invertible. I am not interested in the explicit inverse, only showing it has a nonzero determinant as the existence of the inverse is enough for my proof. I have the important stipulation that x is a non root of unity. (Lets work over the complex numbers) \begin{array}{ccccc} x-1 & x^2-1 & x^3-1 & \ldots & x^n-1 \\ x^2-1 & x^4-1 & x^6-1 &\ldots & x^{2n}-1 \\ x^3-1 & x^6-1 & x^9-1 &\ldots & x^{3n}-1\\ \vdots& \ldots& \ldots& \ldots & \vdots \\ x^n-1 & x^{2n}-1 & x^{3n}-1 & \ldots & x^{n\times n}-1    \end{array} I have computed various examples in maple and shown that this has an inverse when x is a non-root of 1 (this seems essential) but I require a general argument. Any assistance would be very helpful.","I have been working on a proof and am stuck with showing that the below matrix is invertible. I am not interested in the explicit inverse, only showing it has a nonzero determinant as the existence of the inverse is enough for my proof. I have the important stipulation that x is a non root of unity. (Lets work over the complex numbers) \begin{array}{ccccc} x-1 & x^2-1 & x^3-1 & \ldots & x^n-1 \\ x^2-1 & x^4-1 & x^6-1 &\ldots & x^{2n}-1 \\ x^3-1 & x^6-1 & x^9-1 &\ldots & x^{3n}-1\\ \vdots& \ldots& \ldots& \ldots & \vdots \\ x^n-1 & x^{2n}-1 & x^{3n}-1 & \ldots & x^{n\times n}-1    \end{array} I have computed various examples in maple and shown that this has an inverse when x is a non-root of 1 (this seems essential) but I require a general argument. Any assistance would be very helpful.",,"['matrices', 'determinant', 'inverse', 'roots-of-unity']"
98,Why is this not a valid proof?,Why is this not a valid proof?,,"A thread I saw recently has led me to believe that this is not a valid proof of the fact that for matrices $A$ and $B$, $AB=I\implies BA=I$. Suppose $AB=I$. Then $$A^{-1}AB=A^{-1}I$$ $$B=A^{-1}$$ $$BA=A^{-1}A$$ $$BA=I$$ what step is wrong in this? I assume $A$ has an inverse because $\det A\det B=\det AB=\det I=1$, so $\det A\neq 0$.","A thread I saw recently has led me to believe that this is not a valid proof of the fact that for matrices $A$ and $B$, $AB=I\implies BA=I$. Suppose $AB=I$. Then $$A^{-1}AB=A^{-1}I$$ $$B=A^{-1}$$ $$BA=A^{-1}A$$ $$BA=I$$ what step is wrong in this? I assume $A$ has an inverse because $\det A\det B=\det AB=\det I=1$, so $\det A\neq 0$.",,"['linear-algebra', 'matrices']"
99,Inverse of the Toeplitz matrix,Inverse of the Toeplitz matrix,,"I am working on the inverse of the sum of an identity matrix and a Toepltz matrix, and trying to find the formula for the (1,1) element of the inverse. For example, Assume $c$ is a nonzero constant, and let $$A_{t}=cI_{t}+\Omega_{t},$$ where, for $t=4$, $$ \Omega _{t}=\left( \begin{array}{cccc} 1 & \rho  & \rho ^{2} & \rho ^{3} \\ \rho  & 1 & \rho  & \rho ^{2} \\ \rho ^{2} & \rho  & 1 & \rho  \\ \rho ^{3} & \rho ^{2} & \rho  & 1% \end{array}% \right), \quad \text{with} \quad \left\vert \rho \right\vert <1. $$ In general, the $(i,j)$-th element of $\Omega _{t}$ is given by $\rho ^{|i-j|}$. Is there any way to find a general formula for the $(1,1)$-th element of $A_{t}^{-1}$ for different $t,$ say, $t=2,3,4,\ldots ?$ Many thanks!","I am working on the inverse of the sum of an identity matrix and a Toepltz matrix, and trying to find the formula for the (1,1) element of the inverse. For example, Assume $c$ is a nonzero constant, and let $$A_{t}=cI_{t}+\Omega_{t},$$ where, for $t=4$, $$ \Omega _{t}=\left( \begin{array}{cccc} 1 & \rho  & \rho ^{2} & \rho ^{3} \\ \rho  & 1 & \rho  & \rho ^{2} \\ \rho ^{2} & \rho  & 1 & \rho  \\ \rho ^{3} & \rho ^{2} & \rho  & 1% \end{array}% \right), \quad \text{with} \quad \left\vert \rho \right\vert <1. $$ In general, the $(i,j)$-th element of $\Omega _{t}$ is given by $\rho ^{|i-j|}$. Is there any way to find a general formula for the $(1,1)$-th element of $A_{t}^{-1}$ for different $t,$ say, $t=2,3,4,\ldots ?$ Many thanks!",,"['matrices', 'inverse']"
