,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Determining Jordan canonical form(JCF) of an operator given by complex differentiation.,Determining Jordan canonical form(JCF) of an operator given by complex differentiation.,,"Let $W$ be the subspace of $\Bbb C$ linear combination of the following functions: $$f_1(z)=\sin z,\qquad f_2(z)=\cos z,\qquad f_3(z)=\sin2z,\qquad f_4(z)=\cos2z.$$ Let $T$ be the linear opeartor on $W$ given by complex differentition.Which of the following statements are true? $1$ .Dimension of $W$ is $3$ . $2$ .The span of $f_1$ and $f_2$ is a Jordan block of $T$ . $3$ . $T$ has two Jordan blocks. $4$ . $T$ has four Jordan blocks. Since $f_1(z)=\sin z$ , $f_2(z)=\cos z$ , $f_3(z)=\sin2z$ and $f_4(z)=\cos2z$ are linearly independent,hence dimension of $W$ is $4$ .So option (1) is incorrect. Also $W$ is $T$ invariant (as $f'_1(z)=\cos z$ , $f'_2(z)=-\sin z$ , $f'_3(z)=2\cos2z$ , $f'_4(z)=-2\sin2z$ ) Now for other options one should find Jordan canonical form of $T$ and for Jordan canonical form one must have eigenvalues but I am unable to find eigenvalues of $T$ .","Let be the subspace of linear combination of the following functions: Let be the linear opeartor on given by complex differentition.Which of the following statements are true? .Dimension of is . .The span of and is a Jordan block of . . has two Jordan blocks. . has four Jordan blocks. Since , , and are linearly independent,hence dimension of is .So option (1) is incorrect. Also is invariant (as , , , ) Now for other options one should find Jordan canonical form of and for Jordan canonical form one must have eigenvalues but I am unable to find eigenvalues of .","W \Bbb C f_1(z)=\sin z,\qquad f_2(z)=\cos z,\qquad f_3(z)=\sin2z,\qquad f_4(z)=\cos2z. T W 1 W 3 2 f_1 f_2 T 3 T 4 T f_1(z)=\sin z f_2(z)=\cos z f_3(z)=\sin2z f_4(z)=\cos2z W 4 W T f'_1(z)=\cos z f'_2(z)=-\sin z f'_3(z)=2\cos2z f'_4(z)=-2\sin2z T T","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'jordan-normal-form']"
1,How to factor $(x-y)^5 + (y-z)^5 + (z-x)^5$,How to factor,(x-y)^5 + (y-z)^5 + (z-x)^5,"We see that polynomial is cyclic. For $x=y$ , $P(y)=0$ so $x-y$ factors polynomial. Because it is cyclic we instantly know other two factor $(y-z)$ and $(z-x)$ . $P(x,y,z)=(x-y)(y-z)(z-x)*N(x)$ Because we know that P(x,y,z) has the highest 4th degree of $x,y$ and $z$ it must me that $N(x)$ is the second-degree polynomial. I just don't know how the $N(x)$ would look like in terms of its coefficients. If the coefficient for all members would be the same as it looks like in the solution it would be easy to solve this, however I can't logically comprehend why we can claim that given that P(x,y,z) has members with coefficients 10,5,1.","We see that polynomial is cyclic. For , so factors polynomial. Because it is cyclic we instantly know other two factor and . Because we know that P(x,y,z) has the highest 4th degree of and it must me that is the second-degree polynomial. I just don't know how the would look like in terms of its coefficients. If the coefficient for all members would be the same as it looks like in the solution it would be easy to solve this, however I can't logically comprehend why we can claim that given that P(x,y,z) has members with coefficients 10,5,1.","x=y P(y)=0 x-y (y-z) (z-x) P(x,y,z)=(x-y)(y-z)(z-x)*N(x) x,y z N(x) N(x)","['linear-algebra', 'polynomials']"
2,"For orthogonal matrices $A$ and $B$, prove $\det(A^{t}B - B^{t}A)=\det(A+B)\det(A-B)$","For orthogonal matrices  and , prove",A B \det(A^{t}B - B^{t}A)=\det(A+B)\det(A-B),"I can't prove this formula: $$ \det(A^{t}B - B^{t}A)=\det(A+B)\det(A-B) $$ I tried using fact that $A^{t}A = I$ (similarly for $B$ ): $$ A^{t}B-B^{t}A=A^{t}B-B^{t}A + A^{t}A - B^{t}B=A^{t}(A+B)+B^{t}(A - B) $$ I don't understand how I should continue, and don't see an alternative method.","I can't prove this formula: I tried using fact that (similarly for ): I don't understand how I should continue, and don't see an alternative method.","
\det(A^{t}B - B^{t}A)=\det(A+B)\det(A-B)
 A^{t}A = I B 
A^{t}B-B^{t}A=A^{t}B-B^{t}A + A^{t}A - B^{t}B=A^{t}(A+B)+B^{t}(A - B)
","['linear-algebra', 'determinant', 'orthogonal-matrices']"
3,"For any $A\in M_n(\mathbb{R})$, there is a $B$ s.t. $\operatorname{rank}(A)+\operatorname{rank}(B)=n$ and $AB=0$.","For any , there is a  s.t.  and .",A\in M_n(\mathbb{R}) B \operatorname{rank}(A)+\operatorname{rank}(B)=n AB=0,"This is a qualifying exam question which I have little experience approaching. This is certainly an exercise in linear algebra so I would like to obtain a method in solving similar questions. Let $M_n(\mathbb{R})$ be the ring of $n\times n$ matrices over $\mathbb{R}$ . If $A\in M_n(\mathbb{R})$ , then there exists a $B\in M_n(\mathbb{R})$ s.t. $\operatorname{rank}(A)+\operatorname{rank}(B)=n$ and $AB=0$ . Also does there exist a $B$ with the same properties such that $BA=0$ as well? As far as I can tell, there is a similar question , but the question is over $\mathbb{C}$ coefficients so this is not a repeat question. My approach is as follows. Since we work over $\mathbb{R}$ , we should focus on the Rational Canonical Form. So write $A$ as a direct sum of companion matrices. The rank is preserved under similarity classes. The rank of $A$ is the sum of the ranks of the companion matrices. So the problem reduces to proving the result for a companion matrix. Let $A=\begin{pmatrix} 0 & \dots & \dots & -a_1\\ 1 & 0 & \dots &-a_2\\ \ddots &\ddots & \ddots & \ddots \\ 0 & \dots & 1 &-a_{n-1} \end{pmatrix}$ be a companion matrix. This has either rank $n-1$ or $n-2$ . Then there should be choices of $B$ which work. Of course, this would be a tedious computation. My question. Is there a better approach to this question? For example, some way to relate $M_n(\mathbb{R})$ to $M_n(\mathbb{C})$ so that I can use Jordan canonical forms instead?","This is a qualifying exam question which I have little experience approaching. This is certainly an exercise in linear algebra so I would like to obtain a method in solving similar questions. Let be the ring of matrices over . If , then there exists a s.t. and . Also does there exist a with the same properties such that as well? As far as I can tell, there is a similar question , but the question is over coefficients so this is not a repeat question. My approach is as follows. Since we work over , we should focus on the Rational Canonical Form. So write as a direct sum of companion matrices. The rank is preserved under similarity classes. The rank of is the sum of the ranks of the companion matrices. So the problem reduces to proving the result for a companion matrix. Let be a companion matrix. This has either rank or . Then there should be choices of which work. Of course, this would be a tedious computation. My question. Is there a better approach to this question? For example, some way to relate to so that I can use Jordan canonical forms instead?","M_n(\mathbb{R}) n\times n \mathbb{R} A\in M_n(\mathbb{R}) B\in M_n(\mathbb{R}) \operatorname{rank}(A)+\operatorname{rank}(B)=n AB=0 B BA=0 \mathbb{C} \mathbb{R} A A A=\begin{pmatrix}
0 & \dots & \dots & -a_1\\
1 & 0 & \dots &-a_2\\
\ddots &\ddots & \ddots & \ddots \\
0 & \dots & 1 &-a_{n-1}
\end{pmatrix} n-1 n-2 B M_n(\mathbb{R}) M_n(\mathbb{C})","['linear-algebra', 'matrices', 'matrix-rank']"
4,If $T$ is self-adjoint then $||T^n|| = ||T||^n$,If  is self-adjoint then,T ||T^n|| = ||T||^n,"Let $T$ be a bounded linear operator on a Hilbert space $H$ . If $T$ is self-adjoint then $||T^n|| = ||T||^n$ . It is easy to see that $||T||^n$ is an upper bound. Indeed, there exists a $C>0$ such that, $||T^nx|| \leq C||x||$ . Then $||T^n|| \leq ||T||^n$ . To prove the other direction, \begin{align*}||T||^2  = \sup_{||x|| =1}\{||Tx||^2\} & = \sup_{||x|| =1}\{\langle Tx,Tx\rangle\}= \sup_{||x|| =1}\{\langle x,TTx\rangle\} \\ & \leq \sup_{||x|| =1}\{||x||^2||T^2||\} = ||T^2||.\end{align*} But for the case $n = 3$ (the induction step), it seems that trick I've used doesn't work. Since \begin{align*} ||T||^3 = \sup_{||x|| =1}\{||Tx||^3\} & = \sup_{||x|| =1}\{\sqrt{\langle Tx,Tx\rangle}^3\}= \sup_{||x|| =1}\{\sqrt{\langle x,TTx\rangle}^3\} \\ & \leq \sup_{||x|| =1}\{\sqrt{||x||^2||T^2||}^3\} = ||T^2||^{\frac{3}{2}}. \end{align*} How to show the induction step? I read some comments using the spectral theorem but I have not learnt it yet. Is there another proof using just properties of being self-adjoint?","Let be a bounded linear operator on a Hilbert space . If is self-adjoint then . It is easy to see that is an upper bound. Indeed, there exists a such that, . Then . To prove the other direction, But for the case (the induction step), it seems that trick I've used doesn't work. Since How to show the induction step? I read some comments using the spectral theorem but I have not learnt it yet. Is there another proof using just properties of being self-adjoint?","T H T ||T^n|| = ||T||^n ||T||^n C>0 ||T^nx|| \leq C||x|| ||T^n|| \leq ||T||^n \begin{align*}||T||^2  = \sup_{||x|| =1}\{||Tx||^2\} & = \sup_{||x|| =1}\{\langle Tx,Tx\rangle\}= \sup_{||x|| =1}\{\langle x,TTx\rangle\} \\ & \leq \sup_{||x|| =1}\{||x||^2||T^2||\} = ||T^2||.\end{align*} n = 3 \begin{align*}
||T||^3 = \sup_{||x|| =1}\{||Tx||^3\} & = \sup_{||x|| =1}\{\sqrt{\langle Tx,Tx\rangle}^3\}= \sup_{||x|| =1}\{\sqrt{\langle x,TTx\rangle}^3\} \\ & \leq \sup_{||x|| =1}\{\sqrt{||x||^2||T^2||}^3\} = ||T^2||^{\frac{3}{2}}.
\end{align*}","['real-analysis', 'linear-algebra', 'functional-analysis']"
5,What do we actually rotate with rotational matrices,What do we actually rotate with rotational matrices,,"We just learned about rotational matrices in a more theoretical context, but we only ""glanced"" at them. My question: What exactly can a matrix rotate (vectors, other matrices, etc...?) and I still don't really get it how it actually rotates another object. Can somebody please give me a simple explanation (it can also be rigorous).","We just learned about rotational matrices in a more theoretical context, but we only ""glanced"" at them. My question: What exactly can a matrix rotate (vectors, other matrices, etc...?) and I still don't really get it how it actually rotates another object. Can somebody please give me a simple explanation (it can also be rigorous).",,"['real-analysis', 'linear-algebra', 'abstract-algebra']"
6,"Prove that if $A$ is a positive definite matrix, then $A$ is non-singular.","Prove that if  is a positive definite matrix, then  is non-singular.",A A,"First, going through what it means to be positive definite and non-singular: Positive definite implies $\det(A) > 0$ All eigenvalues of $A$ are positive, and so $0$ is not an eigenvalue of $A$ Nonsingular implies $\det(A) \neq 0$ All eigenvalues of A are nonzero The product of eigenvalues of $A$ $= \det(A)$ It seems as though these two characterizations go hand in hand, though I assume negative eigenvalues could form a non-singular matrix but not a positive definite matrix. Can this be proven directly, or do I need to figure out how to prove by contradiction? Thanks!","First, going through what it means to be positive definite and non-singular: Positive definite implies All eigenvalues of are positive, and so is not an eigenvalue of Nonsingular implies All eigenvalues of A are nonzero The product of eigenvalues of It seems as though these two characterizations go hand in hand, though I assume negative eigenvalues could form a non-singular matrix but not a positive definite matrix. Can this be proven directly, or do I need to figure out how to prove by contradiction? Thanks!",\det(A) > 0 A 0 A \det(A) \neq 0 A = \det(A),"['linear-algebra', 'matrices', 'proof-writing', 'positive-definite']"
7,Why do we need a zero vector space?,Why do we need a zero vector space?,,"Reading through some lecture notes, I found out the first example of a vector space is the zero vector space, which contains only the zero vector. I understand that it meets all the axioms of a vector space, but why do we need it? Why bother mentioning that it is a vector space? Particularly, in what way is it useful while answering questions about Linear Algebra?","Reading through some lecture notes, I found out the first example of a vector space is the zero vector space, which contains only the zero vector. I understand that it meets all the axioms of a vector space, but why do we need it? Why bother mentioning that it is a vector space? Particularly, in what way is it useful while answering questions about Linear Algebra?",,['linear-algebra']
8,Proving product of two column stochastic matrices is column stochastic (Proof verification),Proving product of two column stochastic matrices is column stochastic (Proof verification),,"For a matrix to be column stochastic we know $\sum_{i=1}^nA_{ij}=1$ for each column $j\in\{1,\ldots,n\}$. We have $$\sum_{j=1}^n (AB)_{ji} = \sum_{j=1}^n \sum_{k=1}^n A_{jk}B_{ki} = \sum_{j=1}^nA_{jk} \sum_{k=1}^nB_{ki} = 1*1 = 1  $$","For a matrix to be column stochastic we know $\sum_{i=1}^nA_{ij}=1$ for each column $j\in\{1,\ldots,n\}$. We have $$\sum_{j=1}^n (AB)_{ji} = \sum_{j=1}^n \sum_{k=1}^n A_{jk}B_{ki} = \sum_{j=1}^nA_{jk} \sum_{k=1}^nB_{ki} = 1*1 = 1  $$",,"['linear-algebra', 'proof-verification', 'proof-writing', 'markov-chains', 'stochastic-matrices']"
9,Some point in a line $Ax+By+C=0$,Some point in a line,Ax+By+C=0,"Given a line in a 2D space, defined by equation $Ax+By+C=0$, is it possible to find a point of it without assume that A or B are different of 0 ? That is, usually the method to find some point is ""assume $A \ne 0$, if we fix $y=0$ the solution of the equation gives that point $(-C/A,0)$ is a point of the line, otherwise $B \ne 0$ and ...· But it is possible any other method without split the problem in two ? In other words, is it possible to find an expression for some point (any one) of the line that doesn't contains a division by A, B or C or by any other term that can be zero in some cases ? The question could be expressed in another way: given a line $Ax+By+C=0$, give an expression of the same line in vector/parametric form that is valid for any value of A, B, C. The direction vector is easy to find, (-B,A), the remainder target is to find the expression of some point.","Given a line in a 2D space, defined by equation $Ax+By+C=0$, is it possible to find a point of it without assume that A or B are different of 0 ? That is, usually the method to find some point is ""assume $A \ne 0$, if we fix $y=0$ the solution of the equation gives that point $(-C/A,0)$ is a point of the line, otherwise $B \ne 0$ and ...· But it is possible any other method without split the problem in two ? In other words, is it possible to find an expression for some point (any one) of the line that doesn't contains a division by A, B or C or by any other term that can be zero in some cases ? The question could be expressed in another way: given a line $Ax+By+C=0$, give an expression of the same line in vector/parametric form that is valid for any value of A, B, C. The direction vector is easy to find, (-B,A), the remainder target is to find the expression of some point.",,"['linear-algebra', 'analytic-geometry']"
10,Permute any two entries in a $n\times n$ matrix.,Permute any two entries in a  matrix.,n\times n,"Let $A$ be an $8 \times 8$ matrix with integer coefficients. I want to permute two entries in $A$, any two entries as needed: In general, for any two entries $a_j,b_k$ in the matrix is it possible to do this with some matrix $B$ dependent on $a_j,b_k$? I know about permutation matrices, but they only permute entire rows and columns not individual entries. Edit 1: For example: Say I want to permute $x_{12}$ with $x_{33}$, I want a matrix $B$ suh that : $$ \begin{bmatrix}     x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\     x_{31} & x_{32} & x_{33} & \dots  & x_{3n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nn} \end{bmatrix}B= \begin{bmatrix}     x_{11} & x_{33} & x_{13} & \dots  & x_{1n} \\     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\     x_{31} & x_{32} & x_{12} & \dots  & x_{3n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nn} \end{bmatrix}$$ Thanks. P.s[Moderators]: Edit tags as appropriate, I added as many as made sense to me.","Let $A$ be an $8 \times 8$ matrix with integer coefficients. I want to permute two entries in $A$, any two entries as needed: In general, for any two entries $a_j,b_k$ in the matrix is it possible to do this with some matrix $B$ dependent on $a_j,b_k$? I know about permutation matrices, but they only permute entire rows and columns not individual entries. Edit 1: For example: Say I want to permute $x_{12}$ with $x_{33}$, I want a matrix $B$ suh that : $$ \begin{bmatrix}     x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\     x_{31} & x_{32} & x_{33} & \dots  & x_{3n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nn} \end{bmatrix}B= \begin{bmatrix}     x_{11} & x_{33} & x_{13} & \dots  & x_{1n} \\     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\     x_{31} & x_{32} & x_{12} & \dots  & x_{3n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nn} \end{bmatrix}$$ Thanks. P.s[Moderators]: Edit tags as appropriate, I added as many as made sense to me.",,"['linear-algebra', 'matrices', 'permutations', 'linear-transformations']"
11,Least additive modification to a matrix to make it invertible.,Least additive modification to a matrix to make it invertible.,,"Would it be well defined? Say I have a matrix $\bf A$ which I know is not invertible. Is there some way to define the ""minimal added contribution"" $\bf B$ that will make it invertible? Own work: I've been thinking maybe something like this will make it well defined in some sense: $$\min_{\bf B,P}\{\epsilon_1\|(\bf A+ B)P-I\|+ \epsilon_2\|B\|\}$$ Where $\bf P$ is the candidate inverse. But I doubt it would be very well behaved since $\bf B$ and $\bf P$ are multiplied together.","Would it be well defined? Say I have a matrix $\bf A$ which I know is not invertible. Is there some way to define the ""minimal added contribution"" $\bf B$ that will make it invertible? Own work: I've been thinking maybe something like this will make it well defined in some sense: $$\min_{\bf B,P}\{\epsilon_1\|(\bf A+ B)P-I\|+ \epsilon_2\|B\|\}$$ Where $\bf P$ is the candidate inverse. But I doubt it would be very well behaved since $\bf B$ and $\bf P$ are multiplied together.",,"['linear-algebra', 'matrices', 'optimization', 'soft-question', 'inverse']"
12,"If $T^2 = I$, how one can proof that $V = W \oplus U$?","If , how one can proof that ?",T^2 = I V = W \oplus U,"Let $K$ be a field with cardinality different of $2$ and let $V$ be a $K-$vector space. Let $T: V \to V$ be a linear operator such that $T^2 = I$. Let $W = \{ v \in V: \, Tv = v \}$  and $U = \{ v \in V : Tv = -v \}$. I'm trying to proof that $V = W \oplus U$. My attempt: If $x \in W \cap U$, $Tx = x$ e $Tx = -x$, i.e $x = -x$, hence $x = 0$. Then, $W \cap U = \{0\}$. If $x \in V$, we have that $x = x - Tx + Tx$ and $x - Tx \in U$, since $$ T(x - Tx) = Tx - T^2 x = Tx -x = - (x-Tx). $$ However, $Tx$ which is not necessarily in $W$. Help?","Let $K$ be a field with cardinality different of $2$ and let $V$ be a $K-$vector space. Let $T: V \to V$ be a linear operator such that $T^2 = I$. Let $W = \{ v \in V: \, Tv = v \}$  and $U = \{ v \in V : Tv = -v \}$. I'm trying to proof that $V = W \oplus U$. My attempt: If $x \in W \cap U$, $Tx = x$ e $Tx = -x$, i.e $x = -x$, hence $x = 0$. Then, $W \cap U = \{0\}$. If $x \in V$, we have that $x = x - Tx + Tx$ and $x - Tx \in U$, since $$ T(x - Tx) = Tx - T^2 x = Tx -x = - (x-Tx). $$ However, $Tx$ which is not necessarily in $W$. Help?",,['linear-algebra']
13,What is the difference between orthogonal subspaces and orthogonal complements?,What is the difference between orthogonal subspaces and orthogonal complements?,,"I am learning linear algebra through professor Giblert Strang's lectures on MIT OCW. The professor says that the row space and the null space of a matrix are orthogonal subspaces . This I can follow, since any vector in the nullspace takes any linear combination of the rows to zero. He then says that the row space and null space are more than just orthogonal subspaces, they are orthogonal complements , Because The ""nullspace contains all the vectors that are perpendicular to the row space"" , and vice versa. Consider: $$A= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}=0 $$ A is rank 2, Dimension of nullspace of A=1 Null space: $$X=c \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$ Graphically, the row space of A is the x-y plane, and the null space is the z-axis. It's easy to see that the nullspace does not contain all the vectors that are perpendicular to a vector in the row space. If we look at $[1 ~0~ 0]$, it has the entire y-z plane perpendicular to it, not just the z-axis. All I can see is two orthogonal subspaces. I do not understand what additional property has earned these subspaces the term orthogonal complements . Refer: Lecture Video (from 31:16 to 33:00) Lecture Notes Page 2 paragraph 1","I am learning linear algebra through professor Giblert Strang's lectures on MIT OCW. The professor says that the row space and the null space of a matrix are orthogonal subspaces . This I can follow, since any vector in the nullspace takes any linear combination of the rows to zero. He then says that the row space and null space are more than just orthogonal subspaces, they are orthogonal complements , Because The ""nullspace contains all the vectors that are perpendicular to the row space"" , and vice versa. Consider: $$A= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}=0 $$ A is rank 2, Dimension of nullspace of A=1 Null space: $$X=c \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$ Graphically, the row space of A is the x-y plane, and the null space is the z-axis. It's easy to see that the nullspace does not contain all the vectors that are perpendicular to a vector in the row space. If we look at $[1 ~0~ 0]$, it has the entire y-z plane perpendicular to it, not just the z-axis. All I can see is two orthogonal subspaces. I do not understand what additional property has earned these subspaces the term orthogonal complements . Refer: Lecture Video (from 31:16 to 33:00) Lecture Notes Page 2 paragraph 1",,['linear-algebra']
14,"What really is the difference between $\mathbf{v}=(2,3) $ and $\mathbf{u}=(2,3,0)$?",What really is the difference between  and ?,"\mathbf{v}=(2,3)  \mathbf{u}=(2,3,0)","I used to believe that there is no difference between $\mathbf{v}=(2,3)$ which is a vector lying in $xy$ plane and $\mathbf{u}=(2,3,0)$ which is a three dimensional vector but still lying in $xy$ plane. So, for me both the vectors $\mathbf{v}$ and  $\mathbf{u}$ were same. The analogy I used was that both the tail and head coincide for $\mathbf{v}$ and $\mathbf{u}$. But on the second page of the textbook I am using as reference (Gilbert Strang-Introduction to linear algebra), I found something different . The author says, The vector $(x,y)$ in a plane is different from vector $(x,y,0)$ in $3-$space . That's it. Since this statement is not well explained there so I am here. Hoping for help. Since I am beginning the Linear algebra course so Please show some tolerance. Thanks.","I used to believe that there is no difference between $\mathbf{v}=(2,3)$ which is a vector lying in $xy$ plane and $\mathbf{u}=(2,3,0)$ which is a three dimensional vector but still lying in $xy$ plane. So, for me both the vectors $\mathbf{v}$ and  $\mathbf{u}$ were same. The analogy I used was that both the tail and head coincide for $\mathbf{v}$ and $\mathbf{u}$. But on the second page of the textbook I am using as reference (Gilbert Strang-Introduction to linear algebra), I found something different . The author says, The vector $(x,y)$ in a plane is different from vector $(x,y,0)$ in $3-$space . That's it. Since this statement is not well explained there so I am here. Hoping for help. Since I am beginning the Linear algebra course so Please show some tolerance. Thanks.",,['linear-algebra']
15,How Hahn-Banach theorem implies that the dual space is non-trivial?,How Hahn-Banach theorem implies that the dual space is non-trivial?,,Why does the theorem of Hahn-Banach implies that the dual space is not empty ($X^*\neq\emptyset)$ ? Is there an important corollary which I've missed ?,Why does the theorem of Hahn-Banach implies that the dual space is not empty ($X^*\neq\emptyset)$ ? Is there an important corollary which I've missed ?,,"['real-analysis', 'linear-algebra', 'functional-analysis', 'normed-spaces', 'dual-spaces']"
16,Polynomial form of $\det(A+xB)$,Polynomial form of,\det(A+xB),"Let $A$ and $B$ be two $2 \times 2$ matrices with integer entries. Prove that $\det(A+xB)$ is an integer polynomial of the form $$P(x) = \det(A+xB) = \det(B)x^2+mx+\det(A).$$ I tried expanding the determinant of $\det(A+xB)$ for two arbitrary matrices, but it got computational. Is there another way?","Let $A$ and $B$ be two $2 \times 2$ matrices with integer entries. Prove that $\det(A+xB)$ is an integer polynomial of the form $$P(x) = \det(A+xB) = \det(B)x^2+mx+\det(A).$$ I tried expanding the determinant of $\det(A+xB)$ for two arbitrary matrices, but it got computational. Is there another way?",,"['linear-algebra', 'determinant']"
17,Necessary and sufficient condition for equality of two tensor products,Necessary and sufficient condition for equality of two tensor products,,"Let $v_1, v_2 \in V; w_1, w_2 \in W$. What is necessary and sufficient condition for equality $v_1 \otimes w_1 = v_2 \otimes w_2$ in $V \otimes W$?","Let $v_1, v_2 \in V; w_1, w_2 \in W$. What is necessary and sufficient condition for equality $v_1 \otimes w_1 = v_2 \otimes w_2$ in $V \otimes W$?",,"['linear-algebra', 'vector-spaces', 'tensor-products']"
18,Is there a matrix $A$ such that: $A^4=\begin{pmatrix}0 & 2 & -1 & 1\\ 0 & 0 & 3 &1\\ 0 & 0& 0 & 4\\ 0 & 0 & 0 & 0 \\ \end{pmatrix} ~?$,Is there a matrix  such that:,A A^4=\begin{pmatrix}0 & 2 & -1 & 1\\ 0 & 0 & 3 &1\\ 0 & 0& 0 & 4\\ 0 & 0 & 0 & 0 \\ \end{pmatrix} ~?,Is there a matrix $A\in \mathbb{C}^{4\times 4}$ such that: $$A^4=\left(\begin{array}{cccc} 0 & 2 & -1 & 1\\ 0 &  0 & 3 &1\\ 0 & 0& 0 & 4\\ 0 & 0 & 0 & 0 \\ \end{array} \right) ~?$$ Any hint is appreciated. Thanks a lot!,Is there a matrix $A\in \mathbb{C}^{4\times 4}$ such that: $$A^4=\left(\begin{array}{cccc} 0 & 2 & -1 & 1\\ 0 &  0 & 3 &1\\ 0 & 0& 0 & 4\\ 0 & 0 & 0 & 0 \\ \end{array} \right) ~?$$ Any hint is appreciated. Thanks a lot!,,"['linear-algebra', 'matrices']"
19,Find the image of a linear transformation.,Find the image of a linear transformation.,,"I need to find the $Im(T)$, where $T:\mathbb{R^2} \rightarrow \mathbb{R^3}; T(x_1,x_2) = (x_1+x_2,x_1-x_2,3x_2)$. I know that $$Im(T) = \{y \in \mathbb{R^3}| \exists x \in \mathbb{R^2} \  T(x) = y\}$$ So I need to solve the system: $$\left\{\begin{matrix} x_1+x_2=y_1\\  x_1-x_2=y_2\\  3x_2=y_3 \end{matrix}\right.$$ And I get the solution $x_1 = y_1 - \frac{y_3}{3}\ and\ x_2 =\frac{y_3}{3}$. My problem is that if I try to find $x_1$ and $x_2$ for the vector $y = (1,2,3)$  I get, for example; $x_1 = 0$ and $x_2 = 1$. But $T(0,1)$ is not $(1,2,3)$","I need to find the $Im(T)$, where $T:\mathbb{R^2} \rightarrow \mathbb{R^3}; T(x_1,x_2) = (x_1+x_2,x_1-x_2,3x_2)$. I know that $$Im(T) = \{y \in \mathbb{R^3}| \exists x \in \mathbb{R^2} \  T(x) = y\}$$ So I need to solve the system: $$\left\{\begin{matrix} x_1+x_2=y_1\\  x_1-x_2=y_2\\  3x_2=y_3 \end{matrix}\right.$$ And I get the solution $x_1 = y_1 - \frac{y_3}{3}\ and\ x_2 =\frac{y_3}{3}$. My problem is that if I try to find $x_1$ and $x_2$ for the vector $y = (1,2,3)$  I get, for example; $x_1 = 0$ and $x_2 = 1$. But $T(0,1)$ is not $(1,2,3)$",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
20,Does this operation exist? What's its name?,Does this operation exist? What's its name?,,"I need to do something like this $$ \begin{bmatrix} A \\ B \\ C \\ \end{bmatrix} \begin{bmatrix} x_1 & x_2 & \cdots & x_n \\ y_1 & y_2 & \cdots & y_n \\ z_1 & z_2 & \cdots & z_n \\ \end{bmatrix} = \begin{bmatrix} A x_1 & A x_2 & \cdots & A x_n \\ B y_1 & B y_2 & \cdots & B y_n \\ C z_1 & C z_2 & \cdots & C z_n \\ \end{bmatrix} $$ You get the idea. I want to know if this operation already has a name, in order to see if my linear algebra library already supports it.","I need to do something like this $$ \begin{bmatrix} A \\ B \\ C \\ \end{bmatrix} \begin{bmatrix} x_1 & x_2 & \cdots & x_n \\ y_1 & y_2 & \cdots & y_n \\ z_1 & z_2 & \cdots & z_n \\ \end{bmatrix} = \begin{bmatrix} A x_1 & A x_2 & \cdots & A x_n \\ B y_1 & B y_2 & \cdots & B y_n \\ C z_1 & C z_2 & \cdots & C z_n \\ \end{bmatrix} $$ You get the idea. I want to know if this operation already has a name, in order to see if my linear algebra library already supports it.",,"['linear-algebra', 'matrices', 'math-software']"
21,Is the matrix filled with the areas of pairwise intersections of disks in a plane always positive semidefinite?,Is the matrix filled with the areas of pairwise intersections of disks in a plane always positive semidefinite?,,"Consider disks $s_1, \cdots, s_n$ in the plane and let $a_{ij}$ be the area of $s_i\cap s_j$. Is it true that for any real numbers $x_1,\cdots, x_n$ we have  $$ \sum_{i,j=1}^n x_ix_j a_{ij} \geq 0$$ Equivalent formulation: one can put $a_{ij}$ into a matrix $A$ and ask whether it is positive semidefinite. For $n=2$ this is true since  $$a_{12}^2\le \min(a_{11},a_{22})^2 \le a_{11}a_{22} $$","Consider disks $s_1, \cdots, s_n$ in the plane and let $a_{ij}$ be the area of $s_i\cap s_j$. Is it true that for any real numbers $x_1,\cdots, x_n$ we have  $$ \sum_{i,j=1}^n x_ix_j a_{ij} \geq 0$$ Equivalent formulation: one can put $a_{ij}$ into a matrix $A$ and ask whether it is positive semidefinite. For $n=2$ this is true since  $$a_{12}^2\le \min(a_{11},a_{22})^2 \le a_{11}a_{22} $$",,"['linear-algebra', 'geometry', 'area', 'bilinear-form', 'positive-definite']"
22,"$A,B \in M_2(\mathbb C)$ be such that $AB-BA=B^2$ ; then is it true that $AB=BA$?",be such that  ; then is it true that ?,"A,B \in M_2(\mathbb C) AB-BA=B^2 AB=BA","Let $A,B \in M_2(\mathbb C)$ be such that $AB-BA=B^2$. Then is it true that $AB=BA$ ? If we can show $\mathrm{tr}(B)=\det (B)=0$, then we are done by using $B^2-(\mathrm{tr}(B)) B+\det (B)=0$; but I don't know how to show even that.  Please help. Thanks in advance.","Let $A,B \in M_2(\mathbb C)$ be such that $AB-BA=B^2$. Then is it true that $AB=BA$ ? If we can show $\mathrm{tr}(B)=\det (B)=0$, then we are done by using $B^2-(\mathrm{tr}(B)) B+\det (B)=0$; but I don't know how to show even that.  Please help. Thanks in advance.",,['linear-algebra']
23,If $XYZ=ZXY$ does $e^Xe^Ye^Z=e^Ze^Xe^Y$?,If  does ?,XYZ=ZXY e^Xe^Ye^Z=e^Ze^Xe^Y,"It is well known that if $X,Y$ are commuting matrices, then their exponential commute: $$XY=YX\quad\implies\quad e^Xe^Y=e^Ye^X.$$ Now, I am wondering if the following generalization holds: Question: If $XYZ=ZXY$, does $e^Xe^Ye^Z=e^Ze^Xe^Y$? Note that if $Z$ commutes with both $X$ and $Y$, then it is obvious.","It is well known that if $X,Y$ are commuting matrices, then their exponential commute: $$XY=YX\quad\implies\quad e^Xe^Y=e^Ye^X.$$ Now, I am wondering if the following generalization holds: Question: If $XYZ=ZXY$, does $e^Xe^Ye^Z=e^Ze^Xe^Y$? Note that if $Z$ commutes with both $X$ and $Y$, then it is obvious.",,['linear-algebra']
24,If J is a 101×101 matrix with all entries equal to 1 and let I denote the identity matrix of order 101. Then what is the determinant of J-I?,If J is a 101×101 matrix with all entries equal to 1 and let I denote the identity matrix of order 101. Then what is the determinant of J-I?,,If $J$ is a $101\times 101$ matrix with all entries equal to $1$ and let $I$ denote the identity matrix of order $101$. Then what is the determinant of $J-I$ ?,If $J$ is a $101\times 101$ matrix with all entries equal to $1$ and let $I$ denote the identity matrix of order $101$. Then what is the determinant of $J-I$ ?,,"['linear-algebra', 'matrices', 'determinant']"
25,Cyclic vector implies commuting linear operator is a polynomial [duplicate],Cyclic vector implies commuting linear operator is a polynomial [duplicate],,"This question already has answers here : Prove that $T$ has a cyclic vector iff its minimal and characteristic polynomials are the same (2 answers) Closed 5 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Let $T$ be a linear operator on the finite dimensional vector space $V$. Suppose $T$ has a cyclic vector. Prove that if $U$ is any linear operator which commutes with $T$, then $U$ is a polynomial in $T$.","This question already has answers here : Prove that $T$ has a cyclic vector iff its minimal and characteristic polynomials are the same (2 answers) Closed 5 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Let $T$ be a linear operator on the finite dimensional vector space $V$. Suppose $T$ has a cyclic vector. Prove that if $U$ is any linear operator which commutes with $T$, then $U$ is a polynomial in $T$.",,"['linear-algebra', 'linear-transformations']"
26,How to find a real $3\times3$ matrix that has no cubic root,How to find a real  matrix that has no cubic root,3\times3,"How does one find a real $3\times3$ matrix that does not have a cubic root? If given a matrix without a cubic root, how can one prove that it does not have a cube root?","How does one find a real $3\times3$ matrix that does not have a cubic root? If given a matrix without a cubic root, how can one prove that it does not have a cube root?",,"['linear-algebra', 'matrices']"
27,What is the good way to remember the signs of the rotational matrix?,What is the good way to remember the signs of the rotational matrix?,,"Recall rotational matrix in (x,y) is given by: $R =  \begin{bmatrix}  \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix}$ For the life of me I cannot remember if the top right sin entry has a negative or positive sign, and I have known this matrix since 5 years ago (obviously I am a failure!!). Sure one could always derive from first principle via: But if someone just went up to you and asked you to write down the matrix, what is a good way in the least time write down the matrix and verify that it is correct (with the correct signs)?","Recall rotational matrix in (x,y) is given by: For the life of me I cannot remember if the top right sin entry has a negative or positive sign, and I have known this matrix since 5 years ago (obviously I am a failure!!). Sure one could always derive from first principle via: But if someone just went up to you and asked you to write down the matrix, what is a good way in the least time write down the matrix and verify that it is correct (with the correct signs)?",R =  \begin{bmatrix}  \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix},"['linear-algebra', 'matrices', 'intuition', 'rotations', 'mnemonic']"
28,$A$ and $B$ be two orthogonal matrices with $|A|+|B|=0$. Then prove that $(A+B)$ is singular [duplicate],and  be two orthogonal matrices with . Then prove that  is singular [duplicate],A B |A|+|B|=0 (A+B),"This question already has answers here : Is sum of two orthogonal matrices singular? (2 answers) Closed 8 years ago . Let , $A$ and $B$ be two orthogonal matrices with $|A|+|B|=0$. Then prove that $(A+B)$ is singular. From the relation $|A|+|B|=0$ it is clear that $|AB|=-1$. Also , $|A|=|B|=\pm 1$. But how I find out $|A+B|$ ?","This question already has answers here : Is sum of two orthogonal matrices singular? (2 answers) Closed 8 years ago . Let , $A$ and $B$ be two orthogonal matrices with $|A|+|B|=0$. Then prove that $(A+B)$ is singular. From the relation $|A|+|B|=0$ it is clear that $|AB|=-1$. Also , $|A|=|B|=\pm 1$. But how I find out $|A+B|$ ?",,"['linear-algebra', 'matrices']"
29,Example of a bilinear map whose image is not a subspace,Example of a bilinear map whose image is not a subspace,,"I am looking for an example of a bilinear map $\tau:V \times V \to W$ whose image $im(\tau)=(\tau(u,v):u,v \in V)$ is not a subspace of $W$. I considered the tensor map $\tau:U \times V \to U \otimes V$, since its images consists of all decomposable tensors. I have the idea that if $u \otimes v, u'\otimes v'$ are decomposable tensors, then $u \otimes v + u'\otimes v'$ is not necessarily a decomposable vector, but I am not sure why.","I am looking for an example of a bilinear map $\tau:V \times V \to W$ whose image $im(\tau)=(\tau(u,v):u,v \in V)$ is not a subspace of $W$. I considered the tensor map $\tau:U \times V \to U \otimes V$, since its images consists of all decomposable tensors. I have the idea that if $u \otimes v, u'\otimes v'$ are decomposable tensors, then $u \otimes v + u'\otimes v'$ is not necessarily a decomposable vector, but I am not sure why.",,"['linear-algebra', 'tensor-products']"
30,Eigenvalues of Householder matrix,Eigenvalues of Householder matrix,,What would be the eigenvalues for a Householder matrix defined as: $H = I - 2 u u^T$? Can someone explain it  to me intuitively or with a simple proof?,What would be the eigenvalues for a Householder matrix defined as: $H = I - 2 u u^T$? Can someone explain it  to me intuitively or with a simple proof?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
31,Why people use the Gram-Schmidt process instead of just chosing the standard basis,Why people use the Gram-Schmidt process instead of just chosing the standard basis,,"I really can't find a reason for going through all the work of the Gram-Schmidt method to make a new orthogonal basis $B'$ given an old basis $B$. If I want to change to an orthogonal basis, the most simple solution to me is just the standard basis.","I really can't find a reason for going through all the work of the Gram-Schmidt method to make a new orthogonal basis $B'$ given an old basis $B$. If I want to change to an orthogonal basis, the most simple solution to me is just the standard basis.",,"['linear-algebra', 'vector-spaces', 'orthogonality', 'gram-schmidt']"
32,"examples of linear map $f:V \rightarrow V$, which is injective but not surjective","examples of linear map , which is injective but not surjective",f:V \rightarrow V,"I am trying to find a linear map $f:V \rightarrow V$, which is injective but not surjective. I always thought that if the dimension of the domain and codomain are equal and the map is injective it implies that a map is surjective. Maybe we need an infinite basis of the vector space $V$? What can be an example of that? Thank you!","I am trying to find a linear map $f:V \rightarrow V$, which is injective but not surjective. I always thought that if the dimension of the domain and codomain are equal and the map is injective it implies that a map is surjective. Maybe we need an infinite basis of the vector space $V$? What can be an example of that? Thank you!",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'linear-transformations']"
33,Show that matrix $A$ is NOT diagonalizable.,Show that matrix  is NOT diagonalizable.,A,"Let $A$ be a square matrix $A^2=0$ and $A\neq0$ and show that it is not diagonalizable. I decided to use the sample matrix of $$A = \begin{bmatrix}0 & 1\\0 & 0 \end{bmatrix}$$ which satisfies the conditions above. So my question is: how would I prove this is not diagonalizable. The matrix leads to a eigenvalue of $\lambda=0$ with an algebraic multiplicity of $2$. I know that if the algebraic multiplicity and geometric multiplicity are equal, then it is diagonalizable. But I am kind of stuck from here since when I use $\det(A-0I)=0$, it just leads to get $x_2=0$ but then also that $x_2=t$ so I don't really know what to do. Any help would be appreciated.","Let $A$ be a square matrix $A^2=0$ and $A\neq0$ and show that it is not diagonalizable. I decided to use the sample matrix of $$A = \begin{bmatrix}0 & 1\\0 & 0 \end{bmatrix}$$ which satisfies the conditions above. So my question is: how would I prove this is not diagonalizable. The matrix leads to a eigenvalue of $\lambda=0$ with an algebraic multiplicity of $2$. I know that if the algebraic multiplicity and geometric multiplicity are equal, then it is diagonalizable. But I am kind of stuck from here since when I use $\det(A-0I)=0$, it just leads to get $x_2=0$ but then also that $x_2=t$ so I don't really know what to do. Any help would be appreciated.",,"['linear-algebra', 'matrices', 'diagonalization']"
34,Prove: the sum of two nilpotent and exchangable matrices is nipotent.,Prove: the sum of two nilpotent and exchangable matrices is nipotent.,,"If $A$ and $B$ are two $n\times n$ nilpotent matrices, and they are exchangable: $AB = BA$, it is said that the sum $A+B$ is also nilpotent. Could you pls give me some hint how to prove that?","If $A$ and $B$ are two $n\times n$ nilpotent matrices, and they are exchangable: $AB = BA$, it is said that the sum $A+B$ is also nilpotent. Could you pls give me some hint how to prove that?",,"['linear-algebra', 'matrices']"
35,Solutions of homogeneous linear differential equation form a vector space,Solutions of homogeneous linear differential equation form a vector space,,Show that the solutions of a homogeneous linear differential equation $y''+a(x)y'+b(x)y = 0$ form a vector space. What is its dimension? I understand that the dimension is 2 and that 0 is a solution to the differential equation ($0''+a(x)*0'+b(x)*0=0$). How does one go about proving the other two properties of a vector space: closed under addition and closed under multiplication?,Show that the solutions of a homogeneous linear differential equation $y''+a(x)y'+b(x)y = 0$ form a vector space. What is its dimension? I understand that the dimension is 2 and that 0 is a solution to the differential equation ($0''+a(x)*0'+b(x)*0=0$). How does one go about proving the other two properties of a vector space: closed under addition and closed under multiplication?,,"['linear-algebra', 'ordinary-differential-equations', 'vector-spaces']"
36,Explain why the determinant of $A$ is the index of the subring?,Explain why the determinant of  is the index of the subring?,A,"Let $a$ be an algebraic number, whose minimal polynomial has integral coefficients. Let $K = \Bbb Q(a)$ be an algebraic number field. Let $\mathcal O_K$ be the ring of integers in this algebraic number field. Fix an basis for $\mathcal O_K$ and $\Bbb Z[a]$. Let $A$ be the matrix whose column vectors express the basis elements of $\Bbb Z[a]$ as a $Z$-linear combination of the basis for $\mathcal O_K$. Then $\det(A) = [\mathcal O_K : \Bbb Z[a]]$ I have no idea why this is the case. Can someone explain this to me?","Let $a$ be an algebraic number, whose minimal polynomial has integral coefficients. Let $K = \Bbb Q(a)$ be an algebraic number field. Let $\mathcal O_K$ be the ring of integers in this algebraic number field. Fix an basis for $\mathcal O_K$ and $\Bbb Z[a]$. Let $A$ be the matrix whose column vectors express the basis elements of $\Bbb Z[a]$ as a $Z$-linear combination of the basis for $\mathcal O_K$. Then $\det(A) = [\mathcal O_K : \Bbb Z[a]]$ I have no idea why this is the case. Can someone explain this to me?",,"['linear-algebra', 'algebraic-number-theory']"
37,"If rank$(A)=r$, show that rank$(A^\top A)=r$","If rank, show that rank",(A)=r (A^\top A)=r,"Let $A$ be $m\times n$ matrix with rank $r=\min(m,n)$. How do we show that rank$(A^T A)$ is $r$.","Let $A$ be $m\times n$ matrix with rank $r=\min(m,n)$. How do we show that rank$(A^T A)$ is $r$.",,['linear-algebra']
38,Linear Algebra determinant reduction,Linear Algebra determinant reduction,,"Prove, without expanding, that \begin{vmatrix} 1 &a  &a^2-bc \\  1 &b  &b^2-ca \\  1 &c  &c^2-ab  \end{vmatrix} vanishes. Any hints ?","Prove, without expanding, that \begin{vmatrix} 1 &a  &a^2-bc \\  1 &b  &b^2-ca \\  1 &c  &c^2-ab  \end{vmatrix} vanishes. Any hints ?",,['linear-algebra']
39,"How to find an equation of the plane, given its normal vector and a point on the plane? [duplicate]","How to find an equation of the plane, given its normal vector and a point on the plane? [duplicate]",,"This question already has answers here : Find plane by normal and instance point + distance between origin and plane (2 answers) Closed 10 years ago . I have a question regarding vectors: Find the equation of the plane perpendicular to the vector $\vec{n}\space=(2,3,6)$ and which goes through the point $ A(1,5,3)$. (A cartesian and parametric equation). Also find the distance between the beginning of axis and this plane. I'm not really sure where to start. Any help would be appreciated.","This question already has answers here : Find plane by normal and instance point + distance between origin and plane (2 answers) Closed 10 years ago . I have a question regarding vectors: Find the equation of the plane perpendicular to the vector $\vec{n}\space=(2,3,6)$ and which goes through the point $ A(1,5,3)$. (A cartesian and parametric equation). Also find the distance between the beginning of axis and this plane. I'm not really sure where to start. Any help would be appreciated.",,"['linear-algebra', 'multivariable-calculus', 'vectors']"
40,How to show that there is no $3\times3$ real matrix $A$ such that $A^2+I=0$?,How to show that there is no  real matrix  such that ?,3\times3 A A^2+I=0,Question: show that there is no $3\times3$ real matrix $A$ such that $A^2+I=0$? Is it because:  $$\det(A^2)=\det(-I)\\ \implies \det(A)\det(A)=-1\\ \implies \det(A)=-i$$ How to continue?,Question: show that there is no $3\times3$ real matrix $A$ such that $A^2+I=0$? Is it because:  $$\det(A^2)=\det(-I)\\ \implies \det(A)\det(A)=-1\\ \implies \det(A)=-i$$ How to continue?,,"['linear-algebra', 'matrices', 'determinant']"
41,Does conjugation preserve spectrum of matrices?,Does conjugation preserve spectrum of matrices?,,"Actually, I saw normalizer of diagonal matrices are permutation matrices . I read the answer but I don't know how to prove that conjugation preserves the spectrum. Actually I do some proof on 2x2 matrices, but had no idea on expanding this proof to n by n matrices.","Actually, I saw normalizer of diagonal matrices are permutation matrices . I read the answer but I don't know how to prove that conjugation preserves the spectrum. Actually I do some proof on 2x2 matrices, but had no idea on expanding this proof to n by n matrices.",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
42,How to calculate this determinant of matrix raised to high power?,How to calculate this determinant of matrix raised to high power?,,"I'm trying to get ready to my exam from linear algebra by doing some random tasks and with this one i'm pretty stuck. $$A = \begin{pmatrix} 8-5i & -6 \\ 4-5i & -3+5i \end{pmatrix}$$ Given this matrix $A$ i have to calculate determinant of $A^{-2013}$. I tried to do this by using eigenvalues. And i know this matrix has inversion ( since $det(A) \neq 0 $ ), so it's true that $det (A^{-2013}) = det (A^{2013})^{-1}$. What more i got is that eigenvalues are $\lambda_1 = 5-5i$ and $\lambda_2 = 5i$. With that, we can use similarity matrix B that $$B= \begin {pmatrix} 5-5i & 0 \\ 0 & 5i \end{pmatrix} $$ and $$A = P^{-1}\times B \times P$$ so we have to find $A \times C = B$ And this is the part i'm stuck with...  I have no idea how to do it nicely, because by staight matrix multiplication i end up with 4 complicated equations. From that i think it's okay to do $ ( P^{-1} \times B \times P ) ^{2013} = (P^{-1} \times B \times P \times P^{-1} \times B \dots ) = P^{-1} \times B^{2013} \times P $, so the final answer will be $det(P^{-1} \times B^{2013} \times P)^{-1}$, since $B$ is diagonal matrix, raising it to 2013th power will be easy. So, please help me figuring out this hard part, i don't want to make it overcomplicated :-) Thanks in advance!!!","I'm trying to get ready to my exam from linear algebra by doing some random tasks and with this one i'm pretty stuck. $$A = \begin{pmatrix} 8-5i & -6 \\ 4-5i & -3+5i \end{pmatrix}$$ Given this matrix $A$ i have to calculate determinant of $A^{-2013}$. I tried to do this by using eigenvalues. And i know this matrix has inversion ( since $det(A) \neq 0 $ ), so it's true that $det (A^{-2013}) = det (A^{2013})^{-1}$. What more i got is that eigenvalues are $\lambda_1 = 5-5i$ and $\lambda_2 = 5i$. With that, we can use similarity matrix B that $$B= \begin {pmatrix} 5-5i & 0 \\ 0 & 5i \end{pmatrix} $$ and $$A = P^{-1}\times B \times P$$ so we have to find $A \times C = B$ And this is the part i'm stuck with...  I have no idea how to do it nicely, because by staight matrix multiplication i end up with 4 complicated equations. From that i think it's okay to do $ ( P^{-1} \times B \times P ) ^{2013} = (P^{-1} \times B \times P \times P^{-1} \times B \dots ) = P^{-1} \times B^{2013} \times P $, so the final answer will be $det(P^{-1} \times B^{2013} \times P)^{-1}$, since $B$ is diagonal matrix, raising it to 2013th power will be easy. So, please help me figuring out this hard part, i don't want to make it overcomplicated :-) Thanks in advance!!!",,['linear-algebra']
43,Show that $\operatorname{rank}A+\operatorname{rank}A^m \leq n$ where $A^{m+1}=0$,Show that  where,\operatorname{rank}A+\operatorname{rank}A^m \leq n A^{m+1}=0,"Let $A$ be an $n \times n$ nilpotent matrix over a field $F$ with $A^{m+1}=0$. Show that    $$\operatorname{rank}A+\operatorname{rank}A^m \leq n.$$ By FTLA, it is equivalent to  $$ n \leq \operatorname{nullity}A+\operatorname{nullity}A^m$$ By definition, $\operatorname{nullity}A^m>0$. Thus it suffices to show that $$\operatorname{nullity}A^r>\operatorname{nullity}A^{r+1}$$ for $0<r<m$","Let $A$ be an $n \times n$ nilpotent matrix over a field $F$ with $A^{m+1}=0$. Show that    $$\operatorname{rank}A+\operatorname{rank}A^m \leq n.$$ By FTLA, it is equivalent to  $$ n \leq \operatorname{nullity}A+\operatorname{nullity}A^m$$ By definition, $\operatorname{nullity}A^m>0$. Thus it suffices to show that $$\operatorname{nullity}A^r>\operatorname{nullity}A^{r+1}$$ for $0<r<m$",,"['linear-algebra', 'abstract-algebra', 'matrices', 'inequality', 'matrix-rank']"
44,"For self-adjoint operators, eigenvectors that correspond to distinct eigenvalues are orthogonal","For self-adjoint operators, eigenvectors that correspond to distinct eigenvalues are orthogonal",,"So I was looking for a proof for the next theorem. $V$ is inner product space $T: V\rightarrow V$ self adjoint linear map. $ \lambda_{1},\lambda_{2} \in \mathbb{F}$ so that $ \lambda_{1} \neq \lambda_{2}$ $ v_{1},v_{2} \in V$ so that   $ 0_{v} \neq v_{1} \neq v_{2} \neq 0_{v}$ $T(v_{1}) = \lambda_{1}v_{1}$ $T(v_{2}) = \lambda_{2}v_{2}$ then $\langle v_{1},v_{2}\rangle = 0$ I searched for a proof but I did not find it. Please show me the proof or give me a link to where the proof is. Thanks in advanced!","So I was looking for a proof for the next theorem. $V$ is inner product space $T: V\rightarrow V$ self adjoint linear map. $ \lambda_{1},\lambda_{2} \in \mathbb{F}$ so that $ \lambda_{1} \neq \lambda_{2}$ $ v_{1},v_{2} \in V$ so that   $ 0_{v} \neq v_{1} \neq v_{2} \neq 0_{v}$ $T(v_{1}) = \lambda_{1}v_{1}$ $T(v_{2}) = \lambda_{2}v_{2}$ then $\langle v_{1},v_{2}\rangle = 0$ I searched for a proof but I did not find it. Please show me the proof or give me a link to where the proof is. Thanks in advanced!",,['linear-algebra']
45,How to Tell If Matrices Are Linearly Independent,How to Tell If Matrices Are Linearly Independent,,"If I have two matrices, for example: $\begin{pmatrix}1&0\\2&1 \end{pmatrix}$ and $\begin{pmatrix} 1&2\\4&3\end{pmatrix},$ how do I determine if they are linearly independent or not in $\mathbb{R}^4$? I am familiar with checking for independence with vectors, such as by checking the determinant to be non-zero, or using the definition of linear independence such as $a(1,2)+b(2,3)=(0,0)$ and checking if $a=b=0$ is the only solution.","If I have two matrices, for example: $\begin{pmatrix}1&0\\2&1 \end{pmatrix}$ and $\begin{pmatrix} 1&2\\4&3\end{pmatrix},$ how do I determine if they are linearly independent or not in $\mathbb{R}^4$? I am familiar with checking for independence with vectors, such as by checking the determinant to be non-zero, or using the definition of linear independence such as $a(1,2)+b(2,3)=(0,0)$ and checking if $a=b=0$ is the only solution.",,"['linear-algebra', 'matrices']"
46,Can we derive that $A$ commutes with $B$ from this?,Can we derive that  commutes with  from this?,A B,"Based on some Physics backgrounds, I want to confirm the following thing. Let $[A,B]:=AB-BA$, where $A,B$ are matrices. Now the question is as follows: If for any real number $\lambda$, $[A,e^{\lambda B}]=0$, then is $[A,B]=0$ true? Where $A,B$ are matrices. If the above statement is true, how to give a rigorous proof ? So far the approach by myself is: $[A,e^{\lambda B}]=\lambda[A,B]+\frac{\lambda^2}{2}[A,B^2]+...=0\Rightarrow\frac{1}{\lambda}[A,e^{\lambda B}]=[A,B]+\frac{\lambda}{2}[A,B^2]+...=0 $(for any nonzero $\lambda$), then $\lim_{ \lambda\rightarrow 0}\frac{1}{\lambda}[A,e^{\lambda B}]=[A,B]=0$. Is my proof rigorous from the Math viewpoint? Other excellent method is welcome ! Thanks!","Based on some Physics backgrounds, I want to confirm the following thing. Let $[A,B]:=AB-BA$, where $A,B$ are matrices. Now the question is as follows: If for any real number $\lambda$, $[A,e^{\lambda B}]=0$, then is $[A,B]=0$ true? Where $A,B$ are matrices. If the above statement is true, how to give a rigorous proof ? So far the approach by myself is: $[A,e^{\lambda B}]=\lambda[A,B]+\frac{\lambda^2}{2}[A,B^2]+...=0\Rightarrow\frac{1}{\lambda}[A,e^{\lambda B}]=[A,B]+\frac{\lambda}{2}[A,B^2]+...=0 $(for any nonzero $\lambda$), then $\lim_{ \lambda\rightarrow 0}\frac{1}{\lambda}[A,e^{\lambda B}]=[A,B]=0$. Is my proof rigorous from the Math viewpoint? Other excellent method is welcome ! Thanks!",,"['linear-algebra', 'analysis', 'matrices']"
47,Proving existence of $T$-invariant subspace,Proving existence of -invariant subspace,T,Let $T:\mathbb{R}^{3}\rightarrow \mathbb{R}^{3}$ be a linear transformation. I'm trying to prove that there exists a T-invariant subspace $W\subset \mathbb{R}^3$ so that $\dim W=2$. How can I prove it? Any advice?,Let $T:\mathbb{R}^{3}\rightarrow \mathbb{R}^{3}$ be a linear transformation. I'm trying to prove that there exists a T-invariant subspace $W\subset \mathbb{R}^3$ so that $\dim W=2$. How can I prove it? Any advice?,,['linear-algebra']
48,Number Of Solutions $X^{2}=X$,Number Of Solutions,X^{2}=X,"The equation $x^{2}=x$ $,x\in \mathbb{R}$ has only the solutions $x=0$ and $x=1$. For a $2 \times 2$ matrix X, how many solutions does $X^{2}=X$ have? What if $X$ is symmetric?","The equation $x^{2}=x$ $,x\in \mathbb{R}$ has only the solutions $x=0$ and $x=1$. For a $2 \times 2$ matrix X, how many solutions does $X^{2}=X$ have? What if $X$ is symmetric?",,"['linear-algebra', 'matrices']"
49,Calculating the centralizer of a matrix in a general linear group.,Calculating the centralizer of a matrix in a general linear group.,,"Let $G = GL(3,\mathbb{R})$ be the general linear group over the reals , of order $3$ , and let  $A\in G$ be : $$ A=\begin{pmatrix}  -1 & 0 & 0 \\\  0 & 1  & 0 \\\  0 &  0 & 2 \end{pmatrix}. $$ I'm having a hard time calculating $C(A)$ , I've deduced the following : $$C(A)=\langle A \rangle \cup \{ kI | k \in R , k \neq 0 \}$$ Is there any other elements in $C(A)$? and how do I go about solving such a question? Note : $\langle A \rangle$ is the cyclic sub-group of $G$, generated by $A$","Let $G = GL(3,\mathbb{R})$ be the general linear group over the reals , of order $3$ , and let  $A\in G$ be : $$ A=\begin{pmatrix}  -1 & 0 & 0 \\\  0 & 1  & 0 \\\  0 &  0 & 2 \end{pmatrix}. $$ I'm having a hard time calculating $C(A)$ , I've deduced the following : $$C(A)=\langle A \rangle \cup \{ kI | k \in R , k \neq 0 \}$$ Is there any other elements in $C(A)$? and how do I go about solving such a question? Note : $\langle A \rangle$ is the cyclic sub-group of $G$, generated by $A$",,"['linear-algebra', 'abstract-algebra', 'matrices']"
50,Orientations on Manifold,Orientations on Manifold,,This is a very basic definition of orientable and very basic example 20.5 however ı could not understand definition in an good way so ı want you to explain my green writing please :) and my example please help me ı want to learn orientation on manifold if ı could not understand in a good way this ı will not understand in a good way rest of the subject please help me,This is a very basic definition of orientable and very basic example 20.5 however ı could not understand definition in an good way so ı want you to explain my green writing please :) and my example please help me ı want to learn orientation on manifold if ı could not understand in a good way this ı will not understand in a good way rest of the subject please help me,,"['linear-algebra', 'differential-geometry', 'manifolds', 'differential-forms']"
51,Show that the zero set of $f$ is an orientable submanifold of $\Bbb R^{n+1}$.,Show that the zero set of  is an orientable submanifold of .,f \Bbb R^{n+1},"Suppose $f(x_1,...,x_{n+1})$ is a$ C^∞$ function on $\Bbb R^{n+1}$ with $0$ as a regular value. Show that the zero set of $f$ is an orientable submanifold of $\Bbb R_{n+1}$. In particular, the unit n-sphere $S_n$ in $\Bbb R^{n+1}$ is orientable. I think that By the regular level set theorem, if $0$ is a regular value of a $C^∞$ function $f(x_1,...x_{n+1})$ on $\Bbb R^{n+1}$, then the zero set $f^{−1}(0)$ is a $C^∞$ manifold. And I guess that I need to apply a theorem. The theorem is following.. THM: A manifold M of dimension n is orientable if and only if there exists a $C^∞$ nowhere-vanishing n-form on M. But I dont know how to apply this theorem. Please help me show how to apply this theorem to my question explicitly and instructively if my solution way is correct. Thank you for help.","Suppose $f(x_1,...,x_{n+1})$ is a$ C^∞$ function on $\Bbb R^{n+1}$ with $0$ as a regular value. Show that the zero set of $f$ is an orientable submanifold of $\Bbb R_{n+1}$. In particular, the unit n-sphere $S_n$ in $\Bbb R^{n+1}$ is orientable. I think that By the regular level set theorem, if $0$ is a regular value of a $C^∞$ function $f(x_1,...x_{n+1})$ on $\Bbb R^{n+1}$, then the zero set $f^{−1}(0)$ is a $C^∞$ manifold. And I guess that I need to apply a theorem. The theorem is following.. THM: A manifold M of dimension n is orientable if and only if there exists a $C^∞$ nowhere-vanishing n-form on M. But I dont know how to apply this theorem. Please help me show how to apply this theorem to my question explicitly and instructively if my solution way is correct. Thank you for help.",,"['linear-algebra', 'differential-geometry', 'manifolds']"
52,Eigenvalues of a rotation,Eigenvalues of a rotation,,How do I show that the rotation by a non zero angle $\theta$ in $\mathbb{R}^2 $ does not have any real eigenvalues. I know the matrix of a rotation but I don't how to show the above proposition. Thank you,How do I show that the rotation by a non zero angle $\theta$ in $\mathbb{R}^2 $ does not have any real eigenvalues. I know the matrix of a rotation but I don't how to show the above proposition. Thank you,,['linear-algebra']
53,How to compute Nullspace on maple?,How to compute Nullspace on maple?,,I have the matrix  $$A := \begin{bmatrix}6& 9& 15\\-5& -10& -21\\ 2& 5& 11\end{bmatrix}.$$ Can anyone please tell me how to both find the eigenspaces by hand and also by using the Nullspace command on maple? Thanks.,I have the matrix  $$A := \begin{bmatrix}6& 9& 15\\-5& -10& -21\\ 2& 5& 11\end{bmatrix}.$$ Can anyone please tell me how to both find the eigenspaces by hand and also by using the Nullspace command on maple? Thanks.,,"['linear-algebra', 'maple']"
54,"let $A\in M{_n}\ (\mathbb R)$ s.t $A^2=I$ such that $A\neq I$, $A\neq -I$ how prove $-(n-1)\le tr A\le n-1$","let  s.t  such that ,  how prove",A\in M{_n}\ (\mathbb R) A^2=I A\neq I A\neq -I -(n-1)\le tr A\le n-1,"let $A\in M{_n}\ (\mathbb R)$ s.t $A^2=I$ such that $A\neq I$, $A\neq -I$ how prove $-(n-1)\le tr A\le n-1.$  Thanks in advance","let $A\in M{_n}\ (\mathbb R)$ s.t $A^2=I$ such that $A\neq I$, $A\neq -I$ how prove $-(n-1)\le tr A\le n-1.$  Thanks in advance",,['linear-algebra']
55,Prove that $V=U \bigoplus W \approx U \times W$,Prove that,V=U \bigoplus W \approx U \times W,"Let $V=U \bigoplus W$, $V \approx U \times W$.  Note that $U,W$, are finite dimensional subspaces of the vector space V, and also that $U \bigoplus W$ means $V=U+W$ and $U \cap W = \{0\}$ I'm really not sure how to go about this, because it doesn't seem to be true to me.  But after some research, it does seem to be true. Thanks in advance.","Let $V=U \bigoplus W$, $V \approx U \times W$.  Note that $U,W$, are finite dimensional subspaces of the vector space V, and also that $U \bigoplus W$ means $V=U+W$ and $U \cap W = \{0\}$ I'm really not sure how to go about this, because it doesn't seem to be true to me.  But after some research, it does seem to be true. Thanks in advance.",,['linear-algebra']
56,Counting invariant subspaces of a Vector space,Counting invariant subspaces of a Vector space,,"Well, I was reading about invariant subspaces and related things and this question came to my mind: If I choose a vector space and fix a linear transformation on itself, then how many invariant subspaces will there be? Is there any formula or materials to read?","Well, I was reading about invariant subspaces and related things and this question came to my mind: If I choose a vector space and fix a linear transformation on itself, then how many invariant subspaces will there be? Is there any formula or materials to read?",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
57,Linear span of functions,Linear span of functions,,"Maybe this is something basic but I am not familiar with the term ""linear span"" and in one question it mentions the linear span of the functions $f_n(t) = e^{nt}$, $n = 0,1,2,...$ $t \in [a,b]$ What I understood is linear span is the set of linear combination of all elements but I am still unable to understand what that set is. Thank you very much.","Maybe this is something basic but I am not familiar with the term ""linear span"" and in one question it mentions the linear span of the functions $f_n(t) = e^{nt}$, $n = 0,1,2,...$ $t \in [a,b]$ What I understood is linear span is the set of linear combination of all elements but I am still unable to understand what that set is. Thank you very much.",,['linear-algebra']
58,Understanding Cauchy-Schwarz inequality for matrices,Understanding Cauchy-Schwarz inequality for matrices,,"Can somebody please help me understand this Cauchy-Schwarz inequality? I am told that $A\neq 0$ is Hermitian with nonzero eigenvalues $\lambda_{1},\cdots,\lambda_{r}$ . And according to the Cauchy-Schwarz inequality it is obvious that... $\newcommand{\tr}{\mathrm{tr}}(\tr (A))^{2}=(\Sigma_{i=1}^{k}(\lambda_{i}))^{2}\leq k\Sigma_{i=1}^{k}\lambda_{i}^{2}=k (\tr A^{2}) $ However, I do not follow this reasoning. Thanks in advance.","Can somebody please help me understand this Cauchy-Schwarz inequality? I am told that is Hermitian with nonzero eigenvalues . And according to the Cauchy-Schwarz inequality it is obvious that... However, I do not follow this reasoning. Thanks in advance.","A\neq 0 \lambda_{1},\cdots,\lambda_{r} \newcommand{\tr}{\mathrm{tr}}(\tr (A))^{2}=(\Sigma_{i=1}^{k}(\lambda_{i}))^{2}\leq k\Sigma_{i=1}^{k}\lambda_{i}^{2}=k (\tr A^{2}) ","['linear-algebra', 'matrices', 'cauchy-schwarz-inequality', 'hermitian-matrices']"
59,Can a Gaussian integer matrix have an inverse with Gaussian integer entries?,Can a Gaussian integer matrix have an inverse with Gaussian integer entries?,,"Is there any way to characterize the set of complex matrices with Gaussian integer entries whose inverses also have Gaussian integer entries? I'm aware of the numerous examples of integer matrices whose inverses also have integer entries (usually involving binomial coefficients), but I'm wondering if those constructions can be generalized to Gaussian integers.","Is there any way to characterize the set of complex matrices with Gaussian integer entries whose inverses also have Gaussian integer entries? I'm aware of the numerous examples of integer matrices whose inverses also have integer entries (usually involving binomial coefficients), but I'm wondering if those constructions can be generalized to Gaussian integers.",,"['linear-algebra', 'matrices', 'complex-numbers']"
60,Does the topology determine the vector space structure of a topological vector space?,Does the topology determine the vector space structure of a topological vector space?,,"Let $(V, \cdot, +)$ be a topological vector space over $\mathbb{R}$ or $\mathbb{C}$ with topology $\mathcal{T}$ and let $0 \in V$ be the zero vector. Is then the linear structure $(\cdot, +)$ on the pointed topological space $(V,0)$ uniquely determined by the topology $\mathcal{T}$ and the distinguished zero element $0$ ? If not in general, does this hold, if one specifies further requirements on $\mathcal{T}$ , i.e. separation axioms?","Let be a topological vector space over or with topology and let be the zero vector. Is then the linear structure on the pointed topological space uniquely determined by the topology and the distinguished zero element ? If not in general, does this hold, if one specifies further requirements on , i.e. separation axioms?","(V, \cdot, +) \mathbb{R} \mathbb{C} \mathcal{T} 0 \in V (\cdot, +) (V,0) \mathcal{T} 0 \mathcal{T}","['linear-algebra', 'general-topology', 'functional-analysis', 'vector-spaces', 'topological-vector-spaces']"
61,"What are the linear transformations that preserves the cross product, i.e. $ R(v\times w) = (Rv) \times (Rw), \forall v,w \in \mathbb{R}^3 $","What are the linear transformations that preserves the cross product, i.e."," R(v\times w) = (Rv) \times (Rw), \forall v,w \in \mathbb{R}^3 ","Let us just focus on $\mathbb{R}^3$ currently. We study the set of all $3\times 3$ matrices $R$ satisfying $$ R(v\times w) = (Rv) \times (Rw), \forall v,w \in \mathbb{R}^3 $$ where $\times$ is the cross product. It is known that this condition is satisfied when $R \in \text{SO}(3)$ . However, is there any matrix other than the ones in $\text{SO}(3)$ that satisfies this condition?","Let us just focus on currently. We study the set of all matrices satisfying where is the cross product. It is known that this condition is satisfied when . However, is there any matrix other than the ones in that satisfies this condition?","\mathbb{R}^3 3\times 3 R 
R(v\times w) = (Rv) \times (Rw), \forall v,w \in \mathbb{R}^3
 \times R \in \text{SO}(3) \text{SO}(3)","['linear-algebra', 'geometry', 'lie-groups', 'vector-analysis', 'tensors']"
62,Why is the kernel of a group homomorphism so called?,Why is the kernel of a group homomorphism so called?,,"I took a course in Linear Algebra last fall. I have come to associate the word kernel with the nullspace of matrices/linear transformations - and now that I am studying Group Theory, similar intuition doesn't seem to carry forward. Why would anyone name it kernel (in group theory) if it really isn't the same thing as that in linear algebra? Perhaps there is a relation or connect between the two definitions that I am missing. Could someone help me better understand why the kernel of a homomorphism is so defined? To document the definitions: (Linear Algebra): Consider a linear transformation $T:V\to W$ . $$\ker T = \{x\in V: T(x) = 0\}$$ (Group Theory): Let $\phi:G\to H$ be a group homomorphism. $$\ker\phi = \{x\in G: \phi(x) = 1_H\}$$ where $1_H$ is the identity in $H$ . Thanks!","I took a course in Linear Algebra last fall. I have come to associate the word kernel with the nullspace of matrices/linear transformations - and now that I am studying Group Theory, similar intuition doesn't seem to carry forward. Why would anyone name it kernel (in group theory) if it really isn't the same thing as that in linear algebra? Perhaps there is a relation or connect between the two definitions that I am missing. Could someone help me better understand why the kernel of a homomorphism is so defined? To document the definitions: (Linear Algebra): Consider a linear transformation . (Group Theory): Let be a group homomorphism. where is the identity in . Thanks!",T:V\to W \ker T = \{x\in V: T(x) = 0\} \phi:G\to H \ker\phi = \{x\in G: \phi(x) = 1_H\} 1_H H,"['linear-algebra', 'abstract-algebra', 'group-theory', 'terminology', 'definition']"
63,Proving that any solution to the differential equation of an oscillator can be written as a sum of sinusoids.,Proving that any solution to the differential equation of an oscillator can be written as a sum of sinusoids.,,"Suppose you have a differential equation with n distinct functions of $t$ where $\frac{d^2x_1}{dt^2}=k_{11}x_1+...k_{1n}x_n$ . . . $\frac{d^2x_n}{dt^2}=k_{n1}x_1+...k_{nn}x_n$ I want to show that any set of solutions of this differential equation $(x_1,x_2,...,x_n)  $ can be written as a linear combination of solutions of the form $(e^{iw_1t},...,e^{iw_1t}), (e^{iw_2t},...e^{iw_2t}), ...,(e^{iw_mt},...e^{iw_mt})$ where each $w_j$ is a real number. i.e. I want to know why the motion of any oscillator can be written as a linear combination of its normal modes. I would also appreciate it if you could tell me if a proof of this fact has to do with eigenvalues and eigenvectors in general.",Suppose you have a differential equation with n distinct functions of where . . . I want to show that any set of solutions of this differential equation can be written as a linear combination of solutions of the form where each is a real number. i.e. I want to know why the motion of any oscillator can be written as a linear combination of its normal modes. I would also appreciate it if you could tell me if a proof of this fact has to do with eigenvalues and eigenvectors in general.,"t \frac{d^2x_1}{dt^2}=k_{11}x_1+...k_{1n}x_n \frac{d^2x_n}{dt^2}=k_{n1}x_1+...k_{nn}x_n (x_1,x_2,...,x_n)   (e^{iw_1t},...,e^{iw_1t}), (e^{iw_2t},...e^{iw_2t}), ...,(e^{iw_mt},...e^{iw_mt}) w_j","['linear-algebra', 'ordinary-differential-equations', 'physics']"
64,Determinant is linear as a function of each of the rows of the matrix.,Determinant is linear as a function of each of the rows of the matrix.,,Today I heard in a lecture (some video on YouTube) that the determinant is linear as a function of each of the rows of the matrix. I am not able to understand the above statement. I know that determinant is a special function which assign to each $x$ in $\mathbb K^{n \times n}$ a scalar. This is the intuitive idea. And this map is not linear as well. One way to see this is to consider the fact that determinant of $cA$ is $c^n\det(A)$ Can someone please explain what did the person mean by saying that the determinant is linear as a function of each of the rows of matrix?,Today I heard in a lecture (some video on YouTube) that the determinant is linear as a function of each of the rows of the matrix. I am not able to understand the above statement. I know that determinant is a special function which assign to each in a scalar. This is the intuitive idea. And this map is not linear as well. One way to see this is to consider the fact that determinant of is Can someone please explain what did the person mean by saying that the determinant is linear as a function of each of the rows of matrix?,x \mathbb K^{n \times n} cA c^n\det(A),"['linear-algebra', 'matrices', 'determinant']"
65,Real n-by-n Matrices...,Real n-by-n Matrices...,,"Let $M_n(\mathbb{R})$ denote the vector space of real $n\times n$ matrics, and let $A \in M_n(\mathbb{R})$ . Part (a) of this question says: Suppose $B \in M_n(\mathbb{R})$ such that $AB = I_n$ (the $n \times n$ identity matrix. If $C \in M_n(\mathbb{R})$ such that $CA = 0$ , then prove $C = 0$ . I have already proven part (a). Part (b) asks: Assume there exists a least positive integer $m$ such that $t_0I + t_1A + \dots + t_mA^m = 0$ for some $t_0, \dots, t_m \in \mathbb{R}$ with $t_m \neq 0$ . Also, suppose that $AB = I_n$ for some $B \in M_n(\mathbb{R})$ .Prove that $t_0 \neq 0$ . (Hint: Use the result from part (a)). My idea is to use induction on $m$ . That is, suppose $$t_0I = 0.$$ But this implies that $t_0 = 0$ since $I$ is the identity. But we could see this as \begin{align*} t_0I &= 0 \\ t_0AB &= 0 \\ t_0CAB &= 0 \\ t_0C &= 0. \end{align*} But I don't think this is the right approach. So I have a few questions: (i) Is this the correct approach? (ii) How do I use the result from part (a) properly? (iii) What's a good resource for these types of questions? The book that I am using is ""Linear Algebra Done Right by Sheldon Axler"". I am studying for my linear algebra comp in a few weeks so any help is appreciated!","Let denote the vector space of real matrics, and let . Part (a) of this question says: Suppose such that (the identity matrix. If such that , then prove . I have already proven part (a). Part (b) asks: Assume there exists a least positive integer such that for some with . Also, suppose that for some .Prove that . (Hint: Use the result from part (a)). My idea is to use induction on . That is, suppose But this implies that since is the identity. But we could see this as But I don't think this is the right approach. So I have a few questions: (i) Is this the correct approach? (ii) How do I use the result from part (a) properly? (iii) What's a good resource for these types of questions? The book that I am using is ""Linear Algebra Done Right by Sheldon Axler"". I am studying for my linear algebra comp in a few weeks so any help is appreciated!","M_n(\mathbb{R}) n\times n A \in M_n(\mathbb{R}) B \in M_n(\mathbb{R}) AB = I_n n \times n C \in M_n(\mathbb{R}) CA = 0 C = 0 m t_0I + t_1A + \dots + t_mA^m = 0 t_0, \dots, t_m \in \mathbb{R} t_m \neq 0 AB = I_n B \in M_n(\mathbb{R}) t_0 \neq 0 m t_0I = 0. t_0 = 0 I \begin{align*}
t_0I &= 0
\\
t_0AB &= 0
\\
t_0CAB &= 0
\\
t_0C &= 0.
\end{align*}","['linear-algebra', 'matrices', 'vector-spaces']"
66,How can skew-symmetric matrices be thought of as infinitesimal rotations?,How can skew-symmetric matrices be thought of as infinitesimal rotations?,,"I've recently stumbled upon the fact that skew-symmetric matrices represent somehow infinitesimal rotations. Having never encountered them, I looked them up and learnt they have to do with Lie algebras and groups, but this is beyond what I've studied so far. Is it possible to have a more intuitive understanding of this? Also, from Wikipedia: skew-symmetric matrices are derivatives, while an actual infinitesimal rotation matrix has the form $I+Ad\theta$ where $d\theta$ is vanishingly small and $A ∈ so(3)$. Having read this is about derivatives and has applications in physics, that ""lonely"" $d\theta$ is actually a bit suspicious. What about it?","I've recently stumbled upon the fact that skew-symmetric matrices represent somehow infinitesimal rotations. Having never encountered them, I looked them up and learnt they have to do with Lie algebras and groups, but this is beyond what I've studied so far. Is it possible to have a more intuitive understanding of this? Also, from Wikipedia: skew-symmetric matrices are derivatives, while an actual infinitesimal rotation matrix has the form $I+Ad\theta$ where $d\theta$ is vanishingly small and $A ∈ so(3)$. Having read this is about derivatives and has applications in physics, that ""lonely"" $d\theta$ is actually a bit suspicious. What about it?",,"['linear-algebra', 'geometry', 'lie-groups', 'intuition', 'infinitesimals']"
67,Directional derivative: what is the relation between definition by limit and definition as dot product?,Directional derivative: what is the relation between definition by limit and definition as dot product?,,I'm trying to get intuition about why gradient is pointing to the direction of the steepest ascent. I got confused because I found that directional derivative is explained with help of gradient and gradient is explained with help of directional derivative. Please explain what are the exact steps that lead from  directional derivative defined by the limit $\nabla_{v} f(x_0) = \lim_{h\to 0} \frac{f(x_0+hv)-f(x_0)}h$ to directional derivative defined as dot product of gradient and vector $\nabla_{v} f(x_0) = \nabla f(x_0)\cdot{v}$ ? In other words how to prove the following? $$\lim_{h\to 0} \frac{f(x_0+hv)-f(x_0)}h = \nabla f(x_0)\cdot{v}$$,I'm trying to get intuition about why gradient is pointing to the direction of the steepest ascent. I got confused because I found that directional derivative is explained with help of gradient and gradient is explained with help of directional derivative. Please explain what are the exact steps that lead from  directional derivative defined by the limit $\nabla_{v} f(x_0) = \lim_{h\to 0} \frac{f(x_0+hv)-f(x_0)}h$ to directional derivative defined as dot product of gradient and vector $\nabla_{v} f(x_0) = \nabla f(x_0)\cdot{v}$ ? In other words how to prove the following? $$\lim_{h\to 0} \frac{f(x_0+hv)-f(x_0)}h = \nabla f(x_0)\cdot{v}$$,,"['linear-algebra', 'derivatives', 'vector-analysis']"
68,"Let $a,b \in \mathbb Z$. Prove that if $3 \mid (a+2b)$ then $3 \mid (2a+b)$",Let . Prove that if  then,"a,b \in \mathbb Z 3 \mid (a+2b) 3 \mid (2a+b)","Let $a,b \in \mathbb Z$. Prove that if $3 \mid (a+2b)$ then $3 \mid (2a+b)$ This is how I solved this: $3m = a+2b \iff a = 3m-2b$ $2a+b = 2(3m-2b)+b = 6m -3b = 3(2m-b)$  And now, my solution seems to work. However, when - instead of solving for $a$, I solve for $b$, then I get this: $2a+b = 3 \frac{a+m}{2}$ How can I even be sure that this number is an integer? Is there a way to fix the second solution? If my first solution is correct. the second one should work as well.","Let $a,b \in \mathbb Z$. Prove that if $3 \mid (a+2b)$ then $3 \mid (2a+b)$ This is how I solved this: $3m = a+2b \iff a = 3m-2b$ $2a+b = 2(3m-2b)+b = 6m -3b = 3(2m-b)$  And now, my solution seems to work. However, when - instead of solving for $a$, I solve for $b$, then I get this: $2a+b = 3 \frac{a+m}{2}$ How can I even be sure that this number is an integer? Is there a way to fix the second solution? If my first solution is correct. the second one should work as well.",,"['linear-algebra', 'number-theory', 'elementary-number-theory']"
69,Codimension of intersection,Codimension of intersection,,"Suppose $E$ is a vector space over a field of characteristic $0$ . Let $E_1, F_1$ be subspaces of finite codimension and let $E_2, F_2$ be their respective complements, i.e., $E = E_1 \oplus E_2 = F_1 \oplus F_2$ . $\DeclareMathOperator{\codim}{codim}$ I know that $\dim E_2 = \codim E_1$ and $\dim  F_2 = \codim F_1$ because $E_2 \cong E/E_1$ and $F_2 \cong E/F_1$ . But I don't know how to prove that $\codim (E_1 \cap F_1) \le \dim E_2 + \dim F_2$ . I saw a proof that $\codim (E_1 \cap F_1)$ is finite but it used some fancy isomorphism theorem, so I think a more bare hands approach would be helpful. Thanks.","Suppose is a vector space over a field of characteristic . Let be subspaces of finite codimension and let be their respective complements, i.e., . I know that and because and . But I don't know how to prove that . I saw a proof that is finite but it used some fancy isomorphism theorem, so I think a more bare hands approach would be helpful. Thanks.","E 0 E_1, F_1 E_2, F_2 E = E_1 \oplus E_2 = F_1 \oplus F_2 \DeclareMathOperator{\codim}{codim} \dim E_2 = \codim E_1 \dim  F_2 = \codim F_1 E_2 \cong E/E_1 F_2 \cong E/F_1 \codim (E_1 \cap F_1) \le \dim E_2 + \dim F_2 \codim (E_1 \cap F_1)",['linear-algebra']
70,"If $0$ is the only eigenvalue of a linear operator, is the operator nilpotent","If  is the only eigenvalue of a linear operator, is the operator nilpotent",0,"In a finite dimensional vector space, if $0$ is an eigenvalue and the only eigenvalue of a linear operator, is that operator nilpotent? There is this post which shows the other direction. Prove that the only eigenvalue of a nilpotent operator is 0? I would think the question would be posed as ""iff"" to the extent the answer to my question is affirmative. To the extent that is not the case, I would please appreciate an example to that effect. Thanks","In a finite dimensional vector space, if $0$ is an eigenvalue and the only eigenvalue of a linear operator, is that operator nilpotent? There is this post which shows the other direction. Prove that the only eigenvalue of a nilpotent operator is 0? I would think the question would be posed as ""iff"" to the extent the answer to my question is affirmative. To the extent that is not the case, I would please appreciate an example to that effect. Thanks",,['linear-algebra']
71,Map not preserving vector addition but preserving scalar multiplication,Map not preserving vector addition but preserving scalar multiplication,,"The question Map closed under addition but not multiplication asks for a map between two vector spaces where vector addition is preserved but also where scalar multiplication is not preserved. A student of mine switched this, and I have not found an example. Thus, what is an example of a map between two vector spaces where vector addition is not preserved but scalar multiplication is preserved? Thank you.","The question Map closed under addition but not multiplication asks for a map between two vector spaces where vector addition is preserved but also where scalar multiplication is not preserved. A student of mine switched this, and I have not found an example. Thus, what is an example of a map between two vector spaces where vector addition is not preserved but scalar multiplication is preserved? Thank you.",,['linear-algebra']
72,"Why ""similar"" matrices instead of ""equivalent""?","Why ""similar"" matrices instead of ""equivalent""?",,"We define matrices $A$ and $B$ to be equivalent iff there exist invertible matrices $P$ and $Q$ such that $B = Q^{-1}AP$. If $A$ and $B$ are both square matrices, then we consider a different equivalence relation: similarity ($A$ and $B$ similar iff $A = CBC^{-1}$ for some invertible $C$). Equivalent matrices represent the same mapping under a different choice of basis for the domain and range. When we deal with similar matrices, clearly the domain and the range are the same. But why is it necessary to introduce a new relation? Why doesn't equivalence suffice?","We define matrices $A$ and $B$ to be equivalent iff there exist invertible matrices $P$ and $Q$ such that $B = Q^{-1}AP$. If $A$ and $B$ are both square matrices, then we consider a different equivalence relation: similarity ($A$ and $B$ similar iff $A = CBC^{-1}$ for some invertible $C$). Equivalent matrices represent the same mapping under a different choice of basis for the domain and range. When we deal with similar matrices, clearly the domain and the range are the same. But why is it necessary to introduce a new relation? Why doesn't equivalence suffice?",,"['linear-algebra', 'linear-transformations']"
73,Can orthogonal matrix be complex? [closed],Can orthogonal matrix be complex? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question A matrix $Q$ is orthogonal if $Q^TQ=I$. My question is can $Q$ be complex? Can anyone help show an example, or provide a proof that $Q$ has to be real? Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question A matrix $Q$ is orthogonal if $Q^TQ=I$. My question is can $Q$ be complex? Can anyone help show an example, or provide a proof that $Q$ has to be real? Thanks.",,['linear-algebra']
74,Invariant subspace of cyclic space is cyclic,Invariant subspace of cyclic space is cyclic,,"Let $V$ be a finite dimensional vector space and let $T:V\rightarrow V$ be a cyclic linear operator, that is, there exists $v \in V$ such that $\{v, Tv, T^2v, \dots\}$ generates $V$. Let $W\subset V$ be a $T$-invariant subspace, that is, $T[W]\subset W$. I'm trying to see that $T|W$ is also $T|W$-cyclic, that is, there exists a $w \in W$ such that $W=\langle w, Tw, T^2w, \dots\rangle$.","Let $V$ be a finite dimensional vector space and let $T:V\rightarrow V$ be a cyclic linear operator, that is, there exists $v \in V$ such that $\{v, Tv, T^2v, \dots\}$ generates $V$. Let $W\subset V$ be a $T$-invariant subspace, that is, $T[W]\subset W$. I'm trying to see that $T|W$ is also $T|W$-cyclic, that is, there exists a $w \in W$ such that $W=\langle w, Tw, T^2w, \dots\rangle$.",,[]
75,"Determinant of matrix with $A_{ij} = \min (i, j)$",Determinant of matrix with,"A_{ij} = \min (i, j)","Given a $n\times n$ matrix whose $(i, j)$-th entry is the lower of $i,j$, eg. $$\begin{pmatrix}1 & 1 & 1 & 1\\ 1 & 2 & 2 & 2 \\ 1 & 2 & 3 & 3\\ 1 & 2 & 3 & 4 \end{pmatrix}.$$ The determinant of any such matrix is $1$.  How do I prove this? Tried induction but the assumption would only help me to compute the term for $A_{nn}^*$ mirror.","Given a $n\times n$ matrix whose $(i, j)$-th entry is the lower of $i,j$, eg. $$\begin{pmatrix}1 & 1 & 1 & 1\\ 1 & 2 & 2 & 2 \\ 1 & 2 & 3 & 3\\ 1 & 2 & 3 & 4 \end{pmatrix}.$$ The determinant of any such matrix is $1$.  How do I prove this? Tried induction but the assumption would only help me to compute the term for $A_{nn}^*$ mirror.",,"['linear-algebra', 'matrices', 'determinant']"
76,Is every positive definite matrix symmetric? [duplicate],Is every positive definite matrix symmetric? [duplicate],,"This question already has answers here : What is the agreed upon definition of a ""positive definite matrix""? (3 answers) Closed 7 years ago . whether every positive definite matrix has to be symmetric? If not, what will be the example?","This question already has answers here : What is the agreed upon definition of a ""positive definite matrix""? (3 answers) Closed 7 years ago . whether every positive definite matrix has to be symmetric? If not, what will be the example?",,"['linear-algebra', 'matrices']"
77,When pseudo inverse and general inverse of a invertible square matrix will be equal or not equal?,When pseudo inverse and general inverse of a invertible square matrix will be equal or not equal?,,"I calculated general inverse and pseudo inverse of a ivertible symmetrix matrix in MATLAB by using function inv and pinv respectively, but, I got different output. I didn't get the proper reason behind that. Therefore, I want to know in which case, pinv and inv will produce same result and in which case, pinv and inv will produce different result?","I calculated general inverse and pseudo inverse of a ivertible symmetrix matrix in MATLAB by using function inv and pinv respectively, but, I got different output. I didn't get the proper reason behind that. Therefore, I want to know in which case, pinv and inv will produce same result and in which case, pinv and inv will produce different result?",,"['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
78,Lower bound on quadratic form,Lower bound on quadratic form,,Suppose I have a non-symmetric matrix $A$ and I can prove that $x^T A x = x^T \left(\frac{A+A^T}{2}\right) x>0$ for any $x \ne 0$? Can I then say that $x^T A x \ge \lambda_{\text{min}}(A) \|x\|^2 > 0$?,Suppose I have a non-symmetric matrix $A$ and I can prove that $x^T A x = x^T \left(\frac{A+A^T}{2}\right) x>0$ for any $x \ne 0$? Can I then say that $x^T A x \ge \lambda_{\text{min}}(A) \|x\|^2 > 0$?,,"['linear-algebra', 'matrices', 'quadratic-forms', 'positive-definite']"
79,Confused by the SVD of a real symmetric matrix,Confused by the SVD of a real symmetric matrix,,"For a real symmetric matrix A , it's true that: $$A A^T = A^T A = A^2$$ And since the right and left singular vectors of $A$ are the eigenvectors of $A^T A$ and $A A^T$ respectively, the right and left singular vectors ought to be identical. Here's the particular situation that is causing my confusion. Let $$A = \begin{bmatrix} 4 & 1 & -2 & 2 \\ 1 & 2 & 0 & 1 \\ -2 & 0 & 3 & -2 \\ 2 & 1 & -2 & -1\end{bmatrix}$$ Now when I try to take the SVD of this matrix, the matrices $U$ and $V$, of the left and right singular vectors respectively, are identical. Just as I thought they would be. In particular, for $U$, $\Sigma$, and $V$ I get: $$U = V = \begin{bmatrix} -0.718 & 0.202 & 0.177 & -0.642 \\ -0.221 & 0.789 & 0.178 & 0.544 \\ 0.557 & 0.580 & -0.288 & -0.520 \\ -0.353 & 0.010 & -0.924 & 0.144 \end{bmatrix}$$ and $$\Sigma = \begin{bmatrix} 6.845 & 0 & 0 & 0 \\ 0 & 2.269 & 0 & 0 \\ 0 & 0 & 2.198 & 0 \\ 0 & 0 & 0 & 1.084 \end{bmatrix}$$ But if I use this to calculate $U \Sigma V^T$ I don't get $A$. This already confuses me, but to make matters worse, if I take $V$ to be $U$ except with the signs changed on the third column, then it comes out correctly! I feel like I must be misunderstanding something about the SVD or Eigenvectors in general, as I thought that as long as each eigenvector had norm 1, it didn't matter what the sign of it was (i.e. I think about it as more of an eigendirection). Can anyone point out why I'm getting these results?","For a real symmetric matrix A , it's true that: $$A A^T = A^T A = A^2$$ And since the right and left singular vectors of $A$ are the eigenvectors of $A^T A$ and $A A^T$ respectively, the right and left singular vectors ought to be identical. Here's the particular situation that is causing my confusion. Let $$A = \begin{bmatrix} 4 & 1 & -2 & 2 \\ 1 & 2 & 0 & 1 \\ -2 & 0 & 3 & -2 \\ 2 & 1 & -2 & -1\end{bmatrix}$$ Now when I try to take the SVD of this matrix, the matrices $U$ and $V$, of the left and right singular vectors respectively, are identical. Just as I thought they would be. In particular, for $U$, $\Sigma$, and $V$ I get: $$U = V = \begin{bmatrix} -0.718 & 0.202 & 0.177 & -0.642 \\ -0.221 & 0.789 & 0.178 & 0.544 \\ 0.557 & 0.580 & -0.288 & -0.520 \\ -0.353 & 0.010 & -0.924 & 0.144 \end{bmatrix}$$ and $$\Sigma = \begin{bmatrix} 6.845 & 0 & 0 & 0 \\ 0 & 2.269 & 0 & 0 \\ 0 & 0 & 2.198 & 0 \\ 0 & 0 & 0 & 1.084 \end{bmatrix}$$ But if I use this to calculate $U \Sigma V^T$ I don't get $A$. This already confuses me, but to make matters worse, if I take $V$ to be $U$ except with the signs changed on the third column, then it comes out correctly! I feel like I must be misunderstanding something about the SVD or Eigenvectors in general, as I thought that as long as each eigenvector had norm 1, it didn't matter what the sign of it was (i.e. I think about it as more of an eigendirection). Can anyone point out why I'm getting these results?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'svd', 'symmetric-matrices']"
80,Is any subspace of a direct sum necessarily a direct sum of subspaces?,Is any subspace of a direct sum necessarily a direct sum of subspaces?,,"If I have a direct sum $V = V_1 \oplus V_2$ and a subspace $W \subset V$, it it necessarily true that $W = W_1 \oplus W_2$ where $W_1 \subset V_1$ and $W_2 \subset V_2$? I believe this is true since we should be able take $W_1 :=W \cap V_1$ and $W_2 := W \cap V_2$, but I just want to make sure there isn't a flaw this this argument.  Thanks!","If I have a direct sum $V = V_1 \oplus V_2$ and a subspace $W \subset V$, it it necessarily true that $W = W_1 \oplus W_2$ where $W_1 \subset V_1$ and $W_2 \subset V_2$? I believe this is true since we should be able take $W_1 :=W \cap V_1$ and $W_2 := W \cap V_2$, but I just want to make sure there isn't a flaw this this argument.  Thanks!",,"['linear-algebra', 'vector-spaces', 'direct-sum']"
81,"Linear Algebra by Hoffman and Kunze, Section 3.4, Exercise Problem 12","Linear Algebra by Hoffman and Kunze, Section 3.4, Exercise Problem 12",,"I am trying to work out problems from Linear Algebra , by Hoffman and Kunze and came across this problem in the exercise of Section 3.4, I have a difficulty solving the (c) part of the problem. Problem 12 If $V$ is an $n$ - dimensional vector space over the field $F$, and let $B = \{\alpha_1,\alpha_2,\alpha_3, \cdots \alpha_n\}$ be an ordered basis for $V$. (a) There is a unique Linear Operator $T$ on $V$ such that : $$  \begin{align} T(\alpha_j) = \alpha_{j+1}, \quad j=1, \cdots ,n-1 \quad T(\alpha_n)=0 \end{align} $$ What is the matrix $A$ of $T$ in the ordered basis $B$? (b) Prove that $T^n=0$, but $T^{n-1} \neq 0$ (c) Let $S$ be any linear operator on $V$ such that $S^n=0$, but $S^{n-1} \neq 0$ . Prove that there is an ordered basis $B^{'}$ for $V$ such that the matrix of $S$ in the ordered basis $B^{'}$ is the matrix $A$ of part (a). (d) Prove that if $M$ and $N$ are $n \times n$ matrices $F$ such that $M^n = N^n = 0$, but $M^{n-1} \neq 0 \neq N^{n-1}$, then $M,N$ are similar. However the solution for the problem (c) can be obtained assuming (d) is true, in the following way: Let $[S]_B$ represent the matrix corresponding to the linear operator $S$ under some basis $B$, and we want to show that this operator has the matrix $[S]_{B^{'}}=A$ under some basis $B^{'}$.  If the two nilpotent matrices of order $n$ are similar, then there must be an invertible matrix $P$ such that $[S]_{B^{'}} = P^{-1}[S]_BP$. The matrix $P$ can be used to prove the existence of another basis $B^{'}$ such that: $$ \begin{align} [\alpha]_{B} = P[\alpha]_{B^{'}} \end{align} $$ and therefore, there indeed exists a basis $B^{'}$ such that $$ \begin{align} [S]_{B^{'}} = P^{-1}[S]_BP \end{align} $$ I am also having a problem solving part (d), however, I was wondering if one could prove (c) without using (d) explicitly ? I will be thankful for any hints to solve the problem!","I am trying to work out problems from Linear Algebra , by Hoffman and Kunze and came across this problem in the exercise of Section 3.4, I have a difficulty solving the (c) part of the problem. Problem 12 If $V$ is an $n$ - dimensional vector space over the field $F$, and let $B = \{\alpha_1,\alpha_2,\alpha_3, \cdots \alpha_n\}$ be an ordered basis for $V$. (a) There is a unique Linear Operator $T$ on $V$ such that : $$  \begin{align} T(\alpha_j) = \alpha_{j+1}, \quad j=1, \cdots ,n-1 \quad T(\alpha_n)=0 \end{align} $$ What is the matrix $A$ of $T$ in the ordered basis $B$? (b) Prove that $T^n=0$, but $T^{n-1} \neq 0$ (c) Let $S$ be any linear operator on $V$ such that $S^n=0$, but $S^{n-1} \neq 0$ . Prove that there is an ordered basis $B^{'}$ for $V$ such that the matrix of $S$ in the ordered basis $B^{'}$ is the matrix $A$ of part (a). (d) Prove that if $M$ and $N$ are $n \times n$ matrices $F$ such that $M^n = N^n = 0$, but $M^{n-1} \neq 0 \neq N^{n-1}$, then $M,N$ are similar. However the solution for the problem (c) can be obtained assuming (d) is true, in the following way: Let $[S]_B$ represent the matrix corresponding to the linear operator $S$ under some basis $B$, and we want to show that this operator has the matrix $[S]_{B^{'}}=A$ under some basis $B^{'}$.  If the two nilpotent matrices of order $n$ are similar, then there must be an invertible matrix $P$ such that $[S]_{B^{'}} = P^{-1}[S]_BP$. The matrix $P$ can be used to prove the existence of another basis $B^{'}$ such that: $$ \begin{align} [\alpha]_{B} = P[\alpha]_{B^{'}} \end{align} $$ and therefore, there indeed exists a basis $B^{'}$ such that $$ \begin{align} [S]_{B^{'}} = P^{-1}[S]_BP \end{align} $$ I am also having a problem solving part (d), however, I was wondering if one could prove (c) without using (d) explicitly ? I will be thankful for any hints to solve the problem!",,['linear-algebra']
82,volume of a $n$-d parallelepiped with sides given by the row vectors of a matrix $A$ is the product of the singular values of this matrix $A$,volume of a -d parallelepiped with sides given by the row vectors of a matrix  is the product of the singular values of this matrix,n A A,"Why the volume of a $n$-dimensional parallelepiped with sides given by the row vectors of a matrix $A$ can be seen as the product of the singular values of this matrix $A$? I only know in 3-dimensinal vector space, the volume of a parallelepiped is equal to the determinant of the corresponding matrix.","Why the volume of a $n$-dimensional parallelepiped with sides given by the row vectors of a matrix $A$ can be seen as the product of the singular values of this matrix $A$? I only know in 3-dimensinal vector space, the volume of a parallelepiped is equal to the determinant of the corresponding matrix.",,"['linear-algebra', 'geometry']"
83,Understanding first part of dual basis proof,Understanding first part of dual basis proof,,"The textbook I'm reading attempts to proof the following: given $\left\{v_1, \ldots, v_n \right\}$ a basis for a vectorspace $V$ over $K$, there exists a basis $\left\{ \phi_1, \ldots, \phi_n \right\}$ for $V^{*}$ defined by \begin{align*} \phi_i (v_j) = \delta_{ij}. \end{align*} Here, each $\phi_i$ is a linear functional, i.e. an element of $V^{*}$. I understand the part of the proof where he shows linear independence, but I'm not sure if I can follow him where he proves these linear functionals span $V^{*}$. Here is what the author claims: We first show that $\left\{ \phi_1, \ldots, \phi_n \right\}$ spans   $V^{*}$. Let $\phi$ be an arbitrary element of $V^{*}$, and suppose   $\phi(v_1) = k_1, \phi(v_2) = k_2, \ldots, \phi(v_n) = k_n$. Set   $\sigma = k_1 \phi_1 + \ldots + k_n \phi_n$. Then \begin{align*} \sigma(v_1) &= (k_1 \phi_1 + \ldots + k_n \phi_n) (v_1) \\ &= k_1 \phi_1 (v_1) + k_2 \phi_2 (v_1) + \ldots + k_n \phi_n (v_1) \\ &= k_1 \cdot 1 + k_2 \cdot 0 + \ldots + k_n \cdot 0 = k_1. \end{align*}   Similarly for $i= 2, \ldots, n$. Since $\phi$ and $\sigma$ agree on   the basisvectors, $\phi = \sigma = k_1 \phi_1 + \ldots k_n \phi_n$.   Accordingly, $\left\{ \phi_1, \ldots, \phi_n \right\}$ spans $V^{*}$. I got two questions/complaints. 1) Why does he set $\sigma = k_1 \phi_1 + \ldots + k_n \phi_n$ right away at the beginning? Is he allowed to do that? To me, this seems like circular reasoning. 2) What does he mean with the assertion that $\phi$ and $\sigma$ agree on the basisvectors? Does he mean they determine the same action on the basis? I don't realy understand how he concludes from this that $\phi_1, \ldots, \phi_n$ span $V^{*}$. Some help/clarifications would be appreciated.","The textbook I'm reading attempts to proof the following: given $\left\{v_1, \ldots, v_n \right\}$ a basis for a vectorspace $V$ over $K$, there exists a basis $\left\{ \phi_1, \ldots, \phi_n \right\}$ for $V^{*}$ defined by \begin{align*} \phi_i (v_j) = \delta_{ij}. \end{align*} Here, each $\phi_i$ is a linear functional, i.e. an element of $V^{*}$. I understand the part of the proof where he shows linear independence, but I'm not sure if I can follow him where he proves these linear functionals span $V^{*}$. Here is what the author claims: We first show that $\left\{ \phi_1, \ldots, \phi_n \right\}$ spans   $V^{*}$. Let $\phi$ be an arbitrary element of $V^{*}$, and suppose   $\phi(v_1) = k_1, \phi(v_2) = k_2, \ldots, \phi(v_n) = k_n$. Set   $\sigma = k_1 \phi_1 + \ldots + k_n \phi_n$. Then \begin{align*} \sigma(v_1) &= (k_1 \phi_1 + \ldots + k_n \phi_n) (v_1) \\ &= k_1 \phi_1 (v_1) + k_2 \phi_2 (v_1) + \ldots + k_n \phi_n (v_1) \\ &= k_1 \cdot 1 + k_2 \cdot 0 + \ldots + k_n \cdot 0 = k_1. \end{align*}   Similarly for $i= 2, \ldots, n$. Since $\phi$ and $\sigma$ agree on   the basisvectors, $\phi = \sigma = k_1 \phi_1 + \ldots k_n \phi_n$.   Accordingly, $\left\{ \phi_1, \ldots, \phi_n \right\}$ spans $V^{*}$. I got two questions/complaints. 1) Why does he set $\sigma = k_1 \phi_1 + \ldots + k_n \phi_n$ right away at the beginning? Is he allowed to do that? To me, this seems like circular reasoning. 2) What does he mean with the assertion that $\phi$ and $\sigma$ agree on the basisvectors? Does he mean they determine the same action on the basis? I don't realy understand how he concludes from this that $\phi_1, \ldots, \phi_n$ span $V^{*}$. Some help/clarifications would be appreciated.",,['linear-algebra']
84,"Producing lower bounds for $\text{trace}(A^2)$ for a positive semidefinite, symmetric matrix $A$","Producing lower bounds for  for a positive semidefinite, symmetric matrix",\text{trace}(A^2) A,"Are there any lower bounds on $\DeclareMathOperator{trace}{trace}$ \begin{align*} \trace(A^2), \end{align*} where $A$ is positive semi-definite and symmetric? I am aware of the inequality  $$ \trace(A^2) \le\bigl(\trace(A)\bigr)^2 $$ Are there any inequalities in the other direction?","Are there any lower bounds on $\DeclareMathOperator{trace}{trace}$ \begin{align*} \trace(A^2), \end{align*} where $A$ is positive semi-definite and symmetric? I am aware of the inequality  $$ \trace(A^2) \le\bigl(\trace(A)\bigr)^2 $$ Are there any inequalities in the other direction?",,"['linear-algebra', 'matrices']"
85,"Prove that for any positive integer $n$, $A^n ≠ I$.","Prove that for any positive integer , .",n A^n ≠ I,"Let $A$ be a $2\times 2$ matrix with $tr(A) > 2$. Prove that for any positive integer $n$, $A^n ≠ I$. I feel like I should approach this with respect to eigenvalues, i.e. the sum of the eigenvalues of $A$ is greater than $2$. However, I don't know where to head from here. Any help or guidance to a direction would be greatly appreciated!","Let $A$ be a $2\times 2$ matrix with $tr(A) > 2$. Prove that for any positive integer $n$, $A^n ≠ I$. I feel like I should approach this with respect to eigenvalues, i.e. the sum of the eigenvalues of $A$ is greater than $2$. However, I don't know where to head from here. Any help or guidance to a direction would be greatly appreciated!",,['linear-algebra']
86,"Finding the largest singular value ""easily""","Finding the largest singular value ""easily""",,"I'm only interested in finding the largest singular value of an $m\times n$ real matrix $A$ . I don't need the corresponding (left and right) singular vectors. Is there a way to do so without performing full SVD? Is there an analytical expression? If not, is there an analytical approximation?","I'm only interested in finding the largest singular value of an real matrix . I don't need the corresponding (left and right) singular vectors. Is there a way to do so without performing full SVD? Is there an analytical expression? If not, is there an analytical approximation?",m\times n A,"['linear-algebra', 'svd']"
87,Finding Jordan Canonical form given the minimal and characteristic polynomial.,Finding Jordan Canonical form given the minimal and characteristic polynomial.,,I have the following information: the characteristic polynomial of $A$ is $p_A(t)=(t-4)^3(t+6)^2$ and the minimal polynomial is $q_A(t)=(t-4)^2(t+6).$ I'm having problems seeing how one would work backwards with just this information. My attempt at JCF. $\begin{pmatrix} 4&1&0&0&0\\ 0&4&0&0&0\\ 0&0&4&0&0\\ 0&0&0&-6&0\\ 0&0&0&0&-6\\ \end{pmatrix}$,I have the following information: the characteristic polynomial of $A$ is $p_A(t)=(t-4)^3(t+6)^2$ and the minimal polynomial is $q_A(t)=(t-4)^2(t+6).$ I'm having problems seeing how one would work backwards with just this information. My attempt at JCF. $\begin{pmatrix} 4&1&0&0&0\\ 0&4&0&0&0\\ 0&0&4&0&0\\ 0&0&0&-6&0\\ 0&0&0&0&-6\\ \end{pmatrix}$,,"['linear-algebra', 'matrices', 'jordan-normal-form']"
88,Almost complex structure which fails to be compatible,Almost complex structure which fails to be compatible,,"Let $V$ be a real vector space equipped with a scalar product $\langle, \rangle$ (i.e. a positive definite symmetric bilinear form). We say that an endomorphism $J: V \to V$ is an almost complex structure if $J^2=-Id.$ $J$ is said to be compatible with the scalar product if $\langle J v, J w \rangle = \langle v, w \rangle. $ I'd like a very simple example of a scalar product and almost complex structure such that $J$ FAILS to be compatible with $\langle, \rangle.$   This is very basic -and hopefully trivial- but I can't find any counterexamples.","Let $V$ be a real vector space equipped with a scalar product $\langle, \rangle$ (i.e. a positive definite symmetric bilinear form). We say that an endomorphism $J: V \to V$ is an almost complex structure if $J^2=-Id.$ $J$ is said to be compatible with the scalar product if $\langle J v, J w \rangle = \langle v, w \rangle. $ I'd like a very simple example of a scalar product and almost complex structure such that $J$ FAILS to be compatible with $\langle, \rangle.$   This is very basic -and hopefully trivial- but I can't find any counterexamples.",,"['linear-algebra', 'differential-geometry', 'complex-geometry', 'multilinear-algebra', 'almost-complex']"
89,"Given nonzero vectors $x$ and $y$, does there exist an invertible matrix $A$ such that $Ax=y$?","Given nonzero vectors  and , does there exist an invertible matrix  such that ?",x y A Ax=y,"Given any $x, y \in \mathbb{R}^n \setminus\{0_n\}$ , does there exist an invertible matrix $A$ such that $Ax=y$ ? I was examining the orbits of set $\mathbb{R^n}$ under the action of the group $\mbox{GL}_n(R)$ by left multiplication, i.e., $$O_x = \left\{ A x \mid A \in \mbox{GL}_n(R) \right\}$$ and $x\in \mathbb{R^n}$ . Since $O_0=\{0\}$ , I thought there is only one more orbit of nonzero vectors. This can be shown if my above question holds true.","Given any , does there exist an invertible matrix such that ? I was examining the orbits of set under the action of the group by left multiplication, i.e., and . Since , I thought there is only one more orbit of nonzero vectors. This can be shown if my above question holds true.","x, y \in \mathbb{R}^n \setminus\{0_n\} A Ax=y \mathbb{R^n} \mbox{GL}_n(R) O_x = \left\{ A x \mid A \in \mbox{GL}_n(R) \right\} x\in \mathbb{R^n} O_0=\{0\}","['linear-algebra', 'abstract-algebra', 'matrices']"
90,Determinant of a tridiagonal Toeplitz matrix,Determinant of a tridiagonal Toeplitz matrix,,Let $A_n$ denote an $n \times n$ tridiagonal matrix. $$A_n=\begin{pmatrix}2 & -1 & & & 0 \\ -1 & 2 & -1 & & \\ & \ddots & \ddots & \ddots & \\ & & -1 & 2 & -1 \\ 0 & & & -1 & 2 \end{pmatrix} \quad\text{for }n \ge 2$$ Set $D_n = \det(A_n)$ . Prove that $D_n = 2D_{n-1} - D_{n-2}$ for $n \ge 4$ .,Let denote an tridiagonal matrix. Set . Prove that for .,A_n n \times n A_n=\begin{pmatrix}2 & -1 & & & 0 \\ -1 & 2 & -1 & & \\ & \ddots & \ddots & \ddots & \\ & & -1 & 2 & -1 \\ 0 & & & -1 & 2 \end{pmatrix} \quad\text{for }n \ge 2 D_n = \det(A_n) D_n = 2D_{n-1} - D_{n-2} n \ge 4,"['linear-algebra', 'matrices', 'determinant', 'tridiagonal-matrices', 'toeplitz-matrices']"
91,"For a symmetric, positive definite matrix, why does each diagonal element exceeds the small eigenvalue?","For a symmetric, positive definite matrix, why does each diagonal element exceeds the small eigenvalue?",,"In the PDE book I am reading. It took for granted, $A$ is positive and symmetric $a_{11}\geq \lambda$ where $\lambda$ is the smallest eigenvalue. (I initially wrote $>$, but that was wrong) I have managed to show this is indeed the case for $2\times 2$ matrix. why is this true for an $n\times n$ matrix? Proof for $2\times2$ matrices: Let $\Lambda$ denote the larger eigenvalue. WLOG $a_{11}\geq a_{22}$, the determinant of the matrix is $a_{11}a_{22}-a_{12}^2>0$, this forces $a_{11}a_{22}>0$. Moreover by the trace rule $\lambda+\Lambda = a_{11}+a_{22}>0$, so $\lambda\Lambda\leq a_{11}a_{22}$ and $\lambda+\Lambda=a_{11}+a_{22}$ squaring the 2nd relation and subtracting the first relation twice, we arrive at $$(\Lambda-\lambda)^2\geq (a_{11}-a_{22})^2$$ which implies $$\Lambda -\lambda \geq a_{11}-a_{22}$$ since $\Lambda +\lambda =a_{11}+a_{22}$, we must have $\Lambda\geq a_{11}\geq a_{22}\geq \lambda$. I cannot see how to extend this result for higher dimensions, essentially because I used 2 conditions and it is not sufficient to solve this problem in higher dimensions.","In the PDE book I am reading. It took for granted, $A$ is positive and symmetric $a_{11}\geq \lambda$ where $\lambda$ is the smallest eigenvalue. (I initially wrote $>$, but that was wrong) I have managed to show this is indeed the case for $2\times 2$ matrix. why is this true for an $n\times n$ matrix? Proof for $2\times2$ matrices: Let $\Lambda$ denote the larger eigenvalue. WLOG $a_{11}\geq a_{22}$, the determinant of the matrix is $a_{11}a_{22}-a_{12}^2>0$, this forces $a_{11}a_{22}>0$. Moreover by the trace rule $\lambda+\Lambda = a_{11}+a_{22}>0$, so $\lambda\Lambda\leq a_{11}a_{22}$ and $\lambda+\Lambda=a_{11}+a_{22}$ squaring the 2nd relation and subtracting the first relation twice, we arrive at $$(\Lambda-\lambda)^2\geq (a_{11}-a_{22})^2$$ which implies $$\Lambda -\lambda \geq a_{11}-a_{22}$$ since $\Lambda +\lambda =a_{11}+a_{22}$, we must have $\Lambda\geq a_{11}\geq a_{22}\geq \lambda$. I cannot see how to extend this result for higher dimensions, essentially because I used 2 conditions and it is not sufficient to solve this problem in higher dimensions.",,"['linear-algebra', 'matrices']"
92,Transpose of a linear operator is well-defined,Transpose of a linear operator is well-defined,,"I would like to define the transpose of a linear operator $T$ between finite-dimensional vector spaces $V$ and $W$. Wikipedia gives a straightforward one, but I would like to define it based on what I already know about matrices. Thus, I define the transpose of $T$ to be given by choosing bases $B_1$ and $B_2$ of $W$ and $V$ respectively, and taking the transpose of $T$'s matrix with respect to these bases. I would like to show that this definition is independent of basis choice, however. In other words, if the matrix chosen above is $A$, then if I changed basis from $B_1$ with change of basis matrix $Q$ and from $B_2$ with matrix $P$, that $QA^TP^{-1} = (PAQ^{-1})^T$. This does not seem to be the case, however. What have I done incorrectly, if anything?","I would like to define the transpose of a linear operator $T$ between finite-dimensional vector spaces $V$ and $W$. Wikipedia gives a straightforward one, but I would like to define it based on what I already know about matrices. Thus, I define the transpose of $T$ to be given by choosing bases $B_1$ and $B_2$ of $W$ and $V$ respectively, and taking the transpose of $T$'s matrix with respect to these bases. I would like to show that this definition is independent of basis choice, however. In other words, if the matrix chosen above is $A$, then if I changed basis from $B_1$ with change of basis matrix $Q$ and from $B_2$ with matrix $P$, that $QA^TP^{-1} = (PAQ^{-1})^T$. This does not seem to be the case, however. What have I done incorrectly, if anything?",,['linear-algebra']
93,Matrices A+B=AB implies A commutes with B,Matrices A+B=AB implies A commutes with B,,"$A$ and $B$ are $n\times n$ matrices and $A+B=AB$. I have an interesting proof that this implies $A$ commutes with $B$, but the proof only works when $||B|| \lt 1$. I'm looking for a way to salvage the proof so that it works for all $B$. $$A=AB-B$$ $$A=(A-I)B$$ by repeatedly substituting $A$ with $(A-I)B$ on the RHS, $$A= (A-I)B^N - \sum_{i=1} ^{N-1} B^i$$ Since the space of $n\times n$ matrices is Banach (hence complete) under the standard operator norm, the limit $\sum_{i=1}^{\infty}B^i$ is well defined given $||B|| \lt 1$. Hence, taking the limit as $N \rightarrow \infty$, $$A =  -\sum_{i=1}^{\infty}B^i$$ Now the LHS commutes with B, hence so does the RHS. Is there some trick to extend the method to work with any $B$ where $||B|| \geq 1$? I know that the result is true due to another simple algebraic proof, but I'd like to see if this method can be salvaged.","$A$ and $B$ are $n\times n$ matrices and $A+B=AB$. I have an interesting proof that this implies $A$ commutes with $B$, but the proof only works when $||B|| \lt 1$. I'm looking for a way to salvage the proof so that it works for all $B$. $$A=AB-B$$ $$A=(A-I)B$$ by repeatedly substituting $A$ with $(A-I)B$ on the RHS, $$A= (A-I)B^N - \sum_{i=1} ^{N-1} B^i$$ Since the space of $n\times n$ matrices is Banach (hence complete) under the standard operator norm, the limit $\sum_{i=1}^{\infty}B^i$ is well defined given $||B|| \lt 1$. Hence, taking the limit as $N \rightarrow \infty$, $$A =  -\sum_{i=1}^{\infty}B^i$$ Now the LHS commutes with B, hence so does the RHS. Is there some trick to extend the method to work with any $B$ where $||B|| \geq 1$? I know that the result is true due to another simple algebraic proof, but I'd like to see if this method can be salvaged.",,"['linear-algebra', 'functional-analysis']"
94,Proof for real Jordan canonical form,Proof for real Jordan canonical form,,"Let $A \in \operatorname{Mat}(n\times n, \mathbb{R})$ be a matrix that is diagonalizable in $\mathbb C$ with $k$ real eigenvalues of algebraic multiplicity $1$ and $(n-k)/2$ pairs of complex-conjugated eigenvalues of algebraic multiplicity $1$ . I need to show that the Jordan canonical form of $A$ in $\mathbb R$ is: $$ \begin{pmatrix} \lambda_1 & \ldots & \ldots & \ldots & \ldots &0\\ 0 & \ddots & \ldots & \ldots & \ldots & 0\\ \vdots & \ldots & \lambda_k & \ldots & \ldots & 0\\ \vdots & \ldots & \ldots & B_1 & \ldots & 0\\ \vdots & \ldots & \ldots & \ldots & \ddots & 0\\ 0 & \ldots & \ldots & \ldots & 0 & B_{(n-k)/2} \end{pmatrix} $$ $\in \operatorname{Mat}(n\times n, \mathbb{R})$ , and $B_j = $ $$ \begin{pmatrix} a_j & b_j\\ -b_j & a_j \end{pmatrix} $$ I already determined for $2\times 2$ -matrices that we can write $AP=PB$ , where $P$ is a matrix with vectors of the decomposition of an eigenvector of $A$ . But I don't really know how to prove this general case. Thanks for help.","Let be a matrix that is diagonalizable in with real eigenvalues of algebraic multiplicity and pairs of complex-conjugated eigenvalues of algebraic multiplicity . I need to show that the Jordan canonical form of in is: , and I already determined for -matrices that we can write , where is a matrix with vectors of the decomposition of an eigenvector of . But I don't really know how to prove this general case. Thanks for help.","A \in \operatorname{Mat}(n\times n, \mathbb{R}) \mathbb C k 1 (n-k)/2 1 A \mathbb R 
\begin{pmatrix}
\lambda_1 & \ldots & \ldots & \ldots & \ldots &0\\
0 & \ddots & \ldots & \ldots & \ldots & 0\\
\vdots & \ldots & \lambda_k & \ldots & \ldots & 0\\
\vdots & \ldots & \ldots & B_1 & \ldots & 0\\
\vdots & \ldots & \ldots & \ldots & \ddots & 0\\
0 & \ldots & \ldots & \ldots & 0 & B_{(n-k)/2}
\end{pmatrix}
 \in \operatorname{Mat}(n\times n, \mathbb{R}) B_j =  
\begin{pmatrix}
a_j & b_j\\
-b_j & a_j
\end{pmatrix}
 2\times 2 AP=PB P A","['linear-algebra', 'matrices', 'jordan-normal-form']"
95,Smooth spectral decomposition of a matrix,Smooth spectral decomposition of a matrix,,"Let $A : x \mapsto A(x)$ be a $C^\infty$ map from the half-plane $\left\{ (x_1,x_2,\cdots,x_n) \in \mathbb{R}^n,\ x_n>0\right\}$ to the space of symmetric matrices with real coefficients. Suppose that $\dim \ker A(x)$ is constant on the half-plane. Then for all $x$ we can find an orthogonal matrix $P(x)$ such that $^tP(x) A(x)P(x)$ is a diagonal matrix. Can we find a map $P$ such that $x\mapsto P(x)$ is regular ($C^\infty$ would be perfect but $C^1$ also good) ? Are projections on the eigenspaces regular? If $A(x)$ is definite positive for all $x$ in the half-plane, is the square-root of $A(x)$ regular? If now, we suppose that $P$ is only invertible (but we still have $^tP(x) A(x)P(x)$ is a diagonal matrix), can $P$ be regular ? If the answer is no for one of this question, do you have a counterexample? If this answer is yes but if we assume more regularity of $A$ (for example real analytic), please tell me. Any references would be appreciated.","Let $A : x \mapsto A(x)$ be a $C^\infty$ map from the half-plane $\left\{ (x_1,x_2,\cdots,x_n) \in \mathbb{R}^n,\ x_n>0\right\}$ to the space of symmetric matrices with real coefficients. Suppose that $\dim \ker A(x)$ is constant on the half-plane. Then for all $x$ we can find an orthogonal matrix $P(x)$ such that $^tP(x) A(x)P(x)$ is a diagonal matrix. Can we find a map $P$ such that $x\mapsto P(x)$ is regular ($C^\infty$ would be perfect but $C^1$ also good) ? Are projections on the eigenspaces regular? If $A(x)$ is definite positive for all $x$ in the half-plane, is the square-root of $A(x)$ regular? If now, we suppose that $P$ is only invertible (but we still have $^tP(x) A(x)P(x)$ is a diagonal matrix), can $P$ be regular ? If the answer is no for one of this question, do you have a counterexample? If this answer is yes but if we assume more regularity of $A$ (for example real analytic), please tell me. Any references would be appreciated.",,"['linear-algebra', 'analysis', 'matrices', 'reference-request']"
96,Prove that $\lambda = 0$ is an eigenvalue if and only if A is singular.,Prove that  is an eigenvalue if and only if A is singular.,\lambda = 0,I'm trying to prove that statement: Prove that $\lambda = 0$ is an eigenvalue if and only if $A$ is singular. I'm not sure if my proof is totally correct: Suppose that $\lambda = 0$ if det(A) = $\lambda_1 \cdot \lambda_2 . . .\lambda_n = 0$ then A is singular. If anyone could show me a better proof that would help.,I'm trying to prove that statement: Prove that $\lambda = 0$ is an eigenvalue if and only if $A$ is singular. I'm not sure if my proof is totally correct: Suppose that $\lambda = 0$ if det(A) = $\lambda_1 \cdot \lambda_2 . . .\lambda_n = 0$ then A is singular. If anyone could show me a better proof that would help.,,['linear-algebra']
97,Proving that similar matrices have identical ranks,Proving that similar matrices have identical ranks,,"Prove that if $A$ and $B$ are similar $n\times n$ matrices, then $\text{rank}A=\text{rank}B$. I can't seem to think of any relation between rank and similar matrices. The book does not have a solution to this problem. Any ideas?","Prove that if $A$ and $B$ are similar $n\times n$ matrices, then $\text{rank}A=\text{rank}B$. I can't seem to think of any relation between rank and similar matrices. The book does not have a solution to this problem. Any ideas?",,"['linear-algebra', 'matrices']"
98,How can one convert rational matrices into integer ones?,How can one convert rational matrices into integer ones?,,Let $G$ be a finite subgroup of $GL_n(\Bbb{Q})$. I want to show the existence of a matrix $A\in GL_n(\Bbb{Q})$ with the property that $AGA^{-1} \subseteq M_n(\Bbb{Z})$.,Let $G$ be a finite subgroup of $GL_n(\Bbb{Q})$. I want to show the existence of a matrix $A\in GL_n(\Bbb{Q})$ with the property that $AGA^{-1} \subseteq M_n(\Bbb{Z})$.,,['linear-algebra']
99,Every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$,Every element of  can be expressed uniquely in the form,U + V + W u + v + w,"Suppose that $U$, $V$ and $W$ are subspaces of some given vector space. With the obvious definition of $U + V + W$, show that every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$, where $u ∈ U$, $v ∈ V$ and $w ∈ W$, if and only if $$\dim(U + V + W) = \dim(U) + \dim(V) + \dim(W)$$ 1) Say every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$. Let $u_1,...,u_n$, $v_1,...,v_m$, $w_1,...,w_q$ be respective bases. Then every element in $U + V + W$ can be expressed uniquely as $$(\alpha_1u_1+...+\alpha_nu_n)+(\beta_1v_1+...+\beta_mv_m)+(\gamma_1w_1+...+\gamma_qw_q)$$ showing that $$\dim(U + V + W) = \dim(U) + \dim(V) + \dim(W)$$ 2) How do I prove the converse? Hints are appreciated (also verification, that what I did, is not useless :) ). Thanks!","Suppose that $U$, $V$ and $W$ are subspaces of some given vector space. With the obvious definition of $U + V + W$, show that every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$, where $u ∈ U$, $v ∈ V$ and $w ∈ W$, if and only if $$\dim(U + V + W) = \dim(U) + \dim(V) + \dim(W)$$ 1) Say every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$. Let $u_1,...,u_n$, $v_1,...,v_m$, $w_1,...,w_q$ be respective bases. Then every element in $U + V + W$ can be expressed uniquely as $$(\alpha_1u_1+...+\alpha_nu_n)+(\beta_1v_1+...+\beta_mv_m)+(\gamma_1w_1+...+\gamma_qw_q)$$ showing that $$\dim(U + V + W) = \dim(U) + \dim(V) + \dim(W)$$ 2) How do I prove the converse? Hints are appreciated (also verification, that what I did, is not useless :) ). Thanks!",,"['linear-algebra', 'vector-spaces']"
