,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,properties of a real analytic function,properties of a real analytic function,,"If there are a radius $r>0$ and constants $M,C\in\mathbb R$ for all $y\in U$ with $$|\partial^if(x)|\leq M\cdot i!\cdot C^{|i|}\space\space\space\space \forall x\in\mathbb B_r(y),i\in\mathbb N_0^n$$ then $f\in C^\infty(U)$ is real analaytic. But I don't have any idea how to prove this. I just know a function is called analytic if there are power series (convergent) in each point of U. thanks for helping! :)","If there are a radius $r>0$ and constants $M,C\in\mathbb R$ for all $y\in U$ with $$|\partial^if(x)|\leq M\cdot i!\cdot C^{|i|}\space\space\space\space \forall x\in\mathbb B_r(y),i\in\mathbb N_0^n$$ then $f\in C^\infty(U)$ is real analaytic. But I don't have any idea how to prove this. I just know a function is called analytic if there are power series (convergent) in each point of U. thanks for helping! :)",,"['calculus', 'real-analysis', 'analysis', 'asymptotics', 'harmonic-analysis']"
1,"What is $\limsup\limits_{n\to\infty} \cos (n)$, when $n$ is a natural number?","What is , when  is a natural number?",\limsup\limits_{n\to\infty} \cos (n) n,"I think the answer should be $1$, but am having some difficulties proving it. I can't seem to show that, for any $\delta$ and $n > m$, $|n - k(2\pi)| < \delta$. Is there another approach to this or is there something I'm missing?","I think the answer should be $1$, but am having some difficulties proving it. I can't seem to show that, for any $\delta$ and $n > m$, $|n - k(2\pi)| < \delta$. Is there another approach to this or is there something I'm missing?",,"['real-analysis', 'analysis']"
2,extending a continuous function from a closed subset,extending a continuous function from a closed subset,,Is it true that a continuous function defined on a closed subset of $\mathbb{R^n}$ extends to a continuous function on the whole space?,Is it true that a continuous function defined on a closed subset of $\mathbb{R^n}$ extends to a continuous function on the whole space?,,"['calculus', 'analysis']"
3,"What is the total derivative of a partial derivative of a function, whos total derivative depends on that function?","What is the total derivative of a partial derivative of a function, whos total derivative depends on that function?",,"I have a function $c(t,\lambda)$ with $$\frac{\text dc}{\text dt}=f(c,\lambda),$$ or $$\left(\frac{\text dc}{\text dt}\right)(t,\lambda)=f(c(t,\lambda),\lambda),$$ How to compute    $$\frac{\text d}{\text dt}\left(\frac{\partial c}{\partial \lambda}\right)\ ?$$ Or more importantly why/how?","I have a function $c(t,\lambda)$ with $$\frac{\text dc}{\text dt}=f(c,\lambda),$$ or $$\left(\frac{\text dc}{\text dt}\right)(t,\lambda)=f(c(t,\lambda),\lambda),$$ How to compute    $$\frac{\text d}{\text dt}\left(\frac{\partial c}{\partial \lambda}\right)\ ?$$ Or more importantly why/how?",,"['analysis', 'derivatives']"
4,Locally Bounded Functional Equation $f(x+y) = f(x) + f(y)$ and Continuity,Locally Bounded Functional Equation  and Continuity,f(x+y) = f(x) + f(y),"Let $f$ be a real-valued function on $\mathbb{R}$ s.t. $f(x+y) = f(x) + f(y)$ for all $x,y$ reals. Suppose there are reals $c$ and $M$ s.t. $|f(x)| \leq M $ for all $x$ in $[-c,c]$. Show that $f$ is continuous. I am able to show that $f$ must take the form $f(x) = xf(1)$ for $x$ rational, but am having trouble showing this holds for the irrationals as well. Hints appreciated!","Let $f$ be a real-valued function on $\mathbb{R}$ s.t. $f(x+y) = f(x) + f(y)$ for all $x,y$ reals. Suppose there are reals $c$ and $M$ s.t. $|f(x)| \leq M $ for all $x$ in $[-c,c]$. Show that $f$ is continuous. I am able to show that $f$ must take the form $f(x) = xf(1)$ for $x$ rational, but am having trouble showing this holds for the irrationals as well. Hints appreciated!",,['analysis']
5,On the continuity in $L^p$,On the continuity in,L^p,"Let $C_p$ the class of functions $f\in L^p(\mathbb{R}^d)$ such that $$\lim_{|\mathbf{h}|\to 0} \|f(\mathbf{x+h})-f(\mathbf{x})\|_p = 0 .$$ To prove that $C_p = L^p(\mathbb{R}^d)$ we start proving the result for indicators functions of cubes. The book Measure and Integral of Weeden and Zygmund, just says: ""Clearly the characteristic function of a cube belongs to $C_p$."" Why is that clear? The problem can be reduced to prove that $m(A\cap (A+\mathbf{h}))\to m(A)$. I have proved that $m(A)=\sup\{m(A\cap (A+\mathbf{h})):\mathbf{h}\in\mathbb{R}^d\}$. But that still isn't enough to conclude the desired result.","Let $C_p$ the class of functions $f\in L^p(\mathbb{R}^d)$ such that $$\lim_{|\mathbf{h}|\to 0} \|f(\mathbf{x+h})-f(\mathbf{x})\|_p = 0 .$$ To prove that $C_p = L^p(\mathbb{R}^d)$ we start proving the result for indicators functions of cubes. The book Measure and Integral of Weeden and Zygmund, just says: ""Clearly the characteristic function of a cube belongs to $C_p$."" Why is that clear? The problem can be reduced to prove that $m(A\cap (A+\mathbf{h}))\to m(A)$. I have proved that $m(A)=\sup\{m(A\cap (A+\mathbf{h})):\mathbf{h}\in\mathbb{R}^d\}$. But that still isn't enough to conclude the desired result.",,"['analysis', 'measure-theory']"
6,Need Help sequence of functions/ convergence,Need Help sequence of functions/ convergence,,"Problem: Prove that the sequence of functions $\{ f_n \}$ defined by:   $f_n(x)= n \sin (\sqrt{4\pi^2n^2+x^2})$ converges uniformly on $[ 0, \alpha]$ where $\alpha > 0$. Does $\{ f_n \}$ converge uniformly on $\mathbb{R}$? Here is what I did: I proved that the pointwise limit of $\{ f_{n} \}$ is the function $f( x ) = \frac{x^2}{4\pi}$. Then, in order to prove the uniform convergence, I need to prove that $$ \sup_{x\in [ 0, \alpha] } \left \{ | f_n (x)- f(x) |  \right \} \to 0 ,$$  as $n \to \infty  $ and that's where I am stuck. In the book, there is a hint saying that for $x$ in the mentioned interval $x \in [ 0, \alpha]$, and using the inequality $\sin x \geqslant x-\frac{x^{3}}{3!}$,  we get:  $$ \left| n \sin \sqrt{4\pi^2 n^2+x^2}  -\frac{x^2}{4\pi} \right| \leqslant \frac{a^2}{4\pi } \left ( 1-\frac{2}{\sqrt{1+\frac{a^2}{4\pi^2 n^2}}+1} \right ) + \frac{n}{3!} \frac{\alpha^6}{8n^3 \pi^3} .$$ I don't understand how the book got this inequality based on $\sin x \geqslant x - \frac{x^3}{3!}$ for $x \geqslant 0$. Can anyone give me a detailed proof how to get that inequality given by the book? From that point, I can easily prove the uniform convergence. For the uniform convergence on $\mathbb R$, there is hint saying that I should use $| \sin x | \leqslant \left | x \right |$ to get: $$ \left| n \sin\sqrt{4\pi^2 n^2+x^2} \ -\frac{x^2}{4\pi}\right| \geqslant  \frac{x^2}{4\pi} \left( 1-\frac{2}{\sqrt{1+\frac{x^2}{4\pi^2 n^2}}+1} \right) .$$ Any help please how can we derive the last inequality too? because from this inequality, I can easily prove the non-uniform convergence on $\mathbb R$.","Problem: Prove that the sequence of functions $\{ f_n \}$ defined by:   $f_n(x)= n \sin (\sqrt{4\pi^2n^2+x^2})$ converges uniformly on $[ 0, \alpha]$ where $\alpha > 0$. Does $\{ f_n \}$ converge uniformly on $\mathbb{R}$? Here is what I did: I proved that the pointwise limit of $\{ f_{n} \}$ is the function $f( x ) = \frac{x^2}{4\pi}$. Then, in order to prove the uniform convergence, I need to prove that $$ \sup_{x\in [ 0, \alpha] } \left \{ | f_n (x)- f(x) |  \right \} \to 0 ,$$  as $n \to \infty  $ and that's where I am stuck. In the book, there is a hint saying that for $x$ in the mentioned interval $x \in [ 0, \alpha]$, and using the inequality $\sin x \geqslant x-\frac{x^{3}}{3!}$,  we get:  $$ \left| n \sin \sqrt{4\pi^2 n^2+x^2}  -\frac{x^2}{4\pi} \right| \leqslant \frac{a^2}{4\pi } \left ( 1-\frac{2}{\sqrt{1+\frac{a^2}{4\pi^2 n^2}}+1} \right ) + \frac{n}{3!} \frac{\alpha^6}{8n^3 \pi^3} .$$ I don't understand how the book got this inequality based on $\sin x \geqslant x - \frac{x^3}{3!}$ for $x \geqslant 0$. Can anyone give me a detailed proof how to get that inequality given by the book? From that point, I can easily prove the uniform convergence. For the uniform convergence on $\mathbb R$, there is hint saying that I should use $| \sin x | \leqslant \left | x \right |$ to get: $$ \left| n \sin\sqrt{4\pi^2 n^2+x^2} \ -\frac{x^2}{4\pi}\right| \geqslant  \frac{x^2}{4\pi} \left( 1-\frac{2}{\sqrt{1+\frac{x^2}{4\pi^2 n^2}}+1} \right) .$$ Any help please how can we derive the last inequality too? because from this inequality, I can easily prove the non-uniform convergence on $\mathbb R$.",,"['calculus', 'real-analysis', 'analysis']"
7,A question about hyperbolic functions,A question about hyperbolic functions,,"Suppose $(x,y,z),(a,b,c)$ satisfy $$x^2+y^2-z^2=-1, z\ge 1,$$ $$ax+by-cz=0,$$ $$a^2+b^2-c^2=1.$$ Does it follow that $$z\cosh(t)+c\sinh(t)\ge 1$$ for all real number $t$?","Suppose $(x,y,z),(a,b,c)$ satisfy $$x^2+y^2-z^2=-1, z\ge 1,$$ $$ax+by-cz=0,$$ $$a^2+b^2-c^2=1.$$ Does it follow that $$z\cosh(t)+c\sinh(t)\ge 1$$ for all real number $t$?",,"['analysis', 'hyperbolic-geometry']"
8,Is this set equicontinuous?,Is this set equicontinuous?,,"Let for $n\geq 1$: $f_n(x):=\dfrac{x}{1+nx^2}$ and $\mathcal{F}:=\{f_n:n=1,2,3,\ldots\}$ I'd like to know if $\mathcal{F}$ is equicontinuous. This is what I have done: $$\left|\frac{x}{1+nx^2}-\frac{x_0}{1+nx_0^2}\right|<\varepsilon$$ this becames $|x-x_0|\left|\frac{1-nxx_0}{(1+nx^2)(1+nx_0^2)}\right|<\varepsilon$. Now I thought to define this function $g(n)=\frac{1-nxx_0}{(1+nx^2)(1+nx_0^2)}$ where we think $n\in\mathbb{R}$ and $x,x_0$ as parameters. Now $f(n)\rightarrow0$ if $n\rightarrow\infty$ and it is continuous, so there will be an interval $[-M,M]$ such that this function will be less than a certain $\nu$ ($\nu$ and $M$ depend on $x$ and $x_0$). And on $[-M,M]$ $g(n)$ will have a maximum $N$ (depending on $x,x_0$). So we have $|x-x_0|\left|\frac{1-nxx_0}{(1+nx^2)(1+nx_0^2)}\right|<\delta\cdot\mathrm{max}\{{N,\nu}\}$ But now I don't know how to continue, please could you help me?","Let for $n\geq 1$: $f_n(x):=\dfrac{x}{1+nx^2}$ and $\mathcal{F}:=\{f_n:n=1,2,3,\ldots\}$ I'd like to know if $\mathcal{F}$ is equicontinuous. This is what I have done: $$\left|\frac{x}{1+nx^2}-\frac{x_0}{1+nx_0^2}\right|<\varepsilon$$ this becames $|x-x_0|\left|\frac{1-nxx_0}{(1+nx^2)(1+nx_0^2)}\right|<\varepsilon$. Now I thought to define this function $g(n)=\frac{1-nxx_0}{(1+nx^2)(1+nx_0^2)}$ where we think $n\in\mathbb{R}$ and $x,x_0$ as parameters. Now $f(n)\rightarrow0$ if $n\rightarrow\infty$ and it is continuous, so there will be an interval $[-M,M]$ such that this function will be less than a certain $\nu$ ($\nu$ and $M$ depend on $x$ and $x_0$). And on $[-M,M]$ $g(n)$ will have a maximum $N$ (depending on $x,x_0$). So we have $|x-x_0|\left|\frac{1-nxx_0}{(1+nx^2)(1+nx_0^2)}\right|<\delta\cdot\mathrm{max}\{{N,\nu}\}$ But now I don't know how to continue, please could you help me?",,"['real-analysis', 'analysis']"
9,Weird infimum of fractional part expression,Weird infimum of fractional part expression,,"I found this problem a while ago on AoPS. Let $k$ be a squarefree positive integer. Find $\inf_{n \in \Bbb{Z}_+^*}  n\{n\sqrt{k}\,\}$ , where $\{\cdot\}$ denotes the fractional part. I tried to find a solution, but it is still unsolved. My guess for the infimum is $0$ , and I tried proving the existence of a sequence $(n_i)$ such that $\displaystyle \{n_i \sqrt{k}\} < \frac{1}{n_i^2}$ . Denote $p_i$ the integer part of $n_i\sqrt{k}$ . Then we have $\displaystyle p_i<n_i\sqrt{k}<p_i+\frac{1}{n_i^2}$ . Squaring we get $\displaystyle p_i^2<n_i^2k<p_i^2+2p_i\frac{1}{n_i^2}+\frac{1}{n_i^4}$ . The first inequality must be strict, and the best case would be $p_i^2-kn_i^2=-1$ , which is a Pell equation, solvable for some squarefree $k$ , but not for all squarefree $k$ . Do you have some ideas of what to do next?","I found this problem a while ago on AoPS. Let be a squarefree positive integer. Find , where denotes the fractional part. I tried to find a solution, but it is still unsolved. My guess for the infimum is , and I tried proving the existence of a sequence such that . Denote the integer part of . Then we have . Squaring we get . The first inequality must be strict, and the best case would be , which is a Pell equation, solvable for some squarefree , but not for all squarefree . Do you have some ideas of what to do next?","k \inf_{n \in \Bbb{Z}_+^*}  n\{n\sqrt{k}\,\} \{\cdot\} 0 (n_i) \displaystyle \{n_i \sqrt{k}\} < \frac{1}{n_i^2} p_i n_i\sqrt{k} \displaystyle p_i<n_i\sqrt{k}<p_i+\frac{1}{n_i^2} \displaystyle p_i^2<n_i^2k<p_i^2+2p_i\frac{1}{n_i^2}+\frac{1}{n_i^4} p_i^2-kn_i^2=-1 k k","['number-theory', 'analysis']"
10,How to prove that $C = \bigcup_{a \in A \times \{0\}} \{(1-t)a+tv| 0\leq t \leq 1\}$ is Jordan-measurable if $A$ is Jordan-measurable?,How to prove that  is Jordan-measurable if  is Jordan-measurable?,C = \bigcup_{a \in A \times \{0\}} \{(1-t)a+tv| 0\leq t \leq 1\} A,"Let $A \subseteq \mathbb{R}^{n-1}$ be Jordan-measurable and $$C = \bigcup_{a \in A \times \{0\}} \{(1-t)a+tv| \; 0\leq t \leq 1\}$$ be an object in $\mathbb{R}^n$ with the base area $A \times \{0\}$ and the apex $v \in \mathbb{R}^n$. I have to show that $C$ is Jordan-measurable and $\mu_n(C) = \frac{|v_n|}{n} \mu_{n-1}(A)$. Now our assistant professor told us that one usually proves ""such"" propositions in three steps: For every cuboid $A$. For $A$ being a finite union of disjoint cuboids $Q_i$. For every Jordan-measurable $A$. Now I don't understand how this helps or more specific: How do I apply this strategy? What exactly do I want to prove for every cuboid $A$ (and for $A$ being a finite union of disjoint $Q_i$ and eventually for every Jordan-measurable $A$)? Also, why does this strategy work? What idea is there behind this strategy? Thanks for any help in advance.","Let $A \subseteq \mathbb{R}^{n-1}$ be Jordan-measurable and $$C = \bigcup_{a \in A \times \{0\}} \{(1-t)a+tv| \; 0\leq t \leq 1\}$$ be an object in $\mathbb{R}^n$ with the base area $A \times \{0\}$ and the apex $v \in \mathbb{R}^n$. I have to show that $C$ is Jordan-measurable and $\mu_n(C) = \frac{|v_n|}{n} \mu_{n-1}(A)$. Now our assistant professor told us that one usually proves ""such"" propositions in three steps: For every cuboid $A$. For $A$ being a finite union of disjoint cuboids $Q_i$. For every Jordan-measurable $A$. Now I don't understand how this helps or more specific: How do I apply this strategy? What exactly do I want to prove for every cuboid $A$ (and for $A$ being a finite union of disjoint $Q_i$ and eventually for every Jordan-measurable $A$)? Also, why does this strategy work? What idea is there behind this strategy? Thanks for any help in advance.",,['analysis']
11,Help with the proof of a Gaussian type inequality and some numerical results,Help with the proof of a Gaussian type inequality and some numerical results,,"I am trying to figure out if I made a mistake in the following proof - in particular I have been trying to verify the inequality (*) below.  I have attached my proof but I have been getting some error messages in Mathematica that make me think something is wrong when I try to compare the left hand side of the inequality to the right hand side.  The background of this problem is related to the study of moments of self similar processes and I have no idea if the estimates are valid besides from some crude numerical calculations for $N= 2,3,4$ and all $H_i = 1/2$.  I have attached the proof for the case N =2. 1) Did I make a mistake in one of the estimates or applying fubini somewhere?  In particular I am starting to worry about line (**) is breaking the integral up like this and then using $x_1 < x_2$ and $x_2 < x_1$ in the respective regions of integration to arrive at the estimate on the next line valid? Let $H_1 \in (0,1), c$ a positive constant and let $\Phi(t) = \int_{t}^{\infty} e^{\frac{-x^2}{2}}dx$.  Then we have the following inequality (*) $\int_{1}^{\infty} \int_{1}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 \leq 2 \Phi(1) \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} }dx_2$ Proof: $\int_{1}^{\infty} \int_{1}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2$ (**) $ = \int_{1}^{\infty} \int_{1}^{x_2}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2  + \int_{1}^{\infty} \int_{x_2}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 $ $ \leq \int_{1}^{\infty} \int_{1}^{x_2}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } dx_1 dx_2 + \int_{1}^{\infty} \int_{x_2}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c}  |x_1|^{1+H_1} } dx_1 dx_2 $ $ =  \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } (\int_{1}^{x_2} e^{-\frac{ x_{1}^{2}}{2}}dx_1)dx_2 + \int_{1}^{\infty}  e^{ -\frac{ x_{1}^{2}}{2} + \frac{1}{c} |x_1| ^{1+H_1} } (\int_{x_2}^{\infty} e^{-\frac{ x_{2}^{2}}{2}}dx_2)dx_1 $ $ \leq 2  \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } (\int_{1}^{\infty} e^{-\frac{ x_{1}^{2}}{2}}dx_1)dx_2   = 2 \Phi(1) \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} }dx_2$ 2)I have tried comparing the LHS to the right hand side for the case $N = 2, N= 4$ and all H_i = 1/2 but Mathematica gives me the following errors attached below.  I am thinking I will just need to test the above inequality's for  $N > 3$ in matlab but I am not sure what the easiest method would be.  Would monte carlo integration apply to these type of integrals even though they are over an unbounded region?  Any numerical suggestions/references are greatly appreciated NIntegrate[   2*(1/Sqrt[2*Pi])^2*Exp [1/2 (-x ^2 - y^2)] + 1/10*x*y^(1/2), {x, 0,    Infinity}, {y, 0, Infinity}] NIntegrate::slwcon: Numerical integration converging too slowly; suspect one of the following: singularity, value of the integration is 0, highly oscillatory integrand, or WorkingPrecision too small. >> NIntegrate::ncvb: NIntegrate failed to converge to prescribed accuracy after 18 recursive bisections in x near {x,y} = {4.557658617658054*10^1348705,0.184661}. NIntegrate obtained 1.541751650430305 15.954589770191005*^88741243 and 3.653228868630915 15.954589770191005*^88741243 for the integral and error estimates. >> 0.*10^88741243 NIntegrate[  2*(1/Sqrt[2*Pi])^4 Exp [    1/2 (-x^2 - y^2 - z^2 - w^2) +      1/10 x Sqrt[y] z^(1/4)* w^(1/8)], {x, 0, [Infinity]}, {y,    0, [Infinity]}, {z, 0, [Infinity]}, {w, 0, [Infinity]}] NIntegrate::slwcon: Numerical integration converging too slowly; suspect one of the following: singularity, value of the integration is 0, highly oscillatory integrand, or WorkingPrecision too small. >> NIntegrate::eincr: The global error of the strategy GlobalAdaptive has increased more than 2000 times. The global error is expected to decrease monotonically after a number of integrand evaluations. Suspect one of the following: the working precision is insufficient for the specified precision goal; the integrand is highly oscillatory or it is not a (piecewise) smooth function; or the true value of the integral is 0. Increasing the value of the GlobalAdaptive option MaxErrorIncreases might lead to a convergent numerical integration. NIntegrate obtained 0.1321276729737014 and 4.5493840913708495 *^-7 for the integral and error estimates. >> 0.132128","I am trying to figure out if I made a mistake in the following proof - in particular I have been trying to verify the inequality (*) below.  I have attached my proof but I have been getting some error messages in Mathematica that make me think something is wrong when I try to compare the left hand side of the inequality to the right hand side.  The background of this problem is related to the study of moments of self similar processes and I have no idea if the estimates are valid besides from some crude numerical calculations for $N= 2,3,4$ and all $H_i = 1/2$.  I have attached the proof for the case N =2. 1) Did I make a mistake in one of the estimates or applying fubini somewhere?  In particular I am starting to worry about line (**) is breaking the integral up like this and then using $x_1 < x_2$ and $x_2 < x_1$ in the respective regions of integration to arrive at the estimate on the next line valid? Let $H_1 \in (0,1), c$ a positive constant and let $\Phi(t) = \int_{t}^{\infty} e^{\frac{-x^2}{2}}dx$.  Then we have the following inequality (*) $\int_{1}^{\infty} \int_{1}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 \leq 2 \Phi(1) \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} }dx_2$ Proof: $\int_{1}^{\infty} \int_{1}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2$ (**) $ = \int_{1}^{\infty} \int_{1}^{x_2}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2  + \int_{1}^{\infty} \int_{x_2}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_1|  |x_2|^{H_1} } dx_1 dx_2 $ $ \leq \int_{1}^{\infty} \int_{1}^{x_2}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } dx_1 dx_2 + \int_{1}^{\infty} \int_{x_2}^{\infty}  e^{-\frac{ x_{1}^{2}}{2} -\frac{ x_{2}^{2}}{2} + \frac{1}{c}  |x_1|^{1+H_1} } dx_1 dx_2 $ $ =  \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } (\int_{1}^{x_2} e^{-\frac{ x_{1}^{2}}{2}}dx_1)dx_2 + \int_{1}^{\infty}  e^{ -\frac{ x_{1}^{2}}{2} + \frac{1}{c} |x_1| ^{1+H_1} } (\int_{x_2}^{\infty} e^{-\frac{ x_{2}^{2}}{2}}dx_2)dx_1 $ $ \leq 2  \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} } (\int_{1}^{\infty} e^{-\frac{ x_{1}^{2}}{2}}dx_1)dx_2   = 2 \Phi(1) \int_{1}^{\infty}  e^{ -\frac{ x_{2}^{2}}{2} + \frac{1}{c} |x_2| ^{1+H_1} }dx_2$ 2)I have tried comparing the LHS to the right hand side for the case $N = 2, N= 4$ and all H_i = 1/2 but Mathematica gives me the following errors attached below.  I am thinking I will just need to test the above inequality's for  $N > 3$ in matlab but I am not sure what the easiest method would be.  Would monte carlo integration apply to these type of integrals even though they are over an unbounded region?  Any numerical suggestions/references are greatly appreciated NIntegrate[   2*(1/Sqrt[2*Pi])^2*Exp [1/2 (-x ^2 - y^2)] + 1/10*x*y^(1/2), {x, 0,    Infinity}, {y, 0, Infinity}] NIntegrate::slwcon: Numerical integration converging too slowly; suspect one of the following: singularity, value of the integration is 0, highly oscillatory integrand, or WorkingPrecision too small. >> NIntegrate::ncvb: NIntegrate failed to converge to prescribed accuracy after 18 recursive bisections in x near {x,y} = {4.557658617658054*10^1348705,0.184661}. NIntegrate obtained 1.541751650430305 15.954589770191005*^88741243 and 3.653228868630915 15.954589770191005*^88741243 for the integral and error estimates. >> 0.*10^88741243 NIntegrate[  2*(1/Sqrt[2*Pi])^4 Exp [    1/2 (-x^2 - y^2 - z^2 - w^2) +      1/10 x Sqrt[y] z^(1/4)* w^(1/8)], {x, 0, [Infinity]}, {y,    0, [Infinity]}, {z, 0, [Infinity]}, {w, 0, [Infinity]}] NIntegrate::slwcon: Numerical integration converging too slowly; suspect one of the following: singularity, value of the integration is 0, highly oscillatory integrand, or WorkingPrecision too small. >> NIntegrate::eincr: The global error of the strategy GlobalAdaptive has increased more than 2000 times. The global error is expected to decrease monotonically after a number of integrand evaluations. Suspect one of the following: the working precision is insufficient for the specified precision goal; the integrand is highly oscillatory or it is not a (piecewise) smooth function; or the true value of the integral is 0. Increasing the value of the GlobalAdaptive option MaxErrorIncreases might lead to a convergent numerical integration. NIntegrate obtained 0.1321276729737014 and 4.5493840913708495 *^-7 for the integral and error estimates. >> 0.132128",,"['calculus', 'real-analysis', 'analysis', 'numerical-methods']"
12,a problem in fractional calculus,a problem in fractional calculus,,One of the early applications of fractional calculus is the tautochrone problem set up by Abel in the integral form or its fractional derivative one. i wish to know its solution.,One of the early applications of fractional calculus is the tautochrone problem set up by Abel in the integral form or its fractional derivative one. i wish to know its solution.,,"['calculus', 'analysis', 'fractional-calculus']"
13,Find a closed form for $\sum_{k=1}^{x-1} a^{1/k}$,Find a closed form for,\sum_{k=1}^{x-1} a^{1/k},Please find a closed form for partial sum of a function $$f(x)=a^{1/x}$$ I want it to be expressed in terms of bounded number of elementary functions and/or well known special functions. No computer algebra systems I have tried so far could find a satisfactory solution. I believe that the expression can exist in the terms of incomplete Gamma function or its generalizations because indefinite integral  of this function can be expressed in terms of incomplete Gamma function: $$\int f(x) dx= x\sqrt[x]{a}-\operatorname{Ei}\left(\frac{\ln a}{x}\right)\ln a$$,Please find a closed form for partial sum of a function $$f(x)=a^{1/x}$$ I want it to be expressed in terms of bounded number of elementary functions and/or well known special functions. No computer algebra systems I have tried so far could find a satisfactory solution. I believe that the expression can exist in the terms of incomplete Gamma function or its generalizations because indefinite integral  of this function can be expressed in terms of incomplete Gamma function: $$\int f(x) dx= x\sqrt[x]{a}-\operatorname{Ei}\left(\frac{\ln a}{x}\right)\ln a$$,,"['analysis', 'sequences-and-series']"
14,Evaluating a given limit,Evaluating a given limit,,"Let $$\nu(x) = \frac{1}{2} \log{2\pi x} - x + \int\limits_{1}^{x} \frac{\lfloor{t\rfloor}}{t} \ dt$$ then what is $\displaystyle \lim_{x \to \infty} x \cdot \nu(x)$? I really don't know to proceed. Tried something using the expansions of $\log$ and seeing how the integral behaves, but couldn't figure out.","Let $$\nu(x) = \frac{1}{2} \log{2\pi x} - x + \int\limits_{1}^{x} \frac{\lfloor{t\rfloor}}{t} \ dt$$ then what is $\displaystyle \lim_{x \to \infty} x \cdot \nu(x)$? I really don't know to proceed. Tried something using the expansions of $\log$ and seeing how the integral behaves, but couldn't figure out.",,['number-theory']
15,Proving that a certain function is convex,Proving that a certain function is convex,,"Let $k=\arccos(\frac{1}{4})$ , and let $f(x)=\arccos(\cos(x)\cdot\cos(kx))$ . Prove that $f$ is concave ( $f''(x)<0$ ) in the interval $\left(0,1\right)$ . Note that the function $f(x)$ is the length of the `hypotenuse' of a right spherical triangle with legs $x,kx$ (see the spherical cosine rule ) Context: I am trying to prove some lemmas about spherical trigonometry and I need this function to be convex in order to apply Jensen's inequality.","Let , and let . Prove that is concave ( ) in the interval . Note that the function is the length of the `hypotenuse' of a right spherical triangle with legs (see the spherical cosine rule ) Context: I am trying to prove some lemmas about spherical trigonometry and I need this function to be convex in order to apply Jensen's inequality.","k=\arccos(\frac{1}{4}) f(x)=\arccos(\cos(x)\cdot\cos(kx)) f f''(x)<0 \left(0,1\right) f(x) x,kx","['analysis', 'derivatives', 'inequality', 'convex-analysis']"
16,When is measurability of a function $f$ equivalent to $f$ being almost everywhere the limit of measurable functions?,When is measurability of a function  equivalent to  being almost everywhere the limit of measurable functions?,f f,"I have a suspicion that the following is true: $\def\AAA {\mathcal{A}} \def\BBB {\mathcal{B}} \def\NN {\mathbb{N}}$ Theorem: let $(X,\AAA,\mu)$ be a complete measure space, and $(Y,\BBB)$ a measurable space where $\BBB$ is the Borel $\sigma$ -algebra of some metrizable topology of $Y$ . Then, the following are equivalent: $f:X\to Y$ is measurable. $f$ is almost everywhere the pointwise limit of measurable functions $f_n$ . What I do know. The result holds if $f_n\to f$ everywhere. For a proof, see here . The result does not hold if the domain space is not complete (a counterexample can be constructed from this post ). My attempt. I have been trying to modify this proof so as to deal with the case where $f_n\to f$ only almost everywhere. I first partition $X$ into $$X_* := \{ x\in X : f_n(x)\to f\} \ \ \ \ \text{ and } \ \ \ \ X_0 := X\setminus X_*.$$ For any closed $C\subseteq Y$ , completeness of $X$ allows us to claim $$\Bigg(f^{-1}(C)\Bigg) \triangle \Bigg(\bigcap_{n\in\NN}\bigcup_{N\in\NN}\bigcap_{k\ge N}f_k^{-1}(C_{2^{-n}})\Bigg)$$ is a null set, call it $C_0$ . As the set on the right, call it $M$ , is measurable by hypothesis, we have $$\big(f^{-1}(C)\setminus M\big)\cup\big(M\setminus f^{-1}(C)\big) = C_0$$ which hopefully, somehow, implies $f^{-1}(C)$ is measurable as well. Is the 'Theorem' correct? If so, how could one prove it? If not, what is a counterexample? Related. This post . I struggle to understand large part of the terminology, so it hasn't been of much help.","I have a suspicion that the following is true: Theorem: let be a complete measure space, and a measurable space where is the Borel -algebra of some metrizable topology of . Then, the following are equivalent: is measurable. is almost everywhere the pointwise limit of measurable functions . What I do know. The result holds if everywhere. For a proof, see here . The result does not hold if the domain space is not complete (a counterexample can be constructed from this post ). My attempt. I have been trying to modify this proof so as to deal with the case where only almost everywhere. I first partition into For any closed , completeness of allows us to claim is a null set, call it . As the set on the right, call it , is measurable by hypothesis, we have which hopefully, somehow, implies is measurable as well. Is the 'Theorem' correct? If so, how could one prove it? If not, what is a counterexample? Related. This post . I struggle to understand large part of the terminology, so it hasn't been of much help.","\def\AAA {\mathcal{A}} \def\BBB {\mathcal{B}} \def\NN {\mathbb{N}} (X,\AAA,\mu) (Y,\BBB) \BBB \sigma Y f:X\to Y f f_n f_n\to f f_n\to f X X_* := \{ x\in X : f_n(x)\to f\} \ \ \ \ \text{ and } \ \ \ \ X_0 := X\setminus X_*. C\subseteq Y X \Bigg(f^{-1}(C)\Bigg) \triangle \Bigg(\bigcap_{n\in\NN}\bigcup_{N\in\NN}\bigcap_{k\ge N}f_k^{-1}(C_{2^{-n}})\Bigg) C_0 M \big(f^{-1}(C)\setminus M\big)\cup\big(M\setminus f^{-1}(C)\big) = C_0 f^{-1}(C)","['analysis', 'measure-theory', 'solution-verification', 'measurable-functions', 'borel-measures']"
17,Is every $\alpha$-Hölder continuous function of bounded variation absolutely continuous?,Is every -Hölder continuous function of bounded variation absolutely continuous?,\alpha,"Let $f:[a,b] \to \mathbb{R}$ be $\alpha$ -Hölder continuous function of bounded variation, does it follow that $f$ is absolutely continuous ? Here $\alpha \in (0,1)$ is fixed . Here are some sources : If $f$ is $\alpha$ -Hölder continuous for $\alpha \geqslant 1$ , then the absolute continuity follows straight from the definition. If $f$ is only $\alpha$ -Hölder continuous for $\alpha \in (0,1)$ but not necessarily of bounded variation, the Weirstrass function ( Hölder continuity of Weierstrass Function ) is a counterexample. If $f$ is only (uniformly) continuous and of bounded variation, then the devil's staircase is a counterexample. This counterexample is not $\alpha$ -Hölder continuous, see https://mathoverflow.net/questions/45020/non-h%C3%B6lder-continuous-devils-staircases .","Let be -Hölder continuous function of bounded variation, does it follow that is absolutely continuous ? Here is fixed . Here are some sources : If is -Hölder continuous for , then the absolute continuity follows straight from the definition. If is only -Hölder continuous for but not necessarily of bounded variation, the Weirstrass function ( Hölder continuity of Weierstrass Function ) is a counterexample. If is only (uniformly) continuous and of bounded variation, then the devil's staircase is a counterexample. This counterexample is not -Hölder continuous, see https://mathoverflow.net/questions/45020/non-h%C3%B6lder-continuous-devils-staircases .","f:[a,b] \to \mathbb{R} \alpha f \alpha \in (0,1) f \alpha \alpha \geqslant 1 f \alpha \alpha \in (0,1) f \alpha","['real-analysis', 'functional-analysis', 'analysis', 'continuity', 'absolute-continuity']"
18,Proof of uniqueness of power series representation of function,Proof of uniqueness of power series representation of function,,"My question relates to the theorem and proof given in Erwin Kreyszig's Advanced engineering mathematics: Theorem 1 is as follows: My question is as follows: In the proof of Theorem 2, what is the justification for concluding $a_{m+1}$ = $b_{m+1}$ , given that $a_{m+1}$ = $b_{m+1}$ only holds when z = 0 and the second equation has not been shown to be equivalent to the first at z = 0?","My question relates to the theorem and proof given in Erwin Kreyszig's Advanced engineering mathematics: Theorem 1 is as follows: My question is as follows: In the proof of Theorem 2, what is the justification for concluding = , given that = only holds when z = 0 and the second equation has not been shown to be equivalent to the first at z = 0?",a_{m+1} b_{m+1} a_{m+1} b_{m+1},"['analysis', 'power-series']"
19,"Find a $(X,\mathscr{A})$ and finite measures $\mu$ and $\nu$ such that $\mu(X)=\nu(X)$ but $\{A\in\mathscr{A}:\mu(A)=\nu(A)\}$ not a sigma algebra",Find a  and finite measures  and  such that  but  not a sigma algebra,"(X,\mathscr{A}) \mu \nu \mu(X)=\nu(X) \{A\in\mathscr{A}:\mu(A)=\nu(A)\}","Problem The Problem is to find a measurable space $(X,\mathscr{A})$ together with finite measures $\mu$ and $\nu$ that agree on the whole set $\mu(X)=\nu(X)$ but are so that the collection \begin{align*} \{A\in\mathscr{A}:\mu(A)=\nu(A)\} \end{align*} is not a $\sigma$ -algebra. My Attempt Let \begin{align*} X=\{1,2,3,4\} \end{align*} and \begin{align*} \mathscr{A}=\left\{\emptyset,X,\{1\},\{2,3,4\},\{2\},\{1,3,4\},\{3\},\{1,2,4\},\{4\},\{1,2,3\},\{1,2\},\{3,4\},\{1,3\},\{2,4\},\{1,4\},\{2,3\}\right\}.  \end{align*} Then, $\mathscr{A}$ is a $\sigma$ -algebra on $X$ , because it contains $X$ , it is closed under complementation, and it is closed under the formation of countable union. Define $\mu:\mathscr{A}\to[0,+\infty]$ by \begin{align*} \begin{alignedat}{16} &\mu(\emptyset)=0,      &&\mu(X)=4,              &&                  &&                 \\ &\mu(\{1\})=1,          &&\mu(\{2\})=3,          &&\mu(\{3\})=0,     &&\mu(\{4\})=0,    \\ &\mu(\{2,3,4\})=3,\quad &&\mu(\{1,3,4\})=1,\quad &&\mu(\{1,2,4\})=4,\quad &&\mu(\{1,2,3\})=4,\\ &\mu(\{1,2\})=4,        &&\mu(\{1,3\})=1,   &&\mu(\{1,4\})=1,   &&                 \\ &\mu(\{3,4\})=0,   &&\mu(\{2,4\})=3,   &&\mu(\{2,3\})=3.   &&                  \end{alignedat} \end{align*} Define $\nu:\mathscr{A}\to[0,+\infty]$ by \begin{align*} \begin{alignedat}{16} &\nu(\emptyset)=0, &&\nu(X)=4,         &&                  &&                 \\ &\nu(\{1\})=2,     &&\nu(\{2\})=2,     &&\nu(\{3\})=0,     &&\nu(\{4\})=0,    \\ &\nu(\{2,3,4\})=2,\quad &&\nu(\{1,3,4\})=2,\quad &&\nu(\{1,2,4\})=4,\quad &&\nu(\{1,2,3\})=4,\\ &\nu(\{1,2\})=4,   &&\nu(\{1,3\})=2,   &&\nu(\{1,4\})=1,   &&                 \\ &\nu(\{3,4\})=0,   &&\nu(\{2,4\})=2,   &&\nu(\{2,3\})=3.   &&                  \end{alignedat} \end{align*} Then, $\mu$ and $\nu$ are both measure on $(X,\mathscr{A})$ , because they are both countably additive for sequences of disjoint sets in $\mathscr{A}$ and $\mu(\emptyset)=\nu(\emptyset)=0$ . We have that \begin{align*} \{A\in\mathscr{A}:\mu(A)=\nu(A)\}=\{\emptyset,X,\{3\},\{1,2,4\},\{4\},\{1,2,3\},\{1,2\},\{3,4\},\{1,4\},\{2,3\}\}. \end{align*} This collection is not a $\sigma$ -algebra on $X$ , because $\{2,3,4\}\notin\mathscr{A}$ but it is a countable union of elements in this collection. Question Could someone please help me check if my example is valid? In addition, is there a simpler solution to this problem? Thanks a lot in advance! Reference Measure Theory by Donald Cohn Section 1.6 Dynkin Classes Example 1.6.1. Update Thanks for @Zerox's checking. My example was wrong.","Problem The Problem is to find a measurable space together with finite measures and that agree on the whole set but are so that the collection is not a -algebra. My Attempt Let and Then, is a -algebra on , because it contains , it is closed under complementation, and it is closed under the formation of countable union. Define by Define by Then, and are both measure on , because they are both countably additive for sequences of disjoint sets in and . We have that This collection is not a -algebra on , because but it is a countable union of elements in this collection. Question Could someone please help me check if my example is valid? In addition, is there a simpler solution to this problem? Thanks a lot in advance! Reference Measure Theory by Donald Cohn Section 1.6 Dynkin Classes Example 1.6.1. Update Thanks for @Zerox's checking. My example was wrong.","(X,\mathscr{A}) \mu \nu \mu(X)=\nu(X) \begin{align*}
\{A\in\mathscr{A}:\mu(A)=\nu(A)\}
\end{align*} \sigma \begin{align*}
X=\{1,2,3,4\}
\end{align*} \begin{align*}
\mathscr{A}=\left\{\emptyset,X,\{1\},\{2,3,4\},\{2\},\{1,3,4\},\{3\},\{1,2,4\},\{4\},\{1,2,3\},\{1,2\},\{3,4\},\{1,3\},\{2,4\},\{1,4\},\{2,3\}\right\}. 
\end{align*} \mathscr{A} \sigma X X \mu:\mathscr{A}\to[0,+\infty] \begin{align*}
\begin{alignedat}{16}
&\mu(\emptyset)=0,      &&\mu(X)=4,              &&                  &&                 \\
&\mu(\{1\})=1,          &&\mu(\{2\})=3,          &&\mu(\{3\})=0,     &&\mu(\{4\})=0,    \\
&\mu(\{2,3,4\})=3,\quad &&\mu(\{1,3,4\})=1,\quad &&\mu(\{1,2,4\})=4,\quad &&\mu(\{1,2,3\})=4,\\
&\mu(\{1,2\})=4,        &&\mu(\{1,3\})=1,   &&\mu(\{1,4\})=1,   &&                 \\
&\mu(\{3,4\})=0,   &&\mu(\{2,4\})=3,   &&\mu(\{2,3\})=3.   &&                 
\end{alignedat}
\end{align*} \nu:\mathscr{A}\to[0,+\infty] \begin{align*}
\begin{alignedat}{16}
&\nu(\emptyset)=0, &&\nu(X)=4,         &&                  &&                 \\
&\nu(\{1\})=2,     &&\nu(\{2\})=2,     &&\nu(\{3\})=0,     &&\nu(\{4\})=0,    \\
&\nu(\{2,3,4\})=2,\quad &&\nu(\{1,3,4\})=2,\quad &&\nu(\{1,2,4\})=4,\quad &&\nu(\{1,2,3\})=4,\\
&\nu(\{1,2\})=4,   &&\nu(\{1,3\})=2,   &&\nu(\{1,4\})=1,   &&                 \\
&\nu(\{3,4\})=0,   &&\nu(\{2,4\})=2,   &&\nu(\{2,3\})=3.   &&                 
\end{alignedat}
\end{align*} \mu \nu (X,\mathscr{A}) \mathscr{A} \mu(\emptyset)=\nu(\emptyset)=0 \begin{align*}
\{A\in\mathscr{A}:\mu(A)=\nu(A)\}=\{\emptyset,X,\{3\},\{1,2,4\},\{4\},\{1,2,3\},\{1,2\},\{3,4\},\{1,4\},\{2,3\}\}.
\end{align*} \sigma X \{2,3,4\}\notin\mathscr{A}","['real-analysis', 'analysis', 'measure-theory', 'solution-verification', 'alternative-proof']"
20,Finding Value of the Infinite Product $ \prod_{n=1}^{\infty}\frac{ e^\frac{1}{n}}{1+\frac{1}{n}}$,Finding Value of the Infinite Product, \prod_{n=1}^{\infty}\frac{ e^\frac{1}{n}}{1+\frac{1}{n}},How I can find the value of the Infinite Product $$ \prod_{n=1}^{\infty}\frac{ e^\frac{1}{n}}{1+\frac{1}{n}}$$ I tried like this : $p_n =\prod_{k=1}^{n}\frac{ e^\frac{1}{k}}{1+\frac{1}{k}}= \prod_{k=1}^{n}\frac{k}{k+1}.e^\frac{1}{n}=\frac{1}{2}.\frac{2}{3}.\frac{3}{4}.....\frac{n}{n+1}.e^{\sum_{k=1}^n \frac{1}{k}}= \frac{1}{n+1}.e^{\sum_{k=1}^n \frac{1}{k}} $ but I can't find its limit,How I can find the value of the Infinite Product I tried like this : but I can't find its limit, \prod_{n=1}^{\infty}\frac{ e^\frac{1}{n}}{1+\frac{1}{n}} p_n =\prod_{k=1}^{n}\frac{ e^\frac{1}{k}}{1+\frac{1}{k}}= \prod_{k=1}^{n}\frac{k}{k+1}.e^\frac{1}{n}=\frac{1}{2}.\frac{2}{3}.\frac{3}{4}.....\frac{n}{n+1}.e^{\sum_{k=1}^n \frac{1}{k}}= \frac{1}{n+1}.e^{\sum_{k=1}^n \frac{1}{k}} ,"['analysis', 'products']"
21,Prove that $g(x) = \sum_{n=0}^{+\infty}\frac{1}{2^n+x^2}$ ($x\in\mathbb{R}$) is differentiable and check whether $g'(x)$ is continuous.,Prove that  () is differentiable and check whether  is continuous.,g(x) = \sum_{n=0}^{+\infty}\frac{1}{2^n+x^2} x\in\mathbb{R} g'(x),"The function $g(x)$ is a function series, so it is differentiable when $g'(x)$ converges uniformly. So I should just check uniform convergence of $g'(x)$ by using the Weierstrass M-test: $$g'(x) = \left(\sum_{n=0}^{+\infty}\frac{1}{2^n+x^2}\right)' = \sum_{n=0}^{+\infty}\left(\frac{1}{2^n+x^2}\right)',$$ then $$\left|-\frac{2x}{(2^n+x^2)^2}\right| = \frac{2|x|}{(2^n+x^2)^2} \leq \frac{2|x|}{(2^n)^2} =  \frac{2|x|}{4^n}.$$ But now I can't find a sequence that is bigger than $\frac{2|x|}{4^n}$ to use. For checking whether function $g'(x)$ is continuous or not, I think I will use the same argument: If $g''(x)$ converges uniformly, then $g'(x)$ is differentiable $\Longrightarrow$ continuous. Am I solving this problem in a correct way? Any help would be much appreciated.","The function is a function series, so it is differentiable when converges uniformly. So I should just check uniform convergence of by using the Weierstrass M-test: then But now I can't find a sequence that is bigger than to use. For checking whether function is continuous or not, I think I will use the same argument: If converges uniformly, then is differentiable continuous. Am I solving this problem in a correct way? Any help would be much appreciated.","g(x) g'(x) g'(x) g'(x) = \left(\sum_{n=0}^{+\infty}\frac{1}{2^n+x^2}\right)' = \sum_{n=0}^{+\infty}\left(\frac{1}{2^n+x^2}\right)', \left|-\frac{2x}{(2^n+x^2)^2}\right| = \frac{2|x|}{(2^n+x^2)^2} \leq \frac{2|x|}{(2^n)^2} =  \frac{2|x|}{4^n}. \frac{2|x|}{4^n} g'(x) g''(x) g'(x) \Longrightarrow","['calculus', 'analysis', 'functions']"
22,"Proving Existence of Sequence and Series Satisfying Hardy-Littlewood Convergence Condition Prove that for every $\vartheta, 0 < \vartheta < 1$,","Proving Existence of Sequence and Series Satisfying Hardy-Littlewood Convergence Condition Prove that for every ,","\vartheta, 0 < \vartheta < 1","Prove that for every $\vartheta, 0 < \vartheta < 1$ , there exists a sequence $\lambda_{n}$ of positive integers and a series $\sum_{n=1}^{\infty} a_{n}$ such that: \begin{align*}     (i) & \quad \lambda_{n+1} - \lambda_{n} > (\lambda_{n})^{\vartheta}, \\     (ii) & \quad \lim_{r \to 1-0} \sum_{n=1}^{\infty} a_{n} r^{\lambda_{n}} \text{ exists}, \\     (iii) & \quad \sum_{n=1}^{\infty} a_{n} \text{ is divergent}. \end{align*} Background: András Simonovits noted that $\lambda_{n+1} > c\lambda_{n}$ ( $c > 1$ ) and the $(C,1)$ summability together guarantee convergence. Moreover, if we substitute $x = e^{-y}$ in G. H. Hardy, $\textit{Divergent Series}$ , Clarendon Press, Oxford, 1949, p. 87, Theorem 114 can be stated as follows: Given $c > 1$ constant and a sequence of natural numbers, such that $\lambda_{n+1} > c\lambda_{n}$ ( $c > 1$ ), $\sum_{n=1}^{\infty} a_{n} r^{\lambda_{n}}$ converges in the unit circle, and its limit exists as $r \to 1^{+}$ , then $\sum_{n=1}^{\infty} a_{n}$ is convergent. So, this theorem, conjectured by Littlewood and proved by Hardy and Littlewood, states that Abel summability and convergence are equivalent notions in the case of sequences satisfying the Hadamard gap condition $\lambda_{n+1} > c\lambda_{n}$ ( $c > 1$ ). Our problem states that the Hadamard gap condition cannot be substituted with a much weaker condition. The participants gave two kinds of generalizations for the problem. Some of them substitute Abel summability with the stronger $(C,1)$ summability; others showed that for any sequence $\lambda_{n}$ satisfying $\liminf (\lambda_{n+1} - \lambda_{n})/\lambda_{n} = 0$ , there exists a sequence $\sum_{n=1}^{\infty} a_{n}$ that satisfies the conditions of the problem. This latter statement is also a generalization of the theorem, since if $\lambda_{n} = e^{\sqrt{n}}$ , then... $$\lambda_{n+1} - \lambda_n = e^{\sqrt{n+1}} - e^{\sqrt{n}} = (e^{\sqrt{n+1} -  \sqrt{n}}  - 1)e^{\sqrt{n}}$$ $$= e^{\sqrt{n}} \left(e^{\frac{1}{\sqrt{n} + \sqrt{n+1}}} - 1\right) = \left(1 + o(1)\right) \frac{e^{\sqrt{n}}}{2\sqrt{n}}$$ And therefore $$\frac{\lambda_{n+1} - \lambda_{n}}{\lambda_{n}} \rightarrow 0, \quad \frac{\lambda_{n+1} - \lambda_{n}}{\lambda_{n}^{\vartheta}} \rightarrow \infty \quad \text{for } \vartheta < 1$$","Prove that for every , there exists a sequence of positive integers and a series such that: Background: András Simonovits noted that ( ) and the summability together guarantee convergence. Moreover, if we substitute in G. H. Hardy, , Clarendon Press, Oxford, 1949, p. 87, Theorem 114 can be stated as follows: Given constant and a sequence of natural numbers, such that ( ), converges in the unit circle, and its limit exists as , then is convergent. So, this theorem, conjectured by Littlewood and proved by Hardy and Littlewood, states that Abel summability and convergence are equivalent notions in the case of sequences satisfying the Hadamard gap condition ( ). Our problem states that the Hadamard gap condition cannot be substituted with a much weaker condition. The participants gave two kinds of generalizations for the problem. Some of them substitute Abel summability with the stronger summability; others showed that for any sequence satisfying , there exists a sequence that satisfies the conditions of the problem. This latter statement is also a generalization of the theorem, since if , then... And therefore","\vartheta, 0 < \vartheta < 1 \lambda_{n} \sum_{n=1}^{\infty} a_{n} \begin{align*}
    (i) & \quad \lambda_{n+1} - \lambda_{n} > (\lambda_{n})^{\vartheta}, \\
    (ii) & \quad \lim_{r \to 1-0} \sum_{n=1}^{\infty} a_{n} r^{\lambda_{n}} \text{ exists}, \\
    (iii) & \quad \sum_{n=1}^{\infty} a_{n} \text{ is divergent}.
\end{align*} \lambda_{n+1} > c\lambda_{n} c > 1 (C,1) x = e^{-y} \textit{Divergent Series} c > 1 \lambda_{n+1} > c\lambda_{n} c > 1 \sum_{n=1}^{\infty} a_{n} r^{\lambda_{n}} r \to 1^{+} \sum_{n=1}^{\infty} a_{n} \lambda_{n+1} > c\lambda_{n} c > 1 (C,1) \lambda_{n} \liminf (\lambda_{n+1} - \lambda_{n})/\lambda_{n} = 0 \sum_{n=1}^{\infty} a_{n} \lambda_{n} = e^{\sqrt{n}} \lambda_{n+1} - \lambda_n = e^{\sqrt{n+1}} - e^{\sqrt{n}} = (e^{\sqrt{n+1} -  \sqrt{n}}  - 1)e^{\sqrt{n}} = e^{\sqrt{n}} \left(e^{\frac{1}{\sqrt{n} + \sqrt{n+1}}} - 1\right) = \left(1 + o(1)\right) \frac{e^{\sqrt{n}}}{2\sqrt{n}} \frac{\lambda_{n+1} - \lambda_{n}}{\lambda_{n}} \rightarrow 0, \quad \frac{\lambda_{n+1} - \lambda_{n}}{\lambda_{n}^{\vartheta}} \rightarrow \infty \quad \text{for } \vartheta < 1","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'summation']"
23,The normal cone to a ball at a boundary point of the ball,The normal cone to a ball at a boundary point of the ball,,"I got stuck in a problem that in the end I needed to know what is the normal cone to a ball $B = \left\{ x \in \mathbb{R}^n : \lVert x \rVert \leq 1 \right\}$ (here $\lVert \cdot \rVert$ is an arbirtrary norm), at a point at the boundary $\partial B$ . Recalling that the normal cone to a closed convex set $C$ at a point $x$ in $\mathbb{R}^n$ is the set $$N_C(x) = \left\{ y \in \mathbb{R}^n : \langle y,z-x \rangle \leq 0 \ \forall z \in C \right\}.$$ I manage to compute it for $n = 1$ , you can assume $B = [-1,1]$ , if I'm not mistaken, it is $$N_B(x) = \begin{cases}  [0,+\infty), & \ \text{if} \ x=1, \\ (-\infty,0], & \ \text{if} \ x=-1.\end{cases}$$ Intuitively, I think the normal cone $N_B(x)$ will always be a half space at boundary points, but I don't know how this in a more general setting, is my intuition correct? If not, what is it? Any help, hints or references would be appreciated. $\textbf{Edit}:$ Actually when the norm is differentiable at the point I managed to see that the normal cone is a line (using e.g. Theorem 23.7 of Rockafellar's Convex Analysis book), so the problem actually lies in points where the norm is not differentiable.","I got stuck in a problem that in the end I needed to know what is the normal cone to a ball (here is an arbirtrary norm), at a point at the boundary . Recalling that the normal cone to a closed convex set at a point in is the set I manage to compute it for , you can assume , if I'm not mistaken, it is Intuitively, I think the normal cone will always be a half space at boundary points, but I don't know how this in a more general setting, is my intuition correct? If not, what is it? Any help, hints or references would be appreciated. Actually when the norm is differentiable at the point I managed to see that the normal cone is a line (using e.g. Theorem 23.7 of Rockafellar's Convex Analysis book), so the problem actually lies in points where the norm is not differentiable.","B = \left\{ x \in \mathbb{R}^n : \lVert x \rVert \leq 1 \right\} \lVert \cdot \rVert \partial B C x \mathbb{R}^n N_C(x) = \left\{ y \in \mathbb{R}^n : \langle y,z-x \rangle \leq 0 \ \forall z \in C \right\}. n = 1 B = [-1,1] N_B(x) = \begin{cases} 
[0,+\infty), & \ \text{if} \ x=1, \\
(-\infty,0], & \ \text{if} \ x=-1.\end{cases} N_B(x) \textbf{Edit}:","['analysis', 'convex-analysis', 'convex-geometry', 'convex-cone']"
24,Extracting the leading non-analytic piece of $\int_0^{1/2} \frac{t dx}{x^{-t} - x^t}$ at small real $t$,Extracting the leading non-analytic piece of  at small real,\int_0^{1/2} \frac{t dx}{x^{-t} - x^t} t,"Consider the integral $$I(t) = \int_0^{1/2} \frac{t dx}{x^{-t} - x^t}$$ in the vicinity of $t=0$ . When $t$ is viewed as a real variable, $I(t)$ is very well-behaved. For example, it's infinitely differentiable. On the other hand, when $t$ is on the imaginary axis, the integrand is very poorly behaved as a function of $x$ close to zero. The denominator becomes a rapidly oscillating function that repeatedly passes through zero. This is easiest to see by writing $x^{-t} - x^t = 2\sinh(t \log(1/x))$ which is $2i\sin(\Im(t) \log(1/x))$ for imaginary $t$ . Thus $I(t)$ is infinitely-differentiable for real $t$ , but is non-analytic in the complex plane about $t=0$ . Note that I expect that $I(t)$ will be asymptotic to some power series in $t$ for small real $t$ , $I(t) \sim \sum_{n=0}^\infty c_n t^n$ , but that is not quite what I'm looking for. Rather, I'm interested in the leading ""non-analytic"" contribution to $I(t)$ at small real $t$ . For example, one might guess such a piece to go something like $e^{-1/t^2}$ . Is there a systematic way to extract the leading ""non-analytic"" piece of $I(t)$ for small real $t$ ?","Consider the integral in the vicinity of . When is viewed as a real variable, is very well-behaved. For example, it's infinitely differentiable. On the other hand, when is on the imaginary axis, the integrand is very poorly behaved as a function of close to zero. The denominator becomes a rapidly oscillating function that repeatedly passes through zero. This is easiest to see by writing which is for imaginary . Thus is infinitely-differentiable for real , but is non-analytic in the complex plane about . Note that I expect that will be asymptotic to some power series in for small real , , but that is not quite what I'm looking for. Rather, I'm interested in the leading ""non-analytic"" contribution to at small real . For example, one might guess such a piece to go something like . Is there a systematic way to extract the leading ""non-analytic"" piece of for small real ?",I(t) = \int_0^{1/2} \frac{t dx}{x^{-t} - x^t} t=0 t I(t) t x x^{-t} - x^t = 2\sinh(t \log(1/x)) 2i\sin(\Im(t) \log(1/x)) t I(t) t t=0 I(t) t t I(t) \sim \sum_{n=0}^\infty c_n t^n I(t) t e^{-1/t^2} I(t) t,"['analysis', 'asymptotics']"
25,AN EXAMPLE OF EXTREMUM VALUES OF FUNCTION FROM Thomas Calculus,AN EXAMPLE OF EXTREMUM VALUES OF FUNCTION FROM Thomas Calculus,,"The picture is from Thomas Calculus 12th Ed. In the answers it says that the point $(2,1)$ is a local maximum point of the function $f$ which is correct to me because $f(2)\geq f(x)$ for some open interval containing $2$ . But when we check the sign of the first derivative, it does not change at $2$ , it is positive either side of the point $2$ and the book says ""if the derivative's sign does not change, then the point itself fails to be a local extremum"" consequently, $x=2$ is local maximum point as the definition of local maximum point and is not since derivative's sign does not change at $x=2$ . I am so confused, any hint or help will greatly be aprreciated.  Should we assume that x=2 is an end-point of the left function?  the book says ""A function ƒ has a local maximum value at a point c within its domain D if $ƒ(x)\leqƒ(c)$ for all $x\in D$ lying in some open interval containing c."" so $x=2$ is an end-point for the left function","The picture is from Thomas Calculus 12th Ed. In the answers it says that the point is a local maximum point of the function which is correct to me because for some open interval containing . But when we check the sign of the first derivative, it does not change at , it is positive either side of the point and the book says ""if the derivative's sign does not change, then the point itself fails to be a local extremum"" consequently, is local maximum point as the definition of local maximum point and is not since derivative's sign does not change at . I am so confused, any hint or help will greatly be aprreciated.  Should we assume that x=2 is an end-point of the left function?  the book says ""A function ƒ has a local maximum value at a point c within its domain D if for all lying in some open interval containing c."" so is an end-point for the left function","(2,1) f f(2)\geq f(x) 2 2 2 x=2 x=2 ƒ(x)\leqƒ(c) x\in D x=2","['calculus', 'analysis']"
26,Simplify $n$th derivative of $\frac{\log^3 x}{1-x}$,Simplify th derivative of,n \frac{\log^3 x}{1-x},"I need to simplify $n$ th derivative of $$f(x)=\frac{\log^3 x}{1-x}$$ where $0<x<1$ I tried writing $f(x)=u_1u_2u_3u_4$ where $u_1=u_2=u_3=\log x$ and $u_4=\frac{1}{1-x}$ . Using Leibniz rule for $n$ th derivative we get $$f^{(n)}(x)=\sum_{k_1+k_2+k_3+k_4=n} \binom{n}{k_{1},k_{2},k_{3},k_{4}}u_1^{(k_{1})}u_2^{(k_{2})}u_3^{(k_{3})}u_4^{(k_{4})}$$ Now we have using $u_1^{(k_1)}=\frac{(-1)^{k_1+1} (k_1-1)!}{x^{k_1}}$ and $u_4^{(k_4)}=\frac{k_4!}{(1-x)^{1+k_4}}$ $$f^{(n)}(x)=\sum_{k_1+k_2+k_3+k_4=n} \left(\frac{n!}{k_1!k_2!k_3!k_4!}\right)\frac{(-1)^{k_1+1} (k_1-1)!(-1)^{k_2+1} (k_2-1)!(-1)^{k_3+1} (k_3-1)!k_4!}{x^{k_1+k_2+k_3} (1-x)^{1+k_4}} $$ $$f^{(n)}(x)=n!\sum_{k_1+k_2+k_3+k_4=n} \frac{(-1)^{k_1+k_2+k_3+1}}{k_1k_2k_3 x^{k_!+k_2+k_3}(1-x)^{1+k_4}} $$ $$f^{(n)}(x)=\frac{n!(-1)^{n+1}}{x^n}\sum_{k_1+k_2+k_3+k_4=n} \frac{(-1)^{k_4}}{k_1k_2k_3}\frac{x^{k_4}}{(1-x)^{1+k_4}} $$ Any help would be highly appreciated. Thank you.",I need to simplify th derivative of where I tried writing where and . Using Leibniz rule for th derivative we get Now we have using and Any help would be highly appreciated. Thank you.,"n f(x)=\frac{\log^3 x}{1-x} 0<x<1 f(x)=u_1u_2u_3u_4 u_1=u_2=u_3=\log x u_4=\frac{1}{1-x} n f^{(n)}(x)=\sum_{k_1+k_2+k_3+k_4=n} \binom{n}{k_{1},k_{2},k_{3},k_{4}}u_1^{(k_{1})}u_2^{(k_{2})}u_3^{(k_{3})}u_4^{(k_{4})} u_1^{(k_1)}=\frac{(-1)^{k_1+1} (k_1-1)!}{x^{k_1}} u_4^{(k_4)}=\frac{k_4!}{(1-x)^{1+k_4}} f^{(n)}(x)=\sum_{k_1+k_2+k_3+k_4=n} \left(\frac{n!}{k_1!k_2!k_3!k_4!}\right)\frac{(-1)^{k_1+1} (k_1-1)!(-1)^{k_2+1} (k_2-1)!(-1)^{k_3+1} (k_3-1)!k_4!}{x^{k_1+k_2+k_3} (1-x)^{1+k_4}}  f^{(n)}(x)=n!\sum_{k_1+k_2+k_3+k_4=n} \frac{(-1)^{k_1+k_2+k_3+1}}{k_1k_2k_3 x^{k_!+k_2+k_3}(1-x)^{1+k_4}}  f^{(n)}(x)=\frac{n!(-1)^{n+1}}{x^n}\sum_{k_1+k_2+k_3+k_4=n} \frac{(-1)^{k_4}}{k_1k_2k_3}\frac{x^{k_4}}{(1-x)^{1+k_4}} ","['real-analysis', 'calculus', 'analysis', 'derivatives', 'logarithms']"
27,Example of smooth compactly supported function,Example of smooth compactly supported function,,"Is there an easy example of a smooth compactly supported function (ie. a test function) that equals $e^x$ on the interval $[-1,1]$ ? This is in reference to the following stack exchange question here , where they describe a nice procedure but don't give an explicit example of such a function. I've been trying to construct one and it does not seem obvious at all. Is there something more explicit to know it's existence?","Is there an easy example of a smooth compactly supported function (ie. a test function) that equals on the interval ? This is in reference to the following stack exchange question here , where they describe a nice procedure but don't give an explicit example of such a function. I've been trying to construct one and it does not seem obvious at all. Is there something more explicit to know it's existence?","e^x [-1,1]","['analysis', 'smooth-functions']"
28,"Find $\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt$ if $g_n \nearrow g$ and $g^+ \in L^1[0,1]$",Find  if  and,"\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt g_n \nearrow g g^+ \in L^1[0,1]","Let $g_n: [0,1]\to \Bbb R$ be measurable for all $n\in \Bbb N$ , and $g_n \nearrow g: [0,1]\to \Bbb R$ . Let $g^+ = \max\{0,g\}$ . Assume $g^+ \in L^1[0,1]$ . Evaluate $$\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt.$$ My work. My guess is that the limit approaches $\int_0^1 g(t)\, dt$ . However, the correct limit (this is a quals problem) seems to be $\int_0^1 g^+(t)\, dt$ . I have shown my work below. As $g_n \nearrow g$ , $g$ is measurable. $g^+$ is also measurable. We have $$\left| \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt - \int_0^1 g(t)\, dt \right| \le \left|\frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt  - \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  \right| + \left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right|.$$ We can control the second term using Lebesgue's DCT. We have $$\frac{\ln (1+e^{ng(t)})}{n} \le \frac{\log 2}{n} + g(t) \le 1 + g^{+}(t) \in L^1[0,1]$$ implying $$\lim_{n\to\infty}\left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right| = 0$$ by DCT. As a result, for some $N_1 \in \Bbb N$ , we have $$\left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right| < \frac \varepsilon 2$$ for all $n\ge N_1$ . The first term is $$\left|\frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt  - \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt \right| = \left| \frac 1n \int_0^1\ln\left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}}\right)\, dt\right|$$ As $g_n \nearrow g$ , we get $$\ln \left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}} \right)\xrightarrow{n\to\infty} 0.$$ So, for some $N_2 \in \Bbb N$ , $$\left|\ln \left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}} \right)\right| < \frac{\varepsilon}{2}$$ for all $n\ge N_2$ . Hence, for $n\ge N:= \max\{N_1,N_2\}$ , we have $$\left| \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt - \int_0^1 g(t)\, dt \right| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2N} < \varepsilon.$$ Thus, $$\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt = \int_0^1 g(t)\, dt.$$ I'd like to know where I'm going wrong, and how to justify the correct limit $\int_0^1 g^+(t)\, dt$ . I guess my estimate of the first term might be incorrect. Thanks!","Let be measurable for all , and . Let . Assume . Evaluate My work. My guess is that the limit approaches . However, the correct limit (this is a quals problem) seems to be . I have shown my work below. As , is measurable. is also measurable. We have We can control the second term using Lebesgue's DCT. We have implying by DCT. As a result, for some , we have for all . The first term is As , we get So, for some , for all . Hence, for , we have Thus, I'd like to know where I'm going wrong, and how to justify the correct limit . I guess my estimate of the first term might be incorrect. Thanks!","g_n: [0,1]\to \Bbb R n\in \Bbb N g_n \nearrow g: [0,1]\to \Bbb R g^+ = \max\{0,g\} g^+ \in L^1[0,1] \lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt. \int_0^1 g(t)\, dt \int_0^1 g^+(t)\, dt g_n \nearrow g g g^+ \left| \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt - \int_0^1 g(t)\, dt \right| \le \left|\frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt  - \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  \right| + \left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right|. \frac{\ln (1+e^{ng(t)})}{n} \le \frac{\log 2}{n} + g(t) \le 1 + g^{+}(t) \in L^1[0,1] \lim_{n\to\infty}\left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right| = 0 N_1 \in \Bbb N \left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right| < \frac \varepsilon 2 n\ge N_1 \left|\frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt  - \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt \right| = \left| \frac 1n \int_0^1\ln\left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}}\right)\, dt\right| g_n \nearrow g \ln \left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}} \right)\xrightarrow{n\to\infty} 0. N_2 \in \Bbb N \left|\ln \left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}} \right)\right| < \frac{\varepsilon}{2} n\ge N_2 n\ge N:= \max\{N_1,N_2\} \left| \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt - \int_0^1 g(t)\, dt \right| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2N} < \varepsilon. \lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt = \int_0^1 g(t)\, dt. \int_0^1 g^+(t)\, dt","['real-analysis', 'analysis', 'measure-theory', 'solution-verification']"
29,"For an arbitrary continuous function $f$, is the Stieltjes integral $\int_0^1(df(x))^3=0$?","For an arbitrary continuous function , is the Stieltjes integral ?",f \int_0^1(df(x))^3=0,"Suppose $f:[0,1]\to\mathbb R$ is continuous, possibly with unbounded variation. We consider sums of the form $$\sum_{i=1}^n\Big(f(x_i)-f(x_{i-1})\Big)^3$$ where $0=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=1$ is a partition of the unit interval. As the norm $\max_i|x_i-x_{i-1}|$ approaches $0$ , do these sums also approach $0$ ? For any $p>0$ , there are continuous functions without finite $\int_0^1|df(x)|^p$ (for example $x^{1/p}\cos x^{-1}$ , or $|\ln x/2|^{-1}\cos x^{-1}$ ). On the other hand, without the absolute value, we always have $\int_0^1df(x)^1=f(1)-f(0)$ . It's not clear whether $\int_0^1df(x)^3$ always exists.","Suppose is continuous, possibly with unbounded variation. We consider sums of the form where is a partition of the unit interval. As the norm approaches , do these sums also approach ? For any , there are continuous functions without finite (for example , or ). On the other hand, without the absolute value, we always have . It's not clear whether always exists.","f:[0,1]\to\mathbb R \sum_{i=1}^n\Big(f(x_i)-f(x_{i-1})\Big)^3 0=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=1 \max_i|x_i-x_{i-1}| 0 0 p>0 \int_0^1|df(x)|^p x^{1/p}\cos x^{-1} |\ln x/2|^{-1}\cos x^{-1} \int_0^1df(x)^1=f(1)-f(0) \int_0^1df(x)^3","['analysis', 'bounded-variation', 'stieltjes-integral', 'partitions-for-integration']"
30,Forcing fractional parts to be less than $\varepsilon$ simultaneously,Forcing fractional parts to be less than  simultaneously,\varepsilon,"Edit : For $x \in \mathbb{R}$ , let $\langle x \rangle $ denote the unique number in $[-\frac{1}{2},\frac{1}{2})$ s.t. $x - \langle x \rangle \in \mathbb{Z}$ . Suppose that we have $x_1, ..., x_n \in \mathbb{R}$ . Then, given $\varepsilon >0$ , does there necessarily exist $k \in \mathbb{N}$ such that: $$|\langle kx_i \rangle| < \varepsilon$$ for every $i$ ? The original problem, which I read incorrectly from where I was working on it (the same question with $\langle x \rangle = x - \lfloor x \rfloor$ the standard fractional part), was disproved by the counterexample given by Haran.","Edit : For , let denote the unique number in s.t. . Suppose that we have . Then, given , does there necessarily exist such that: for every ? The original problem, which I read incorrectly from where I was working on it (the same question with the standard fractional part), was disproved by the counterexample given by Haran.","x \in \mathbb{R} \langle x \rangle  [-\frac{1}{2},\frac{1}{2}) x - \langle x \rangle \in \mathbb{Z} x_1, ..., x_n \in \mathbb{R} \varepsilon >0 k \in \mathbb{N} |\langle kx_i \rangle| < \varepsilon i \langle x \rangle = x - \lfloor x \rfloor","['real-analysis', 'combinatorics', 'analysis']"
31,Proving that $(G_\lambda)_\lambda$ is a resolvent family,Proving that  is a resolvent family,(G_\lambda)_\lambda,"Let $\mathcal{E}$ be a bilinear form on dense subset $\mathcal{D}\subset H$ of a Hilbert space $H$ . Assume $\mathcal{E}$ is a closed, symmetric and positive definite, i.e., $\mathcal{E}(u,u)\geq0$ . For $\lambda>0$ , define the scalar product $\mathcal{E}_\lambda(\cdot,\cdot): \mathcal{D} \times\mathcal{D}\to \Bbb R$ , $$ \mathcal{E}_\lambda(u,v)= \lambda (u,v)_H+ \mathcal{E}(u,v).  $$ Since $\mathcal{E}(\cdot,\cdot)$ is closed, it follows that $(\mathcal{D}, \mathcal{E}_\lambda(\cdot,\cdot))$ is a Hilbert space. Applying the  Riesz representation theorem on the space $(\mathcal{D}, \mathcal{E}_\lambda(\cdot,\cdot))$ yields the existence of a linear operator $G_\lambda : H\to \mathcal{D}$ such that for $u\in H$ and $v\in \mathcal{D}$ we have $$  (u,v)_H= \mathcal{E}_\lambda(G_\lambda u,v)= \lambda (G_\lambda u,v)_H+ \mathcal{E}(G_\lambda u,v).   $$ In particular,  taking $v=G_\lambda u$ yields $$  \lambda \|G_\lambda u\|^2_H\leq  \lambda \|G_\lambda u\|^2_H+ \mathcal{E}(G_\lambda u,G_\lambda u)= (u,G_\lambda u)_H\leq \|u\|_H\|G_\lambda u\|_H.   $$ This implies $$   \|G_\lambda u\|_H\leq  \frac{1}{\lambda}\|u\|_H.  $$ Question Show that $(G_\lambda)_\lambda$ is a resolvent family: that is $G_{\lambda_2}G_{\lambda_1}=G_{\lambda_1}G_{\lambda_2}$ , $$G_{\lambda_1}-G_{\lambda_2}= (\lambda_2-\lambda_1) G_{\lambda_1}G_{\lambda_2}$$ and $$\lim_{\lambda\to\infty}\|\lambda G_\lambda u-u\|_H=0.$$","Let be a bilinear form on dense subset of a Hilbert space . Assume is a closed, symmetric and positive definite, i.e., . For , define the scalar product , Since is closed, it follows that is a Hilbert space. Applying the  Riesz representation theorem on the space yields the existence of a linear operator such that for and we have In particular,  taking yields This implies Question Show that is a resolvent family: that is , and","\mathcal{E} \mathcal{D}\subset H H \mathcal{E} \mathcal{E}(u,u)\geq0 \lambda>0 \mathcal{E}_\lambda(\cdot,\cdot): \mathcal{D} \times\mathcal{D}\to \Bbb R 
\mathcal{E}_\lambda(u,v)= \lambda (u,v)_H+ \mathcal{E}(u,v). 
 \mathcal{E}(\cdot,\cdot) (\mathcal{D}, \mathcal{E}_\lambda(\cdot,\cdot)) (\mathcal{D}, \mathcal{E}_\lambda(\cdot,\cdot)) G_\lambda : H\to \mathcal{D} u\in H v\in \mathcal{D} 
 (u,v)_H= \mathcal{E}_\lambda(G_\lambda u,v)= \lambda (G_\lambda u,v)_H+ \mathcal{E}(G_\lambda u,v). 
  v=G_\lambda u 
 \lambda \|G_\lambda u\|^2_H\leq  \lambda \|G_\lambda u\|^2_H+ \mathcal{E}(G_\lambda u,G_\lambda u)= (u,G_\lambda u)_H\leq \|u\|_H\|G_\lambda u\|_H. 
  
  \|G_\lambda u\|_H\leq  \frac{1}{\lambda}\|u\|_H.
  (G_\lambda)_\lambda G_{\lambda_2}G_{\lambda_1}=G_{\lambda_1}G_{\lambda_2} G_{\lambda_1}-G_{\lambda_2}= (\lambda_2-\lambda_1) G_{\lambda_1}G_{\lambda_2} \lim_{\lambda\to\infty}\|\lambda G_\lambda u-u\|_H=0.","['functional-analysis', 'analysis', 'operator-theory', 'semigroup-of-operators']"
32,Fourier transform and the heat semigroup,Fourier transform and the heat semigroup,,"Let $f \in H^{1}(\mathbb{R}^{d})$ and $t > 0$ . I want to prove that: $$\mathcal{F}({e^{t\Delta}f})(k) = e^{-t|2\pi k|^{2}}\mathcal{F}(f)(k),$$ where $\Delta$ denotes the Laplacian operator on $\mathbb{R}^{d}$ and $\mathcal{F}$ denotes the Fourier transform: $$(\mathcal{F}f)(k) = \int_{\mathbb{R}^{d}}e^{-2\pi i \langle k,x\rangle}f(x)dx.$$ I know the following result from functional analysis: let $\mathscr{H}_{1}$ and $\mathscr{H}_{2}$ be Hilbert spaces and $T: \mathscr{H}_{1} \to \mathscr{H}_{1}$ and $S: \mathscr{H}_{2} \to \mathscr{H}_{2}$ be two self-adjoint (possibly unbounded) which are unitarily equivalent, that is, $T = U^{-1}SU$ for some unitary operator $U: \mathscr{H}_{1} \to \mathscr{H}_{2}$ . Then, for every continuous and bounded function $f: \mathbb{R} \to \mathbb{C}$ , one has $f(T) = U^{-1}f(S)U$ . Can I use this result to prove the formula above? My idea is: let $f(x) = e^{-x}$ and consider $T = -t\Delta$ and $U = \mathcal{F}$ , which is an unitary map. The point that is not clear to me is: $f(x) = e^{-x}$ is not bounded on $\mathbb{R}$ , but it is bounded on $[0,\infty)$ , which is the spectrum of $-t \Delta$ . Can I use the formula in this case? In other words, is it enough that $f$ is bounded and continuous on the spectrum of $T$ ?","Let and . I want to prove that: where denotes the Laplacian operator on and denotes the Fourier transform: I know the following result from functional analysis: let and be Hilbert spaces and and be two self-adjoint (possibly unbounded) which are unitarily equivalent, that is, for some unitary operator . Then, for every continuous and bounded function , one has . Can I use this result to prove the formula above? My idea is: let and consider and , which is an unitary map. The point that is not clear to me is: is not bounded on , but it is bounded on , which is the spectrum of . Can I use the formula in this case? In other words, is it enough that is bounded and continuous on the spectrum of ?","f \in H^{1}(\mathbb{R}^{d}) t > 0 \mathcal{F}({e^{t\Delta}f})(k) = e^{-t|2\pi k|^{2}}\mathcal{F}(f)(k), \Delta \mathbb{R}^{d} \mathcal{F} (\mathcal{F}f)(k) = \int_{\mathbb{R}^{d}}e^{-2\pi i \langle k,x\rangle}f(x)dx. \mathscr{H}_{1} \mathscr{H}_{2} T: \mathscr{H}_{1} \to \mathscr{H}_{1} S: \mathscr{H}_{2} \to \mathscr{H}_{2} T = U^{-1}SU U: \mathscr{H}_{1} \to \mathscr{H}_{2} f: \mathbb{R} \to \mathbb{C} f(T) = U^{-1}f(S)U f(x) = e^{-x} T = -t\Delta U = \mathcal{F} f(x) = e^{-x} \mathbb{R} [0,\infty) -t \Delta f T","['functional-analysis', 'analysis', 'partial-differential-equations', 'fourier-analysis']"
33,Showing uniqueness of solution to a non-linear Poisson problem,Showing uniqueness of solution to a non-linear Poisson problem,,"I'm trying to prove that a non-linear Poisson problem has a unique solution. The context is the following: Let $\Omega \subset \mathbb{R}^n$ be a bounded open subset of class $C^2$ . Consider the following non-linear problem: $$\begin{cases} &(\Delta u)(x_0) = f(x_0, u(x_0)), \text{ for every } x_0 \in \Omega \\ &u(x_0) = \varphi(x_0), \text{ for every $x_0 \in \partial \Omega$}\end{cases}$$ where $f(\bullet, u) \in C^{0, \gamma}(\overline{\Omega})$ , $f(x, \bullet) \in C^1(\mathbb{R})$ and $f$ is non decreasing in $u$ , i.e $\displaystyle{\frac{\partial f}{\partial u}(x) \geq 0}$ for every $x \in \overline{\Omega}$ . Prove that this problem has at most one solution in $C^2(\Omega) \cap C(\overline{\Omega})$ . If $u, v$ are both solutions of the problem, ideally, I'd like to use the maximum principle for $w = u -v$ . Since the problem is non-linear, the usual maximum principle doesn't apply, so I need to come up with another linear problem that $w$ satisfies on which the maximum principle can be used. I think the non-negativity of the derivative of $f$ will play an essential role, but I haven't been able to come up with anything helpful. Any help will be greatly appreciated. Thanks in advance!","I'm trying to prove that a non-linear Poisson problem has a unique solution. The context is the following: Let be a bounded open subset of class . Consider the following non-linear problem: where , and is non decreasing in , i.e for every . Prove that this problem has at most one solution in . If are both solutions of the problem, ideally, I'd like to use the maximum principle for . Since the problem is non-linear, the usual maximum principle doesn't apply, so I need to come up with another linear problem that satisfies on which the maximum principle can be used. I think the non-negativity of the derivative of will play an essential role, but I haven't been able to come up with anything helpful. Any help will be greatly appreciated. Thanks in advance!","\Omega \subset \mathbb{R}^n C^2 \begin{cases} &(\Delta u)(x_0) = f(x_0, u(x_0)), \text{ for every } x_0 \in \Omega \\ &u(x_0) = \varphi(x_0), \text{ for every x_0 \in \partial \Omega}\end{cases} f(\bullet, u) \in C^{0, \gamma}(\overline{\Omega}) f(x, \bullet) \in C^1(\mathbb{R}) f u \displaystyle{\frac{\partial f}{\partial u}(x) \geq 0} x \in \overline{\Omega} C^2(\Omega) \cap C(\overline{\Omega}) u, v w = u -v w f","['analysis', 'partial-differential-equations', 'elliptic-equations', 'maximum-principle', 'poissons-equation']"
34,Do the Exponent Properties Apply in Constructive Mathematics?,Do the Exponent Properties Apply in Constructive Mathematics?,,"I have been studying smooth infinitesimal analysis which depends entirely on the set $\Delta$ of numbers whose square (and all higher powers) are equal to $0$ . Now, I know smooth infinitesimal analysis is a branch of constructive mathematics, and I was just wondering if the properties of exponents like, $a^x \cdot a^y = a^{x+y}, (a^x)^y = a^{xy}, \text{ and } a^{1/n} = \sqrt[n]{a}$ would hold if $a \in \Delta$ . Also, is this a valid proof of $\epsilon = 0$ where $\epsilon \in \Delta$ and $\epsilon \geq 0$ : Assume $\epsilon \geq 0$ is an element of $\Delta$ . Clearly, $\epsilon^1=\epsilon^{2/2} = \sqrt{\epsilon^2}$ . But, $\epsilon^2 = 0$ as $\epsilon \in \Delta$ and $x,y \in \mathbb{R}$ , so we have $\epsilon = 0$ . I do not have a great amount of experience in constructive mathematics and have only learned smooth infinitesimal analysis because of its applications (like deriving the formula for the surface area of a sphere) up to this point, so any guidance on how to properly apply constructive mathematics would be useful. I am also new to the site, so I will gladly accept advice on how to improve my question.","I have been studying smooth infinitesimal analysis which depends entirely on the set of numbers whose square (and all higher powers) are equal to . Now, I know smooth infinitesimal analysis is a branch of constructive mathematics, and I was just wondering if the properties of exponents like, would hold if . Also, is this a valid proof of where and : Assume is an element of . Clearly, . But, as and , so we have . I do not have a great amount of experience in constructive mathematics and have only learned smooth infinitesimal analysis because of its applications (like deriving the formula for the surface area of a sphere) up to this point, so any guidance on how to properly apply constructive mathematics would be useful. I am also new to the site, so I will gladly accept advice on how to improve my question.","\Delta 0 a^x \cdot a^y = a^{x+y}, (a^x)^y = a^{xy}, \text{ and } a^{1/n} = \sqrt[n]{a} a \in \Delta \epsilon = 0 \epsilon \in \Delta \epsilon \geq 0 \epsilon \geq 0 \Delta \epsilon^1=\epsilon^{2/2} = \sqrt{\epsilon^2} \epsilon^2 = 0 \epsilon \in \Delta x,y \in \mathbb{R} \epsilon = 0","['analysis', 'constructive-mathematics']"
35,Show $\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} = O\left(\log(N)^2\right)$,Show,\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} = O\left(\log(N)^2\right),"I am not good with inequalities or big-O notation. Here is my attempt: It suffices to show that $\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} \leq \log(N)^2$ . We have \begin{align*} \sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} &\leq \sum_{n = 1}^N \frac{\log(n) + 1}{n}\\ &= \sum_{n = 1}^N \frac{\log(n)}{n} + \sum_{n = 1}^N \frac{1}{n}\\ &\leq \sum_{n = 1}^N \frac{\log(n)}{n} + 1 + \log(N) \end{align*} I'm not sure if I'm making the right moves, but at any rate I'm stuck. Thanks.","I am not good with inequalities or big-O notation. Here is my attempt: It suffices to show that . We have I'm not sure if I'm making the right moves, but at any rate I'm stuck. Thanks.","\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} \leq \log(N)^2 \begin{align*}
\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} &\leq \sum_{n = 1}^N \frac{\log(n) + 1}{n}\\
&= \sum_{n = 1}^N \frac{\log(n)}{n} + \sum_{n = 1}^N \frac{1}{n}\\
&\leq \sum_{n = 1}^N \frac{\log(n)}{n} + 1 + \log(N)
\end{align*}","['real-analysis', 'calculus', 'analysis', 'inequality', 'asymptotics']"
36,Interpolation inequality - what does it mean?,Interpolation inequality - what does it mean?,,"Suppose $f_{k} \to f$ strongly in $L^{2}(\mathbb{R}^{n})$ . Let $2 \le 2q < \eta$ where $\eta = \infty$ if $n \le 2$ and $\eta = 2n/(n-2)$ if $n \ge 3$ . During a proof, my professor wrote that $\|f_{k}-f\|_{L^{2q}(\mathbb{R}^{n})} \to 0$ as $k \to \infty$ by interpolation inequality. I searched the term interpolation inequality on the internet and found a lot of different results. What does it mean in the present context? Edit: Let me add more details about my problem. I am trying to prove the following result: Let $g \in L^{p}(\mathbb{R}^{n})$ with $p > \max\{n/2,1\}$ and suppose $f_{n} \rightharpoonup f$ on $H^{1}(\mathbb{R}^{n})$ , meaning that $\langle f_{n}, h\rangle_{L^{2}}+\langle \nabla f_{n},\nabla h\rangle_{L^{2}} \to \langle f, h\rangle_{L^{2}}+\langle \nabla f, \nabla h\rangle_{L^{2}}$ for every $h \in H^{1}(\mathbb{R}^{n})$ . Then: $$\lim \int_{\mathbb{R}^{n}}g(x)|f_{n}(x)|^{2}dx = \int_{\mathbb{R}^{n}} g(x)|f(x)|^{2}dx$$ The proof goes like this. Let $q$ be such that $1/p+1/q = 1$ . Then, by Hölder and Minkowski inequalities: $$\int_{\mathbb{R}^{n}}|g(x)|||f_{n}(x)|^{2}-|f(x)|^{2}|dx \le \int_{\mathbb{R}^{n}}|g(x)||f_{n}(x)-|f(x)|(|f_{n}(x)|+|f(x)|)dx \le \|f_{n}-f\|_{L^{2q}}\||f_{n}|+|f|\|_{L^{2q}}\|g\|_{L^{p}} \le \|f_{n}-f\|_{L^{2q}}(\|f_{n}\|_{L^{2q}}+\|f\|_{L^{2q}})\|g\|_{L^{p}} \le K\|f_{n}-f\|_{L^{2q}}$$ and the latter must go to zero by some interpolation inequality.","Suppose strongly in . Let where if and if . During a proof, my professor wrote that as by interpolation inequality. I searched the term interpolation inequality on the internet and found a lot of different results. What does it mean in the present context? Edit: Let me add more details about my problem. I am trying to prove the following result: Let with and suppose on , meaning that for every . Then: The proof goes like this. Let be such that . Then, by Hölder and Minkowski inequalities: and the latter must go to zero by some interpolation inequality.","f_{k} \to f L^{2}(\mathbb{R}^{n}) 2 \le 2q < \eta \eta = \infty n \le 2 \eta = 2n/(n-2) n \ge 3 \|f_{k}-f\|_{L^{2q}(\mathbb{R}^{n})} \to 0 k \to \infty g \in L^{p}(\mathbb{R}^{n}) p > \max\{n/2,1\} f_{n} \rightharpoonup f H^{1}(\mathbb{R}^{n}) \langle f_{n}, h\rangle_{L^{2}}+\langle \nabla f_{n},\nabla h\rangle_{L^{2}} \to \langle f, h\rangle_{L^{2}}+\langle \nabla f, \nabla h\rangle_{L^{2}} h \in H^{1}(\mathbb{R}^{n}) \lim \int_{\mathbb{R}^{n}}g(x)|f_{n}(x)|^{2}dx = \int_{\mathbb{R}^{n}} g(x)|f(x)|^{2}dx q 1/p+1/q = 1 \int_{\mathbb{R}^{n}}|g(x)|||f_{n}(x)|^{2}-|f(x)|^{2}|dx \le \int_{\mathbb{R}^{n}}|g(x)||f_{n}(x)-|f(x)|(|f_{n}(x)|+|f(x)|)dx \le \|f_{n}-f\|_{L^{2q}}\||f_{n}|+|f|\|_{L^{2q}}\|g\|_{L^{p}} \le \|f_{n}-f\|_{L^{2q}}(\|f_{n}\|_{L^{2q}}+\|f\|_{L^{2q}})\|g\|_{L^{p}} \le K\|f_{n}-f\|_{L^{2q}}","['functional-analysis', 'analysis', 'distribution-theory']"
37,"Let $\mathcal{B}_0(X, Y)$ is a Banach space. Does this imply that $Y$ is a Banach space?",Let  is a Banach space. Does this imply that  is a Banach space?,"\mathcal{B}_0(X, Y) Y","Consider $$\mathcal{B}_0(X, Y) =\\{T\in\mathcal{B}(X,Y): \overline{T(B_X[0,1])}\subset Y \text{compact}\\}$$ where $B_X[0, 1]=\{x\in X: \|x\|\le 1\}$ Claim: $\mathcal{B}_0(X, Y)$ is a Banach space ( or closed subspace of $(\mathcal{B}(X, Y), \|•\|_{op}) $ iff $Y$ is Banach. If $Y$ is a Banach space then it can be proved easily that the operator limit of a sequence of compact operator is compact. I guess the other implication is not true! Let $\mathcal{B}_0(X, Y)$ is a Banach space. Does this imply that $Y$ is a Banach space? Can we characterize all normed linear spaces $X$ such that $\mathcal{B}_0(X)=\mathcal{B}_0(X,X) $ is closed?",Consider where Claim: is a Banach space ( or closed subspace of iff is Banach. If is a Banach space then it can be proved easily that the operator limit of a sequence of compact operator is compact. I guess the other implication is not true! Let is a Banach space. Does this imply that is a Banach space? Can we characterize all normed linear spaces such that is closed?,"\mathcal{B}_0(X, Y) =\\{T\in\mathcal{B}(X,Y): \overline{T(B_X[0,1])}\subset Y \text{compact}\\} B_X[0, 1]=\{x\in X: \|x\|\le 1\} \mathcal{B}_0(X, Y) (\mathcal{B}(X, Y), \|•\|_{op})  Y Y \mathcal{B}_0(X, Y) Y X \mathcal{B}_0(X)=\mathcal{B}_0(X,X) ","['functional-analysis', 'analysis', 'operator-theory', 'banach-spaces', 'banach-algebras']"
38,Max and min of a three variable function over a set,Max and min of a three variable function over a set,,"I need help for the following exercise, because I am not sure of myself. Consider the function $$f(x, y, z) = -(xy + zy)^2 + x - z$$ and the set $$A = \{ (x, y, z) \in \mathbb{R}^3; xy + zy + x - z = 0 \}$$ Does $g(t) = f(e^t, 1, e^t)$ have absolute max/min as $t$ changes in $\mathbb{R}$ ? Does $f(x, y, z)$ have absolute max/min over $A$ ? If yes, find it/them. My Attempts So, for what concerns the first request, I just calculated $$g(t) = -4e^{2t}$$ And this is a monotone decreasing function over $\mathbb{R}$ hence it has no max/min (absolute or relative). For what concerns the second request, I'm a bit perplexed. First of all, I don't know how to show if $A$ is bounded and or closed (not asked, but I always want to do this part). So I have no clue if I could use Weierstrass theorem, for not knowing if $A$ is compact cannot make me to proceed. But alas, in spite of that, I went on. I wrote the set $A$ in function of $z$ as $z = \frac{x(1+y)}{1-y}$ and I thence wrote $f$ restricted to that $$f\bigg|_z = -\frac{2xy}{1-y}\left(1 + \frac{2xy}{1-y}\right)$$ At this point I calculated the gradient (tedious): $$\nabla f = \left(-\frac{4 x y^2}{(1-y)^2}-\frac{2 y \left(\frac{2 x y}{1-y}+1\right)}{1-y},-\frac{2 x \left(\frac{2 x y}{1-y}+1\right)}{1-y}-\frac{2 x y \left(\frac{2 x y}{1-y}+1\right)}{(1-y)^2}-\frac{2 x y \left(\frac{2 x y}{(1-y)^2}+\frac{2 x}{1-y}\right)}{1-y}\right)$$ Now, I found only two solutions: $x = y = 0$ and $y = \frac{1}{1-4x}$ (with $x \neq 0$ ). At this point I don't know if I should proceed with the Hessian study, or not. Maybe there is a more conveniente way to attack this problem. I'm just puzzled about the set $A$ , I cannot figure it out in terms of what it does represent. Thank you so much!","I need help for the following exercise, because I am not sure of myself. Consider the function and the set Does have absolute max/min as changes in ? Does have absolute max/min over ? If yes, find it/them. My Attempts So, for what concerns the first request, I just calculated And this is a monotone decreasing function over hence it has no max/min (absolute or relative). For what concerns the second request, I'm a bit perplexed. First of all, I don't know how to show if is bounded and or closed (not asked, but I always want to do this part). So I have no clue if I could use Weierstrass theorem, for not knowing if is compact cannot make me to proceed. But alas, in spite of that, I went on. I wrote the set in function of as and I thence wrote restricted to that At this point I calculated the gradient (tedious): Now, I found only two solutions: and (with ). At this point I don't know if I should proceed with the Hessian study, or not. Maybe there is a more conveniente way to attack this problem. I'm just puzzled about the set , I cannot figure it out in terms of what it does represent. Thank you so much!","f(x, y, z) = -(xy + zy)^2 + x - z A = \{ (x, y, z) \in \mathbb{R}^3; xy + zy + x - z = 0 \} g(t) = f(e^t, 1, e^t) t \mathbb{R} f(x, y, z) A g(t) = -4e^{2t} \mathbb{R} A A A z z = \frac{x(1+y)}{1-y} f f\bigg|_z = -\frac{2xy}{1-y}\left(1 + \frac{2xy}{1-y}\right) \nabla f = \left(-\frac{4 x y^2}{(1-y)^2}-\frac{2 y \left(\frac{2 x y}{1-y}+1\right)}{1-y},-\frac{2 x \left(\frac{2 x y}{1-y}+1\right)}{1-y}-\frac{2 x y \left(\frac{2 x y}{1-y}+1\right)}{(1-y)^2}-\frac{2 x y \left(\frac{2 x y}{(1-y)^2}+\frac{2 x}{1-y}\right)}{1-y}\right) x = y = 0 y = \frac{1}{1-4x} x \neq 0 A","['real-analysis', 'analysis', 'multivariable-calculus', 'optimization', 'maxima-minima']"
39,Inequality involving the Gagliardo seminorm,Inequality involving the Gagliardo seminorm,,"Does the following estimate hold for any value of $p \in [1,+\infty)$ ? $$\int\int_{\mathbb{R}^N \times \mathbb{R}^N} \dfrac{|u(x) - u(y)|^p}{|x - y|^{N + sp}}dxdy \leq 2\left[\left(\int\int_{\mathbb{R}^N \times \{|y| \geq 2|x|\}} \dfrac{|u(x)|^p}{|x - y|^{N + sp}}dxdy\right)^{1/p} + \left(\int\int_{\mathbb{R}^N \times \{|y| \geq 2|x|\}} \dfrac{|u(y)|^p}{|x - y|^{N + sp}}dxdy\right)^{1/p}\right]^{p} + 2\int\int_{\mathbb{R}^N \times \{|y| < 2|x|\}} \dfrac{|u(x) - u(y)|^p}{|x - y|^{N + sp}}dxdy$$ Based on some calculations performed using the traditional inequality $|a + b|^p \leq 2^p(|a|^p + |b|^p)$ , I noticed that the constant $2^p$ appears in the first part of the second inequality expression. This presence of the constant $2^p$ represents a problem in my specific case.","Does the following estimate hold for any value of ? Based on some calculations performed using the traditional inequality , I noticed that the constant appears in the first part of the second inequality expression. This presence of the constant represents a problem in my specific case.","p \in [1,+\infty) \int\int_{\mathbb{R}^N \times \mathbb{R}^N} \dfrac{|u(x) - u(y)|^p}{|x - y|^{N + sp}}dxdy \leq 2\left[\left(\int\int_{\mathbb{R}^N \times \{|y| \geq 2|x|\}} \dfrac{|u(x)|^p}{|x - y|^{N + sp}}dxdy\right)^{1/p} + \left(\int\int_{\mathbb{R}^N \times \{|y| \geq 2|x|\}} \dfrac{|u(y)|^p}{|x - y|^{N + sp}}dxdy\right)^{1/p}\right]^{p} + 2\int\int_{\mathbb{R}^N \times \{|y| < 2|x|\}} \dfrac{|u(x) - u(y)|^p}{|x - y|^{N + sp}}dxdy |a + b|^p \leq 2^p(|a|^p + |b|^p) 2^p 2^p","['analysis', 'sobolev-spaces', 'fractional-sobolev-spaces']"
40,Convergence of the integration of three functions,Convergence of the integration of three functions,,"I'm considering a convergence problem of the integration of three functions say $f_{k},g_{k},h_{k}$ . What I have is on a bounded smooth domain $\Omega$ , $f_{k}\to f$ strongly in $L^{2}$ , $g_{k}\rightharpoonup g$ weakly in $L^{2}$ and $h_{k}\to h$ a.e. in $\Omega$ and $|h_{k}|,|h|\leq C$ for all $x\in\Omega$ . Besides, I also have $\lVert f_{k}\rVert_{2},\lVert g_{k}\rVert_{2}\leq C$ . Then my question is whether \begin{align} \int_{\Omega}f_{k}g_{k}h_{k}dx\to\int_{\Omega}fghdx. \end{align} My try is \begin{align*} &|\int_{\Omega}f_{k}g_{k}h_{k}-\int_{\Omega}fghdx|\\ \leq&|\int_{\Omega}f(g_{k}-g)hdx|+|\int_{\Omega}(f_{k}-f)g_{k}h_{k}dx|+|\int_{\Omega}fg_{k}(h_{k}-h)dx| \end{align*} The first term goes to zero by weak convergence, the second by strong convergence and the last by dominated convergence theorem. Does my thought make sense?","I'm considering a convergence problem of the integration of three functions say . What I have is on a bounded smooth domain , strongly in , weakly in and a.e. in and for all . Besides, I also have . Then my question is whether My try is The first term goes to zero by weak convergence, the second by strong convergence and the last by dominated convergence theorem. Does my thought make sense?","f_{k},g_{k},h_{k} \Omega f_{k}\to f L^{2} g_{k}\rightharpoonup g L^{2} h_{k}\to h \Omega |h_{k}|,|h|\leq C x\in\Omega \lVert f_{k}\rVert_{2},\lVert g_{k}\rVert_{2}\leq C \begin{align}
\int_{\Omega}f_{k}g_{k}h_{k}dx\to\int_{\Omega}fghdx.
\end{align} \begin{align*}
&|\int_{\Omega}f_{k}g_{k}h_{k}-\int_{\Omega}fghdx|\\
\leq&|\int_{\Omega}f(g_{k}-g)hdx|+|\int_{\Omega}(f_{k}-f)g_{k}h_{k}dx|+|\int_{\Omega}fg_{k}(h_{k}-h)dx|
\end{align*}","['real-analysis', 'analysis', 'sobolev-spaces', 'weak-convergence']"
41,"How to prove $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ does not uniformly converge in $\left( 0,2\pi \right)$ with the conditions below?",How to prove  does not uniformly converge in  with the conditions below?,"\sum_{n=0}^{\infty}a_n \cos{(nx)} \left( 0,2\pi \right)","Let $a_n$ be a sequence of real numbers such that: $a_n \ge 0$ $\forall n\in \mathbb{N}: a_n \ge a_{n+1}$ $\lim_{n \to \infty} a_n =0$ $\sum_{n=0}^{\infty } a_n =\infty $ Consider the sum $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ . Prove: $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ uniformly converges in $\left[ \frac{\pi}{2},\pi \right]$ $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ does not uniformly converge in $\left( 0,2\pi \right)$ I only need help with 2 (I already solved 1). I'm trying to solve it for almost a day now. I'd appreciate if you could explain how to solve it. Thanks.",Let be a sequence of real numbers such that: Consider the sum . Prove: uniformly converges in does not uniformly converge in I only need help with 2 (I already solved 1). I'm trying to solve it for almost a day now. I'd appreciate if you could explain how to solve it. Thanks.,"a_n a_n \ge 0 \forall n\in \mathbb{N}: a_n \ge a_{n+1} \lim_{n \to \infty} a_n =0 \sum_{n=0}^{\infty } a_n =\infty  \sum_{n=0}^{\infty}a_n \cos{(nx)} \sum_{n=0}^{\infty}a_n \cos{(nx)} \left[ \frac{\pi}{2},\pi \right] \sum_{n=0}^{\infty}a_n \cos{(nx)} \left( 0,2\pi \right)","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'uniform-convergence']"
42,"The equation $r^2 \frac{\partial ^3\Psi(r,s)}{\partial r^3}=s^2 \frac{\partial \Psi(r,s)}{\partial s}.$ Possible connections in physics and math?",The equation  Possible connections in physics and math?,"r^2 \frac{\partial ^3\Psi(r,s)}{\partial r^3}=s^2 \frac{\partial \Psi(r,s)}{\partial s}.","Recently I wrote down the following linear third order partial differential equation: $$r^2 \frac{\partial ^3\Psi(r,s)}{\partial r^3}=s^2 \frac{\partial \Psi(r,s)}{\partial s} \tag{1}$$ The particular solution I obtained for $(1)$ is: $$\Psi(r,s)=2 \sqrt{\frac{r}{s}}K_1(2\sqrt{r s})$$ where $K_1$ is the modified Bessel function of the second kind and $r,s>0.$ I'll be honest, I guessed the solution, so I don't know the normal method. I searched online for a list of most common partial differential equations but didn't see anything that resembled the form of $(1).$ I also talked to some people knowledegable in physics but most of the differential equations applied are second order. I learned from them that $(1)$ is dimensionally inconsistent and that one can make it dimensionally consistent using physical constants with the right units. At this moment I'm not interested in the steps to obtain my solution. I would like to know if the PDE or the analytic solution I found are connected to other areas of math or physics. I tried thinking of an ansatz that would help but couldn't come up with anything. I actually don't see anything of this form appearing in the literature so I can't just learn from someone else's solution and explanation. I thought, Bessel functions are applied in physics...the Bessel differential equation. Some solutions find applications in vibrating membranes. However, there's some crucial differences in my case because I'm dealing with a modified Bessel function, of fixed index and it's $2$ dimensional i.e. is a function of $r,s.$ Does $(1)$ appear in the literature anywhere? Is $(1)$ or the solution $\Psi(r,s)$ connected to other problems in math or physics? Thanks for the possible resources and/or feedback.","Recently I wrote down the following linear third order partial differential equation: The particular solution I obtained for is: where is the modified Bessel function of the second kind and I'll be honest, I guessed the solution, so I don't know the normal method. I searched online for a list of most common partial differential equations but didn't see anything that resembled the form of I also talked to some people knowledegable in physics but most of the differential equations applied are second order. I learned from them that is dimensionally inconsistent and that one can make it dimensionally consistent using physical constants with the right units. At this moment I'm not interested in the steps to obtain my solution. I would like to know if the PDE or the analytic solution I found are connected to other areas of math or physics. I tried thinking of an ansatz that would help but couldn't come up with anything. I actually don't see anything of this form appearing in the literature so I can't just learn from someone else's solution and explanation. I thought, Bessel functions are applied in physics...the Bessel differential equation. Some solutions find applications in vibrating membranes. However, there's some crucial differences in my case because I'm dealing with a modified Bessel function, of fixed index and it's dimensional i.e. is a function of Does appear in the literature anywhere? Is or the solution connected to other problems in math or physics? Thanks for the possible resources and/or feedback.","r^2 \frac{\partial ^3\Psi(r,s)}{\partial r^3}=s^2 \frac{\partial \Psi(r,s)}{\partial s} \tag{1} (1) \Psi(r,s)=2 \sqrt{\frac{r}{s}}K_1(2\sqrt{r s}) K_1 r,s>0. (1). (1) 2 r,s. (1) (1) \Psi(r,s)","['analysis', 'partial-differential-equations', 'soft-question', 'physics', 'bessel-functions']"
43,Integration on foliated domain: more like Fubini-Tonelli or coarea?,Integration on foliated domain: more like Fubini-Tonelli or coarea?,,"This is (should be) a very basic question in differential geometry/analysis, but I'm not quite sure. Everybody knows Fubini-Tonelli theorem, where if you integrate over a domain $\Omega$ in, say, $\mathbb{R}^3$ , you can actually integrate on the slices, or integrate line by line etc (this is actually Fubini or Tonelli theorem, I don't remember which is which, one of the two involves exchanging the order of integration, but whatever). So if you take, say, a cube $[0,1]^3$ in $\mathbb{R}^3$ you get $$ \int_{[0,1]^3} f(x,y,z) dxdydz = \int_{[0,1]} \left( \int_{[0,1]^2} f(x,y,z) dx dy\right) dz= \int_{[0,1]^2} \left(\int_{[0,1]} f(x,y,z) dx\right)dy dz = \dots ,$$ for an appropriate $f$ . Of course this generalizes to $n$ dimensions without a problem. Now, a big leap forward and one sees coarea formula, where given a Lipshitz function $f$ and a measurable function $g$ one has $$ \int_{E} g |\nabla f| = \int_{\mathbb{R} } \left(\int_{E \cap f^{-1} (t)} g  \right) dt,$$ where the inner integral is with respect to an appropriate Hausdorff measure, over rectifiable sets which are the level sets of $f$ in $E$ . Now, if I have a domain $\Omega$ which is foliated by some collection of submanifolds, which are not necessarily the level sets of some function, what is the correct formula? I suspect there is a way to view this as a particualr case of coarea with $|\nabla f|=1$ , because my feeling is that if you have a domain $(x,y) \in \Omega \subset \mathbb{R}^n \times \mathbb{R}^m$ and $$\Omega = \cup _{x \in \Omega' } M_x $$ with $M_x$ disjoint $m$ -manifolds, I suspect $$\int_{\Omega} f(x,y) dxdy= \int_{\Omega'} \left( \int_{M_x} f \right)dx ,$$ and that this can be rigorously proved by exhibiting a vector valued projection whose level sets are precisely the submanifolds $M_x$ and whose coarea factor is identically equal to 1. But maybe I am wrong or maybe it's simpler, I am not sure.","This is (should be) a very basic question in differential geometry/analysis, but I'm not quite sure. Everybody knows Fubini-Tonelli theorem, where if you integrate over a domain in, say, , you can actually integrate on the slices, or integrate line by line etc (this is actually Fubini or Tonelli theorem, I don't remember which is which, one of the two involves exchanging the order of integration, but whatever). So if you take, say, a cube in you get for an appropriate . Of course this generalizes to dimensions without a problem. Now, a big leap forward and one sees coarea formula, where given a Lipshitz function and a measurable function one has where the inner integral is with respect to an appropriate Hausdorff measure, over rectifiable sets which are the level sets of in . Now, if I have a domain which is foliated by some collection of submanifolds, which are not necessarily the level sets of some function, what is the correct formula? I suspect there is a way to view this as a particualr case of coarea with , because my feeling is that if you have a domain and with disjoint -manifolds, I suspect and that this can be rigorously proved by exhibiting a vector valued projection whose level sets are precisely the submanifolds and whose coarea factor is identically equal to 1. But maybe I am wrong or maybe it's simpler, I am not sure.","\Omega \mathbb{R}^3 [0,1]^3 \mathbb{R}^3  \int_{[0,1]^3} f(x,y,z) dxdydz = \int_{[0,1]} \left( \int_{[0,1]^2} f(x,y,z) dx dy\right) dz= \int_{[0,1]^2} \left(\int_{[0,1]} f(x,y,z) dx\right)dy dz = \dots , f n f g  \int_{E} g |\nabla f| = \int_{\mathbb{R} } \left(\int_{E \cap f^{-1} (t)} g  \right) dt, f E \Omega |\nabla f|=1 (x,y) \in \Omega \subset \mathbb{R}^n \times \mathbb{R}^m \Omega = \cup _{x \in \Omega' } M_x  M_x m \int_{\Omega} f(x,y) dxdy= \int_{\Omega'} \left( \int_{M_x} f \right)dx , M_x","['real-analysis', 'integration', 'analysis', 'differential-geometry', 'manifolds']"
44,Easiest way to see that $\log X < X^a$ as $X \to \infty$ for fixed $a > 0$,Easiest way to see that  as  for fixed,\log X < X^a X \to \infty a > 0,"Let $a > 0$ . It is ""obvious"" that for large enough $X$ , we have $X^a > \log X$ . I would like to prove this using as little machinery as possible. Here are two approaches: One way to do this is to note that our goal is equivalent to $X < \exp\{X^a\}$ as $X\to\infty$ . The Taylor series of $\exp$ at $0$ will have terms of order greater than $X$ , so we can see the aymptotic from that. We could also observe that $$ X^a - \log X = 1 + \int_1^X t^{-1}(t^a - 1)dt. $$ Let $C$ be a real number such that $C^a \geq 2$ . For $X \geq C$ , we have $$ \int_C^X t^{-1}(t^a - 1) dt \geq \int_C^X t^{-1} = \log X - \log C, $$ and it follows that $$ X^a - \log X = k + \log X  $$ for some constant $k$ . Since $\log X \to \infty$ as $X\to \infty$ , it follows that $X^a > \log X$ for large enough $X$ . These arguments both feel a bit unsatisfying to me. The first one uses Taylor series, which feel like quite heavy machinery, and the second one, while more elementary, feels like it doesn't really get to the heart of why $X^a$ is bigger than $\log X$ . Can anyone see a more satisfying argument? I would also accept an answer that convinces me that one of my two arguments is actually a nice solution; currently I don't have much intuition for the aesthetics of these sorts of proofs.","Let . It is ""obvious"" that for large enough , we have . I would like to prove this using as little machinery as possible. Here are two approaches: One way to do this is to note that our goal is equivalent to as . The Taylor series of at will have terms of order greater than , so we can see the aymptotic from that. We could also observe that Let be a real number such that . For , we have and it follows that for some constant . Since as , it follows that for large enough . These arguments both feel a bit unsatisfying to me. The first one uses Taylor series, which feel like quite heavy machinery, and the second one, while more elementary, feels like it doesn't really get to the heart of why is bigger than . Can anyone see a more satisfying argument? I would also accept an answer that convinces me that one of my two arguments is actually a nice solution; currently I don't have much intuition for the aesthetics of these sorts of proofs.","a > 0 X X^a > \log X X < \exp\{X^a\} X\to\infty \exp 0 X 
X^a - \log X = 1 + \int_1^X t^{-1}(t^a - 1)dt.
 C C^a \geq 2 X \geq C 
\int_C^X t^{-1}(t^a - 1) dt \geq \int_C^X t^{-1} = \log X - \log C,
 
X^a - \log X = k + \log X 
 k \log X \to \infty X\to \infty X^a > \log X X X^a \log X","['real-analysis', 'analysis']"
45,How to prove: $\left|\int_0^1 f(x) d x\right| \leq \frac{1-a}{2} \max _{0<x<1}\left|f^{\prime}(x)\right|$,How to prove:,\left|\int_0^1 f(x) d x\right| \leq \frac{1-a}{2} \max _{0<x<1}\left|f^{\prime}(x)\right|,"I found this question on the 1998 Roorkee Paper during my crazy hunt for mind-blowing calculus questions. Let $f:[0,1] \rightarrow \mathbb{R}$ be a continuous function, differentiable on $(0,1)$ such that $\int_0^{a}  f(x) dx=0$ for some $a \in(0,1)$ , then (1) $\left|\int_0^1 f(x) d x\right| \leq \frac{1-a}{2} \max  _{0<x<1}\left|f^{\prime}(x)\right|$ (2) $\left|\int_0^1 f(x)dx\right|>\frac{1-\mathrm{a}}{2} \max_{0<x<1}\left|f'(x)\right|$ (3) $\left|\int_0^1 f(x) d x\right|=\frac{1-a}{2} \max_{0<x<1}\left|f^{\prime}(x)\right|$ for some function $f(x)$ (4) none of the other options is correct And ofcourse I have no idea on how to solve this. However on carefully reading the options and noticing $\frac{1-a}{2}$ , I thought of making a connection with the concept of ' Definite Integral as Limit of Sum ' where $\frac{b-a}{h}$ could  be $\frac{1-a}{2}$ here. Then I also believe there is an involvement of Lagranges Mean Value Theorem somewhere as the questions says continuous and differentiable. I presented my observations to my teacher and he gave me a hint: ""Put $x=at$ and change limits accordingly and try to apply your concepts."" Could someone kindly help me with this one.","I found this question on the 1998 Roorkee Paper during my crazy hunt for mind-blowing calculus questions. Let be a continuous function, differentiable on such that for some , then (1) (2) (3) for some function (4) none of the other options is correct And ofcourse I have no idea on how to solve this. However on carefully reading the options and noticing , I thought of making a connection with the concept of ' Definite Integral as Limit of Sum ' where could  be here. Then I also believe there is an involvement of Lagranges Mean Value Theorem somewhere as the questions says continuous and differentiable. I presented my observations to my teacher and he gave me a hint: ""Put and change limits accordingly and try to apply your concepts."" Could someone kindly help me with this one.","f:[0,1] \rightarrow \mathbb{R} (0,1) \int_0^{a}
 f(x) dx=0 a \in(0,1) \left|\int_0^1 f(x) d x\right| \leq \frac{1-a}{2} \max
 _{0<x<1}\left|f^{\prime}(x)\right| \left|\int_0^1 f(x)dx\right|>\frac{1-\mathrm{a}}{2} \max_{0<x<1}\left|f'(x)\right| \left|\int_0^1 f(x) d x\right|=\frac{1-a}{2} \max_{0<x<1}\left|f^{\prime}(x)\right| f(x) \frac{1-a}{2} \frac{b-a}{h} \frac{1-a}{2} x=at","['real-analysis', 'integration', 'analysis', 'inequality', 'definite-integrals']"
46,What does $\sum_{1\le k\le n}$ mean in this context?,What does  mean in this context?,\sum_{1\le k\le n},"Why is: $$\sum_{k=1}^\infty \sum_{n=k}^\infty \frac{1}{n(n+1)} k\, a_k =\sum_{1\le k\le n}\frac{1}{n(n+1)} k\, a_k=\sum_{n=1}^\infty \frac{1}{n+1}\; \frac{1}{n}\sum_{k=1}^n k\, a_k$$ What does the middle term mean and why is it equal to the first? I thought $\sum_{1\le k\le n}=\sum \limits_{k=1}^{n}$ .",Why is: What does the middle term mean and why is it equal to the first? I thought .,"\sum_{k=1}^\infty \sum_{n=k}^\infty \frac{1}{n(n+1)} k\, a_k
=\sum_{1\le k\le n}\frac{1}{n(n+1)} k\, a_k=\sum_{n=1}^\infty \frac{1}{n+1}\; \frac{1}{n}\sum_{k=1}^n k\, a_k \sum_{1\le k\le n}=\sum \limits_{k=1}^{n}","['analysis', 'summation', 'notation']"
47,Second distributional derivative of $P.V. \frac{1}{x}$,Second distributional derivative of,P.V. \frac{1}{x},"I computed first derivation and I get that $$\langle(\mathcal{P}\frac{1}{x})', \varphi\rangle = v.p. \int_{ \mathbb{R}} \frac{\varphi(0) - \varphi(x)}{x^2} dx$$ In order to get second derivative we use $\left<g'', \varphi \right> = \left<g, \varphi''\right> $ but I stuck there since I have just a lot of equations that gives us nothing. Any help would be great!",I computed first derivation and I get that In order to get second derivative we use but I stuck there since I have just a lot of equations that gives us nothing. Any help would be great!,"\langle(\mathcal{P}\frac{1}{x})', \varphi\rangle = v.p. \int_{ \mathbb{R}} \frac{\varphi(0) - \varphi(x)}{x^2} dx \left<g'', \varphi \right> = \left<g, \varphi''\right> ","['analysis', 'functions', 'derivatives', 'distribution-theory', 'cauchy-principal-value']"
48,Finite Levy measure implies finite $L^1$ norm away from 0,Finite Levy measure implies finite  norm away from 0,L^1,"So I'm reading this paper on Symmetrization of Levy processes and I'm trying to wrap my head around a particular detail. Namely, on page 10, for the Levy density $\phi(x)$ with $$\int_{\mathbb{R}^d}\frac{|x|^2}{1+|x|^2}\phi(x)dx<\infty$$ they introduce a function $$\phi_n(x)=\phi(x)\mathbb{I}(x)$$ where $\mathbb{I}(x)$ is $1$ if $x\in\mathbb{R}^d\setminus B_{1/n}(0)$ and $0$ otherwise. Hence $\phi_n$ is simply $\phi(x)$ away from some ball of radius $1/n$ away from the origin 0 (so that $\mathbb{I}$ is simply an indicator function). They make the claim that from the above integral being finite, that: $$\int_{\mathbb{R}^d}\phi_n(x)dx=\int_{\mathbb{R}^d\setminus B_{1/n}(0)}\phi(x)dx<\infty$$ I'm trying to figure out why that is the case. Thanks! EDIT: You may assume $\phi\ge0$ here.","So I'm reading this paper on Symmetrization of Levy processes and I'm trying to wrap my head around a particular detail. Namely, on page 10, for the Levy density with they introduce a function where is if and otherwise. Hence is simply away from some ball of radius away from the origin 0 (so that is simply an indicator function). They make the claim that from the above integral being finite, that: I'm trying to figure out why that is the case. Thanks! EDIT: You may assume here.",\phi(x) \int_{\mathbb{R}^d}\frac{|x|^2}{1+|x|^2}\phi(x)dx<\infty \phi_n(x)=\phi(x)\mathbb{I}(x) \mathbb{I}(x) 1 x\in\mathbb{R}^d\setminus B_{1/n}(0) 0 \phi_n \phi(x) 1/n \mathbb{I} \int_{\mathbb{R}^d}\phi_n(x)dx=\int_{\mathbb{R}^d\setminus B_{1/n}(0)}\phi(x)dx<\infty \phi\ge0,"['probability', 'analysis']"
49,"Proof of the measurability of functions in Theorem 1.10 (Product measure) in ""Analysis"" by Lieb and Loss","Proof of the measurability of functions in Theorem 1.10 (Product measure) in ""Analysis"" by Lieb and Loss",,"I am confronted with the following problem: Let $\left(\Omega_1,\Sigma_1,\mu_1\right)$ and $\left(\Omega_2,\Sigma_2,\mu_2\right)$ be two sigma-finite measure spaces. Let $A$ be a measurable set in $\Sigma_1\times\Sigma_2$ , where $\Sigma_1\times\Sigma_2$ is the smallest sigma-algebra containing all rectangles defined by $ A_1\times A_2 = \left\{ \left(x_1,x_2\right) {}:{} x_1\in A_1,\quad x_2\in A_2 \right\},\quad A_1\in\Sigma_1,\quad A_2\in\Sigma_2 $ . Next, for every $x_2\in\Omega_2$ , set $f\left( x_2\right) :=\mu_1\left( A_1\left(x_2\right)\right)$ and, for every $x_1\in\Omega_1$ , $g\left( x_1\right) :=\mu_2\left(A_2\left(x_1\right)\right)$ , where $ A_1\left( x_2\right) = \left\{x_1\in\Omega_1\mid \left( x_1,x_2\right)\in A \right\}\in\Sigma_1 $ and $ A_2\left( x_1\right) = \left\{x_2\in\Omega_2\mid \left( x_1,x_2\right)\in A \right\}\in\Sigma_2 $ . Then $f$ is $\Sigma_2$ -measurable, $g$ is $\Sigma_1$ -measurable and $\left(\mu_1\times\mu_2\right)(A) :=\int_{\Omega_2}f\left( x_2\right)\mu_2\left(\mathrm{d}x_2\right) =\int_{\Omega_1}g\left(x_1\right)\mu_1\left(\mathrm{d}x_1\right).$ Now the first part of the proof says: The measurability of $f$ and $g$ parallels the proof of the section property in Section 1.2 and uses the Monotone Class Theorem, while the section property says that for every $A\in\Sigma_1\times\Sigma_2$ the following applies: $A_1\left(x_2\right)\in\Sigma_1\quad\forall x_2\in\Omega_2\quad$ and $\quad A_2\left(x_2\right)\in\Sigma_2\quad\forall x_1\in\Omega_1.$ To show the section property, you define a class of sets which have the section property and show that this class is a sigma-algebra. Then, this class is also the smallest sigma-algebra containing all rectangles by definition - as far as I understood it, at least. Now, to show that $f$ and $g$ are measurable, I think that I have to define a class $\mathcal{M}$ of sets,  for which $f$ is $\Sigma_2$ -measurable and $g$ is $\Sigma_1$ -measurable, and show that $\mathcal{M}$ is a monotone class. The Monotone Class Theorem then states, that this class is also a sigma-algebra and therefore I should infer that the measurability of $f$ and $g$ holds. But my problem is that I am having a hard time to construct such a class $\mathcal{M}$ . Do you have any ideas for an approach? I'd appreciate the help.","I am confronted with the following problem: Let and be two sigma-finite measure spaces. Let be a measurable set in , where is the smallest sigma-algebra containing all rectangles defined by . Next, for every , set and, for every , , where and . Then is -measurable, is -measurable and Now the first part of the proof says: The measurability of and parallels the proof of the section property in Section 1.2 and uses the Monotone Class Theorem, while the section property says that for every the following applies: and To show the section property, you define a class of sets which have the section property and show that this class is a sigma-algebra. Then, this class is also the smallest sigma-algebra containing all rectangles by definition - as far as I understood it, at least. Now, to show that and are measurable, I think that I have to define a class of sets,  for which is -measurable and is -measurable, and show that is a monotone class. The Monotone Class Theorem then states, that this class is also a sigma-algebra and therefore I should infer that the measurability of and holds. But my problem is that I am having a hard time to construct such a class . Do you have any ideas for an approach? I'd appreciate the help.","\left(\Omega_1,\Sigma_1,\mu_1\right) \left(\Omega_2,\Sigma_2,\mu_2\right) A \Sigma_1\times\Sigma_2 \Sigma_1\times\Sigma_2  A_1\times A_2 = \left\{ \left(x_1,x_2\right) {}:{} x_1\in A_1,\quad x_2\in A_2 \right\},\quad A_1\in\Sigma_1,\quad A_2\in\Sigma_2  x_2\in\Omega_2 f\left( x_2\right) :=\mu_1\left( A_1\left(x_2\right)\right) x_1\in\Omega_1 g\left( x_1\right) :=\mu_2\left(A_2\left(x_1\right)\right)  A_1\left( x_2\right) = \left\{x_1\in\Omega_1\mid \left( x_1,x_2\right)\in A \right\}\in\Sigma_1   A_2\left( x_1\right) = \left\{x_2\in\Omega_2\mid \left( x_1,x_2\right)\in A \right\}\in\Sigma_2  f \Sigma_2 g \Sigma_1 \left(\mu_1\times\mu_2\right)(A) :=\int_{\Omega_2}f\left( x_2\right)\mu_2\left(\mathrm{d}x_2\right) =\int_{\Omega_1}g\left(x_1\right)\mu_1\left(\mathrm{d}x_1\right). f g A\in\Sigma_1\times\Sigma_2 A_1\left(x_2\right)\in\Sigma_1\quad\forall x_2\in\Omega_2\quad \quad A_2\left(x_2\right)\in\Sigma_2\quad\forall x_1\in\Omega_1. f g \mathcal{M} f \Sigma_2 g \Sigma_1 \mathcal{M} f g \mathcal{M}","['analysis', 'measurable-functions', 'product-measure', 'monotone-class-theorem']"
50,"Bounded first and second derivative on [-1,1]","Bounded first and second derivative on [-1,1]",,"Let f be a function which is twice differentiable on $\mathbb R$ . Suppose that there exist $\alpha, \beta>0$ such that $|f(x)|\le \alpha$ and $|f''(x)|\le \beta$ for any $x\in [-1,1]$ . Prove that $|f'(x)|\le \alpha+\beta$ for any $x\in [-1,1]$ . By applying the MVT on $f'$ I got $|{f'(x)-f'(0)\over x-0}|\le \beta $ . Hence $|f'(x)-f'(0)|\le B|x|$ . It follows that for any $x\in[-1,1]$ , $f'(0)-\beta \le f'(x)\le f'(0)+\beta$ . However, I do not really know how to use the condition $|f(x)|\le \alpha$ , and cannot figure out the proof. Any help is appreciated","Let f be a function which is twice differentiable on . Suppose that there exist such that and for any . Prove that for any . By applying the MVT on I got . Hence . It follows that for any , . However, I do not really know how to use the condition , and cannot figure out the proof. Any help is appreciated","\mathbb R \alpha, \beta>0 |f(x)|\le \alpha |f''(x)|\le \beta x\in [-1,1] |f'(x)|\le \alpha+\beta x\in [-1,1] f' |{f'(x)-f'(0)\over x-0}|\le \beta  |f'(x)-f'(0)|\le B|x| x\in[-1,1] f'(0)-\beta \le f'(x)\le f'(0)+\beta |f(x)|\le \alpha","['analysis', 'derivatives']"
51,Convergence of series $\sum_n|b_n g(nx+a_n)|$ in a set of positive measure implies convergence of $\sum_n|b_n|$,Convergence of series  in a set of positive measure implies convergence of,\sum_n|b_n g(nx+a_n)| \sum_n|b_n|,"This problem is in a set of old past qualifying tests I am using for practice. Suppose $g$ is bounded measurable function of period $T$ with $m(x:|f(x)|>0)>0$ . Let $(a_n)$ and $(b_n)$ numerical sequences and suppose that $G(x)=\sum_n|b_n g(nx+a_n)|$ converges for all points $x$ in a set of positive (Lebesgue) measure. Show that $\sum_nb_n$ converges absolutely. I define $E=\{x\in\mathbb{R}: G(x)<\infty\}$ and $E_k=\{x\in \mathbb{R}: G(x)\leq k\}$ . Then $E=\bigcup^\infty_{k=1}E_k$ which implies that there is $k'$ such $m(E_{k'})>0$ . From this I obtain that \begin{align*} \int_{E_{k'}}G(x)\,dx=\sum_n|b_n|\int_{E_{k'}}|g(nx+a_n)|\,dx<k'm(E_{k'}) \end{align*} This is as far I can go. I will appreciate any hint.",This problem is in a set of old past qualifying tests I am using for practice. Suppose is bounded measurable function of period with . Let and numerical sequences and suppose that converges for all points in a set of positive (Lebesgue) measure. Show that converges absolutely. I define and . Then which implies that there is such . From this I obtain that This is as far I can go. I will appreciate any hint.,"g T m(x:|f(x)|>0)>0 (a_n) (b_n) G(x)=\sum_n|b_n g(nx+a_n)| x \sum_nb_n E=\{x\in\mathbb{R}: G(x)<\infty\} E_k=\{x\in \mathbb{R}: G(x)\leq k\} E=\bigcup^\infty_{k=1}E_k k' m(E_{k'})>0 \begin{align*}
\int_{E_{k'}}G(x)\,dx=\sum_n|b_n|\int_{E_{k'}}|g(nx+a_n)|\,dx<k'm(E_{k'})
\end{align*}","['real-analysis', 'integration', 'analysis', 'lebesgue-integral']"
52,How can I show that this subset of $L^2(Y)$ is equal to the zero set if we assume $T:Y\rightarrow Y$ is strong mixing?,How can I show that this subset of  is equal to the zero set if we assume  is strong mixing?,L^2(Y) T:Y\rightarrow Y,"Let $(Y, \mathcal{A}, \mu)$ be a probability space and $T:Y\rightarrow Y$ a measure preseving map. Define $U=\{f\in L^2(Y): f\circ T=e^{-i\theta}f~~\text{a.e.}\}$ where $\theta\not \in 2\pi\Bbb{Z}$ . I want to show that if $T$ is strong mixing, then $U=\{0\}$ . Clearly $\{0\}\subset U$ . So one pick $f\in U$ , i.e. $f\circ T=e^{-i\theta}f$ . Now we can use the following fact with $g=1$ $T$ strong mixing iff for all $f,g\in L^2(Y)$ , $\int_Y f(T^n(y)) g(y)~d\mu=\int_Y f(y)~d\mu \int_Y g(y)~d\mu$ Then we deduce that $$\lim_{n\rightarrow \infty} \int_Y f(T^n(y))~d\mu=\int_Y f(y) ~d\mu$$ or equivalently $$\lim_{n\rightarrow \infty} (e^{-in\theta}-1)\int_Y f(y) ~d\mu=0$$ Since $\theta\not \in 2\pi \Bbb{Z}$ , we deduce that $e^{-in\theta}\neq 1$ , therefore $\int_Y f(y) ~d\mu=0$ . But from here I cannot deduce that $f=0$ . Can someone help me how to conclude? I also thought about working with $A:=\{y\in Y: f(y)\geq0\}$ and then define $g(y)=\Bbb{1}_A$ so that I somehow can guarantee that $f\geq 0$ and deduce $f=0$ but this also did not work.","Let be a probability space and a measure preseving map. Define where . I want to show that if is strong mixing, then . Clearly . So one pick , i.e. . Now we can use the following fact with strong mixing iff for all , Then we deduce that or equivalently Since , we deduce that , therefore . But from here I cannot deduce that . Can someone help me how to conclude? I also thought about working with and then define so that I somehow can guarantee that and deduce but this also did not work.","(Y, \mathcal{A}, \mu) T:Y\rightarrow Y U=\{f\in L^2(Y): f\circ T=e^{-i\theta}f~~\text{a.e.}\} \theta\not \in 2\pi\Bbb{Z} T U=\{0\} \{0\}\subset U f\in U f\circ T=e^{-i\theta}f g=1 T f,g\in L^2(Y) \int_Y f(T^n(y)) g(y)~d\mu=\int_Y f(y)~d\mu \int_Y g(y)~d\mu \lim_{n\rightarrow \infty} \int_Y f(T^n(y))~d\mu=\int_Y f(y) ~d\mu \lim_{n\rightarrow \infty} (e^{-in\theta}-1)\int_Y f(y) ~d\mu=0 \theta\not \in 2\pi \Bbb{Z} e^{-in\theta}\neq 1 \int_Y f(y) ~d\mu=0 f=0 A:=\{y\in Y: f(y)\geq0\} g(y)=\Bbb{1}_A f\geq 0 f=0","['functional-analysis', 'analysis', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
53,Rudin's RCA Theorem $4.18$.,Rudin's RCA Theorem .,4.18,"There is the definition which we need for the proof: There is the theorem which we need for the $4.18$ : There is $4.18$ : Let { $u_\alpha : \alpha$ $\in$ $A$ } be an orthonormal set in $H$ . Each of the following four conditions on { $u_\alpha$ } implies the other three: $(1)$ ${u_\alpha}$ is a maximal orthonormal set in $H$ . $(2)$ The set $P$ of all finite linear combinations of members of { $u_\alpha$ } is dense in $H$ . $(3)$ The equality $\sum_{\alpha \in A}$ $|\hat x(\alpha)|^2$ $=$ $||x||^2$ holds for every $x$ $\in$ $H$ . $(4)$ The equality $\sum_{\alpha \in A}$ $\hat x(\alpha)$ $\overline {\hat y(\alpha)}$ $=$ $(x,y)$ holds for all $x$ . $\in$ $H$ and $y$ $\in$ $H$ . We shall prove that $(1)$ $\to$ $(2)$ $\to$ $(3)$ $\to$ $(4)$ $\to$ $(1)$ . I understand that $(1)$ $\to$ $(2)$ . In the book it's written that if $(2)$ holds, so does $(3)$ by Theorem $4.17$ . I don't understand how does $(2)$ imply $(3)$ by Theorem $4.17$ ? Also there's written that the implication $(3)$ $\to$ $(4)$ follows from the easily proved Hilbert space identity $4(x,y)$ $=$ $||x+y||^2$ $-$ $||x-y||^2 $ $+$ $i||x+iy||^2$ $-$ $i||x-iy||^2$ . How does it follow from this identity? Any help would be appreciated.","There is the definition which we need for the proof: There is the theorem which we need for the : There is : Let { } be an orthonormal set in . Each of the following four conditions on { } implies the other three: is a maximal orthonormal set in . The set of all finite linear combinations of members of { } is dense in . The equality holds for every . The equality holds for all . and . We shall prove that . I understand that . In the book it's written that if holds, so does by Theorem . I don't understand how does imply by Theorem ? Also there's written that the implication follows from the easily proved Hilbert space identity . How does it follow from this identity? Any help would be appreciated.","4.18 4.18 u_\alpha : \alpha \in A H u_\alpha (1) {u_\alpha} H (2) P u_\alpha H (3) \sum_{\alpha \in A} |\hat x(\alpha)|^2 = ||x||^2 x \in H (4) \sum_{\alpha \in A} \hat x(\alpha) \overline {\hat y(\alpha)} = (x,y) x \in H y \in H (1) \to (2) \to (3) \to (4) \to (1) (1) \to (2) (2) (3) 4.17 (2) (3) 4.17 (3) \to (4) 4(x,y) = ||x+y||^2 - ||x-y||^2  + i||x+iy||^2 - i||x-iy||^2","['analysis', 'hilbert-spaces', 'combinations', 'orthonormal', 'dense-subspaces']"
54,Quantifier question proof,Quantifier question proof,,"The following question I have been doing is below and I'm having trouble with part ii). My working out is below but I think something is wrong. Would anyone be able to help me out with what is wrong with part ii? By the way I'm new here so my mathematical writing may not be typed out the best it can. Thank you! Consider the statement $$\exists K\in \{1,2,3,4,...\}\quad\forall x < \frac{1}{K}\quad\exists k\geq K,\quad\frac{kx}{1+kx^2}>\frac{1}{2x}$$ i) What is the negation of this statement? ii) Which is true, the statement or its negation? Prove it."" My answer for part i) is $$\forall K\in \{1,2,3,4,...\}\quad\exists x < \frac{1}{K}\quad\forall k\geq K,\quad\frac{kx}{1+kx^2}\leq\frac{1}{2x}.$$ My answer for part ii) is Let $K\in \{1,2,3,4,...\}$ and further let $x=\frac{1}{\sqrt{k}} < \frac{1}{K}$ . Now suppose that we have that $k\geq K$ . Then we get the following: $$\frac{kx}{1+kx^2}\leq\frac{1}{2x}$$ when $x=\frac{1}{\sqrt{k}}$ . Thus, the negation of the statement is true.","The following question I have been doing is below and I'm having trouble with part ii). My working out is below but I think something is wrong. Would anyone be able to help me out with what is wrong with part ii? By the way I'm new here so my mathematical writing may not be typed out the best it can. Thank you! Consider the statement i) What is the negation of this statement? ii) Which is true, the statement or its negation? Prove it."" My answer for part i) is My answer for part ii) is Let and further let . Now suppose that we have that . Then we get the following: when . Thus, the negation of the statement is true.","\exists K\in \{1,2,3,4,...\}\quad\forall x < \frac{1}{K}\quad\exists k\geq K,\quad\frac{kx}{1+kx^2}>\frac{1}{2x} \forall K\in \{1,2,3,4,...\}\quad\exists x < \frac{1}{K}\quad\forall k\geq K,\quad\frac{kx}{1+kx^2}\leq\frac{1}{2x}. K\in \{1,2,3,4,...\} x=\frac{1}{\sqrt{k}} < \frac{1}{K} k\geq K \frac{kx}{1+kx^2}\leq\frac{1}{2x} x=\frac{1}{\sqrt{k}}","['analysis', 'logic']"
55,Question about series as infinite summation,Question about series as infinite summation,,"I have two questions about the series. Grandi's series $1-1+1-1+ \cdots$ is not convergent since its partial summation is not convergent. But someone told me it could be convergent if we rearrange the series. For example, $1-1+1-1+1-1+...=(1-1)+(1-1)+(1-1)+...=0+0+0+...=0$ . (1) My question is for this new series, we still need to calculate its partial summation by definition and obviously, the partial summation is $S_n=1$ for odds and $S_n=0$ for even, how can we say it is now $lim S_n=0$ . About the definition of a series. Do we still regard series as infinite summation( $\sum_{n=1}^{\infty}a_n=a_1+a_2+...$ ) even have defined it as the limit of partial summation? And we have $\sum_{n=1}^{m-1}a_n+\sum_{n=m}^{\infty}a_n= \sum_{n=1}^{\infty}a_n$ for convergent series? And we allow add in parentheses like in (1)?","I have two questions about the series. Grandi's series is not convergent since its partial summation is not convergent. But someone told me it could be convergent if we rearrange the series. For example, . (1) My question is for this new series, we still need to calculate its partial summation by definition and obviously, the partial summation is for odds and for even, how can we say it is now . About the definition of a series. Do we still regard series as infinite summation( ) even have defined it as the limit of partial summation? And we have for convergent series? And we allow add in parentheses like in (1)?",1-1+1-1+ \cdots 1-1+1-1+1-1+...=(1-1)+(1-1)+(1-1)+...=0+0+0+...=0 S_n=1 S_n=0 lim S_n=0 \sum_{n=1}^{\infty}a_n=a_1+a_2+... \sum_{n=1}^{m-1}a_n+\sum_{n=m}^{\infty}a_n= \sum_{n=1}^{\infty}a_n,"['calculus', 'sequences-and-series', 'analysis']"
56,Initial value problems for Laplace equation,Initial value problems for Laplace equation,,"Usually in the textbooks well-posedness of the boundary value problem is studied for Laplace equation $$-\Delta u=0.$$ Why dont we study the Initial value problems like \begin{eqnarray} -\Delta u&=&0 \quad \text{ in } \mathbb{R}^2\\ u(x,0)=u_y(x,0)&=&0  \quad  \text{ on } \mathbb{R}  \end{eqnarray} Clearly, in the one dimensional case, the IVP given by \begin{eqnarray} -u_{xx}&=&0 \quad \text{ in } \mathbb{R}\\ u(0)=u'(0)&=&0   \end{eqnarray} is well-posed. Thanks in advance","Usually in the textbooks well-posedness of the boundary value problem is studied for Laplace equation Why dont we study the Initial value problems like Clearly, in the one dimensional case, the IVP given by is well-posed. Thanks in advance","-\Delta u=0. \begin{eqnarray}
-\Delta u&=&0 \quad \text{ in } \mathbb{R}^2\\
u(x,0)=u_y(x,0)&=&0  \quad  \text{ on } \mathbb{R} 
\end{eqnarray} \begin{eqnarray}
-u_{xx}&=&0 \quad \text{ in } \mathbb{R}\\
u(0)=u'(0)&=&0  
\end{eqnarray}","['analysis', 'partial-differential-equations']"
57,"some basic questions about topology- is this $[1,2]$ open under topology $\{\emptyset,X,[1,2]\}$",some basic questions about topology- is this  open under topology,"[1,2] \{\emptyset,X,[1,2]\}","I remember the lecturer said when you ask if a set is open or closed you need to specify the topology, then the following example I am not sure if is it closed or open. Let's say define a topology on $X=[0,\infty)$ and the topology is $\{\emptyset,X,[1,2]\}$ I got the following questions: 1 is $[1,2]$ considered as open by definition? if so, then 2 $[0,1)\cup(2,\infty)$ considered as closed, what about $[0,1)$ ? closed or Neither? I love to think like the finite union of closed is closed, even though I know that the finite union of neither open nor closed sets could be closed, but that is for the usual topology. Thank you for clearing my mind about these two questions. Extra edited As I retrive this post I have another mind hope you guys may help me to clear up. I agree that $[0,1)$ cannot construct by finite intersection nor arbitrary union. But if I look another way by arguing evey points in $[0,1)$ equipped with $\epsilon$ -radius ball still lay in $[0,1)$ . Here is what my confusion come in : the point at $0$ with $\epsilon$ -radius ball do not fully lay in $[0,1)$ becasue left hand side of the ball do not lay in $[0,1)$ , so $[0,1)$ is not open. However my another argument goes:  when centered at $0$ , there is no left hand side of $\epsilon$ -radius ball, beacuse we only defined on $[0,\infty)$ , so it is open. (I personally incline to this), why is this argument incorrect.","I remember the lecturer said when you ask if a set is open or closed you need to specify the topology, then the following example I am not sure if is it closed or open. Let's say define a topology on and the topology is I got the following questions: 1 is considered as open by definition? if so, then 2 considered as closed, what about ? closed or Neither? I love to think like the finite union of closed is closed, even though I know that the finite union of neither open nor closed sets could be closed, but that is for the usual topology. Thank you for clearing my mind about these two questions. Extra edited As I retrive this post I have another mind hope you guys may help me to clear up. I agree that cannot construct by finite intersection nor arbitrary union. But if I look another way by arguing evey points in equipped with -radius ball still lay in . Here is what my confusion come in : the point at with -radius ball do not fully lay in becasue left hand side of the ball do not lay in , so is not open. However my another argument goes:  when centered at , there is no left hand side of -radius ball, beacuse we only defined on , so it is open. (I personally incline to this), why is this argument incorrect.","X=[0,\infty) \{\emptyset,X,[1,2]\} [1,2] [0,1)\cup(2,\infty) [0,1) [0,1) [0,1) \epsilon [0,1) 0 \epsilon [0,1) [0,1) [0,1) 0 \epsilon [0,\infty)","['real-analysis', 'general-topology', 'analysis']"
58,Change of variable vs Fundamental Theorem of Calculus,Change of variable vs Fundamental Theorem of Calculus,,"I was thinking about integrals where we should be careful with the domain because we perform a trivial but not 1-1 change of variables. For example, in $$\int_{-1}^2\dfrac{4x^3dx}{1+x^4},$$ the classic change of variables $u=x^4$ is not injective; it can be evaluated simply by using the FTC, whose application here doesn't even see the fact that the $x^4$ is not 1-1. This fact actually freaked me out a bit: most of the examples that I know of for change of variable can be replaced by a direct (albeit uglier) application of the FTC, completely bypassing the bijectivity and other issues associated with change of variable. So, I am wondering whether there really is a gap in my reasoning, that is, whether $$\int_a^b f'(g(x))g'(x)dx=f(g(b))-f(g(a))$$ actually always holds (assuming of course continuous differentiability), no matter how many oscillations $f$ and $g$ have, no matter how many pre-images every $g(x)$ has, etc. In the above example, I would actually like to perform a change of variable $u=g(x),$ then—just then—apply the FTC; but that would require injectivity of $g,$ etc.","I was thinking about integrals where we should be careful with the domain because we perform a trivial but not 1-1 change of variables. For example, in the classic change of variables is not injective; it can be evaluated simply by using the FTC, whose application here doesn't even see the fact that the is not 1-1. This fact actually freaked me out a bit: most of the examples that I know of for change of variable can be replaced by a direct (albeit uglier) application of the FTC, completely bypassing the bijectivity and other issues associated with change of variable. So, I am wondering whether there really is a gap in my reasoning, that is, whether actually always holds (assuming of course continuous differentiability), no matter how many oscillations and have, no matter how many pre-images every has, etc. In the above example, I would actually like to perform a change of variable then—just then—apply the FTC; but that would require injectivity of etc.","\int_{-1}^2\dfrac{4x^3dx}{1+x^4}, u=x^4 x^4 \int_a^b f'(g(x))g'(x)dx=f(g(b))-f(g(a)) f g g(x) u=g(x), g,","['real-analysis', 'calculus', 'integration', 'analysis', 'substitution']"
59,A few $(3)$ questions regarding Spivak's proof of the Inverse Function Theorem.,A few  questions regarding Spivak's proof of the Inverse Function Theorem.,(3),"I have a few questions regarding Spivak's proof of the Inverse Function Theorem: Theorem: Let $f$ be a function $\mathbb{R}^n\to\mathbb{R}^n$ . If $\ \ \ \ a)$ $f$ is $C^1$ in an open set containing $a\in\mathbb{R}^n$ , and $\ \ \ \ b)$ $f'(a)$ is invertible i.e. $\det f'(a) \ne 0$ , then there is an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f:V\to W$ is bijective. $f^{-1}:W\to V$ is $C^1$ . The equation $$(f^{-1})'(y) = \left[f'(x)\right]^{-1}$$ holds for any $y\in W$ and $x := f^{-1}(y)$ . Proof: Letting $\lambda = f'(a)$ , we have that $$(\lambda^{-1}\circ f)'(a) = (\lambda^{-1})'(f(a))\circ \lambda = \lambda^{-1}\circ \lambda = \text{Id}.$$ That is, if the theorem holds for $\lambda^{-1}\circ f$ , then it clearly holds for $f$ . Therefore we may assume at the outset that $f'(a)$ is the identity. Whenever $f(a + h) = f(a)$ we have $$1 = \lim_{h\to 0}\frac{|h|}{|h|} = \lim_{h\to 0}\frac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0.$$ Meaning we cannot have $f(x)=f(a)$ for $x$ arbitrarily close to, but unequal to, $a$ . Therefore there is a closed rectangle $U$ containing $a$ in its interior such that $f(x)\ne f(a)$ for all $x\in U$ different than $a$ . Since $f$ is $C^1$ in an open set containing $a$ , we can also assume that $\det f'(x) \ne 0$ for all $x\in U$ . $|\partial_jf_i(x) - \partial_jf_i(a)|<1/2n^2$ for all $i,j$ , and $x\in U$ . Observe that $$|x_1-x_2|-|f(x_1)-f(x_2)| \le |f(x_1)-x_1 - [f(x_2)-x_2]| \le \frac{1}{2}|x_1-x_2|$$ where the first inequality comes from the Inverse Triangle Inequality , while the second is derived by $(3)$ and a Lemma . We obtain $|x_1-x_2|\le 2|f(x_1)-f(x_2)|$ for $x_1,x_2\in U$ . Now $f(\partial U)$ is a compact set which, by $(1)$ , does not contain $f(a)$ . Therefore there is a number $d>0$ such that $|f(a)-f(x)|\ge d$ for $x\in \partial U$ . Let $$W:= \{ y:|y-f(a)|<d/2 \} .$$ We have $|y-f(a)| < |y-f(x)|$ for any $y\in W$ and $x\in \partial U$ . We will show that for any $y\in W$ there is a unique $x\in \text{Int}(U)$ such that $f(x)=y$ . To prove this consider the function $g:U\to \mathbb{R}$ defined by $$g:x\mapsto |y-f(x)|^2 = \sum_{i=1}^n\big(y_i-f_i(x)\big)^2.$$ This function is continuous and therefore has a minimum on $U$ . If $x\in \partial U$ , then, by $(5)$ , we have $g(a) < g(x)$ . Therefore the minimum of $g$ does not occur on the boundary of $U$ . There is a point $x\in \text{Int}(U)$ such that $\partial_jg(x) = 0$ for all $j$ , that is $$\sum_{i=1}^n 2\big(y_i-f_i(x)\big)\partial_jf_i(x)=0 \ \ \ \ \forall j.$$ By (2) the matrix $(\partial_jf_i(x))$ has non-zero determinant. Therefore we must have $y_i - f_i(x) = 0$ for all $i$ , that is $y = f(x)$ . Question 1: why does the equation above and $(2)$ imply $y_i - f_i(x) = 0$ for all $i$ ? This proves the existence of $x$ . Uniqueness follows immediately from $(4)$ . If $V = (\text{Int}(U)) \cap f^{-1}(W)$ , we have shown that the function $f:V\to W$ has an inverse $f^{-1}: W \to V$ . We can rewrite $(4)$ as $|f^{-1}(y_1)-f^{-1}(y_2)|\le 2|y_1-y_2|$ for $y_1,y_2\in W$ . This shows that $f^{-1}$ is continuous. Only the proof that $f^{-1}$ is differentiable remains. Let $\mu = f'(x)$ . We will show that $f^{-1}$ is differentiable at $y$ with derivative $\mu^{-1}$ . For $x_1\in V$ , we have $$f(x_1) = f(x) + \mu(x_1-x) + \phi(x_1-x)$$ where $$\lim_{x_1\to x} \frac{|\phi(x_1-x)|}{|x_1-x|}=0.$$ Therefore $$\mu^{-1}\big(f(x_1)-f(x)\big) = x_1 - x + \mu^{-1}\big(\phi(x_1-x)\big).$$ Since every $y_1\in W$ is of the form $f(x_1)$ for some $x_1\in V$ , this can be written as $$f^{-1}(y_1) = f^{-1}(y) + \mu^{-1}(y_1-y) - \mu^{-1}\big(\phi(f^{-1}(y)-f^{-1}(y))\big),$$ and it therefore suffices to show that $$\lim_{y_1\to y}\frac{\left|\mu^{-1}\big(\phi(f^{-1}(y_1)-f^{-1}(y))\big)\right|}{|y_1-y|}=0.$$ Therefore it suffices to show that $$\lim_{y_1\to y}\frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|y_1-y|}=0.$$ Question 2: the trick here -I believe- requires us to divide (and multiply) by $\phi(f^{-1}(y_1)-f^{-1}(y))$ . How do we know said quantity is non-zero? Now $$\frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|y_1-y|} = \frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|f^{-1}(y_1)-f^{-1}(y)|} \frac{|f^{-1}(y_1)-f^{-1}(y)|}{|y_1-y|}.$$ Since $f^{-1}$ is continuous we have that $f^{-1}(y_1)\to f^{-1}(y)$ as $y_1\to y$ . Therefore the first factor approaches $0$ . Since, by $(6)$ , the second factor is less than $2$ , the product also approaches $0$ . Question 3: in some versions of the theorem, the function $f^{-1}$ is said to be $C^1$ , yet Spivak does not state -nor prove- such fact. How can it be proven?","I have a few questions regarding Spivak's proof of the Inverse Function Theorem: Theorem: Let be a function . If is in an open set containing , and is invertible i.e. , then there is an open set containing and an open set containing such that is bijective. is . The equation holds for any and . Proof: Letting , we have that That is, if the theorem holds for , then it clearly holds for . Therefore we may assume at the outset that is the identity. Whenever we have Meaning we cannot have for arbitrarily close to, but unequal to, . Therefore there is a closed rectangle containing in its interior such that for all different than . Since is in an open set containing , we can also assume that for all . for all , and . Observe that where the first inequality comes from the Inverse Triangle Inequality , while the second is derived by and a Lemma . We obtain for . Now is a compact set which, by , does not contain . Therefore there is a number such that for . Let We have for any and . We will show that for any there is a unique such that . To prove this consider the function defined by This function is continuous and therefore has a minimum on . If , then, by , we have . Therefore the minimum of does not occur on the boundary of . There is a point such that for all , that is By (2) the matrix has non-zero determinant. Therefore we must have for all , that is . Question 1: why does the equation above and imply for all ? This proves the existence of . Uniqueness follows immediately from . If , we have shown that the function has an inverse . We can rewrite as for . This shows that is continuous. Only the proof that is differentiable remains. Let . We will show that is differentiable at with derivative . For , we have where Therefore Since every is of the form for some , this can be written as and it therefore suffices to show that Therefore it suffices to show that Question 2: the trick here -I believe- requires us to divide (and multiply) by . How do we know said quantity is non-zero? Now Since is continuous we have that as . Therefore the first factor approaches . Since, by , the second factor is less than , the product also approaches . Question 3: in some versions of the theorem, the function is said to be , yet Spivak does not state -nor prove- such fact. How can it be proven?","f \mathbb{R}^n\to\mathbb{R}^n \ \ \ \ a) f C^1 a\in\mathbb{R}^n \ \ \ \ b) f'(a) \det f'(a) \ne 0 V a W f(a) f:V\to W f^{-1}:W\to V C^1 (f^{-1})'(y) = \left[f'(x)\right]^{-1} y\in W x := f^{-1}(y) \lambda = f'(a) (\lambda^{-1}\circ f)'(a) = (\lambda^{-1})'(f(a))\circ \lambda = \lambda^{-1}\circ \lambda = \text{Id}. \lambda^{-1}\circ f f f'(a) f(a + h) = f(a) 1 = \lim_{h\to 0}\frac{|h|}{|h|}
= \lim_{h\to 0}\frac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0. f(x)=f(a) x a U a f(x)\ne f(a) x\in U a f C^1 a \det f'(x) \ne 0 x\in U |\partial_jf_i(x) - \partial_jf_i(a)|<1/2n^2 i,j x\in U |x_1-x_2|-|f(x_1)-f(x_2)|
\le |f(x_1)-x_1 - [f(x_2)-x_2]|
\le \frac{1}{2}|x_1-x_2| (3) |x_1-x_2|\le 2|f(x_1)-f(x_2)| x_1,x_2\in U f(\partial U) (1) f(a) d>0 |f(a)-f(x)|\ge d x\in \partial U W:= \{ y:|y-f(a)|<d/2 \} . |y-f(a)| < |y-f(x)| y\in W x\in \partial U y\in W x\in \text{Int}(U) f(x)=y g:U\to \mathbb{R} g:x\mapsto |y-f(x)|^2 = \sum_{i=1}^n\big(y_i-f_i(x)\big)^2. U x\in \partial U (5) g(a) < g(x) g U x\in \text{Int}(U) \partial_jg(x) = 0 j \sum_{i=1}^n 2\big(y_i-f_i(x)\big)\partial_jf_i(x)=0 \ \ \ \ \forall j. (\partial_jf_i(x)) y_i - f_i(x) = 0 i y = f(x) (2) y_i - f_i(x) = 0 i x (4) V = (\text{Int}(U)) \cap f^{-1}(W) f:V\to W f^{-1}: W \to V (4) |f^{-1}(y_1)-f^{-1}(y_2)|\le 2|y_1-y_2| y_1,y_2\in W f^{-1} f^{-1} \mu = f'(x) f^{-1} y \mu^{-1} x_1\in V f(x_1) = f(x) + \mu(x_1-x) + \phi(x_1-x) \lim_{x_1\to x} \frac{|\phi(x_1-x)|}{|x_1-x|}=0. \mu^{-1}\big(f(x_1)-f(x)\big) = x_1 - x + \mu^{-1}\big(\phi(x_1-x)\big). y_1\in W f(x_1) x_1\in V f^{-1}(y_1) = f^{-1}(y) + \mu^{-1}(y_1-y) - \mu^{-1}\big(\phi(f^{-1}(y)-f^{-1}(y))\big), \lim_{y_1\to y}\frac{\left|\mu^{-1}\big(\phi(f^{-1}(y_1)-f^{-1}(y))\big)\right|}{|y_1-y|}=0. \lim_{y_1\to y}\frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|y_1-y|}=0. \phi(f^{-1}(y_1)-f^{-1}(y)) \frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|y_1-y|}
= \frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|f^{-1}(y_1)-f^{-1}(y)|}
\frac{|f^{-1}(y_1)-f^{-1}(y)|}{|y_1-y|}. f^{-1} f^{-1}(y_1)\to f^{-1}(y) y_1\to y 0 (6) 2 0 f^{-1} C^1","['real-analysis', 'analysis', 'multivariable-calculus', 'proof-explanation']"
60,Cantor's Diagonalization Process(CDP) and North-east to South-west Diagonalization Process(NSDP),Cantor's Diagonalization Process(CDP) and North-east to South-west Diagonalization Process(NSDP),,"While looking at the function $f:\mathbb{N}\rightarrow \mathbb{N} \times \mathbb{N}$ , I accidentally made an inversion in my labelling for $f(x)$ What I mean is, we go with a zig-zag path in CDP(either $f(2)=(1,2)$ or $f(2)=(2,1)$ , I am going with the former), but while writing the value for each of the members of $\mathbb{N}$ , I went in the north-east(NE) to south-west(SW) manner. Elaboration: $$(1,1),\ (1,2),\ (1,3),\ ...\\ (2,1),\ (2,2),\ (2,3),\ ... \\ (3,1),\ (3,2),\ (3,3),\ ... \\   .\\ .\\ .\\$$ $\big($ Also, regardless of CDP or NSDP, we can observe below, that the last element in each row $n(n\geq2)$ is of the form ${n+1 \choose 2} \ \big( 3={2+1 \choose 2}$ , $6={3+1 \choose 2}$ , $10={4+1 \choose 2}\big)\big)$ In CDP we have: $f(1)=(1,1)$ $f(2)=(1,2), \ f(3)=(2,1)$ ( $\leftarrow$ NE to SW) $f(4)=(3,1), \ f(5)=(2,2),\ f(6)=(1,3)$ ( $\rightarrow$ SW to NE) $f(7)=(1,4),\ f(8)=(2,3),\ f(9)=(3,2)\ f(10)=(4,1)$ ( $\leftarrow$ NE to SW) $f(11)=(5,1),\ f(12)=(4,2),\ f(13)=(3,3), \ f(14)=(2,4),\ f(15)=(1,5)$ ( $\rightarrow$ SW to NE) ... But, I did: $f(1)=(1,1)$ $f(2)=(1,2), \ f(3)=(2,1)$ ( $\leftarrow$ NE to SW) $f(4)=(1,3), \ f(5)=(2,2),\ f(6)=(3,1)$ ( $\leftarrow$ NE to SW) $f(7)=(1,4),\ f(8)=(2,3),\ f(9)=(3,2)\ f(10)=(4,1)$ ( $\leftarrow$ NE to SW) $f(11)=(1,5),\ f(12)=(2,4),\ f(13)=(3,3), \ f(14)=(4,2),\ f(15)=(5,1)$ ( $\leftarrow$ NE to SW) ... Now, in doing so I saw a pattern: Take the first column. Let's label the set of the domains' elements in the first column. $$\begin{array}\{X_1&:=\{1,2,4,7,11,16,22,29,37,...\} \\                        &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array}$$ and for the range: $$\begin{array}\{Y_1&:=\{(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(1,8),(1,9),...\} \\                        &=\{(1,y_1),(1,y_2),(1,y_3,(1,y_4),(1,y_5),(1,y_6),(1,y_7),(1,y_8),(1,y_9),...\}\\ \end{array}$$ We can see that the first co-ordinate is constant, and the second one increases at a linear pace. Moreover, $x_i+y_i=x_{i+1}$ Also, if we observe the $x_r's$ , $x_r=\frac{(r-1)(r)}{2}+1, \ \forall r \in \mathbb{N}$ In case of the second column: $$\begin{array}\{X_2&:=\{3,5,8,12,17,23,30,38,47,...\} \\                        &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array}$$ $$\begin{array}\{Y_2&:=\{(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(2,7),(2,8),(2,9),...\} \\                        &=\{(2,y_1),(2,y_2),(2,y_3,(2,y_4),(2,y_5),(2,y_6),(2,y_7),(2,y_8),(2,y_9),...\}\\ \end{array}$$ Observing the $x_s's$ , $x_s=\frac{(s)(s+1)}{2}+2, \ \forall s \in \mathbb{N}$ In case of the third column: $$\begin{array}\{X_3&:=\{6,9,13,18,24,31,39,48,58,...\} \\                        &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array}$$ $$\begin{array}\{Y_3&:=\{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7),(3,8),(3,9),...\} \\                        &=\{(3,y_1),(3,y_2),(3,y_3,(3,y_4),(3,y_5),(3,y_6),(3,y_7),(3,y_8),(3,y_9),...\}\\ \end{array}$$ Observing the $x_p's$ , $x_p=\frac{(p+1)(p+2)}{2}+3, \ \forall p \in \mathbb{N}$ By the first principle of mathematical induction, we have for the $n^{th}$ column: for sets $X_n,Y_n$ : $\big(x_q=\frac{(q+(n-1))(q+(n-2))}{2}+n, \forall q \in \mathbb{N} \big)\forall n \in \mathbb{N}$ Thus, in general, the function looks like $f(x)=(\alpha,\beta)$ , where $x=\frac{(\beta+(\alpha-1))(\beta+(\alpha-2))}{2}+\alpha$ . Hence there is a dependence of at least two variables that we know of, on x. Now, my question is: (A) In NSDP, can I make a proper explicit form of the function?(I just know $x$ drives $\alpha,\beta$ , but I am unable to arbitrarily locate the value, other than knowing the column type slection, which is good while dealing with one column at a time, but in a general manner, I am not able to make the function's definition explicit.) (B) In CDP, how do I proceed with the logic used in NSDP, or is there a better way out? (I know that the last element of each row $n$ is of the form ${n+1 \choose 2}$ and the sum of co-ordinates of each elements is equal in case of each row(eg: 1+3=2+2=3+1).)","While looking at the function , I accidentally made an inversion in my labelling for What I mean is, we go with a zig-zag path in CDP(either or , I am going with the former), but while writing the value for each of the members of , I went in the north-east(NE) to south-west(SW) manner. Elaboration: Also, regardless of CDP or NSDP, we can observe below, that the last element in each row is of the form , , In CDP we have: ( NE to SW) ( SW to NE) ( NE to SW) ( SW to NE) ... But, I did: ( NE to SW) ( NE to SW) ( NE to SW) ( NE to SW) ... Now, in doing so I saw a pattern: Take the first column. Let's label the set of the domains' elements in the first column. and for the range: We can see that the first co-ordinate is constant, and the second one increases at a linear pace. Moreover, Also, if we observe the , In case of the second column: Observing the , In case of the third column: Observing the , By the first principle of mathematical induction, we have for the column: for sets : Thus, in general, the function looks like , where . Hence there is a dependence of at least two variables that we know of, on x. Now, my question is: (A) In NSDP, can I make a proper explicit form of the function?(I just know drives , but I am unable to arbitrarily locate the value, other than knowing the column type slection, which is good while dealing with one column at a time, but in a general manner, I am not able to make the function's definition explicit.) (B) In CDP, how do I proceed with the logic used in NSDP, or is there a better way out? (I know that the last element of each row is of the form and the sum of co-ordinates of each elements is equal in case of each row(eg: 1+3=2+2=3+1).)","f:\mathbb{N}\rightarrow \mathbb{N} \times \mathbb{N} f(x) f(2)=(1,2) f(2)=(2,1) \mathbb{N} (1,1),\ (1,2),\ (1,3),\ ...\\
(2,1),\ (2,2),\ (2,3),\ ... \\
(3,1),\ (3,2),\ (3,3),\ ... \\
  .\\
.\\
.\\ \big( n(n\geq2) {n+1 \choose 2} \ \big( 3={2+1 \choose 2} 6={3+1 \choose 2} 10={4+1 \choose 2}\big)\big) f(1)=(1,1) f(2)=(1,2), \ f(3)=(2,1) \leftarrow f(4)=(3,1), \ f(5)=(2,2),\ f(6)=(1,3) \rightarrow f(7)=(1,4),\ f(8)=(2,3),\ f(9)=(3,2)\ f(10)=(4,1) \leftarrow f(11)=(5,1),\ f(12)=(4,2),\ f(13)=(3,3), \ f(14)=(2,4),\ f(15)=(1,5) \rightarrow f(1)=(1,1) f(2)=(1,2), \ f(3)=(2,1) \leftarrow f(4)=(1,3), \ f(5)=(2,2),\ f(6)=(3,1) \leftarrow f(7)=(1,4),\ f(8)=(2,3),\ f(9)=(3,2)\ f(10)=(4,1) \leftarrow f(11)=(1,5),\ f(12)=(2,4),\ f(13)=(3,3), \ f(14)=(4,2),\ f(15)=(5,1) \leftarrow \begin{array}\{X_1&:=\{1,2,4,7,11,16,22,29,37,...\} \\
                       &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array} \begin{array}\{Y_1&:=\{(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(1,8),(1,9),...\} \\
                       &=\{(1,y_1),(1,y_2),(1,y_3,(1,y_4),(1,y_5),(1,y_6),(1,y_7),(1,y_8),(1,y_9),...\}\\ \end{array} x_i+y_i=x_{i+1} x_r's x_r=\frac{(r-1)(r)}{2}+1, \ \forall r \in \mathbb{N} \begin{array}\{X_2&:=\{3,5,8,12,17,23,30,38,47,...\} \\
                       &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array} \begin{array}\{Y_2&:=\{(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(2,7),(2,8),(2,9),...\} \\
                       &=\{(2,y_1),(2,y_2),(2,y_3,(2,y_4),(2,y_5),(2,y_6),(2,y_7),(2,y_8),(2,y_9),...\}\\ \end{array} x_s's x_s=\frac{(s)(s+1)}{2}+2, \ \forall s \in \mathbb{N} \begin{array}\{X_3&:=\{6,9,13,18,24,31,39,48,58,...\} \\
                       &=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,...\}\\ \end{array} \begin{array}\{Y_3&:=\{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7),(3,8),(3,9),...\} \\
                       &=\{(3,y_1),(3,y_2),(3,y_3,(3,y_4),(3,y_5),(3,y_6),(3,y_7),(3,y_8),(3,y_9),...\}\\ \end{array} x_p's x_p=\frac{(p+1)(p+2)}{2}+3, \ \forall p \in \mathbb{N} n^{th} X_n,Y_n \big(x_q=\frac{(q+(n-1))(q+(n-2))}{2}+n, \forall q \in \mathbb{N} \big)\forall n \in \mathbb{N} f(x)=(\alpha,\beta) x=\frac{(\beta+(\alpha-1))(\beta+(\alpha-2))}{2}+\alpha x \alpha,\beta n {n+1 \choose 2}","['real-analysis', 'sequences-and-series', 'analysis', 'elementary-set-theory', 'set-theory']"
61,Grafakos Classical Fourier Analysis problem 1.1.14,Grafakos Classical Fourier Analysis problem 1.1.14,,"I'll first type the problem. Let $(X,\mu)$ be a measure space and let $s>0$ . (a) Let $f$ be a measurable function on $X$ . Show that if $0 < p < q < \infty$ , we have $$ \int_{|f| \leq s} |f|^q \leq \frac{q}{q-p} s^{q-p} \|f\|_{L^{p,\infty}}^p. $$ (b) Let $f_j, 1 \le j \le m$ , be measurable functions on $X$ and let $0 < p < \infty$ . Show that $$\bigg|\bigg|\max_{1 \leq j \leq m} |f_j|\bigg|\bigg|_{L^{p,\infty}}^p \leq \sum_{j=1}^m \| f_j\|_{L^{p,\infty}}^p. $$ (c) Conclude from part (b) that for $0 < p < 1$ we have $$\|f_1 + \dots + f_m \|_{L^{p,\infty}}^p \leq \frac{2-p}{1-p} \sum_{j=1}^m \|f_j\|_{L^{p,\infty}}^p. $$ Hint: Part (a): Use the distribution function. Part (c): First obtain the estimate $$d_{f_1+\dots+ f_m}(\alpha) ≤ \mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})+d_{\max_j| f_j|}(\alpha)$$ for all $\alpha > 0$ and then use part (b). I have managed to solve part (a) and (b) and to prove the estimate of the hint given for part (c) but I haven't been able to solve part (c). What I have done is this $$\| f_1+\dots+f_m\|_{L^{p,\infty}}^p = \sup_{\alpha> 0} \alpha^p d_{f_1+\dots+f_m}(\alpha). $$ Thus I need to bound the right hand side for any $\alpha>0$ . We have, by the estimate given in the hint \begin{align*}\alpha^p d_{f_1+\dots+ f_m}(\alpha) &\le \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})+\alpha^pd_{\max j} | f_j|(\alpha) \\ &\leq \alpha^p\mu({| f_1+\dots+ f_m|>α,\max| f_j|≤\alpha}) + \bigg|\bigg|\max_{1 \leq j \leq m} |f_j|\bigg|\bigg|_{L^{p,\infty}}^p\\ & \leq \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})+\sum_{j=1}^m \| f_j\|_{L^{p,\infty}}^p \end{align*} So I need to estimate $$\alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|\le\alpha})$$ I noticed that if in part (a) I plug $q = 1$ and $s =1$ for $g= \max |f_j|$ I would obtain the result but I can't see how to actually relate $\alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})$ to $\int_{|\max_j |f_j|| \leq 1} | \max |f_j||$ Any help is appreciated.","I'll first type the problem. Let be a measure space and let . (a) Let be a measurable function on . Show that if , we have (b) Let , be measurable functions on and let . Show that (c) Conclude from part (b) that for we have Hint: Part (a): Use the distribution function. Part (c): First obtain the estimate for all and then use part (b). I have managed to solve part (a) and (b) and to prove the estimate of the hint given for part (c) but I haven't been able to solve part (c). What I have done is this Thus I need to bound the right hand side for any . We have, by the estimate given in the hint So I need to estimate I noticed that if in part (a) I plug and for I would obtain the result but I can't see how to actually relate to Any help is appreciated.","(X,\mu) s>0 f X 0 < p < q < \infty  \int_{|f| \leq s} |f|^q \leq \frac{q}{q-p} s^{q-p} \|f\|_{L^{p,\infty}}^p.  f_j, 1 \le j \le m X 0 < p < \infty \bigg|\bigg|\max_{1 \leq j \leq m} |f_j|\bigg|\bigg|_{L^{p,\infty}}^p \leq \sum_{j=1}^m \| f_j\|_{L^{p,\infty}}^p.  0 < p < 1 \|f_1 + \dots + f_m \|_{L^{p,\infty}}^p \leq \frac{2-p}{1-p} \sum_{j=1}^m \|f_j\|_{L^{p,\infty}}^p.  d_{f_1+\dots+ f_m}(\alpha) ≤ \mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})+d_{\max_j| f_j|}(\alpha) \alpha > 0 \| f_1+\dots+f_m\|_{L^{p,\infty}}^p = \sup_{\alpha> 0} \alpha^p d_{f_1+\dots+f_m}(\alpha).  \alpha>0 \begin{align*}\alpha^p d_{f_1+\dots+ f_m}(\alpha) &\le \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})+\alpha^pd_{\max j} | f_j|(\alpha) \\
&\leq \alpha^p\mu({| f_1+\dots+ f_m|>α,\max| f_j|≤\alpha}) + \bigg|\bigg|\max_{1 \leq j \leq m} |f_j|\bigg|\bigg|_{L^{p,\infty}}^p\\
& \leq \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha})+\sum_{j=1}^m \| f_j\|_{L^{p,\infty}}^p
\end{align*} \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|\le\alpha}) q = 1 s =1 g= \max |f_j| \alpha^p\mu({| f_1+\dots+ f_m|>\alpha,\max| f_j|≤\alpha}) \int_{|\max_j |f_j|| \leq 1} | \max |f_j||","['real-analysis', 'analysis', 'inequality', 'fourier-analysis', 'weak-lp-spaces']"
62,Proof that countable union of countable sets is valid?,Proof that countable union of countable sets is valid?,,"Analysis beginner here. Of course this has been asked many times, but all the others proofs I found were slightly different from what I have. So I was just wondering whether this is valid, and in particular, if it is equivalent to the diagonal argument that is found in Rudin, for example. I think so, but I am not sure. Thank you! Let $A_1$ , $A_2$ , ... be a sequence of countable sets. Then for each $i \in \mathbb{N}$ , we have $A_i = \{a_{i,1},a_{i,2},...\}$ . Define $B=\bigcup_{i=1}^{\infty} A_i$ . For each element $b\in B$ , we have $b \in A_i$ for at least one $i$ , so $b = a_{i,j}$ for some positive integers $i,j$ . Pick some element of $B$ and call it $b_k$ . Define $f(k) = min\{ i \in \mathbb{N}: b_k \in A_i \}$ and $g(k, f(k)) = \{j \in \mathbb{N}: b_k = a_{f(k),j} \}$ Then define the sequence $k \rightarrow b_k$ by $\{b_1, b_2, ...\}=\{a_{f(1),g(1,f(1))},a_{f(2),g(2,f(2))},...$ }. This sequence contains each element of $B$ exactly once, so $B$ is countable.","Analysis beginner here. Of course this has been asked many times, but all the others proofs I found were slightly different from what I have. So I was just wondering whether this is valid, and in particular, if it is equivalent to the diagonal argument that is found in Rudin, for example. I think so, but I am not sure. Thank you! Let , , ... be a sequence of countable sets. Then for each , we have . Define . For each element , we have for at least one , so for some positive integers . Pick some element of and call it . Define and Then define the sequence by }. This sequence contains each element of exactly once, so is countable.","A_1 A_2 i \in \mathbb{N} A_i = \{a_{i,1},a_{i,2},...\} B=\bigcup_{i=1}^{\infty} A_i b\in B b \in A_i i b = a_{i,j} i,j B b_k f(k) = min\{ i \in \mathbb{N}: b_k \in A_i \} g(k, f(k)) = \{j \in \mathbb{N}: b_k = a_{f(k),j} \} k \rightarrow b_k \{b_1, b_2, ...\}=\{a_{f(1),g(1,f(1))},a_{f(2),g(2,f(2))},... B B","['analysis', 'elementary-set-theory', 'solution-verification']"
63,Convexity of a ball in any normed space and in any metric space [closed],Convexity of a ball in any normed space and in any metric space [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I would like to show that an open (and closed) ball in a normed space is convex. Consider $(X,\lVert\rVert)$ a normed space and $B(X_0, r)$ an open ball centered in $X_0\in X$ . Take $X_1\in B(X_0, r)$ , $X_2\in B(X_0, r)$ and $s\in[0,1]$ Clearly we have : $\lVert sX_1 + (1-s)X_2 - X_0\rVert = \lVert sX_1 - sX_0 + (1-s)X_2 -(1-s)X_0\rVert $ $ =\lVert s(X_1 - X_0) + (1-s)(X_2 - X_0)\rVert\leq s\lVert X_1 - X_0\rVert + (1-s)\lVert X_2 - X_0\rVert < sr + (1-s)r = r $ In the same way we conclude for a closed ball in $X$ . I would like to know if this result can be extended to any metric space ? I have tried using the property of the distance function but I was not successfull, here is my attempt. Consider $(X,d)$ a metric space and $B(X_0, r)$ . Then for all $s\in[0,1]$ : $d(s(X_1-X_0) + (1-s)(X_2-X_0), X_0)\leq d(s(X_1-X_0) + (1-s)(X_2-X_0), s(X_1-X_0)) + d(s(X_1-X_0), X_0)$ and get stuck here because I don't see any property that could help me at this point, obviously I have tried to do something the triangle inequality but was not successfull. Any idea please ? I think I am missing something Thank you a lot ! EDIT : The question is not correct put in this way since we can define plenty of metric space where the notion of convexity has no meaning. We need to consider metric space over a vector space as did Esgeriath in his answer. Thus the question is the following : Is a ball in a metric space over a vector space convex ? Thanks to Esgeriath and geetha290krm for their helps.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I would like to show that an open (and closed) ball in a normed space is convex. Consider a normed space and an open ball centered in . Take , and Clearly we have : In the same way we conclude for a closed ball in . I would like to know if this result can be extended to any metric space ? I have tried using the property of the distance function but I was not successfull, here is my attempt. Consider a metric space and . Then for all : and get stuck here because I don't see any property that could help me at this point, obviously I have tried to do something the triangle inequality but was not successfull. Any idea please ? I think I am missing something Thank you a lot ! EDIT : The question is not correct put in this way since we can define plenty of metric space where the notion of convexity has no meaning. We need to consider metric space over a vector space as did Esgeriath in his answer. Thus the question is the following : Is a ball in a metric space over a vector space convex ? Thanks to Esgeriath and geetha290krm for their helps.","(X,\lVert\rVert) B(X_0, r) X_0\in X X_1\in B(X_0, r) X_2\in B(X_0, r) s\in[0,1] \lVert sX_1 + (1-s)X_2 - X_0\rVert = \lVert sX_1 - sX_0 + (1-s)X_2 -(1-s)X_0\rVert   =\lVert s(X_1 - X_0) + (1-s)(X_2 - X_0)\rVert\leq s\lVert X_1 - X_0\rVert + (1-s)\lVert X_2 - X_0\rVert < sr + (1-s)r = r  X (X,d) B(X_0, r) s\in[0,1] d(s(X_1-X_0) + (1-s)(X_2-X_0), X_0)\leq d(s(X_1-X_0) + (1-s)(X_2-X_0), s(X_1-X_0)) + d(s(X_1-X_0), X_0)","['analysis', 'metric-spaces', 'normed-spaces', 'convex-analysis']"
64,Is this PDE $xu_x-yu_y=u$ Well posed?,Is this PDE  Well posed?,xu_x-yu_y=u,"We have the given problem $$xu_x-yu_y=u,\ x>0, \ y>0 \  (1)$$ $$u(x,x)=x^2, \ x>0, \ (2)$$ They ask to check if the problem is well-posed and solve it next. I know that a problem is well-posed if: It has a solution The solution is unique A small change to PDE/side conditions produce only small changes in the solution. Also, we can observe that the PDE is linear, first order and homogeneous if we write it in the form $$xu_x-yu_y-u=0,\ x>0, \ y>0 \  (1)$$ $$u(x,x)=x^2, \ x>0, \ (2)$$ With characteristic curves $$\frac{dx}{dt}=x\Leftrightarrow \frac{1}{x} \frac{dx}{dt}=1$$ $$\frac{dy}{dt}=-y \Leftrightarrow -\frac{1}{y} \frac{dy}{dt}=1 $$ and $$\frac{du}{dt}=u \Leftrightarrow \frac{1}{u} \frac{du}{dt}=1$$ From which we obtain : $\begin{cases} yx=c_1 \\ \frac{u}{x}=c_2 \\ \end{cases}$ Combine the forms: $$c_2=G(c_1) \Leftrightarrow \frac{u}{x}=G(xy)\Leftrightarrow u=xG(yx)$$ Now, using the initial condition $u(x,x)=x^2$ gives $$x^2=xG(x*x) \Leftrightarrow G(x^2)=x $$ So, how could I check if it is well-posed before solving it? And if yes!!! How? Any suggestion?","We have the given problem They ask to check if the problem is well-posed and solve it next. I know that a problem is well-posed if: It has a solution The solution is unique A small change to PDE/side conditions produce only small changes in the solution. Also, we can observe that the PDE is linear, first order and homogeneous if we write it in the form With characteristic curves and From which we obtain : Combine the forms: Now, using the initial condition gives So, how could I check if it is well-posed before solving it? And if yes!!! How? Any suggestion?","xu_x-yu_y=u,\ x>0, \ y>0 \  (1) u(x,x)=x^2, \ x>0, \ (2) xu_x-yu_y-u=0,\ x>0, \ y>0 \  (1) u(x,x)=x^2, \ x>0, \ (2) \frac{dx}{dt}=x\Leftrightarrow \frac{1}{x} \frac{dx}{dt}=1 \frac{dy}{dt}=-y \Leftrightarrow -\frac{1}{y} \frac{dy}{dt}=1  \frac{du}{dt}=u \Leftrightarrow \frac{1}{u} \frac{du}{dt}=1 \begin{cases}
yx=c_1 \\
\frac{u}{x}=c_2 \\
\end{cases} c_2=G(c_1) \Leftrightarrow \frac{u}{x}=G(xy)\Leftrightarrow u=xG(yx) u(x,x)=x^2 x^2=xG(x*x) \Leftrightarrow G(x^2)=x ","['analysis', 'partial-differential-equations', 'partial-derivative', 'problem-solving', 'linear-pde']"
65,"Let $\{\phi_n\}$ be an orthonormal set in $L^2(\mu)$, $g\in L^2(\mu)$ such that $|\phi_n(x)|\le |g(x)|$ and $\sum a_n\phi_n(x)$ converges a.e.","Let  be an orthonormal set in ,  such that  and  converges a.e.",\{\phi_n\} L^2(\mu) g\in L^2(\mu) |\phi_n(x)|\le |g(x)| \sum a_n\phi_n(x),"Let $\mu$ be a  positive measure on measurable space $(X,\mathfrak{M})$ . Let $\{\phi_n\}$ be an orthonormal set in $L^2(\mu)$ and $g\in L^2(\mu)$ such that $$|\phi_n(x)|\le |g(x)|\text{ for a.e. }x\ \forall n$$ and $\sum\limits_{n=1}^\infty a_n\phi_n(x)$ converges a.e., then prove that $\lim a_n=0$ I define $f(x)=\sum\limits_{n=1}^\infty a_n\phi_n(x)$ . I want to prove that $f\in L^2(\mu)$ . If I canprove so, as $\{\phi_n\}$ is orthonormal set, $\lVert f\rVert_2=\sum|a_n|^2<\infty$ , therefore $\lim a_n=0$ . By Minkowski integral inequality, we have- $\lVert f\rVert_2$ $=\left[\int|\sum a_n\phi_n(x)|^2\ dx\right]^{1/2}$ $\le \sum\left[\int|a_n\phi_n(x)|^2 dx\right]^{1/2}$ $\le \sum|a_n|\left[\int|g(x)|^2\ dx\right]^{1/2}$ $=\sum |a_n|\lVert g\rVert_2$ But this says nothing. Can anyone suggest me a wayout to finish the proof? Thanks for your help in advance.","Let be a  positive measure on measurable space . Let be an orthonormal set in and such that and converges a.e., then prove that I define . I want to prove that . If I canprove so, as is orthonormal set, , therefore . By Minkowski integral inequality, we have- But this says nothing. Can anyone suggest me a wayout to finish the proof? Thanks for your help in advance.","\mu (X,\mathfrak{M}) \{\phi_n\} L^2(\mu) g\in L^2(\mu) |\phi_n(x)|\le |g(x)|\text{ for a.e. }x\ \forall n \sum\limits_{n=1}^\infty a_n\phi_n(x) \lim a_n=0 f(x)=\sum\limits_{n=1}^\infty a_n\phi_n(x) f\in L^2(\mu) \{\phi_n\} \lVert f\rVert_2=\sum|a_n|^2<\infty \lim a_n=0 \lVert f\rVert_2 =\left[\int|\sum a_n\phi_n(x)|^2\ dx\right]^{1/2} \le \sum\left[\int|a_n\phi_n(x)|^2 dx\right]^{1/2} \le \sum|a_n|\left[\int|g(x)|^2\ dx\right]^{1/2} =\sum |a_n|\lVert g\rVert_2","['functional-analysis', 'analysis', 'measure-theory', 'hilbert-spaces', 'lp-spaces']"
66,Fourier transform of product: proof without invoking Fourier inversion theorem,Fourier transform of product: proof without invoking Fourier inversion theorem,,"Let $f,g$ be Schwartz functions, then $f*g$ and $fg$ are both Schwartz functions. Denote the Fourier tranform of $f$ by $F(f)$ . It is very straightforward to prove $F(f*g)=F(f)F(g)$ from definition and Fubini theorem. However, in most textbooks on Fourier analysis, the other formula $F(fg)=F(f)*F(g)$ is often not proved by direct methods. Instead, they prove the Fourier inversion formula first, and then apply Fourier transform to both sides of $F^{-1}(f*g)=F^{-1}(f)F^{-1}(g)$ . I am wondering whether we can give a proof of $F(fg)=F(f)*F(g)$ without invoking Fourier inversion theorem. In other words, is Fourier inversion theorem essential to this formula? I have tried to prove it directly but find it very difficult. I have little knowledge in mathematical logics, so I don't know whether it is a valid question,  but it really bothers me. Thank you very much if you are willing to help.","Let be Schwartz functions, then and are both Schwartz functions. Denote the Fourier tranform of by . It is very straightforward to prove from definition and Fubini theorem. However, in most textbooks on Fourier analysis, the other formula is often not proved by direct methods. Instead, they prove the Fourier inversion formula first, and then apply Fourier transform to both sides of . I am wondering whether we can give a proof of without invoking Fourier inversion theorem. In other words, is Fourier inversion theorem essential to this formula? I have tried to prove it directly but find it very difficult. I have little knowledge in mathematical logics, so I don't know whether it is a valid question,  but it really bothers me. Thank you very much if you are willing to help.","f,g f*g fg f F(f) F(f*g)=F(f)F(g) F(fg)=F(f)*F(g) F^{-1}(f*g)=F^{-1}(f)F^{-1}(g) F(fg)=F(f)*F(g)","['real-analysis', 'analysis', 'fourier-analysis', 'fourier-transform', 'harmonic-analysis']"
67,Which region is this triple integral reffering to?,Which region is this triple integral reffering to?,,"$$Integral =  \iiint_V x+z\;dV $$ $$Region$$ $$x^2 + y^2 + z^2 \leq 1$$ $$z \leq \sqrt{x^2 + y^2}$$ Geogebra shows that the region looks like in the picture below but given the inquality sign in the equations, is the actual region inside the cone and sphere (icecream cone) or is the actual region inside the sphere but minus the icecream cone? Initially I thought it is the first one but given the inquality, it made me think it is the second one. Which one is it? Thank you very much!","Geogebra shows that the region looks like in the picture below but given the inquality sign in the equations, is the actual region inside the cone and sphere (icecream cone) or is the actual region inside the sphere but minus the icecream cone? Initially I thought it is the first one but given the inquality, it made me think it is the second one. Which one is it? Thank you very much!",Integral =  \iiint_V x+z\;dV  Region x^2 + y^2 + z^2 \leq 1 z \leq \sqrt{x^2 + y^2},"['calculus', 'integration', 'analysis', 'multivariable-calculus', 'spherical-coordinates']"
68,finding partial derivative using Rudin limit definition,finding partial derivative using Rudin limit definition,,"I was just reading the definition of Rudin on partial derivative given $f:\mathbb{R}^n \to \mathbb{R}^m$ , $e_1,\dots,e_n$ and $u_1,\dots ,u_m$ be basis for the spaces, we have $(D_jf_i)(x)=\lim_{t\to 0}\frac{f_i(x+te_j)-f_i(x)}{t}$ . Where $f_i(x)=f(x)\cdot u_i$ Now let $f(x,y)=\frac{xy}{x^2+y^2}$ and $f(0,0)=0$ , if I want to use the definition to evaluate partial of $x$ at $(x,y)\neq 0$ , I have $D_1f_i(x,y)=\lim_{t\to 0}\frac{f_i(x+t,y)-f_i(x)}{t}$ , what exactly is the component function $f_i$ here? Since $m=1$ I'm assuming $f_i=f(x,y)$ and we have $\lim_{t\to 0}\frac{f_i(x+t,y)-f_i(x)}{t}=\lim_{t\to 0}\frac{\frac{(x+t)(y)}{(x+t)^2+y^2}-\frac{xy}{x^2+y^2}}{t}$ , I've played a lot with this fraction over here but I cant get anywhere, and everything online is evaluating this function at origin. I'm not sure what I am missing here. If anyone can help me understand this? Thanks a lot!","I was just reading the definition of Rudin on partial derivative given , and be basis for the spaces, we have . Where Now let and , if I want to use the definition to evaluate partial of at , I have , what exactly is the component function here? Since I'm assuming and we have , I've played a lot with this fraction over here but I cant get anywhere, and everything online is evaluating this function at origin. I'm not sure what I am missing here. If anyone can help me understand this? Thanks a lot!","f:\mathbb{R}^n \to \mathbb{R}^m e_1,\dots,e_n u_1,\dots ,u_m (D_jf_i)(x)=\lim_{t\to 0}\frac{f_i(x+te_j)-f_i(x)}{t} f_i(x)=f(x)\cdot u_i f(x,y)=\frac{xy}{x^2+y^2} f(0,0)=0 x (x,y)\neq 0 D_1f_i(x,y)=\lim_{t\to 0}\frac{f_i(x+t,y)-f_i(x)}{t} f_i m=1 f_i=f(x,y) \lim_{t\to 0}\frac{f_i(x+t,y)-f_i(x)}{t}=\lim_{t\to 0}\frac{\frac{(x+t)(y)}{(x+t)^2+y^2}-\frac{xy}{x^2+y^2}}{t}","['calculus', 'analysis']"
69,How to prove that $\int_{0}^{1} (f'(x))^2 dx \geq \frac{2}{e^2}$,How to prove that,\int_{0}^{1} (f'(x))^2 dx \geq \frac{2}{e^2},"Here is the question I am trying to solve: Suppose $f$ is a real, continuously differentiable function on $[0,1], f(0) = f(1) = 0,$ and $$\int_{0}^{1} f(x)e^x dx = 1.$$ Prove that $(a) \int_{0}^{1} f'(x) e^x dx = -1,$ and $(b) \int_{0}^{1} (f'(x))^2 dx \geq \frac{2}{e^2}.$ My question is: I know how to prove $(a)$ where I used integration by parts but I do not know how to solve $(b),$ should I use the mean value theorem or intermediate value theorem? Can anyone help me in solving letter $(b)$ please?","Here is the question I am trying to solve: Suppose is a real, continuously differentiable function on and Prove that and My question is: I know how to prove where I used integration by parts but I do not know how to solve should I use the mean value theorem or intermediate value theorem? Can anyone help me in solving letter please?","f [0,1], f(0) = f(1) = 0, \int_{0}^{1} f(x)e^x dx = 1. (a) \int_{0}^{1} f'(x) e^x dx = -1, (b) \int_{0}^{1} (f'(x))^2 dx \geq \frac{2}{e^2}. (a) (b), (b)","['real-analysis', 'calculus', 'integration', 'analysis', 'mean-value-theorem']"
70,Continuity of parametric integral $I(\alpha)=\int_0^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx$,Continuity of parametric integral,I(\alpha)=\int_0^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx,"How to prove that parametric integral $I(\alpha)=\int_0^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx$ is differentiable on $(1,\infty)$ ? I wrote $I(\alpha)$ as $\int_0^1 \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx+\int_1^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx=I_1(\alpha)+I_2(\alpha)$ , but I'm having difficulties to show that $I_2(\alpha)$ is differentiable on $(1,\infty)$ . Since $f(x,\alpha)= \begin{cases} \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}, 0<x<1\\ \alpha^2, x=1 \end{cases}$ and $f'(x,\alpha)$ are continuous on $[0,1)\times (1,\infty)$ , $I_1(\alpha)$ is continuous on $(1,\infty)$ , am I right? Any help is welcome. Thanks in advance.","How to prove that parametric integral is differentiable on ? I wrote as , but I'm having difficulties to show that is differentiable on . Since and are continuous on , is continuous on , am I right? Any help is welcome. Thanks in advance.","I(\alpha)=\int_0^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx (1,\infty) I(\alpha) \int_0^1 \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx+\int_1^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx=I_1(\alpha)+I_2(\alpha) I_2(\alpha) (1,\infty) f(x,\alpha)=
\begin{cases}
\frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}, 0<x<1\\
\alpha^2, x=1
\end{cases} f'(x,\alpha) [0,1)\times (1,\infty) I_1(\alpha) (1,\infty)","['integration', 'analysis', 'convergence-divergence', 'parametric']"
71,Convergence in operator norm equivalent to uniform convergence on bounded sets,Convergence in operator norm equivalent to uniform convergence on bounded sets,,"Let $E$ be a Banach space and let $\{T_n\}_{n=1}^{\infty}$ be a sequence of bounded linear operators on $E$ . I am trying to prove the following claim: Claim: $T_n \rightarrow T$ in operator norm (i.e. $\lim\limits_{n \to \infty} \|T_n - T\| = 0)$ if and only if $T_n \rightarrow T$ uniformly on every bounded subset of $E$ (i.e. $\lim\limits_{n \to \infty} \sup\limits_{x \in S} \|T_n(x) - T(x) \| = 0$ for every bounded subset $S \subset E$ ). I am having a little trouble with the ""only if"" part of the proof. Here is what I have so far: Proof attempt : ( $\implies$ ) Suppose that $T_n \rightarrow T$ in the operator norm. Let $S \subset E$ be a bounded set and let $x_0 \in E$ . Pick $r > 0$ large enough so that $S$ is contained in the closed ball $B_r(x_0) := \{x \in E: \|x-x_0\| \leq r \}$ . Since the map $x \mapsto \|x\|$ is continuous and $B_r(x_0)$ is compact, there exists an $M > 0$ such that $\|x\| \leq M$ for all $x \in B_{r}(x_0)$ . We then have $$ \|T_n(x) - T(x)\| \leq \|T_n - T \| \|x\| \leq \|T_n - T\| M \quad \forall x \in B_{r}(x_0). $$ Thus, $$\sup_{x \in S} \|T_n(x) - T(x)\| \leq \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| \leq \|T_n - T\| M.  $$ And since $\|T_n - T\| M \rightarrow 0$ as $n \to \infty$ , it follows that $\lim\limits_{n \to \infty} \sup\limits_{x \in S} \|T_n(x) - T(x)\| \to 0$ . So far so good (I think). Now here's the direction where I'm not so sure: $(\impliedby)$ Suppose $T_n \rightarrow T$ on every bounded subset of $E$ . Pick some $x_0 \in E$ . Then for each $r \in \mathbb{N}$ , we have $$ \lim_{n \to \infty} \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = 0. $$ Then taking the limit in $r$ , we have $$ \lim_{r \to \infty} \lim_{n \to \infty} \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = 0. \hspace{1cm} (*)$$ Now the following would suffice to give the result: The limits in $(*)$ can be interchanged. $\lim\limits\limits_{r \to \infty} \sup\limits_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = \sup\limits_{x \in E} \|T_n(x) - T(x)\|$ My main questions: Are 1) and 2) true? For 2) I am somewhat skeptical and am not sure how to rigorously justify interchanging the limits. For 2) I haven't quite proved it yet, but I am pretty confident it is true. (Or am I wrong?) Any insights would be appreciated.","Let be a Banach space and let be a sequence of bounded linear operators on . I am trying to prove the following claim: Claim: in operator norm (i.e. if and only if uniformly on every bounded subset of (i.e. for every bounded subset ). I am having a little trouble with the ""only if"" part of the proof. Here is what I have so far: Proof attempt : ( ) Suppose that in the operator norm. Let be a bounded set and let . Pick large enough so that is contained in the closed ball . Since the map is continuous and is compact, there exists an such that for all . We then have Thus, And since as , it follows that . So far so good (I think). Now here's the direction where I'm not so sure: Suppose on every bounded subset of . Pick some . Then for each , we have Then taking the limit in , we have Now the following would suffice to give the result: The limits in can be interchanged. My main questions: Are 1) and 2) true? For 2) I am somewhat skeptical and am not sure how to rigorously justify interchanging the limits. For 2) I haven't quite proved it yet, but I am pretty confident it is true. (Or am I wrong?) Any insights would be appreciated.",E \{T_n\}_{n=1}^{\infty} E T_n \rightarrow T \lim\limits_{n \to \infty} \|T_n - T\| = 0) T_n \rightarrow T E \lim\limits_{n \to \infty} \sup\limits_{x \in S} \|T_n(x) - T(x) \| = 0 S \subset E \implies T_n \rightarrow T S \subset E x_0 \in E r > 0 S B_r(x_0) := \{x \in E: \|x-x_0\| \leq r \} x \mapsto \|x\| B_r(x_0) M > 0 \|x\| \leq M x \in B_{r}(x_0)  \|T_n(x) - T(x)\| \leq \|T_n - T \| \|x\| \leq \|T_n - T\| M \quad \forall x \in B_{r}(x_0).  \sup_{x \in S} \|T_n(x) - T(x)\| \leq \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| \leq \|T_n - T\| M.   \|T_n - T\| M \rightarrow 0 n \to \infty \lim\limits_{n \to \infty} \sup\limits_{x \in S} \|T_n(x) - T(x)\| \to 0 (\impliedby) T_n \rightarrow T E x_0 \in E r \in \mathbb{N}  \lim_{n \to \infty} \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = 0.  r  \lim_{r \to \infty} \lim_{n \to \infty} \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = 0. \hspace{1cm} (*) (*) \lim\limits\limits_{r \to \infty} \sup\limits_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = \sup\limits_{x \in E} \|T_n(x) - T(x)\|,"['linear-algebra', 'functional-analysis', 'analysis', 'operator-theory', 'uniform-convergence']"
72,"If $T: E \rightarrow E$ is a bounded linear operator, then $e^T$ is as well","If  is a bounded linear operator, then  is as well",T: E \rightarrow E e^T,"On page 39 of Differential Dynamical Systems (revised edition) by Meiss, the author gives the following Lemma. Lemma 2.8 : If $T$ is a bounded linear operator, then $e^T$ is as well. We are assuming (I think) that $T: E \rightarrow E$ , where $E$ is a normed vector space over $\mathbb{R}$ . I have a couple doubts about the proof of the lemma, which goes as follows: Proof : Choose an arbitrary $x \in E$ and consider the value of $e^T(x)$ . By definition (2.25) [ $e^T \equiv \sum_{k=0}^{\infty} \frac{T^k}{k!}$ ], this is a series whose terms are elements of $E$ . The norm of this series is bounded by the sum of the norms of each term. By the definition of the operator norm, for any $x$ , $$ |T(x)| \leq \|T\| |x|, $$ $$ |T^k(x)|  = |T(T^{k-1}(x))| \leq \|T\| |T^{k-1}(x)| \leq \cdots \leq \|T\|^k |x|. $$ Consequently, each of the terms in the series $e^T(x)$ can be bounded by $$ \left|\frac{T^k(x)}{k!} \right| \leq \frac{\|T\|^k}{k!} |x| = M_k. $$ The series of real numbers $$ \sum_{k=0}^{\infty} M_k = \sum_{k=0}^{\infty} \frac{\|T\|^k}{k!}|x| = e^{\|T\|}|x| $$ converges for any finite value of $\|T\|$ . By the Weierstrass M-test $|e^T(x)| \leq e^{\|T\|}|x|$ and the series for $e^T(x)$ converges uniformly in $x$ . Moreover, $\|e^T\| \leq e^{\|T\|}$ , so the exponential is a bounded operator. $\qquad \square$ My questions: How are we allowed to use the Weierstrass M-test here? Don't we require that the norm of each term in the series is bounded uniformly in $x$ , i.e. $\left|\frac{T^k(x)}{k!}\right| \leq M_k$ for all $x \in E$ ?  (I am going by the version of the Weierstrass M-Test that I stated in this previous post . Don't we also need to assume that $E$ is a Banach-space? (This was an assumption I used in the proof I linked.)","On page 39 of Differential Dynamical Systems (revised edition) by Meiss, the author gives the following Lemma. Lemma 2.8 : If is a bounded linear operator, then is as well. We are assuming (I think) that , where is a normed vector space over . I have a couple doubts about the proof of the lemma, which goes as follows: Proof : Choose an arbitrary and consider the value of . By definition (2.25) [ ], this is a series whose terms are elements of . The norm of this series is bounded by the sum of the norms of each term. By the definition of the operator norm, for any , Consequently, each of the terms in the series can be bounded by The series of real numbers converges for any finite value of . By the Weierstrass M-test and the series for converges uniformly in . Moreover, , so the exponential is a bounded operator. My questions: How are we allowed to use the Weierstrass M-test here? Don't we require that the norm of each term in the series is bounded uniformly in , i.e. for all ?  (I am going by the version of the Weierstrass M-Test that I stated in this previous post . Don't we also need to assume that is a Banach-space? (This was an assumption I used in the proof I linked.)","T e^T T: E \rightarrow E E \mathbb{R} x \in E e^T(x) e^T \equiv \sum_{k=0}^{\infty} \frac{T^k}{k!} E x  |T(x)| \leq \|T\| |x|,   |T^k(x)|  = |T(T^{k-1}(x))| \leq \|T\| |T^{k-1}(x)| \leq \cdots \leq \|T\|^k |x|.  e^T(x)  \left|\frac{T^k(x)}{k!} \right| \leq \frac{\|T\|^k}{k!} |x| = M_k.   \sum_{k=0}^{\infty} M_k = \sum_{k=0}^{\infty} \frac{\|T\|^k}{k!}|x| = e^{\|T\|}|x|  \|T\| |e^T(x)| \leq e^{\|T\|}|x| e^T(x) x \|e^T\| \leq e^{\|T\|} \qquad \square x \left|\frac{T^k(x)}{k!}\right| \leq M_k x \in E E","['linear-algebra', 'analysis', 'vector-spaces', 'operator-theory', 'uniform-convergence']"
73,"This metrics $d_1(s, t)=\left | {s\over 1 +|s|}- {t\over 1 +|t|}\right| $ and $d_2(s,t)=|s-t|$ ar e equivalents? [duplicate]",This metrics  and  ar e equivalents? [duplicate],"d_1(s, t)=\left | {s\over 1 +|s|}- {t\over 1 +|t|}\right|  d_2(s,t)=|s-t|","This question already has answers here : Prove that $d_{1}$ and $d_{2}$ induce the same topology (2 answers) Closed 1 year ago . In  several lists of exercises they affirm that these norms are equivalent: show that this metrics $d_1(s, t)= \left| {s\over 1 +|s|}- {t\over 1 +|t|}\right| $ and $d_2(s,t)=|s-t|$ , with $t,s \in \mathbb{R}$ , are equivalents. I tried to show that equivalence of metrics using this definition: for each $ s \in X$ , there exist positive constants $\alpha$ and $\beta$ such that, for every point $ t \in X $ , $$\alpha d_{2}(s, t) \leq d_{1}(s, t) \leq \beta d_{2}(s, t)$$ . Taking $X=\mathbb{R}$ . I choose for example, $s=4$ , if that equivalence of norms was truth must exists $\alpha$ and $\beta$ such that, for every $t \in \mathbb{R}$ . $$\alpha |4-t|  \leq \left| {4\over 5}- {t\over 1 +|t|}\right| \leq \beta |4-t|$$ . also if $t\geqslant 0$ , then $$\alpha |4-t|  \leq \left| {4\over 5}- {t\over 1 +t}\right| \leq \beta |4-t|$$ . $$\alpha |4-t|  \leq  {|4-t|\over 5(1 +t)} \leq \beta |4-t|$$ $$5\alpha   \leq  {1\over (1 +t)} \leq 5 \beta $$ In this part, I can't find an $\alpha>0$ that satisfies, for all $t$ very big in $\mathbb{R}$ . Can I conclude that these metrics aren't equivalents. or how can I show that they are equivalents?","This question already has answers here : Prove that $d_{1}$ and $d_{2}$ induce the same topology (2 answers) Closed 1 year ago . In  several lists of exercises they affirm that these norms are equivalent: show that this metrics and , with , are equivalents. I tried to show that equivalence of metrics using this definition: for each , there exist positive constants and such that, for every point , . Taking . I choose for example, , if that equivalence of norms was truth must exists and such that, for every . . also if , then . In this part, I can't find an that satisfies, for all very big in . Can I conclude that these metrics aren't equivalents. or how can I show that they are equivalents?","d_1(s, t)= \left| {s\over 1 +|s|}- {t\over 1 +|t|}\right|  d_2(s,t)=|s-t| t,s \in \mathbb{R}  s \in X \alpha \beta  t \in X  \alpha d_{2}(s, t) \leq d_{1}(s, t) \leq \beta d_{2}(s, t) X=\mathbb{R} s=4 \alpha \beta t \in \mathbb{R} \alpha |4-t|  \leq \left| {4\over 5}- {t\over 1 +|t|}\right| \leq \beta |4-t| t\geqslant 0 \alpha |4-t|  \leq \left| {4\over 5}- {t\over 1 +t}\right| \leq \beta |4-t| \alpha |4-t|  \leq  {|4-t|\over 5(1 +t)} \leq \beta |4-t| 5\alpha   \leq  {1\over (1 +t)} \leq 5 \beta  \alpha>0 t \mathbb{R}","['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'equivalent-metrics']"
74,Proving the existence of KKT points,Proving the existence of KKT points,,"Let a function $f:\mathbb{R}^d \rightarrow \mathbb{R}$ an index set $I$ , and inequality constraints $g_i(x)\leq 0, ~\forall i \in I $ . How do we prove the existence of KKT points for $f$ subject to $g_i, ~i \in I$ without using e.g the existence of a minimum of $f$ (through Weierstrass for example) in conjunction with a constraint qualification? I have a feeling that it could be a fixed point theorem. PS: In an optimization problem, we implicitly prove that the set of KKT points is nonempty through evoking some constraint qualification for feasible local optima of the problem. i.e. , Since local optima must be KKT points then the set of KKT points is nonempty. Can we use a straightforward argument for the existence of KKT points irregardless of local optima and constraint qualifications?","Let a function an index set , and inequality constraints . How do we prove the existence of KKT points for subject to without using e.g the existence of a minimum of (through Weierstrass for example) in conjunction with a constraint qualification? I have a feeling that it could be a fixed point theorem. PS: In an optimization problem, we implicitly prove that the set of KKT points is nonempty through evoking some constraint qualification for feasible local optima of the problem. i.e. , Since local optima must be KKT points then the set of KKT points is nonempty. Can we use a straightforward argument for the existence of KKT points irregardless of local optima and constraint qualifications?","f:\mathbb{R}^d \rightarrow \mathbb{R} I g_i(x)\leq 0, ~\forall i \in I  f g_i, ~i \in I f","['analysis', 'optimization', 'lagrange-multiplier', 'karush-kuhn-tucker']"
75,Asymptotics applied to approximate solutions of trascendental equations,Asymptotics applied to approximate solutions of trascendental equations,,"I'm self studying ""Olver - Asymptotics and Special Functions"": I'm new to asymptotics and so there are some things I am not getting. First, the author states the following theorem: Theorem 3.1. Let $\sum_{s=0}^\infty a_s z^s$ converge when $|z|<r$ . Then for fixed $n$ it is $\sum_{s=n}^\infty a_s z^s=O(z^n)$ in any disk $|z|\le\rho$ such that $\rho <r$ . And proceeds to study the asymptotic behavior of the roots of trascendental equations: for instance, he considers the equation $x \tan x=1$ for large $x>0$ . Inverting, it is $x=n\pi+\arctan(1/x)$ for $n\in\mathbb{Z}$ for the principal value of arctan. Since arctan is bounded, it is $x \sim n\pi$ as $n \to \infty$ . Since for $x>1$ the Taylor's series of $\arctan(1/x)$ converges, he then writes: $$x=n\pi+\frac{1}{x}-\frac{1}{3x^3}+\frac{1}{5x^5}-\frac{1}{7x^7}+\dots$$ Finally, he claims that hence $x=n\pi+O(x^{-1})=n\pi+O(n^{-1})$ and that next two substitutions produce $$x=n\pi+\frac{1}{n\pi}+O\left(\frac{1}{n^3}\right) \tag{1}$$ $$x=n\pi+\frac{1}{n\pi}-\frac{4}{3(n\pi)^3}+O\left(\frac{1}{n^5}\right) \tag{2}$$ First question: why is it $n\pi+O(x^{-1})=n\pi+O(n^{-1})$ ? I know that $f(x)=O(g(x))$ as $x \to x_0$ means that $|f(x)|\le K|g(x)|$ for some constant $K$ in a neighborhood of $x_0$ , maybe this comes from the Archimedean property and I can always find a natural $n$ such that $n<x$ and so $\frac{1}{x}<\frac{1}{n}$ , so I can say (for $x>0$ ) that $f(x)=O(x^{-1})\implies f(x)\le \frac{K}{x}<\frac{K}{n}\implies f(x) \le \frac{K}{n} \implies f(x)=O(n^{-1})$ ? Or this is wrong and he is using another kind of reasoning? Second question: I am not getting the calculations to obtain equations $(1)$ and $(2)$ . What I tried so far is the following. From theorem 3.1 with $n=1$ , I know that $$x=n\pi+\frac{1}{x}+O\left(\frac{1}{x^3}\right)$$ Substituting $x=n\pi+O(n^{-1})$ and using the properties $O(cf)=O(f)$ for $c>0$ , $fO(g)=O(fg)$ and $O(f)O(g)=O(fg)$ , it is $$x=n\pi+\frac{1}{n\pi+O\left(\frac{1}{n\pi}\right)}+O\left(\frac{1}{\left(n\pi+O\left(\frac{1}{n\pi}\right)\right)^3}\right)=n\pi+\frac{1}{n\pi+O\left(\frac{1}{n\pi}\right)}+O\left(\frac{1}{n^3\pi^3+O(n)+O\left(\frac{1}{n}\right)+O\left(\frac{1}{n^3}\right)}\right)$$ But I don't know how to manipulate the big-Os in the denominators. I have similar problems for $(2)$ , putting $n=2$ in theorem 3.1 and have no clue how to manipulate the big-Os.","I'm self studying ""Olver - Asymptotics and Special Functions"": I'm new to asymptotics and so there are some things I am not getting. First, the author states the following theorem: Theorem 3.1. Let converge when . Then for fixed it is in any disk such that . And proceeds to study the asymptotic behavior of the roots of trascendental equations: for instance, he considers the equation for large . Inverting, it is for for the principal value of arctan. Since arctan is bounded, it is as . Since for the Taylor's series of converges, he then writes: Finally, he claims that hence and that next two substitutions produce First question: why is it ? I know that as means that for some constant in a neighborhood of , maybe this comes from the Archimedean property and I can always find a natural such that and so , so I can say (for ) that ? Or this is wrong and he is using another kind of reasoning? Second question: I am not getting the calculations to obtain equations and . What I tried so far is the following. From theorem 3.1 with , I know that Substituting and using the properties for , and , it is But I don't know how to manipulate the big-Os in the denominators. I have similar problems for , putting in theorem 3.1 and have no clue how to manipulate the big-Os.",\sum_{s=0}^\infty a_s z^s |z|<r n \sum_{s=n}^\infty a_s z^s=O(z^n) |z|\le\rho \rho <r x \tan x=1 x>0 x=n\pi+\arctan(1/x) n\in\mathbb{Z} x \sim n\pi n \to \infty x>1 \arctan(1/x) x=n\pi+\frac{1}{x}-\frac{1}{3x^3}+\frac{1}{5x^5}-\frac{1}{7x^7}+\dots x=n\pi+O(x^{-1})=n\pi+O(n^{-1}) x=n\pi+\frac{1}{n\pi}+O\left(\frac{1}{n^3}\right) \tag{1} x=n\pi+\frac{1}{n\pi}-\frac{4}{3(n\pi)^3}+O\left(\frac{1}{n^5}\right) \tag{2} n\pi+O(x^{-1})=n\pi+O(n^{-1}) f(x)=O(g(x)) x \to x_0 |f(x)|\le K|g(x)| K x_0 n n<x \frac{1}{x}<\frac{1}{n} x>0 f(x)=O(x^{-1})\implies f(x)\le \frac{K}{x}<\frac{K}{n}\implies f(x) \le \frac{K}{n} \implies f(x)=O(n^{-1}) (1) (2) n=1 x=n\pi+\frac{1}{x}+O\left(\frac{1}{x^3}\right) x=n\pi+O(n^{-1}) O(cf)=O(f) c>0 fO(g)=O(fg) O(f)O(g)=O(fg) x=n\pi+\frac{1}{n\pi+O\left(\frac{1}{n\pi}\right)}+O\left(\frac{1}{\left(n\pi+O\left(\frac{1}{n\pi}\right)\right)^3}\right)=n\pi+\frac{1}{n\pi+O\left(\frac{1}{n\pi}\right)}+O\left(\frac{1}{n^3\pi^3+O(n)+O\left(\frac{1}{n}\right)+O\left(\frac{1}{n^3}\right)}\right) (2) n=2,"['analysis', 'asymptotics']"
76,Calculating the center of mass of a lemniscate rotated around the x-axis?,Calculating the center of mass of a lemniscate rotated around the x-axis?,,"This is a problem I have been stuck on a while, it goes as follows: A lemniscate has the equation $(x^2+y^2)^2 = 4(x^2-y^2)$ . Let the part of the curve that lies in the first quadrant rotate around the $x$ -axis to create an object. This object has an homogeneous massdistribution with the density $1$ . Determine the coordinates of the center of mass $(x_T,y_T,z_T)$ , if $x_T = \frac{1}{M}\int _K x dm$ , where $M$ is the mass of the object. I have so far managed to switch to polar coordinates and found the relation $r^2 = 4\cos{2\theta}$ and tried to calculate the volume (which equals the mass in this case due to the density being equal to 1) with the formula $\int _K \pi y^2 dx$ . After the switch to polar coordinates, a point on the curve is $(x,y) = (r\cos{\theta},r\sin{\theta})$ , which gives $\pi y^2 = \pi r^2\sin^2{\theta} = 4\pi\cos{2\theta}\sin^2{\theta}$ . I then reason that a small change $dx$ in polar coordinates, corresponds to a small change in $\theta$ , hence I substitute $dx$ with $\frac{dx}{d\theta} 2\sqrt{\cos{2\theta}}\cos{\theta} \Leftrightarrow dx = -2(\frac{\sin{2\theta}\cos{\theta}}{\sqrt{\cos{2\theta}}}+ \sin{\theta}\sqrt{\cos{2\theta}}) d\theta$ . But when I try and calculate the integral $\int^2_0 4\pi\cos{2\theta}\sin^2{\theta} \cdot (-2)(\frac{\sin{2\theta}\cos{\theta}}{\sqrt{\cos{2\theta}}}+ \sin{\theta}\sqrt{\cos{2\theta}}) d\theta $ , I get an incorrect volume/mass. The correct solution is $(x_T,y_T,z_T) = (\frac{2}{3\sqrt{2}\ln{(\sqrt{2}+1)}-2},0,0)$ I would greatly appreciate any tips on how to proceed, thanks in advance!","This is a problem I have been stuck on a while, it goes as follows: A lemniscate has the equation . Let the part of the curve that lies in the first quadrant rotate around the -axis to create an object. This object has an homogeneous massdistribution with the density . Determine the coordinates of the center of mass , if , where is the mass of the object. I have so far managed to switch to polar coordinates and found the relation and tried to calculate the volume (which equals the mass in this case due to the density being equal to 1) with the formula . After the switch to polar coordinates, a point on the curve is , which gives . I then reason that a small change in polar coordinates, corresponds to a small change in , hence I substitute with . But when I try and calculate the integral , I get an incorrect volume/mass. The correct solution is I would greatly appreciate any tips on how to proceed, thanks in advance!","(x^2+y^2)^2 = 4(x^2-y^2) x 1 (x_T,y_T,z_T) x_T = \frac{1}{M}\int _K x dm M r^2 = 4\cos{2\theta} \int _K \pi y^2 dx (x,y) = (r\cos{\theta},r\sin{\theta}) \pi y^2 = \pi r^2\sin^2{\theta} = 4\pi\cos{2\theta}\sin^2{\theta} dx \theta dx \frac{dx}{d\theta} 2\sqrt{\cos{2\theta}}\cos{\theta} \Leftrightarrow dx = -2(\frac{\sin{2\theta}\cos{\theta}}{\sqrt{\cos{2\theta}}}+ \sin{\theta}\sqrt{\cos{2\theta}}) d\theta \int^2_0 4\pi\cos{2\theta}\sin^2{\theta} \cdot (-2)(\frac{\sin{2\theta}\cos{\theta}}{\sqrt{\cos{2\theta}}}+ \sin{\theta}\sqrt{\cos{2\theta}}) d\theta  (x_T,y_T,z_T) = (\frac{2}{3\sqrt{2}\ln{(\sqrt{2}+1)}-2},0,0)","['analysis', 'definite-integrals', 'volume']"
77,"Show that $\beta(f,g)=\int_0^1 f(x)g'(x)dx$ is nondegenerate.",Show that  is nondegenerate.,"\beta(f,g)=\int_0^1 f(x)g'(x)dx","We have the bilinear form $\beta:V \times V \to \mathbb{R}, (f,g) \mapsto \int_0^1 f(x)g'(x)dx$ with $V=\{f\in C^1[0,1] \mid f(0)=f(1)=0 \}$ . Show that $\beta$ is nondegenerate. My attempt: For $0\neq f \in V$ we have $\beta(f,F)=\int_0^1 f(x)^2dx > 0$ where $F(x)=\int_0^x f(t)dt$ . But $F(1)$ is not always $0$ . So i tried something like $G(x)=F(x)-F(1)x$ or $G(x)=F(x)(1-x)$ but i am not able to show $\beta(f,G)\neq 0$ .",We have the bilinear form with . Show that is nondegenerate. My attempt: For we have where . But is not always . So i tried something like or but i am not able to show .,"\beta:V \times V \to \mathbb{R}, (f,g) \mapsto \int_0^1 f(x)g'(x)dx V=\{f\in C^1[0,1] \mid f(0)=f(1)=0 \} \beta 0\neq f \in V \beta(f,F)=\int_0^1 f(x)^2dx > 0 F(x)=\int_0^x f(t)dt F(1) 0 G(x)=F(x)-F(1)x G(x)=F(x)(1-x) \beta(f,G)\neq 0","['integration', 'analysis', 'derivatives', 'bilinear-form']"
78,Equivalent condition for Cauchy Sequences,Equivalent condition for Cauchy Sequences,,"I am trying to prove the following : Let $E$ be a topologic vector space. $(x_n)\subset E$ is a Cauchy sequence in $E \ $ iff $\ \lim_{k\to\infty}(x_{m_k}-x_{n_k})=0$ for any $n_k, m_k$ pair of strictly increasing sequences of $\Bbb N$ . I know that $x_n$ is a Cauchy sequence in $E \ $ means $\forall V\in\mathscr V_0 \ \ \ \exists n_0\in \Bbb N \ \ \textrm{such that} \ \ \forall n,m\geq n_0 \ \ x_n-x_m\in V$ where $\mathscr V_0 $ is collection of nbds of zero. I am sorry for this easy question but I even don't know how should I start because it seems as if there is nothing to prove.  Thanks in advance for any help for any direction of this proposition. I appreciate any help.",I am trying to prove the following : Let be a topologic vector space. is a Cauchy sequence in iff for any pair of strictly increasing sequences of . I know that is a Cauchy sequence in means where is collection of nbds of zero. I am sorry for this easy question but I even don't know how should I start because it seems as if there is nothing to prove.  Thanks in advance for any help for any direction of this proposition. I appreciate any help.,"E (x_n)\subset E E \  \ \lim_{k\to\infty}(x_{m_k}-x_{n_k})=0 n_k, m_k \Bbb N x_n E \  \forall V\in\mathscr V_0 \ \ \ \exists n_0\in \Bbb N \ \ \textrm{such that} \ \ \forall n,m\geq n_0 \ \ x_n-x_m\in V \mathscr V_0 ","['general-topology', 'analysis', 'cauchy-sequences']"
79,"Show that $C^\infty(0,T; L^2(\Omega(t)))$ is dense in $L^2(0,T; L^2(\Omega(t)))$.",Show that  is dense in .,"C^\infty(0,T; L^2(\Omega(t))) L^2(0,T; L^2(\Omega(t)))","Let $T > 0$ , $\Omega \subset \mathbb R^2$ and $f:[0,T] \to \mathbb R^2$ a continuous and bounded function. We define $$\Omega(t) = \Omega + f(t),$$ and $\widetilde \Omega \subset \mathbb R^2$ such that $\Omega(t)\subset \widetilde \Omega$ for all $t$ . Moreover, we set \begin{align} L^2(0,T; L^2(\Omega(t))) &= \{v \in L^2(0,T; L^2(\widetilde \Omega ))~|~ v(t, \cdot)|_{\widetilde \Omega \backslash\Omega(t)} = 0 ~\text{ a.e. in }[0,T] \},\\ C^\infty(0,T; L^2(\Omega(t))) &= \{v \in C^\infty(0,T; L^2(\widetilde \Omega ))~|~v(t, \cdot)|_{\widetilde \Omega \backslash\Omega(t)} = 0~~\forall t \in [0,T] \}. \end{align} It is not too hard to show that $ C^\infty(0,T; L^2(\widetilde \Omega ))$ is dense in $L^2(0,T; L^2(\widetilde \Omega ))$ . Now, I would like to know if $C^\infty(0,T; L^2(\Omega(t)))$ is dense in $L^2(0,T; L^2(\Omega(t)))$ . Here is my attempt: Let $v \in L^2(0,T; L^2(\Omega(t)))$ and $(v_n)_n \subset C^\infty(0,T; L^2(\widetilde \Omega ))$ such that $v_n \to v$ in $L^2(0,T; L^2(\widetilde \Omega ))$ . This means that $$\int_0^T\int_{\widetilde \Omega} |v - v_n|^2dy dt \to 0 \quad \text{for }n \to \infty.$$ This implies that $\|v(t, \cdot) - v_n(t, \cdot)\|_{L^2(\widetilde \Omega)} \to 0$ for every $t \in [0,T] \backslash E$ , where $E$ has a zero measure. Therefore, there exists $N(t) \ge 0$ such that $\forall n \ge N(t)$ , $\text{supp } v_n(t, \cdot) \subset \Omega(t)$ . Now for $t \in E$ , we can find a sequence $t_k \subset [0,T] \backslash E$ such that $t_k \to t$ and by continuity of the function $f$ , for $k$ and $n$ sufficiently big, we have $\text{supp } v_n(t_k, \cdot) \subset \Omega(t) \cap \Omega(t_k)$ and as $t \mapsto v_n(t, \cdot)$ is continuous, $\text{supp } v_n(t, \cdot) \subset \Omega(t)$ . Now I would like to discard a finite number of $v_n$ 's in my sequence so that $\text{supp } v(t, \cdot) \subset \Omega(t)$ for every $t \in [0,T]$ . My problem is that $N(t)$ depends on $t$ , such that for every $t$ I can discard a finite number for $n$ 's in my sequence but how can I find a $N$ that does not depend on $t$ ? These $N(t)$ 's could go to infinity for $t$ going to a certain value... Is it the right way to do that ? How can I conclude ? EDIT: Actually the above argument is wrong. Indeed, when I deduce that $\forall n \ge N(t)$ , $\text{supp } v_n(t, \cdot) \subset \Omega(t)$ , it is as if I assumed that the support of $v(t, \cdot)$ was strictly include in $\Omega(t)$ while it is not true, we only have that the trace of $v$ is zero, but the support may not be strictly included. I should find a way to approach $v$ from inside $\Omega(t)$ . I really have no idea how to do that..","Let , and a continuous and bounded function. We define and such that for all . Moreover, we set It is not too hard to show that is dense in . Now, I would like to know if is dense in . Here is my attempt: Let and such that in . This means that This implies that for every , where has a zero measure. Therefore, there exists such that , . Now for , we can find a sequence such that and by continuity of the function , for and sufficiently big, we have and as is continuous, . Now I would like to discard a finite number of 's in my sequence so that for every . My problem is that depends on , such that for every I can discard a finite number for 's in my sequence but how can I find a that does not depend on ? These 's could go to infinity for going to a certain value... Is it the right way to do that ? How can I conclude ? EDIT: Actually the above argument is wrong. Indeed, when I deduce that , , it is as if I assumed that the support of was strictly include in while it is not true, we only have that the trace of is zero, but the support may not be strictly included. I should find a way to approach from inside . I really have no idea how to do that..","T > 0 \Omega \subset \mathbb R^2 f:[0,T] \to \mathbb R^2 \Omega(t) = \Omega + f(t), \widetilde \Omega \subset \mathbb R^2 \Omega(t)\subset \widetilde \Omega t \begin{align}
L^2(0,T; L^2(\Omega(t))) &= \{v \in L^2(0,T; L^2(\widetilde \Omega ))~|~ v(t, \cdot)|_{\widetilde \Omega \backslash\Omega(t)} = 0 ~\text{ a.e. in }[0,T] \},\\
C^\infty(0,T; L^2(\Omega(t))) &= \{v \in C^\infty(0,T; L^2(\widetilde \Omega ))~|~v(t, \cdot)|_{\widetilde \Omega \backslash\Omega(t)} = 0~~\forall t \in [0,T] \}.
\end{align}  C^\infty(0,T; L^2(\widetilde \Omega )) L^2(0,T; L^2(\widetilde \Omega )) C^\infty(0,T; L^2(\Omega(t))) L^2(0,T; L^2(\Omega(t))) v \in L^2(0,T; L^2(\Omega(t))) (v_n)_n \subset C^\infty(0,T; L^2(\widetilde \Omega )) v_n \to v L^2(0,T; L^2(\widetilde \Omega )) \int_0^T\int_{\widetilde \Omega} |v - v_n|^2dy dt \to 0 \quad \text{for }n \to \infty. \|v(t, \cdot) - v_n(t, \cdot)\|_{L^2(\widetilde \Omega)} \to 0 t \in [0,T] \backslash E E N(t) \ge 0 \forall n \ge N(t) \text{supp } v_n(t, \cdot) \subset \Omega(t) t \in E t_k \subset [0,T] \backslash E t_k \to t f k n \text{supp } v_n(t_k, \cdot) \subset \Omega(t) \cap \Omega(t_k) t \mapsto v_n(t, \cdot) \text{supp } v_n(t, \cdot) \subset \Omega(t) v_n \text{supp } v(t, \cdot) \subset \Omega(t) t \in [0,T] N(t) t t n N t N(t) t \forall n \ge N(t) \text{supp } v_n(t, \cdot) \subset \Omega(t) v(t, \cdot) \Omega(t) v v \Omega(t)","['functional-analysis', 'analysis', 'lp-spaces']"
80,A sort of converse of Banach-Steinhaus theorem.,A sort of converse of Banach-Steinhaus theorem.,,"$(X, \|•\|) $ and $(Y, \|•\|') $ be two normed space. $\begin{align} {\scr{B}}{(X, Y) }&=\{T\in {\scr{L}}{(X,Y)}: T \text{ is bounded } \}\end{align}$ $\|T\|_{op}=\sup\{\|Tx\|':\|x\|\le 1 \}$ Question: $\forall (T_n) \subset {\scr{B}}{(X, Y)}$ be such that $T_n\to T $ pointwise $[$ i.e $\forall x\in x,$$ T_nx\to Tx $ in the space $(Y, \|•\|') ]$ implies $T\in {\scr{B}}{(X, Y)}$ . Does this implies $(X, \|•\|) $ is a Banach space? The converse is well known ( Banach- Steinhaus theorem) . But i think the above question can be answered negatively , I mean there is some counter examples but neither I can prove it not cite a counter example. I sincerely need help. Thanks.","and be two normed space. Question: be such that pointwise i.e in the space implies . Does this implies is a Banach space? The converse is well known ( Banach- Steinhaus theorem) . But i think the above question can be answered negatively , I mean there is some counter examples but neither I can prove it not cite a counter example. I sincerely need help. Thanks.","(X, \|•\|)  (Y, \|•\|')  \begin{align} {\scr{B}}{(X, Y) }&=\{T\in {\scr{L}}{(X,Y)}: T \text{ is bounded } \}\end{align} \|T\|_{op}=\sup\{\|Tx\|':\|x\|\le 1 \} \forall (T_n) \subset {\scr{B}}{(X, Y)} T_n\to T  [ \forall x\in x, T_nx\to Tx  (Y, \|•\|') ] T\in {\scr{B}}{(X, Y)} (X, \|•\|) ","['functional-analysis', 'analysis', 'banach-spaces', 'examples-counterexamples', 'pointwise-convergence']"
81,"Is this simple operator bounded from $L^p([1,2])$ to $L^p([0,1])$?",Is this simple operator bounded from  to ?,"L^p([1,2]) L^p([0,1])","Let $f\in L^{p}([1,2])$ with $p>1$ , and define $$Tf(x):= \int_{1<y<2}\frac{f(y)}{|x-y|}dy,\quad 0<x<1.$$ I am trying to prove/disporove that $$\|Tf\|_{L^p([0,1])}\leq C \|f\|_{L^p([1,2])}\quad (1)$$ I did not go far enough by Minkowski's integral inequality which implies $$\left( \int_{0}^{1}\left(\int_{1<y<2}\frac{f(y)}{|x-y|}dy\right)^{p} dx \right)^{1/p}\leq \int_{1<y<2}f(y)\left(\int_{0}^{1}\frac{dx}{(y-x)^{p}}\right)^{1/p}dy$$ One can estimate $$\left(\int_{0}^{1}\frac{dx}{(y-x)^{p}}\right)^{1/p}=\frac{1}{p-1}\left(\frac{1}{(y-1)^{p-1}}-\frac{1}{y^{p-1}}\right)^{1/p}\leq \frac{C}{(y-1)^{1-\frac{1}{p}}}$$ for $y$ close enough to $1$ . But this is not very helpful since $\int_{0}^{1}\frac{f(y)}{(y-1)^{1-\frac{1}{p}}} dx$ is not bounded by $\|f\|_{L^{p}([1,2])}$ considering the counterexample $g_{p}(x):=\frac{\chi_{[1,3/2]}(x)}{|\log{(x-1)}|(x-1)^{\frac{1}{p}}}$ .","Let with , and define I am trying to prove/disporove that I did not go far enough by Minkowski's integral inequality which implies One can estimate for close enough to . But this is not very helpful since is not bounded by considering the counterexample .","f\in L^{p}([1,2]) p>1 Tf(x):= \int_{1<y<2}\frac{f(y)}{|x-y|}dy,\quad 0<x<1. \|Tf\|_{L^p([0,1])}\leq C \|f\|_{L^p([1,2])}\quad (1) \left( \int_{0}^{1}\left(\int_{1<y<2}\frac{f(y)}{|x-y|}dy\right)^{p} dx \right)^{1/p}\leq \int_{1<y<2}f(y)\left(\int_{0}^{1}\frac{dx}{(y-x)^{p}}\right)^{1/p}dy \left(\int_{0}^{1}\frac{dx}{(y-x)^{p}}\right)^{1/p}=\frac{1}{p-1}\left(\frac{1}{(y-1)^{p-1}}-\frac{1}{y^{p-1}}\right)^{1/p}\leq \frac{C}{(y-1)^{1-\frac{1}{p}}} y 1 \int_{0}^{1}\frac{f(y)}{(y-1)^{1-\frac{1}{p}}} dx \|f\|_{L^{p}([1,2])} g_{p}(x):=\frac{\chi_{[1,3/2]}(x)}{|\log{(x-1)}|(x-1)^{\frac{1}{p}}}","['real-analysis', 'functional-analysis', 'analysis', 'inequality', 'harmonic-analysis']"
82,Folland theorem 6.19 proof doubt,Folland theorem 6.19 proof doubt,,"Consider the following fragment from Folland's real analysis book: They mention theorem 6.14, which is the following theorem: However, to apply theorem 6.14, shouldn't we check that if $h(x)= \int_Y f(x,y)d\nu(y)$ , then $\int_A |h| d\mu<\infty $ for every $A \in \mathcal{M}$ with $\mu(A) < \infty$ ? My question is, in other words, how is theorem 6.14 applied? Note that we may assume that the right hand side of the inequality we want to prove is finite, and this ensures that the supremum in theorem 6.14 is finite. Thanks in advance for any help!","Consider the following fragment from Folland's real analysis book: They mention theorem 6.14, which is the following theorem: However, to apply theorem 6.14, shouldn't we check that if , then for every with ? My question is, in other words, how is theorem 6.14 applied? Note that we may assume that the right hand side of the inequality we want to prove is finite, and this ensures that the supremum in theorem 6.14 is finite. Thanks in advance for any help!","h(x)= \int_Y f(x,y)d\nu(y) \int_A |h| d\mu<\infty  A \in \mathcal{M} \mu(A) < \infty","['functional-analysis', 'analysis', 'measure-theory', 'inequality', 'lp-spaces']"
83,Rudin's PMA: theorem 10.39,Rudin's PMA: theorem 10.39,,"This theorem is used in the proof of our theorem. Here is our  theorem and its proof : My question is that, how are the representations of $(118)$ and $(121)$ equal of each other? Any help would be appreciated.","This theorem is used in the proof of our theorem. Here is our  theorem and its proof : My question is that, how are the representations of and equal of each other? Any help would be appreciated.",(118) (121),"['real-analysis', 'calculus', 'analysis', 'multivariable-calculus']"
84,The proof of an elementary equality,The proof of an elementary equality,,"For all $t\geq0$ , $x>0$ , we have the following important equality: \begin{equation} \lim_{\lambda\rightarrow\infty}e^{-\lambda t}\sum_{k\leq\lambda x}\frac{(\lambda t)^k}{k!}=\chi_{[0,x)}(t)+\frac{1}{2}\chi_{\{x\}}(t), \end{equation} where $\chi$ is the characteristic function. Actually, I can prove this equality with the help of the Poisson distribution, but I prefer to prove this equality by a basic analysis method. I've struggled with this problem for a few days, still have no idea. I'll appreciate it for any hints! Thanks for any help!","For all , , we have the following important equality: where is the characteristic function. Actually, I can prove this equality with the help of the Poisson distribution, but I prefer to prove this equality by a basic analysis method. I've struggled with this problem for a few days, still have no idea. I'll appreciate it for any hints! Thanks for any help!","t\geq0 x>0 \begin{equation}
\lim_{\lambda\rightarrow\infty}e^{-\lambda t}\sum_{k\leq\lambda x}\frac{(\lambda t)^k}{k!}=\chi_{[0,x)}(t)+\frac{1}{2}\chi_{\{x\}}(t),
\end{equation} \chi","['real-analysis', 'probability', 'analysis', 'harmonic-analysis']"
85,$f$ is uniformly continuous iff $ \|\tau_y f - f \|_u \rightarrow 0 $ as $y \to 0$ (Proof verification),is uniformly continuous iff  as  (Proof verification),f  \|\tau_y f - f \|_u \rightarrow 0  y \to 0,"I'm trying to prove the following elementary fact mentioned on page 238 of Folland's Real Analysis : A function $f$ is called uniformly continuous if $\|\tau_y f - f \|_u \rightarrow 0$ as $y \to 0$ . (The reader should pause to check that this is equivalent to the usual $\epsilon$ - $\delta$ definition of uniform continuity.) I am taking Folland's advice and trying to show the equivalence of these two definitions of uniform continuity. First, some notation and definitions: If $f$ is a function on $\mathbb{R}^d$ and $y \in \mathbb{R}^d$ , then $\tau_y f(x) := f(x-y)$ . $\|f \|_u := \sup\left\{|f(x)|: x \in \mathbb{R}^d \right\}$ . ( $\|\cdot \|_u$ is called the uniform norm .) $e_1 := (1,0,\ldots,0) \in \mathbb{R}^d$ My working $\epsilon$ - $\delta$ definition of uniform continuity: A function $f: \mathbb{R}^d \to \mathbb{C}$ is uniformly continuous if, for each $\epsilon > 0$ , there exists some $\delta > 0$ such that $$x,y \in \mathbb{R}^d \text{ and } |x-y| < \delta \implies |f(x) - f(y)| < \epsilon.$$ My proof : [ Edit 3/21/22: This proof is wrong! See updated proof below. ] $\implies$ direction : Suppose $f$ is uniformly continuous in the $\epsilon$ - $\delta$ sense. Then we may pick a sequence of positive real numbers $\{\delta_n\}_{n=1}^{\infty}$ such that $$ x,y \in \mathbb{R}^d \text{ and } |x-y| \leq \delta_n \implies |f(x) - f(y)| < \frac{1}{n}$$ for each $n \in \mathbb{N}$ . Then $|f(x - \delta_n e_1) - f(x)| < \frac{1}{n}$ for all $x \in \mathbb{R}^d$ , for each $n \in \mathbb{N}$ (since $|(x - \delta_n e_1) - x| = |\delta_n e_1| = \delta_n \leq \delta_n$ ), and so $\sup\{|f(x-\delta_n e_1) - f(x)|: x \in \mathbb{R}^d \} \leq 1/n$ for each $n \in \mathbb{N}$ . In other words, $\|\tau_{\delta_n e_1}f - f \|_u \leq 1/n$ for all $n$ . Then since $\{\delta_n e_1 \}_{n=1}^{\infty}$ is a sequence in $\mathbb{R}^d$ and $\delta_n e_1 \rightarrow 0$ as $n \to \infty$ (with respect to $| \cdot |$ , the standard metric on $\mathbb{R}^d$ ), we have \begin{align*}    \lim_{y \to 0} \|\tau_{y}f - f \|_u &= \lim_{n \to \infty} \|\tau_{\delta_n e_1} f - f \|_u \leq \lim_{n \to \infty} \frac{1}{n} = 0. \end{align*} Thus, $\lim_{y \to 0} \|\tau_{y}f - f \|_u  = 0$ . (Just a note that $y \in \mathbb{R^d}$ here, so $y \to 0$ really means $y \to (0,\ldots,0)$ .) $\impliedby$ direction : Suppose $\|\tau_y f - f \|_u \to 0$ as $y \to 0$ . Then for each $\epsilon > 0$ , there exists a $\delta > 0$ such that \begin{align*}     & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies \|\tau_y f - f \|_u < \epsilon \\[4pt] \implies \quad & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies \sup \left\{|f(x-y) - f(x)|: x \in \mathbb{R}^d \right\} < \epsilon \\[4pt] \implies \quad & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \text{ for all } x \in \mathbb{R}^d \\[4pt] \implies \quad & x,y \in \mathbb{R}^d \text{ and } |x - y| < \delta \implies |f(x) - f(y)| < \epsilon, \end{align*} and so $f$ is uniformly continuous in the $\epsilon$ - $\delta$ sense. $\qquad \square$ Does this proof look ok? Any feedback or suggestions for improvement are welcomed. Update 3/21/22 : Thanks to @Mason for pointing out the issue with my "" $\implies$ "" direction proof. I've attempted to write a corrected proof: Corrected "" $\implies$ "" proof: Suppose $f$ is uniformly continuous in the $\epsilon$ - $\delta$ sense. Then for each $\epsilon > 0$ , there exists a $\delta > 0$ such that \begin{align*}     & x,y \in \mathbb{R}^d \;\text{ and }\; |x-y| < \delta \implies |f(x)-f(y)| < \epsilon \\[3pt]  \implies \quad & x,y \in \mathbb{R}^d \;\text{ and }\; |(x-y) - x| < \delta \implies |f(x-y) - f(y)| < \epsilon \\[3pt]    \implies \quad & x,y \in \mathbb{R}^d \;\text{ and }\;  |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \\[3pt]     \implies \quad & y \in \mathbb{R}^d \;\text{ and }\;  |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \quad \text{for all } x \in \mathbb{R}^d \\[3pt]    \implies \quad & y \in \mathbb{R}^d \; \text{ and }\; |y| < \delta \implies \sup_{x \in \mathbb{R}^d} |f(x-y) - f(x)| \leq \epsilon \\[3pt]     \implies \quad & y \in \mathbb{R}^d \; \text{ and }\; |y| < \delta \implies \|\tau_y f - f \|_u \leq \epsilon. \end{align*} Does this look okay now?","I'm trying to prove the following elementary fact mentioned on page 238 of Folland's Real Analysis : A function is called uniformly continuous if as . (The reader should pause to check that this is equivalent to the usual - definition of uniform continuity.) I am taking Folland's advice and trying to show the equivalence of these two definitions of uniform continuity. First, some notation and definitions: If is a function on and , then . . ( is called the uniform norm .) My working - definition of uniform continuity: A function is uniformly continuous if, for each , there exists some such that My proof : [ Edit 3/21/22: This proof is wrong! See updated proof below. ] direction : Suppose is uniformly continuous in the - sense. Then we may pick a sequence of positive real numbers such that for each . Then for all , for each (since ), and so for each . In other words, for all . Then since is a sequence in and as (with respect to , the standard metric on ), we have Thus, . (Just a note that here, so really means .) direction : Suppose as . Then for each , there exists a such that and so is uniformly continuous in the - sense. Does this proof look ok? Any feedback or suggestions for improvement are welcomed. Update 3/21/22 : Thanks to @Mason for pointing out the issue with my "" "" direction proof. I've attempted to write a corrected proof: Corrected "" "" proof: Suppose is uniformly continuous in the - sense. Then for each , there exists a such that Does this look okay now?","f \|\tau_y f - f \|_u \rightarrow 0 y \to 0 \epsilon \delta f \mathbb{R}^d y \in \mathbb{R}^d \tau_y f(x) := f(x-y) \|f \|_u := \sup\left\{|f(x)|: x \in \mathbb{R}^d \right\} \|\cdot \|_u e_1 := (1,0,\ldots,0) \in \mathbb{R}^d \epsilon \delta f: \mathbb{R}^d \to \mathbb{C} \epsilon > 0 \delta > 0 x,y \in \mathbb{R}^d \text{ and } |x-y| < \delta \implies |f(x) - f(y)| < \epsilon. \implies f \epsilon \delta \{\delta_n\}_{n=1}^{\infty}  x,y \in \mathbb{R}^d \text{ and } |x-y| \leq \delta_n \implies |f(x) - f(y)| < \frac{1}{n} n \in \mathbb{N} |f(x - \delta_n e_1) - f(x)| < \frac{1}{n} x \in \mathbb{R}^d n \in \mathbb{N} |(x - \delta_n e_1) - x| = |\delta_n e_1| = \delta_n \leq \delta_n \sup\{|f(x-\delta_n e_1) - f(x)|: x \in \mathbb{R}^d \} \leq 1/n n \in \mathbb{N} \|\tau_{\delta_n e_1}f - f \|_u \leq 1/n n \{\delta_n e_1 \}_{n=1}^{\infty} \mathbb{R}^d \delta_n e_1 \rightarrow 0 n \to \infty | \cdot | \mathbb{R}^d \begin{align*}
   \lim_{y \to 0} \|\tau_{y}f - f \|_u &= \lim_{n \to \infty} \|\tau_{\delta_n e_1} f - f \|_u \leq \lim_{n \to \infty} \frac{1}{n} = 0.
\end{align*} \lim_{y \to 0} \|\tau_{y}f - f \|_u  = 0 y \in \mathbb{R^d} y \to 0 y \to (0,\ldots,0) \impliedby \|\tau_y f - f \|_u \to 0 y \to 0 \epsilon > 0 \delta > 0 \begin{align*}
    & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies \|\tau_y f - f \|_u < \epsilon \\[4pt]
\implies \quad & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies \sup \left\{|f(x-y) - f(x)|: x \in \mathbb{R}^d \right\} < \epsilon \\[4pt]
\implies \quad & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \text{ for all } x \in \mathbb{R}^d \\[4pt]
\implies \quad & x,y \in \mathbb{R}^d \text{ and } |x - y| < \delta \implies |f(x) - f(y)| < \epsilon,
\end{align*} f \epsilon \delta \qquad \square \implies \implies f \epsilon \delta \epsilon > 0 \delta > 0 \begin{align*}
    & x,y \in \mathbb{R}^d \;\text{ and }\; |x-y| < \delta \implies |f(x)-f(y)| < \epsilon \\[3pt]
 \implies \quad & x,y \in \mathbb{R}^d \;\text{ and }\; |(x-y) - x| < \delta \implies |f(x-y) - f(y)| < \epsilon \\[3pt]
   \implies \quad & x,y \in \mathbb{R}^d \;\text{ and }\;  |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \\[3pt]
    \implies \quad & y \in \mathbb{R}^d \;\text{ and }\;  |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \quad \text{for all } x \in \mathbb{R}^d \\[3pt]
   \implies \quad & y \in \mathbb{R}^d \; \text{ and }\; |y| < \delta \implies \sup_{x \in \mathbb{R}^d} |f(x-y) - f(x)| \leq \epsilon \\[3pt]
    \implies \quad & y \in \mathbb{R}^d \; \text{ and }\; |y| < \delta \implies \|\tau_y f - f \|_u \leq \epsilon.
\end{align*}","['real-analysis', 'analysis', 'solution-verification', 'uniform-continuity']"
86,Is this a viable technique for integration that I should persue exploring? Thoughts?,Is this a viable technique for integration that I should persue exploring? Thoughts?,,"I started with the integral of: $$\int \frac{\ln(x)}{x}dx = \frac{\ln^2(x)}{2} + C$$ Which was very easy to integrate. Then, I moved to a more difficult problem: $$\int \frac{\ln(x-t)}{x} dx$$ I started by letting $t$ tend to some arbitrary, zero-like variable $O_t$ . Where some $a_0 - O_t = a_0$ , but $O_t$ is not technically equal to zero. In other words, expressions containing $O_t$ cannot be simplified and compacted, but it behaves like zero when the cyclical nature of the integral pops up. Using it, the DI method yields: $$\int \frac{\ln(x-O_t)}{x}dx = \ln(x-O_t)\ln(x) - \int \frac{\ln(x)}{x-O_t}dx$$ Since this $O_t$ is technically equal to zero, then by definition: $$\int \frac{\ln(x-O_t)}{x}dx = \int \frac{\ln(x)}{x-O_t}dx$$ Plugging this is, you find that: $$\int \frac{\ln(x-O_t)}{x}dx = \ln(x-O_t)\ln(x) - \int \frac{\ln(x-O_t)}{x}dx$$ You can add the integral to both sides, and divide by two: $$\int \frac{\ln(x-O_t)}{x}dx = \frac{\ln(x-O_t)\ln(x)}{2}+C$$ Then, in order to complete the problem, you can let $O_t$ re-approach $t$ : $$\int \frac{\ln(x-t)}{x}dx = \lim\int \frac{\ln(x-O_t)}{x}dx = \frac{\ln(x-t)\ln(x)}{2} + C$$ In order to test this, you can plug in $t=0$ to see this conclusion in action: $$\int \frac{\ln(x-0)}{x} dx = \frac{\ln(x-0)\ln(x)}{2} + C = \frac{\ln^2(x)}{2} + C$$ Which indeed is correct. Is this new ""zero-substitution"" technique for integration viable? Or does it have major gap holes. Should I pursue it and apply it to other integrals? Thank you for your help.","I started with the integral of: Which was very easy to integrate. Then, I moved to a more difficult problem: I started by letting tend to some arbitrary, zero-like variable . Where some , but is not technically equal to zero. In other words, expressions containing cannot be simplified and compacted, but it behaves like zero when the cyclical nature of the integral pops up. Using it, the DI method yields: Since this is technically equal to zero, then by definition: Plugging this is, you find that: You can add the integral to both sides, and divide by two: Then, in order to complete the problem, you can let re-approach : In order to test this, you can plug in to see this conclusion in action: Which indeed is correct. Is this new ""zero-substitution"" technique for integration viable? Or does it have major gap holes. Should I pursue it and apply it to other integrals? Thank you for your help.",\int \frac{\ln(x)}{x}dx = \frac{\ln^2(x)}{2} + C \int \frac{\ln(x-t)}{x} dx t O_t a_0 - O_t = a_0 O_t O_t \int \frac{\ln(x-O_t)}{x}dx = \ln(x-O_t)\ln(x) - \int \frac{\ln(x)}{x-O_t}dx O_t \int \frac{\ln(x-O_t)}{x}dx = \int \frac{\ln(x)}{x-O_t}dx \int \frac{\ln(x-O_t)}{x}dx = \ln(x-O_t)\ln(x) - \int \frac{\ln(x-O_t)}{x}dx \int \frac{\ln(x-O_t)}{x}dx = \frac{\ln(x-O_t)\ln(x)}{2}+C O_t t \int \frac{\ln(x-t)}{x}dx = \lim\int \frac{\ln(x-O_t)}{x}dx = \frac{\ln(x-t)\ln(x)}{2} + C t=0 \int \frac{\ln(x-0)}{x} dx = \frac{\ln(x-0)\ln(x)}{2} + C = \frac{\ln^2(x)}{2} + C,"['calculus', 'integration', 'analysis', 'substitution']"
87,About the limit of a function,About the limit of a function,,"We know that if $A\subseteq\mathbb{R}$ and $x_{0}\in \mathbb{R}$ a cluster point of $A$ . Then the real number $L$ is called limit of $f:A\rightarrow \mathbb{R}$ at $x_{0}$ if for all $\epsilon>0$ , there is a $\delta_{\epsilon}>0$ such that $\forall x\in A$ such that $0<|x-x_{0}|<\delta_{\epsilon} \Rightarrow |f(x)-f(x_{0})|<\epsilon$ . My questions are (i) Why we take a cluster point here instead of any real number? (ii) $\delta$ depends on $\epsilon$ but in the example $f(x)=\frac{1}{x}$ for all $x\in A=(0,\infty)$ , we have for all $\epsilon>0$ , a $\delta$ is $\delta=\inf\{\frac{x_{0}}{2},\frac{x_{0}^{2}}{2}\}$ so that $f(x)\rightarrow \frac{1}{x_{0}}$ as $x\rightarrow x_{0}$ . Clearly $\delta$ depends on $\epsilon$ and $x_{0}$ . In continuity, we called uniform continuity. Can we called here the convergence uniform convergence if $\delta$ depends only on $\epsilon$ .","We know that if and a cluster point of . Then the real number is called limit of at if for all , there is a such that such that . My questions are (i) Why we take a cluster point here instead of any real number? (ii) depends on but in the example for all , we have for all , a is so that as . Clearly depends on and . In continuity, we called uniform continuity. Can we called here the convergence uniform convergence if depends only on .","A\subseteq\mathbb{R} x_{0}\in \mathbb{R} A L f:A\rightarrow \mathbb{R} x_{0} \epsilon>0 \delta_{\epsilon}>0 \forall x\in A 0<|x-x_{0}|<\delta_{\epsilon} \Rightarrow |f(x)-f(x_{0})|<\epsilon \delta \epsilon f(x)=\frac{1}{x} x\in A=(0,\infty) \epsilon>0 \delta \delta=\inf\{\frac{x_{0}}{2},\frac{x_{0}^{2}}{2}\} f(x)\rightarrow \frac{1}{x_{0}} x\rightarrow x_{0} \delta \epsilon x_{0} \delta \epsilon","['real-analysis', 'calculus', 'analysis']"
88,Extraction of pointwise convergenct subsequence using Arzela-Ascoli theorem,Extraction of pointwise convergenct subsequence using Arzela-Ascoli theorem,,"Let $f_n:[a, b] \rightarrow \mathbb{R}$ be a sequence of continuous functions which is uniformly bounded i.e. $||f_n||_{L^{\infty}} \leq M <\infty$ and satisfies $f_n(a)=A$ for all $n\in \mathbb{N}.$ In addition for every $\epsilon >0,$ the function $f_n$ also satisfies the following equicontinuity estimate, $$|f_n(x)-f_n(y)| \leq c(\epsilon)|x-y| \quad \quad \quad \text{ for all } x,y\in [a+\epsilon, b],$$ with $c(\epsilon) \rightarrow \infty$ as $\epsilon \rightarrow 0^+.$ Now can we conclude that up to a subsequence $f_n \rightarrow f$ pointwise where $f \in C([0,b])$ and $f(a)=A?$ P.S.: If the answer is yes, a clean proof of the same would be appreciated.","Let be a sequence of continuous functions which is uniformly bounded i.e. and satisfies for all In addition for every the function also satisfies the following equicontinuity estimate, with as Now can we conclude that up to a subsequence pointwise where and P.S.: If the answer is yes, a clean proof of the same would be appreciated.","f_n:[a, b] \rightarrow \mathbb{R} ||f_n||_{L^{\infty}} \leq M <\infty f_n(a)=A n\in \mathbb{N}. \epsilon >0, f_n |f_n(x)-f_n(y)| \leq c(\epsilon)|x-y| \quad \quad \quad \text{ for all } x,y\in [a+\epsilon, b], c(\epsilon) \rightarrow \infty \epsilon \rightarrow 0^+. f_n \rightarrow f f \in C([0,b]) f(a)=A?","['analysis', 'real-numbers', 'sequence-of-function', 'arzela-ascoli']"
89,Decide whether this integration $\displaystyle \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx$ converges or diverges,Decide whether this integration  converges or diverges,\displaystyle \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx,"\begin{equation} \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx \end{equation} I don't know how to prove if this integration is convergent or divergent. My first idea is to find another function and do a limit comparison with $\dfrac{x}{1+x^2\cos x}$ at $x= \infty$ , but I couldn't find a suitable function to do that. Then, I tried to find whether $\dfrac{x}{1+x^2\cos x}$ this function is always less than another function or whether it is always greater than another function. However, since $\cos $ is oscillating between 1 and -1, I failed I also tried to ""split"" this integral by only considering some intervals on which $\cos$ is greater than 0 or is less than zero (something like $\int ^\frac{\pi}{2} _ 0 \frac{x}{1+x^2\cos x} dx + \int ^\frac{3\pi}{2} _ \frac{\pi}{2} \frac{x}{1+x^2\cos x} dx+...$ ) As a result, I still can't figure it out. Even though I can prove this the absolute value of $\dfrac{x}{1+x^2\cos x}$ is divergent, but it also seems not to be helpful. Thus, any helps? Thanks! Edit:(The integration is wrong, please dismiss this edition) I tried integration by parts: \begin{equation} \displaystyle \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx = \dfrac{x^2}{2(1+x^2\cos x)}|^{\infty} _{0} + \dfrac{1}{2}\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx \end{equation} This function's absolute value is divergent, by writing the right hand side integration to be \begin{equation} \int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx =\int^\infty_0|\frac{x}{(1+x^2\cos x)}|^2 dx \end{equation} Thus $\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx $ is divergent to infinity at $x= \infty$ And $\dfrac{x^2}{2(1+x^2\cos x)}$ oscillates at positive infinity and negative infinity As a result, I think this integration doesn't converge Is this a reasonable argument?","I don't know how to prove if this integration is convergent or divergent. My first idea is to find another function and do a limit comparison with at , but I couldn't find a suitable function to do that. Then, I tried to find whether this function is always less than another function or whether it is always greater than another function. However, since is oscillating between 1 and -1, I failed I also tried to ""split"" this integral by only considering some intervals on which is greater than 0 or is less than zero (something like ) As a result, I still can't figure it out. Even though I can prove this the absolute value of is divergent, but it also seems not to be helpful. Thus, any helps? Thanks! Edit:(The integration is wrong, please dismiss this edition) I tried integration by parts: This function's absolute value is divergent, by writing the right hand side integration to be Thus is divergent to infinity at And oscillates at positive infinity and negative infinity As a result, I think this integration doesn't converge Is this a reasonable argument?","\begin{equation}
\int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx
\end{equation} \dfrac{x}{1+x^2\cos x} x= \infty \dfrac{x}{1+x^2\cos x} \cos  \cos \int ^\frac{\pi}{2} _ 0 \frac{x}{1+x^2\cos x} dx + \int ^\frac{3\pi}{2} _ \frac{\pi}{2} \frac{x}{1+x^2\cos x} dx+... \dfrac{x}{1+x^2\cos x} \begin{equation}
\displaystyle \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx = \dfrac{x^2}{2(1+x^2\cos x)}|^{\infty} _{0} + \dfrac{1}{2}\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx
\end{equation} \begin{equation}
\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx =\int^\infty_0|\frac{x}{(1+x^2\cos x)}|^2 dx
\end{equation} \int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx  x= \infty \dfrac{x^2}{2(1+x^2\cos x)}","['integration', 'analysis', 'convergence-divergence', 'solution-verification']"
90,Convergence in Bochner space.,Convergence in Bochner space.,,"Let $\Omega \subset \mathbb{R}^n$ be a bounded set and $\{f_n\}_{n \in \mathbb{N}}$ be a sequence which converges to $f$ in $C([0,T];L^2(\Omega))$ . I need to prove that $\int_\Omega f_n^2 \phi \,dx \to \int_\Omega f^2\phi\,dx$ in $L^\infty([0,T])$ as $n \to \infty$ for any $\phi \in C^\infty_c(\Omega)$ . And does boundedness really matter, can we choose $\Omega =\mathbb{R}^n$ as well? We have $\lim_{n \to \infty} \sup_{[0,T]} \int_\Omega |f_n(t)-f(t)|^2\,dx = 0$ . We therefore consider $\left| \int \left(f^2_n\phi-f^2\phi\right) \,dx \right|\le \int \left|f^2_n-f^2\right| \phi\,dx \le \|\phi\|_\infty \int |f^2_n-f^2|\,dx $ . But I do not understand how this dominating part tends to zero.","Let be a bounded set and be a sequence which converges to in . I need to prove that in as for any . And does boundedness really matter, can we choose as well? We have . We therefore consider . But I do not understand how this dominating part tends to zero.","\Omega \subset \mathbb{R}^n \{f_n\}_{n \in \mathbb{N}} f C([0,T];L^2(\Omega)) \int_\Omega f_n^2 \phi \,dx \to \int_\Omega f^2\phi\,dx L^\infty([0,T]) n \to \infty \phi \in C^\infty_c(\Omega) \Omega =\mathbb{R}^n \lim_{n \to \infty} \sup_{[0,T]} \int_\Omega |f_n(t)-f(t)|^2\,dx = 0 \left| \int \left(f^2_n\phi-f^2\phi\right) \,dx \right|\le \int \left|f^2_n-f^2\right| \phi\,dx \le \|\phi\|_\infty \int |f^2_n-f^2|\,dx ","['functional-analysis', 'analysis', 'bochner-spaces']"
91,"Let $f,g \in \mathbb{R}[x]$ , $f(x)+g(x)=5$ and $f(g(x))=8-4x $ find $g(2)$","Let  ,  and  find","f,g \in \mathbb{R}[x] f(x)+g(x)=5 f(g(x))=8-4x  g(2)","Let $f,g$ are two polynomials $f,g \in \mathbb{R}[x]$ such that $f(x)+g(x)=5$ and $f(g(x))=8-4x $ . Find set of all possible values of $g(2)$ . Let $\deg f=n , \deg g=m$ so $\deg f(g(x)) \le mn$ so $mn=1$ then $m=n=1$ then $f(x)=ax+b$ and $g(x)=cx+d$ We have $f(g(2))=0$",Let are two polynomials such that and . Find set of all possible values of . Let so so then then and We have,"f,g f,g \in \mathbb{R}[x] f(x)+g(x)=5 f(g(x))=8-4x  g(2) \deg f=n , \deg g=m \deg f(g(x)) \le mn mn=1 m=n=1 f(x)=ax+b g(x)=cx+d f(g(2))=0","['real-analysis', 'calculus', 'algebra-precalculus', 'analysis', 'functions']"
92,Understanding a statement about the series $S =\sum_2 ^\infty \frac{1}{n\ln n}$,Understanding a statement about the series,S =\sum_2 ^\infty \frac{1}{n\ln n},"In Mathematical Methods for Physicists , the author writes the following with reference to the series $S =\sum_2 ^\infty \frac{1}{n\ln n}$ : We form the integral $\int_2^\infty \frac{1}{x\ln x} dx$ which diverges, indicating that $S$ is divergent. Because $n\ln n \gt n$ , the divergence is slower than that of the harmonic series. So far so good. He further writes: But because $\ln(n)$ increases more slowly than $n^\epsilon$ , where $\epsilon$ can have an arbitrarily small positive value, we have divergence even though the series $\sum_n n^{-{(1+\epsilon)}}$ converges. This is the part that confuses me. Perhaps the author is referring to the comparison test. It is known that $\sum_n n^{-{(1+\epsilon)}}$ , i.e. $\sum_n \frac{1}{n.n^\epsilon}$ converges since $1+\epsilon \gt 1$ . It would suffice to prove that $n\ln(n) \gt n.n^\epsilon$ . For this we compare how fast $\ln(n)$ and $n^\epsilon$ increase with $n$ . Let's take their derivatives, $\frac{1}{n}$ and $\frac{\epsilon}{n^{1-\epsilon}}$ . Now we do not know the nature of $\epsilon$ . How do I proceed with the conclusion?","In Mathematical Methods for Physicists , the author writes the following with reference to the series : We form the integral which diverges, indicating that is divergent. Because , the divergence is slower than that of the harmonic series. So far so good. He further writes: But because increases more slowly than , where can have an arbitrarily small positive value, we have divergence even though the series converges. This is the part that confuses me. Perhaps the author is referring to the comparison test. It is known that , i.e. converges since . It would suffice to prove that . For this we compare how fast and increase with . Let's take their derivatives, and . Now we do not know the nature of . How do I proceed with the conclusion?",S =\sum_2 ^\infty \frac{1}{n\ln n} \int_2^\infty \frac{1}{x\ln x} dx S n\ln n \gt n \ln(n) n^\epsilon \epsilon \sum_n n^{-{(1+\epsilon)}} \sum_n n^{-{(1+\epsilon)}} \sum_n \frac{1}{n.n^\epsilon} 1+\epsilon \gt 1 n\ln(n) \gt n.n^\epsilon \ln(n) n^\epsilon n \frac{1}{n} \frac{\epsilon}{n^{1-\epsilon}} \epsilon,"['real-analysis', 'calculus', 'analysis']"
93,How do i find the value for the series $ \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)}$? [duplicate],How do i find the value for the series ? [duplicate], \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)},"This question already has answers here : Find the sum of the series $\sum \frac{1}{n(n+1)(n+2)}$ (10 answers) $\sum_1^\infty{\frac{1}{n(n+1)(n+2)}}$? [duplicate] (1 answer) Closed 2 years ago . I have a general problem understand how to solve these kind of questions where you have a series and you need to find the value of it: $$  \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)} $$ I know that $  \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)} $ = $ \frac{a_1}{n}+\frac{a_2}{n+1}+\frac{a_3}{n+2} $ which can be rewritten as: $ n^2(a_1+a_2+a_3)+n^1(3a_1+2a_2+a_3)n^0(2a_1)=1$ Using a LGS i come up with the solutions: $ a_1 = \frac{1}{2}$ ; $ a_2 = -1 $ ; $ a_3 = \frac{1}{2} $ ; Okay, So i put that into my initial formula: $  \sum_{n=1}^{\infty} \frac{0.5}{n}+\frac{-1}{n+1}+\frac{0.5}{n+2} $ Nice, but here i need a clear guidance whats next as this step is confusing me. It would be really great so see a solution for this so that i can study it further. Thanks in advance everyone!","This question already has answers here : Find the sum of the series $\sum \frac{1}{n(n+1)(n+2)}$ (10 answers) $\sum_1^\infty{\frac{1}{n(n+1)(n+2)}}$? [duplicate] (1 answer) Closed 2 years ago . I have a general problem understand how to solve these kind of questions where you have a series and you need to find the value of it: I know that = which can be rewritten as: Using a LGS i come up with the solutions: ; ; ; Okay, So i put that into my initial formula: Nice, but here i need a clear guidance whats next as this step is confusing me. It would be really great so see a solution for this so that i can study it further. Thanks in advance everyone!",  \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)}    \sum_{n=1}^{\infty} \frac{1}{n(n+1)(n+2)}   \frac{a_1}{n}+\frac{a_2}{n+1}+\frac{a_3}{n+2}   n^2(a_1+a_2+a_3)+n^1(3a_1+2a_2+a_3)n^0(2a_1)=1  a_1 = \frac{1}{2}  a_2 = -1   a_3 = \frac{1}{2}    \sum_{n=1}^{\infty} \frac{0.5}{n}+\frac{-1}{n+1}+\frac{0.5}{n+2} ,"['calculus', 'sequences-and-series', 'analysis']"
94,Infinite Sum of Infinite Product,Infinite Sum of Infinite Product,,"I've got an expression, $$ \left( \prod^{n}_{i = 1} \mu a_i \right) \left(\sum^{n}_{j=1} \frac{1 - a_j}{\prod^{j}_{k=1} \mu a_k} \right)  $$ Where each $0 < a_j < 1 / \mu$ and $\mu > 1$ , so for all $a_j$ , $0 < 1 - a_j$ . I'm trying to find an analytic solution, or determine if one exists, for this expression. I know beforehand that this converges as $n \rightarrow \infty$ . Both the case of finite $n$ and for infinite $n$ terms are useful. What methods or approaches are helpful here? Pointing to similar problems is also helpful. I'm not sure what to study to understand when looking for simplifications of sums of products.","I've got an expression, Where each and , so for all , . I'm trying to find an analytic solution, or determine if one exists, for this expression. I know beforehand that this converges as . Both the case of finite and for infinite terms are useful. What methods or approaches are helpful here? Pointing to similar problems is also helpful. I'm not sure what to study to understand when looking for simplifications of sums of products.","
\left( \prod^{n}_{i = 1} \mu a_i \right) \left(\sum^{n}_{j=1} \frac{1 - a_j}{\prod^{j}_{k=1} \mu a_k} \right) 
 0 < a_j < 1 / \mu \mu > 1 a_j 0 < 1 - a_j n \rightarrow \infty n n","['sequences-and-series', 'analysis', 'summation', 'products', 'upper-lower-bounds']"
95,How to find du and dv?,How to find du and dv?,,Find $du$ and $dv$ if $u+v=x+y$ and $\frac{\sin(u)}{\sin(v)}=\frac{x}{y}$ . How to solve this? Found almost an answer: But how do we get $du=...$ from the second?,Find and if and . How to solve this? Found almost an answer: But how do we get from the second?,du dv u+v=x+y \frac{\sin(u)}{\sin(v)}=\frac{x}{y} du=...,"['analysis', 'implicit-differentiation']"
96,$\varepsilon$-chain property for the inverse function,-chain property for the inverse function,\varepsilon,"I'm trying to solve a problem of Lan Wen's differentiable Dynamical Systems book. Before writing the problem I introduce some notions in here: Denote by $X$ a compact metric space and by $f:X \to X$ a homeomorphism. By an $\varepsilon$ -chain we mean a sequence $x_0 , \cdots , x_n$ which satisfy $d(f(x_i),x_{i+1}) < \varepsilon$ for $ 0 \leq i \leq n-1$ Here is the problem: Prove that for any $\delta >0 $ there is $ \eta>0$ such that if $ x_0 ,\cdots , x_k$ is an $\eta$ -chain of $f$ , then $x_k , \cdots, x_0$ is a $\delta$ -chain of $f^{-1}$ . My try : I believe that here we could use the fact that $ f$ is uniformly continuous on $X$ and we could consider $0<\eta<\delta$ and for any $x,y \in X$ if $d(x,y) < \eta$ then $d(f(x) , f(y))<\varepsilon$ . considering this if we suppose $x_0 , \cdots , x_k$ is an $\eta$ -chain then we need to prove $x_k, \cdots , x_0$ is a $\delta$ -chain under $f^{-1}$ . Is this idea a true way to solve this problem or I need to think differently?","I'm trying to solve a problem of Lan Wen's differentiable Dynamical Systems book. Before writing the problem I introduce some notions in here: Denote by a compact metric space and by a homeomorphism. By an -chain we mean a sequence which satisfy for Here is the problem: Prove that for any there is such that if is an -chain of , then is a -chain of . My try : I believe that here we could use the fact that is uniformly continuous on and we could consider and for any if then . considering this if we suppose is an -chain then we need to prove is a -chain under . Is this idea a true way to solve this problem or I need to think differently?","X f:X \to X \varepsilon x_0 , \cdots , x_n d(f(x_i),x_{i+1}) < \varepsilon  0 \leq i \leq n-1 \delta >0   \eta>0  x_0 ,\cdots , x_k \eta f x_k , \cdots, x_0 \delta f^{-1}  f X 0<\eta<\delta x,y \in X d(x,y) < \eta d(f(x) , f(y))<\varepsilon x_0 , \cdots , x_k \eta x_k, \cdots , x_0 \delta f^{-1}","['general-topology', 'analysis', 'dynamical-systems']"
97,"The $E_k$ in the definition of the simple function on pg. 61 in Royden real analysis ""4th edition"".","The  in the definition of the simple function on pg. 61 in Royden real analysis ""4th edition"".",E_k,"I want to prove that the sum of 2 simple functions is a simple function and to do so I want to use the following facts: $$\chi_{A_i}=\sum_{j}\chi_{A_i\cap B_j} \text{ and } \chi_{B_j}=\sum_{i}\chi_{A_i\cap B_j}.$$ But say if I want to prove the first fact, upon fixing $i,$ I want $A_i$ to be subset of $E$ and $E = \cup_{j}B_j$ Here is the definition of the simple function stated in Royden: If $\varphi$ is simple, has domain $E$ and takes the distinct values $c_1, \dots, c_n,$ then $$\varphi = \sum_{i=1}^{n_1} c_k \chi_{E_k} \text{ where } E_k = \{x \in E| \varphi(x) = c_k \}.$$ My question is: Are we considering the $E_k's$ in the definition of the simple function to be a partition of $E$ ? Could someone clarify this to me please?","I want to prove that the sum of 2 simple functions is a simple function and to do so I want to use the following facts: But say if I want to prove the first fact, upon fixing I want to be subset of and Here is the definition of the simple function stated in Royden: If is simple, has domain and takes the distinct values then My question is: Are we considering the in the definition of the simple function to be a partition of ? Could someone clarify this to me please?","\chi_{A_i}=\sum_{j}\chi_{A_i\cap B_j} \text{ and } \chi_{B_j}=\sum_{i}\chi_{A_i\cap B_j}. i, A_i E E = \cup_{j}B_j \varphi E c_1, \dots, c_n, \varphi = \sum_{i=1}^{n_1} c_k \chi_{E_k} \text{ where } E_k = \{x \in E| \varphi(x) = c_k \}. E_k's E","['real-analysis', 'analysis']"
98,"If $|f_n|\leq g \in L^1$, and $f_n\rightarrow f$ in measure, then $f_n \rightarrow f \in L^1$ (Folland chp 2 ex. 34)","If , and  in measure, then  (Folland chp 2 ex. 34)",|f_n|\leq g \in L^1 f_n\rightarrow f f_n \rightarrow f \in L^1,"In Folland's Real Analysis, 1999 2nd edition. So the precise statement is actually: Let $(X, M, \mu)$ be a measure space. If $|f_n|\leq g \in L^1$ , and $f_n\rightarrow f$ in measure, then  (1) $\lim_{n\rightarrow \infty} \int f_n = \int f$ and (2) $f_n \rightarrow f \in L^1$ , however unless I am seriously mistaken 2 implies 1... So I have attempted to solve this problem and I am stuck, and want to ask for some help. My approach is contradiction. I will detail the work I have so far. I would prefer help which builds on my current work, however if my approach is futile then of course starting over is preferable, but in the end how you help is up to you. Suppose that $f_n \rightarrow f$ in $L^1$ does not hold, then $\exists\; \epsilon > 0$ s.t. $\forall$ $N$ , $\exists$ $n\geq N$ s.t. $\int_P |f_n -f| \geq \epsilon $ (where $P$ is a set of positive measure). Now, we know that $f_n$ converges in measure, denote $\{x\;|\;|f_m(x)-f(x)|\geq \epsilon\}=F_{m,\epsilon}$ , then $\exists$ M s.t. $\forall$ $m\geq M$ , we have $\mu(F_{m,\epsilon})<\epsilon$ . Now, at this point I start to become unsure of what to do. THe most interesting thing I can come up with is this: $$\sup_P |f_n -f| \mu(P)\geq \int_P |f_n-f| \geq \epsilon \mu(P) > 0$$ Also, we have that |f_n|\leq g, so we need to use this, but I am not sure how. Of course $\sup_P |f_n -f| \mu(P)\leq \sup_p 2g \mu(P)$ but I am not sure how this helps. Another interesting thing I found was that, $f_n \rightarrow f$ in $L^1$ is equivalent to $|f_n - f|\rightarrow 0$ in $L^1$ , so if we could show that $\int_P |f_n-f|\rightarrow 0$ somehow, than perhaps this would say something close to "" $f_n\rightarrow f$ in $L^1$ ""... (but I don't think it would say exactly that. Thanks!","In Folland's Real Analysis, 1999 2nd edition. So the precise statement is actually: Let be a measure space. If , and in measure, then  (1) and (2) , however unless I am seriously mistaken 2 implies 1... So I have attempted to solve this problem and I am stuck, and want to ask for some help. My approach is contradiction. I will detail the work I have so far. I would prefer help which builds on my current work, however if my approach is futile then of course starting over is preferable, but in the end how you help is up to you. Suppose that in does not hold, then s.t. , s.t. (where is a set of positive measure). Now, we know that converges in measure, denote , then M s.t. , we have . Now, at this point I start to become unsure of what to do. THe most interesting thing I can come up with is this: Also, we have that |f_n|\leq g, so we need to use this, but I am not sure how. Of course but I am not sure how this helps. Another interesting thing I found was that, in is equivalent to in , so if we could show that somehow, than perhaps this would say something close to "" in ""... (but I don't think it would say exactly that. Thanks!","(X, M, \mu) |f_n|\leq g \in L^1 f_n\rightarrow f \lim_{n\rightarrow \infty} \int f_n = \int f f_n \rightarrow f \in L^1 f_n \rightarrow f L^1 \exists\; \epsilon > 0 \forall N \exists n\geq N \int_P |f_n -f| \geq \epsilon  P f_n \{x\;|\;|f_m(x)-f(x)|\geq \epsilon\}=F_{m,\epsilon} \exists \forall m\geq M \mu(F_{m,\epsilon})<\epsilon \sup_P |f_n -f| \mu(P)\geq \int_P |f_n-f| \geq \epsilon \mu(P) > 0 \sup_P |f_n -f| \mu(P)\leq \sup_p 2g \mu(P) f_n \rightarrow f L^1 |f_n - f|\rightarrow 0 L^1 \int_P |f_n-f|\rightarrow 0 f_n\rightarrow f L^1","['real-analysis', 'integration', 'analysis', 'measure-theory']"
99,$f^{-1}(c)$ is measurable for each $c.$ Is $f$ necessarily measurable? [closed],is measurable for each  Is  necessarily measurable? [closed],f^{-1}(c) c. f,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to answer the following question: Suppose $f$ is a real-valued function on $\mathbb R$ such that $f^{-1}(c)$ is measurable for each $c.$ Is $f$ necessarily  measurable? Here is the solution I found online: ""Let $E$ denote a non-measurable subset of $(0,1).$ We know such a set exists from Theorem $17$ of Chapter $2.$ Consider the function $f$ defined as $$f(x) = e^x . (2 \chi_E - 1)$$ where $\chi_E$ is the characteristic function of the set $E.$ Then $\{x\in \mathbb R: f(x) > 0 \} = E$ is not a measurable set. However $f$ is one-to-one, so $f^{-1}(c)$ is either empty or a singleton set and therefore measurable."" My questions are: I am not sure what is the intuition behind the author defining $f(x)$ in terms of the the characteristic function of the non-measurable function $E$ . And what is the relation between $c$ and the function given in the example. Any elaboration will be greatly appreciated! Also, is there a proof for that the Vitali characteristic function is a nonmeasurable function? or just it is because its domain is nonmeasurable?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to answer the following question: Suppose is a real-valued function on such that is measurable for each Is necessarily  measurable? Here is the solution I found online: ""Let denote a non-measurable subset of We know such a set exists from Theorem of Chapter Consider the function defined as where is the characteristic function of the set Then is not a measurable set. However is one-to-one, so is either empty or a singleton set and therefore measurable."" My questions are: I am not sure what is the intuition behind the author defining in terms of the the characteristic function of the non-measurable function . And what is the relation between and the function given in the example. Any elaboration will be greatly appreciated! Also, is there a proof for that the Vitali characteristic function is a nonmeasurable function? or just it is because its domain is nonmeasurable?","f \mathbb R f^{-1}(c) c. f E (0,1). 17 2. f f(x) = e^x . (2 \chi_E - 1) \chi_E E. \{x\in \mathbb R: f(x) > 0 \} = E f f^{-1}(c) f(x) E c","['real-analysis', 'analysis']"
