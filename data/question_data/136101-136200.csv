,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How prove this $a_{1}=a_{2}=\cdots=a_{p}=0$,How prove this,a_{1}=a_{2}=\cdots=a_{p}=0,"let $a_{1},\cdots,a_{p}$ be real numbers,and let $b_{1},b_{2},\cdots,b_{p}$ be distinct postive numbers with $b_{1}$ being the greatest of them,and such $$\sum_{i=1}^{p}a_{i}b^k_{i}=0$$ for all natural numbers $k$. show that: $$a_{1}=a_{2}=\cdots=a_{p}=0$$ my try: since $\begin{cases}a_{1}+a_{2}+a_{3}+\cdots+a_{p}=0\\ a_{1}b_{1}+a_{2}b_{2}+\cdots+a_{p}b_{p}=0\\ a_{1}b^2_{1}+a_{2}b^2_{2}+\cdots+a_{p}b^2_{p}=0\\ \cdots\cdots\\ a_{1}b^n_{1}+a_{2}b^n_{2}+\cdots+a_{p}b^{n}_{p}=0\\ \cdots\cdots\cdots \end{cases}$ then I can't prove $$a_{1}=a_{2}=\cdots=a_{p}=0$$","let $a_{1},\cdots,a_{p}$ be real numbers,and let $b_{1},b_{2},\cdots,b_{p}$ be distinct postive numbers with $b_{1}$ being the greatest of them,and such $$\sum_{i=1}^{p}a_{i}b^k_{i}=0$$ for all natural numbers $k$. show that: $$a_{1}=a_{2}=\cdots=a_{p}=0$$ my try: since $\begin{cases}a_{1}+a_{2}+a_{3}+\cdots+a_{p}=0\\ a_{1}b_{1}+a_{2}b_{2}+\cdots+a_{p}b_{p}=0\\ a_{1}b^2_{1}+a_{2}b^2_{2}+\cdots+a_{p}b^2_{p}=0\\ \cdots\cdots\\ a_{1}b^n_{1}+a_{2}b^n_{2}+\cdots+a_{p}b^{n}_{p}=0\\ \cdots\cdots\cdots \end{cases}$ then I can't prove $$a_{1}=a_{2}=\cdots=a_{p}=0$$",,[]
1,Obtaining explicit solutions of the differential equation $\left(\frac{dy}{dx}\right)^{2}=\frac{1}{ay^2+by+c}$,Obtaining explicit solutions of the differential equation,\left(\frac{dy}{dx}\right)^{2}=\frac{1}{ay^2+by+c},"I'm trying to see if it is possible to obtain an explicit form of the following differential equation $$\left(\frac{dy}{dx}\right)^{2}=\frac{1}{ay^2+by+c}$$ where $a,b$ and $c\in\mathbb{R}$\{$0$}","I'm trying to see if it is possible to obtain an explicit form of the following differential equation $$\left(\frac{dy}{dx}\right)^{2}=\frac{1}{ay^2+by+c}$$ where $a,b$ and $c\in\mathbb{R}$\{$0$}",,"['ordinary-differential-equations', 'mathematical-physics']"
2,How to solve $(f'(x+1)+f'(x-1))f(x)-(f(x+1)+f(x-1))f'(x)=0$,How to solve,(f'(x+1)+f'(x-1))f(x)-(f(x+1)+f(x-1))f'(x)=0,$$(f'(x+1)+f'(x-1))f(x)-(f(x+1)+f(x-1))f'(x)=0$$ I don't have any ideas about the solution of this problem. How can I solve this differential equation?,$$(f'(x+1)+f'(x-1))f(x)-(f(x+1)+f(x-1))f'(x)=0$$ I don't have any ideas about the solution of this problem. How can I solve this differential equation?,,"['ordinary-differential-equations', 'functional-equations']"
3,Partial differentials vs normal differential (notation question/clarification only),Partial differentials vs normal differential (notation question/clarification only),,"In physics, it seems like the use of $\dfrac{dy}{dx}$ and $\dfrac{\partial y}{\partial x}$ are used somewhat interchangeably. My understanding is that, technically $\dfrac{dy}{dx}$ is only appropriate where $y$ only has a single variable (x), but may have many other constants. By contrast $\dfrac{\partial y}{\partial x}$ says $y$ has more than just a single $x$ variable, but for now we're holding those variables as constants and treating the $x$ as the only variable. Essentially, $\dfrac{dy}{dx}$ and $\dfrac{\partial y}{\partial x}$ should give you the same result when applied to $y=x^2+3x-2\theta+\lambda^2 $, except $\dfrac{dy}{dx}$ implies that $\theta$ and $\lambda$ are constants and always will be, while  $\dfrac{\partial y}{\partial x}$ suggests that at least one of $\lambda$ or $\theta$ is a variable. Is this understanding correct, or have I missed something?","In physics, it seems like the use of $\dfrac{dy}{dx}$ and $\dfrac{\partial y}{\partial x}$ are used somewhat interchangeably. My understanding is that, technically $\dfrac{dy}{dx}$ is only appropriate where $y$ only has a single variable (x), but may have many other constants. By contrast $\dfrac{\partial y}{\partial x}$ says $y$ has more than just a single $x$ variable, but for now we're holding those variables as constants and treating the $x$ as the only variable. Essentially, $\dfrac{dy}{dx}$ and $\dfrac{\partial y}{\partial x}$ should give you the same result when applied to $y=x^2+3x-2\theta+\lambda^2 $, except $\dfrac{dy}{dx}$ implies that $\theta$ and $\lambda$ are constants and always will be, while  $\dfrac{\partial y}{\partial x}$ suggests that at least one of $\lambda$ or $\theta$ is a variable. Is this understanding correct, or have I missed something?",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'notation']"
4,ODE with delta function,ODE with delta function,,"Consider the following ODE $$y''+a\delta (x)y+\lambda y=0$$ subject to the initial conditions $$y(\pm\pi )=0$$ (1) Show that there is a set of eigenvalues $$\tan (\pi \sqrt{\lambda })=\frac{2\sqrt{\lambda }}{a}$$ (2) Investigate if the condition $$\lambda =-u^{2}$$ where u is a positive number, is possible. I tried Laplace transform but noticed that the initial value is not satisfying. Then I tried to separate cases where x is zero or non-zero, but it didn't lead to the answer. Keep trying: So I noticed  $$\int_{-\epsilon }^{\epsilon }[y''+a\delta (x)y+\lambda y]dx=\int_{-\epsilon }^{\epsilon }y''dx+\int_{-\epsilon }^{\epsilon }a\delta (x)ydx+\int_{-\epsilon }^{\epsilon }\lambda ydx$$ $$=y'(\epsilon )-y'(-\epsilon )+ay(0)+\lambda \int_{-\epsilon }^{\epsilon } ydx$$ Then I took $$\lim_{\epsilon \rightarrow 0^{+}}$$ and obtain the following equation $$y'(0^{+})-y'(0^{-})+ay(0)=0$$ Next I tried to solve the ODE on the interval $$(-\infty ,0)\cup (0,\infty )$$ (in this way the delta function is evaluated to be zero) So the ODE becomes $$y''+\lambda y=0$$ which has the solution form$$y=c_{1}\sin \sqrt{\lambda }x+c_{2}\cos \sqrt{\lambda }x$$ But after I substitute the initial conditions $$y(\pm\pi )=0$$ I got $$c_{1}=c_{2}=0$$ I am wondering what went wrong here...","Consider the following ODE $$y''+a\delta (x)y+\lambda y=0$$ subject to the initial conditions $$y(\pm\pi )=0$$ (1) Show that there is a set of eigenvalues $$\tan (\pi \sqrt{\lambda })=\frac{2\sqrt{\lambda }}{a}$$ (2) Investigate if the condition $$\lambda =-u^{2}$$ where u is a positive number, is possible. I tried Laplace transform but noticed that the initial value is not satisfying. Then I tried to separate cases where x is zero or non-zero, but it didn't lead to the answer. Keep trying: So I noticed  $$\int_{-\epsilon }^{\epsilon }[y''+a\delta (x)y+\lambda y]dx=\int_{-\epsilon }^{\epsilon }y''dx+\int_{-\epsilon }^{\epsilon }a\delta (x)ydx+\int_{-\epsilon }^{\epsilon }\lambda ydx$$ $$=y'(\epsilon )-y'(-\epsilon )+ay(0)+\lambda \int_{-\epsilon }^{\epsilon } ydx$$ Then I took $$\lim_{\epsilon \rightarrow 0^{+}}$$ and obtain the following equation $$y'(0^{+})-y'(0^{-})+ay(0)=0$$ Next I tried to solve the ODE on the interval $$(-\infty ,0)\cup (0,\infty )$$ (in this way the delta function is evaluated to be zero) So the ODE becomes $$y''+\lambda y=0$$ which has the solution form$$y=c_{1}\sin \sqrt{\lambda }x+c_{2}\cos \sqrt{\lambda }x$$ But after I substitute the initial conditions $$y(\pm\pi )=0$$ I got $$c_{1}=c_{2}=0$$ I am wondering what went wrong here...",,['ordinary-differential-equations']
5,help with the fundamental matrix,help with the fundamental matrix,,"Let $$ y'=\begin{bmatrix} 0 & 0 & 1\\  1 & 0 & 0\\  0 & 1 &0  \end{bmatrix}y $$ Find the fundamental matrix at the point $k = 0$ , coincides with the unit matrix. I have found the fundamental matrix with the following column vectors: $$ x_1(t)=\begin{bmatrix} 1\\  1\\  1 \end{bmatrix}e^{-\frac{1}{2}t} $$ $$ x_2(t)=\begin{bmatrix} -\frac{1}{2}\cos(\frac{\sqrt{3}}{2}t)+\frac{\sqrt{3}}{2}\sin(\frac{\sqrt{3}}{2}t)\\ -\frac{1}{2}\cos(\frac{\sqrt{3}}{2}t)-\frac{\sqrt{3}}{2}\sin(\frac{\sqrt{3}}{2}t)\\  \cos(\frac{\sqrt{3}}{2}t) \end{bmatrix}e^{-\frac{1}{2}t} $$ $$ x_3(t)=\begin{bmatrix} -\frac{1}{2}\sin(\frac{\sqrt{3}}{2}t)-\frac{\sqrt{3}}{2}\cos(\frac{\sqrt{3}}{2}t)\\ -\frac{1}{2}\sin(\frac{\sqrt{3}}{2}t)+\frac{\sqrt{3}}{2}\cos(\frac{\sqrt{3}}{2}t)\\  \sin(\frac{\sqrt{3}}{2}t) \end{bmatrix}e^{-\frac{1}{2}t} $$ but I do not understand the part that says that at point $0$ coincides with the unit matrix. Can you help me please?","Let Find the fundamental matrix at the point , coincides with the unit matrix. I have found the fundamental matrix with the following column vectors: but I do not understand the part that says that at point coincides with the unit matrix. Can you help me please?","
y'=\begin{bmatrix}
0 & 0 & 1\\ 
1 & 0 & 0\\ 
0 & 1 &0 
\end{bmatrix}y
 k = 0 
x_1(t)=\begin{bmatrix}
1\\ 
1\\ 
1
\end{bmatrix}e^{-\frac{1}{2}t}
 
x_2(t)=\begin{bmatrix}
-\frac{1}{2}\cos(\frac{\sqrt{3}}{2}t)+\frac{\sqrt{3}}{2}\sin(\frac{\sqrt{3}}{2}t)\\
-\frac{1}{2}\cos(\frac{\sqrt{3}}{2}t)-\frac{\sqrt{3}}{2}\sin(\frac{\sqrt{3}}{2}t)\\ 
\cos(\frac{\sqrt{3}}{2}t)
\end{bmatrix}e^{-\frac{1}{2}t}
 
x_3(t)=\begin{bmatrix}
-\frac{1}{2}\sin(\frac{\sqrt{3}}{2}t)-\frac{\sqrt{3}}{2}\cos(\frac{\sqrt{3}}{2}t)\\
-\frac{1}{2}\sin(\frac{\sqrt{3}}{2}t)+\frac{\sqrt{3}}{2}\cos(\frac{\sqrt{3}}{2}t)\\ 
\sin(\frac{\sqrt{3}}{2}t)
\end{bmatrix}e^{-\frac{1}{2}t}
 0",['ordinary-differential-equations']
6,Inhomogeneous diffusion equation on the half plane,Inhomogeneous diffusion equation on the half plane,,"Consider one-dimensional linear PDE of the form $$ {\rm u}_{xx}\left(x,t\right) -  {\rm u}_{t}\left(x,t\right) = {\rm f}\left(x,t\right) \quad \mbox{in the domain}\quad \left\{x > 0\,,\ t <  \infty\right\}$$ Boundary conditions: ${\rm u}\left(0,t\right)=0\,,\quad{\rm u}\left(x,0\right)=0\ \quad{\rm u}\left(x,t\right) = 0$ at  x→∞. I tried so solve the equation by using Green function G(x,t): $$ G_{xx}-G_{t}=\delta(x-x_0)\delta(t-t_0)$$ Where $$ G(x,0)=G(0,t)=0 \quad and \quad G→0 \quad at \quad x→∞$$ By using Laplace transform we get: $$ g(x,p)=\int_0^\infty e^{-pt}G(x,t)dt\, $$ and we get a newODE of the form: $$ d^2g/dx^2-pg=\delta(x-x_0)$$ I don't know how to solve this equation for x>0. The Internet is full of solutions for this equation for the whole plane but not for half of the plane (x>0). Please help me if you have any idea how to solve this equation. Thank you!","Consider one-dimensional linear PDE of the form $$ {\rm u}_{xx}\left(x,t\right) -  {\rm u}_{t}\left(x,t\right) = {\rm f}\left(x,t\right) \quad \mbox{in the domain}\quad \left\{x > 0\,,\ t <  \infty\right\}$$ Boundary conditions: ${\rm u}\left(0,t\right)=0\,,\quad{\rm u}\left(x,0\right)=0\ \quad{\rm u}\left(x,t\right) = 0$ at  x→∞. I tried so solve the equation by using Green function G(x,t): $$ G_{xx}-G_{t}=\delta(x-x_0)\delta(t-t_0)$$ Where $$ G(x,0)=G(0,t)=0 \quad and \quad G→0 \quad at \quad x→∞$$ By using Laplace transform we get: $$ g(x,p)=\int_0^\infty e^{-pt}G(x,t)dt\, $$ and we get a newODE of the form: $$ d^2g/dx^2-pg=\delta(x-x_0)$$ I don't know how to solve this equation for x>0. The Internet is full of solutions for this equation for the whole plane but not for half of the plane (x>0). Please help me if you have any idea how to solve this equation. Thank you!",,"['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform']"
7,Prove $0$ is an exponentially stable equilibrium of the system $x'=f(x)+g(x)$ if $f(0)=g(0)=0$,Prove  is an exponentially stable equilibrium of the system  if,0 x'=f(x)+g(x) f(0)=g(0)=0,"Besides the conditions in the title, we have: $0$ is an exponential equilibrium of the system $y'=f(y)$ $|g(x)|\leq \mu|x|,\forall x \in \mathbb{R}^n$ $\mu$ is sufficiently small! What I have tried so far is that: $||y(0)||<\delta_1\Rightarrow ||y(t)||<\alpha_1||y(0)||e^{-\beta_1 t},t\geq0$ We want to prove: $\exists \delta,\alpha,\beta>0,$ such that $||x(0)||<\delta\Rightarrow ||x(t)||<\alpha||x(0)||e^{-\beta t},t\geq0$ Considering the following system: $x_1'(t)=f(x_1)$ $x_2'(t)=g(x_2)$ Then we will have: $\exists \alpha_1 ,\beta_1, \delta>0$ such that $||x_1(0)||<\delta_1\Rightarrow ||x_1(t)||<\alpha_1||x_1(0)||e^{-\beta_1 t},t\geq0$ $x_2'(t)\leq \mu|x_2(t)|$, and by Gronwall inequality $x_2(t)\leq x_2(0)e^{\mu t}$ I have a sense that it might be somewhat close to what we want to get. And I am stuck at this stage. I would really appreciate any suggestion to proceed.","Besides the conditions in the title, we have: $0$ is an exponential equilibrium of the system $y'=f(y)$ $|g(x)|\leq \mu|x|,\forall x \in \mathbb{R}^n$ $\mu$ is sufficiently small! What I have tried so far is that: $||y(0)||<\delta_1\Rightarrow ||y(t)||<\alpha_1||y(0)||e^{-\beta_1 t},t\geq0$ We want to prove: $\exists \delta,\alpha,\beta>0,$ such that $||x(0)||<\delta\Rightarrow ||x(t)||<\alpha||x(0)||e^{-\beta t},t\geq0$ Considering the following system: $x_1'(t)=f(x_1)$ $x_2'(t)=g(x_2)$ Then we will have: $\exists \alpha_1 ,\beta_1, \delta>0$ such that $||x_1(0)||<\delta_1\Rightarrow ||x_1(t)||<\alpha_1||x_1(0)||e^{-\beta_1 t},t\geq0$ $x_2'(t)\leq \mu|x_2(t)|$, and by Gronwall inequality $x_2(t)\leq x_2(0)e^{\mu t}$ I have a sense that it might be somewhat close to what we want to get. And I am stuck at this stage. I would really appreciate any suggestion to proceed.",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
8,Show the system of differential equations has the solution of the form $ \phi(x)=e^{kx}\alpha$,Show the system of differential equations has the solution of the form, \phi(x)=e^{kx}\alpha,"Consider the following linear system   $$y_1 '=ay_1+by_2$$$$y_2 '=cy_1+dy_2$$   where $a,b,c,d$ are constants. Then show that this system always has a solution $\phi(x)=e^{kx}\alpha,$ where $\alpha=(\alpha_1,\alpha_2)\ne(0,0)$ is a constant vector and $k$ is a constant. My approach was to suppose $\phi(x)$ to be a solution and define the function vector $\psi(x)=e^{-kx} \phi(x)$. And show that $\psi'(x)=(0,0)=\mathbf 0$ by using the given system and the fact that $\phi(x)$ is the solution. However, I did get a trouble in showing that $\psi'(x)=(0,0)=\mathbf 0$. Could anyone give me a hand on this part? OR does anyone have a simpler way to show this? Thanks in advance.","Consider the following linear system   $$y_1 '=ay_1+by_2$$$$y_2 '=cy_1+dy_2$$   where $a,b,c,d$ are constants. Then show that this system always has a solution $\phi(x)=e^{kx}\alpha,$ where $\alpha=(\alpha_1,\alpha_2)\ne(0,0)$ is a constant vector and $k$ is a constant. My approach was to suppose $\phi(x)$ to be a solution and define the function vector $\psi(x)=e^{-kx} \phi(x)$. And show that $\psi'(x)=(0,0)=\mathbf 0$ by using the given system and the fact that $\phi(x)$ is the solution. However, I did get a trouble in showing that $\psi'(x)=(0,0)=\mathbf 0$. Could anyone give me a hand on this part? OR does anyone have a simpler way to show this? Thanks in advance.",,['ordinary-differential-equations']
9,Boundary Value Problem with Robin condition,Boundary Value Problem with Robin condition,,"How to solve the problem: $\left(3\right)$  \begin{cases} u_{tt}-a^{2}u_{xx}=f\left(x,t\right)\\ u_{x}\left(0,t\right)-h_{0}u\left(0,t\right)=g_{0}\left(t\right)\\ u_{x}\left(L,t\right)-h_{L}u\left(L,t\right)=g_{L}\left(t\right)\\ u\left(x,0\right)=u_{0}\left(x\right) & u_{t}\left(x,0\right)=u_{1}\left(x\right) \end{cases} My opinion is solve the two following problems before solve $\left(3\right)$ $\left(1\right)$ \begin{cases} u_{tt}-a^{2}u_{xx}=0 & 0<x<L,t>0\\ u_{x}\left(0,t\right)-h_{0}u\left(0,t\right)=u_{x}\left(L,0\right)+h_{L}u\left(L,t\right)=0\\ u\left(x,0\right)=u_{0}\left(x\right) & u_{t}\left(x,0\right)=u_{1}\left(x\right) \end{cases} $\left(2\right)$ \begin{cases} u_{tt}-a^{2}u_{xx}=f\left(x,t\right)\\ u_{x}\left(0,t\right)-h_{0}u\left(0,t\right)=u_{x}\left(L,0\right)+h_{L}u\left(L,t\right)=0\\ u\left(x,0\right)=0 & u_{t}\left(x,0\right)=0 \end{cases}","How to solve the problem: $\left(3\right)$  \begin{cases} u_{tt}-a^{2}u_{xx}=f\left(x,t\right)\\ u_{x}\left(0,t\right)-h_{0}u\left(0,t\right)=g_{0}\left(t\right)\\ u_{x}\left(L,t\right)-h_{L}u\left(L,t\right)=g_{L}\left(t\right)\\ u\left(x,0\right)=u_{0}\left(x\right) & u_{t}\left(x,0\right)=u_{1}\left(x\right) \end{cases} My opinion is solve the two following problems before solve $\left(3\right)$ $\left(1\right)$ \begin{cases} u_{tt}-a^{2}u_{xx}=0 & 0<x<L,t>0\\ u_{x}\left(0,t\right)-h_{0}u\left(0,t\right)=u_{x}\left(L,0\right)+h_{L}u\left(L,t\right)=0\\ u\left(x,0\right)=u_{0}\left(x\right) & u_{t}\left(x,0\right)=u_{1}\left(x\right) \end{cases} $\left(2\right)$ \begin{cases} u_{tt}-a^{2}u_{xx}=f\left(x,t\right)\\ u_{x}\left(0,t\right)-h_{0}u\left(0,t\right)=u_{x}\left(L,0\right)+h_{L}u\left(L,t\right)=0\\ u\left(x,0\right)=0 & u_{t}\left(x,0\right)=0 \end{cases}",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics']"
10,Power series method to solve Airy’s differential equation [duplicate],Power series method to solve Airy’s differential equation [duplicate],,"This question already has answers here : Solution of $y''+xy=0$ (3 answers) Closed 9 years ago . Using power series method, solve Airy’s equation $$y′′+ xy = 0.$$ How do I start solving this? Thanks in advance!","This question already has answers here : Solution of $y''+xy=0$ (3 answers) Closed 9 years ago . Using power series method, solve Airy’s equation $$y′′+ xy = 0.$$ How do I start solving this? Thanks in advance!",,['ordinary-differential-equations']
11,"Definition of weak solution in $W^{1,2}(\Omega)$.",Definition of weak solution in .,"W^{1,2}(\Omega)","I have a problem: For $\Omega$ be a bounded domain in $\Bbb R^n$. We consider   $$\left\{\begin{matrix} \Delta u-\lambda u =f  \ \rm in \ \Omega & \\    u\mid_{\partial {\Omega}} =0 \end{matrix}\right. \tag I$$ a/ Definition of classical solution ; b/ Definition of weak solution in the $W^{1,2}(\Omega)$ space. Finding the conditions so that weak solution becomes classical solution . c/ Prove that if $\lambda=0$ then $(I)$ has a unique solution in $W^{1,2}(\Omega)$; d/ Determine the existence and uniqueness of solution of $(I)$ by $\lambda$. ============================================== I'm not sure but I still write it here: *) For a): Definition of classical solution Let $f \in C^{0}(\Omega)$. A function $u \in C^{2}(\Omega) \cap C^0(\overline{\Omega})$ such that $(I)$ forall $x \in \overline{\Omega}$. *) For b): Definition of weak solution in the $W^{1,2}(\Omega)$ space Let $f \in H^{-1}(\Omega)=(H_{0}^{1}(\Omega))^*$. A function $u$ such that: $$\Delta u - \lambda u=f  \ \rm in \ \Omega \iff \left \langle \Delta u - \lambda u,\varphi  \right \rangle=\left \langle f,\varphi  \right \rangle \  \text{for any test function} \ \varphi \in H_{0}^{1}(\Omega)$$ Finding the conditions so that weak solution becomes classical solution : For $f \in H^k(\Omega)$ and $\partial \Omega \subset C^{k+2}(\Omega)\implies u \in H^{k+2}(\Omega)\cap H_{0}^{1}(\Omega)$. If $k+2>\dfrac{n}{2}+2 \implies k>\dfrac{n}{2}$. By applying Sobolev embedding theorem , we have $u \in C^2(\overline{\Omega})$. Because $$H^{k+2}(\Omega)\overset{\rm embedding}{\rightarrow} C^2(\overline{\Omega})$$... =============================================== Now, I have stuch when I go on...Can anyone help me! Any help will be appreciated! Thanks!","I have a problem: For $\Omega$ be a bounded domain in $\Bbb R^n$. We consider   $$\left\{\begin{matrix} \Delta u-\lambda u =f  \ \rm in \ \Omega & \\    u\mid_{\partial {\Omega}} =0 \end{matrix}\right. \tag I$$ a/ Definition of classical solution ; b/ Definition of weak solution in the $W^{1,2}(\Omega)$ space. Finding the conditions so that weak solution becomes classical solution . c/ Prove that if $\lambda=0$ then $(I)$ has a unique solution in $W^{1,2}(\Omega)$; d/ Determine the existence and uniqueness of solution of $(I)$ by $\lambda$. ============================================== I'm not sure but I still write it here: *) For a): Definition of classical solution Let $f \in C^{0}(\Omega)$. A function $u \in C^{2}(\Omega) \cap C^0(\overline{\Omega})$ such that $(I)$ forall $x \in \overline{\Omega}$. *) For b): Definition of weak solution in the $W^{1,2}(\Omega)$ space Let $f \in H^{-1}(\Omega)=(H_{0}^{1}(\Omega))^*$. A function $u$ such that: $$\Delta u - \lambda u=f  \ \rm in \ \Omega \iff \left \langle \Delta u - \lambda u,\varphi  \right \rangle=\left \langle f,\varphi  \right \rangle \  \text{for any test function} \ \varphi \in H_{0}^{1}(\Omega)$$ Finding the conditions so that weak solution becomes classical solution : For $f \in H^k(\Omega)$ and $\partial \Omega \subset C^{k+2}(\Omega)\implies u \in H^{k+2}(\Omega)\cap H_{0}^{1}(\Omega)$. If $k+2>\dfrac{n}{2}+2 \implies k>\dfrac{n}{2}$. By applying Sobolev embedding theorem , we have $u \in C^2(\overline{\Omega})$. Because $$H^{k+2}(\Omega)\overset{\rm embedding}{\rightarrow} C^2(\overline{\Omega})$$... =============================================== Now, I have stuch when I go on...Can anyone help me! Any help will be appreciated! Thanks!",,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'sobolev-spaces']"
12,Differential equation with infinitely many solutions,Differential equation with infinitely many solutions,,"The problem is to solve for $-1<x<1$ $$y'(x)=\frac{4x^3y(x)}{x^2+y(x)^2}$$ with $y(0)=0$. I need to show that this equation has infinitely many solutions. Note that $\frac{4y(x)x^3}{x^2+y(x)^2}$ is undefined for $y(0)=0$, but note that $\frac{4x^3y(x)}{x^2+y(x)^2}=2x^2(\frac{2xy(x)}{x^2+y(x)^2})=O(x^2)$ since $|\frac{2xy(x)}{x^2+y(x)^2}|\le 1$. I do not know how solve the problem with an explicit formula. Can anyone point out how to prove that this problem has infinitely many solutions?","The problem is to solve for $-1<x<1$ $$y'(x)=\frac{4x^3y(x)}{x^2+y(x)^2}$$ with $y(0)=0$. I need to show that this equation has infinitely many solutions. Note that $\frac{4y(x)x^3}{x^2+y(x)^2}$ is undefined for $y(0)=0$, but note that $\frac{4x^3y(x)}{x^2+y(x)^2}=2x^2(\frac{2xy(x)}{x^2+y(x)^2})=O(x^2)$ since $|\frac{2xy(x)}{x^2+y(x)^2}|\le 1$. I do not know how solve the problem with an explicit formula. Can anyone point out how to prove that this problem has infinitely many solutions?",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
13,How do you solve $y''+\frac{x}{2}y'+y=0$ a 2nd order homogenous equation?,How do you solve  a 2nd order homogenous equation?,y''+\frac{x}{2}y'+y=0,How do you solve $y''+\frac{x}{2}y'+y=0$ where $y'=\frac{\partial y}{\partial x}$ What is confusing me is that once I say $g=y'$ I'm left with : $g'+\frac{x}{2}g+y=0$ I can't use seperation by parts here. How would one proceed here? Thank you. Best,How do you solve $y''+\frac{x}{2}y'+y=0$ where $y'=\frac{\partial y}{\partial x}$ What is confusing me is that once I say $g=y'$ I'm left with : $g'+\frac{x}{2}g+y=0$ I can't use seperation by parts here. How would one proceed here? Thank you. Best,,"['ordinary-differential-equations', 'differential']"
14,Domain of a solution of a differential equation,Domain of a solution of a differential equation,,"let $(x(t), y(t))$ be a solution to the system $x'=y-x^3, y'=-x-y^3$. I want to prove that $(x(t), y(t))$ is bounded for $t>0$ What I did: from the equations comes $0\ge -(x^4+y^4)=x'x+y'y=\frac d{dt}(\frac{x^2+y^2}2)$, so $\frac{x^2+y^2}2$ decreases. So $\sqrt{x^2+y^2}$ also decreases. this means that $||(x,y)||$ decreases and so $||(x(t),y(t))||\leq ||(x(0),y(0))||$ for all $t>0$. the above proves what I want, but why should $(x(t), y(t))$ be defined at $t=0$?","let $(x(t), y(t))$ be a solution to the system $x'=y-x^3, y'=-x-y^3$. I want to prove that $(x(t), y(t))$ is bounded for $t>0$ What I did: from the equations comes $0\ge -(x^4+y^4)=x'x+y'y=\frac d{dt}(\frac{x^2+y^2}2)$, so $\frac{x^2+y^2}2$ decreases. So $\sqrt{x^2+y^2}$ also decreases. this means that $||(x,y)||$ decreases and so $||(x(t),y(t))||\leq ||(x(0),y(0))||$ for all $t>0$. the above proves what I want, but why should $(x(t), y(t))$ be defined at $t=0$?",,['ordinary-differential-equations']
15,Picard's Theorem (Lipschitz) Problem.,Picard's Theorem (Lipschitz) Problem.,,"I have the following question: Determine if the function $F(x,y) = xy^{1/3}$ satisfy a Lipschitz condition on the rectangle $\{ (x,y) : |x| \leq h, |y| \leq k \}$ where $h > 0$ and $k > 0$? If $b>0$ determine the region $|x| < h, |y|<k$ which has the largest value of $h$ in which Picard's theorem can be used to show that the initial-value problem $y' = xy^{1/3}$, $y(0) = b$ has a unique solution (you may assume Picard's theorem, but should prove that the assumptions are satisfied) and find the solution explicitly. If $b=0$ show that for any $c > 0$ there is a solution $y$ which is identically zero on $[-c,c]$ and positive when $|x| > c$. Find these solutions explicitly and show that the resulting solutions satisfy the ODE for all values of $x$. In trying to determine whether or not this satisfies the Lipschitz condition or not I've done the following: $|F(x,u) - F(x,v)| = |xu^{1/3} - xv^{1/3}| = |x||u^{1/3} - v^{1/3}| \leq h|u^{1/3} - v^{1/3}|$ But get stuck at this point as $h|u^{1/3} - v^{1/3}| \leq h|u-v|$ if and only if $u,v \geq 1$ - so I'm thinking this doesn't satisfy the Lipschitz condition but can't really formulate it. For the two conditions of Picard's theorem to be satisfied, we must have: $F(x,y)$ is continuous in $R$, where $R$ is the rectangle: $|x|<h$, $|y-b|<k$ and that $F$ is bounded by $M$ so $|F(x,y)| \leq M$ and $Mh \leq k$ The second condition being that $F$ satisfies a Lipschitz condition in $R$. Beyond this I'm unsure what to do, thanks. EDIT: Sorry, to include the version of Picard's theorem I'm using: The ODE $y' = f(x,y)$ with $y(a) = b$ has a solution in the rectangle $R: |x-a| \leq h, |y-b| \leq k$ provided: (i) $f$ is continuous in $R$, bounded by $M$ (so $|f(x,y)| \leq M$) and $Mh \leq k$ (ii) $f$ satisfies a Lipschitz condition in $R$. Furthermore, this solution is unique.","I have the following question: Determine if the function $F(x,y) = xy^{1/3}$ satisfy a Lipschitz condition on the rectangle $\{ (x,y) : |x| \leq h, |y| \leq k \}$ where $h > 0$ and $k > 0$? If $b>0$ determine the region $|x| < h, |y|<k$ which has the largest value of $h$ in which Picard's theorem can be used to show that the initial-value problem $y' = xy^{1/3}$, $y(0) = b$ has a unique solution (you may assume Picard's theorem, but should prove that the assumptions are satisfied) and find the solution explicitly. If $b=0$ show that for any $c > 0$ there is a solution $y$ which is identically zero on $[-c,c]$ and positive when $|x| > c$. Find these solutions explicitly and show that the resulting solutions satisfy the ODE for all values of $x$. In trying to determine whether or not this satisfies the Lipschitz condition or not I've done the following: $|F(x,u) - F(x,v)| = |xu^{1/3} - xv^{1/3}| = |x||u^{1/3} - v^{1/3}| \leq h|u^{1/3} - v^{1/3}|$ But get stuck at this point as $h|u^{1/3} - v^{1/3}| \leq h|u-v|$ if and only if $u,v \geq 1$ - so I'm thinking this doesn't satisfy the Lipschitz condition but can't really formulate it. For the two conditions of Picard's theorem to be satisfied, we must have: $F(x,y)$ is continuous in $R$, where $R$ is the rectangle: $|x|<h$, $|y-b|<k$ and that $F$ is bounded by $M$ so $|F(x,y)| \leq M$ and $Mh \leq k$ The second condition being that $F$ satisfies a Lipschitz condition in $R$. Beyond this I'm unsure what to do, thanks. EDIT: Sorry, to include the version of Picard's theorem I'm using: The ODE $y' = f(x,y)$ with $y(a) = b$ has a solution in the rectangle $R: |x-a| \leq h, |y-b| \leq k$ provided: (i) $f$ is continuous in $R$, bounded by $M$ (so $|f(x,y)| \leq M$) and $Mh \leq k$ (ii) $f$ satisfies a Lipschitz condition in $R$. Furthermore, this solution is unique.",,['ordinary-differential-equations']
16,Constructing a differential equation for hyperbolic crochet,Constructing a differential equation for hyperbolic crochet,,"There is plenty of information about hyperbolic geometry and its melding with crochet, however I have yet to find an exact equation for determining the number of stitches in each row. I will try to ask my question without including crochet terms. Let's say you have a row with a certain number of stitches, fox example 6. For every 2 stitches, you add 3 stitches on top. So the second row will have 9 stitches, and the third row will have 13.5 (ignore the decimals; in crochet you can't have half a stitch), then 20.25, 30.375, etc. Each row will be 1.5 longer than the previous. The circumference grows exponentially. Heuristically (plotting and adding a trendline in Excel) I know the equation to be s(r) = 4*Exp(0.405*r). 4 comes from 6/1.5, or initial stitches divided by rate. 0.405 is a constant of integration, solved using s(2) = 9. So I know I have to construct a differential equation, but the only form I can think of is s'(r) = 6*1.5*s(r). The change of stitches equals the number of initial stitches times the rate of increase times the current number of stitches of the previous row. How does the constant of integration find itself in the exponent when most solutions have the form s(r) = A*Exp(r)? Thanks for your help! edit: Changed 4 = 6*1.5 to 4 = 6/1.5","There is plenty of information about hyperbolic geometry and its melding with crochet, however I have yet to find an exact equation for determining the number of stitches in each row. I will try to ask my question without including crochet terms. Let's say you have a row with a certain number of stitches, fox example 6. For every 2 stitches, you add 3 stitches on top. So the second row will have 9 stitches, and the third row will have 13.5 (ignore the decimals; in crochet you can't have half a stitch), then 20.25, 30.375, etc. Each row will be 1.5 longer than the previous. The circumference grows exponentially. Heuristically (plotting and adding a trendline in Excel) I know the equation to be s(r) = 4*Exp(0.405*r). 4 comes from 6/1.5, or initial stitches divided by rate. 0.405 is a constant of integration, solved using s(2) = 9. So I know I have to construct a differential equation, but the only form I can think of is s'(r) = 6*1.5*s(r). The change of stitches equals the number of initial stitches times the rate of increase times the current number of stitches of the previous row. How does the constant of integration find itself in the exponent when most solutions have the form s(r) = A*Exp(r)? Thanks for your help! edit: Changed 4 = 6*1.5 to 4 = 6/1.5",,"['ordinary-differential-equations', 'exponential-function', 'hyperbolic-geometry']"
17,Solution of $y'' + ay^3 = 0$,Solution of,y'' + ay^3 = 0,"I am looking for the solution of this non-linear differential equation : $y'' + a y^3=0$ where $a$ is positive ($a>0$), and $y=y(t)$. If necessary, one could provide initial conditions $y(0) =0, y'(0)=b$","I am looking for the solution of this non-linear differential equation : $y'' + a y^3=0$ where $a$ is positive ($a>0$), and $y=y(t)$. If necessary, one could provide initial conditions $y(0) =0, y'(0)=b$",,['ordinary-differential-equations']
18,Differential Equation and Bernoulli,Differential Equation and Bernoulli,,"I'm working on the following question with initial condition (x,u)=(2,1) : $$ \frac{du}{dx} = u(2-u)$$ This appears to be a Bernoulli form and substitution v=1/u is needed. $$ v=1/u $$ $$ v' = \frac{-1}{u^2} \frac{du}{dx} $$ $$ v' = \frac{-1}{u^2} u(2-u) $$ $$ v' = \frac{-1}{u}(2-u) $$ $$ v' = \frac{-2}{u}+1  $$ $$ v' + \frac{2}{u} = 1  $$ $$ v' + 2v = 1  $$ Is this the correct away to approach this problem? It now looks like it is in standard linear form and can be solved by taking the integrating factor. However, I'm not sure what v' represents, $ \frac{dv}{du} $ OR $ \frac{dv}{dx} $? How do I get the solution in u(x)=... form?","I'm working on the following question with initial condition (x,u)=(2,1) : $$ \frac{du}{dx} = u(2-u)$$ This appears to be a Bernoulli form and substitution v=1/u is needed. $$ v=1/u $$ $$ v' = \frac{-1}{u^2} \frac{du}{dx} $$ $$ v' = \frac{-1}{u^2} u(2-u) $$ $$ v' = \frac{-1}{u}(2-u) $$ $$ v' = \frac{-2}{u}+1  $$ $$ v' + \frac{2}{u} = 1  $$ $$ v' + 2v = 1  $$ Is this the correct away to approach this problem? It now looks like it is in standard linear form and can be solved by taking the integrating factor. However, I'm not sure what v' represents, $ \frac{dv}{du} $ OR $ \frac{dv}{dx} $? How do I get the solution in u(x)=... form?",,"['calculus', 'ordinary-differential-equations', 'differential']"
19,Construct a Liapunov function for this system,Construct a Liapunov function for this system,,"Construct a Liapunov function for the system ( Determine the stability of $x \equiv 0$ ): I have an example:$$\begin{cases}  & \mathrm { } \dot{x}= -x^3+xy^2\\   & \mathrm { } \dot{y}= -2x^2y-y^3 \end{cases}  \tag{1}$$ Here's my solution: Let's try $V(x,y)=ax^2+by^2$. Then we have: $\dot{V}(x,y)=-2ax^4+2(a-2b)x^2y^2-2by^4$ When $a-2b<0$, for instance $a=b=1$. We have $V(x,y)=x^2+y^2$ such that:  $$V(0,0)=0,V(x,y)>0, \forall (x,y) \ne (0,0)\ \text{and} \ \dot{V}(x,y)<0$$ Hence, $x=y=0$ is asymptotically stable . ============================================================ What about the system: $$\begin{cases}  & \mathrm{  } \dot{x}= y-3x^3\\   & \mathrm{  } \dot{y}= -x-7y^3 \end{cases} \tag{2}$$ How can we construct a Liapunov function for this system (Determine the stability of $x \equiv 0$ of the system). I'm sorry I fixed it!","Construct a Liapunov function for the system ( Determine the stability of $x \equiv 0$ ): I have an example:$$\begin{cases}  & \mathrm { } \dot{x}= -x^3+xy^2\\   & \mathrm { } \dot{y}= -2x^2y-y^3 \end{cases}  \tag{1}$$ Here's my solution: Let's try $V(x,y)=ax^2+by^2$. Then we have: $\dot{V}(x,y)=-2ax^4+2(a-2b)x^2y^2-2by^4$ When $a-2b<0$, for instance $a=b=1$. We have $V(x,y)=x^2+y^2$ such that:  $$V(0,0)=0,V(x,y)>0, \forall (x,y) \ne (0,0)\ \text{and} \ \dot{V}(x,y)<0$$ Hence, $x=y=0$ is asymptotically stable . ============================================================ What about the system: $$\begin{cases}  & \mathrm{  } \dot{x}= y-3x^3\\   & \mathrm{  } \dot{y}= -x-7y^3 \end{cases} \tag{2}$$ How can we construct a Liapunov function for this system (Determine the stability of $x \equiv 0$ of the system). I'm sorry I fixed it!",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
20,Legendre Equation Properties,Legendre Equation Properties,,"Is there a nice way to derive, starting from the Legendre differential equation, the generating function, the recurrence relation, the Rodrigues differential form & the Schlafli integral form without memorizing crazy generating functions or stuff that magically works? Every time I try to study this stuff I'm given formula's that are supposed to work, how do you derive all this from scratch?","Is there a nice way to derive, starting from the Legendre differential equation, the generating function, the recurrence relation, the Rodrigues differential form & the Schlafli integral form without memorizing crazy generating functions or stuff that magically works? Every time I try to study this stuff I'm given formula's that are supposed to work, how do you derive all this from scratch?",,['ordinary-differential-equations']
21,How to proceed solving this problem?,How to proceed solving this problem?,,"I'm working on the problem ""soundwaves under the water"" (page 16 in the document is in English) from a numerical analysis book. I've got the following problem that is taken from the numerical analysis book by Kahaner-Moler-Nash (P8-15). I've made the plot of the data point and the approximation model function: z=[0:500:4000 5000:1000:12000]; data=[5050 4980 4930 4890 4870 4865 4860 4860 4865 4875 4885 4905 4920 4935 4950 4970 4990]; fun=@(p)(4800 + p(1))*ones(size(z)) +p(2)/1000*z+p(3)*exp(p(4)/1000*z)-data; x0=[0 0 0 -1]; opt = optimset('MaxFunEvals',1000); p=lsqnonlin(fun,x0,[],[],opt); fitf=@(t)(4800 + p(1))*ones(size(t)) + p(2)/1000*t+ p(3)*exp(p(4)/1000*t); tt=linspace(0,12000,1000); plot(z,data,'r-',tt,fitf(tt),'b-'); Since the sound speed varies with depth, sound rays will travel   in curved paths. A ﬁxed underwater point emits rays in all directions.   Given a particular point and initial direction we would like to follow   the ray path. Thus letting x be the horizontal coordinate we know the   initial values: x = 0, z = z0, dz=dx = tan 0, where 0 denotes the   angle between the horizontal line z = z0 and the ray in the start   point. The ray path z(x) is described by the following second order   diﬀerential equation $\displaystyle\frac{d^2z}{dx^2}= \frac{-q_0c'(z)}{c(z)^3}$ where $\displaystyle q_0 = \left(\frac{c(z_0)}{\cos b_0}\right)^2$ Use the Runge-Kutta method (or ode45) to trace the ray beginning at z0   = 2000 feet and b0 = 7.8 degrees. Follow the ray for 25 nautical miles (1 nautical mile is 6076 feet). Plot the curve z(x). You should   ﬁnd that the depth at xf = 25 nautical miles is close to 2500 feet. Now suppose that a sound source at a depth of 2000 feet transmits to a   receiver 25 miles away at a depth of 2500 feet. The above calculation   shows that one of the rays from the source to the receiver leaves the   source at an angle close to 7.8 degrees. Because of the nonlinearity   of the equation there may be other rays leaving at diﬀerent angles   that reach the same receiver. Run your program for b0 in the range   from 10 up to 14 degrees, plot the ray paths and print a table of the   values z(xf). We are interested in ﬁnding values of 0 for which z(xf) = 2500. Use   an eﬃcient algorithm to determine the rays which pass through the   receiver. Discuss the accuracy of your results. So how should I proceed with solving this? I should study the methods, I think I know RK4 already in theory but I dont' know how to do it in matlab and the non-linear least squares is done above. Can you help me?","I'm working on the problem ""soundwaves under the water"" (page 16 in the document is in English) from a numerical analysis book. I've got the following problem that is taken from the numerical analysis book by Kahaner-Moler-Nash (P8-15). I've made the plot of the data point and the approximation model function: z=[0:500:4000 5000:1000:12000]; data=[5050 4980 4930 4890 4870 4865 4860 4860 4865 4875 4885 4905 4920 4935 4950 4970 4990]; fun=@(p)(4800 + p(1))*ones(size(z)) +p(2)/1000*z+p(3)*exp(p(4)/1000*z)-data; x0=[0 0 0 -1]; opt = optimset('MaxFunEvals',1000); p=lsqnonlin(fun,x0,[],[],opt); fitf=@(t)(4800 + p(1))*ones(size(t)) + p(2)/1000*t+ p(3)*exp(p(4)/1000*t); tt=linspace(0,12000,1000); plot(z,data,'r-',tt,fitf(tt),'b-'); Since the sound speed varies with depth, sound rays will travel   in curved paths. A ﬁxed underwater point emits rays in all directions.   Given a particular point and initial direction we would like to follow   the ray path. Thus letting x be the horizontal coordinate we know the   initial values: x = 0, z = z0, dz=dx = tan 0, where 0 denotes the   angle between the horizontal line z = z0 and the ray in the start   point. The ray path z(x) is described by the following second order   diﬀerential equation $\displaystyle\frac{d^2z}{dx^2}= \frac{-q_0c'(z)}{c(z)^3}$ where $\displaystyle q_0 = \left(\frac{c(z_0)}{\cos b_0}\right)^2$ Use the Runge-Kutta method (or ode45) to trace the ray beginning at z0   = 2000 feet and b0 = 7.8 degrees. Follow the ray for 25 nautical miles (1 nautical mile is 6076 feet). Plot the curve z(x). You should   ﬁnd that the depth at xf = 25 nautical miles is close to 2500 feet. Now suppose that a sound source at a depth of 2000 feet transmits to a   receiver 25 miles away at a depth of 2500 feet. The above calculation   shows that one of the rays from the source to the receiver leaves the   source at an angle close to 7.8 degrees. Because of the nonlinearity   of the equation there may be other rays leaving at diﬀerent angles   that reach the same receiver. Run your program for b0 in the range   from 10 up to 14 degrees, plot the ray paths and print a table of the   values z(xf). We are interested in ﬁnding values of 0 for which z(xf) = 2500. Use   an eﬃcient algorithm to determine the rays which pass through the   receiver. Discuss the accuracy of your results. So how should I proceed with solving this? I should study the methods, I think I know RK4 already in theory but I dont' know how to do it in matlab and the non-linear least squares is done above. Can you help me?",,"['ordinary-differential-equations', 'numerical-methods', 'matlab']"
22,Square root of a stochastic process,Square root of a stochastic process,,"I need help with the following problem. How can I derive $d\sqrt v$ using Ito's lemma for the following process: $$d\sqrt v=(\alpha−\beta \sqrt v)dt+\delta dX$$ The parameters $\alpha, \beta, \delta$ are constant. Using Itô's lemma show that $$dv = (\delta^2 + 2\alpha\sqrt v − 2\beta v)dt + 2\delta \sqrt vdX$$ any ideas?",I need help with the following problem. How can I derive using Ito's lemma for the following process: The parameters are constant. Using Itô's lemma show that any ideas?,"d\sqrt v d\sqrt v=(\alpha−\beta \sqrt v)dt+\delta dX \alpha, \beta, \delta dv = (\delta^2 + 2\alpha\sqrt v − 2\beta v)dt + 2\delta \sqrt vdX","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus']"
23,Show the solution of this ODE has to be the trivial one,Show the solution of this ODE has to be the trivial one,,"I'm trying to solve this question: Let $f$ be continuous in $\Omega=\{(t,x);|t|\leq a,|x|\leq b\}$. If   $f(t,x)\lt 0$ when $tx\gt0$ and $f(t,x)\gt 0$ when $tx\lt0$. Show that $x'=f(t,x), x(0)=0$ has $\varphi=0$ as the unique solution. I'm completely confused with this question, $x$ is a function or a variable? I need a hint to begin to solve this question. Thanks a lot!","I'm trying to solve this question: Let $f$ be continuous in $\Omega=\{(t,x);|t|\leq a,|x|\leq b\}$. If   $f(t,x)\lt 0$ when $tx\gt0$ and $f(t,x)\gt 0$ when $tx\lt0$. Show that $x'=f(t,x), x(0)=0$ has $\varphi=0$ as the unique solution. I'm completely confused with this question, $x$ is a function or a variable? I need a hint to begin to solve this question. Thanks a lot!",,['ordinary-differential-equations']
24,Steady State Solution Non-Linear ODE,Steady State Solution Non-Linear ODE,,"I'm working through some problems studying for a numerical methods course, but I'm stuck on how to answer the following question analytically. It says to find the steady state solution of the following equation: $\frac{dy}{dx} = -ay + e^{-y}$ where $y(0)=0$. It says that the steady state solution is when $\frac{dy}{dx} = 0$. It's also claimed that the steady state solution is a fixed point of the system. I'm stuck on where to start with this. If I immediately try to solve $0 = -ay + e^{-y}$, I can't isolate y. Then I tried to start with the homogenous equation, but that devolved also. What am I missing?","I'm working through some problems studying for a numerical methods course, but I'm stuck on how to answer the following question analytically. It says to find the steady state solution of the following equation: $\frac{dy}{dx} = -ay + e^{-y}$ where $y(0)=0$. It says that the steady state solution is when $\frac{dy}{dx} = 0$. It's also claimed that the steady state solution is a fixed point of the system. I'm stuck on where to start with this. If I immediately try to solve $0 = -ay + e^{-y}$, I can't isolate y. Then I tried to start with the homogenous equation, but that devolved also. What am I missing?",,"['ordinary-differential-equations', 'numerical-methods']"
25,Small question about ODE,Small question about ODE,,"i have this question : Given  three parameters $L,a$ et $\alpha$, we consider the   differential equation : $$(E)\qquad x''+\alpha x' +a x + \sin x =L, \  > t\geq0$$ 1) Show that the maximal solutions of $(E)$ are defined on all $\mathbb {R}$. 2) Assume that $a>0$ et $\alpha \geq 0$. a) Establish the existence of a positive constant $C$ such that : $\displaystyle \frac{a}{4}x^2+\frac{y^2}{2}\leq C+1+\frac{L^2}{a^2}$ (You can use the functional $V(x,y)=\frac12 y^2+\frac{a}{2}x^2-L  x-\cos x)$ I suppose that $y(t)=x'(t)$ , i have $\frac{dV}{dt}=-\alpha y^2$ so $V$ is decreasing but i don't know how to prove the existence of a positive $C$ such that :$\displaystyle \frac{a}{4}x^2+\frac{y^2}{2}\leq C+1+\frac{L^2}{a^2}$ I just find that : $\displaystyle\frac{y^2}{2}+\frac{a}{4}x^2 \leq V(x_0;y_0) + 1 + Lx-\frac{a}{4}x^2$ but how to find $\displaystyle\frac{L^2}{a^2}$ ?????????? Please help me Thank you .","i have this question : Given  three parameters $L,a$ et $\alpha$, we consider the   differential equation : $$(E)\qquad x''+\alpha x' +a x + \sin x =L, \  > t\geq0$$ 1) Show that the maximal solutions of $(E)$ are defined on all $\mathbb {R}$. 2) Assume that $a>0$ et $\alpha \geq 0$. a) Establish the existence of a positive constant $C$ such that : $\displaystyle \frac{a}{4}x^2+\frac{y^2}{2}\leq C+1+\frac{L^2}{a^2}$ (You can use the functional $V(x,y)=\frac12 y^2+\frac{a}{2}x^2-L  x-\cos x)$ I suppose that $y(t)=x'(t)$ , i have $\frac{dV}{dt}=-\alpha y^2$ so $V$ is decreasing but i don't know how to prove the existence of a positive $C$ such that :$\displaystyle \frac{a}{4}x^2+\frac{y^2}{2}\leq C+1+\frac{L^2}{a^2}$ I just find that : $\displaystyle\frac{y^2}{2}+\frac{a}{4}x^2 \leq V(x_0;y_0) + 1 + Lx-\frac{a}{4}x^2$ but how to find $\displaystyle\frac{L^2}{a^2}$ ?????????? Please help me Thank you .",,"['analysis', 'ordinary-differential-equations', 'dynamical-systems']"
26,"Differential equation two solutions, how so?","Differential equation two solutions, how so?",,"I tried to solve $7x^3y'=4*\sqrt{y}$ with $y(1)=1$ now I thought that Picard Lindelöf would tell me that there is a (at least in a local area for x=1) unique solution unfortunately I found two:  $y(x)=(-\frac{1}{7x^2}+\frac{8}{7})^2, y(x)=(-\frac{1}{7x^2}-\frac{6}{7})^2$. Can somebody explain to me why this is the case here?","I tried to solve $7x^3y'=4*\sqrt{y}$ with $y(1)=1$ now I thought that Picard Lindelöf would tell me that there is a (at least in a local area for x=1) unique solution unfortunately I found two:  $y(x)=(-\frac{1}{7x^2}+\frac{8}{7})^2, y(x)=(-\frac{1}{7x^2}-\frac{6}{7})^2$. Can somebody explain to me why this is the case here?",,['calculus']
27,Solving Differential equation with partial fraction decomposition,Solving Differential equation with partial fraction decomposition,,I am a little rusty with some calculus and need some help with the follow equation: \begin{equation} \int\dfrac{f'(x)}{f(x)}dx = \int\dfrac{1}{-x+x^2y}dx \end{equation} Where $y$ is a constant. My idea is to use some kind of $U$ substitution as I know $\int\frac{dv}{v} = \ln(v)$. This gives: \begin{align} \ln(f(x)) =& \int\dfrac{1}{-x+x^2y}dx \\ \end{align} Then I see to solve for $f(x)$ I can exponentiate. To solve the right integrand I would first do partial fraction decomposition which gives: $A=-1$ and $B=y$. Then I have: \begin{align} \ln(f(x)) = \int\dfrac{-1}{x}dx + \int\dfrac{y}{-1+xy}dx \\ \end{align} Then I get: \begin{equation} f(x) = x+-1+xy = -1+(y+1)x \end{equation} Is this correct?,I am a little rusty with some calculus and need some help with the follow equation: \begin{equation} \int\dfrac{f'(x)}{f(x)}dx = \int\dfrac{1}{-x+x^2y}dx \end{equation} Where $y$ is a constant. My idea is to use some kind of $U$ substitution as I know $\int\frac{dv}{v} = \ln(v)$. This gives: \begin{align} \ln(f(x)) =& \int\dfrac{1}{-x+x^2y}dx \\ \end{align} Then I see to solve for $f(x)$ I can exponentiate. To solve the right integrand I would first do partial fraction decomposition which gives: $A=-1$ and $B=y$. Then I have: \begin{align} \ln(f(x)) = \int\dfrac{-1}{x}dx + \int\dfrac{y}{-1+xy}dx \\ \end{align} Then I get: \begin{equation} f(x) = x+-1+xy = -1+(y+1)x \end{equation} Is this correct?,,"['calculus', 'ordinary-differential-equations', 'integration']"
28,Stability of solution,Stability of solution,,"I have this differential equation $x'=-x^3$ , How to study the stability and the asymtotic stability of $x=0$ ? Please help me Thank you .","I have this differential equation $x'=-x^3$ , How to study the stability and the asymtotic stability of $x=0$ ? Please help me Thank you .",,"['ordinary-differential-equations', 'analysis', 'dynamical-systems', 'stability-theory']"
29,Reduction of order of 2nd order ODE,Reduction of order of 2nd order ODE,,"$$ Q''+2aQ'+(a^2+w^2)=(a^2+w^2)gs $$ Where $''$ indicates the second derivative, $'$ indicates the first. $a$, $w$, $g$, $s$ are parameters. The problem is, I don't know any particular solution, so how can I reduce without knowing one?","$$ Q''+2aQ'+(a^2+w^2)=(a^2+w^2)gs $$ Where $''$ indicates the second derivative, $'$ indicates the first. $a$, $w$, $g$, $s$ are parameters. The problem is, I don't know any particular solution, so how can I reduce without knowing one?",,['ordinary-differential-equations']
30,ODE solving in Scilab,ODE solving in Scilab,,I have a certain ODE problem which needs to be solved using Scilab. dx(1)/dt=k*x(1)-x(5) dx(2)/dt=k2*x(2)-k1*x(1) dx(3)/dt=k1*[x(2)-x(3)] dx(4)/dt=k1*[x(3)-x(4)] $x(5)$ takes value $0$ till $t_0$. Can anyone help me with coding this problem? Any suggestions would be of immense help.,I have a certain ODE problem which needs to be solved using Scilab. dx(1)/dt=k*x(1)-x(5) dx(2)/dt=k2*x(2)-k1*x(1) dx(3)/dt=k1*[x(2)-x(3)] dx(4)/dt=k1*[x(3)-x(4)] $x(5)$ takes value $0$ till $t_0$. Can anyone help me with coding this problem? Any suggestions would be of immense help.,,"['ordinary-differential-equations', 'numerical-methods']"
31,How to solve differential equation involving an integral,How to solve differential equation involving an integral,,"The integro-differential equation takes the form $$c′(t)+iac(t)+\int_{0}^{\infty} f(t−\tau)c(\tau)d\tau=0$$ in which $$f(t−\tau)=\frac{\pi}{4}∫_{0}^{\infty} J(\omega)e^{−i\omega(t−\tau)} d\omega,$$ $$J(ω)=2\pi \alpha \omega_{c}^{1−b}\omega^{b}\Theta(\omega_{c} −\omega),$$ and $a,\alpha,\omega_{c},b>0$ are all real constants. $\Theta(x)$ is the usual step function. To solve this equation, it seems the Laplace transform does not work.","The integro-differential equation takes the form $$c′(t)+iac(t)+\int_{0}^{\infty} f(t−\tau)c(\tau)d\tau=0$$ in which $$f(t−\tau)=\frac{\pi}{4}∫_{0}^{\infty} J(\omega)e^{−i\omega(t−\tau)} d\omega,$$ $$J(ω)=2\pi \alpha \omega_{c}^{1−b}\omega^{b}\Theta(\omega_{c} −\omega),$$ and $a,\alpha,\omega_{c},b>0$ are all real constants. $\Theta(x)$ is the usual step function. To solve this equation, it seems the Laplace transform does not work.",,"['ordinary-differential-equations', 'integro-differential-equations']"
32,Yang–Mills theory,Yang–Mills theory,,"We define the energy as $$E = I_F + I_K + I_V,$$ where, $$I_F [A]= \frac{1}{2} \int d^Dx \operatorname{tr} F^2_{ij},$$ $F_{ij}$ represents the electromagnetic force. $$I_K [\phi,A]= \frac{1}{2} \int d^Dx (D_j \phi)^\dagger (D_j \phi),$$ $$I_V [\phi]= \int d^Dx V(\phi),$$ Given a solution $\bar \phi(x)$, $\bar A_j(x)$, we define scaled fields $$f_\lambda(x)= \bar\phi(\lambda x),$$ $$g_{j\lambda}(x)= \lambda \bar A_j(\lambda x).$$ Proceeding as in the pure scalar case, we find that \begin{align}  E(\lambda) &= I_F [g_\lambda ] + I_K [f_\lambda , g_\lambda ] + I_V [f_\lambda ] \\  & = \lambda^{4-D} I_F [\bar A] +  \lambda^{2-D} I_K [\bar \phi,\bar A]+  \lambda^{-D} I_V [\bar \phi] \tag{1} \end{align} which is stationary at $\lambda = 1$ if I just don't understand how $$I_F [A]=\lambda^{4-D} I_F [\bar A],$$ is in the equation (1) The post is slightly related with this: link","We define the energy as $$E = I_F + I_K + I_V,$$ where, $$I_F [A]= \frac{1}{2} \int d^Dx \operatorname{tr} F^2_{ij},$$ $F_{ij}$ represents the electromagnetic force. $$I_K [\phi,A]= \frac{1}{2} \int d^Dx (D_j \phi)^\dagger (D_j \phi),$$ $$I_V [\phi]= \int d^Dx V(\phi),$$ Given a solution $\bar \phi(x)$, $\bar A_j(x)$, we define scaled fields $$f_\lambda(x)= \bar\phi(\lambda x),$$ $$g_{j\lambda}(x)= \lambda \bar A_j(\lambda x).$$ Proceeding as in the pure scalar case, we find that \begin{align}  E(\lambda) &= I_F [g_\lambda ] + I_K [f_\lambda , g_\lambda ] + I_V [f_\lambda ] \\  & = \lambda^{4-D} I_F [\bar A] +  \lambda^{2-D} I_K [\bar \phi,\bar A]+  \lambda^{-D} I_V [\bar \phi] \tag{1} \end{align} which is stationary at $\lambda = 1$ if I just don't understand how $$I_F [A]=\lambda^{4-D} I_F [\bar A],$$ is in the equation (1) The post is slightly related with this: link",,"['calculus', 'ordinary-differential-equations']"
33,Solve differential equation by a variable change.,Solve differential equation by a variable change.,,"This is the problem I am facing: Find the general solution to $(1-t^2)^2 x'' - t x' + a^2 x = 0$ Hint: Use a change of variable $t = \phi(u)$ such that after the change of variable there is no coefficient in x' and then use techniques to reduce the order of the equation. This is my progress: I am starting with differential equations so I will write $dx/dt$ instead of just $x'$ in order to not get confused between differentiating against $t$ or $u$ (sorry if the english grammar is not correct). Thus, I'll rewrite my equation as $(1-t^2)^2 \dfrac{d^2x}{dt^2} - t \dfrac{dx}{dt} + a^2x = 0$ Then I apply then change of variable $t = \phi(u)$. Let's put $\dfrac{dx}{dt}$ and $ \dfrac{d^2x}{dt^2} $ in terms of  $\dfrac{dx}{du}$ by using the chain rule (I will omit the argument of $\phi$ (which is always $u$ so that the equations don't get messy): $\dfrac{dx}{du}$ = $\dfrac{dx}{dt}$ $\dfrac{dt}{du}$ = $\dfrac{dx}{dt}$ $\phi' \Longrightarrow \dfrac{dx}{dt} = \dfrac{dx}{du} \dfrac{1}{\phi'}$ $\dfrac{d^2x}{du^2} =  \dfrac{d}{du}\left( \dfrac{dx}{du}\right) =  \dfrac{d}{du}\left( \dfrac{dx}{dt}  \dfrac{dt}{du}\right) =  \dfrac{d^2x}{dt^2} \dfrac{dx}{du} \dfrac{dx}{du} + \dfrac{dx}{dt} \dfrac{d^2t}{du^2} =  \dfrac{d^2x}{dt^2} \phi'^2 + \dfrac{dx}{dt} \phi'' $ From the last line we can obtain $d^2x/dt^2$: $\dfrac{d^2x}{dt^2} = \dfrac{1}{\phi'^2}(\dfrac{d^2x}{du^2} - \phi'' \dfrac{dx}{dt}) = \dfrac{1}{\phi'^2}\left(\dfrac{d^2x}{du^2} - \dfrac{\phi''}{\phi'} \dfrac{dx}{du}\right)$ Now we can rewrite the original equation as: $(1-\phi(u)^2)^2\left(\frac{1}{\phi'^2}\left(\dfrac{d^2x}{du^2} - \dfrac{\phi''}{\phi'} \dfrac{dx}{du}\right)\right) - \phi(u) \left(\dfrac{dx}{du} \dfrac{1}{\phi'}\right) + a^2 x = 0$ Sorting and using $x'$ and $x''$ again: $\dfrac{(1-\phi(u)^2)^2}{(\phi(u)')^2} x'' +\left(\dfrac{(1-\phi(u)^2)^2 \phi''(u)}{(\phi'(u))^3} + \dfrac{\phi(u)}{\phi'(u)}\right) x'+ a^2 x = 0$ Now following the hint: $\dfrac{(1-\phi(u)^2)^2 \phi''(u)}{(\phi'(u))^3} + \dfrac{\phi(u)}{\phi'(u)} = 0 \Longrightarrow (1-\phi^2)^2 \phi'' + \phi (\phi')^2 = 0 $ I cannot solve that for $\phi$ :S","This is the problem I am facing: Find the general solution to $(1-t^2)^2 x'' - t x' + a^2 x = 0$ Hint: Use a change of variable $t = \phi(u)$ such that after the change of variable there is no coefficient in x' and then use techniques to reduce the order of the equation. This is my progress: I am starting with differential equations so I will write $dx/dt$ instead of just $x'$ in order to not get confused between differentiating against $t$ or $u$ (sorry if the english grammar is not correct). Thus, I'll rewrite my equation as $(1-t^2)^2 \dfrac{d^2x}{dt^2} - t \dfrac{dx}{dt} + a^2x = 0$ Then I apply then change of variable $t = \phi(u)$. Let's put $\dfrac{dx}{dt}$ and $ \dfrac{d^2x}{dt^2} $ in terms of  $\dfrac{dx}{du}$ by using the chain rule (I will omit the argument of $\phi$ (which is always $u$ so that the equations don't get messy): $\dfrac{dx}{du}$ = $\dfrac{dx}{dt}$ $\dfrac{dt}{du}$ = $\dfrac{dx}{dt}$ $\phi' \Longrightarrow \dfrac{dx}{dt} = \dfrac{dx}{du} \dfrac{1}{\phi'}$ $\dfrac{d^2x}{du^2} =  \dfrac{d}{du}\left( \dfrac{dx}{du}\right) =  \dfrac{d}{du}\left( \dfrac{dx}{dt}  \dfrac{dt}{du}\right) =  \dfrac{d^2x}{dt^2} \dfrac{dx}{du} \dfrac{dx}{du} + \dfrac{dx}{dt} \dfrac{d^2t}{du^2} =  \dfrac{d^2x}{dt^2} \phi'^2 + \dfrac{dx}{dt} \phi'' $ From the last line we can obtain $d^2x/dt^2$: $\dfrac{d^2x}{dt^2} = \dfrac{1}{\phi'^2}(\dfrac{d^2x}{du^2} - \phi'' \dfrac{dx}{dt}) = \dfrac{1}{\phi'^2}\left(\dfrac{d^2x}{du^2} - \dfrac{\phi''}{\phi'} \dfrac{dx}{du}\right)$ Now we can rewrite the original equation as: $(1-\phi(u)^2)^2\left(\frac{1}{\phi'^2}\left(\dfrac{d^2x}{du^2} - \dfrac{\phi''}{\phi'} \dfrac{dx}{du}\right)\right) - \phi(u) \left(\dfrac{dx}{du} \dfrac{1}{\phi'}\right) + a^2 x = 0$ Sorting and using $x'$ and $x''$ again: $\dfrac{(1-\phi(u)^2)^2}{(\phi(u)')^2} x'' +\left(\dfrac{(1-\phi(u)^2)^2 \phi''(u)}{(\phi'(u))^3} + \dfrac{\phi(u)}{\phi'(u)}\right) x'+ a^2 x = 0$ Now following the hint: $\dfrac{(1-\phi(u)^2)^2 \phi''(u)}{(\phi'(u))^3} + \dfrac{\phi(u)}{\phi'(u)} = 0 \Longrightarrow (1-\phi^2)^2 \phi'' + \phi (\phi')^2 = 0 $ I cannot solve that for $\phi$ :S",,['ordinary-differential-equations']
34,Normal form of a vector field in $\mathbb {R}^2$,Normal form of a vector field in,\mathbb {R}^2,"Edited after considering the comments Problem: What is the normal form of the vector field: $$\dot x_1=x_1+x_2^2$$ $$\dot x_2=2x_2+x_1^2$$ Solution: The eugine values of the matrix of the linearised around $(0,0)$ system are $2$ and $1$. We, therefore, have the only resonance $2=2\dot{}1+0\dot{} 2$. The resonant vector-monome is $(0,x_1^2)$. The normal form is then $$\dot x_1=x_1$$ $$\dot x_2=2x_2+cx_1^2$$ Question: I believe this is correct, is it not?","Edited after considering the comments Problem: What is the normal form of the vector field: $$\dot x_1=x_1+x_2^2$$ $$\dot x_2=2x_2+x_1^2$$ Solution: The eugine values of the matrix of the linearised around $(0,0)$ system are $2$ and $1$. We, therefore, have the only resonance $2=2\dot{}1+0\dot{} 2$. The resonant vector-monome is $(0,x_1^2)$. The normal form is then $$\dot x_1=x_1$$ $$\dot x_2=2x_2+cx_1^2$$ Question: I believe this is correct, is it not?",,"['ordinary-differential-equations', 'dynamical-systems']"
35,Solving the next differential equation,Solving the next differential equation,,"How can I solve the next differential equation? $$\cos(x+y)dx=x\sin(x+y)dx+x\sin(x+y)dy$$ I dont know what kind of equation it is. It's not homogeneous, separable differential equation or linear. Any suggestions?","How can I solve the next differential equation? $$\cos(x+y)dx=x\sin(x+y)dx+x\sin(x+y)dy$$ I dont know what kind of equation it is. It's not homogeneous, separable differential equation or linear. Any suggestions?",,"['calculus', 'ordinary-differential-equations']"
36,Systems of Differential Equations and higher order Differential Equations.,Systems of Differential Equations and higher order Differential Equations.,,"I've seen how one can transform a higher order ordinary differential equation into a system of first-order differential equations, but I haven't been able to find the converse. Is it true that one can transform any system into a higher-order differential equation? If so, is there a general method to do so?","I've seen how one can transform a higher order ordinary differential equation into a system of first-order differential equations, but I haven't been able to find the converse. Is it true that one can transform any system into a higher-order differential equation? If so, is there a general method to do so?",,['ordinary-differential-equations']
37,Question on the notation of the interval for which a general solution or equation is defined,Question on the notation of the interval for which a general solution or equation is defined,,"when we say that a solution is true for  $-\infty < x < +\infty$ does that mean that it is true for all real numbers or any number between negative infinity and positive infinity, which is quite vague.","when we say that a solution is true for  $-\infty < x < +\infty$ does that mean that it is true for all real numbers or any number between negative infinity and positive infinity, which is quite vague.",,['ordinary-differential-equations']
38,Differential Equation - Word Problem,Differential Equation - Word Problem,,"Need help with this problem. I want to write it in a differential equation of the form: $$p(t)' + f(t) p(t) = g(t)$$ Because of restoration of an island habitat, the maximum population of birds it can support at a time t is given by $1100*\exp(t/80)$. The growth rate of the population $p(t)$ is equal to 1/20 of the difference between the maximum population and the current population. Initially the island has a population of 200 birds.","Need help with this problem. I want to write it in a differential equation of the form: $$p(t)' + f(t) p(t) = g(t)$$ Because of restoration of an island habitat, the maximum population of birds it can support at a time t is given by $1100*\exp(t/80)$. The growth rate of the population $p(t)$ is equal to 1/20 of the difference between the maximum population and the current population. Initially the island has a population of 200 birds.",,['ordinary-differential-equations']
39,Up and Down Motion (Two objects meeting in time?),Up and Down Motion (Two objects meeting in time?),,"PROBLEM: Suppose than an object is thrown upward with an initial velocity of 200ft/sec and that another one is thrown upward 5 seconds later with an initial velocity of 300ft/sec. When and where do they meet? CONCERNS/QUESTIONS: The answer I arrive at seems correct from my own understanding of the problem. However, I do not understand what this problem is teaching me? What is the relationship among these two objects when they meet at the same time (t) with the same displacement (s)? Does it mean they have the same slope ""velocity""? I am confused on how I should attempt to solve this problem. MY STRATEGY: I found the expression for both vectors, (a) acceleration  and (v) velocity, for both objects by using integration. I then found the expression for displacement (s) for both objects using integration. Object 1 $$ s=-16t^2+200t\\ \vec{v}=-32t+200\\\vec{a}=-32$$ Object 2 $$ s=-16t^2+460t\\\vec{v}=-32t+460\\\vec{a}=-32$$ Then from here I was lost in what to do. Therefore, I just found $t$ for both objects when there slope is equal to $0$. This gave me the time at each objects maximum height. Object 1 $t=6.25$ seconds Object 2 $t=14.375$ seconds I divided Object 2's (t) value by Object 1's (t) value and got $2.3$ seconds. This seems correct in my own mind after thinking long and hard about it. $14.375 / 6.25$ = $2.3$ seconds after the second object has been thrown I do not understand these last few steps or if I was even right. I'm not understanding the concept from which the problem is trying to teach me. May someone address my concerns and questions I have mentioned.","PROBLEM: Suppose than an object is thrown upward with an initial velocity of 200ft/sec and that another one is thrown upward 5 seconds later with an initial velocity of 300ft/sec. When and where do they meet? CONCERNS/QUESTIONS: The answer I arrive at seems correct from my own understanding of the problem. However, I do not understand what this problem is teaching me? What is the relationship among these two objects when they meet at the same time (t) with the same displacement (s)? Does it mean they have the same slope ""velocity""? I am confused on how I should attempt to solve this problem. MY STRATEGY: I found the expression for both vectors, (a) acceleration  and (v) velocity, for both objects by using integration. I then found the expression for displacement (s) for both objects using integration. Object 1 $$ s=-16t^2+200t\\ \vec{v}=-32t+200\\\vec{a}=-32$$ Object 2 $$ s=-16t^2+460t\\\vec{v}=-32t+460\\\vec{a}=-32$$ Then from here I was lost in what to do. Therefore, I just found $t$ for both objects when there slope is equal to $0$. This gave me the time at each objects maximum height. Object 1 $t=6.25$ seconds Object 2 $t=14.375$ seconds I divided Object 2's (t) value by Object 1's (t) value and got $2.3$ seconds. This seems correct in my own mind after thinking long and hard about it. $14.375 / 6.25$ = $2.3$ seconds after the second object has been thrown I do not understand these last few steps or if I was even right. I'm not understanding the concept from which the problem is trying to teach me. May someone address my concerns and questions I have mentioned.",,"['calculus', 'ordinary-differential-equations', 'physics']"
40,How do I solve the DE $y''+6y'+8y = 2t+e^t$?,How do I solve the DE ?,y''+6y'+8y = 2t+e^t,"The differential equation I am trying to solve is  $$ \dfrac{d^2y}{dt^2} + 6\dfrac{dy}{dt} + 8y = 2t +e^t $$ I know how to start off. I have done the $s^2 + 6s + 8 = 0$ to get $s = -4$ and $s = -2$ and have the  $$ y_p(t) = k_1e^{-4t} + k_2e^{-2t} $$ What I am having a problem with is gettting $y_h(t)$ which will be dependent on the $2t + e^t$. I know that the solution will be in the form $c_1 + c_2t + c_3e^t$, I am just not sure how to solve for those constants Thank you very much for any help you can give me.","The differential equation I am trying to solve is  $$ \dfrac{d^2y}{dt^2} + 6\dfrac{dy}{dt} + 8y = 2t +e^t $$ I know how to start off. I have done the $s^2 + 6s + 8 = 0$ to get $s = -4$ and $s = -2$ and have the  $$ y_p(t) = k_1e^{-4t} + k_2e^{-2t} $$ What I am having a problem with is gettting $y_h(t)$ which will be dependent on the $2t + e^t$. I know that the solution will be in the form $c_1 + c_2t + c_3e^t$, I am just not sure how to solve for those constants Thank you very much for any help you can give me.",,['ordinary-differential-equations']
41,Linear stability analysis on a constrained three-dimensional system of ODE,Linear stability analysis on a constrained three-dimensional system of ODE,,"Let $\begin{cases} \dot x = f({\bf u}) \\ \dot y = g({\bf u}) \\ \dot z = h({\bf u})\end{cases}$ be a well-defined nonlinear system with ${\bf u} = (x,y,z)$ and restricted to domain $x,y,z \geq 0$. Suppose that $f+g+h=0$ so that $\dot x + \dot y + \dot z = 0 \Rightarrow x + y + z = K$ constant. I want to perform a linear stability analysis of some fixed point $\bf u_0$ but when I calculate the Jacobian and find eigenvalues I get a $0$ eigenvalue. I would like to know: If I calculate the Jacobian of the system in $x,y$ and I get eigenvalues with nonzero real part, can I use the result to classify the stability of $\bf u_0$? If I calculate the Jacobian in $x,y$, $y,z$, and $x,z$ and get the same result (eigenvalues with nonzero real part) can I use the result to classify the stability of $\bf u_0$? Is it necessarily the case that a constrained system as described would have a $0$ eigenvalue in the Jacobian?","Let $\begin{cases} \dot x = f({\bf u}) \\ \dot y = g({\bf u}) \\ \dot z = h({\bf u})\end{cases}$ be a well-defined nonlinear system with ${\bf u} = (x,y,z)$ and restricted to domain $x,y,z \geq 0$. Suppose that $f+g+h=0$ so that $\dot x + \dot y + \dot z = 0 \Rightarrow x + y + z = K$ constant. I want to perform a linear stability analysis of some fixed point $\bf u_0$ but when I calculate the Jacobian and find eigenvalues I get a $0$ eigenvalue. I would like to know: If I calculate the Jacobian of the system in $x,y$ and I get eigenvalues with nonzero real part, can I use the result to classify the stability of $\bf u_0$? If I calculate the Jacobian in $x,y$, $y,z$, and $x,z$ and get the same result (eigenvalues with nonzero real part) can I use the result to classify the stability of $\bf u_0$? Is it necessarily the case that a constrained system as described would have a $0$ eigenvalue in the Jacobian?",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-optimization']"
42,Differential equation must satisfy its edge conditions.,Differential equation must satisfy its edge conditions.,,"I have this variation problem $$\text{Minimize} \; \int_0^1 \left( 12xt- \dot{x}^2-2 \dot{x} \right) \; dt$$ With the edge conditions $x(0)=0$ and $x(1)$ is ""free"". And from here solve it: $$x(t)\to -t^3 +c_1t+c_2$$ From here it should've been correctly. Now I must solve the equation and compute $c_1$ and $c_2$ However I'm not aware of the edge condition $x(1)$ as free. How do I solve this, and what does it exactly mean by ""free""? Result: $c_1$ should be $2$ and $c_2$ should be $0$. (Ps. If you can show it with Mathematica It would be great!)","I have this variation problem $$\text{Minimize} \; \int_0^1 \left( 12xt- \dot{x}^2-2 \dot{x} \right) \; dt$$ With the edge conditions $x(0)=0$ and $x(1)$ is ""free"". And from here solve it: $$x(t)\to -t^3 +c_1t+c_2$$ From here it should've been correctly. Now I must solve the equation and compute $c_1$ and $c_2$ However I'm not aware of the edge condition $x(1)$ as free. How do I solve this, and what does it exactly mean by ""free""? Result: $c_1$ should be $2$ and $c_2$ should be $0$. (Ps. If you can show it with Mathematica It would be great!)",,"['ordinary-differential-equations', 'partial-differential-equations']"
43,About calculating a Green's function,About calculating a Green's function,,"For a positive integer $d>=3$ and a real number $h_0$ I have the differential equation for a function $f$ of $x$, $$x^2 h^2 f'' + \frac{6-d}{2}xh^2 f' = \frac{d(d-2)}{2}h^2f$$ where $h = h_0 x^{1- \frac{d}{2}}$.  (..and I believe that this explicit relationship between $h$ and $x$ is not necessary to solve the question..) Its full Green's function is claimed as $$G(x,x') = \theta(x-x')\frac{2x'^{d-3}}{h_0^2(3d-4)}[(x/x')^{-d/2} - (x/x')^{d-2}  ]$$ Let me state here as I would have solved this question. First let me shift to variable $a$ such that $x = e^a$ and drop the overall factor of $h$ (it can never be zero) and then I have the differential equation as, $[\frac{d^2}{da^2} + (2 - \frac{d}{2})\frac{d}{da} - \frac{d(d-2)}{2}]f(a) = 0$ So the Green's function equation that one wants to solve is, $[\frac{d^2}{da^2} + (2 - \frac{d}{2})\frac{d}{da} - \frac{d(d-2)}{2}]G(a-a') = \delta (a-a')$ Now we use Dirac delta function convention as $\delta (a-a') = \int \frac{dk}{\sqrt{2\pi}}e^{ik.(a-a')} $ and hence the Green's function and}( its Fourier transform is defined as $G(a-a') = \int \frac{dk}{\sqrt{2\pi}} \tilde{G}(k) e^{ik(a-a')}$ And substituting the above one is solving the algebraic equation, $[(ik)^2 + (2 - \frac{d}{2})ik - \frac{d(d-2)}{2}]\tilde{G}(k) =  1$ hence we have, $\tilde{G}(k) = \frac{-1}{(k-\frac{id}{2})(k-i(2-d))}$ Now with the above $\tilde{G}(k)$ we want to compute the contour integral, $ \int \frac{dk}{\sqrt{2\pi}} \tilde{G}(k) e^{ik(a-a')}$ Since $d\geq3$ one of the poles is on the positive imaginary axis, $\frac{id}{2}$ and one is on the negative imaginary axis, $i(2-d)$. For $a>a'$ I need to close the contour in the upper half plane for convergence and also I need both the poles to contribute. Hence I choose to close contour as the usual semi-circle on the UHP with the flat side on the real axis but while going near the origin I take a detour around the origin to go around the $i(2-d)$ pole in the LHP. Then I take the limit of this neck width to pinch to zero. Then doing the usual $\sqrt{2\pi i}Residue$ analysis I get, (after changing back to ""x"") $G(x,x') = \frac{-2\sqrt{2\pi}}{3d-4}[(x/x')^{-d/2} + (x/x')^{d-2}]$ But the above is not the right answer! - What can be changed in this Fourier transform based method to get the right answer? (the questionable thing that I am doing is to kind of deform the semi-circle contour such that it actually has to deform across the LHP pole and which is kind of tricky)","For a positive integer $d>=3$ and a real number $h_0$ I have the differential equation for a function $f$ of $x$, $$x^2 h^2 f'' + \frac{6-d}{2}xh^2 f' = \frac{d(d-2)}{2}h^2f$$ where $h = h_0 x^{1- \frac{d}{2}}$.  (..and I believe that this explicit relationship between $h$ and $x$ is not necessary to solve the question..) Its full Green's function is claimed as $$G(x,x') = \theta(x-x')\frac{2x'^{d-3}}{h_0^2(3d-4)}[(x/x')^{-d/2} - (x/x')^{d-2}  ]$$ Let me state here as I would have solved this question. First let me shift to variable $a$ such that $x = e^a$ and drop the overall factor of $h$ (it can never be zero) and then I have the differential equation as, $[\frac{d^2}{da^2} + (2 - \frac{d}{2})\frac{d}{da} - \frac{d(d-2)}{2}]f(a) = 0$ So the Green's function equation that one wants to solve is, $[\frac{d^2}{da^2} + (2 - \frac{d}{2})\frac{d}{da} - \frac{d(d-2)}{2}]G(a-a') = \delta (a-a')$ Now we use Dirac delta function convention as $\delta (a-a') = \int \frac{dk}{\sqrt{2\pi}}e^{ik.(a-a')} $ and hence the Green's function and}( its Fourier transform is defined as $G(a-a') = \int \frac{dk}{\sqrt{2\pi}} \tilde{G}(k) e^{ik(a-a')}$ And substituting the above one is solving the algebraic equation, $[(ik)^2 + (2 - \frac{d}{2})ik - \frac{d(d-2)}{2}]\tilde{G}(k) =  1$ hence we have, $\tilde{G}(k) = \frac{-1}{(k-\frac{id}{2})(k-i(2-d))}$ Now with the above $\tilde{G}(k)$ we want to compute the contour integral, $ \int \frac{dk}{\sqrt{2\pi}} \tilde{G}(k) e^{ik(a-a')}$ Since $d\geq3$ one of the poles is on the positive imaginary axis, $\frac{id}{2}$ and one is on the negative imaginary axis, $i(2-d)$. For $a>a'$ I need to close the contour in the upper half plane for convergence and also I need both the poles to contribute. Hence I choose to close contour as the usual semi-circle on the UHP with the flat side on the real axis but while going near the origin I take a detour around the origin to go around the $i(2-d)$ pole in the LHP. Then I take the limit of this neck width to pinch to zero. Then doing the usual $\sqrt{2\pi i}Residue$ analysis I get, (after changing back to ""x"") $G(x,x') = \frac{-2\sqrt{2\pi}}{3d-4}[(x/x')^{-d/2} + (x/x')^{d-2}]$ But the above is not the right answer! - What can be changed in this Fourier transform based method to get the right answer? (the questionable thing that I am doing is to kind of deform the semi-circle contour such that it actually has to deform across the LHP pole and which is kind of tricky)",,"['calculus', 'analysis', 'integration', 'ordinary-differential-equations']"
44,Steady-state and Equation System,Steady-state and Equation System,,"Two questions: Given the transition matrix: $ \begin{vmatrix} \ 0.4 & 0.4 & 0.2 \\ \ 0.5 & 0.3 & 0.2 \\ \ 0.1 & 0.5 & 0.4 \end{vmatrix} $ I would like to know HOW to find the steady-state of this. I've tried to do that trying to solve the system by hand: $ \begin{aligned} \ a = 0.4a+0.5b+0.1c \\ \ b = 0.4a+0.3b+0.5c \\ \ c = 0.2a+0.2b+0.4c \\ \ 1 = a+b+c \\ \end{aligned} $ Question 1 : Is it possible to solve this system by hand? What is the best way to do that? If it not possible to solve, how do i see this impossibility? Question 2 : Wolfram said to me that this system is impossible (please, answer the first question (: ). What this fact means to this Markov chain?","Two questions: Given the transition matrix: $ \begin{vmatrix} \ 0.4 & 0.4 & 0.2 \\ \ 0.5 & 0.3 & 0.2 \\ \ 0.1 & 0.5 & 0.4 \end{vmatrix} $ I would like to know HOW to find the steady-state of this. I've tried to do that trying to solve the system by hand: $ \begin{aligned} \ a = 0.4a+0.5b+0.1c \\ \ b = 0.4a+0.3b+0.5c \\ \ c = 0.2a+0.2b+0.4c \\ \ 1 = a+b+c \\ \end{aligned} $ Question 1 : Is it possible to solve this system by hand? What is the best way to do that? If it not possible to solve, how do i see this impossibility? Question 2 : Wolfram said to me that this system is impossible (please, answer the first question (: ). What this fact means to this Markov chain?",,"['ordinary-differential-equations', 'markov-chains']"
45,What is the asymptotic behaviour of the solution of an inhomogeneous linear ODE?,What is the asymptotic behaviour of the solution of an inhomogeneous linear ODE?,,"This is probably a pretty easy question to someone working in the field of ODEs (which I generally do not), but nonetheless, I've haven't been able to find clear a answer in the textbooks I've looked in. Consider the ODE $X'(t) = A + BX(t)$ with some initial condition $X(0)$. Here, A is a vector of dimension $n$, and $B$ is an $n\times n$ matrix. I know that this equation has a unique solution given by $X(t) = \exp(tB)X(0) + \int_0^t \exp((t-s)B)A ds$ with $\exp(tB)$ denoting the matrix exponential. I also know that if all eigenvalues of $B$ have strictly negative real part, then $B$ is in particular invertible, and it holds that $X(t)$ converges to $-B^{-1}A$ as $t$ tends to infinity. My question is this: What can be said about the limit of $X(t)$ when the eigenvalues of $B$ do not all have strictly negative real parts? Ideally, I would like to know about a criterion for which coordinates of $X(t)$ tend to infinity, minus infinity or a finite limit as a function of, say, the eigenvalues of $B$.","This is probably a pretty easy question to someone working in the field of ODEs (which I generally do not), but nonetheless, I've haven't been able to find clear a answer in the textbooks I've looked in. Consider the ODE $X'(t) = A + BX(t)$ with some initial condition $X(0)$. Here, A is a vector of dimension $n$, and $B$ is an $n\times n$ matrix. I know that this equation has a unique solution given by $X(t) = \exp(tB)X(0) + \int_0^t \exp((t-s)B)A ds$ with $\exp(tB)$ denoting the matrix exponential. I also know that if all eigenvalues of $B$ have strictly negative real part, then $B$ is in particular invertible, and it holds that $X(t)$ converges to $-B^{-1}A$ as $t$ tends to infinity. My question is this: What can be said about the limit of $X(t)$ when the eigenvalues of $B$ do not all have strictly negative real parts? Ideally, I would like to know about a criterion for which coordinates of $X(t)$ tend to infinity, minus infinity or a finite limit as a function of, say, the eigenvalues of $B$.",,['ordinary-differential-equations']
46,Constant of Integration of the Integrating Factor,Constant of Integration of the Integrating Factor,,"I was reading my notes and at some point I write that in solving a first order linear DE using the integrating factor the final solution should have only one arvitrary constant arising from the integration of $Q(x)e^{\int P(x)dx}$ and so $e^{\int P(x)dx}$ does not contain an arbitrary constant. Essentially what this means is that if $$\frac{dy}{dx}+P(x)\cdot y=Q(x) \Rightarrow y=e^{-\int P(x)dx}\int Q(x)e^{\int P(x)dx}dx,$$the only constant that would determine a particular solution would be the one arising from the second integral and not the one in the exponent. Why is this? I understand that since this is a firstorder DE is should only involve one constant, but what about what I ask above? If I define $\int P(x)dx=f(x) + C$ and substitute above, this would yield $$y=e^{-f(x)+c}\int Q(x)e^{-f(x)+c}dx=e^{-f(x)}e^{2c}\int Q(x)e^{-f(x)}dx$$and now also define $\int Q(x)e^{-f(x)}dx=g(x)+k$ we have$$y=e^{-f(x)}e^{2c}\left(g(x)+k\right)=e^{2c}\cdot e^{-f(x)}g(x)+ke^{2c}\cdot e^{-f(x)}$$ so wouldn't the constant of integration $c$ also affect the final solution? Thanks in advance!","I was reading my notes and at some point I write that in solving a first order linear DE using the integrating factor the final solution should have only one arvitrary constant arising from the integration of $Q(x)e^{\int P(x)dx}$ and so $e^{\int P(x)dx}$ does not contain an arbitrary constant. Essentially what this means is that if $$\frac{dy}{dx}+P(x)\cdot y=Q(x) \Rightarrow y=e^{-\int P(x)dx}\int Q(x)e^{\int P(x)dx}dx,$$the only constant that would determine a particular solution would be the one arising from the second integral and not the one in the exponent. Why is this? I understand that since this is a firstorder DE is should only involve one constant, but what about what I ask above? If I define $\int P(x)dx=f(x) + C$ and substitute above, this would yield $$y=e^{-f(x)+c}\int Q(x)e^{-f(x)+c}dx=e^{-f(x)}e^{2c}\int Q(x)e^{-f(x)}dx$$and now also define $\int Q(x)e^{-f(x)}dx=g(x)+k$ we have$$y=e^{-f(x)}e^{2c}\left(g(x)+k\right)=e^{2c}\cdot e^{-f(x)}g(x)+ke^{2c}\cdot e^{-f(x)}$$ so wouldn't the constant of integration $c$ also affect the final solution? Thanks in advance!",,['ordinary-differential-equations']
47,Differential Equations question,Differential Equations question,,"Here's my problem: If $y' = y^2$ and $y(0) = 3$, then find $y(-1)$. I'm not quite sure what to do with it. What I've watched on Khan Academy doesn't seem to be in the same format as this question.","Here's my problem: If $y' = y^2$ and $y(0) = 3$, then find $y(-1)$. I'm not quite sure what to do with it. What I've watched on Khan Academy doesn't seem to be in the same format as this question.",,['ordinary-differential-equations']
48,Find a general control and then show that this could have been achieved at x2,Find a general control and then show that this could have been achieved at x2,,"Determine the general form of $u_0, u_1 ~\text{and} ~ u_2$ if a system of difference equations of the form $$x_{n+1} = Ax_n  + Bu_n,$$ where: $$A = \begin{pmatrix}   3 & 2 & 2  \\   -1 & 0 & -1 \\    0 & 0 & 1   \end{pmatrix}$$ and: $$B = \begin{pmatrix}   0 & 0  \\   0 & 1 \\   1 & 0   \end{pmatrix}$$ is to be controlled for $x_0 = 0 ~ to ~ x_3 = [2, 1, 2]^T$ . Show this target could have been achieved at  $x_2$ Solution So far I have caculated the controlability matrix to be $$ C =\begin{pmatrix} 0&0&2&2&6&6\\ 0&1&-1&0&-3&-2\\ 1&0&1&0&1&0 \end{pmatrix}. $$ Thus the system is controlable Now putting Cv=x3 i have the 3 equations $$ 2c+2d+6e+6f=2\\ b-c-3e-2f=1\\ c+e+f=2\\ $$ which i have then put into augmented matrix row echleon form which i have found to be $$ a-d-2e-3f=1\\ b+d+f=2\\ c+d+3e+3e=1\\ $$ How do I now solve with so many unknowns? also can you please check my working so far is correct. many thanks","Determine the general form of $u_0, u_1 ~\text{and} ~ u_2$ if a system of difference equations of the form $$x_{n+1} = Ax_n  + Bu_n,$$ where: $$A = \begin{pmatrix}   3 & 2 & 2  \\   -1 & 0 & -1 \\    0 & 0 & 1   \end{pmatrix}$$ and: $$B = \begin{pmatrix}   0 & 0  \\   0 & 1 \\   1 & 0   \end{pmatrix}$$ is to be controlled for $x_0 = 0 ~ to ~ x_3 = [2, 1, 2]^T$ . Show this target could have been achieved at  $x_2$ Solution So far I have caculated the controlability matrix to be $$ C =\begin{pmatrix} 0&0&2&2&6&6\\ 0&1&-1&0&-3&-2\\ 1&0&1&0&1&0 \end{pmatrix}. $$ Thus the system is controlable Now putting Cv=x3 i have the 3 equations $$ 2c+2d+6e+6f=2\\ b-c-3e-2f=1\\ c+e+f=2\\ $$ which i have then put into augmented matrix row echleon form which i have found to be $$ a-d-2e-3f=1\\ b+d+f=2\\ c+d+3e+3e=1\\ $$ How do I now solve with so many unknowns? also can you please check my working so far is correct. many thanks",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'continuity', 'control-theory']"
49,Linear ODE repeated eigenvalues how to find more than 2 generalized eigenvectors,Linear ODE repeated eigenvalues how to find more than 2 generalized eigenvectors,,"So I've searched around the web for a few hours now, as (i) $\mathbf A = \begin{pmatrix}2&1\\0&2\end{pmatrix}$ The characteristic polynomial is $(\lambda-2)^2=0$, so $\lambda=2$, repeated. A bit of algebra convinces us that $b=(1,0)^t$ is an eigenvector. Setting $(\mathbf A-\lambda \mathbf I)r=b$, with $r$ and $b$ vectors. Maybe a bit unmotivated but lets proceed. We find that $r=(0,1)^t$ is a solution. Now the solution is $x(t)= c_1*e^{2t}+c_2(te^{2t}b+e^{2t}r)$ However lets say I have 3 or more repeated eigenvalues, or two eigenvalues that are both eigenvalues what then? For instance: $\mathbf A = \begin{pmatrix}1&2&0\\0&1&3/4\\0&0&1\end{pmatrix}$ I find $\lambda=1$ and the vector $b_2=(1,0,0)^t$ is a solution. But I'm not sure how to proceed. Any help would be appreciated.","So I've searched around the web for a few hours now, as (i) $\mathbf A = \begin{pmatrix}2&1\\0&2\end{pmatrix}$ The characteristic polynomial is $(\lambda-2)^2=0$, so $\lambda=2$, repeated. A bit of algebra convinces us that $b=(1,0)^t$ is an eigenvector. Setting $(\mathbf A-\lambda \mathbf I)r=b$, with $r$ and $b$ vectors. Maybe a bit unmotivated but lets proceed. We find that $r=(0,1)^t$ is a solution. Now the solution is $x(t)= c_1*e^{2t}+c_2(te^{2t}b+e^{2t}r)$ However lets say I have 3 or more repeated eigenvalues, or two eigenvalues that are both eigenvalues what then? For instance: $\mathbf A = \begin{pmatrix}1&2&0\\0&1&3/4\\0&0&1\end{pmatrix}$ I find $\lambda=1$ and the vector $b_2=(1,0,0)^t$ is a solution. But I'm not sure how to proceed. Any help would be appreciated.",,"['linear-algebra', 'ordinary-differential-equations']"
50,Non-existence of closed orbits via construction of Liapunov function,Non-existence of closed orbits via construction of Liapunov function,,Show that the system $$\begin{aligned} x^{'} &= y-x^{3}\\ y^{'} &= -x-y^{3}\end{aligned}$$ has no closed orbits by constructing a Liapunov function $$V = ax^{2}+by^{2}$$ with suitable $a$ and $b$ . All I really know is that this is an offshoot on the idea of a gradient field and that i want to show that one side = $0$ and the other side clearly isn't $0$ to derive a contradiction but i am defiantly not well verse'd in the process.,Show that the system has no closed orbits by constructing a Liapunov function with suitable and . All I really know is that this is an offshoot on the idea of a gradient field and that i want to show that one side = and the other side clearly isn't to derive a contradiction but i am defiantly not well verse'd in the process.,\begin{aligned} x^{'} &= y-x^{3}\\ y^{'} &= -x-y^{3}\end{aligned} V = ax^{2}+by^{2} a b 0 0,"['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions']"
51,Solving a second-order nonlinear ordinary differential equation,Solving a second-order nonlinear ordinary differential equation,,"Let's start with equation with two parameters $$y=a x^b.$$ Then we calculate $y'=a b x^{b-1}$ and solve $a=y/x^b$ from original. Substitute that to the derivative and $$y'=b \frac{y}{x}.$$ Then differentiate again and substitute $b=x y'/y$ and we get $$y''=\frac{y'^2}{y}-\frac{y'}{x}.$$ How to solve this ""properly"", without knowing where the final form came from?","Let's start with equation with two parameters $$y=a x^b.$$ Then we calculate $y'=a b x^{b-1}$ and solve $a=y/x^b$ from original. Substitute that to the derivative and $$y'=b \frac{y}{x}.$$ Then differentiate again and substitute $b=x y'/y$ and we get $$y''=\frac{y'^2}{y}-\frac{y'}{x}.$$ How to solve this ""properly"", without knowing where the final form came from?",,['ordinary-differential-equations']
52,Homogeneous Linear Equation General Solution,Homogeneous Linear Equation General Solution,,"I’m having some difficulty understanding the solution to the following differential equation problem. Find a general solution to the given differential equation $4y’’ – 4y’ + y = 0$ The steps I’ve taken in solving this problem was to first find the auxiliary equation and then factor to find the roots. I listed the steps below: $4r^2 – 4r + 1$ $(2r – 1) \cdot (2r-1)$ $\therefore r = \frac{1}{2} \text{is the root}$ Given this information, I supposed that the general solution to the differential equation would be as follows: $y(t) = c_{1} \cdot e^{\frac{1}{2} t}$ But when I look at the back of my textbook, the correct answer is supposed to be $y(t) = c_{1} \cdot e^{\frac{1}{2} t} + c_{2} \cdot te^{\frac{1}{2} t}$ Now I know that understanding the correct solution has something to do with linear independence, but I’m having a hard time getting a deep understanding of what’s going on. Any help would be appreciated in understanding the solution.","I’m having some difficulty understanding the solution to the following differential equation problem. Find a general solution to the given differential equation The steps I’ve taken in solving this problem was to first find the auxiliary equation and then factor to find the roots. I listed the steps below: Given this information, I supposed that the general solution to the differential equation would be as follows: But when I look at the back of my textbook, the correct answer is supposed to be Now I know that understanding the correct solution has something to do with linear independence, but I’m having a hard time getting a deep understanding of what’s going on. Any help would be appreciated in understanding the solution.",4y’’ – 4y’ + y = 0 4r^2 – 4r + 1 (2r – 1) \cdot (2r-1) \therefore r = \frac{1}{2} \text{is the root} y(t) = c_{1} \cdot e^{\frac{1}{2} t} y(t) = c_{1} \cdot e^{\frac{1}{2} t} + c_{2} \cdot te^{\frac{1}{2} t},['ordinary-differential-equations']
53,equilibrium points of an differential equation,equilibrium points of an differential equation,,I want to find the equilibrium points of this differential equation: $y'(t)=ay(1-y)-by$ i have found that $y=\frac{(a-b)}{a }$makes $y'(t)=0$  but i don't know if $y'(t)=0$ is also an equilibrium solution thanks for the help!,I want to find the equilibrium points of this differential equation: $y'(t)=ay(1-y)-by$ i have found that $y=\frac{(a-b)}{a }$makes $y'(t)=0$  but i don't know if $y'(t)=0$ is also an equilibrium solution thanks for the help!,,['ordinary-differential-equations']
54,Lipschitz condition on a second order nonlinear ODE?,Lipschitz condition on a second order nonlinear ODE?,,Preliminaries: Let the matrix norm be $$\sqrt{\sum_{j=1}^n\sum_{i=1}^n a_{ij}^2}=||\mathbf A||.$$ I am trying to prove uniqueness and existence of a second order nonlinear ODE (Ordinary Differential Equation). So I need to show f a function is continuous and satisfies a Lipschitz condition. Take for example the second order nonlinear ODE $$x''=-\cos(x).$$ Now simplify to a first order ODE by letting $$x'=y$$ and $$y'=x''=-\cos(x).$$ So how do I set up the matrix $\mathbf A$ is it $$\begin{pmatrix} 0 & 1 \\ -\cos(x) & 0 \end{pmatrix}$$ How do I show the lipschitz condition for this.  Do I just take the $${||\mathbf Ax_1 - \mathbf Ax_2 || \over |x_1-x_2|}  \le L$$ where L is you Lipschitz constant.,Preliminaries: Let the matrix norm be $$\sqrt{\sum_{j=1}^n\sum_{i=1}^n a_{ij}^2}=||\mathbf A||.$$ I am trying to prove uniqueness and existence of a second order nonlinear ODE (Ordinary Differential Equation). So I need to show f a function is continuous and satisfies a Lipschitz condition. Take for example the second order nonlinear ODE $$x''=-\cos(x).$$ Now simplify to a first order ODE by letting $$x'=y$$ and $$y'=x''=-\cos(x).$$ So how do I set up the matrix $\mathbf A$ is it $$\begin{pmatrix} 0 & 1 \\ -\cos(x) & 0 \end{pmatrix}$$ How do I show the lipschitz condition for this.  Do I just take the $${||\mathbf Ax_1 - \mathbf Ax_2 || \over |x_1-x_2|}  \le L$$ where L is you Lipschitz constant.,,"['real-analysis', 'analysis', 'functional-analysis', 'ordinary-differential-equations']"
55,Solving ODE by linearization,Solving ODE by linearization,,"I am given this ODE: $$\sin(y''+\epsilon y)+y=1+ \sin\epsilon +\epsilon \sin t$$ And it is given that: $$y(0)= \cos \epsilon$$ $$y'(0)=\sin \epsilon$$ where $\epsilon \approx 0$  and $t\in[-\delta, \delta]$ It is clear this ODE should be linearized, but with the additional parameter and the inexplicit $y''$, I'm having trouble understanding how to tackle the problem correctly. Any help would be greatly appreciated!","I am given this ODE: $$\sin(y''+\epsilon y)+y=1+ \sin\epsilon +\epsilon \sin t$$ And it is given that: $$y(0)= \cos \epsilon$$ $$y'(0)=\sin \epsilon$$ where $\epsilon \approx 0$  and $t\in[-\delta, \delta]$ It is clear this ODE should be linearized, but with the additional parameter and the inexplicit $y''$, I'm having trouble understanding how to tackle the problem correctly. Any help would be greatly appreciated!",,['ordinary-differential-equations']
56,Determining if a function satisfies a Lipschitz condition,Determining if a function satisfies a Lipschitz condition,,"I have $f(t, y) = e^{t - y}$. I want to see if $f$ satisfies a Lipschitz condition on $D = \{(t, y): 0 \le t \le 1, \; - \infty < y < \infty   \}$. I am using this definition: A function $f(t, y)$ satisfies a LC in the variable $y$ on the set $D \subset \mathbb R^2$ if there is a constant $L > 0$ such that $|f(t, y_1) - f(t, y_2)| \le L |y_1 - y_2|$ with $(t, y_1)$ and $(t, y_2)$ in $D$. I tried: $$ |e^{t - y_1} - e^{t - y_2} |$$ $$= |e^t(e^{-y_1} - e^{-y_2})|$$ $$= |e^t||e^{-y_1} - e^{-y_2}|$$ $$\le e^1 |y_1 - y_2| $$ At this step, can I say that this does not satisfy a LC, since if $y_1 = -1$ and $y_2 = -2$, the inequality would not be true?","I have $f(t, y) = e^{t - y}$. I want to see if $f$ satisfies a Lipschitz condition on $D = \{(t, y): 0 \le t \le 1, \; - \infty < y < \infty   \}$. I am using this definition: A function $f(t, y)$ satisfies a LC in the variable $y$ on the set $D \subset \mathbb R^2$ if there is a constant $L > 0$ such that $|f(t, y_1) - f(t, y_2)| \le L |y_1 - y_2|$ with $(t, y_1)$ and $(t, y_2)$ in $D$. I tried: $$ |e^{t - y_1} - e^{t - y_2} |$$ $$= |e^t(e^{-y_1} - e^{-y_2})|$$ $$= |e^t||e^{-y_1} - e^{-y_2}|$$ $$\le e^1 |y_1 - y_2| $$ At this step, can I say that this does not satisfy a LC, since if $y_1 = -1$ and $y_2 = -2$, the inequality would not be true?",,"['ordinary-differential-equations', 'numerical-methods']"
57,equilibrium of a flow,equilibrium of a flow,,"Flow $ \phi : \mathbb R \times X \rightarrow X $ on metric space $ (X, d) $, we know there is a sequence of points $ x_n \in X $ s.t. the orbit through $ x_n $ is a periodic orbit with period $T_n > 0$, also $ \lim_{n \to \infty} x_n = x_*, \lim_{n \to \infty} T_n = 0 $ for some $ x_* \in X $. How to prove that $ x_* $ is an equilibrium of the flow?","Flow $ \phi : \mathbb R \times X \rightarrow X $ on metric space $ (X, d) $, we know there is a sequence of points $ x_n \in X $ s.t. the orbit through $ x_n $ is a periodic orbit with period $T_n > 0$, also $ \lim_{n \to \infty} x_n = x_*, \lim_{n \to \infty} T_n = 0 $ for some $ x_* \in X $. How to prove that $ x_* $ is an equilibrium of the flow?",,"['ordinary-differential-equations', 'dynamical-systems']"
58,Solve Differential Equation,Solve Differential Equation,,"Solve differential equation  $$y^2y'^2+2axyy'+(1-a)y^2+ax^2+(a-1)b=0,$$ where $y=y(x)$ and $a,b \in \mathbb{R}.$ My work : Let $y^2=z, \,\,\,z=z(x).$ Then $2yy'=z'$ and our differential equation become  $$\frac{z'^2}{4}+axz'+(1-a)z+ax^2+(a-1)b=0.$$ But, what now? Thanks.","Solve differential equation  $$y^2y'^2+2axyy'+(1-a)y^2+ax^2+(a-1)b=0,$$ where $y=y(x)$ and $a,b \in \mathbb{R}.$ My work : Let $y^2=z, \,\,\,z=z(x).$ Then $2yy'=z'$ and our differential equation become  $$\frac{z'^2}{4}+axz'+(1-a)z+ax^2+(a-1)b=0.$$ But, what now? Thanks.",,['ordinary-differential-equations']
59,A Newtonian differential equation I'm having trouble with,A Newtonian differential equation I'm having trouble with,,"As an illustration to an answer over at Physics, I have been trying to work out what, for $r(0)=1$, $r'(0)=0$ and $r''(t)={{-1}\over{r(t)^2}}$, the (real) solution $r(t)$ is. Actually, I only need to figure out $r'(t)$, with $t$ such that $r(t)=0$. It has been fun to try for a while, but now I need help. Well, I don't need it, but I would certainly be pleased if somebody could open my eyes.","As an illustration to an answer over at Physics, I have been trying to work out what, for $r(0)=1$, $r'(0)=0$ and $r''(t)={{-1}\over{r(t)^2}}$, the (real) solution $r(t)$ is. Actually, I only need to figure out $r'(t)$, with $t$ such that $r(t)=0$. It has been fun to try for a while, but now I need help. Well, I don't need it, but I would certainly be pleased if somebody could open my eyes.",,['ordinary-differential-equations']
60,Behavior of differential equation as argument goes to zero,Behavior of differential equation as argument goes to zero,,"I'm trying to solve a coupled set of ODEs, but before attempting the full numerical solution, I would like to get an idea of what the solution looks like around the origin. The equation at hand is: $$ y''_l - (f'+g')y'_l  + \biggr[ \frac{2-l^2-l}{x^2}e^{2f}  - \frac{2}{x}(f'+g') - \frac{2}{x^2} \biggr]y_l = \frac{4}{x}(f'+g')z_l$$ $y,f,g,z$ are all functions of $x$, which has the domain ($0, X_0$). If I specifically take the $l=2$ case I of course have $$ y''_2 - (f'+g')y'_2  + \biggr[ \frac{-4}{x^2}e^{2f}  - \frac{2}{x}(f'+g') - \frac{2}{x^2} \biggr]y_2 = \frac{4}{x}(f'+g')z_2$$ To avoid issues with singularities, I multiply both sides of the equation by $x^2$, to get, lets call it EQ1. $$ x^2 y''_2 - x^2(f'+g')y'_2  + \biggr[ -4e^{2f}  - x(f'+g') - 2 \biggr]y_2 = 4 x(f'+g')z_2$$ Now if by some magic I know by that as $x \rightarrow 0$, $y_l = x^{l+1}$ so that $y_2 = x^3$. How would I determine what the functions $z_2$ is around the origin? Actually I already know the answer: $z_2$ should also go as $x^3$, but I have not been able to show it. Any help is much appreciated. My attempt at a solution: I have tried to keep things general so I expand $f[x] = 1 + f_1 x + f_2 x^2 +f_3 x^3$, and similarly $g[x] = 1 + g_1 x + g_2 x^2 +g_3 x^3$ where $f_1,f_2,f_3, g_1,g_2,g_3$ are constants. I have kept up to third order because I want to substitute $y_2 = x^3$, so I figured I should take the other functions to that order as well. As for the $e^{2f}$ term, I use a truncated Taylor series for the exponential, $1+\frac{x^2}{2!} + \frac{x^3}{3!}$, into which I substitue my expansion of $f[x]$. After expanding everything out and eliminating terms of higher order than $x^3$ from the right hand side of EQ1, I get something like $constant*x^3$, while the left hand side is third order polynomial times $z_2$. I really just don't know how to proceed. Should I have left higher order terms in the RHS, so that I could divide by a third order polynomial and still end up with $z_2 \propto x^3$. I don't know how this would work because on the RHS I had terms has high as $x^{12}$.","I'm trying to solve a coupled set of ODEs, but before attempting the full numerical solution, I would like to get an idea of what the solution looks like around the origin. The equation at hand is: $$ y''_l - (f'+g')y'_l  + \biggr[ \frac{2-l^2-l}{x^2}e^{2f}  - \frac{2}{x}(f'+g') - \frac{2}{x^2} \biggr]y_l = \frac{4}{x}(f'+g')z_l$$ $y,f,g,z$ are all functions of $x$, which has the domain ($0, X_0$). If I specifically take the $l=2$ case I of course have $$ y''_2 - (f'+g')y'_2  + \biggr[ \frac{-4}{x^2}e^{2f}  - \frac{2}{x}(f'+g') - \frac{2}{x^2} \biggr]y_2 = \frac{4}{x}(f'+g')z_2$$ To avoid issues with singularities, I multiply both sides of the equation by $x^2$, to get, lets call it EQ1. $$ x^2 y''_2 - x^2(f'+g')y'_2  + \biggr[ -4e^{2f}  - x(f'+g') - 2 \biggr]y_2 = 4 x(f'+g')z_2$$ Now if by some magic I know by that as $x \rightarrow 0$, $y_l = x^{l+1}$ so that $y_2 = x^3$. How would I determine what the functions $z_2$ is around the origin? Actually I already know the answer: $z_2$ should also go as $x^3$, but I have not been able to show it. Any help is much appreciated. My attempt at a solution: I have tried to keep things general so I expand $f[x] = 1 + f_1 x + f_2 x^2 +f_3 x^3$, and similarly $g[x] = 1 + g_1 x + g_2 x^2 +g_3 x^3$ where $f_1,f_2,f_3, g_1,g_2,g_3$ are constants. I have kept up to third order because I want to substitute $y_2 = x^3$, so I figured I should take the other functions to that order as well. As for the $e^{2f}$ term, I use a truncated Taylor series for the exponential, $1+\frac{x^2}{2!} + \frac{x^3}{3!}$, into which I substitue my expansion of $f[x]$. After expanding everything out and eliminating terms of higher order than $x^3$ from the right hand side of EQ1, I get something like $constant*x^3$, while the left hand side is third order polynomial times $z_2$. I really just don't know how to proceed. Should I have left higher order terms in the RHS, so that I could divide by a third order polynomial and still end up with $z_2 \propto x^3$. I don't know how this would work because on the RHS I had terms has high as $x^{12}$.",,"['ordinary-differential-equations', 'taylor-expansion']"
61,"For a first order inhomogenous system of linear differential equations, what is a good way of defining resonance?","For a first order inhomogenous system of linear differential equations, what is a good way of defining resonance?",,"I apologize for the title being slightly unclear (at least to me it seems so), so if anyone has a better suggestion feel free to change it. Anyways, for example, when dealing with a second order differential equation of the form: $$ ay'' + by' + cy = f(x) $$ with solutions $y_1(x)$ and $y_2(x)$ we can say that there is resonance in the system provided that $f(x)$ is linearly dependant on $y_1(x)$ and/or $y_2(x)$. Now consider the linear system  $$\textbf{x}' (t)  + \textbf{P}\textbf{x}(t) = \textbf{z}(t)$$ where $\textbf{x}(t)$, $\textbf{z}(t)$ are n-vectors, $\textbf{P}$ is a n-by-n matrix and $\textbf{z}(t)$ is a forcing term of the system. As the solutions are then of the form $$\textbf{x}_i(t)=a_i \textbf{v}_i e^{\lambda_i t} $$ where the $\lambda_i$ and $\textbf{v}_i$ are eigenvalues and eigenvectors of $\textbf{P}$ respectively (assuming the eigenvectors are orthogonal), can we say that resonance is occuring when the $\textbf{z}(t)$ is linearly dependant of the solutions $\textbf{x}_i(t)$?","I apologize for the title being slightly unclear (at least to me it seems so), so if anyone has a better suggestion feel free to change it. Anyways, for example, when dealing with a second order differential equation of the form: $$ ay'' + by' + cy = f(x) $$ with solutions $y_1(x)$ and $y_2(x)$ we can say that there is resonance in the system provided that $f(x)$ is linearly dependant on $y_1(x)$ and/or $y_2(x)$. Now consider the linear system  $$\textbf{x}' (t)  + \textbf{P}\textbf{x}(t) = \textbf{z}(t)$$ where $\textbf{x}(t)$, $\textbf{z}(t)$ are n-vectors, $\textbf{P}$ is a n-by-n matrix and $\textbf{z}(t)$ is a forcing term of the system. As the solutions are then of the form $$\textbf{x}_i(t)=a_i \textbf{v}_i e^{\lambda_i t} $$ where the $\lambda_i$ and $\textbf{v}_i$ are eigenvalues and eigenvectors of $\textbf{P}$ respectively (assuming the eigenvectors are orthogonal), can we say that resonance is occuring when the $\textbf{z}(t)$ is linearly dependant of the solutions $\textbf{x}_i(t)$?",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
62,Determining the effective coefficient in a boundary value problem.,Determining the effective coefficient in a boundary value problem.,,"I'm trying to understand the setup given in Multiscale analytical solutions and homogenization of n-dimensional generalized elliptic equations , pg 41: Given the boundary value problem: $$\frac{d}{dx}\left( -K(x)\frac{du}{dx}\right)=0\text{ in } \Omega\equiv(0,1)$$ $$u(0)=0$$ $$u(1)=1$$ where $K(x)$ is a smooth, strictly positive function in $\Omega$. We can consider the quantity $q^0 \equiv -K(x)\frac{du}{dx}$ to be a constant in $\Omega$, since the equation specifically states that $\frac{d}{dx}\left( q^0\right)=0$.  I understand that we can write: $$\frac{du}{dx}=-\frac{q^0}{K(x)}$$ $$\Rightarrow u=\int_0^x -\frac{q^0}{K(\tau)} d\tau$$. But the author claims that we can further rewrite this as: $$u=K^0\int_0^x\frac{d\tau}{K(\tau)}$$  where  $$K^0=-q^0=\frac{1}{\int_0^1 \frac{d\tau}{K(\tau)}}$$. It is not immediately apparent from the given information that we can deduce that $-q^0=\frac{1}{\int_0^1 \frac{d\tau}{K(\tau)}}$.  How could I have derived this from the given information?","I'm trying to understand the setup given in Multiscale analytical solutions and homogenization of n-dimensional generalized elliptic equations , pg 41: Given the boundary value problem: $$\frac{d}{dx}\left( -K(x)\frac{du}{dx}\right)=0\text{ in } \Omega\equiv(0,1)$$ $$u(0)=0$$ $$u(1)=1$$ where $K(x)$ is a smooth, strictly positive function in $\Omega$. We can consider the quantity $q^0 \equiv -K(x)\frac{du}{dx}$ to be a constant in $\Omega$, since the equation specifically states that $\frac{d}{dx}\left( q^0\right)=0$.  I understand that we can write: $$\frac{du}{dx}=-\frac{q^0}{K(x)}$$ $$\Rightarrow u=\int_0^x -\frac{q^0}{K(\tau)} d\tau$$. But the author claims that we can further rewrite this as: $$u=K^0\int_0^x\frac{d\tau}{K(\tau)}$$  where  $$K^0=-q^0=\frac{1}{\int_0^1 \frac{d\tau}{K(\tau)}}$$. It is not immediately apparent from the given information that we can deduce that $-q^0=\frac{1}{\int_0^1 \frac{d\tau}{K(\tau)}}$.  How could I have derived this from the given information?",,"['ordinary-differential-equations', 'partial-differential-equations', 'potential-theory']"
63,Solve differential equation with variation of parameters,Solve differential equation with variation of parameters,,"I'm having a little trouble understanding how to solve the following differential equation. The equation that has to be solved is: $$y'' - y' - 2y = 2e^{-t}$$ The problem I am having is that I don't understand why they equate that part with the derivatives of the $u$ parameters to $0$. See the problem below Example 13. Problem 2: $y''-y'-2y=2e^{-t}$. The characteristic equation is:   $$r^2-r-2=0\iff (r-2)(r-1)=0\implies y_h(t)=c_1e^{2t}+c2e^{-t}.$$   Suppose that $y(t)=u_1(t)e^{2t}+u_2(t)e^{-t}$, then it follows:   $$ y'(t)=\underbrace{u_1'(t)e^{2t}+u_2'(t)e^{-t}}_{=0}+2u_1(t)e^{2t}-u_2(t)e^{-t} $$ Here they first find the characteristic equation and write down the general solution. They then replace the constants with the parameter ""$u$"" and take the derivative. As you can see, they just say that the derivative part of the $u$ parameter is equal to $0$. But why? How? Where did that come from? I can't find it anywhere in my book. It's probably a facepalm answer but I would really appreciate it.","I'm having a little trouble understanding how to solve the following differential equation. The equation that has to be solved is: $$y'' - y' - 2y = 2e^{-t}$$ The problem I am having is that I don't understand why they equate that part with the derivatives of the $u$ parameters to $0$. See the problem below Example 13. Problem 2: $y''-y'-2y=2e^{-t}$. The characteristic equation is:   $$r^2-r-2=0\iff (r-2)(r-1)=0\implies y_h(t)=c_1e^{2t}+c2e^{-t}.$$   Suppose that $y(t)=u_1(t)e^{2t}+u_2(t)e^{-t}$, then it follows:   $$ y'(t)=\underbrace{u_1'(t)e^{2t}+u_2'(t)e^{-t}}_{=0}+2u_1(t)e^{2t}-u_2(t)e^{-t} $$ Here they first find the characteristic equation and write down the general solution. They then replace the constants with the parameter ""$u$"" and take the derivative. As you can see, they just say that the derivative part of the $u$ parameter is equal to $0$. But why? How? Where did that come from? I can't find it anywhere in my book. It's probably a facepalm answer but I would really appreciate it.",,"['calculus', 'ordinary-differential-equations']"
64,How to determine phase image,How to determine phase image,,"I need to sketch the phase image belonging to the following vector field (I'm sorry, I don't know the exact terms in English, so I have just freely translated them - thanks for sharing the correct terms, if you want to :)) $$F(x,y) = \begin{pmatrix} \frac{1}{2x}\\ yx^2\\ \end{pmatrix}  $$ In order to draft the phase image, I have to know the flow $\phi(t, x, y)$ (again, freely translated) of the vector field. Thus, I solved the differential equations $\frac{d}{dt}x(t) = \frac{1}{2x}$ and $\frac{d}{dt} y(t) = yx^2$, which led me to the following result (using initial valus $y(0) = y_0$ and $x(0) = x_0$: $$\phi(t,x,y) = \begin{pmatrix} \sqrt{t + x^2}\\ ye^{tx^2}\end{pmatrix}$$ My question is: How I am supposed to draft the phase image of this vector flow (it's dependending of three variables, after all) and is it the correct solutiona for the vector flow anyway? Thanks for your help!","I need to sketch the phase image belonging to the following vector field (I'm sorry, I don't know the exact terms in English, so I have just freely translated them - thanks for sharing the correct terms, if you want to :)) $$F(x,y) = \begin{pmatrix} \frac{1}{2x}\\ yx^2\\ \end{pmatrix}  $$ In order to draft the phase image, I have to know the flow $\phi(t, x, y)$ (again, freely translated) of the vector field. Thus, I solved the differential equations $\frac{d}{dt}x(t) = \frac{1}{2x}$ and $\frac{d}{dt} y(t) = yx^2$, which led me to the following result (using initial valus $y(0) = y_0$ and $x(0) = x_0$: $$\phi(t,x,y) = \begin{pmatrix} \sqrt{t + x^2}\\ ye^{tx^2}\end{pmatrix}$$ My question is: How I am supposed to draft the phase image of this vector flow (it's dependending of three variables, after all) and is it the correct solutiona for the vector flow anyway? Thanks for your help!",,"['ordinary-differential-equations', 'multivariable-calculus']"
65,Can I differentiate both side of a differential equation?,Can I differentiate both side of a differential equation?,,"This is a trivial question, please note I'm not a professional in this environment, I'm just learning. Let's suppose I've got this simple eqution: $L\frac{d i(t)}{dt}=-\frac{1}{C}\int i(t) dt$ I suppose you guess what is, but is not so important: I want to solve it as a differential equation and so I need to remove the integral part. I know that the eq before can be written as: $\frac{d^2 i(t)}{dt^2}+\frac{1}{LC}i(t)   = 0$ It seems to me there is a differentiation of both side of the equation and a division of both sides by L. Is my guess correct ? If so, is it correct to say that differentiating both side of a differenctial equation yield to the same result? I suppose we need to state i(t) being differentiable twice, and maybe there is other restrictions...","This is a trivial question, please note I'm not a professional in this environment, I'm just learning. Let's suppose I've got this simple eqution: $L\frac{d i(t)}{dt}=-\frac{1}{C}\int i(t) dt$ I suppose you guess what is, but is not so important: I want to solve it as a differential equation and so I need to remove the integral part. I know that the eq before can be written as: $\frac{d^2 i(t)}{dt^2}+\frac{1}{LC}i(t)   = 0$ It seems to me there is a differentiation of both side of the equation and a division of both sides by L. Is my guess correct ? If so, is it correct to say that differentiating both side of a differenctial equation yield to the same result? I suppose we need to state i(t) being differentiable twice, and maybe there is other restrictions...",,"['ordinary-differential-equations', 'derivatives']"
66,Non-regularity of non-elliptic operator,Non-regularity of non-elliptic operator,,"Let $\Omega\subset\mathbb{R}^d$ be open,and $P(D)=\operatorname{\sum_{|\alpha|\le N}}f_{\alpha}D^{\alpha}$ be an elliptic differential operator. Rudin proves in Functional Analysis Part II the Regularity Theorem for Elliptic Operators that if $f_{\alpha}$ are smooth and $f_{\alpha}$ are constants for $|\alpha|=N$, then $P(D)u$ is locally in $H_s(\Omega)$ if and only if $u$ is locally in $H_{s+N}(\Omega)$, where \begin{equation} H_s=\{u\in D'(\mathbb{R}^d):(1+|y|^2)^{s/2}\hat{u}\in\mathcal{L}^2\}. \end{equation} I know almost nothing about partial differential equations, and am ignorant of  special examples especially. I need some examples showing the conclusion is false for other types of differential operators. That is, something like $Lu=v$ where $v$ is good but $u$ is bad when $L$ is not elliptic. Thanks!","Let $\Omega\subset\mathbb{R}^d$ be open,and $P(D)=\operatorname{\sum_{|\alpha|\le N}}f_{\alpha}D^{\alpha}$ be an elliptic differential operator. Rudin proves in Functional Analysis Part II the Regularity Theorem for Elliptic Operators that if $f_{\alpha}$ are smooth and $f_{\alpha}$ are constants for $|\alpha|=N$, then $P(D)u$ is locally in $H_s(\Omega)$ if and only if $u$ is locally in $H_{s+N}(\Omega)$, where \begin{equation} H_s=\{u\in D'(\mathbb{R}^d):(1+|y|^2)^{s/2}\hat{u}\in\mathcal{L}^2\}. \end{equation} I know almost nothing about partial differential equations, and am ignorant of  special examples especially. I need some examples showing the conclusion is false for other types of differential operators. That is, something like $Lu=v$ where $v$ is good but $u$ is bad when $L$ is not elliptic. Thanks!",,"['calculus', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'distribution-theory']"
67,Differential equation leading behavior,Differential equation leading behavior,,Show that the solution of $x^{3}y''=y$ whose leading behavior as $x\rightarrow0$ is $e^{-2x^{-1/2}}$ is actually given by $x^{3/4}e^{-2x^{-1/2}}$. Do this by writing $y=e^{S(x)}$ and finding the subleading part of S.,Show that the solution of $x^{3}y''=y$ whose leading behavior as $x\rightarrow0$ is $e^{-2x^{-1/2}}$ is actually given by $x^{3/4}e^{-2x^{-1/2}}$. Do this by writing $y=e^{S(x)}$ and finding the subleading part of S.,,"['ordinary-differential-equations', 'asymptotics']"
68,The general solutions of ODE for matrix,The general solutions of ODE for matrix,,"If $p(x)$ is a polynomial with real coefficients, consider the following ODE: $$A'(t)=p(A)$$ where $A$ is an $n \times n$ matrix. My question is, can we have a general solutions for this kind of ODE? (For example, if $n=1$ we can definitely use the separation of variables)","If $p(x)$ is a polynomial with real coefficients, consider the following ODE: $$A'(t)=p(A)$$ where $A$ is an $n \times n$ matrix. My question is, can we have a general solutions for this kind of ODE? (For example, if $n=1$ we can definitely use the separation of variables)",,['ordinary-differential-equations']
69,At most three different eigenvalues,At most three different eigenvalues,,"I have a problem with this first order DE: let $-\infty<a\leq x\leq b<+\infty$ and $$u'(x)+(\lambda+q(x))u(x)=0,\tag{1}$$ where $u$ is a continuous and real valued, while $\lambda$ is a parameter not depending on $x$. A strange non trivial boundary condition is given, namely $$\alpha u(a)+\alpha'u'(a)+\beta u(b)+\beta'u'(b)=0.$$ Then I have to show that this problem admits at most three eigenvalues. What I have tried: basically to convert this problem into a Sturm Liouville problem, however I couldn't conclude anything. Can anybody help me? How to go through this kind of problems? thanks in advance. -Guido-","I have a problem with this first order DE: let $-\infty<a\leq x\leq b<+\infty$ and $$u'(x)+(\lambda+q(x))u(x)=0,\tag{1}$$ where $u$ is a continuous and real valued, while $\lambda$ is a parameter not depending on $x$. A strange non trivial boundary condition is given, namely $$\alpha u(a)+\alpha'u'(a)+\beta u(b)+\beta'u'(b)=0.$$ Then I have to show that this problem admits at most three eigenvalues. What I have tried: basically to convert this problem into a Sturm Liouville problem, however I couldn't conclude anything. Can anybody help me? How to go through this kind of problems? thanks in advance. -Guido-",,['ordinary-differential-equations']
70,Relationship between Legendre polynomials and Legendre functions of the second kind,Relationship between Legendre polynomials and Legendre functions of the second kind,,"I'm taking an ODE course at the moment, and my instructor gave us the following problem: Derive the following formula for Legendre functions $Q_n(x)$ of the second kind: $$Q_n(x) = P_n(x) \int \frac{1}{[P_n(x)]^2 (1-x^2)}dx$$ where $P_n(x)$ is the $n$ -th Legendre polynomial . He introduced Legendre functions in the context of second order ODEs,  but we haven't really used them for anything - moreover, this is the only problem we were assigned that has anything to do with them. As a result, I'm sort of at a loss of where to start. I've tried a couple of things (like using the actual Legendre ODE $$(1-x^2)y^{\prime \prime} - 2xy^{\prime} + n(n+1)y = 0$$ and plugging in the solution $y(x)=a_1P_n(x)+a_2Q_n(x)$ and proceeding from there) but so far, haven't been able to go anywhere. Any help (preferably as elementary as possible) would be much appreciated. Thanks!","I'm taking an ODE course at the moment, and my instructor gave us the following problem: Derive the following formula for Legendre functions of the second kind: where is the -th Legendre polynomial . He introduced Legendre functions in the context of second order ODEs,  but we haven't really used them for anything - moreover, this is the only problem we were assigned that has anything to do with them. As a result, I'm sort of at a loss of where to start. I've tried a couple of things (like using the actual Legendre ODE and plugging in the solution and proceeding from there) but so far, haven't been able to go anywhere. Any help (preferably as elementary as possible) would be much appreciated. Thanks!",Q_n(x) Q_n(x) = P_n(x) \int \frac{1}{[P_n(x)]^2 (1-x^2)}dx P_n(x) n (1-x^2)y^{\prime \prime} - 2xy^{\prime} + n(n+1)y = 0 y(x)=a_1P_n(x)+a_2Q_n(x),"['ordinary-differential-equations', 'special-functions']"
71,Implicit Euler for 2nd Order DE,Implicit Euler for 2nd Order DE,,"I have given the following 2nd order DE: $$ \ddot{q_1} = - \frac{q_1}{|q|^3} $$ $$ \ddot{q_2} = - \frac{q_2}{|q|^3} $$ with $|q| = \sqrt{q_1^2 + q_2^2}$ The assignment is to solve this using explicit and implizit Euler. As for the explicit, I could split this into a system of 1st order DEs using $\dot{q} = p$, thus I had $$ \dot{p_1} = - \frac{q_1}{|q|^3} $$ $$ \dot{p_2} = - \frac{q_2}{|q|^3} $$ $$ \dot{q} = p $$ But now I am stuck at the implicit Euler. The reason is: The equation for this is $\dot{y}_{n+1} = y_n + \Delta t f(y_{n+1})$. My problem now is: I do not have all q or all p in my function f, but $\dot{p} = f(q)$ So how can I formulate this method?","I have given the following 2nd order DE: $$ \ddot{q_1} = - \frac{q_1}{|q|^3} $$ $$ \ddot{q_2} = - \frac{q_2}{|q|^3} $$ with $|q| = \sqrt{q_1^2 + q_2^2}$ The assignment is to solve this using explicit and implizit Euler. As for the explicit, I could split this into a system of 1st order DEs using $\dot{q} = p$, thus I had $$ \dot{p_1} = - \frac{q_1}{|q|^3} $$ $$ \dot{p_2} = - \frac{q_2}{|q|^3} $$ $$ \dot{q} = p $$ But now I am stuck at the implicit Euler. The reason is: The equation for this is $\dot{y}_{n+1} = y_n + \Delta t f(y_{n+1})$. My problem now is: I do not have all q or all p in my function f, but $\dot{p} = f(q)$ So how can I formulate this method?",,"['ordinary-differential-equations', 'numerical-methods']"
72,Differential Equation Past Paper Question,Differential Equation Past Paper Question,,a) Find the general solution to the differential equation: $$2\frac{d^{2}y}{dx^{2}}-4\frac{dy}{dx}+20y=0.$$ b) Find the general solution to the differential equation: $$x^{2}\frac{d^{2}y}{dx^{2}}+4x\frac{dy}{dx}+2y=\frac{1}{x}.$$ I am having trouble with these two differential equations in a past paper I am going through.  Thanks in advance for any replies!,a) Find the general solution to the differential equation: $$2\frac{d^{2}y}{dx^{2}}-4\frac{dy}{dx}+20y=0.$$ b) Find the general solution to the differential equation: $$x^{2}\frac{d^{2}y}{dx^{2}}+4x\frac{dy}{dx}+2y=\frac{1}{x}.$$ I am having trouble with these two differential equations in a past paper I am going through.  Thanks in advance for any replies!,,['ordinary-differential-equations']
73,Reference request: stability theory in infinite dimensional dynamical systems/ partial differential equations,Reference request: stability theory in infinite dimensional dynamical systems/ partial differential equations,,"I am looking for some references (text books, elementary review papers, journal articles etc) regarding the phenomenon of breakdown in stability for (nonlinear) partial differential equations, i.e if we start off with a partial differential equation and we have a steady state solution and suppose we perturb it, how do we analyse if this leads to stability or instability, I suppose we use semigroup methods and spectral theory amongst other things. What are the various tools, techniques etc that are available. When does linear stability imply nonlinear stability? When does breakdown in stability occur? Are there ways/ theorems to that establish necessary / sufficient conditions for nonlinear instability? Thank you.","I am looking for some references (text books, elementary review papers, journal articles etc) regarding the phenomenon of breakdown in stability for (nonlinear) partial differential equations, i.e if we start off with a partial differential equation and we have a steady state solution and suppose we perturb it, how do we analyse if this leads to stability or instability, I suppose we use semigroup methods and spectral theory amongst other things. What are the various tools, techniques etc that are available. When does linear stability imply nonlinear stability? When does breakdown in stability occur? Are there ways/ theorems to that establish necessary / sufficient conditions for nonlinear instability? Thank you.",,"['reference-request', 'ordinary-differential-equations']"
74,hints on solving $ \sin^2 x {d^2y \over dx^2} = 2 y$,hints on solving, \sin^2 x {d^2y \over dx^2} = 2 y,How to solve this differentiation equation? $$\sin^2 x {d^2y \over dx^2} = 2 y$$ I don't know how to begin. Can it be any simpler than this ?,How to solve this differentiation equation? $$\sin^2 x {d^2y \over dx^2} = 2 y$$ I don't know how to begin. Can it be any simpler than this ?,,['ordinary-differential-equations']
75,Help with a partial fraction decomposition,Help with a partial fraction decomposition,,One of my homework problems last week was to find the inverse Laplace transform of the following: $$F(s)=\frac{2s+1}{s^2-2s+2}.$$ The answer is $f(t)= 2e^t \cos t + 3e^t \sin t$. Obviously once you have the decomposed fraction the remainder of the problem is simple but I can't seem to get to that point.  I already turned in the assignment (they are ungraded) but I have a test coming up and want to make sure I'm ready.  Could someone please lay out the steps to decompose $F(s)$?,One of my homework problems last week was to find the inverse Laplace transform of the following: $$F(s)=\frac{2s+1}{s^2-2s+2}.$$ The answer is $f(t)= 2e^t \cos t + 3e^t \sin t$. Obviously once you have the decomposed fraction the remainder of the problem is simple but I can't seem to get to that point.  I already turned in the assignment (they are ungraded) but I have a test coming up and want to make sure I'm ready.  Could someone please lay out the steps to decompose $F(s)$?,,"['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
76,When are attracting sets invariant?,When are attracting sets invariant?,,"Consider a control system of the form $\dot{x}(t) = f(x(t), u(t))$ where $u(t)$ is the control input, $x \in \mathbb{R}^{n}$, $u \in \mathbb{R}^{m}$. Assume $f$ is Lipschitz continuous so that the existence and uniqueness of solutions holds. Following are the definitions of attracting sets and invariant sets that I use: A set $S$ is (control) invariant if for any initial state $x(0) \in S$, there exists a control signal $u(\cdot)$ such that $x(t) \in S$ for all $t \geq 0$. A set $A$ is (weakly) attracting with basin of attraction $B$ if for any initial state $x(0) \in B$, there exists a control signal $u(\cdot)$ such that $x(t)$ converges to $A$ as $t \to +\infty$, that is $\lim_{t \to +\infty} \operatorname{dist} (x(t), A) = 0$. It seems to me that attracting sets and invariant sets are closely related, in particular when $B = \mathbb{R}^{n}$ then in many cases, $A$ is also invariant. My questions are: Is there any example in which an attracting set is not invariant? Under what condition an attracting set is invariant? I come from the control community and not many control textbooks mention these concepts. If there are any good (math) books that discuss these sets well, please let me know.","Consider a control system of the form $\dot{x}(t) = f(x(t), u(t))$ where $u(t)$ is the control input, $x \in \mathbb{R}^{n}$, $u \in \mathbb{R}^{m}$. Assume $f$ is Lipschitz continuous so that the existence and uniqueness of solutions holds. Following are the definitions of attracting sets and invariant sets that I use: A set $S$ is (control) invariant if for any initial state $x(0) \in S$, there exists a control signal $u(\cdot)$ such that $x(t) \in S$ for all $t \geq 0$. A set $A$ is (weakly) attracting with basin of attraction $B$ if for any initial state $x(0) \in B$, there exists a control signal $u(\cdot)$ such that $x(t)$ converges to $A$ as $t \to +\infty$, that is $\lim_{t \to +\infty} \operatorname{dist} (x(t), A) = 0$. It seems to me that attracting sets and invariant sets are closely related, in particular when $B = \mathbb{R}^{n}$ then in many cases, $A$ is also invariant. My questions are: Is there any example in which an attracting set is not invariant? Under what condition an attracting set is invariant? I come from the control community and not many control textbooks mention these concepts. If there are any good (math) books that discuss these sets well, please let me know.",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'set-invariance']"
77,Going in the direction of the gradient,Going in the direction of the gradient,,"First, a motivating example. Suppose $f(x)$ is convex, differentiable, with a single minimum $x^*$. Then the differential equation $$\dot{x}(t) = -\nabla f(x(t))$$ drives $x(t)$ to $x^*$. Now my question is about a generalization of this. Let $f(x), g(x)$ be two smooth convex functions and let ${\cal G}$ be the set of minima of $g(x)$, which we assume to be nonempty. Consider the differential equation  $$ \dot{x}(t) = - \frac{1}{t} \nabla f(x(t)) - \nabla g(x(t))$$ Is it true that this equation drives $x(t)$ to the minimum of $f(x)$ on ${\cal G}$? If not, would it be true if we replaced $1/t$ by a different function, say one which perhaps decays slower or faster? This statement seems to be true in a few simple examples I tried. For example, taking $g(x)=(x_1+x_2-2)^2$ and $f(x)=x_1^2+x_2^2$ and solving the resulting equation numerically, I do get that solutions seem to approach $(1,1)$. Update: I asked this question on MO and received a solution. Unfortunately, the solution was given at a level of mathematical maturity that exceeds my own and I am unable to fill in the details of the arguments. Asking the answerer a followup question has not really clarified matters. I am hoping that someone here on could fill in the details.","First, a motivating example. Suppose $f(x)$ is convex, differentiable, with a single minimum $x^*$. Then the differential equation $$\dot{x}(t) = -\nabla f(x(t))$$ drives $x(t)$ to $x^*$. Now my question is about a generalization of this. Let $f(x), g(x)$ be two smooth convex functions and let ${\cal G}$ be the set of minima of $g(x)$, which we assume to be nonempty. Consider the differential equation  $$ \dot{x}(t) = - \frac{1}{t} \nabla f(x(t)) - \nabla g(x(t))$$ Is it true that this equation drives $x(t)$ to the minimum of $f(x)$ on ${\cal G}$? If not, would it be true if we replaced $1/t$ by a different function, say one which perhaps decays slower or faster? This statement seems to be true in a few simple examples I tried. For example, taking $g(x)=(x_1+x_2-2)^2$ and $f(x)=x_1^2+x_2^2$ and solving the resulting equation numerically, I do get that solutions seem to approach $(1,1)$. Update: I asked this question on MO and received a solution. Unfortunately, the solution was given at a level of mathematical maturity that exceeds my own and I am unable to fill in the details of the arguments. Asking the answerer a followup question has not really clarified matters. I am hoping that someone here on could fill in the details.",,"['ordinary-differential-equations', 'optimization', 'convex-analysis', 'convex-optimization']"
78,Trignometric shifting in ODE. Wolframalpha gives different answer?,Trignometric shifting in ODE. Wolframalpha gives different answer?,,"The ODE looks very identical ( kinda of) The ODE I have is $$y'' - y' + y = 0$$ $$y(0) = 5$$ $$y(1) = y(-1)$$ The solution (nontrivial) I got is $$y = 5e^{t/2}[\cos(\sqrt{3}x/2) + \left ( \frac{1-e}{1+e} \right )\cot(\sqrt{3}/2)\sin(\sqrt{3}x/2)]$$ Wolframalpha http://www.wolframalpha.com/input/?i=y%27%27+-+y%27+%2B+y+%3D+0+%2Cy%280%29+%3D+5+%2Cy%281%29+%3D+y%28-1%29 Could someone explain to me how do I get from my solution to do that weird shifting with the x - 1 part inside the sine functions? Wolframalpha doesn't show the ""simplification"" Thanks","The ODE looks very identical ( kinda of) The ODE I have is $$y'' - y' + y = 0$$ $$y(0) = 5$$ $$y(1) = y(-1)$$ The solution (nontrivial) I got is $$y = 5e^{t/2}[\cos(\sqrt{3}x/2) + \left ( \frac{1-e}{1+e} \right )\cot(\sqrt{3}/2)\sin(\sqrt{3}x/2)]$$ Wolframalpha http://www.wolframalpha.com/input/?i=y%27%27+-+y%27+%2B+y+%3D+0+%2Cy%280%29+%3D+5+%2Cy%281%29+%3D+y%28-1%29 Could someone explain to me how do I get from my solution to do that weird shifting with the x - 1 part inside the sine functions? Wolframalpha doesn't show the ""simplification"" Thanks",,"['ordinary-differential-equations', 'trigonometry']"
79,"The solutions are bounded on $[0,+\infty)$",The solutions are bounded on,"[0,+\infty)","I have to consider the following Cauchy problem $$\begin{cases}u''(t)+u(t)=a(t)u(t)\\ u(0)=1\\ u'(0)=0,\end{cases}$$ where $a\in C([0,+\infty))$ and $$\int_0^{+\infty}|a(t)|\mathrm dt<\infty.$$ I am asked to show that THE solution to the Cauchy problem above is bounded on $[0,+\infty)$. In my attempt I tried to avoid any information about the RHS so I called it $f(t)=a(t)u(t)$ and I used the method of the variation of the parameters. What I got is that the solution to the Cauchy problem $$\begin{cases}u''(t)+u(t)=f(t)\\ u(0)=1\\ u'(0)=0,\end{cases}$$ is given by $$u(t)=-\cos(t)\int_0^t f(\xi)\sin(\xi)\mathrm d\xi+\sin(t)\int_0^tf(\xi)\cos(\xi)\mathrm d\xi+\cos(t).$$ Then I substitute back $a(\xi)u(\xi)=f(\xi)$ in the above and I tried to deduce some consequences, but unfortunately with no success. Am I on the right direction? Can you give any Hint please? even answer are accepted of course. Regards, -Guido-","I have to consider the following Cauchy problem $$\begin{cases}u''(t)+u(t)=a(t)u(t)\\ u(0)=1\\ u'(0)=0,\end{cases}$$ where $a\in C([0,+\infty))$ and $$\int_0^{+\infty}|a(t)|\mathrm dt<\infty.$$ I am asked to show that THE solution to the Cauchy problem above is bounded on $[0,+\infty)$. In my attempt I tried to avoid any information about the RHS so I called it $f(t)=a(t)u(t)$ and I used the method of the variation of the parameters. What I got is that the solution to the Cauchy problem $$\begin{cases}u''(t)+u(t)=f(t)\\ u(0)=1\\ u'(0)=0,\end{cases}$$ is given by $$u(t)=-\cos(t)\int_0^t f(\xi)\sin(\xi)\mathrm d\xi+\sin(t)\int_0^tf(\xi)\cos(\xi)\mathrm d\xi+\cos(t).$$ Then I substitute back $a(\xi)u(\xi)=f(\xi)$ in the above and I tried to deduce some consequences, but unfortunately with no success. Am I on the right direction? Can you give any Hint please? even answer are accepted of course. Regards, -Guido-",,['ordinary-differential-equations']
80,$\frac{dx}{dt}=-\lambda x +\epsilon x(t-a)$ series solution via Laplace method,series solution via Laplace method,\frac{dx}{dt}=-\lambda x +\epsilon x(t-a),"Consider the following equation $$\frac{dx}{dt}=-\lambda x +\epsilon x(t-a), \quad x(0)=1,\quad |\epsilon|\ll1$$ where $a$ and $\lambda$ are positive constants and $x(t-a)$ means the function $x(t)$ evaluated at $t-a$. The task is to obtain a solution as a series in powers of $\epsilon$. The book suggests to show that $x(t)=e^{-\lambda t} w(t)$ satisfies $$\frac{dw}{dt}=\delta w(t-a)\quad w(0)=1,\quad \delta=\epsilon e^{\lambda a}$$ and show that the perturbation expansion is given by the following expression: $$w(t)=1+t\sum_{n=1}^{\infty}\frac{\delta^n}{n!}\left(t-na\right)^{n-1}$$ hence $$x(t)=e^{-\lambda t}\left(1+t\sum_{n=1}^{\infty}\frac{\delta^n}{n!}\left(t-na\right)^{n-1}\right)$$ Since the method given in the solution is basically verification by substitution and proof by induction, I have been looking for a more systematic approach. The presence of time delay in RHS suggests that Laplace transform method could be appropriate here. So I proceed as follows. Apply Laplace transform to both sides of the original equation observing the initial conditions: $$x(t)\fallingdotseq X(s)$$ $$sX(s)-x(0)=-\lambda X(s) +\epsilon e^{-sa}X(s)$$ Solve for $X(s)$: $$X(s)=\frac{x(0)}{s+\lambda-\epsilon e^{-sa}}=\frac{1}{s+\lambda-\epsilon e^{-sa}}$$ Expand in powers of $\epsilon$: $$X(s)=\frac{1}{s+\lambda}\left(\frac{1}{1-\epsilon\frac{e^{-sa}}{s+\lambda}}\right)=\frac{1}{s+\lambda}\left(1+\sum_{n=1}^{\infty}\frac{\epsilon^n e^{-nas}}{(s+\lambda)^n}\right)=\frac{1}{s+\lambda}+\sum_{n=1}^{\infty}\frac{\epsilon^n e^{-nas}}{(s+\lambda)^{n+1}}$$ Taking inverse transforms $$\frac{1}{s+\lambda} \risingdotseq e^{-\lambda t}$$ $$\frac{e^{-nas}}{(s+\lambda)^{n+1}} \risingdotseq \frac{(t-na)^n e^{-\lambda(t-na)}}{n!}$$ But then I lose the factor of $t$ before the sum as compared to the book solution. Where am I making the slip? Thanks in advance.","Consider the following equation $$\frac{dx}{dt}=-\lambda x +\epsilon x(t-a), \quad x(0)=1,\quad |\epsilon|\ll1$$ where $a$ and $\lambda$ are positive constants and $x(t-a)$ means the function $x(t)$ evaluated at $t-a$. The task is to obtain a solution as a series in powers of $\epsilon$. The book suggests to show that $x(t)=e^{-\lambda t} w(t)$ satisfies $$\frac{dw}{dt}=\delta w(t-a)\quad w(0)=1,\quad \delta=\epsilon e^{\lambda a}$$ and show that the perturbation expansion is given by the following expression: $$w(t)=1+t\sum_{n=1}^{\infty}\frac{\delta^n}{n!}\left(t-na\right)^{n-1}$$ hence $$x(t)=e^{-\lambda t}\left(1+t\sum_{n=1}^{\infty}\frac{\delta^n}{n!}\left(t-na\right)^{n-1}\right)$$ Since the method given in the solution is basically verification by substitution and proof by induction, I have been looking for a more systematic approach. The presence of time delay in RHS suggests that Laplace transform method could be appropriate here. So I proceed as follows. Apply Laplace transform to both sides of the original equation observing the initial conditions: $$x(t)\fallingdotseq X(s)$$ $$sX(s)-x(0)=-\lambda X(s) +\epsilon e^{-sa}X(s)$$ Solve for $X(s)$: $$X(s)=\frac{x(0)}{s+\lambda-\epsilon e^{-sa}}=\frac{1}{s+\lambda-\epsilon e^{-sa}}$$ Expand in powers of $\epsilon$: $$X(s)=\frac{1}{s+\lambda}\left(\frac{1}{1-\epsilon\frac{e^{-sa}}{s+\lambda}}\right)=\frac{1}{s+\lambda}\left(1+\sum_{n=1}^{\infty}\frac{\epsilon^n e^{-nas}}{(s+\lambda)^n}\right)=\frac{1}{s+\lambda}+\sum_{n=1}^{\infty}\frac{\epsilon^n e^{-nas}}{(s+\lambda)^{n+1}}$$ Taking inverse transforms $$\frac{1}{s+\lambda} \risingdotseq e^{-\lambda t}$$ $$\frac{e^{-nas}}{(s+\lambda)^{n+1}} \risingdotseq \frac{(t-na)^n e^{-\lambda(t-na)}}{n!}$$ But then I lose the factor of $t$ before the sum as compared to the book solution. Where am I making the slip? Thanks in advance.",,"['ordinary-differential-equations', 'laplace-transform', 'perturbation-theory']"
81,Initial-boundary value problem for PDE,Initial-boundary value problem for PDE,,"I need a little help with solving IBVP for hyperbolic and parabolic equations like these: $$ hyperbolic:  \left\{\begin{matrix} \frac{\partial^2u}{\partial t^2}=\frac{\partial^2 u}{\partial x^2}+1\\ u(0,t) =u(1, t) =0\\ u(x,0) = 0\\ u_t(x,0)=x \end{matrix}\right. \\parabolic:  \left\{\begin{matrix} 3\frac{\partial u}{\partial t}=4\frac{\partial^2 u}{\partial x^2}\\ u(0,t) =u(5, t) =0\\ u(x,0) = x \end{matrix}\right. $$ Don't even know where to start. I'm not actually asking for full solution, but would appreciate it if you could give me direction. Links to tutorials and examples are also highly welcomed. Update: Second equation looks like 1 dimensional heat conduction equation: $u_t = c^2u_{xx}$ with $c=\frac{2\sqrt{3}}{3}$. Ok, for the second one method of separation of variables could be applied. We assume that $u$ can be written as a product of single variable functions of each independent variable, $u(x, t) = X (x)T (t)$. Substituting this guess into the heat equation, we find that $XT′ =c^2X′′T$. Dividing both sides by $c^2$ and $u = XT$, we then get $\frac{1}{c^2}\frac{T'}{T} = \frac{X''}{X} = \lambda$. This leads to two equations: $$ T′ =c^2\lambda T\\ X'' = \lambda X $$ giving $$ T(t) = Ae^{c^2λt}\\ X(x) = c_1e^{\sqrt{\lambda}x} + c_2e^{\sqrt{-\lambda}x}$$ The aim is to force our product solutions to satisfy both the boundary conditions and initial conditions. Is it possible to use this method for the first equation?","I need a little help with solving IBVP for hyperbolic and parabolic equations like these: $$ hyperbolic:  \left\{\begin{matrix} \frac{\partial^2u}{\partial t^2}=\frac{\partial^2 u}{\partial x^2}+1\\ u(0,t) =u(1, t) =0\\ u(x,0) = 0\\ u_t(x,0)=x \end{matrix}\right. \\parabolic:  \left\{\begin{matrix} 3\frac{\partial u}{\partial t}=4\frac{\partial^2 u}{\partial x^2}\\ u(0,t) =u(5, t) =0\\ u(x,0) = x \end{matrix}\right. $$ Don't even know where to start. I'm not actually asking for full solution, but would appreciate it if you could give me direction. Links to tutorials and examples are also highly welcomed. Update: Second equation looks like 1 dimensional heat conduction equation: $u_t = c^2u_{xx}$ with $c=\frac{2\sqrt{3}}{3}$. Ok, for the second one method of separation of variables could be applied. We assume that $u$ can be written as a product of single variable functions of each independent variable, $u(x, t) = X (x)T (t)$. Substituting this guess into the heat equation, we find that $XT′ =c^2X′′T$. Dividing both sides by $c^2$ and $u = XT$, we then get $\frac{1}{c^2}\frac{T'}{T} = \frac{X''}{X} = \lambda$. This leads to two equations: $$ T′ =c^2\lambda T\\ X'' = \lambda X $$ giving $$ T(t) = Ae^{c^2λt}\\ X(x) = c_1e^{\sqrt{\lambda}x} + c_2e^{\sqrt{-\lambda}x}$$ The aim is to force our product solutions to satisfy both the boundary conditions and initial conditions. Is it possible to use this method for the first equation?",,"['ordinary-differential-equations', 'partial-differential-equations']"
82,Two-Point boundary value problem,Two-Point boundary value problem,,"To solve ${d^2y \over dx^2} =f(x)$, $0<x<1$ with $y(0)=\alpha, y(1) = \beta$. We can get a finite difference approximation by taking $$\frac{y_{j+1}-2y_j+y_{j-1}}{h^2} =f_j \\\Rightarrow \frac{1}{2}y_{j+1}-y_j+\frac{1}{2}y_{j-1} =\frac{h^2}{2}f_j$$ Then we get a system of linear equations which can be written as $\left(\begin{array}{ccccc} -1 & 1/2 & 0 &0 &0 \\ 1/2  & -1 & 1/2 &0 &0\\ 0 & 1/2 & -1 &1/2&0\\ 0 & 0 & 1/2&-1 &1/2\\ 0 & 0 & 0 &1/2 &-1\end{array} \right) \cdot \left(\begin{array}{c} y_1 \\ y_2  \\ y_3  \\ y_4  \\ y_5 \end{array} \right) = \left(\begin{array}{c} \frac{h^2}{2}f_1 -\frac{\alpha}{2}\\ \frac{h^2}{2}f_2  \\ \frac{h^2}{2}f_3  \\ \frac{h^2}{2}f_4 \\ \frac{h^2}{2}f_5 -\frac{\beta}{2} \end{array} \right)$ which we can solve by gaussian elimination. I got the idea from http://www2.imperial.ac.uk/~pavl/finite_dff.pdf I understand the above, but how do I calculate/find/write out $f_2$, $f_3$, $f_4$ etc.? What if my DE is$$-{d^2y\over dx^2} + {dy\over dx} =x \Leftrightarrow {d^2y\over dx^2}={dy\over dx}-x $$ with $u(0)=\alpha, u(1) = \beta$? Edit: I rearranged the equation and got $f_{j+1}(1-\frac{h}{2})+y_{j-1}(1+\frac{h}{2})+y_j(-2)=h^2f_j$. $j=1 \Rightarrow f_{2}(1-\frac{h}{2})+y_{0}(1+\frac{h}{2})+y_1(-2)=h^2f_1$. So does $h^2f_1 = \frac{1}{36}\cdot(-\frac{1}{6})?$ And does $h^2f_2 = \frac{1}{36}\cdot(-\frac{2}{6})$?","To solve ${d^2y \over dx^2} =f(x)$, $0<x<1$ with $y(0)=\alpha, y(1) = \beta$. We can get a finite difference approximation by taking $$\frac{y_{j+1}-2y_j+y_{j-1}}{h^2} =f_j \\\Rightarrow \frac{1}{2}y_{j+1}-y_j+\frac{1}{2}y_{j-1} =\frac{h^2}{2}f_j$$ Then we get a system of linear equations which can be written as $\left(\begin{array}{ccccc} -1 & 1/2 & 0 &0 &0 \\ 1/2  & -1 & 1/2 &0 &0\\ 0 & 1/2 & -1 &1/2&0\\ 0 & 0 & 1/2&-1 &1/2\\ 0 & 0 & 0 &1/2 &-1\end{array} \right) \cdot \left(\begin{array}{c} y_1 \\ y_2  \\ y_3  \\ y_4  \\ y_5 \end{array} \right) = \left(\begin{array}{c} \frac{h^2}{2}f_1 -\frac{\alpha}{2}\\ \frac{h^2}{2}f_2  \\ \frac{h^2}{2}f_3  \\ \frac{h^2}{2}f_4 \\ \frac{h^2}{2}f_5 -\frac{\beta}{2} \end{array} \right)$ which we can solve by gaussian elimination. I got the idea from http://www2.imperial.ac.uk/~pavl/finite_dff.pdf I understand the above, but how do I calculate/find/write out $f_2$, $f_3$, $f_4$ etc.? What if my DE is$$-{d^2y\over dx^2} + {dy\over dx} =x \Leftrightarrow {d^2y\over dx^2}={dy\over dx}-x $$ with $u(0)=\alpha, u(1) = \beta$? Edit: I rearranged the equation and got $f_{j+1}(1-\frac{h}{2})+y_{j-1}(1+\frac{h}{2})+y_j(-2)=h^2f_j$. $j=1 \Rightarrow f_{2}(1-\frac{h}{2})+y_{0}(1+\frac{h}{2})+y_1(-2)=h^2f_1$. So does $h^2f_1 = \frac{1}{36}\cdot(-\frac{1}{6})?$ And does $h^2f_2 = \frac{1}{36}\cdot(-\frac{2}{6})$?",,"['ordinary-differential-equations', 'numerical-methods']"
83,matrix differential equation,matrix differential equation,,"Given a matrix $X(t)=e^{tA}$, we know that $X(t)$ is the solution of the following matrix differential equation: $$ \frac{dX(t)}{dt} =X(t) \cdot A .$$ Now could anyone help to construct a matrix differential equation in terms of $Y(t)$, such that $Y(t)=e^{tA} \cdot e^{tB}$ is its solution? (NOTE: the matrices $A$ and $B$ do not commute, meaning that $e^{A+B} \neq e^A \cdot e^B.$ )","Given a matrix $X(t)=e^{tA}$, we know that $X(t)$ is the solution of the following matrix differential equation: $$ \frac{dX(t)}{dt} =X(t) \cdot A .$$ Now could anyone help to construct a matrix differential equation in terms of $Y(t)$, such that $Y(t)=e^{tA} \cdot e^{tB}$ is its solution? (NOTE: the matrices $A$ and $B$ do not commute, meaning that $e^{A+B} \neq e^A \cdot e^B.$ )",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'matrix-equations']"
84,Exercise from Stein with partial differential operator,Exercise from Stein with partial differential operator,,"I have again something from Stein-Shakarchi I would really appreciate some help with. Any references are also welcome! Suppose $L$ is a linear partial differential operator with constant coefficients. Show that when $d \geq 2$, the linear space of solutions $u$ of $L(u)=0$ with $ u \in C^{\infty}(\mathbb{R}^{d})$ is not finitely dimensional. Thanks in advance! EDIT: $L$ takes the form $$L= \sum_{|\alpha| \leq n}{a_{\alpha}\left(\frac{\partial{}}{\partial{x}}\right)^{\alpha}}$$ with $a_{\alpha} \in \mathbb{C}$ constants.","I have again something from Stein-Shakarchi I would really appreciate some help with. Any references are also welcome! Suppose $L$ is a linear partial differential operator with constant coefficients. Show that when $d \geq 2$, the linear space of solutions $u$ of $L(u)=0$ with $ u \in C^{\infty}(\mathbb{R}^{d})$ is not finitely dimensional. Thanks in advance! EDIT: $L$ takes the form $$L= \sum_{|\alpha| \leq n}{a_{\alpha}\left(\frac{\partial{}}{\partial{x}}\right)^{\alpha}}$$ with $a_{\alpha} \in \mathbb{C}$ constants.",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
85,initial problem ODE solve with Laplace Transformation,initial problem ODE solve with Laplace Transformation,,"Consider the initial value problem: $$x''(t) +3x'(t)+2x(t)= t + (ae^{-(t-a)}-t) H(t-a)$$ $$x(0)=0$$ $$x'(0)=0$$ where $a>0$ is constant and $H$ is the Heaviside function. Let $f(t)=t + (ae^{-(t-a)}-t) H(t-a)$ . Applying the Laplace transformation with $ \mathcal{L \{x(t) \}}=X(s)$ and $\mathcal{L \{f(t) \} } =F(s)$ , and using the initial conditions, and after some calculation we get: $X(s) =\dfrac{F(s)}{(s+1)(s+2)}$ , from which we get $x(t) = \mathcal{L}^{-1} \left\{\dfrac{F(s)}{(s+1)(s+2)} \right\}$ . And so it is the case that: $$x(t) = \mathcal{L}^{-1} \left\{\frac{F(s)}{s+1} \right\}  -\mathcal{L}^{-1} \left\{\frac{F(s)}{s+2} \right\}  = e^{-t}*f(t) - e^{-2t}*f(t)= e^{-t}(1-e^{-t})*f(t)$$ where $*$ is the operator of convolution. How can I continue now to do as few calculations as possible? Thanks in advance! Edit: I am interested also if there is a simpler way (using Laplace Transformation). Tell me.","Consider the initial value problem: where is constant and is the Heaviside function. Let . Applying the Laplace transformation with and , and using the initial conditions, and after some calculation we get: , from which we get . And so it is the case that: where is the operator of convolution. How can I continue now to do as few calculations as possible? Thanks in advance! Edit: I am interested also if there is a simpler way (using Laplace Transformation). Tell me.",x''(t) +3x'(t)+2x(t)= t + (ae^{-(t-a)}-t) H(t-a) x(0)=0 x'(0)=0 a>0 H f(t)=t + (ae^{-(t-a)}-t) H(t-a)  \mathcal{L \{x(t) \}}=X(s) \mathcal{L \{f(t) \} } =F(s) X(s) =\dfrac{F(s)}{(s+1)(s+2)} x(t) = \mathcal{L}^{-1} \left\{\dfrac{F(s)}{(s+1)(s+2)} \right\} x(t) = \mathcal{L}^{-1} \left\{\frac{F(s)}{s+1} \right\}  -\mathcal{L}^{-1} \left\{\frac{F(s)}{s+2} \right\}  = e^{-t}*f(t) - e^{-2t}*f(t)= e^{-t}(1-e^{-t})*f(t) *,"['ordinary-differential-equations', 'laplace-transform']"
86,Geometric reason for 3/4-power growth rate?,Geometric reason for 3/4-power growth rate?,,"It seems pretty well established that organisms grow according to a 3/4-power law. For example, Niklas and Enquist, in their paper ""Invariant scaling relationships for interspecific plant biomass production rates and body size,"" PNAS 2001, 98(5):2922 -2927 , say: Annualized rates of growth $G$ scale as the 3/4-power of body mass $M$    over 20 orders of magnitude of $M$ (i.e., $G \propto M^{\frac{3}{4}}$). Does anyone know if there is some geometric reason to expect such a growth-rate law? $$\frac{d M}{d t} \sim M^{\frac{3}{4}}$$ Apparently attempts to derive this growth-rate law from Kleiber's Law , which  claims that metabolic rate scales as $M^{\frac{3}{4}}$, are controversial. So I was wondering if there might be some geometric viewpoint that makes growth proportional to $M^{\frac{3}{4}}$ not unexpected.","It seems pretty well established that organisms grow according to a 3/4-power law. For example, Niklas and Enquist, in their paper ""Invariant scaling relationships for interspecific plant biomass production rates and body size,"" PNAS 2001, 98(5):2922 -2927 , say: Annualized rates of growth $G$ scale as the 3/4-power of body mass $M$    over 20 orders of magnitude of $M$ (i.e., $G \propto M^{\frac{3}{4}}$). Does anyone know if there is some geometric reason to expect such a growth-rate law? $$\frac{d M}{d t} \sim M^{\frac{3}{4}}$$ Apparently attempts to derive this growth-rate law from Kleiber's Law , which  claims that metabolic rate scales as $M^{\frac{3}{4}}$, are controversial. So I was wondering if there might be some geometric viewpoint that makes growth proportional to $M^{\frac{3}{4}}$ not unexpected.",,"['geometry', 'ordinary-differential-equations', 'applications', 'biology']"
87,Existence of Smooth Functions which Satisfy a Condition Regarding $C^\infty$-Functions in $\mathbb{R}^n$,Existence of Smooth Functions which Satisfy a Condition Regarding -Functions in,C^\infty \mathbb{R}^n,"I recently ran into the following exercise: If $R(x)$ is a $C^\infty$-function near the origin in $\mathbb{R}^n$, satisfying $R(0)=0$ and $DR(0)=0$, show that there exist smooth functions $r_{jk}(x)$ such that$$R(x)=\sum r_{jk}(x)x_jx_k.$$ I know that the fundamental theorem of calculus applied to $\varphi(t)=F(x+ty)$ yields$$F(x+y)=F(x)+\int_0^1DF(x+ty)y\text{ }dt,$$provided that $F$ is $C^1$. Using this result, we can write $R(x)=\Phi(x)x$, $\Phi(x)=\int_0^1DR(tx)\text{ }dt$, since $R(0)=0$. Then $\Phi(0)=DR(0)=0$, so we can apply it again to obtain $\Phi(x)=\Psi(x)x$. I get stuck here, however. Do you guys have any ideas on how to begin to prove that such smooth functions exist? Thanks in advance.","I recently ran into the following exercise: If $R(x)$ is a $C^\infty$-function near the origin in $\mathbb{R}^n$, satisfying $R(0)=0$ and $DR(0)=0$, show that there exist smooth functions $r_{jk}(x)$ such that$$R(x)=\sum r_{jk}(x)x_jx_k.$$ I know that the fundamental theorem of calculus applied to $\varphi(t)=F(x+ty)$ yields$$F(x+y)=F(x)+\int_0^1DF(x+ty)y\text{ }dt,$$provided that $F$ is $C^1$. Using this result, we can write $R(x)=\Phi(x)x$, $\Phi(x)=\int_0^1DR(tx)\text{ }dt$, since $R(0)=0$. Then $\Phi(0)=DR(0)=0$, so we can apply it again to obtain $\Phi(x)=\Psi(x)x$. I get stuck here, however. Do you guys have any ideas on how to begin to prove that such smooth functions exist? Thanks in advance.",,['ordinary-differential-equations']
88,Asymptotic behavior of a sequence given by a recurrence relation,Asymptotic behavior of a sequence given by a recurrence relation,,"Original problem is to determine asymptotic behavior of ${a_i}\left( t \right)$ as $t \to \infty $ given by recurrence relations ${a_1}\left( 0 \right) = 1$ ${a_1}\left( t \right) = \frac{{2t + 1}}{{2t}}{a_1}\left( {t - 1} \right)$ ${a_i}\left( t \right) = \frac{{2t + 1}}{{2t}}{a_i}\left( {t - 1} \right) + \frac{1}{{2t}}{a_{i - 1}}\left( {t - 1} \right)$ for $i \in \mathbb{N}$. My attempt at solution: Generating function for ${a_i}$ is $\displaystyle{F_i}\left( x \right) = \frac{{{{\left( {1 - x} \right)}^{ - \frac{3} {2}}}{{\ln }^{i - 1}}\frac{1} {{1 - x}}}} {{\left( {2i - 2} \right)!!}}$. If we rewrite recurrence relations in a different way: ${a_1}\left( t \right) - {a_1}\left( {t - 1} \right) = \frac{1} {{2t}}{a_1}\left( {t - 1} \right)$ ${a_i}\left( t \right) - {a_i}\left( {t - 1} \right) = \frac{1} {{2t}}{a_i}\left( {t - 1} \right) + \frac{1} {{2t}}{a_{i - 1}}\left( {t - 1} \right)$ And then pretend that difference operator is differential operator ${a_1}^\prime \left( t \right) = \frac{1} {{2t}}{a_1}\left( t \right)$ ${a_i}^\prime \left( t \right) = \frac{1} {{2t}}{a_i}\left( t \right) + \frac{1} {{2t}}{a_{i - 1}}\left( t \right)$ by solving a system of differential equations, we get ${a_i}\left( t \right) = \frac{{\sqrt {1 + t} {{\ln }^{i - 1}}\left( {1 + t} \right)}} {{\left( {2i - 2} \right)!!}}$. Now we define ${G_i} = \sum\limits_{t = 0}^\infty  {\frac{{\sqrt {1 + t} {{\ln }^{i - 1}}\left( {1 + t} \right)}} {{\left( {2i - 2} \right)!!}}{x^t}} $. It turns out that (see http://www.math.upenn.edu/~pemantle/papers/twenty.pdf , page 210-211) if ${L_i} = \frac{{{G_i}\left( x \right)}} {{{F_i}\left( x \right)}} = \mathop {\lim }\limits_{x \to 1 - } \frac{{\sum\limits_{t = 1}^\infty  {\sqrt t {x^t}{{\ln }^{i - 1}}t} }} {{x{{\left( {1 - x} \right)}^{ - \frac{3} {2}}}{{\ln }^{i - 1}}\frac{1} {{1 - x}}}} = \mathop {\lim }\limits_{x \to 1 - } \frac{{{{\left( {1 - x} \right)}^{\frac{3} {2}}}\sum\limits_{k = 1}^\infty  {\sqrt k {x^k}{{\ln }^{i - 1}}k} }} {{{{\ln }^{i - 1}}\frac{1} {{1 - x}}}}$ exists and is nonzero, then $\displaystyle\mathop {\lim }\limits_{t \to  + \infty } {a_i}\left( t \right) \cdot \frac{{\left( {2i - 2} \right)!! \cdot {L_i}}} {{\sqrt {t + 1} {{\ln }^{i - 1}}\left( {t + 1} \right)}} = 1$. For several values of ${L_i}$ this seems to be true, which strikes me as unexpected. For $i = 1$ wolfram's mathematica returns ${L_1} = \frac{{\sqrt \pi  }}{2}$, but can not evaluate other expressions. However, they are around 0.9 and rising as $i$ rises. My question is how to analytically evaluate ${L_i}$, or alternatively, disregarding the previous arguments, how to determine asymptotic behavior of a sequence ${a_i}\left( t \right)$? More generally, is it usual that the solutions of difference and corresponding differential equation (given by replacing difference operator with differential operator) have the same asymptotic behavior up to a multiplicative constant? If that is the case and such behavior is well understood, are there any textbooks about it?","Original problem is to determine asymptotic behavior of ${a_i}\left( t \right)$ as $t \to \infty $ given by recurrence relations ${a_1}\left( 0 \right) = 1$ ${a_1}\left( t \right) = \frac{{2t + 1}}{{2t}}{a_1}\left( {t - 1} \right)$ ${a_i}\left( t \right) = \frac{{2t + 1}}{{2t}}{a_i}\left( {t - 1} \right) + \frac{1}{{2t}}{a_{i - 1}}\left( {t - 1} \right)$ for $i \in \mathbb{N}$. My attempt at solution: Generating function for ${a_i}$ is $\displaystyle{F_i}\left( x \right) = \frac{{{{\left( {1 - x} \right)}^{ - \frac{3} {2}}}{{\ln }^{i - 1}}\frac{1} {{1 - x}}}} {{\left( {2i - 2} \right)!!}}$. If we rewrite recurrence relations in a different way: ${a_1}\left( t \right) - {a_1}\left( {t - 1} \right) = \frac{1} {{2t}}{a_1}\left( {t - 1} \right)$ ${a_i}\left( t \right) - {a_i}\left( {t - 1} \right) = \frac{1} {{2t}}{a_i}\left( {t - 1} \right) + \frac{1} {{2t}}{a_{i - 1}}\left( {t - 1} \right)$ And then pretend that difference operator is differential operator ${a_1}^\prime \left( t \right) = \frac{1} {{2t}}{a_1}\left( t \right)$ ${a_i}^\prime \left( t \right) = \frac{1} {{2t}}{a_i}\left( t \right) + \frac{1} {{2t}}{a_{i - 1}}\left( t \right)$ by solving a system of differential equations, we get ${a_i}\left( t \right) = \frac{{\sqrt {1 + t} {{\ln }^{i - 1}}\left( {1 + t} \right)}} {{\left( {2i - 2} \right)!!}}$. Now we define ${G_i} = \sum\limits_{t = 0}^\infty  {\frac{{\sqrt {1 + t} {{\ln }^{i - 1}}\left( {1 + t} \right)}} {{\left( {2i - 2} \right)!!}}{x^t}} $. It turns out that (see http://www.math.upenn.edu/~pemantle/papers/twenty.pdf , page 210-211) if ${L_i} = \frac{{{G_i}\left( x \right)}} {{{F_i}\left( x \right)}} = \mathop {\lim }\limits_{x \to 1 - } \frac{{\sum\limits_{t = 1}^\infty  {\sqrt t {x^t}{{\ln }^{i - 1}}t} }} {{x{{\left( {1 - x} \right)}^{ - \frac{3} {2}}}{{\ln }^{i - 1}}\frac{1} {{1 - x}}}} = \mathop {\lim }\limits_{x \to 1 - } \frac{{{{\left( {1 - x} \right)}^{\frac{3} {2}}}\sum\limits_{k = 1}^\infty  {\sqrt k {x^k}{{\ln }^{i - 1}}k} }} {{{{\ln }^{i - 1}}\frac{1} {{1 - x}}}}$ exists and is nonzero, then $\displaystyle\mathop {\lim }\limits_{t \to  + \infty } {a_i}\left( t \right) \cdot \frac{{\left( {2i - 2} \right)!! \cdot {L_i}}} {{\sqrt {t + 1} {{\ln }^{i - 1}}\left( {t + 1} \right)}} = 1$. For several values of ${L_i}$ this seems to be true, which strikes me as unexpected. For $i = 1$ wolfram's mathematica returns ${L_1} = \frac{{\sqrt \pi  }}{2}$, but can not evaluate other expressions. However, they are around 0.9 and rising as $i$ rises. My question is how to analytically evaluate ${L_i}$, or alternatively, disregarding the previous arguments, how to determine asymptotic behavior of a sequence ${a_i}\left( t \right)$? More generally, is it usual that the solutions of difference and corresponding differential equation (given by replacing difference operator with differential operator) have the same asymptotic behavior up to a multiplicative constant? If that is the case and such behavior is well understood, are there any textbooks about it?",,"['sequences-and-series', 'ordinary-differential-equations', 'limits', 'asymptotics', 'generating-functions']"
89,Solve $y '' + 3y ' + 2y = \frac{1}{e^{x} + 1} $,Solve,y '' + 3y ' + 2y = \frac{1}{e^{x} + 1} ,"I am trying to solve this differential equation. $$y'' + 3y' + 2y = \frac{1}{e^{x} + 1}$$ I know that I have to solve $$x^2+3x+2=0$$ when the solutions of this equation are $(x_1,x_2) = (-2,-1)$ so $$y_0(x) = c_1 e^{-2x}+c_2e^{-x}$$ Then I am looking for a solution of $y(x) = K \frac{1}{e^x +1}$ and I calculate $y'$ and $y''$. The problem is that I end up to nowhere. Can someone help me?","I am trying to solve this differential equation. $$y'' + 3y' + 2y = \frac{1}{e^{x} + 1}$$ I know that I have to solve $$x^2+3x+2=0$$ when the solutions of this equation are $(x_1,x_2) = (-2,-1)$ so $$y_0(x) = c_1 e^{-2x}+c_2e^{-x}$$ Then I am looking for a solution of $y(x) = K \frac{1}{e^x +1}$ and I calculate $y'$ and $y''$. The problem is that I end up to nowhere. Can someone help me?",,['ordinary-differential-equations']
90,What are the Properties of eigenvalues and eigenfunctions of periodic Sturm Liouville problem?,What are the Properties of eigenvalues and eigenfunctions of periodic Sturm Liouville problem?,,I've read for a regular Sturm-Liouville problem for each eigenvalue there corresponds unique eigenfunction. For periodic Sturm Liouville problem Which of the following are true? Each eigenvalue of (periodic Sturm Liouville problem) corresponds to 1. one eigenfunction 2. two eigenfunctions 3. two linearly independent eigenfunctions 4.  two orthogonal eigenfunctions What are the Properties of eigenvalues and eigenfunctions of periodic Sturm Liouville problem? Are these depend on boundary conditions or same for all periodic Sturm Liouville problems?,I've read for a regular Sturm-Liouville problem for each eigenvalue there corresponds unique eigenfunction. For periodic Sturm Liouville problem Which of the following are true? Each eigenvalue of (periodic Sturm Liouville problem) corresponds to 1. one eigenfunction 2. two eigenfunctions 3. two linearly independent eigenfunctions 4.  two orthogonal eigenfunctions What are the Properties of eigenvalues and eigenfunctions of periodic Sturm Liouville problem? Are these depend on boundary conditions or same for all periodic Sturm Liouville problems?,,['ordinary-differential-equations']
91,Solving a simple second order ODE with initial condition,Solving a simple second order ODE with initial condition,,"Okay i've been at it for far too long now. It comes from a bigger question from working with a PDE. I did seperation of variables and now I am stuck near the end of the problem. Here is the ODE in question $$ \Phi''(y)= \lambda^2\cdot\Phi(y)$$ with the following initial condition $$ \Phi'(H)=0$$ where $H$ is a positive number. Also I know $\displaystyle \lambda = \frac{n\pi}{L} >0$. There is another condition but I dont think it can help  $$ \Phi(0) = \begin{cases} 0  & x > L/2  \\ 1 & x < L/2 \end{cases}$$ sorry, i dont know how to do cases in latex and yes, that is an $x$ in the initial condition. Like i said this is a bigger problem that has both x and y. The solution should be $$\Phi(y) = B \cdot \cosh{(\lambda(H-y))}$$ I've tried going through the following general solutions $$\Phi(y) = Ae^{\lambda y} + B e^{-\lambda y}$$ and $$\Phi(y) = A \sinh{\lambda y} + B \cosh{\lambda y}$$ but no luck that way :(","Okay i've been at it for far too long now. It comes from a bigger question from working with a PDE. I did seperation of variables and now I am stuck near the end of the problem. Here is the ODE in question $$ \Phi''(y)= \lambda^2\cdot\Phi(y)$$ with the following initial condition $$ \Phi'(H)=0$$ where $H$ is a positive number. Also I know $\displaystyle \lambda = \frac{n\pi}{L} >0$. There is another condition but I dont think it can help  $$ \Phi(0) = \begin{cases} 0  & x > L/2  \\ 1 & x < L/2 \end{cases}$$ sorry, i dont know how to do cases in latex and yes, that is an $x$ in the initial condition. Like i said this is a bigger problem that has both x and y. The solution should be $$\Phi(y) = B \cdot \cosh{(\lambda(H-y))}$$ I've tried going through the following general solutions $$\Phi(y) = Ae^{\lambda y} + B e^{-\lambda y}$$ and $$\Phi(y) = A \sinh{\lambda y} + B \cosh{\lambda y}$$ but no luck that way :(",,"['ordinary-differential-equations', 'partial-differential-equations']"
92,"Indicial equation power series, ODE and Frobenius theorem","Indicial equation power series, ODE and Frobenius theorem",,"Consider an ODE of the form $y'' + P(x)y' + Q(x)y = 0$ where $(x-x_0)P(x)$ and $(x-x_0)^2 Q(x)$ are analytic functions on $x_0$. The Frobenius Theorem asserts  that there exist at least one solution of the form: $$y=(x - x_0)^r \cdot \sum_{k = 0}^\infty  a_k (x - x_0)^k  $$ where $r$ is an incognit, there is a formula for finding the $r$? In some books appears, when $x_0 = 0$ that changes something? If this is the case, the formula is given by  $$ r(r-1) + p_0 r + q_0  = 0 $$ where $q_0, p_0$ are the first terms of  $$ \begin{align*} P(x) &= (x-x_0)^{-1} \cdot \sum_{k = 0}^\infty p_k (x-x_0)^k     \\  Q(x) &= (x-x_0)^{-2} \cdot \sum_{k = 0}^\infty q_k (x-x_0)^k    \end{align*}  $$ Thanks, I only want to know that.","Consider an ODE of the form $y'' + P(x)y' + Q(x)y = 0$ where $(x-x_0)P(x)$ and $(x-x_0)^2 Q(x)$ are analytic functions on $x_0$. The Frobenius Theorem asserts  that there exist at least one solution of the form: $$y=(x - x_0)^r \cdot \sum_{k = 0}^\infty  a_k (x - x_0)^k  $$ where $r$ is an incognit, there is a formula for finding the $r$? In some books appears, when $x_0 = 0$ that changes something? If this is the case, the formula is given by  $$ r(r-1) + p_0 r + q_0  = 0 $$ where $q_0, p_0$ are the first terms of  $$ \begin{align*} P(x) &= (x-x_0)^{-1} \cdot \sum_{k = 0}^\infty p_k (x-x_0)^k     \\  Q(x) &= (x-x_0)^{-2} \cdot \sum_{k = 0}^\infty q_k (x-x_0)^k    \end{align*}  $$ Thanks, I only want to know that.",,['ordinary-differential-equations']
93,How do I solve this system of differential equations?,How do I solve this system of differential equations?,,"I have to solve the following system of differential equations $$ \begin{align}\dot x &= 2000 - 3xy -2x\\ \dot y &= 3xy - 6y\\ \dot z &= 4y - 2z\end{align} $$ Which steps are required to solve it? The only method I know (so far) is separation of variables, which I (probably) can't use. How do I solve it with Matlab?","I have to solve the following system of differential equations Which steps are required to solve it? The only method I know (so far) is separation of variables, which I (probably) can't use. How do I solve it with Matlab?","
\begin{align}\dot x &= 2000 - 3xy -2x\\
\dot y &= 3xy - 6y\\
\dot z &= 4y - 2z\end{align}
","['ordinary-differential-equations', 'systems-of-equations', 'matlab']"
94,No closed orbits and Liapunov functions,No closed orbits and Liapunov functions,,"What are some techniques to finding/picking Liapunov functions? What does ""by considering straight lines connecting fixed points show that there are no closed orbits"" mean?  If fixed points are saddle point, unstable node, stable node, how are there straight lines connecting the fixed points?","What are some techniques to finding/picking Liapunov functions? What does ""by considering straight lines connecting fixed points show that there are no closed orbits"" mean?  If fixed points are saddle point, unstable node, stable node, how are there straight lines connecting the fixed points?",,"['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions']"
95,General question about differential equations,General question about differential equations,,"I have never studied differential equations and have just run across some notes that involve one.  I have a discrete function P that maps points in time (e.g. day1, day2, day3....) onto some real number on the interval [0, 1]. $P(t+1) = (1-3a)P(t) + a(1-P(t))$ the notes then define the change in P to be: $\Delta P(t) = P(t+1) - P(t) = (1-3a)P(t) + a(1-P(t)) - P(t) = a - 4aP(t)$ Ok, fine.  The notes then go on to say let's instead treat time as continuous.  As such the rate of change of this continuous function is: $ dP(t)/dt =  a - 4aP(t)$ The notes then state that the solution to this differential equation is: $P(t) = 1 / 4 + (P(0) - 1 / 4)e^{-4at}$ My question is for what values of $t$ does the above equation hold?  It appears to me from the broader context of this problem that this equation does not in fact hold for any specific $t$ but only for an unfixed $t$ as $t$ increases without bound.  As such, if I wanted to know the value of $P(500)$ in the continuous case I can't actually use this equation.  Is this understanding correct?","I have never studied differential equations and have just run across some notes that involve one.  I have a discrete function P that maps points in time (e.g. day1, day2, day3....) onto some real number on the interval [0, 1]. $P(t+1) = (1-3a)P(t) + a(1-P(t))$ the notes then define the change in P to be: $\Delta P(t) = P(t+1) - P(t) = (1-3a)P(t) + a(1-P(t)) - P(t) = a - 4aP(t)$ Ok, fine.  The notes then go on to say let's instead treat time as continuous.  As such the rate of change of this continuous function is: $ dP(t)/dt =  a - 4aP(t)$ The notes then state that the solution to this differential equation is: $P(t) = 1 / 4 + (P(0) - 1 / 4)e^{-4at}$ My question is for what values of $t$ does the above equation hold?  It appears to me from the broader context of this problem that this equation does not in fact hold for any specific $t$ but only for an unfixed $t$ as $t$ increases without bound.  As such, if I wanted to know the value of $P(500)$ in the continuous case I can't actually use this equation.  Is this understanding correct?",,['ordinary-differential-equations']
96,Fixed point: a consequence of symmetry?,Fixed point: a consequence of symmetry?,,"I'm studying a dynamical system with $\mathbf{D}_{3}$ symmetry (the symmetry group of an equilateral triangle), which is given by: $\begin{align*} d\mathbf{x}_{0}/dt &= \mathbf{f}(\mathbf{x}_{2}, \mathbf{x}_{0}, \mathbf{x}_{1}) \\ d\mathbf{x}_{1}/dt &= \mathbf{f}(\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}) \\ d\mathbf{x}_{2}/dt &= \mathbf{f}(\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{0}) \end{align*}$, where $\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2} \in \mathbb{R}^{k}$ and $\mathbf{f}(\mathbf{u}, \mathbf{v}, \mathbf{w}) = \mathbf{f}(\mathbf{w}, \mathbf{v}, \mathbf{u})$. Is a fixed point at $(\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}) = (\mathbf{0}, \mathbf{0}, \mathbf{0})$ guaranteed by the symmetry in the problem?","I'm studying a dynamical system with $\mathbf{D}_{3}$ symmetry (the symmetry group of an equilateral triangle), which is given by: $\begin{align*} d\mathbf{x}_{0}/dt &= \mathbf{f}(\mathbf{x}_{2}, \mathbf{x}_{0}, \mathbf{x}_{1}) \\ d\mathbf{x}_{1}/dt &= \mathbf{f}(\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}) \\ d\mathbf{x}_{2}/dt &= \mathbf{f}(\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{0}) \end{align*}$, where $\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2} \in \mathbb{R}^{k}$ and $\mathbf{f}(\mathbf{u}, \mathbf{v}, \mathbf{w}) = \mathbf{f}(\mathbf{w}, \mathbf{v}, \mathbf{u})$. Is a fixed point at $(\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}) = (\mathbf{0}, \mathbf{0}, \mathbf{0})$ guaranteed by the symmetry in the problem?",,"['group-theory', 'ordinary-differential-equations', 'dynamical-systems', 'fixed-point-theorems']"
97,(Probably simple) Differential equation,(Probably simple) Differential equation,,"What are the solutions to the diff eqn: $A\dot{x} + \cos(x) - 1 = B$ subject to boundary conditions $\lim\limits_{x \to -\infty} x(t) = 0$ and $\lim\limits_{x \to +\infty} x(t) = C$, where $ A, B, C$ are constants?","What are the solutions to the diff eqn: $A\dot{x} + \cos(x) - 1 = B$ subject to boundary conditions $\lim\limits_{x \to -\infty} x(t) = 0$ and $\lim\limits_{x \to +\infty} x(t) = C$, where $ A, B, C$ are constants?",,['ordinary-differential-equations']
98,Method of variation of parameters,Method of variation of parameters,,"In the method of Variation of Parameters of solving differential equations, where do the values used for $y_1$, and $y_2$ come from? Are they roots of the homogenous equation? Also, I assume that I first need to standardize my given equation before I can retrieve my $g(x)$. Is this correct?","In the method of Variation of Parameters of solving differential equations, where do the values used for $y_1$, and $y_2$ come from? Are they roots of the homogenous equation? Also, I assume that I first need to standardize my given equation before I can retrieve my $g(x)$. Is this correct?",,['ordinary-differential-equations']
99,fundamental theorem of ODE,fundamental theorem of ODE,,"I have some questions regarding the following theorems: Theorem Let $A$ be an open subset of a Banach space $W$, let $I$ be an open interval in $\mathbb{R}$, and let $F$ be a continuous mapping from $I \times A$ to $W$ which is locally uniformly Lipschitz in its second variable. Then for any point $<t_0,\alpha_0>$ in $I \times A$, for some neighborhood $U$ of $\alpha_0$ and for any sufficiently small interval $J$ containing $t_0$, there is a unique function $f$ from $J$ to $U$ which is a solution of the differential equation passing through the point $<t_0, \alpha_0>$. the solution function is proved to be the fixed point of the mapping $K: f\rightarrow g$, where $f:J \rightarrow  A$ is any continuous mapping and $g:J \rightarrow W$ is defined by  $$ g(t)=\alpha_0+\int^t_{t_0}F(s,f(s))ds$$ the proof starts by choosing a neighborhood $L \times U$ of $<t_0,\alpha_0>$ on which $F$ is bounded and Lipschitz in $\alpha$ uniformly over $t$. Question 1: why not choose $I \times A$ instead of the neighborhood $L \times U$. Is the purpose to make $F$ bounded? A lemma to the above theorem: Let $g_1$ and $g_2$ be any two solutions of $d\alpha/dt=F(t,\alpha)$ through $<t_0,\alpha_0>$. Then $g_1(t)=g_2(t)$ for all t in the intersection $J=J_1 \cap J_2$ of their domains. Proof by contradiction. Otherwise there is a point $s$ in $J$ such that $g_1(s) \neq g_2(s)$. Suppose that $s>t_0$, and set $C=\{t:t>t_0 \mbox{and} g_1(t)\neq g_2(t)\}$ and x = glbC. The set $C$ is open, and therefore $x$ is not in $C$. That is $g_1(x)=g_2(x)$ Question 2: why is the set $C$ open? Call this common value $\alpha$ and apply the theorem to $<x, \alpha>$. With $r$ such that $B_r(\alpha) \subset C$ [ Edit (T.B.): This should be $B_r(\alpha) \subset A$], we choose $\delta$ small enough so that the differential equation has a unique solution $g$ from $(x-\delta,x+\delta)$ to $B_r(\alpha)$ Question 3: why $B_r(\alpha)$ has to be a subset of $C$? Why is this possible? The above lemma allows us to remove the restriction on the range of $f$ Question 4: Can you elaborate this? and why removing the restriction useful? Thanks Edit (T.B.): This is taken from Section 6 of Lynn H. Loomis, Shlomo Sternberg, Advanced Calculus , Jones and Bartlett Publishers, 1990.","I have some questions regarding the following theorems: Theorem Let $A$ be an open subset of a Banach space $W$, let $I$ be an open interval in $\mathbb{R}$, and let $F$ be a continuous mapping from $I \times A$ to $W$ which is locally uniformly Lipschitz in its second variable. Then for any point $<t_0,\alpha_0>$ in $I \times A$, for some neighborhood $U$ of $\alpha_0$ and for any sufficiently small interval $J$ containing $t_0$, there is a unique function $f$ from $J$ to $U$ which is a solution of the differential equation passing through the point $<t_0, \alpha_0>$. the solution function is proved to be the fixed point of the mapping $K: f\rightarrow g$, where $f:J \rightarrow  A$ is any continuous mapping and $g:J \rightarrow W$ is defined by  $$ g(t)=\alpha_0+\int^t_{t_0}F(s,f(s))ds$$ the proof starts by choosing a neighborhood $L \times U$ of $<t_0,\alpha_0>$ on which $F$ is bounded and Lipschitz in $\alpha$ uniformly over $t$. Question 1: why not choose $I \times A$ instead of the neighborhood $L \times U$. Is the purpose to make $F$ bounded? A lemma to the above theorem: Let $g_1$ and $g_2$ be any two solutions of $d\alpha/dt=F(t,\alpha)$ through $<t_0,\alpha_0>$. Then $g_1(t)=g_2(t)$ for all t in the intersection $J=J_1 \cap J_2$ of their domains. Proof by contradiction. Otherwise there is a point $s$ in $J$ such that $g_1(s) \neq g_2(s)$. Suppose that $s>t_0$, and set $C=\{t:t>t_0 \mbox{and} g_1(t)\neq g_2(t)\}$ and x = glbC. The set $C$ is open, and therefore $x$ is not in $C$. That is $g_1(x)=g_2(x)$ Question 2: why is the set $C$ open? Call this common value $\alpha$ and apply the theorem to $<x, \alpha>$. With $r$ such that $B_r(\alpha) \subset C$ [ Edit (T.B.): This should be $B_r(\alpha) \subset A$], we choose $\delta$ small enough so that the differential equation has a unique solution $g$ from $(x-\delta,x+\delta)$ to $B_r(\alpha)$ Question 3: why $B_r(\alpha)$ has to be a subset of $C$? Why is this possible? The above lemma allows us to remove the restriction on the range of $f$ Question 4: Can you elaborate this? and why removing the restriction useful? Thanks Edit (T.B.): This is taken from Section 6 of Lynn H. Loomis, Shlomo Sternberg, Advanced Calculus , Jones and Bartlett Publishers, 1990.",,"['analysis', 'ordinary-differential-equations']"
