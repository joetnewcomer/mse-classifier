,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Decomposing operators into the sum of a quasinilpotent and something else,Decomposing operators into the sum of a quasinilpotent and something else,,"I seem to remember some result of the following sort: Alleged Theorem. Every bounded operator on a separable complex Hilbert space can be decomposed as the sum of a normal operator and a quasinilpotent operator. However, I can't remember if this is the correct statement of the result. Is the theorem as stated obviously true? obviously false? Is there a related result with ""normal"" replaced by some other class of operators? I googled it and found this paper , which is similar to the above stated theorem, but it places spectral restrictions on the class of bounded operators for which it works.","I seem to remember some result of the following sort: Alleged Theorem. Every bounded operator on a separable complex Hilbert space can be decomposed as the sum of a normal operator and a quasinilpotent operator. However, I can't remember if this is the correct statement of the result. Is the theorem as stated obviously true? obviously false? Is there a related result with ""normal"" replaced by some other class of operators? I googled it and found this paper , which is similar to the above stated theorem, but it places spectral restrictions on the class of bounded operators for which it works.",,"['functional-analysis', 'operator-theory']"
1,Prove or disprove existence of a sequence converging weakly to $0$ in an infinite dim Hilbert space,Prove or disprove existence of a sequence converging weakly to  in an infinite dim Hilbert space,0,"This is a problem on an old analysis qual, the prompt is: ""Prove or give a counter example: if $H$ is an infinite dimensional Hilbert space and $0$ is the zero vector in $H$, then there exists a sequence $\{x_n\}$ in $H$ so that $||x_n|| \ge 1$ and $\{x_n\}$ converges weakly to the zero vector $0$ in $H$."" I know that the unit ball is not necessarily weakly compact in an infinite dimensional space if it is not reflexive. But this is specifying the existence of a single sequence, which doesn't say anything about every sequence having a convergent subsequence etc. Since it is a Hilbert space I know it is equivalent to $(x_n,y) \rightarrow 0$ for all $y \in H$ for such a space. I was tempted to assume a countable orthonormal  use Parseval's Identity to show $||x_n||^2$ could be made less than 1, but this would seem to require $(x_n,e_k)$ to converge uniformly (ie independently of $k$ where the $e_k$'s are the orthonormal set). Anyway, I am stuck. Any suggestions? Thanks!","This is a problem on an old analysis qual, the prompt is: ""Prove or give a counter example: if $H$ is an infinite dimensional Hilbert space and $0$ is the zero vector in $H$, then there exists a sequence $\{x_n\}$ in $H$ so that $||x_n|| \ge 1$ and $\{x_n\}$ converges weakly to the zero vector $0$ in $H$."" I know that the unit ball is not necessarily weakly compact in an infinite dimensional space if it is not reflexive. But this is specifying the existence of a single sequence, which doesn't say anything about every sequence having a convergent subsequence etc. Since it is a Hilbert space I know it is equivalent to $(x_n,y) \rightarrow 0$ for all $y \in H$ for such a space. I was tempted to assume a countable orthonormal  use Parseval's Identity to show $||x_n||^2$ could be made less than 1, but this would seem to require $(x_n,e_k)$ to converge uniformly (ie independently of $k$ where the $e_k$'s are the orthonormal set). Anyway, I am stuck. Any suggestions? Thanks!",,"['functional-analysis', 'hilbert-spaces', 'weak-convergence']"
2,"Adjoint operator, bijective","Adjoint operator, bijective",,"Let $A\in\mathcal{L}(X,Y)$, where $X,Y$ are normed vector spaces. Define the adjoint operator $$\begin{array}{ll} A^{\prime}\ : & Y^{\prime}\rightarrow X^{\prime},\\  & G \mapsto A^{\prime}(G)\ =\ G\circ A. \end{array}$$ Its easy to show that $A^{\prime}\in\mathcal{L}(Y^{\prime}, X^{\prime})$ and $\|A\| = \|A^{\prime}\|$. Now, suppose that $A$ is bijective, then show that $A^{\prime}$ is also bijective. First at all, I have proven that $[\mbox{Im}(A)]^{\circ} = \mbox{Ker}(A^{\prime})$, and then $\mbox{Ker}(A^{\prime}) = \{O\}$. Therefore $A^{\prime}$ is injective. My problem is that I don't know how I can show that $A^{\prime}$ is onto. Please I need help. Thanks in advance.","Let $A\in\mathcal{L}(X,Y)$, where $X,Y$ are normed vector spaces. Define the adjoint operator $$\begin{array}{ll} A^{\prime}\ : & Y^{\prime}\rightarrow X^{\prime},\\  & G \mapsto A^{\prime}(G)\ =\ G\circ A. \end{array}$$ Its easy to show that $A^{\prime}\in\mathcal{L}(Y^{\prime}, X^{\prime})$ and $\|A\| = \|A^{\prime}\|$. Now, suppose that $A$ is bijective, then show that $A^{\prime}$ is also bijective. First at all, I have proven that $[\mbox{Im}(A)]^{\circ} = \mbox{Ker}(A^{\prime})$, and then $\mbox{Ker}(A^{\prime}) = \{O\}$. Therefore $A^{\prime}$ is injective. My problem is that I don't know how I can show that $A^{\prime}$ is onto. Please I need help. Thanks in advance.",,"['functional-analysis', 'operator-theory']"
3,Residual spectrum is empty,Residual spectrum is empty,,"I'm following Kreyszig's ""Introductory Functional Analysis with Applications"" and am trying to follow the proof of the following Theorem (9.2-4 on p. 468) For a bounded self-adjoint linear operator $T:H\to H$  on a complex Hilbert space $H$, $$\sigma_{r}\left(T\right)=\emptyset,$$ i.e. its residual spectrum is empty. The proof refers to the following Lemma: Lemma (projection theorem) Suppose that $Y$ is a closed subspace of a Hilbert space $H$. Then  $$ H=Y\oplus Y^{\perp}. $$ Kreysig's begins his argument as follows. "" Suppose, for contradiction, that $\sigma_{r}\left(T\right)$ is non-empty; indeed, pick $\lambda\in\sigma_{r}\left(T\right)$. Then by definition of the residual spectrum, $R_{\lambda}:=T_{\lambda}^{-1}=(T-\lambda I)^{-1}$ exists but its domain is not dense in $H$. "" I understand everything so far. He then goes to argue: ""Hence by the Projection Theorem, there is a $y \neq 0$ in $H$ which is orthogonal to the domain of $R_{\lambda}$."" I don't see why this is. I wonder if someone could very kindly explain?","I'm following Kreyszig's ""Introductory Functional Analysis with Applications"" and am trying to follow the proof of the following Theorem (9.2-4 on p. 468) For a bounded self-adjoint linear operator $T:H\to H$  on a complex Hilbert space $H$, $$\sigma_{r}\left(T\right)=\emptyset,$$ i.e. its residual spectrum is empty. The proof refers to the following Lemma: Lemma (projection theorem) Suppose that $Y$ is a closed subspace of a Hilbert space $H$. Then  $$ H=Y\oplus Y^{\perp}. $$ Kreysig's begins his argument as follows. "" Suppose, for contradiction, that $\sigma_{r}\left(T\right)$ is non-empty; indeed, pick $\lambda\in\sigma_{r}\left(T\right)$. Then by definition of the residual spectrum, $R_{\lambda}:=T_{\lambda}^{-1}=(T-\lambda I)^{-1}$ exists but its domain is not dense in $H$. "" I understand everything so far. He then goes to argue: ""Hence by the Projection Theorem, there is a $y \neq 0$ in $H$ which is orthogonal to the domain of $R_{\lambda}$."" I don't see why this is. I wonder if someone could very kindly explain?",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
4,On $L^p$ and $\ell^p$,On  and,L^p \ell^p,"If a continuous and infinitely differentiable function $f(x): \mathbb{R}\to\mathbb{C}$ is in $L^p$, is it also true that $f(n),\ n\in \mathbb{Z}$ is in $\ell^p$?","If a continuous and infinitely differentiable function $f(x): \mathbb{R}\to\mathbb{C}$ is in $L^p$, is it also true that $f(n),\ n\in \mathbb{Z}$ is in $\ell^p$?",,"['functional-analysis', 'metric-spaces', 'normed-spaces', 'lp-spaces']"
5,Heat equation in bounded domain,Heat equation in bounded domain,,"Let $\Omega \subseteq \mathbb{R}^n$, $n\geq 2$, be a bounded domain with boundary $\partial \Omega \subseteq C^2,v$ outer unit normal vector on $\partial \Omega$, $h  \in L^2(\Omega)$. Let $u \in C\left( \left[ 0,\infty  \right);{{L}^{2}}\left( \Omega  \right) \right)\cap C^1\left( \left( 0,\infty  \right);{{H}^{2}}\left( \Omega  \right) \right)$ be a solution to the Dirichlet problem $$ \left\{ \begin{matrix}    {{u}_{t}}\left( t,x \right)={{\Delta }_{x}}u\left( t,x \right),t\in \left( 0,\infty  \right),\ x\in \Omega   \\    u\left( t,x \right)=0,t\in \left( 0,\infty  \right),\ x\in \partial \Omega   \\    u\left( 0,x \right)=h\left( x \right),\ x\in \Omega   \\ \end{matrix} \right.$$ Let $w \in C\left( \left[ 0,\infty  \right);{{L}^{2}}\left( \Omega  \right) \right)\cap C^1\left( \left( 0,\infty  \right);{{H}^{2}}\left( \Omega  \right) \right)$ be a solution to the Neumann problem $$\left\{ \begin{matrix}    {{w}_{t}}\left( t,x \right)={{\Delta }_{x}}w\left( t,x \right),t\in \left( 0,\infty  \right),\ x\in \Omega   \\    \frac{\partial w}{\partial v}\left( t,x \right)=0,t\in \left( 0,\infty  \right),\ x\in \partial \Omega   \\    w\left( 0,x \right)=h\left( x \right),\ x\in \Omega   \\ \end{matrix} \right.$$ Show that there are bounded linear operators $ {{E}_{D}}\left( t \right):{{L}^{2}}\left( \Omega  \right)\to {{L}^{2}}\left( \Omega  \right)$ and $ {{E}_{N}}\left( t \right):{{L}^{2}}\left( \Omega  \right)\to {{L}^{2}}\left( \Omega  \right)$ such that $u\left( t,\centerdot  \right)={{E}_{D}}\left( t \right)h$ and $w\left( t,\centerdot  \right)={{E}_{N}}\left( t \right)h,t\in \left[ 0,\infty  \right)$ and find their norms. Furthermore show that there are  $0\ne {{h}_{D}}\in {{L}^{2}}\left( \Omega  \right)$ y $0\ne {{h}_{N}}\in {{L}^{2}}\left( \Omega  \right)$ such that ${{E}_{D}}\left( t \right)h_{D}=\left\| {{E}_{D}}\left( t \right) \right\|{{h}_{D}}$ y ${{E}_{N}}\left( t \right)h_{N}=\left\| {{E}_{N}}\left( t \right) \right\|{{h}_{N}}, \forall t\in \left[ 0,\infty  \right)$. Describe those elements ${{h}_{D}}$ and ${{h}_{N}}$ Attempt: Let ${{\left\{ {{\lambda }_{n}^D} \right\}}_{n\in \mathbb{N}}}$ be eigenvalues with their respectives eigenfunctions ${{\left\{ {{\phi }_{n}^D} \right\}}_{n\in \mathbb{N}}}$ orthonormal basis of ${{L}^{2}}\left( \Omega  \right)$, of the Dirichlet Laplacian. Therefore the solution is $u\left( t,x \right)=\sum\limits_{n\in \mathbb{N}}{{{\left\langle \phi _{n}^{D},h \right\rangle }_{{{L}^{2}}\left( \Omega  \right)}}{{e}^{-\lambda _{n}^{D}t}}\phi _{n}^{D}\left( x \right)}$, so that ${{E}_{D}}\left( t \right)\left( f \right)=\sum\limits_{n\in \mathbb{N}}{{{\left\langle \phi _{n}^{D},f \right\rangle }_{{{L}^{2}}\left( \Omega  \right)}}{{e}^{-\lambda _{n}^{D}t}}\phi _{n}^{D}\left( x \right)},f\in {{L}^{2}}\left( \Omega  \right)$, but I don't know how to find the norm.","Let $\Omega \subseteq \mathbb{R}^n$, $n\geq 2$, be a bounded domain with boundary $\partial \Omega \subseteq C^2,v$ outer unit normal vector on $\partial \Omega$, $h  \in L^2(\Omega)$. Let $u \in C\left( \left[ 0,\infty  \right);{{L}^{2}}\left( \Omega  \right) \right)\cap C^1\left( \left( 0,\infty  \right);{{H}^{2}}\left( \Omega  \right) \right)$ be a solution to the Dirichlet problem $$ \left\{ \begin{matrix}    {{u}_{t}}\left( t,x \right)={{\Delta }_{x}}u\left( t,x \right),t\in \left( 0,\infty  \right),\ x\in \Omega   \\    u\left( t,x \right)=0,t\in \left( 0,\infty  \right),\ x\in \partial \Omega   \\    u\left( 0,x \right)=h\left( x \right),\ x\in \Omega   \\ \end{matrix} \right.$$ Let $w \in C\left( \left[ 0,\infty  \right);{{L}^{2}}\left( \Omega  \right) \right)\cap C^1\left( \left( 0,\infty  \right);{{H}^{2}}\left( \Omega  \right) \right)$ be a solution to the Neumann problem $$\left\{ \begin{matrix}    {{w}_{t}}\left( t,x \right)={{\Delta }_{x}}w\left( t,x \right),t\in \left( 0,\infty  \right),\ x\in \Omega   \\    \frac{\partial w}{\partial v}\left( t,x \right)=0,t\in \left( 0,\infty  \right),\ x\in \partial \Omega   \\    w\left( 0,x \right)=h\left( x \right),\ x\in \Omega   \\ \end{matrix} \right.$$ Show that there are bounded linear operators $ {{E}_{D}}\left( t \right):{{L}^{2}}\left( \Omega  \right)\to {{L}^{2}}\left( \Omega  \right)$ and $ {{E}_{N}}\left( t \right):{{L}^{2}}\left( \Omega  \right)\to {{L}^{2}}\left( \Omega  \right)$ such that $u\left( t,\centerdot  \right)={{E}_{D}}\left( t \right)h$ and $w\left( t,\centerdot  \right)={{E}_{N}}\left( t \right)h,t\in \left[ 0,\infty  \right)$ and find their norms. Furthermore show that there are  $0\ne {{h}_{D}}\in {{L}^{2}}\left( \Omega  \right)$ y $0\ne {{h}_{N}}\in {{L}^{2}}\left( \Omega  \right)$ such that ${{E}_{D}}\left( t \right)h_{D}=\left\| {{E}_{D}}\left( t \right) \right\|{{h}_{D}}$ y ${{E}_{N}}\left( t \right)h_{N}=\left\| {{E}_{N}}\left( t \right) \right\|{{h}_{N}}, \forall t\in \left[ 0,\infty  \right)$. Describe those elements ${{h}_{D}}$ and ${{h}_{N}}$ Attempt: Let ${{\left\{ {{\lambda }_{n}^D} \right\}}_{n\in \mathbb{N}}}$ be eigenvalues with their respectives eigenfunctions ${{\left\{ {{\phi }_{n}^D} \right\}}_{n\in \mathbb{N}}}$ orthonormal basis of ${{L}^{2}}\left( \Omega  \right)$, of the Dirichlet Laplacian. Therefore the solution is $u\left( t,x \right)=\sum\limits_{n\in \mathbb{N}}{{{\left\langle \phi _{n}^{D},h \right\rangle }_{{{L}^{2}}\left( \Omega  \right)}}{{e}^{-\lambda _{n}^{D}t}}\phi _{n}^{D}\left( x \right)}$, so that ${{E}_{D}}\left( t \right)\left( f \right)=\sum\limits_{n\in \mathbb{N}}{{{\left\langle \phi _{n}^{D},f \right\rangle }_{{{L}^{2}}\left( \Omega  \right)}}{{e}^{-\lambda _{n}^{D}t}}\phi _{n}^{D}\left( x \right)},f\in {{L}^{2}}\left( \Omega  \right)$, but I don't know how to find the norm.",,"['functional-analysis', 'partial-differential-equations']"
6,When does Stinespring dilation yield a faithful representation?,When does Stinespring dilation yield a faithful representation?,,"Let $A$ be a $C^*$-algebra, $H$ a Hilbert space, $\phi: A \to B(H)$ a completely positive map.  The Stinespring construction yields a triple $(K, V, \pi)$ where $K$ is a Hilbert space, $V: H \to K$ a bounded linear map with $\|V\|^2 = \|\phi(1)\|$, and $\pi: A \to B(K)$ a *-representation satisfying $V^* \pi(\cdot) V = \phi(\cdot)$.  The triple $(K, V, \pi)$ becomes unique up to unitary equivalence once we impose the minimality condition $\overline{\pi(A) VH} = K$. I'm interested in what conditions on $\phi$ are equivalent to (or at least imply) the faithfulness of the representation $\pi$.  Here are a few observations: Clearly, faithfulness of $\phi$ implies faithfulness of $\pi$, since any operator with a nonzero compression is perforce nonzero. The converse fails; for instance, if $\phi$ is a vector state on $M_2(\mathbb{C})$ (so $H$ is one-dimensional), then $\phi$ is not faithful, but Stinespring (=GNS) yields the identity representation. It is possible for $\pi$ not to be faithful.  For instance, if $A = C(X)$ for a compact Hausdorff space $X$, if $\phi$ is the state induced by a probability measure supported on a proper subset $E$ (so again $H$ is one-dimensional), and if $f \in A$ is supported on $X \setminus E$, then $\pi(f) = 0$.  Similarly, if $A = A_1 \oplus A_2$ and $\phi = \phi_1 \oplus 0$, then $\pi(0,a_2) = 0$. In general, just by following through the details of the Stinespring construction, it looks like $\pi(a) = 0$ is equivalent to $$ \forall b \in A: \, \phi(b^* a^* ab) = 0 $$ so that faithfulness of $\pi$ would be equivalent to the statement $$ \forall a \in A:  \Big( \, \forall b \in A: \, \phi(b^* a^* ab) = 0 \Big) \Rightarrow a =0 $$ or equivalently, of course, $$ \forall a \in A: \, a \neq 0 \Rightarrow \exists b \in A: \, \phi(b^* a^* ab) \neq 0. $$ Is there a nicer way to say this, or a way to relate it to named/more easily understood properties of $\phi$?","Let $A$ be a $C^*$-algebra, $H$ a Hilbert space, $\phi: A \to B(H)$ a completely positive map.  The Stinespring construction yields a triple $(K, V, \pi)$ where $K$ is a Hilbert space, $V: H \to K$ a bounded linear map with $\|V\|^2 = \|\phi(1)\|$, and $\pi: A \to B(K)$ a *-representation satisfying $V^* \pi(\cdot) V = \phi(\cdot)$.  The triple $(K, V, \pi)$ becomes unique up to unitary equivalence once we impose the minimality condition $\overline{\pi(A) VH} = K$. I'm interested in what conditions on $\phi$ are equivalent to (or at least imply) the faithfulness of the representation $\pi$.  Here are a few observations: Clearly, faithfulness of $\phi$ implies faithfulness of $\pi$, since any operator with a nonzero compression is perforce nonzero. The converse fails; for instance, if $\phi$ is a vector state on $M_2(\mathbb{C})$ (so $H$ is one-dimensional), then $\phi$ is not faithful, but Stinespring (=GNS) yields the identity representation. It is possible for $\pi$ not to be faithful.  For instance, if $A = C(X)$ for a compact Hausdorff space $X$, if $\phi$ is the state induced by a probability measure supported on a proper subset $E$ (so again $H$ is one-dimensional), and if $f \in A$ is supported on $X \setminus E$, then $\pi(f) = 0$.  Similarly, if $A = A_1 \oplus A_2$ and $\phi = \phi_1 \oplus 0$, then $\pi(0,a_2) = 0$. In general, just by following through the details of the Stinespring construction, it looks like $\pi(a) = 0$ is equivalent to $$ \forall b \in A: \, \phi(b^* a^* ab) = 0 $$ so that faithfulness of $\pi$ would be equivalent to the statement $$ \forall a \in A:  \Big( \, \forall b \in A: \, \phi(b^* a^* ab) = 0 \Big) \Rightarrow a =0 $$ or equivalently, of course, $$ \forall a \in A: \, a \neq 0 \Rightarrow \exists b \in A: \, \phi(b^* a^* ab) \neq 0. $$ Is there a nicer way to say this, or a way to relate it to named/more easily understood properties of $\phi$?",,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
7,Understanding why the positive bounded linear functionals on $C_0(X)$ are given by integration against finite Radon measures.,Understanding why the positive bounded linear functionals on  are given by integration against finite Radon measures.,C_0(X),"Folland says in his chapter on the dual of $C_0(X)$ (continuous functions on $X$ vanishing at $\infty$ ): We recall that for any LCH space $X$ , $C_0(X)$ is the uniform closure of $C_c(X)$ , and hence if $\mu$ is a Radon measure on $X$ , the functional $I(f)=\int f\ d\mu$ extends continuously to $C_0(X)$ iff it is bounded with respect to the uniform norm . In view of the equality $$\mu(X)=\sup \big\{ \int f\ d\mu: f\in C_c(X), 0\le f\le 1 \big\}$$ together with the fact that $|\int f\ d\mu| \le \int |f|\ d\mu$ , this happens precisely when $\mu(X)<\infty$ , in which case $\mu(X)$ is the operator norm of $I$ . We have therefore identified the positive bounded linear functionals on $C_0(X)$ : they are given by integration against finite Radon measures. I feel like an idiot but I just can't seem to parse this passage. Can someone link me to another explanation of this? I'm specifically having trouble understanding why the first ""iff"" statement is true and why the finiteness of the measure is implied exactly. I just keep reading it and it's not sinking in. Forgive me if this is not a good question.","Folland says in his chapter on the dual of (continuous functions on vanishing at ): We recall that for any LCH space , is the uniform closure of , and hence if is a Radon measure on , the functional extends continuously to iff it is bounded with respect to the uniform norm . In view of the equality together with the fact that , this happens precisely when , in which case is the operator norm of . We have therefore identified the positive bounded linear functionals on : they are given by integration against finite Radon measures. I feel like an idiot but I just can't seem to parse this passage. Can someone link me to another explanation of this? I'm specifically having trouble understanding why the first ""iff"" statement is true and why the finiteness of the measure is implied exactly. I just keep reading it and it's not sinking in. Forgive me if this is not a good question.","C_0(X) X \infty X C_0(X) C_c(X) \mu X I(f)=\int f\ d\mu C_0(X) \mu(X)=\sup \big\{ \int f\ d\mu: f\in C_c(X), 0\le f\le 1 \big\} |\int f\ d\mu| \le \int |f|\ d\mu \mu(X)<\infty \mu(X) I C_0(X)",[]
8,Ideals in Commutative Banach Algebras,Ideals in Commutative Banach Algebras,,"Suppose that $A$ is a natural Banach function algebra on $K$, a compact Hausdorff space.  So $A$ is realised as an algebra of continuous functions on $K$, is a Banach algebra for some norm (necessarily dominating the supremum norm) and each character on $A$ is given by evaluation at a point of $K$. If $F\subseteq K$ is closed, then $$ I(F)=\{f\in A : f(k)=0 \ (k\in F) \}$$ is a closed ideal in $A$.  If e.g. $A=C(K)$ then every closed ideal is of this form. What's a simple example of an $A$ where not every closed ideal is of this form? If I look in Bonsall+Duncan, I find that the Disc Algebra is an example.  But quite a bit of theory is needed to show this.  I'd like an easy example which I can explain to students.  For bonus marks: Can we find an $A$ which is conjugate closed?","Suppose that $A$ is a natural Banach function algebra on $K$, a compact Hausdorff space.  So $A$ is realised as an algebra of continuous functions on $K$, is a Banach algebra for some norm (necessarily dominating the supremum norm) and each character on $A$ is given by evaluation at a point of $K$. If $F\subseteq K$ is closed, then $$ I(F)=\{f\in A : f(k)=0 \ (k\in F) \}$$ is a closed ideal in $A$.  If e.g. $A=C(K)$ then every closed ideal is of this form. What's a simple example of an $A$ where not every closed ideal is of this form? If I look in Bonsall+Duncan, I find that the Disc Algebra is an example.  But quite a bit of theory is needed to show this.  I'd like an easy example which I can explain to students.  For bonus marks: Can we find an $A$ which is conjugate closed?",,"['functional-analysis', 'banach-algebras']"
9,Monotone convergence to a fixpoint in a Banach space,Monotone convergence to a fixpoint in a Banach space,,"Let $\mathscr X$ be a complete separable metric space and $\mathbb B$ be the Banach space of all real-valued bounded measurable functions on $\mathscr X$. The partial order on this space is introduced by  $$ f\leq g \text{ iff }f(x)\leq g(x)\text{ for all }x\in \mathscr X. $$ The operator $\mathscr A:\mathbb B\to\mathbb B$ is called monotone if $f\leq g$ implies $\mathscr Af\leq \mathscr Ag$, such operator is not necessary linear. Let us consider the function $f_0\in \mathbb B$ such that $\mathscr Af_0\geq f_0$ and construct the sequence $f_{n+1} = \mathscr A f_n$. Clearly, for any fixed $x\in \mathscr X$ the limit $\lim\limits_{n}f_n(x)$ exists (though it may be infinite) and the convergence is monotone. Let us assume that for any $x\in\mathscr X$ the limit is finite and denote it by $f(x)$. Is it true that $$ f = \mathscr Af\quad? $$","Let $\mathscr X$ be a complete separable metric space and $\mathbb B$ be the Banach space of all real-valued bounded measurable functions on $\mathscr X$. The partial order on this space is introduced by  $$ f\leq g \text{ iff }f(x)\leq g(x)\text{ for all }x\in \mathscr X. $$ The operator $\mathscr A:\mathbb B\to\mathbb B$ is called monotone if $f\leq g$ implies $\mathscr Af\leq \mathscr Ag$, such operator is not necessary linear. Let us consider the function $f_0\in \mathbb B$ such that $\mathscr Af_0\geq f_0$ and construct the sequence $f_{n+1} = \mathscr A f_n$. Clearly, for any fixed $x\in \mathscr X$ the limit $\lim\limits_{n}f_n(x)$ exists (though it may be infinite) and the convergence is monotone. Let us assume that for any $x\in\mathscr X$ the limit is finite and denote it by $f(x)$. Is it true that $$ f = \mathscr Af\quad? $$",,"['functional-analysis', 'banach-spaces']"
10,Uniqueness of the derivative in locally convex topological vector space,Uniqueness of the derivative in locally convex topological vector space,,"I need a hint of proof of uniqueness of the derivative in locally convex topological vector space (it's asserted in Lang's ""Introduction to differentiable manifolds""). Define derivative of a function $f: E \to F$ between two topological vector spaces at the point $x_0$ as a linear operator $f'(x_0) \in L(E, F)$ such that ${}^2\!\!f(y) := f(x_0 + y) - f(x_0) - f'(x_0) y$ is tangent to zero (a function $\varphi$ is tangent to zero if for any neighborhood of zero $W \subset F$ there exists a neighborhood of zero $V \subset E$ such that $\varphi(tV) = o(t)W$). Now suppose that two operators $A_1$ and $A_2$ satisfy the condition of the derviative. I need to prove then that $A_1 = A_2$. It is easy to see that there must be a neighborhood of zero $V \subset E$ such that $(A_1 - A_2)V = 0$, so we have $\operatorname{span}V \subset \ker (A_1 - A_2) \subset E$, so it is sufficient to prove that $\operatorname{span}V = E$. In case of Banach space it is very easy (any non-empty open set must contain a ball), but in a more general case it seems that this attempt fails (consider a discrete space), and I can't see another strategy. Is the derivative in fact unique or is some condition stronger than what is stated required for it to be unique?","I need a hint of proof of uniqueness of the derivative in locally convex topological vector space (it's asserted in Lang's ""Introduction to differentiable manifolds""). Define derivative of a function $f: E \to F$ between two topological vector spaces at the point $x_0$ as a linear operator $f'(x_0) \in L(E, F)$ such that ${}^2\!\!f(y) := f(x_0 + y) - f(x_0) - f'(x_0) y$ is tangent to zero (a function $\varphi$ is tangent to zero if for any neighborhood of zero $W \subset F$ there exists a neighborhood of zero $V \subset E$ such that $\varphi(tV) = o(t)W$). Now suppose that two operators $A_1$ and $A_2$ satisfy the condition of the derviative. I need to prove then that $A_1 = A_2$. It is easy to see that there must be a neighborhood of zero $V \subset E$ such that $(A_1 - A_2)V = 0$, so we have $\operatorname{span}V \subset \ker (A_1 - A_2) \subset E$, so it is sufficient to prove that $\operatorname{span}V = E$. In case of Banach space it is very easy (any non-empty open set must contain a ball), but in a more general case it seems that this attempt fails (consider a discrete space), and I can't see another strategy. Is the derivative in fact unique or is some condition stronger than what is stated required for it to be unique?",,"['functional-analysis', 'topological-vector-spaces', 'locally-convex-spaces']"
11,Kernel of an operator defined by a power series,Kernel of an operator defined by a power series,,"Let $K = K(x,y) \in L^{2}(\mathbb{R}^{d}\times \mathbb{R}^{d})$ . This function defines an integral operator on $L^{2}(\mathbb{R}^{d})$ , which I denote by $K$ , as follows: $$(Kf)(x) = \int_{\mathbb{R}^{d}}K(x,y)f(y)dy$$ This operator $K$ is proven to be a Hilbert-Schmidt operator, with Hilbert-Schmidt norm $\|K\|_{\mathcal{J}_{2}}$ . Analogously, the complex conjugate $\overline{K} = \overline{K(x,y)}$ induces an operator $\overline{K}$ on $L^{2}(\mathbb{R}^{d})$ . Next, consider the operators $A_{K}$ and $B_{K}$ defined by its power series: $$A_{K} = \sum_{n=0}^{\infty}\frac{(K\overline{K})^{n}K}{(2n+1)!} \quad \mbox{and} \quad B_{K} = \sum_{n=1}^{\infty}\frac{(K\overline{K})^{n}}{(2n)!}$$ These two series converge in the Hilbert-Schmidt norm, so these operators are well-defined as Hilbert-Schmidt operators. Hence, both $A_{K}$ and $B_{K}$ can be written as integral operators, with integral kernels $A_{K}(x,y)$ and $B_{K}(x,y)$ , that is: $$(A_{K}f)(x) = \int_{\mathbb{R}^{d}}A_{K}(x,y)f(y)dy \quad \mbox{and} \quad (B_{K}f)(x) = \int_{\mathbb{R}^{d}}B_{K}(x,y)f(y)dy $$ Questions: Suppose that $K$ (and, consequently, $\overline{K}$ ) is symmetric in the sense that $K(x,y) = K(y,x)$ for every $x,y \in \mathbb{R}^{d}$ . Is it true that $A_{K}(x,y) = A_{K}(y,x)$ and $B_{K}(x,y) = B_{K}(y,x)$ as well? What about the product $KA_{K}$ and $KB_{K}$ ? Do they satisfy $(KA_{K})(x,y) = (KA_{K})(y,x)$ and $(KB_{K})(x,y) = (KB_{K})(y,x)$ ? Note: the notation $K\bar{K}$ means composite maps: $$((K\bar{K})f)(x) = \int_{\mathbb{R}^{d}}dy K(x,y) \int_{\mathbb{R}^{d}}dz \overline{K(y,z)}f(z)$$","Let . This function defines an integral operator on , which I denote by , as follows: This operator is proven to be a Hilbert-Schmidt operator, with Hilbert-Schmidt norm . Analogously, the complex conjugate induces an operator on . Next, consider the operators and defined by its power series: These two series converge in the Hilbert-Schmidt norm, so these operators are well-defined as Hilbert-Schmidt operators. Hence, both and can be written as integral operators, with integral kernels and , that is: Questions: Suppose that (and, consequently, ) is symmetric in the sense that for every . Is it true that and as well? What about the product and ? Do they satisfy and ? Note: the notation means composite maps:","K = K(x,y) \in L^{2}(\mathbb{R}^{d}\times \mathbb{R}^{d}) L^{2}(\mathbb{R}^{d}) K (Kf)(x) = \int_{\mathbb{R}^{d}}K(x,y)f(y)dy K \|K\|_{\mathcal{J}_{2}} \overline{K} = \overline{K(x,y)} \overline{K} L^{2}(\mathbb{R}^{d}) A_{K} B_{K} A_{K} = \sum_{n=0}^{\infty}\frac{(K\overline{K})^{n}K}{(2n+1)!} \quad \mbox{and} \quad B_{K} = \sum_{n=1}^{\infty}\frac{(K\overline{K})^{n}}{(2n)!} A_{K} B_{K} A_{K}(x,y) B_{K}(x,y) (A_{K}f)(x) = \int_{\mathbb{R}^{d}}A_{K}(x,y)f(y)dy \quad \mbox{and} \quad (B_{K}f)(x) = \int_{\mathbb{R}^{d}}B_{K}(x,y)f(y)dy  K \overline{K} K(x,y) = K(y,x) x,y \in \mathbb{R}^{d} A_{K}(x,y) = A_{K}(y,x) B_{K}(x,y) = B_{K}(y,x) KA_{K} KB_{K} (KA_{K})(x,y) = (KA_{K})(y,x) (KB_{K})(x,y) = (KB_{K})(y,x) K\bar{K} ((K\bar{K})f)(x) = \int_{\mathbb{R}^{d}}dy K(x,y) \int_{\mathbb{R}^{d}}dz \overline{K(y,z)}f(z)","['functional-analysis', 'analysis', 'operator-theory']"
12,Perturbing a measure $\mu$ so that the integral $\int fd\mu$ becomes nonzero,Perturbing a measure  so that the integral  becomes nonzero,\mu \int fd\mu,"Let $X$ be a compact subset of $\mathbb{R}^d$ , let $f\in L^2(X)$ be an unknown function with $\lVert f\rVert_2=1$ for which we may assume suitable regularity (e.g. Lipschitz, $C^1$ ), and let $\mu$ be a Borel probability measure on $X$ . Suppose $\int fd\mu=0$ . Is there an efficient (in an algorithmic sense, time complexity not exponential in $d$ ) way to find a perturbation $\mu'$ close to $\mu$ such that $\int fd\mu'\neq 0$ ? For example, we could divide the domain into $N^d$ hypercubes $A_1,A_2,\cdots$ and try the density proportional to $\mu+\epsilon 1_{A_i}$ until we hit a region of nonzero $f$ . Then the integral value is guaranteed to change due to Lipschitzity for large enough $N$ . However, this incurs the curse of dimensionality in $d$ . For a similar reason, random perturbations also seem to require exponential time to detect nonzero $f$ . I am unsure if this is inevitable. For context, I am studying the stability of gradient dynamics of a functional on the space of probability measures, and trying to come up with a scheme that will always find an unstable direction to escape to, if one exists. This can be quantified by the magnitude of the perturbation in the direction of an unstable eigenfunction $f$ .","Let be a compact subset of , let be an unknown function with for which we may assume suitable regularity (e.g. Lipschitz, ), and let be a Borel probability measure on . Suppose . Is there an efficient (in an algorithmic sense, time complexity not exponential in ) way to find a perturbation close to such that ? For example, we could divide the domain into hypercubes and try the density proportional to until we hit a region of nonzero . Then the integral value is guaranteed to change due to Lipschitzity for large enough . However, this incurs the curse of dimensionality in . For a similar reason, random perturbations also seem to require exponential time to detect nonzero . I am unsure if this is inevitable. For context, I am studying the stability of gradient dynamics of a functional on the space of probability measures, and trying to come up with a scheme that will always find an unstable direction to escape to, if one exists. This can be quantified by the magnitude of the perturbation in the direction of an unstable eigenfunction .","X \mathbb{R}^d f\in L^2(X) \lVert f\rVert_2=1 C^1 \mu X \int fd\mu=0 d \mu' \mu \int fd\mu'\neq 0 N^d A_1,A_2,\cdots \mu+\epsilon 1_{A_i} f N d f f","['functional-analysis', 'measure-theory', 'optimization', 'calculus-of-variations', 'perturbation-theory']"
13,What is the motivation for Besov spaces?,What is the motivation for Besov spaces?,,I am trying to understand the definition of Besov spaces. With such a complicated definition I wonder what is the motivation behind them and why are they so often used in PDE? What advantage do they give over Sobolev spaces? Are there any nice (hopefully short) references that introduce them?,I am trying to understand the definition of Besov spaces. With such a complicated definition I wonder what is the motivation behind them and why are they so often used in PDE? What advantage do they give over Sobolev spaces? Are there any nice (hopefully short) references that introduce them?,,"['functional-analysis', 'partial-differential-equations', 'function-spaces', 'besov-space']"
14,"Does there exist an everywhere weakly continuous but nowhere strongly continuous function $f : [0,1] \to X$?",Does there exist an everywhere weakly continuous but nowhere strongly continuous function ?,"f : [0,1] \to X","Let $(X, \|\cdot\|)$ be a Banach space and $\mathbb{K}$ be its field of scalars ( $\mathbb{R}$ or $\mathbb{C}$ ). A function $f : [0,1] \to X$ is said to be strongly continuous in $x \in [0,1]$ if it is continuous in $x$ as a function $f : ([0,1], |\cdot|) \to (X, \|\cdot\|)$ , while it is said to be weakly continuous in $x$ if, for all $\Lambda \in X^* := \mathcal{L}(X,\mathbb{K})$ , $\Lambda \circ f : [0,1] \to \mathbb{K}$ is continuous in $x$ . Of course, strong continuity implies weak continuity, and in finite dimension weak continuity implies strong continuity too (it suffices to write $f =: (f_1, f_2, \dots, f_d)$ and to remember that $f$ is continuous iff the $f_k$ s are), but there are functions that are weakly continuous everywhere but not strongly continuous in one point: this post for example features such a function (they chose to make the function go from a domain of $\mathbb{C}$ there but it's very much adaptable to an interval of $\mathbb{R}$ ). and I feel like it should not be hard to have countably many discontinuities by summing countably many functions that are each weakly continuous everywhere and strongly continuous everywhere except in one point each. On the other hand, the existence of nowhere (strongly) continuous functions is guaranteed, for example by picking any non-linear solution of the Cauchy functional equation $g(x+y) = g(x) + g(y)$ and a vector $x$ in $X$ , and then taking $f : t \in [0,1] \mapsto g(t)x \in X$ , and you can probably take Hamel bases of $\mathbb{R}$ and $X$ and make an example with an infinite-dimensional range $f : \sum_i t_i e_i\mapsto \sum_i g(t_i)e'_i$ , but it's hard to see whether those highly irregular functions can make place for a weakly continuous function... As such this got me wondering how ""badly"" can weak continuous functions behave in regards to strong continuity, and as such here is my question: Question : Does there exist $X$ an infinite-dimensional Banach space such that there exists an everywhere weakly continuous yet nowhere strongly continuous function $f : [0,1] \to X$ ? Feel free to edit or re-tag if needed.","Let be a Banach space and be its field of scalars ( or ). A function is said to be strongly continuous in if it is continuous in as a function , while it is said to be weakly continuous in if, for all , is continuous in . Of course, strong continuity implies weak continuity, and in finite dimension weak continuity implies strong continuity too (it suffices to write and to remember that is continuous iff the s are), but there are functions that are weakly continuous everywhere but not strongly continuous in one point: this post for example features such a function (they chose to make the function go from a domain of there but it's very much adaptable to an interval of ). and I feel like it should not be hard to have countably many discontinuities by summing countably many functions that are each weakly continuous everywhere and strongly continuous everywhere except in one point each. On the other hand, the existence of nowhere (strongly) continuous functions is guaranteed, for example by picking any non-linear solution of the Cauchy functional equation and a vector in , and then taking , and you can probably take Hamel bases of and and make an example with an infinite-dimensional range , but it's hard to see whether those highly irregular functions can make place for a weakly continuous function... As such this got me wondering how ""badly"" can weak continuous functions behave in regards to strong continuity, and as such here is my question: Question : Does there exist an infinite-dimensional Banach space such that there exists an everywhere weakly continuous yet nowhere strongly continuous function ? Feel free to edit or re-tag if needed.","(X, \|\cdot\|) \mathbb{K} \mathbb{R} \mathbb{C} f : [0,1] \to X x \in [0,1] x f : ([0,1], |\cdot|) \to (X, \|\cdot\|) x \Lambda \in X^* := \mathcal{L}(X,\mathbb{K}) \Lambda \circ f : [0,1] \to \mathbb{K} x f =: (f_1, f_2, \dots, f_d) f f_k \mathbb{C} \mathbb{R} g(x+y) = g(x) + g(y) x X f : t \in [0,1] \mapsto g(t)x \in X \mathbb{R} X f : \sum_i t_i e_i\mapsto \sum_i g(t_i)e'_i X f : [0,1] \to X","['functional-analysis', 'continuity', 'banach-spaces', 'examples-counterexamples']"
15,Peculiar subspace for a measure on infinite-dimensional separable Banach,Peculiar subspace for a measure on infinite-dimensional separable Banach,,"Let $E$ be an infinite-dimensional separable Banach space, and $P$ be a Borel probability measure on $E$ . There exists $F$ verifying the following conditions: $F$ is in the Borel $\sigma$ -algebra of $E$ , $F$ is a vector subspace of $E$ , $F\subsetneqq E$ , $P(F)=1$ . I'm looking for a proof of this fact, which is Exercise 7 p.82 in this book : As Eric Wofsey points out, $F$ is not necessarily unique, so I've removed this part of the question. Here's the theorem mentioned in the hint: Since $E$ is Polish, $P$ is a Radon measure, hence there exists a separable Borel set $A$ with $P(A)=1$ . However this doesn't help much. Since $E$ is separable, there is an increasing sequence of finite-dimensional subspaces $G_n$ such that $\bigcup_n G_n$ is dense in $E$ , but that doesn't seem to help either.","Let be an infinite-dimensional separable Banach space, and be a Borel probability measure on . There exists verifying the following conditions: is in the Borel -algebra of , is a vector subspace of , , . I'm looking for a proof of this fact, which is Exercise 7 p.82 in this book : As Eric Wofsey points out, is not necessarily unique, so I've removed this part of the question. Here's the theorem mentioned in the hint: Since is Polish, is a Radon measure, hence there exists a separable Borel set with . However this doesn't help much. Since is separable, there is an increasing sequence of finite-dimensional subspaces such that is dense in , but that doesn't seem to help either.",E P E F F \sigma E F E F\subsetneqq E P(F)=1 F E P A P(A)=1 E G_n \bigcup_n G_n E,"['functional-analysis', 'probability-theory', 'measure-theory', 'banach-spaces']"
16,Prove that $\exists c > 0$ such that $||f||_2 \le c ||T(f)||_{\infty}$,Prove that  such that,\exists c > 0 ||f||_2 \le c ||T(f)||_{\infty},"Problem (Unsolvable, since the statement is false.) Let $X = (C^2([0,1]),||\cdot||_2)$ , where $||f||_2 = ||f''||_{\infty} + ||f'||_{\infty} + ||f||_{\infty}$ . Define $T:X\to C([0,1])$ by: $\ \ \ \ T(f) = f'' + af' + bf$ where $a,b \in C([0,1])$ such that $T$ is surjective. Prove that $\exists c > 0$ such that $||f||_2 \le c ||T(f)||_{\infty}$ What I've gathered I know that $X$ is complete and that $T$ is linear. $T$ is bounded as well, since for $||f||_2 = 1$ : $||T(f)||_\infty = ||f'' + af' + bf||_\infty \le ||f''||_\infty + A ||f'||_\infty + B ||f||_\infty \le (1+A + B)$ . where $A = ||a||_\infty, B=||b||_\infty$ . So far, I know that $T$ is a bounded linear map, so I know that: $\forall f \in X: ||T(f)||_\infty \le ||A||\cdot||f_2||$ So what I need to prove is that $\exists c > 0$ such that $\forall f \in X:$ $\frac{||T(f)||_\infty}{||A||} \le ||f||_2 \le c ||T(f)||_\infty$ i.e. that $||\cdot||_2$ and $||T(\cdot)||_\infty$ are equivalent norms, right? I have no idea how to proceed from here.. Edit Since I got a counterexample showing that above is not true in general, I show below the original question (taken from an exam), which I shortened, but maybe changed the question in doing so: Original Problem Consider the Banach space $C^2([0,1])$ with $||\cdot||_2$ as defined above. Let $a,b \in C([0,1])$ and assume that for every $g\in C([0,1])$ , the differential equation $f''+ a f' +b f = g$ has a solution $f\in C^2([0,1])$ . Prove that there exists $c > 0$ , such that for every $g \in C([0,1])$ , the above equation has a solution $f\in C^2([0,1])$ , with $||f||_2 \le c ||g||$ Problem $\neq$ Original Problem I realize now that the property $ \forall f: ||f||_2 \le ||T(f)||_\infty $ is not explicitly required. My bad.","Problem (Unsolvable, since the statement is false.) Let , where . Define by: where such that is surjective. Prove that such that What I've gathered I know that is complete and that is linear. is bounded as well, since for : . where . So far, I know that is a bounded linear map, so I know that: So what I need to prove is that such that i.e. that and are equivalent norms, right? I have no idea how to proceed from here.. Edit Since I got a counterexample showing that above is not true in general, I show below the original question (taken from an exam), which I shortened, but maybe changed the question in doing so: Original Problem Consider the Banach space with as defined above. Let and assume that for every , the differential equation has a solution . Prove that there exists , such that for every , the above equation has a solution , with Problem Original Problem I realize now that the property is not explicitly required. My bad.","X = (C^2([0,1]),||\cdot||_2) ||f||_2 = ||f''||_{\infty} + ||f'||_{\infty} + ||f||_{\infty} T:X\to C([0,1]) \ \ \ \ T(f) = f'' + af' + bf a,b \in C([0,1]) T \exists c > 0 ||f||_2 \le c ||T(f)||_{\infty} X T T ||f||_2 = 1 ||T(f)||_\infty = ||f'' + af' + bf||_\infty \le ||f''||_\infty + A ||f'||_\infty + B ||f||_\infty \le (1+A + B) A = ||a||_\infty, B=||b||_\infty T \forall f \in X: ||T(f)||_\infty \le ||A||\cdot||f_2|| \exists c > 0 \forall f \in X: \frac{||T(f)||_\infty}{||A||} \le ||f||_2 \le c ||T(f)||_\infty ||\cdot||_2 ||T(\cdot)||_\infty C^2([0,1]) ||\cdot||_2 a,b \in C([0,1]) g\in C([0,1]) f''+ a f' +b f = g f\in C^2([0,1]) c > 0 g \in C([0,1]) f\in C^2([0,1]) ||f||_2 \le c ||g|| \neq  \forall f: ||f||_2 \le ||T(f)||_\infty ","['functional-analysis', 'continuity', 'normed-spaces']"
17,Universal properties of mapping spaces in functional analysis,Universal properties of mapping spaces in functional analysis,,"Recently I've been trying to learn more about functional analysis, and have been wondering about what universal characterisations are there of mapping spaces for topological vector spaces, (semi)normed spaces, and Banach spaces. In particular, I've seen the following characterisation invoked a few times: The operator norm $||{-}||$ on $\mathcal{Ban}(X,Y)$ with $X$ normed and $Y$ Banach is the unique one such that $\mathcal{Ban}(X,Y)$ is also a Banach space. For any sequence $\{T_{n}\}_{n\in\mathbf{N}}$ of operators, if $\displaystyle\lim_{n\to\infty}(||T_{n}||)=0$ , then $\displaystyle\lim_{n\to\infty}(T_{n}(x))=0$ for all $x\in X$ . So if $||{-}||'$ is another norm on $\mathcal{Ban}(X,Y)$ satisfying those conditions, then it must be the case that $||{-}||$ and $||{-}||'$ are equivalent. What would be a reference (or proof) for the above? Are there other similarly nice such ""universal characterisations"" of $\mathcal{Ban}(X,Y)$ ? Lastly, are there analogous results as the above one for semi/normed spaces and topological vector spaces?","Recently I've been trying to learn more about functional analysis, and have been wondering about what universal characterisations are there of mapping spaces for topological vector spaces, (semi)normed spaces, and Banach spaces. In particular, I've seen the following characterisation invoked a few times: The operator norm on with normed and Banach is the unique one such that is also a Banach space. For any sequence of operators, if , then for all . So if is another norm on satisfying those conditions, then it must be the case that and are equivalent. What would be a reference (or proof) for the above? Are there other similarly nice such ""universal characterisations"" of ? Lastly, are there analogous results as the above one for semi/normed spaces and topological vector spaces?","||{-}|| \mathcal{Ban}(X,Y) X Y \mathcal{Ban}(X,Y) \{T_{n}\}_{n\in\mathbf{N}} \displaystyle\lim_{n\to\infty}(||T_{n}||)=0 \displaystyle\lim_{n\to\infty}(T_{n}(x))=0 x\in X ||{-}||' \mathcal{Ban}(X,Y) ||{-}|| ||{-}||' \mathcal{Ban}(X,Y)","['functional-analysis', 'banach-spaces', 'normed-spaces', 'topological-vector-spaces']"
18,Density of bounded continuous functions in $L^1$,Density of bounded continuous functions in,L^1,"Let $X$ be a metric space, $\mathbb P$ a Borelian probability measure on $X$ and $L^1 = L^1(X,\mathcal B(X) , \mathbb P)$ the associated Lebesgue space. Also let $C_b = C_b(X,\mathbb R)$ be the set of continuous bounded functions from $X$ to $\mathbb R$ . Under what hypothesis on $X$ and $\mathbb P$ the set $C_b$ is dense in $L^1$ ? If $X$ is a topological space that is Hausdorff locally compact and $\mu$ is locally finite regular then I know that $C_c$ the set of continuous functions from $X$ to $\mathbb R$ with compact support is dense in $L^p(X,\mathcal B(X),\mu)$ , $p \in [1,\infty)$ . The counter example I know of $C_c$ not being dense in $L^1$ does not work because of the compacity. Take $X$ infinite dimensional normed vector space, then $C_c = \{ 0 \}$ . Then if we ask in addition $X$ Polish and $\mu$ probability measure we get a counter example. The space $C_b$ being bigger and without any notion of compacity I expect less hypothesis on $X$ and $\mu$ than the one I gave for $C_c$ . To allow $C_b$ to be a subset of $L^1$ we should take finite measure, thus without loss of generality I choosed a probability measure.","Let be a metric space, a Borelian probability measure on and the associated Lebesgue space. Also let be the set of continuous bounded functions from to . Under what hypothesis on and the set is dense in ? If is a topological space that is Hausdorff locally compact and is locally finite regular then I know that the set of continuous functions from to with compact support is dense in , . The counter example I know of not being dense in does not work because of the compacity. Take infinite dimensional normed vector space, then . Then if we ask in addition Polish and probability measure we get a counter example. The space being bigger and without any notion of compacity I expect less hypothesis on and than the one I gave for . To allow to be a subset of we should take finite measure, thus without loss of generality I choosed a probability measure.","X \mathbb P X L^1 = L^1(X,\mathcal B(X) , \mathbb P) C_b = C_b(X,\mathbb R) X \mathbb R X \mathbb P C_b L^1 X \mu C_c X \mathbb R L^p(X,\mathcal B(X),\mu) p \in [1,\infty) C_c L^1 X C_c = \{ 0 \} X \mu C_b X \mu C_c C_b L^1","['functional-analysis', 'measure-theory', 'continuity', 'metric-spaces', 'lebesgue-integral']"
19,Representation of the magnetic field in 2D magnetostatics,Representation of the magnetic field in 2D magnetostatics,,"Consider a magnetostatics problem in $\mathbb{R}^3$ . The problem is governed by the following equations $$\begin{aligned}\text{Maxwell's equations}\quad &\begin{cases}\nabla\times H(x)=J(x)\\\nabla\cdot B(x)=0\end{cases}\\[3pt] \text{Constitutive relation}\quad &\begin{cases}B(x)=\mu(x)H(x)\end{cases}\\[3pt] \text{Boundary conditions}\quad &\begin{cases}\nu(x)\cdot(B(x^+)-B(x^-))=0\\\nu(x)\times (H(x^+)-H(x^-))=J_s(x)\end{cases}\end{aligned}$$ where quantities like $B(x^+)$ and $B(x^-)$ denote one-sided limits at a boundary point $x$ and $\nu(x)$ is the normal at $x$ . Consider now an index set $\mathcal{K}$ and bounded Lipschitz domains $\Omega_k\subset\mathbb{R}^2$ with $k\in\mathcal{K}$ . Let's assume that $\mu(x)=\mu_k$ and $J(x)=J_k$ for $x\in\Omega_k\times\mathbb{R}$ and $\mu(x)=0$ and $J(x)=0$ for $x\in(\mathcal{A}\triangleq\overline{\bigcup_{k\in\mathcal{K}}\Omega_k}^c)\times\mathbb{R}$ . It's not hard to see that boundedness of the domains $\Omega_k$ implies that the magnetic field has finite energy per unit length in the last dimension (which I'll refer to as the z direction from now on). Using Maxwell's equations it can be shown that the z component of the magnetic field will be zero and considering the problem setup, the magnetic flux density can be written as $B(x)=B_x(x_x,x_y)\,e_x+B_y(x_x,x_y)\,e_y$ , where $e_x$ and $e_y$ are unit vectors in x and y direction, respectively. Omitting the z component of $B$ it can now be seen that $B(x_x,x_y)\in L(\mathbb{R}^2)^2$ . Assuming sane surface currents $J_s$ (which are further assumed to have a z component only), it follows now from results about Hodge decompositions in [1] that $$B(x_x,x_y)=\nabla B_{\nabla}(x_x,x_y)+\text{Curl} B_{\times}(x_x,x_y),$$ where $\text{Curl}\triangleq[\partial_y,-\partial_x]^T$ . Furthermore, the derivatives are strictly speaking only considered in a weak sense as the above result resorts to the use of Sobolev spaces. Also, in the equation above $B$ is assumed to be the restriction to either one of the $\Omega_k$ or $\mathcal{A}$ and the entire magnetic field is given by combination of all of these restrictions. Next, note that assuming $B_{\nabla}=0$ is equivalent to writing $B=\nabla\times A$ with $A=B_{\times}\,e_z$ . My experience with electromagnetism is very limited, but I realize that the magnetic flux density is often written in exactly this form, which leads me to my actual question : Given the problem setup, is it fine to assume $B_{\nabla}=0$ , meaning that there exists a function $B_{\times}$ (or a distribution for the sake of mathematical correctness) such that Maxwell's equations and the boundary conditions can still be satisfied? [1] Dautray, 1990, Mathematical Analysis and Numerical Methods for Science and Technology: Volume 3 Spectral Theory and Applications","Consider a magnetostatics problem in . The problem is governed by the following equations where quantities like and denote one-sided limits at a boundary point and is the normal at . Consider now an index set and bounded Lipschitz domains with . Let's assume that and for and and for . It's not hard to see that boundedness of the domains implies that the magnetic field has finite energy per unit length in the last dimension (which I'll refer to as the z direction from now on). Using Maxwell's equations it can be shown that the z component of the magnetic field will be zero and considering the problem setup, the magnetic flux density can be written as , where and are unit vectors in x and y direction, respectively. Omitting the z component of it can now be seen that . Assuming sane surface currents (which are further assumed to have a z component only), it follows now from results about Hodge decompositions in [1] that where . Furthermore, the derivatives are strictly speaking only considered in a weak sense as the above result resorts to the use of Sobolev spaces. Also, in the equation above is assumed to be the restriction to either one of the or and the entire magnetic field is given by combination of all of these restrictions. Next, note that assuming is equivalent to writing with . My experience with electromagnetism is very limited, but I realize that the magnetic flux density is often written in exactly this form, which leads me to my actual question : Given the problem setup, is it fine to assume , meaning that there exists a function (or a distribution for the sake of mathematical correctness) such that Maxwell's equations and the boundary conditions can still be satisfied? [1] Dautray, 1990, Mathematical Analysis and Numerical Methods for Science and Technology: Volume 3 Spectral Theory and Applications","\mathbb{R}^3 \begin{aligned}\text{Maxwell's equations}\quad &\begin{cases}\nabla\times H(x)=J(x)\\\nabla\cdot B(x)=0\end{cases}\\[3pt]
\text{Constitutive relation}\quad &\begin{cases}B(x)=\mu(x)H(x)\end{cases}\\[3pt]
\text{Boundary conditions}\quad &\begin{cases}\nu(x)\cdot(B(x^+)-B(x^-))=0\\\nu(x)\times (H(x^+)-H(x^-))=J_s(x)\end{cases}\end{aligned} B(x^+) B(x^-) x \nu(x) x \mathcal{K} \Omega_k\subset\mathbb{R}^2 k\in\mathcal{K} \mu(x)=\mu_k J(x)=J_k x\in\Omega_k\times\mathbb{R} \mu(x)=0 J(x)=0 x\in(\mathcal{A}\triangleq\overline{\bigcup_{k\in\mathcal{K}}\Omega_k}^c)\times\mathbb{R} \Omega_k B(x)=B_x(x_x,x_y)\,e_x+B_y(x_x,x_y)\,e_y e_x e_y B B(x_x,x_y)\in L(\mathbb{R}^2)^2 J_s B(x_x,x_y)=\nabla B_{\nabla}(x_x,x_y)+\text{Curl} B_{\times}(x_x,x_y), \text{Curl}\triangleq[\partial_y,-\partial_x]^T B \Omega_k \mathcal{A} B_{\nabla}=0 B=\nabla\times A A=B_{\times}\,e_z B_{\nabla}=0 B_{\times}","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'boundary-value-problem', 'electromagnetism']"
20,"Complementary book for Conway's book ""A Course in Functional Analysis""","Complementary book for Conway's book ""A Course in Functional Analysis""",,"I am first year graduate student and I have taken Functional Analysis course this semester. The main book of teaching is Conway's book ""A Course in Functional Analysis"" but the lecturer is notorious for giving exams from questions that are not exercises of the book he's teaching. So I need an accompanying book for functional analysis with lots of cool problems that covers same materials of Conway's book and is on graduate level. Any suggestions would be much appreciated.","I am first year graduate student and I have taken Functional Analysis course this semester. The main book of teaching is Conway's book ""A Course in Functional Analysis"" but the lecturer is notorious for giving exams from questions that are not exercises of the book he's teaching. So I need an accompanying book for functional analysis with lots of cool problems that covers same materials of Conway's book and is on graduate level. Any suggestions would be much appreciated.",,['functional-analysis']
21,Convolution of tight measures is tight,Convolution of tight measures is tight,,"Let $E$ be a Banach space (think of $E=\mathbb R^d$ , if it's easier to understand the following for you), $(\mu_n)_{n\in\mathbb N}$ and $(\nu_n)_{n\in\mathbb N}$ be tight sequences of finite nonnegative measures on $\mathcal B(E)$ . I would like to conclude that the sequence $(\mu_n\ast\nu_n)_{n\in\mathbb N}$ of convolutions is tight as well. Let $\varepsilon>0$ . By the tightness assumption, there is a compact $K\subseteq E$ with $$\max\left(\sup_{n\in\mathbb N}\mu_n(K^c),\sup_{n\in\mathbb N}\nu_n(K^c)\right)<\varepsilon\tag1.$$ Now, $K+K$ is clearly compact and \begin{equation}\begin{split}(\mu_n\ast\nu_n)(K+K)&=(\mu_n\otimes\nu_n)(\{(x,y)\in E^2:x+y\in K+K\})\\&\ge(\mu_n\otimes\nu_n)(K\times K)=\mu_n(K)\nu_n(K)\end{split}\tag2\end{equation} for all $n\in\mathbb N$ . If I assume that $\mu_n,\nu_n$ are probability measures for all $n\in\mathbb N$ , then $(2)$ yields $$(\mu_n\ast\nu_n)(K+K)\ge(1-\varepsilon)^2\tag3$$ and hence we obtain the claim. But how can we show the claim in general?","Let be a Banach space (think of , if it's easier to understand the following for you), and be tight sequences of finite nonnegative measures on . I would like to conclude that the sequence of convolutions is tight as well. Let . By the tightness assumption, there is a compact with Now, is clearly compact and for all . If I assume that are probability measures for all , then yields and hence we obtain the claim. But how can we show the claim in general?","E E=\mathbb R^d (\mu_n)_{n\in\mathbb N} (\nu_n)_{n\in\mathbb N} \mathcal B(E) (\mu_n\ast\nu_n)_{n\in\mathbb N} \varepsilon>0 K\subseteq E \max\left(\sup_{n\in\mathbb N}\mu_n(K^c),\sup_{n\in\mathbb N}\nu_n(K^c)\right)<\varepsilon\tag1. K+K \begin{equation}\begin{split}(\mu_n\ast\nu_n)(K+K)&=(\mu_n\otimes\nu_n)(\{(x,y)\in E^2:x+y\in K+K\})\\&\ge(\mu_n\otimes\nu_n)(K\times K)=\mu_n(K)\nu_n(K)\end{split}\tag2\end{equation} n\in\mathbb N \mu_n,\nu_n n\in\mathbb N (2) (\mu_n\ast\nu_n)(K+K)\ge(1-\varepsilon)^2\tag3","['functional-analysis', 'probability-theory', 'measure-theory']"
22,"Is $B^s_{p,p} = W^{s,p}$?",Is ?,"B^s_{p,p} = W^{s,p}","In Adams & Fournier's book ""Sobolev Spaces"" page 255, there are assertions: $$ W^{s,p} = F^{s}_{p,2} \\ B^s_{p,p} = F^s_{p,p}$$ Note: Here $s>0$ , $1\leq p<\infty$ , and the fractional order Sobolev spaces are defined as complex interpolation spaces $W^{s,p} = [L^p , W^{m,p} ] _{s/m}$ where $m$ is the smallest integer greater than $s$ . AFAIK, the above definition is equivalent to the following definition (at least for $0<s<1$ ), employing Gagliardo seminorms: $f \in W^{s,p}$ iff $f \in W^{\left \lfloor{s}\right \rfloor ,p}$ and $[D^\alpha f] := (\int \frac{|D^{\alpha}f(x)-D^{\alpha}f(y)|^p } {|x-y|^{(s-\left \lfloor{s}\right \rfloor)p + n}} dxdy)^{1/p} < \infty$ for all $|\alpha|=\left \lfloor{s}\right \rfloor$ . The Triebel-Lizorkin spaces and the Besov spaces are defined by Paley-Littlewood decompositions. In Triebel's book ""Interpolation theory, function spaces, differential operators(1978)"", page 169, the fractional order Sobolev spaces are defined as $W^{s,p} = B^s_{p,p}$ for $s>0$ non-integer. Therefore, the two monographs are coherent only when $p=2$ . Could someone explain about this situation? My background: I have little experience and knowledge of function spaces. To get some knowledge, I am starting to have a brief overview of the famous monographs. However, the problem described above makes me very confused. Any bits of help are welcome. Thank you!!","In Adams & Fournier's book ""Sobolev Spaces"" page 255, there are assertions: Note: Here , , and the fractional order Sobolev spaces are defined as complex interpolation spaces where is the smallest integer greater than . AFAIK, the above definition is equivalent to the following definition (at least for ), employing Gagliardo seminorms: iff and for all . The Triebel-Lizorkin spaces and the Besov spaces are defined by Paley-Littlewood decompositions. In Triebel's book ""Interpolation theory, function spaces, differential operators(1978)"", page 169, the fractional order Sobolev spaces are defined as for non-integer. Therefore, the two monographs are coherent only when . Could someone explain about this situation? My background: I have little experience and knowledge of function spaces. To get some knowledge, I am starting to have a brief overview of the famous monographs. However, the problem described above makes me very confused. Any bits of help are welcome. Thank you!!"," W^{s,p} = F^{s}_{p,2}
\\
B^s_{p,p} = F^s_{p,p} s>0 1\leq p<\infty W^{s,p} = [L^p , W^{m,p} ] _{s/m} m s 0<s<1 f \in W^{s,p} f \in W^{\left \lfloor{s}\right \rfloor ,p} [D^\alpha f] := (\int \frac{|D^{\alpha}f(x)-D^{\alpha}f(y)|^p } {|x-y|^{(s-\left \lfloor{s}\right \rfloor)p + n}} dxdy)^{1/p} < \infty |\alpha|=\left \lfloor{s}\right \rfloor W^{s,p} = B^s_{p,p} s>0 p=2","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
23,Different terminology for spectrum?,Different terminology for spectrum?,,"The common terminology we get used to on spectrum of linear operators is point spectrum $ \sigma_p $ , continuous spectrum $ \sigma_c $ , residual spectrum $ \sigma_r $ . However, we recently notice an terminology of another type: purely point spectrum $ \sigma_{pp} $ , absolutely continuous spectrum $ \sigma_{ac} $ , singularly continuous spectrum $ \sigma_{sc} $ . The latter type is defined with respect to decomposition measure and orthogonal decomposition of spectral projections. For details, see for instance Chapter 3.3 in ""G. Teschl: Mathematical Methods in Quantum Mechanics - With   Applications to Schrödinger Operators (2014)"". Question: What is the relation between above two terminologies, can we give an exact correspondence for them? Edit: The relation I have found so far is $ \sigma_{pp} = \overline{\sigma_{p}} $ . One possibly useful result: Chapter IV, Proposition 1.16 in [Engel, Nagel: One-Parameter Semigroups for Linear Evolution Equations   (2000)] However, I can not see clearly how to use this result to explain the connections between the two terminologies.","The common terminology we get used to on spectrum of linear operators is point spectrum , continuous spectrum , residual spectrum . However, we recently notice an terminology of another type: purely point spectrum , absolutely continuous spectrum , singularly continuous spectrum . The latter type is defined with respect to decomposition measure and orthogonal decomposition of spectral projections. For details, see for instance Chapter 3.3 in ""G. Teschl: Mathematical Methods in Quantum Mechanics - With   Applications to Schrödinger Operators (2014)"". Question: What is the relation between above two terminologies, can we give an exact correspondence for them? Edit: The relation I have found so far is . One possibly useful result: Chapter IV, Proposition 1.16 in [Engel, Nagel: One-Parameter Semigroups for Linear Evolution Equations   (2000)] However, I can not see clearly how to use this result to explain the connections between the two terminologies.", \sigma_p   \sigma_c   \sigma_r   \sigma_{pp}   \sigma_{ac}   \sigma_{sc}   \sigma_{pp} = \overline{\sigma_{p}} ,"['functional-analysis', 'terminology', 'spectral-theory']"
24,Jensen's inequality for integral without l.s.c. assumption,Jensen's inequality for integral without l.s.c. assumption,,"I start with the following lemma Lemma 1. (Fundamental inclusion for convex sets) Let $C$ be a closed subset of $\mathbb{R}^n$. Then, $C$ is convex if and only if   \begin{equation*} \int_\Omega f\;d\mu \in C \end{equation*} for all   measures spaces $(\Omega,\mu)$ with $\mu(\Omega) = 1$ and all   $\mathbb{R}^n$-valued integrable functions $f$ on $\Omega$ satisfying   $f(x)\in C$ for $\mu$-a.e. $x\in\Omega$. A proof can be given based on Hahn-Banach theorem (strictly separable): Proof: Assume the latter condition holds, we show that $C$ is convex. Let $x,y\in C$ and $\lambda\in (0,1)$, we consider $\Omega =  \{0,1\}$ with the discrete $\sigma$-algebra, $\mu$ be the discrete    measure $\mu(\{0\}) = \lambda$ and $\mu(\{1\}) = 1-\lambda$, and    $f:\Omega\longrightarrow C$ defined by $f(0) = x$ and $f(1) = y$, then    clearly $\int_\Omega f\;d\mu = \lambda x+ (1-\lambda)y \in C$ and    therfore $C$ is convex. Conversely, if $C$ is convex, we prove by    contradiction. Assume that there exists $(\Omega,\mu)$ and $f$ as    above but \begin{equation*} x_0 = \int_\Omega f\;d\mu \notin C.  \end{equation*} Using Hahn-Banach theorem\footnote{In $\mathbb{R}^n$    then every two nonempty disjoint convex sets are separable by a closed    hyperplane.}, there exists $\xi\in \mathbb{R}^n$ and $\alpha\in  \mathbb{R}$ such that \begin{equation*} \xi\cdot x < \alpha < \xi\cdot  x_0 \qquad\text{for all}\qquad x\in C. \end{equation*} But since    $f(z)\in C$ for $\mu$-a.e. $z\in \Omega$, we also have    \begin{equation*} \alpha<\xi\cdot x_0 =  \xi\cdot\left(\int_\Omega  f\;d\mu\right) = \int_\Omega \xi\cdot f(z)\;d\mu(z) \leq \alpha   \end{equation*} which is a contradiction. Using that lemma we can prove the Jensen inequality with the assumption $\Phi$ is lower semicontinuous as following (we need $\mathrm{epi}(\Phi)$ to be closed so that we can use strictly separation of Hahn Banach theorem) Theorem [Jensen inequality for convex l.s.c. functions] Let $(\Omega,\mathcal{A},\mu)$ be a probability space, i.e., $\mu(\Omega)  = 1$. If $f$ is a function $\Omega\longrightarrow\mathbb{R}^n$ such that it is $\mu$-integrable and if $\Phi$ is a l.s.c. convex function    $\mathbb{R}^n\longrightarrow\mathbb{R}$ then \begin{equation*}  \Phi\left(\int_\Omega f\;d\mu\right) \leq \int_\Omega\Phi\circ  f\;d\mu. \end{equation*} Proof. Let us define $C = \mathrm{epi}\;\Phi = \{(x,\lambda)\in \mathbb{R}^n\times\mathbb{R} :\Phi(x)\leq \lambda\}$. Since $\Phi$ is    convex l.s.c, $C$ is a closed convex subset of $\mathbb{R}^{n+1}$. Let    us define  \begin{equation*}  \mathcal{F}:\Omega\longrightarrow\mathbb{R}^n\times\mathbb{R}  \qquad\text{by}\qquad \mathcal{F}(z) = \big(f(z),\Phi(f(z))\big).  \end{equation*} It is clear that $\mathcal{F}(z)\in \mathrm{epi}\;\Phi  = C$ for $\mu$-a.e. $z\in \Omega$, hence by Lemma 1 we deduce that  \begin{equation*} \int_\Omega \mathcal{F}(z)\;dz =  \left(\int_\Omega f\;d\mu, \int_\Omega \Phi\circ f\;\mu\right) \in  \mathrm{epi}\;\Phi \end{equation*} which gives us our desired claim. My question is, We have a version of Hahn Banach theorem saying that in $\mathbb{R}^n$, any two nonempty disjoint convex sets can be separated by a closed hyperplane (not strictly seperated). Can we use that to relax the condition $\Phi$ is lower semicontinuous in Lemma 1 so that we can generalize Jensen inquality without l.s.c. assumption?","I start with the following lemma Lemma 1. (Fundamental inclusion for convex sets) Let $C$ be a closed subset of $\mathbb{R}^n$. Then, $C$ is convex if and only if   \begin{equation*} \int_\Omega f\;d\mu \in C \end{equation*} for all   measures spaces $(\Omega,\mu)$ with $\mu(\Omega) = 1$ and all   $\mathbb{R}^n$-valued integrable functions $f$ on $\Omega$ satisfying   $f(x)\in C$ for $\mu$-a.e. $x\in\Omega$. A proof can be given based on Hahn-Banach theorem (strictly separable): Proof: Assume the latter condition holds, we show that $C$ is convex. Let $x,y\in C$ and $\lambda\in (0,1)$, we consider $\Omega =  \{0,1\}$ with the discrete $\sigma$-algebra, $\mu$ be the discrete    measure $\mu(\{0\}) = \lambda$ and $\mu(\{1\}) = 1-\lambda$, and    $f:\Omega\longrightarrow C$ defined by $f(0) = x$ and $f(1) = y$, then    clearly $\int_\Omega f\;d\mu = \lambda x+ (1-\lambda)y \in C$ and    therfore $C$ is convex. Conversely, if $C$ is convex, we prove by    contradiction. Assume that there exists $(\Omega,\mu)$ and $f$ as    above but \begin{equation*} x_0 = \int_\Omega f\;d\mu \notin C.  \end{equation*} Using Hahn-Banach theorem\footnote{In $\mathbb{R}^n$    then every two nonempty disjoint convex sets are separable by a closed    hyperplane.}, there exists $\xi\in \mathbb{R}^n$ and $\alpha\in  \mathbb{R}$ such that \begin{equation*} \xi\cdot x < \alpha < \xi\cdot  x_0 \qquad\text{for all}\qquad x\in C. \end{equation*} But since    $f(z)\in C$ for $\mu$-a.e. $z\in \Omega$, we also have    \begin{equation*} \alpha<\xi\cdot x_0 =  \xi\cdot\left(\int_\Omega  f\;d\mu\right) = \int_\Omega \xi\cdot f(z)\;d\mu(z) \leq \alpha   \end{equation*} which is a contradiction. Using that lemma we can prove the Jensen inequality with the assumption $\Phi$ is lower semicontinuous as following (we need $\mathrm{epi}(\Phi)$ to be closed so that we can use strictly separation of Hahn Banach theorem) Theorem [Jensen inequality for convex l.s.c. functions] Let $(\Omega,\mathcal{A},\mu)$ be a probability space, i.e., $\mu(\Omega)  = 1$. If $f$ is a function $\Omega\longrightarrow\mathbb{R}^n$ such that it is $\mu$-integrable and if $\Phi$ is a l.s.c. convex function    $\mathbb{R}^n\longrightarrow\mathbb{R}$ then \begin{equation*}  \Phi\left(\int_\Omega f\;d\mu\right) \leq \int_\Omega\Phi\circ  f\;d\mu. \end{equation*} Proof. Let us define $C = \mathrm{epi}\;\Phi = \{(x,\lambda)\in \mathbb{R}^n\times\mathbb{R} :\Phi(x)\leq \lambda\}$. Since $\Phi$ is    convex l.s.c, $C$ is a closed convex subset of $\mathbb{R}^{n+1}$. Let    us define  \begin{equation*}  \mathcal{F}:\Omega\longrightarrow\mathbb{R}^n\times\mathbb{R}  \qquad\text{by}\qquad \mathcal{F}(z) = \big(f(z),\Phi(f(z))\big).  \end{equation*} It is clear that $\mathcal{F}(z)\in \mathrm{epi}\;\Phi  = C$ for $\mu$-a.e. $z\in \Omega$, hence by Lemma 1 we deduce that  \begin{equation*} \int_\Omega \mathcal{F}(z)\;dz =  \left(\int_\Omega f\;d\mu, \int_\Omega \Phi\circ f\;\mu\right) \in  \mathrm{epi}\;\Phi \end{equation*} which gives us our desired claim. My question is, We have a version of Hahn Banach theorem saying that in $\mathbb{R}^n$, any two nonempty disjoint convex sets can be separated by a closed hyperplane (not strictly seperated). Can we use that to relax the condition $\Phi$ is lower semicontinuous in Lemma 1 so that we can generalize Jensen inquality without l.s.c. assumption?",,"['functional-analysis', 'convex-analysis']"
25,Inequality related to spectral radius,Inequality related to spectral radius,,"Let $\mathcal{L}(E)$ the algebra of all bounded linear operators from $E$ to $E$. For $A = (A_1,\cdots,A_d)\in\mathcal{L}(E)^d$, the algebraic spectral radius of $A$ was given by $$ r_a(A)=\inf_{n\in \mathbb{N}^*}\left\|\sum_{f\in F(n,d)} A_f^* A_f\right\|^{\frac{1}{2n}} =\lim_{n\to+\infty}\left\|\sum_{f\in F(n,d)} A_f^* A_f\right\|^{\frac{1}{2n}} , $$ where $F(n,d):=\{f:\,\{1,\cdots,n\}\longrightarrow \{1,\cdots,d\}\}$ and $A_f:=A_{f(1)}\cdots A_{f(n)}$, for $f\in F(n,d)$. I don't understand why if $n=1$, we have   $$r_a(A)\leq  \|A\|:=\displaystyle\sup_{\|x\|=1}\bigg(\displaystyle\sum_{k=1}^d\|A_kx\|^2\bigg)^{\frac{1}{2}}?$$","Let $\mathcal{L}(E)$ the algebra of all bounded linear operators from $E$ to $E$. For $A = (A_1,\cdots,A_d)\in\mathcal{L}(E)^d$, the algebraic spectral radius of $A$ was given by $$ r_a(A)=\inf_{n\in \mathbb{N}^*}\left\|\sum_{f\in F(n,d)} A_f^* A_f\right\|^{\frac{1}{2n}} =\lim_{n\to+\infty}\left\|\sum_{f\in F(n,d)} A_f^* A_f\right\|^{\frac{1}{2n}} , $$ where $F(n,d):=\{f:\,\{1,\cdots,n\}\longrightarrow \{1,\cdots,d\}\}$ and $A_f:=A_{f(1)}\cdots A_{f(n)}$, for $f\in F(n,d)$. I don't understand why if $n=1$, we have   $$r_a(A)\leq  \|A\|:=\displaystyle\sup_{\|x\|=1}\bigg(\displaystyle\sum_{k=1}^d\|A_kx\|^2\bigg)^{\frac{1}{2}}?$$",,"['functional-analysis', 'operator-algebras']"
26,Schur's test : an infinite matrix defines a bounded operator on $\ell^2$,Schur's test : an infinite matrix defines a bounded operator on,\ell^2,"I want to prove Schur test. Schur test says Let $\{\alpha_{ij}\}_{i,j=1}^{\infty}$ be an infinite matrix such that $\alpha_{ij} \ge 0$ for all $i,j$ and such that there are scalars $p_i>0$ and $\beta,\gamma>0$ with   $\sum_{i=1}^{\infty}\alpha_{ij}p_{i} \le \beta p_j \quad$ , $\quad \sum_{j=1}^{\infty}\alpha_{ij}p_{j} \le \gamma p_i \quad$ for all $i,j\ge 1$ then there is an operator $A$ on $\ell^{2}(\mathbb{N})$ with $\langle Ae_{j},e_{i}\rangle =\alpha_{ij}$ and $\|A\|^2 \le \beta \gamma$. I know that I must use the orthogonal projection and Fourier series, But I don't know how. This is an exercise from section1, Chapter2,  A Course in Functional Analysis Conway(second edition).","I want to prove Schur test. Schur test says Let $\{\alpha_{ij}\}_{i,j=1}^{\infty}$ be an infinite matrix such that $\alpha_{ij} \ge 0$ for all $i,j$ and such that there are scalars $p_i>0$ and $\beta,\gamma>0$ with   $\sum_{i=1}^{\infty}\alpha_{ij}p_{i} \le \beta p_j \quad$ , $\quad \sum_{j=1}^{\infty}\alpha_{ij}p_{j} \le \gamma p_i \quad$ for all $i,j\ge 1$ then there is an operator $A$ on $\ell^{2}(\mathbb{N})$ with $\langle Ae_{j},e_{i}\rangle =\alpha_{ij}$ and $\|A\|^2 \le \beta \gamma$. I know that I must use the orthogonal projection and Fourier series, But I don't know how. This is an exercise from section1, Chapter2,  A Course in Functional Analysis Conway(second edition).",,"['functional-analysis', 'operator-theory']"
27,The boundedness of norms of iterates of a linear operator,The boundedness of norms of iterates of a linear operator,,"Let $T$ be a linear bounded operator on a Banach space $X$. Suppose that $(a_n)$ and $(b_n)$ are two increasing sequences of positive integers such that $b_n > a_n$ for any $n$ and such that $\sup_{n}(b_n - a_n)<\infty$. Furthermore, assume that $\sup_{n}\|T^{b_n}\|<\infty$. Does it follow necessarily that $\sup_{n}\|T^{a_n}\|<\infty$? Note: the assumption clearly implies that the spectral radius of $T$, i.e., $\rho(T) = \lim \|T^n\|^{1/n}$, is at most $1$. However, an operator with spectral radius $1$ can   have unbounded norms of iterates, for example $T = \left(\begin{smallmatrix} 1 & 1 \\ 0 & 1 \end{smallmatrix}\right) $ on $\mathbb{R}^2$.","Let $T$ be a linear bounded operator on a Banach space $X$. Suppose that $(a_n)$ and $(b_n)$ are two increasing sequences of positive integers such that $b_n > a_n$ for any $n$ and such that $\sup_{n}(b_n - a_n)<\infty$. Furthermore, assume that $\sup_{n}\|T^{b_n}\|<\infty$. Does it follow necessarily that $\sup_{n}\|T^{a_n}\|<\infty$? Note: the assumption clearly implies that the spectral radius of $T$, i.e., $\rho(T) = \lim \|T^n\|^{1/n}$, is at most $1$. However, an operator with spectral radius $1$ can   have unbounded norms of iterates, for example $T = \left(\begin{smallmatrix} 1 & 1 \\ 0 & 1 \end{smallmatrix}\right) $ on $\mathbb{R}^2$.",,"['functional-analysis', 'operator-theory', 'normed-spaces']"
28,Proof that the convolution of a tempered distribution with some $\varphi\in\scr{S}$ is a $C^\infty$ function,Proof that the convolution of a tempered distribution with some  is a  function,\varphi\in\scr{S} C^\infty,"Let $u\in\mathscr{S}'(\mathbb{R}^n)$ be a tempered distribution, and let $\varphi\in\mathscr{S}(\mathbb{R}^n)$ be Schwartz class, then we consider $\varphi*u\in\scr{S}'$ by setting  $$(\varphi*u)(\psi)=u(\tilde{\varphi}*\psi)$$ where $\tilde{\varphi}(x)=\varphi(-x)$, for all $\varphi\in\scr{S}$, and now it is not too hard to check that this is itself a tempered distribution. However, I'm struggling with the following proof that $\varphi*u$ is actually $C^\infty$. Let $\psi\in\scr{S}(\mathbb{R}^n)$, so we have that   \begin{align} (\varphi*u)(\psi)=u(\tilde{\varphi}*\psi)  &= u\left(\int_{\mathbb{R}^n}\tilde{\varphi}(\cdot-y)\psi(y)\,\mathrm{d}y \right) \\ &= u\left(\int_{\mathbb{R}^n}\tau^y(\tilde{\varphi})(\cdot)\psi(y)\,\mathrm{d}y \right) \\ &= \int_{\mathbb{R}^n}u(\tau^y(\tilde{\varphi}))\psi(y)\,\mathrm{d}y \end{align} and I'm told that the last step is justified by the continuity of $u$ and the fact that the Riemann sums converge in the $\mathscr{S}$ topology, though this isn't proven. I suspect that the proof of that last statement will be very long and tedious, so I'm more curious about the general techniques that are necessary to prove it. Is there any specific sequence of test sets in $\mathbb{R}^n$ over which we can conveniently integrate, such that we have convergence w.r.t. $x$? My first naive attempt was to define $\Lambda_k=[-k,k]^n\cap\frac{1}{k}\mathbb{Z}^n$ but I was unable to prove that this would give us the necessary convergence.","Let $u\in\mathscr{S}'(\mathbb{R}^n)$ be a tempered distribution, and let $\varphi\in\mathscr{S}(\mathbb{R}^n)$ be Schwartz class, then we consider $\varphi*u\in\scr{S}'$ by setting  $$(\varphi*u)(\psi)=u(\tilde{\varphi}*\psi)$$ where $\tilde{\varphi}(x)=\varphi(-x)$, for all $\varphi\in\scr{S}$, and now it is not too hard to check that this is itself a tempered distribution. However, I'm struggling with the following proof that $\varphi*u$ is actually $C^\infty$. Let $\psi\in\scr{S}(\mathbb{R}^n)$, so we have that   \begin{align} (\varphi*u)(\psi)=u(\tilde{\varphi}*\psi)  &= u\left(\int_{\mathbb{R}^n}\tilde{\varphi}(\cdot-y)\psi(y)\,\mathrm{d}y \right) \\ &= u\left(\int_{\mathbb{R}^n}\tau^y(\tilde{\varphi})(\cdot)\psi(y)\,\mathrm{d}y \right) \\ &= \int_{\mathbb{R}^n}u(\tau^y(\tilde{\varphi}))\psi(y)\,\mathrm{d}y \end{align} and I'm told that the last step is justified by the continuity of $u$ and the fact that the Riemann sums converge in the $\mathscr{S}$ topology, though this isn't proven. I suspect that the proof of that last statement will be very long and tedious, so I'm more curious about the general techniques that are necessary to prove it. Is there any specific sequence of test sets in $\mathbb{R}^n$ over which we can conveniently integrate, such that we have convergence w.r.t. $x$? My first naive attempt was to define $\Lambda_k=[-k,k]^n\cap\frac{1}{k}\mathbb{Z}^n$ but I was unable to prove that this would give us the necessary convergence.",,"['functional-analysis', 'partial-differential-equations']"
29,Is a positive linear functional on $L^p$ necessarily bounded?,Is a positive linear functional on  necessarily bounded?,L^p,"I was reading a source that suggested that if $X$ is a measure space (or perhaps just the interval with Lebesgue measure), and if $1 \leq p < \infty$, then any non-negative linear  functional $T$ on $L^p(X)$ is continuous. ($f \geq 0$ a.e. implies $T(f) \geq 0$ as real numbers.) Is this true, or even obviously true? I read here that positive linear functionals on $C^*$ algebras are continuous, but $L^p$ spaces don't have this structure in general... I also  learned that it is true when $p = \infty$ and $X$ is a countable set with counting measure: Any positive linear functional $\phi$ on $\ell^\infty$ is a bounded linear operator and has $\|\phi \| = \phi((1,1,...)) $ ... however, I don't think the same technique carries over. There is also this question, which is somewhat weaker and more general: Positive linear functional on an involutive Banach algebra I also found some discussion  here ( question (2) ): application of positive linear functionl","I was reading a source that suggested that if $X$ is a measure space (or perhaps just the interval with Lebesgue measure), and if $1 \leq p < \infty$, then any non-negative linear  functional $T$ on $L^p(X)$ is continuous. ($f \geq 0$ a.e. implies $T(f) \geq 0$ as real numbers.) Is this true, or even obviously true? I read here that positive linear functionals on $C^*$ algebras are continuous, but $L^p$ spaces don't have this structure in general... I also  learned that it is true when $p = \infty$ and $X$ is a countable set with counting measure: Any positive linear functional $\phi$ on $\ell^\infty$ is a bounded linear operator and has $\|\phi \| = \phi((1,1,...)) $ ... however, I don't think the same technique carries over. There is also this question, which is somewhat weaker and more general: Positive linear functional on an involutive Banach algebra I also found some discussion  here ( question (2) ): application of positive linear functionl",,"['functional-analysis', 'lebesgue-measure', 'lp-spaces']"
30,Does a linear map preserving norm also approximately preserve inner products?,Does a linear map preserving norm also approximately preserve inner products?,,"Suppose I have distribution on linear map  $D_{\epsilon,\delta}$  over $\mathbb{R}^{d\times d}$ such that for any $0<\epsilon,\delta<1/2$ we have $\forall x$, $$\Pr_{A \sim D_{\epsilon,\delta}}(\lvert \lVert Ax\rVert^2- \lVert x\rVert^2\rvert>\epsilon \lVert x\rVert^2)<\delta$$ Now given this can I say that $A$ drawn from $D_{\epsilon,\delta}$ also preserves inner product in the following sense $$\Pr_{A \sim D_{\epsilon,\delta}}(\left\lvert \langle Ax_1,Ax_2\rangle-\langle x_1,x_2\rangle\right\rvert>\epsilon\lVert x_1\rVert\lVert x_2\rVert)<\delta$$ I tried using polarization identity to get the result, I have following thing so far \begin{align*}  \langle Ax_1,Ax_2\rangle&=\frac{1}{4}(\lVert A(x_1+x_2)\rVert^2-A(x_1-x_2)\rVert^2) \\  &\le\frac{1}{4}((1+\epsilon)\lVert x_1+x_2\rVert^2-(1-\epsilon)\lVert x_1+x_2\rVert^2)\\ &=\langle x_1,x_2 \rangle+\frac{\epsilon}{2}(\lVert x_1\rVert^2+\lVert x_2\rVert^2) \end{align*} Now I am not able to go further as  $\lVert x_1\rVert^2+\lVert x_2\rVert^2\geq 2\lVert x_1\rVert\lVert x_2\rVert$. Any help, comments, hints are greatly appreciated. Thanks.","Suppose I have distribution on linear map  $D_{\epsilon,\delta}$  over $\mathbb{R}^{d\times d}$ such that for any $0<\epsilon,\delta<1/2$ we have $\forall x$, $$\Pr_{A \sim D_{\epsilon,\delta}}(\lvert \lVert Ax\rVert^2- \lVert x\rVert^2\rvert>\epsilon \lVert x\rVert^2)<\delta$$ Now given this can I say that $A$ drawn from $D_{\epsilon,\delta}$ also preserves inner product in the following sense $$\Pr_{A \sim D_{\epsilon,\delta}}(\left\lvert \langle Ax_1,Ax_2\rangle-\langle x_1,x_2\rangle\right\rvert>\epsilon\lVert x_1\rVert\lVert x_2\rVert)<\delta$$ I tried using polarization identity to get the result, I have following thing so far \begin{align*}  \langle Ax_1,Ax_2\rangle&=\frac{1}{4}(\lVert A(x_1+x_2)\rVert^2-A(x_1-x_2)\rVert^2) \\  &\le\frac{1}{4}((1+\epsilon)\lVert x_1+x_2\rVert^2-(1-\epsilon)\lVert x_1+x_2\rVert^2)\\ &=\langle x_1,x_2 \rangle+\frac{\epsilon}{2}(\lVert x_1\rVert^2+\lVert x_2\rVert^2) \end{align*} Now I am not able to go further as  $\lVert x_1\rVert^2+\lVert x_2\rVert^2\geq 2\lVert x_1\rVert\lVert x_2\rVert$. Any help, comments, hints are greatly appreciated. Thanks.",,"['functional-analysis', 'probability-theory', 'hilbert-spaces']"
31,Extension of a linear map,Extension of a linear map,,"For any $x \in X$, define the set $\mathcal{F}(X) = \overline{span \{ \delta_x : x \in X \}}$ where $\delta_x(f)=f(x)$ for all $f \in$ $Lip_0(X)$. The set Lip$_0(X)$ is the set of all real-valued Lipschitz functions which vanish at $0$. Note that $\delta_x$ is an evaluation functional on Lip$_0(X)$. Suppose $X$ and $Y$ are Banach spaces. Let $L:X \rightarrow Y$ be a   Lipschitz map such that $L(0)=0$. Then there exists a unique linear   map $\hat{L}: \mathcal{F}(X) \rightarrow \mathcal{F}(Y)$ such that   $\hat{L}\delta_X = \delta_YL$ The statement above is Lemma $2.2$ . The following is its proof: The linear map $L^\# : Lip_0(Y) \rightarrow Lip_0(X)$ defined by $L^\#(F) = F \circ L$ is pointwise-to-pointwise continuous, hence there is a linear map $\hat{L}$ between the preduals such that $\hat{L^*} = L^\#$. It is clear that $\| L^\# \| = Lip(L)$, and $\| \hat{L} \| = \| \hat{L^*} \| = \| L^\# \|$. The other assertions are clear. Question: $(1)$ To show the map $L^\#$ is pointwise-to-pointwise continuous, do I show that if $\lim_{n \rightarrow \infty}{F_n}=F$, then $ \lim_{n \rightarrow \infty}{L^\#F_n}= L^\#F$? $(2)$ Why there exists a linear map $\hat{L}$ after showing that $L^\#$ is point-to-pointwise continuous? $(3)$ Is that a typo in $\| L^\# \| = Lip(L)$? I think it should be $\| L^\# \| = \| L \|_{Lip}$. After that, how to show that $\| L^\# \| = \| L \|_{Lip}$? $(4)$ How to show that the map $\hat{L}$ is unique?","For any $x \in X$, define the set $\mathcal{F}(X) = \overline{span \{ \delta_x : x \in X \}}$ where $\delta_x(f)=f(x)$ for all $f \in$ $Lip_0(X)$. The set Lip$_0(X)$ is the set of all real-valued Lipschitz functions which vanish at $0$. Note that $\delta_x$ is an evaluation functional on Lip$_0(X)$. Suppose $X$ and $Y$ are Banach spaces. Let $L:X \rightarrow Y$ be a   Lipschitz map such that $L(0)=0$. Then there exists a unique linear   map $\hat{L}: \mathcal{F}(X) \rightarrow \mathcal{F}(Y)$ such that   $\hat{L}\delta_X = \delta_YL$ The statement above is Lemma $2.2$ . The following is its proof: The linear map $L^\# : Lip_0(Y) \rightarrow Lip_0(X)$ defined by $L^\#(F) = F \circ L$ is pointwise-to-pointwise continuous, hence there is a linear map $\hat{L}$ between the preduals such that $\hat{L^*} = L^\#$. It is clear that $\| L^\# \| = Lip(L)$, and $\| \hat{L} \| = \| \hat{L^*} \| = \| L^\# \|$. The other assertions are clear. Question: $(1)$ To show the map $L^\#$ is pointwise-to-pointwise continuous, do I show that if $\lim_{n \rightarrow \infty}{F_n}=F$, then $ \lim_{n \rightarrow \infty}{L^\#F_n}= L^\#F$? $(2)$ Why there exists a linear map $\hat{L}$ after showing that $L^\#$ is point-to-pointwise continuous? $(3)$ Is that a typo in $\| L^\# \| = Lip(L)$? I think it should be $\| L^\# \| = \| L \|_{Lip}$. After that, how to show that $\| L^\# \| = \| L \|_{Lip}$? $(4)$ How to show that the map $\hat{L}$ is unique?",,"['functional-analysis', 'proof-verification', 'banach-spaces', 'lipschitz-functions']"
32,Simple proof of Hahn-Banach in finite dimensional space,Simple proof of Hahn-Banach in finite dimensional space,,"Let $E$ be a finite dimensional Banach space and $C \subseteq E$ a non empty and convex set such that $0 \notin C$. I have to use the following steps to prove that $\exists T \in E'$ such that $T(x) \geq 0 \, \forall x \in C$: 1) Let $(x_n)_{n=1}^\infty\subset C$ dense and consider $C_n = \text{co}(x_1,\dots,x_n)$. Prove that $C_n$ is compact and $D=\bigcup_n C_n$ is dense in $C$. 2) Prove that exists $T_n \in E'$ such that $||T_n||=1$ and $T_n(x) \geq 0$ for all $x \in C_n$. 3) Prove that exists $T \in E'$ such that $||T||=1$ and $T(x) \geq 0$ for all $x \in C$. My solution: 1) Let $f\colon \mathbb{R}^n \to E$, $f(\lambda_1,\dots,\lambda_n)=\sum_{i=1}^n \lambda_ix_i$ then $C_n = f(S)$ with $S = \{\lambda \in \mathbb{R}^n \colon \lambda \geq 0, \, \sum_{i=1}^n \lambda_i = 1\}$. hence $C_n$ is compact. $D=\bigcup_n C_n$ is dense because $(x_n) \subset D$. 2) I don't know how to do this. 3) $E'$ is finite dimensional and $T_n$ lives on the closed ball, then exists a subsequence $(T_{n_k})_k$ which converges to an operator $T$ with $||T||=1$. Now if $x \in D$ then $\exists N>0$ such that $x \in C_n$ for all $n \geq N$, then we have $$T_{n_k}(x) \geq 0 $$ for $k$ big. Taking limit we get $T(x) \geq 0$ for all $x \in D$ and this extend to all $C$ because $D$ is dense on $C$. I need help with the second part. any help will be appreciated.","Let $E$ be a finite dimensional Banach space and $C \subseteq E$ a non empty and convex set such that $0 \notin C$. I have to use the following steps to prove that $\exists T \in E'$ such that $T(x) \geq 0 \, \forall x \in C$: 1) Let $(x_n)_{n=1}^\infty\subset C$ dense and consider $C_n = \text{co}(x_1,\dots,x_n)$. Prove that $C_n$ is compact and $D=\bigcup_n C_n$ is dense in $C$. 2) Prove that exists $T_n \in E'$ such that $||T_n||=1$ and $T_n(x) \geq 0$ for all $x \in C_n$. 3) Prove that exists $T \in E'$ such that $||T||=1$ and $T(x) \geq 0$ for all $x \in C$. My solution: 1) Let $f\colon \mathbb{R}^n \to E$, $f(\lambda_1,\dots,\lambda_n)=\sum_{i=1}^n \lambda_ix_i$ then $C_n = f(S)$ with $S = \{\lambda \in \mathbb{R}^n \colon \lambda \geq 0, \, \sum_{i=1}^n \lambda_i = 1\}$. hence $C_n$ is compact. $D=\bigcup_n C_n$ is dense because $(x_n) \subset D$. 2) I don't know how to do this. 3) $E'$ is finite dimensional and $T_n$ lives on the closed ball, then exists a subsequence $(T_{n_k})_k$ which converges to an operator $T$ with $||T||=1$. Now if $x \in D$ then $\exists N>0$ such that $x \in C_n$ for all $n \geq N$, then we have $$T_{n_k}(x) \geq 0 $$ for $k$ big. Taking limit we get $T(x) \geq 0$ for all $x \in D$ and this extend to all $C$ because $D$ is dense on $C$. I need help with the second part. any help will be appreciated.",,['functional-analysis']
33,Why lower semicontinuity?,Why lower semicontinuity?,,"I'm reading a proof on the existence of a solution to a minimisation problem, but I'm stuck. I give a brief summary of the arguments up to the point at which I'm stuck(at the yellow box). Information: The feasible set $X$ is sequentially compact with respect to weak$^\star$ convergence, and the objective function $f: X \longrightarrow \mathbb{R}$ is nonnegative. The arguments: We consider the set \begin{align} W = \{ f(x): x \in X \}. \end{align} Now, since $f$ is nonnegative, $W$ is bounded from below and therefore has an infimum. Write $\kappa = \inf W$. Then, by the definition of the infimum there is a sequence $(f(x_n))_n \subset W$ such that \begin{align} f(x_n) \rightarrow \kappa. \end{align} By sequential compactness we can find a convergent subsequence $(x_{n_k})_k$ of $(x_n)_n$, such that \begin{align} x_{n_k}\overset{w^\star}{\rightarrow}x^\star \end{align} for some $x \in X$. Clearly, we also have that \begin{align} f(x_{n_k}) \rightarrow \kappa. \end{align} Hence, the proof is completed by showing that the functional $f$ is lower semicontinuous, that is, if $(x_n)_n$ is convergent with weak$^\star$ limit $x$, then    \begin{align} \lim\inf\limits_{n \rightarrow \infty} f(x_n) \geq f(x). \end{align} My problem: I don't see why the argument if the yellow box works. I guess I am confused about what is meant by existence here. I thought that existence of a solution was defined as follows: There is an $x^\star \in X$ such that \begin{align} \inf_{x \in X}f(x) = f(x^\star). \end{align} I do not see how the lower semicontinuity of $f$ implies this. More information: $X$ is convex.","I'm reading a proof on the existence of a solution to a minimisation problem, but I'm stuck. I give a brief summary of the arguments up to the point at which I'm stuck(at the yellow box). Information: The feasible set $X$ is sequentially compact with respect to weak$^\star$ convergence, and the objective function $f: X \longrightarrow \mathbb{R}$ is nonnegative. The arguments: We consider the set \begin{align} W = \{ f(x): x \in X \}. \end{align} Now, since $f$ is nonnegative, $W$ is bounded from below and therefore has an infimum. Write $\kappa = \inf W$. Then, by the definition of the infimum there is a sequence $(f(x_n))_n \subset W$ such that \begin{align} f(x_n) \rightarrow \kappa. \end{align} By sequential compactness we can find a convergent subsequence $(x_{n_k})_k$ of $(x_n)_n$, such that \begin{align} x_{n_k}\overset{w^\star}{\rightarrow}x^\star \end{align} for some $x \in X$. Clearly, we also have that \begin{align} f(x_{n_k}) \rightarrow \kappa. \end{align} Hence, the proof is completed by showing that the functional $f$ is lower semicontinuous, that is, if $(x_n)_n$ is convergent with weak$^\star$ limit $x$, then    \begin{align} \lim\inf\limits_{n \rightarrow \infty} f(x_n) \geq f(x). \end{align} My problem: I don't see why the argument if the yellow box works. I guess I am confused about what is meant by existence here. I thought that existence of a solution was defined as follows: There is an $x^\star \in X$ such that \begin{align} \inf_{x \in X}f(x) = f(x^\star). \end{align} I do not see how the lower semicontinuity of $f$ implies this. More information: $X$ is convex.",,"['functional-analysis', 'optimization', 'convex-optimization']"
34,Part of Lomonosov's Invariant Subspace Theorem,Part of Lomonosov's Invariant Subspace Theorem,,"Let $X$ be a complex Banach space of infinite dimension, let $T\in\mathcal{B}(X)\backslash\{0\}$ be compact. Define $$\Gamma := \{S\in\mathcal{B}(X)\,|\,S\circ T=T\circ S\}$$and define, for each $y\in X$, $$\Gamma(y):=\{S(y)\,|\,S\in\Gamma\}$$ I am trying to prove that $\Gamma(y)\in Closed(X)$ for each $y\in X$. I have already proven that $\Gamma \in Closed(\mathcal{B}(X))$, which was easy, but I don't see how from that it follows that $\Gamma(y)$ is closed. This is part of theorem $\boxed{10.35}$ (""Lomonosolv's Invariant Subspace Theorem"") in Rudin's Functional Analysis book. What I have tried: I tried to take a convergent sequence in $\Gamma(y)$ and show it converges to a point necessarily within $\Gamma(y)$. That entails taking a sequence of points in $\Gamma$, $\{S_n\}_{n\in\mathbb{N}}$ such that the limit exits in $X$: $$\lim_{n\to\infty} S_n(y)$$Now if it would be possible to show that $\{S_n\}_{n\in\mathbb{N}}$ converges to some $S\in\Gamma$ then we would be finished. However, I'm not sure how to use the data to show that, because in order for $\{S_n\}_{n\in\mathbb{N}}$ to converge you need to know something about, let's say, $||S_{n_1}-S_{n_2}|| $ whereas you only know something about $||S_{n_1}(y)-S_{n_2}(y)||$ and you then only have $||S_{n_1}(y)-S_{n_2}(y)||\leq||S_{n_1}-S_{n_2}||||y|| $ by linearity. I tried to define a mapping $\Psi_y:\mathcal{B}(X)\to X$ by $S\mapsto S(y)$. Then $\Psi$ is linear and continuous. The goal would be to prove $\Psi_y$ is a closed mapping, but I am not sure how to do that.","Let $X$ be a complex Banach space of infinite dimension, let $T\in\mathcal{B}(X)\backslash\{0\}$ be compact. Define $$\Gamma := \{S\in\mathcal{B}(X)\,|\,S\circ T=T\circ S\}$$and define, for each $y\in X$, $$\Gamma(y):=\{S(y)\,|\,S\in\Gamma\}$$ I am trying to prove that $\Gamma(y)\in Closed(X)$ for each $y\in X$. I have already proven that $\Gamma \in Closed(\mathcal{B}(X))$, which was easy, but I don't see how from that it follows that $\Gamma(y)$ is closed. This is part of theorem $\boxed{10.35}$ (""Lomonosolv's Invariant Subspace Theorem"") in Rudin's Functional Analysis book. What I have tried: I tried to take a convergent sequence in $\Gamma(y)$ and show it converges to a point necessarily within $\Gamma(y)$. That entails taking a sequence of points in $\Gamma$, $\{S_n\}_{n\in\mathbb{N}}$ such that the limit exits in $X$: $$\lim_{n\to\infty} S_n(y)$$Now if it would be possible to show that $\{S_n\}_{n\in\mathbb{N}}$ converges to some $S\in\Gamma$ then we would be finished. However, I'm not sure how to use the data to show that, because in order for $\{S_n\}_{n\in\mathbb{N}}$ to converge you need to know something about, let's say, $||S_{n_1}-S_{n_2}|| $ whereas you only know something about $||S_{n_1}(y)-S_{n_2}(y)||$ and you then only have $||S_{n_1}(y)-S_{n_2}(y)||\leq||S_{n_1}-S_{n_2}||||y|| $ by linearity. I tried to define a mapping $\Psi_y:\mathcal{B}(X)\to X$ by $S\mapsto S(y)$. Then $\Psi$ is linear and continuous. The goal would be to prove $\Psi_y$ is a closed mapping, but I am not sure how to do that.",,"['functional-analysis', 'banach-spaces', 'banach-algebras']"
35,When can we exchange the trace and an integral/limit/derivative?,When can we exchange the trace and an integral/limit/derivative?,,"For a trace class operator $A$ (acting on a Hilbert space), that is parameterised by a real variable $x$, what are the conditions for the following to hold? $$ \mathrm{tr} \int_a^b A(x) \, dx = \int_a^b \mathrm{tr} \, A(x) \, dx $$ $$ \mathrm{tr} \lim_{x \to a} A(x) = \lim_{x \to a} \mathrm{tr} \, A(x) $$ $$ \mathrm{tr} \frac{d A(x)}{dx} = \frac{d}{dx} \mathrm{tr} \, A(x) $$ I guess you could always pick a basis, write the expressions as infinite sums and use uniform convergence as a criterium. Is there a more economical way?","For a trace class operator $A$ (acting on a Hilbert space), that is parameterised by a real variable $x$, what are the conditions for the following to hold? $$ \mathrm{tr} \int_a^b A(x) \, dx = \int_a^b \mathrm{tr} \, A(x) \, dx $$ $$ \mathrm{tr} \lim_{x \to a} A(x) = \lim_{x \to a} \mathrm{tr} \, A(x) $$ $$ \mathrm{tr} \frac{d A(x)}{dx} = \frac{d}{dx} \mathrm{tr} \, A(x) $$ I guess you could always pick a basis, write the expressions as infinite sums and use uniform convergence as a criterium. Is there a more economical way?",,"['functional-analysis', 'operator-theory']"
36,Converse of uniform boundedness principle,Converse of uniform boundedness principle,,"The uniform boundedness principle says if we have a collection of bounded linear operators $\Gamma$ from a banach space $X$ into a normed vector space $Y$, which is pointwise bounded on $X$, i.e. $$\sup \{\|Tx\| : T \in \Gamma \} < \infty \, \forall x \in X,$$ then $\Gamma$ must be uniformly bounded, $$\sup \{\|T\| : T \in \Gamma \} < \infty.$$ Now, I wonder is there some kind of converse statement? So, if $X$ is a normed space and the following holds for every normed space $Y$: $$\Gamma \subset \mathcal{L}(X,Y) \text{ pointwise bounded on }X \Rightarrow \Gamma \text{ uniformly bounded},$$ where $\mathcal{L}(X,Y)$ is the space of bounded linear operators from $X$ to $Y$. Then does it follow that $X$ is a banach space? Edit: Or are there any partial converses? Edit 2: I think I need to clarify further. The converses I am looking are not of the form ""uniformly bounded $\Rightarrow$ pointwise bounded"". Instead we should have a space $X$, more general than a Banach Space, in which the uniform boundedness principle holds. So every pointwise bounded collection $\Gamma$ of linear bounded operators (into an arbitrary space $Y$) must be uniformly bounded. And this should then imply that $X$ is a Banach space. It's okay if the theorem requires more conditions, than just the uniform boundedness principle, on $X$ for the conclusion to hold.","The uniform boundedness principle says if we have a collection of bounded linear operators $\Gamma$ from a banach space $X$ into a normed vector space $Y$, which is pointwise bounded on $X$, i.e. $$\sup \{\|Tx\| : T \in \Gamma \} < \infty \, \forall x \in X,$$ then $\Gamma$ must be uniformly bounded, $$\sup \{\|T\| : T \in \Gamma \} < \infty.$$ Now, I wonder is there some kind of converse statement? So, if $X$ is a normed space and the following holds for every normed space $Y$: $$\Gamma \subset \mathcal{L}(X,Y) \text{ pointwise bounded on }X \Rightarrow \Gamma \text{ uniformly bounded},$$ where $\mathcal{L}(X,Y)$ is the space of bounded linear operators from $X$ to $Y$. Then does it follow that $X$ is a banach space? Edit: Or are there any partial converses? Edit 2: I think I need to clarify further. The converses I am looking are not of the form ""uniformly bounded $\Rightarrow$ pointwise bounded"". Instead we should have a space $X$, more general than a Banach Space, in which the uniform boundedness principle holds. So every pointwise bounded collection $\Gamma$ of linear bounded operators (into an arbitrary space $Y$) must be uniformly bounded. And this should then imply that $X$ is a Banach space. It's okay if the theorem requires more conditions, than just the uniform boundedness principle, on $X$ for the conclusion to hold.",,"['functional-analysis', 'banach-spaces', 'normed-spaces']"
37,An inequality for positive operators,An inequality for positive operators,,"Let $S$ and $T$ be positive operators on a Hilbert space $\mathcal{H}$. Suppose that $S \le T$. Since the square root function is operator monotone, it follows that $S^{1/2} \le T^{1/2}$. Does the inequality $$S^{1/2}RS^{1/2} \le T^{1/2}RT^{1/2}$$ hold for all positive operators $R$?","Let $S$ and $T$ be positive operators on a Hilbert space $\mathcal{H}$. Suppose that $S \le T$. Since the square root function is operator monotone, it follows that $S^{1/2} \le T^{1/2}$. Does the inequality $$S^{1/2}RS^{1/2} \le T^{1/2}RT^{1/2}$$ hold for all positive operators $R$?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
38,Density of smooth compactly supported functions in Sobolev space over unbounded domain.,Density of smooth compactly supported functions in Sobolev space over unbounded domain.,,"Prove that $C^{\infty}_ {c}(\mathbb{R}^n)$ is dense in $W^{k,p}(U)$ for any open $U\subset \mathbb{R}^n$ with $\partial U\in C^1.$ In which $p\in [1,\infty)$ Note: In Lawrence Evans's PDE text, the case where $U$ is bounded was proved as a Theorem.  It thus only remained to prove the case when $U$ is unbounded. Given $f \in W^{k,p}(U).$ I am thinking about partition $U$ into a sequence of annulli. Let $A_n:= \{x\in \mathbb{R}^n:n-1<|x|<n\},\forall n\in \mathbb{N}$ and let $U_n:=A_n\cap U.$ Then each $U_n$ satisfies the hypothesis of the Theorem for being a bounded set, and thus on each $U_n$ there exists a sequence $(f^{(n)}_j:j\in \mathbb{N}) \subset C^{\infty}_ {c}(U_n)$ which converges to the restriction $f\chi_{U_n}.$ But, how does one guarentee that the ""glued-fuction"" $\sum_{n=1}^{\infty}f^{(n)}_j\chi_{U_n}$ for each fixed $j$ is in $C^{\infty}_ {c}(U)$?? Is my idea plausible? If so, how to make it rigorous? Thanks for any feedback!","Prove that $C^{\infty}_ {c}(\mathbb{R}^n)$ is dense in $W^{k,p}(U)$ for any open $U\subset \mathbb{R}^n$ with $\partial U\in C^1.$ In which $p\in [1,\infty)$ Note: In Lawrence Evans's PDE text, the case where $U$ is bounded was proved as a Theorem.  It thus only remained to prove the case when $U$ is unbounded. Given $f \in W^{k,p}(U).$ I am thinking about partition $U$ into a sequence of annulli. Let $A_n:= \{x\in \mathbb{R}^n:n-1<|x|<n\},\forall n\in \mathbb{N}$ and let $U_n:=A_n\cap U.$ Then each $U_n$ satisfies the hypothesis of the Theorem for being a bounded set, and thus on each $U_n$ there exists a sequence $(f^{(n)}_j:j\in \mathbb{N}) \subset C^{\infty}_ {c}(U_n)$ which converges to the restriction $f\chi_{U_n}.$ But, how does one guarentee that the ""glued-fuction"" $\sum_{n=1}^{\infty}f^{(n)}_j\chi_{U_n}$ for each fixed $j$ is in $C^{\infty}_ {c}(U)$?? Is my idea plausible? If so, how to make it rigorous? Thanks for any feedback!",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'regularity-theory-of-pdes']"
39,Properties of the square norm in Banach spaces,Properties of the square norm in Banach spaces,,"Let $X$ be a Banach space with its dual $X^*$. Consider the mapping $f: X\rightarrow \mathbb{R}$ given by $$ f(x)=\frac{1}{2}\|x\|^2. $$ We have know that when $X$ is a real Hilbert space ($X=X^*$) then $f$ is strongly convex with modulus $\lambda=1$, i.e. $$ \alpha f(x)+(1-\alpha)f(y)\geq f(\alpha x+(1-\alpha)y)+\frac{\alpha(1-\alpha)}{2}\|x-y\|^2 $$ for all $x, y\in X$ and $\lambda\in [0, 1]$. Moreover, $f$ is Frechet differentiable and $$ \nabla_F f(x)=x\quad \forall x\in X. $$ I do not know besides Hilbert space, what kind of Banach spaces do we have two above properties (Frechet differentiability and strong convexity) of the function $f(x)$. I would like to thank for all constructive comments, helping and pointing out the references related to this problem. Note. If $X^*$ is strictly convex then $f(x)=\frac{1}{2}\|x\|^2$ is Gateaux differentiable and $$ \nabla_G f(x)=\{x^*\in X^*: \|x^*\|^2=\|x\|^2=\langle x^*, x\rangle\}; $$ If $f(x)$ is strongly convex with constant $\lambda=1$ then $X$ is a Hilbert space.","Let $X$ be a Banach space with its dual $X^*$. Consider the mapping $f: X\rightarrow \mathbb{R}$ given by $$ f(x)=\frac{1}{2}\|x\|^2. $$ We have know that when $X$ is a real Hilbert space ($X=X^*$) then $f$ is strongly convex with modulus $\lambda=1$, i.e. $$ \alpha f(x)+(1-\alpha)f(y)\geq f(\alpha x+(1-\alpha)y)+\frac{\alpha(1-\alpha)}{2}\|x-y\|^2 $$ for all $x, y\in X$ and $\lambda\in [0, 1]$. Moreover, $f$ is Frechet differentiable and $$ \nabla_F f(x)=x\quad \forall x\in X. $$ I do not know besides Hilbert space, what kind of Banach spaces do we have two above properties (Frechet differentiability and strong convexity) of the function $f(x)$. I would like to thank for all constructive comments, helping and pointing out the references related to this problem. Note. If $X^*$ is strictly convex then $f(x)=\frac{1}{2}\|x\|^2$ is Gateaux differentiable and $$ \nabla_G f(x)=\{x^*\in X^*: \|x^*\|^2=\|x\|^2=\langle x^*, x\rangle\}; $$ If $f(x)$ is strongly convex with constant $\lambda=1$ then $X$ is a Hilbert space.",,"['functional-analysis', 'convex-analysis', 'convex-optimization']"
40,Lower semicontinuity of indicator function,Lower semicontinuity of indicator function,,"For any set $\mathcal{S} \subseteq \mathbb{R}^{N}$, let us define the indicator function $$\delta_{\mathcal{S}}(\mathbf{x}) \triangleq \begin{cases} 0, & \quad \textrm{if } \mathbf{x} \in \mathcal{S} \\ \infty, & \quad \textrm{otherwise}. \end{cases}$$ Is this function lower semicontinuous? I suppose it is if the set $\mathcal{S}$ is closed, but I'm not absolutely sure.","For any set $\mathcal{S} \subseteq \mathbb{R}^{N}$, let us define the indicator function $$\delta_{\mathcal{S}}(\mathbf{x}) \triangleq \begin{cases} 0, & \quad \textrm{if } \mathbf{x} \in \mathcal{S} \\ \infty, & \quad \textrm{otherwise}. \end{cases}$$ Is this function lower semicontinuous? I suppose it is if the set $\mathcal{S}$ is closed, but I'm not absolutely sure.",,"['functional-analysis', 'optimization']"
41,Why do we need dual space [closed],Why do we need dual space [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question In functional analysis there are many places where dual space is mentioned, but I still don't understand the real power of that concept. Why do we need the dual space?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question In functional analysis there are many places where dual space is mentioned, but I still don't understand the real power of that concept. Why do we need the dual space?",,"['functional-analysis', 'soft-question', 'vector-spaces', 'topological-vector-spaces']"
42,Show that a Bilinear form is Coercive,Show that a Bilinear form is Coercive,,"I'm reading through Brezis' book on functional analysis, Sobolev spaces and PDE, and I'm having trouble showing that the Bilinear form: $a(u,v) = \int_{0}^{2}u'v'dx+\left(\int_{0}^{1}u dx\right) \left(\int_{0}^{1}v dx\right)$ is coercive.(Problem 8.25 #2). The book offers the following hint: (Argue by contradiction and assume that there is a sequence $(u_n)$ in $H^1$ such that $a(u_n,u_n)\rightarrow 0$ and $||u_n||_{H^1}=1$. Let $(u_{n_k})$ be a subsequence such that $u_{n_k} \rightharpoonup u$ in $H^1$ and $||u_{n_k}||_{L^2}\rightarrow||u||_{L^2}$ for some limit u. Show that u=0. From the previous question I have: $|a(u,v)|\leq ||u||_{H^1}||v||_{H^1}~~~~~~~$     and $~~~~~~~a(u,u)=0 ~~~\Rightarrow~~~ u=0$ I've been throwing ideas at this for most of today and I've failed to come up with anything leading to the contradiction. Any suggestions for the right place to start?","I'm reading through Brezis' book on functional analysis, Sobolev spaces and PDE, and I'm having trouble showing that the Bilinear form: $a(u,v) = \int_{0}^{2}u'v'dx+\left(\int_{0}^{1}u dx\right) \left(\int_{0}^{1}v dx\right)$ is coercive.(Problem 8.25 #2). The book offers the following hint: (Argue by contradiction and assume that there is a sequence $(u_n)$ in $H^1$ such that $a(u_n,u_n)\rightarrow 0$ and $||u_n||_{H^1}=1$. Let $(u_{n_k})$ be a subsequence such that $u_{n_k} \rightharpoonup u$ in $H^1$ and $||u_{n_k}||_{L^2}\rightarrow||u||_{L^2}$ for some limit u. Show that u=0. From the previous question I have: $|a(u,v)|\leq ||u||_{H^1}||v||_{H^1}~~~~~~~$     and $~~~~~~~a(u,u)=0 ~~~\Rightarrow~~~ u=0$ I've been throwing ideas at this for most of today and I've failed to come up with anything leading to the contradiction. Any suggestions for the right place to start?",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'bilinear-form']"
43,Integral kernel of the resolvent operator,Integral kernel of the resolvent operator,,"Suppose we have an explicit formula for the integral kernel $k(x,y)$ of an operator $D$ acting on smooth $\mathbb{C}^n$-valued functions defined on an interval $[0,\beta]$, that is $$ Df(x) = \int_0^\beta k(x,y)f(y)\,dy \,,\quad f \in C^\infty([0,\beta],\mathbb{C}^n)\,.  $$ I need to use the resolvent $(D - \lambda)^{-1}$, is there a way to immediately deduce a formula for its kernel, given that I have complete knowledge about $k(x,y)$? In case the answer would be too extensive I would also be grateful for a reference. Thank you!","Suppose we have an explicit formula for the integral kernel $k(x,y)$ of an operator $D$ acting on smooth $\mathbb{C}^n$-valued functions defined on an interval $[0,\beta]$, that is $$ Df(x) = \int_0^\beta k(x,y)f(y)\,dy \,,\quad f \in C^\infty([0,\beta],\mathbb{C}^n)\,.  $$ I need to use the resolvent $(D - \lambda)^{-1}$, is there a way to immediately deduce a formula for its kernel, given that I have complete knowledge about $k(x,y)$? In case the answer would be too extensive I would also be grateful for a reference. Thank you!",,"['functional-analysis', 'partial-differential-equations', 'operator-theory']"
44,Why is this space dense in this Sobolev space? (Bochner spaces),Why is this space dense in this Sobolev space? (Bochner spaces),,"Let $V$ be a separable Banach space, and let $H$ be a Hilbert space. We have the Hilbert triple $$V \subset H \subset V^*.$$ By separability, there exists subspaces $V_k$ with $V_k \subset V_{k+1}$ such that $\bigcup_k V_k \subset V$ dense. Let $W^{1} = \{ u \in L^2(0,T;V) : u' \in L^2(0,T;V^*), u(0) = u(T)\}$ and endow it with the norm $$\lVert {u}\rVert_{W^1} = \lVert{u}\rVert_{L^2(0,T;V)} + \lVert{u'}\rVert_{L^2(0,T;V^*)}.$$ Let $S_k = \text{span}(\varphi v : v \in V_k, \varphi \in C([0,T]), \varphi(0) = \varphi(T)\}$. The claim is that $\bigcup_k S_k$ is dense in $W^1.$ No idea why this is true?","Let $V$ be a separable Banach space, and let $H$ be a Hilbert space. We have the Hilbert triple $$V \subset H \subset V^*.$$ By separability, there exists subspaces $V_k$ with $V_k \subset V_{k+1}$ such that $\bigcup_k V_k \subset V$ dense. Let $W^{1} = \{ u \in L^2(0,T;V) : u' \in L^2(0,T;V^*), u(0) = u(T)\}$ and endow it with the norm $$\lVert {u}\rVert_{W^1} = \lVert{u}\rVert_{L^2(0,T;V)} + \lVert{u'}\rVert_{L^2(0,T;V^*)}.$$ Let $S_k = \text{span}(\varphi v : v \in V_k, \varphi \in C([0,T]), \varphi(0) = \varphi(T)\}$. The claim is that $\bigcup_k S_k$ is dense in $W^1.$ No idea why this is true?",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
45,Can we visualize the closed balls for the space $l^{\infty}$ equipped with the $\sup$ norm,Can we visualize the closed balls for the space  equipped with the  norm,l^{\infty} \sup,The closed unit balls for the $l^{p}$ in $\mathbb{R}^2$ look like I want to know could we also visualize the closed balls for the space $l^{\infty}$ equipped with the $\sup$ norm . Thanks,The closed unit balls for the $l^{p}$ in $\mathbb{R}^2$ look like I want to know could we also visualize the closed balls for the space $l^{\infty}$ equipped with the $\sup$ norm . Thanks,,"['functional-analysis', 'banach-spaces']"
46,Maximal abelian subalgebra of Banach algebra is closed and contains the unit,Maximal abelian subalgebra of Banach algebra is closed and contains the unit,,"I'm studying Murphy's book: C*-Algebras and Operator Theory, and got stuck on exercise 8 from chapter 1: ""Show that if $B$ is a maximal abelian subalgebra of a unital Banach algebra $A$, then $B$ is closed and contains the unit. Show that $\sigma_A(b)=\sigma_B(b)$ for all $b\in B$."" (where $\sigma_A(b)=\left\{\lambda\in\mathbb{C}:\lambda 1-b\text{ is not invertible in }A\right\}$, and $\sigma_B(b)$ is defined analogously) I tried to argue by contradiction: suppose that $B$ is not closed. Then $\overline{B}$ is a closed commutative subalgebra of $A$ that contains $B$ strictly. By maximality, we have $\overline{B}=A$, hence $B$ is a dense maximal subalgebra of a unital commutative Banach algebra. But I don't see how this leads to a contradiction, although it smells like Gelfand Transform... EDIT: Actually, I tried to show that $B$ must contain the unit, and maybe the problem is wrong: Let $B$ be a commutative non-unital Banach algebra, for example, $B=C_0(\Omega)$, where $\Omega$ is a non-compact, locally compact, Hausdorff topological space, say $\mathbb{R}$. Let $A=B\oplus\mathbb{C}$ be its unitization, that is $A=B\times\mathbb{C}$ as a set and the operations on $A$ are defined as $$(b,\alpha)+\gamma(c,\beta)=(b+\gamma c,\alpha+\gamma\beta),\quad (b,\alpha)(c,\beta)=(bc+\alpha c+\beta b,\alpha\beta)$$ and $A$ has the norm $\Vert(b,\alpha)\Vert=\Vert b\Vert+|\alpha|$. It is easily checked that $A$ is a commutative Banach algebra with unit $(0,1)$. If we identify $B$ with the subalgebra $B\oplus\left\{ 0\right\}$ of $A$, then it's clear that $B$ is a maximal commutative subalgebra of $A$ which does not contain the unit. If my arguments were correct, then it is not true in general that $B$ contains the unit. But maybe we can still assure that $B$ is closed in $A$ and that, if $B$ contains the unit, then $\sigma_A(b)=\sigma_B(b)$ for every $b\in B$.","I'm studying Murphy's book: C*-Algebras and Operator Theory, and got stuck on exercise 8 from chapter 1: ""Show that if $B$ is a maximal abelian subalgebra of a unital Banach algebra $A$, then $B$ is closed and contains the unit. Show that $\sigma_A(b)=\sigma_B(b)$ for all $b\in B$."" (where $\sigma_A(b)=\left\{\lambda\in\mathbb{C}:\lambda 1-b\text{ is not invertible in }A\right\}$, and $\sigma_B(b)$ is defined analogously) I tried to argue by contradiction: suppose that $B$ is not closed. Then $\overline{B}$ is a closed commutative subalgebra of $A$ that contains $B$ strictly. By maximality, we have $\overline{B}=A$, hence $B$ is a dense maximal subalgebra of a unital commutative Banach algebra. But I don't see how this leads to a contradiction, although it smells like Gelfand Transform... EDIT: Actually, I tried to show that $B$ must contain the unit, and maybe the problem is wrong: Let $B$ be a commutative non-unital Banach algebra, for example, $B=C_0(\Omega)$, where $\Omega$ is a non-compact, locally compact, Hausdorff topological space, say $\mathbb{R}$. Let $A=B\oplus\mathbb{C}$ be its unitization, that is $A=B\times\mathbb{C}$ as a set and the operations on $A$ are defined as $$(b,\alpha)+\gamma(c,\beta)=(b+\gamma c,\alpha+\gamma\beta),\quad (b,\alpha)(c,\beta)=(bc+\alpha c+\beta b,\alpha\beta)$$ and $A$ has the norm $\Vert(b,\alpha)\Vert=\Vert b\Vert+|\alpha|$. It is easily checked that $A$ is a commutative Banach algebra with unit $(0,1)$. If we identify $B$ with the subalgebra $B\oplus\left\{ 0\right\}$ of $A$, then it's clear that $B$ is a maximal commutative subalgebra of $A$ which does not contain the unit. If my arguments were correct, then it is not true in general that $B$ contains the unit. But maybe we can still assure that $B$ is closed in $A$ and that, if $B$ contains the unit, then $\sigma_A(b)=\sigma_B(b)$ for every $b\in B$.",,"['functional-analysis', 'banach-algebras']"
47,Spectrum of elements in $C^*$-subalgebras,Spectrum of elements in -subalgebras,C^*,"Assume $\mathcal{A}$ is a $C^*$-algebra with unit $1$ and $\mathcal{B}\subset\mathcal{A}$ is a $C^*$-subalgebra (i.e. a closed $*$-subalgebra) such that $1\in\mathcal{B}$. It is said that under these assumptions, for any $a\in\mathcal{B}$ the spectrum $\sigma_\mathcal{B}(a)$ of $a$ in $\mathcal{B}$ coincides with $\sigma_\mathcal{A}(a)$, that is: If $a-\lambda 1$ has an inverse $b\in\mathcal{A}$, then $b\in\mathcal{B}$. Now, my questions are the following: Do you know a nice proof of the above statement? I have found a proof that goes like this: If $b=(a-\lambda)^{-1}$ exists in $\mathcal{A}$, it can be expressed as a convergent power series, i.e. it is the norm limit of partial sums each belonging to $\mathcal{B}$, hene also $b\in\mathcal{B}$. Although this argument looks really nice and I'm aware of Neumann series, I do not see why $b$ can be expressed as a power series. Do you? Under the above assumptions, is the more general statement $\mathcal{B}^\times=\mathcal{A}^\times\cap B$ true? (here, we denote by $\mathcal{A}^\times$ and $\mathcal{B}^\times$ the set of invertible elements in $\mathcal{A}$ and $\mathcal{B}$, respectively)","Assume $\mathcal{A}$ is a $C^*$-algebra with unit $1$ and $\mathcal{B}\subset\mathcal{A}$ is a $C^*$-subalgebra (i.e. a closed $*$-subalgebra) such that $1\in\mathcal{B}$. It is said that under these assumptions, for any $a\in\mathcal{B}$ the spectrum $\sigma_\mathcal{B}(a)$ of $a$ in $\mathcal{B}$ coincides with $\sigma_\mathcal{A}(a)$, that is: If $a-\lambda 1$ has an inverse $b\in\mathcal{A}$, then $b\in\mathcal{B}$. Now, my questions are the following: Do you know a nice proof of the above statement? I have found a proof that goes like this: If $b=(a-\lambda)^{-1}$ exists in $\mathcal{A}$, it can be expressed as a convergent power series, i.e. it is the norm limit of partial sums each belonging to $\mathcal{B}$, hene also $b\in\mathcal{B}$. Although this argument looks really nice and I'm aware of Neumann series, I do not see why $b$ can be expressed as a power series. Do you? Under the above assumptions, is the more general statement $\mathcal{B}^\times=\mathcal{A}^\times\cap B$ true? (here, we denote by $\mathcal{A}^\times$ and $\mathcal{B}^\times$ the set of invertible elements in $\mathcal{A}$ and $\mathcal{B}$, respectively)",,"['functional-analysis', 'banach-algebras', 'c-star-algebras']"
48,A problem regarding functional calculus.,A problem regarding functional calculus.,,"Let $A$ be a Banach Space and $T$ be a bounded operator on $A$ . Given that, spectrum of $T$ , $\sigma[T]$ , is $  F_1 \cup F_2;$ where $  F_1, F_2$ are disjoint closed set in complex plane.   Show that there exist topologically complemented subspace $A_1,A_2$ of $A$ such that $A_1,A_2$ are invariant subspace for $T$ and $\sigma(T|A_i)=F_i$ for $i=1,2.$ Until now what I have done is, taking disjoint open set $G_i$ containing $F_i$ and have chosen $f_i= 1_{G_i}-$ the characteristic function on $G_i$ , (which are actually analytic on $G_1\cup G_2$ ). Then I have taken $A_i$ as range of the projection $f_i(T)$ , using the functional calculus for $T$ . Using the spectral mapping theorem one can tell that, $\sigma(Tf_i(T))= F_i\cup$ { $0$ }. If my guess is correct then I have to show $\sigma(Tf_i(T)|A_i)= F_i$ . At this stage I need help.","Let be a Banach Space and be a bounded operator on . Given that, spectrum of , , is where are disjoint closed set in complex plane.   Show that there exist topologically complemented subspace of such that are invariant subspace for and for Until now what I have done is, taking disjoint open set containing and have chosen the characteristic function on , (which are actually analytic on ). Then I have taken as range of the projection , using the functional calculus for . Using the spectral mapping theorem one can tell that, { }. If my guess is correct then I have to show . At this stage I need help.","A T A T \sigma[T]   F_1 \cup F_2;   F_1, F_2 A_1,A_2 A A_1,A_2 T \sigma(T|A_i)=F_i i=1,2. G_i F_i f_i= 1_{G_i}- G_i G_1\cup G_2 A_i f_i(T) T \sigma(Tf_i(T))= F_i\cup 0 \sigma(Tf_i(T)|A_i)= F_i",['functional-analysis']
49,Convolution of measures,Convolution of measures,,"We say that a family of measures $\mu_{t}\to \mu$ weakly if for any $g\in C_{0}$, $\int g d\mu_{t} \to \int g d\mu$. Show that if $\mu_{t}\to \mu$ weakly, then $\nu*\mu_{t}\to \nu*\mu$ weakly, where $\nu*\mu$ denotes the convolution of the measures $\mu$ and $\nu$.","We say that a family of measures $\mu_{t}\to \mu$ weakly if for any $g\in C_{0}$, $\int g d\mu_{t} \to \int g d\mu$. Show that if $\mu_{t}\to \mu$ weakly, then $\nu*\mu_{t}\to \nu*\mu$ weakly, where $\nu*\mu$ denotes the convolution of the measures $\mu$ and $\nu$.",,"['measure-theory', 'functional-analysis', 'convolution']"
50,Hahn-Banach to extend to the Lebesgue Measure,Hahn-Banach to extend to the Lebesgue Measure,,"I remember reading an example in a textbook that went something like this: if we take a function $\ell(f) = \int_{0}^{1}f(t)\, dt$, (with this being the Riemann integral) defined only on the set of continuous functions on $[0,1]$, then we may extend it to all bounded functions in $[0,1]$ by noting that $p(f) = \sup_{[0,1]}(|f(x)|)$ satisfies the criteria of the Hahn-Banach theorem. The textbook went on to note that this defined a finitely additive set function on every subset of $[0,1]$ via allowing our set function $w(A) = \ell(\chi_{A})$.  It goes on to note that, for Borel sets, this actually forms the Lebesgue measure. Question 1. What is the reason for the ""finitely additive"" part of $w$ in the latter paragraph?  Is it just because we required bounded functions? Question 2. How can one see that this forms the Lebesgue measure if we consider Borel Measurable sets?  I do not think this comes from Hahn-Banach. Question 3. Is the reason that we cannot easily extend this to a larger domain (like, say, rational functions on $[0,1]$ or something) that the $\sup$ function is no longer adequate, and there is no longer a function which satisfies Hahn-Banach?","I remember reading an example in a textbook that went something like this: if we take a function $\ell(f) = \int_{0}^{1}f(t)\, dt$, (with this being the Riemann integral) defined only on the set of continuous functions on $[0,1]$, then we may extend it to all bounded functions in $[0,1]$ by noting that $p(f) = \sup_{[0,1]}(|f(x)|)$ satisfies the criteria of the Hahn-Banach theorem. The textbook went on to note that this defined a finitely additive set function on every subset of $[0,1]$ via allowing our set function $w(A) = \ell(\chi_{A})$.  It goes on to note that, for Borel sets, this actually forms the Lebesgue measure. Question 1. What is the reason for the ""finitely additive"" part of $w$ in the latter paragraph?  Is it just because we required bounded functions? Question 2. How can one see that this forms the Lebesgue measure if we consider Borel Measurable sets?  I do not think this comes from Hahn-Banach. Question 3. Is the reason that we cannot easily extend this to a larger domain (like, say, rational functions on $[0,1]$ or something) that the $\sup$ function is no longer adequate, and there is no longer a function which satisfies Hahn-Banach?",,['analysis']
51,Isometries of $\ell^p_n(\mathbb{C})$,Isometries of,\ell^p_n(\mathbb{C}),"Let $1<p<\infty$, and define an isometry of normed linear spaces to be a norm-preserving surjection. Then all isometries from $\ell^p_n(\mathbb{R})$ to itself are given by linear transformations $T$ such that Mat$(T)$ is product of a permutation matrix and a diagonal matrix with $1$'s and $-1$'s along the diagonal. However it is slightly more complicated to classify all isometries from $\ell^p_n(\mathbb{C})$ to itself since $z\mapsto \overline{z}$ is an isometry of $\mathbb{C}$. Question: What are all isometries from $\ell^p_n(\mathbb{C})$ to itself?","Let $1<p<\infty$, and define an isometry of normed linear spaces to be a norm-preserving surjection. Then all isometries from $\ell^p_n(\mathbb{R})$ to itself are given by linear transformations $T$ such that Mat$(T)$ is product of a permutation matrix and a diagonal matrix with $1$'s and $-1$'s along the diagonal. However it is slightly more complicated to classify all isometries from $\ell^p_n(\mathbb{C})$ to itself since $z\mapsto \overline{z}$ is an isometry of $\mathbb{C}$. Question: What are all isometries from $\ell^p_n(\mathbb{C})$ to itself?",,[]
52,On the isometry between bounded linear operators and the dual of nuclear linear operators,On the isometry between bounded linear operators and the dual of nuclear linear operators,,"Let $H$ be a separable Hilbert space. Let $(e_i)_i$ be an orthonormal basis. For any bounded linear map $T$ we write, whenever possible $$\operatorname{tr} T := \sum_{i}^{\infty} \langle T e_i, e_i \rangle$$ Now let $L(H)$ be the set of bounded operators, $N(H)$ the set of nuclear operators. We want to show $$i: L(H) \rightarrow N(H)' , \quad T \mapsto ( S \mapsto \operatorname{tr}(TS))$$ is an isometric isomorphism. By the ideal property of the nuclear operators, it is easy to see that $i$ is well-defined, linear and bounded with $|i| \leq 1$. Recall that the operators $\langle \cdot, e_i \rangle e_j$ form a basis of $N(H)$. You easily see the operator is injective (if the induced functional is zero, it's preimage must have been zero,too) and surjective (simply build up an operator $T \in L(H)$ which induces a given functional in $N(H)$). Question: How can I finish the proof with showing that $i$ is an isometry?","Let $H$ be a separable Hilbert space. Let $(e_i)_i$ be an orthonormal basis. For any bounded linear map $T$ we write, whenever possible $$\operatorname{tr} T := \sum_{i}^{\infty} \langle T e_i, e_i \rangle$$ Now let $L(H)$ be the set of bounded operators, $N(H)$ the set of nuclear operators. We want to show $$i: L(H) \rightarrow N(H)' , \quad T \mapsto ( S \mapsto \operatorname{tr}(TS))$$ is an isometric isomorphism. By the ideal property of the nuclear operators, it is easy to see that $i$ is well-defined, linear and bounded with $|i| \leq 1$. Recall that the operators $\langle \cdot, e_i \rangle e_j$ form a basis of $N(H)$. You easily see the operator is injective (if the induced functional is zero, it's preimage must have been zero,too) and surjective (simply build up an operator $T \in L(H)$ which induces a given functional in $N(H)$). Question: How can I finish the proof with showing that $i$ is an isometry?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'compact-operators', 'trace']"
53,Positive unbounded self-adjoint operator relatively bounded by sum with another positive unbounded self-adjoint operator?,Positive unbounded self-adjoint operator relatively bounded by sum with another positive unbounded self-adjoint operator?,,"Let $A$ and $B$ be two positive (i.e. $\langle A x, x \rangle \geqslant 0$ and the same for $B$ ) unbounded self-adjoint operators on a separable complex Hilbert space. Is it true that $A$ is relatively bounded by $A+B$ ? Or in other words, is it true that there exists constants $C_1$ and $C_2$ such that for all $x \in \mathcal{D}(A) \cap \mathcal{D}(B)$ (assume the intersection of domains is non-trivial), $$||Ax|| \leqslant C_1||(A+B)x|| + C_2 ||x|| ?$$ I have seen this claimed in a book, but I can't think of how to prove it. Edit 1: The reference in question is Adiabatic Perturbation Theory in Quantum Dynamics by Stefan Teufel. On page 48, equation (2.36) the second inequality, the author draws a conclusion like the one I am asking about using the fact that the operator $1 \otimes H_{e0}$ (which I called $B$ ), which is being added to an operator $\epsilon^2 D_A^2 \otimes 1$ (which I called $A$ ), is bounded below. Here is a screenshot: Edit 2: In the reference there is a tensor product structure at play, however I don't see how this could influence the specific inequality I want to understand, beyond the simple fact that $\epsilon^2 D_A^2 \otimes 1$ and $1 \otimes H_{e0}$ commute. But this isn't mentioned by the author so I suspect its not important.","Let and be two positive (i.e. and the same for ) unbounded self-adjoint operators on a separable complex Hilbert space. Is it true that is relatively bounded by ? Or in other words, is it true that there exists constants and such that for all (assume the intersection of domains is non-trivial), I have seen this claimed in a book, but I can't think of how to prove it. Edit 1: The reference in question is Adiabatic Perturbation Theory in Quantum Dynamics by Stefan Teufel. On page 48, equation (2.36) the second inequality, the author draws a conclusion like the one I am asking about using the fact that the operator (which I called ), which is being added to an operator (which I called ), is bounded below. Here is a screenshot: Edit 2: In the reference there is a tensor product structure at play, however I don't see how this could influence the specific inequality I want to understand, beyond the simple fact that and commute. But this isn't mentioned by the author so I suspect its not important.","A B \langle A x, x \rangle \geqslant 0 B A A+B C_1 C_2 x \in \mathcal{D}(A) \cap \mathcal{D}(B) ||Ax|| \leqslant C_1||(A+B)x|| + C_2 ||x|| ? 1 \otimes H_{e0} B \epsilon^2 D_A^2 \otimes 1 A \epsilon^2 D_A^2 \otimes 1 1 \otimes H_{e0}","['functional-analysis', 'operator-theory']"
54,An attempt at showing that function is lipschitzian,An attempt at showing that function is lipschitzian,,"Let $X$ be normed space and let $B$ denote the closed unit ball in $X$ . Suppose we have map $T_0: B \rightarrow B$ which satisfies Lipschitz condition with some constant $k > 1$ . We extend this map to $T_1: 2B \rightarrow B$ in the following way: $$ T_1x = \begin{cases} T_0x, & \text{for } \|x\| \leq 1, \\ (2-\|x\|)T_0(Px), & \text{for } 1 < \|x\| \leq 2, \end{cases}$$ where $Px = \frac{x}{\|x\|}$ for $\|x \| > 1$ and $Px = x$ for $\|x\| \leq 1$ ( $P$ is radial projection onto unit ball). Im reading a paper where the author claims this extended map still satisfies Lipschitz condition, this time with constant $2k + 1$ . I tried to verify this. $\textbf{Case 1}:$ If $x,y \in B$ , then $\|T_1x - T_1y\| = \|T_0x - T_0y\| \leq k\|x - y\|$ . $\textbf{Case 2}:$ Let $x \in B$ and $y \in 2B \setminus B$ . Then \begin{equation}  \begin{split} \|T_1x - T_1y\| & = \|T_0x - (2-\|y\|)T_0(Py)\| \\  &\leq \Bigl\|T_0x - T_0(Py) - T_0(Py) + \|y\|T_0(Py) \Bigr\| \\ &\leq \|T_0(Px) - T_0(Py)\| + (\|y\| - 1)\|T_0(Py)\| \\ &\leq k\|Px - Py\| + \|y\| - \|x\| \\  &\leq 2k\|x - y\| + \|x - y\| \\ & = (2k + 1)\|x - y\|. \end{split} \end{equation} Here I used the fact that in every normed space projection $P$ is Lipschitz with constant $2$ . $\textbf{Case 3}:$ Now this is where I can't finish the job. Let $x,y \in 2B \setminus B$ . I only managed to show that this map is Lipschitz with constant $8k + 1$ . \begin{equation} \begin{split} \|T_1x - T_1y\| & = \Bigl\|(2-\|x\|)T_0(Px) - (2-\|y\|)T_0(Py) \Bigr\| \\  & = \Bigl\|2T_0(Px) - 2T_0(Py) - \|x\|T_0(Px) + \|y\|T_0(Py) \Bigr\| \\ & \leq 2\|T_0(Px) - T_0(Py)\| + \Bigl\| \|x\|T_0(Px) - \|y\|T_0(Px) \Bigr\| + \Bigl \|\|y\|T_0(P_x) - \|y\|T_0(Py) \Bigr\| \\ & \leq 4k\|x - y\| + \Bigl | \|x\| - \|y\| \Bigr | \|T_0(Px)\| + \|y\|\|T_0(Px) - T_0(Py)\| \\ & \leq 4k\|x - y\| + \|x-y\| + 4k\|x-y\| \\ & = (8k + 1)\|x - y\|. \end{split} \end{equation} This is all I can get at the moment. Any help will be appreciated. $\textbf{Edit}:$ There is extra assumption on map $T_0$ which I didnt mention, because I thought its irrelevant in evaluating Lipschitz constant. Namely, we assume $\inf\{\|x - T_0x\|: x \in B\} > 0$ (such maps exist in infinite dimensional spaces).","Let be normed space and let denote the closed unit ball in . Suppose we have map which satisfies Lipschitz condition with some constant . We extend this map to in the following way: where for and for ( is radial projection onto unit ball). Im reading a paper where the author claims this extended map still satisfies Lipschitz condition, this time with constant . I tried to verify this. If , then . Let and . Then Here I used the fact that in every normed space projection is Lipschitz with constant . Now this is where I can't finish the job. Let . I only managed to show that this map is Lipschitz with constant . This is all I can get at the moment. Any help will be appreciated. There is extra assumption on map which I didnt mention, because I thought its irrelevant in evaluating Lipschitz constant. Namely, we assume (such maps exist in infinite dimensional spaces).","X B X T_0: B \rightarrow B k > 1 T_1: 2B \rightarrow B  T_1x = \begin{cases} T_0x, & \text{for } \|x\| \leq 1, \\
(2-\|x\|)T_0(Px), & \text{for } 1 < \|x\| \leq 2,
\end{cases} Px = \frac{x}{\|x\|} \|x \| > 1 Px = x \|x\| \leq 1 P 2k + 1 \textbf{Case 1}: x,y \in B \|T_1x - T_1y\| = \|T_0x - T_0y\| \leq k\|x - y\| \textbf{Case 2}: x \in B y \in 2B \setminus B \begin{equation} 
\begin{split}
\|T_1x - T_1y\| & = \|T_0x - (2-\|y\|)T_0(Py)\| \\ 
&\leq \Bigl\|T_0x - T_0(Py) - T_0(Py) + \|y\|T_0(Py) \Bigr\| \\
&\leq \|T_0(Px) - T_0(Py)\| + (\|y\| - 1)\|T_0(Py)\| \\
&\leq k\|Px - Py\| + \|y\| - \|x\| \\ 
&\leq 2k\|x - y\| + \|x - y\| \\
& = (2k + 1)\|x - y\|.
\end{split}
\end{equation} P 2 \textbf{Case 3}: x,y \in 2B \setminus B 8k + 1 \begin{equation}
\begin{split}
\|T_1x - T_1y\| & = \Bigl\|(2-\|x\|)T_0(Px) - (2-\|y\|)T_0(Py) \Bigr\| \\ 
& = \Bigl\|2T_0(Px) - 2T_0(Py) - \|x\|T_0(Px) + \|y\|T_0(Py) \Bigr\| \\
& \leq 2\|T_0(Px) - T_0(Py)\| + \Bigl\| \|x\|T_0(Px) - \|y\|T_0(Px) \Bigr\| + \Bigl \|\|y\|T_0(P_x) - \|y\|T_0(Py) \Bigr\| \\
& \leq 4k\|x - y\| + \Bigl | \|x\| - \|y\| \Bigr | \|T_0(Px)\| + \|y\|\|T_0(Px) - T_0(Py)\| \\
& \leq 4k\|x - y\| + \|x-y\| + 4k\|x-y\| \\
& = (8k + 1)\|x - y\|.
\end{split}
\end{equation} \textbf{Edit}: T_0 \inf\{\|x - T_0x\|: x \in B\} > 0",['functional-analysis']
55,Div-Curl lemma and precompactness in $H^{-1}$,Div-Curl lemma and precompactness in,H^{-1},I am trying to understand $\operatorname{div-curl}$ lemma. An important requirement to apply $\operatorname{div-curl}$ lemma is the precomapctness of the sequences $\operatorname{div}(A_n)$ and $\operatorname{curl}(B_n)$ in $H^{-1}(\Omega).$ What are the important compactness results which help in checking whether a sequence is precompact in $H^{-1}(\Omega)$ or not? Where can I find the details? Are there any books which illustrate the application of $\operatorname{div-curl}$ through some examples? Any help is appreciated..,I am trying to understand lemma. An important requirement to apply lemma is the precomapctness of the sequences and in What are the important compactness results which help in checking whether a sequence is precompact in or not? Where can I find the details? Are there any books which illustrate the application of through some examples? Any help is appreciated..,\operatorname{div-curl} \operatorname{div-curl} \operatorname{div}(A_n) \operatorname{curl}(B_n) H^{-1}(\Omega). H^{-1}(\Omega) \operatorname{div-curl},"['functional-analysis', 'partial-differential-equations', 'compactness', 'sobolev-spaces', 'compact-operators']"
56,"Is the topological space $ (X^{*},\sigma (X ^ {*}, X)) $ metrizable?",Is the topological space  metrizable?," (X^{*},\sigma (X ^ {*}, X)) ","Definition 1: Let $X$ be a normed space. The operator \begin{align*}     J_{X}\colon X&\to X^{**}\\     x&\mapsto J_{X}(x)(x^{*})=x^{*}(x) \end{align*} is called canonical lace (or canonical immersion ) of $X$ into $X^{**}. $ Lemma 1: Let $X$ be a normed space. Then the canonical immersion $J_{X}$ of $X$ into $X^{∗∗}$ is an isometric isomorphism. Definition 2: Let $ (X, \Vert \cdot \Vert) $ be a normed space. The weak- topology*, denoted by $ \sigma (X ^{*}, X), $ is the smallest topology at $ X ^{*} $ that makes every element of $ \{J_ {X} (x) \mid x \in X \} \subseteq {X ^{**}}$ continuous. Let \begin{equation*}     X=c_{00}=\{x=(x_{n})_{n\in\mathbb{N}}\in\mathbb{K}^{\mathbb{N}}\mid \exists N\in\mathbb{N}, x_{n}=0, \forall n\geq N\}. \end{equation*} Question: Is the topological space $ (X^{*},\sigma (X ^ {*}, X)) $ metrizable? Attempt: We know that $(c_{00},\Vert\cdot\Vert_{\infty})$ is a normed space, which is not Banach. A completion of $(c_{00},\Vert\cdot\Vert_{\infty})$ is \begin{equation*}     c_{0}=\left\{y=(y_{n})_{n\in\mathbb{N}}\in\mathbb{K}^{\mathbb{N}}\ \middle|\ \lim_{n\rightarrow{\infty}}{x_{n}}=0\right\} \end{equation*} that is, there is $J\colon c_{00}\to c_{0}$ linear such that $\Vert J(x)\Vert=\Vert x\Vert,\forall x\in c_{00}$ and $\overline{J(c_{00})}=c_{0}.$ Furthermore, we know that \begin{equation*}     X^{*}=c_{0}^{*}=l_{1}=\left\{z=(z_{n})_{n\in\mathbb{N}}\in\mathbb{K}^{\mathbb{N}}\ \middle|\ \sum_{n\in\mathbb{N}}{z_{n}}<\infty\right\}. \end{equation*} and \begin{equation*}     l_{1}^{*}=l^{\infty}=\left\{w=(w_{n})\in\mathbb{K}^{\mathbb{N}} \ \middle|\ \sup_{n\in\mathbb{N}}\{|w_{n}|\}<\infty \right\}. \end{equation*} I don't know if I can conclude what I want with the above.","Definition 1: Let be a normed space. The operator is called canonical lace (or canonical immersion ) of into Lemma 1: Let be a normed space. Then the canonical immersion of into is an isometric isomorphism. Definition 2: Let be a normed space. The weak- topology*, denoted by is the smallest topology at that makes every element of continuous. Let Question: Is the topological space metrizable? Attempt: We know that is a normed space, which is not Banach. A completion of is that is, there is linear such that and Furthermore, we know that and I don't know if I can conclude what I want with the above.","X \begin{align*}
    J_{X}\colon X&\to X^{**}\\
    x&\mapsto J_{X}(x)(x^{*})=x^{*}(x)
\end{align*} X X^{**}.  X J_{X} X X^{∗∗}  (X, \Vert \cdot \Vert)   \sigma (X ^{*}, X),   X ^{*}   \{J_ {X} (x) \mid x \in X \} \subseteq {X ^{**}} \begin{equation*}
    X=c_{00}=\{x=(x_{n})_{n\in\mathbb{N}}\in\mathbb{K}^{\mathbb{N}}\mid \exists N\in\mathbb{N}, x_{n}=0, \forall n\geq N\}.
\end{equation*}  (X^{*},\sigma (X ^ {*}, X))  (c_{00},\Vert\cdot\Vert_{\infty}) (c_{00},\Vert\cdot\Vert_{\infty}) \begin{equation*}
    c_{0}=\left\{y=(y_{n})_{n\in\mathbb{N}}\in\mathbb{K}^{\mathbb{N}}\ \middle|\ \lim_{n\rightarrow{\infty}}{x_{n}}=0\right\}
\end{equation*} J\colon c_{00}\to c_{0} \Vert J(x)\Vert=\Vert x\Vert,\forall x\in c_{00} \overline{J(c_{00})}=c_{0}. \begin{equation*}
    X^{*}=c_{0}^{*}=l_{1}=\left\{z=(z_{n})_{n\in\mathbb{N}}\in\mathbb{K}^{\mathbb{N}}\ \middle|\ \sum_{n\in\mathbb{N}}{z_{n}}<\infty\right\}.
\end{equation*} \begin{equation*}
    l_{1}^{*}=l^{\infty}=\left\{w=(w_{n})\in\mathbb{K}^{\mathbb{N}} \ \middle|\ \sup_{n\in\mathbb{N}}\{|w_{n}|\}<\infty \right\}.
\end{equation*}","['functional-analysis', 'banach-spaces', 'isometry', 'dual-spaces']"
57,Convolution of Distribution with smooth function,Convolution of Distribution with smooth function,,"I meet an exercise in my homework: Let $f\in \mathcal{D}'(\mathbb{R}^n)$ and $g\in \mathcal{D}(\mathbb{R}^n)$ , show that the biliner map $(f,g)\mapsto f*g $ is continuous in $f$ and $g$ , respectively. I have proved that for any fixed $f\in \mathcal{D}'(\mathbb{R}^n)$ , if $g_{i}\in \mathcal{D}(\mathbb{R}^n)$ such that $g_{i}\to 0$ in $\mathcal{D}(\mathbb{R}^n)$ , then $f*g_{i}\to 0$ in $\mathcal{E}(\mathbb{R}^n)$ , but I don't know how to prove that for any fixd $g\in \mathcal{D}(\mathbb{R}^n)$ and $f_{i}\to 0$ in $\mathcal{D'}(\mathbb{R}^n)$ then $f_{i}*g\to 0$ in $\mathcal{E}(\mathbb{R}^n)$ . Is this conclusion correct? Here is my approach: To prove $f_{i}*g\to 0$ in $\mathcal{E}(\mathbb{R}^n)$ , we need to show that, for any compact set $K\subset \mathbb{R}^n$ , and any multi-index $\alpha$ , $$\sup_{x\in K}|\partial^{\alpha}(f_{i}\ast g)|=\sup_{x\in K}|f_{i}\ast \partial^{\alpha}g| =\sup_{x\in K}|\langle f_{i}(y),\partial_{x}^{\alpha}g(x-y)\rangle|\to 0$$ holds for fixed $g\in \mathcal{D}(\mathbb{R}^n)$ as $i\to +\infty$ . But we only have $f_{i}\to 0$ in $\mathcal{D}'(\mathbb{R}^n)$ , it seems we can only show that, for any fixed $x\in K$ , $$ |\langle f_{i}(y),\partial_{x}^{\alpha}g(x-y)\rangle|\to 0$$ for $i\to +\infty$ . Can someone help me with the question above? Thank you very much!","I meet an exercise in my homework: Let and , show that the biliner map is continuous in and , respectively. I have proved that for any fixed , if such that in , then in , but I don't know how to prove that for any fixd and in then in . Is this conclusion correct? Here is my approach: To prove in , we need to show that, for any compact set , and any multi-index , holds for fixed as . But we only have in , it seems we can only show that, for any fixed , for . Can someone help me with the question above? Thank you very much!","f\in \mathcal{D}'(\mathbb{R}^n) g\in \mathcal{D}(\mathbb{R}^n) (f,g)\mapsto f*g  f g f\in \mathcal{D}'(\mathbb{R}^n) g_{i}\in \mathcal{D}(\mathbb{R}^n) g_{i}\to 0 \mathcal{D}(\mathbb{R}^n) f*g_{i}\to 0 \mathcal{E}(\mathbb{R}^n) g\in \mathcal{D}(\mathbb{R}^n) f_{i}\to 0 \mathcal{D'}(\mathbb{R}^n) f_{i}*g\to 0 \mathcal{E}(\mathbb{R}^n) f_{i}*g\to 0 \mathcal{E}(\mathbb{R}^n) K\subset \mathbb{R}^n \alpha \sup_{x\in K}|\partial^{\alpha}(f_{i}\ast g)|=\sup_{x\in K}|f_{i}\ast \partial^{\alpha}g|
=\sup_{x\in K}|\langle f_{i}(y),\partial_{x}^{\alpha}g(x-y)\rangle|\to 0 g\in \mathcal{D}(\mathbb{R}^n) i\to +\infty f_{i}\to 0 \mathcal{D}'(\mathbb{R}^n) x\in K  |\langle f_{i}(y),\partial_{x}^{\alpha}g(x-y)\rangle|\to 0 i\to +\infty","['functional-analysis', 'partial-differential-equations', 'distribution-theory', 'convolution']"
58,On the proof of Heisenberg's uncertainty principle,On the proof of Heisenberg's uncertainty principle,,"Theorem 1 For $f \in L^2(\mathbb R)$ and $a,b \in \mathbb R$ $$ \frac{1}{2} \| f \|_2^2 \le \left( \int_{\mathbb R} (x - a)^2 | f(x) |^2 d x\right)^\frac{1}{2} \left( \int_{\mathbb R} (\xi - b)^2 | \hat{f}(\xi) |^2 d\xi \right)^\frac{1}{2}. $$ holds. In our lecture we proved theorem 1 using the following theorem Theorem 2 For self-adjoint (possibly unbounded) operators $S,T$ on a Hilbert space $H$ and $a,b \in \mathbb R$ $$ \| (S - a I) f \| \| (T - b I) \| \ge \frac{1}{2} | \langle [S,T] f, f \rangle | $$ holds for all $f \in \text{dom}(ST) \cap \text{dom}(TS)$ , where $[S,T] := S T - T S$ is the commutator of $S$ and $T$ . Proof. Define $(S f)(x) := x f(x)$ for $f \in L^2(\mathbb R^n)$ and $(T f)(x) := i f'(x)$ for differentiable $f \in L^2(\mathbb R^n)$ . For $f \in \text{dom}(ST) \cap \text{dom}(TS)$ we have \begin{align*} \tag{1} ([S,T] f)(x) & = i x f'(x) = i \frac{d}{dx} ( x \cdot f(x)) \\ & = i x f'(x) - i f(x) - i x f'(x) = - i f(x) \end{align*} and by theorem 2 $$ \tag{2} \frac{1}{2} \| f \|_2^2 = \frac{1}{2} | \langle - i f(x), f(x) \rangle | \le \| (S - a I) f \|_2 \| (T - b I) f \|_2. $$ By the Plancherel theorem we have $$ \tag{3} \| (T - b I) f \|_2  = \| \mathcal{F}((T - b I) f) \|_2 = \| (\xi - b) \hat{f} \|_2, $$ which yields the statement. $\square$ My Questions Into what spaces do $S$ and $T$ map? Is it $L^2(\mathbb{R}^n)$ ? Why can we use the Plancherel theorem?  I have tried to calculate \begin{align*}     \| (T - b I) f \|_2^2     & = \int_{\mathbb{R}^n} | i f'(x) - b f(x) |^2 dx \\     & = \int_{\mathbb{R}^n} | f'(x) |^2 - i b \overline{f(x)} f'(x) + i b f(x) \overline{f'(x)} + | b f(x) |^2 dx \\     & = \int_{\mathbb{R}^n} | f'(x) |^2 dx     - i b \int_{\mathbb{R}^n} \frac{d}{dx} | f(x) |^2 dx     + | b |^2 \| f \|_2^2 \\     & = \int_{\mathbb{R}^n} | f'(x) |^2 dx     + | b |^2 \| f \|_2^2     - i b \bigg[| f(x) |^2\bigg]_{x = - \infty}^{\infty} \end{align*} Is this correct? Can we conclude this is finite? How do we deal with the $\int_{\mathbb{R}} | f'(x) |^2 dx$ term? Under the suitable assumptions (in another proof of theorem 1, where $f \in  L^2(\mathbb{R})$ we used this) we can say \begin{equation*} \int_{\mathbb R^n}  | f'(x) |^2 dx = \int_{\mathbb R^n}  | \mathcal{F}(f')(x) |^2 dx = \int_{\mathbb R^n} x^2 | \hat{f}(x) |^2 dx, \end{equation*} but for this we would need that $f' \in L^2$ . Our lecture assistant conjectured that we need to require $f' \in L^2$ . How can we show this is necessary? I know that $\big[| f(x) |^2\big]_{x = - \infty}^{\infty}$ only makes sense for $n = 1$ . How can we generalise it? Can we conclude that it vanishes as $|f(x)| \xrightarrow{x \to \pm \infty} 0$ because $f \in L^2(\mathbb{R}^n)$ ?","Theorem 1 For and holds. In our lecture we proved theorem 1 using the following theorem Theorem 2 For self-adjoint (possibly unbounded) operators on a Hilbert space and holds for all , where is the commutator of and . Proof. Define for and for differentiable . For we have and by theorem 2 By the Plancherel theorem we have which yields the statement. My Questions Into what spaces do and map? Is it ? Why can we use the Plancherel theorem?  I have tried to calculate Is this correct? Can we conclude this is finite? How do we deal with the term? Under the suitable assumptions (in another proof of theorem 1, where we used this) we can say but for this we would need that . Our lecture assistant conjectured that we need to require . How can we show this is necessary? I know that only makes sense for . How can we generalise it? Can we conclude that it vanishes as because ?","f \in L^2(\mathbb R) a,b \in \mathbb R 
\frac{1}{2} \| f \|_2^2
\le \left( \int_{\mathbb R} (x - a)^2 | f(x) |^2 d x\right)^\frac{1}{2} \left( \int_{\mathbb R} (\xi - b)^2 | \hat{f}(\xi) |^2 d\xi \right)^\frac{1}{2}.
 S,T H a,b \in \mathbb R 
\| (S - a I) f \| \| (T - b I) \|
\ge \frac{1}{2} | \langle [S,T] f, f \rangle |
 f \in \text{dom}(ST) \cap \text{dom}(TS) [S,T] := S T - T S S T (S f)(x) := x f(x) f \in L^2(\mathbb R^n) (T f)(x) := i f'(x) f \in L^2(\mathbb R^n) f \in \text{dom}(ST) \cap \text{dom}(TS) \begin{align*} \tag{1}
([S,T] f)(x)
& = i x f'(x)
= i \frac{d}{dx} ( x \cdot f(x)) \\
& = i x f'(x) - i f(x) - i x f'(x)
= - i f(x)
\end{align*}  \tag{2}
\frac{1}{2} \| f \|_2^2
= \frac{1}{2} | \langle - i f(x), f(x) \rangle |
\le \| (S - a I) f \|_2 \| (T - b I) f \|_2.
  \tag{3}
\| (T - b I) f \|_2 
= \| \mathcal{F}((T - b I) f) \|_2
= \| (\xi - b) \hat{f} \|_2,
 \square S T L^2(\mathbb{R}^n) \begin{align*}
    \| (T - b I) f \|_2^2
    & = \int_{\mathbb{R}^n} | i f'(x) - b f(x) |^2 dx \\
    & = \int_{\mathbb{R}^n} | f'(x) |^2 - i b \overline{f(x)} f'(x) + i b f(x) \overline{f'(x)} + | b f(x) |^2 dx \\
    & = \int_{\mathbb{R}^n} | f'(x) |^2 dx
    - i b \int_{\mathbb{R}^n} \frac{d}{dx} | f(x) |^2 dx
    + | b |^2 \| f \|_2^2 \\
    & = \int_{\mathbb{R}^n} | f'(x) |^2 dx
    + | b |^2 \| f \|_2^2
    - i b \bigg[| f(x) |^2\bigg]_{x = - \infty}^{\infty}
\end{align*} \int_{\mathbb{R}} | f'(x) |^2 dx f \in  L^2(\mathbb{R}) \begin{equation*}
\int_{\mathbb R^n}  | f'(x) |^2 dx
= \int_{\mathbb R^n}  | \mathcal{F}(f')(x) |^2 dx
= \int_{\mathbb R^n} x^2 | \hat{f}(x) |^2 dx,
\end{equation*} f' \in L^2 f' \in L^2 \big[| f(x) |^2\big]_{x = - \infty}^{\infty} n = 1 |f(x)| \xrightarrow{x \to \pm \infty} 0 f \in L^2(\mathbb{R}^n)","['functional-analysis', 'proof-explanation', 'fourier-transform']"
59,Different definitions of a relatively compact operator,Different definitions of a relatively compact operator,,"Let $T,K$ be unbounded operators on a Hilbert space $H$ . I've seen the following definition of a relatively compact operator: (i) The operator $K$ is called relatively compact with respect to $T$ , if for some $z$ in the resolvent set of $T$ , $KR_T(z)$ is compact, where $R_T(z):=(T-z)^{-1}.$ I've also seen: (ii) The operator $K$ is called relatively compact with respect to $T$ , if for every sequence $(x_n)_{n \in \mathbb{N}}\subseteq H$ such that $(Tx_n)_{n \in \mathbb{N}}$ is bounded, $(Kx_n)_{n \in \mathbb{N}}$ contains a convergent subsequence. Do definitions (i) and (ii) have something to do with each other, or are they distinct? What is the intuition behind these definitions? Definition (ii) looks like a generalisation of a compact operator, but definition (i) is just weird. This question has also been posted on Math Overflow .","Let be unbounded operators on a Hilbert space . I've seen the following definition of a relatively compact operator: (i) The operator is called relatively compact with respect to , if for some in the resolvent set of , is compact, where I've also seen: (ii) The operator is called relatively compact with respect to , if for every sequence such that is bounded, contains a convergent subsequence. Do definitions (i) and (ii) have something to do with each other, or are they distinct? What is the intuition behind these definitions? Definition (ii) looks like a generalisation of a compact operator, but definition (i) is just weird. This question has also been posted on Math Overflow .","T,K H K T z T KR_T(z) R_T(z):=(T-z)^{-1}. K T (x_n)_{n \in \mathbb{N}}\subseteq H (Tx_n)_{n \in \mathbb{N}} (Kx_n)_{n \in \mathbb{N}}","['functional-analysis', 'operator-theory', 'definition', 'spectral-theory', 'compact-operators']"
60,$c_{00}$ is not complete,is not complete,c_{00},"I try to show that the space $c_{00}=\{(x_n):x_n=0 \text{ all but finitely many }n\}$ is not complete with respect to the norm $\|x\|_\infty=\max |x_n|$ . My attempt: Let $(z_n)=\left(1,\frac{1}{2},\dots,\frac{1}{n},0,0,\dots\right)$ be a sequence. Clearly $(z_n)\in c_{00}$ . We have convergence $(z_n)\to (z_\infty)$ with $(z_\infty)\in c_{0}$ but $(z_\infty) \not\in c_{00}$ . So we have only to show that $(z_n)$ is a Cauchy sequence with respect to $\|\|_\infty$ . Now I am confused because of the $\|\cdot\|_\infty$ norm. Do I just subtract the entries of the sequences and then take the maximum entry? Meaning for $m>n$ I have $$z_n-z_m=\left(0, \dots,0,\frac{1}{n+1},\dots,\frac{1}{m},0,0,\dots\right)$$ $$\|z_n-z_m\|_\infty=\frac{1}{n+1}\to 0 \text{  as } n\to \infty $$ Thank you for help",I try to show that the space is not complete with respect to the norm . My attempt: Let be a sequence. Clearly . We have convergence with but . So we have only to show that is a Cauchy sequence with respect to . Now I am confused because of the norm. Do I just subtract the entries of the sequences and then take the maximum entry? Meaning for I have Thank you for help,"c_{00}=\{(x_n):x_n=0 \text{ all but finitely many }n\} \|x\|_\infty=\max |x_n| (z_n)=\left(1,\frac{1}{2},\dots,\frac{1}{n},0,0,\dots\right) (z_n)\in c_{00} (z_n)\to (z_\infty) (z_\infty)\in c_{0} (z_\infty) \not\in c_{00} (z_n) \|\|_\infty \|\cdot\|_\infty m>n z_n-z_m=\left(0, \dots,0,\frac{1}{n+1},\dots,\frac{1}{m},0,0,\dots\right) \|z_n-z_m\|_\infty=\frac{1}{n+1}\to 0 \text{  as } n\to \infty ","['functional-analysis', 'convergence-divergence']"
61,Representing a Banach space as a function space,Representing a Banach space as a function space,,"When I think about a vector space, I like to see them as spaces of parameters. Although finite dimensional spaces have been generalized to Banach spaces, I see function spaces as the best generalization, at least heuristically, for a space of parameters. Given the function space $L^p(X,\Omega,\mu)$ , for simplicity $L^p(X)$ , the set of Dirac's deltas at each point works as a kind of canonical base, so when we choose a function, we are specifying somehow $|X|$ number of parameters; $|X|$ is the cardinality of $X$ . My first question is: what are sufficient and necessaries conditions for a Banach space to be a function space? To be more precise: When is a Banach space $B$ isometric to some space $L^p(X)$ ? (isometric representation) When is there a linear isomorphism between $B$ and some $L^p(X)$ , that is, a linear operator $A:B\to L^p(X)$ such that $c\lVert x\rVert_B\le \lVert Ax\rVert_{L^p}\le C\lVert x\rVert_B$ ? (continuous representation) When is there a space $X$ and a continuous operator $A:C_c(X)\to B$ , such that $A$ is 1-1 and the image of $A$ is dense? $C_c(X)$ is the space of compactly supported functions on $X$ ; we can endow it with the usual topology. (weak representation) The definition I like the most is the third, I think it's the weaker. The first two definitions involve $L^p$ spaces, and they are not the sole function spaces out there, so these definitions are incomplete. Can you imagine a weaker way of representing a Banach space? If $B$ is a Hilbert space, take an orthonormal expansion, so every vector is $f=\sum_{i\in I} f(i)e_i$ and we have an isometric representation with $L^2(I)$ , but this is not the most appealing representation, and it leaves more questions. What can we say about $X$ ? If you take $L^2(\mathbb{R}^n)$ , you can use a partition of unity and Fourier series to write every function $f$ as a sum $\sum_{i\in I} f(i)e_i$ , where $I$ is countable and the $e_i$ form an orthonormal base. But, what is then the difference between $L^2(\mathbb{R}^2)$ and $L^2(\mathbb{R})$ ? Doesn't the structure of the space $X$ matter? I feel uneasy. Many times in my work I have encountered operators like $A:L^p(X)\to L^q(Y)$ , where $X$ and $Y$ are topological spaces of dimensions $N$ and $M$ , respectively; for simplicity, suppose that $A$ is injective. Heuristically I apply the finite dimensional logic, for example take $N=M$ , and I think: the cokernel of $A$ is like a ""tiny"" function space, compared with $X$ or $Y$ , over a topological space $Z$ of dimension $<M$ . And I'm ""always"" right, I can even guess properties of the function space $Z$ , like $\text{dim}(Z)$ , from topological properties of $X$ and $Y$ . Then, what does it happen with $L^2(\mathbb{R}^n)$ ? Can we, under additional assumption, prove a kind of invariance of domain theorem for function spaces? It may help the reader to think about the operator $\iota:C_c(\mathbb{R})\to C_c({\mathbb{R}^2})$ , given by $\iota f(x,y)\mapsto f(x)\varphi(y)$ , where $\varphi$ is supported near to zero, and the quotient $C_c({\mathbb{R}^2})/\iota C_c(\mathbb{R})$ ; we can think that the dimension of the cokernel is "" $\mathbb{R}^2-\mathbb{R}\approx \mathbb{R}^2$ "".","When I think about a vector space, I like to see them as spaces of parameters. Although finite dimensional spaces have been generalized to Banach spaces, I see function spaces as the best generalization, at least heuristically, for a space of parameters. Given the function space , for simplicity , the set of Dirac's deltas at each point works as a kind of canonical base, so when we choose a function, we are specifying somehow number of parameters; is the cardinality of . My first question is: what are sufficient and necessaries conditions for a Banach space to be a function space? To be more precise: When is a Banach space isometric to some space ? (isometric representation) When is there a linear isomorphism between and some , that is, a linear operator such that ? (continuous representation) When is there a space and a continuous operator , such that is 1-1 and the image of is dense? is the space of compactly supported functions on ; we can endow it with the usual topology. (weak representation) The definition I like the most is the third, I think it's the weaker. The first two definitions involve spaces, and they are not the sole function spaces out there, so these definitions are incomplete. Can you imagine a weaker way of representing a Banach space? If is a Hilbert space, take an orthonormal expansion, so every vector is and we have an isometric representation with , but this is not the most appealing representation, and it leaves more questions. What can we say about ? If you take , you can use a partition of unity and Fourier series to write every function as a sum , where is countable and the form an orthonormal base. But, what is then the difference between and ? Doesn't the structure of the space matter? I feel uneasy. Many times in my work I have encountered operators like , where and are topological spaces of dimensions and , respectively; for simplicity, suppose that is injective. Heuristically I apply the finite dimensional logic, for example take , and I think: the cokernel of is like a ""tiny"" function space, compared with or , over a topological space of dimension . And I'm ""always"" right, I can even guess properties of the function space , like , from topological properties of and . Then, what does it happen with ? Can we, under additional assumption, prove a kind of invariance of domain theorem for function spaces? It may help the reader to think about the operator , given by , where is supported near to zero, and the quotient ; we can think that the dimension of the cokernel is "" "".","L^p(X,\Omega,\mu) L^p(X) |X| |X| X B L^p(X) B L^p(X) A:B\to L^p(X) c\lVert x\rVert_B\le \lVert Ax\rVert_{L^p}\le C\lVert x\rVert_B X A:C_c(X)\to B A A C_c(X) X L^p B f=\sum_{i\in I} f(i)e_i L^2(I) X L^2(\mathbb{R}^n) f \sum_{i\in I} f(i)e_i I e_i L^2(\mathbb{R}^2) L^2(\mathbb{R}) X A:L^p(X)\to L^q(Y) X Y N M A N=M A X Y Z <M Z \text{dim}(Z) X Y L^2(\mathbb{R}^n) \iota:C_c(\mathbb{R})\to C_c({\mathbb{R}^2}) \iota f(x,y)\mapsto f(x)\varphi(y) \varphi C_c({\mathbb{R}^2})/\iota C_c(\mathbb{R}) \mathbb{R}^2-\mathbb{R}\approx \mathbb{R}^2","['functional-analysis', 'banach-spaces']"
62,Proof explanation related to the operator matrices,Proof explanation related to the operator matrices,,"Let $F$ be a complex Hilbert space. Let $A,B,C,D\in \mathcal{B}(F)$. Consider the operator matrix $T$ such that \begin{equation*} T=\begin{pmatrix}A & B \\ C & D \end{pmatrix}\in \mathcal{B}(F\oplus F). \end{equation*} Consider \begin{equation*} \widetilde{T}=\begin{pmatrix}\|A\| & \|B\| \\ \|C\| & \|D\| \end{pmatrix}. \end{equation*} I want to understand why   $$r(T)\leq r (\widetilde{T})?$$ Note that the inner product on $F\oplus F$ is defined as follows: If $x=\begin{pmatrix} x_1\\ x_2\end{pmatrix}\in F\oplus F$ with $x_1,x_2\in F$, and $x'=\begin{pmatrix}x_1'\\ x_2'\end{pmatrix}$ similarly, then $$\langle x,x'\rangle_{F\oplus F}:= \langle x_1,x_1'\rangle_F +\langle x_2,x_2'\rangle.$$ I see the following theorem in a more general context but I'm facing difficulties to understand the proof Theorem 1.1 Let $A = (A_{ij})_{n \times n}$ be an operator matrix and $\tilde{A} = (\| A_{ij} \|)_{n \times n}$ its block-norm matrix.   Then $\omega(A) \leq \omega(\tilde{A})$, $\| A \| \leq \| \tilde{A} \|$, $r_\sigma(A) \leq r_\sigma(\tilde{A})$. (Original image here .) Proof: 3. Notice, in general, that for operators $A = (A_{ij})_{n \times n}$ and $B = (B_{ij})_{n \times n}$ we have   $$        \| \widetilde{AB} \|   \leq \| \tilde{A} \tilde{B} \| . $$   In fact, $\tilde{A} \tilde{B} - \widetilde{AB}$ is a nonnegative matrix since $\widetilde{AB} = (\| \sum_{k=1}^n A_{ik} B_{kj} \|)_{n \times n}$ and $\tilde{A} \tilde{B} = (\sum_{k=1}^n \| A_{ik} \| \| B_{kj} \|)_{n \times n}$.   Therefore, by the norm monotonicity of nonnegative matrices, we get $\| \widetilde{AB} \| \leq \| \tilde{A} \tilde{B} \|$.   Using induction, we have   $$        \| A^m \|   \leq \| \widetilde{A^m} \|   \leq \| \smash{\tilde{A}}\vphantom{A}^m \| , $$   for every positive integer $m$.   This leads to   $$        r_\sigma(A)   \leq r_\sigma(\tilde{A}). $$ (Original images here and here .)","Let $F$ be a complex Hilbert space. Let $A,B,C,D\in \mathcal{B}(F)$. Consider the operator matrix $T$ such that \begin{equation*} T=\begin{pmatrix}A & B \\ C & D \end{pmatrix}\in \mathcal{B}(F\oplus F). \end{equation*} Consider \begin{equation*} \widetilde{T}=\begin{pmatrix}\|A\| & \|B\| \\ \|C\| & \|D\| \end{pmatrix}. \end{equation*} I want to understand why   $$r(T)\leq r (\widetilde{T})?$$ Note that the inner product on $F\oplus F$ is defined as follows: If $x=\begin{pmatrix} x_1\\ x_2\end{pmatrix}\in F\oplus F$ with $x_1,x_2\in F$, and $x'=\begin{pmatrix}x_1'\\ x_2'\end{pmatrix}$ similarly, then $$\langle x,x'\rangle_{F\oplus F}:= \langle x_1,x_1'\rangle_F +\langle x_2,x_2'\rangle.$$ I see the following theorem in a more general context but I'm facing difficulties to understand the proof Theorem 1.1 Let $A = (A_{ij})_{n \times n}$ be an operator matrix and $\tilde{A} = (\| A_{ij} \|)_{n \times n}$ its block-norm matrix.   Then $\omega(A) \leq \omega(\tilde{A})$, $\| A \| \leq \| \tilde{A} \|$, $r_\sigma(A) \leq r_\sigma(\tilde{A})$. (Original image here .) Proof: 3. Notice, in general, that for operators $A = (A_{ij})_{n \times n}$ and $B = (B_{ij})_{n \times n}$ we have   $$        \| \widetilde{AB} \|   \leq \| \tilde{A} \tilde{B} \| . $$   In fact, $\tilde{A} \tilde{B} - \widetilde{AB}$ is a nonnegative matrix since $\widetilde{AB} = (\| \sum_{k=1}^n A_{ik} B_{kj} \|)_{n \times n}$ and $\tilde{A} \tilde{B} = (\sum_{k=1}^n \| A_{ik} \| \| B_{kj} \|)_{n \times n}$.   Therefore, by the norm monotonicity of nonnegative matrices, we get $\| \widetilde{AB} \| \leq \| \tilde{A} \tilde{B} \|$.   Using induction, we have   $$        \| A^m \|   \leq \| \widetilde{A^m} \|   \leq \| \smash{\tilde{A}}\vphantom{A}^m \| , $$   for every positive integer $m$.   This leads to   $$        r_\sigma(A)   \leq r_\sigma(\tilde{A}). $$ (Original images here and here .)",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
63,"Bloch's theorem in one dimension, confusion about proof","Bloch's theorem in one dimension, confusion about proof",,"I was looking at the derivation of Bloch's theorem in Griffith's QM: If $V(x+a)=V(x)$ for any $x$ and some $a$, and $\psi$ solves $$ H\psi =\lambda \psi $$ for $H=-\frac{\hbar^2}{2\pi}\frac{d^2}{dx^2}+V(x)$ then $$ \psi(x+a)=e^{ika}\psi(x) $$ The proof relies on the fact that since the shift operator $D\psi(x)=\psi(x+a)$ commutes with the Hamiltonian, we may take an eigenfunction of $H$ to serve as an eigenfunction for $D$. The wikipedia article on the theorem (on a 3 dimensional crystal lattice but the principle is the same) once again asserts the existence of a simultaneous eigenbasis for the two operators. What am I missing here? I know that this is a fact for matrices, but we are working over $L^2(\mathbb{R})$ here, where I don't believe this is true for unbounded self adjoint operators, and it is not clear to me why it should be true with this particular operator (indeed I think it is not). Thanks for any help and I apologize if I am being naive.","I was looking at the derivation of Bloch's theorem in Griffith's QM: If $V(x+a)=V(x)$ for any $x$ and some $a$, and $\psi$ solves $$ H\psi =\lambda \psi $$ for $H=-\frac{\hbar^2}{2\pi}\frac{d^2}{dx^2}+V(x)$ then $$ \psi(x+a)=e^{ika}\psi(x) $$ The proof relies on the fact that since the shift operator $D\psi(x)=\psi(x+a)$ commutes with the Hamiltonian, we may take an eigenfunction of $H$ to serve as an eigenfunction for $D$. The wikipedia article on the theorem (on a 3 dimensional crystal lattice but the principle is the same) once again asserts the existence of a simultaneous eigenbasis for the two operators. What am I missing here? I know that this is a fact for matrices, but we are working over $L^2(\mathbb{R})$ here, where I don't believe this is true for unbounded self adjoint operators, and it is not clear to me why it should be true with this particular operator (indeed I think it is not). Thanks for any help and I apologize if I am being naive.",,"['functional-analysis', 'mathematical-physics', 'quantum-mechanics', 'unbounded-operators']"
64,Two equivalent norms on $\mathcal{L}(E)^n$,Two equivalent norms on,\mathcal{L}(E)^n,"Let $E$ be a complex Hilbert space, and $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For ${\bf A}:=(A_1,...,A_n) \in \mathcal{L}(E)^n$ we recall the definitions of the following two norms on $\mathcal{L}(E)^n$: $$\|{\bf A}\|=\displaystyle\sup_{\|x\|=1}\bigg(\displaystyle\sum_{k=1}^n\|A_kx\|^2\bigg)^{\frac{1}{2}},$$ and  $$\omega_e({\bf A})=\displaystyle\sup_{\|x\|=1}\bigg(\displaystyle\sum_{k=1}^n|\langle A_kx\;|\;x\rangle|^2\bigg)^{1/2}.$$ It's not difficult to prove that $\omega_e({\bf A}) \leq \|{\bf A}\|$. Are $\|\cdot\|$ and $\omega_e(\cdot)$ two equivalent norms on $\mathcal{L}(E)^n\,?$ If the answer is true, I hope to find $\alpha$ such that   $$\alpha \|{\bf A}\|\leq \omega_e({\bf A}) \leq \|{\bf A}\|.$$   Note that if $n=1$, it is well known that   $$\displaystyle\frac{1}{2}\|A\|\leq \omega(A)\leq\|A\|.$$ And you for you help.","Let $E$ be a complex Hilbert space, and $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For ${\bf A}:=(A_1,...,A_n) \in \mathcal{L}(E)^n$ we recall the definitions of the following two norms on $\mathcal{L}(E)^n$: $$\|{\bf A}\|=\displaystyle\sup_{\|x\|=1}\bigg(\displaystyle\sum_{k=1}^n\|A_kx\|^2\bigg)^{\frac{1}{2}},$$ and  $$\omega_e({\bf A})=\displaystyle\sup_{\|x\|=1}\bigg(\displaystyle\sum_{k=1}^n|\langle A_kx\;|\;x\rangle|^2\bigg)^{1/2}.$$ It's not difficult to prove that $\omega_e({\bf A}) \leq \|{\bf A}\|$. Are $\|\cdot\|$ and $\omega_e(\cdot)$ two equivalent norms on $\mathcal{L}(E)^n\,?$ If the answer is true, I hope to find $\alpha$ such that   $$\alpha \|{\bf A}\|\leq \omega_e({\bf A}) \leq \|{\bf A}\|.$$   Note that if $n=1$, it is well known that   $$\displaystyle\frac{1}{2}\|A\|\leq \omega(A)\leq\|A\|.$$ And you for you help.",,"['functional-analysis', 'hilbert-spaces']"
65,Multiplying a Curve by a Plane,Multiplying a Curve by a Plane,,"Is there a field of mathematics that considers multiplying functions in a manner analogous to matrix multiplication? For instance, Let $\mathbf{x}$ is an $n$-dimensional vector such that $x_i=\sin(2\pi \frac {i} {n}))$, for $i=1,\ldots, n$. Let $\mathbf{A}$ be an $n\times n$ matrix where $A_{i,j}=\cos(4\pi \frac {i} {n} - 2\pi \frac {j} {n})$ for each $i,j$. Let $\mathbf{y}=\mathbf{A}\mathbf{x}$ so that $y_i=\frac 1 2 \sin(4\pi \frac {i} {n})$. If we imagine $n$ going to infinity, then we can define an operator, $\text{prod_fcn}$, such that $\text{prod_fcn}(x(t), A(t, s)) = y(t)$. This example is illustrated graphically below: Obviously this is just one simple example of a curve being ""warped"" by a plane. I'm curious to know if this sort of functional operation is a commonly studied thing. If so, any information about this would be appreciated.","Is there a field of mathematics that considers multiplying functions in a manner analogous to matrix multiplication? For instance, Let $\mathbf{x}$ is an $n$-dimensional vector such that $x_i=\sin(2\pi \frac {i} {n}))$, for $i=1,\ldots, n$. Let $\mathbf{A}$ be an $n\times n$ matrix where $A_{i,j}=\cos(4\pi \frac {i} {n} - 2\pi \frac {j} {n})$ for each $i,j$. Let $\mathbf{y}=\mathbf{A}\mathbf{x}$ so that $y_i=\frac 1 2 \sin(4\pi \frac {i} {n})$. If we imagine $n$ going to infinity, then we can define an operator, $\text{prod_fcn}$, such that $\text{prod_fcn}(x(t), A(t, s)) = y(t)$. This example is illustrated graphically below: Obviously this is just one simple example of a curve being ""warped"" by a plane. I'm curious to know if this sort of functional operation is a commonly studied thing. If so, any information about this would be appreciated.",,"['functional-analysis', 'matrix-calculus']"
66,Open set in such a set w.r.t weak topology,Open set in such a set w.r.t weak topology,,"Let $(\mathcal X,\mathcal F)$ be a measurable space and $\mathcal P$ the set of probability measures  on this space. With weak convergence (i.e., $P_n\to P$ weakly if and only if $\mathbb E_{P_n}(f)\to\mathbb E_P(f)$ for every bounded continuous functions $f:\mathcal X\to\mathbb R$), a basis of the corresponding weak topology induced by this weak convergence is a collection of the following sets: $$U(P,\alpha)=\left\{Q:\left|\int f_idP-\int f_idQ\right|<\alpha,i=1,\ldots,n\right\}$$ for some $P\in\mathcal P$, $f_i$ bounded continuous functions, $n$ is finite, and $\alpha>0$. Then there exists a finite measurable partition $A_1,\ldots,A_m$ of $\mathcal X$ and $\beta>0$ such that \begin{align} \tilde{U}(P,\beta)=\{Q:|P(A_j)-Q(A_j)|<\beta\}\subset{U}(P,\alpha). \end{align} My questions are: is $\tilde{U}(P,\epsilon)$ also open? if not, is there an open set, say $U'(P, \alpha')$, that is contained in $\tilde{U}(P,\beta)$? or under what conditions there exists such a $U'(P, \alpha')$? (might have different bounded continuous functions) Can you help me with these question? or please see my thinking below. Let me first prove the existence of such $\tilde{U}(P,\beta)\subset U(P,\alpha).$ Since $f_i$ is bounded continuous, then there exists a simple function $\tilde f_i$ such that $\|f_i-\tilde f_i\|_{\infty} <\frac{\alpha}{3}$. Then if $|\int \tilde{f}_idP - \int \tilde{f}_idQ|<\frac{\alpha}{3}$, we have  \begin{align} &\left|\int f_idP-\int f_idQ\right|\\ =~&\left|\int f_idP-\int \tilde f_idP+\int \tilde f_idP-\int \tilde f_idQ+\int \tilde f_idQ-\int f_idQ\right|\\ <~&\alpha. \end{align} We can then define a finite partition according to these simple functions, as $f_i$'s are bounded. By choosing a small enough $\beta$, $|P(A_j)-Q(A_j)|<\beta$ for all $i$ implies $|\int \tilde f_idP-\int \tilde f_idQ|<\frac{\alpha}{3}$. I have no idea about question 1, so I go to question 2, thinking to construct some bounded continuous functions to build a new open set. a). for each measurable set $A_i$, find two continuous functions $\bar{f_i}$ and $\underline{f_i}$, such that  $$\bar{f_i}(x)\geq I_{A_i}(x)\geq \underline{f_i}(x),~\text{for every $x\in\mathcal X$},$$ $$\int \bar{f_i} dP- \int \underline{f_i}dP<\frac{\beta}{2},$$ where $I_{A_i}(\cdot)$ is the indicator function. b). For any probability measure $Q$, we have $$\int\underline{f_i}dQ\leq Q(A_i)\leq\int\bar{f_i}dQ.$$ Thus, if $|\int\bar{f_i}dP - \int\underline{f_i}dQ|<\beta$ and $|\int\underline{f_i}dP - \int\bar{f_i}dQ|<\beta$, we must have $|P(A_i)-Q(A_i)|<\beta$. We also have \begin{align} &\left|\int\underline{f_i}dP - \int\bar{f_i}dQ\right|\\ =~&\left|\int\underline{f_i}dP-\int\bar{f_i}dP+\int\bar{f_i}dP-\int\bar{f_i}dQ \right|\\ \leq~&\left|\int\underline{f_i}dP-\int\bar{f_i}dP\right|+\left|\int\bar{f_i}dP-\int\bar{f_i}dQ \right| \end{align} Similarly, we have $$\left|\int\bar{f_i}dP - \int\underline{f_i}dQ\right|\leq\left|\int\bar{f_i}dP-\int\underline{f_i}dP\right|+\left|\int\underline{f_i}dP-\underline{f_i}dQ \right|$$ As such, adding the following constraints $$\left|\int\underline{f_i}dP-\underline{f_i}dQ \right|<\frac{\beta}{2}, \left|\int\bar{f_i}dP-\bar{f_i}dQ \right|<\frac{\beta}{2}$$ will construct a new open set that is contained in $\tilde{U}(P,\beta)$. It seems that a) is not always true. When $\mathcal X=\mathbb R$, consider $P= \frac{1}{2}I_{[0,1]} + \frac{1}{2}I_{\{3\}}$. For a measurable set $A=[2,3)$, no such $\bar{f_i}$ and $\underline{f_i}$ exist for each $\beta>0$. I think it possible to place some constraints on the partition $A_1,A_2,\ldots, A_m$ for the simple functions $\tilde f_i$, but I got no formal proof/reference to it. Can you advise me how to make this work? Thanks!!~","Let $(\mathcal X,\mathcal F)$ be a measurable space and $\mathcal P$ the set of probability measures  on this space. With weak convergence (i.e., $P_n\to P$ weakly if and only if $\mathbb E_{P_n}(f)\to\mathbb E_P(f)$ for every bounded continuous functions $f:\mathcal X\to\mathbb R$), a basis of the corresponding weak topology induced by this weak convergence is a collection of the following sets: $$U(P,\alpha)=\left\{Q:\left|\int f_idP-\int f_idQ\right|<\alpha,i=1,\ldots,n\right\}$$ for some $P\in\mathcal P$, $f_i$ bounded continuous functions, $n$ is finite, and $\alpha>0$. Then there exists a finite measurable partition $A_1,\ldots,A_m$ of $\mathcal X$ and $\beta>0$ such that \begin{align} \tilde{U}(P,\beta)=\{Q:|P(A_j)-Q(A_j)|<\beta\}\subset{U}(P,\alpha). \end{align} My questions are: is $\tilde{U}(P,\epsilon)$ also open? if not, is there an open set, say $U'(P, \alpha')$, that is contained in $\tilde{U}(P,\beta)$? or under what conditions there exists such a $U'(P, \alpha')$? (might have different bounded continuous functions) Can you help me with these question? or please see my thinking below. Let me first prove the existence of such $\tilde{U}(P,\beta)\subset U(P,\alpha).$ Since $f_i$ is bounded continuous, then there exists a simple function $\tilde f_i$ such that $\|f_i-\tilde f_i\|_{\infty} <\frac{\alpha}{3}$. Then if $|\int \tilde{f}_idP - \int \tilde{f}_idQ|<\frac{\alpha}{3}$, we have  \begin{align} &\left|\int f_idP-\int f_idQ\right|\\ =~&\left|\int f_idP-\int \tilde f_idP+\int \tilde f_idP-\int \tilde f_idQ+\int \tilde f_idQ-\int f_idQ\right|\\ <~&\alpha. \end{align} We can then define a finite partition according to these simple functions, as $f_i$'s are bounded. By choosing a small enough $\beta$, $|P(A_j)-Q(A_j)|<\beta$ for all $i$ implies $|\int \tilde f_idP-\int \tilde f_idQ|<\frac{\alpha}{3}$. I have no idea about question 1, so I go to question 2, thinking to construct some bounded continuous functions to build a new open set. a). for each measurable set $A_i$, find two continuous functions $\bar{f_i}$ and $\underline{f_i}$, such that  $$\bar{f_i}(x)\geq I_{A_i}(x)\geq \underline{f_i}(x),~\text{for every $x\in\mathcal X$},$$ $$\int \bar{f_i} dP- \int \underline{f_i}dP<\frac{\beta}{2},$$ where $I_{A_i}(\cdot)$ is the indicator function. b). For any probability measure $Q$, we have $$\int\underline{f_i}dQ\leq Q(A_i)\leq\int\bar{f_i}dQ.$$ Thus, if $|\int\bar{f_i}dP - \int\underline{f_i}dQ|<\beta$ and $|\int\underline{f_i}dP - \int\bar{f_i}dQ|<\beta$, we must have $|P(A_i)-Q(A_i)|<\beta$. We also have \begin{align} &\left|\int\underline{f_i}dP - \int\bar{f_i}dQ\right|\\ =~&\left|\int\underline{f_i}dP-\int\bar{f_i}dP+\int\bar{f_i}dP-\int\bar{f_i}dQ \right|\\ \leq~&\left|\int\underline{f_i}dP-\int\bar{f_i}dP\right|+\left|\int\bar{f_i}dP-\int\bar{f_i}dQ \right| \end{align} Similarly, we have $$\left|\int\bar{f_i}dP - \int\underline{f_i}dQ\right|\leq\left|\int\bar{f_i}dP-\int\underline{f_i}dP\right|+\left|\int\underline{f_i}dP-\underline{f_i}dQ \right|$$ As such, adding the following constraints $$\left|\int\underline{f_i}dP-\underline{f_i}dQ \right|<\frac{\beta}{2}, \left|\int\bar{f_i}dP-\bar{f_i}dQ \right|<\frac{\beta}{2}$$ will construct a new open set that is contained in $\tilde{U}(P,\beta)$. It seems that a) is not always true. When $\mathcal X=\mathbb R$, consider $P= \frac{1}{2}I_{[0,1]} + \frac{1}{2}I_{\{3\}}$. For a measurable set $A=[2,3)$, no such $\bar{f_i}$ and $\underline{f_i}$ exist for each $\beta>0$. I think it possible to place some constraints on the partition $A_1,A_2,\ldots, A_m$ for the simple functions $\tilde f_i$, but I got no formal proof/reference to it. Can you advise me how to make this work? Thanks!!~",,"['functional-analysis', 'probability-theory', 'measure-theory']"
67,Is there a Stochastic Time derivative,Is there a Stochastic Time derivative,,"The Setup Suppose I have a stochastic process $f(t,Z_t)$ where $Z_t$ solve the $d$-dimensional SDE $$ dZ_t = \mu(t,Z_t)dt + \sigma(t,Z_t)dW_t $$ and $f$ is a smooth function. My Question Is there a notion of time-derivative ""$d_t$"" of the process $f(t,Z_t)$ which satisfies: Some sort of chain rule like $$ \partial_t f(t,Z_t) * d_t(Z_t), $$ where $\partial_t$ is the usual derivative wrt $t$. If $Z_t$ is deterministic (ie: $\sigma(t,Z_t)=0$) and $\mu(t,z)$ is $C^1$ in $t$ then $$d_t=\partial_t,$$ ie: $d_t$ reduces to the usual derivative when $f(t,Z_t)$ is a smooth function of $t$.","The Setup Suppose I have a stochastic process $f(t,Z_t)$ where $Z_t$ solve the $d$-dimensional SDE $$ dZ_t = \mu(t,Z_t)dt + \sigma(t,Z_t)dW_t $$ and $f$ is a smooth function. My Question Is there a notion of time-derivative ""$d_t$"" of the process $f(t,Z_t)$ which satisfies: Some sort of chain rule like $$ \partial_t f(t,Z_t) * d_t(Z_t), $$ where $\partial_t$ is the usual derivative wrt $t$. If $Z_t$ is deterministic (ie: $\sigma(t,Z_t)=0$) and $\mu(t,z)$ is $C^1$ in $t$ then $$d_t=\partial_t,$$ ie: $d_t$ reduces to the usual derivative when $f(t,Z_t)$ is a smooth function of $t$.",,"['functional-analysis', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
68,Prove that $e^x$ is not a tempered distribution on $\mathbb{R}$,Prove that  is not a tempered distribution on,e^x \mathbb{R},"Consider the following sequence of functions $\psi_n(x) = e^{-(1+\varepsilon)x} \dfrac{1_{|x|\leq n}}{n}$. Clearly, $|\psi_n^{(m)}(x)|\leq\dfrac{(1+\varepsilon)^m}{n}$. Hence, the $\psi_n$-s are convergent to $0$ in $\mathscr{S}(\mathbb{R})$. However, it is easily computed that $\int_{\mathbb{R}} \psi_n(x)e^xdx = \int_{-n}^{n} e^{-\varepsilon x}dx = \dfrac{1}{\varepsilon}\dfrac{e^{n\varepsilon} - e^{-n\varepsilon }}{n}\geq\dfrac{e^\varepsilon - 1}{\varepsilon}$. Therefore, $v(x) = e^x$ is not a tempered distrubition. Can anybody check if my attempt at proving the claim correct? My idea was based on this discussion","Consider the following sequence of functions $\psi_n(x) = e^{-(1+\varepsilon)x} \dfrac{1_{|x|\leq n}}{n}$. Clearly, $|\psi_n^{(m)}(x)|\leq\dfrac{(1+\varepsilon)^m}{n}$. Hence, the $\psi_n$-s are convergent to $0$ in $\mathscr{S}(\mathbb{R})$. However, it is easily computed that $\int_{\mathbb{R}} \psi_n(x)e^xdx = \int_{-n}^{n} e^{-\varepsilon x}dx = \dfrac{1}{\varepsilon}\dfrac{e^{n\varepsilon} - e^{-n\varepsilon }}{n}\geq\dfrac{e^\varepsilon - 1}{\varepsilon}$. Therefore, $v(x) = e^x$ is not a tempered distrubition. Can anybody check if my attempt at proving the claim correct? My idea was based on this discussion",,"['functional-analysis', 'distribution-theory']"
69,Reflexive Banach space: Boundedness of subset implies weak compactness. Closed or not?,Reflexive Banach space: Boundedness of subset implies weak compactness. Closed or not?,,"Claim:In a reflexive Banach space, the weak compactness of a subset is equivalent to the boundedness of the subset. But there is no guarantee that the bounded subset would even have its sequences converge in the bounded subset. 1) Is this statement true how it is? 2) Is it true with the additional condition of the subset being closed? Note: I understand that weakly compact implies bounded direction works from the definition of weak compactness having these subsequences converge in the subset. Thank you","Claim:In a reflexive Banach space, the weak compactness of a subset is equivalent to the boundedness of the subset. But there is no guarantee that the bounded subset would even have its sequences converge in the bounded subset. 1) Is this statement true how it is? 2) Is it true with the additional condition of the subset being closed? Note: I understand that weakly compact implies bounded direction works from the definition of weak compactness having these subsequences converge in the subset. Thank you",,"['functional-analysis', 'banach-spaces', 'normed-spaces']"
70,"$T:X\to Y$ a linear operator, $Y$ finite-dimensional, then $\ker T$ is closed iff $T$ is continuous","a linear operator,  finite-dimensional, then  is closed iff  is continuous",T:X\to Y Y \ker T T,"I have to prove that if $T:X\to Y$ a linear operator on normed spaces, $Y$ finite-dimensional, then $\ker T$ is closed iff $T$ is continuous. On a lot of places I see a proof that looks like this : it starts by assuming $Y=\mathbb{R}$ for simplicity. (I'm also aware of this that use a completely different construction). Anyway, I did not at all find it obvious that there was any kind of generalisation of the first linked proof to higher dimensions. I will give my (quite long a tedious) generalisation here, but my question is: is there a more direct/easier generalisation of the construction given in the first linked proof? (I also wouldn't object to someone checking the actual correctness of this proof) We will take $Y=\mathbb{R}^n$. Take $x_n\in X$ s.t. $||x_n||=1$, $||Tx_n||\to \infty$. w.l.o.g. we may assume that $\frac{Tx_n}{||Tx_n||}\to v_1\in \overline{B(0,1)}\subset \mathbb{R^n}$, by the compactness of the closed unit ball in finite dimensions. We now take $y_n=x_j$ for some $j$ s.t. $||\frac{Tx_n}{||Tx_n||}-v_1||<\frac{1}{n}$. Then we adjust the $y_n$ a little bit: if $||Ty_n||>n$ we set $y_n=\frac{y_n}{||Ty_n||}n$. We now obtain a sequence $y_n$ s.t. $$ \begin{align*} ||y_n||\leq 1\\ ||Ty_n||\leq n\\ ||Ty_n||\to \infty \end{align*} $$ In particular we have that $||Ty_n-||Ty_n||v_1||<1$ (this is the critical property that we needed). We now take an orthonormal basis $\{v_i\}$ for $\mathcal{R}T$ containing $v_1$, and $v_i'$ s.t. $Tv_i'=v_i$. We conclude that for $j>1$ we must have $(v_j,Ty_n)<1$. We now define $z_n=y_n-\sum_{j>1}(v_j,Ty_n)v_j'$. Then $$||z_n||=||y_n-\sum_{j>1}(v_j,Ty_n)v_j'||\leq ||y_n||+\sum_{j>1}(v_j,Ty_n)||v_j'||\leq 1+\sum_{j>1}||v_j'||\leq C$$ Hence these $z_n$ are bounded. Furthermore we clearly still have that $$||Tz_n||\to\infty$$ because the norm of $T\sum_{j>1}(v_j,Ty_n)v_j'$ is bounded. We can now finally define  $$w_n=v'_1-\frac{z_n}{||Tz_n||}$$ Then  $$Tw_n=v_1-\frac{1}{||Tz_n||}(Ty_n-\sum_{j>1}(v_j,Ty_n)v_j)=0$$ Since $Ty_n-\sum_{j>1}(v_j,Ty_n)v_j$ is by construction a vector pointing in the direction of $v_1$. But again $w_n\to v'_1$, which contradicts the closedness of the kernel. For the intuition: We take the unbounded sequence in $X$. The the project is onto the unit ball, which is compact, hence the sequence a convergent subsequence. We take the sequence, with limit $v_1$. Then we 'limit the growth' of our unbounded sequence to make sure that if $\frac{Tx}{||Tx||}$ is close to $v_1$, then $Tx$ is close to the line spanned by $v_1$. Then we remove any orthogonal components from the $Tx$, and because $Tx$ is now close to $<v_1>$ we can remove the orthogonal components by subtracting a vector of a certain bounded length. This finally gives a bounded sequence in $X$ with unbounded images in $Y$ all lying on the lines $<v_1>$, and from here the previous argument goes through. Thanks for reading.","I have to prove that if $T:X\to Y$ a linear operator on normed spaces, $Y$ finite-dimensional, then $\ker T$ is closed iff $T$ is continuous. On a lot of places I see a proof that looks like this : it starts by assuming $Y=\mathbb{R}$ for simplicity. (I'm also aware of this that use a completely different construction). Anyway, I did not at all find it obvious that there was any kind of generalisation of the first linked proof to higher dimensions. I will give my (quite long a tedious) generalisation here, but my question is: is there a more direct/easier generalisation of the construction given in the first linked proof? (I also wouldn't object to someone checking the actual correctness of this proof) We will take $Y=\mathbb{R}^n$. Take $x_n\in X$ s.t. $||x_n||=1$, $||Tx_n||\to \infty$. w.l.o.g. we may assume that $\frac{Tx_n}{||Tx_n||}\to v_1\in \overline{B(0,1)}\subset \mathbb{R^n}$, by the compactness of the closed unit ball in finite dimensions. We now take $y_n=x_j$ for some $j$ s.t. $||\frac{Tx_n}{||Tx_n||}-v_1||<\frac{1}{n}$. Then we adjust the $y_n$ a little bit: if $||Ty_n||>n$ we set $y_n=\frac{y_n}{||Ty_n||}n$. We now obtain a sequence $y_n$ s.t. $$ \begin{align*} ||y_n||\leq 1\\ ||Ty_n||\leq n\\ ||Ty_n||\to \infty \end{align*} $$ In particular we have that $||Ty_n-||Ty_n||v_1||<1$ (this is the critical property that we needed). We now take an orthonormal basis $\{v_i\}$ for $\mathcal{R}T$ containing $v_1$, and $v_i'$ s.t. $Tv_i'=v_i$. We conclude that for $j>1$ we must have $(v_j,Ty_n)<1$. We now define $z_n=y_n-\sum_{j>1}(v_j,Ty_n)v_j'$. Then $$||z_n||=||y_n-\sum_{j>1}(v_j,Ty_n)v_j'||\leq ||y_n||+\sum_{j>1}(v_j,Ty_n)||v_j'||\leq 1+\sum_{j>1}||v_j'||\leq C$$ Hence these $z_n$ are bounded. Furthermore we clearly still have that $$||Tz_n||\to\infty$$ because the norm of $T\sum_{j>1}(v_j,Ty_n)v_j'$ is bounded. We can now finally define  $$w_n=v'_1-\frac{z_n}{||Tz_n||}$$ Then  $$Tw_n=v_1-\frac{1}{||Tz_n||}(Ty_n-\sum_{j>1}(v_j,Ty_n)v_j)=0$$ Since $Ty_n-\sum_{j>1}(v_j,Ty_n)v_j$ is by construction a vector pointing in the direction of $v_1$. But again $w_n\to v'_1$, which contradicts the closedness of the kernel. For the intuition: We take the unbounded sequence in $X$. The the project is onto the unit ball, which is compact, hence the sequence a convergent subsequence. We take the sequence, with limit $v_1$. Then we 'limit the growth' of our unbounded sequence to make sure that if $\frac{Tx}{||Tx||}$ is close to $v_1$, then $Tx$ is close to the line spanned by $v_1$. Then we remove any orthogonal components from the $Tx$, and because $Tx$ is now close to $<v_1>$ we can remove the orthogonal components by subtracting a vector of a certain bounded length. This finally gives a bounded sequence in $X$ with unbounded images in $Y$ all lying on the lines $<v_1>$, and from here the previous argument goes through. Thanks for reading.",,"['functional-analysis', 'solution-verification', 'linear-transformations', 'alternative-proof']"
71,Existence of a continuous mapping that maps the closed unit ball onto its exterior,Existence of a continuous mapping that maps the closed unit ball onto its exterior,,"I saw the post about a continuous mapping $F: X \to X$ where $X$ is an infinite-dimensional Banach space that maps the closed unit ball of $X$ onto the unbounded set. A continuous mapping with the unbounded image of the unit ball in an infinite-dimensional Banach space It is very interesting to me, is there such a continuous mapping $F: X \to X$ that maps the closed unit ball of $X$ onto its exterior. That is, let, as in the previous link, $X$ be an infinite-dimensional Banach space, $B=\{x\in X: \|x\| \leq 1\}$, $E=\{x\in X: \|x\| \geq 1\}$. Does there exist a continuous mapping $F: X \to X$ such that $F(B)=E$? Or, at least, $F(B)\subset E$.","I saw the post about a continuous mapping $F: X \to X$ where $X$ is an infinite-dimensional Banach space that maps the closed unit ball of $X$ onto the unbounded set. A continuous mapping with the unbounded image of the unit ball in an infinite-dimensional Banach space It is very interesting to me, is there such a continuous mapping $F: X \to X$ that maps the closed unit ball of $X$ onto its exterior. That is, let, as in the previous link, $X$ be an infinite-dimensional Banach space, $B=\{x\in X: \|x\| \leq 1\}$, $E=\{x\in X: \|x\| \geq 1\}$. Does there exist a continuous mapping $F: X \to X$ such that $F(B)=E$? Or, at least, $F(B)\subset E$.",,['functional-analysis']
72,"Looking for a ""job description"" for Hölder's inequality","Looking for a ""job description"" for Hölder's inequality",,"Here's an example of what I mean by ""job description"" in the post's title: triangle inequality : to be used, whenever the (unsigned) distances between adjacent points in a sequence $x_0, x_1, x_2, \dots, x_{n>1}$ are given, to derive an upper bound on the (unsigned) distance between the endpoints $x_0$ and $x_n$. 1 Is it possible to give a similarly high-level characterization of the kind of problem that Hölder's inequality is useful for? BTW, the generality of the theorem's statement, which always involves exponents $s, t \in [1, \infty]$ satisfying $s^{-1}+t^{-1} = 1$, greatly exceeds the modest needs of every application area I ever come across, where only the case $s = t = 2$ ever matters.  This may have something to do with my inability to formulate a job description for this inequality. 1 This is the basic idea, though there are several variations of it.  Also, I don't claim that this is, even ""in essence"", the only way to interpret the triangle inequality's utility.  This ""job description"" is only meant to make concrete the difference between knowledge that is somehow ""in active use"", and that, like my present ""knowledge"" of the Hölder's inequality, which is passive at best.","Here's an example of what I mean by ""job description"" in the post's title: triangle inequality : to be used, whenever the (unsigned) distances between adjacent points in a sequence $x_0, x_1, x_2, \dots, x_{n>1}$ are given, to derive an upper bound on the (unsigned) distance between the endpoints $x_0$ and $x_n$. 1 Is it possible to give a similarly high-level characterization of the kind of problem that Hölder's inequality is useful for? BTW, the generality of the theorem's statement, which always involves exponents $s, t \in [1, \infty]$ satisfying $s^{-1}+t^{-1} = 1$, greatly exceeds the modest needs of every application area I ever come across, where only the case $s = t = 2$ ever matters.  This may have something to do with my inability to formulate a job description for this inequality. 1 This is the basic idea, though there are several variations of it.  Also, I don't claim that this is, even ""in essence"", the only way to interpret the triangle inequality's utility.  This ""job description"" is only meant to make concrete the difference between knowledge that is somehow ""in active use"", and that, like my present ""knowledge"" of the Hölder's inequality, which is passive at best.",,"['functional-analysis', 'measure-theory', 'inequality']"
73,Separable Hahn-Banach and the axiom of choice,Separable Hahn-Banach and the axiom of choice,,"We had in our functional analysis course a proof for the Hahn-Banach theorem on a separable Banach space which doesn't need, according to our professsor, the axiom of choice. Yesterday I read the proof again and I am not sure if we really don't need the AC. The proof starts with: ""Let $\{ x_1, x_2, \dots \}$ be a countable dense set."" And this is exactly the point I am confused about. The definition of a separable space just gives us the existence of countable dense sets but not a concrete one. Don't we need, in general, the axiom of choice for choosing a concrete set?","We had in our functional analysis course a proof for the Hahn-Banach theorem on a separable Banach space which doesn't need, according to our professsor, the axiom of choice. Yesterday I read the proof again and I am not sure if we really don't need the AC. The proof starts with: ""Let $\{ x_1, x_2, \dots \}$ be a countable dense set."" And this is exactly the point I am confused about. The definition of a separable space just gives us the existence of countable dense sets but not a concrete one. Don't we need, in general, the axiom of choice for choosing a concrete set?",,"['functional-analysis', 'axiom-of-choice', 'hahn-banach-theorem']"
74,Riesz's Lemma for $l^\infty$ and $\alpha = 1$,Riesz's Lemma for  and,l^\infty \alpha = 1,"Riesz's Lemma says the following: Let $X$ be a normed vector space and $Y$ a proper closed subspace of $X$ . Pick $\alpha \in (0,1)$ . Then $\exists x\in X$ such that $|x|=1$ and $d(x,y) \geq \alpha$ for all $y \in Y$ . I am investigating why we cannot take $\alpha=1$ in general. Wikipedia says ""the space $l^\infty$ of all bounded sequences shows that the lemma does not hold for $\alpha = 1$ "". I believe there's a simple example but I don't see what to take as $Y$ ... So what exactly is the example (i.e. $Y$ )? Edit: It is unfortunate that Wikipedia states the lemma with the version $>\alpha$ . With this version, I do not need the specific space of $l^\infty$ to show that the lemma does not hold for $\alpha = 1$ . Let $X$ be any normed vector space, and take any $x$ with $|x|=1$ . Then $\inf_{y\in Y} d(x,y) \leq d(x,0) = 1$ . As Bombyx pointed out, $l^\infty$ is not reflexive so there should be an example for the version I have stated Riesz's Lemma. Since Wikipedia raised $l^\infty$ , I was thinking that maybe there is a (simple) constructive proof for this example. Hence, I was asking if anyone has any idea on what $Y$ should be.","Riesz's Lemma says the following: Let be a normed vector space and a proper closed subspace of . Pick . Then such that and for all . I am investigating why we cannot take in general. Wikipedia says ""the space of all bounded sequences shows that the lemma does not hold for "". I believe there's a simple example but I don't see what to take as ... So what exactly is the example (i.e. )? Edit: It is unfortunate that Wikipedia states the lemma with the version . With this version, I do not need the specific space of to show that the lemma does not hold for . Let be any normed vector space, and take any with . Then . As Bombyx pointed out, is not reflexive so there should be an example for the version I have stated Riesz's Lemma. Since Wikipedia raised , I was thinking that maybe there is a (simple) constructive proof for this example. Hence, I was asking if anyone has any idea on what should be.","X Y X \alpha \in (0,1) \exists x\in X |x|=1 d(x,y) \geq \alpha y \in Y \alpha=1 l^\infty \alpha = 1 Y Y >\alpha l^\infty \alpha = 1 X x |x|=1 \inf_{y\in Y} d(x,y) \leq d(x,0) = 1 l^\infty l^\infty Y","['functional-analysis', 'normed-spaces']"
75,Proof of equivalence of Sobolev Space and Lipschitz functions,Proof of equivalence of Sobolev Space and Lipschitz functions,,"The attachment is a proof from Evans book ""Measure Theory and Fine Properties of Functions"" pg 132 Theorem 5. The statement of the theorem is: Let $f:U \rightarrow \mathbb{R}$. Then $f$ is locally Lipschitz in $U$ if and only if $f \in W^{1,\infty}_{loc}(U)$. My two questions are only regarding the first direction($\Rightarrow$) of the theorem. Can we get the sequence $g_{i}^{h_{j}} \rightharpoonup g_{i}$ weakly in $L^{p}_{loc}(U)$, where $g_{i} \in L_{loc}^{\infty}(U)$ by noting: Since $(g_{i}^{h_{j}})_{j}$ is bounded in $L^{\infty}_{loc}(U)$ we can find a weak* convergent subsequence such that $g_{i}^{h_{j}} \rightharpoonup g_{i}$ in $L^{\infty}_{loc}(U)$ and then use Theroem 3 in section 1.9 (which states: since $g_{i}^{h_{j}}$ is bounded in $L^{\infty}_{loc}(U)$ there exists a subsequence which converges weakly in $L^{p}_{loc}(U)$) to get a further subsequence (which I will again call $g_{i}^{h_{j}}$) such that $g_{i}^{h_{j}} \rightharpoonup v$ weakly in $L^{p}_{loc}(U)$, by uniqueness of limits we get $v = g_{i}$. Does this reasoning work? Is there a simpler way to show this? To get the result that $\lim\limits_{j \rightarrow \infty}\int_{U}f(x)\frac{\varphi(x+h_{j}e_{i})-\varphi(x)}{h_{j}}dx = \int_{U}f(x)\frac{\partial \varphi}{\partial x_{i}}dx$ at the end, can I use the Lebesgue Dominated Convergence Theorem where I would dominate $f(x)\frac{\varphi(x+h_{j}e_{i})-\varphi(x)}{h_{j}}$. Since $\varphi \in C_{c}^{1}(V)$ is continuous on a compact  set it is bounded, so $\frac{\varphi(x+h_{j}e_{i})-\varphi(x)}{h_{j}} \leq C $ for some constant $C > 0$. But since $V$ is bounded we get $\int_{V}Cdx \leq C\int_{V}dx = C|V| < +\infty$. Then $g := MC$ becomes the summable function which dominates  , where $M := \Vert f \Vert_{L^{\infty}_{loc}(U)}$. Does this reasoning work? Is there a simpler way to show this? Please let me know where I went wrong first before showing the correct reasoning. Thanks for any assistance. Let me know if anything is unclear.","The attachment is a proof from Evans book ""Measure Theory and Fine Properties of Functions"" pg 132 Theorem 5. The statement of the theorem is: Let $f:U \rightarrow \mathbb{R}$. Then $f$ is locally Lipschitz in $U$ if and only if $f \in W^{1,\infty}_{loc}(U)$. My two questions are only regarding the first direction($\Rightarrow$) of the theorem. Can we get the sequence $g_{i}^{h_{j}} \rightharpoonup g_{i}$ weakly in $L^{p}_{loc}(U)$, where $g_{i} \in L_{loc}^{\infty}(U)$ by noting: Since $(g_{i}^{h_{j}})_{j}$ is bounded in $L^{\infty}_{loc}(U)$ we can find a weak* convergent subsequence such that $g_{i}^{h_{j}} \rightharpoonup g_{i}$ in $L^{\infty}_{loc}(U)$ and then use Theroem 3 in section 1.9 (which states: since $g_{i}^{h_{j}}$ is bounded in $L^{\infty}_{loc}(U)$ there exists a subsequence which converges weakly in $L^{p}_{loc}(U)$) to get a further subsequence (which I will again call $g_{i}^{h_{j}}$) such that $g_{i}^{h_{j}} \rightharpoonup v$ weakly in $L^{p}_{loc}(U)$, by uniqueness of limits we get $v = g_{i}$. Does this reasoning work? Is there a simpler way to show this? To get the result that $\lim\limits_{j \rightarrow \infty}\int_{U}f(x)\frac{\varphi(x+h_{j}e_{i})-\varphi(x)}{h_{j}}dx = \int_{U}f(x)\frac{\partial \varphi}{\partial x_{i}}dx$ at the end, can I use the Lebesgue Dominated Convergence Theorem where I would dominate $f(x)\frac{\varphi(x+h_{j}e_{i})-\varphi(x)}{h_{j}}$. Since $\varphi \in C_{c}^{1}(V)$ is continuous on a compact  set it is bounded, so $\frac{\varphi(x+h_{j}e_{i})-\varphi(x)}{h_{j}} \leq C $ for some constant $C > 0$. But since $V$ is bounded we get $\int_{V}Cdx \leq C\int_{V}dx = C|V| < +\infty$. Then $g := MC$ becomes the summable function which dominates  , where $M := \Vert f \Vert_{L^{\infty}_{loc}(U)}$. Does this reasoning work? Is there a simpler way to show this? Please let me know where I went wrong first before showing the correct reasoning. Thanks for any assistance. Let me know if anything is unclear.",,"['functional-analysis', 'measure-theory', 'sobolev-spaces', 'lp-spaces', 'weak-convergence']"
76,A proposition about Voiculescu's Theorem in C*-algebra,A proposition about Voiculescu's Theorem in C*-algebra,,"It is the quotation below: Exploiting the duality between completely positive map $A \rightarrow M_{n}(C)$ and states on $M_{n}(A)$, it is not too hard to deduce the next result from Glimm's lemma. (Glimm's lemma) Let $A\subset B(H)$ be a separable C*-algebra containing no nonzero compact operators on $H$. If $\phi$ is a state on $A$, then there exist orthonormal vectors $\{\xi_{n}\}$ such that $\langle a\xi_{n}, \xi_{n} \rangle \rightarrow \phi(a)$ for all $a\in A$. Proposition 1.7.1. Let $H$ be separable Hilbert space, $1\in A\subset B(H)$ be a separable C*-algebra and $\phi: A\rightarrow M_{n}(C)$ be a unital completely positive map such that $\phi|_{A\cap K(H)}=0$. Then there exist isometries $V_{k}:l_{n}^{2}\rightarrow H$ with the following properties: (1) the ranges of the $V_{k}$ 's are pairwise orthogonal; (2) $||\phi(a)-V_{k}^{*}aV_{k}||\rightarrow 0$ for every $a\in A$. (Here, $K(H)$ denotes the compacts and $l_{n}^{2}$ denotes the n-dimensional Hilbert space.) I do not know how to utilise the duality to deduce the proposition, could someone show me more details?","It is the quotation below: Exploiting the duality between completely positive map $A \rightarrow M_{n}(C)$ and states on $M_{n}(A)$, it is not too hard to deduce the next result from Glimm's lemma. (Glimm's lemma) Let $A\subset B(H)$ be a separable C*-algebra containing no nonzero compact operators on $H$. If $\phi$ is a state on $A$, then there exist orthonormal vectors $\{\xi_{n}\}$ such that $\langle a\xi_{n}, \xi_{n} \rangle \rightarrow \phi(a)$ for all $a\in A$. Proposition 1.7.1. Let $H$ be separable Hilbert space, $1\in A\subset B(H)$ be a separable C*-algebra and $\phi: A\rightarrow M_{n}(C)$ be a unital completely positive map such that $\phi|_{A\cap K(H)}=0$. Then there exist isometries $V_{k}:l_{n}^{2}\rightarrow H$ with the following properties: (1) the ranges of the $V_{k}$ 's are pairwise orthogonal; (2) $||\phi(a)-V_{k}^{*}aV_{k}||\rightarrow 0$ for every $a\in A$. (Here, $K(H)$ denotes the compacts and $l_{n}^{2}$ denotes the n-dimensional Hilbert space.) I do not know how to utilise the duality to deduce the proposition, could someone show me more details?",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'von-neumann-algebras']"
77,Weak periodic solution of parabolic PDE,Weak periodic solution of parabolic PDE,,"Take $$ u_t(t) + A(t)u(t) = f(t), $$ $$ u(0) = u(T), $$ where $A$ is an linear elliptic operator and the first equation is an equality in $L^2(0,T;V^*)$ for $V \subset H \subset V^*$ Hilbert triple. (there is slight abuse of notation in the equality but never mind) Under what conditions does a solution to this problem exist? By solution I mean $u \in L^2(0,T;V)$ with $u_t \in L^2(0,T;V^*)$ (or $u_t \in L^2(0,T;H)$ if data is smooth enough). Apart from requiring maybe $A(0) = A(T)$ and $f(0) = f(T)$. How does one prove this via the Galerkin approach? Thanks","Take $$ u_t(t) + A(t)u(t) = f(t), $$ $$ u(0) = u(T), $$ where $A$ is an linear elliptic operator and the first equation is an equality in $L^2(0,T;V^*)$ for $V \subset H \subset V^*$ Hilbert triple. (there is slight abuse of notation in the equality but never mind) Under what conditions does a solution to this problem exist? By solution I mean $u \in L^2(0,T;V)$ with $u_t \in L^2(0,T;V^*)$ (or $u_t \in L^2(0,T;H)$ if data is smooth enough). Apart from requiring maybe $A(0) = A(T)$ and $f(0) = f(T)$. How does one prove this via the Galerkin approach? Thanks",,"['functional-analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces']"
78,Definition of the Hamiltonian via Legendre transform.,Definition of the Hamiltonian via Legendre transform.,,"In my book of classical mechanics ( Mathematical methods of classical mechanics by V.I. Arnold), the Hamiltonian is introduced in this way (my translation): Let us consider the system of equations $\dot p = \partial L /\partial \dot q$ ( $p\in \mathbb R^n$ , $q\in \mathbb R ^n$ , the second member is the gradient of the Lagrangian with respect to $\dot q$ ), defined by a Lagrangian that we will suppose convex with respect to the second argument $\dot q$ . [...] By definition, the Legendre transform in $\dot q$ of $L(q,\dot q ,t)$ is a function $H(p)=p\dot q-L(\dot q)$ , where $\dot q$ is given by the relation: $$p=\dfrac{\partial L}{\partial \dot q}.$$ Now, my definition of Legendre transform of a function $f:\mathbb R ^n \to \mathbb R $ is: $$g(p)=\sup _{x\in \mathbb R^n} (\langle p,x\rangle-f(x)).$$ I can see that the quoted definition coincides with mine if for example, we suppose that $f$ is a quadratic form $$f(x)=x^T A x,$$ for a positive definite symmetric matrix. In the general case of a convex $f$ (that, to clarify, here means “definite positive Hessian matrix”), however, I don't see how are we granted that: The maximum is attained at a point $x\in \mathbb R ^n$ (we should at least require that $f$ is coercive right? Counterexample: $f(x)=-\ln x$ ) The equation $p=\partial f / \partial x$ has a unique solution. What are sufficient (or maybe necessary and sufficient) conditions for the above formulas to properly define a function that coincides with the Legendre transform of $L$ ? I've made some progress: Suppose that $f:\mathbb R ^n \to \mathbb R$ has a positive definite hessian matrix $f''(x)$ for all $x$ . Also suppose that $$\lim _{|x|\to \infty } \frac{f(x)}{|x|} = \infty .$$ Then the transform $f^*$ exists and is given by $$f^*(p)=\left\langle p,\xi (p)\right\rangle-f(\xi (p)),$$ where $\xi$ is the inverse of $f'$ . In fact, if the limit holds, one can easily see that $G_p (x) = f(x)-\left\langle x,p\right\rangle$ is coercitive and admits a minimum in $\mathbb R ^n$ , that corresponds to $-f^*(p)$ . At this point, the derivative vanishes, so $f'(x)=p$ (this also proves that $f'$ is surjective). Finally, since $f''>0$ , $f'$ is injective and has an inverse $\xi$ and the Legendre transform is as said above.","In my book of classical mechanics ( Mathematical methods of classical mechanics by V.I. Arnold), the Hamiltonian is introduced in this way (my translation): Let us consider the system of equations ( , , the second member is the gradient of the Lagrangian with respect to ), defined by a Lagrangian that we will suppose convex with respect to the second argument . [...] By definition, the Legendre transform in of is a function , where is given by the relation: Now, my definition of Legendre transform of a function is: I can see that the quoted definition coincides with mine if for example, we suppose that is a quadratic form for a positive definite symmetric matrix. In the general case of a convex (that, to clarify, here means “definite positive Hessian matrix”), however, I don't see how are we granted that: The maximum is attained at a point (we should at least require that is coercive right? Counterexample: ) The equation has a unique solution. What are sufficient (or maybe necessary and sufficient) conditions for the above formulas to properly define a function that coincides with the Legendre transform of ? I've made some progress: Suppose that has a positive definite hessian matrix for all . Also suppose that Then the transform exists and is given by where is the inverse of . In fact, if the limit holds, one can easily see that is coercitive and admits a minimum in , that corresponds to . At this point, the derivative vanishes, so (this also proves that is surjective). Finally, since , is injective and has an inverse and the Legendre transform is as said above.","\dot p = \partial L /\partial \dot q p\in \mathbb R^n q\in \mathbb R ^n \dot q \dot q \dot q L(q,\dot q ,t) H(p)=p\dot q-L(\dot q) \dot q p=\dfrac{\partial L}{\partial \dot q}. f:\mathbb R ^n \to \mathbb R  g(p)=\sup _{x\in \mathbb R^n} (\langle p,x\rangle-f(x)). f f(x)=x^T A x, f x\in \mathbb R ^n f f(x)=-\ln x p=\partial f / \partial x L f:\mathbb R ^n \to \mathbb R f''(x) x \lim _{|x|\to \infty } \frac{f(x)}{|x|} = \infty . f^* f^*(p)=\left\langle p,\xi (p)\right\rangle-f(\xi (p)), \xi f' G_p (x) = f(x)-\left\langle x,p\right\rangle \mathbb R ^n -f^*(p) f'(x)=p f' f''>0 f' \xi","['functional-analysis', 'transformation', 'classical-mechanics']"
79,Show that an unbouned normal operator is closed,Show that an unbouned normal operator is closed,,"A linear operator $A$ is called nomal if $\mathcal{D}(A)=\mathcal{D}(A^{*})$ and $\lVert A\phi\rVert =\lVert A^{*}\phi\rVert$ for every $\phi\in \mathcal{D}(A)$. Show that normal operators are closed. Here I am not assuming that $A$ is bounded nor define on the whole Hilbert space $\mathcal{H}$. Since $A^*$ is defined we know that $\mathcal{D}(A)$ is dense and since $\mathcal{D}(A)=\mathcal{D}(A^*)$ we have that $A^*$ is densely defined this implys that $A$ is closable.  From the hypothesis I know that for any $\phi\in \mathcal{D}(A)$ that $\langle A\phi,A\phi\rangle = \langle A^*\phi,A^*\phi\rangle$ this gives that $\langle A^*A\phi,\phi\rangle=\langle A^{**}A^{*}\phi,\phi\rangle $ here I am assuming that $\phi\in \mathcal{D}(A^{**}A^{*})\cap \mathcal{D}(A^{*}A)$. But now I'm not sure where to go from here.","A linear operator $A$ is called nomal if $\mathcal{D}(A)=\mathcal{D}(A^{*})$ and $\lVert A\phi\rVert =\lVert A^{*}\phi\rVert$ for every $\phi\in \mathcal{D}(A)$. Show that normal operators are closed. Here I am not assuming that $A$ is bounded nor define on the whole Hilbert space $\mathcal{H}$. Since $A^*$ is defined we know that $\mathcal{D}(A)$ is dense and since $\mathcal{D}(A)=\mathcal{D}(A^*)$ we have that $A^*$ is densely defined this implys that $A$ is closable.  From the hypothesis I know that for any $\phi\in \mathcal{D}(A)$ that $\langle A\phi,A\phi\rangle = \langle A^*\phi,A^*\phi\rangle$ this gives that $\langle A^*A\phi,\phi\rangle=\langle A^{**}A^{*}\phi,\phi\rangle $ here I am assuming that $\phi\in \mathcal{D}(A^{**}A^{*})\cap \mathcal{D}(A^{*}A)$. But now I'm not sure where to go from here.",,"['functional-analysis', 'hilbert-spaces']"
80,Can the orbits of a semigroup touch without one being included in the other?,Can the orbits of a semigroup touch without one being included in the other?,,"Question . Let $(S(t))_{t \ge 0}$ be a continuous semigroup of linear operators on some Banach space $X$. Might there exist $f, g\in X$ and $0<t_0<t_1$ such that    \begin{equation}S(t_0)f=S(t_1)g\end{equation}   but \begin{equation} S(t_0-\varepsilon_0)f\ne S(t_1-\varepsilon_1)g \end{equation}   for all $0<\varepsilon_0\le t_0$ and $0<\varepsilon_1\le t_1$ (in particular, $f\ne g$)? Pictorially, I am wondering if the following configuration is possible: Of course we know that, when the evolution is given by a group , this is not possible: orbits either coincide or are disjoint. This is the case of autonomous ODE systems or of the Schrödinger equation. But here we have a semi group, such as the heat one, which only goes forward in time, not backwards. So the only obvious thing that we can say is that, as soon as they touch, orbits merge into one. But in principle I don't see why they should coincide in the past . Added : After some searches, I have found that for the special case of the heat equation, the answer is negative . This is commonly referred to as backward uniqueness property . Here is a simplified version, which takes into account classical solutions on bounded domains: Theorem (Taken from Evans's book on PDE, 2nd ed., pag.64) Let $U\subset \mathbb{R}^n$ be an open and bounded domain. Suppose $u, \bar{u}$ are classical solutions of  \begin{equation} \begin{cases} u_t=\Delta u & \text{in }U\times(0, T) \\  u=0 &\text{on }\partial U \times [0, T] \end{cases} \end{equation} If at time $T$ we have  \begin{equation} u(x, T)=\bar{u}(x, T),\quad \forall x\in U, \end{equation}  then $u\equiv \bar{u}$ on the whole parabolic cylinder $U\times (0, T]$. The property holds in much more general functional settings, as I read here (look for the keyword backward uniqueness ). All of this leaves the general question open. Is the backward uniqueness property true for all continuous linear semigroups? I guess that the answer should be negative, otherwise this would not be regarded as a special feature of the heat equation. However, I cannot find an explicit example.","Question . Let $(S(t))_{t \ge 0}$ be a continuous semigroup of linear operators on some Banach space $X$. Might there exist $f, g\in X$ and $0<t_0<t_1$ such that    \begin{equation}S(t_0)f=S(t_1)g\end{equation}   but \begin{equation} S(t_0-\varepsilon_0)f\ne S(t_1-\varepsilon_1)g \end{equation}   for all $0<\varepsilon_0\le t_0$ and $0<\varepsilon_1\le t_1$ (in particular, $f\ne g$)? Pictorially, I am wondering if the following configuration is possible: Of course we know that, when the evolution is given by a group , this is not possible: orbits either coincide or are disjoint. This is the case of autonomous ODE systems or of the Schrödinger equation. But here we have a semi group, such as the heat one, which only goes forward in time, not backwards. So the only obvious thing that we can say is that, as soon as they touch, orbits merge into one. But in principle I don't see why they should coincide in the past . Added : After some searches, I have found that for the special case of the heat equation, the answer is negative . This is commonly referred to as backward uniqueness property . Here is a simplified version, which takes into account classical solutions on bounded domains: Theorem (Taken from Evans's book on PDE, 2nd ed., pag.64) Let $U\subset \mathbb{R}^n$ be an open and bounded domain. Suppose $u, \bar{u}$ are classical solutions of  \begin{equation} \begin{cases} u_t=\Delta u & \text{in }U\times(0, T) \\  u=0 &\text{on }\partial U \times [0, T] \end{cases} \end{equation} If at time $T$ we have  \begin{equation} u(x, T)=\bar{u}(x, T),\quad \forall x\in U, \end{equation}  then $u\equiv \bar{u}$ on the whole parabolic cylinder $U\times (0, T]$. The property holds in much more general functional settings, as I read here (look for the keyword backward uniqueness ). All of this leaves the general question open. Is the backward uniqueness property true for all continuous linear semigroups? I guess that the answer should be negative, otherwise this would not be regarded as a special feature of the heat equation. However, I cannot find an explicit example.",,"['functional-analysis', 'partial-differential-equations']"
81,A question about operator norm,A question about operator norm,,"I hope this is not an obviously stupid question, I'm quite tired and hence extra slow today. I don't understand the following proof: Given the context I think $X = L^2 (\mathbb T)$, so $T_n : L^2 (\mathbb T) \to \mathbb R$. (The context is: Fourier series and uniform boundedness principle) I don't see how we get the last line from the penultimate one. The operator norm is the $\sup$ over $f$ with norm equal to $1$. But the domain is $L^2$ so it comes with the $L^2$ norm. I'm aware that $\|f\|_1 \leq \|f\|_2$ and that I can apply Hölder to either get $\|T_n\| \leq \|f\|_2 \|D_n\|_2 = \|D_n\|_2$ or $\|T_n\| \leq \|f\|_\infty \|D_n\|_1$. But I want $\|T_n\| \leq \|D_n\|_1$. Thanks for your help.","I hope this is not an obviously stupid question, I'm quite tired and hence extra slow today. I don't understand the following proof: Given the context I think $X = L^2 (\mathbb T)$, so $T_n : L^2 (\mathbb T) \to \mathbb R$. (The context is: Fourier series and uniform boundedness principle) I don't see how we get the last line from the penultimate one. The operator norm is the $\sup$ over $f$ with norm equal to $1$. But the domain is $L^2$ so it comes with the $L^2$ norm. I'm aware that $\|f\|_1 \leq \|f\|_2$ and that I can apply Hölder to either get $\|T_n\| \leq \|f\|_2 \|D_n\|_2 = \|D_n\|_2$ or $\|T_n\| \leq \|f\|_\infty \|D_n\|_1$. But I want $\|T_n\| \leq \|D_n\|_1$. Thanks for your help.",,"['functional-analysis', 'measure-theory']"
82,Why need two directions to make $\sim_{\rm wa}$ an equivalence relation?,Why need two directions to make  an equivalence relation?,\sim_{\rm wa},"Let $\pi$ and $\sigma$ be representations of a $C^*$-algebra $\mathcal{A}$. They are weak approximately equivalent ($\pi\mathbin{\sim_{\rm wa}}\sigma$) if there are sequences of unitary operators $\{U_n\}$ and $\{V_n\}$ such that \begin{equation} \sigma(A)=\operatorname{WOT-lim} U_n\pi(A) U_n^*,  \end{equation} \begin{equation} \pi(A)=\operatorname{WOT-lim} V_n\sigma(A) V_n^* \end{equation} for all $A\in\mathcal{A}$. Many books point out that both directions are needed to obtain an equivalence relation but no clue is given why. Since for approximate equivalence ($\mathbin{\sim_{\rm a}}$), only one direction is needed, I wonder why for $\mathbin{\sim_{\rm wa}}$ we need two. Thanks!","Let $\pi$ and $\sigma$ be representations of a $C^*$-algebra $\mathcal{A}$. They are weak approximately equivalent ($\pi\mathbin{\sim_{\rm wa}}\sigma$) if there are sequences of unitary operators $\{U_n\}$ and $\{V_n\}$ such that \begin{equation} \sigma(A)=\operatorname{WOT-lim} U_n\pi(A) U_n^*,  \end{equation} \begin{equation} \pi(A)=\operatorname{WOT-lim} V_n\sigma(A) V_n^* \end{equation} for all $A\in\mathcal{A}$. Many books point out that both directions are needed to obtain an equivalence relation but no clue is given why. Since for approximate equivalence ($\mathbin{\sim_{\rm a}}$), only one direction is needed, I wonder why for $\mathbin{\sim_{\rm wa}}$ we need two. Thanks!",,"['functional-analysis', 'representation-theory', 'c-star-algebras']"
83,Nonexistence of a cyclic vector for a representation on $\ell^2(\mathbb{Z})$,Nonexistence of a cyclic vector for a representation on,\ell^2(\mathbb{Z}),"Let $S$ be the bilateral shift on $\ell^2(\mathbb{Z})$ and let $T = S + S^*$. I want to show that there is no cyclic vector for the representation of $T$ on $\ell^2(\mathbb{Z})$ i.e. $\forall x\in \ell^2(\mathbb{Z})$ the set $P_x = \{f(x) : f\mbox{ is a polynomial in }T\}$ is not dense in $\ell^2(\mathbb{Z})$. I tried to start simple, showing that if $x = \delta_n$ for $n \in \mathbb{Z}$ then $P_x$ isn't dense. This worked out, but I was exploiting a symmetry that doesn't exist when considering finite linear combinations of such elements. I'm pretty sure this idea isn't going to work out. Then I tried considering translating this operator to an operator on $L^2(S^1)$ via the Fourier transform. The corresponding operator $\hat{T}$ is simply a multiplication operator, the multiplying function being $g(t) = 2\cos(t)$. This seemed more promising initially, but I still don't see how to approach the problem outside of explicitly constructing an element $y_x$ s.t. $y_x \notin \bar{P}_x$  for arbitrary $x$. Any hints or pointers in the right direction would be much appreciated. Edit: Working in the $L^2(S^1)$ setting, the problem amounts to showing that $\forall f\in L^2(S^1)$ $\exists h \in L^2(S^1)$ such that $h$ cannot be approximated in $L^2$ norm by functions of the form $(\sum_{n=0}^m c_n cos^n(t))f(t).$ This seems like it should be clear, but I have been unable to prove it for general $f$.","Let $S$ be the bilateral shift on $\ell^2(\mathbb{Z})$ and let $T = S + S^*$. I want to show that there is no cyclic vector for the representation of $T$ on $\ell^2(\mathbb{Z})$ i.e. $\forall x\in \ell^2(\mathbb{Z})$ the set $P_x = \{f(x) : f\mbox{ is a polynomial in }T\}$ is not dense in $\ell^2(\mathbb{Z})$. I tried to start simple, showing that if $x = \delta_n$ for $n \in \mathbb{Z}$ then $P_x$ isn't dense. This worked out, but I was exploiting a symmetry that doesn't exist when considering finite linear combinations of such elements. I'm pretty sure this idea isn't going to work out. Then I tried considering translating this operator to an operator on $L^2(S^1)$ via the Fourier transform. The corresponding operator $\hat{T}$ is simply a multiplication operator, the multiplying function being $g(t) = 2\cos(t)$. This seemed more promising initially, but I still don't see how to approach the problem outside of explicitly constructing an element $y_x$ s.t. $y_x \notin \bar{P}_x$  for arbitrary $x$. Any hints or pointers in the right direction would be much appreciated. Edit: Working in the $L^2(S^1)$ setting, the problem amounts to showing that $\forall f\in L^2(S^1)$ $\exists h \in L^2(S^1)$ such that $h$ cannot be approximated in $L^2$ norm by functions of the form $(\sum_{n=0}^m c_n cos^n(t))f(t).$ This seems like it should be clear, but I have been unable to prove it for general $f$.",,"['functional-analysis', 'operator-theory', 'harmonic-analysis', 'spectral-theory']"
84,Wiener Lemma for matrix valued functions,Wiener Lemma for matrix valued functions,,"The ordinary Wiener lemma states that if $f(x):=\sum_{n \in \mathbb{Z}} a_n \exp(inx)$ ( $\sum |a_n|<\infty$ ) and if $f(x)\neq 0$ everywhere, then $g:=1/f$ can also be written as $$g(x)= \sum b_n \exp(inx)$$ with $\sum_n |b_n|<\infty$ The proof of this uses the fact, that an element $u$ of a commutative Banach Algebra is invertible iff $h(u)\neq 0$ for each complex homomorphism. Now, I have seen a similar result used in a paper - more specifically: substitute square- matrices in place of the coefficients $a_n$ , i.e. consider functions of the form $$f(x):= \sum A_n \exp(inx)$$ Assume now that each $f(x)$ is an invertible matrix. Does it then follow (and if so, why) that $f^{-1}(x)$ can be expressed as $$f^{-1}(x):= \sum B_n \exp(inx)$$ for some matrices satisfying $\sum |B_n|<\infty $ I don't know how to proceed here, because in this case the Banach Algebra is not commutative. Can I still adapt the proof of the original Wiener Lemma somehow?","The ordinary Wiener lemma states that if ( ) and if everywhere, then can also be written as with The proof of this uses the fact, that an element of a commutative Banach Algebra is invertible iff for each complex homomorphism. Now, I have seen a similar result used in a paper - more specifically: substitute square- matrices in place of the coefficients , i.e. consider functions of the form Assume now that each is an invertible matrix. Does it then follow (and if so, why) that can be expressed as for some matrices satisfying I don't know how to proceed here, because in this case the Banach Algebra is not commutative. Can I still adapt the proof of the original Wiener Lemma somehow?",f(x):=\sum_{n \in \mathbb{Z}} a_n \exp(inx) \sum |a_n|<\infty f(x)\neq 0 g:=1/f g(x)= \sum b_n \exp(inx) \sum_n |b_n|<\infty u h(u)\neq 0 a_n f(x):= \sum A_n \exp(inx) f(x) f^{-1}(x) f^{-1}(x):= \sum B_n \exp(inx) \sum |B_n|<\infty ,"['functional-analysis', 'banach-algebras', 'wiener-algebra']"
85,A technical lemma on linear combinations.,A technical lemma on linear combinations.,,"Lemma Let $\{x_1,\dots, x_n\}$ be a linearly independent set of vectors in a normed space $X$ . Then there is a number $c>0$ such that for every choice of scalars $\alpha_1,\dots,\alpha_n$ we have $$\lVert \alpha_1x_1+\cdots+\alpha_nx_n\rVert \ge c\left(\lvert\alpha_1\rvert+\dots+\lvert \alpha_n\rvert\right)\quad (c>0).\tag1$$ Comment: We all know that this is the notorious Lemma 2.4-1 of the book Introductory to Functional Analysis by Kreyszig. Before formalizing the question I searched the forum, but no answer fully satisfied my doubts, the answers I found were alternative proofs or not very exhaustive explanations. Please, I would like you to give me as detailed explanations as possible to the questions, evenif the are trivial, that I am going to ask you. I rewrite the proof and I hope that some of you can clarify my doubts also for those who will come after me. Proof We write $s=\lvert\alpha_1\rvert+\cdots+\lvert\alpha_n \rvert$ If $s=0$ , all $\alpha_j$ are zero, so $(1)$ holds for any $c$ . Let $s>0$ . Then $(1)$ is equivalent to the inequality which we obtain from $(1)$ by dividing by $s$ and writing $\beta_j=\alpha_j/s$ , that is, $$\tag2 \lVert\beta_1x_1+\cdots+\beta_n x_n\rVert\ge c\qquad \left( \sum_{j=1}^n \lvert\beta_j\rvert=1\right).$$ Hence it suffices to prove the existence of a $c>0$ such that $(2)$ holds for every $n-tuple$ of scalars $\beta_1,\dots, \beta_n$ with $\sum\lvert \beta_j \rvert=1$ . Suppose that this is false. Then there exists a sequence $\{y_m\}$ of vectors $$y_m=\beta_1^{(m)}x_1+\cdots+\beta_n^{(m)}x_n\qquad \left(\sum_{j=1}^n\lvert \beta_j^{(m)}\rvert=1\right)$$ such that $$\lVert y_m \rVert\to 0\quad\text{as}\; m\to \infty$$ Question 1 Why this fact deny the hypothesis of the existence of $c$ ? Now we reason as follows. Sice $\sum\lvert \beta_j^{(m)}\rvert=1$ , we have $\lvert\beta_j^{(m)}\rvert\le 1$ . Hence for each fixed $j$ the sequence $$\left(\beta_j^{(m)}\right)=\left(\beta_j^{(1)},\beta_j^{(2)},\dots\right)$$ is bounded. Consequently, by the Bolzano-Weierstrass theorem, $\left(\beta_1^{(m)}\right)$ has a converget subsequence. Let $\beta_1$ denote the limit of that subsequence, and let $\left(y_{1,m}\right)$ denote the corresponding subsequence, and let $\left(y_{1,m}\right)$ denote the corresponding subsequence of $\left( y_m\right)$ . By the same argument, $\left(y_{1,m}\right)$ has a subsequence $\left(y_{2,m}\right)$ for which the corresponding subsequence of scalars $\beta_2^{(m)}$ converges; let $\beta_2$ denote the limit. Continuing in this way, after $n$ steps we obtain a subsequence $(y_{n,m})=(y_{n,1},y_{n,2},\dots)$ of $(y_m)$ whose terms are of the form $$y_{n,m}=\sum_{j=1}^n\gamma^{(m)}x_j\qquad\left(\sum_{j=1}^n\lvert\gamma_j^{(m)}\rvert=1\right)$$ with scalars $\gamma_j^{(m)}$ satisfyng $\gamma_j^{(m)}\to\beta_j$ as $m\to \infty$ . Hence, as $m\to\infty,$ $$y_{n,m}\to y=\sum_{j=1}^n\beta_jx_j$$ where $\sum_\lvert\beta_j\rvert=1$ , so that not all $\beta_j$ can be zero. Question 2. I didn't understand the highlighted part above, could someone please explain the details to me?","Lemma Let be a linearly independent set of vectors in a normed space . Then there is a number such that for every choice of scalars we have Comment: We all know that this is the notorious Lemma 2.4-1 of the book Introductory to Functional Analysis by Kreyszig. Before formalizing the question I searched the forum, but no answer fully satisfied my doubts, the answers I found were alternative proofs or not very exhaustive explanations. Please, I would like you to give me as detailed explanations as possible to the questions, evenif the are trivial, that I am going to ask you. I rewrite the proof and I hope that some of you can clarify my doubts also for those who will come after me. Proof We write If , all are zero, so holds for any . Let . Then is equivalent to the inequality which we obtain from by dividing by and writing , that is, Hence it suffices to prove the existence of a such that holds for every of scalars with . Suppose that this is false. Then there exists a sequence of vectors such that Question 1 Why this fact deny the hypothesis of the existence of ? Now we reason as follows. Sice , we have . Hence for each fixed the sequence is bounded. Consequently, by the Bolzano-Weierstrass theorem, has a converget subsequence. Let denote the limit of that subsequence, and let denote the corresponding subsequence, and let denote the corresponding subsequence of . By the same argument, has a subsequence for which the corresponding subsequence of scalars converges; let denote the limit. Continuing in this way, after steps we obtain a subsequence of whose terms are of the form with scalars satisfyng as . Hence, as where , so that not all can be zero. Question 2. I didn't understand the highlighted part above, could someone please explain the details to me?","\{x_1,\dots, x_n\} X c>0 \alpha_1,\dots,\alpha_n \lVert \alpha_1x_1+\cdots+\alpha_nx_n\rVert \ge c\left(\lvert\alpha_1\rvert+\dots+\lvert \alpha_n\rvert\right)\quad (c>0).\tag1 s=\lvert\alpha_1\rvert+\cdots+\lvert\alpha_n \rvert s=0 \alpha_j (1) c s>0 (1) (1) s \beta_j=\alpha_j/s \tag2 \lVert\beta_1x_1+\cdots+\beta_n x_n\rVert\ge c\qquad \left( \sum_{j=1}^n \lvert\beta_j\rvert=1\right). c>0 (2) n-tuple \beta_1,\dots, \beta_n \sum\lvert \beta_j \rvert=1 \{y_m\} y_m=\beta_1^{(m)}x_1+\cdots+\beta_n^{(m)}x_n\qquad \left(\sum_{j=1}^n\lvert \beta_j^{(m)}\rvert=1\right) \lVert y_m \rVert\to 0\quad\text{as}\; m\to \infty c \sum\lvert \beta_j^{(m)}\rvert=1 \lvert\beta_j^{(m)}\rvert\le 1 j \left(\beta_j^{(m)}\right)=\left(\beta_j^{(1)},\beta_j^{(2)},\dots\right) \left(\beta_1^{(m)}\right) \beta_1 \left(y_{1,m}\right) \left(y_{1,m}\right) \left( y_m\right) \left(y_{1,m}\right) \left(y_{2,m}\right) \beta_2^{(m)} \beta_2 n (y_{n,m})=(y_{n,1},y_{n,2},\dots) (y_m) y_{n,m}=\sum_{j=1}^n\gamma^{(m)}x_j\qquad\left(\sum_{j=1}^n\lvert\gamma_j^{(m)}\rvert=1\right) \gamma_j^{(m)} \gamma_j^{(m)}\to\beta_j m\to \infty m\to\infty, y_{n,m}\to y=\sum_{j=1}^n\beta_jx_j \sum_\lvert\beta_j\rvert=1 \beta_j","['functional-analysis', 'proof-explanation']"
86,"Which ""difference"" functionals only depend on the margins?","Which ""difference"" functionals only depend on the margins?",,"Let the real-valued random variables $(X_1,X_2)$ have joint distribution $P_1\times P_2$ . Let the distribution of $X_1-X_2$ be $P_{\mathrm{diff}}$ . Let $\psi(P_{\mathrm{diff}})$ denote a functional of distribution of the difference. The functional is identified by the margins if $$\psi(P_{\mathrm{diff}})=\psi_m(P_1, P_2)$$ for some functional $\psi_m$ that only depends on the marginal distributions $P_1$ and $P_2$ . My question: What is the class of all functionals of the distribution of the difference which are identified by the margins? Clearly by the linearity of expectation, the expectation-of-difference functional $\psi(P_\mathrm{diff}) = \int_{\mathbb{R}^2} x_1 - x_2 d(P_1\times P_2)(x_1,x_2)$ is identified by the margins. I cannot find other examples or prove a general theorem.","Let the real-valued random variables have joint distribution . Let the distribution of be . Let denote a functional of distribution of the difference. The functional is identified by the margins if for some functional that only depends on the marginal distributions and . My question: What is the class of all functionals of the distribution of the difference which are identified by the margins? Clearly by the linearity of expectation, the expectation-of-difference functional is identified by the margins. I cannot find other examples or prove a general theorem.","(X_1,X_2) P_1\times P_2 X_1-X_2 P_{\mathrm{diff}} \psi(P_{\mathrm{diff}}) \psi(P_{\mathrm{diff}})=\psi_m(P_1, P_2) \psi_m P_1 P_2 \psi(P_\mathrm{diff}) = \int_{\mathbb{R}^2} x_1 - x_2 d(P_1\times P_2)(x_1,x_2)","['functional-analysis', 'probability-theory', 'statistics']"
87,Completeness of Besov spaces,Completeness of Besov spaces,,"Problem: Let us recall that: $$\dot{B}^{-\sigma}_{\infty,\infty}=\{u \in S'(\mathbb{R}^d): \|u\|_{\dot{B}^{-\sigma}_{\infty,\infty}} < \infty\}$$ where $$\|u\|_{\dot{B}^{-\sigma}_{\infty,\infty}}=\sup\limits_{A>0}\{A^{d-\sigma}\|  \theta(A\cdot)* u\|_{L^{\infty}}\}$$ and $S'(\mathbb{R}^d)$ is the set of tempered distributions. I want to understand two things: why $\dot{B}^{-\sigma}_{\infty,\infty}$ is complete (i.e. a Banach space) why if I choose another $\theta'$ I obtain an equivalent norm over $\dot{B}^{-\sigma}_{\infty,\infty}$ . Attempt. I tried exploiting the Fourier transorm of a tempered distribution: $$\theta(A\cdot)* u= \mathcal{F}^{-1}\mathcal{F}(\theta(A\cdot)* u)=\mathcal{F}^{-1}\bigg(A^{-d}\mathcal{F}\Big(\theta\Big(\frac{\cdot}{A}\Big)\Big) \mathcal{F}(u)\bigg)$$ but I cannot go on. Any help or reference will be appreciate. Please notice that I would like a self-contained proof instead one for general Besov spaces as $\dot{B}^{\sigma}_{p,q}$ . Edit : we assume $\mathcal{F}(\theta)$ is such that $\mathcal{F}(\theta) \in C^{\infty}_c(\mathbb{R}^d)$ , $0 \leq \mathcal{F}(\theta) \leq 1$ and $\mathcal{F}(\theta)=1$ in a neighborhood of $0$ .","Problem: Let us recall that: where and is the set of tempered distributions. I want to understand two things: why is complete (i.e. a Banach space) why if I choose another I obtain an equivalent norm over . Attempt. I tried exploiting the Fourier transorm of a tempered distribution: but I cannot go on. Any help or reference will be appreciate. Please notice that I would like a self-contained proof instead one for general Besov spaces as . Edit : we assume is such that , and in a neighborhood of .","\dot{B}^{-\sigma}_{\infty,\infty}=\{u \in S'(\mathbb{R}^d): \|u\|_{\dot{B}^{-\sigma}_{\infty,\infty}} < \infty\} \|u\|_{\dot{B}^{-\sigma}_{\infty,\infty}}=\sup\limits_{A>0}\{A^{d-\sigma}\|  \theta(A\cdot)* u\|_{L^{\infty}}\} S'(\mathbb{R}^d) \dot{B}^{-\sigma}_{\infty,\infty} \theta' \dot{B}^{-\sigma}_{\infty,\infty} \theta(A\cdot)* u= \mathcal{F}^{-1}\mathcal{F}(\theta(A\cdot)* u)=\mathcal{F}^{-1}\bigg(A^{-d}\mathcal{F}\Big(\theta\Big(\frac{\cdot}{A}\Big)\Big) \mathcal{F}(u)\bigg) \dot{B}^{\sigma}_{p,q} \mathcal{F}(\theta) \mathcal{F}(\theta) \in C^{\infty}_c(\mathbb{R}^d) 0 \leq \mathcal{F}(\theta) \leq 1 \mathcal{F}(\theta)=1 0","['functional-analysis', 'distribution-theory', 'besov-space']"
88,Prove that $\lim\limits_{r\rightarrow 1^-} rf(r)|f'(r)|=0$,Prove that,\lim\limits_{r\rightarrow 1^-} rf(r)|f'(r)|=0,"Let $f\in\mathcal{C}^0[0,1]\cap\mathcal{C}^2(0,1)$ and satisfy Boundary Condition: $f(0)=0=f(1)$ Positive: $f(r)>0,\quad r\in(0,1)$ Finite Energy: $\int_0^1\left(r(f')^2+\dfrac{f^2}{r}\right)dr<\infty$ I want to prove the 2nd problem. I include the first to show what I am able to do. $\lim\limits_{r\rightarrow 0^+} rf(r)|f'(r)|=0$ $\lim\limits_{r\rightarrow 1^-} rf(r)|f'(r)|=0$ . I am able to show the first one is true via a contradiction. However, the approach does not work for the second one. Here is the proof for the first one. Suppose 1 is not true, but the limit exists, i.e., $$\lim\limits_{r\rightarrow 0^+} rf(r)|f'(r)|=L>0.$$ By the definition of the limit, for any $\epsilon>0$ there is an $r_0\in(0,1)$ such that $$\big|rf(r)|f'(r)|-L\big|<\epsilon\quad\forall r\in(0,r_0).$$ We then have $$L-\epsilon<rf(r)|f'(r)|<L+\epsilon\quad\forall r\in(0,r_0).$$ Now, we can choose $\epsilon=\dfrac{L}{2}>0$ to get $$\dfrac{L}{2}<rf(r)|f'(r)|\quad\forall r\in(0,r_0).$$ Then dividing by $r$ and integrating on $(0,r_0)$ , we arrive at $\infty=\int_0^{r_0}\dfrac{L}{2r}dr\leq\int_0^{r_0}f(r)|f'(r)|dr\leq\left(\int_0^{r_0}\dfrac{(f(r))^2}{r}dr\right)^{1/2}\left(\int_0^{r_0}r(f')^2dr\right)^{1/2}<\infty$ , a contradiction to the finite energy condition. If I approach the second one via a contradiction, it is clear that $f'(r)\rightarrow\infty$ as $r\rightarrow 1$ and, consequently, should have a contradiction to the finite energy condition. But, I am unable to establish rigorous proof.","Let and satisfy Boundary Condition: Positive: Finite Energy: I want to prove the 2nd problem. I include the first to show what I am able to do. . I am able to show the first one is true via a contradiction. However, the approach does not work for the second one. Here is the proof for the first one. Suppose 1 is not true, but the limit exists, i.e., By the definition of the limit, for any there is an such that We then have Now, we can choose to get Then dividing by and integrating on , we arrive at , a contradiction to the finite energy condition. If I approach the second one via a contradiction, it is clear that as and, consequently, should have a contradiction to the finite energy condition. But, I am unable to establish rigorous proof.","f\in\mathcal{C}^0[0,1]\cap\mathcal{C}^2(0,1) f(0)=0=f(1) f(r)>0,\quad r\in(0,1) \int_0^1\left(r(f')^2+\dfrac{f^2}{r}\right)dr<\infty \lim\limits_{r\rightarrow 0^+} rf(r)|f'(r)|=0 \lim\limits_{r\rightarrow 1^-} rf(r)|f'(r)|=0 \lim\limits_{r\rightarrow 0^+} rf(r)|f'(r)|=L>0. \epsilon>0 r_0\in(0,1) \big|rf(r)|f'(r)|-L\big|<\epsilon\quad\forall r\in(0,r_0). L-\epsilon<rf(r)|f'(r)|<L+\epsilon\quad\forall r\in(0,r_0). \epsilon=\dfrac{L}{2}>0 \dfrac{L}{2}<rf(r)|f'(r)|\quad\forall r\in(0,r_0). r (0,r_0) \infty=\int_0^{r_0}\dfrac{L}{2r}dr\leq\int_0^{r_0}f(r)|f'(r)|dr\leq\left(\int_0^{r_0}\dfrac{(f(r))^2}{r}dr\right)^{1/2}\left(\int_0^{r_0}r(f')^2dr\right)^{1/2}<\infty f'(r)\rightarrow\infty r\rightarrow 1","['functional-analysis', 'analysis']"
89,"Basic workings of probability theory on $\mathcal{C}([0,1])$",Basic workings of probability theory on,"\mathcal{C}([0,1])","I am confused as to how exactly the underlying probablity space $(\Omega,\mathscr{F},\mathbb{P})$ relates to a probability measure $\mu:\mathcal{B}(\mathcal{C})\longrightarrow[0,1]$ where $\mathcal{C}:=\mathcal{C}([0,1])$ is the class of continuous functions $f:[0,1]\longrightarrow\mathbb{R}$ and $\mathcal{B}(\mathcal{C})$ is the Borel $\sigma$ -field of subsets of $\mathcal{C}$ . As per the excellent question and answer here Natural & important probability measures on $\mathcal{C}[0,1]$, in particular the Wiener measure , I have a natural preference to avoid, at least initially, the stochastic process viewpoint. However in case this is relevant to my questions, my understanding is as follows: for each $f\in\mathcal{C}$ and for fixed $t\in[0,1]$ , $f_{t}(\omega):\Omega\longrightarrow\mathbb{R}$ is a random variable as $\omega$ runs through $\Omega$ if $f$ is $\mathscr{F}/\mathscr{R}$ measurable ( $\mathscr{R}$ being the linear Borel sets). I think each $f$ are measurable since continuity implies measurability. On the other hand for fixed $\omega\in \Omega$ , $f_{t}(\omega):[0,1]\longrightarrow\mathbb{R}$ is a path as $t$ runs through $[0,1]$ , i.e. a realisation of a real valued function on $[0,1]$ . Thus using the dot notation to signify a variable, $f_{t}(\cdot)$ is a random variable at $t$ and $f_{(\cdot)}(\omega)$ is the entire path of $f$ at $\omega$ . Furthermore $(f_{t}(\omega))_{\omega,t}:=(f_{t}(\omega))_{\omega\in\Omega,t\in[0,1]}$ is a stochastic process $f:\Omega\times[0,1]\longrightarrow\mathbb{R}$ . Random elements in $\mathcal{C}$ I now, perhaps naively, apply my knowledge of how the basics of probability theory work for $X$ - i.e. a random variable in a standard probability set-up) Suppose $\tilde{F}:\Omega\longrightarrow\mathcal{C}$ is a $\mathscr{F}/\mathcal{C}$ -measurable mapping between the measurbale spaces $(\Omega,\mathscr{F})$ and $(\mathcal{C},\mathcal{B}(\mathcal{C}))$ , i.e. for each $\omega\in\Omega$ , $\tilde{F}(\omega)\mapsto \tilde{f}$ for some $\tilde{f}\in\mathcal{C}$ . Then $\tilde{F}$ is a random element in $\mathcal{C}$ and is analagous to a $\mathscr{F}/\mathscr{R}$ -measurable mapping $X(\omega)\mapsto x$ for some $x\in\mathbb{R}$ - i.e. a standard random variable. According to Billinglsey (Convergence of Probability Measures, 2nd Ed, 1999) elements can mean scalars, vectors, sequences as well, and I am trying to follow this general language so I understand probability theory applied to metric spaces (of which $(\mathcal{C},d_{\infty})$ where $d_{\infty}$ is the sup metric is the most general I wll probably ever have the need to consider). The open $d_{\infty}$ -balls $B_{\epsilon}(f)=\{g\in\mathcal{C}:d_{\infty}(f,g)<\epsilon\}$ centred at $f$ of radius $\epsilon$ are conceptually the same as in Euclidean space: i.e. the class of $d_{\infty}$ -open sets of $\mathcal{C}$ denoted $\mathcal{U}(\mathcal{C})$ satisfy each $U\in \mathcal{U}(\mathcal{C})$ being a union of open balls, and the metric space $(\mathcal{C},d_{\infty})$ is separable and complete. The Borel $\sigma$ -field $\mathcal{C}$ is crucially also generated by $\mathcal{U}(\mathcal{C})$ (this is crucial in the sense that oftentimes proving a result on the generating class then extends to the entire $\sigma$ -field). However here I do not need to use such properties here. Let $\require{enclose}      \enclose{horizontalstrike}{\mu_{\tilde{F}}:\mathcal{C}\longrightarrow[0,1]}$ $\mu_{\tilde{F}}:\mathcal{B}(\mathcal{C})\longrightarrow[0,1]$ be the probability law of $\tilde{F}$ : what does this mean and how does $\mu_{\tilde{F}}$ assign probabilities to elements in $\mathcal{C}$ ?; $$\mu_{\tilde{F}}(B)=\mathbb{P}\left\{\omega\in\Omega:\tilde{F}(\omega)\in B\right\}\hspace{20pt}\text{ for all }B\in\mathcal{B}(\mathcal{C})$$ In words: $B$ is a set of continous real-valued functions on $[0,1]$ and $\tilde{F}(\omega)\in B$ means the entire path $\tilde{F}(\omega):=\tilde{f}_{(\cdot)}(\omega)$ at $\omega$ lies in $B$ . The above often gets shortened to $\mu_{\tilde{F}}(B)=\mathbb{P}\{\tilde{F}\in B\}=\mathbb{P}\{\tilde{F}^{-1}(B)\}$ . Measurable mappings and intergals of Measurable mappings of random elements in $\mathcal{C}$ If my understanding about random elements is OK, how are measurable mappings of these random elements defined, and furthermore how are intergals of these mappings of elements defined? Once again I now perhaps naively apply my knowledge of how this works for $X$ ) . Let $\phi$ be a positive-valued $\mathcal{C}/\mathscr{R}$ -measurable function (i.e. function of functions such as $\phi(\tilde{f})\mapsto\text{max}\left\{|\tilde{f}_{t}|:t\in[0,1]\right\}$ ). Then $\phi(\tilde{F})=\phi\circ \tilde{F}$ is a $\mathscr{F}/\mathscr{R}$ -measurable mapping (theorem 13.1 (ii), Billingsley 1995, Proabability and Measure, 3rd Ed), and as far as I can tell, I am free now to apply standard measure theory pertaining to the integral of random variables to this mapping - i.e. no new theory of integration required (Billingsley 1995 uses a quite general set-up so perhaps here I am benefiting from investing in this extra generality). Using the defintion of expectation of random variables, the change of variable theorem (lines 5 and 6), and the definition of the integral for positive mappings; $$\begin{align*} \require{enclose}      \enclose{horizontalstrike}{\mathbb{E}_{\mu_{\tilde{F}}}\left[\phi(\tilde{F})\right] }\mathbb{E}_{\mu_{\tilde{F}}}\left[\phi\right]&:=\int_{\mathcal{C}}\phi(g)\mu_{\tilde{F}}(dg)\\ &:=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mu_{\tilde{F}}(B_{i})\right\}\\ &=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mathbb{P}\left[\omega:\tilde{F}(\omega)\in B_{i}\right]\right\}\\ &=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mathbb{P}\left[\tilde{F}^{-1}(B_{i})\right]\right\}\\ &=\int_{\mathcal{C}}\phi(g)\mathbb{P}\circ\tilde{F}^{-1}(dg)\\ &=\int_{\Omega}\phi(\tilde{F}(\omega))\mathbb{P}(d\omega)\\ &:=\text{sup}\sum_{i}\left\{\left[\underset{\omega\in F_{i}}{\text{inf}}\phi(\tilde{F}(\omega))\right]\mathbb{P}(F_{i})\right\}\\ &:=\mathbb{E}_{\mathbb{P}}\left[\phi(\tilde{F})\right], \end{align*}$$ where the supremums extend over all finite decompositions $\{B_{i}\}$ of $\mathcal{C}$ into $\mathcal{B}(\mathcal{C})$ -sets and all finite decompositions $\{F_{i}\}$ of $\Omega$ into $\mathscr{F}$ -sets. This all seems perfectly reasonable to me but please do point out where I have gone wrong.","I am confused as to how exactly the underlying probablity space relates to a probability measure where is the class of continuous functions and is the Borel -field of subsets of . As per the excellent question and answer here Natural & important probability measures on $\mathcal{C}[0,1]$, in particular the Wiener measure , I have a natural preference to avoid, at least initially, the stochastic process viewpoint. However in case this is relevant to my questions, my understanding is as follows: for each and for fixed , is a random variable as runs through if is measurable ( being the linear Borel sets). I think each are measurable since continuity implies measurability. On the other hand for fixed , is a path as runs through , i.e. a realisation of a real valued function on . Thus using the dot notation to signify a variable, is a random variable at and is the entire path of at . Furthermore is a stochastic process . Random elements in I now, perhaps naively, apply my knowledge of how the basics of probability theory work for - i.e. a random variable in a standard probability set-up) Suppose is a -measurable mapping between the measurbale spaces and , i.e. for each , for some . Then is a random element in and is analagous to a -measurable mapping for some - i.e. a standard random variable. According to Billinglsey (Convergence of Probability Measures, 2nd Ed, 1999) elements can mean scalars, vectors, sequences as well, and I am trying to follow this general language so I understand probability theory applied to metric spaces (of which where is the sup metric is the most general I wll probably ever have the need to consider). The open -balls centred at of radius are conceptually the same as in Euclidean space: i.e. the class of -open sets of denoted satisfy each being a union of open balls, and the metric space is separable and complete. The Borel -field is crucially also generated by (this is crucial in the sense that oftentimes proving a result on the generating class then extends to the entire -field). However here I do not need to use such properties here. Let be the probability law of : what does this mean and how does assign probabilities to elements in ?; In words: is a set of continous real-valued functions on and means the entire path at lies in . The above often gets shortened to . Measurable mappings and intergals of Measurable mappings of random elements in If my understanding about random elements is OK, how are measurable mappings of these random elements defined, and furthermore how are intergals of these mappings of elements defined? Once again I now perhaps naively apply my knowledge of how this works for ) . Let be a positive-valued -measurable function (i.e. function of functions such as ). Then is a -measurable mapping (theorem 13.1 (ii), Billingsley 1995, Proabability and Measure, 3rd Ed), and as far as I can tell, I am free now to apply standard measure theory pertaining to the integral of random variables to this mapping - i.e. no new theory of integration required (Billingsley 1995 uses a quite general set-up so perhaps here I am benefiting from investing in this extra generality). Using the defintion of expectation of random variables, the change of variable theorem (lines 5 and 6), and the definition of the integral for positive mappings; where the supremums extend over all finite decompositions of into -sets and all finite decompositions of into -sets. This all seems perfectly reasonable to me but please do point out where I have gone wrong.","(\Omega,\mathscr{F},\mathbb{P}) \mu:\mathcal{B}(\mathcal{C})\longrightarrow[0,1] \mathcal{C}:=\mathcal{C}([0,1]) f:[0,1]\longrightarrow\mathbb{R} \mathcal{B}(\mathcal{C}) \sigma \mathcal{C} f\in\mathcal{C} t\in[0,1] f_{t}(\omega):\Omega\longrightarrow\mathbb{R} \omega \Omega f \mathscr{F}/\mathscr{R} \mathscr{R} f \omega\in \Omega f_{t}(\omega):[0,1]\longrightarrow\mathbb{R} t [0,1] [0,1] f_{t}(\cdot) t f_{(\cdot)}(\omega) f \omega (f_{t}(\omega))_{\omega,t}:=(f_{t}(\omega))_{\omega\in\Omega,t\in[0,1]} f:\Omega\times[0,1]\longrightarrow\mathbb{R} \mathcal{C} X \tilde{F}:\Omega\longrightarrow\mathcal{C} \mathscr{F}/\mathcal{C} (\Omega,\mathscr{F}) (\mathcal{C},\mathcal{B}(\mathcal{C})) \omega\in\Omega \tilde{F}(\omega)\mapsto \tilde{f} \tilde{f}\in\mathcal{C} \tilde{F} \mathcal{C} \mathscr{F}/\mathscr{R} X(\omega)\mapsto x x\in\mathbb{R} (\mathcal{C},d_{\infty}) d_{\infty} d_{\infty} B_{\epsilon}(f)=\{g\in\mathcal{C}:d_{\infty}(f,g)<\epsilon\} f \epsilon d_{\infty} \mathcal{C} \mathcal{U}(\mathcal{C}) U\in \mathcal{U}(\mathcal{C}) (\mathcal{C},d_{\infty}) \sigma \mathcal{C} \mathcal{U}(\mathcal{C}) \sigma \require{enclose}
     \enclose{horizontalstrike}{\mu_{\tilde{F}}:\mathcal{C}\longrightarrow[0,1]} \mu_{\tilde{F}}:\mathcal{B}(\mathcal{C})\longrightarrow[0,1] \tilde{F} \mu_{\tilde{F}} \mathcal{C} \mu_{\tilde{F}}(B)=\mathbb{P}\left\{\omega\in\Omega:\tilde{F}(\omega)\in B\right\}\hspace{20pt}\text{ for all }B\in\mathcal{B}(\mathcal{C}) B [0,1] \tilde{F}(\omega)\in B \tilde{F}(\omega):=\tilde{f}_{(\cdot)}(\omega) \omega B \mu_{\tilde{F}}(B)=\mathbb{P}\{\tilde{F}\in B\}=\mathbb{P}\{\tilde{F}^{-1}(B)\} \mathcal{C} X \phi \mathcal{C}/\mathscr{R} \phi(\tilde{f})\mapsto\text{max}\left\{|\tilde{f}_{t}|:t\in[0,1]\right\} \phi(\tilde{F})=\phi\circ \tilde{F} \mathscr{F}/\mathscr{R} \begin{align*}
\require{enclose}
     \enclose{horizontalstrike}{\mathbb{E}_{\mu_{\tilde{F}}}\left[\phi(\tilde{F})\right]
}\mathbb{E}_{\mu_{\tilde{F}}}\left[\phi\right]&:=\int_{\mathcal{C}}\phi(g)\mu_{\tilde{F}}(dg)\\
&:=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mu_{\tilde{F}}(B_{i})\right\}\\
&=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mathbb{P}\left[\omega:\tilde{F}(\omega)\in B_{i}\right]\right\}\\
&=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mathbb{P}\left[\tilde{F}^{-1}(B_{i})\right]\right\}\\
&=\int_{\mathcal{C}}\phi(g)\mathbb{P}\circ\tilde{F}^{-1}(dg)\\
&=\int_{\Omega}\phi(\tilde{F}(\omega))\mathbb{P}(d\omega)\\
&:=\text{sup}\sum_{i}\left\{\left[\underset{\omega\in F_{i}}{\text{inf}}\phi(\tilde{F}(\omega))\right]\mathbb{P}(F_{i})\right\}\\
&:=\mathbb{E}_{\mathbb{P}}\left[\phi(\tilde{F})\right],
\end{align*} \{B_{i}\} \mathcal{C} \mathcal{B}(\mathcal{C}) \{F_{i}\} \Omega \mathscr{F}","['functional-analysis', 'probability-theory', 'measure-theory', 'metric-spaces', 'borel-measures']"
90,Invertibility in the Complex Group Ring,Invertibility in the Complex Group Ring,,"Let $\Gamma$ be a discrete group, and let $\Bbb{C}[\Gamma]$ be the associated complex group ring. If $\sum_{g \in \Gamma} a_g g$ represents some element in the group ring, where all but finitely many of the $a_g \in \Bbb{C}$ are nonzero, then we can equip $\Bbb{C}[\Gamma]$ with the follow natural involution: $$\left(\sum_{g \in \Gamma} a_g g \right)^* := \sum_{g \in \Gamma} \overline{a_g} g^{-1}$$ This gives us a notion of self-adjoint elemnts ( $f^* = f$ for $f \in \Bbb{C}[\Gamma]$ ) and positive elements (those of the form $\sum h_i^* h_i$ for $h_i \in \Bbb{C}[\Gamma]$ ). From here we can get an ordering on the self-adjoint elements: $f \le g$ if and only if $g - f$ is positive. I am wondering whether the following is true: Let $f \in \Bbb{C}[\Gamma]$ be a positive element. Then $f$ is invertible in $\Bbb{C}[\Gamma]$ if and only if there exists $\epsilon > 0$ such that $f \ge \epsilon 1$ . I know this is true for $C^*$ -algebras, and I suspect that the proof just amounts to $C^*$ -algebraic tricks. For example, the forward direction is trivial, because if $f$ is invertible in $\Bbb{C}[\Gamma]$ , then it is invertible in the full group $C^*$ -algebra $C^*(\Gamma)$ , which is just the completion of $\Bbb{C}[\Gamma]$ with respect to a particular norm (see this ). However, the other direction is not obvious. If $f \ge \epsilon 1$ for some $\epsilon > 0$ , then $f$ is invertible in $C^*(\Gamma)$ . But why must its inverse live in $\Bbb{C}[\Gamma]$ ? My gut tells me this is true, but I don't see it at the moment.","Let be a discrete group, and let be the associated complex group ring. If represents some element in the group ring, where all but finitely many of the are nonzero, then we can equip with the follow natural involution: This gives us a notion of self-adjoint elemnts ( for ) and positive elements (those of the form for ). From here we can get an ordering on the self-adjoint elements: if and only if is positive. I am wondering whether the following is true: Let be a positive element. Then is invertible in if and only if there exists such that . I know this is true for -algebras, and I suspect that the proof just amounts to -algebraic tricks. For example, the forward direction is trivial, because if is invertible in , then it is invertible in the full group -algebra , which is just the completion of with respect to a particular norm (see this ). However, the other direction is not obvious. If for some , then is invertible in . But why must its inverse live in ? My gut tells me this is true, but I don't see it at the moment.",\Gamma \Bbb{C}[\Gamma] \sum_{g \in \Gamma} a_g g a_g \in \Bbb{C} \Bbb{C}[\Gamma] \left(\sum_{g \in \Gamma} a_g g \right)^* := \sum_{g \in \Gamma} \overline{a_g} g^{-1} f^* = f f \in \Bbb{C}[\Gamma] \sum h_i^* h_i h_i \in \Bbb{C}[\Gamma] f \le g g - f f \in \Bbb{C}[\Gamma] f \Bbb{C}[\Gamma] \epsilon > 0 f \ge \epsilon 1 C^* C^* f \Bbb{C}[\Gamma] C^* C^*(\Gamma) \Bbb{C}[\Gamma] f \ge \epsilon 1 \epsilon > 0 f C^*(\Gamma) \Bbb{C}[\Gamma],"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'group-rings']"
91,Operator core of Dirichlet Laplacian,Operator core of Dirichlet Laplacian,,"I am reading Reed-Simon Vol. 4, page 274. We have the following situation: Let $\Omega \subset \mathbb R^m$ be an $m$ -dimensional cube and consider the Dirichlet-Laplacian $-\Delta_D=\Delta_D^\Omega$ with quadratic form domain $H^1_0(\Omega)$ . $\textbf{Proposition. }$ The set $D_D := \{f: f\in C^\infty(\Omega),  f {\restriction}_ {\partial\Omega} = 0 \}$ is an operator core for $-\Delta_D$ and for all $f\in D_D$ , $$-\Delta_D f = -\sum_{i=1}^n \frac{\partial^2}{\partial x_i^2} f.$$ In the proof of this proposition they state the following: Let $A$ be the operator $-\Delta$ with domain $D_D$ . $A$ is clearly symmetric, and by separation of variables (multiple Fourier series) we can find a complete orthonormal basis of eigenfunctions $\{ \varphi_n \}$ for A. If $A\varphi_n = \lambda_n \varphi_n$ , it is easy to see that $\varphi \in D(\overline{A})$ if and only if $\sum \lambda_n^2 \lvert (\varphi_n, \varphi) \rvert^2 < \infty$ ; and from this we conclude that $\overline{A}$ is self-adjoint. Can someone explain this paragraph to me? Two pages later they explicitly give the eigenbasis so I'll count that as an explanation, but the next thing is what I don't see so easily. Why does the ""if and only if"" hold and why can one conclude that $\overline{A}$ is self-adjoint? Book excerpt (thanks to Keith McClary).","I am reading Reed-Simon Vol. 4, page 274. We have the following situation: Let be an -dimensional cube and consider the Dirichlet-Laplacian with quadratic form domain . The set is an operator core for and for all , In the proof of this proposition they state the following: Let be the operator with domain . is clearly symmetric, and by separation of variables (multiple Fourier series) we can find a complete orthonormal basis of eigenfunctions for A. If , it is easy to see that if and only if ; and from this we conclude that is self-adjoint. Can someone explain this paragraph to me? Two pages later they explicitly give the eigenbasis so I'll count that as an explanation, but the next thing is what I don't see so easily. Why does the ""if and only if"" hold and why can one conclude that is self-adjoint? Book excerpt (thanks to Keith McClary).","\Omega \subset \mathbb R^m m -\Delta_D=\Delta_D^\Omega H^1_0(\Omega) \textbf{Proposition. } D_D := \{f: f\in C^\infty(\Omega),  f {\restriction}_ {\partial\Omega} = 0 \} -\Delta_D f\in D_D -\Delta_D f = -\sum_{i=1}^n \frac{\partial^2}{\partial x_i^2} f. A -\Delta D_D A \{ \varphi_n \} A\varphi_n = \lambda_n \varphi_n \varphi \in D(\overline{A}) \sum \lambda_n^2 \lvert (\varphi_n, \varphi) \rvert^2 < \infty \overline{A} \overline{A}","['functional-analysis', 'partial-differential-equations', 'operator-theory', 'spectral-theory']"
92,Question Regarding the Standard Picture of $K_{0}(A)$ for a C$^{*}$-Algebra $A$,Question Regarding the Standard Picture of  for a C-Algebra,K_{0}(A) ^{*} A,"I am working through Rordam's book and I am stuck on exercise 4.4. Suppose we have a (not necessarily unital) C$^{*}$-algebra $A$. We are asked to prove that every element of $K_{0}(A)$ can be written as $$ [p]_{0}-\left[\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}\right]_{0}, $$  for some projection $p\in M_{2n}(\widetilde{A})$ satisfying $$ p-\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}\in M_{2n}(A). $$ Following one of the proofs in the book (a quarter-way down page 64), I took an arbitrary $g\in K_{0}(A)$ and expressed it as $[e]_{0}-[f]_{0}$ for some projections $e,f\in M_{n}(\widetilde{A})$. Then, I put  $$ p=\begin{pmatrix}1_{n}-f & 0\\ 0 & e\end{pmatrix}\qquad\text{and}\qquad q=\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}. $$ As in the proof in the book, it follows that $g=[p]_{0}-[q]_{0}$ and that  $$ [s(p)]_{0}=[q]_{0}, $$ where $s\colon\widetilde{A}\to\widetilde{A}$ is the map given by $s(a+\lambda\cdot1_{\widetilde{A}})=\lambda\cdot 1_{\widetilde{A}}$. So, we have $g=[p]_{0}-[s(p)]_{0}$, but to guarantee that  $p-\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}\in M_{2n}(A)$, I need $s(p)=q$, which obviously does not hold in general. I've been thinking for hours how to modify this construction to produce the result, but I haven't been able to come up with anything else. Any help is really appreciated. Thank you.","I am working through Rordam's book and I am stuck on exercise 4.4. Suppose we have a (not necessarily unital) C$^{*}$-algebra $A$. We are asked to prove that every element of $K_{0}(A)$ can be written as $$ [p]_{0}-\left[\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}\right]_{0}, $$  for some projection $p\in M_{2n}(\widetilde{A})$ satisfying $$ p-\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}\in M_{2n}(A). $$ Following one of the proofs in the book (a quarter-way down page 64), I took an arbitrary $g\in K_{0}(A)$ and expressed it as $[e]_{0}-[f]_{0}$ for some projections $e,f\in M_{n}(\widetilde{A})$. Then, I put  $$ p=\begin{pmatrix}1_{n}-f & 0\\ 0 & e\end{pmatrix}\qquad\text{and}\qquad q=\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}. $$ As in the proof in the book, it follows that $g=[p]_{0}-[q]_{0}$ and that  $$ [s(p)]_{0}=[q]_{0}, $$ where $s\colon\widetilde{A}\to\widetilde{A}$ is the map given by $s(a+\lambda\cdot1_{\widetilde{A}})=\lambda\cdot 1_{\widetilde{A}}$. So, we have $g=[p]_{0}-[s(p)]_{0}$, but to guarantee that  $p-\begin{pmatrix}1_{n} & 0_{n}\\ 0_{n} & 0_{n}\end{pmatrix}\in M_{2n}(A)$, I need $s(p)=q$, which obviously does not hold in general. I've been thinking for hours how to modify this construction to produce the result, but I haven't been able to come up with anything else. Any help is really appreciated. Thank you.",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'k-theory']"
93,How to show Plancherel's Theorem for Fourier Transform implies $L^2$ Transform Convergence.,How to show Plancherel's Theorem for Fourier Transform implies  Transform Convergence.,L^2,"The Plancherel Theorem for the Fourier transform $\hat{f}(s)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(t)e^{-ist}dt$ on $\mathbb{R}$ states that $$            \int_{-\infty}^{\infty}|\hat{f}(s)|^2ds = \int_{-\infty}^{\infty}|f(t)|^2dt,\;\;\;\; f \in L^2(\mathbb{R})\cap L^1(\mathbb{R}). $$ Without appealing to classical convergence results, can this result be used to show that $$      \lim_{u,v\rightarrow\infty}\left\|\frac{1}{\sqrt{2\pi}}\int_{-u}^{v}\hat{f}(s)e^{isx}ds-f\right\|_{L^2(\mathbb{R})} = 0. \;\;\; ? $$ Does anyone know of a nice way to show this? Background: This is not a homework problem or something I found in a text. It would seem reasonable to expect this result because of the discrete version where Parseval's equality for the Fourier series implies $L^2$ convergence of the Fourier series; or, if $\{ e_n \}_{n=1}^{\infty}$ is an orthonormal set in an inner product space, then  $$\sum_{n=1}^{\infty}|(f,e_n)|^2=\|f\|^2 \iff \lim_{N\rightarrow\infty} \left\|\sum_{n=1}^{N}(f,e_n)e_n -f \right\|=0. $$","The Plancherel Theorem for the Fourier transform $\hat{f}(s)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f(t)e^{-ist}dt$ on $\mathbb{R}$ states that $$            \int_{-\infty}^{\infty}|\hat{f}(s)|^2ds = \int_{-\infty}^{\infty}|f(t)|^2dt,\;\;\;\; f \in L^2(\mathbb{R})\cap L^1(\mathbb{R}). $$ Without appealing to classical convergence results, can this result be used to show that $$      \lim_{u,v\rightarrow\infty}\left\|\frac{1}{\sqrt{2\pi}}\int_{-u}^{v}\hat{f}(s)e^{isx}ds-f\right\|_{L^2(\mathbb{R})} = 0. \;\;\; ? $$ Does anyone know of a nice way to show this? Background: This is not a homework problem or something I found in a text. It would seem reasonable to expect this result because of the discrete version where Parseval's equality for the Fourier series implies $L^2$ convergence of the Fourier series; or, if $\{ e_n \}_{n=1}^{\infty}$ is an orthonormal set in an inner product space, then  $$\sum_{n=1}^{\infty}|(f,e_n)|^2=\|f\|^2 \iff \lim_{N\rightarrow\infty} \left\|\sum_{n=1}^{N}(f,e_n)e_n -f \right\|=0. $$",,"['functional-analysis', 'fourier-analysis', 'fourier-transform']"
94,Proving that scalar multiplication is continuous,Proving that scalar multiplication is continuous,,"Let $\mathbb{K} \in \{ \mathbb{R} , \mathbb{C} \}$ and $s= \mathbb{K}^{\omega}$ be the usual sequence set with entries on $\mathbb{K}$. I proved that $\mathbb{K}$ induces a $\mathbb{K}$-vector space structure on $s$ and that the function $\rho : s \times s \to \mathbb{R}$ given by $$\displaystyle \rho(x,y) = \sum_{n=1}^{\infty} \frac{1}{2^n} \frac{|x_n - y_n|}{(1+|x_n - y_n|)} \ \ , $$ for all $x = (x_n) \in s$ and all $y = (y_n) \in s$, is a translation-invariant metric on $s$. Then I proved that addition $A: (s \times s , \tau_*) \to (s,\tau)$ is continuous, where $\tau$ is the topology on $s$ induced by $\rho$ and  $\tau_*$ is the product topology of $(s,\tau)$ with itself. For that purpose I used the metric $\sigma : (s \times s) \times (s \times s) \to \mathbb{R}$ such that $\sigma \big( (x,y) , (a,b) \big) = \rho(x,a) + \rho(y,b)$, $\forall x,y,a,b \in s$, which induces $\tau_*$ on $s \times s$. Then I was trying to prove that the scalar multiplication $M: (\mathbb{K} \times s , \tilde{\tau}) \to (s, \tau)$ is continuous and I failed, where $\tilde{\tau}$ is the product topology on $\mathbb{K} \times s$ of $(\mathbb{K}, \eta)$ with $(s,\tau)$ and $\eta$ is the topology on $\mathbb{K}$ induced by the norm given by the absolute value function $| \cdot | : \mathbb{K} \to \mathbb{R}$. I don't know which metric to consider and I don't know how to do it. I need help. Any hint is appreciated. Thanks in advance.","Let $\mathbb{K} \in \{ \mathbb{R} , \mathbb{C} \}$ and $s= \mathbb{K}^{\omega}$ be the usual sequence set with entries on $\mathbb{K}$. I proved that $\mathbb{K}$ induces a $\mathbb{K}$-vector space structure on $s$ and that the function $\rho : s \times s \to \mathbb{R}$ given by $$\displaystyle \rho(x,y) = \sum_{n=1}^{\infty} \frac{1}{2^n} \frac{|x_n - y_n|}{(1+|x_n - y_n|)} \ \ , $$ for all $x = (x_n) \in s$ and all $y = (y_n) \in s$, is a translation-invariant metric on $s$. Then I proved that addition $A: (s \times s , \tau_*) \to (s,\tau)$ is continuous, where $\tau$ is the topology on $s$ induced by $\rho$ and  $\tau_*$ is the product topology of $(s,\tau)$ with itself. For that purpose I used the metric $\sigma : (s \times s) \times (s \times s) \to \mathbb{R}$ such that $\sigma \big( (x,y) , (a,b) \big) = \rho(x,a) + \rho(y,b)$, $\forall x,y,a,b \in s$, which induces $\tau_*$ on $s \times s$. Then I was trying to prove that the scalar multiplication $M: (\mathbb{K} \times s , \tilde{\tau}) \to (s, \tau)$ is continuous and I failed, where $\tilde{\tau}$ is the product topology on $\mathbb{K} \times s$ of $(\mathbb{K}, \eta)$ with $(s,\tau)$ and $\eta$ is the topology on $\mathbb{K}$ induced by the norm given by the absolute value function $| \cdot | : \mathbb{K} \to \mathbb{R}$. I don't know which metric to consider and I don't know how to do it. I need help. Any hint is appreciated. Thanks in advance.",,"['functional-analysis', 'topological-vector-spaces']"
95,two generalizations of Lax-Milgram theorem,two generalizations of Lax-Milgram theorem,,"Apart from the classic Lax-Milgram theorem which is powerful under the Hilbert setting, there are two generalizations: (from Wikipedia) Babuska-Lax-Milgram theorem suppose $U,V$ Hilbert spaces (real-valued), $B: U\times V\rightarrow \mathbb{R}$ a continuous bilinear form, if $B$ is weakly coercive: for some constant $c>0$, $\forall u\in U$, $$ \sup_{||v||=1}B(u,v)\geq c||u|| $$ and for all $0\neq v\in V$ $$ \sup_{||u||=1}|B(u,v)|>0 $$ then for $f\in V^\ast$, there exists an unique solution $u_f\in U$ to the weak formulation: $$ B(u_f,v)=(f,v),\quad \forall v\in V $$ moreover, the solution depends continuously on the datum: $$ ||u_f||\leq\frac{1}{c}||f|| $$ Lions-Lax-Milgram theorem Let $H$ a Hilbert space and $V$ a normed space, $B:H\times V\rightarrow \mathbb{R}$ a continuous bilinear form, the following are equivalent: for constant $c>0$, $$ \inf_{||v||_V=1}\sup_{||h||_H\leq 1}|B(h,v)|\geq c; $$ for each continuous linear functional $f\in V^\ast$, there exists $h\in H$ s.t. $$ B(h,v)=(f,v)\quad \forall v\in V. $$ I am thinking are there any specific reason or problem to apply these two theorem? In Wikipedia, it says the second theorem may be useful for problem with time-dependent boundary where the classic Lax-Milgram cannot apply, but it does not provide more justification like how the second one can prove the existence.  Does someone see these two theorem applied in any specific problem? It would be great if you can offer some links or reading list for me.","Apart from the classic Lax-Milgram theorem which is powerful under the Hilbert setting, there are two generalizations: (from Wikipedia) Babuska-Lax-Milgram theorem suppose $U,V$ Hilbert spaces (real-valued), $B: U\times V\rightarrow \mathbb{R}$ a continuous bilinear form, if $B$ is weakly coercive: for some constant $c>0$, $\forall u\in U$, $$ \sup_{||v||=1}B(u,v)\geq c||u|| $$ and for all $0\neq v\in V$ $$ \sup_{||u||=1}|B(u,v)|>0 $$ then for $f\in V^\ast$, there exists an unique solution $u_f\in U$ to the weak formulation: $$ B(u_f,v)=(f,v),\quad \forall v\in V $$ moreover, the solution depends continuously on the datum: $$ ||u_f||\leq\frac{1}{c}||f|| $$ Lions-Lax-Milgram theorem Let $H$ a Hilbert space and $V$ a normed space, $B:H\times V\rightarrow \mathbb{R}$ a continuous bilinear form, the following are equivalent: for constant $c>0$, $$ \inf_{||v||_V=1}\sup_{||h||_H\leq 1}|B(h,v)|\geq c; $$ for each continuous linear functional $f\in V^\ast$, there exists $h\in H$ s.t. $$ B(h,v)=(f,v)\quad \forall v\in V. $$ I am thinking are there any specific reason or problem to apply these two theorem? In Wikipedia, it says the second theorem may be useful for problem with time-dependent boundary where the classic Lax-Milgram cannot apply, but it does not provide more justification like how the second one can prove the existence.  Does someone see these two theorem applied in any specific problem? It would be great if you can offer some links or reading list for me.",,"['functional-analysis', 'partial-differential-equations']"
96,What is an integral of operators?,What is an integral of operators?,,"I am reading the book Semigroups of Linear Operatos and Applications to Partial Differential Equations which studies a uniformly continuous semigroup, this is a family $(T_t)_{t \geq 0}$ of bounded operators $T :X \to X$ on a Banachspace $X$ with $T(0)=Id$ $T_{s+t}=T_s T_t$ $ \lim_{t \to 0^+} || T_t - I || = 0$ They define the infinitesimal generator as $$ Ax :=\lim_{t \to 0^+} \frac{T_tx -x}{t} $$ for $x \in \mathcal{D}(A) := \{  x \in X \mid \lim_{t \to 0^+} \frac{T_tx -x}{t} \text{ exists} \}.$ Theorem 1.2 states A linear operator A is the infinitesimal generator of a uniformly continuous  semigroup if and only if A is a bounded linear operator I am having difficulties to understand one direction of the proof, namely that for a uniformly continuous semigroup you find a bounded linear operator. The start of the proof goes like this: Let $(T_t)_{t \geq 0}$ be a uniformly continuous semigroup of bounded linear operators on $X$. Fix $\rho >0 $ small enough, such that $$||I - \rho^{-1} \int_0^{\rho}T(s) \, ds|| < 1$$ But how is the integral of operators defined and why can you assure such a $\rho> 0$?","I am reading the book Semigroups of Linear Operatos and Applications to Partial Differential Equations which studies a uniformly continuous semigroup, this is a family $(T_t)_{t \geq 0}$ of bounded operators $T :X \to X$ on a Banachspace $X$ with $T(0)=Id$ $T_{s+t}=T_s T_t$ $ \lim_{t \to 0^+} || T_t - I || = 0$ They define the infinitesimal generator as $$ Ax :=\lim_{t \to 0^+} \frac{T_tx -x}{t} $$ for $x \in \mathcal{D}(A) := \{  x \in X \mid \lim_{t \to 0^+} \frac{T_tx -x}{t} \text{ exists} \}.$ Theorem 1.2 states A linear operator A is the infinitesimal generator of a uniformly continuous  semigroup if and only if A is a bounded linear operator I am having difficulties to understand one direction of the proof, namely that for a uniformly continuous semigroup you find a bounded linear operator. The start of the proof goes like this: Let $(T_t)_{t \geq 0}$ be a uniformly continuous semigroup of bounded linear operators on $X$. Fix $\rho >0 $ small enough, such that $$||I - \rho^{-1} \int_0^{\rho}T(s) \, ds|| < 1$$ But how is the integral of operators defined and why can you assure such a $\rho> 0$?",,"['functional-analysis', 'semigroup-of-operators']"
97,"Euler-Lagrange Equation and ""Eigen Value ""","Euler-Lagrange Equation and ""Eigen Value """,,"The Eigen value $\lambda(t)$ which is characterised by the Rayleigh quotient (where $t$ is a scalar variable):  $$R(u,\Omega_t)= \frac{\int_{\Omega_t} |\nabla u|^2 dy }{\int_{\Omega_t} u^2 dy}$$  $\lambda(\Omega_t) = \mathbf{min} \{R(v, \Omega_t) : v \in H^{1,2}(\Omega_t)\}$  The minimiser $u*$ of the Rayleigh quotient satisfies : $$\left. \frac{d}{ds} R(u* + s\varphi) \right \vert_{s=0} =0 $$ Solving it , the conditions i get are $$\Delta u^* + \lambda^* (\Omega_t) u^* =0 \quad \textrm{in} \quad  \Omega_t $$ $$\partial_{\nu} u^* =  0 \quad \textrm{in} \quad \partial \Omega_t$$ $\textbf{Similarly, I want to find the conditions that are}$  $ \textbf{fulfilled by the minimizer $u^*$ for the Rayleigh  quotient defined as follows .}$ $$R(u,\Omega_t)= \frac{\int_{\Omega_t} |\nabla u|^2 dy + \alpha \oint_{\partial \Omega_t} u^2 dS}{\left(\int_{\Omega_t} u^q dy \right)^{2/q}}$$  $\textbf{My Attempt: }$ We have $$\lambda(x, \Omega_t) = \frac{\int_{\Omega_t} {\mid \nabla u(x) \mid  }^2 \, dx + \alpha \oint_{\partial \Omega_t} u^2\, ds} { \left( \int_{\Omega_t} u^q  \, dx\right )^{2/q}} $$  Corresponding eigen value equation is given by ,  Let $t \in \mathbb{R} , \varphi \in H^{1,2}\Omega$ . If $\lambda(x, \Omega_t) $ is the eigen value then the following holds  $\frac{d}{ds} R(u_t + s \varphi , \Omega_t) \rvert_{s=0}$ $$\frac{d}{ds} \left(\frac {\int_{\Omega_t} {\mid \nabla (u+s\varphi) \mid}^2\,dx}{\left(\int_{\Omega_t} {\mid u +s\varphi \mid}^q \, dx \right)^{2/q}} \right) = \frac{d}{ds} \left (\frac{\int_{\Omega_t} {\mid \nabla u \mid}^2 + 2 s \nabla u \nabla \varphi + s^2 {\mid \nabla \varphi \mid}^2 \, dx} {\left( \int_{\Omega_t} (u+s \varphi )^q  \, dx\right )^{2/q}} \right)_{s=0}  $$ $$=\frac{2 \int_{\Omega_t} \nabla u \nabla \varphi\,dx }{ \left( \int_{\Omega_t} u^q  \, dx\right )^{2/q}} - 2 \left( \int_{\Omega_t} {\mid u \mid }^q\right)^{\frac{-2}{q} -1 } \int_{\Omega_t} u^{q-1} \varphi \, dx  \int_{\Omega_t} {\mid \nabla u \mid}^2 dx  ... (1*) $$ Evaluating next term,  $$\frac{d}{ds} \left( \frac{\alpha \oint_{\partial \Omega_t} (u+s \varphi)^2 \, ds}{ \left(\int_{\Omega_t} (u+s\varphi)^q \, dx \right )^{2/q} }\right )_{s= 0} $$ $$= \frac{2 \alpha \int_{\partial \Omega_t} u \varphi  \, ds - 2 \alpha \oint_{\partial \Omega_t} u^2 \, ds \left(\int_{\Omega_t} u^q \, dx \right )^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx }{ \left( \int_{\Omega_t} u^q \, dx \right )^{2/q} } ... (2*) $$ Adding $(1)$ and $(2)$ gives : $$\frac{2  \int_{\Omega_t} \nabla u \nabla \varphi dx - \int_{\Omega_t} {\mid \nabla u \mid}^2 dx \left( \int_{\Omega_t} u^q  \, dx \right )^{-1}  \int_{\Omega_t} u^{q-1} \varphi \, dx + \alpha \int_{\partial \Omega_t} u \varphi \, ds - \alpha \int_{\partial \Omega_t } u^2 \, ds \left( \int_{\Omega_t} u^q  dx \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx }{ \left(\int_{\Omega_t} {\mid  u\mid }^q\, dx \right)^{2/q}}$$ Using integration by parts yields, $$\frac{-2 \int_{\Omega_t} \Delta u \varphi dx + 2 \int_{\partial \Omega_t} \partial_{\nu} u \varphi \, ds  - 2 \left ( \int_{\Omega_t} u^q \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx \int_{\Omega_t} {\mid \nabla u \mid}^2 \, dx   +2 \alpha \int_{\partial \Omega_t} u \varphi \, ds }{\left(\int_{\Omega_t} {\mid  u\mid }^q \, dx \right)^{2/q}}$$ $$ -\frac{2 \alpha \int_{\partial \Omega_t} u^2 \, ds \left(\int_{\Omega_t} {\mid u \mid}^q \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx  }{\left(\int_{\Omega_t} {\mid  u\mid }^q \, dx \right)^{2/q}}$$ $$ \Delta u + \lambda \mu (u) u^{q-1} = 0 \mathtt \;on\; \Omega_t $$ $$ \partial_{\nu} u + \alpha u = 0 \mathtt \; on \; \partial \Omega_t $$ Where $$\mu(u) = \left ( \int_{\Omega_t} u^{q-1} \, dx \right )^{-1} $$ Can someone check if the resulted equations are the right necessary conditions ? I am looking also for the sufficient conditions so as to know for which $q \in \mathbb R$  the minimum even exists .","The Eigen value $\lambda(t)$ which is characterised by the Rayleigh quotient (where $t$ is a scalar variable):  $$R(u,\Omega_t)= \frac{\int_{\Omega_t} |\nabla u|^2 dy }{\int_{\Omega_t} u^2 dy}$$  $\lambda(\Omega_t) = \mathbf{min} \{R(v, \Omega_t) : v \in H^{1,2}(\Omega_t)\}$  The minimiser $u*$ of the Rayleigh quotient satisfies : $$\left. \frac{d}{ds} R(u* + s\varphi) \right \vert_{s=0} =0 $$ Solving it , the conditions i get are $$\Delta u^* + \lambda^* (\Omega_t) u^* =0 \quad \textrm{in} \quad  \Omega_t $$ $$\partial_{\nu} u^* =  0 \quad \textrm{in} \quad \partial \Omega_t$$ $\textbf{Similarly, I want to find the conditions that are}$  $ \textbf{fulfilled by the minimizer $u^*$ for the Rayleigh  quotient defined as follows .}$ $$R(u,\Omega_t)= \frac{\int_{\Omega_t} |\nabla u|^2 dy + \alpha \oint_{\partial \Omega_t} u^2 dS}{\left(\int_{\Omega_t} u^q dy \right)^{2/q}}$$  $\textbf{My Attempt: }$ We have $$\lambda(x, \Omega_t) = \frac{\int_{\Omega_t} {\mid \nabla u(x) \mid  }^2 \, dx + \alpha \oint_{\partial \Omega_t} u^2\, ds} { \left( \int_{\Omega_t} u^q  \, dx\right )^{2/q}} $$  Corresponding eigen value equation is given by ,  Let $t \in \mathbb{R} , \varphi \in H^{1,2}\Omega$ . If $\lambda(x, \Omega_t) $ is the eigen value then the following holds  $\frac{d}{ds} R(u_t + s \varphi , \Omega_t) \rvert_{s=0}$ $$\frac{d}{ds} \left(\frac {\int_{\Omega_t} {\mid \nabla (u+s\varphi) \mid}^2\,dx}{\left(\int_{\Omega_t} {\mid u +s\varphi \mid}^q \, dx \right)^{2/q}} \right) = \frac{d}{ds} \left (\frac{\int_{\Omega_t} {\mid \nabla u \mid}^2 + 2 s \nabla u \nabla \varphi + s^2 {\mid \nabla \varphi \mid}^2 \, dx} {\left( \int_{\Omega_t} (u+s \varphi )^q  \, dx\right )^{2/q}} \right)_{s=0}  $$ $$=\frac{2 \int_{\Omega_t} \nabla u \nabla \varphi\,dx }{ \left( \int_{\Omega_t} u^q  \, dx\right )^{2/q}} - 2 \left( \int_{\Omega_t} {\mid u \mid }^q\right)^{\frac{-2}{q} -1 } \int_{\Omega_t} u^{q-1} \varphi \, dx  \int_{\Omega_t} {\mid \nabla u \mid}^2 dx  ... (1*) $$ Evaluating next term,  $$\frac{d}{ds} \left( \frac{\alpha \oint_{\partial \Omega_t} (u+s \varphi)^2 \, ds}{ \left(\int_{\Omega_t} (u+s\varphi)^q \, dx \right )^{2/q} }\right )_{s= 0} $$ $$= \frac{2 \alpha \int_{\partial \Omega_t} u \varphi  \, ds - 2 \alpha \oint_{\partial \Omega_t} u^2 \, ds \left(\int_{\Omega_t} u^q \, dx \right )^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx }{ \left( \int_{\Omega_t} u^q \, dx \right )^{2/q} } ... (2*) $$ Adding $(1)$ and $(2)$ gives : $$\frac{2  \int_{\Omega_t} \nabla u \nabla \varphi dx - \int_{\Omega_t} {\mid \nabla u \mid}^2 dx \left( \int_{\Omega_t} u^q  \, dx \right )^{-1}  \int_{\Omega_t} u^{q-1} \varphi \, dx + \alpha \int_{\partial \Omega_t} u \varphi \, ds - \alpha \int_{\partial \Omega_t } u^2 \, ds \left( \int_{\Omega_t} u^q  dx \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx }{ \left(\int_{\Omega_t} {\mid  u\mid }^q\, dx \right)^{2/q}}$$ Using integration by parts yields, $$\frac{-2 \int_{\Omega_t} \Delta u \varphi dx + 2 \int_{\partial \Omega_t} \partial_{\nu} u \varphi \, ds  - 2 \left ( \int_{\Omega_t} u^q \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx \int_{\Omega_t} {\mid \nabla u \mid}^2 \, dx   +2 \alpha \int_{\partial \Omega_t} u \varphi \, ds }{\left(\int_{\Omega_t} {\mid  u\mid }^q \, dx \right)^{2/q}}$$ $$ -\frac{2 \alpha \int_{\partial \Omega_t} u^2 \, ds \left(\int_{\Omega_t} {\mid u \mid}^q \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx  }{\left(\int_{\Omega_t} {\mid  u\mid }^q \, dx \right)^{2/q}}$$ $$ \Delta u + \lambda \mu (u) u^{q-1} = 0 \mathtt \;on\; \Omega_t $$ $$ \partial_{\nu} u + \alpha u = 0 \mathtt \; on \; \partial \Omega_t $$ Where $$\mu(u) = \left ( \int_{\Omega_t} u^{q-1} \, dx \right )^{-1} $$ Can someone check if the resulted equations are the right necessary conditions ? I am looking also for the sufficient conditions so as to know for which $q \in \mathbb R$  the minimum even exists .",,"['functional-analysis', 'multivariable-calculus', 'calculus-of-variations']"
98,Definition of gradient?,Definition of gradient?,,"Reference: A primer of nonlinear analysis - Antonio & Giovanni Let $H$ be a hilbert space over $\mathbb{K}$ and $U$ be open in $H$ and $p\in U$ and $f:U\rightarrow \mathbb{K}$ be a functional Fréchet differentiable at $p$. By Riesz Representation theorem, there exists a unique $y\in H$ such that $\forall x\in H$, $Df(p)(x)=<x,y>$. Is $y$ the gradient of $f$ at $p$?","Reference: A primer of nonlinear analysis - Antonio & Giovanni Let $H$ be a hilbert space over $\mathbb{K}$ and $U$ be open in $H$ and $p\in U$ and $f:U\rightarrow \mathbb{K}$ be a functional Fréchet differentiable at $p$. By Riesz Representation theorem, there exists a unique $y\in H$ such that $\forall x\in H$, $Df(p)(x)=<x,y>$. Is $y$ the gradient of $f$ at $p$?",,"['functional-analysis', 'definition']"
99,Dual of product isometric to product of dual Banach spaces,Dual of product isometric to product of dual Banach spaces,,"I'm tying to show that if $X$ is a real Banach space then there is an isometric injection $$\tau: X' \times X' \to (X \times X)' $$ Where $X \times X$ has the norm $\|(x_1, x_2)\| = \|x_1\| + \|x_2\|$ and $X' \times X'$ has norm $\|(\phi_1, \phi_2)\| = \max\{\|\phi_1\|, \|\phi_2\|\}$ I've defined $\tau$ by $\tau(\phi_1, \phi_2)(x_1,x_2) = \phi_1(x_1) + \phi_2(x_2)$ and I'm now trying to show this is a well defined isometry. I can easily show that $\tau$ is well defined and linear and by direct calculation I have that $\|\tau(\phi_1, \phi_2)\| \le \|(\phi_1, \phi_2)\|$. Now I'm having trouble showing the other direction of this inequality to ensure $\tau$ is an isometry. I'm trying to make a smart choice for $(x_1, x_2)$ to show this but I'm not sure what values to pick! Thanks","I'm tying to show that if $X$ is a real Banach space then there is an isometric injection $$\tau: X' \times X' \to (X \times X)' $$ Where $X \times X$ has the norm $\|(x_1, x_2)\| = \|x_1\| + \|x_2\|$ and $X' \times X'$ has norm $\|(\phi_1, \phi_2)\| = \max\{\|\phi_1\|, \|\phi_2\|\}$ I've defined $\tau$ by $\tau(\phi_1, \phi_2)(x_1,x_2) = \phi_1(x_1) + \phi_2(x_2)$ and I'm now trying to show this is a well defined isometry. I can easily show that $\tau$ is well defined and linear and by direct calculation I have that $\|\tau(\phi_1, \phi_2)\| \le \|(\phi_1, \phi_2)\|$. Now I'm having trouble showing the other direction of this inequality to ensure $\tau$ is an isometry. I'm trying to make a smart choice for $(x_1, x_2)$ to show this but I'm not sure what values to pick! Thanks",,"['analysis', 'functional-analysis', 'banach-spaces']"
