,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Writing a sum of unit step functions as a piecewise function,Writing a sum of unit step functions as a piecewise function,,"After taking the inverse Laplace transform of the following $$\mathcal{L}^{-1}\{G(s)\}=\mathcal{L}^{-1}\left\{\frac{e^{-2s}+e^{-3s}}{s^2-3s+2}\right\}$$ I have $g(t)=\mathcal{U}(t-2)[e^{2(t-2)}-e^{t-2}]+\mathcal{U}(t-3)[e^{2(t-3)}-e^{t-3}]$, where $\mathcal{U}$ is the unit step/Heaviside function. Now, I'm trying to write $g$ as a piecewise function, and I'd like to know if this is correct, based on how $g(t)$ is written as a sum of unit step functions: $g(t) = \begin{cases} 0 &\mbox{if } t \lt 2 \\ e^{2(t-2)}-e^{t-2} & \mbox{if } 2\leq t \lt 3 \\ e^{2(t-3)}-e^{t-3} & \mbox{if } t \geq 3 \end{cases}$ There are no examples in my text in which we have a sum of step functions where the $a$ in $\mathcal{U}(t-a)$ differs, so this is my best shot at converting $g$ into a piecewise function.","After taking the inverse Laplace transform of the following $$\mathcal{L}^{-1}\{G(s)\}=\mathcal{L}^{-1}\left\{\frac{e^{-2s}+e^{-3s}}{s^2-3s+2}\right\}$$ I have $g(t)=\mathcal{U}(t-2)[e^{2(t-2)}-e^{t-2}]+\mathcal{U}(t-3)[e^{2(t-3)}-e^{t-3}]$, where $\mathcal{U}$ is the unit step/Heaviside function. Now, I'm trying to write $g$ as a piecewise function, and I'd like to know if this is correct, based on how $g(t)$ is written as a sum of unit step functions: $g(t) = \begin{cases} 0 &\mbox{if } t \lt 2 \\ e^{2(t-2)}-e^{t-2} & \mbox{if } 2\leq t \lt 3 \\ e^{2(t-3)}-e^{t-3} & \mbox{if } t \geq 3 \end{cases}$ There are no examples in my text in which we have a sum of step functions where the $a$ in $\mathcal{U}(t-a)$ differs, so this is my best shot at converting $g$ into a piecewise function.",,"['ordinary-differential-equations', 'laplace-transform']"
1,Can this differential equation be transformed into an hypergeometric equation?,Can this differential equation be transformed into an hypergeometric equation?,,"$$(1+x^2)y'' -4xy' + 6y = 0 $$ Can this be transformed into an hypergeometric equation of the form $x(1-x)y'' + (c - (a + b + 1)x)y' -aby = 0$? I know that we can do the transform is the term before y'' a polynomial of degree 2, the term before y' of degree 1 and before y is a constant.  Another condition is that the polynomial term of degree two has two distincts roots.  Nothing is said about complex numbers, so I'm not sure.  $ 1 + x^2 $ has two complex roots.  Can I actually transform the equation into an hypergeometric equation? Thank you.","$$(1+x^2)y'' -4xy' + 6y = 0 $$ Can this be transformed into an hypergeometric equation of the form $x(1-x)y'' + (c - (a + b + 1)x)y' -aby = 0$? I know that we can do the transform is the term before y'' a polynomial of degree 2, the term before y' of degree 1 and before y is a constant.  Another condition is that the polynomial term of degree two has two distincts roots.  Nothing is said about complex numbers, so I'm not sure.  $ 1 + x^2 $ has two complex roots.  Can I actually transform the equation into an hypergeometric equation? Thank you.",,['ordinary-differential-equations']
2,Uniqueness of the solution of $y'=y^{1/3}+1$,Uniqueness of the solution of,y'=y^{1/3}+1,"This Cauchy problem $$ \begin{cases}y'=y^{1/3}+1,\\ y (0)=0, \end{cases}$$ has a unique solution. How I can prove this?","This Cauchy problem $$ \begin{cases}y'=y^{1/3}+1,\\ y (0)=0, \end{cases}$$ has a unique solution. How I can prove this?",,['ordinary-differential-equations']
3,Neutral type and retarded DDEs,Neutral type and retarded DDEs,,What are neutral and retarded type delayed-differential equations? Please explain the basic difference between them with examples.,What are neutral and retarded type delayed-differential equations? Please explain the basic difference between them with examples.,,"['calculus', 'ordinary-differential-equations', 'derivatives', 'delay-differential-equations']"
4,How to solve the differential equation $y'=y(1-y)$.,How to solve the differential equation .,y'=y(1-y),"Up until now, we simply rearranged and integrated both sides, so $$y'=y(1-y)$$ $$\frac{dy}{dx}=y(1-y)$$ $$\frac{dy}{y(1-y)}=dx$$ $$\int\frac{dy}{y(1-y)}=\int dx$$ With partial fraction decomposition one gets $$\int\frac{dy}{y} +\int\frac{dy}{(1-y)}=\int dx$$ $$\log|y| + C_1 + \log|1-y|+C_2=x+C_3$$ $$\log|y| + \log|1-y|=x+C_4$$ $$|y||1-y|=C_5e^x$$ How do I continue from here? Edit: It should be $$\int\frac{dy}{(1-y)}=-\log|1-y|+C$$","Up until now, we simply rearranged and integrated both sides, so With partial fraction decomposition one gets How do I continue from here? Edit: It should be",y'=y(1-y) \frac{dy}{dx}=y(1-y) \frac{dy}{y(1-y)}=dx \int\frac{dy}{y(1-y)}=\int dx \int\frac{dy}{y} +\int\frac{dy}{(1-y)}=\int dx \log|y| + C_1 + \log|1-y|+C_2=x+C_3 \log|y| + \log|1-y|=x+C_4 |y||1-y|=C_5e^x \int\frac{dy}{(1-y)}=-\log|1-y|+C,"['ordinary-differential-equations', 'absolute-value']"
5,Ideas for solving this IVP,Ideas for solving this IVP,,"I am curious how to approach solving the initial value problem: $\begin{cases} y'(t) = 5t - 3\sqrt{y} \\ y(0) = 2 \end{cases}$. The equation isn't separable, and more generally it is not an exact equation.  Nor does it seem to be readily convertible into an exact equation via an integrating factor.  I am interested in obtaining at least an implicit expression for $y$.  Is it possible to use a Laplace transform to solve this nonlinear IVP?  If not, what approach might one take? How do I solve the ODE by hand, without the help of Maple?","I am curious how to approach solving the initial value problem: $\begin{cases} y'(t) = 5t - 3\sqrt{y} \\ y(0) = 2 \end{cases}$. The equation isn't separable, and more generally it is not an exact equation.  Nor does it seem to be readily convertible into an exact equation via an integrating factor.  I am interested in obtaining at least an implicit expression for $y$.  Is it possible to use a Laplace transform to solve this nonlinear IVP?  If not, what approach might one take? How do I solve the ODE by hand, without the help of Maple?",,"['ordinary-differential-equations', 'problem-solving', 'initial-value-problems']"
6,Two ODE planar systems that are orthogonal to each other,Two ODE planar systems that are orthogonal to each other,,"Given two planar systems X'=F(X) and X'=G(X) (so F and G are both $C^1$). Assume the dot product of F(X) and G(X) is always zero on $R^2$. Now if F has a closed orbit,  prove that G has a zero. My attempt: The first thing pops up in my mind is a circle (the closed orbit of F). On this circle, G(X) is perpendicular to F(X), and then there must be an equilibrium point inside the circle. However I cannot generalize my idea. P.S.: this should be an easy exercise after learning the Poincare-Bendixson theorem.","Given two planar systems X'=F(X) and X'=G(X) (so F and G are both $C^1$). Assume the dot product of F(X) and G(X) is always zero on $R^2$. Now if F has a closed orbit,  prove that G has a zero. My attempt: The first thing pops up in my mind is a circle (the closed orbit of F). On this circle, G(X) is perpendicular to F(X), and then there must be an equilibrium point inside the circle. However I cannot generalize my idea. P.S.: this should be an easy exercise after learning the Poincare-Bendixson theorem.",,"['ordinary-differential-equations', 'dynamical-systems']"
7,How are second order nonlinear ordinary differential equations solved?,How are second order nonlinear ordinary differential equations solved?,,"I conceived the following second order nonlinear ordinary differential equation: $$\frac{d^2y(x)}{dx^2}=\frac{k}{(y(x))^2}$$ I can tell it's nonlinear because of the $\frac{k}{(y(x))^2}$ term and second order because of the second order derivative. Also, I did some research and concluded that it is of the type ""missing x "". In this category we use the relation, according to SOS math :  $$\frac{d^2y(x)}{dx^2}=v\frac{dv}{dy} (1)$$  by making v equal to $$\frac{dy(x)}{dx} (2)$$ and then, using the chain rule, simplify to obtain equation (1).  Despite all that, I can't seem to find a logical solution to my ODE. I would appreciate any help or clue! Thanks!","I conceived the following second order nonlinear ordinary differential equation: $$\frac{d^2y(x)}{dx^2}=\frac{k}{(y(x))^2}$$ I can tell it's nonlinear because of the $\frac{k}{(y(x))^2}$ term and second order because of the second order derivative. Also, I did some research and concluded that it is of the type ""missing x "". In this category we use the relation, according to SOS math :  $$\frac{d^2y(x)}{dx^2}=v\frac{dv}{dy} (1)$$  by making v equal to $$\frac{dy(x)}{dx} (2)$$ and then, using the chain rule, simplify to obtain equation (1).  Despite all that, I can't seem to find a logical solution to my ODE. I would appreciate any help or clue! Thanks!",,"['ordinary-differential-equations', 'nonlinear-system']"
8,How to find dispersion relation for a system of linear ODEs,How to find dispersion relation for a system of linear ODEs,,"I am trying to find the dispersion relation for a system of linear ODEs. I can do this for a single linear PDE, for example $$u_x = u_t$$ by substituting $u = Ae^{i(kx-wt)}$, here $w = w(k)$ where $k$ is the wavenumber. Then we get $$(ik) = -iw $$$$\Rightarrow w = -k$$ However , now I am trying to find the dispersion relation for a system of linear ODEs. For example $$\dot{u_n} =u_{n-1}+u_{n+1}$$ and $n = 1,2,3,...,2N$ when $u_0 = u_{2N}, u_1 = u_{2N+1}$ In the first example, $w$ was a function of a continuous variable $k$. But since there are no $x$ derivatives now, $w$ is a function of a discrete variable $w = w_k$. My question : what do I use to substitute into equation $(1)$ to find the dispersion relation. Would I use the same substitution as the first example? What's confusing me is that we have the term $u_{n-1}$ and I am not sure how we are meant to reflect that in the substitution. Thanks a lot.","I am trying to find the dispersion relation for a system of linear ODEs. I can do this for a single linear PDE, for example $$u_x = u_t$$ by substituting $u = Ae^{i(kx-wt)}$, here $w = w(k)$ where $k$ is the wavenumber. Then we get $$(ik) = -iw $$$$\Rightarrow w = -k$$ However , now I am trying to find the dispersion relation for a system of linear ODEs. For example $$\dot{u_n} =u_{n-1}+u_{n+1}$$ and $n = 1,2,3,...,2N$ when $u_0 = u_{2N}, u_1 = u_{2N+1}$ In the first example, $w$ was a function of a continuous variable $k$. But since there are no $x$ derivatives now, $w$ is a function of a discrete variable $w = w_k$. My question : what do I use to substitute into equation $(1)$ to find the dispersion relation. Would I use the same substitution as the first example? What's confusing me is that we have the term $u_{n-1}$ and I am not sure how we are meant to reflect that in the substitution. Thanks a lot.",,"['ordinary-differential-equations', 'systems-of-equations']"
9,How to know wheter or not is possible to solve an equations explicitly,How to know wheter or not is possible to solve an equations explicitly,,"On an assignment I got the following question: Characterize the this DE: $$\frac{1}{4x^2}(y')^2+\frac{x}{2}y'-y=0 $$ My suggested solution is: It is a first order, non-linear, ordinary differential equation. I am not sure whether it is implicit or explicit. In our textbook, an implicit differential equation is defined as: Implicit :  is not possible to express $y^{(n)}$ explicitly  as a function of $y^{(n-1)},...,y$. So, my question essentially comes down to: Is it possible to write quadratic equation in $y'$ in an explicit form. I know the solution formula but there is this $\pm$. Is this $\pm$ allowed in an explicit function? How do I know in general whether or not an implicit expression can be rearranged to yield an explicit function?","On an assignment I got the following question: Characterize the this DE: $$\frac{1}{4x^2}(y')^2+\frac{x}{2}y'-y=0 $$ My suggested solution is: It is a first order, non-linear, ordinary differential equation. I am not sure whether it is implicit or explicit. In our textbook, an implicit differential equation is defined as: Implicit :  is not possible to express $y^{(n)}$ explicitly  as a function of $y^{(n-1)},...,y$. So, my question essentially comes down to: Is it possible to write quadratic equation in $y'$ in an explicit form. I know the solution formula but there is this $\pm$. Is this $\pm$ allowed in an explicit function? How do I know in general whether or not an implicit expression can be rearranged to yield an explicit function?",,"['ordinary-differential-equations', 'functions']"
10,Invariant Subspaces Dynamical Interpretation,Invariant Subspaces Dynamical Interpretation,,"Consider the linear system $\dot{x}=A_{nx n}x$ ; $x(0)=x_0$ . Recall that a subspace $\mathcal{U}$ s (dynamically) invariant to the flow if for all initial conditions $x_0 \in \mathcal{U}$ the solutions $x(t) \in \mathcal{U}$ $\forall t \geq 0$ . Show that a subspace $\mathcal{U}$ is dynamically invariant if and only if $A\mathcal{U}\subseteq U$ Solution so far Assume $A\mathcal{U}\subseteq U$ . That is , $v \in \mathcal{U} \implies Av \in \mathcal{U}$ . Let $x(0) \in\mathcal{U}$ .","Consider the linear system ; . Recall that a subspace s (dynamically) invariant to the flow if for all initial conditions the solutions . Show that a subspace is dynamically invariant if and only if Solution so far Assume . That is , . Let .",\dot{x}=A_{nx n}x x(0)=x_0 \mathcal{U} x_0 \in \mathcal{U} x(t) \in \mathcal{U} \forall t \geq 0 \mathcal{U} A\mathcal{U}\subseteq U A\mathcal{U}\subseteq U v \in \mathcal{U} \implies Av \in \mathcal{U} x(0) \in\mathcal{U},"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
11,Solve a nonlinear differential equation,Solve a nonlinear differential equation,,"I am interested in the following differential equation : $$ y(y-1)\ddot{y} - (2y-1)(\dot{y})^2 = 0. $$ In order to solve it, I notice that it is equivalent to $$ \frac{\ddot{y}}{\dot{y}} - \frac{\dot{y}}{1-y} + \frac{\dot{y}}{y} = 0 $$ which would integrate as : $$ \ln(\vert \dot{y} \vert) + \ln(1-y) + \ln(y) = c, \; c \in \mathbb{R}. $$ But I do not know the sign of $\dot{y}$. By Cauchy-Lipschitz, I can show that, since $y \equiv 0$ and $y \equiv 1$ are solutions, $y \in ]0,1[$ for an initial condition $y(0) \in ]0,1[$. But I am wondering if a change of variables could help solve this differential equation since my method is not really neat.","I am interested in the following differential equation : $$ y(y-1)\ddot{y} - (2y-1)(\dot{y})^2 = 0. $$ In order to solve it, I notice that it is equivalent to $$ \frac{\ddot{y}}{\dot{y}} - \frac{\dot{y}}{1-y} + \frac{\dot{y}}{y} = 0 $$ which would integrate as : $$ \ln(\vert \dot{y} \vert) + \ln(1-y) + \ln(y) = c, \; c \in \mathbb{R}. $$ But I do not know the sign of $\dot{y}$. By Cauchy-Lipschitz, I can show that, since $y \equiv 0$ and $y \equiv 1$ are solutions, $y \in ]0,1[$ for an initial condition $y(0) \in ]0,1[$. But I am wondering if a change of variables could help solve this differential equation since my method is not really neat.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
12,Laplace transformations on a homogeneous ODE,Laplace transformations on a homogeneous ODE,,"$$y^{\prime\prime} - 3y^{\prime} + 2y = 0$$ $y(0) = 14$, $y^{\prime}(0)=0$, and using the Laplace transformation I'm trying to solve this IVP","$$y^{\prime\prime} - 3y^{\prime} + 2y = 0$$ $y(0) = 14$, $y^{\prime}(0)=0$, and using the Laplace transformation I'm trying to solve this IVP",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
13,Steady periodic solution to $x''+2x'+4x=9\sin(t)$,Steady periodic solution to,x''+2x'+4x=9\sin(t),"Find the steady periodic solution to the differential equation $x''+2x'+4x=9\sin(t)$ in the form $x_{sp}(t)=C\cos(\omega t−\alpha)$, with $C > 0$ and $0\le\alpha<2\pi$. I don't know how to begin. First of all, what is a steady periodic solution? And how would I begin solving this problem?","Find the steady periodic solution to the differential equation $x''+2x'+4x=9\sin(t)$ in the form $x_{sp}(t)=C\cos(\omega t−\alpha)$, with $C > 0$ and $0\le\alpha<2\pi$. I don't know how to begin. First of all, what is a steady periodic solution? And how would I begin solving this problem?",,"['calculus', 'ordinary-differential-equations']"
14,How could I solve the equation $y''+y= \frac{1}{\sin(t)}$?,How could I solve the equation ?,y''+y= \frac{1}{\sin(t)},"How could I solve the differential equation $y''+y= \frac{1}{\sin(t)}$? A short moment, I thought I could use the equation $y''+y= \sin(t)$. I think I have to find the general solution (trivial), but what about the particular solution? Any idea?","How could I solve the differential equation $y''+y= \frac{1}{\sin(t)}$? A short moment, I thought I could use the equation $y''+y= \sin(t)$. I think I have to find the general solution (trivial), but what about the particular solution? Any idea?",,[]
15,Reduce a Riccati Equation to a Bernoulli Equation,Reduce a Riccati Equation to a Bernoulli Equation,,"I've seen plenty of proofs and exercises where people reduce a Riccati equation to a linear equation, but not the intermediate step of a Bernoulli equation. I'm trying to reduce the Riccati equation $y' = p(t) + q(t)y + r(t)y^2$ to a Bernoulli equation, which has the form $y' + p(t)y = f(t)y^n$, with the substitution $y = y_1 + u$. I'm having some trouble understanding this particular substitution since the variables $y_1$ and $u$ are not defined. If I just follow the direction of ""substitute the equation,"" I immediately get confused when I attempt to find $y'$ since I can't tell if we get $y' = y_1' + u'$ or $y' = 0$. EDIT: I completely forgot to include the fact that $y_1$ is a particular solution of the equation.","I've seen plenty of proofs and exercises where people reduce a Riccati equation to a linear equation, but not the intermediate step of a Bernoulli equation. I'm trying to reduce the Riccati equation $y' = p(t) + q(t)y + r(t)y^2$ to a Bernoulli equation, which has the form $y' + p(t)y = f(t)y^n$, with the substitution $y = y_1 + u$. I'm having some trouble understanding this particular substitution since the variables $y_1$ and $u$ are not defined. If I just follow the direction of ""substitute the equation,"" I immediately get confused when I attempt to find $y'$ since I can't tell if we get $y' = y_1' + u'$ or $y' = 0$. EDIT: I completely forgot to include the fact that $y_1$ is a particular solution of the equation.",,['ordinary-differential-equations']
16,If given the Vasicek Interest rate model $dR(t)=(\alpha-\beta R(t))dt +\sigma dW(t)$ how do I use Ito's lemma to find $d(e^{\beta t}R(t))$?,If given the Vasicek Interest rate model  how do I use Ito's lemma to find ?,dR(t)=(\alpha-\beta R(t))dt +\sigma dW(t) d(e^{\beta t}R(t)),"If given the Vasicek Interest rate model $dR(t)=(\alpha-\beta R(t))dt +\sigma dW(t)$ how do I use Ito's lemma to find $d(e^{\beta*t}R(t))$ and simplify so it is a solution that does not include R(t). To further this question, how would I go about integrating the step above to solve for R(t) then use this to find the expectation and variance of R(t) along with it's limiting behaviors Could someone please walk me through the steps of doing this! It would be very much appreciated EDIT: I think I figured out how to do the first part: let $f(R(t))=e^{\beta t}R(t)$ $f'(R(t))=e^{\beta t}$ $f''(R(t))=0$ $f'(t)=\beta e^{\beta t}R(t)$ then, $d(e^{\beta t} R(t)) = \beta e^{\beta t} R(t)dt + e^{\beta t} dR(t) + 0$ $=e^{\beta t} dR(t) + \beta e^{\beta t} R(t)dt$ Now i sub dR(t) into the above equation and simplify to get $d(e^{\beta t} R(t)) = e^{\beta t}((\alpha - \beta R(t))dt + \sigma dW(t)) + \beta e^{\beta t} R(t)dt = \alpha e^{\beta t}dt + \sigma e^{\beta t}dW(t)$ Is the above correct notation so far?","If given the Vasicek Interest rate model $dR(t)=(\alpha-\beta R(t))dt +\sigma dW(t)$ how do I use Ito's lemma to find $d(e^{\beta*t}R(t))$ and simplify so it is a solution that does not include R(t). To further this question, how would I go about integrating the step above to solve for R(t) then use this to find the expectation and variance of R(t) along with it's limiting behaviors Could someone please walk me through the steps of doing this! It would be very much appreciated EDIT: I think I figured out how to do the first part: let $f(R(t))=e^{\beta t}R(t)$ $f'(R(t))=e^{\beta t}$ $f''(R(t))=0$ $f'(t)=\beta e^{\beta t}R(t)$ then, $d(e^{\beta t} R(t)) = \beta e^{\beta t} R(t)dt + e^{\beta t} dR(t) + 0$ $=e^{\beta t} dR(t) + \beta e^{\beta t} R(t)dt$ Now i sub dR(t) into the above equation and simplify to get $d(e^{\beta t} R(t)) = e^{\beta t}((\alpha - \beta R(t))dt + \sigma dW(t)) + \beta e^{\beta t} R(t)dt = \alpha e^{\beta t}dt + \sigma e^{\beta t}dW(t)$ Is the above correct notation so far?",,"['ordinary-differential-equations', 'stochastic-calculus', 'brownian-motion']"
17,"Solve the system of differential equations $\frac{du}{dt} - 2\Omega v \cos\alpha=0,$ and $\frac{dv}{dt} + 2\Omega u \cos\alpha = -9.8\sin\alpha$.",Solve the system of differential equations  and .,"\frac{du}{dt} - 2\Omega v \cos\alpha=0, \frac{dv}{dt} + 2\Omega u \cos\alpha = -9.8\sin\alpha","Question: Solve the system of differential equations $$\begin{cases}\displaystyle\frac{du}{dt} - 2\Omega v \cos\alpha=0\\ \displaystyle\frac{dv}{dt} + 2\Omega u \cos\alpha = -9.8\sin\alpha\end{cases}$$ with initial conditions $u(0) = 0$, and $v(0) = 0$. My attempt: I have attempted to solve the system, and I've come up with an answer, but it looks fantastically complicated, and I think I may have done something wrong. I'm out of practice with differential equations, so I'm hoping someone can check it over and tell me if I've made any major mistakes. Let $f = 2\Omega\cos\alpha$. The general solution to the homogeneous system is $$u_h = c\sin(ft + \phi), \ \ \ v_h = c\cos(ft+\phi),$$ where $c$ and $\phi$ are constants of integration. We can also find a particular solution of the non-homogeneous system, $$u_p = -\frac{9.8}{f}\tan\alpha, \ \ \ v_p = 0,$$ so that the solution to the system is $$u = c\sin(ft + \phi) - \frac{9.8}{f}\tan\alpha, \ \ \ v = c\cos(ft+\phi).$$ Finally, we need to solve for the constants. Notice that at $t = 0$, $u$ and $v$ are both $0$, so $$c = \frac{1}{\cos\phi}, \ \ \ \text{ and } \ \ \  \tan\phi = \frac{9.8}{f}\tan\alpha \implies \phi = \tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right).$$ So, we have $$u = \frac{1}{\cos\left(\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right)}\sin \left(ft +\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right) - \frac{9.8}{f}\tan\alpha,$$ and $$v = \frac{1}{\cos\left(\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right)}\sin \left(ft +\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right).$$","Question: Solve the system of differential equations $$\begin{cases}\displaystyle\frac{du}{dt} - 2\Omega v \cos\alpha=0\\ \displaystyle\frac{dv}{dt} + 2\Omega u \cos\alpha = -9.8\sin\alpha\end{cases}$$ with initial conditions $u(0) = 0$, and $v(0) = 0$. My attempt: I have attempted to solve the system, and I've come up with an answer, but it looks fantastically complicated, and I think I may have done something wrong. I'm out of practice with differential equations, so I'm hoping someone can check it over and tell me if I've made any major mistakes. Let $f = 2\Omega\cos\alpha$. The general solution to the homogeneous system is $$u_h = c\sin(ft + \phi), \ \ \ v_h = c\cos(ft+\phi),$$ where $c$ and $\phi$ are constants of integration. We can also find a particular solution of the non-homogeneous system, $$u_p = -\frac{9.8}{f}\tan\alpha, \ \ \ v_p = 0,$$ so that the solution to the system is $$u = c\sin(ft + \phi) - \frac{9.8}{f}\tan\alpha, \ \ \ v = c\cos(ft+\phi).$$ Finally, we need to solve for the constants. Notice that at $t = 0$, $u$ and $v$ are both $0$, so $$c = \frac{1}{\cos\phi}, \ \ \ \text{ and } \ \ \  \tan\phi = \frac{9.8}{f}\tan\alpha \implies \phi = \tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right).$$ So, we have $$u = \frac{1}{\cos\left(\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right)}\sin \left(ft +\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right) - \frac{9.8}{f}\tan\alpha,$$ and $$v = \frac{1}{\cos\left(\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right)}\sin \left(ft +\tan^{-1}\left(\frac{9.8}{f}\tan\alpha\right)\right).$$",,['ordinary-differential-equations']
18,Checking a solution for $x'' + 4x' + 5 = 0 $,Checking a solution for,x'' + 4x' + 5 = 0 ,"Given the differential equation $x'' + 4x' + 5 = 0 $, I applied standard methods: $ \lambda^2 + 4\lambda + 5 = 0 \, \, \Rightarrow \lambda_{1,2} = -2 \pm i $ So a complex solution is $\hat x(t) = c_1 e^{(-2+i)t} + c_2 e^{(-2-i)t}$ And a real solution is $x(t) = Re(\hat x(t)) + Im (\hat x(t)) = e^{-2t}(d_1\cos(t) + d_2\sin(t))$ with $d_1 := c_1 +c_2 $ and $d_2 := c_1 - c_2 $. However, if I check my solution, I get a contradiction: $x'(t) = e^{-2t}(-d_1\sin(t) -2d_1\cos(t) + d_2\cos(t) -2d_2\sin(t))$ $x''(t) = e^{-2t}(3d_1 \cos(t) + 4d_1 \sin(t) + 3d_2 \sin(t) - 4d_2 \cos(t))$ If I insert those expressions in my original equation, I obtain $d_1 \cos(t) + d_2 \sin(t) = 1 $. Where's my mistake?","Given the differential equation $x'' + 4x' + 5 = 0 $, I applied standard methods: $ \lambda^2 + 4\lambda + 5 = 0 \, \, \Rightarrow \lambda_{1,2} = -2 \pm i $ So a complex solution is $\hat x(t) = c_1 e^{(-2+i)t} + c_2 e^{(-2-i)t}$ And a real solution is $x(t) = Re(\hat x(t)) + Im (\hat x(t)) = e^{-2t}(d_1\cos(t) + d_2\sin(t))$ with $d_1 := c_1 +c_2 $ and $d_2 := c_1 - c_2 $. However, if I check my solution, I get a contradiction: $x'(t) = e^{-2t}(-d_1\sin(t) -2d_1\cos(t) + d_2\cos(t) -2d_2\sin(t))$ $x''(t) = e^{-2t}(3d_1 \cos(t) + 4d_1 \sin(t) + 3d_2 \sin(t) - 4d_2 \cos(t))$ If I insert those expressions in my original equation, I obtain $d_1 \cos(t) + d_2 \sin(t) = 1 $. Where's my mistake?",,['ordinary-differential-equations']
19,Differential equation where Picard-Lindelöf can not be applied,Differential equation where Picard-Lindelöf can not be applied,,"My question is the following : Let $f:\mathbb{R}\to\mathbb{R}$ be  continuous function and let $u:[a,b]\to\mathbb{R}$ be a $C^1$ function such that   $$\forall t\in[a,b],u'(t)=f(u(t))\text{ and }u(a)=u(b)=k\in\mathbb{R}.$$   Show that $u$ must be constant. What I have done : We show that $\max u=k$ (and a similar proof would show that $\min u=k).$ Let's suppose that $\max u=M>k.$ We note $c\in]a,b[$ a point such that $u(c)=M.$ The mean value theorem give us a point $d^+\in ]a,c[$ such that $$u'(d^+)=\frac{u(c)-u(a)}{c-a}=\frac{M-k}{c-a}>0.$$ We note $m=u(d^+).$ The intermediate value theorem tell us that it exists a point $d^-\in]c,b[$ such that $u(d^-)=m$ : we can suppose that $u'(d^-)\leq 0,$ else we would have that, for all $x\in]c,b[$ such that $u(x)=m,$ $u'(x)>0.$ It would be true in particular for $x_{max},$ the biggest $x$ such that $u(x)=m$ : as $u'$ is continuous, it would give us $u'(t)>0$ for $t$ in a neighborhood $[x_l,x_r]$ of $x_{max},$ which would implies $$\int_{x_{max}}^{x_r}u'(t)\,dt=u(x_r)-u(x_{max})>0,$$ and so $u(x_r)>m.$ By continuity of $u$, it is impossible (mean value theorem applied to $u$ on $[x_r,b]$ : there would be no point in this interval which would reach $m$, while $u(x_r)>m$ and $u(c)<m$). Finally, we have two points $d^+$ and $d^-$ such that  $$0<u'(d^+)=f(u(d^+))=f(m)=f(u(d^-))=u'(d^-)\leq 0,$$  and so a contradiction. We get our result. My question : Is my proof correct ? And if yes, is there a faster way to prove that result ? Thank you for your time ! Edit : After some discussion with Jonas, we agree that this proof is correct and are talking about details in this specific way of proving that result. I am starting a bounty to answer the second part of my question : does someone see a faster way to solve it ? Thank you for your attention.","My question is the following : Let $f:\mathbb{R}\to\mathbb{R}$ be  continuous function and let $u:[a,b]\to\mathbb{R}$ be a $C^1$ function such that   $$\forall t\in[a,b],u'(t)=f(u(t))\text{ and }u(a)=u(b)=k\in\mathbb{R}.$$   Show that $u$ must be constant. What I have done : We show that $\max u=k$ (and a similar proof would show that $\min u=k).$ Let's suppose that $\max u=M>k.$ We note $c\in]a,b[$ a point such that $u(c)=M.$ The mean value theorem give us a point $d^+\in ]a,c[$ such that $$u'(d^+)=\frac{u(c)-u(a)}{c-a}=\frac{M-k}{c-a}>0.$$ We note $m=u(d^+).$ The intermediate value theorem tell us that it exists a point $d^-\in]c,b[$ such that $u(d^-)=m$ : we can suppose that $u'(d^-)\leq 0,$ else we would have that, for all $x\in]c,b[$ such that $u(x)=m,$ $u'(x)>0.$ It would be true in particular for $x_{max},$ the biggest $x$ such that $u(x)=m$ : as $u'$ is continuous, it would give us $u'(t)>0$ for $t$ in a neighborhood $[x_l,x_r]$ of $x_{max},$ which would implies $$\int_{x_{max}}^{x_r}u'(t)\,dt=u(x_r)-u(x_{max})>0,$$ and so $u(x_r)>m.$ By continuity of $u$, it is impossible (mean value theorem applied to $u$ on $[x_r,b]$ : there would be no point in this interval which would reach $m$, while $u(x_r)>m$ and $u(c)<m$). Finally, we have two points $d^+$ and $d^-$ such that  $$0<u'(d^+)=f(u(d^+))=f(m)=f(u(d^-))=u'(d^-)\leq 0,$$  and so a contradiction. We get our result. My question : Is my proof correct ? And if yes, is there a faster way to prove that result ? Thank you for your time ! Edit : After some discussion with Jonas, we agree that this proof is correct and are talking about details in this specific way of proving that result. I am starting a bounty to answer the second part of my question : does someone see a faster way to solve it ? Thank you for your attention.",,"['ordinary-differential-equations', 'proof-verification']"
20,"If $y'+y=|x|$ and $y(-1)=0$, what is $y(1)$?","If  and , what is ?",y'+y=|x| y(-1)=0 y(1),"If $y'+y=|x|$ and $y(-1)=0$, what is $y(1)$? I calculated the integrating factor to be $e^x$. Then $e^x y'+ e^x y=e^x |x|$ hence $\frac {d(e^x y)}{dx}=e^x |x|$ hence $d(e^x y)=e^x|x|dx $ Integrating both sides, $e^xy=e^x \int |x|dx- \int [(\int |x|dx)(e^x)]dx +c.$ From here how to proceed? Can I write $\int |x|dx=-2 \int xdx, x \lt 0$? Then I will get the constant $c$ by using $y(-1)=0$. But how will it help me to calculate $y(1)$?","If $y'+y=|x|$ and $y(-1)=0$, what is $y(1)$? I calculated the integrating factor to be $e^x$. Then $e^x y'+ e^x y=e^x |x|$ hence $\frac {d(e^x y)}{dx}=e^x |x|$ hence $d(e^x y)=e^x|x|dx $ Integrating both sides, $e^xy=e^x \int |x|dx- \int [(\int |x|dx)(e^x)]dx +c.$ From here how to proceed? Can I write $\int |x|dx=-2 \int xdx, x \lt 0$? Then I will get the constant $c$ by using $y(-1)=0$. But how will it help me to calculate $y(1)$?",,"['integration', 'ordinary-differential-equations', 'indefinite-integrals', 'problem-solving', 'boundary-value-problem']"
21,checking that an initial condition holds for the heat equation,checking that an initial condition holds for the heat equation,,"I'm trying to follow a video lecture on solving the heat equation. $I) \space u_t = ku_{xx}, x \in \mathbb{R}, t > 0$ $II) \space u(x,0)=\phi (x), $ $k$ is const, $\phi (x) $ is a known function. An earlier video in lecture series proved these properties: If $u(x,t)$ is a solution to $I)$ then for each fixed $y$, the translate   $u(x-y,t)$ is also a solution. If $u$ is a solution then every derivative of $u$ is also a solution. Every linear combination of solutions is also a solution. If $S(x,t)$ is a solution then so is its integral. Thus $S(x-y,t)$ is a   solution and so is $\int_{-\infty}^{\infty} S(x-y,t)g(y)dy$ for each   function $g$ that makes the integral converge. If $u(x,t)$ is a solution the for each $a>0$ the dilates function   $u(\sqrt{a}x,at)$ is also a solution. Using the the translate and integral properties, you can find this solution to $I)$ as $u(x,t)=\displaystyle \int_{-\infty}^{\infty} S(x-y,t)\phi(y)dy $ The first part of the video shows that $I)$ holds. To show that the initial condition $II)$ holds, you begin by doing the following: $S := \frac{\partial }{\partial x}Q$ $u(x,t)=\displaystyle \int_{-\infty}^{\infty} \frac{\partial }{\partial x} Q(x-y,t)\phi(y)dy $ $ \color{green}{\mathbb{\bf*}} \\ =  \displaystyle \int_{-\infty}^{\infty} \frac{\partial }{\partial y}[-Q (x-y,t)]\phi(y)dy     $ Using integration by parts: $- \displaystyle \int_{-\infty}^{\infty} Q_y (x-y,t)\phi(y)dy     $ $\begin{array}{ll} w &=\phi(y)     & v_y = Q_y(x-y,t) \\ w_y &=\phi'(y)  & v = -Q(x-y,t)  \end{array}$ $=-  \left[ -Q (x-y,t)\phi(y)\bigg |_{y \rightarrow -\infty}^{y \rightarrow \infty}- \displaystyle \int_{-\infty}^{\infty}  -Q (x-y,t)   \phi'(y)dy  \right]$ $=Q (x-y,t)\phi(y)\bigg |_{y \rightarrow -\infty}^{y \rightarrow \infty}- \displaystyle \int_{-\infty}^{\infty}  Q (x-y,t)   \phi'(y)dy  $ I don't understand what happens on the line labeled $\color{green}{\mathbb{\bf*}}$. Why can you switch from $Q_x$ to $-Q_y$?","I'm trying to follow a video lecture on solving the heat equation. $I) \space u_t = ku_{xx}, x \in \mathbb{R}, t > 0$ $II) \space u(x,0)=\phi (x), $ $k$ is const, $\phi (x) $ is a known function. An earlier video in lecture series proved these properties: If $u(x,t)$ is a solution to $I)$ then for each fixed $y$, the translate   $u(x-y,t)$ is also a solution. If $u$ is a solution then every derivative of $u$ is also a solution. Every linear combination of solutions is also a solution. If $S(x,t)$ is a solution then so is its integral. Thus $S(x-y,t)$ is a   solution and so is $\int_{-\infty}^{\infty} S(x-y,t)g(y)dy$ for each   function $g$ that makes the integral converge. If $u(x,t)$ is a solution the for each $a>0$ the dilates function   $u(\sqrt{a}x,at)$ is also a solution. Using the the translate and integral properties, you can find this solution to $I)$ as $u(x,t)=\displaystyle \int_{-\infty}^{\infty} S(x-y,t)\phi(y)dy $ The first part of the video shows that $I)$ holds. To show that the initial condition $II)$ holds, you begin by doing the following: $S := \frac{\partial }{\partial x}Q$ $u(x,t)=\displaystyle \int_{-\infty}^{\infty} \frac{\partial }{\partial x} Q(x-y,t)\phi(y)dy $ $ \color{green}{\mathbb{\bf*}} \\ =  \displaystyle \int_{-\infty}^{\infty} \frac{\partial }{\partial y}[-Q (x-y,t)]\phi(y)dy     $ Using integration by parts: $- \displaystyle \int_{-\infty}^{\infty} Q_y (x-y,t)\phi(y)dy     $ $\begin{array}{ll} w &=\phi(y)     & v_y = Q_y(x-y,t) \\ w_y &=\phi'(y)  & v = -Q(x-y,t)  \end{array}$ $=-  \left[ -Q (x-y,t)\phi(y)\bigg |_{y \rightarrow -\infty}^{y \rightarrow \infty}- \displaystyle \int_{-\infty}^{\infty}  -Q (x-y,t)   \phi'(y)dy  \right]$ $=Q (x-y,t)\phi(y)\bigg |_{y \rightarrow -\infty}^{y \rightarrow \infty}- \displaystyle \int_{-\infty}^{\infty}  Q (x-y,t)   \phi'(y)dy  $ I don't understand what happens on the line labeled $\color{green}{\mathbb{\bf*}}$. Why can you switch from $Q_x$ to $-Q_y$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
22,Construct differential equation given the phase portrait (non-linear pendulum),Construct differential equation given the phase portrait (non-linear pendulum),,"I am curious to know how to recover the differential equation that goes with a phase portrait.  I have seen the following posts but the first one was a $y'$ (and easy enough to ""do in my head"") and the second one lacks the figure. As an example, I solved (in Mathematica ) the non-linear pendulum equation and plotted the phase portrait as below: $$y''(t)+\frac{y'(t)}{q}+\sin \left(y(t)\right)=g \cos (\omega t)$$ Where $q$ is damping, $g$ is the forcing term while $\omega$ is the frequency of forcing.  The phase portrait ($y'$ vs $y$) is plotted: So, now if I were given this phase portrait, how do I derive the autonomous differential equation for it? I ask because I do not know the ""language"" to search for online. The reason I posted this question here instead of mathematica.SE is because this is more of a Mathematics question and not so much a Mathematica question. My Mathematica code, in case interested: g = 1/10; q = 8; \[Omega] = -0.04; TMax = 40; pSolNL = NDSolveValue[{y''[t] + (1/q) y'[t] + Sin[y[t]] ==      g  Cos[\[Omega]  t], y[0] == 0, y'[0] == 0}, y, {t, 0, TMax}];  ParametricPlot[{pSolNL[\[Tau]] /. \[Tau] -> t,    D[pSolNL[\[Tau]], \[Tau]] /. \[Tau] -> t}, {t, 0, TMax},   AxesLabel -> {""y(t)"", ""y'(t)""}]","I am curious to know how to recover the differential equation that goes with a phase portrait.  I have seen the following posts but the first one was a $y'$ (and easy enough to ""do in my head"") and the second one lacks the figure. As an example, I solved (in Mathematica ) the non-linear pendulum equation and plotted the phase portrait as below: $$y''(t)+\frac{y'(t)}{q}+\sin \left(y(t)\right)=g \cos (\omega t)$$ Where $q$ is damping, $g$ is the forcing term while $\omega$ is the frequency of forcing.  The phase portrait ($y'$ vs $y$) is plotted: So, now if I were given this phase portrait, how do I derive the autonomous differential equation for it? I ask because I do not know the ""language"" to search for online. The reason I posted this question here instead of mathematica.SE is because this is more of a Mathematics question and not so much a Mathematica question. My Mathematica code, in case interested: g = 1/10; q = 8; \[Omega] = -0.04; TMax = 40; pSolNL = NDSolveValue[{y''[t] + (1/q) y'[t] + Sin[y[t]] ==      g  Cos[\[Omega]  t], y[0] == 0, y'[0] == 0}, y, {t, 0, TMax}];  ParametricPlot[{pSolNL[\[Tau]] /. \[Tau] -> t,    D[pSolNL[\[Tau]], \[Tau]] /. \[Tau] -> t}, {t, 0, TMax},   AxesLabel -> {""y(t)"", ""y'(t)""}]",,"['ordinary-differential-equations', 'mathematical-modeling', 'stability-in-odes', 'chaos-theory']"
23,Solving a differential equation with Bernoulli's Method,Solving a differential equation with Bernoulli's Method,,"What approach do you take to solve the differential equation $ y' + (6y/x) = (y^3)/ x^5\ $ through the use of Bernoulli's method? I've assumed u = y^(-2) for substitution, but I don't know where to go from there. The answer is  y $ ((C\x^2\) + (1/8x^4))^(-.5) \ $","What approach do you take to solve the differential equation $ y' + (6y/x) = (y^3)/ x^5\ $ through the use of Bernoulli's method? I've assumed u = y^(-2) for substitution, but I don't know where to go from there. The answer is  y $ ((C\x^2\) + (1/8x^4))^(-.5) \ $",,"['linear-algebra', 'ordinary-differential-equations', 'bernoulli-numbers']"
24,Phase portrait of the pendulum,Phase portrait of the pendulum,,Lets consider the system of differential equation: $$\phi'(t)=\psi(t)$$ $$\psi'(t)=-\sin(\phi(t))$$ How one can get the corresponding phase portrait? For example I found this one but I dont know how to get it. Is it possible to get it without solving the differential equation? Thanks in advance.,Lets consider the system of differential equation: $$\phi'(t)=\psi(t)$$ $$\psi'(t)=-\sin(\phi(t))$$ How one can get the corresponding phase portrait? For example I found this one but I dont know how to get it. Is it possible to get it without solving the differential equation? Thanks in advance.,,"['real-analysis', 'ordinary-differential-equations']"
25,Phase portrait for degenerate node [duplicate],Phase portrait for degenerate node [duplicate],,"This question already has an answer here : Phase plots of solutions for repeated eigenvalues (1 answer) Closed 8 years ago . We have two eigenvalues $\lambda_1$ and $\lambda_2$, with $\lambda_1 = \lambda_2 < 0$. Consider the case we have only one linearly independent eigenvector. Then the phase portrait is a (stable) degenerate node. I have seen two different phase portraits for this case: In general, how we decide which phase portrait we need?","This question already has an answer here : Phase plots of solutions for repeated eigenvalues (1 answer) Closed 8 years ago . We have two eigenvalues $\lambda_1$ and $\lambda_2$, with $\lambda_1 = \lambda_2 < 0$. Consider the case we have only one linearly independent eigenvector. Then the phase portrait is a (stable) degenerate node. I have seen two different phase portraits for this case: In general, how we decide which phase portrait we need?",,['ordinary-differential-equations']
26,Inverse Laplace transform of $\tan^{−1}\left(\frac{1}{s}\right)$,Inverse Laplace transform of,\tan^{−1}\left(\frac{1}{s}\right),"I'm studying Laplace transformations, but I don't understand where $-\frac{1}{t}$ comes from. And what is the relationship between the corollary and the example?","I'm studying Laplace transformations, but I don't understand where $-\frac{1}{t}$ comes from. And what is the relationship between the corollary and the example?",,"['linear-algebra', 'ordinary-differential-equations', 'laplace-transform']"
27,The k-th derivative of the resolvent set,The k-th derivative of the resolvent set,,"I want to prove $$\frac{d^{k}}{dz^{k}}(zI-A)^{-1}=(-1)^{k}k!(zI-A)^{-k-1}$$ I have the resolvent equation $(zI-A)^{-1}-(\lambda I-A)^{1}=(\lambda-z)(zI-A)^{-1}(\lambda I-A)^{-1}$, i.e. $$\begin{aligned}(zI-A)^{-1}&=(\lambda-z)(zI-A)^{-1}(\lambda I-A)^{-1}+(\lambda I-A)^{-1} \\ &=(\lambda I-A)^{-1}[(\lambda-z)(zI-A)^{-1}+1] \\ \end{aligned}$$ $$\implies \frac{d^{k}}{dz^{k}}(zI-A)^{-1}=(\lambda I-A)^{-1}\frac{d^{k}}{dz^{k}}[(\lambda-z)(zI-A)^{-1}]$$ But I don't see how the final equality is derived from this.","I want to prove $$\frac{d^{k}}{dz^{k}}(zI-A)^{-1}=(-1)^{k}k!(zI-A)^{-k-1}$$ I have the resolvent equation $(zI-A)^{-1}-(\lambda I-A)^{1}=(\lambda-z)(zI-A)^{-1}(\lambda I-A)^{-1}$, i.e. $$\begin{aligned}(zI-A)^{-1}&=(\lambda-z)(zI-A)^{-1}(\lambda I-A)^{-1}+(\lambda I-A)^{-1} \\ &=(\lambda I-A)^{-1}[(\lambda-z)(zI-A)^{-1}+1] \\ \end{aligned}$$ $$\implies \frac{d^{k}}{dz^{k}}(zI-A)^{-1}=(\lambda I-A)^{-1}\frac{d^{k}}{dz^{k}}[(\lambda-z)(zI-A)^{-1}]$$ But I don't see how the final equality is derived from this.",,"['functional-analysis', 'ordinary-differential-equations', 'spectral-theory']"
28,"Let $A(t), (a_{ij}): \Bbb R\to \Bbb R$ be a periodic matrix with period $1$. Prove a solution to $x'=Ax$ is bounded.",Let  be a periodic matrix with period . Prove a solution to  is bounded.,"A(t), (a_{ij}): \Bbb R\to \Bbb R 1 x'=Ax","Question Let $A(t), (a_{ij}): \Bbb R\to \Bbb R$ be a periodic $n\times n$ matrix with period $1$. Prove that if $x(t)$ is a solution to $$x'(t)=A(t)x(t)\tag 1$$ defined on $\Bbb R$, witch satisfies $x(1)=-x(0)$, then $x$ is bounded. Attempt I had no idea on how to face this so I just tried the first thing that came to mind. I thought evaluating the equation at $t\mapsto t+1$, we see that $x_1(t)=x(t+1)$ is a solution to $(1)$, and $x_1(0)=x(1)$. Suppose that $x(t+n)$ is a solution to $(1)$, thus $$ x(t+n)=A(t)x(t+n) $$ By induction, setting $t\mapsto t+1$, we get that $x_n(t)=x(t+n)$ is a solution to $(1)$ for all $n$, and that $x_n(0)=x(n)$. I got stuck there, and I don't think I'll get anywhere this way... Could someone give me some hints?","Question Let $A(t), (a_{ij}): \Bbb R\to \Bbb R$ be a periodic $n\times n$ matrix with period $1$. Prove that if $x(t)$ is a solution to $$x'(t)=A(t)x(t)\tag 1$$ defined on $\Bbb R$, witch satisfies $x(1)=-x(0)$, then $x$ is bounded. Attempt I had no idea on how to face this so I just tried the first thing that came to mind. I thought evaluating the equation at $t\mapsto t+1$, we see that $x_1(t)=x(t+1)$ is a solution to $(1)$, and $x_1(0)=x(1)$. Suppose that $x(t+n)$ is a solution to $(1)$, thus $$ x(t+n)=A(t)x(t+n) $$ By induction, setting $t\mapsto t+1$, we get that $x_n(t)=x(t+n)$ is a solution to $(1)$ for all $n$, and that $x_n(0)=x(n)$. I got stuck there, and I don't think I'll get anywhere this way... Could someone give me some hints?",,['ordinary-differential-equations']
29,Non-linear SDE: how to?,Non-linear SDE: how to?,,"$$ \newcommand{\mcl}[1]{\mathcal{#1}} \newcommand{\mrm}[1]{\mathrm{#1}} \newcommand{\avg}[1]{\langle#1 \rangle} \newcommand{\pth}[1]{\left( #1 \right)} \newcommand{\bck}[1]{\left\{ #1 \right\}} \newcommand{\sbck}[1]{\left[ #1 \right]} \newcommand{\bsbck}[1]{\Big[ #1 \Big]} \newcommand{\deriv}[1]{\frac{\mrm{d} #1}{\mrm{d}t}} $$ I recently learned that stochastic differential equations (SDEs) had a whole lot of theory behind them and I am genuinely surprised at the complexity that a ""simple"" noise term seems to introduce in the numerical strategies employed to solve such equations. I found this paper by D.Higham very instructive, and learned about Ito and Stratonovich frameworks for SDEs, but I don't understand how to get started on my own set of equations. My system is of the form: $$ \deriv{X_k} = -X_k + f\left( \sum_i \alpha_{i,k} X_i + \eta \right) $$ where $\alpha_{i,j}$ are real constants, $f$ is a smooth non-linear function and $\eta$ is normally distributed. From Higham's paper, I assume that the ""Stochastic Chain Rule"" would be relevant here, but it is unclear to me how to reduce this set of equations to an Ito or any other suitable form. Most numerical solvers I encountered so far require an Ito form, but clearly here the noise term is not directly additive. I don't know how to go about integrating this system with noise. Without noise, I was using a simple Runge-Kutta 4 method but from what I read I assume that ""adapting"" this with noise is not going to be straightforward at all? EDIT: Based on the recommendations of @LutzL, a sensible approach seems to be as follows: Define the non-linear feedback term  $$Z_k = \sum_i \alpha_{i,k} X_i + \eta$$  the differential of which is  $$d Z_k = \sum_i \alpha_{i,k} d X_i + \sigma_k d W_k$$ We know from the original equation that $$d X_k = ( -X_k + f(Z_k) )dt$$ so we can rewrite $$d Z_k = \sum_i \alpha_{i,k} ( f(Z_k)-X_k )dt + \sigma_k d W_k$$ These equations are now in Ito form and can be solved using e.g. an Euler-Maruyama method. Note that if we started off with a system of $N$ equations, the resulting Ito form now comprises $2N$ equations. Questions that persist for me are: @LutzL mentioned that the noise timestep $\Delta t$ has to be larger than the solver's time-step ; in practice this causes the noise time-course to have unexpected spectral properties (ie, a sub-sampled white-noise is no longer white). Why is this necessary? Is there a reference in the literature about this specific problem? Is it ok to use a mixed method, Euler-Maruyama for $Z_k$, and RK4 for $X_k$? If so, should intermediary steps of $X_k$ each sample a value of $Z_k$, or should there be only one sample of $Z_k$ for all intermediary steps?","$$ \newcommand{\mcl}[1]{\mathcal{#1}} \newcommand{\mrm}[1]{\mathrm{#1}} \newcommand{\avg}[1]{\langle#1 \rangle} \newcommand{\pth}[1]{\left( #1 \right)} \newcommand{\bck}[1]{\left\{ #1 \right\}} \newcommand{\sbck}[1]{\left[ #1 \right]} \newcommand{\bsbck}[1]{\Big[ #1 \Big]} \newcommand{\deriv}[1]{\frac{\mrm{d} #1}{\mrm{d}t}} $$ I recently learned that stochastic differential equations (SDEs) had a whole lot of theory behind them and I am genuinely surprised at the complexity that a ""simple"" noise term seems to introduce in the numerical strategies employed to solve such equations. I found this paper by D.Higham very instructive, and learned about Ito and Stratonovich frameworks for SDEs, but I don't understand how to get started on my own set of equations. My system is of the form: $$ \deriv{X_k} = -X_k + f\left( \sum_i \alpha_{i,k} X_i + \eta \right) $$ where $\alpha_{i,j}$ are real constants, $f$ is a smooth non-linear function and $\eta$ is normally distributed. From Higham's paper, I assume that the ""Stochastic Chain Rule"" would be relevant here, but it is unclear to me how to reduce this set of equations to an Ito or any other suitable form. Most numerical solvers I encountered so far require an Ito form, but clearly here the noise term is not directly additive. I don't know how to go about integrating this system with noise. Without noise, I was using a simple Runge-Kutta 4 method but from what I read I assume that ""adapting"" this with noise is not going to be straightforward at all? EDIT: Based on the recommendations of @LutzL, a sensible approach seems to be as follows: Define the non-linear feedback term  $$Z_k = \sum_i \alpha_{i,k} X_i + \eta$$  the differential of which is  $$d Z_k = \sum_i \alpha_{i,k} d X_i + \sigma_k d W_k$$ We know from the original equation that $$d X_k = ( -X_k + f(Z_k) )dt$$ so we can rewrite $$d Z_k = \sum_i \alpha_{i,k} ( f(Z_k)-X_k )dt + \sigma_k d W_k$$ These equations are now in Ito form and can be solved using e.g. an Euler-Maruyama method. Note that if we started off with a system of $N$ equations, the resulting Ito form now comprises $2N$ equations. Questions that persist for me are: @LutzL mentioned that the noise timestep $\Delta t$ has to be larger than the solver's time-step ; in practice this causes the noise time-course to have unexpected spectral properties (ie, a sub-sampled white-noise is no longer white). Why is this necessary? Is there a reference in the literature about this specific problem? Is it ok to use a mixed method, Euler-Maruyama for $Z_k$, and RK4 for $X_k$? If so, should intermediary steps of $X_k$ each sample a value of $Z_k$, or should there be only one sample of $Z_k$ for all intermediary steps?",,"['ordinary-differential-equations', 'numerical-methods', 'stochastic-integrals', 'stochastic-differential-equations']"
30,Finding a matrix for a system of differential equations,Finding a matrix for a system of differential equations,,"Let $$m_0\theta''+k\theta' +\frac{m_0g}{l}\theta=0$$ and let   $$y(t)=[\theta(t), \theta'(t)]^T$$ Find matrix A such that $y'=Ay$ and it's eigenvalues. My guess was to define: $x_1=\theta, x_2=\theta'$, Hence, $x_1'=x_2=\theta'$ and $x_2=\theta''=-(\frac{g}{l}x_1+\frac{k}{m_0}x_2)$ So we obtain: $$x(t)=[x_1, x_2]^T$$ And we obtain matrix A: $$A=\begin{bmatrix}     1 & 0 \\     \frac{g}{l} & \frac{k}{m_0} \\  \end{bmatrix}$$ And $$x'(t)=Ax(t)$$ But I am not sure whether this matrix is correct, as the characteristic polynomial makes little sense in this case. Hence, I believe that the first line of the matrix is incorrect, but could anyone point me out where I made the mistake?","Let $$m_0\theta''+k\theta' +\frac{m_0g}{l}\theta=0$$ and let   $$y(t)=[\theta(t), \theta'(t)]^T$$ Find matrix A such that $y'=Ay$ and it's eigenvalues. My guess was to define: $x_1=\theta, x_2=\theta'$, Hence, $x_1'=x_2=\theta'$ and $x_2=\theta''=-(\frac{g}{l}x_1+\frac{k}{m_0}x_2)$ So we obtain: $$x(t)=[x_1, x_2]^T$$ And we obtain matrix A: $$A=\begin{bmatrix}     1 & 0 \\     \frac{g}{l} & \frac{k}{m_0} \\  \end{bmatrix}$$ And $$x'(t)=Ax(t)$$ But I am not sure whether this matrix is correct, as the characteristic polynomial makes little sense in this case. Hence, I believe that the first line of the matrix is incorrect, but could anyone point me out where I made the mistake?",,"['linear-algebra', 'ordinary-differential-equations']"
31,How to obtain this partial fraction decomposition?,How to obtain this partial fraction decomposition?,,"I am studying Laplace transforms right now and got stuck at this step that involves a weird partial fraction decomposition. It looks like the instructor skipped a bunch of steps and assigned numerators to a bunch of the fractions without assigning them dummy variables. Any idea how he got to this step so quickly? I tried doing this the standard way (i.e. breaking each polynomial in the denominator into its own fraction, assigning variables for each numerator, and trying to solve), but it got messy very quickly. laplace transform up to the part where I got stuck $$ X=\frac{2}{s^4-1}\left(e^{-s}\cdot\frac{1}{s}-e^{-2s}\cdot\frac{1}{s}\right)=\frac{2}{\left(s^4-1\right)s}\left(e^{-s}-e^{-2s}\right)=2\cdot\frac{1}{\left(s^2+1\right)\left(s+1\right)\left(s-1\right)s}\left(e^{-s}-e^{-2s}\right)=2\left(\frac{As+B}{s^2+1}+\frac{\left(\frac{1}{4}\right)}{\left(s+1\right)}+\frac{\left(\frac{1}{4}\right)}{s-1}+\frac{\left(-1\right)}{s}\right)\left(e^{-s}-e^{-2s}\right) $$","I am studying Laplace transforms right now and got stuck at this step that involves a weird partial fraction decomposition. It looks like the instructor skipped a bunch of steps and assigned numerators to a bunch of the fractions without assigning them dummy variables. Any idea how he got to this step so quickly? I tried doing this the standard way (i.e. breaking each polynomial in the denominator into its own fraction, assigning variables for each numerator, and trying to solve), but it got messy very quickly. laplace transform up to the part where I got stuck $$ X=\frac{2}{s^4-1}\left(e^{-s}\cdot\frac{1}{s}-e^{-2s}\cdot\frac{1}{s}\right)=\frac{2}{\left(s^4-1\right)s}\left(e^{-s}-e^{-2s}\right)=2\cdot\frac{1}{\left(s^2+1\right)\left(s+1\right)\left(s-1\right)s}\left(e^{-s}-e^{-2s}\right)=2\left(\frac{As+B}{s^2+1}+\frac{\left(\frac{1}{4}\right)}{\left(s+1\right)}+\frac{\left(\frac{1}{4}\right)}{s-1}+\frac{\left(-1\right)}{s}\right)\left(e^{-s}-e^{-2s}\right) $$",,"['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
32,"Matrix ODE, defective eigenvalue: Where does the extra '$t$' come from?","Matrix ODE, defective eigenvalue: Where does the extra '' come from?",t,"Given $A \in \Bbb R^{2\times 2}$, the system   $$ \dot X=AX $$ Has the solution    $$ X= c_1e^{\lambda t}\xi_1+c_2e^{\lambda t}\left (\xi_1 t+\xi_2 \right) $$ Where $\xi_1$ is the unique eigenvector associated to the double eigenvalue $\lambda$ and $\xi_2$ satisfies: $$ (A-\lambda I)\xi_2=\xi_1 $$ I've seen a proof of this fact, but I still don't understand why the second linearly independent solution has that form: Where does the multiplying $t$ come from? I'm interested in a linear algebra perspective of this.","Given $A \in \Bbb R^{2\times 2}$, the system   $$ \dot X=AX $$ Has the solution    $$ X= c_1e^{\lambda t}\xi_1+c_2e^{\lambda t}\left (\xi_1 t+\xi_2 \right) $$ Where $\xi_1$ is the unique eigenvector associated to the double eigenvalue $\lambda$ and $\xi_2$ satisfies: $$ (A-\lambda I)\xi_2=\xi_1 $$ I've seen a proof of this fact, but I still don't understand why the second linearly independent solution has that form: Where does the multiplying $t$ come from? I'm interested in a linear algebra perspective of this.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
33,boundary value problem $-u''+u=\delta_y$ where $u'(0)=u'(1)=0$,boundary value problem  where,-u''+u=\delta_y u'(0)=u'(1)=0,"Consider the boundary value problem (BVP) $$-u''+u=\delta_y\;\text{on}\; I=(0,1)$$ $$u'(0)=u'(1)=0,$$where $y\in I$, $\delta_y:H^1(I)\to\mathbb{R}$ defined by $\delta_y(v)=v(y)$. For all $y\in I$ find a weak solution $u\in H^1(I)$ of the BVP. This means, I have to consider $$\int_0^1{u'(t)v'(t)+u(t)v(t)dt}=v(y)$$ for all $v\in H^1((0, 1))$ and maybe it's useful to consider $(0,y)$ and $(y,1)$ and $\int_0^y{u'(t)v'(t)+u(t)v(t)dt}=v(y)$ and $\int_y^1{u'(t)v'(t)+u(t)v(t)dt}=v(y)$, but I'm not sure. How do I find $u$?","Consider the boundary value problem (BVP) $$-u''+u=\delta_y\;\text{on}\; I=(0,1)$$ $$u'(0)=u'(1)=0,$$where $y\in I$, $\delta_y:H^1(I)\to\mathbb{R}$ defined by $\delta_y(v)=v(y)$. For all $y\in I$ find a weak solution $u\in H^1(I)$ of the BVP. This means, I have to consider $$\int_0^1{u'(t)v'(t)+u(t)v(t)dt}=v(y)$$ for all $v\in H^1((0, 1))$ and maybe it's useful to consider $(0,y)$ and $(y,1)$ and $\int_0^y{u'(t)v'(t)+u(t)v(t)dt}=v(y)$ and $\int_y^1{u'(t)v'(t)+u(t)v(t)dt}=v(y)$, but I'm not sure. How do I find $u$?",,"['functional-analysis', 'ordinary-differential-equations', 'boundary-value-problem']"
34,Find the value of $f(1)$.,Find the value of .,f(1),"How to solve this pde: Let $u(x,y)=2f(y)\cos(x-2y)$ be a solution of the Initial Value Problem $2u_x+u_y=u$; $u(x,0)=\cos(x)$. Then find the value of $f(1)$. By Lagrange's Auxiliary Equations $\dfrac{\operatorname{dx}}{2}=\dfrac{\operatorname{dy}}{1}=\dfrac{\operatorname{du}}{u}$ Hence the solutions are $x=2y+c_1;\ln u=y+c_2;x=\ln u^2+c_3$ But how can I find the required value from here.Please help.","How to solve this pde: Let $u(x,y)=2f(y)\cos(x-2y)$ be a solution of the Initial Value Problem $2u_x+u_y=u$; $u(x,0)=\cos(x)$. Then find the value of $f(1)$. By Lagrange's Auxiliary Equations $\dfrac{\operatorname{dx}}{2}=\dfrac{\operatorname{dy}}{1}=\dfrac{\operatorname{du}}{u}$ Hence the solutions are $x=2y+c_1;\ln u=y+c_2;x=\ln u^2+c_3$ But how can I find the required value from here.Please help.",,"['ordinary-differential-equations', 'initial-value-problems']"
35,Image of a cube under the flow's action,Image of a cube under the flow's action,,"Let's consider a system of ODEs: $$ \dot{x_{1}} = \sin{x_{2}}+x_{1}\\  \dot{x_{2}} = \cos{x_{3}}-2x_{2}+x_{1}\\ \dot{x_{3}}=\arctan{x_{1}}-x_{2}+x_{3}$$ I would like to find an image of the unit cube under the phase flow of the system for the time $t=1$. While dealing with linear systems $\dot{x} = Ax$, $A \in Mat_{n \times n}{\mathbb{C}}$, it can be done directly -- by  finding out the general solution of a system $(x(t), y(t))^{T} = e^{tA} \cdot (x_{0}, y_{0})^{T}$ (here it's 2-dimensional case) and considering a phase curve, passing through the vertex of the cube, given in the form $(x(t), y(t))^{T} = e^{tA} \cdot (1, 0)^{T}$ (assume that $(1, 0)$ is a vertex of a cube). Putting $t=1$, we'll get the image of $(1, 0)$  under the flow of a system for the time $t=1$. By the linearity of a flow and convexity of  a cube , i.e. any $x \in I^{n}$ can be written as $x = \alpha_{1}x_{1} + \alpha_{2} x_{2} + \ldots + \alpha_{2^{n}} x_{2^{n}} = \sum_{n=0}^{2^{n}}{\alpha_{n}x_{n}}, \sum_{n=0}^{2^{n}}{\alpha_{n}}=1$, then $f(x) = \sum_{n=0}^{2^{n}}{\alpha_{n}f(x_{n})}$, so it's sufficient to find images of vertices. Well, but how to proceed in non-linear case. Surely, solving that system solves the exact problem, but can it be proceed in a relatively 'good' way? Actually, seems that it's possible to linearize the system, by  exchanging transcendental functions with their expansions in the neighborhood of current points but i'm not sure, whether it works, even if it does work -- it would be wonderful to find out a rigorous  justification. Any help would be much appreciated.","Let's consider a system of ODEs: $$ \dot{x_{1}} = \sin{x_{2}}+x_{1}\\  \dot{x_{2}} = \cos{x_{3}}-2x_{2}+x_{1}\\ \dot{x_{3}}=\arctan{x_{1}}-x_{2}+x_{3}$$ I would like to find an image of the unit cube under the phase flow of the system for the time $t=1$. While dealing with linear systems $\dot{x} = Ax$, $A \in Mat_{n \times n}{\mathbb{C}}$, it can be done directly -- by  finding out the general solution of a system $(x(t), y(t))^{T} = e^{tA} \cdot (x_{0}, y_{0})^{T}$ (here it's 2-dimensional case) and considering a phase curve, passing through the vertex of the cube, given in the form $(x(t), y(t))^{T} = e^{tA} \cdot (1, 0)^{T}$ (assume that $(1, 0)$ is a vertex of a cube). Putting $t=1$, we'll get the image of $(1, 0)$  under the flow of a system for the time $t=1$. By the linearity of a flow and convexity of  a cube , i.e. any $x \in I^{n}$ can be written as $x = \alpha_{1}x_{1} + \alpha_{2} x_{2} + \ldots + \alpha_{2^{n}} x_{2^{n}} = \sum_{n=0}^{2^{n}}{\alpha_{n}x_{n}}, \sum_{n=0}^{2^{n}}{\alpha_{n}}=1$, then $f(x) = \sum_{n=0}^{2^{n}}{\alpha_{n}f(x_{n})}$, so it's sufficient to find images of vertices. Well, but how to proceed in non-linear case. Surely, solving that system solves the exact problem, but can it be proceed in a relatively 'good' way? Actually, seems that it's possible to linearize the system, by  exchanging transcendental functions with their expansions in the neighborhood of current points but i'm not sure, whether it works, even if it does work -- it would be wonderful to find out a rigorous  justification. Any help would be much appreciated.",,"['ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
36,How to solve differential equation $y'+\frac xy=x^2y^3$,How to solve differential equation,y'+\frac xy=x^2y^3,"I've tried to let $u=y^2$, and got $$u'+2x=2x^2u^2,$$ but I still can't solve it.","I've tried to let $u=y^2$, and got $$u'+2x=2x^2u^2,$$ but I still can't solve it.",,['ordinary-differential-equations']
37,Cahn-Hilliard equation,Cahn-Hilliard equation,,"Is there any possibility to write the so-called Cahn-Hilliard equation: $\frac{\partial c}{\partial t}=D\nabla^2(c^3-c-\gamma\nabla^2c)$, in terms of two coupled partial differential equations (second order)? (even in $1D$ case, i.e., $c(x,t)$)","Is there any possibility to write the so-called Cahn-Hilliard equation: $\frac{\partial c}{\partial t}=D\nabla^2(c^3-c-\gamma\nabla^2c)$, in terms of two coupled partial differential equations (second order)? (even in $1D$ case, i.e., $c(x,t)$)",,['ordinary-differential-equations']
38,Matrix Differential Equation $P'(t)= A(t)P(t)$,Matrix Differential Equation,P'(t)= A(t)P(t),"Given $n\times n$ matrix $P(t)$ and $A(t)$ , if $P(t)$ satisfies the matrix differential equation $P'(t)= A(t)P(t)$ and the initial condition $P(0)=P_0$. Then Prove    $$\det P(t) = \det P_0 \times \exp\left(\int_0^t tr(A)(s)\,ds\right)$$ If the matrix $A$ is constant matrix then it is easy. But I don't know how to prove it when $A = A(t)$ which has dependent on variable $t$. Please help...","Given $n\times n$ matrix $P(t)$ and $A(t)$ , if $P(t)$ satisfies the matrix differential equation $P'(t)= A(t)P(t)$ and the initial condition $P(0)=P_0$. Then Prove    $$\det P(t) = \det P_0 \times \exp\left(\int_0^t tr(A)(s)\,ds\right)$$ If the matrix $A$ is constant matrix then it is easy. But I don't know how to prove it when $A = A(t)$ which has dependent on variable $t$. Please help...",,"['ordinary-differential-equations', 'matrix-calculus']"
39,Does Globally Lipchitz prove a solution exists for all time?,Does Globally Lipchitz prove a solution exists for all time?,,"From ODE I learned if $g$ is Lipchitz on $\mathbb{R}^n$ there exists a unique solution $y:\mathbb{R} \Rightarrow \mathbb{R}^n$ to the IVP  \begin{eqnarray} y' &=& g(y)\\ y(t_0) &=& y_0 \end{eqnarray} where $t_0=0$ From what I understand, since $g$ is globally Lipchitz, then it is locally Lipchitz. From the fact it is locally Lipchitz, I can prove there exist a $c>0$ such that our IVP has a unique solution $y(t)$ on a small interval $[-c,c]$. What I don't understand is, how does a Globally Lipchitz function implies there exist a unique solution on $\mathbb{R}$? The only thing that comes to mind is this, (continuing from what we previously established) since $g$ is Globally Lipchitz and we have proven there exist a unique solution $y(t)$ on $[-c,c]$, I can pick a $t_1>0\in [-c,c]$ such that we have a new IVP  \begin{eqnarray} y' &=& g(y)\\ y(t_1) &=& y_1 \end{eqnarray} Afterwards, it can be proven that this new IVP has a unique solution on some interval $[-c_1+t_1,c_1+t_1]$. Then you would continue iterating this procedure (making sure to pick $t_i>t_{i-1}$) until you construct $[c,\infty]$. You would apply a similar argument to construct the other half of our interval to get the complete $\mathbb{R}$. My only issue is, what if (let's say) after some large $n$, $[-c_n+t_n,c_n+t_n]$ becomes so small that the maximal interval converges to a fixed point and doesn't reach $\infty$?  How would you prevent that issue from happening? I'm new to the forum so I hope my question wasn't to vague. If you need me to clarify, please let me know. Thanks for the help.","From ODE I learned if $g$ is Lipchitz on $\mathbb{R}^n$ there exists a unique solution $y:\mathbb{R} \Rightarrow \mathbb{R}^n$ to the IVP  \begin{eqnarray} y' &=& g(y)\\ y(t_0) &=& y_0 \end{eqnarray} where $t_0=0$ From what I understand, since $g$ is globally Lipchitz, then it is locally Lipchitz. From the fact it is locally Lipchitz, I can prove there exist a $c>0$ such that our IVP has a unique solution $y(t)$ on a small interval $[-c,c]$. What I don't understand is, how does a Globally Lipchitz function implies there exist a unique solution on $\mathbb{R}$? The only thing that comes to mind is this, (continuing from what we previously established) since $g$ is Globally Lipchitz and we have proven there exist a unique solution $y(t)$ on $[-c,c]$, I can pick a $t_1>0\in [-c,c]$ such that we have a new IVP  \begin{eqnarray} y' &=& g(y)\\ y(t_1) &=& y_1 \end{eqnarray} Afterwards, it can be proven that this new IVP has a unique solution on some interval $[-c_1+t_1,c_1+t_1]$. Then you would continue iterating this procedure (making sure to pick $t_i>t_{i-1}$) until you construct $[c,\infty]$. You would apply a similar argument to construct the other half of our interval to get the complete $\mathbb{R}$. My only issue is, what if (let's say) after some large $n$, $[-c_n+t_n,c_n+t_n]$ becomes so small that the maximal interval converges to a fixed point and doesn't reach $\infty$?  How would you prevent that issue from happening? I'm new to the forum so I hope my question wasn't to vague. If you need me to clarify, please let me know. Thanks for the help.",,"['real-analysis', 'ordinary-differential-equations', 'lipschitz-functions']"
40,Solve the differential equation for obtaining $x$ as a relation of $t$: $\frac{d^2x}{dt^2}=\alpha\sqrt{x}$,Solve the differential equation for obtaining  as a relation of :,x t \frac{d^2x}{dt^2}=\alpha\sqrt{x},Question: Solve the differential equation for obtaining $x$ as a relation of $t$: $$\frac{d^2x}{dt^2}=\alpha\sqrt{x}$$ My attempt: $$\frac{d^2x}{dt^2}=\alpha\sqrt{x}$$ $$\Rightarrow 2\frac{dx}{dt}\frac{d^2x}{dt^2}=\alpha\sqrt{x}\cdot 2\frac{dx}{dt}$$ $$\Rightarrow \frac{d}{dt}\left[\left(\frac{dx}{dt}\right)^2\right]=\frac{4}{3}\alpha\cdot \frac{d}{dt}\left(x^\frac{3}{2}\right)$$ $$\Rightarrow \left(\frac{dx}{dt}\right)^2=\frac{4}{3}\alpha x^\frac{3}{2}+c_1$$ Now we have $$\frac{dx}{dt}=\sqrt{\frac{4}{3}\alpha x^\frac{3}{2}+c_1}=\sqrt{k x^\frac{3}{2}+c_1}$$ $$\Rightarrow \frac{dx}{\sqrt{k x^\frac{3}{2}+c_1}}=dt$$ Can anyone suggest how to proceed? Any substitutions?,Question: Solve the differential equation for obtaining $x$ as a relation of $t$: $$\frac{d^2x}{dt^2}=\alpha\sqrt{x}$$ My attempt: $$\frac{d^2x}{dt^2}=\alpha\sqrt{x}$$ $$\Rightarrow 2\frac{dx}{dt}\frac{d^2x}{dt^2}=\alpha\sqrt{x}\cdot 2\frac{dx}{dt}$$ $$\Rightarrow \frac{d}{dt}\left[\left(\frac{dx}{dt}\right)^2\right]=\frac{4}{3}\alpha\cdot \frac{d}{dt}\left(x^\frac{3}{2}\right)$$ $$\Rightarrow \left(\frac{dx}{dt}\right)^2=\frac{4}{3}\alpha x^\frac{3}{2}+c_1$$ Now we have $$\frac{dx}{dt}=\sqrt{\frac{4}{3}\alpha x^\frac{3}{2}+c_1}=\sqrt{k x^\frac{3}{2}+c_1}$$ $$\Rightarrow \frac{dx}{\sqrt{k x^\frac{3}{2}+c_1}}=dt$$ Can anyone suggest how to proceed? Any substitutions?,,"['calculus', 'integration', 'ordinary-differential-equations', 'kinematics']"
41,Exact solution of Second order ODE,Exact solution of Second order ODE,,"We have the second order differential equation $\epsilon \dfrac{d^{2}y}{dx^{2}} + \dfrac{dy}{dx} +y = 0$ with boundary values $y(0)=0,\, \, \,  y(1)=1$. I would like to get the exact solution in the form $$y(x) = C \exp(\alpha x)\sinh(\beta x)$$ with $\alpha, \beta$ and $C$ as constants. I'm too sure how to go about this, I have tried to substitute the solution form into the differential equation but I don't think I am going in the right direction.","We have the second order differential equation $\epsilon \dfrac{d^{2}y}{dx^{2}} + \dfrac{dy}{dx} +y = 0$ with boundary values $y(0)=0,\, \, \,  y(1)=1$. I would like to get the exact solution in the form $$y(x) = C \exp(\alpha x)\sinh(\beta x)$$ with $\alpha, \beta$ and $C$ as constants. I'm too sure how to go about this, I have tried to substitute the solution form into the differential equation but I don't think I am going in the right direction.",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
42,"Looking for a Simple Argument for ""Integral Curve Starting at A Singular Point is Constant""","Looking for a Simple Argument for ""Integral Curve Starting at A Singular Point is Constant""",,"Let $U$ be an open subset of $\mathbf R^n$ and $V:U\to\mathbf R^n$ be a differentiable vector field on $U$. Let $\mathbf p\in U$ be a singular point of $V$, that is, $V(\mathbf p)=\mathbf 0$. Then the only integral curve which starts at $\mathbf p$ is the constant curve. I know that one could simply use the theorem of ""uniqueness of integral curves"" and in fact adapt the same proof for this particular case. But is anybody aware of a simple argument for this?","Let $U$ be an open subset of $\mathbf R^n$ and $V:U\to\mathbf R^n$ be a differentiable vector field on $U$. Let $\mathbf p\in U$ be a singular point of $V$, that is, $V(\mathbf p)=\mathbf 0$. Then the only integral curve which starts at $\mathbf p$ is the constant curve. I know that one could simply use the theorem of ""uniqueness of integral curves"" and in fact adapt the same proof for this particular case. But is anybody aware of a simple argument for this?",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
43,Laplace transform of $\cos^2(\omega t)$,Laplace transform of,\cos^2(\omega t),"Find the Laplace Transform of $\cos^2(\omega t)$, where $\omega$ is a constant. From a cosine identity: $cos^2(\omega t) = \frac{1}{2}(1+\cos(2\omega t))$. So then I get: \begin{align} \mathcal{L}(\cos^2(\omega t)) & = \frac{1}{2}\mathcal{L}(1+\cos(2\omega t))\\  &=\frac{1}{2}(\mathcal{L}(1) + \mathcal{L}(\cos(2\omega t)))\\ & =\frac{1}{2}\left(\frac{1}{s} + \mathcal{L}(\cos(2\omega t))\right) \end{align} This is the part where I'm unsure how to proceed. Can I use the fact that $\mathcal{L}(\cos(\omega t)) = \frac{s}{s^2+\omega^2}$ to get $\mathcal{L}(\cos(2\omega t)) = \frac{s}{s^2 + (2 \omega)^2}$ or do I have to use the definition of Laplace transformation and do the integration to solve this?","Find the Laplace Transform of $\cos^2(\omega t)$, where $\omega$ is a constant. From a cosine identity: $cos^2(\omega t) = \frac{1}{2}(1+\cos(2\omega t))$. So then I get: \begin{align} \mathcal{L}(\cos^2(\omega t)) & = \frac{1}{2}\mathcal{L}(1+\cos(2\omega t))\\  &=\frac{1}{2}(\mathcal{L}(1) + \mathcal{L}(\cos(2\omega t)))\\ & =\frac{1}{2}\left(\frac{1}{s} + \mathcal{L}(\cos(2\omega t))\right) \end{align} This is the part where I'm unsure how to proceed. Can I use the fact that $\mathcal{L}(\cos(\omega t)) = \frac{s}{s^2+\omega^2}$ to get $\mathcal{L}(\cos(2\omega t)) = \frac{s}{s^2 + (2 \omega)^2}$ or do I have to use the definition of Laplace transformation and do the integration to solve this?",,"['calculus', 'integration', 'ordinary-differential-equations', 'definite-integrals', 'laplace-transform']"
44,First-order nonlinear ordinary differential equation $f'(t)=\frac{1+t}{2t^2+f^2(t)}$,First-order nonlinear ordinary differential equation,f'(t)=\frac{1+t}{2t^2+f^2(t)},"I don't know how to solve this Cauchy problem, \begin{cases} f'(t)=\frac{1+t}{2t^2+f^2(t)} \\ f(0)=1 \end{cases} I tried by applying all theorems and definitions I know, I tried to consider that if I can show that it is limited then the solution exists on all $\mathbb{R}$, but I don't know how to show that. Then I thought that if the derivative is limited, I can prove the local existence... Can anyone help me? Thank you very much","I don't know how to solve this Cauchy problem, \begin{cases} f'(t)=\frac{1+t}{2t^2+f^2(t)} \\ f(0)=1 \end{cases} I tried by applying all theorems and definitions I know, I tried to consider that if I can show that it is limited then the solution exists on all $\mathbb{R}$, but I don't know how to show that. Then I thought that if the derivative is limited, I can prove the local existence... Can anyone help me? Thank you very much",,"['calculus', 'ordinary-differential-equations']"
45,Initial values problem with absolute value,Initial values problem with absolute value,,"I've some doubts about initial values problems involving differential equation with absolute values. For example if I have a differential equation like $y'=|x+1|$ with initial condition $y(3)=-2$, since $3>-1$ I can trascurate the absolute value and solve $y'=x+1$, is it correct? But if the condition is for instance $y(-1)=2$ then I must consider the two different cases? That is, $y'=x+1$ if $x>-1$ and $y'=-x-1$ if $x<-1$ Is this the right way to solve this kind of problems?","I've some doubts about initial values problems involving differential equation with absolute values. For example if I have a differential equation like $y'=|x+1|$ with initial condition $y(3)=-2$, since $3>-1$ I can trascurate the absolute value and solve $y'=x+1$, is it correct? But if the condition is for instance $y(-1)=2$ then I must consider the two different cases? That is, $y'=x+1$ if $x>-1$ and $y'=-x-1$ if $x<-1$ Is this the right way to solve this kind of problems?",,"['calculus', 'ordinary-differential-equations', 'absolute-value', 'initial-value-problems']"
46,"Regularity of solutions: the ODE $u'=|u|^2u$, and the semilinear heat equation with cubic nonlinearity","Regularity of solutions: the ODE , and the semilinear heat equation with cubic nonlinearity",u'=|u|^2u,"Suppose $u\in C^1(I\to\mathbb{C})$, where $I\subset\mathbb{R}$, is a solution to the ODE \begin{cases} u'(t) = |u|^2u \\ u(0) = a \in\mathbb{C}. \end{cases} Without solving the ODE, can we way that $u\in C^{\infty}(I)$? Is it true if the right hand side is any polynomial of $u$? If I expand this to a PDE, say $u\in C^1_tW^{5,2}_x(I\times\mathbb{R}^n\to\mathbb{C})$ is a solution to the PDE \begin{cases} \partial_t u = \Delta_x u + |u|^2u \\ u(0,x) = u_0 \in W^{5,2}, \end{cases} then can I say $u\in C^{\infty}_t$?","Suppose $u\in C^1(I\to\mathbb{C})$, where $I\subset\mathbb{R}$, is a solution to the ODE \begin{cases} u'(t) = |u|^2u \\ u(0) = a \in\mathbb{C}. \end{cases} Without solving the ODE, can we way that $u\in C^{\infty}(I)$? Is it true if the right hand side is any polynomial of $u$? If I expand this to a PDE, say $u\in C^1_tW^{5,2}_x(I\times\mathbb{R}^n\to\mathbb{C})$ is a solution to the PDE \begin{cases} \partial_t u = \Delta_x u + |u|^2u \\ u(0,x) = u_0 \in W^{5,2}, \end{cases} then can I say $u\in C^{\infty}_t$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation', 'regularity-theory-of-pdes']"
47,Geometric interpretation of Q in Lyapunov's equation,Geometric interpretation of Q in Lyapunov's equation,,"Lyapunov's equation says: given any $Q > 0$ ($Q$ positive definite) there is $P > 0$ such that $A^T P + P A + Q = 0$ if and only if for $\frac{dx(t)}{dt}=A x(t)$ it is the case that the real part of each eigenvalue of $A$ is negative. Then, the ellipsoid $x^T P x \leq 1$ is an invariant of $\frac{dx(t)}{dt}=A x(t)$. The geometric interpretation of $P$ is such that the eigenvectors of $P$ form the principal axes of the ellipsoid and each eigenvalue is related to the length of the ellipsoid along the axis represented by the corresponding eigenvector. What is the geometric interpretation of $Q$ resp. how does the choice of $Q$ affect $P$?","Lyapunov's equation says: given any $Q > 0$ ($Q$ positive definite) there is $P > 0$ such that $A^T P + P A + Q = 0$ if and only if for $\frac{dx(t)}{dt}=A x(t)$ it is the case that the real part of each eigenvalue of $A$ is negative. Then, the ellipsoid $x^T P x \leq 1$ is an invariant of $\frac{dx(t)}{dt}=A x(t)$. The geometric interpretation of $P$ is such that the eigenvectors of $P$ form the principal axes of the ellipsoid and each eigenvalue is related to the length of the ellipsoid along the axis represented by the corresponding eigenvector. What is the geometric interpretation of $Q$ resp. how does the choice of $Q$ affect $P$?",,"['linear-algebra', 'ordinary-differential-equations', 'control-theory', 'semidefinite-programming', 'linear-control']"
48,"Stable subspace, unstable subspace, centre subspace: Of which kind of stability are we talking?","Stable subspace, unstable subspace, centre subspace: Of which kind of stability are we talking?",,"Let $x'(t)=Ax(t)$ be a linear ODE system. Then the span of the eigenvectors belonging to eigenvalues with negative real-part is called the stable subspace, the subspace spanned by the eigenvectors of eigenvalues of positive real-part is called unstable subspace and the subspace spanned by the eigenvectors belonging to eigenvalues with zero real-part is called centre subspace. What kind of stability is the stable subspace of? What kind of unstability (as negation of what kind of stability) is the unstable subspace of? Is the centre subspace stable or unstable? Are there situation in which it is stable (and which kind of stability then)?","Let $x'(t)=Ax(t)$ be a linear ODE system. Then the span of the eigenvectors belonging to eigenvalues with negative real-part is called the stable subspace, the subspace spanned by the eigenvectors of eigenvalues of positive real-part is called unstable subspace and the subspace spanned by the eigenvectors belonging to eigenvalues with zero real-part is called centre subspace. What kind of stability is the stable subspace of? What kind of unstability (as negation of what kind of stability) is the unstable subspace of? Is the centre subspace stable or unstable? Are there situation in which it is stable (and which kind of stability then)?",,"['ordinary-differential-equations', 'stability-in-odes']"
49,converge of ODE solution,converge of ODE solution,,"for x(t) solution of $\dot{x}=f(x)$, f(x)  differentiable and the derivative continuous. show that $lim_{t\to{\infty}}x(t)=+-\infty $ or $lim_{t\to{\infty}}x(t)=$stationary point it can visualized through a graph but Im not sure how to prove it in a formal way thanks ahead","for x(t) solution of $\dot{x}=f(x)$, f(x)  differentiable and the derivative continuous. show that $lim_{t\to{\infty}}x(t)=+-\infty $ or $lim_{t\to{\infty}}x(t)=$stationary point it can visualized through a graph but Im not sure how to prove it in a formal way thanks ahead",,"['ordinary-differential-equations', 'derivatives']"
50,"Dirichlet's Energy on $X:=\{u\in C^1([0,1]) |\ u(0)=0, u'(1)=1\}$?",Dirichlet's Energy on ?,"X:=\{u\in C^1([0,1]) |\ u(0)=0, u'(1)=1\}","Let $X:=\{u\in C^1([0,1]) |\ u(0)=0, u'(1)=1\}$. I want to show that  $J(u):= \int_0^1 (u'(x))^2dx$ doesn't have an infimum on $X$. Hi, this looks an awful lot like an application for Dirichlet's energy. But I don't even know if $u \in C^2(0,1)$. If I assume that it is, then an infimum would be a solution of the problem $u''=0$ implying that $u'=c$ and subsequently $u'=1$. If $u'=1$, then $u(x)=x$ if I want $u(0)=0$. But where is the contradiction? Thank you","Let $X:=\{u\in C^1([0,1]) |\ u(0)=0, u'(1)=1\}$. I want to show that  $J(u):= \int_0^1 (u'(x))^2dx$ doesn't have an infimum on $X$. Hi, this looks an awful lot like an application for Dirichlet's energy. But I don't even know if $u \in C^2(0,1)$. If I assume that it is, then an infimum would be a solution of the problem $u''=0$ implying that $u'=c$ and subsequently $u'=1$. If $u'=1$, then $u(x)=x$ if I want $u(0)=0$. But where is the contradiction? Thank you",,"['real-analysis', 'ordinary-differential-equations', 'calculus-of-variations']"
51,Find a second solution of the Legendre differential equation,Find a second solution of the Legendre differential equation,,"The Legendre differential equation is known as: $ (1-t^2) y'' - 2ty' + n(n+1)y = 0$ I know there is a relation $y(t) = c_1 P_n (t) + c_2 Q_n (t)$, where $P_n (t)$ is the Legendre polynomial. Given that $P_0 (t) = 1$ is a solution, how can I find $Q_0 (t)$ directly from the diffential equation for $n=0$, where $Q_0 (t)$ is a second independent solution? (I already know that $Q_0 (t) = \frac{1}{2} \ln(\frac{1+t}{1-t})$)","The Legendre differential equation is known as: $ (1-t^2) y'' - 2ty' + n(n+1)y = 0$ I know there is a relation $y(t) = c_1 P_n (t) + c_2 Q_n (t)$, where $P_n (t)$ is the Legendre polynomial. Given that $P_0 (t) = 1$ is a solution, how can I find $Q_0 (t)$ directly from the diffential equation for $n=0$, where $Q_0 (t)$ is a second independent solution? (I already know that $Q_0 (t) = \frac{1}{2} \ln(\frac{1+t}{1-t})$)",,['ordinary-differential-equations']
52,System of ordinary differential equations,System of ordinary differential equations,,"I need help to sketch the trajectories in the phase plane for this system of ODE: $$\begin{array}{|l} &x'=y  \\ &y'= \beta x-\alpha(1-x^2)y \end{array} $$ where $\alpha$ and $\beta$ are positive constants with $x,y \ge 0$. I already found that the origin is the steady point of the system.","I need help to sketch the trajectories in the phase plane for this system of ODE: $$\begin{array}{|l} &x'=y  \\ &y'= \beta x-\alpha(1-x^2)y \end{array} $$ where $\alpha$ and $\beta$ are positive constants with $x,y \ge 0$. I already found that the origin is the steady point of the system.",,['ordinary-differential-equations']
53,How to find a sensible approximation of $R\dot{\theta}^2+\ddot{\theta}(R\theta-l)+g\cos\theta=0$,How to find a sensible approximation of,R\dot{\theta}^2+\ddot{\theta}(R\theta-l)+g\cos\theta=0,"The differential equation  $$R\dot{\theta}^2+\ddot{\theta}(R\theta-l)+g\cos\theta=0$$ is derived from considering a pendulum attached to the uppermost part of a disk. As in the picture above (but remember the string is attached to the intersection with the y-axis). Anyways, I was approximating this by considering small oscillations, as usual. To end up with something like $$\ddot{\theta}+\frac{R\dot{\theta}}{R\theta_0-l}+\frac{g\cos\theta_0}{R\theta_0-l}=0$$ But this has a complex solution, and a real valued log function, which doesn't seem like an oscillation at all or physically sound. The solution manual to my book approximates the equation as $$\ddot{\epsilon}+\frac{g\sin\theta_0}{l-R\theta_0}\epsilon=\frac{g\cos\theta_0}{l-R\theta_0}$$ Where $\epsilon=\theta-\theta_0$. The equation doesn't make sense to me, especially the second term on the LHS. So I was wondering if you guys could help me. Perhaps give me a hint or two... Or perhaps tell me what the author of the solutions manual did? Thanks!","The differential equation  $$R\dot{\theta}^2+\ddot{\theta}(R\theta-l)+g\cos\theta=0$$ is derived from considering a pendulum attached to the uppermost part of a disk. As in the picture above (but remember the string is attached to the intersection with the y-axis). Anyways, I was approximating this by considering small oscillations, as usual. To end up with something like $$\ddot{\theta}+\frac{R\dot{\theta}}{R\theta_0-l}+\frac{g\cos\theta_0}{R\theta_0-l}=0$$ But this has a complex solution, and a real valued log function, which doesn't seem like an oscillation at all or physically sound. The solution manual to my book approximates the equation as $$\ddot{\epsilon}+\frac{g\sin\theta_0}{l-R\theta_0}\epsilon=\frac{g\cos\theta_0}{l-R\theta_0}$$ Where $\epsilon=\theta-\theta_0$. The equation doesn't make sense to me, especially the second term on the LHS. So I was wondering if you guys could help me. Perhaps give me a hint or two... Or perhaps tell me what the author of the solutions manual did? Thanks!",,"['ordinary-differential-equations', 'physics']"
54,Finding particular integral to $ (D^3 + 4D)y = \sin2x$,Finding particular integral to, (D^3 + 4D)y = \sin2x,The question is to find particular integral of this differential equation $$ (D^3 + 4D)y = \sin2x.$$ where $$ D = \frac{d}{dx} $$ Please looking for your quick tip guys.,The question is to find particular integral of this differential equation $$ (D^3 + 4D)y = \sin2x.$$ where $$ D = \frac{d}{dx} $$ Please looking for your quick tip guys.,,['ordinary-differential-equations']
55,Find a particular solution of a nonhomogenous equation,Find a particular solution of a nonhomogenous equation,,"Find a particular solution to $y''-2y' + y= {e^{px}}$ where p is any real constant. My attempt/idea is as follows: Since $Y_p$ is ${e^{px}}$ and it appears in the complementary solution, $Y_c$ is $c_1{e^x} + c_2x{e^x}$ , we will have to multiply $Y_p$ by $x^2$ so that $Y_p$ becomes $A{x^2}e^x$ Then, find $Y_p'$ and $Y_p''$ to get: $2Ae^x = e^{px}$ $2A = 1$ $A= \frac{1}{2}$ Then, $Y_p = \frac{1}{2}x^2e^{px}$ Is this right?","Find a particular solution to $y''-2y' + y= {e^{px}}$ where p is any real constant. My attempt/idea is as follows: Since $Y_p$ is ${e^{px}}$ and it appears in the complementary solution, $Y_c$ is $c_1{e^x} + c_2x{e^x}$ , we will have to multiply $Y_p$ by $x^2$ so that $Y_p$ becomes $A{x^2}e^x$ Then, find $Y_p'$ and $Y_p''$ to get: $2Ae^x = e^{px}$ $2A = 1$ $A= \frac{1}{2}$ Then, $Y_p = \frac{1}{2}x^2e^{px}$ Is this right?",,['ordinary-differential-equations']
56,Comparison of Wronskian at three different points,Comparison of Wronskian at three different points,,"Let $P$ be a continuous function on $\mathbb R$ and $W$ the Wronskian of two linearly independent solutions $y_1$ and $y_2$ of the ODE:   $$\dfrac{d^2y}{dx^2}+(1+x^2)\dfrac{dy}{dx}+ P(x)y=0,x\in\mathbb R$$   Let $W(1)=a,W(2)=b$ and $W(3)=c$, then $a<0$ and $b>0$ $a<b<c$ or $a>b>c$ $\frac a{|a|}=\frac b{|b|}=\frac c{|c|}$ $0<a<b$ and $b>c>0$ Attempt: By Abel's identity, for ODE $$y''+p(x)y'+q(x)=0$$  with solutions $y_1,y_2$ $$W_{(y_1,y_2)}(x)=W_{(y_1,y_2)}(x_0)e^{-\int\limits_{x_0}^xp(\zeta)d\zeta}$$ Playing around, let $x_0=1$, $p(x)=(1+x^2)$ $$W_{(y_1,y_2)}(2)=ae^{-\frac{10}3}=b$$ and  $$W_{(y_1,y_2)}(3)=ae^{-\frac{32}3}=c$$ Observations: $a,b,c$ are of same sign, depending on sign of $a$ - either positive or negative $a<b<c$ or $a>b>c$ depending on sign of $a$ Suggesting 2 and 3 are the right options. Doubts: Am I correct in choosing $x_0=1$ ( since the theorem says for every $x_0\in I$ )? If not, how is $x_0$ chosen? If my choice of $x_0$ is wrong, am right in guessing that the observations I made stand with the correct $x_0$? DE being one of my weak points: did I go wrong somewhere else/in observations I made?","Let $P$ be a continuous function on $\mathbb R$ and $W$ the Wronskian of two linearly independent solutions $y_1$ and $y_2$ of the ODE:   $$\dfrac{d^2y}{dx^2}+(1+x^2)\dfrac{dy}{dx}+ P(x)y=0,x\in\mathbb R$$   Let $W(1)=a,W(2)=b$ and $W(3)=c$, then $a<0$ and $b>0$ $a<b<c$ or $a>b>c$ $\frac a{|a|}=\frac b{|b|}=\frac c{|c|}$ $0<a<b$ and $b>c>0$ Attempt: By Abel's identity, for ODE $$y''+p(x)y'+q(x)=0$$  with solutions $y_1,y_2$ $$W_{(y_1,y_2)}(x)=W_{(y_1,y_2)}(x_0)e^{-\int\limits_{x_0}^xp(\zeta)d\zeta}$$ Playing around, let $x_0=1$, $p(x)=(1+x^2)$ $$W_{(y_1,y_2)}(2)=ae^{-\frac{10}3}=b$$ and  $$W_{(y_1,y_2)}(3)=ae^{-\frac{32}3}=c$$ Observations: $a,b,c$ are of same sign, depending on sign of $a$ - either positive or negative $a<b<c$ or $a>b>c$ depending on sign of $a$ Suggesting 2 and 3 are the right options. Doubts: Am I correct in choosing $x_0=1$ ( since the theorem says for every $x_0\in I$ )? If not, how is $x_0$ chosen? If my choice of $x_0$ is wrong, am right in guessing that the observations I made stand with the correct $x_0$? DE being one of my weak points: did I go wrong somewhere else/in observations I made?",,['ordinary-differential-equations']
57,The general solution for inhomogeneous differential equation,The general solution for inhomogeneous differential equation,,"I am working with the following inhomogeneous differential equation, $$x''+x=3\cos (\omega t)$$ The general solution for this is $x(t)=x_h(t)+x_p(t)$ First step is to find $x_h(t):$ So the characteristic equation is, $$\lambda^2+0 \lambda+1=0$$  and its roots are $$\lambda =\frac{\sqrt{-4}}{2}=\frac{i\sqrt{4}}{2}=\pm i$$  So $$x_h(t)=c_1 \cos(t)+c_2 \sin(t)$$ Second step is to find $x_p(t):$ My guess will be, $$x_p(t)=A \cos(\omega t)+B \sin(\omega t)$$ Now I take the derivative of my guess. $$x_p'(t)=-A \sin(\omega t) \cdot \omega+B \cos(\omega t) \cdot \omega$$ $$x_p''(t)= -A \cos(\omega t) \cdot \omega^2-B \sin(\omega t) \cdot \omega^2$$ The I have replace it in the equation $$-A \cos(\omega t) \cdot \omega^2-B \sin(\omega t) \cdot \omega^2 + A \cos(\omega t)+B \sin(\omega t) = 3 \cos(\omega t)$$ Since we have no $\sin(\omega t)$ on the RHS the B must be $0$ on the LHS. Then $$-A \cos(\omega t) \cdot \omega^2 - A \cos(\omega t) = 3 \cos(\omega t)$$ And isolate A $$A\big(- \cos(\omega t) \cdot \omega^2-\cos(\omega t)\big)=3 \cos(\omega t)$$ $$A=\frac{3 \cos(\omega t)}{- \cos(\omega t) \cdot \omega^2-\cos(\omega t)}$$ $$A=\frac{3 \cos(\omega t)}{\cos(\omega t)\big(-\omega^2-1 \big)}$$ $$A=\frac{3}{-\omega^2-1}$$ Then replace this into our guess $$x_p(t)=\frac{3}{-\omega^2-1} \cos(\omega t)$$ $$x_p(t)=\frac{3 \cos(\omega t)}{-\omega^2-1}$$ Last the general solution is  $$x(t)=c_1 \cos(t)+c_2 \sin(t)-\frac{3 \cos(\omega t)}{\omega^2-1}$$","I am working with the following inhomogeneous differential equation, $$x''+x=3\cos (\omega t)$$ The general solution for this is $x(t)=x_h(t)+x_p(t)$ First step is to find $x_h(t):$ So the characteristic equation is, $$\lambda^2+0 \lambda+1=0$$  and its roots are $$\lambda =\frac{\sqrt{-4}}{2}=\frac{i\sqrt{4}}{2}=\pm i$$  So $$x_h(t)=c_1 \cos(t)+c_2 \sin(t)$$ Second step is to find $x_p(t):$ My guess will be, $$x_p(t)=A \cos(\omega t)+B \sin(\omega t)$$ Now I take the derivative of my guess. $$x_p'(t)=-A \sin(\omega t) \cdot \omega+B \cos(\omega t) \cdot \omega$$ $$x_p''(t)= -A \cos(\omega t) \cdot \omega^2-B \sin(\omega t) \cdot \omega^2$$ The I have replace it in the equation $$-A \cos(\omega t) \cdot \omega^2-B \sin(\omega t) \cdot \omega^2 + A \cos(\omega t)+B \sin(\omega t) = 3 \cos(\omega t)$$ Since we have no $\sin(\omega t)$ on the RHS the B must be $0$ on the LHS. Then $$-A \cos(\omega t) \cdot \omega^2 - A \cos(\omega t) = 3 \cos(\omega t)$$ And isolate A $$A\big(- \cos(\omega t) \cdot \omega^2-\cos(\omega t)\big)=3 \cos(\omega t)$$ $$A=\frac{3 \cos(\omega t)}{- \cos(\omega t) \cdot \omega^2-\cos(\omega t)}$$ $$A=\frac{3 \cos(\omega t)}{\cos(\omega t)\big(-\omega^2-1 \big)}$$ $$A=\frac{3}{-\omega^2-1}$$ Then replace this into our guess $$x_p(t)=\frac{3}{-\omega^2-1} \cos(\omega t)$$ $$x_p(t)=\frac{3 \cos(\omega t)}{-\omega^2-1}$$ Last the general solution is  $$x(t)=c_1 \cos(t)+c_2 \sin(t)-\frac{3 \cos(\omega t)}{\omega^2-1}$$",,"['calculus', 'ordinary-differential-equations']"
58,How to solve this system of nonlinear ODE's?,How to solve this system of nonlinear ODE's?,,"We have $a,b,c,d$, functions of $x$, such that $a_x=-d-4acd$ $b_x=c+4bcd$ $c_x=b+4abc$ $d_x=-a-4abd$ I've already discovered that $(ab+cd)_x=0$, which I believe may be helpful. I've tried taking second derivatives of the above in an attempt to reduce the number of variables, but this didn't lead anywhere for me. Edit: The boundary conditions are that $a\rightarrow\pm a_0$ as $x\rightarrow\pm\infty$, where $a_0$ is a constant, and similarly for $b,c,d$.","We have $a,b,c,d$, functions of $x$, such that $a_x=-d-4acd$ $b_x=c+4bcd$ $c_x=b+4abc$ $d_x=-a-4abd$ I've already discovered that $(ab+cd)_x=0$, which I believe may be helpful. I've tried taking second derivatives of the above in an attempt to reduce the number of variables, but this didn't lead anywhere for me. Edit: The boundary conditions are that $a\rightarrow\pm a_0$ as $x\rightarrow\pm\infty$, where $a_0$ is a constant, and similarly for $b,c,d$.",,['ordinary-differential-equations']
59,exponential commutes integral,exponential commutes integral,,"Let $B(t)$ be a $n\times n$ function matrix with respect to $t$, consider $B(t)$ is differentiable , the following conclusion is obvious  $$\text{if}\ \ B'B=BB',\ \ \text{then} \ \ B'e^B=e^BB'$$ So my question is if the converse direction correct? i.e. can be found some $B$ such that $$B'B\neq BB'\ \ \text{but}\ \ B'e^B=e^BB'.$$. In deed, I found a complex matrix satisfy this condition $$B=\begin{pmatrix} \frac{2i\pi}{t-1} & \frac{2i\pi}{t-1}\\ 0 & \frac{2i\pi t}{t-1}\end{pmatrix}$$ However, I am wondering if there exist real matrix (I mean the coefficients are real ) example??? Updates Above is in differential form, now I consider the integral form: Can we find some $B$ such that: $$B\int_{t_0}^tB(s)ds\neq \int_{t_0}^tB(s)dsB\;\;but\;\;Be^{\int_{t_0}^tB(s)ds}=e^{\int_{t_0}^tB(s)ds}B$$ @John Ma, I tried you example, but failed to the integral case.","Let $B(t)$ be a $n\times n$ function matrix with respect to $t$, consider $B(t)$ is differentiable , the following conclusion is obvious  $$\text{if}\ \ B'B=BB',\ \ \text{then} \ \ B'e^B=e^BB'$$ So my question is if the converse direction correct? i.e. can be found some $B$ such that $$B'B\neq BB'\ \ \text{but}\ \ B'e^B=e^BB'.$$. In deed, I found a complex matrix satisfy this condition $$B=\begin{pmatrix} \frac{2i\pi}{t-1} & \frac{2i\pi}{t-1}\\ 0 & \frac{2i\pi t}{t-1}\end{pmatrix}$$ However, I am wondering if there exist real matrix (I mean the coefficients are real ) example??? Updates Above is in differential form, now I consider the integral form: Can we find some $B$ such that: $$B\int_{t_0}^tB(s)ds\neq \int_{t_0}^tB(s)dsB\;\;but\;\;Be^{\int_{t_0}^tB(s)ds}=e^{\int_{t_0}^tB(s)ds}B$$ @John Ma, I tried you example, but failed to the integral case.",,"['matrices', 'ordinary-differential-equations']"
60,Solution interval for $\dot x(t)=x(t)^2$?,Solution interval for ?,\dot x(t)=x(t)^2,"In my course notes, the professors introduces the example ODE: $$ \dot x(t)=x(t)^2 $$ The solution is: $$ x(t)=\frac{c}{1-tc} $$ However, this is undefined (division by zero) for $t=1/c$. The professors says no solution is defined for $t\ge 1/c$ . While I agree that no solution exists for $t=1/c$, help me understand why we cannot also have the solution for $t>1/c$, where $x(t)$ by the above formula is perfectly well defined (no division by zero)?","In my course notes, the professors introduces the example ODE: $$ \dot x(t)=x(t)^2 $$ The solution is: $$ x(t)=\frac{c}{1-tc} $$ However, this is undefined (division by zero) for $t=1/c$. The professors says no solution is defined for $t\ge 1/c$ . While I agree that no solution exists for $t=1/c$, help me understand why we cannot also have the solution for $t>1/c$, where $x(t)$ by the above formula is perfectly well defined (no division by zero)?",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
61,"Why does the follsolution of the ODE $x'=(t.cos(t)+sin(t),t^2.cos(t)+2t.sin(t))$, $x(0)=(0,0)$ doesn't contradict Picard?","Why does the follsolution of the ODE ,  doesn't contradict Picard?","x'=(t.cos(t)+sin(t),t^2.cos(t)+2t.sin(t)) x(0)=(0,0)","I have found the following solution: $\varphi(t)=(t.sin(t),t^2.sin(t))$, but $\varphi(0)=\varphi(2\pi)$, while $\varphi'(0)$ and $\varphi'(2\pi)$ are linearly independent. My professor said that this answer is correct, but he told me to think a bit more why it doesn't contradict Picard's theorem. But I can't find a good reason.","I have found the following solution: $\varphi(t)=(t.sin(t),t^2.sin(t))$, but $\varphi(0)=\varphi(2\pi)$, while $\varphi'(0)$ and $\varphi'(2\pi)$ are linearly independent. My professor said that this answer is correct, but he told me to think a bit more why it doesn't contradict Picard's theorem. But I can't find a good reason.",,['ordinary-differential-equations']
62,uniqueness of the solution of heat equation in convolution form,uniqueness of the solution of heat equation in convolution form,,"I wish show the solution expressed in the form $$u(t,x)=\int_{\mathbb{R}}\Phi_t(x-y)f(y)\,dy$$ is unique for any $f\in\mathcal{S}(\mathbb{R})$ (the space of rapidly decreasing functions in $\mathbb{R}$.) Here, $u$ is a classical solution satisfies Consider an equation on $\mathbb{R}$:     \begin{align*}\tag{1} 		\begin{cases} 			\frac{\partial u}{\partial t}(t,x)&=\frac{\partial^2 u}{\partial x^2}(t,x) \qquad t>0, x\in\mathbb{R}^d, \\ 		u(0,x)&=f(x),\qquad x\in\mathbb{R}^n 		\end{cases}	 	\end{align*} Here we start with assuming there exist two solutions, $u_1$ and $u_2$ both satisfies (1). Then I want to find the differential inequality for the function $W(t)=|u_2(t)-u_1(t)|=\int_{\mathbb{R}}|u_2(t,x)-u_1(t,x)|\,dx$ and the Gronwall inequality. So I have \begin{align*} w(t)&=\int_{\mathbb{R}}|u_2(t)-u_1(t)|\,dx\\ \Longrightarrow [w(t)]^2&=\left(\int_{\mathbb{R}}|u_2(t)-u_1(t)|\,dx\right)^2 \end{align*} Then $$\frac{1}{2}\frac{d}{dt}F^2(t)=F(t)\frac{d}{dt}F(t),$$ now I am lost...any help appreciated.","I wish show the solution expressed in the form $$u(t,x)=\int_{\mathbb{R}}\Phi_t(x-y)f(y)\,dy$$ is unique for any $f\in\mathcal{S}(\mathbb{R})$ (the space of rapidly decreasing functions in $\mathbb{R}$.) Here, $u$ is a classical solution satisfies Consider an equation on $\mathbb{R}$:     \begin{align*}\tag{1} 		\begin{cases} 			\frac{\partial u}{\partial t}(t,x)&=\frac{\partial^2 u}{\partial x^2}(t,x) \qquad t>0, x\in\mathbb{R}^d, \\ 		u(0,x)&=f(x),\qquad x\in\mathbb{R}^n 		\end{cases}	 	\end{align*} Here we start with assuming there exist two solutions, $u_1$ and $u_2$ both satisfies (1). Then I want to find the differential inequality for the function $W(t)=|u_2(t)-u_1(t)|=\int_{\mathbb{R}}|u_2(t,x)-u_1(t,x)|\,dx$ and the Gronwall inequality. So I have \begin{align*} w(t)&=\int_{\mathbb{R}}|u_2(t)-u_1(t)|\,dx\\ \Longrightarrow [w(t)]^2&=\left(\int_{\mathbb{R}}|u_2(t)-u_1(t)|\,dx\right)^2 \end{align*} Then $$\frac{1}{2}\frac{d}{dt}F^2(t)=F(t)\frac{d}{dt}F(t),$$ now I am lost...any help appreciated.",,"['functional-analysis', 'ordinary-differential-equations', 'inequality', 'partial-differential-equations']"
63,Solving $x'-2xt-t=0$.,Solving .,x'-2xt-t=0,"Find the general and particular solution of: $$x'-2xt-t=0\\ x(0)=1$$ Separating the variable I got $\frac {dx}{2x+1}=tdt \implies \ln|2x+1|=t^2+c$. After this I got confused, after exponentiating everything, how do I get rid of the absolute values? Also, what does ""the maximal interval of existence"" mean? Aren't all solutions to an ODE defined in the same domain?","Find the general and particular solution of: $$x'-2xt-t=0\\ x(0)=1$$ Separating the variable I got $\frac {dx}{2x+1}=tdt \implies \ln|2x+1|=t^2+c$. After this I got confused, after exponentiating everything, how do I get rid of the absolute values? Also, what does ""the maximal interval of existence"" mean? Aren't all solutions to an ODE defined in the same domain?",,['ordinary-differential-equations']
64,Uniqueness of the solution of a PDE,Uniqueness of the solution of a PDE,,"I have the following initial-value problem: $$ \begin{cases} u_t + c u_x = 0, \quad x \in \mathbb{R}, \ t > 0 \\ u(x,0) = g(x) \end{cases} \qquad [1] $$where $u_t$ and $u_x$ are the partial derivatives. It has been asked me to prove that the problem is well-posed if $g \in C_b ^1 (\mathbb{R})$ - id est to prove that a solution exists and it is unique, and that the solution depends continuously on the initial data in the norm $ \sup_{x \in \mathbb{R}} |g(x)|$. Now, the existence and the continuous dependence are pretty straightforward: it is sufficient to notice that $u(x,t)=g(x-ct)$ is a solution of $[1]$. But what about the uniqueness? I couldn't figure out how to reach a contraddiction by assuming the existence of another solution $v(x,t)$... I would conclude by myself, so just give me an (eventual) hint. Thanks in advance.","I have the following initial-value problem: $$ \begin{cases} u_t + c u_x = 0, \quad x \in \mathbb{R}, \ t > 0 \\ u(x,0) = g(x) \end{cases} \qquad [1] $$where $u_t$ and $u_x$ are the partial derivatives. It has been asked me to prove that the problem is well-posed if $g \in C_b ^1 (\mathbb{R})$ - id est to prove that a solution exists and it is unique, and that the solution depends continuously on the initial data in the norm $ \sup_{x \in \mathbb{R}} |g(x)|$. Now, the existence and the continuous dependence are pretty straightforward: it is sufficient to notice that $u(x,t)=g(x-ct)$ is a solution of $[1]$. But what about the uniqueness? I couldn't figure out how to reach a contraddiction by assuming the existence of another solution $v(x,t)$... I would conclude by myself, so just give me an (eventual) hint. Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations']"
65,Why is this a solution to the ODE system,Why is this a solution to the ODE system,,"Given a system of differential equations $\dot{x}(t)=Ax(t)+Bu$ where $x,B \in \mathbb R^n$, $A \in Mat_{n\times n}(\mathbb R)$ and $u \in \mathbb R$, we want to find a solution. I was given a solution of the form $x(t)=e^{At}x(0)+\int_{0}^{t}e^{A(t-\tau)}Bu(\tau)d\tau$. I'm just wondering why is this a solution? if we derive it, we don't seem to get $\dot{x}(t)=Ax(t)+Bu$.","Given a system of differential equations $\dot{x}(t)=Ax(t)+Bu$ where $x,B \in \mathbb R^n$, $A \in Mat_{n\times n}(\mathbb R)$ and $u \in \mathbb R$, we want to find a solution. I was given a solution of the form $x(t)=e^{At}x(0)+\int_{0}^{t}e^{A(t-\tau)}Bu(\tau)d\tau$. I'm just wondering why is this a solution? if we derive it, we don't seem to get $\dot{x}(t)=Ax(t)+Bu$.",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
66,A 100-gallon tank intiially contains 100 gallons of sugar water at a concentration of .25lbs of sugar/gallon.,A 100-gallon tank intiially contains 100 gallons of sugar water at a concentration of .25lbs of sugar/gallon.,,"A 100-gallon tank initially contains 100 gallons of sugar water at a concentration of .25lbs of sugar/gallon.  Suppose that sugar is added to the tank at a rate of p pounds/min. Suppose that sugar water is removed at a rate of 1gallon/min and that the water in the tank is well mixed.  What value of P should we pick so that  that when 5 gallons of sugar solution is left in the tank the concentration is .5lbs of sugar/gallon? I have that S(t)=k(100-t)-P(ln(100-t)(100-t)) So when t=0, I have 100k-100Pln(100)=25 and    when t=95, I have 5k-5Pln(5)=2.5 I have tried solving this system of equation but even my ODE mind cannot wrap around the solution. I have that P= 1/(4ln20), am I on the right path?","A 100-gallon tank initially contains 100 gallons of sugar water at a concentration of .25lbs of sugar/gallon.  Suppose that sugar is added to the tank at a rate of p pounds/min. Suppose that sugar water is removed at a rate of 1gallon/min and that the water in the tank is well mixed.  What value of P should we pick so that  that when 5 gallons of sugar solution is left in the tank the concentration is .5lbs of sugar/gallon? I have that S(t)=k(100-t)-P(ln(100-t)(100-t)) So when t=0, I have 100k-100Pln(100)=25 and    when t=95, I have 5k-5Pln(5)=2.5 I have tried solving this system of equation but even my ODE mind cannot wrap around the solution. I have that P= 1/(4ln20), am I on the right path?",,"['ordinary-differential-equations', 'solution-verification', 'applications']"
67,Spring - Mass problem solved with scilab,Spring - Mass problem solved with scilab,,"I need to solve a spring - mass problem with a numerical method in scilab using this differential equation $$\frac{d^2x}{d t^{2}} + \frac{k}{m} x = 0$$ with these initial values: $$k = 1 , m = 1 , x_0=1,x'(0) = 0 m/s$$ what I have done: function ydot=f(t, y)     k=1     m=1     ydot(1) = y(2);     ydot(2) = k/m*y(1); endfunction y0 = [10;10] t = 0:0.5:10; y = ode(y0,0,t,f) plot(t,y) xtitle(""Mass-Spring"",""T length"",""Y mass""); The result's graph: Can you help me? cause I think my code is not complete","I need to solve a spring - mass problem with a numerical method in scilab using this differential equation $$\frac{d^2x}{d t^{2}} + \frac{k}{m} x = 0$$ with these initial values: $$k = 1 , m = 1 , x_0=1,x'(0) = 0 m/s$$ what I have done: function ydot=f(t, y)     k=1     m=1     ydot(1) = y(2);     ydot(2) = k/m*y(1); endfunction y0 = [10;10] t = 0:0.5:10; y = ode(y0,0,t,f) plot(t,y) xtitle(""Mass-Spring"",""T length"",""Y mass""); The result's graph: Can you help me? cause I think my code is not complete",,"['ordinary-differential-equations', 'numerical-methods']"
68,Showing a function $\varphi_t$ is a homeomorphism,Showing a function  is a homeomorphism,\varphi_t,"Question: Let $f: \Omega = \mathbb R \times \mathbb R^n \to \mathbb R^n$ continuous with $f(t,x) = f(x)$, locally Lipschitz such that $|f| \leq M$ in $\Omega$. Show that for every $t \in \mathbb R$ $$\begin{align}\varphi_t : \mathbb R^n &\to \mathbb R^n\\x_0 &\mapsto \varphi(t, x_0)\end{align}$$   is a homeomorphism, where $\varphi (t,x_0)$ is a unique solution defined in $\mathbb R$ for the IPV $$x' = f(x) ,\,\, x(0) = x_0 \tag{*}$$ Attempt: $\varphi_t$ is injective As $\varphi (t,x_0)$ is unique for each $x_0$, if $x_0 \neq y_0$ then $\varphi_t (x_0) = \varphi (t,x_0) \neq \varphi (t,y_0) = \varphi_t (y_0) $. $\varphi_t$ is surjective $\varphi_t$ is injective and defined from $\mathbb R^n$ to $\mathbb R^n$ $\varphi_t$ is continuous Suppose it isn't. Then there exists a sequence of points $x_n$ in $\mathbb R^n$ such that $x_n \to x_0$ and $\varphi (t,x_n)\not\to \varphi(t,x_0)$. Consider $\varphi_n(\tau) = \varphi(\tau,x_n)$, where, $\tau \in [0,t]$. Then $$|\varphi'_n(\tau)| = |\varphi'(\tau, x_n)| = |f(\tau, \varphi(\tau, x_n))| \leq M$$ Then the set $E =\{\varphi_n : [0,1] \to \mathbb R^n\}$ is equicontinous. Plus, by the Mean Value Theorem $$|\varphi_n(\tau) - \varphi_n (\sigma)| \leq \sup_{\theta \in [0,1]} |\varphi'(\theta)| |\tau - \sigma| \leq M |\tau - \sigma|$$ Thus $(\varphi_n)$ is uniformly bounded. By the Arzelá-Ascoli Theorem there exists a subsequence $(\varphi_{n_k})$ converging to $\hat\varphi_0 \neq \varphi_0$ given by $$\varphi_{n_k} = x_{n_k} + \int_0^{\tau} f(s, \varphi_{n_k}(s)) ds$$ Letting $k\to \infty$ we have $$\hat\varphi_{0} = x_{0} + \int_0^{\tau} f(s, \hat\varphi_{0}(s)) ds$$ which is solution to $(*)$ and $\hat\varphi_0 \neq \varphi_0$, yielding a contradiction. $\varphi_t^{-1}$ is continuous $\phi_{t + s} = \phi_t \circ \phi_s$, for any given $t,s \in \mathbb R$. Note: We still don't have the Continous Dependence on solutions Theorem I'm having trouble with the last two parts.","Question: Let $f: \Omega = \mathbb R \times \mathbb R^n \to \mathbb R^n$ continuous with $f(t,x) = f(x)$, locally Lipschitz such that $|f| \leq M$ in $\Omega$. Show that for every $t \in \mathbb R$ $$\begin{align}\varphi_t : \mathbb R^n &\to \mathbb R^n\\x_0 &\mapsto \varphi(t, x_0)\end{align}$$   is a homeomorphism, where $\varphi (t,x_0)$ is a unique solution defined in $\mathbb R$ for the IPV $$x' = f(x) ,\,\, x(0) = x_0 \tag{*}$$ Attempt: $\varphi_t$ is injective As $\varphi (t,x_0)$ is unique for each $x_0$, if $x_0 \neq y_0$ then $\varphi_t (x_0) = \varphi (t,x_0) \neq \varphi (t,y_0) = \varphi_t (y_0) $. $\varphi_t$ is surjective $\varphi_t$ is injective and defined from $\mathbb R^n$ to $\mathbb R^n$ $\varphi_t$ is continuous Suppose it isn't. Then there exists a sequence of points $x_n$ in $\mathbb R^n$ such that $x_n \to x_0$ and $\varphi (t,x_n)\not\to \varphi(t,x_0)$. Consider $\varphi_n(\tau) = \varphi(\tau,x_n)$, where, $\tau \in [0,t]$. Then $$|\varphi'_n(\tau)| = |\varphi'(\tau, x_n)| = |f(\tau, \varphi(\tau, x_n))| \leq M$$ Then the set $E =\{\varphi_n : [0,1] \to \mathbb R^n\}$ is equicontinous. Plus, by the Mean Value Theorem $$|\varphi_n(\tau) - \varphi_n (\sigma)| \leq \sup_{\theta \in [0,1]} |\varphi'(\theta)| |\tau - \sigma| \leq M |\tau - \sigma|$$ Thus $(\varphi_n)$ is uniformly bounded. By the Arzelá-Ascoli Theorem there exists a subsequence $(\varphi_{n_k})$ converging to $\hat\varphi_0 \neq \varphi_0$ given by $$\varphi_{n_k} = x_{n_k} + \int_0^{\tau} f(s, \varphi_{n_k}(s)) ds$$ Letting $k\to \infty$ we have $$\hat\varphi_{0} = x_{0} + \int_0^{\tau} f(s, \hat\varphi_{0}(s)) ds$$ which is solution to $(*)$ and $\hat\varphi_0 \neq \varphi_0$, yielding a contradiction. $\varphi_t^{-1}$ is continuous $\phi_{t + s} = \phi_t \circ \phi_s$, for any given $t,s \in \mathbb R$. Note: We still don't have the Continous Dependence on solutions Theorem I'm having trouble with the last two parts.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
69,direct relationship between diffusion and wave equation,direct relationship between diffusion and wave equation,,"We find direct relationship between the heat and wave equation. Let $u(x,t)$ solve the wave equation on the whole line, and suppose the second derivatives of $u$ are bounded. Let:  $$v(x,t) = \frac{c}{\sqrt{4\pi kt}}\int_{-\infty}^{\infty}e^{-s^2c^2/4kt}u(x,s)ds$$ Show that $v(x,t)$ solves the diffusion equation $u_t -ku_{xx} = 0$ I know I have to write this $v(x,t)$ in the form of $v(x,t) = \int_{-\infty}^{\infty} H(s,t)u(x,s)ds$. But, I don't know how to differentiate this to satisfy the diffusion equation. Can someone please show me how? Thank you a lot.","We find direct relationship between the heat and wave equation. Let $u(x,t)$ solve the wave equation on the whole line, and suppose the second derivatives of $u$ are bounded. Let:  $$v(x,t) = \frac{c}{\sqrt{4\pi kt}}\int_{-\infty}^{\infty}e^{-s^2c^2/4kt}u(x,s)ds$$ Show that $v(x,t)$ solves the diffusion equation $u_t -ku_{xx} = 0$ I know I have to write this $v(x,t)$ in the form of $v(x,t) = \int_{-\infty}^{\infty} H(s,t)u(x,s)ds$. But, I don't know how to differentiate this to satisfy the diffusion equation. Can someone please show me how? Thank you a lot.",,"['ordinary-differential-equations', 'partial-differential-equations']"
70,repametrization ODE,repametrization ODE,,My dear comrades well an elementary question about change of parametrization in ODE : let's say I have $$ \frac{dx}{dt}(t) = f(t) $$ I made the change of parametrization $t\rightarrow u=s(t)$ is it correct to write $$ \frac{d\tilde{x}}{du}(s(t)) s'(t) = \frac{dx}{dt}(t)  $$ ie $$ \dot{\tilde{x}}(s(t)) = \frac{1}{s'(t)}f(t) ? $$ then the change of variables u = s(t) : $$ \dot{\tilde{x}}(u) = \frac{1}{s'(s^{-1}(u))}f(s^{-1}(u)) ? $$ Thanks,My dear comrades well an elementary question about change of parametrization in ODE : let's say I have $$ \frac{dx}{dt}(t) = f(t) $$ I made the change of parametrization $t\rightarrow u=s(t)$ is it correct to write $$ \frac{d\tilde{x}}{du}(s(t)) s'(t) = \frac{dx}{dt}(t)  $$ ie $$ \dot{\tilde{x}}(s(t)) = \frac{1}{s'(t)}f(t) ? $$ then the change of variables u = s(t) : $$ \dot{\tilde{x}}(u) = \frac{1}{s'(s^{-1}(u))}f(s^{-1}(u)) ? $$ Thanks,,"['real-analysis', 'ordinary-differential-equations']"
71,"Gronwall inequality for $Z_t \leq \int_0^t k(Z_s)\, ds$",Gronwall inequality for,"Z_t \leq \int_0^t k(Z_s)\, ds","In Ikeda and Watanabe, page 170 one reads I can't see why the relation $$Z_t \leq \int_0^t \kappa (Z_s)\, ds$$ implies that $Z_t = 0$ . How do we use the condition $$\int_{0+} \frac{1}{\kappa(u)}\, du = \infty\tag{*}$$ to get to the conclusion?","In Ikeda and Watanabe, page 170 one reads I can't see why the relation implies that . How do we use the condition to get to the conclusion?","Z_t \leq \int_0^t \kappa (Z_s)\, ds Z_t = 0 \int_{0+} \frac{1}{\kappa(u)}\, du = \infty\tag{*}","['analysis', 'ordinary-differential-equations', 'integral-inequality']"
72,"Solving differential equations, without 'time' in the equation","Solving differential equations, without 'time' in the equation",,"If two equations are presented, both being rates of change (with respect to time), how would one solve one specifically, when only the following information is provided? $$\frac{dl}{dt}=-2l$$ $$\frac{dm}{dt}=2l-3m$$ And that at $t=0, l=2,\!000$ and $m=1,\!000$. The goal is to be able to find $m$ (and $l$, if possible) with only the information provided above. I have attempting 'flipping' the equations to receive $\frac{dt}{dm}$ and $\frac{dt}{dl}$, derived, and integrated values, however all calculations send me in a circle. What is desired is to find $m$ (and $l$) in terms of $t$, and only $t$. How is this achieved?","If two equations are presented, both being rates of change (with respect to time), how would one solve one specifically, when only the following information is provided? $$\frac{dl}{dt}=-2l$$ $$\frac{dm}{dt}=2l-3m$$ And that at $t=0, l=2,\!000$ and $m=1,\!000$. The goal is to be able to find $m$ (and $l$, if possible) with only the information provided above. I have attempting 'flipping' the equations to receive $\frac{dt}{dm}$ and $\frac{dt}{dl}$, derived, and integrated values, however all calculations send me in a circle. What is desired is to find $m$ (and $l$) in terms of $t$, and only $t$. How is this achieved?",,"['ordinary-differential-equations', 'derivatives']"
73,Central orbit - Find eccentricity of the orbit,Central orbit - Find eccentricity of the orbit,,"A particle moving in an ellipse under the action of a force towards the focus O, moves from greatest distance from O to an extremity of the minor axis in time t, and then to the least distance from O in time $\frac{t}{k}$. Show that the eccentricity of the orbit is $$\frac{k-1}{k+1}\frac{\pi}{2}.$$ Approach: Let the equation of an ellipse with focus as a pole is $$\frac{l}{r}=1+e \cos{\theta}$$ We know that law of force with focus as center of force is $$F=\frac{\mu}{r^2}$$ Let ACA' and BCB' are the major and minor axes of the ellipse, C being center. Given the particle moves from A' (as it is at the greatest distance from O) to B in time t and particle takes time t/k to move from B to A (as it is the least distance from O). Now how to find the eccentricity?","A particle moving in an ellipse under the action of a force towards the focus O, moves from greatest distance from O to an extremity of the minor axis in time t, and then to the least distance from O in time $\frac{t}{k}$. Show that the eccentricity of the orbit is $$\frac{k-1}{k+1}\frac{\pi}{2}.$$ Approach: Let the equation of an ellipse with focus as a pole is $$\frac{l}{r}=1+e \cos{\theta}$$ We know that law of force with focus as center of force is $$F=\frac{\mu}{r^2}$$ Let ACA' and BCB' are the major and minor axes of the ellipse, C being center. Given the particle moves from A' (as it is at the greatest distance from O) to B in time t and particle takes time t/k to move from B to A (as it is the least distance from O). Now how to find the eccentricity?",,"['calculus', 'ordinary-differential-equations', 'physics', 'mathematical-physics', 'classical-mechanics']"
74,Solution of $xu_x + yu_y = 0$,Solution of,xu_x + yu_y = 0,"I have the first order PDE $$xu_x + yu_y = 0 \; \text{on} \; \mathbb{R}^2$$ and I found the solution of that PDE is $$u(x,y) = f\left(\frac{y}{x}\right) = e^C = K$$ which is a constant solution. So, my question is how can I find solution on puncture plan $\mathbb{R}^2 \setminus (0,0)$ that is not constant.","I have the first order PDE $$xu_x + yu_y = 0 \; \text{on} \; \mathbb{R}^2$$ and I found the solution of that PDE is $$u(x,y) = f\left(\frac{y}{x}\right) = e^C = K$$ which is a constant solution. So, my question is how can I find solution on puncture plan $\mathbb{R}^2 \setminus (0,0)$ that is not constant.",,"['ordinary-differential-equations', 'partial-differential-equations']"
75,Homogeneous 1st order ODE,Homogeneous 1st order ODE,,"This question comes from Schaums Calculus, CH59 Q18 which has had me confused for a couple of days now. Solve: $$ {dy \over dx} + y = xy^2 $$ I understand that this is a non-linear first order ode, where a substitution of $z=y^{-n}$ gives: $$ {dz\over dx} - z = -x $$ So in this case $P(x) = -1$ and so the integrating factor, $u = e^{-x}$ and hence we try to solve: $$ e^{-x}{dz \over dx} - ze^{-x} = -xe^{-x} $$ Schaums suggests (and this is the bit where I am lost), that this is just: $$ e^{-x} dx - ze^{-x} dx= -xe^{-x} dx $$ Implying that all dx's cancel: $$ ze^{-x}= xe^{-x} + e^{-x} + C $$ $$ {1 \over y} = 1 + Ce^x $$ However I am missing something because to me this looks like: $$ e^{-x} dz - ze^{-x} dx= -xe^{-x} dx $$ Where one term is the integral wrt z and the other 2 wrt x, however Schaum's method looks rather elegant but and I don't quite understand how to replicate it. Thanks in advance","This question comes from Schaums Calculus, CH59 Q18 which has had me confused for a couple of days now. Solve: $$ {dy \over dx} + y = xy^2 $$ I understand that this is a non-linear first order ode, where a substitution of $z=y^{-n}$ gives: $$ {dz\over dx} - z = -x $$ So in this case $P(x) = -1$ and so the integrating factor, $u = e^{-x}$ and hence we try to solve: $$ e^{-x}{dz \over dx} - ze^{-x} = -xe^{-x} $$ Schaums suggests (and this is the bit where I am lost), that this is just: $$ e^{-x} dx - ze^{-x} dx= -xe^{-x} dx $$ Implying that all dx's cancel: $$ ze^{-x}= xe^{-x} + e^{-x} + C $$ $$ {1 \over y} = 1 + Ce^x $$ However I am missing something because to me this looks like: $$ e^{-x} dz - ze^{-x} dx= -xe^{-x} dx $$ Where one term is the integral wrt z and the other 2 wrt x, however Schaum's method looks rather elegant but and I don't quite understand how to replicate it. Thanks in advance",,['ordinary-differential-equations']
76,"Kernel, Green function and the functional derivative.","Kernel, Green function and the functional derivative.",,"I am pretty new to the subject of differential equations and am reading about Green functions and Kernels for the first time. I am more familiar with functional differentiation and am comfortable with the idea that the functional derivative at a point is given by the Dirac delta function. \begin{equation} \frac{\delta F}{\delta f(x)}=\delta (x-a) \end{equation} With $F=f(a)$. I have noticed that this is very similar to a Green function. My question is: Are the above ideas related in a way more than coincidence? In particular functional derivatives, Green functions and Kernels. My understanding of a Green function is that it is the solution to the inhomogeneous linear ODE while the Kernel is the solution to the homogeneous linear ODE. As you can see it is not extensive!","I am pretty new to the subject of differential equations and am reading about Green functions and Kernels for the first time. I am more familiar with functional differentiation and am comfortable with the idea that the functional derivative at a point is given by the Dirac delta function. \begin{equation} \frac{\delta F}{\delta f(x)}=\delta (x-a) \end{equation} With $F=f(a)$. I have noticed that this is very similar to a Green function. My question is: Are the above ideas related in a way more than coincidence? In particular functional derivatives, Green functions and Kernels. My understanding of a Green function is that it is the solution to the inhomogeneous linear ODE while the Kernel is the solution to the homogeneous linear ODE. As you can see it is not extensive!",,"['functional-analysis', 'ordinary-differential-equations', 'dirac-delta', 'greens-function']"
77,Solving 1st order ODE,Solving 1st order ODE,,"I am trying to solve $$ g'+\lambda^2 g = 0  $$ I am guessing I can use the P(x) and Q(x) method, but I cannot seem to get it to workout. Obviously: $$\mu(x)=e^{\int{p(x)}dx}  $$ where $$p(x)=\lambda^2$$ From there I say: $$e^{\lambda^2}g'=0$$ This does not seem to work. Any help?","I am trying to solve $$ g'+\lambda^2 g = 0  $$ I am guessing I can use the P(x) and Q(x) method, but I cannot seem to get it to workout. Obviously: $$\mu(x)=e^{\int{p(x)}dx}  $$ where $$p(x)=\lambda^2$$ From there I say: $$e^{\lambda^2}g'=0$$ This does not seem to work. Any help?",,['ordinary-differential-equations']
78,Classifying a differential equation,Classifying a differential equation,,"How do I classify the following differential equation?  In particular, is this differential equation ""homogeneous?"" $$(x^3+3y^2)dx-2xydy=0$$ Solving it is not the problem, but I don't know how to recognize it.","How do I classify the following differential equation?  In particular, is this differential equation ""homogeneous?"" $$(x^3+3y^2)dx-2xydy=0$$ Solving it is not the problem, but I don't know how to recognize it.",,"['calculus', 'ordinary-differential-equations', 'homogeneous-equation']"
79,How do I demonstrate that the given functions solve this system of ODEs?,How do I demonstrate that the given functions solve this system of ODEs?,,"The system is $$\left\{ \begin{array}{rcl} x'&=&y-x(x^2+y^2-1) \\ y'&=&-x-y(x^2+y^2-1), \end{array} \right.$$ and the given solution is $$x(t)=\sin(t), \quad y(t)=\cos(t) .$$ Since the solution is given, should I just plug them into the system and show that on paper?","The system is $$\left\{ \begin{array}{rcl} x'&=&y-x(x^2+y^2-1) \\ y'&=&-x-y(x^2+y^2-1), \end{array} \right.$$ and the given solution is $$x(t)=\sin(t), \quad y(t)=\cos(t) .$$ Since the solution is given, should I just plug them into the system and show that on paper?",,"['ordinary-differential-equations', 'solution-verification']"
80,Function of the trajectory of a differential equation,Function of the trajectory of a differential equation,,"I want to show that there is no continuously differentiable non-constant function $H : \mathbb{R}^2 \to \mathbb{R}$ with $\nabla H(x,y) \neq (0,0)$ so that for every solution of the differential equation $\dot x = -x , \dot y = -2y :$ $H(x(t),y(t)) = c, c \in \mathbb{R}$ holds. The solution of the differential equation is $x(t) = e^{-t}, y(t) = e^{-2t}$, then if $H$ is such a function: $d/dt H(e^{-t}, e^{-2t}) = -\partial_xH(e^{-t}, e^{-2t})e^{-t} -\partial_yH(e^{-t}, e^{-2t})2e^{-2t} = 0 $ for all $ t \in \mathbb{R} $. So we have $-\partial_yH(e^{-t}, e^{-2t})2e^{-t} = \partial_xH(e^{-t}, e^{-2t})$ and for $t \to \infty: \partial_xH(0,0) = 0 $ which is a contradicition to $\nabla H(x,y) \neq (0,0)$. Where am I wrong?","I want to show that there is no continuously differentiable non-constant function $H : \mathbb{R}^2 \to \mathbb{R}$ with $\nabla H(x,y) \neq (0,0)$ so that for every solution of the differential equation $\dot x = -x , \dot y = -2y :$ $H(x(t),y(t)) = c, c \in \mathbb{R}$ holds. The solution of the differential equation is $x(t) = e^{-t}, y(t) = e^{-2t}$, then if $H$ is such a function: $d/dt H(e^{-t}, e^{-2t}) = -\partial_xH(e^{-t}, e^{-2t})e^{-t} -\partial_yH(e^{-t}, e^{-2t})2e^{-2t} = 0 $ for all $ t \in \mathbb{R} $. So we have $-\partial_yH(e^{-t}, e^{-2t})2e^{-t} = \partial_xH(e^{-t}, e^{-2t})$ and for $t \to \infty: \partial_xH(0,0) = 0 $ which is a contradicition to $\nabla H(x,y) \neq (0,0)$. Where am I wrong?",,"['real-analysis', 'ordinary-differential-equations']"
81,Why am I getting two different answers for this diff. equation? Completing the square vs quad form.,Why am I getting two different answers for this diff. equation? Completing the square vs quad form.,,"With this differential equation, after seperating and integrating, and using the initial condition to solve for C, and then substituting that value of C into the general solution, I must solve for Y to get the particular solution.  Why am I getting two different solutions, where in method 1 I complete the square, and method 2, use the quadratic formula? The ODE is: $\frac{dy}{dx} = \frac{2x}{1+2y}$ The initial condition is: $y(2) = 0$ Separating and integrating, I get the general soln.: $y + y^2 = x^2 + c$ Plugging in the initial condition: $0 + 0^2 = (2)^2 + c$ $c = -4$ Now, plugging in (-4) into the general solution: $y + y^2 = x^2 - 4$ Now, method 1, completing the square: $y^2 + y = x^2 - 4$ $(y + \frac{1}{2})^2 = x^2 - 4 + \frac{1}{4}$ $y = -\frac{1}{2} ^+_- (4x^2 - 15)^{\frac{1}{2}}$ Now, method 2, quad. formula: $y = \frac{-1 ^+_- (1^2 - 4(1)(-x^2+4))^{\frac{1}{2}}}{2}$ $y = -\frac{1}{2} ^+_- \frac{1}{2}(4x^2-15)^{\frac{1}{2}}$ So, what did I do wrong with completing the square?   The quad. formula is the same answer from the back of the book.  There is a 1/2 in front of the square root, whereas with  completing the square, the 1/2 is not there.   What did I do wrong?","With this differential equation, after seperating and integrating, and using the initial condition to solve for C, and then substituting that value of C into the general solution, I must solve for Y to get the particular solution.  Why am I getting two different solutions, where in method 1 I complete the square, and method 2, use the quadratic formula? The ODE is: $\frac{dy}{dx} = \frac{2x}{1+2y}$ The initial condition is: $y(2) = 0$ Separating and integrating, I get the general soln.: $y + y^2 = x^2 + c$ Plugging in the initial condition: $0 + 0^2 = (2)^2 + c$ $c = -4$ Now, plugging in (-4) into the general solution: $y + y^2 = x^2 - 4$ Now, method 1, completing the square: $y^2 + y = x^2 - 4$ $(y + \frac{1}{2})^2 = x^2 - 4 + \frac{1}{4}$ $y = -\frac{1}{2} ^+_- (4x^2 - 15)^{\frac{1}{2}}$ Now, method 2, quad. formula: $y = \frac{-1 ^+_- (1^2 - 4(1)(-x^2+4))^{\frac{1}{2}}}{2}$ $y = -\frac{1}{2} ^+_- \frac{1}{2}(4x^2-15)^{\frac{1}{2}}$ So, what did I do wrong with completing the square?   The quad. formula is the same answer from the back of the book.  There is a 1/2 in front of the square root, whereas with  completing the square, the 1/2 is not there.   What did I do wrong?",,"['algebra-precalculus', 'ordinary-differential-equations']"
82,Simplifying a nonhomogenous ODE via limits as t --> infinity.,Simplifying a nonhomogenous ODE via limits as t --> infinity.,,"I am self studying a systems biology paper on segmentation in evolutionary developmental biology and trying to replicate the simulation. The simulation implements systems of differential equations which model the transcriptional network in play for a given cell. The following network and its correspond system of ODEs is where I am struggling a little bit: Network 1: System of ODES: $$ \dfrac{dP}{dt} = \dfrac{G^{n_1}}{G^{n_1} + G_P^{n_1}} \dfrac{1}{1 + \left(\dfrac{R(t)}{R_P}\right)^{n_2}}\cdot S_P - \delta_P P \hspace{10mm} (1)$$ $$ \dfrac{dR}{dt} = \dfrac{G^{n_3}}{G^{n_3} + G_R^{n_3}} \cdot S_R - \delta_R R \hspace{36mm} (2)$$ where $G$ is a morphogen gradient dependent only on position, $x$, $$ G = \exp (-0.05x)$$ and can be treated a constant along with the variables $$\delta_R, \delta_P, G_P, G_R, R_P, S_P, n_1, n_2, n_3.$$ Solving equation (2) using an integrating factor results in: $$R(t) = \dfrac{G^{n_3}}{G^{n_3} + G_R^{n_3}} \cdot \dfrac{S_R}{\delta_R} \cdot (1 - e^{\delta_R t}) \hspace{26mm} (3)$$ My issue comes with trying to solve equation (1) due to the $R(t)$ term. If I'm not mistaken, that term renders equation (1) into a non-homogenous, linear differential equation. Is that correct? If so, I solved (1) as a homogenous equation, then tried using the method of undetermined coefficients to construct a particular solution. However, the result was too complex for me to solve. It looks something like this:     $$ \left(-\dfrac{A}{\delta _R} e^{-\delta _R t}\right) = \dfrac{G(x)^{n_1}}{G(x)^{n_1} + G^{n_1}_P}  \dfrac{1}{1+\left(\dfrac{R(t)}{R_P}\right)^{n_2} } S_P - \delta _P \left(A e^{-\delta _R t} \right)$$ I then thought I might be able to take the limit as $t \rightarrow \infty$ to simply $R(t)$, then plug that value into (1) as a constant, then solve (1) as a normal linear ODE by using a integrating factor. But as I write this out I begin to think that that approach is deathly incorrect. After two years away from mathematics, my mind is a bit frazzled at this point as to which direction to proceed in from here - or, it is very likely I have made an elementary mistake in either my approach or calculations. If anyone has any suggestions or pointers in the right direction, I would be immensely appreciative!","I am self studying a systems biology paper on segmentation in evolutionary developmental biology and trying to replicate the simulation. The simulation implements systems of differential equations which model the transcriptional network in play for a given cell. The following network and its correspond system of ODEs is where I am struggling a little bit: Network 1: System of ODES: $$ \dfrac{dP}{dt} = \dfrac{G^{n_1}}{G^{n_1} + G_P^{n_1}} \dfrac{1}{1 + \left(\dfrac{R(t)}{R_P}\right)^{n_2}}\cdot S_P - \delta_P P \hspace{10mm} (1)$$ $$ \dfrac{dR}{dt} = \dfrac{G^{n_3}}{G^{n_3} + G_R^{n_3}} \cdot S_R - \delta_R R \hspace{36mm} (2)$$ where $G$ is a morphogen gradient dependent only on position, $x$, $$ G = \exp (-0.05x)$$ and can be treated a constant along with the variables $$\delta_R, \delta_P, G_P, G_R, R_P, S_P, n_1, n_2, n_3.$$ Solving equation (2) using an integrating factor results in: $$R(t) = \dfrac{G^{n_3}}{G^{n_3} + G_R^{n_3}} \cdot \dfrac{S_R}{\delta_R} \cdot (1 - e^{\delta_R t}) \hspace{26mm} (3)$$ My issue comes with trying to solve equation (1) due to the $R(t)$ term. If I'm not mistaken, that term renders equation (1) into a non-homogenous, linear differential equation. Is that correct? If so, I solved (1) as a homogenous equation, then tried using the method of undetermined coefficients to construct a particular solution. However, the result was too complex for me to solve. It looks something like this:     $$ \left(-\dfrac{A}{\delta _R} e^{-\delta _R t}\right) = \dfrac{G(x)^{n_1}}{G(x)^{n_1} + G^{n_1}_P}  \dfrac{1}{1+\left(\dfrac{R(t)}{R_P}\right)^{n_2} } S_P - \delta _P \left(A e^{-\delta _R t} \right)$$ I then thought I might be able to take the limit as $t \rightarrow \infty$ to simply $R(t)$, then plug that value into (1) as a constant, then solve (1) as a normal linear ODE by using a integrating factor. But as I write this out I begin to think that that approach is deathly incorrect. After two years away from mathematics, my mind is a bit frazzled at this point as to which direction to proceed in from here - or, it is very likely I have made an elementary mistake in either my approach or calculations. If anyone has any suggestions or pointers in the right direction, I would be immensely appreciative!",,['ordinary-differential-equations']
83,Maximum condition for second order differential operators,Maximum condition for second order differential operators,,"Let $A$ be a second order differential operator such that $$Af(x) = \sum_{ij} a_{ij}(x) \big(\partial_i \partial_j f(x)\big) + \sum_j b_j(x) \partial_j f(x) $$ Assume that  $x \in B(0,r)\Rightarrow Af(x)>0$. How do I see that $f$ cannot attain a maximum inside of $B(0,r)$? Here $f : \Bbb{R}^n \to \Bbb{R}$ is $C^2$ and for every $x\in \Bbb{R}^n$$$\sum_{ij}x_ia_{ij}x_j \geq 0$$","Let $A$ be a second order differential operator such that $$Af(x) = \sum_{ij} a_{ij}(x) \big(\partial_i \partial_j f(x)\big) + \sum_j b_j(x) \partial_j f(x) $$ Assume that  $x \in B(0,r)\Rightarrow Af(x)>0$. How do I see that $f$ cannot attain a maximum inside of $B(0,r)$? Here $f : \Bbb{R}^n \to \Bbb{R}$ is $C^2$ and for every $x\in \Bbb{R}^n$$$\sum_{ij}x_ia_{ij}x_j \geq 0$$",,"['ordinary-differential-equations', 'partial-differential-equations']"
84,construct a path in $\Bbb{R}^n$ with specific derivatives,construct a path in  with specific derivatives,\Bbb{R}^n,"If we want a curve $\gamma:(-\epsilon,\epsilon)\to \Bbb{R}^n$ to have  $\gamma(0) = x$, and $\gamma'(0) = b$. It suffices to take $\gamma(u)= x + u \cdot b$. At the same time, I have a function on $\mathbb{R}^n$( $f: \Bbb{R}^n \to \Bbb{R}$). I would like to consider a path $\gamma$ such that $f\circ\gamma(t)$ satisfies $$f\circ\gamma(0) = f(x) \qquad (f\circ\gamma)'(0) = f'(x)\cdot b \qquad (f\circ\gamma)''(0) = \sum_{ij} a_{ij} (\partial_i \partial_j f(x))$$ Is this possible? Note: it is not the case that $a_{ij} = c_i c_j$  for some $c \in \Bbb{R}^n$","If we want a curve $\gamma:(-\epsilon,\epsilon)\to \Bbb{R}^n$ to have  $\gamma(0) = x$, and $\gamma'(0) = b$. It suffices to take $\gamma(u)= x + u \cdot b$. At the same time, I have a function on $\mathbb{R}^n$( $f: \Bbb{R}^n \to \Bbb{R}$). I would like to consider a path $\gamma$ such that $f\circ\gamma(t)$ satisfies $$f\circ\gamma(0) = f(x) \qquad (f\circ\gamma)'(0) = f'(x)\cdot b \qquad (f\circ\gamma)''(0) = \sum_{ij} a_{ij} (\partial_i \partial_j f(x))$$ Is this possible? Note: it is not the case that $a_{ij} = c_i c_j$  for some $c \in \Bbb{R}^n$",,"['real-analysis', 'ordinary-differential-equations']"
85,Differential equation for finding closest point on surface.,Differential equation for finding closest point on surface.,,Inspired by this question I got to think about a more general case. Say I have any discretized surface and want to find closest point from each point outside of surface to the surface. Say that I can estimate a local tensor for the local tangent space and normal space. Can I create a differential equation to encode with a vector field which optimally points out trajectories to go from each point and end up on a point on the surface of the object which would be the closest point?,Inspired by this question I got to think about a more general case. Say I have any discretized surface and want to find closest point from each point outside of surface to the surface. Say that I can estimate a local tensor for the local tangent space and normal space. Can I create a differential equation to encode with a vector field which optimally points out trajectories to go from each point and end up on a point on the surface of the object which would be the closest point?,,"['ordinary-differential-equations', 'differential-geometry', 'optimization', 'numerical-linear-algebra']"
86,Bound on a process satisfying certain integral and differential inequalities.,Bound on a process satisfying certain integral and differential inequalities.,,"Suppose a non-negative process $x$ satisfies the following integral and differential inequalities: $$ x_t+C\int_0^tx_s^2ds\,\,<\,\,x_0+\delta+bt, $$ $$ \dot{x}_t\,\,<\,\,Kx_t^2+a, $$ where $0<\delta \ll 1$, and $a$, $b$, $C$, $K$ and  are all positive. Is it possible to obtain some constant bound on the process $x$? Comment: Note that first equation alone is not enough. $x$ can stay close to zero for a long time and then increase to a large value abruptly so that $x_t$ is large and $\int x_s^2$ is near zero and both add up close to $x_0+\delta+bt$. This way we only get that $x_t<x_0+\delta+bt$. The second equation does not allow such a rapid rise of $x$. Gronwall kind of inequalities are not helpful because they assume $C<0$. Motivation: Consider $y\geq 0$ satisfying $$ \dot{y}_t=-Cy_t^2+b. $$ Then it is easy to see that $y_t\leq \max\{y_0,\sqrt{b/C}\}$. Instead, if $y$ satisfied  $$ \dot{y}_t \leq -Cy_t^2+b. $$ then a comparison principle would give same answer $y_t\leq \max\{y_0,\sqrt{b/C}\}$. Now consider a process $z\geq 0$ satisfying $$ \dot{z}_t \leq -Cz_t^2+b + G(z_t,t/\epsilon), $$ where $\epsilon\ll 1$ and $G$ as a function of its second argument is periodic with mean zero, and as a function of its first argument has atmost quadratic dependence. Using the fact that $G$ has atmost quadratic dependence in $z$ we get the second equation of the question . The effect of $G$ on $\dot{z}$ is a very rapid mean zero oscillation. So if we consider integral form of the above equation and approximate the effect of $\int G$ (and assume some other things) then we get the form of the first equation of the question .","Suppose a non-negative process $x$ satisfies the following integral and differential inequalities: $$ x_t+C\int_0^tx_s^2ds\,\,<\,\,x_0+\delta+bt, $$ $$ \dot{x}_t\,\,<\,\,Kx_t^2+a, $$ where $0<\delta \ll 1$, and $a$, $b$, $C$, $K$ and  are all positive. Is it possible to obtain some constant bound on the process $x$? Comment: Note that first equation alone is not enough. $x$ can stay close to zero for a long time and then increase to a large value abruptly so that $x_t$ is large and $\int x_s^2$ is near zero and both add up close to $x_0+\delta+bt$. This way we only get that $x_t<x_0+\delta+bt$. The second equation does not allow such a rapid rise of $x$. Gronwall kind of inequalities are not helpful because they assume $C<0$. Motivation: Consider $y\geq 0$ satisfying $$ \dot{y}_t=-Cy_t^2+b. $$ Then it is easy to see that $y_t\leq \max\{y_0,\sqrt{b/C}\}$. Instead, if $y$ satisfied  $$ \dot{y}_t \leq -Cy_t^2+b. $$ then a comparison principle would give same answer $y_t\leq \max\{y_0,\sqrt{b/C}\}$. Now consider a process $z\geq 0$ satisfying $$ \dot{z}_t \leq -Cz_t^2+b + G(z_t,t/\epsilon), $$ where $\epsilon\ll 1$ and $G$ as a function of its second argument is periodic with mean zero, and as a function of its first argument has atmost quadratic dependence. Using the fact that $G$ has atmost quadratic dependence in $z$ we get the second equation of the question . The effect of $G$ on $\dot{z}$ is a very rapid mean zero oscillation. So if we consider integral form of the above equation and approximate the effect of $\int G$ (and assume some other things) then we get the form of the first equation of the question .",,"['ordinary-differential-equations', 'integral-inequality']"
87,Separation of variables Calculus,Separation of variables Calculus,,"The given differential equation I need to solve is $dy/dx=1/x$ with the initial conditon of $x=1$ and $y=10$ My attempt:  $dy=\frac 1x dx$ Integrating yields  \begin{align*}     y&=\log x+C\\    x=1 & y=10\\  10 &=\log 1 + C\\   10=C\\ \leadsto y&=\log x+10 \end{align*} Something seems off with my work, if anyone could give me a clue where I went wrong. Thanks","The given differential equation I need to solve is $dy/dx=1/x$ with the initial conditon of $x=1$ and $y=10$ My attempt:  $dy=\frac 1x dx$ Integrating yields  \begin{align*}     y&=\log x+C\\    x=1 & y=10\\  10 &=\log 1 + C\\   10=C\\ \leadsto y&=\log x+10 \end{align*} Something seems off with my work, if anyone could give me a clue where I went wrong. Thanks",,"['calculus', 'ordinary-differential-equations', 'solution-verification']"
88,"Show that $\frac{\int_\Omega|\nabla u|^2+\int_\Omega\alpha|u|^2}{\int_\Omega|u|^2}$ attains a minimum in $W_0^{1,2}(\Omega)$",Show that  attains a minimum in,"\frac{\int_\Omega|\nabla u|^2+\int_\Omega\alpha|u|^2}{\int_\Omega|u|^2} W_0^{1,2}(\Omega)","Let $\Omega\subseteq\mathbb{R}^n$ be a bounded domain $H:=W_0^{1,2}(\Omega)$ be the Sobolev space $|\;\cdot\;|_p$ be the seminorm $$|u|_p^p:=\int_\Omega|\nabla u|^p\;d\lambda^n\;\;\;\text{for }u:\Omega\to\mathbb{R}\;\text{weakly differentiable}$$ on $L^p:=L^p(\Omega)$ $\left\|\;\cdot\;\right\|_p$ be the $L^p$-norm From the basic theory of the eigenvalue problem of the Laplacian, one knows that $$R(u):=\frac{|u|_2^2}{\left\|u\right\|_2^2}\;\;\;\text{for }u\in H\setminus\left\{0\right\}\tag{1}$$ attains its infimum in $H\setminus\left\{0\right\}$. Now, I would like to show, that $$\tilde{R}(u):=R(u)+\frac{\left\|\sqrt{\alpha}u\right\|_2^2}{\left\|u\right\|_2^2}\;\;\;\text{for }u\in H\setminus\left\{0\right\}\tag{2}\;,$$ for some $\alpha\in L^\infty$, has a minimum, too. We may note, that we can assume, that $R$ attains its minimum $\lambda_1$ in $u_1\in H$ with $\left\|u_1\right\|_2^2=1$. Then, $$\tilde{R}(u_1)=\lambda_1+\left\|\sqrt{\alpha} u_1\right\|_2^2\;.$$ However, I don't see how I need to proceed for me. Maybe this is not the right track and we need to use the Poincaré inequality $$\left\|u\right\|_2^2\le C|u|_2^2\;\;\;\text{for all }u\in H\;,$$ for some $C>0$, instead.","Let $\Omega\subseteq\mathbb{R}^n$ be a bounded domain $H:=W_0^{1,2}(\Omega)$ be the Sobolev space $|\;\cdot\;|_p$ be the seminorm $$|u|_p^p:=\int_\Omega|\nabla u|^p\;d\lambda^n\;\;\;\text{for }u:\Omega\to\mathbb{R}\;\text{weakly differentiable}$$ on $L^p:=L^p(\Omega)$ $\left\|\;\cdot\;\right\|_p$ be the $L^p$-norm From the basic theory of the eigenvalue problem of the Laplacian, one knows that $$R(u):=\frac{|u|_2^2}{\left\|u\right\|_2^2}\;\;\;\text{for }u\in H\setminus\left\{0\right\}\tag{1}$$ attains its infimum in $H\setminus\left\{0\right\}$. Now, I would like to show, that $$\tilde{R}(u):=R(u)+\frac{\left\|\sqrt{\alpha}u\right\|_2^2}{\left\|u\right\|_2^2}\;\;\;\text{for }u\in H\setminus\left\{0\right\}\tag{2}\;,$$ for some $\alpha\in L^\infty$, has a minimum, too. We may note, that we can assume, that $R$ attains its minimum $\lambda_1$ in $u_1\in H$ with $\left\|u_1\right\|_2^2=1$. Then, $$\tilde{R}(u_1)=\lambda_1+\left\|\sqrt{\alpha} u_1\right\|_2^2\;.$$ However, I don't see how I need to proceed for me. Maybe this is not the right track and we need to use the Poincaré inequality $$\left\|u\right\|_2^2\le C|u|_2^2\;\;\;\text{for all }u\in H\;,$$ for some $C>0$, instead.",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'sobolev-spaces']"
89,How do I find the Laplace Transform of $ \delta(t-2\pi)\cos(t) $?,How do I find the Laplace Transform of ?, \delta(t-2\pi)\cos(t) ,"How do I find the Laplace Transform of $$ \delta(t-2\pi)\cos(t) $$ where $\delta(t) $ is the Dirac Delta Function. I know that it boils down to the following integral $$ \int_{0}^\infty e^{-st}\delta(t-2\pi)\cos(t)dt $$ and I know that the answer is simply $$ e^{-2\pi s} $$ and I know how to integrate just the Dirac Delta Function. I don't know how to integrate it as the product of another function of the same variable. It doesn't explain anywhere in the chapter in my text, either, so I'm a bit confused. I assume I'll have to integrate by parts, or something, but I'm not to sure what to do with that Delta Function in there. Any help would be appreciated. Thanks!","How do I find the Laplace Transform of $$ \delta(t-2\pi)\cos(t) $$ where $\delta(t) $ is the Dirac Delta Function. I know that it boils down to the following integral $$ \int_{0}^\infty e^{-st}\delta(t-2\pi)\cos(t)dt $$ and I know that the answer is simply $$ e^{-2\pi s} $$ and I know how to integrate just the Dirac Delta Function. I don't know how to integrate it as the product of another function of the same variable. It doesn't explain anywhere in the chapter in my text, either, so I'm a bit confused. I assume I'll have to integrate by parts, or something, but I'm not to sure what to do with that Delta Function in there. Any help would be appreciated. Thanks!",,"['calculus', 'integration', 'ordinary-differential-equations', 'laplace-transform', 'dirac-delta']"
90,Solve one dimensional wave equation using fourier transform,Solve one dimensional wave equation using fourier transform,,"I'm trying solve this wave equation using Fourier method, but I am stuck... $${ u }_{ tt } ={ c }^{ 2 }{ u }_{ xx } - \alpha{ u } =0, \  0<x\le L, t  >0 $$ $${ u }( 0,t) = { u }( L,t) = 0$$ $${ u }( x,0) = f(x), { u }_{ t }( x,0) = g(x) $$ I know that first I have to use separation of variables: $${ u }( x,t) = T(t)X(x). $$ Making the calculations $$\frac{T''+ \alpha T}{c^{2}T} = \frac{X''}{X} = -\lambda  $$ I guess I'm right at this point? Okay? Now I have to solve: $$X'' + \lambda X = 0,$$ and $$\frac{T'' + \alpha T}{c^{2}T}  = -\lambda ,$$ $$ T'' + (\alpha + \lambda c^2)T = 0.$$ I don't now how to solve the second equation and how I add the two equation to solve the first problem. I will be very grateful for the help!!!!","I'm trying solve this wave equation using Fourier method, but I am stuck... $${ u }_{ tt } ={ c }^{ 2 }{ u }_{ xx } - \alpha{ u } =0, \  0<x\le L, t  >0 $$ $${ u }( 0,t) = { u }( L,t) = 0$$ $${ u }( x,0) = f(x), { u }_{ t }( x,0) = g(x) $$ I know that first I have to use separation of variables: $${ u }( x,t) = T(t)X(x). $$ Making the calculations $$\frac{T''+ \alpha T}{c^{2}T} = \frac{X''}{X} = -\lambda  $$ I guess I'm right at this point? Okay? Now I have to solve: $$X'' + \lambda X = 0,$$ and $$\frac{T'' + \alpha T}{c^{2}T}  = -\lambda ,$$ $$ T'' + (\alpha + \lambda c^2)T = 0.$$ I don't now how to solve the second equation and how I add the two equation to solve the first problem. I will be very grateful for the help!!!!",,"['sequences-and-series', 'ordinary-differential-equations', 'fourier-analysis', 'fourier-series', 'wave-equation']"
91,An applied problem in ODE leading to Interval of Existence issue,An applied problem in ODE leading to Interval of Existence issue,,"ODE books have a section on interval of existence of a solution with a standard set of problems, such as what is the interval of existence for $y'+(\tan t) y = t/(t-2), y(3)=5$, or $y'=y^3, y(0)=2$. I am looking for a ""modelling"" problem leading to such an issue. That is, a natural situation where the issue comes up and has a physically meaningful answer.","ODE books have a section on interval of existence of a solution with a standard set of problems, such as what is the interval of existence for $y'+(\tan t) y = t/(t-2), y(3)=5$, or $y'=y^3, y(0)=2$. I am looking for a ""modelling"" problem leading to such an issue. That is, a natural situation where the issue comes up and has a physically meaningful answer.",,"['ordinary-differential-equations', 'education']"
92,Population changes with time,Population changes with time,,"Question The population of a certain community is known to increase at a rate proportional to the number of people present at time $t$. If the population has doubled in 5 years, how long will it take to triple? To quadruple? Suppose it is known that the population of the community in Problem 1 is 10,000 after 3 years. What was the initial population? What will be the population in 10 years? Note - I need help with #2. I only included #1 because the first line of the second problem points to it. The logistic equation demonstrated to us in class is $$ P =\frac{ak}{e^{-at}+bk} $$ where $a$ = birth rate and $b$ = death rate. I assumed that the death rate is zero given the scope of this problem and attempted to solve for $a$, but I cannot isolate the variable. $$ -3a = ln(\frac{ak}{10000}) $$ Am I using the right equation? If not, then when is this one supposed to be used?","Question The population of a certain community is known to increase at a rate proportional to the number of people present at time $t$. If the population has doubled in 5 years, how long will it take to triple? To quadruple? Suppose it is known that the population of the community in Problem 1 is 10,000 after 3 years. What was the initial population? What will be the population in 10 years? Note - I need help with #2. I only included #1 because the first line of the second problem points to it. The logistic equation demonstrated to us in class is $$ P =\frac{ak}{e^{-at}+bk} $$ where $a$ = birth rate and $b$ = death rate. I assumed that the death rate is zero given the scope of this problem and attempted to solve for $a$, but I cannot isolate the variable. $$ -3a = ln(\frac{ak}{10000}) $$ Am I using the right equation? If not, then when is this one supposed to be used?",,['ordinary-differential-equations']
93,Find a quadratic equation that approaches exponential equation.,Find a quadratic equation that approaches exponential equation.,,"I have the following equation: \begin{equation} \frac{dy(x)}{dx}=1-y(x)-e^{-ay} \end{equation} I need to find a quadratic equation that approaches the right side of the equation. I know it's something along the lines of $-\frac{a^2}{2}y^2(x)+(a-1)y(x)$, but I don't know how you get there.","I have the following equation: \begin{equation} \frac{dy(x)}{dx}=1-y(x)-e^{-ay} \end{equation} I need to find a quadratic equation that approaches the right side of the equation. I know it's something along the lines of $-\frac{a^2}{2}y^2(x)+(a-1)y(x)$, but I don't know how you get there.",,['ordinary-differential-equations']
94,Solving for a family of orthagonal trajectories,Solving for a family of orthagonal trajectories,,"Given $y = cx^2$, I'm asked to find the family of trajectories orthagonal to that. I begin by differentiating to get $\frac{dy}{dx} = 2cx $ Then I replace $c$ with $\frac{y}{x^2}$, so $\frac{dy}{dx} = \frac{2y}{x}$ Then I take the negative reciprocal so the the trajectories are orthogonal, getting $\frac{dy}{dx} _{orthagonal} = \frac{-x}{2y}$ Then I perform seperation of variables, so $2y dy = -x dx$ and integrating $y^2 + \frac{x^2}{2} = C$ (that C is meant to be a constant of integration, obviously, not the earlier lowercase variable) Unfortunately, I'm told that the correct solution is $x^2 + 2y^2 = c$, which looks kind of similar but is clearly not the same thing. Where did I go wrong?","Given $y = cx^2$, I'm asked to find the family of trajectories orthagonal to that. I begin by differentiating to get $\frac{dy}{dx} = 2cx $ Then I replace $c$ with $\frac{y}{x^2}$, so $\frac{dy}{dx} = \frac{2y}{x}$ Then I take the negative reciprocal so the the trajectories are orthogonal, getting $\frac{dy}{dx} _{orthagonal} = \frac{-x}{2y}$ Then I perform seperation of variables, so $2y dy = -x dx$ and integrating $y^2 + \frac{x^2}{2} = C$ (that C is meant to be a constant of integration, obviously, not the earlier lowercase variable) Unfortunately, I'm told that the correct solution is $x^2 + 2y^2 = c$, which looks kind of similar but is clearly not the same thing. Where did I go wrong?",,"['integration', 'ordinary-differential-equations', 'orthogonality']"
95,Finding the orthogonal trajectory of $y = x + ce^{-x}$,Finding the orthogonal trajectory of,y = x + ce^{-x},I encountered this question on a finals study guide but my attempts to solve it were unsuccessful. $y = x + ce^{-x}$ $\frac{\mathrm{d} y}{\mathrm{d} x} = 1 - ce^{-x}$ $\frac{-\mathrm{d} x}{\mathrm{d} y} = 1 - ce^{-x}$ I do not know where to go from here.,I encountered this question on a finals study guide but my attempts to solve it were unsuccessful. $y = x + ce^{-x}$ $\frac{\mathrm{d} y}{\mathrm{d} x} = 1 - ce^{-x}$ $\frac{-\mathrm{d} x}{\mathrm{d} y} = 1 - ce^{-x}$ I do not know where to go from here.,,['ordinary-differential-equations']
96,Solving $\text{D} f \cdot x = f(x)$,Solving,\text{D} f \cdot x = f(x),"Let ${D}f$ denote the Jacobian matrix of $f$. What are the solutions of $Df \cdot x = f(x)$? In one-dimension, this is just $f' x = f$ with linear functions as the only solutions. In general, linear maps, i.e., functions of the form $A x$ for some constant matrix $A$, do indeed satisfy the equation, but it is unclear whether these are the only solutions.","Let ${D}f$ denote the Jacobian matrix of $f$. What are the solutions of $Df \cdot x = f(x)$? In one-dimension, this is just $f' x = f$ with linear functions as the only solutions. In general, linear maps, i.e., functions of the form $A x$ for some constant matrix $A$, do indeed satisfy the equation, but it is unclear whether these are the only solutions.",,['ordinary-differential-equations']
97,Find general solution and particular solution given a initial condition,Find general solution and particular solution given a initial condition,,Given this differential equation: $$ \frac{dx}{dt} = te^{-x} $$ I want to; (1) Find the general solution. (2) Find the particular solution given the initial condition $x(0)=1$ So this is how i proceed: $$ \int\frac{dx}{e^{-x}} = \int tdt \implies e^x = \frac{t^2}{2} $$ So if i take that last equation as a general solution for the initial condition $x(0)=1$ i have that: $$ 0 = ln \frac{1}{2}$$ So i think i am not doing it right and i would thank any kind of help.,Given this differential equation: $$ \frac{dx}{dt} = te^{-x} $$ I want to; (1) Find the general solution. (2) Find the particular solution given the initial condition $x(0)=1$ So this is how i proceed: $$ \int\frac{dx}{e^{-x}} = \int tdt \implies e^x = \frac{t^2}{2} $$ So if i take that last equation as a general solution for the initial condition $x(0)=1$ i have that: $$ 0 = ln \frac{1}{2}$$ So i think i am not doing it right and i would thank any kind of help.,,['ordinary-differential-equations']
98,Escaping a stampede in Buffalo Country,Escaping a stampede in Buffalo Country,,"A person is standing at a random point in square ABCD (vertices labelled clockwise) of side length 10 units. The person is capable of instantly reaching and running at 1 unit/second. A herd of buffaloes forming a straight front 10 units wide is charging in a straight line through the square ABCD, at uniform speed 2 units/second. They enter through AB exactly and exit through CD exactly. Assume the person becomes aware of the stampede when the buffaloes first enter the square, and that the person takes the path which gives them the best chance of running out of the way of the stampede. What is the chance the person escapes the stampede? Will the best escape path always be straight in this case? What is the general strategy for solving these types of pursuit/escape problems?","A person is standing at a random point in square ABCD (vertices labelled clockwise) of side length 10 units. The person is capable of instantly reaching and running at 1 unit/second. A herd of buffaloes forming a straight front 10 units wide is charging in a straight line through the square ABCD, at uniform speed 2 units/second. They enter through AB exactly and exit through CD exactly. Assume the person becomes aware of the stampede when the buffaloes first enter the square, and that the person takes the path which gives them the best chance of running out of the way of the stampede. What is the chance the person escapes the stampede? Will the best escape path always be straight in this case? What is the general strategy for solving these types of pursuit/escape problems?",,"['geometry', 'ordinary-differential-equations']"
99,Proving a differential equation is a circle,Proving a differential equation is a circle,,"So, I have solved the differential equation, to find the general solution of: $$\frac{y^2}{2} = 2x - \frac{x^2}{2} + c$$ I am told that is passes through the point $(4,2)$. Using this information, I found $C$ to be $2$. Then I am asked to prove that the equation forms a circle, and find the radius and centre point. So I know to try and get it into the form: $$x^2 + y^2 = r^2$$ So I can get: $$\frac{y^2}{2} + \frac{x^2}{2} = 2x + 2$$ Multiply both sides by $2$: $$y^2 + x^2 = 4x + 4$$ But from here I am not sure how to group the $x$'s to leave only a constant on the RHS. Any guidance would be much appreciated.","So, I have solved the differential equation, to find the general solution of: $$\frac{y^2}{2} = 2x - \frac{x^2}{2} + c$$ I am told that is passes through the point $(4,2)$. Using this information, I found $C$ to be $2$. Then I am asked to prove that the equation forms a circle, and find the radius and centre point. So I know to try and get it into the form: $$x^2 + y^2 = r^2$$ So I can get: $$\frac{y^2}{2} + \frac{x^2}{2} = 2x + 2$$ Multiply both sides by $2$: $$y^2 + x^2 = 4x + 4$$ But from here I am not sure how to group the $x$'s to leave only a constant on the RHS. Any guidance would be much appreciated.",,"['ordinary-differential-equations', 'circles']"
