,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Markov chain state time,Markov chain state time,,"I have a question regarding markov chains in continuous time. I have a birth/death process(for every step we go 1 up & or 1 down), queue with 4 states. Customers arrive with the intensity of lambda per hour and can be seen as poisson distributed. The time to serve each customer can be seen as exponentially distributed and independent. We start at time = 0 and state = 0. We are going to do 100 steps. State 0 => There is no customers. (Arrival rate = 2, Departure rate = 0) State 1 => There is one customer being served. (Arrival rate = 2, Departure rate = 10) State 2 => There is one customer being served + 1 in queue. (Arrival rate = 2, Departure rate = 10) State 3 => There is one customer being served + 2 in queue. (Arrival rate = 0, Departure rate = 10) Givens:  Arrival rate = 2.  Depature rate = 10. Is there a formula to calculate the total time spent in each state?","I have a question regarding markov chains in continuous time. I have a birth/death process(for every step we go 1 up & or 1 down), queue with 4 states. Customers arrive with the intensity of lambda per hour and can be seen as poisson distributed. The time to serve each customer can be seen as exponentially distributed and independent. We start at time = 0 and state = 0. We are going to do 100 steps. State 0 => There is no customers. (Arrival rate = 2, Departure rate = 0) State 1 => There is one customer being served. (Arrival rate = 2, Departure rate = 10) State 2 => There is one customer being served + 1 in queue. (Arrival rate = 2, Departure rate = 10) State 3 => There is one customer being served + 2 in queue. (Arrival rate = 0, Departure rate = 10) Givens:  Arrival rate = 2.  Depature rate = 10. Is there a formula to calculate the total time spent in each state?",,"['statistics', 'markov-chains', 'queueing-theory']"
1,Averaging averages without losing standard deviation information [duplicate],Averaging averages without losing standard deviation information [duplicate],,"This question already has an answer here : How do I obtain the ""final"" standard deviation from a series of values containing individual (also null) SD values? (1 answer) Closed 7 years ago . I've got a computerized procedure which runs a test and takes a few hundred samples during testing.  When finished, it spits back out the average and standard deviation of the samples. Now, I've run this test several times on test subjects that should be identical, but of course I get back slightly different average and standard deviation data each time.  I only have a record of the average and standard deviation returned at the end of the test, not of each individual sample point.  A typical data set will look something like the following: SUBJECT   AVG     STD       1   129.2   31.0       2   125.0   37.3       3   123.6   34.7       4   130.1   31.3      ...   ...    ... Now, if I just average together the averages I get 127.0, but the standard deviation is only 3.2, when in fact the standard deviation between any actual samples is likely to be closer to 30.  Is there a way I can combine my summary statistics that preserves the information I have about the standard deviation between samples? Unfortunately, I don't have access to the size of the data sets which generated the outputs above (it's somewhere around a hundred points, but is different each time and not something I have access to records of).","This question already has an answer here : How do I obtain the ""final"" standard deviation from a series of values containing individual (also null) SD values? (1 answer) Closed 7 years ago . I've got a computerized procedure which runs a test and takes a few hundred samples during testing.  When finished, it spits back out the average and standard deviation of the samples. Now, I've run this test several times on test subjects that should be identical, but of course I get back slightly different average and standard deviation data each time.  I only have a record of the average and standard deviation returned at the end of the test, not of each individual sample point.  A typical data set will look something like the following: SUBJECT   AVG     STD       1   129.2   31.0       2   125.0   37.3       3   123.6   34.7       4   130.1   31.3      ...   ...    ... Now, if I just average together the averages I get 127.0, but the standard deviation is only 3.2, when in fact the standard deviation between any actual samples is likely to be closer to 30.  Is there a way I can combine my summary statistics that preserves the information I have about the standard deviation between samples? Unfortunately, I don't have access to the size of the data sets which generated the outputs above (it's somewhere around a hundred points, but is different each time and not something I have access to records of).",,"['statistics', 'average', 'standard-deviation']"
2,Standard Normal Distribution Findng A,Standard Normal Distribution Findng A,,I have the following question and i am dumbfounded on how to find the a in my given question. $$\sigma= 10000$$ $$\mu= 50000 $$ Find the monthly income which is exceeded by 10 % of employees. I have  $$ P(X=a) = 0.1$$ $$P(\frac{a-\mu/}{\sigma}) = 0.1$$ I am stuck at this part.,I have the following question and i am dumbfounded on how to find the a in my given question. $$\sigma= 10000$$ $$\mu= 50000 $$ Find the monthly income which is exceeded by 10 % of employees. I have  $$ P(X=a) = 0.1$$ $$P(\frac{a-\mu/}{\sigma}) = 0.1$$ I am stuck at this part.,,"['statistics', 'normal-distribution', 'standard-deviation']"
3,"If you roll two six-sided dice, what is the probability that the dice add to 10 or higher?","If you roll two six-sided dice, what is the probability that the dice add to 10 or higher?",,"When answering these sort of questions people mostly resort to diagrams and I'm wondering if there is a way to calculate the probability without going through each outcome, just solely on the given variables.","When answering these sort of questions people mostly resort to diagrams and I'm wondering if there is a way to calculate the probability without going through each outcome, just solely on the given variables.",,"['probability', 'statistics']"
4,probability of rank of a number,probability of rank of a number,,"Suppose I have 10 sample means. I want to find the probability of rank of the population means using sample means. Therefore, I want to perform two experiments. First experiment: I pick one of the sample means and compute the probability of being rank from 1 to 10 of the population mean correspond to the sample mean, i.e, what is the probability that this population mean's rank is 1 or 2 or 3....or 10. Note that I used only one sample mean here. Second experiment:I sort all 10 sample means in the descending order so that the maximum number is in the first and the minimum number is in the last. Then compute the probability of the corresponding population means to be rank 1 if the sample mean's rank is 1, to be rank 2 if the sample mean's rank is 2, and so on. Here I used all sample means. My question is, are these two experiments equivalent? Note that I assume that I have the tools to compute the probability of the rank. I am are trying to find the ranking probabilities of the population means using sample means. In practice, we estimate population mean by sample mean but we never know the actual population mean. Thank you","Suppose I have 10 sample means. I want to find the probability of rank of the population means using sample means. Therefore, I want to perform two experiments. First experiment: I pick one of the sample means and compute the probability of being rank from 1 to 10 of the population mean correspond to the sample mean, i.e, what is the probability that this population mean's rank is 1 or 2 or 3....or 10. Note that I used only one sample mean here. Second experiment:I sort all 10 sample means in the descending order so that the maximum number is in the first and the minimum number is in the last. Then compute the probability of the corresponding population means to be rank 1 if the sample mean's rank is 1, to be rank 2 if the sample mean's rank is 2, and so on. Here I used all sample means. My question is, are these two experiments equivalent? Note that I assume that I have the tools to compute the probability of the rank. I am are trying to find the ranking probabilities of the population means using sample means. In practice, we estimate population mean by sample mean but we never know the actual population mean. Thank you",,"['probability', 'statistics', 'discrete-mathematics', 'statistical-inference']"
5,clarity on empirical probability,clarity on empirical probability,,"My definition of empirical probability: Let $(X_1,\ldots,X_n)$ be a random sample of i.i.d random variables. Then, the empirical probability for a subset A contained in the sample space $\Omega$ is $P_n(A)=\frac{1}{n}\sum_{i=1}^n \mathbb{1}(X_i \in A).$ Fine. What I don't get is that $P_n(A)$ is a random variable for a fixed event A. Therefore, won't it have it's own (theoretical) probability distribution? My understanding of empirical probability with an example: Say you roll a 6 sided die and you introduce the random variable X, where X=1 if you roll a $1$, $X=2$ if you roll a $2$ etc. You don't know anything about how $X$ is distributed (it may or may not be a fair die for example). Roll the same die $n$ times, producing the random sample $(X_1,\ldots,X_n)$ Let $A=\{1\}$ and $P(X \in A)=p$. By defintion, $P_n(A) = \{\text{counting the number of times 1 appears}\}/n$. Since the indicator function in this case is a random variable that has a Bernoulli distribution with parameter $p$, $nP_n(A)$ is just a random variable that has a binomial distribution with parameters $p$ and $n$. Why then is the empirical probability called ""empirical"" as if to suggest its distribution would be generated by experiment? I kind of just view this as related to (if not a generalization of) the sampling distribution of sample proportions, where $X_i$ are not necessarily Bernoulli.","My definition of empirical probability: Let $(X_1,\ldots,X_n)$ be a random sample of i.i.d random variables. Then, the empirical probability for a subset A contained in the sample space $\Omega$ is $P_n(A)=\frac{1}{n}\sum_{i=1}^n \mathbb{1}(X_i \in A).$ Fine. What I don't get is that $P_n(A)$ is a random variable for a fixed event A. Therefore, won't it have it's own (theoretical) probability distribution? My understanding of empirical probability with an example: Say you roll a 6 sided die and you introduce the random variable X, where X=1 if you roll a $1$, $X=2$ if you roll a $2$ etc. You don't know anything about how $X$ is distributed (it may or may not be a fair die for example). Roll the same die $n$ times, producing the random sample $(X_1,\ldots,X_n)$ Let $A=\{1\}$ and $P(X \in A)=p$. By defintion, $P_n(A) = \{\text{counting the number of times 1 appears}\}/n$. Since the indicator function in this case is a random variable that has a Bernoulli distribution with parameter $p$, $nP_n(A)$ is just a random variable that has a binomial distribution with parameters $p$ and $n$. Why then is the empirical probability called ""empirical"" as if to suggest its distribution would be generated by experiment? I kind of just view this as related to (if not a generalization of) the sampling distribution of sample proportions, where $X_i$ are not necessarily Bernoulli.",,"['statistics', 'probability-distributions']"
6,Proof related to the least squares method,Proof related to the least squares method,,"I've seen this exercise in several statistics text, but how they get to the final formula is something that I don't quite get. How do two squared terms suddenly become a binomial term? I've been trying to figure out how to get to the final formula but I don't get anywhere near. Hope you can help me.","I've seen this exercise in several statistics text, but how they get to the final formula is something that I don't quite get. How do two squared terms suddenly become a binomial term? I've been trying to figure out how to get to the final formula but I don't get anywhere near. Hope you can help me.",,"['sequences-and-series', 'statistics', 'least-squares']"
7,What is the difference between observed information and Fisher information?,What is the difference between observed information and Fisher information?,,I have in literature saying the observed information $J(\theta)$ is equal to the Fisher information $I(\theta)$. They are given different donations and same parameter. It is not clear why if equal they have different donations. Could anyone please explain?,I have in literature saying the observed information $J(\theta)$ is equal to the Fisher information $I(\theta)$. They are given different donations and same parameter. It is not clear why if equal they have different donations. Could anyone please explain?,,['statistics']
8,"When deciding the Hypothesis, do I, in some way, define the rejection region?","When deciding the Hypothesis, do I, in some way, define the rejection region?",,"This has always troubled me a bit. When I choose my hypothesis, do I define in some way the rejection region [RR], or, do I do that by choosing the test statistic I want to use? By fixing the significance level, I'm in a way determining the area/volume of the RR. In two different contexts(different null hypothesis), I've seen the same statistic being used with two different RR. In some books, the authors give the sense that once we decide the hypothesis, we've chosen the RR. Others, state explicitly that at least in some hypothesis, the RR is not completely defined, and we need other criteria... I would like to structure this as best as possible. Any help would be appreciated","This has always troubled me a bit. When I choose my hypothesis, do I define in some way the rejection region [RR], or, do I do that by choosing the test statistic I want to use? By fixing the significance level, I'm in a way determining the area/volume of the RR. In two different contexts(different null hypothesis), I've seen the same statistic being used with two different RR. In some books, the authors give the sense that once we decide the hypothesis, we've chosen the RR. Others, state explicitly that at least in some hypothesis, the RR is not completely defined, and we need other criteria... I would like to structure this as best as possible. Any help would be appreciated",,"['probability', 'statistics', 'statistical-inference']"
9,Can you suggest a method to generate random sample from following PDF?,Can you suggest a method to generate random sample from following PDF?,,"‎Let‎ ${‎‎\bf{\alpha}}=(\alpha_1, \alpha_2, \ldots, \alpha_m)$ ‎and ‎‎$‎‎\textbf{b}=(b_1, b_2, \ldots, b_m, b_{m+1}).$ I intend ‎to ‎generate ‎sample ‎from PDF $$ g(\alpha_1, \alpha_2, \ldots, \alpha_m)=Const. ‎\frac{(1+\sum_{i=1}^{m}\alpha_i)^{m+1}}{\prod_{j=1}^{2}(\sum_{i=1}^{m}\alpha_ic_i+d_j)^{a_j}}‎‎\times f_{Dirichlet}({\bf{\alpha}}) , ‎~~~\alpha_i, c_i, d_j, ‎a_j>0‎ $$ where ‎$‎f_{Dirichlet}‎$ ‎is ‎the ‎PDF ‎of ‎Dirichlet ‎Type-II ‎distribution, i.e.‎ $$ ‎f_{Dirichlet}({\bf{\alpha}})=‎\frac{\Gamma(\sum_{i=1}^{m+1}b_i) \prod_{i=1}^{m}\alpha_i^{b_i-1}}{\prod_{i=1}^{m+1}\Gamma(b_i)(1+\sum_{i=1}^{m}\alpha_i)^{\sum_{i=1}^{m+1}b_i}}‎~~~b_i>0. $$ I think the Metropolis-Hastings (MH) algorithm is useful. But I don't  know how to choose a suitable jump distribution for choosing candidate values. What is your idea?‎","‎Let‎ ${‎‎\bf{\alpha}}=(\alpha_1, \alpha_2, \ldots, \alpha_m)$ ‎and ‎‎$‎‎\textbf{b}=(b_1, b_2, \ldots, b_m, b_{m+1}).$ I intend ‎to ‎generate ‎sample ‎from PDF $$ g(\alpha_1, \alpha_2, \ldots, \alpha_m)=Const. ‎\frac{(1+\sum_{i=1}^{m}\alpha_i)^{m+1}}{\prod_{j=1}^{2}(\sum_{i=1}^{m}\alpha_ic_i+d_j)^{a_j}}‎‎\times f_{Dirichlet}({\bf{\alpha}}) , ‎~~~\alpha_i, c_i, d_j, ‎a_j>0‎ $$ where ‎$‎f_{Dirichlet}‎$ ‎is ‎the ‎PDF ‎of ‎Dirichlet ‎Type-II ‎distribution, i.e.‎ $$ ‎f_{Dirichlet}({\bf{\alpha}})=‎\frac{\Gamma(\sum_{i=1}^{m+1}b_i) \prod_{i=1}^{m}\alpha_i^{b_i-1}}{\prod_{i=1}^{m+1}\Gamma(b_i)(1+\sum_{i=1}^{m}\alpha_i)^{\sum_{i=1}^{m+1}b_i}}‎~~~b_i>0. $$ I think the Metropolis-Hastings (MH) algorithm is useful. But I don't  know how to choose a suitable jump distribution for choosing candidate values. What is your idea?‎",,"['statistics', 'probability-distributions', 'markov-chains', 'monte-carlo']"
10,Unusual graph measure,Unusual graph measure,,"Integrated information theory of consciousness is a complex mathematical model of information transfer in neural networks. Some of its conclusions are obvious: neither fully disconnected nor the complete structures have integrated information because it's ""too primitive"". Only intricate, multilevel structures may be related to consciousness. The procedure for calculating the measure of integrated information in IIT is enough complicated. I wondered what relatively simple well-known graph measures can be used to estimate integrated information. It is known that the number of edges of complete graph is  $n\cdot(n-1)/2$, so ""optimal"" structures should have number of edges not far from $n\cdot(n-1)/4$ But this tells us nothing about the internal structure of the graph, which can be quite different for the same number of edges. It seems that we should think about some graph clustering measure . But clustering coefficient shows a rather ambiguous results. So I have ""invented"" own measure which is very naive but shows more interesting results. Let $g$ is a connected subgraph of graph $G$ with $n_g$ edges (""internal links"") and $m_g$ - the number of edges connecting $g$ with $G\diagdown g$ (""external links""). So my measure for clustering of $g$ is just $\varkappa=n_g/m_g$ My first question is: Is there already such a definition? Does it make some nontrivial sense? At the moment, it is impossible for me to offer a measure of clustering all the graph based on $\varkappa$ for subgraphs. But (log) histograms of possible values $\varkappa$ for a given order of subgraphs look  interesting. Is there simple explanation for the specific form of the envelope of (log) histograms? Whether the spectrum of $\varkappa$ is to be a non-trivial measure of structuring of the graph?","Integrated information theory of consciousness is a complex mathematical model of information transfer in neural networks. Some of its conclusions are obvious: neither fully disconnected nor the complete structures have integrated information because it's ""too primitive"". Only intricate, multilevel structures may be related to consciousness. The procedure for calculating the measure of integrated information in IIT is enough complicated. I wondered what relatively simple well-known graph measures can be used to estimate integrated information. It is known that the number of edges of complete graph is  $n\cdot(n-1)/2$, so ""optimal"" structures should have number of edges not far from $n\cdot(n-1)/4$ But this tells us nothing about the internal structure of the graph, which can be quite different for the same number of edges. It seems that we should think about some graph clustering measure . But clustering coefficient shows a rather ambiguous results. So I have ""invented"" own measure which is very naive but shows more interesting results. Let $g$ is a connected subgraph of graph $G$ with $n_g$ edges (""internal links"") and $m_g$ - the number of edges connecting $g$ with $G\diagdown g$ (""external links""). So my measure for clustering of $g$ is just $\varkappa=n_g/m_g$ My first question is: Is there already such a definition? Does it make some nontrivial sense? At the moment, it is impossible for me to offer a measure of clustering all the graph based on $\varkappa$ for subgraphs. But (log) histograms of possible values $\varkappa$ for a given order of subgraphs look  interesting. Is there simple explanation for the specific form of the envelope of (log) histograms? Whether the spectrum of $\varkappa$ is to be a non-trivial measure of structuring of the graph?",,"['statistics', 'graph-theory', 'clustering']"
11,Expected Number of flips for alternating Heads/Tails 10 times,Expected Number of flips for alternating Heads/Tails 10 times,,"What is the expected number of flips needed to flip a coin 10 times and have the outcome be alternating heads/tails (starting with heads, then tails, then heads etc...). I wrote a c++ program and it gives me 2,730. Is this correct and how would you do this mathematically?","What is the expected number of flips needed to flip a coin 10 times and have the outcome be alternating heads/tails (starting with heads, then tails, then heads etc...). I wrote a c++ program and it gives me 2,730. Is this correct and how would you do this mathematically?",,['statistics']
12,Is the probability addition rule commutative?,Is the probability addition rule commutative?,,"The probability of $A$ and $B$ is the intersection between the venn diagrams for $A$ and $B$. Then is $P(A \cap B) = P(B \cap A)$? If so, then surely $\frac{P(A)}{P(B)} = \frac{P(A|B)}{P(B|A)}$. Is that correct? Let me rephrase the question.. Why is it always the case that the probability of A happening, then B happening is the same when the order is flipped?","The probability of $A$ and $B$ is the intersection between the venn diagrams for $A$ and $B$. Then is $P(A \cap B) = P(B \cap A)$? If so, then surely $\frac{P(A)}{P(B)} = \frac{P(A|B)}{P(B|A)}$. Is that correct? Let me rephrase the question.. Why is it always the case that the probability of A happening, then B happening is the same when the order is flipped?",,"['probability', 'statistics']"
13,information measure for matrix that is analogous to rank,information measure for matrix that is analogous to rank,,"Is there a measure for matrix that is analogous to rank of the matrix, but it is continuous on matrix elements? Say, we could say the information in identity matrix $I_n$ is $n$, and when the off-diagonal elements change from 0 to 1, the information contained in the matrix reduces continuously. Example: considering a matrix $A(a)$ \begin{pmatrix} 1&0&0&\\ 0&1&a\\ 0&a&1 \end{pmatrix}. How do I define an information measure $I(A(a))$ that is continuous to $a$ for matrix $A(a)$, so that $I(A(0)) = I(I_3) = 3$ and $I(A(1)) = I(I_2)= 2$? Rank is not continuous; Shannon information entropy on eigenvalues does not give desired values; and von Neumann entropy $S(\rho) = -tr(\rho\log(\rho))$ equals zero for any identity matrix while the dimension information lost; and $S(A(1))$ from my example does not reduce to S($I_2$).","Is there a measure for matrix that is analogous to rank of the matrix, but it is continuous on matrix elements? Say, we could say the information in identity matrix $I_n$ is $n$, and when the off-diagonal elements change from 0 to 1, the information contained in the matrix reduces continuously. Example: considering a matrix $A(a)$ \begin{pmatrix} 1&0&0&\\ 0&1&a\\ 0&a&1 \end{pmatrix}. How do I define an information measure $I(A(a))$ that is continuous to $a$ for matrix $A(a)$, so that $I(A(0)) = I(I_3) = 3$ and $I(A(1)) = I(I_2)= 2$? Rank is not continuous; Shannon information entropy on eigenvalues does not give desired values; and von Neumann entropy $S(\rho) = -tr(\rho\log(\rho))$ equals zero for any identity matrix while the dimension information lost; and $S(A(1))$ from my example does not reduce to S($I_2$).",,"['linear-algebra', 'matrices', 'measure-theory', 'statistics', 'information-theory']"
14,Measuring the degree of convergence of a stochastic process,Measuring the degree of convergence of a stochastic process,,"Consider a set of random variables $(X_1,X_2,X_3,...X_k)$ that are i.i.d. $Bernoulli(p)$ While I do not know $p$, I can estimate it using $$ Y(k)=\frac{1}{k}\sum_{i=1}^k X_i $$ Notice that $Y(k)$ is a random variable, and its distribution has mean $p$ and variance $\frac{p(1-p)}{k}$. So $Y(k)$ is a consistent estimator of $p$. Question: How can I determine the sample size that guarantees a minimum (arbitrary) 'degree of convergence' of the estimator? In other words, what is the point after which we can be confident that increasing the sample size from $k$ to $k+1$ will only yield a small change in our estimate of $p$ (for any $p$)? One idea I had was to look at the convergence of the variance of the sequence of estimators that is obtained by increasing $k$ gradually; that is  $$S(k)=\frac{1}{k}\sum_{i=1}^k (Y(i)-\bar{Y}(i))^2$$ for $\bar{Y(k)}=\frac{1}{k}\sum_{i=1}^k Y(i)$ Numerically, I find that $S(k)$ converges to $0$ as expected; so perhaps what I need is a condition on $S(k)$?","Consider a set of random variables $(X_1,X_2,X_3,...X_k)$ that are i.i.d. $Bernoulli(p)$ While I do not know $p$, I can estimate it using $$ Y(k)=\frac{1}{k}\sum_{i=1}^k X_i $$ Notice that $Y(k)$ is a random variable, and its distribution has mean $p$ and variance $\frac{p(1-p)}{k}$. So $Y(k)$ is a consistent estimator of $p$. Question: How can I determine the sample size that guarantees a minimum (arbitrary) 'degree of convergence' of the estimator? In other words, what is the point after which we can be confident that increasing the sample size from $k$ to $k+1$ will only yield a small change in our estimate of $p$ (for any $p$)? One idea I had was to look at the convergence of the variance of the sequence of estimators that is obtained by increasing $k$ gradually; that is  $$S(k)=\frac{1}{k}\sum_{i=1}^k (Y(i)-\bar{Y}(i))^2$$ for $\bar{Y(k)}=\frac{1}{k}\sum_{i=1}^k Y(i)$ Numerically, I find that $S(k)$ converges to $0$ as expected; so perhaps what I need is a condition on $S(k)$?",,"['sequences-and-series', 'statistics', 'convergence-divergence', 'stochastic-processes', 'numerical-methods']"
15,questions about 2 sample t-tests,questions about 2 sample t-tests,,"So I'm just a bit confused about 2 sample t-tests and just want to write out what I think I know and see if that's correct, so if anyone could tell me whether or not what I'm writting is true that would be great. What I'm mostly asking about is if you have 2 samples with populations $X$ and $Y$ respectively and you want to measure at some confindence intervel some hypothesis about the difference of their means. The part that's confusing me is I know 2 methods of doing this and I'm not sure which should be applied where so the first one is: 1) Let me start by saying you're always given $\sum x, \sum y, \sum x^2, \sum y^2$. So what we do is we calculate $\overline x, \overline y$ and we calculate the estimate for $Var(X)$ and $Var(Y)$ using the formulas $$\frac{1}{n}(\sum x^2 -\frac{(\sum x)^2}{n})$$ and then once we have both variances we then define $Z=X-Y$ for example, then we calculate $Var(Z)$ by $$Var(Z)=\frac{Var(X)}{n_x}+\frac{Var(Y)}{n_y}$$ and then we get our t-value by  $$t=\frac{\overline x - \overline y}{Var(Z)}$$ and then simple check the t-table. 2) Method 2 goes by calculating the pooled estimate of population variance through:$$S^2=\frac{\sum (x-\overline x)^2+\sum (y - \overline y)^2}{n_x + n_y -2}. $$ Then once we have that we calculate the t value using: $$t=\frac{\overline x - \overline y}{S^2(\frac{1}{n_x}+\frac{1}{n_y})}$$ Now my understanding is that method 2 requires the assumption that the variance of both $X$ and $Y$ is equal, is that the only difference between the two? Is the second method usually more accurate if the variance is actually equal","So I'm just a bit confused about 2 sample t-tests and just want to write out what I think I know and see if that's correct, so if anyone could tell me whether or not what I'm writting is true that would be great. What I'm mostly asking about is if you have 2 samples with populations $X$ and $Y$ respectively and you want to measure at some confindence intervel some hypothesis about the difference of their means. The part that's confusing me is I know 2 methods of doing this and I'm not sure which should be applied where so the first one is: 1) Let me start by saying you're always given $\sum x, \sum y, \sum x^2, \sum y^2$. So what we do is we calculate $\overline x, \overline y$ and we calculate the estimate for $Var(X)$ and $Var(Y)$ using the formulas $$\frac{1}{n}(\sum x^2 -\frac{(\sum x)^2}{n})$$ and then once we have both variances we then define $Z=X-Y$ for example, then we calculate $Var(Z)$ by $$Var(Z)=\frac{Var(X)}{n_x}+\frac{Var(Y)}{n_y}$$ and then we get our t-value by  $$t=\frac{\overline x - \overline y}{Var(Z)}$$ and then simple check the t-table. 2) Method 2 goes by calculating the pooled estimate of population variance through:$$S^2=\frac{\sum (x-\overline x)^2+\sum (y - \overline y)^2}{n_x + n_y -2}. $$ Then once we have that we calculate the t value using: $$t=\frac{\overline x - \overline y}{S^2(\frac{1}{n_x}+\frac{1}{n_y})}$$ Now my understanding is that method 2 requires the assumption that the variance of both $X$ and $Y$ is equal, is that the only difference between the two? Is the second method usually more accurate if the variance is actually equal",,"['statistics', 'statistical-inference']"
16,Use cdf to find expectation,Use cdf to find expectation,,"I have a cdf for a $\mathbf {discrete}$ random variable, $X$, $$F_X(x)=1-(1-p)^{xn}$$ where $p\in(0,1)$, $n\in\mathbb N$, $x\in\mathbb N$ My thought is to use $$E[X]=\sum_{x=0}^\infty (1-F_X(x))=\frac {1}{1-(1-p)^n}$$ the final answer correct but I'm not sure about my work. Does the expected value equal to this in the discrete case? If it does, why is that? And is writing the sum starting from $x=0$ correct and why? Or is there any other way to get the expected value using cdf? On the other hand, does the equation of expected value and cdf correct in the continuous case by changing the sum into integral?","I have a cdf for a $\mathbf {discrete}$ random variable, $X$, $$F_X(x)=1-(1-p)^{xn}$$ where $p\in(0,1)$, $n\in\mathbb N$, $x\in\mathbb N$ My thought is to use $$E[X]=\sum_{x=0}^\infty (1-F_X(x))=\frac {1}{1-(1-p)^n}$$ the final answer correct but I'm not sure about my work. Does the expected value equal to this in the discrete case? If it does, why is that? And is writing the sum starting from $x=0$ correct and why? Or is there any other way to get the expected value using cdf? On the other hand, does the equation of expected value and cdf correct in the continuous case by changing the sum into integral?",,"['statistics', 'probability-distributions', 'expectation', 'order-statistics', 'density-function']"
17,Properties of the solution of a linear system with random equations,Properties of the solution of a linear system with random equations,,"$x_i$ is drawn from $\mathrm{unif}(a,b)$, $y_i$ is drawn from $\mathrm{unif}(c,d)$. $x_i$ are independent from each other. $y_i$ are independent from each other. $x_i$ are independent of $y_i$. $i$ goes from $1$ to $n$. I have the linear system $$y_i=\alpha \cdot x_i+\beta$$ I solve the linear system for $\alpha$ and $\beta$ with least squares; what properties do the estimates of $\alpha$ and $\beta$ have?","$x_i$ is drawn from $\mathrm{unif}(a,b)$, $y_i$ is drawn from $\mathrm{unif}(c,d)$. $x_i$ are independent from each other. $y_i$ are independent from each other. $x_i$ are independent of $y_i$. $i$ goes from $1$ to $n$. I have the linear system $$y_i=\alpha \cdot x_i+\beta$$ I solve the linear system for $\alpha$ and $\beta$ with least squares; what properties do the estimates of $\alpha$ and $\beta$ have?",,"['statistics', 'systems-of-equations', 'uniform-distribution', 'least-squares']"
18,Why prefer the t-score when the sample size is low?,Why prefer the t-score when the sample size is low?,,"According to this link : The general rule of thumb for when to use a t score is when your sample size meets the following two requirements: The sample size is below 30 The population standard deviation is unknown (estimated from your sample data) In other words, you must know the standard deviation of the population and your sample size must be above 30 in order for you to be able to use the z-score. Otherwise, use the t-score. Question: Suppose that our sample size is below 30 and that we do know our population standard deviation. Why not use the z-score? I understand that according to the central limit theorem, with $n < 30$ so low we would have no expectation that our sample estimator would be normal. Does this have something to do with why we should prefer the t-score in this case?","According to this link : The general rule of thumb for when to use a t score is when your sample size meets the following two requirements: The sample size is below 30 The population standard deviation is unknown (estimated from your sample data) In other words, you must know the standard deviation of the population and your sample size must be above 30 in order for you to be able to use the z-score. Otherwise, use the t-score. Question: Suppose that our sample size is below 30 and that we do know our population standard deviation. Why not use the z-score? I understand that according to the central limit theorem, with so low we would have no expectation that our sample estimator would be normal. Does this have something to do with why we should prefer the t-score in this case?",n < 30,['statistics']
19,Testing the Uniformly Most Powerful Test against the alternative,Testing the Uniformly Most Powerful Test against the alternative,,"Hi I am working on the following problem A single observation $X$ is made from one of three densities listed below with parameter space $\Theta=\{0,1,2\}$. \begin{align*} x=0\hspace{0.4cm}x=1\hspace{.4cm}x=2\hspace{.4cm}x=3\hspace{.4cm}x=4\\ f(x|\theta=0)\,\,\,\,\,\, 0.05\hspace{0.9cm} 0.05\hspace{0.9cm} 0.40\hspace{0.9cm} 0.50\hspace{0.9cm}0.00\\ f(x|\theta=1)\,\,\,\,\,\, 0.30\hspace{0.9cm} 0.40\hspace{0.9cm} 0.05\hspace{0.9cm} 0.20\hspace{0.9cm}0.05\\ f(x|\theta=2)\,\,\,\,\,\, 0.40\hspace{0.9cm} 0.30\hspace{0.9cm} 0.10\hspace{0.9cm} 0.10\hspace{0.9cm}0.10 \end{align*} a) Find the likelihood ratio test of size $\alpha=0.1$ for testing $H_0: \theta=0$ against $H_1: \theta=\{1,2\}$ b) Is the test in part a UMP against the alternative? Why or why not? c) Determine whether there exists a UMP test of size $\alpha=0.05$ for testing $H_0: \theta=0$ against $H_1: \theta=\{1,2\}$. I got the part (a) which is  \begin{align*} R=\{x\in\{0,1,4\}\}\\ \alpha=P_{\theta=0}(x\in\{0,1,4\}) \end{align*} I am stuck with (b) and (c) any help would be highly appreciated. Thanks in advance.","Hi I am working on the following problem A single observation $X$ is made from one of three densities listed below with parameter space $\Theta=\{0,1,2\}$. \begin{align*} x=0\hspace{0.4cm}x=1\hspace{.4cm}x=2\hspace{.4cm}x=3\hspace{.4cm}x=4\\ f(x|\theta=0)\,\,\,\,\,\, 0.05\hspace{0.9cm} 0.05\hspace{0.9cm} 0.40\hspace{0.9cm} 0.50\hspace{0.9cm}0.00\\ f(x|\theta=1)\,\,\,\,\,\, 0.30\hspace{0.9cm} 0.40\hspace{0.9cm} 0.05\hspace{0.9cm} 0.20\hspace{0.9cm}0.05\\ f(x|\theta=2)\,\,\,\,\,\, 0.40\hspace{0.9cm} 0.30\hspace{0.9cm} 0.10\hspace{0.9cm} 0.10\hspace{0.9cm}0.10 \end{align*} a) Find the likelihood ratio test of size $\alpha=0.1$ for testing $H_0: \theta=0$ against $H_1: \theta=\{1,2\}$ b) Is the test in part a UMP against the alternative? Why or why not? c) Determine whether there exists a UMP test of size $\alpha=0.05$ for testing $H_0: \theta=0$ against $H_1: \theta=\{1,2\}$. I got the part (a) which is  \begin{align*} R=\{x\in\{0,1,4\}\}\\ \alpha=P_{\theta=0}(x\in\{0,1,4\}) \end{align*} I am stuck with (b) and (c) any help would be highly appreciated. Thanks in advance.",,"['statistics', 'statistical-inference']"
20,Paired T-Tests vs Independent,Paired T-Tests vs Independent,,"The effectiveness of a training course is examined, and performance of each individual in a group is taken both before and after, and the differences are used in a paired T test. Would it be possible to also perform a two-independent-samples t test to investigate the mean difference if the data before and after were mixed as to no longer be paired? Or would the independent condition still not be satisfied, as the two sets of observations are not independent?","The effectiveness of a training course is examined, and performance of each individual in a group is taken both before and after, and the differences are used in a paired T test. Would it be possible to also perform a two-independent-samples t test to investigate the mean difference if the data before and after were mixed as to no longer be paired? Or would the independent condition still not be satisfied, as the two sets of observations are not independent?",,"['statistics', 'statistical-inference']"
21,Type 2 Error Question - How to calculate for a two tailed?,Type 2 Error Question - How to calculate for a two tailed?,,"The modulus of rupture (MOR) for a particular grade of pencil lead is known to have a standard deviation of 250 psi. Process standards call for a target value of 6500 psi for the true mean MOR. For each batch, an inspector tests a random sample of 16 leads. Management wishes to detect any change in the true mean MOR. (Assume normal distribution.) QUESTION : Find the probability of type II error of the test when the true mean MOR is 6400. How do we solve for two tails? i am able to get 0.4821 but with 1 tail method.  6397-6400/62.5 = 0.045 than using normcdf on MATLAB, i got this value","The modulus of rupture (MOR) for a particular grade of pencil lead is known to have a standard deviation of 250 psi. Process standards call for a target value of 6500 psi for the true mean MOR. For each batch, an inspector tests a random sample of 16 leads. Management wishes to detect any change in the true mean MOR. (Assume normal distribution.) QUESTION : Find the probability of type II error of the test when the true mean MOR is 6400. How do we solve for two tails? i am able to get 0.4821 but with 1 tail method.  6397-6400/62.5 = 0.045 than using normcdf on MATLAB, i got this value",,"['probability', 'statistics', 'data-analysis', 'hypothesis-testing']"
22,Facebook Data Science Question (Expected Payout and Probability),Facebook Data Science Question (Expected Payout and Probability),,"I saw this question on Glassdoor and couldn't seem to find a answer to validate mine anywhere: You're at a casino with two dice, if you roll a 5 you win, and get paid $10.    What is your expected payout? If you play until you win (however long that    takes) then stop, what is your expected payout? I interpret this as ""if at least one of the dice gives you a 5"", so expected payout for one roll: $(1 - (\frac{5}{6})^2) \times 10$ But I am kind of confused on calculating expected payout until you win. Is it: $(1 - (\frac{5}{6})^2)^n \times 10$ where you have to indicate the number of rolls? Or is there another way? I am fairly new to this so I really appreciate your help! Thanks!","I saw this question on Glassdoor and couldn't seem to find a answer to validate mine anywhere: You're at a casino with two dice, if you roll a 5 you win, and get paid $10.    What is your expected payout? If you play until you win (however long that    takes) then stop, what is your expected payout? I interpret this as ""if at least one of the dice gives you a 5"", so expected payout for one roll: $(1 - (\frac{5}{6})^2) \times 10$ But I am kind of confused on calculating expected payout until you win. Is it: $(1 - (\frac{5}{6})^2)^n \times 10$ where you have to indicate the number of rolls? Or is there another way? I am fairly new to this so I really appreciate your help! Thanks!",,"['probability', 'statistics', 'bayesian']"
23,sampling distribution question,sampling distribution question,,"Need clarification on a binomial sample example: we drew a sample of size $100$ from a binomial($m = 2$,$p = 0.2$) distribution and observed $76$ of the $x_i = 0$, $20$ of the $x_i = 1$ and $4$ of the $x_i = 2$ now as  $n → ∞$ the empirical distribution will look more and more like the binomial$(2,0.2)$ it was drawn from (why?). I am guessing this is because of Law of Large No. (not sure if WLLN or SLLN).  The empirical dist mean and variance will approach Binomial dist mean and variance?? my class notes has the following explanation: For example, if $X_i ∼ Binom(2, 0.2)$ then for $n = 100$, $$  P \bigg( \frac{1}{n} \sum_{i=1}^{n} 1_{[X_i=0]}  \geq 0.76 \bigg) \simeq	0.007   $$ but for $n = 1000$ $$  P \bigg( \frac{1}{n} \sum_{i=1}^{n} 1_{[X_i=0]}  \geq 0.76 \bigg) \simeq	2.3\times(10^{-16})   $$ I understand the LHS in the probability function, that is the total proportion of $x_i = 0$, but I dont get why RHS is $0.76$, shouldn't it be $0.64\,$ (which is the probability of $x_i = 0$, from R code dbinom(0,2,0.2)) .","Need clarification on a binomial sample example: we drew a sample of size $100$ from a binomial($m = 2$,$p = 0.2$) distribution and observed $76$ of the $x_i = 0$, $20$ of the $x_i = 1$ and $4$ of the $x_i = 2$ now as  $n → ∞$ the empirical distribution will look more and more like the binomial$(2,0.2)$ it was drawn from (why?). I am guessing this is because of Law of Large No. (not sure if WLLN or SLLN).  The empirical dist mean and variance will approach Binomial dist mean and variance?? my class notes has the following explanation: For example, if $X_i ∼ Binom(2, 0.2)$ then for $n = 100$, $$  P \bigg( \frac{1}{n} \sum_{i=1}^{n} 1_{[X_i=0]}  \geq 0.76 \bigg) \simeq	0.007   $$ but for $n = 1000$ $$  P \bigg( \frac{1}{n} \sum_{i=1}^{n} 1_{[X_i=0]}  \geq 0.76 \bigg) \simeq	2.3\times(10^{-16})   $$ I understand the LHS in the probability function, that is the total proportion of $x_i = 0$, but I dont get why RHS is $0.76$, shouldn't it be $0.64\,$ (which is the probability of $x_i = 0$, from R code dbinom(0,2,0.2)) .",,"['statistics', 'sampling']"
24,using Negative values for a box plot (box whisker),using Negative values for a box plot (box whisker),,"I am drawing a box plot for a question where the data set is the bulb life time(in hours) My 5 number summary is Q1  : 208.25 Q2  : 297 Q3  : 376.5 min : 172 max : 1020  IQR : 168.25 3/2 * IQR : 252.375 Upper Bound : 628.875 Lower Bound : -44.125 Since the data set is about time, I know I can not use negative values,so what should I use as my negative value, I know I won't get any outliers here but still I'm curious.","I am drawing a box plot for a question where the data set is the bulb life time(in hours) My 5 number summary is Q1  : 208.25 Q2  : 297 Q3  : 376.5 min : 172 max : 1020  IQR : 168.25 3/2 * IQR : 252.375 Upper Bound : 628.875 Lower Bound : -44.125 Since the data set is about time, I know I can not use negative values,so what should I use as my negative value, I know I won't get any outliers here but still I'm curious.",,"['statistics', 'descriptive-statistics']"
25,Clarifying the assumptions about a paired t-test,Clarifying the assumptions about a paired t-test,,"I've wrote my question in red ink (see links). There are two questions that I have. Primarily I want to know why they concluded that ""there is some evidence that there is some difference in mean circulating levels of androgens"" even though the t-statistic 2.06 value falls within the no rejection region $-2.145 < t < 2.145,$ another reason for not rejecting the $H_0$ is because our p-value of 0.6 is greater than our significance level of 0.5. These two things agree with one another to not reject $H_0$ but they still concluded that there is some difference. Or maybe the reason why they said ""there is SOME evidence that there is a difference in mean"" is because the t-value of 2.06 is so close to the critical value of 2.145? Another question I have pertains to to how they got the p-value of 0.6. It has that squiggly equal sign $\approx$ so it means roughly equal to 0.6. I've uploaded the t-distribution table. With $\nu=14$ and we're finding the probability of $T > 2.06.$ Now whatever we get, we need to multiply by 2 since we have a two-tailed test. I found that probability to be between 0.05 and 0.025, indicated with red horizontal and vertical lines. Half way between those two proportion is 0.0375, multiply be 2 (since two-tailed test), which equals to 0.075. Everything is illustrated in both images. Are my assumptions correct? Sorry it's a bit lengthy but I'm just really CURIOUS. Click here to see the question Click here to see the t-distribution table","I've wrote my question in red ink (see links). There are two questions that I have. Primarily I want to know why they concluded that ""there is some evidence that there is some difference in mean circulating levels of androgens"" even though the t-statistic 2.06 value falls within the no rejection region $-2.145 < t < 2.145,$ another reason for not rejecting the $H_0$ is because our p-value of 0.6 is greater than our significance level of 0.5. These two things agree with one another to not reject $H_0$ but they still concluded that there is some difference. Or maybe the reason why they said ""there is SOME evidence that there is a difference in mean"" is because the t-value of 2.06 is so close to the critical value of 2.145? Another question I have pertains to to how they got the p-value of 0.6. It has that squiggly equal sign $\approx$ so it means roughly equal to 0.6. I've uploaded the t-distribution table. With $\nu=14$ and we're finding the probability of $T > 2.06.$ Now whatever we get, we need to multiply by 2 since we have a two-tailed test. I found that probability to be between 0.05 and 0.025, indicated with red horizontal and vertical lines. Half way between those two proportion is 0.0375, multiply be 2 (since two-tailed test), which equals to 0.075. Everything is illustrated in both images. Are my assumptions correct? Sorry it's a bit lengthy but I'm just really CURIOUS. Click here to see the question Click here to see the t-distribution table",,"['statistics', 'statistical-inference']"
26,Can a prediction interval be interpreted as a probability?,Can a prediction interval be interpreted as a probability?,,"Suppose I find a 90% prediction interval for some data distribution. This implies that if I sample large enough data from this distribution, then 90% of such data will lie inside the prediction interval. Is it same as saying that any randomly sampled data point from the distribution will lie inside the prediction interval with 0.9 probability?","Suppose I find a 90% prediction interval for some data distribution. This implies that if I sample large enough data from this distribution, then 90% of such data will lie inside the prediction interval. Is it same as saying that any randomly sampled data point from the distribution will lie inside the prediction interval with 0.9 probability?",,"['statistics', 'regression', 'estimation', 'regression-analysis', 'confidence-interval']"
27,Covariance of a random vector,Covariance of a random vector,,"If $Y_1,..Y_n$ are independent random variables, how do I work out cov$\begin{pmatrix}Y_1\\.\\.\\.\\Y_n \end{pmatrix}$? The covariance of the vector","If $Y_1,..Y_n$ are independent random variables, how do I work out cov$\begin{pmatrix}Y_1\\.\\.\\.\\Y_n \end{pmatrix}$? The covariance of the vector",,['statistics']
28,Literature on transformed Gaussian matrices,Literature on transformed Gaussian matrices,,"I am considering real $n$-by-$m$ matrices of the following type: $$ M=SM^\prime,\\ M^\prime_{ij} \overset{\text{iid}} \sim N(0,1). $$ Here, $S$ is a fixed $n$-by-$n$ matrix and the entries of $M^\prime$ (same size as $M$) are just i.i.d Gaussian. It is important that the considered matrices are rectangular and not simply square. $S$ can be identity but, ideally, should be an arbitrary full-rank matrix. As far as I know, in the special case of $S=I, n=m$ this construction is called the real Ginibre ensemble . Can anyone suggest some literature/references for the more general case? I'm particularly interested in spectral properties of these matrices such as singular value/vector distributions.","I am considering real $n$-by-$m$ matrices of the following type: $$ M=SM^\prime,\\ M^\prime_{ij} \overset{\text{iid}} \sim N(0,1). $$ Here, $S$ is a fixed $n$-by-$n$ matrix and the entries of $M^\prime$ (same size as $M$) are just i.i.d Gaussian. It is important that the considered matrices are rectangular and not simply square. $S$ can be identity but, ideally, should be an arbitrary full-rank matrix. As far as I know, in the special case of $S=I, n=m$ this construction is called the real Ginibre ensemble . Can anyone suggest some literature/references for the more general case? I'm particularly interested in spectral properties of these matrices such as singular value/vector distributions.",,"['probability-theory', 'statistics', 'reference-request', 'random-matrices']"
29,Find f(z) where Z= X+Y,Find f(z) where Z= X+Y,,"Let $f_{X,Y}(x,y) = \frac{1}{8}$ for $-2<x<2$ and $0<y<2$. Find $f(z)$ where $Z = X+Y.$ Should I find the marginal of X and Y first, then $$f(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x)dx ?$$","Let $f_{X,Y}(x,y) = \frac{1}{8}$ for $-2<x<2$ and $0<y<2$. Find $f(z)$ where $Z = X+Y.$ Should I find the marginal of X and Y first, then $$f(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x)dx ?$$",,"['statistics', 'probability-distributions']"
30,"What does the ""$+$"" typically denote when summing random variables?","What does the """" typically denote when summing random variables?",+,"Let $X_1$ and $X_2$ be two random variables. When in the literature (for example, in the context of the law of large numbers) one sees statements along the lines of $$ X = X_1 + X_2 $$ ...does the ""$+$"" simply denote normal function addition, as in $$ X(k) = X_1(k) + X_2(k) $$ for all $k$ the domain of $X$, $X_1$, and $X_2$?  Or does this $+$ denote something else?","Let $X_1$ and $X_2$ be two random variables. When in the literature (for example, in the context of the law of large numbers) one sees statements along the lines of $$ X = X_1 + X_2 $$ ...does the ""$+$"" simply denote normal function addition, as in $$ X(k) = X_1(k) + X_2(k) $$ for all $k$ the domain of $X$, $X_1$, and $X_2$?  Or does this $+$ denote something else?",,"['statistics', 'random-variables']"
31,what sample size is necessary for 95% CI?,what sample size is necessary for 95% CI?,,"My take: FOR a) $n=10,$ UCB = $95$%, then $z_{\alpha/2}$ $= 1.645$ as we denote $p = 0.1 = w$, using $$n = \frac{\frac{z^2_{\alpha/2}}{2} - z^2_{\alpha/2}w^2 + \sqrt {z^4_{\alpha/2}(0.25-w^2)+w^2z^4_{\alpha/2}}}{w^2}$$ $$n = 380.3 = 381$$ FOR b) I've treid $p = \frac{2}3 =w$, and use same formula as above, but I am not getting an answer I'm supposed to get, which is 339... what did I do wrong?","My take: FOR a) $n=10,$ UCB = $95$%, then $z_{\alpha/2}$ $= 1.645$ as we denote $p = 0.1 = w$, using $$n = \frac{\frac{z^2_{\alpha/2}}{2} - z^2_{\alpha/2}w^2 + \sqrt {z^4_{\alpha/2}(0.25-w^2)+w^2z^4_{\alpha/2}}}{w^2}$$ $$n = 380.3 = 381$$ FOR b) I've treid $p = \frac{2}3 =w$, and use same formula as above, but I am not getting an answer I'm supposed to get, which is 339... what did I do wrong?",,"['statistics', 'confidence-interval']"
32,The probability of a successful optical alignment in an assembly of an optical data storage product is 0.9.,The probability of a successful optical alignment in an assembly of an optical data storage product is 0.9.,,"Can someone tell me if I did part a,b,c correctly and help me with part d? Thank you for the help. (a) What is the probability that the ﬁrst successful alignment requires exactly four trials? P(x=4) = .9 * (.1)^3 = .0009 (b) What is the probability that the ﬁrst successful alignment requires at most four trials? P(x <= 4) so it would be P(x=1)+ P(x=2)+P(x=3)+P(x=4) = .9999 (c) What is the probability that the ﬁrst successful alignment requires at least four trials? P(x>4) so it would be 1-p(x<=4) = .001 (d)What is the average number of trials necessary to obtain the ﬁrst successful alignment? I'm lost on this one.","Can someone tell me if I did part a,b,c correctly and help me with part d? Thank you for the help. (a) What is the probability that the ﬁrst successful alignment requires exactly four trials? P(x=4) = .9 * (.1)^3 = .0009 (b) What is the probability that the ﬁrst successful alignment requires at most four trials? P(x <= 4) so it would be P(x=1)+ P(x=2)+P(x=3)+P(x=4) = .9999 (c) What is the probability that the ﬁrst successful alignment requires at least four trials? P(x>4) so it would be 1-p(x<=4) = .001 (d)What is the average number of trials necessary to obtain the ﬁrst successful alignment? I'm lost on this one.",,"['probability', 'statistics']"
33,Auto-covariance of Multiply Process,Auto-covariance of Multiply Process,,"Given the process $I=XY$, where $X$ and $Y$ are: independent WSS with auto-covariances $B_X(\tau)=E\left\lbrace x(t)x(t+\tau)\right\rbrace-E^2\left\lbrace X\right\rbrace$ and $B_Y(\tau)$ $E\left\lbrace X\right\rbrace >0$, $E\left\lbrace Y\right\rbrace >0$ positive-axis distributed (non-negative) The question is, what is $B_I(\tau)$ as a function of  $B_X(\tau)$ and $B_Y(\tau)$ ? I've found in literature, that for special normalized case of $\hat{I}=I/{E\left\lbrace I\right\rbrace}$, the resulting auto-covariance is $B_I(\tau) = B_X(\tau)+B_Y(\tau)+B_X(\tau)B_Y(\tau)$. Unfortunately, I could not even prove the relation above.","Given the process $I=XY$, where $X$ and $Y$ are: independent WSS with auto-covariances $B_X(\tau)=E\left\lbrace x(t)x(t+\tau)\right\rbrace-E^2\left\lbrace X\right\rbrace$ and $B_Y(\tau)$ $E\left\lbrace X\right\rbrace >0$, $E\left\lbrace Y\right\rbrace >0$ positive-axis distributed (non-negative) The question is, what is $B_I(\tau)$ as a function of  $B_X(\tau)$ and $B_Y(\tau)$ ? I've found in literature, that for special normalized case of $\hat{I}=I/{E\left\lbrace I\right\rbrace}$, the resulting auto-covariance is $B_I(\tau) = B_X(\tau)+B_Y(\tau)+B_X(\tau)B_Y(\tau)$. Unfortunately, I could not even prove the relation above.",,"['statistics', 'covariance', 'stationary-processes']"
34,Determining the sampling distribution [closed],Determining the sampling distribution [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let the random variable $X$ represent the number of defective components in  a lot of components. Assume that $X$ can take on four values: $0, 1, 2, 3$. The probability distribution of $X$ is shown in the table below: X   |  0  |  1  |  2  | 3  P(X) | 0.4 | 0.2 | 0.1 | 0.3 1) Randomly pick two lots of components, what is the sampling distribution of average  number of defective components in a lot. 2) Find $Pr(\bar{X} > 2)$. How do I go about approaching this problem?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let the random variable $X$ represent the number of defective components in  a lot of components. Assume that $X$ can take on four values: $0, 1, 2, 3$. The probability distribution of $X$ is shown in the table below: X   |  0  |  1  |  2  | 3  P(X) | 0.4 | 0.2 | 0.1 | 0.3 1) Randomly pick two lots of components, what is the sampling distribution of average  number of defective components in a lot. 2) Find $Pr(\bar{X} > 2)$. How do I go about approaching this problem?",,"['probability', 'statistics', 'probability-distributions']"
35,Interpreting the scatter plots of two random variables,Interpreting the scatter plots of two random variables,,"Suppose I sample random variables A and B from normal distributions. When I do a scatter plot of these two variables I see a radial pattern centered around (0,0). If I zoom into the circle I see that the points look roughly uniform, which is what I expect since they are not correlated. Now say I have two random variables C and `D from t-distributions. When I do a scatter plot of these two variables I see a 't' shape which seems to imply they are somehow correlated. However they are not. I am wondering if I am interpreting these correctly or if I need to perform some transformation in order to see the uniformity that I expect.","Suppose I sample random variables A and B from normal distributions. When I do a scatter plot of these two variables I see a radial pattern centered around (0,0). If I zoom into the circle I see that the points look roughly uniform, which is what I expect since they are not correlated. Now say I have two random variables C and `D from t-distributions. When I do a scatter plot of these two variables I see a 't' shape which seems to imply they are somehow correlated. However they are not. I am wondering if I am interpreting these correctly or if I need to perform some transformation in order to see the uniformity that I expect.",,"['statistics', 'correlation', 'descriptive-statistics']"
36,"If X ∼ N(0, σ2), find the pdf of Y = |X|.","If X ∼ N(0, σ2), find the pdf of Y = |X|.",,"If X ∼ N(0, $σ^2$ ), find the pdf of Y = |X|. So far I have $F_Y(y) = P(\lvert x \rvert < y) = P(-y < x < y) = F_X(y) - F_X(-y)$ but I don't know where to go from there","If X ∼ N(0, $σ^2$ ), find the pdf of Y = |X|. So far I have $F_Y(y) = P(\lvert x \rvert < y) = P(-y < x < y) = F_X(y) - F_X(-y)$ but I don't know where to go from there",,"['probability', 'statistics', 'probability-distributions']"
37,How can I create a $1-\alpha$ nonparametric confidence interval for the median using order statistics?,How can I create a  nonparametric confidence interval for the median using order statistics?,1-\alpha,"I need to create a 95% confidence interval for the median based on 9 ordered statistics. I know how to determine the confidence level for a given interval but I admit I'm stuck when it comes to creating the interval for a given level. Here is my work so far: $0.95=P(Y_i<m<Y_j)$ $\implies0.05=P(i-0.5<W<j+0.5)$ adjusting for correction, with $W\sim\mathrm{bin}(9,0.5)$ with $\mu=4.5$ and $\sigma^2=2.25$ By the central limit theorem, we have that $\frac{W-\mu}{\sigma}\approx Z$, where $Z\sim N(0,1)$. Therefore, $0.95=P(\frac{i-5}{1.5}<Z<\frac{j-4}{1.5})$ There must clearly be a more efficient way of finding $i$ and $j$ then by looking in my tables for the normal distribution in order to find a match. Am I doing something wrong? Also, is there a way to solve a problem like this in R?","I need to create a 95% confidence interval for the median based on 9 ordered statistics. I know how to determine the confidence level for a given interval but I admit I'm stuck when it comes to creating the interval for a given level. Here is my work so far: $0.95=P(Y_i<m<Y_j)$ $\implies0.05=P(i-0.5<W<j+0.5)$ adjusting for correction, with $W\sim\mathrm{bin}(9,0.5)$ with $\mu=4.5$ and $\sigma^2=2.25$ By the central limit theorem, we have that $\frac{W-\mu}{\sigma}\approx Z$, where $Z\sim N(0,1)$. Therefore, $0.95=P(\frac{i-5}{1.5}<Z<\frac{j-4}{1.5})$ There must clearly be a more efficient way of finding $i$ and $j$ then by looking in my tables for the normal distribution in order to find a match. Am I doing something wrong? Also, is there a way to solve a problem like this in R?",,"['statistics', 'confidence-interval']"
38,Confidence Intervals for a Large Sample,Confidence Intervals for a Large Sample,,"The following $95\%$ confidence interval was constructed using a large sample of data: $(86.52,89.48)$ . Which of the following could be a $99\%$ confidence interval for the same set of data? $I. (86.98,89.02)$ $II. (86.37,89.63)$ $III. (87.04,88.98)$ My attempt: It is a large sample of data so we can approximate the sampling distribution with a Normal model. The mean is $$\bar{x} = \frac{86.52+89.48}{2}=88$$ The margin of error for the $95\%$ confidence interval is $$z^*\cdot (\text{Standard Error}) = 1.96(SE) = (89.48-88) = 1.48$$ This gives us that $SE$ is $.755$ . The critical $z$ value for a $99\%$ interval is about $2.58$ . The new margin of error is now $.755\cdot2.58 = 1.95$ So the $99\%$ confidence interval is now $(88-1.95,88+1.95) = (86.05,89.95)$ . Which is not one of the answers. Where did I go wrong?",The following confidence interval was constructed using a large sample of data: . Which of the following could be a confidence interval for the same set of data? My attempt: It is a large sample of data so we can approximate the sampling distribution with a Normal model. The mean is The margin of error for the confidence interval is This gives us that is . The critical value for a interval is about . The new margin of error is now So the confidence interval is now . Which is not one of the answers. Where did I go wrong?,"95\% (86.52,89.48) 99\% I. (86.98,89.02) II. (86.37,89.63) III. (87.04,88.98) \bar{x} = \frac{86.52+89.48}{2}=88 95\% z^*\cdot (\text{Standard Error}) = 1.96(SE) = (89.48-88) = 1.48 SE .755 z 99\% 2.58 .755\cdot2.58 = 1.95 99\% (88-1.95,88+1.95) = (86.05,89.95)","['statistics', 'confidence-interval']"
39,Mode of a frequency distribution with unequal class length,Mode of a frequency distribution with unequal class length,,"How can I find the mode for a grouped frequency distribution with unequal class lengths? I have to find the mode for the following problem: \begin{array}{c|c}  \text{Marks} & \text{# of Students} \\ \hline \text{0 - 20} & 32 \\ \hline \text{20 - 50} & 45 \\ \hline \text{50 - 70} & 15 \\ \hline \text{70 - 100} & 8 \\ \hline \end{array}$$ For equal class lengths, we use the formula $$\text{Mode} = l+\frac{(f_0-f_{-1})}{2f_0-f_{-1}-f_{+1}}W_o$$ where $l$ is the lower class boundary of the modal class, $f_0$ is the frequency of the modal class, $f_{-1}$ is the frequency preceding the modal class,  $f_{+1}$ is the frequency following the modal class, $W_{o}$ is the class width of the modal class But how to proceed for the above example?","How can I find the mode for a grouped frequency distribution with unequal class lengths? I have to find the mode for the following problem: \begin{array}{c|c}  \text{Marks} & \text{# of Students} \\ \hline \text{0 - 20} & 32 \\ \hline \text{20 - 50} & 45 \\ \hline \text{50 - 70} & 15 \\ \hline \text{70 - 100} & 8 \\ \hline \end{array}$$ For equal class lengths, we use the formula $$\text{Mode} = l+\frac{(f_0-f_{-1})}{2f_0-f_{-1}-f_{+1}}W_o$$ where $l$ is the lower class boundary of the modal class, $f_0$ is the frequency of the modal class, $f_{-1}$ is the frequency preceding the modal class,  $f_{+1}$ is the frequency following the modal class, $W_{o}$ is the class width of the modal class But how to proceed for the above example?",,['statistics']
40,Can someone give me an idea of finding the distribution of $\frac{\sum_{i=1}^N (X_i-E(X))^2}{\sum_{i=1}^M (Y_i-E(Y))^2}$,Can someone give me an idea of finding the distribution of,\frac{\sum_{i=1}^N (X_i-E(X))^2}{\sum_{i=1}^M (Y_i-E(Y))^2},"X~N(4, $\sigma^2$) and  Y~N(1, $\sigma^2$) are independent. $$A=\frac{\sum_{i=1}^N (X_i-E(X))^2}{\sum_{i=1}^M (Y_i-E(Y))^2}$$ Find the distribution of A? I tried this way. $$\frac{M-1}{N-1}A=\frac{\frac{1}{N-1}\sum_{i=1}^N (X_i-E(X))^2}{\frac{1}{M-1}\sum_{i=1}^M (Y_i-E(Y))^2}=\frac{S_x^2}{S_y^2}=\frac{(\frac{N-1}{\sigma^2})S_x^2/(N-1)}{(\frac{M-1}{\sigma^2})S_y^2/(M-1)}$$ So I concluded that $\frac{M-1}{N-1}A$ ~$F_N-_1,_M-_1$ But the question is about A, not about $\frac{M-1}{N-1}A$ I need some help! Please give me an advice.","X~N(4, $\sigma^2$) and  Y~N(1, $\sigma^2$) are independent. $$A=\frac{\sum_{i=1}^N (X_i-E(X))^2}{\sum_{i=1}^M (Y_i-E(Y))^2}$$ Find the distribution of A? I tried this way. $$\frac{M-1}{N-1}A=\frac{\frac{1}{N-1}\sum_{i=1}^N (X_i-E(X))^2}{\frac{1}{M-1}\sum_{i=1}^M (Y_i-E(Y))^2}=\frac{S_x^2}{S_y^2}=\frac{(\frac{N-1}{\sigma^2})S_x^2/(N-1)}{(\frac{M-1}{\sigma^2})S_y^2/(M-1)}$$ So I concluded that $\frac{M-1}{N-1}A$ ~$F_N-_1,_M-_1$ But the question is about A, not about $\frac{M-1}{N-1}A$ I need some help! Please give me an advice.",,['statistics']
41,T-Statistics and s Calculations,T-Statistics and s Calculations,,How can I calculate s in T statistics? Example: John H. takes one sample of size 20 and finds that the sample mean in 32.8. Calculate a 95% confidence interval for John. (Assume John knows the true standard deviation.)??? I know what formula to use but I want to understand how can I calculate s without knowing the sample #s.,How can I calculate s in T statistics? Example: John H. takes one sample of size 20 and finds that the sample mean in 32.8. Calculate a 95% confidence interval for John. (Assume John knows the true standard deviation.)??? I know what formula to use but I want to understand how can I calculate s without knowing the sample #s.,,"['probability', 'statistics']"
42,Question on Type I and Type II Errors for a Test of Binomial $p$ [closed],Question on Type I and Type II Errors for a Test of Binomial  [closed],p,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question A random variable $Y$ representing the number of successes can be modeled by a binomial distribution with parameters $n=250$ and $p,$ whose value is unknown. A significance test is performed, based on a sample value $Y,$ to test the hypothesis $p=0.6$ against the alternative hypothesis $p>0.6.$ The probability of Type I error is $0.05.$ a. Find the critical region for $Y$. b. Find the probability of making a Type II error in the case when in actual fact $p=0.675.$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question A random variable $Y$ representing the number of successes can be modeled by a binomial distribution with parameters $n=250$ and $p,$ whose value is unknown. A significance test is performed, based on a sample value $Y,$ to test the hypothesis $p=0.6$ against the alternative hypothesis $p>0.6.$ The probability of Type I error is $0.05.$ a. Find the critical region for $Y$. b. Find the probability of making a Type II error in the case when in actual fact $p=0.675.$",,"['statistics', 'hypothesis-testing']"
43,I need someone to check my work for this normally distributed random variable question (Statistics)?,I need someone to check my work for this normally distributed random variable question (Statistics)?,,"The image contains the question details This is my work:  a) (50-42)/10 = 0.8 => z-value = 0.7881    (32-42)/10 = -1 => z-value = 0.1587    0.7881 - 0.1587 = 0.6294 = 62.94%  b) 10% = 0.1, corresponding z-value = 0.5398    x = μ+zσ    x = 42 + (-1.28)(10) = 29.1 = 29 months c) n = 75    (41-42)/(10/sqrt(75)) = -0.87 => z-value = 0.1922    (40-42)/(10/sqrt(75)) = -1.73 => z-value = 0.0418    0.1922 - 0.0418 = 0.1504","The image contains the question details This is my work:  a) (50-42)/10 = 0.8 => z-value = 0.7881    (32-42)/10 = -1 => z-value = 0.1587    0.7881 - 0.1587 = 0.6294 = 62.94%  b) 10% = 0.1, corresponding z-value = 0.5398    x = μ+zσ    x = 42 + (-1.28)(10) = 29.1 = 29 months c) n = 75    (41-42)/(10/sqrt(75)) = -0.87 => z-value = 0.1922    (40-42)/(10/sqrt(75)) = -1.73 => z-value = 0.0418    0.1922 - 0.0418 = 0.1504",,['statistics']
44,Bayesian inference exercise,Bayesian inference exercise,,"I am learning online Bayesian Statistics and I have a test in a couple of days. I have no idea how to solve this exercise, any help will be appreciated. There might be something similar in the quiz... Statistical decision theory: a decision-theoretic approach to the estimation of an unknown parameter $\theta$ introduces the loss function $L(\theta, a)$ which, loosely speaking, gives the cost of deciding that the parameter has the value $a$, when it is in fact equal to $\theta$. The estimate $a$ can be chosen to minimize the posterior expected loss, $$E(L(a|y))=  \int L(\theta,a)p(\theta|y)d\theta$$ This optimal choice of $a$ is called a Bayes estimate for the loss function $L$. Show that: (a) If $L(\theta, a) = (\theta − a)^2$ (squared error loss), then the posterior mean, $E(\theta|y)$, if it exists, is the unique Bayes estimate of $\theta$. (b) If $L(\theta, a) = |\theta − a|$, then any posterior median of $\theta$ is a Bayes estimate of $\theta$. (c) If $k_0$ and $k_1$ are non negative numbers, not both zero, and $L(\theta,a)= k_0(\theta−a)$ if $\theta\geq a$,  $k_1(a−\theta)$ if $\theta<a$, then any $k_0$ quantile of the posterior distribution $p(\theta|y)$ is a Bayes estimate of $\theta$.","I am learning online Bayesian Statistics and I have a test in a couple of days. I have no idea how to solve this exercise, any help will be appreciated. There might be something similar in the quiz... Statistical decision theory: a decision-theoretic approach to the estimation of an unknown parameter $\theta$ introduces the loss function $L(\theta, a)$ which, loosely speaking, gives the cost of deciding that the parameter has the value $a$, when it is in fact equal to $\theta$. The estimate $a$ can be chosen to minimize the posterior expected loss, $$E(L(a|y))=  \int L(\theta,a)p(\theta|y)d\theta$$ This optimal choice of $a$ is called a Bayes estimate for the loss function $L$. Show that: (a) If $L(\theta, a) = (\theta − a)^2$ (squared error loss), then the posterior mean, $E(\theta|y)$, if it exists, is the unique Bayes estimate of $\theta$. (b) If $L(\theta, a) = |\theta − a|$, then any posterior median of $\theta$ is a Bayes estimate of $\theta$. (c) If $k_0$ and $k_1$ are non negative numbers, not both zero, and $L(\theta,a)= k_0(\theta−a)$ if $\theta\geq a$,  $k_1(a−\theta)$ if $\theta<a$, then any $k_0$ quantile of the posterior distribution $p(\theta|y)$ is a Bayes estimate of $\theta$.",,"['statistics', 'statistical-inference', 'bayesian']"
45,Probability of record breaking temperature on Feb 29 vs other days,Probability of record breaking temperature on Feb 29 vs other days,,"Is there a higher likelihood of breaking a temperature record (as recorded since NOAA started doing so) on Feb 29th than any other given day, simply because there are fewer sample points to be compared against? The temperature points would still tend to follow the same bell curve, but would the slightly lower resolution (or fewer sample points) increase the odds of a temperature falling outside the limits of existing points?","Is there a higher likelihood of breaking a temperature record (as recorded since NOAA started doing so) on Feb 29th than any other given day, simply because there are fewer sample points to be compared against? The temperature points would still tend to follow the same bell curve, but would the slightly lower resolution (or fewer sample points) increase the odds of a temperature falling outside the limits of existing points?",,"['probability', 'statistics']"
46,Equation for estimation of sample size is a quadratic?,Equation for estimation of sample size is a quadratic?,,"The equation for calculation of sample size for a prevalence study happens to be  $$\ n= \frac {Z p (1-p)}{e^2}$$ where $Z$ is the $Z$ score, $e$ is the precision we want to achieve and $p$ is the 'original truth'. So my doubt is, why is this equation a quadratic? Say the prevalence is zero, then the sample size becomes zero. The same is the case with prevalence being 100%. That means, the estimated sample size is going to be low when the prevalence is either too high or too low. A low estimated sample size for a high prevalence makes sense but what is the rationale behind a low sample size for a low expected prevalence? In short why is this equation a quadratic? Shouldn't it be linear? What's the derivation?","The equation for calculation of sample size for a prevalence study happens to be  $$\ n= \frac {Z p (1-p)}{e^2}$$ where $Z$ is the $Z$ score, $e$ is the precision we want to achieve and $p$ is the 'original truth'. So my doubt is, why is this equation a quadratic? Say the prevalence is zero, then the sample size becomes zero. The same is the case with prevalence being 100%. That means, the estimated sample size is going to be low when the prevalence is either too high or too low. A low estimated sample size for a high prevalence makes sense but what is the rationale behind a low sample size for a low expected prevalence? In short why is this equation a quadratic? Shouldn't it be linear? What's the derivation?",,"['statistics', 'sampling']"
47,Comparative Dice Statistics,Comparative Dice Statistics,,"I am part of a role playing game where we roll dice to set our statistics. Our current system is to roll 4d6, reroll the lowest of the 4, then keep the highest 3 out of the original 3 and the new one you rolled. I'm trying to convince the group that rolling 5d6 and keeping the highest 3 is the exact same thing. Please help me statistically prove this, one way or the other. Preferably this would show the statistical probability of each possible outcome (3-18)","I am part of a role playing game where we roll dice to set our statistics. Our current system is to roll 4d6, reroll the lowest of the 4, then keep the highest 3 out of the original 3 and the new one you rolled. I'm trying to convince the group that rolling 5d6 and keeping the highest 3 is the exact same thing. Please help me statistically prove this, one way or the other. Preferably this would show the statistical probability of each possible outcome (3-18)",,"['statistics', 'dice']"
48,Around De Moivre–Laplace theorem/Poisson law,Around De Moivre–Laplace theorem/Poisson law,,"The task is: Typist printed 1000 pages of text, and made 140 errors. What is the probability that a randomly chosen page contains zero errors? one? two? The error distribution is described with Poisson law. Using Poisson's law, I got that $P(m=0)=0.86,$ while the correct answer is 0.79. I tried to use ML-theorem, because $pnq=140*0.86 >> 20$ and $n >> 50.$ So under my $\exp$ function I get an argument like $-70$ and $e^{-70} \approx 4E-30,$ and so on. But its highly illogical, that the random page would contain mistake about for sure and its far enough from answer. Where am i wrong; What's the real way?","The task is: Typist printed 1000 pages of text, and made 140 errors. What is the probability that a randomly chosen page contains zero errors? one? two? The error distribution is described with Poisson law. Using Poisson's law, I got that $P(m=0)=0.86,$ while the correct answer is 0.79. I tried to use ML-theorem, because $pnq=140*0.86 >> 20$ and $n >> 50.$ So under my $\exp$ function I get an argument like $-70$ and $e^{-70} \approx 4E-30,$ and so on. But its highly illogical, that the random page would contain mistake about for sure and its far enough from answer. Where am i wrong; What's the real way?",,['statistics']
49,Correlated multivariate linear least squares,Correlated multivariate linear least squares,,"Linear Least Squares solves $y = X\beta + \varepsilon$ for $\beta$ when $y, \beta$ are vectors of size $n$. and $\operatorname{Var}[\,\varepsilon \mid X\,] = \sigma^2 I_n$ (spherical errors). I have multivariate measurements, meaning that $y$ is a matrix, but also, the different variates of each measurement are correlated and I have a known covariance matrix for each measurement (also means that each measurement have a different covariance). So, obviously, the spherical errors assumption of ordinary least squares doesn't hold. Is there some extension for least squares that allows to solve such problem? clarification update: In my problem, every row of $y$, $y_i$, is a column vector $\begin{pmatrix}y_{i1},y_{i2},y_{i3}\end{pmatrix}$ which is a vector of random variables, with a known (not diagonal) covariance. That's a 3d location measurement for input time $X_i$. (every row in $X$ is 1,time, and time squared).","Linear Least Squares solves $y = X\beta + \varepsilon$ for $\beta$ when $y, \beta$ are vectors of size $n$. and $\operatorname{Var}[\,\varepsilon \mid X\,] = \sigma^2 I_n$ (spherical errors). I have multivariate measurements, meaning that $y$ is a matrix, but also, the different variates of each measurement are correlated and I have a known covariance matrix for each measurement (also means that each measurement have a different covariance). So, obviously, the spherical errors assumption of ordinary least squares doesn't hold. Is there some extension for least squares that allows to solve such problem? clarification update: In my problem, every row of $y$, $y_i$, is a column vector $\begin{pmatrix}y_{i1},y_{i2},y_{i3}\end{pmatrix}$ which is a vector of random variables, with a known (not diagonal) covariance. That's a 3d location measurement for input time $X_i$. (every row in $X$ is 1,time, and time squared).",,"['statistics', 'optimization', 'least-squares', 'linear-regression']"
50,How to find equation of a curve from points,How to find equation of a curve from points,,"I have got a curved line that I would like to find the equation of using Microsoft Excel. The curve seems to be either a polynomial or some sort of a trig graph. I've done some research and looked at regressions and things like that, but I don't quite understand how to use them properly and the results that actually come out. How would you do non-linear regeression (or use some other Excel tool) and how would you interpret/make sense of the ""summary of results"" (i.e. R² values, confidence range etc)? By the way, I am a high school student, if that helps in identifying the level of explanation required.","I have got a curved line that I would like to find the equation of using Microsoft Excel. The curve seems to be either a polynomial or some sort of a trig graph. I've done some research and looked at regressions and things like that, but I don't quite understand how to use them properly and the results that actually come out. How would you do non-linear regeression (or use some other Excel tool) and how would you interpret/make sense of the ""summary of results"" (i.e. R² values, confidence range etc)? By the way, I am a high school student, if that helps in identifying the level of explanation required.",,"['statistics', 'regression', 'data-analysis', 'regression-analysis']"
51,Likelihood Ratio Test for Exponential Distribution with a Limited Parameter Space,Likelihood Ratio Test for Exponential Distribution with a Limited Parameter Space,,"Suppose that we are given an exponential distribution model with a pdf $f(x,\theta) = \theta^{-1}\exp(-x/\theta)$ with an iid sample $X_1, \ldots, X_n$, and we would like to test hypothesis $H_0 : \theta = 1$ and $H_1 : \theta > 1$. We shall investigate the limiting distribution of the likelihood test statistic $2n(l_n (\hat\theta) - l_n (1))$ where $l_n(\theta)$ is the log-likelihood function $\sum \log f(x,\theta$) and $\hat \theta$ is the MLE of $\theta$ in the parameter space. We shall investigate the asymptotic behavior of the test statistic. Note that the parameter space is not the whole real line, so it does not create an open neighborhood around $H_0: \theta = 1$). Here is what I have discovered so far: we know that the MLE of exponential distribution is $\overline X_n$, the sample mean if we are allowed to take the whole parameter space. Using a similar argument, if the sample mean is greater than one, the same argument works. However, if it is smaller than or equal to one, the MLE is 1 (we can show it using a monotonicity argument). Hence, the test statistic becomes $$\begin{cases} 2n(\ln \overline X_n - (\overline X_n - 1)) && \text{if $\overline X_n > 1$} \\ 0 && \text{if $\overline X_n \leq 1$} \end{cases}$$ we can use then a taylor expansion for the first case to show that it is indeed approximately equal to $n(\overline X_n - 1)^2$ that has a $\chi^2$-distribution asymptotically. However, how can we incorporate the case where $\overline X_n \leq 1$? If $H_1$ were $\theta \neq 1$ it would have been safe and sound. I guess the test becomes $\sum \overline X_n > k$ for some $k$, due to the zero part for the second case?","Suppose that we are given an exponential distribution model with a pdf $f(x,\theta) = \theta^{-1}\exp(-x/\theta)$ with an iid sample $X_1, \ldots, X_n$, and we would like to test hypothesis $H_0 : \theta = 1$ and $H_1 : \theta > 1$. We shall investigate the limiting distribution of the likelihood test statistic $2n(l_n (\hat\theta) - l_n (1))$ where $l_n(\theta)$ is the log-likelihood function $\sum \log f(x,\theta$) and $\hat \theta$ is the MLE of $\theta$ in the parameter space. We shall investigate the asymptotic behavior of the test statistic. Note that the parameter space is not the whole real line, so it does not create an open neighborhood around $H_0: \theta = 1$). Here is what I have discovered so far: we know that the MLE of exponential distribution is $\overline X_n$, the sample mean if we are allowed to take the whole parameter space. Using a similar argument, if the sample mean is greater than one, the same argument works. However, if it is smaller than or equal to one, the MLE is 1 (we can show it using a monotonicity argument). Hence, the test statistic becomes $$\begin{cases} 2n(\ln \overline X_n - (\overline X_n - 1)) && \text{if $\overline X_n > 1$} \\ 0 && \text{if $\overline X_n \leq 1$} \end{cases}$$ we can use then a taylor expansion for the first case to show that it is indeed approximately equal to $n(\overline X_n - 1)^2$ that has a $\chi^2$-distribution asymptotically. However, how can we incorporate the case where $\overline X_n \leq 1$? If $H_1$ were $\theta \neq 1$ it would have been safe and sound. I guess the test becomes $\sum \overline X_n > k$ for some $k$, due to the zero part for the second case?",,"['statistics', 'statistical-inference', 'hypothesis-testing', 'maximum-likelihood']"
52,Combinatorics exam question: number of possible actions when order matters,Combinatorics exam question: number of possible actions when order matters,,"I'm preparing for my exam later this week, and I've come across a question of which I do not understand the answer. The question reads thus: A stock market dealer trades only with shares ""A"". Six times per trading day he performs one of the following three actions: he buys a number of shares ""A"", or he sells a number of shares ""A"", or he does not buy nor sell. Because the actions have effect on the share price ""A"", the sequence of the actions are of interest. How many possibilities are there to have exactly four selling actions on a trading day (and thus two other actions)? I thought the answer would be in the form of $\frac{n!}{(n-k)!}$ or in this case $\frac{6!}{(6-4)!}$ but this appears to be off by a factor 2. I suspect this has something to do with the fact that there are 'two other actions', thus multiplying the total amount of outcomes, though I would not be able to explain it fully. Could someone tell me how to make sense of this?","I'm preparing for my exam later this week, and I've come across a question of which I do not understand the answer. The question reads thus: A stock market dealer trades only with shares ""A"". Six times per trading day he performs one of the following three actions: he buys a number of shares ""A"", or he sells a number of shares ""A"", or he does not buy nor sell. Because the actions have effect on the share price ""A"", the sequence of the actions are of interest. How many possibilities are there to have exactly four selling actions on a trading day (and thus two other actions)? I thought the answer would be in the form of $\frac{n!}{(n-k)!}$ or in this case $\frac{6!}{(6-4)!}$ but this appears to be off by a factor 2. I suspect this has something to do with the fact that there are 'two other actions', thus multiplying the total amount of outcomes, though I would not be able to explain it fully. Could someone tell me how to make sense of this?",,"['combinatorics', 'statistics', 'combinations']"
53,Bayesian Expected loss integral,Bayesian Expected loss integral,,"Thanks. I don't understand how to calculate the integral for a Bayesian Expected Loss. The problem is from Berger 1985 Stat Decision Theory and Bayesian Analysis page 8. Example 1. Assume no data is obtained, so that the believed distribution of $\theta_2$ is simply $\pi(\theta_2)=10 I_{0.1,0.2} (\theta_2) d\theta_2$ . Then $$\rho (\pi^*, a)=E^{\pi^*} L(\theta,a)= \int_{\Theta} L(\theta,a) dF^{\pi^*} (\theta)  $$ $$=\int^a_0 2(a-\theta_2) 10 I_{0.1,0.2} (\theta_2)d\theta_2 +\int^1_a (\theta_2-a) 10 I_{0.1,0.2} (\theta_2)d\theta_2 $$ The resulting is a step function: $$0.15-a, \text{ if } a\le0.1,$$ $$15a^2-4a+0.3, \text{ if } 0.1\le a\le 0.2,$$ $$2a-0.3, \text{ if } a\ge 0.2$$ The part I am confused is how to go from the integral to the step function.  I have done Stieljes-R. Integrals before, but I am not quite sure how to address this. I have tried, unsuccessfully, to solve it, but I am not sure how to proceed. Thank you very much.","Thanks. I don't understand how to calculate the integral for a Bayesian Expected Loss. The problem is from Berger 1985 Stat Decision Theory and Bayesian Analysis page 8. Example 1. Assume no data is obtained, so that the believed distribution of $\theta_2$ is simply $\pi(\theta_2)=10 I_{0.1,0.2} (\theta_2) d\theta_2$ . Then $$\rho (\pi^*, a)=E^{\pi^*} L(\theta,a)= \int_{\Theta} L(\theta,a) dF^{\pi^*} (\theta)  $$ $$=\int^a_0 2(a-\theta_2) 10 I_{0.1,0.2} (\theta_2)d\theta_2 +\int^1_a (\theta_2-a) 10 I_{0.1,0.2} (\theta_2)d\theta_2 $$ The resulting is a step function: $$0.15-a, \text{ if } a\le0.1,$$ $$15a^2-4a+0.3, \text{ if } 0.1\le a\le 0.2,$$ $$2a-0.3, \text{ if } a\ge 0.2$$ The part I am confused is how to go from the integral to the step function.  I have done Stieljes-R. Integrals before, but I am not quite sure how to address this. I have tried, unsuccessfully, to solve it, but I am not sure how to proceed. Thank you very much.",,"['probability', 'statistics', 'bayesian', 'decision-theory']"
54,"Proof that this distribution is N(0,1)?","Proof that this distribution is N(0,1)?",,"I'm having trouble with this question. Let $X_1, \ldots, X_n$ be a random sample from some distribution with mean $\mu_1$ and variance $\sigma_1^2>0$. Let $Y_1, \ldots, Y_n$ be a random sample from some other completely unrelated distribution with mean $\mu_2$ and variance $\sigma_2^2>0$. Prove that as $n \to \infty$,   $$ Z_n  = \frac{(\bar{X}_n - \bar{Y}_n) - (\mu_1-\mu_2)}          {\sqrt{\left(\sigma_1^2 + \sigma_2^2 \right)}/n}  \to \mathcal{N}(0,1). $$ The fact that I don't know the distribution of $X_i$ and $Y_i$ throws me off. Does the CLT always go to $\mathcal{N}(0,1)$?","I'm having trouble with this question. Let $X_1, \ldots, X_n$ be a random sample from some distribution with mean $\mu_1$ and variance $\sigma_1^2>0$. Let $Y_1, \ldots, Y_n$ be a random sample from some other completely unrelated distribution with mean $\mu_2$ and variance $\sigma_2^2>0$. Prove that as $n \to \infty$,   $$ Z_n  = \frac{(\bar{X}_n - \bar{Y}_n) - (\mu_1-\mu_2)}          {\sqrt{\left(\sigma_1^2 + \sigma_2^2 \right)}/n}  \to \mathcal{N}(0,1). $$ The fact that I don't know the distribution of $X_i$ and $Y_i$ throws me off. Does the CLT always go to $\mathcal{N}(0,1)$?",,"['statistics', 'central-limit-theorem']"
55,"Hypotheis Testing (paired T test): 1) Is my work correct? 2) How to graph in SPSS,?","Hypotheis Testing (paired T test): 1) Is my work correct? 2) How to graph in SPSS,?",,"I have the following Hypotheis testing problem. The statement of the exercise is: Experiment: Eleven different varieties of barley were considered. Of   each variety, half was kiln-dried and the other half was left   untreated. Then the two batches of seed were sown in adjacent plots.   The experimenter observed that kiln-dried seeds gave, on average, the   larger yield of kernels and straw but the quality was generally   inferior. The data set “KilnDriedBarley.txt” contains data on the year   of the planting, and the value of the crop (in shillings per acre) for   both kiln-dried and non-kiln-dried seeds. Seeds sown on adjacent plots   are listed in the same row of the file. Year   NonKilnDried    KilnDried  1899       140.5            152 1899       152.5            145 1899       158.5            161 1899       204.5            199.5 1899         162            164 1899         142            139.5 1899         168            155 1900         118            117.5 1900       128.5            121 1900       109.5            116.5 1900       120              120.5 (i) “Does kiln-drying barley seeds increase the average value of the   crop?” Should we use an independent sample or a paired sample test to   answer this question? Defend your choice. My answer: For this experiment we need to use t test paried sample, because it is assumed that each of the eleven types of seed will have the same condition to growth execect the parameter we want to study, in this case is kiln-dried seed and non-klin dried seeds. (ii) Use SPSS to create an appropriate graphical display of your data.   Your graph should show how the kiln-drying affects crop value.   Describe what you can see in the graph. I need help in this part, because I have no idea how to display my data in SPSS showing my data and the test. Part (iii) Conduct a statistical hypothesis test for the question in   (i) at significance level $\alpha = 0.05.$ Include relevant SPSS output.   Formulate a conclusion for the test and cite the appropriate p-value. This is my output: My answer : The P-value is .602. We have not enough evidence to reject $H_0$ in favor of $H_a.$ Therefore there isn’t any significance differences between the average value of the crop with kiln-drying barley seeds at a significance level of ($\alpha=5\%$). Overall question: 1) Are my assumption, and conclusion correct for this experiment?       I notice also that both samples has very similar sample std. therefore I assumed homoscedastic (anyway is computed by SPSS). 2) The second problem I have is that I want to use spss, but I can't find an option to graph the t test. (I belive is that what the question ii asked for). Thank you for your help.","I have the following Hypotheis testing problem. The statement of the exercise is: Experiment: Eleven different varieties of barley were considered. Of   each variety, half was kiln-dried and the other half was left   untreated. Then the two batches of seed were sown in adjacent plots.   The experimenter observed that kiln-dried seeds gave, on average, the   larger yield of kernels and straw but the quality was generally   inferior. The data set “KilnDriedBarley.txt” contains data on the year   of the planting, and the value of the crop (in shillings per acre) for   both kiln-dried and non-kiln-dried seeds. Seeds sown on adjacent plots   are listed in the same row of the file. Year   NonKilnDried    KilnDried  1899       140.5            152 1899       152.5            145 1899       158.5            161 1899       204.5            199.5 1899         162            164 1899         142            139.5 1899         168            155 1900         118            117.5 1900       128.5            121 1900       109.5            116.5 1900       120              120.5 (i) “Does kiln-drying barley seeds increase the average value of the   crop?” Should we use an independent sample or a paired sample test to   answer this question? Defend your choice. My answer: For this experiment we need to use t test paried sample, because it is assumed that each of the eleven types of seed will have the same condition to growth execect the parameter we want to study, in this case is kiln-dried seed and non-klin dried seeds. (ii) Use SPSS to create an appropriate graphical display of your data.   Your graph should show how the kiln-drying affects crop value.   Describe what you can see in the graph. I need help in this part, because I have no idea how to display my data in SPSS showing my data and the test. Part (iii) Conduct a statistical hypothesis test for the question in   (i) at significance level $\alpha = 0.05.$ Include relevant SPSS output.   Formulate a conclusion for the test and cite the appropriate p-value. This is my output: My answer : The P-value is .602. We have not enough evidence to reject $H_0$ in favor of $H_a.$ Therefore there isn’t any significance differences between the average value of the crop with kiln-drying barley seeds at a significance level of ($\alpha=5\%$). Overall question: 1) Are my assumption, and conclusion correct for this experiment?       I notice also that both samples has very similar sample std. therefore I assumed homoscedastic (anyway is computed by SPSS). 2) The second problem I have is that I want to use spss, but I can't find an option to graph the t test. (I belive is that what the question ii asked for). Thank you for your help.",,"['statistics', 'statistical-inference']"
56,Probability problems on Loom,Probability problems on Loom,,"A loom experiences one yarn breakage approximately every $10$ hours. A particular style of cloth is being produced that will take $25$ hours on this loom. If $3$ or more breaks are required to render the product unsatisfactory, find the probability that the style of cloth is finished with acceptable quality.","A loom experiences one yarn breakage approximately every $10$ hours. A particular style of cloth is being produced that will take $25$ hours on this loom. If $3$ or more breaks are required to render the product unsatisfactory, find the probability that the style of cloth is finished with acceptable quality.",,"['probability', 'statistics', 'poisson-distribution', 'binomial-distribution']"
57,Asymptotic Relative Efficiency of Sample Mean and Median,Asymptotic Relative Efficiency of Sample Mean and Median,,"I'm following some online lecture notes on AREs and don't understand where a certain value came from. Consider a distribution function $F$ with a density function $f$ symmetric about $\theta$. We're considering two estimators, $\bar{X}_n$ (sample mean) and $\operatorname{Med} \lbrace X_1, X_2, \ldots, X_n\rbrace$ For large sample size $n$, I know that: $$\bar{X}_n \sim N\left(\theta, \frac{\sigma_F^2}{n}\right) \text{ and } \operatorname{Med} \left\lbrace X_1, X_2, \ldots, X_n\right\rbrace \sim N\left(\theta, \frac{1}{4f(\theta)^2n}\right)$$ Hence it follows that $\operatorname{ARE}(\operatorname{Med}, \bar{X}) = 4f(\theta)^2\sigma_F^2$. Now it says suppose $F \sim N(\theta, \sigma_F^2)$. Then $\operatorname{ARE}( \operatorname{Med}, \bar{X}) = \frac{2}{\pi} = 0.64$ Where is the $\frac{2}{\pi}$ coming from? I certainly believe it to be true, but I don't understand how that comes from the $4f(\theta)^2\sigma_F^2$.","I'm following some online lecture notes on AREs and don't understand where a certain value came from. Consider a distribution function $F$ with a density function $f$ symmetric about $\theta$. We're considering two estimators, $\bar{X}_n$ (sample mean) and $\operatorname{Med} \lbrace X_1, X_2, \ldots, X_n\rbrace$ For large sample size $n$, I know that: $$\bar{X}_n \sim N\left(\theta, \frac{\sigma_F^2}{n}\right) \text{ and } \operatorname{Med} \left\lbrace X_1, X_2, \ldots, X_n\right\rbrace \sim N\left(\theta, \frac{1}{4f(\theta)^2n}\right)$$ Hence it follows that $\operatorname{ARE}(\operatorname{Med}, \bar{X}) = 4f(\theta)^2\sigma_F^2$. Now it says suppose $F \sim N(\theta, \sigma_F^2)$. Then $\operatorname{ARE}( \operatorname{Med}, \bar{X}) = \frac{2}{\pi} = 0.64$ Where is the $\frac{2}{\pi}$ coming from? I certainly believe it to be true, but I don't understand how that comes from the $4f(\theta)^2\sigma_F^2$.",,"['statistics', 'asymptotics']"
58,A white noise process which does not consist of independent random variables?,A white noise process which does not consist of independent random variables?,,"I know the definition of a white noise $\{u_t\}$ (see below), but I cannot see how to find an example of a white noise which is serially dependent, e.g. where $u_t$ depends on the value of $u_{t-1}$. Can someone provide an example of a dependent white noise process? Definition of a white noise process. The stochastic process $\{u_t\}$ is white noise if and only if (1) $E(u_t)=0$ and (2) $E(u_tu_{t+k})=\sigma^2\textbf{1}\{k=0\}$, where $\textbf{1}\{k=0\} =1$ if $k=0$ and $0$ otherwise,  and where $\sigma>0$ is finite.","I know the definition of a white noise $\{u_t\}$ (see below), but I cannot see how to find an example of a white noise which is serially dependent, e.g. where $u_t$ depends on the value of $u_{t-1}$. Can someone provide an example of a dependent white noise process? Definition of a white noise process. The stochastic process $\{u_t\}$ is white noise if and only if (1) $E(u_t)=0$ and (2) $E(u_tu_{t+k})=\sigma^2\textbf{1}\{k=0\}$, where $\textbf{1}\{k=0\} =1$ if $k=0$ and $0$ otherwise,  and where $\sigma>0$ is finite.",,"['statistics', 'random-variables', 'independence']"
59,Binomial Distribution and Proof Relating to Factorials,Binomial Distribution and Proof Relating to Factorials,,I am studying probability and statistics at my university but haven't had a solid math course in awhile(mostly forget algebra dealing with factorials)thus I am stuck with the following proof. According to my book there is a recurrence relation algorithm detailing the following: $P(X=k+1) = c_k * P(X=k)$ with $c_k =[ \frac{(n-k)}{(k+1)} ] * \frac{p}{1-p}$ with X being a binomial random variable. I have the following proof down thus far: P(X=k+1) $=(\binom{N}{K+1}) * p^{k+1} * (1-p)^{n-k-1}=$ As per Binomial definition $= [ \frac{n!}{(k+1)!(n-k-1)!} ] * \frac{p}{1-p} * p^k * (1-p)^{n-k}$ $= [ \frac{n!}{(k+1)!(n-k-1)!} ] * \frac{p}{1-p} * P(X=k)$ And I am left to prove that $\frac{n!}{(k+1)!(n-k-1)!} = \frac{(n-k) }{ (k+1)}$ Any help with steps would be gratefully appreciated or if there is an issue with my proof thus far. Thank You,I am studying probability and statistics at my university but haven't had a solid math course in awhile(mostly forget algebra dealing with factorials)thus I am stuck with the following proof. According to my book there is a recurrence relation algorithm detailing the following: $P(X=k+1) = c_k * P(X=k)$ with $c_k =[ \frac{(n-k)}{(k+1)} ] * \frac{p}{1-p}$ with X being a binomial random variable. I have the following proof down thus far: P(X=k+1) $=(\binom{N}{K+1}) * p^{k+1} * (1-p)^{n-k-1}=$ As per Binomial definition $= [ \frac{n!}{(k+1)!(n-k-1)!} ] * \frac{p}{1-p} * p^k * (1-p)^{n-k}$ $= [ \frac{n!}{(k+1)!(n-k-1)!} ] * \frac{p}{1-p} * P(X=k)$ And I am left to prove that $\frac{n!}{(k+1)!(n-k-1)!} = \frac{(n-k) }{ (k+1)}$ Any help with steps would be gratefully appreciated or if there is an issue with my proof thus far. Thank You,,"['statistics', 'factorial', 'binomial-distribution']"
60,"Calculate $P[A,B,C]$ from $P[A,B]$ and $P[B,C]$",Calculate  from  and,"P[A,B,C] P[A,B] P[B,C]","I have 3 (not independent) events $A, B, C$ and I know everything about how any two of them correlate. For example, I know: $$ P[A], P[B], P[C], P[A,B], P[A,C], P[B,C], P[A|B], P[A|C], P[B|C], P[B|A], ...$$ Is there any way to use this information to calculate a correlation for the three of them, i.e. $$    P[A,B,C] \text{ or } P[A,B|C]  \text{ or } P[A|B,C] $$ A numerical algorithm would also be fine if there isn't an exact formula. If it is not possible, is there a way to get a confidence interval for these values?","I have 3 (not independent) events $A, B, C$ and I know everything about how any two of them correlate. For example, I know: $$ P[A], P[B], P[C], P[A,B], P[A,C], P[B,C], P[A|B], P[A|C], P[B|C], P[B|A], ...$$ Is there any way to use this information to calculate a correlation for the three of them, i.e. $$    P[A,B,C] \text{ or } P[A,B|C]  \text{ or } P[A|B,C] $$ A numerical algorithm would also be fine if there isn't an exact formula. If it is not possible, is there a way to get a confidence interval for these values?",,"['probability', 'statistics', 'correlation', 'confidence-interval']"
61,"Likelihood Function for the Uniform Density. $ (\theta-1,\theta+1)$",Likelihood Function for the Uniform Density.," (\theta-1,\theta+1)","Let the random variables $X_1,X_2,\ldots,X_n$  iid $U[\theta-1\,,\theta+1]$.  So the likelihood function therefore has the form: $$L(\theta\mid X)=\prod_{i=1}^nf(X_i\mid \theta)=\frac{1}{2^n}I(X_1, \ldots , X_n \in [\theta-1\,,\theta+1])\text{ ?}$$ It is correct ?","Let the random variables $X_1,X_2,\ldots,X_n$  iid $U[\theta-1\,,\theta+1]$.  So the likelihood function therefore has the form: $$L(\theta\mid X)=\prod_{i=1}^nf(X_i\mid \theta)=\frac{1}{2^n}I(X_1, \ldots , X_n \in [\theta-1\,,\theta+1])\text{ ?}$$ It is correct ?",,"['statistics', 'random-variables', 'statistical-inference']"
62,Obtaining quadratic equation using Least Squares Method,Obtaining quadratic equation using Least Squares Method,,"This question is most likely extremely trivial, but I'm having some difficulty obtaining the least squares equation from the following data points: {{1.08, 0}, {1.07, 0.0659232}, {0.97, 0.1695168}, {0.77, 0.188352}, {0.84, 0.0847584}} In particular, I'm trying to obtain a quadratic equation using least squares, so I was wondering if someone could show me the method they used to obtain it. I have based my working (and obtained the data above) from the following link: http://www.maths.manchester.ac.uk/~pjohnson/resources/math60082/lecture-monte-carlo-ls.pdf and I have tried to obtain the least squares equation by using the method as given on slide 14. My working is given as follows: Firstly, since I am trying to find a quadratic least squares form, I must solve three equations and attempt to find the coefficients $a_{0}, a_{1}, a_{2}$. Using the equations on slide 14 given in the link above, I then plug in the data points I was given into the equations, which are now given below: (0 + .07*.94176 + .18*.94176 +.20*.94176 + .09*.94176) = 5*$a_{0}$ + $a_{1}$*(1.08 + 1.07 + .97 + .77 + .84)+$a_{2}$*(1.08^2 + 1.07^2 + .97^2 + .77^2 + .84^2) (0*1.08 + .07*.94176*1.07 + .18*.94176*.97 +.20*.94176* .77 + .09*.94176*.84) = $a_{0}$*(1.08 + 1.07 + .97 + .77 + .84)+$a_{1}$*(1.08^2 + 1.07^2 +.97^2 + .77^2 + .84^2)+$a_{2}$*(1.08^3 + 1.07^3 + .97^3 + .77^3 + .84^3) (.07*.94176*1.07^2+.18*.94176*.97^2+.20*.94176*.77^2+.09*.94176*.84^2) = $a_{0}$*(1.08^2 + 1.07^2 + .97^2 + .77^2 + .84^2)+$a_{1}$*(1.08^3 + 1.07^3 + .97^3 + .77^3+.84^3)+$a_{2}$*(1.08^4+1.07^4+.97^4 + .77^4 + .84^4) However, putting this into Wolfram Alpha gives coefficient values -1.13685, 3.12955, and -1.89201, which are incorrect, since the quadratic equation should be -1.81357x^2 + 2.9834x - 1.06998. If it's not too much trouble could someone please show me how to obtain the quadratic equation using least squares, or at least show me what I've done wrong? Thanks in advance","This question is most likely extremely trivial, but I'm having some difficulty obtaining the least squares equation from the following data points: {{1.08, 0}, {1.07, 0.0659232}, {0.97, 0.1695168}, {0.77, 0.188352}, {0.84, 0.0847584}} In particular, I'm trying to obtain a quadratic equation using least squares, so I was wondering if someone could show me the method they used to obtain it. I have based my working (and obtained the data above) from the following link: http://www.maths.manchester.ac.uk/~pjohnson/resources/math60082/lecture-monte-carlo-ls.pdf and I have tried to obtain the least squares equation by using the method as given on slide 14. My working is given as follows: Firstly, since I am trying to find a quadratic least squares form, I must solve three equations and attempt to find the coefficients $a_{0}, a_{1}, a_{2}$. Using the equations on slide 14 given in the link above, I then plug in the data points I was given into the equations, which are now given below: (0 + .07*.94176 + .18*.94176 +.20*.94176 + .09*.94176) = 5*$a_{0}$ + $a_{1}$*(1.08 + 1.07 + .97 + .77 + .84)+$a_{2}$*(1.08^2 + 1.07^2 + .97^2 + .77^2 + .84^2) (0*1.08 + .07*.94176*1.07 + .18*.94176*.97 +.20*.94176* .77 + .09*.94176*.84) = $a_{0}$*(1.08 + 1.07 + .97 + .77 + .84)+$a_{1}$*(1.08^2 + 1.07^2 +.97^2 + .77^2 + .84^2)+$a_{2}$*(1.08^3 + 1.07^3 + .97^3 + .77^3 + .84^3) (.07*.94176*1.07^2+.18*.94176*.97^2+.20*.94176*.77^2+.09*.94176*.84^2) = $a_{0}$*(1.08^2 + 1.07^2 + .97^2 + .77^2 + .84^2)+$a_{1}$*(1.08^3 + 1.07^3 + .97^3 + .77^3+.84^3)+$a_{2}$*(1.08^4+1.07^4+.97^4 + .77^4 + .84^4) However, putting this into Wolfram Alpha gives coefficient values -1.13685, 3.12955, and -1.89201, which are incorrect, since the quadratic equation should be -1.81357x^2 + 2.9834x - 1.06998. If it's not too much trouble could someone please show me how to obtain the quadratic equation using least squares, or at least show me what I've done wrong? Thanks in advance",,"['probability', 'statistics', 'data-analysis', 'least-squares']"
63,"Calculate the probability, that a man repair 20 machines in 8 hours. It is correct my work?","Calculate the probability, that a man repair 20 machines in 8 hours. It is correct my work?",,"The problem statement said: The servicing of a machine requires two separate steps, with the time   needed for the 1st step being an exponential random variable with mean   10 minutes and the time for the second step being an independent   exponential random variable with mean 15 minutes. If a repairperson   has 20 machines to service, compute the aproximate the probability   that all the work can be completed in 8 hours. My work: *) Let Xi be the time to service the ith machine, then E[Xi]=E[X1]+E[X2]=10+15=25min (Thanks Andre!)   V[Xi]=V[X1]+V[X2]=100+225=325 **) Therefore, Xi is modeled by a normal r.v. with parameters E[X]=20*E[Xi]=500  V[X]=20*V[Xi]=6500  then std=80.622 ***) Last Part Now, I know that X is the sum of 20 independent and identict distributives r.v. x1, x2 ….x20 where each xi is the time for the repair of the ith machine. P(X<8*60min)=P(z<(480-500)/80.622)=P(z<-0.248071)=  2.74855E-10 Question: Is may procedure correct? Looks very small probability, it is right? Thanks so much for your help.","The problem statement said: The servicing of a machine requires two separate steps, with the time   needed for the 1st step being an exponential random variable with mean   10 minutes and the time for the second step being an independent   exponential random variable with mean 15 minutes. If a repairperson   has 20 machines to service, compute the aproximate the probability   that all the work can be completed in 8 hours. My work: *) Let Xi be the time to service the ith machine, then E[Xi]=E[X1]+E[X2]=10+15=25min (Thanks Andre!)   V[Xi]=V[X1]+V[X2]=100+225=325 **) Therefore, Xi is modeled by a normal r.v. with parameters E[X]=20*E[Xi]=500  V[X]=20*V[Xi]=6500  then std=80.622 ***) Last Part Now, I know that X is the sum of 20 independent and identict distributives r.v. x1, x2 ….x20 where each xi is the time for the repair of the ith machine. P(X<8*60min)=P(z<(480-500)/80.622)=P(z<-0.248071)=  2.74855E-10 Question: Is may procedure correct? Looks very small probability, it is right? Thanks so much for your help.",,"['probability', 'probability-theory', 'statistics']"
64,"If $P(ABC)=0.2$, are $A$ and $C$ mutually exclusive?","If , are  and  mutually exclusive?",P(ABC)=0.2 A C,"I was finishing up my statistics homework But I was unsure if I was thinking of the last problem correctly. It reads If $P(ABC)=0.2$, are $A$ and $C$ mutually exclusive? My thinking is that mutually exclusive means $A\cap C = \emptyset$ And if that's true $P(ABC)$ would equal $0$, so they are not mutually exclusive. Am I thinking about this correctly? If not could someone please help me with how to do this question? Thank you.","I was finishing up my statistics homework But I was unsure if I was thinking of the last problem correctly. It reads If $P(ABC)=0.2$, are $A$ and $C$ mutually exclusive? My thinking is that mutually exclusive means $A\cap C = \emptyset$ And if that's true $P(ABC)$ would equal $0$, so they are not mutually exclusive. Am I thinking about this correctly? If not could someone please help me with how to do this question? Thank you.",,"['statistics', 'discrete-mathematics']"
65,Estimating Number of Scratch Tickets Remaining,Estimating Number of Scratch Tickets Remaining,,"So, in my statistics class we discussed the recent huge jackpot, but we decided to focus on something more ""trackable""—scratch tickets. A part of the assignment is to figure out how many tickets are remaining given the following data: chances of winning , number of prize brackets , number of prizes in each bracket , and number of prizes remaining in each bracket . I've made the following example in order to figure out an approach to solve this: 1 in 3 chance of winning , 3 prize brackets (e.g. $50, $25, $5) , 1, 3, 7, prizes in each bracket (respectively) and 1, 2, 5, prizes remaining (respectively) . Obviously, we know for a fact that at least 3 tickets were sold (3-2)+(7-5) = 3 , meaning that there are 30 tickets left. However, this isn't the best conclusion for how many tickets are remaining given that the chances of winning are 1 in 3. I'm having trouble figuring out how the 1 in 3, would factor in to show how possibly more than 3 tickets were sold given the number of prizes already claimed. Any suggestions?","So, in my statistics class we discussed the recent huge jackpot, but we decided to focus on something more ""trackable""—scratch tickets. A part of the assignment is to figure out how many tickets are remaining given the following data: chances of winning , number of prize brackets , number of prizes in each bracket , and number of prizes remaining in each bracket . I've made the following example in order to figure out an approach to solve this: 1 in 3 chance of winning , 3 prize brackets (e.g. $50, $25, $5) , 1, 3, 7, prizes in each bracket (respectively) and 1, 2, 5, prizes remaining (respectively) . Obviously, we know for a fact that at least 3 tickets were sold (3-2)+(7-5) = 3 , meaning that there are 30 tickets left. However, this isn't the best conclusion for how many tickets are remaining given that the chances of winning are 1 in 3. I'm having trouble figuring out how the 1 in 3, would factor in to show how possibly more than 3 tickets were sold given the number of prizes already claimed. Any suggestions?",,"['probability', 'statistics', 'education']"
66,Calculating the standard deviation involving a moment generating function,Calculating the standard deviation involving a moment generating function,,"An actuary determines that the claim size for a certain class of accidents is a random variable, X, with moment generation function: $$M_X(t)=\frac{1}{(1-2500t)^4}.$$ Calculate the standard deviation of the claim size for this class of accidents. (Note: the shortcut for this question is seeing that it's a gamma distribution function with parameters 4 and 2500. Therefore, Var(X) will be $4\cdot 2500^2 = 25,000,000$ and the standard deviation will then be $\sqrt{25,000,000} = 5,000$. However, let's assume we cannot see this.) From I understand, the SD can be found by doing this calculation: $$SD(X)=\sqrt{M''(0)-[M'(0)]^2}$$ The reason to that is because $SD(X)=\sqrt{Var(X)}=\sqrt{E(X^2)-[E(X)]^2}$ and $E(X^n) = M^n(0)$. ($M^n(0)$ is the derivative of $M(X)$ to the $n$-th time at $0$) Why is $E(X^n) = M^n(0)$ true?","An actuary determines that the claim size for a certain class of accidents is a random variable, X, with moment generation function: $$M_X(t)=\frac{1}{(1-2500t)^4}.$$ Calculate the standard deviation of the claim size for this class of accidents. (Note: the shortcut for this question is seeing that it's a gamma distribution function with parameters 4 and 2500. Therefore, Var(X) will be $4\cdot 2500^2 = 25,000,000$ and the standard deviation will then be $\sqrt{25,000,000} = 5,000$. However, let's assume we cannot see this.) From I understand, the SD can be found by doing this calculation: $$SD(X)=\sqrt{M''(0)-[M'(0)]^2}$$ The reason to that is because $SD(X)=\sqrt{Var(X)}=\sqrt{E(X^2)-[E(X)]^2}$ and $E(X^n) = M^n(0)$. ($M^n(0)$ is the derivative of $M(X)$ to the $n$-th time at $0$) Why is $E(X^n) = M^n(0)$ true?",,"['probability', 'statistics', 'standard-deviation', 'actuarial-science', 'variance']"
67,Arriving time of a Normal Variable. What is the correct procedure?,Arriving time of a Normal Variable. What is the correct procedure?,,"I'm stuck in how to salve this problem: The length of time X (in minutes) it takes to go from your home to donwtown is normally distributed with μ = 30 minutes and  X = 5 minutes. What is the latest time that you should leave home if you want to be over 99% sure of arriving in time for a job interview taking place in downtown at 2pm? I was thinking on using that the 99% is 3 standard deviation from the mean, but I can not reach the correct answer 1.18pm. Question: I need to know the procedure to salve this kind of exercises. Thanks!","I'm stuck in how to salve this problem: The length of time X (in minutes) it takes to go from your home to donwtown is normally distributed with μ = 30 minutes and  X = 5 minutes. What is the latest time that you should leave home if you want to be over 99% sure of arriving in time for a job interview taking place in downtown at 2pm? I was thinking on using that the 99% is 3 standard deviation from the mean, but I can not reach the correct answer 1.18pm. Question: I need to know the procedure to salve this kind of exercises. Thanks!",,"['probability', 'statistics']"
68,How do I characterize the distribution of the expected number of periods before the first success in a binomial distribution?,How do I characterize the distribution of the expected number of periods before the first success in a binomial distribution?,,"I came across this statement in something at work: An exponential distribution of the length of up time would result from a model in which the probability of failure (down time) is constant through time. I interpreted this as a binomial distribution where 0 = up and 1 = down , where the probability of a $1$ occurring is $0 < p < 1$. I realize this notation is a little backward, since we don't usually think of ""up"" as equating with ""failure"". Now I'm trying to characterize the distribution of the length of time that a device is ""up"". Let $X$ be the discrete probability distribution of the length of the first up time. Then \begin{align} Pr(x = 0) &= p \\ Pr(x = 1) &= (1 - p) p \\ Pr(x = 2) &= (1 - p)^2 p \\ \ldots \\ Pr(x = n) &= (1 - p)^{n-1} p \\ \end{align} Taking the expectation: \begin{align} \mathbb{E}[X] &= \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n-1} p \\ &= p \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n-1} \\ &= \cfrac{p}{1-p} \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n} \\ \end{align} Since \begin{align} \displaystyle \lim_{n \to \infty} \left\lvert \cfrac{a_{n+1}}{a_n} \right\rvert &= \displaystyle \lim_{n \to \infty} \left\lvert \cfrac{(n+1)(1-p)^{n+1}}{n(1-p)^n} \right\rvert \\ &= (1 - p)\displaystyle \lim_{n \to \infty} \left\lvert \cfrac{(n+1)}{n} \right\rvert \\ &= (1 - p)\displaystyle \lim_{n \to \infty} \left\lvert 1 + \cfrac{1}{n} \right\rvert \\ &= 1 - p \\ &< 1 \end{align} I know the series converges by the Ratio Test. My problem is how to evaluate the infinite series $\displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n}$, given that I know it converges, although I'm also unsure if I interpreted the initial statement correctly. EDIT: In response to one of the comments about calculating the sum, is this a correct way to do so? Let $S = \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n$. Then \begin{align} S - (1 - p)S &= \left( \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n \right) - \left( (1 - p) \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n \right) \\ pS &= (1 - p) + (1 - p)^2 + (1 - p)^3 + \cdots \\ pS &= \cfrac{1}{1 - (1 - p)} \\ \Rightarrow S &= \cfrac{1}{p^2} \end{align}","I came across this statement in something at work: An exponential distribution of the length of up time would result from a model in which the probability of failure (down time) is constant through time. I interpreted this as a binomial distribution where 0 = up and 1 = down , where the probability of a $1$ occurring is $0 < p < 1$. I realize this notation is a little backward, since we don't usually think of ""up"" as equating with ""failure"". Now I'm trying to characterize the distribution of the length of time that a device is ""up"". Let $X$ be the discrete probability distribution of the length of the first up time. Then \begin{align} Pr(x = 0) &= p \\ Pr(x = 1) &= (1 - p) p \\ Pr(x = 2) &= (1 - p)^2 p \\ \ldots \\ Pr(x = n) &= (1 - p)^{n-1} p \\ \end{align} Taking the expectation: \begin{align} \mathbb{E}[X] &= \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n-1} p \\ &= p \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n-1} \\ &= \cfrac{p}{1-p} \displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n} \\ \end{align} Since \begin{align} \displaystyle \lim_{n \to \infty} \left\lvert \cfrac{a_{n+1}}{a_n} \right\rvert &= \displaystyle \lim_{n \to \infty} \left\lvert \cfrac{(n+1)(1-p)^{n+1}}{n(1-p)^n} \right\rvert \\ &= (1 - p)\displaystyle \lim_{n \to \infty} \left\lvert \cfrac{(n+1)}{n} \right\rvert \\ &= (1 - p)\displaystyle \lim_{n \to \infty} \left\lvert 1 + \cfrac{1}{n} \right\rvert \\ &= 1 - p \\ &< 1 \end{align} I know the series converges by the Ratio Test. My problem is how to evaluate the infinite series $\displaystyle\sum_{n=0}^{\infty} n (1 - p)^{n}$, given that I know it converges, although I'm also unsure if I interpreted the initial statement correctly. EDIT: In response to one of the comments about calculating the sum, is this a correct way to do so? Let $S = \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n$. Then \begin{align} S - (1 - p)S &= \left( \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n \right) - \left( (1 - p) \displaystyle \sum_{n=0}^{\infty} n(1 - p)^n \right) \\ pS &= (1 - p) + (1 - p)^2 + (1 - p)^3 + \cdots \\ pS &= \cfrac{1}{1 - (1 - p)} \\ \Rightarrow S &= \cfrac{1}{p^2} \end{align}",,"['sequences-and-series', 'statistics', 'convergence-divergence']"
69,Probability Mass Functions,Probability Mass Functions,,"I have a question regarding probability mass functions. I have been learning about joint p.m.fs and how to find the marginal p.m.fs from the joint p.m.fs, however I'm completely unsure how to tackle this problem below. Any help being pointed in the right direction would be greatly appreciated. Given the table below and the fact that $X_1$ and $X_2$ are independent random variables, how would I go about finding P($X_1$+$X_2$$\le$$1$ | $X_1$=$0$)? $$\begin{array}{c|c|c|}   & \text{P($X_i$=0)} & \text{P($X_i$=1)}& \text{P($X_i$=2)} & \text{$i$=1,2}\\ \hline \text{$X_1$} & p & 3p & 1-4p &(0<p<1/4) \\ \hline \text{$X_2$} & p & p^2 & 1-p-p^2 & (0<p<1/2)\\ \hline \end{array}$$","I have a question regarding probability mass functions. I have been learning about joint p.m.fs and how to find the marginal p.m.fs from the joint p.m.fs, however I'm completely unsure how to tackle this problem below. Any help being pointed in the right direction would be greatly appreciated. Given the table below and the fact that $X_1$ and $X_2$ are independent random variables, how would I go about finding P($X_1$+$X_2$$\le$$1$ | $X_1$=$0$)? $$\begin{array}{c|c|c|}   & \text{P($X_i$=0)} & \text{P($X_i$=1)}& \text{P($X_i$=2)} & \text{$i$=1,2}\\ \hline \text{$X_1$} & p & 3p & 1-4p &(0<p<1/4) \\ \hline \text{$X_2$} & p & p^2 & 1-p-p^2 & (0<p<1/2)\\ \hline \end{array}$$",,"['probability', 'statistics']"
70,Confidence interval of the parameter of $\exp$ and normal distribution from MLE?,Confidence interval of the parameter of  and normal distribution from MLE?,\exp,"I have a sample $X_1,X_2,\ldots,X_n$ If the sample is from exponential distribution, I want to use MLE to estimate the parameter $\beta$. I know the result that $$\hat{\beta}=\frac{X_1+X_2+\ldots+X_n}{n}$$But how can I calculate the $95\%$ confidence interval of $\beta$? If the sample is from normal distribution, I want to use MLE to estimate the parameter $\mu$ and $\sigma$. I also know the result that $$\hat{\mu}=\frac{X_1+X_2+\ldots+X_n}{n}, \quad \hat{\sigma}=\left[\frac{n-1}{n}\cdot S^2(n)\right]^{1/2}$$ But how can I calculate the $95\%$ confidence interval of $\mu$ and $\sigma$? Step1 to calculate confidence interval ,  $$\hat{θ}\pm z_{1-\frac\alpha2}\sqrt{\frac{\delta(\hat{θ})}{n}}$$ Step2 to calculate confidence interval $$\delta(\hat{θ})=-n\left(E\left[\frac{d^2}{dθ^2}\ln \mathcal L(θ)\right]\right)^{-1}$$ I know the equation, but I am still confuzed how to calculate $$\left(E\left[\frac{d^2}{dθ^2}\ln \mathcal L(θ)\right]\right)$$ I want to know the equation in the red frame of this picture.","I have a sample $X_1,X_2,\ldots,X_n$ If the sample is from exponential distribution, I want to use MLE to estimate the parameter $\beta$. I know the result that $$\hat{\beta}=\frac{X_1+X_2+\ldots+X_n}{n}$$But how can I calculate the $95\%$ confidence interval of $\beta$? If the sample is from normal distribution, I want to use MLE to estimate the parameter $\mu$ and $\sigma$. I also know the result that $$\hat{\mu}=\frac{X_1+X_2+\ldots+X_n}{n}, \quad \hat{\sigma}=\left[\frac{n-1}{n}\cdot S^2(n)\right]^{1/2}$$ But how can I calculate the $95\%$ confidence interval of $\mu$ and $\sigma$? Step1 to calculate confidence interval ,  $$\hat{θ}\pm z_{1-\frac\alpha2}\sqrt{\frac{\delta(\hat{θ})}{n}}$$ Step2 to calculate confidence interval $$\delta(\hat{θ})=-n\left(E\left[\frac{d^2}{dθ^2}\ln \mathcal L(θ)\right]\right)^{-1}$$ I know the equation, but I am still confuzed how to calculate $$\left(E\left[\frac{d^2}{dθ^2}\ln \mathcal L(θ)\right]\right)$$ I want to know the equation in the red frame of this picture.",,"['statistics', 'statistical-inference', 'parameter-estimation', 'confidence-interval']"
71,Using the Central Limit Theorem to work out the approximate distribution of $\frac{1}{n}\sum_{i=1}^nX_i^2$,Using the Central Limit Theorem to work out the approximate distribution of,\frac{1}{n}\sum_{i=1}^nX_i^2,"Suppose $X_1,X_2,\ldots,X_n$ are I.I.D $N(0,1)$. Then, what is the approximate distribution of $\frac{1}{n}\sum_{i=1}^n X_i^2\,$? I have the solution but none of it is making any sense to me. Thank you for your help!","Suppose $X_1,X_2,\ldots,X_n$ are I.I.D $N(0,1)$. Then, what is the approximate distribution of $\frac{1}{n}\sum_{i=1}^n X_i^2\,$? I have the solution but none of it is making any sense to me. Thank you for your help!",,['statistics']
72,Confidence interval in comparing service times,Confidence interval in comparing service times,,"A store manager wishes to compare the service times of the express checkout with the service times of the self-serve checkout.  Suppose that independent random samples of 121 customers at express and selfserve checkouts were taken, and the service times for each customer was recorded.  The mean and standard deviation of the sample of customers using the express checkout were 3.7 and 0.9 minutes, respectively.  For the self-serve customers, the mean and standard deviation were 4.2 and 1.7 minutes, respectively. a) Calculate the ratio of the maximum sample variance to the minimum sample variance.  Does it appear that the population variances are equal or unequal?  Explain. b) Construct the appropriate 95% confidence interval for the difference in the mean service times for customers using the express and self-serve checkouts, and interpret your result. My work: For a) I'm not really sure how to get the answer of 3.57 , I tried $0.9^2+1.7^2,$ which equals 3.7 , but that is wrong. For b) I tried $3.7-4.2±1.96*\sqrt{(0.81/121)+(2.89/121)}$, to which I get the confidence interval of $(-0.84,-0.16)$ The answer says it should be $-0.5 ± 0.81,$ which results in $(-1.31,0.31).$ Any help is very much appreciated!!","A store manager wishes to compare the service times of the express checkout with the service times of the self-serve checkout.  Suppose that independent random samples of 121 customers at express and selfserve checkouts were taken, and the service times for each customer was recorded.  The mean and standard deviation of the sample of customers using the express checkout were 3.7 and 0.9 minutes, respectively.  For the self-serve customers, the mean and standard deviation were 4.2 and 1.7 minutes, respectively. a) Calculate the ratio of the maximum sample variance to the minimum sample variance.  Does it appear that the population variances are equal or unequal?  Explain. b) Construct the appropriate 95% confidence interval for the difference in the mean service times for customers using the express and self-serve checkouts, and interpret your result. My work: For a) I'm not really sure how to get the answer of 3.57 , I tried $0.9^2+1.7^2,$ which equals 3.7 , but that is wrong. For b) I tried $3.7-4.2±1.96*\sqrt{(0.81/121)+(2.89/121)}$, to which I get the confidence interval of $(-0.84,-0.16)$ The answer says it should be $-0.5 ± 0.81,$ which results in $(-1.31,0.31).$ Any help is very much appreciated!!",,"['probability', 'statistics', 'confidence-interval']"
73,"Probability - analyzing ""randomness"" of data","Probability - analyzing ""randomness"" of data",,"Forgive me, I am a probability novice and am looking for a little guidance. My question is based on real-world data. I have obscured it a bit for confidentiality reasons but the spirit of the question is the same. Okay, here is the setup: Suppose that 100 students from 37 different schools have applied to take part in a math camp. We are told that 188 students will be chosen randomly from the 3700 total applications. Suppose the following number of students are selected from each school: 7 8 1 5 11 3 6 15 3 7 43 1 1 2 1 23 4 3 5 5 6 2 16 1 1 2 (Schools 27-37 have 0 students selected) Now, I am suspicious about the large number of students chosen from School 11 (43 students), so I wish to analyze this data to determine the likelihood that the applications were randomly selected. Mathematically I believe this equates to determining whether or not the data follows a normal distribution. My attempt at the solution is the following. Since there are 37 different schools, and each school submitted the same number of applications, I would expect 188/37 ~ 5 students chosen from each school (i.e. this is the mean of my random variable). I would like to determine a range such that - if the students were randomly selected - ""there is a 99% probability that the number of students chosen from each school would be between x and y"" (so that I can see whether 43 falls into this range). However I am unsure what to use for the standard deviation. Thanks in advance for your help.","Forgive me, I am a probability novice and am looking for a little guidance. My question is based on real-world data. I have obscured it a bit for confidentiality reasons but the spirit of the question is the same. Okay, here is the setup: Suppose that 100 students from 37 different schools have applied to take part in a math camp. We are told that 188 students will be chosen randomly from the 3700 total applications. Suppose the following number of students are selected from each school: 7 8 1 5 11 3 6 15 3 7 43 1 1 2 1 23 4 3 5 5 6 2 16 1 1 2 (Schools 27-37 have 0 students selected) Now, I am suspicious about the large number of students chosen from School 11 (43 students), so I wish to analyze this data to determine the likelihood that the applications were randomly selected. Mathematically I believe this equates to determining whether or not the data follows a normal distribution. My attempt at the solution is the following. Since there are 37 different schools, and each school submitted the same number of applications, I would expect 188/37 ~ 5 students chosen from each school (i.e. this is the mean of my random variable). I would like to determine a range such that - if the students were randomly selected - ""there is a 99% probability that the number of students chosen from each school would be between x and y"" (so that I can see whether 43 falls into this range). However I am unsure what to use for the standard deviation. Thanks in advance for your help.",,"['probability', 'statistics']"
74,Unbiased estimator of $\int_0^t \mu (s) ds$,Unbiased estimator of,\int_0^t \mu (s) ds,"Let $\mu,\alpha_n:\mathbb R^+\to \mathbb R$ continuous function with $\mu$ bounded function. Let $N^{(n)}$ the trajectory of a Poisson process with intensity $(\alpha_n \mu)(t)$. Let $0=T_0^{(n)}<T_1^{(n)}<..$ jumps of $N^{(n)}$. Let $M_n(t)=\sum_{i=1}^{N_t^{(n)}} \frac {1} {\alpha_n (T_i^{(n)})}$ Show $M_n(t)$ is an unbiased estimator of $M(t)=\int_0^t \mu (s) ds$. My idea: I calculate $E(M_n(t)|N^n=u)$ and I find: $E(M_n(t)|N^n=u)=u \frac {1} {\int_0^t (\alpha_n \mu)(s)ds} M(t)$ $E(E(M_n(t)|N^n=u))=E(M_n(t))$. But $E(M_n(t))-M(t)=u \frac {1} {\int_0^t (\alpha_n \mu)(s)ds} M(t)-M(t)$ is not equal to 0. Can you help me?","Let $\mu,\alpha_n:\mathbb R^+\to \mathbb R$ continuous function with $\mu$ bounded function. Let $N^{(n)}$ the trajectory of a Poisson process with intensity $(\alpha_n \mu)(t)$. Let $0=T_0^{(n)}<T_1^{(n)}<..$ jumps of $N^{(n)}$. Let $M_n(t)=\sum_{i=1}^{N_t^{(n)}} \frac {1} {\alpha_n (T_i^{(n)})}$ Show $M_n(t)$ is an unbiased estimator of $M(t)=\int_0^t \mu (s) ds$. My idea: I calculate $E(M_n(t)|N^n=u)$ and I find: $E(M_n(t)|N^n=u)=u \frac {1} {\int_0^t (\alpha_n \mu)(s)ds} M(t)$ $E(E(M_n(t)|N^n=u))=E(M_n(t))$. But $E(M_n(t))-M(t)=u \frac {1} {\int_0^t (\alpha_n \mu)(s)ds} M(t)-M(t)$ is not equal to 0. Can you help me?",,['statistics']
75,Stats: Confidence Interval and Upper Limit,Stats: Confidence Interval and Upper Limit,,"A random sample of n = 18 E-glass fibre test specimens of a certain type yielded a sample average interfacial heard yield of 40 and a standard deviation of 4. Assume the interfacial shear yield stress is normally distributed. Compute the upper limit of a 95% confidence interval for the true mean stress. Attempt: $\bar x = 40, n = 18, s=4$. Upper limit : $$ \bar x + t_{\alpha/2,df=17}\frac{4}{\sqrt{18}}  = 40 + 2.110 \times \frac{4}{\sqrt{18}} \approx 41.98932708. $$ However, the book uses $2.262$ for the $t_{\alpha/2}$ value, which seems to have df of $9$ rather than $17$ that I've used. Did I make a mistake? Table found @ : http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf","A random sample of n = 18 E-glass fibre test specimens of a certain type yielded a sample average interfacial heard yield of 40 and a standard deviation of 4. Assume the interfacial shear yield stress is normally distributed. Compute the upper limit of a 95% confidence interval for the true mean stress. Attempt: $\bar x = 40, n = 18, s=4$. Upper limit : $$ \bar x + t_{\alpha/2,df=17}\frac{4}{\sqrt{18}}  = 40 + 2.110 \times \frac{4}{\sqrt{18}} \approx 41.98932708. $$ However, the book uses $2.262$ for the $t_{\alpha/2}$ value, which seems to have df of $9$ rather than $17$ that I've used. Did I make a mistake? Table found @ : http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf",,"['statistics', 'random-variables', 'normal-distribution']"
76,Hidden Markov Models and Viterbi Algorithm: Fair and Biased Die,Hidden Markov Models and Viterbi Algorithm: Fair and Biased Die,,"So following is the problem that I am trying to solve using Viterbi algorithm and HMM: Before attempting to write a program, I want to do this problem by hand for the first 3 observations($651$). Based on the question, I understand that: $P(i | Fair) = \frac1{6} , 1\leq i \leq 6 $ $P( 6 | Biased ) = \frac5{10} = \frac1{2} $ $P( i | Biased ) = \frac1{10} , 1\leq i \leq 5 $ and the transition matrix is $Fair$ $Biased$ $\begin{bmatrix}     0.95 & 0.05  \\     0.1 & 0.9  \\ \end{bmatrix}$ but where should I go from here ? EDIT: Assume that initially either fair or biased is equally likely($\frac1{2}$) from a ""fictitious"" state $O$. I managed to compute the first 4 highest probabilities:  $1 , 0.25 , \frac9{400}, \frac{81}{40000}$ corresponding to $O , B,  B,  B$ Is this right ?","So following is the problem that I am trying to solve using Viterbi algorithm and HMM: Before attempting to write a program, I want to do this problem by hand for the first 3 observations($651$). Based on the question, I understand that: $P(i | Fair) = \frac1{6} , 1\leq i \leq 6 $ $P( 6 | Biased ) = \frac5{10} = \frac1{2} $ $P( i | Biased ) = \frac1{10} , 1\leq i \leq 5 $ and the transition matrix is $Fair$ $Biased$ $\begin{bmatrix}     0.95 & 0.05  \\     0.1 & 0.9  \\ \end{bmatrix}$ but where should I go from here ? EDIT: Assume that initially either fair or biased is equally likely($\frac1{2}$) from a ""fictitious"" state $O$. I managed to compute the first 4 highest probabilities:  $1 , 0.25 , \frac9{400}, \frac{81}{40000}$ corresponding to $O , B,  B,  B$ Is this right ?",,"['probability', 'statistics', 'stochastic-processes', 'markov-chains', 'bayesian-network']"
77,Expectation and variance without replacement,Expectation and variance without replacement,,"Let $X_{N_1},\cdots,X_{N_n}$ is a sample without replacement from the set $\{1,2,\cdots,N\}$, and let $\bar X_n=\sum_{i=1}^n X_{N_{i}}/n$. Then, how one can find $E(\bar X_n)$, $Var(\bar X_n)$, $\max_{1\le i\le N} (X_i-\bar X_n)^2$, and $\sum_{i=1}^N(X_i-\bar X_n)^2$?","Let $X_{N_1},\cdots,X_{N_n}$ is a sample without replacement from the set $\{1,2,\cdots,N\}$, and let $\bar X_n=\sum_{i=1}^n X_{N_{i}}/n$. Then, how one can find $E(\bar X_n)$, $Var(\bar X_n)$, $\max_{1\le i\le N} (X_i-\bar X_n)^2$, and $\sum_{i=1}^N(X_i-\bar X_n)^2$?",,"['probability', 'statistics', 'sampling', 'variance']"
78,Minimum of an Order Statistic with probability,Minimum of an Order Statistic with probability,,"Let $X_1,\ldots,X_n$ constitute a random sample of size $n$ from a normal distribution with $\mu = 0$ and var= 2. Find the smallest value of n such that $P(\min(X^2_1,\ldots,X^2_n)\leq .002) \geq .8$ Essentially we want the smallest value of n that would make the min of an order statistic less than .002 with 80% certainty.","Let $X_1,\ldots,X_n$ constitute a random sample of size $n$ from a normal distribution with $\mu = 0$ and var= 2. Find the smallest value of n such that $P(\min(X^2_1,\ldots,X^2_n)\leq .002) \geq .8$ Essentially we want the smallest value of n that would make the min of an order statistic less than .002 with 80% certainty.",,"['probability', 'statistics', 'order-statistics']"
79,Comparing a probability to Chebyshev's Theorem.,Comparing a probability to Chebyshev's Theorem.,,"I am a little confused with this questions I was assigned. For the experiment, flip a coin until heads shows, assume that the probability on heads on one flip is $$\frac{3/4}$$ . We define a RV X = the number of flips. The probability distribution function for X I calculated was p(X=x) = 3/4(1/4)^x-1. 1) What is the expected number of flips? My result: 4/3 2) What is the standard deviation of the number of flips until heads shows?  My result: 2/3 3) Compute the probability P(4/3-2/3 < x < 4/3+2/3) and compare to what Chebyshev's Theorem says it should be. My result (for first half): .33438","I am a little confused with this questions I was assigned. For the experiment, flip a coin until heads shows, assume that the probability on heads on one flip is $$\frac{3/4}$$ . We define a RV X = the number of flips. The probability distribution function for X I calculated was p(X=x) = 3/4(1/4)^x-1. 1) What is the expected number of flips? My result: 4/3 2) What is the standard deviation of the number of flips until heads shows?  My result: 2/3 3) Compute the probability P(4/3-2/3 < x < 4/3+2/3) and compare to what Chebyshev's Theorem says it should be. My result (for first half): .33438",,"['probability', 'statistics', 'random-variables']"
80,How to calculate if taking a toll road is worth it,How to calculate if taking a toll road is worth it,,"I'm trying to figure out if there's a way to calculate if it's worth taking a toll road if it will save me time. For example, if the toll only costs 50 cents and my trip will save me 30 minutes, I will take the toll.  On the other hand, if the toll is $5 but it will only save me 5 minutes, I will not take the toll. If feel as if there's a mathematical way to calculate this decision, based on: p = price of the toll t = time saved (estimated) v = value of my time Sample inputs: p = 2 dollars t = 15 minutes v = $30/hr Based on this example, is it worth it to take the toll?  And how to make this decision for any inputs?  Perhaps different inputs are needed?  I'm not sure how to set up an equation for this.","I'm trying to figure out if there's a way to calculate if it's worth taking a toll road if it will save me time. For example, if the toll only costs 50 cents and my trip will save me 30 minutes, I will take the toll.  On the other hand, if the toll is $5 but it will only save me 5 minutes, I will not take the toll. If feel as if there's a mathematical way to calculate this decision, based on: p = price of the toll t = time saved (estimated) v = value of my time Sample inputs: p = 2 dollars t = 15 minutes v = $30/hr Based on this example, is it worth it to take the toll?  And how to make this decision for any inputs?  Perhaps different inputs are needed?  I'm not sure how to set up an equation for this.",,['statistics']
81,Different colored dice placed in urns.,Different colored dice placed in urns.,,"You roll $6$ die, $2$ red dice, $2$ blue dice and $2$ green dice. Any  number rolled between $1-2$ is placed in Urn 1, any number rolled between $3-4$ goes into Urn $2$, and any number rolled between 5-6 goes into Urn $3$. a) What is the probability that each 2 of the same colored die end up in the same urn (ie. you cant have the 2 red and the 2 blue dice in the same urn)? b) What is the probability that exactly 2 die end up in each urn (color doesn't matter for this part)? For part a), I just thought of it like this... first die of any color can be placed anywhere and the die of the same color has a 1/3 chance of being placed with it. The second color die can be placed in 2 of the remaining urns (2/3) and the other same colored die needs to be placed in that urn as well (1/3), the final colored die need to be placed in the last remaining urn so (1/3) and the second die of that color needs to be placed in that urn as well (1/3). So i got $(3/3)*(1/3)*(2/3)*(1/3)*(1/3)*(1/3)=6/729$ For part b, i got the probability that exactly two die end up in each urn to be 1/3 since the die color doesnt matter here. Do these answer make sense? If they are incorrect where did i go wrong?","You roll $6$ die, $2$ red dice, $2$ blue dice and $2$ green dice. Any  number rolled between $1-2$ is placed in Urn 1, any number rolled between $3-4$ goes into Urn $2$, and any number rolled between 5-6 goes into Urn $3$. a) What is the probability that each 2 of the same colored die end up in the same urn (ie. you cant have the 2 red and the 2 blue dice in the same urn)? b) What is the probability that exactly 2 die end up in each urn (color doesn't matter for this part)? For part a), I just thought of it like this... first die of any color can be placed anywhere and the die of the same color has a 1/3 chance of being placed with it. The second color die can be placed in 2 of the remaining urns (2/3) and the other same colored die needs to be placed in that urn as well (1/3), the final colored die need to be placed in the last remaining urn so (1/3) and the second die of that color needs to be placed in that urn as well (1/3). So i got $(3/3)*(1/3)*(2/3)*(1/3)*(1/3)*(1/3)=6/729$ For part b, i got the probability that exactly two die end up in each urn to be 1/3 since the die color doesnt matter here. Do these answer make sense? If they are incorrect where did i go wrong?",,"['probability', 'probability-theory', 'statistics']"
82,The maximum-likelihood estimators of $\sigma^2$,The maximum-likelihood estimators of,\sigma^2,"A sample of size $n$ is drawn from each of four normal populations, all of which have the same variance $\sigma^2$ . The means of the four populations are $a+b+c$ , $a+b-c$ , $a-b+c$ and $a-b-c$ . What are the maximum-likelihood estimators of $c$ , $b$ , $a$ and $\sigma^2$ . $a$ , $b$ , $c$ solved. But for $\sigma^2$ . Is this answer correct $\sigma^2 = \frac{1}{4}(s^2_1+s^2_2 + s^2_3 + s^2_4)$ .","A sample of size is drawn from each of four normal populations, all of which have the same variance . The means of the four populations are , , and . What are the maximum-likelihood estimators of , , and . , , solved. But for . Is this answer correct .",n \sigma^2 a+b+c a+b-c a-b+c a-b-c c b a \sigma^2 a b c \sigma^2 \sigma^2 = \frac{1}{4}(s^2_1+s^2_2 + s^2_3 + s^2_4),"['probability', 'statistics', 'statistical-inference', 'estimation']"
83,"""Set of observations"" is an identity map - why such a framework?","""Set of observations"" is an identity map - why such a framework?",,"First of all, I do not ask this on CrossValidated since I'm speaking here of mathematical statistics, which explicitly belongs to mathematics. I am currently working on rank tests in order to write an introduction on this theory. My main reference work is Theory of Rank Tests (Hájek, Šidák and Sen, 1999, 2nd edition) They begin, naturally, by defining the terms and the general framework they will use, as follows: 2.1.1 Probability space and observations From a probabilistic point of view, a random experiment is represented by a probability space $(\Omega,\mathcal{A},P)$, where $\Omega$ is the space of all possible outcomes of the experiment, $\mathcal{A}$ is a $\sigma$-field determining how finely the outcomes are distinguished, and $P(\cdot)$ is a probability measure. In this setup, observations are defined as $\mathcal{A}$-measurable functions. Until here, I understand perfectly. It is the classical framework. From a statistical point of view, an experiment is described by an indexed set of observations $X=(X_{1},X_{2},\dots,X_{N})$, each observation $X_{i}$ taking its values $x_{i}$ on the real line or on a proper subset of the real line. The set of observations is determined by its (joint) distribution function   $$\begin{align}F(x_{1},\dots,x_{N})&=P(X_{1}\le x_{1},\dots,X_{N}\le x_{N}) \tag{1}\\ &-\infty<x_{1},\dots,x_{N}<+\infty \end{align}$$ Here, I understand that $P$ in $(1)$ is the induced probability measure on $(E^N,\mathcal{E}_{N})$ by $\hat{P}$ where $X_{i}:(\Omega,\mathcal{A},\hat{P})\to(E,\mathcal{E})$ is a $(\mathcal{A},\mathcal{E})$-measurable function for all $i=1,\dots,N$ ($\mathcal{E}$ and $\mathcal{E}_{N}$ are appropriate $\sigma$-fields). Is that correct? The book continues with: 2.1.2 Statistics, $\sigma$-fields and $\lambda$-fields A set of observations $X$ together with a distribution function $F(x)$ generates a probability space $(\mathcal{X},\mathcal{A},P)$ as follows: $\mathcal{X}$ is the space of all possible values $x=(x_{1},\dots,x_{N})$ of $X$, i.e. it is an N-dimensional Euclidean space; $\mathcal{A}$ is the $\sigma$-field of Borel subsets of $\mathcal{X}$, i.e. the smallest $\sigma$-field containing all events appearing on the right side of $(1)$; finally, $P(\cdot)$ is the probability measure uniquely determined by $(1)$. The set of observations may be considered as an identity map $X(x)=x,\,x\in\mathcal{X}$. [...] As a rule, the probability distribution $P(\cdot)$ will be determined by a density $p$,   $$P(A)=\int_{A}p(x)\text{d}x\tag{$A\in\mathcal{A}$}$$ This is where I do not fully understand what they mean. They consider $X$ as being defined as follows: $$X:(\mathcal{X},\mathcal{A},P)\to(\mathcal{X},\mathcal{A},P):x\mapsto X(x)=x$$ And then, while usually $P(X\in A)$ was a notation for $P(X^{-1}(A))$, we have now $P(X^{-1}(A))=P(A)$ since $X$ is the identity. But what motivates such a framework? I mean, why would someone consider $X$ as an identity map. I must admit that I am more used to statistics viewed from a probabilistic point of view (actually, it does not make sense to me to speak about ""probalistic point of view"" and ""statistical point of view""). What does motivate someone to consider the ""observation set"" (whose components are usually considered as a random variables, as far as I know) as an identity map? Why would we need to consider it that way? Why can't we stay in a general ""probabilistic point of view"" with $X_{i}$ being general random variables? I understand it can be a very basic question, but I am quite confused with such a choice in the framework.","First of all, I do not ask this on CrossValidated since I'm speaking here of mathematical statistics, which explicitly belongs to mathematics. I am currently working on rank tests in order to write an introduction on this theory. My main reference work is Theory of Rank Tests (Hájek, Šidák and Sen, 1999, 2nd edition) They begin, naturally, by defining the terms and the general framework they will use, as follows: 2.1.1 Probability space and observations From a probabilistic point of view, a random experiment is represented by a probability space $(\Omega,\mathcal{A},P)$, where $\Omega$ is the space of all possible outcomes of the experiment, $\mathcal{A}$ is a $\sigma$-field determining how finely the outcomes are distinguished, and $P(\cdot)$ is a probability measure. In this setup, observations are defined as $\mathcal{A}$-measurable functions. Until here, I understand perfectly. It is the classical framework. From a statistical point of view, an experiment is described by an indexed set of observations $X=(X_{1},X_{2},\dots,X_{N})$, each observation $X_{i}$ taking its values $x_{i}$ on the real line or on a proper subset of the real line. The set of observations is determined by its (joint) distribution function   $$\begin{align}F(x_{1},\dots,x_{N})&=P(X_{1}\le x_{1},\dots,X_{N}\le x_{N}) \tag{1}\\ &-\infty<x_{1},\dots,x_{N}<+\infty \end{align}$$ Here, I understand that $P$ in $(1)$ is the induced probability measure on $(E^N,\mathcal{E}_{N})$ by $\hat{P}$ where $X_{i}:(\Omega,\mathcal{A},\hat{P})\to(E,\mathcal{E})$ is a $(\mathcal{A},\mathcal{E})$-measurable function for all $i=1,\dots,N$ ($\mathcal{E}$ and $\mathcal{E}_{N}$ are appropriate $\sigma$-fields). Is that correct? The book continues with: 2.1.2 Statistics, $\sigma$-fields and $\lambda$-fields A set of observations $X$ together with a distribution function $F(x)$ generates a probability space $(\mathcal{X},\mathcal{A},P)$ as follows: $\mathcal{X}$ is the space of all possible values $x=(x_{1},\dots,x_{N})$ of $X$, i.e. it is an N-dimensional Euclidean space; $\mathcal{A}$ is the $\sigma$-field of Borel subsets of $\mathcal{X}$, i.e. the smallest $\sigma$-field containing all events appearing on the right side of $(1)$; finally, $P(\cdot)$ is the probability measure uniquely determined by $(1)$. The set of observations may be considered as an identity map $X(x)=x,\,x\in\mathcal{X}$. [...] As a rule, the probability distribution $P(\cdot)$ will be determined by a density $p$,   $$P(A)=\int_{A}p(x)\text{d}x\tag{$A\in\mathcal{A}$}$$ This is where I do not fully understand what they mean. They consider $X$ as being defined as follows: $$X:(\mathcal{X},\mathcal{A},P)\to(\mathcal{X},\mathcal{A},P):x\mapsto X(x)=x$$ And then, while usually $P(X\in A)$ was a notation for $P(X^{-1}(A))$, we have now $P(X^{-1}(A))=P(A)$ since $X$ is the identity. But what motivates such a framework? I mean, why would someone consider $X$ as an identity map. I must admit that I am more used to statistics viewed from a probabilistic point of view (actually, it does not make sense to me to speak about ""probalistic point of view"" and ""statistical point of view""). What does motivate someone to consider the ""observation set"" (whose components are usually considered as a random variables, as far as I know) as an identity map? Why would we need to consider it that way? Why can't we stay in a general ""probabilistic point of view"" with $X_{i}$ being general random variables? I understand it can be a very basic question, but I am quite confused with such a choice in the framework.",,"['statistics', 'probability-distributions', 'order-statistics']"
84,Explain how the following expression was derived?,Explain how the following expression was derived?,,"Can someone explain how the author gets to the expression after the words ""This leads to:""","Can someone explain how the author gets to the expression after the words ""This leads to:""",,"['calculus', 'algebra-precalculus', 'statistics', 'bayesian']"
85,Probability of earnings from lottery,Probability of earnings from lottery,,"Question: A city's lottery works in the following way: An individual selects 6 numbers from the first 30 numbers. The city then selects 6 numbers from the first 30 numbers. If the individual selects the same 6 numbers as the city selected, then they win the lottery. A lottery ticket costs 1 dollar, and the lottery winner receives $500,000 if he or she wins. 800,000 people are expected to play the lottery. What is the probability that the city loses money on the lottery? Attempt: I know that the city loses money if 2 or more individuals win. The probability of someone selecting the correct six numbers is $\frac{{6 \choose 6}}{{30 \choose 6}}$ Not sure how to proceed after this.","Question: A city's lottery works in the following way: An individual selects 6 numbers from the first 30 numbers. The city then selects 6 numbers from the first 30 numbers. If the individual selects the same 6 numbers as the city selected, then they win the lottery. A lottery ticket costs 1 dollar, and the lottery winner receives $500,000 if he or she wins. 800,000 people are expected to play the lottery. What is the probability that the city loses money on the lottery? Attempt: I know that the city loses money if 2 or more individuals win. The probability of someone selecting the correct six numbers is $\frac{{6 \choose 6}}{{30 \choose 6}}$ Not sure how to proceed after this.",,"['probability', 'statistics', 'actuarial-science']"
86,Writing power series for $AR(2)$ model polynomials,Writing power series for  model polynomials,AR(2),"So I have found the following problem in my textbook without solutions, which presents the $AR(2)$ process defined by  $$X_{t} = 0.5X_{t-1} + 0.25 X_{t-2} + Z_{t}$$ I am asked what the polynomial $\phi(z)$ for this model is, and further to write the polynomial as a product of linear terms in $z$ and show how to expand the inverse of each of these linear terms as a power series in $z$, while showing why these power series converge for small enough $z$. The first part of the problem I found easy, and I found the polynomial is  $$ \phi(B) = 1 - 0.5B - 0.25 B^{2}$$ with roots $r_{1}, r_{2} = \dfrac{0.5 \pm \sqrt{1.25}}{-0.5}$. $\implies$ since the roots lie outside the unit circle, there exists a solution in the causal form $\sum \limits_{j=0}^{\infty} \theta_{j} Z_{t-j}$. But I do not know what is meant by ""expanding the inverse of each linear term as a power series"", or how I should go about doing that for this problem. Further, I don't really see the purpose?","So I have found the following problem in my textbook without solutions, which presents the $AR(2)$ process defined by  $$X_{t} = 0.5X_{t-1} + 0.25 X_{t-2} + Z_{t}$$ I am asked what the polynomial $\phi(z)$ for this model is, and further to write the polynomial as a product of linear terms in $z$ and show how to expand the inverse of each of these linear terms as a power series in $z$, while showing why these power series converge for small enough $z$. The first part of the problem I found easy, and I found the polynomial is  $$ \phi(B) = 1 - 0.5B - 0.25 B^{2}$$ with roots $r_{1}, r_{2} = \dfrac{0.5 \pm \sqrt{1.25}}{-0.5}$. $\implies$ since the roots lie outside the unit circle, there exists a solution in the causal form $\sum \limits_{j=0}^{\infty} \theta_{j} Z_{t-j}$. But I do not know what is meant by ""expanding the inverse of each linear term as a power series"", or how I should go about doing that for this problem. Further, I don't really see the purpose?",,"['statistics', 'power-series', 'stationary-processes']"
87,"Defined $X,Y,Z,W$ find $P(W=0)$ and $P(W=X)$",Defined  find  and,"X,Y,Z,W P(W=0) P(W=X)","Suppose $X,Y,Z$ are iid binary random variables satisfying $P(X=0)=P(Y=0)=P(Z=0)=0.5$ and define a new random variable $W$ as $W=X$ if $Z=0$ and $W=Y$ if $Z=1$ . Then find $P(W=0)$ and $P(W=X)$ . I just wanted the community to see that my solution is correct. This came in an exam in my university that my friends are giving various answers. $P(W=0)=P(W=0,Z=0)+P(W=0,Z=1)=P(W=0|Z=0)P(Z=0)+P(W=0|Z=1)P(Z=1)=P(X=0)P(Z=0)+P(Y=0)P(Z=1)=0.5^2+0.5^2=0.5$ $P(W=X)=P(W=X,Z=0)+P(W=X,Z=1)=P(W=X|Z=0)P(Z=0)+P(W=Y|Z=1)P(Z=1)=P(Z=0)+P(W=X=Y)P(Z=1)=P(Z=0)+P(X=Y)P(Z=1)=0.5+0.5\times0.5=0.75$",Suppose are iid binary random variables satisfying and define a new random variable as if and if . Then find and . I just wanted the community to see that my solution is correct. This came in an exam in my university that my friends are giving various answers.,"X,Y,Z P(X=0)=P(Y=0)=P(Z=0)=0.5 W W=X Z=0 W=Y Z=1 P(W=0) P(W=X) P(W=0)=P(W=0,Z=0)+P(W=0,Z=1)=P(W=0|Z=0)P(Z=0)+P(W=0|Z=1)P(Z=1)=P(X=0)P(Z=0)+P(Y=0)P(Z=1)=0.5^2+0.5^2=0.5 P(W=X)=P(W=X,Z=0)+P(W=X,Z=1)=P(W=X|Z=0)P(Z=0)+P(W=Y|Z=1)P(Z=1)=P(Z=0)+P(W=X=Y)P(Z=1)=P(Z=0)+P(X=Y)P(Z=1)=0.5+0.5\times0.5=0.75","['probability', 'statistics']"
88,Find the error (cumulative function of abs.cont. random variable),Find the error (cumulative function of abs.cont. random variable),,"Let $X$ be an abs. cont. random variable. Then, for $k \in \mathbb{R}$, $prob(X \leqslant k \leqslant \alpha X)=prob (X\leqslant k)-prob(X\leqslant \frac{k}{\alpha})$. I cannot understeand why it doesn't hold $prob(X \leqslant k \leqslant \alpha X)=prob(X\geqslant \frac{k}{\alpha})-prob (X\leqslant k)=1-prob(X\leqslant \frac{k}{\alpha})-prob (X\leqslant k)$. Thansk. KB","Let $X$ be an abs. cont. random variable. Then, for $k \in \mathbb{R}$, $prob(X \leqslant k \leqslant \alpha X)=prob (X\leqslant k)-prob(X\leqslant \frac{k}{\alpha})$. I cannot understeand why it doesn't hold $prob(X \leqslant k \leqslant \alpha X)=prob(X\geqslant \frac{k}{\alpha})-prob (X\leqslant k)=1-prob(X\leqslant \frac{k}{\alpha})-prob (X\leqslant k)$. Thansk. KB",,"['statistics', 'random-variables']"
89,Determining distributions/relations between two random variables,Determining distributions/relations between two random variables,,"""A manufacturer of a certain product has a label weight for this product of 20.4 grams. The weight of the products are given by $X \sim N(21.37,0.16)$. Suppose 15 of these products are selected randomly and weighed. Let Y be the number of these products that weigh less than $20.857$ grams. Find $P(Y \leq 2)$"" My first thought was that Y must be binomialy distributed, each element having the option of being either over or under the weight $20.857$. The number of trials is $n = 15$, the probability  $p = P(X \leq 20.857)$. Thus we seek to determine binomialcdf(n,p,k) = binomialcdf(15,p,2). To determine $p$ we calculate Normalcdf(a,b,$\mu$,$\sigma$) = Normalcdf(0,20.857,21.37,0.4) $\approx 0.099833$. Thus we get binomialcdf(15,p,2) $\approx 0.81657$. The answer in the textbook is $0.8159$, frustratingly close. Is my entire line of reasoning wrong, or did I miss something in my calculations? (Note that in my actual caculations, I did not 'round' anything as I went along)","""A manufacturer of a certain product has a label weight for this product of 20.4 grams. The weight of the products are given by $X \sim N(21.37,0.16)$. Suppose 15 of these products are selected randomly and weighed. Let Y be the number of these products that weigh less than $20.857$ grams. Find $P(Y \leq 2)$"" My first thought was that Y must be binomialy distributed, each element having the option of being either over or under the weight $20.857$. The number of trials is $n = 15$, the probability  $p = P(X \leq 20.857)$. Thus we seek to determine binomialcdf(n,p,k) = binomialcdf(15,p,2). To determine $p$ we calculate Normalcdf(a,b,$\mu$,$\sigma$) = Normalcdf(0,20.857,21.37,0.4) $\approx 0.099833$. Thus we get binomialcdf(15,p,2) $\approx 0.81657$. The answer in the textbook is $0.8159$, frustratingly close. Is my entire line of reasoning wrong, or did I miss something in my calculations? (Note that in my actual caculations, I did not 'round' anything as I went along)",,"['probability', 'statistics']"
90,Standard deviation for measurements with errors - least squares?,Standard deviation for measurements with errors - least squares?,,"I have been bugged by a simple problem in statistics recently. Let's assume that I have made a set of measurements of a certain quantity, each with an uncertainty estimate. I have a set of tuples $\{x_i,s_i\}$ ($x$ is a value, $s$ its uncertainty). The goal is to calculate the most likely true value $y$ and its uncertainty $S$. My solution: I assume that each measurement $x_i$ follows a Gaussian distribution with a standard deviation $s_i$, meaning that the probability that $y$ is the true value provided $x_i$ is $$ P(y|x_i,s_i) \propto \exp \left ( -\frac{(x_i-y)^2}{2s_i^2} \right ) \,. $$ Therefore the total probability (the measurements are independent) that $y$ is the true value should be $$ P(y|\{x_i,s_i\}) \propto \exp \left ( -\sum_i \frac{(x_i-y)^2}{2s_i^2} \right ) \,. $$ Searching for the most probable $y$, I just set $\partial P / \partial y = 0$. I then get $$ \sum_i \frac{y}{s_i^2} = \sum_i \frac{x_i}{s_i^2} \, . $$ Therefore the most probable value should just be an average measurements, using the squares of reciprocals of their uncertainties as weights. So far so good. I would then like to know that the standard deviation (its estimate) in $y$ is. I therefore look at $P(y+a|\{x_i,s_i\})/P(y|\{x_i,s_i\})$ (for some $a$). What I get is $$ \frac{P(y+a)}{P(y)} = \exp \left ( - \sum_i \frac{a^2}{2s_i^2} \right ) \, , $$ therefore the standard deviation should be $$ \frac{1}{S^2} = \sum_i \frac{1}{s_i^2} \, . $$ That all makes sense. Provided that all the errors are the same, then $S = s/\sqrt{N}$. However, if I set all errors to 0 ($s_i = 0$), then the $S$ is also be 0. But there is still some uncertainty due to the  spread of values $\{x_i\}$. My question is: How should I approach the problem in order to incorporate both the spread of $\{x_i\}$ and their uncertainties $\{s_i\}$ in the total uncertainty $S$? Thanks a lot. SSF","I have been bugged by a simple problem in statistics recently. Let's assume that I have made a set of measurements of a certain quantity, each with an uncertainty estimate. I have a set of tuples $\{x_i,s_i\}$ ($x$ is a value, $s$ its uncertainty). The goal is to calculate the most likely true value $y$ and its uncertainty $S$. My solution: I assume that each measurement $x_i$ follows a Gaussian distribution with a standard deviation $s_i$, meaning that the probability that $y$ is the true value provided $x_i$ is $$ P(y|x_i,s_i) \propto \exp \left ( -\frac{(x_i-y)^2}{2s_i^2} \right ) \,. $$ Therefore the total probability (the measurements are independent) that $y$ is the true value should be $$ P(y|\{x_i,s_i\}) \propto \exp \left ( -\sum_i \frac{(x_i-y)^2}{2s_i^2} \right ) \,. $$ Searching for the most probable $y$, I just set $\partial P / \partial y = 0$. I then get $$ \sum_i \frac{y}{s_i^2} = \sum_i \frac{x_i}{s_i^2} \, . $$ Therefore the most probable value should just be an average measurements, using the squares of reciprocals of their uncertainties as weights. So far so good. I would then like to know that the standard deviation (its estimate) in $y$ is. I therefore look at $P(y+a|\{x_i,s_i\})/P(y|\{x_i,s_i\})$ (for some $a$). What I get is $$ \frac{P(y+a)}{P(y)} = \exp \left ( - \sum_i \frac{a^2}{2s_i^2} \right ) \, , $$ therefore the standard deviation should be $$ \frac{1}{S^2} = \sum_i \frac{1}{s_i^2} \, . $$ That all makes sense. Provided that all the errors are the same, then $S = s/\sqrt{N}$. However, if I set all errors to 0 ($s_i = 0$), then the $S$ is also be 0. But there is still some uncertainty due to the  spread of values $\{x_i\}$. My question is: How should I approach the problem in order to incorporate both the spread of $\{x_i\}$ and their uncertainties $\{s_i\}$ in the total uncertainty $S$? Thanks a lot. SSF",,"['statistics', 'standard-deviation', 'least-squares']"
91,"Please Explain a simple Formula, calculating time in-between a call queue","Please Explain a simple Formula, calculating time in-between a call queue",,"I have a simple algebra formula, proven to work. But I need help in understanding why it works. The Scenario: I work at a call center, and am trying to calculate the time free in-between calls. I have the 3 variables, provided by Live data: Staff Available (not on calls) Staff Busy (on calls) Average call length of 5 minutes So once I end a call, I go to the back of the line of available staff before I get the next call. This is the formula, tested to work: : (Staff Available / Staff Busy) * 5 minutes call length = Time in-between calls Example: 100 staff. 80 busy, 20 available. [20/80 * 5 = 1.25 minutes] Example: 100 staff. 50 busy, 50 available. [50/50 * 5 = 5 minutes] (Which is expected, as we are double staffed.) Example: 100 staff. 20 busy, 80 available. [80/20 * 5 = 20 minutes] Question - Why does this equation work? I must be taking shortcuts. Why do we divide Available/Busy instead of Available/Total? I'd greatly appreciate any explanation. Thank you very much. -Brennan","I have a simple algebra formula, proven to work. But I need help in understanding why it works. The Scenario: I work at a call center, and am trying to calculate the time free in-between calls. I have the 3 variables, provided by Live data: Staff Available (not on calls) Staff Busy (on calls) Average call length of 5 minutes So once I end a call, I go to the back of the line of available staff before I get the next call. This is the formula, tested to work: : (Staff Available / Staff Busy) * 5 minutes call length = Time in-between calls Example: 100 staff. 80 busy, 20 available. [20/80 * 5 = 1.25 minutes] Example: 100 staff. 50 busy, 50 available. [50/50 * 5 = 5 minutes] (Which is expected, as we are double staffed.) Example: 100 staff. 20 busy, 80 available. [80/20 * 5 = 20 minutes] Question - Why does this equation work? I must be taking shortcuts. Why do we divide Available/Busy instead of Available/Total? I'd greatly appreciate any explanation. Thank you very much. -Brennan",,['statistics']
92,I dont understand the following notation- could it be explained?,I dont understand the following notation- could it be explained?,,"$f_n$:D$\rightarrow$$\mathbb{R}$ Can someone just explain what the above notation means in literal words. The way i see it is that a function $f_n$ has a domain D which the function sends to a range of the real numbers? Could someone also define the following: $\mathbb{P}$:$\mathscr{B}$$\rightarrow$[0,1] $A\rightarrow$$\mathbb{P}$(A) To me this means that the probability that a function in the Borel set fit between 0 to 1 is equal to the probability of A going to P(A)?","$f_n$:D$\rightarrow$$\mathbb{R}$ Can someone just explain what the above notation means in literal words. The way i see it is that a function $f_n$ has a domain D which the function sends to a range of the real numbers? Could someone also define the following: $\mathbb{P}$:$\mathscr{B}$$\rightarrow$[0,1] $A\rightarrow$$\mathbb{P}$(A) To me this means that the probability that a function in the Borel set fit between 0 to 1 is equal to the probability of A going to P(A)?",,"['real-analysis', 'statistics', 'notation']"
93,How do I write the square of covariance?,How do I write the square of covariance?,,"Should it be $\mathrm{cov}^{2}(X,Y)$ or should it be $\mathrm{cov}(X,Y)^2$ or $(\mathrm{cov}(X,Y))^2$ or something completely different? Thank you.","Should it be $\mathrm{cov}^{2}(X,Y)$ or should it be $\mathrm{cov}(X,Y)^2$ or $(\mathrm{cov}(X,Y))^2$ or something completely different? Thank you.",,"['statistics', 'notation', 'covariance']"
94,"Exponential distribution, how to apply in this task?","Exponential distribution, how to apply in this task?",,"We have a restaurant, where glasses brake every $6$ month with exponential distribution. What is the probability, that From $5$ glasses, at most $3$ will break in $12$ month? From $500$ glasses, at most $300$ will break in $12$ month? I understand the task, but I have no idea how to applay the definition of exponential distribution. We have $\lambda = 1/6$ month? What is $X$? Any help or hint appreciated. Or we have $E(X) = 1/6$ if $X=1$?","We have a restaurant, where glasses brake every $6$ month with exponential distribution. What is the probability, that From $5$ glasses, at most $3$ will break in $12$ month? From $500$ glasses, at most $300$ will break in $12$ month? I understand the task, but I have no idea how to applay the definition of exponential distribution. We have $\lambda = 1/6$ month? What is $X$? Any help or hint appreciated. Or we have $E(X) = 1/6$ if $X=1$?",,"['probability', 'statistics']"
95,Statistical method to prove that variations are not important?,Statistical method to prove that variations are not important?,,"I'm checking the effect a specific substance has in the elongation of the root of variousplants of the Solanum genus. I had my plants grow in soil with different concentration of hormones. My results are quite obscure from what I expected, so I'm guessing that at very low concentrations the substance doesn't have any effect. However, I did noticed a small difference in length from  those samples that grew up without the substance. Is there any statistical method to prove that the variations I observe are not important?","I'm checking the effect a specific substance has in the elongation of the root of variousplants of the Solanum genus. I had my plants grow in soil with different concentration of hormones. My results are quite obscure from what I expected, so I'm guessing that at very low concentrations the substance doesn't have any effect. However, I did noticed a small difference in length from  those samples that grew up without the substance. Is there any statistical method to prove that the variations I observe are not important?",,['statistics']
96,Likelihood of a correct diagnosis of a disease,Likelihood of a correct diagnosis of a disease,,"A certain cancer is found in one person in 5000. If a person does have the disease, in 92% of the cases the diagnostic procedure will show that he or she actually has it. If a person does not have the disease, the diagnostic procedure in one out of 500 cases gives a false positive result. Determine the probability that a person with a positive test has the cancer. Very lost on how to go about formulating an equation for this problem. I know it has something to do with conditional probabilties but I'm unsure where to begin. My best guess would be $((1-0.92)*(1/500))/(1/5000)=0.8$ but I am unsure if this is even close to right. Any advice would be appreciated.","A certain cancer is found in one person in 5000. If a person does have the disease, in 92% of the cases the diagnostic procedure will show that he or she actually has it. If a person does not have the disease, the diagnostic procedure in one out of 500 cases gives a false positive result. Determine the probability that a person with a positive test has the cancer. Very lost on how to go about formulating an equation for this problem. I know it has something to do with conditional probabilties but I'm unsure where to begin. My best guess would be $((1-0.92)*(1/500))/(1/5000)=0.8$ but I am unsure if this is even close to right. Any advice would be appreciated.",,"['probability', 'statistics']"
97,How to calculate the expected value of a discrete random variable?,How to calculate the expected value of a discrete random variable?,,"A private investor has capital of £16,000. He divides this into eight   units of £2,000, each of which he invests in a separate one-year   investment. Each of these investments has three possible outcomes at   the end of the year: total loss of capital probability 0.1 capital payment of £2,000 probability 0.7 capital payment of £5,000 probability 0.2 The investments behave independently of one another, and there is no     other return from them. Calculate the expected payment received by the investor at the end of     the year. I am unable to figure out how to create a probability distribution from the given data. If I could create it,then I think I can calculate it just applying the formula.","A private investor has capital of £16,000. He divides this into eight   units of £2,000, each of which he invests in a separate one-year   investment. Each of these investments has three possible outcomes at   the end of the year: total loss of capital probability 0.1 capital payment of £2,000 probability 0.7 capital payment of £5,000 probability 0.2 The investments behave independently of one another, and there is no     other return from them. Calculate the expected payment received by the investor at the end of     the year. I am unable to figure out how to create a probability distribution from the given data. If I could create it,then I think I can calculate it just applying the formula.",,"['probability', 'statistics', 'random-variables', 'expectation']"
98,Pearson Correlation Coefficient Formula Understanding,Pearson Correlation Coefficient Formula Understanding,,"As part of a program I'm writing I need to use the Pearson Correlation Coefficient. However I'm getting incorrect results. Here is the formula I'm using: $$ \rho_{X, Y} = \frac{\frac{1}{n}\sum_{i = 1}^{i = n}(x_i - \mu_X)(y_i - \mu_Y)}{\sigma_X\sigma_Y} $$ Specifically, I'm not sure if I'm getting the $\frac{1}{n}$ part correct in the numerator. Does this mean I need to multiply the whole sum by $\frac{1}{n}$ or is it just some notation for the sum. Any help would be appreciated.","As part of a program I'm writing I need to use the Pearson Correlation Coefficient. However I'm getting incorrect results. Here is the formula I'm using: $$ \rho_{X, Y} = \frac{\frac{1}{n}\sum_{i = 1}^{i = n}(x_i - \mu_X)(y_i - \mu_Y)}{\sigma_X\sigma_Y} $$ Specifically, I'm not sure if I'm getting the $\frac{1}{n}$ part correct in the numerator. Does this mean I need to multiply the whole sum by $\frac{1}{n}$ or is it just some notation for the sum. Any help would be appreciated.",,"['statistics', 'summation']"
99,Linear regression and standardization,Linear regression and standardization,,"I am trying to use a linear regression to model an expected value Y for an input X. X and Y have a large difference between them, so I was converting to standard (z) score, doing my calculation (finding where Y would be when X is at a certain level) and converting back to the raw score for Y. However I am getting numbers that make me think I'm doing something wrong. Is this a valid way to predict what a ""raw score"" would be, or should I be doing this another way? It's been many years since I've taken statistics.","I am trying to use a linear regression to model an expected value Y for an input X. X and Y have a large difference between them, so I was converting to standard (z) score, doing my calculation (finding where Y would be when X is at a certain level) and converting back to the raw score for Y. However I am getting numbers that make me think I'm doing something wrong. Is this a valid way to predict what a ""raw score"" would be, or should I be doing this another way? It's been many years since I've taken statistics.",,"['statistics', 'regression']"
