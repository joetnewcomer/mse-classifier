,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Questions About Four Definitions of The Upper and Lower Limits of A Sequence,Questions About Four Definitions of The Upper and Lower Limits of A Sequence,,"Related questions have been posted here and here . Background I have seen the following four definitions of the upper and lower limits of a sequence from textbooks and MSE posts: Definition 1 $\quad$ [ Baby Rudin ] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ is the supreme of the set of numbers $x$ in the extended real number system such that $s_{n_k}\to x$ for some subsequence $\{s_{n_k}\}$ of $\{s_n\}$ . Definition 2 $\quad$ [ Measure Theory by Donald Cohn] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ is defined by \begin{align*} \limsup_{n\to\infty}s_n = \inf_{n}\sup_{m\geq n}s_m = \inf_{n}\left\{\sup\{s_n,s_{n+1},\dots\}\right\}. \end{align*} Definition 3 $\quad$ [MSE] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ is defined by \begin{align*} \limsup_{n\to\infty}s_n = \lim_{n\to\infty}\sup_{m\geq n}s_m = \lim_{n\to\infty}\left(\sup\{s_n,s_{n+1},\dots\}\right). \end{align*} Definition 4 $\quad$ [MSE] $\quad$ A number $t$ in the extended real number system is the upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ if \begin{align*} \text{for all $s<t$, we have $s<s_n$ for infinitely many $n$'s} \end{align*} and \begin{align*} \text{for all $s>t$, we have $s<s_n$ for finitely many $n$'s.} \end{align*} I have some questions about the definitions themselves as well as the equivalence among them. Question 1 I am not sure if I am right about this. I think Definition 3 should have been stated as follows: Definition 3 $\quad$ [Revised] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ shall be defined as follows: \begin{align*} \begin{cases} \limsup_{n\to\infty}s_n &= +\infty &\text{if}\ \ \sup\{s_n,s_{n+1},\dots\}\to+\infty,\\ \limsup_{n\to\infty}s_n &= -\infty &\text{if}\ \ \sup\{s_n,s_{n+1},\dots\}\to-\infty,\ \ \text{and}\\ \limsup_{n\to\infty}s_n &= \lim_{n\to\infty}\sup_{m\geq n}s_m &\text{if}\ \ \text{the limit exists}. \end{cases} \end{align*} My concern about this revised definition is that what should we say about the case when the limit of $\sup_{m\geq n}s_m$ does not exist yet neither $\sup\{s_n,s_{n+1},\dots\}\to\pm\infty$ ? Question 2 In one of the link I provided in the beginning, there is a proof of the equivalence between Definition 1 and Definition 3. However, I am not sure if my understanding of that proof is correct, espectially for the proof of $\sup S \leq \limsup_{n\to\infty}s_n$ where the right-hand side of the inequality is in the sense of Definition 3. Moreover, I would like to try to work out all the details. I would really appreciate it if someone could help me check my work! Proof $\quad$ Let $S$ be the set of numbers $x$ in the extended real number system such that $s_{n_k}\to x$ for some subsequence $\{s_{n_k}\}$ of a sequence $\{s_n\}$ . We want to prove that \begin{align*} \sup S = \limsup_{n\to\infty}s_n, \end{align*} where the right-hand-side is in the sense of the revised Definition 3. Let $F_n=\{s_n,s_{n+1},\dots\}$ . We first prove that \begin{align*} \sup S \leq \limsup_{n\to\infty}s_n. \end{align*} Suppose first that $\sup F_n\to+\infty$ . Then $\limsup_{n\to\infty}s_n=+\infty$ by definition, and we are done. Suppose next that $\sup F_n\to-\infty$ . We want to show that $\sup S = -\infty$ , which is true if and only if every subsequence $\{s_{n_k}\}$ of $\{s_n\}$ such that $s_{n_k}\to x\in S$ satisfies $s_{n_k}\to-\infty$ . Assume to the contrary that there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to\alpha>-\infty$ . For each $n$ , let $n_k$ be the smallest integer bigger than or equal to $n$ , so that $\{s_{n_k},s_{n_{k+1}},\dots\}\subseteq F_n$ . Then $\sup F_n \geq \sup\{s_{n_k},s_{n_{k+1}},\dots\}\geq\alpha$ for each $n$ , contradicting the fact that $\sup F_n\to-\infty$ . Therefore, $\sup S=-\infty$ . Finally, suppose that the limit $\lim_{n\to\infty}(\sup F_n)$ exists. Then for each subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to x\in S$ , construct a sequence $\{u_n\}$ as follows: For index $n$ such that $n=n_k$ for some $n_k$ let $u_n=s_n=s_{n_k}$ , and for other index $n$ let $u_n=u_{n+1}$ . Let $E_n=\{u_n,u_{n+1},\dots\}$ . Then for all $n$ we have $E_n\subseteq F_n$ . Thus for all $n$ we have $\sup E_n\leq\sup F_n$ . Note that $s_{n_k}\not\to+\infty$ . If $s_{n_k}\to x=-\infty$ , then $u_n\to-\infty\leq\lim_{n\to\infty}(\sup F_n)$ . If $\lim_{k\to\infty}s_{n_k}=x\in\mathbb{R}$ , then $\lim_{n\to\infty}u_n=\lim_{k\to\infty}s_{n_k}=x$ . Then $E_n\supseteq E_{n+1}$ and so $\sup E_n\geq\sup E_{n+1}$ for all $n$ . Moreover, $\sup E_n\geq x$ for all $n$ . It follows that $\{\sup E_n\}$ is a bounded decreasing sequence, so $\lim_{n\to\infty}(\sup E_n)$ exists. Since $E_n\subseteq F_n$ for all $n$ , we have $u_n\leq\sup E_n\leq\sup F_n$ for all $n$ , and it follows that \begin{align*} \limsup_{n\to\infty}s_n = \lim_{n\to\infty}(\sup F_n) \geq \lim_{n\to\infty}(\sup E_n) \geq \lim_{n\to\infty}u_n=\lim_{k\to\infty}s_{n_k}=x\in\mathbb{R}. \end{align*} Therefore, for any subsequence $\{s_{n_k}\}$ of $\{s_n\}$ such that $s_{n_k}\to x \in S$ we have $x\leq\lim_{n\to\infty}(\sup F_n)$ ; that is, $x\leq\lim_{n\to\infty}(\sup F_n)$ for all $x\in S$ . Hence, $\sup S\leq\lim_{n\to\infty}(\sup F_n)=\limsup_{n\to\infty}s_n$ . This complets the proof of $\sup S \leq \limsup_{n\to\infty}s_n$ . Now we prove the opposite direction \begin{align*} \sup S\geq\limsup_{n\to\infty}s_n. \end{align*} If $\sup S=+\infty$ , then there is nothing to prove. If $\sup S=-\infty$ , then every subsequence $\{s_{n_k}\}$ of $\{s_n\}$ such that $s_{n_k}\to x\in S$ satisfies $s_{n_k}\to-\infty$ . We want to show that $\sup F_n\to-\infty$ . If $\sup F_n\to\alpha>-\infty$ , then $\sup F_n\geq\sup F_{n+1}$ for all $n$ implies $\sup F_n\geq\alpha$ for all $n$ . Hence, there must exists a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to\alpha$ , a contradiction. Therefore, we must have $\sup F_n\to-\infty$ , which means $\limsup_{n\to\infty}s_n=-\infty$ . Now suppose $\sup S\in\mathbb{R}$ . If $\sup F_n\to+\infty$ , we have seen that it will imply the existence of a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to+\infty$ , contradicting our assumption of $\sup S\in\mathbb{R}$ . If $\sup F_n\to-\infty$ , then there is nothing to prove. So suppose that the limit $\lim_{n\to\infty}(\sup F_n)$ exists. For each $n$ , let $l_n\in\mathbb{N}$ be such that $l_n\geq n$ and \begin{align*} \sup F_n \geq s_{l_n} \geq \sup F_n - \frac{1}{2^n}. \end{align*} Then, \begin{align*} \lim_{n\to\infty}(\sup F_n) = \lim_{n\to\infty}s_{l_n}. \end{align*} Since $l_n\geq n$ for each $n$ , it follows that $\{l_n\}$ forms a sequence of positive integers such that $l_n\to+\infty$ . There there is an increasing subsequence $\{l_{n_k}\}$ of $\{l_n\}$ . Let $n_k=l_{n_k}$ . Since $\{s_{n_k}\}$ is a subsequence of $\{s_{l_n}\}$ which converges to $\lim_{n\to\infty}(\sup F_n)$ , it follows that $\lim_{k\to\infty}s_{n_k} = \lim_{n\to\infty}(\sup F_n)$ , and so $\lim_{n\to\infty}(\sup F_n)\in S$ . Hence, $\lim_{n\to\infty}(\sup F_n)\leq\sup S$ . This complets the proof of $\sup S \geq \limsup_{n\to\infty}s_n$ . I am not sure if this is all correct and rigorous. For example, when proving $\sup S\geq\limsup_{n\to\infty}s_n$ , I wrote the following without proof: If $\sup F_n\to\alpha>-\infty$ , then $\sup F_n\geq\sup F_{n+1}$ for all $n$ implies $\sup F_n\geq\alpha$ for all $n$ . Hence, there must exists a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to\alpha$ , a contradiction."" Honestly, I just think this is intuitively ture, but couldn't figure out how to prove this claim. It would be great if someone can help me out. Moreover, any other improvement suggestion would be greatly appreciated! Question 3 This question is about the equivalence of Definition 2 and 3. I was wondering if the equivalence follows from the Monotone Convergence Theorem? Question 4 How would one provide a proof for the equivalence between Definition 4 and one of the other three definitions? Thanks a lot in advance!","Related questions have been posted here and here . Background I have seen the following four definitions of the upper and lower limits of a sequence from textbooks and MSE posts: Definition 1 [ Baby Rudin ] The upper limit, , of a given sequence is the supreme of the set of numbers in the extended real number system such that for some subsequence of . Definition 2 [ Measure Theory by Donald Cohn] The upper limit, , of a given sequence is defined by Definition 3 [MSE] The upper limit, , of a given sequence is defined by Definition 4 [MSE] A number in the extended real number system is the upper limit, , of a given sequence if and I have some questions about the definitions themselves as well as the equivalence among them. Question 1 I am not sure if I am right about this. I think Definition 3 should have been stated as follows: Definition 3 [Revised] The upper limit, , of a given sequence shall be defined as follows: My concern about this revised definition is that what should we say about the case when the limit of does not exist yet neither ? Question 2 In one of the link I provided in the beginning, there is a proof of the equivalence between Definition 1 and Definition 3. However, I am not sure if my understanding of that proof is correct, espectially for the proof of where the right-hand side of the inequality is in the sense of Definition 3. Moreover, I would like to try to work out all the details. I would really appreciate it if someone could help me check my work! Proof Let be the set of numbers in the extended real number system such that for some subsequence of a sequence . We want to prove that where the right-hand-side is in the sense of the revised Definition 3. Let . We first prove that Suppose first that . Then by definition, and we are done. Suppose next that . We want to show that , which is true if and only if every subsequence of such that satisfies . Assume to the contrary that there is a subsequence such that . For each , let be the smallest integer bigger than or equal to , so that . Then for each , contradicting the fact that . Therefore, . Finally, suppose that the limit exists. Then for each subsequence such that , construct a sequence as follows: For index such that for some let , and for other index let . Let . Then for all we have . Thus for all we have . Note that . If , then . If , then . Then and so for all . Moreover, for all . It follows that is a bounded decreasing sequence, so exists. Since for all , we have for all , and it follows that Therefore, for any subsequence of such that we have ; that is, for all . Hence, . This complets the proof of . Now we prove the opposite direction If , then there is nothing to prove. If , then every subsequence of such that satisfies . We want to show that . If , then for all implies for all . Hence, there must exists a subsequence such that , a contradiction. Therefore, we must have , which means . Now suppose . If , we have seen that it will imply the existence of a subsequence such that , contradicting our assumption of . If , then there is nothing to prove. So suppose that the limit exists. For each , let be such that and Then, Since for each , it follows that forms a sequence of positive integers such that . There there is an increasing subsequence of . Let . Since is a subsequence of which converges to , it follows that , and so . Hence, . This complets the proof of . I am not sure if this is all correct and rigorous. For example, when proving , I wrote the following without proof: If , then for all implies for all . Hence, there must exists a subsequence such that , a contradiction."" Honestly, I just think this is intuitively ture, but couldn't figure out how to prove this claim. It would be great if someone can help me out. Moreover, any other improvement suggestion would be greatly appreciated! Question 3 This question is about the equivalence of Definition 2 and 3. I was wondering if the equivalence follows from the Monotone Convergence Theorem? Question 4 How would one provide a proof for the equivalence between Definition 4 and one of the other three definitions? Thanks a lot in advance!","\quad \quad \limsup_{n\to\infty}s_n \{s_n\} x s_{n_k}\to x \{s_{n_k}\} \{s_n\} \quad \quad \limsup_{n\to\infty}s_n \{s_n\} \begin{align*}
\limsup_{n\to\infty}s_n = \inf_{n}\sup_{m\geq n}s_m = \inf_{n}\left\{\sup\{s_n,s_{n+1},\dots\}\right\}.
\end{align*} \quad \quad \limsup_{n\to\infty}s_n \{s_n\} \begin{align*}
\limsup_{n\to\infty}s_n = \lim_{n\to\infty}\sup_{m\geq n}s_m = \lim_{n\to\infty}\left(\sup\{s_n,s_{n+1},\dots\}\right).
\end{align*} \quad \quad t \limsup_{n\to\infty}s_n \{s_n\} \begin{align*}
\text{for all s<t, we have s<s_n for infinitely many n's}
\end{align*} \begin{align*}
\text{for all s>t, we have s<s_n for finitely many n's.}
\end{align*} \quad \quad \limsup_{n\to\infty}s_n \{s_n\} \begin{align*}
\begin{cases}
\limsup_{n\to\infty}s_n &= +\infty &\text{if}\ \ \sup\{s_n,s_{n+1},\dots\}\to+\infty,\\
\limsup_{n\to\infty}s_n &= -\infty &\text{if}\ \ \sup\{s_n,s_{n+1},\dots\}\to-\infty,\ \ \text{and}\\
\limsup_{n\to\infty}s_n &= \lim_{n\to\infty}\sup_{m\geq n}s_m &\text{if}\ \ \text{the limit exists}.
\end{cases}
\end{align*} \sup_{m\geq n}s_m \sup\{s_n,s_{n+1},\dots\}\to\pm\infty \sup S \leq \limsup_{n\to\infty}s_n \quad S x s_{n_k}\to x \{s_{n_k}\} \{s_n\} \begin{align*}
\sup S = \limsup_{n\to\infty}s_n,
\end{align*} F_n=\{s_n,s_{n+1},\dots\} \begin{align*}
\sup S \leq \limsup_{n\to\infty}s_n.
\end{align*} \sup F_n\to+\infty \limsup_{n\to\infty}s_n=+\infty \sup F_n\to-\infty \sup S = -\infty \{s_{n_k}\} \{s_n\} s_{n_k}\to x\in S s_{n_k}\to-\infty \{s_{n_k}\} s_{n_k}\to\alpha>-\infty n n_k n \{s_{n_k},s_{n_{k+1}},\dots\}\subseteq F_n \sup F_n \geq \sup\{s_{n_k},s_{n_{k+1}},\dots\}\geq\alpha n \sup F_n\to-\infty \sup S=-\infty \lim_{n\to\infty}(\sup F_n) \{s_{n_k}\} s_{n_k}\to x\in S \{u_n\} n n=n_k n_k u_n=s_n=s_{n_k} n u_n=u_{n+1} E_n=\{u_n,u_{n+1},\dots\} n E_n\subseteq F_n n \sup E_n\leq\sup F_n s_{n_k}\not\to+\infty s_{n_k}\to x=-\infty u_n\to-\infty\leq\lim_{n\to\infty}(\sup F_n) \lim_{k\to\infty}s_{n_k}=x\in\mathbb{R} \lim_{n\to\infty}u_n=\lim_{k\to\infty}s_{n_k}=x E_n\supseteq E_{n+1} \sup E_n\geq\sup E_{n+1} n \sup E_n\geq x n \{\sup E_n\} \lim_{n\to\infty}(\sup E_n) E_n\subseteq F_n n u_n\leq\sup E_n\leq\sup F_n n \begin{align*}
\limsup_{n\to\infty}s_n = \lim_{n\to\infty}(\sup F_n) \geq \lim_{n\to\infty}(\sup E_n) \geq \lim_{n\to\infty}u_n=\lim_{k\to\infty}s_{n_k}=x\in\mathbb{R}.
\end{align*} \{s_{n_k}\} \{s_n\} s_{n_k}\to x \in S x\leq\lim_{n\to\infty}(\sup F_n) x\leq\lim_{n\to\infty}(\sup F_n) x\in S \sup S\leq\lim_{n\to\infty}(\sup F_n)=\limsup_{n\to\infty}s_n \sup S \leq \limsup_{n\to\infty}s_n \begin{align*}
\sup S\geq\limsup_{n\to\infty}s_n.
\end{align*} \sup S=+\infty \sup S=-\infty \{s_{n_k}\} \{s_n\} s_{n_k}\to x\in S s_{n_k}\to-\infty \sup F_n\to-\infty \sup F_n\to\alpha>-\infty \sup F_n\geq\sup F_{n+1} n \sup F_n\geq\alpha n \{s_{n_k}\} s_{n_k}\to\alpha \sup F_n\to-\infty \limsup_{n\to\infty}s_n=-\infty \sup S\in\mathbb{R} \sup F_n\to+\infty \{s_{n_k}\} s_{n_k}\to+\infty \sup S\in\mathbb{R} \sup F_n\to-\infty \lim_{n\to\infty}(\sup F_n) n l_n\in\mathbb{N} l_n\geq n \begin{align*}
\sup F_n \geq s_{l_n} \geq \sup F_n - \frac{1}{2^n}.
\end{align*} \begin{align*}
\lim_{n\to\infty}(\sup F_n) = \lim_{n\to\infty}s_{l_n}.
\end{align*} l_n\geq n n \{l_n\} l_n\to+\infty \{l_{n_k}\} \{l_n\} n_k=l_{n_k} \{s_{n_k}\} \{s_{l_n}\} \lim_{n\to\infty}(\sup F_n) \lim_{k\to\infty}s_{n_k} = \lim_{n\to\infty}(\sup F_n) \lim_{n\to\infty}(\sup F_n)\in S \lim_{n\to\infty}(\sup F_n)\leq\sup S \sup S \geq \limsup_{n\to\infty}s_n \sup S\geq\limsup_{n\to\infty}s_n \sup F_n\to\alpha>-\infty \sup F_n\geq\sup F_{n+1} n \sup F_n\geq\alpha n \{s_{n_k}\} s_{n_k}\to\alpha","['real-analysis', 'sequences-and-series', 'limits', 'supremum-and-infimum', 'limsup-and-liminf']"
1,Can convolution fails to be commutative and distributive in the sense of (generalized) Riemann integral?,Can convolution fails to be commutative and distributive in the sense of (generalized) Riemann integral?,,"For two (say continuous) function $f,g\in C(\mathbb R^n)$ , let us define \begin{equation} f\ast g(x):=\lim_{R\to+\infty}\int_{B^n(0,R)}f(x-y)g(y)dy, \end{equation} whenever the integral converges. It is partially clear that under such definition the convolution can be non-associative (e.g. this post ). Indeed by taking Fourier transform convolutions become multiplications, where the multiplications of distributions can be non-associative without certain restrictions. My question is, can it fails to be commutative and distributive as well? More precisely, Can we find $f,g\in C(\mathbb R^n)$ such that $f\ast g$ and $g\ast f$ both converge, but not equal? Can we find $f,g,h\in C(\mathbb R^n)$ such that $f\ast h$ , $g\ast h$ and $(f+g)\ast h$ all converge, but $f\ast h+g\ast h\neq(f+g)\ast h$ ? Note that both questions don't have analogy on the products of distributions. For a bonus part from Proposition 8.6 in Folland's Real Analysis , set $\tau_hf(x)=f(x-h)$ , Can we find $f,g\in C(\mathbb R^n)$ and $h\in\mathbb R^n$ such that $f\ast g$ and $(\tau_hf)\ast g$ are both converge, but $\tau_h(f\ast g)\neq(\tau_hf)\ast g$ ? Can we do the similar thing to $f\ast(\tau_hg)$ as well?","For two (say continuous) function , let us define whenever the integral converges. It is partially clear that under such definition the convolution can be non-associative (e.g. this post ). Indeed by taking Fourier transform convolutions become multiplications, where the multiplications of distributions can be non-associative without certain restrictions. My question is, can it fails to be commutative and distributive as well? More precisely, Can we find such that and both converge, but not equal? Can we find such that , and all converge, but ? Note that both questions don't have analogy on the products of distributions. For a bonus part from Proposition 8.6 in Folland's Real Analysis , set , Can we find and such that and are both converge, but ? Can we do the similar thing to as well?","f,g\in C(\mathbb R^n) \begin{equation}
f\ast g(x):=\lim_{R\to+\infty}\int_{B^n(0,R)}f(x-y)g(y)dy,
\end{equation} f,g\in C(\mathbb R^n) f\ast g g\ast f f,g,h\in C(\mathbb R^n) f\ast h g\ast h (f+g)\ast h f\ast h+g\ast h\neq(f+g)\ast h \tau_hf(x)=f(x-h) f,g\in C(\mathbb R^n) h\in\mathbb R^n f\ast g (\tau_hf)\ast g \tau_h(f\ast g)\neq(\tau_hf)\ast g f\ast(\tau_hg)","['real-analysis', 'analysis', 'convolution', 'riemann-integration']"
2,"Is there an ""elementary irrational number"" without a certain digit in its decimal presentation?","Is there an ""elementary irrational number"" without a certain digit in its decimal presentation?",,"In this question, I define an ""elementary irrational number"" as an irrational number which is built up of a finite combination of integers, field operations (addition, multiplication, division, and root extractions -- the elementary operations) and exponential and trigonometric functions and their inverses under repeated compositions, as if the definition of elementary function (I made up the name since I couldn't find it on the Internet). The numbers $\pi=\arccos(-1)$ , $\mathrm e=\exp(1)$ and $\phi=\frac{1+\sqrt5}2$ are ""elementary irrational numbers"", while $1$ , $\frac32$ and $1.2121121112\dots$ are not. It's easy to find either a rational number or an irrational one without a certain digit in its decimal presentation, for example, $1.2121121112\dots$ doesn't contain digit $0$ . However, among ""elementary irrational numbers"", it seems hard to find such number, despite proving there is no such number is not easy either. I can come up with some possible ways to solve the problem: Most constructed numbers aren't ""elementary"", while there does exists possibility to find one. To disprove the existence of such number, we may prove through all the operations to build up the elementary functions that if its arguments are ""elementary number(s)"" which is/are either rational or contains every kind of digit, then the result satisfies the same property, but it seems to be quite difficult to prove it. Or maybe the problem is connected to some existed open problems in mathematics (normal numbers might be an example), then we may be convinced with the difficulty of the problem. I'd appreciate it if you could give me any advice on the problem.","In this question, I define an ""elementary irrational number"" as an irrational number which is built up of a finite combination of integers, field operations (addition, multiplication, division, and root extractions -- the elementary operations) and exponential and trigonometric functions and their inverses under repeated compositions, as if the definition of elementary function (I made up the name since I couldn't find it on the Internet). The numbers , and are ""elementary irrational numbers"", while , and are not. It's easy to find either a rational number or an irrational one without a certain digit in its decimal presentation, for example, doesn't contain digit . However, among ""elementary irrational numbers"", it seems hard to find such number, despite proving there is no such number is not easy either. I can come up with some possible ways to solve the problem: Most constructed numbers aren't ""elementary"", while there does exists possibility to find one. To disprove the existence of such number, we may prove through all the operations to build up the elementary functions that if its arguments are ""elementary number(s)"" which is/are either rational or contains every kind of digit, then the result satisfies the same property, but it seems to be quite difficult to prove it. Or maybe the problem is connected to some existed open problems in mathematics (normal numbers might be an example), then we may be convinced with the difficulty of the problem. I'd appreciate it if you could give me any advice on the problem.",\pi=\arccos(-1) \mathrm e=\exp(1) \phi=\frac{1+\sqrt5}2 1 \frac32 1.2121121112\dots 1.2121121112\dots 0,"['real-analysis', 'irrational-numbers', 'elementary-functions']"
3,Is this condition sufficient to conclude the graph is a straight line?,Is this condition sufficient to conclude the graph is a straight line?,,"Out of curiosity, I was wondering whether if a continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$ has the property that in  any 2 intervals $[a,b]$ and $[c,d]$ on which it is defined, $(a-b = c -d) \implies (f(a)-f(b) = f(c)-f(d))$ , then the graph of this function is a straight line. So far, I have tried reasoning the contrapositive of this-  that not being a straight line means there exist intervals of equal length where the function changes a different amount over them. While this seemed more intuitive, I didn't see a rigorous way to prove this. So is this claim true, and if so, what is the proof?","Out of curiosity, I was wondering whether if a continuous function has the property that in  any 2 intervals and on which it is defined, , then the graph of this function is a straight line. So far, I have tried reasoning the contrapositive of this-  that not being a straight line means there exist intervals of equal length where the function changes a different amount over them. While this seemed more intuitive, I didn't see a rigorous way to prove this. So is this claim true, and if so, what is the proof?","f:\mathbb{R}\rightarrow\mathbb{R} [a,b] [c,d] (a-b = c -d) \implies (f(a)-f(b) = f(c)-f(d))","['real-analysis', 'functions', 'analytic-geometry']"
4,Euler's proof of $\frac{\pi}{6}=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\cdots$,Euler's proof of,\frac{\pi}{6}=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\cdots,"Euler proved $$\frac{\pi}{6}=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\cdots$$ where the reasoning of the signs thus is prepared, so that of the second may be had as $-$ , prime numbers of the form $4m-1$ have a $-$ sign, prime numbers of the form $4m+1$ a $+$ sign, but composite numbers have that sign which agrees with the account of the multiplication from the primes. He proved it by combining $$\frac{\pi}{4}=\prod_{p\,\text{prime},p\gt 2}\frac{1}{1-(-1)^{\frac{p-1}{2}}p^{-1}}$$ (which is a special case of the Euler product of the Dirichlet beta function) with $$\frac{1}{(1-az)(1-bz)(1-cz)\cdots}=1+Az+Bz^2+\cdots$$ where $$A=\text{sum of the individual terms},$$ $$B=\text{sum of the two factors at a time},$$ etc. with the same factors not excluded, and set $z=1$ . But, there is one problem with Euler's proof. He is multiplying infinitely many geometric series and collecting like powers – treating it like polynomial multiplication, this surely requires some justification; can we prove that the factors in $$\frac{1}{(1-az)(1-bz)(1-cz)\cdots}$$ (where $a,b,c,\ldots$ are the signed reciprocal primes) can be arbitrarily rearranged? Euler is not very rigorous and I can't figure it out myself – how to save Euler's proof? Added: GH from MO has already provided an answer on MO: https://mathoverflow.net/questions/459675/eulers-proof-of-frac-pi6-1-frac12-frac13-frac14-frac15","Euler proved where the reasoning of the signs thus is prepared, so that of the second may be had as , prime numbers of the form have a sign, prime numbers of the form a sign, but composite numbers have that sign which agrees with the account of the multiplication from the primes. He proved it by combining (which is a special case of the Euler product of the Dirichlet beta function) with where etc. with the same factors not excluded, and set . But, there is one problem with Euler's proof. He is multiplying infinitely many geometric series and collecting like powers – treating it like polynomial multiplication, this surely requires some justification; can we prove that the factors in (where are the signed reciprocal primes) can be arbitrarily rearranged? Euler is not very rigorous and I can't figure it out myself – how to save Euler's proof? Added: GH from MO has already provided an answer on MO: https://mathoverflow.net/questions/459675/eulers-proof-of-frac-pi6-1-frac12-frac13-frac14-frac15","\frac{\pi}{6}=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\cdots - 4m-1 - 4m+1 + \frac{\pi}{4}=\prod_{p\,\text{prime},p\gt 2}\frac{1}{1-(-1)^{\frac{p-1}{2}}p^{-1}} \frac{1}{(1-az)(1-bz)(1-cz)\cdots}=1+Az+Bz^2+\cdots A=\text{sum of the individual terms}, B=\text{sum of the two factors at a time}, z=1 \frac{1}{(1-az)(1-bz)(1-cz)\cdots} a,b,c,\ldots","['real-analysis', 'sequences-and-series', 'pi', 'absolute-convergence']"
5,Is $\left(1+\frac1n\right)^{n+1/2}$ decreasing?,Is  decreasing?,\left(1+\frac1n\right)^{n+1/2},"Using the Cauchy-Schwarz Inequality , we have $$ \begin{align} 1 &=\left(\int_n^{n+1}1\,\mathrm{d}x\right)^2\\ &\le\left(\int_n^{n+1}x\,\mathrm{d}x\right)\left(\int_n^{n+1}\frac1x\,\mathrm{d}x\right)\\ &=\left(n+\frac12\right)\log\left(1+\frac1n\right) \end{align} $$ which means that $$ \left(1+\frac1n\right)^{n+1/2}\ge e $$ This hints that $\left(1+\frac1n\right)^{n+1/2}$ might be decreasing. In this answer , it is shown that $\left(1+\frac1n\right)^n$ is increasing and $\left(1+\frac1n\right)^{n+1}$ is decreasing. The proofs use Bernoulli's Inequality . However, applying Bernoulli to $\left(1+\frac1n\right)^{n+1/2}$ is inconclusive. Attempt to show decrease: $$ \begin{align} \frac{\left(1+\frac1{n-1}\right)^{2n-1}}{\left(1+\frac1n\right)^{2n+1}} &=\left(1+\frac1{n^2-1}\right)^{2n}\frac{n-1}{n+1}\\ &\ge\left(1+\frac{2n}{n^2-1}\right)\frac{n-1}{n+1}\\[6pt] &=1-\frac{2}{(n+1)^2} \end{align} $$ Attempt to show increase: $$ \begin{align} \frac{\left(1+\frac1n\right)^{2n+1}}{\left(1+\frac1{n-1}\right)^{2n-1}} &=\left(1-\frac1{n^2}\right)^{2n}\frac{n+1}{n-1}\\ &\ge\left(1-\frac2n\right)\frac{n+1}{n-1}\\[6pt] &=1-\frac{2}{n(n-1)} \end{align} $$ Neither works. Without resorting to derivatives, is there something stronger than Bernoulli, but similarly elementary, that might be used to show that $\left(1+\frac1n\right)^{n+1/2}$ decreases?","Using the Cauchy-Schwarz Inequality , we have which means that This hints that might be decreasing. In this answer , it is shown that is increasing and is decreasing. The proofs use Bernoulli's Inequality . However, applying Bernoulli to is inconclusive. Attempt to show decrease: Attempt to show increase: Neither works. Without resorting to derivatives, is there something stronger than Bernoulli, but similarly elementary, that might be used to show that decreases?","
\begin{align}
1
&=\left(\int_n^{n+1}1\,\mathrm{d}x\right)^2\\
&\le\left(\int_n^{n+1}x\,\mathrm{d}x\right)\left(\int_n^{n+1}\frac1x\,\mathrm{d}x\right)\\
&=\left(n+\frac12\right)\log\left(1+\frac1n\right)
\end{align}
 
\left(1+\frac1n\right)^{n+1/2}\ge e
 \left(1+\frac1n\right)^{n+1/2} \left(1+\frac1n\right)^n \left(1+\frac1n\right)^{n+1} \left(1+\frac1n\right)^{n+1/2} 
\begin{align}
\frac{\left(1+\frac1{n-1}\right)^{2n-1}}{\left(1+\frac1n\right)^{2n+1}}
&=\left(1+\frac1{n^2-1}\right)^{2n}\frac{n-1}{n+1}\\
&\ge\left(1+\frac{2n}{n^2-1}\right)\frac{n-1}{n+1}\\[6pt]
&=1-\frac{2}{(n+1)^2}
\end{align}
 
\begin{align}
\frac{\left(1+\frac1n\right)^{2n+1}}{\left(1+\frac1{n-1}\right)^{2n-1}}
&=\left(1-\frac1{n^2}\right)^{2n}\frac{n+1}{n-1}\\
&\ge\left(1-\frac2n\right)\frac{n+1}{n-1}\\[6pt]
&=1-\frac{2}{n(n-1)}
\end{align}
 \left(1+\frac1n\right)^{n+1/2}","['inequality', 'exponential-function', 'cauchy-schwarz-inequality', 'eulers-number-e']"
6,A lemma in the application of Concentration compactness principle in Hardy-Littlewood-Sobolev inequality,A lemma in the application of Concentration compactness principle in Hardy-Littlewood-Sobolev inequality,,"I'm encountering some problems when reading Lions' paper ""the concentration-compactness principle in the calculus of variations. The limit case, Part 2"". The Hardy-Littlewood-Sobolev (HLS) inequality is stated as $$ \| K*u\|_q\le C\|u\|_p $$ where $K=|x|^{-\lambda}$ and $1<p<q<\infty, 0<\lambda<n, \tfrac 1p+\tfrac \lambda n=1+\tfrac 1q$ . When proving the existence of maximizers of HLS inequality, he used his second concentration compactness principle. (Lemma2.1) Let $u_n$ converge weakly in $L^p(\mathbb R^n)$ to $u$ and assume $|u_n|^p$ is tight. We may assume without loss of generality that $|K*u_n|^q$ , $|u_n|^p$ converge weakly (or tightly) in the sense of measure to some bounded nonnegative measures $\nu, \mu$ on $\mathbb R^n$ . Then we have: there exist some at most countable set (possibly empty) and two families $(x_j)_{j\in J}$ of distinct points in $\mathbb R^n$ , $(\nu_j)_{j\in J}$ in $(0,\infty)$ such that: $$ \nu=|K*u|^q+\sum_{j\in J}\nu_j\delta_{x_j}$$ where $K$ is the kernel $|x|^{-\lambda}$ . To prove this lemma (lemma 2.1 in his paper), he cited his another lemma in his paper ""the concentration-compactness principle in the calculus of variations. The limit case, Part 1"" (Lemma 1.2) Let $\mu,\nu$ be two bounded nonnegative measures on $\mathbb R^n$ satisfying for some constant $C_0\ge 0$ $$\left(\int_{\mathbb R^n} |\varphi|^qd\nu\right)^{1/q}\le C_0 \left(   \int_{\mathbb R^n} |\varphi|^pd\mu\right)^{1/p}~~~~\text{for any}~\varphi\in C_c^\infty(\mathbb R^n)\tag 1$$ In order to get the inequality (1). After some progress, we can assume that $u=0$ , i.e., $u_n\to 0$ weakly in $L^p$ . And he showed that we only need to estimate an upper bound of (See the page 51 of his paper) $$ v_n(x)=\left|\int_{|y|\le R}\cfrac{\varphi(y)-\varphi(x)}{|x-y|^\lambda}u_n(y)dy       \right| $$ He wrote ' Denoting by $R(x,y)=(\varphi(y)-\varphi(x))|x-y|^{-\lambda}$ and observing that $R(x,y) 1_{|y\le R}\in L^r(\mathbb R^n)$ for each $x$ where $r<\frac{n}{\lambda-1}$ if $\lambda>1$ , $r\le \infty$ if $\lambda \le 1$ , we see that: $v_n\to 0$ a.e. on $\mathbb R^n$ '. My question: 1 . It seems that we cannot get that $R(x,y)1_{|y|\le R}\in L^{p'}$ for a.e. $x\in \mathbb R^n$ if we only have $R(x,y)1_{|y|\le R} \in L^r$ . Thus we can not use the assumption that $u_n\to 0$ weakly in $L^p$ . I am wondering whether I missed some other important information in his proof and note that if $R(x,y)1_{|y|\le R}$ is not in $L^{p'}$ , we don't know whether $v_n$ exists. 2 . It seems that we do not need the statement about ' $R(x,y)1_{|y|\le R}$ '. In his beginning, he got $K*u_n \to K*u$ a.e.. Since $u=0$ , we have $$ \int \cfrac{u_n(y)}{|x-y|^{\lambda}}dy\to 0 ~~~a.e.~x\in \mathbb R^n $$ and hence $$ |v_n|\le 2\|\varphi\|_\infty \int \cfrac{u_n(y)}{|x-y|^{\lambda}}dy \to 0. $$ (We can take $u_n$ as nonnegative sequence since $|u_n|$ is also a maximizing sequence) Does my way work? Thanks in advance . Thanks for your attention and I hope you can help me with this.","I'm encountering some problems when reading Lions' paper ""the concentration-compactness principle in the calculus of variations. The limit case, Part 2"". The Hardy-Littlewood-Sobolev (HLS) inequality is stated as where and . When proving the existence of maximizers of HLS inequality, he used his second concentration compactness principle. (Lemma2.1) Let converge weakly in to and assume is tight. We may assume without loss of generality that , converge weakly (or tightly) in the sense of measure to some bounded nonnegative measures on . Then we have: there exist some at most countable set (possibly empty) and two families of distinct points in , in such that: where is the kernel . To prove this lemma (lemma 2.1 in his paper), he cited his another lemma in his paper ""the concentration-compactness principle in the calculus of variations. The limit case, Part 1"" (Lemma 1.2) Let be two bounded nonnegative measures on satisfying for some constant In order to get the inequality (1). After some progress, we can assume that , i.e., weakly in . And he showed that we only need to estimate an upper bound of (See the page 51 of his paper) He wrote ' Denoting by and observing that for each where if , if , we see that: a.e. on '. My question: 1 . It seems that we cannot get that for a.e. if we only have . Thus we can not use the assumption that weakly in . I am wondering whether I missed some other important information in his proof and note that if is not in , we don't know whether exists. 2 . It seems that we do not need the statement about ' '. In his beginning, he got a.e.. Since , we have and hence (We can take as nonnegative sequence since is also a maximizing sequence) Does my way work? Thanks in advance . Thanks for your attention and I hope you can help me with this.","
\| K*u\|_q\le C\|u\|_p
 K=|x|^{-\lambda} 1<p<q<\infty, 0<\lambda<n, \tfrac 1p+\tfrac \lambda n=1+\tfrac 1q u_n L^p(\mathbb R^n) u |u_n|^p |K*u_n|^q |u_n|^p \nu, \mu \mathbb R^n (x_j)_{j\in J} \mathbb R^n (\nu_j)_{j\in J} (0,\infty)  \nu=|K*u|^q+\sum_{j\in J}\nu_j\delta_{x_j} K |x|^{-\lambda} \mu,\nu \mathbb R^n C_0\ge 0 \left(\int_{\mathbb R^n} |\varphi|^qd\nu\right)^{1/q}\le C_0 \left( 
 \int_{\mathbb R^n} |\varphi|^pd\mu\right)^{1/p}~~~~\text{for any}~\varphi\in C_c^\infty(\mathbb R^n)\tag 1 u=0 u_n\to 0 L^p 
v_n(x)=\left|\int_{|y|\le R}\cfrac{\varphi(y)-\varphi(x)}{|x-y|^\lambda}u_n(y)dy       \right|
 R(x,y)=(\varphi(y)-\varphi(x))|x-y|^{-\lambda} R(x,y) 1_{|y\le R}\in L^r(\mathbb R^n) x r<\frac{n}{\lambda-1} \lambda>1 r\le \infty \lambda \le 1 v_n\to 0 \mathbb R^n R(x,y)1_{|y|\le R}\in L^{p'} x\in \mathbb R^n R(x,y)1_{|y|\le R} \in L^r u_n\to 0 L^p R(x,y)1_{|y|\le R} L^{p'} v_n R(x,y)1_{|y|\le R} K*u_n \to K*u u=0 
\int \cfrac{u_n(y)}{|x-y|^{\lambda}}dy\to 0 ~~~a.e.~x\in \mathbb R^n
 
|v_n|\le 2\|\varphi\|_\infty \int \cfrac{u_n(y)}{|x-y|^{\lambda}}dy \to 0.
 u_n |u_n|","['real-analysis', 'weak-convergence', 'functional-inequalities', 'concentration-of-measure']"
7,"Show $\bigcup\limits_{n\mid n\geq N}A_{n,m}=\left\{\sup\limits_{n\mid n\geq N}\left|\frac{S_n(\omega)}{n}-p\right|\geq\frac{1}{m}\right\}$",Show,"\bigcup\limits_{n\mid n\geq N}A_{n,m}=\left\{\sup\limits_{n\mid n\geq N}\left|\frac{S_n(\omega)}{n}-p\right|\geq\frac{1}{m}\right\}","Let be $m,n\in\mathbb{N}$ , $p\in(0,1)$ , $S_n$ is a binomially distributed random variable and $ \begin{align*} &A_{n,m}:=\left\{\omega\in\Omega\mid \left|\frac{S_n(\omega)}{n}-p\right|\geq \frac{1}{m}\right\}. \end{align*} $ Here, $\Omega=\{0,1\}^{\mathbb{N}}$ and we can assume that there exists a probability space $(\Omega,\mathcal{F},P)$ such that $A_{n,m}\in \mathcal{F}$ . Show that $ \begin{align*} \bigcup\limits_{n\mid n\geq N}A_{n,m}=\left\{\omega\in\Omega\mid\sup\limits_{n\mid n\geq N}\left|\frac{S_n(\omega)}{n}-p\right|\geq\frac{1}{m} \right\}. \end{align*} $ The relation $\subset$ is not a problem but I am not sure why $\supset$ holds. I tried to prove it with a contraposition $(\lnot B\implies \lnot A)\iff (A\implies B)$ but it gets me nowhere: We choose an $\omega\in\Omega$ with $\sup\limits_{n\mid n\geq N}\left|\frac{S_n(\omega)}{n}-p\right|\geq\frac{1}{m}$ and fix it. If we assume that there exists no $n\geq N$ such that $\left|\frac{S_n(\omega)}{n}-p\right|\geq \frac{1}{m}$ , then there must be a $n\geq N$ with $\left|\frac{S_n(\omega)}{n}-p\right|< \frac{1}{m}$ . But this doesn't lead to a contradiction... Any suggestions? Am I missing someting?","Let be , , is a binomially distributed random variable and Here, and we can assume that there exists a probability space such that . Show that The relation is not a problem but I am not sure why holds. I tried to prove it with a contraposition but it gets me nowhere: We choose an with and fix it. If we assume that there exists no such that , then there must be a with . But this doesn't lead to a contradiction... Any suggestions? Am I missing someting?","m,n\in\mathbb{N} p\in(0,1) S_n 
\begin{align*}
&A_{n,m}:=\left\{\omega\in\Omega\mid \left|\frac{S_n(\omega)}{n}-p\right|\geq \frac{1}{m}\right\}.
\end{align*}
 \Omega=\{0,1\}^{\mathbb{N}} (\Omega,\mathcal{F},P) A_{n,m}\in \mathcal{F} 
\begin{align*}
\bigcup\limits_{n\mid n\geq N}A_{n,m}=\left\{\omega\in\Omega\mid\sup\limits_{n\mid n\geq N}\left|\frac{S_n(\omega)}{n}-p\right|\geq\frac{1}{m} \right\}.
\end{align*}
 \subset \supset (\lnot B\implies \lnot A)\iff (A\implies B) \omega\in\Omega \sup\limits_{n\mid n\geq N}\left|\frac{S_n(\omega)}{n}-p\right|\geq\frac{1}{m} n\geq N \left|\frac{S_n(\omega)}{n}-p\right|\geq \frac{1}{m} n\geq N \left|\frac{S_n(\omega)}{n}-p\right|< \frac{1}{m}","['real-analysis', 'probability-theory', 'random-variables', 'supremum-and-infimum']"
8,Calculate $\int _0^1\frac{\arcsin ^2\left(x\right)\ln \left(x\right)\ln \left(1-x\right)}{x}\:\mathrm{d}x$,Calculate,\int _0^1\frac{\arcsin ^2\left(x\right)\ln \left(x\right)\ln \left(1-x\right)}{x}\:\mathrm{d}x,"this integral got posted on a mathematics group by a friend $$I=\int _0^1\frac{\arcsin ^2\left(x\right)\ln \left(x\right)\ln \left(1-x\right)}{x}\:\mathrm{d}x$$ I tried seeing what I'd get from integrating by parts which got me $$I=\frac{1}{2}\int _0^1\frac{\ln ^2\left(x\right)\arcsin ^2\left(x\right)}{1-x}\:\mathrm{d}x-\int _0^1\frac{\ln ^2\left(x\right)\arcsin \left(x\right)\ln \left(1-x\right)}{\sqrt{1-x^2}}\:\mathrm{d}x$$ and these seem quite hard to evaluate, I also tried expanding the integral in series but got nowhere. I also tried using expressing $I$ the following way $$I=\frac{1}{2}\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(x\right)}{x}\:\mathrm{d}x+\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(1-x\right)}{x}\:\mathrm{d}x$$ $$-\frac{1}{2}\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(\frac{x}{1-x}\right)}{x}\:\mathrm{d}x$$ and the first $2$ integrals are probably doable but I can't think of a way to approach the last one. Is it possible to evaluate $I$ or the third integral on the last equation in a fairly simple way? Edit: By Dilogarithm functional equations the following are also related $$\int _0^1\frac{\arcsin ^2\left(x\right)\operatorname{Li}_2\left(x\right)}{x}\:\mathrm{d}x,\:\int _0^1\frac{\arcsin ^2\left(x\right)\operatorname{Li}_2\left(1-x\right)}{x}\:\mathrm{d}x$$ and therefore some very rough hamornic series are involved.","this integral got posted on a mathematics group by a friend I tried seeing what I'd get from integrating by parts which got me and these seem quite hard to evaluate, I also tried expanding the integral in series but got nowhere. I also tried using expressing the following way and the first integrals are probably doable but I can't think of a way to approach the last one. Is it possible to evaluate or the third integral on the last equation in a fairly simple way? Edit: By Dilogarithm functional equations the following are also related and therefore some very rough hamornic series are involved.","I=\int _0^1\frac{\arcsin ^2\left(x\right)\ln \left(x\right)\ln \left(1-x\right)}{x}\:\mathrm{d}x I=\frac{1}{2}\int _0^1\frac{\ln ^2\left(x\right)\arcsin ^2\left(x\right)}{1-x}\:\mathrm{d}x-\int _0^1\frac{\ln ^2\left(x\right)\arcsin \left(x\right)\ln \left(1-x\right)}{\sqrt{1-x^2}}\:\mathrm{d}x I I=\frac{1}{2}\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(x\right)}{x}\:\mathrm{d}x+\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(1-x\right)}{x}\:\mathrm{d}x -\frac{1}{2}\int _0^1\frac{\arcsin ^2\left(x\right)\ln ^2\left(\frac{x}{1-x}\right)}{x}\:\mathrm{d}x 2 I \int _0^1\frac{\arcsin ^2\left(x\right)\operatorname{Li}_2\left(x\right)}{x}\:\mathrm{d}x,\:\int _0^1\frac{\arcsin ^2\left(x\right)\operatorname{Li}_2\left(1-x\right)}{x}\:\mathrm{d}x","['real-analysis', 'calculus', 'definite-integrals', 'harmonic-numbers', 'polylogarithm']"
9,The Ñ function. Where is this chaotic tweak of the geometric series continuous?,The Ñ function. Where is this chaotic tweak of the geometric series continuous?,,"Let us consider the function $\nu:(-1,1)\to[-1,1]$ defined as follows $$\nu(x):=\begin{cases}0 & \text{ if } x=0\\ \frac{1}{\left\lfloor \frac{1}{x}+\frac{1}{2}\right\rfloor} & \text{ if } x\neq0\end{cases}$$ noticing that, since $\lfloor t+\frac{1}{2}\rfloor$ is the rounding function, it is the case that $\nu(x)\approx \frac{1}{x^{-1}}=x=\text{Id}(x)$ . So we might see this function as a tweak function that takes every number of $(-1,1)$ to its closest one in $S:=\{0\}\cup\{\frac{1}{n}:n\in\mathbb{Z}^*\}$ . But this function alone is not that much of a deal. For instance, its continuity is quite easy to study. It is continuous everywhere except at the points $(-1,1)\cap\{\frac{2}{n}: n\text{ odd}\}$ since the rounding function jumps precisely when $t=n+\frac{1}{2}$ . Now, I wanted to examine what would happen if I tweaked known well behaved series with this function. For this reason, I considered the series $\tilde N:(-1,1)\to\mathbb{R}$ defined as $\tilde N(x)=\sum_{n=0}^\infty \nu(x^n)$ which looks like this . $ \ \ \ \ \  \ \ \ \ \  \ \ \ \ \  \ \ \ \ \ $ My main questions are on how to study its continuity. More precisely: Is this function continuous at $S=\{0\}\cup\{\frac{1}{n}:n\in\mathbb{Z}^*\}$ ? Is this function discontinuous at $(-1,1)\cap\{\frac{2}{n}:n \text{ odd}\}$ ? Precisely when is it continuous/discontinuous? What is happening where the domain is negative? (Reminds me of the logistic map ) Though the $3$ rd one is the one that I really care for. Now I will prove the things that I know although they aren't that much. $\color{green}{\bf{Proposition\ 1.}}$ $\tilde N(x)$ converges absolutely on $(-1,1)$ . Hence, it is well defined. Proof. Notice that it can be easily checked that $|\nu(x)|\leq\frac{3}{2}|x|$ . Because of this, we know that $\sum_{n=0}^\infty |\nu(x^n)|\leq \frac{3}{2}\frac{1}{1-|x|}$ . $\quad\Box$ $\color{green}{\bf{Proposition\ 2.}}$ If we consider $N(x)=\frac{1}{1-x}$ , then $\forall x\in S:\tilde N(x)=N(x)$ . Proof. If $x=0$ then $\tilde N(0)=1=N(0)$ . Now, if $x=\frac{1}{m}$ for some $m\in\mathbb{Z}^*$ then the series becomes $$\tilde N\left(\frac{1}{m}\right)=\sum_{n=0}^\infty\frac{1}{\lfloor m^n+1/2\rfloor}=\sum_{n=0}^\infty\frac{1}{m^n}=N\left(\frac{1}{m}\right)\quad\Box$$ Update: It is quite easy to show that $\tilde N$ is continuous at $0$ by bounding the $\nu$ function. $\color{seagreen}{\bf{Lemma\ 1.}}$ If $x\in(-1,1)$ is such that $|x|<\frac{2}{k}$ for some odd $k\geq1$ then $\frac{k}{k+1}|x|\leq|\nu(x)|\leq\frac{k+2}{k+1}|x|$ . Proof. This can be easily checked by making use of known inequalities of the floor function. $$|x|<\frac{2}{k}\Rightarrow \color{blue}{\frac{k}{k+1}|x|}\leq\color{purple}{|\nu(x)|} \leq \color{red}{\frac{k+2}{k+1}|x|}$$ $\color{green}{\bf{Proposition\ 3.}}$ The $\tilde N(x)$ function is continuous at $x=0$ . Proof. There are several ways of formalizing it but the idea is to make use of the above lemma since it is saying that $\nu(x)\sim x$ as $x\to0$ . We will examine $\lim_{x\to0^+}\tilde N(x)$ and $\lim_{x\to0^-}\tilde N(x)$ to see that they are both equal to $1=\tilde N(0)$ and, consequently, to see that $\tilde N$ is indeed continuous at $0$ . Suppose that $0<x<\frac{2}{k}$ for odd $k\geq3$ . Then we know that $\frac{k}{k+1}x\leq\nu(x)\leq\frac{k+2}{k+1}x$ which implies that, since $\frac{k}{k+1}\leq\nu(1)\leq\frac{k+2}{k+1}$ is trivially true (as $\nu(1)=1$ ) and $\forall n\geq1:x^n\leq x<2/k$ (as $0<x<1$ ), the following inequalities hold $$\underline{\frac{k}{k+1}}\leq\frac{k}{k+1}\frac{1}{1-x}\leq\underline{\tilde N(x)}\leq\frac{k+2}{k+1}\frac{1}{1-x}\leq\underline{\frac{k+2}{k+1}\frac{1}{1-2/k}}$$ which implies $\lim_{x\to0^+}\tilde N(x)=1$ by the squeeze theorem since both bounds tend to $1$ as $k\to+\infty$ . The limit from the left, however, is somewhat trickier. But not that much, to be fair. The idea is to notice that $\text{sign}(\nu(x))=\text{sign}(x)$ . Thus, if $x<0$ , we have that $\nu(x^n)=(-1)^n|\nu(x^n)|$ . With this in mind, let's study the other limit. Suppose now that $-\frac{2}{k}<x<0$ for odd $k\geq3$ . Let's try to bound $\tilde N(x)$ with bounds that only depend on $k$ that tend to $1$ as $k\to+\infty$ . As an upper bound, we have \begin{align*} \tilde N(x) &=\sum_{n=0}^\infty|\nu(x^{2n})|-\sum_{n=0}^\infty|\nu(x^{2n+1})|\leq\sum_{n=0}^\infty|\nu(x^{2n})|\leq\frac{k+2}{k+1}\frac{1}{1-x^2}\leq\frac{k+2}{k+1}\frac{1}{1-4/k^2} \end{align*} and, as a lower bound, we have \begin{align*} \tilde N(x) &=\sum_{n=0}^\infty|\nu(x^{2n})|-\sum_{n=0}^\infty|\nu(x^{2n+1})|\geq\frac{k}{k+1}\frac{1}{1-x^2}-\frac{k+2}{k+1}\frac{|x|}{1-x^2}\\ &=\frac{k-(k+2)|x|}{(k+1)(1-x^2)}\geq\frac{k-(k+2)\frac{2}{k}}{(k+1)(1-x^2)}\geq\frac{k-2-\frac{4}{k}}{k+1}\end{align*} which, finally, implies that $\lim_{x\to0^-}\tilde N(x)=1$ as well. $\quad\Box$","Let us consider the function defined as follows noticing that, since is the rounding function, it is the case that . So we might see this function as a tweak function that takes every number of to its closest one in . But this function alone is not that much of a deal. For instance, its continuity is quite easy to study. It is continuous everywhere except at the points since the rounding function jumps precisely when . Now, I wanted to examine what would happen if I tweaked known well behaved series with this function. For this reason, I considered the series defined as which looks like this . My main questions are on how to study its continuity. More precisely: Is this function continuous at ? Is this function discontinuous at ? Precisely when is it continuous/discontinuous? What is happening where the domain is negative? (Reminds me of the logistic map ) Though the rd one is the one that I really care for. Now I will prove the things that I know although they aren't that much. converges absolutely on . Hence, it is well defined. Proof. Notice that it can be easily checked that . Because of this, we know that . If we consider , then . Proof. If then . Now, if for some then the series becomes Update: It is quite easy to show that is continuous at by bounding the function. If is such that for some odd then . Proof. This can be easily checked by making use of known inequalities of the floor function. The function is continuous at . Proof. There are several ways of formalizing it but the idea is to make use of the above lemma since it is saying that as . We will examine and to see that they are both equal to and, consequently, to see that is indeed continuous at . Suppose that for odd . Then we know that which implies that, since is trivially true (as ) and (as ), the following inequalities hold which implies by the squeeze theorem since both bounds tend to as . The limit from the left, however, is somewhat trickier. But not that much, to be fair. The idea is to notice that . Thus, if , we have that . With this in mind, let's study the other limit. Suppose now that for odd . Let's try to bound with bounds that only depend on that tend to as . As an upper bound, we have and, as a lower bound, we have which, finally, implies that as well.","\nu:(-1,1)\to[-1,1] \nu(x):=\begin{cases}0 & \text{ if } x=0\\ \frac{1}{\left\lfloor \frac{1}{x}+\frac{1}{2}\right\rfloor} & \text{ if } x\neq0\end{cases} \lfloor t+\frac{1}{2}\rfloor \nu(x)\approx \frac{1}{x^{-1}}=x=\text{Id}(x) (-1,1) S:=\{0\}\cup\{\frac{1}{n}:n\in\mathbb{Z}^*\} (-1,1)\cap\{\frac{2}{n}: n\text{ odd}\} t=n+\frac{1}{2} \tilde N:(-1,1)\to\mathbb{R} \tilde N(x)=\sum_{n=0}^\infty \nu(x^n)  \ \ \ \ \  \ \ \ \ \  \ \ \ \ \  \ \ \ \ \  S=\{0\}\cup\{\frac{1}{n}:n\in\mathbb{Z}^*\} (-1,1)\cap\{\frac{2}{n}:n \text{ odd}\} 3 \color{green}{\bf{Proposition\ 1.}} \tilde N(x) (-1,1) |\nu(x)|\leq\frac{3}{2}|x| \sum_{n=0}^\infty |\nu(x^n)|\leq \frac{3}{2}\frac{1}{1-|x|} \quad\Box \color{green}{\bf{Proposition\ 2.}} N(x)=\frac{1}{1-x} \forall x\in S:\tilde N(x)=N(x) x=0 \tilde N(0)=1=N(0) x=\frac{1}{m} m\in\mathbb{Z}^* \tilde N\left(\frac{1}{m}\right)=\sum_{n=0}^\infty\frac{1}{\lfloor m^n+1/2\rfloor}=\sum_{n=0}^\infty\frac{1}{m^n}=N\left(\frac{1}{m}\right)\quad\Box \tilde N 0 \nu \color{seagreen}{\bf{Lemma\ 1.}} x\in(-1,1) |x|<\frac{2}{k} k\geq1 \frac{k}{k+1}|x|\leq|\nu(x)|\leq\frac{k+2}{k+1}|x| |x|<\frac{2}{k}\Rightarrow \color{blue}{\frac{k}{k+1}|x|}\leq\color{purple}{|\nu(x)|} \leq \color{red}{\frac{k+2}{k+1}|x|} \color{green}{\bf{Proposition\ 3.}} \tilde N(x) x=0 \nu(x)\sim x x\to0 \lim_{x\to0^+}\tilde N(x) \lim_{x\to0^-}\tilde N(x) 1=\tilde N(0) \tilde N 0 0<x<\frac{2}{k} k\geq3 \frac{k}{k+1}x\leq\nu(x)\leq\frac{k+2}{k+1}x \frac{k}{k+1}\leq\nu(1)\leq\frac{k+2}{k+1} \nu(1)=1 \forall n\geq1:x^n\leq x<2/k 0<x<1 \underline{\frac{k}{k+1}}\leq\frac{k}{k+1}\frac{1}{1-x}\leq\underline{\tilde N(x)}\leq\frac{k+2}{k+1}\frac{1}{1-x}\leq\underline{\frac{k+2}{k+1}\frac{1}{1-2/k}} \lim_{x\to0^+}\tilde N(x)=1 1 k\to+\infty \text{sign}(\nu(x))=\text{sign}(x) x<0 \nu(x^n)=(-1)^n|\nu(x^n)| -\frac{2}{k}<x<0 k\geq3 \tilde N(x) k 1 k\to+\infty \begin{align*}
\tilde N(x) &=\sum_{n=0}^\infty|\nu(x^{2n})|-\sum_{n=0}^\infty|\nu(x^{2n+1})|\leq\sum_{n=0}^\infty|\nu(x^{2n})|\leq\frac{k+2}{k+1}\frac{1}{1-x^2}\leq\frac{k+2}{k+1}\frac{1}{1-4/k^2}
\end{align*} \begin{align*}
\tilde N(x) &=\sum_{n=0}^\infty|\nu(x^{2n})|-\sum_{n=0}^\infty|\nu(x^{2n+1})|\geq\frac{k}{k+1}\frac{1}{1-x^2}-\frac{k+2}{k+1}\frac{|x|}{1-x^2}\\
&=\frac{k-(k+2)|x|}{(k+1)(1-x^2)}\geq\frac{k-(k+2)\frac{2}{k}}{(k+1)(1-x^2)}\geq\frac{k-2-\frac{4}{k}}{k+1}\end{align*} \lim_{x\to0^-}\tilde N(x)=1 \quad\Box","['real-analysis', 'sequences-and-series', 'continuity']"
10,A tighter bound of Vitali covering lemma for $\Bbb R^2$: $\lambda(V)\geq\frac{\lambda \left( \bigcup D_i \right) }{4}$,A tighter bound of Vitali covering lemma for :,\Bbb R^2 \lambda(V)\geq\frac{\lambda \left( \bigcup D_i \right) }{4},"I need help proving the following statement: Let $\{D_i\}$ be a finite colection of disks on $\mathbb{R}^2$ , $\lambda$ the Lebesgue measure on $\mathbb{R}^2$ . Show that there exists a subcollection of disjoint disks in the collection such that their union $V$ satisfies: $$ \lambda(V)\geq\frac{\lambda \left( \bigcup D_i \right) }{4} . $$ So far the only thing I got is that it suffices to show the theorem for a set of disks that are ""clustered"" in the sense that there is an ennumeration of the $D_i$ 's such that $$D_i\cap D_{i+1}≠\emptyset$$ And that the theorem is obviously true for up to 4 disks.","I need help proving the following statement: Let be a finite colection of disks on , the Lebesgue measure on . Show that there exists a subcollection of disjoint disks in the collection such that their union satisfies: So far the only thing I got is that it suffices to show the theorem for a set of disks that are ""clustered"" in the sense that there is an ennumeration of the 's such that And that the theorem is obviously true for up to 4 disks.","\{D_i\} \mathbb{R}^2 \lambda \mathbb{R}^2 V 
\lambda(V)\geq\frac{\lambda \left( \bigcup D_i \right) }{4} .
 D_i D_i\cap D_{i+1}≠\emptyset","['real-analysis', 'geometry', 'analysis', 'measure-theory', 'geometric-measure-theory']"
11,"Is $\ \{x(t) \in L^1[0,1] \mid 0 \le x(t) \le t^2\} $ a compact in $ L^1[0,1]$?",Is  a compact in ?,"\ \{x(t) \in L^1[0,1] \mid 0 \le x(t) \le t^2\}   L^1[0,1]","In my functional analysis class I was given a problem: Is $\ \{x(t) \in L^1[0,1] \mid  0 \le x(t) \le t^2\} $ a compact in $ L^1[0,1]?$ $L^1[0,1]$ is Banach space and I know that subset of a Banach space is compact if and only if it is closed and totally bounded. And also I know that for banach spaces totaly bounded is equivalent to relative compactness. So I need to check if my subset is closed and relatively compact or not. But I don’t understand how to do this by definition. In our lectures we proved criterions of relative compactness for some banach spaces (for example: $l_2$ , $C[a,b]$ ), but haven’t proved and even discussed criterion for $L^p[0,1]$ , where $p \geq 1$ . Nevertheless, in the functional analysis exercise book of professors of my deparment I found a criterion for relative compactness for $L^p[0,1]$ which was proved by Kolmogorov (1931) and M. Riesz (1933): Set $M$ in $L^p[0,1], \ 1 \leq p < \infty$ is relatively compact if and only if and only if $\ \forall\varepsilon > 0 \ \exists \delta>0 \ \forall x \in [0,1] \ \forall h \in [0,\delta] $ $$ \displaystyle \left (\int \limits_0^{1-h} |x(t+h)-x(t)|^p dt \right)^{\frac{1}{p}} \leq \varepsilon $$ But we haven’t discussed this theorem, so it is better to solve this problem by definition. But I will be grateful if you give me a source where the proof of this theorem is written. So I appreciate any hint about how to solve this problem using only definitions and criterions which I mentioned in the second paragraph.","In my functional analysis class I was given a problem: Is a compact in is Banach space and I know that subset of a Banach space is compact if and only if it is closed and totally bounded. And also I know that for banach spaces totaly bounded is equivalent to relative compactness. So I need to check if my subset is closed and relatively compact or not. But I don’t understand how to do this by definition. In our lectures we proved criterions of relative compactness for some banach spaces (for example: , ), but haven’t proved and even discussed criterion for , where . Nevertheless, in the functional analysis exercise book of professors of my deparment I found a criterion for relative compactness for which was proved by Kolmogorov (1931) and M. Riesz (1933): Set in is relatively compact if and only if and only if But we haven’t discussed this theorem, so it is better to solve this problem by definition. But I will be grateful if you give me a source where the proof of this theorem is written. So I appreciate any hint about how to solve this problem using only definitions and criterions which I mentioned in the second paragraph.","\ \{x(t) \in L^1[0,1] \mid  0 \le x(t) \le t^2\}   L^1[0,1]? L^1[0,1] l_2 C[a,b] L^p[0,1] p \geq 1 L^p[0,1] M L^p[0,1], \ 1 \leq p < \infty \ \forall\varepsilon > 0 \ \exists \delta>0 \ \forall x \in [0,1] \ \forall h \in [0,\delta]   \displaystyle \left (\int \limits_0^{1-h} |x(t+h)-x(t)|^p dt \right)^{\frac{1}{p}} \leq \varepsilon ","['real-analysis', 'functional-analysis', 'analysis']"
12,Building $\textit{techniques}$ (as opposed to $\textit{concepts}$) for advanced complex analysis,Building  (as opposed to ) for advanced complex analysis,\textit{techniques} \textit{concepts},"I'm struggling with the exercises in Stein and Shakarchi's Complex Analysis , and believe what I'm missing is advanced techniques (as opposed to concepts ) in real analysis. The difficulty I consistently have is not with the new material introduced regarding complex analysis, but with techniques assumed (not discussed or presented) of real analysis.  Examples: Replace $\frac{1}{z-a}$ with an infinite geometric series Replace $\sin \theta$ with $2\theta/\pi$ using Jordan's inequality Replace $1 - e^{iz}$ with the power series of $e$ which tends to $-iz$ as $z \to 0$ ( assumed on p.45 of text, explained on math.SE, though I can't find the link) and many other similar cases All these examples have in common: The proof or problem applies a non-trivial technique to go from one step to the next, usually to replace an intractable expression with a tractable one The technique is not discussed in the text The technique applies to real analysis (not complex) The technique is a technique* , as opposed to understanding a concept How, then, can I build my real analysis techniques so that I can follow Stein and Sharakachi? I know real analysis well enough to solve all the problems on a MIT OCW real analysis final (although sometimes with difficulty), but not well enough to solve the problems on a Rudin based course final .  This might suggest to learn real analysis again, using Rudin.  However, looking at the Rudin text and course, I don't see them teaching these advanced techniques but rather more advanced concepts , such as point-set topology and metric spaces beyond the line. What course should I use to learn the techniques needed to approach Stein and Shakarachi, then? Or should I simply work through Stein and Shakarachi, and use  it as a motivator to look up (and learn) new techniques? *By technique I mean a means of simplifying an expression that does not follow directly from the underlying definitions and theorems, but instead is a creative application from elsewhere (such as those discussed here .)  I distinguish technique from concept , which means understanding the objects, definitions, and theorems.  Both of them, of course, are crucial. I'm not the first person to raise this issue .  But the recommendation given there (to first learn complex variable calculus before learning the proofs) seems to miss the point, as the issue, as shown by those examples, is not with complex variables but with techniques from real analysis.","I'm struggling with the exercises in Stein and Shakarchi's Complex Analysis , and believe what I'm missing is advanced techniques (as opposed to concepts ) in real analysis. The difficulty I consistently have is not with the new material introduced regarding complex analysis, but with techniques assumed (not discussed or presented) of real analysis.  Examples: Replace with an infinite geometric series Replace with using Jordan's inequality Replace with the power series of which tends to as ( assumed on p.45 of text, explained on math.SE, though I can't find the link) and many other similar cases All these examples have in common: The proof or problem applies a non-trivial technique to go from one step to the next, usually to replace an intractable expression with a tractable one The technique is not discussed in the text The technique applies to real analysis (not complex) The technique is a technique* , as opposed to understanding a concept How, then, can I build my real analysis techniques so that I can follow Stein and Sharakachi? I know real analysis well enough to solve all the problems on a MIT OCW real analysis final (although sometimes with difficulty), but not well enough to solve the problems on a Rudin based course final .  This might suggest to learn real analysis again, using Rudin.  However, looking at the Rudin text and course, I don't see them teaching these advanced techniques but rather more advanced concepts , such as point-set topology and metric spaces beyond the line. What course should I use to learn the techniques needed to approach Stein and Shakarachi, then? Or should I simply work through Stein and Shakarachi, and use  it as a motivator to look up (and learn) new techniques? *By technique I mean a means of simplifying an expression that does not follow directly from the underlying definitions and theorems, but instead is a creative application from elsewhere (such as those discussed here .)  I distinguish technique from concept , which means understanding the objects, definitions, and theorems.  Both of them, of course, are crucial. I'm not the first person to raise this issue .  But the recommendation given there (to first learn complex variable calculus before learning the proofs) seems to miss the point, as the issue, as shown by those examples, is not with complex variables but with techniques from real analysis.",\frac{1}{z-a} \sin \theta 2\theta/\pi 1 - e^{iz} e -iz z \to 0,"['real-analysis', 'complex-analysis', 'soft-question']"
13,"Both $f(x,\cdot)$ and $f(\cdot,y)$ $\mathcal{A}$ measurable,but $f(x,y)$ not $\sigma \left ( \mathcal{A}\times \mathcal{A}\right )$ measurable","Both  and   measurable,but  not  measurable","f(x,\cdot) f(\cdot,y) \mathcal{A} f(x,y) \sigma \left ( \mathcal{A}\times \mathcal{A}\right )","From Richard Bass' Real Analysis for Graduate Students Version 3.1  p.94: $\textbf{Example 11.5}$ There exists a set $X$ together with a partial order $""\leq""$ such that $X$ is uncountable but for any $y \in X$ , the set $\{x\in X: x\leq y\}$ is countable.  The $\sigma$ -algebra is the collection of subsets $A$ of $X$ such that either $A$ or $A^c$ is countable.  Define $\mu$ on $X$ by $\mu(A) = 0$ if $A$ is countable and 1 if $A$ is uncountable.  Define $f$ on $X\times X$ by $f(x,y)=1$ if $x\leq y$ and zero otherwise.  Then $\int \int f(x,y)\, dy\, dx = 1$ but $\int \int f(x,y)\, dx \, dy = 0$ .The reason there is no contradiction is that f is not measurable with respect to the product $\sigma$ -algebra. I don't understand why $f$ is not measurable with respect to the product $\sigma$ -algebra. Let $\mathcal{A}$ denote the $\sigma$ -algebra the collection of subsets $A$ of $X$ such that either $A$ or $A^c$ is countable. Actually,How can I prove either $\left\{\left (x,y \right )\in X\times X\right | x\le y  \}\notin \sigma \left (  \mathcal{A}\times \mathcal{A}\right )$ or $\left\{\left (x,y \right )\in X\times X\right | x> y  \}\notin \sigma \left (  \mathcal{A}\times \mathcal{A}\right )?$","From Richard Bass' Real Analysis for Graduate Students Version 3.1  p.94: There exists a set together with a partial order such that is uncountable but for any , the set is countable.  The -algebra is the collection of subsets of such that either or is countable.  Define on by if is countable and 1 if is uncountable.  Define on by if and zero otherwise.  Then but .The reason there is no contradiction is that f is not measurable with respect to the product -algebra. I don't understand why is not measurable with respect to the product -algebra. Let denote the -algebra the collection of subsets of such that either or is countable. Actually,How can I prove either or","\textbf{Example 11.5} X ""\leq"" X y \in X \{x\in X: x\leq y\} \sigma A X A A^c \mu X \mu(A) = 0 A A f X\times X f(x,y)=1 x\leq y \int \int f(x,y)\, dy\, dx = 1 \int \int f(x,y)\, dx \, dy = 0 \sigma f \sigma \mathcal{A} \sigma A X A A^c \left\{\left (x,y \right )\in X\times X\right | x\le y  \}\notin \sigma \left (  \mathcal{A}\times \mathcal{A}\right ) \left\{\left (x,y \right )\in X\times X\right | x> y  \}\notin \sigma \left (  \mathcal{A}\times \mathcal{A}\right )?","['real-analysis', 'measure-theory']"
14,How to go from Radon–Nikodym derivative to classical derivative in change of variables formula of p.d.f.?,How to go from Radon–Nikodym derivative to classical derivative in change of variables formula of p.d.f.?,,"Let $X$ be an absolutely continuous real-valued random variable whose  distribution is $\mu_X$ and whose p.d.f. is $p_X$ . Let $f:\mathbb R \to \mathbb R$ be differentiable such that $f'(t)>0$ for all $t\in \mathbb R$ . This implies $f$ is strictly increasing. Let $Y := f(X)$ . Clearly, the distribution $\mu_Y$ of $Y$ is absolutely continuous w.r.t. Lebesgue measure $\lambda$ . Let $F_Y,p_Y$ be the c.d.f. and p.d.f. of $Y$ respectively. Clearly, $\mu_Y$ is absolutely continuous w.r.t. $\mu_X$ . According to this Wikipedia page , $$ p_Y := \frac{\mathrm d  \mu_Y}{\mathrm d  \lambda} = \frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} \frac{\mathrm d  \mu_X}{\mathrm d  \lambda}. $$ Here $\frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X}$ is the Radon–Nikodym derivative of $\mu_Y$ w.r.t. $\mu_X$ . Similarly, $\frac{\mathrm d  \mu_X}{\mathrm d  \lambda}$ is the Radon–Nikodym derivative of $\mu_X$ w.r.t. $\lambda$ . At page $14$ of this lecture note , the author mentioned that $$ p_Y (t) = \frac{\mathrm d F_Y (t)}{\mathrm d t} = \frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} \frac{\mathrm d  f^{-1} (t)}{\mathrm d t} =\frac{p_X (f^{-1} (t))}{f'(f^{-1} (t))}. $$ Clearly, the map $F_\lambda:\mathbb R \to \mathbb R, t \mapsto t$ is the corresponding c.d.f. of $\lambda$ . Obviously, the p.d.f. of $\lambda$ is $p_\lambda:\mathbb R \to \mathbb R, t \mapsto 1$ . It seems to me the author meant by $\frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t}$ the value of $p_X :=\frac{\mathrm d  \mu_X}{\mathrm d  \lambda}$ at $f^{-1} (t)$ , i.e., $$ \frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} = \frac{\mathrm d  \mu_X}{\mathrm d  \lambda} (f^{-1} (t)) =p_X (f^{-1} (t)). $$ Could you elaborate on how to transfer from the Radon–Nikodym derivative $\frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X}$ to the classical derivative $\frac{\mathrm d  f^{-1} (t)}{\mathrm d t}$ , i.e., $$ \frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} (t) = \frac{\mathrm d  f^{-1}}{\mathrm d t} (t)? $$","Let be an absolutely continuous real-valued random variable whose  distribution is and whose p.d.f. is . Let be differentiable such that for all . This implies is strictly increasing. Let . Clearly, the distribution of is absolutely continuous w.r.t. Lebesgue measure . Let be the c.d.f. and p.d.f. of respectively. Clearly, is absolutely continuous w.r.t. . According to this Wikipedia page , Here is the Radon–Nikodym derivative of w.r.t. . Similarly, is the Radon–Nikodym derivative of w.r.t. . At page of this lecture note , the author mentioned that Clearly, the map is the corresponding c.d.f. of . Obviously, the p.d.f. of is . It seems to me the author meant by the value of at , i.e., Could you elaborate on how to transfer from the Radon–Nikodym derivative to the classical derivative , i.e.,","X \mu_X p_X f:\mathbb R \to \mathbb R f'(t)>0 t\in \mathbb R f Y := f(X) \mu_Y Y \lambda F_Y,p_Y Y \mu_Y \mu_X 
p_Y := \frac{\mathrm d  \mu_Y}{\mathrm d  \lambda} = \frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} \frac{\mathrm d  \mu_X}{\mathrm d  \lambda}.
 \frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} \mu_Y \mu_X \frac{\mathrm d  \mu_X}{\mathrm d  \lambda} \mu_X \lambda 14 
p_Y (t) = \frac{\mathrm d F_Y (t)}{\mathrm d t} = \frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} \frac{\mathrm d  f^{-1} (t)}{\mathrm d t} =\frac{p_X (f^{-1} (t))}{f'(f^{-1} (t))}.
 F_\lambda:\mathbb R \to \mathbb R, t \mapsto t \lambda \lambda p_\lambda:\mathbb R \to \mathbb R, t \mapsto 1 \frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} p_X :=\frac{\mathrm d  \mu_X}{\mathrm d  \lambda} f^{-1} (t) 
\frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} = \frac{\mathrm d  \mu_X}{\mathrm d  \lambda} (f^{-1} (t)) =p_X (f^{-1} (t)).
 \frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} \frac{\mathrm d  f^{-1} (t)}{\mathrm d t} 
\frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} (t) = \frac{\mathrm d  f^{-1}}{\mathrm d t} (t)?
","['real-analysis', 'probability-theory', 'measure-theory', 'probability-distributions', 'radon-nikodym']"
15,"$f'$ decreasing, is $f'$ continuous?","decreasing, is  continuous?",f' f',"Suppose $f:(0,\infty)\rightarrow \mathbb R$ is positive and differentiable and $f'$ is decreasing to $0$ . Is $f'$ continuous? I know that $f'$ has to have intermediate value property so it can not have any discontinuous jumps, but is the assumption that $f'$ is decreasing sufficient for the continuity? Proof or counterexample would be great","Suppose is positive and differentiable and is decreasing to . Is continuous? I know that has to have intermediate value property so it can not have any discontinuous jumps, but is the assumption that is decreasing sufficient for the continuity? Proof or counterexample would be great","f:(0,\infty)\rightarrow \mathbb R f' 0 f' f' f'","['real-analysis', 'calculus', 'derivatives']"
16,Closed form of negative root of $x+x^2+x^4+x^8+x^{16}+...=0$,Closed form of negative root of,x+x^2+x^4+x^8+x^{16}+...=0,"I was investiging the series $\sum_{k=0}^\infty{x^{(2^k)}}$ where $x\in\mathbb{R}$ , and came up with this question: Is there a closed form of the negative root of $\sum_{k=0}^\infty{x^{(2^k)}}=0$ ? On Desmos, taking the first $5$ terms, and taking the first $1001$ terms, both give the negative root as $-0.6586$ to $4$ decimal places. Here are my thoughts. According to this , ""there is no solution in radicals to general polynomial equations of degree five or higher with arbitrary coefficients"". So if we take the first five terms of the series, I would not expect a closed form of the negative root. But, based on my experience with power series, if we take the limit as the number of terms approaches $\infty$ , I would be more inclined to believe that a closed form might exist.","I was investiging the series where , and came up with this question: Is there a closed form of the negative root of ? On Desmos, taking the first terms, and taking the first terms, both give the negative root as to decimal places. Here are my thoughts. According to this , ""there is no solution in radicals to general polynomial equations of degree five or higher with arbitrary coefficients"". So if we take the first five terms of the series, I would not expect a closed form of the negative root. But, based on my experience with power series, if we take the limit as the number of terms approaches , I would be more inclined to believe that a closed form might exist.",\sum_{k=0}^\infty{x^{(2^k)}} x\in\mathbb{R} \sum_{k=0}^\infty{x^{(2^k)}}=0 5 1001 -0.6586 4 \infty,"['real-analysis', 'sequences-and-series', 'polynomials', 'power-series', 'closed-form']"
17,Absolutely continuous push forward measure,Absolutely continuous push forward measure,,"Consider a measure space $(E,\mathcal{B}(E),\mu)$ and let $f:E\to E$ be a measurable mapping. The push forward of the measure $\mu$ by the mapping $f$ is defined to be $$ f_\#\mu(A) = \mu(f^{-1}(A)) \quad \text{for all } A\in\mathcal{B}(E). $$ My question is the following: what regularity do we need to assume on $f$ to make sure that $f_\#\mu\ll\mu$ ? I was writing this question when I found this other post that helped a lot. But now I am wondering, for the Lebesgue measure on $\mathbb{R}^n$ , instead of locally Lipschitz mappings, could we relax this condition to locally Hölder with some $\alpha \in (0,1)$ ? Thanks in advance!","Consider a measure space and let be a measurable mapping. The push forward of the measure by the mapping is defined to be My question is the following: what regularity do we need to assume on to make sure that ? I was writing this question when I found this other post that helped a lot. But now I am wondering, for the Lebesgue measure on , instead of locally Lipschitz mappings, could we relax this condition to locally Hölder with some ? Thanks in advance!","(E,\mathcal{B}(E),\mu) f:E\to E \mu f  f_\#\mu(A) = \mu(f^{-1}(A)) \quad \text{for all } A\in\mathcal{B}(E).  f f_\#\mu\ll\mu \mathbb{R}^n \alpha \in (0,1)","['real-analysis', 'functional-analysis', 'measure-theory']"
18,Study the convergence of $\sum_{n=1}^{\infty}\frac{\sin (n\sqrt{n})}{\sqrt{n}}.$,Study the convergence of,\sum_{n=1}^{\infty}\frac{\sin (n\sqrt{n})}{\sqrt{n}}.,"I try to use the following result: If $f(x)\in C^1[1,+\infty)$ and $\displaystyle\int_1^{+\infty} |f'(x)|{\rm d}x$ is convergent, then $\displaystyle\sum_{n=1}^{\infty} f(n)$ has the same convergence or divergence as $\displaystyle\int_1^{+\infty} f(x) {\rm d}x$ . Now, define $f(x):=\dfrac{\sin x^{3/2}}{x^{1/2}}$ . Then \begin{align*}  \int_1^{+\infty}|f'(x)|{\rm d}x&=\int_1^{+\infty}\left|\frac{3}{2}\cos x^{3/2}-\frac{\sin x^{3/2}}{2x^{3/2}}\right|{\rm d}x\\ &=\int_1^{+\infty}\left|\frac{\cos x}{x^{1/3}}-\frac{\sin x}{3x^{4/3}}\right|{\rm d}x\\ &\ge \int_1^{+\infty}\frac{|\cos x|}{x^{1/3}}-\frac{|\sin x|}{3x^{4/3}}{\rm d}x=+\infty, \end{align*} which is not convergent.","I try to use the following result: If and is convergent, then has the same convergence or divergence as . Now, define . Then which is not convergent.","f(x)\in C^1[1,+\infty) \displaystyle\int_1^{+\infty} |f'(x)|{\rm d}x \displaystyle\sum_{n=1}^{\infty} f(n) \displaystyle\int_1^{+\infty} f(x) {\rm d}x f(x):=\dfrac{\sin x^{3/2}}{x^{1/2}} \begin{align*}  \int_1^{+\infty}|f'(x)|{\rm d}x&=\int_1^{+\infty}\left|\frac{3}{2}\cos x^{3/2}-\frac{\sin x^{3/2}}{2x^{3/2}}\right|{\rm d}x\\ &=\int_1^{+\infty}\left|\frac{\cos x}{x^{1/3}}-\frac{\sin x}{3x^{4/3}}\right|{\rm d}x\\ &\ge \int_1^{+\infty}\frac{|\cos x|}{x^{1/3}}-\frac{|\sin x|}{3x^{4/3}}{\rm d}x=+\infty, \end{align*}","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
19,For which values of $q$ is $\int_{\mathbb{R}^{2N}}\frac{|u(x)-u(y)|^q}{|x-y|^{N+sq}}dxdy$ is finite?,For which values of  is  is finite?,q \int_{\mathbb{R}^{2N}}\frac{|u(x)-u(y)|^q}{|x-y|^{N+sq}}dxdy,"Let $u=u(r)$ be radially symmetric, nonnegative and decreasing function. Let $s\in (0, 1)$ and $p, q\in\mathbb{R}$ such that $1<q<p$ and $ps<N$ with $N\in\mathbb{N}, N\ge 2$ . Assume that $u\in D^{s, p}(\mathbb{R}^N)$ , where $D^{s, p}(\mathbb{R}^N) = \left\lbrace u\in L^{Np/(N-sp)}(\mathbb{R}^N): \int_{\mathbb{R}^{2N}}\frac{|u(x)-u(y)|^p}{|x-y|^{N+sp}}dxdy <+\infty\right\rbrace.$ Assume also that two positive constants $c_1, c_2>0$ exist such that for all $r\ge 1$ : $$ \frac{c_1}{r^{(N-sp)/(p-1)}}\le u(r)\le \frac{c_2}{r^{(N-sp)/(p-1)}}.$$ I would like to find suitable value of $q$ which guarantee that $\int_{R^{2N}}\frac{|u(x)-u(y)|^q}{|x-y|^{N+sq}}dxdy <+\infty.$ This should be a result known for people working with nonlocal problems (fractional Laplacian).Any hint or reference about the solution of this problem is well appreciated. EDIT Using the above inequality, I obtain $$\int_{\mathbb{R}^{2N}}\frac{|u(x)-u(y)|^q}{|x-y|^{N+sq}}dxdy\le 2^{q-1}\int_{\mathbb{R}^{2N}} \left(\frac{c_1^q}{x^{(N-sp)q/(p-1)}}-\frac{c_2^q}{y^{(N-sp)q/(p-1)}}\right)\frac{1}{|x-y|^{N+sq}}dxdy.$$ I am adding a bounty. That result I am looking for should be something "" standard "". More information about that function $u$ and its properties are at page $11$ of the paper . Thank you.","Let be radially symmetric, nonnegative and decreasing function. Let and such that and with . Assume that , where Assume also that two positive constants exist such that for all : I would like to find suitable value of which guarantee that This should be a result known for people working with nonlocal problems (fractional Laplacian).Any hint or reference about the solution of this problem is well appreciated. EDIT Using the above inequality, I obtain I am adding a bounty. That result I am looking for should be something "" standard "". More information about that function and its properties are at page of the paper . Thank you.","u=u(r) s\in (0, 1) p, q\in\mathbb{R} 1<q<p ps<N N\in\mathbb{N}, N\ge 2 u\in D^{s, p}(\mathbb{R}^N) D^{s, p}(\mathbb{R}^N) = \left\lbrace u\in L^{Np/(N-sp)}(\mathbb{R}^N): \int_{\mathbb{R}^{2N}}\frac{|u(x)-u(y)|^p}{|x-y|^{N+sp}}dxdy <+\infty\right\rbrace. c_1, c_2>0 r\ge 1  \frac{c_1}{r^{(N-sp)/(p-1)}}\le u(r)\le \frac{c_2}{r^{(N-sp)/(p-1)}}. q \int_{R^{2N}}\frac{|u(x)-u(y)|^q}{|x-y|^{N+sq}}dxdy <+\infty. \int_{\mathbb{R}^{2N}}\frac{|u(x)-u(y)|^q}{|x-y|^{N+sq}}dxdy\le 2^{q-1}\int_{\mathbb{R}^{2N}} \left(\frac{c_1^q}{x^{(N-sp)q/(p-1)}}-\frac{c_2^q}{y^{(N-sp)q/(p-1)}}\right)\frac{1}{|x-y|^{N+sq}}dxdy. u 11","['real-analysis', 'calculus', 'functional-analysis', 'partial-differential-equations', 'reference-request']"
20,A curious limit: $\lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}\left[\left(\frac{n}{n+1-i}\right)\right]^{a}f(i) = c\sum\limits_{i\geq 1}f(i)$,A curious limit:,\lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}\left[\left(\frac{n}{n+1-i}\right)\right]^{a}f(i) = c\sum\limits_{i\geq 1}f(i),"I am trying to prove, for the general case whereby $\zeta(\cdot\,,\cdot)$ is the Hurwitz-Zeta function, and $a\in \mathbb{N}$ , that $$\mathcal{L} = \lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}\Big[\zeta(a,i)-\zeta(a,n+1)\Big] = 2\zeta(a-1),\quad (a>2)$$ Importantly, I was hoping to accomplish this without transitioning the limit of the sum to a Riemann integral. However, evaluating the limit is tricky because of the behavior of the factor $f_n=[n/(n+1-i)]$ since $i$ spans all the way to $n$ itself. I realized after a bit that $$\mathcal{L} = 2\zeta(a-1)=2\sum\limits_{i\geq 1}\sum\limits_{k\geq 0}\left(\frac{1}{k+i} \right)^a=2\sum\limits_{i\geq 1}\zeta(a,i)$$ where the summand corresponds to the main $i$ -dependent function in the original sum. Although I still don't know how to prove the limit rigorously, it made me wonder if it's true more generally. For example, let $g(a,i)=\exp(-ai)$ and $h(a,i)=(1/i!)^a$ then is it true that for some constant $c$ $$\lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}g(a,i)\stackrel{?}{=}c\sum\limits_{i\geq 1}g(a,i),\quad (a>0)$$ $$\lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}h(a,i)\stackrel{?}{=}c\sum\limits_{i\geq 1}h(a,i),\quad (a>0)$$ Numerical evaluations show that $c$ is most likely $1$ in both of these cases. Assuming $\lim\limits{n\to\infty} f_n \sim 1$ , then the limits with $g$ and $h$ make sense. However why doesn't the first limit I proposed follow the same convention and instead have $c=2$ ? It appears that for $0<k<a$ , factor $c$ is reduced back to $1$ (at least by numerical evaluations). $$\lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{k}\Big[\zeta(a,i)-\zeta(a,n+1)\Big] \stackrel{?}{=} \zeta(a-1),\quad (a>2)$$ I'm wondering how you guys would approach any of these limits, since I don't have a convincing proof for any of them. Potentially would also like to know how/why the exponent on $f_n$ is able to change things in the first limit example. Please share your thoughts!","I am trying to prove, for the general case whereby is the Hurwitz-Zeta function, and , that Importantly, I was hoping to accomplish this without transitioning the limit of the sum to a Riemann integral. However, evaluating the limit is tricky because of the behavior of the factor since spans all the way to itself. I realized after a bit that where the summand corresponds to the main -dependent function in the original sum. Although I still don't know how to prove the limit rigorously, it made me wonder if it's true more generally. For example, let and then is it true that for some constant Numerical evaluations show that is most likely in both of these cases. Assuming , then the limits with and make sense. However why doesn't the first limit I proposed follow the same convention and instead have ? It appears that for , factor is reduced back to (at least by numerical evaluations). I'm wondering how you guys would approach any of these limits, since I don't have a convincing proof for any of them. Potentially would also like to know how/why the exponent on is able to change things in the first limit example. Please share your thoughts!","\zeta(\cdot\,,\cdot) a\in \mathbb{N} \mathcal{L} = \lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}\Big[\zeta(a,i)-\zeta(a,n+1)\Big] = 2\zeta(a-1),\quad (a>2) f_n=[n/(n+1-i)] i n \mathcal{L} = 2\zeta(a-1)=2\sum\limits_{i\geq 1}\sum\limits_{k\geq 0}\left(\frac{1}{k+i} \right)^a=2\sum\limits_{i\geq 1}\zeta(a,i) i g(a,i)=\exp(-ai) h(a,i)=(1/i!)^a c \lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}g(a,i)\stackrel{?}{=}c\sum\limits_{i\geq 1}g(a,i),\quad (a>0) \lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}h(a,i)\stackrel{?}{=}c\sum\limits_{i\geq 1}h(a,i),\quad (a>0) c 1 \lim\limits{n\to\infty} f_n \sim 1 g h c=2 0<k<a c 1 \lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{k}\Big[\zeta(a,i)-\zeta(a,n+1)\Big] \stackrel{?}{=} \zeta(a-1),\quad (a>2) f_n","['real-analysis', 'limits', 'summation', 'riemann-zeta']"
21,"Existence of a minimal continuous map on $S^1 \times [0, 1]$",Existence of a minimal continuous map on,"S^1 \times [0, 1]","First please see the question: Let $X = S^1 \times [0, 1]$ . Does there exist a continuous map $f: X \rightarrow X$ satisfying, for any point $x \in X$ , $\{f^{(n)}(x)\}_{n=1}^\infty$ is dense in $X$ ? Here we denote $f^{(n)}=f \circ f \circ ... \circ f$ by the composition of $f$ for $n$ times. (EDIT: Here ""for any point"" means ""for all points"", "" $\forall$ "") Note that if $X = S^1 \times S^1$ is the torus, then the answer is ""yes"": just let $f(a, b)=(ae^{2\pi ip}, be^{2 \pi iq})$ where $p, q$ are rationally independent irrational numbers and we see that for any $x \in X$ , $\{f^{(n)}(x)\}_{n=1}^\infty$ is dense in $X$ , by the property of irrational numbers. Note also that if $X=[0, 1] \times [0, 1]$ , then the answer is ""no"". From the Brouwer's fixed point theorem we see that given any continuous $f : X \rightarrow X$ we can find a fixed point $t \in X$ , and $f^{(n)}(t)=t$ for all $n$ . I am trying to show that given a continuous $f: S^1\times [0, 1] \rightarrow S^1 \times [0, 1]$ ,  there always exists some $a \in X$ such that the closure of $\{f^{(n)}(a)\}_{n=1}^\infty$ is a closed curve but not the whole of $X$ . I don't know if this is true for all $f$ , but it works for many examples. The simplest example is as follows: let $f(r, b)=(re^{2\pi ip}, b)$ , where $p$ is an irrational number. I have not found any other clues. Maybe this question requires some knowledge in dynamical systems or algebraic topology, which is not what I am familiar with. Any hint would be greatly appreciated!","First please see the question: Let . Does there exist a continuous map satisfying, for any point , is dense in ? Here we denote by the composition of for times. (EDIT: Here ""for any point"" means ""for all points"", "" "") Note that if is the torus, then the answer is ""yes"": just let where are rationally independent irrational numbers and we see that for any , is dense in , by the property of irrational numbers. Note also that if , then the answer is ""no"". From the Brouwer's fixed point theorem we see that given any continuous we can find a fixed point , and for all . I am trying to show that given a continuous ,  there always exists some such that the closure of is a closed curve but not the whole of . I don't know if this is true for all , but it works for many examples. The simplest example is as follows: let , where is an irrational number. I have not found any other clues. Maybe this question requires some knowledge in dynamical systems or algebraic topology, which is not what I am familiar with. Any hint would be greatly appreciated!","X = S^1 \times [0, 1] f: X \rightarrow X x \in X \{f^{(n)}(x)\}_{n=1}^\infty X f^{(n)}=f \circ f \circ ... \circ f f n \forall X = S^1 \times S^1 f(a, b)=(ae^{2\pi ip}, be^{2 \pi iq}) p, q x \in X \{f^{(n)}(x)\}_{n=1}^\infty X X=[0, 1] \times [0, 1] f : X \rightarrow X t \in X f^{(n)}(t)=t n f: S^1\times [0, 1] \rightarrow S^1 \times [0, 1] a \in X \{f^{(n)}(a)\}_{n=1}^\infty X f f(r, b)=(re^{2\pi ip}, b) p","['real-analysis', 'algebraic-topology', 'dynamical-systems', 'ergodic-theory', 'topological-dynamics']"
22,"Is the solution to $\theta''+0.021\,\text{sgn}(\theta')\sqrt{|\theta'|}+0.02\sin(\theta)=0,\,\theta_0=\pi/2,\,\theta'_0=0$ of finite duration?",Is the solution to  of finite duration?,"\theta''+0.021\,\text{sgn}(\theta')\sqrt{|\theta'|}+0.02\sin(\theta)=0,\,\theta_0=\pi/2,\,\theta'_0=0","Is the solution to $\ddot{\theta}+0.021\,\text{sgn}(\dot{\theta})\sqrt{|\dot{\theta}|}+0.02\sin(\theta)=0,\,\,\theta(0)=\frac{\pi}{2},\,\dot{\theta}(0) = 0 \quad\text{(Eq. 1)}$ of finite duration? I would like to know if the solution is of finite duration, meaning this that the solution reaches zero in finite time and stays there forever after (instead of being ""vanishing at infinity""): this means I need to prove if there exists a finite extinction time such after it passes the solution becomes zero and stays there $$\exists\,\, T<\infty \,/ \,\theta(t)=0 \,\,\forall t\geq T $$ Looking for a mathematical prove of that (theoretical or numerical, but such as confirms without any doubt if is truly a solution of finite duration). To be explicit about what I am asking for, here is an example: lets think in the equation $\dot{y}=-\sqrt{y},\,y(0)=1$ which has as solution $y(t)=\frac{1}{4}\left(t-2\right)^2$ , here the solution is not of finite duration, since after reaching zero at $t=2$ , it will start to rising again. But the equation $\dot{x} = -\text{sgn}(x)\sqrt{|x|},\,x(0)=1$ will have as solution $x(t) = \frac{1}{4}\left(1-\frac{t}{2}+\left|1-\frac{t}{2}\right|\right)^2$ which indeed is of finite duration reaching zero at $t=2$ and staying there forever (see plot here , and demo here ). I want to know if the solution $\theta(t)$ behaves at the end like the solution $x(t)$ , reaching zero at some ending time and staying there forever. I made this question later on a physics forum here , but in the actual question I am only focusing it on the mathematical side of it: please don´t close it as duplicate, or move the other one to the math forum instead my attempts so far I am trying to understand the papers by Vardia T. Haimo: Finite Time Differential Equations and Finite Time Controllers but they use tools I don´t have enough background to fully understand (like Liapunov Theory), so I change the classical nonlinear pendulum equation introducing a non-Lipschitz component similar to used on the papers which solutions numerically looks like having finite duration, to have something familiar to work with it. PS: Obviously, if you give the exact solution $\theta(t)$ it will be awesome! Added later Following some comments I have tried to research about Liapunov theory on the book ""Nonlinear Dynamics"" by H. K. Khalil, and I believe I am understanding the general idea, but is unreal to believe I will master the topic at least before the bounty ends (I am certainly, not a genius). But in this line, I found the paper Finite-Time Stability of Continuous Autonomous Systems by Sanjay P. Bhat and Dennis S. Bernstein where they used some Liapunov based tools to determine if a function has a Finite-settling-time behavior, but is too advanced to me. Hope someone could answer the question, to have a guide to focus on the specific topic of solution of finite-duration. another attempt Following this answer to other question I did, I believe the following: Since Eq. 1 have the zero function as a trivial solution Since the components $\theta$ , $\dot{\theta}$ , and $\ddot{\theta}$ are all jointly present in Eq . 1. So the Eq. 1 stands the existence of some time $t^*$ such $\theta(t^*)=\dot{\theta}(t^*)=\ddot{\theta}(t^*)=0$ , don't meaning here that this/these time $t^*$ exists on the nontrivial solutions, but at least the equation could stand its existence. To prove that the equation stands a finite-duration solution is equivalent to prove if there exist some ""ending time"" $T$ where $\theta(T)=\dot{\theta}(T)=0$ , because of the following: If I can choose a finite duration solution $y(t) = \theta(t)H(T-t)$ with $H(t)$ the Heaviside unitary step function: $$H(t) = \begin{cases} 1,\quad t > 0 \\ 0,\quad t \leq 0 \end{cases}$$ I will have that: $$\begin{array}{r c l} \dot{y}(t) & = & \frac{d}{dt}\left(\theta(t)H(T-t) \right)\\ & = & \dot{\theta}(t)H(T-t) + \theta(t)\delta(T-t) \\ & = & \dot{\theta}(t)H(T-t) + \underbrace{\theta(T)\delta(T-t)}_{=\,0\,\text{if}\,\theta(T)\,=\,0} \\ & = & \dot{\theta}(t)H(T-t) \end{array}$$ And equivalently: $$\begin{array}{r c l} \ddot{y}(t) & = & \frac{d}{dt}\left(\dot{\theta}(t)H(T-t) \right)\\ & = & \ddot{\theta}(t)H(T-t) + \dot{\theta}(t)\delta(T-t) \\ & = & \ddot{\theta}(t)H(T-t) + \underbrace{\dot{\theta}(T)\delta(T-t)}_{=\,0\,\text{if}\,\dot{\theta}(T)\,=\,0} \\ & = & \ddot{\theta}(t)H(T-t) \end{array}$$ So, for the second order system with zero as trivial solution, if there exists a ending time $T$ such as $\theta(T)=\dot{\theta}(T)=0$ the finite duration solution could be made by any solution after time $T$ ""stiched"" with the trivial zero function solution after time $T$ (this because $y(t) \equiv \theta(t),\, t<T$ ). I don´t know if is easier or harder to prove that exists a point $T$ that fulfills $\theta(T)=\dot{\theta}(T)=0$ than using Liapunov theory tools, but is an idea I have tried, unsuccessfully so far, but maybe it gives an idea of how to solve the problem to anybody else. Other attempts Solving Eq. 1 for $\ddot{\theta}(t) = 0$ leads to a $\tan^{-1}$ which never achieves exactly zero. Here is the attempt done by other person in the question of the physics forum. Interestingly are found a time $T \approx 90$ which is similar to the plot ending. Solving Eq. 1 for $\dot{\theta}(t) = 0$ will lead to the classic equation of the nonlinear pendulum without friction, which solutions are Jacobi Elliptic functions, so since are periodic, there are going to be infinite times where it lands on zero (I have a related question here ). Solving Eq. 1 for $\theta(t) = 0$ leads to $\dot{\theta}(t) = \frac{a^2}{4}(T-t)^2H(T-t)$ as is explained here , which looks promising since it has only one ending time $T$ , but with the actual initial conditions it shows to be centered at $T=0$ (at least under my approach), so it don´t work either (since from the plot the only point with $\ddot{\theta}=\dot{\theta}=0$ is at the end, I was hopeful about its validity). Maybe here I am making some mistake, so If someone could try it too and can fix it it will solve the question, since founding $T$ such as $\theta(T)=\dot{\theta}(T)=0$ will answer the problem (if I am not mistaken). PS (2) : About the ""arbitrarity"" of the selection of the non-lipschitz component,  the traditional pendulum model friction as proportional to the rate of change as is explained on Wikipedia as the Newtonian Law of Viscosity , but in the same section is explained that are other models, like the Power Law Model which has components actually similar with which I am using here. I hope that knowing how to model finite duration solution will lead to adapts these classic models to show solutions that indeed behaves as having an ending time $T < \infty$ , which I believe is the case on everyday phenomena, as a clear example, the Euler's Disk toy that show its ending time when the sound stops. Last update I have just found the following papers under the term sublinear damping that shows example in physics of equations really similar to the proposed example: ""A note on the dynamics of an oscillator in the presence of strong friction"" - H.Amann & J.I.Diaz ""A conservation law with spatially localized sublinear damping"" - Christophe Besse & Sylvain Ervedoza ""Behavior of Solutions of Second-Order Differential Equations with Sublinear Damping"" - J. Karsai & J. R. Graef Particularly, the first one, since graphically the solutions shows having a decaying envelope, I think could be proving than when the solutions enters the small-angle approximation regimen $\sin(x) \approx x$ it will be indeed being a solution with a finite extinction time. Among the three papers I thought a formal prove to the main question could be done, but I wasn't able to find it by myself.","Is the solution to of finite duration? I would like to know if the solution is of finite duration, meaning this that the solution reaches zero in finite time and stays there forever after (instead of being ""vanishing at infinity""): this means I need to prove if there exists a finite extinction time such after it passes the solution becomes zero and stays there Looking for a mathematical prove of that (theoretical or numerical, but such as confirms without any doubt if is truly a solution of finite duration). To be explicit about what I am asking for, here is an example: lets think in the equation which has as solution , here the solution is not of finite duration, since after reaching zero at , it will start to rising again. But the equation will have as solution which indeed is of finite duration reaching zero at and staying there forever (see plot here , and demo here ). I want to know if the solution behaves at the end like the solution , reaching zero at some ending time and staying there forever. I made this question later on a physics forum here , but in the actual question I am only focusing it on the mathematical side of it: please don´t close it as duplicate, or move the other one to the math forum instead my attempts so far I am trying to understand the papers by Vardia T. Haimo: Finite Time Differential Equations and Finite Time Controllers but they use tools I don´t have enough background to fully understand (like Liapunov Theory), so I change the classical nonlinear pendulum equation introducing a non-Lipschitz component similar to used on the papers which solutions numerically looks like having finite duration, to have something familiar to work with it. PS: Obviously, if you give the exact solution it will be awesome! Added later Following some comments I have tried to research about Liapunov theory on the book ""Nonlinear Dynamics"" by H. K. Khalil, and I believe I am understanding the general idea, but is unreal to believe I will master the topic at least before the bounty ends (I am certainly, not a genius). But in this line, I found the paper Finite-Time Stability of Continuous Autonomous Systems by Sanjay P. Bhat and Dennis S. Bernstein where they used some Liapunov based tools to determine if a function has a Finite-settling-time behavior, but is too advanced to me. Hope someone could answer the question, to have a guide to focus on the specific topic of solution of finite-duration. another attempt Following this answer to other question I did, I believe the following: Since Eq. 1 have the zero function as a trivial solution Since the components , , and are all jointly present in Eq . 1. So the Eq. 1 stands the existence of some time such , don't meaning here that this/these time exists on the nontrivial solutions, but at least the equation could stand its existence. To prove that the equation stands a finite-duration solution is equivalent to prove if there exist some ""ending time"" where , because of the following: If I can choose a finite duration solution with the Heaviside unitary step function: I will have that: And equivalently: So, for the second order system with zero as trivial solution, if there exists a ending time such as the finite duration solution could be made by any solution after time ""stiched"" with the trivial zero function solution after time (this because ). I don´t know if is easier or harder to prove that exists a point that fulfills than using Liapunov theory tools, but is an idea I have tried, unsuccessfully so far, but maybe it gives an idea of how to solve the problem to anybody else. Other attempts Solving Eq. 1 for leads to a which never achieves exactly zero. Here is the attempt done by other person in the question of the physics forum. Interestingly are found a time which is similar to the plot ending. Solving Eq. 1 for will lead to the classic equation of the nonlinear pendulum without friction, which solutions are Jacobi Elliptic functions, so since are periodic, there are going to be infinite times where it lands on zero (I have a related question here ). Solving Eq. 1 for leads to as is explained here , which looks promising since it has only one ending time , but with the actual initial conditions it shows to be centered at (at least under my approach), so it don´t work either (since from the plot the only point with is at the end, I was hopeful about its validity). Maybe here I am making some mistake, so If someone could try it too and can fix it it will solve the question, since founding such as will answer the problem (if I am not mistaken). PS (2) : About the ""arbitrarity"" of the selection of the non-lipschitz component,  the traditional pendulum model friction as proportional to the rate of change as is explained on Wikipedia as the Newtonian Law of Viscosity , but in the same section is explained that are other models, like the Power Law Model which has components actually similar with which I am using here. I hope that knowing how to model finite duration solution will lead to adapts these classic models to show solutions that indeed behaves as having an ending time , which I believe is the case on everyday phenomena, as a clear example, the Euler's Disk toy that show its ending time when the sound stops. Last update I have just found the following papers under the term sublinear damping that shows example in physics of equations really similar to the proposed example: ""A note on the dynamics of an oscillator in the presence of strong friction"" - H.Amann & J.I.Diaz ""A conservation law with spatially localized sublinear damping"" - Christophe Besse & Sylvain Ervedoza ""Behavior of Solutions of Second-Order Differential Equations with Sublinear Damping"" - J. Karsai & J. R. Graef Particularly, the first one, since graphically the solutions shows having a decaying envelope, I think could be proving than when the solutions enters the small-angle approximation regimen it will be indeed being a solution with a finite extinction time. Among the three papers I thought a formal prove to the main question could be done, but I wasn't able to find it by myself.","\ddot{\theta}+0.021\,\text{sgn}(\dot{\theta})\sqrt{|\dot{\theta}|}+0.02\sin(\theta)=0,\,\,\theta(0)=\frac{\pi}{2},\,\dot{\theta}(0) = 0 \quad\text{(Eq. 1)} \exists\,\, T<\infty \,/ \,\theta(t)=0 \,\,\forall t\geq T  \dot{y}=-\sqrt{y},\,y(0)=1 y(t)=\frac{1}{4}\left(t-2\right)^2 t=2 \dot{x} = -\text{sgn}(x)\sqrt{|x|},\,x(0)=1 x(t) = \frac{1}{4}\left(1-\frac{t}{2}+\left|1-\frac{t}{2}\right|\right)^2 t=2 \theta(t) x(t) \theta(t) \theta \dot{\theta} \ddot{\theta} t^* \theta(t^*)=\dot{\theta}(t^*)=\ddot{\theta}(t^*)=0 t^* T \theta(T)=\dot{\theta}(T)=0 y(t) = \theta(t)H(T-t) H(t) H(t) = \begin{cases} 1,\quad t > 0 \\ 0,\quad t \leq 0 \end{cases} \begin{array}{r c l}
\dot{y}(t) & = & \frac{d}{dt}\left(\theta(t)H(T-t) \right)\\
& = & \dot{\theta}(t)H(T-t) + \theta(t)\delta(T-t) \\
& = & \dot{\theta}(t)H(T-t) + \underbrace{\theta(T)\delta(T-t)}_{=\,0\,\text{if}\,\theta(T)\,=\,0} \\
& = & \dot{\theta}(t)H(T-t)
\end{array} \begin{array}{r c l}
\ddot{y}(t) & = & \frac{d}{dt}\left(\dot{\theta}(t)H(T-t) \right)\\
& = & \ddot{\theta}(t)H(T-t) + \dot{\theta}(t)\delta(T-t) \\
& = & \ddot{\theta}(t)H(T-t) + \underbrace{\dot{\theta}(T)\delta(T-t)}_{=\,0\,\text{if}\,\dot{\theta}(T)\,=\,0} \\
& = & \ddot{\theta}(t)H(T-t)
\end{array} T \theta(T)=\dot{\theta}(T)=0 T T y(t) \equiv \theta(t),\, t<T T \theta(T)=\dot{\theta}(T)=0 \ddot{\theta}(t) = 0 \tan^{-1} T \approx 90 \dot{\theta}(t) = 0 \theta(t) = 0 \dot{\theta}(t) = \frac{a^2}{4}(T-t)^2H(T-t) T T=0 \ddot{\theta}=\dot{\theta}=0 T \theta(T)=\dot{\theta}(T)=0 T < \infty \sin(x) \approx x","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'singular-solution', 'finite-duration']"
23,Rudin's PMA 10.38 theorem,Rudin's PMA 10.38 theorem,,"Suppose $E$ is  a  convex open set in $\Bbb R^{n}$ , $f \in C^{1}$ , $p$ is an integer, $1\leq p  \leq n$ and $(D_{j}f)(x)=0$ ( $p<j \leq n ,x\in E$ ). Then there exists an $F \in C^{1}$ such that $$ (D_{p}F)(x)=f(x), (D_{j}F)(x)=0$$ if $p<j\leq n ,x \in E$ . Proof : Write $x=(x',x_{p},x'')$ , where $x'=(x_{1},\dots,x_{p-1})$ , $x''=(x_{p+1},\dots ,x_{n})$ . When $p=1$ , $x'$ is absent and when $p=n$ , $x''$ is absent. Let $V$ be the set of all ( $x',x_{p}$ ) such that $(x',x_{p},x'') \in E$ for some $x''$ . Being a projection of $E$ , $V$ is convex open set in $\Bbb R^{p}$ . Since $E$ is convex and $(D_{j}f)(x)=0$ for $p<j\leq n$ , $f(x)$ does not depend on $x''$ . Hence there's a function $\phi$ with domain $V$ s.t $f(x)=\phi(x',x_{p})$ , for all $x\in E$ . If $p$ =1, $V$ is a segment in $\Bbb R^{1}$ . Pick $c \in V$ and define $$F(x)=\int_{c}^{x_{1}} \phi (t) dt$$ ( mark this integral by ( $\star$ )). If $p$ >1, let $U$ be the set of all $x'\in \Bbb R^{p-1}$ such that $(x,x_{p}) \in V$ for some $x_{p}$ . Then $U$ is a convex open set in $\Bbb R^{p-1}$ and there' a function $\alpha \in C^{1}$ such that $(x,\alpha (x')) \in V$ for all $x' \in U$ . Define $$F(x)=\int_{\alpha (x')}^{x_{p}} \phi (x',t) dt$$ (mark this integral by ( $\oplus$ )). I don't understand why is it necessary for $E$ to be convex. I also don't understand how do ( $\star$ ) and ( $\oplus$ ) guarantees that $$ (D_{p}F)(x)=f(x), (D_{j}F)(x)=0$$ , if $p<j\leq n ,x \in E$ Would be satisfied. Any help would be appreciated.","Suppose is  a  convex open set in , , is an integer, and ( ). Then there exists an such that if . Proof : Write , where , . When , is absent and when , is absent. Let be the set of all ( ) such that for some . Being a projection of , is convex open set in . Since is convex and for , does not depend on . Hence there's a function with domain s.t , for all . If =1, is a segment in . Pick and define ( mark this integral by ( )). If >1, let be the set of all such that for some . Then is a convex open set in and there' a function such that for all . Define (mark this integral by ( )). I don't understand why is it necessary for to be convex. I also don't understand how do ( ) and ( ) guarantees that , if Would be satisfied. Any help would be appreciated.","E \Bbb R^{n} f \in C^{1} p 1\leq p  \leq n (D_{j}f)(x)=0 p<j \leq n ,x\in E F \in C^{1}  (D_{p}F)(x)=f(x), (D_{j}F)(x)=0 p<j\leq n ,x \in E x=(x',x_{p},x'') x'=(x_{1},\dots,x_{p-1}) x''=(x_{p+1},\dots ,x_{n}) p=1 x' p=n x'' V x',x_{p} (x',x_{p},x'') \in E x'' E V \Bbb R^{p} E (D_{j}f)(x)=0 p<j\leq n f(x) x'' \phi V f(x)=\phi(x',x_{p}) x\in E p V \Bbb R^{1} c \in V F(x)=\int_{c}^{x_{1}} \phi (t) dt \star p U x'\in \Bbb R^{p-1} (x,x_{p}) \in V x_{p} U \Bbb R^{p-1} \alpha \in C^{1} (x,\alpha (x')) \in V x' \in U F(x)=\int_{\alpha (x')}^{x_{p}} \phi (x',t) dt \oplus E \star \oplus  (D_{p}F)(x)=f(x), (D_{j}F)(x)=0 p<j\leq n ,x \in E","['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations', 'multivariable-calculus']"
24,"If a Taylor series agrees with its function in $U$, that function is analytic in $U$","If a Taylor series agrees with its function in , that function is analytic in",U U,"Assume that $f:\mathbb{R}\rightarrow\mathbb{R}$ coincides with its Taylor series centered at a point $p\in U$ on an open set $U$ , i.e. $$\exists p\in U, \forall x\in U, f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(p)}{k!}(x-p)^n.$$ Is $f$ analytic on $U$ ? That is: $$\forall p\in U, \exists \varepsilon>0, \forall x\in \mathbb{R}, |p-x|<\varepsilon\Rightarrow f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(p)}{k!}(x-p)^n?$$","Assume that coincides with its Taylor series centered at a point on an open set , i.e. Is analytic on ? That is:","f:\mathbb{R}\rightarrow\mathbb{R} p\in U U \exists p\in U, \forall x\in U, f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(p)}{k!}(x-p)^n. f U \forall p\in U, \exists \varepsilon>0, \forall x\in \mathbb{R}, |p-x|<\varepsilon\Rightarrow f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(p)}{k!}(x-p)^n?","['real-analysis', 'calculus']"
25,Extremely slowly divergent series - Putnam 2008 A.4 generalization,Extremely slowly divergent series - Putnam 2008 A.4 generalization,,"INTRODUCTION Starting from Putnam 2008, Exercise A.4, I tried to generalize the result to series that diverge slowlier than the one presented there. Since I am not too familiar with the necessary instruments (e.g. the Lambert function), I wanted to show my partial results and ask you to comment them, find any mistake, or give some insight on the situation. (I HAVE DONE SOME EDITING on the last two paragraphs, to account for a mistake I discovered) ORIGINAL SETTING The original divergent series is constructed as follows. Let $$f(x)=  \begin{cases} x & (x \leq e)\\ x\log f(x) & (x>e). \end{cases} $$ Then the series $$\sum_{n=1}^{\infty} \frac1{f(n)}$$ diverges. The function is basically a product of nested logarithms , where the number of logarithmic factors grows with $x$ , in such a way that $f(x)$ is continuous and monotonically increasing. Thus divergence is proved via divergence of the integral $$\int_e^{\infty} \frac1{f(x)}dx.$$ GENERALIZATION (CLAIM) Let $\alpha$ be a real number such that $$1< \alpha\leq \frac{e}{e-1}\tag{1}\label{1}$$ and $$\beta = e^{-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}\alpha\right)},\tag{2}\label{2}$$ where $W(x)$ is the principal branch of the Lambert function. Let also $E_1 = e^\beta$ and $$E_k = e^{E_{k-1}}$$ for $k>1$ . Similarly define $L_1(x) = \log x$ and $$L_k(x) =\log L_{k-1}(x)$$ for $k>1$ . If $f(x)$ is defined as $$f_\alpha(x)=\begin{cases} x\log^\alpha x & \left(e < x \leq e^\beta\right)\\ x\log x\log^\alpha\log x & \left( e^\beta <x \leq e^{e^\beta}\right)\\ x\log x\log\log x \log^\alpha \log\log x & \left(e^{e^\beta} < x \leq e^{e^{e^\beta}}\right)\\ \vdots & \vdots \\ x\cdot \prod_{k=1}^{m}L_k(x) \cdot L_{m+1}^\alpha(x) & (E_m < x \leq E_{m+1})\ \ \ (m\geq 1)\\ \vdots & \vdots. \end{cases} $$ Then the series $$\sum_{n=3}\frac1{f_\alpha(n)}\tag{3}\label{3}$$ diverges. SOME NOTES In order to guarantee monotonicity of the piecewise function $f_\alpha(x)$ we need the constituent functions to behave properly. This gives limitations on $\alpha$ given by \eqref{1} and the relationship between $\alpha$ and the connection points given by \eqref{2}. First of all we wish the component to intersect at some point, and this happens if, for some $\overline x$ , $$x\prod_{k=1}^m L_k(\overline x) L_{m+1}^\alpha(\overline x) = x\prod_{k=1}^{m+1}L_k(\overline x) L_{m+2}^\alpha(\overline x) $$ that is $$L_{m+1}^{\alpha-1}(\overline x) = L_{m+2}^{\alpha} (\overline x),$$ Letting $\xi = L_{m+1}(\overline x)$ we obtain the equation $$ \xi^{\alpha-1} = \log^\alpha\xi.$$ This equation has only one solution when $\alpha = \frac{e}{e-1}$ (the two red lines in the Figure below, that are tangent in point $B$ ), and has to solutions for $0< \alpha <\frac{e}{e-1}$ . In the latter case, the smallest one is $$\beta = e^{-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}\alpha\right)},$$ (the larger one being given by the lower branch of the Lambert function) from which we obtain the connection point $$\overline x = E_{m+1}.$$ In the Figure, the case $\alpha = \frac32$ is depicted ( $A$ marks the intersection between the two lines). Note that, from the asymptotic beahviour of the logarithmic function, we can also deduce that in a right neighborhood of $\beta$ we have $$x\prod_{k=1}^m L_k(x) L_{m+1}^\alpha(x) < x\prod_{k=1}^{m+1}L_k(x) L_{m+2}^\alpha(x).$$ Below a plot of the function $$\beta = \beta(\alpha)$$ (equation \eqref{2}) for $0 <\alpha \leq \frac{e}{e-1}$ . I haven't studied this function formally but we surely have $$\lim_{\alpha \to \frac{e}{e-1}}\beta(\alpha)= e^{-e\cdot W\left(-\frac1{e}\right)}=e^e,\tag{4}\label{4}$$ and it seems we get $$\lim_{\alpha \to 1} \beta(\alpha) = e \tag{5}\label{5}.$$ In particular \eqref{5} shows that we can think of the series \eqref{3} as a generalization of the one proposed in the Putnam competition (where $\alpha = 1$ , $\beta =e$ ). It appears that the reasoning (also in what follows) can be extended to the more general case $0<\alpha\leq \frac{e}{e-1}$ . However the divergence when $0<\alpha< 1$ is trivial. When $\alpha > 1$ , though, each constituent function would give rise by itself to a convergent series (by Condensation Test), and yet our series diverge. PROOF OF DIVERGENCE (SKETCH) As stated we need to prove that the integral diverges. Now, we have $$\mathcal I_0 = \int_e^{e^\beta} \frac1{x \log^{\alpha} x}dx = -\frac1{\alpha-1}\left[\frac1{\log^{\alpha-1}x}\right]^{e^\beta}_e=\frac1{\alpha-1}\left(1-\frac1{\beta^{\alpha-1}}\right),$$ which, using \eqref{2}, can be expressed as $$\mathcal I_0 = \frac1{\alpha-1}\left[1-e^{\alpha\cdot W\left(-\frac{\alpha-1}{\alpha}\right)}\right].$$ For all the other intervals we get $$\mathcal I_1 = \int_{E_m}^{E_{m+1}} \frac1{x\prod_{k=1}^m L_m(x)\cdot L_{m+1}^{\alpha}(x)}dx =-\frac1{\alpha-1}\left[\frac1{L_{m+1}^{\alpha-1}(x)}\right]_{E_m}^{E_{m+1}}=\frac1{\alpha-1}\left(\frac1{\log^{\alpha-1}\beta}-\frac1{\beta^{\alpha-1}}\right).$$ Using again \eqref{2} yields $$\mathcal I_1 = \frac1{\alpha-1}\left[\frac1{\left(-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}{\alpha}\right)\right)^{\alpha-1}}-e^{\alpha\cdot W\left(-\frac{\alpha-1}{\alpha}\right)}\right].$$ A plot of $\mathcal I_0$ (yellow) and $\mathcal I_1$ (green) as functions of $\alpha$ is depicted below. Here again a more formal study of these functions is required. From what I have done so far, it appears that both are limited functions of $\alpha$ . We conclude that the integral diverges, and so does the series. Also, as $\alpha$ approaches $\frac{e}{e-1}$ the ""speed"" at which the series diverges rapidly decreases, since the integral increments are less then $1$ in the (larger and larger) intervals $[E_m, E_{m+1}]$ . QUESTIONS First of all, I am interested in spotting any error in my approach. Secondarily, since I have been largely qualitative in some steps of the reasoning, I was wondering if everything can be made more formal, in the last two paragraphs above especially (study of $\beta(\alpha)$ , $\mathcal I_0(\alpha)$ , and $\mathcal I_1(\alpha)$ , for example). Of course any other insight, comment, suggestion is more than welcome.","INTRODUCTION Starting from Putnam 2008, Exercise A.4, I tried to generalize the result to series that diverge slowlier than the one presented there. Since I am not too familiar with the necessary instruments (e.g. the Lambert function), I wanted to show my partial results and ask you to comment them, find any mistake, or give some insight on the situation. (I HAVE DONE SOME EDITING on the last two paragraphs, to account for a mistake I discovered) ORIGINAL SETTING The original divergent series is constructed as follows. Let Then the series diverges. The function is basically a product of nested logarithms , where the number of logarithmic factors grows with , in such a way that is continuous and monotonically increasing. Thus divergence is proved via divergence of the integral GENERALIZATION (CLAIM) Let be a real number such that and where is the principal branch of the Lambert function. Let also and for . Similarly define and for . If is defined as Then the series diverges. SOME NOTES In order to guarantee monotonicity of the piecewise function we need the constituent functions to behave properly. This gives limitations on given by \eqref{1} and the relationship between and the connection points given by \eqref{2}. First of all we wish the component to intersect at some point, and this happens if, for some , that is Letting we obtain the equation This equation has only one solution when (the two red lines in the Figure below, that are tangent in point ), and has to solutions for . In the latter case, the smallest one is (the larger one being given by the lower branch of the Lambert function) from which we obtain the connection point In the Figure, the case is depicted ( marks the intersection between the two lines). Note that, from the asymptotic beahviour of the logarithmic function, we can also deduce that in a right neighborhood of we have Below a plot of the function (equation \eqref{2}) for . I haven't studied this function formally but we surely have and it seems we get In particular \eqref{5} shows that we can think of the series \eqref{3} as a generalization of the one proposed in the Putnam competition (where , ). It appears that the reasoning (also in what follows) can be extended to the more general case . However the divergence when is trivial. When , though, each constituent function would give rise by itself to a convergent series (by Condensation Test), and yet our series diverge. PROOF OF DIVERGENCE (SKETCH) As stated we need to prove that the integral diverges. Now, we have which, using \eqref{2}, can be expressed as For all the other intervals we get Using again \eqref{2} yields A plot of (yellow) and (green) as functions of is depicted below. Here again a more formal study of these functions is required. From what I have done so far, it appears that both are limited functions of . We conclude that the integral diverges, and so does the series. Also, as approaches the ""speed"" at which the series diverges rapidly decreases, since the integral increments are less then in the (larger and larger) intervals . QUESTIONS First of all, I am interested in spotting any error in my approach. Secondarily, since I have been largely qualitative in some steps of the reasoning, I was wondering if everything can be made more formal, in the last two paragraphs above especially (study of , , and , for example). Of course any other insight, comment, suggestion is more than welcome.","f(x)= 
\begin{cases}
x & (x \leq e)\\
x\log f(x) & (x>e).
\end{cases}
 \sum_{n=1}^{\infty} \frac1{f(n)} x f(x) \int_e^{\infty} \frac1{f(x)}dx. \alpha 1< \alpha\leq \frac{e}{e-1}\tag{1}\label{1} \beta = e^{-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}\alpha\right)},\tag{2}\label{2} W(x) E_1 = e^\beta E_k = e^{E_{k-1}} k>1 L_1(x) = \log x L_k(x) =\log L_{k-1}(x) k>1 f(x) f_\alpha(x)=\begin{cases}
x\log^\alpha x & \left(e < x \leq e^\beta\right)\\
x\log x\log^\alpha\log x & \left( e^\beta <x \leq e^{e^\beta}\right)\\
x\log x\log\log x \log^\alpha \log\log x & \left(e^{e^\beta} < x \leq e^{e^{e^\beta}}\right)\\
\vdots & \vdots \\
x\cdot \prod_{k=1}^{m}L_k(x) \cdot L_{m+1}^\alpha(x) & (E_m < x \leq E_{m+1})\ \ \ (m\geq 1)\\
\vdots & \vdots.
\end{cases}
 \sum_{n=3}\frac1{f_\alpha(n)}\tag{3}\label{3} f_\alpha(x) \alpha \alpha \overline x x\prod_{k=1}^m L_k(\overline x) L_{m+1}^\alpha(\overline x) = x\prod_{k=1}^{m+1}L_k(\overline x) L_{m+2}^\alpha(\overline x)  L_{m+1}^{\alpha-1}(\overline x) = L_{m+2}^{\alpha} (\overline x), \xi = L_{m+1}(\overline x)  \xi^{\alpha-1} = \log^\alpha\xi. \alpha = \frac{e}{e-1} B 0< \alpha <\frac{e}{e-1} \beta = e^{-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}\alpha\right)}, \overline x = E_{m+1}. \alpha = \frac32 A \beta x\prod_{k=1}^m L_k(x) L_{m+1}^\alpha(x) < x\prod_{k=1}^{m+1}L_k(x) L_{m+2}^\alpha(x). \beta = \beta(\alpha) 0 <\alpha \leq \frac{e}{e-1} \lim_{\alpha \to \frac{e}{e-1}}\beta(\alpha)= e^{-e\cdot W\left(-\frac1{e}\right)}=e^e,\tag{4}\label{4} \lim_{\alpha \to 1} \beta(\alpha) = e \tag{5}\label{5}. \alpha = 1 \beta =e 0<\alpha\leq \frac{e}{e-1} 0<\alpha< 1 \alpha > 1 \mathcal I_0 = \int_e^{e^\beta} \frac1{x \log^{\alpha} x}dx = -\frac1{\alpha-1}\left[\frac1{\log^{\alpha-1}x}\right]^{e^\beta}_e=\frac1{\alpha-1}\left(1-\frac1{\beta^{\alpha-1}}\right), \mathcal I_0 = \frac1{\alpha-1}\left[1-e^{\alpha\cdot W\left(-\frac{\alpha-1}{\alpha}\right)}\right]. \mathcal I_1 = \int_{E_m}^{E_{m+1}} \frac1{x\prod_{k=1}^m L_m(x)\cdot L_{m+1}^{\alpha}(x)}dx =-\frac1{\alpha-1}\left[\frac1{L_{m+1}^{\alpha-1}(x)}\right]_{E_m}^{E_{m+1}}=\frac1{\alpha-1}\left(\frac1{\log^{\alpha-1}\beta}-\frac1{\beta^{\alpha-1}}\right). \mathcal I_1 = \frac1{\alpha-1}\left[\frac1{\left(-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}{\alpha}\right)\right)^{\alpha-1}}-e^{\alpha\cdot W\left(-\frac{\alpha-1}{\alpha}\right)}\right]. \mathcal I_0 \mathcal I_1 \alpha \alpha \alpha \frac{e}{e-1} 1 [E_m, E_{m+1}] \beta(\alpha) \mathcal I_0(\alpha) \mathcal I_1(\alpha)","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'solution-verification']"
26,"Show that $S=\mathbb{Q} \cap [0,1]$.",Show that .,"S=\mathbb{Q} \cap [0,1]","Can you give a hint to show the following exercise? Let $S \subset [0,1]$ such that $0, 1\in S$ and for all $n\in \mathbb{N}$ , $s_1,...,s_n \in S$ distinct, then $\dfrac{s_1+...+s_n}{n}\in S$ . Show that $S=\mathbb{Q} \cap [0,1]$ . Thank you","Can you give a hint to show the following exercise? Let such that and for all , distinct, then . Show that . Thank you","S \subset [0,1] 0, 1\in S n\in \mathbb{N} s_1,...,s_n \in S \dfrac{s_1+...+s_n}{n}\in S S=\mathbb{Q} \cap [0,1]","['real-analysis', 'rational-numbers']"
27,Is this operator between $\ell^{25}$ and $\ell^{12}$ continuous?,Is this operator between  and  continuous?,\ell^{25} \ell^{12},"Problem: Let us define $\ell^p$ as the space of sequences $(x_n)_{n \in \mathbb{N}}$ such that $\sum\limits_{n \in \mathbb{N}}|x_n|^p < +\infty$ with the usual norm $$\|x\|_p = \big( \sum\limits_{n \in \mathbb{N}}|x_n|^p \big)^{\frac 1 p}$$ Define $T:\ell^{25} \to \ell^{12}$ in the following way: $$T(x_1,\dots,x_n, \dots) = (x_1^{2018},\dots,x_n^{2018}, \dots)$$ I am asking if the map $T$ is continuous. Attempt: Let us fix $x \in \ell^{25}$ and let us study what happens for $y \in \ell^{25}$ with $\|y-x\|_p \leq 1$ . We have that $|y_n-x_n| \leq 1$ for all $n \in \mathbb{N}$ and thus $|x_n^{2018}-y_n^{2018}| \leq |x_n - y_n| K$ for $K > 0$ depending only on $\|x\|_p$ which has been fixed. This is because $a^n-b^n = (a-b)p(a, b)$ where $p$ is a polynomial on $a$ and $b$ . Thus $\|Tx-Ty\|_{12}^{12} \leq K^{12} \|x-y\|_{12}^{12}$ but this does not help me to conclude because we cannot control the $\mathcal{l}^{12}$ with the $\mathcal{l}^{25}$ norm. Trivial remark: the operator is not linear and thus we have no hope to prove that $T$ is Lipschitz continuous.",Problem: Let us define as the space of sequences such that with the usual norm Define in the following way: I am asking if the map is continuous. Attempt: Let us fix and let us study what happens for with . We have that for all and thus for depending only on which has been fixed. This is because where is a polynomial on and . Thus but this does not help me to conclude because we cannot control the with the norm. Trivial remark: the operator is not linear and thus we have no hope to prove that is Lipschitz continuous.,"\ell^p (x_n)_{n \in \mathbb{N}} \sum\limits_{n \in \mathbb{N}}|x_n|^p < +\infty \|x\|_p = \big( \sum\limits_{n \in \mathbb{N}}|x_n|^p \big)^{\frac 1 p} T:\ell^{25} \to \ell^{12} T(x_1,\dots,x_n, \dots) = (x_1^{2018},\dots,x_n^{2018}, \dots) T x \in \ell^{25} y \in \ell^{25} \|y-x\|_p \leq 1 |y_n-x_n| \leq 1 n \in \mathbb{N} |x_n^{2018}-y_n^{2018}| \leq |x_n - y_n| K K > 0 \|x\|_p a^n-b^n = (a-b)p(a, b) p a b \|Tx-Ty\|_{12}^{12} \leq K^{12} \|x-y\|_{12}^{12} \mathcal{l}^{12} \mathcal{l}^{25} T","['real-analysis', 'functional-analysis', 'lp-spaces']"
28,Counterexample on mixed partials,Counterexample on mixed partials,,"I was thinking about Young's and Schwarz's theorem on when do partial derivatives be equal and I was wondering about how smooth can a function whose mixed partial are not equal be. I was wondering there is a function $f:\mathbb{R}^2 \to \mathbb{R}$ , such that the following hold. a) $f$ is continuous in $\mathbb{R}^2  $ (We can assume $f(0,0) = 0$ ) b) $f_x$ and $f_y$ and their partial derivativess exist everywhere (We can assume $f_x(0,0) = f_y(0,0) = 0$ ) c) $f_x$ and $f_y$ are continuous in $\mathbb{R}^2$ (so $f$ is differentiable at $(0,0)$ ) and the directional derivatives at $(0, 0)$ can be expressed as $L(v)$ , where $L$ is linear. d) $f_{xx}$ , $f_{yx}$ , $f_{xy}$ , $f_{yy}$ are continuous in $\mathbb{R}^2\smallsetminus\{(0, 0)\}$ . e) $f_{xx}$ is continuous at $(0,0)$ (so $f_x$ is differentiable at $(0, 0)$ ). (We can assume $f_{xx}(0,0) = f_{yy}(0,0) = 0$ . If $f_y$ was also differentiable at $(0, 0)$ , by Young's theorem mixed partial would be equal. If $f_{yy}$ or $f_{xy}$ we continuous at $(0,0)$ , $f_y$ would be differentiable at ( $0, 0$ ). Assumptions can be made because if such and $f$ exist, then $f - f(0,0)- xf_x(0,0) - yf_y(0,0) -  \frac{x^2}{2}f_{xx}(0,0) - \frac{y^2}{2}f_{yy}(0,0)$ will also satisfy the desired property.","I was thinking about Young's and Schwarz's theorem on when do partial derivatives be equal and I was wondering about how smooth can a function whose mixed partial are not equal be. I was wondering there is a function , such that the following hold. a) is continuous in (We can assume ) b) and and their partial derivativess exist everywhere (We can assume ) c) and are continuous in (so is differentiable at ) and the directional derivatives at can be expressed as , where is linear. d) , , , are continuous in . e) is continuous at (so is differentiable at ). (We can assume . If was also differentiable at , by Young's theorem mixed partial would be equal. If or we continuous at , would be differentiable at ( ). Assumptions can be made because if such and exist, then will also satisfy the desired property.","f:\mathbb{R}^2 \to \mathbb{R} f \mathbb{R}^2   f(0,0) = 0 f_x f_y f_x(0,0) = f_y(0,0) = 0 f_x f_y \mathbb{R}^2 f (0,0) (0, 0) L(v) L f_{xx} f_{yx} f_{xy} f_{yy} \mathbb{R}^2\smallsetminus\{(0, 0)\} f_{xx} (0,0) f_x (0, 0) f_{xx}(0,0) = f_{yy}(0,0) = 0 f_y (0, 0) f_{yy} f_{xy} (0,0) f_y 0, 0 f f - f(0,0)- xf_x(0,0) - yf_y(0,0) -  \frac{x^2}{2}f_{xx}(0,0) - \frac{y^2}{2}f_{yy}(0,0)",['real-analysis']
29,"Continuous real function $f$ such that $f(a)<0,f(b)>0$ but with no ""switching point"" $c\in(a,b)$","Continuous real function  such that  but with no ""switching point""","f f(a)<0,f(b)>0 c\in(a,b)","Let $f:[a,b]\rightarrow\mathbb{R}$ be a continuous real function and assume $f(a)<0<f(b)$ . Does $f$ necessarily have a point $c\in (a,b)$ such that $f\leq 0$ on a left neighborhood of $c$ and $f\geq 0$ on a right neighborhood of $c$ (a ""switching point"", for that matter)? This is a seemingly simple question, but it took me longer than I expected to answer it. The answer turns out to be negative; however the counterexample I found it a bit ""exotic"" and I wondered if simpler examples can be found. My counterexample is based on the Cantor set , and it actually produces a smooth $f$ . First, for a closed interval $I$ , let $g_I$ be a smooth bump function whose support is $I$ . Specifically, $g_I(x)>0$ for $x\in I$ and $g_I(x)=0$ otherwise. Let $\mathcal{I}_n$ be the set of intervals removed at the $n$ -th stage of the construction of the Cantor set (of which there are $2^{n-1}$ ). Define $f:[0,1]\rightarrow\mathbb R$ by $$f(x)=\sum_{n=1}^\infty  \sum_{I\in\cal{I}_n}(-1)^ng_{I}(x).$$ Note that all the removed intervals are pairwise disjoint, so $f$ is well-defined. Now, it's not to hard to prove that: $f$ is smooth. The set $\left\{x:f(x)=0\right\}$ is the Cantor set $C$ . Therefore any $x\not\in C$ can't be a switching point of $f$ . For every $c\in C$ , there's a negative point and a positive point in every right and every left neighborhood of $c$ . Therefore $f$ has no switching points. Now it only remains to choose $0<a<b<1$ such that $f(a)<0<f(b)$ . This turned out to be more involved than I expected; are any simpler counterexamples, preferably ones that would appear more ""familiar"" to a standard Calculus course major?","Let be a continuous real function and assume . Does necessarily have a point such that on a left neighborhood of and on a right neighborhood of (a ""switching point"", for that matter)? This is a seemingly simple question, but it took me longer than I expected to answer it. The answer turns out to be negative; however the counterexample I found it a bit ""exotic"" and I wondered if simpler examples can be found. My counterexample is based on the Cantor set , and it actually produces a smooth . First, for a closed interval , let be a smooth bump function whose support is . Specifically, for and otherwise. Let be the set of intervals removed at the -th stage of the construction of the Cantor set (of which there are ). Define by Note that all the removed intervals are pairwise disjoint, so is well-defined. Now, it's not to hard to prove that: is smooth. The set is the Cantor set . Therefore any can't be a switching point of . For every , there's a negative point and a positive point in every right and every left neighborhood of . Therefore has no switching points. Now it only remains to choose such that . This turned out to be more involved than I expected; are any simpler counterexamples, preferably ones that would appear more ""familiar"" to a standard Calculus course major?","f:[a,b]\rightarrow\mathbb{R} f(a)<0<f(b) f c\in (a,b) f\leq 0 c f\geq 0 c f I g_I I g_I(x)>0 x\in I g_I(x)=0 \mathcal{I}_n n 2^{n-1} f:[0,1]\rightarrow\mathbb R f(x)=\sum_{n=1}^\infty  \sum_{I\in\cal{I}_n}(-1)^ng_{I}(x). f f \left\{x:f(x)=0\right\} C x\not\in C f c\in C c f 0<a<b<1 f(a)<0<f(b)","['real-analysis', 'calculus', 'continuity', 'examples-counterexamples']"
30,Confused by the many versions of Hahn-Banach Separation Theorem,Confused by the many versions of Hahn-Banach Separation Theorem,,"I am studying the Hahn-Banach Separation Theorem. However, there are many versions of the theorem, which is quite confusing. So I want to ask some questions to clarify my confusion. First, some defintions. The following definitions are adopted from the book Infinite Dimensional Analysis - A Hitchhiker's Guide Let $X$ be a real vector space and $A, B \subseteq X$ be two non-empty, disjoint sets. A hyperplane is a set of the form $\{f = c\}$ , where $f: X \to \mathbb{R}$ is a non-zero linear functional and $c \in \mathbb{R}$ . The hyperplane is closed if $X$ is topological and $f$ is continuous . We say $A, B$ are weakly separated by a hyperplane $\{f = c\}$ if $ \sup f(A) \leq c \leq \inf f(B) $ . We say $A, B$ are properly separated by a hyperplane if it separates $A, B$ weakly and does not contain all of $A \cup B$ . We say $A, B$ are strictly separated by a hyperplane $\{f = c\}$ if $ f(a) < c < f(b) $ for all $a \in A, b \in B$ . We say $A, B$ are strongly separated by a hyperplane $\{f = c\}$ if $ \sup f(A) < c < \inf f(B) $ . Question 1 : Why do we need the extra condition in proper separation? Are there two non-empty, disjoint, convex sets that can be weakly separated but cannot be properly separated? The first version of Hahn-Banach Separation Theorem is purely algebraic and does not require any topological structure. Let $X$ be a real vector space and $A, B \subseteq X$ be two non-empty, disjoint, convex sets. If one of $A, B$ has an internal point, then they can be properly separated by a hyperplane. Question 2: Is it true that any internal point $x_0$ of $A$ or $B$ must satisfy $f(x_0) < c$ or $f(x_0) > c$ ? The second version is from the wiki page . Let $X$ be a locally convex topological vector space and $A, B \subseteq X$ be two non-empty, disjoint, convex sets. If $A$ has a non-empty interior , then they can be properly separated by a closed hyperplane $\{f = c\}$ . Moreover, any $x_0 \in A^\circ$ must satisfy $f(x_0) < c$ . Question 3 : The wiki page requires $X$ to be locally convex. However, from what I have read from ""Infinite Dimensional Analysis"", it seems that $X$ is only required to be a topological vector space. Does this version still hold if $X$ is merely a topological vector space? Let $X$ be a topological vector space and $A, B \subseteq X$ be two non-empty, disjoint, convex sets. If both $A, B$ are open , then they can be strictly separated by a closed hyperplane. This seems to be a direct corollary of the previous version, provided that the previous version still holds if $X$ is merely a topological vector space . Let $X$ be a locally convex topological vector space and $A, B \subseteq X$ be two non-empty, disjoint, convex sets. If $A$ is closed and $B$ is compact , then they can be strongly separated by a closed hyperplane. Question 4 : I know that if $B$ is merely closed, then we cannot even separate $A, B$ strictly, let alone strong separation. What if we merely need proper separation or weak separation? Can we separate two closed, non-empty, disjoint, convex sets properly or weakly?","I am studying the Hahn-Banach Separation Theorem. However, there are many versions of the theorem, which is quite confusing. So I want to ask some questions to clarify my confusion. First, some defintions. The following definitions are adopted from the book Infinite Dimensional Analysis - A Hitchhiker's Guide Let be a real vector space and be two non-empty, disjoint sets. A hyperplane is a set of the form , where is a non-zero linear functional and . The hyperplane is closed if is topological and is continuous . We say are weakly separated by a hyperplane if . We say are properly separated by a hyperplane if it separates weakly and does not contain all of . We say are strictly separated by a hyperplane if for all . We say are strongly separated by a hyperplane if . Question 1 : Why do we need the extra condition in proper separation? Are there two non-empty, disjoint, convex sets that can be weakly separated but cannot be properly separated? The first version of Hahn-Banach Separation Theorem is purely algebraic and does not require any topological structure. Let be a real vector space and be two non-empty, disjoint, convex sets. If one of has an internal point, then they can be properly separated by a hyperplane. Question 2: Is it true that any internal point of or must satisfy or ? The second version is from the wiki page . Let be a locally convex topological vector space and be two non-empty, disjoint, convex sets. If has a non-empty interior , then they can be properly separated by a closed hyperplane . Moreover, any must satisfy . Question 3 : The wiki page requires to be locally convex. However, from what I have read from ""Infinite Dimensional Analysis"", it seems that is only required to be a topological vector space. Does this version still hold if is merely a topological vector space? Let be a topological vector space and be two non-empty, disjoint, convex sets. If both are open , then they can be strictly separated by a closed hyperplane. This seems to be a direct corollary of the previous version, provided that the previous version still holds if is merely a topological vector space . Let be a locally convex topological vector space and be two non-empty, disjoint, convex sets. If is closed and is compact , then they can be strongly separated by a closed hyperplane. Question 4 : I know that if is merely closed, then we cannot even separate strictly, let alone strong separation. What if we merely need proper separation or weak separation? Can we separate two closed, non-empty, disjoint, convex sets properly or weakly?","X A, B \subseteq X \{f = c\} f: X \to \mathbb{R} c \in \mathbb{R} X f A, B \{f = c\}  \sup f(A) \leq c \leq \inf f(B)  A, B A, B A \cup B A, B \{f = c\}  f(a) < c < f(b)  a \in A, b \in B A, B \{f = c\}  \sup f(A) < c < \inf f(B)  X A, B \subseteq X A, B x_0 A B f(x_0) < c f(x_0) > c X A, B \subseteq X A \{f = c\} x_0 \in A^\circ f(x_0) < c X X X X A, B \subseteq X A, B X X A, B \subseteq X A B B A, B","['real-analysis', 'general-topology', 'functional-analysis']"
31,"Donald L. Cohn, ""Measure Theory"" (2nd ed., 2013), Exercises 2.4.9 and 2.5.5: are there unnecessary hypotheses, or have I misunderstood the questions?","Donald L. Cohn, ""Measure Theory"" (2nd ed., 2013), Exercises 2.4.9 and 2.5.5: are there unnecessary hypotheses, or have I misunderstood the questions?",,"From Donald L. Cohn, Measure Theory (2nd ed., 2013): [Exercise 2.4.9] Let $(X, \mathscr{A}, \mu)$ be a measure space, let $g$ be a $[0, +\infty]$ -valued integrable function on $X,$ and let $f$ and $f_t$ (for $t$ in $[0, +\infty)$ ) be real-valued $\mathscr{A}$ -measurable functions on $X$ such that $$ f(x) = \lim_{t \to +\infty} f_t(x) $$ and $$ |f_t(x)| \leq g(x) \text{ for } t \text{ in } [0, +\infty) $$ hold at almost every $x$ in $X.$ Show that $\int f\,d\mu = \lim_{t \to +\infty} \int f_t\, d\mu.$ (Hint: Give a simplified version of the argument in Example 2.4.6.) Example 2.4.6, which I needn't quote, is a justification of differentiation under the integral sign, whose relevance lies in its use of a result from an appendix: C.7. Let $A$ be a subset of $\mathbb{R}^d,$ let $f$ be a real- or complex-valued function on $A,$ and let $a$ be a limit point of $A.$ [ $\ldots$ ] One can check that $\lim_{x \to a} f(x) = L$ if and only if $\lim_n f(x_n) = L$ for every sequence $\{x_n\}$ of elements of $A,$ all different from $a,$ such that $\lim_n x_n = a.$ When doing the exercise, I was a bit worried, because I couldn't understand the reason for requiring $f$ and $f_t$ to be real-valued rather than $[-\infty, +\infty]$ -valued. I went ahead and gave this argument in my handwritten notes (because I wasn't yet worried enough to type it up in $\LaTeX{}$ for checking, or for asking a question in Maths.SE): Proposition. Let $(X, \mathscr{A}, \mu)$ be a measure space, let $g$ be a $[0, +\infty]$ -valued $\mu$ -integrable function on $X,$ and let $f$ be a $[-\infty, +\infty]$ -valued $\mathscr{A}$ -measurable function on $X.$ Let $T$ be a first-countable topological space, $A$ a subset of $T,$ and $a$ a limit point of $A.$ Suppose that $f_t,$ for $t \in A,$ is a $[-\infty, +\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ such that $$ f(x) = \lim_{\substack{t \to a \\ t \in A}} f_t(x) $$ and $$ |f_t(x)| \leqslant g(x) \text{ for } t \text{ in } A $$ hold at almost every $x$ in $X.$ Then $$ \label{cohn:measure:2:ex:2.4.9:eq:1}\tag{$1$} \int f\,d\mu = \lim_{\substack{t\to a \\ t\in A}} \int f_t(x)\,d\mu. $$ Proof. Let $(t_n)_{n \geqslant 1}$ be any sequence in $A \setminus \{a\}$ such that $\lim_{n \to \infty}t_n = a.$ Define a sequence $(h_n)_{n \geqslant 1}$ of $[-\infty, +\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ by $h_n(x) = f_{t_n}(x).$ Then $$ f(x) = \lim_{n \to \infty} h_n(x) $$ and $$ |h_n(x)| \leqslant g(x) \quad (n \geqslant 1) $$ hold for $\mu$ -almost all $x \in X.$ Therefore, by the DCT (Theorem 2.4.5 in Cohn), $$ \int f\,d\mu = \lim_{n \to \infty} \int h_n\,d\mu = \lim_{n \to \infty} \int f_{t_n}\,d\mu. $$ The required result \eqref{cohn:measure:2:ex:2.4.9:eq:1} now follows from the obvious generalisation of C.7. $\ \square$ This argument seemed (and still seems) so straightforward that I never seriously thought there could be anything badly wrong with it, until I came to the later exercise: [Exercise 2.5.5] Show that if $f \in  \mathscr{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda, \mathbb{R}),$ then $$ \int_{\mathbb{R}} f\,d\lambda = \lim_{\substack{a \to -\infty \\b \to +\infty}} \int_{[a, b]} f\,d\lambda. $$ (Hint: Use the dominated convergence theorem and a modification of the hint given for Exercise 2.4.9.) I have two worries about this exercise.  The smaller worry is the same as before: I do not see why $f$ has to be real-valued, rather than $[-\infty, +\infty]$ -valued.  The vector space properties of the set $\mathscr{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda, \mathbb{R})$ - in particular, its closure under addition - seem to play no role in the question.  Nor is Riemann integration involved, in spite of the exercise's location at the end of a section on that topic.  So these two possible explanations can be ruled out. Cohn is a careful author. (I absolutely love this book, by the way!) He does not throw unnecessary hypotheses around at random.  So there is already reason, from these two exercises, to suspect that I have misunderstood something at a pretty deep level.  Still, the point is small enough - like several other smallish difficulties I have had in studying the book on my own - that I would see no need to ask a question about it in Maths.SE, were it not for a larger worry. The larger worry is the appearance of the $\sigma$ -algebra $\mathscr{B}(\mathbb{R})$ of Borel subsets of $\mathbb{R}.$ Why isn't it $\mathscr{M}_{\lambda^*}(\mathbb{R}),$ the $\sigma$ -algebra of Lebesgue measurable subsets of $\mathbb{R}$ ? (I notice belatedly that Exercise 2.4.10 also contains a hypothesis of Borel measurability, which my long and messy handwritten answer - my computer has recently become so erratic that I have fallen back on writing notes by hand - does not appear to have used.  I won't complicate this already long question by detailing that exercise too, but I may later ask a separate question about it.) Setting these worries aside, the exercise seems straightforward (especially as I'd proved a general enough version of C.7 when doing the first exercise). Thus: Let $f \colon \mathbb{R} \to [-\infty, +\infty]$ be a Lebesgue integrable function.  The space $T = [-\infty, +\infty] \times [-\infty, +\infty]$ is first-countable, and $(-\infty, +\infty)$ is a limit point of the set $A = \{(a, b) \in T : -\infty < a < b < +\infty\},$ so the DCT (with $|f|$ as the bound), together with the generalised version of C.7, shows that $$ \int f\,d\lambda = \lim_{(a, b) \to (-\infty, +\infty)} \int f\chi_{[a, b]}\,d\lambda = \lim_{\substack{a \to -\infty \\ b \to +\infty}} \int_a^b f\,d\lambda. $$ But am I merely fooling myself into thinking that these are valid solutions?","From Donald L. Cohn, Measure Theory (2nd ed., 2013): [Exercise 2.4.9] Let be a measure space, let be a -valued integrable function on and let and (for in ) be real-valued -measurable functions on such that and hold at almost every in Show that (Hint: Give a simplified version of the argument in Example 2.4.6.) Example 2.4.6, which I needn't quote, is a justification of differentiation under the integral sign, whose relevance lies in its use of a result from an appendix: C.7. Let be a subset of let be a real- or complex-valued function on and let be a limit point of [ ] One can check that if and only if for every sequence of elements of all different from such that When doing the exercise, I was a bit worried, because I couldn't understand the reason for requiring and to be real-valued rather than -valued. I went ahead and gave this argument in my handwritten notes (because I wasn't yet worried enough to type it up in for checking, or for asking a question in Maths.SE): Proposition. Let be a measure space, let be a -valued -integrable function on and let be a -valued -measurable function on Let be a first-countable topological space, a subset of and a limit point of Suppose that for is a -valued -measurable function on such that and hold at almost every in Then Proof. Let be any sequence in such that Define a sequence of -valued -measurable functions on by Then and hold for -almost all Therefore, by the DCT (Theorem 2.4.5 in Cohn), The required result \eqref{cohn:measure:2:ex:2.4.9:eq:1} now follows from the obvious generalisation of C.7. This argument seemed (and still seems) so straightforward that I never seriously thought there could be anything badly wrong with it, until I came to the later exercise: [Exercise 2.5.5] Show that if then (Hint: Use the dominated convergence theorem and a modification of the hint given for Exercise 2.4.9.) I have two worries about this exercise.  The smaller worry is the same as before: I do not see why has to be real-valued, rather than -valued.  The vector space properties of the set - in particular, its closure under addition - seem to play no role in the question.  Nor is Riemann integration involved, in spite of the exercise's location at the end of a section on that topic.  So these two possible explanations can be ruled out. Cohn is a careful author. (I absolutely love this book, by the way!) He does not throw unnecessary hypotheses around at random.  So there is already reason, from these two exercises, to suspect that I have misunderstood something at a pretty deep level.  Still, the point is small enough - like several other smallish difficulties I have had in studying the book on my own - that I would see no need to ask a question about it in Maths.SE, were it not for a larger worry. The larger worry is the appearance of the -algebra of Borel subsets of Why isn't it the -algebra of Lebesgue measurable subsets of ? (I notice belatedly that Exercise 2.4.10 also contains a hypothesis of Borel measurability, which my long and messy handwritten answer - my computer has recently become so erratic that I have fallen back on writing notes by hand - does not appear to have used.  I won't complicate this already long question by detailing that exercise too, but I may later ask a separate question about it.) Setting these worries aside, the exercise seems straightforward (especially as I'd proved a general enough version of C.7 when doing the first exercise). Thus: Let be a Lebesgue integrable function.  The space is first-countable, and is a limit point of the set so the DCT (with as the bound), together with the generalised version of C.7, shows that But am I merely fooling myself into thinking that these are valid solutions?","(X, \mathscr{A}, \mu) g [0, +\infty] X, f f_t t [0, +\infty) \mathscr{A} X 
f(x) = \lim_{t \to +\infty} f_t(x)
 
|f_t(x)| \leq g(x) \text{ for } t \text{ in } [0, +\infty)
 x X. \int f\,d\mu = \lim_{t \to +\infty} \int f_t\, d\mu. A \mathbb{R}^d, f A, a A. \ldots \lim_{x \to a} f(x) = L \lim_n f(x_n) = L \{x_n\} A, a, \lim_n x_n = a. f f_t [-\infty, +\infty] \LaTeX{} (X, \mathscr{A}, \mu) g [0, +\infty] \mu X, f [-\infty, +\infty] \mathscr{A} X. T A T, a A. f_t, t \in A, [-\infty, +\infty] \mathscr{A} X 
f(x) = \lim_{\substack{t \to a \\ t \in A}} f_t(x)
 
|f_t(x)| \leqslant g(x) \text{ for } t \text{ in } A
 x X. 
\label{cohn:measure:2:ex:2.4.9:eq:1}\tag{1}
\int f\,d\mu = \lim_{\substack{t\to a \\ t\in A}} \int f_t(x)\,d\mu.
 (t_n)_{n \geqslant 1} A \setminus \{a\} \lim_{n \to \infty}t_n = a. (h_n)_{n \geqslant 1} [-\infty, +\infty] \mathscr{A} X h_n(x) = f_{t_n}(x). 
f(x) = \lim_{n \to \infty} h_n(x)
 
|h_n(x)| \leqslant g(x) \quad (n \geqslant 1)
 \mu x \in X. 
\int f\,d\mu = \lim_{n \to \infty} \int h_n\,d\mu =
\lim_{n \to \infty} \int f_{t_n}\,d\mu.
 \ \square f \in  \mathscr{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda,
\mathbb{R}), 
\int_{\mathbb{R}} f\,d\lambda =
\lim_{\substack{a \to -\infty \\b \to +\infty}}
\int_{[a, b]} f\,d\lambda.
 f [-\infty, +\infty] \mathscr{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda,
\mathbb{R}) \sigma \mathscr{B}(\mathbb{R}) \mathbb{R}. \mathscr{M}_{\lambda^*}(\mathbb{R}), \sigma \mathbb{R} f \colon \mathbb{R} \to [-\infty, +\infty] T = [-\infty, +\infty] \times [-\infty, +\infty] (-\infty, +\infty) A = \{(a, b) \in T : -\infty < a < b < +\infty\}, |f| 
\int f\,d\lambda = \lim_{(a, b) \to (-\infty, +\infty)}
\int f\chi_{[a, b]}\,d\lambda =
\lim_{\substack{a \to -\infty \\ b \to +\infty}}
\int_a^b f\,d\lambda.
","['real-analysis', 'measure-theory', 'solution-verification', 'lebesgue-integral']"
32,Prove that this function is increasing,Prove that this function is increasing,,"Let $g : \mathbb{N} \rightarrow \mathbb{R}_+$ with $g(0) = 0$ . Let's start by assuming that there exists positive constants $\Gamma_-\leq 1 \leq \Gamma_+$ such that $$\Gamma_- \leq g(k)-g(k-1)\leq \Gamma_+.$$ For $\phi \in \mathbb{R_+}$ , consider a random variable $X_\phi$ that assumes natural values ​​with the following distribution: $$ \mathbb{P}( X_\phi =k) = \frac{1}{Z(\phi)}\frac{\phi^k}{g(k)!} \text{, for all } k \in \mathbb{N}, $$ where $g(k)!=g(k)g(k-1)\cdot \cdot \cdot g(1)$ , $g(0)!=1$ and $Z(\phi)$ is a normalizing constant given by \begin{equation} Z(\phi)=\sum_{k=0}^{\infty}\frac{\phi^k}{g(k)!}. \end{equation} I need to show that the expectation of this random variable is a increasing function of $\phi$ . Denoting this expectation by $R(\phi)$ , a simple calculation shows that $$R(\phi)= \frac{1}{Z(\phi)}\sum_{k=0}^{\infty}\frac{k\phi^k}{g(k)!} = \phi \frac{Z^\prime(\phi)}{Z(\phi)}.$$ By deriving this expression, we get $$R'(\phi)= \frac{Z'(\phi)Z(\phi)+\phi Z''(\phi)Z(\phi)-\phi Z'(\phi)^2}{Z(\phi)^2}.$$ So we just need to show that the numerator of the above expression is positive. But I fail to show this. We can also see that $R(\phi)= \phi \frac{d}{d \phi} \ln Z(\phi)$ , but I don't know if it helps either. Any help in solving this exercise will be appreciated. Thank you!","Let with . Let's start by assuming that there exists positive constants such that For , consider a random variable that assumes natural values ​​with the following distribution: where , and is a normalizing constant given by I need to show that the expectation of this random variable is a increasing function of . Denoting this expectation by , a simple calculation shows that By deriving this expression, we get So we just need to show that the numerator of the above expression is positive. But I fail to show this. We can also see that , but I don't know if it helps either. Any help in solving this exercise will be appreciated. Thank you!","g : \mathbb{N} \rightarrow \mathbb{R}_+ g(0) = 0 \Gamma_-\leq 1 \leq \Gamma_+ \Gamma_- \leq g(k)-g(k-1)\leq \Gamma_+. \phi \in \mathbb{R_+} X_\phi  \mathbb{P}( X_\phi =k) = \frac{1}{Z(\phi)}\frac{\phi^k}{g(k)!} \text{, for all } k \in \mathbb{N},  g(k)!=g(k)g(k-1)\cdot \cdot \cdot g(1) g(0)!=1 Z(\phi) \begin{equation}
Z(\phi)=\sum_{k=0}^{\infty}\frac{\phi^k}{g(k)!}.
\end{equation} \phi R(\phi) R(\phi)= \frac{1}{Z(\phi)}\sum_{k=0}^{\infty}\frac{k\phi^k}{g(k)!} = \phi \frac{Z^\prime(\phi)}{Z(\phi)}. R'(\phi)= \frac{Z'(\phi)Z(\phi)+\phi Z''(\phi)Z(\phi)-\phi Z'(\phi)^2}{Z(\phi)^2}. R(\phi)= \phi \frac{d}{d \phi} \ln Z(\phi)","['real-analysis', 'probability', 'expected-value']"
33,How to show that $\left\|F_n^* - F\right\|={{O}_{p}}\left({{n}^{-\frac{1}{2}}}\right)$,How to show that,\left\|F_n^* - F\right\|={{O}_{p}}\left({{n}^{-\frac{1}{2}}}\right),"Let $X_1,\ldots,X_n$ be iid from a cdf $F$ on $R^d$ , $X_1^*,\ldots,X_n^*$ iid from empirical cdf $F_n$ . Let $F_n^*$ be empirical cdf based on $X_i^*$ 's. Using DKW inequality, Let $\rho\left(F_1,F_2\right)=\left\|F_1-F_2\right\|$ How to show that (a) ${{\rho }_{\infty }}\left(F_{n}^{*},F\right)\xrightarrow{a.s}0$ (b) ${{\rho }_{\infty }}\left(F_{n}^{*},F\right)={{O}_{p}}\left({{n}^{-\frac{1}{2}}}\right)$ (c) ${{\rho }_{L_p}}(F_{n}^{*},F)={{O}_{p}}({{n}^{-\frac{1}{2}}})$ My thought is  that (a) \begin{equation} \begin{aligned} P({{\rho }_{\infty }}(F_{n}^{*},F)>z)& < P({{\rho }_{\infty }}(F_{n}^{*},{{F}_{n}})+{{\rho }_{\infty}}\left({{F}_{n}},F\right)>z) \\   & < \int_{0}^{z}{P({{\rho }_{\infty }}({{F}_{n}},F)>{{z}_{1}})}P({{\rho }_{\infty }}(F_{n}^{*},{{F}_{n}})>z-{{z}_{1}})\mathrm d{{z}_{1}} \quad \text{(Correct?)}\\   & \le \int_{0}^{z}{{{C}_{\varepsilon ,d}}{{e}^{-(2-\varepsilon )nz_{1}^{2}}}{{{{C}'}}_{\varepsilon ,d}}{{e}^{-(2-\varepsilon )n{{(z-{{z}_{1}})}^{2}}}}d{{z}_{1}}} (DKW)\\   & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )nz_{1}^{2}}}{{e}^{-(2-\varepsilon )n{{(z-{{z}_{1}})}^{2}}}}d{{z}_{1}}} \\   & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )n({{z}^{2}}-2{{z}_{1}}z+2z_{1}^{2})}}d{{z}_{1}}} \\   & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}{{e}^{\frac{-(2-\varepsilon )n{{z}^{2}}}{2}}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )n2{{({{z}_{1}}-\frac{z}{2})}^{2}}}}d{{z}_{1}}\propto {{{{C}''}}_{\varepsilon ,d}}{{e}^{-{{C}_{\varepsilon ,d}}^{\prime \prime \prime }n{{z}^{2}}}}}   \end{aligned} \end{equation} \begin{equation}     \sum\limits_{n=1}^{\infty }{P({{\rho }_{\infty }}(F_{n}^{*},F)>z)\le \sum\limits_{n=1}^{\infty }{{{{{C}''}}_{\varepsilon ,d}}{{e}^{-{{C}_{\varepsilon ,d}}^{\prime \prime \prime }n{{z}^{2}}}}}}<\infty  \end{equation} \begin{equation}     {{\rho }_{\infty }}(F_{n}^{*},F)\xrightarrow{a.s}0 \end{equation} (b) From part a $P({{\rho }_{\infty }}(F_{n}^{*},F)>z)\le C{{e}^{an{{z}^{2}}}}$ , where $a,C$ are constant. Let $ C{{e}^{an{{z}^{2}}}}=\epsilon$ , then we can derive $z=\sqrt{\frac{\ln \varepsilon -\ln C}{an}}$ . So $\exists$ constant ${{D}_{\varepsilon }}=\frac{\sqrt{\ln \varepsilon -\ln C}}{\sqrt{a}}$ s.t. $\forall\epsilon>0$ , $P({{\rho }_{\infty }}(F_{n}^{*},F)>\frac{{{D}_{\varepsilon }}}{\sqrt{n}})\le \varepsilon $ . In other words, ${{\rho }_{\infty }}(F_{n}^{*},F)={{O}_{p}}({{n}^{-\frac{1}{2}}})$ . (c) $$\rho_{L_p}=\|F_n^*-F\|_{L_p}\le\|F_n^*-F_n\|_{L_p}+\|F_n-F\|_{L_p}\le[\|F_n^*-F_n\|_{\infty}\|F_n^*-F_n\|_{L_1}]^{\frac{1}{p}}+[\|F_n-F\|_{\infty}\|F_n-F\|_{L_1}]^{\frac{1}{p}}$$ Then I think we can also use DKW inequality, but I don't know how to deal with this $\|\|_{L_1}$ , do you have some ideas? I also post this question in mathoverflow https://mathoverflow.net/q/403434/362220","Let be iid from a cdf on , iid from empirical cdf . Let be empirical cdf based on 's. Using DKW inequality, Let How to show that (a) (b) (c) My thought is  that (a) (b) From part a , where are constant. Let , then we can derive . So constant s.t. , . In other words, . (c) Then I think we can also use DKW inequality, but I don't know how to deal with this , do you have some ideas? I also post this question in mathoverflow https://mathoverflow.net/q/403434/362220","X_1,\ldots,X_n F R^d X_1^*,\ldots,X_n^* F_n F_n^* X_i^* \rho\left(F_1,F_2\right)=\left\|F_1-F_2\right\| {{\rho }_{\infty }}\left(F_{n}^{*},F\right)\xrightarrow{a.s}0 {{\rho }_{\infty }}\left(F_{n}^{*},F\right)={{O}_{p}}\left({{n}^{-\frac{1}{2}}}\right) {{\rho }_{L_p}}(F_{n}^{*},F)={{O}_{p}}({{n}^{-\frac{1}{2}}}) \begin{equation}
\begin{aligned}
P({{\rho }_{\infty }}(F_{n}^{*},F)>z)& < P({{\rho }_{\infty }}(F_{n}^{*},{{F}_{n}})+{{\rho }_{\infty}}\left({{F}_{n}},F\right)>z) \\ 
 & < \int_{0}^{z}{P({{\rho }_{\infty }}({{F}_{n}},F)>{{z}_{1}})}P({{\rho }_{\infty }}(F_{n}^{*},{{F}_{n}})>z-{{z}_{1}})\mathrm d{{z}_{1}} \quad \text{(Correct?)}\\ 
 & \le \int_{0}^{z}{{{C}_{\varepsilon ,d}}{{e}^{-(2-\varepsilon )nz_{1}^{2}}}{{{{C}'}}_{\varepsilon ,d}}{{e}^{-(2-\varepsilon )n{{(z-{{z}_{1}})}^{2}}}}d{{z}_{1}}} (DKW)\\ 
 & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )nz_{1}^{2}}}{{e}^{-(2-\varepsilon )n{{(z-{{z}_{1}})}^{2}}}}d{{z}_{1}}} \\ 
 & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )n({{z}^{2}}-2{{z}_{1}}z+2z_{1}^{2})}}d{{z}_{1}}} \\ 
 & ={{C}_{\varepsilon ,d}}{{{{C}'}}_{\varepsilon ,d}}{{e}^{\frac{-(2-\varepsilon )n{{z}^{2}}}{2}}}\int_{0}^{z}{{{e}^{-(2-\varepsilon )n2{{({{z}_{1}}-\frac{z}{2})}^{2}}}}d{{z}_{1}}\propto {{{{C}''}}_{\varepsilon ,d}}{{e}^{-{{C}_{\varepsilon ,d}}^{\prime \prime \prime }n{{z}^{2}}}}}  
\end{aligned}
\end{equation} \begin{equation}
    \sum\limits_{n=1}^{\infty }{P({{\rho }_{\infty }}(F_{n}^{*},F)>z)\le \sum\limits_{n=1}^{\infty }{{{{{C}''}}_{\varepsilon ,d}}{{e}^{-{{C}_{\varepsilon ,d}}^{\prime \prime \prime }n{{z}^{2}}}}}}<\infty 
\end{equation} \begin{equation}
    {{\rho }_{\infty }}(F_{n}^{*},F)\xrightarrow{a.s}0
\end{equation} P({{\rho }_{\infty }}(F_{n}^{*},F)>z)\le C{{e}^{an{{z}^{2}}}} a,C  C{{e}^{an{{z}^{2}}}}=\epsilon z=\sqrt{\frac{\ln \varepsilon -\ln C}{an}} \exists {{D}_{\varepsilon }}=\frac{\sqrt{\ln \varepsilon -\ln C}}{\sqrt{a}} \forall\epsilon>0 P({{\rho }_{\infty }}(F_{n}^{*},F)>\frac{{{D}_{\varepsilon }}}{\sqrt{n}})\le \varepsilon  {{\rho }_{\infty }}(F_{n}^{*},F)={{O}_{p}}({{n}^{-\frac{1}{2}}}) \rho_{L_p}=\|F_n^*-F\|_{L_p}\le\|F_n^*-F_n\|_{L_p}+\|F_n-F\|_{L_p}\le[\|F_n^*-F_n\|_{\infty}\|F_n^*-F_n\|_{L_1}]^{\frac{1}{p}}+[\|F_n-F\|_{\infty}\|F_n-F\|_{L_1}]^{\frac{1}{p}} \|\|_{L_1}","['real-analysis', 'probability', 'functional-analysis', 'probability-theory', 'metric-spaces']"
34,"Finding a Solution to a Log-Linear System of Equations, or Showing Existence of Such a Solution","Finding a Solution to a Log-Linear System of Equations, or Showing Existence of Such a Solution",,"I'm trying to find the solution ( $x^*_1, y^*_1, x^*_2, y^*_2$ ) to the following system of equations: $$ gx_1=\lambda\left(\log \frac{x_2}{1-x_2}-\log \frac{x_2 + y_2}{2-x_2-y_2}\right)\\ by_1=\lambda\left(\log \frac{y_2}{1-y_2}-\log \frac{x_2 + y_2}{2-x_2-y_2}\right)\\ bx_2=\lambda\left(\log \frac{x_1}{1-x_1}-\log \frac{x_1 + y_1}{2-x_1-y_1}\right)\\ gy_2=\lambda\left(\log \frac{y_1}{1-y_1}-\log \frac{x_1 + y_1}{2-x_1-y_1}\right)\\ $$ Here, $g>0>b$ and $\lambda>0$ . I suspect that this cannot be done analytically, but would love to be corrected. If it can't be done analytically, I'd also like to prove that such a solution must exist. I'd really appreciate any help, or any possible direction in how to attempt this.","I'm trying to find the solution ( ) to the following system of equations: Here, and . I suspect that this cannot be done analytically, but would love to be corrected. If it can't be done analytically, I'd also like to prove that such a solution must exist. I'd really appreciate any help, or any possible direction in how to attempt this.","x^*_1, y^*_1, x^*_2, y^*_2 
gx_1=\lambda\left(\log \frac{x_2}{1-x_2}-\log \frac{x_2 + y_2}{2-x_2-y_2}\right)\\
by_1=\lambda\left(\log \frac{y_2}{1-y_2}-\log \frac{x_2 + y_2}{2-x_2-y_2}\right)\\
bx_2=\lambda\left(\log \frac{x_1}{1-x_1}-\log \frac{x_1 + y_1}{2-x_1-y_1}\right)\\
gy_2=\lambda\left(\log \frac{y_1}{1-y_1}-\log \frac{x_1 + y_1}{2-x_1-y_1}\right)\\
 g>0>b \lambda>0","['real-analysis', 'algebra-precalculus', 'systems-of-equations', 'fixed-point-theorems']"
35,Why do we write the rule for integration by substitution in a way that is different from what we're actually doing?,Why do we write the rule for integration by substitution in a way that is different from what we're actually doing?,,"The statement of the rule is the following: For a continuous function $f:I\rightarrow\mathbb{R}$ on a real interval $I$ , and a continuously differentiable function $\phi:[a,b]\rightarrow I$ , it holds that \begin{equation}\int_a^bf(\varphi(t))\cdot\varphi'(t)dt=\int_{\varphi(a)}^{\varphi(b)}f(x)dx\tag{1}\end{equation} My question is how one is supposed to actually use this equation, or how to make it intuitively agree with what one is actually doing, mechanically, when integrating by substitution. This has always bugged me, and I give the precise difficulties in my understanding below. I first learned integration by substitution 'procedurally', and was confronted with the actual theorem only later. To demonstrate what I mean by procedurally, consider this example integral, disregarding whether integration by substitution is useful for solving it, as that is not the purpose of my question. $$\int_1^2 t\exp(\frac{t}{2})dt$$ Naively, I could try to do the following: $$x:=\varphi(t)=\frac{t}{2} \implies t=2x$$ $$\frac{dx}{dt}=\frac{1}{2} \implies \text{'}dt=2dx\text{'}$$ and since I 'replace $x$ by $\frac{t}{2}$ ' the limits of integration become $\frac{1}{2}=0.5$ and $\frac{2}{2}=1$ . Substituting $x$ and $dx$ in my integral accordingly, I arrive at $$\int_{0.5}^1 2x \exp(x) 2dx$$ Now I see that the function $f$ which 'fits' my chosen $\varphi$ 'in hindsight' is $f(x)=4x \exp(x)$ , and indeed $f(\varphi(t))=2t\exp(\frac{t}{2})$ and $\varphi'(t)=\frac{1}{2}$ , which allows me to write my original integral in the form $$\int_1^2 2t\exp(\frac{t}{2})\frac{1}{2}dt$$ which agrees with the left-hand-side of (1) and thus justifies what I did after the fact, since $f$ is continuous and $\varphi$ is continuously differentiable on $[1,2]$ . However, assume I do not know the above procedure, and instead only equation (1). How could I apply it, not yet knowing $f$ ? If I were to just chose $\varphi(t)=\frac{t}{2}$ as above, I would be left with this partially substituted integral: $$\int_1^2t\exp(\varphi(t))dt$$ and proceed how? To go strictly by the formula, I would still need to find $f$ , whose form isn't immediate from the integral (or at least it isn't obvious to me that it would always be reasonably apparent, in particular for more complicated integrals). You might object that I could start on the other side of equation (1), defining $f(x)=x\exp(\frac{x}{2})$ , choose $\varphi(t)=2t$ , and immediately obtain $$f(\varphi(t))\varphi'(t)=2t\exp(t)2$$ However , this leads to a problem with the limits of the integral: The rule for integration by substitution does not require $\varphi$ to have an inverse. And even if it did, then to make the equation agree with what is actually done it would be more sensible to write $$\int_a^b f(x)dx = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)}f(\varphi(t))\varphi'(t)dt$$ because this is what I'm actually doing. How to amend one's mental model so equation (1) and the actual procedure agree with each other?","The statement of the rule is the following: For a continuous function on a real interval , and a continuously differentiable function , it holds that My question is how one is supposed to actually use this equation, or how to make it intuitively agree with what one is actually doing, mechanically, when integrating by substitution. This has always bugged me, and I give the precise difficulties in my understanding below. I first learned integration by substitution 'procedurally', and was confronted with the actual theorem only later. To demonstrate what I mean by procedurally, consider this example integral, disregarding whether integration by substitution is useful for solving it, as that is not the purpose of my question. Naively, I could try to do the following: and since I 'replace by ' the limits of integration become and . Substituting and in my integral accordingly, I arrive at Now I see that the function which 'fits' my chosen 'in hindsight' is , and indeed and , which allows me to write my original integral in the form which agrees with the left-hand-side of (1) and thus justifies what I did after the fact, since is continuous and is continuously differentiable on . However, assume I do not know the above procedure, and instead only equation (1). How could I apply it, not yet knowing ? If I were to just chose as above, I would be left with this partially substituted integral: and proceed how? To go strictly by the formula, I would still need to find , whose form isn't immediate from the integral (or at least it isn't obvious to me that it would always be reasonably apparent, in particular for more complicated integrals). You might object that I could start on the other side of equation (1), defining , choose , and immediately obtain However , this leads to a problem with the limits of the integral: The rule for integration by substitution does not require to have an inverse. And even if it did, then to make the equation agree with what is actually done it would be more sensible to write because this is what I'm actually doing. How to amend one's mental model so equation (1) and the actual procedure agree with each other?","f:I\rightarrow\mathbb{R} I \phi:[a,b]\rightarrow I \begin{equation}\int_a^bf(\varphi(t))\cdot\varphi'(t)dt=\int_{\varphi(a)}^{\varphi(b)}f(x)dx\tag{1}\end{equation} \int_1^2 t\exp(\frac{t}{2})dt x:=\varphi(t)=\frac{t}{2} \implies t=2x \frac{dx}{dt}=\frac{1}{2} \implies \text{'}dt=2dx\text{'} x \frac{t}{2} \frac{1}{2}=0.5 \frac{2}{2}=1 x dx \int_{0.5}^1 2x \exp(x) 2dx f \varphi f(x)=4x \exp(x) f(\varphi(t))=2t\exp(\frac{t}{2}) \varphi'(t)=\frac{1}{2} \int_1^2 2t\exp(\frac{t}{2})\frac{1}{2}dt f \varphi [1,2] f \varphi(t)=\frac{t}{2} \int_1^2t\exp(\varphi(t))dt f f(x)=x\exp(\frac{x}{2}) \varphi(t)=2t f(\varphi(t))\varphi'(t)=2t\exp(t)2 \varphi \int_a^b f(x)dx = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)}f(\varphi(t))\varphi'(t)dt","['real-analysis', 'integration', 'substitution']"
36,The $L^p$ Norm of an Integral Average,The  Norm of an Integral Average,L^p,"Im generally curious about $L^p$ estimates of integral averages. In particular, let $M$ be a (possibly noncompact) space and $f$ a (probably) smooth function with (probably, but in general maybe not) compact support. I want to consider the integral averages of f at fixed scale r>0: $$ f_r(x):=\frac{1}{vol(B_r(x))}\int_{B_r(x)}f(y)\mathrm{d}\mu(y).$$ Here M may be a Riemannian manifold, or some other space where $vol(B_r(x))$ may be non-constant, but where we assume noncollapsing so that $\inf_{x\in M}vol(B_r(x))>0$ . I would like to know what can generally be said about the $p$ norms of $f_r$ , in terms of $f$ itself. For instance, it is trivial to prove a $(1,\infty)$ estimate: $$\|f_r\|_\infty\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))}\|f\|_1.$$ I am led to suspect that an estimate like $$\|f_r\|_2\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))^{1/2}}\|f\|_1$$ should hold, perhaps via an application of the $(1,\infty)$ estimate, Holders inequality, Jensen, or something of this nature. In any case, I havent been able to pin such a thing down. Writing out the $L^2$ norm of $f_r$ leads me an estimate like $$\|f_r\|^2_2\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))}\|f\|_1\|f_r\|_1$$ which would be perfect if there were a $(1,1)$ estimate with constant $1$ . Any ideas or directions would be very appreciated!","Im generally curious about estimates of integral averages. In particular, let be a (possibly noncompact) space and a (probably) smooth function with (probably, but in general maybe not) compact support. I want to consider the integral averages of f at fixed scale r>0: Here M may be a Riemannian manifold, or some other space where may be non-constant, but where we assume noncollapsing so that . I would like to know what can generally be said about the norms of , in terms of itself. For instance, it is trivial to prove a estimate: I am led to suspect that an estimate like should hold, perhaps via an application of the estimate, Holders inequality, Jensen, or something of this nature. In any case, I havent been able to pin such a thing down. Writing out the norm of leads me an estimate like which would be perfect if there were a estimate with constant . Any ideas or directions would be very appreciated!","L^p M f  f_r(x):=\frac{1}{vol(B_r(x))}\int_{B_r(x)}f(y)\mathrm{d}\mu(y). vol(B_r(x)) \inf_{x\in M}vol(B_r(x))>0 p f_r f (1,\infty) \|f_r\|_\infty\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))}\|f\|_1. \|f_r\|_2\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))^{1/2}}\|f\|_1 (1,\infty) L^2 f_r \|f_r\|^2_2\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))}\|f\|_1\|f_r\|_1 (1,1) 1","['real-analysis', 'integration', 'functional-analysis', 'harmonic-analysis']"
37,Why does not differentiability at a point imply the existence of limit of derivative tending to the point,Why does not differentiability at a point imply the existence of limit of derivative tending to the point,,"What is wrong with the following argument? Let $f$ be a real continuous function which is differentiable at every point $x\not=a$ . By mean value theorem, $$ \frac{f(a+h)-f(a)}{h} = f'(\xi) \qquad (h\not=0)$$ where $\xi$ is between $a$ and $a+h$ . Letting $h\rightarrow0$ , we obtain $$ f'(a) = \lim\limits_{\xi\rightarrow a}f'(\xi) \mbox{.} $$ Therefore, f'(a) exists if and only if $\lim\limits_{\xi\rightarrow a}f'(\xi)$ exists. That the only if statement is false can be seen from $f(x)=x^2\sin\frac{1}{x}$ for $x\not=0$ , $f(0)=0$ , which shows that $f'(0)$ exists but $\lim\limits_{x\rightarrow0}f'(x)$ does not. Why?","What is wrong with the following argument? Let be a real continuous function which is differentiable at every point . By mean value theorem, where is between and . Letting , we obtain Therefore, f'(a) exists if and only if exists. That the only if statement is false can be seen from for , , which shows that exists but does not. Why?",f x\not=a  \frac{f(a+h)-f(a)}{h} = f'(\xi) \qquad (h\not=0) \xi a a+h h\rightarrow0  f'(a) = \lim\limits_{\xi\rightarrow a}f'(\xi) \mbox{.}  \lim\limits_{\xi\rightarrow a}f'(\xi) f(x)=x^2\sin\frac{1}{x} x\not=0 f(0)=0 f'(0) \lim\limits_{x\rightarrow0}f'(x),['real-analysis']
38,Inverse of structured block matrix,Inverse of structured block matrix,,"Let $V$ be a finite-dimensional vector space and consider the space $X=V\times V\times V\times V.$ Consider the block matrix $$A = \begin{pmatrix} A_1 & A_2 \\ A_2^* & -A_1\end{pmatrix}$$ where $A_1 = \operatorname{diag}(\lambda_1,\lambda_2)$ for $\lambda_i \in \mathbb C$ and $A_2: V^2 \to V^2.$ We then consider $$K=(A-\lambda)^{-1}.$$ Question: Can we express the resolvent in the form $$K = \begin{pmatrix} T_1(\lambda)(T_2-\lambda)^{-1} & * \\ * & T_3(\lambda) (T_4-\lambda)^{-1}\end{pmatrix}$$ where $T_1,..., T_4$ are some matrices and $T_1,T_3$ may depend smoothly on $\lambda$ , whereas $T_2,T_4$ are independent of $\lambda$ and $*$ are elements I do not really care about. Please let me know if you have any questions.","Let be a finite-dimensional vector space and consider the space Consider the block matrix where for and We then consider Question: Can we express the resolvent in the form where are some matrices and may depend smoothly on , whereas are independent of and are elements I do not really care about. Please let me know if you have any questions.","V X=V\times V\times V\times V. A = \begin{pmatrix} A_1 & A_2 \\ A_2^* & -A_1\end{pmatrix} A_1 = \operatorname{diag}(\lambda_1,\lambda_2) \lambda_i \in \mathbb C A_2: V^2 \to V^2. K=(A-\lambda)^{-1}. K = \begin{pmatrix} T_1(\lambda)(T_2-\lambda)^{-1} & * \\ * & T_3(\lambda) (T_4-\lambda)^{-1}\end{pmatrix} T_1,..., T_4 T_1,T_3 \lambda T_2,T_4 \lambda *","['real-analysis', 'linear-algebra', 'matrices', 'analysis', 'operator-theory']"
39,Sufficient conditions for being a harmonic function,Sufficient conditions for being a harmonic function,,"Let $f(x,y)$ be a real-valued function defined on an open subset $U \subset \mathbb R^2$ . Suppose that $f$ is twice differentiable separately in each variable and satisfies Laplace's equation $f_{xx} + f_{yy} = 0$ . Using the given information, we cannot conclude that $f$ is even continuous, let alone twice continuously differentiable. Therefore, we cannot guarantee that $f$ is harmonic. My questions are: Suppose we know that $f$ is continuous. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f$ is differentiable. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f$ is continuously differentiable. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f_{xy}$ and $f_{yx}$ exist. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f_{xx}$ and $f_{yy}$ are continuous. Is this sufficient to guarantee that $f$ is harmonic? Suppose we know that $f$ is twice differentiable (but not continuously so!). Is this sufficient to guarantee that $f$ is harmonic?","Let be a real-valued function defined on an open subset . Suppose that is twice differentiable separately in each variable and satisfies Laplace's equation . Using the given information, we cannot conclude that is even continuous, let alone twice continuously differentiable. Therefore, we cannot guarantee that is harmonic. My questions are: Suppose we know that is continuous. Is this sufficient to guarantee that is harmonic? Suppose we know that is differentiable. Is this sufficient to guarantee that is harmonic? Suppose we know that is continuously differentiable. Is this sufficient to guarantee that is harmonic? Suppose we know that and exist. Is this sufficient to guarantee that is harmonic? Suppose we know that and are continuous. Is this sufficient to guarantee that is harmonic? Suppose we know that is twice differentiable (but not continuously so!). Is this sufficient to guarantee that is harmonic?","f(x,y) U \subset \mathbb R^2 f f_{xx} + f_{yy} = 0 f f f f f f f f f_{xy} f_{yx} f f_{xx} f_{yy} f f f","['real-analysis', 'partial-differential-equations', 'partial-derivative', 'harmonic-functions']"
40,Flaw in Proof of Arzela-Ascoli in Carothers' Real Analysis?,Flaw in Proof of Arzela-Ascoli in Carothers' Real Analysis?,,"I came across this overview of a talk given by a CalTech student which claims that the proof of Arzela-Ascoli given in Carothers' ""Real Analysis"" is incorrect. I've been trying to find the flaw for some time now but I don't see it. Maybe there isn't one? The usual proofs I've seen involve obtaining a countably dense subset (via separability) and making a diagonal argument to find a subsequence, but this one does not do that. Here is the proof (of the just the converse direction because the forward direction is simple): Arzela-Ascoli Theorem 11.8 : Let $X$ be a compact metric space, and let $\mathcal{F}$ be a subset of $C(X)$ . Then $\mathcal{F}$ is compact if and only if $\mathcal{F}$ is closed, uniformly bounded, and equicontinuous. Proof. ( $\impliedby$ ) $ \ $ Suppose $\mathcal{F}$ is closed, uniformly bounded, and equicontinuous and let $(f_n)\in\mathcal{F}.$ We need to show that $(f_n)$ has a uniformly convergent subsequence. First note that $(f_n)$ is equicontinuous. Thus given $\epsilon>0, \exists \delta >0: d(x,y)<\delta\implies |f_n(x)-f_n(y)|<\epsilon/3, \forall n$ . Next since $X$ is totally bounded, $X$ has a finite $\delta$ -net, i.e, there exists $x_1,\ldots, x_k\in X$ such that each $x\in X$ satisfies $d(x,x_i)<\delta$ . Now since $(f_n)$ is also uniformly bounded, each of the sequences $(f_n(x_i))_{n=1}^{\infty}$ is bounded in $\mathbb{R}$ for $i=1,2,\ldots, k$ . Thus by passing  to a subsequence of the $f_n$ (and relabeling), we may suppose that $(f_n(x_i))_{n=1}^{\infty}$ converges for each $i=1,2,\ldots, k$ . In particular, we can find some $N$ such that $|f_m(x_i)-f_n(x_i)|<\epsilon/3$ for any $m,n\geq N$ and any $i=1,2,\ldots, k$ . And now we are done! Given $x\in X$ , first find $i$ such that $d(x,x_i)<\delta$ and then whenever $m,n\geq N$ , we will have \begin{align*} &|f_m(x)-f_n(x)|\\ &\leq|f_m(x)-f_m(x_i)|+|f_m(x_i)-f_n(x_i)|+|f_n(x_i)-f_n(x)|\\ &<\epsilon/3+\epsilon/3+\epsilon/3\\ &=\epsilon \end{align*} That is, $f_n$ is uniformy Cauchy, since our choice of $N$ does not depend on $x$ . Since $\mathcal{F}$ is closed in $C(X)$ by assumption, it follows that $f_n(x)$ converges to some $f\in\mathcal{F}$ , uniformly.","I came across this overview of a talk given by a CalTech student which claims that the proof of Arzela-Ascoli given in Carothers' ""Real Analysis"" is incorrect. I've been trying to find the flaw for some time now but I don't see it. Maybe there isn't one? The usual proofs I've seen involve obtaining a countably dense subset (via separability) and making a diagonal argument to find a subsequence, but this one does not do that. Here is the proof (of the just the converse direction because the forward direction is simple): Arzela-Ascoli Theorem 11.8 : Let be a compact metric space, and let be a subset of . Then is compact if and only if is closed, uniformly bounded, and equicontinuous. Proof. ( ) Suppose is closed, uniformly bounded, and equicontinuous and let We need to show that has a uniformly convergent subsequence. First note that is equicontinuous. Thus given . Next since is totally bounded, has a finite -net, i.e, there exists such that each satisfies . Now since is also uniformly bounded, each of the sequences is bounded in for . Thus by passing  to a subsequence of the (and relabeling), we may suppose that converges for each . In particular, we can find some such that for any and any . And now we are done! Given , first find such that and then whenever , we will have That is, is uniformy Cauchy, since our choice of does not depend on . Since is closed in by assumption, it follows that converges to some , uniformly.","X \mathcal{F} C(X) \mathcal{F} \mathcal{F} \impliedby  \  \mathcal{F} (f_n)\in\mathcal{F}. (f_n) (f_n) \epsilon>0, \exists \delta >0: d(x,y)<\delta\implies |f_n(x)-f_n(y)|<\epsilon/3, \forall n X X \delta x_1,\ldots, x_k\in X x\in X d(x,x_i)<\delta (f_n) (f_n(x_i))_{n=1}^{\infty} \mathbb{R} i=1,2,\ldots, k f_n (f_n(x_i))_{n=1}^{\infty} i=1,2,\ldots, k N |f_m(x_i)-f_n(x_i)|<\epsilon/3 m,n\geq N i=1,2,\ldots, k x\in X i d(x,x_i)<\delta m,n\geq N \begin{align*}
&|f_m(x)-f_n(x)|\\
&\leq|f_m(x)-f_m(x_i)|+|f_m(x_i)-f_n(x_i)|+|f_n(x_i)-f_n(x)|\\ &<\epsilon/3+\epsilon/3+\epsilon/3\\
&=\epsilon
\end{align*} f_n N x \mathcal{F} C(X) f_n(x) f\in\mathcal{F}","['real-analysis', 'solution-verification', 'arzela-ascoli']"
41,How can you prove that a function has no closed form integral?,How can you prove that a function has no closed form integral?,,"In the past, I've come across statements  along the lines of ""function $f(x)$ has no closed form integral"", which I assume means that there is no combination of the operations: addition/subtraction multiplication/division raising to powers and roots trigonometric functions exponential functions logarithmic functions which when differentiated gives the function $f(x)$ . I've heard this said about the function $f(x) = x^x$ , for example. What sort of techniques are used to prove statements like this? What is this branch of mathematics called? Merged with "" How to prove that some functions don't have a primitive "" by Ismael : Sometimes we are told that some functions like $\dfrac{\sin(x)}{x}$ don't have an indefinite integral, or that it can't be expressed in term of other simple functions. I wonder how we can prove that kind of assertion?","In the past, I've come across statements  along the lines of ""function has no closed form integral"", which I assume means that there is no combination of the operations: addition/subtraction multiplication/division raising to powers and roots trigonometric functions exponential functions logarithmic functions which when differentiated gives the function . I've heard this said about the function , for example. What sort of techniques are used to prove statements like this? What is this branch of mathematics called? Merged with "" How to prove that some functions don't have a primitive "" by Ismael : Sometimes we are told that some functions like don't have an indefinite integral, or that it can't be expressed in term of other simple functions. I wonder how we can prove that kind of assertion?",f(x) f(x) f(x) = x^x \dfrac{\sin(x)}{x},"['real-analysis', 'calculus', 'integration', 'faq', 'differential-algebra']"
42,Show integral is continuously differentiable with bounded derivative,Show integral is continuously differentiable with bounded derivative,,"I have a question to this notes , page 52. It is written that $$ \left\lvert\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\right\rvert\leq \frac{\lVert g\rVert_\infty}{\sqrt{t-s}}\frac{\lvert z\rvert e^{-\lvert z\rvert^2}}{(4\pi (t-s))^{n/2}},\quad z=\frac{x-y}{\sqrt{t-s}} $$ As the volume element has the form $dz=\frac{dx}{(t-s)^{n/2}}$ , this shows that the $x_i$ -derivative of the integrand [of $J(t,x)$ , see below] is dominated by an integrable function in $t$ and $z$ . I have two questions to this. (1) The first is a minor thing: Shouldn't the right hand side be $$ \frac{\lVert g\rVert_\infty}{\sqrt{t-s}}\frac{\lvert z\rvert e^{-\lvert z\rvert^2/4}}{(4\pi (t-s))^{n/2}}? $$ Because when I substitute $x-y=z\sqrt{t-s}$ , I get the additional $4$ in the exponent of the exponential. (2) It is also said that one can use the estimate to conclude that $J(t,x)$ is $C^1$ in $x$ with bounded derivative. How can one deduce that? Here $J(t,x)$ is defined on page 51, namely $$ J(t,x)=\int_0^t\int_{\mathbb{R}^n}\frac{1}{(4\pi (t-s))^{n/2}}e^{-(x-y)^2/4 (t-s)} g(s,y)\, dy\, ds $$ I think the boundedness of $\frac{\partial}{\partial x_i}J(t,x)$ is a consequence of the dominated convergence theorem which tells us that, due to the domination of the derivative of the integrand by an integrable function, we have $$ \frac{\partial}{\partial x_i}J(t,x)=\int_{0}^t\int_{\mathbb{R}^n}\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\, dy\, ds.\tag{*} $$ Now I use this to estimate $\left\lvert\frac{\partial}{\partial x_i}J(t,x)\right\rvert$ by substituting $z=\frac{y-x}{(t-s)^{n/2}}$ (I don't know why in the link they use $z=\frac{x-y}{(t-s)^{n/2}}$ because this produces a minus sign which is not helpful at all...): $$ \begin{align*} \left\lvert\frac{\partial}{\partial x_i}J(t,x)\right\rvert&\leq\int_0^t\int_\mathbb{R}\left\lvert\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\right\rvert\, dy\, ds\\ &\leq \frac{\lVert g\rVert_\infty}{(4\pi)^{n/2}}\int_0^t\int_\mathbb{R} \frac{\lvert z\rvert e^{-\lvert z\rvert^2/4}}{\sqrt{t-s}}\, dz\, ds=\frac{8\sqrt{t}\lVert g\rVert_\infty}{(4\pi)^{n/2}}<\infty \end{align*} $$ Thus, the derivative is bounded. Does this make sense? As to the continuity of $\frac{\partial}{\partial x_i}J(t,x)$ , this should be a direct consequence of the dominant convergence theorem and $(*)$ , see for instance Theorem 4.4.1 . Applying this theorem to the integrand of $(*)$ , we should get the continuity directly, shouldn't we? Is there a more direct way to see the continuity? Would be nice.","I have a question to this notes , page 52. It is written that As the volume element has the form , this shows that the -derivative of the integrand [of , see below] is dominated by an integrable function in and . I have two questions to this. (1) The first is a minor thing: Shouldn't the right hand side be Because when I substitute , I get the additional in the exponent of the exponential. (2) It is also said that one can use the estimate to conclude that is in with bounded derivative. How can one deduce that? Here is defined on page 51, namely I think the boundedness of is a consequence of the dominated convergence theorem which tells us that, due to the domination of the derivative of the integrand by an integrable function, we have Now I use this to estimate by substituting (I don't know why in the link they use because this produces a minus sign which is not helpful at all...): Thus, the derivative is bounded. Does this make sense? As to the continuity of , this should be a direct consequence of the dominant convergence theorem and , see for instance Theorem 4.4.1 . Applying this theorem to the integrand of , we should get the continuity directly, shouldn't we? Is there a more direct way to see the continuity? Would be nice.","
\left\lvert\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\right\rvert\leq \frac{\lVert g\rVert_\infty}{\sqrt{t-s}}\frac{\lvert z\rvert e^{-\lvert z\rvert^2}}{(4\pi (t-s))^{n/2}},\quad z=\frac{x-y}{\sqrt{t-s}}
 dz=\frac{dx}{(t-s)^{n/2}} x_i J(t,x) t z 
\frac{\lVert g\rVert_\infty}{\sqrt{t-s}}\frac{\lvert z\rvert e^{-\lvert z\rvert^2/4}}{(4\pi (t-s))^{n/2}}?
 x-y=z\sqrt{t-s} 4 J(t,x) C^1 x J(t,x) 
J(t,x)=\int_0^t\int_{\mathbb{R}^n}\frac{1}{(4\pi (t-s))^{n/2}}e^{-(x-y)^2/4 (t-s)} g(s,y)\, dy\, ds
 \frac{\partial}{\partial x_i}J(t,x) 
\frac{\partial}{\partial x_i}J(t,x)=\int_{0}^t\int_{\mathbb{R}^n}\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\, dy\, ds.\tag{*}
 \left\lvert\frac{\partial}{\partial x_i}J(t,x)\right\rvert z=\frac{y-x}{(t-s)^{n/2}} z=\frac{x-y}{(t-s)^{n/2}} 
\begin{align*}
\left\lvert\frac{\partial}{\partial x_i}J(t,x)\right\rvert&\leq\int_0^t\int_\mathbb{R}\left\lvert\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\right\rvert\, dy\, ds\\
&\leq \frac{\lVert g\rVert_\infty}{(4\pi)^{n/2}}\int_0^t\int_\mathbb{R} \frac{\lvert z\rvert e^{-\lvert z\rvert^2/4}}{\sqrt{t-s}}\, dz\, ds=\frac{8\sqrt{t}\lVert g\rVert_\infty}{(4\pi)^{n/2}}<\infty
\end{align*}
 \frac{\partial}{\partial x_i}J(t,x) (*) (*)","['real-analysis', 'analysis', 'solution-verification']"
43,"Justifying: $\frac{\mathrm d}{\mathrm d c}{_2F_1}(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm d c}\frac{(a)_k(b)_k}{(c)_k}\frac{z^k}{k!}$",Justifying:,"\frac{\mathrm d}{\mathrm d c}{_2F_1}(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm d c}\frac{(a)_k(b)_k}{(c)_k}\frac{z^k}{k!}","I was reading this paper on derivatives of the hypergeometric function $F(a,b;c;x)$ w.r.t. the parameters $a$ , $b$ , and $c$ . In the paper the authors simply state without justification $$ \tag{1} \frac{\mathrm d^n}{\mathrm dc^n}F(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d^n}{\mathrm dc^n}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}. $$ Now I am willing to accept that this is just a formal representation of the derivatives; however, from a rigorous point of view this certain does not hold in general. For example, we know the series expansion for the hypergeometric function diverges outside the unit disk (with the exception of when $a$ or $b$ are nonpositive integers). So for $|x|>1$ , $(1)$ certainly does not hold. After a little digging I found that we can differentiate functional series so long as we can justify a few criteria. In particular one needs to justify uniform convergence of the differentiated series. So my question is this: Let $x\in[0,1]$ , $a,b\in\Bbb R$ , and $c>0$ with $c-a-b>0$ so that the hypergeometric series converges absolutely for all $x\in[0,1]$ . How can we rigorously justify the differentiation of the hypergeometric series: $$ \frac{\mathrm d}{\mathrm dc}F(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm dc}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}? $$ Note that $$ \sum_{k=1}^\infty\frac{\mathrm d}{\mathrm dc}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}=-\sum_{k=1}^\infty\underbrace{\frac{(a)_k(b)_k}{(c)_k}(\psi(c+k)-\psi(c))\frac{x^k}{k!}}_{f^\prime(c)}, $$ where $\psi(z)$ is the digamma function. So we need to show that $\sum_{k=1}^\infty f^\prime(c)$ converges uniformly for all $c>0$ .","I was reading this paper on derivatives of the hypergeometric function w.r.t. the parameters , , and . In the paper the authors simply state without justification Now I am willing to accept that this is just a formal representation of the derivatives; however, from a rigorous point of view this certain does not hold in general. For example, we know the series expansion for the hypergeometric function diverges outside the unit disk (with the exception of when or are nonpositive integers). So for , certainly does not hold. After a little digging I found that we can differentiate functional series so long as we can justify a few criteria. In particular one needs to justify uniform convergence of the differentiated series. So my question is this: Let , , and with so that the hypergeometric series converges absolutely for all . How can we rigorously justify the differentiation of the hypergeometric series: Note that where is the digamma function. So we need to show that converges uniformly for all .","F(a,b;c;x) a b c 
\tag{1}
\frac{\mathrm d^n}{\mathrm dc^n}F(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d^n}{\mathrm dc^n}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}.
 a b |x|>1 (1) x\in[0,1] a,b\in\Bbb R c>0 c-a-b>0 x\in[0,1] 
\frac{\mathrm d}{\mathrm dc}F(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm dc}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}?
 
\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm dc}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}=-\sum_{k=1}^\infty\underbrace{\frac{(a)_k(b)_k}{(c)_k}(\psi(c+k)-\psi(c))\frac{x^k}{k!}}_{f^\prime(c)},
 \psi(z) \sum_{k=1}^\infty f^\prime(c) c>0","['real-analysis', 'sequences-and-series', 'uniform-convergence', 'hypergeometric-function']"
44,Total variation and bounded variation epsilon inequality,Total variation and bounded variation epsilon inequality,,"I have been trying to solve a certain exercise which i found in my Real Analysis book but have not been able to solve it. Context/Relevant definitions: Given $f:[a, b] \rightarrow \mathbb{R}$ , for each partition $P=\left\{t_{0}, \ldots, t_{n}\right\}$ in $[a, b]$ let: $$V(f ; P)=\sum_{i=1}^{n}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right|$$ When the set $\{V(f ; P) ; P=$ partition of $[a, b]\}$ is bounded , one says that f is a function of bounded variation and writes that: $$ V_{a}^{b}(f) = \sup _{p} V(f ; P) $$ Problem: Let $f$ be a continuous function of bounded variation. Show that, for each $\epsilon > 0$ , there exists $\delta > 0$ such that: $$|P|<\delta \Rightarrow\left|V(f ; P)-V_{a}^{b}(f)\right|<\varepsilon$$ where $|P|$ stands for the length of the partition. (The book does not say it, but I am thinking that $|P| = t_{n} - t_{0}$ does that make sense?) What have I tried so far? Given $f$ a continuous function of bounded variation and $\epsilon > 0$ we observe that: $$\left|V(f ; P)-V_{a}^{b}(f)\right| = \left|V(f ; P)-\sup _{p} V(f ; P)\right|$$ Now, by using the triangle inequality, we obtain that: $$ \begin{split} \left|V(f ; P)-V_{a}^{b}(f)\right| & = \left|V(f ; P)-\sup _{p} V(f ; P)\right| \\ & \leq \left |V(f ; P)\right|+\left| \sup _{p} V(f ; P) \right| \end{split}$$ Now, since $f$ is of bounded variation, the set $V(f ; P)$ is bounded and we can write that: $$\left|V(f ; P)\right|+\left| \sup _{p} V(f ; P) \right| \leq $$ $$\leq M + \left| \sup _{p} V(f ; P) \right|$$ Here I got stuck. I just don't know how to proceed. Second attempt: I tried writing what $\sup _{p} V(f ; P)$ means to see if i could do something meaningful with it, but i was only able to write that if: $$ V_{a}^{b}(f) = \sup _{p} V(f ; P)$$ Then, given $\epsilon > 0$ there exists $\Phi \in V(f ; P)$ such that: $$V_{a}^{b}(f) - \epsilon < \Phi \leq V_{a}^{b}(f)$$ which means that: $$V_{a}^{b}(f) - \epsilon < \sum_{i=1}^{n}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right| \leq V_{a}^{b}(f)$$ But got nothing good out of it. I also tried reverse engineering the inequality, but did not go so well. Can someone help? I would greatly appreciate it. Thanks in advance, Lucas","I have been trying to solve a certain exercise which i found in my Real Analysis book but have not been able to solve it. Context/Relevant definitions: Given , for each partition in let: When the set partition of is bounded , one says that f is a function of bounded variation and writes that: Problem: Let be a continuous function of bounded variation. Show that, for each , there exists such that: where stands for the length of the partition. (The book does not say it, but I am thinking that does that make sense?) What have I tried so far? Given a continuous function of bounded variation and we observe that: Now, by using the triangle inequality, we obtain that: Now, since is of bounded variation, the set is bounded and we can write that: Here I got stuck. I just don't know how to proceed. Second attempt: I tried writing what means to see if i could do something meaningful with it, but i was only able to write that if: Then, given there exists such that: which means that: But got nothing good out of it. I also tried reverse engineering the inequality, but did not go so well. Can someone help? I would greatly appreciate it. Thanks in advance, Lucas","f:[a, b] \rightarrow \mathbb{R} P=\left\{t_{0}, \ldots, t_{n}\right\} [a, b] V(f ; P)=\sum_{i=1}^{n}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right| \{V(f ; P) ; P= [a, b]\} 
V_{a}^{b}(f) = \sup _{p} V(f ; P)
 f \epsilon > 0 \delta > 0 |P|<\delta \Rightarrow\left|V(f ; P)-V_{a}^{b}(f)\right|<\varepsilon |P| |P| = t_{n} - t_{0} f \epsilon > 0 \left|V(f ; P)-V_{a}^{b}(f)\right| = \left|V(f ; P)-\sup _{p} V(f ; P)\right| 
\begin{split}
\left|V(f ; P)-V_{a}^{b}(f)\right| & = \left|V(f ; P)-\sup _{p} V(f ; P)\right| \\
& \leq \left |V(f ; P)\right|+\left| \sup _{p} V(f ; P) \right|
\end{split} f V(f ; P) \left|V(f ; P)\right|+\left| \sup _{p} V(f ; P) \right| \leq  \leq M + \left| \sup _{p} V(f ; P) \right| \sup _{p} V(f ; P)  V_{a}^{b}(f) = \sup _{p} V(f ; P) \epsilon > 0 \Phi \in V(f ; P) V_{a}^{b}(f) - \epsilon < \Phi \leq V_{a}^{b}(f) V_{a}^{b}(f) - \epsilon < \sum_{i=1}^{n}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right| \leq V_{a}^{b}(f)","['real-analysis', 'inequality', 'supremum-and-infimum', 'bounded-variation']"
45,Integration is zero implies $g$ is continuous,Integration is zero implies  is continuous,g,"Let $g$ be a monotone function such that $$ \int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y)=0$$ where $f(x,y)= 1$ if $x-y \in \mathbb{Z}$ otherwise it is $0$ . The integration is w.r.t. Riemann-Stieltjes sense. How can we show that $g$ is continuous? My try: If $g$ is not continuous at $c$ , $g$ can have only jump discontinuity at $c$ . I am not being able to use this fact. Any help or hint will be appreciated. Thanks in advance. Update: If $f$ is not continuous at a point $c \in (0,1)$ . Let us choose $\epsilon>0$ such that, $(c-\epsilon, c+ \epsilon) \subset (0,1)$ . Then, $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq \int_0^1 \int_0^1 f(x,x) \,dg(x) \,dg(x)=  \int_0^1 \int_0^1  \,dg(x) \,dg(x) = [  \int_0^1  \,dg(x) ]^2 \geq [  \int_{c -\epsilon}^{c+ \epsilon}  \,dg(x) ]^2 = [g(c+\epsilon)- g(c-\epsilon)]^2 >0$$ , which is a contradiction. Thus $g$ is continuous at $c\in (0,1)$ . If $g$ is not continuous at $0$ , then $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq [g(\epsilon)- g(0)]^2 >0$$ , a contradiction. If $g$ is not continuous at $1$ , then $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq [ g(1)-  g(1-\epsilon)]^2 >0$$ , a contradiction. Thus, $g$ is continuous on [0,1]. I want  to justify the step $$ 0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq \int_0^1 \int_0^1 f(x,x) \,dg(x) \,dg(x)$$ with proof and hope the other parts are correct. Any help or hint will be appreciated. Thanks in advance.","Let be a monotone function such that where if otherwise it is . The integration is w.r.t. Riemann-Stieltjes sense. How can we show that is continuous? My try: If is not continuous at , can have only jump discontinuity at . I am not being able to use this fact. Any help or hint will be appreciated. Thanks in advance. Update: If is not continuous at a point . Let us choose such that, . Then, , which is a contradiction. Thus is continuous at . If is not continuous at , then , a contradiction. If is not continuous at , then , a contradiction. Thus, is continuous on [0,1]. I want  to justify the step with proof and hope the other parts are correct. Any help or hint will be appreciated. Thanks in advance.","g  \int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y)=0 f(x,y)= 1 x-y \in \mathbb{Z} 0 g g c g c f c \in (0,1) \epsilon>0 (c-\epsilon, c+ \epsilon) \subset (0,1)  0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq \int_0^1 \int_0^1 f(x,x) \,dg(x) \,dg(x)=
 \int_0^1 \int_0^1  \,dg(x) \,dg(x) = [  \int_0^1  \,dg(x) ]^2 \geq [  \int_{c -\epsilon}^{c+ \epsilon}  \,dg(x) ]^2 = [g(c+\epsilon)- g(c-\epsilon)]^2 >0 g c\in (0,1) g 0  0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq [g(\epsilon)- g(0)]^2 >0 g 1  0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq [ g(1)- 
g(1-\epsilon)]^2 >0 g  0=\int_0^1 \int_0^1 f(x,y) \,dg(x) \,dg(y) \geq \int_0^1 \int_0^1 f(x,x) \,dg(x) \,dg(x)","['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis']"
46,"True or false: If the function $f + g$ is continuous and $g$ is continuous, then $f$ is also continuous","True or false: If the function  is continuous and  is continuous, then  is also continuous",f + g g f,"This question comes from Advanced Calculus by Fitzpatrick, Chapter 3, Section 3.1 Exercise 1c. I wrote a similar question today and so I think I'm getting the hang of this. My question is: is my proof/justification correct? If not, why? If the functions $ f + g : \mathbb{R} \rightarrow \mathbb{R}$ and $g : \mathbb{R} \rightarrow \mathbb{R}$ are continuous, then function $f : \mathbb{R} \rightarrow \mathbb{R}$ is also continuous My attempt Since $g$ is continuous, then so is $-g$ . Note that $(f + g) + (-g) = f$ . Since $f$ is the sum of continuous functions, $f$ is continuous. Therefore this statement is true.","This question comes from Advanced Calculus by Fitzpatrick, Chapter 3, Section 3.1 Exercise 1c. I wrote a similar question today and so I think I'm getting the hang of this. My question is: is my proof/justification correct? If not, why? If the functions and are continuous, then function is also continuous My attempt Since is continuous, then so is . Note that . Since is the sum of continuous functions, is continuous. Therefore this statement is true.", f + g : \mathbb{R} \rightarrow \mathbb{R} g : \mathbb{R} \rightarrow \mathbb{R} f : \mathbb{R} \rightarrow \mathbb{R} g -g (f + g) + (-g) = f f f,"['real-analysis', 'solution-verification']"
47,prove that $f$ is nowhere differentiable,prove that  is nowhere differentiable,f,"$$f(x)=\sum_{j=1}^\infty \left({\frac{3}{4}}\right)^{j}\sin(4^jx)$$ How can I prove that $f$ is nowhere differentiable? I know that the $j$ th summands are all continuous and bounded by $\left({\frac{3}{4}}\right)^{j}$ so by Weierstrass test, $f$ converges uniformly and therefore $f$ is continuous.","How can I prove that is nowhere differentiable? I know that the th summands are all continuous and bounded by so by Weierstrass test, converges uniformly and therefore is continuous.",f(x)=\sum_{j=1}^\infty \left({\frac{3}{4}}\right)^{j}\sin(4^jx) f j \left({\frac{3}{4}}\right)^{j} f f,"['real-analysis', 'derivatives']"
48,Exponential-like power series with Fibonnaci coefficients,Exponential-like power series with Fibonnaci coefficients,,"Background I am considering a problem (for fun, not homework!) of the exponential of a certain matrix, \begin{equation} A = \begin{bmatrix} 0 & 0 & a & b\\ 0 & 0 & c & 0\\ a & -c & 0 & 0\\b & 0 & 0 & 0\end{bmatrix}. \end{equation} This has relevance as a certain generator of transformations in SO(3,1), for example. By computing powers of the matrix, I found that only four matrices are involved in its powers: $A$ itself, these two: \begin{equation} P = \begin{bmatrix} a^2+b^2 & -ac& 0&0\\ac&-c^2&0&0\\0&0&a^2-c^2&ab\\0&0&ab&b^2\end{bmatrix},\quad \tilde{A} = \begin{bmatrix} 0 & 0 & 0 & c\\ 0 & 0 &-b&a\\0&b&0&0\\c&-a&0&0\end{bmatrix} \end{equation} and the identity matrix. Defining scalars $\alpha = a^2+b^2-c^2$ and $\beta = bc$ , the powers of $A$ are: $ A^0 = I\\ A^1 = A\\ A^2 = P\\ A^3 = \alpha A + \beta\tilde{A}\\ A^4 = \alpha P + \beta^2I\\ A^5 = (\alpha^2+\beta^2)A + \alpha\beta\widetilde{A}\\ \quad\quad\cdots $ because $A\tilde{A} = \beta I$ . I have been able to find series solutions for $\alpha = 0$ and $\beta = 0$ separately (and of course $\alpha=\beta=0$ ), because there is a clear pattern in the series coefficients for $\exp(A)$ . These correspond to certain special cases in the vectors describing the transformation (here, $\vec{A}=(0,a,b)$ and $\vec{B}=(0,0,c)$ , so the cases correspond to $\alpha=|A|^2-|B|^2=0$ and $\beta=\vec{A}\cdot\vec{B}=0$ respectively). My question is about a third interesting special case, where $\alpha=\beta\neq 0$ . In this case, the pattern in the matrix products generates a Fibonnaci sequence, so that the exponential series may be grouped into four series, \begin{equation} \exp(A) = A\sum_{k=0}^\infty \frac{\text{Fib}_{k+1}}{(2k+1)!}\alpha^k + \text{ other matrices} \end{equation} with the other series also containing similar sequences. My question is , is anyone was familiar with the function: \begin{equation} f(x) = \sum_{k=0}^\infty\frac{\text{Fib}_{k+1}}{(2k+1)!}x^{2k+1} \end{equation} I know that it has an infinite radius of convergence by ratio test. Does it relate to any special functions? Edit I have mainly resolved this thanks to Professor Vector's help! The odd powers of A can be written \begin{equation} A^{2n+1} = c_{n+1}A + d_{n+1}\tilde{A} \end{equation} where the coefficients satisfy the recurrence relations \begin{equation} c_n = \alpha c_{n-1} + \beta^2 c_{n-2},\quad c_0 = 0, c_1 = 1 \end{equation} and $d_n = \beta c_{n-1}$ . This is a second-order linear recurrence relation and has closed-form solutions (omitted for brevity). In the special case $\alpha = \beta$ , then the coefficients are $c_n = F_n\alpha^{n-1}$ . In the matrix exponential, one finds that function as the coefficient of $A$ , \begin{equation} \sum_{n=0}^\infty\frac{F_{k+1}}{(2k+1)!}\alpha^k = \frac{\sinh(\sqrt{\phi\alpha}) - \sin(\sqrt{\alpha/\phi})}{\sqrt{5}\alpha} \end{equation} where $\phi$ is the Golden ratio, using Binet's formula for $F_n$ , as Professor Vector pointed out! Interesting to see $\phi$ show up here. Thank you again for your help.","Background I am considering a problem (for fun, not homework!) of the exponential of a certain matrix, This has relevance as a certain generator of transformations in SO(3,1), for example. By computing powers of the matrix, I found that only four matrices are involved in its powers: itself, these two: and the identity matrix. Defining scalars and , the powers of are: because . I have been able to find series solutions for and separately (and of course ), because there is a clear pattern in the series coefficients for . These correspond to certain special cases in the vectors describing the transformation (here, and , so the cases correspond to and respectively). My question is about a third interesting special case, where . In this case, the pattern in the matrix products generates a Fibonnaci sequence, so that the exponential series may be grouped into four series, with the other series also containing similar sequences. My question is , is anyone was familiar with the function: I know that it has an infinite radius of convergence by ratio test. Does it relate to any special functions? Edit I have mainly resolved this thanks to Professor Vector's help! The odd powers of A can be written where the coefficients satisfy the recurrence relations and . This is a second-order linear recurrence relation and has closed-form solutions (omitted for brevity). In the special case , then the coefficients are . In the matrix exponential, one finds that function as the coefficient of , where is the Golden ratio, using Binet's formula for , as Professor Vector pointed out! Interesting to see show up here. Thank you again for your help.","\begin{equation}
A = \begin{bmatrix} 0 & 0 & a & b\\ 0 & 0 & c & 0\\ a & -c & 0 & 0\\b & 0 & 0 & 0\end{bmatrix}.
\end{equation} A \begin{equation}
P = \begin{bmatrix} a^2+b^2 & -ac& 0&0\\ac&-c^2&0&0\\0&0&a^2-c^2&ab\\0&0&ab&b^2\end{bmatrix},\quad \tilde{A} = \begin{bmatrix} 0 & 0 & 0 & c\\ 0 & 0 &-b&a\\0&b&0&0\\c&-a&0&0\end{bmatrix}
\end{equation} \alpha = a^2+b^2-c^2 \beta = bc A 
A^0 = I\\
A^1 = A\\
A^2 = P\\
A^3 = \alpha A + \beta\tilde{A}\\
A^4 = \alpha P + \beta^2I\\
A^5 = (\alpha^2+\beta^2)A + \alpha\beta\widetilde{A}\\
\quad\quad\cdots
 A\tilde{A} = \beta I \alpha = 0 \beta = 0 \alpha=\beta=0 \exp(A) \vec{A}=(0,a,b) \vec{B}=(0,0,c) \alpha=|A|^2-|B|^2=0 \beta=\vec{A}\cdot\vec{B}=0 \alpha=\beta\neq 0 \begin{equation}
\exp(A) = A\sum_{k=0}^\infty \frac{\text{Fib}_{k+1}}{(2k+1)!}\alpha^k + \text{ other matrices}
\end{equation} \begin{equation}
f(x) = \sum_{k=0}^\infty\frac{\text{Fib}_{k+1}}{(2k+1)!}x^{2k+1}
\end{equation} \begin{equation}
A^{2n+1} = c_{n+1}A + d_{n+1}\tilde{A}
\end{equation} \begin{equation}
c_n = \alpha c_{n-1} + \beta^2 c_{n-2},\quad c_0 = 0, c_1 = 1
\end{equation} d_n = \beta c_{n-1} \alpha = \beta c_n = F_n\alpha^{n-1} A \begin{equation}
\sum_{n=0}^\infty\frac{F_{k+1}}{(2k+1)!}\alpha^k = \frac{\sinh(\sqrt{\phi\alpha}) - \sin(\sqrt{\alpha/\phi})}{\sqrt{5}\alpha}
\end{equation} \phi F_n \phi","['real-analysis', 'calculus', 'sequences-and-series', 'matrices', 'group-theory']"
49,What is the Fourier transform of the bump function $e^{-\frac{1}{1-|x|^2}}$?,What is the Fourier transform of the bump function ?,e^{-\frac{1}{1-|x|^2}},"Let $$f(x):=  \left\{   \begin{array}{ll}     e^{-\frac{1}{1-|x|^2}}, & \hbox{$|x|<1$;} \\     0, & \hbox{$|x|\geq1$.}   \end{array} \right.$$ This is a generic bump function (a smooth positive bounded function with compact support). It is a Schwartz function, so it has a Schwartz Fourier transform. My question is about calculating its Fourier transform $\hat{f}$ : Since $f$ is radial (i.e. rotationally invariant),  then so is $\hat{f}$ . So $\hat{f}(\xi)=\hat{f}(|\xi|,0,...,0)$ for all $\xi \in \mathbb{R}^{n}$ . Therefore, denoting $x^\prime=(x_2,x_3,...,x_n)$ , we have $$\hat{f}(\xi)=\int_{|x|\leq 1}e^{\dot{\imath}x_{1}|\xi|}e^{-\frac{1}{1-|x|^2}}dx=\int_{|x|\leq 1}e^{\dot{\imath}x_{1}|\xi|}e^{-\frac{1}{1-|x|^2}}dx\\= \int_{-1}^{1}e^{\dot{\imath}x_{1}|\xi|}\int_{|x^\prime|\leq \sqrt{1-x_1^2}}e^{-\frac{1}{1-x_1^2-|x^\prime|^2}}dx^\prime dx_1\\= \int_{-1}^{1}e^{\dot{\imath}x_{1}|\xi|}\int_{\mathbb{S}^{n-2}}\int_{0}^{\sqrt{1-x_1^2}}e^{-\frac{1}{1-x_1^2-\rho^2}}\rho^{n-2}d\rho d\omega_{n-2} dx_1\\ =|\mathbb{S}^{n-2}|\int_{-1}^{1}e^{\dot{\imath}x_{1}|\xi|}\int_{0}^{\sqrt{1-x_1^2}}e^{-\frac{1}{1-x_1^2-\rho^2}}\rho^{n-2}d\rho  dx_1$$ And I am stuck here!","Let This is a generic bump function (a smooth positive bounded function with compact support). It is a Schwartz function, so it has a Schwartz Fourier transform. My question is about calculating its Fourier transform : Since is radial (i.e. rotationally invariant),  then so is . So for all . Therefore, denoting , we have And I am stuck here!","f(x):= 
\left\{
  \begin{array}{ll}
    e^{-\frac{1}{1-|x|^2}}, & \hbox{|x|<1;} \\
    0, & \hbox{|x|\geq1.}
  \end{array}
\right. \hat{f} f \hat{f} \hat{f}(\xi)=\hat{f}(|\xi|,0,...,0) \xi \in \mathbb{R}^{n} x^\prime=(x_2,x_3,...,x_n) \hat{f}(\xi)=\int_{|x|\leq 1}e^{\dot{\imath}x_{1}|\xi|}e^{-\frac{1}{1-|x|^2}}dx=\int_{|x|\leq 1}e^{\dot{\imath}x_{1}|\xi|}e^{-\frac{1}{1-|x|^2}}dx\\= \int_{-1}^{1}e^{\dot{\imath}x_{1}|\xi|}\int_{|x^\prime|\leq \sqrt{1-x_1^2}}e^{-\frac{1}{1-x_1^2-|x^\prime|^2}}dx^\prime dx_1\\= \int_{-1}^{1}e^{\dot{\imath}x_{1}|\xi|}\int_{\mathbb{S}^{n-2}}\int_{0}^{\sqrt{1-x_1^2}}e^{-\frac{1}{1-x_1^2-\rho^2}}\rho^{n-2}d\rho d\omega_{n-2} dx_1\\
=|\mathbb{S}^{n-2}|\int_{-1}^{1}e^{\dot{\imath}x_{1}|\xi|}\int_{0}^{\sqrt{1-x_1^2}}e^{-\frac{1}{1-x_1^2-\rho^2}}\rho^{n-2}d\rho  dx_1","['real-analysis', 'fourier-analysis', 'fourier-transform', 'harmonic-analysis']"
50,analysis/ode question,analysis/ode question,,"Prove that there does not exist any twice continuously differentiable function $f:[0,1]\to \mathbb{R}$ with $f(0)=0$ and $f(1)=1$ such that $$ - e^{-f'} f'' + 2 f = 0. $$ As an attempt, I drew a graph of what the solution should look like.  At zero, we know that $u(0)=u''(0)=0$ from the equation and at one, we know that $u(1)=1$ and $u''(1)= 2e^{u'(1)}>0$ .  Thus, we know that there must be a point $c$ such that $u'(c)=0$ and $u(c)>0$ .  But from the equation this implies that $u''(c) = 2 u(c)2e^{u'(c)}>0$ .  This seems to contradict the ability of the solution to match the boundary condition at $x=0$ , but I have trouble formalizing this.  Is there a simpler argument?","Prove that there does not exist any twice continuously differentiable function with and such that As an attempt, I drew a graph of what the solution should look like.  At zero, we know that from the equation and at one, we know that and .  Thus, we know that there must be a point such that and .  But from the equation this implies that .  This seems to contradict the ability of the solution to match the boundary condition at , but I have trouble formalizing this.  Is there a simpler argument?","f:[0,1]\to \mathbb{R} f(0)=0 f(1)=1 
- e^{-f'} f'' + 2 f = 0.
 u(0)=u''(0)=0 u(1)=1 u''(1)= 2e^{u'(1)}>0 c u'(c)=0 u(c)>0 u''(c) = 2 u(c)2e^{u'(c)}>0 x=0","['real-analysis', 'ordinary-differential-equations']"
51,Is the square root of a monotonic function whose all derivatives vanish smooth?,Is the square root of a monotonic function whose all derivatives vanish smooth?,,"Now cross-posted at MO. Let $g:[0,\infty] \to [0,\infty]$ be a smooth strictly increasing function satisfying $g(0)=0$ and $g^{(k)}(0)=0$ for every natural $k$ . Is $\sqrt g$ is infinitely (right) differentiable at $x=0$ ? I know that $\sqrt g \in C^1$ at zero*, and that in complete generality, one cannot expect for $\sqrt g$ to be even $C^2$ . However, in the counter-example given in the linked question, $g$ was not monotonic. Does this additional assumption of (strict) monotonicity save us? I tried to look at the literature, but did not find a treatment of this particular case. *The proof that $\sqrt g \in C^1$ goes by rewriting $g(x)=x^2h(x)$ where $h \ge 0$ is smooth (this is possible since $g(0)=g'(0)=0$ ). Comment: If we assume that $g''>0$ in a neighbourhood of zero (which implies that $g'>0$ ), then $\sqrt g \in C^2$ . (details below). I think that there is a chance for smoothness under the additional assumption that $g^{(k)}>0$ in a neighbourhood of zero for every $k$ , but I am not sure. The calculations become quite messy even when trying to establish $\sqrt g \in C^3$ . A proof $\sqrt g \in C^2$ when $g',g''>0$ near zero: (We use these assumptions when applying L'Hôpital's rule). $$\sqrt{g}'' = \frac{g''}{2\sqrt{g}} - \frac{(g')^2}{4g^{3/2}}.$$ Thus it is enough to prove that $(g'')^2/g\to 0$ and $(g')^4/g^3\to 0$ . $$ \lim_{x\to 0^+} \frac{(g'')^2}{g} = \lim_{x\to 0^+} 2\frac{g''g^{(3)}}{g'} = \lim_{x\to 0^+} 2\frac{g''g^{(4)}+(g^{(3)})^2}{g''} = 0, $$ where in the last equality we applied $\frac{(h')^2}{h}\to 0$ above for $h=g''$ . $$ \lim_{x\to 0^+} \frac{(g')^4}{g^3} = \lim_{x\to 0^+} \frac{4(g')^2g''}{3g^2} = \lim_{x\to 0^+} \frac{8(g'')^2 + 4g' g^{(3)}}{6g} = \lim_{x\to 0^+} \frac{2g' g^{(3)}}{3g} = \lim_{x\to 0^+} \left(\frac{2g^{(4)}}{3} + \frac{2g''g^{(3)}}{3g'}\right)=\lim_{x\to 0^+} \frac{2g''g^{(3)}}{3g'} = \lim_{x\to 0^+} \frac{2g^{(4)}}{3}+\frac{2(g^{(3)})^2}{3g''} = 0,$$ where in the first row we used the first calculation, and in the second we again applied $\frac{(h')^2}{h}\to 0$ to $h=g''$ .","Now cross-posted at MO. Let be a smooth strictly increasing function satisfying and for every natural . Is is infinitely (right) differentiable at ? I know that at zero*, and that in complete generality, one cannot expect for to be even . However, in the counter-example given in the linked question, was not monotonic. Does this additional assumption of (strict) monotonicity save us? I tried to look at the literature, but did not find a treatment of this particular case. *The proof that goes by rewriting where is smooth (this is possible since ). Comment: If we assume that in a neighbourhood of zero (which implies that ), then . (details below). I think that there is a chance for smoothness under the additional assumption that in a neighbourhood of zero for every , but I am not sure. The calculations become quite messy even when trying to establish . A proof when near zero: (We use these assumptions when applying L'Hôpital's rule). Thus it is enough to prove that and . where in the last equality we applied above for . where in the first row we used the first calculation, and in the second we again applied to .","g:[0,\infty] \to [0,\infty] g(0)=0 g^{(k)}(0)=0 k \sqrt g x=0 \sqrt g \in C^1 \sqrt g C^2 g \sqrt g \in C^1 g(x)=x^2h(x) h \ge 0 g(0)=g'(0)=0 g''>0 g'>0 \sqrt g \in C^2 g^{(k)}>0 k \sqrt g \in C^3 \sqrt g \in C^2 g',g''>0 \sqrt{g}'' = \frac{g''}{2\sqrt{g}} - \frac{(g')^2}{4g^{3/2}}. (g'')^2/g\to 0 (g')^4/g^3\to 0 
\lim_{x\to 0^+} \frac{(g'')^2}{g} = \lim_{x\to 0^+} 2\frac{g''g^{(3)}}{g'} = \lim_{x\to 0^+} 2\frac{g''g^{(4)}+(g^{(3)})^2}{g''} = 0,
 \frac{(h')^2}{h}\to 0 h=g'' 
\lim_{x\to 0^+} \frac{(g')^4}{g^3} = \lim_{x\to 0^+} \frac{4(g')^2g''}{3g^2} = \lim_{x\to 0^+} \frac{8(g'')^2 + 4g' g^{(3)}}{6g} = \lim_{x\to 0^+} \frac{2g' g^{(3)}}{3g} = \lim_{x\to 0^+} \left(\frac{2g^{(4)}}{3} + \frac{2g''g^{(3)}}{3g'}\right)=\lim_{x\to 0^+} \frac{2g''g^{(3)}}{3g'} = \lim_{x\to 0^+} \frac{2g^{(4)}}{3}+\frac{2(g^{(3)})^2}{3g''} = 0, \frac{(h')^2}{h}\to 0 h=g''","['real-analysis', 'calculus', 'derivatives', 'reference-request', 'singularity']"
52,Why do we do functional analysis over $\mathbb{C}$? Why is algebraic completeness so important?,Why do we do functional analysis over ? Why is algebraic completeness so important?,\mathbb{C},"From the perspective of functional analysis, why is it that we so often focus on vector spaces (Hilbert, Banach, whatever) over $\mathbb{C}$ instead of $\mathbb{R}$ ? Many of these objects obviously have natural real and complex versions, but I don't yet understand why we spend so much time on complex Hilbert spaces or $*$ -algebras or whatever, when my naive guess would be that the theory is simpler over $\mathbb{R}$ and we could then extrapolate from there to the complex setting. I assume that algebraic completeness factors into the equation, but while I can see the importance of working over $\mathbb{C}$ in the finite-dimensional world of linear algebra, where we have niceties like characteristic polynomials, polynomials seem themselves rather ""finitary"" in nature, and seem to often kinda disappear once we move to the infinite-dimensional setting. Where in the finite-dimensional setting we can say, ""every matrix has a characteristic polynomial, so it must have at least one eigenvalue over $\mathbb{C}$ because the characteristic polynomial will split"", once we move to even a separable space like $\ell^p(\mathbb{N})$ , we can lose that luxury. My assumption would be that when polynomials become scarce, the ability to solve them because less vital. One counterexample to this I could think of would be the density of $1$ -dimensional representations in $L^1(G)$ for a compact group $G$ . If I want to study $S^1$ , I have to study the continuous homomorphisms $S^1 \to \mathbb{T} : = \{ z \in \mathbb{C} : |z| = 1 \}$ . I can't get away with studying just the homomorphisms into the real part of $\mathbb{T}$ because that'd only leave me with the trivial homomorphism. I can still study real-valued functions on $S^1$ with Fourier analysis, but I have to drop down from the complex world to the real, not the other way around. I am starting with $\left\{ e^{2 \pi i k t} : k \in \mathbb{Z} \right\}$ and coming back to the real-valued functions on $S^1$ from there. What are some other examples of results or concepts in analysis where the complex view of the field is indispensable when compared to the merely real?","From the perspective of functional analysis, why is it that we so often focus on vector spaces (Hilbert, Banach, whatever) over instead of ? Many of these objects obviously have natural real and complex versions, but I don't yet understand why we spend so much time on complex Hilbert spaces or -algebras or whatever, when my naive guess would be that the theory is simpler over and we could then extrapolate from there to the complex setting. I assume that algebraic completeness factors into the equation, but while I can see the importance of working over in the finite-dimensional world of linear algebra, where we have niceties like characteristic polynomials, polynomials seem themselves rather ""finitary"" in nature, and seem to often kinda disappear once we move to the infinite-dimensional setting. Where in the finite-dimensional setting we can say, ""every matrix has a characteristic polynomial, so it must have at least one eigenvalue over because the characteristic polynomial will split"", once we move to even a separable space like , we can lose that luxury. My assumption would be that when polynomials become scarce, the ability to solve them because less vital. One counterexample to this I could think of would be the density of -dimensional representations in for a compact group . If I want to study , I have to study the continuous homomorphisms . I can't get away with studying just the homomorphisms into the real part of because that'd only leave me with the trivial homomorphism. I can still study real-valued functions on with Fourier analysis, but I have to drop down from the complex world to the real, not the other way around. I am starting with and coming back to the real-valued functions on from there. What are some other examples of results or concepts in analysis where the complex view of the field is indispensable when compared to the merely real?",\mathbb{C} \mathbb{R} * \mathbb{R} \mathbb{C} \mathbb{C} \ell^p(\mathbb{N}) 1 L^1(G) G S^1 S^1 \to \mathbb{T} : = \{ z \in \mathbb{C} : |z| = 1 \} \mathbb{T} S^1 \left\{ e^{2 \pi i k t} : k \in \mathbb{Z} \right\} S^1,"['real-analysis', 'linear-algebra', 'complex-analysis', 'functional-analysis', 'examples-counterexamples']"
53,$e^\pi - \pi^e < 1$? [duplicate],? [duplicate],e^\pi - \pi^e < 1,This question already has answers here : Proving that $e^{\pi}-{\pi}^e\lt 1$ without using a calculator (4 answers) Closed 4 years ago . We have Comparing $\pi^e$ and $e^\pi$ without calculating them but it doesn't give an approximation of the actual difference. Is there a way without calcualting an approximation of them to prove $e^\pi - \pi^e < 1$ ?,This question already has answers here : Proving that $e^{\pi}-{\pi}^e\lt 1$ without using a calculator (4 answers) Closed 4 years ago . We have Comparing $\pi^e$ and $e^\pi$ without calculating them but it doesn't give an approximation of the actual difference. Is there a way without calcualting an approximation of them to prove ?,e^\pi - \pi^e < 1,"['real-analysis', 'inequality', 'exponentiation', 'pi']"
54,A valid proof for the invariance of domain theorem?,A valid proof for the invariance of domain theorem?,,"The invariance of domain theorem states that, given an open subset $U\subseteq \mathbb{R}^n$ and an injective and continuous function $f:U\rightarrow\mathbb{R}^n$ then $f$ is a homeomorphism between $U$ and $f$'s image. I tried proving it by using another theorem: if $g:K\rightarrow X$ is injective and continuous, $K$ is compact and $X$ is Hausdorff then $g$ is a homeomorphism between $K$ and $f(K)$. But I'm not sure on how to prove this (sub)-theorem? or perhaps there exists an easier proof of the invariance of domain theorem?","The invariance of domain theorem states that, given an open subset $U\subseteq \mathbb{R}^n$ and an injective and continuous function $f:U\rightarrow\mathbb{R}^n$ then $f$ is a homeomorphism between $U$ and $f$'s image. I tried proving it by using another theorem: if $g:K\rightarrow X$ is injective and continuous, $K$ is compact and $X$ is Hausdorff then $g$ is a homeomorphism between $K$ and $f(K)$. But I'm not sure on how to prove this (sub)-theorem? or perhaps there exists an easier proof of the invariance of domain theorem?",,"['general-topology', 'algebraic-topology']"
55,Does Khinchin's constant have an analog for nested radicals?,Does Khinchin's constant have an analog for nested radicals?,,"Edit: as multiple users have pointed out, the premise of my question assumes some canonical representation of real numbers as infinite nested radicals. There does not seem to be any such representation. Khinchin's constant is the peculiar number $K$ such that for almost any real number $x$ , if we write out $x$ 's continued fraction representation $$x = a_0+\frac1{a_1+\frac1{\ddots}}$$ Then we have $$\lim_{n\to\infty}\sqrt[n]{a_1a_2\dots a_n} = K$$ My question begins with the fact that any real number $x$ may be written as $$x = b_0+\sqrt{b_1 + \sqrt{b_2+\dots}}$$ And, given the similarity between continued fractions and nested radicals as iterated function systems/contractions, I would think there must be some number $S$ and non-trivial function $f$ such that for almost all $x$ we have $$\lim_{n \to\infty}f(b_0,b_1 \dots b_n) = S$$ Where $f$ is probably defined independent of $b_0$ . I nervously tag this post ergodic-theory because I know Khinchin relied on it in the proof for his constant.","Edit: as multiple users have pointed out, the premise of my question assumes some canonical representation of real numbers as infinite nested radicals. There does not seem to be any such representation. Khinchin's constant is the peculiar number such that for almost any real number , if we write out 's continued fraction representation Then we have My question begins with the fact that any real number may be written as And, given the similarity between continued fractions and nested radicals as iterated function systems/contractions, I would think there must be some number and non-trivial function such that for almost all we have Where is probably defined independent of . I nervously tag this post ergodic-theory because I know Khinchin relied on it in the proof for his constant.","K x x x = a_0+\frac1{a_1+\frac1{\ddots}} \lim_{n\to\infty}\sqrt[n]{a_1a_2\dots a_n} = K x x = b_0+\sqrt{b_1 + \sqrt{b_2+\dots}} S f x \lim_{n \to\infty}f(b_0,b_1 \dots b_n) = S f b_0","['real-analysis', 'ergodic-theory', 'continued-fractions', 'nested-radicals', 'iterated-function-system']"
56,Proving that any continuously differentiable functions preserve Jordan measurability of bounded sets,Proving that any continuously differentiable functions preserve Jordan measurability of bounded sets,,"Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be continuously differentiable and $U\subset \mathbb{R}^n$ be bounded and Jordan measurable. Prove that $f(U)$ is also Jordan measurable. I know that  any bounded set is Jordan measurable iff its boundary is of measure $0$ and any continuously differentiable function maps sets of measure $0$ to sets of measure $0$ . Then I wanted to show that the boundary of $f(U)$ is contained in  the image of the boundary of $U$ , i.e. $\partial f(U)\subset f(\partial U)$ . But in this problem, $f$ is only continuously differentiable. I got stuck in carrying out this idea. Any hints? Thanks in advance! Added Put $M=\{x:\det Jf (x)=0\}$ . I realized that it suffices to prove that $f(M)$ is of measure $0$ , which looks like a conclusion of sard theorem. But in this case $f$ is only $C^1$ . Then I tried the following For $x\in M$ , it's known that $$\lim_{r\to 0}\frac{m(f(B(x,r)))}{m(B(x,r))}=0$$ where $m$ denotes the Lebesgue measure. Given $\varepsilon$ . Then for each $x\in M$ , we can choose $r_x$ such that $\frac{m(f(B(x,r)))}{m(B(x,r))} \le \varepsilon$ for all $r\le r_x$ . There exists an open cover $\bigcup_{x\in M} B(x,r_x)$ . Considering that $M$ is compact, we know that there exists a subcover $\bigcup_{1}^N B(x_i,r_{x_i})$ . Then we have $$m(f(M))\le \varepsilon \sum_1^N m( B(x_i,r_{x_i}))$$ But how can we show that $\varepsilon \sum_1^N m( B(x_i,r_{x_i}))$ can be arbitrarily small? Can we find a subcover such that $\sum_1^N m( B(x_i,r_{x_i}))\le C$ where $C$ is a constant  independent of $\varepsilon$ ? Any hints? Thanks in advance!","Let be continuously differentiable and be bounded and Jordan measurable. Prove that is also Jordan measurable. I know that  any bounded set is Jordan measurable iff its boundary is of measure and any continuously differentiable function maps sets of measure to sets of measure . Then I wanted to show that the boundary of is contained in  the image of the boundary of , i.e. . But in this problem, is only continuously differentiable. I got stuck in carrying out this idea. Any hints? Thanks in advance! Added Put . I realized that it suffices to prove that is of measure , which looks like a conclusion of sard theorem. But in this case is only . Then I tried the following For , it's known that where denotes the Lebesgue measure. Given . Then for each , we can choose such that for all . There exists an open cover . Considering that is compact, we know that there exists a subcover . Then we have But how can we show that can be arbitrarily small? Can we find a subcover such that where is a constant  independent of ? Any hints? Thanks in advance!","f:\mathbb{R}^n\to\mathbb{R}^n U\subset \mathbb{R}^n f(U) 0 0 0 f(U) U \partial f(U)\subset f(\partial U) f M=\{x:\det Jf (x)=0\} f(M) 0 f C^1 x\in M \lim_{r\to 0}\frac{m(f(B(x,r)))}{m(B(x,r))}=0 m \varepsilon x\in M r_x \frac{m(f(B(x,r)))}{m(B(x,r))} \le \varepsilon r\le r_x \bigcup_{x\in M} B(x,r_x) M \bigcup_{1}^N B(x_i,r_{x_i}) m(f(M))\le \varepsilon \sum_1^N m( B(x_i,r_{x_i})) \varepsilon \sum_1^N m( B(x_i,r_{x_i})) \sum_1^N m( B(x_i,r_{x_i}))\le C C \varepsilon","['real-analysis', 'measure-theory']"
57,"Solve the functional equation $F(xy)+xy=xy(f(x)+f(y))$,$\forall x,y,\in (0,\infty)$","Solve the functional equation ,","F(xy)+xy=xy(f(x)+f(y)) \forall x,y,\in (0,\infty)","Let $f:(0,\infty)\to \mathbb{R}$ be a function such that $f(1)=0$ . If $f$ has an antiderivative $F: (0,\infty)\to \mathbb{R}$ such that $$F(xy)+xy=xy(f(x)+f(y)),\forall x,y,\in (0,\infty),$$ find $f$ . This problem is from the Romanian magazine ""Gazeta Matematica"", No.6-7-8/2018. I would like you to review my two solutions to this problem and post any others that you may find. Solution 1: For $y=1$ in the given relation we get that $$F(x)+x=xf(x),\forall x\in (0,\infty).(*)$$ From here it follows that $f(x)=\frac{F(x)}{x}+1,\forall x\in (0,\infty)$ and therefore $f$ is differentiable. By differentiating $(*)$ we get that $$f(x)+1=f(x)+xf'(x),\forall x\in (0,\infty)\iff f'(x)=\frac{1}{x},\forall x\in (0,\infty)$$ Hence, $f(x)=\ln x+C$ , $\forall x\in (0,\infty)$ , where C is a constant. Since $f(1)=0$ , it follows that $C=0$ and $f(x)=\ln x,\forall x\in (0,\infty)$ . Solution 2: As in Solution 1, we deduce that $f$ is differentiable, so in particular it is continuous. From $f(x)=\frac{F(x)}{x}+1,\forall x\in (0,\infty)$ , we get that $f(xy)=\frac{F(xy)}{xy}+1,\forall x,y\in (0,\infty)\iff F(xy)+xy=xyf(xy),\forall x,y\in (0,\infty)$ . After substituting this back into the original equation we get that $$xyf(xy)=xy(f(x)+f(y)),\forall x,y\in (0,\infty)\iff f(xy)=f(x)+f(y),\forall x,y\in (0,\infty)$$ Now, this is Cauchy's logarithmic equation and since $f$ is continuous it follows that $f(x)=C\ln x, \forall x\in (0,\infty)$ , where $C$ is a constant. Substituting back into the original equation we get that $C=1$ , so $f(x)=\ln x$ , $\forall x\in (0,\infty)$","Let be a function such that . If has an antiderivative such that find . This problem is from the Romanian magazine ""Gazeta Matematica"", No.6-7-8/2018. I would like you to review my two solutions to this problem and post any others that you may find. Solution 1: For in the given relation we get that From here it follows that and therefore is differentiable. By differentiating we get that Hence, , , where C is a constant. Since , it follows that and . Solution 2: As in Solution 1, we deduce that is differentiable, so in particular it is continuous. From , we get that . After substituting this back into the original equation we get that Now, this is Cauchy's logarithmic equation and since is continuous it follows that , where is a constant. Substituting back into the original equation we get that , so ,","f:(0,\infty)\to \mathbb{R} f(1)=0 f F: (0,\infty)\to \mathbb{R} F(xy)+xy=xy(f(x)+f(y)),\forall x,y,\in (0,\infty), f y=1 F(x)+x=xf(x),\forall x\in (0,\infty).(*) f(x)=\frac{F(x)}{x}+1,\forall x\in (0,\infty) f (*) f(x)+1=f(x)+xf'(x),\forall x\in (0,\infty)\iff f'(x)=\frac{1}{x},\forall x\in (0,\infty) f(x)=\ln x+C \forall x\in (0,\infty) f(1)=0 C=0 f(x)=\ln x,\forall x\in (0,\infty) f f(x)=\frac{F(x)}{x}+1,\forall x\in (0,\infty) f(xy)=\frac{F(xy)}{xy}+1,\forall x,y\in (0,\infty)\iff F(xy)+xy=xyf(xy),\forall x,y\in (0,\infty) xyf(xy)=xy(f(x)+f(y)),\forall x,y\in (0,\infty)\iff f(xy)=f(x)+f(y),\forall x,y\in (0,\infty) f f(x)=C\ln x, \forall x\in (0,\infty) C C=1 f(x)=\ln x \forall x\in (0,\infty)","['real-analysis', 'proof-verification', 'functional-equations']"
58,What does Hardy mean in this lemma?,What does Hardy mean in this lemma?,,"I am concerned with the paper ""Oscillating Dirichlet's Integrals"" by G.H. Hardy (Quarterly Journal of Pure and Applied Mathematics, Vol. XLIV, pg 1-40). I don't understand Lemma 1, but I suppose this may be a problem of English comprehension? First, some notation. Hardy defines (for positive functions) $f\prec g$ (and $g\succ f $ ) to mean that $g/f \to \infty $ (as $x\to 0$ ), $f \asymp g$ to mean that $f/g \in (\delta, \Delta)$ for some constants $0<\delta <\Delta$ , $f\sim Ag$ to mean that $f/g\to A$ (for an unspecified constant $A$ , that may change from line to line) and $f\sim g$ to mean that $f/g\to 1$ (the usual asymptotic notation). Then he further states that he uses the symbols $\delta,\Delta$ to mean two different things (which cannot both occur at the same time): he writes $$ \log(1/x)^\Delta \prec (1/x)^\delta$$ to mean that the statement holds for any positive numbers $\Delta\gg1$ sufficiently large and $\delta \ll 1$ sufficiently small. At the same time, he would write $$ (1/x)^\delta \prec f \prec (1/x)^\Delta$$ to mean that there exists $\delta,\Delta>0$ such that this statement is true. Now the lemma (Lemma 1). He has already made some assumptions on the functions so that it is always true for any $f,g$ under consideration that one of $f\prec g, f \succ g, f \sim Ag$ is true. Lemma 1. If $f\succ 1, \phi \succ 1$ , then either $f\succ \phi^\Delta$ or there is a number $a$ ( $a\ge 0$ ) such that $f=\phi^a f_1$ , where $\phi^{-\delta} \prec f_1 \prec \phi^{\delta}$ . A similar result holds when $f\prec 1, \phi \prec 1$ . (Proof) For if it is not true that $f\succ \phi^\Delta$ , we can find numbers $\alpha$ such that $$ f \prec \phi^\alpha$$ and we can divide the positive real numbers $\alpha,$ including zero, with at most one exception, into two classes such that for one class $f\succ \phi^\alpha$ , and for the other $f\prec \phi^\alpha$ . There is at most one number, viz. $a$ , the number which divides the two classes, for which $f\sim A\phi^\alpha$ . If $f\sim A \phi^a$ , $a$ belongs to neither class. If, however $a$ belongs to one class or the other, and we put $f = \phi^a f_1$ , it is clear that $\phi^{-\delta}\prec f_1 \prec \phi^{\delta}$ . Thus the result of the lemma is true in either case. Some notes - ""viz."" means ""namely"" It is written $\phi^{-\delta} f_1\prec \phi^\delta$ in the original lemma statement in the paper...surely this is wrong, and I have corrected it in the above. I'm quite sure that I transcribed the $\alpha s$ and $a$ s correctly, but I cannot say for sure due to the quality of my digital copy, which I include a clip of here if there is any doubt - https://i.sstatic.net/KWaUE.png . Questions (at long last) In the lemma statement, which interpretation of $\delta,\Delta$ is being used? I guess it is $f\succ g^\Delta$ for all $\Delta \gg 1$ ? and $\phi^{-\delta} \prec f_1 \prec \phi^{\delta}$ for $\delta \ll 1$ ? The other interpretation seems impossible when he also claims later that there are $\alpha $ such that $f \succ \phi^a$ . When he claims there is at most one $a$ , is he implicitly saying also that the existence of $a$ is trivial and deserves no mention? I guess he would define $a$ as a supremum/infimum? How does he claim that there is at most one $a$ such that $f\sim A \phi^a$ , and then write "" If $f\sim A\phi^a""$ ?","I am concerned with the paper ""Oscillating Dirichlet's Integrals"" by G.H. Hardy (Quarterly Journal of Pure and Applied Mathematics, Vol. XLIV, pg 1-40). I don't understand Lemma 1, but I suppose this may be a problem of English comprehension? First, some notation. Hardy defines (for positive functions) (and ) to mean that (as ), to mean that for some constants , to mean that (for an unspecified constant , that may change from line to line) and to mean that (the usual asymptotic notation). Then he further states that he uses the symbols to mean two different things (which cannot both occur at the same time): he writes to mean that the statement holds for any positive numbers sufficiently large and sufficiently small. At the same time, he would write to mean that there exists such that this statement is true. Now the lemma (Lemma 1). He has already made some assumptions on the functions so that it is always true for any under consideration that one of is true. Lemma 1. If , then either or there is a number ( ) such that , where . A similar result holds when . (Proof) For if it is not true that , we can find numbers such that and we can divide the positive real numbers including zero, with at most one exception, into two classes such that for one class , and for the other . There is at most one number, viz. , the number which divides the two classes, for which . If , belongs to neither class. If, however belongs to one class or the other, and we put , it is clear that . Thus the result of the lemma is true in either case. Some notes - ""viz."" means ""namely"" It is written in the original lemma statement in the paper...surely this is wrong, and I have corrected it in the above. I'm quite sure that I transcribed the and s correctly, but I cannot say for sure due to the quality of my digital copy, which I include a clip of here if there is any doubt - https://i.sstatic.net/KWaUE.png . Questions (at long last) In the lemma statement, which interpretation of is being used? I guess it is for all ? and for ? The other interpretation seems impossible when he also claims later that there are such that . When he claims there is at most one , is he implicitly saying also that the existence of is trivial and deserves no mention? I guess he would define as a supremum/infimum? How does he claim that there is at most one such that , and then write "" If ?","f\prec g g\succ f  g/f \to \infty  x\to 0 f \asymp g f/g \in (\delta, \Delta) 0<\delta <\Delta f\sim Ag f/g\to A A f\sim g f/g\to 1 \delta,\Delta  \log(1/x)^\Delta \prec (1/x)^\delta \Delta\gg1 \delta \ll 1  (1/x)^\delta \prec f \prec (1/x)^\Delta \delta,\Delta>0 f,g f\prec g, f \succ g, f \sim Ag f\succ 1, \phi \succ 1 f\succ \phi^\Delta a a\ge 0 f=\phi^a f_1 \phi^{-\delta} \prec f_1 \prec \phi^{\delta} f\prec 1, \phi \prec 1 f\succ \phi^\Delta \alpha  f \prec \phi^\alpha \alpha, f\succ \phi^\alpha f\prec \phi^\alpha a f\sim A\phi^\alpha f\sim A \phi^a a a f = \phi^a f_1 \phi^{-\delta}\prec f_1 \prec \phi^{\delta} \phi^{-\delta} f_1\prec \phi^\delta \alpha s a \delta,\Delta f\succ g^\Delta \Delta \gg 1 \phi^{-\delta} \prec f_1 \prec \phi^{\delta} \delta \ll 1 \alpha  f \succ \phi^a a a a a f\sim A \phi^a f\sim A\phi^a""","['real-analysis', 'asymptotics', 'proof-explanation', 'oscillatory-integral']"
59,A multitude of challenging logarithmic integrals (Third part),A multitude of challenging logarithmic integrals (Third part),,"In this post you'll find the last group of logarithmic integrals from the new preprint ""The derivation of eighteen special challenging logarithmic integrals"" by Cornel Ioan Valean which are evaluated beautifully by fruitfully combining the Fourier series and the integral representation of the Polylogarithm given in Section 1.6, page 4, from the book (Almost) Impossible Integrals, Sums, and Series (it is also stated in the preprint - see Lemma 4), to turn the single integrals into double integrals. The following equalities hold: \begin{equation*} i) \ \int_0^1 \frac{\log ^2(x)}{x}\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right) \textrm{d}x=\frac{3}{64}\log (2)\pi ^4 +\frac{5}{48}\pi ^2 \zeta (3)-\frac{31}{128} \zeta (5); \end{equation*} \begin{equation*} ii) \ \int_0^1 \frac{\log ^2(x)}{x}\operatorname{Li}_2\left(-\frac{2 x}{1+x^2}\right) \textrm{d}x=-\frac{7}{192}\log (2)\pi ^4-\frac{1}{12}\pi ^2\zeta (3)-\frac{31}{128} \zeta (5); \end{equation*} \begin{equation*} iii) \ \int_0^1 \frac{\log^4(x)}{x}\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right) \textrm{d}x=\frac{33}{640} \log (2)\pi ^6+\frac{89}{960} \pi ^4 \zeta (3)+\frac{13}{32} \pi ^2 \zeta (5)-\frac{381}{512} \zeta (7); \end{equation*} \begin{equation*} iv) \ \int_0^1 \frac{\log^4(x)}{x}\operatorname{Li}_2\left(-\frac{2 x}{1+x^2}\right) \textrm{d}x=-\frac{31}{640}\log (2) \pi ^6-\frac{91}{960}\pi ^4 \zeta (3)-\frac{19}{64} \pi ^2 \zeta (5)-\frac{381}{512} \zeta (7); \end{equation*} \begin{equation*} v) \ \int_0^1 \frac{\log^6(x)}{x}\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right) \textrm{d}x \end{equation*} \begin{equation*} =\frac{2193}{14336}\log(2)\pi ^8+\frac{215}{768}\pi ^6 \zeta (3)+\frac{113}{128} \pi ^4 \zeta (5)+\frac{825}{256}\pi ^2 \zeta (7)-\frac{22995}{4096} \zeta (9); \end{equation*} \begin{equation*} vi) \ \int_0^1 \frac{\log^6(x)}{x}\operatorname{Li}_2\left(-\frac{2 x}{1+x^2}\right) \textrm{d}x \end{equation*} \begin{equation*} =-\frac{2159}{14336}\log (2)\pi ^8-\frac{217}{768}\pi ^6 \zeta (3)-\frac{7}{8}\pi ^4 \zeta (5)-\frac{1185}{512}\pi ^2 \zeta (7)-\frac{22995}{4096}\zeta(9). \end{equation*} Question: Is it possible to avoid completely the use of Fourier series and calculate it by another route involving harmonic series and generating functions with harmonic series? A note: Starting from these forms and getting integrals from the previous two posts, A multitude of challenging logarithmic integrals (First part) and A multitude of challenging logarithmic integrals (Second part) might be an easier task. However, starting from the previous two groups of integrals and reducing the calculations to the integrals from this post is not something obvious at all. Good to know where we start from! This is assured by the identity: A special dilogarithmic identity . Let $x<1$ be a real number. Then the following equality holds: \begin{equation*} \int_0^x \frac{t \log (1-t)}{1+t^2} \textrm{d}t=\frac{1}{4} \left(\frac{1}{2} \log^2(1+x^2)-2 \operatorname{Li}_2(x)+\frac{1}{2}\operatorname{Li}_2\left(-x^2\right)+\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right)\right), \end{equation*} where $\displaystyle \operatorname{Li}_2(x)=-\int_0^x\frac{\log(1-t)}{t}\textrm{d}t$ is the Dilogarithm function.","In this post you'll find the last group of logarithmic integrals from the new preprint ""The derivation of eighteen special challenging logarithmic integrals"" by Cornel Ioan Valean which are evaluated beautifully by fruitfully combining the Fourier series and the integral representation of the Polylogarithm given in Section 1.6, page 4, from the book (Almost) Impossible Integrals, Sums, and Series (it is also stated in the preprint - see Lemma 4), to turn the single integrals into double integrals. The following equalities hold: Question: Is it possible to avoid completely the use of Fourier series and calculate it by another route involving harmonic series and generating functions with harmonic series? A note: Starting from these forms and getting integrals from the previous two posts, A multitude of challenging logarithmic integrals (First part) and A multitude of challenging logarithmic integrals (Second part) might be an easier task. However, starting from the previous two groups of integrals and reducing the calculations to the integrals from this post is not something obvious at all. Good to know where we start from! This is assured by the identity: A special dilogarithmic identity . Let be a real number. Then the following equality holds: where is the Dilogarithm function.","\begin{equation*}
i) \ \int_0^1 \frac{\log ^2(x)}{x}\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right) \textrm{d}x=\frac{3}{64}\log (2)\pi ^4 +\frac{5}{48}\pi ^2 \zeta (3)-\frac{31}{128} \zeta (5);
\end{equation*} \begin{equation*}
ii) \ \int_0^1 \frac{\log ^2(x)}{x}\operatorname{Li}_2\left(-\frac{2 x}{1+x^2}\right) \textrm{d}x=-\frac{7}{192}\log (2)\pi ^4-\frac{1}{12}\pi ^2\zeta (3)-\frac{31}{128} \zeta (5);
\end{equation*} \begin{equation*}
iii) \ \int_0^1 \frac{\log^4(x)}{x}\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right) \textrm{d}x=\frac{33}{640} \log (2)\pi ^6+\frac{89}{960} \pi ^4 \zeta (3)+\frac{13}{32} \pi ^2 \zeta (5)-\frac{381}{512} \zeta (7);
\end{equation*} \begin{equation*}
iv) \ \int_0^1 \frac{\log^4(x)}{x}\operatorname{Li}_2\left(-\frac{2 x}{1+x^2}\right) \textrm{d}x=-\frac{31}{640}\log (2) \pi ^6-\frac{91}{960}\pi ^4 \zeta (3)-\frac{19}{64} \pi ^2 \zeta (5)-\frac{381}{512} \zeta (7);
\end{equation*} \begin{equation*}
v) \ \int_0^1 \frac{\log^6(x)}{x}\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right) \textrm{d}x
\end{equation*} \begin{equation*}
=\frac{2193}{14336}\log(2)\pi ^8+\frac{215}{768}\pi ^6 \zeta (3)+\frac{113}{128} \pi ^4 \zeta (5)+\frac{825}{256}\pi ^2 \zeta (7)-\frac{22995}{4096} \zeta (9);
\end{equation*} \begin{equation*}
vi) \ \int_0^1 \frac{\log^6(x)}{x}\operatorname{Li}_2\left(-\frac{2 x}{1+x^2}\right) \textrm{d}x
\end{equation*} \begin{equation*}
=-\frac{2159}{14336}\log (2)\pi ^8-\frac{217}{768}\pi ^6 \zeta (3)-\frac{7}{8}\pi ^4 \zeta (5)-\frac{1185}{512}\pi ^2 \zeta (7)-\frac{22995}{4096}\zeta(9).
\end{equation*} x<1 \begin{equation*}
\int_0^x \frac{t \log (1-t)}{1+t^2} \textrm{d}t=\frac{1}{4} \left(\frac{1}{2} \log^2(1+x^2)-2 \operatorname{Li}_2(x)+\frac{1}{2}\operatorname{Li}_2\left(-x^2\right)+\operatorname{Li}_2\left(\frac{2 x}{1+x^2}\right)\right),
\end{equation*} \displaystyle \operatorname{Li}_2(x)=-\int_0^x\frac{\log(1-t)}{t}\textrm{d}t","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'complex-analysis']"
60,A multitude of challenging logarithmic integrals (Second part),A multitude of challenging logarithmic integrals (Second part),,"In this post I list another group of integrals from the new preprint ""The derivation of eighteen special challenging logarithmic integrals"" by Cornel Ioan Valean where they are nicely evaluated. The following equalities hold: \begin{equation*} i) \ \int_0^1 \frac{x \log ^3(x)\log (1-x) }{1+x^2} \textrm{d}x=\frac{1761}{512} \zeta (5)-\frac{3}{32} \pi ^2 \zeta (3)-\frac{9}{256}\log (2)\pi ^4; \end{equation*} \begin{equation*} ii) \ \int_0^1 \frac{x \log ^3(x)\log (1+x) }{1+x^2} \textrm{d}x=\frac{7}{256}\log (2)\pi ^4+\frac{3}{64} \pi ^2 \zeta (3)-\frac{1215}{512} \zeta (5); \end{equation*} \begin{equation*} iii) \ \int_0^1 \frac{x\log^5(x)\log (1-x) }{1+x^2} \textrm{d}x=\frac{129495}{2048}\zeta (7)-\frac{75 }{128}\pi ^2 \zeta (5)-\frac{\pi ^4}{8} \zeta (3)-\frac{33}{512} \log (2)\pi ^6; \end{equation*} \begin{equation*} iv) \ \int_0^1 \frac{x\log^5(x)\log (1+x) }{1+x^2} \textrm{d}x=\frac{31}{512}\log(2)\pi ^6+\frac{7}{64} \pi ^4 \zeta (3)+\frac{75}{256}\pi^2 \zeta (5)-\frac{114345 }{2048}\zeta (7); \end{equation*} \begin{equation*} v) \ \int_0^1 \frac{x\log^7(x)\log (1-x) }{1+x^2} \textrm{d}x \end{equation*} \begin{equation*} =\frac{42010605}{16384}\zeta (9)-\frac{6615}{1024} \pi ^2 \zeta (7)-\frac{105}{64} \pi ^4 \zeta (5)-\frac{\pi ^6 }{2}\zeta (3)-\frac{2193}{8192}  \log (2)\pi ^8; \end{equation*} \begin{equation*} vi) \ \int_0^1 \frac{x\log^7(x)\log (1+x) }{1+x^2} \textrm{d}x \end{equation*} \begin{equation*} =\frac{2159}{8192}\log(2)\pi ^8+\frac{31}{64} \pi ^6 \zeta (3)+\frac{735}{512} \pi ^4 \zeta (5)+\frac{6615}{2048} \pi ^2 \zeta (7)-\frac{40403475}{16384} \zeta (9). \end{equation*} Question : How would you calculate the integrals by using harmonic series? Are these integrals known in the mathematical literature? For people that took interest in the Problem 11966 from The American Mathematical Monthly, \begin{equation*}  \int_0^1 \frac{x\log (1+x) }{1+x^2} \textrm{d}x, \end{equation*} you might want to observe that the integrals from this post contain some additional logs, and in a way can be viewed as more advanced integrals of the one from AMM. The Problem 11966 may also be found in the book (Almost) Impossible Integrals, Sums, and Series , and in the same book there is the identity 3.62 , page 97, (possibly new in the mathematical literature) which has been used for the calculation of these integrals in the present preprint.","In this post I list another group of integrals from the new preprint ""The derivation of eighteen special challenging logarithmic integrals"" by Cornel Ioan Valean where they are nicely evaluated. The following equalities hold: Question : How would you calculate the integrals by using harmonic series? Are these integrals known in the mathematical literature? For people that took interest in the Problem 11966 from The American Mathematical Monthly, you might want to observe that the integrals from this post contain some additional logs, and in a way can be viewed as more advanced integrals of the one from AMM. The Problem 11966 may also be found in the book (Almost) Impossible Integrals, Sums, and Series , and in the same book there is the identity 3.62 , page 97, (possibly new in the mathematical literature) which has been used for the calculation of these integrals in the present preprint.","\begin{equation*}
i) \ \int_0^1 \frac{x \log ^3(x)\log (1-x) }{1+x^2} \textrm{d}x=\frac{1761}{512} \zeta (5)-\frac{3}{32} \pi ^2 \zeta (3)-\frac{9}{256}\log (2)\pi ^4;
\end{equation*} \begin{equation*}
ii) \ \int_0^1 \frac{x \log ^3(x)\log (1+x) }{1+x^2} \textrm{d}x=\frac{7}{256}\log (2)\pi ^4+\frac{3}{64} \pi ^2 \zeta (3)-\frac{1215}{512} \zeta (5);
\end{equation*} \begin{equation*}
iii) \ \int_0^1 \frac{x\log^5(x)\log (1-x) }{1+x^2} \textrm{d}x=\frac{129495}{2048}\zeta (7)-\frac{75 }{128}\pi ^2 \zeta (5)-\frac{\pi ^4}{8} \zeta (3)-\frac{33}{512} \log (2)\pi ^6;
\end{equation*} \begin{equation*}
iv) \ \int_0^1 \frac{x\log^5(x)\log (1+x) }{1+x^2} \textrm{d}x=\frac{31}{512}\log(2)\pi ^6+\frac{7}{64} \pi ^4 \zeta (3)+\frac{75}{256}\pi^2 \zeta (5)-\frac{114345 }{2048}\zeta (7);
\end{equation*} \begin{equation*}
v) \ \int_0^1 \frac{x\log^7(x)\log (1-x) }{1+x^2} \textrm{d}x
\end{equation*} \begin{equation*}
=\frac{42010605}{16384}\zeta (9)-\frac{6615}{1024} \pi ^2 \zeta (7)-\frac{105}{64} \pi ^4 \zeta (5)-\frac{\pi ^6 }{2}\zeta (3)-\frac{2193}{8192}  \log (2)\pi ^8;
\end{equation*} \begin{equation*}
vi) \ \int_0^1 \frac{x\log^7(x)\log (1+x) }{1+x^2} \textrm{d}x
\end{equation*} \begin{equation*}
=\frac{2159}{8192}\log(2)\pi ^8+\frac{31}{64} \pi ^6 \zeta (3)+\frac{735}{512} \pi ^4 \zeta (5)+\frac{6615}{2048} \pi ^2 \zeta (7)-\frac{40403475}{16384} \zeta (9).
\end{equation*} \begin{equation*}
 \int_0^1 \frac{x\log (1+x) }{1+x^2} \textrm{d}x,
\end{equation*}","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'complex-analysis']"
61,Is $\frac{\arccos\left((\sqrt{r}+1)/(r+1/\sqrt{r})\right)}{\pi \left|1-r^{-3/2}\right|}$ analytic at $r=1$?,Is  analytic at ?,\frac{\arccos\left((\sqrt{r}+1)/(r+1/\sqrt{r})\right)}{\pi \left|1-r^{-3/2}\right|} r=1,"Is the function $f:(0,\infty)\rightarrow(0, 1)$ , defined below, analytic at $r=1$ ? $$f(r) := \frac{\arccos\left(\frac{\sqrt{r}+1}{r+\frac{1}{\sqrt{r}}}\right)}{\pi     \left|1-\frac{1}{r^{3/2}}\right|}\quad \mathrm{if\ } r>0\mathrm{\ and\ } r\neq1, $$ and $f(1) :=\frac{\sqrt{2}}{3 \pi }$ where $\arccos(x)\in [0,\pi].$ If you are interested, this function arises from retrograde motion . The following statements seem to be true: $\lim_{r\rightarrow\infty} f(r) = 1/2$ , $\lim_{r\rightarrow 0} \frac{f(r)}{r^{3/2}}=1/2$ , $f(r) = f(1/r)r^{(3/2)}$ , $g(r) = \frac{\sqrt{r}+1}{r+\frac{1}{\sqrt{r}}}$ is analytic at $r=1$ , $g(r) = 1-\frac{1}{4} (r-1)^2+\frac{1}{4} (r-1)^3-\frac{11}{64} (r-1)^4+ \frac{3}{32} (r-1)^5 -\frac{21}{512} (r-1)^6+\frac{7}{512}    (r-1)^7+ O((r-1)^8),$ $1-g(r) = \frac{(r-1)(1-1/\sqrt{r})}{r+1/\sqrt{r}}\geq 0$ , $\mathrm{sgn}(x)\arccos(1-x^2) = \sqrt{2} x + \frac{x^3}{6\sqrt{2}} +\frac{3 x^5}{80 \sqrt{2}} + \frac{5 x^7}{448 \sqrt{2}}+O(x^9)$ , and $1-1/r^{3/2} = \frac{3 (r-1)}{2}-\frac{15}{8} (r-1)^2+\frac{35}{16}    (r-1)^3-\frac{315}{128} (r-1)^4+\frac{693}{256} (r-1)^5-\frac{3003    (r-1)^6}{1024}+\frac{6435 (r-1)^7}{2048}+O\left((r-1)^8\right).$","Is the function , defined below, analytic at ? and where If you are interested, this function arises from retrograde motion . The following statements seem to be true: , , , is analytic at , , , and","f:(0,\infty)\rightarrow(0, 1) r=1 f(r) := \frac{\arccos\left(\frac{\sqrt{r}+1}{r+\frac{1}{\sqrt{r}}}\right)}{\pi 
   \left|1-\frac{1}{r^{3/2}}\right|}\quad
\mathrm{if\ } r>0\mathrm{\ and\ } r\neq1,
 f(1) :=\frac{\sqrt{2}}{3 \pi } \arccos(x)\in [0,\pi]. \lim_{r\rightarrow\infty} f(r) = 1/2 \lim_{r\rightarrow 0} \frac{f(r)}{r^{3/2}}=1/2 f(r) = f(1/r)r^{(3/2)} g(r) = \frac{\sqrt{r}+1}{r+\frac{1}{\sqrt{r}}} r=1 g(r) = 1-\frac{1}{4} (r-1)^2+\frac{1}{4} (r-1)^3-\frac{11}{64} (r-1)^4+ \frac{3}{32} (r-1)^5 -\frac{21}{512} (r-1)^6+\frac{7}{512}
   (r-1)^7+ O((r-1)^8), 1-g(r) = \frac{(r-1)(1-1/\sqrt{r})}{r+1/\sqrt{r}}\geq 0 \mathrm{sgn}(x)\arccos(1-x^2) = \sqrt{2} x + \frac{x^3}{6\sqrt{2}} +\frac{3 x^5}{80 \sqrt{2}} + \frac{5 x^7}{448 \sqrt{2}}+O(x^9) 1-1/r^{3/2} = \frac{3 (r-1)}{2}-\frac{15}{8} (r-1)^2+\frac{35}{16}
   (r-1)^3-\frac{315}{128} (r-1)^4+\frac{693}{256} (r-1)^5-\frac{3003
   (r-1)^6}{1024}+\frac{6435 (r-1)^7}{2048}+O\left((r-1)^8\right).","['real-analysis', 'mathematical-astronomy']"
62,Does the following imply Lipschitz continuity?,Does the following imply Lipschitz continuity?,,"Let $f: \mathbb{C}^d \rightarrow \mathbb{C}^d$ be a function such that there is a $c > 0$ with $$ |\langle f(x) - f(y),x-y \rangle| \leq c \langle x-y,x-y \rangle $$ for all $x, \; y \in \mathbb{C}^d$ , where $\langle \cdot , \cdot \rangle$ denotes the Hermitian product. Does this imply that $f$ is Lipschitz continuous?","Let be a function such that there is a with for all , where denotes the Hermitian product. Does this imply that is Lipschitz continuous?","f: \mathbb{C}^d \rightarrow \mathbb{C}^d c > 0 
|\langle f(x) - f(y),x-y \rangle| \leq c \langle x-y,x-y \rangle
 x, \; y \in \mathbb{C}^d \langle \cdot , \cdot \rangle f","['real-analysis', 'calculus', 'continuity', 'vector-analysis', 'lipschitz-functions']"
63,When is this rearrangement theorem for integrals true?,When is this rearrangement theorem for integrals true?,,"Bernhard Riemann proved that if $(a_n)$ is a sequence in $\mathbb{R}$ , then the sum of the infinite series $\Sigma_{n=1}^\infty a_n$ stays the same regardless of how you rearrange the terms if and only if the series $\Sigma_{n=1}^\infty |a_n|$ is convergent.  I’d like to see if something analogous for integrals is true. My question is, for what functions $f:[a,b]\rightarrow\mathbb{R}$ is it true that $\int_a^b f(g(x)) dx = \int_a^b f(x) dx$ for all bijective functions $g:[a,b]\rightarrow[a,b]$ ? Or is that too stringent a condition to be interesting, and do we need to impose some conditions on $g$ to get a more meaningful result?","Bernhard Riemann proved that if is a sequence in , then the sum of the infinite series stays the same regardless of how you rearrange the terms if and only if the series is convergent.  I’d like to see if something analogous for integrals is true. My question is, for what functions is it true that for all bijective functions ? Or is that too stringent a condition to be interesting, and do we need to impose some conditions on to get a more meaningful result?","(a_n) \mathbb{R} \Sigma_{n=1}^\infty a_n \Sigma_{n=1}^\infty |a_n| f:[a,b]\rightarrow\mathbb{R} \int_a^b f(g(x)) dx = \int_a^b f(x) dx g:[a,b]\rightarrow[a,b] g","['real-analysis', 'calculus', 'measure-theory', 'lebesgue-integral', 'riemann-integration']"
64,"Tao Analysis I, exercise 5.2.2.","Tao Analysis I, exercise 5.2.2.",,"I have proved the following statement and would like to know if my proof seems correct. Let ε > 0. Show that if $(a_n)$ and $(b_n)$ are eventually ε-close, then $(a_n)$ is bounded iff $(b_n)$ is bounded. see this snapshot from the text itself Proof: Since $a_n$ and $b_n$ are ε-close, we know that for all $ε > 0$ there exists a natural number $N$ , such that $$|a_n - b_n| \le ε,$$ $$ n\ge N $$ Since $b_n$ is bounded, $|b_n|<=M_1$ for some natural number $M_1$ . Set $ε = 1$ . Then there exist a natural number $N$ such that $$|a_n - b_n| \le 1,$$ $$ n\ge N $$ Applying the reverse triangle inequality: $$ |a_n| - |b_n| \le |a_n - b_n| \le 1$$ thus: $$ |a_n| \le 1 + |b_n| $$ For $ n < N $ , $ (a_n)$ is a sequence of finite elements and thus bounded (it is assumed this fact does not need to be proved), say by the natural number $M_2$ . For $ n \ge N, |a_n| \le  1 + M_1$ (since $b_n$ is bounded). Set $M_3 =  max(M_2, 1 + M_1)$ . Then for all $n$ , $|a_n| \le M_3$ . Thus $(a_n)$ is bounded, and the proof is completed.","I have proved the following statement and would like to know if my proof seems correct. Let ε > 0. Show that if and are eventually ε-close, then is bounded iff is bounded. see this snapshot from the text itself Proof: Since and are ε-close, we know that for all there exists a natural number , such that Since is bounded, for some natural number . Set . Then there exist a natural number such that Applying the reverse triangle inequality: thus: For , is a sequence of finite elements and thus bounded (it is assumed this fact does not need to be proved), say by the natural number . For (since is bounded). Set . Then for all , . Thus is bounded, and the proof is completed.","(a_n) (b_n) (a_n) (b_n) a_n b_n ε > 0 N |a_n - b_n| \le ε,  n\ge N  b_n |b_n|<=M_1 M_1 ε = 1 N |a_n - b_n| \le 1,  n\ge N   |a_n| - |b_n| \le |a_n - b_n| \le 1  |a_n| \le 1 + |b_n|   n < N   (a_n) M_2  n \ge N, |a_n| \le  1 + M_1 b_n M_3 =  max(M_2, 1 + M_1) n |a_n| \le M_3 (a_n)","['real-analysis', 'proof-verification']"
65,"If $f$ is continuous on $[a,b]$ and $g$ equals $f$ except at $x=c\in(a,b)$ then $\int_{a}^{b} f = \int_{a}^{b} g$.",If  is continuous on  and  equals  except at  then .,"f [a,b] g f x=c\in(a,b) \int_{a}^{b} f = \int_{a}^{b} g","If $f$ is continuous on $[a,b]$ and $g$ equals $f$ except at $x=c \in (a,b)$ where $g$ is defined arbitrarily. Then prove that $g$ is integrable on $[a,b]$ and $\int_{a}^{b} f = \int_{a}^{b} g$ . Here's my attempt: I will be using this to prove it: Let $f$ be bounded on $[a,b]$ and continuous everywhere except at one   point $c$ in $[a,b]$ . Then $f$ is integrable on $[a,b]$ . Clearly, if $f$ is continuous on $[a,b]$ , then there is some $M\in\mathbb{R}^{+}$ such that $|f(x)| \le M$ . Hence, it must be that $|g(x)| \le \max \{ M , g(c) \} $ . Hence, $g(x)$ is bounded above and is continuous on $[a,b]$ except at $x=c$ . Thus, by the theorem above, it is integrable. Now, we need to show that $\int_{a}^{b} f =\int_{a}^{b} g$ . If we assume that $f(c) < g(c)$ (the other case may be handled similarly), then $f(x) \le g(x)$ for all $x \in [a,b]$ and so, $\int_{a}^{b} f \le \int_{a}^{b} g$ . Let $\varepsilon >0$ be given. Then there exists a partition $P$ of $[a,b]$ such that $\int_{a}^{b} g - \varepsilon  < L(P,g)$ . Now we claim that for any partition $P$ of $[a,b]$ , we have $L(P,g)=L(P,f)$ (we prove this claim later). Hence it follows that $\int_{a}^{b} g - \varepsilon  < L(P,g) = L(P,f) \le \int_{a}^{b} f$ . Since, $\varepsilon$ was arbitrary, we have $\int_{a}^{b} g \le \int_{a}^{b} f$ . Hence, $\int_{a}^{b} f = \int_{a}^{b} g$ . Proof of my claim: Let $P$ be any partition of $[a,b]$ . We need to show that $L(P,g) = L(P,f)$ . First, we note that there must be some $i$ such that $c\in[x_{i-1} , x_{i}]$ . Also, $f(x) \le g(x)$ on $[x_{i-1} , x_{i}]$ , hence, $\inf_{[x_{i-1} , x_{i}]} f(x) \le \inf_{[x_{i-1} , x_{i}]} g(x)$ . Also, since $f$ is continuous on $[x_{i-1} , x_{i}]$ , there must be some $y\in[x_{i-1} , x_{i}]$ such that $f(y) =\inf_{[x_{i-1} , x_{i}]} f(x)$ by the Extreme Value Theorem. If $y \ne c$ then it is clear that $f(y) = g(y) =\inf_{[x_{i-1} , x_{i}]} g(x)$ otherwise if $y=c$ , then for any $\varepsilon >0$ , $\inf_{[x_{i-1} , x_{i}]} g(x) \le g(z)=f(z) < f(c)+\varepsilon$ for some $z \in [x_{i-1}, x_{i}]\setminus \{c\}$ ( for otherwise it would contradict our assumption that $f(c) < g(c)$ ). Since, $\varepsilon$ was arbitrary, $\inf_{[x_{i-1} , x_{i}]} g(x) \le  f(c)$ . Thus, $\inf_{[x_{i-1} , x_{i}]} f(x) = \inf_{[x_{i-1} , x_{i}]} g(x)$ . Since, this is true for the rest of the subintervals as well, we must have that $L(P,g) = L(P,f)$ . Is my proof correct? I'm a bit unsure about the last part. Any alternative proofs? EDIT: Found a easier way to complete the second part . Thanks to the hint provided by Kavi Rama Murthy. Since, we've established that $g$ is integrable, consider the function $h:[a,b] \to \mathbb{R}$ given by $h(x)=f(x)-g(x)$ . Then $h$ is integrable. Also, note that we're assuming here that $f(c) < g(c)$ . Let $P$ be any partition of $[a,b]$ . Then $U(P,h)=0$ because $M_k(h)=0$ for any subintervals. Hence, it must be that $\int_{a}^{b} h = \overline{\int_{a}^{b}} h = \inf \{ U(P,h) \} =0$ . Thus, again by linearity, we have $\int_{a}^{b} f = \int_{a}^{b}g$ .","If is continuous on and equals except at where is defined arbitrarily. Then prove that is integrable on and . Here's my attempt: I will be using this to prove it: Let be bounded on and continuous everywhere except at one   point in . Then is integrable on . Clearly, if is continuous on , then there is some such that . Hence, it must be that . Hence, is bounded above and is continuous on except at . Thus, by the theorem above, it is integrable. Now, we need to show that . If we assume that (the other case may be handled similarly), then for all and so, . Let be given. Then there exists a partition of such that . Now we claim that for any partition of , we have (we prove this claim later). Hence it follows that . Since, was arbitrary, we have . Hence, . Proof of my claim: Let be any partition of . We need to show that . First, we note that there must be some such that . Also, on , hence, . Also, since is continuous on , there must be some such that by the Extreme Value Theorem. If then it is clear that otherwise if , then for any , for some ( for otherwise it would contradict our assumption that ). Since, was arbitrary, . Thus, . Since, this is true for the rest of the subintervals as well, we must have that . Is my proof correct? I'm a bit unsure about the last part. Any alternative proofs? EDIT: Found a easier way to complete the second part . Thanks to the hint provided by Kavi Rama Murthy. Since, we've established that is integrable, consider the function given by . Then is integrable. Also, note that we're assuming here that . Let be any partition of . Then because for any subintervals. Hence, it must be that . Thus, again by linearity, we have .","f [a,b] g f x=c \in (a,b) g g [a,b] \int_{a}^{b} f =
\int_{a}^{b} g f [a,b] c [a,b] f [a,b] f [a,b] M\in\mathbb{R}^{+} |f(x)| \le M |g(x)| \le \max \{ M , g(c) \}  g(x) [a,b] x=c \int_{a}^{b} f =\int_{a}^{b} g f(c) < g(c) f(x) \le g(x) x \in [a,b] \int_{a}^{b} f \le \int_{a}^{b} g \varepsilon >0 P [a,b] \int_{a}^{b} g - \varepsilon  < L(P,g) P [a,b] L(P,g)=L(P,f) \int_{a}^{b} g - \varepsilon  < L(P,g) = L(P,f) \le \int_{a}^{b} f \varepsilon \int_{a}^{b} g \le \int_{a}^{b} f \int_{a}^{b} f =
\int_{a}^{b} g P [a,b] L(P,g) = L(P,f) i c\in[x_{i-1} , x_{i}] f(x) \le g(x) [x_{i-1} , x_{i}] \inf_{[x_{i-1} , x_{i}]} f(x) \le \inf_{[x_{i-1} , x_{i}]} g(x) f [x_{i-1} , x_{i}] y\in[x_{i-1} , x_{i}] f(y) =\inf_{[x_{i-1} , x_{i}]} f(x) y \ne c f(y) = g(y) =\inf_{[x_{i-1} , x_{i}]} g(x) y=c \varepsilon >0 \inf_{[x_{i-1} , x_{i}]} g(x) \le g(z)=f(z) < f(c)+\varepsilon z \in [x_{i-1}, x_{i}]\setminus \{c\} f(c) < g(c) \varepsilon \inf_{[x_{i-1} , x_{i}]} g(x) \le  f(c) \inf_{[x_{i-1} , x_{i}]} f(x) = \inf_{[x_{i-1} , x_{i}]} g(x) L(P,g) = L(P,f) g h:[a,b] \to \mathbb{R} h(x)=f(x)-g(x) h f(c) < g(c) P [a,b] U(P,h)=0 M_k(h)=0 \int_{a}^{b} h = \overline{\int_{a}^{b}} h = \inf \{ U(P,h) \} =0 \int_{a}^{b} f = \int_{a}^{b}g","['real-analysis', 'proof-verification', 'alternative-proof', 'riemann-integration']"
66,When Bochner and Pettis integrals coincide?,When Bochner and Pettis integrals coincide?,,"A well-known fact is that if a space $X$ is reflexive, then Dunford and Pettis integrals coincide. But I'm inerested in the relation between Bochner and Pettis integral. It seems obvious that if a function is Bochner integrable, then it is Pettis integrable as well. What are the necessary conditions so that they coincide? I thought that $X$ being separable is enough. Then the function we integrate becomes strongly measurable. Is it enough, or maybe separability is too much?","A well-known fact is that if a space is reflexive, then Dunford and Pettis integrals coincide. But I'm inerested in the relation between Bochner and Pettis integral. It seems obvious that if a function is Bochner integrable, then it is Pettis integrable as well. What are the necessary conditions so that they coincide? I thought that being separable is enough. Then the function we integrate becomes strongly measurable. Is it enough, or maybe separability is too much?",X X,"['real-analysis', 'integration', 'functional-analysis']"
67,On the solvability of the Dirichlet Problem $\Delta u =f$ for $f$ locally Holder continuous and $L^p$ for $p>n/2$,On the solvability of the Dirichlet Problem  for  locally Holder continuous and  for,\Delta u =f f L^p p>n/2,"It's well known that if $\Omega$ is a bounded set and $f$ is locally Holder continuous on $\Omega$ and bounded, then $u = \int_{\Omega} \Gamma(x-y)f(y) \ dy$ is a classical solution to $\Delta u=f$ , where $\Gamma$ here is the fundamental solution of the Laplacian. I have read the proof of this in Gilbarg and Trudinger. There is a problem in this book to prove that we can replace the assumption $f \in L^\infty(\Omega)$ with the assumption $f \in L^p(\Omega)$ for $p>n/2$ . In the proof of the bounded case, the assumption of boundedness only comes in (as far as I can tell) in showing that $$\int_{B_\epsilon(x)} |\nabla \Gamma(x-y)| |f(y)| \to 0$$ as $\epsilon \to 0$ . If we assume $f \in L^p(\Omega)$ with $p>n$ , this can be shown with Holder's inequality. But just $p>n/2$ is giving me trouble. I'm wondering if this is a mistake, because another book I've glanced through (Analysis, by Lieb and Loss) seems to state that for $p>n$ , $u$ is indeed a classical $C^2$ solution, and all they say for $n/2 \le p <n$ is that $u$ is Holder continuous. So, is the result true? Or is it just a mistake by Gilbarg and Trudinger.","It's well known that if is a bounded set and is locally Holder continuous on and bounded, then is a classical solution to , where here is the fundamental solution of the Laplacian. I have read the proof of this in Gilbarg and Trudinger. There is a problem in this book to prove that we can replace the assumption with the assumption for . In the proof of the bounded case, the assumption of boundedness only comes in (as far as I can tell) in showing that as . If we assume with , this can be shown with Holder's inequality. But just is giving me trouble. I'm wondering if this is a mistake, because another book I've glanced through (Analysis, by Lieb and Loss) seems to state that for , is indeed a classical solution, and all they say for is that is Holder continuous. So, is the result true? Or is it just a mistake by Gilbarg and Trudinger.",\Omega f \Omega u = \int_{\Omega} \Gamma(x-y)f(y) \ dy \Delta u=f \Gamma f \in L^\infty(\Omega) f \in L^p(\Omega) p>n/2 \int_{B_\epsilon(x)} |\nabla \Gamma(x-y)| |f(y)| \to 0 \epsilon \to 0 f \in L^p(\Omega) p>n p>n/2 p>n u C^2 n/2 \le p <n u,"['real-analysis', 'analysis', 'partial-differential-equations', 'laplacian', 'elliptic-equations']"
68,Harnack Inequality for nonnegative subsolutions to uniformly elliptic PDE,Harnack Inequality for nonnegative subsolutions to uniformly elliptic PDE,,"I am trying to prove a Harnack inequality for a nonnegative subsolution $u \in H^1(B_2)$ to the PDE $\text{div}(A Du) \ge 0$ ,where $A = A(x)$ is uniformly elliptic. The proof outline I am following is from a set of notes by a professor at my university, and the key step is the following inductive scheme: Set $x_0$ to be a point such that $$u(x_0) = \sup_{B_{(0,1/2)}} u,$$ and choose $x_k$ inductively such that $x_{k+1}$ is such that $$u(x_{k+1}) = \sup_{B(x_k, r_k)} u$$ for $r_k$ sufficiently small to be chosen in a moment. I have all of the steps except the following: suppose $$\frac{\text{sup}_{B_{0,1/4}} u}{ u(0)}$$ is sufficiently large, then we can choose a sequence $r_k$ such that $\sum r_k <1/2$ and a $\beta>1$ such that $u(x_{k+1}) \ge \beta u(x_k)$ . That this would imply the result is immediate because it would contradict the boundedness of $u$ . The preceding step, which I am led to believe is what implies the claim, is the following: $$u(x_{k+1}) \ge \frac{u(x_k) - cr_k^{-q} u(0)}{1-\theta}$$ where $c$ , $q$ are absolute constants, and $1-\theta \ge \text{osc}_{B_1}u>0$ and $0<\theta \le \inf_{B_1} u$ . Here $c,q>0$ are absolute constants. I basically don't know what to do with this. Even if I assume the ratio in question gets very large, the estimate (from the prior step) becomes useless as $r_k \to 0$ . So it's unclear to me how to use it infinitely many times. I have the Nash-Digiorgi Holder regularity theorem at my disposal. Any hints or references would be much appreciated! I cannot find a similar proof anywhere, and given that I have provided the details for all of the other (numerous) steps, I would like to complete it.","I am trying to prove a Harnack inequality for a nonnegative subsolution to the PDE ,where is uniformly elliptic. The proof outline I am following is from a set of notes by a professor at my university, and the key step is the following inductive scheme: Set to be a point such that and choose inductively such that is such that for sufficiently small to be chosen in a moment. I have all of the steps except the following: suppose is sufficiently large, then we can choose a sequence such that and a such that . That this would imply the result is immediate because it would contradict the boundedness of . The preceding step, which I am led to believe is what implies the claim, is the following: where , are absolute constants, and and . Here are absolute constants. I basically don't know what to do with this. Even if I assume the ratio in question gets very large, the estimate (from the prior step) becomes useless as . So it's unclear to me how to use it infinitely many times. I have the Nash-Digiorgi Holder regularity theorem at my disposal. Any hints or references would be much appreciated! I cannot find a similar proof anywhere, and given that I have provided the details for all of the other (numerous) steps, I would like to complete it.","u \in H^1(B_2) \text{div}(A Du) \ge 0 A = A(x) x_0 u(x_0) = \sup_{B_{(0,1/2)}} u, x_k x_{k+1} u(x_{k+1}) = \sup_{B(x_k, r_k)} u r_k \frac{\text{sup}_{B_{0,1/4}} u}{ u(0)} r_k \sum r_k <1/2 \beta>1 u(x_{k+1}) \ge \beta u(x_k) u u(x_{k+1}) \ge \frac{u(x_k) - cr_k^{-q} u(0)}{1-\theta} c q 1-\theta \ge \text{osc}_{B_1}u>0 0<\theta \le \inf_{B_1} u c,q>0 r_k \to 0","['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations', 'elliptic-equations']"
69,"Show that any non-trivial ideal of $(L_1,*)$ is dense",Show that any non-trivial ideal of  is dense,"(L_1,*)","This is related to this other question , I mean, the linked question comes to my mind trying to solve the following exercise: Show that any non-trivial ideal of $(L_1,*)$ is dense. Here $(L_1,*)$ is the space $L_1(\Bbb R^n,\Bbb R)$ and $*$ stay for convolution. I know that $(L_1,+,*)$ is a Banach algebra without unity. I dont know the concept of ""ideal"" related to a Banach algebra so I assumed that it is asking for a set $I\subset L_1$ such that $f*\varphi\in I$ for any pair $(f,\varphi)\in L_1\times I$ . Correct me if my interpretation was wrong. FIRST APPROACH: I know that if $g\in L_1$ and $\|g\|_1=1$ then $g_\epsilon(x):=\epsilon^{-n}g(x/\epsilon)$ defines a kernel $\{g_\epsilon:\epsilon>0\}$ that approximates the unity, that is, I know that $$\lim_{\epsilon\to 0} g_\epsilon* f=\|g\|_1 f=f\tag1$$ in $L_1$ for any chosen $f\in L_1$ . Then my first idea was construct some approximation to the unity from a function $f*\varphi\in I$ (where $(f,\varphi)\in L_1\times I$ ), that is, I want to show that if $f*\varphi\in I$ then $(f*\varphi)_\epsilon\in I$ for any $\epsilon>0$ , from here it would be easy to see that $$h_\epsilon:=\frac{(f*\varphi)_\epsilon}{\|f*\varphi\|_1}\in I\tag2$$ and $\lim_{\epsilon\to 0}h_\epsilon*r=r$ for any chosen $r\in L_1$ , what would imply that $I$ is dense in $L_1$ . Then note that $$(f*\varphi)_\epsilon(x)=\epsilon^{-n}\int f(x/\epsilon-y)\varphi(y)\, dy=\epsilon^{-n}\int f(x-y+K)\varphi(y)\, dy\tag3$$ for $K:= x/\epsilon-x$ . Then also note that $\epsilon^{-n} f(\,\cdot+K)\in L_1$ for any chosen $K\in\Bbb R^n$ , so an heuristic argument make me think from $(3)$ that $(f*\varphi)_\epsilon\in I$ . However, this heuristic argument seems wrong, because we cannot fix $K$ as a constant and say that $g(x-y):=f(x/\epsilon-y)$ belongs to $L_1$ because $g$ is not well-defined. SECOND APPROACH: Note that $$(f*\varphi)_\epsilon(x)=\epsilon^{-n}\int f(x/\epsilon-y)\varphi(y)\, dy=\epsilon^{-n}\int \tau_{x/\epsilon}\check f(y)\varphi(y)\, dy\\ =\epsilon^{-n}\int(\tau_{x/\epsilon}\check f\cdot \varphi)(y)\, dy =\epsilon^{-2n}\int\tau_{x/\epsilon}\check f(y/\epsilon)\varphi(y/\epsilon)\, dy \\=\epsilon^{-2n}\int f\left(\frac{x-y}\epsilon\right)\varphi(y/\epsilon)=(f_\epsilon*\varphi_\epsilon)(x)\tag4$$ where $\tau_a f(x)=f(x+a)$ and $\check f(x)= f(-x)$ . Then it would be enough to show that if $\varphi\in I$ then $\varphi_\epsilon\in I$ , but this doesn't seems feasible. THIRD APPROACH: Let any smoothing kernel $\{\varphi_\epsilon:\epsilon>0\}\subset L_1$ (by example the Gaussian kernel) and some $\psi\in I\setminus\{0\}$ . Then, by the definition of ideal, we knows that $\varphi_\epsilon*\psi\in I$ for all $\epsilon>0$ , thus by $(1)$ we can see that $\psi$ is a limit point of $I$ , and because this holds for every function on the ideal then we conclude that $I$ is closed and perfect. Then, if $I$ is dense, it must be the case that $I=L_1$ . However Im again stuck here, that is, I dont know how to show that $I=L_1$ . Moreover: if it would be true that $I=L_1$ then $I$ is, indeed, a trivial ideal of $L_1$ , contradicting the existence of non-trivial ideals, so the statement to be proved will be a vacuous truth. Some help will be appreciated, thank you.","This is related to this other question , I mean, the linked question comes to my mind trying to solve the following exercise: Show that any non-trivial ideal of is dense. Here is the space and stay for convolution. I know that is a Banach algebra without unity. I dont know the concept of ""ideal"" related to a Banach algebra so I assumed that it is asking for a set such that for any pair . Correct me if my interpretation was wrong. FIRST APPROACH: I know that if and then defines a kernel that approximates the unity, that is, I know that in for any chosen . Then my first idea was construct some approximation to the unity from a function (where ), that is, I want to show that if then for any , from here it would be easy to see that and for any chosen , what would imply that is dense in . Then note that for . Then also note that for any chosen , so an heuristic argument make me think from that . However, this heuristic argument seems wrong, because we cannot fix as a constant and say that belongs to because is not well-defined. SECOND APPROACH: Note that where and . Then it would be enough to show that if then , but this doesn't seems feasible. THIRD APPROACH: Let any smoothing kernel (by example the Gaussian kernel) and some . Then, by the definition of ideal, we knows that for all , thus by we can see that is a limit point of , and because this holds for every function on the ideal then we conclude that is closed and perfect. Then, if is dense, it must be the case that . However Im again stuck here, that is, I dont know how to show that . Moreover: if it would be true that then is, indeed, a trivial ideal of , contradicting the existence of non-trivial ideals, so the statement to be proved will be a vacuous truth. Some help will be appreciated, thank you.","(L_1,*) (L_1,*) L_1(\Bbb R^n,\Bbb R) * (L_1,+,*) I\subset L_1 f*\varphi\in I (f,\varphi)\in L_1\times I g\in L_1 \|g\|_1=1 g_\epsilon(x):=\epsilon^{-n}g(x/\epsilon) \{g_\epsilon:\epsilon>0\} \lim_{\epsilon\to 0} g_\epsilon* f=\|g\|_1 f=f\tag1 L_1 f\in L_1 f*\varphi\in I (f,\varphi)\in L_1\times I f*\varphi\in I (f*\varphi)_\epsilon\in I \epsilon>0 h_\epsilon:=\frac{(f*\varphi)_\epsilon}{\|f*\varphi\|_1}\in I\tag2 \lim_{\epsilon\to 0}h_\epsilon*r=r r\in L_1 I L_1 (f*\varphi)_\epsilon(x)=\epsilon^{-n}\int f(x/\epsilon-y)\varphi(y)\, dy=\epsilon^{-n}\int f(x-y+K)\varphi(y)\, dy\tag3 K:= x/\epsilon-x \epsilon^{-n} f(\,\cdot+K)\in L_1 K\in\Bbb R^n (3) (f*\varphi)_\epsilon\in I K g(x-y):=f(x/\epsilon-y) L_1 g (f*\varphi)_\epsilon(x)=\epsilon^{-n}\int f(x/\epsilon-y)\varphi(y)\, dy=\epsilon^{-n}\int \tau_{x/\epsilon}\check f(y)\varphi(y)\, dy\\
=\epsilon^{-n}\int(\tau_{x/\epsilon}\check f\cdot \varphi)(y)\, dy
=\epsilon^{-2n}\int\tau_{x/\epsilon}\check f(y/\epsilon)\varphi(y/\epsilon)\, dy
\\=\epsilon^{-2n}\int f\left(\frac{x-y}\epsilon\right)\varphi(y/\epsilon)=(f_\epsilon*\varphi_\epsilon)(x)\tag4 \tau_a f(x)=f(x+a) \check f(x)= f(-x) \varphi\in I \varphi_\epsilon\in I \{\varphi_\epsilon:\epsilon>0\}\subset L_1 \psi\in I\setminus\{0\} \varphi_\epsilon*\psi\in I \epsilon>0 (1) \psi I I I I=L_1 I=L_1 I=L_1 I L_1","['real-analysis', 'abstract-algebra', 'lp-spaces', 'convolution']"
70,Showing that the equation $x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i$ has a unique solution.,Showing that the equation  has a unique solution.,x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i,"Exercise : Consider the infinite-dimensional system of equations : $$x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i, \quad i=1,2,3,\dots$$ We suppose that $b=(b_1,b_2,\dots) \in \ell^\infty$ and that it exists $0<\theta<1$ such that : $$\sup_i \sum_{j=1}^\infty |a_{ij}|\leq \theta$$ Show that the system of equations given has a unique solution $x=(x_1,x_2,\dots) \in \ell^\infty$ . Attempt : In previous exercises and lessons, I have proved the following lemma : Let $X$ be a Banach space and $T \in B(X)$ with $\|T\| \leq \theta < 1$ . Then, if $y \in X$ , the equation $x = y + Tx$ has a unique solution $x \in X$ . My initial idea is to work over manipulating the exercise to reach the statements of the lemma above. The equation for the infinite-dimensional system of equations given, can be rewritten as : $$x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i \Rightarrow x = b + Tx$$ where $x = (x_1,x_2,\dots)$ and $T$ be an operator, such that : $$Tx = \sum_{j=1}^\infty a_{ij}x_j$$ Now, I observe that $\ell^\infty$ is a Banach space and since $b \in \ell^\infty$ then $x - Tx \in \ell^\infty$ which means that $x \in \ell^\infty$ and $T$ is an operator defined over $\ell^\infty$ . Now, I need to prove that $T \in B(\ell^\infty)$ , which means that $T$ is a bounded linear operator $T : \ell^\infty \to \ell^\infty$ and that $\|T\| < 1$ . For the case of linearity, let $x,y \in \ell^\infty$ and $\lambda \in \mathbb R$ . Then, it is $$T(\lambda x + y) = \sum_{j=1}^\infty a_{ij}(\lambda x_j + y_j) = \sum_{j=1}^\infty \lambda a_{ij} x_j + \sum_{j=1}^\infty a_{ij}y_j$$ $$=$$ $$\lambda \sum_{j=1}^\infty a_{ij}x_j + \sum_{j=1}^\infty a_{ij}y_j = \lambda T x + Ty$$ which proves that $T$ is a linear operator. Now, for the case of $T$ being bounded : $$\|Tx\|_\infty = \bigg\|\sum_{j=1}^\infty a_{ij}x_j \bigg\|_\infty \leq \sum_{j=1}^\infty \|a_{ij}x_j\|_\infty \leq \sum_{j=1}^\infty |a_{ij}| \|x\|_\infty$$ But, it is $$\sup_i \sum_{j=1}^\infty |a_{ij}|\leq \theta < 1$$ thus, by combining the two last results above, we get : $$\|Tx\|_\infty \leq \theta \|x\|_\infty$$ which proves that $T$ is a bounded linear operator $T \in B(\ell^\infty)$ and thus the equation we transformed has a unique solution $x \in \ell^\infty$ which means that the initial infinite-dimensional system of equations has a unique solution $x=(x_1,x2,\dots) \in \ell^\infty$ . Question : Is my approach correct and rigorous enough ? Any recommendations on what I can improve or any mistakes ? I would appreciate any input or approvement of my approach.","Exercise : Consider the infinite-dimensional system of equations : We suppose that and that it exists such that : Show that the system of equations given has a unique solution . Attempt : In previous exercises and lessons, I have proved the following lemma : Let be a Banach space and with . Then, if , the equation has a unique solution . My initial idea is to work over manipulating the exercise to reach the statements of the lemma above. The equation for the infinite-dimensional system of equations given, can be rewritten as : where and be an operator, such that : Now, I observe that is a Banach space and since then which means that and is an operator defined over . Now, I need to prove that , which means that is a bounded linear operator and that . For the case of linearity, let and . Then, it is which proves that is a linear operator. Now, for the case of being bounded : But, it is thus, by combining the two last results above, we get : which proves that is a bounded linear operator and thus the equation we transformed has a unique solution which means that the initial infinite-dimensional system of equations has a unique solution . Question : Is my approach correct and rigorous enough ? Any recommendations on what I can improve or any mistakes ? I would appreciate any input or approvement of my approach.","x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i, \quad i=1,2,3,\dots b=(b_1,b_2,\dots) \in \ell^\infty 0<\theta<1 \sup_i \sum_{j=1}^\infty |a_{ij}|\leq \theta x=(x_1,x_2,\dots) \in \ell^\infty X T \in B(X) \|T\| \leq \theta < 1 y \in X x = y + Tx x \in X x_i - \sum_{j=1}^\infty a_{ij}x_j = b_i \Rightarrow x = b + Tx x = (x_1,x_2,\dots) T Tx = \sum_{j=1}^\infty a_{ij}x_j \ell^\infty b \in \ell^\infty x - Tx \in \ell^\infty x \in \ell^\infty T \ell^\infty T \in B(\ell^\infty) T T : \ell^\infty \to \ell^\infty \|T\| < 1 x,y \in \ell^\infty \lambda \in \mathbb R T(\lambda x + y) = \sum_{j=1}^\infty a_{ij}(\lambda x_j + y_j) = \sum_{j=1}^\infty \lambda a_{ij} x_j + \sum_{j=1}^\infty a_{ij}y_j = \lambda \sum_{j=1}^\infty a_{ij}x_j + \sum_{j=1}^\infty a_{ij}y_j = \lambda T x + Ty T T \|Tx\|_\infty = \bigg\|\sum_{j=1}^\infty a_{ij}x_j \bigg\|_\infty \leq \sum_{j=1}^\infty \|a_{ij}x_j\|_\infty \leq \sum_{j=1}^\infty |a_{ij}| \|x\|_\infty \sup_i \sum_{j=1}^\infty |a_{ij}|\leq \theta < 1 \|Tx\|_\infty \leq \theta \|x\|_\infty T T \in B(\ell^\infty) x \in \ell^\infty x=(x_1,x2,\dots) \in \ell^\infty","['real-analysis', 'functional-analysis', 'proof-verification', 'operator-theory', 'banach-spaces']"
71,Start understanding analysis [closed],Start understanding analysis [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Currently, I have read and re-read the Stephen Abbott Understanding Analysis for about $3$ or $4$ times up to and including the chapter $6$ . However, I can't say that now I am feeling more confident about understanding and completing problems in analysis, than when I hadn't read it. It is so frustrating to read the text, when you think that you get almost everything in the explanation parts, because it is so well written and clear, but after that struggling to complete even the easiest exercise in the problems section of the chapter, even when redoing it the $3$ -rd time.  Therefore, I am willing to ask, can you suggest a way to start really understanding the analysis? How should I approach to solving the problems? Will it make a sense to switch to another book (I was thinking about Apostol/Pugh/Kolmogorov)? $P.S.$ I am a self-learner. It is my second book in analysis, after Fichtengoltz's first volume, which was too theoretical for my test as there were no problem sets. My problem is not just in exercises. While reading the proof I understand it, however if I try to do the proof on my own, for example, after $3$ hours from reading I can not manage to recall how it was done.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Currently, I have read and re-read the Stephen Abbott Understanding Analysis for about or times up to and including the chapter . However, I can't say that now I am feeling more confident about understanding and completing problems in analysis, than when I hadn't read it. It is so frustrating to read the text, when you think that you get almost everything in the explanation parts, because it is so well written and clear, but after that struggling to complete even the easiest exercise in the problems section of the chapter, even when redoing it the -rd time.  Therefore, I am willing to ask, can you suggest a way to start really understanding the analysis? How should I approach to solving the problems? Will it make a sense to switch to another book (I was thinking about Apostol/Pugh/Kolmogorov)? I am a self-learner. It is my second book in analysis, after Fichtengoltz's first volume, which was too theoretical for my test as there were no problem sets. My problem is not just in exercises. While reading the proof I understand it, however if I try to do the proof on my own, for example, after hours from reading I can not manage to recall how it was done.",3 4 6 3 P.S. 3,"['real-analysis', 'analysis', 'soft-question', 'book-recommendation']"
72,Proof of a technical fact in the book of Schapire and Freund on boosting,Proof of a technical fact in the book of Schapire and Freund on boosting,,"I am currently looking at Exercise 10.3, Chapter 10 of the book on Boosting by Schapire and Freund. More precisely, in the middle of the exercise they propose to use, without proof, the technical fact summarized below. Obviously, since it can be used without proof, I am now curious to know how to prove it! To summarize the problem, let $\mathcal{H}$ bet a set of functions $h : \mathcal{X} \times \mathcal{\bar{Y}} \rightarrow [-1,1]$ . Define $\text{co}(\mathcal{H})$ as \begin{align*} \text{co}(\mathcal{H}) = \left\lbrace f : x,\bar{y} \mapsto \sum_{t=1}^T a_t h_t(x,\bar{y}) \left| a_1,\ldots,a_T \geq 0; \sum_{t=1}^Ta_t = 1; h_1,\ldots h_T \in \mathcal{H}; T\geq 1 \right.  \right\rbrace\text{.} \end{align*} Notice that $f : \mathcal{X} \times \mathcal{\bar{Y}} \rightarrow [-1,1]$ . For $f \in \text{co}\left(\mathcal{H}\right)$ , $\eta > 0$ , $\bar{K} = |\mathcal{\bar{Y}}|$ , and $(x,y) \in \mathcal{X} \times \mathcal{Y}$ , let \begin{align*} \nu_{f,\eta}(x,y) = - \frac{1}{\eta} \ln\left(\frac{1}{\bar{K}} \sum_{\bar{y} \in \mathcal{\bar{Y}}} \exp\Big(-\eta \Omega(y,\bar{y}) f(x,\bar{y})\Big)\right) \end{align*} where $\Omega(y,\bar{y}) = 1$ if $\bar{y} \in \Omega(y)$ and $-1$ otherwise. $\Omega(y)$ associates each element from $\mathcal{Y}$ to a subset of $\mathcal{\bar{Y}}$ . Notice that $\nu_{f,\eta} : \mathcal{X} \times \mathcal{Y} \rightarrow [-1,1]$ . The technical fact is as follows. Let $1 \geq \theta > 0$ and define the grid: \begin{align*} \varepsilon_\theta = \left\lbrace \frac{4\ln\bar{K}}{i\theta} : i = 1, \ldots, \left\lceil \frac{8\ln\bar{K}}{\theta^2} \right\rceil \right\rbrace\text{.} \end{align*} For any $\eta > 0$ , let $\hat{\eta}$ be the closest value in $\varepsilon_\theta$ to $\eta$ . Then for all $f \in \text{co}(\mathcal{H})$ and for all $(x,y) \in \mathcal{X} \times \mathcal{Y}$ , \begin{align*} \left| \nu_{f,\eta}(x,y) - \nu_{f,\hat{\eta}}(x,y) \right| \leq \frac{\theta}{4}\text{.} \end{align*} So far, I proved the statement when $\eta > \frac{4\ln\bar{K}}{\theta}$ (using the properties of the LogSumExp function). Furthermore, using the grid, I showed that \begin{align*} && \left| \eta - \hat{\eta} \right| \leq \frac{\ln \bar{K}}{\theta} \\ &\Rightarrow& \left| \eta\nu_{f,\eta}(x,y) - \hat{\eta}\nu_{f,\hat{\eta}}(x,y) \right| \leq \frac{\ln \bar{K}}{\theta}\text{.} \end{align*} However, I did not manage to go further than that. Am I going in the right direction? If yes, what would be the trick for the last step? If no, what method should I consider to prove this statement? Note that I am not asking for a full proof, but rather some hints on how to proceed to show the result.","I am currently looking at Exercise 10.3, Chapter 10 of the book on Boosting by Schapire and Freund. More precisely, in the middle of the exercise they propose to use, without proof, the technical fact summarized below. Obviously, since it can be used without proof, I am now curious to know how to prove it! To summarize the problem, let bet a set of functions . Define as Notice that . For , , , and , let where if and otherwise. associates each element from to a subset of . Notice that . The technical fact is as follows. Let and define the grid: For any , let be the closest value in to . Then for all and for all , So far, I proved the statement when (using the properties of the LogSumExp function). Furthermore, using the grid, I showed that However, I did not manage to go further than that. Am I going in the right direction? If yes, what would be the trick for the last step? If no, what method should I consider to prove this statement? Note that I am not asking for a full proof, but rather some hints on how to proceed to show the result.","\mathcal{H} h : \mathcal{X} \times \mathcal{\bar{Y}} \rightarrow [-1,1] \text{co}(\mathcal{H}) \begin{align*}
\text{co}(\mathcal{H}) = \left\lbrace f : x,\bar{y} \mapsto \sum_{t=1}^T a_t h_t(x,\bar{y}) \left| a_1,\ldots,a_T \geq 0; \sum_{t=1}^Ta_t = 1; h_1,\ldots h_T \in \mathcal{H}; T\geq 1 \right.  \right\rbrace\text{.}
\end{align*} f : \mathcal{X} \times \mathcal{\bar{Y}} \rightarrow [-1,1] f \in \text{co}\left(\mathcal{H}\right) \eta > 0 \bar{K} = |\mathcal{\bar{Y}}| (x,y) \in \mathcal{X} \times \mathcal{Y} \begin{align*}
\nu_{f,\eta}(x,y) = - \frac{1}{\eta} \ln\left(\frac{1}{\bar{K}} \sum_{\bar{y} \in \mathcal{\bar{Y}}} \exp\Big(-\eta \Omega(y,\bar{y}) f(x,\bar{y})\Big)\right)
\end{align*} \Omega(y,\bar{y}) = 1 \bar{y} \in \Omega(y) -1 \Omega(y) \mathcal{Y} \mathcal{\bar{Y}} \nu_{f,\eta} : \mathcal{X} \times \mathcal{Y} \rightarrow [-1,1] 1 \geq \theta > 0 \begin{align*}
\varepsilon_\theta = \left\lbrace \frac{4\ln\bar{K}}{i\theta} : i = 1, \ldots, \left\lceil \frac{8\ln\bar{K}}{\theta^2} \right\rceil \right\rbrace\text{.}
\end{align*} \eta > 0 \hat{\eta} \varepsilon_\theta \eta f \in \text{co}(\mathcal{H}) (x,y) \in \mathcal{X} \times \mathcal{Y} \begin{align*}
\left| \nu_{f,\eta}(x,y) - \nu_{f,\hat{\eta}}(x,y) \right| \leq \frac{\theta}{4}\text{.}
\end{align*} \eta > \frac{4\ln\bar{K}}{\theta} \begin{align*}
&& \left| \eta - \hat{\eta} \right| \leq \frac{\ln \bar{K}}{\theta} \\
&\Rightarrow& \left| \eta\nu_{f,\eta}(x,y) - \hat{\eta}\nu_{f,\hat{\eta}}(x,y) \right| \leq \frac{\ln \bar{K}}{\theta}\text{.}
\end{align*}","['real-analysis', 'statistics', 'inequality']"
73,Open sets and intersections,Open sets and intersections,,"Suppose $G$ is an open subset of the real number that is not upper bounded. Is there a real number $x > 0$ such that the set of all integer multiples of $x$ intersects $G$ at infinitely many points? That is, is it true that $\exists x \in \mathbb{R}$ such that $\{mx\mid m\in \mathbb{Z}\}\bigcap G$ is infinite? My intuition tells me yes, since the fact that $G$ is not upper bounded seems to be a major factor here, but I can’t seem to prove it.","Suppose is an open subset of the real number that is not upper bounded. Is there a real number such that the set of all integer multiples of intersects at infinitely many points? That is, is it true that such that is infinite? My intuition tells me yes, since the fact that is not upper bounded seems to be a major factor here, but I can’t seem to prove it.",G x > 0 x G \exists x \in \mathbb{R} \{mx\mid m\in \mathbb{Z}\}\bigcap G G,['real-analysis']
74,Inverse powers of Bessel process not a martingale,Inverse powers of Bessel process not a martingale,,"I am trying to show that $X_t^{1-2a}$ is not a martingale, where the solution $X_t$ to $$dX_t = \frac{a}{X_t} \ dt + \ dB_t, \ \ X_0=1$$ for $a>1/2$ . I am able to do this directly for $a = (d-1)/2$ , because then $X_t$ is the norm of the standard $d-$ dimensional Brownian motion starting on the sphere of radius $1$ . I also have the following facts: $X_t \to \infty$ almost surely as $t \to \infty$ , as well as $$\mathbb{P}(X_t \text{ hits } R \text{ before } \epsilon) = \frac{1- \epsilon^{1-2a}}{R^{1-2a} - \epsilon^{1-2a}}.$$ Here $\epsilon < 1<R$ . If $X_t$ had started at $x$ we would replace $1$ there with $x^{1-2a}$ . But I am not sure how to use these facts to show that $M_t\equiv X_t^{1-2a}$ is not a martingale. I can't seem to justify the limit swap to show $\mathbb{E}[M_t] \to 0$ . In particular there doesn't seem to be anything I can dominate with. If there were some uniform integrability I could use Egorov and be done, but I just don't know enough about the Bessel process to know if this is true. Does anyone have any hints?","I am trying to show that is not a martingale, where the solution to for . I am able to do this directly for , because then is the norm of the standard dimensional Brownian motion starting on the sphere of radius . I also have the following facts: almost surely as , as well as Here . If had started at we would replace there with . But I am not sure how to use these facts to show that is not a martingale. I can't seem to justify the limit swap to show . In particular there doesn't seem to be anything I can dominate with. If there were some uniform integrability I could use Egorov and be done, but I just don't know enough about the Bessel process to know if this is true. Does anyone have any hints?","X_t^{1-2a} X_t dX_t = \frac{a}{X_t} \ dt + \ dB_t, \ \ X_0=1 a>1/2 a = (d-1)/2 X_t d- 1 X_t \to \infty t \to \infty \mathbb{P}(X_t \text{ hits } R \text{ before } \epsilon) = \frac{1- \epsilon^{1-2a}}{R^{1-2a} - \epsilon^{1-2a}}. \epsilon < 1<R X_t x 1 x^{1-2a} M_t\equiv X_t^{1-2a} \mathbb{E}[M_t] \to 0","['real-analysis', 'probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus']"
75,If for each $x \in \mathbb R$ there exists $n$ such that $f^{(m)}(x)=0$ for all $m \ge n$ then prove that $f$ must be a polynomial,If for each  there exists  such that  for all  then prove that  must be a polynomial,x \in \mathbb R n f^{(m)}(x)=0 m \ge n f,"Let $f:\mathbb R \to \mathbb R$ be an infinitely differentiable function such that for every $x \in \mathbb R$ there exists $n$ such that $f^{(m)}(x)=0$ for all $m \ge n$ . I need to prove that $f$ is a polynomial. It is trivial to show using Baire's theorem that $f$ must be a polynomial in some open ball, but since $f$ is not necessarily analytic I cannot apply the identity principle to obtain the desired result. I would appreciate a hint.","Let be an infinitely differentiable function such that for every there exists such that for all . I need to prove that is a polynomial. It is trivial to show using Baire's theorem that must be a polynomial in some open ball, but since is not necessarily analytic I cannot apply the identity principle to obtain the desired result. I would appreciate a hint.",f:\mathbb R \to \mathbb R x \in \mathbb R n f^{(m)}(x)=0 m \ge n f f f,"['calculus', 'real-analysis', 'derivatives']"
76,What are the necessary and sufficient conditions for a function to be Henstock–Kurzweil integrable?,What are the necessary and sufficient conditions for a function to be Henstock–Kurzweil integrable?,,I recently stumbled upon Lebesgue’s criterion for Riemann integrability. It didn't take very long until I found this result quite intuitive. I then began studying the Henstock–Kurzweil integral. Very quickly I realized that finding the necessary and sufficient conditions for a function to be Henstock–Kurzweil integrable would be much more difficult since functions with significantly worse discontinuities have to be treated. Has there been any research on this topic?,I recently stumbled upon Lebesgue’s criterion for Riemann integrability. It didn't take very long until I found this result quite intuitive. I then began studying the Henstock–Kurzweil integral. Very quickly I realized that finding the necessary and sufficient conditions for a function to be Henstock–Kurzweil integrable would be much more difficult since functions with significantly worse discontinuities have to be treated. Has there been any research on this topic?,,"['calculus', 'real-analysis', 'integration', 'analysis', 'gauge-integral']"
77,"Checking whether $X=\mathbb R$ with $d(x,y)=\min\{ \sqrt{|x-y|},|x-y|^2\}$ is a metric space",Checking whether  with  is a metric space,"X=\mathbb R d(x,y)=\min\{ \sqrt{|x-y|},|x-y|^2\}","Examine whether $d$ is a metric on $X=\mathbb{R}$ where $d\left(x,y\right)=\min\{ \sqrt{|x-y|},|x-y|^2\}$ for all $x,y\in \mathbb{R}$ I think it is not. Even though it satisfies all other properties but it doesn't satisfy triangle inequality. Take $x=2,y=1.5,z=1$ $d\left(x,z\right)=1$ $d\left(x,y\right)=0.25$ $d\left(y,z\right)=0.25$ Can someone confirm and verify. It is correct or not.","Examine whether $d$ is a metric on $X=\mathbb{R}$ where $d\left(x,y\right)=\min\{ \sqrt{|x-y|},|x-y|^2\}$ for all $x,y\in \mathbb{R}$ I think it is not. Even though it satisfies all other properties but it doesn't satisfy triangle inequality. Take $x=2,y=1.5,z=1$ $d\left(x,z\right)=1$ $d\left(x,y\right)=0.25$ $d\left(y,z\right)=0.25$ Can someone confirm and verify. It is correct or not.",,"['real-analysis', 'metric-spaces']"
78,Applying Rolle's theorem,Applying Rolle's theorem,,"This is the first time that I'm doing a proof-problem on my own and I'm not really sure how to check if my answer is correct, so I was wondering if someone could tell me with certainty if my proof is correct. The problem states: Let $f:\mathbb{R} \to \mathbb{R}$ be a twice differentiable function (on its entire domain) such that $f(0) = f(1) = f(2)$. Prove that there exists some point $x_0 \in (0, 2)$ such that $f''(x_0) = 0$. So I developed my proof based on Rolle's theorem that states that if a function $f$ is continous on a closed interval $[a, b]$ and differentiable on an open interval $(a, b)$ and if $f(a) = f(b)$ then there exists some point $c \in (a, b)$ such that $f'(c) = 0$. Proof: $f$ is differentiable $\forall x \in \mathbb{R}$, meaning $f$ must be differentiable on $(0, 1)$ and $(1, 2)$ $f$ is differentiable $\forall x\in\mathbb{R}$ meaning $f$ must be continuous $\forall x \in \mathbb{R}$ meaning $f$ must be continuous on $[0, 1]$ and $[1, 2]$. $f(0) = f(1)$ and $f(1) = f(2)$. From 1., 2. and 3. we conclude that $f$ satisfies the conditions of Rolle's theorem for both of these intervals, meaning: $\exists c_1 \in (0, 1) : f'(c_1) = 0$ and $\exists c_2 \in (1, 2) : f'(c_2) = 0$ Let $F(x) = f'(x)$. We know that the original function is twice differentiable $\forall x \in \mathbb{R}$, therefore $F(x)$ is differentiable on $\mathbb{R}$ meaning it must be differentiable on $(c_1, c_2)$ also meaning it must be continuous on $[c_1, c_2]$. Now, since $f'(c_1) = f'(c_2) = 0$, the function $F(x)$ satisfies the conditions of Rolle's theorem (on the interval $(c_1, c_2)$) meaning : $\exists x_0 \in (c_1, c_2) \subset(0, 2): F'(x_0) = f''(x_0) = 0$.","This is the first time that I'm doing a proof-problem on my own and I'm not really sure how to check if my answer is correct, so I was wondering if someone could tell me with certainty if my proof is correct. The problem states: Let $f:\mathbb{R} \to \mathbb{R}$ be a twice differentiable function (on its entire domain) such that $f(0) = f(1) = f(2)$. Prove that there exists some point $x_0 \in (0, 2)$ such that $f''(x_0) = 0$. So I developed my proof based on Rolle's theorem that states that if a function $f$ is continous on a closed interval $[a, b]$ and differentiable on an open interval $(a, b)$ and if $f(a) = f(b)$ then there exists some point $c \in (a, b)$ such that $f'(c) = 0$. Proof: $f$ is differentiable $\forall x \in \mathbb{R}$, meaning $f$ must be differentiable on $(0, 1)$ and $(1, 2)$ $f$ is differentiable $\forall x\in\mathbb{R}$ meaning $f$ must be continuous $\forall x \in \mathbb{R}$ meaning $f$ must be continuous on $[0, 1]$ and $[1, 2]$. $f(0) = f(1)$ and $f(1) = f(2)$. From 1., 2. and 3. we conclude that $f$ satisfies the conditions of Rolle's theorem for both of these intervals, meaning: $\exists c_1 \in (0, 1) : f'(c_1) = 0$ and $\exists c_2 \in (1, 2) : f'(c_2) = 0$ Let $F(x) = f'(x)$. We know that the original function is twice differentiable $\forall x \in \mathbb{R}$, therefore $F(x)$ is differentiable on $\mathbb{R}$ meaning it must be differentiable on $(c_1, c_2)$ also meaning it must be continuous on $[c_1, c_2]$. Now, since $f'(c_1) = f'(c_2) = 0$, the function $F(x)$ satisfies the conditions of Rolle's theorem (on the interval $(c_1, c_2)$) meaning : $\exists x_0 \in (c_1, c_2) \subset(0, 2): F'(x_0) = f''(x_0) = 0$.",,"['real-analysis', 'proof-verification']"
79,"Solution verification post : Problem $12.2$, Mathematical Analysis, Apostol","Solution verification post : Problem , Mathematical Analysis, Apostol",12.2,"As a continuation of this post , I'm posting my solutions (or attempts) to the exercise problems of Chapter 12 (Multivariable Differential Calculus), Mathematical Analysis by Apostol . Since I'm essentially self-studying, I'd really appreciate if anyone checks the solutions and let me know if there is any gap in my arguments or if there exists any cleverer solutions. Thank you. Problem $12.2.$ Calculate all first order partial derivatives and the directional derivative $f'(x;u)$ for each of the real-valued functions defined on $\mathbb{R}^n$ as follows : \begin{align*} &(a)\,\, f(x)=a \boldsymbol{\cdot} x\\ &(b)\,\, f(x)=\left\Vert x \right\Vert^4\\ &(c)\,\, f(x)=x \boldsymbol{\cdot} L(x), \text{ where } L:\mathbb{R}^n \to \mathbb{R}^n \text{ is a linear function.}\\ &(d)\,\, f(x)=\sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j, \text{ where } a_{ij}=a_{ji}. \end{align*} Solution. $(a)$ Let $a=(a_1,\dots,a_n)$, $x=(x_1,\dots,x_n)$. Thus, $$f(x)=a \boldsymbol{\cdot} x=a_1x_1+\dots+a_nx_n \tag1$$ Then we have, $$D_kf(x)=\frac{\partial f}{\partial x_k}(x)=a_k=a \boldsymbol{\cdot} e_k; \,\,k=1,\dots,n \tag2$$ $D_kf(x)$ is constant for all $k$ $\Rightarrow$ $D_kf(x)$ is continuous for all $k$. We recall the following : Theorem. Assume that one of the partial derivatives $D_1f,\dots,D_nf$ exists at $c$ and that the remaining $n−1$ partial derivatives exists in some $\delta$-ball $B_{\delta}(c)$, and are continuous at $c$. Then $f$ is differentiable at $c$. Using the theorem, we conclude that $f$ is differentiable and hence directional derivative at any direction exists. Then, \begin{align*} f'(x;u)&=\lim_{h \to 0} \frac{f(x+hu)-f(x)}{h}\\ &=\lim_{h \to 0} \frac{a \boldsymbol{\cdot} (x+hu)-a \boldsymbol{\cdot} x}{h}\\ &=\lim_{h \to 0} \frac{a \boldsymbol{\cdot} (hu)}{h}\\ &=\lim_{h \to 0} \frac{h \sum_{i=1}^n a_iu_i}{h}\\ &=\sum_{i=1}^n a_iu_i=a \boldsymbol{\cdot} u \tag3 \end{align*} Solution. $(b)$ \begin{align*} f(x)&=\left\Vert x \right\Vert^4=\left(\sum_{i=1}^n x_i^2\right)^2=\sum_{i=1}^n x_i^4+\sum_{i \neq j} x_i^2x_j^2\\ &=x_k^4+\sum_{i \neq k} x_i^4+x_k^2\left(\sum_{i \neq k} x_i^2\right)+\left(\sum_{i \neq k} x_i^2\right)x_k^2+\sum_{i \neq k,\,j \neq k,\,i \neq j} x_i^2x_j^2\\ &=x_k^4+\sum_{i \neq k} x_i^4+2x_k^2\left(\sum_{i \neq k} x_i^2\right)+\sum_{i \neq k,\,j \neq k,\,i \neq j} x_i^2x_j^2 \tag4 \end{align*} Let $k \in \{1,\dots,n\}$. Then, $$D_kf(x)=4x_k^3+4x_k\left(\sum_{i \neq k}x_i^2\right)=4x_k\left(\sum_{i=1}^nx_i^2\right)=4x_k\left\Vert x \right\Vert^2 \tag5$$ Thus, $D_kf(x)$ exists and is continuous for all $k \in \{1,\dots,n\}$. By the statement stated above, $f$ is differentiable, and hence directional derivative exists in all directions. Let $u=(u_1,\dots,u_n)=u_1e_1+\dots+u_ne_n$. Now, \begin{align*} f'(x;u)&=f'(x)(u)=f'(x)(u_1e_1+\dots+u_ne_n)=\sum_{k=1}^n u_kf'(x)(e_k)=\sum_{k=1}^n u_kf'(x;e_k)\\ &=\sum_{k=1}^n u_k\frac{\partial f}{\partial x_k}(x)=\sum_{k=1}^n u_k\left(4x_k\left\Vert x \right\Vert^2\right)=4\left\Vert x \right\Vert^2\sum_{k=1}^n x_ku_k=4\left\Vert x \right\Vert^2 \left(x \boldsymbol{\cdot} u\right) \tag6 \end{align*} $(c)$ \begin{align*} D_kf_(x)&=\lim_{h \to 0} \frac{f(x+he_k)-f(x)}{h}\\ &=\lim_{h \to 0} \frac{(x+he_k) \boldsymbol{\cdot} L(x+he_k)-x \boldsymbol{\cdot} L(x)}{h}\\ &=\lim_{h \to 0} \frac{(x+he_k) \boldsymbol{\cdot} \left(L(x)+L(he_k)\right)-x \boldsymbol{\cdot} L(x)}{h}\\ &=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} L(x)+x \boldsymbol{\cdot} L(he_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(he_k)-x \boldsymbol{\cdot} L(x)}{h}\\ &=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} L(he_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(he_k)}{h}\\ &=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} hL(e_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} hL(e_k)}{h}\\ &=\lim_{h \to 0} \frac{hx \boldsymbol{\cdot} L(e_k)+he_k \boldsymbol{\cdot} L(x)+h^2e_k \boldsymbol{\cdot} L(e_k)}{h}\\ &=\lim_{h \to 0} \frac{h\left(x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(e_k)\right)}{h}\\ &=\lim_{h \to 0} x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(e_k)\\ &=x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x) \tag7 \end{align*} By continuity of $x$ and $L(x)$, we conclude that $D_kf_(x)$ is continuous for all $k \in \{1,\dots,n\}$. Thus, by the theorem stated above, $f$ is differentiable and hence, directional derivative of $f$ exists in all directions. Let $u=(u_1,\dots,u_n)$. Then, \begin{align*} f'(x;u)&=\sum_{k=1}^n u_kD_kf_(x)\\ &=\sum_{k=1}^n u_k\left(x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)\right)\\ &=\sum_{k=1}^n \left(x \boldsymbol{\cdot} u_kL(e_k)+u_ke_k \boldsymbol{\cdot} L(x)\right)\\ &=\sum_{k=1}^n \left(x \boldsymbol{\cdot} L(u_ke_k)+u_ke_k \boldsymbol{\cdot} L(x)\right)\\ &=x \boldsymbol{\cdot} \sum_{k=1}^nL(u_ke_k)+\sum_{k=1}^n\left(u_ke_k\right) \boldsymbol{\cdot} L(x)\\ &=x \boldsymbol{\cdot} L(\sum_{k=1}^n u_ke_k)+\sum_{k=1}^n\left(u_ke_k\right) \boldsymbol{\cdot} L(x)\\ &=x \boldsymbol{\cdot} L(u)+u \boldsymbol{\cdot} L(x) \tag8 \end{align*} $(d)$ \begin{align*} f(x)&=\sum_{i=1}^n\sum_{j=1}^n a_{ij}x_ix_j\\ &=a_{kk}x_k^2+\sum{i \neq k}a_{ik}x_ix_k+\sum{i \neq k}a_{ki}x_kx_i+\sum_{i \neq k,\,j \neq k,\,i \neq j} a_{ij}x_ix_j \tag9 \end{align*} Then we have, \begin{align*} D_kf(x)&=2a_{kk}x_k+\sum_{i \neq k} a_{ik}x_i+\sum_{i \neq k} a_{ki}x_i\\ &=2a_{kk}x_k+2\sum_{i \neq k} a_{ik}x_i \,\left(\text{since } a_{ik}=a_{ki}\right)\\ &=2\sum_{i=1}^n a_{ik}x_i \tag{10} \end{align*} Thus, $D_kf(x)$ exists and is continuous. Hence, $f$ is differnetiable and thus, it has directional derivative in every direction. Then, $$f'(x;u)=\sum_{k=1}^n u_k \frac{\partial f}{\partial x_k}(x)=2\sum_{i=1}^n u_k\left(\sum_{i=1}^n a_{ik}x_i\right)=2\sum_{i=1}^n\sum_{i=1}^n a_{ik}x_iu_k=2x^TAu, \tag{11}$$ where $A={(a_{ij})_{i=1}^n}_{j=1}^n$.","As a continuation of this post , I'm posting my solutions (or attempts) to the exercise problems of Chapter 12 (Multivariable Differential Calculus), Mathematical Analysis by Apostol . Since I'm essentially self-studying, I'd really appreciate if anyone checks the solutions and let me know if there is any gap in my arguments or if there exists any cleverer solutions. Thank you. Problem $12.2.$ Calculate all first order partial derivatives and the directional derivative $f'(x;u)$ for each of the real-valued functions defined on $\mathbb{R}^n$ as follows : \begin{align*} &(a)\,\, f(x)=a \boldsymbol{\cdot} x\\ &(b)\,\, f(x)=\left\Vert x \right\Vert^4\\ &(c)\,\, f(x)=x \boldsymbol{\cdot} L(x), \text{ where } L:\mathbb{R}^n \to \mathbb{R}^n \text{ is a linear function.}\\ &(d)\,\, f(x)=\sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j, \text{ where } a_{ij}=a_{ji}. \end{align*} Solution. $(a)$ Let $a=(a_1,\dots,a_n)$, $x=(x_1,\dots,x_n)$. Thus, $$f(x)=a \boldsymbol{\cdot} x=a_1x_1+\dots+a_nx_n \tag1$$ Then we have, $$D_kf(x)=\frac{\partial f}{\partial x_k}(x)=a_k=a \boldsymbol{\cdot} e_k; \,\,k=1,\dots,n \tag2$$ $D_kf(x)$ is constant for all $k$ $\Rightarrow$ $D_kf(x)$ is continuous for all $k$. We recall the following : Theorem. Assume that one of the partial derivatives $D_1f,\dots,D_nf$ exists at $c$ and that the remaining $n−1$ partial derivatives exists in some $\delta$-ball $B_{\delta}(c)$, and are continuous at $c$. Then $f$ is differentiable at $c$. Using the theorem, we conclude that $f$ is differentiable and hence directional derivative at any direction exists. Then, \begin{align*} f'(x;u)&=\lim_{h \to 0} \frac{f(x+hu)-f(x)}{h}\\ &=\lim_{h \to 0} \frac{a \boldsymbol{\cdot} (x+hu)-a \boldsymbol{\cdot} x}{h}\\ &=\lim_{h \to 0} \frac{a \boldsymbol{\cdot} (hu)}{h}\\ &=\lim_{h \to 0} \frac{h \sum_{i=1}^n a_iu_i}{h}\\ &=\sum_{i=1}^n a_iu_i=a \boldsymbol{\cdot} u \tag3 \end{align*} Solution. $(b)$ \begin{align*} f(x)&=\left\Vert x \right\Vert^4=\left(\sum_{i=1}^n x_i^2\right)^2=\sum_{i=1}^n x_i^4+\sum_{i \neq j} x_i^2x_j^2\\ &=x_k^4+\sum_{i \neq k} x_i^4+x_k^2\left(\sum_{i \neq k} x_i^2\right)+\left(\sum_{i \neq k} x_i^2\right)x_k^2+\sum_{i \neq k,\,j \neq k,\,i \neq j} x_i^2x_j^2\\ &=x_k^4+\sum_{i \neq k} x_i^4+2x_k^2\left(\sum_{i \neq k} x_i^2\right)+\sum_{i \neq k,\,j \neq k,\,i \neq j} x_i^2x_j^2 \tag4 \end{align*} Let $k \in \{1,\dots,n\}$. Then, $$D_kf(x)=4x_k^3+4x_k\left(\sum_{i \neq k}x_i^2\right)=4x_k\left(\sum_{i=1}^nx_i^2\right)=4x_k\left\Vert x \right\Vert^2 \tag5$$ Thus, $D_kf(x)$ exists and is continuous for all $k \in \{1,\dots,n\}$. By the statement stated above, $f$ is differentiable, and hence directional derivative exists in all directions. Let $u=(u_1,\dots,u_n)=u_1e_1+\dots+u_ne_n$. Now, \begin{align*} f'(x;u)&=f'(x)(u)=f'(x)(u_1e_1+\dots+u_ne_n)=\sum_{k=1}^n u_kf'(x)(e_k)=\sum_{k=1}^n u_kf'(x;e_k)\\ &=\sum_{k=1}^n u_k\frac{\partial f}{\partial x_k}(x)=\sum_{k=1}^n u_k\left(4x_k\left\Vert x \right\Vert^2\right)=4\left\Vert x \right\Vert^2\sum_{k=1}^n x_ku_k=4\left\Vert x \right\Vert^2 \left(x \boldsymbol{\cdot} u\right) \tag6 \end{align*} $(c)$ \begin{align*} D_kf_(x)&=\lim_{h \to 0} \frac{f(x+he_k)-f(x)}{h}\\ &=\lim_{h \to 0} \frac{(x+he_k) \boldsymbol{\cdot} L(x+he_k)-x \boldsymbol{\cdot} L(x)}{h}\\ &=\lim_{h \to 0} \frac{(x+he_k) \boldsymbol{\cdot} \left(L(x)+L(he_k)\right)-x \boldsymbol{\cdot} L(x)}{h}\\ &=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} L(x)+x \boldsymbol{\cdot} L(he_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(he_k)-x \boldsymbol{\cdot} L(x)}{h}\\ &=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} L(he_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(he_k)}{h}\\ &=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} hL(e_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} hL(e_k)}{h}\\ &=\lim_{h \to 0} \frac{hx \boldsymbol{\cdot} L(e_k)+he_k \boldsymbol{\cdot} L(x)+h^2e_k \boldsymbol{\cdot} L(e_k)}{h}\\ &=\lim_{h \to 0} \frac{h\left(x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(e_k)\right)}{h}\\ &=\lim_{h \to 0} x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(e_k)\\ &=x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x) \tag7 \end{align*} By continuity of $x$ and $L(x)$, we conclude that $D_kf_(x)$ is continuous for all $k \in \{1,\dots,n\}$. Thus, by the theorem stated above, $f$ is differentiable and hence, directional derivative of $f$ exists in all directions. Let $u=(u_1,\dots,u_n)$. Then, \begin{align*} f'(x;u)&=\sum_{k=1}^n u_kD_kf_(x)\\ &=\sum_{k=1}^n u_k\left(x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)\right)\\ &=\sum_{k=1}^n \left(x \boldsymbol{\cdot} u_kL(e_k)+u_ke_k \boldsymbol{\cdot} L(x)\right)\\ &=\sum_{k=1}^n \left(x \boldsymbol{\cdot} L(u_ke_k)+u_ke_k \boldsymbol{\cdot} L(x)\right)\\ &=x \boldsymbol{\cdot} \sum_{k=1}^nL(u_ke_k)+\sum_{k=1}^n\left(u_ke_k\right) \boldsymbol{\cdot} L(x)\\ &=x \boldsymbol{\cdot} L(\sum_{k=1}^n u_ke_k)+\sum_{k=1}^n\left(u_ke_k\right) \boldsymbol{\cdot} L(x)\\ &=x \boldsymbol{\cdot} L(u)+u \boldsymbol{\cdot} L(x) \tag8 \end{align*} $(d)$ \begin{align*} f(x)&=\sum_{i=1}^n\sum_{j=1}^n a_{ij}x_ix_j\\ &=a_{kk}x_k^2+\sum{i \neq k}a_{ik}x_ix_k+\sum{i \neq k}a_{ki}x_kx_i+\sum_{i \neq k,\,j \neq k,\,i \neq j} a_{ij}x_ix_j \tag9 \end{align*} Then we have, \begin{align*} D_kf(x)&=2a_{kk}x_k+\sum_{i \neq k} a_{ik}x_i+\sum_{i \neq k} a_{ki}x_i\\ &=2a_{kk}x_k+2\sum_{i \neq k} a_{ik}x_i \,\left(\text{since } a_{ik}=a_{ki}\right)\\ &=2\sum_{i=1}^n a_{ik}x_i \tag{10} \end{align*} Thus, $D_kf(x)$ exists and is continuous. Hence, $f$ is differnetiable and thus, it has directional derivative in every direction. Then, $$f'(x;u)=\sum_{k=1}^n u_k \frac{\partial f}{\partial x_k}(x)=2\sum_{i=1}^n u_k\left(\sum_{i=1}^n a_{ik}x_i\right)=2\sum_{i=1}^n\sum_{i=1}^n a_{ik}x_iu_k=2x^TAu, \tag{11}$$ where $A={(a_{ij})_{i=1}^n}_{j=1}^n$.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'proof-verification']"
80,What does a segment in the plane as a metric space defined by a $p$-norm look like?,What does a segment in the plane as a metric space defined by a -norm look like?,p,"In the metric space $\mathbb R^2$ with the metric $d$ defined by $d(x,y)= (|x_1-y_1|^p+|x_2-y_2|^p)^{1/p}$, where $p\gt1$ is a real number, like what does the set of all $m\in \mathbb R^2$ with $d(a,m)+d(m,b)=d(a,b)$ look, where $a$ and $b$ are two arbitrary points of $\mathbb R^2$? I think for all $p$’s it is a straight line segment as in the obvious case $p=2$ but I do not know how to deal with the other values of $p$.","In the metric space $\mathbb R^2$ with the metric $d$ defined by $d(x,y)= (|x_1-y_1|^p+|x_2-y_2|^p)^{1/p}$, where $p\gt1$ is a real number, like what does the set of all $m\in \mathbb R^2$ with $d(a,m)+d(m,b)=d(a,b)$ look, where $a$ and $b$ are two arbitrary points of $\mathbb R^2$? I think for all $p$’s it is a straight line segment as in the obvious case $p=2$ but I do not know how to deal with the other values of $p$.",,['real-analysis']
81,A Smooth Function with Peculiar Properties,A Smooth Function with Peculiar Properties,,Suppose $f:\mathbb R^n\to\mathbb R$ is a smooth function and define $E_f := \{x\in\mathbb R^n\;|\;f(x)=0\text{ and }\nabla f(x)\ne 0\}$. Can we find $f$ such that $E_f$ has positive $n$-dimensional Lebesgue measure?,Suppose $f:\mathbb R^n\to\mathbb R$ is a smooth function and define $E_f := \{x\in\mathbb R^n\;|\;f(x)=0\text{ and }\nabla f(x)\ne 0\}$. Can we find $f$ such that $E_f$ has positive $n$-dimensional Lebesgue measure?,,"['calculus', 'real-analysis', 'measure-theory', 'smooth-functions']"
82,Laplace equation in unit square,Laplace equation in unit square,,"I would like to solve the $\triangle u(x,y) = 0$ in the unit square, with periodic BC when $x=0,1$ and Neumann condition when $y=0,1$  $$\partial_y u(x,0) = \begin{cases} A \quad &\text{for } 0\leq x\leq L\\ B \quad &\text{for } L<x\leq 1\\ \end{cases}\quad \partial_y u(x,1) = \begin{cases} B \quad &\text{for } 0\leq x\leq 1-L\\  A \quad &\text{for } 1-L<x\leq 1\\ \end{cases} $$ We also fix $u(0,0) = 0$ to make the solution unique. The hint said instead of considering Green's functions or separation of variables, think about simple solutions of $\triangle u = 0$, draw a picture consider the solution in different parts of the domain and derive jump conditions to connect the solutions. My attempt: Here let us call  $$g(x) = \begin{cases} A \quad &\text{for } 0\leq x\leq L\\ B \quad &\text{for } L<x\leq 1\\ \end{cases}$$  and note that the Nuemann condition at $z=0$ is $g(x)$, and at $z=1$ is $g(1-x)$. If provided $g(x) = \sum_n a_n \cos(n\pi x)$ (point-wise or uniform), we can define  $$u(x,y) = \sum_n a_n \frac{\sin(n\pi x - n\pi y)}{-n\pi}.$$ (I realized this $u$ is not harmonic) $u$ is periodic when $x=0,1$, and $$\partial_y u(x,0) = \sum _n a_n \cos(n\pi x) = g(x) $$ and  $$\partial_y u(x,1) = \sum _n a_n \cos(n\pi (x - 1))=\sum _n a_n \cos(n\pi (1-x)) = g(1-x).$$ But I am not sure if we can find a cosine series which pointwise converges to $g(x)$, how could we fix this? I think I have $L^2$ convergence  $$\sum_n a_n \cos(n\pi x) \rightarrow g \text{ in } L^2$$ which means I have pointwise a.e. convergence because of the summation.","I would like to solve the $\triangle u(x,y) = 0$ in the unit square, with periodic BC when $x=0,1$ and Neumann condition when $y=0,1$  $$\partial_y u(x,0) = \begin{cases} A \quad &\text{for } 0\leq x\leq L\\ B \quad &\text{for } L<x\leq 1\\ \end{cases}\quad \partial_y u(x,1) = \begin{cases} B \quad &\text{for } 0\leq x\leq 1-L\\  A \quad &\text{for } 1-L<x\leq 1\\ \end{cases} $$ We also fix $u(0,0) = 0$ to make the solution unique. The hint said instead of considering Green's functions or separation of variables, think about simple solutions of $\triangle u = 0$, draw a picture consider the solution in different parts of the domain and derive jump conditions to connect the solutions. My attempt: Here let us call  $$g(x) = \begin{cases} A \quad &\text{for } 0\leq x\leq L\\ B \quad &\text{for } L<x\leq 1\\ \end{cases}$$  and note that the Nuemann condition at $z=0$ is $g(x)$, and at $z=1$ is $g(1-x)$. If provided $g(x) = \sum_n a_n \cos(n\pi x)$ (point-wise or uniform), we can define  $$u(x,y) = \sum_n a_n \frac{\sin(n\pi x - n\pi y)}{-n\pi}.$$ (I realized this $u$ is not harmonic) $u$ is periodic when $x=0,1$, and $$\partial_y u(x,0) = \sum _n a_n \cos(n\pi x) = g(x) $$ and  $$\partial_y u(x,1) = \sum _n a_n \cos(n\pi (x - 1))=\sum _n a_n \cos(n\pi (1-x)) = g(1-x).$$ But I am not sure if we can find a cosine series which pointwise converges to $g(x)$, how could we fix this? I think I have $L^2$ convergence  $$\sum_n a_n \cos(n\pi x) \rightarrow g \text{ in } L^2$$ which means I have pointwise a.e. convergence because of the summation.",,"['real-analysis', 'partial-differential-equations', 'harmonic-functions', 'boundary-value-problem']"
83,Series whose Cauchy product is absolutely convergent - A general example,Series whose Cauchy product is absolutely convergent - A general example,,"Is there series that is divergent or conditionally convergent with absolutely convergent Cauchy product? Seems like there is a group of these examples! Perhaps finding divergent series with absolutely convergent Cauchy product isn't that difficult (see here for an example), but perhaps finding conditionally convergent series with absolutely convergent Cauchy product is more complicated. I've been studying the paper by Florian Cajori after RRL posted a comment in my post: Conditionally converges $\sum_{k=0}^\infty a_n$, $\sum_{k=0}^\infty b_n$ and also their Cauchy product . Perhaps the paper is a bit 'ancient' (100 years ago) (perhaps mathematicians 100 years ago had a style a little bit differently than modern day mathematicians). Perhaps mathematicians 100 years ago had a style a little bit differently than modern day mathematicians. Or else, I think my mathematical maturity isn't enough, so I used lots of time to read his paper but still fails to understand all materials in the  paper. (For example, I only realize what is meant by 'dropping parentheses' after I read it for an hour...) The author of the paper gives two examples, the first example is about, there are conditionally convergent series such that their Cauchy product are absolutely convergent.  That example, is digested by me after some works. My works on this example, hope that there aren't typos or errors :). P.1 P.2 P.3 P.4 P.5 He gives 'the general method' later in the paper. That's what I'm stuggling. Not to say that I can't understand why the constraints are set that way, I even don't know how to prove the general method (and the author can't write down his delicate proof because the margin is too small). Following his words, I conjectured that: If  $\hspace{2ex}$$b,c\in\mathbb N, a=2c;\vert d\vert=\min\{\vert l\vert:b\equiv l\text{  }(\text{mod }a)\}, d\mid c$; $\hspace{4ex}$$\forall(k=1,...,c),\gamma_k=1$; $\forall (k=c+1,...,a), >  > \gamma_k=-1$; $\hspace{4ex}$$\forall(k=1,...,b),\delta_k\in\{-1,1\}$; $\hspace{4ex}$$\forall (n\ge m_1,k=1,...,a),s_{n,k}=\frac{\gamma_k} {an+\alpha_k},\text{ while }m_1\in\mathbb Z$ $\hspace{8ex}$is chosen s.t. $(s_{n,k})$ are well-defined; $\hspace{4ex}$$\forall (n\ge m_2,k=1,...,b), t_{n,k}=\frac{\delta_k} {bn+\beta_k},\text{ while }m_2\in\mathbb Z$ $\hspace{8ex}$is chosen s.t. $(t_{n,k})$ are well-defined; $\hspace{4ex}$$\forall(n\ge m_1,k=1,...,a), u_{an+k}=s_{n,k}$; $\hspace{4ex}$$\forall (n\ge m_2,k=1,...,b), v_{bn+k}= t_{n,k}$; $\hspace{4ex}$$m\ge \max\{am_1+1,bm_2+1\}$ is an arbitrary integer. Then $\sum_{n=m}^\infty u_n$ converges conditionally and Cauchy   product of $\bigg(\sum_{n=m}^\infty u_n,\sum_{n=m}^\infty v_n\bigg)$, $\hspace{4ex}$i.e. $\sum_{n=m}^\infty c_n=\sum_{n=m}^\infty \bigg( \sum_{k=m}^n u_k v_{n+m-k} \bigg)$ converges absolutely. Here's what I've tried: I hope to break (It is called 'dropping paraentheses', as mentioned in the paper) the series $\sum_n^\infty \Big(c_{nab+1}+c_{nab+2}+...+c_{nab+(ab-1)}\Big)$ into $\sum_n^\infty c_{nab}, \sum_n^\infty c_{nab+1},...\sum_n^\infty c_{nab+(ab-1)}$ so that it can be managed as the way for the particular example before. Should I require further that $(ab)$ divides $m$? But even after that,... Inspired by the proof of Merten's theorem , does $\sum_{i=0}^N\sum_{j=0}^i a_jb_{i-j}=\sum_{j=0}^Na_{N-j}B_j$ helps? Perhaps we can use the value of the series that converges conditionally to prove the Cauchy product above converges? But it is only convergent then, not necessarily absolutely. However, I thought, even managing the case when $a=b=4$ (the example before) is complicated and heavy, how can I allow $a,b$ to be arbitray (and maybe distinct) natural number and develop a general algorithm to prove the theorem? If there is something like mathematical induction to handle countable infinite amount of statement, which is applicable here, maybe I can handle it. Well... however, induction obviously doesn't seems to work here directly . (or maybe, there is, an exciting, trick?) I observed that in the first example stated in the paper, $\sum_{n=0}^\infty c_n=0$, which is because $\big(\sum_{n=0}^\infty a_n\big)\big(\sum_{n=0}^\infty b_n\big)=0$. Should we make more conditions (such as requiring $\sum_{n=0}^\infty u_n=0$?) according to this? Any hope will be appreciated. I may just want some constructive hints... Thank you!","Is there series that is divergent or conditionally convergent with absolutely convergent Cauchy product? Seems like there is a group of these examples! Perhaps finding divergent series with absolutely convergent Cauchy product isn't that difficult (see here for an example), but perhaps finding conditionally convergent series with absolutely convergent Cauchy product is more complicated. I've been studying the paper by Florian Cajori after RRL posted a comment in my post: Conditionally converges $\sum_{k=0}^\infty a_n$, $\sum_{k=0}^\infty b_n$ and also their Cauchy product . Perhaps the paper is a bit 'ancient' (100 years ago) (perhaps mathematicians 100 years ago had a style a little bit differently than modern day mathematicians). Perhaps mathematicians 100 years ago had a style a little bit differently than modern day mathematicians. Or else, I think my mathematical maturity isn't enough, so I used lots of time to read his paper but still fails to understand all materials in the  paper. (For example, I only realize what is meant by 'dropping parentheses' after I read it for an hour...) The author of the paper gives two examples, the first example is about, there are conditionally convergent series such that their Cauchy product are absolutely convergent.  That example, is digested by me after some works. My works on this example, hope that there aren't typos or errors :). P.1 P.2 P.3 P.4 P.5 He gives 'the general method' later in the paper. That's what I'm stuggling. Not to say that I can't understand why the constraints are set that way, I even don't know how to prove the general method (and the author can't write down his delicate proof because the margin is too small). Following his words, I conjectured that: If  $\hspace{2ex}$$b,c\in\mathbb N, a=2c;\vert d\vert=\min\{\vert l\vert:b\equiv l\text{  }(\text{mod }a)\}, d\mid c$; $\hspace{4ex}$$\forall(k=1,...,c),\gamma_k=1$; $\forall (k=c+1,...,a), >  > \gamma_k=-1$; $\hspace{4ex}$$\forall(k=1,...,b),\delta_k\in\{-1,1\}$; $\hspace{4ex}$$\forall (n\ge m_1,k=1,...,a),s_{n,k}=\frac{\gamma_k} {an+\alpha_k},\text{ while }m_1\in\mathbb Z$ $\hspace{8ex}$is chosen s.t. $(s_{n,k})$ are well-defined; $\hspace{4ex}$$\forall (n\ge m_2,k=1,...,b), t_{n,k}=\frac{\delta_k} {bn+\beta_k},\text{ while }m_2\in\mathbb Z$ $\hspace{8ex}$is chosen s.t. $(t_{n,k})$ are well-defined; $\hspace{4ex}$$\forall(n\ge m_1,k=1,...,a), u_{an+k}=s_{n,k}$; $\hspace{4ex}$$\forall (n\ge m_2,k=1,...,b), v_{bn+k}= t_{n,k}$; $\hspace{4ex}$$m\ge \max\{am_1+1,bm_2+1\}$ is an arbitrary integer. Then $\sum_{n=m}^\infty u_n$ converges conditionally and Cauchy   product of $\bigg(\sum_{n=m}^\infty u_n,\sum_{n=m}^\infty v_n\bigg)$, $\hspace{4ex}$i.e. $\sum_{n=m}^\infty c_n=\sum_{n=m}^\infty \bigg( \sum_{k=m}^n u_k v_{n+m-k} \bigg)$ converges absolutely. Here's what I've tried: I hope to break (It is called 'dropping paraentheses', as mentioned in the paper) the series $\sum_n^\infty \Big(c_{nab+1}+c_{nab+2}+...+c_{nab+(ab-1)}\Big)$ into $\sum_n^\infty c_{nab}, \sum_n^\infty c_{nab+1},...\sum_n^\infty c_{nab+(ab-1)}$ so that it can be managed as the way for the particular example before. Should I require further that $(ab)$ divides $m$? But even after that,... Inspired by the proof of Merten's theorem , does $\sum_{i=0}^N\sum_{j=0}^i a_jb_{i-j}=\sum_{j=0}^Na_{N-j}B_j$ helps? Perhaps we can use the value of the series that converges conditionally to prove the Cauchy product above converges? But it is only convergent then, not necessarily absolutely. However, I thought, even managing the case when $a=b=4$ (the example before) is complicated and heavy, how can I allow $a,b$ to be arbitray (and maybe distinct) natural number and develop a general algorithm to prove the theorem? If there is something like mathematical induction to handle countable infinite amount of statement, which is applicable here, maybe I can handle it. Well... however, induction obviously doesn't seems to work here directly . (or maybe, there is, an exciting, trick?) I observed that in the first example stated in the paper, $\sum_{n=0}^\infty c_n=0$, which is because $\big(\sum_{n=0}^\infty a_n\big)\big(\sum_{n=0}^\infty b_n\big)=0$. Should we make more conditions (such as requiring $\sum_{n=0}^\infty u_n=0$?) according to this? Any hope will be appreciated. I may just want some constructive hints... Thank you!",,"['real-analysis', 'sequences-and-series', 'absolute-convergence', 'conditional-convergence', 'cauchy-product']"
84,On the positivity of the distance between two disjoint ellipsoids,On the positivity of the distance between two disjoint ellipsoids,,"Let closed convex sets $P_1$ and $P_2$ be defined as follows $$P_i := \{ x \in \Bbb R^n  : x^\top A_i x + b_i^\top x + c_i \leq 0  \}$$ where $A_1$ and $A_2$ are positive semidefinite matrices. Assume $P_1 \cap P_2 = \emptyset$ . Prove (or provide a counter-example) that $$d(P_1 , P_2):= \inf \{ \|x-y\| \; | \; x \in P_1 ,~ y \in P_2  \}  >  0$$",Let closed convex sets and be defined as follows where and are positive semidefinite matrices. Assume . Prove (or provide a counter-example) that,"P_1 P_2 P_i := \{ x \in \Bbb R^n  : x^\top A_i x + b_i^\top x + c_i \leq 0  \} A_1 A_2 P_1 \cap P_2 = \emptyset d(P_1 , P_2):= \inf \{ \|x-y\| \; | \; x \in P_1 ,~ y \in P_2  \}  >  0","['real-analysis', 'convex-analysis', 'convex-optimization']"
85,Continuity of $x^\alpha$,Continuity of,x^\alpha,"Let $f(x)=x^\alpha$ where $\alpha\in(0,1)$ I want to show to that $f(x)$ is continuous on the interval $I=[0,+\infty)$ Let $x_{0}\in[0,+\infty)$ $|f(x)-f(x_{0}|=|x^\alpha-{x_{0}}^\alpha|$ For $\alpha\in\mathbb{Q}\cap(0,1)$, we can write $\alpha=\frac{a}{b}$ where $a,b\in\mathbb{N}$ and $a<b$ I know how to prove that $x^{\frac{1}{b}}$ is continuous on $I$ and that $x^a$ is continous on $I$. So by composition, so is $x^{\frac{a}{b}}$. However, what do I do in the case where $\alpha\in\mathbb{Q^\complement}\cap(0,1)$?","Let $f(x)=x^\alpha$ where $\alpha\in(0,1)$ I want to show to that $f(x)$ is continuous on the interval $I=[0,+\infty)$ Let $x_{0}\in[0,+\infty)$ $|f(x)-f(x_{0}|=|x^\alpha-{x_{0}}^\alpha|$ For $\alpha\in\mathbb{Q}\cap(0,1)$, we can write $\alpha=\frac{a}{b}$ where $a,b\in\mathbb{N}$ and $a<b$ I know how to prove that $x^{\frac{1}{b}}$ is continuous on $I$ and that $x^a$ is continous on $I$. So by composition, so is $x^{\frac{a}{b}}$. However, what do I do in the case where $\alpha\in\mathbb{Q^\complement}\cap(0,1)$?",,"['real-analysis', 'continuity']"
86,"Showing $\int_0^s \int_0^u (F_1(u)-F_1(u-v)) f_1(u-v) e^{-v}\, dv\, du \leq \int_0^s \int_0^u (F_2(u)-F_2(u-v)) f_2(u-v) e^{-v}\, dv\, du$",Showing,"\int_0^s \int_0^u (F_1(u)-F_1(u-v)) f_1(u-v) e^{-v}\, dv\, du \leq \int_0^s \int_0^u (F_2(u)-F_2(u-v)) f_2(u-v) e^{-v}\, dv\, du","Question Let $F(s)$ be a cumulative distribution function (cdf) of a random variable on $[0,\infty)$ which only has an atom at $0$, i.e. $F(0) >0$ and for all $s>0$:$$ \lim_{h\rightarrow 0}F(s+h)=F(s).$$ Let $\bar{F}(s)=1-F(s)$ and $f = F'(s)$. Note that$$ F(0) = 1-\int_0^\infty f(s) ds.$$ Now define the following operation on $F$: $$ HF(s)=\int_0^s \int_0^u (F(u)-F(u-v)) f(u-v) e^{-v}\, dv\, du. $$ Now suppose $F_1,F_2$ are both cdfs as described above, and suppose for all $s$, $F_1(s) \leq F_2(s)$ and $F_1(0)=F_2(0)$, show that $HF_1(s) \leq HF_2(s)$ for all $s$. Thoughts One can use Fubini to change the order of integration, the second part can then be solved by writing it as: \begin{align*} -\int_0^s \int_v^s F(u-v) f(u-v) \, du e^{-v}\, dv &= -\int_0^s\int_0^{s-v}F(u)f(u)\,du e^{-v}\,dv\\ &=-\frac{1}{2} \int_0^s \left( F(s-v)^2-F(0)^2 \right) e^{-v}\, dv. \end{align*} However we can not do something similar for the first part as here the argument of $F$ and $f$ are not the same.. Partal Integration As suggested in the answer, partial integration can be used to get rid of the density $f$. An alternative partial integration to use to simplify the first part is to note that the first part equals: $$ \int_0^sF(u)\int_0^uf(u-v) e^{-v}dv du $$ and then use partial integration on $\int_0^uf(u-v) e^{-v}dv$, integrating $f(u-v)$ and differentiating $e^{-v}$.","Question Let $F(s)$ be a cumulative distribution function (cdf) of a random variable on $[0,\infty)$ which only has an atom at $0$, i.e. $F(0) >0$ and for all $s>0$:$$ \lim_{h\rightarrow 0}F(s+h)=F(s).$$ Let $\bar{F}(s)=1-F(s)$ and $f = F'(s)$. Note that$$ F(0) = 1-\int_0^\infty f(s) ds.$$ Now define the following operation on $F$: $$ HF(s)=\int_0^s \int_0^u (F(u)-F(u-v)) f(u-v) e^{-v}\, dv\, du. $$ Now suppose $F_1,F_2$ are both cdfs as described above, and suppose for all $s$, $F_1(s) \leq F_2(s)$ and $F_1(0)=F_2(0)$, show that $HF_1(s) \leq HF_2(s)$ for all $s$. Thoughts One can use Fubini to change the order of integration, the second part can then be solved by writing it as: \begin{align*} -\int_0^s \int_v^s F(u-v) f(u-v) \, du e^{-v}\, dv &= -\int_0^s\int_0^{s-v}F(u)f(u)\,du e^{-v}\,dv\\ &=-\frac{1}{2} \int_0^s \left( F(s-v)^2-F(0)^2 \right) e^{-v}\, dv. \end{align*} However we can not do something similar for the first part as here the argument of $F$ and $f$ are not the same.. Partal Integration As suggested in the answer, partial integration can be used to get rid of the density $f$. An alternative partial integration to use to simplify the first part is to note that the first part equals: $$ \int_0^sF(u)\int_0^uf(u-v) e^{-v}dv du $$ and then use partial integration on $\int_0^uf(u-v) e^{-v}dv$, integrating $f(u-v)$ and differentiating $e^{-v}$.",,"['real-analysis', 'inequality']"
87,Discontinuous function which is continuous when restricted to any algebraic curve,Discontinuous function which is continuous when restricted to any algebraic curve,,"Background : In a first course on multivariable calculus, it's really common to find examples of functions which are discontinuous, but continuous when restricted to any line, in order to build intuition for multivariable derivatives and limits. An example of such a function is the following: $$\begin{cases}        \frac{xy}{x^2+y^2} & x,y\neq 0 \\       0 & x,y=0    \end{cases}$$ This function is continuous when restricted to any line,  and by reparameterizing $y\mapsto y^k$, we find examples of functions which are discontinuous but continuous when restricted to higher-order algebraic curves. Question : Is there an example of a function which is discontinuous but continuous when restricted to any algebraic curve (specifically, the vanishing locus of some polynomial)? Inspiration : I recently tutored some students through some examples of the type mentioned above, and this popped up as a natural question to ask where I was unsure about what the answer was. Work/thoughts : I've tried replacing $y$ with $e^y$ or $e^y-1$, but it appears that this doesn't even handle the case of lines.","Background : In a first course on multivariable calculus, it's really common to find examples of functions which are discontinuous, but continuous when restricted to any line, in order to build intuition for multivariable derivatives and limits. An example of such a function is the following: $$\begin{cases}        \frac{xy}{x^2+y^2} & x,y\neq 0 \\       0 & x,y=0    \end{cases}$$ This function is continuous when restricted to any line,  and by reparameterizing $y\mapsto y^k$, we find examples of functions which are discontinuous but continuous when restricted to higher-order algebraic curves. Question : Is there an example of a function which is discontinuous but continuous when restricted to any algebraic curve (specifically, the vanishing locus of some polynomial)? Inspiration : I recently tutored some students through some examples of the type mentioned above, and this popped up as a natural question to ask where I was unsure about what the answer was. Work/thoughts : I've tried replacing $y$ with $e^y$ or $e^y-1$, but it appears that this doesn't even handle the case of lines.",,"['real-analysis', 'multivariable-calculus']"
88,"Looking for ${f_n}$ such that $\int_0^1 (x-t)^{m-1}f_n(t) dt = \delta_{n,m}$",Looking for  such that,"{f_n} \int_0^1 (x-t)^{m-1}f_n(t) dt = \delta_{n,m}","Good day, I am wondering whether it is possible to find a sequence of functions $f_n$ such that $$\int_0^1 (1-t)^{m-1}f_n(t) dt = \delta_{n,m}$$ for every $0<n,m$. Thank you.","Good day, I am wondering whether it is possible to find a sequence of functions $f_n$ such that $$\int_0^1 (1-t)^{m-1}f_n(t) dt = \delta_{n,m}$$ for every $0<n,m$. Thank you.",,"['real-analysis', 'integration', 'functional-analysis', 'definite-integrals', 'kronecker-delta']"
89,Limit at Infinity of Maclaurin Series,Limit at Infinity of Maclaurin Series,,"Let $f(x) = \sum_{n=1}^\infty a_n x^n$. What is $\lim_{x\rightarrow \infty} f(x)$ in terms of the $a_i$? That question may be too broad, so here are some restrictions: Assume f(x) is continuous (and therefore well-defined) on all of $\mathbb{R}^+$ and the limit exists in the extended reals. For example, if I were given the sequence $$\{a_i\}_{i=0}^\infty = \{1,-1,1/2,-1/6,1/24,\cdots, (-1)^i / i!, \cdots \}$$, I would have no idea what its limit at infinity is, but as soon I knew it was $e^{-x}$, taking the limit would be easy ($\lim_{x \rightarrow \infty} e^{-x} = 0$). The same is true for $e^x$. Any help is appreciated. Thanks edit: To make the question simpler, I would be happy with determining if the limit is finite or infinite.","Let $f(x) = \sum_{n=1}^\infty a_n x^n$. What is $\lim_{x\rightarrow \infty} f(x)$ in terms of the $a_i$? That question may be too broad, so here are some restrictions: Assume f(x) is continuous (and therefore well-defined) on all of $\mathbb{R}^+$ and the limit exists in the extended reals. For example, if I were given the sequence $$\{a_i\}_{i=0}^\infty = \{1,-1,1/2,-1/6,1/24,\cdots, (-1)^i / i!, \cdots \}$$, I would have no idea what its limit at infinity is, but as soon I knew it was $e^{-x}$, taking the limit would be easy ($\lim_{x \rightarrow \infty} e^{-x} = 0$). The same is true for $e^x$. Any help is appreciated. Thanks edit: To make the question simpler, I would be happy with determining if the limit is finite or infinite.",,"['real-analysis', 'sequences-and-series', 'limits']"
90,Estimating the measure of a sum of subsets,Estimating the measure of a sum of subsets,,"Let $A,B \subset \mathbb R^n$ be compact measurable subsets and let $A + B = \{ a+b \mid a \in A, b \in B \}$. If $m$ is the Lebesgue measure, is it possible to relate $m(A + B)$ to $m(A)$ and $m(B)$? In $\mathbb R$, for instance, $m([a,b] + [c,d]) = m([a+c, b+d]) = (b-a) + (d-c) = m([a,b]) + m([c,d])$. I would be happy, in general, with something like $m(A+B) \le m(A) + m(B)$ but I do not know whether this is possible.","Let $A,B \subset \mathbb R^n$ be compact measurable subsets and let $A + B = \{ a+b \mid a \in A, b \in B \}$. If $m$ is the Lebesgue measure, is it possible to relate $m(A + B)$ to $m(A)$ and $m(B)$? In $\mathbb R$, for instance, $m([a,b] + [c,d]) = m([a+c, b+d]) = (b-a) + (d-c) = m([a,b]) + m([c,d])$. I would be happy, in general, with something like $m(A+B) \le m(A) + m(B)$ but I do not know whether this is possible.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
91,"A question on Group of homeomorphism of $[0,1]$.",A question on Group of homeomorphism of .,"[0,1]","Consider $[0,1]$ with the usual euclidean topology. Now $G$ be the set of homeomorphisms from $[0,1]$ onto $[0,1]$. $G$ forms a group under composition. Now, Let $F=\{ f\in G | f(0)=0 \}\ $. Now $F$ is a normal subgroup of $G$. The problem is the following. Suppose $g\in F$ has exactly one fixed interior point. I need a homeomorphism $h \in F$ which is conjugate to $g$ in $F$ such that either of the following happens: $$  h(x)>g(x)  \hspace{4 cm} \forall x\in(0,1)$$  $$  h(x)>g^{-1}(x)  \hspace{3.5 cm} \forall x\in (0,1)$$ I have no idea how to come up with such a homeomorphism satifying either of the two properties and also being conjugate to $g$ in $F$. I suppose that $g$ having exactly one fixed interior point is crucial but I am not sure how to use it! Thanks in advance for any kind of help!","Consider $[0,1]$ with the usual euclidean topology. Now $G$ be the set of homeomorphisms from $[0,1]$ onto $[0,1]$. $G$ forms a group under composition. Now, Let $F=\{ f\in G | f(0)=0 \}\ $. Now $F$ is a normal subgroup of $G$. The problem is the following. Suppose $g\in F$ has exactly one fixed interior point. I need a homeomorphism $h \in F$ which is conjugate to $g$ in $F$ such that either of the following happens: $$  h(x)>g(x)  \hspace{4 cm} \forall x\in(0,1)$$  $$  h(x)>g^{-1}(x)  \hspace{3.5 cm} \forall x\in (0,1)$$ I have no idea how to come up with such a homeomorphism satifying either of the two properties and also being conjugate to $g$ in $F$. I suppose that $g$ having exactly one fixed interior point is crucial but I am not sure how to use it! Thanks in advance for any kind of help!",,"['real-analysis', 'general-topology', 'group-theory', 'topological-groups']"
92,$e^{\left(\pi^{(e^\pi)}\right)}\;$ or $\;\pi^{\left(e^{(\pi^e)}\right)}$. Which one is greater than the other?,or . Which one is greater than the other?,e^{\left(\pi^{(e^\pi)}\right)}\; \;\pi^{\left(e^{(\pi^e)}\right)},$\newcommand{\bigxl}[1]{\mathopen{\displaystyle#1}} \newcommand{\bigxr}[1]{\mathclose{\displaystyle#1}} $ $$\large e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\quad\text{or}\quad\pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ Which one is greater? Effort. I know that $$e^\pi\ge \pi^e$$ Then $$\pi^{(e^\pi)}\ge e^{(\pi^e)}$$ But I can't say $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\le \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ or $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\ge \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$,$\newcommand{\bigxl}[1]{\mathopen{\displaystyle#1}} \newcommand{\bigxr}[1]{\mathclose{\displaystyle#1}} $ $$\large e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\quad\text{or}\quad\pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ Which one is greater? Effort. I know that $$e^\pi\ge \pi^e$$ Then $$\pi^{(e^\pi)}\ge e^{(\pi^e)}$$ But I can't say $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\le \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ or $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\ge \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$,,"['real-analysis', 'inequality', 'exponential-function', 'pi']"
93,Can one use properties of polynomials in order to generalize the generalized Cauchy-Schwartz inequality?,Can one use properties of polynomials in order to generalize the generalized Cauchy-Schwartz inequality?,,"Sorry about the edits guys, I forgot to add binomial coefficients, I hope I didn't cause any needless confusion. Edit(again): I've been thinking about this a bit and perhaps I should clarify the question a bit more. I'm essentially asking the following question: Does generalizing the inner product allow one to use properties of polynomials, such as discriminants, in order to derive analogues of the Cauchy-Schwartz inequality by means of inequality $(II)$? If my talk of ""generalizing the generalized Cauchy Schwartz"" is utter nonsense please focus on this question instead. Another edit: I've been pondering this question all morning and it occurs to me that it may be rather trivial, most of the people who use this website are mathematicians right? This means that something like ""generalizing the inner product and exploring its consequences"" has surely been thought of before. This tells me that my question is probably highly imperfect, and even possibly unnatural. I apologize if this is the case. If this more general framework exists and allows one to come to a better understanding of a variety of questions posed which are similar to mine, could someone perhaps reveal this to me. Stated differently, what branch of mathematics can I study that would allow me to see just how naive this question is? Again, I'm very sorry if I have offended anyone by not knowing more mathematics. Take the following proof of Cauchy-Schwartz. The intuition behind this proof becomes clear when viewed from the perspective of an inner product. Start with an inner product, that is, a function $f:V^2\rightarrow\mathbb{R}$, $V$ a vector space, that satisfies the following axioms for scalars k. $(i) f(u,v) = f(v,u)$ $(ii) f(ku,v) = kf(u,v)$ $(iii) f(u+v,w) = f(u,w) + f(v,w)$ $(iv) f(u,u) \ge 0$ Where equality in $(iv)$ holds iff $u = 0$. Leveraging each of these axioms gives us the following inequality. $$f(u,v)^2 \le f(u,u)f(v,v) \tag{A}$$ since for scalars $t$ $$0 \le f(ut+v,ut+v)$$ implies $$0 \le f(u,u)t^2 + 2f(u,v)t + f(v,v)\tag{I}$$ whose discriminant must be  less than or equal to zero. If $f(u,v) = \sum u_kv_k $ then $(A)$ reduces to Cauchy-Schwartz. Now, generalize the inner product as follows, let $n$ be even and $f:V^n \rightarrow \mathbb{R}$ satisfy the following axioms $(i*) f(u_1,u_2,\cdots,u_n) = f(v_1,v_2,\cdots,v_n)$ whenever $(v_1,v_2,\cdots,v_n)$ is a permutation of $(u_1,u_2,\cdots,u_n)$ $(ii*) f(u_1,u_2,\cdots,ku_m, \cdots, u_n) = kf(u_1,u_2,\cdots,u_m, \cdots, u_n)$ $(iii*) f(u_1,u_2,\cdots,u_m + u_m', \cdots, u_n) = f(u_1,u_2,\cdots,u_m, \cdots, u_n) + f(u_1,u_2,\cdots,u_m', \cdots, u_n)$ $(iv*) f(u,u,\cdots,u) \ge 0$ where equality holds iff $u=0$ Such functions obviously exist, take $f(u,v,w,s) = u_1v_1w_1s_1 + u_2v_2w_2s_2 + \cdots + u_rv_rw_rs_r$ for example. Now, axiom $(iv*)$ tells us that for scalars $t$. $$0 \le f(ut+v,ut+v,\cdots,ut+v)$$ which implies (using axioms $(i*),(ii*)$, and $(iii*)$) that $$0 \le f(u,u,\cdots, u)t^n + \binom{n}{1}f(v,u,\cdots, u)t^{n-1} + \cdots + f(v,v,\cdots, v) \tag{II}$$ Notice that $n$ in $V^n$ must be even, otherwise inequality $(II)$ is contradictory. If $n$ were odd then the image of RHS must be $\mathbb{R}$, which is absurd. Also notice that letting $n = 2$ reduces $(II)$ to $(I)$ To give a specific example suppose that n=4 so that $$0 \le f(u,u,u,u)t^4 + 4f(v,u,u,u)t^3 + 6f(v,v,u,u)t^2 + 4f(v,v,v,u)t + f(v,v,v,v)$$ I'm wondering if one can use, say in the case of n=4, well known properties of quartic polynomials in order to derive more fundamental inequalities than Cauchy Schwartz. Perhaps this could tell us something about the function $u_1v_1w_1s_1 + u_2v_2w_2s_2 + \cdots + u_rv_rw_rs_r$ for example? More generally, can one use properties of polynomials, such as discriminants (I'm not very familiar with discriminants to be honest) in order to derive more fundamental inequalities than Cauchy-Schwartz on the basis of $(II)$? If not, what kind of useful and important inequalities can be produced by means of $(II)$. Just how far does inequality $(II)$ go? In the case of $n=2$ we obtain Cauchy-Schwartz, which is quite profound. What about $n=4, 6, 8, \ldots$? Just what does $(II)$ reveal to us in these cases?","Sorry about the edits guys, I forgot to add binomial coefficients, I hope I didn't cause any needless confusion. Edit(again): I've been thinking about this a bit and perhaps I should clarify the question a bit more. I'm essentially asking the following question: Does generalizing the inner product allow one to use properties of polynomials, such as discriminants, in order to derive analogues of the Cauchy-Schwartz inequality by means of inequality $(II)$? If my talk of ""generalizing the generalized Cauchy Schwartz"" is utter nonsense please focus on this question instead. Another edit: I've been pondering this question all morning and it occurs to me that it may be rather trivial, most of the people who use this website are mathematicians right? This means that something like ""generalizing the inner product and exploring its consequences"" has surely been thought of before. This tells me that my question is probably highly imperfect, and even possibly unnatural. I apologize if this is the case. If this more general framework exists and allows one to come to a better understanding of a variety of questions posed which are similar to mine, could someone perhaps reveal this to me. Stated differently, what branch of mathematics can I study that would allow me to see just how naive this question is? Again, I'm very sorry if I have offended anyone by not knowing more mathematics. Take the following proof of Cauchy-Schwartz. The intuition behind this proof becomes clear when viewed from the perspective of an inner product. Start with an inner product, that is, a function $f:V^2\rightarrow\mathbb{R}$, $V$ a vector space, that satisfies the following axioms for scalars k. $(i) f(u,v) = f(v,u)$ $(ii) f(ku,v) = kf(u,v)$ $(iii) f(u+v,w) = f(u,w) + f(v,w)$ $(iv) f(u,u) \ge 0$ Where equality in $(iv)$ holds iff $u = 0$. Leveraging each of these axioms gives us the following inequality. $$f(u,v)^2 \le f(u,u)f(v,v) \tag{A}$$ since for scalars $t$ $$0 \le f(ut+v,ut+v)$$ implies $$0 \le f(u,u)t^2 + 2f(u,v)t + f(v,v)\tag{I}$$ whose discriminant must be  less than or equal to zero. If $f(u,v) = \sum u_kv_k $ then $(A)$ reduces to Cauchy-Schwartz. Now, generalize the inner product as follows, let $n$ be even and $f:V^n \rightarrow \mathbb{R}$ satisfy the following axioms $(i*) f(u_1,u_2,\cdots,u_n) = f(v_1,v_2,\cdots,v_n)$ whenever $(v_1,v_2,\cdots,v_n)$ is a permutation of $(u_1,u_2,\cdots,u_n)$ $(ii*) f(u_1,u_2,\cdots,ku_m, \cdots, u_n) = kf(u_1,u_2,\cdots,u_m, \cdots, u_n)$ $(iii*) f(u_1,u_2,\cdots,u_m + u_m', \cdots, u_n) = f(u_1,u_2,\cdots,u_m, \cdots, u_n) + f(u_1,u_2,\cdots,u_m', \cdots, u_n)$ $(iv*) f(u,u,\cdots,u) \ge 0$ where equality holds iff $u=0$ Such functions obviously exist, take $f(u,v,w,s) = u_1v_1w_1s_1 + u_2v_2w_2s_2 + \cdots + u_rv_rw_rs_r$ for example. Now, axiom $(iv*)$ tells us that for scalars $t$. $$0 \le f(ut+v,ut+v,\cdots,ut+v)$$ which implies (using axioms $(i*),(ii*)$, and $(iii*)$) that $$0 \le f(u,u,\cdots, u)t^n + \binom{n}{1}f(v,u,\cdots, u)t^{n-1} + \cdots + f(v,v,\cdots, v) \tag{II}$$ Notice that $n$ in $V^n$ must be even, otherwise inequality $(II)$ is contradictory. If $n$ were odd then the image of RHS must be $\mathbb{R}$, which is absurd. Also notice that letting $n = 2$ reduces $(II)$ to $(I)$ To give a specific example suppose that n=4 so that $$0 \le f(u,u,u,u)t^4 + 4f(v,u,u,u)t^3 + 6f(v,v,u,u)t^2 + 4f(v,v,v,u)t + f(v,v,v,v)$$ I'm wondering if one can use, say in the case of n=4, well known properties of quartic polynomials in order to derive more fundamental inequalities than Cauchy Schwartz. Perhaps this could tell us something about the function $u_1v_1w_1s_1 + u_2v_2w_2s_2 + \cdots + u_rv_rw_rs_r$ for example? More generally, can one use properties of polynomials, such as discriminants (I'm not very familiar with discriminants to be honest) in order to derive more fundamental inequalities than Cauchy-Schwartz on the basis of $(II)$? If not, what kind of useful and important inequalities can be produced by means of $(II)$. Just how far does inequality $(II)$ go? In the case of $n=2$ we obtain Cauchy-Schwartz, which is quite profound. What about $n=4, 6, 8, \ldots$? Just what does $(II)$ reveal to us in these cases?",,"['real-analysis', 'functional-analysis', 'inequality', 'polynomials', 'cauchy-schwarz-inequality']"
94,"If the solution is too simple, it must be incorrect?","If the solution is too simple, it must be incorrect?",,"Prove that to each $\epsilon > 0$ there exists a $\delta > 0$ such that $\displaystyle \int_E |f| d \mu < \epsilon$ whenever $\mu (E) < \delta$, where $f \in L^1(\mu)$. I found this question asked on MSE here , here , and here , but some of the suggestions seem relatively complicated (at least compared to what I did). When I was solving the problem, no such ideas/theorems found in those links came to my mind; indeed, I just followed my nose by playing with inequalities. But, after looking at some other solutions, I am beginning to doubt that mine is correct. Here is what I did: First note that by definition \begin{align} \int_{E} |f|d \mu &= \sup \{\sum_{i=1}^m a_i \mu (A_i \cap E) \mid m \in \Bbb{N}, 0 \le a_i \le |f|, E = \bigsqcup_{i=1}^m A_i, A_i \mbox{measurable} \} \\ &\le \sup\{\max \{a_i\} \sum_{i=1}^m \mu(A_i \cap E) \mid ... \} \\ &= \sup\{\max \{a_i\} \mu(E) \mid ... \} \\ &= \mu(E) \cdot \sup\{\max \{a_i\} \mid ... \} \\ &\le \mu(E) \inf_{x \in E} |f(x)| \\ \end{align} The first inequality follows simply because the elements in the one set are larger than the other. The second because $0 \le a_i \le |f|$ on $E$ implies $a_i \le \inf_{x \in E} |f(x)|$ and therefore $\max \{a_i \} \le \inf_{x \in E} |f(x)|$. Now, if $\inf_{x \in E} |f(x)| = 0$, then any $\delta > 0$ will do, since the above together with this implies $\int_{E} |f| d \mu =0$. If not, then $\displaystyle \delta = \frac{\epsilon}{\inf_{x \in X} |f(x)|}$ will work. I can't spot the error.","Prove that to each $\epsilon > 0$ there exists a $\delta > 0$ such that $\displaystyle \int_E |f| d \mu < \epsilon$ whenever $\mu (E) < \delta$, where $f \in L^1(\mu)$. I found this question asked on MSE here , here , and here , but some of the suggestions seem relatively complicated (at least compared to what I did). When I was solving the problem, no such ideas/theorems found in those links came to my mind; indeed, I just followed my nose by playing with inequalities. But, after looking at some other solutions, I am beginning to doubt that mine is correct. Here is what I did: First note that by definition \begin{align} \int_{E} |f|d \mu &= \sup \{\sum_{i=1}^m a_i \mu (A_i \cap E) \mid m \in \Bbb{N}, 0 \le a_i \le |f|, E = \bigsqcup_{i=1}^m A_i, A_i \mbox{measurable} \} \\ &\le \sup\{\max \{a_i\} \sum_{i=1}^m \mu(A_i \cap E) \mid ... \} \\ &= \sup\{\max \{a_i\} \mu(E) \mid ... \} \\ &= \mu(E) \cdot \sup\{\max \{a_i\} \mid ... \} \\ &\le \mu(E) \inf_{x \in E} |f(x)| \\ \end{align} The first inequality follows simply because the elements in the one set are larger than the other. The second because $0 \le a_i \le |f|$ on $E$ implies $a_i \le \inf_{x \in E} |f(x)|$ and therefore $\max \{a_i \} \le \inf_{x \in E} |f(x)|$. Now, if $\inf_{x \in E} |f(x)| = 0$, then any $\delta > 0$ will do, since the above together with this implies $\int_{E} |f| d \mu =0$. If not, then $\displaystyle \delta = \frac{\epsilon}{\inf_{x \in X} |f(x)|}$ will work. I can't spot the error.",,"['real-analysis', 'measure-theory']"
95,"If $Y$ and $Z$ are subspaces of a vector space $X$, show that $Y\cap Z$ is a subspace of $X$, but $Y\cup Z$ need not be one. Give examples.","If  and  are subspaces of a vector space , show that  is a subspace of , but  need not be one. Give examples.",Y Z X Y\cap Z X Y\cup Z,"If $Y$ and $Z$ are subspaces of a vector space $X$, show that $Y\cap Z$ is a subspace of $X$, but $Y\cup Z$ need not be one. Give examples. Both $Y, Z$ are subspaces of $X$, which tells us that $0 ∈ Y$ and $0 ∈ Z$, which means $0 ∈ Y ∩ Z$. If $u, w ∈ Y ∩ Z$, then by definition $u, w ∈ Y$ and $u, w ∈ Z$. Since $Y,Z$ are subspaces, they are closed under addition - meaning $u + w ∈ Y$ and $u + w ∈ Z$, implying $u + w ∈ Y ∩ Z$. Any single element $u ∈ Y ∩ Z$ is in $Y$ and in $Z$, so that $cu ∈ Y$ and $cu ∈ Z$ for any scalar $c$, meaning of course that $cu ∈ Y ∩ Z$. Thus the intersection $Y ∩ Z$ is a subspace of $X$. For a counterexample, let $Y = \{(x,0): x ∈ \mathbb{R}\}$ and $Z = \{(0,y): y ∈ \mathbb{R}\}$. Then $u =(1,0) ∈ Y ⊂Y ∪ Z$,  $w =(0,1)∈ Z ⊂ Y ∪ Z$ But $u + w =(1,1)\notin Y \text{ or } Z ⇒ (u + w)\notin Y ∪ Z$.","If $Y$ and $Z$ are subspaces of a vector space $X$, show that $Y\cap Z$ is a subspace of $X$, but $Y\cup Z$ need not be one. Give examples. Both $Y, Z$ are subspaces of $X$, which tells us that $0 ∈ Y$ and $0 ∈ Z$, which means $0 ∈ Y ∩ Z$. If $u, w ∈ Y ∩ Z$, then by definition $u, w ∈ Y$ and $u, w ∈ Z$. Since $Y,Z$ are subspaces, they are closed under addition - meaning $u + w ∈ Y$ and $u + w ∈ Z$, implying $u + w ∈ Y ∩ Z$. Any single element $u ∈ Y ∩ Z$ is in $Y$ and in $Z$, so that $cu ∈ Y$ and $cu ∈ Z$ for any scalar $c$, meaning of course that $cu ∈ Y ∩ Z$. Thus the intersection $Y ∩ Z$ is a subspace of $X$. For a counterexample, let $Y = \{(x,0): x ∈ \mathbb{R}\}$ and $Z = \{(0,y): y ∈ \mathbb{R}\}$. Then $u =(1,0) ∈ Y ⊂Y ∪ Z$,  $w =(0,1)∈ Z ⊂ Y ∪ Z$ But $u + w =(1,1)\notin Y \text{ or } Z ⇒ (u + w)\notin Y ∪ Z$.",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'proof-verification']"
96,How to attack the integral $\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(x)\cos(2x)}dx$?,How to attack the integral ?,\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(x)\cos(2x)}dx,"I've considered some integrals, some of the form $$\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(ax)\cos(bx)}dx.\tag{1}$$ I would like to know how to attack this kind of integrals. Can you help me about next integral? Is not required all details or a closed-form, only how start to work with the purpose to get the indefinite integral if you think that it is feasible, or well the definite integral, or justify a good approximation of it. Question. How to attack the integral $$\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(x)\cos(2x)}dx\,?$$ Many thanks.","I've considered some integrals, some of the form $$\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(ax)\cos(bx)}dx.\tag{1}$$ I would like to know how to attack this kind of integrals. Can you help me about next integral? Is not required all details or a closed-form, only how start to work with the purpose to get the indefinite integral if you think that it is feasible, or well the definite integral, or justify a good approximation of it. Question. How to attack the integral $$\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(x)\cos(2x)}dx\,?$$ Many thanks.",,"['real-analysis', 'integration']"
97,Prove $kf(x)+f'(x)=0 $ when conditions of Rolle's theorem are satisfied .,Prove  when conditions of Rolle's theorem are satisfied .,kf(x)+f'(x)=0 ,"Prove that if $f$ is differentiable on $ [a,b]$ and if $ f(a)=f(b)=0$ then for any real $k$ there is an $ x \in (a,b) $such that $$kf(x)+f'(x)=0 $$  As all the conditions of Rolle's theorem are satisfied one can say that there is at least one  $c \in (a,b) $ such that $f'(c) =0$ How should I proceed furthur ? How can I use this to get to the required equation ?","Prove that if $f$ is differentiable on $ [a,b]$ and if $ f(a)=f(b)=0$ then for any real $k$ there is an $ x \in (a,b) $such that $$kf(x)+f'(x)=0 $$  As all the conditions of Rolle's theorem are satisfied one can say that there is at least one  $c \in (a,b) $ such that $f'(c) =0$ How should I proceed furthur ? How can I use this to get to the required equation ?",,"['calculus', 'real-analysis', 'functions', 'derivatives']"
98,Do Mobius transformations preserve Hausdorff dimension?,Do Mobius transformations preserve Hausdorff dimension?,,"Do Mobius transformations preserve Hausdorff dimension? This may be related to: Is there a measure invariant with respect to the Möbius transformation? I believe the answer is yes, but I want some intuition behind it, and eventually, a proof. Since a Mobius transformation is a combination of translations, dilations, rotations, and inversions, it suffices to show that each of these mappings preserves the Hausdorff dimension. Motivation: This image and this image are the same (up to an inversion/Mobius transformation). In both images, the ""dust"" or ""residual set"" left between the tangent circles has the same Hausdorff dimension $ \delta \approx 1.30568$.","Do Mobius transformations preserve Hausdorff dimension? This may be related to: Is there a measure invariant with respect to the Möbius transformation? I believe the answer is yes, but I want some intuition behind it, and eventually, a proof. Since a Mobius transformation is a combination of translations, dilations, rotations, and inversions, it suffices to show that each of these mappings preserves the Hausdorff dimension. Motivation: This image and this image are the same (up to an inversion/Mobius transformation). In both images, the ""dust"" or ""residual set"" left between the tangent circles has the same Hausdorff dimension $ \delta \approx 1.30568$.",,"['real-analysis', 'complex-analysis', 'mobius-transformation', 'hausdorff-measure']"
99,Does this modified notion of a limit have a name?,Does this modified notion of a limit have a name?,,"Given a function $f : X \rightarrow Y$ between metric spaces, define: $$\mathop{\mathrm{clust}}_{x \rightarrow a} f(x) = \left\{b \in Y : \forall \varepsilon>0\ \forall \delta>0\ \exists a' \in X\text{ s.t. }d_X(a',a)<\delta\text{ and }d_Y(fa',b)<\varepsilon)\right\}$$ So basically, it's like we're taking a limit, except that we can get more than one value. For instance, if we take the $\mathrm{clust}$ of a step function as $x$ approaches the argument at which the step occurs, we'll get 'both' values. This seems to be closely related to the notion of a cluster point of a filter in topology . Question. Does what I'm denoting $\mathrm{clust}$ have an accepted name or notation, and has there been any work in the direction of doing calculus with it? For example, I'd like to know about the properties of the 'cluster derivative' $$\mathop{\mathrm{clust}}_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}.$$","Given a function $f : X \rightarrow Y$ between metric spaces, define: $$\mathop{\mathrm{clust}}_{x \rightarrow a} f(x) = \left\{b \in Y : \forall \varepsilon>0\ \forall \delta>0\ \exists a' \in X\text{ s.t. }d_X(a',a)<\delta\text{ and }d_Y(fa',b)<\varepsilon)\right\}$$ So basically, it's like we're taking a limit, except that we can get more than one value. For instance, if we take the $\mathrm{clust}$ of a step function as $x$ approaches the argument at which the step occurs, we'll get 'both' values. This seems to be closely related to the notion of a cluster point of a filter in topology . Question. Does what I'm denoting $\mathrm{clust}$ have an accepted name or notation, and has there been any work in the direction of doing calculus with it? For example, I'd like to know about the properties of the 'cluster derivative' $$\mathop{\mathrm{clust}}_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}.$$",,"['real-analysis', 'general-topology', 'reference-request', 'terminology']"
