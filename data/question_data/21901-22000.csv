,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"True or False: If $A + A^2$ is invertible, then $A$ is also invertible","True or False: If  is invertible, then  is also invertible",A + A^2 A,"A is a square $n$ by $n$ matrix here. I understand the proof for $A^2$ being invertible given that $A$ is invertible, but I fail to see how to incorporate the $A + A^2$ factor into it. What I have tried so far is a rough factoring to give: $A(I_n + A)$ But that is where I am stuck. Any help is much appreciated!","A is a square $n$ by $n$ matrix here. I understand the proof for $A^2$ being invertible given that $A$ is invertible, but I fail to see how to incorporate the $A + A^2$ factor into it. What I have tried so far is a rough factoring to give: $A(I_n + A)$ But that is where I am stuck. Any help is much appreciated!",,"['linear-algebra', 'matrices']"
1,If det $A = 0$ and $\det B \neq 0$ then show that $abc = -1$,If det  and  then show that,A = 0 \det B \neq 0 abc = -1,This has been hurting my head for a while now.... If $$ \det\begin{bmatrix}a&a^2&1+a^3\\b&b^2&1+b^3\\c&c^2&1+c^3\end{bmatrix}=0 $$ And $$ \det\begin{bmatrix}a&a^2&1\\b&b^2&1\\c&c^2&1\end{bmatrix} ≠0 $$ Then show that $abc=-1$.,This has been hurting my head for a while now.... If $$ \det\begin{bmatrix}a&a^2&1+a^3\\b&b^2&1+b^3\\c&c^2&1+c^3\end{bmatrix}=0 $$ And $$ \det\begin{bmatrix}a&a^2&1\\b&b^2&1\\c&c^2&1\end{bmatrix} ≠0 $$ Then show that $abc=-1$.,,"['linear-algebra', 'matrices', 'determinant']"
2,"Determinant of matrices with entries $a_{i, j} = \operatorname{gcd}(i, j)$",Determinant of matrices with entries,"a_{i, j} = \operatorname{gcd}(i, j)","Suppose we have $n \times n$ matrix $A$ with $a_{i,j}={\rm gcd}(i,j)$. What is the determinant of $A$?","Suppose we have $n \times n$ matrix $A$ with $a_{i,j}={\rm gcd}(i,j)$. What is the determinant of $A$?",,"['linear-algebra', 'matrices', 'elementary-number-theory', 'determinant']"
3,Prove that the eigenvalues of skew-symmetric matrices are purely imaginary numbers,Prove that the eigenvalues of skew-symmetric matrices are purely imaginary numbers,,Prove that all of the eigenvalues of skew-symmetric matrix are complex numbers with the real part equal to $0$ . Has anyone got a clue how to do it?,Prove that all of the eigenvalues of skew-symmetric matrix are complex numbers with the real part equal to . Has anyone got a clue how to do it?,0,"['linear-algebra', 'matrices', 'complex-numbers', 'eigenvalues-eigenvectors', 'skew-symmetric-matrices']"
4,Automorphism on a finite dimensional vector space,Automorphism on a finite dimensional vector space,,"If $V$ is a vector space over $\mathbb Q$ with $\operatorname{dim}(V)=3$. How we can prove that there is NO automorphism $\phi: V \rightarrow V$ such that $\phi^{-1}=2\phi$. I tried: Let $\phi: V \rightarrow V$ is an automorphism such that $\phi^{-1}=2\phi$, then let $v,u\in V$, then since $\phi^{-1}$ is also a homomorphism we have: $$\phi^{-1}(uv)=\phi^{-1}(u)\phi^{-1}(v)=2\phi(u).2\phi(v)=4\phi(u)\phi(v)$$  On the other hand, $\phi^{-1}(uv)=2\phi(uv)=2\phi(u)\phi(v)$, so $$4\phi(u)\phi(v)=2\phi(u)\phi(v)$$ implies $\phi(u)\phi(v)=0$, for all $u,v \in V$, which means $\phi=0$. I think there is something missing in my argument! I didn't need the dimension of $V$ !! Any suggestion!?","If $V$ is a vector space over $\mathbb Q$ with $\operatorname{dim}(V)=3$. How we can prove that there is NO automorphism $\phi: V \rightarrow V$ such that $\phi^{-1}=2\phi$. I tried: Let $\phi: V \rightarrow V$ is an automorphism such that $\phi^{-1}=2\phi$, then let $v,u\in V$, then since $\phi^{-1}$ is also a homomorphism we have: $$\phi^{-1}(uv)=\phi^{-1}(u)\phi^{-1}(v)=2\phi(u).2\phi(v)=4\phi(u)\phi(v)$$  On the other hand, $\phi^{-1}(uv)=2\phi(uv)=2\phi(u)\phi(v)$, so $$4\phi(u)\phi(v)=2\phi(u)\phi(v)$$ implies $\phi(u)\phi(v)=0$, for all $u,v \in V$, which means $\phi=0$. I think there is something missing in my argument! I didn't need the dimension of $V$ !! Any suggestion!?",,['linear-algebra']
5,Is A diagonalizable?,Is A diagonalizable?,,"$$A=\left(\begin{array}{ccccc} 1 &1  &\cdots  &1  &1 \\  1 &0  &\cdots & 0 & 1\\  \vdots &  & \ddots &  & \vdots\\  1 & 0 & \cdots & 0 &1 \\  1 &1  &\cdots  &1  &1  \end{array}\right)\in M_{n}(\mathbb{R})$$ It has 1's around it and 0's everywhere else, and it is of size $n$. I need to decide whether this matrix is Diagonalizable,what's it's eigenvalues, eigenvectors and it's eigenspace (${v|Av=גv}$) Ok, So first I want to make one thing clear to myself and get your approval for that, If a matrix have n different eigenvalues it means that it's diagonalizable for sure, but it's not ""iff"" right? I mean, in this case I can't decide that it's not Diagonalizable since I can clearly see that it doesn't have $n$ different eigenvalues, Is it true? I notice that 1 and 0 are the only eigenvalues of this matrix since the Characteristic polynomial is $|XI-A|$ and $f_{A}(0)=|0I-A|=0$ and I would like to find it's eigenspace- Is it defined by $\dim \mathrm{Ker}(0I-A)$? and if so, It should be $n-1$, So does it actually say that it has $n-1$ independent eigenvectors for this eigenvector? Another eigenvector is 1, in a similar way I get that  $\dim \mathrm{Ker}(1I-A)$ is 1 and finally I get that it has base of $n$ independent eigenvectors and it is diagonalizable? Thank you very much guys.","$$A=\left(\begin{array}{ccccc} 1 &1  &\cdots  &1  &1 \\  1 &0  &\cdots & 0 & 1\\  \vdots &  & \ddots &  & \vdots\\  1 & 0 & \cdots & 0 &1 \\  1 &1  &\cdots  &1  &1  \end{array}\right)\in M_{n}(\mathbb{R})$$ It has 1's around it and 0's everywhere else, and it is of size $n$. I need to decide whether this matrix is Diagonalizable,what's it's eigenvalues, eigenvectors and it's eigenspace (${v|Av=גv}$) Ok, So first I want to make one thing clear to myself and get your approval for that, If a matrix have n different eigenvalues it means that it's diagonalizable for sure, but it's not ""iff"" right? I mean, in this case I can't decide that it's not Diagonalizable since I can clearly see that it doesn't have $n$ different eigenvalues, Is it true? I notice that 1 and 0 are the only eigenvalues of this matrix since the Characteristic polynomial is $|XI-A|$ and $f_{A}(0)=|0I-A|=0$ and I would like to find it's eigenspace- Is it defined by $\dim \mathrm{Ker}(0I-A)$? and if so, It should be $n-1$, So does it actually say that it has $n-1$ independent eigenvectors for this eigenvector? Another eigenvector is 1, in a similar way I get that  $\dim \mathrm{Ker}(1I-A)$ is 1 and finally I get that it has base of $n$ independent eigenvectors and it is diagonalizable? Thank you very much guys.",,[]
6,Sum of coefficients of an orthogonal matrix,Sum of coefficients of an orthogonal matrix,,"Let $(a_{ij})_{1 \le i,j \le n}$ be a real orthogonal matrix. Show that   $$\left| \sum_{1 \le i,j \le n} a_{ij}\right| \le n.$$ Naively applying the Cauchy-Schwarz inequality only gives $n^{\frac{3}{2}}$ (but only relies on the columns being of norm $1$, and not orthogonality). How do we get the stronger bound $n$?","Let $(a_{ij})_{1 \le i,j \le n}$ be a real orthogonal matrix. Show that   $$\left| \sum_{1 \le i,j \le n} a_{ij}\right| \le n.$$ Naively applying the Cauchy-Schwarz inequality only gives $n^{\frac{3}{2}}$ (but only relies on the columns being of norm $1$, and not orthogonality). How do we get the stronger bound $n$?",,"['linear-algebra', 'euclidean-geometry']"
7,Equating determinants using properties,Equating determinants using properties,,Prove without expanding $$ \begin{vmatrix} a^3 & a^2 & 1 \\  b^3 & b^2 & 1 \\ c^3 & c^2 & 1 \end{vmatrix} = (ab + bc + ca)\begin{vmatrix} a^2 & a & 1 \\ b^2 & b & 1 \\ c^2 & c & 1 \end{vmatrix} $$ Well I tried but can't figure out a way to factor out $ (ab + bc + ca) $ directly without expanding. I can easily show both equal but not directly without expanding.,Prove without expanding Well I tried but can't figure out a way to factor out directly without expanding. I can easily show both equal but not directly without expanding.," \begin{vmatrix}
a^3 & a^2 & 1 \\ 
b^3 & b^2 & 1 \\
c^3 & c^2 & 1
\end{vmatrix} = (ab + bc + ca)\begin{vmatrix}
a^2 & a & 1 \\
b^2 & b & 1 \\
c^2 & c & 1
\end{vmatrix}   (ab + bc + ca) ","['linear-algebra', 'determinant']"
8,Finding a matrix given its characteristic polynomial,Finding a matrix given its characteristic polynomial,,"I am asked to find a $2 \times 2$ matrix with real and whole entries given it's characteristic polynomial: $$p^2 -5p +1.$$ This is what I have done thus far: I equated the polynomial to zero, and the roots (eigenvalues) were found to be $2.5 \pm \sqrt{21}/2$ I named the matrix to be solved $C$ , so $\det(C) =$ product of eigenvalues $= 1$ $\text{trace}(C) =$ sum of eigenvalues $=5$ I then tried to find C by solving $T^{-1} \times D \times T$ , where $D$ is a matrix whose diagonal entries are the eigenvalues solved above, and $T$ is any matrix who's determinant is non zero. I used $T$ as a $2 \times 2$ being $$\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}.$$ I solved $T^{-1} \times D \times T$ , and threw it in a calculator to make sure I made no algebraic mistakes, but the answer I received is wrong. I appreciate any help, thank you.","I am asked to find a matrix with real and whole entries given it's characteristic polynomial: This is what I have done thus far: I equated the polynomial to zero, and the roots (eigenvalues) were found to be I named the matrix to be solved , so product of eigenvalues sum of eigenvalues I then tried to find C by solving , where is a matrix whose diagonal entries are the eigenvalues solved above, and is any matrix who's determinant is non zero. I used as a being I solved , and threw it in a calculator to make sure I made no algebraic mistakes, but the answer I received is wrong. I appreciate any help, thank you.",2 \times 2 p^2 -5p +1. 2.5 \pm \sqrt{21}/2 C \det(C) = = 1 \text{trace}(C) = =5 T^{-1} \times D \times T D T T 2 \times 2 \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}. T^{-1} \times D \times T,"['linear-algebra', 'matrices', 'polynomials']"
9,Show that this matrix is not diagonalizable,Show that this matrix is not diagonalizable,,"Say I have a matrix: $$A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} $$ Is this matrix diagonalizable? Does a 2x2 matrix always have 2 eigenvalues (multipicity counts). Why is this? I know this matrix (because it's lower triangular) has the eigenvalue of 2 with multiplicity 2... but does a matrix of this size always have 2 eigenvalues. Why is this? Is there any way to know if the eigenvalue of 2 has two eigenvectors or not quickly? Here's the way I know to find the eigenvector: $$\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} - \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} $$ $$ eigenvector = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ t \end{bmatrix} = t * \begin{bmatrix} 0 \\ 1 \end{bmatrix}$$ By this theorem, it is not diagonalizable because it only has 1 eigenvector right and the matrix has 2 rows and 2 columns:","Say I have a matrix: Is this matrix diagonalizable? Does a 2x2 matrix always have 2 eigenvalues (multipicity counts). Why is this? I know this matrix (because it's lower triangular) has the eigenvalue of 2 with multiplicity 2... but does a matrix of this size always have 2 eigenvalues. Why is this? Is there any way to know if the eigenvalue of 2 has two eigenvectors or not quickly? Here's the way I know to find the eigenvector: By this theorem, it is not diagonalizable because it only has 1 eigenvector right and the matrix has 2 rows and 2 columns:",A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix}  \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} - \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}   eigenvector = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ t \end{bmatrix} = t * \begin{bmatrix} 0 \\ 1 \end{bmatrix},"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
10,Why we have introduced linear algebra? [duplicate],Why we have introduced linear algebra? [duplicate],,"This question already has answers here : Why study linear algebra? (13 answers) Closed 5 years ago . I am new to linear algebra and am trying to find the motivation behind defining it in such a way and need for defining it.  To study $2$-D,$3$-D space we have geometry, so why do we need linear algebra then? Why did they choose the exact properties which a vector space should have? I haven't found any satisfactory answer. The text books directly starts with the theory. Can anyone explain please? Edit: ""Why we study linear algebra?"" is different from my question. That question need the applications of linear algebra, I already know that and there are many source to answer that question. What I need is background of introduction to linear algebra, not only the history, also the motivation behind choosing the properties that need to be satisfied to be a vector space - why not extra why not less? Will taking extra or less condition give something which is not very useful? Please explain it.","This question already has answers here : Why study linear algebra? (13 answers) Closed 5 years ago . I am new to linear algebra and am trying to find the motivation behind defining it in such a way and need for defining it.  To study $2$-D,$3$-D space we have geometry, so why do we need linear algebra then? Why did they choose the exact properties which a vector space should have? I haven't found any satisfactory answer. The text books directly starts with the theory. Can anyone explain please? Edit: ""Why we study linear algebra?"" is different from my question. That question need the applications of linear algebra, I already know that and there are many source to answer that question. What I need is background of introduction to linear algebra, not only the history, also the motivation behind choosing the properties that need to be satisfied to be a vector space - why not extra why not less? Will taking extra or less condition give something which is not very useful? Please explain it.",,[]
11,Solving for the determinant only given one column of values.,Solving for the determinant only given one column of values.,,"Given $$ \det     \begin{bmatrix}     a & 1 & d \\     b & 1 & e \\     c & 1 & f \\     \end{bmatrix} = 4 $$ and $$ \det     \begin{bmatrix}     a & 1 & d \\     b & 2 & e \\     c & 3 & f \\     \end{bmatrix} = -2 $$ I am asked to find $$ \det     \begin{bmatrix}     a & 8 & d \\     b & 8 & e \\     c & 8 & f \\     \end{bmatrix} $$ along with, $$ \det     \begin{bmatrix}     a & 4 & d \\     b & 5 & e \\     c & 6 & f \\     \end{bmatrix} $$ How would I go about doing this, I understand that the first one would just be 32 since when any row (or column) is multiplied by a scalar the determinant is multiplied by the same value. How do I find the determinant of the second matrix?","Given $$ \det     \begin{bmatrix}     a & 1 & d \\     b & 1 & e \\     c & 1 & f \\     \end{bmatrix} = 4 $$ and $$ \det     \begin{bmatrix}     a & 1 & d \\     b & 2 & e \\     c & 3 & f \\     \end{bmatrix} = -2 $$ I am asked to find $$ \det     \begin{bmatrix}     a & 8 & d \\     b & 8 & e \\     c & 8 & f \\     \end{bmatrix} $$ along with, $$ \det     \begin{bmatrix}     a & 4 & d \\     b & 5 & e \\     c & 6 & f \\     \end{bmatrix} $$ How would I go about doing this, I understand that the first one would just be 32 since when any row (or column) is multiplied by a scalar the determinant is multiplied by the same value. How do I find the determinant of the second matrix?",,"['linear-algebra', 'matrices', 'determinant']"
12,How can the infinity norm minimization problem be rewritten as a linear program?,How can the infinity norm minimization problem be rewritten as a linear program?,,"I have been trying to solve the infinity norm minimization problem and after quite a bit of reading I have found out that infinity norm minimization problem can be re-written as linear optimization problem. I have been trying to understand how and why it is done so, but failing miserably. Could anyone please explain me about this and also tell how the cost function looks like? My optimization problem looks like following: (I have to solve for $x$ when $A$ and $b$ are given.) $$\mbox{minimize}  \quad \|A x - b\|_{\infty}$$ which can be rewritten as follows \begin{split}\begin{array}{lccl}  \mbox{minimize}    & t             &       &\\  \mbox{subject to}  & Ax + t \mathbb 1 - b  & \geq  & 0,\\                     & A x - t \mathbb 1  - b  & \leq  & 0,  \end{array}\end{split} where $\mathbb 1$ is a vector of ones.","I have been trying to solve the infinity norm minimization problem and after quite a bit of reading I have found out that infinity norm minimization problem can be re-written as linear optimization problem. I have been trying to understand how and why it is done so, but failing miserably. Could anyone please explain me about this and also tell how the cost function looks like? My optimization problem looks like following: (I have to solve for $x$ when $A$ and $b$ are given.) $$\mbox{minimize}  \quad \|A x - b\|_{\infty}$$ which can be rewritten as follows \begin{split}\begin{array}{lccl}  \mbox{minimize}    & t             &       &\\  \mbox{subject to}  & Ax + t \mathbb 1 - b  & \geq  & 0,\\                     & A x - t \mathbb 1  - b  & \leq  & 0,  \end{array}\end{split} where $\mathbb 1$ is a vector of ones.",,"['linear-algebra', 'optimization', 'normed-spaces', 'convex-optimization', 'linear-programming']"
13,What is an elegant way to find the third eigenvalue of $A=\left[\begin{smallmatrix} 51&-12 & -21\\ 60 & -40&28\\ 57&-68&1 \end{smallmatrix}\right]$?,What is an elegant way to find the third eigenvalue of ?,A=\left[\begin{smallmatrix} 51&-12 & -21\\ 60 & -40&28\\ 57&-68&1 \end{smallmatrix}\right],"We had this in an exam today: Given the matrix $$A=\begin{bmatrix} 51&-12 & -21\\ 60 & -40&28\\ 57&-68&1 \end{bmatrix}$$ Here is the precise question as asked in our exam: Someone tells you (accurately) that $-48$ and $24$  are eigenvalues of the matrix $A$. Without using a computer, calculator or writing  anything down find the third eigenvalue of $A$. I have used the classical method (through the characteristic polynomial of A) but this leaded to a some tremendous computation and finally got the answer. But the question intentionally asked to do not make any use of such computation. How can one elegantly find the third eigenvalue of $A$?","We had this in an exam today: Given the matrix $$A=\begin{bmatrix} 51&-12 & -21\\ 60 & -40&28\\ 57&-68&1 \end{bmatrix}$$ Here is the precise question as asked in our exam: Someone tells you (accurately) that $-48$ and $24$  are eigenvalues of the matrix $A$. Without using a computer, calculator or writing  anything down find the third eigenvalue of $A$. I have used the classical method (through the characteristic polynomial of A) but this leaded to a some tremendous computation and finally got the answer. But the question intentionally asked to do not make any use of such computation. How can one elegantly find the third eigenvalue of $A$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
14,Generating Random Orthogonal Matrices,Generating Random Orthogonal Matrices,,"If I generate a random matrix A with every element sampled from a uniform [0, 1) distribution, and then use the Gram Schmidt procedure to get an orthogonal matrix Q. Will this generate every orthogonal matrix with elements within some interval on the real line? and will it do so with equal probability for each matrix? If not, how could I generate a random orthogonal matrix, preferably using python? Thank you.","If I generate a random matrix A with every element sampled from a uniform [0, 1) distribution, and then use the Gram Schmidt procedure to get an orthogonal matrix Q. Will this generate every orthogonal matrix with elements within some interval on the real line? and will it do so with equal probability for each matrix? If not, how could I generate a random orthogonal matrix, preferably using python? Thank you.",,"['linear-algebra', 'probability']"
15,Check if polynomials are linearly independent.,Check if polynomials are linearly independent.,,"I would like to check if polynomials $1, 1+t^2, 1+t+t^2$ are linearly independent. My idea is: $1 \to [1,0,0]$ $1+t^2\to [1,1,0]$ $1+t^2+t^3 \to [1,1,1]$ And now $\left( \begin{array}{ccc} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 0 & 1 & 1 \end{array} \right)$ I would like to find rank of this array. Rank of this array is $3$ so columns are linearly independent. Is it correct reasoning ?","I would like to check if polynomials $1, 1+t^2, 1+t+t^2$ are linearly independent. My idea is: $1 \to [1,0,0]$ $1+t^2\to [1,1,0]$ $1+t^2+t^3 \to [1,1,1]$ And now $\left( \begin{array}{ccc} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 0 & 1 & 1 \end{array} \right)$ I would like to find rank of this array. Rank of this array is $3$ so columns are linearly independent. Is it correct reasoning ?",,['linear-algebra']
16,Determinant of a $4 \times 4$ matrix $A$ and $(\det(A))^5$,Determinant of a  matrix  and,4 \times 4 A (\det(A))^5,"Calculate $\det(A)$ and $\det(A)^5$: $$A= \begin{bmatrix}a&a&a&a\\a&b&b&b\\a&b&c&c\\a&b&c&d\end{bmatrix}$$ I found $\det A$ with Laplace expansion: $$a(-abc+b^2c+a c^2-b c^2+a b d-b^2 d-a c d+b c d) .$$ But how can I determine $\det A^5$ easily/fast? I know that there is the possibility to use $\det(A^5)=\det(A)^5)$, but this is too long for given time to resolve the problem.","Calculate $\det(A)$ and $\det(A)^5$: $$A= \begin{bmatrix}a&a&a&a\\a&b&b&b\\a&b&c&c\\a&b&c&d\end{bmatrix}$$ I found $\det A$ with Laplace expansion: $$a(-abc+b^2c+a c^2-b c^2+a b d-b^2 d-a c d+b c d) .$$ But how can I determine $\det A^5$ easily/fast? I know that there is the possibility to use $\det(A^5)=\det(A)^5)$, but this is too long for given time to resolve the problem.",,"['linear-algebra', 'matrices', 'determinant']"
17,What is a dual space?,What is a dual space?,,I've started studying differential geometry by myself and I ran into dual spaces in a section on 1-forms. I'm not very well versed in linear algebra so any help is much appreciated.,I've started studying differential geometry by myself and I ran into dual spaces in a section on 1-forms. I'm not very well versed in linear algebra so any help is much appreciated.,,"['linear-algebra', 'soft-question', 'intuition']"
18,"For $ A \in M_{m \times n}(\mathbb{R})$, does $\ker(A)$ relate to $\ker(A^T) $?","For , does  relate to ?", A \in M_{m \times n}(\mathbb{R}) \ker(A) \ker(A^T) ,"For $ A \in M_{m \times n}(\mathbb{R})$, does $\ker(A)$ relate to $\ker(A^T) $? And if so, what would be the connection? I wouldn't imagine there being any without additional conditions on $A$ (like symmetry or something), but I'm not sure. To provide some context, I'm trying to show that if $Ax = 0_m $ has a unique solution $x \in \mathbb{R^n}$, i.e. $\ker(A) = \{0_n\}$, then the matrix $A^TA$ is invertible. I reasoned that $Ax=0_m$ having a unique solution will not necessarily yield $\ker(A^TA) = \{0_n\}$ as $\ker(A^T)$ might not be $ \{0_m\}$. However, if $\ker(A) = \{0_n\}$ implies $\ker(A^T) = \{0_m\}$ then the solution should follow. Thanks in advance, Brandon","For $ A \in M_{m \times n}(\mathbb{R})$, does $\ker(A)$ relate to $\ker(A^T) $? And if so, what would be the connection? I wouldn't imagine there being any without additional conditions on $A$ (like symmetry or something), but I'm not sure. To provide some context, I'm trying to show that if $Ax = 0_m $ has a unique solution $x \in \mathbb{R^n}$, i.e. $\ker(A) = \{0_n\}$, then the matrix $A^TA$ is invertible. I reasoned that $Ax=0_m$ having a unique solution will not necessarily yield $\ker(A^TA) = \{0_n\}$ as $\ker(A^T)$ might not be $ \{0_m\}$. However, if $\ker(A) = \{0_n\}$ implies $\ker(A^T) = \{0_m\}$ then the solution should follow. Thanks in advance, Brandon",,"['linear-algebra', 'transpose']"
19,Differentiating matrix exponential,Differentiating matrix exponential,,"I know that $$\frac{d}{dt}e^{At} = Ae^{At}$$ However, in one lecture, I found the following $$\frac{d}{dt}e^{A^Tt} = e^{A^Tt}A^T$$ The lecture is as follows How to show the second case? Why not $A^Te^{A^Tt}$ ?","I know that However, in one lecture, I found the following The lecture is as follows How to show the second case? Why not ?",\frac{d}{dt}e^{At} = Ae^{At} \frac{d}{dt}e^{A^Tt} = e^{A^Tt}A^T A^Te^{A^Tt},"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential']"
20,Changes in singular values of matrix when rows are added,Changes in singular values of matrix when rows are added,,"I know that if a column is added to a matrix then the matrix largest signular value increases and the smallest singular value decreases. That is: Given matrix $A \in R^{m \text{x} n}$, $m>n$, and $z \in R^{m}$ then $$\sigma_{max}([A |z]) >= \sigma_{max}(A),$$ and $$\sigma_{min}([A |z]) <= \sigma_{min}(A),$$ But how do I show that when a row is added, the singular values of $A$ also change as follows: ($w \in R^{n}$) $$\sigma_{n}(\left[\begin{matrix}A \\ w^{T} \end{matrix}\right])>=\sigma_{n}(A)$$ and  $$\sigma_{1}(\left[\begin{matrix}A \\ w^{T} \end{matrix}\right])<=\sqrt{||A||_2^2 + ||w||_2^2}$$","I know that if a column is added to a matrix then the matrix largest signular value increases and the smallest singular value decreases. That is: Given matrix $A \in R^{m \text{x} n}$, $m>n$, and $z \in R^{m}$ then $$\sigma_{max}([A |z]) >= \sigma_{max}(A),$$ and $$\sigma_{min}([A |z]) <= \sigma_{min}(A),$$ But how do I show that when a row is added, the singular values of $A$ also change as follows: ($w \in R^{n}$) $$\sigma_{n}(\left[\begin{matrix}A \\ w^{T} \end{matrix}\right])>=\sigma_{n}(A)$$ and  $$\sigma_{1}(\left[\begin{matrix}A \\ w^{T} \end{matrix}\right])<=\sqrt{||A||_2^2 + ||w||_2^2}$$",,"['linear-algebra', 'matrices', 'svd']"
21,Symmetric Matrices of $I_{2}$,Symmetric Matrices of,I_{2},"Find $10$ symmetric matrices $ A = \begin{pmatrix} a &b \\   c&d  \end{pmatrix}$ such that $A^{2}=I_{2}$ (I'm going to call matrix A the ""square root"" of $A^{2}$. If this is the incorrect name for it, may someone please tell me what it is actually called?) My professor posed this question in class and told us there was an infinite amount of square roots. (Assuming I understood him correctly). However I don't see how there would be many of these, as I was under the impression that a matrix only has one inverse, for $A  A^{-1}=I_{n}$. If someone could tell me if I either misunderstood the professor or if I'm thinking about something incorrectly, please correct me. My other question is other than blatant guess and check, is there a method to think of these symmetric square roots? Thanks in advance.","Find $10$ symmetric matrices $ A = \begin{pmatrix} a &b \\   c&d  \end{pmatrix}$ such that $A^{2}=I_{2}$ (I'm going to call matrix A the ""square root"" of $A^{2}$. If this is the incorrect name for it, may someone please tell me what it is actually called?) My professor posed this question in class and told us there was an infinite amount of square roots. (Assuming I understood him correctly). However I don't see how there would be many of these, as I was under the impression that a matrix only has one inverse, for $A  A^{-1}=I_{n}$. If someone could tell me if I either misunderstood the professor or if I'm thinking about something incorrectly, please correct me. My other question is other than blatant guess and check, is there a method to think of these symmetric square roots? Thanks in advance.",,"['linear-algebra', 'matrices']"
22,how to solve a system with more equations than unkowns?,how to solve a system with more equations than unkowns?,,"In general, how do you solve a system with more equations than unknowns? I know that if I select the equations to match them with the number of unknowns, there may be zero or many solutions depending on our selection. Where can I go from there? And how does a ""pseudo inverse"" come in the picture? Thank you. -Cody EDIT: I know overdetermined system usually have no solution, but in my case (triangulation matting, i.e filtering out the weatherman from the blue screen), I was told there is a solution and I need to find it using pseudo inverse.","In general, how do you solve a system with more equations than unknowns? I know that if I select the equations to match them with the number of unknowns, there may be zero or many solutions depending on our selection. Where can I go from there? And how does a ""pseudo inverse"" come in the picture? Thank you. -Cody EDIT: I know overdetermined system usually have no solution, but in my case (triangulation matting, i.e filtering out the weatherman from the blue screen), I was told there is a solution and I need to find it using pseudo inverse.",,['linear-algebra']
23,Example of a submodule of $\mathbb{Z} \oplus \mathbb{Z}$ that is not a direct summand,Example of a submodule of  that is not a direct summand,\mathbb{Z} \oplus \mathbb{Z},"We will call a submodule $A$ a direct summand of $K$ if there exists a submodule $B$ such that $A \oplus B = K$.  I think this is a question that can be formulated in terms of rank of a proper free sumbmodules but I am not sure how to ask it. Consider the $\mathbb{Z}$-module $\mathbb{Z} \oplus \mathbb{Z}$.  Is there an example of two submodules of $A,B$ of $\mathbb{Z} \oplus \mathbb{Z}$ such that $A$ and $B$ are direct summands of $\mathbb{Z} \oplus \mathbb{Z}$ but $A+B$ is not a direct summand of $\mathbb{Z} \oplus \mathbb{Z}$? I first thought that $\mathbb{Z}\oplus 0$ and $ 0 \oplus \mathbb{Z}$ was an example until I realized every module is a direct summand of itself...","We will call a submodule $A$ a direct summand of $K$ if there exists a submodule $B$ such that $A \oplus B = K$.  I think this is a question that can be formulated in terms of rank of a proper free sumbmodules but I am not sure how to ask it. Consider the $\mathbb{Z}$-module $\mathbb{Z} \oplus \mathbb{Z}$.  Is there an example of two submodules of $A,B$ of $\mathbb{Z} \oplus \mathbb{Z}$ such that $A$ and $B$ are direct summands of $\mathbb{Z} \oplus \mathbb{Z}$ but $A+B$ is not a direct summand of $\mathbb{Z} \oplus \mathbb{Z}$? I first thought that $\mathbb{Z}\oplus 0$ and $ 0 \oplus \mathbb{Z}$ was an example until I realized every module is a direct summand of itself...",,"['linear-algebra', 'abstract-algebra', 'modules']"
24,Find eigenvalues of a projection and explain what they mean,Find eigenvalues of a projection and explain what they mean,,"Suppose B represents the matrix of orthogonal (perpendicular) projection of $\mathbb{R}^{3}$ onto the plane $x_{2} = x_{1}$. Compute the eigenvalues and eigenvectors of B and explain their geometric meaning. What I have I come up so far as an attempt to deduce this question down is, for instance. If we pick an arbitrary point in space ($\mathbb{R}^{3}$), then we must project this point onto a plane (in particular $x_{2} = x_{1}$) which I imagine on a three dimensional axis of ($x,y,z$), if we choose $x_{1}$ to represent the $x$-axis, $x_{2}$ to represent the $y$-axis, and $x_{3}$ to represent the $z$-axis then we would have a plane of the equation that looks like $y=z$ or conversely ($x_{2}=x_{1}$) which is its equivalent. Once you project this point onto the plane, I see that it is true to be perpendicular and its vector is coming out of the plane. My troubles are finding the new coordinates of the new point that is projected onto the plane. Here is a skeleton sketch of what I had in math-ese. $\left[\begin{array}{c} ?\\ ?\\ ? \end{array} \right]  =  \left[\begin{array}{ccc} \Box & \Box & \Box \\ \Box & \Box & \Box \\ \Box & \Box & \Box               \end{array} \right]   \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array} \right] $, $~~$ where B is the matrix with empty boxes for elements. These question marks inside of the first matrix represent the coordinates in which I am trying to find. Once these our found, making some appropriate choices for the entries in the coefficient matrix labeled B in the question can be found, so that when B is multiplied by the last matrix $\left(\left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array} \right]~\right) $, we will get back the matrix with the ? marks in the entries. I believe this is known as doing a linear transformation. I didn't know how to include graphics, but I hope the words was enough detail to be able to duplicate what I am saying on paper in a graphical meaning. If not, please let me know how I can clarify anything up. Some help would be very appreciated. Thanks","Suppose B represents the matrix of orthogonal (perpendicular) projection of $\mathbb{R}^{3}$ onto the plane $x_{2} = x_{1}$. Compute the eigenvalues and eigenvectors of B and explain their geometric meaning. What I have I come up so far as an attempt to deduce this question down is, for instance. If we pick an arbitrary point in space ($\mathbb{R}^{3}$), then we must project this point onto a plane (in particular $x_{2} = x_{1}$) which I imagine on a three dimensional axis of ($x,y,z$), if we choose $x_{1}$ to represent the $x$-axis, $x_{2}$ to represent the $y$-axis, and $x_{3}$ to represent the $z$-axis then we would have a plane of the equation that looks like $y=z$ or conversely ($x_{2}=x_{1}$) which is its equivalent. Once you project this point onto the plane, I see that it is true to be perpendicular and its vector is coming out of the plane. My troubles are finding the new coordinates of the new point that is projected onto the plane. Here is a skeleton sketch of what I had in math-ese. $\left[\begin{array}{c} ?\\ ?\\ ? \end{array} \right]  =  \left[\begin{array}{ccc} \Box & \Box & \Box \\ \Box & \Box & \Box \\ \Box & \Box & \Box               \end{array} \right]   \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array} \right] $, $~~$ where B is the matrix with empty boxes for elements. These question marks inside of the first matrix represent the coordinates in which I am trying to find. Once these our found, making some appropriate choices for the entries in the coefficient matrix labeled B in the question can be found, so that when B is multiplied by the last matrix $\left(\left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array} \right]~\right) $, we will get back the matrix with the ? marks in the entries. I believe this is known as doing a linear transformation. I didn't know how to include graphics, but I hope the words was enough detail to be able to duplicate what I am saying on paper in a graphical meaning. If not, please let me know how I can clarify anything up. Some help would be very appreciated. Thanks",,"['linear-algebra', 'geometry']"
25,"Prove that if $v$ is an eigenvector for A, than $v$ is also and eigenvector for $adj(A)$.","Prove that if  is an eigenvector for A, than  is also and eigenvector for .",v v adj(A),"Let $A$ be a complex square matrix and $\operatorname{adj}(A)$ its adjugate (so the entries of $\operatorname{adj}A$ are minors of $A$ , up to sign). Prove that if $v$ is an eigenvector for $A$ , than $v$ is also an eigenvector for $\operatorname{adj}(A)$ .","Let be a complex square matrix and its adjugate (so the entries of are minors of , up to sign). Prove that if is an eigenvector for , than is also an eigenvector for .",A \operatorname{adj}(A) \operatorname{adj}A A v A v \operatorname{adj}(A),"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
26,Eigenvalues of a matrix whose square is zero,Eigenvalues of a matrix whose square is zero,,Let $A$ be a nonzero $3 \times 3$ matrix such that $A^2=0$ . Then what is the number of non-zero eigenvalues of the matrix? I am unable to figure out the eigenvalues of the above matrix. P.S.:  how would the answer change if it were given that $A^3=0$ ?,Let be a nonzero matrix such that . Then what is the number of non-zero eigenvalues of the matrix? I am unable to figure out the eigenvalues of the above matrix. P.S.:  how would the answer change if it were given that ?,A 3 \times 3 A^2=0 A^3=0,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'nilpotence']"
27,"For matrix $A$, if $A^4 = 0$ does this also mean that $A^2 = 0$?","For matrix , if  does this also mean that ?",A A^4 = 0 A^2 = 0,"For an arbitrary matrix $A$,  if $A^4 = 0$ does this also mean that $A^2 = 0$? My thinking is that it does since I can reduce $A^4$ into $(A^2)^2$ but I'm not sure if this helps or not.","For an arbitrary matrix $A$,  if $A^4 = 0$ does this also mean that $A^2 = 0$? My thinking is that it does since I can reduce $A^4$ into $(A^2)^2$ but I'm not sure if this helps or not.",,"['linear-algebra', 'matrices', 'nilpotence']"
28,Find $3a+b+3c+4d$ if $\left(\begin{smallmatrix}-4&-15\\2&7\end{smallmatrix}\right)^{100}=\left(\begin{smallmatrix}a&b\\c&d\end{smallmatrix}\right)$.,Find  if .,3a+b+3c+4d \left(\begin{smallmatrix}-4&-15\\2&7\end{smallmatrix}\right)^{100}=\left(\begin{smallmatrix}a&b\\c&d\end{smallmatrix}\right),"Let $\begin{pmatrix} -4 & -15 \\ 2 & 7 \end{pmatrix}^{100} = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ . Find $3a + b + 3c + 4d$ . I've gotten this answer for the matrix to the 100th power (via calculator), but it's HUGE. I need a simpler method to find it. Can I get some tips please?","Let . Find . I've gotten this answer for the matrix to the 100th power (via calculator), but it's HUGE. I need a simpler method to find it. Can I get some tips please?",\begin{pmatrix} -4 & -15 \\ 2 & 7 \end{pmatrix}^{100} = \begin{pmatrix} a & b \\ c & d \end{pmatrix} 3a + b + 3c + 4d,"['linear-algebra', 'matrices', 'algebra-precalculus']"
29,A matrix is the unit matrix iff any equation of the form $X^{k}=A$ has at least one solution.,A matrix is the unit matrix iff any equation of the form  has at least one solution.,X^{k}=A,"Let $A \in M_{n}(\mathbb{Z}) $ be a matrix with $\det A \neq 0$ and with the following property:  for any positive integer $k$ the equation $X^{k}=A$ has at least one solution in $M_{n}(\mathbb{Z})$. Prove that $A$ is the unit matrix. A.M.M. Dec. 2008, Problem 11401 , author: Marius Cavachi","Let $A \in M_{n}(\mathbb{Z}) $ be a matrix with $\det A \neq 0$ and with the following property:  for any positive integer $k$ the equation $X^{k}=A$ has at least one solution in $M_{n}(\mathbb{Z})$. Prove that $A$ is the unit matrix. A.M.M. Dec. 2008, Problem 11401 , author: Marius Cavachi",,"['linear-algebra', 'group-theory']"
30,Is the set of periodic functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^\mathbb{R}$?,Is the set of periodic functions from  to  a subspace of ?,\mathbb{R} \mathbb{R} \mathbb{R}^\mathbb{R},"A function $f: \mathbb{R} \to \mathbb{R}$ is called periodic if there exists a positive number $p$ such that $f(x) = f(x + p)$ for all $x \in \mathbb{R}$. Is the set of periodic functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^\mathbb{R}$? Explain So I have seen a solution to this question and my question has more to do with what thought process was used to even think of the sort of function to show that the set of periodic functions is not a subspace?  First I do have a question of what $\mathbb{R}^{\mathbb{R}}$ would look like? I'm visualizing elements being of some sort of infinite list of the sort $(x_1, x_2, x_3,..........), x_i \in \mathbb{R}$. But to the main question.  So the function chosen was $$h(x) = sin\sqrt{2}x + cos(x)$$ where $f(x) = sin\sqrt{2}x$ and $g(x) = cos(x)$ Using these functions, the author arrived at a contradiction with regards to $\sqrt{2}$ being shown to be rational (which it is not). Working this out after being given that function was fine, but what was the motivation to use that function? Where did the idea to show something is irrational would help to disprove a set being a subspace? It almost feels like it arose from osmosis and brilliance.....","A function $f: \mathbb{R} \to \mathbb{R}$ is called periodic if there exists a positive number $p$ such that $f(x) = f(x + p)$ for all $x \in \mathbb{R}$. Is the set of periodic functions from $\mathbb{R}$ to $\mathbb{R}$ a subspace of $\mathbb{R}^\mathbb{R}$? Explain So I have seen a solution to this question and my question has more to do with what thought process was used to even think of the sort of function to show that the set of periodic functions is not a subspace?  First I do have a question of what $\mathbb{R}^{\mathbb{R}}$ would look like? I'm visualizing elements being of some sort of infinite list of the sort $(x_1, x_2, x_3,..........), x_i \in \mathbb{R}$. But to the main question.  So the function chosen was $$h(x) = sin\sqrt{2}x + cos(x)$$ where $f(x) = sin\sqrt{2}x$ and $g(x) = cos(x)$ Using these functions, the author arrived at a contradiction with regards to $\sqrt{2}$ being shown to be rational (which it is not). Working this out after being given that function was fine, but what was the motivation to use that function? Where did the idea to show something is irrational would help to disprove a set being a subspace? It almost feels like it arose from osmosis and brilliance.....",,"['linear-algebra', 'abstract-algebra', 'proof-explanation']"
31,Prove that $\dim(U+W) + \dim(U\cap W) = \dim U + \dim W$,Prove that,\dim(U+W) + \dim(U\cap W) = \dim U + \dim W,"Let $V$ be a vector space over a field $k$ and let $U$ , $W$ be finite-dimensional subspaces of $V$ . Prove that $\dim(U+W) + \dim(U\cap W) = \dim U + \dim W$ I'm given that to begin this problem I can find the bases: $\{v_1,\dots,v_p\}$ for $U\cap W$ $\{v_1,\dots,v_p, u_1,\dots,u_q\}$ for $U$ and $\{v_1,\dots,v_p, w_1,\dots,w_r\}$ for $W$ and then I just need to show that $\{v_1,\dots,v_p, u_1,\dots,u_q, w_1,\dots,w_r\}$ is a basis for $U+W$ . My question is: how does one go about showing that it is a basis for $U+W$ and then use that to prove the above question? Side note: This question has already been asked here: Given two subspaces $U,W$ of vector space $V$, how to show that $\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$ However, the first answer given does not apply to solving it the way I want to with finding the bases. The second answer simply gives me what I already knew to start with. Thus, I am asking this question again since I'm asking how to solve it a particular way instead of just any general hints towards solving it.","Let be a vector space over a field and let , be finite-dimensional subspaces of . Prove that I'm given that to begin this problem I can find the bases: for for and for and then I just need to show that is a basis for . My question is: how does one go about showing that it is a basis for and then use that to prove the above question? Side note: This question has already been asked here: Given two subspaces $U,W$ of vector space $V$, how to show that $\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$ However, the first answer given does not apply to solving it the way I want to with finding the bases. The second answer simply gives me what I already knew to start with. Thus, I am asking this question again since I'm asking how to solve it a particular way instead of just any general hints towards solving it.","V k U W V \dim(U+W) + \dim(U\cap W) = \dim U + \dim W \{v_1,\dots,v_p\} U\cap W \{v_1,\dots,v_p, u_1,\dots,u_q\} U \{v_1,\dots,v_p, w_1,\dots,w_r\} W \{v_1,\dots,v_p, u_1,\dots,u_q, w_1,\dots,w_r\} U+W U+W","['linear-algebra', 'vector-spaces', 'proof-writing']"
32,Some basic questions regarding rank-$1$ matrices,Some basic questions regarding rank- matrices,1,"If an $n \times n$ matrix $B$ has rank $1$ , and $A$ is another $n \times n$ matrix, then why does $A B$ also have rank $1$ ?  This showed up in a solution that I read through, but it doesn't seem like an obvious fact. And one more thing that came up in this solution:  it says that since this matrix has rank $1$ , then it must have $(n-1)$ eigenvalues that are all zero, and only one non-zero eigenvalue.  I don't see how this has to be true either. Any ideas are welcome.","If an matrix has rank , and is another matrix, then why does also have rank ?  This showed up in a solution that I read through, but it doesn't seem like an obvious fact. And one more thing that came up in this solution:  it says that since this matrix has rank , then it must have eigenvalues that are all zero, and only one non-zero eigenvalue.  I don't see how this has to be true either. Any ideas are welcome.",n \times n B 1 A n \times n A B 1 1 (n-1),"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-rank', 'rank-1-matrices']"
33,How to prove $\det(I+uv^\intercal)=1+v^\intercal u$,How to prove,\det(I+uv^\intercal)=1+v^\intercal u,"Let be $u,v\in\mathbb{R}^n$ , then $\det(I+uv^\intercal)=1+v^\intercal u $ where $I$ denotes the identity matrix of order $n$ . How to prove this? what I did: let be $A=\{n\in\mathbb{N}: \forall u,v \in \mathbb{R^n}, \det(I+uv^\intercal)\neq1+v^\intercal u  \} $ , and suppose $A\neq \varnothing $ , then for the well-orderer-principle there exists $n_0\in A$ such $n_0\leq n,\;\forall n\in A$ . since $1\notin A,\; n_0\neq 1 \rightarrow n_0-1\in\mathbb{N}\setminus A  $ so: $$\forall u,v\in\mathbb{R^{n_0-1}}: \det(I+uv^\intercal)=1+v^\intercal u $$ Then, let be $u,v\in\mathbb{R}^{n_0}$ , so $$\det(I+uv^\intercal)= \displaystyle\sum_{j=1}^{n_0}(-)^{1+j}a_{1j}\det(A_{1j})$$ where $a_{ij}=\begin{cases} u_iv_j+1 & i=j \\ u_iv_j & i\neq j \end{cases}$ , and $A_{1j}$ is the  submatrix of $I+uv^\intercal$ that results deleting the i-file and j-column. Then I try to open the term for $j=1$ and try to use the relation for the new matrix of $n_0-1$ order with the determinant because it holds the same form but I have other terms which difficult me the work. Do you know other method?. Ps. here , in page 2 I've found something similar but I don't understand what it means PS. I'm taking a course of numerical analisys","Let be , then where denotes the identity matrix of order . How to prove this? what I did: let be , and suppose , then for the well-orderer-principle there exists such . since so: Then, let be , so where , and is the  submatrix of that results deleting the i-file and j-column. Then I try to open the term for and try to use the relation for the new matrix of order with the determinant because it holds the same form but I have other terms which difficult me the work. Do you know other method?. Ps. here , in page 2 I've found something similar but I don't understand what it means PS. I'm taking a course of numerical analisys","u,v\in\mathbb{R}^n \det(I+uv^\intercal)=1+v^\intercal u  I n A=\{n\in\mathbb{N}: \forall u,v \in \mathbb{R^n}, \det(I+uv^\intercal)\neq1+v^\intercal u  \}  A\neq \varnothing  n_0\in A n_0\leq n,\;\forall n\in A 1\notin A,\; n_0\neq 1 \rightarrow n_0-1\in\mathbb{N}\setminus A   \forall u,v\in\mathbb{R^{n_0-1}}: \det(I+uv^\intercal)=1+v^\intercal u  u,v\in\mathbb{R}^{n_0} \det(I+uv^\intercal)= \displaystyle\sum_{j=1}^{n_0}(-)^{1+j}a_{1j}\det(A_{1j}) a_{ij}=\begin{cases} u_iv_j+1 & i=j \\ u_iv_j & i\neq j \end{cases} A_{1j} I+uv^\intercal j=1 n_0-1","['linear-algebra', 'numerical-methods', 'determinant']"
34,Simultaneously diagonalization of two matrices.,Simultaneously diagonalization of two matrices.,,Let $A$ be a real symmetric matrix and $B$ a real positive-definite matrix. Is it possible to simultaneously diagonalize of $A$ and $B$? Thank you very much.,Let $A$ be a real symmetric matrix and $B$ a real positive-definite matrix. Is it possible to simultaneously diagonalize of $A$ and $B$? Thank you very much.,,"['linear-algebra', 'matrices', 'matrix-equations', 'diagonalization']"
35,Linear Algebra solution when determinant is zero,Linear Algebra solution when determinant is zero,,"I am doing practice questions in my book and I came upon this True/False question: If $\det(A) = 0$, then the linear system $Ax=b$, $b\neq 0$, has no solution. The book is saying that the answer is false. But why is that? I thought the answer is true because of something like this $  Ax  = b$  $$\left(\begin{array}{ccc|c}1&0&0&1\\0&0&0&2\\0&0&1&3\end{array}\right)$$ When a matrix has its rref taken, the resulting matrix, when the determinant is zero, would always have a zero in its diagonal, right? This would result in a matrix with no solution because row 2 is impossible. Am I misunderstanding the question somehow? I am also confused by this question because I am not sure how augmented matrices work with square matrices because you can only find the determinant of a square matrix. Can someone please explain why the answer is false?","I am doing practice questions in my book and I came upon this True/False question: If $\det(A) = 0$, then the linear system $Ax=b$, $b\neq 0$, has no solution. The book is saying that the answer is false. But why is that? I thought the answer is true because of something like this $  Ax  = b$  $$\left(\begin{array}{ccc|c}1&0&0&1\\0&0&0&2\\0&0&1&3\end{array}\right)$$ When a matrix has its rref taken, the resulting matrix, when the determinant is zero, would always have a zero in its diagonal, right? This would result in a matrix with no solution because row 2 is impossible. Am I misunderstanding the question somehow? I am also confused by this question because I am not sure how augmented matrices work with square matrices because you can only find the determinant of a square matrix. Can someone please explain why the answer is false?",,['linear-algebra']
36,matrix inverse in tensor notation,matrix inverse in tensor notation,,"Suppose there is a matrix $A$ that transforms vectors, $$    Y = A x $$ Now express this in some other coordinate system, with $x = B z, \,\, y = B w$, so \begin{align*} & Bw = A B z \\ \Rightarrow & w = B^{-1} A B z \end{align*} So $A$ expressed in the other system is $B^{-1} A B$. What would be the equivalent in tensor notation, in particular of the $B^{-1}$? Here's what I'm trying  \begin{align*} & y^i = A^i_j x_j \\ & \quad\quad x^j = B^j_k z^k \\ & \quad\quad y^i = B^i_m w^m \\ \text{so}\quad & B^i_m w^m = A^i_j B^j_k z_k \end{align*} Now what is the tensor equivalent of premultiplying by $B^{-1}$ on the left, in order to find what $A$ looks like in tensor notation in the new coordinate system?","Suppose there is a matrix $A$ that transforms vectors, $$    Y = A x $$ Now express this in some other coordinate system, with $x = B z, \,\, y = B w$, so \begin{align*} & Bw = A B z \\ \Rightarrow & w = B^{-1} A B z \end{align*} So $A$ expressed in the other system is $B^{-1} A B$. What would be the equivalent in tensor notation, in particular of the $B^{-1}$? Here's what I'm trying  \begin{align*} & y^i = A^i_j x_j \\ & \quad\quad x^j = B^j_k z^k \\ & \quad\quad y^i = B^i_m w^m \\ \text{so}\quad & B^i_m w^m = A^i_j B^j_k z_k \end{align*} Now what is the tensor equivalent of premultiplying by $B^{-1}$ on the left, in order to find what $A$ looks like in tensor notation in the new coordinate system?",,"['linear-algebra', 'notation', 'tensors']"
37,"A rational ""fifth root"" of the scalar matrix $2I$","A rational ""fifth root"" of the scalar matrix",2I,"I am working on the following problem from a past exam. Find a necessary and sufficient condition for there to exist a square matrix $A$ of order $n$ whose entries are all rational, such that $A^5 = 2I_n$, where $I_n$ stands for the identity matrix of order $n$. By an argument using determinants, one easily sees that $n\in 5\mathbb Z$ is a necessary condition.  Moreover, if we assume such an $A =: A_5$ exists for $n=5$, then for general $n = 5k$, $A = \bigoplus^kA_5$ is what we want. Thus the problem boils down to whether there exists a $5\times5$ rational matrix $A$ such that $A^5 = 2I_5$. What I would like to ask is whether such an $A$ exists, and how one can find such $A$ by hand most efficiently.  I would be grateful for your help.","I am working on the following problem from a past exam. Find a necessary and sufficient condition for there to exist a square matrix $A$ of order $n$ whose entries are all rational, such that $A^5 = 2I_n$, where $I_n$ stands for the identity matrix of order $n$. By an argument using determinants, one easily sees that $n\in 5\mathbb Z$ is a necessary condition.  Moreover, if we assume such an $A =: A_5$ exists for $n=5$, then for general $n = 5k$, $A = \bigoplus^kA_5$ is what we want. Thus the problem boils down to whether there exists a $5\times5$ rational matrix $A$ such that $A^5 = 2I_5$. What I would like to ask is whether such an $A$ exists, and how one can find such $A$ by hand most efficiently.  I would be grateful for your help.",,"['linear-algebra', 'matrices', 'block-matrices']"
38,How did my professor do this?,How did my professor do this?,,"I'm trying to figure out how  my professor got to the step circled in red in the image below: How did he get the values of the first row to become completely positive, and how did he derive the values of the entire 2nd row?","I'm trying to figure out how  my professor got to the step circled in red in the image below: How did he get the values of the first row to become completely positive, and how did he derive the values of the entire 2nd row?",,"['linear-algebra', 'matrices']"
39,Disjoint Union of Subsets and Direct Sum of Subspaces (Clarify Explanation),Disjoint Union of Subsets and Direct Sum of Subspaces (Clarify Explanation),,"Can someone clarify for me the side note found in Axler's Linear Algebra Done Right, that is:  Direct sums of subspaces are analogous to disjoint union of subsets. I am not sure what exactly is a disjoint union of subsets (having checked online definitions) and thus its relation to direct sums. Thanks.","Can someone clarify for me the side note found in Axler's Linear Algebra Done Right, that is:  Direct sums of subspaces are analogous to disjoint union of subsets. I am not sure what exactly is a disjoint union of subsets (having checked online definitions) and thus its relation to direct sums. Thanks.",,['linear-algebra']
40,Why not use the identity matrix instead of the Kronecker delta?,Why not use the identity matrix instead of the Kronecker delta?,,"The Kronecker delta is defined as :  $$\delta_{mn} = \begin{cases} 1 & \text{if }m=n,\\ 0 & \text{if }m\neq n. \end{cases}$$ This is equal to the matrix $E_n$ which is a matrix with the diagonal filled with ones. Why not use $E_n$ instead of Kronecker delta? Does anybody see why? Please do tell me.","The Kronecker delta is defined as :  $$\delta_{mn} = \begin{cases} 1 & \text{if }m=n,\\ 0 & \text{if }m\neq n. \end{cases}$$ This is equal to the matrix $E_n$ which is a matrix with the diagonal filled with ones. Why not use $E_n$ instead of Kronecker delta? Does anybody see why? Please do tell me.",,['linear-algebra']
41,Is a countable direct sum of free modules free?,Is a countable direct sum of free modules free?,,I had a very basic question about free modules that I wanted to use as an ingredient in a proof but I am not sure if it is true in general. Let $R$ be a commutative ring with multiplicative identity.  Suppose $ \{ F_i \}_{i \in \mathbb{N} }$ are a countable family of free $R$-modules. Is the countable direct sum $F_1 \oplus F_2 \oplus F_3 \oplus F_4 \oplus \ldots$ free? If so is there an easy way to prove this?,I had a very basic question about free modules that I wanted to use as an ingredient in a proof but I am not sure if it is true in general. Let $R$ be a commutative ring with multiplicative identity.  Suppose $ \{ F_i \}_{i \in \mathbb{N} }$ are a countable family of free $R$-modules. Is the countable direct sum $F_1 \oplus F_2 \oplus F_3 \oplus F_4 \oplus \ldots$ free? If so is there an easy way to prove this?,,"['linear-algebra', 'abstract-algebra', 'modules']"
42,Conditions for vectors to span a vector space,Conditions for vectors to span a vector space,,I have a basic doubt. Can we say that a set of vectors span the entire vector space iff they are linearly independent ? Do they need to satisfy any other property ?,I have a basic doubt. Can we say that a set of vectors span the entire vector space iff they are linearly independent ? Do they need to satisfy any other property ?,,"['linear-algebra', 'vector-spaces']"
43,Repeated applications of a (rotation) matrix keep you in the same subspace?,Repeated applications of a (rotation) matrix keep you in the same subspace?,,"Say I have a unit vector, $v$ in $\Bbb{R}^n$ ( $|v|=1$ ). I multiply a rotation matrix, $R$ with this vector to get a new vector, $u=Rv$ . Then, I get a third vector by applying the rotation matrix yet another time: $w=R^2v$ . Is it true that $w$ lies in the space spanned by $v$ and $u$ ? Can this be proven or a counter-example shown? And if this is true for rotation matrices, then is it true for other kinds of matrices as well? My attempt: We must have $w=c_1v+c_2 w$ for some $c_1$ and $c_2$ . In other words, $$R^2v = c_1v+c_2 Rv$$ I'm stuck at this stage, unfortunately.","Say I have a unit vector, in ( ). I multiply a rotation matrix, with this vector to get a new vector, . Then, I get a third vector by applying the rotation matrix yet another time: . Is it true that lies in the space spanned by and ? Can this be proven or a counter-example shown? And if this is true for rotation matrices, then is it true for other kinds of matrices as well? My attempt: We must have for some and . In other words, I'm stuck at this stage, unfortunately.",v \Bbb{R}^n |v|=1 R u=Rv w=R^2v w v u w=c_1v+c_2 w c_1 c_2 R^2v = c_1v+c_2 Rv,"['linear-algebra', 'rotations']"
44,Orthogonal vectors in complex vector space,Orthogonal vectors in complex vector space,,"Consider $u,v\in \mathbb{R}^2$ where $u=(2,1), v=(-1, 2)$ . $u$ and $v$ are orthogonal since $u\cdot v=0$ . If we put them in $\mathbb{C}$ , they should be still orthogonal. However, $ \langle u,v \rangle=(2+i)(-1+2i)\ne 0$ . I must misunderstand something here. Could anyone explain this? Thanks in advance. EDIT: $v=(-1, 2)$ should be converted to $-1 + 2i$ not $-1-2i$ .","Consider where . and are orthogonal since . If we put them in , they should be still orthogonal. However, . I must misunderstand something here. Could anyone explain this? Thanks in advance. EDIT: should be converted to not .","u,v\in \mathbb{R}^2 u=(2,1), v=(-1, 2) u v u\cdot v=0 \mathbb{C}  \langle u,v \rangle=(2+i)(-1+2i)\ne 0 v=(-1, 2) -1 + 2i -1-2i",['linear-algebra']
45,Proving $\det A = 1$,Proving,\det A = 1,"Given a real invertible $2 \times 2$ matrix $A$ with $A + A^{-1} = I$ , I need to prove that $\det A = 1$ . I know how to prove that $\det A = \frac{1}{\det A^{-1}}$ , but don't have access to the fact that the determinant of a sum is the sum of the determinants (but only multiplicativity). Is there another way to prove this?","Given a real invertible matrix with , I need to prove that . I know how to prove that , but don't have access to the fact that the determinant of a sum is the sum of the determinants (but only multiplicativity). Is there another way to prove this?",2 \times 2 A A + A^{-1} = I \det A = 1 \det A = \frac{1}{\det A^{-1}},"['linear-algebra', 'matrices', 'proof-explanation', 'determinant']"
46,Relationship between a determinant and the roots of a cubic equation,Relationship between a determinant and the roots of a cubic equation,,"The following is a question in a 1947 university entrance exam. Let $a$ , $b$ and $c$ be the roots of cubic equation: $$ x^3+px+q=0$$ and denote $$ S_n=a^n+b^n+c^n$$ Question (1): express the below determinant in terms of $p$ and $q$ : $$ \Delta = \begin{vmatrix}  S_0 & S_1 & S_2 \\ S_1 & S_2 & S_3 \\ S_2 & S_3 & S_4 \end{vmatrix} $$ Question (2): show that When $\Delta>0$ , $a$ , $b$ and $c$ are distinct real roots When $\Delta<0$ , one root is real, and the other two are complex conjugated When $\Delta=0$ , all roots are real, and at least two of them are equal For me, I can just plug in the cubic formula (such as using Mathematica) to get the answer to question (1). However such computer tools are not available in 1947. Also considering that it is in an exam environment, one is not expected to spend too much time on it. So my question is, whether there is an ingenious way to solve it?","The following is a question in a 1947 university entrance exam. Let , and be the roots of cubic equation: and denote Question (1): express the below determinant in terms of and : Question (2): show that When , , and are distinct real roots When , one root is real, and the other two are complex conjugated When , all roots are real, and at least two of them are equal For me, I can just plug in the cubic formula (such as using Mathematica) to get the answer to question (1). However such computer tools are not available in 1947. Also considering that it is in an exam environment, one is not expected to spend too much time on it. So my question is, whether there is an ingenious way to solve it?","a b c  x^3+px+q=0  S_n=a^n+b^n+c^n p q  \Delta = \begin{vmatrix} 
S_0 & S_1 & S_2 \\
S_1 & S_2 & S_3 \\
S_2 & S_3 & S_4
\end{vmatrix}  \Delta>0 a b c \Delta<0 \Delta=0","['linear-algebra', 'determinant', 'cubics']"
47,Trace of a Matrix: when to use? what is trace trick?,Trace of a Matrix: when to use? what is trace trick?,,"On calculating log-likelihood function for some multivariate distributions, such as multivariate Normal, I see some examples where the matrices are suddenly changed to trace, even when the matrix is not diagonal. I searched online to find a plausible explanation for this ""trace trick"" without success. What is it all about? Can someone clarify the usage of trace in this situation? Bellow a slide with an example where you can find this usage of trace.","On calculating log-likelihood function for some multivariate distributions, such as multivariate Normal, I see some examples where the matrices are suddenly changed to trace, even when the matrix is not diagonal. I searched online to find a plausible explanation for this ""trace trick"" without success. What is it all about? Can someone clarify the usage of trace in this situation? Bellow a slide with an example where you can find this usage of trace.",,"['linear-algebra', 'matrices', 'statistics', 'matrix-calculus', 'maximum-likelihood']"
48,Trace of product of two Hermitian matrices,Trace of product of two Hermitian matrices,,"Let $A$ and $B$ be two Hermitian complex matrices. (a) Prove that   $\operatorname{tr}(AB)$ is real. (b) Prove that if $A, B$ are   positive, then $\operatorname{tr}(AB)>0$. (a) The trace of Hermitian matrix is a real number, since $a_{ii} = \bar{a}_{ii}$, that also means that all eigenvalues are real. I can't proceed to conclusion that $\operatorname{tr}(AB)$ is real, since $\operatorname{tr}(AB) \neq \operatorname{tr}A\cdot\operatorname{tr}B$ and product of two Hermitian matrices is also Hermitian only if these matrices commute, which is not the case for arbitrary Hermitian matrices. (b) Am I missing something or the question is indeed so easy? If all entries $a_{ij}>0$ and $b_{ij}>0$, then all $c_{ij}=\sum_{m}a_{im}\cdot b_{mj}>0$ too, finally $\operatorname{tr}(AB)=\sum_{i}c_{ii}>0$.","Let $A$ and $B$ be two Hermitian complex matrices. (a) Prove that   $\operatorname{tr}(AB)$ is real. (b) Prove that if $A, B$ are   positive, then $\operatorname{tr}(AB)>0$. (a) The trace of Hermitian matrix is a real number, since $a_{ii} = \bar{a}_{ii}$, that also means that all eigenvalues are real. I can't proceed to conclusion that $\operatorname{tr}(AB)$ is real, since $\operatorname{tr}(AB) \neq \operatorname{tr}A\cdot\operatorname{tr}B$ and product of two Hermitian matrices is also Hermitian only if these matrices commute, which is not the case for arbitrary Hermitian matrices. (b) Am I missing something or the question is indeed so easy? If all entries $a_{ij}>0$ and $b_{ij}>0$, then all $c_{ij}=\sum_{m}a_{im}\cdot b_{mj}>0$ too, finally $\operatorname{tr}(AB)=\sum_{i}c_{ii}>0$.",,"['linear-algebra', 'matrices', 'trace']"
49,"If a matrix $A$, not necessarily symmetric, has real, nonnegative eigenvalues, is it positive semidefinite?","If a matrix , not necessarily symmetric, has real, nonnegative eigenvalues, is it positive semidefinite?",A,"We know that a symmetric matrix $A$ is positive semidefinite i.e. $x^TAx \geq 0$ if and only if all its eigenvalues are nonnegative. Now suppose I have a matrix (not necessarily symmetric) $A$, whereby all its eigenvalues are nonnegative (obviously real), is it positive semidefinite? My hunch is yes. Because such matrix $A$ would satisfy $\lambda_{\min}(A)\|x\|^2 \leq x^TAx$. Therefore it has to be positive semidefinite. But I have searched up and down through every linear algebra book that I have came across, virtually all of them states definition with respect to symmetric positive semidefinite matrix only.","We know that a symmetric matrix $A$ is positive semidefinite i.e. $x^TAx \geq 0$ if and only if all its eigenvalues are nonnegative. Now suppose I have a matrix (not necessarily symmetric) $A$, whereby all its eigenvalues are nonnegative (obviously real), is it positive semidefinite? My hunch is yes. Because such matrix $A$ would satisfy $\lambda_{\min}(A)\|x\|^2 \leq x^TAx$. Therefore it has to be positive semidefinite. But I have searched up and down through every linear algebra book that I have came across, virtually all of them states definition with respect to symmetric positive semidefinite matrix only.",,"['linear-algebra', 'matrices', 'reference-request', 'eigenvalues-eigenvectors', 'positive-semidefinite']"
50,Is it possible to get an example of two matrices,Is it possible to get an example of two matrices,,"Is it possible to get an example of two matrices $A,B\in M_4(\mathbb{R})$ both having $rank<2$ but $\det(A-\lambda B)\ne 0$ i.e it is not identically a zero polynomial. where $\lambda$ is indeterminate, I mean a variable. I want to say $(A-\lambda B)$ is of full rank matrix, assuming entries of $A-\lambda B$ is a polynomial matrix with linear polynomial.","Is it possible to get an example of two matrices $A,B\in M_4(\mathbb{R})$ both having $rank<2$ but $\det(A-\lambda B)\ne 0$ i.e it is not identically a zero polynomial. where $\lambda$ is indeterminate, I mean a variable. I want to say $(A-\lambda B)$ is of full rank matrix, assuming entries of $A-\lambda B$ is a polynomial matrix with linear polynomial.",,"['linear-algebra', 'matrices']"
51,Show that the linear combination is unique?,Show that the linear combination is unique?,,"I have the following question: Let S be a subset of the vector space $\Bbb R^3$ defined by $$ S =  \left\{  ~ \begin{bmatrix}2\\-1\\ 0\end{bmatrix}, \begin{bmatrix}1\\3\\ -2\end{bmatrix},  \begin{bmatrix}1\\1\\ 4\end{bmatrix}  ~\right\} $$ show that  $v= \begin{bmatrix}-4\\4\\ -6\end{bmatrix}$ is in $\text{span}(S)$ by constructing it as a linear combination of the vectors in $S$. Show that this linear combination is unique. This is what I got after solving part 1:  $$ c_1\begin{bmatrix}2\\-1\\ 0\end{bmatrix}  + c_2\begin{bmatrix}1\\3\\ -2\end{bmatrix}  + c_3\begin{bmatrix}1\\1\\ 4\end{bmatrix} = \begin{bmatrix}-4\\4\\ -6\end{bmatrix}$$ after solving this I got  $c_1= -2$, $c_2= 1$, $c_3= -1$. How do I show that the linear combination is unique?","I have the following question: Let S be a subset of the vector space $\Bbb R^3$ defined by $$ S =  \left\{  ~ \begin{bmatrix}2\\-1\\ 0\end{bmatrix}, \begin{bmatrix}1\\3\\ -2\end{bmatrix},  \begin{bmatrix}1\\1\\ 4\end{bmatrix}  ~\right\} $$ show that  $v= \begin{bmatrix}-4\\4\\ -6\end{bmatrix}$ is in $\text{span}(S)$ by constructing it as a linear combination of the vectors in $S$. Show that this linear combination is unique. This is what I got after solving part 1:  $$ c_1\begin{bmatrix}2\\-1\\ 0\end{bmatrix}  + c_2\begin{bmatrix}1\\3\\ -2\end{bmatrix}  + c_3\begin{bmatrix}1\\1\\ 4\end{bmatrix} = \begin{bmatrix}-4\\4\\ -6\end{bmatrix}$$ after solving this I got  $c_1= -2$, $c_2= 1$, $c_3= -1$. How do I show that the linear combination is unique?",,"['linear-algebra', 'vector-spaces']"
52,Hoffman-Wielandt Theorem Proof,Hoffman-Wielandt Theorem Proof,,"Exercise 3.3 of Izenman's Modern Multivariate Statistical Techniques : let $\mathbf{A}$, $\mathbf{B}$ be symmetric $J \times J$ matrices, with eigenvalues $\{\lambda_j(\mathbf{A})\}$ and $\{\lambda_j(\mathbf{B})\}$ respectively, arranged in descending order with respect to $j$ (so $\lambda_1$ is largest, $\lambda_J$ is the smallest for both matrices). Prove that $$\sum_{j=1}^{J}\left[\lambda_j(\mathbf{A}) - \lambda_j(\mathbf{B})\right]^2 \leq \text{tr}\{(\mathbf{A}-\mathbf{B})(\mathbf{A}-\mathbf{B})^{T}\}\text{.}$$ The hint says to use spectral decomposition, so $$\begin{align*} \mathbf{A} &= \sum_{j=1}^{J}\lambda_j(\mathbf{A})\mathbf{v}_j(\mathbf{A})\mathbf{v}^T_j(\mathbf{A}) \\ \mathbf{B} &= \sum_{j=1}^{J}\lambda_j(\mathbf{B})\mathbf{v}_j(\mathbf{B})\mathbf{v}^T_j(\mathbf{B}) \end{align*}$$ where the $\mathbf{v}_j(\cdot)$ denote the eigenvectors of the matrix $\cdot$ corresponding to $\lambda_j$. Then it says to express $$\text{tr}\{(\mathbf{A}-\mathbf{B})(\mathbf{A}-\mathbf{B})^{T}\}$$ in terms of the decomposition. I have $$\mathbf{A}-\mathbf{B} = \sum_{j=1}^{J}[\lambda_j(\mathbf{A})\mathbf{v}_j(\mathbf{A})\mathbf{v}^T_j(\mathbf{A})-\lambda_j(\mathbf{B})\mathbf{v}_j(\mathbf{B})\mathbf{v}^T_j(\mathbf{B})] $$ and $$(\mathbf{A}-\mathbf{B})^{T} = \mathbf{A}^{T}-\mathbf{B}^{T} = \sum_{j=1}^{J}[\lambda_j(\mathbf{A})\mathbf{v}^T_j(\mathbf{A})\mathbf{v}_j(\mathbf{A})-\lambda_j(\mathbf{B})\mathbf{v}^T_j(\mathbf{B})\mathbf{v}_j(\mathbf{B})]\tag{1}\text{.}$$ I suppose we could assume the vectors are normalized, so we get $\mathbf{v}^T_j(\mathbf{A})\mathbf{v}_j(\mathbf{A}) = \mathbf{v}^T_j(\mathbf{B})\mathbf{v}_j(\mathbf{B}) = 1$. But I'm not sure what else to do. Direct multiplication looks like a very messy approach (which would possibly involve induction on $J$), but I thought I'd ask here for suggestions.","Exercise 3.3 of Izenman's Modern Multivariate Statistical Techniques : let $\mathbf{A}$, $\mathbf{B}$ be symmetric $J \times J$ matrices, with eigenvalues $\{\lambda_j(\mathbf{A})\}$ and $\{\lambda_j(\mathbf{B})\}$ respectively, arranged in descending order with respect to $j$ (so $\lambda_1$ is largest, $\lambda_J$ is the smallest for both matrices). Prove that $$\sum_{j=1}^{J}\left[\lambda_j(\mathbf{A}) - \lambda_j(\mathbf{B})\right]^2 \leq \text{tr}\{(\mathbf{A}-\mathbf{B})(\mathbf{A}-\mathbf{B})^{T}\}\text{.}$$ The hint says to use spectral decomposition, so $$\begin{align*} \mathbf{A} &= \sum_{j=1}^{J}\lambda_j(\mathbf{A})\mathbf{v}_j(\mathbf{A})\mathbf{v}^T_j(\mathbf{A}) \\ \mathbf{B} &= \sum_{j=1}^{J}\lambda_j(\mathbf{B})\mathbf{v}_j(\mathbf{B})\mathbf{v}^T_j(\mathbf{B}) \end{align*}$$ where the $\mathbf{v}_j(\cdot)$ denote the eigenvectors of the matrix $\cdot$ corresponding to $\lambda_j$. Then it says to express $$\text{tr}\{(\mathbf{A}-\mathbf{B})(\mathbf{A}-\mathbf{B})^{T}\}$$ in terms of the decomposition. I have $$\mathbf{A}-\mathbf{B} = \sum_{j=1}^{J}[\lambda_j(\mathbf{A})\mathbf{v}_j(\mathbf{A})\mathbf{v}^T_j(\mathbf{A})-\lambda_j(\mathbf{B})\mathbf{v}_j(\mathbf{B})\mathbf{v}^T_j(\mathbf{B})] $$ and $$(\mathbf{A}-\mathbf{B})^{T} = \mathbf{A}^{T}-\mathbf{B}^{T} = \sum_{j=1}^{J}[\lambda_j(\mathbf{A})\mathbf{v}^T_j(\mathbf{A})\mathbf{v}_j(\mathbf{A})-\lambda_j(\mathbf{B})\mathbf{v}^T_j(\mathbf{B})\mathbf{v}_j(\mathbf{B})]\tag{1}\text{.}$$ I suppose we could assume the vectors are normalized, so we get $\mathbf{v}^T_j(\mathbf{A})\mathbf{v}_j(\mathbf{A}) = \mathbf{v}^T_j(\mathbf{B})\mathbf{v}_j(\mathbf{B}) = 1$. But I'm not sure what else to do. Direct multiplication looks like a very messy approach (which would possibly involve induction on $J$), but I thought I'd ask here for suggestions.",,"['linear-algebra', 'matrices']"
53,Trace of a nilpotent matrix is zero. [duplicate],Trace of a nilpotent matrix is zero. [duplicate],,"This question already has answers here : Direct proof that nilpotent matrix has zero trace (2 answers) Closed 5 years ago . I'm trying to prove that the trace of a nilpotent matrix is $0$. Here's what I have: Let $A \in M_n(k)$, where $k$ is a field. If $A$ is nilpotent then, in fact, $A^n=0$, (this result was proven in the previous problem.) By Proposition 20 on page 428 (Dummit & Foote) the characteristic polynomial of $A$ divides some power of the minimal polynomial of $A$, and so is of the form $x^m$ for some $m \ge t$. Thus, the eigenvalues of $A$ are all equal to $0$ and the only solution to the equation $xv-Av=0$ is the trivial solution. Since it is assumed that $v \neq 0$, then it must be that $A=0$. This is a much stronger conclusion than the conclusion I'm aiming for and so I am suspicious. I probably made an error. Please help.","This question already has answers here : Direct proof that nilpotent matrix has zero trace (2 answers) Closed 5 years ago . I'm trying to prove that the trace of a nilpotent matrix is $0$. Here's what I have: Let $A \in M_n(k)$, where $k$ is a field. If $A$ is nilpotent then, in fact, $A^n=0$, (this result was proven in the previous problem.) By Proposition 20 on page 428 (Dummit & Foote) the characteristic polynomial of $A$ divides some power of the minimal polynomial of $A$, and so is of the form $x^m$ for some $m \ge t$. Thus, the eigenvalues of $A$ are all equal to $0$ and the only solution to the equation $xv-Av=0$ is the trivial solution. Since it is assumed that $v \neq 0$, then it must be that $A=0$. This is a much stronger conclusion than the conclusion I'm aiming for and so I am suspicious. I probably made an error. Please help.",,"['linear-algebra', 'matrices']"
54,How can an engineer make use of commutative diagram?,How can an engineer make use of commutative diagram?,,"I am in engineering and I have being awed by commutative diagram my entire life. But I do not see the purpose in knowing commutative diagram as arises in linear algebra? Is there something in engineering that can be describe by commutative diagram? For example, operators such as laplace transform and fourier transform to map stuff to different spaces? Or expectation operator... Thanks","I am in engineering and I have being awed by commutative diagram my entire life. But I do not see the purpose in knowing commutative diagram as arises in linear algebra? Is there something in engineering that can be describe by commutative diagram? For example, operators such as laplace transform and fourier transform to map stuff to different spaces? Or expectation operator... Thanks",,['linear-algebra']
55,Why Gaussian Elimination only works over field?,Why Gaussian Elimination only works over field?,,"When I was solving system of linear congruences ( n variables, n equations ), like this: $AX \equiv b \pmod p$ I was told that ordinary Gaussian Elimination works if $p$ is prime . And I figured out that when $p$ is prime, integers $\pmod p$ form a field , otherwise it doesn't form a field, but a ring. Here comes my problem, why Gaussian Elimination does not work over a ring? AFAIK, the difference between a ring and a field, is that elements in a field are multiplicatively invertible , but does it have something to do with the applicability of Gaussian Elimination? If yes, how?","When I was solving system of linear congruences ( n variables, n equations ), like this: $AX \equiv b \pmod p$ I was told that ordinary Gaussian Elimination works if $p$ is prime . And I figured out that when $p$ is prime, integers $\pmod p$ form a field , otherwise it doesn't form a field, but a ring. Here comes my problem, why Gaussian Elimination does not work over a ring? AFAIK, the difference between a ring and a field, is that elements in a field are multiplicatively invertible , but does it have something to do with the applicability of Gaussian Elimination? If yes, how?",,"['linear-algebra', 'number-theory']"
56,How to find the eigenvalues and Jordan canonical form of this matrix,How to find the eigenvalues and Jordan canonical form of this matrix,,"Question: let $a_{i,j}\in R,A=(a_{i,j})_{n\times n} $,and  $a_{i,j}=\begin{cases} 1&i+j\in\{n,n+1\}\\ 0&i+j\notin\{n,n+1\} \end{cases}$ that's meaning: $$A=\begin{bmatrix} 0&0&0&\cdots&0&1&1\\ 0&0&0&\cdots&1&1&0\\ 0&0&\cdots&1&1&0&0\\ \vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\ 0&1&1&\cdots&0&0&0\\ 1&1&0&\cdots&0&0&0\\ 1&0&0&\cdots&0&0&0 \end{bmatrix}_{n\times n}$$ Problem (1): Find the Jordan canonical form of $A$. I know this matrix Jordan is  $$diag(\lambda_{1},\lambda_{2},\cdots,\lambda_{n})$$ where $\lambda_{i}$ is eigenvalue But this problem key find the eigenvalue is hard, Thank you.maybe this problem is not easy,But I hope see someone can solve it.Thank you very much!","Question: let $a_{i,j}\in R,A=(a_{i,j})_{n\times n} $,and  $a_{i,j}=\begin{cases} 1&i+j\in\{n,n+1\}\\ 0&i+j\notin\{n,n+1\} \end{cases}$ that's meaning: $$A=\begin{bmatrix} 0&0&0&\cdots&0&1&1\\ 0&0&0&\cdots&1&1&0\\ 0&0&\cdots&1&1&0&0\\ \vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\ 0&1&1&\cdots&0&0&0\\ 1&1&0&\cdots&0&0&0\\ 1&0&0&\cdots&0&0&0 \end{bmatrix}_{n\times n}$$ Problem (1): Find the Jordan canonical form of $A$. I know this matrix Jordan is  $$diag(\lambda_{1},\lambda_{2},\cdots,\lambda_{n})$$ where $\lambda_{i}$ is eigenvalue But this problem key find the eigenvalue is hard, Thank you.maybe this problem is not easy,But I hope see someone can solve it.Thank you very much!",,"['linear-algebra', 'matrices', 'trigonometry']"
57,how to find inverse of a matrix in $\Bbb Z_5$,how to find inverse of a matrix in,\Bbb Z_5,"how to find inverse of a matrix in $\Bbb Z_5$ please help me explicitly how to find the inverse of matrix below, what I was thinking that to find inverses separately of the each term in $\Bbb Z_5$ and then form the matrix? $$\begin{pmatrix}1&2&0\\0&2&4\\0&0&3\end{pmatrix}$$ Thank you.","how to find inverse of a matrix in $\Bbb Z_5$ please help me explicitly how to find the inverse of matrix below, what I was thinking that to find inverses separately of the each term in $\Bbb Z_5$ and then form the matrix? $$\begin{pmatrix}1&2&0\\0&2&4\\0&0&3\end{pmatrix}$$ Thank you.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'inverse']"
58,"Why is the name general ""linear"" group?","Why is the name general ""linear"" group?",,"Well, I just want to know if is there any significance of the term ""linear"" in the of  name ""General Linear Group"" - for example, $\text{GL}_ n(\mathbb{R})$?","Well, I just want to know if is there any significance of the term ""linear"" in the of  name ""General Linear Group"" - for example, $\text{GL}_ n(\mathbb{R})$?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'terminology', 'lie-groups']"
59,Derivative of Determinant Map,Derivative of Determinant Map,,"For $ V= ( V_1, V_2) $  and $ W= ( W_1, W_2) $, given a determinant map $ \det : \mathbb{R}^2\times \mathbb{R}^2\rightarrow \mathbb{R}$ defined as $ \det (V,W)=  V_1W_2-V_2W_1$. Then have to find the derivative of the determinant map at $( V, W)\in R^2$ evaluated at $(H,K)\in \mathbb{R}^2$ . Please help me with this terminology. It seems to me if $ U= V_1W_2-V_2W_1$,  then derivative of $U  =  V_1W_2-V_2W_1$. (by using the Jacobian technique) Then, in that case derivative of U at $(H, K) = \det (H, K)$.","For $ V= ( V_1, V_2) $  and $ W= ( W_1, W_2) $, given a determinant map $ \det : \mathbb{R}^2\times \mathbb{R}^2\rightarrow \mathbb{R}$ defined as $ \det (V,W)=  V_1W_2-V_2W_1$. Then have to find the derivative of the determinant map at $( V, W)\in R^2$ evaluated at $(H,K)\in \mathbb{R}^2$ . Please help me with this terminology. It seems to me if $ U= V_1W_2-V_2W_1$,  then derivative of $U  =  V_1W_2-V_2W_1$. (by using the Jacobian technique) Then, in that case derivative of U at $(H, K) = \det (H, K)$.",,['linear-algebra']
60,How can I construct the matrix representing a linear transformation of a 2x2 matrix to its transpose with respect to a given set of bases?,How can I construct the matrix representing a linear transformation of a 2x2 matrix to its transpose with respect to a given set of bases?,,"I have been given that I am working with the space of all 2x2 matrices. The basis  $B$ for this space is given as a set of four 2x2 matrices, each with an entry of 1 in a unique position and zeroes everywhere else (sorry about the description in words - I don't know how to format matrices for this site). I have also been given the basis $B' = ({1, x, x^2})$ for the space of all polynomials of degree 2 or less and the basis $B'' = ({1})$ for $R$. Then I am given a series of linear transformations and asked to find the matrices associated with them with respect to the bases above. I am completely lost as to how to do this! I would like help with how to achieve one of them so that I can then go and apply what I learn here to the other transformations. The example I've chosen is the transformation T that maps 2x2 matrices to their transposes. I can't seem to construct a matrix that will bring the element in position '21' up to position '12'. Can anyone give me some direction with this? Many thanks!!","I have been given that I am working with the space of all 2x2 matrices. The basis  $B$ for this space is given as a set of four 2x2 matrices, each with an entry of 1 in a unique position and zeroes everywhere else (sorry about the description in words - I don't know how to format matrices for this site). I have also been given the basis $B' = ({1, x, x^2})$ for the space of all polynomials of degree 2 or less and the basis $B'' = ({1})$ for $R$. Then I am given a series of linear transformations and asked to find the matrices associated with them with respect to the bases above. I am completely lost as to how to do this! I would like help with how to achieve one of them so that I can then go and apply what I learn here to the other transformations. The example I've chosen is the transformation T that maps 2x2 matrices to their transposes. I can't seem to construct a matrix that will bring the element in position '21' up to position '12'. Can anyone give me some direction with this? Many thanks!!",,"['linear-algebra', 'transformation']"
61,Is the matrix exponential map injective?,Is the matrix exponential map injective?,,Is the matrix exponential $exp:M_n(\mathbb C) \to GL_n(\mathbb C)$ injective? Can it be that $e^A=I$ where $A$ is not the zero matrix?,Is the matrix exponential $exp:M_n(\mathbb C) \to GL_n(\mathbb C)$ injective? Can it be that $e^A=I$ where $A$ is not the zero matrix?,,"['linear-algebra', 'matrices']"
62,Using Wolfram Alpha For Solving A System Of Equations,Using Wolfram Alpha For Solving A System Of Equations,,"How do i input  the below system of equations in wolfram alpha in order to solve for the unknowns and plot them? If i just say ""solve"" and input these equations one after the other with a simicolen {solve $2x - y +0z = 0$;$-x + 2y -z = -1$;$0x - 3y + 4z = 4$} it simply throws the value of $x$,$y$ and $z$ without showing any steps nor the plot. I'am Wondering if there's some kind of code that can be written in order to make wolfram alpha understand what i'am talking about. $$\left.\begin{matrix}  2x - y +0z = 0\\  -x + 2y -z = -1\\  0x - 3y + 4z = 4  \end{matrix}\right\}$$","How do i input  the below system of equations in wolfram alpha in order to solve for the unknowns and plot them? If i just say ""solve"" and input these equations one after the other with a simicolen {solve $2x - y +0z = 0$;$-x + 2y -z = -1$;$0x - 3y + 4z = 4$} it simply throws the value of $x$,$y$ and $z$ without showing any steps nor the plot. I'am Wondering if there's some kind of code that can be written in order to make wolfram alpha understand what i'am talking about. $$\left.\begin{matrix}  2x - y +0z = 0\\  -x + 2y -z = -1\\  0x - 3y + 4z = 4  \end{matrix}\right\}$$",,"['linear-algebra', 'computer-algebra-systems', 'wolfram-alpha']"
63,How is the matrix identity $\det\begin{pmatrix}A&B\\B&A\end{pmatrix}=\det(A+B)\det(A-B)$ proved?,How is the matrix identity  proved?,\det\begin{pmatrix}A&B\\B&A\end{pmatrix}=\det(A+B)\det(A-B),"The Wikipedia page about the determinant mentions the following matrix identity $$\det\begin{pmatrix}A&B\\B&A\end{pmatrix}=\det(A+B)\det(A-B),$$ valid for squared matrices $A$ and $B$ of the same size. How is this result proved?",The Wikipedia page about the determinant mentions the following matrix identity valid for squared matrices and of the same size. How is this result proved?,"\det\begin{pmatrix}A&B\\B&A\end{pmatrix}=\det(A+B)\det(A-B), A B","['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
64,Why is the 2-norm better than the 1-norm?,Why is the 2-norm better than the 1-norm?,,Why we use the 2-norm all over the place instead of the 1-norm although both are equivalent and the 1-norm is more effieciently computable? I am currently implementing an algorithm that normalizes a vector in each step (using 2-norm). So i could also just use the 1-norm instead? More details: power iteration for the calculation of the dominat eigenvalue of a matrix: x:=random vector of length numrows(A) x=x/||x||  while l changes (e.g. abs(l-l_last)<tolerance)     x=Av     l=||x||     x=x/||x||,Why we use the 2-norm all over the place instead of the 1-norm although both are equivalent and the 1-norm is more effieciently computable? I am currently implementing an algorithm that normalizes a vector in each step (using 2-norm). So i could also just use the 1-norm instead? More details: power iteration for the calculation of the dominat eigenvalue of a matrix: x:=random vector of length numrows(A) x=x/||x||  while l changes (e.g. abs(l-l_last)<tolerance)     x=Av     l=||x||     x=x/||x||,,"['linear-algebra', 'computer-science']"
65,Cumulative sum over random numbers gives interesting results,Cumulative sum over random numbers gives interesting results,,"Recently, I encountered an interesting thing with numbers. I couldn't find a satisfactory explanation hence, I decided to post it here. So, basically say we generate 5 random numbers with mean = 0 and standard deviation = 1 and take a cumulative sum over it and take the index which gives us the max value. To explain this with an example : 5 random numbers with mean = 0 and sd = 1 1.8871962   -2.0479581   -0.7508212   -1.2745548    -0.7129499 Take cumulative sum over it which gives us 1.8871962   -0.1607619    -0.9115832     -2.1861380   -2.8990878 and now we take the index of the highest value which is : 1 in this case. Now I repeat this procedure for million observation and check what is the ratio of each index to occur and it gives me  this ratio : 1 -   0.273311 2 -   0.156218  3 -   0.140228  4 -   0.156293  5 -   0.273950 I ran this simulation multiple times and this ratio stays more or less same. I find this weird. I expected the ratio to be more or less the same for all the indices. If you also observe the 1st and 5th index have same ratio, 2nd and 4th have same ratio. Instead of taking 5 random variables, I changed it to 10 and it changed the ratio numbers but it still holds the symmetry where 1st and 10th index have the highest value and are same, 2nd and 9th are second highest and same and so on. Do you know what is going on there? I don't know what this phenomenon is called so couldn't find appropriate words to google it as well. This is a reproducible R script with results, if that is of any help. generate_random_numbers <- function() {    num = numeric()    for (i in seq(1000000)) {      vec = rnorm(5)      num[i] = which.max(cumsum(vec))  }  table(num)/length(num) }  generate_random_numbers() #num #       1        2        3        4        5  #0.272608 0.156398 0.140701 0.156537 0.273756   generate_random_numbers() #num #       1        2        3        4        5  #0.273287 0.156035 0.140446 0.156400 0.273832","Recently, I encountered an interesting thing with numbers. I couldn't find a satisfactory explanation hence, I decided to post it here. So, basically say we generate 5 random numbers with mean = 0 and standard deviation = 1 and take a cumulative sum over it and take the index which gives us the max value. To explain this with an example : 5 random numbers with mean = 0 and sd = 1 1.8871962   -2.0479581   -0.7508212   -1.2745548    -0.7129499 Take cumulative sum over it which gives us 1.8871962   -0.1607619    -0.9115832     -2.1861380   -2.8990878 and now we take the index of the highest value which is : 1 in this case. Now I repeat this procedure for million observation and check what is the ratio of each index to occur and it gives me  this ratio : 1 -   0.273311 2 -   0.156218  3 -   0.140228  4 -   0.156293  5 -   0.273950 I ran this simulation multiple times and this ratio stays more or less same. I find this weird. I expected the ratio to be more or less the same for all the indices. If you also observe the 1st and 5th index have same ratio, 2nd and 4th have same ratio. Instead of taking 5 random variables, I changed it to 10 and it changed the ratio numbers but it still holds the symmetry where 1st and 10th index have the highest value and are same, 2nd and 9th are second highest and same and so on. Do you know what is going on there? I don't know what this phenomenon is called so couldn't find appropriate words to google it as well. This is a reproducible R script with results, if that is of any help. generate_random_numbers <- function() {    num = numeric()    for (i in seq(1000000)) {      vec = rnorm(5)      num[i] = which.max(cumsum(vec))  }  table(num)/length(num) }  generate_random_numbers() #num #       1        2        3        4        5  #0.272608 0.156398 0.140701 0.156537 0.273756   generate_random_numbers() #num #       1        2        3        4        5  #0.273287 0.156035 0.140446 0.156400 0.273832",,"['linear-algebra', 'normal-distribution']"
66,If $A$ is $2\times2 $ matrix such that $\operatorname{tr} A =\det A=3$ then trace of $A^{-1}=$,If  is  matrix such that  then trace of,A 2\times2  \operatorname{tr} A =\det A=3 A^{-1}=,If $A$ is $2\times2 $ matrix such that $\operatorname{tr} A=\det A=3$ then trace of $A^{-1}$ is? $(A) \quad 1   \qquad  (B) \quad \dfrac{1}{3} \qquad  (C) \quad \dfrac{1}{6} \qquad  (D) \quad\dfrac{1}{2}$ I did it in this way: $$\lambda_1+\lambda_2 = 3 $$ $$\lambda_1\cdot\lambda_2=3$$ $$\frac{1}{\lambda_1}+\frac{1}{\lambda_2} \implies \frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2} \implies \frac{3}{3}=1$$ I am practicing these types of question and after $5$ minutes of digging I came up with this answer. I wanna know if there is any alternative approach to solve this problem.,If $A$ is $2\times2 $ matrix such that $\operatorname{tr} A=\det A=3$ then trace of $A^{-1}$ is? $(A) \quad 1   \qquad  (B) \quad \dfrac{1}{3} \qquad  (C) \quad \dfrac{1}{6} \qquad  (D) \quad\dfrac{1}{2}$ I did it in this way: $$\lambda_1+\lambda_2 = 3 $$ $$\lambda_1\cdot\lambda_2=3$$ $$\frac{1}{\lambda_1}+\frac{1}{\lambda_2} \implies \frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2} \implies \frac{3}{3}=1$$ I am practicing these types of question and after $5$ minutes of digging I came up with this answer. I wanna know if there is any alternative approach to solve this problem.,,['linear-algebra']
67,Justify that the vectors are linearly dependent,Justify that the vectors are linearly dependent,,"Let $V$ be a real vector space and $a,b,c,d,e\in V$. We have the vectors \begin{align*}&v_1=a+b+c \\ &v_2=2a+2b+2c-d \\ &v_3=a-b-e \\ &v_4=5a+6b-c+d+e \\ &v_5=a-c+3e \\ &v_6=a+b+d+e\end{align*} I want to justify that these vectors are linearly dependent. $$$$ These six vectors are linearly dependetn iff we can get the zer0-vector by a linear combination of these vectors, \begin{equation*}\lambda_1v_1+\lambda_2v_2+\lambda_3v_3+\lambda_4v_4+\lambda_5v_5+\lambda_6v_6=0\end{equation*} where at least of the coefficients $\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_4$ or $\lambda_6$ is not equal to zero. We have the following: \begin{align*}&\lambda_1v_1+\lambda_2v_2+\lambda_3v_3+\lambda_4v_4+\lambda_5v_5+\lambda_6v_6=0  \\ & \Rightarrow \lambda_1\left (a+b+c\right )+\lambda_2\left (2a+2b+2c-d\right )+\lambda_3\left (a-b-e\right )+\lambda_4\left (5a+6b-c+d+e\right )+\lambda_5\left (a-c+3e\right )+\lambda_6\left (a+b+d+e\right )=0 \\ & \Rightarrow \lambda_1a+\lambda_1b+\lambda_1c+2\lambda_2 a+2\lambda_2 b+2\lambda_2 c-\lambda_2d+\lambda_3 a-\lambda_3 b-\lambda_3 e+5\lambda_4 a+6\lambda_4 b-\lambda_4 c+\lambda_4 d+\lambda_4 e+\lambda_5 a-\lambda_5 c+3\lambda_5 e+\lambda_6a+\lambda_6b+\lambda_6d+\lambda_6e=0 \\ & \Rightarrow a\left (\lambda_1+2\lambda_2+\lambda_3+5\lambda_4+\lambda_5+\lambda_6\right )+b\left (\lambda_1+2\lambda_2-\lambda_3+6\lambda_4+\lambda_6\right )+c\left (\lambda_1+2\lambda_2 -\lambda_4-\lambda_5\right )+d\left (-\lambda_2+\lambda_4+\lambda_6\right ) +e\left (-\lambda_3   +\lambda_4  +3\lambda_5 +\lambda_6\right )=0\end{align*} Is everything correct? How could we continue? Do we use the fact that $a,b,c,d,e\in V$ ? But how?","Let $V$ be a real vector space and $a,b,c,d,e\in V$. We have the vectors \begin{align*}&v_1=a+b+c \\ &v_2=2a+2b+2c-d \\ &v_3=a-b-e \\ &v_4=5a+6b-c+d+e \\ &v_5=a-c+3e \\ &v_6=a+b+d+e\end{align*} I want to justify that these vectors are linearly dependent. $$$$ These six vectors are linearly dependetn iff we can get the zer0-vector by a linear combination of these vectors, \begin{equation*}\lambda_1v_1+\lambda_2v_2+\lambda_3v_3+\lambda_4v_4+\lambda_5v_5+\lambda_6v_6=0\end{equation*} where at least of the coefficients $\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_4$ or $\lambda_6$ is not equal to zero. We have the following: \begin{align*}&\lambda_1v_1+\lambda_2v_2+\lambda_3v_3+\lambda_4v_4+\lambda_5v_5+\lambda_6v_6=0  \\ & \Rightarrow \lambda_1\left (a+b+c\right )+\lambda_2\left (2a+2b+2c-d\right )+\lambda_3\left (a-b-e\right )+\lambda_4\left (5a+6b-c+d+e\right )+\lambda_5\left (a-c+3e\right )+\lambda_6\left (a+b+d+e\right )=0 \\ & \Rightarrow \lambda_1a+\lambda_1b+\lambda_1c+2\lambda_2 a+2\lambda_2 b+2\lambda_2 c-\lambda_2d+\lambda_3 a-\lambda_3 b-\lambda_3 e+5\lambda_4 a+6\lambda_4 b-\lambda_4 c+\lambda_4 d+\lambda_4 e+\lambda_5 a-\lambda_5 c+3\lambda_5 e+\lambda_6a+\lambda_6b+\lambda_6d+\lambda_6e=0 \\ & \Rightarrow a\left (\lambda_1+2\lambda_2+\lambda_3+5\lambda_4+\lambda_5+\lambda_6\right )+b\left (\lambda_1+2\lambda_2-\lambda_3+6\lambda_4+\lambda_6\right )+c\left (\lambda_1+2\lambda_2 -\lambda_4-\lambda_5\right )+d\left (-\lambda_2+\lambda_4+\lambda_6\right ) +e\left (-\lambda_3   +\lambda_4  +3\lambda_5 +\lambda_6\right )=0\end{align*} Is everything correct? How could we continue? Do we use the fact that $a,b,c,d,e\in V$ ? But how?",,"['linear-algebra', 'vector-spaces']"
68,Multiply $1\times 3$ matrix by corresponding numbers,Multiply  matrix by corresponding numbers,1\times 3,"For example, I want to this to happen: $$\begin{bmatrix}1& 2& 3\end{bmatrix}\times\begin{bmatrix}2& 3& 4\end{bmatrix} = \begin{bmatrix}2& 6& 12\end{bmatrix}$$ It's not exactly matrix multiplication, but I hope you can see what I'm getting at. Is there some notation in linear algebra that allows this function to be valid?","For example, I want to this to happen: $$\begin{bmatrix}1& 2& 3\end{bmatrix}\times\begin{bmatrix}2& 3& 4\end{bmatrix} = \begin{bmatrix}2& 6& 12\end{bmatrix}$$ It's not exactly matrix multiplication, but I hope you can see what I'm getting at. Is there some notation in linear algebra that allows this function to be valid?",,['linear-algebra']
69,"Solve system of simultaneous equations in $3$ variables: $x+y+xy=19$, $y+z+yz=11$, $z+x+zx=14$","Solve system of simultaneous equations in  variables: , ,",3 x+y+xy=19 y+z+yz=11 z+x+zx=14,"Solve the following equation system: $$x+y+xy=19$$ $$y+z+yz=11$$ $$z+x+zx=14$$ I've tried substituting, adding, subtracting, multiplying... Nothing works. Could anyone drop me a few hints without actually solving it? Thanks!","Solve the following equation system: $$x+y+xy=19$$ $$y+z+yz=11$$ $$z+x+zx=14$$ I've tried substituting, adding, subtracting, multiplying... Nothing works. Could anyone drop me a few hints without actually solving it? Thanks!",,"['linear-algebra', 'algebra-precalculus', 'systems-of-equations']"
70,Can $AB-BA=I$ hold if both $A$ and $B$ are operators on an infinitely-dimensional vector space over $\mathbb C$?,Can  hold if both  and  are operators on an infinitely-dimensional vector space over ?,AB-BA=I A B \mathbb C,"Of course, it can't hold if operators are over finite-dimensional spaces, as is evident from trace considerations. Can it be true for infinite-dimensional spaces? I think not, but I don't see how we can argue in this case.","Of course, it can't hold if operators are over finite-dimensional spaces, as is evident from trace considerations. Can it be true for infinite-dimensional spaces? I think not, but I don't see how we can argue in this case.",,"['linear-algebra', 'operator-algebras']"
71,Does $\mathbb{R}^n$ have a real vector space structure with dimension other than $n$?,Does  have a real vector space structure with dimension other than ?,\mathbb{R}^n n,Can we define a vector space structure on $\mathbb {R}^n$ other than usual scalar multiplication and usual addition such that the dimension of $\mathbb {R}^n$ over $\mathbb {R}$ is not $n$ but some $m$ not equal to $n$?,Can we define a vector space structure on $\mathbb {R}^n$ other than usual scalar multiplication and usual addition such that the dimension of $\mathbb {R}^n$ over $\mathbb {R}$ is not $n$ but some $m$ not equal to $n$?,,"['linear-algebra', 'vector-spaces']"
72,Rule for squaring arbitrary powers?,Rule for squaring arbitrary powers?,,"This is a really simple question, but I don't know how to phrase it well enough for Google. I'm going through a proof and don't understand how: $$ (q^{2^{n+1}})^2 = q^{2^{n+2}} $$ I thought it would be $q^{4^{n+1}}$instead, or are they equivalent? If so, why?","This is a really simple question, but I don't know how to phrase it well enough for Google. I'm going through a proof and don't understand how: $$ (q^{2^{n+1}})^2 = q^{2^{n+2}} $$ I thought it would be $q^{4^{n+1}}$instead, or are they equivalent? If so, why?",,['linear-algebra']
73,$AB=BA$ with same eigenvector matrix,with same eigenvector matrix,AB=BA,"I read in G. Strang's Linear Algebra and its Applications that, if $A$ and $B$ are diagonalisable matrices of the form  such that $AB=BA$, then their eigenvector matrices $S_1$ and $S_2$ (such that $A=S_1\Lambda_1S_1^{-1}$ and $B=S_2\Lambda_2 S_2^{-1}$) can be chosen to be equal: $S_1=S_2$. How can it be proved? I have found a proof here , but it is not clear to me how to see that $C$ is diagonalisable as $DCD^{-1}=Q$. Matrix $C$ obviously is the matrix with the coordinates of $B\mathbf{x}$ with respect to the basis $\{\mathbf{x}_1,...,\mathbf{x}_k\}$ of the eigenspaceof $V_\lambda (A)$, but I do not see how we can know that it is diagonalisable. Thank you very much for any explanation of the linked proof or other proof!!! EDIT : Corrected statement of the lemma I am interested in. See comments below by the users whom I thank for what they have noticed.","I read in G. Strang's Linear Algebra and its Applications that, if $A$ and $B$ are diagonalisable matrices of the form  such that $AB=BA$, then their eigenvector matrices $S_1$ and $S_2$ (such that $A=S_1\Lambda_1S_1^{-1}$ and $B=S_2\Lambda_2 S_2^{-1}$) can be chosen to be equal: $S_1=S_2$. How can it be proved? I have found a proof here , but it is not clear to me how to see that $C$ is diagonalisable as $DCD^{-1}=Q$. Matrix $C$ obviously is the matrix with the coordinates of $B\mathbf{x}$ with respect to the basis $\{\mathbf{x}_1,...,\mathbf{x}_k\}$ of the eigenspaceof $V_\lambda (A)$, but I do not see how we can know that it is diagonalisable. Thank you very much for any explanation of the linked proof or other proof!!! EDIT : Corrected statement of the lemma I am interested in. See comments below by the users whom I thank for what they have noticed.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
74,How to find orthogonal projection of vector on a subspace?,How to find orthogonal projection of vector on a subspace?,,"Well, I have this subspace: $V = \operatorname{span}\left\{ \begin{pmatrix}\frac{1}{3} \\\frac{2}{3} \\\frac{2}{3}\end{pmatrix},\begin{pmatrix}1 \\3 \\4\end{pmatrix}\right\}$ and the vector $v = \begin{pmatrix}9 \\0 \\0\end{pmatrix}$ How can I find the orthogonal projection of $v$ on $V$? This is what I did so far: \begin{align}&P_v(v)=\langle v,v_1\rangle v_1+\langle v,v_2\rangle v_2 =\\=& \left\langle\begin{pmatrix}9 \\0 \\0\end{pmatrix},\begin{pmatrix}\frac{1}{3} \\\frac{2}{3} \\\frac{2}{3}\end{pmatrix}\right\rangle\begin{pmatrix}\frac{1}{3} \\\frac{2}{3} \\\frac{2}{3}\end{pmatrix}+\left<\begin{pmatrix}9 \\0 \\0\end{pmatrix},\begin{pmatrix}1 \\3 \\4\end{pmatrix}\right>\begin{pmatrix}1 \\3 \\4\end{pmatrix} = \begin{pmatrix}10 \\29 \\38\end{pmatrix}\end{align} Is this the right method to compute this?","Well, I have this subspace: $V = \operatorname{span}\left\{ \begin{pmatrix}\frac{1}{3} \\\frac{2}{3} \\\frac{2}{3}\end{pmatrix},\begin{pmatrix}1 \\3 \\4\end{pmatrix}\right\}$ and the vector $v = \begin{pmatrix}9 \\0 \\0\end{pmatrix}$ How can I find the orthogonal projection of $v$ on $V$? This is what I did so far: \begin{align}&P_v(v)=\langle v,v_1\rangle v_1+\langle v,v_2\rangle v_2 =\\=& \left\langle\begin{pmatrix}9 \\0 \\0\end{pmatrix},\begin{pmatrix}\frac{1}{3} \\\frac{2}{3} \\\frac{2}{3}\end{pmatrix}\right\rangle\begin{pmatrix}\frac{1}{3} \\\frac{2}{3} \\\frac{2}{3}\end{pmatrix}+\left<\begin{pmatrix}9 \\0 \\0\end{pmatrix},\begin{pmatrix}1 \\3 \\4\end{pmatrix}\right>\begin{pmatrix}1 \\3 \\4\end{pmatrix} = \begin{pmatrix}10 \\29 \\38\end{pmatrix}\end{align} Is this the right method to compute this?",,"['linear-algebra', 'inner-products', 'orthogonality']"
75,How to define the magnitude of a rotation in $\mathbb{R}^n$?,How to define the magnitude of a rotation in ?,\mathbb{R}^n,"Is there some well established way on how to quantify rotations in $\mathbb{R}^n$? To say which rotation is greater and which is smaller? In $\mathbb{R}^2$ the rotation is characterized by a single angle. This angle can be interpreted as the magnitude of the rotation. In $\mathbb{R}^3$ however we have three possible axes around which to rotate. Furthermore we can get a zero rotation by rotating around each axis by $\pi$ radians. Therefore devices such as the sum of the three angles do not make sense. In $\mathbb{R}^n$ the situation is even more complicated. Let's say that the rotation is characterized by matrix $R$. Is the distance of $R$ from the identity matrix $I$ of the appropriate dimension a reasonable choice? For example  $$ m_{R} = || R - I||  $$  where $||.||$ is some matrix norm. If so, which norm would be appropriate?","Is there some well established way on how to quantify rotations in $\mathbb{R}^n$? To say which rotation is greater and which is smaller? In $\mathbb{R}^2$ the rotation is characterized by a single angle. This angle can be interpreted as the magnitude of the rotation. In $\mathbb{R}^3$ however we have three possible axes around which to rotate. Furthermore we can get a zero rotation by rotating around each axis by $\pi$ radians. Therefore devices such as the sum of the three angles do not make sense. In $\mathbb{R}^n$ the situation is even more complicated. Let's say that the rotation is characterized by matrix $R$. Is the distance of $R$ from the identity matrix $I$ of the appropriate dimension a reasonable choice? For example  $$ m_{R} = || R - I||  $$  where $||.||$ is some matrix norm. If so, which norm would be appropriate?",,"['linear-algebra', 'geometry', 'soft-question', 'definition']"
76,How to prove a set of positive semi definite matrices forms a convex set?,How to prove a set of positive semi definite matrices forms a convex set?,,"Let $C$ be the set of positive semi-definite matrices, how can I prove it is a convex set?","Let $C$ be the set of positive semi-definite matrices, how can I prove it is a convex set?",,"['linear-algebra', 'matrices', 'convex-optimization']"
77,Book on matrix computation,Book on matrix computation,,I'm taking a machine learning course and it involves a lot of matrix computation like compute the derivatives of a matrix with respect to a vector term. In my linear algebra course these material is not covered and I browsed some book in the school library but didn't find something relevant to my problem. So can you recommend me a book which covers these matrix computations? Thanks!,I'm taking a machine learning course and it involves a lot of matrix computation like compute the derivatives of a matrix with respect to a vector term. In my linear algebra course these material is not covered and I browsed some book in the school library but didn't find something relevant to my problem. So can you recommend me a book which covers these matrix computations? Thanks!,,"['linear-algebra', 'matrices', 'reference-request']"
78,Is there a converse for the following Theorem? Equivalent systems of linear equations have exactly the same solutions,Is there a converse for the following Theorem? Equivalent systems of linear equations have exactly the same solutions,,"On page $5$ of Linear Algebra from Hoffman and Kunze's book they prove the following Theorem: Theorem 1. Equivalent systems of linear equations have exactly the same solutions I will add the definition of equivalent system of linear equations. Two systems of linear equations are equivalent if each equation in   each system is a linear combination of the equations in the other   system. I was wondering if the converse is true. The converse appears to be true, but maybe I am missing something. All examples that I found (systems having exactly the same solutions), the systems were equivalent. I think I am missing something trivial!","On page $5$ of Linear Algebra from Hoffman and Kunze's book they prove the following Theorem: Theorem 1. Equivalent systems of linear equations have exactly the same solutions I will add the definition of equivalent system of linear equations. Two systems of linear equations are equivalent if each equation in   each system is a linear combination of the equations in the other   system. I was wondering if the converse is true. The converse appears to be true, but maybe I am missing something. All examples that I found (systems having exactly the same solutions), the systems were equivalent. I think I am missing something trivial!",,[]
79,Linear algebra: What is the difference between target space of a function and the image of a function?,Linear algebra: What is the difference between target space of a function and the image of a function?,,"I'm confused about the difference between target space of a function and image of a function. My textbook (Linear Algebra with Applications 5th edition Otto Bretscher) provided this information when introducing image, but I'm still struggling to understand conceptually the difference between target space and image. Textbook definition of image in relation to target space","I'm confused about the difference between target space of a function and image of a function. My textbook (Linear Algebra with Applications 5th edition Otto Bretscher) provided this information when introducing image, but I'm still struggling to understand conceptually the difference between target space and image. Textbook definition of image in relation to target space",,"['linear-algebra', 'functions']"
80,Generalized Method to find nth power of matrix in $P^n = 5I - 8P$,Generalized Method to find nth power of matrix in,P^n = 5I - 8P,Question: Let $I$ be an identity matrix of order $2 \times 2$ and $P = \begin{bmatrix}     2 & -1 \\     5 & -3 \\    \end{bmatrix}$ . Then the value of $n ∈ N$ for which $P^n = 5I - 8P$ is equal to _______. Answer: 6 Question Source: JEE Mains $18^{th}$ March Shift-2 2021 By characteristic Equation: $P^2-(\operatorname {Tr}(P))P+(\det (P))I=0$ we can find $n=6$ by hit and trial. i.e. multiplying equation by P gives $P^3=(\operatorname {Tr}(P))P^2-(\det (P))P = (\operatorname {Tr}(P))[(\operatorname {Tr}(P))P+(\det(P))I]$ And going on solving for $P^6$ . But is there a generalized method because it can't be done for high values of $n$ (eg: $100$ )?,Question: Let be an identity matrix of order and . Then the value of for which is equal to _______. Answer: 6 Question Source: JEE Mains March Shift-2 2021 By characteristic Equation: we can find by hit and trial. i.e. multiplying equation by P gives And going on solving for . But is there a generalized method because it can't be done for high values of (eg: )?,"I 2 \times 2 P = \begin{bmatrix}
    2 & -1 \\
    5 & -3 \\
   \end{bmatrix} n ∈ N P^n = 5I - 8P 18^{th} P^2-(\operatorname {Tr}(P))P+(\det (P))I=0 n=6 P^3=(\operatorname {Tr}(P))P^2-(\det (P))P = (\operatorname {Tr}(P))[(\operatorname {Tr}(P))P+(\det(P))I] P^6 n 100","['linear-algebra', 'matrices', 'contest-math', 'matrix-equations']"
81,"Suppose that for two $n \times n$ matrices $A,B$ we have $AB = A + B$. Prove that $\text{rank}(A^2) + \text{rank} (B^2) \leq 2 \text{rank} (AB).$",Suppose that for two  matrices  we have . Prove that,"n \times n A,B AB = A + B \text{rank}(A^2) + \text{rank} (B^2) \leq 2 \text{rank} (AB).","Suppose that for two $n \times n$ matrices $A,B$ we have $AB = A + B$ . Prove that $\text{rank}(A^2) + \text{rank} (B^2) \leq 2 \text{rank} (AB).$ This reminds me of Sylvester's Rank Inequality theorem, but I'm not sure if that's really helpful here. I haven't really made significant progress on this beyond writing out a few matrix multiplication. Would appreciate any help at all! Thank you.","Suppose that for two matrices we have . Prove that This reminds me of Sylvester's Rank Inequality theorem, but I'm not sure if that's really helpful here. I haven't really made significant progress on this beyond writing out a few matrix multiplication. Would appreciate any help at all! Thank you.","n \times n A,B AB = A + B \text{rank}(A^2) + \text{rank} (B^2) \leq 2 \text{rank} (AB).","['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
82,"Why is the ""solving for cubic equation roots general rule"" sometimes not applicable while the equation obviously has roots?","Why is the ""solving for cubic equation roots general rule"" sometimes not applicable while the equation obviously has roots?",,"the general rule: we have $ax^3+bx^2+cx+d=0$ $\Delta_0=b^2-3ac$ $\Delta_1=2b^3-9abc+27a^2d$ $C=\sqrt{\Delta_1^2-4\Delta_0^3}$ $D=(\frac{\Delta_1+C}{2})^\frac{1}{3}$ $x=-\frac{1}{3a}(b+D+\frac{\Delta_0}{D})$ imagine $x^3-6x^2+11x-6=0$ we know its roots are $x=1$ , $x=2$ and $x=3$ . but when you use the general rule, you will find a negative $\Delta_1^2-4\Delta_0^3$ and thus; you can't continue the process! Also, when you use this rule, you will just find ONE real root (remember the last process to find final $x$ ); while that equation has 3 real roots and no imaginary roots. so how to find the other real roots using the general rule?!","the general rule: we have imagine we know its roots are , and . but when you use the general rule, you will find a negative and thus; you can't continue the process! Also, when you use this rule, you will just find ONE real root (remember the last process to find final ); while that equation has 3 real roots and no imaginary roots. so how to find the other real roots using the general rule?!",ax^3+bx^2+cx+d=0 \Delta_0=b^2-3ac \Delta_1=2b^3-9abc+27a^2d C=\sqrt{\Delta_1^2-4\Delta_0^3} D=(\frac{\Delta_1+C}{2})^\frac{1}{3} x=-\frac{1}{3a}(b+D+\frac{\Delta_0}{D}) x^3-6x^2+11x-6=0 x=1 x=2 x=3 \Delta_1^2-4\Delta_0^3 x,['linear-algebra']
83,Why is $\det(I + A^{50}) = 4$ here?,Why is  here?,\det(I + A^{50}) = 4,"I’m trying to understand the reasoning behind the following: Let $A \in \mathbb{R}^{3\times3}$ with eigenvalues $1$ , $-1$ , $0$ . What is $\det \left(I + A^{50} \right)$ ? The given answer is 4. A is obviously singular due to eigenvalue 0. Moreover, spectral decomposition of $A$ exists: $$ A = T diag(1, -1, 0) T^{-1}\\ A^{50} = T diag(1, -1, 0)^{50} T^{-1} $$ But I can’t see why $\det(I + T diag(1, -1, 0)^{50}T^{-1})$ would be 4 either, due to the limited knowledge about $T$ . Can somebody shed light on this for me?","I’m trying to understand the reasoning behind the following: Let with eigenvalues , , . What is ? The given answer is 4. A is obviously singular due to eigenvalue 0. Moreover, spectral decomposition of exists: But I can’t see why would be 4 either, due to the limited knowledge about . Can somebody shed light on this for me?","A \in \mathbb{R}^{3\times3} 1 -1 0 \det \left(I + A^{50} \right) A 
A = T diag(1, -1, 0) T^{-1}\\
A^{50} = T diag(1, -1, 0)^{50} T^{-1}
 \det(I + T diag(1, -1, 0)^{50}T^{-1}) T","['linear-algebra', 'matrices', 'determinant']"
84,What is $X^TX$ called where $X$ is a matrix?,What is  called where  is a matrix?,X^TX X,"With $X$ being a matrix, is there a general name for the expression $X^TX$ ?","With being a matrix, is there a general name for the expression ?",X X^TX,"['linear-algebra', 'matrices']"
85,"Why do the properties of determinants (used to calculate determinants from multiple matrices) apply not only to rows, but to columns as well?","Why do the properties of determinants (used to calculate determinants from multiple matrices) apply not only to rows, but to columns as well?",,"A set of rules in my textbook is as follows: a. If $A$ has a zero row (column), then $\det A = 0$ b. If $B$ is obtained by interchanging two rows (columns) of $A$ , then $\det B = -\det A$ c. If $A$ has two identical rows (columns), then $\det A = 0$ d. If $B$ is obtained by multiplying a row (column) of $A$ by $k$ , then $\det B = k\cdot\det A$ e. If $A$ , $B$ , and $C$ are identical except that the $i$ -th row (column) of $C$ is the sum of the $i$ -th rows (columns) of $A$ and $B$ , then $\det C = \det B + \det A$ f. If $B$ is obtained by adding a multiple of one row (column) of $A$ to another row (column), then $\det B = \det A$ I don't understand why ""column"" is in parentheses after every instance of row. Is that the same as saying, for example with clause a: ""if $A$ has a zero row or a zero column, then $\det A = 0$ ""? As in, it's saying that column and row can be used interchangably in the statement, as the statement holds true either way? If the above interpretation is correct, then how? Why would these statements that apply to rows also apply to columns. The only situation I could see it applying is if the matrix is symmetrical, but the question doesn't specify that, it only says that the matrix is square. Any help is appreciated.","A set of rules in my textbook is as follows: a. If has a zero row (column), then b. If is obtained by interchanging two rows (columns) of , then c. If has two identical rows (columns), then d. If is obtained by multiplying a row (column) of by , then e. If , , and are identical except that the -th row (column) of is the sum of the -th rows (columns) of and , then f. If is obtained by adding a multiple of one row (column) of to another row (column), then I don't understand why ""column"" is in parentheses after every instance of row. Is that the same as saying, for example with clause a: ""if has a zero row or a zero column, then ""? As in, it's saying that column and row can be used interchangably in the statement, as the statement holds true either way? If the above interpretation is correct, then how? Why would these statements that apply to rows also apply to columns. The only situation I could see it applying is if the matrix is symmetrical, but the question doesn't specify that, it only says that the matrix is square. Any help is appreciated.",A \det A = 0 B A \det B = -\det A A \det A = 0 B A k \det B = k\cdot\det A A B C i C i A B \det C = \det B + \det A B A \det B = \det A A \det A = 0,"['linear-algebra', 'matrices', 'determinant']"
86,Why $A$ invertible $\iff \det A\neq 0$,Why  invertible,A \iff \det A\neq 0,"Let $A$ a matrix $n\times n$ over $\mathbb R$. I'm trying to prove that $A$ is invertible $\iff\det A\neq 0$. If $A$ is invertible, there is $B$ s.t. $AB=I$, and thus $$\det(A)\det(B)=\det(AB)=\det(I)=1,$$ and thus $\det A\neq 0$. I have problem to prove the converse. Let $A$ s.t. $\det(A)\neq 0$. I would like to say that $$\det(A)\det(A)^{-1}=1.$$ I know that if $A^{-1}$ exist then $\det(A)^{-1}=\det(A^{-1})$, but since I have to prove that $A^{-1}$ exist, I can't use this formula... so how can I conclude ?","Let $A$ a matrix $n\times n$ over $\mathbb R$. I'm trying to prove that $A$ is invertible $\iff\det A\neq 0$. If $A$ is invertible, there is $B$ s.t. $AB=I$, and thus $$\det(A)\det(B)=\det(AB)=\det(I)=1,$$ and thus $\det A\neq 0$. I have problem to prove the converse. Let $A$ s.t. $\det(A)\neq 0$. I would like to say that $$\det(A)\det(A)^{-1}=1.$$ I know that if $A^{-1}$ exist then $\det(A)^{-1}=\det(A^{-1})$, but since I have to prove that $A^{-1}$ exist, I can't use this formula... so how can I conclude ?",,['linear-algebra']
87,"When a matrix, $M$ equals $M^2$, why is this property true?","When a matrix,  equals , why is this property true?",M M^2,"While we were looking at idempotent matrices, my teacher brought up some properties about them, specifically when the square of a matrix equals itself, the matrix's null space equals the column space of the matrix minus its respective identity matrix. Is this something that should be trivial, I've been trying to convince myself this is true but I can't seem to reason this to myself.","While we were looking at idempotent matrices, my teacher brought up some properties about them, specifically when the square of a matrix equals itself, the matrix's null space equals the column space of the matrix minus its respective identity matrix. Is this something that should be trivial, I've been trying to convince myself this is true but I can't seem to reason this to myself.",,"['linear-algebra', 'matrices', 'vector-spaces']"
88,Matrix invertiblity and its Inverse,Matrix invertiblity and its Inverse,,"I'd like to prove that the Matrix $L:={ M }^{ T }M$ is invertible and determine its inverse (in dependence of $A$ and $B$). $M:=\begin{pmatrix}A & B \\ 0_{q\times p}& I_q\end{pmatrix}$ and $A\in K^{p\times p},B\in K^{p\times q}$.  Further is given that the matrix $A$ has rank $p$. I tried to form $L=\begin{pmatrix}{ A }^T A& A^T B\\ B^T A &{ B^T B+I }_{ q }\end{pmatrix}$ into the Unity Blockmatrix using elementary row operations. Since A has a full rank it must be invertible and because A is a $p\times p$ matrix, its transpose must be invertible too, with this knowledge its fairly easy to get ${ a }_{ 21 }=0$ and  I'm stuck in getting ${ a }_{ 12 }=0$ since I know nothing about the invertibility of $B$. Are there any other ways to solve this problem?","I'd like to prove that the Matrix $L:={ M }^{ T }M$ is invertible and determine its inverse (in dependence of $A$ and $B$). $M:=\begin{pmatrix}A & B \\ 0_{q\times p}& I_q\end{pmatrix}$ and $A\in K^{p\times p},B\in K^{p\times q}$.  Further is given that the matrix $A$ has rank $p$. I tried to form $L=\begin{pmatrix}{ A }^T A& A^T B\\ B^T A &{ B^T B+I }_{ q }\end{pmatrix}$ into the Unity Blockmatrix using elementary row operations. Since A has a full rank it must be invertible and because A is a $p\times p$ matrix, its transpose must be invertible too, with this knowledge its fairly easy to get ${ a }_{ 21 }=0$ and  I'm stuck in getting ${ a }_{ 12 }=0$ since I know nothing about the invertibility of $B$. Are there any other ways to solve this problem?",,"['linear-algebra', 'matrices', 'inverse']"
89,Solving these two equations,Solving these two equations,,"Solve for $x$ and $y$   $$5x(1+\frac{1}{x^2+y^2})=12$$ $$5y(1-\frac{1}{x^2+y^2})=4$$ Combining these two equations we get  $$5x^3-15x^2y+5xy^2-15y^3+5x+15y=0$$ Which could be factored if it had $-15y$ instead of $+15y$ for the last term. Generally I do not post questions involving solving simple equations, but I find it really hard!","Solve for $x$ and $y$   $$5x(1+\frac{1}{x^2+y^2})=12$$ $$5y(1-\frac{1}{x^2+y^2})=4$$ Combining these two equations we get  $$5x^3-15x^2y+5xy^2-15y^3+5x+15y=0$$ Which could be factored if it had $-15y$ instead of $+15y$ for the last term. Generally I do not post questions involving solving simple equations, but I find it really hard!",,"['linear-algebra', 'algebra-precalculus', 'polynomials', 'systems-of-equations', 'quartics']"
90,Prove that $(A+B)^n = 0$,Prove that,(A+B)^n = 0,Let $A$ and $B$ be $n \times n$ real matrices such that $A^n = B^n = 0$ and $AB = BA$. Prove that $(A+B)^n = 0$. We have $$(A+B)^n = A^n+A^{n-1}B\binom{n}{1}+\cdots+AB^{n-1}\binom{n}{n-1}+B^n.$$ I tried proving this for just the case $n = 2$. We have $(A+B)^2 = A^2+2AB+B^2 = 2AB$ since $A^2 = B^2 = 0$. Then I didn't see how to show that $AB = 0$.,Let $A$ and $B$ be $n \times n$ real matrices such that $A^n = B^n = 0$ and $AB = BA$. Prove that $(A+B)^n = 0$. We have $$(A+B)^n = A^n+A^{n-1}B\binom{n}{1}+\cdots+AB^{n-1}\binom{n}{n-1}+B^n.$$ I tried proving this for just the case $n = 2$. We have $(A+B)^2 = A^2+2AB+B^2 = 2AB$ since $A^2 = B^2 = 0$. Then I didn't see how to show that $AB = 0$.,,['linear-algebra']
91,Why aren't all matrices diagonalisable?,Why aren't all matrices diagonalisable?,,"I'm wondering what the difference between canonical forms of a matrix are; in particular why they aren't all equivalent to diagonal matrices. For example, we know that all matrices have an upper triangular form. If we use elementary row and column operations, then wouldn't the matrix become diagonal? Not sure how this changes diagonalisability, since the pre and post-multiplication matrices are still both square and invertible. Similarly for other canonical forms such as Jordan Normal Form. Under this logic, why aren't all matrices diagonalisable? Do I have the definition of diagonalisability wrong?","I'm wondering what the difference between canonical forms of a matrix are; in particular why they aren't all equivalent to diagonal matrices. For example, we know that all matrices have an upper triangular form. If we use elementary row and column operations, then wouldn't the matrix become diagonal? Not sure how this changes diagonalisability, since the pre and post-multiplication matrices are still both square and invertible. Similarly for other canonical forms such as Jordan Normal Form. Under this logic, why aren't all matrices diagonalisable? Do I have the definition of diagonalisability wrong?",,"['linear-algebra', 'matrices']"
92,Construct an endomorphism $f$ such that $f\circ f=-Id$,Construct an endomorphism  such that,f f\circ f=-Id,"Let $E$ be a real vector space of finite dimension $n$ and $f$ an endomorphism such that   $$f\circ f=-Id_E$$ Show that $n = \dim (E)$ is an even integer Assume $n$ is even, $n=2p$. Construct an endomorphism $f$ such that $f\circ f=-Id$ 1) I have,  $f^2 = -Id \implies \det{(f^2)} = (\det{f})^2 = (-1)^n$.  So being on a real space, n is even. 2) I thought about rotation for $f$, but I have no ideas to construct it explicitly. Thanks in advance,","Let $E$ be a real vector space of finite dimension $n$ and $f$ an endomorphism such that   $$f\circ f=-Id_E$$ Show that $n = \dim (E)$ is an even integer Assume $n$ is even, $n=2p$. Construct an endomorphism $f$ such that $f\circ f=-Id$ 1) I have,  $f^2 = -Id \implies \det{(f^2)} = (\det{f})^2 = (-1)^n$.  So being on a real space, n is even. 2) I thought about rotation for $f$, but I have no ideas to construct it explicitly. Thanks in advance,",,[]
93,If the Wronskian of two functions is zero then these functions are LD,If the Wronskian of two functions is zero then these functions are LD,,"I'm studying a book of differential equations which says that if the Wronskian of two functions is zero then these functions are linearly dependent. the author doesn't prove it, he simply said as a easy consequence of basic properties of determinants, I tried to prove by myself without success. I need help. thanks a lot.","I'm studying a book of differential equations which says that if the Wronskian of two functions is zero then these functions are linearly dependent. the author doesn't prove it, he simply said as a easy consequence of basic properties of determinants, I tried to prove by myself without success. I need help. thanks a lot.",,"['linear-algebra', 'ordinary-differential-equations']"
94,does a matrix like this exist?,does a matrix like this exist?,,Question: Does a matrix $A \in M_{3 \times 3}(F)$ exist s.t. $A^4= \begin{bmatrix} 0&0&1\\0&0&0\\0&0&0\end{bmatrix}$ What I thought: I think it doesn't. How do you start a proof of such a thing (prefer hints at first). Thx,Question: Does a matrix $A \in M_{3 \times 3}(F)$ exist s.t. $A^4= \begin{bmatrix} 0&0&1\\0&0&0\\0&0&0\end{bmatrix}$ What I thought: I think it doesn't. How do you start a proof of such a thing (prefer hints at first). Thx,,['linear-algebra']
95,What free software can I use to solve a system of linear equations containing an unknown?,What free software can I use to solve a system of linear equations containing an unknown?,,"Question : What free software can I use to solve a system of linear equations $M\mathbf{x}=\mathbf{y}$ where the entries of $\mathbf{y}$ vary with an unknown quantity $n$? Presumably I could do this in Maple or Mathematica, but I don't have access to these packages at this time. Motivation : In an attempt to answer my question here , I've set up a system of linear equations where: $M$ is a $30 \times 27$ $(0,1)$-matrix. $\mathbf{x}$ is a vector of length $27$, which I'd like to attempt to solve for (the entries represent the number of symbols of a given type in a given block in my problem).  If this method were to work, there should be no solutions. $\mathbf{y}$ is a vector of length $27$, where the entries are either $0$, $(n-2k)^2$, $(n-2k)k$, or $k^2$, where $k=\lfloor n/3 \rfloor+1$. I can solve this in GAP using SolutionMat for a given value of $n$.  But I would like to do it symbolically, which would instead result in a proof for all $n$.  I don't believe this is possible in GAP (since x:=Indeterminate(Rationals,""x""); followed by SolutionMat([1],[x]); returns an error).","Question : What free software can I use to solve a system of linear equations $M\mathbf{x}=\mathbf{y}$ where the entries of $\mathbf{y}$ vary with an unknown quantity $n$? Presumably I could do this in Maple or Mathematica, but I don't have access to these packages at this time. Motivation : In an attempt to answer my question here , I've set up a system of linear equations where: $M$ is a $30 \times 27$ $(0,1)$-matrix. $\mathbf{x}$ is a vector of length $27$, which I'd like to attempt to solve for (the entries represent the number of symbols of a given type in a given block in my problem).  If this method were to work, there should be no solutions. $\mathbf{y}$ is a vector of length $27$, where the entries are either $0$, $(n-2k)^2$, $(n-2k)k$, or $k^2$, where $k=\lfloor n/3 \rfloor+1$. I can solve this in GAP using SolutionMat for a given value of $n$.  But I would like to do it symbolically, which would instead result in a proof for all $n$.  I don't believe this is possible in GAP (since x:=Indeterminate(Rationals,""x""); followed by SolutionMat([1],[x]); returns an error).",,"['linear-algebra', 'math-software', 'computational-mathematics', 'computer-algebra-systems', 'gap']"
96,Singular value proofs,Singular value proofs,,"1.) Let $A$ be a nonsingular square matrix. a.) Prove that the product of the singular values of $A$ equals the   absolute value of its determinant:   $\sigma_1\sigma_2...\sigma_n=|detA|$. b.) Does the sum equal the absolute value of the trace? And a matrix   whose determinant is very small is ill-conditioned? c.) Show that if $|detA|< 10^{-k}$, then its minimal singular value   satisfies $\sigma_n <10^{-\frac{k}{n}}$. Can you construct an   ill-conditioned matrix with $detA = 1$? 2.) Let A be a square matrix. Prove that its maximum eigenvalue is smaller than its maximal singular value. My attempt: a.) I know that the equation for singular values is $A = U \dot\ E \dot\ V$ so $|det(A)| = |det(U) \dot\ det(E) \dot\ det(V)| = |\pm1 \dot\ $ (product of singular values) | = product of singular values. Is that correct? b.) True for the first part. For the second part, do they mean a matrix can have ill-conditioned determinant if its determinant is small? Not sure what they are asking. c.) I do not know how to do. 2.) Not sure how to do because I thought that the singular value should be smaller than the maximum eigenvalue?","1.) Let $A$ be a nonsingular square matrix. a.) Prove that the product of the singular values of $A$ equals the   absolute value of its determinant:   $\sigma_1\sigma_2...\sigma_n=|detA|$. b.) Does the sum equal the absolute value of the trace? And a matrix   whose determinant is very small is ill-conditioned? c.) Show that if $|detA|< 10^{-k}$, then its minimal singular value   satisfies $\sigma_n <10^{-\frac{k}{n}}$. Can you construct an   ill-conditioned matrix with $detA = 1$? 2.) Let A be a square matrix. Prove that its maximum eigenvalue is smaller than its maximal singular value. My attempt: a.) I know that the equation for singular values is $A = U \dot\ E \dot\ V$ so $|det(A)| = |det(U) \dot\ det(E) \dot\ det(V)| = |\pm1 \dot\ $ (product of singular values) | = product of singular values. Is that correct? b.) True for the first part. For the second part, do they mean a matrix can have ill-conditioned determinant if its determinant is small? Not sure what they are asking. c.) I do not know how to do. 2.) Not sure how to do because I thought that the singular value should be smaller than the maximum eigenvalue?",,['linear-algebra']
97,How do you find a basis for the set of all $3 \times 3$ matrices whose rows and columns add up to zero?,How do you find a basis for the set of all  matrices whose rows and columns add up to zero?,3 \times 3,"If $W$ is the set of all $3 \times 3$ matrices whose rows and columns add up to zero, how would you find a basis for this? There seem to be so many scenarios we'd need to cover, I can't find a way to succinctly find the answer / represent it well. On a related note, how would you extend the basis to be a basis for $M_3(R)$? (I feel like I might be able to get this one, though, once I can clearly find a basis to begin with.) Thanks in advance for the advice.","If $W$ is the set of all $3 \times 3$ matrices whose rows and columns add up to zero, how would you find a basis for this? There seem to be so many scenarios we'd need to cover, I can't find a way to succinctly find the answer / represent it well. On a related note, how would you extend the basis to be a basis for $M_3(R)$? (I feel like I might be able to get this one, though, once I can clearly find a basis to begin with.) Thanks in advance for the advice.",,['linear-algebra']
98,Is this subset a subspace?,Is this subset a subspace?,,"$$S = \{(x_1, x_2, x_3) \in \mathbb{R}^3\ |\ x_2 − (x_1)^2 = 0\}$$ I found this question in an old exam and I'm not sure how to prove this question, but I know it is not a subspace. Any help is appreciated.","$$S = \{(x_1, x_2, x_3) \in \mathbb{R}^3\ |\ x_2 − (x_1)^2 = 0\}$$ I found this question in an old exam and I'm not sure how to prove this question, but I know it is not a subspace. Any help is appreciated.",,"['linear-algebra', 'vector-spaces']"
99,On the eigenvalues of the adjugate of a non-singular matrix,On the eigenvalues of the adjugate of a non-singular matrix,,"How could we prove that If $\lambda_1,\lambda_2,\lambda_3,\dots \lambda_n $ are the eigenvalues of a non-singular square matrix $A$ then the eigenvalues of $\operatorname{adj} A$ are $$~\frac {\det A}{\lambda_1},\frac {\det A}{\lambda_2},\frac {\det A}{\lambda_3},\dots, \frac {\det A}{\lambda_n}$$ I stumbled upon this property while solving a MCQ type question, in the solution there is no proof, I was just wondering if anybody could show me how to prove this one.","How could we prove that If are the eigenvalues of a non-singular square matrix then the eigenvalues of are I stumbled upon this property while solving a MCQ type question, in the solution there is no proof, I was just wondering if anybody could show me how to prove this one.","\lambda_1,\lambda_2,\lambda_3,\dots \lambda_n  A \operatorname{adj} A ~\frac {\det A}{\lambda_1},\frac {\det A}{\lambda_2},\frac {\det A}{\lambda_3},\dots, \frac {\det A}{\lambda_n}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
