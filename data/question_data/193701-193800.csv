,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"History question about the difference quotient: what does ""h"" mean?","History question about the difference quotient: what does ""h"" mean?",,"Regarding the difference quotient (f(x+h) - f(x)/h), what is the origin of the usage of ""h""? Does it mean anything? Thank you!","Regarding the difference quotient (f(x+h) - f(x)/h), what is the origin of the usage of ""h""? Does it mean anything? Thank you!",,"['limits', 'derivatives', 'notation', 'math-history']"
1,"In non-standard analysis, should we necessarily consider derivative as slope between two infinitesimally apart points?","In non-standard analysis, should we necessarily consider derivative as slope between two infinitesimally apart points?",,"For standard analysis, an answer (in the concept of limit) is here . However, when dealing with infinitesimals in non-standard analysis, that limit technique will not work. So in non standard analysis, should we necessarily consider derivative as slope between two infinitesimally apart points? Or should we still consider it as slope at ""a point in hyper real number line $(\varepsilon)$""? If the latter is true, please give a reason.","For standard analysis, an answer (in the concept of limit) is here . However, when dealing with infinitesimals in non-standard analysis, that limit technique will not work. So in non standard analysis, should we necessarily consider derivative as slope between two infinitesimally apart points? Or should we still consider it as slope at ""a point in hyper real number line $(\varepsilon)$""? If the latter is true, please give a reason.",,"['calculus', 'derivatives', 'nonstandard-analysis']"
2,Is the piecewise-defined function differentiable,Is the piecewise-defined function differentiable,,"The function is defined as $$f(x)=\begin{cases}x^2, &\text{ for }x\leq 1\\ \sqrt{x}, &\text{ for }x>1\end{cases}$$ and Is this function differentiable at $x=1$? I thought that since $\lim_{x\to 1}$ of $f'(x)$ exists then it IS differentiable. And I think this limit does exist so it should be differentiable. Book says no. My logic must not be correct here.","The function is defined as $$f(x)=\begin{cases}x^2, &\text{ for }x\leq 1\\ \sqrt{x}, &\text{ for }x>1\end{cases}$$ and Is this function differentiable at $x=1$? I thought that since $\lim_{x\to 1}$ of $f'(x)$ exists then it IS differentiable. And I think this limit does exist so it should be differentiable. Book says no. My logic must not be correct here.",,"['calculus', 'real-analysis', 'derivatives']"
3,"If $x:[0,a]\to \Bbb{R}^n$ is differentiable, what is $\frac{d}{dt}\Vert x(t)\Vert^2?$","If  is differentiable, what is","x:[0,a]\to \Bbb{R}^n \frac{d}{dt}\Vert x(t)\Vert^2?","Let $\Vert\;\Vert$ be the Euclidean norm on $\Bbb{R}^n$. Let $x:[0,a]\to \Bbb{R}^n$ be differentiable. How do I define \begin{align}\frac{d}{dt}\Vert x(t)\Vert^2?\end{align} Please, I need help on this! A detailed answer would suit me. Thanks a lot!","Let $\Vert\;\Vert$ be the Euclidean norm on $\Bbb{R}^n$. Let $x:[0,a]\to \Bbb{R}^n$ be differentiable. How do I define \begin{align}\frac{d}{dt}\Vert x(t)\Vert^2?\end{align} Please, I need help on this! A detailed answer would suit me. Thanks a lot!",,['derivatives']
4,Is the given expression about higher derivatives valid?,Is the given expression about higher derivatives valid?,,"$$\left(\frac{d}{{dx}}\right)^2y   = \frac{d^2}{{dx}^2}y$$ Can we use both of them to express second order derivative? I came across this (I am just confused about the notation they have used, so to be sure,I posted this question.) While searching over internet, I only found $$\left(\frac{d^2y}{{dx}^2}\right)$$ is not equal to $$\left(\frac{dy}{{dx}}\right)^2$$, about which I am pretty clear.","$$\left(\frac{d}{{dx}}\right)^2y   = \frac{d^2}{{dx}^2}y$$ Can we use both of them to express second order derivative? I came across this (I am just confused about the notation they have used, so to be sure,I posted this question.) While searching over internet, I only found $$\left(\frac{d^2y}{{dx}^2}\right)$$ is not equal to $$\left(\frac{dy}{{dx}}\right)^2$$, about which I am pretty clear.",,"['calculus', 'derivatives', 'notation']"
5,Meaning of the nth order derivative,Meaning of the nth order derivative,,"We say that the number zero is infinitely differentiable(because every higher order derivative exists and is identically zero). But then if that is the case , shouldn’t every function be infinitely differentiable ? Suppose we differentiate a differentiable function $n$ number of times and we get zero , we can still differentiate it infinitely right ? Then why do many textbooks call a function twice differentiable , thrice differentiable etc. ? I’m sorry if this sounds really stupid but this was something me and my friends had a huge debate on so I wanted to clear it once and for all ! Please correct me if I am mistaken somewhere Thanks for your help","We say that the number zero is infinitely differentiable(because every higher order derivative exists and is identically zero). But then if that is the case , shouldn’t every function be infinitely differentiable ? Suppose we differentiate a differentiable function $n$ number of times and we get zero , we can still differentiate it infinitely right ? Then why do many textbooks call a function twice differentiable , thrice differentiable etc. ? I’m sorry if this sounds really stupid but this was something me and my friends had a huge debate on so I wanted to clear it once and for all ! Please correct me if I am mistaken somewhere Thanks for your help",,"['derivatives', 'soft-question']"
6,$f(x)$ differentiable at $x_0$ with full rank derivatives implies $|f(x_0) - f(x)| \ge C |x_0 - x|$?!,differentiable at  with full rank derivatives implies ?!,f(x) x_0 |f(x_0) - f(x)| \ge C |x_0 - x|,"I found a claim in this highly cited paper: Pakes, A., & Pollard, D. (1989). Simulation and the asymptotics of optimization estimators. Econometrica: Journal of the Econometric Society, 1027-1057. The claim appears in the proof of theorem 3.3 and is essentially the following: (1?) If $f(x)$ is differentiable at $x_0$ with a derivative matrix $\Gamma$ of full rank, then there exists a $C > 0$ and a neighborhood $\mathcal{B}$ of $x_0$ such that, for every $x$ in $\mathcal{B}$ , $$ \| f(x) - f(x_0) \| \ge C \| x - x_0 \|$$ How do I prove this statement? Any help would be very appreciated.","I found a claim in this highly cited paper: Pakes, A., & Pollard, D. (1989). Simulation and the asymptotics of optimization estimators. Econometrica: Journal of the Econometric Society, 1027-1057. The claim appears in the proof of theorem 3.3 and is essentially the following: (1?) If is differentiable at with a derivative matrix of full rank, then there exists a and a neighborhood of such that, for every in , How do I prove this statement? Any help would be very appreciated.",f(x) x_0 \Gamma C > 0 \mathcal{B} x_0 x \mathcal{B}  \| f(x) - f(x_0) \| \ge C \| x - x_0 \|,"['real-analysis', 'derivatives', 'lipschitz-functions']"
7,Why Rolle's theorem is giving two roots here?,Why Rolle's theorem is giving two roots here?,,"I am trying to find Number of distinct roots of $$f(x)=x+5\cos x=0$$ in $\left[0, \pi \right]$ we have $f(0)=5 \gt 0$ and $f(\pi)=\pi-5 \lt 0$ so by IVT we have at least one root in $\left[0,  \pi \right]$ Now IVT does not give information on number of roots, So I assumed let $p$ and $q$ are two distinct roots of $f(x)$ in $\left[0, \pi \right]$ Now $$f(p)=f(q)=0$$ Now applying Rolle's Theorem we get $$f'(c)=0$$ that is $$1-5 \sin c=0$$ so $$c =\arcsin(0.2)$$ which  $\in$ $\left[0,\pi \right]$ hence My assumption that there are two roots $p$ and $q$ is correct. But Graph shows there is only one root in $\left[0,\pi \right]$ Where I went wrong?","I am trying to find Number of distinct roots of $$f(x)=x+5\cos x=0$$ in $\left[0, \pi \right]$ we have $f(0)=5 \gt 0$ and $f(\pi)=\pi-5 \lt 0$ so by IVT we have at least one root in $\left[0,  \pi \right]$ Now IVT does not give information on number of roots, So I assumed let $p$ and $q$ are two distinct roots of $f(x)$ in $\left[0, \pi \right]$ Now $$f(p)=f(q)=0$$ Now applying Rolle's Theorem we get $$f'(c)=0$$ that is $$1-5 \sin c=0$$ so $$c =\arcsin(0.2)$$ which  $\in$ $\left[0,\pi \right]$ hence My assumption that there are two roots $p$ and $q$ is correct. But Graph shows there is only one root in $\left[0,\pi \right]$ Where I went wrong?",,"['algebra-precalculus', 'trigonometry', 'derivatives', 'continuity', 'rolles-theorem']"
8,Lipschitz constant of $\|Ax-b\|_2^2$ and $A^T(Ax-b)$,Lipschitz constant of  and,\|Ax-b\|_2^2 A^T(Ax-b),"The Lipschitz constant $L$ of a function $f$ is defined as follows $$\| f(y) - f(x) \|_2 \leq L \|y-x\|_2$$ I want to find the Lipschitz constants for the following functions: $$f_1(x) = \|Ax-b\|_2^2$$ $$f_2(x) = A^T (Ax-b)$$ where $A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, b\in\mathbb{R}^m$. Any help?","The Lipschitz constant $L$ of a function $f$ is defined as follows $$\| f(y) - f(x) \|_2 \leq L \|y-x\|_2$$ I want to find the Lipschitz constants for the following functions: $$f_1(x) = \|Ax-b\|_2^2$$ $$f_2(x) = A^T (Ax-b)$$ where $A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, b\in\mathbb{R}^m$. Any help?",,"['calculus', 'derivatives', 'inequality', 'optimization', 'lipschitz-functions']"
9,Derivative of the nuclear norm ${\left\| {XA} \right\|_*}$ with respect to $X$,Derivative of the nuclear norm  with respect to,{\left\| {XA} \right\|_*} X,"The nuclear norm (also known as trace norm) is defined as \begin{equation} {\left\| M \right\|_*} = \mbox{tr} \left( {\sqrt {{M^T}M} } \right) = \sum\limits_{i = 1}^{\min \left\{ {m,n} \right\}} {{\sigma _i}\left( M \right)}  \end{equation} where ${\sigma _i}\left( M \right)$ denotes the $i$-th singular value of $M$. My question is how to compute the derivative of ${\left\| {XA} \right\|_*}$ with respect to $X$, i.e., \begin{equation} \frac{{\partial {{\left\| {XA} \right\|}_*}}}{{\partial X}} \end{equation} In fact, I want to use it for the gradient descent optimization algorithm. Note that there is a similar question , according to which the sub-gradient of ${\left\| X \right\|_*}$  is $U{V^T}$, where $U\Sigma {V^T}$ is the SVD decomposition of $X$. I hope this is helpful. Thanks a lot for your help.","The nuclear norm (also known as trace norm) is defined as \begin{equation} {\left\| M \right\|_*} = \mbox{tr} \left( {\sqrt {{M^T}M} } \right) = \sum\limits_{i = 1}^{\min \left\{ {m,n} \right\}} {{\sigma _i}\left( M \right)}  \end{equation} where ${\sigma _i}\left( M \right)$ denotes the $i$-th singular value of $M$. My question is how to compute the derivative of ${\left\| {XA} \right\|_*}$ with respect to $X$, i.e., \begin{equation} \frac{{\partial {{\left\| {XA} \right\|}_*}}}{{\partial X}} \end{equation} In fact, I want to use it for the gradient descent optimization algorithm. Note that there is a similar question , according to which the sub-gradient of ${\left\| X \right\|_*}$  is $U{V^T}$, where $U\Sigma {V^T}$ is the SVD decomposition of $X$. I hope this is helpful. Thanks a lot for your help.",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'nuclear-norm']"
10,Equidifferentiable iff derivative is equicontinuous?,Equidifferentiable iff derivative is equicontinuous?,,"Let $\{f_n\}$ a sequence of function differentiable at $x_0$ We have equidifferentiability at $x_0$, if $\lim_{h \to 0} \max_n \left|   \frac{f_n(x_0+h) - f(x_0)}{h}- f'_n(x_0)\right| = 0$ Are the two following statement true? (1?) The family is equidifferentiable at $x_0$ iff the derivatives $f'_n(x_0)$ are equicontinuous at $x_0$. (2?) If the derivatives $f'_n(x_0)$ are uniformly bounded, then $f_n(x_0)$ is equicontinuous at $x_0$ The result would be motivated by similar results when studying a single function $f$ over multiple $x$, (1') A function $f$ is uniformly differentiable iff its derivative is uniformly continuous. (2') If the derivative of $f$ is bounded, then $f$ is uniformly continuous.","Let $\{f_n\}$ a sequence of function differentiable at $x_0$ We have equidifferentiability at $x_0$, if $\lim_{h \to 0} \max_n \left|   \frac{f_n(x_0+h) - f(x_0)}{h}- f'_n(x_0)\right| = 0$ Are the two following statement true? (1?) The family is equidifferentiable at $x_0$ iff the derivatives $f'_n(x_0)$ are equicontinuous at $x_0$. (2?) If the derivatives $f'_n(x_0)$ are uniformly bounded, then $f_n(x_0)$ is equicontinuous at $x_0$ The result would be motivated by similar results when studying a single function $f$ over multiple $x$, (1') A function $f$ is uniformly differentiable iff its derivative is uniformly continuous. (2') If the derivative of $f$ is bounded, then $f$ is uniformly continuous.",,"['derivatives', 'uniform-continuity', 'sequence-of-function', 'equicontinuity']"
11,"For any two functions $f_1 : [0,1] →\mathbb R$ and $f_2 : [0,1] →\mathbb R$, check which among the statements are true.","For any two functions  and , check which among the statements are true.","f_1 : [0,1] →\mathbb R f_2 : [0,1] →\mathbb R","For any two functions $f_1 : [0,1] →\mathbb R$ and $f_2 : [0,1]  →\mathbb R$, deﬁne the function $g : [0,1] →\mathbb R$ as $g(x) =  \max(f_1(x),f_2(x))$ for all $x ∈ [0,1]$. A. If $f_1$ and $f_2$ are linear, then $g$ is linear B. If $f_1$ and $f_2$ are diﬀerentiable, then g is diﬀerentiable C. If $f_1$ and $f_2$ are convex, then g is convex D. None of the above My attempt:- $g(x) =  \max(f_1(x),f_2(x))=\frac{f_1(x)+f_2(x)}{2}+\frac{|f_1(x)-f_2(x)|}{2}$ A. $g(0)=\max\{f_1(0),f_2(0)\}=0$ $g(cx+y)=\max\{f_1(cx+y),f_2(cx+y)\}=\frac{f_1(cx+y)+f_2(cx+y)}{2}+\frac{|f_1(cx+y)-f_2(cx+y)|}{2}=\frac{cf_1(x)+f_1(y)+cf_2(x)+f_2(y)}{2}+\frac{|cf_1(x)+f_1(y)-(cf_2(x)+f_2(y))|}{2}=c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+\frac{|c(f_1(x)-f_2(x))+(f_1(y)-f_2(y))|}{2}$. $c(f_1(x)-f_2(x))$ and $(f_1(x)-f_2(x))$ lie in the same line. so, Equality holds in the triangular ineqality. $c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+\frac{|c(f_1(x)-f_2(x))|}{2}+\frac{|(f_1(y)-f_2(y))|}{2}\implies$ $c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+|c|\frac{|(f_1(x)-f_2(x))|}{2}+\frac{|(f_1(y)-f_2(y))|}{2}$ It need not be linear. Since,If $c<0$, $|c|=-c$. B. $\lim_{h\to 0}\frac{g(h)-g(0)}{h}=\lim_{h\to 0}\frac{f_1(h)+f_2(h)}{2h}+\frac{|f_1(h)-f_2(h)|}{2h}-(\frac{f_1(0)+f_2(0)}{2h}+\frac{|f_1(0)-f_2(0)|}{2h})=\lim_{h\to 0}\frac{f_1(h)-f_1(0)+f_2(h)-f_2(0)}{2h}+\frac{|f_1(h)-f_2(h)|}{2h}-\frac{|f_1(0)-f_2(0)|}{2h}$ How do I proceed further? C. Let $x,y \in[0,1],g(tx+(1-t)y)=\max(f_1(tx+(1-t)y),f_2(tx+(1-t)y))=\frac{f_1(tx+(1-t)y)+f_2(tx+(1-t)y)}{2}+\frac{|f_1(tx+(1-t)y)-f_2(tx+(1-t)y)|}{2}\leq \frac{f_1(tx+(1-t)y)+f_2(tx+(1-t)y)}{2}+\frac{|f_1(tx+(1-t)y)|}{2}+\frac{f_2(tx+(1-t)y)|}{2}$ How do I proceed further? I couldn't find any counterexample for $B$ and $C$. So, I tried to prove it. I am not able to complete the proof. Plese help me.","For any two functions $f_1 : [0,1] →\mathbb R$ and $f_2 : [0,1]  →\mathbb R$, deﬁne the function $g : [0,1] →\mathbb R$ as $g(x) =  \max(f_1(x),f_2(x))$ for all $x ∈ [0,1]$. A. If $f_1$ and $f_2$ are linear, then $g$ is linear B. If $f_1$ and $f_2$ are diﬀerentiable, then g is diﬀerentiable C. If $f_1$ and $f_2$ are convex, then g is convex D. None of the above My attempt:- $g(x) =  \max(f_1(x),f_2(x))=\frac{f_1(x)+f_2(x)}{2}+\frac{|f_1(x)-f_2(x)|}{2}$ A. $g(0)=\max\{f_1(0),f_2(0)\}=0$ $g(cx+y)=\max\{f_1(cx+y),f_2(cx+y)\}=\frac{f_1(cx+y)+f_2(cx+y)}{2}+\frac{|f_1(cx+y)-f_2(cx+y)|}{2}=\frac{cf_1(x)+f_1(y)+cf_2(x)+f_2(y)}{2}+\frac{|cf_1(x)+f_1(y)-(cf_2(x)+f_2(y))|}{2}=c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+\frac{|c(f_1(x)-f_2(x))+(f_1(y)-f_2(y))|}{2}$. $c(f_1(x)-f_2(x))$ and $(f_1(x)-f_2(x))$ lie in the same line. so, Equality holds in the triangular ineqality. $c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+\frac{|c(f_1(x)-f_2(x))|}{2}+\frac{|(f_1(y)-f_2(y))|}{2}\implies$ $c\frac{f_1(x)+f_2(x)}{2}+\frac{f_1(y)+f_2(y)}{2}+|c|\frac{|(f_1(x)-f_2(x))|}{2}+\frac{|(f_1(y)-f_2(y))|}{2}$ It need not be linear. Since,If $c<0$, $|c|=-c$. B. $\lim_{h\to 0}\frac{g(h)-g(0)}{h}=\lim_{h\to 0}\frac{f_1(h)+f_2(h)}{2h}+\frac{|f_1(h)-f_2(h)|}{2h}-(\frac{f_1(0)+f_2(0)}{2h}+\frac{|f_1(0)-f_2(0)|}{2h})=\lim_{h\to 0}\frac{f_1(h)-f_1(0)+f_2(h)-f_2(0)}{2h}+\frac{|f_1(h)-f_2(h)|}{2h}-\frac{|f_1(0)-f_2(0)|}{2h}$ How do I proceed further? C. Let $x,y \in[0,1],g(tx+(1-t)y)=\max(f_1(tx+(1-t)y),f_2(tx+(1-t)y))=\frac{f_1(tx+(1-t)y)+f_2(tx+(1-t)y)}{2}+\frac{|f_1(tx+(1-t)y)-f_2(tx+(1-t)y)|}{2}\leq \frac{f_1(tx+(1-t)y)+f_2(tx+(1-t)y)}{2}+\frac{|f_1(tx+(1-t)y)|}{2}+\frac{f_2(tx+(1-t)y)|}{2}$ How do I proceed further? I couldn't find any counterexample for $B$ and $C$. So, I tried to prove it. I am not able to complete the proof. Plese help me.",,['real-analysis']
12,Taking derivatives of Trig Functions,Taking derivatives of Trig Functions,,So I have the equation $\frac{d}{dx}[\cos(2x)-3\sin(x)-1]$ What I did was simplify $\cos(2x)$ to $1-2\sin^2(x)$ by applying the double angle formula. Here are my steps: $\frac{d}{dx}[1-2\sin^2(x)-3\sin(x)-1]$ Simplified = $\frac{d}{dx}[-2\sin^2(x)-3\sin(x)]$ Apply chain rule for $-2\sin^2(x)$ = $-4\sin(x)\cos(x)$ Derivative of $-3\sin(x)$ = $-3\cos(x)$ Final result = $-4\sin(x)\cos(x)-3\cos(x)$ The correct answer is $-2\sin(2x)-3\cos(x)$ . I have a feeling I shouldn't be using the double angle formula but I'm not sure why? Simplifying using algebra is allowed before differentiating/integrating as far as I know. Any help is appreciated!,So I have the equation What I did was simplify to by applying the double angle formula. Here are my steps: Simplified = Apply chain rule for = Derivative of = Final result = The correct answer is . I have a feeling I shouldn't be using the double angle formula but I'm not sure why? Simplifying using algebra is allowed before differentiating/integrating as far as I know. Any help is appreciated!,\frac{d}{dx}[\cos(2x)-3\sin(x)-1] \cos(2x) 1-2\sin^2(x) \frac{d}{dx}[1-2\sin^2(x)-3\sin(x)-1] \frac{d}{dx}[-2\sin^2(x)-3\sin(x)] -2\sin^2(x) -4\sin(x)\cos(x) -3\sin(x) -3\cos(x) -4\sin(x)\cos(x)-3\cos(x) -2\sin(2x)-3\cos(x),"['calculus', 'trigonometry', 'derivatives']"
13,Proof that $g(x) = xf(x)$ is differentiable at 0 given that f is continuous at 0,Proof that  is differentiable at 0 given that f is continuous at 0,g(x) = xf(x),"Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be continous at $0$ and let $g : \mathbb{R} \rightarrow \mathbb{R}$ be $g(x)=xf(x)$ for all $x\in \mathbb{R}$. How do I proof $g$ is differentiable at $0$? I know that the product of two differentiable functions is differentiable, and that $x$ is differentiable. So I guess I need to proof that $f(x)$ is differentiable at $0$? Continuity does however not imply differentiability, so I wouldn't know how.","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be continous at $0$ and let $g : \mathbb{R} \rightarrow \mathbb{R}$ be $g(x)=xf(x)$ for all $x\in \mathbb{R}$. How do I proof $g$ is differentiable at $0$? I know that the product of two differentiable functions is differentiable, and that $x$ is differentiable. So I guess I need to proof that $f(x)$ is differentiable at $0$? Continuity does however not imply differentiability, so I wouldn't know how.",,"['real-analysis', 'derivatives', 'continuity']"
14,Second derivative of a third degree polynomial function,Second derivative of a third degree polynomial function,,"Let $f(x) = ax^3 + bx^2 + cx +d $ be a third degree polynomial function. $$f'' = 6ax + 2b = 0 \Longrightarrow x = \frac{-b}{3a} $$ This is equal to $\frac{1}{3}$ of the sum of the roots of $f(x)$. So my question is: can we say that the root of the second derivative is equal to $\frac{1}{3}$ of the sum (of roots) for all third degree polynomial functions? If not, why?","Let $f(x) = ax^3 + bx^2 + cx +d $ be a third degree polynomial function. $$f'' = 6ax + 2b = 0 \Longrightarrow x = \frac{-b}{3a} $$ This is equal to $\frac{1}{3}$ of the sum of the roots of $f(x)$. So my question is: can we say that the root of the second derivative is equal to $\frac{1}{3}$ of the sum (of roots) for all third degree polynomial functions? If not, why?",,"['calculus', 'algebra-precalculus', 'derivatives', 'polynomials']"
15,A function not equal to its Taylor series.,A function not equal to its Taylor series.,,"I have been given this problem as a practice for an upcoming exam and I am unsure how to approach it. Give and example of a function that has derivatives of all orders, but is not equal to the sum of its Taylor series. Justify your example.","I have been given this problem as a practice for an upcoming exam and I am unsure how to approach it. Give and example of a function that has derivatives of all orders, but is not equal to the sum of its Taylor series. Justify your example.",,"['sequences-and-series', 'derivatives', 'taylor-expansion']"
16,Derivative of a random process using a Karhunen-Loève-expansion/basis expansion,Derivative of a random process using a Karhunen-Loève-expansion/basis expansion,,"Suppose a random function $X(t)$ can be written with the help of the Karhunen-Loève expansion (or some other basis expansion) by  $$X(t) = \sum_{k=1}^\infty \xi_k \psi_k(t),$$ where  $\psi_1(t), \psi_2(t),\dots$ are orthonormal eigenfunctions corresponding to the non-negative eigenvalues $\lambda_1,\lambda_2,\dots$ of the covariance operator $\Gamma$ and the random variables $\xi_k = \int_{a}^bX(s)\psi_k(s)ds$  are the uncorrelated scores with $E(\xi_k)=0$, $E(\xi_k \xi_l)=0$  for $k\neq l$ and $E(\xi_k^2)=\lambda_k$. I am interested in the derivative $X'(t)$ of $X(t)$. Suppose the eigenfunctions $\psi_k(t)$ are differentiable, then, a natural approach would be to use the basis expansion above to get $$X'(t) = \sum_{k=1}^\infty \xi_k \psi_k'(t).$$ This approach doesn't work always. For example, in the case where $X(t)$ is a Wiener process, $\psi_k(t)=\sqrt{2}\sin((k-1/2)\pi t)$ and $\lambda_k \sim 1/k^2$. However, while the derivative of $\psi_k$ exists and is obviously bounded, it is well known that the Wiener process is a.s. not differentiable. Why can't I see this fact from the derivative of the expansion? Which property am I missing? (What irritates me additionally is the fact that the variance of the right hand side, $E((\sum_{k=1}^\infty \xi_k \psi_k'(t))^2) = \sum_{k=1}^\infty \lambda_k \psi_k'(t)^2 \leq const \sum_{k=1}^\infty 1/k^2$, is bounded?!) Can someone enlighten me why this natural approach doesn't work in general and/or give me some conditions when it works?","Suppose a random function $X(t)$ can be written with the help of the Karhunen-Loève expansion (or some other basis expansion) by  $$X(t) = \sum_{k=1}^\infty \xi_k \psi_k(t),$$ where  $\psi_1(t), \psi_2(t),\dots$ are orthonormal eigenfunctions corresponding to the non-negative eigenvalues $\lambda_1,\lambda_2,\dots$ of the covariance operator $\Gamma$ and the random variables $\xi_k = \int_{a}^bX(s)\psi_k(s)ds$  are the uncorrelated scores with $E(\xi_k)=0$, $E(\xi_k \xi_l)=0$  for $k\neq l$ and $E(\xi_k^2)=\lambda_k$. I am interested in the derivative $X'(t)$ of $X(t)$. Suppose the eigenfunctions $\psi_k(t)$ are differentiable, then, a natural approach would be to use the basis expansion above to get $$X'(t) = \sum_{k=1}^\infty \xi_k \psi_k'(t).$$ This approach doesn't work always. For example, in the case where $X(t)$ is a Wiener process, $\psi_k(t)=\sqrt{2}\sin((k-1/2)\pi t)$ and $\lambda_k \sim 1/k^2$. However, while the derivative of $\psi_k$ exists and is obviously bounded, it is well known that the Wiener process is a.s. not differentiable. Why can't I see this fact from the derivative of the expansion? Which property am I missing? (What irritates me additionally is the fact that the variance of the right hand side, $E((\sum_{k=1}^\infty \xi_k \psi_k'(t))^2) = \sum_{k=1}^\infty \lambda_k \psi_k'(t)^2 \leq const \sum_{k=1}^\infty 1/k^2$, is bounded?!) Can someone enlighten me why this natural approach doesn't work in general and/or give me some conditions when it works?",,"['derivatives', 'stochastic-processes', 'brownian-motion']"
17,Finding derivative of function after finding inverse,Finding derivative of function after finding inverse,,"This is my problem: $f(x)=2x+e^x$ and $g(x)$ is $f^{-1}(x)$ and $(0,1)$ is on the graph for $f(x)$. What is $g^{'}(x)?$ My steps First we find inverse for function $\to x=2y+e^y$ Then do implicit differentiation to get: $2\frac{dy}{dx}+e^y\frac{dy}{dx}=1$ then plug in $y=1$. So then I thought the answer was $\frac{1}{2+e^y}$ But the textbook tells me that the answer was $\frac{1}{3}$.","This is my problem: $f(x)=2x+e^x$ and $g(x)$ is $f^{-1}(x)$ and $(0,1)$ is on the graph for $f(x)$. What is $g^{'}(x)?$ My steps First we find inverse for function $\to x=2y+e^y$ Then do implicit differentiation to get: $2\frac{dy}{dx}+e^y\frac{dy}{dx}=1$ then plug in $y=1$. So then I thought the answer was $\frac{1}{2+e^y}$ But the textbook tells me that the answer was $\frac{1}{3}$.",,"['calculus', 'derivatives', 'inverse', 'implicit-differentiation']"
18,Limits with different domain limit points,Limits with different domain limit points,,How do I show that $$\lim_{x \to x_0} \frac{x^{\alpha }-x_{0}^{\alpha } }{x-x_0} = \lim_{x \to 1} \frac{x_{0}^{\alpha }x^{\alpha }-x_{0}^{\alpha } }{x_0 x-x_0}$$,How do I show that $$\lim_{x \to x_0} \frac{x^{\alpha }-x_{0}^{\alpha } }{x-x_0} = \lim_{x \to 1} \frac{x_{0}^{\alpha }x^{\alpha }-x_{0}^{\alpha } }{x_0 x-x_0}$$,,"['limits', 'derivatives']"
19,Jacobian of a quaternion rotation wrt the quaternion,Jacobian of a quaternion rotation wrt the quaternion,,"I am trying to implement an extended Kalman filter which takes a vector as a sensor measurement. To model this I need to rotate the vector to the satellite reference frame using quaternion rotation. For the filter I need to find the Jacobian of my measurement function. I would like to calculate the Jacobian of some function h which performs a passive quaternion rotation, where q is my quaternion and p is some vector: $h(q) = q p q^{-1}$ I'd like to find: $H = \frac{\partial h(q)}{\partial q}$ I'm using unit quaternions in the form $q = [w, \vec{v}]$ Many thanks.","I am trying to implement an extended Kalman filter which takes a vector as a sensor measurement. To model this I need to rotate the vector to the satellite reference frame using quaternion rotation. For the filter I need to find the Jacobian of my measurement function. I would like to calculate the Jacobian of some function h which performs a passive quaternion rotation, where q is my quaternion and p is some vector: $h(q) = q p q^{-1}$ I'd like to find: $H = \frac{\partial h(q)}{\partial q}$ I'm using unit quaternions in the form $q = [w, \vec{v}]$ Many thanks.",,"['derivatives', 'quaternions']"
20,"$x_{1}$ and $x_{2}$ roots of $f$ with $f'(x_{1}) \gt 0$ and $f'(x_{2}) \gt 0$. Existence of another root between $(x_{1},x_{2})$.",and  roots of  with  and . Existence of another root between .,"x_{1} x_{2} f f'(x_{1}) \gt 0 f'(x_{2}) \gt 0 (x_{1},x_{2})","I'm struggling with this question. I tried proving by contradiction but I don't know if I'm doing it right. Any help is highly appreciated! Let $f:\mathbb{R} \to \mathbb{R}$ be smooth such as $f'(x_{1}) \gt 0$ and $f'(x_{2}) \gt 0$ for $x_{1} \lt x_{2}$ roots of $f$. Show that $f$ has at least one root in $(x_{1},x_{2})$. My atempt: By Rolle's Theorem, $\exists c \in (x_{1},x_{2}):f'(c)=0$, therefore, $f(c) \gt 0$. Suppose by contradiction that $\nexists k \in (x_{1},x_{2}):f(k)=0.$ Therefore, $\forall x \in (c,x_{2}), f'(x) \lt 0$. Since $f$ is smooth, $f'$ is continuous. So, $\exists \delta \gt 0: f'(y) \gt 0, \forall y \in (x_{2}-\delta,x_{2}+\delta)$. Let $y=x_{2}-\frac{\delta}{2}$. Since $y \in (c,x_{2}) \implies f'(y) \lt 0$. Contradiction.","I'm struggling with this question. I tried proving by contradiction but I don't know if I'm doing it right. Any help is highly appreciated! Let $f:\mathbb{R} \to \mathbb{R}$ be smooth such as $f'(x_{1}) \gt 0$ and $f'(x_{2}) \gt 0$ for $x_{1} \lt x_{2}$ roots of $f$. Show that $f$ has at least one root in $(x_{1},x_{2})$. My atempt: By Rolle's Theorem, $\exists c \in (x_{1},x_{2}):f'(c)=0$, therefore, $f(c) \gt 0$. Suppose by contradiction that $\nexists k \in (x_{1},x_{2}):f(k)=0.$ Therefore, $\forall x \in (c,x_{2}), f'(x) \lt 0$. Since $f$ is smooth, $f'$ is continuous. So, $\exists \delta \gt 0: f'(y) \gt 0, \forall y \in (x_{2}-\delta,x_{2}+\delta)$. Let $y=x_{2}-\frac{\delta}{2}$. Since $y \in (c,x_{2}) \implies f'(y) \lt 0$. Contradiction.",,"['real-analysis', 'derivatives', 'proof-verification', 'continuity']"
21,Jacobian of a quadratic vector field,Jacobian of a quadratic vector field,,"I am trying to calculate the partial derivative of a function of several variables, and it is pushing my understanding of matrix calculus. The function is the following $$f(x) = M D(x) R x$$ where $D(x)$ is the diagonal matrix with the vector $x = (x_1,\dots,x_n)$ on the main diagonal, and $M$ and $R$ are $n \times n $ real matrices. What I am looking for is the matrix of partial derivatives $$\frac{\partial f(x)}{\partial x_i}$$ I can derive this by expanding the above into non-matrix notation, but it is quite messy and I can't figure out how to simplify it. Ideally I'd like to have $\partial f(x) / \partial x_i$ in terms of $M$ and $R$. I'm hoping this is a fairly straightforward application of matrix calculus rules, but I can't seem to find any useful way of dealing with this combined function of matrix. Thanks!","I am trying to calculate the partial derivative of a function of several variables, and it is pushing my understanding of matrix calculus. The function is the following $$f(x) = M D(x) R x$$ where $D(x)$ is the diagonal matrix with the vector $x = (x_1,\dots,x_n)$ on the main diagonal, and $M$ and $R$ are $n \times n $ real matrices. What I am looking for is the matrix of partial derivatives $$\frac{\partial f(x)}{\partial x_i}$$ I can derive this by expanding the above into non-matrix notation, but it is quite messy and I can't figure out how to simplify it. Ideally I'd like to have $\partial f(x) / \partial x_i$ in terms of $M$ and $R$. I'm hoping this is a fairly straightforward application of matrix calculus rules, but I can't seem to find any useful way of dealing with this combined function of matrix. Thanks!",,"['matrices', 'derivatives', 'partial-derivative', 'matrix-calculus', 'jacobian']"
22,Show that $f(x)=x^{5/3}-kx^{4/3}+k^2x$ is increasing for $k\neq0$,Show that  is increasing for,f(x)=x^{5/3}-kx^{4/3}+k^2x k\neq0,"So to show the function is increasing/decreasing we differentiate and show it is more than zero/less than zero: We have $$f(x)=x^{5/3}-kx^{4/3}+k^2x$$ Hence, $$f'(x)=\frac{5}{3}x^{2/3}-\frac{4k}{3}x^{1/3}+k^2$$ But how do I show $$\frac{5}{3}x^{\frac{2}{3}}-\frac{4k}{3}x^{\frac{1}{3}}+k^2>0$$","So to show the function is increasing/decreasing we differentiate and show it is more than zero/less than zero: We have $$f(x)=x^{5/3}-kx^{4/3}+k^2x$$ Hence, $$f'(x)=\frac{5}{3}x^{2/3}-\frac{4k}{3}x^{1/3}+k^2$$ But how do I show $$\frac{5}{3}x^{\frac{2}{3}}-\frac{4k}{3}x^{\frac{1}{3}}+k^2>0$$",,"['calculus', 'derivatives', 'monotone-functions']"
23,The Rolle's theorem for continuous function with one-sided derivative,The Rolle's theorem for continuous function with one-sided derivative,,"Let $f:[a,b] \rightarrow \mathbb R$ be a continuous function with $f(a)=f(b)$ such that there exists in each point $x\in (a,b)$ the right derivative $f'_{+}(x)$. I want to show that there exist points $c_1,c_2 \in (a,b)$ such that $f'_{+}(c_1) \leq 0$ and $f'_{+}(c_2) \geq 0$. (It is stated  in Remark to the Rolle's theorem in Course of mathematical analysis by L. Schwarz, chap.3, $\S2$.) This is obvious if $f$ is a constant function. Let us assume that $f$ has a positive value. Then the point $c_1$ we find exactly as in the proof of the classic Rolle's theorem: let $f$ takes the maximum value in $c_1,$ then $f_{+}'(c_1)=\lim_{x\rightarrow c_1^+} \frac{f(x)-f(c_1)}{x-c_1} \leq 0$. If $\min f>0$, let $f$ takes the minimum value in $c_2$. Then    $f_{+}'(c_2)=\lim_{x\rightarrow c_1^+} \frac{f(x)-f(c_1)}{x-c_1} \geq 0$. My question is: How to find $c_2$ in general?","Let $f:[a,b] \rightarrow \mathbb R$ be a continuous function with $f(a)=f(b)$ such that there exists in each point $x\in (a,b)$ the right derivative $f'_{+}(x)$. I want to show that there exist points $c_1,c_2 \in (a,b)$ such that $f'_{+}(c_1) \leq 0$ and $f'_{+}(c_2) \geq 0$. (It is stated  in Remark to the Rolle's theorem in Course of mathematical analysis by L. Schwarz, chap.3, $\S2$.) This is obvious if $f$ is a constant function. Let us assume that $f$ has a positive value. Then the point $c_1$ we find exactly as in the proof of the classic Rolle's theorem: let $f$ takes the maximum value in $c_1,$ then $f_{+}'(c_1)=\lim_{x\rightarrow c_1^+} \frac{f(x)-f(c_1)}{x-c_1} \leq 0$. If $\min f>0$, let $f$ takes the minimum value in $c_2$. Then    $f_{+}'(c_2)=\lim_{x\rightarrow c_1^+} \frac{f(x)-f(c_1)}{x-c_1} \geq 0$. My question is: How to find $c_2$ in general?",,"['real-analysis', 'analysis', 'derivatives']"
24,Partial of Modulo operator ? (with non-integers),Partial of Modulo operator ? (with non-integers),,"I am trying to derive gradient for a special neural network, but got stuck on the Modulo Arithmetic . With usual funcitons such as $f(a,x) = a/x$ the partials would be $\frac{1}{x}$ and $-\frac{1}{x^2}$, but am struggling to find such a rule on the internet for Modulo operator. So I have: $$f(A, x) = A\%x$$   or in other words:   $$f(A, x) = mod(A, x)$$ This means the value A ""wraps around"" the value X several times, and spits out remainder. The trick in my case is A or X are not integers - they can be any real number such as 0.123 etc $$\frac{\partial f}{\partial A}f(A,x) = ?$$    $$\frac{\partial f}{\partial x}f(A,x) = ?$$ Edit: $A\%x$  Will be real number (such as 19.123 etc), can be positive, negative, or zero","I am trying to derive gradient for a special neural network, but got stuck on the Modulo Arithmetic . With usual funcitons such as $f(a,x) = a/x$ the partials would be $\frac{1}{x}$ and $-\frac{1}{x^2}$, but am struggling to find such a rule on the internet for Modulo operator. So I have: $$f(A, x) = A\%x$$   or in other words:   $$f(A, x) = mod(A, x)$$ This means the value A ""wraps around"" the value X several times, and spits out remainder. The trick in my case is A or X are not integers - they can be any real number such as 0.123 etc $$\frac{\partial f}{\partial A}f(A,x) = ?$$    $$\frac{\partial f}{\partial x}f(A,x) = ?$$ Edit: $A\%x$  Will be real number (such as 19.123 etc), can be positive, negative, or zero",,"['derivatives', 'modular-arithmetic', 'partial-derivative']"
25,Derivative of additive and multiplication functions,Derivative of additive and multiplication functions,,I'm watching a machine learning lecture from MIT https://www.youtube.com/watch?v=nFTQ7kHQWtc&t=1063s where following is given : Given $x+y$ why is $\frac{\partial f}{\partial x} = 1 $ & $\frac{\partial f}{\partial y} = 1 $ ? Is this logic correct : $\frac{\partial f}{\partial x} = x^{1-1} = 1 $ $\frac{\partial f}{\partial y} = y^{1-1} = 1 $ Given $xy$ why is $\frac{\partial f}{\partial x} = y $ & $\frac{\partial f}{\partial y} = x $ and/or how is computed ? Do I need to use product rule ( https://en.wikipedia.org/wiki/Product_rule ) ?,I'm watching a machine learning lecture from MIT https://www.youtube.com/watch?v=nFTQ7kHQWtc&t=1063s where following is given : Given $x+y$ why is $\frac{\partial f}{\partial x} = 1 $ & $\frac{\partial f}{\partial y} = 1 $ ? Is this logic correct : $\frac{\partial f}{\partial x} = x^{1-1} = 1 $ $\frac{\partial f}{\partial y} = y^{1-1} = 1 $ Given $xy$ why is $\frac{\partial f}{\partial x} = y $ & $\frac{\partial f}{\partial y} = x $ and/or how is computed ? Do I need to use product rule ( https://en.wikipedia.org/wiki/Product_rule ) ?,,"['calculus', 'derivatives', 'partial-derivative']"
26,"Chain rule problem: given $f(x)=\sqrt{4x+7}$ and $g(x)=e^{x+4}$, compute $f(g(x))'$.","Chain rule problem: given  and , compute .",f(x)=\sqrt{4x+7} g(x)=e^{x+4} f(g(x))',"Question: Given the functions $f(x)=\sqrt{4x+7}$ and $g(x)=e^{x+4}$, compute $f(g(x))'$. My Approach: I have found that found that $f(g(x))=\sqrt{4e^{x+4}+7}$. Should I now just differentiate it to get my answer or is there any simpler method to solve this problem. Any helpful suggestions or answers.","Question: Given the functions $f(x)=\sqrt{4x+7}$ and $g(x)=e^{x+4}$, compute $f(g(x))'$. My Approach: I have found that found that $f(g(x))=\sqrt{4e^{x+4}+7}$. Should I now just differentiate it to get my answer or is there any simpler method to solve this problem. Any helpful suggestions or answers.",,"['calculus', 'algebra-precalculus']"
27,Derive $\frac{1}{x}$,Derive,\frac{1}{x},I try to derive $\frac{1}{x}$. I try to do it with the $x_0$ method. I have $m=\frac {f(x_0)-f(x)}{x_0-x}$ I am using $\frac {1}{x} $ to replace $f(x_0)$ and $f(x)$. Now we have $m=\frac {\frac {1}{x_0}-\frac {1}{x}}{x_0-x} $ However I want to derive so I need to shorten $x_0-x$ in the denominator. So I need to have anothe $x_0-x$ in the numerator. I basically need to do: $\left(\frac {1}{x_0}-\frac {1}{x}\right)÷(x_0-x)$ However here I get stuck. It would be nice if someone could help me to calculate this division.,I try to derive $\frac{1}{x}$. I try to do it with the $x_0$ method. I have $m=\frac {f(x_0)-f(x)}{x_0-x}$ I am using $\frac {1}{x} $ to replace $f(x_0)$ and $f(x)$. Now we have $m=\frac {\frac {1}{x_0}-\frac {1}{x}}{x_0-x} $ However I want to derive so I need to shorten $x_0-x$ in the denominator. So I need to have anothe $x_0-x$ in the numerator. I basically need to do: $\left(\frac {1}{x_0}-\frac {1}{x}\right)÷(x_0-x)$ However here I get stuck. It would be nice if someone could help me to calculate this division.,,"['discrete-mathematics', 'derivatives', 'polynomials']"
28,"If $y=\ln(\frac{1}{3}(1+e^{-2x}))$, show that $\frac{dy}{dx}=\frac{2}{3}(e^{-y}-3)$.","If , show that .",y=\ln(\frac{1}{3}(1+e^{-2x})) \frac{dy}{dx}=\frac{2}{3}(e^{-y}-3),"I don't understand why there is only the $y$ variable in the derivative. I have differentiated it directly and I got $\frac{dy}{dx}=\frac{-2e^{-2x}}{1+e^{-2x}}$, but I don't really see how I can get to the final answer. Any guidance please?","I don't understand why there is only the $y$ variable in the derivative. I have differentiated it directly and I got $\frac{dy}{dx}=\frac{-2e^{-2x}}{1+e^{-2x}}$, but I don't really see how I can get to the final answer. Any guidance please?",,['derivatives']
29,Evaluating $\lim\limits_{x \to a} \frac{x^a - a^x}{x^a - a^a}$ without L'Hopital,Evaluating  without L'Hopital,\lim\limits_{x \to a} \frac{x^a - a^x}{x^a - a^a},"I'm trying to solve this limit without using L'Hopital rule. I already tried multiplying up and down by $x^a+a^a$, finding bounds for squeeze theorem, substitution of variables, but got nothing... $a$ is a positive real number different than $1$. Any help would be appreciated.","I'm trying to solve this limit without using L'Hopital rule. I already tried multiplying up and down by $x^a+a^a$, finding bounds for squeeze theorem, substitution of variables, but got nothing... $a$ is a positive real number different than $1$. Any help would be appreciated.",,"['calculus', 'real-analysis']"
30,Inverting a derivative: convex conjugate and envelope theorem [closed],Inverting a derivative: convex conjugate and envelope theorem [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question I need your help to go through the following mathematical steps. I am completely new to the rules used below and any help would be extremely appreciated. Consider the function $G(\boldsymbol{x}): \mathbb{R}^J \rightarrow \mathbb{R}$, where $\boldsymbol{x}\equiv (x_1,..., x_J)$. $G(\cdot )$ is known: give me any $\boldsymbol{x}$ and I can tell you the value $G(\boldsymbol{x})$. We know that $G(\cdot )$ is convex. Fix $\boldsymbol{x}=\boldsymbol{x}^*$ and let  $$ (1) \hspace{1cm}\frac{\partial G(\boldsymbol{x}^*)}{\partial x_j}=a^*_j \text{ }\forall j=1,...,J $$ where $a^*_j\in \mathbb{R}$ is a (known) parameter $\forall j=1,...,J$ and $\boldsymbol{a}^*\equiv (a_1,..., a_J)$. My objective is to recover $\boldsymbol{x}^*$ by ""inverting"" (1). The source that I found uses the following steps: A) Consider the function $\bar{G}(\boldsymbol{a}): \mathbb{R}^J\rightarrow \mathbb{R}$ prescribed by $$ \bar{G}(\boldsymbol{a})\equiv  \begin{cases}  \max_{\boldsymbol{\tilde{x}}} \Big(\sum_{j=1}^J \tilde{x}_j a_j- G(\boldsymbol{\tilde{x}}) \Big) & \text{if  $\sum_{j=1}^Ja_j\leq 1$} \\  \infty & \text{otherwise}  \end{cases} $$ This is the Legendre-Fenchel transform or convex conjugate of $G(\boldsymbol{x})$ and we know that $\bar{G}(\boldsymbol{a})$ is convex. Since $G(\cdot )$ is known, also $\bar{G}(\cdot)$ is known. B) Let $\boldsymbol{x}^{\text{opt}}(\boldsymbol{a}): \mathbb{R}^J\rightarrow \mathbb{R}^J$ be the function delivering the (unique?) solution of  $$ \max_{\boldsymbol{\tilde{x}}} \Big(\sum_{j=1}^J \tilde{x}_j a_j- G(\boldsymbol{\tilde{x}}) \Big) $$ for every $\boldsymbol{a}\in \mathbb{R}^J$. C) By the envelope theorem $$ \frac{\partial \bar{G}(\boldsymbol{a})}{\partial a_j}=x_j^{\text{opt}}(\boldsymbol{a}) \text{ }\forall j\in \{1,...,J\} $$ D) The source concludes that  $$ \frac{\partial \bar{G}(\boldsymbol{a}^*)}{\partial a_j}=x_j^* \text{ }\forall j\in \{1,...,J\} $$ E) Since $\frac{\partial \bar{G}(\boldsymbol{a}^*)}{\partial a_j}$ is known $\forall j\in \{1,...,J\}$, we have recovered $\boldsymbol{x}^*$. My doubts: (i) How do we know that only $\boldsymbol{x}^*$ satisfies (1)? (ii) Which properties of $G(\cdot)$ are sufficient to go through the steps above (convexity, strict convexity, C1,...)? And could you highlight where do we use those properties? (iii) Where do we use the convexity (strict?) of $\bar{G}(\cdot)$? Do we need other properties of $\bar{G}(\cdot)$ to apply the envelope theorem? (iv) How can we go from (C) to (D)?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question I need your help to go through the following mathematical steps. I am completely new to the rules used below and any help would be extremely appreciated. Consider the function $G(\boldsymbol{x}): \mathbb{R}^J \rightarrow \mathbb{R}$, where $\boldsymbol{x}\equiv (x_1,..., x_J)$. $G(\cdot )$ is known: give me any $\boldsymbol{x}$ and I can tell you the value $G(\boldsymbol{x})$. We know that $G(\cdot )$ is convex. Fix $\boldsymbol{x}=\boldsymbol{x}^*$ and let  $$ (1) \hspace{1cm}\frac{\partial G(\boldsymbol{x}^*)}{\partial x_j}=a^*_j \text{ }\forall j=1,...,J $$ where $a^*_j\in \mathbb{R}$ is a (known) parameter $\forall j=1,...,J$ and $\boldsymbol{a}^*\equiv (a_1,..., a_J)$. My objective is to recover $\boldsymbol{x}^*$ by ""inverting"" (1). The source that I found uses the following steps: A) Consider the function $\bar{G}(\boldsymbol{a}): \mathbb{R}^J\rightarrow \mathbb{R}$ prescribed by $$ \bar{G}(\boldsymbol{a})\equiv  \begin{cases}  \max_{\boldsymbol{\tilde{x}}} \Big(\sum_{j=1}^J \tilde{x}_j a_j- G(\boldsymbol{\tilde{x}}) \Big) & \text{if  $\sum_{j=1}^Ja_j\leq 1$} \\  \infty & \text{otherwise}  \end{cases} $$ This is the Legendre-Fenchel transform or convex conjugate of $G(\boldsymbol{x})$ and we know that $\bar{G}(\boldsymbol{a})$ is convex. Since $G(\cdot )$ is known, also $\bar{G}(\cdot)$ is known. B) Let $\boldsymbol{x}^{\text{opt}}(\boldsymbol{a}): \mathbb{R}^J\rightarrow \mathbb{R}^J$ be the function delivering the (unique?) solution of  $$ \max_{\boldsymbol{\tilde{x}}} \Big(\sum_{j=1}^J \tilde{x}_j a_j- G(\boldsymbol{\tilde{x}}) \Big) $$ for every $\boldsymbol{a}\in \mathbb{R}^J$. C) By the envelope theorem $$ \frac{\partial \bar{G}(\boldsymbol{a})}{\partial a_j}=x_j^{\text{opt}}(\boldsymbol{a}) \text{ }\forall j\in \{1,...,J\} $$ D) The source concludes that  $$ \frac{\partial \bar{G}(\boldsymbol{a}^*)}{\partial a_j}=x_j^* \text{ }\forall j\in \{1,...,J\} $$ E) Since $\frac{\partial \bar{G}(\boldsymbol{a}^*)}{\partial a_j}$ is known $\forall j\in \{1,...,J\}$, we have recovered $\boldsymbol{x}^*$. My doubts: (i) How do we know that only $\boldsymbol{x}^*$ satisfies (1)? (ii) Which properties of $G(\cdot)$ are sufficient to go through the steps above (convexity, strict convexity, C1,...)? And could you highlight where do we use those properties? (iii) Where do we use the convexity (strict?) of $\bar{G}(\cdot)$? Do we need other properties of $\bar{G}(\cdot)$ to apply the envelope theorem? (iv) How can we go from (C) to (D)?",,"['derivatives', 'optimization', 'convex-analysis', 'convex-optimization']"
31,Taking the derivative of the following integral,Taking the derivative of the following integral,,"I have the following double integral which I want to differentiate one times with respect to $1$. Or simply I want to obtain $h'(1)$. $h(x)=\int_{0}^{x}\int_{0}^{x} f(u,v)du dv$ The following answer has been provided. $\int_{0}^{1}(f(1,t)+f(t,1))dt$ I am not able to understand how this has been obtained. Although, I know how to take derivative of the integral when only one integral is provided. Please help. Thanks in advance.","I have the following double integral which I want to differentiate one times with respect to $1$. Or simply I want to obtain $h'(1)$. $h(x)=\int_{0}^{x}\int_{0}^{x} f(u,v)du dv$ The following answer has been provided. $\int_{0}^{1}(f(1,t)+f(t,1))dt$ I am not able to understand how this has been obtained. Although, I know how to take derivative of the integral when only one integral is provided. Please help. Thanks in advance.",,"['derivatives', 'definite-integrals']"
32,"$f, G : [1,\infty) \to \mathbb{R}$ continuous. Find $G'(2)$ without using FTC",continuous. Find  without using FTC,"f, G : [1,\infty) \to \mathbb{R} G'(2)","Let $f: [1,\infty) \to \mathbb{R}$ and $G: [1,\infty) \to \mathbb{R}$ be continuous. Where $G(x) = \int_1^{x^2} f(t)dt$  Find $G'(2)$ Using the definition of derivative ( without using FTC ). Making a substitution $u = x^2$ then we have $\displaystyle \lim_{h \to 0} \frac{\int_1^{u+h}f(t)dt-\int_1^u f(t)dt}{h} = \lim_{h \to 0 } \frac{\int_u^{u+h}f(t)dt}{h}$ Using MVT $\exists c \in [u,u+h]$ such that $\int_u^{u+h}f(t)dt = f(c)h$ So $\lim_{h \to 0} \frac{\int_u^{u+h} f(t)}{h} = f(c)$ And now it follows that $c \to u$ so $c \to x^2$ subbing back in $x^2$ for $u$. So it seems that $G'(2) = f(2^2) = f(4)$ I don't think this is what I want. I remember solving this problem before but can't recall what I did. I don't remember if I make a substitution to set it up similar to proving the FTC. Feels like I am proving the derivative part of the FTC rather than solving the problem as is.","Let $f: [1,\infty) \to \mathbb{R}$ and $G: [1,\infty) \to \mathbb{R}$ be continuous. Where $G(x) = \int_1^{x^2} f(t)dt$  Find $G'(2)$ Using the definition of derivative ( without using FTC ). Making a substitution $u = x^2$ then we have $\displaystyle \lim_{h \to 0} \frac{\int_1^{u+h}f(t)dt-\int_1^u f(t)dt}{h} = \lim_{h \to 0 } \frac{\int_u^{u+h}f(t)dt}{h}$ Using MVT $\exists c \in [u,u+h]$ such that $\int_u^{u+h}f(t)dt = f(c)h$ So $\lim_{h \to 0} \frac{\int_u^{u+h} f(t)}{h} = f(c)$ And now it follows that $c \to u$ so $c \to x^2$ subbing back in $x^2$ for $u$. So it seems that $G'(2) = f(2^2) = f(4)$ I don't think this is what I want. I remember solving this problem before but can't recall what I did. I don't remember if I make a substitution to set it up similar to proving the FTC. Feels like I am proving the derivative part of the FTC rather than solving the problem as is.",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
33,"Find, from the first principle, the derivative of $\sqrt {\sin (2x)}$","Find, from the first principle, the derivative of",\sqrt {\sin (2x)},"Find, from the first principle, the derivative of: $$\sqrt {\sin (2x)}$$ My Attempt: $$f(x)=\sqrt {\sin (2x)}$$ $$f(x+\Delta x)=\sqrt {\sin (2x+2\Delta x)}$$ Now, $$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$ $$=\lim_{\Delta x\to 0} \dfrac {\sqrt {\sin (2x+2\Delta x)} - \sqrt {\sin (2x)}}{\Delta x}$$","Find, from the first principle, the derivative of: $$\sqrt {\sin (2x)}$$ My Attempt: $$f(x)=\sqrt {\sin (2x)}$$ $$f(x+\Delta x)=\sqrt {\sin (2x+2\Delta x)}$$ Now, $$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$ $$=\lim_{\Delta x\to 0} \dfrac {\sqrt {\sin (2x+2\Delta x)} - \sqrt {\sin (2x)}}{\Delta x}$$",,"['calculus', 'derivatives']"
34,Is numerical approximation the only option when derivative cannot be expressed explicitly as an expression?,Is numerical approximation the only option when derivative cannot be expressed explicitly as an expression?,,My problem is the following: Are there any differentiable functions on $\Bbb R$ for which we don't know or can't find an explicit expression for the derivative? So is approximating the derivative numerically the only choice?,My problem is the following: Are there any differentiable functions on $\Bbb R$ for which we don't know or can't find an explicit expression for the derivative? So is approximating the derivative numerically the only choice?,,"['calculus', 'real-analysis', 'derivatives']"
35,Directional derivative of smooth real values functions wrt. smooth curves,Directional derivative of smooth real values functions wrt. smooth curves,,"Let $M$ be a $m$-dimensional smooth manifold, $p \in M$ a point and consider smooth curves $\alpha: (-\epsilon,\epsilon) \to M$, $\beta: (-\tilde{\epsilon},\tilde{\epsilon}) \to M$ centered at $p$, meaning $\alpha(0)=\beta(0)=p$. We define the following equivalence relation on these curves: $\alpha \sim \beta :\Longleftrightarrow $ for every chart $\varphi: U \to U'$ with $p \in U$ we have $(\varphi \circ \alpha)'(0) = (\varphi \circ \beta)'(0)$. We call an equivalence class a tangent vector . Let $\alpha$ be such a curve  through $p \in M$ and $f: V \to \mathbb{R}$ a smooth real valued function defined on an open neighborhood $V$ of $p$. Then we call $(f \circ \alpha)'(0)$ the (directional) derivative of $f$ in direction $\alpha$. My notes say that this derivative only depends on the equivalence class of $\alpha$, so this must mean that if $\alpha \sim \beta$ we have $(f \circ \alpha)'(0) = (f \circ \beta)'(0)$. But I do not understand why this holds . I know that if $\alpha \sim \beta$ we have $(\varphi \circ \alpha)'(0) = (\varphi \circ \beta)'(0)$ for any chart $\varphi: U \to U'$ defined on an open neighborhood $U$ in $p$. But I do not know how to make the transition from $\varphi$, which maps to an open subset $U' \subset \mathbb{R}^m$, to $f$ which only maps to $\mathbb{R}$. Could you please explain this problem to me? Thank you!","Let $M$ be a $m$-dimensional smooth manifold, $p \in M$ a point and consider smooth curves $\alpha: (-\epsilon,\epsilon) \to M$, $\beta: (-\tilde{\epsilon},\tilde{\epsilon}) \to M$ centered at $p$, meaning $\alpha(0)=\beta(0)=p$. We define the following equivalence relation on these curves: $\alpha \sim \beta :\Longleftrightarrow $ for every chart $\varphi: U \to U'$ with $p \in U$ we have $(\varphi \circ \alpha)'(0) = (\varphi \circ \beta)'(0)$. We call an equivalence class a tangent vector . Let $\alpha$ be such a curve  through $p \in M$ and $f: V \to \mathbb{R}$ a smooth real valued function defined on an open neighborhood $V$ of $p$. Then we call $(f \circ \alpha)'(0)$ the (directional) derivative of $f$ in direction $\alpha$. My notes say that this derivative only depends on the equivalence class of $\alpha$, so this must mean that if $\alpha \sim \beta$ we have $(f \circ \alpha)'(0) = (f \circ \beta)'(0)$. But I do not understand why this holds . I know that if $\alpha \sim \beta$ we have $(\varphi \circ \alpha)'(0) = (\varphi \circ \beta)'(0)$ for any chart $\varphi: U \to U'$ defined on an open neighborhood $U$ in $p$. But I do not know how to make the transition from $\varphi$, which maps to an open subset $U' \subset \mathbb{R}^m$, to $f$ which only maps to $\mathbb{R}$. Could you please explain this problem to me? Thank you!",,"['derivatives', 'differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds']"
36,Proof that a continuous monotone function is a.e differentiable,Proof that a continuous monotone function is a.e differentiable,,In S&S there is the proof that a monotone function is a.e differentiable. It says it is enough to prove 2 properties I understand why we need to prove both properties but I am confused as to why we can consider -F(-x) and how they get the final inequality. Isn't $D_- < D_+$ trivially since the function is monotonic? If this is true then why do we need to consider -F(-x) to get that inequality?,In S&S there is the proof that a monotone function is a.e differentiable. It says it is enough to prove 2 properties I understand why we need to prove both properties but I am confused as to why we can consider -F(-x) and how they get the final inequality. Isn't $D_- < D_+$ trivially since the function is monotonic? If this is true then why do we need to consider -F(-x) to get that inequality?,,"['derivatives', 'lebesgue-integral']"
37,Proving $y=\tan(x)$ is of the form $P_{n+1}(\tan(x))$,Proving  is of the form,y=\tan(x) P_{n+1}(\tan(x)),"So as the title states I have to prove using induction that the nth derivative of $y=\tan(x)$ is of the form $P_{n+1}(\tan(x))$, where $P_{n+1}$ is a polynomial of  degree $n+1$ So what's the intuition behind this? Usually I would like to find a general formula for the derivative of $\tan(x)$ and then following the steps of mathematical induction. Which would get me: $n = k+1$ for all n. How do I approach this problem? Thank you in advance","So as the title states I have to prove using induction that the nth derivative of $y=\tan(x)$ is of the form $P_{n+1}(\tan(x))$, where $P_{n+1}$ is a polynomial of  degree $n+1$ So what's the intuition behind this? Usually I would like to find a general formula for the derivative of $\tan(x)$ and then following the steps of mathematical induction. Which would get me: $n = k+1$ for all n. How do I approach this problem? Thank you in advance",,"['derivatives', 'polynomials', 'induction']"
38,Existence of second derivative implies symmetric second derivative,Existence of second derivative implies symmetric second derivative,,"I would like to know if the following statement is true. Let $f:(a,b)\subset\mathbb{R}  \rightarrow \mathbb{R}$, such $f'$   exits for all $x \in (a,b)$ and also exists $f''(x_0)$ for some   $a<x_0<b$. Then $ \lim_{h \to 0} \frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0)$. Obs: I'm not assuming that $f'$ is continuos in some neighborhood of $x_0$. However, I think that requirement of the existence of $f'$ in a neighborhood of $x_0$ is a necessity for the existence of $f''(x_0)$.","I would like to know if the following statement is true. Let $f:(a,b)\subset\mathbb{R}  \rightarrow \mathbb{R}$, such $f'$   exits for all $x \in (a,b)$ and also exists $f''(x_0)$ for some   $a<x_0<b$. Then $ \lim_{h \to 0} \frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0)$. Obs: I'm not assuming that $f'$ is continuos in some neighborhood of $x_0$. However, I think that requirement of the existence of $f'$ in a neighborhood of $x_0$ is a necessity for the existence of $f''(x_0)$.",,['calculus']
39,Proving that a ratio between two functions is decreasing,Proving that a ratio between two functions is decreasing,,"I have a question that seems to be easy, but I haven't been able to prove it. Any help would be appreciated: Let $f,g\rightarrow[0,1]$ such that $f(0)=0$, $f'(0)>0$, $f(x)\geq0$, $f''(x)\leq0$, $\forall x$ and $g(0)=0$, $g(x)\geq0$, $g'(x)\geq0$, $g''(x)\geq0$, $\forall x$.    Prove that $\frac{f(x)}{g(x)}$ is decreasing. I tried to use the property that say: if $\frac{a}{b}<\frac{c}{d}$ then $\frac{a+c}{b+d}<\frac{c}{d}$. I used it with the fact that $f(x)=f(0)+\lim_{n\to \infty} \sum_{i=0}^{n-1} f'(\frac{ix}{n})\frac{x}{n}$ . With this two elements I can prove that if $\lim_{x\to 0} \frac{f(0)}{g(0)}$ exists,, then the solution is true. However, I haven't been able to prove it for the general case. Any help is greatly appreciated.","I have a question that seems to be easy, but I haven't been able to prove it. Any help would be appreciated: Let $f,g\rightarrow[0,1]$ such that $f(0)=0$, $f'(0)>0$, $f(x)\geq0$, $f''(x)\leq0$, $\forall x$ and $g(0)=0$, $g(x)\geq0$, $g'(x)\geq0$, $g''(x)\geq0$, $\forall x$.    Prove that $\frac{f(x)}{g(x)}$ is decreasing. I tried to use the property that say: if $\frac{a}{b}<\frac{c}{d}$ then $\frac{a+c}{b+d}<\frac{c}{d}$. I used it with the fact that $f(x)=f(0)+\lim_{n\to \infty} \sum_{i=0}^{n-1} f'(\frac{ix}{n})\frac{x}{n}$ . With this two elements I can prove that if $\lim_{x\to 0} \frac{f(0)}{g(0)}$ exists,, then the solution is true. However, I haven't been able to prove it for the general case. Any help is greatly appreciated.",,"['limits', 'derivatives', 'convex-analysis']"
40,About the derivative definition for real valued functions,About the derivative definition for real valued functions,,"I have some questions about the definitions of the derivatives of real valued functions which are mostly to make sure I got things correctly. Are the following statements correct? The analogous of partial derivatives of functions $f:\Bbb R^n \to \Bbb R$ for functions $g:\Bbb R \to \Bbb R$ is derivatives, correct? The analogous of the derivative of functions such as $f$ for functions such as $g$ can also be defined and it is something different than the usual derivative of $g$, correct? (If $g(x)=(x-2)^3+3$ then $g'(x)=3(x-2)^2$ which is not a linear map but $Dg=$??) If the above are correct then we call these linear mappings derivatives because they simply have very similar properties to the derivatives of functions defined in $\Bbb R$. I hope an answer alleviates this confusion. Thanks in advance","I have some questions about the definitions of the derivatives of real valued functions which are mostly to make sure I got things correctly. Are the following statements correct? The analogous of partial derivatives of functions $f:\Bbb R^n \to \Bbb R$ for functions $g:\Bbb R \to \Bbb R$ is derivatives, correct? The analogous of the derivative of functions such as $f$ for functions such as $g$ can also be defined and it is something different than the usual derivative of $g$, correct? (If $g(x)=(x-2)^3+3$ then $g'(x)=3(x-2)^2$ which is not a linear map but $Dg=$??) If the above are correct then we call these linear mappings derivatives because they simply have very similar properties to the derivatives of functions defined in $\Bbb R$. I hope an answer alleviates this confusion. Thanks in advance",,"['calculus', 'real-analysis', 'derivatives', 'partial-derivative']"
41,Show that if $''(x_0)$ then,Show that if  then,''(x_0),"Prove that $f$ has a simple zero at $x_0$ if and only if: $$f(x) =g(x)(x-x_0),$$ where $g$ is continuous at $x_0$ and differentiable on a deleted neighborhood of $x_0$, and $g(x_0)\neq{0}$ I know that a function has a simple zero at $x_0$ if $f$ is differentiable at $x_0$ and $f(x_0) = 0$, while $f'(x_0)\neq{0}$ But I am unsure how to start on the proof. Any help is appreciated.","Prove that $f$ has a simple zero at $x_0$ if and only if: $$f(x) =g(x)(x-x_0),$$ where $g$ is continuous at $x_0$ and differentiable on a deleted neighborhood of $x_0$, and $g(x_0)\neq{0}$ I know that a function has a simple zero at $x_0$ if $f$ is differentiable at $x_0$ and $f(x_0) = 0$, while $f'(x_0)\neq{0}$ But I am unsure how to start on the proof. Any help is appreciated.",,"['real-analysis', 'derivatives', 'roots']"
42,"Given $f(x)=x^n+px + q, \ n\in{\mathbb{N}}$",Given,"f(x)=x^n+px + q, \ n\in{\mathbb{N}}","Problem: Given $f(x)=x^n+px+q, \ n\in{\mathbb{N}}, \ x\in\mathbb{R},$ determine (as a function of $n$) the maximum number of distinctive real roots that $f$ can have for $p,q\in\mathbb{R}.$ I need help to understand this problem. Below follows a solution from my professor, but I don't really understand. Solution: $n=1:$ One real root for $p\neq-1$ and no real root if $p=-1$. $n\geq2:$ In this case, $f'(x)$ must have a root between the two roots for $f(x)$. The derivative $f'(x)=nx^{n-1}+p$ has one root for even $n$ and max two roots for odd $n\Rightarrow f$ has max two roots for even $n$ and max three for odd $n$. Can someone explain to me what is going on here? I understand the first $n=1$ case but for the second case I have the following questions: If $n=2$, then I understand why $f'(x)$ must have one root between the two roots of $f(x)$. This is because in order for $f(x)$ to have two roots, the derivative must change sign so that the function changes direction and intercepts the $x$-axis again. But she never specifies if it's for $n>2$ or $n=2.$ Why is this statement true: ""the derivative has one root for even $n$ and max two roots for odd $n$""? Any help to understand this is welcome. If there are simpler methods to prove this, only using differentiation, please let me know.","Problem: Given $f(x)=x^n+px+q, \ n\in{\mathbb{N}}, \ x\in\mathbb{R},$ determine (as a function of $n$) the maximum number of distinctive real roots that $f$ can have for $p,q\in\mathbb{R}.$ I need help to understand this problem. Below follows a solution from my professor, but I don't really understand. Solution: $n=1:$ One real root for $p\neq-1$ and no real root if $p=-1$. $n\geq2:$ In this case, $f'(x)$ must have a root between the two roots for $f(x)$. The derivative $f'(x)=nx^{n-1}+p$ has one root for even $n$ and max two roots for odd $n\Rightarrow f$ has max two roots for even $n$ and max three for odd $n$. Can someone explain to me what is going on here? I understand the first $n=1$ case but for the second case I have the following questions: If $n=2$, then I understand why $f'(x)$ must have one root between the two roots of $f(x)$. This is because in order for $f(x)$ to have two roots, the derivative must change sign so that the function changes direction and intercepts the $x$-axis again. But she never specifies if it's for $n>2$ or $n=2.$ Why is this statement true: ""the derivative has one root for even $n$ and max two roots for odd $n$""? Any help to understand this is welcome. If there are simpler methods to prove this, only using differentiation, please let me know.",,"['calculus', 'derivatives', 'polynomials']"
43,Does a differentiable clustering algorithm exist?,Does a differentiable clustering algorithm exist?,,"I have some vectors $x_0, x_1, x_2, ... x_n$ for a fixed $n$ (the vector dimension is about 2-20). I know there are $m$ clusters for these $n$ vectors (where $1\leq m \leq n$). For every input vector $x_i$ I should get a probability for each cluster (e.g. generated by $\mathrm{softmax}$). This algorithm should contain as few parameters as possible. I tried to implement a differentiable k-means, but the result was not that great, especially it seems to be hard to select good initial cluster centers. Maybe there already exists such an algorithm or it is easy to port one? Thank you","I have some vectors $x_0, x_1, x_2, ... x_n$ for a fixed $n$ (the vector dimension is about 2-20). I know there are $m$ clusters for these $n$ vectors (where $1\leq m \leq n$). For every input vector $x_i$ I should get a probability for each cluster (e.g. generated by $\mathrm{softmax}$). This algorithm should contain as few parameters as possible. I tried to implement a differentiable k-means, but the result was not that great, especially it seems to be hard to select good initial cluster centers. Maybe there already exists such an algorithm or it is easy to port one? Thank you",,"['derivatives', 'clustering']"
44,Complex derivative from definition,Complex derivative from definition,,"Working directly from the definition of a derivative, show that $f(z)=3ze^{iz}+|z|^2+4$ is differentiable at the origin an determine $f'(0)$. I got: $$f'(0)=\lim_{z \to 0}\frac{3ze^{iz}+|z|^2+4-4}{z}=\lim_{z \to 0}\frac{3ze^{iz}+|z|^2}{z}\frac{\bar{z}}{\bar{z}}$$ but at this point I don't know what do do with that $e^{iz}$, is that $e^{-y}(\cos x + i \sin x)$? (If $z = x+iy$) Also, $|z|$ is not differentiable anywhere, so, how is this even possible!?","Working directly from the definition of a derivative, show that $f(z)=3ze^{iz}+|z|^2+4$ is differentiable at the origin an determine $f'(0)$. I got: $$f'(0)=\lim_{z \to 0}\frac{3ze^{iz}+|z|^2+4-4}{z}=\lim_{z \to 0}\frac{3ze^{iz}+|z|^2}{z}\frac{\bar{z}}{\bar{z}}$$ but at this point I don't know what do do with that $e^{iz}$, is that $e^{-y}(\cos x + i \sin x)$? (If $z = x+iy$) Also, $|z|$ is not differentiable anywhere, so, how is this even possible!?",,"['complex-analysis', 'derivatives']"
45,"Prove that if $ u(x,t) = f(x-ct) + g(x + ct)$ then $u_{tt} = c^2 u_{xx}$",Prove that if  then," u(x,t) = f(x-ct) + g(x + ct) u_{tt} = c^2 u_{xx}","Prove that if $$ u(x,t) = f(x-ct) + g(x + ct)$$ then $$u_{tt} = c^2 u_{xx}$$ My approach: Let $f(x-ct) = f(A)$ and $g(x+ct)=g(B)$ $$ u_t = \frac{\partial u}{\partial t} = \frac{\partial f}{\partial A }\frac{\partial A}{\partial t } + \frac{\partial f }{\partial B } \frac{\partial B }{\partial t } $$ $$ u_{tt} = \frac{\partial }{\partial t} \left(  \frac{\partial u}{\partial t }  \right) = \frac{\partial^2 f }{\partial t \partial A } \cdot \frac{\partial    A}{\partial t } + \frac{\partial f }{\partial A }  \cdot \frac{\partial^2 A }{\partial t^2 }   +  \frac{\partial^2 f}{\partial t \partial B } \cdot \frac{\partial B }{\partial t} + \frac{\partial f }{\partial B } \cdot \frac{\partial^2 B }{\partial B \partial t } $$ Evaluating some partials $$\frac{\partial A }{\partial t } = -c ,  \frac{\partial^2 A }{\partial t^2} =0, \frac{\partial B }{\partial t } =c , \frac{\partial^2 B }{\partial t^2 } =0$$ hence $$u_{tt} =\frac{\partial^2 f }{\partial t \partial A } \cdot -c + \frac{\partial^2 f }{\partial t \partial B }\cdot c$$ Using similar approach, $$ u_{xx} =\frac{\partial^2 f }{\partial x \partial A } + \frac{\partial^2 f }{\partial x \partial B }$$ Is what I did correct? I am stuck at this level. What needs to be done to complete the proof?","Prove that if $$ u(x,t) = f(x-ct) + g(x + ct)$$ then $$u_{tt} = c^2 u_{xx}$$ My approach: Let $f(x-ct) = f(A)$ and $g(x+ct)=g(B)$ $$ u_t = \frac{\partial u}{\partial t} = \frac{\partial f}{\partial A }\frac{\partial A}{\partial t } + \frac{\partial f }{\partial B } \frac{\partial B }{\partial t } $$ $$ u_{tt} = \frac{\partial }{\partial t} \left(  \frac{\partial u}{\partial t }  \right) = \frac{\partial^2 f }{\partial t \partial A } \cdot \frac{\partial    A}{\partial t } + \frac{\partial f }{\partial A }  \cdot \frac{\partial^2 A }{\partial t^2 }   +  \frac{\partial^2 f}{\partial t \partial B } \cdot \frac{\partial B }{\partial t} + \frac{\partial f }{\partial B } \cdot \frac{\partial^2 B }{\partial B \partial t } $$ Evaluating some partials $$\frac{\partial A }{\partial t } = -c ,  \frac{\partial^2 A }{\partial t^2} =0, \frac{\partial B }{\partial t } =c , \frac{\partial^2 B }{\partial t^2 } =0$$ hence $$u_{tt} =\frac{\partial^2 f }{\partial t \partial A } \cdot -c + \frac{\partial^2 f }{\partial t \partial B }\cdot c$$ Using similar approach, $$ u_{xx} =\frac{\partial^2 f }{\partial x \partial A } + \frac{\partial^2 f }{\partial x \partial B }$$ Is what I did correct? I am stuck at this level. What needs to be done to complete the proof?",,"['real-analysis', 'derivatives', 'partial-derivative']"
46,How do I show that $V_n$ is as required?,How do I show that  is as required?,V_n,"Question : If $V_n=\frac{d^n}{dx^n}(x^n \log x)$, show that $V_n=nV_{n-1}+(n-1)!$ Hence show that $$V_n=n! (\log x + 1 + \frac{1}{2}+\frac{1}{3}+\dot{} \dot{} \dot{}+\frac{1}{n})$$ What I have managed to do so far: I have found that $V_{n+1}=\frac{n!}{x}$ but I cannot use it further to answer the questions. PS : Here $V_n$ is $n^{th}$ derivative of $V$ Can someone kindly guide me on how to pursue further in this problem?","Question : If $V_n=\frac{d^n}{dx^n}(x^n \log x)$, show that $V_n=nV_{n-1}+(n-1)!$ Hence show that $$V_n=n! (\log x + 1 + \frac{1}{2}+\frac{1}{3}+\dot{} \dot{} \dot{}+\frac{1}{n})$$ What I have managed to do so far: I have found that $V_{n+1}=\frac{n!}{x}$ but I cannot use it further to answer the questions. PS : Here $V_n$ is $n^{th}$ derivative of $V$ Can someone kindly guide me on how to pursue further in this problem?",,['derivatives']
47,How to prove $\frac{\partial}{\partial{W}} \operatorname{trace}((Y-XW)(Y-XW)^T)=2X^T(XW-Y)$?,How to prove ?,\frac{\partial}{\partial{W}} \operatorname{trace}((Y-XW)(Y-XW)^T)=2X^T(XW-Y),$\newcommand{\tr}{\operatorname{tr}}$ I want to prove the following expression  with simple matrix operations. \begin{align} & \frac{\partial}{\partial{W}} \tr((Y-XW)(Y-XW)^T)=2X^T(XW-Y) \\[10pt] = {} & \frac{\partial}{\partial{W}} \tr((Y-XW)(Y-XW)^T) \\[10pt] = {} & \frac{\partial}{\partial{W}} \tr(YY^T - YW^TX^T - XWY^T + XW(XW)^T) \\[10pt] = {} & \frac{\partial}{\partial{W}} \tr( -YW^TX^T - XWY^T + XW(XW)^T) \\[10pt] = {} & -2X^TY  +\frac{\partial}{\partial{W}} \tr(XW(XW)^T) \end{align} Now I need to calculate  $ \dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)$. Can anyone help to calculate this derivate? Thanks. ========= based on the suggestion to write the expression in Einstein notation I found: $\begin{align} &\dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)=\\[10pt] = {} &   \dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)\\[10pt] = {} &  \dfrac{\partial}{\partial{W}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W^T_{kl}X^T_{li}\\[10pt] = {} &  \dfrac{\partial}{\partial{W_{jk}}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W^T_{kl}X^T_{li} + \dfrac{\partial}{\partial{W_{lk}}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W_{lk}X^T_{li} \\[10pt] = {} & \sum_i\sum_k\sum_l(W^TX^T)_{ki} X_{ij}   +  \sum_i\sum_j\sum_k X_{ij}W_{jk} X^T_{li}  \\[10pt] = {} & \sum_i\sum_k\sum_l(W^TX^T)_{ki} X_{ij}   +  \sum_i\sum_j\sum_kX^T_{li} X_{ij}W_{jk} \\[10pt] ={} &2X^TXW   \end{align}$ Please let me know if something is wrong with this. Thanks.,$\newcommand{\tr}{\operatorname{tr}}$ I want to prove the following expression  with simple matrix operations. \begin{align} & \frac{\partial}{\partial{W}} \tr((Y-XW)(Y-XW)^T)=2X^T(XW-Y) \\[10pt] = {} & \frac{\partial}{\partial{W}} \tr((Y-XW)(Y-XW)^T) \\[10pt] = {} & \frac{\partial}{\partial{W}} \tr(YY^T - YW^TX^T - XWY^T + XW(XW)^T) \\[10pt] = {} & \frac{\partial}{\partial{W}} \tr( -YW^TX^T - XWY^T + XW(XW)^T) \\[10pt] = {} & -2X^TY  +\frac{\partial}{\partial{W}} \tr(XW(XW)^T) \end{align} Now I need to calculate  $ \dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)$. Can anyone help to calculate this derivate? Thanks. ========= based on the suggestion to write the expression in Einstein notation I found: $\begin{align} &\dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)=\\[10pt] = {} &   \dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)\\[10pt] = {} &  \dfrac{\partial}{\partial{W}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W^T_{kl}X^T_{li}\\[10pt] = {} &  \dfrac{\partial}{\partial{W_{jk}}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W^T_{kl}X^T_{li} + \dfrac{\partial}{\partial{W_{lk}}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W_{lk}X^T_{li} \\[10pt] = {} & \sum_i\sum_k\sum_l(W^TX^T)_{ki} X_{ij}   +  \sum_i\sum_j\sum_k X_{ij}W_{jk} X^T_{li}  \\[10pt] = {} & \sum_i\sum_k\sum_l(W^TX^T)_{ki} X_{ij}   +  \sum_i\sum_j\sum_kX^T_{li} X_{ij}W_{jk} \\[10pt] ={} &2X^TXW   \end{align}$ Please let me know if something is wrong with this. Thanks.,,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'gradient-descent']"
48,Differentiation of $\dfrac{d(\sin x)^{\ln x}}{dx}$,Differentiation of,\dfrac{d(\sin x)^{\ln x}}{dx},What is the value of $\dfrac{d(\sin x)^{\ln x}}{dx}$? My attempt: I applied chain rule in the following maner: $$\frac{d\sin(x)^{\ln x}}{d\ln x}\cdot \frac{d(\ln x)}{dx} = \ln x\sin x^{\ln x -1} \cdot \frac{1}{x}$$ However my answer doesn't match with the one given in the key. Where have I gone wrong?,What is the value of $\dfrac{d(\sin x)^{\ln x}}{dx}$? My attempt: I applied chain rule in the following maner: $$\frac{d\sin(x)^{\ln x}}{d\ln x}\cdot \frac{d(\ln x)}{dx} = \ln x\sin x^{\ln x -1} \cdot \frac{1}{x}$$ However my answer doesn't match with the one given in the key. Where have I gone wrong?,,['algebra-precalculus']
49,Derivatives involving inner product and Hadamard product,Derivatives involving inner product and Hadamard product,,"Let $x,y,z\in\mathbb{R}^n$. I am trying to compute $$ \frac{\partial}{\partial x} (x\circ y)^Tz\\ \frac{\partial}{\partial x} (x\circ y)^T(x \circ y) $$ where $x\circ y$ is the Hadamard product of $x$ and $y$, but it is throwing me for a loop. Can someone show me how to proceed with these derivatives? Based on this answer , it appears that I can write $f(x,y)=(x\circ y)^T(x\circ y) = (x\circ y)^TI(x\circ y)$ and thus $$ \frac{\partial f}{\partial x} = y\circ (I^T+I)(x\circ y) $$ but I am confused about the first part, $y\circ(I^T+I)$. The dimensions do not seem to match up properly since $y\in\mathbb{R}^n$ and $I\in\mathbb{R}^{n\times n}$. Context: I would like to compute the gradient of the following: $$ \begin{split} ||x-\alpha\circ y||_2^2 &= (x-\alpha\circ y)^T(x-\alpha\circ y)\\ & = x^Tx - x^T(\alpha\circ y) - (\alpha\circ y)^Tx + (\alpha\circ y)^T(\alpha\circ y) \end{split} $$ with respect to $\alpha$ as part of the derivation of a gradient descent update. If there is  a simpler way to compute the derivative of this 2-norm, please share; however, I'd still like to know how to compute the individual derivatives as well!","Let $x,y,z\in\mathbb{R}^n$. I am trying to compute $$ \frac{\partial}{\partial x} (x\circ y)^Tz\\ \frac{\partial}{\partial x} (x\circ y)^T(x \circ y) $$ where $x\circ y$ is the Hadamard product of $x$ and $y$, but it is throwing me for a loop. Can someone show me how to proceed with these derivatives? Based on this answer , it appears that I can write $f(x,y)=(x\circ y)^T(x\circ y) = (x\circ y)^TI(x\circ y)$ and thus $$ \frac{\partial f}{\partial x} = y\circ (I^T+I)(x\circ y) $$ but I am confused about the first part, $y\circ(I^T+I)$. The dimensions do not seem to match up properly since $y\in\mathbb{R}^n$ and $I\in\mathbb{R}^{n\times n}$. Context: I would like to compute the gradient of the following: $$ \begin{split} ||x-\alpha\circ y||_2^2 &= (x-\alpha\circ y)^T(x-\alpha\circ y)\\ & = x^Tx - x^T(\alpha\circ y) - (\alpha\circ y)^Tx + (\alpha\circ y)^T(\alpha\circ y) \end{split} $$ with respect to $\alpha$ as part of the derivation of a gradient descent update. If there is  a simpler way to compute the derivative of this 2-norm, please share; however, I'd still like to know how to compute the individual derivatives as well!",,"['derivatives', 'vector-analysis', 'gradient-descent', 'hadamard-product']"
50,Why doesn't derivative show the complex maxima minima?,Why doesn't derivative show the complex maxima minima?,,"Let's find the global minimum of $y=x^2$ First we calculate its first derivative, and make it equal to zero. $y'=2x=0$ ; $x=0$ Then we check its second derivative, if it's positive then it is minimum. $y''=2$ $(0,0)$ turned out to be global minimum. Why doesn't derivative show the complex minimum? $(i,-1)$ is below it and that derivation should show the minima are $(∞i,-∞)$ and $(-∞i,-∞)$ . How can I calculate complex minima of other functions?","Let's find the global minimum of First we calculate its first derivative, and make it equal to zero. ; Then we check its second derivative, if it's positive then it is minimum. turned out to be global minimum. Why doesn't derivative show the complex minimum? is below it and that derivation should show the minima are and . How can I calculate complex minima of other functions?","y=x^2 y'=2x=0 x=0 y''=2 (0,0) (i,-1) (∞i,-∞) (-∞i,-∞)",['derivatives']
51,Prove that a function $g$ is differentiable in 0,Prove that a function  is differentiable in 0,g,"Let $f : \mathbb{R} \to \mathbb{R}$ continu in 0 and let $g : \mathbb{R} \to \mathbb{R}$ defined by $g(x) = x f(x)$, for $x \in \mathbb{R}$. Prove that $g$ is differentiable at 0 and determine $g'(0)$. This is an assignment for my class, but I'm having trouble finding the right argumentation. My thoughts were the following : If I can prove that the function f is differentiable at 0, and I can prove that a function $h : \mathbb{R} \to \mathbb{R}$ defined by $h(x) = x$ is differentiable at 0, then also the function $g$ is differentiable at 0? Well, I know how to prove that the function $h$ is differentiable in 0 and what the derivative is. But how do I prove that the function $f$ is differentiable?","Let $f : \mathbb{R} \to \mathbb{R}$ continu in 0 and let $g : \mathbb{R} \to \mathbb{R}$ defined by $g(x) = x f(x)$, for $x \in \mathbb{R}$. Prove that $g$ is differentiable at 0 and determine $g'(0)$. This is an assignment for my class, but I'm having trouble finding the right argumentation. My thoughts were the following : If I can prove that the function f is differentiable at 0, and I can prove that a function $h : \mathbb{R} \to \mathbb{R}$ defined by $h(x) = x$ is differentiable at 0, then also the function $g$ is differentiable at 0? Well, I know how to prove that the function $h$ is differentiable in 0 and what the derivative is. But how do I prove that the function $f$ is differentiable?",,"['real-analysis', 'derivatives']"
52,"$f:\Bbb R\to\Bbb R$ continuous at 0; prove $g:\Bbb R\to\Bbb R,g(x)=xf(x)$ differentiable at 0 and find $g'(0)$",continuous at 0; prove  differentiable at 0 and find,"f:\Bbb R\to\Bbb R g:\Bbb R\to\Bbb R,g(x)=xf(x) g'(0)","Suppose that $f \colon ℝ → ℝ$ is continuous at $x = 0$. Prove that the function $g \colon ℝ → ℝ$ given by $g(x) = xf(x)$ is differentiable at $x=0$, and find $g'(0)$. I'm honestly confused about what I can do, my teacher said using the definition of derivative might help, however, I can't see how. Here's my attempt anyways: Since $f(x)$ is continuous at $x=0$, it must be differentiable at $x=0$. Therefore a scalar multiple of it is also differentiable at $x=0$, so $g(x)$ is differentiable at $x=0$. Second part: $g(x) = xf(x)+f'(x)$. Not sure what to do from here now.","Suppose that $f \colon ℝ → ℝ$ is continuous at $x = 0$. Prove that the function $g \colon ℝ → ℝ$ given by $g(x) = xf(x)$ is differentiable at $x=0$, and find $g'(0)$. I'm honestly confused about what I can do, my teacher said using the definition of derivative might help, however, I can't see how. Here's my attempt anyways: Since $f(x)$ is continuous at $x=0$, it must be differentiable at $x=0$. Therefore a scalar multiple of it is also differentiable at $x=0$, so $g(x)$ is differentiable at $x=0$. Second part: $g(x) = xf(x)+f'(x)$. Not sure what to do from here now.",,"['calculus', 'derivatives', 'continuity']"
53,Questions relating differentiability to continuity and integrability,Questions relating differentiability to continuity and integrability,,"These questions are from Stephen Abbott's ""Understanding Analysis"", 7.5.2, following a brief explanation of the Fundamental Theorem of Calculus. I have ideas but I feel like I need help still. Decide whether each statement is true or false, providing a short justification for each conclusion. (a). If $h'=g$ on $[a,b]$, then $g$ is continuous on $[a,b]$ If I understand the question correctly, it's asking that if a function is differentiable on $[a,b]$ then the derivative has to be continuous on that interval. I have seen a counter example to this from previous questions, which is $f(x)=\{x^2 sin(1/x),  x \neq 0$,and $0, x=0 \}$ (b) If $g$ is continuous on $[a,b]$, then $g=h'$ for some $h$ on $[a,b]$ Is this asking if a function is continuous does that mean it's differentiable? Then the answer is no because $f(x)=|x|$ is not differentiable at $x=0$. But I'm not sure if I'm even reading the problem correctly. (c) If $H(x) = \int_{a}^{x}h$ is differentiable at $c \in [a,b], $ then $h$ is continuous at $c$. Intuition tells me yes. I can integrate a step function, for example. The result is continuous but not differentiable. Any input or hints would be appreciated.","These questions are from Stephen Abbott's ""Understanding Analysis"", 7.5.2, following a brief explanation of the Fundamental Theorem of Calculus. I have ideas but I feel like I need help still. Decide whether each statement is true or false, providing a short justification for each conclusion. (a). If $h'=g$ on $[a,b]$, then $g$ is continuous on $[a,b]$ If I understand the question correctly, it's asking that if a function is differentiable on $[a,b]$ then the derivative has to be continuous on that interval. I have seen a counter example to this from previous questions, which is $f(x)=\{x^2 sin(1/x),  x \neq 0$,and $0, x=0 \}$ (b) If $g$ is continuous on $[a,b]$, then $g=h'$ for some $h$ on $[a,b]$ Is this asking if a function is continuous does that mean it's differentiable? Then the answer is no because $f(x)=|x|$ is not differentiable at $x=0$. But I'm not sure if I'm even reading the problem correctly. (c) If $H(x) = \int_{a}^{x}h$ is differentiable at $c \in [a,b], $ then $h$ is continuous at $c$. Intuition tells me yes. I can integrate a step function, for example. The result is continuous but not differentiable. Any input or hints would be appreciated.",,"['real-analysis', 'integration', 'derivatives', 'continuity']"
54,How to prove that $\lim_{x\to\infty}f'(x) = 0$ is equivalent to $\lim_{x\to\infty}f(x)/x = 0$?,How to prove that  is equivalent to ?,\lim_{x\to\infty}f'(x) = 0 \lim_{x\to\infty}f(x)/x = 0,"Suppose we have a differentiable real-valued function $f(x)$. The task is to prove, that if $\lim_{x\to\infty}f'(x) = 0$ then $\lim_{x\to\infty}f(x)/x = 0$, and, conversely, if $\lim_{x\to\infty}f(x)/x = 0$ than if the $\lim_{x\to\infty}f'(x)$ exists, than it's equal to zero. I've tried using Lagrange theorem several times, but it didn't help. Could you please suggest a proof, or maybe help me to show that this fact is not true (though it seems to be true)?","Suppose we have a differentiable real-valued function $f(x)$. The task is to prove, that if $\lim_{x\to\infty}f'(x) = 0$ then $\lim_{x\to\infty}f(x)/x = 0$, and, conversely, if $\lim_{x\to\infty}f(x)/x = 0$ than if the $\lim_{x\to\infty}f'(x)$ exists, than it's equal to zero. I've tried using Lagrange theorem several times, but it didn't help. Could you please suggest a proof, or maybe help me to show that this fact is not true (though it seems to be true)?",,"['real-analysis', 'limits', 'derivatives']"
55,Concluding divergence based on first and second derivatives,Concluding divergence based on first and second derivatives,,"I've just started a calc III course and we're currently reviewing calc II. While doing some problems about infinite sequences, I had the thought that if $\{a_n\} = f(n)$ for $n = 1, 2, 3, ...$ and if, for all values of $x$ greater than some $c$, $f$ is continuous with $f '(x) > 0$ and $f ''(x) > 0$ (or $f '(x) < 0$ and $f ''(x) < 0$), $\{a_n\}$ must be divergent. I saw my prof at office hours today and he agreed with the intuition. He said that if I could find or come up with a proof for this, I could use it on our quiz tomorrow to prove that sequences diverge. I haven't found any sort of proof and am having trouble figuring it out on my own (or maybe my intuition was wrong), so would anyone be able to confirm/deny this and help come up with a proof? Thanks in advance, Nicholas","I've just started a calc III course and we're currently reviewing calc II. While doing some problems about infinite sequences, I had the thought that if $\{a_n\} = f(n)$ for $n = 1, 2, 3, ...$ and if, for all values of $x$ greater than some $c$, $f$ is continuous with $f '(x) > 0$ and $f ''(x) > 0$ (or $f '(x) < 0$ and $f ''(x) < 0$), $\{a_n\}$ must be divergent. I saw my prof at office hours today and he agreed with the intuition. He said that if I could find or come up with a proof for this, I could use it on our quiz tomorrow to prove that sequences diverge. I haven't found any sort of proof and am having trouble figuring it out on my own (or maybe my intuition was wrong), so would anyone be able to confirm/deny this and help come up with a proof? Thanks in advance, Nicholas",,"['calculus', 'sequences-and-series', 'derivatives', 'convergence-divergence', 'proof-writing']"
56,Leibniz's rule and proving that $x(t)$ satisfies integral,Leibniz's rule and proving that  satisfies integral,x(t),"The question is: Show that if $x(t)$ satisfies the integral equation $$x(t) = a + bt + \int_{0}^{t}(t-s)f(x(s))ds $$ then $x(t)$ is a solution to the initial value problem $$x''(t) = f(x(t))$$ for $t>0$, with $x(0) = a, x'(0) = b.$ This is what I've done and I'm not sure if it's even remotely correct. $$\frac{d}{dt}x(t) = \frac{d}{dt}\left(a + bt + \int_{0}^{t}(t-s)f(x(s))ds\right) \\ x'(t) = b + \frac{d}{dt} \left(\int_{0}^{t}(t-s)f(x(s))ds\right)  \\ x'(t) = b + \int_{0}^{t} \frac{\partial}{\partial t}(t-s)f(x(s))ds \\ x'(t) = b + \int_{0}^{t} f(x(s)) + \frac{\partial f(x(s))}{\partial t}(t-s)\frac{\partial x(s)}{\partial t} ds \qquad \text{by chain rule and product rule} $$ Then I take the derivative again, and I don't seem to get the required result. Can someone tell me if I did something wrong and guide me in the correct way?","The question is: Show that if $x(t)$ satisfies the integral equation $$x(t) = a + bt + \int_{0}^{t}(t-s)f(x(s))ds $$ then $x(t)$ is a solution to the initial value problem $$x''(t) = f(x(t))$$ for $t>0$, with $x(0) = a, x'(0) = b.$ This is what I've done and I'm not sure if it's even remotely correct. $$\frac{d}{dt}x(t) = \frac{d}{dt}\left(a + bt + \int_{0}^{t}(t-s)f(x(s))ds\right) \\ x'(t) = b + \frac{d}{dt} \left(\int_{0}^{t}(t-s)f(x(s))ds\right)  \\ x'(t) = b + \int_{0}^{t} \frac{\partial}{\partial t}(t-s)f(x(s))ds \\ x'(t) = b + \int_{0}^{t} f(x(s)) + \frac{\partial f(x(s))}{\partial t}(t-s)\frac{\partial x(s)}{\partial t} ds \qquad \text{by chain rule and product rule} $$ Then I take the derivative again, and I don't seem to get the required result. Can someone tell me if I did something wrong and guide me in the correct way?",,"['integration', 'derivatives']"
57,"Let $f$ be a differentiable function with $|f'(x)|\leq1$ and $f(-3)=-3, f(3)=3$. Then find $f(0)$. [duplicate]",Let  be a differentiable function with  and . Then find . [duplicate],"f |f'(x)|\leq1 f(-3)=-3, f(3)=3 f(0)","This question already has answers here : Find a function value given 2 points (2 answers) Closed 7 years ago . Let $f$ be a differentiable function with $|f'(x)|\leq1$ and $f(-3)=-3, f(3)=3$. Then find $f(0)$. I think this is mean value theorem problem. But I can't solve... help me please.","This question already has answers here : Find a function value given 2 points (2 answers) Closed 7 years ago . Let $f$ be a differentiable function with $|f'(x)|\leq1$ and $f(-3)=-3, f(3)=3$. Then find $f(0)$. I think this is mean value theorem problem. But I can't solve... help me please.",,"['calculus', 'derivatives']"
58,Calculate $f^\prime(0)$ of $f(x) = \prod_{n=0}^{100} (x-n)$.,Calculate  of .,f^\prime(0) f(x) = \prod_{n=0}^{100} (x-n),"How can one calculate $f^\prime(0)$ of $f(x) = \prod_{n=0}^{100} (x-n)$ by hand? I tried to compute the derivative of $x$ and got $1$, and the derivative of $x(x-1)$ at $0$ is $-1$, and the derivative of $x(x-1)(x-2)$ at $0$ is $2$, and the derivative of $x(x-1)(x-2)(x-3)$ is $-6$. I don't see a pattern, and I'm not sure of an alternative route.","How can one calculate $f^\prime(0)$ of $f(x) = \prod_{n=0}^{100} (x-n)$ by hand? I tried to compute the derivative of $x$ and got $1$, and the derivative of $x(x-1)$ at $0$ is $-1$, and the derivative of $x(x-1)(x-2)$ at $0$ is $2$, and the derivative of $x(x-1)(x-2)(x-3)$ is $-6$. I don't see a pattern, and I'm not sure of an alternative route.",,['calculus']
59,Find continuous function $f: \mathbb R \to \mathbb R$ that is differentiable everywhere except $ \forall c \in \mathbb Z$,Find continuous function  that is differentiable everywhere except,f: \mathbb R \to \mathbb R  \forall c \in \mathbb Z,"Find continuous function $f: \mathbb R \to \mathbb R$ that is differentiable everywhere except $\forall c \in \mathbb Z$ Attempt: Let $$f(x) = \sqrt{1-\cos(2 \pi x)}$$ Then $f: \mathbb R \to \mathbb R$ and is continuous everywhere. $$f'(x) = \frac{\pi \sin(2 \pi x)}{\sqrt{1 - \cos(2 \pi x)}}$$ This is undefined $\forall c \in \mathbb Z$ However, since this is a case of $\frac00$, it seems necessary to me to observe the limiting behavior before I conclude it's undefined for all integers. A quick test on Mathematica showed that the left hand limit is $-\sqrt2 \pi$ and right hand limit is $\sqrt2 \pi$, so the limit does not exist. This should validate my function, however I'm still a little unsatisfied. First, is the function a valid candidate? second, how would I go about calculating that limit if so?","Find continuous function $f: \mathbb R \to \mathbb R$ that is differentiable everywhere except $\forall c \in \mathbb Z$ Attempt: Let $$f(x) = \sqrt{1-\cos(2 \pi x)}$$ Then $f: \mathbb R \to \mathbb R$ and is continuous everywhere. $$f'(x) = \frac{\pi \sin(2 \pi x)}{\sqrt{1 - \cos(2 \pi x)}}$$ This is undefined $\forall c \in \mathbb Z$ However, since this is a case of $\frac00$, it seems necessary to me to observe the limiting behavior before I conclude it's undefined for all integers. A quick test on Mathematica showed that the left hand limit is $-\sqrt2 \pi$ and right hand limit is $\sqrt2 \pi$, so the limit does not exist. This should validate my function, however I'm still a little unsatisfied. First, is the function a valid candidate? second, how would I go about calculating that limit if so?",,"['limits', 'derivatives', 'continuity']"
60,Prove continuity of a function.,Prove continuity of a function.,,"Let $$f(x) = \begin{cases}1 \ \ \ \ 1 \le |x| \\ 1/n \ \ \ \ 1/n \le |x| \lt 1/(n-1), \ \ n \in \mathbb{Z} \text{ and } n \ge 2 \\ 0  \ \ \ \ x = 0\end{cases}$$   Prove that $f$ is continuous at $x= 0$ but discontinuous at $x = \pm 1/n$ Let $f^*$ be a hyperreal extension of $f$. $$\lim_{x \to 0} f^*(x) = st(f^*(|\varepsilon|))$$ Now for any infinitesimal $\varepsilon$ we can find a infinite hyperinteger $K$ such that $${1\over K} \le |\varepsilon| \lt {1\over K-1}$$ Therefore $$ f^*(|\varepsilon|) = {1\over K} = \delta$$ Since $ \lim_{x \to c} f(c) = st(f^*(c))$ $$\therefore \lim_{x \to 0} f(x) = st(f^*(\varepsilon)) = st(\delta) = 0  = f(0)$$ Hence $f$ continous at $0$. Similarly, $$\lim_{x \to 1/n} f^*(x) = st(f^*(1/n + \epsilon))$$ Let $n$ be a infinite hyperinterger $H$, $$f^*(1/H + \varepsilon) = f^*(\delta + \epsilon)$$ Like the proof above we can find a $J$ such that $\displaystyle {1\over J} \le |\delta + \epsilon| \le {1\over J- 1}$ $$\therefore st(f^*(\delta + \epsilon)) = st\left({1\over J}\right) \ne {1\over H} = f^*(1/ H)$$ Therefore $f^*$ is discontinuous at $\pm 1/n$ and therefore $f$ is also discontinuous. I getting a feeling like I have used too many symbols that mean nothing and my proof is wrong. I think the second part is probably is wrong. Please tell me if this proof is correct and if not then how to correct it ?","Let $$f(x) = \begin{cases}1 \ \ \ \ 1 \le |x| \\ 1/n \ \ \ \ 1/n \le |x| \lt 1/(n-1), \ \ n \in \mathbb{Z} \text{ and } n \ge 2 \\ 0  \ \ \ \ x = 0\end{cases}$$   Prove that $f$ is continuous at $x= 0$ but discontinuous at $x = \pm 1/n$ Let $f^*$ be a hyperreal extension of $f$. $$\lim_{x \to 0} f^*(x) = st(f^*(|\varepsilon|))$$ Now for any infinitesimal $\varepsilon$ we can find a infinite hyperinteger $K$ such that $${1\over K} \le |\varepsilon| \lt {1\over K-1}$$ Therefore $$ f^*(|\varepsilon|) = {1\over K} = \delta$$ Since $ \lim_{x \to c} f(c) = st(f^*(c))$ $$\therefore \lim_{x \to 0} f(x) = st(f^*(\varepsilon)) = st(\delta) = 0  = f(0)$$ Hence $f$ continous at $0$. Similarly, $$\lim_{x \to 1/n} f^*(x) = st(f^*(1/n + \epsilon))$$ Let $n$ be a infinite hyperinterger $H$, $$f^*(1/H + \varepsilon) = f^*(\delta + \epsilon)$$ Like the proof above we can find a $J$ such that $\displaystyle {1\over J} \le |\delta + \epsilon| \le {1\over J- 1}$ $$\therefore st(f^*(\delta + \epsilon)) = st\left({1\over J}\right) \ne {1\over H} = f^*(1/ H)$$ Therefore $f^*$ is discontinuous at $\pm 1/n$ and therefore $f$ is also discontinuous. I getting a feeling like I have used too many symbols that mean nothing and my proof is wrong. I think the second part is probably is wrong. Please tell me if this proof is correct and if not then how to correct it ?",,"['real-analysis', 'derivatives']"
61,Differentiation of a function,Differentiation of a function,,"We need to find out the derivative $\frac{dy}{dx}$ of the following: $x^m$$y^n$ = $(x+y)^{m+n}$ I know how to differentiate the function and on solving we get $\frac{dy}{dx}$ = $\frac{y}{x}$ . But we notice that $\frac{dy}{dx}$ is independent of values of $m$ and $n$ . So what I did was again starting the problem from start but this time substituting any arbitrary values of $m$ and $n$ , like $m$ = $n$ = $1$ (lets say). That is we get $xy$ = $(x+y)^2$ . Now opening the square bracket and solving we get $x^2+y^2+xy=0$ So $\frac{dy}{dx}$ = $\frac{-2x-y}{2y+x}$ But the answer should be $\frac{y}{x}$ .Whats wrong in assuming arbitrary values of $m$ and $n$ if the answer dosen't depend upon values of $m$ and $n$ .Please help. How I got $\frac{dy}{dx}$ = $\frac{y}{x}$ ? $x+y$ = $x^\frac{m}{m+n}$$y^\frac{m}{m+n}$ $ln(x+y)$ = $\frac{m(ln(x))}{(m+n)}$ + $\frac{n(ln(y))}{(m+n)}$ Differentiating we get $\frac{1}{x+y}$ . $(1+\frac{dy}{dx})$ = $\frac{m}{(m+n)x}$ + $\frac{n}{(m+n)y}\frac{dy}{dx}$ $\frac{dy}{dx}(\frac{1}{x+y}-\frac{n}{(m+n)y})$ = $\frac{m}{(m+n)x}-\frac{1}{x+y}$ Just rearrange and you are done $\frac{dy}{dx}$ = $\frac{y}{x}$","We need to find out the derivative of the following: = I know how to differentiate the function and on solving we get = . But we notice that is independent of values of and . So what I did was again starting the problem from start but this time substituting any arbitrary values of and , like = = (lets say). That is we get = . Now opening the square bracket and solving we get So = But the answer should be .Whats wrong in assuming arbitrary values of and if the answer dosen't depend upon values of and .Please help. How I got = ? = = + Differentiating we get . = + = Just rearrange and you are done =",\frac{dy}{dx} x^my^n (x+y)^{m+n} \frac{dy}{dx} \frac{y}{x} \frac{dy}{dx} m n m n m n 1 xy (x+y)^2 x^2+y^2+xy=0 \frac{dy}{dx} \frac{-2x-y}{2y+x} \frac{y}{x} m n m n \frac{dy}{dx} \frac{y}{x} x+y x^\frac{m}{m+n}y^\frac{m}{m+n} ln(x+y) \frac{m(ln(x))}{(m+n)} \frac{n(ln(y))}{(m+n)} \frac{1}{x+y} (1+\frac{dy}{dx}) \frac{m}{(m+n)x} \frac{n}{(m+n)y}\frac{dy}{dx} \frac{dy}{dx}(\frac{1}{x+y}-\frac{n}{(m+n)y}) \frac{m}{(m+n)x}-\frac{1}{x+y} \frac{dy}{dx} \frac{y}{x},['derivatives']
62,Solving Trigonometric Derivatives,Solving Trigonometric Derivatives,,I have a function $F(θ) = \sin^{−1} \sqrt{\sin(11θ)}$ I derived the following answer using basic trigonometric and quotient rules. $\dfrac{11\csc \left(\left(11\theta \right)^{\left(\frac{1}{2}\right)}\right)\cos \left(\left(11\theta \right)\right)}{2\sqrt{1-\sin \left(11\theta \right)}}$ My answer however is wrong. Can anyone outline how to go about getting the correct solution to a problem such as this?,I have a function $F(θ) = \sin^{−1} \sqrt{\sin(11θ)}$ I derived the following answer using basic trigonometric and quotient rules. $\dfrac{11\csc \left(\left(11\theta \right)^{\left(\frac{1}{2}\right)}\right)\cos \left(\left(11\theta \right)\right)}{2\sqrt{1-\sin \left(11\theta \right)}}$ My answer however is wrong. Can anyone outline how to go about getting the correct solution to a problem such as this?,,"['calculus', 'trigonometry', 'derivatives']"
63,Show that if $\chi(r)=rR(r)$ then $\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)=r\frac{d^2 \chi(r)}{dr^2}$,Show that if  then,\chi(r)=rR(r) \frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)=r\frac{d^2 \chi(r)}{dr^2},"My Attempt: I first applied the product rule to  $$\fbox{$\color{blue}{\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)}$}\tag{1}\label1$$ to obtain  $$2r\frac{dR(r)}{dr}+r^2\frac{d^2R(r)}{dr^2}\tag{2}\label2,$$ and then wrote out the chain rule for  $$\frac{dR(r)}{dr}$$ as  $$\frac{dR(r)}{dr}=\frac{dR(r)}{d \chi(r)}\frac{d\chi(r)}{dr}\tag{3}\label3.$$ Then from $\chi(r)=rR(r)$ and using the product rule again  $$\frac{d\chi(r)}{dr}=R(r)+r\frac{d^2R(r)}{dr^2}\tag{4}\label4,$$ and since $R(r)=\dfrac{\chi(r)}{r},$ then $$\frac{dR(r)}{d \chi(r)}=\frac{1}{r}\tag{5}\label5.$$ Substituting $\eqref4$ and $\eqref5$ into $\eqref3$, I find that  $$\frac{dR(r)}{dr}=\frac{1}{r}\left(R(r)+r\frac{d^2R(r)}{dr^2}\right)=\frac{1}{r}R(r)+\frac{d^2R(r)}{dr^2}\tag{6}\label6$$ Substituting $\eqref6$ into $\eqref2$ yields  $$\begin{align}\fbox{$\color{blue}{\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)}$}&=2r\frac{dR(r)}{dr}+r^2\frac{d^2R(r)}{dr^2}\\&=2r\left(\frac{1}{r}R(r)+\frac{d^2R(r)}{dr^2}\right)+r^2\frac{d^2R(r)}{dr^2}\\&=2R(r)+(2r+r^2)\frac{d^2R(r)}{dr^2}.\end{align}$$ As you can see the expression I desire for $$\fbox{$\color{blue}{\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)}$}$$ is getting every-more complicated and I haven't even computed the second derivative of $R(r)$ yet. I fear that I am either making a mistake or missing a much simpler approach. In my notes it said that It is straightforward to show that $$\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)=r\frac{d^2 \chi(r)}{dr^2}.$$ So this implies that I am missing something very simple. If someone would care to point out my errors and/or give me any hints/tips on how to reach the desired result I would be most thankful. Regards.","My Attempt: I first applied the product rule to  $$\fbox{$\color{blue}{\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)}$}\tag{1}\label1$$ to obtain  $$2r\frac{dR(r)}{dr}+r^2\frac{d^2R(r)}{dr^2}\tag{2}\label2,$$ and then wrote out the chain rule for  $$\frac{dR(r)}{dr}$$ as  $$\frac{dR(r)}{dr}=\frac{dR(r)}{d \chi(r)}\frac{d\chi(r)}{dr}\tag{3}\label3.$$ Then from $\chi(r)=rR(r)$ and using the product rule again  $$\frac{d\chi(r)}{dr}=R(r)+r\frac{d^2R(r)}{dr^2}\tag{4}\label4,$$ and since $R(r)=\dfrac{\chi(r)}{r},$ then $$\frac{dR(r)}{d \chi(r)}=\frac{1}{r}\tag{5}\label5.$$ Substituting $\eqref4$ and $\eqref5$ into $\eqref3$, I find that  $$\frac{dR(r)}{dr}=\frac{1}{r}\left(R(r)+r\frac{d^2R(r)}{dr^2}\right)=\frac{1}{r}R(r)+\frac{d^2R(r)}{dr^2}\tag{6}\label6$$ Substituting $\eqref6$ into $\eqref2$ yields  $$\begin{align}\fbox{$\color{blue}{\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)}$}&=2r\frac{dR(r)}{dr}+r^2\frac{d^2R(r)}{dr^2}\\&=2r\left(\frac{1}{r}R(r)+\frac{d^2R(r)}{dr^2}\right)+r^2\frac{d^2R(r)}{dr^2}\\&=2R(r)+(2r+r^2)\frac{d^2R(r)}{dr^2}.\end{align}$$ As you can see the expression I desire for $$\fbox{$\color{blue}{\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)}$}$$ is getting every-more complicated and I haven't even computed the second derivative of $R(r)$ yet. I fear that I am either making a mistake or missing a much simpler approach. In my notes it said that It is straightforward to show that $$\frac{d}{dr}\left(r^2\frac{dR(r)}{dr}\right)=r\frac{d^2 \chi(r)}{dr^2}.$$ So this implies that I am missing something very simple. If someone would care to point out my errors and/or give me any hints/tips on how to reach the desired result I would be most thankful. Regards.",,"['calculus', 'derivatives', 'chain-rule']"
64,The rate at which distance between ships change,The rate at which distance between ships change,,"I know this looks like a physics problem but its given in my maths question sheets so I am asking it here.  Two ships A and B are sailing away from a fixed point   O such that $AOB=120 ^\circ $ . At a particular instant OA is $8km $ and OB is $6km $ and ships A,B are sailing at $20km/hr,30km/hr $ respectively. Then the distance between them is changing at the rate (km/hr)?$$\text {Attempt} $$  I assumed  B is moving along X axis and A is having motion in both directions . So  at any time t the distance travelled by $B $ is $30t $ and that by A is $x=-10t,y=10\sqrt{3}t $ thus total distance between A and B  is given by $d=\sqrt {{20}t^2}+10\sqrt {3}t $ .Now we want rate change so I differentiated it . But I am not sure if this is correct so wanted to verify and get idea on how to solve such problems.","I know this looks like a physics problem but its given in my maths question sheets so I am asking it here.  Two ships A and B are sailing away from a fixed point   O such that $AOB=120 ^\circ $ . At a particular instant OA is $8km $ and OB is $6km $ and ships A,B are sailing at $20km/hr,30km/hr $ respectively. Then the distance between them is changing at the rate (km/hr)?$$\text {Attempt} $$  I assumed  B is moving along X axis and A is having motion in both directions . So  at any time t the distance travelled by $B $ is $30t $ and that by A is $x=-10t,y=10\sqrt{3}t $ thus total distance between A and B  is given by $d=\sqrt {{20}t^2}+10\sqrt {3}t $ .Now we want rate change so I differentiated it . But I am not sure if this is correct so wanted to verify and get idea on how to solve such problems.",,"['calculus', 'trigonometry', 'derivatives']"
65,"Differentiate by first principle,$f(x)=\frac{x^2}{\sin x}$ [closed]","Differentiate by first principle, [closed]",f(x)=\frac{x^2}{\sin x},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Differentiate by first principle,$f(x)=\frac{x^2}{\sin x}$ $$f'(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ $$f'(x)=\lim_{h\to0}\frac{\frac{(x+h)^2}{\sin(x+h)}-\frac{x^2}{\sin x}}{h}$$ $$f'(x)=\lim_{h\to0}\frac{\frac{(x+h)^2\sin x-x^2\sin(x+h)}{\sin(x+h)\sin x}}{h}$$ $$f'(x)=\lim_{h\to0}\frac{(x+h)^2\sin x-x^2\sin(x+h)}{h\sin x\sin(x+h)}$$ $$f'(x)=\lim_{h\to0}\frac{x^2}{\sin x}\frac{(1+\frac{h}{x})^2\sin x-\sin(x+h)}{h\sin(x+h)}$$ I am stuck here.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Differentiate by first principle,$f(x)=\frac{x^2}{\sin x}$ $$f'(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ $$f'(x)=\lim_{h\to0}\frac{\frac{(x+h)^2}{\sin(x+h)}-\frac{x^2}{\sin x}}{h}$$ $$f'(x)=\lim_{h\to0}\frac{\frac{(x+h)^2\sin x-x^2\sin(x+h)}{\sin(x+h)\sin x}}{h}$$ $$f'(x)=\lim_{h\to0}\frac{(x+h)^2\sin x-x^2\sin(x+h)}{h\sin x\sin(x+h)}$$ $$f'(x)=\lim_{h\to0}\frac{x^2}{\sin x}\frac{(1+\frac{h}{x})^2\sin x-\sin(x+h)}{h\sin(x+h)}$$ I am stuck here.",,"['limits', 'derivatives', 'limits-without-lhopital']"
66,Sufficient condition for inflection point,Sufficient condition for inflection point,,"I am a bit confused about the following statement. Let $f$ be a real function of a real variable and $x_0 \in \mathbb{R}$. If $f$ is differentiable in a neighborhood $I$ of $x_0$, $f'(x_0) = 0$ and $f'(x_0) > 0$ for all $x \in I \setminus \{x_0\}$, then $x_0$ is an inflection point. Is this statement true? Reading this question: Prove that $f$ has an inflection point at zero if $f$ is a function that satisfies a given set of hypotheses , I think my statement is false, but I cannot give a counterexample. Can anyone help me out? EDIT. (Thanks to @Zestylemonzi's comment) My definition of inflection point is as follows: $x_0$ is an inflection point if $f$ is differentiable at $x_0$ and there exists a neighborhood $J$ of $x_0$ such that the function $d(x) = f(x) − f(x_0) − f'(x_0) (x − x_0)$ has the same sign as $x−x_0$ for all $x \in J \setminus \{ x_0 \}$, or $d(x) = f(x) − f(x_0) − f'(x_0) (x − x_0)$ and $x−x_0$ have opposite signs for all $x \in J \setminus \{ x_0 \}$ I think that according to this definition my statement is true. I am aware of another possible definition of inflection point : $x_0$ is an inflection point if $f$ is differentiable at $x_0$ and there exists $\delta > 0$ such that $f$ is convex (respectively, concave) for $x \in (x_0 - \delta, x_0)$ and concave (respectively, convex) for $x \in (x_0, x_0 + \delta)$. Is my statement true according to the second definition?","I am a bit confused about the following statement. Let $f$ be a real function of a real variable and $x_0 \in \mathbb{R}$. If $f$ is differentiable in a neighborhood $I$ of $x_0$, $f'(x_0) = 0$ and $f'(x_0) > 0$ for all $x \in I \setminus \{x_0\}$, then $x_0$ is an inflection point. Is this statement true? Reading this question: Prove that $f$ has an inflection point at zero if $f$ is a function that satisfies a given set of hypotheses , I think my statement is false, but I cannot give a counterexample. Can anyone help me out? EDIT. (Thanks to @Zestylemonzi's comment) My definition of inflection point is as follows: $x_0$ is an inflection point if $f$ is differentiable at $x_0$ and there exists a neighborhood $J$ of $x_0$ such that the function $d(x) = f(x) − f(x_0) − f'(x_0) (x − x_0)$ has the same sign as $x−x_0$ for all $x \in J \setminus \{ x_0 \}$, or $d(x) = f(x) − f(x_0) − f'(x_0) (x − x_0)$ and $x−x_0$ have opposite signs for all $x \in J \setminus \{ x_0 \}$ I think that according to this definition my statement is true. I am aware of another possible definition of inflection point : $x_0$ is an inflection point if $f$ is differentiable at $x_0$ and there exists $\delta > 0$ such that $f$ is convex (respectively, concave) for $x \in (x_0 - \delta, x_0)$ and concave (respectively, convex) for $x \in (x_0, x_0 + \delta)$. Is my statement true according to the second definition?",,"['calculus', 'real-analysis', 'derivatives', 'examples-counterexamples']"
67,How can I find the $n$ th derivatve of this function,How can I find the  th derivatve of this function,n,"Let $f(x)=\sqrt{1+\sqrt{1-x}}\,\,\,\,\,\ \forall x\in[0,1].$ It is not difficult to find first few derivatives of this function as $$f'(x)=-\dfrac1{2^2}(1+\sqrt{1-x})^{-1/2}(1-x)^{-1/2}$$ $$f''(x)=-\frac1{2^4}(1+\sqrt{1-x})^{-3/2}(1-x)^{-1}-\frac1{2^3}(1+\sqrt{1-x})^{-1/2}(1-x)^{-3/2}$$ $$f'''(x)=-\frac3{2^6}(1+\sqrt{1-x})^{-5/2}(1-x)^{-3/2}-\frac3{2^5}(1+\sqrt{1-x})^{-3/2}(1-x)^{-2}-\frac3{2^4}(1+\sqrt{1-x})^{-1/2}(1-x)^{-5/2}$$ However it is difficult to observe a pattern between these derivatives. How can I find the $n$th derivative of $f$?","Let $f(x)=\sqrt{1+\sqrt{1-x}}\,\,\,\,\,\ \forall x\in[0,1].$ It is not difficult to find first few derivatives of this function as $$f'(x)=-\dfrac1{2^2}(1+\sqrt{1-x})^{-1/2}(1-x)^{-1/2}$$ $$f''(x)=-\frac1{2^4}(1+\sqrt{1-x})^{-3/2}(1-x)^{-1}-\frac1{2^3}(1+\sqrt{1-x})^{-1/2}(1-x)^{-3/2}$$ $$f'''(x)=-\frac3{2^6}(1+\sqrt{1-x})^{-5/2}(1-x)^{-3/2}-\frac3{2^5}(1+\sqrt{1-x})^{-3/2}(1-x)^{-2}-\frac3{2^4}(1+\sqrt{1-x})^{-1/2}(1-x)^{-5/2}$$ However it is difficult to observe a pattern between these derivatives. How can I find the $n$th derivative of $f$?",,"['real-analysis', 'algebra-precalculus', 'derivatives', 'induction', 'pattern-recognition']"
68,A question about differentiation,A question about differentiation,,"$g(x)=\sin(1/x)  ,  g(0)=0$ Define $G(x)=\int _{0}^{x}g\left( t\right) dt$ Show $G'(0)=g(0)$ I use the definition $$G'(0) = \lim _{h\rightarrow 0}\dfrac {G\left( h\right) -G\left( 0\right) } {h}$$ $$=\lim _{h\rightarrow 0}\dfrac {\int _{0}^{h}g\left( t\right) dt} {h}$$ $$=\lim _{h\rightarrow 0}\dfrac {\int _{0}^{h}\sin(1/t) dt} {h}$$ I have no idea about next step Can you give me some hint Thank you!!!","$g(x)=\sin(1/x)  ,  g(0)=0$ Define $G(x)=\int _{0}^{x}g\left( t\right) dt$ Show $G'(0)=g(0)$ I use the definition $$G'(0) = \lim _{h\rightarrow 0}\dfrac {G\left( h\right) -G\left( 0\right) } {h}$$ $$=\lim _{h\rightarrow 0}\dfrac {\int _{0}^{h}g\left( t\right) dt} {h}$$ $$=\lim _{h\rightarrow 0}\dfrac {\int _{0}^{h}\sin(1/t) dt} {h}$$ I have no idea about next step Can you give me some hint Thank you!!!",,"['calculus', 'derivatives']"
69,How to check the differentiability of this function.,How to check the differentiability of this function.,,"Question: Let $\text{f(x) = max{1-x, 1+x, 2}}$. Prove that $f(x)$ is continuous at all points but not differentiable at $x = 1$ and $x = -1$. Doubt: I checked for the continuity of this function and successfully found it to be continuous at all points. For differentiability, at $x=1$, I calculated the right and left hand derivative using: $$f'(a)=\lim_{h\to0}\frac{f(a\pm h)-f(a)}{\pm h}.$$ For $Rf'(1)$, the value of $f(1+h)$ will be $\text{1+1+h}$ as $\text{1+x}$ is the maximum in this case. On solving, I am getting $Rf'(1)=1$. For $Lf'(1)$, the value of $f(1-h)$ will be $\text{2}$ as it is the maximum in this case. On solving, I am getting $Lf'(1)=\lim_{h\to0}\frac{2-2}{h}$. The case is similar with $\text{-1}$. But for $\text{x=0}$, I am getting both, Right and Left hand derivative similar to $\lim_{h\to0}\frac{2-2}{h}$. But according to question, the function is non-differentiable at only $\text{x=-1,1}$. Kindly help.","Question: Let $\text{f(x) = max{1-x, 1+x, 2}}$. Prove that $f(x)$ is continuous at all points but not differentiable at $x = 1$ and $x = -1$. Doubt: I checked for the continuity of this function and successfully found it to be continuous at all points. For differentiability, at $x=1$, I calculated the right and left hand derivative using: $$f'(a)=\lim_{h\to0}\frac{f(a\pm h)-f(a)}{\pm h}.$$ For $Rf'(1)$, the value of $f(1+h)$ will be $\text{1+1+h}$ as $\text{1+x}$ is the maximum in this case. On solving, I am getting $Rf'(1)=1$. For $Lf'(1)$, the value of $f(1-h)$ will be $\text{2}$ as it is the maximum in this case. On solving, I am getting $Lf'(1)=\lim_{h\to0}\frac{2-2}{h}$. The case is similar with $\text{-1}$. But for $\text{x=0}$, I am getting both, Right and Left hand derivative similar to $\lim_{h\to0}\frac{2-2}{h}$. But according to question, the function is non-differentiable at only $\text{x=-1,1}$. Kindly help.",,"['calculus', 'limits', 'derivatives', 'continuity']"
70,"If f is a continuous function that's differentiable show that there exists $d∈(a,b)$ such that $f′(d)=0$",If f is a continuous function that's differentiable show that there exists  such that,"d∈(a,b) f′(d)=0","Hi all i'm struggling on this question and I don't know how to do it. Let $f : [a, b] → R$ be a continuous function that is differentiable on $(a, b)$. We assume that $f(a) < f(p)$ and $f(p) > f(b)$ for some $p ∈ (a, b)$. Show that there exists $d ∈ (a, b)$ such that $f ' (d) = 0.$ Any help would be very much appreciated","Hi all i'm struggling on this question and I don't know how to do it. Let $f : [a, b] → R$ be a continuous function that is differentiable on $(a, b)$. We assume that $f(a) < f(p)$ and $f(p) > f(b)$ for some $p ∈ (a, b)$. Show that there exists $d ∈ (a, b)$ such that $f ' (d) = 0.$ Any help would be very much appreciated",,"['calculus', 'derivatives', 'proof-writing']"
71,The basic of functional derivative,The basic of functional derivative,,"I've just started to learn mathematical physics, and I read Stone and Goldbart's Mathematics for Physics . But right at the beginning when they introduce the functional derivative, I couldn't understand their explanation: 1.2.1. The functional derivative: We restrict ourselves to expressions of the form $$J[y] = \int_{x_1}^{x_2} f[ \, x, y, y',y'',...y^{(n)} ] \, dx $$ Consider a functional $J=\int f dx$ in which f depends only on $x, y$ and $y'$ . Make a change $y(x) \rightarrow y(x) + \epsilon \eta (x)$ , where $\epsilon$ is a (small) $x$ -independent constant. The resultant change in J is: $$\begin{align} J[y+\epsilon \eta]-J[y] &=\int_{x_1}^{x_2}\{f(x,y+\epsilon \eta,y'+\epsilon \eta')-f(x,y,y')\}dx \\ &= \int_{x_1}^{x_2} \{\epsilon \eta\frac{\partial f}{\partial y}+\epsilon\frac{d\eta}{dx}\frac{\partial f}{\partial y'}+O(\epsilon^2)\}\\ &= ......(\text{this part I understand})\end{align} $$ I can't really wrap my head around this. What is that $\eta(x)$ that suddenly showed up? How did they manage from the first line to the second line in the above expression? And why is there a big-O at the end? (If you don't have the book you can actually find on page 16 right here , the next part is just an integration by part). I'm honestly clueless. It'd be awesome if someone can explain this to me please?","I've just started to learn mathematical physics, and I read Stone and Goldbart's Mathematics for Physics . But right at the beginning when they introduce the functional derivative, I couldn't understand their explanation: 1.2.1. The functional derivative: We restrict ourselves to expressions of the form Consider a functional in which f depends only on and . Make a change , where is a (small) -independent constant. The resultant change in J is: I can't really wrap my head around this. What is that that suddenly showed up? How did they manage from the first line to the second line in the above expression? And why is there a big-O at the end? (If you don't have the book you can actually find on page 16 right here , the next part is just an integration by part). I'm honestly clueless. It'd be awesome if someone can explain this to me please?","J[y] = \int_{x_1}^{x_2} f[ \, x, y, y',y'',...y^{(n)} ] \, dx  J=\int f dx x, y y' y(x) \rightarrow y(x) + \epsilon \eta (x) \epsilon x \begin{align} J[y+\epsilon \eta]-J[y] &=\int_{x_1}^{x_2}\{f(x,y+\epsilon \eta,y'+\epsilon \eta')-f(x,y,y')\}dx \\ &= \int_{x_1}^{x_2} \{\epsilon \eta\frac{\partial f}{\partial y}+\epsilon\frac{d\eta}{dx}\frac{\partial f}{\partial y'}+O(\epsilon^2)\}\\ &= ......(\text{this part I understand})\end{align}  \eta(x)",['derivatives']
72,Different results implicit differentiation,Different results implicit differentiation,,"When trying to differentiate $\frac{x+y}{xy}=x$, I get different results. If I use the quotient rule: $$ \begin{array}{rcl} \dfrac{xy(1+y´)-(x+y)(xy´+y)}{(xy)^2} &=& 1 \\ \dfrac{xy+xyy´-x^2y´-xy-xyy´-y^2}{(xy)^2} &=& 1 \\ -x^2y´ -y^2 &=& x^2y^2 \\ y´ &=& \dfrac{-(y^2+x^2y^2)}{x^2} \end{array} $$ On the other hand if I do the following $$ \begin{array}{rcl} \dfrac{x+y}{xy} &=& x \\ x+y &=& x^2y \\ 1+y´ &=& x^2y´ +2xy \\ y´ -x^2y´ &=& 2xy-1 \\ y´ &=& \dfrac{2xy-1}{1-x^2} \end{array} $$ I don't understand what's happening.","When trying to differentiate $\frac{x+y}{xy}=x$, I get different results. If I use the quotient rule: $$ \begin{array}{rcl} \dfrac{xy(1+y´)-(x+y)(xy´+y)}{(xy)^2} &=& 1 \\ \dfrac{xy+xyy´-x^2y´-xy-xyy´-y^2}{(xy)^2} &=& 1 \\ -x^2y´ -y^2 &=& x^2y^2 \\ y´ &=& \dfrac{-(y^2+x^2y^2)}{x^2} \end{array} $$ On the other hand if I do the following $$ \begin{array}{rcl} \dfrac{x+y}{xy} &=& x \\ x+y &=& x^2y \\ 1+y´ &=& x^2y´ +2xy \\ y´ -x^2y´ &=& 2xy-1 \\ y´ &=& \dfrac{2xy-1}{1-x^2} \end{array} $$ I don't understand what's happening.",,"['calculus', 'derivatives', 'implicit-differentiation']"
73,Segment by $a$ and $b$ and let $z$ be a complex number not in it. Show that $\frac{z-a}{z-b}$ isn't a real $\le 0$,Segment by  and  and let  be a complex number not in it. Show that  isn't a real,a b z \frac{z-a}{z-b} \le 0,"I need to show that, in a segment $ab$, when $z$ is a complex number outside it, the expression below is never a real $\le 0$. $$\frac{z-a}{z-b}$$ I think it has something to do with the Log function, as it's not analytic exactly in the real negative line with $0$ included. Later in this exercise it asks me to derivate $\log \frac{z-a}{z-b}$ and I did: $\log \frac{z-a}{z-b} = \log (z-a)-\log (z-b)$ then I took the derivative, but I need to know that the expression above is never on the negative real axis. Any ideas?","I need to show that, in a segment $ab$, when $z$ is a complex number outside it, the expression below is never a real $\le 0$. $$\frac{z-a}{z-b}$$ I think it has something to do with the Log function, as it's not analytic exactly in the real negative line with $0$ included. Later in this exercise it asks me to derivate $\log \frac{z-a}{z-b}$ and I did: $\log \frac{z-a}{z-b} = \log (z-a)-\log (z-b)$ then I took the derivative, but I need to know that the expression above is never on the negative real axis. Any ideas?",,"['calculus', 'complex-analysis', 'derivatives', 'complex-numbers']"
74,What is meant by $d(xy)$?,What is meant by ?,d(xy),"$dy/dx$ means derivative of $y$ with respect to $x$ But what is meant by $d(xy)$? Where is the ""with respect to term"" here ?","$dy/dx$ means derivative of $y$ with respect to $x$ But what is meant by $d(xy)$? Where is the ""with respect to term"" here ?",,"['calculus', 'derivatives']"
75,Prove that this function is differentiable: $a(x)=(x-3)^{2}f(x)$,Prove that this function is differentiable:,a(x)=(x-3)^{2}f(x),"Given is a (not necessary continuous) function $f: \mathbb{R} \rightarrow \mathbb{R}$ with $|f(x)|<1$ for all $x \in \mathbb{R}$.   Let $a(x):=(x-3)^{2}f(x)$. Prove that $a$ is differentiable at $x_{0}=3$. We know from a previous task that $b(x)=(x-3)f(x)$ is continuous at $x_{0}=3$ From this we can conclude that this function $a$ will be continuous as well because stuff like summation or multiplication of continuous things stay continuous. Now we know that $a(x)$ is continuous but what does it tell us? Unfortunately, a continuous function isn't necessarily differentiable... What to do here? I thought about using the difference quotient but we will have troubles with $f(x)$, I mean it could be smaller than $0$ if we don't use the modulus. Or can we just set the modulus when we use difference quotient? This is no homework, it's from an old exam and if you want I can upload it here but it won't be in English!","Given is a (not necessary continuous) function $f: \mathbb{R} \rightarrow \mathbb{R}$ with $|f(x)|<1$ for all $x \in \mathbb{R}$.   Let $a(x):=(x-3)^{2}f(x)$. Prove that $a$ is differentiable at $x_{0}=3$. We know from a previous task that $b(x)=(x-3)f(x)$ is continuous at $x_{0}=3$ From this we can conclude that this function $a$ will be continuous as well because stuff like summation or multiplication of continuous things stay continuous. Now we know that $a(x)$ is continuous but what does it tell us? Unfortunately, a continuous function isn't necessarily differentiable... What to do here? I thought about using the difference quotient but we will have troubles with $f(x)$, I mean it could be smaller than $0$ if we don't use the modulus. Or can we just set the modulus when we use difference quotient? This is no homework, it's from an old exam and if you want I can upload it here but it won't be in English!",,"['calculus', 'analysis', 'derivatives', 'continuity', 'epsilon-delta']"
76,Why does $\lim_{h \to 0} f(x+h) = f(x)$ according to Lang's Short Calculus?,Why does  according to Lang's Short Calculus?,\lim_{h \to 0} f(x+h) = f(x),"On Chapter III ( Derivatives ), section 5 ( Sums, products, & quotients ) of Lang's Short Calculus , Lang writes the following excerpt: It's unclear to me why Lang asserts $$f(x+h) = f(x) + h \frac{f(x+h)-f(x)}{h} $$ From independent research, I know this is true if the function is continuous ( https://www.quora.com/Does-Lim-f-x+h-f-x-as-h-approaches-0 ). However, my question is how did Lang arrive at this assertion (aka why is the assertion true)? Thanks.","On Chapter III ( Derivatives ), section 5 ( Sums, products, & quotients ) of Lang's Short Calculus , Lang writes the following excerpt: It's unclear to me why Lang asserts $$f(x+h) = f(x) + h \frac{f(x+h)-f(x)}{h} $$ From independent research, I know this is true if the function is continuous ( https://www.quora.com/Does-Lim-f-x+h-f-x-as-h-approaches-0 ). However, my question is how did Lang arrive at this assertion (aka why is the assertion true)? Thanks.",,"['calculus', 'limits', 'derivatives']"
77,How many real solutions does this equation have: $x^{2}-2\sin x+2x=33$,How many real solutions does this equation have:,x^{2}-2\sin x+2x=33,"How many real solutions does this equation have: $x^{2}-2\sin x+2x=33$ I have created this equation myself, so it's not homework and I hope it's even solvable.. It will be only for practice. $$x^{2}-2\sin x+2x=33$$ $$x^{2}-2\sin x+2x-33 = 0$$ $$f'(x) = 0$$ $$2x-2\cos x+2 =0$$ $$x = 0$$ Now check if it's maximum or minimum: $$f''(x) = 2+2\sin x$$ $$f''(0) = 2+2\sin (0) > 0 \Rightarrow minimum$$ Now let's calculate the point of minimum: $$0^{2}-2\sin (0)+2\cdot 0 -33= -33$$ $\Rightarrow P(0|-33)$ Now let's check if function is monotonic: $f'(x) = 2x -2\cos x + 2$ For $x>0$ we get $f'(x) > 0$ , so monotonic increasing For $x<0$ we get $f'(x) < 0$ , so monotonic decreasing $\Rightarrow$ Function has $2$ real solutions My questions: Did I do it correctly? If correct, is there something I could have skipped because not very necessary (I'm thinking to skip the calculation of maximum / minimum and just do the monotony)?","How many real solutions does this equation have: $x^{2}-2\sin x+2x=33$ I have created this equation myself, so it's not homework and I hope it's even solvable.. It will be only for practice. $$x^{2}-2\sin x+2x=33$$ $$x^{2}-2\sin x+2x-33 = 0$$ $$f'(x) = 0$$ $$2x-2\cos x+2 =0$$ $$x = 0$$ Now check if it's maximum or minimum: $$f''(x) = 2+2\sin x$$ $$f''(0) = 2+2\sin (0) > 0 \Rightarrow minimum$$ Now let's calculate the point of minimum: $$0^{2}-2\sin (0)+2\cdot 0 -33= -33$$ $\Rightarrow P(0|-33)$ Now let's check if function is monotonic: $f'(x) = 2x -2\cos x + 2$ For $x>0$ we get $f'(x) > 0$ , so monotonic increasing For $x<0$ we get $f'(x) < 0$ , so monotonic decreasing $\Rightarrow$ Function has $2$ real solutions My questions: Did I do it correctly? If correct, is there something I could have skipped because not very necessary (I'm thinking to skip the calculation of maximum / minimum and just do the monotony)?",,"['calculus', 'analysis', 'derivatives', 'monotone-functions']"
78,Understanding the use of continuity and 'derivative change' in finding Green functions,Understanding the use of continuity and 'derivative change' in finding Green functions,,"Below is a university question and the corresponding solution for which I do not understand small parts of: Question: Show that the Green’s function for the range $x \ge 0$, satisfying $$\frac{\partial^2 G(x,z)}{\partial x^2}+G(x,z)=\delta(x-z)$$ with boundary conditions:$$G(x,z)=\frac{\partial G(x,z)}{\partial x}=0\quad\text{at}\quad x=0$$ is $$G(x,z)=\begin{cases}\cos z\sin x-\sin z\cos x, &\text{for} & x\gt z \\ 0 &\text{for} & x\lt z\end{cases}$$ Solution: For $x \ne z$    $$\frac{\partial^2 G(x,z)}{\partial x^2}+G(x,z)=0$$ with solution   $$G(x,z)=\begin{cases}A(z)\sin x+B(z)\cos x, &\text{for} & x\lt z \\ C(z)\sin x+D(z)\cos x &\text{for} & x\gt z\end{cases}$$   $$G(x=0,z)=0\implies B(z)=0$$ The derivative is (for $x\le z$)   $$\frac{\partial G(x,z)}{\partial x}=A(z)\cos x$$   Since this is zero at $x=0$, we conclude that $A(z)=0$. Hence, $$G(x,z)=0\qquad (x\lt z)$$   $\color{red}{\mathrm{For}}$ $\color{red}{x\gt z,}$ $\color{red}{\text{we use continuity of}}$ $\color{red}{G(x,z)}$ $\color{red}{\mathrm{at}}$ $\color{red}{x=z,}$ $\color{red}{\mathrm{ie.}}$   $$\color{red}{C(z)\sin z+D(z)\cos z=0\tag{1}}$$   $\color{red}{\text{The first derivative changes by}}$ $\color{red}{1,}$ $\color{red}{\text{so since}}$ $\color{red}{G(x,z)=0}$ $\color{red}{\text{for}}$ $\color{red}{x\lt z,}$   $$\color{red}{C(z)\cos z -D(z)\sin z=1\tag{2}}$$   We can eliminate $D$ (or $C$) from equations $(1)$ and $(2)$:   $$C(z)\cos z + C(z)\frac{\sin^2z}{\cos z}=1\tag{3}$$   Multiplying equation $(3)$ by $\cos z$ gives $$C(z)\cos^2 z+C(z)\sin^2 z=\cos z$$ and since $\cos^2 z+\sin^2 z=1$,   $$C(z)=\cos z$$ and we then find $$D(z)=-\sin z$$   Thus,   $$G(x,z)=\begin{cases}\cos z\sin x-\sin z\cos x, &\text{for} & x\gt z \\ 0 &\text{for} & x\lt z\end{cases}$$   $\fbox{}$ I understand everything apart from the part marked red; Why is equation $\color{red}{(1)}$ equal to zero? What does this have to do with continuity? Also equation $\color{red}{(1)}$ has products: $C(z)\sin z$ and $D(z)\cos z$ since $C(z)$ and $D(z)$ are both functions of $z$. So why isn't the product rule being used to yield $$C'(z)\sin z + C(z)\cos z + D'(z)\cos z -D(z)\sin z\text{?}$$ Why is it that the ""first derivative changes by $1$""?","Below is a university question and the corresponding solution for which I do not understand small parts of: Question: Show that the Green’s function for the range $x \ge 0$, satisfying $$\frac{\partial^2 G(x,z)}{\partial x^2}+G(x,z)=\delta(x-z)$$ with boundary conditions:$$G(x,z)=\frac{\partial G(x,z)}{\partial x}=0\quad\text{at}\quad x=0$$ is $$G(x,z)=\begin{cases}\cos z\sin x-\sin z\cos x, &\text{for} & x\gt z \\ 0 &\text{for} & x\lt z\end{cases}$$ Solution: For $x \ne z$    $$\frac{\partial^2 G(x,z)}{\partial x^2}+G(x,z)=0$$ with solution   $$G(x,z)=\begin{cases}A(z)\sin x+B(z)\cos x, &\text{for} & x\lt z \\ C(z)\sin x+D(z)\cos x &\text{for} & x\gt z\end{cases}$$   $$G(x=0,z)=0\implies B(z)=0$$ The derivative is (for $x\le z$)   $$\frac{\partial G(x,z)}{\partial x}=A(z)\cos x$$   Since this is zero at $x=0$, we conclude that $A(z)=0$. Hence, $$G(x,z)=0\qquad (x\lt z)$$   $\color{red}{\mathrm{For}}$ $\color{red}{x\gt z,}$ $\color{red}{\text{we use continuity of}}$ $\color{red}{G(x,z)}$ $\color{red}{\mathrm{at}}$ $\color{red}{x=z,}$ $\color{red}{\mathrm{ie.}}$   $$\color{red}{C(z)\sin z+D(z)\cos z=0\tag{1}}$$   $\color{red}{\text{The first derivative changes by}}$ $\color{red}{1,}$ $\color{red}{\text{so since}}$ $\color{red}{G(x,z)=0}$ $\color{red}{\text{for}}$ $\color{red}{x\lt z,}$   $$\color{red}{C(z)\cos z -D(z)\sin z=1\tag{2}}$$   We can eliminate $D$ (or $C$) from equations $(1)$ and $(2)$:   $$C(z)\cos z + C(z)\frac{\sin^2z}{\cos z}=1\tag{3}$$   Multiplying equation $(3)$ by $\cos z$ gives $$C(z)\cos^2 z+C(z)\sin^2 z=\cos z$$ and since $\cos^2 z+\sin^2 z=1$,   $$C(z)=\cos z$$ and we then find $$D(z)=-\sin z$$   Thus,   $$G(x,z)=\begin{cases}\cos z\sin x-\sin z\cos x, &\text{for} & x\gt z \\ 0 &\text{for} & x\lt z\end{cases}$$   $\fbox{}$ I understand everything apart from the part marked red; Why is equation $\color{red}{(1)}$ equal to zero? What does this have to do with continuity? Also equation $\color{red}{(1)}$ has products: $C(z)\sin z$ and $D(z)\cos z$ since $C(z)$ and $D(z)$ are both functions of $z$. So why isn't the product rule being used to yield $$C'(z)\sin z + C(z)\cos z + D'(z)\cos z -D(z)\sin z\text{?}$$ Why is it that the ""first derivative changes by $1$""?",,"['derivatives', 'continuity', 'intuition', 'proof-explanation', 'greens-function']"
79,Understanding intuitively Why the rate of change of area of a circle with radius is the circumference of the circle,Understanding intuitively Why the rate of change of area of a circle with radius is the circumference of the circle,,"I don't know whether this a rigorous mathematical question. But I was trying to figure out the intuition behind the answer . I have through out my high school used the fact that d/dr (πr^2)= 2πr . It is obvious from the formula. My question is how should I understand the answer intuitively or geometrically ? We all know that the rate of change of displacement with time is the instantaneous velocity. The answer seems obvious in the first reading of any basic calculus text. Derivatives of certain other functions also seem obvious (or atleast not as unobvious as the above circle one ) at the first go , or if they don't seem so obvious we can understand them after drawing a picture and analysing it geometrically. But how should I intuitively  understand the rate of change of area of circle with radius being equal to the circumference of the circle ? Why is it so ? I need to understand it geometrically too.  How can the rate of change of area of circle with radius give the circumference of the circle ( intuitively ) ?","I don't know whether this a rigorous mathematical question. But I was trying to figure out the intuition behind the answer . I have through out my high school used the fact that d/dr (πr^2)= 2πr . It is obvious from the formula. My question is how should I understand the answer intuitively or geometrically ? We all know that the rate of change of displacement with time is the instantaneous velocity. The answer seems obvious in the first reading of any basic calculus text. Derivatives of certain other functions also seem obvious (or atleast not as unobvious as the above circle one ) at the first go , or if they don't seem so obvious we can understand them after drawing a picture and analysing it geometrically. But how should I intuitively  understand the rate of change of area of circle with radius being equal to the circumference of the circle ? Why is it so ? I need to understand it geometrically too.  How can the rate of change of area of circle with radius give the circumference of the circle ( intuitively ) ?",,"['geometry', 'derivatives', 'euclidean-geometry', 'analytic-geometry']"
80,Can we use this as definition of the derivative?,Can we use this as definition of the derivative?,,Let $D$ be a subset of $\mathbb{K}$ where $\mathbb{K}=\mathbb{R}$ or $\mathbb{C}$. Let $f:D\to \mathbb{K}^N$ be a function. Then $f$ is differentiable at $x$ if $$\lim_{n\to \infty}\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}$$ exists.,Let $D$ be a subset of $\mathbb{K}$ where $\mathbb{K}=\mathbb{R}$ or $\mathbb{C}$. Let $f:D\to \mathbb{K}^N$ be a function. Then $f$ is differentiable at $x$ if $$\lim_{n\to \infty}\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}$$ exists.,,['derivatives']
81,Find $f^{(100)}(x)$ where $f(x)=\frac{1}{4x^2-1}$,Find  where,f^{(100)}(x) f(x)=\frac{1}{4x^2-1},"Find $f^{(100)}(x)$ where $f(x)=\frac{1}{4x^2-1}$. I found first,second and third derivative: $$f'(x)=\frac{-8x}{(4x^2-1)^2}$$ $$f''(x)=\frac{96x^2+8}{(4x^2-1)^3} $$ $$f'''(x)=\frac{-384x(4x^2+1)}{(4x^2-1)^4}$$ I can't seem to find any rule between them. Anyone has any ideas or hints?","Find $f^{(100)}(x)$ where $f(x)=\frac{1}{4x^2-1}$. I found first,second and third derivative: $$f'(x)=\frac{-8x}{(4x^2-1)^2}$$ $$f''(x)=\frac{96x^2+8}{(4x^2-1)^3} $$ $$f'''(x)=\frac{-384x(4x^2+1)}{(4x^2-1)^4}$$ I can't seem to find any rule between them. Anyone has any ideas or hints?",,"['real-analysis', 'derivatives']"
82,"Show that, for all $n > 1: \log \frac{2n + 1}{n} < \frac1n + \frac{1}{n + 1} + \cdots + \frac{1}{2n} < \log \frac{2n}{n - 1}$","Show that, for all",n > 1: \log \frac{2n + 1}{n} < \frac1n + \frac{1}{n + 1} + \cdots + \frac{1}{2n} < \log \frac{2n}{n - 1},"I'm learning calculus, specifically limit of sequences and derivatives, and need help with the following exercise: Show that for every $n > 1$ , $$\log \frac{2n + 1}{n} < \frac1n + \frac{1}{n + 1} + \cdots + \frac{1}{2n} < \log \frac{2n}{n - 1} \quad \quad (1)$$ Important: this exercise is the continuation of a previous problem showing that, for every $n > 1$ , $$\frac{1}{n + 1} < \log(1 + \frac1n) < \frac1n \quad \quad (2)$$ A detailed solution of the latter inequality using the MVT can be found here . Now back to inequality $(1)$ . My first guess was to use mathematical induction in order to prove it but I didn't get far. I think I should make use of inequality $(2)$ from the previous exercise but I'm stuck here.","I'm learning calculus, specifically limit of sequences and derivatives, and need help with the following exercise: Show that for every , Important: this exercise is the continuation of a previous problem showing that, for every , A detailed solution of the latter inequality using the MVT can be found here . Now back to inequality . My first guess was to use mathematical induction in order to prove it but I didn't get far. I think I should make use of inequality from the previous exercise but I'm stuck here.",n > 1 \log \frac{2n + 1}{n} < \frac1n + \frac{1}{n + 1} + \cdots + \frac{1}{2n} < \log \frac{2n}{n - 1} \quad \quad (1) n > 1 \frac{1}{n + 1} < \log(1 + \frac1n) < \frac1n \quad \quad (2) (1) (2),"['calculus', 'sequences-and-series', 'derivatives', 'inequality']"
83,Differentiating $\int_{-\infty}^{\infty}e^{ixt}dt$,Differentiating,\int_{-\infty}^{\infty}e^{ixt}dt,"Why is the derivative of  $F(x)=\int_{-\infty}^{\infty}e^{ixt}dt$ (with respect to $x$) equal to $i\int_{-\infty}^{\infty} te^{itx}dt$? If I ignore the integral sign, I see that $\frac{d}{dx}e^{itx}=e^{itx}it$ by the chain rule, but I don't see why I am allowed to disregard the integral sign. I don't think the fundamental theorem of calculus applies since due to the limits of integration not being functions of $x$. Edited What conditions have to be checked in order to differentiate this type of function (with imaginary number in integrand) under the integral sign?","Why is the derivative of  $F(x)=\int_{-\infty}^{\infty}e^{ixt}dt$ (with respect to $x$) equal to $i\int_{-\infty}^{\infty} te^{itx}dt$? If I ignore the integral sign, I see that $\frac{d}{dx}e^{itx}=e^{itx}it$ by the chain rule, but I don't see why I am allowed to disregard the integral sign. I don't think the fundamental theorem of calculus applies since due to the limits of integration not being functions of $x$. Edited What conditions have to be checked in order to differentiate this type of function (with imaginary number in integrand) under the integral sign?",,"['calculus', 'derivatives', 'improper-integrals']"
84,Why is the derivative of $\frac{3e^{2x}}{2}$ equal to $3e^{2x}$?,Why is the derivative of  equal to ?,\frac{3e^{2x}}{2} 3e^{2x},$$f(x) = \frac{3e^{2x}}{2}$$ $$f'(x) = 3e^{2x}$$ Why is this true?,$$f(x) = \frac{3e^{2x}}{2}$$ $$f'(x) = 3e^{2x}$$ Why is this true?,,"['calculus', 'derivatives']"
85,How do I find the derivative of $(1 +1/x)^x $,How do I find the derivative of,(1 +1/x)^x ,"I tried one approach but the correction in the book shows me a total different answer. Here's what I did: $(1+ 1/x)^x=xln(1+1/x)$ Thus, now we try to find the derivative of a multiplication: $ u(x)=x$ $(u(x))'=1$ $v(x)=ln(1+1/x)$ $(v(x))'= -1/(x^2) +1/x$ And so: $(uv)'=u'v+uv'$ which gives us: $(uv)'=xln(1 +1/x) +(-1/x^2 -1/x)$ Yet, the correction gives me this as an answer: $ln(1+1/x)-1/(1+x)$","I tried one approach but the correction in the book shows me a total different answer. Here's what I did: $(1+ 1/x)^x=xln(1+1/x)$ Thus, now we try to find the derivative of a multiplication: $ u(x)=x$ $(u(x))'=1$ $v(x)=ln(1+1/x)$ $(v(x))'= -1/(x^2) +1/x$ And so: $(uv)'=u'v+uv'$ which gives us: $(uv)'=xln(1 +1/x) +(-1/x^2 -1/x)$ Yet, the correction gives me this as an answer: $ln(1+1/x)-1/(1+x)$",,['derivatives']
86,Proof from Calculus 1,Proof from Calculus 1,,"Last days, from going into a website of the university of Pisa, I found an exercise given in the previous exams, in 1999. The problem was like: Given a continuous function $f$ in $\mathbb R$, and which satisfies the relation:      $$4\left(f(x)\right)^2   -   4 f(x) + 1 > 0, \forall x \in \mathbb R$$      show that if a point $x_0$ exists such that $f(x_0) = 0,$ then the function has an   upper limit . . I tried to solve that, and think about it, but didn't know how to do it. What could a possible proof be?","Last days, from going into a website of the university of Pisa, I found an exercise given in the previous exams, in 1999. The problem was like: Given a continuous function $f$ in $\mathbb R$, and which satisfies the relation:      $$4\left(f(x)\right)^2   -   4 f(x) + 1 > 0, \forall x \in \mathbb R$$      show that if a point $x_0$ exists such that $f(x_0) = 0,$ then the function has an   upper limit . . I tried to solve that, and think about it, but didn't know how to do it. What could a possible proof be?",,"['derivatives', 'calculus']"
87,Find Polynomial of order 10 for $f(x)=sin(x)$ near x=0,Find Polynomial of order 10 for  near x=0,f(x)=sin(x),My work so far : I presume the answer should look more like a summation? Thanks!,My work so far : I presume the answer should look more like a summation? Thanks!,,"['calculus', 'derivatives', 'taylor-expansion']"
88,"Given that $f(x)=\frac{1}{x^n}$, show that $x f'(x)+n f'(x)=0$.","Given that , show that .",f(x)=\frac{1}{x^n} x f'(x)+n f'(x)=0,"This exercise was in my math book and of course had no solution as it's a ""show"" type of question. I don't see how this could hold except for when $x=-n$. Given that $f(x)=\frac{1}{x^n}$, show that $x f'(x)+n f'(x)=0$.","This exercise was in my math book and of course had no solution as it's a ""show"" type of question. I don't see how this could hold except for when $x=-n$. Given that $f(x)=\frac{1}{x^n}$, show that $x f'(x)+n f'(x)=0$.",,['derivatives']
89,Differentiability of a integral,Differentiability of a integral,,"Define $$L(x)=\int_0^x \left(\sqrt{1+\cos^2(1/u)}\right)du$$, then is L right-differentiable at 0? As far as I'm aware, l'hopitals rule doesn't apply here, but I still think it should be differentiable, because the oscillations speed up as you approach 0.","Define $$L(x)=\int_0^x \left(\sqrt{1+\cos^2(1/u)}\right)du$$, then is L right-differentiable at 0? As far as I'm aware, l'hopitals rule doesn't apply here, but I still think it should be differentiable, because the oscillations speed up as you approach 0.",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
90,Non-differentiable at how many points?,Non-differentiable at how many points?,,"There are two functions whose domain is $[-1/2,2]$ and whose co-domain is the real numbers $\mathbb{R}$. They are $f(x) = \lfloor x^2-3\rfloor$,  where $\lfloor x \rfloor$ denotes the greatest integer less than or equal to $x$, and $g(x) = |x|f(x) + |4x-7|f(x) $. We have to find at how many points $g(x)$ is non-differentiable. I tried but my answer is wrong . Why?","There are two functions whose domain is $[-1/2,2]$ and whose co-domain is the real numbers $\mathbb{R}$. They are $f(x) = \lfloor x^2-3\rfloor$,  where $\lfloor x \rfloor$ denotes the greatest integer less than or equal to $x$, and $g(x) = |x|f(x) + |4x-7|f(x) $. We have to find at how many points $g(x)$ is non-differentiable. I tried but my answer is wrong . Why?",,"['algebra-precalculus', 'derivatives', 'continuity']"
91,Two methods of implicit differentiation don't correspond,Two methods of implicit differentiation don't correspond,,"I recently attempted a question on implicit differentiation twice. I differentiated using one method in the first attempt and then another method in the second attempt but they do not correspond when I plug in values for the variables x and y. Please look at the two methods and tell me what I am doing wrong. In the first attempt I just took the first derivative of each side of the equation with respect to x. In the second attempt, I took the ln of both sides of the equation first and then found the derivatives of either side of the equation. Errata: The answer for the first attempt is y' = y(y - e^(x/y)) / (y ^ 2 - xe^(x/y))","I recently attempted a question on implicit differentiation twice. I differentiated using one method in the first attempt and then another method in the second attempt but they do not correspond when I plug in values for the variables x and y. Please look at the two methods and tell me what I am doing wrong. In the first attempt I just took the first derivative of each side of the equation with respect to x. In the second attempt, I took the ln of both sides of the equation first and then found the derivatives of either side of the equation. Errata: The answer for the first attempt is y' = y(y - e^(x/y)) / (y ^ 2 - xe^(x/y))",,"['calculus', 'derivatives', 'implicit-differentiation']"
92,I want to find whether the expression $D = \sqrt{5t^2 - 40t+125}$ is increasing or decreasing when $t=5$.,I want to find whether the expression  is increasing or decreasing when .,D = \sqrt{5t^2 - 40t+125} t=5,I want to find whether the expression $D = \sqrt{5t^2 - 40t+125}$ is increasing or decreasing when $t=5$. My logic is I want to find whether is $f'(5)>0$ or $f'(5) < 0$. I need to use the chain rule $h'(x) = g'(f(x))f'(x)$ $g'(f(x)) = \frac{1}{2}(5t^2 - 40t+125)^\frac{-1}{2}$ $f'(x) = 10t-40$ $h'(x) = \frac{1}{2}(5t^2 - 40t+125)(10t-40)^\frac{-1}{2}$ This is a non calculator paper and is this really possible without a calculator?,I want to find whether the expression $D = \sqrt{5t^2 - 40t+125}$ is increasing or decreasing when $t=5$. My logic is I want to find whether is $f'(5)>0$ or $f'(5) < 0$. I need to use the chain rule $h'(x) = g'(f(x))f'(x)$ $g'(f(x)) = \frac{1}{2}(5t^2 - 40t+125)^\frac{-1}{2}$ $f'(x) = 10t-40$ $h'(x) = \frac{1}{2}(5t^2 - 40t+125)(10t-40)^\frac{-1}{2}$ This is a non calculator paper and is this really possible without a calculator?,,['derivatives']
93,Confusion on Differential Operators and Notation,Confusion on Differential Operators and Notation,,"My teacher wrote the following on the board: $\frac{d}{dx}: C^1(a,b)\rightarrow C^0(a,b)$ I thought that a 1st order differential operator takes a function which is continuous and maps it to some function space where we don't know the properties (i.e. I don't know that the derivative is continuous). To me, what he wrote looks like it takes a function with a continuous derivative and maps it to a continuous function. Would someone explain this to me more clearly?","My teacher wrote the following on the board: $\frac{d}{dx}: C^1(a,b)\rightarrow C^0(a,b)$ I thought that a 1st order differential operator takes a function which is continuous and maps it to some function space where we don't know the properties (i.e. I don't know that the derivative is continuous). To me, what he wrote looks like it takes a function with a continuous derivative and maps it to a continuous function. Would someone explain this to me more clearly?",,"['calculus', 'linear-algebra', 'functional-analysis', 'derivatives', 'vector-spaces']"
94,If double derivative test fails can we go for Higher derivatives,If double derivative test fails can we go for Higher derivatives,,"if $f(x)=x^4$ we have critical points given by $$4x^3=0$$ which is $x=0$ Now $$f''(x)=12x^2$$ so $$f''(0)=0$$ and also $x=0$ is not Point of Inflexion since $f''(0^{+})$ and $f''(0^{-})$ have same sign. Now if we take triple derivative $$f'''(x)=24x$$ and $$f''''(0) \gt 0$$ Can we say $x=0$ is Local Minima because from graph of $x^4$,Minima occurs at $x=0$","if $f(x)=x^4$ we have critical points given by $$4x^3=0$$ which is $x=0$ Now $$f''(x)=12x^2$$ so $$f''(0)=0$$ and also $x=0$ is not Point of Inflexion since $f''(0^{+})$ and $f''(0^{-})$ have same sign. Now if we take triple derivative $$f'''(x)=24x$$ and $$f''''(0) \gt 0$$ Can we say $x=0$ is Local Minima because from graph of $x^4$,Minima occurs at $x=0$",,"['algebra-precalculus', 'derivatives']"
95,find the rate of change $f(x) = 4\sin^3 x$ when $x = \frac{5\pi}{6}$,find the rate of change  when,f(x) = 4\sin^3 x x = \frac{5\pi}{6},"find the rate of change $f(x) = 4\sin^3 x$ when $x = \frac{5\pi}{6}$ To find the rate of change I need to find $\frac{dy}{dx}$ using the chain rule $h'(x) = g'(f(x)).f'(x)$ $g'(f(x)) = 12\sin^2 x$ $f'(x) = \cos x$ $h'(x) = 12\sin^2 x \cos x$ After this, I am completely stuck, this comes from a paper that does not allow calculators and am I supposed to know what $\frac{5\pi}6$ means and be able to apply it to the derivative function? If so could somebody offer a link or something for me to learn becasue I do not know this.","find the rate of change $f(x) = 4\sin^3 x$ when $x = \frac{5\pi}{6}$ To find the rate of change I need to find $\frac{dy}{dx}$ using the chain rule $h'(x) = g'(f(x)).f'(x)$ $g'(f(x)) = 12\sin^2 x$ $f'(x) = \cos x$ $h'(x) = 12\sin^2 x \cos x$ After this, I am completely stuck, this comes from a paper that does not allow calculators and am I supposed to know what $\frac{5\pi}6$ means and be able to apply it to the derivative function? If so could somebody offer a link or something for me to learn becasue I do not know this.",,"['calculus', 'derivatives']"
96,How to solve the antiderivative of $x\cos\left(x^3\right)$,How to solve the antiderivative of,x\cos\left(x^3\right),"What is the way to solve: $$\int x\cos\left(x^3\right)\space\text{d}x$$ Thanks, I've no idea how to start","What is the way to solve: $$\int x\cos\left(x^3\right)\space\text{d}x$$ Thanks, I've no idea how to start",,['integration']
97,Is the right-hand derivative equal to the right-hand limit of the derivative?,Is the right-hand derivative equal to the right-hand limit of the derivative?,,"Let $f(x)$ be a function on the interval $[a,b]$ which is differentiable on $(a,b)$. Is it true that $$f'_+(a)=\displaystyle\lim_{x\to a^+}f'(x)$$ if both limits exist? Darboux's theorem seems to imply that it is indeed the case, but my idea of proof is somewhat fishy (uses an ``odd extension"" of $f(x)$, etc.) Can anyone confirm or disprove that I'm right? Thanks. Here $f'_+(a):=\displaystyle\lim_{h\to 0^+}\frac{f(a+h)-f(a)}{h}$.","Let $f(x)$ be a function on the interval $[a,b]$ which is differentiable on $(a,b)$. Is it true that $$f'_+(a)=\displaystyle\lim_{x\to a^+}f'(x)$$ if both limits exist? Darboux's theorem seems to imply that it is indeed the case, but my idea of proof is somewhat fishy (uses an ``odd extension"" of $f(x)$, etc.) Can anyone confirm or disprove that I'm right? Thanks. Here $f'_+(a):=\displaystyle\lim_{h\to 0^+}\frac{f(a+h)-f(a)}{h}$.",,"['calculus', 'real-analysis', 'derivatives']"
98,"What's the exterior derivative of the two form: $x \,dy \wedge dz + y \,dx \wedge dz + z\,dx \wedge dy$?",What's the exterior derivative of the two form: ?,"x \,dy \wedge dz + y \,dx \wedge dz + z\,dx \wedge dy","What's the exterior derivative of the two form: $x \,dy \wedge dz + y\, dx \wedge dz + z\,dx \wedge dy$? I saw some calculations on this site and I'm pretty sure that the answer is $3\, dx\wedge  dy\wedge dz$ , however, this conflicts with the answer posted on here, Exterior derivative of a 2-form ,  which I get $ dx\wedge  dy\wedge dz$. How do I do this correctly?  Thanks in advance!","What's the exterior derivative of the two form: $x \,dy \wedge dz + y\, dx \wedge dz + z\,dx \wedge dy$? I saw some calculations on this site and I'm pretty sure that the answer is $3\, dx\wedge  dy\wedge dz$ , however, this conflicts with the answer posted on here, Exterior derivative of a 2-form ,  which I get $ dx\wedge  dy\wedge dz$. How do I do this correctly?  Thanks in advance!",,"['calculus', 'derivatives', 'differential-forms']"
99,100th derivative of $\frac{x^2+x}{2^x}$ at point 0,100th derivative of  at point 0,\frac{x^2+x}{2^x},Problem: $$\frac{\mathrm d^{100}}{\mathrm dx^{100}}\frac{x^2+x}{2^x}$$ Can anyone help me with this because I tried to use General Leibniz rule and I didn't get much better information and I also tried some proof by induction but also no success Without Taylor,Problem: $$\frac{\mathrm d^{100}}{\mathrm dx^{100}}\frac{x^2+x}{2^x}$$ Can anyone help me with this because I tried to use General Leibniz rule and I didn't get much better information and I also tried some proof by induction but also no success Without Taylor,,"['calculus', 'real-analysis', 'derivatives']"
