,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is $\lim\limits_{n \to \infty}\frac{1}{n}\left( \cos{\frac{\pi}{n}} + \cos{\frac{2\pi}{n}} + \ldots + \cos{\frac{n\pi}{n}} \right)$ a Riemann sum?,Is  a Riemann sum?,\lim\limits_{n \to \infty}\frac{1}{n}\left( \cos{\frac{\pi}{n}} + \cos{\frac{2\pi}{n}} + \ldots + \cos{\frac{n\pi}{n}} \right),"This is probably simple, but I'm solving a practice problem: $\lim_{n \to \infty}\frac{1}{n}\left( \cos{\frac{\pi}{n}} + \cos{\frac{2\pi}{n}} + \ldots +\cos{\frac{n\pi}{n}} \right)$ I recognize this as the Riemann sum from 0 to $\pi$ on $\cos{x}$, i.e. I think its the integral $\int_0^\pi{ \cos{x}dx }$ which is 0, but the book I'm using says it should be $ \frac{1}{\pi}\int_0^\pi{ \cos{x}dx }$ Still 0 anyway, but where did the $\frac{1}{\pi}$ in front come from?","This is probably simple, but I'm solving a practice problem: $\lim_{n \to \infty}\frac{1}{n}\left( \cos{\frac{\pi}{n}} + \cos{\frac{2\pi}{n}} + \ldots +\cos{\frac{n\pi}{n}} \right)$ I recognize this as the Riemann sum from 0 to $\pi$ on $\cos{x}$, i.e. I think its the integral $\int_0^\pi{ \cos{x}dx }$ which is 0, but the book I'm using says it should be $ \frac{1}{\pi}\int_0^\pi{ \cos{x}dx }$ Still 0 anyway, but where did the $\frac{1}{\pi}$ in front come from?",,"['calculus', 'integration', 'limits']"
1,Prove : $\int_{{π}/{4}}^{{π}/{2}} e^{\cos x + \cos^2{x}}\mathrm dx>\sqrt2$,Prove :,\int_{{π}/{4}}^{{π}/{2}} e^{\cos x + \cos^2{x}}\mathrm dx>\sqrt2,"Exactly as the title says , we need to prove : $$\int_\frac{π}{4} ^ \frac{π}{2} e^{\cos x + \cos^2{x}}\mathrm dx>\sqrt2$$ My unsuccessful approach: I tried by searching for a function $f$ such that : $$e^{\cos x + \cos^2{x}}>f$$ and $$\int_\frac{π}{4} ^ \frac{π}{2}f\mathrm dx = \sqrt2$$ Since the limits are in terms of $π$ while the integral should be equal to $\sqrt 2$ , the indefinite integral should be of the form having denominator equal to $π$ . Unfortunately, I have never encountered such type of functions or integrals. For the same reasons above, I also cannot think of functions to apply sandwich theorem. I am looking for some elementary methods but all level of answers are welcome. Thanks !","Exactly as the title says , we need to prove : My unsuccessful approach: I tried by searching for a function such that : and Since the limits are in terms of while the integral should be equal to , the indefinite integral should be of the form having denominator equal to . Unfortunately, I have never encountered such type of functions or integrals. For the same reasons above, I also cannot think of functions to apply sandwich theorem. I am looking for some elementary methods but all level of answers are welcome. Thanks !",\int_\frac{π}{4} ^ \frac{π}{2} e^{\cos x + \cos^2{x}}\mathrm dx>\sqrt2 f e^{\cos x + \cos^2{x}}>f \int_\frac{π}{4} ^ \frac{π}{2}f\mathrm dx = \sqrt2 π \sqrt 2 π,"['calculus', 'inequality', 'definite-integrals']"
2,Prove $\zeta(3)=2\sum_{n=1}^\infty\frac{H_n}{n}\left[\frac1{4^n}{2n\choose n}\left(H_{2n}-H_n-\frac1{2n}-\ln2\right)+\frac1{2n}\right]$,Prove,\zeta(3)=2\sum_{n=1}^\infty\frac{H_n}{n}\left[\frac1{4^n}{2n\choose n}\left(H_{2n}-H_n-\frac1{2n}-\ln2\right)+\frac1{2n}\right],"How to prove $$\zeta(3)=2\sum_{n=1}^\infty\frac{H_n}{n}\left[\frac1{4^n}{2n\choose n}\left(H_{2n}-H_n-\frac1{2n}-\ln2\right)+\frac1{2n}\right]$$ where $H_n$ is the harmonic number and $\zeta$ is the Riemann zeta function. This problem is proposed by Cornel which can be found here and no solution has been submitted yet. I know the following identity $$H_{2n}-H_n-\ln2=-\int_0^1\frac{x^{2n}}{1+x}dx$$ is related but I do not know how to exploit it. I prefer a solution without calculating each sum separately because if we seperate, all these sums are calculated here but the first one $\sum_{n=1}^\infty\frac{H_nH_{2n}}{n4^n}{2n\choose n}$ .","How to prove where is the harmonic number and is the Riemann zeta function. This problem is proposed by Cornel which can be found here and no solution has been submitted yet. I know the following identity is related but I do not know how to exploit it. I prefer a solution without calculating each sum separately because if we seperate, all these sums are calculated here but the first one .",\zeta(3)=2\sum_{n=1}^\infty\frac{H_n}{n}\left[\frac1{4^n}{2n\choose n}\left(H_{2n}-H_n-\frac1{2n}-\ln2\right)+\frac1{2n}\right] H_n \zeta H_{2n}-H_n-\ln2=-\int_0^1\frac{x^{2n}}{1+x}dx \sum_{n=1}^\infty\frac{H_nH_{2n}}{n4^n}{2n\choose n},"['calculus', 'integration', 'sequences-and-series', 'riemann-zeta', 'harmonic-numbers']"
3,"How to evaluate the integral $\int_0^{\pi/2}x^2(\sin x+\cos x)^3\sqrt{\sin x\cos x} \, dx$?",How to evaluate the integral ?,"\int_0^{\pi/2}x^2(\sin x+\cos x)^3\sqrt{\sin x\cos x} \, dx","How to evaluate the integral $$\int_0^{\pi/2}x^2(\sin x+\cos x)^3(\sin x\cos x)^{1/2} \, dx \text{ ?}$$ I tried to subsititution $x=\frac{\pi}{2}-t$, but it doesn't work. can someone help me, any hint or trick are appreciated.","How to evaluate the integral $$\int_0^{\pi/2}x^2(\sin x+\cos x)^3(\sin x\cos x)^{1/2} \, dx \text{ ?}$$ I tried to subsititution $x=\frac{\pi}{2}-t$, but it doesn't work. can someone help me, any hint or trick are appreciated.",,"['calculus', 'integration', 'trigonometry', 'closed-form']"
4,Do all continuous functions have antiderivatives?,Do all continuous functions have antiderivatives?,,"If not all continuous functions are differentiable, so how is it that all continuous functions have anti-derivatives?","If not all continuous functions are differentiable, so how is it that all continuous functions have anti-derivatives?",,"['calculus', 'integration', 'derivatives', 'continuity']"
5,Dividing a circle into $3$ equal pieces using $2$ parallel lines,Dividing a circle into  equal pieces using  parallel lines,3 2,"I originally found this question in James Stewart's Calculus, specifically in one of the Problems Plus sections. The question asks how $3$ people can share a pizza while making just $2$ cuts, instead of the usual $3$.  The basic idea is to divide a circle into $3$ equal pieces using $2$ parallel lines that are the same distance from the center, as in the picture below. I am having trouble getting started on this question, mainly because I have no idea how to find the areas of the $3$ pieces.  I can see that the cuts should not be too close to the edges or the center, as this would make middle piece too large or too small respectively.  But I cannot see much more than this.","I originally found this question in James Stewart's Calculus, specifically in one of the Problems Plus sections. The question asks how $3$ people can share a pizza while making just $2$ cuts, instead of the usual $3$.  The basic idea is to divide a circle into $3$ equal pieces using $2$ parallel lines that are the same distance from the center, as in the picture below. I am having trouble getting started on this question, mainly because I have no idea how to find the areas of the $3$ pieces.  I can see that the cuts should not be too close to the edges or the center, as this would make middle piece too large or too small respectively.  But I cannot see much more than this.",,"['calculus', 'area']"
6,Evaluate $\lim_{x\to 1} \frac{p}{1-x^p}-\frac{q}{1-x^q}$,Evaluate,\lim_{x\to 1} \frac{p}{1-x^p}-\frac{q}{1-x^q},"Evaluate $$\lim_{x\to 1} \frac{p}{1-x^p}-\frac{q}{1-x^q}$$   where $p,q$ are Natural Numbers. I tried rationalization, but I wasn't able to get anywhere. I'm not being able to remove the $\infty-\infty$ indeterminate form.  Any help would be appreciated. Many thanks! EDIT: The given answer is $\frac{p-q}{2}$","Evaluate $$\lim_{x\to 1} \frac{p}{1-x^p}-\frac{q}{1-x^q}$$   where $p,q$ are Natural Numbers. I tried rationalization, but I wasn't able to get anywhere. I'm not being able to remove the $\infty-\infty$ indeterminate form.  Any help would be appreciated. Many thanks! EDIT: The given answer is $\frac{p-q}{2}$",,"['calculus', 'limits', 'limits-without-lhopital']"
7,Integration of $\int\frac{\sin^4x+\cos^4x}{\sin^3x+\cos^3x}dx$,Integration of,\int\frac{\sin^4x+\cos^4x}{\sin^3x+\cos^3x}dx,How can we integrate: $$ \int\frac{\sin^4x+\cos^4x}{\sin^3x+\cos^3x}dx $$ Using simple algebraic identities I deduced it to $$ \int\frac{1-2\sin^2x\cdot\cos^2x}{(\sin x+\cos x)(1-\sin x\cdot\cos x)}dx $$ but can't proceed further. Please provide some directions?,How can we integrate: $$ \int\frac{\sin^4x+\cos^4x}{\sin^3x+\cos^3x}dx $$ Using simple algebraic identities I deduced it to $$ \int\frac{1-2\sin^2x\cdot\cos^2x}{(\sin x+\cos x)(1-\sin x\cdot\cos x)}dx $$ but can't proceed further. Please provide some directions?,,"['calculus', 'integration', 'indefinite-integrals']"
8,How should I calculate the $n$th derivative of $f(x)=x^x$?,How should I calculate the th derivative of ?,n f(x)=x^x,"What would be the $n$th derivative of $ f (x) = x ^ x$ I have reached the fifth derivative, very long indeed but I see no pattern that will help me find a general expression. \begin{align*} \frac{df}{dx} &= x^x(1+\ln(x))\\ \frac{d^2f}{dx^2} &= x^x\left(\frac{1}{x}+1+2\ln(x)+\ln(x)^2\right)\\ \frac{d^3f}{dx^3} &= x^x\left( \frac{-1}{x^2}+\frac{3}{x} + \ln(x)^3 + 3 \ln(x)^2 + \frac{3\ln(x)}{x} + 3\ln(x) + 1 \right)\\ \frac{d^4f}{dx^4} &= x^x\left( \frac{2}{x^3}-\frac{1}{x^2}-\frac{4\ln(x)}{x^2}+\frac{6}{x}+\ln(x)^4+4\ln(x)^3\right.\\ &\qquad\qquad\left.+\frac{6\ln(x)^2}{x}+6\ln(x)^2+\frac{12\ln(x)}{x}+4\ln(x)+1 \right)\\ \frac{d^5f}{dx^5} &= x^x\left( \frac{-6}{x^4}+\frac{10\ln(x)}{x^3}+\frac{5}{x^2}-\frac{10\ln(x)^2}{x^2}+\frac{10}{x}+\ln(x)^5+5\ln(x)^4\right.\\ &\qquad\qquad\left.+\frac{10\ln(x)^3}{x}+10\ln(x)^3+\frac{30\ln(x)^2}{x}+10\ln(x)^2+\frac{30\ln(x)}{x}+5\ln(x)+1 \right) \end{align*}","What would be the $n$th derivative of $ f (x) = x ^ x$ I have reached the fifth derivative, very long indeed but I see no pattern that will help me find a general expression. \begin{align*} \frac{df}{dx} &= x^x(1+\ln(x))\\ \frac{d^2f}{dx^2} &= x^x\left(\frac{1}{x}+1+2\ln(x)+\ln(x)^2\right)\\ \frac{d^3f}{dx^3} &= x^x\left( \frac{-1}{x^2}+\frac{3}{x} + \ln(x)^3 + 3 \ln(x)^2 + \frac{3\ln(x)}{x} + 3\ln(x) + 1 \right)\\ \frac{d^4f}{dx^4} &= x^x\left( \frac{2}{x^3}-\frac{1}{x^2}-\frac{4\ln(x)}{x^2}+\frac{6}{x}+\ln(x)^4+4\ln(x)^3\right.\\ &\qquad\qquad\left.+\frac{6\ln(x)^2}{x}+6\ln(x)^2+\frac{12\ln(x)}{x}+4\ln(x)+1 \right)\\ \frac{d^5f}{dx^5} &= x^x\left( \frac{-6}{x^4}+\frac{10\ln(x)}{x^3}+\frac{5}{x^2}-\frac{10\ln(x)^2}{x^2}+\frac{10}{x}+\ln(x)^5+5\ln(x)^4\right.\\ &\qquad\qquad\left.+\frac{10\ln(x)^3}{x}+10\ln(x)^3+\frac{30\ln(x)^2}{x}+10\ln(x)^2+\frac{30\ln(x)}{x}+5\ln(x)+1 \right) \end{align*}",,"['calculus', 'derivatives']"
9,Proving $\lim_{n\to \infty}\int_0^\pi\frac{\sin\left(nx\right)}{1+x^2}dx=0 $,Proving,\lim_{n\to \infty}\int_0^\pi\frac{\sin\left(nx\right)}{1+x^2}dx=0 ,$$\lim_{n\to \infty}\int_0^\pi\frac{\sin\left(nx\right)}{1+x^2}dx=0 $$ I consider if it can be solved by other methods without Riemann lemma. I try my best to do it as follow: Let $t=nx$   $$\lim_{n\to \infty}\int_0^\pi\frac{\sin\left(nx\right)}{1+x^2}dx$$   \begin{eqnarray}  &=&\lim_{n\to \infty}\int_0^{n\pi}\frac{\sin\left(t\right)}{1+(\frac{t}{n})^2}d\frac{t}{n}\\ &=&\lim_{n\to \infty}\int_0^{n\pi}\frac{n\sin\left(t\right)}{n^2+t^2}dt\\ &=&\lim_{n\to \infty}\sum_{k=0}^{k=n-1}\int_{k\pi}^{(k+1)\pi}\frac{n\sin\left(t\right)}{n^2+t^2}dt\\ \end{eqnarray} Then I can not go on. Who can tell me how to prove it? Thank you.,$$\lim_{n\to \infty}\int_0^\pi\frac{\sin\left(nx\right)}{1+x^2}dx=0 $$ I consider if it can be solved by other methods without Riemann lemma. I try my best to do it as follow: Let $t=nx$   $$\lim_{n\to \infty}\int_0^\pi\frac{\sin\left(nx\right)}{1+x^2}dx$$   \begin{eqnarray}  &=&\lim_{n\to \infty}\int_0^{n\pi}\frac{\sin\left(t\right)}{1+(\frac{t}{n})^2}d\frac{t}{n}\\ &=&\lim_{n\to \infty}\int_0^{n\pi}\frac{n\sin\left(t\right)}{n^2+t^2}dt\\ &=&\lim_{n\to \infty}\sum_{k=0}^{k=n-1}\int_{k\pi}^{(k+1)\pi}\frac{n\sin\left(t\right)}{n^2+t^2}dt\\ \end{eqnarray} Then I can not go on. Who can tell me how to prove it? Thank you.,,"['calculus', 'integration', 'limits', 'definite-integrals']"
10,Why does implicit differentiation work on non-functions?,Why does implicit differentiation work on non-functions?,,"I've been reading Keisler's Calculus book, and there's an example where he does implicit differentiation on the equation: $$x^2+y^2=1$$ which yields:$$\frac{dy}{dx}=-\frac{x}{y}$$ I understand the technique of implicit differentiation, and I understand that it's just an application of the chain rule when $y$ is a function of $x$, or $x$ and $y$ are both functions of some third variable. What I don't understand is how you can get the derivative of a non-function. In the case above, $y$ is not a function of $x$, and as far as I know, looking at the definition of the derivative, derivatives only seem to be defined for functions. Sorry for the noob question, but what am I missing here?","I've been reading Keisler's Calculus book, and there's an example where he does implicit differentiation on the equation: $$x^2+y^2=1$$ which yields:$$\frac{dy}{dx}=-\frac{x}{y}$$ I understand the technique of implicit differentiation, and I understand that it's just an application of the chain rule when $y$ is a function of $x$, or $x$ and $y$ are both functions of some third variable. What I don't understand is how you can get the derivative of a non-function. In the case above, $y$ is not a function of $x$, and as far as I know, looking at the definition of the derivative, derivatives only seem to be defined for functions. Sorry for the noob question, but what am I missing here?",,['calculus']
11,Sign of Laplacian at critical points of $\mathbb R^n$,Sign of Laplacian at critical points of,\mathbb R^n,"Suppose we are in $\mathbb R^n$. What can we say about the sign of $\Delta u(\vec x)$ if u($\vec x$) has a local max/min at $\vec x$? I've tried looking at the reverse of the second partial derivative test and it seems to suggest that $\Delta u(\vec x) \leq 0$ if $\vec x$ is a min and $\Delta u(\vec x) \geq 0$ if $\vec x$ is a max. However, I'm not convinced of this result. Can somebody point me towards a statement (theorem, etc.) involving the above. I appreciate the help.","Suppose we are in $\mathbb R^n$. What can we say about the sign of $\Delta u(\vec x)$ if u($\vec x$) has a local max/min at $\vec x$? I've tried looking at the reverse of the second partial derivative test and it seems to suggest that $\Delta u(\vec x) \leq 0$ if $\vec x$ is a min and $\Delta u(\vec x) \geq 0$ if $\vec x$ is a max. However, I'm not convinced of this result. Can somebody point me towards a statement (theorem, etc.) involving the above. I appreciate the help.",,"['calculus', 'multivariable-calculus', 'partial-differential-equations']"
12,Sum of radicals greater than 1,Sum of radicals greater than 1,,"Prove that for every $n,m \in \Bbb N $ $$ \frac{1}{\sqrt[n]{1+m}} + \frac{1}{\sqrt[m]{1+n}} \ge 1 $$","Prove that for every $n,m \in \Bbb N $ $$ \frac{1}{\sqrt[n]{1+m}} + \frac{1}{\sqrt[m]{1+n}} \ge 1 $$",,"['calculus', 'inequality']"
13,Could Residue theorem be seen as a special case of Stokes' theorem?,Could Residue theorem be seen as a special case of Stokes' theorem?,,"Residue theorem in complex analysis is seems like Stokes' theorem in real calculus, so a question arose that could Residue theorem be seen as a special case of  Stokes' theorem?","Residue theorem in complex analysis is seems like Stokes' theorem in real calculus, so a question arose that could Residue theorem be seen as a special case of  Stokes' theorem?",,"['calculus', 'complex-analysis', 'differential-forms']"
14,Quick way to expand $\cos^{-1}(\cos^2 x)$ up to $O(x^2)$,Quick way to expand  up to,\cos^{-1}(\cos^2 x) O(x^2),"For a part of a question, I need to expand $\cos^{-1}(\cos^2 x)$ up to $O(x^2)$ about $x=0$. It took me quite a while to get an incorrect answer. What are some quick and efficient offline (i.e, no alpha) ways to get a good approximation? EDIT: To elaborate a bit more, the first order term can be confidently stated to be $\sqrt{2}x$ as this expression is the length of a hypotenuse for a right spherical triangle, which must reduce to the flat triangle hypotenuse. No ""reasoning"" like this could work for second order term so I had to take the cumbersome way.","For a part of a question, I need to expand $\cos^{-1}(\cos^2 x)$ up to $O(x^2)$ about $x=0$. It took me quite a while to get an incorrect answer. What are some quick and efficient offline (i.e, no alpha) ways to get a good approximation? EDIT: To elaborate a bit more, the first order term can be confidently stated to be $\sqrt{2}x$ as this expression is the length of a hypotenuse for a right spherical triangle, which must reduce to the flat triangle hypotenuse. No ""reasoning"" like this could work for second order term so I had to take the cumbersome way.",,"['calculus', 'taylor-expansion']"
15,Taking the second derivative of a parametric curve [duplicate],Taking the second derivative of a parametric curve [duplicate],,"This question already has answers here : Explanation behind Second Derivative of a Parametric Equation Formula (3 answers) Closed 3 years ago . I understand that for the parametric equations $$\begin{align*}x&=f(t)\\   y&=g(t)\end{align*}$$ If $F(x)$ is the function with parameter removed then $\displaystyle F'(x) = \frac{\text{d}y}{\text{d}t}\big/\frac{\text{d}x}{\text{d}t}$ But the procedure for taking the second derivative is just described as "" replace $y$ with dy/dx "" to get $$\frac{\text{d}^2y}{\text{d}x^2}=\frac{\text{d}}{\text{d}x}\left(\frac{\text{d}y}{\text{d}x}\right)=\frac{\left[\frac{\text{d}}{\text{d}t}\left(\frac{\text{d}y}{\text{d}t}\right)\right]}{\left(\frac{\text{d}x}{\text{d}t}\right)}$$ I don't understand the justification for this step. Not at all. But that's all my book says on the matter then it launches in to plugging things in to this formula, and it seems to work well enough, but I don't know why. I often find answers about question on differentials are beyond my level, I'd really like to get this, it'd mean a lot to me if someone could break it down.","This question already has answers here : Explanation behind Second Derivative of a Parametric Equation Formula (3 answers) Closed 3 years ago . I understand that for the parametric equations $$\begin{align*}x&=f(t)\\   y&=g(t)\end{align*}$$ If $F(x)$ is the function with parameter removed then $\displaystyle F'(x) = \frac{\text{d}y}{\text{d}t}\big/\frac{\text{d}x}{\text{d}t}$ But the procedure for taking the second derivative is just described as "" replace $y$ with dy/dx "" to get $$\frac{\text{d}^2y}{\text{d}x^2}=\frac{\text{d}}{\text{d}x}\left(\frac{\text{d}y}{\text{d}x}\right)=\frac{\left[\frac{\text{d}}{\text{d}t}\left(\frac{\text{d}y}{\text{d}t}\right)\right]}{\left(\frac{\text{d}x}{\text{d}t}\right)}$$ I don't understand the justification for this step. Not at all. But that's all my book says on the matter then it launches in to plugging things in to this formula, and it seems to work well enough, but I don't know why. I often find answers about question on differentials are beyond my level, I'd really like to get this, it'd mean a lot to me if someone could break it down.",,['calculus']
16,Question on conservative fields,Question on conservative fields,,"I'm hoping to really knock out several questions I have in my mind with just this one. I've been doing a lot of practice problems on this topic, and although I get the right answers, I really don't know what the answers mean. So there's a theorem that says If F is a vector field defined on all   of $R^3$ whose component functions   have continuous partial derivatives and   curl F=0, then F is a conservative   vector field. So that's great, but it really doesn't give me an understanding of what that really means. First of all, what does it mean for all of its component functions to have continuous partial derivatives. I mean I know how to determine if they are or not, but what does it mean if they are all continuous or if some are not, and how does that change what a conservative field is? Second, what does it mean that the curl F=0, and why is it such an important occurrence that we gave it a special name such as conservative? And lastly, (this I should know but sadly not), is there a difference in a vector field being defined on all of $R^3$ compared to a vector field being continuous on all of $R^3$. Do those mean the same thing?","I'm hoping to really knock out several questions I have in my mind with just this one. I've been doing a lot of practice problems on this topic, and although I get the right answers, I really don't know what the answers mean. So there's a theorem that says If F is a vector field defined on all   of $R^3$ whose component functions   have continuous partial derivatives and   curl F=0, then F is a conservative   vector field. So that's great, but it really doesn't give me an understanding of what that really means. First of all, what does it mean for all of its component functions to have continuous partial derivatives. I mean I know how to determine if they are or not, but what does it mean if they are all continuous or if some are not, and how does that change what a conservative field is? Second, what does it mean that the curl F=0, and why is it such an important occurrence that we gave it a special name such as conservative? And lastly, (this I should know but sadly not), is there a difference in a vector field being defined on all of $R^3$ compared to a vector field being continuous on all of $R^3$. Do those mean the same thing?",,['calculus']
17,Convergence of $ I=\int_0^\infty \sin x\sin(x^2)\mathrm{d}x$,Convergence of, I=\int_0^\infty \sin x\sin(x^2)\mathrm{d}x,"I am trying to prove that the improper integral $$ I=\int_0^\infty \sin x\sin(x^2)\mathrm{d}x$$ converges. Here's my work: It suffices to show that $$\int_\frac{\pi}{2}^\infty \sin (x) \sin(x^2)\mathrm{d}x$$ converges. Using integration by parts, \begin{align*} \int_\frac{\pi}{2}^\infty \sin (t) \sin(t^2)\mathrm{d}t&=\int_\frac{\pi}{2}^\infty \frac{\sin (t)}{2t}\cdot 2t\sin(t^2)\mathrm{d}t\\ &=\underline{\bigg[ -\frac{\sin (t)}{2t}\cos(t^2)\bigg]_\frac{\pi}{2}^\infty}+{\int_\frac{\pi}{2}^\infty \left(\frac{\sin (t)}{2t}\right)'\cos(t^2)\mathrm{d}t} \end{align*} The underlined part is a constant... Then I got stuck. I'd like to use ""sandwich rule"" using the fact that $-1\leq \cos(t^2)\leq 1$, but I can't find a way to apply it properly. How can I proceed from here? Any correction and/or help would be appreciated. :)","I am trying to prove that the improper integral $$ I=\int_0^\infty \sin x\sin(x^2)\mathrm{d}x$$ converges. Here's my work: It suffices to show that $$\int_\frac{\pi}{2}^\infty \sin (x) \sin(x^2)\mathrm{d}x$$ converges. Using integration by parts, \begin{align*} \int_\frac{\pi}{2}^\infty \sin (t) \sin(t^2)\mathrm{d}t&=\int_\frac{\pi}{2}^\infty \frac{\sin (t)}{2t}\cdot 2t\sin(t^2)\mathrm{d}t\\ &=\underline{\bigg[ -\frac{\sin (t)}{2t}\cos(t^2)\bigg]_\frac{\pi}{2}^\infty}+{\int_\frac{\pi}{2}^\infty \left(\frac{\sin (t)}{2t}\right)'\cos(t^2)\mathrm{d}t} \end{align*} The underlined part is a constant... Then I got stuck. I'd like to use ""sandwich rule"" using the fact that $-1\leq \cos(t^2)\leq 1$, but I can't find a way to apply it properly. How can I proceed from here? Any correction and/or help would be appreciated. :)",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
18,A hard integral,A hard integral,,"Looking for a solution for an integral: $$I(k)=\int_0^{\infty } \frac{e^{-\frac{(\log (u)-k)^2}{2 s^2}}}{\sqrt{2 \pi } s \left(1+u\right)} \, du .$$ So far I tried substitutions and by parts to no avail.","Looking for a solution for an integral: $$I(k)=\int_0^{\infty } \frac{e^{-\frac{(\log (u)-k)^2}{2 s^2}}}{\sqrt{2 \pi } s \left(1+u\right)} \, du .$$ So far I tried substitutions and by parts to no avail.",,"['calculus', 'probability-distributions', 'definite-integrals', 'normal-distribution', 'gamma-function']"
19,Spivak or Apostol?,Spivak or Apostol?,,"Which one of those is the best for a person interested in pure mathematics and who wants to have a deep understanding of calculus? Apostol or Spivak? Could you guys tell me some differences between the approaches of them? What about the exercises? I would like to be challenged, but in a constructive way.","Which one of those is the best for a person interested in pure mathematics and who wants to have a deep understanding of calculus? Apostol or Spivak? Could you guys tell me some differences between the approaches of them? What about the exercises? I would like to be challenged, but in a constructive way.",,"['calculus', 'reference-request', 'book-recommendation']"
20,Differential Equation Math Puzzle,Differential Equation Math Puzzle,,"Dog race: Edit 2: I posted a possible answer below. However, I am unsure how the authors arrived at the solution. Maybe someone can offer an explanation. Four dogs are positioned at the corners of a square ($d= 1m$), chase each other in clockwise direction with the same constant speed . As their target is moving, they will follow a curved path, eventually colliding in the center of the square. (a) Why is the total length of the path just $1 m$? (b) Find and solve a differential equation for the radius $r(\theta)$ in polar coordinates. This is a homework question! I just want hints, no solutions please. The tough part is setting up an equation for the radius i.e for the motion of one of the dogs. I was thinking about an equation similar to that of an archimedean spiral. $$r(\theta)=a+b\theta \space \space \space \text{or} \space \space \space r(\theta)=a\theta^{\frac{1}{n}}$$ However, I have no idea what values $a$ or $b$ should be. Any hints are appreciated. Edit 1: This is my second attempt at a solution: If Dog 1 is positioned at $(r, \theta)$ $\implies$ Dog 2 is positioned at $(r, \theta+\frac{\pi}{2})$ Picture: $$x_1=r \cos (\theta) \\ y_1= r \sin (\theta) \\ \\ \\ x_2=r \cos (\theta+\frac{\pi}{2})=-r \sin(\theta) \\y_2=r \sin (\theta+\frac{\pi}{2})=r \cos (\theta)$$ If these are the two position vectors then the vector joining the two points is my velocity vector. $$\implies \frac{dy}{dx}=\frac{y_2-y_1}{x_2-x_1}=\frac{r \sin (\theta+\frac{\pi}{2})-r \sin (\theta)}{r \cos (\theta+\frac{\pi}{2})-r \cos (\theta)}=\frac{ \sin (\theta+\frac{\pi}{2})- \sin (\theta)}{ \cos (\theta+\frac{\pi}{2})- \cos (\theta)}=\frac{\cos(\theta)-\sin(\theta)}{-  \sin(\theta)-\cos(\theta)}$$ Am I on the right path? But How do I deduce $\dfrac{dr}{d\theta}$?","Dog race: Edit 2: I posted a possible answer below. However, I am unsure how the authors arrived at the solution. Maybe someone can offer an explanation. Four dogs are positioned at the corners of a square ($d= 1m$), chase each other in clockwise direction with the same constant speed . As their target is moving, they will follow a curved path, eventually colliding in the center of the square. (a) Why is the total length of the path just $1 m$? (b) Find and solve a differential equation for the radius $r(\theta)$ in polar coordinates. This is a homework question! I just want hints, no solutions please. The tough part is setting up an equation for the radius i.e for the motion of one of the dogs. I was thinking about an equation similar to that of an archimedean spiral. $$r(\theta)=a+b\theta \space \space \space \text{or} \space \space \space r(\theta)=a\theta^{\frac{1}{n}}$$ However, I have no idea what values $a$ or $b$ should be. Any hints are appreciated. Edit 1: This is my second attempt at a solution: If Dog 1 is positioned at $(r, \theta)$ $\implies$ Dog 2 is positioned at $(r, \theta+\frac{\pi}{2})$ Picture: $$x_1=r \cos (\theta) \\ y_1= r \sin (\theta) \\ \\ \\ x_2=r \cos (\theta+\frac{\pi}{2})=-r \sin(\theta) \\y_2=r \sin (\theta+\frac{\pi}{2})=r \cos (\theta)$$ If these are the two position vectors then the vector joining the two points is my velocity vector. $$\implies \frac{dy}{dx}=\frac{y_2-y_1}{x_2-x_1}=\frac{r \sin (\theta+\frac{\pi}{2})-r \sin (\theta)}{r \cos (\theta+\frac{\pi}{2})-r \cos (\theta)}=\frac{ \sin (\theta+\frac{\pi}{2})- \sin (\theta)}{ \cos (\theta+\frac{\pi}{2})- \cos (\theta)}=\frac{\cos(\theta)-\sin(\theta)}{-  \sin(\theta)-\cos(\theta)}$$ Am I on the right path? But How do I deduce $\dfrac{dr}{d\theta}$?",,"['calculus', 'ordinary-differential-equations']"
21,Closed form of factorial and cascading power sum,Closed form of factorial and cascading power sum,,Consider the following sum: $$ \sum_{i =0}^{j} \left( \frac{(j-i)^ix^i \ln(x)^{(j-i)}\ln(x)^i}{(j-i)!i!} \right) $$ I can simplify the sum to: $$ \ln(x)^j\sum_{i =0}^{j} \left( \frac{(j-i)^ix^i}{(j-i)!i!} \right) $$  Furthermore I can observe that $$ \frac{1}{(j-i)!(i!)}  = \frac{1}{j!} \begin{pmatrix} j \\ i\end{pmatrix} $$ Thus: $$ \frac{\ln(x)^j}{j!}\sum_{i =0}^{j} \left( \begin{pmatrix}j \\ i \end{pmatrix}(j-i)^ix^i \right) $$ But I don't know how to go in for the kill.,Consider the following sum: $$ \sum_{i =0}^{j} \left( \frac{(j-i)^ix^i \ln(x)^{(j-i)}\ln(x)^i}{(j-i)!i!} \right) $$ I can simplify the sum to: $$ \ln(x)^j\sum_{i =0}^{j} \left( \frac{(j-i)^ix^i}{(j-i)!i!} \right) $$  Furthermore I can observe that $$ \frac{1}{(j-i)!(i!)}  = \frac{1}{j!} \begin{pmatrix} j \\ i\end{pmatrix} $$ Thus: $$ \frac{\ln(x)^j}{j!}\sum_{i =0}^{j} \left( \begin{pmatrix}j \\ i \end{pmatrix}(j-i)^ix^i \right) $$ But I don't know how to go in for the kill.,,"['calculus', 'sequences-and-series', 'combinatorics', 'generating-functions', 'hypergeometric-function']"
22,Why doesn't it work when I calculate the second order derivative?,Why doesn't it work when I calculate the second order derivative?,,Let $y=y(x)$ be determined by the equation \begin{align*}\begin{cases} x=t-\sin{t}\\ y=1-\cos{t}.\end{cases} \end{align*}  I understand the solution:   $$\frac{d^2y}{dx^2}=\frac{d(\frac{dy}{dx})}{dt}\frac{1}{\frac{dx}{dt}}=-\frac{1}{(1-\cos{t})^2}$$ But what's wrong with the following calculations:                       $$\frac{d^2y}{dx^2}=\frac{d^2y}{dt^2}\frac{dt^2}{dx^2}=\frac{\cos{t}dt^2}{dt^2}\frac{dt^2}{((1-\cos{t})dt)^2}=\frac{\cos{t}}{(1-\cos{t})^2}$$,Let $y=y(x)$ be determined by the equation \begin{align*}\begin{cases} x=t-\sin{t}\\ y=1-\cos{t}.\end{cases} \end{align*}  I understand the solution:   $$\frac{d^2y}{dx^2}=\frac{d(\frac{dy}{dx})}{dt}\frac{1}{\frac{dx}{dt}}=-\frac{1}{(1-\cos{t})^2}$$ But what's wrong with the following calculations:                       $$\frac{d^2y}{dx^2}=\frac{d^2y}{dt^2}\frac{dt^2}{dx^2}=\frac{\cos{t}dt^2}{dt^2}\frac{dt^2}{((1-\cos{t})dt)^2}=\frac{\cos{t}}{(1-\cos{t})^2}$$,,['calculus']
23,"How to evaluate $ \int_0^1 {\log x \log(1-x) \log^2(1+x) \over x} \,dx $ [duplicate]",How to evaluate  [duplicate]," \int_0^1 {\log x \log(1-x) \log^2(1+x) \over x} \,dx ","This question already has answers here : A Challenging Logarithmic Integral $\int_0^1 \frac{\log(x)\log(1-x)\log^2(1+x)}{x}dx$ (4 answers) Closed 9 years ago . Solve that the following integral: $$ \int_0^1 {\log x \log(1-x) \log^2(1+x) \over x} \,dx. $$ I haven't solved it yet.","This question already has answers here : A Challenging Logarithmic Integral $\int_0^1 \frac{\log(x)\log(1-x)\log^2(1+x)}{x}dx$ (4 answers) Closed 9 years ago . Solve that the following integral: $$ \int_0^1 {\log x \log(1-x) \log^2(1+x) \over x} \,dx. $$ I haven't solved it yet.",,"['calculus', 'integration']"
24,"Integration of sawtooth, square and triangle wave functions","Integration of sawtooth, square and triangle wave functions",,"Context After a discussion about how to plot the results of a frequency modulation between two signals on Stack Overflow, I understood that I need to find the time-integral of the following wave functions before using them in the general FM formula (as illustrated in the first answer ). Research Integrating a sine wave function is indeed easy, but things gets a lot complicated when it comes to other waveforms. Here follow the equations I'm using to display the waveforms: Sawtooth wave: $ f(x) = \bmod(f_c x, 1.0); $ Square wave: $ f(x) = \operatorname{sign}(\cos(f_c x)); $ Triangle wave: $ f(x) = \frac{1}{f_c}|\bmod(x, f_c) - \frac{1}{2}f_c|$ These functions looks right, but as I don't have any particular background in mathematics or calculus I won't be surprised if I made some bad mistakes. Please be patient. Questions Is there a better way to describe mathematically the wave functions above? If these are right, what is the correct time-integral? Updates Thanks to the the functions with period $T$ in the form Rahul suggested I get: $$\begin{align}\operatorname{sawtooth}(x) = \int_0^x \frac{2x}T-1 \ \mathrm dx &= \frac{x(x - T)}T \end{align}$$ $$\begin{align} \operatorname{square}(x) &= \int_0^x \begin{cases}1&\text{if } x<T/2\\-1&\text{if }x\ge T/2\end{cases} \ \mathrm dx &= \begin{cases}x&\text{if } x<T/2\\-x&\text{if }x\ge T/2\end{cases} \end{align}$$ $$\begin{align} \operatorname{triangle}(x) &= \int_0^x \begin{cases}\frac{4x}T-1&\text{if } x<T/2\\3-\frac{4x}T&\text{if }x\ge T/2\end{cases} \ \mathrm dx &= \begin{cases}x(\frac{2x}T-1)&\text{if } x<T/2\\x(3-\frac{2x}T)&\text{if }x\ge T/2\end{cases} \end{align}$$ By using a modulo operator it's easy to make them periodic $f(x) = \operatorname{sawtooth}(x \% T)$ and they all work as expected when placed as modulators in the frequency modulation equation: $$\begin{align} f(x) = \cos\left(2\pi f_c x + 2\pi f_\Delta \int_0^xg(x)\,\mathrm dx\right) \end{align}$$","Context After a discussion about how to plot the results of a frequency modulation between two signals on Stack Overflow, I understood that I need to find the time-integral of the following wave functions before using them in the general FM formula (as illustrated in the first answer ). Research Integrating a sine wave function is indeed easy, but things gets a lot complicated when it comes to other waveforms. Here follow the equations I'm using to display the waveforms: Sawtooth wave: $ f(x) = \bmod(f_c x, 1.0); $ Square wave: $ f(x) = \operatorname{sign}(\cos(f_c x)); $ Triangle wave: $ f(x) = \frac{1}{f_c}|\bmod(x, f_c) - \frac{1}{2}f_c|$ These functions looks right, but as I don't have any particular background in mathematics or calculus I won't be surprised if I made some bad mistakes. Please be patient. Questions Is there a better way to describe mathematically the wave functions above? If these are right, what is the correct time-integral? Updates Thanks to the the functions with period $T$ in the form Rahul suggested I get: $$\begin{align}\operatorname{sawtooth}(x) = \int_0^x \frac{2x}T-1 \ \mathrm dx &= \frac{x(x - T)}T \end{align}$$ $$\begin{align} \operatorname{square}(x) &= \int_0^x \begin{cases}1&\text{if } x<T/2\\-1&\text{if }x\ge T/2\end{cases} \ \mathrm dx &= \begin{cases}x&\text{if } x<T/2\\-x&\text{if }x\ge T/2\end{cases} \end{align}$$ $$\begin{align} \operatorname{triangle}(x) &= \int_0^x \begin{cases}\frac{4x}T-1&\text{if } x<T/2\\3-\frac{4x}T&\text{if }x\ge T/2\end{cases} \ \mathrm dx &= \begin{cases}x(\frac{2x}T-1)&\text{if } x<T/2\\x(3-\frac{2x}T)&\text{if }x\ge T/2\end{cases} \end{align}$$ By using a modulo operator it's easy to make them periodic $f(x) = \operatorname{sawtooth}(x \% T)$ and they all work as expected when placed as modulators in the frequency modulation equation: $$\begin{align} f(x) = \cos\left(2\pi f_c x + 2\pi f_\Delta \int_0^xg(x)\,\mathrm dx\right) \end{align}$$",,"['calculus', 'integration', 'signal-processing']"
25,Generalized binomial theorem,Generalized binomial theorem,,"Prove that:   $$(1+x)^{\alpha}=\sum_{n=0}^{+\infty}{\alpha \choose n} x^n$$   for $x\in[0;1), \alpha \in\mathbb{R}$ based on Taylor's theorem with Lagrange remainder. I don't feel such proofs. Isn't it kind of.. obvious? That when we expand $(1+x)^{\alpha}$ into series we get right side? I really want to understand this.","Prove that:   $$(1+x)^{\alpha}=\sum_{n=0}^{+\infty}{\alpha \choose n} x^n$$   for $x\in[0;1), \alpha \in\mathbb{R}$ based on Taylor's theorem with Lagrange remainder. I don't feel such proofs. Isn't it kind of.. obvious? That when we expand $(1+x)^{\alpha}$ into series we get right side? I really want to understand this.",,"['calculus', 'taylor-expansion']"
26,Integrating a product of exponentials and error functions,Integrating a product of exponentials and error functions,,"I have the following integral  $$ \int\limits_0^\infty x^2\exp(-\delta x^2)\operatorname{erf}(\gamma x)\,dx. $$ Ideally, I would like a closed-form in terms of common functions, but a series answer will do.","I have the following integral  $$ \int\limits_0^\infty x^2\exp(-\delta x^2)\operatorname{erf}(\gamma x)\,dx. $$ Ideally, I would like a closed-form in terms of common functions, but a series answer will do.",,"['calculus', 'integration', 'special-functions', 'definite-integrals']"
27,Derivative commuting over integral,Derivative commuting over integral,,Can a derivative operation commute over an integral operation irrespective of the properties of the function under the integral ?,Can a derivative operation commute over an integral operation irrespective of the properties of the function under the integral ?,,"['calculus', 'integration']"
28,Show that $\sum_{n=1}^{+\infty}\frac{1}{(n\cdot\sinh(n\pi))^2} = \frac{2}{3}\sum_{n=1}^{+\infty}\frac{(-1)^{n-1}}{(2n-1)^2} - \frac{11\pi^2}{180}$,Show that,\sum_{n=1}^{+\infty}\frac{1}{(n\cdot\sinh(n\pi))^2} = \frac{2}{3}\sum_{n=1}^{+\infty}\frac{(-1)^{n-1}}{(2n-1)^2} - \frac{11\pi^2}{180},"What I do so far \begin{align*} \text{Show that} \quad &\sum_{n=1}^{+\infty}\frac{1}{(n\cdot\sinh(n\pi))^2} = \frac{2}{3}\sum_{n=1}^{+\infty}\frac{(-1)^{n-1}}{(2n-1)^2} - \frac{11\pi^2}{180} \\ \text{Lemma 1 } &\sum_{n = - \infty }^\infty \frac{1}{{z + n}} = \frac{\pi }{{\tan (\pi z)}} \\ \text{Lemma 2 } &\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{\pi^2 z^2}} + \frac{4z^2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{z^2 + k^2}} \\ &\text{Because:} \nonumber \\ &\frac{\pi }{{\tan (\pi z)}} = \sum_{k = - \infty }^\infty \frac{1}{{z + k}} \Rightarrow \left( \frac{\pi }{{\tan (\pi z)}} \right)' = -\sum_{k = - \infty }^\infty \frac{1}{{(z + k)^2}} \nonumber \\ &\Rightarrow \boxed{\frac{\pi^2}{\sin^2(\pi z)}} = \sum_{k = - \infty }^\infty \frac{1}{{(z + k)^2}} \Rightarrow \nonumber \\ &\Rightarrow \frac{\pi^2}{\sin^2(\pi iz)} = \sum_{k = - \infty }^\infty \frac{1}{{(iz + k)^2}} \Rightarrow \frac{\pi^2}{\sinh^2(\pi z)} = \sum_{k = - \infty }^\infty \frac{1}{{(iz + k)^2}} \Rightarrow \boxed{\frac{\pi^2}{\sinh^2(\pi z)} = \sum_{k = - \infty }^\infty \frac{1}{{(z + ik)^2}} = \sum_{k = - \infty }^\infty \frac{1}{{(z - ik)^2}}} \nonumber \\ &\text{then} \nonumber \\ &\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{2\pi^2}}\sum_{k = - \infty }^\infty \left( \frac{1}{{(z + ik)^2}} + \frac{1}{{(z - ik)^2}} \right) \nonumber \\ &= \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{z^2 - k^2}{{(z^2 + k^2)^2}} = \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{2z^2 - (z^2 + k^2)}{{(z^2 + k^2)^2}} \nonumber \\ &= \frac{1}{{\pi^2}} \left( 2z^2\sum_{k = - \infty }^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{1}{{z^2 + k^2}} \right) \nonumber \\ &\Rightarrow \boxed{\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{\pi^2 z^2}} + \frac{4z^2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{z^2 + k^2}}} \end{align*} We replace $z$ with $n$ and sum, so it results: \begin{align*} &\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} \\ &= \frac{1}{\pi^2}\sum_{n=1}^\infty \frac{1}{n^4} + \frac{4}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} - \frac{2}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)} \end{align*} But it is known that $$ \frac{1}{\pi^2}\sum_{n=1}^\infty \frac{1}{n^4} = \frac{1}{\pi^2}\cdot \zeta(4) = \frac{\pi^2}{90} $$ for the sum $$ \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)} $$ we have: \begin{align*} S &= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)} \\ &= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2}\left(\frac{1}{n^2} - \frac{1}{n^2+k^2}\right) \\ &= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2}\frac{1}{n^2} - \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2(n^2+k^2)} \\ &\Rightarrow S = \left(\sum_{n=1}^\infty \frac{1}{n^2}\right)\left(\sum_{k=1}^\infty \frac{1}{k^2}\right) - S \\ &\Rightarrow 2S = \left(\frac{\pi^2}{6}\right)^2 \\ &\Rightarrow S = \frac{\pi^4}{72} \end{align*} So finally $$ \boxed{\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} = \frac{4}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} - \frac{\pi^2}{60}} $$ The final double sum is related to functions $\zeta(2)$ & $\zeta(4)$ that are missing all add-ins of natural ones that are not written as sum of two squares. From Fermat's theorem (proved by Euler) we know that if some natural number has a prime factor of form $p = 4k + 3$ raised to odd force, then it is not written as the sum of two squares. I read that $$ \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} = \zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4) $$ somewhere, but I have no proof of this, nor have I been able to discover any. Given the above: \begin{align*} &\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} \\ &= \frac{4}{\pi^2}\left(\zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)\right) - \frac{\pi^2}{60} \\ &= \frac{4}{\pi^2}\left(\frac{\pi^2}{6} \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \frac{\pi^4}{90}\right) - \frac{\pi^2}{60} \\ &= \frac{2}{3}\sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \left(\frac{2\pi^2}{45} + \frac{\pi^2}{60}\right) \\ &= \frac{2}{3}\sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \frac{11\pi^2}{180} \end{align*} I would be very interested to see a proof of the relationship $$ \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} = \zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4) $$ which is true (tested with handmade program in Visual Basic).","What I do so far We replace with and sum, so it results: But it is known that for the sum we have: So finally The final double sum is related to functions & that are missing all add-ins of natural ones that are not written as sum of two squares. From Fermat's theorem (proved by Euler) we know that if some natural number has a prime factor of form raised to odd force, then it is not written as the sum of two squares. I read that somewhere, but I have no proof of this, nor have I been able to discover any. Given the above: I would be very interested to see a proof of the relationship which is true (tested with handmade program in Visual Basic).","\begin{align*}
\text{Show that} \quad &\sum_{n=1}^{+\infty}\frac{1}{(n\cdot\sinh(n\pi))^2} = \frac{2}{3}\sum_{n=1}^{+\infty}\frac{(-1)^{n-1}}{(2n-1)^2} - \frac{11\pi^2}{180} \\
\text{Lemma 1 } &\sum_{n = - \infty }^\infty \frac{1}{{z + n}} = \frac{\pi }{{\tan (\pi z)}} \\
\text{Lemma 2 } &\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{\pi^2 z^2}} + \frac{4z^2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{z^2 + k^2}} \\
&\text{Because:} \nonumber \\
&\frac{\pi }{{\tan (\pi z)}} = \sum_{k = - \infty }^\infty \frac{1}{{z + k}} \Rightarrow \left( \frac{\pi }{{\tan (\pi z)}} \right)' = -\sum_{k = - \infty }^\infty \frac{1}{{(z + k)^2}} \nonumber \\
&\Rightarrow \boxed{\frac{\pi^2}{\sin^2(\pi z)}} = \sum_{k = - \infty }^\infty \frac{1}{{(z + k)^2}} \Rightarrow \nonumber \\
&\Rightarrow \frac{\pi^2}{\sin^2(\pi iz)} = \sum_{k = - \infty }^\infty \frac{1}{{(iz + k)^2}} \Rightarrow \frac{\pi^2}{\sinh^2(\pi z)} = \sum_{k = - \infty }^\infty \frac{1}{{(iz + k)^2}} \Rightarrow \boxed{\frac{\pi^2}{\sinh^2(\pi z)} = \sum_{k = - \infty }^\infty \frac{1}{{(z + ik)^2}} = \sum_{k = - \infty }^\infty \frac{1}{{(z - ik)^2}}} \nonumber \\
&\text{then} \nonumber \\
&\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{2\pi^2}}\sum_{k = - \infty }^\infty \left( \frac{1}{{(z + ik)^2}} + \frac{1}{{(z - ik)^2}} \right) \nonumber \\
&= \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{z^2 - k^2}{{(z^2 + k^2)^2}} = \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{2z^2 - (z^2 + k^2)}{{(z^2 + k^2)^2}} \nonumber \\
&= \frac{1}{{\pi^2}} \left( 2z^2\sum_{k = - \infty }^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{1}{{\pi^2}}\sum_{k = - \infty }^\infty \frac{1}{{z^2 + k^2}} \right) \nonumber \\
&\Rightarrow \boxed{\frac{1}{{\sinh^2(\pi z)}} = \frac{1}{{\pi^2 z^2}} + \frac{4z^2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{(z^2 + k^2)^2}} - \frac{2}{{\pi^2}}\sum_{k=1}^\infty \frac{1}{{z^2 + k^2}}}
\end{align*} z n \begin{align*}
&\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} \\
&= \frac{1}{\pi^2}\sum_{n=1}^\infty \frac{1}{n^4} + \frac{4}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} - \frac{2}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)}
\end{align*} 
\frac{1}{\pi^2}\sum_{n=1}^\infty \frac{1}{n^4} = \frac{1}{\pi^2}\cdot \zeta(4) = \frac{\pi^2}{90}
 
\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)}
 \begin{align*}
S &= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{n^2(n^2+k^2)} \\
&= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2}\left(\frac{1}{n^2} - \frac{1}{n^2+k^2}\right) \\
&= \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2}\frac{1}{n^2} - \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{k^2(n^2+k^2)} \\
&\Rightarrow S = \left(\sum_{n=1}^\infty \frac{1}{n^2}\right)\left(\sum_{k=1}^\infty \frac{1}{k^2}\right) - S \\
&\Rightarrow 2S = \left(\frac{\pi^2}{6}\right)^2 \\
&\Rightarrow S = \frac{\pi^4}{72}
\end{align*} 
\boxed{\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} = \frac{4}{\pi^2}\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} - \frac{\pi^2}{60}}
 \zeta(2) \zeta(4) p = 4k + 3 
\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} = \zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)
 \begin{align*}
&\sum_{n=1}^\infty \frac{1}{n^2\sinh^2(\pi n)} \\
&= \frac{4}{\pi^2}\left(\zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)\right) - \frac{\pi^2}{60} \\
&= \frac{4}{\pi^2}\left(\frac{\pi^2}{6} \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \frac{\pi^4}{90}\right) - \frac{\pi^2}{60} \\
&= \frac{2}{3}\sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \left(\frac{2\pi^2}{45} + \frac{\pi^2}{60}\right) \\
&= \frac{2}{3}\sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \frac{11\pi^2}{180}
\end{align*} 
\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{1}{(n^2+k^2)^2} = \zeta(2) \cdot \sum_{n=1}^\infty (-1)^{n-1}\frac{1}{(2n-1)^2} - \zeta(4)
","['calculus', 'sequences-and-series', 'summation']"
29,Integrate $\int \sqrt{(x+a)(x+b)} \space dx$,Integrate,\int \sqrt{(x+a)(x+b)} \space dx,"Integrate $\int \sqrt{(x+a)(x+b)} \space dx$ I've tried to use first Euler substitution: $$\sqrt{x^2 + (a+b)x + ab} = x + t \implies x = \frac{t^2 - ab}{a + b - 2t}$$ $$dx = \frac{-2t^2 + 2(a+b)t - 2ab}{(a + b - 2t)^2}dt$$ Then: $$(x + a) = \frac{t^2 - 2at + a^2}{a + b - 2t} = \frac{(t-a)^2}{a + b - 2t}$$ $$(x + b) = \frac{t^2 - 2bt + b^2}{a + b - 2t} = \frac{(t-b)^2}{a + b - 2t}$$ $$\sqrt{(x+a)(x+b)} = \frac{(t-a)(t-b)}{a+b-2t} = \frac{t^2 - (a+b)t + ab}{a + b - 2t}$$ Then we have multiplication by $dx$ : $$\int \frac{t^2 - (a+b)t + ab}{a + b - 2t} \cdot \frac{-2t^2 + 2(a+b)t - 2ab}{(a + b - 2t)^2}dt = -2 \int \frac{(t^2 -(a+b)t + ab)^2}{(a+b - 2t)^3}dt$$ Substitution $ t = \sqrt{(x+a)(x+b)} - x$ doesn't help. Second Euler substitution $\sqrt{t^2 - (a+b)t + ab} = t + l$ won't help too. Okay, another idea. $$(x+a)(x+b) =\left (x + \frac{a+b}{2}\right)^2 - \left(\frac{a-b}{2}\right)^2$$ Then $$u =  x + \frac{a+b}{2}, dx = du  ; \sqrt{u^2 - (\frac{a -b}{2})^2} = \frac{a-b}{2}\sqrt{\frac{4 u^2}{(a-b)^2} - 1}$$ If we substitute $s = \sec^{-1}  \frac{2u}{a-b}$ : $$\frac{a-b}{2}\sqrt{\frac{4 u^2}{(a-b)^2} - 1} = \frac{a-b}{2}\sqrt{\sec^2 s - 1} = \frac{a-b}{2} \tan s$$ As for the $ds$ : $$\sec s = \frac{2u}{a-b} \implies u = \frac{(a-b)}{2} \sec(s)$$ $$ du = \left(\frac{(a-b)}{2} \sec(s)\right)' du = \frac{(a-b)}{2} \sec u \tan u \cdot ds$$ As a result: $$\int \sqrt{(x+a)(x+b)} dx = \int \frac{(a-b)^2}{4} \tan s \tan u \sec u \cdot ds$$ Something is wrong. I have a formula for finding this integral, but still I would like to ""understand"" this problem. I want to find it step by step. Any ideas for solution?","Integrate I've tried to use first Euler substitution: Then: Then we have multiplication by : Substitution doesn't help. Second Euler substitution won't help too. Okay, another idea. Then If we substitute : As for the : As a result: Something is wrong. I have a formula for finding this integral, but still I would like to ""understand"" this problem. I want to find it step by step. Any ideas for solution?","\int \sqrt{(x+a)(x+b)} \space dx \sqrt{x^2 + (a+b)x + ab} = x + t \implies x = \frac{t^2 - ab}{a + b - 2t} dx = \frac{-2t^2 + 2(a+b)t - 2ab}{(a + b - 2t)^2}dt (x + a) = \frac{t^2 - 2at + a^2}{a + b - 2t} = \frac{(t-a)^2}{a + b - 2t} (x + b) = \frac{t^2 - 2bt + b^2}{a + b - 2t} = \frac{(t-b)^2}{a + b - 2t} \sqrt{(x+a)(x+b)} = \frac{(t-a)(t-b)}{a+b-2t} = \frac{t^2 - (a+b)t + ab}{a + b - 2t} dx \int \frac{t^2 - (a+b)t + ab}{a + b - 2t} \cdot \frac{-2t^2 + 2(a+b)t - 2ab}{(a + b - 2t)^2}dt = -2 \int \frac{(t^2 -(a+b)t + ab)^2}{(a+b - 2t)^3}dt  t = \sqrt{(x+a)(x+b)} - x \sqrt{t^2 - (a+b)t + ab} = t + l (x+a)(x+b) =\left (x + \frac{a+b}{2}\right)^2 - \left(\frac{a-b}{2}\right)^2 u =  x + \frac{a+b}{2}, dx = du  ; \sqrt{u^2 - (\frac{a -b}{2})^2} = \frac{a-b}{2}\sqrt{\frac{4 u^2}{(a-b)^2} - 1} s = \sec^{-1}  \frac{2u}{a-b} \frac{a-b}{2}\sqrt{\frac{4 u^2}{(a-b)^2} - 1} = \frac{a-b}{2}\sqrt{\sec^2 s - 1} = \frac{a-b}{2} \tan s ds \sec s = \frac{2u}{a-b} \implies u = \frac{(a-b)}{2} \sec(s)  du = \left(\frac{(a-b)}{2} \sec(s)\right)' du = \frac{(a-b)}{2} \sec u \tan u \cdot ds \int \sqrt{(x+a)(x+b)} dx = \int \frac{(a-b)^2}{4} \tan s \tan u \sec u \cdot ds","['calculus', 'integration', 'indefinite-integrals']"
30,In a unit circle what is the average distance to the center of circle?,In a unit circle what is the average distance to the center of circle?,,"In a circle with radius 1 what is the average distance between a randomly placed point and the center of the circle? So far I have tried a few things but gotten different results. Approach 1: My idea here is to take the weighted average of all distances where the circumference of the corresponding circle is the weight. The definition of the weighted average is: $\frac{\sum_{0} ^{n} {w_n v_n}}{\sum_{0} ^{n} {w_n}}$ where $w_n$ and $v_n$ are lists of weights and values This leads me to: $\frac{\int_0^1 {2\pi r^2}dr}{\int_0^1 {2\pi r} dr} = \frac{2}{3}$ I believe this is correct due to a computer simulation I made, but I could have done that wrong. Approach 2: Since the probability of a point falling in a certain section of a circle is proportional to the area of the section my other idea was to find the distance where the probability of falling inside and outside is equal, or the area of the inner circle is equal to the area of the rest of the circle. $\pi x^2 = \pi 1^2 - \pi x^2$ $x = \frac{\sqrt{2}}{2}$ Why is this different than my first answer? Which approach is wrong and why?","In a circle with radius 1 what is the average distance between a randomly placed point and the center of the circle? So far I have tried a few things but gotten different results. Approach 1: My idea here is to take the weighted average of all distances where the circumference of the corresponding circle is the weight. The definition of the weighted average is: where and are lists of weights and values This leads me to: I believe this is correct due to a computer simulation I made, but I could have done that wrong. Approach 2: Since the probability of a point falling in a certain section of a circle is proportional to the area of the section my other idea was to find the distance where the probability of falling inside and outside is equal, or the area of the inner circle is equal to the area of the rest of the circle. Why is this different than my first answer? Which approach is wrong and why?",\frac{\sum_{0} ^{n} {w_n v_n}}{\sum_{0} ^{n} {w_n}} w_n v_n \frac{\int_0^1 {2\pi r^2}dr}{\int_0^1 {2\pi r} dr} = \frac{2}{3} \pi x^2 = \pi 1^2 - \pi x^2 x = \frac{\sqrt{2}}{2},"['calculus', 'geometry']"
31,Moment of inertia of a sphere,Moment of inertia of a sphere,,"My physics textbook said that the moment of inertia of a sphere is $\frac25mr^2$ where $m$ and $r$ are the mass and the radius of the sphere respectively. I wanted to verify the result by finding it myself, however, I always end up with $\frac35mr^2$ instead of $\frac25mr^2$. What is wrong with my method? My approach involved cutting up a sphere into an infinite amount of hollow sphere shells, each of which having volume $4\pi r^2dr$. Then, calling $d$ the density of the sphere and $R$ the radius, the moment of inertia should be: $$\int_{0}^{R}4\pi r^2\cdot d\cdot r^2\cdot dr$$ Calculating this integral gives the following: $$4\pi d\frac{R^5}5$$ Using $m=d\cdot\frac43\pi r^3$, we can simplify this to: $$\frac35mR^2$$ This is clearly not the correct answer. Where have I gone wrong in my method? Is cutting the sphere into hollow shells conceptually wrong in the first place?","My physics textbook said that the moment of inertia of a sphere is $\frac25mr^2$ where $m$ and $r$ are the mass and the radius of the sphere respectively. I wanted to verify the result by finding it myself, however, I always end up with $\frac35mr^2$ instead of $\frac25mr^2$. What is wrong with my method? My approach involved cutting up a sphere into an infinite amount of hollow sphere shells, each of which having volume $4\pi r^2dr$. Then, calling $d$ the density of the sphere and $R$ the radius, the moment of inertia should be: $$\int_{0}^{R}4\pi r^2\cdot d\cdot r^2\cdot dr$$ Calculating this integral gives the following: $$4\pi d\frac{R^5}5$$ Using $m=d\cdot\frac43\pi r^3$, we can simplify this to: $$\frac35mR^2$$ This is clearly not the correct answer. Where have I gone wrong in my method? Is cutting the sphere into hollow shells conceptually wrong in the first place?",,"['calculus', 'integration', 'physics']"
32,Prove the existence of Laplace transform of $2te^{t^2}\cos(e^{t^2})$,Prove the existence of Laplace transform of,2te^{t^2}\cos(e^{t^2}),"I tried to directly compute the integral, but I was unable to and wolfram alpha says it cannot find the answer in terms of elementary integrals. How can I prove the existence of the Laplace transform without directly computing it? Helpful hints leading me to the answer will be accepted as the answer(and I actually prefer this to outright stating it. Any help is appreciated! I messed up and put the wrong function in the title initially- it should be the derivative of the function I gave. I am sorry for wasting people's time.","I tried to directly compute the integral, but I was unable to and wolfram alpha says it cannot find the answer in terms of elementary integrals. How can I prove the existence of the Laplace transform without directly computing it? Helpful hints leading me to the answer will be accepted as the answer(and I actually prefer this to outright stating it. Any help is appreciated! I messed up and put the wrong function in the title initially- it should be the derivative of the function I gave. I am sorry for wasting people's time.",,"['calculus', 'laplace-transform']"
33,Does Leibniz's rule hold for improper integrals?,Does Leibniz's rule hold for improper integrals?,,"Does this hold in general? $$ \frac{\mathrm{d}}{\mathrm{d}t}\int_{-\infty}^{\infty} f(x,t) \mathrm{d}x = \int_{-\infty}^{\infty}\frac{\partial}{\partial t}f(x,t) \mathrm{d}x. $$ I know it is true if the bounds on the integral are finite but can the result be extended to improper integrals? Also if it is true in special cases, what are those cases? Thanks a lot!","Does this hold in general? $$ \frac{\mathrm{d}}{\mathrm{d}t}\int_{-\infty}^{\infty} f(x,t) \mathrm{d}x = \int_{-\infty}^{\infty}\frac{\partial}{\partial t}f(x,t) \mathrm{d}x. $$ I know it is true if the bounds on the integral are finite but can the result be extended to improper integrals? Also if it is true in special cases, what are those cases? Thanks a lot!",,"['calculus', 'integration', 'improper-integrals']"
34,Why is $\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $?,Why is ?,\sup f_- (n) \inf f_+ (m) = \frac{5}{4} ,"Let $f_- (n) = \Pi_{i=0}^n ( \sin(i) - \frac{5}{4}) $ And let $ f_+(m) = \Pi_{i=0}^m ( \sin(i) + \frac{5}{4} ) $ It appears that $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ Why is that so ? Notice $$\int_0^{2 \pi} \ln(\sin(x) + \frac{5}{4}) dx = Re \int_0^{2 \pi} \ln (\sin(x) - \frac{5}{4}) dx = \int_0^{2 \pi} \ln (\cos(x) + \frac{5}{4}) dx = Re \int_0^{2 \pi} \ln(\cos(x) - \frac{5}{4}) dx = 0 $$ $$ \int_0^{2 \pi} \ln (\sin(x) - \frac{5}{4}) dx = \int_0^{2 \pi} \ln (\cos(x) - \frac{5}{4}) dx = 2 \pi^2 i $$ That explains the finite values of $\sup $ and $ \inf $ .. well almost. It can be proven that both are finite. But that does not explain the value of their product. Update This is probably not helpful at all , but it can be shown ( not easy ) that there exist a unique pair of functions $g_-(x) , g_+(x) $ , both entire and with period $2 \pi $ such that $$ g_-(n) = f_-(n) , g_+(m) = f_+(m) $$ However i have no closed form for any of those ... As for the numerical test i got about $ln(u) (2 \pi)^{-1}$ correct digits , where $u = m + n$ and the ratio $m/n$ is close to $1$ . Assuming no round-off errors i ended Up with $1.2499999999(?) $ . That was enough to convince me. I often get accused of "" no context "" or "" no effort "" but i have NOO idea how to even start here. I considered telescoping but failed and assumed it is not related. Since I also have no closed form for the product I AM STUCK. I get upset when people assume this is homework. It clearly is not imho ! What kind of teacher or book contains this ? ——- Example : Taking $m = n = 8000 $ we get $$ max(f_-(1),f_-(2),...,f_-(8000)) = 1,308587092.. $$ $$ min(f_+(1),f_+(2),...,f_+(8000)) = 0,955226916.. $$ $$ 1.308587092.. X 0.955226916.. = 1.249997612208568.. $$ Supporting the claim. Im not sure if $sup f_+ = 7,93.. $ or the average of $f_+ $ ( $ 3,57..$ ) relate to the above $1,308.. $ and $0,955..$ or the truth of the claimed value $5/4$ . In principe we could write the values $1,308..$ and $0,955..$ as complicated integrals. By using the continuum product functions $f_-(v),f_+(w)$ where $v,w$ are positive reals. This is by noticing $ \sum^t \sum_i a_i \exp(t \space i) = \sum_i a_i ( \exp((t+1)i) - 1)(\exp(i) - 1)^{-1} $ and noticing the functions $f_+,f_-$ are periodic with $2 \pi$ . Next with contour integration you can find min and max over that period $2 \pi$ for the continuum product functions. Then the product of those 2 integrals should give you $\frac{5}{4}$ . —- Maybe all of this is unnessarily complicated and some simple theorems from trigonometry or calculus could easily explain the conjectured value $\frac{5}{4}$ .. but I do not see it. —— —— Update This conjecture is part of a more general phenomenon. For example the second conjecture : Let $g(n) = \prod_{i=0}^n (\sin^2(n) + \frac{9}{16} ) $ $$ \sup g(n) \space \inf g(n) = \frac{9}{16} $$ It feels like this second conjecture could somehow follow from the first conjecture since $$-(\cos(n) + \frac{5}{4})(\cos(n) - \frac{5}{4}) = - \cos^2(n) + \frac{25}{16} = \sin^2(n) + \frac{9}{16} $$ And perhaps the first conjecture could also follow from this second one ? Since these are additional questions and I can only accept one answer , I started a new thread with these additional questions : Why is $\inf g \sup g = \frac{9}{16} $? ---EDIT--- I want to explain better how to get a closed form for these numbers. I already mentioned that the periods of these functions are $2 \pi$ and how to use that. But those few lines deserve more attention. Basically this is what we do : We use the fourier series $$ f(x) = \ln(\sin(x) + 5/4) = \sum_{k=1}^{\infty} \frac{-2 \cos(k(x + \pi/2))}{2^k k} $$ Now we use the inverse of the backward difference operator ( similar but distinct from the so-called ""indefinite sum"" which is defined as inverse of the forward difference operator ) In other words we solve for $F(x)$ such that $$F(x) - F(x-1) = f(x)$$ We call this the "" continuum sum "" (CS) and write/define : $$ CS(f(x),x) := F(x) $$ $$ CS(f(x),y) := F(y) $$ For clarity : $$ \sum_0^y f(x) = CS(f(x),y) - CS(f(x),-1) $$ $$ \sum_0^0 f(x) = CS(f(x),0) - CS(f(x),-1) = f(0) $$ This operator is linear so we make use of that : $$CS(2 \cos(k(x+\pi/2),x) = \csc(k/2) \sin(k(x+1/2 + \pi/2)) -1.$$ This implies that : $$ CS(f(x),x) = F(x) = - \sum_{k=1}^{\infty} \frac{\csc(k/2)\sin(k(x + 1/2 + \pi/2)) - 1}{2^k k} $$ and $$ G(x) = \frac{d F(x)}{dx} = - \sum_{k=1}^{\infty} \frac{\csc(k/2) \cos(k(x+1/2+\pi/2))}{2^k} $$ Now $\sup f_+ = 7.93.. $ is the supremum of $ \exp(F(x) - F(-1) $ and $\inf f_+ = 0.95..$ is the infimum of $ \exp(F(x) - F(-1) $ . And both these values are achieved at $x$ such that $G(x) = 0$ . The analogue for $f_-$ works. So the numbers from the OP can ( more or less) be given by these infinite series and hence the whole conjectures can be stated by these infinite series. We also know for instance $$  \sum_{k=1}^{\infty} \frac{1}{k 2^k} = \ln(2) ,  \sum_{k=1}^{\infty} \frac{(-1)^k}{k 2^k} = - \ln(3/2) $$ So that is hopefull. Also the max and min of functions can be given by contour integrals but that might not make things easier ? Many trig identities and symmetry are probably related. But I see no clear proof. So that is how we compute the values and it might just help. Also notice : $$ t(x) = \ln(\sin(x) - 5/4) =  \ln(-1) + \ln(\sin(-x) + 5/4) = \ln(-1) + \sum_{k=1}^{\infty} \frac{-2 \cos(k(x + \pi/2))}{2^k k} $$ So the other case is no mystery. And ofcourse the case $ \ln(\cos^2(x) - 9/16) $ is also simply related. ( trig addition identities can be used ) EDIT : I wanted to make as little conjectures as possible when I posted this. Just one question/conjecture per post is the usual rule. But many related conjectures exist. Many might turn out to be equivalent or have already been shown to be equivalent. (such as the analogue cosine cases with $\frac{3}{4}$ ) I thought it would be best not to flood with related conjectures and post the most important one. Which I did. But one comment of Richard can not be ignored. Richard Stanley wrote $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{4}{5} $$ He might not be the first to notice this, even if I exclude my mentor who came up with the main conjecture and related ones. (I think it was also mentioned on chat and comments on MSE) Anyways it is also a nice conjecture. Is it equivalent ? No. Lets use shorthand notations : $x = \sup f_-(n), y = \inf f_+$ . Then combining both conjectures ( the original and the ""Richard"" variant) we get $$x y = \frac{5}{4} , x^2 - y^2 = (x-y)(x+y) = \frac{4}{5}$$ Now this implies we can compute the value of $x$ (or $y$ ). We get an interesting situation here. If 2 conjectures are true than so is the third : $$x y = \frac{5}{4}$$ $$ x^2 - y^2 = (x-y)(x+y) = \frac{4}{5} $$ $x$ is the positive real solution to $5 * 4^2 x^4 - 4^3 x^2 - 5^3 $ or $80 x^4 - 64 x^2 - 125$ . NOTICE the 5's and 4's all over again. (or closed form for $x = \frac{1}{2} \sqrt {\frac{8 + \sqrt {689}}{5}}$ ) ( $689 = 13*53$ if that matters to anyone) This was all known in the $90's$ by my mentor. I call it the Raes-Stanley conjecture. Update ! I had a talk with my mentor why he did not mention the $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{4}{5} $$ part of the Raes-Stanley conjecture. Although he noticed the apparant identity, he does not actually believe that last part. He said that the value converges fast for $n,m > 210000$ to $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{3.999789007..}{5} $$ In fact increasing $n,m$ from $210000$ to $314314$ both gives $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{3.999789007..}{5} $$ so barely any noticeable change, while $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ seems already heuristically confirmed. Testing for $n,m$ smaller than $100000$ might give the wrong impression and might be the cause of the mistake. Numerical coincidence might then lead to wrong conclusions. Roundoff errors might create an effect in the computations but he does not believe the ""Stanley part "" $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ can be saved by those errors. If anyone can prove or argue or compute a higher value than $\frac{3.999789007..}{5}$ , please inform me. Finally the Full Raes conjecture is the slight generalization : Let $f_- (n) = \Pi_{i=0}^n ( \sin(i) - \frac{5}{4}) $ And let $ f_+(m) = \Pi_{i=0}^m ( \sin(i) + \frac{5}{4} ) $ It appears that $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ and $$\inf |f_- (n)| \sup f_+ (m) = \frac{5}{4} $$ Why is that so ? $|*|$ means the absolute value here. Indeed $$\inf |f_- (n)| = 0.157559...$$ $$\sup f_- (n) =  1.308592...$$ $$\inf f_+ (n) = 0.955225...$$ $$\sup f_+ (n) =  7.933553...$$ $$0.955225... * 1.308592... = 1.25 = \frac{5}{4}$$ $$0.157559... * 7.933553... = 1.25 = \frac{5}{4}$$ Let $g_- (n) = \Pi_{i=0}^n ( \cos(i) - \frac{5}{4}) $ And let $ g_+(m) = \Pi_{i=0}^m ( \cos(i) + \frac{5}{4} ) $ It appears that $$\sup g_- (n) \inf g_+ (m) = \frac{3}{4} $$ and $$\inf |g_- (n)| \sup g_+ (m) = \frac{3}{4} $$","Let And let It appears that Why is that so ? Notice That explains the finite values of and .. well almost. It can be proven that both are finite. But that does not explain the value of their product. Update This is probably not helpful at all , but it can be shown ( not easy ) that there exist a unique pair of functions , both entire and with period such that However i have no closed form for any of those ... As for the numerical test i got about correct digits , where and the ratio is close to . Assuming no round-off errors i ended Up with . That was enough to convince me. I often get accused of "" no context "" or "" no effort "" but i have NOO idea how to even start here. I considered telescoping but failed and assumed it is not related. Since I also have no closed form for the product I AM STUCK. I get upset when people assume this is homework. It clearly is not imho ! What kind of teacher or book contains this ? ——- Example : Taking we get Supporting the claim. Im not sure if or the average of ( ) relate to the above and or the truth of the claimed value . In principe we could write the values and as complicated integrals. By using the continuum product functions where are positive reals. This is by noticing and noticing the functions are periodic with . Next with contour integration you can find min and max over that period for the continuum product functions. Then the product of those 2 integrals should give you . —- Maybe all of this is unnessarily complicated and some simple theorems from trigonometry or calculus could easily explain the conjectured value .. but I do not see it. —— —— Update This conjecture is part of a more general phenomenon. For example the second conjecture : Let It feels like this second conjecture could somehow follow from the first conjecture since And perhaps the first conjecture could also follow from this second one ? Since these are additional questions and I can only accept one answer , I started a new thread with these additional questions : Why is $\inf g \sup g = \frac{9}{16} $? ---EDIT--- I want to explain better how to get a closed form for these numbers. I already mentioned that the periods of these functions are and how to use that. But those few lines deserve more attention. Basically this is what we do : We use the fourier series Now we use the inverse of the backward difference operator ( similar but distinct from the so-called ""indefinite sum"" which is defined as inverse of the forward difference operator ) In other words we solve for such that We call this the "" continuum sum "" (CS) and write/define : For clarity : This operator is linear so we make use of that : This implies that : and Now is the supremum of and is the infimum of . And both these values are achieved at such that . The analogue for works. So the numbers from the OP can ( more or less) be given by these infinite series and hence the whole conjectures can be stated by these infinite series. We also know for instance So that is hopefull. Also the max and min of functions can be given by contour integrals but that might not make things easier ? Many trig identities and symmetry are probably related. But I see no clear proof. So that is how we compute the values and it might just help. Also notice : So the other case is no mystery. And ofcourse the case is also simply related. ( trig addition identities can be used ) EDIT : I wanted to make as little conjectures as possible when I posted this. Just one question/conjecture per post is the usual rule. But many related conjectures exist. Many might turn out to be equivalent or have already been shown to be equivalent. (such as the analogue cosine cases with ) I thought it would be best not to flood with related conjectures and post the most important one. Which I did. But one comment of Richard can not be ignored. Richard Stanley wrote He might not be the first to notice this, even if I exclude my mentor who came up with the main conjecture and related ones. (I think it was also mentioned on chat and comments on MSE) Anyways it is also a nice conjecture. Is it equivalent ? No. Lets use shorthand notations : . Then combining both conjectures ( the original and the ""Richard"" variant) we get Now this implies we can compute the value of (or ). We get an interesting situation here. If 2 conjectures are true than so is the third : is the positive real solution to or . NOTICE the 5's and 4's all over again. (or closed form for ) ( if that matters to anyone) This was all known in the by my mentor. I call it the Raes-Stanley conjecture. Update ! I had a talk with my mentor why he did not mention the part of the Raes-Stanley conjecture. Although he noticed the apparant identity, he does not actually believe that last part. He said that the value converges fast for to In fact increasing from to both gives so barely any noticeable change, while seems already heuristically confirmed. Testing for smaller than might give the wrong impression and might be the cause of the mistake. Numerical coincidence might then lead to wrong conclusions. Roundoff errors might create an effect in the computations but he does not believe the ""Stanley part "" can be saved by those errors. If anyone can prove or argue or compute a higher value than , please inform me. Finally the Full Raes conjecture is the slight generalization : Let And let It appears that and Why is that so ? means the absolute value here. Indeed Let And let It appears that and","f_- (n) = \Pi_{i=0}^n ( \sin(i) - \frac{5}{4})   f_+(m) = \Pi_{i=0}^m ( \sin(i) + \frac{5}{4} )  \sup f_- (n) \inf f_+ (m) = \frac{5}{4}  \int_0^{2 \pi} \ln(\sin(x) + \frac{5}{4}) dx = Re \int_0^{2 \pi} \ln (\sin(x) - \frac{5}{4}) dx = \int_0^{2 \pi} \ln (\cos(x) + \frac{5}{4}) dx = Re \int_0^{2 \pi} \ln(\cos(x) - \frac{5}{4}) dx = 0   \int_0^{2 \pi} \ln (\sin(x) - \frac{5}{4}) dx = \int_0^{2 \pi} \ln (\cos(x) - \frac{5}{4}) dx = 2 \pi^2 i  \sup   \inf  g_-(x) , g_+(x)  2 \pi   g_-(n) = f_-(n) , g_+(m) = f_+(m)  ln(u) (2 \pi)^{-1} u = m + n m/n 1 1.2499999999(?)  m = n = 8000   max(f_-(1),f_-(2),...,f_-(8000)) = 1,308587092..   min(f_+(1),f_+(2),...,f_+(8000)) = 0,955226916..   1.308587092.. X 0.955226916.. = 1.249997612208568..  sup f_+ = 7,93..  f_+   3,57.. 1,308..  0,955.. 5/4 1,308.. 0,955.. f_-(v),f_+(w) v,w  \sum^t \sum_i a_i \exp(t \space i) = \sum_i a_i ( \exp((t+1)i) - 1)(\exp(i) - 1)^{-1}  f_+,f_- 2 \pi 2 \pi \frac{5}{4} \frac{5}{4} g(n) = \prod_{i=0}^n (\sin^2(n) + \frac{9}{16} )   \sup g(n) \space \inf g(n) = \frac{9}{16}  -(\cos(n) + \frac{5}{4})(\cos(n) - \frac{5}{4}) = - \cos^2(n) + \frac{25}{16} = \sin^2(n) + \frac{9}{16}  2 \pi  f(x) = \ln(\sin(x) + 5/4) = \sum_{k=1}^{\infty} \frac{-2 \cos(k(x + \pi/2))}{2^k k}  F(x) F(x) - F(x-1) = f(x)  CS(f(x),x) := F(x)   CS(f(x),y) := F(y)   \sum_0^y f(x) = CS(f(x),y) - CS(f(x),-1)   \sum_0^0 f(x) = CS(f(x),0) - CS(f(x),-1) = f(0)  CS(2 \cos(k(x+\pi/2),x) = \csc(k/2) \sin(k(x+1/2 + \pi/2)) -1.  CS(f(x),x) = F(x) = - \sum_{k=1}^{\infty} \frac{\csc(k/2)\sin(k(x + 1/2 + \pi/2)) - 1}{2^k k}   G(x) = \frac{d F(x)}{dx} = - \sum_{k=1}^{\infty} \frac{\csc(k/2) \cos(k(x+1/2+\pi/2))}{2^k}  \sup f_+ = 7.93..   \exp(F(x) - F(-1)  \inf f_+ = 0.95..  \exp(F(x) - F(-1)  x G(x) = 0 f_-   \sum_{k=1}^{\infty} \frac{1}{k 2^k} = \ln(2) ,  \sum_{k=1}^{\infty} \frac{(-1)^k}{k 2^k} = - \ln(3/2)   t(x) = \ln(\sin(x) - 5/4) =  \ln(-1) + \ln(\sin(-x) + 5/4) = \ln(-1) + \sum_{k=1}^{\infty} \frac{-2 \cos(k(x + \pi/2))}{2^k k}   \ln(\cos^2(x) - 9/16)  \frac{3}{4}  \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{4}{5}  x = \sup f_-(n), y = \inf f_+ x y = \frac{5}{4} , x^2 - y^2 = (x-y)(x+y) = \frac{4}{5} x y x y = \frac{5}{4}  x^2 - y^2 = (x-y)(x+y) = \frac{4}{5}  x 5 * 4^2 x^4 - 4^3 x^2 - 5^3  80 x^4 - 64 x^2 - 125 x = \frac{1}{2} \sqrt {\frac{8 + \sqrt {689}}{5}} 689 = 13*53 90's  \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{4}{5}  n,m > 210000  \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{3.999789007..}{5}  n,m 210000 314314  \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{3.999789007..}{5}  \sup f_- (n) \inf f_+ (m) = \frac{5}{4}  n,m 100000 \sup f_- (n) \inf f_+ (m) = \frac{5}{4}  \frac{3.999789007..}{5} f_- (n) = \Pi_{i=0}^n ( \sin(i) - \frac{5}{4})   f_+(m) = \Pi_{i=0}^m ( \sin(i) + \frac{5}{4} )  \sup f_- (n) \inf f_+ (m) = \frac{5}{4}  \inf |f_- (n)| \sup f_+ (m) = \frac{5}{4}  |*| \inf |f_- (n)| = 0.157559... \sup f_- (n) =  1.308592... \inf f_+ (n) = 0.955225... \sup f_+ (n) =  7.933553... 0.955225... * 1.308592... = 1.25 = \frac{5}{4} 0.157559... * 7.933553... = 1.25 = \frac{5}{4} g_- (n) = \Pi_{i=0}^n ( \cos(i) - \frac{5}{4})   g_+(m) = \Pi_{i=0}^m ( \cos(i) + \frac{5}{4} )  \sup g_- (n) \inf g_+ (m) = \frac{3}{4}  \inf |g_- (n)| \sup g_+ (m) = \frac{3}{4} ","['calculus', 'geometry', 'fractions', 'limsup-and-liminf', 'products']"
35,How to smoothly approximate a sign function,How to smoothly approximate a sign function,,"I have a function that defined as following $$f(x) = \begin{cases} 1,  & \text{if $x > 0$ } \\ 0, & \text{if $x=0$ } \\ -1, & \text{if $x<0$ } \end{cases}$$ In practice, the $f(x)$ is approximated by a smooth $\tanh(kx)$ or Heaviside function as bellow figure. Could you have other way to represent the $f(x)$ function? What are the benefits of your way? Update : $\tanh(kx)$ function $k$ controls the smoothness of the sign function. As $k \to \infty$ , the function defined in $f(x)=\tanh(kx)$ converges to standard sign function. Similarly, the derivative of $\tanh(x)$ also converges to Dirac delta function as $k \to \infty$ . If $k$ is too small, the evolution equation for $x$ acts locally only on a few values around $\{x=0\}$ . Hence, the $\tanh(kx)$ function is sensitive with parameter $k$ . The parameter $k$ must be chosen carefully. Hence, My purposed want to reduce/ignore the affect of k, but remains the above smoothly approximation.","I have a function that defined as following In practice, the is approximated by a smooth or Heaviside function as bellow figure. Could you have other way to represent the function? What are the benefits of your way? Update : function controls the smoothness of the sign function. As , the function defined in converges to standard sign function. Similarly, the derivative of also converges to Dirac delta function as . If is too small, the evolution equation for acts locally only on a few values around . Hence, the function is sensitive with parameter . The parameter must be chosen carefully. Hence, My purposed want to reduce/ignore the affect of k, but remains the above smoothly approximation.","f(x) =
\begin{cases}
1,  & \text{if x > 0 } \\
0, & \text{if x=0 } \\
-1, & \text{if x<0 }
\end{cases} f(x) \tanh(kx) f(x) \tanh(kx) k k \to \infty f(x)=\tanh(kx) \tanh(x) k \to \infty k x \{x=0\} \tanh(kx) k k","['calculus', 'functional-analysis', 'functions', 'trigonometry']"
36,Other integral related to Ahmed's integral,Other integral related to Ahmed's integral,,"I have a doubt regarding the evaluation of the following integral : $$ \int_0^\frac{1}{\sqrt{5}} \frac{\tan^{-1}\left({\sqrt{(1 + x^2)/2}}\right)} {(1 + 3x^2)\sqrt{1 + x^2}}\,du = \frac{\pi^2\sqrt{2}}{60}. $$ Could anybody please help by offering useful hints or solutions? I think very difficult to prove.","I have a doubt regarding the evaluation of the following integral : $$ \int_0^\frac{1}{\sqrt{5}} \frac{\tan^{-1}\left({\sqrt{(1 + x^2)/2}}\right)} {(1 + 3x^2)\sqrt{1 + x^2}}\,du = \frac{\pi^2\sqrt{2}}{60}. $$ Could anybody please help by offering useful hints or solutions? I think very difficult to prove.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
37,Fourier transform of squared exponential integral $\operatorname{Ei}^2(-|x|)$,Fourier transform of squared exponential integral,\operatorname{Ei}^2(-|x|),"Let $\operatorname{Ei}(x)$ denote the exponential integral : $$\operatorname{Ei}(x)=-\int_{-x}^\infty\frac{e^{-t}}tdt.$$ Now consider the function $\operatorname{Ei}(-|x|)$. Its Fourier transform is $$\frac1{\sqrt{2\pi}}\int_{-\infty}^\infty\operatorname{Ei}(-|x|)\,e^{i\hspace{.05em}k\hspace{.05em}x}\,dx=-\sqrt{\frac2\pi}\frac{\arctan|k|}{|k|}.$$ I'm interested in a Fourier transform of its square: $$\frac1{\sqrt{2\pi}}\int_{-\infty}^\infty\operatorname{Ei}^2(-|x|)\,e^{i\hspace{.05em}k\hspace{.05em}x}\,dx=\,?$$","Let $\operatorname{Ei}(x)$ denote the exponential integral : $$\operatorname{Ei}(x)=-\int_{-x}^\infty\frac{e^{-t}}tdt.$$ Now consider the function $\operatorname{Ei}(-|x|)$. Its Fourier transform is $$\frac1{\sqrt{2\pi}}\int_{-\infty}^\infty\operatorname{Ei}(-|x|)\,e^{i\hspace{.05em}k\hspace{.05em}x}\,dx=-\sqrt{\frac2\pi}\frac{\arctan|k|}{|k|}.$$ I'm interested in a Fourier transform of its square: $$\frac1{\sqrt{2\pi}}\int_{-\infty}^\infty\operatorname{Ei}^2(-|x|)\,e^{i\hspace{.05em}k\hspace{.05em}x}\,dx=\,?$$",,"['calculus', 'integration', 'definite-integrals', 'fourier-analysis', 'special-functions']"
38,Resources for Integrals?,Resources for Integrals?,,"I want to learn to solve integrals of some type, probably definite integrals with results involving various constants such as Catalan's, Euler-Mascheroni,Golden-ratio etc. and involving various functions like hyperbolic, inverse hyperbolic, hypergeometric, zeta etc. but can't find any resources on them. I found various sites with integration techniques but most of them list basic integration formulas.I am looking for something outside the elementary functions.Wikipedia and Wolfram are sudden spikes of unexplained complex results.","I want to learn to solve integrals of some type, probably definite integrals with results involving various constants such as Catalan's, Euler-Mascheroni,Golden-ratio etc. and involving various functions like hyperbolic, inverse hyperbolic, hypergeometric, zeta etc. but can't find any resources on them. I found various sites with integration techniques but most of them list basic integration formulas.I am looking for something outside the elementary functions.Wikipedia and Wolfram are sudden spikes of unexplained complex results.",,"['calculus', 'integration', 'reference-request', 'soft-question', 'big-list']"
39,Do you use degrees or radians for trig functions?,Do you use degrees or radians for trig functions?,,I was just wondering if you use degrees or radians in trig functions. For example if I have a degree of 0.5 would I do: Sin(0.5) or would I have to convert that to radians? Or does it not matter either way?,I was just wondering if you use degrees or radians in trig functions. For example if I have a degree of 0.5 would I do: Sin(0.5) or would I have to convert that to radians? Or does it not matter either way?,,"['calculus', 'algebra-precalculus', 'functions', 'trigonometry']"
40,Solving integral $ \int \frac{x+\sqrt{1+x+x^2}}{1+x+\sqrt{1+x+x^2}}\:\mathrm{d}x $,Solving integral, \int \frac{x+\sqrt{1+x+x^2}}{1+x+\sqrt{1+x+x^2}}\:\mathrm{d}x ,there is integral $$ \int \frac{x+\sqrt{1+x+x^2}}{1+x+\sqrt{1+x+x^2}}\:\mathrm{d}x$$ i am trying to separate this : $$=\int \mathrm{d}x -\int \frac{\mathrm{d}x}{1+x+\sqrt{1+x+x^2}} $$ but have no idea about second,there is integral $$ \int \frac{x+\sqrt{1+x+x^2}}{1+x+\sqrt{1+x+x^2}}\:\mathrm{d}x$$ i am trying to separate this : $$=\int \mathrm{d}x -\int \frac{\mathrm{d}x}{1+x+\sqrt{1+x+x^2}} $$ but have no idea about second,,"['calculus', 'integration', 'indefinite-integrals']"
41,Explanation for $\lim_{x\to\infty}\sqrt{x^2-4x}-x=-2$ and not $0$,Explanation for  and not,\lim_{x\to\infty}\sqrt{x^2-4x}-x=-2 0,"I am trying to intuitively understand why the solution to the following problem is $-2$. $$\lim_{x\to\infty}\sqrt{x^2-4x}-x$$ $$\lim_{x\to\infty}(\sqrt{x^2-4x}-x)\frac{\sqrt{x^2-4x}+x}{\sqrt{x^2-4x}+x}$$ $$\lim_{x\to\infty}\frac{x^2-4x-x^2}{\sqrt{x^2-4x}+x}$$ $$\lim_{x\to\infty}\frac{-4x}{\sqrt{x^2-4x}+x}$$ $$\lim_{x\to\infty}\frac{-4}{\sqrt{1-\frac{4}{x}}+1}$$ $$\frac{-4}{\sqrt{1-0}+1}$$ $$\frac{-4}{2}$$ $$-2$$ I can understand the process that results in the answer being $-2$. However, I expected the result to be $0$. I have learned that when dealing with a limit approaching $\infty$, only the highest degree term matters because the others will not be as significant. For this reason, I thought that the $4x$ would be ignored, resulting in: $$\lim_{x\to\infty}\sqrt{x^2-4x}-x$$ $$\lim_{x\to\infty}\sqrt{x^2}-x$$ $$\lim_{x\to\infty}x-x$$ $$\lim_{x\to\infty}0$$ $$0$$ Why is the above process incorrect?","I am trying to intuitively understand why the solution to the following problem is $-2$. $$\lim_{x\to\infty}\sqrt{x^2-4x}-x$$ $$\lim_{x\to\infty}(\sqrt{x^2-4x}-x)\frac{\sqrt{x^2-4x}+x}{\sqrt{x^2-4x}+x}$$ $$\lim_{x\to\infty}\frac{x^2-4x-x^2}{\sqrt{x^2-4x}+x}$$ $$\lim_{x\to\infty}\frac{-4x}{\sqrt{x^2-4x}+x}$$ $$\lim_{x\to\infty}\frac{-4}{\sqrt{1-\frac{4}{x}}+1}$$ $$\frac{-4}{\sqrt{1-0}+1}$$ $$\frac{-4}{2}$$ $$-2$$ I can understand the process that results in the answer being $-2$. However, I expected the result to be $0$. I have learned that when dealing with a limit approaching $\infty$, only the highest degree term matters because the others will not be as significant. For this reason, I thought that the $4x$ would be ignored, resulting in: $$\lim_{x\to\infty}\sqrt{x^2-4x}-x$$ $$\lim_{x\to\infty}\sqrt{x^2}-x$$ $$\lim_{x\to\infty}x-x$$ $$\lim_{x\to\infty}0$$ $$0$$ Why is the above process incorrect?",,"['calculus', 'limits', 'arithmetic', 'intuition', 'radicals']"
42,Convergence of a sequence $c_n$,Convergence of a sequence,c_n,Suppose that $(a_n)$ and $(b_n)$ be sequences such that $\lim (a_n)=0$ and $\displaystyle \lim \left( \sum_{i=1}^n b_i \right)$ exists. Define $c_n = a_1 b_n + a_2 b_{n-1} + \dots + a_n b_1$. Prove that $\lim (c_n)=0$,Suppose that $(a_n)$ and $(b_n)$ be sequences such that $\lim (a_n)=0$ and $\displaystyle \lim \left( \sum_{i=1}^n b_i \right)$ exists. Define $c_n = a_1 b_n + a_2 b_{n-1} + \dots + a_n b_1$. Prove that $\lim (c_n)=0$,,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
43,Reference for upper and lower bounds on $e^x$,Reference for upper and lower bounds on,e^x,"I'm looking for a reference for deriving the following commonly used upper and lower bounds for $e^x$: $$1 - x \le e^{-x}$$ and, assuming $x \le 1/2$, $$1 - x \ge e^{-2x}. $$","I'm looking for a reference for deriving the following commonly used upper and lower bounds for $e^x$: $$1 - x \le e^{-x}$$ and, assuming $x \le 1/2$, $$1 - x \ge e^{-2x}. $$",,"['calculus', 'inequality']"
44,What are gradients and how would I use them?,What are gradients and how would I use them?,,"I keep seeing this symbol $\nabla$ around and I know enough to understand that it represents the term ""gradient."" But what is a gradient? When would I want to use one mathematically?","I keep seeing this symbol $\nabla$ around and I know enough to understand that it represents the term ""gradient."" But what is a gradient? When would I want to use one mathematically?",,"['calculus', 'terminology']"
45,"Find $f(x)$ wher $f$ is an infinitely differentiable function such that $f(1) = 0, f(5) = \ln 4, f'(1) = 2$ and $f'(5) = −2$",Find  wher  is an infinitely differentiable function such that  and,"f(x) f f(1) = 0, f(5) = \ln 4, f'(1) = 2 f'(5) = −2","The given equation is: $$x^2\left(f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^2\right)=1$$ The full question actually requires you to find f(x) and then use it to evaluate $$\int_1^5 e^{f(x)} dx$$ However I am stuck on finding f(x) first. First I attempted to guess what it could be using the boundaries given but that of course failed, so I am trying to get f(x) by manipulating the given equation by integrating both sides. Here is what I tried: $$ \begin{aligned} & x^{2}\left(f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^{2}\right)=1 . \\ & f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^{2}=\frac{1}{x^{2}} \end{aligned} $$ Integrate both sides: $$ \int f^{\prime \prime}(x) d x+\int\left(f^{\prime}(x)\right)^{2} d x=\int \frac{1}{x^{2}} $$ let $u=f^{\prime}(x)$ $$ d u=f(x) d x \Rightarrow \frac{d u}{f(x)}=d x $$ then, $$ \int \frac{u^{\prime}}{f(x)} d u+\int \frac{u^{2}}{f(x)} d u=\int \frac{1}{x^{2}} $$ Reaching here I am understanding that I am headed in the wrong direction however I tried a few other ways using the equation, but I cant seem to get it in a form that is more manageable. All help and hints are appreciated, Thank You.","The given equation is: The full question actually requires you to find f(x) and then use it to evaluate However I am stuck on finding f(x) first. First I attempted to guess what it could be using the boundaries given but that of course failed, so I am trying to get f(x) by manipulating the given equation by integrating both sides. Here is what I tried: Integrate both sides: let then, Reaching here I am understanding that I am headed in the wrong direction however I tried a few other ways using the equation, but I cant seem to get it in a form that is more manageable. All help and hints are appreciated, Thank You.","x^2\left(f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^2\right)=1 \int_1^5 e^{f(x)} dx 
\begin{aligned}
& x^{2}\left(f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^{2}\right)=1 . \\
& f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^{2}=\frac{1}{x^{2}}
\end{aligned}
 
\int f^{\prime \prime}(x) d x+\int\left(f^{\prime}(x)\right)^{2} d x=\int \frac{1}{x^{2}}
 u=f^{\prime}(x) 
d u=f(x) d x \Rightarrow \frac{d u}{f(x)}=d x
 
\int \frac{u^{\prime}}{f(x)} d u+\int \frac{u^{2}}{f(x)} d u=\int \frac{1}{x^{2}}
","['calculus', 'ordinary-differential-equations', 'definite-integrals']"
46,Comparing two definite integrals analytically,Comparing two definite integrals analytically,,"How do you find which one is greater analytically: $\displaystyle \int_{0}^{\int_0^1e^{-x^2}\mathrm dx} e^{x^2}\mathrm dx$ or $\displaystyle \int_{0}^{\int_0^1e^{x^2}\mathrm dx} e^{-x^2}\mathrm dx$ ? SMMC is an international undergrad level math competition (Eastern counterpart of the Putnam). This is a sample question from their site . There's a solution put up there which I'm presenting here in a more detailed manner. Define as follows, $f(t):=\displaystyle \int_{0}^{\int_0^t e^{-x^2}\mathrm dx} \exp(x^2)\mathrm dx\tag{01}$ $g(t):=\displaystyle \int_{0}^{\int_0^t e^{x^2}\mathrm dx} \exp(-x^2)\mathrm dx\tag{02}$ Clearly, $f(0)=g(0)=0$ . We intend to compare $f(1)$ and $g(1)$ . Differentiating w.r.t. $t$ (use the fundamental theorem of calculus and the chain rule), $f'(t)=\displaystyle \exp\left[\left(\int_0^t e^{-x^2}\mathrm dx\right)^2\right]\cdot e^{-t^2}\tag{03}$ $g'(t)=\displaystyle \exp\left[-\left(\int_0^t e^{x^2}\mathrm dx\right)^2\right]\cdot e^{t^2}\tag{04}$ It looks like both $f$ and $g$ are increasing functions because $f'$ and $g'$ are $+$ ve for all $t$ . Given their initial value is same i.e., $0$ at $t=0$ , it’s sufficient to check which one of them grows faster to compare their values at $t=1$ . $\displaystyle \frac{f'(t)}{g'(t)}=\exp\left[\left(\int_0^t e^{-x^2}\mathrm dx\right)^2+\left(\int_0^t e^{x^2}\mathrm dx\right)^2-2t^2\right]\tag{05}$ By A.M.-G.M. inequality, we have: $\displaystyle\left(\int_0^t e^{-x^2}\mathrm dx\right)^2+\left(\int_0^t e^{x^2}\mathrm dx\right)^2\\ \displaystyle \geq 2\int_0^t e^{-x^2}\mathrm dx\cdot \int_0^t e^{x^2}\mathrm dx\tag*{}$ Notice that $e^{x^2}$ is increasing and $e^{-x^2}$ is decreasing over the interval $(0, t)$ . We can apply the continuous analog of Chebyshev’s sum inequality . If $f(x)$ is an increasing function and $g(x)$ is a decreasing function (or vice-versa) over the interval $(a,b)$ , we have the following inequality: $\displaystyle \frac{1}{b-a}\int_a^b f(x)\cdot g(x)\ \mathrm dx \\ \displaystyle \leq \left(\frac{1}{b-a}\int_a^b f(x)\mathrm dx\right)\cdot\left(\frac{1}{b-a}\int_a^b g(x)\mathrm dx\right)\tag*{}$ The inequality is reversed if $f(x)$ and $g(x)$ are both increasing or both decreasing. This is valid for discrete sum as well, where instead of functions, we consider sequences. $\displaystyle \int_0^t e^{-x^2}\mathrm dx\cdot \int_0^t e^{x^2}\mathrm dx\\ \geq \displaystyle (t-0)\int_0^t e^{-x^2}\cdot e^{x^2}\mathrm dx =t^2 \tag*{}$ Now we have established that: $\displaystyle \left(\int_0^t e^{-x^2}\mathrm dx\right)^2+\left(\int_0^t e^{x^2}\mathrm dx\right)^2> 2t^2\tag{06}$ The equality holds only if $t=0$ . From $(05)$ and $(06)$ , we have that: $\displaystyle \frac{f'(t)}{g'(t)}>1 \text{ i.e., } f'(t)>g'(t)\tag*{}$ $\therefore$ $f$ grows faster than $g$ . $f(0) = g(0)$ and hence, $f(1)>g(1)$ . $\blacksquare$ I hope there is no mistake in my solution. Is there any alternate way to do this without making use of the sum inequality?","How do you find which one is greater analytically: or ? SMMC is an international undergrad level math competition (Eastern counterpart of the Putnam). This is a sample question from their site . There's a solution put up there which I'm presenting here in a more detailed manner. Define as follows, Clearly, . We intend to compare and . Differentiating w.r.t. (use the fundamental theorem of calculus and the chain rule), It looks like both and are increasing functions because and are ve for all . Given their initial value is same i.e., at , it’s sufficient to check which one of them grows faster to compare their values at . By A.M.-G.M. inequality, we have: Notice that is increasing and is decreasing over the interval . We can apply the continuous analog of Chebyshev’s sum inequality . If is an increasing function and is a decreasing function (or vice-versa) over the interval , we have the following inequality: The inequality is reversed if and are both increasing or both decreasing. This is valid for discrete sum as well, where instead of functions, we consider sequences. Now we have established that: The equality holds only if . From and , we have that: grows faster than . and hence, . I hope there is no mistake in my solution. Is there any alternate way to do this without making use of the sum inequality?","\displaystyle \int_{0}^{\int_0^1e^{-x^2}\mathrm dx} e^{x^2}\mathrm dx \displaystyle \int_{0}^{\int_0^1e^{x^2}\mathrm dx} e^{-x^2}\mathrm dx f(t):=\displaystyle \int_{0}^{\int_0^t e^{-x^2}\mathrm dx} \exp(x^2)\mathrm dx\tag{01} g(t):=\displaystyle \int_{0}^{\int_0^t e^{x^2}\mathrm dx} \exp(-x^2)\mathrm dx\tag{02} f(0)=g(0)=0 f(1) g(1) t f'(t)=\displaystyle \exp\left[\left(\int_0^t e^{-x^2}\mathrm dx\right)^2\right]\cdot e^{-t^2}\tag{03} g'(t)=\displaystyle \exp\left[-\left(\int_0^t e^{x^2}\mathrm dx\right)^2\right]\cdot e^{t^2}\tag{04} f g f' g' + t 0 t=0 t=1 \displaystyle \frac{f'(t)}{g'(t)}=\exp\left[\left(\int_0^t e^{-x^2}\mathrm dx\right)^2+\left(\int_0^t e^{x^2}\mathrm dx\right)^2-2t^2\right]\tag{05} \displaystyle\left(\int_0^t e^{-x^2}\mathrm dx\right)^2+\left(\int_0^t e^{x^2}\mathrm dx\right)^2\\ \displaystyle \geq 2\int_0^t e^{-x^2}\mathrm dx\cdot \int_0^t e^{x^2}\mathrm dx\tag*{} e^{x^2} e^{-x^2} (0, t) f(x) g(x) (a,b) \displaystyle \frac{1}{b-a}\int_a^b f(x)\cdot g(x)\ \mathrm dx \\ \displaystyle \leq \left(\frac{1}{b-a}\int_a^b f(x)\mathrm dx\right)\cdot\left(\frac{1}{b-a}\int_a^b g(x)\mathrm dx\right)\tag*{} f(x) g(x) \displaystyle \int_0^t e^{-x^2}\mathrm dx\cdot \int_0^t e^{x^2}\mathrm dx\\ \geq \displaystyle (t-0)\int_0^t e^{-x^2}\cdot e^{x^2}\mathrm dx =t^2 \tag*{} \displaystyle \left(\int_0^t e^{-x^2}\mathrm dx\right)^2+\left(\int_0^t e^{x^2}\mathrm dx\right)^2> 2t^2\tag{06} t=0 (05) (06) \displaystyle \frac{f'(t)}{g'(t)}>1 \text{ i.e., } f'(t)>g'(t)\tag*{} \therefore f g f(0) = g(0) f(1)>g(1) \blacksquare","['calculus', 'definite-integrals', 'solution-verification']"
47,Does $\int_{0}^{\infty} \sin^x(x) dx$ converge?,Does  converge?,\int_{0}^{\infty} \sin^x(x) dx,"From what I have found the indefinite integral does not have a closed form solution. Also, the function takes complex values except for in the intervals $[0,\pi],[2\pi,3\pi],$ (and for whole x between those intervals). But if we only considered those intervals where the function takes on real values, i.e. $$\int_{0}^{\pi}\sin^x(x)dx+\int_{2\pi}^{3\pi}\sin^x(x)dx+\int_{4\pi}^{5\pi}\sin^x(x)dx+{...}$$ Does this infinite sum converge? For the record I have no idea how to go about proving this, but I am curious if anyone does.","From what I have found the indefinite integral does not have a closed form solution. Also, the function takes complex values except for in the intervals (and for whole x between those intervals). But if we only considered those intervals where the function takes on real values, i.e. Does this infinite sum converge? For the record I have no idea how to go about proving this, but I am curious if anyone does.","[0,\pi],[2\pi,3\pi], \int_{0}^{\pi}\sin^x(x)dx+\int_{2\pi}^{3\pi}\sin^x(x)dx+\int_{4\pi}^{5\pi}\sin^x(x)dx+{...}","['calculus', 'limits', 'trigonometry', 'convergence-divergence']"
48,Is there a connection between the sum of $\sum_{n=1}^\infty \frac{\ln(n+1)-\ln(n)}{n}$ and the Riemann zeta function?,Is there a connection between the sum of  and the Riemann zeta function?,\sum_{n=1}^\infty \frac{\ln(n+1)-\ln(n)}{n},"It can be shown that for every integer $p\geq 0$ , this integral identity holds: \begin{align} \int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx &= -1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\\ &= -1+\frac{\zeta(2)+\zeta(3)+\zeta(4)+\cdots+\zeta(p+2)}{p+1} \end{align} Pretty neat! I find it interesting that the quantity $$\frac{\zeta(2)+\zeta(3)+\zeta(4)+\cdots+\zeta(p+2)}{p+1}$$ is precisely the arithmetic mean of the numbers $\zeta(2),\zeta(3),\zeta(4),\dots,\zeta(p+2)$ . Anyway, notice that the integral $$\int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx$$ also converges for $p=-1$ . This is intuitively plausible because $\lfloor x\rfloor$ grows roughly like $x$ , so $$\frac{1}{x^{-1+2}\lfloor x\rfloor}=\frac{1}{x\lfloor x\rfloor}$$ behaves like $1/x^2$ as $x\to\infty$ , yielding a convergent integral. To clear up any doubts, I've left a proof at the bottom of the post. Now, the equation $$\int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx=-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)$$ doesn't make sense for $p=-1$ because substituting $-1$ for $p$ involves division by zero. Nevertheless, given the obvious connection between $$\int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx$$ and $\zeta(n)$ for $n\in\mathbb{N}$ , I can't help but wonder if there's a link between \begin{align} \int_1^\infty\frac{1}{x\lfloor x\rfloor}\text{ }dx &= \lim_{k\to\infty}\left(\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\right)\\ &= \lim_{k\to\infty}\left(\sum_{n=1}^k \frac{\ln(n+1)-\ln(n)}{n}\right)\\ &= \sum_{n=1}^\infty \frac{\ln(n+1)-\ln(n)}{n} \end{align} and the zeta function. Maybe it’s the exact value of zeta evaluated at some other point in the complex plane? Seeing the pattern in the identity, my only ""reasonable"" idea was to try and find a link between the sum the series (I'll call it $S$ ) and $\zeta(1)$ . This expression is undefined ( $\zeta$ has a singularity at $1$ ), but its principal value exists and equals the Euler-Mascheroni constant: $$\lim_{h\to 0}\frac{\zeta(1-h)+\zeta(1+h)}{2}=\gamma$$ We can't have an exact equality; $\gamma\approx 0.57$ , but WolframAlpha says that $S\approx 1.25$ . It's unlikely, but could these numbers be related in some other way? If not, are there any other possible links between $S$ and the zeta function? Any answer is greatly appreciated. Proof : the sequence $$\left\lbrace\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\right\rbrace_{k=0}^\infty$$ is strictly increasing because the integrand is strictly positive over $[1,k+1]$ . Now, \begin{align} \int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx &= \int_1^2\frac{1}{x\lfloor x\rfloor}\text{ }dx+\int_2^3\frac{1}{x\lfloor x\rfloor}\text{ }dx+\cdots+\int_k^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\\ &= \sum_{n=1}^k \int_n^{n+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\\ &= \sum_{n=1}^k \int_n^{n+1}\frac{1}{x\cdot n}\text{ }dx\\ &= \sum_{n=1}^k \frac{1}{n}\int_n^{n+1}\frac{1}{x}\text{ }dx\\ &= \color{green}{\sum_{n=1}^k \frac{\ln(n+1)-\ln(n)}{n}}\\ &= \sum_{n=1}^k \frac{1}{n}\ln\left(\frac{n+1}{n}\right)\\ &= \sum_{n=1}^k \frac{1}{n}\ln\left(1+\frac{1}{n}\right)\\ &< \sum_{n=1}^k \frac{1}{n}\cdot\frac{1}{n}\\ &< \sum_{n=1}^\infty \frac{1}{n^2} \end{align} so the terms of the sequence are bounded above. It follows that $$\left\lbrace\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\right\rbrace_{k=0}^\infty$$ is a convergent sequence. $\blacksquare$","It can be shown that for every integer , this integral identity holds: Pretty neat! I find it interesting that the quantity is precisely the arithmetic mean of the numbers . Anyway, notice that the integral also converges for . This is intuitively plausible because grows roughly like , so behaves like as , yielding a convergent integral. To clear up any doubts, I've left a proof at the bottom of the post. Now, the equation doesn't make sense for because substituting for involves division by zero. Nevertheless, given the obvious connection between and for , I can't help but wonder if there's a link between and the zeta function. Maybe it’s the exact value of zeta evaluated at some other point in the complex plane? Seeing the pattern in the identity, my only ""reasonable"" idea was to try and find a link between the sum the series (I'll call it ) and . This expression is undefined ( has a singularity at ), but its principal value exists and equals the Euler-Mascheroni constant: We can't have an exact equality; , but WolframAlpha says that . It's unlikely, but could these numbers be related in some other way? If not, are there any other possible links between and the zeta function? Any answer is greatly appreciated. Proof : the sequence is strictly increasing because the integrand is strictly positive over . Now, so the terms of the sequence are bounded above. It follows that is a convergent sequence.","p\geq 0 \begin{align}
\int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx &= -1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m)\\
&= -1+\frac{\zeta(2)+\zeta(3)+\zeta(4)+\cdots+\zeta(p+2)}{p+1}
\end{align} \frac{\zeta(2)+\zeta(3)+\zeta(4)+\cdots+\zeta(p+2)}{p+1} \zeta(2),\zeta(3),\zeta(4),\dots,\zeta(p+2) \int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx p=-1 \lfloor x\rfloor x \frac{1}{x^{-1+2}\lfloor x\rfloor}=\frac{1}{x\lfloor x\rfloor} 1/x^2 x\to\infty \int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx=-1+\frac{1}{p+1}\sum_{m=2}^{p+2}\zeta(m) p=-1 -1 p \int_1^\infty\frac{1}{x^{p+2}\lfloor x\rfloor}\text{ }dx \zeta(n) n\in\mathbb{N} \begin{align}
\int_1^\infty\frac{1}{x\lfloor x\rfloor}\text{ }dx &= \lim_{k\to\infty}\left(\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\right)\\
&= \lim_{k\to\infty}\left(\sum_{n=1}^k \frac{\ln(n+1)-\ln(n)}{n}\right)\\
&= \sum_{n=1}^\infty \frac{\ln(n+1)-\ln(n)}{n}
\end{align} S \zeta(1) \zeta 1 \lim_{h\to 0}\frac{\zeta(1-h)+\zeta(1+h)}{2}=\gamma \gamma\approx 0.57 S\approx 1.25 S \left\lbrace\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\right\rbrace_{k=0}^\infty [1,k+1] \begin{align}
\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx &= \int_1^2\frac{1}{x\lfloor x\rfloor}\text{ }dx+\int_2^3\frac{1}{x\lfloor x\rfloor}\text{ }dx+\cdots+\int_k^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\\
&= \sum_{n=1}^k \int_n^{n+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\\
&= \sum_{n=1}^k \int_n^{n+1}\frac{1}{x\cdot n}\text{ }dx\\
&= \sum_{n=1}^k \frac{1}{n}\int_n^{n+1}\frac{1}{x}\text{ }dx\\
&= \color{green}{\sum_{n=1}^k \frac{\ln(n+1)-\ln(n)}{n}}\\
&= \sum_{n=1}^k \frac{1}{n}\ln\left(\frac{n+1}{n}\right)\\
&= \sum_{n=1}^k \frac{1}{n}\ln\left(1+\frac{1}{n}\right)\\
&< \sum_{n=1}^k \frac{1}{n}\cdot\frac{1}{n}\\
&< \sum_{n=1}^\infty \frac{1}{n^2}
\end{align} \left\lbrace\int_1^{k+1}\frac{1}{x\lfloor x\rfloor}\text{ }dx\right\rbrace_{k=0}^\infty \blacksquare","['calculus', 'sequences-and-series', 'closed-form', 'riemann-zeta']"
49,How is calculus useful in economics; a subject where the quantities studied are often discrete?,How is calculus useful in economics; a subject where the quantities studied are often discrete?,,"Problem statement: It costs: $$c(x)=x^{3}-6x^{2}+15x$$ dollars to produce x toys when 8 to 30 toys are produced and that $$r(x)=x^{3}-3x^{2}+12x$$ gives the dollar revenue from selling $x$ toys. Your toy shop currently produces $10$ toys a day. About how much extra will it cost to produce one more toy a day, and what is your estimated increase in revenue for selling 11 toys? Solution: The cost of producing one more toy a day when 10 are produced is about $c'(x)=3x^{2}-12x+15$ and $c'(10)=195$ . The additional cost will be about $195$ dollars. The marginal revenue is: $r'(x)=3x^{2}-6x+12$ . The marginal revenue function estimates the increase in revenue that will result from selling one additional unit. If you currently sell $10$ toys a day, you can expect your revenue to increase by about $r'(10)=252$ dollars if you increase sales to 11 toys a day. Why do we use calculus here? I can already calculate the cost and the revenue of selling $11$ toys by calculating $c(11)$ and $r(11)$ , and then I could calculate the increase in the cost and revenue by simply calculating $c(11)-c(10)$ and $r(11)-r(10)$ . However, if I do that, the result is not the same. I'm confused here. I'd be glad if you could help me with this one. Thank you so much in advance!!! Cheers","Problem statement: It costs: dollars to produce x toys when 8 to 30 toys are produced and that gives the dollar revenue from selling toys. Your toy shop currently produces toys a day. About how much extra will it cost to produce one more toy a day, and what is your estimated increase in revenue for selling 11 toys? Solution: The cost of producing one more toy a day when 10 are produced is about and . The additional cost will be about dollars. The marginal revenue is: . The marginal revenue function estimates the increase in revenue that will result from selling one additional unit. If you currently sell toys a day, you can expect your revenue to increase by about dollars if you increase sales to 11 toys a day. Why do we use calculus here? I can already calculate the cost and the revenue of selling toys by calculating and , and then I could calculate the increase in the cost and revenue by simply calculating and . However, if I do that, the result is not the same. I'm confused here. I'd be glad if you could help me with this one. Thank you so much in advance!!! Cheers",c(x)=x^{3}-6x^{2}+15x r(x)=x^{3}-3x^{2}+12x x 10 c'(x)=3x^{2}-12x+15 c'(10)=195 195 r'(x)=3x^{2}-6x+12 10 r'(10)=252 11 c(11) r(11) c(11)-c(10) r(11)-r(10),['calculus']
50,Fundamental lemma of calculus of variations with second derivative,Fundamental lemma of calculus of variations with second derivative,,"Intense debate at work place around the solution for this: Let $M \in C[a,b]$ be a continuous function on the closed interval $[a,b]$ that satisfies $$\int_{a}^{b}M(x)\eta^{\prime\prime}dx = 0,$$ for all $\eta \in C^{2}\left[a,b\right]$ satisfying $\eta(a) = \eta(b) = \eta^{\prime}(a) = \eta^{\prime}(b) = 0$ . Prove that $M(x) = c_{0}+c_{1}x$ for suitable $c_{0}$ $c_{1}$ .  What can you say about $c_{0}$ , $c_{1}$ ? I tried to use integration by parts, and use the fundamental lemma of the calculus of variations, and the lemma of Du Bois and Reynolds to prove it, but that requires $M$ to be $C^1([a, b])$ .","Intense debate at work place around the solution for this: Let be a continuous function on the closed interval that satisfies for all satisfying . Prove that for suitable .  What can you say about , ? I tried to use integration by parts, and use the fundamental lemma of the calculus of variations, and the lemma of Du Bois and Reynolds to prove it, but that requires to be .","M \in C[a,b] [a,b] \int_{a}^{b}M(x)\eta^{\prime\prime}dx = 0, \eta \in C^{2}\left[a,b\right] \eta(a) = \eta(b) = \eta^{\prime}(a) = \eta^{\prime}(b) = 0 M(x) = c_{0}+c_{1}x c_{0} c_{1} c_{0} c_{1} M C^1([a, b])","['calculus', 'calculus-of-variations']"
51,Show $\int_0^{2\pi} \exp\left(\frac{7+5 \cos x}{10+6\cos x}\right) \cos \left( \frac{\sin x}{10+6 \cos x} \right) dx = 2\pi e^{2/3}$ Real Methods,Show  Real Methods,\int_0^{2\pi} \exp\left(\frac{7+5 \cos x}{10+6\cos x}\right) \cos \left( \frac{\sin x}{10+6 \cos x} \right) dx = 2\pi e^{2/3},"Taken from the post: The Integral that Stumped Feynman? I want to know if the integral: $$\int_0^{2\pi} \exp\left(\frac{7+5 \cos x}{10+6\cos x}\right) \cos \left( \frac{\sin x}{10+6 \cos x} \right) dx = 2\pi e^{2/3}$$ can be evaluated using strictly real methods. I've tried series of $e^x$ and $\cos x$ but to no avail. I tried differentiating under the integral, but nothing seemed to come out of it. Is there any wizardry that can conjure up this answer without complex analysis.","Taken from the post: The Integral that Stumped Feynman? I want to know if the integral: $$\int_0^{2\pi} \exp\left(\frac{7+5 \cos x}{10+6\cos x}\right) \cos \left( \frac{\sin x}{10+6 \cos x} \right) dx = 2\pi e^{2/3}$$ can be evaluated using strictly real methods. I've tried series of $e^x$ and $\cos x$ but to no avail. I tried differentiating under the integral, but nothing seemed to come out of it. Is there any wizardry that can conjure up this answer without complex analysis.",,"['calculus', 'integration']"
52,"Math competition problem, prove that $\int_{-\infty}^\infty e^{-\pi x^2 \left(\frac{\alpha +x}{\beta +x}\right)^2}dx=1~$ for $~0<\beta<\alpha$.","Math competition problem, prove that  for .",\int_{-\infty}^\infty e^{-\pi x^2 \left(\frac{\alpha +x}{\beta +x}\right)^2}dx=1~ ~0<\beta<\alpha,"Recently there was a math competition in our university where this question Question: Prove that $\displaystyle\int_{-\infty}^\infty e^{-\pi  x^2 \left(\frac{\scriptstyle\alpha +x}{\scriptstyle\beta +x}\right)^2}dx=1~$ for $~0<\beta<\alpha$ has been asked, but nobody could solve it. I know that $$ \int_{-\infty}^\infty e^{-\pi  x^2}dx=1 $$ but this doesn't help much. What are possible routes to deal with this kind of integrals? Any integration experts has any clue how this is done? Thanks.","Recently there was a math competition in our university where this question Question: Prove that $\displaystyle\int_{-\infty}^\infty e^{-\pi  x^2 \left(\frac{\scriptstyle\alpha +x}{\scriptstyle\beta +x}\right)^2}dx=1~$ for $~0<\beta<\alpha$ has been asked, but nobody could solve it. I know that $$ \int_{-\infty}^\infty e^{-\pi  x^2}dx=1 $$ but this doesn't help much. What are possible routes to deal with this kind of integrals? Any integration experts has any clue how this is done? Thanks.",,"['calculus', 'integration', 'definite-integrals', 'contest-math', 'closed-form']"
53,Prove that $f'(0)$ exists and $f'(0) = b/(a - 1)$,Prove that  exists and,f'(0) f'(0) = b/(a - 1),"Problem : If $f(x)$ is continous at $x=0$, and $\lim\limits_{x\to 0} \dfrac{f(ax)-f(x)}{x}=b$, $a, b$ are constants and $|a|>1$, prove that $f'(0)$ exists and $f'(0)=\dfrac{b}{a-1}$. This approach is definitely wrong: \begin{align} b&=\lim_{x\to 0} \frac{f(ax)-f(x)}{x}\\ &=\lim_{x\to 0} \frac{f(ax)-f(0)-(f(x)-f(0))}{x}\\ &=af'(0)-f'(0)\\ &=(a-1)f'(0) \end{align} I will show you a case why this approach is wrong: \[f(x)= \begin{cases} 1,&x\neq0\\ 0,&x=0 \end{cases}\]   $\lim_{x\to0}\dfrac{f(3x)-f(x)}{x}=\lim_{x\to0} \dfrac{1-1}{x}=0$ but $\lim_{x\to0}\dfrac{f(3x)}{x}=\infty$,$\lim_{x\to0}\dfrac{f(x)}{x}=\infty$ Does anyone know how to prove it? Thanks in advance!","Problem : If $f(x)$ is continous at $x=0$, and $\lim\limits_{x\to 0} \dfrac{f(ax)-f(x)}{x}=b$, $a, b$ are constants and $|a|>1$, prove that $f'(0)$ exists and $f'(0)=\dfrac{b}{a-1}$. This approach is definitely wrong: \begin{align} b&=\lim_{x\to 0} \frac{f(ax)-f(x)}{x}\\ &=\lim_{x\to 0} \frac{f(ax)-f(0)-(f(x)-f(0))}{x}\\ &=af'(0)-f'(0)\\ &=(a-1)f'(0) \end{align} I will show you a case why this approach is wrong: \[f(x)= \begin{cases} 1,&x\neq0\\ 0,&x=0 \end{cases}\]   $\lim_{x\to0}\dfrac{f(3x)-f(x)}{x}=\lim_{x\to0} \dfrac{1-1}{x}=0$ but $\lim_{x\to0}\dfrac{f(3x)}{x}=\infty$,$\lim_{x\to0}\dfrac{f(x)}{x}=\infty$ Does anyone know how to prove it? Thanks in advance!",,"['calculus', 'derivatives']"
54,How to calculate $\int_0^\pi \ln(1+\sin x)\mathrm dx$,How to calculate,\int_0^\pi \ln(1+\sin x)\mathrm dx,"How to calculate this integral $$\int_0^\pi \ln(1+\sin x)\mathrm dx$$ I didn't find this question in the previous questions. With the help of Wolframalpha I got an answer $-\pi \ln 2+4\mathbf{G}$, where $\mathbf{G}$ denotes Catalan's Constant.","How to calculate this integral $$\int_0^\pi \ln(1+\sin x)\mathrm dx$$ I didn't find this question in the previous questions. With the help of Wolframalpha I got an answer $-\pi \ln 2+4\mathbf{G}$, where $\mathbf{G}$ denotes Catalan's Constant.",,"['calculus', 'integration', 'analysis', 'closed-form']"
55,Solving an exponential equation with different bases,Solving an exponential equation with different bases,,"Solve the equation $2^x + 5^x = 3^x + 4^x$. I can figure out two special solutions $x=0$ and $x=1$, and I try to prove that they are the only two solutions. However, I find it hard to do so because I can't prove the monotony given there are also exponential in the derivative. Any hints to that?","Solve the equation $2^x + 5^x = 3^x + 4^x$. I can figure out two special solutions $x=0$ and $x=1$, and I try to prove that they are the only two solutions. However, I find it hard to do so because I can't prove the monotony given there are also exponential in the derivative. Any hints to that?",,"['calculus', 'exponential-function']"
56,find the maximum possible area of $\triangle{ABC}$,find the maximum possible area of,\triangle{ABC},"Let $ABC$ be of triangle with $\angle BAC = 60^\circ$ . Let $P$ be a point in its interior so that $PA=1, PB=2$ and $PC=3$. Find the maximum area of triangle $ABC$. I took reflection of point $P$ about the three sides of triangle and joined them to vertices of triangle. Thus I got a hexagon having area double of triangle, having one angle $120$ and sides $1,1,2,2,3,3$. We have to maximize area of this hexagon. For that, I used some trigonometry but it went very complicated and I couldn't get the solution.","Let $ABC$ be of triangle with $\angle BAC = 60^\circ$ . Let $P$ be a point in its interior so that $PA=1, PB=2$ and $PC=3$. Find the maximum area of triangle $ABC$. I took reflection of point $P$ about the three sides of triangle and joined them to vertices of triangle. Thus I got a hexagon having area double of triangle, having one angle $120$ and sides $1,1,2,2,3,3$. We have to maximize area of this hexagon. For that, I used some trigonometry but it went very complicated and I couldn't get the solution.",,"['calculus', 'geometry', 'optimization', 'triangles', 'area']"
57,"Global Minimum of $f(a) = \int _{-\infty}^{\infty} \exp\left(-|x|^a\right)dx, a\in(0,\infty)$",Global Minimum of,"f(a) = \int _{-\infty}^{\infty} \exp\left(-|x|^a\right)dx, a\in(0,\infty)","Playing around with the Standard Normal distribution, $\exp\left(-x^2\right)$, I was wondering about generalizing the distribution by parameterizing the $2$ to a variable $a$. After graphing the distribution for different values of $a$, I decided to see how the value of $a$ affected the area under the curve, which I call $f(a) = \int _{-\infty}^{\infty} \exp\left(-|x|^a\right)dx$. Unable to calculate this analytically (the usual method for calculating the case of $a=2$ by polar coordinates does not generalize) I decided to calculate the function numerically. Here is a graph of the function for $a\in[1,10]$: As you can see, the function has a minimum near (but not at) $a=2$. I am particularly interested in finding the exact value of the minimum value, though any other facts about this function would be interesting to know. Using Python, my best estimate for the global minimum is $a\approx2.1662269$. Here is a full list of what I know about the function: As $a\to0$, $f(a)\to\infty$ As $a\to\infty$, $f(a)\to2$ There is a global minimum around $a\approx2.1662269$ $f$ is monotonically decreasing on the interval $(0,\mathrm{minimum})$ and monotonically increasing on $(\mathrm{minimum},\infty)$ Besides that I'm stumped. I tried calculating $\frac{d}{da}f(a) = \frac{d}{da}\int _{-\infty}^{\infty} \exp\left(-|x|^a\right)dx$ but I'm not sure how to evaluate that (maybe there's some multivariable generalization of the Fundamental Theorem of Calculus I've forgotten?). I've taken Calculus classes through Calc III and Diff Eq but don't have a clue how to solve this.","Playing around with the Standard Normal distribution, $\exp\left(-x^2\right)$, I was wondering about generalizing the distribution by parameterizing the $2$ to a variable $a$. After graphing the distribution for different values of $a$, I decided to see how the value of $a$ affected the area under the curve, which I call $f(a) = \int _{-\infty}^{\infty} \exp\left(-|x|^a\right)dx$. Unable to calculate this analytically (the usual method for calculating the case of $a=2$ by polar coordinates does not generalize) I decided to calculate the function numerically. Here is a graph of the function for $a\in[1,10]$: As you can see, the function has a minimum near (but not at) $a=2$. I am particularly interested in finding the exact value of the minimum value, though any other facts about this function would be interesting to know. Using Python, my best estimate for the global minimum is $a\approx2.1662269$. Here is a full list of what I know about the function: As $a\to0$, $f(a)\to\infty$ As $a\to\infty$, $f(a)\to2$ There is a global minimum around $a\approx2.1662269$ $f$ is monotonically decreasing on the interval $(0,\mathrm{minimum})$ and monotonically increasing on $(\mathrm{minimum},\infty)$ Besides that I'm stumped. I tried calculating $\frac{d}{da}f(a) = \frac{d}{da}\int _{-\infty}^{\infty} \exp\left(-|x|^a\right)dx$ but I'm not sure how to evaluate that (maybe there's some multivariable generalization of the Fundamental Theorem of Calculus I've forgotten?). I've taken Calculus classes through Calc III and Diff Eq but don't have a clue how to solve this.",,"['calculus', 'optimization', 'transcendental-equations']"
58,Second Mean Value Theorem for Integrals Meaning,Second Mean Value Theorem for Integrals Meaning,,"The Second Mean Value Theorem for Integrals says that for $f (x)$ and $g(x)$ continuous on $[a, b]$  and $g(x)\ge 0$ $$\int_a^bf(x)g(x)\,dx=f(a)\int_a^cg(x)\,dx+f(b)\int_c^bg(x)\,dx$$ I have a difficult time understanding what this means, as opposed to the first mean value theorem for integrals, which is easy to conceptualize. Is there a graphical or 'in words' interpretation of this theorem that I may use to understand it better?","The Second Mean Value Theorem for Integrals says that for $f (x)$ and $g(x)$ continuous on $[a, b]$  and $g(x)\ge 0$ $$\int_a^bf(x)g(x)\,dx=f(a)\int_a^cg(x)\,dx+f(b)\int_c^bg(x)\,dx$$ I have a difficult time understanding what this means, as opposed to the first mean value theorem for integrals, which is easy to conceptualize. Is there a graphical or 'in words' interpretation of this theorem that I may use to understand it better?",,['calculus']
59,Tangent of a Straight Line,Tangent of a Straight Line,,"I just got back a math test in my EF Calc class, and I disagree with my teacher on this one problem. We are using derivatives to determine equations of lines tangent to a given equation. The equation given was: $f(x) = 27$ My response was ""No Solution. Not a Curve."" My teacher responded by saying that $y = 27$ was the correct answer. My argument is that these two lines have an infinite number of intersections, while my teacher says they only intersect once. So, my question is whether two lines with the same equation intersect once or infinitely. I apologize if this question is of a lower degree of difficulty than the rest of the site, but this seemed the best place to ask. Thank you in advance for any help.","I just got back a math test in my EF Calc class, and I disagree with my teacher on this one problem. We are using derivatives to determine equations of lines tangent to a given equation. The equation given was: $f(x) = 27$ My response was ""No Solution. Not a Curve."" My teacher responded by saying that $y = 27$ was the correct answer. My argument is that these two lines have an infinite number of intersections, while my teacher says they only intersect once. So, my question is whether two lines with the same equation intersect once or infinitely. I apologize if this question is of a lower degree of difficulty than the rest of the site, but this seemed the best place to ask. Thank you in advance for any help.",,"['calculus', 'derivatives']"
60,If $dx/dy =\sin(x)$ then is $dy/dx = 1/\sin(x)$?,If  then is ?,dx/dy =\sin(x) dy/dx = 1/\sin(x),"If $\dfrac{dx}{dy} = \sin(x),$ then is $\dfrac{dy}{dx} = \dfrac{1}{\sin(x)}$? I'm trying to understand how to manipulate $dx$ and $dy$ quantities effectively.","If $\dfrac{dx}{dy} = \sin(x),$ then is $\dfrac{dy}{dx} = \dfrac{1}{\sin(x)}$? I'm trying to understand how to manipulate $dx$ and $dy$ quantities effectively.",,"['calculus', 'derivatives']"
61,Proving the existence of a point with a certain property for a continuous function,Proving the existence of a point with a certain property for a continuous function,,"Let $f:[0,1]\to\mathbb{R}$ a continuous function and $\int_0^1xf(x)dx=0$. Show that there exists a point $c\in(0,1)$ so that $f(c)=(\int_c^1f(x)dx)^2$. As a potential solution, I tried assuming that no such point exists, then the function $g(x)=f(x)-(\int_x^1f(t)dt)^2$ would have constant sign for all $x\in(0,1)$. $g(x)>0$ can't be true for all $x\in(0,1)$ since then  $\int_0^1xf(x)dx>0$. But I can't figure out how to prove that $g(x)<0$ for all $x$ can't be true. Thanks","Let $f:[0,1]\to\mathbb{R}$ a continuous function and $\int_0^1xf(x)dx=0$. Show that there exists a point $c\in(0,1)$ so that $f(c)=(\int_c^1f(x)dx)^2$. As a potential solution, I tried assuming that no such point exists, then the function $g(x)=f(x)-(\int_x^1f(t)dt)^2$ would have constant sign for all $x\in(0,1)$. $g(x)>0$ can't be true for all $x\in(0,1)$ since then  $\int_0^1xf(x)dx>0$. But I can't figure out how to prove that $g(x)<0$ for all $x$ can't be true. Thanks",,"['calculus', 'integration', 'functions', 'continuity']"
62,Importance of Rolle's and Lagrange's theorem in daily life: for school children,Importance of Rolle's and Lagrange's theorem in daily life: for school children,,"I am school teacher and teaching math. I know the Rolle's theorem and Lagrange's theorem. I can solve the problems numerically then and there. However, I was completely failed to explain the significance or applications of these theorems by practical. If this site can help me to explain both of the theorems with good practical examples are enough and I can teach my students well. Awaiting your replies.","I am school teacher and teaching math. I know the Rolle's theorem and Lagrange's theorem. I can solve the problems numerically then and there. However, I was completely failed to explain the significance or applications of these theorems by practical. If this site can help me to explain both of the theorems with good practical examples are enough and I can teach my students well. Awaiting your replies.",,['calculus']
63,"$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x$ by substitution",by substitution,"\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x","I'm trying to solve the following integral: $$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x$$ Using the substitution method with the substitution $u = \sin\left(x\right)$. The exercise has two parts: the first one is using the substitution $u = \cos\left(x\right)$. No problem. I'm having difficulties with the second part, which is using the substitution $u = \sin\left(x\right)$. I spent a couple of hours with the exercise before asking here, and after some trials I got this: $$\int f\left(g\left(x\right)\right)g'\left(x\right)\,\mathrm{d}x = \int f\left(u\right)\,\mathrm{d}u$$ $$g\left(x\right) = \sin\left(x\right)$$ $$g'\left(x\right) = \cos\left(x\right)$$ $$f\left(x\right) = \frac{x}{\cos^2\left(\arcsin(x)\right)} = \frac{x}{1 - x^2}$$ $$\int f\left(u\right)\,\mathrm{d}u = -\frac{1}{2}\log|1 - u^2| + C = -\frac{1}{2}\log|1 - \sin^2\left(x\right)| + C$$ $$1 - \sin^2\left(x\right) = \cos^2\left(x\right)$$ $$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x = -\frac{1}{2}\log|\cos^2\left(x\right)| + C = -\log|\cos\left(x\right)| + C$$ But it feels too complicated, $f\left(x\right)$ was really hard for me to discover. What am I missing?","I'm trying to solve the following integral: $$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x$$ Using the substitution method with the substitution $u = \sin\left(x\right)$. The exercise has two parts: the first one is using the substitution $u = \cos\left(x\right)$. No problem. I'm having difficulties with the second part, which is using the substitution $u = \sin\left(x\right)$. I spent a couple of hours with the exercise before asking here, and after some trials I got this: $$\int f\left(g\left(x\right)\right)g'\left(x\right)\,\mathrm{d}x = \int f\left(u\right)\,\mathrm{d}u$$ $$g\left(x\right) = \sin\left(x\right)$$ $$g'\left(x\right) = \cos\left(x\right)$$ $$f\left(x\right) = \frac{x}{\cos^2\left(\arcsin(x)\right)} = \frac{x}{1 - x^2}$$ $$\int f\left(u\right)\,\mathrm{d}u = -\frac{1}{2}\log|1 - u^2| + C = -\frac{1}{2}\log|1 - \sin^2\left(x\right)| + C$$ $$1 - \sin^2\left(x\right) = \cos^2\left(x\right)$$ $$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x = -\frac{1}{2}\log|\cos^2\left(x\right)| + C = -\log|\cos\left(x\right)| + C$$ But it feels too complicated, $f\left(x\right)$ was really hard for me to discover. What am I missing?",,"['calculus', 'integration']"
64,On integrals related to $\int^{+\infty}_{-\infty} e^{-x^2} dx = \sqrt{\pi}$,On integrals related to,\int^{+\infty}_{-\infty} e^{-x^2} dx = \sqrt{\pi},"You are given the result that $$\int^{+\infty}_{-\infty} e^{-x^2} dx = \sqrt{\pi}$$ a. Use this result to find $$\int^{+\infty}_{-\infty} e^{-ax^2} dx$$ b. Use the above results to find $$\int^{+\infty}_{-\infty} x^2e^{-ax^2} dx$$ [Hint: Consider $\frac{d}{d\alpha} \int^{+\infty}_{-\infty}e^{-ax^2}dx$] c. Use the above results to to show that $$P(x,t)=\frac{1}{\sqrt{4 \pi D t}} \exp \left( -\frac{x^2}{4Dt}\right) $$ is a normalized distribution. (Note: I'm still learning - this problem is somewhat advanced for my level, so if someone could write out an explicit and complete solution, that would be the most helpful answer for me.) As of now, @matt has successfully helped me to understand the solutions for (a) and (b) in his response below; but I still need to have the solution for (c) explained to me.  I do not understand what needs to be done. for (c).","You are given the result that $$\int^{+\infty}_{-\infty} e^{-x^2} dx = \sqrt{\pi}$$ a. Use this result to find $$\int^{+\infty}_{-\infty} e^{-ax^2} dx$$ b. Use the above results to find $$\int^{+\infty}_{-\infty} x^2e^{-ax^2} dx$$ [Hint: Consider $\frac{d}{d\alpha} \int^{+\infty}_{-\infty}e^{-ax^2}dx$] c. Use the above results to to show that $$P(x,t)=\frac{1}{\sqrt{4 \pi D t}} \exp \left( -\frac{x^2}{4Dt}\right) $$ is a normalized distribution. (Note: I'm still learning - this problem is somewhat advanced for my level, so if someone could write out an explicit and complete solution, that would be the most helpful answer for me.) As of now, @matt has successfully helped me to understand the solutions for (a) and (b) in his response below; but I still need to have the solution for (c) explained to me.  I do not understand what needs to be done. for (c).",,"['calculus', 'integration']"
65,What is the limit $\lim\limits_{n \to \infty} \frac{n^{\sqrt{n}}}{2^n}$?,What is the limit ?,\lim\limits_{n \to \infty} \frac{n^{\sqrt{n}}}{2^n},"Question : Prove that $\displaystyle \lim\limits_{n \to \infty} \frac{n^{\sqrt{n}}}{2^n} = 0. $ I was thinking of using the Squeeze Theorem (might not be the right way to go), but finding an upper-bound function proved to be quite tricky.","Question : Prove that I was thinking of using the Squeeze Theorem (might not be the right way to go), but finding an upper-bound function proved to be quite tricky.","\displaystyle
\lim\limits_{n \to \infty} \frac{n^{\sqrt{n}}}{2^n} = 0.
","['calculus', 'limits']"
66,Why is $dy dx = r dr d \theta$ [duplicate],Why is  [duplicate],dy dx = r dr d \theta,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Explain $\iint \mathrm dx\mathrm dy = \iint r \mathrm d\alpha\mathrm dr$ I'm reading the proof of Gaussian integration. When we change to polar coordinates, why do we get an ""extra"" r in there? \begin{align} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2+y^2)}\ dx dy &= \int_0^{2\pi} \int_0^{\infty} e^{-r^2}r\ dr\ d\theta\\ \end{align} I've looked at a few different proofs: http://www.math.uconn.edu/~kconrad/blurbs/analysis/probint.pdf ""The differential dx dy represents an element of area in cartesian coordinates, with the domain of integration extending over the entire xy-plane. An alternative representation of the last integral can be expressed in plane polar coordinates r, $\theta$"" http://www.umich.edu/~chem461/Gaussian%20Integrals.pdf but none explain this step fully enough for me to really see why this happened.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Explain $\iint \mathrm dx\mathrm dy = \iint r \mathrm d\alpha\mathrm dr$ I'm reading the proof of Gaussian integration. When we change to polar coordinates, why do we get an ""extra"" r in there? \begin{align} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(x^2+y^2)}\ dx dy &= \int_0^{2\pi} \int_0^{\infty} e^{-r^2}r\ dr\ d\theta\\ \end{align} I've looked at a few different proofs: http://www.math.uconn.edu/~kconrad/blurbs/analysis/probint.pdf ""The differential dx dy represents an element of area in cartesian coordinates, with the domain of integration extending over the entire xy-plane. An alternative representation of the last integral can be expressed in plane polar coordinates r, $\theta$"" http://www.umich.edu/~chem461/Gaussian%20Integrals.pdf but none explain this step fully enough for me to really see why this happened.",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'integration', 'polar-coordinates']"
67,Interesting integral $\int_0^\pi \frac{\sin(x)}{x} e^{x \cot(x)} \ \mathrm{d}x = \pi$,Interesting integral,\int_0^\pi \frac{\sin(x)}{x} e^{x \cot(x)} \ \mathrm{d}x = \pi,"I recently received the following integral result which I've been trying to prove, from a friend who himself got it from the famous M.L. Glasser as a potential new identity: $$\boxed{\int_0^\pi \frac{\sin(x)}{x} e^{x \cot(x)} \ \mathrm{d}x = \pi}$$ Numerically this checks out, but I've made virtually no progress on this, due to how bizarre the integrand is. The following are some results I have: The Lobachevsky identity doesn't work in this case as we'd need an integral from $0$ to $\infty$ , and the only $2\pi$ -length interval the integrand converges on is $(-\pi, \pi)$ . I haven't found any complex analytic ways of proceeding, but given the size and complexity of possible functions and contours, this doesn't really mean much. Series expanding the exponential is bad since term-by-term integration yields divergent values, and so the sum is not well-behaved. One thing I have noticed is that $\frac{\sin(x)}{x} e^{x \cot(x)} = \frac{\sin(x)}{x} \exp\left (\frac{x}{\sin(x)} \cos(x)\right )$ so DUTIS may be a viable option, but I didn't make much progress from here. Any help with proving this amazing result would be great!","I recently received the following integral result which I've been trying to prove, from a friend who himself got it from the famous M.L. Glasser as a potential new identity: Numerically this checks out, but I've made virtually no progress on this, due to how bizarre the integrand is. The following are some results I have: The Lobachevsky identity doesn't work in this case as we'd need an integral from to , and the only -length interval the integrand converges on is . I haven't found any complex analytic ways of proceeding, but given the size and complexity of possible functions and contours, this doesn't really mean much. Series expanding the exponential is bad since term-by-term integration yields divergent values, and so the sum is not well-behaved. One thing I have noticed is that so DUTIS may be a viable option, but I didn't make much progress from here. Any help with proving this amazing result would be great!","\boxed{\int_0^\pi \frac{\sin(x)}{x} e^{x \cot(x)} \ \mathrm{d}x = \pi} 0 \infty 2\pi (-\pi, \pi) \frac{\sin(x)}{x} e^{x \cot(x)} = \frac{\sin(x)}{x} \exp\left (\frac{x}{\sin(x)} \cos(x)\right )","['calculus', 'integration', 'definite-integrals']"
68,What is the reason that equations such as $\tan x = 2x$ can only be solved with the help of algorithms?,What is the reason that equations such as  can only be solved with the help of algorithms?,\tan x = 2x,"This is my first StackExchange question: What is the reason that equations such as $\tan x = 2x$ , $\cos x = x$ , $\sin(x) = x^2$ and other questions that involve the same variable within a trigonometric function and outside the trigonometric function can only be solved with computer algorithms and have extremely complex closed forms; such as the Dottie number, the root of $\cos x = x$ , having the mind-blowing closed form $\sqrt{1-\left(2I_{\frac{1}{2}}^{-1}\left(\frac{1}{2},\frac{3}{2}\right)-1\right)^2}\,,$ which is really fascinating to me. For $\cos x = x$ , I tried solving the equation using the complex definition of $\cos x$ (aka. the cooler version of cosine), $\frac{e^{ix}+e^{-ix}}{2} = x$ but after using a quadratic formula to solve for $e^{ix}$ , I found that I just ended up with $x$ being equal to the complex definition of inverse cosine (arccos) in the formula $x = -i\ln(x + \sqrt{x^2-1})$ and I had gone around in circles, I thought ""Is this really an impossible equation to solve for $x$ ?"" The sum inside the natural logarithm, the $i$ being present in the equation even though the Dottie number is a real number approximately $0.739085$ ... and the $\sqrt{x^2-1}$ , I had seen that thing everywhere in Pythagoras and trigonometric calculus. It's as if mathematics had put as many barriers around the $x$ as possible to prevent you from solving for $x$ . And I had a similiar problem with $\sin x = x$ , I knew for a fact that the solution was definitely $0$ and yet the ""solution"" I got was total garbage that looked nothing like $0$ . And apparently this equation also had infinitely many complex solutions as well, which I don't grasp at all. These equations look so simple from first glance and yet are mathematically impossible for a human to solve for an exact form, but why is that? is it something to do with ""transcendental numbers""? Does it have any applications in trigonometry and calculus?","This is my first StackExchange question: What is the reason that equations such as , , and other questions that involve the same variable within a trigonometric function and outside the trigonometric function can only be solved with computer algorithms and have extremely complex closed forms; such as the Dottie number, the root of , having the mind-blowing closed form which is really fascinating to me. For , I tried solving the equation using the complex definition of (aka. the cooler version of cosine), but after using a quadratic formula to solve for , I found that I just ended up with being equal to the complex definition of inverse cosine (arccos) in the formula and I had gone around in circles, I thought ""Is this really an impossible equation to solve for ?"" The sum inside the natural logarithm, the being present in the equation even though the Dottie number is a real number approximately ... and the , I had seen that thing everywhere in Pythagoras and trigonometric calculus. It's as if mathematics had put as many barriers around the as possible to prevent you from solving for . And I had a similiar problem with , I knew for a fact that the solution was definitely and yet the ""solution"" I got was total garbage that looked nothing like . And apparently this equation also had infinitely many complex solutions as well, which I don't grasp at all. These equations look so simple from first glance and yet are mathematically impossible for a human to solve for an exact form, but why is that? is it something to do with ""transcendental numbers""? Does it have any applications in trigonometry and calculus?","\tan x = 2x \cos x = x \sin(x) = x^2 \cos x = x \sqrt{1-\left(2I_{\frac{1}{2}}^{-1}\left(\frac{1}{2},\frac{3}{2}\right)-1\right)^2}\,, \cos x = x \cos x \frac{e^{ix}+e^{-ix}}{2} = x e^{ix} x x = -i\ln(x + \sqrt{x^2-1}) x i 0.739085 \sqrt{x^2-1} x x \sin x = x 0 0","['calculus', 'complex-analysis']"
69,Find $f(1729)$ if $n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=nf(x)+0.5$,Find  if,f(1729) n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=nf(x)+0.5,"I was asked this question- Let $f$ be a real continuous function satisfying $f(0)=0$ and for each natural number $n$ $$n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=nf(x)+0.5$$ Then find the value of $f(1729)$ I couldn't make much of a progress. But I tried using $$F(x)=\int_0^x f(x)\;\text{d}x$$ so that $$\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=F\left(x+\frac 1 n\right)-F(x)$$ and then maybe letting $n$ go to infinity such that we can think about the behaviour of $$\lim_{n\to \infty} n^2\cdot \left(F\left(x+\frac 1 n\right)-F(x)\right)$$ But, soon I realised that we don't even know whether this limit exists, since we don't have any idea of whether the limit in the RHS exists, i.e., we don't know whether $$\lim_{n\to \infty} nf(x)$$ exists. However, if this would somehow exist, we could have argued that the limit of the LHS also exists, and thus, $$\left(F\left(x+\frac 1 n\right)-F(x)\right)=\frac {h(x)}{n^2}$$ But still, there's no progress. Another approach that I tried was to sum up the equation from $1$ to $k$ to get $$\sum_{n=1}^k n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t = \frac {k(k+1)}2 f(x) + \frac k 2$$ But how to calculate the sum on the LHS? I tried writing it as $$\sum_{n=1}^k n^2\cdot \left(F\left(x+\frac 1 n\right)-F(x)\right)=\sum_{n=1}^k n^2\cdot F\left(x+\frac 1 n\right) - \sum_{n=1}^k n^2\cdot F(x)$$ where the second term maybe easy to calculate, but what about the first term? Any help would be appreciated. Also, I'm not really sure about the tags- feel free to edit them.","I was asked this question- Let be a real continuous function satisfying and for each natural number Then find the value of I couldn't make much of a progress. But I tried using so that and then maybe letting go to infinity such that we can think about the behaviour of But, soon I realised that we don't even know whether this limit exists, since we don't have any idea of whether the limit in the RHS exists, i.e., we don't know whether exists. However, if this would somehow exist, we could have argued that the limit of the LHS also exists, and thus, But still, there's no progress. Another approach that I tried was to sum up the equation from to to get But how to calculate the sum on the LHS? I tried writing it as where the second term maybe easy to calculate, but what about the first term? Any help would be appreciated. Also, I'm not really sure about the tags- feel free to edit them.",f f(0)=0 n n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=nf(x)+0.5 f(1729) F(x)=\int_0^x f(x)\;\text{d}x \int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=F\left(x+\frac 1 n\right)-F(x) n \lim_{n\to \infty} n^2\cdot \left(F\left(x+\frac 1 n\right)-F(x)\right) \lim_{n\to \infty} nf(x) \left(F\left(x+\frac 1 n\right)-F(x)\right)=\frac {h(x)}{n^2} 1 k \sum_{n=1}^k n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t = \frac {k(k+1)}2 f(x) + \frac k 2 \sum_{n=1}^k n^2\cdot \left(F\left(x+\frac 1 n\right)-F(x)\right)=\sum_{n=1}^k n^2\cdot F\left(x+\frac 1 n\right) - \sum_{n=1}^k n^2\cdot F(x),"['calculus', 'algebra-precalculus', 'functions']"
70,Is this function decreasing in $x$?,Is this function decreasing in ?,x,"Consider $$x \mapsto \frac{\int_{x-b}^{x+b} e^{-\frac{z^2}{2} }\text d z}{\int_{x-c}^{x+c} e^{-\frac{z^2}{2} }\text d z}$$ decreasing in $x\in [0,\infty)$ , if $c > b > 0$ ? What I tried: taking the derivative in $x$ yields: $$\frac{\left( e^{-\frac{(x+b)^2}{2}} - e^{-\frac{(x-b)^2}{2}} \right)\int_{x-c}^{x+c} e^{-\frac{z^2}{2}}\text d z - \left( e^{-\frac{(x+c)^2}{2}} - e^{-\frac{(x-c)^2}{2}} \right)\int_{x-b}^{x+b} e^{-\frac{z^2}{2}}\text d z}{\left( \int_{x-c}^{x+c} e^{-\frac{z^2}{2}}\text d z \right)^2} $$ where the sign is determined by the numerator, so the question is if $$\frac{e^{-\frac{(x+b)^2}{2}} - e^{-\frac{(x-b)^2}{2}}}{e^{-\frac{(x+c)^2}{2}} - e^{-\frac{(x-c)^2}{2}}} - \frac{\int_{x-b}^{x+b} e^{-\frac{z^2}{2} }\text d z}{\int_{x-c}^{x+c} e^{-\frac{z^2}{2} }\text d z} \geq 0 \ ?$$","Consider decreasing in , if ? What I tried: taking the derivative in yields: where the sign is determined by the numerator, so the question is if","x \mapsto \frac{\int_{x-b}^{x+b} e^{-\frac{z^2}{2} }\text d z}{\int_{x-c}^{x+c} e^{-\frac{z^2}{2} }\text d z} x\in [0,\infty) c > b > 0 x \frac{\left( e^{-\frac{(x+b)^2}{2}} - e^{-\frac{(x-b)^2}{2}} \right)\int_{x-c}^{x+c} e^{-\frac{z^2}{2}}\text d z - \left( e^{-\frac{(x+c)^2}{2}} - e^{-\frac{(x-c)^2}{2}} \right)\int_{x-b}^{x+b} e^{-\frac{z^2}{2}}\text d z}{\left( \int_{x-c}^{x+c} e^{-\frac{z^2}{2}}\text d z \right)^2}  \frac{e^{-\frac{(x+b)^2}{2}} - e^{-\frac{(x-b)^2}{2}}}{e^{-\frac{(x+c)^2}{2}} - e^{-\frac{(x-c)^2}{2}}} - \frac{\int_{x-b}^{x+b} e^{-\frac{z^2}{2} }\text d z}{\int_{x-c}^{x+c} e^{-\frac{z^2}{2} }\text d z} \geq 0 \ ?","['calculus', 'integration', 'inequality', 'integral-inequality', 'error-function']"
71,Integral $\int_{-1}^{1} \frac{1}{(1+u^2)^{n/2}} \exp{\left(-2\pi \frac{a^2+b^2}{1+u^2}\right)} \exp{\left(-4\pi i ab \frac{u}{1+u^2}\right)} du $,Integral,\int_{-1}^{1} \frac{1}{(1+u^2)^{n/2}} \exp{\left(-2\pi \frac{a^2+b^2}{1+u^2}\right)} \exp{\left(-4\pi i ab \frac{u}{1+u^2}\right)} du ,"My recent research attempts got stuck in the following integral: $$\int_{-1}^{1} \frac{1}{(1+u^2)^{n/2}} \exp{\left(-2\pi \frac{a^2+b^2}{1+u^2}\right)} \exp{\left(-4\pi i ab \frac{u}{1+u^2}\right)} \mathrm{d}u $$ It is the most ""compact"" version (apart from numerical factors) I obtained from the starting problem $$\int_{0}^{1} \frac{1}{(2x^2-2x+1)^{n/2}} e^{-A(x)\pi a^2} e^{-B(x)\pi b^2}e^{2\pi C(x) ab}\mathrm{d}x$$ where $$A(x)=2-\frac{(2x-1)^2}{2x^2-2x+1}\qquad B(x)=\frac{1}{2x^2-2x+1}\qquad C(x)=-\frac{2x-1}{2x^2-2x+1}$$ and with $a,b\in \mathbb{R}^n$ ( $ab=a\cdot b$ ).  I tried with several substitutions (apart $u=2x-1$ ), integration by parts, complex integration, derivation with respect to a parameter (""Feynman's trick""), exploiting the complex exponential as cosine/sine, but I believe that the main obstacle to a ""good"" substitution is the presence of the linear term $u$ in the complex exponential. I am interested in a closed form, if any. Update 1 By symmetry arguments and with the suggested substitution I got $$\int_0^{\pi/4}  (\cos \theta)^{n-2} e^{-2\pi(a^2+b^2)\cos^2 \theta} \cos(2\pi ab \sin(2\theta)) \mathrm{d}\theta$$ but I have no further ideas. I tried with several substitutions (log-tan, half-angle, etc.) but with no results. I think that some special function is involved (the last term calls for Bessel) but I am definitely not an expert. (See mathstackuser12's answer for a complete account on this). Update 2 Before assigning the bounty I would ask if a complex integration way could be fruitful: since $$\frac{1}{1+u^2} = \frac{1}{2} \left(\frac{i}{u+i} - \frac{i}{u-i} \right)$$ and $$\overline{\frac{1}{u-i}}=\frac{1}{u+i}$$ we can write $$ \int_{0}^1 \frac{1}{(1+u^2)^{n/2}}\exp{\left(-2\pi (a-b)^2 \frac{1}{1+u^2}\right)} \exp{\left(-4\pi ab \frac{i}{u+i}\right)} $$ or $$ \int_{0}^1 \frac{1}{(1+u^2)^{n/2}} \exp{\left(\pi (a-b)^2 \frac{i}{u-i}\right)} \exp{\left(-(\pi(a-b)^2+4\pi ab)\frac{i}{u+i}\right)} \mathrm{d}u$$ and I wonder if there exists a useful path in the complex plane, surrounding a domain of analyticity (I tried without success), or if computing the residues in a smart way could be a good thing.‌","My recent research attempts got stuck in the following integral: It is the most ""compact"" version (apart from numerical factors) I obtained from the starting problem where and with ( ).  I tried with several substitutions (apart ), integration by parts, complex integration, derivation with respect to a parameter (""Feynman's trick""), exploiting the complex exponential as cosine/sine, but I believe that the main obstacle to a ""good"" substitution is the presence of the linear term in the complex exponential. I am interested in a closed form, if any. Update 1 By symmetry arguments and with the suggested substitution I got but I have no further ideas. I tried with several substitutions (log-tan, half-angle, etc.) but with no results. I think that some special function is involved (the last term calls for Bessel) but I am definitely not an expert. (See mathstackuser12's answer for a complete account on this). Update 2 Before assigning the bounty I would ask if a complex integration way could be fruitful: since and we can write or and I wonder if there exists a useful path in the complex plane, surrounding a domain of analyticity (I tried without success), or if computing the residues in a smart way could be a good thing.‌","\int_{-1}^{1} \frac{1}{(1+u^2)^{n/2}} \exp{\left(-2\pi \frac{a^2+b^2}{1+u^2}\right)} \exp{\left(-4\pi i ab \frac{u}{1+u^2}\right)} \mathrm{d}u  \int_{0}^{1} \frac{1}{(2x^2-2x+1)^{n/2}} e^{-A(x)\pi a^2} e^{-B(x)\pi b^2}e^{2\pi C(x) ab}\mathrm{d}x A(x)=2-\frac{(2x-1)^2}{2x^2-2x+1}\qquad B(x)=\frac{1}{2x^2-2x+1}\qquad C(x)=-\frac{2x-1}{2x^2-2x+1} a,b\in \mathbb{R}^n ab=a\cdot b u=2x-1 u \int_0^{\pi/4}  (\cos \theta)^{n-2} e^{-2\pi(a^2+b^2)\cos^2 \theta} \cos(2\pi ab \sin(2\theta)) \mathrm{d}\theta \frac{1}{1+u^2} = \frac{1}{2} \left(\frac{i}{u+i} - \frac{i}{u-i} \right) \overline{\frac{1}{u-i}}=\frac{1}{u+i}  \int_{0}^1 \frac{1}{(1+u^2)^{n/2}}\exp{\left(-2\pi (a-b)^2 \frac{1}{1+u^2}\right)} \exp{\left(-4\pi ab \frac{i}{u+i}\right)}   \int_{0}^1 \frac{1}{(1+u^2)^{n/2}} \exp{\left(\pi (a-b)^2 \frac{i}{u-i}\right)} \exp{\left(-(\pi(a-b)^2+4\pi ab)\frac{i}{u+i}\right)} \mathrm{d}u","['calculus', 'integration', 'definite-integrals', 'special-functions', 'complex-integration']"
72,Understanding the flat (uniform) Dirichlet distribution density over a simplex,Understanding the flat (uniform) Dirichlet distribution density over a simplex,,"This should be really straightforward from the formula, but somehow I'm having trouble understanding the density of a Dirichlet distribution with $\alpha = [1, 1, ... 1] \in R^k$, which is a uniform distribution over the $k-1$ dimensional simplex. For example, $Dir(x;[1,1,1])$ is the same as a uniform distribution over the triangle with vertices $[0,0,1], [0,1,0]$, and $[0,0,1]$ (see pic below). Since there's 1 unit of probability mass uniformly spread over the triangle, I thought the density should simply be $1/Area(triangle)=2/\sqrt(3)$; but the formula is giving me $Dir(x;[1,1,1])=2$. How come?","This should be really straightforward from the formula, but somehow I'm having trouble understanding the density of a Dirichlet distribution with $\alpha = [1, 1, ... 1] \in R^k$, which is a uniform distribution over the $k-1$ dimensional simplex. For example, $Dir(x;[1,1,1])$ is the same as a uniform distribution over the triangle with vertices $[0,0,1], [0,1,0]$, and $[0,0,1]$ (see pic below). Since there's 1 unit of probability mass uniformly spread over the triangle, I thought the density should simply be $1/Area(triangle)=2/\sqrt(3)$; but the formula is giving me $Dir(x;[1,1,1])=2$. How come?",,"['calculus', 'probability', 'probability-distributions']"
73,Evaluation of $\sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right)$,Evaluation of,\sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right),"Evaluation of $$\sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right)$$ $\bf{My\; Try::}$  Here I have solved it using Definite Integration, Like $$\sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right)=\sum^{\infty}_{n=1}\int_{0}^{1}\left(x^{3n}-x^{3n+1}\right)dx$$ So we get $$ = \int_{0}^{1}(1-x)\sum^{\infty}_{n=1}\left(x^{3n}\right)dx = \int_{0}^{1}\frac{(1-x)x^3}{1-x^3}dx=\int_{0}^{1}\frac{x^3}{x^2+x+1}dx$$ So we get Sum $$ = \sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right) = \int_{0}^{1}\frac{x^3}{x^2+x+1}dx = \frac{1}{18}\left(2\pi\sqrt{3}-9\right)$$ My Question is can we solve above sum without Using DEfinite Integration, If yes Then how can I solve it, Help required Thanks","Evaluation of $$\sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right)$$ $\bf{My\; Try::}$  Here I have solved it using Definite Integration, Like $$\sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right)=\sum^{\infty}_{n=1}\int_{0}^{1}\left(x^{3n}-x^{3n+1}\right)dx$$ So we get $$ = \int_{0}^{1}(1-x)\sum^{\infty}_{n=1}\left(x^{3n}\right)dx = \int_{0}^{1}\frac{(1-x)x^3}{1-x^3}dx=\int_{0}^{1}\frac{x^3}{x^2+x+1}dx$$ So we get Sum $$ = \sum^{\infty}_{n=1}\left(\frac{1}{3n+1}-\frac{1}{3n+2}\right) = \int_{0}^{1}\frac{x^3}{x^2+x+1}dx = \frac{1}{18}\left(2\pi\sqrt{3}-9\right)$$ My Question is can we solve above sum without Using DEfinite Integration, If yes Then how can I solve it, Help required Thanks",,['calculus']
74,"When could we get $f' = f^{-1}$, where $f^{-1}$ is the inverse function of $f$, and not $\frac{1}{f}$","When could we get , where  is the inverse function of , and not",f' = f^{-1} f^{-1} f \frac{1}{f},"Personal question : Is there a general solution to the equation $f' = f^{-1}$, where $f^{-1}$   is the inverse function of $f$, and not $\frac{1}{f}$. I think this question is difficult, and I don't have the competence to answer it. Are there someone who is able to rigorously answer this question?","Personal question : Is there a general solution to the equation $f' = f^{-1}$, where $f^{-1}$   is the inverse function of $f$, and not $\frac{1}{f}$. I think this question is difficult, and I don't have the competence to answer it. Are there someone who is able to rigorously answer this question?",,"['calculus', 'ordinary-differential-equations']"
75,Closed form to an interesting series: $\sum_{n=1}^\infty \frac{1}{1+n^3}$,Closed form to an interesting series:,\sum_{n=1}^\infty \frac{1}{1+n^3},"Intutitively, I feel that there is a closed form to  $$\sum_{n=1}^\infty \frac{1}{1+n^3}$$ I don't know why but this sum has really proved difficult. Attempted manipulating a Mellin Transform on the integral solution: $$\int_0^\infty \frac{\text{d}x}{1+x^3}=\frac{\pi}{3}\csc \frac{\pi}{3}$$ But to little avail. Checking W|A gives the austere solution: $$\frac{1}{3}\sum_{\{x|x^3+1=0\}} x \space\text{digamma}(1-x) $$ Which I completely don't understand. Thank you for any help.","Intutitively, I feel that there is a closed form to  $$\sum_{n=1}^\infty \frac{1}{1+n^3}$$ I don't know why but this sum has really proved difficult. Attempted manipulating a Mellin Transform on the integral solution: $$\int_0^\infty \frac{\text{d}x}{1+x^3}=\frac{\pi}{3}\csc \frac{\pi}{3}$$ But to little avail. Checking W|A gives the austere solution: $$\frac{1}{3}\sum_{\{x|x^3+1=0\}} x \space\text{digamma}(1-x) $$ Which I completely don't understand. Thank you for any help.",,"['calculus', 'integration']"
76,Is the integral always the area under the curve?,Is the integral always the area under the curve?,,"It might be a stupid question but if I were to ask to compute the definite integral $$\int_{\frac{- \pi}{2}}^{\frac{\pi}{2}} \sin(x) \ dx$$ then on plugging the values then I would get ""$0$"" as the answer. But if I were to find the area covered by the function then, should I integrate as following: $$AREA =  \left|\int_{\frac{- \pi}{2}}^{0} \sin(x) \ dx\right| + \int_{0}^{\frac{\pi}{2}} \sin(x) \ dx = 2??$$","It might be a stupid question but if I were to ask to compute the definite integral $$\int_{\frac{- \pi}{2}}^{\frac{\pi}{2}} \sin(x) \ dx$$ then on plugging the values then I would get ""$0$"" as the answer. But if I were to find the area covered by the function then, should I integrate as following: $$AREA =  \left|\int_{\frac{- \pi}{2}}^{0} \sin(x) \ dx\right| + \int_{0}^{\frac{\pi}{2}} \sin(x) \ dx = 2??$$",,"['calculus', 'definite-integrals']"
77,gaussian integral of power of cdf : $\int_{-\infty}^{+\infty} \Phi(x)^n \cdot \phi(a+bx) \cdot dx$,gaussian integral of power of cdf :,\int_{-\infty}^{+\infty} \Phi(x)^n \cdot \phi(a+bx) \cdot dx,"Is there an analytic solution for the following Gaussian integral? $$\int_{-\infty}^{+\infty} \Phi(x)^n \cdot \phi(a+bx) \cdot dx$$ with $n$, a positive integer (typically under 10) $a,b$, real numbers (typical values: $a$ between 1 and 30, and $b$ between 1 and 10) $\Phi(\cdot)$, the standard normal cumulative distribution function $\phi(\cdot)$, the standard normal density function I found a solution for $n=1$ and $n=2$ (see http://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions ) However, I would need a general solution (for any $n$) if it exists. If not, is there a good approximation? Thanks.","Is there an analytic solution for the following Gaussian integral? $$\int_{-\infty}^{+\infty} \Phi(x)^n \cdot \phi(a+bx) \cdot dx$$ with $n$, a positive integer (typically under 10) $a,b$, real numbers (typical values: $a$ between 1 and 30, and $b$ between 1 and 10) $\Phi(\cdot)$, the standard normal cumulative distribution function $\phi(\cdot)$, the standard normal density function I found a solution for $n=1$ and $n=2$ (see http://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions ) However, I would need a general solution (for any $n$) if it exists. If not, is there a good approximation? Thanks.",,"['calculus', 'integration', 'gaussian-integral']"
78,Closed-forms of real parts of special value dilogarithm identities from inverse tangent integral function,Closed-forms of real parts of special value dilogarithm identities from inverse tangent integral function,,"The inverse tangent integral is defined as $$\operatorname{Ti}_2(x)=\Im\operatorname{Li}_2\left(ix\right)$$ Because this we have some special value identitiy. Let $c_1 = \operatorname{Li}_2(i)$, then $\Im c_1 = G$, where $G$ is Catalan's constant . Let $c_2 = 4\operatorname{Li}_2\left(\frac{i}{2}\right) + 2\operatorname{Li}_2\left(\frac{i}{3}\right)+\operatorname{Li}_2\left(\frac{3i}{4} \right)$, then $\Im c_2=6G - \pi \ln 2$. Let $c_3 = \operatorname{Li}_2\left(2i-\sqrt 3\,i\right)$, then $\Im c_3 = \frac{2G}{3}-\frac{\pi}{12}\ln(2+\sqrt 3)$. Let $c_4 = \operatorname{Li}_2\left(2i+\sqrt 3\,i\right)$, then $\Im c_4 = \frac{2G}{3}+\frac{5\pi}{12}\ln(2+\sqrt 3)$. Let $c_5 = \operatorname{Li}_2\left(\frac{\sqrt{3}-\sqrt{2}}{\sqrt 2 +1 }i\right)-\operatorname{Li}_2\left(\frac{\sqrt{3}-\sqrt{2}}{\sqrt 2 -1 }i\right)+\frac{2}{3}\operatorname{Li}_2\left((\sqrt 2 - 1)i\right)$, then $\Im c_5 = \frac{\pi}{6}\ln\left(\frac{\sqrt{2} - 1}{(\sqrt3 - \sqrt 2)(\sqrt 2 + 1)}\right)$. Furthermore we know that $\Re c_1 = -\frac{\pi^2}{48}$. Question. Is there a closed-form of $\Re c_i$, $i=2\dots5$ ? If we couldn't specify closed-forms of them, then of any of their combinations?","The inverse tangent integral is defined as $$\operatorname{Ti}_2(x)=\Im\operatorname{Li}_2\left(ix\right)$$ Because this we have some special value identitiy. Let $c_1 = \operatorname{Li}_2(i)$, then $\Im c_1 = G$, where $G$ is Catalan's constant . Let $c_2 = 4\operatorname{Li}_2\left(\frac{i}{2}\right) + 2\operatorname{Li}_2\left(\frac{i}{3}\right)+\operatorname{Li}_2\left(\frac{3i}{4} \right)$, then $\Im c_2=6G - \pi \ln 2$. Let $c_3 = \operatorname{Li}_2\left(2i-\sqrt 3\,i\right)$, then $\Im c_3 = \frac{2G}{3}-\frac{\pi}{12}\ln(2+\sqrt 3)$. Let $c_4 = \operatorname{Li}_2\left(2i+\sqrt 3\,i\right)$, then $\Im c_4 = \frac{2G}{3}+\frac{5\pi}{12}\ln(2+\sqrt 3)$. Let $c_5 = \operatorname{Li}_2\left(\frac{\sqrt{3}-\sqrt{2}}{\sqrt 2 +1 }i\right)-\operatorname{Li}_2\left(\frac{\sqrt{3}-\sqrt{2}}{\sqrt 2 -1 }i\right)+\frac{2}{3}\operatorname{Li}_2\left((\sqrt 2 - 1)i\right)$, then $\Im c_5 = \frac{\pi}{6}\ln\left(\frac{\sqrt{2} - 1}{(\sqrt3 - \sqrt 2)(\sqrt 2 + 1)}\right)$. Furthermore we know that $\Re c_1 = -\frac{\pi^2}{48}$. Question. Is there a closed-form of $\Re c_i$, $i=2\dots5$ ? If we couldn't specify closed-forms of them, then of any of their combinations?",,"['calculus', 'special-functions', 'closed-form', 'polylogarithm']"
79,How to find $\int \frac{x\ln(x+\sqrt{1+x^2})}{\sqrt{1+x^2}}\mathrm dx$,How to find,\int \frac{x\ln(x+\sqrt{1+x^2})}{\sqrt{1+x^2}}\mathrm dx,"$$I=\int x.\frac{\ln(x+\sqrt{1+x^2})}{\sqrt{1+x^2}}\mathrm dx$$ Try 1: Put $z= \ln(x+\sqrt{1+x^2})$, $\mathrm dz=1/\sqrt{1+x^2}\mathrm dx$ $$I=\int \underbrace{x}_{\mathbb u}\underbrace{z}_{\mathbb v}\mathrm dz=x\int zdz-\int (z^2/2)\mathrm dz\tag{Wrong}$$ Try 2: Put  $z= x+\sqrt{1+x^2}$ $$\implies x-z =\sqrt{1+x^2}\implies x^2+z^2-2xz =1+x^2\implies x =\frac{z^2-1}{2z}$$  $$\mathrm dz =\left(1+\frac{x}{\sqrt{1+x^2}}\right)\mathrm dx =\frac{z\mathrm dx}{x-z}=\frac{-2z^2\mathrm dx}{1+z^2}$$ $$I =\int\frac{(z^2-1)\ln z}{2z}.\frac{(1+z^2)\mathrm dz}{-2z^2}$$ $$=\int\frac{(z^4-1)\mathrm dz}{4z^3} =\frac14\int\left(z-\frac1{z^3}\right)\mathrm dz =z^2/2+2/z^2+C\tag{Wrong}$$ Try 3: Put  $z =\sqrt{1+x^2},\mathrm dx =x/\sqrt{1+x^2}\mathrm dx$ $$I =\int \ln(x+z)\mathrm dz =\int \ln(z+\sqrt{z^2-1})\mathrm dz$$ Don't know how to solve this integral. [Note that if I take $u=z+\sqrt{z^2-1}$, it is $=\sqrt{1+x^2}+\sqrt{1+x^2-1}=x+\sqrt{1+x^2}$; same as first try.] What's wrong in try 1 & 2, how to further solve try 3 and the best method to solve this question? Update: Sorry, I don't know hyperbolic/inverse hyperbolic trigonometry.","$$I=\int x.\frac{\ln(x+\sqrt{1+x^2})}{\sqrt{1+x^2}}\mathrm dx$$ Try 1: Put $z= \ln(x+\sqrt{1+x^2})$, $\mathrm dz=1/\sqrt{1+x^2}\mathrm dx$ $$I=\int \underbrace{x}_{\mathbb u}\underbrace{z}_{\mathbb v}\mathrm dz=x\int zdz-\int (z^2/2)\mathrm dz\tag{Wrong}$$ Try 2: Put  $z= x+\sqrt{1+x^2}$ $$\implies x-z =\sqrt{1+x^2}\implies x^2+z^2-2xz =1+x^2\implies x =\frac{z^2-1}{2z}$$  $$\mathrm dz =\left(1+\frac{x}{\sqrt{1+x^2}}\right)\mathrm dx =\frac{z\mathrm dx}{x-z}=\frac{-2z^2\mathrm dx}{1+z^2}$$ $$I =\int\frac{(z^2-1)\ln z}{2z}.\frac{(1+z^2)\mathrm dz}{-2z^2}$$ $$=\int\frac{(z^4-1)\mathrm dz}{4z^3} =\frac14\int\left(z-\frac1{z^3}\right)\mathrm dz =z^2/2+2/z^2+C\tag{Wrong}$$ Try 3: Put  $z =\sqrt{1+x^2},\mathrm dx =x/\sqrt{1+x^2}\mathrm dx$ $$I =\int \ln(x+z)\mathrm dz =\int \ln(z+\sqrt{z^2-1})\mathrm dz$$ Don't know how to solve this integral. [Note that if I take $u=z+\sqrt{z^2-1}$, it is $=\sqrt{1+x^2}+\sqrt{1+x^2-1}=x+\sqrt{1+x^2}$; same as first try.] What's wrong in try 1 & 2, how to further solve try 3 and the best method to solve this question? Update: Sorry, I don't know hyperbolic/inverse hyperbolic trigonometry.",,"['calculus', 'integration', 'indefinite-integrals']"
80,"$f'$ exists, but $\lim \frac{f(x)-f(y)}{x-y}$ does not exist","exists, but  does not exist",f' \lim \frac{f(x)-f(y)}{x-y},"Suppose $f$ is differentiable at $a$, i.e. $\lim_{x\to a}\frac{f(x)-f(a)} {x-a}$ exists. I wondered whether it was necessarily true that $$\lim_{\substack{x,y\to a\\x\neq y}}\frac{f(x)-f(y)}{x-y} \tag{1}$$ exists and equals the same thing. I believe that my friends and I have found a counterexample, which I'll place below the fold. I believe we have found some conditions under which (1) must exist and equal $f'(a)$, and I wonder if anyone has some others: If $x$ and $y$ approach from opposite sides of $a$, then I claim that the secant line from $x$ to $y$ has slope between the slope of the secant line from $x$ to $a$ and the slope of the secant line from $y$ to $a$. (Draw a picture.) Therefore a counter-example can only come where $x$ and $y$ do not approach $a$ from different sides. ( Wrong ) Note that if $a,b\geq 0$ and $c,d>0$, then $\frac{a+c}{b+d}$ is between $\frac{a}{c}$ and $\frac{b}{d}$. So, assuming that $x$ and $y$ are approaching from the same side, if $f(x)-a$ and $f(y)-a$ have the same sign in a neighborhood of $a$, then (1) must exist and equal $f'(a)$. So a counter-example can only come when for all $\delta>0$, $f(x)-f(a)$ is both positive and negative either on $(a,a+\delta)$ or on $(a-\delta, a)$. From the definition of the derivative, this also shows that a counterexample can only come when $f'(a)=0$. Edit: TonyK has pointed out that I am in error here. One of my friends conjectured that if $f$ were rectifiable, then a counterexample cannot exist. Does anyone have any thoughts on this? Counterexample. Suppose $$f:x\mapsto \begin{cases}x^2\sin \left( 1/x \right) & x \neq 0 \\ 0 & x=0.\end{cases}$$ $f$ is differentiable at zero with $f'(0)=0.$ Take $x_n$ and $y_n$ to be adjacent peaks and valleys: $$x_n := \frac1{\pi/2 + 2\pi n}\\ y_n := \frac1{3\pi/2 + 2\pi n}.$$ $x_n$ and $y_n$ go to zero as $n\to \infty$. Then I claim  $$\frac{f(x_n)-f(y_n)}{x_n-y_n} \xrightarrow{n\to\infty}\frac2\pi \neq f'(0)$$","Suppose $f$ is differentiable at $a$, i.e. $\lim_{x\to a}\frac{f(x)-f(a)} {x-a}$ exists. I wondered whether it was necessarily true that $$\lim_{\substack{x,y\to a\\x\neq y}}\frac{f(x)-f(y)}{x-y} \tag{1}$$ exists and equals the same thing. I believe that my friends and I have found a counterexample, which I'll place below the fold. I believe we have found some conditions under which (1) must exist and equal $f'(a)$, and I wonder if anyone has some others: If $x$ and $y$ approach from opposite sides of $a$, then I claim that the secant line from $x$ to $y$ has slope between the slope of the secant line from $x$ to $a$ and the slope of the secant line from $y$ to $a$. (Draw a picture.) Therefore a counter-example can only come where $x$ and $y$ do not approach $a$ from different sides. ( Wrong ) Note that if $a,b\geq 0$ and $c,d>0$, then $\frac{a+c}{b+d}$ is between $\frac{a}{c}$ and $\frac{b}{d}$. So, assuming that $x$ and $y$ are approaching from the same side, if $f(x)-a$ and $f(y)-a$ have the same sign in a neighborhood of $a$, then (1) must exist and equal $f'(a)$. So a counter-example can only come when for all $\delta>0$, $f(x)-f(a)$ is both positive and negative either on $(a,a+\delta)$ or on $(a-\delta, a)$. From the definition of the derivative, this also shows that a counterexample can only come when $f'(a)=0$. Edit: TonyK has pointed out that I am in error here. One of my friends conjectured that if $f$ were rectifiable, then a counterexample cannot exist. Does anyone have any thoughts on this? Counterexample. Suppose $$f:x\mapsto \begin{cases}x^2\sin \left( 1/x \right) & x \neq 0 \\ 0 & x=0.\end{cases}$$ $f$ is differentiable at zero with $f'(0)=0.$ Take $x_n$ and $y_n$ to be adjacent peaks and valleys: $$x_n := \frac1{\pi/2 + 2\pi n}\\ y_n := \frac1{3\pi/2 + 2\pi n}.$$ $x_n$ and $y_n$ go to zero as $n\to \infty$. Then I claim  $$\frac{f(x_n)-f(y_n)}{x_n-y_n} \xrightarrow{n\to\infty}\frac2\pi \neq f'(0)$$",,"['calculus', 'limits', 'derivatives']"
81,$\int dx/x =\cdots$ (pedantic nitpicking?),(pedantic nitpicking?),\int dx/x =\cdots,"It seems that ""everybody knows"" that $\displaystyle\int\frac{dx}{x}=\log|x|+C$ (or if one really must, then $\ln$ instead of they synonymous $\log$).  Does any textbook or reference work say that $$ \int\frac{dx}{x} = \log |x| + \left.\begin{cases} A & \text{if }x>0, \\ B & \text{if }x<0. \end{cases}\right\} \text{where $A$ and $B$ are constant ?} $$ And now a subtler question: When and how often would this matter?  Maybe never? Or maybe only on examinations (Maybe there's some subtle difference between that and ""never"".)?","It seems that ""everybody knows"" that $\displaystyle\int\frac{dx}{x}=\log|x|+C$ (or if one really must, then $\ln$ instead of they synonymous $\log$).  Does any textbook or reference work say that $$ \int\frac{dx}{x} = \log |x| + \left.\begin{cases} A & \text{if }x>0, \\ B & \text{if }x<0. \end{cases}\right\} \text{where $A$ and $B$ are constant ?} $$ And now a subtler question: When and how often would this matter?  Maybe never? Or maybe only on examinations (Maybe there's some subtle difference between that and ""never"".)?",,['calculus']
82,Mean Value Property of Harmonic Function on a Square,Mean Value Property of Harmonic Function on a Square,,A friend of mine presented me the following problem a couple days ago: Let $S$ in $\mathbb{R}^2$ be a square and $u$ a continuous harmonic function on the closure of $S$.  Show that the average of $u$ over the perimeter of $S$ is equal to the average of $u$ over the union of two diagonals. I recall that the 'standard' mean value property of harmonic functions is proven over a sphere using greens identities.  I've given this one some thought but I haven't come up with any ideas of how to proceed.  It's driving me crazy! Maybe it has something to do with the triangles resulting from a diagonal?  Any ideas?,A friend of mine presented me the following problem a couple days ago: Let $S$ in $\mathbb{R}^2$ be a square and $u$ a continuous harmonic function on the closure of $S$.  Show that the average of $u$ over the perimeter of $S$ is equal to the average of $u$ over the union of two diagonals. I recall that the 'standard' mean value property of harmonic functions is proven over a sphere using greens identities.  I've given this one some thought but I haven't come up with any ideas of how to proceed.  It's driving me crazy! Maybe it has something to do with the triangles resulting from a diagonal?  Any ideas?,,"['calculus', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
83,A divergent series from Futurama,A divergent series from Futurama,,"I was watching Futurama and in a recent episode, the professor creates a duplication machine. The machine basically took something and then made 2 copies at 60% the size. Somehow Bender got caught in the machine and he started duplicating infinitely. The problem is, Bender needed excess matter to create these duplicates. The professor then reveals this equation which would explain why it was a threat to the universe as the ammount of matter needed to create the excess bending units would never converge to 0 thus, eventually, needing the entire mass of the universe to keep the series going: This was already asked on SciFi.SE, but I'm still confused as to what this equation is actually saying. And how it is divergent. Or if it is even accurate in it's concept.","I was watching Futurama and in a recent episode, the professor creates a duplication machine. The machine basically took something and then made 2 copies at 60% the size. Somehow Bender got caught in the machine and he started duplicating infinitely. The problem is, Bender needed excess matter to create these duplicates. The professor then reveals this equation which would explain why it was a threat to the universe as the ammount of matter needed to create the excess bending units would never converge to 0 thus, eventually, needing the entire mass of the universe to keep the series going: This was already asked on SciFi.SE, but I'm still confused as to what this equation is actually saying. And how it is divergent. Or if it is even accurate in it's concept.",,"['calculus', 'sequences-and-series', 'divergent-series', 'popular-math']"
84,Interesting integral $\int_{0}^{1}\frac{\tanh^{-1}x\ln x}{x(1-x^{2})}{ d}x$,Interesting integral,\int_{0}^{1}\frac{\tanh^{-1}x\ln x}{x(1-x^{2})}{ d}x,"I ran across an integral that is rather tough and I am wondering if anyone could give me a shove in the right direction. $$\int_{0}^{1}\frac{\tanh^{-1}x\ln x}{x(1-x^{2})}{ d}x=-\frac{7}{16}\zeta(3)-\frac{{\pi}^{2}}{8}\ln(2)$$ This solution is almost exactly like the solution to $\int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\text{ d}x=\frac{7}{16}\zeta(3)-\frac{{\pi}^{2}}{8}\ln(2)$ I solved the latter integral by using the identity $\displaystyle -\ln(\sin(x))-\ln(2)=\sum_{k=1}^{\infty}\frac{\cos(2kx)}{k}$ , then integrating: $\displaystyle -\int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\text{ d}x-\frac{{\pi}^{2}}{8}\ln(2)=\int_{0}^{\frac{\pi}{2}}x\cos(2x)\text{ d}x+\frac{\int_{0}^{\frac{\pi}{2}}\cos(4x)\text{ d}x}{2}+\frac{\int_{0}^{\frac{\pi}{2}}\cos(6x)}{3}\cdot\cdot\cdot\cdot$ But, $\displaystyle \int_{0}^{\frac{\pi}{2}}x\cdot \cos(2kx)\text{ d}x=-\left(\frac{1+(-1)^{k+1}}{(2k)^{2}}\right)$ Thus: $\displaystyle \int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\text{ d}x=\frac{1}{4}\sum_{k=1}^{\infty}\frac{1+(-1)^{k+1}}{k^{3}}-\frac{{\pi}^{2}}{8}\ln(2)$ $\displaystyle =\frac{1}{2}\displaystyle\sum_{k=0}^{\infty}\frac{1}{(2k+1)^{3}}-\frac{{\pi}^{2}}{8}\ln(2)$ and so on.  Which results in the solution I mentioned in the beginning. Sorry for all that, but I wanted to show you what I was using in order to some how relate it to the integral I am wanting to solve. I have been trying and trying to relate the aforementioned $\displaystyle \tanh$ integral with this one.  The solutions are so nearly the same, I figured there has to be a way to relate them and solve the integral.  Does anyone have some ideas?. I have tried the identity $\displaystyle \tanh^{-1}(x)=\frac{1}{2}\left[\ln(1+x)-\ln(1-x)\right]$ , then breaking it up: Resulting in $\displaystyle \frac{1}{2}\int_{0}^{1}\frac{\ln(x)\ln(1+x)}{x(x^{2}-1)}\text{ d}x-\frac{1}{2}\int_{0}^{1}\frac{\ln(x)\ln(1-x)}{x(x^{2}-1)} \text{ d}x$ ,  then I used the various series representations for $\displaystyle \ln(1+x)$ , $\displaystyle \frac{1}{1-x^{2}}$ , etc. I tried double integrals, but I always get stuck. I even broke it up per partial fraction expansion, but several of the resulting integrals were still nasty. Does anyone have some clever ideas?.","I ran across an integral that is rather tough and I am wondering if anyone could give me a shove in the right direction. This solution is almost exactly like the solution to I solved the latter integral by using the identity , then integrating: But, Thus: and so on.  Which results in the solution I mentioned in the beginning. Sorry for all that, but I wanted to show you what I was using in order to some how relate it to the integral I am wanting to solve. I have been trying and trying to relate the aforementioned integral with this one.  The solutions are so nearly the same, I figured there has to be a way to relate them and solve the integral.  Does anyone have some ideas?. I have tried the identity , then breaking it up: Resulting in ,  then I used the various series representations for , , etc. I tried double integrals, but I always get stuck. I even broke it up per partial fraction expansion, but several of the resulting integrals were still nasty. Does anyone have some clever ideas?.",\int_{0}^{1}\frac{\tanh^{-1}x\ln x}{x(1-x^{2})}{ d}x=-\frac{7}{16}\zeta(3)-\frac{{\pi}^{2}}{8}\ln(2) \int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\text{ d}x=\frac{7}{16}\zeta(3)-\frac{{\pi}^{2}}{8}\ln(2) \displaystyle -\ln(\sin(x))-\ln(2)=\sum_{k=1}^{\infty}\frac{\cos(2kx)}{k} \displaystyle -\int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\text{ d}x-\frac{{\pi}^{2}}{8}\ln(2)=\int_{0}^{\frac{\pi}{2}}x\cos(2x)\text{ d}x+\frac{\int_{0}^{\frac{\pi}{2}}\cos(4x)\text{ d}x}{2}+\frac{\int_{0}^{\frac{\pi}{2}}\cos(6x)}{3}\cdot\cdot\cdot\cdot \displaystyle \int_{0}^{\frac{\pi}{2}}x\cdot \cos(2kx)\text{ d}x=-\left(\frac{1+(-1)^{k+1}}{(2k)^{2}}\right) \displaystyle \int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\text{ d}x=\frac{1}{4}\sum_{k=1}^{\infty}\frac{1+(-1)^{k+1}}{k^{3}}-\frac{{\pi}^{2}}{8}\ln(2) \displaystyle =\frac{1}{2}\displaystyle\sum_{k=0}^{\infty}\frac{1}{(2k+1)^{3}}-\frac{{\pi}^{2}}{8}\ln(2) \displaystyle \tanh \displaystyle \tanh^{-1}(x)=\frac{1}{2}\left[\ln(1+x)-\ln(1-x)\right] \displaystyle \frac{1}{2}\int_{0}^{1}\frac{\ln(x)\ln(1+x)}{x(x^{2}-1)}\text{ d}x-\frac{1}{2}\int_{0}^{1}\frac{\ln(x)\ln(1-x)}{x(x^{2}-1)} \text{ d}x \displaystyle \ln(1+x) \displaystyle \frac{1}{1-x^{2}},"['calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
85,"Prove that the sequence$ c_1 = 1$, $c_{n+1} = 4/(1 + 5c_n) $ , $ n \geq 1$ is convergent and find its limit","Prove that the sequence,  ,  is convergent and find its limit", c_1 = 1 c_{n+1} = 4/(1 + 5c_n)   n \geq 1,"Prove that the sequence $c_{1} = 1$, $c_{(n+1)}= 4/(1 + 5c_{n})$     , $n \geq 1$  is convergent and find its limit. Ok so up to now I've worked out a couple of things. $c_1 = 1$ $c_2 = 2/3$ $c_3 = 12/13$ $c_4 = 52/73$ So the odd $c_n$ are decreasing and the even $c_n$ are increasing.  Intuitively, it's clear the the two sequences for odd and even $c_n$ are decreasing/increasing less and less. Therefore it seems like the sequence may converge to some limit $L$. If the sequence has a limit, let $L=\underset{n\rightarrow \infty }{\lim }a_{n}.$   Then $L = 1/(1+5L).$ So we yield $L = 4/5$ and $L = -1$.  But since the even sequence is increasing and >0, then $L$ must be $4/5$. Ok, here I am stuck.  I'm not sure how to go ahead and show that the sequence converges to this limit (I tried using the definition of the limit but I didn't manage) and and not sure about the separate sequences how I would go about showing their limits. A few notes : I am in 2nd year calculus. This is a bonus question, but I enjoy the challenge and would love the extra marks. Note : Once again I apologize I don't know how to use the HTML code to make it nice.","Prove that the sequence $c_{1} = 1$, $c_{(n+1)}= 4/(1 + 5c_{n})$     , $n \geq 1$  is convergent and find its limit. Ok so up to now I've worked out a couple of things. $c_1 = 1$ $c_2 = 2/3$ $c_3 = 12/13$ $c_4 = 52/73$ So the odd $c_n$ are decreasing and the even $c_n$ are increasing.  Intuitively, it's clear the the two sequences for odd and even $c_n$ are decreasing/increasing less and less. Therefore it seems like the sequence may converge to some limit $L$. If the sequence has a limit, let $L=\underset{n\rightarrow \infty }{\lim }a_{n}.$   Then $L = 1/(1+5L).$ So we yield $L = 4/5$ and $L = -1$.  But since the even sequence is increasing and >0, then $L$ must be $4/5$. Ok, here I am stuck.  I'm not sure how to go ahead and show that the sequence converges to this limit (I tried using the definition of the limit but I didn't manage) and and not sure about the separate sequences how I would go about showing their limits. A few notes : I am in 2nd year calculus. This is a bonus question, but I enjoy the challenge and would love the extra marks. Note : Once again I apologize I don't know how to use the HTML code to make it nice.",,['calculus']
86,Solving $\int_0^1\frac{\log^4(1-x^2)}{1+x}dx$,Solving,\int_0^1\frac{\log^4(1-x^2)}{1+x}dx,"This question is similar to my last question . It's also from the same book, Problem 1.8(iii). $$I=\int_0^1\dfrac{\log^4(1-x^2)}{1+x}dx$$ Wolfram Alpha can't give the closed form, it is only giving a numerical expression. According to the book $$I=\dfrac{16}{5}\log^52-16\log^32\zeta(2)+48\log^22\zeta(3)-54\log2\zeta(4)-24\zeta(2)\zeta(3)+72\zeta(5)$$ I tried taking substitutions like $y=1-x^2$ . It didn't take me anywhere. Is there some way to solve this integral?","This question is similar to my last question . It's also from the same book, Problem 1.8(iii). Wolfram Alpha can't give the closed form, it is only giving a numerical expression. According to the book I tried taking substitutions like . It didn't take me anywhere. Is there some way to solve this integral?",I=\int_0^1\dfrac{\log^4(1-x^2)}{1+x}dx I=\dfrac{16}{5}\log^52-16\log^32\zeta(2)+48\log^22\zeta(3)-54\log2\zeta(4)-24\zeta(2)\zeta(3)+72\zeta(5) y=1-x^2,"['calculus', 'integration', 'definite-integrals']"
87,Evaluate $\lim_{n \to \infty} \frac{a_n}{2 ^ {n - 1}}$ if $a_n = a_{n - 1} + \sqrt{a_{n - 1}^2 + 1}$,Evaluate  if,\lim_{n \to \infty} \frac{a_n}{2 ^ {n - 1}} a_n = a_{n - 1} + \sqrt{a_{n - 1}^2 + 1},"Let $a_i (i \in \mathbb{N}_{0})$ be a sequence of real numbers such that $a_0 = 0$ and $$a_n = a_{n - 1} + \sqrt{a_{n - 1}^2 + 1} \text{ } \forall n \geq 1$$ Evaluate the limit $$\lim_{n \to \infty} \frac{a_n}{2 ^ {n - 1}}$$ Hello, I am trying to solve this problem. I honestly have no idea how to approach this, but I think the answer will be $1$ because as $n \to \infty$ , $a_n$ gets bigger and $a_n \approx 2a_{n-1}$ and since $a_1 = 1 = 2^{0}$ , the limit will approach $1$ . (Of course, this is just a guess). The intended solution is too much magic. Substitute $a_n = \cot \theta$ . Now $$a_{n + 1} = a_n + \sqrt{a_{n}^2 + 1} = \cot \theta + \csc \theta$$ $$=\frac{\cos \theta + 1}{\sin \theta}$$ $$=\frac{2 \cos^2{\frac{\theta}{2}}}{2 \sin \frac{\theta}{2} \cos \frac{\theta}{2}}$$ $$=\cot \frac{\theta}{2}$$ And now, we can solve the limit and answer is $\frac{4}{\pi}$ . But, is there a more normal method to solving this (other than just thinking out of nowhere that substituting $a_n = \cot \theta$ is helpful)? Please note that my question is about finding an alternative solution that's much more ""thinkable"". So, this is not a duplicate. Thanks","Let be a sequence of real numbers such that and Evaluate the limit Hello, I am trying to solve this problem. I honestly have no idea how to approach this, but I think the answer will be because as , gets bigger and and since , the limit will approach . (Of course, this is just a guess). The intended solution is too much magic. Substitute . Now And now, we can solve the limit and answer is . But, is there a more normal method to solving this (other than just thinking out of nowhere that substituting is helpful)? Please note that my question is about finding an alternative solution that's much more ""thinkable"". So, this is not a duplicate. Thanks",a_i (i \in \mathbb{N}_{0}) a_0 = 0 a_n = a_{n - 1} + \sqrt{a_{n - 1}^2 + 1} \text{ } \forall n \geq 1 \lim_{n \to \infty} \frac{a_n}{2 ^ {n - 1}} 1 n \to \infty a_n a_n \approx 2a_{n-1} a_1 = 1 = 2^{0} 1 a_n = \cot \theta a_{n + 1} = a_n + \sqrt{a_{n}^2 + 1} = \cot \theta + \csc \theta =\frac{\cos \theta + 1}{\sin \theta} =\frac{2 \cos^2{\frac{\theta}{2}}}{2 \sin \frac{\theta}{2} \cos \frac{\theta}{2}} =\cot \frac{\theta}{2} \frac{4}{\pi} a_n = \cot \theta,"['calculus', 'sequences-and-series', 'limits']"
88,"If $f(f(x)) = x+1, f(x+1) = f(x) + 1$, is it true that $f(x) = x + 1/2$?","If , is it true that ?","f(f(x)) = x+1, f(x+1) = f(x) + 1 f(x) = x + 1/2","If $f(f(x)) = x+1, f(x+1) = f(x) + 1$ , where $f: \Bbb R \rightarrow \Bbb R$ is real-analytic, bijective, monotonically increasing, is it true that $f(x) = x + 1/2$ ? I have tried to represent $f(x)$ as power series in a neighborhood of arbitrary $x_{0}$ and $y_{0} = f(x_{0})$ . It's obvious that we can choose such a neighborhood $U_{0}(x_{0})$ that it's image by $f$ is $U_{1}(y_{0})$ . Then, we can try to find the power series of $f(f(x))$ , which is equal to $x+1$ . But I cannot see, how such an equation shows that $f(x) = x + 1/2$ .","If , where is real-analytic, bijective, monotonically increasing, is it true that ? I have tried to represent as power series in a neighborhood of arbitrary and . It's obvious that we can choose such a neighborhood that it's image by is . Then, we can try to find the power series of , which is equal to . But I cannot see, how such an equation shows that .","f(f(x)) = x+1, f(x+1) = f(x) + 1 f: \Bbb R \rightarrow \Bbb R f(x) = x + 1/2 f(x) x_{0} y_{0} = f(x_{0}) U_{0}(x_{0}) f U_{1}(y_{0}) f(f(x)) x+1 f(x) = x + 1/2","['calculus', 'functional-equations', 'iterated-function-system']"
89,Showing the Fibonacci inequality $f_1^{f_1}f_2^{f_2}f_3^{f_3}\cdots f_n^{f_n}\leq f_1!f_2!f_3!\cdots f_n!\;e^{({f_{n+2}-n-1)}}$ without induction.,Showing the Fibonacci inequality  without induction.,f_1^{f_1}f_2^{f_2}f_3^{f_3}\cdots f_n^{f_n}\leq f_1!f_2!f_3!\cdots f_n!\;e^{({f_{n+2}-n-1)}},"Today I saw the following 2 beautiful inequalities on a facebook page. $$1^12^23^34^4\cdots n^n\leq 1!2!3!4!\cdots n!e^{\frac{n(n-1)}{2}}$$ $$f_{1}^{f_{1}}f_{2}^{f_{2}}f_{3}^{f_{3}}\cdots f_{n}^{f_{n}}\leq f_{1}!f_{2}!f_{3}!\cdots f_{n}!e^{({f_{n+2}-n-1)}}$$ Here $f_{n}$ denotes the fibonacci numbers $f_{1}=f_{2}=1$ and $f_{n+2}=f_{n+1}+f_{n}$ for $n\in N$ Here is How I proved the first one. Method 1:- $$1^12^23^34^4...n^n\leq 1!2!3!4!...n!e^{\frac{n(n-1)}{2}}$$ $$\implies\bigg(\frac{1^1}{1!}\bigg)\bigg(\frac{2^2}{2!}\bigg)\bigg(\frac{3^3}{3!}\bigg)...\bigg(\frac{n^n}{n!}\bigg)\leq e^{\frac{n(n-1)}{2}}$$ $\implies \displaystyle\prod_{x=1}^{n} \frac{x^x}{x!}\leq  e^{\frac{n(n-1)}{2}}$ Now taking logarithm on both sides of inequality. $\implies \displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\leq  {\frac{n(n-1)}{2}}$ $\implies \displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\leq  {\frac{n(n+1)}{2}}-n$ $\implies \displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\leq  \bigg(\sum_{x=1}^{n} x \bigg)-n$ $\implies \displaystyle \ \sum_{x=1}^{n} x -\displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\geq n$ $\implies \displaystyle \ \sum_{x=1}^{n} x + \ln\bigg(\frac{x!}{x^x}\bigg)\geq n$ $$\implies 1+1.30+1.49+1.63+1.74+1.82+1.90+1.97+2.02+...\geq n$$ $\implies 1+(1+0.30)+(1+0.49)+(1+0.63)+(1+0.74)+...\geq  {\smash[b]{1+\! \underbrace{1+1+\cdots1\,}_\text{$n$ times}}}$ This proves our first inequality. Method 2:- Using $\frac{n^n}{n!}\leq e^{n-1}$ for $n\in N$ . $\frac{1^1}{1!}\leq e^{0}$ $\frac{2^2}{2!}\leq e^{1}$ $\frac{3^3}{3!}\leq e^{2}$ $\vdots\\$ $\frac{n^n}{n!}\leq e^{n-1}$ Now multiply all above inequalities we get, $\implies \displaystyle\prod_{x=1}^{n} \frac{x^x}{x!}\leq  e^{\frac{n(n-1)}{2}}$ Using the same approach for $2^{nd}$ inequality we get, $$\sum_{x=1}^{n} \ln\bigg(\frac{{f_{x}}^{f_{x}}}{f_{x}!}\bigg)\leq f_{n+2}-n-1$$ Now I got stuck on this step. How can we prove the $2^{nd}$ inequality. Moreover can we show without numerically calculating(as I do above) that $\displaystyle \ \sum_{x=1}^{n} x + \ln\bigg(\frac{x!}{x^x}\bigg)\geq n$","Today I saw the following 2 beautiful inequalities on a facebook page. Here denotes the fibonacci numbers and for Here is How I proved the first one. Method 1:- Now taking logarithm on both sides of inequality. This proves our first inequality. Method 2:- Using for . Now multiply all above inequalities we get, Using the same approach for inequality we get, Now I got stuck on this step. How can we prove the inequality. Moreover can we show without numerically calculating(as I do above) that","1^12^23^34^4\cdots n^n\leq 1!2!3!4!\cdots n!e^{\frac{n(n-1)}{2}} f_{1}^{f_{1}}f_{2}^{f_{2}}f_{3}^{f_{3}}\cdots f_{n}^{f_{n}}\leq f_{1}!f_{2}!f_{3}!\cdots f_{n}!e^{({f_{n+2}-n-1)}} f_{n} f_{1}=f_{2}=1 f_{n+2}=f_{n+1}+f_{n} n\in N 1^12^23^34^4...n^n\leq 1!2!3!4!...n!e^{\frac{n(n-1)}{2}} \implies\bigg(\frac{1^1}{1!}\bigg)\bigg(\frac{2^2}{2!}\bigg)\bigg(\frac{3^3}{3!}\bigg)...\bigg(\frac{n^n}{n!}\bigg)\leq e^{\frac{n(n-1)}{2}} \implies \displaystyle\prod_{x=1}^{n} \frac{x^x}{x!}\leq 
e^{\frac{n(n-1)}{2}} \implies \displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\leq 
{\frac{n(n-1)}{2}} \implies \displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\leq 
{\frac{n(n+1)}{2}}-n \implies \displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\leq 
\bigg(\sum_{x=1}^{n} x \bigg)-n \implies \displaystyle \ \sum_{x=1}^{n} x -\displaystyle\sum_{x=1}^{n} \ln\bigg(\frac{x^x}{x!}\bigg)\geq n \implies \displaystyle \ \sum_{x=1}^{n} x + \ln\bigg(\frac{x!}{x^x}\bigg)\geq n \implies 1+1.30+1.49+1.63+1.74+1.82+1.90+1.97+2.02+...\geq n \implies 1+(1+0.30)+(1+0.49)+(1+0.63)+(1+0.74)+...\geq  {\smash[b]{1+\! \underbrace{1+1+\cdots1\,}_\text{n times}}} \frac{n^n}{n!}\leq e^{n-1} n\in N \frac{1^1}{1!}\leq e^{0} \frac{2^2}{2!}\leq e^{1} \frac{3^3}{3!}\leq e^{2} \vdots\\ \frac{n^n}{n!}\leq e^{n-1} \implies \displaystyle\prod_{x=1}^{n} \frac{x^x}{x!}\leq 
e^{\frac{n(n-1)}{2}} 2^{nd} \sum_{x=1}^{n} \ln\bigg(\frac{{f_{x}}^{f_{x}}}{f_{x}!}\bigg)\leq f_{n+2}-n-1 2^{nd} \displaystyle \ \sum_{x=1}^{n} x + \ln\bigg(\frac{x!}{x^x}\bigg)\geq n","['calculus', 'sequences-and-series', 'inequality', 'fibonacci-numbers']"
90,Infinite sum of reciprocals of squares of lengths of tangents from origin to the curve $y=\sin x$,Infinite sum of reciprocals of squares of lengths of tangents from origin to the curve,y=\sin x,"Let tangents be drawn to the curve $y=\sin x$ from the origin. Let the points of contact of these tangents with the curve be $(x_k,y_k)$ where $x_k\gt 0; k\ge 1$ such that $x_k\in (\pi k,  (k+1)\pi)$ and $$a_k=\sqrt {x_k^2+y_k^2}$$ (Which is basically the distance between the corresponding point of contact and the origin i.e. the length of tangent from origin)  . I wanted to know the value of $$\sum_{k=1}^{\infty} \frac {1}{a_k ^2}$$ Now this question has just popped out in my brain and is not copied from any assignment or any book so I don't know whether it will finally reach a conclusion or not. I tried writing the equation of tangent to this curve from origin and then finding the points of contact but did not get a proper result which just that the $x$ coordinates of the points of contact will be the positive solutions of the equation $\tan x=x$ On searching internet for sometime about the solutions of $\tan x=x$ I got two important properties of this equation.  If $(\lambda _n)_{n\in N}$ denote the roots of this equation then $$1)\sum_n^{\infty} \lambda _n \to \infty$$ $$2)\sum_n^{\infty} \frac {1}{\lambda _n^2} =\frac {1}{10}$$ But were not of much help. I also tried writing the points in polar coordinates to see if that could be of some help but I still failed miserably. I could not think of any method so any other method would be openly welcomed. Any help would be very beneficial to solve this problem. Thanks in advance. Edit: On trying a bit more using some coordinate geometry I found that the locus of the points of contact is $$x^2-y^2=x^2y^2$$ Hence for sum we just need to find $$\sum_{k=1}^{\infty} \frac {\lambda _k ^2 +1}{\lambda _k ^2 (\lambda _k ^2 +2)}=\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2} -\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2 (\lambda _k ^2 +2)}=\frac {1}{10} -\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2 (\lambda _k ^2 +2)}=\frac {1}{10} -\sum_{k=1}^{\infty} \frac {1}{2\lambda _k ^2} +\sum_{k=1}^{\infty} \frac {1}{2(\lambda _k ^2 +2)} =\frac {1}{20}+\frac {1}{2}\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2 +2}   $$ Now for the second summation I did think about it to form a series but for the roots to be $\lambda _k^2 +2$ we just need to substitute $x\to \sqrt {x−2}$ in power series of $\frac {\sin x-x\cos x}{x^3}$ and then get the result but it was still a lot confusing for me. Using $x\to\sqrt {x-2}$ in the above power series and using Wolfy I have got a series. So we need ratio of coefficient of $x$ to the constant term so is the value of second summation equal to $$\frac {5\sqrt 2\sinh(\sqrt 2)−6\cosh(\sqrt 2)}{4(2\cosh(\sqrt 2)−\sqrt 2\sinh(\sqrt 2))}?$$ Is this value correct or did I do it wrong? I would also like to know if there is some other method to solve this problem",Let tangents be drawn to the curve from the origin. Let the points of contact of these tangents with the curve be where such that and (Which is basically the distance between the corresponding point of contact and the origin i.e. the length of tangent from origin)  . I wanted to know the value of Now this question has just popped out in my brain and is not copied from any assignment or any book so I don't know whether it will finally reach a conclusion or not. I tried writing the equation of tangent to this curve from origin and then finding the points of contact but did not get a proper result which just that the coordinates of the points of contact will be the positive solutions of the equation On searching internet for sometime about the solutions of I got two important properties of this equation.  If denote the roots of this equation then But were not of much help. I also tried writing the points in polar coordinates to see if that could be of some help but I still failed miserably. I could not think of any method so any other method would be openly welcomed. Any help would be very beneficial to solve this problem. Thanks in advance. Edit: On trying a bit more using some coordinate geometry I found that the locus of the points of contact is Hence for sum we just need to find Now for the second summation I did think about it to form a series but for the roots to be we just need to substitute in power series of and then get the result but it was still a lot confusing for me. Using in the above power series and using Wolfy I have got a series. So we need ratio of coefficient of to the constant term so is the value of second summation equal to Is this value correct or did I do it wrong? I would also like to know if there is some other method to solve this problem,"y=\sin x (x_k,y_k) x_k\gt 0; k\ge 1 x_k\in (\pi k,  (k+1)\pi) a_k=\sqrt {x_k^2+y_k^2} \sum_{k=1}^{\infty} \frac {1}{a_k ^2} x \tan x=x \tan x=x (\lambda _n)_{n\in N} 1)\sum_n^{\infty} \lambda _n \to \infty 2)\sum_n^{\infty} \frac {1}{\lambda _n^2} =\frac {1}{10} x^2-y^2=x^2y^2 \sum_{k=1}^{\infty} \frac {\lambda _k ^2 +1}{\lambda _k ^2 (\lambda _k ^2 +2)}=\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2} -\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2 (\lambda _k ^2 +2)}=\frac {1}{10} -\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2 (\lambda _k ^2 +2)}=\frac {1}{10} -\sum_{k=1}^{\infty} \frac {1}{2\lambda _k ^2} +\sum_{k=1}^{\infty} \frac {1}{2(\lambda _k ^2 +2)} =\frac {1}{20}+\frac {1}{2}\sum_{k=1}^{\infty} \frac {1}{\lambda _k ^2 +2} 
  \lambda _k^2 +2 x\to \sqrt {x−2} \frac {\sin x-x\cos x}{x^3} x\to\sqrt {x-2} x \frac {5\sqrt 2\sinh(\sqrt 2)−6\cosh(\sqrt 2)}{4(2\cosh(\sqrt 2)−\sqrt 2\sinh(\sqrt 2))}?","['calculus', 'sequences-and-series', 'trigonometry', 'coordinate-systems']"
91,Help to evaluate $\int_{0}^{\pi}\sec(x)\sqrt{\tan\left(\frac{x}{2}\right)}\ln^n\tan\left(\frac{x}{2}\right)dx$,Help to evaluate,\int_{0}^{\pi}\sec(x)\sqrt{\tan\left(\frac{x}{2}\right)}\ln^n\tan\left(\frac{x}{2}\right)dx,"I am trying to evaluate this integral $$I=\large\int_{0}^{\pi}\sec(x)\sqrt{\tan\left(\frac{x}{2}\right)}\ln^n\tan\left(\frac{x}{2}\right)\mathrm dx$$ Making a substitution $\large u=\frac{x}{2}$ $\large \mathrm dx=\mathrm 2du$ $$I=2\large \int_{0}^{\pi/2}\sec(2u)\sqrt{\tan(u)}\ln^n\tan(u)\mathrm du$$ Using trigonometrical identities, making a transformation $$I=-2\large \int_{0}^{\pi/2}\sec(2u)\cdot \frac{\sqrt{\tan(u)}\ln^n\tan(u)}{\tan^2(u)-1}\mathrm du$$ Doing another substitution $\large v=\tan(u)$ $\large \mathrm du=\frac{1}{\sec^2(u)}\mathrm dv$ $$\large I=-2\int_{0}^{\infty}\frac{\sqrt{v}\ln^n(v)}{v^2-1}\mathrm dv$$ Doing another substitution $\large y=\sqrt{v}$ $\large \mathrm dv=2\sqrt{v}\mathrm dy$ $$\large I=-64\int_{0}^{\infty}\frac{y^2\ln^n(y)}{y^4-1}\mathrm dy$$ $$\large I=-64\int_{0}^{\infty}\frac{y^2\ln^n(y)}{(y^2+1)(y-1)(y+1)}\mathrm dy$$ Using partial fraction decomposition $$\large I=-32\int_{0}^{\infty}\frac{\ln^n(y)}{y^2+1}\mathrm dy+16\int_{0}^{\infty}\frac{\ln^n(y)}{y+1}\mathrm dy-16\int_{0}^{\infty}\frac{\ln^n(y)}{y-1}\mathrm dy$$ I can't continue from this point... I would like some help, please.","I am trying to evaluate this integral $$I=\large\int_{0}^{\pi}\sec(x)\sqrt{\tan\left(\frac{x}{2}\right)}\ln^n\tan\left(\frac{x}{2}\right)\mathrm dx$$ Making a substitution $\large u=\frac{x}{2}$ $\large \mathrm dx=\mathrm 2du$ $$I=2\large \int_{0}^{\pi/2}\sec(2u)\sqrt{\tan(u)}\ln^n\tan(u)\mathrm du$$ Using trigonometrical identities, making a transformation $$I=-2\large \int_{0}^{\pi/2}\sec(2u)\cdot \frac{\sqrt{\tan(u)}\ln^n\tan(u)}{\tan^2(u)-1}\mathrm du$$ Doing another substitution $\large v=\tan(u)$ $\large \mathrm du=\frac{1}{\sec^2(u)}\mathrm dv$ $$\large I=-2\int_{0}^{\infty}\frac{\sqrt{v}\ln^n(v)}{v^2-1}\mathrm dv$$ Doing another substitution $\large y=\sqrt{v}$ $\large \mathrm dv=2\sqrt{v}\mathrm dy$ $$\large I=-64\int_{0}^{\infty}\frac{y^2\ln^n(y)}{y^4-1}\mathrm dy$$ $$\large I=-64\int_{0}^{\infty}\frac{y^2\ln^n(y)}{(y^2+1)(y-1)(y+1)}\mathrm dy$$ Using partial fraction decomposition $$\large I=-32\int_{0}^{\infty}\frac{\ln^n(y)}{y^2+1}\mathrm dy+16\int_{0}^{\infty}\frac{\ln^n(y)}{y+1}\mathrm dy-16\int_{0}^{\infty}\frac{\ln^n(y)}{y-1}\mathrm dy$$ I can't continue from this point... I would like some help, please.",,"['calculus', 'integration']"
92,Pick the right notation for calculus!,Pick the right notation for calculus!,,"Within calculus we have many different symbols attached with differentiating (or is that deriving?) a function, probably to the dismay of the modern calculus student. So here's the setup: As a modern researcher, you and your collaborators N, L, C, and E have just discovered a new field called ""kalkulus of one variable."" Unfortunately, discord has broken out among your co-horts and they simply cannot agree on whose notation is best for your collaborative research papers and text books. As the only person who didn't come up with an original notation, they have agreed to let you arbitrate and single-handedly pick the one-and-forever notation. Whose do you pick and why? For the following, interpret everything as a function of the appropriate variables. Also, ignore that all four of your collaborators weren't actually ever really active at the same time. The contenders (cribbing from wikipedia ) have sent you their notation and a short argument: Nutonne (Newton) $\dot y, ~\ddot y, ~\dot{\ddot y} , \dots, \overset{n}{\dot y}$ for derivatives..err...fluxions  and $\square y$ or $\boxed{y}$ for anti-derivatives...err...fluents...err..absements (yes, that symbol rendered correctly). Fundamental Theorem: $\square \dot y = y$ AFF: Newtonne seems to be the first of your collaborators to have developed the full theory, although he failed to email the rest of you. Claims absolute naming rights. Notation generally typesets well, unlike his nemesis's. CON: There is a historical anecdote of your contemporaries floundering for a century. Libknittz (Leibniz) $\frac{dy}{dx}, ~\frac{d^2 y}{dx^2}, \dots, \frac{d^n y}{d x^n}$ for derivatives and $\int y ~\mathrm{d}x$ for anti-derivatives: $\frac{d}{dx} \int y ~\mathrm{d}x = y$ . AFF: Emphasizes the kalkulus as being about a rate of change and we always see the dependent and independent variables. $\mathrm{d}$ for difference and $\int$ from the symbol ""long s"" for sum. Thinks like the chain rule are particularly easy to remember. CON: Screws up line spacing in typeset text. Also, is this a fraction? Can I multiply and find common denominators like I would for $\frac{a}{b}$ ? Better develop a theory differential forms and wedge operators... Couchie (Cauchy) $f', ~f'', \dots, f^{(n)}$ for derivatives and $f^{(-1)}$ for anti-derivatives: $\left(f^{(-1)} \right)^{(1)} = (f^{(-1)})' = f$ . AFF: nice and compact, emphasizes that we have a new function that is related to $f$ CON: $\sin^{-1} x$ vs $\sin^{(-1)} x$ vs $\left( \sin x \right)^{-1}$ Eyoulrrr (Euler) $Df, ~D^2 f, \dots, D^n f$ for derivatives and $D^{-1} f$ for antiderivatives: $D (D^{-1} f) = f$ . AFF: Emphasizes derivatives as an operator on function spaces, looks like the ""add exponents"" rule for multiplication CON: Pretty abstract ""A camel is a horse designed by committee"" Pick a notation for the current problem and sub-sub-field. Switch for the next problem. AFF: Get to pick and choose whichever notation emphasizes the most important aspect at hand (new function / slope / operator on function spaces /  ...) CON: Students will hate you. Edited to add Please note the soft-question tag! I am personally of the opinion that we end up using whichever notation is most convenient for the work at hand (outside of Newton's notation, which is almost solely relegated to time-derivatives and physics). Furthermore, all notation is abstract and strictly symbols on a page that we have agreed define meaning (cue linguistics discussion). If it helps, think of this as a response to the hypothetical Calc 1 student asking why you notate derivatives so many different ways. Alternatively, if you had to pick one of these notations to use for the rest of your mathematical career (and the careers of all subsequent mathematicians), which would it be and why?","Within calculus we have many different symbols attached with differentiating (or is that deriving?) a function, probably to the dismay of the modern calculus student. So here's the setup: As a modern researcher, you and your collaborators N, L, C, and E have just discovered a new field called ""kalkulus of one variable."" Unfortunately, discord has broken out among your co-horts and they simply cannot agree on whose notation is best for your collaborative research papers and text books. As the only person who didn't come up with an original notation, they have agreed to let you arbitrate and single-handedly pick the one-and-forever notation. Whose do you pick and why? For the following, interpret everything as a function of the appropriate variables. Also, ignore that all four of your collaborators weren't actually ever really active at the same time. The contenders (cribbing from wikipedia ) have sent you their notation and a short argument: Nutonne (Newton) for derivatives..err...fluxions  and or for anti-derivatives...err...fluents...err..absements (yes, that symbol rendered correctly). Fundamental Theorem: AFF: Newtonne seems to be the first of your collaborators to have developed the full theory, although he failed to email the rest of you. Claims absolute naming rights. Notation generally typesets well, unlike his nemesis's. CON: There is a historical anecdote of your contemporaries floundering for a century. Libknittz (Leibniz) for derivatives and for anti-derivatives: . AFF: Emphasizes the kalkulus as being about a rate of change and we always see the dependent and independent variables. for difference and from the symbol ""long s"" for sum. Thinks like the chain rule are particularly easy to remember. CON: Screws up line spacing in typeset text. Also, is this a fraction? Can I multiply and find common denominators like I would for ? Better develop a theory differential forms and wedge operators... Couchie (Cauchy) for derivatives and for anti-derivatives: . AFF: nice and compact, emphasizes that we have a new function that is related to CON: vs vs Eyoulrrr (Euler) for derivatives and for antiderivatives: . AFF: Emphasizes derivatives as an operator on function spaces, looks like the ""add exponents"" rule for multiplication CON: Pretty abstract ""A camel is a horse designed by committee"" Pick a notation for the current problem and sub-sub-field. Switch for the next problem. AFF: Get to pick and choose whichever notation emphasizes the most important aspect at hand (new function / slope / operator on function spaces /  ...) CON: Students will hate you. Edited to add Please note the soft-question tag! I am personally of the opinion that we end up using whichever notation is most convenient for the work at hand (outside of Newton's notation, which is almost solely relegated to time-derivatives and physics). Furthermore, all notation is abstract and strictly symbols on a page that we have agreed define meaning (cue linguistics discussion). If it helps, think of this as a response to the hypothetical Calc 1 student asking why you notate derivatives so many different ways. Alternatively, if you had to pick one of these notations to use for the rest of your mathematical career (and the careers of all subsequent mathematicians), which would it be and why?","\dot y, ~\ddot y, ~\dot{\ddot y} , \dots, \overset{n}{\dot y} \square y \boxed{y} \square \dot y = y \frac{dy}{dx}, ~\frac{d^2 y}{dx^2}, \dots, \frac{d^n y}{d x^n} \int y ~\mathrm{d}x \frac{d}{dx} \int y ~\mathrm{d}x = y \mathrm{d} \int \frac{a}{b} f', ~f'', \dots, f^{(n)} f^{(-1)} \left(f^{(-1)} \right)^{(1)} = (f^{(-1)})' = f f \sin^{-1} x \sin^{(-1)} x \left( \sin x \right)^{-1} Df, ~D^2 f, \dots, D^n f D^{-1} f D (D^{-1} f) = f","['calculus', 'derivatives', 'soft-question', 'notation', 'math-history']"
93,How prove this equation has only one solution $\cos{(2x)}+\cos{x}\cdot\cos{(\sqrt{(\pi-3x)(\pi+x)}})=0$,How prove this equation has only one solution,\cos{(2x)}+\cos{x}\cdot\cos{(\sqrt{(\pi-3x)(\pi+x)}})=0,"Let $x\in (0,\dfrac{\pi}{3}]$.   Show that this equation    $$\cos{(2x)}+\cos{x}\cdot\cos{(\sqrt{(\pi-3x)(\pi+x)}})=0$$   has a unique solution $x=\dfrac{\pi}{3}$ I try to the constructor $$f(x)=\cos{(2x)}+\cos{x}\cdot\cos{(\sqrt{(\pi-3x)(\pi+x)}})=0\, , \quad\quad  f(\dfrac{\pi}{3})=0$$but I use found this function is not  a monotonic function see wolframpha Now the key How to prove this function $f(x)$ in $(0,\frac{\pi}{3})$ has no solution, since $$f\left(\frac{\pi}{6}\right)=\dfrac{1}{2}+\dfrac{1}{\sqrt{3}}\cos{\left(\dfrac{1}{2}\sqrt{\dfrac{7}{3}}\pi\right)}=-0.138\cdots<0$$ in other words, how to prove that $$f(x)<0,\forall x\in(0,\dfrac{\pi}{3}) \, .$$","Let $x\in (0,\dfrac{\pi}{3}]$.   Show that this equation    $$\cos{(2x)}+\cos{x}\cdot\cos{(\sqrt{(\pi-3x)(\pi+x)}})=0$$   has a unique solution $x=\dfrac{\pi}{3}$ I try to the constructor $$f(x)=\cos{(2x)}+\cos{x}\cdot\cos{(\sqrt{(\pi-3x)(\pi+x)}})=0\, , \quad\quad  f(\dfrac{\pi}{3})=0$$but I use found this function is not  a monotonic function see wolframpha Now the key How to prove this function $f(x)$ in $(0,\frac{\pi}{3})$ has no solution, since $$f\left(\frac{\pi}{6}\right)=\dfrac{1}{2}+\dfrac{1}{\sqrt{3}}\cos{\left(\dfrac{1}{2}\sqrt{\dfrac{7}{3}}\pi\right)}=-0.138\cdots<0$$ in other words, how to prove that $$f(x)<0,\forall x\in(0,\dfrac{\pi}{3}) \, .$$",,"['calculus', 'trigonometry', 'inequality']"
94,"Calculus question with circle, and string tracing an area","Calculus question with circle, and string tracing an area",,"The figure shows a piece of string tied to a circle with a radius of one unit. The string is just long enough to reach the opposite side of the circle. Find the area of the region, not including the circle itself that is traced out when the string is unwound counterclockwise and continues counterclockwise until it reaches the opposite side again.","The figure shows a piece of string tied to a circle with a radius of one unit. The string is just long enough to reach the opposite side of the circle. Find the area of the region, not including the circle itself that is traced out when the string is unwound counterclockwise and continues counterclockwise until it reaches the opposite side again.",,['calculus']
95,When and why is $x^x$ undefined?,When and why is  undefined?,x^x,"I was learning in logarithmic differentiation how to differentiate a function like $f(x)=x^x$ so naturally i was curious about how the graph of the actual function looks like. I graphed it on this app and to my surprise the entire section from $(- \infty,0)$ is undefined. I do understand why we could have problems and abnormalities about $x=0$ but what about $x=-10$ shouldn't that just be $(-10)^{-10}$  which from my understanding is a very valid real number. So the question really begs itself why is the function undefined at $x<0$?","I was learning in logarithmic differentiation how to differentiate a function like $f(x)=x^x$ so naturally i was curious about how the graph of the actual function looks like. I graphed it on this app and to my surprise the entire section from $(- \infty,0)$ is undefined. I do understand why we could have problems and abnormalities about $x=0$ but what about $x=-10$ shouldn't that just be $(-10)^{-10}$  which from my understanding is a very valid real number. So the question really begs itself why is the function undefined at $x<0$?",,"['calculus', 'algebra-precalculus', 'analysis']"
96,Integral $\int_0^1(x(1-x))^n\frac{d^n}{d^n x}(\log x \cdot\log (1-x))dx$,Integral,\int_0^1(x(1-x))^n\frac{d^n}{d^n x}(\log x \cdot\log (1-x))dx,"While playing around with the first values of the integral $$ I_n:=-\int_0^1\left(x(1-x)\right)^n\frac{d^n}{d^nx}\left(\log x \cdot\log (1-x)\right){\rm d}x, \quad \quad n=1,2,3,\cdots, $$ I got  $$ \small{\begin{align} I_1&=0,&I_2&=\frac19,&I_3&=0,&I_4&=\frac3{25},\\ I_5&=0,&I_6&=\frac{40}{49},&I_7&=0,&I_8&=\frac{140}{9},\\ I_9&=0,&I_{10}&=\frac{72576}{121},&I_{11}&=0,&I_{12}&=\frac{6652800}{169},\\ I_{13}&=0,&I_{14}&=\color{#99004d}{3953664},&I_{15}&=0,&I_{16}&=\frac{163459296000}{289},\\ I_{17}&=0,&I_{18}&=\frac{39520825344000}{361},&I_{19}&=0,&I_{20}&=\color{#99004d}{27583922995200},\\ I_{21}&=0,&I_{22}&=\frac{4644631106519040000}{529},&I_{23}&=0,&I_{24}&=\color{#99004d}{3446935565184663552},\\ I_{25}&=0,&I_{26}&=\color{#99004d}{1636721540923392000000},&I_{27}&=0,&I_{28}&=\frac{777776389315596582912000000}{841},\\ I_{29}&=0,&I_{30}&=\cdots. \end{align}} $$ By splitting up the initial integral into $\displaystyle \int_0^{1/2}$,  $\displaystyle \int_{1/2}^1$ and by using the symmetry of the integrand, I've indeed proved that $I_{2n+1}=0, \, n=0,1,2,3,\cdots.$ Now observing the first values above, my question is: Does the integral $I_{2n}$ take on infinitely integer values?","While playing around with the first values of the integral $$ I_n:=-\int_0^1\left(x(1-x)\right)^n\frac{d^n}{d^nx}\left(\log x \cdot\log (1-x)\right){\rm d}x, \quad \quad n=1,2,3,\cdots, $$ I got  $$ \small{\begin{align} I_1&=0,&I_2&=\frac19,&I_3&=0,&I_4&=\frac3{25},\\ I_5&=0,&I_6&=\frac{40}{49},&I_7&=0,&I_8&=\frac{140}{9},\\ I_9&=0,&I_{10}&=\frac{72576}{121},&I_{11}&=0,&I_{12}&=\frac{6652800}{169},\\ I_{13}&=0,&I_{14}&=\color{#99004d}{3953664},&I_{15}&=0,&I_{16}&=\frac{163459296000}{289},\\ I_{17}&=0,&I_{18}&=\frac{39520825344000}{361},&I_{19}&=0,&I_{20}&=\color{#99004d}{27583922995200},\\ I_{21}&=0,&I_{22}&=\frac{4644631106519040000}{529},&I_{23}&=0,&I_{24}&=\color{#99004d}{3446935565184663552},\\ I_{25}&=0,&I_{26}&=\color{#99004d}{1636721540923392000000},&I_{27}&=0,&I_{28}&=\frac{777776389315596582912000000}{841},\\ I_{29}&=0,&I_{30}&=\cdots. \end{align}} $$ By splitting up the initial integral into $\displaystyle \int_0^{1/2}$,  $\displaystyle \int_{1/2}^1$ and by using the symmetry of the integrand, I've indeed proved that $I_{2n+1}=0, \, n=0,1,2,3,\cdots.$ Now observing the first values above, my question is: Does the integral $I_{2n}$ take on infinitely integer values?",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
97,What is the difference between stationary point and critical point in Calculus?,What is the difference between stationary point and critical point in Calculus?,,"What is the difference between stationary point and critical point? We find critical points by finding the roots of the derivative, but in which cases is a critical point not a stationary point? An example would be most helpful. I am asking this question because I ran into the following question: Locate the critical points and identify which critical points are stationary points. So, obviously It's implying that not every critical point is a stationary point.","What is the difference between stationary point and critical point? We find critical points by finding the roots of the derivative, but in which cases is a critical point not a stationary point? An example would be most helpful. I am asking this question because I ran into the following question: Locate the critical points and identify which critical points are stationary points. So, obviously It's implying that not every critical point is a stationary point.",,"['calculus', 'derivatives']"
98,Alternating series involving zeta function,Alternating series involving zeta function,,Can anyone help me attain the result for the following series? $$\sum_{n=2}^{\infty} \frac{(-1)^n \zeta(n)}{n(n+1)}= \frac{1}{2} \left( \log 2 + \log \pi +\gamma -2 \right)$$ I don't know how to start. I am seriously thinking that this can be done using residues or contour integration since with real analysis I cannot see a pattern.,Can anyone help me attain the result for the following series? $$\sum_{n=2}^{\infty} \frac{(-1)^n \zeta(n)}{n(n+1)}= \frac{1}{2} \left( \log 2 + \log \pi +\gamma -2 \right)$$ I don't know how to start. I am seriously thinking that this can be done using residues or contour integration since with real analysis I cannot see a pattern.,,"['calculus', 'sequences-and-series', 'riemann-zeta']"
99,"Closed-form of integral $\int_0^1 \int_0^1 \frac{\arcsin\left(\sqrt{1-s}\sqrt{y}\right)}{\sqrt{1-y} \cdot (sy-y+1)}\,ds\,dy $",Closed-form of integral,"\int_0^1 \int_0^1 \frac{\arcsin\left(\sqrt{1-s}\sqrt{y}\right)}{\sqrt{1-y} \cdot (sy-y+1)}\,ds\,dy ","I'm looking for a closed form of this definite iterated integral. $$I = \int_0^1 \int_0^1 \frac{\arcsin\left(\sqrt{1-s}\sqrt{y}\right)}{\sqrt{1-y} \cdot (sy-y+1)}\,ds\,dy. $$ From Vladimir Reshetnikov we already know it, that the numerical value of it is $$I\approx4.49076009892257799033708885767243640685411695804791115741588093621176851...$$ There are similar integrals having closed forms: $$ \begin{align} J_1 = & \int_0^1 \int_0^1 {\frac {\arcsin \left( \sqrt {1-s}\sqrt {y} \right) }{\sqrt {1-y} \sqrt {sy-y+1}}}\,ds\,dy = 2\pi -2\pi \ln 2. \\ J_2 = & \int_0^1 \int_0^1 {\frac {\arcsin \left( \sqrt {1-s}\sqrt {y} \right) }{\sqrt {1-s} \sqrt {y}\sqrt {sy-y+1}}}\,ds\,dy = -\frac{7}{4}\zeta\left( 3 \right)+\frac{1}{2}{\pi }^{2}\ln 2. \end{align}$$","I'm looking for a closed form of this definite iterated integral. $$I = \int_0^1 \int_0^1 \frac{\arcsin\left(\sqrt{1-s}\sqrt{y}\right)}{\sqrt{1-y} \cdot (sy-y+1)}\,ds\,dy. $$ From Vladimir Reshetnikov we already know it, that the numerical value of it is $$I\approx4.49076009892257799033708885767243640685411695804791115741588093621176851...$$ There are similar integrals having closed forms: $$ \begin{align} J_1 = & \int_0^1 \int_0^1 {\frac {\arcsin \left( \sqrt {1-s}\sqrt {y} \right) }{\sqrt {1-y} \sqrt {sy-y+1}}}\,ds\,dy = 2\pi -2\pi \ln 2. \\ J_2 = & \int_0^1 \int_0^1 {\frac {\arcsin \left( \sqrt {1-s}\sqrt {y} \right) }{\sqrt {1-s} \sqrt {y}\sqrt {sy-y+1}}}\,ds\,dy = -\frac{7}{4}\zeta\left( 3 \right)+\frac{1}{2}{\pi }^{2}\ln 2. \end{align}$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
