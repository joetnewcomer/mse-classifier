,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Constructing tangent to a curve in $\mathbb{R}^2$,Constructing tangent to a curve in,\mathbb{R}^2,"I've been studying Basic Mathematics for Physics courses. While teaching about derivatives my prof. said that there are actually two points the tangent at a point passes through (and those points are almost coincident). Symbolically $x_0$ and $x_0+\mathrm dx$ . Clearly this sounds absurd because a tangent by definition touches any curve only at a single point. But also, this puts forth the discrepancy that at least two points are required to construct a line. I thought that if we somehow know the curvature of the curve we would indeed be able to construct a tangent using only a single point by using definition of curvature, $\kappa=1/r$ and follow as we do in the case of a circle. But how to actually measure the curvature of curve using only its derivative? Feel free to present a model of constructing a tangent this way or any other that you may find relevant and fitting into intermediate-to-advanced calculus courses.","I've been studying Basic Mathematics for Physics courses. While teaching about derivatives my prof. said that there are actually two points the tangent at a point passes through (and those points are almost coincident). Symbolically and . Clearly this sounds absurd because a tangent by definition touches any curve only at a single point. But also, this puts forth the discrepancy that at least two points are required to construct a line. I thought that if we somehow know the curvature of the curve we would indeed be able to construct a tangent using only a single point by using definition of curvature, and follow as we do in the case of a circle. But how to actually measure the curvature of curve using only its derivative? Feel free to present a model of constructing a tangent this way or any other that you may find relevant and fitting into intermediate-to-advanced calculus courses.",x_0 x_0+\mathrm dx \kappa=1/r,"['calculus', 'geometry', 'derivatives', 'tangent-line']"
1,Derivative of a function involving a characteristic function.,Derivative of a function involving a characteristic function.,,"Consider the function $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ defined by $$f(x,y,z) = y^2z\chi_{(0,\infty)^3}.$$ I would like to find $$\frac{\partial^3f}{\partial x\,\partial y\,\partial z}.$$ I would have thought that if $x,y,z >0$ then $$\frac{\partial^3f}{\partial x\,\partial y\,\partial z} = \frac{\partial^2}{\partial x\,\partial y}y^2 = \frac{\partial}{\partial x}2y = 0.$$ In the other case the function would be identically $0$ and hence it will have $0$ derivative. Is this correct?",Consider the function defined by I would like to find I would have thought that if then In the other case the function would be identically and hence it will have derivative. Is this correct?,"f:\mathbb{R}^3 \rightarrow \mathbb{R} f(x,y,z) = y^2z\chi_{(0,\infty)^3}. \frac{\partial^3f}{\partial x\,\partial y\,\partial z}. x,y,z >0 \frac{\partial^3f}{\partial x\,\partial y\,\partial z} = \frac{\partial^2}{\partial x\,\partial y}y^2 = \frac{\partial}{\partial x}2y = 0. 0 0","['real-analysis', 'complex-analysis', 'derivatives', 'characteristic-functions']"
2,Should $d/dx$ ever be used when solving for a partial derivative? Or would all instances of it be replaced with $\partial/\partial x$?,Should  ever be used when solving for a partial derivative? Or would all instances of it be replaced with ?,d/dx \partial/\partial x,"You hold one of the variables constant and find the derivative of the other when trying to find the partial derivative, so wouldn't something like this be correct?: $\frac{\partial}{\partial x}(|xy|) = \frac{\partial}{\partial x}(|x||y|) = |y| \frac{d}{dx}|x|=\frac{|y|x}{|x|}$ Or would this be correct: $\frac{\partial}{\partial x}(|xy|) = \frac{\partial}{\partial x}(|x||y|) = |y| \frac{\partial}{\partial x}|x|=\frac{|y|x}{|x|}$ Or are either of those fine? Any help is appreciated.","You hold one of the variables constant and find the derivative of the other when trying to find the partial derivative, so wouldn't something like this be correct?: Or would this be correct: Or are either of those fine? Any help is appreciated.",\frac{\partial}{\partial x}(|xy|) = \frac{\partial}{\partial x}(|x||y|) = |y| \frac{d}{dx}|x|=\frac{|y|x}{|x|} \frac{\partial}{\partial x}(|xy|) = \frac{\partial}{\partial x}(|x||y|) = |y| \frac{\partial}{\partial x}|x|=\frac{|y|x}{|x|},"['calculus', 'derivatives', 'notation', 'partial-derivative', 'convention']"
3,Problem with left limits.,Problem with left limits.,,"Let $F: \mathbb{R} \to \mathbb{R}$ a non-decreasing function and suppose $G: \mathbb{R} \to \mathbb{R}$ defined by $G(x) = F(x+)$ (= the right limit of $F$ in $x$ , which always exists for non-decreasing functions) is differentiable almost everywhere. Is it true that $F$ is differentiable almost everywhere? Here is the relevant fragment from the book that I'm reading: Note that $F$ and $G$ are continuous at the same points and they agree   at each point at which they are continuous; furthermore, if $F(x_0) =  G(x_0)$ , then $\frac{F(x)−F(x_0)}{x−x_0}$ lies between $\frac{G(x)−G(x_0)}{x−x0}$ and $\frac{G(x−)−G(x_0)}{x−x_0}$ . Hence if $G$ is   differentiable at $x_0$ , then $F$ is differentiable at $x_0$ , and $F'(x_0) = G'(x_0)$ . The almost everywhere differentiability of F   follows. I suspect the author uses the squeeze theorem here, but I see no reason why we should have that $$\frac{G(x-)-G(x_0)}{x-x_0} \to G'(x_0)$$","Let a non-decreasing function and suppose defined by (= the right limit of in , which always exists for non-decreasing functions) is differentiable almost everywhere. Is it true that is differentiable almost everywhere? Here is the relevant fragment from the book that I'm reading: Note that and are continuous at the same points and they agree   at each point at which they are continuous; furthermore, if , then lies between and . Hence if is   differentiable at , then is differentiable at , and . The almost everywhere differentiability of F   follows. I suspect the author uses the squeeze theorem here, but I see no reason why we should have that","F: \mathbb{R} \to \mathbb{R} G: \mathbb{R} \to \mathbb{R} G(x) = F(x+) F x F F G F(x_0) =
 G(x_0) \frac{F(x)−F(x_0)}{x−x_0} \frac{G(x)−G(x_0)}{x−x0} \frac{G(x−)−G(x_0)}{x−x_0} G x_0 F x_0 F'(x_0) = G'(x_0) \frac{G(x-)-G(x_0)}{x-x_0} \to G'(x_0)","['real-analysis', 'measure-theory']"
4,Derivative of $a^x$ for $a<0$,Derivative of  for,a^x a<0,"Is it correct to write that $$ \frac{\mathrm{d}}{\mathrm{d}x} \left( a^x \right)  = a^x \log(a) $$ also for $a < 0$ , where $\log$ is the natural logarithm? I was trying to compute the derivative of $(-10)^x$ . WolframAlpha gave the following result: $$ \frac{\mathrm{d}}{\mathrm{d}x} \bigl( (-10)^x \bigr)  = (-10)^x (\log(10) + \mathrm{i}\pi),$$ but I am not sure if it is correct, because in this case the argument of logarithm is negative. The reason why I am asking this question is becasue I want to calculate the following limit $$\lim_{x \to \infty}\frac{|C_1a^x+C_2(ka)^x|^{A}+|C_3|^{A}-|C_1a^x+C_2(ka)^x-C_3|^{A}}{a^xD_1+(ka)^xD_2}$$ knowing that $-1<a<1$ , $-1<k<1$ , $1<A<2$ and $x \in \mathbb{N}$ . Since for $x \to \infty$ we have $\left[\frac{0}{0}\right]$ , I use the de L'Hopital's rule. $$\lim_{x \to \infty}\frac{|C_1a^x+C_2(ka)^x|^{A}+|C_3|^{A}-|C_1a^x+C_2(ka)^x-C_3|^{A}}{a^xD_1+(ka)^xD_2}=\left[\frac{0}{0}\right]=\lim_{x \to \infty}\frac{A|C_1a^x+C_2(ka)^x|^{A-1}sgn(C_1a^x+C_2(ka)^x)(C_1a^xlog(a)+C_2(ka)^xlog(ka)) ...}{a^xlog(a)D_1+(ka)^xlog(ka)D_2}\\\frac{-A|C_1a^x+C_2(ka)^x-C_3|^{A-1}sgn(C_1a^x+C_2(ka)^x-C_3)(C_1a^xlog(a)+C_2(ka)^xlog(ka))}{a^xlog(a)D_1+(ka)^xlog(ka)D_2}$$ Now, I take the expression $a^x$ in front of both nominator and denominator and reduce it obtaining $$\lim_{x \to \infty}\frac{A|C_1a^x+C_2(ka)^x|^{A-1}sgn(C_1a^x+C_2(ka)^x)(C_1log(a)+C_2(k)^xlog(ka)) ...}{log(a)D_1+(k)^xlog(ka)D_2}\\\frac{-A|C_1a^x+C_2(ka)^x-C_3|^{A-1}sgn(C_1a^x+C_2(ka)^x-C_3)(C_1log(a)+C_2(k)^xlog(ka))}{log(a)D_1+(k)^xlog(ka)D_2}$$ and finally I obtain $\frac{A|C_3|^{A-1}sgn(C_3)C_1log(a)}{log(a)D_1}$ as the result. At some point, I calculate the derivative of $a^x$ which is equal to $a^xlog(a)$ and I am sure that it is correct in the case of $0<a<1$ , but what if $-1<a<0$ ? Can I write that the derivative of $a^x$ is $a^xlog(a)$ when $-1<a<0$ ? If not, how should I calculate this limit in the case of $-1<a<0$ ?","Is it correct to write that also for , where is the natural logarithm? I was trying to compute the derivative of . WolframAlpha gave the following result: but I am not sure if it is correct, because in this case the argument of logarithm is negative. The reason why I am asking this question is becasue I want to calculate the following limit knowing that , , and . Since for we have , I use the de L'Hopital's rule. Now, I take the expression in front of both nominator and denominator and reduce it obtaining and finally I obtain as the result. At some point, I calculate the derivative of which is equal to and I am sure that it is correct in the case of , but what if ? Can I write that the derivative of is when ? If not, how should I calculate this limit in the case of ?"," \frac{\mathrm{d}}{\mathrm{d}x} \left( a^x \right)
 = a^x \log(a)  a < 0 \log (-10)^x  \frac{\mathrm{d}}{\mathrm{d}x} \bigl( (-10)^x \bigr)
 = (-10)^x (\log(10) + \mathrm{i}\pi), \lim_{x \to \infty}\frac{|C_1a^x+C_2(ka)^x|^{A}+|C_3|^{A}-|C_1a^x+C_2(ka)^x-C_3|^{A}}{a^xD_1+(ka)^xD_2} -1<a<1 -1<k<1 1<A<2 x \in \mathbb{N} x \to \infty \left[\frac{0}{0}\right] \lim_{x \to \infty}\frac{|C_1a^x+C_2(ka)^x|^{A}+|C_3|^{A}-|C_1a^x+C_2(ka)^x-C_3|^{A}}{a^xD_1+(ka)^xD_2}=\left[\frac{0}{0}\right]=\lim_{x \to \infty}\frac{A|C_1a^x+C_2(ka)^x|^{A-1}sgn(C_1a^x+C_2(ka)^x)(C_1a^xlog(a)+C_2(ka)^xlog(ka)) ...}{a^xlog(a)D_1+(ka)^xlog(ka)D_2}\\\frac{-A|C_1a^x+C_2(ka)^x-C_3|^{A-1}sgn(C_1a^x+C_2(ka)^x-C_3)(C_1a^xlog(a)+C_2(ka)^xlog(ka))}{a^xlog(a)D_1+(ka)^xlog(ka)D_2} a^x \lim_{x \to \infty}\frac{A|C_1a^x+C_2(ka)^x|^{A-1}sgn(C_1a^x+C_2(ka)^x)(C_1log(a)+C_2(k)^xlog(ka)) ...}{log(a)D_1+(k)^xlog(ka)D_2}\\\frac{-A|C_1a^x+C_2(ka)^x-C_3|^{A-1}sgn(C_1a^x+C_2(ka)^x-C_3)(C_1log(a)+C_2(k)^xlog(ka))}{log(a)D_1+(k)^xlog(ka)D_2} \frac{A|C_3|^{A-1}sgn(C_3)C_1log(a)}{log(a)D_1} a^x a^xlog(a) 0<a<1 -1<a<0 a^x a^xlog(a) -1<a<0 -1<a<0","['derivatives', 'logarithms', 'exponential-function']"
5,Leibniz rule for expression containing derivative.,Leibniz rule for expression containing derivative.,,"I have found the following algebraic manipulation in a solution to a homework problem, and I don't understand it. The claim is (assume continuous, differentiable $f$ ): $$ \frac{x^2}{2} f^\prime(x) = \int_0^x f^\prime(x)(x-t)\,dt $$ I find this confusing, because it seems to me that I can write $$\int_0^x t f^\prime(x)\,dt = \left[\frac{t^2}{2} f^\prime(x)\right]_0^x = \frac{x^2}{2} f^\prime(x),$$ since $f^\prime(x)$ does not depend on $t$ , which does not agree with the expression above (where does the $-x$ part come from)? What am I missing?","I have found the following algebraic manipulation in a solution to a homework problem, and I don't understand it. The claim is (assume continuous, differentiable ): I find this confusing, because it seems to me that I can write since does not depend on , which does not agree with the expression above (where does the part come from)? What am I missing?","f  \frac{x^2}{2} f^\prime(x) = \int_0^x f^\prime(x)(x-t)\,dt  \int_0^x t f^\prime(x)\,dt = \left[\frac{t^2}{2} f^\prime(x)\right]_0^x = \frac{x^2}{2} f^\prime(x), f^\prime(x) t -x","['integration', 'derivatives']"
6,Differentiating areas of polygons with respect to its inscribed circle,Differentiating areas of polygons with respect to its inscribed circle,,"When I was an undergraduate, I attempted to generalize the common observation that $\dfrac{d}{dr} \pi r^{2} = 2\pi r$ for other shapes (that is, when is the derivative of area equal to perimeter for some other shape?). My finding was that for any regular polygon, differenting the area of a regular polygon with respect to the radius of its inscribed circle will give its perimeter as a function of the radius of its inscribed circle. I detailed my experience with the problem here ( https://youtu.be/0vYWsOBBXxw ), and because of the experience I had with it (starting with a common observation, finding a pattern, making a conjecture, and then proving it) I have given a talk on the problem to the undergraduate math majors at my university a couple of times, I think the problem has a lot of pedogogical value at that level. Anyways, last week, I gave the talk, and one of the professors that attended remarked that he thinks that the general result here is that given any polygon, if we find its ""center of mass"", then the measurement we want is the shortest distance from the center of mass to the edge of the polygon. Can anyone comment on what the more general result is here? That is, in describing ""when the derivative of area is perimeter"", what's the most general statement we can say about the most general shapes? Or, how could we show that the center of mass to edge is always the measurement we want?","When I was an undergraduate, I attempted to generalize the common observation that for other shapes (that is, when is the derivative of area equal to perimeter for some other shape?). My finding was that for any regular polygon, differenting the area of a regular polygon with respect to the radius of its inscribed circle will give its perimeter as a function of the radius of its inscribed circle. I detailed my experience with the problem here ( https://youtu.be/0vYWsOBBXxw ), and because of the experience I had with it (starting with a common observation, finding a pattern, making a conjecture, and then proving it) I have given a talk on the problem to the undergraduate math majors at my university a couple of times, I think the problem has a lot of pedogogical value at that level. Anyways, last week, I gave the talk, and one of the professors that attended remarked that he thinks that the general result here is that given any polygon, if we find its ""center of mass"", then the measurement we want is the shortest distance from the center of mass to the edge of the polygon. Can anyone comment on what the more general result is here? That is, in describing ""when the derivative of area is perimeter"", what's the most general statement we can say about the most general shapes? Or, how could we show that the center of mass to edge is always the measurement we want?",\dfrac{d}{dr} \pi r^{2} = 2\pi r,"['calculus', 'geometry', 'derivatives', 'reference-request']"
7,Material Derivative in Cylindrical Coordinates,Material Derivative in Cylindrical Coordinates,,"I'm testing myself on my knowledge from this book by taking the material derivative of velocity in cylindrical coordinates: $$\frac{D\mathbf u}{Dt}=\mathbf u\cdot \nabla \mathbf u$$ Which, in tensor notation, can be written as: $$\frac{\partial u^ig_i}{\partial t} + u^jg_j\cdot g^k\nabla_k u^i g_i$$ Which can be simplified to: $$\frac{\partial u^ig_i}{\partial t} + u^j\delta^k_j\nabla_k u^i g_i$$ And further: $$\frac{\partial u^ig_i}{\partial t} + u^j\nabla_j u^i g_i$$ Finally, the contravariant components can be written as: $$\frac{\partial u^i}{\partial t} + u^j(u^i_{,j}+\Gamma^i_{jk}u^k)$$ The expression above yielded the correct expressions for the $r$ and $z$ coordinates. For the $\theta$ -coordinate, however, I get the following expression: $$\frac{\partial u^\theta}{\partial t}+u^r(\frac{\partial u^\theta}{\partial r}+\frac{u^\theta}{r})+u^\theta(\frac{\partial u^\theta}{\partial r}+\frac{u^r}{r})+u^z(\frac{\partial u^\theta}{\partial r})$$ Which can be simplified to: $$\frac{\partial u^\theta}{\partial t}+u^r\frac{\partial u^\theta}{\partial r}+u^\theta\frac{\partial u^\theta}{\partial r}+\frac{2u^r u^\theta}{r}+u^z\frac{\partial u^\theta}{\partial r}$$ The physical components to this velocities are: \begin{align} u^{(r)} & =u^r \\  u^{(\theta)} &= ru^\theta \\ u^{(z)} &= u^z \end{align} However, putting the physical components back into the last expression, I end up with: $$\frac{1}{r}\frac{\partial u^{(\theta)}}{\partial t}+\frac{1}{r}u^{(r)}\frac{\partial u^{(\theta)}}{\partial r}+\frac{u^{(\theta)}}{r^2}\frac{\partial u^{(\theta)}}{\partial r}+\frac{u^{(r)} u^{(\theta)}}{r^2}+\frac{u^{(z)}}{r}\frac{\partial u^{(\theta)}}{\partial r}$$ Which is almost the definition for the material derivative, except that it is divided by $r$ for some reason. Any ideas where I might have gone wrong?","I'm testing myself on my knowledge from this book by taking the material derivative of velocity in cylindrical coordinates: Which, in tensor notation, can be written as: Which can be simplified to: And further: Finally, the contravariant components can be written as: The expression above yielded the correct expressions for the and coordinates. For the -coordinate, however, I get the following expression: Which can be simplified to: The physical components to this velocities are: However, putting the physical components back into the last expression, I end up with: Which is almost the definition for the material derivative, except that it is divided by for some reason. Any ideas where I might have gone wrong?","\frac{D\mathbf u}{Dt}=\mathbf u\cdot \nabla \mathbf u \frac{\partial u^ig_i}{\partial t} + u^jg_j\cdot g^k\nabla_k u^i g_i \frac{\partial u^ig_i}{\partial t} + u^j\delta^k_j\nabla_k u^i g_i \frac{\partial u^ig_i}{\partial t} + u^j\nabla_j u^i g_i \frac{\partial u^i}{\partial t} + u^j(u^i_{,j}+\Gamma^i_{jk}u^k) r z \theta \frac{\partial u^\theta}{\partial t}+u^r(\frac{\partial u^\theta}{\partial r}+\frac{u^\theta}{r})+u^\theta(\frac{\partial u^\theta}{\partial r}+\frac{u^r}{r})+u^z(\frac{\partial u^\theta}{\partial r}) \frac{\partial u^\theta}{\partial t}+u^r\frac{\partial u^\theta}{\partial r}+u^\theta\frac{\partial u^\theta}{\partial r}+\frac{2u^r u^\theta}{r}+u^z\frac{\partial u^\theta}{\partial r} \begin{align}
u^{(r)} & =u^r \\ 
u^{(\theta)} &= ru^\theta \\
u^{(z)} &= u^z
\end{align} \frac{1}{r}\frac{\partial u^{(\theta)}}{\partial t}+\frac{1}{r}u^{(r)}\frac{\partial u^{(\theta)}}{\partial r}+\frac{u^{(\theta)}}{r^2}\frac{\partial u^{(\theta)}}{\partial r}+\frac{u^{(r)} u^{(\theta)}}{r^2}+\frac{u^{(z)}}{r}\frac{\partial u^{(\theta)}}{\partial r} r","['linear-algebra', 'derivatives', 'tensors', 'fluid-dynamics']"
8,Prove or disprove sentence about $\int f$,Prove or disprove sentence about,\int f,"I post this question a few months ago. I solved the items (1) and (3), but I cannot to solve (2). Today I read this question again and I'm curious about solution of (2). Today, I had an idea, but I dont know if it works. Idea . A monotone function has only jump discontinuities. So, $f|_{[f(x_{0}^{-}),f(x_{0}^{+})]}$ is continuous, then there is a maximum and minimum. If $w$ is a minimum on $[f(x_{0}^{-}),f(x_{0}^{+})]$ , then $$w \leq f(x) \Longrightarrow w(x-x_{0}) \leq \int_{x_{0}}^{x}f(t)dt = F(x) - F(x_{0})$$ taking, WLOG, $x \geq x_{0}$ . But, this works for $f$ on $[f(x_{0}^{-}),f(x_{0}^{+})]$ . What about the general case?","I post this question a few months ago. I solved the items (1) and (3), but I cannot to solve (2). Today I read this question again and I'm curious about solution of (2). Today, I had an idea, but I dont know if it works. Idea . A monotone function has only jump discontinuities. So, is continuous, then there is a maximum and minimum. If is a minimum on , then taking, WLOG, . But, this works for on . What about the general case?","f|_{[f(x_{0}^{-}),f(x_{0}^{+})]} w [f(x_{0}^{-}),f(x_{0}^{+})] w \leq f(x) \Longrightarrow w(x-x_{0}) \leq \int_{x_{0}}^{x}f(t)dt = F(x) - F(x_{0}) x \geq x_{0} f [f(x_{0}^{-}),f(x_{0}^{+})]","['real-analysis', 'derivatives']"
9,derivative of log(y) function. [closed],derivative of log(y) function. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I am trying to find derivative of log function. How would I apply the chain rule on a basic example? $$\ln(y)=m+bx$$ I think it is $$\frac{dy}{dx}=e^{m+bx} *b.$$ Is this correct?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I am trying to find derivative of log function. How would I apply the chain rule on a basic example? I think it is Is this correct?",\ln(y)=m+bx \frac{dy}{dx}=e^{m+bx} *b.,"['derivatives', 'logarithms']"
10,Fourier Transform of $r^n e^{-\alpha r}$,Fourier Transform of,r^n e^{-\alpha r},"I believe I can solve this problem, but I would like some help deciphering what my professor is asking me to do. He is having us find the Fourier transform of \begin{equation} r^n e^{-\alpha r} \end{equation} and says ""find this in terms of $\alpha$ derivatives of the Fourier transform of part (i)"". My result from part (i) is \begin{equation} f(r) = \dfrac{1}{r}e^{-\alpha r} \Rightarrow \tilde{f}(k) = -\dfrac{4\pi}{(\alpha + ik)^2} \end{equation} In that problem, he did explicitly ask us to evaluate part (i) in the limit of $\alpha \rightarrow 0$ , which of course is just $\tilde{f}(k) = 4\pi / k^2$ . Any help in understanding what to do would be great. I know how to take the Fourier transforms, but I'm unsure what he is asking us to do with the $\alpha$ derivatives. Thanks!","I believe I can solve this problem, but I would like some help deciphering what my professor is asking me to do. He is having us find the Fourier transform of and says ""find this in terms of derivatives of the Fourier transform of part (i)"". My result from part (i) is In that problem, he did explicitly ask us to evaluate part (i) in the limit of , which of course is just . Any help in understanding what to do would be great. I know how to take the Fourier transforms, but I'm unsure what he is asking us to do with the derivatives. Thanks!","\begin{equation}
r^n e^{-\alpha r}
\end{equation} \alpha \begin{equation}
f(r) = \dfrac{1}{r}e^{-\alpha r} \Rightarrow \tilde{f}(k) = -\dfrac{4\pi}{(\alpha + ik)^2}
\end{equation} \alpha \rightarrow 0 \tilde{f}(k) = 4\pi / k^2 \alpha","['integration', 'derivatives', 'fourier-transform']"
11,When is $f''(x)$ undefined but $x$ not an inflection point of $f$?,When is  undefined but  not an inflection point of ?,f''(x) x f,"This is supposed to be for bullet 5.2 in this question: A rigorous yet intuitive summary of inflection and critical points for beginning calculus? There are examples for undulation points, where $f: A \to B, A,B \subseteq \mathbb R, f''(x)=0$ but $(x,f(x))$ is not an inflection point of $f$ like $f(t) = \frac{t^4}{12},f''(t)=t^2$ ( If $f''(x)=0$ but is not an inflection point, what is it called? ) There are examples where $f''(x)$ is undefined but $(x,f(x))$ is an inflection point like $f'(t)=|t|$ ( An inflection point where the second derivative doesn't exist? ) I'm missing something very obvious, but what's an example where $f''(x)$ is undefined, but $(x,f(x))$ is not an inflection point of $f$ ? After some thought, I have come up with some examples. Are these correct? First example : This example is one where $f$ is defined but not continuous at $x$ . $f: \mathbb R \to \mathbb R, f(t)=\text{sign}(t), x=0$ . $f'(0)$ is undefined and thus so is $f''(0)$ . However $f''(t)=0$ for $t \ne 0$ . In case someone asks further 'Okay, what if we assumed $f$ is continuous? Then $x$ would have to be an inflection point right?', then we proceed: Second example: This example is one where $f$ is continuous but not differentiable at $x$ . A sharp turn like $g(t)=|t|$ but always increasing at an increasing rate but at different rates $f: \mathbb R \to \mathbb R, f(t)= t 1_{t \le 0} + 3t 1_{t \ge 0}$ . This gives us $f': \mathbb R \setminus \{0\} \to \mathbb R, f'(t) = \text{sign}(t)+2$ . $f'(0)$ is undefined and thus so is $f''(0)$ . However $f''(t)>0$ for $t \ne 0$ . In case someone asks further 'Okay, what if we assumed $f$ is differentiable? Then $x$ would have to be an inflection point right?', then we proceed: Third example : This example is one where $f$ is differentiable at $x$ . $f': \mathbb R \to \mathbb R, f'(t)= t 1_{t \le 0} + 3t 1_{t \ge 0}$ .","This is supposed to be for bullet 5.2 in this question: A rigorous yet intuitive summary of inflection and critical points for beginning calculus? There are examples for undulation points, where but is not an inflection point of like ( If $f''(x)=0$ but is not an inflection point, what is it called? ) There are examples where is undefined but is an inflection point like ( An inflection point where the second derivative doesn't exist? ) I'm missing something very obvious, but what's an example where is undefined, but is not an inflection point of ? After some thought, I have come up with some examples. Are these correct? First example : This example is one where is defined but not continuous at . . is undefined and thus so is . However for . In case someone asks further 'Okay, what if we assumed is continuous? Then would have to be an inflection point right?', then we proceed: Second example: This example is one where is continuous but not differentiable at . A sharp turn like but always increasing at an increasing rate but at different rates . This gives us . is undefined and thus so is . However for . In case someone asks further 'Okay, what if we assumed is differentiable? Then would have to be an inflection point right?', then we proceed: Third example : This example is one where is differentiable at . .","f: A \to B, A,B \subseteq \mathbb R, f''(x)=0 (x,f(x)) f f(t) = \frac{t^4}{12},f''(t)=t^2 f''(x) (x,f(x)) f'(t)=|t| f''(x) (x,f(x)) f f x f: \mathbb R \to \mathbb R, f(t)=\text{sign}(t), x=0 f'(0) f''(0) f''(t)=0 t \ne 0 f x f x g(t)=|t| f: \mathbb R \to \mathbb R, f(t)= t 1_{t \le 0} + 3t 1_{t \ge 0} f': \mathbb R \setminus \{0\} \to \mathbb R, f'(t) = \text{sign}(t)+2 f'(0) f''(0) f''(t)>0 t \ne 0 f x f x f': \mathbb R \to \mathbb R, f'(t)= t 1_{t \le 0} + 3t 1_{t \ge 0}","['calculus', 'derivatives']"
12,Variables change in derivation. What is wrong?,Variables change in derivation. What is wrong?,,"I have two set of variables, which are related: $\left\{ \alpha, \beta, \gamma \right\}$ and $\left\{ v_0, v_1, v_2 \right\} = \left\{ \alpha, \beta, \beta \gamma \right\}$ Now, I want to compute the partial derivative $\frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)}$ . My first approach: as $\alpha$ and $\gamma$ do not depend on $\beta$ , $\frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} = \frac{1}{\alpha  \gamma} \frac{\partial \beta}{\partial \beta} = \frac{1}{\alpha  \gamma} = \frac{v_1}{v_0 v_2}$ . But also, one could thinks as follows: $\frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} =  \frac{\partial v_1}{\partial \left( v_0 v_2 \right)} = 0$ Something should be meshed up related with the dependencies, but I cannot see it. Can anyone give me a hand?","I have two set of variables, which are related: and Now, I want to compute the partial derivative . My first approach: as and do not depend on , . But also, one could thinks as follows: Something should be meshed up related with the dependencies, but I cannot see it. Can anyone give me a hand?","\left\{ \alpha, \beta, \gamma \right\} \left\{ v_0, v_1, v_2 \right\} = \left\{ \alpha, \beta, \beta \gamma \right\} \frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} \alpha \gamma \beta \frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} = \frac{1}{\alpha  \gamma} \frac{\partial \beta}{\partial \beta} = \frac{1}{\alpha  \gamma} = \frac{v_1}{v_0 v_2} \frac{\partial \beta}{\partial \left(\alpha \beta \gamma \right)} =  \frac{\partial v_1}{\partial \left( v_0 v_2 \right)} = 0","['derivatives', 'chain-rule']"
13,The explicit expression of $\frac{δF}{δP}$,The explicit expression of,\frac{δF}{δP},"I'm writing simulation code of ferroelectric domain, and there is a math problem that I can't solve. The expression of $F$ is $$ F = \frac{|\vec{k} \cdot \vec{P}(\vec{k})|^2}{k^2}. $$ $\vec{k}$ is a wave-vector in Fourier space, i.e. $\vec{k}=(k_x,k_y)$ , and $\vec{P}(\vec{k}) = (P_x,P_y)$ is the Fourier transform of Polarization in real space. What's the explicit expression of $$ \frac{δF}{δP_x}? $$ More details are available here .","I'm writing simulation code of ferroelectric domain, and there is a math problem that I can't solve. The expression of is is a wave-vector in Fourier space, i.e. , and is the Fourier transform of Polarization in real space. What's the explicit expression of More details are available here .","F 
F = \frac{|\vec{k} \cdot \vec{P}(\vec{k})|^2}{k^2}.
 \vec{k} \vec{k}=(k_x,k_y) \vec{P}(\vec{k}) = (P_x,P_y) 
\frac{δF}{δP_x}?
","['derivatives', 'variational-analysis']"
14,Area of sector bounded by line and curve,Area of sector bounded by line and curve,,"I have a line segment $\overline{AB}$ , with endpoints $(x_1,y_1)$ and $(x_2,y_2)$ . Drawing any function $f(x)$ through those 2 points, the function must have an average velocity over $[x_1,x_2]$ of $\frac{y_2-y_1}{x_2-x_1}$ . However, the area bounded by the line and $f(x)$ can vary greatly, and what I'm trying to find is the relationship between $f'(x)$ and that area. I'm not sure about how to formally express this next part, but as the average velocity of both the line and function over $[x_1,x_2]$ is the same, the average of the sum of the values of $f'(x)$ at all the infinite points on $f(x)$ over the domain $[x_1,x_2]$ will be the same as the average of the sum of the slope/value of the line's derivative at all the infinite points the line in the domain $[x_1,x_2]$ . However, the actual values of $f'(x)$ could vary significantly. Visually, it appears the greater the greatest y-value of $f'(x)$ over $[x_1,x_2]$ , the greater the area if $f(x)$ has no stationary points in the domain $[x_1,x_2]$ . For example, with the line segment with endpoints $(0,0)$ and $(2,4)$ , and the function $f(x)=x^\frac{3}{2}*\sqrt{2}$ , the area bounded by $y=2x$ and $f(x)$ is $\frac{16-2\sqrt{2}}{5}$ , and the greatest y-value of $f'(x)$ over $[1,2]$ is 3. If $f(x)=\frac{x^3}{2}$ , then the area bounded is 4, and the greatest y-value of $f'(x)$ over $[0,2]$ is 6. Am I correct in saying that the greater the greatest y-value of $f'(x)$ over $[x_1,x_2]$ , the greater the area bounded, if $f(x)$ does not have stationary points in the domain $[x_1,x_2]$ ? And if so, does there exist a relation that, given a line segment over $[x_1,x_2]$ , can give the area bounded by the line segment and $f(x)$ , given only the greatest y-value of $f'(x)$ over $[x_1,x_2]$ ? Thanks to David K for answering the above questions. I only have one last question: I am aware of how to actually find the exact area, but, is there some general correlation that can be drawn between some property of the derivative, and the area? Which is to say, using no calculus methods, can one draw any conclusions about the area given the equation of the derivative? Or even given the actual function? I also have no idea what tags I should use on this, so I would appreciate it too if someone could put on the right tags.","I have a line segment , with endpoints and . Drawing any function through those 2 points, the function must have an average velocity over of . However, the area bounded by the line and can vary greatly, and what I'm trying to find is the relationship between and that area. I'm not sure about how to formally express this next part, but as the average velocity of both the line and function over is the same, the average of the sum of the values of at all the infinite points on over the domain will be the same as the average of the sum of the slope/value of the line's derivative at all the infinite points the line in the domain . However, the actual values of could vary significantly. Visually, it appears the greater the greatest y-value of over , the greater the area if has no stationary points in the domain . For example, with the line segment with endpoints and , and the function , the area bounded by and is , and the greatest y-value of over is 3. If , then the area bounded is 4, and the greatest y-value of over is 6. Am I correct in saying that the greater the greatest y-value of over , the greater the area bounded, if does not have stationary points in the domain ? And if so, does there exist a relation that, given a line segment over , can give the area bounded by the line segment and , given only the greatest y-value of over ? Thanks to David K for answering the above questions. I only have one last question: I am aware of how to actually find the exact area, but, is there some general correlation that can be drawn between some property of the derivative, and the area? Which is to say, using no calculus methods, can one draw any conclusions about the area given the equation of the derivative? Or even given the actual function? I also have no idea what tags I should use on this, so I would appreciate it too if someone could put on the right tags.","\overline{AB} (x_1,y_1) (x_2,y_2) f(x) [x_1,x_2] \frac{y_2-y_1}{x_2-x_1} f(x) f'(x) [x_1,x_2] f'(x) f(x) [x_1,x_2] [x_1,x_2] f'(x) f'(x) [x_1,x_2] f(x) [x_1,x_2] (0,0) (2,4) f(x)=x^\frac{3}{2}*\sqrt{2} y=2x f(x) \frac{16-2\sqrt{2}}{5} f'(x) [1,2] f(x)=\frac{x^3}{2} f'(x) [0,2] f'(x) [x_1,x_2] f(x) [x_1,x_2] [x_1,x_2] f(x) f'(x) [x_1,x_2]","['calculus', 'derivatives', 'area']"
15,Finding the second antiderivative given f(x) values,Finding the second antiderivative given f(x) values,,"Find $f$ . $f''(t)=7e^t + 3\sin(t)$ , $f(0) = 0$ and $f(\pi)=0$ I found that: $f'(t)= 7e^t + 3\cos(t) + C$ $f(t) = 7e^t - 3\sin(t) + Ct + D$ How do I know what equation to plug the $x$ -values into?","Find . , and I found that: How do I know what equation to plug the -values into?",f f''(t)=7e^t + 3\sin(t) f(0) = 0 f(\pi)=0 f'(t)= 7e^t + 3\cos(t) + C f(t) = 7e^t - 3\sin(t) + Ct + D x,"['calculus', 'integration', 'derivatives']"
16,How fast the height of the cone is changing?,How fast the height of the cone is changing?,,"Suppose that sand is collecting in the shape of a cone in such a way that the base radius of the cone is always one-third of its height. If $3\,\mathrm{cm^3/min}$ is the rate at which the sand is being added to the cone, how fast is the height of the cone changing when it is $7\,\mathrm{cm}$ tall? I got $Dv/dt=3cm^3 $ and $r=1/3h$ . So I'm looking for $dh/dt$ when $h=7$ . I'm not sure if I'm doing the problem right.","Suppose that sand is collecting in the shape of a cone in such a way that the base radius of the cone is always one-third of its height. If is the rate at which the sand is being added to the cone, how fast is the height of the cone changing when it is tall? I got and . So I'm looking for when . I'm not sure if I'm doing the problem right.","3\,\mathrm{cm^3/min} 7\,\mathrm{cm} Dv/dt=3cm^3  r=1/3h dh/dt h=7","['calculus', 'geometry', 'derivatives', 'volume']"
17,Does the Time Evolution Operator Satisfy the Schrodinger Equation?,Does the Time Evolution Operator Satisfy the Schrodinger Equation?,,"I would just like to confirm my solution to the following question. I'm a bit hesitant on my solution because of a specific step. I would just like confirmation if that step, which I will point out, is mathematically legal. The question I'm working on is: Show that the time evolution operator, given by the Dyson series, \begin{equation}     \mathcal{U}(t,0)          =              1 + \sum_{n=1}^\infty \bigg ( \dfrac{-i}{\hbar} \bigg )^n \int_0^t dt_1 \int_0^{t_1} dt_2 \dots \int_0^t dt_{n-1} H(t_1) H(t_2) \dots H(t_n) \end{equation} satisfies Schrodinger's equation \begin{equation}     i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) = H\mathcal{U}(t,0).     \label{SE} \end{equation} For this problem, I will evaluate the left-hand side of the Schrodinger equation to show that it is equivalent to the right-hand side. Firstly, we have that the Dyson series can be rewritten as \begin{equation} \mathcal{U}(t,0)  =  1 + \sum_{n=1}^\infty \bigg ( \dfrac{-i}{\hbar} \bigg )^n \int_0^t dt_1 \int_0^{t_1} dt_2 \dots \int_0^t dt_{n-1} H(t_1) H(t_2) \dots H(t_n) = T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \nonumber \end{equation} The following steps is where my question is. The part I'm referring too is when I take the time-derivative with respect to $t'$ . Using this, we have that \begin{align} i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) &= i\hbar \, \partial_{t'} \bigg [ T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \bigg ] \nonumber \\[.5em] &= i\hbar \bigg [  \partial_{t'}  \big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )  \bigg ] \bigg [ T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \bigg ] \nonumber \\[.5em] &= -i\hbar \dfrac{i}{\hbar} \partial_{t'}\int_0^t dt' H(t') \mathcal{U}(t,0)  \nonumber \\[.5em] &= H \mathcal{U}(t,0)  \end{align} Therefore, $i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) = H\mathcal{U}(t,0)$ and $\mathcal{U}(t,0)$ satisfies the Schrodinger equation. I'm essentially performing the following differential: \begin{equation} \partial_x e^{f(x)} = f'(x) e^{f(x)} \end{equation} Since the time-evolution operator is defined as the sum, I'm not sure how legal it is to take that derivative in the way that I did. If this is incorrect, I would appreciate an alternative method of working this problem. Any guidance would be appreciated, thank you!","I would just like to confirm my solution to the following question. I'm a bit hesitant on my solution because of a specific step. I would just like confirmation if that step, which I will point out, is mathematically legal. The question I'm working on is: Show that the time evolution operator, given by the Dyson series, satisfies Schrodinger's equation For this problem, I will evaluate the left-hand side of the Schrodinger equation to show that it is equivalent to the right-hand side. Firstly, we have that the Dyson series can be rewritten as The following steps is where my question is. The part I'm referring too is when I take the time-derivative with respect to . Using this, we have that Therefore, and satisfies the Schrodinger equation. I'm essentially performing the following differential: Since the time-evolution operator is defined as the sum, I'm not sure how legal it is to take that derivative in the way that I did. If this is incorrect, I would appreciate an alternative method of working this problem. Any guidance would be appreciated, thank you!","\begin{equation}
    \mathcal{U}(t,0) 
        = 
            1 + \sum_{n=1}^\infty \bigg ( \dfrac{-i}{\hbar} \bigg )^n \int_0^t dt_1 \int_0^{t_1} dt_2 \dots \int_0^t dt_{n-1} H(t_1) H(t_2) \dots H(t_n)
\end{equation} \begin{equation}
    i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) = H\mathcal{U}(t,0).
    \label{SE}
\end{equation} \begin{equation}
\mathcal{U}(t,0) 
= 
1 + \sum_{n=1}^\infty \bigg ( \dfrac{-i}{\hbar} \bigg )^n \int_0^t dt_1 \int_0^{t_1} dt_2 \dots \int_0^t dt_{n-1} H(t_1) H(t_2) \dots H(t_n)
=
T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \}
\nonumber
\end{equation} t' \begin{align}
i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0)
&=
i\hbar \, \partial_{t'} \bigg [ T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \bigg ]
\nonumber
\\[.5em]
&=
i\hbar \bigg [  \partial_{t'}  \big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )  \bigg ] \bigg [ T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \bigg ]
\nonumber
\\[.5em]
&=
-i\hbar \dfrac{i}{\hbar} \partial_{t'}\int_0^t dt' H(t') \mathcal{U}(t,0) 
\nonumber
\\[.5em]
&=
H \mathcal{U}(t,0) 
\end{align} i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) = H\mathcal{U}(t,0) \mathcal{U}(t,0) \begin{equation}
\partial_x e^{f(x)} = f'(x) e^{f(x)}
\end{equation}","['derivatives', 'physics', 'mathematical-physics', 'quantum-mechanics']"
18,How to change the variables in $\frac{1}{(z+1)^{n+1}}=\frac{(-1)^n}{n!}\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right)$?,How to change the variables in ?,\frac{1}{(z+1)^{n+1}}=\frac{(-1)^n}{n!}\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right),We know that $$\frac{1}{(z+1)^{n+1}}=\frac{(-1)^n}{n!}\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right).$$ If we put $z=e^t$ how can we change the variables above equation? I know that $z\frac{d}{dz}=\frac{d}{dt}$ . So how can we write $\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right)$ ? Is $z^n \frac{d^n}{dz^n}=\frac{d^n}{dt^n}$ ?,We know that If we put how can we change the variables above equation? I know that . So how can we write ? Is ?,\frac{1}{(z+1)^{n+1}}=\frac{(-1)^n}{n!}\frac{d^n}{dz^n}\left( \frac{1}{z+1}\right). z=e^t z\frac{d}{dz}=\frac{d}{dt} \frac{d^n}{dz^n}\left( \frac{1}{z+1}\right) z^n \frac{d^n}{dz^n}=\frac{d^n}{dt^n},"['calculus', 'derivatives']"
19,Solve $\frac{dx}{dy}+\frac{x}{\sqrt{x^2+y^2}}=y$,Solve,\frac{dx}{dy}+\frac{x}{\sqrt{x^2+y^2}}=y,Solve the differential equation Solve $$\frac{dx}{dy}+\frac{x}{\sqrt{x^2+y^2}}=y$$ My try: I used $x=y \tan z$ $$\frac{dx}{dy}=\tan z+\sec^2 z\frac{dz}{dy}$$ So we get: $$\tan z+\sec^2 z\frac{dz}{dy}+\sin z=y$$ Any clue from here?,Solve the differential equation Solve My try: I used So we get: Any clue from here?,\frac{dx}{dy}+\frac{x}{\sqrt{x^2+y^2}}=y x=y \tan z \frac{dx}{dy}=\tan z+\sec^2 z\frac{dz}{dy} \tan z+\sec^2 z\frac{dz}{dy}+\sin z=y,"['ordinary-differential-equations', 'derivatives', 'trigonometry']"
20,Apply the Implicit Function Theorem to find a root of polynomial,Apply the Implicit Function Theorem to find a root of polynomial,,"Caculate the value of the real solution of the equation $x^7+0.99x-2.03$ , and give a estimate for the error. The hint is: use the Implicit Function Theorem. I dont know how to use the IFT in this case, I'm not familiarized with this. I think in construct a function $F:\mathbb{R}^n \times \mathbb{R} \to \mathbb{R}$ with some parameters of which one is the root. Maybe $$F(c_1,c_2,c_3,x) = c_{1}x^7 + c_{2}x - c_{3}.$$ But I'm note sure about this. Can someone help me?","Caculate the value of the real solution of the equation , and give a estimate for the error. The hint is: use the Implicit Function Theorem. I dont know how to use the IFT in this case, I'm not familiarized with this. I think in construct a function with some parameters of which one is the root. Maybe But I'm note sure about this. Can someone help me?","x^7+0.99x-2.03 F:\mathbb{R}^n \times \mathbb{R} \to \mathbb{R} F(c_1,c_2,c_3,x) = c_{1}x^7 + c_{2}x - c_{3}.","['real-analysis', 'derivatives', 'implicit-function-theorem']"
21,The derivative of an integral of a product of functions.,The derivative of an integral of a product of functions.,,"I'm trying to comprehend the following result, which is required for fractional calculus: Let $w(x,y)$ and $f(z)$ be two real functions, such that they both vanish at a point $a$ . Then the following relation holds: $$\frac{\text d}{\text dx} \int_a^xw(x,y)f(y)\text dy=w(x)f(x)+\int_a^xf(y)\frac{\partial{w(x,y)}}{\partial{x}}\text dy.$$ My intuition for a proof points towards the integration by parts of a product of functions. But there might be a sign error if that's the case. The reference for this is ""Construction & Physical Applications Of The Fractional Calculus"", Nicholas Wheeler, 1997 . Thank you for the help!","I'm trying to comprehend the following result, which is required for fractional calculus: Let and be two real functions, such that they both vanish at a point . Then the following relation holds: My intuition for a proof points towards the integration by parts of a product of functions. But there might be a sign error if that's the case. The reference for this is ""Construction & Physical Applications Of The Fractional Calculus"", Nicholas Wheeler, 1997 . Thank you for the help!","w(x,y) f(z) a \frac{\text d}{\text dx} \int_a^xw(x,y)f(y)\text dy=w(x)f(x)+\int_a^xf(y)\frac{\partial{w(x,y)}}{\partial{x}}\text dy.","['integration', 'derivatives', 'products']"
22,Find $\lambda$ such that $f$ it is differentiable in zero and has a continuous derivative in zero,Find  such that  it is differentiable in zero and has a continuous derivative in zero,\lambda f,"I am trying to solve this task Find $\lambda>0$ such that $f=\begin{cases}0& x=0\\ |x|^{\lambda}\cdot \sin\frac{1}{x} & x\neq 0 \end{cases}$ a) is differentiable in zero b) has a continuous derivative in zero. My try a) $$\lim_{h \rightarrow 0^+}\frac{f(h)-f(0)}{h} = \lim_{h \rightarrow 0^+}\frac{|h|^{\lambda}}{h} \cdot \sin\frac{1}{h} = 0$$ $$\lim_{h \rightarrow 0^-}\frac{f(h)-f(0)}{h} = \lim_{h \rightarrow 0^+}\frac{|h|^{\lambda}}{h} \cdot \sin\frac{1}{h} = 0$$ But there I have some doubts. It seems like $\lambda$ does not matter there.... Moreover, can somebody give me hints to b)? I think that I have do the same thing but this is unbelievable","I am trying to solve this task Find such that a) is differentiable in zero b) has a continuous derivative in zero. My try a) But there I have some doubts. It seems like does not matter there.... Moreover, can somebody give me hints to b)? I think that I have do the same thing but this is unbelievable",\lambda>0 f=\begin{cases}0& x=0\\ |x|^{\lambda}\cdot \sin\frac{1}{x} & x\neq 0 \end{cases} \lim_{h \rightarrow 0^+}\frac{f(h)-f(0)}{h} = \lim_{h \rightarrow 0^+}\frac{|h|^{\lambda}}{h} \cdot \sin\frac{1}{h} = 0 \lim_{h \rightarrow 0^-}\frac{f(h)-f(0)}{h} = \lim_{h \rightarrow 0^+}\frac{|h|^{\lambda}}{h} \cdot \sin\frac{1}{h} = 0 \lambda,"['real-analysis', 'calculus']"
23,How do I find the following limit of $f'(a+(1/x))$ without assumption of continuity of $f '$?,How do I find the following limit of  without assumption of continuity of ?,f'(a+(1/x)) f ',"We are given $a$ is a real number such that $f(a)=5$ , $f'(a)=2$ . Calculate the limit: $$\lim_{n \to \infty}\left(\frac{f\left(a+1/n\right)}{f(a)}\right)^n$$ Here is what I tried: I just raised $e$ to the power of all of that and then took $\log$ and I got that I'd need to calculate the limit of $$\exp\left( n \cdot \log\left(\frac{f(a+(1/n))}{f(a)}\right)\right).$$ Now using L'Hospitals rule, I got: $$\lim_{x \to \infty}\frac{\log(f(a+(1/x))-\log(f(a))}{1/x}=\lim_{x \to \infty}(f'(a+(1/x))/f(a+(1/x)). $$ Now $f(a+(1/x))$ approaches $f(a)=5$ , because $f$ is continuous at $a$ . But what does $f'((a+(1/x))$ approach? If we knew $f'$ was continuous, the answer would be $2$ , and then the total answer would be $e^{2/5}$ which is the real answer, but how can you get it without knowing $f'$ is continuous at $a$ ?","We are given is a real number such that , . Calculate the limit: Here is what I tried: I just raised to the power of all of that and then took and I got that I'd need to calculate the limit of Now using L'Hospitals rule, I got: Now approaches , because is continuous at . But what does approach? If we knew was continuous, the answer would be , and then the total answer would be which is the real answer, but how can you get it without knowing is continuous at ?",a f(a)=5 f'(a)=2 \lim_{n \to \infty}\left(\frac{f\left(a+1/n\right)}{f(a)}\right)^n e \log \exp\left( n \cdot \log\left(\frac{f(a+(1/n))}{f(a)}\right)\right). \lim_{x \to \infty}\frac{\log(f(a+(1/x))-\log(f(a))}{1/x}=\lim_{x \to \infty}(f'(a+(1/x))/f(a+(1/x)).  f(a+(1/x)) f(a)=5 f a f'((a+(1/x)) f' 2 e^{2/5} f' a,"['calculus', 'limits', 'derivatives']"
24,How to prove this property using convexity?,How to prove this property using convexity?,,"Suppose that $f:[a,b]\to\mathbb{R}$ be a twice-differentiable function, and that there exists $c\in(a,b)$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$ . Show that if $f''(x)>0$ for all $x\in[a,b]$ and $f''$ is strictly increasing on $[a,b]$ , then $c>\frac{a+b}{2}$ . Using Taylor's theorem, i solved the problem. But, i'd like to prove that using convexity. Give some comments or hints. Thank you!","Suppose that be a twice-differentiable function, and that there exists such that . Show that if for all and is strictly increasing on , then . Using Taylor's theorem, i solved the problem. But, i'd like to prove that using convexity. Give some comments or hints. Thank you!","f:[a,b]\to\mathbb{R} c\in(a,b) f'(c)=\frac{f(b)-f(a)}{b-a} f''(x)>0 x\in[a,b] f'' [a,b] c>\frac{a+b}{2}","['real-analysis', 'derivatives', 'convexity-inequality']"
25,"Why is the operation of differentiation called ""instantaneous change""?","Why is the operation of differentiation called ""instantaneous change""?",,It is said that Differentiation is an operation which finds the instantaneous change of the direction of a curve. Why is the word change emphasized by the word instantaneous ?,It is said that Differentiation is an operation which finds the instantaneous change of the direction of a curve. Why is the word change emphasized by the word instantaneous ?,,"['calculus', 'derivatives']"
26,Partial derivative 8,Partial derivative 8,,"I have the following derivative: $$f'(k)=\frac{a(1+bk)}{k^{1-a}(1+abk)^{^{a}}}$$ and have to compute the partial derivative with respect to b, possibly obtaining the following result: $$\frac{\partial f'(k)}{\partial b}=\frac{ak^{a}(1-a)(1+a+abk)}{(1+abk)^{a+1}}$$ I've tried to do the exercise but I cannot get closer to the desired result than this: $$\frac{\partial f'{(k)}}{\partial b}=ak^{a}(1+abk)^{-a}\left [ 1-\frac{a(a+abk)}{(1+abk)} \right ]$$ Thank you so much and Happy Christmas! :)","I have the following derivative: and have to compute the partial derivative with respect to b, possibly obtaining the following result: I've tried to do the exercise but I cannot get closer to the desired result than this: Thank you so much and Happy Christmas! :)",f'(k)=\frac{a(1+bk)}{k^{1-a}(1+abk)^{^{a}}} \frac{\partial f'(k)}{\partial b}=\frac{ak^{a}(1-a)(1+a+abk)}{(1+abk)^{a+1}} \frac{\partial f'{(k)}}{\partial b}=ak^{a}(1+abk)^{-a}\left [ 1-\frac{a(a+abk)}{(1+abk)} \right ],"['calculus', 'derivatives', 'partial-derivative']"
27,Show that the function $f$ is strictly bounded by an arbitrary number $A$,Show that the function  is strictly bounded by an arbitrary number,f A,"Suppose the function $f(x)$ is differentiable on $[a,b]$ and there exists an arbitrary number $A$ such that for all $x \in (a,b)$ $$f(a)<A$$ $$f(x)+f'(x)<A$$ Show that $f(x)<A$ for all $x \in (a,b)$ . My attempt: Generate a function $h(x)$ such that $$h(x)=e^xf(x)$$ $$\Longrightarrow h'(x)=[e^xf(x)]'=e^x[f(x)+f'(x)]<e^xA$$ So $h'(x)$ is strictly bounded by $e^xA$ . By MVT, there exists two points $c,d \in (a,b)$ such that $$h(c)-h(d)=h'(\frac{c+d}{2})(c-d)$$ $$e^cf(c)<Ae^{\frac{c+d}{2}}(c-d)+Ae^d$$ $$f(c)<Ae^{\frac{d-c}{2}}(c-d)+Ae^{d-c}=Ae^{d-c}[e^{\frac{1}{2}}(c-d)+1]$$ It seems my proof is wrong and get discontinued when I focus on simplifying for $f(c)$ but the question requires that $f(x)$ is strictly bounded by $A$ for the interval $I=(a,b)$ .","Suppose the function is differentiable on and there exists an arbitrary number such that for all Show that for all . My attempt: Generate a function such that So is strictly bounded by . By MVT, there exists two points such that It seems my proof is wrong and get discontinued when I focus on simplifying for but the question requires that is strictly bounded by for the interval .","f(x) [a,b] A x \in (a,b) f(a)<A f(x)+f'(x)<A f(x)<A x \in (a,b) h(x) h(x)=e^xf(x) \Longrightarrow h'(x)=[e^xf(x)]'=e^x[f(x)+f'(x)]<e^xA h'(x) e^xA c,d \in (a,b) h(c)-h(d)=h'(\frac{c+d}{2})(c-d) e^cf(c)<Ae^{\frac{c+d}{2}}(c-d)+Ae^d f(c)<Ae^{\frac{d-c}{2}}(c-d)+Ae^{d-c}=Ae^{d-c}[e^{\frac{1}{2}}(c-d)+1] f(c) f(x) A I=(a,b)","['real-analysis', 'derivatives']"
28,How to use the chain rule for change of variable,How to use the chain rule for change of variable,,"I have asked this questions: Change of variables in differential equation? ...but after thinking about it, I am still a little confused of how to rigorously use the chain rule to calculate the derivative(s) of a function for a change of variable. I have the following derivative: $f(x) = \frac{dw(x)}{dx}$ Now I introduce the change of variable: $\hat{x}=\frac{x}{L}$ and I apply the chain rule: I write: $g(\hat{x}) = L \hat{x} = x$ I substitute: $f(g(\hat{x})) = \frac{dw(g(\hat{x}))}{d(g(\hat{x}))}$ ...but this does not help me... I am confusing something. I would be glad, if someone could show me in detail and step by step how to do this rigorously. Thanks a lot.","I have asked this questions: Change of variables in differential equation? ...but after thinking about it, I am still a little confused of how to rigorously use the chain rule to calculate the derivative(s) of a function for a change of variable. I have the following derivative: Now I introduce the change of variable: and I apply the chain rule: I write: I substitute: ...but this does not help me... I am confusing something. I would be glad, if someone could show me in detail and step by step how to do this rigorously. Thanks a lot.",f(x) = \frac{dw(x)}{dx} \hat{x}=\frac{x}{L} g(\hat{x}) = L \hat{x} = x f(g(\hat{x})) = \frac{dw(g(\hat{x}))}{d(g(\hat{x}))},"['derivatives', 'chain-rule']"
29,Stuck on proof using Cauchy's integral formula,Stuck on proof using Cauchy's integral formula,,"I posted my attempted proof to this question here but I realized that I was wrong in taking the limit, and that the proof did not make sense. So I am still stuck on this problem let $f: \Omega \rightarrow \mathbb{C}$ be analytic and $z_0 \in \mathbb{C}$ . Define $$g(z) = \begin{cases}        \frac{f(z)-f(z_0)}{z- z_0} & z \not = z_0 \\       f'(z_0) & z = z_0     \end{cases}$$ now pick $\varepsilon$ small enough so that $\overline{D(z_0, \varepsilon)} \subset \Omega$ Show that whenever $z \in D(z_0, \varepsilon)$ $$\frac{g(z) - g(z_0)}{z-z_0} = \frac{1}{2\pi i}\int_{\partial D(z_0, \varepsilon)}\frac{f(\zeta)}{(\zeta-z)(\zeta-z_0)^2}d\zeta$$ So is Cauchy's integral formula still the right way to go? I end up getting that $$\frac{g(z) - g(z_0)}{z-z_0} = \frac{f(z) - f(z_0)}{z-z_0} - \frac{1}{z-z_0}\int_{\partial D(z_0, \varepsilon)} \frac{f(\zeta)}{(\zeta-z_0)^2}d\zeta$$ and I am not sure how to proceed from here","I posted my attempted proof to this question here but I realized that I was wrong in taking the limit, and that the proof did not make sense. So I am still stuck on this problem let be analytic and . Define now pick small enough so that Show that whenever So is Cauchy's integral formula still the right way to go? I end up getting that and I am not sure how to proceed from here","f: \Omega \rightarrow \mathbb{C} z_0 \in \mathbb{C} g(z) = \begin{cases} 
      \frac{f(z)-f(z_0)}{z- z_0} & z \not = z_0 \\
      f'(z_0) & z = z_0 
   \end{cases} \varepsilon \overline{D(z_0, \varepsilon)} \subset \Omega z \in D(z_0, \varepsilon) \frac{g(z) - g(z_0)}{z-z_0} = \frac{1}{2\pi i}\int_{\partial D(z_0, \varepsilon)}\frac{f(\zeta)}{(\zeta-z)(\zeta-z_0)^2}d\zeta \frac{g(z) - g(z_0)}{z-z_0} = \frac{f(z) - f(z_0)}{z-z_0} - \frac{1}{z-z_0}\int_{\partial D(z_0, \varepsilon)} \frac{f(\zeta)}{(\zeta-z_0)^2}d\zeta","['complex-analysis', 'derivatives', 'contour-integration', 'cauchy-integral-formula']"
30,Chain rule when applying L'Hopital's rule,Chain rule when applying L'Hopital's rule,,"I have a very basic question regarding derivation function: $$f(\omega(t)) = \frac{2 +x(t)\cdot \frac{d\omega(t)}{dt}}{\omega(t)} $$ when I check for $$= \lim_{\omega(t)\to\ 0}\frac{2 +x(t)\cdot\frac{d\omega(t)}{dt}}{\omega(t)} = \frac{2}{0} $$ now if we apply L'Hopital's rule  we get $$= \lim_{\omega(t)\to\ 0}(\frac{\frac{d(2)}{d\omega(t)} +[\frac{d(x(t))}{d\omega(t)}\cdot \frac{d\omega(t)}{dt} + \frac{d(\frac{\omega(t)}{dt})}{d\omega(t)}\cdot \frac{dx(t)}{dt}]}{\frac{d\omega(t)}{d\omega(t)}} )$$ So here is my question this $$ \frac{d(x(t))}{d\omega(t)}\cdot \frac{d\omega(t)}{dt}$$ should become $$ \frac{dx(t)}{dt}$$ according to the chain rule , or am I mathematically wrong. Please also let me know if I have done something mathematically wrong during the derivation.","I have a very basic question regarding derivation function: when I check for now if we apply L'Hopital's rule  we get So here is my question this should become according to the chain rule , or am I mathematically wrong. Please also let me know if I have done something mathematically wrong during the derivation.",f(\omega(t)) = \frac{2 +x(t)\cdot \frac{d\omega(t)}{dt}}{\omega(t)}  = \lim_{\omega(t)\to\ 0}\frac{2 +x(t)\cdot\frac{d\omega(t)}{dt}}{\omega(t)} = \frac{2}{0}  = \lim_{\omega(t)\to\ 0}(\frac{\frac{d(2)}{d\omega(t)} +[\frac{d(x(t))}{d\omega(t)}\cdot \frac{d\omega(t)}{dt} + \frac{d(\frac{\omega(t)}{dt})}{d\omega(t)}\cdot \frac{dx(t)}{dt}]}{\frac{d\omega(t)}{d\omega(t)}} )  \frac{d(x(t))}{d\omega(t)}\cdot \frac{d\omega(t)}{dt}  \frac{dx(t)}{dt},"['derivatives', 'partial-derivative', 'limits-without-lhopital', 'chain-rule']"
31,Differentiation for a function in the integral form.,Differentiation for a function in the integral form.,,"I want to know generally how we differentiate a function $F(x)$ in the following form, $$F(x)=\int_a^x f(x,t)dt$$ For example, if we can work out the explicit form of $F(x)$ as the example below $$F(x)=\int_0^x e^{xt}dt=\frac{e^{x^2}-1}{x}$$ if we differentiate $F(x)$ , we get $$\frac {dF(x)}{dx}=\frac{2x^2e^{x^2}-e^{x^2}+1}{x^2}$$ This simple example shows a very different approach from the case when we different $F(x)=\int_a^xf(t)dt$ , which simply gives us $f(x)$ . Now suppose I have the formula for calculating the money I get after $T$ years, $P(T)$ with continuous depositing, of which the rate is given by $S(t)$ , and continuous compounding, of which the annual rate is given by $r$ . The formula is $$P(T)=\int_0^TS(t)e^{r(T-t)}dt$$ Since the $S(t)$ is not given explicitly, then how do I get an expression for $\frac{dP(T)}{dT}$ ? I have tried using the first principle by $$\frac{dP(T)}{dT}=\lim_{\Delta T\to0} \frac{P(T+\Delta T)-P(T)}{\Delta T}$$ and I could not figure it out. Is there a way to find an expression for the derivative $\frac{dP(T)}{dT}$ ?","I want to know generally how we differentiate a function in the following form, For example, if we can work out the explicit form of as the example below if we differentiate , we get This simple example shows a very different approach from the case when we different , which simply gives us . Now suppose I have the formula for calculating the money I get after years, with continuous depositing, of which the rate is given by , and continuous compounding, of which the annual rate is given by . The formula is Since the is not given explicitly, then how do I get an expression for ? I have tried using the first principle by and I could not figure it out. Is there a way to find an expression for the derivative ?","F(x) F(x)=\int_a^x f(x,t)dt F(x) F(x)=\int_0^x e^{xt}dt=\frac{e^{x^2}-1}{x} F(x) \frac {dF(x)}{dx}=\frac{2x^2e^{x^2}-e^{x^2}+1}{x^2} F(x)=\int_a^xf(t)dt f(x) T P(T) S(t) r P(T)=\int_0^TS(t)e^{r(T-t)}dt S(t) \frac{dP(T)}{dT} \frac{dP(T)}{dT}=\lim_{\Delta T\to0} \frac{P(T+\Delta T)-P(T)}{\Delta T} \frac{dP(T)}{dT}","['calculus', 'derivatives', 'implicit-differentiation']"
32,derivative of function and fundamental theorem of calculus,derivative of function and fundamental theorem of calculus,,"Let $f\colon[a,b]\to\mathbb{R}$ be differentiable on $(a,b)$ . Suppose that the limits $f(a+)=\lim_{x\to a+}f(x)$ and $f(b-)=\lim_{x\to b-}f(x)$ exist and are finite. My question is: Do we have $$\int_{a}^{b}f'(x)dx=f(b-)-f(a+)$$ without further assumption on $f$ ? If yes, what would be a reference for this result? If no, is there a counterexample for this? Any help is highly appreciated.","Let be differentiable on . Suppose that the limits and exist and are finite. My question is: Do we have without further assumption on ? If yes, what would be a reference for this result? If no, is there a counterexample for this? Any help is highly appreciated.","f\colon[a,b]\to\mathbb{R} (a,b) f(a+)=\lim_{x\to a+}f(x) f(b-)=\lim_{x\to b-}f(x) \int_{a}^{b}f'(x)dx=f(b-)-f(a+) f","['calculus', 'real-analysis', 'integration', 'derivatives']"
33,Damped vibrations of a membrane stretched over a circular frame,Damped vibrations of a membrane stretched over a circular frame,,"I am given this following PDE with the initial and boundary conditions with $0 < r < 1$ , $t > 0$ , and $v_0$ being a constant: $u,_t,_t + 2bu,_t = u,_r,_r + \frac{1}{r} u,_r$ $u(t,r=0) = 0, \vert u(t,r=0)\vert < \infty$ $u(t=0,r)=0, u,_t(t=0,r) = v_0$ Given this information I am suppose to obtain the following solution:: $$u(t,r) = 2v_0e^{-bt}\sum_{i=0}^\infty \frac{J_0(z_{0n}r)}{z_{0n}J_1(z_{0n})} \frac{\sin(t\sqrt{z_{0n}^2 -b^2})}{z_{0n}^2 -b^2}$$ I am at a loss on how to start this problem. My best guess would be use to separation of variables in which case I would get the following: $u(t,r)=h(t)*f(r)$ $\frac{h''}{h'} + \frac{2bh'}{h} = \frac{f''}{f} + \frac{f'}{rf} = -\lambda$ With the separated ODEs: $h'' + 2bh' +h\lambda = 0$ $f'' + f' + fr\lambda$ = 0 Question: Am I going in the right direction to obtain the given solution and if not what am I missing?","I am given this following PDE with the initial and boundary conditions with , , and being a constant: Given this information I am suppose to obtain the following solution:: I am at a loss on how to start this problem. My best guess would be use to separation of variables in which case I would get the following: With the separated ODEs: = 0 Question: Am I going in the right direction to obtain the given solution and if not what am I missing?","0 < r < 1 t > 0 v_0 u,_t,_t + 2bu,_t = u,_r,_r + \frac{1}{r} u,_r u(t,r=0) = 0, \vert u(t,r=0)\vert < \infty u(t=0,r)=0, u,_t(t=0,r) = v_0 u(t,r) = 2v_0e^{-bt}\sum_{i=0}^\infty \frac{J_0(z_{0n}r)}{z_{0n}J_1(z_{0n})} \frac{\sin(t\sqrt{z_{0n}^2 -b^2})}{z_{0n}^2 -b^2} u(t,r)=h(t)*f(r) \frac{h''}{h'} + \frac{2bh'}{h} = \frac{f''}{f} + \frac{f'}{rf} = -\lambda h'' + 2bh' +h\lambda = 0 f'' + f' + fr\lambda","['derivatives', 'partial-differential-equations', 'orthogonality', 'bessel-functions']"
34,Find $y^{(n)}$ if $y=x^2e^x \cos(x)$,Find  if,y^{(n)} y=x^2e^x \cos(x),"Q :Find $y_n$ if $$y=x^2e^x \cos(x)$$ My Approach : By product rule, $$y'=2xe^x \cos(x)+x^2e^x \cos(x)+x^2e^x \cos(\frac{\pi}{2}+x)$$ $$y''=2e^x \cos(x)+4xe^x \cos(\frac{\pi}{2}+x)+4xe^x \cos(x)+2x^2e^x \cos(\frac{3\pi}{2}-x)$$ If i go in this process than i think i couldn't reach any conclusion because i didn't find any pattern.Any hint or solution will be appreciated. Thanks in advance.","Q :Find if My Approach : By product rule, If i go in this process than i think i couldn't reach any conclusion because i didn't find any pattern.Any hint or solution will be appreciated. Thanks in advance.",y_n y=x^2e^x \cos(x) y'=2xe^x \cos(x)+x^2e^x \cos(x)+x^2e^x \cos(\frac{\pi}{2}+x) y''=2e^x \cos(x)+4xe^x \cos(\frac{\pi}{2}+x)+4xe^x \cos(x)+2x^2e^x \cos(\frac{3\pi}{2}-x),"['calculus', 'derivatives']"
35,Showing the Monge-Ampere equation is elliptic,Showing the Monge-Ampere equation is elliptic,,"I have a question about my books definition of ellipticy on how it relates to the Monge-Ampere equation in $\mathbb R^2$ . The Monge-Ampere equation is as follows. Let $\Omega$ be an open subset of $ \mathbb R^2=\{x=(x_1,x_2)\}$ and let $u\in C^2(\Omega)$ satisfy $u_{x_1x_1}(x)u_{x_2x_2}(x)-u^2_{x_1x_2}(x)-f(x)=0$ where $f>0$ in $\Omega$ and the Hessian of $u$ is positive definite ( $u$ is convex). The definition of ellipticy given in my book is as follows: Consider a general differential equation $F[u]=F(x,u(x),Du(x),D^2u(x))=0$ where $F:S:=\Omega \times \mathbb R \times \mathbb R^d \times S(d,\mathbb R) \to \mathbb R$ where $S(d,\mathbb R)$ is the space of real symmetric $d\times d$ matricies. Elements of $ S$ are written as $(x,z,p,r)$ where $p=(p_1,...,p_d)$ and $r=(r_{ij})_{1\le i,j \le d}$ The differential equation $F[u]$ is said to be elliptic at $u$ if $\left( \dfrac{\partial F}{\partial r_{i,j}}(x,u(x),Du(x),D^2u(x))\right)_{1 \le i,j \le d} $ is positive definite. So using this definition how do we compute $ \dfrac{\partial F}{\partial r_{i,j}}(x,u(x),Du(x),D^2u(x)) $ for each pair of $(i,j)$ ? We can see that $r_{i,j}=u_{x_ixj}$ so for example does $$ \dfrac{\partial F}{\partial r_{1,1}}(x,u(x),Du(x),D^2u(x)) = \dfrac{\partial}{u_{x_1x_1}}\left(u_{x_1x_1}(x)u_{x_2x_2}(x)-u^2_{x_1x_2}(x)-f(x)\right)=u_{x_2x_2}(x) $$ Then by this logic $\left( \dfrac{\partial F}{\partial r_{i,j}}(x,u(x),Du(x),D^2u(x))\right)_{1 \le i,j \le d}= \begin{bmatrix}     u_{x_2x_2}       & -2u_{x_1x_2}  \\     -2u_{x_1x_2}      & u_{x_1x_1}  \\ \end{bmatrix}$ but if this is correct why is this matrix positive definite? Also where does the condition of $f>0$ come into play. Any help is appreciated!",I have a question about my books definition of ellipticy on how it relates to the Monge-Ampere equation in . The Monge-Ampere equation is as follows. Let be an open subset of and let satisfy where in and the Hessian of is positive definite ( is convex). The definition of ellipticy given in my book is as follows: Consider a general differential equation where where is the space of real symmetric matricies. Elements of are written as where and The differential equation is said to be elliptic at if is positive definite. So using this definition how do we compute for each pair of ? We can see that so for example does Then by this logic but if this is correct why is this matrix positive definite? Also where does the condition of come into play. Any help is appreciated!,"\mathbb R^2 \Omega  \mathbb R^2=\{x=(x_1,x_2)\} u\in C^2(\Omega) u_{x_1x_1}(x)u_{x_2x_2}(x)-u^2_{x_1x_2}(x)-f(x)=0 f>0 \Omega u u F[u]=F(x,u(x),Du(x),D^2u(x))=0 F:S:=\Omega \times \mathbb R \times \mathbb R^d \times S(d,\mathbb R) \to \mathbb R S(d,\mathbb R) d\times d  S (x,z,p,r) p=(p_1,...,p_d) r=(r_{ij})_{1\le i,j \le d} F[u] u \left( \dfrac{\partial F}{\partial r_{i,j}}(x,u(x),Du(x),D^2u(x))\right)_{1 \le i,j \le d}   \dfrac{\partial F}{\partial r_{i,j}}(x,u(x),Du(x),D^2u(x))  (i,j) r_{i,j}=u_{x_ixj}  \dfrac{\partial F}{\partial r_{1,1}}(x,u(x),Du(x),D^2u(x)) = \dfrac{\partial}{u_{x_1x_1}}\left(u_{x_1x_1}(x)u_{x_2x_2}(x)-u^2_{x_1x_2}(x)-f(x)\right)=u_{x_2x_2}(x)  \left( \dfrac{\partial F}{\partial r_{i,j}}(x,u(x),Du(x),D^2u(x))\right)_{1 \le i,j \le d}= \begin{bmatrix}
    u_{x_2x_2}       & -2u_{x_1x_2}  \\
    -2u_{x_1x_2}      & u_{x_1x_1}  \\
\end{bmatrix} f>0","['derivatives', 'partial-differential-equations']"
36,Lipschitz continuous and differentiability,Lipschitz continuous and differentiability,,"Hey i'm trying to solve this question: Let U be an open interval in R and a ∈ U. Prove that if f : U → R is Lipschitz continuous, then $g(x) = (f(x) − f(a))^2$ is differentiable at a. so far I have the following, but im not sure how to piece it all together $|f(x) - f(y)| \le k|x-y|$ => $ (f(x) − f(a))^2 \le (k|x-a|)^2$ if $g(x)$ is differentiable at a then $\lim_{x\to a} \frac{g(x) - g(a)}{x - a }$ must exist $\lim_{x\to a} \frac{g(x) - g(a)}{x - a }  = \lim_{x\to a} \frac{(f(x) − f(a))^2 - (f(a) − f(a))^2}{x - a } = \lim_{x\to a} \frac{(f(x) − f(a))^2}{x - a } \le \frac{(k|x-a|)^2}{x-a} = k^2(x-a)$ and from here im not sure where to go to show the limit exists","Hey i'm trying to solve this question: Let U be an open interval in R and a ∈ U. Prove that if f : U → R is Lipschitz continuous, then is differentiable at a. so far I have the following, but im not sure how to piece it all together => if is differentiable at a then must exist and from here im not sure where to go to show the limit exists",g(x) = (f(x) − f(a))^2 |f(x) - f(y)| \le k|x-y|  (f(x) − f(a))^2 \le (k|x-a|)^2 g(x) \lim_{x\to a} \frac{g(x) - g(a)}{x - a } \lim_{x\to a} \frac{g(x) - g(a)}{x - a }  = \lim_{x\to a} \frac{(f(x) − f(a))^2 - (f(a) − f(a))^2}{x - a } = \lim_{x\to a} \frac{(f(x) − f(a))^2}{x - a } \le \frac{(k|x-a|)^2}{x-a} = k^2(x-a),"['real-analysis', 'derivatives', 'lipschitz-functions']"
37,Dual Numbers and Automatic Differentiation,Dual Numbers and Automatic Differentiation,,"I have the following equation. I would like to solve this at the point x = a, but using dual numbers. $$f\left(x\right) = \dfrac{1}{x} + \sin\left(\dfrac{1}{x}\right)$$ Now, the derivative of this function is below, and that is the function we'd like to reach in our answer. $$-\dfrac{1}{x^2} - \dfrac{\cos\left(\dfrac{1}{x}\right)}{x^2}$$ Here is what I have tried, and I am finding it difficult to simplify beyond that. $$\begin{align} f\left(x\right) = \dfrac{1}{x} + \sin\left(\dfrac{1}{x}\right)\\ &= \frac{1}{a^{\prime}+1\epsilon}+ sin(\dfrac{1}{a+1\epsilon})\\ &= \frac{a-1\epsilon}{(a-1\epsilon)(a+1\epsilon)} + sin(\frac{a-1\epsilon}{(a-1\epsilon)(a+1\epsilon)} )\end{align}$$ I do this using the following identities: $$\sin\left(\alpha + \beta \epsilon\right) = \sin\left(\alpha\right) + \cos\left(\alpha\right)\beta\epsilon$$ $$\dfrac{1}{\alpha + \beta\epsilon} = \dfrac{\alpha - \beta\epsilon}{\left(\alpha + \beta\epsilon\right)\left(\alpha - \beta\epsilon\right)}.$$ However, I am not certain how to simplify beyond that painfully obvious first step. Any hints?","I have the following equation. I would like to solve this at the point x = a, but using dual numbers. Now, the derivative of this function is below, and that is the function we'd like to reach in our answer. Here is what I have tried, and I am finding it difficult to simplify beyond that. I do this using the following identities: However, I am not certain how to simplify beyond that painfully obvious first step. Any hints?","f\left(x\right) = \dfrac{1}{x} + \sin\left(\dfrac{1}{x}\right) -\dfrac{1}{x^2} - \dfrac{\cos\left(\dfrac{1}{x}\right)}{x^2} \begin{align} f\left(x\right) = \dfrac{1}{x} + \sin\left(\dfrac{1}{x}\right)\\
&= \frac{1}{a^{\prime}+1\epsilon}+ sin(\dfrac{1}{a+1\epsilon})\\ &= \frac{a-1\epsilon}{(a-1\epsilon)(a+1\epsilon)} + sin(\frac{a-1\epsilon}{(a-1\epsilon)(a+1\epsilon)} )\end{align} \sin\left(\alpha + \beta \epsilon\right) = \sin\left(\alpha\right) + \cos\left(\alpha\right)\beta\epsilon \dfrac{1}{\alpha + \beta\epsilon} = \dfrac{\alpha - \beta\epsilon}{\left(\alpha + \beta\epsilon\right)\left(\alpha - \beta\epsilon\right)}.",['derivatives']
38,Proving that a minimum is unique,Proving that a minimum is unique,,"I want to prove that a function $f(x)$ has its unique minimum at $f(c)$ . By that I mean, $f(c) < f(x)$ for all $x \neq c$ . The strict inequality is obviously vital. My question is, does this follow automatically if $f'(x) = 0$ if and only if $x = c$ ? I've heard the words ""sufficient, necessary"" be thrown around when it comes to derivatives. What do they mean? edit: I forgot to mention: $f$ is bounded below by zero. It is quadratic in $\beta$ .","I want to prove that a function has its unique minimum at . By that I mean, for all . The strict inequality is obviously vital. My question is, does this follow automatically if if and only if ? I've heard the words ""sufficient, necessary"" be thrown around when it comes to derivatives. What do they mean? edit: I forgot to mention: is bounded below by zero. It is quadratic in .",f(x) f(c) f(c) < f(x) x \neq c f'(x) = 0 x = c f \beta,['derivatives']
39,Calculating derivatives with the product rule,Calculating derivatives with the product rule,,"I am trying to calculate the derivative of $x^{T}x$ where x is a column vector. A correct way of doing this is shown in this formula However, I am getting different results with the product rule: $\frac{d(x^{T}x)}{dx}=x^{T}*\frac{dx}{dx}+\frac{d(x^{T})}{dx}*x = x^T + x \ \ (\neq 2x^{T})$ (I used this formula in Leibniz notation from Wikipedia) The problem is probably that it is a dot product and not a regular product. So my question is: how do I apply the product rule for dot products correctly?","I am trying to calculate the derivative of where x is a column vector. A correct way of doing this is shown in this formula However, I am getting different results with the product rule: (I used this formula in Leibniz notation from Wikipedia) The problem is probably that it is a dot product and not a regular product. So my question is: how do I apply the product rule for dot products correctly?",x^{T}x \frac{d(x^{T}x)}{dx}=x^{T}*\frac{dx}{dx}+\frac{d(x^{T})}{dx}*x = x^T + x \ \ (\neq 2x^{T}),"['calculus', 'derivatives', 'vectors']"
40,How to take the derivative of something with respect to something else?,How to take the derivative of something with respect to something else?,,"Just when I thought I understood taking derivatives, a textbook example of taking a derivative the equation $$[x(t)]^2+4000^2=[s(t)]^2$$ with respect to time shows that the derivative is $$x\frac{dx}{dt} = s\frac{ds}{dt}$$ However, after  applying the power rule and the chain rule, I come up with $$2x(t)\cdot x'(t)=2s(t)\cdot s'(t)$$ . What concept am I missing about taking the derivative with respect to time? To elaborate a bit, I'm confused as to where the $\mathbf{x}$ and the $\mathbf{s}$ come from in $$\mathbf{x}\frac{dx}{dt} = \mathbf{s}\frac{ds}{dt}$$ from the example. Furthermore, to me it seems like they forgot to apply the power rule. Example: An Airplane Flying at a Constant Elevation https://cnx.org/contents/ [email protected] :74vQD30u@6/Related-Rates","Just when I thought I understood taking derivatives, a textbook example of taking a derivative the equation with respect to time shows that the derivative is However, after  applying the power rule and the chain rule, I come up with . What concept am I missing about taking the derivative with respect to time? To elaborate a bit, I'm confused as to where the and the come from in from the example. Furthermore, to me it seems like they forgot to apply the power rule. Example: An Airplane Flying at a Constant Elevation https://cnx.org/contents/ [email protected] :74vQD30u@6/Related-Rates",[x(t)]^2+4000^2=[s(t)]^2 x\frac{dx}{dt} = s\frac{ds}{dt} 2x(t)\cdot x'(t)=2s(t)\cdot s'(t) \mathbf{x} \mathbf{s} \mathbf{x}\frac{dx}{dt} = \mathbf{s}\frac{ds}{dt},"['calculus', 'derivatives']"
41,Finite Taylor series of Lagrange Basis functions,Finite Taylor series of Lagrange Basis functions,,Lagrange basis functions are defined as follows $$L_{j}(x) = \prod_{i\neq j} \frac{x-x_{i}}{x_{j}-x_{i}} $$ then $$\ln\Big(L_{j}(x)\Big) = \ln\Big(\prod_{i\neq j} \frac{x-x_{i}}{x_{j}-x_{i}}  \Big) = \sum_{i \neq j } \ln\Big( \frac{x-x_{i}}{x_{j}-x_{i}} \Big) $$ if we derivate we have: $$ \frac{L'_{j}(x)}{L_{j}(x)} =\sum_{i \neq j} \frac{\frac{1}{x_{j}-x_{i}}}{\frac{x-x_{i}}{x_{j}-x_{i}}} = \sum_{i \neq j} \frac{1}{x-x_{i}}  $$ then $$ L_{j}^{(1)}(x) = L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)  $$ if we use the product rule for  derivatives we have that: $$ L''_{j}(x) = L'_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)+L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)' \\ =   L'_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)+L_{j}(x) \Big( \sum_{i \neq j} \frac{-1}{(x-x_{i})^2}  \Big) $$ we know $ L'_{j}(x)  $ so $$ \\ =   L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)\Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)-L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^2}  \Big)\\ =  L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)^2 -L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^2}  \Big) $$ finally $$  L_{j}^{(2)}(x) =  L_{j}(x) \Big\{\Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)^2 -  \sum_{i \neq j} \frac{1}{(x-x_{i})^2}     \Big\} $$ if we continue this way we get: $$ L_{j}^{(3)}(x) =  L_{j}(x)\Big\{ \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)^3 -3 \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big) \Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^2}  \Big) +2\Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^3}  \Big)  \Big\} $$ I want to compute the Taylor Series of $ L_{j}(x) $ and I need to find $ L_{j}^{(n)}(x) $ .  Do you know an expression for $ L_{j}^{(n)}(x) $ or something that could help ? Thank you.,Lagrange basis functions are defined as follows then if we derivate we have: then if we use the product rule for  derivatives we have that: we know so finally if we continue this way we get: I want to compute the Taylor Series of and I need to find .  Do you know an expression for or something that could help ? Thank you.,"L_{j}(x) = \prod_{i\neq j} \frac{x-x_{i}}{x_{j}-x_{i}}  \ln\Big(L_{j}(x)\Big) = \ln\Big(\prod_{i\neq j} \frac{x-x_{i}}{x_{j}-x_{i}}  \Big) = \sum_{i \neq j } \ln\Big( \frac{x-x_{i}}{x_{j}-x_{i}} \Big)   \frac{L'_{j}(x)}{L_{j}(x)} =\sum_{i \neq j} \frac{\frac{1}{x_{j}-x_{i}}}{\frac{x-x_{i}}{x_{j}-x_{i}}} = \sum_{i \neq j} \frac{1}{x-x_{i}}    L_{j}^{(1)}(x) = L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)    L''_{j}(x) = L'_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)+L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)' \\ = 
 L'_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)+L_{j}(x) \Big( \sum_{i \neq j} \frac{-1}{(x-x_{i})^2}  \Big)   L'_{j}(x)    \\ = 
 L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)\Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)-L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^2}  \Big)\\ = 
L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)^2 -L_{j}(x) \Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^2}  \Big)    L_{j}^{(2)}(x) = 
L_{j}(x) \Big\{\Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)^2 -  \sum_{i \neq j} \frac{1}{(x-x_{i})^2}     \Big\}   L_{j}^{(3)}(x) =  L_{j}(x)\Big\{ \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big)^3 -3 \Big( \sum_{i \neq j} \frac{1}{x-x_{i}}  \Big) \Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^2}  \Big) +2\Big( \sum_{i \neq j} \frac{1}{(x-x_{i})^3}  \Big)  \Big\}   L_{j}(x)   L_{j}^{(n)}(x)   L_{j}^{(n)}(x) ","['derivatives', 'taylor-expansion', 'lagrange-interpolation']"
42,$\frac{d}{dx}x^n=nx^{n-1}$ proof by induction,proof by induction,\frac{d}{dx}x^n=nx^{n-1},Problem Proof following statement with induction: $$ \frac{d}{dx}x ^n=nx^{n-1} : \forall n \in \mathbb{Z}_+ $$ Attempt to solve Base case $$ \frac{d}{dx}x^1=1\cdot x^{0}=1 $$ which holds true Induction step $$ \frac{d}{dx}x\cdot x^n =_{\text{ind. hyp}} nx^{n-1}:\forall n \in \mathbb{Z}_+ $$ $$\frac{d}{dx}x^{n+1}=(n+1)x^{n}$$ It is known that product rule holds: $$ \frac{d}{dx}f(x)g(x)=\frac{df}{dx}g(x)+f(x)\frac{dg}{dx} $$ then $$ \frac{d}{dx}x^{n+1}=\frac{d}{dx}x\cdot x^n =(\frac{d}{dx}x)\cdot x^n + x \cdot (\frac{d}{dx}x^n)=x^n+x(nx^{n-1})=x^n+nx^{1-1+1} $$ $$ = x^n+nx^n=(n+1)x^n \tag*{$\square$}  $$ I would like to have some feedback if my solution seems correct and  or not.,Problem Proof following statement with induction: Attempt to solve Base case which holds true Induction step It is known that product rule holds: then I would like to have some feedback if my solution seems correct and  or not., \frac{d}{dx}x ^n=nx^{n-1} : \forall n \in \mathbb{Z}_+   \frac{d}{dx}x^1=1\cdot x^{0}=1   \frac{d}{dx}x\cdot x^n =_{\text{ind. hyp}} nx^{n-1}:\forall n \in \mathbb{Z}_+  \frac{d}{dx}x^{n+1}=(n+1)x^{n}  \frac{d}{dx}f(x)g(x)=\frac{df}{dx}g(x)+f(x)\frac{dg}{dx}   \frac{d}{dx}x^{n+1}=\frac{d}{dx}x\cdot x^n =(\frac{d}{dx}x)\cdot x^n + x \cdot (\frac{d}{dx}x^n)=x^n+x(nx^{n-1})=x^n+nx^{1-1+1}   = x^n+nx^n=(n+1)x^n \tag*{\square}  ,"['derivatives', 'proof-verification', 'induction']"
43,Derivative of a function from real number space to Wasserstein space,Derivative of a function from real number space to Wasserstein space,,"I'm not really having a good background in math, so please correct me if I say something very vague or even wrong. Suppose a function $\rho=f(\theta):\mathbb{R}\to\mathcal{W}_p(\mathbb{R}^d)$ . I would like to calculate a sort of derivative $\tilde{\frac{d}{d\theta}}\rho=f'(\theta)$ , which magnitude corresponds to $$ \left\vert \tilde{\frac{d}{d\theta}}\rho \right\vert =\lim_{\Delta\theta\to0}\frac{\mathcal{W}_p[f(\theta+\Delta\theta),f(\theta)]}{\Delta\theta}. $$ Although, at this point I'm not at all clear about (1)what kind of quantity $\tilde{\frac{d}{d\theta}}\rho$ is, and (2)what kind of norm $\vert\cdot\vert$ we use here. For example a common point-wise derivative $$ \frac{d\rho}{d\theta}(x) = \lim_{\Delta\theta\to0}\frac{\rho(x;\theta+\Delta\theta) - \rho(x;\theta)}{\Delta\theta} $$ with its $L_2$ -norm definitely does not correspond to such quantity. So the question is, is there any concept that corresponds to or similar at all to this kind of derivative?","I'm not really having a good background in math, so please correct me if I say something very vague or even wrong. Suppose a function . I would like to calculate a sort of derivative , which magnitude corresponds to Although, at this point I'm not at all clear about (1)what kind of quantity is, and (2)what kind of norm we use here. For example a common point-wise derivative with its -norm definitely does not correspond to such quantity. So the question is, is there any concept that corresponds to or similar at all to this kind of derivative?","\rho=f(\theta):\mathbb{R}\to\mathcal{W}_p(\mathbb{R}^d) \tilde{\frac{d}{d\theta}}\rho=f'(\theta) 
\left\vert \tilde{\frac{d}{d\theta}}\rho \right\vert
=\lim_{\Delta\theta\to0}\frac{\mathcal{W}_p[f(\theta+\Delta\theta),f(\theta)]}{\Delta\theta}.
 \tilde{\frac{d}{d\theta}}\rho \vert\cdot\vert 
\frac{d\rho}{d\theta}(x) = \lim_{\Delta\theta\to0}\frac{\rho(x;\theta+\Delta\theta) - \rho(x;\theta)}{\Delta\theta}
 L_2","['calculus', 'derivatives', 'differential-geometry', 'optimal-transport']"
44,Legendre functions as solution of differential equation,Legendre functions as solution of differential equation,,"In Abramowitz/Stegun you read that $$Q_s(x) := \int_{0}^\infty  ( x  + \sqrt{x^2-1}  \cosh t )^{-s} dt, \quad x > 1, \Re(s)>0$$ is a solution to the differential equation $$(1-x^2)y''-2xy'+s(s-1)y=0.$$ How do you prove that? I differentiated under the integral sign and wrote down $(1-x^2)y''-2xy'+s(s-1)y$ with $Q_s$ for $y$ but couldn't see how it vanishs. Do you need to use some fancy functional equations of $\cosh$ to obtain the result?","In Abramowitz/Stegun you read that $$Q_s(x) := \int_{0}^\infty  ( x  + \sqrt{x^2-1}  \cosh t )^{-s} dt, \quad x > 1, \Re(s)>0$$ is a solution to the differential equation $$(1-x^2)y''-2xy'+s(s-1)y=0.$$ How do you prove that? I differentiated under the integral sign and wrote down $(1-x^2)y''-2xy'+s(s-1)y$ with $Q_s$ for $y$ but couldn't see how it vanishs. Do you need to use some fancy functional equations of $\cosh$ to obtain the result?",,"['integration', 'ordinary-differential-equations', 'derivatives']"
45,A way of understanding the $dx$,A way of understanding the,dx,"In an attempt to explain the concept of the infinitesimal change, I have defined it as such : Given an interval of size $L$, we could express $L$ as follows :$L=\alpha.dx$ thus, theoretically, $\alpha = \frac{L}{dx}$ could be seen as the total number of $x$s since $dx$ is infinitesimally small. Do you think this to be a conceptually wrong explanation?","In an attempt to explain the concept of the infinitesimal change, I have defined it as such : Given an interval of size $L$, we could express $L$ as follows :$L=\alpha.dx$ thus, theoretically, $\alpha = \frac{L}{dx}$ could be seen as the total number of $x$s since $dx$ is infinitesimally small. Do you think this to be a conceptually wrong explanation?",,['calculus']
46,Question about indexes in sum,Question about indexes in sum,,"Let $k_1=2$ and $k_2=3$ and $\mu_1=1$  and $\mu_2=3$. I would like to evaluate the following sum \begin{align}    S=&\sum_{i=1}^{2}            \sum_{j=1}^{k_i}\\    \times& \sum_{\begin{matrix}    n_1+n_2=k_i-j\\    n_i=0 \end{matrix}}^{}\prod_{\begin{matrix}    l=1\\    l\neq i \end{matrix}}^{2} \binom{k_l+n_l-1}{n_l}\frac {u_l^{k_l}} {(\mu_l-\mu_i)^{k_l+n_l}} \end{align} My question is how we get $n_1$ and $n_2$ and  evaluate the above sum? What is the meaning of this summation and it name where only $k_j$ and $\mu_j$ are known? $$ \sum_{ \begin{matrix}    n_1+n_2+\cdots n_n=k_i-j\\    n_i=0    \end{matrix}}^{} $$ Does any one have work with this sum or have some examples about it? This is independent part, I found the same sum in Leibniz formula which is the derivation of product of $m$ functions given by $$ (v_1v_2v_3\cdots v_m)^n=\sum_{ \begin{matrix}    t_1+t_2+\cdots t_m=n\\    t_1,t_2,\cdots t_m\geq0    \end{matrix}}^{}\frac {n!} {t_1!t_2!\cdots t_m!}v_1^{(t_1)}v_2^{(t_2)}\cdots v_m^{(t_m)} $$ Just how we can found all $t_i$ or $n_i$ and the sum?","Let $k_1=2$ and $k_2=3$ and $\mu_1=1$  and $\mu_2=3$. I would like to evaluate the following sum \begin{align}    S=&\sum_{i=1}^{2}            \sum_{j=1}^{k_i}\\    \times& \sum_{\begin{matrix}    n_1+n_2=k_i-j\\    n_i=0 \end{matrix}}^{}\prod_{\begin{matrix}    l=1\\    l\neq i \end{matrix}}^{2} \binom{k_l+n_l-1}{n_l}\frac {u_l^{k_l}} {(\mu_l-\mu_i)^{k_l+n_l}} \end{align} My question is how we get $n_1$ and $n_2$ and  evaluate the above sum? What is the meaning of this summation and it name where only $k_j$ and $\mu_j$ are known? $$ \sum_{ \begin{matrix}    n_1+n_2+\cdots n_n=k_i-j\\    n_i=0    \end{matrix}}^{} $$ Does any one have work with this sum or have some examples about it? This is independent part, I found the same sum in Leibniz formula which is the derivation of product of $m$ functions given by $$ (v_1v_2v_3\cdots v_m)^n=\sum_{ \begin{matrix}    t_1+t_2+\cdots t_m=n\\    t_1,t_2,\cdots t_m\geq0    \end{matrix}}^{}\frac {n!} {t_1!t_2!\cdots t_m!}v_1^{(t_1)}v_2^{(t_2)}\cdots v_m^{(t_m)} $$ Just how we can found all $t_i$ or $n_i$ and the sum?",,"['derivatives', 'summation']"
47,"Prob. 17, Sec. 6.1, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: Straddle Lemma","Prob. 17, Sec. 6.1, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: Straddle Lemma",,"Here is Prob. 17, Sec. 6.1, in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let $f \colon I \to \mathbb{R}$ be differentiable at $c \in I$ . Establish the Straddle Lemma . Give $\varepsilon > 0$ there exists $\delta (\varepsilon) > 0$ such that if $u, v \in I$ satisfy $c-\delta(\varepsilon)<u\leq c \leq v < c+\delta(\varepsilon)$ , then we have $\left\lvert f(v) - f(u) - (v-u)f^\prime(c) \right\rvert \leq \varepsilon (v-u)$ . [ Hint : The $\delta(\varepsilon)$ is given by Definition 6.1.1. Subtract and add the term $f(c) - c f^\prime(c)$ on the left side and use the Triangle Inequality.] Here is Definition 6.1.1 in Bartle & Sherbert, 4th edition: Let $I \subset \mathbb{R}$ be an interval, let $f \colon I \to \mathbb{R}$ , and let $c \in I$ . We say that a real number $L$ is the derivative of $f$ at $c$ if given any $\varepsilon > 0$ there exists $\delta(\varepsilon)>0$ such that if $x \in I$ satisfies $0 < \lvert x-c \rvert < \delta(\varepsilon)$ , then $$ \tag{1} \left\lvert \frac{ f(x) - f(c) }{ x-c} - L \right\rvert < \varepsilon. $$ In this case we say that $f$ is differentiable at $c$ , and we write $f^\prime(c)$ for $L$ . In other words, the derivative of $f$ at $c$ is given by the limit $$ \tag{2} f^\prime(c) = \lim_{x \to c} \frac{ f(x) - f(c) }{ x-c}  $$ provided this limit exists. (We allow the possibility that $c$ may be the endpoint of the interval.) My Attempt: As $f$ is differentiable at $c \in I$ , so there is a real number $f^\prime(c)$ such that  given any real number $\varepsilon > 0$ we can find a real number $\delta(\varepsilon) > 0$ such that $$ \left\lvert \frac{ f(x) - f(c) }{ x-c} - f^\prime(c) \right\rvert < \varepsilon \tag{1} $$ or $$ \left\lvert \frac{ f(x) - f(c) - (x-c) f^\prime(c)  }{ x-c} \right\rvert < \varepsilon \tag{1'} $$ for all $x \in I$ which satisfy $$ 0 < \lvert x-c\rvert < \delta(\varepsilon), $$ which is equivalent to $$ c-\delta(\varepsilon) < x < c+ \delta(\varepsilon) \ \mbox{ and } \ x \neq c. $$ So if $x \in I$ and $0 < \lvert x-c \rvert < \delta(\varepsilon)$ , then upon multiplying both sides of (1') by $\lvert x-c\rvert$ , we get $$ \left\lvert f(x)-f(c) - (x-c)f^\prime(c) \right\rvert < \varepsilon \lvert x-c \rvert. \tag{2} $$ And for $x=c$ both sides of (2) equal $0$ . Therefore we can conclude that $$ \left\lvert f(x)-f(c) - (x-c)f^\prime(c) \right\rvert \leq \varepsilon \lvert x - c \rvert \tag{2'} $$ for all $x \in I$ for which $c-\delta(\varepsilon) < x < c+\delta(\varepsilon)$ . From (2') we conclude that if $u, v \in I$ and $c-\delta(\varepsilon) < u \leq c \leq v < c + \delta(\varepsilon)$ , then we have $$ \left\lvert f(u)-f(c) - (u-c)f^\prime(c) \right\rvert \leq \varepsilon \lvert u-c \rvert =  \varepsilon ( c-u ) \tag{2*} $$ and also $$ \left\lvert f(v)-f(c) - (v-c)f^\prime(c) \right\rvert \leq \varepsilon \lvert v-c \rvert =  \varepsilon (v-c), \tag{2**} $$ and therefore $$  \begin{align} & \ \ \ \ \left\lvert f(v) - f(u) - (v-u) f^\prime(c) \right\rvert \\  &= \left\lvert f(v) - f(c) + f(c) - f(u) - (v-c + c - u) f^\prime(c) \right\rvert \\ &= \left\lvert \left( f(v)-f(c) - (v-c)f^\prime(c) \right) + \left( f(c) - f(u) - (c-u)f^\prime(c) \right) \right\rvert \\ &\leq \left\lvert f(v)-f(c) - (v-c)f^\prime(c) \right\rvert + \left\lvert f(c) - f(u) - (c-u)f^\prime(c) \right\rvert \\ &\leq \varepsilon ( v-c ) + \varepsilon ( c-u ) \qquad \mbox{[ using (2*) and (2**) above ] } \\ &= \varepsilon (v-u). \end{align} $$ Is this proof good enough? Or, are there any problems in it? Where has this lemma originated? Any applications of this lemma? Some references please.","Here is Prob. 17, Sec. 6.1, in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let be differentiable at . Establish the Straddle Lemma . Give there exists such that if satisfy , then we have . [ Hint : The is given by Definition 6.1.1. Subtract and add the term on the left side and use the Triangle Inequality.] Here is Definition 6.1.1 in Bartle & Sherbert, 4th edition: Let be an interval, let , and let . We say that a real number is the derivative of at if given any there exists such that if satisfies , then In this case we say that is differentiable at , and we write for . In other words, the derivative of at is given by the limit provided this limit exists. (We allow the possibility that may be the endpoint of the interval.) My Attempt: As is differentiable at , so there is a real number such that  given any real number we can find a real number such that or for all which satisfy which is equivalent to So if and , then upon multiplying both sides of (1') by , we get And for both sides of (2) equal . Therefore we can conclude that for all for which . From (2') we conclude that if and , then we have and also and therefore Is this proof good enough? Or, are there any problems in it? Where has this lemma originated? Any applications of this lemma? Some references please.","f \colon I \to \mathbb{R} c \in I \varepsilon > 0 \delta (\varepsilon) > 0 u, v \in I c-\delta(\varepsilon)<u\leq c \leq v < c+\delta(\varepsilon) \left\lvert f(v) - f(u) - (v-u)f^\prime(c) \right\rvert \leq \varepsilon (v-u) \delta(\varepsilon) f(c) - c f^\prime(c) I \subset \mathbb{R} f \colon I \to \mathbb{R} c \in I L f c \varepsilon > 0 \delta(\varepsilon)>0 x \in I 0 < \lvert x-c \rvert < \delta(\varepsilon)  \tag{1} \left\lvert \frac{ f(x) - f(c) }{ x-c} - L \right\rvert < \varepsilon.  f c f^\prime(c) L f c  \tag{2} f^\prime(c) = \lim_{x \to c} \frac{ f(x) - f(c) }{ x-c}   c f c \in I f^\prime(c) \varepsilon > 0 \delta(\varepsilon) > 0  \left\lvert \frac{ f(x) - f(c) }{ x-c} - f^\prime(c) \right\rvert < \varepsilon \tag{1}   \left\lvert \frac{ f(x) - f(c) - (x-c) f^\prime(c)  }{ x-c} \right\rvert < \varepsilon \tag{1'}  x \in I  0 < \lvert x-c\rvert < \delta(\varepsilon),   c-\delta(\varepsilon) < x < c+ \delta(\varepsilon) \ \mbox{ and } \ x \neq c.  x \in I 0 < \lvert x-c \rvert < \delta(\varepsilon) \lvert x-c\rvert  \left\lvert f(x)-f(c) - (x-c)f^\prime(c) \right\rvert < \varepsilon \lvert x-c \rvert. \tag{2}  x=c 0  \left\lvert f(x)-f(c) - (x-c)f^\prime(c) \right\rvert \leq \varepsilon \lvert x - c \rvert \tag{2'}  x \in I c-\delta(\varepsilon) < x < c+\delta(\varepsilon) u, v \in I c-\delta(\varepsilon) < u \leq c \leq v < c + \delta(\varepsilon)  \left\lvert f(u)-f(c) - (u-c)f^\prime(c) \right\rvert \leq \varepsilon \lvert u-c \rvert =  \varepsilon ( c-u ) \tag{2*}   \left\lvert f(v)-f(c) - (v-c)f^\prime(c) \right\rvert \leq \varepsilon \lvert v-c \rvert =  \varepsilon (v-c), \tag{2**}   
\begin{align}
& \ \ \ \ \left\lvert f(v) - f(u) - (v-u) f^\prime(c) \right\rvert \\ 
&= \left\lvert f(v) - f(c) + f(c) - f(u) - (v-c + c - u) f^\prime(c) \right\rvert \\
&= \left\lvert \left( f(v)-f(c) - (v-c)f^\prime(c) \right) + \left( f(c) - f(u) - (c-u)f^\prime(c) \right) \right\rvert \\
&\leq \left\lvert f(v)-f(c) - (v-c)f^\prime(c) \right\rvert + \left\lvert f(c) - f(u) - (c-u)f^\prime(c) \right\rvert \\
&\leq \varepsilon ( v-c ) + \varepsilon ( c-u ) \qquad \mbox{[ using (2*) and (2**) above ] } \\
&= \varepsilon (v-u).
\end{align}
","['real-analysis', 'derivatives', 'inequality', 'solution-verification']"
48,"Find ratio between volume of optimal cuboid bound in ellipse, and volume of the ellipse.","Find ratio between volume of optimal cuboid bound in ellipse, and volume of the ellipse.",,"I am trying to find the 'general' ratio between the cuboid of maximum volume bound in an ellipse, and the volume of that same ellipse. After doing some partial derivatives and optimisation, it seems possible to find the ratio but looks like it would take a LONG time. I have: $$V_{ellipse}=4\pi*abc/3 : \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$ $$V_{cuboid}=8xyz=8xyc\sqrt{1-x^2/a^2-y^2/b^2}$$ (Attained by rearranging the ellipse formula in terms of z, and then substituting.) After several partial derivatives: Optimally;  $$x=\sqrt{\frac{a^2-\frac{a^2y^2}{b^2}}{2}}$$ $$y=\sqrt\frac{b^2-\frac{b^2x^2}{a^2}}{2}$$ Then you'd proceed to sub this into the cuboid equation, and then finally divide by the volume of the ellipse. This clearly would take a long time, the equation would be a huge mess, and it would be a pain to actually calculate. Surely there is an easier way? Maybe using triple integrals or something like that?","I am trying to find the 'general' ratio between the cuboid of maximum volume bound in an ellipse, and the volume of that same ellipse. After doing some partial derivatives and optimisation, it seems possible to find the ratio but looks like it would take a LONG time. I have: $$V_{ellipse}=4\pi*abc/3 : \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$ $$V_{cuboid}=8xyz=8xyc\sqrt{1-x^2/a^2-y^2/b^2}$$ (Attained by rearranging the ellipse formula in terms of z, and then substituting.) After several partial derivatives: Optimally;  $$x=\sqrt{\frac{a^2-\frac{a^2y^2}{b^2}}{2}}$$ $$y=\sqrt\frac{b^2-\frac{b^2x^2}{a^2}}{2}$$ Then you'd proceed to sub this into the cuboid equation, and then finally divide by the volume of the ellipse. This clearly would take a long time, the equation would be a huge mess, and it would be a pain to actually calculate. Surely there is an easier way? Maybe using triple integrals or something like that?",,"['calculus', 'geometry', 'derivatives', 'optimization', 'partial-derivative']"
49,If $f$ is differentiable for $x\neq x_0$ and $\lim_{x\to x_0} f'(x) = c$ then $f'(x_0) = c$,If  is differentiable for  and  then,f x\neq x_0 \lim_{x\to x_0} f'(x) = c f'(x_0) = c,Exercise : Let $f : \mathbb R \to \mathbb R$ and $x_0 \in \mathbb R$. Suppose that $f$ is differentiable for all $x \neq x_0$. If $\lim_{x \to x_0}f'(x) = c \in \mathbb R$ show that $f$ is differentiable at $x_0$ and $f'(x_0) = c$. Attempt : Isn't it pretty straight forward that since $\lim_{x \to x_0} f'(x) = c$ then $f'$ is continuous at $x_0$ and thus differentiablewith $f'(x_0) = c$ ? Does it need some more delicate or rigorous mathematical proof ?,Exercise : Let $f : \mathbb R \to \mathbb R$ and $x_0 \in \mathbb R$. Suppose that $f$ is differentiable for all $x \neq x_0$. If $\lim_{x \to x_0}f'(x) = c \in \mathbb R$ show that $f$ is differentiable at $x_0$ and $f'(x_0) = c$. Attempt : Isn't it pretty straight forward that since $\lim_{x \to x_0} f'(x) = c$ then $f'$ is continuous at $x_0$ and thus differentiablewith $f'(x_0) = c$ ? Does it need some more delicate or rigorous mathematical proof ?,,"['calculus', 'limits', 'derivatives']"
50,Find points of discontinuity of $x(x-1)^{2/3}e^{\sqrt[3]{x}}$,Find points of discontinuity of,x(x-1)^{2/3}e^{\sqrt[3]{x}},"I want to find the points of discontinuity for the following function: $$f(x)=x(x-1)^{2/3}e^{\sqrt[3]{x}}$$ My textbook says it can be derived for every number except $0$ and $1$ which could be points of discontinuity. I have no idea how it reached that conclusion without deriving the function first.  Now, I find the first derivative by using the product rule: $$f'(x)=e^{\sqrt[3]{x}}\left[{\frac {x^{4/3}-x^{1/3}+5x-3}{3(x-1)^{1/3}}}\right]$$ My textbook asks for the domain. I know that $\sqrt[3]{x}$ are defined for any $x$ so I focus on the denominator which should not be equal to $0$, therefore $3(x-1)^{1/3}\neq0\implies x-1\neq0\implies x \neq 1$. Wolfram Alpha says the domain should be $x>1$. Any hints on what I'm doing wrong?","I want to find the points of discontinuity for the following function: $$f(x)=x(x-1)^{2/3}e^{\sqrt[3]{x}}$$ My textbook says it can be derived for every number except $0$ and $1$ which could be points of discontinuity. I have no idea how it reached that conclusion without deriving the function first.  Now, I find the first derivative by using the product rule: $$f'(x)=e^{\sqrt[3]{x}}\left[{\frac {x^{4/3}-x^{1/3}+5x-3}{3(x-1)^{1/3}}}\right]$$ My textbook asks for the domain. I know that $\sqrt[3]{x}$ are defined for any $x$ so I focus on the denominator which should not be equal to $0$, therefore $3(x-1)^{1/3}\neq0\implies x-1\neq0\implies x \neq 1$. Wolfram Alpha says the domain should be $x>1$. Any hints on what I'm doing wrong?",,"['calculus', 'derivatives']"
51,Monotonicity of function at a point,Monotonicity of function at a point,,"The question says : Let   $$f(x)=\begin{cases}  -x^3+\frac{b^3-b^2+b-1}{b^2+3b+2} &:0\le x\lt1\\                                  2x-3 &:1\le x\le3\end{cases}$$.    Find all possible values of b such that f(x)has the smallest value at $x=1$. Since this question was an example question, the solution said, The Limiting value of f(x) from the left of $x=1$ should be either greater or equal to the value of the function at $x=1$. My question is for $x=1$ to have the smallest possible value, shouldn't the limiting value be Less than or equal to the function at $x=1$? The answer is $b\in (-2,-1)\cup (1,+\infty)$","The question says : Let   $$f(x)=\begin{cases}  -x^3+\frac{b^3-b^2+b-1}{b^2+3b+2} &:0\le x\lt1\\                                  2x-3 &:1\le x\le3\end{cases}$$.    Find all possible values of b such that f(x)has the smallest value at $x=1$. Since this question was an example question, the solution said, The Limiting value of f(x) from the left of $x=1$ should be either greater or equal to the value of the function at $x=1$. My question is for $x=1$ to have the smallest possible value, shouldn't the limiting value be Less than or equal to the function at $x=1$? The answer is $b\in (-2,-1)\cup (1,+\infty)$",,"['derivatives', 'maxima-minima']"
52,Application of chain rule for partial derivatives,Application of chain rule for partial derivatives,,"A rectangular metal block above has length y com and a square cross section of side x cm. When the metal block is heated, the area of cross-section A and the length of the metal block increase at a constant rate of $0.3 cm/s$ and $0.2 cm/s$ respectively. Find the rate in which the volume of the metal block, V is increasing when $A= 4cm^2$ and $y=5 cm$ Here’s my workings - $\frac{dV}{dt} = \frac{\partial V}{\partial A} \cdot \frac{dA}{dt} +  \frac{\partial V}{\partial L} \cdot \frac{dL}{dt} $ $ V = y \cdot x^2 $ $A = x^2$ $A= x^2 = 4$ $x=2$ $ \frac{\partial V}{\partial A} = 2xy $ $\frac{\partial V}{\partial L} = x^2 $ Therefore $\frac{dV}{dt} = 2(2)(5) \cdot (0.3) + (2)^2 \cdot (0.2) $ However, my answer is wrong and my mistake was in $ 2(2)(5) \cdot (0.3) $ Why is this so ? Thanks..","A rectangular metal block above has length y com and a square cross section of side x cm. When the metal block is heated, the area of cross-section A and the length of the metal block increase at a constant rate of $0.3 cm/s$ and $0.2 cm/s$ respectively. Find the rate in which the volume of the metal block, V is increasing when $A= 4cm^2$ and $y=5 cm$ Here’s my workings - $\frac{dV}{dt} = \frac{\partial V}{\partial A} \cdot \frac{dA}{dt} +  \frac{\partial V}{\partial L} \cdot \frac{dL}{dt} $ $ V = y \cdot x^2 $ $A = x^2$ $A= x^2 = 4$ $x=2$ $ \frac{\partial V}{\partial A} = 2xy $ $\frac{\partial V}{\partial L} = x^2 $ Therefore $\frac{dV}{dt} = 2(2)(5) \cdot (0.3) + (2)^2 \cdot (0.2) $ However, my answer is wrong and my mistake was in $ 2(2)(5) \cdot (0.3) $ Why is this so ? Thanks..",,"['calculus', 'derivatives', 'partial-derivative']"
53,Question based on a continuous function [closed],Question based on a continuous function [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f(x)$ be a continuous and differentiable function such that $f(-1)=-2,f(0)=-1,f(1)=0,f(2)=1,f(3)=0,f(4)=-1$ and $f(5)=1$.Then what is the minimum number of roots of $g(x)=f'(x)+xf''(x)=0$, in the interval $[-1,5]$. Any hint or clue will be appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f(x)$ be a continuous and differentiable function such that $f(-1)=-2,f(0)=-1,f(1)=0,f(2)=1,f(3)=0,f(4)=-1$ and $f(5)=1$.Then what is the minimum number of roots of $g(x)=f'(x)+xf''(x)=0$, in the interval $[-1,5]$. Any hint or clue will be appreciated.",,"['derivatives', 'continuity']"
54,"Show that $g(x,y) = x^2y^2\log(x^2+y^2), 0$ is differentiable in (0,0)","Show that  is differentiable in (0,0)","g(x,y) = x^2y^2\log(x^2+y^2), 0","I have the following function: $$ g(x,y) = \left\{\begin{array}{ll} x^2y^2\log(x^2+y^2), & (x,y) \neq (0,0) \\          0, & (x,y) = (0,0)  \end{array}\right. $$ I've already calculated the partial derivatives and showed that $g$ is partially differentiable: $$ \lim_{h\rightarrow0} \frac{g(0+h,0) - g(0,0)}{h} = \frac{0-0}{h} \\ \lim_{h\rightarrow0} \frac{g(0,0+h) - g(0,0)}{h} = \frac{0-0}{h} = 0 $$ Now I want to show that $g$ is differentiable in $(x,y)=(0,0)$. This is my try so far: $$ \begin{align} &\lim_{h\rightarrow0} \frac{g(0+h) - g(0,0) - g'(0,0)h}{|h|} \\ &= \lim_{h\rightarrow0} \frac{h_1^2h_2^2 \log(h_1^2+h_2^2)}{\sqrt{h_1^2+h_2^2}} \\ \end{align} $$ But now I don't know how to continue and show that the limit is $0$.","I have the following function: $$ g(x,y) = \left\{\begin{array}{ll} x^2y^2\log(x^2+y^2), & (x,y) \neq (0,0) \\          0, & (x,y) = (0,0)  \end{array}\right. $$ I've already calculated the partial derivatives and showed that $g$ is partially differentiable: $$ \lim_{h\rightarrow0} \frac{g(0+h,0) - g(0,0)}{h} = \frac{0-0}{h} \\ \lim_{h\rightarrow0} \frac{g(0,0+h) - g(0,0)}{h} = \frac{0-0}{h} = 0 $$ Now I want to show that $g$ is differentiable in $(x,y)=(0,0)$. This is my try so far: $$ \begin{align} &\lim_{h\rightarrow0} \frac{g(0+h) - g(0,0) - g'(0,0)h}{|h|} \\ &= \lim_{h\rightarrow0} \frac{h_1^2h_2^2 \log(h_1^2+h_2^2)}{\sqrt{h_1^2+h_2^2}} \\ \end{align} $$ But now I don't know how to continue and show that the limit is $0$.",,"['real-analysis', 'derivatives', 'partial-derivative']"
55,How to differentiate a tensor Frobenius norm?,How to differentiate a tensor Frobenius norm?,,"I am wondering how to calculate $$\nabla_{\mathcal{T}} \|\mathcal{T}-\mathcal{C}\|_F^2$$ where $\mathcal{T}, \mathcal{C} \in \mathbb{R}^{n\times n\times n\times n}$ are $4$ th order tensors. Any help would be appreciated. Thank you.","I am wondering how to calculate $$\nabla_{\mathcal{T}} \|\mathcal{T}-\mathcal{C}\|_F^2$$ where $\mathcal{T}, \mathcal{C} \in \mathbb{R}^{n\times n\times n\times n}$ are $4$ th order tensors. Any help would be appreciated. Thank you.",,"['linear-algebra', 'derivatives', 'tensors', 'matrix-calculus']"
56,convergence of functions and their derivatives in measure leads to relations between the limits?,convergence of functions and their derivatives in measure leads to relations between the limits?,,"I've seen a lot of related questions, but no one ever consider the convergence in measure. Suppose you have a sequence of functions $k_n$ that are $C^1$ on $[0,1]$. Suppose that: $k_n$ converge in measure to a function $k$ $k'_n$ converge in measure to a function $h$ Call $E$ the set of points of differentiability for $k$. Is it true that $k'(x) = h(x)$ almost everywhere in $E$? Notice that in the case $k\in C^1$, we can rephrase the problem as $$ k_m\to 0,\quad k'_m\to h \implies h=0 \quad a.e.? $$ From what I've seen, I expect the existence of a counterexample Thanks to the properties of the convergence in measure, you can consider equivalently the convergence almost everywhere. Moreover, if $k_m'$ are equibounded, then the claim is true thanks to Lebesgue's Dominated Convergence Theorem, so a possible counterexample must have $k_m'$ not equibounded.","I've seen a lot of related questions, but no one ever consider the convergence in measure. Suppose you have a sequence of functions $k_n$ that are $C^1$ on $[0,1]$. Suppose that: $k_n$ converge in measure to a function $k$ $k'_n$ converge in measure to a function $h$ Call $E$ the set of points of differentiability for $k$. Is it true that $k'(x) = h(x)$ almost everywhere in $E$? Notice that in the case $k\in C^1$, we can rephrase the problem as $$ k_m\to 0,\quad k'_m\to h \implies h=0 \quad a.e.? $$ From what I've seen, I expect the existence of a counterexample Thanks to the properties of the convergence in measure, you can consider equivalently the convergence almost everywhere. Moreover, if $k_m'$ are equibounded, then the claim is true thanks to Lebesgue's Dominated Convergence Theorem, so a possible counterexample must have $k_m'$ not equibounded.",,"['derivatives', 'convergence-divergence', 'lebesgue-measure']"
57,Derivative of Modified Bessel function of second kind,Derivative of Modified Bessel function of second kind,,"I have to differentiate the following function with respect to $x \dfrac{dF}{dx}$, where $\alpha$ is constant: $$\\F(x)=\sqrt{4\alpha x}K_v(\sqrt{4\alpha x})$$ I am aware that chain rule of differentiation will apply but what about the Bessel function? Any clue?","I have to differentiate the following function with respect to $x \dfrac{dF}{dx}$, where $\alpha$ is constant: $$\\F(x)=\sqrt{4\alpha x}K_v(\sqrt{4\alpha x})$$ I am aware that chain rule of differentiation will apply but what about the Bessel function? Any clue?",,"['ordinary-differential-equations', 'derivatives', 'partial-derivative', 'bessel-functions']"
58,Linearized Pitot system,Linearized Pitot system,,I am trying to create a linearized model of a compressible pitot tube system with altitude $h$ as the input and velocity as the output. When I take the derivative and try and linearize around a point the derivative does not match (approximately) the delta between two points around it. I have pitot tube velocity equation: $T(h) =$ Temperature $P(h) =$ static pressure $P_o =$ stagnation pressure ------------------------------Constants------------------------------- $P_s = 101$ KPa $T_o = 290$ K $\gamma = 1.4$ $M = 0.0289644$ ----------------------- Governing Equations------------------------ $P = P_se^\frac{h*g*M}{RT}$ $ T = T_o-.0065*h$ $Mach = \sqrt{\frac{2}{(\gamma-1)}\biggr((\frac{P_o}{P(h)})^\frac{\gamma}{\gamma-1}-1\biggr)}$ $a = \sqrt{\gamma RT}$ $v = Mach*a$ My approach was to take the derivative (using chain rule) and add that to the nominal point $dv = \frac{dv}{dT}\frac{dT}{dh} + \frac{dv}{dP}\frac{dP}{dh}$ so that: $v = v_{nom} + dv$ I'm not sure about my method or my derivation using the chain rule.,I am trying to create a linearized model of a compressible pitot tube system with altitude $h$ as the input and velocity as the output. When I take the derivative and try and linearize around a point the derivative does not match (approximately) the delta between two points around it. I have pitot tube velocity equation: $T(h) =$ Temperature $P(h) =$ static pressure $P_o =$ stagnation pressure ------------------------------Constants------------------------------- $P_s = 101$ KPa $T_o = 290$ K $\gamma = 1.4$ $M = 0.0289644$ ----------------------- Governing Equations------------------------ $P = P_se^\frac{h*g*M}{RT}$ $ T = T_o-.0065*h$ $Mach = \sqrt{\frac{2}{(\gamma-1)}\biggr((\frac{P_o}{P(h)})^\frac{\gamma}{\gamma-1}-1\biggr)}$ $a = \sqrt{\gamma RT}$ $v = Mach*a$ My approach was to take the derivative (using chain rule) and add that to the nominal point $dv = \frac{dv}{dT}\frac{dT}{dh} + \frac{dv}{dP}\frac{dP}{dh}$ so that: $v = v_{nom} + dv$ I'm not sure about my method or my derivation using the chain rule.,,"['derivatives', 'linearization']"
59,Derivative of the trace of a product,Derivative of the trace of a product,,"I would like to compute the following. How can I do this? $$\frac{\partial }{\partial X}\text{Tr} \big( X \log X \big)$$ Or, more generally, how to compute derivatives of the following form? $$\frac{\partial }{\partial X}\text{Tr} \big( f(X) g(X) \big)$$ Many thanks!","I would like to compute the following. How can I do this? $$\frac{\partial }{\partial X}\text{Tr} \big( X \log X \big)$$ Or, more generally, how to compute derivatives of the following form? $$\frac{\partial }{\partial X}\text{Tr} \big( f(X) g(X) \big)$$ Many thanks!",,"['matrices', 'derivatives', 'matrix-calculus']"
60,Solving $u = xu_x + u_t$ by method of characteristics,Solving  by method of characteristics,u = xu_x + u_t,"I'm learning the method of characteristics. Suppose we want to find $$u(t,x)$$ such that $$u = xu_x + u_t$$ $$u(0,x) = f(x)$$ By the multivariable chain rule, and putting the PDE again below to compare: $$\frac{du}{dt} = \frac{\partial u}{\partial x}\frac{dx}{dt} + \frac{\partial u}{\partial t}\frac{dt}{dt}$$ $$u = xu_x + u_t$$ By comparsion we get: $$\frac{du}{dt} = u \rightarrow u = K_1e^t$$ $$\frac{dx}{dt} = x \rightarrow x = K_2e^t$$ So, at the path $(t,K_2e^t)$ we have that $u$ is $K_1e^t$. But there is a family of paths $(t,K_2e^t)$ because $K_2$ is undetermined. Let's see the graph for one possible $K_2$: If I knew $K_2$ then I'd have this path in the graph, and then I'd know that on this path, $u(0,K_2) = f(K_2) = K_1$ so the solution for $u$ on the path $(t,K_2e^t)$ should be $u = K_1e^t = f(K_2)e^t$. However this solution is not for every $x$ and every $t$, it's just in the path, that is: $$u(t, K_2e^t) = f(K_2)e^t$$ there's still work needed to generalize it for $u(t,x)$ in general. So the two questions are: how to find $K_2$ first, and how to transform $u(t, K_2e^t) = f(K_2)e^t$ into a solution dependent of $t$ and $x$, that is, $u(x,t)$?","I'm learning the method of characteristics. Suppose we want to find $$u(t,x)$$ such that $$u = xu_x + u_t$$ $$u(0,x) = f(x)$$ By the multivariable chain rule, and putting the PDE again below to compare: $$\frac{du}{dt} = \frac{\partial u}{\partial x}\frac{dx}{dt} + \frac{\partial u}{\partial t}\frac{dt}{dt}$$ $$u = xu_x + u_t$$ By comparsion we get: $$\frac{du}{dt} = u \rightarrow u = K_1e^t$$ $$\frac{dx}{dt} = x \rightarrow x = K_2e^t$$ So, at the path $(t,K_2e^t)$ we have that $u$ is $K_1e^t$. But there is a family of paths $(t,K_2e^t)$ because $K_2$ is undetermined. Let's see the graph for one possible $K_2$: If I knew $K_2$ then I'd have this path in the graph, and then I'd know that on this path, $u(0,K_2) = f(K_2) = K_1$ so the solution for $u$ on the path $(t,K_2e^t)$ should be $u = K_1e^t = f(K_2)e^t$. However this solution is not for every $x$ and every $t$, it's just in the path, that is: $$u(t, K_2e^t) = f(K_2)e^t$$ there's still work needed to generalize it for $u(t,x)$ in general. So the two questions are: how to find $K_2$ first, and how to transform $u(t, K_2e^t) = f(K_2)e^t$ into a solution dependent of $t$ and $x$, that is, $u(x,t)$?",,"['calculus', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
61,Finding maximum using elementary calculus. (Using Derivatives),Finding maximum using elementary calculus. (Using Derivatives),,"I'm reading these set of online notes and it reads as following: $f(x)- P(x)$=$\frac{(x-x_0)(x-x_1)(x-x_2)}{3!}$$f'''c(x)$ where $c(x)$ is some point between the minimum and maximum of the points in ${x, x_0, x_1, x_2}$. Then they say: Let $x_1=x_0+h$, $x_2=x_1+h$. Denote $\phi_2(x)=(x-x_0)(x-x_1)(x-x_2)$ We must compute:If we want a uniform bound for $x_0 ≤ x ≤ x_2$, we must compute $\max\limits_{x_0\leq x\leq x_2}$ $|\phi_2(x)|$ = $\max\limits_{x_0\leq x\leq x_2}$ $|(x − x_0) (x − x_1) (x − x_2)| $. Using Calculus: $\max\limits_{x_0\leq x\leq x_2}$ $|\phi_2(x)|$=$\frac{(2h^3)}{3\sqrt3}$ at $x=x_1 \pm$$\frac{(h)}{\sqrt3}$ . I'm confused by Step 5. How did they determine that the maximum was at $x=x_1 \pm$$\frac{(h)}{\sqrt3}$ .  I tried setting the derivative equal to $0$ but I can't seem to get the same result.  The confusing thing is that you can write $x_0$,$x_1$, $x_2$ in terms of h.  So I'm not sure if they converted $\phi_2(x)$ in terms of $x_1$'s and $h$'s.  Any help would be much appreciated. Sorry for the basic problem, I just really need to know how they derived step 5.  Thank you very much.","I'm reading these set of online notes and it reads as following: $f(x)- P(x)$=$\frac{(x-x_0)(x-x_1)(x-x_2)}{3!}$$f'''c(x)$ where $c(x)$ is some point between the minimum and maximum of the points in ${x, x_0, x_1, x_2}$. Then they say: Let $x_1=x_0+h$, $x_2=x_1+h$. Denote $\phi_2(x)=(x-x_0)(x-x_1)(x-x_2)$ We must compute:If we want a uniform bound for $x_0 ≤ x ≤ x_2$, we must compute $\max\limits_{x_0\leq x\leq x_2}$ $|\phi_2(x)|$ = $\max\limits_{x_0\leq x\leq x_2}$ $|(x − x_0) (x − x_1) (x − x_2)| $. Using Calculus: $\max\limits_{x_0\leq x\leq x_2}$ $|\phi_2(x)|$=$\frac{(2h^3)}{3\sqrt3}$ at $x=x_1 \pm$$\frac{(h)}{\sqrt3}$ . I'm confused by Step 5. How did they determine that the maximum was at $x=x_1 \pm$$\frac{(h)}{\sqrt3}$ .  I tried setting the derivative equal to $0$ but I can't seem to get the same result.  The confusing thing is that you can write $x_0$,$x_1$, $x_2$ in terms of h.  So I'm not sure if they converted $\phi_2(x)$ in terms of $x_1$'s and $h$'s.  Any help would be much appreciated. Sorry for the basic problem, I just really need to know how they derived step 5.  Thank you very much.",,"['calculus', 'sequences-and-series']"
62,Inverse Function Theorem in Immersions.,Inverse Function Theorem in Immersions.,,"Let $\varphi: U \to \mathbb{R}^{n}$ of class $C^{k}$ ($k\geq 1$) in the open $U \subset \mathbb{R}^{m}$. If $a \in U$ is such that $\varphi'(a): \mathbb{R}^{m} \to \mathbb{R}^{n}$ is injective, then there exists a decomposition in direct sum $\mathbb{R}^{n} = \mathbb{R}^{m}\oplus\mathbb{R}^{n-m}$ and an open $V$, with $a \in V$, such that $\varphi(V)$ is the graph of an aplication $f: W \to \mathbb{R}^{n-m}$, of class $C^{k}$ in the open $W \subset \mathbb{R}^{m}$. We can write $C^{k} \ni \varphi: U \to \mathbb{R}^{m+(n-m)}$. As a consequence of the Inverse Function Theorem applied to immersions and by hypothesis about $\varphi$, there is a diffeomorphism ($C^{k}$) $h: Z \to X \times Y$ where $Z \ni \varphi(a)$ is open in $\mathbb{R}^{m+(n-m)}$ and $X \times Y \ni (a,0)$ is open in  $\mathbb{R}^{m}\times \mathbb{R}^{m+(n-m)}$, such that $(h\circ \varphi)(x) = (x,0)$ for all $x \in X$ and $h$ is strongly differentiable in $\varphi(a)$. Seems intuitive that $``h$ is $f""$ and $``X$ is $W""$, but I couldn't develop more than this. A detail that confused me is: the dimmensions of $X \times Y$ and $\mathbb{R}^{n-m}$ are not equals. Maybe I'm using this result incorrectly. Can anybody help me? Thanks for the advance!","Let $\varphi: U \to \mathbb{R}^{n}$ of class $C^{k}$ ($k\geq 1$) in the open $U \subset \mathbb{R}^{m}$. If $a \in U$ is such that $\varphi'(a): \mathbb{R}^{m} \to \mathbb{R}^{n}$ is injective, then there exists a decomposition in direct sum $\mathbb{R}^{n} = \mathbb{R}^{m}\oplus\mathbb{R}^{n-m}$ and an open $V$, with $a \in V$, such that $\varphi(V)$ is the graph of an aplication $f: W \to \mathbb{R}^{n-m}$, of class $C^{k}$ in the open $W \subset \mathbb{R}^{m}$. We can write $C^{k} \ni \varphi: U \to \mathbb{R}^{m+(n-m)}$. As a consequence of the Inverse Function Theorem applied to immersions and by hypothesis about $\varphi$, there is a diffeomorphism ($C^{k}$) $h: Z \to X \times Y$ where $Z \ni \varphi(a)$ is open in $\mathbb{R}^{m+(n-m)}$ and $X \times Y \ni (a,0)$ is open in  $\mathbb{R}^{m}\times \mathbb{R}^{m+(n-m)}$, such that $(h\circ \varphi)(x) = (x,0)$ for all $x \in X$ and $h$ is strongly differentiable in $\varphi(a)$. Seems intuitive that $``h$ is $f""$ and $``X$ is $W""$, but I couldn't develop more than this. A detail that confused me is: the dimmensions of $X \times Y$ and $\mathbb{R}^{n-m}$ are not equals. Maybe I'm using this result incorrectly. Can anybody help me? Thanks for the advance!",,"['real-analysis', 'derivatives', 'inverse-function-theorem']"
63,Stokes theorem and integration over fibers,Stokes theorem and integration over fibers,,"I want to understand the following: Let $\pi:X \to Y$ a fiber bundle and $\omega$ a closed smooth differential form. Define $I: Y\to \mathbb C$, $y\mapsto I(y)=\int_{\pi^{-1}(y)} \omega$. Then Stoke's theorem implies $I$ is constant. This situation arises in a text I want to understand. There are more assumptions given (for example $\pi$ proper), but I think they are only needed to arive at this situation. So far I noticed that if $\pi^{-1}(y)=\partial M$, then by Stoke's thm  $$\int_{\pi^{-1}(y)} \omega=\int_M d\omega=0 .$$ So maybe I need to relate this integral with $dI$ or $\frac d {dy} I(y)$, but I don't know how to do that.","I want to understand the following: Let $\pi:X \to Y$ a fiber bundle and $\omega$ a closed smooth differential form. Define $I: Y\to \mathbb C$, $y\mapsto I(y)=\int_{\pi^{-1}(y)} \omega$. Then Stoke's theorem implies $I$ is constant. This situation arises in a text I want to understand. There are more assumptions given (for example $\pi$ proper), but I think they are only needed to arive at this situation. So far I noticed that if $\pi^{-1}(y)=\partial M$, then by Stoke's thm  $$\int_{\pi^{-1}(y)} \omega=\int_M d\omega=0 .$$ So maybe I need to relate this integral with $dI$ or $\frac d {dy} I(y)$, but I don't know how to do that.",,"['integration', 'derivatives', 'differential-geometry', 'fiber-bundles', 'stokes-theorem']"
64,"Prove that $f (y) \geq f(x) + f'(x)(y-x)$ for all $x, y \in (a,b)$.",Prove that  for all .,"f (y) \geq f(x) + f'(x)(y-x) x, y \in (a,b)","Let $f$ be differentiable and convex on $(a,b)$. Prove that $f(y) \geq f(x) + f'(x)(y-x)$ for all $x, y \in  (a, b)$ by using the definition of the derivative. I proved as follows: $$ f'(x) = \lim_{y \to x} \frac{f(y)-f(x)}{y-x}.$$ If $y> x$, $$f'(x) \leq \frac{f(y)-f(x)}{y-x}.$$ So, $f(y) \geq f'(x)(y-x) + f(x)$. But, my professor said I was wrong. Can someone explain to me where I am wrong?","Let $f$ be differentiable and convex on $(a,b)$. Prove that $f(y) \geq f(x) + f'(x)(y-x)$ for all $x, y \in  (a, b)$ by using the definition of the derivative. I proved as follows: $$ f'(x) = \lim_{y \to x} \frac{f(y)-f(x)}{y-x}.$$ If $y> x$, $$f'(x) \leq \frac{f(y)-f(x)}{y-x}.$$ So, $f(y) \geq f'(x)(y-x) + f(x)$. But, my professor said I was wrong. Can someone explain to me where I am wrong?",,"['analysis', 'derivatives', 'convex-analysis']"
65,Verification of proof on Bounded Variation.,Verification of proof on Bounded Variation.,,"Let $f:[-1,1]\rightarrow \mathbb R$ be a function given by $f(x) =  \begin{cases} x^2\cos(\frac{1}{x}),  & \text{if $x\neq 0$ } \\ 0, &  \text{if $x=0$ } \end{cases}$then $(a)$ $f$ is of bounded variation on $[-1,1]$. $(b)$ $f'$ is of bounded variation on $[-1,1]$. $(c)$ $\vert f(x) \vert\le1 \forall x\in [-1,1]$ is of bounded   variation on $[-1,1]$. $(d)$ $\vert f(x) \vert\le3 \forall x\in [-1,1]$ is of bounded   variation on $[-1,1]$. Solution:$f'(x)=2x\cos(\frac{1}{x})-\sin(\frac{1}{x})\implies \vert f'(x)\vert\le3$ on $[1,1]\implies f$ is of bounded variation on $[-1,1]$.So,option $(a),(b)$ are correct. Please check the following argument: Since, the only problem creator for $f'$ being of bounded variation is $0$. Now, if I take a partition of $[-1,1]$ which contains $0$ as one of its points, then the limit of $f'$ as $x$ approaches zero does not exist, but the variation remains bounded, hence $f'$ is of bounded variation on $[-1,1]$. Please provide some argument which can discard option $c$","Let $f:[-1,1]\rightarrow \mathbb R$ be a function given by $f(x) =  \begin{cases} x^2\cos(\frac{1}{x}),  & \text{if $x\neq 0$ } \\ 0, &  \text{if $x=0$ } \end{cases}$then $(a)$ $f$ is of bounded variation on $[-1,1]$. $(b)$ $f'$ is of bounded variation on $[-1,1]$. $(c)$ $\vert f(x) \vert\le1 \forall x\in [-1,1]$ is of bounded   variation on $[-1,1]$. $(d)$ $\vert f(x) \vert\le3 \forall x\in [-1,1]$ is of bounded   variation on $[-1,1]$. Solution:$f'(x)=2x\cos(\frac{1}{x})-\sin(\frac{1}{x})\implies \vert f'(x)\vert\le3$ on $[1,1]\implies f$ is of bounded variation on $[-1,1]$.So,option $(a),(b)$ are correct. Please check the following argument: Since, the only problem creator for $f'$ being of bounded variation is $0$. Now, if I take a partition of $[-1,1]$ which contains $0$ as one of its points, then the limit of $f'$ as $x$ approaches zero does not exist, but the variation remains bounded, hence $f'$ is of bounded variation on $[-1,1]$. Please provide some argument which can discard option $c$",,"['real-analysis', 'derivatives', 'proof-verification', 'bounded-variation', 'interval-arithmetic']"
66,"Given $f(x,y)=5+2x+4y+x^2+y^2+(x^2y^4)^\frac15$, show that it is differentiable at $(0,0)$.","Given , show that it is differentiable at .","f(x,y)=5+2x+4y+x^2+y^2+(x^2y^4)^\frac15 (0,0)","I was given the function: $f(x,y)=5+2x+4y+x^2+y^2+(x^2y^4)^\frac15$ I need to show it is differentiable at $(0,0)$. I started using the method of differentials and infinitesimal functions: $\Delta f=f(0+\Delta x,0+\Delta y)-f(0,0)$ $\Delta f=2\Delta x + 4 \Delta y + (\Delta x)^2 + (\Delta y)^2 + ((\Delta x)^2(\Delta y)^4)^\frac15$ Now I cannot see a way how I can obtain infinitesimal functions* $\alpha$ and $\beta$ such that: $\Delta f=2\Delta x + 4 \Delta y + \alpha\Delta x + \beta \Delta y$, where $\alpha,\beta=o(\rho)$ (Where $\rho^2 = x^2+y^2$, and $o$ is the Landau little-$o$ notation defined as $\alpha=o(\rho)\iff\lim \alpha/\rho=0$). I managed to get 3 ""infinitesimal functions"": $\alpha=\Delta x$, $\beta=\Delta y$, and $\gamma=(\Delta x^2\Delta y^{-1})^\frac15$ but I am having trouble showing $\gamma=o(\rho)$ because frankly it doesn't seem to be. Any hints? If this is not possible, are there any other methods that one would recommend? *My prof defines infinitesimal functions as: $\alpha(x)$ is infinitesimal at $a$ if $\lim_{x\rightarrow a}\alpha(x)=0$","I was given the function: $f(x,y)=5+2x+4y+x^2+y^2+(x^2y^4)^\frac15$ I need to show it is differentiable at $(0,0)$. I started using the method of differentials and infinitesimal functions: $\Delta f=f(0+\Delta x,0+\Delta y)-f(0,0)$ $\Delta f=2\Delta x + 4 \Delta y + (\Delta x)^2 + (\Delta y)^2 + ((\Delta x)^2(\Delta y)^4)^\frac15$ Now I cannot see a way how I can obtain infinitesimal functions* $\alpha$ and $\beta$ such that: $\Delta f=2\Delta x + 4 \Delta y + \alpha\Delta x + \beta \Delta y$, where $\alpha,\beta=o(\rho)$ (Where $\rho^2 = x^2+y^2$, and $o$ is the Landau little-$o$ notation defined as $\alpha=o(\rho)\iff\lim \alpha/\rho=0$). I managed to get 3 ""infinitesimal functions"": $\alpha=\Delta x$, $\beta=\Delta y$, and $\gamma=(\Delta x^2\Delta y^{-1})^\frac15$ but I am having trouble showing $\gamma=o(\rho)$ because frankly it doesn't seem to be. Any hints? If this is not possible, are there any other methods that one would recommend? *My prof defines infinitesimal functions as: $\alpha(x)$ is infinitesimal at $a$ if $\lim_{x\rightarrow a}\alpha(x)=0$",,"['real-analysis', 'analysis', 'derivatives', 'infinitesimals']"
67,Partial derivative of tensor with respect to tensor,Partial derivative of tensor with respect to tensor,,"My question is related to continuum mechanics, taking partial derivative of tensor with respect to tensor. $$\mathbf{\sigma} = \lambda \hspace{1pt} tr(\mathbf{\epsilon})+ 2\mu\mathbf{\epsilon}$$ Where, $\mathbf{\sigma,\epsilon}$ are second order tensors, $tr(\mathbf{\epsilon})$ is trace of the tensor. I want to find $$\frac{\partial \mathbf{\sigma}}{\partial\mathbf{\epsilon}}$$ I start like this: $$ \mathbb{C_{ijmn}}=\frac{\partial {\sigma_{ij}}}{\partial{\epsilon_{mn}}} = \frac{\partial}{\partial \epsilon_{mn}} \big(\lambda\delta_{ij}\epsilon_{kk} + 2\mu\epsilon_{ij} \big)$$ $$=\lambda\delta_{ij}\frac{\partial}{\partial \epsilon_{mn}}\epsilon_{kk} + 2\mu\frac{\partial}{\partial \epsilon_{mn}}\epsilon_{ij}$$ $$=\lambda\delta_{ij}\delta_{km}\delta_{kn} + 2\mu\delta_{im}\delta_{jn}$$ $$=\lambda\delta_{ij}\delta_{mn}+2\mu\delta_{im}\delta_{jn}$$ Is this correct?","My question is related to continuum mechanics, taking partial derivative of tensor with respect to tensor. $$\mathbf{\sigma} = \lambda \hspace{1pt} tr(\mathbf{\epsilon})+ 2\mu\mathbf{\epsilon}$$ Where, $\mathbf{\sigma,\epsilon}$ are second order tensors, $tr(\mathbf{\epsilon})$ is trace of the tensor. I want to find $$\frac{\partial \mathbf{\sigma}}{\partial\mathbf{\epsilon}}$$ I start like this: $$ \mathbb{C_{ijmn}}=\frac{\partial {\sigma_{ij}}}{\partial{\epsilon_{mn}}} = \frac{\partial}{\partial \epsilon_{mn}} \big(\lambda\delta_{ij}\epsilon_{kk} + 2\mu\epsilon_{ij} \big)$$ $$=\lambda\delta_{ij}\frac{\partial}{\partial \epsilon_{mn}}\epsilon_{kk} + 2\mu\frac{\partial}{\partial \epsilon_{mn}}\epsilon_{ij}$$ $$=\lambda\delta_{ij}\delta_{km}\delta_{kn} + 2\mu\delta_{im}\delta_{jn}$$ $$=\lambda\delta_{ij}\delta_{mn}+2\mu\delta_{im}\delta_{jn}$$ Is this correct?",,"['derivatives', 'tensor-products', 'tensors']"
68,$f''(e^{x})$ and it's definition,and it's definition,f''(e^{x}),"I'm working through ""The Calculus Tutoring Book"" by Carol and Robert Ash (0-7803-1044-6).  In chapter 3.3, when discussing derivatives of basic functions, they show and define the $D_{x}e^{x}$ and subsequently define $e$.  Page 67 for those that have the book. In any event, they define a ""base"", eventually, they land at $(1)$: $$D_{x}b^x = \lim_{\Delta x\to 0} b^x\lbrack \frac{b^{\Delta x}-1}{\Delta x} \rbrack$$  I have no problem here.  And I understand why they effectively ignore the constant $b^x$ when you factor it out $(2)$: $$\frac{b^{\Delta x}-1}{\Delta x}$$ to $(3)$ $$D_{x}b^x=mb^x$$ And go on to state that the latter must be the slope of a line centered at $(0, 1)$ Then from the former, they land on the fact the $(4)$ $$D_{x}e^x = e^x$$ I don't see the path from $(2)$ to $(3)$ and subsequently $(4)$.","I'm working through ""The Calculus Tutoring Book"" by Carol and Robert Ash (0-7803-1044-6).  In chapter 3.3, when discussing derivatives of basic functions, they show and define the $D_{x}e^{x}$ and subsequently define $e$.  Page 67 for those that have the book. In any event, they define a ""base"", eventually, they land at $(1)$: $$D_{x}b^x = \lim_{\Delta x\to 0} b^x\lbrack \frac{b^{\Delta x}-1}{\Delta x} \rbrack$$  I have no problem here.  And I understand why they effectively ignore the constant $b^x$ when you factor it out $(2)$: $$\frac{b^{\Delta x}-1}{\Delta x}$$ to $(3)$ $$D_{x}b^x=mb^x$$ And go on to state that the latter must be the slope of a line centered at $(0, 1)$ Then from the former, they land on the fact the $(4)$ $$D_{x}e^x = e^x$$ I don't see the path from $(2)$ to $(3)$ and subsequently $(4)$.",,"['derivatives', 'exponential-function', 'definition']"
69,NURBS: Advice for specifying the first derivative at the endpoints of a C2 cubic spline interpolation.,NURBS: Advice for specifying the first derivative at the endpoints of a C2 cubic spline interpolation.,,"I am working through The NURBS Book by Piegl and Tiller.  In the book they give an efficient algorithm for computing the traditional $C^2$ cubic spline for global interpolation of arbitrary data points.  However, this algorithm requires that the user specify the first derivative (unit direction and magnitude) at each endpoint. Obviously this choice can significantly affect the way the curve turns out looking.  Is there any ""recommended"" choice for these first derivatives if we have no prior knowledge of what the data looks like that we are trying to interpolate?  I.e. is there a best choice for a very general interpolating algorithm?","I am working through The NURBS Book by Piegl and Tiller.  In the book they give an efficient algorithm for computing the traditional $C^2$ cubic spline for global interpolation of arbitrary data points.  However, this algorithm requires that the user specify the first derivative (unit direction and magnitude) at each endpoint. Obviously this choice can significantly affect the way the curve turns out looking.  Is there any ""recommended"" choice for these first derivatives if we have no prior knowledge of what the data looks like that we are trying to interpolate?  I.e. is there a best choice for a very general interpolating algorithm?",,"['derivatives', 'interpolation', 'spline']"
70,partial differentiability implies differentiability?,partial differentiability implies differentiability?,,"Let $f(x_1, x_2, \ldots x_n)$ be a real-valued function in $\mathbb{R}^n$, $n \in \mathbb{N}$. Assume that for any integer $i \in [1,n]$, and for any collection of real values of $(x_j)_{j \neq i}$, the function  $$ F(x_i) = f(x_1, \ldots, x_i, \ldots x_n) $$ is differentiable in $\mathbb{R}$ (which means, we keep the coordinates $x_j$ for $j \neq i$ fixed and we consider $f$ as a function only of $x_i$). Does it necessarily mean that $f(x_1, \ldots, x_n)$ is differentiable in $\mathbb{R}^n$?","Let $f(x_1, x_2, \ldots x_n)$ be a real-valued function in $\mathbb{R}^n$, $n \in \mathbb{N}$. Assume that for any integer $i \in [1,n]$, and for any collection of real values of $(x_j)_{j \neq i}$, the function  $$ F(x_i) = f(x_1, \ldots, x_i, \ldots x_n) $$ is differentiable in $\mathbb{R}$ (which means, we keep the coordinates $x_j$ for $j \neq i$ fixed and we consider $f$ as a function only of $x_i$). Does it necessarily mean that $f(x_1, \ldots, x_n)$ is differentiable in $\mathbb{R}^n$?",,"['real-analysis', 'complex-analysis', 'ordinary-differential-equations', 'derivatives', 'partial-derivative']"
71,What is the derivative of absolute value of a complex number? [closed],What is the derivative of absolute value of a complex number? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Consider a function $\mathbb{C} \rightarrow \mathbb{R}$, $f(z) = |z|^2$. What is the first order derivative of $f(z)$? And what is the gradient of $f(z)$? I found in the The Matrix Cookbook , it said that the  gradient of $f(z)$ is $2z$. Is there any difference between gradient and first order derivative?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Consider a function $\mathbb{C} \rightarrow \mathbb{R}$, $f(z) = |z|^2$. What is the first order derivative of $f(z)$? And what is the gradient of $f(z)$? I found in the The Matrix Cookbook , it said that the  gradient of $f(z)$ is $2z$. Is there any difference between gradient and first order derivative?",,"['complex-analysis', 'derivatives', 'complex-numbers']"
72,From concavity to second derivative,From concavity to second derivative,,"Let $f(x)$ be continuous and differentiable on $\mathbb{R}$. I want to show that if $f(x)$ is concave, then $f''(x)\le0$ $\forall\space x\in\mathbb{R}$ Proof: Let $x,y$ be fixed in $\mathbb{R}$ such that $y>x$ Without loss of generality, since $f$ is concave, then $f'(x)\ge\cfrac{f(y)-f(x)}{y-x}$ by the calculus criterion. By the mean value theorem, $\exists\space c\in[x,y]$ such that $f'(c)=\cfrac{f(y)-f(x)}{y-x}$ So I now have: $f'(x)\ge f'(c)$ Since $x$ was arbitrarily chosen, then we have shown $a\ge b\implies f'(b)\ge f'(a)$ Since $y>x$, then we have $f'(x)\ge f'(y)$ and this is true for all $x,y\in\mathbb{R}$ So $f''(x)\le 0$ Is my approach correct?","Let $f(x)$ be continuous and differentiable on $\mathbb{R}$. I want to show that if $f(x)$ is concave, then $f''(x)\le0$ $\forall\space x\in\mathbb{R}$ Proof: Let $x,y$ be fixed in $\mathbb{R}$ such that $y>x$ Without loss of generality, since $f$ is concave, then $f'(x)\ge\cfrac{f(y)-f(x)}{y-x}$ by the calculus criterion. By the mean value theorem, $\exists\space c\in[x,y]$ such that $f'(c)=\cfrac{f(y)-f(x)}{y-x}$ So I now have: $f'(x)\ge f'(c)$ Since $x$ was arbitrarily chosen, then we have shown $a\ge b\implies f'(b)\ge f'(a)$ Since $y>x$, then we have $f'(x)\ge f'(y)$ and this is true for all $x,y\in\mathbb{R}$ So $f''(x)\le 0$ Is my approach correct?",,"['calculus', 'derivatives']"
73,Determine order of approximation for the finite difference method,Determine order of approximation for the finite difference method,,I try to solve the following task. It is given a finite difference:         $$g(x)=f'(x)\approx\frac{f(x+h)-f(x)}{h}$$         $$g'(x)\approx\frac{g(x)-g(x-h)}{h}\approx\frac{\frac{f(x+h)-f(x)}{h}\;-\;\frac{f(x)-f(x-h)}{h}}{h}$$         $$\iff g'(x)\approx\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\approx f''(x)$$ I shall determine the order of approximation of $f''$ depending on Big-O. The order of approximation should say something about the quality of the approximation (size of the possible error?). There wasn't much explanation in the lecture so I am not sure. But they want us to give the order of approximation depending on $\mathcal(O)$ (Landau symbol). (Definition of Big-O is clear) On our frames is only a barely explained example in which something with Taylor Expansions is done. Still without further explanations it is hard for me to understand what I have to do... I really hope for your help - please explain very detailed because I am a computer scientist and not a mathematician and could probably missing some more in detail information about certain subjects.,I try to solve the following task. It is given a finite difference:         $$g(x)=f'(x)\approx\frac{f(x+h)-f(x)}{h}$$         $$g'(x)\approx\frac{g(x)-g(x-h)}{h}\approx\frac{\frac{f(x+h)-f(x)}{h}\;-\;\frac{f(x)-f(x-h)}{h}}{h}$$         $$\iff g'(x)\approx\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\approx f''(x)$$ I shall determine the order of approximation of $f''$ depending on Big-O. The order of approximation should say something about the quality of the approximation (size of the possible error?). There wasn't much explanation in the lecture so I am not sure. But they want us to give the order of approximation depending on $\mathcal(O)$ (Landau symbol). (Definition of Big-O is clear) On our frames is only a barely explained example in which something with Taylor Expansions is done. Still without further explanations it is hard for me to understand what I have to do... I really hope for your help - please explain very detailed because I am a computer scientist and not a mathematician and could probably missing some more in detail information about certain subjects.,,"['derivatives', 'approximation', 'computational-complexity', 'finite-differences']"
74,Use induction to show that a polynomial with $k$ terms has $k$ solutions.,Use induction to show that a polynomial with  terms has  solutions.,k k,"First Part of the problem: Let $n_1 < . . . < n_k$ be non-negative integers, let $a_1, . . . , a_k$ be positive real numbers and let $f(x) = a_1x^ {n_1} + · · · + a_kx^{n_k}$ . Suppose $0 < s < t$ and $f(s) = f(t) = 0$. Show that the polynomial $g(x) = a_1 + a_2x^{n_2−n_1} + · · · + a_kx^{n_k−n_1} = \frac{1}{x^{n_1}}f(x)$ satisfies $g'(x) = 0$ for some $x$ between $s$ and $t$. (Hint: Use Rolle’s Theorem). My proof: We know that polynomials are continuous and differentiable, $\therefore$ we use Rolle's Theorem, since $f(s) = f(t)$, $\exists c \in (s,t)$ with $f'(c) = 0$. Now we take the derivative of $g(x)$ using the product rule: $g'(x) = (\frac{1}{x^n})'f(x) + (\frac{1}{x^n})f'(x)=$ $-\frac{n}{x^{-(1+n_1)}} + (\frac{1}{x^n})f'(x)$ From Rolle's theorem we know there is an x $\in (s,t)$ such that $f'(x) = 0$ and if $n_1=0$ then $g'(x) = 0$ for that same $x$. Second Part of the problem:  Use induction on $k$ and Question 7 to show that the polynomial $f(x) = a_1x^{n_1} + · · · + a_kx^{n_k}$ has at most $k$ positive solutions to $f(x) = 0$. My attempt: Quite honestly I'm not sure how to start. I know I can use the base case $k = 1$ (and there is only one solution to that: $x = 0$), so I can assume for $k$ terms there are $k$ solutions. But I don't know where to go from there. Do I just keep taking derivatives and prove $k-1$? How can I use induction to prove $k+1$?","First Part of the problem: Let $n_1 < . . . < n_k$ be non-negative integers, let $a_1, . . . , a_k$ be positive real numbers and let $f(x) = a_1x^ {n_1} + · · · + a_kx^{n_k}$ . Suppose $0 < s < t$ and $f(s) = f(t) = 0$. Show that the polynomial $g(x) = a_1 + a_2x^{n_2−n_1} + · · · + a_kx^{n_k−n_1} = \frac{1}{x^{n_1}}f(x)$ satisfies $g'(x) = 0$ for some $x$ between $s$ and $t$. (Hint: Use Rolle’s Theorem). My proof: We know that polynomials are continuous and differentiable, $\therefore$ we use Rolle's Theorem, since $f(s) = f(t)$, $\exists c \in (s,t)$ with $f'(c) = 0$. Now we take the derivative of $g(x)$ using the product rule: $g'(x) = (\frac{1}{x^n})'f(x) + (\frac{1}{x^n})f'(x)=$ $-\frac{n}{x^{-(1+n_1)}} + (\frac{1}{x^n})f'(x)$ From Rolle's theorem we know there is an x $\in (s,t)$ such that $f'(x) = 0$ and if $n_1=0$ then $g'(x) = 0$ for that same $x$. Second Part of the problem:  Use induction on $k$ and Question 7 to show that the polynomial $f(x) = a_1x^{n_1} + · · · + a_kx^{n_k}$ has at most $k$ positive solutions to $f(x) = 0$. My attempt: Quite honestly I'm not sure how to start. I know I can use the base case $k = 1$ (and there is only one solution to that: $x = 0$), so I can assume for $k$ terms there are $k$ solutions. But I don't know where to go from there. Do I just keep taking derivatives and prove $k-1$? How can I use induction to prove $k+1$?",,"['real-analysis', 'derivatives', 'induction', 'rolles-theorem']"
75,what is the best approach to show that F is complex differentiable?,what is the best approach to show that F is complex differentiable?,,"Suppose that f is differentiable on an open set G and $z_0 ∈ G$. Let $F(z) = \frac{f(z)-f(z_0)}{z-z_0},$ $(z \neq z_0), F(z_0) = f ′ (z_0)$ Prove that F is differentiable on G. I wanted to use an argument which relied on the definition of differentiability to show that $F'(z)=lim_{h \to 0}\frac{F(z+h)-F(z)}{h}$. But apparently you can't use arguments like that here , because then it would imply that it is true for the reals also. Now I'm thinking that if f is differentiable it has a taylor series so if I can show that F(z) has a taylor series then this will imply that F is differentiable. Is this the right approach ? and if not is- there any suggestions as to what approach\proofs might prove more fruitful ?","Suppose that f is differentiable on an open set G and $z_0 ∈ G$. Let $F(z) = \frac{f(z)-f(z_0)}{z-z_0},$ $(z \neq z_0), F(z_0) = f ′ (z_0)$ Prove that F is differentiable on G. I wanted to use an argument which relied on the definition of differentiability to show that $F'(z)=lim_{h \to 0}\frac{F(z+h)-F(z)}{h}$. But apparently you can't use arguments like that here , because then it would imply that it is true for the reals also. Now I'm thinking that if f is differentiable it has a taylor series so if I can show that F(z) has a taylor series then this will imply that F is differentiable. Is this the right approach ? and if not is- there any suggestions as to what approach\proofs might prove more fruitful ?",,"['complex-analysis', 'derivatives', 'proof-verification']"
76,Simplifying integral,Simplifying integral,,"There is a problem : $\int x \ln(x^2)\,dx = \int 2x \ln x\,dx$ How can I simplify the integral like in the right? I tried to use substitution $x^2=u$, $2x=\frac{du}{dx}$ $\int x \ln(x^2)\,dx= \int\frac{u}{2} \ln u\, du$, so its not the same? Can someone point out what is wrong with my attempt?","There is a problem : $\int x \ln(x^2)\,dx = \int 2x \ln x\,dx$ How can I simplify the integral like in the right? I tried to use substitution $x^2=u$, $2x=\frac{du}{dx}$ $\int x \ln(x^2)\,dx= \int\frac{u}{2} \ln u\, du$, so its not the same? Can someone point out what is wrong with my attempt?",,"['integration', 'derivatives']"
77,Differentiability of $\cos{|x|}$ and $\sin{|x|}$ at $x=0$,Differentiability of  and  at,\cos{|x|} \sin{|x|} x=0,"Define differentiability of  $\cos{|x|}$ and $\sin{|x|}$ at $x=0$ It is said that $\cos|x|$ is continuous and $\sin|x|$ is discontinuous at $x=0$. $$ Lf'(0)=\lim_{h\to 0^-}\frac{\cos|0+h|-\cos|0|}{h}=\lim_{h\to{0}}\frac{\cos h-1}{h}=Rf'(0) $$ Thus $\cos|x|$ is continuous. Fine, but applying chain rule, let $|x|=t$ $$ \frac{d}{dx}\cos|x|\bigg|_{x=0}=\frac{d}{dx}\cos t\bigg|_{x=0}=\frac{d(\cos t)}{dt}.\frac{dt}{dx}\bigg|_{x=0}=-\sin t.\frac{dt}{dx}\bigg|_{x=0}=-\sin t.\frac{d|x|}{dx}\bigg|_{x=0} $$ As $|x|$ is not differentiable at $x$=$0$, how can we define the derivative of $\cos|x|$ at $x=0$ ?","Define differentiability of  $\cos{|x|}$ and $\sin{|x|}$ at $x=0$ It is said that $\cos|x|$ is continuous and $\sin|x|$ is discontinuous at $x=0$. $$ Lf'(0)=\lim_{h\to 0^-}\frac{\cos|0+h|-\cos|0|}{h}=\lim_{h\to{0}}\frac{\cos h-1}{h}=Rf'(0) $$ Thus $\cos|x|$ is continuous. Fine, but applying chain rule, let $|x|=t$ $$ \frac{d}{dx}\cos|x|\bigg|_{x=0}=\frac{d}{dx}\cos t\bigg|_{x=0}=\frac{d(\cos t)}{dt}.\frac{dt}{dx}\bigg|_{x=0}=-\sin t.\frac{dt}{dx}\bigg|_{x=0}=-\sin t.\frac{d|x|}{dx}\bigg|_{x=0} $$ As $|x|$ is not differentiable at $x$=$0$, how can we define the derivative of $\cos|x|$ at $x=0$ ?",,"['calculus', 'derivatives']"
78,"Prove properties of $\frac{\ln x}{x}$, and use them to show $(\sin x)^{\cos x} > (\cos x)^{\sin x} $","Prove properties of , and use them to show",\frac{\ln x}{x} (\sin x)^{\cos x} > (\cos x)^{\sin x} ,"Given $$f(x)={\ln x \over x}, x>0$$ I) Find the monotony of $f.$ ΙΙ) Calculate the following integral: $$\int_1^2{\ln x \over x}{dx}$$ IIΙ) Find the domain of $f$ and, then, show that the equation $$3f(x)=1$$ has got exactly two positive roots $.$ IV) If $x_1,x_2 (x_1<x_2)$ the roots of question II . Show that exists $ξ\in(x_1,x_2)$ such as that $$3f(ξ)+3ξf'(ξ)=1.$$ V) Solve at $(0,{π\over 2})$ the inequality $$(\sin x)^{\cos x} > (\cos x)^{\sin x}.$$ Personal work: I) $f$ is increasing at $(0,e]$ and decreasing at $[e,+\infty)$ . Also, $e$ is the global maximum of $f$ . $f(e)={1\over e}$ II) Let $u=\ln x$ hence $du=\frac {dx}{x}\iff xdu=dx$ The integral changes to $$\int_0^{\ln2} udu=\frac {u^2}{2}=\frac {(\ln 2)^2}{2}$$ III) The domain of $f$ is: $(-\infty,{1 \over e}]\cup[e,+\infty)=[e,+\infty)$ I've tried solving for $f(x)$ so this is what I got: $$3f(x)=1\iff f(x)={1\over 3}\iff{\ln x\over x}={1\over 3}\iff 3\ln x=x\iff\ln x={x\over 3}\iff e^{\ln x}=e^{x\over 3}\iff x=e^{x\over 3}.$$ And then I could get to nowhere. Since all the questions are linked to each other, if question III remains unsolved, thus, questions IV and V cannot be solved. IV) I've thought of using Rolle's theorem since all the conditions are met. I chose Rolle over Bolzano because the equation has a derivative in it. Also, another idea would be that I find the anti-derivative of $$3f(ξ)+3ξf'(ξ)$$ and then let that be a function $g(x)$ and apply either Bolzano's theorem or Rolle's theorem. V) I really have no idea about connecting $f(x)$ with either part of the inequality.","Given I) Find the monotony of ΙΙ) Calculate the following integral: IIΙ) Find the domain of and, then, show that the equation has got exactly two positive roots IV) If the roots of question II . Show that exists such as that V) Solve at the inequality Personal work: I) is increasing at and decreasing at . Also, is the global maximum of . II) Let hence The integral changes to III) The domain of is: I've tried solving for so this is what I got: And then I could get to nowhere. Since all the questions are linked to each other, if question III remains unsolved, thus, questions IV and V cannot be solved. IV) I've thought of using Rolle's theorem since all the conditions are met. I chose Rolle over Bolzano because the equation has a derivative in it. Also, another idea would be that I find the anti-derivative of and then let that be a function and apply either Bolzano's theorem or Rolle's theorem. V) I really have no idea about connecting with either part of the inequality.","f(x)={\ln x \over x}, x>0 f. \int_1^2{\ln x \over x}{dx} f 3f(x)=1 . x_1,x_2 (x_1<x_2) ξ\in(x_1,x_2) 3f(ξ)+3ξf'(ξ)=1. (0,{π\over 2}) (\sin x)^{\cos x} > (\cos x)^{\sin x}. f (0,e] [e,+\infty) e f f(e)={1\over e} u=\ln x du=\frac {dx}{x}\iff xdu=dx \int_0^{\ln2} udu=\frac {u^2}{2}=\frac {(\ln 2)^2}{2} f (-\infty,{1 \over e}]\cup[e,+\infty)=[e,+\infty) f(x) 3f(x)=1\iff f(x)={1\over 3}\iff{\ln x\over x}={1\over 3}\iff 3\ln x=x\iff\ln x={x\over 3}\iff e^{\ln x}=e^{x\over 3}\iff x=e^{x\over 3}. 3f(ξ)+3ξf'(ξ) g(x) f(x)","['calculus', 'real-analysis', 'derivatives']"
79,Differentiable everywhere,Differentiable everywhere,,"So I want to show that $f(z)=|z|$ where $z$ is a complex number is not differentiable anywhere, and that $g(z)=|z|^2$ is differentiable at $z=0$ only.  Now with some computation, I got: $\lim_{h\rightarrow 0}\frac{|z+h|-|z|}{h}=\frac{\partial}{\partial x}\sqrt{x^2+y^2}=\frac{x}{\sqrt{x^2+y^2}}$ and $\lim_{ih\rightarrow 0}\frac{|z+ih|-|z|}{ih}=\frac{1}{i}\frac{\partial}{\partial y}\sqrt{x^2+y^2}=\frac{y}{i\sqrt{x^2+y^2}}$. Thus they are not equal for $z\ne x+iy.$ Now what I do not get it is, this result would imply what I have to prove in the question, but why would this mean that this $f(z)$ is not differentiable anywhere? Similary, for $g(z)$, the computation I get for $\lim_{h\rightarrow 0}\frac{|z+h|^{2}-|z|^2}{h}=2x$ and $\lim_{ih\rightarrow 0}\frac{|z+ih|^{2}-|z|^2}{ih}=\frac{2y}{i}$, this is not equal unless $z=0$. The conclusion once again is $g$ is not differentiable unless at $z=0.$ Questions 1) Why is $f$ not differentiable anywhere? Is it because the only possible place where it is differentiable is if $z=0?$, but that would imply the denominator is $0 $ and hence undefined? 2) Why do we have to have the condition in which the partials have to be equal in order for the function to be differentiable at some given domain? I would appreciate some clear explanation. Thank You.","So I want to show that $f(z)=|z|$ where $z$ is a complex number is not differentiable anywhere, and that $g(z)=|z|^2$ is differentiable at $z=0$ only.  Now with some computation, I got: $\lim_{h\rightarrow 0}\frac{|z+h|-|z|}{h}=\frac{\partial}{\partial x}\sqrt{x^2+y^2}=\frac{x}{\sqrt{x^2+y^2}}$ and $\lim_{ih\rightarrow 0}\frac{|z+ih|-|z|}{ih}=\frac{1}{i}\frac{\partial}{\partial y}\sqrt{x^2+y^2}=\frac{y}{i\sqrt{x^2+y^2}}$. Thus they are not equal for $z\ne x+iy.$ Now what I do not get it is, this result would imply what I have to prove in the question, but why would this mean that this $f(z)$ is not differentiable anywhere? Similary, for $g(z)$, the computation I get for $\lim_{h\rightarrow 0}\frac{|z+h|^{2}-|z|^2}{h}=2x$ and $\lim_{ih\rightarrow 0}\frac{|z+ih|^{2}-|z|^2}{ih}=\frac{2y}{i}$, this is not equal unless $z=0$. The conclusion once again is $g$ is not differentiable unless at $z=0.$ Questions 1) Why is $f$ not differentiable anywhere? Is it because the only possible place where it is differentiable is if $z=0?$, but that would imply the denominator is $0 $ and hence undefined? 2) Why do we have to have the condition in which the partials have to be equal in order for the function to be differentiable at some given domain? I would appreciate some clear explanation. Thank You.",,"['complex-analysis', 'derivatives']"
80,Suppose the function $\sqrt{x}$ is continuous at any $c>0$. Show that this function is differentiable at any $c>0$ and find $f'(c)$.,Suppose the function  is continuous at any . Show that this function is differentiable at any  and find .,\sqrt{x} c>0 c>0 f'(c),Suppose the function $\sqrt{x}$ is continuous at any $c>0$. Show that this function is differentiable at any $c>0$ and find $f'(c)$. I know that if a function is continuous any any $c>0$ then for any sequence $x_n \rightarrow c$ we have that $f(x_n) \rightarrow f(c)$ Now I want to show that $\frac{f(x_n)-f(c)}{x_n-c} \rightarrow f'(c)$ So suppose that the function is continuous then let $x_n \rightarrow c$ then we know that $f(x_n) \rightarrow f(c)$ now I can't see how to connect this to what I want to show. Can I go with trying to evaluate what the limit will be? such as $\lim_{x \rightarrow c} \frac{\sqrt(x_n)-\sqrt(c)}{x_n-c}$ then multiplying by the conjugate $\sqrt(x_n) + \sqrt(c)$ I end up with $\lim_{x \rightarrow c} \frac{1}{\sqrt{x_n}+\sqrt(c)}=\frac{1}{2\sqrt(c)}$,Suppose the function $\sqrt{x}$ is continuous at any $c>0$. Show that this function is differentiable at any $c>0$ and find $f'(c)$. I know that if a function is continuous any any $c>0$ then for any sequence $x_n \rightarrow c$ we have that $f(x_n) \rightarrow f(c)$ Now I want to show that $\frac{f(x_n)-f(c)}{x_n-c} \rightarrow f'(c)$ So suppose that the function is continuous then let $x_n \rightarrow c$ then we know that $f(x_n) \rightarrow f(c)$ now I can't see how to connect this to what I want to show. Can I go with trying to evaluate what the limit will be? such as $\lim_{x \rightarrow c} \frac{\sqrt(x_n)-\sqrt(c)}{x_n-c}$ then multiplying by the conjugate $\sqrt(x_n) + \sqrt(c)$ I end up with $\lim_{x \rightarrow c} \frac{1}{\sqrt{x_n}+\sqrt(c)}=\frac{1}{2\sqrt(c)}$,,"['derivatives', 'continuity']"
81,Use the chain rule to find the formula for $g^\prime (f(c))$,Use the chain rule to find the formula for,g^\prime (f(c)),"Let $I_1,I_2$ be intervals. Let $f:I_1\rightarrow I_2$ be a bijective function, and  $g:I_2\rightarrow I_1$ be the inverse. Suppose that both $f$ is differentiable at $c\in I_1$ and $f^\prime (c) \not = 0$ and $g$ is differentiable at $f(c)$. Use the chain rule to find a formula for $g^\prime (f(c))$ (in terms of $f^\prime (c)).$ Attempt:  $g \circ f = g(f(x))=h(x)$. Then, $h(x)$ is differentiable at $c$. $h^\prime (c)= g^\prime (f(c))f^\prime (c) \rightarrow g^\prime (f(c))= \frac {h^\prime (c)}{f^\prime (c)}$. I have to express $h^\prime (c)$ in terms of $f^\prime (c)$, but don't know how to do it. Could you give some hint for this? Thank you in advance!","Let $I_1,I_2$ be intervals. Let $f:I_1\rightarrow I_2$ be a bijective function, and  $g:I_2\rightarrow I_1$ be the inverse. Suppose that both $f$ is differentiable at $c\in I_1$ and $f^\prime (c) \not = 0$ and $g$ is differentiable at $f(c)$. Use the chain rule to find a formula for $g^\prime (f(c))$ (in terms of $f^\prime (c)).$ Attempt:  $g \circ f = g(f(x))=h(x)$. Then, $h(x)$ is differentiable at $c$. $h^\prime (c)= g^\prime (f(c))f^\prime (c) \rightarrow g^\prime (f(c))= \frac {h^\prime (c)}{f^\prime (c)}$. I have to express $h^\prime (c)$ in terms of $f^\prime (c)$, but don't know how to do it. Could you give some hint for this? Thank you in advance!",,"['real-analysis', 'derivatives']"
82,Product rule for quaternions,Product rule for quaternions,,"The following exercise is from Naive Lie Theory by Stillwell, and is designed (I assume) to illustrate how non-commutativity of quaternions affects the product rule. The definition of derivative for any function $c(t)$ of a real variable $t$ is  $$c'(t) = \lim_{\Delta t \to 0} \frac{c(t + \Delta t) - c(t)}{\Delta t}.$$ (1) By imitating the usual proof of the product rule, show that if $c(t) = a(t)b(t)$ then $$c'(t) = a'(t)b(t) + a(t)b'(t).$$ (Do not assume the product rule is  commutative.) (2) Show also that if $c(t) = a(t)^{-1}$, and $a(0) = 1$, then $c'(0)= -a'(0),$ again without assuming that the product is commutative. (3) Show, however, that if $c(t)=a(t)^2$ then $c'(t)$ is not equal to $2a(t)a'(t)$ for a certain quaternionic-valued function $a(t)$. Parts (1) and (2) are straightforward, but I just cannot get the certain quaternionic-valued function for part (3). Does anyone know what it is? For the function $a(t)$ I've tried: $it, jt, (i+j)t, ijt, e^jt$ and a whole bunch of others without success. If anyone knows it, i'd be grateful, thanks.","The following exercise is from Naive Lie Theory by Stillwell, and is designed (I assume) to illustrate how non-commutativity of quaternions affects the product rule. The definition of derivative for any function $c(t)$ of a real variable $t$ is  $$c'(t) = \lim_{\Delta t \to 0} \frac{c(t + \Delta t) - c(t)}{\Delta t}.$$ (1) By imitating the usual proof of the product rule, show that if $c(t) = a(t)b(t)$ then $$c'(t) = a'(t)b(t) + a(t)b'(t).$$ (Do not assume the product rule is  commutative.) (2) Show also that if $c(t) = a(t)^{-1}$, and $a(0) = 1$, then $c'(0)= -a'(0),$ again without assuming that the product is commutative. (3) Show, however, that if $c(t)=a(t)^2$ then $c'(t)$ is not equal to $2a(t)a'(t)$ for a certain quaternionic-valued function $a(t)$. Parts (1) and (2) are straightforward, but I just cannot get the certain quaternionic-valued function for part (3). Does anyone know what it is? For the function $a(t)$ I've tried: $it, jt, (i+j)t, ijt, e^jt$ and a whole bunch of others without success. If anyone knows it, i'd be grateful, thanks.",,"['derivatives', 'lie-groups', 'quaternions']"
83,noncompact difference,noncompact difference,,The Taylor series can be used to approximate a derivative to a desired amount of error. For instance $\displaystyle \frac{du}{dx} = \frac{u_{i+1}-u_{i-1}}{2\Delta x} + O((\Delta x)^2)$ I have a problem where it asks for a 4th order noncompact difference. Can someone please explain what noncompact means in this context and how it differs from a compact scheme? Thanks,The Taylor series can be used to approximate a derivative to a desired amount of error. For instance $\displaystyle \frac{du}{dx} = \frac{u_{i+1}-u_{i-1}}{2\Delta x} + O((\Delta x)^2)$ I have a problem where it asks for a 4th order noncompact difference. Can someone please explain what noncompact means in this context and how it differs from a compact scheme? Thanks,,"['ordinary-differential-equations', 'derivatives', 'taylor-expansion', 'approximation', 'finite-differences']"
84,Implicit Differentiation Coordinates at dy/dx = 0,Implicit Differentiation Coordinates at dy/dx = 0,,"With the equation $x^2+2xy-3y^2+16=0$, I need to find the coordinates of the points on the curve where $\frac{dy}{dx} = 0$. I think I have correctly used implicit differentiation to get $\frac{dy}{dx} = \frac{-x-y}{x-3y}$. And the multiplied by the denominator and subsituted zero to get, $0 = -x-y$, however, I'm not sure where to go from here. If anyone could help that would be great. Thanks.","With the equation $x^2+2xy-3y^2+16=0$, I need to find the coordinates of the points on the curve where $\frac{dy}{dx} = 0$. I think I have correctly used implicit differentiation to get $\frac{dy}{dx} = \frac{-x-y}{x-3y}$. And the multiplied by the denominator and subsituted zero to get, $0 = -x-y$, however, I'm not sure where to go from here. If anyone could help that would be great. Thanks.",,"['calculus', 'derivatives', 'implicit-differentiation']"
85,Derivative of log $x_i$ inside a $\log \sum$ of $x$,Derivative of log  inside a  of,x_i \log \sum x,"I'm trying to differentiate the following: $\frac{\delta}{\delta \log x_m} \log \sum_{m=1}^M x_m^k \, f(y_m)$ So we only care about a particular index of $x$, and $k$ is a constant. I'm stumped at how to take the derivative of $\log x_m$. Since I can't push the $\log$ inside the sum, how can I even obtain $\log x_m$? I thought maybe the chain rule could apply here, but it's not obvious to me how that would work in this instance either.","I'm trying to differentiate the following: $\frac{\delta}{\delta \log x_m} \log \sum_{m=1}^M x_m^k \, f(y_m)$ So we only care about a particular index of $x$, and $k$ is a constant. I'm stumped at how to take the derivative of $\log x_m$. Since I can't push the $\log$ inside the sum, how can I even obtain $\log x_m$? I thought maybe the chain rule could apply here, but it's not obvious to me how that would work in this instance either.",,"['calculus', 'derivatives', 'logarithms', 'chain-rule']"
86,Fundamental theorem of calculus with 2-variables integrand,Fundamental theorem of calculus with 2-variables integrand,,"I have an equation in the form $$\int_x^\infty f(x,y) \, dy = \int_x^\infty g(y) \, dy$$ from which I would like to derive a relation between the functions $f$ and $g$. If I take the partial derivative w.r. to $x$ of both sides, I get, using the fundamental theorem of calculus, $$\partial_x \left( \int_x^\infty f(x,y) \, dy\right) = -g(x)$$ However, I don't know what to do with the first term. Is there anything I can conclude about it without having to find an analytical expression for the integral?","I have an equation in the form $$\int_x^\infty f(x,y) \, dy = \int_x^\infty g(y) \, dy$$ from which I would like to derive a relation between the functions $f$ and $g$. If I take the partial derivative w.r. to $x$ of both sides, I get, using the fundamental theorem of calculus, $$\partial_x \left( \int_x^\infty f(x,y) \, dy\right) = -g(x)$$ However, I don't know what to do with the first term. Is there anything I can conclude about it without having to find an analytical expression for the integral?",,"['calculus', 'integration', 'derivatives', 'improper-integrals', 'partial-derivative']"
87,"Relation between the derivative of a function and the ""change"" in the function","Relation between the derivative of a function and the ""change"" in the function",,"Suppose we have the function $$M(t)=ce^{rt}$$ where $c$ is a constant. Trivially, it follows that $\frac{dM_t}{dt}=r\cdot c e^{rt}=rM_t$, hence we can write this in the form $$\frac{dM_t}{M_t}=rdt$$ I'm trying to derive the same equation by looking at the change $dM_t$ I have  $$dM_t=M_{t+dt}-M_t=ce^{r(t+dt)}-ce^{rt}=M_t(e^{rdt}-1)$$ Hence, I'd expect to have $$e^{rdt}-1=rdt$$ which makes no sense to me. As far as I know, the derivative with respect to a variable is the change when all the other variables are held fixed. Can someone explain to me where's the issue and in general, how to relate the change to the derivative especially in the case when the function depends on more than $1$ variable?","Suppose we have the function $$M(t)=ce^{rt}$$ where $c$ is a constant. Trivially, it follows that $\frac{dM_t}{dt}=r\cdot c e^{rt}=rM_t$, hence we can write this in the form $$\frac{dM_t}{M_t}=rdt$$ I'm trying to derive the same equation by looking at the change $dM_t$ I have  $$dM_t=M_{t+dt}-M_t=ce^{r(t+dt)}-ce^{rt}=M_t(e^{rdt}-1)$$ Hence, I'd expect to have $$e^{rdt}-1=rdt$$ which makes no sense to me. As far as I know, the derivative with respect to a variable is the change when all the other variables are held fixed. Can someone explain to me where's the issue and in general, how to relate the change to the derivative especially in the case when the function depends on more than $1$ variable?",,"['derivatives', 'partial-derivative']"
88,Calculating the points of minimum pressure on the surface of a cylinder,Calculating the points of minimum pressure on the surface of a cylinder,,"I was wondering if anyone could help me with the following problem, as I'm unsure on how to begin.  The problem is the following. Two equal line sources of strength $k$ are located at $x=3a$ and $x=-3a$, near a circular cylinder of radius $a$ with axis normal to the $x$-$y$ plane and passing through the origin. The fluid is incompressible and the flow is irrotational and inviscid. I have used  the Milne-Thomson circle theorem to show that the complex potential for this flow is  $$ w(z) = k\ln\left(a^{4}-9a^{2}z^{2}-9\frac{a^{6}}{z^{2}}+81a^{4}\right). $$ I also know that the speed of the flow at the surface of the cylinder is given by $\left|\frac{dw}{dz}\right|$. This was given in the previous part of the question that I am doing:  $$ \left|\frac{dw}{dz}\right| = v = \frac{18k\sin(2\theta)}{a(41-9\cos(2\theta))} $$  However I do not know how to show this. Computing the derivative gives  $$ \frac{dw}{dz}=\frac{2kz}{z^{2}-9a^{2}} - \frac{2kz^{2}a^{4}}{a^{4}z^{3}-9a^{2}z^{3}} $$  If anyone could show me how to simplify this expression to the required one i would be extremely grateful. My main problem is how to determine the positions of the points on the surface of the cylinder at which the pressure is minimum? Edit: The problem about computing the flow speed on the cylinder has been answered here: How can I show the following trigonometric function?","I was wondering if anyone could help me with the following problem, as I'm unsure on how to begin.  The problem is the following. Two equal line sources of strength $k$ are located at $x=3a$ and $x=-3a$, near a circular cylinder of radius $a$ with axis normal to the $x$-$y$ plane and passing through the origin. The fluid is incompressible and the flow is irrotational and inviscid. I have used  the Milne-Thomson circle theorem to show that the complex potential for this flow is  $$ w(z) = k\ln\left(a^{4}-9a^{2}z^{2}-9\frac{a^{6}}{z^{2}}+81a^{4}\right). $$ I also know that the speed of the flow at the surface of the cylinder is given by $\left|\frac{dw}{dz}\right|$. This was given in the previous part of the question that I am doing:  $$ \left|\frac{dw}{dz}\right| = v = \frac{18k\sin(2\theta)}{a(41-9\cos(2\theta))} $$  However I do not know how to show this. Computing the derivative gives  $$ \frac{dw}{dz}=\frac{2kz}{z^{2}-9a^{2}} - \frac{2kz^{2}a^{4}}{a^{4}z^{3}-9a^{2}z^{3}} $$  If anyone could show me how to simplify this expression to the required one i would be extremely grateful. My main problem is how to determine the positions of the points on the surface of the cylinder at which the pressure is minimum? Edit: The problem about computing the flow speed on the cylinder has been answered here: How can I show the following trigonometric function?",,"['derivatives', 'mathematical-physics', 'fluid-dynamics']"
89,differentiability - neighborhood,differentiability - neighborhood,,"Could someone let me know if I'm on the right track? And if not give me a hint or let me know what doesn't make sense? Thanks in advance!! Suppose $f\colon \mathbb{R}\ \to \mathbb{R} $ is differentiable at   $x_{0}$ $\in$ $(a,b)$ and $f'(x_{0})>0$. Prove $\exists$ a   neighborhood $I$ of $x_{0}$ s.t. $\forall x \in I$ we have $x<x_{0}  \implies f(x_{})<f(x_{0})$ and $x>x_{0} \implies f(x_{})>f(x_{0})$. This is what I was thinking: Suppose there doesn't exist such an interval $I$. Then $\forall n \in \mathbb{N}$ we can find an $x_{n} \in (x_{0}-\frac{1}{n},x_{0}+\frac{1}{n})$ s.t. if $x_{n}<x_{0} \implies f(x_{n})\ge f(x_{0})$ and if $ x_{n}>x_{0} \implies f(x_{n})\le f(x_{0})$. This defines a sequence $\{x_{n}\}$, where $x_{n} \ne x_{0} \space \forall n \in \mathbb{N}$, with $\{x_{n}\} \to x_{0}$. Choose $\{x_{n_{k}}\}$ to be the subsequence of $\{x_{n}\}$ s.t. $x_{n_{k}} \lt x_{0} \space\forall n_{k}\in \mathbb{N}$. Note $\{x_{n_{k}}\} \to x_{0} $. We know $f$ is differentiable at $x_{0}$. So we have that $\{\frac{f(x_{n_{k}})-f(x_{0})}{x_{n_{k}}-x_{0}}\}$ $\le 0$ so that $f'(x_{0})$ $\le 0$. Contradiction - since by hypothesis $f'(x_{0})$ $\gt 0$. Thus, there does exist such an interval. Edit: Also, I don't understand where the interval $(a,b)$ comes to play. Why are we given it? We are given the domain of $f$ is $\mathbb{R}$ so I feel like its redundant information?","Could someone let me know if I'm on the right track? And if not give me a hint or let me know what doesn't make sense? Thanks in advance!! Suppose $f\colon \mathbb{R}\ \to \mathbb{R} $ is differentiable at   $x_{0}$ $\in$ $(a,b)$ and $f'(x_{0})>0$. Prove $\exists$ a   neighborhood $I$ of $x_{0}$ s.t. $\forall x \in I$ we have $x<x_{0}  \implies f(x_{})<f(x_{0})$ and $x>x_{0} \implies f(x_{})>f(x_{0})$. This is what I was thinking: Suppose there doesn't exist such an interval $I$. Then $\forall n \in \mathbb{N}$ we can find an $x_{n} \in (x_{0}-\frac{1}{n},x_{0}+\frac{1}{n})$ s.t. if $x_{n}<x_{0} \implies f(x_{n})\ge f(x_{0})$ and if $ x_{n}>x_{0} \implies f(x_{n})\le f(x_{0})$. This defines a sequence $\{x_{n}\}$, where $x_{n} \ne x_{0} \space \forall n \in \mathbb{N}$, with $\{x_{n}\} \to x_{0}$. Choose $\{x_{n_{k}}\}$ to be the subsequence of $\{x_{n}\}$ s.t. $x_{n_{k}} \lt x_{0} \space\forall n_{k}\in \mathbb{N}$. Note $\{x_{n_{k}}\} \to x_{0} $. We know $f$ is differentiable at $x_{0}$. So we have that $\{\frac{f(x_{n_{k}})-f(x_{0})}{x_{n_{k}}-x_{0}}\}$ $\le 0$ so that $f'(x_{0})$ $\le 0$. Contradiction - since by hypothesis $f'(x_{0})$ $\gt 0$. Thus, there does exist such an interval. Edit: Also, I don't understand where the interval $(a,b)$ comes to play. Why are we given it? We are given the domain of $f$ is $\mathbb{R}$ so I feel like its redundant information?",,"['calculus', 'real-analysis', 'derivatives']"
90,Existence of another critical point,Existence of another critical point,,"I would like some input in my proof on this question: Let $f:\mathbb{R} \to \mathbb{R}$ smooth, with local minimum at $x=0$, and suppose that this minimum it is not global. Show that there is another critical point besides $x=0$. My answer: Suppose $x=0$ is not a global minimum, hence, $\exists x_{1} \in \mathbb{R}:f(x_{1}) \lt f(0).$ Without loss of generality, suppose $x_{1} \lt 0$. By the Mean Value Theorem, $\exists c \in (x_{1},0):f'(c)=\frac{f(0)-f(x_{1})}{x_{1}} \gt 0$. But $x=0$ is local minimum, therefore, $\exists \delta \gt 0:y \in (-\delta,\delta) \implies f(y) \geq f(0)$. Consider the closed interval $[-\delta,0]$. By the Mean Value Theorem, $\exists k \in (-\delta,0): f'(k)=\frac{f(0)-f(-\delta)}{\delta} \lt 0$. Since $f$ is smooth, it is continuous in $\mathbb{R}$. Therefore, we have $c,k \in (x_{1},0)$ and $f'(c) \gt 0$ and $f'(k) \lt 0$, hence, $\exists z \in (c,k): f'(z) =0$ and $z$ is a critical point of $f$.","I would like some input in my proof on this question: Let $f:\mathbb{R} \to \mathbb{R}$ smooth, with local minimum at $x=0$, and suppose that this minimum it is not global. Show that there is another critical point besides $x=0$. My answer: Suppose $x=0$ is not a global minimum, hence, $\exists x_{1} \in \mathbb{R}:f(x_{1}) \lt f(0).$ Without loss of generality, suppose $x_{1} \lt 0$. By the Mean Value Theorem, $\exists c \in (x_{1},0):f'(c)=\frac{f(0)-f(x_{1})}{x_{1}} \gt 0$. But $x=0$ is local minimum, therefore, $\exists \delta \gt 0:y \in (-\delta,\delta) \implies f(y) \geq f(0)$. Consider the closed interval $[-\delta,0]$. By the Mean Value Theorem, $\exists k \in (-\delta,0): f'(k)=\frac{f(0)-f(-\delta)}{\delta} \lt 0$. Since $f$ is smooth, it is continuous in $\mathbb{R}$. Therefore, we have $c,k \in (x_{1},0)$ and $f'(c) \gt 0$ and $f'(k) \lt 0$, hence, $\exists z \in (c,k): f'(z) =0$ and $z$ is a critical point of $f$.",,"['real-analysis', 'derivatives', 'proof-verification', 'continuity']"
91,Determining if a function described by a Taylor series has a relative extremum at a point,Determining if a function described by a Taylor series has a relative extremum at a point,,"I have a very easy question from the 2004 BC2 (Form B) AP Calculus exam. The question is: $f$ is a function with derivatives of all orders for all real numbers. The third-degree Taylor polynomial for $f$ about $x=2$ is given by    $$T(x)=7-9(x-2)^2-3(x-2)^3$$   b) [...] Determine whether $f(2)$ is a relative maximum, minimum, or neither, and justify your answer. To solve this, I used the second derivative test for relative extremum: Since $f'(2)=T'(2)=2$ and $f''(0)=T''(2)=-18<0$, $f$ has a relative maximum at $x=2$. I was thinking, though, what if I used the first derivative test for relative extremum instead? x         2      <–––––––|–––––––> T'(x)    +   0   – Since the first 3 derivatives of $f(x)$ equal those of $T(x)$ around $x=2$, and $T'(x)$ changes from positive to negative at $x=2$, $f$ has a relative maximum there. Now first of all, of course the second method is longer. However, I think it may also be incorrect, or at least require further justification. I do not think, though, that further justification is needed since $f$ does indeed equal $T$ for those first three derivatives at $x=2$ by definition. Of course, that last statement of mine may very well be wrong. So, my question is, what is wrong with the second method for determining that $f$ has a relative maximum at $x=2$?","I have a very easy question from the 2004 BC2 (Form B) AP Calculus exam. The question is: $f$ is a function with derivatives of all orders for all real numbers. The third-degree Taylor polynomial for $f$ about $x=2$ is given by    $$T(x)=7-9(x-2)^2-3(x-2)^3$$   b) [...] Determine whether $f(2)$ is a relative maximum, minimum, or neither, and justify your answer. To solve this, I used the second derivative test for relative extremum: Since $f'(2)=T'(2)=2$ and $f''(0)=T''(2)=-18<0$, $f$ has a relative maximum at $x=2$. I was thinking, though, what if I used the first derivative test for relative extremum instead? x         2      <–––––––|–––––––> T'(x)    +   0   – Since the first 3 derivatives of $f(x)$ equal those of $T(x)$ around $x=2$, and $T'(x)$ changes from positive to negative at $x=2$, $f$ has a relative maximum there. Now first of all, of course the second method is longer. However, I think it may also be incorrect, or at least require further justification. I do not think, though, that further justification is needed since $f$ does indeed equal $T$ for those first three derivatives at $x=2$ by definition. Of course, that last statement of mine may very well be wrong. So, my question is, what is wrong with the second method for determining that $f$ has a relative maximum at $x=2$?",,"['calculus', 'derivatives', 'taylor-expansion']"
92,Derivative product rule $\frac{d}{dx}\sqrt{xe^{-3x}}$,Derivative product rule,\frac{d}{dx}\sqrt{xe^{-3x}},"I have this question in one of my textbooks and no matter what approach I take to solving it the answer is always wrong $\frac{d}{dx}\sqrt{xe^{-3x}}$ It would be greatly appreciated if someone could explain it to me. ps, I'm new to math stack exchange, so please tell me if am asking the right kind of questions. Thanks :) The textbook says the answer is: $\frac{e^{-3x}}{2{\sqrt{x}}}-3{\sqrt{xe^{-3x}}}$","I have this question in one of my textbooks and no matter what approach I take to solving it the answer is always wrong $\frac{d}{dx}\sqrt{xe^{-3x}}$ It would be greatly appreciated if someone could explain it to me. ps, I'm new to math stack exchange, so please tell me if am asking the right kind of questions. Thanks :) The textbook says the answer is: $\frac{e^{-3x}}{2{\sqrt{x}}}-3{\sqrt{xe^{-3x}}}$",,"['calculus', 'derivatives']"
93,Can solutions that we get from a direction field of a Linear ODE cross each other,Can solutions that we get from a direction field of a Linear ODE cross each other,,"Question: When we sketch the direction fields of a given Linear ODE, can they cross each other ? Reasoning: Let we have a linear ODE of the form $$y'(x) = f(x,y(x))$$ This means that for a given $(x,y)$, $f(x,y)$ have a definite value, hence at that point, the derivative of any function that satisfies the linear ODE is that unique value $f(x,y)$. Therefore, if at some point we have more than one solution, the solutions cannot cross each (they can meet, but not cross) other because that would violate the uniqness of the derivative at that point as I have argued. Now, I have asked the same question to one of my professors by giving the above argument, but he said that ""we don't know the form of $f(x,y)$, so we cannot sure such a thing"", however, my argument directly eliminates his argument in my view, but I want to make sure that the above argument is a valid argument and my conclusion is correct.","Question: When we sketch the direction fields of a given Linear ODE, can they cross each other ? Reasoning: Let we have a linear ODE of the form $$y'(x) = f(x,y(x))$$ This means that for a given $(x,y)$, $f(x,y)$ have a definite value, hence at that point, the derivative of any function that satisfies the linear ODE is that unique value $f(x,y)$. Therefore, if at some point we have more than one solution, the solutions cannot cross each (they can meet, but not cross) other because that would violate the uniqness of the derivative at that point as I have argued. Now, I have asked the same question to one of my professors by giving the above argument, but he said that ""we don't know the form of $f(x,y)$, so we cannot sure such a thing"", however, my argument directly eliminates his argument in my view, but I want to make sure that the above argument is a valid argument and my conclusion is correct.",,"['ordinary-differential-equations', 'derivatives']"
94,On why $d/dz(\Re(z))$ does not exist,On why  does not exist,d/dz(\Re(z)),"Let $f(z)=\Re(z)\in\mathbb{C}$, $z=x+iy$ and $h=h_x+ih_y$, then $$f'(z)=\lim_{h\to0}\frac{f(z+h)-f(z)}{h}$$ $$= \lim_{h\to0}\frac{f(x+h_x+i(y+h_y))-f(x+iy)}{h}$$ $$= \lim_{h\to0}\frac{\Re(x+h_x+i(y+h_y))-\Re(x+iy)}{h}$$ $$= \lim_{h\to0}\frac{x+h_x-x}{h}$$ $$= \lim_{h\to0}\frac{\Re(h)}{h}$$ now, as we take the limit, it's worth considering how our expression behaves depending on where we take the limit from. If we consider what happens as we take the limit as $h\to0$ along the real axis, we have that $\Re(h)=h$, hence the ratio between $\Re(h)$ and $h$ is $1$, and the limit hence is $1$. However, if we repeat this process, but along the imaginary axis, we find that $\Re(h)=0$, and hence the ratio between $\Re(h)$ and $h$ is $0$, and our limit evaluates to $0$. We notice that already, the limit approaches different values as $h\to0$. My question, then, is whether or not it is therefore safe to say that the limit of the difference quotient does not exist, and that therefore, $f(z)$ is not differentiable, or are there other conditions that we need to check before we jump to such a conclusion?","Let $f(z)=\Re(z)\in\mathbb{C}$, $z=x+iy$ and $h=h_x+ih_y$, then $$f'(z)=\lim_{h\to0}\frac{f(z+h)-f(z)}{h}$$ $$= \lim_{h\to0}\frac{f(x+h_x+i(y+h_y))-f(x+iy)}{h}$$ $$= \lim_{h\to0}\frac{\Re(x+h_x+i(y+h_y))-\Re(x+iy)}{h}$$ $$= \lim_{h\to0}\frac{x+h_x-x}{h}$$ $$= \lim_{h\to0}\frac{\Re(h)}{h}$$ now, as we take the limit, it's worth considering how our expression behaves depending on where we take the limit from. If we consider what happens as we take the limit as $h\to0$ along the real axis, we have that $\Re(h)=h$, hence the ratio between $\Re(h)$ and $h$ is $1$, and the limit hence is $1$. However, if we repeat this process, but along the imaginary axis, we find that $\Re(h)=0$, and hence the ratio between $\Re(h)$ and $h$ is $0$, and our limit evaluates to $0$. We notice that already, the limit approaches different values as $h\to0$. My question, then, is whether or not it is therefore safe to say that the limit of the difference quotient does not exist, and that therefore, $f(z)$ is not differentiable, or are there other conditions that we need to check before we jump to such a conclusion?",,"['complex-analysis', 'limits', 'derivatives']"
95,"prove that exist $c$ at $[a,b]$ ,so that for every $\varepsilon>0$ there is $x$ that $0<|x-c|<\varepsilon$ and $f(x)=0$","prove that exist  at  ,so that for every  there is  that  and","c [a,b] \varepsilon>0 x 0<|x-c|<\varepsilon f(x)=0","Let $f(x)$ Differentiable at $[a,b]$ and have infinity numbers of zero at $[a,b]$ . prove that exist $c$ at $[a,b]$ ,so that for every $\varepsilon>0$ there is $x$ that $0<|x-c|<\varepsilon$ and $f(x)=0$ $f^{(i)}(c)=0$ for every $0\le i$ my answer for the first one : Let $x_n$ sequence with different elements at $[a,b]$ that $f(x_n)=0$ . then i tried to use Bolzano–Weierstrass theorem . so we get that for $x_n$ there is convergent subsequence $a_k\le x_{n_k} \le b_k$ , that  convergent to $c$ when $k\to \infty$ . is that enough to prove that first one ? if not how to continue? for the second : i tried to use Rolle's theorem $f$ Derivative at $[a,b]$ and $f(a)=f(b)=0$ then there is $c$ such that $f'(c)=0$ but how to prove it when $f^{(i)}(c)=0$ ? thanks a lot .","Let Differentiable at and have infinity numbers of zero at . prove that exist at ,so that for every there is that and for every my answer for the first one : Let sequence with different elements at that . then i tried to use Bolzano–Weierstrass theorem . so we get that for there is convergent subsequence , that  convergent to when . is that enough to prove that first one ? if not how to continue? for the second : i tried to use Rolle's theorem Derivative at and then there is such that but how to prove it when ? thanks a lot .","f(x) [a,b] [a,b] c [a,b] \varepsilon>0 x 0<|x-c|<\varepsilon f(x)=0 f^{(i)}(c)=0 0\le i x_n [a,b] f(x_n)=0 x_n a_k\le x_{n_k} \le b_k c k\to \infty f [a,b] f(a)=f(b)=0 c f'(c)=0 f^{(i)}(c)=0","['real-analysis', 'derivatives']"
96,Find all points of contact of horizontal tangents to the curve $y = 2\sqrt x + \frac 1{ \sqrt x}$,Find all points of contact of horizontal tangents to the curve,y = 2\sqrt x + \frac 1{ \sqrt x},Find all points of contact of horizontal tangents to the curve $$y = 2\sqrt x + \frac 1{ \sqrt x}$$ I found the derivative: $$\frac {dy}{dx} = x^{-1/2} - \frac 12x^{-3/2}$$ Which can be simplified down to $$ \frac {dy}{dx} = \frac  1{\sqrt x} - \frac 1 {2x\sqrt x}$$ Then I used the Null Factor Law: $\frac{1}{\sqrt{x}} = 0$ or $\frac{1}{2x \sqrt{x}} = 0$. I got stuck here.,Find all points of contact of horizontal tangents to the curve $$y = 2\sqrt x + \frac 1{ \sqrt x}$$ I found the derivative: $$\frac {dy}{dx} = x^{-1/2} - \frac 12x^{-3/2}$$ Which can be simplified down to $$ \frac {dy}{dx} = \frac  1{\sqrt x} - \frac 1 {2x\sqrt x}$$ Then I used the Null Factor Law: $\frac{1}{\sqrt{x}} = 0$ or $\frac{1}{2x \sqrt{x}} = 0$. I got stuck here.,,"['calculus', 'derivatives', 'differential', 'tangent-line']"
97,Derivative of a diagonal matrix raised to an arbitrary power,Derivative of a diagonal matrix raised to an arbitrary power,,"I have a matrix $(A^2)^k.$ I know that $A$ is a diagonal $n \times n$-matrix. I'm looking to take the derivative of this matrix with respect to $A$. $\frac{d}{dA} (A^2)^k = ???$ I'm not sure the chain rule applies here; if so does $\frac{d}{dA} (A^2)^k = k(A^2)^{(k-1)}(2A)$ If the chain rule does apply, I guess I'm not sure how to simplify that - or if it can be simplified any further. I'm keeping the exponents separate because $A$ is not guaranteed to be positive. Also, $k$ does not need to be a whole number (I'm looking at $0.6$). Is there a better way of dealing with exponents?","I have a matrix $(A^2)^k.$ I know that $A$ is a diagonal $n \times n$-matrix. I'm looking to take the derivative of this matrix with respect to $A$. $\frac{d}{dA} (A^2)^k = ???$ I'm not sure the chain rule applies here; if so does $\frac{d}{dA} (A^2)^k = k(A^2)^{(k-1)}(2A)$ If the chain rule does apply, I guess I'm not sure how to simplify that - or if it can be simplified any further. I'm keeping the exponents separate because $A$ is not guaranteed to be positive. Also, $k$ does not need to be a whole number (I'm looking at $0.6$). Is there a better way of dealing with exponents?",,"['matrices', 'derivatives']"
98,Finding the derivative of $\ln(\sin(x))$ using first principles.,Finding the derivative of  using first principles.,\ln(\sin(x)),Let $y=f(x)=\ln(\sin(x))$ $f(x+h)=\ln(\sin(x+h))$ $$\frac{d}{dx}(y)=\frac{d}{dx}(f(x))=\lim_{h \to0}\frac{f(x+h)-f(x)}{h}=\lim_{h \to0}\frac{\ln(\sin(x+h))-\ln(\sin(x))}{h}$$ $$=\lim_{h \to0}\frac{\ln\left(\frac{\sin(x+h)}{\sin(x)}\right)}{h} =\lim_{h \to0}\frac{\ln\left(\frac{\sin(x)\cos(h)+\cos(x)\sin(h)}{\sin (x)}\right)}{h}$$ $$=\lim_{h \to0}\frac{\ln (\cos(h)+\cot(x)\sin(h))}{h}=\lim_{h \to0}\frac{\ln( \cos(h)(1+\cot(x)\tan(h)))}{h}$$ I am stuck at this step. Please help.,Let $y=f(x)=\ln(\sin(x))$ $f(x+h)=\ln(\sin(x+h))$ $$\frac{d}{dx}(y)=\frac{d}{dx}(f(x))=\lim_{h \to0}\frac{f(x+h)-f(x)}{h}=\lim_{h \to0}\frac{\ln(\sin(x+h))-\ln(\sin(x))}{h}$$ $$=\lim_{h \to0}\frac{\ln\left(\frac{\sin(x+h)}{\sin(x)}\right)}{h} =\lim_{h \to0}\frac{\ln\left(\frac{\sin(x)\cos(h)+\cos(x)\sin(h)}{\sin (x)}\right)}{h}$$ $$=\lim_{h \to0}\frac{\ln (\cos(h)+\cot(x)\sin(h))}{h}=\lim_{h \to0}\frac{\ln( \cos(h)(1+\cot(x)\tan(h)))}{h}$$ I am stuck at this step. Please help.,,"['calculus', 'limits', 'derivatives']"
99,Derivative of a function and an integral,Derivative of a function and an integral,,"For example, if I had an equation like d/dx($x^2$(definite accumulated integral)) How do I solve this? would I just use FTOC II to normally solve the integral then multiply by $x^2$? Or am I supposed to use product rule for derivatives? Here is what I'm working on... $$ \frac d {dx} \left (x^6(\int_{0}^{sinx} \sqrt{t} dt)\right )$$","For example, if I had an equation like d/dx($x^2$(definite accumulated integral)) How do I solve this? would I just use FTOC II to normally solve the integral then multiply by $x^2$? Or am I supposed to use product rule for derivatives? Here is what I'm working on... $$ \frac d {dx} \left (x^6(\int_{0}^{sinx} \sqrt{t} dt)\right )$$",,"['calculus', 'integration', 'derivatives']"
