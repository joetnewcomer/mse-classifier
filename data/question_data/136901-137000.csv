,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Is there a ""uniqueness theorem"" for non-normal ODE?","Is there a ""uniqueness theorem"" for non-normal ODE?",,"I say an ODE $F(x, y, y^{(1)}, \dots , y^{(n)})=0$ is normal if it can be written in a form $y^{(n)} = f(x, y, y^{(1)} , \dots , y^{(n-1)})$ . It is well-known for normal ODEs (satisfying good condition), there's a uniqueness theorem for initial value problem (Picard-Lindelof theorem). This result helps me completely determine the solutions of normal ODEs. Then, what about non-normal ODEs? For instance, I learned that $y = xy' - y'^2/4$ (Clairaut's form) has the general solution $y=Cx-C^2/4$ and singular solution $y=x^2$ in my textbook. However, in this case, general solution and singular solution can be ""connected"" continuously so that there can be another solutions, like $y=x^2(x>0), y=0(x \leq 0)$ . My textbook does not mention these ones. I feel weird about this. I want to determine all the possibility of the solutions of non-normal ODEs. The question is: (1) Is it possible? (2) If so, how to do it? Is there a general result? I'm new to the theory of ODEs. Any help would be appreciated.","I say an ODE is normal if it can be written in a form . It is well-known for normal ODEs (satisfying good condition), there's a uniqueness theorem for initial value problem (Picard-Lindelof theorem). This result helps me completely determine the solutions of normal ODEs. Then, what about non-normal ODEs? For instance, I learned that (Clairaut's form) has the general solution and singular solution in my textbook. However, in this case, general solution and singular solution can be ""connected"" continuously so that there can be another solutions, like . My textbook does not mention these ones. I feel weird about this. I want to determine all the possibility of the solutions of non-normal ODEs. The question is: (1) Is it possible? (2) If so, how to do it? Is there a general result? I'm new to the theory of ODEs. Any help would be appreciated.","F(x, y, y^{(1)}, \dots , y^{(n)})=0 y^{(n)} = f(x, y, y^{(1)} , \dots , y^{(n-1)}) y = xy' - y'^2/4 y=Cx-C^2/4 y=x^2 y=x^2(x>0), y=0(x \leq 0)",['ordinary-differential-equations']
1,How do we know when we have all solutions to a differential equation?,How do we know when we have all solutions to a differential equation?,,"My question is theoretical, but it really bothers me. The way I see differential equation's solutions is that someone observed that, for instance, $Ce^{Ax}$ solves first order equations, etc. The solutions that we know are valid, but how can we be sure that there are no other solutions that we are neglecting? Furthermore, could the existence of such solutions be a problem in the future, since we have incomplete answers for all the problems that involve differential equations?","My question is theoretical, but it really bothers me. The way I see differential equation's solutions is that someone observed that, for instance, solves first order equations, etc. The solutions that we know are valid, but how can we be sure that there are no other solutions that we are neglecting? Furthermore, could the existence of such solutions be a problem in the future, since we have incomplete answers for all the problems that involve differential equations?",Ce^{Ax},['ordinary-differential-equations']
2,Could anyone please help me solve this differential? [closed],Could anyone please help me solve this differential? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $$b∙v_Y^2/m-dv_Y/dt=g$$ $v_Y$ here simply means vertical velocity, b is coefficient for drag force, g is the gravity constant. For $v_Y$ , you may simply regard this as $x$ or other variables. I don't think it would affect calculations. I don't know the steps but I know the answer to this differential. I have attached the image of the answer below. Please teach me the steps to the answer. To explain a bit about the image, $V_{yi}$ stands for the initial velocity and $V_t'$ stands for the terminal velocity. here $V_t'=(mg/b)^{.5}$ Thank You in advance.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question here simply means vertical velocity, b is coefficient for drag force, g is the gravity constant. For , you may simply regard this as or other variables. I don't think it would affect calculations. I don't know the steps but I know the answer to this differential. I have attached the image of the answer below. Please teach me the steps to the answer. To explain a bit about the image, stands for the initial velocity and stands for the terminal velocity. here Thank You in advance.",b∙v_Y^2/m-dv_Y/dt=g v_Y v_Y x V_{yi} V_t' V_t'=(mg/b)^{.5},['ordinary-differential-equations']
3,Eigenfunction expansion and boundary conditions - understanding the nuances,Eigenfunction expansion and boundary conditions - understanding the nuances,,"Given an orthonormal basis of eigenfunctions $\{ f_n\}_{n=1}^{\infty}$ for a Sturm-Liouville (SL) problem in the interval $[0,L]$ , and a function $f(x)$ that does not satisfy the boundary conditions (BC) of the SL problem, in which cases does the fact that $f$ does not satisfy the BC of the SL problem directly imply that the eigenfunction expansion for $f$ does not uniformly converge in $[0,L ]$ ? As shown in example 5 (page 7) in these lecture notes that confuse me , the eigenfunction expansion of $f(x)=1$ in the orthonormal basis $\left \{ \sqrt{\frac{2}{L}} \sin(n\pi x /L)  \right \}_{n=1}^{\infty}$ does not converge uniformly on $[ 0,L ]$ . In example 6, it is shown that the eigenfunction expansion of $f(x)=x$ in the orthonormal basis $\left \{ \sqrt{\frac{1}{L}} \right \} \cup \left \{ \sqrt{\frac{2}{L}} \cos(n\pi x /L)  \right \}_{n=1}^{\infty}$ does converge uniformly on $[ 0,L ]$ (I completely understand how to prove it using Weierstrass M-test and continuity). In both cases, the function $f$ does not satisfy the corresponding boundary conditions, but in the first case, we deduced that this implies that it is not possible to have uniform convergence in the entire interval, but in the second example we did not. What is the reason for this difference? Is this conclusion only true when the boundary conditions are specified for the function itself but not when they are specified on the derivative of the function? For your convenience, here are screenshots of the relevant example, (Jim Lambers Mat 606, usm.edu)","Given an orthonormal basis of eigenfunctions for a Sturm-Liouville (SL) problem in the interval , and a function that does not satisfy the boundary conditions (BC) of the SL problem, in which cases does the fact that does not satisfy the BC of the SL problem directly imply that the eigenfunction expansion for does not uniformly converge in ? As shown in example 5 (page 7) in these lecture notes that confuse me , the eigenfunction expansion of in the orthonormal basis does not converge uniformly on . In example 6, it is shown that the eigenfunction expansion of in the orthonormal basis does converge uniformly on (I completely understand how to prove it using Weierstrass M-test and continuity). In both cases, the function does not satisfy the corresponding boundary conditions, but in the first case, we deduced that this implies that it is not possible to have uniform convergence in the entire interval, but in the second example we did not. What is the reason for this difference? Is this conclusion only true when the boundary conditions are specified for the function itself but not when they are specified on the derivative of the function? For your convenience, here are screenshots of the relevant example, (Jim Lambers Mat 606, usm.edu)","\{ f_n\}_{n=1}^{\infty} [0,L] f(x) f f [0,L ] f(x)=1 \left \{ \sqrt{\frac{2}{L}} \sin(n\pi x /L)  \right \}_{n=1}^{\infty} [ 0,L ] f(x)=x \left \{ \sqrt{\frac{1}{L}} \right \} \cup \left \{ \sqrt{\frac{2}{L}} \cos(n\pi x /L)  \right \}_{n=1}^{\infty} [ 0,L ] f","['ordinary-differential-equations', 'uniform-convergence', 'eigenfunctions', 'sturm-liouville']"
4,Stuck on using WKB to solve $\epsilon\frac{d^2u}{dx^2}-x(1+x)\frac{du}{dx}+xu=0$ as $\epsilon\to 0$,Stuck on using WKB to solve  as,\epsilon\frac{d^2u}{dx^2}-x(1+x)\frac{du}{dx}+xu=0 \epsilon\to 0,"I want to use the WKB method to solve the uniform leading order approximation for the ODE $$\epsilon\frac{d^2u}{dx^2}-x(1+x)\frac{du}{dx}+xu=0,~u(-0.5)=\alpha,~u(0.5)=\beta$$ My attempt: Let $u(x)\approx\exp\left(\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n(x)\right)$ , then we can compute $u'(x)$ and $u''(x)$ . And finally I have $$\epsilon\left[\frac{1}{\epsilon ^2}\left(\sum_{n=0}^{\infty}\epsilon^nS_n'(x)\right)^2+\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n''(x)\right]-x(1+x)\left(\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n'(x)\right)+x=0$$ . However I'm not sure how to continue the proccess. How to deal with the $x$ terms? And how to finish computing these all messing stuffs? Need some help! PS: I know the finally answer shall be $u(x)\approx2\alpha(x+1)+(\beta-3\alpha)\exp^{\frac{6x-3}{8\epsilon}}$","I want to use the WKB method to solve the uniform leading order approximation for the ODE My attempt: Let , then we can compute and . And finally I have . However I'm not sure how to continue the proccess. How to deal with the terms? And how to finish computing these all messing stuffs? Need some help! PS: I know the finally answer shall be","\epsilon\frac{d^2u}{dx^2}-x(1+x)\frac{du}{dx}+xu=0,~u(-0.5)=\alpha,~u(0.5)=\beta u(x)\approx\exp\left(\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n(x)\right) u'(x) u''(x) \epsilon\left[\frac{1}{\epsilon ^2}\left(\sum_{n=0}^{\infty}\epsilon^nS_n'(x)\right)^2+\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n''(x)\right]-x(1+x)\left(\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n'(x)\right)+x=0 x u(x)\approx2\alpha(x+1)+(\beta-3\alpha)\exp^{\frac{6x-3}{8\epsilon}}","['ordinary-differential-equations', 'asymptotics']"
5,Derivative of a vector field is anti-symmetric $\iff$ $dF_t(p)$ is an isometry,Derivative of a vector field is anti-symmetric   is an isometry,\iff dF_t(p),"Let $F$ a complete vector field of class $C^1$ in $\Bbb{R}^n$ . I need to show that $dF(p)$ is an anti-symmetric matrix $\iff dF_t(p)$ is an isometry, that is $$\left<dF_t(p)u,dF_t(p)v\right>=\left<u,v\right>\forall\, t\in\Bbb{R}, p,u,v\in\Bbb{R}^n. $$ Here, $F_t(p)$ is the trajectory of $F$ passing through $p$ . I found some answers, like this one, where $F$ is a linear field. In that case, $F_t(p)$ is just an exponential matrix, and all goes fine. But I couldn't addapt the proof for the non linear case. If someone could give me some hints (not the complete answer) I would appreciate.","Let a complete vector field of class in . I need to show that is an anti-symmetric matrix is an isometry, that is Here, is the trajectory of passing through . I found some answers, like this one, where is a linear field. In that case, is just an exponential matrix, and all goes fine. But I couldn't addapt the proof for the non linear case. If someone could give me some hints (not the complete answer) I would appreciate.","F C^1 \Bbb{R}^n dF(p) \iff dF_t(p) \left<dF_t(p)u,dF_t(p)v\right>=\left<u,v\right>\forall\, t\in\Bbb{R}, p,u,v\in\Bbb{R}^n.  F_t(p) F p F F_t(p)","['ordinary-differential-equations', 'vector-fields', 'isometry']"
6,Which ODEs have solutions in terms of elementary functions?,Which ODEs have solutions in terms of elementary functions?,,I'm wondering what the most general results are on determining which ODEs have solutions which are expressible in terms of elementary functions and which do not. Is there some kind of ODE equivalent of the Risch algorithm?,I'm wondering what the most general results are on determining which ODEs have solutions which are expressible in terms of elementary functions and which do not. Is there some kind of ODE equivalent of the Risch algorithm?,,"['real-analysis', 'calculus', 'ordinary-differential-equations', 'elementary-functions']"
7,How to use the Backwards Euler Method,How to use the Backwards Euler Method,,"Given the ODE $\frac{dy}{dt} = f(t,y)$ and the function $f(y) = -y^3$ , with the initial condition $y(0)=1$ , I want to use the backward Euler Method with $h = \frac{1}{2}$ , combined with the Newton-Rapson method to approximate $y$ at $t = 2$ . This ODE is seperable and an exact solution can be found, which I found to be $y = \sqrt{\frac{1}{2t+1}}.$ Therefore at $t = 2$ the exact solution would be $\sqrt{\frac{1}{5}}$ . However, I want to know how to use these two numerical methods to approximate a solution. So far, I used the Backward Euler Method as follows: $$y_{n+1} \approx y_n + hf(t_{n+1}, y_{n+1})$$ $$y_{n+1} \approx 1 - \frac{1}{2}y_{n+1}^3$$ This forms the cubic equation $y_{n+1}^3 + 2y_{n+1} - 2 \approx 0$ which can be solved via Newton-Raphson, and this gives me a value of $y_{n+1} = 0.7709$ . But where can I go from here? Have I made a mistake? Any guidance would be much appreciated.","Given the ODE and the function , with the initial condition , I want to use the backward Euler Method with , combined with the Newton-Rapson method to approximate at . This ODE is seperable and an exact solution can be found, which I found to be Therefore at the exact solution would be . However, I want to know how to use these two numerical methods to approximate a solution. So far, I used the Backward Euler Method as follows: This forms the cubic equation which can be solved via Newton-Raphson, and this gives me a value of . But where can I go from here? Have I made a mistake? Any guidance would be much appreciated.","\frac{dy}{dt} = f(t,y) f(y) = -y^3 y(0)=1 h = \frac{1}{2} y t = 2 y = \sqrt{\frac{1}{2t+1}}. t = 2 \sqrt{\frac{1}{5}} y_{n+1} \approx y_n + hf(t_{n+1}, y_{n+1}) y_{n+1} \approx 1 - \frac{1}{2}y_{n+1}^3 y_{n+1}^3 + 2y_{n+1} - 2 \approx 0 y_{n+1} = 0.7709","['ordinary-differential-equations', 'numerical-methods', 'approximation']"
8,Finite-time criterion for ODE,Finite-time criterion for ODE,,"In article Finite-Time Stability of Continuous Autonomous Systems i found this [page 4]. That's what I don't understand: Can (2.7) $\dot{y}(t)=-k \cdot {\rm sign}(y(t)) \cdot \lvert y(t) \rvert^{\alpha}$ be reformulated as a condition for convergence in a finite-time, i.e. $\dot{y}(t) + k \cdot {\rm sign}(y(t)) \cdot \lvert y(t) \rvert^{\alpha}=0$ ? What if I have an equation of the form $\dot{y}=\nabla_y f$ and want $\nabla_y f \rightarrow 0$ in finite-time. Does this mean that I must meet the condition $\dot{\nabla_y f} + k \cdot {\rm sign}(\nabla_y f) \cdot \lvert \nabla_y f \rvert^{\alpha}=0$ Please help me figure it out. I would be grateful for help.","In article Finite-Time Stability of Continuous Autonomous Systems i found this [page 4]. That's what I don't understand: Can (2.7) be reformulated as a condition for convergence in a finite-time, i.e. ? What if I have an equation of the form and want in finite-time. Does this mean that I must meet the condition Please help me figure it out. I would be grateful for help.",\dot{y}(t)=-k \cdot {\rm sign}(y(t)) \cdot \lvert y(t) \rvert^{\alpha} \dot{y}(t) + k \cdot {\rm sign}(y(t)) \cdot \lvert y(t) \rvert^{\alpha}=0 \dot{y}=\nabla_y f \nabla_y f \rightarrow 0 \dot{\nabla_y f} + k \cdot {\rm sign}(\nabla_y f) \cdot \lvert \nabla_y f \rvert^{\alpha}=0,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'optimal-control', 'adaptive-control']"
9,System of differential equations blow-up in finite time,System of differential equations blow-up in finite time,,"I have the following system of differential equations: $\begin{cases}y_1'=y_1(y_2-1)\\y_2'=y_2(y_1-1)\end{cases}$ with critical points $(0,0)$ and $(1,1)$ . We can obtain the orbits: $\dfrac{dy_1}{dy_2}=\dfrac{y_1}{y_2}\dfrac{y_2-1}{y_1-1}\Rightarrow \left(1-\dfrac{1}{y_1}\right)dy_1=\left(1-\dfrac{1}{y_2}\right)dy_2\Rightarrow y_1-\log(y_1)=y_2-\log(y_2)+C\Rightarrow $$y_1-\log(y_1)-1=y_2-\log(y_2)-1+C$ . Now we can define $h:(0,+\infty)\to\Bbb R$ as $h(t)=t-\log(t)-1$ , so each orbit is contained in a curve of the form $\{(y_1,y_2):h(y_1)-h(y_2)=C\}$ . $h$ attains a minimum, $h(1)=0$ , and it kind of behaves like a parabola, so we can define $h_1=h_{\mid (0,1)}$ and $h_2=h_{\mid (1,+\infty)}$ , and they both are invertible maps with range $(0,+\infty)$ . I'm interested in the case $C=0$ , which consists of a curve containing $(1,1)$ , and more specifically, when $y_1(0)=y_{10}>0, y_2(0)=y_{20}>0$ . In this case we have $y_2=h_2^{-1}(h(y_1))$ (or $y_1=h_2^{-1}(h(y_2))$ ). I've been told that there exists $0<\beta<+\infty$ such that $y_1(t),y_2(t)\xrightarrow{t\to\beta^-}+\infty$ , but why is $\beta<+\infty$ ? Any ideas on how to show this? The going-to-infinity part I think I can show: It's clear that $0<\beta\leq+\infty$ , and we have $y_1(t),y_2(t)>1\;\forall t\geq0$ , so both $y_1$ and $y_2$ are strictly increasing, and therefore $\exists\, a,b$ such that $\displaystyle\lim_{t\to\beta^-}y_1(t)=a\in(y_{10},+\infty)$ and $\displaystyle\lim_{t\to\beta^-}y_2(t)=b\in(y_{20},+\infty)$ . If we had $a<+\infty$ then $b=\displaystyle\lim_{t\to\beta^-}y_2(t)=\lim_{t\to\beta^-}h_2^{-1}(h(y_1(t)))=h_2^{-1}(h(a))=a$ (because $a>y_{10}>1$ ). But then, as a consequence of LaSalle's invariance principle, $(a,a)$ should be a critical point, which is absurd. Therefore, $y_1(t)\xrightarrow{t\to\beta^-}+\infty$ , and consequently, $y_2(t)\xrightarrow{t\to\beta^-}+\infty$ .","I have the following system of differential equations: with critical points and . We can obtain the orbits: . Now we can define as , so each orbit is contained in a curve of the form . attains a minimum, , and it kind of behaves like a parabola, so we can define and , and they both are invertible maps with range . I'm interested in the case , which consists of a curve containing , and more specifically, when . In this case we have (or ). I've been told that there exists such that , but why is ? Any ideas on how to show this? The going-to-infinity part I think I can show: It's clear that , and we have , so both and are strictly increasing, and therefore such that and . If we had then (because ). But then, as a consequence of LaSalle's invariance principle, should be a critical point, which is absurd. Therefore, , and consequently, .","\begin{cases}y_1'=y_1(y_2-1)\\y_2'=y_2(y_1-1)\end{cases} (0,0) (1,1) \dfrac{dy_1}{dy_2}=\dfrac{y_1}{y_2}\dfrac{y_2-1}{y_1-1}\Rightarrow \left(1-\dfrac{1}{y_1}\right)dy_1=\left(1-\dfrac{1}{y_2}\right)dy_2\Rightarrow y_1-\log(y_1)=y_2-\log(y_2)+C\Rightarrow y_1-\log(y_1)-1=y_2-\log(y_2)-1+C h:(0,+\infty)\to\Bbb R h(t)=t-\log(t)-1 \{(y_1,y_2):h(y_1)-h(y_2)=C\} h h(1)=0 h_1=h_{\mid (0,1)} h_2=h_{\mid (1,+\infty)} (0,+\infty) C=0 (1,1) y_1(0)=y_{10}>0, y_2(0)=y_{20}>0 y_2=h_2^{-1}(h(y_1)) y_1=h_2^{-1}(h(y_2)) 0<\beta<+\infty y_1(t),y_2(t)\xrightarrow{t\to\beta^-}+\infty \beta<+\infty 0<\beta\leq+\infty y_1(t),y_2(t)>1\;\forall t\geq0 y_1 y_2 \exists\, a,b \displaystyle\lim_{t\to\beta^-}y_1(t)=a\in(y_{10},+\infty) \displaystyle\lim_{t\to\beta^-}y_2(t)=b\in(y_{20},+\infty) a<+\infty b=\displaystyle\lim_{t\to\beta^-}y_2(t)=\lim_{t\to\beta^-}h_2^{-1}(h(y_1(t)))=h_2^{-1}(h(a))=a a>y_{10}>1 (a,a) y_1(t)\xrightarrow{t\to\beta^-}+\infty y_2(t)\xrightarrow{t\to\beta^-}+\infty",['ordinary-differential-equations']
10,Importance of stability in optimal control,Importance of stability in optimal control,,"This is a quite general question, so even answers highlighting particular papers or books will be very helpful. Given an optimal control problem of the form $$\min_{u\in\mathbb{R}^k} Loss(x,u) \;\;s.t. \;\; \dot{x}(t) = f(t,x,u),$$ when is the stability under small perturbation in the initial data of the plant equation $\dot{x}(t) = f(t,x,u)$ relevant? In these cases, why is it relevant? For simplicity let us consider a Bolza problem, so $Loss(x,u) = f_u(\Phi^T(x_0))$ , where $\Phi^T$ is the flow of the ODE at time $T>0$ . I am quite new to control problems, so I know the importance of stability for dynamical systems in general and even the various available notions, but I don't get why for control problems it should be important.","This is a quite general question, so even answers highlighting particular papers or books will be very helpful. Given an optimal control problem of the form when is the stability under small perturbation in the initial data of the plant equation relevant? In these cases, why is it relevant? For simplicity let us consider a Bolza problem, so , where is the flow of the ODE at time . I am quite new to control problems, so I know the importance of stability for dynamical systems in general and even the various available notions, but I don't get why for control problems it should be important.","\min_{u\in\mathbb{R}^k} Loss(x,u) \;\;s.t. \;\; \dot{x}(t) = f(t,x,u), \dot{x}(t) = f(t,x,u) Loss(x,u) = f_u(\Phi^T(x_0)) \Phi^T T>0","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'optimal-control']"
11,Proving an integral doesn't converge,Proving an integral doesn't converge,,"I have derived a differential equation for the density of a gas due to gravity as a function of distance to the center of mass. However, in order to prove some properties about it, I need to know if a certain integral converges or diverges to infinity. The latter seems to be the case. In this sense, given that $$\rho'' = \dfrac{4\pi G}{RT}\rho^2 + \dfrac{(\rho')^2}{\rho}-\dfrac{2T+zT'}{zT}\rho'$$ (Where $\rho$ and $T$ are a function of $z$ ) How do I prove that $$\int^{\infty}_{0}x^2\rho(x)\mathrm{d}x$$ does not converge? For simplicity, one may consider $T(z)$ as being constant","I have derived a differential equation for the density of a gas due to gravity as a function of distance to the center of mass. However, in order to prove some properties about it, I need to know if a certain integral converges or diverges to infinity. The latter seems to be the case. In this sense, given that (Where and are a function of ) How do I prove that does not converge? For simplicity, one may consider as being constant",\rho'' = \dfrac{4\pi G}{RT}\rho^2 + \dfrac{(\rho')^2}{\rho}-\dfrac{2T+zT'}{zT}\rho' \rho T z \int^{\infty}_{0}x^2\rho(x)\mathrm{d}x T(z),"['calculus', 'integration', 'ordinary-differential-equations', 'convergence-divergence', 'definite-integrals']"
12,First integrals for topologically conjugate systems,First integrals for topologically conjugate systems,,"I have the two dynamical systems System 1: $(\dot{x},\dot{y})=(x(x+1),-y)$ System 2: $(\dot{x},\dot{y})=(x(x+1),-y+4x^3-12x+3)$ which are topologically conjugate, which we can show using the map $\vec{h}(x,y)=(x,y+p(x))$ where $p(x)=2x^2-6x+3$ . First I need to find the inverse of $\vec{h}$ . Then, given $V(x,y)$ is a first integral of System 2, I need to find a first integral of System 1 . I am not really familiar with inverses of vector fields but I think that $\vec{h}^{-1}(x,y)=(x,y-p(x))$ . I then tried to show that $V(x,y-p(x))$ was a first integral of system 2 but after taking the time derivative and expanding everything out I ended up with a mess, and not much simplified.","I have the two dynamical systems System 1: System 2: which are topologically conjugate, which we can show using the map where . First I need to find the inverse of . Then, given is a first integral of System 2, I need to find a first integral of System 1 . I am not really familiar with inverses of vector fields but I think that . I then tried to show that was a first integral of system 2 but after taking the time derivative and expanding everything out I ended up with a mess, and not much simplified.","(\dot{x},\dot{y})=(x(x+1),-y) (\dot{x},\dot{y})=(x(x+1),-y+4x^3-12x+3) \vec{h}(x,y)=(x,y+p(x)) p(x)=2x^2-6x+3 \vec{h} V(x,y) \vec{h}^{-1}(x,y)=(x,y-p(x)) V(x,y-p(x))","['ordinary-differential-equations', 'dynamical-systems']"
13,Why are continuous partial derivatives up to order two (rather than one) of nonlinear autonomous (2D) systems sufficient for linear approximation?,Why are continuous partial derivatives up to order two (rather than one) of nonlinear autonomous (2D) systems sufficient for linear approximation?,,"In Boyce and Diprima's ODE's and BVP's (10th edition page 522), it says that for the nonlinear autonomous system $$x^\prime = F(x,y)\qquad y^\prime = G(x,y) \qquad\qquad\qquad (10),$$ ""The system (10) is locally linear in the neighborhood of a critical point $(x_0,y_0)$ whenever the functions $F$ and $G$ have continuous partial derivatives up to order two. To show this , we use Taylor expansions about the point $(x_0,y_0)$ to write $F(x,y)$ and $G(x,y)$ in the form $$F(x,y)=F(x_0,y_0)+F_x(x_0,y_0)(x-x_0)+F_y(x_0,y_0)(y-y_0)+\eta_1(x,y)\\ G(x,y)=G(x_0,y_0)+G_x(x_0,y_0)(x-x_0)+G_y(x_0,y_0)(y-y_0)+\eta_2(x,y)$$ where $\eta_1(x,y)/[(x-x_0)^2+(y-y_0)^2]^{1/2}\to 0$ as $(x,y)\to(x_0,y_0)$ , and similarly for $\eta_2$ ."" Why does it say ""whenever the functions $F$ and $G$ have continuous partial derivatives up to order two"" instead of ""whenever the functions $F$ and $G$ have continuous partial derivatives up to order one""? $\eta_1(x,y)$ and $\eta_2(x,y)$ contain the nonlinear parts and the second order partial derivatives, right?  And, Taylor's Theorem says the function being k times differentiable is sufficient for an approximation by a kth degree Taylor polynomial.","In Boyce and Diprima's ODE's and BVP's (10th edition page 522), it says that for the nonlinear autonomous system ""The system (10) is locally linear in the neighborhood of a critical point whenever the functions and have continuous partial derivatives up to order two. To show this , we use Taylor expansions about the point to write and in the form where as , and similarly for ."" Why does it say ""whenever the functions and have continuous partial derivatives up to order two"" instead of ""whenever the functions and have continuous partial derivatives up to order one""? and contain the nonlinear parts and the second order partial derivatives, right?  And, Taylor's Theorem says the function being k times differentiable is sufficient for an approximation by a kth degree Taylor polynomial.","x^\prime = F(x,y)\qquad y^\prime = G(x,y) \qquad\qquad\qquad (10), (x_0,y_0) F G (x_0,y_0) F(x,y) G(x,y) F(x,y)=F(x_0,y_0)+F_x(x_0,y_0)(x-x_0)+F_y(x_0,y_0)(y-y_0)+\eta_1(x,y)\\ G(x,y)=G(x_0,y_0)+G_x(x_0,y_0)(x-x_0)+G_y(x_0,y_0)(y-y_0)+\eta_2(x,y) \eta_1(x,y)/[(x-x_0)^2+(y-y_0)^2]^{1/2}\to 0 (x,y)\to(x_0,y_0) \eta_2 F G F G \eta_1(x,y) \eta_2(x,y)","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'nonlinear-analysis', 'nonlinear-dynamics']"
14,Stability of the equilibrium states,Stability of the equilibrium states,,"I have a function defined by the following differential equation $$ \frac{\mathrm{d}\varphi}{\mathrm{d}t} = \gamma - F(\varphi) $$ where $F(\varphi)$ is a $2\pi$ -periodic function) and the chart of the function $F(\varphi)$ is known. I need to find the equilibrium states (e.s.) and find out if they are stable. I've managed to find out what are they only in some cases (but this is not accurate). Can you help with others? ^ F(φ)                 1|                  |    /\  /\                 b|   /  \/  \                  |  /        \                  | /          \     __-π_______-a____|/____________\____>φ    \            /|0    a π/2   π      \          / |      \        /  |       \  /\  /   |-b        \/  \/    |                  |-1 If $|\gamma| > 1$ then there are no e.s. If $|\gamma| = 1$ there are 2 (at level ±γ each) e.s. in points (a,1), (π-a,1). maybe they're semi-stable? If $b < |\gamma| < 1$ there are $4$ at level $\pm \gamma$ each e.s. If $|\gamma| = b$ there are $3$ at level $\pm\gamma$ each e.s.: point $(\pi/2, b)$ , $\varphi = a\gamma$ , $\gamma = \pi - a\gamma$ . If $0 < |\gamma| < b$ there are $2$ e.s. at level $\pm±\gamma$ each $\varphi  = a\gamma$ (is it stable?) , $\varphi = \pi - a\gamma$ (and is this one unstable?). The form of $F(\varphi)$ is the following one: $$F(\varphi)=\begin{cases}  & {-\dfrac{\varphi}{a}-\dfrac{\pi}{a}},  &\text{if } {\varphi \in [-\pi, -\pi + a]},\\ \\  & \dfrac{1-b}{\dfrac{\pi }{2}-a }\varphi + \dfrac{(1-b)(\pi-a))}{\dfrac{\pi }{2}-a }-1, &\text{if } {\varphi \in [-\pi + a,-\frac{\pi }{2}}], \\  \\  & \dfrac{b-1}{\dfrac{\pi }{2}-a }\varphi + \dfrac{a(1-b)}{\dfrac{\pi }{2}-a }-1, &\text{if } {\varphi \in [-\dfrac{\pi }{2}}, -a], \\ \\  & \dfrac{\varphi}{a},  &\text{if }  {\varphi \in [- a,  a]}, \\ \\ & \dfrac{b-1}{\dfrac{\pi }{2}-a }\varphi - \dfrac{a(1-b)}{\dfrac{\pi }{2}-a }+1, &\text{if } {\varphi \in [a, \frac{\pi }{2}}], \\ \\ & \dfrac{1-b}{\dfrac{\pi }{2}-a }\varphi - \dfrac{(1-b)(\pi-a))}{\dfrac{\pi }{2}-a }+1, &\text{if } {\varphi \in [\frac{\pi }{2}}, \pi-a], \\  \\  & {-\dfrac{\varphi}{a}+\dfrac{\pi}{a}},&\text{if }  {\varphi \in [\pi-a, \pi]}. \end{cases}$$ P.S. sorry for this chart :)","I have a function defined by the following differential equation where is a -periodic function) and the chart of the function is known. I need to find the equilibrium states (e.s.) and find out if they are stable. I've managed to find out what are they only in some cases (but this is not accurate). Can you help with others? ^ F(φ)                 1|                  |    /\  /\                 b|   /  \/  \                  |  /        \                  | /          \     __-π_______-a____|/____________\____>φ    \            /|0    a π/2   π      \          / |      \        /  |       \  /\  /   |-b        \/  \/    |                  |-1 If then there are no e.s. If there are 2 (at level ±γ each) e.s. in points (a,1), (π-a,1). maybe they're semi-stable? If there are at level each e.s. If there are at level each e.s.: point , , . If there are e.s. at level each (is it stable?) , (and is this one unstable?). The form of is the following one: P.S. sorry for this chart :)","
\frac{\mathrm{d}\varphi}{\mathrm{d}t} = \gamma - F(\varphi)
 F(\varphi) 2\pi F(\varphi) |\gamma| > 1 |\gamma| = 1 b < |\gamma| < 1 4 \pm \gamma |\gamma| = b 3 \pm\gamma (\pi/2, b) \varphi = a\gamma \gamma = \pi - a\gamma 0 < |\gamma| < b 2 \pm±\gamma \varphi  = a\gamma \varphi = \pi - a\gamma F(\varphi) F(\varphi)=\begin{cases}
 & {-\dfrac{\varphi}{a}-\dfrac{\pi}{a}},  &\text{if } {\varphi \in [-\pi, -\pi + a]},\\
\\
 & \dfrac{1-b}{\dfrac{\pi }{2}-a }\varphi + \dfrac{(1-b)(\pi-a))}{\dfrac{\pi }{2}-a }-1, &\text{if } {\varphi \in [-\pi + a,-\frac{\pi }{2}}], \\ 
\\
 & \dfrac{b-1}{\dfrac{\pi }{2}-a }\varphi + \dfrac{a(1-b)}{\dfrac{\pi }{2}-a }-1, &\text{if } {\varphi \in [-\dfrac{\pi }{2}}, -a], \\
\\
 & \dfrac{\varphi}{a},  &\text{if }  {\varphi \in [- a,  a]}, \\
\\
& \dfrac{b-1}{\dfrac{\pi }{2}-a }\varphi - \dfrac{a(1-b)}{\dfrac{\pi }{2}-a }+1, &\text{if } {\varphi \in [a, \frac{\pi }{2}}], \\
\\
& \dfrac{1-b}{\dfrac{\pi }{2}-a }\varphi - \dfrac{(1-b)(\pi-a))}{\dfrac{\pi }{2}-a }+1, &\text{if } {\varphi \in [\frac{\pi }{2}}, \pi-a], \\ 
\\
 & {-\dfrac{\varphi}{a}+\dfrac{\pi}{a}},&\text{if }  {\varphi \in [\pi-a, \pi]}.
\end{cases}","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
15,Difference of the solution at two time instants,Difference of the solution at two time instants,,"Consider a nonlinear nonautonomous differential equation of the type $$ \dot{z}(t) = f(t,z(t)) $$ with $f:\mathbb{R}^+\times\mathbb{R}^n\rightarrow\mathbb{R}^n$ being a Lipschitz function. Is there some simple way to control the difference, in norm, of the solution at two time instants? For example given an $h>0$ and a $T>0$ , what can we say about $\|z(T+h)-z(T)\|$ , where by $\|\cdot\|$ we refer to the Euclidean norm? My approach was quite classical, i.e. to study $\frac{d}{dt}(\|z(T)-z(T+h)\|^2)$ and proceed aiming to use some Gronwall type inequality, however, I did not get something interesting.","Consider a nonlinear nonautonomous differential equation of the type with being a Lipschitz function. Is there some simple way to control the difference, in norm, of the solution at two time instants? For example given an and a , what can we say about , where by we refer to the Euclidean norm? My approach was quite classical, i.e. to study and proceed aiming to use some Gronwall type inequality, however, I did not get something interesting."," \dot{z}(t) = f(t,z(t))  f:\mathbb{R}^+\times\mathbb{R}^n\rightarrow\mathbb{R}^n h>0 T>0 \|z(T+h)-z(T)\| \|\cdot\| \frac{d}{dt}(\|z(T)-z(T+h)\|^2)","['ordinary-differential-equations', 'inequality', 'dynamical-systems']"
16,Help solving Boundary Condition Ordinary Differential Equation with absolute value.,Help solving Boundary Condition Ordinary Differential Equation with absolute value.,,"Good day everybody. I'm a physics student enrolled in a scientific computation course and my teacher sent us a bunch of ODEs to solve both analitically and numerically. I haven't really studied boundary value problems so maybe I need to be pointed to a resource for that but right now I'm but having trouble with this one in particular: $$u'' + 9u = 36(5- |x|)^2$$ with the boundary conditions $u(-5) = 0$ and $u(5) = 0$ . My attempt at solving it goes like this: I expand and separate the equation in its two domains $$u""+ 9u = 36x^2 + 360x + 900, \ x < 0$$ $$u""+ 9u = 36x^2 -  360x + 900, \ x \ge 0$$ Now I have two equations so I believe I need two homogeneous solutions, two particular solutions and four conditions. Therefore I solve for to the homogeneous part: $$U_h(x) = A\cos(3x) + B\sin(3x), \ x<0$$ $$U_h(x) = C\cos(3x) + D\sin(3x), \ x\ge 0$$ I resort to undetermined coefficients looking for particular solutions: $$U_p(x) = 4x^2+40x + 892/9, \ x < 0$$ $$U_p(x) = 4x^2-40x + 892/9, \ x \ge 0$$ I add $U_h$ and $U_p$ to make two general solutions: $$U(x) = A\cos(3x) + B\sin(3x) + 4x^2+40x + \frac{892}{9}, \ x<0 $$ $$U(x) = C\cos(3x) + D\sin(3x) + 4x^2-40x + \frac{892}{9}, \ x\ge0 $$ This is where I notice I have only two boundary conditions, not four. So, there is that. Should I merge both solutions back togheter into one? or maybe two conditions are enough and that's what I'm not seeing. Anyway, I hope you guys can help.","Good day everybody. I'm a physics student enrolled in a scientific computation course and my teacher sent us a bunch of ODEs to solve both analitically and numerically. I haven't really studied boundary value problems so maybe I need to be pointed to a resource for that but right now I'm but having trouble with this one in particular: with the boundary conditions and . My attempt at solving it goes like this: I expand and separate the equation in its two domains Now I have two equations so I believe I need two homogeneous solutions, two particular solutions and four conditions. Therefore I solve for to the homogeneous part: I resort to undetermined coefficients looking for particular solutions: I add and to make two general solutions: This is where I notice I have only two boundary conditions, not four. So, there is that. Should I merge both solutions back togheter into one? or maybe two conditions are enough and that's what I'm not seeing. Anyway, I hope you guys can help.","u'' + 9u = 36(5- |x|)^2 u(-5) = 0 u(5) = 0 u""+ 9u = 36x^2 + 360x + 900, \ x < 0 u""+ 9u = 36x^2 -  360x + 900, \ x \ge 0 U_h(x) = A\cos(3x) + B\sin(3x), \ x<0 U_h(x) = C\cos(3x) + D\sin(3x), \ x\ge 0 U_p(x) = 4x^2+40x + 892/9, \ x < 0 U_p(x) = 4x^2-40x + 892/9, \ x \ge 0 U_h U_p U(x) = A\cos(3x) + B\sin(3x) + 4x^2+40x + \frac{892}{9}, \ x<0  U(x) = C\cos(3x) + D\sin(3x) + 4x^2-40x + \frac{892}{9}, \ x\ge0 ","['ordinary-differential-equations', 'absolute-value', 'boundary-value-problem']"
17,General solution for sound propagation in a semi-infinite pipe,General solution for sound propagation in a semi-infinite pipe,,"I need to find the velocity potential $\Phi$ defined by $\vec{u}=\nabla\Phi$ in the domain $D=\{(x,y,z) : x^2+y^2\leq R^2, z\geq0\}$ . We are considering sound propagation so $\Phi$ satisfies the wave equation $\partial_{tt}\Phi=c^2\Delta\Phi$ . I am told that I can separate variables into $\Phi(x,y,z,t)=R(r)e^{in\theta}\psi(z,t)$ . Here is what I have done so far If I put this into the wave equation, after a few lines of algebra I end up with $R''+\frac{1}{r}R'+R(\frac{\partial_{zz}\psi}{\psi}-\frac{n^2}{r^2}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi})=0 \implies \frac{1}{R}\left(R''+\frac{1}{r}R'\right)-\frac{n^2}{r^2}=-\left(\frac{\partial_{zz}\psi}{\psi}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi}\right)$ The left side is dependent on $r$ , the right side is dependent on $z,t$ , so I can set both sides to be equal to a constant $C$ . I can deal with the right side $\frac{\partial_{zz}\psi}{\psi}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi}=C$ by separation of variables once again, say $\psi(z,t)=A(z)B(t)$ . I can show $\frac{A''(z)}{A(z)}-\frac{1}{c^2}\frac{B''(t)}{B(t)}=C \implies \frac{A''(z)}{A(z)}=\frac{1}{c^2}\frac{B''(t)}{B(t)}+C$ . Again I can play the same game, the left side is dependent on $z$ , the right side on $t$ , so both sides are equal to a constant, call it $k_z$ . Both are simple second order ODEs to solve, $A(z)=C_1e^{-ik_zz}+C_2e^{ik_zz}$ $B(t)=C_3e^{-i\omega t}+C_4e^{i\omega t}$ Where $\omega, k_z$ and the $C_i$ are constants. Here is where I get a bit lost For this semi infinite pipe the boundary conditions would be (I think) $u_z=0$ at $z=0$ , $u_r=0$ for $r=R$ . I also think there might be a pressure condition at the open end at $\infty$ , say $p=0$ at $z\rightarrow\infty$ , but I am not sure. The $u_z$ condition tells me that the $z$ constants have to be equal, that is $C_1=C_2=D$ . I am not sure how to eliminate any of the other constants here though, since the initial conditions are confusing me. I am also confused about the R solution If I sub $\psi$ back into the equation $R''+\frac{1}{r}R'+R(\frac{\partial_{zz}\psi}{\psi}-\frac{n^2}{r^2}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi})=0$ which I derived at the start, I get $R''+\frac{1}{r}R'+R\left(-k_z^2-\frac{n^2}{r^2}+\frac{\omega^2}{c^2}\right)=0$ which looks sort of like a Bessel ODE, but I am not sure what the correct transformation is to get it into the Bessel form. I think from here I could get the general solution, but I am confused about (1) the initial/boundary conditions and (2) the Bessel ODE here.","I need to find the velocity potential defined by in the domain . We are considering sound propagation so satisfies the wave equation . I am told that I can separate variables into . Here is what I have done so far If I put this into the wave equation, after a few lines of algebra I end up with The left side is dependent on , the right side is dependent on , so I can set both sides to be equal to a constant . I can deal with the right side by separation of variables once again, say . I can show . Again I can play the same game, the left side is dependent on , the right side on , so both sides are equal to a constant, call it . Both are simple second order ODEs to solve, Where and the are constants. Here is where I get a bit lost For this semi infinite pipe the boundary conditions would be (I think) at , for . I also think there might be a pressure condition at the open end at , say at , but I am not sure. The condition tells me that the constants have to be equal, that is . I am not sure how to eliminate any of the other constants here though, since the initial conditions are confusing me. I am also confused about the R solution If I sub back into the equation which I derived at the start, I get which looks sort of like a Bessel ODE, but I am not sure what the correct transformation is to get it into the Bessel form. I think from here I could get the general solution, but I am confused about (1) the initial/boundary conditions and (2) the Bessel ODE here.","\Phi \vec{u}=\nabla\Phi D=\{(x,y,z) : x^2+y^2\leq R^2, z\geq0\} \Phi \partial_{tt}\Phi=c^2\Delta\Phi \Phi(x,y,z,t)=R(r)e^{in\theta}\psi(z,t) R''+\frac{1}{r}R'+R(\frac{\partial_{zz}\psi}{\psi}-\frac{n^2}{r^2}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi})=0 \implies \frac{1}{R}\left(R''+\frac{1}{r}R'\right)-\frac{n^2}{r^2}=-\left(\frac{\partial_{zz}\psi}{\psi}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi}\right) r z,t C \frac{\partial_{zz}\psi}{\psi}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi}=C \psi(z,t)=A(z)B(t) \frac{A''(z)}{A(z)}-\frac{1}{c^2}\frac{B''(t)}{B(t)}=C \implies \frac{A''(z)}{A(z)}=\frac{1}{c^2}\frac{B''(t)}{B(t)}+C z t k_z A(z)=C_1e^{-ik_zz}+C_2e^{ik_zz} B(t)=C_3e^{-i\omega t}+C_4e^{i\omega t} \omega, k_z C_i u_z=0 z=0 u_r=0 r=R \infty p=0 z\rightarrow\infty u_z z C_1=C_2=D \psi R''+\frac{1}{r}R'+R(\frac{\partial_{zz}\psi}{\psi}-\frac{n^2}{r^2}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi})=0 R''+\frac{1}{r}R'+R\left(-k_z^2-\frac{n^2}{r^2}+\frac{\omega^2}{c^2}\right)=0","['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics', 'bessel-functions', 'fluid-dynamics']"
18,Closedness of second order PDE with complex coefficients.,Closedness of second order PDE with complex coefficients.,,"Is the following operator closed on $L^2(-1,1)$ ? \begin{equation} Tu=(c u')',\quad \mathcal{D}(T)=\{u\in H^2(-1,1):u(-1)=u(1)=0\} \end{equation} Here $c$ is a smooth complex-valued coefficient that is non-vanishing (bounded away from zero) on the interval. I'm aware the operator is closable but want to figure out if it closed and if not, what the domain of its closure is. Most texts I can find treat the case of real coefficients (where of course one can say a lot more). Really what I am after is a method of showing these kind of operators are closed (if they are!) or a nice counterexample. Something that can be extended to higher order (e.g. fourth) derivatives in divergence form as well.","Is the following operator closed on ? Here is a smooth complex-valued coefficient that is non-vanishing (bounded away from zero) on the interval. I'm aware the operator is closable but want to figure out if it closed and if not, what the domain of its closure is. Most texts I can find treat the case of real coefficients (where of course one can say a lot more). Really what I am after is a method of showing these kind of operators are closed (if they are!) or a nice counterexample. Something that can be extended to higher order (e.g. fourth) derivatives in divergence form as well.","L^2(-1,1) \begin{equation}
Tu=(c u')',\quad \mathcal{D}(T)=\{u\in H^2(-1,1):u(-1)=u(1)=0\}
\end{equation} c","['ordinary-differential-equations', 'partial-differential-equations', 'operator-theory']"
19,Determining general O(2)-equivariant ODE,Determining general O(2)-equivariant ODE,,"My goal is to prove that any O(2)-equivariant system of ordinary differential equations in the plane can be written in the form $$\begin{pmatrix} \dot{x} \\ \dot{y} \end{pmatrix} =\left[ \lambda +g(x^2+y^2) \right]\begin{pmatrix} x \\ y \end{pmatrix} $$ In order to be O(2)-equivariant, a function $\zeta:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ must satisfy the relation $$\zeta(\psi(\epsilon,(x,y)))=\psi(\epsilon,\zeta(x,y)),$$ where $\psi(\epsilon,(x,y))$ is the group action, for all $\epsilon$ (the group parameter) and any $(x,y)\in\mathbb{R}^2$ . My thought is that I can construct the general function $\zeta(x,y)=(\zeta_1(x,y),\zeta_2(x,y))^T=\left[ \lambda +g(x^2+y^2) \right]\begin{pmatrix}x, & y\end{pmatrix}^T$ by tackling reflection symmetry and continuous rotation symmetry separately. Looking at reflection first, it's obvious that if a function $\zeta(x,y)=(\zeta_1(x,y),\zeta_2(x,y))^T$ is to be equivariant under reflection about the $y$ -axis, then $\zeta_1(-x,y)=-\zeta_1(x,y)$ and $\zeta_2(-x,y)=\zeta_2(x,y)$ . That is, $\zeta_1(x,y)$ is odd in $x$ while $\zeta_2(x,y)$ is even in $x$ . Now considering the continuous rotation about the origin, I think (but have not proven) that it would be sufficient to show $$\frac{d}{d\epsilon}\bigg\rvert_{\epsilon=0}\zeta(\psi(\epsilon,(x,y)))=\frac{d}{d\epsilon}\bigg\rvert_{\epsilon=0}\psi(\epsilon,\zeta(x,y))$$ $$\Downarrow$$ $$\boldsymbol{v}(\zeta)(x)=\boldsymbol{v}\bigg\rvert_{(x,y)=\zeta(x,y)}$$ where $\boldsymbol{v}=y\partial_x-x\partial_y$ is the infinitesimal generator of the O(2). I found that this equation determines a first-order system of PDEs, $$\zeta_2=y\partial_x\zeta_1-x\partial_y\zeta_1$$ $$-\zeta_1=y\partial_x\zeta_2-x\partial_y\zeta_2$$ which does not seem right. Am I completely off base here? I'm new to Lie groups and algebras so I'm open to all kinds of criticism.","My goal is to prove that any O(2)-equivariant system of ordinary differential equations in the plane can be written in the form In order to be O(2)-equivariant, a function must satisfy the relation where is the group action, for all (the group parameter) and any . My thought is that I can construct the general function by tackling reflection symmetry and continuous rotation symmetry separately. Looking at reflection first, it's obvious that if a function is to be equivariant under reflection about the -axis, then and . That is, is odd in while is even in . Now considering the continuous rotation about the origin, I think (but have not proven) that it would be sufficient to show where is the infinitesimal generator of the O(2). I found that this equation determines a first-order system of PDEs, which does not seem right. Am I completely off base here? I'm new to Lie groups and algebras so I'm open to all kinds of criticism.","\begin{pmatrix} \dot{x} \\ \dot{y} \end{pmatrix}
=\left[ \lambda +g(x^2+y^2) \right]\begin{pmatrix} x \\ y \end{pmatrix}
 \zeta:\mathbb{R}^2 \rightarrow \mathbb{R}^2 \zeta(\psi(\epsilon,(x,y)))=\psi(\epsilon,\zeta(x,y)), \psi(\epsilon,(x,y)) \epsilon (x,y)\in\mathbb{R}^2 \zeta(x,y)=(\zeta_1(x,y),\zeta_2(x,y))^T=\left[ \lambda +g(x^2+y^2) \right]\begin{pmatrix}x, & y\end{pmatrix}^T \zeta(x,y)=(\zeta_1(x,y),\zeta_2(x,y))^T y \zeta_1(-x,y)=-\zeta_1(x,y) \zeta_2(-x,y)=\zeta_2(x,y) \zeta_1(x,y) x \zeta_2(x,y) x \frac{d}{d\epsilon}\bigg\rvert_{\epsilon=0}\zeta(\psi(\epsilon,(x,y)))=\frac{d}{d\epsilon}\bigg\rvert_{\epsilon=0}\psi(\epsilon,\zeta(x,y)) \Downarrow \boldsymbol{v}(\zeta)(x)=\boldsymbol{v}\bigg\rvert_{(x,y)=\zeta(x,y)} \boldsymbol{v}=y\partial_x-x\partial_y \zeta_2=y\partial_x\zeta_1-x\partial_y\zeta_1 -\zeta_1=y\partial_x\zeta_2-x\partial_y\zeta_2","['ordinary-differential-equations', 'lie-groups', 'symmetry', 'characteristics']"
20,Linearized ordinary differential equation for Hydraulic Mill,Linearized ordinary differential equation for Hydraulic Mill,,"The hydraulic mill is consists of two main part; servo valve and hydraulic cylinder. We will consider flow-control servo valve. The dynamics of the servo valve is \begin{equation}  \label{1}\tag{1} Q_s=C_Q\pi dx\sqrt{\frac{2}{\rho}(P_s-P_1)}            \end{equation} Here, $Q_s$ =supply flow, $C_Q$ =flow coefficient, $\rho$ =oil density, $x$ =servo-valve displacement, $P_s$ =supply pressure, $P_1$ =output pressure of the valve. The equation for the flow of oil to the hydraulic cylinder, \begin{equation}  \label{2}\tag{2}Q_s=a\dot{y}+\frac{V_1}{\beta}\dot{P_1} \end{equation} Here, $a$ =area of the cylinder, $y$ =hydraulic piston displacement, $V_1$ =volume of the primary side of the cylinder, $\beta$ =bulk modulus of the oil, $P_1$ =cylinder pressure on primary side. Schematic: Hydraulic servo system Equation \eqref{1} needs to be linearized for both input-flow & output-flow to the cylinder according to valve displacement $x$ . Help me find the ODE for rate of change in cylinder Pressure $\dot{P_1}$ and $\dot{P_2}$ .","The hydraulic mill is consists of two main part; servo valve and hydraulic cylinder. We will consider flow-control servo valve. The dynamics of the servo valve is Here, =supply flow, =flow coefficient, =oil density, =servo-valve displacement, =supply pressure, =output pressure of the valve. The equation for the flow of oil to the hydraulic cylinder, Here, =area of the cylinder, =hydraulic piston displacement, =volume of the primary side of the cylinder, =bulk modulus of the oil, =cylinder pressure on primary side. Schematic: Hydraulic servo system Equation \eqref{1} needs to be linearized for both input-flow & output-flow to the cylinder according to valve displacement . Help me find the ODE for rate of change in cylinder Pressure and .","\begin{equation} 
\label{1}\tag{1} Q_s=C_Q\pi dx\sqrt{\frac{2}{\rho}(P_s-P_1)}           
\end{equation} Q_s C_Q \rho x P_s P_1 \begin{equation} 
\label{2}\tag{2}Q_s=a\dot{y}+\frac{V_1}{\beta}\dot{P_1}
\end{equation} a y V_1 \beta P_1 x \dot{P_1} \dot{P_2}","['ordinary-differential-equations', 'fluid-dynamics', 'linearization']"
21,"Existence of $\delta$ for a sequence of initial value problem, where sequence $f_{m}$ converges uniformly on compact subset of the domain.","Existence of  for a sequence of initial value problem, where sequence  converges uniformly on compact subset of the domain.",\delta f_{m},"Let $D$ be an open set in $\mathbb{R} \times \mathbb{R}^{n}$ . Let $f_{m}$ be a sequence in $C\left(D ; \mathbb{R}^{n}\right)$ that converges uniformly on compact subset of $D$ to $f_{0} \in C\left(D ; R^{n}\right)$ . Let $\left(t_{m}, a_{m}\right)$ be a sequence in $D$ such that $$ \lim _{m \rightarrow \infty}\left(t_{m}, a_{m}\right)=\left(t_{0}, a_{0}\right) $$ for some $\left(t_{0}, a_{0}\right) \in D .$ For each $m \in \mathbb{N} \cup\{0\}$ let $x_{m}$ be a solution of $$ \left\{\begin{array}{l} \frac{d z}{d t}=f_{m}(t, x) \\ x\left(t_{m}\right)=a_{m} \end{array}\right. $$ I have to show that there exists a $\delta>0$ and $N_{1} \in \mathbb{N}$ such that $$ \left[ t_{0}-\delta, t_{0}+\delta\right] \subset I\left(x_{m}\right) $$ for all $m \geq N_{1}$ , where $I\left(x_{m}\right)$ denotes the maximal interval of existence for $x_{m}$ . Can someone help me to solve this problem. I don't know how to approach or what theorem to use for this problem.","Let be an open set in . Let be a sequence in that converges uniformly on compact subset of to . Let be a sequence in such that for some For each let be a solution of I have to show that there exists a and such that for all , where denotes the maximal interval of existence for . Can someone help me to solve this problem. I don't know how to approach or what theorem to use for this problem.","D \mathbb{R} \times \mathbb{R}^{n} f_{m} C\left(D ; \mathbb{R}^{n}\right) D f_{0} \in C\left(D ; R^{n}\right) \left(t_{m}, a_{m}\right) D 
\lim _{m \rightarrow \infty}\left(t_{m}, a_{m}\right)=\left(t_{0}, a_{0}\right)
 \left(t_{0}, a_{0}\right) \in D . m \in \mathbb{N} \cup\{0\} x_{m} 
\left\{\begin{array}{l}
\frac{d z}{d t}=f_{m}(t, x) \\
x\left(t_{m}\right)=a_{m}
\end{array}\right.
 \delta>0 N_{1} \in \mathbb{N} 
\left[ t_{0}-\delta, t_{0}+\delta\right] \subset I\left(x_{m}\right)
 m \geq N_{1} I\left(x_{m}\right) x_{m}","['ordinary-differential-equations', 'initial-value-problems']"
22,Is it possible for me to model mathematically the revolution to sharpen a pencil using complex number?,Is it possible for me to model mathematically the revolution to sharpen a pencil using complex number?,,"I was planning to do research on complex number since I find the topic intriguing, How can I conduct a research by modelling mathematically to calculate the required number of rotation for a pencil in a circular motion when sharpening the pencil with a sharpener using complex number by linking the number of rotations, length of graphite and the time taken?","I was planning to do research on complex number since I find the topic intriguing, How can I conduct a research by modelling mathematically to calculate the required number of rotation for a pencil in a circular motion when sharpening the pencil with a sharpener using complex number by linking the number of rotations, length of graphite and the time taken?",,"['ordinary-differential-equations', 'complex-numbers', 'regression', 'piecewise-continuity', 'eulers-method']"
23,Discrete dynamic system to continuous dynamic system,Discrete dynamic system to continuous dynamic system,,"I have a discrete time dynamical system $$x(t+1)=f(x(t))$$ I would like to write a continuous time system that approximates it well; that is, a continuous system of the form $$\frac{dx}{dt} = g(x)$$ , solve the continuous system, and use the solution to approximate the values of the original discrete time system. Are there any methods for doing this? I did some search and found conversions from a continuous system to a discrete one - what about the other way around? Also, how to bound the error of this approximation?","I have a discrete time dynamical system I would like to write a continuous time system that approximates it well; that is, a continuous system of the form , solve the continuous system, and use the solution to approximate the values of the original discrete time system. Are there any methods for doing this? I did some search and found conversions from a continuous system to a discrete one - what about the other way around? Also, how to bound the error of this approximation?",x(t+1)=f(x(t)) \frac{dx}{dt} = g(x),"['ordinary-differential-equations', 'recurrence-relations', 'dynamical-systems']"
24,Critical points of polar curve of given ode,Critical points of polar curve of given ode,,"How to find maximum minimum radii and inflection point of a curve with the polar symmetry given by the following ode ? Primed on arc,the angle $\psi$ is made between curve and radial lines. $\theta \text{=const.}$ $$ 2a \psi'=\left(1-\dfrac{T^2}{r^2}\right);\;r'= \cos \psi \; \tag1 $$ Patterns above are drawn for constants $ a=1,T= (4,2) ,\theta_i=\pi/4, r_i=1,s_{max=}=320 a, $ after numerical integration. The outer maximum radii  are $ (\approx \sqrt2,2)$ which could not be found analytically. In polar coordinates ... is the following true for local straightness ? $$\psi^{'}_{inflection}= {0}. \tag 2 $$ Pictorially it does not agree for the first pattern (no inflection), but seems a better tally for the second case where: $$ r_{inflection} =T \;$$ To find radial extrema $$\cos \psi=0,\; \psi= \frac{\pi}{2}, \frac{3\pi}{2}..\; \tag 3$$ Differentiating (1) $$ 2a \psi{''}=\dfrac{2T^2\cos \psi}{r^3} \tag4$$ from (3) $$ \psi^{''}=0 \tag5 $$ Is it then a correct criterion for finding max/min radii in general ? And further on how can we find $ (r, \theta ) ? $ Towards a solution for $\psi$ Let $ u=\dfrac{\sin \psi}{r} \to u'= ( r \cos \psi\psi^{'} -\sin \psi \cos \psi)/r^2 \tag6$ Eliminating $ \psi^{'}$ between (1) and (6) and simplifying we obtain $$\dfrac{u'}{\sqrt{1-u^2r^2}}+ \dfrac{u}{r}= \dfrac{1-T^2/r^2}{2ar} \tag 7 $$ whose integral analytical solution $ u=f(r) $ is required. Thanking you for the attention.","How to find maximum minimum radii and inflection point of a curve with the polar symmetry given by the following ode ? Primed on arc,the angle is made between curve and radial lines. Patterns above are drawn for constants after numerical integration. The outer maximum radii  are which could not be found analytically. In polar coordinates ... is the following true for local straightness ? Pictorially it does not agree for the first pattern (no inflection), but seems a better tally for the second case where: To find radial extrema Differentiating (1) from (3) Is it then a correct criterion for finding max/min radii in general ? And further on how can we find Towards a solution for Let Eliminating between (1) and (6) and simplifying we obtain whose integral analytical solution is required. Thanking you for the attention.","\psi \theta \text{=const.}  2a \psi'=\left(1-\dfrac{T^2}{r^2}\right);\;r'= \cos \psi \; \tag1   a=1,T= (4,2) ,\theta_i=\pi/4, r_i=1,s_{max=}=320 a,   (\approx \sqrt2,2) \psi^{'}_{inflection}= {0}. \tag 2   r_{inflection} =T \; \cos \psi=0,\; \psi= \frac{\pi}{2}, \frac{3\pi}{2}..\; \tag 3  2a \psi{''}=\dfrac{2T^2\cos \psi}{r^3} \tag4  \psi^{''}=0 \tag5   (r, \theta ) ?  \psi  u=\dfrac{\sin \psi}{r} \to u'= ( r \cos \psi\psi^{'} -\sin \psi \cos \psi)/r^2 \tag6  \psi^{'} \dfrac{u'}{\sqrt{1-u^2r^2}}+ \dfrac{u}{r}= \dfrac{1-T^2/r^2}{2ar} \tag 7   u=f(r) ","['ordinary-differential-equations', 'differential-geometry', 'polar-coordinates']"
25,Vector valued linear differential equation,Vector valued linear differential equation,,"Let $I$ be an interval in $\mathbb{R}$ and suppose. $$\bf \dot{x(t)} =A(t)x(t),t\in I\hspace{1cm}(\star)$$ is a linear first order differential equation with $A$ an $n\times n$ matrix of continuous functions on an $I$ . Suppose $\bf y_1(t), . . . ,y_n(t)$ are $n$ solutions of this differential equation on $I$ .  Let $W=W(\bf y_1, . . . ,y_n) $ $:I\rightarrow \mathbb{R}$ be the function given by $W(t) = \det [\bf y_1(t), . . . ,y_n(t)],(t∈I)$ ,where $[\bf y_1(t), . . . ,y_n(t)]$ is regarded an $n\times n$ matrix whose $i^{th}$ column is $\bf y_i(t)$ $,i= 1, . . . , n.$ We need to show that either $W$ is identically zero on $I$ or it is nowhere vanishing on $I$ . My idea as below: $[\bf y_1(t), . . . ,y_n(t)]$ are solution of $(\star) $ and $V:=\{x(t):\, (\star) \}$ is clearly $n$ dimensional $\mathbb{R}-$ vector space. Note that, $\bf y_i(t)$ $\in V,i= 1, . . . , n.$ Now if $[\bf y_1(t), . . . ,y_n(t)]$ form a basis ( linearly independent is enough here)then results directly follows as in a $n\times n$ matrix, $\det$ is non zero iff all columns of it are linearly independent. Is this a correct strategy? Any helpful comments or answer are welcome and thank for that.","Let be an interval in and suppose. is a linear first order differential equation with an matrix of continuous functions on an . Suppose are solutions of this differential equation on .  Let be the function given by ,where is regarded an matrix whose column is We need to show that either is identically zero on or it is nowhere vanishing on . My idea as below: are solution of and is clearly dimensional vector space. Note that, Now if form a basis ( linearly independent is enough here)then results directly follows as in a matrix, is non zero iff all columns of it are linearly independent. Is this a correct strategy? Any helpful comments or answer are welcome and thank for that.","I \mathbb{R} \bf \dot{x(t)} =A(t)x(t),t\in I\hspace{1cm}(\star) A n\times n I \bf y_1(t), . . . ,y_n(t) n I W=W(\bf y_1, . . . ,y_n)  :I\rightarrow \mathbb{R} W(t) = \det [\bf y_1(t), . . . ,y_n(t)],(t∈I) [\bf y_1(t), . . . ,y_n(t)] n\times n i^{th} \bf y_i(t) ,i= 1, . . . , n. W I I [\bf y_1(t), . . . ,y_n(t)] (\star)  V:=\{x(t):\, (\star) \} n \mathbb{R}- \bf y_i(t) \in V,i= 1, . . . , n. [\bf y_1(t), . . . ,y_n(t)] n\times n \det",['ordinary-differential-equations']
26,Find a solution to $tx''+x'+2x = 0$,Find a solution to,tx''+x'+2x = 0,"Find a nontrivial (no need to be general) solution to $$tx''(t)+x'(t)+2x(t) = 0$$ W/A spells the solution to some Bessel function. I tried to use the series method so I derive $$ x = \sum_{i=0}^\infty a_nx^n, \quad a_{n+1} = \frac{-2}{(n+1)^2} a_n $$ well it is not the cleanest solution for this, so I'd like to ask if a explicit solution (non-Bessel function) is available.","Find a nontrivial (no need to be general) solution to W/A spells the solution to some Bessel function. I tried to use the series method so I derive well it is not the cleanest solution for this, so I'd like to ask if a explicit solution (non-Bessel function) is available.","tx''(t)+x'(t)+2x(t) = 0 
x = \sum_{i=0}^\infty a_nx^n, \quad a_{n+1} = \frac{-2}{(n+1)^2} a_n
",['ordinary-differential-equations']
27,Regular Lagrangian flows on a domain of $\mathbb{R}^d$ with a boundary,Regular Lagrangian flows on a domain of  with a boundary,\mathbb{R}^d,"I'm looking for some references about the theory of regular Lagrangian flows on a smooth domain $\Omega$ of $\mathbb{R}^d$ (say a smooth bounded open set of $\mathbb{R}^d$ or a half space). Here, regular Lagrangian flows refer to the ones developped in the theory of Ambrosio which intends to explore the link between continuity equations : $\partial_t \rho +\mathrm{div}_x(b \rho)=0$ and ordinary differenrial equations : $\frac{d}{d t} X(t,x)=b(t,X(t,x))$ with $X(0,x)=x $ , for a vector field $b=b(t,x)$ which is not smooth (namely with Sobolev or BV regularity in space). See for instance Definition 13 p.13 in http://php.math.unifi.it/users/cime/Courses/2005/02/CIME-2005-Ambrosio-Lecture_Notes.pdf It actually extends the famous results of Di Perna and Lions of 1989 on transport and continuity equations. But all of these theories actually take place in $\mathbb{R}^d$ , namely the fixed vector field $b$ (and then $\rho=\rho(t,x)$ and $X=X(t,x)$ ) are defined (for the space variable) on the whole space i.e. $b : \mathbb{R}^+ \times\mathbb{R}^d \rightarrow \mathbb{R}^d$ . However, the theory of Di Perna and Lions can be extended to domains included in $\mathbb{R}^d$ and which have a boundary , by constructing renormalized solutions to these equations where there is of course an additional boundary condition (see for instance https://hal.archives-ouvertes.fr/hal-00004420/document for results on a bounded domain with an absorption boundary condition) I am therefore wondering if regular Lagrangian flows can be also defined and used on a domain with a boundary.","I'm looking for some references about the theory of regular Lagrangian flows on a smooth domain of (say a smooth bounded open set of or a half space). Here, regular Lagrangian flows refer to the ones developped in the theory of Ambrosio which intends to explore the link between continuity equations : and ordinary differenrial equations : with , for a vector field which is not smooth (namely with Sobolev or BV regularity in space). See for instance Definition 13 p.13 in http://php.math.unifi.it/users/cime/Courses/2005/02/CIME-2005-Ambrosio-Lecture_Notes.pdf It actually extends the famous results of Di Perna and Lions of 1989 on transport and continuity equations. But all of these theories actually take place in , namely the fixed vector field (and then and ) are defined (for the space variable) on the whole space i.e. . However, the theory of Di Perna and Lions can be extended to domains included in and which have a boundary , by constructing renormalized solutions to these equations where there is of course an additional boundary condition (see for instance https://hal.archives-ouvertes.fr/hal-00004420/document for results on a bounded domain with an absorption boundary condition) I am therefore wondering if regular Lagrangian flows can be also defined and used on a domain with a boundary.","\Omega \mathbb{R}^d \mathbb{R}^d \partial_t \rho +\mathrm{div}_x(b \rho)=0 \frac{d}{d t} X(t,x)=b(t,X(t,x)) X(0,x)=x  b=b(t,x) \mathbb{R}^d b \rho=\rho(t,x) X=X(t,x) b : \mathbb{R}^+ \times\mathbb{R}^d \rightarrow \mathbb{R}^d \mathbb{R}^d","['ordinary-differential-equations', 'measure-theory', 'partial-differential-equations', 'transport-equation']"
28,Inverse Laplace Transform hint,Inverse Laplace Transform hint,,"I am away from Laplace transform for years, and now I have to solve $$\mathcal{L}^{-1} \left\{ s^{-\frac32}\sqrt{\frac{as+b}{cs+b}} \right\}(t),$$ where $a,b,c$ are real positive numbers. I can find inverse Laplace for $s^{-k}$ but really I got stuck on this. Please guide me to solve this form. Thanks in advance. Implicitly it has the condition $\lim\limits_{s\to\infty}F(s)=0$ Honestly I don't know what should I do for $\sqrt{\frac{as+b}{cs+b}}$ .","I am away from Laplace transform for years, and now I have to solve where are real positive numbers. I can find inverse Laplace for but really I got stuck on this. Please guide me to solve this form. Thanks in advance. Implicitly it has the condition Honestly I don't know what should I do for .","\mathcal{L}^{-1} \left\{ s^{-\frac32}\sqrt{\frac{as+b}{cs+b}} \right\}(t), a,b,c s^{-k} \lim\limits_{s\to\infty}F(s)=0 \sqrt{\frac{as+b}{cs+b}}","['integration', 'ordinary-differential-equations', 'laplace-transform']"
29,Method of Frobenius for $x^2y^{''}+x(1-x)y^{'}-(1-2x)y=0$ about $x_0=0$,Method of Frobenius for  about,x^2y^{''}+x(1-x)y^{'}-(1-2x)y=0 x_0=0,"I'm trying to solve this rather mean looking ODE by Method of Frobenius and could use some help on finding the first linearly independent solution. $x^2y^{''}+x(1+x)y^{'}-(1-2x)y=0$ about $x_0=0$ The point $x_0=0$ is a regular singular point as the following limits exist, $p_0=\lim\limits_{x \to 0}[(x)\frac{x(1+x)}{(x^2)}]=1$ and $q_0=\lim\limits_{x \to 0}[(x^2)\frac{-(1-2x)}{(x^2)}]=-1$ The indicial equation will be of the form $r(r-1)+p_0r+q_0$ . Substituting in the values of $q_o$ and $q_0$ from above we find, $r^2-1=0$ with the roots $r=\pm1$ As $r_1-r_2=(1)-(-1)=2$ is a positive integer, we will have our first and second solutions in the form, $y_1=\sum_{n=0}^\infty a_n(x)^{n+r^1}$ and $y_2=Cy_1(x) \ln(x-x_0)+\sum_{n=0}^\infty b_n(x)^{n+r^1}$ . As stated before, I'm having trouble finding the series solution for $y_1$ . Here is my work so far, $y=\sum_{n=0}^\infty a_n(x)^{n+r}$ , $y'=\sum_{n=0}^\infty(n+r)a_n(x)^{n+r-1}$ , $y''=\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r-2}$ Substitute the above series into the given ODE, $x^2[\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r-2}]+(x+x^2)[\sum_{n=0}^\infty(n+r)a_n(x)^{n+r-1}]-(1-2x)[\sum_{n=0}^\infty a_n(x)^{n+r}]=0$ $\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r}+\sum_{n=0}^\infty(n+r)a_n(x)^{n+r}+\sum_{n=0}^\infty(n+r)a_n(x)^{n+r+1}-\sum_{n=0}^\infty a_n(x)^{n+r}+2\sum_{n=0}^\infty a_n(x)^{n+r+1}=0$ Now shift the index so all series are of $x^k$ , $\sum_{k=r}^\infty k(k-1) a_{k-r}x^k+\sum_{k=r}^\infty k a_{k-r}x^k+\sum_{k=r+1}^\infty (k+1) a_{k-r-1}x^k-\sum_{k=r}^\infty a_{k-r}x^k+2\sum_{k=r+1}^\infty a_{k-r-1}x^k=0$ $\sum_{k=r}^\infty [k(k-1)+k-1]a_{k-r}+\sum_{k=r+1}^\infty [(k+1)+2]a_{k-r-1}x^k=0$ Expand the first summation for $k=r$ , $r(r-1)+r-1]a_0x^r+\sum_{k=r+1}^\infty [k(k-1)+k-1]a_{k-1}x^k+\sum_{k=r+1}^\infty [(k+1)+2]a_{k-r-1}x^k=0$ Choose the largest root $r_1=1$ and further collect the series under one grouping, $0a_0x^r+\sum_{k=2}^\infty([k(k-1)+k-1]a_{k-1}+[(k+1)+2]a_{k-2})x^r=0$ Now set the coefficients equal to zero to find the recurrence relation, $[k(k-1)+k-1]a_{k-1}+[(k+1)+2]a_{k-2}=0$ , for all $k\geq2$ $a_{k-1}=-\frac {k+3}{k^2-1}a_{k-2}$ , for all $k\geq2$ Now this is where I begin to fail in finding the first solution. As I choose values of k, there is no observable pattern for the coefficients of $a$ . Could I please receive some help on how to finish this question or identifying where I went wrong?","I'm trying to solve this rather mean looking ODE by Method of Frobenius and could use some help on finding the first linearly independent solution. about The point is a regular singular point as the following limits exist, and The indicial equation will be of the form . Substituting in the values of and from above we find, with the roots As is a positive integer, we will have our first and second solutions in the form, and . As stated before, I'm having trouble finding the series solution for . Here is my work so far, , , Substitute the above series into the given ODE, Now shift the index so all series are of , Expand the first summation for , Choose the largest root and further collect the series under one grouping, Now set the coefficients equal to zero to find the recurrence relation, , for all , for all Now this is where I begin to fail in finding the first solution. As I choose values of k, there is no observable pattern for the coefficients of . Could I please receive some help on how to finish this question or identifying where I went wrong?","x^2y^{''}+x(1+x)y^{'}-(1-2x)y=0 x_0=0 x_0=0 p_0=\lim\limits_{x \to 0}[(x)\frac{x(1+x)}{(x^2)}]=1 q_0=\lim\limits_{x \to 0}[(x^2)\frac{-(1-2x)}{(x^2)}]=-1 r(r-1)+p_0r+q_0 q_o q_0 r^2-1=0 r=\pm1 r_1-r_2=(1)-(-1)=2 y_1=\sum_{n=0}^\infty a_n(x)^{n+r^1} y_2=Cy_1(x)
\ln(x-x_0)+\sum_{n=0}^\infty b_n(x)^{n+r^1} y_1 y=\sum_{n=0}^\infty a_n(x)^{n+r} y'=\sum_{n=0}^\infty(n+r)a_n(x)^{n+r-1} y''=\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r-2} x^2[\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r-2}]+(x+x^2)[\sum_{n=0}^\infty(n+r)a_n(x)^{n+r-1}]-(1-2x)[\sum_{n=0}^\infty a_n(x)^{n+r}]=0 \sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r}+\sum_{n=0}^\infty(n+r)a_n(x)^{n+r}+\sum_{n=0}^\infty(n+r)a_n(x)^{n+r+1}-\sum_{n=0}^\infty a_n(x)^{n+r}+2\sum_{n=0}^\infty a_n(x)^{n+r+1}=0 x^k \sum_{k=r}^\infty k(k-1) a_{k-r}x^k+\sum_{k=r}^\infty k a_{k-r}x^k+\sum_{k=r+1}^\infty (k+1) a_{k-r-1}x^k-\sum_{k=r}^\infty a_{k-r}x^k+2\sum_{k=r+1}^\infty a_{k-r-1}x^k=0 \sum_{k=r}^\infty [k(k-1)+k-1]a_{k-r}+\sum_{k=r+1}^\infty [(k+1)+2]a_{k-r-1}x^k=0 k=r r(r-1)+r-1]a_0x^r+\sum_{k=r+1}^\infty [k(k-1)+k-1]a_{k-1}x^k+\sum_{k=r+1}^\infty [(k+1)+2]a_{k-r-1}x^k=0 r_1=1 0a_0x^r+\sum_{k=2}^\infty([k(k-1)+k-1]a_{k-1}+[(k+1)+2]a_{k-2})x^r=0 [k(k-1)+k-1]a_{k-1}+[(k+1)+2]a_{k-2}=0 k\geq2 a_{k-1}=-\frac {k+3}{k^2-1}a_{k-2} k\geq2 a",['ordinary-differential-equations']
30,Lyapunov Exponents vs Lyapunov Function,Lyapunov Exponents vs Lyapunov Function,,"Suppose I have a system of differential equations $\dot {\vec x} = \vec f(\vec x)$ , and that there is an equilibrium point $\vec x = 0$ . Moreover, suppose I know the Lyapunov exponents of the equilibrium $\vec x = 0$ . Is there a way to calculate the Lyapunov function $V(\vec x)$ given the Lyapunov exponents?","Suppose I have a system of differential equations , and that there is an equilibrium point . Moreover, suppose I know the Lyapunov exponents of the equilibrium . Is there a way to calculate the Lyapunov function given the Lyapunov exponents?",\dot {\vec x} = \vec f(\vec x) \vec x = 0 \vec x = 0 V(\vec x),"['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
31,Stability of Solutions to Linear ODE,Stability of Solutions to Linear ODE,,"The questions is: Check the stability of the solutions to the following equation: $$ y^{(11)}-|cos(t)|y^{(10)} +3t^3ArcTan(t)y=1 $$ My take: --- Solved-- I used Abel's formula $$ \dot W=|Cos(t)|W$$ and got that (edit: it's wrong) $$ W(t) = W(0) \cdot e^{|sint|} $$ I'm not sure if it's enough to show that $W \not \xrightarrow[t\to\infty]{} \infty $ in order to ensure stability of the ODE. Edit: the Wronskian is NOT what I stated: $W(0) \cdot e^{|Sin(t)|}$ because the integral: $$\int_{0}^{t} |Cos(s)| ds \not =|sin(t)| $$ the limit of that integral when t approaches infinity is $\infty$ , $$, W\xrightarrow[t\to\infty]{} e^\infty =\infty$$ from that we can infer that one of the columns of the fundemental matrix is not bounded (because the determinant of that matrix (the Wronskian) is approaching infinity). and from that we can say that one solution is not stable, so every other solution is not stable. - (if I made a mistake please correct me)","The questions is: Check the stability of the solutions to the following equation: My take: --- Solved-- I used Abel's formula and got that (edit: it's wrong) I'm not sure if it's enough to show that in order to ensure stability of the ODE. Edit: the Wronskian is NOT what I stated: because the integral: the limit of that integral when t approaches infinity is , from that we can infer that one of the columns of the fundemental matrix is not bounded (because the determinant of that matrix (the Wronskian) is approaching infinity). and from that we can say that one solution is not stable, so every other solution is not stable. - (if I made a mistake please correct me)","
y^{(11)}-|cos(t)|y^{(10)} +3t^3ArcTan(t)y=1
  \dot W=|Cos(t)|W  W(t) = W(0) \cdot e^{|sint|}  W \not \xrightarrow[t\to\infty]{} \infty  W(0) \cdot e^{|Sin(t)|} \int_{0}^{t} |Cos(s)| ds \not =|sin(t)|  \infty , W\xrightarrow[t\to\infty]{} e^\infty =\infty","['ordinary-differential-equations', 'stability-in-odes', 'wronskian']"
32,Formulas's deduction from the generating function of Hermite polynomials,Formulas's deduction from the generating function of Hermite polynomials,,"In the book ""Essential Mathematical Methods for Physicists"" comes the following problem that I am trying to solve: at first I could see that the first formula that is given as an answer is wrong since the hermite polynomial $ H_4 (x) $ does not match with the known since: $\begin{align*} H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}(-1)^{2s}(2x)^{2s}\cfrac{(2n)!}{(2s)!(n-s)!}\\ H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}\left[(-1)^{2}\right]^s\left[(2x)^{2}\right]^s\cfrac{(2n)!}{(2s)!(n-s)!}\\ H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}\left[1\right]^s\left[4x^2\right]^s\cfrac{(2n)!}{(2s)!(n-s)!}\\ H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}\left[4x^2\right]^s\cfrac{(2n)!}{(2s)!(n-s)!}\\ H_{2(2)}(x)&=(-1)^{2}\sum_{s=0}^{2}\left(4x^2\right)^s\cfrac{(2\cdot 2)!}{(2s)!(2-s)!}\\ H_{4}(x)&=(1)\sum_{s=0}^{2}\left(4x^2\right)^s\cfrac{(4)!}{(2s)!(2-s)!}\\ H_{4}(x)&=\sum_{s=0}^{2}\left(4x^2\right)^s\cfrac{24}{(2s)!(2-s)!}\\ H_{4}(x)&=(4x^2)^{0}\cfrac{24}{(2\cdot 0)!(2-0)!}+(4x^2)^{1}\cfrac{24}{(2\cdot 1)!(2-1)!}+(4x^2)^{2}\cfrac{24}{(2\cdot 2)!(2-2)!}\\ H_{4}(x)&=(1)\cfrac{24}{(0)!(2)!}+(4x^2)\cfrac{24}{(2)!(1)!}+(16x^4)\cfrac{24}{(4)!(0)!}\\ H_{4}(x)&=(1)\cfrac{24}{(1)(2)}+(4x^2)\cfrac{24}{(2)(1)}+(16x^4)\cfrac{24}{(24)(1)}\\ H_{4}(x)&=12+(4x^2)(12)+(16x^4)\\ H_{4}(x)&=12+48x^2+16x^4\\ H_{4}(x)&=16x^4+48x^2+12 \\ \end{align*}$ But the real $H_4(x)$ is $H_4(x)=16x^4-48x^2+12$ so the $H_{2n}$ formula is wrong. But trying to find errors in the formula for $ H_ {2n + 1} $ I did not find any for the polynomials 1,3,5 so I think that this formula is correct. If the generating function of the hermite polynomials is $g (x, t) = e^{-t^2 + 2tx} = \sum_{n = 0} ^ {\infty}H_n (x) \cfrac{t ^ n}{n!} $ How can the formula for $H_{2n + 1}$ be derived? I tried using the formula $H_n(x)=\sum_{s=0}^{[n/2]}(-1)^s\cfrac{n!}{(n-2s)!s!}(2x)^{n-2s}$ ( which is just the equation 13.40 that mentions the problem) substituting in the value of 2n + 1 but couldn't deduce anything, as there were terms that I couldn't adjust to look like the one the book asks for an answer. Any help is really appreciated!","In the book ""Essential Mathematical Methods for Physicists"" comes the following problem that I am trying to solve: at first I could see that the first formula that is given as an answer is wrong since the hermite polynomial does not match with the known since: But the real is so the formula is wrong. But trying to find errors in the formula for I did not find any for the polynomials 1,3,5 so I think that this formula is correct. If the generating function of the hermite polynomials is How can the formula for be derived? I tried using the formula ( which is just the equation 13.40 that mentions the problem) substituting in the value of 2n + 1 but couldn't deduce anything, as there were terms that I couldn't adjust to look like the one the book asks for an answer. Any help is really appreciated!"," H_4 (x)  \begin{align*}
H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}(-1)^{2s}(2x)^{2s}\cfrac{(2n)!}{(2s)!(n-s)!}\\
H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}\left[(-1)^{2}\right]^s\left[(2x)^{2}\right]^s\cfrac{(2n)!}{(2s)!(n-s)!}\\
H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}\left[1\right]^s\left[4x^2\right]^s\cfrac{(2n)!}{(2s)!(n-s)!}\\
H_{2n}(x)&=(-1)^{n}\sum_{s=0}^{n}\left[4x^2\right]^s\cfrac{(2n)!}{(2s)!(n-s)!}\\
H_{2(2)}(x)&=(-1)^{2}\sum_{s=0}^{2}\left(4x^2\right)^s\cfrac{(2\cdot 2)!}{(2s)!(2-s)!}\\
H_{4}(x)&=(1)\sum_{s=0}^{2}\left(4x^2\right)^s\cfrac{(4)!}{(2s)!(2-s)!}\\
H_{4}(x)&=\sum_{s=0}^{2}\left(4x^2\right)^s\cfrac{24}{(2s)!(2-s)!}\\
H_{4}(x)&=(4x^2)^{0}\cfrac{24}{(2\cdot 0)!(2-0)!}+(4x^2)^{1}\cfrac{24}{(2\cdot 1)!(2-1)!}+(4x^2)^{2}\cfrac{24}{(2\cdot 2)!(2-2)!}\\
H_{4}(x)&=(1)\cfrac{24}{(0)!(2)!}+(4x^2)\cfrac{24}{(2)!(1)!}+(16x^4)\cfrac{24}{(4)!(0)!}\\
H_{4}(x)&=(1)\cfrac{24}{(1)(2)}+(4x^2)\cfrac{24}{(2)(1)}+(16x^4)\cfrac{24}{(24)(1)}\\
H_{4}(x)&=12+(4x^2)(12)+(16x^4)\\
H_{4}(x)&=12+48x^2+16x^4\\
H_{4}(x)&=16x^4+48x^2+12 \\
\end{align*} H_4(x) H_4(x)=16x^4-48x^2+12 H_{2n}  H_ {2n + 1}  g (x, t) = e^{-t^2 + 2tx} = \sum_{n = 0} ^ {\infty}H_n (x) \cfrac{t ^ n}{n!}  H_{2n + 1} H_n(x)=\sum_{s=0}^{[n/2]}(-1)^s\cfrac{n!}{(n-2s)!s!}(2x)^{n-2s}","['ordinary-differential-equations', 'generating-functions', 'hermite-polynomials']"
33,How do I numerically approximate a gradient term?,How do I numerically approximate a gradient term?,,"I am trying to solve a system of differential equations. My question is simple. I am using a discrete approximation for the first and second spatial derivatives in my equations, using the definition of a first and second derivative. The delta h is known. I want to understand the last term of this equation. How can I represent it using numerical approximations for the first and second derivative? I have tried distributing the derivative, however, it leads my model to be uunsolvable I am trying to replicate the results of a modeling paper. $$ \frac{d N A_{i}}{d t}=-\mu_{N} N A_{i}+\frac{r_{v}\left(z_{i}\right) N A_{B i}}{V_{T}\left(\bar{z}_{i}\right)}-\chi \nabla_{i}\left(N A_{i} \nabla_{i} T N F_{i}\right) $$","I am trying to solve a system of differential equations. My question is simple. I am using a discrete approximation for the first and second spatial derivatives in my equations, using the definition of a first and second derivative. The delta h is known. I want to understand the last term of this equation. How can I represent it using numerical approximations for the first and second derivative? I have tried distributing the derivative, however, it leads my model to be uunsolvable I am trying to replicate the results of a modeling paper.","
\frac{d N A_{i}}{d t}=-\mu_{N} N A_{i}+\frac{r_{v}\left(z_{i}\right) N A_{B i}}{V_{T}\left(\bar{z}_{i}\right)}-\chi \nabla_{i}\left(N A_{i} \nabla_{i} T N F_{i}\right)
","['calculus', 'ordinary-differential-equations']"
34,How to rewrite $\frac{dx}{dt}=a\cdot x$ into the form $\frac{dx}{x}=a\cdot dt$ using the chain rule,How to rewrite  into the form  using the chain rule,\frac{dx}{dt}=a\cdot x \frac{dx}{x}=a\cdot dt,I'm reading about ODE's in a calculus text. The goal is to solve the differential equation: $\frac{dx}{dt}=ax$ for $x(t)$ . The text says using the chain rule and substitution rewrite the equation as $\frac{dx}{x}=a\:dt$ . I'm not sure how to use the chain rule to do the rewriting. However after this step I was able to follow to the solution.,I'm reading about ODE's in a calculus text. The goal is to solve the differential equation: for . The text says using the chain rule and substitution rewrite the equation as . I'm not sure how to use the chain rule to do the rewriting. However after this step I was able to follow to the solution.,\frac{dx}{dt}=ax x(t) \frac{dx}{x}=a\:dt,['ordinary-differential-equations']
35,Positive definite solution to matrix differential equation (dynamical Riccati equation),Positive definite solution to matrix differential equation (dynamical Riccati equation),,"I am wondering how I can show that the solution ( $P$ ) to the following matrix differential equation $$\begin{align} \dot{P}(t) &= -A^{\top}P - PA + PBR^{-1}B^{\top}P - Q \\[3mm] P(t_{1}) &= S \end{align}$$ where $$\begin{align} A = \begin{bmatrix}-4&0\\0&0\end{bmatrix},\quad B = \begin{bmatrix}2\\-2\end{bmatrix},\quad Q = \begin{bmatrix}4&4\\4&4\end{bmatrix},\quad S = \begin{bmatrix}0&0\\0&0\end{bmatrix}\quad \text{and}\quad R= \begin{bmatrix}1&0\\0&1\end{bmatrix}, \end{align}$$ is positive definite (i.e., $P > 0$ ) without solving the matrix differential equation. Is there some way we can see this, maybe by first rewriting the RHS? What are some useful properties one could use? For those of you unfamiliar with this equation, there actually exists a unique solution $P$ to this equation if $Q$ and $S$ are positive semidefinite and $R$ positive definite, which they all are in case. It is also true that the solution $P$ , in general, is positive semidefinite. I do not have the proof for this, unfortunately.","I am wondering how I can show that the solution ( ) to the following matrix differential equation where is positive definite (i.e., ) without solving the matrix differential equation. Is there some way we can see this, maybe by first rewriting the RHS? What are some useful properties one could use? For those of you unfamiliar with this equation, there actually exists a unique solution to this equation if and are positive semidefinite and positive definite, which they all are in case. It is also true that the solution , in general, is positive semidefinite. I do not have the proof for this, unfortunately.","P \begin{align}
\dot{P}(t) &= -A^{\top}P - PA + PBR^{-1}B^{\top}P - Q \\[3mm]
P(t_{1}) &= S
\end{align} \begin{align}
A = \begin{bmatrix}-4&0\\0&0\end{bmatrix},\quad B = \begin{bmatrix}2\\-2\end{bmatrix},\quad Q = \begin{bmatrix}4&4\\4&4\end{bmatrix},\quad S = \begin{bmatrix}0&0\\0&0\end{bmatrix}\quad \text{and}\quad R= \begin{bmatrix}1&0\\0&1\end{bmatrix},
\end{align} P > 0 P Q S R P","['ordinary-differential-equations', 'matrix-equations', 'control-theory', 'positive-definite', 'optimal-control']"
36,Singular solution for Lagrange differential equation,Singular solution for Lagrange differential equation,,"I am familiar with the following theorem: Let $F(x,y,y')$ be defined, continuous, and with continuous first-order derivative with respect to $y$ and $y~$ . Then the singular solution of the ODE $F(x,y,y')=0$ satisfies the following: $F(x,y,y')=0$ and $F'_{y'}(x,y,y')=0$ . How can I use this theorem to find the form of the singular solution for Lagrange Differential Equation $y(x)=xf(y′)+g(y′)$ ? Thanks in advance.","I am familiar with the following theorem: Let be defined, continuous, and with continuous first-order derivative with respect to and . Then the singular solution of the ODE satisfies the following: and . How can I use this theorem to find the form of the singular solution for Lagrange Differential Equation ? Thanks in advance.","F(x,y,y') y y~ F(x,y,y')=0 F(x,y,y')=0 F'_{y'}(x,y,y')=0 y(x)=xf(y′)+g(y′)",['ordinary-differential-equations']
37,How to solve a third order nonlinear ODE (Falkner-Skan wedge equation),How to solve a third order nonlinear ODE (Falkner-Skan wedge equation),,How can I solve the following ODE numerically $$F'''+FF''+1-F'^2=0$$ $$F(0)=F'(0)=0\qquad F'(\infty)=1$$ Thank you.,How can I solve the following ODE numerically Thank you.,F'''+FF''+1-F'^2=0 F(0)=F'(0)=0\qquad F'(\infty)=1,"['ordinary-differential-equations', 'numerical-methods']"
38,Convergence of Sturm-Liouville eigenfunction expansions at the endpoints of an interval.,Convergence of Sturm-Liouville eigenfunction expansions at the endpoints of an interval.,,"Let $\{\phi_n\}_{n=0}^\infty$ be the eigenfunctions of the regular Sturm-Liouville problem \begin{align}  -(p\,\phi')' + q\, \phi = \lambda \, r \, \phi \quad &\textrm{for } x \in (x_1,x_2)\\   - a_i \, \phi(x_i)  + b_i\, (p\,\phi')(x_i) = 0 \quad &\textrm{for } i=1,2. \end{align} Assume that $p$ and $r$ are positive and twice continuously differentiable; assume that $q$ is continuous; the coefficients $a_i,b_i$ for $i=1,2$ are real. Let $F(x)$ be a twice continuously differentiable function on the interval $[x_1,x_2]$ . Under the above conditions, I know that \begin{equation}   \textrm{(I)} \quad \quad  F(x) = \sum_{n=0}^\infty \left(\int_{x_1}^{x_2} F(z) \, \phi_n(z)\, r(z)\, \textrm{dz} \right)\, \phi_n(x) \end{equation} with point-wise equality in the open interval $(x_1,x_2)$ . My question is: to what value does the end-point $(x=x_i)$ series \begin{equation} \textrm{(II)} \quad \quad  \sum_{n=0}^\infty \left(\int_{x_1}^{x_2} F(z) \, \phi_n(z)\, r(z)\, \textrm{dz} \right)\, \phi_n(x_i) \end{equation} converge to? Is there a general closed form expression? If $F(x)$ satisfies the same boundary conditions as the eigenfunctions $\phi_n$ , then I know that the series (I) converges to $F(x)$ uniformly on the closed interval $[x_1,x_2]$ (and so I obtain point-wise equality on the closed interval). On the other hand, if the eigenfunctions $\phi_n$ satisfy the simpler boundary conditions $\phi_n(x_i)=0$ then the endpoint series (II) must converge to zero. The series (I) must then have a finite-jump discontinuity at the end points, e.g., jumping from $\lim_{x\rightarrow x_2}F(x)$ to $0$ at $x=x_2$ . However, I am interested in the more general boundary conditions above. I am aware of closed-form expressions for the endpoint series in the case of a Fourier expansions; I am wondering whether an analogous expression exists for regular Sturm-Liouville expansions. Any references would be greatly appreciated. Edit: I've linked a related question here. Is there an analogous result for Sturm-Liouville series? Do we obtain point-wise  convergence to $F(x)$ on the closed interval $[x_1,x_2]$ whenever $b_1,b_2 \neq 0$ ? Edit #2: The Sturm-Liouville article on the Encyclopedia of Mathematics states that, with $b_1,b_2 \neq 0$ , the expansion (I) converges under the same conditions as a Cosine series for any $F\in L^1$ . Presumably, from the previous edit, this would imply that we obtain point-wise convergence to $F$ on the whole interval if $F$ is differentiable and $b_1,b_2 \neq 0$ . Unfortunately, I do not have access to the articles cited in the encyclopedia.","Let be the eigenfunctions of the regular Sturm-Liouville problem Assume that and are positive and twice continuously differentiable; assume that is continuous; the coefficients for are real. Let be a twice continuously differentiable function on the interval . Under the above conditions, I know that with point-wise equality in the open interval . My question is: to what value does the end-point series converge to? Is there a general closed form expression? If satisfies the same boundary conditions as the eigenfunctions , then I know that the series (I) converges to uniformly on the closed interval (and so I obtain point-wise equality on the closed interval). On the other hand, if the eigenfunctions satisfy the simpler boundary conditions then the endpoint series (II) must converge to zero. The series (I) must then have a finite-jump discontinuity at the end points, e.g., jumping from to at . However, I am interested in the more general boundary conditions above. I am aware of closed-form expressions for the endpoint series in the case of a Fourier expansions; I am wondering whether an analogous expression exists for regular Sturm-Liouville expansions. Any references would be greatly appreciated. Edit: I've linked a related question here. Is there an analogous result for Sturm-Liouville series? Do we obtain point-wise  convergence to on the closed interval whenever ? Edit #2: The Sturm-Liouville article on the Encyclopedia of Mathematics states that, with , the expansion (I) converges under the same conditions as a Cosine series for any . Presumably, from the previous edit, this would imply that we obtain point-wise convergence to on the whole interval if is differentiable and . Unfortunately, I do not have access to the articles cited in the encyclopedia.","\{\phi_n\}_{n=0}^\infty \begin{align}
 -(p\,\phi')' + q\, \phi = \lambda \, r \, \phi \quad &\textrm{for } x \in (x_1,x_2)\\
  - a_i \, \phi(x_i)  + b_i\, (p\,\phi')(x_i) = 0 \quad &\textrm{for } i=1,2.
\end{align} p r q a_i,b_i i=1,2 F(x) [x_1,x_2] \begin{equation}
  \textrm{(I)} \quad \quad  F(x) = \sum_{n=0}^\infty \left(\int_{x_1}^{x_2} F(z) \, \phi_n(z)\, r(z)\, \textrm{dz} \right)\, \phi_n(x)
\end{equation} (x_1,x_2) (x=x_i) \begin{equation}
\textrm{(II)} \quad \quad  \sum_{n=0}^\infty \left(\int_{x_1}^{x_2} F(z) \, \phi_n(z)\, r(z)\, \textrm{dz} \right)\, \phi_n(x_i)
\end{equation} F(x) \phi_n F(x) [x_1,x_2] \phi_n \phi_n(x_i)=0 \lim_{x\rightarrow x_2}F(x) 0 x=x_2 F(x) [x_1,x_2] b_1,b_2 \neq 0 b_1,b_2 \neq 0 F\in L^1 F F b_1,b_2 \neq 0","['real-analysis', 'sequences-and-series', 'ordinary-differential-equations', 'fourier-analysis', 'sturm-liouville']"
39,"""Piecewise Linear"" Differential Equation","""Piecewise Linear"" Differential Equation",,"I'm interested in the following ""piecewise-linear"" ordinary differential equation: $$ \ddot{x}(t) = \begin{cases} -k_+ x(t) - b_+ \dot x(t) + u(t) & x(t) \geq 0\\ -k_- x(t) - b_- \dot x(t) + u(t) & x(t) < 0, \end{cases} \quad x(0) = x_0, \quad \dot x(0) = \dot x_0, $$ where we are given some (smooth) forcing function $u(t)$ and positive coefficients $k_{\pm}$ and $b_{\pm}$ .  If you prefer, I have step-function coefficients $k(x)$ and $b(x)$ , and my equation is of the form $$ \ddot{x}(t) = -k(x)x(t) - b(x) x(t) + u(t). $$ Ultimately, I'd like to be able to show that for two solutions $x_1,x_2$ , we have $\frac d{dt}|x_1(t) - x_2(t)| < 0$ . The fact that this is not simply a linear differential equation makes this difficult. One approach to the problem is to solve the (countably many) initial value problems, where we switch from one ODE to the other at each time at which we have $x(t) = 0$ , and the ""final values"" for the trajectory over $[0,t]$ serve as the ""initial values"" for the next trajectory. Is there a better way to approach this problem? Does this fall into any well-known class of problems? I would appreciate any ideas and any helpful keywords to search or resources to look for.","I'm interested in the following ""piecewise-linear"" ordinary differential equation: where we are given some (smooth) forcing function and positive coefficients and .  If you prefer, I have step-function coefficients and , and my equation is of the form Ultimately, I'd like to be able to show that for two solutions , we have . The fact that this is not simply a linear differential equation makes this difficult. One approach to the problem is to solve the (countably many) initial value problems, where we switch from one ODE to the other at each time at which we have , and the ""final values"" for the trajectory over serve as the ""initial values"" for the next trajectory. Is there a better way to approach this problem? Does this fall into any well-known class of problems? I would appreciate any ideas and any helpful keywords to search or resources to look for.","
\ddot{x}(t) = \begin{cases}
-k_+ x(t) - b_+ \dot x(t) + u(t) & x(t) \geq 0\\
-k_- x(t) - b_- \dot x(t) + u(t) & x(t) < 0,
\end{cases}
\quad x(0) = x_0, \quad \dot x(0) = \dot x_0,
 u(t) k_{\pm} b_{\pm} k(x) b(x) 
\ddot{x}(t) = -k(x)x(t) - b(x) x(t) + u(t).
 x_1,x_2 \frac d{dt}|x_1(t) - x_2(t)| < 0 x(t) = 0 [0,t]","['ordinary-differential-equations', 'reference-request']"
40,The characteristic polynomial of an ODE is equal to the characteristic polynomial of the companion matrix,The characteristic polynomial of an ODE is equal to the characteristic polynomial of the companion matrix,,"One can write a homogeneous linear differential equation with real constant coefficients $a_k \in \mathbb R$ of degree $n \in \mathbb N$ \begin{equation} \tag{1}     0 = \sum_{k = 0}^{n} a_{k} \cdot u^{(k)}(t)     \qquad \text{with } a_n = 1 \end{equation} as a system of $n$ differential equations like so: \begin{align*} 	\begin{pmatrix} 	u \\ \vdots \\ u^{(n - 1)} 	\end{pmatrix}' 	 + \underbrace{\begin{pmatrix} 	 0 & - 1 & 0 & \ldots & 0 \\ 	 0 & 0 & - 1 & 0 &  0 \\ 	 \vdots & \ddots & \ddots & \ddots & 0 \\ 	 0 & \ldots & \ldots & 0 & -1 \\ 	 a_0 & a_1 & \ldots & \ldots & a_{n - 1} 	 \end{pmatrix}}_{:= A \in \mathbb R^{n \times n}} 	 \begin{pmatrix} 	 u \\ \vdots \\ u^{(n - 1)} 	 \end{pmatrix} 	 = \begin{pmatrix} 	 0 \\ \vdots \\ 0 	 \end{pmatrix} \end{align*} The characteristic polynomial of $(1)$ is defined to be $\chi(\lambda) := \sum_{k = 0}^{n} a_{k} \cdot \lambda^{k}$ with $a_n = 1$ . I want to show that the characterisitic polynomial of $A$ , $\det(\lambda I - A)$ , is equal to the characterisitic polynomial of the differential equation. Here's is what I tried to prove it. If $A = \begin{pmatrix} B & C \\ D & E \end{pmatrix}$ is a block matrix where $E$ is a scalar and thus $C$ and $D$ are vectors, we have $\det(A) = (E - D B^{-1} C) \det(B)$ , if $B$ is invertible. Defining $$B := \begin{pmatrix} \lambda & -1 & 0 & \ldots & 0 \\ 0 & \lambda & -1 & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots & 0 \\ 0 & \ldots & 0 & \lambda & - 1 \\ 0 & \ldots & \ldots & 0 & \lambda \end{pmatrix} \in \mathbb R^{(n - 1) \times (n - 1)}, \qquad C := \begin{pmatrix} 0 \\ \vdots \\ 0 \\ - 1 \end{pmatrix} \in \mathbb R^{n - 1}, \qquad D :=\begin{pmatrix} a_0 & a_1 & \ldots & a_{n - 3} & a_{n - 2} \end{pmatrix} \in \mathbb R^{1, n - 1}$$ I obtain for $\lambda \ne 0$ (as $\det(B) = \lambda^{n - 1}$ ) \begin{align*} \det(\lambda I - A) & = \det\begin{pmatrix} \lambda & 1 & 0 & \ldots & 0 \\ 0 & \lambda & 1 & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots & 0 \\ 0 & \ldots & 0 & \lambda & 1 \\ a_0 & a_1 & \ldots & a_{n - 2} & a_{n - 1} + \lambda \end{pmatrix} \\ & = \lambda^{n - 1} \left[ (a_{n - 1} + \lambda) - \begin{pmatrix} a_0& \ldots & a_{n - 2} \end{pmatrix} \begin{pmatrix} \lambda^{-1} & \lambda^{-2} & \ldots & \lambda^{-(n - 1)} \\ 0 & \lambda^{-1} & \ddots &  \vdots \\ \vdots & \ddots & \ddots & \vdots \\ 0 & \ldots & 0 & \lambda^{-1} \end{pmatrix} \begin{pmatrix} 0 \\ \vdots \\ 0 \\ - 1 \end{pmatrix} \right] \\ & = \lambda^{n - 1} \left[ (a_{n - 1} + \lambda) + \begin{pmatrix} a_0& \ldots & a_{n - 2} \end{pmatrix} \begin{pmatrix}\lambda^{-(n - 1)} \\ \vdots \\  \lambda^{-1} \end{pmatrix} \right] \\ & = \lambda^{n} + a_{n - 1} \lambda^{n - 1} + \begin{pmatrix} a_0 & \ldots & a_{n - 2} \end{pmatrix} \begin{pmatrix} 1 \\ \vdots \\  \lambda^{n-2} \end{pmatrix} \\ & = \lambda^{n} + a_{n - 1} \lambda^{n - 1} + \sum_{k = 0}^{n - 2} a_k \lambda^k.  \end{align*} Edit I miscalculated $B^{-1}$ , so now the calculation works. Is there a simpler way to show this result?","One can write a homogeneous linear differential equation with real constant coefficients of degree as a system of differential equations like so: The characteristic polynomial of is defined to be with . I want to show that the characterisitic polynomial of , , is equal to the characterisitic polynomial of the differential equation. Here's is what I tried to prove it. If is a block matrix where is a scalar and thus and are vectors, we have , if is invertible. Defining I obtain for (as ) Edit I miscalculated , so now the calculation works. Is there a simpler way to show this result?","a_k \in \mathbb R n \in \mathbb N \begin{equation} \tag{1}
    0 = \sum_{k = 0}^{n} a_{k} \cdot u^{(k)}(t)
    \qquad \text{with } a_n = 1
\end{equation} n \begin{align*}
	\begin{pmatrix}
	u \\ \vdots \\ u^{(n - 1)}
	\end{pmatrix}'
	 + \underbrace{\begin{pmatrix}
	 0 & - 1 & 0 & \ldots & 0 \\
	 0 & 0 & - 1 & 0 &  0 \\
	 \vdots & \ddots & \ddots & \ddots & 0 \\
	 0 & \ldots & \ldots & 0 & -1 \\
	 a_0 & a_1 & \ldots & \ldots & a_{n - 1}
	 \end{pmatrix}}_{:= A \in \mathbb R^{n \times n}}
	 \begin{pmatrix}
	 u \\ \vdots \\ u^{(n - 1)}
	 \end{pmatrix}
	 = \begin{pmatrix}
	 0 \\ \vdots \\ 0
	 \end{pmatrix}
\end{align*} (1) \chi(\lambda) := \sum_{k = 0}^{n} a_{k} \cdot \lambda^{k} a_n = 1 A \det(\lambda I - A) A = \begin{pmatrix} B & C \\ D & E \end{pmatrix} E C D \det(A) = (E - D B^{-1} C) \det(B) B B := \begin{pmatrix} \lambda & -1 & 0 & \ldots & 0 \\ 0 & \lambda & -1 & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots & 0 \\ 0 & \ldots & 0 & \lambda & - 1 \\ 0 & \ldots & \ldots & 0 & \lambda \end{pmatrix} \in \mathbb R^{(n - 1) \times (n - 1)}, \qquad
C := \begin{pmatrix} 0 \\ \vdots \\ 0 \\ - 1 \end{pmatrix} \in \mathbb R^{n - 1}, \qquad D :=\begin{pmatrix} a_0 & a_1 & \ldots & a_{n - 3} & a_{n - 2} \end{pmatrix} \in \mathbb R^{1, n - 1} \lambda \ne 0 \det(B) = \lambda^{n - 1} \begin{align*}
\det(\lambda I - A)
& = \det\begin{pmatrix} \lambda & 1 & 0 & \ldots & 0 \\
0 & \lambda & 1 & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \ldots & 0 & \lambda & 1 \\
a_0 & a_1 & \ldots & a_{n - 2} & a_{n - 1} + \lambda
\end{pmatrix} \\
& = \lambda^{n - 1} \left[ (a_{n - 1} + \lambda) - \begin{pmatrix} a_0& \ldots & a_{n - 2} \end{pmatrix}
\begin{pmatrix} \lambda^{-1} & \lambda^{-2} & \ldots & \lambda^{-(n - 1)} \\ 0 & \lambda^{-1} & \ddots &  \vdots \\ \vdots & \ddots & \ddots & \vdots \\ 0 & \ldots & 0 & \lambda^{-1} \end{pmatrix} \begin{pmatrix} 0 \\ \vdots \\ 0 \\ - 1 \end{pmatrix} \right] \\
& = \lambda^{n - 1} \left[ (a_{n - 1} + \lambda) + \begin{pmatrix} a_0& \ldots & a_{n - 2} \end{pmatrix}
\begin{pmatrix}\lambda^{-(n - 1)} \\ \vdots \\  \lambda^{-1} \end{pmatrix} \right] \\
& = \lambda^{n} + a_{n - 1} \lambda^{n - 1} + \begin{pmatrix} a_0 & \ldots & a_{n - 2} \end{pmatrix}
\begin{pmatrix} 1 \\ \vdots \\  \lambda^{n-2} \end{pmatrix} \\
& = \lambda^{n} + a_{n - 1} \lambda^{n - 1} + \sum_{k = 0}^{n - 2} a_k \lambda^k. 
\end{align*} B^{-1}","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'determinant', 'characteristic-polynomial']"
41,Solution of differential Equation using frobenius method,Solution of differential Equation using frobenius method,,"How to calculate the two linearly independent solutions of the differential equation x(x − 1)y'' + (3x−1)y'+ y = 0? My approach was by using the Frobenius method as x=0 and x=1 are regular singular points and then finding the roots of the indicial equation and then by seeing the difference of the roots I got the solution to be 1/(1-x), and by using the method of reduction the second one turns out to be log_e(x)/(1-x), but the second solution does not seem to satisfy the equation when I put it back in the DE. Where am I going wrong here?","How to calculate the two linearly independent solutions of the differential equation x(x − 1)y'' + (3x−1)y'+ y = 0? My approach was by using the Frobenius method as x=0 and x=1 are regular singular points and then finding the roots of the indicial equation and then by seeing the difference of the roots I got the solution to be 1/(1-x), and by using the method of reduction the second one turns out to be log_e(x)/(1-x), but the second solution does not seem to satisfy the equation when I put it back in the DE. Where am I going wrong here?",,"['ordinary-differential-equations', 'frobenius-method']"
42,Solving third order linear ODE using symmetric square basis,Solving third order linear ODE using symmetric square basis,,"In M. van Hoeij talk on ""Solving Third Order Linear Differential Equations in Terms of Second Order Equations"" (2007) he mentions the following algorithm. ""Case 1 (the easiest case): Is $L$ the symmetric square of a second order operator? Let $L = \partial^3 + c_2(x) \partial^2 + c_1(x) \partial + c_0(x) $ . $L$ is a symmetric square if there exist functions $y_1, y_2$ such that $y_1^2, y_1y_2, y_2^2$ is a basis for its solutions. In this case, $y_1, y_2$ are solutions of some operator $L_2 = \partial^2 + a_1(x) \partial + a_0(x)$ . Testing if $L$ is symmetric square and finding $L_2$ is easy: take $a_1 = c_2/3, a_0 = (c_1 - a_1' -2a_1^2)/4$ , and test if $4a_0 a_1 + 2a'_0 = c_0$ ."" I tried looking up over the internet to find more references about this topic but unfortunately could not find anything. My basic questions are: How is result on how to find $L_2$ derived? How is test for $L$ being symmetric square derived? More advanced questions are: What is the intuition behind this method? How can one come up with it? Is there a generalization to $n$ -th order ODEs?","In M. van Hoeij talk on ""Solving Third Order Linear Differential Equations in Terms of Second Order Equations"" (2007) he mentions the following algorithm. ""Case 1 (the easiest case): Is the symmetric square of a second order operator? Let . is a symmetric square if there exist functions such that is a basis for its solutions. In this case, are solutions of some operator . Testing if is symmetric square and finding is easy: take , and test if ."" I tried looking up over the internet to find more references about this topic but unfortunately could not find anything. My basic questions are: How is result on how to find derived? How is test for being symmetric square derived? More advanced questions are: What is the intuition behind this method? How can one come up with it? Is there a generalization to -th order ODEs?","L L = \partial^3 + c_2(x) \partial^2 + c_1(x) \partial + c_0(x)  L y_1, y_2 y_1^2, y_1y_2, y_2^2 y_1, y_2 L_2 = \partial^2 + a_1(x) \partial + a_0(x) L L_2 a_1 = c_2/3, a_0 = (c_1 - a_1' -2a_1^2)/4 4a_0 a_1 + 2a'_0 = c_0 L_2 L n",['ordinary-differential-equations']
43,"Falkner Skan solution using a tridiagonal matrix method. For 0, -ve $\beta$ I get incorrect solutions?","Falkner Skan solution using a tridiagonal matrix method. For 0, -ve  I get incorrect solutions?",\beta,"I am trying to solve the Falkner Skan equation for wedge flows numerically, by first reducing it to a second order system and then solving a tridiagonal matrix equation iteratively till solution converges to a required tolerance. Essentially we have: $$ f''' + ff'' + \beta(1-f'^2) = 0\\ f(0)= f'(0)=0\\ f'(\infty)=1\\ $$ Here, we take $$ f' = G$$ And the equation becomes $$ G'' + G' * \int_{0}^{\eta} G d\eta + (1-G^2)\beta = 0\\ G(0) = f'(0)= 0\\ G(\infty=n) = 1$$ The nonlinear part in the RHS has been approximated as: $$(G^2 - 1) = 2G_{{j}_{old}}G_{j} - (G_{{j}_{old}})^2 - 1$$ G_{j_{old}} is the previous guess in the iterative scheme. We then use the central differences rules for G and write the equation in terms of $G_{j-1}, G_j, G_{j+1}$ over a non-uniform grid of n grid points from 0 to $\eta_n$ i.e. $\eta_j$ where j varies from 0 to n. The equation is something like this, $$a_j G_{j-1} + b_j G_j +c_j G_{j+1} = -(G_{j_{old}}^2+1)\beta$$ And the tridiagonal matrix equation looks like this, \begin{equation}      \begin{bmatrix}     b_2 & c_2 & 0 & 0 & - & -\\     a_3 & b_3 & c_3 & 0 & - & - \\     0 & a_4 & b_4 & c_4 & 0 & -\\     0 & 0 & a_5 & b_5 & c_5 & -\\     0 & 0 & 0 & - & - & - \\     - & - &- & a_{n-2} & b_{n-2} & c_{n-2}\\     - & - & - & - & a_{n-1} & b_{n-1}     \end{bmatrix}  \times \begin{bmatrix}     G_2 \\ G_3 \\ G_4\\ G_5\\-\\-\\ G_{n-1}     \end{bmatrix} = \begin{bmatrix}     -\beta(1+G^{old^2}_{2})-a_2 G_1 \\ -\beta(1+G^{old^2}_{3}) \\ -\beta(1+G^{old^2}_{4})\\ -\beta(1+G^{old^2}_{5})\\-\\-\\ -\beta(1+G^{old^2}_{n-1}) - c_{n-1} G_n     \end{bmatrix} \end{equation} And, now we use the Thomas algorithm to invert the following equation and find $G (or f')$ from this, $$ AG = R $$ I solve this in Fortran and my results look like this: They are supposed to look like this I am unable to understand why this works for positive values of $\beta$ but fails for negative and zero.","I am trying to solve the Falkner Skan equation for wedge flows numerically, by first reducing it to a second order system and then solving a tridiagonal matrix equation iteratively till solution converges to a required tolerance. Essentially we have: Here, we take And the equation becomes The nonlinear part in the RHS has been approximated as: G_{j_{old}} is the previous guess in the iterative scheme. We then use the central differences rules for G and write the equation in terms of over a non-uniform grid of n grid points from 0 to i.e. where j varies from 0 to n. The equation is something like this, And the tridiagonal matrix equation looks like this, And, now we use the Thomas algorithm to invert the following equation and find from this, I solve this in Fortran and my results look like this: They are supposed to look like this I am unable to understand why this works for positive values of but fails for negative and zero."," f''' + ff'' + \beta(1-f'^2) = 0\\
f(0)= f'(0)=0\\
f'(\infty)=1\\
  f' = G  G'' + G' * \int_{0}^{\eta} G d\eta + (1-G^2)\beta = 0\\
G(0) = f'(0)= 0\\
G(\infty=n) = 1 (G^2 - 1) = 2G_{{j}_{old}}G_{j} - (G_{{j}_{old}})^2 - 1 G_{j-1}, G_j, G_{j+1} \eta_n \eta_j a_j G_{j-1} + b_j G_j +c_j G_{j+1} = -(G_{j_{old}}^2+1)\beta \begin{equation}
     \begin{bmatrix}
    b_2 & c_2 & 0 & 0 & - & -\\
    a_3 & b_3 & c_3 & 0 & - & - \\
    0 & a_4 & b_4 & c_4 & 0 & -\\
    0 & 0 & a_5 & b_5 & c_5 & -\\
    0 & 0 & 0 & - & - & - \\
    - & - &- & a_{n-2} & b_{n-2} & c_{n-2}\\
    - & - & - & - & a_{n-1} & b_{n-1}
    \end{bmatrix}  \times \begin{bmatrix}
    G_2 \\ G_3 \\ G_4\\ G_5\\-\\-\\ G_{n-1}
    \end{bmatrix} = \begin{bmatrix}
    -\beta(1+G^{old^2}_{2})-a_2 G_1 \\ -\beta(1+G^{old^2}_{3}) \\ -\beta(1+G^{old^2}_{4})\\ -\beta(1+G^{old^2}_{5})\\-\\-\\ -\beta(1+G^{old^2}_{n-1}) - c_{n-1} G_n
    \end{bmatrix}
\end{equation} G (or f')  AG = R  \beta","['ordinary-differential-equations', 'partial-differential-equations', 'problem-solving', 'fluid-dynamics', 'tridiagonal-matrices']"
44,Solving ODE knowing general solution of a linear ODE.,Solving ODE knowing general solution of a linear ODE.,,"Consider the following ODE. $$ f''(x) + p(x)f'(x) + q(x)f(x) = 0 $$ Assume that it has general solution $f(x) = c_1 f_1(x) + c_2 f_2(x)$ . Now, consider the following ODE. $$ g'''(x) + g'(x)(-2p'(x)-p^2(x)+4q(x)) + g(x)(-p''(x)-p(x)p'(x)+2q'(x)) = 0 $$ The claim is that it is possible to obtain solution of $g(x)$ in terms of $f_1$ and $f_2$ . It is not clear to me how. The only thing that I have noticed (not clear if useful) that ODE can be rewritten in the following way. $$ g'''(x) -2g'(x)(p'(x)+\frac{p^2}{2}(x)-2q(x)) -g(x)(p'+\frac{p^2}{2}-2q)'(x) = 0 $$ It looks very close to the product rule but not quite as there is factor of $2$ difference between terms. I found that solution is supposed to be the following which maybe can help to guess how to find this solution. $$ g(x) = e^{\int p(x) dx} \left( c_1 f_1^2(x) + c_2 f_1(x)f_2(x) + c_3 f_2^2(x) \right)$$ I considered a new function $h(x) = g(x) e^{-\int p(x) dx}$ and found a differential equation that it satisfies. $$h'''+3ph''+(p'+2p^2+4q)h'+(2q'+4pq)h = 0$$ It is not clear to me how this weight factor $e^{-\int p(x) dx}$ is motivated because the resulting differential equation is still not obvious to me on how to solve it.","Consider the following ODE. Assume that it has general solution . Now, consider the following ODE. The claim is that it is possible to obtain solution of in terms of and . It is not clear to me how. The only thing that I have noticed (not clear if useful) that ODE can be rewritten in the following way. It looks very close to the product rule but not quite as there is factor of difference between terms. I found that solution is supposed to be the following which maybe can help to guess how to find this solution. I considered a new function and found a differential equation that it satisfies. It is not clear to me how this weight factor is motivated because the resulting differential equation is still not obvious to me on how to solve it.", f''(x) + p(x)f'(x) + q(x)f(x) = 0  f(x) = c_1 f_1(x) + c_2 f_2(x)  g'''(x) + g'(x)(-2p'(x)-p^2(x)+4q(x)) + g(x)(-p''(x)-p(x)p'(x)+2q'(x)) = 0  g(x) f_1 f_2  g'''(x) -2g'(x)(p'(x)+\frac{p^2}{2}(x)-2q(x)) -g(x)(p'+\frac{p^2}{2}-2q)'(x) = 0  2  g(x) = e^{\int p(x) dx} \left( c_1 f_1^2(x) + c_2 f_1(x)f_2(x) + c_3 f_2^2(x) \right) h(x) = g(x) e^{-\int p(x) dx} h'''+3ph''+(p'+2p^2+4q)h'+(2q'+4pq)h = 0 e^{-\int p(x) dx},['ordinary-differential-equations']
45,Using ODE to model a discrete phenomenon,Using ODE to model a discrete phenomenon,,"SETTING In this paper the authors study the effects of tourism in a national park in Austria on a endangered species of bird. They model the species size (number of breeding couples) with a continuous function of time $S = S(t)$ satisfying the following logistic ODE (with time dependent carrying capacity) $$ \dot{S}(t) = \gamma S(t)\left(1 - \frac{S(t)}{\beta H(t)}\right), \qquad S(0) = S_0 > 0 $$ where $\gamma, \beta > 0$ are constants and $H = H(t)$ , which represent the habitat extent, satisfies the following logistic ODE $$ \dot{H} = \alpha H(t) \left( 1 - \frac{H(t)}{K} \right), \qquad H(0) = H_0 >0, $$ with $K >0$ constant. QUESTION : How reasonable is to model the dynamic of the size of a population with a continuous function $S$ ? For a very large population, I wouldn't see any problem, but in the specific case analyzed in the paper, $S_0 = 12$ and they argue in the appendix A.2 that the partridge carrying capacity is of 16 breeding couples. Therefore the actual discrete function that the authors want to model has a very limited range and I wonder, how legitimate it is to use an ODE. A more realistic model would be to use a discrete state dynamic system to model the population dynamic (actually a stochastic process would probably be even more suitable) and I wonder how legitimate is to approximate such a system with an ODE. Any suggestion/idea is very welcome! Even heuristic arguments. PS : I am definitely more familiar with ODE than with discrete state systems.","SETTING In this paper the authors study the effects of tourism in a national park in Austria on a endangered species of bird. They model the species size (number of breeding couples) with a continuous function of time satisfying the following logistic ODE (with time dependent carrying capacity) where are constants and , which represent the habitat extent, satisfies the following logistic ODE with constant. QUESTION : How reasonable is to model the dynamic of the size of a population with a continuous function ? For a very large population, I wouldn't see any problem, but in the specific case analyzed in the paper, and they argue in the appendix A.2 that the partridge carrying capacity is of 16 breeding couples. Therefore the actual discrete function that the authors want to model has a very limited range and I wonder, how legitimate it is to use an ODE. A more realistic model would be to use a discrete state dynamic system to model the population dynamic (actually a stochastic process would probably be even more suitable) and I wonder how legitimate is to approximate such a system with an ODE. Any suggestion/idea is very welcome! Even heuristic arguments. PS : I am definitely more familiar with ODE than with discrete state systems.","S = S(t) 
\dot{S}(t) = \gamma S(t)\left(1 - \frac{S(t)}{\beta H(t)}\right), \qquad S(0) = S_0 > 0
 \gamma, \beta > 0 H = H(t) 
\dot{H} = \alpha H(t) \left( 1 - \frac{H(t)}{K} \right), \qquad H(0) = H_0 >0,
 K >0 S S_0 = 12","['ordinary-differential-equations', 'discrete-mathematics', 'dynamical-systems', 'mathematical-modeling', 'biology']"
46,Differential equation's domain?,Differential equation's domain?,,"I'm confused a little bit. I have a differential equation: $yy'\sqrt{(1-x^2)/(1-2y^2)}+1=0$ . I just looked at it and solved it without problems, it didn't look like a big deal. It's a nonlinear nonhomogeneous first order differential equation, right? You can separate variables here, so I did it. But when I checked the answer on Wolfram, the answer was different. CAS wxMaxima produced another solution. And then I remembered perhaps I should do something with the domain, or something with the expression I've divided by. I've never really understood those things, because my differential equations course was a chaotic one, no one ever really explained it decently. Oh, so, okay, my question. What I should do about the domain and range of the equation? It looks like $(1-x^2)/(1-2y^2)$ has to be greater than or equal to zero, because it is in the square root. So, numerator and denominator should both be greater than or equal to $0$ OR less than or equal to $0$ . Oh, and denominator shouldn't be equal to zero? But how I suppose to solve the dif.equation correctly? I did this. At first I solve the equation when $x \in [-1,1]$ and $y \in [-\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}]$ , I got the answer $\arcsin x - \frac{\sqrt{1-2y^2}}{2}=C$ . Then I took the case when $x \in [-\infty,-1] \cup [1, +\infty]$ and $y \in [-\infty, -\frac{\sqrt{2}}{2}] \cup [\frac{\sqrt{2}}{2}, +\infty]$ . The solution I got was $\frac{\sqrt{2y^2-1}}{2}+\ln{|\sqrt{x^2-1}+x|}=C$ I did it like that, but I don't really think it's a good solution. Perhaps solutions overlap each other and still not sure if it's a correct method to do this. Oh, and Wolfram gave me this answer: $\frac{1}{2}\sqrt{2y^2-1}=\frac{1}{2}\log(-\frac{x}{\sqrt{x^2-1}}+1)-\frac{1}{2}\log(\frac{x}{\sqrt{x^2-1}}+1)+C$ . CAS wxMaxima gave me this result: $\frac{2\sqrt{x^2-1}\log(2\sqrt{x^2-1}+2x)\sqrt{\frac{x^2-1}{2y^2-1}}+x^2+1}{2\sqrt{x^2-1}\sqrt{\frac{x^2-1}{2y^2-1}}}=C$ I know that you can simplify the answer of Wolfram and wxMaxima, but why they didn't simplify it? There must be reasons.. So, I don't know why these solutions are differently written, which one of them is the most accurate, how to solve exactly correctly, what should I do about the DE domain (not only this equation's, but also every other time, with different DE) and was my solution good? And sorry for my ignorance, I'm  really messed up :D I believe there's some small detail I don't understand at all..","I'm confused a little bit. I have a differential equation: . I just looked at it and solved it without problems, it didn't look like a big deal. It's a nonlinear nonhomogeneous first order differential equation, right? You can separate variables here, so I did it. But when I checked the answer on Wolfram, the answer was different. CAS wxMaxima produced another solution. And then I remembered perhaps I should do something with the domain, or something with the expression I've divided by. I've never really understood those things, because my differential equations course was a chaotic one, no one ever really explained it decently. Oh, so, okay, my question. What I should do about the domain and range of the equation? It looks like has to be greater than or equal to zero, because it is in the square root. So, numerator and denominator should both be greater than or equal to OR less than or equal to . Oh, and denominator shouldn't be equal to zero? But how I suppose to solve the dif.equation correctly? I did this. At first I solve the equation when and , I got the answer . Then I took the case when and . The solution I got was I did it like that, but I don't really think it's a good solution. Perhaps solutions overlap each other and still not sure if it's a correct method to do this. Oh, and Wolfram gave me this answer: . CAS wxMaxima gave me this result: I know that you can simplify the answer of Wolfram and wxMaxima, but why they didn't simplify it? There must be reasons.. So, I don't know why these solutions are differently written, which one of them is the most accurate, how to solve exactly correctly, what should I do about the DE domain (not only this equation's, but also every other time, with different DE) and was my solution good? And sorry for my ignorance, I'm  really messed up :D I believe there's some small detail I don't understand at all..","yy'\sqrt{(1-x^2)/(1-2y^2)}+1=0 (1-x^2)/(1-2y^2) 0 0 x \in [-1,1] y \in [-\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}] \arcsin x - \frac{\sqrt{1-2y^2}}{2}=C x \in [-\infty,-1] \cup [1, +\infty] y \in [-\infty, -\frac{\sqrt{2}}{2}] \cup [\frac{\sqrt{2}}{2}, +\infty] \frac{\sqrt{2y^2-1}}{2}+\ln{|\sqrt{x^2-1}+x|}=C \frac{1}{2}\sqrt{2y^2-1}=\frac{1}{2}\log(-\frac{x}{\sqrt{x^2-1}}+1)-\frac{1}{2}\log(\frac{x}{\sqrt{x^2-1}}+1)+C \frac{2\sqrt{x^2-1}\log(2\sqrt{x^2-1}+2x)\sqrt{\frac{x^2-1}{2y^2-1}}+x^2+1}{2\sqrt{x^2-1}\sqrt{\frac{x^2-1}{2y^2-1}}}=C","['calculus', 'ordinary-differential-equations']"
47,How to use triconfluent Heun's function?,How to use triconfluent Heun's function?,,"When solving an eigenvalue problem, I encounter the triconfluent Heun's function , $HeunT[-\lambda^2,0,-2\alpha,0,-b^2](x)$ , which is the solution of $$u''-(b^2x^2+2\alpha)u'+\lambda^2u=0.$$ I need a $u(x)$ not divergent at $\infty$ and this condition hopefully can give us the eigenvalue $\lambda$ . (This is just like Legendre function can be cut off to Legendre polynomial.) However, I can't find any clear info about this as far as I've searched. Maybe I missed something. The numerical eigensolution to the original problem is very well-behaved. So I presume there must be some condition like this to help find the eigenvalue.","When solving an eigenvalue problem, I encounter the triconfluent Heun's function , , which is the solution of I need a not divergent at and this condition hopefully can give us the eigenvalue . (This is just like Legendre function can be cut off to Legendre polynomial.) However, I can't find any clear info about this as far as I've searched. Maybe I missed something. The numerical eigensolution to the original problem is very well-behaved. So I presume there must be some condition like this to help find the eigenvalue.","HeunT[-\lambda^2,0,-2\alpha,0,-b^2](x) u''-(b^2x^2+2\alpha)u'+\lambda^2u=0. u(x) \infty \lambda","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'special-functions']"
48,Non-homogeneous linear differential equation with all coefficients equal to $1$,Non-homogeneous linear differential equation with all coefficients equal to,1,"Consider continous function $f: X \rightarrow \mathbb{R}$ , where $X$ is an open subset of $\mathbb{R}$ Solve the following equation: $$y+y^{(r)}+y^{(2r)}+...+y^{(r(k-1))}=f(x)$$ Where $y=y(x)$ , $k,r$ are some natural numbers and $y^{(r)}$ denotes $r$ -th derivative of $y$ Here is my attempt: Firstly i consider homogeneous version of this equation: $$y+y^{(r)}+y^{(2r)}+...+y^{(r(k-1))}=0$$ Secondly i associate a characteristic polynomial: $$P(T):=1+T^{r}+T^{2r}+...+T^{(k-1)r}$$ (I notice that this polynomial has exactly $(k-1)r$ distinct roots) Then the general solution to my original equation schould be in the form $$y(x)=\sum_{\alpha_: P(\alpha)=0}C(\alpha, x)e^{\alpha x}$$ Now it is sufficient to calculate all the $C(\alpha, x)$ by substituting $y(x)$ into my equation. However i find this difficult. Thank you in advance","Consider continous function , where is an open subset of Solve the following equation: Where , are some natural numbers and denotes -th derivative of Here is my attempt: Firstly i consider homogeneous version of this equation: Secondly i associate a characteristic polynomial: (I notice that this polynomial has exactly distinct roots) Then the general solution to my original equation schould be in the form Now it is sufficient to calculate all the by substituting into my equation. However i find this difficult. Thank you in advance","f: X \rightarrow \mathbb{R} X \mathbb{R} y+y^{(r)}+y^{(2r)}+...+y^{(r(k-1))}=f(x) y=y(x) k,r y^{(r)} r y y+y^{(r)}+y^{(2r)}+...+y^{(r(k-1))}=0 P(T):=1+T^{r}+T^{2r}+...+T^{(k-1)r} (k-1)r y(x)=\sum_{\alpha_: P(\alpha)=0}C(\alpha, x)e^{\alpha x} C(\alpha, x) y(x)",['ordinary-differential-equations']
49,How do we need to apply the chain rule to obtain this identity for the material derivative?,How do we need to apply the chain rule to obtain this identity for the material derivative?,,"Let $\tau>0$ , $d\in\mathbb N$ , $v:\mathbb R^d\to\mathbb R^d$ be Lipschitz continuous, $X^x\in C^0([-\tau,\tau],\mathbb R^d)$ be the solution of $$X^x(t)=x+\int_0^tv(X^x(s))\:{\rm d}s\;\;\;\text{for }t\in[-\tau,\tau]\tag1$$ for $x\in\mathbb R^d$ , $$T_t(x):=X^x(t)\;\;\;\text{for }x\in\mathbb R^d$$ for $t\in[-\tau,\tau]$ , $\Omega$ be a $d$ -dimensional properly embedded $C^1$ -submanifold of $\mathbb R^d$ with boundary, $$\Omega_t:=T_t(\Omega)\;\;\;\text{for }t\in[-\tau,\tau],$$ $y_t:\Omega_t\to\mathbb R$ for $t\in[-\tau,\tau]$ . Assume $${\rm d}y_0(\Omega;v):=\left.\frac{\rm d}{{\rm d}t}y_t(T_t(x))\right|_{t=0}\tag2$$ exists and $y_0$ is $C^1$ -differentiable. Let $x\in\bigcap_{t\in[-\tau,\:\tau]}\Omega_t$ . How can we show that $$\left.\frac{\rm d}{{\rm d}t}y_t(x)\right|_{t=0}={\rm d}y_0(\Omega;v)-T_x(y_0)v(x)\tag3,$$ where $T_x(y_0)v(x)$ denotes the pushforward of $v(x)$ by $y(\Omega)$ at $x$ . I think the trick is to write $$y_t(x)-y_0(x)=y_t(x)-y_0(T^{-1}(x))-(y_0(x)-y_0(T^{-1}(x)))\tag4$$ and we may note that $$T_t^{-1}=T_{-t}\;\;\;\text{for all }t\in[0,\tau]\tag5,$$ but I don't know how to conclude.","Let , , be Lipschitz continuous, be the solution of for , for , be a -dimensional properly embedded -submanifold of with boundary, for . Assume exists and is -differentiable. Let . How can we show that where denotes the pushforward of by at . I think the trick is to write and we may note that but I don't know how to conclude.","\tau>0 d\in\mathbb N v:\mathbb R^d\to\mathbb R^d X^x\in C^0([-\tau,\tau],\mathbb R^d) X^x(t)=x+\int_0^tv(X^x(s))\:{\rm d}s\;\;\;\text{for }t\in[-\tau,\tau]\tag1 x\in\mathbb R^d T_t(x):=X^x(t)\;\;\;\text{for }x\in\mathbb R^d t\in[-\tau,\tau] \Omega d C^1 \mathbb R^d \Omega_t:=T_t(\Omega)\;\;\;\text{for }t\in[-\tau,\tau], y_t:\Omega_t\to\mathbb R t\in[-\tau,\tau] {\rm d}y_0(\Omega;v):=\left.\frac{\rm d}{{\rm d}t}y_t(T_t(x))\right|_{t=0}\tag2 y_0 C^1 x\in\bigcap_{t\in[-\tau,\:\tau]}\Omega_t \left.\frac{\rm d}{{\rm d}t}y_t(x)\right|_{t=0}={\rm d}y_0(\Omega;v)-T_x(y_0)v(x)\tag3, T_x(y_0)v(x) v(x) y(\Omega) x y_t(x)-y_0(x)=y_t(x)-y_0(T^{-1}(x))-(y_0(x)-y_0(T^{-1}(x)))\tag4 T_t^{-1}=T_{-t}\;\;\;\text{for all }t\in[0,\tau]\tag5,","['ordinary-differential-equations', 'differential-geometry', 'smooth-manifolds', 'diffeomorphism', 'pushforward']"
50,For which $f(x)$ does the solution exist for an arbitrary $c>0$?,For which  does the solution exist for an arbitrary ?,f(x) c>0,"We have the problem $$y''+4y=f(x), \ y(0)=0, y(c \pi )=0$$ with $c>0$ and $f(x)$ an arbitrary smooth function. For which values of the parameter $c$ has the problem always an unique solution? For which $f(x)$ does the solution exist for an arbitrary $c>0$ ? For the first part we have to check when the corresponding homogeneous problem has only the trivial solution. The homogeneous problem is $y''+4y=0, \ y(0)=0, y(c \pi )=0$ . The general solution is $y(x)=a\cos (2x)+b\sin (2x)$ . From $y(0)=0$ we get $a=0$ . So we get $y(x)=b\sin (2x)$ . From $y(c \pi )=0$ we get $b\sin (2c \pi)=0$ . To get the trivial solution we have to get $b=0$ and so $\sin (2c \pi)$ must be different from $0$ , so we have $$\sin (2c \pi)\neq 0 \iff 2c\pi \neq n\pi, \ n \in \mathbb{N} \ \iff c\neq \frac{n}{2}, \ n\in \mathbb{N}$$ Is that correct? Could you give me a hint for the second part?","We have the problem with and an arbitrary smooth function. For which values of the parameter has the problem always an unique solution? For which does the solution exist for an arbitrary ? For the first part we have to check when the corresponding homogeneous problem has only the trivial solution. The homogeneous problem is . The general solution is . From we get . So we get . From we get . To get the trivial solution we have to get and so must be different from , so we have Is that correct? Could you give me a hint for the second part?","y''+4y=f(x), \ y(0)=0, y(c \pi )=0 c>0 f(x) c f(x) c>0 y''+4y=0, \ y(0)=0, y(c \pi )=0 y(x)=a\cos (2x)+b\sin (2x) y(0)=0 a=0 y(x)=b\sin (2x) y(c \pi )=0 b\sin (2c \pi)=0 b=0 \sin (2c \pi) 0 \sin (2c \pi)\neq 0 \iff 2c\pi \neq n\pi, \ n \in \mathbb{N} \ \iff c\neq \frac{n}{2}, \ n\in \mathbb{N}",['ordinary-differential-equations']
51,Multiple solutions to the Riccati equations?,Multiple solutions to the Riccati equations?,,"Closed form solutions of the Riccati equations are used to find the bond price function for specific one factor short rate interest rate models such as the Vasicek and CIS model. Example taken from Filipović, Damir; Mayerhofer, Eberhard , Affine diffusion processes: theory and applications , Albrecher, Hansjörg (ed.) et al., Advanced financial modelling. Berlin: Walter de Gruyter (ISBN 978-3-11-021313-3/hbk; 978-3-11-021314-0/ebook). Radon Series on Computational and Applied Mathematics 8, 125-164 (2009). ZBL1205.91068 . The state space is $\mathbb{R}$ , and we set $r=X$ for the Vasicek short rate model $$ dr=(b+\beta r)dt+\sigma dW. $$ The affine system  reads $$ \Phi(t,\ u)=\frac{1}{2}\sigma^{2}\int_{0}^{t}\Psi^{2}(s,\ u)ds+b\int_{0}^{t}\Psi(s,\ u)ds $$ $$ \partial_{t}\Psi(t,\ u)=\beta\Psi(t,\ u)-1, $$ $$ \Psi(0,\ u)=u $$ which admits a unique global solution with $$ \Psi(t,\ u)=\mathrm{e}^{\beta t}u-\frac{\mathrm{e}^{\beta t}-1}{\beta} $$ $$ \Phi(t,\ u)=\frac{1}{2}\sigma^{2}(\frac{u^{2}}{2\beta}(\mathrm{e}^{2\beta t}-1)+\frac{1}{2\beta^{3}}(\mathrm{e}^{2\beta t}-4\mathrm{e}^{\beta t}+2\beta t+3) $$ $$ -\ \frac{u}{\beta^{2}}(\mathrm{e}^{2\beta t}-2\mathrm{e}^{\beta t}+2\beta))+b(\frac{\mathrm{e}^{\beta t}-1}{\beta}u+\frac{\mathrm{e}^{\beta t}-1-\beta t}{\beta^{2}}) $$ for all $u\in \mathbb{C}$ . Hence (4.6) holds for all $u\in \mathbb{C}$ and $t\leq T$ . In particular, by Corollary 4.2, the bond prices $P(t,\ T)$ can be determined by $A$ and $B,$ $$ B(t)=-\Psi(t,\ 0)=\frac{\mathrm{e}^{\beta t}-1}{\beta}, $$ $$ A(t)=-\Phi(t,\ 0)=-\frac{\sigma^{2}}{4\beta^{3}}(\mathrm{e}^{2\beta t}-4\mathrm{e}^{\beta t}+2\beta t+3)+b\frac{\mathrm{e}^{\beta t}-1-\beta t}{\beta^{2}}. $$ Problem: For certain values $b=0.0012587$ , $β=0.00022$ and $σ=0.0041$ , and with $r_0=-0.0031$ , the closed form solution of $P(t,T)$ does not make economical sense, as in general investments will increase in value in a exponential way (except from actually losing money in the first years). The bond price at $t$ is equal to the amount invested at $t$ to receive EUR 1 at $T$ . The blue line depicted in this plot is made using the observed forward rate (the rate at which the investment increases in a time period) and shows this exponential form. If I use the bond price formula obtained with the Riccati equations, I get the orange line, a related but 'wrong' form. By wrong I mean that the starting point and endpoint are the same, but In the same way you can walk the edges of a rectangle in two ways, two paths are possible from start to end. The small difference at $t=0$ can be ignored and it can safely be assumed that the solution is correct at $t=0$ for all $T$ . I ask myself if its possible that the Riccati equations may have more than one solution to choose from?","Closed form solutions of the Riccati equations are used to find the bond price function for specific one factor short rate interest rate models such as the Vasicek and CIS model. Example taken from Filipović, Damir; Mayerhofer, Eberhard , Affine diffusion processes: theory and applications , Albrecher, Hansjörg (ed.) et al., Advanced financial modelling. Berlin: Walter de Gruyter (ISBN 978-3-11-021313-3/hbk; 978-3-11-021314-0/ebook). Radon Series on Computational and Applied Mathematics 8, 125-164 (2009). ZBL1205.91068 . The state space is , and we set for the Vasicek short rate model The affine system  reads which admits a unique global solution with for all . Hence (4.6) holds for all and . In particular, by Corollary 4.2, the bond prices can be determined by and Problem: For certain values , and , and with , the closed form solution of does not make economical sense, as in general investments will increase in value in a exponential way (except from actually losing money in the first years). The bond price at is equal to the amount invested at to receive EUR 1 at . The blue line depicted in this plot is made using the observed forward rate (the rate at which the investment increases in a time period) and shows this exponential form. If I use the bond price formula obtained with the Riccati equations, I get the orange line, a related but 'wrong' form. By wrong I mean that the starting point and endpoint are the same, but In the same way you can walk the edges of a rectangle in two ways, two paths are possible from start to end. The small difference at can be ignored and it can safely be assumed that the solution is correct at for all . I ask myself if its possible that the Riccati equations may have more than one solution to choose from?","\mathbb{R} r=X 
dr=(b+\beta r)dt+\sigma dW.
 
\Phi(t,\ u)=\frac{1}{2}\sigma^{2}\int_{0}^{t}\Psi^{2}(s,\ u)ds+b\int_{0}^{t}\Psi(s,\ u)ds
 
\partial_{t}\Psi(t,\ u)=\beta\Psi(t,\ u)-1,
 
\Psi(0,\ u)=u
 
\Psi(t,\ u)=\mathrm{e}^{\beta t}u-\frac{\mathrm{e}^{\beta t}-1}{\beta}
 
\Phi(t,\ u)=\frac{1}{2}\sigma^{2}(\frac{u^{2}}{2\beta}(\mathrm{e}^{2\beta t}-1)+\frac{1}{2\beta^{3}}(\mathrm{e}^{2\beta t}-4\mathrm{e}^{\beta t}+2\beta t+3)
 
-\ \frac{u}{\beta^{2}}(\mathrm{e}^{2\beta t}-2\mathrm{e}^{\beta t}+2\beta))+b(\frac{\mathrm{e}^{\beta t}-1}{\beta}u+\frac{\mathrm{e}^{\beta t}-1-\beta t}{\beta^{2}})
 u\in \mathbb{C} u\in \mathbb{C} t\leq T P(t,\ T) A B, 
B(t)=-\Psi(t,\ 0)=\frac{\mathrm{e}^{\beta t}-1}{\beta},
 
A(t)=-\Phi(t,\ 0)=-\frac{\sigma^{2}}{4\beta^{3}}(\mathrm{e}^{2\beta t}-4\mathrm{e}^{\beta t}+2\beta t+3)+b\frac{\mathrm{e}^{\beta t}-1-\beta t}{\beta^{2}}.
 b=0.0012587 β=0.00022 σ=0.0041 r_0=-0.0031 P(t,T) t t T t=0 t=0 T","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'closed-form', 'finance']"
52,"Consider the ODE $\frac{dy}{dx}=f(x,y)$, where $f:\mathbb{R}^2\to \mathbb{R}$","Consider the ODE , where","\frac{dy}{dx}=f(x,y) f:\mathbb{R}^2\to \mathbb{R}","Consider the ODE $\frac{dy}{dx}=f(x,y)$ , where $f:\mathbb{R}^2\to \mathbb{R}$ defined as follow $$ f(x,y) = \left\{ 	\begin{array}{ll} 		\frac{xy}{x^2+y^2}  & (x,y)\neq(0,0), \\ 		0 & (x,y)=(0,0) 	\end{array} \right.$$ a) Show that the given ODE admits solutions for arbitrary initial conditions $y(x_0)=x_0$ b) ¿Does $f$ locally satisfiy the conditions for Picard's Theorem?, c) ¿And what about Peano's existence theorem? Justify My attemps: To solve the ODE $\frac{dy}{dx}=f(x,y)$ for $(x_0,y_0)\neq(0,0)$ I tried it by separable variable method and the solution was $$ \ln(y)=\frac{x^2}{2y^2}+C$$ How do I conclude a),b) and c). Can somebody give me a hint?","Consider the ODE , where defined as follow a) Show that the given ODE admits solutions for arbitrary initial conditions b) ¿Does locally satisfiy the conditions for Picard's Theorem?, c) ¿And what about Peano's existence theorem? Justify My attemps: To solve the ODE for I tried it by separable variable method and the solution was How do I conclude a),b) and c). Can somebody give me a hint?","\frac{dy}{dx}=f(x,y) f:\mathbb{R}^2\to \mathbb{R}  f(x,y) =
\left\{
	\begin{array}{ll}
		\frac{xy}{x^2+y^2}  & (x,y)\neq(0,0), \\
		0 & (x,y)=(0,0)
	\end{array}
\right. y(x_0)=x_0 f \frac{dy}{dx}=f(x,y) (x_0,y_0)\neq(0,0)  \ln(y)=\frac{x^2}{2y^2}+C",['ordinary-differential-equations']
53,Convergence of the solution and their derivative of the equation $y''+y'+y^{3}=0$. [duplicate],Convergence of the solution and their derivative of the equation . [duplicate],y''+y'+y^{3}=0,"This question already has answers here : Limits for the solution of the non-linear ODE (2 answers) Closed 3 years ago . I have the ODE $y''+y'+y^{3}=0$ and I must prove that the solution $y(t)$ and $y'(t)$ converges to zero when $t\to \infty$ . I try to write the associated system of two equations, this is one form \begin{equation*} y'=z;\qquad z'=-z-y^3 \end{equation*} and this is the other \begin{equation*} y'=z-y;\qquad z'=-y^3. \end{equation*} I try to use Lyapounov method with the function $V(y,z)=z^2+\frac{1}{2} y^4$ and obtain that $\nabla V\cdot (y',z') <0$ . But I don't know how to conclude this proof. On the other hand, using the associated matrix for the linear system, the real part of the eigenvalues is not negative, in fact is zero and I don't know how to continue. I accept any suggestion, hint or book to read.","This question already has answers here : Limits for the solution of the non-linear ODE (2 answers) Closed 3 years ago . I have the ODE and I must prove that the solution and converges to zero when . I try to write the associated system of two equations, this is one form and this is the other I try to use Lyapounov method with the function and obtain that . But I don't know how to conclude this proof. On the other hand, using the associated matrix for the linear system, the real part of the eigenvalues is not negative, in fact is zero and I don't know how to continue. I accept any suggestion, hint or book to read.","y''+y'+y^{3}=0 y(t) y'(t) t\to \infty \begin{equation*}
y'=z;\qquad z'=-z-y^3
\end{equation*} \begin{equation*}
y'=z-y;\qquad z'=-y^3.
\end{equation*} V(y,z)=z^2+\frac{1}{2} y^4 \nabla V\cdot (y',z') <0","['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
54,Solving a nonlinear (vector or multi-variable) ODE,Solving a nonlinear (vector or multi-variable) ODE,,"I am interested in solving the following differential equation: $$\frac{d\mathbf{v}}{dt}=A\mathbf{w}, \qquad \mathbf{v}=\left[x,y\right]^T, \mathbf{w}=[x^2,y^2,xy]^T, A\in\mathbb{R}^{2\times 3}$$ Equivalently, for $a_{ij}\in\mathbb{R}, \quad\forall i,j$ $$\frac{dx}{dt} = a_{11}x^2+a_{12}y^2+a_{13}xy$$ $$\frac{dy}{dt} = a_{21}x^2+a_{22}y^2+a_{23}xy$$ I have noticed that $$\mathbf{vv^T}=\left[\begin{array}{cc}x^2&xy\\xy&y^2\end{array}\right]$$ I'm looking for analytical solution to this problem.","I am interested in solving the following differential equation: Equivalently, for I have noticed that I'm looking for analytical solution to this problem.","\frac{d\mathbf{v}}{dt}=A\mathbf{w}, \qquad \mathbf{v}=\left[x,y\right]^T, \mathbf{w}=[x^2,y^2,xy]^T, A\in\mathbb{R}^{2\times 3} a_{ij}\in\mathbb{R}, \quad\forall i,j \frac{dx}{dt} = a_{11}x^2+a_{12}y^2+a_{13}xy \frac{dy}{dt} = a_{21}x^2+a_{22}y^2+a_{23}xy \mathbf{vv^T}=\left[\begin{array}{cc}x^2&xy\\xy&y^2\end{array}\right]","['ordinary-differential-equations', 'differential']"
55,Possible interpretation for points of non equilibrium in a 2D ODE system where only one of the components of the vector field is $=0$.,Possible interpretation for points of non equilibrium in a 2D ODE system where only one of the components of the vector field is .,=0,"I have a 2D ODE system $$ x'=F(x), $$ where $x=(x_1,x_2)$ , i.e., $$ \begin{cases}x_1'(t)=F_1(x)\\ x_2'(t)=F_2(x);\end{cases} $$ A component of the vector field $F$ I have vanishes at a point $x=(x_1,x_2)$ , i.e. $F_2(x)=0$ . If $F_1(x)=0$ , then $x$ would be an equilibrium point. However, I only have $F_2(x)=0$ : can I still get some substantial information about the behavior of the solution $$ x(t)=\big(x_1(t), x_2(t)\big) $$ around such points of $F(x)$ ? Many thanks.","I have a 2D ODE system where , i.e., A component of the vector field I have vanishes at a point , i.e. . If , then would be an equilibrium point. However, I only have : can I still get some substantial information about the behavior of the solution around such points of ? Many thanks.","
x'=F(x),
 x=(x_1,x_2) 
\begin{cases}x_1'(t)=F_1(x)\\
x_2'(t)=F_2(x);\end{cases}
 F x=(x_1,x_2) F_2(x)=0 F_1(x)=0 x F_2(x)=0 
x(t)=\big(x_1(t), x_2(t)\big)
 F(x)","['ordinary-differential-equations', 'dynamical-systems', 'vector-fields']"
56,If $\phi$ and $\sigma$ are solutions of the ODE then $\phi(t)=\sigma(t)$,If  and  are solutions of the ODE then,\phi \sigma \phi(t)=\sigma(t),"Let $\Omega$ be a subset of $\mathbb{R}\times \mathbb{R}^n$ , $f:\Omega\rightarrow \mathbb{R}^n$ a continuos function. Suppose that for all $(t_0,x_0) \in \Omega$ there exists unique local solution $x:I_{t_0,x_0}\rightarrow \mathbb{R}^n$ of \begin{equation*} x'=f(t,x) \quad x(t_0)=x_0 \end{equation*} Prove that if $\phi:I\rightarrow \mathbb{R}^n$ and $\sigma:J\rightarrow \mathbb{R}^n$ are solutions of the ODE $x'=f(t,x)$ such that $\phi$ and $\sigma$ have a coincide point, then $\phi(t)=\sigma(t)$ for all $t\in I\cap J$ . Can somebody give a hint of what I have to do in this problem.","Let be a subset of , a continuos function. Suppose that for all there exists unique local solution of Prove that if and are solutions of the ODE such that and have a coincide point, then for all . Can somebody give a hint of what I have to do in this problem.","\Omega \mathbb{R}\times \mathbb{R}^n f:\Omega\rightarrow \mathbb{R}^n (t_0,x_0) \in \Omega x:I_{t_0,x_0}\rightarrow \mathbb{R}^n \begin{equation*}
x'=f(t,x) \quad x(t_0)=x_0
\end{equation*} \phi:I\rightarrow \mathbb{R}^n \sigma:J\rightarrow \mathbb{R}^n x'=f(t,x) \phi \sigma \phi(t)=\sigma(t) t\in I\cap J",['ordinary-differential-equations']
57,Why this $G$ is non empty?,Why this  is non empty?,G,"Consider the following gradient system on a doubly connected planar domain $\Omega$ : $$\begin{cases} \frac{dx}{dt}&= - u_x;\\  \frac{dy}{dt}&= - u_y, \end{cases}$$ where $u$ is the first eigenfunction of Dirichlet laplacian. i.e. if $\Gamma_0,\Gamma_1$ are respectively inner and outer boundary of $\Omega$ , $u$ satisfies the following: $$\begin{cases}-\Delta u=\lambda_1 u & \text{ in }\Omega, \\ u=0 & \text{ on }\partial \Omega=\Gamma_0\cup\Gamma_1;\end{cases}$$ where $\lambda_1$ is the first eigenvalue. Define, $$G=\{x\in \Omega: \text{the solution curve of the gradient system starting from $x$ touches $\Gamma_0$} \}$$ Why $G$ is non-empty? $\textbf{Note:}$ It is well known that $u>0$ inside $\Omega$ and $\frac{\partial u}{\partial\eta}<0$ on $\Gamma_0$ .","Consider the following gradient system on a doubly connected planar domain : where is the first eigenfunction of Dirichlet laplacian. i.e. if are respectively inner and outer boundary of , satisfies the following: where is the first eigenvalue. Define, Why is non-empty? It is well known that inside and on .","\Omega \begin{cases}
\frac{dx}{dt}&= - u_x;\\ 
\frac{dy}{dt}&= - u_y,
\end{cases} u \Gamma_0,\Gamma_1 \Omega u \begin{cases}-\Delta u=\lambda_1 u & \text{ in }\Omega, \\
u=0 & \text{ on }\partial \Omega=\Gamma_0\cup\Gamma_1;\end{cases} \lambda_1 G=\{x\in \Omega: \text{the solution curve of the gradient system starting from x touches \Gamma_0} \} G \textbf{Note:} u>0 \Omega \frac{\partial u}{\partial\eta}<0 \Gamma_0","['ordinary-differential-equations', 'partial-differential-equations', 'gradient-flows']"
58,"Solve the ODE $(x-1)y'' - xy' + y = 1$ subject to boundary conditions $y(0)=0,y(1)=2$ using Greens function",Solve the ODE  subject to boundary conditions  using Greens function,"(x-1)y'' - xy' + y = 1 y(0)=0,y(1)=2","I'm looking to solve the BVP $$(x-1)\frac{d^2y}{dx^2} - x\frac{dy}{dx} + y = 1$$ subject to the conditions $y(0)=0,y(1)=2$ , firstly by getting the problem into self adjoint form and then by finding the Greens function. I think I have the self adjoint form for the problem, namely $$\frac{d}{dx}(\frac{e^{-x}}{x-1}\frac{dy}{dx}) +\frac{e^{-x}}{(x-1)^2}y=\frac{e^{-x}}{(x-1)^2}$$ where the boundary conditions remain unchanged as I have just multiplied through by a factor and then used the chain rule to simplify. Then, I have noted that $y=x$ and $y=e^x$ are solutions to the homogeneous form of the above which are linearly independent since they have non-zero Wronskian on the domain we are considering. Here I've also noted that $u=2x$ satisfies the homogeneous problem and the inhomogeneous boundary conditions, so then $\hat{y} = y - u$ is a solution to the inhomogeneous problem with the homogeneous boundary conditions (which we can find by the Greens function and use to get the solution $y$ of the problem we are considering). It is in finding the Greens function that I'm struggling, as I seem to always get a wrong answer. I wonder if my understanding of the method is wrong. Could someone provide a step by step process of how to get the Greens function in this case, and then how to get a solution from that?","I'm looking to solve the BVP subject to the conditions , firstly by getting the problem into self adjoint form and then by finding the Greens function. I think I have the self adjoint form for the problem, namely where the boundary conditions remain unchanged as I have just multiplied through by a factor and then used the chain rule to simplify. Then, I have noted that and are solutions to the homogeneous form of the above which are linearly independent since they have non-zero Wronskian on the domain we are considering. Here I've also noted that satisfies the homogeneous problem and the inhomogeneous boundary conditions, so then is a solution to the inhomogeneous problem with the homogeneous boundary conditions (which we can find by the Greens function and use to get the solution of the problem we are considering). It is in finding the Greens function that I'm struggling, as I seem to always get a wrong answer. I wonder if my understanding of the method is wrong. Could someone provide a step by step process of how to get the Greens function in this case, and then how to get a solution from that?","(x-1)\frac{d^2y}{dx^2} - x\frac{dy}{dx} + y = 1 y(0)=0,y(1)=2 \frac{d}{dx}(\frac{e^{-x}}{x-1}\frac{dy}{dx}) +\frac{e^{-x}}{(x-1)^2}y=\frac{e^{-x}}{(x-1)^2} y=x y=e^x u=2x \hat{y} = y - u y","['ordinary-differential-equations', 'boundary-value-problem', 'greens-function', 'self-adjoint-operators']"
59,"Show that $A\in \mathcal{L}(\mathbb{R}^{d},\mathbb{R}^{d})$ is hyperbolic.",Show that  is hyperbolic.,"A\in \mathcal{L}(\mathbb{R}^{d},\mathbb{R}^{d})","If a linear flow $(e^{tA})_{t}$ is topologically conjugated to any linear flow $(e^{tB})_{t}$ with $B$ close to $A$ , then the vector field $A \in \mathcal{L}(\mathbb{R}^{d},\mathbb{R}^{d})$ is hyperbolic. Definition I. $A\colon \mathbb{R}^{d} \longrightarrow  \mathbb{R}^{d}$ is linear hyperbolic if all its eigenvalues ​​have non-zero real part. II. Let $(f^t)_t$ and $(g^t)_t$ be flows of the following autonomous differential equations $x' = F(x)$ , $F: \mathcal{U} \longrightarrow \mathbb{R}^d$ and $y' = G(y), G: \mathcal{V} \longrightarrow \mathbb{R}^d$ , with $F$ and $G$ continuous funtions defined over open sets. The flows $(f^t)_t$ and $(g^t)_t$ are said to be topologically conjugate if there exists a homeomorphism $h: \mathcal{U} \longrightarrow \mathcal{V}$ such that $f^t(x)$ is defined if and only if $g^t(h(x))$ is defined. In such case, $h(f^t(x)) = g^t(h(x))$ . Attempt: I started assuming that $A$ was not hyperbolic so I would have one of its eigenvalues ​​to be of the form $i\lambda$ with $\lambda \in \mathbb{R}$ . I tried to build a $B \in \mathcal{L}(\mathbb{R}^{d},\mathbb{R}^{d}) $ operator close to $A$ so that its flows were not topologically conjugated and concluded an absurdity. However, I couldn't finish it. Every help is welcome.","If a linear flow is topologically conjugated to any linear flow with close to , then the vector field is hyperbolic. Definition I. is linear hyperbolic if all its eigenvalues ​​have non-zero real part. II. Let and be flows of the following autonomous differential equations , and , with and continuous funtions defined over open sets. The flows and are said to be topologically conjugate if there exists a homeomorphism such that is defined if and only if is defined. In such case, . Attempt: I started assuming that was not hyperbolic so I would have one of its eigenvalues ​​to be of the form with . I tried to build a operator close to so that its flows were not topologically conjugated and concluded an absurdity. However, I couldn't finish it. Every help is welcome.","(e^{tA})_{t} (e^{tB})_{t} B A A \in \mathcal{L}(\mathbb{R}^{d},\mathbb{R}^{d}) A\colon \mathbb{R}^{d} \longrightarrow  \mathbb{R}^{d} (f^t)_t (g^t)_t x' = F(x) F: \mathcal{U} \longrightarrow \mathbb{R}^d y' = G(y), G: \mathcal{V} \longrightarrow \mathbb{R}^d F G (f^t)_t (g^t)_t h: \mathcal{U} \longrightarrow \mathcal{V} f^t(x) g^t(h(x)) h(f^t(x)) = g^t(h(x)) A i\lambda \lambda \in \mathbb{R} B \in \mathcal{L}(\mathbb{R}^{d},\mathbb{R}^{d})  A",['ordinary-differential-equations']
60,Solving the Biharmonic Equation on an elliptical domain,Solving the Biharmonic Equation on an elliptical domain,,"I would like to solve the Biharmonic Equation $$\Delta^2 \phi=0$$ on an elliptical domain $c_1<\frac{x^2}{a^2}+\frac{y^2}{b^2}<c_2$ . I converted the Biharmonic equation into $(R,\omega)$ coordinates, where $$R\cos(\omega)=\frac{x}{a},$$ $$R\sin(\omega)=\frac{y}{b}$$ In this new coordinate system the boundary conditions of the PDE correspond to $R=c_1$ and $R=c_2$ and are related the derivatives of $\phi$ with respect to $R$ and $\omega$ . In this new coordinate system I had a PDE where every term had the form $$\frac{C_{k,m}(\omega)}{R^{4-k}}\frac{\partial^{m+k}\phi}{\partial R^k\partial \omega^m}.$$ This looked like a Cauchy-Euler equation in $R$ , so I guessed a solution of the form $$\phi(R,\omega)=R^nH(\omega).$$ When I made this substitution the PDE simplified to an ODE of the form $$P_4(cos(2\omega))H^{(4)}+P_3(\cos(2\omega),\sin(2\omega))H'''+P_2(\cos(2\omega),\sin(2\omega))H''$$ $$+P_1(\cos(2\omega),\sin(2\omega))H' +P_0(\cos(2\omega))H=0,$$ where $$P_4(x)=c_{41}x+c_{42}x^2,$$ $$P_3(x,y)=c_{31}y+c_{32}xy,$$ $$P_2(x,y)=c_{21}+c_{22}y+c_{23}x+c_{33}xy+c_{34}x^2+c_{35}y^2,$$ $$P_1(x,y)=c_{11}y+c_{12}xy,$$ $$P_0(x)=c_{01}+c_{02}x+c_{03}x^2$$ and the coefficients depend on $n$ , $a$ , and $b$ . If I can find $H$ in terms of $n$ and then I am hoping that I can write $$\phi(R,\omega)=\sum_{n\in K}\left(A_nR^nH_n(\omega)\right)$$ for some set $K$ (or maybe it might be an integral rather than a sum if there is an uncountable number of values of $n$ that I would use) and then I would solve for the $A_n$ 's using my boundary conditions. In summary, I want to know how to solve the biharmonic equation on an elliptical domain with a hole in it. Either I would like to know how to start from scratch if my attempt won't get me there, how to solve for $H$ in the ODE I wrote down, or if there is a source where they already solved my problem and where I can find their paper. EDIT: I tried using elliptical coordinates as mattos suggested and I got the PDE $$\left(\frac{\partial^4\phi}{d\mu^4}+\frac{\partial^4\phi}{\partial\mu^2d\nu^2}+\frac{\partial^4\phi}{\partial\nu^4}\right)\left(\cosh(2\mu)-\cos(2\nu)\right)-4\sinh(2\mu)\left(\frac{\partial^3\phi}{\partial\mu^3}+\frac{\partial^3\phi}{\partial\mu\partial\nu^2}\right)$$ $$-4\sin(2\nu)\left(\frac{\partial^3\phi}{\partial\mu^2\partial\nu}+\frac{\partial^3\phi}{\partial\nu^3}\right)$$ $$+4(\cosh(2\mu)+\cos(2\nu))\left(\frac{\partial^2\phi}{\partial\mu^2}+\frac{\partial^2\phi}{\partial\nu^2}\right)=0.$$ I can't use separation of variables because of the mixed derivative terms and the method of characteristics doesn't work well for elliptic PDEs so I'm not sure how to proceed (the only two methods for solving PDEs that I know of are separation of variables and the method of characteristics)","I would like to solve the Biharmonic Equation on an elliptical domain . I converted the Biharmonic equation into coordinates, where In this new coordinate system the boundary conditions of the PDE correspond to and and are related the derivatives of with respect to and . In this new coordinate system I had a PDE where every term had the form This looked like a Cauchy-Euler equation in , so I guessed a solution of the form When I made this substitution the PDE simplified to an ODE of the form where and the coefficients depend on , , and . If I can find in terms of and then I am hoping that I can write for some set (or maybe it might be an integral rather than a sum if there is an uncountable number of values of that I would use) and then I would solve for the 's using my boundary conditions. In summary, I want to know how to solve the biharmonic equation on an elliptical domain with a hole in it. Either I would like to know how to start from scratch if my attempt won't get me there, how to solve for in the ODE I wrote down, or if there is a source where they already solved my problem and where I can find their paper. EDIT: I tried using elliptical coordinates as mattos suggested and I got the PDE I can't use separation of variables because of the mixed derivative terms and the method of characteristics doesn't work well for elliptic PDEs so I'm not sure how to proceed (the only two methods for solving PDEs that I know of are separation of variables and the method of characteristics)","\Delta^2 \phi=0 c_1<\frac{x^2}{a^2}+\frac{y^2}{b^2}<c_2 (R,\omega) R\cos(\omega)=\frac{x}{a}, R\sin(\omega)=\frac{y}{b} R=c_1 R=c_2 \phi R \omega \frac{C_{k,m}(\omega)}{R^{4-k}}\frac{\partial^{m+k}\phi}{\partial R^k\partial \omega^m}. R \phi(R,\omega)=R^nH(\omega). P_4(cos(2\omega))H^{(4)}+P_3(\cos(2\omega),\sin(2\omega))H'''+P_2(\cos(2\omega),\sin(2\omega))H'' +P_1(\cos(2\omega),\sin(2\omega))H'
+P_0(\cos(2\omega))H=0, P_4(x)=c_{41}x+c_{42}x^2, P_3(x,y)=c_{31}y+c_{32}xy, P_2(x,y)=c_{21}+c_{22}y+c_{23}x+c_{33}xy+c_{34}x^2+c_{35}y^2, P_1(x,y)=c_{11}y+c_{12}xy, P_0(x)=c_{01}+c_{02}x+c_{03}x^2 n a b H n \phi(R,\omega)=\sum_{n\in K}\left(A_nR^nH_n(\omega)\right) K n A_n H \left(\frac{\partial^4\phi}{d\mu^4}+\frac{\partial^4\phi}{\partial\mu^2d\nu^2}+\frac{\partial^4\phi}{\partial\nu^4}\right)\left(\cosh(2\mu)-\cos(2\nu)\right)-4\sinh(2\mu)\left(\frac{\partial^3\phi}{\partial\mu^3}+\frac{\partial^3\phi}{\partial\mu\partial\nu^2}\right) -4\sin(2\nu)\left(\frac{\partial^3\phi}{\partial\mu^2\partial\nu}+\frac{\partial^3\phi}{\partial\nu^3}\right) +4(\cosh(2\mu)+\cos(2\nu))\left(\frac{\partial^2\phi}{\partial\mu^2}+\frac{\partial^2\phi}{\partial\nu^2}\right)=0.","['ordinary-differential-equations', 'partial-differential-equations', 'elliptic-equations']"
61,How to find solutions to parametric differential equations as non-parametric level sets?,How to find solutions to parametric differential equations as non-parametric level sets?,,"Say that we have a parametric differential equation, given $g_k(t)$ and $h_k(t)$ $$\cases{\sum g_k(t)\cdot {f_x}^{(k)}(t) = 0\\\sum h_k(t)\cdot {f_y}^{(k)}(t) = 0}$$ with generalized ""prime"" notation $$ (\cdot)^{(n)}= \frac{\partial^n (\cdot)}{\partial t^n}$$ So that a curve is defined by $(x,y) = (f_x(t),f_y(t))$ . Can we transform this problem to a level set for some basis of functions: $$\phi(x,y) =\sum c_k e_k(x,y) = 0$$ Own work An obvious example where this is possible would be the harmonic equations for sin and cos on the unit circle: $$\cases{f_x(t) = \cos(t)\\f_y(t) = \sin(t)}$$ which solves $$\cases{g_k = \{1,0,1\}, f_x(0) = 1\\h_k = \{1,0,1\},f_y(0)=0}$$ and for which $$x^2+y^2 -1 = 0$$ What I am curious about is of course more general settings. Which areas of mathematics will I benefit from learning more from in order to be able to formulate and solve things like this? A suspicion that I have is that $$(\nabla \phi (f_x(t),f_y(t))) \cdot [f'_x(t),f'_y(t)] = 0$$ In other words, the tangent of the trajectory of the parametric curve needs to be orthogonal to the gradient of the level set function. Although I have no proof of this.","Say that we have a parametric differential equation, given and with generalized ""prime"" notation So that a curve is defined by . Can we transform this problem to a level set for some basis of functions: Own work An obvious example where this is possible would be the harmonic equations for sin and cos on the unit circle: which solves and for which What I am curious about is of course more general settings. Which areas of mathematics will I benefit from learning more from in order to be able to formulate and solve things like this? A suspicion that I have is that In other words, the tangent of the trajectory of the parametric curve needs to be orthogonal to the gradient of the level set function. Although I have no proof of this.","g_k(t) h_k(t) \cases{\sum g_k(t)\cdot {f_x}^{(k)}(t) = 0\\\sum h_k(t)\cdot {f_y}^{(k)}(t) = 0}  (\cdot)^{(n)}= \frac{\partial^n (\cdot)}{\partial t^n} (x,y) = (f_x(t),f_y(t)) \phi(x,y) =\sum c_k e_k(x,y) = 0 \cases{f_x(t) = \cos(t)\\f_y(t) = \sin(t)} \cases{g_k = \{1,0,1\}, f_x(0) = 1\\h_k = \{1,0,1\},f_y(0)=0} x^2+y^2 -1 = 0 (\nabla \phi (f_x(t),f_y(t))) \cdot [f'_x(t),f'_y(t)] = 0","['real-analysis', 'geometry', 'ordinary-differential-equations', 'soft-question']"
62,What can be concluded about an equilibrium point that has finitely many trajectories that converge to it?,What can be concluded about an equilibrium point that has finitely many trajectories that converge to it?,,"If you have a dynamical system $\dot{x}=f(x)$ and $f(x)$ is smooth , given that $f(x_0)=0$ and only finitely many trajectories converge to $x_0$ . I think if $x_0$ is isolated then the stable manifold is one dimensional since if it was more than one dimensional it would have infinitely many . I also think that the number of trajectories can be at most two . My questions are : 1) If $x_0$ is isolated , is the stable manifold of $x_0$ only one dimensional and If yes is the smoothness condition needed ? 2) If $x_0$ is not isolated can other conditions be imposed so that the same conclusion can be said ? Edit : I think I could use the linearization of the system at $x_0$ as $\dot{x}=Ax$ , so if the system has one stable eigenvector $v_1$ ,   since trajectories cannot intersect it will have only two stable trajectories $v_1$ and $-v_1$ , if there is another stable eigenvector then the whole plane containing the two vectors will be stable implying there is an infinite number of trajectories . However , this doesn't include the cases where the linearization fails or non-isolated points case .","If you have a dynamical system and is smooth , given that and only finitely many trajectories converge to . I think if is isolated then the stable manifold is one dimensional since if it was more than one dimensional it would have infinitely many . I also think that the number of trajectories can be at most two . My questions are : 1) If is isolated , is the stable manifold of only one dimensional and If yes is the smoothness condition needed ? 2) If is not isolated can other conditions be imposed so that the same conclusion can be said ? Edit : I think I could use the linearization of the system at as , so if the system has one stable eigenvector ,   since trajectories cannot intersect it will have only two stable trajectories and , if there is another stable eigenvector then the whole plane containing the two vectors will be stable implying there is an infinite number of trajectories . However , this doesn't include the cases where the linearization fails or non-isolated points case .",\dot{x}=f(x) f(x) f(x_0)=0 x_0 x_0 x_0 x_0 x_0 x_0 \dot{x}=Ax v_1 v_1 -v_1,"['ordinary-differential-equations', 'differential-topology', 'dynamical-systems', 'control-theory', 'stability-in-odes']"
63,Does the Van der Pol equation with negative parameter explode outside a circle?,Does the Van der Pol equation with negative parameter explode outside a circle?,,"Consider the Van der Pol system $$ \mathrm{\frac d {dt}}\begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}y\\\mu(1-x^2)y-x\end{bmatrix} $$ with $\mu < 0$ . It attains an asymptotically stable equilibrium at the origin. Is there a circle centered at the origin such that for every trajectory that starts outside the circle, $\|x\| \to \infty$ as $t \to \infty$ ? It definitely looks like it, but I don't know how I'd prove it.","Consider the Van der Pol system with . It attains an asymptotically stable equilibrium at the origin. Is there a circle centered at the origin such that for every trajectory that starts outside the circle, as ? It definitely looks like it, but I don't know how I'd prove it.","
\mathrm{\frac d {dt}}\begin{bmatrix}x\\y\end{bmatrix}
=
\begin{bmatrix}y\\\mu(1-x^2)y-x\end{bmatrix}
 \mu < 0 \|x\| \to \infty t \to \infty","['ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
64,Proving that the solution to a nonlinear oscillator equation is bounded,Proving that the solution to a nonlinear oscillator equation is bounded,,"I am working on a fluid-structure-interaction evolution problem: let us suppose that the motion of the rectangular body $B$ , with boundary $\partial B=S$ , immersed in an unbounded 2D channel $\mathbb{R}\times (-L,L)$ , is governed by a nonlinear oscillator equation with restoring force $f=f(h)$ , and forced by the fluid lift exerted on $B$ : \begin{equation}\label{eq:ODE}   \ddot{h}+f(h)=-\hat{e}_2\cdot{\langle \mathbf{T}\cdot\hat{n},1\rangle}_S \qquad \text{in}\,\,(0,T),\qquad (1) \end{equation} where $\mathbf{T}= \mathbf{T}(u,p)=-p\mathbf{I}+\mu[\nabla u+(\nabla u)^T]$ . We suppose that $u\in L^2(0,T; H^1(\Omega))$ , velocity field of the fluid, is $\textit{known}$ and thus \begin{equation}  \mathbf{T}(u,p)\cdot\hat{n}\in L^2(0,T;W^{-\tfrac{2}{3},\tfrac{3}{2}}(S)), \end{equation} In (1), ${\langle \cdot,\cdot\rangle}_S$ precisely labels the duality between $W^{-\tfrac{2}{3},\tfrac{3}{2}}(S)$ and its dual space $W^{\tfrac{2}{3},3}(S)$ . We assume that $f(h)$ satisifes the following hypotheses: $f\in C^1(-L+1,L-1)$ and \begin{equation} f'(h)>0\, \forall h\in (-L+1,L-1), \quad \lim_{|h|\to L-1}\frac{|f(h)|}{(|L-1|-|h|)^{3/2}}=+\infty \end{equation} The ODE (1) enjoys existence and uniqueness of solutions $h\in H^2(0,T)$ . How do I prove that this solution is also $\textit{bounded}$ ?","I am working on a fluid-structure-interaction evolution problem: let us suppose that the motion of the rectangular body , with boundary , immersed in an unbounded 2D channel , is governed by a nonlinear oscillator equation with restoring force , and forced by the fluid lift exerted on : where . We suppose that , velocity field of the fluid, is and thus In (1), precisely labels the duality between and its dual space . We assume that satisifes the following hypotheses: and The ODE (1) enjoys existence and uniqueness of solutions . How do I prove that this solution is also ?","B \partial B=S \mathbb{R}\times (-L,L) f=f(h) B \begin{equation}\label{eq:ODE}
  \ddot{h}+f(h)=-\hat{e}_2\cdot{\langle \mathbf{T}\cdot\hat{n},1\rangle}_S \qquad \text{in}\,\,(0,T),\qquad (1)
\end{equation} \mathbf{T}= \mathbf{T}(u,p)=-p\mathbf{I}+\mu[\nabla u+(\nabla u)^T] u\in L^2(0,T; H^1(\Omega)) \textit{known} \begin{equation}
 \mathbf{T}(u,p)\cdot\hat{n}\in L^2(0,T;W^{-\tfrac{2}{3},\tfrac{3}{2}}(S)),
\end{equation} {\langle \cdot,\cdot\rangle}_S W^{-\tfrac{2}{3},\tfrac{3}{2}}(S) W^{\tfrac{2}{3},3}(S) f(h) f\in C^1(-L+1,L-1) \begin{equation}
f'(h)>0\, \forall h\in (-L+1,L-1), \quad \lim_{|h|\to L-1}\frac{|f(h)|}{(|L-1|-|h|)^{3/2}}=+\infty
\end{equation} h\in H^2(0,T) \textit{bounded}","['ordinary-differential-equations', 'fluid-dynamics']"
65,Derivative of $L^\infty$ norm of a function,Derivative of  norm of a function,L^\infty,"Let $f: \mathbb R_+ \times \mathbb R \to \mathbb R$ . Under which assumptions does it make sense to compute $\frac{d}{dt}\Vert f(t,\cdot) \Vert_{L^\infty(\mathbb R)}$ and what is it?",Let . Under which assumptions does it make sense to compute and what is it?,"f: \mathbb R_+ \times \mathbb R \to \mathbb R \frac{d}{dt}\Vert f(t,\cdot) \Vert_{L^\infty(\mathbb R)}","['real-analysis', 'calculus', 'functional-analysis', 'ordinary-differential-equations']"
66,On Logarithmic Derivative transformation,On Logarithmic Derivative transformation,,"I am curious about a certain transformation called the logarithmic derivative that seems to appear a lot in different cool ideas, for example: The use in generating functions for recursions of the form $a_{n+1}=\sum _{k=0}^n\binom{n}{k}a_kb_{n-k}$ gives the functional equation $A'=AB$ implying that $B=(\log A)'.$ Cumulants are defined as the coefficients of the logarithm of the Moment generating function, so the generating function is $K(x)=\log(\mathbb{E}[e^{tX}]),$ and so $K'(x)=\mu +\sigma ^2x+O(x^2)$ giving, for example, the mean when evaluated at $x=0.$ The method of singular part transformation , when you use $u = (\log x)'$ as a change of variable (Present, for example, in the work of Painleve that gives raise to the Painleve equations). The digamma function is defined as $\psi(z)=(\log \Gamma(z))'$ and for example, evaluated at $z=1$ gives the euler-mascheroni constant $\gamma .$ The relation in between the zeta and sigma Weierstrass functions in number theory is given by this transformation i.e., $\zeta (z,\Lambda) =(\log \sigma(z,\Lambda))'.$ In the multiplicative calculus , the derivative is defined as $$f^*(x)=\lim _{h\rightarrow  \infty}\left (\frac{f(x+h)}{f(x)}\right )^{1/h},$$ it turns out that if $f$ is positive and differentiable at $f$ one has the relation $$\ln (f^*(x))=\left (\ln f\right )'.$$ reference The Maurer-Cartan form on matrix lie groups look like $\omega _g=g^{-1}dg$ as established here In the proof of a condition for the $\mu -$ equidistribution of a sequence in the space of conjugacy classes of a compact group G, with $\mu$ the Haar measure, by some properties of the L-functions $L(s,\rho).$ Discussed, for example, here . To pass from Witt components to ghost components in the theory of Witt vectors. i.e., $$\left (\log \left (\prod _{n=1}^{\infty}(1-x_nt^n)\right )\right )'=\sum _{n=1}^{\infty}w_n(x)t^n.$$ This is done, for example, here. In the proof of the Gauss-Lucas theorem that says that the roots of $p'$ are in the convex hull of the roots of $p$ where $p$ is a polynomial. The analysis is done thru $p'/p.$ See e.g., here. The argument principle theorem relates the integral of the logarithmic derivative of a function with the difference in between the poles and the zeros. I kind of get that this transformation eliminates simple singularities and changes roots to poles and viceversa but I would like to know other parts of math where this is useful or if there is a more general idea of why this transformation is so powerful.","I am curious about a certain transformation called the logarithmic derivative that seems to appear a lot in different cool ideas, for example: The use in generating functions for recursions of the form gives the functional equation implying that Cumulants are defined as the coefficients of the logarithm of the Moment generating function, so the generating function is and so giving, for example, the mean when evaluated at The method of singular part transformation , when you use as a change of variable (Present, for example, in the work of Painleve that gives raise to the Painleve equations). The digamma function is defined as and for example, evaluated at gives the euler-mascheroni constant The relation in between the zeta and sigma Weierstrass functions in number theory is given by this transformation i.e., In the multiplicative calculus , the derivative is defined as it turns out that if is positive and differentiable at one has the relation reference The Maurer-Cartan form on matrix lie groups look like as established here In the proof of a condition for the equidistribution of a sequence in the space of conjugacy classes of a compact group G, with the Haar measure, by some properties of the L-functions Discussed, for example, here . To pass from Witt components to ghost components in the theory of Witt vectors. i.e., This is done, for example, here. In the proof of the Gauss-Lucas theorem that says that the roots of are in the convex hull of the roots of where is a polynomial. The analysis is done thru See e.g., here. The argument principle theorem relates the integral of the logarithmic derivative of a function with the difference in between the poles and the zeros. I kind of get that this transformation eliminates simple singularities and changes roots to poles and viceversa but I would like to know other parts of math where this is useful or if there is a more general idea of why this transformation is so powerful.","a_{n+1}=\sum _{k=0}^n\binom{n}{k}a_kb_{n-k} A'=AB B=(\log A)'. K(x)=\log(\mathbb{E}[e^{tX}]), K'(x)=\mu +\sigma ^2x+O(x^2) x=0. u = (\log x)' \psi(z)=(\log \Gamma(z))' z=1 \gamma . \zeta (z,\Lambda) =(\log \sigma(z,\Lambda))'. f^*(x)=\lim _{h\rightarrow  \infty}\left (\frac{f(x+h)}{f(x)}\right )^{1/h}, f f \ln (f^*(x))=\left (\ln f\right )'. \omega _g=g^{-1}dg \mu - \mu L(s,\rho). \left (\log \left (\prod _{n=1}^{\infty}(1-x_nt^n)\right )\right )'=\sum _{n=1}^{\infty}w_n(x)t^n. p' p p p'/p.","['ordinary-differential-equations', 'reference-request', 'special-functions', 'generating-functions']"
67,Difference in bounds of convolution integral,Difference in bounds of convolution integral,,"In my ODE class, I was taught that the convolution of two functions is calculated by: $$\left(f*g\right)\left(t\right)=\int_0^t{f\left(u\right)g\left(t-u\right)du}$$ According to Wikipedia , the convolution is defined as: $$\left(f*g\right)\left(t\right)=\int_{-\infty}^\infty{f\left(u\right)g\left(t-u\right)du}$$ and can be simplified to the definition I was taught if the functions $f\left(t\right)$ and $g\left(t\right)$ are only defined for $t\in\left[0, \infty\right)$ . $$$$ It makes sense why the lower bound would become $0$ when the functions are only defined for $t\in\left[0, \infty\right)$ , but I am confused why the upper bound becomes $t$ instead of remaining at $\infty$ . Wouldn't this change the value of the convolution? If so, do both definitions have the property that $\mathcal{L}^{-1}\left\{F\left(s\right)G\left(s\right)\right\} = \left(f*g\right)\left(t\right)$ ?","In my ODE class, I was taught that the convolution of two functions is calculated by: According to Wikipedia , the convolution is defined as: and can be simplified to the definition I was taught if the functions and are only defined for . It makes sense why the lower bound would become when the functions are only defined for , but I am confused why the upper bound becomes instead of remaining at . Wouldn't this change the value of the convolution? If so, do both definitions have the property that ?","\left(f*g\right)\left(t\right)=\int_0^t{f\left(u\right)g\left(t-u\right)du} \left(f*g\right)\left(t\right)=\int_{-\infty}^\infty{f\left(u\right)g\left(t-u\right)du} f\left(t\right) g\left(t\right) t\in\left[0, \infty\right)  0 t\in\left[0, \infty\right) t \infty \mathcal{L}^{-1}\left\{F\left(s\right)G\left(s\right)\right\} = \left(f*g\right)\left(t\right)","['ordinary-differential-equations', 'laplace-transform', 'convolution']"
68,First prolongation formula,First prolongation formula,,"My question is about the derivation of the prolongation formula from Olver's book:""Applications of Lie groups to differential equations"" Page 109. Considering a differential equation with independent variable(x) and one dependent variable(u): (x,u) $\subset$ $X \times U$ The coordinate in first jet space $M^{(1)}$ is (x, $u^{(1)}$ ) = (x,u, $u_j$ ). Let u = f(x) is any function with $u_j = \frac{\partial u}{\partial x_j} $ First prolongation of a group action on M is given as: $pr^{1} g_\epsilon . (x,u^{(1)}) = (\tilde{x},\tilde{u}^{(1)})$ The dependent variable is unchanged here; I mean $\tilde{u} = \tilde{f}_\epsilon(\tilde{x}) = f[\Xi^{-1}_\epsilon(\tilde{x})] = f[\Xi_{-\epsilon} (\tilde{x})] $ To find the infinitesimal generator of pr(1) go we must differentiate the formulas for the prolonged transformations with respect to e and set e = O. Thus $pr ^{1} v= \xi ^{i}(x)\frac{\partial}{\partial x^i}+ \phi ^{j}(x,u^{1})\frac{\partial}{\partial u^j},$ where $ \phi ^{j}(x,u^{1})= \frac{d}{ d\epsilon}_{\epsilon|=0} [\frac{\partial \Xi^k_{-\epsilon}}{\partial\tilde{x}^j} ] (\Xi_{\epsilon}(x)). u_{k}= - \frac{\partial \xi^k}{\partial x^j}(x).  $ It seems simple, but I don't know how to calculate $ \phi ^{j}(x,u^{1}) $ . Can anybody help me to find the answer? What kind of derivative is it? Olver obtained two types of terms multiplying $u_{k}$ , first: $\frac{\partial}{\partial\tilde{x}^j} [\frac{d\Xi^k_{-\epsilon}}{d\epsilon} ] (\Xi_{\epsilon}(x)) |_{\epsilon = 0} = \frac{\partial}{\partial x^j} [\frac{d\Xi^k_{\epsilon}}{d\epsilon} ] |_{\epsilon = 0} = - \frac{\partial \xi^k}{\partial x^j}(x)$ and second, $ \frac{\partial^{2} [\Xi^k_{-\epsilon} ]}{\partial\tilde{x}^j \partial\tilde{x}^l}  ((\Xi_{-\epsilon}(x))  [\frac{d\Xi^k_{\epsilon}}{d\epsilon} ](x) |_{\epsilon = 0} =0.$ Shoud I add this two term? How can I obtained them? Thank you in advance!","My question is about the derivation of the prolongation formula from Olver's book:""Applications of Lie groups to differential equations"" Page 109. Considering a differential equation with independent variable(x) and one dependent variable(u): (x,u) The coordinate in first jet space is (x, ) = (x,u, ). Let u = f(x) is any function with First prolongation of a group action on M is given as: The dependent variable is unchanged here; I mean To find the infinitesimal generator of pr(1) go we must differentiate the formulas for the prolonged transformations with respect to e and set e = O. Thus where It seems simple, but I don't know how to calculate . Can anybody help me to find the answer? What kind of derivative is it? Olver obtained two types of terms multiplying , first: and second, Shoud I add this two term? How can I obtained them? Thank you in advance!","\subset X \times U M^{(1)} u^{(1)} u_j u_j = \frac{\partial u}{\partial x_j}  pr^{1} g_\epsilon . (x,u^{(1)}) = (\tilde{x},\tilde{u}^{(1)}) \tilde{u} = \tilde{f}_\epsilon(\tilde{x}) = f[\Xi^{-1}_\epsilon(\tilde{x})] = f[\Xi_{-\epsilon} (\tilde{x})]  pr ^{1} v= \xi ^{i}(x)\frac{\partial}{\partial x^i}+ \phi ^{j}(x,u^{1})\frac{\partial}{\partial u^j},  \phi ^{j}(x,u^{1})= \frac{d}{ d\epsilon}_{\epsilon|=0} [\frac{\partial \Xi^k_{-\epsilon}}{\partial\tilde{x}^j} ] (\Xi_{\epsilon}(x)). u_{k}= - \frac{\partial \xi^k}{\partial x^j}(x).    \phi ^{j}(x,u^{1})  u_{k} \frac{\partial}{\partial\tilde{x}^j} [\frac{d\Xi^k_{-\epsilon}}{d\epsilon} ] (\Xi_{\epsilon}(x)) |_{\epsilon = 0} = \frac{\partial}{\partial x^j} [\frac{d\Xi^k_{\epsilon}}{d\epsilon} ] |_{\epsilon = 0} = - \frac{\partial \xi^k}{\partial x^j}(x)  \frac{\partial^{2} [\Xi^k_{-\epsilon} ]}{\partial\tilde{x}^j \partial\tilde{x}^l}  ((\Xi_{-\epsilon}(x))  [\frac{d\Xi^k_{\epsilon}}{d\epsilon} ](x) |_{\epsilon = 0} =0.","['linear-algebra', 'ordinary-differential-equations', 'differential-geometry', 'lie-groups']"
69,"How I can solve this differential equation: $y'''(x)+ax\,y(x)=0$?",How I can solve this differential equation: ?,"y'''(x)+ax\,y(x)=0","I have done some attempts to solve the following ODE: $$y'''(x)+ax\,y(x)=0\,,$$ with $a >0$ . I put $y(x)= e^{rx}$ , with $r$ is an arbitrary  real number, then $y'''=r^3 e^{rx}$ , by substitution in the titled equation I got: $r^3 e^x +a x e^{rx}=0$ implies $ e^{rx}(r^3+a x)=0$ implies $x=\frac{-a}{r^3}$ this yield to $y(x)= \exp(\frac{-a x}{r^3})$ , But I didn't got the same result using wolfram alpha as shown here , The result were given by Generalized hypergeometric function, Any help?","I have done some attempts to solve the following ODE: with . I put , with is an arbitrary  real number, then , by substitution in the titled equation I got: implies implies this yield to , But I didn't got the same result using wolfram alpha as shown here , The result were given by Generalized hypergeometric function, Any help?","y'''(x)+ax\,y(x)=0\,, a >0 y(x)= e^{rx} r y'''=r^3 e^{rx} r^3 e^x +a x e^{rx}=0  e^{rx}(r^3+a x)=0 x=\frac{-a}{r^3} y(x)= \exp(\frac{-a x}{r^3})","['real-analysis', 'ordinary-differential-equations', 'mathematical-physics', 'hypergeometric-function']"
70,Solving $\int \sqrt {f(x)} \ dx=\sqrt{\int f(x)\ dx}$,Solving,\int \sqrt {f(x)} \ dx=\sqrt{\int f(x)\ dx},"I am trying to solve the following differential equation: $$\int \sqrt {f(x)} \ dx=\sqrt{\int f(x)\  dx}$$ My approach: I started by differentiating both sides and got: $$\sqrt {f(x)}=f(x) \frac{1}{2\sqrt{\int f(x)\  dx}}$$ We can isolate the $2\sqrt{\int f(x)\  dx}$ in the left side: $$2\sqrt{\int f(x)\  dx} = \sqrt{f(x)}$$ We can now square both sides: $$4\int f(x)\  dx = f(x) \ \ \ \ \ \ \ \ \ (1)$$ We can again differentiate both sides and get: $$4f = f'$$ And the solution of this equation is: $f(x)= Ce^{4x}$ My question is the following: In the step where I putted a (1), it should be $$4 \left|\int f(x)\  dx\right| = |f(x)|$$ Instead of $4\int f(x)\  dx = f(x)$ , because $\sqrt{a}^2 = |a|$ . How could I solve this differential equation if I did not make the assumption that $f(x) > 0$ and $\int f(x) dx > 0$ and got rid of the modulus?","I am trying to solve the following differential equation: My approach: I started by differentiating both sides and got: We can isolate the in the left side: We can now square both sides: We can again differentiate both sides and get: And the solution of this equation is: My question is the following: In the step where I putted a (1), it should be Instead of , because . How could I solve this differential equation if I did not make the assumption that and and got rid of the modulus?",\int \sqrt {f(x)} \ dx=\sqrt{\int f(x)\  dx} \sqrt {f(x)}=f(x) \frac{1}{2\sqrt{\int f(x)\  dx}} 2\sqrt{\int f(x)\  dx} 2\sqrt{\int f(x)\  dx} = \sqrt{f(x)} 4\int f(x)\  dx = f(x) \ \ \ \ \ \ \ \ \ (1) 4f = f' f(x)= Ce^{4x} 4 \left|\int f(x)\  dx\right| = |f(x)| 4\int f(x)\  dx = f(x) \sqrt{a}^2 = |a| f(x) > 0 \int f(x) dx > 0,"['integration', 'ordinary-differential-equations']"
71,How to solve this ODE system with an implied integration within it?,How to solve this ODE system with an implied integration within it?,,"I have this system of differential equations in time, which I am trying to solve numerically through a matlab code: $$ \frac{df(x,t)}{dt} = K(t) -Af(x,t) - B\sigma \left[C f(x,t)g(x,t)+\frac{(1-C)}{D^2}\int_0^\infty x'e^{\frac{-x'}{D^2}}f(x,t)g(x,t)dx'\right]$$ $$ \frac{dg(x,t)}{dt} = \beta Af(x,t) + B\sigma f(x,t)g(x,t) - \frac{B}{x}g(x,t) $$ Where A, B, $\beta$ , $\sigma$ , C and D are known constants. I also know K(t) time dependency is gaussian. I have no idea how to solve this, as I have no information about neiter $f(x,t)$ nor $g(x,t).$ The only thing I know is that both are $0$ at $t=0$ and at $x=0$ . I don't want the solution to the problem, just possible ways to solve it numerically.","I have this system of differential equations in time, which I am trying to solve numerically through a matlab code: Where A, B, , , C and D are known constants. I also know K(t) time dependency is gaussian. I have no idea how to solve this, as I have no information about neiter nor The only thing I know is that both are at and at . I don't want the solution to the problem, just possible ways to solve it numerically.","
\frac{df(x,t)}{dt} = K(t) -Af(x,t) - B\sigma \left[C f(x,t)g(x,t)+\frac{(1-C)}{D^2}\int_0^\infty x'e^{\frac{-x'}{D^2}}f(x,t)g(x,t)dx'\right] 
\frac{dg(x,t)}{dt} = \beta Af(x,t) + B\sigma f(x,t)g(x,t) - \frac{B}{x}g(x,t)
 \beta \sigma f(x,t) g(x,t). 0 t=0 x=0","['integration', 'ordinary-differential-equations']"
72,Maximal Solution of ODE Bounded by a Positive Function,Maximal Solution of ODE Bounded by a Positive Function,,"Let $F:\mathbb{R}^{1+d}\rightarrow\mathbb{R}^{d}$ continuous such that $\|F(t,x)\|\leq g(\|x\|)$ for some function $g:[0,\infty)\rightarrow\mathbb{R}$ satisfying \begin{equation} \int_{0}^{\infty}\frac{ds}{g(s)}=\infty. \;\;\;\;\;\;\;\;\;\;\;(1) \end{equation} Prove that all maximum solution of $x'=F(t,x)$ with $x(t_0)=0$ are definite for all $t\geq t_0$ . I tried it this way: By Picard Theorem, we can write the ODE solution as $$\gamma(t)=\int_{t_0}^{t}F(s,\gamma(s))ds.$$ So, by hypothesis we have $$\|\gamma(t)\|\leq\int_{t_0}^{t}g(\|\gamma(s)\|)ds.$$ From here, I didn't know how to continue. I've no idea how to use $(1)$ . Somebody can help me? Any hint? Thanks! :D","Let continuous such that for some function satisfying Prove that all maximum solution of with are definite for all . I tried it this way: By Picard Theorem, we can write the ODE solution as So, by hypothesis we have From here, I didn't know how to continue. I've no idea how to use . Somebody can help me? Any hint? Thanks! :D","F:\mathbb{R}^{1+d}\rightarrow\mathbb{R}^{d} \|F(t,x)\|\leq g(\|x\|) g:[0,\infty)\rightarrow\mathbb{R} \begin{equation}
\int_{0}^{\infty}\frac{ds}{g(s)}=\infty. \;\;\;\;\;\;\;\;\;\;\;(1)
\end{equation} x'=F(t,x) x(t_0)=0 t\geq t_0 \gamma(t)=\int_{t_0}^{t}F(s,\gamma(s))ds. \|\gamma(t)\|\leq\int_{t_0}^{t}g(\|\gamma(s)\|)ds. (1)",['ordinary-differential-equations']
73,Book for The Lyapunov Function and topologically equivalent linear systems,Book for The Lyapunov Function and topologically equivalent linear systems,,"I encountered the concept of the Lyapunov Function from the book Ordinary Differential Equations by Arnold . The book states the following lemma: Let $A:\mathbb{R}^n \to \mathbb{R}^n$ be a linear operator, all of whose eigenvalues have positive real part. Then the system $$ \dot{x} = Ax, \quad x \in \mathbb{R}^n $$ is topologically equivalent to the standard system $$ \dot{x} = x, \quad x \in \mathbb{R}^n $$ The author states that the proof of this lemma is based on the construction of a special quadratic function, the so-called Lyapunov function. However, when I searched for the term Lyapunove Function, it seems that it is close related to a theory about equilibrium point, which is not like what this book described. I then think that I need more referrences to understand this topic. Any recommendations?","I encountered the concept of the Lyapunov Function from the book Ordinary Differential Equations by Arnold . The book states the following lemma: Let be a linear operator, all of whose eigenvalues have positive real part. Then the system is topologically equivalent to the standard system The author states that the proof of this lemma is based on the construction of a special quadratic function, the so-called Lyapunov function. However, when I searched for the term Lyapunove Function, it seems that it is close related to a theory about equilibrium point, which is not like what this book described. I then think that I need more referrences to understand this topic. Any recommendations?","A:\mathbb{R}^n \to \mathbb{R}^n 
\dot{x} = Ax, \quad x \in \mathbb{R}^n
 
\dot{x} = x, \quad x \in \mathbb{R}^n
","['ordinary-differential-equations', 'reference-request', 'book-recommendation', 'lyapunov-functions']"
74,Solution to nonlinear ODE,Solution to nonlinear ODE,,"Find all solutions the to initial value problem $$\frac{dy}{dx}=y^3e^{\sin{(x+y^2)}}, \quad y(2020)=0$$ and explain why there are no other solutions. So there exists a unique solution due to the existence and uniqueness theorem centred at $x=2020$ . Do we use separation of variables here?  Thanks for the help.",Find all solutions the to initial value problem and explain why there are no other solutions. So there exists a unique solution due to the existence and uniqueness theorem centred at . Do we use separation of variables here?  Thanks for the help.,"\frac{dy}{dx}=y^3e^{\sin{(x+y^2)}}, \quad y(2020)=0 x=2020","['calculus', 'ordinary-differential-equations']"
75,Bounded Maximal Solutions of ODE System,Bounded Maximal Solutions of ODE System,,"Let $(x(t), y(t))$ be a maximal solution of the following diferential equation: $(x,y)' = (-x + y, \hspace{0.2cm} log(10+|x|) - y)$ and consider $t_0 \in \mathbb{R}$ a point in its domain. Show that both real functions $x(t)$ and $y(t)$ are limited for $t \geq t_0$ . Attempts : I've tried to write $x(t)$ and $y(t)$ in a way that makes it possible to apply Gronwall's Lemma for each one of them, but the inequalities don't work quite right. Trying to solve this ode system without using exponencial of matrices (which we can't use) seems difficult enough. Any help is appreciated!","Let be a maximal solution of the following diferential equation: and consider a point in its domain. Show that both real functions and are limited for . Attempts : I've tried to write and in a way that makes it possible to apply Gronwall's Lemma for each one of them, but the inequalities don't work quite right. Trying to solve this ode system without using exponencial of matrices (which we can't use) seems difficult enough. Any help is appreciated!","(x(t), y(t)) (x,y)' = (-x + y, \hspace{0.2cm} log(10+|x|) - y) t_0 \in \mathbb{R} x(t) y(t) t \geq t_0 x(t) y(t)","['ordinary-differential-equations', 'analysis']"
76,Solve $(1-x^2)y^{\prime\prime} + xy^\prime-y=0$ using Frobenius method,Solve  using Frobenius method,(1-x^2)y^{\prime\prime} + xy^\prime-y=0,"I'm stuck with this problem because the equation has 2 singular regular points $x =-1$ and $x=1$ , so how can I get the solution?","I'm stuck with this problem because the equation has 2 singular regular points and , so how can I get the solution?",x =-1 x=1,['ordinary-differential-equations']
77,How to construct the matrix of a 2-D system of differential equations,How to construct the matrix of a 2-D system of differential equations,,"The task is to find and classify the fixed (equilibrium) points of the system : $$ \begin{cases} \dot x = (2x - y) (x - 2) \\ \dot y = xy - 2\end{cases}$$ Solving $$ \begin{cases} \dot x = (2x - y) (x - 2) = 0 \\ \dot y = xy - 2 = 0\end{cases}$$ gives three points $(1,2), (2,1)$ and $(-1,-2)$ . I have already classified the first two so no problems with those. However, for $(-1, -2)$ , if we shift the origin by changing the variables $$ \begin{cases} x = p - 1 \\ y = q - 2\end{cases}$$ we have $$ \begin{cases} \dot p = (2x - y) (x - 2) = (2(p - 1) - q + 2 )(p - 3) = -6p + 3q -pq + 2p^2\\ \dot q = xy - 2 =(p-1)(p-2) + 2 = -2p - q + pq\end{cases}$$ Now, to find the linear approximation around the origin, I know that the higher-order terms can be ignored. What about the $pq$ terms? If I ignore them too, then I'll have $$\begin{cases} \dot p \approx -6p + 3q\\ \dot q \approx -2p - q \end{cases} \implies \lambda_{1,2} = \{-3, -4\}$$ Which means the equilibrium point is a sink or stable non-degenerate node . Is that right?","The task is to find and classify the fixed (equilibrium) points of the system : Solving gives three points and . I have already classified the first two so no problems with those. However, for , if we shift the origin by changing the variables we have Now, to find the linear approximation around the origin, I know that the higher-order terms can be ignored. What about the terms? If I ignore them too, then I'll have Which means the equilibrium point is a sink or stable non-degenerate node . Is that right?"," \begin{cases} \dot x = (2x - y) (x - 2) \\ \dot y = xy - 2\end{cases}  \begin{cases} \dot x = (2x - y) (x - 2) = 0 \\ \dot y = xy - 2 = 0\end{cases} (1,2), (2,1) (-1,-2) (-1, -2)  \begin{cases} x = p - 1 \\ y = q - 2\end{cases}  \begin{cases} \dot p = (2x - y) (x - 2) = (2(p - 1) - q + 2 )(p - 3) = -6p + 3q -pq + 2p^2\\ \dot q = xy - 2 =(p-1)(p-2) + 2 = -2p - q + pq\end{cases} pq \begin{cases} \dot p \approx -6p + 3q\\ \dot q \approx -2p - q \end{cases} \implies \lambda_{1,2} = \{-3, -4\}","['ordinary-differential-equations', 'systems-of-equations', 'approximation-theory', 'linear-approximation']"
78,Wrong notation/domain for the functions in this system of ODEs?,Wrong notation/domain for the functions in this system of ODEs?,,"For $x: \mathbb R\rightarrow \mathbb R^n$ and $u:\mathbb R \rightarrow \mathbb R^m$ we have the constrained ordinary differential equation (ODE) \begin{align} \dot x(t) &= f(x(t),u(t)) \tag 1\\ 0&\geq h(x(t),u(t)) \tag 2 \end{align} where $f:\mathbb R^{n\times m} \rightarrow\mathbb R^n$ and $h: \mathbb R^{n\times m}\rightarrow \mathbb R^n$ . From the notation I guess the domain of $f$ and $h$ are matrices, i.e. $\mathbb R^{n\times m}$ .  But is that not a typo? Should it not be $+$ instead of $\times$ ? I mean the following: $f$ and $h$ are a compositions of $x$ with the range $\mathbb R^n$ and $u$ with the range $\mathbb R^m$ . Therefore \begin{align} f&:\mathbb R^n \times \mathbb R^m = \mathbb R^{n+m} \rightarrow \mathbb R^n \tag 3 \\ g&:\mathbb R^n \times \mathbb R^m = \mathbb R^{n+m} \rightarrow \mathbb R^n \tag 4 \end{align} I.e. the cartesian product of $\mathbb R^n$ and $\mathbb R^m$ . Is this correct or am I wrong?","For and we have the constrained ordinary differential equation (ODE) where and . From the notation I guess the domain of and are matrices, i.e. .  But is that not a typo? Should it not be instead of ? I mean the following: and are a compositions of with the range and with the range . Therefore I.e. the cartesian product of and . Is this correct or am I wrong?","x: \mathbb R\rightarrow \mathbb R^n u:\mathbb R \rightarrow \mathbb R^m \begin{align}
\dot x(t) &= f(x(t),u(t)) \tag 1\\
0&\geq h(x(t),u(t)) \tag 2
\end{align} f:\mathbb R^{n\times m} \rightarrow\mathbb R^n h: \mathbb R^{n\times m}\rightarrow \mathbb R^n f h \mathbb R^{n\times m} + \times f h x \mathbb R^n u \mathbb R^m \begin{align}
f&:\mathbb R^n \times \mathbb R^m = \mathbb R^{n+m} \rightarrow \mathbb R^n \tag 3
\\
g&:\mathbb R^n \times \mathbb R^m = \mathbb R^{n+m} \rightarrow \mathbb R^n \tag 4
\end{align} \mathbb R^n \mathbb R^m","['calculus', 'linear-algebra', 'ordinary-differential-equations', 'multivariable-calculus', 'dynamical-systems']"
79,Is ODE factorisation with split-quaternions possible?,Is ODE factorisation with split-quaternions possible?,,"In Simple complex ODE's in matrix form? we discussed how we can solve certain complex ODE's in matrix form. As an example: $$y(x)^2+y'(x)^2=0$$ can be factored as $$(y(x)+ iy'(x))(y(x)-iy'(x))=0$$ and if we insist we could solve the two above equations by representing each term in a matrix form, i.e. $$y(x)\equiv \begin{pmatrix}y_r(x)&-y_i(x)\\y_i(x)& y_r(x)\end{pmatrix}\\ y'(x)\equiv \begin{pmatrix}y'_r(x)&-y'_i(x)\\y'_i(x)& y'_r(x)\end{pmatrix}\\ i \equiv  \begin{pmatrix}0&-1\\1& 0\end{pmatrix}\\ $$ Multiplying the matrices out we get a system of coupled ODE's and the solution is found. However, I was trying to apply this technique for a different problem: $$y(x)^2+y'(x)^2-1=0$$ This expression cannot be factored as a product of two complex numbers (except the trivial factoring of 1). However, I tried to factor it over different types of numbers and in this post we discussed, that split-quaternions allow the following factorisation: $$(y(x)+i y'(x)+j)(y(x)-i y'(x)-j)=0$$ Just like regular complex numbers allow a representation by 2x2 real matrices, split-quaternions can be written as 2x2 complex matrices. Wikipedia . The true solution is: $${y(x) = c_1\sin(x)}\\  {y(x) = c_2\cos(x)}$$ and what we get from the method I developed is the following $$y(x)= c_1 \cos(x)-c_2 \sin(x)+\\(c_1 \sin(x)+c_2 \cos(x))i+ \\(c_4 \sin(x)+c_3 \cos(x)-1)j+ \\(c_4 \cos(x)-c_3 \sin(x))k$$ So that $\mathcal{R}\{y(x)\}=c_1 \cos(x)-c_2 \sin(x)$ and the results agree. My question is: when can I use this technique? Is it legitimate or is it just a lucky coincidence?","In Simple complex ODE's in matrix form? we discussed how we can solve certain complex ODE's in matrix form. As an example: can be factored as and if we insist we could solve the two above equations by representing each term in a matrix form, i.e. Multiplying the matrices out we get a system of coupled ODE's and the solution is found. However, I was trying to apply this technique for a different problem: This expression cannot be factored as a product of two complex numbers (except the trivial factoring of 1). However, I tried to factor it over different types of numbers and in this post we discussed, that split-quaternions allow the following factorisation: Just like regular complex numbers allow a representation by 2x2 real matrices, split-quaternions can be written as 2x2 complex matrices. Wikipedia . The true solution is: and what we get from the method I developed is the following So that and the results agree. My question is: when can I use this technique? Is it legitimate or is it just a lucky coincidence?","y(x)^2+y'(x)^2=0 (y(x)+ iy'(x))(y(x)-iy'(x))=0 y(x)\equiv \begin{pmatrix}y_r(x)&-y_i(x)\\y_i(x)& y_r(x)\end{pmatrix}\\
y'(x)\equiv \begin{pmatrix}y'_r(x)&-y'_i(x)\\y'_i(x)& y'_r(x)\end{pmatrix}\\
i \equiv  \begin{pmatrix}0&-1\\1& 0\end{pmatrix}\\  y(x)^2+y'(x)^2-1=0 (y(x)+i y'(x)+j)(y(x)-i y'(x)-j)=0 {y(x) = c_1\sin(x)}\\
 {y(x) = c_2\cos(x)} y(x)= c_1 \cos(x)-c_2 \sin(x)+\\(c_1 \sin(x)+c_2 \cos(x))i+ \\(c_4 \sin(x)+c_3 \cos(x)-1)j+ \\(c_4 \cos(x)-c_3 \sin(x))k \mathcal{R}\{y(x)\}=c_1 \cos(x)-c_2 \sin(x)","['ordinary-differential-equations', 'factoring', 'quaternions']"
80,Nonlinear 1st-Order Perturbed ODE,Nonlinear 1st-Order Perturbed ODE,,"Exercise: Solve the following nonlinear problem, for $\,0<\epsilon\ll1$ $$ \left\{\, \begin{aligned} &y'' +\frac{y'}{y^2} \epsilon - y' = 0,\qquad -\infty<x<\infty\\[1mm] &y(-\infty) = 1, \ y(\infty)=\epsilon \end{aligned} \right. $$ Then, study the asymptotic behaviour of its solution for $\,x<0\,$ and $\,x\approx 0\,$ using the equation. Check the results comparing with the exact solution. My solution To solve the problem, we reduce the order introducing a constant $\,C$ $$ y' - \frac{\epsilon}{y} - y = C. $$ It is easy to see that the general solution does not satisfy the conditions at infinity if we set $\,C=0$ . Setting $\,C\neq 0\,$ and knowing $\,y\not\equiv 0$ , we can integrate the previous equation as follows $$ \int\frac{y\,dy}{y^2 + Cy + \epsilon} = x + D. $$ We notice that if $\,C^2 - 4\epsilon < 0\,$ then $\,y^2 + Cy + \epsilon > 0\,$ because $$ y^2 + Cy + \epsilon = \left(y + \frac{C}{2}\right)^2 +\epsilon - \frac{C^2}{4} $$ and we can compute the integral as (step by step integration omitted) $$ \int\frac{y\,dy}{y^2 + Cy + \epsilon} = \log\sqrt{y^2+Cy+\epsilon\,} - \frac{C}{\sqrt{4\epsilon - C^2}}\,\text{atan}\!\left(\frac{2y + C}{\sqrt{4\epsilon - C^2}}\right). $$ The expression above does not verify the boundary conditions. Therefore, let's assume the opposite, i.e, $\,C^2-4\epsilon\geq0$ . If $C^2=4\epsilon$ , $$ \int\frac{y\,dy}{y^2 + Cy + \epsilon} = \int\frac{y\,dy}{(y+C/2)^2} = -\frac{y}{y+C/2} + \log\,\left|y+\frac{C}{2}\right| $$ but $$ \lim_{x\to-\infty}\exp\left(-\frac{y(x)}{y(x) + C/2}\right)\cdot\left|y(x)+\frac{C}{2}\right| = 0 = \lim_{x\to-\infty}Ke^x  $$ if and only if $\,C = -2$ . This means $\,\epsilon = 1$ , which is a contradiction. The only possibility left is $\,C^2-4\epsilon>0$ , thus $$ y^2 + Cy + \epsilon = (y - y_1)(y - y_2), $$ where $\,y_1 > y_2\,$ and $\,C = - y_1 - y_2$ , $\,\epsilon = y_1y_2$ . We have to remark that $\,y(x)>0\,$ for all $\,x\in\mathbb{R}$ . Otherwise, it'd exist $x_0$ such that $y(x_0)=0$ and the equation would make no sense. Assuming $\,y_1>y_2>0$ , $$ \int\frac{y\,dy}{y^2 + Cy + \epsilon} = \int\frac{A}{y - y_1}\,dy\, + \int\frac{B}{y - y_2}\,dy = \frac{1}{y_1 - y_2}\log\frac{|y - y_1|^{y_1}}{|y - y_2|^{y_2}} $$ and back again, we apply the condition when $\,x\to-\infty\,$ to $$ \lim_{x\to-\infty} \frac{|y(x) - y_1|^{y_1}}{|y(x) - y_2|^{y_2}} = 0  = \lim_{x\to-\infty} Ke^{(y_1 - y_2)x}. $$ We deduce $\,y_1 = 1$ , hence $\,y_2 = \epsilon\,$ and $\,C = -1 - \epsilon$ . It is easy to prove that the solution verifies the condition at $\,\infty$ . Furthermore, considering the roots $y_1, y_2$ , the equation has the form $$ yy' = (y-1)(y-\epsilon), $$ and it can be proved $\,y(x)$ stays between $\,y_1$ and $\,y_2$ . So, we can assure $$ \frac{1 - y(x)}{(y(x) - \epsilon)^\epsilon} = Ke^{(1-\epsilon)x}. $$ I do not obtain a closed formula for $y(x)$ and the last equality still depends on $K$ , so how am I supposed to compare the asymptotic behaviour with the exact solution? Thanks in advance for the help.","Exercise: Solve the following nonlinear problem, for Then, study the asymptotic behaviour of its solution for and using the equation. Check the results comparing with the exact solution. My solution To solve the problem, we reduce the order introducing a constant It is easy to see that the general solution does not satisfy the conditions at infinity if we set . Setting and knowing , we can integrate the previous equation as follows We notice that if then because and we can compute the integral as (step by step integration omitted) The expression above does not verify the boundary conditions. Therefore, let's assume the opposite, i.e, . If , but if and only if . This means , which is a contradiction. The only possibility left is , thus where and , . We have to remark that for all . Otherwise, it'd exist such that and the equation would make no sense. Assuming , and back again, we apply the condition when to We deduce , hence and . It is easy to prove that the solution verifies the condition at . Furthermore, considering the roots , the equation has the form and it can be proved stays between and . So, we can assure I do not obtain a closed formula for and the last equality still depends on , so how am I supposed to compare the asymptotic behaviour with the exact solution? Thanks in advance for the help.","\,0<\epsilon\ll1 
\left\{\,
\begin{aligned}
&y'' +\frac{y'}{y^2} \epsilon - y' = 0,\qquad -\infty<x<\infty\\[1mm]
&y(-\infty) = 1, \ y(\infty)=\epsilon
\end{aligned}
\right.
 \,x<0\, \,x\approx 0\, \,C 
y' - \frac{\epsilon}{y} - y = C.
 \,C=0 \,C\neq 0\, \,y\not\equiv 0 
\int\frac{y\,dy}{y^2 + Cy + \epsilon} = x + D.
 \,C^2 - 4\epsilon < 0\, \,y^2 + Cy + \epsilon > 0\, 
y^2 + Cy + \epsilon = \left(y + \frac{C}{2}\right)^2 +\epsilon - \frac{C^2}{4}
 
\int\frac{y\,dy}{y^2 + Cy + \epsilon} = \log\sqrt{y^2+Cy+\epsilon\,} - \frac{C}{\sqrt{4\epsilon - C^2}}\,\text{atan}\!\left(\frac{2y + C}{\sqrt{4\epsilon - C^2}}\right).
 \,C^2-4\epsilon\geq0 C^2=4\epsilon 
\int\frac{y\,dy}{y^2 + Cy + \epsilon} = \int\frac{y\,dy}{(y+C/2)^2} = -\frac{y}{y+C/2} + \log\,\left|y+\frac{C}{2}\right|
 
\lim_{x\to-\infty}\exp\left(-\frac{y(x)}{y(x) + C/2}\right)\cdot\left|y(x)+\frac{C}{2}\right| = 0 = \lim_{x\to-\infty}Ke^x 
 \,C = -2 \,\epsilon = 1 \,C^2-4\epsilon>0 
y^2 + Cy + \epsilon = (y - y_1)(y - y_2),
 \,y_1 > y_2\, \,C = - y_1 - y_2 \,\epsilon = y_1y_2 \,y(x)>0\, \,x\in\mathbb{R} x_0 y(x_0)=0 \,y_1>y_2>0 
\int\frac{y\,dy}{y^2 + Cy + \epsilon} = \int\frac{A}{y - y_1}\,dy\, + \int\frac{B}{y - y_2}\,dy = \frac{1}{y_1 - y_2}\log\frac{|y - y_1|^{y_1}}{|y - y_2|^{y_2}}
 \,x\to-\infty\, 
\lim_{x\to-\infty} \frac{|y(x) - y_1|^{y_1}}{|y(x) - y_2|^{y_2}} = 0  = \lim_{x\to-\infty} Ke^{(y_1 - y_2)x}.
 \,y_1 = 1 \,y_2 = \epsilon\, \,C = -1 - \epsilon \,\infty y_1, y_2 
yy' = (y-1)(y-\epsilon),
 \,y(x) \,y_1 \,y_2 
\frac{1 - y(x)}{(y(x) - \epsilon)^\epsilon} = Ke^{(1-\epsilon)x}.
 y(x) K",['ordinary-differential-equations']
81,What is this integrating-factor approach to homogeneous ODE's good for?: Better for some equations? Illustrates something worthwhile?,What is this integrating-factor approach to homogeneous ODE's good for?: Better for some equations? Illustrates something worthwhile?,,"We have an ODE of the form $$M(x,y)\,dx + N(x,y)\,dy = 0$$ in which both $M$ and $N$ are homogeneous functions of the same degree. The standard method for handling such an equation (assuming it's not separable or linear, in which case(s) we have a choice of ""standard methods"") is to use the substitution $\{y=ux, dy=udx+xdu\}$ , obtaining a separable equation. If the resulting integral in 'u' fails to be nice (the one on the 'x' side is always nice), we can try swapping the roles of $x$ and $y$ for an alternative substitution. This is all very standard, AFAIK. Now, from an exercise in a book (Boyce & DiPrima), I see that any homogeneous equation may be made exact via the integrating factor $$\mu(x,y)=\frac{1}{xM(x,y)+yN(x,y)}.$$ This is great, it illustrates that integrating factors can exist that aren't constant in either variable, and doing it this way is an interesting exercise, as is proving that the given $\mu$ actually works. However... is there any example of a homogeneous DE where this is a better method than just using the normal substitutions? I haven't encountered an example that makes me think: ""This is the nice way to do it!"" Can anyone display a nice example, that plays to the strengths of this technique? Does this method perhaps exist more as a curiosity than as a practical tool? Does the resulting exact equation give us access to some kind of insight we would not have encountered the other way? Is the interesting part simply the fact that any homogeneous equation can be made exact?","We have an ODE of the form in which both and are homogeneous functions of the same degree. The standard method for handling such an equation (assuming it's not separable or linear, in which case(s) we have a choice of ""standard methods"") is to use the substitution , obtaining a separable equation. If the resulting integral in 'u' fails to be nice (the one on the 'x' side is always nice), we can try swapping the roles of and for an alternative substitution. This is all very standard, AFAIK. Now, from an exercise in a book (Boyce & DiPrima), I see that any homogeneous equation may be made exact via the integrating factor This is great, it illustrates that integrating factors can exist that aren't constant in either variable, and doing it this way is an interesting exercise, as is proving that the given actually works. However... is there any example of a homogeneous DE where this is a better method than just using the normal substitutions? I haven't encountered an example that makes me think: ""This is the nice way to do it!"" Can anyone display a nice example, that plays to the strengths of this technique? Does this method perhaps exist more as a curiosity than as a practical tool? Does the resulting exact equation give us access to some kind of insight we would not have encountered the other way? Is the interesting part simply the fact that any homogeneous equation can be made exact?","M(x,y)\,dx + N(x,y)\,dy = 0 M N \{y=ux, dy=udx+xdu\} x y \mu(x,y)=\frac{1}{xM(x,y)+yN(x,y)}. \mu","['ordinary-differential-equations', 'homogeneous-equation', 'integrating-factor']"
82,Why does the Jacobi accessory equation coincide with the Euler-Lagrange equation for the perturbation function?,Why does the Jacobi accessory equation coincide with the Euler-Lagrange equation for the perturbation function?,,"I'm studying the second variation in Calculus of Variation. For functional $\int_{a}^{b}f(t,y,y')$ ( $y(a), y(b)$ are fixed constants), the second variation $\delta^2J[\eta,y]$ is given by $$\int_{a}^{b}\frac{1}{2}f_{y'y'}\eta'^2 + \frac{1}{2}(f_{yy}-\frac{d}{dx}f_{yy'})\eta^2 dx$$ where $y$ is chosen to be an extremal curve, i.e. the one that satisfies the Euler-Lagrange equation. And the Jacobi accessory equation is given by $$\frac{d}{dx}(f_{y'y'}u') - (f_{yy}-\frac{d}{dx}f_{yy'})u=0$$ which coincides (in form) with the Euler lagrange equation for the functional $G[u]:=\delta^2J[u,y]$ . Namely, if we consider the fucntional $\delta^2[\eta,y]$ as a functional of $\eta$ , writing down the EL eqn. will give you the Jacobi accessory equation. So, why is that? Why do they coincide?","I'm studying the second variation in Calculus of Variation. For functional ( are fixed constants), the second variation is given by where is chosen to be an extremal curve, i.e. the one that satisfies the Euler-Lagrange equation. And the Jacobi accessory equation is given by which coincides (in form) with the Euler lagrange equation for the functional . Namely, if we consider the fucntional as a functional of , writing down the EL eqn. will give you the Jacobi accessory equation. So, why is that? Why do they coincide?","\int_{a}^{b}f(t,y,y') y(a), y(b) \delta^2J[\eta,y] \int_{a}^{b}\frac{1}{2}f_{y'y'}\eta'^2 + \frac{1}{2}(f_{yy}-\frac{d}{dx}f_{yy'})\eta^2 dx y \frac{d}{dx}(f_{y'y'}u') - (f_{yy}-\frac{d}{dx}f_{yy'})u=0 G[u]:=\delta^2J[u,y] \delta^2[\eta,y] \eta","['ordinary-differential-equations', 'partial-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation', 'hamilton-jacobi-equation']"
83,Explanation on a proof of a lemma on the Adams-Bashforth method,Explanation on a proof of a lemma on the Adams-Bashforth method,,"Lemma: For the two-step Adams-Bashforth (AB) method $$T(t)=\frac{5}{12} y'''(t)h^2+\mathcal{O}(h^3),$$ where $T(t)$ denotes the local truncation error. By definition, $$T(t)=\frac{y(t+h)-y(t)}{h}-f(t,y(t)).$$ For the AB method, $$T(t)=\frac{y(t+h)-y(t)}{h}-\sum_{j=0}^{r}b_jf(t-jh,y(t-jh)).$$ The lemma is the result for $r=1$ .  By Taylor's theorem, \begin{align} \frac{y(t+h)-y(t)}{h}&=\frac{\left(y(t)+y'(t)h+y''(t)h^2/2+y'''(t)h^3/6+\mathcal{O}(h^4)\right)-y(t)}{h} \tag{1}\\ &=y'(t)+\frac{y''(t)}{2}h+\frac{y'''(t)}{6}h^2+\mathcal{O}(h^3) \tag{*} \end{align} For $r=1$ : \begin{align} \sum_{j=0}^{1}b_jf(t-jh,y(t-jh))&=b_0f(t,y(t))+b_1f(t-h,y(t-h)) \\ &=\frac{3}{2}f(t,y(t))-\frac{1}{2}f(t-h,y(t-h))  \ \ \ (b_0=3/2, \ b_1=-1/2)\\ &=\frac{3}{2}y'(t)-\frac{1}{2}y'(t-h) \ \ \ (y'(t)=f(t,y(t)) ) \\ &=\frac{3}{2}y'(t)-\frac{1}{2}\left(y'(t)-y''(t)h+\frac{1}{2}y'''(t)h^2+\mathcal{O}(h^3)\right) \tag{2} \\ &=y'(t)+\frac{y''(t)}{2}h-\frac{y'''(t)}{4}h^2+\mathcal{O}(h^3) \tag{**} \end{align} Now the result follows by subtracting $(**)$ from $(*)$ , but I don't understand how Taylor's theorem was used in steps $(1)$ and $(2)$ , i.e. the expansions of $y(t+h)$ and $y'(t-h)$ .","Lemma: For the two-step Adams-Bashforth (AB) method where denotes the local truncation error. By definition, For the AB method, The lemma is the result for .  By Taylor's theorem, For : Now the result follows by subtracting from , but I don't understand how Taylor's theorem was used in steps and , i.e. the expansions of and .","T(t)=\frac{5}{12} y'''(t)h^2+\mathcal{O}(h^3), T(t) T(t)=\frac{y(t+h)-y(t)}{h}-f(t,y(t)). T(t)=\frac{y(t+h)-y(t)}{h}-\sum_{j=0}^{r}b_jf(t-jh,y(t-jh)). r=1 \begin{align}
\frac{y(t+h)-y(t)}{h}&=\frac{\left(y(t)+y'(t)h+y''(t)h^2/2+y'''(t)h^3/6+\mathcal{O}(h^4)\right)-y(t)}{h} \tag{1}\\
&=y'(t)+\frac{y''(t)}{2}h+\frac{y'''(t)}{6}h^2+\mathcal{O}(h^3) \tag{*}
\end{align} r=1 \begin{align}
\sum_{j=0}^{1}b_jf(t-jh,y(t-jh))&=b_0f(t,y(t))+b_1f(t-h,y(t-h)) \\
&=\frac{3}{2}f(t,y(t))-\frac{1}{2}f(t-h,y(t-h))  \ \ \ (b_0=3/2, \ b_1=-1/2)\\
&=\frac{3}{2}y'(t)-\frac{1}{2}y'(t-h) \ \ \ (y'(t)=f(t,y(t)) ) \\
&=\frac{3}{2}y'(t)-\frac{1}{2}\left(y'(t)-y''(t)h+\frac{1}{2}y'''(t)h^2+\mathcal{O}(h^3)\right) \tag{2} \\
&=y'(t)+\frac{y''(t)}{2}h-\frac{y'''(t)}{4}h^2+\mathcal{O}(h^3) \tag{**}
\end{align} (**) (*) (1) (2) y(t+h) y'(t-h)","['ordinary-differential-equations', 'numerical-methods', 'proof-explanation', 'taylor-expansion']"
84,How the partial fraction decomposition works for finding this Inverse Laplace Transform?,How the partial fraction decomposition works for finding this Inverse Laplace Transform?,,"I've been working to find inverse Laplace transform for the following : $$ \frac{A}{(s-a)(s-r_1)(s-r_2)} $$ However, I'm getting stuck on the partial fraction decomposition. When I run the decomposition in Wolfram Alpha, it comes back as $$-\frac{A}{(s-r_1)(a - r_1)(r_1 - r_2)} -\frac{A}{(s - r_2)(a - r_2)(r_2 - r_1)} + \frac{A}{(a - r_1)(a - r_2)(s - a)}$$ Any thoughts on how this decomposition works? I can solve the inverse Laplace easily from this point but for the life of me I can't figure out how this partial fraction is working. Thanks for the help!","I've been working to find inverse Laplace transform for the following : However, I'm getting stuck on the partial fraction decomposition. When I run the decomposition in Wolfram Alpha, it comes back as Any thoughts on how this decomposition works? I can solve the inverse Laplace easily from this point but for the life of me I can't figure out how this partial fraction is working. Thanks for the help!","
\frac{A}{(s-a)(s-r_1)(s-r_2)}
 -\frac{A}{(s-r_1)(a - r_1)(r_1 - r_2)} -\frac{A}{(s - r_2)(a - r_2)(r_2 - r_1)} + \frac{A}{(a - r_1)(a - r_2)(s - a)}","['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
85,On Spivak's introduction to one parameter group of diffeomorphisms,On Spivak's introduction to one parameter group of diffeomorphisms,,"In Chapter 5 of M. Spivak's A Comprehensive Introduction To Differential Geometry he states the existence and uniqueness of integral curves $\alpha_x:(-b,b)\rightarrow U $ of an ODE with initial condition $$\alpha_x'(t)=f(\alpha(t)) $$ $$\alpha_x(0)=x $$ This is proven for a map $f:U\rightarrow \mathbb{R}^n$ defined in an open set $U$ and every $x\in B_a(x_0)$ for some $a>0$ and $x_0\in U$ such that the closed ball $\overline{B}_{2a}(x_0)\subset U$ , we also ask $f$ to be locally Lipschitz (there are as well some restriction on $b$ ). Next he introduces the flow $$\alpha:(-b,b)\times B_a(x_0)\rightarrow U\quad \;\text{as }\quad (t,x)\mapsto\alpha_x(t) $$ Ok! this is all great and fairly straightforward, but the next discussion has an important step that I do not follow, here it is: He argues that $\alpha$ is continuous (proven, no problem!) and since every $\alpha_y$ satisfies the condition $\alpha_y(0)=y$ , then we have $$\alpha:\{0\}\times \overline{B} _{a/2}(x_0)\rightarrow \overline{B} _{a/2}(x_0) $$ Then by continuity of $\alpha$ and compactness of $\{0\} \times  \overline{B} _{a/2}(x_0)$ , then there exists $\epsilon>0$ such that $$\alpha:(-\epsilon,\epsilon) \times B_{a/2}(x_0)\rightarrow B_a(x_0) $$ This last step I do not get, it does not seems trivial to me the existence of such $\epsilon$ . My attempt of give a proof of such statement is the following: First, since $B_a(x_0)$ is an open neighborhood of $x_0$ and $\alpha$ is continuous we have an open set $V=(-\epsilon,\epsilon)\times B_\delta(x_0)$ for some $\epsilon,\delta>0$ , such that $$\alpha(V)\subset B_a(x_0)$$ Here compactness should play an important role to bound $\delta$ , but I cannot see this. Can someone please give me a hint or full proof of this last statement? Thanks in advance!","In Chapter 5 of M. Spivak's A Comprehensive Introduction To Differential Geometry he states the existence and uniqueness of integral curves of an ODE with initial condition This is proven for a map defined in an open set and every for some and such that the closed ball , we also ask to be locally Lipschitz (there are as well some restriction on ). Next he introduces the flow Ok! this is all great and fairly straightforward, but the next discussion has an important step that I do not follow, here it is: He argues that is continuous (proven, no problem!) and since every satisfies the condition , then we have Then by continuity of and compactness of , then there exists such that This last step I do not get, it does not seems trivial to me the existence of such . My attempt of give a proof of such statement is the following: First, since is an open neighborhood of and is continuous we have an open set for some , such that Here compactness should play an important role to bound , but I cannot see this. Can someone please give me a hint or full proof of this last statement? Thanks in advance!","\alpha_x:(-b,b)\rightarrow U  \alpha_x'(t)=f(\alpha(t))  \alpha_x(0)=x  f:U\rightarrow \mathbb{R}^n U x\in B_a(x_0) a>0 x_0\in U \overline{B}_{2a}(x_0)\subset U f b \alpha:(-b,b)\times B_a(x_0)\rightarrow U\quad \;\text{as }\quad (t,x)\mapsto\alpha_x(t)  \alpha \alpha_y \alpha_y(0)=y \alpha:\{0\}\times \overline{B} _{a/2}(x_0)\rightarrow \overline{B} _{a/2}(x_0)  \alpha \{0\} \times  \overline{B} _{a/2}(x_0) \epsilon>0 \alpha:(-\epsilon,\epsilon) \times B_{a/2}(x_0)\rightarrow B_a(x_0)  \epsilon B_a(x_0) x_0 \alpha V=(-\epsilon,\epsilon)\times B_\delta(x_0) \epsilon,\delta>0 \alpha(V)\subset B_a(x_0) \delta","['ordinary-differential-equations', 'differential-geometry']"
86,'Completing' incomplete vector fields,'Completing' incomplete vector fields,,"Suppose $M$ is a smooth manifold with a smooth vector field $X$ on it. If $X$ is not a complete vector field (a complete vector field is one for which all integral curves exist for all time) is it possible to embed it in another smooth manifold and extend $X$ to a complete vector field on the new manifold? More precisely, can one find a smooth manifold $N$ , a complete vector field $Y$ on $N$ , and a smooth embedding $F : M \to N$ such that $F_*X = Y$ , i.e. for all $p \in M$ , we have $dF_p (X_p) = Y_{F (p)}$ ? For example, if $M = (0, 1) \subset \mathbb R$ and $X = \partial_x$ then clearly $X$ is not complete, but $M$ embeds inside $N = \mathbb R$ with the extension $Y = \partial_x$ , which is complete.","Suppose is a smooth manifold with a smooth vector field on it. If is not a complete vector field (a complete vector field is one for which all integral curves exist for all time) is it possible to embed it in another smooth manifold and extend to a complete vector field on the new manifold? More precisely, can one find a smooth manifold , a complete vector field on , and a smooth embedding such that , i.e. for all , we have ? For example, if and then clearly is not complete, but embeds inside with the extension , which is complete.","M X X X N Y N F : M \to N F_*X = Y p \in M dF_p (X_p) = Y_{F (p)} M = (0, 1) \subset \mathbb R X = \partial_x X M N = \mathbb R Y = \partial_x","['ordinary-differential-equations', 'differential-geometry', 'differential-topology', 'smooth-manifolds', 'vector-fields']"
87,Existence of a periodic solution,Existence of a periodic solution,,"Consider the differential equation $x'=f(t,x)$ , where $f(t,x)$ is continuously differentiable in $t$ and $x$ . Suppose that $f(t+T,x)=f(t,x)$ for all $t$ . Suppose there are constants $p$ , $q$ such that $f(t,p)\ge0,f(t,q)\le0$ for all $t$ . Prove that there is a periodic solution $x(t)$ for this equation with $p<x(0)<q$ I have seen this question here - Prove that there is a periodic solution $x(t)$ with $p<x(0)<q$ My professor claims that the statement is also true if the inequalities $f(t,p)\gt0,f(t,q)\lt0$ are not strict. However, the proof in the link depicts the idea that a solution for the ODE satisfying $x(0)=p$ satisfies $x(t)>p$ for all $t>0$ and uses the fact that the inequality is strict (otherwise, the point $\tau$ satisfying $\varphi(\tau) = p$ could be an inflection point). I'd appreciate any proof of this claim given the non-strict inequalities or a counterexample if the proposition is false.","Consider the differential equation , where is continuously differentiable in and . Suppose that for all . Suppose there are constants , such that for all . Prove that there is a periodic solution for this equation with I have seen this question here - Prove that there is a periodic solution $x(t)$ with $p<x(0)<q$ My professor claims that the statement is also true if the inequalities are not strict. However, the proof in the link depicts the idea that a solution for the ODE satisfying satisfies for all and uses the fact that the inequality is strict (otherwise, the point satisfying could be an inflection point). I'd appreciate any proof of this claim given the non-strict inequalities or a counterexample if the proposition is false.","x'=f(t,x) f(t,x) t x f(t+T,x)=f(t,x) t p q f(t,p)\ge0,f(t,q)\le0 t x(t) p<x(0)<q f(t,p)\gt0,f(t,q)\lt0 x(0)=p x(t)>p t>0 \tau \varphi(\tau) = p","['ordinary-differential-equations', 'dynamical-systems']"
88,Solve $x'(t)=x$,Solve,x'(t)=x,"Please correct me I have a differential equation to solve: $ x'(t)=x$ As I will interpret it, it would be $ \frac{dt}{dx}=x$ it's a separable and exact equation. I will multiply by $dx$ , integrate both sides and end up with $ x^2 = 2t+c$ I checked the answer on WolframA. and I'm wrong the answer is $ x(t)=ce^t$ . Could you please explain me why I'm wrong and give hints how to solve it properly. Thank you","Please correct me I have a differential equation to solve: As I will interpret it, it would be it's a separable and exact equation. I will multiply by , integrate both sides and end up with I checked the answer on WolframA. and I'm wrong the answer is . Could you please explain me why I'm wrong and give hints how to solve it properly. Thank you", x'(t)=x  \frac{dt}{dx}=x dx  x^2 = 2t+c  x(t)=ce^t,[]
89,Unique increasing solution to a separable differential equation (piecewise $C_1$),Unique increasing solution to a separable differential equation (piecewise ),C_1,"I want to find the increasing function $y(x): [0,1] \rightarrow [0,1]$ which is defined implicitely as the solution to the following equation: $f_1(x) = f_2(y(x)) \quad \forall x \in [0,1]$ where $f_1(), f_2()$ are known and $C_1$ on $[0,1]$ $f_1$ and $f_2$ are such that an increasing solution exists (I have existence already proved elsewhere). I want to show that there exists a unique increasing solution $y(x)$ starting from $y(0) = 0$ . This solution should be unique as long as the set such that $f_1’(x) = 0$ is of null measure .  The intuition is quite simple: both function goes through exactly the same points, in the same order (just not at the same speed) (you can see this graphically). So as long as there is no flat part, there should be a unique mapping from $x$ to $y$ . Remark: on the optimal path, given the equation, this equation will also imply that the set such that $f_2’(y(x)) = 0$ is also of null measure, and the null derivative coincides. Remark 2: Obviously, if $f_2()$ is strictly monotone, we can just invert it to find the solution: $y(x) = f_2^{-1}(f_1(x)) \quad \forall x \in [0,1]$ To solve my problem, since I’m solving for a function, I thought I could resort to properties of differential equations (Picard-Lindelof for e.g.); Differentiating my equation yields: $f_2’(y(x)) y’(x) = f_1’(x)$ . $\iff y’(x) = f_1’(x) / f_2’(y(x)) := g(x, y(x))$ when $f_2’(y) \neq 0$ By Picard-Lindelof, if $g()$ is $C_1$ on $[0,1]$ , then I have a unique solution. But this would correspond to the monotone case in my remark 2 above. The difficulty I have is with the points with null derivatives. These null derivatives yields problem because there is a indeterminacy in $g()$ on the optimal path ( $0$ over $0$ , since $y’ > 0$ ), and at other points the function is not even defined. So I cannot resort to Picard Lindelof. However, since I’m looking for an increasing solution , the solution should still be unique . But I cannot find any proof of that, or any variation of Picard-Lindelof doing this kind of stuff (i.e adding increasing constraint into the system). Hence my first question: Question: does a variation of Picard-Lindelof uniqueness result exists for my kind of problem? (i.e. problem with increasing constraint on the solution) would be great if I could just find a reference for this as I was not able to find any . Otherwise, I am trying to build the solution myself, resorting to some kind of pasting. But I’m not sure it’s correct to do it this way. The idea is that, I’m using Picard-Lindelof starting from $y(0)=0$ up to the first point where $f_2’(y) = 0$ that I denote $y_1$ . Then I’m doing it again from $y_1$ to $y_2$ (the second point where $f_2’(y) = 0$ ). And so on and so forth. There are two ‘problems’ with this solution: How do I ‘start’ from the $y_k$ points as generally speaking, $g(x_k, y_k(x_k))$ is not defined (or actually indeterminate $0$ over $0$ ), so I’m not sure I can write it down. I should have trouble with this notion of ‘points’ with null derivatives. What if they are an infinite number of such points? I think I can prove that it is not possible on the compact set [0,1] if $f_1, f_2$ are $C_1$ and if the set such that $f’_1 = 0$ is of null measure. But I’m not sure and there might exist some $x sin(x)$ case I did not think about (even if $x sin(x)$ is not $C_1$ in $0$ so it does not enter my setup). And more importantly, if I can resort to this concept of points, I can just use the original system directly: because it is piecewise monotone between those points, I can invert it between those points! All of this to say I’m really not sure about how to build a proof so far. I have a simple showcase example to illustrate the problem: Imagine $f_1(x) = 1 - 4 (x - 0.5)^2$ , $f_2(y) = 4(\sqrt{y} - y)$ . This example is built such that the unique increasing solution to this problem with initial value $y(0) = 0$ is $y(x) = x^2$ . The question is: how can I recover this using my differential equation $f’_2(y(x)) y’(x) = f’_1(x)$ ? By my method, I’ll just realize that $y_1 = 0.25$ and $x_1 = 0.5$ (and that there is only one point such that the derivatives are $0$ ). And I’ll just have to split the Picard-Lindelof in two parts: starting from $(x,y)=(0,0)$ to $(0.5, 0.25)$ ; and then from $(0.5, 0.25)$ to $(1, 1)$ . But even there, you see that the system of differential equations is ‘indeterminate’ at $(0.5, 0.25)$ , so how can I ‘start’ again from there? That’s why I find my ‘proof’ to be quite clumsy so far, and why I would like something more correct, if possible an already built result that I could cite or from which I could start my proof. EDIT: since my ODE is separable I should solve it by getting to the initial implicit form, so I’m not sure getting to the differential equation is even necessary; I thought it could be easier to get some standard results about uniqueness though. But maybe I could get them with the initial implicit definition of $y(x)$ by adding a monotonicity constraint: i.e. there exists only one strictly increasing part in the integral curve. But I don’t know how to show this. Sketch of an answer: If the set such that $f_1’(x) = 0$ has a finite number of points (instead of imposing measure zero), then I will do the proof as follows: Denote $x_k$ the ordered set of $K$ points such that $f_1’(x_k) = 0$ . Since we know the functions f_1, f_2, are such that an increasing solution exists, $f_2’(y)$ must also have the same finite number $K$ of points with $f_2’(y_k) = 0$ . Using the order of these points and the monotonicity constraint on the solution, we know that $y(x_k) = y_k \forall k$ . Now, I can just use the fact that $f_2()$ is piecewise monotone between each $y_k$ . By monotonicity, since $y_k = y(x_k)$ , then all $y \in [y_k, y_{k+1}]$ are images of $x \in [x_k, x_{k+1}]$ . This way starting from (x, y) = (0,0), we have: $\forall x \in [0, x_1), \quad f_2(y) = f_1(x)$ . Moreover, on $[0, y(x_1))$ , $f_2$ is monotone (and $C_1$ ). So it is inversible, and we get: $\forall x \in [0, x_1), \quad y = f_2^{-1}(f_1(x))$ . Then we have $y(x_1) = y_1$ . And we repeat the same procedure on $[x_1, x_2)$ , and so on and so forth up to $[x_K, 1]$ . This way, I get a unique increasing mapping $y(x)$ exploiting piecewise monotonicity of my function. Now, I’m not $100\%$ sure of this proof. And more importantly, I’m not sure how to write it down if I have an infinite number of points with null derivative. I guess the idea would be exactly the same (all that matters is the ordering). It’s just that I’m not sure everything I wrote above would follow through and still be correct. Maybe I did not think about some particular case that would forbid my piecewise inversions. For example, $f_2(y) = y^3 sin(1/y)$ cannot be considered piecewise monotone near $0$ I suppose... But I’m not sure. Remark: I thought ODE would have results on this (i.e. proving existence of a unique increasing solution), but I cannot find anything, so in the end using the ODE form of my equation seems quite useless. I just exploit the initial implicit form.","I want to find the increasing function which is defined implicitely as the solution to the following equation: where are known and on and are such that an increasing solution exists (I have existence already proved elsewhere). I want to show that there exists a unique increasing solution starting from . This solution should be unique as long as the set such that is of null measure .  The intuition is quite simple: both function goes through exactly the same points, in the same order (just not at the same speed) (you can see this graphically). So as long as there is no flat part, there should be a unique mapping from to . Remark: on the optimal path, given the equation, this equation will also imply that the set such that is also of null measure, and the null derivative coincides. Remark 2: Obviously, if is strictly monotone, we can just invert it to find the solution: To solve my problem, since I’m solving for a function, I thought I could resort to properties of differential equations (Picard-Lindelof for e.g.); Differentiating my equation yields: . when By Picard-Lindelof, if is on , then I have a unique solution. But this would correspond to the monotone case in my remark 2 above. The difficulty I have is with the points with null derivatives. These null derivatives yields problem because there is a indeterminacy in on the optimal path ( over , since ), and at other points the function is not even defined. So I cannot resort to Picard Lindelof. However, since I’m looking for an increasing solution , the solution should still be unique . But I cannot find any proof of that, or any variation of Picard-Lindelof doing this kind of stuff (i.e adding increasing constraint into the system). Hence my first question: Question: does a variation of Picard-Lindelof uniqueness result exists for my kind of problem? (i.e. problem with increasing constraint on the solution) would be great if I could just find a reference for this as I was not able to find any . Otherwise, I am trying to build the solution myself, resorting to some kind of pasting. But I’m not sure it’s correct to do it this way. The idea is that, I’m using Picard-Lindelof starting from up to the first point where that I denote . Then I’m doing it again from to (the second point where ). And so on and so forth. There are two ‘problems’ with this solution: How do I ‘start’ from the points as generally speaking, is not defined (or actually indeterminate over ), so I’m not sure I can write it down. I should have trouble with this notion of ‘points’ with null derivatives. What if they are an infinite number of such points? I think I can prove that it is not possible on the compact set [0,1] if are and if the set such that is of null measure. But I’m not sure and there might exist some case I did not think about (even if is not in so it does not enter my setup). And more importantly, if I can resort to this concept of points, I can just use the original system directly: because it is piecewise monotone between those points, I can invert it between those points! All of this to say I’m really not sure about how to build a proof so far. I have a simple showcase example to illustrate the problem: Imagine , . This example is built such that the unique increasing solution to this problem with initial value is . The question is: how can I recover this using my differential equation ? By my method, I’ll just realize that and (and that there is only one point such that the derivatives are ). And I’ll just have to split the Picard-Lindelof in two parts: starting from to ; and then from to . But even there, you see that the system of differential equations is ‘indeterminate’ at , so how can I ‘start’ again from there? That’s why I find my ‘proof’ to be quite clumsy so far, and why I would like something more correct, if possible an already built result that I could cite or from which I could start my proof. EDIT: since my ODE is separable I should solve it by getting to the initial implicit form, so I’m not sure getting to the differential equation is even necessary; I thought it could be easier to get some standard results about uniqueness though. But maybe I could get them with the initial implicit definition of by adding a monotonicity constraint: i.e. there exists only one strictly increasing part in the integral curve. But I don’t know how to show this. Sketch of an answer: If the set such that has a finite number of points (instead of imposing measure zero), then I will do the proof as follows: Denote the ordered set of points such that . Since we know the functions f_1, f_2, are such that an increasing solution exists, must also have the same finite number of points with . Using the order of these points and the monotonicity constraint on the solution, we know that . Now, I can just use the fact that is piecewise monotone between each . By monotonicity, since , then all are images of . This way starting from (x, y) = (0,0), we have: . Moreover, on , is monotone (and ). So it is inversible, and we get: . Then we have . And we repeat the same procedure on , and so on and so forth up to . This way, I get a unique increasing mapping exploiting piecewise monotonicity of my function. Now, I’m not sure of this proof. And more importantly, I’m not sure how to write it down if I have an infinite number of points with null derivative. I guess the idea would be exactly the same (all that matters is the ordering). It’s just that I’m not sure everything I wrote above would follow through and still be correct. Maybe I did not think about some particular case that would forbid my piecewise inversions. For example, cannot be considered piecewise monotone near I suppose... But I’m not sure. Remark: I thought ODE would have results on this (i.e. proving existence of a unique increasing solution), but I cannot find anything, so in the end using the ODE form of my equation seems quite useless. I just exploit the initial implicit form.","y(x): [0,1] \rightarrow [0,1] f_1(x) = f_2(y(x)) \quad \forall x \in [0,1] f_1(), f_2() C_1 [0,1] f_1 f_2 y(x) y(0) = 0 f_1’(x) = 0 x y f_2’(y(x)) = 0 f_2() y(x) = f_2^{-1}(f_1(x)) \quad \forall x \in [0,1] f_2’(y(x)) y’(x) = f_1’(x) \iff y’(x) = f_1’(x) / f_2’(y(x)) := g(x, y(x)) f_2’(y) \neq 0 g() C_1 [0,1] g() 0 0 y’ > 0 y(0)=0 f_2’(y) = 0 y_1 y_1 y_2 f_2’(y) = 0 y_k g(x_k, y_k(x_k)) 0 0 f_1, f_2 C_1 f’_1 = 0 x sin(x) x sin(x) C_1 0 f_1(x) = 1 - 4 (x - 0.5)^2 f_2(y) = 4(\sqrt{y} - y) y(0) = 0 y(x) = x^2 f’_2(y(x)) y’(x) = f’_1(x) y_1 = 0.25 x_1 = 0.5 0 (x,y)=(0,0) (0.5, 0.25) (0.5, 0.25) (1, 1) (0.5, 0.25) y(x) f_1’(x) = 0 x_k K f_1’(x_k) = 0 f_2’(y) K f_2’(y_k) = 0 y(x_k) = y_k \forall k f_2() y_k y_k = y(x_k) y \in [y_k, y_{k+1}] x \in [x_k, x_{k+1}] \forall x \in [0, x_1), \quad f_2(y) = f_1(x) [0, y(x_1)) f_2 C_1 \forall x \in [0, x_1), \quad y = f_2^{-1}(f_1(x)) y(x_1) = y_1 [x_1, x_2) [x_K, 1] y(x) 100\% f_2(y) = y^3 sin(1/y) 0","['real-analysis', 'ordinary-differential-equations', 'monotone-functions', 'continuity', 'piecewise-continuity']"
90,Find $f(r) $ if $\nabla ^2 f(r) =0$.,Find  if .,f(r)  \nabla ^2 f(r) =0,The answer is $$f(r) = b + \frac{a}{r} $$ where $a$ and $b$ are constants. Unfortunately I don't know how to find f(r). I was hinted that $$ \nabla ^2 f(r)=\frac{d^2f}{dr^2} + \frac{2}{r} \frac{df}{dr}$$ Where $f(r)$ is harmonic. I hope someone can give me a detailed answer. Thank you.,The answer is where and are constants. Unfortunately I don't know how to find f(r). I was hinted that Where is harmonic. I hope someone can give me a detailed answer. Thank you.,f(r) = b + \frac{a}{r}  a b  \nabla ^2 f(r)=\frac{d^2f}{dr^2} + \frac{2}{r} \frac{df}{dr} f(r),"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'vector-fields', 'laplacian']"
91,The integral of the Dickman function,The integral of the Dickman function,,"It is well known that the integration of the Dickman function with the weight $\frac{1}{(t+2)}$ or $\frac{1}{(1+t)^2}$ gives Golomb – Dickman constant : $$\lambda=  \int_0^\infty \frac{\rho(t)}{t+2}\,dt$$ or $$\lambda=  \int_0^\infty \frac{\rho(t)}{(1+t)^2}\,dt,$$ Where $\lambda=0.624...$ - Golomb–Dickman constant, $\rho(t)$ - Dickman function. But what about the integral of the Dickman function itself $\int_0^\infty \rho(t)dt= $ ? I tried to integrate numerically with the help of WolframAlpha using its function DickmanRho(t). At the limit of accuracy, I got the value $1.7811$ , which, within the error, coincides with $ e^{\gamma} $ , where $\gamma=0.5772...$ - Euler–Mascheroni constant .This result is not unexpected considering Mertens' third theorem , but how this result сan be proved based on the definition of the Dickman function i.e. through delay differential equation with initial conditions(Definition see here )?","It is well known that the integration of the Dickman function with the weight or gives Golomb – Dickman constant : or Where - Golomb–Dickman constant, - Dickman function. But what about the integral of the Dickman function itself ? I tried to integrate numerically with the help of WolframAlpha using its function DickmanRho(t). At the limit of accuracy, I got the value , which, within the error, coincides with , where - Euler–Mascheroni constant .This result is not unexpected considering Mertens' third theorem , but how this result сan be proved based on the definition of the Dickman function i.e. through delay differential equation with initial conditions(Definition see here )?","\frac{1}{(t+2)} \frac{1}{(1+t)^2} \lambda=  \int_0^\infty \frac{\rho(t)}{t+2}\,dt \lambda=  \int_0^\infty \frac{\rho(t)}{(1+t)^2}\,dt, \lambda=0.624... \rho(t) \int_0^\infty \rho(t)dt=  1.7811  e^{\gamma}  \gamma=0.5772...","['integration', 'ordinary-differential-equations', 'number-theory']"
92,Third order linear differential equation and Painlevé II solution,Third order linear differential equation and Painlevé II solution,,"Consider the solution to the Painlevé II equation on $\mathbb{R}$ $$q''=2q^3+rq$$ with the boundary condition $q(r)\sim_{r\to +\infty} \mathrm{Ai}(r)$ and consider the function $f$ such that for $r\in \mathbb{R}$ , $$f(r)=\cosh\left(\frac{1}{2}\int_r^{+\infty} \mathrm{d} s \, q(s)\right)$$ We have that $f$ verifies the third order differential equation $$f'''+P(r) f' +Q(r) f =0 \qquad \qquad (\mathcal{E})$$ with $$\begin{cases} P(r)=-\frac{9}{4}q(r)^2-r\\ Q(r)=\frac{1}{6}P'(r)+\frac{1}{6} \end{cases}$$ My question is the following, if one is given the differential equation $(\mathcal{E})$ with the proper boundary condition, is there a standard technique / change of variable to find back the expression of the function $f$ ?","Consider the solution to the Painlevé II equation on with the boundary condition and consider the function such that for , We have that verifies the third order differential equation with My question is the following, if one is given the differential equation with the proper boundary condition, is there a standard technique / change of variable to find back the expression of the function ?","\mathbb{R} q''=2q^3+rq q(r)\sim_{r\to +\infty} \mathrm{Ai}(r) f r\in \mathbb{R} f(r)=\cosh\left(\frac{1}{2}\int_r^{+\infty} \mathrm{d} s \, q(s)\right) f f'''+P(r) f' +Q(r) f =0 \qquad \qquad (\mathcal{E}) \begin{cases}
P(r)=-\frac{9}{4}q(r)^2-r\\
Q(r)=\frac{1}{6}P'(r)+\frac{1}{6}
\end{cases} (\mathcal{E}) f","['ordinary-differential-equations', 'nonlinear-analysis']"
93,Solving coupled differential equation via perturbation method,Solving coupled differential equation via perturbation method,,"In the paper by Caroli, de-Gennes and Matricon on Bound fermion state in superconductor, they have reached a system of first order differential equation which reads roughly as: \begin{align} -i\hbar v_F\frac{df}{dr}+\Delta(r)g(r)&=(\epsilon+\frac{\mu\hbar^2}{2mr^2})f(r)  \\ i\hbar v_F\frac{dg}{dr}+\Delta(r)f(r)&=(\epsilon+\frac{\mu\hbar^2}{2mr^2})g(r)  \\[.5em] v_F&=\frac{\hbar k_F}{m} \end{align} $f(r),g(r), \Delta(r)$ are functions of $r$ . $\mu$ is an integer and $\epsilon$ is a small number and can be treated as a small perturbation. What I fail to understand is how they are able to solve the above system by treating the right hand side as a small perturbation, to first order, and obtain the following results: \begin{align} f(r)&=e^{\frac{1}{2}i\psi(r)-K(r)}\\ g(r)&=-ie^{-\frac{1}{2}i\psi(r)-K(r)} \end{align} where $$K(r)=\frac{\int_{0}^{r}\Delta(r')dr'}{\hbar v_F}$$ and $$\psi(r)=-e^{2K(r)}\int_{r}^{\infty}e^{-K(r')}(\frac{2\epsilon}{\hbar v_F}+\frac{\mu}{k_Fr'^2})dr'$$ If someone can explain briefly the procedure to get to the final results, it would be very helpful. Thanks in advance for anyone willing to answer.","In the paper by Caroli, de-Gennes and Matricon on Bound fermion state in superconductor, they have reached a system of first order differential equation which reads roughly as: are functions of . is an integer and is a small number and can be treated as a small perturbation. What I fail to understand is how they are able to solve the above system by treating the right hand side as a small perturbation, to first order, and obtain the following results: where and If someone can explain briefly the procedure to get to the final results, it would be very helpful. Thanks in advance for anyone willing to answer.","\begin{align}
-i\hbar v_F\frac{df}{dr}+\Delta(r)g(r)&=(\epsilon+\frac{\mu\hbar^2}{2mr^2})f(r) 
\\
i\hbar v_F\frac{dg}{dr}+\Delta(r)f(r)&=(\epsilon+\frac{\mu\hbar^2}{2mr^2})g(r) 
\\[.5em]
v_F&=\frac{\hbar k_F}{m}
\end{align} f(r),g(r), \Delta(r) r \mu \epsilon \begin{align}
f(r)&=e^{\frac{1}{2}i\psi(r)-K(r)}\\
g(r)&=-ie^{-\frac{1}{2}i\psi(r)-K(r)}
\end{align} K(r)=\frac{\int_{0}^{r}\Delta(r')dr'}{\hbar v_F} \psi(r)=-e^{2K(r)}\int_{r}^{\infty}e^{-K(r')}(\frac{2\epsilon}{\hbar v_F}+\frac{\mu}{k_Fr'^2})dr'","['ordinary-differential-equations', 'perturbation-theory']"
94,Modeling a point following another point,Modeling a point following another point,,"Suppose you have a point starting at the origin $(0,0),$ i.e., $$(x(0)=0,y(0)=0).$$ We'll denote its coordinates with $(x(t),y(t)),$ and it is tracking another point with constant velocity $v$ . This point lies a few units to the right, i.e., $(x_0,0).$ The second point is going directly upwards with parametric coordinates $$(x_0,vt).$$ In figure 1, this is what the curve will look like for an instantaneous amount of time. figure 1 Using this, we can derive that \begin{align*} \sqrt{dx^2+dy^2}&=v\,dt \\  \dfrac{v\,dt}{x_0}&=\dfrac{dy}{dx}, \end{align*} which implies \begin{align*} \sqrt{(x'(t))^2+(y'(t))^2}&=v \\  \frac{v\,dt}{x_0}&=\frac{dy}{dx}. \end{align*} However I got stuck here and I wasn't able to set up a nice differential equation. I took a different approach and tried to generalize recursively what the curve would look like as you can see in figure 2 figure 2 Let $$x_n=x(n\,dt), y_n=y(n\,dt)$$ and using properties of similar triangles and shared angles, we'll find $$x_n=v\,dt \cdot \cos\left(\arctan\left(\frac{nv\,dt−y_{n−1}}{x_0−x_{n−1}}\right)\right)+x_{n−1} \\ y_n=v\,dt \cdot \sin\left(\arctan\left(\frac{nv\,dt−y_{n−1}}{x_0−x_{n−1}}\right)\right)+y_{n−1}$$ which implies $$x_n=\frac{v\,dt}{\sqrt{1+\left(\frac{nv\,dt-y_{n-1}}{x_0-x_{n-1}}\right)^{\!2}}}+x_{n-1} \\ y_n=\frac{(v\,dt)\frac{nv\,dt−y_{n−1}}{x_0−x_{n−1}}}{\sqrt{1+\left(\frac{nv\,dt-y_{n-1}}{x_0-x_{n-1}}\right)^{\!2}}}+y_{n-1}$$ However my progress stops here. Maybe I can set up a differential equation by reversing Euler's method through the recursive method? I am unsure of the most simple way to do this. Any ideas on finding $(x(t),y(t))?$","Suppose you have a point starting at the origin i.e., We'll denote its coordinates with and it is tracking another point with constant velocity . This point lies a few units to the right, i.e., The second point is going directly upwards with parametric coordinates In figure 1, this is what the curve will look like for an instantaneous amount of time. figure 1 Using this, we can derive that which implies However I got stuck here and I wasn't able to set up a nice differential equation. I took a different approach and tried to generalize recursively what the curve would look like as you can see in figure 2 figure 2 Let and using properties of similar triangles and shared angles, we'll find which implies However my progress stops here. Maybe I can set up a differential equation by reversing Euler's method through the recursive method? I am unsure of the most simple way to do this. Any ideas on finding","(0,0), (x(0)=0,y(0)=0). (x(t),y(t)), v (x_0,0). (x_0,vt). \begin{align*} \sqrt{dx^2+dy^2}&=v\,dt \\ 
\dfrac{v\,dt}{x_0}&=\dfrac{dy}{dx},
\end{align*} \begin{align*}
\sqrt{(x'(t))^2+(y'(t))^2}&=v \\ 
\frac{v\,dt}{x_0}&=\frac{dy}{dx}.
\end{align*} x_n=x(n\,dt), y_n=y(n\,dt) x_n=v\,dt \cdot \cos\left(\arctan\left(\frac{nv\,dt−y_{n−1}}{x_0−x_{n−1}}\right)\right)+x_{n−1} \\ y_n=v\,dt \cdot \sin\left(\arctan\left(\frac{nv\,dt−y_{n−1}}{x_0−x_{n−1}}\right)\right)+y_{n−1} x_n=\frac{v\,dt}{\sqrt{1+\left(\frac{nv\,dt-y_{n-1}}{x_0-x_{n-1}}\right)^{\!2}}}+x_{n-1} \\ y_n=\frac{(v\,dt)\frac{nv\,dt−y_{n−1}}{x_0−x_{n−1}}}{\sqrt{1+\left(\frac{nv\,dt-y_{n-1}}{x_0-x_{n-1}}\right)^{\!2}}}+y_{n-1} (x(t),y(t))?","['calculus', 'ordinary-differential-equations']"
95,How to solve $xy'+x=y\ln(xy)$?,How to solve ?,xy'+x=y\ln(xy),"i.e. $x\frac{\mathrm{d}y}{\mathrm{d}x}+x=y\ln(xy)$ . The MATHEMATICA can't solve this problem. DSolve[x*y'[x]+x==y[x]*Log[y[x]*x],y[x],x] leads Solve::ifun: Inverse functions are being used by Solve, so some solutions may not be found; use Reduce for complete solution information.","i.e. . The MATHEMATICA can't solve this problem. DSolve[x*y'[x]+x==y[x]*Log[y[x]*x],y[x],x] leads Solve::ifun: Inverse functions are being used by Solve, so some solutions may not be found; use Reduce for complete solution information.",x\frac{\mathrm{d}y}{\mathrm{d}x}+x=y\ln(xy),['ordinary-differential-equations']
96,Question about a matrix in a problem of differential equations,Question about a matrix in a problem of differential equations,,"I am studyng differential equations from a book called linear ordinary differential equation by Earl A. Coddington this is the problem Consider the nonhomogeneous system \begin{equation}\label{equation:1} X'=A(t)X+B(t),  \  A\in C(I,M_n(\mathcal F)), \ B \in C(I, \mathcal F^n  )\  \end{equation} on some interval $I$ , together with the corresponding homogeneous system \begin{equation} X'=A(t)X \end{equation} For $t,\tau \in I$ . let $S(t, r) \in M_n(\mathcal F)$ be the unique matrix satisfying \begin{equation} \frac{\partial S}{\partial t}(t,\tau)=A(t)S(t,\tau), \ S(\tau,\tau)=I_n.  \end{equation} show that if $X$ is any basis for $X'=A(t)X$ then \begin{equation} S(t,\tau)=X(t)X^{-1}(\tau), \ t,\tau \in I  \end{equation} $S(t,\tau)S(\tau,\sigma)=S(t,\sigma), \ t,\tau,\sigma \in I$ I'm confused that the matrix has two evaluations $S(t,\tau)$ I don't see what it means and the book dont have examples about this.","I am studyng differential equations from a book called linear ordinary differential equation by Earl A. Coddington this is the problem Consider the nonhomogeneous system on some interval , together with the corresponding homogeneous system For . let be the unique matrix satisfying show that if is any basis for then I'm confused that the matrix has two evaluations I don't see what it means and the book dont have examples about this.","\begin{equation}\label{equation:1}
X'=A(t)X+B(t),  \  A\in C(I,M_n(\mathcal F)), \ B \in C(I, \mathcal F^n  )\ 
\end{equation} I \begin{equation}
X'=A(t)X
\end{equation} t,\tau \in I S(t, r) \in M_n(\mathcal F) \begin{equation}
\frac{\partial S}{\partial t}(t,\tau)=A(t)S(t,\tau), \ S(\tau,\tau)=I_n. 
\end{equation} X X'=A(t)X \begin{equation}
S(t,\tau)=X(t)X^{-1}(\tau), \ t,\tau \in I 
\end{equation} S(t,\tau)S(\tau,\sigma)=S(t,\sigma), \ t,\tau,\sigma \in I S(t,\tau)","['linear-algebra', 'ordinary-differential-equations']"
97,"If u,g and f satisfy this condition, $p_1$ that fit the equation will also be the exact solution of the DE f at point $(v_0+h)$ if $g_pf+g_v=0$","If u,g and f satisfy this condition,  that fit the equation will also be the exact solution of the DE f at point  if",p_1 (v_0+h) g_pf+g_v=0,"Tried to propose this statement before but it looks like i have problem in writing the correct mathematical symbol especially dealing v and p making it very confusing. Please help to edit the symbol if the idea sound comprehensible. Given $u(v,p)$ , $g(v,p)$ and $f(v,p)$ satisfy the relationship such that $f=\frac{dp}{dv}=\frac{g-u_v}{u_p}$ with initial condition $p(v_0)=p_0$ . While $p_1$ is a solution that satisfy below equation $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ The solution of $p_1$ that satisfy the above equation will also be the exact solution of the DE $\frac{dp}{dv}=f(v,p)$ at point $p(v_0+h)$ if $g(v,p)$ satisfy the condition $g_pf+g_v=0$ Example $u(v,p)=pv^{2.4}$ and $g(v,p)=pv^{1.4}$ then $\frac{dp}{dv}=\frac{g-u_v}{u_p}=\frac{pv^{1.4}-2.4pv^{1.4}}{v^{1.4}}=-1.4\frac{p}{v}$ Solve this DE and will obtain $pv^{1.4}$ =constant, consider $p(v_0)=p_0$ hence the value of p at point $(v_0+h)$ will be $p(v_0+h)=\frac{p_0v_0^{1.4}}{(v_0+h)^{1.4}}$ while the value obtain directly from this formulae $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ $p_0v_0^{2.4}+hp_0v_0^{1.4}=p_1(v_0+h)^{2.4}$ $p_0v_0^{1.4}(v_0+h)=p_1(v_0+h)^{2.4}$ $p_1=\frac{p_0v_0^{1.4}}{(v_0+h)^{1.4}}$ Can try with u(v,p) and g(v,p) with other function such that it yield the same DE $\frac{dp}{dv}=-1.4\frac{p}{v}$ but it wont give the same result by direct application of this formulae $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ ie $u(v,p)=pv^a$ and $g(v,p)=(-1.4+a)pv^{a-1}$ It produce the same $f(v,p)$ with all value of a but only $a=2.4$ give the above result Is the proof below able to verify both sequence yield the same value of $p_n$ when n tend to infinity? This equation is actually a one step numerical integration which is approximate using an Euler method but a=2.4 give the most accurate result since it is the exact solution. Does this apply the same for all u,g and f if it satisfy above conditions? Example 2 u(v,p)=pv and g(v,p)=p then $\frac{dp}{dv}=\frac{g-u_v}{u_p}=\frac{p-p}{v}=0$ assume $p(v_0)=p_0$ while from this formula $u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h$ $p_0v_0+p_0(h)=p_1(v_0+h)$ $p_1=p_0$","Tried to propose this statement before but it looks like i have problem in writing the correct mathematical symbol especially dealing v and p making it very confusing. Please help to edit the symbol if the idea sound comprehensible. Given , and satisfy the relationship such that with initial condition . While is a solution that satisfy below equation The solution of that satisfy the above equation will also be the exact solution of the DE at point if satisfy the condition Example and then Solve this DE and will obtain =constant, consider hence the value of p at point will be while the value obtain directly from this formulae Can try with u(v,p) and g(v,p) with other function such that it yield the same DE but it wont give the same result by direct application of this formulae ie and It produce the same with all value of a but only give the above result Is the proof below able to verify both sequence yield the same value of $p_n$ when n tend to infinity? This equation is actually a one step numerical integration which is approximate using an Euler method but a=2.4 give the most accurate result since it is the exact solution. Does this apply the same for all u,g and f if it satisfy above conditions? Example 2 u(v,p)=pv and g(v,p)=p then assume while from this formula","u(v,p) g(v,p) f(v,p) f=\frac{dp}{dv}=\frac{g-u_v}{u_p} p(v_0)=p_0 p_1 u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h p_1 \frac{dp}{dv}=f(v,p) p(v_0+h) g(v,p) g_pf+g_v=0 u(v,p)=pv^{2.4} g(v,p)=pv^{1.4} \frac{dp}{dv}=\frac{g-u_v}{u_p}=\frac{pv^{1.4}-2.4pv^{1.4}}{v^{1.4}}=-1.4\frac{p}{v} pv^{1.4} p(v_0)=p_0 (v_0+h) p(v_0+h)=\frac{p_0v_0^{1.4}}{(v_0+h)^{1.4}} u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h p_0v_0^{2.4}+hp_0v_0^{1.4}=p_1(v_0+h)^{2.4} p_0v_0^{1.4}(v_0+h)=p_1(v_0+h)^{2.4} p_1=\frac{p_0v_0^{1.4}}{(v_0+h)^{1.4}} \frac{dp}{dv}=-1.4\frac{p}{v} u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h u(v,p)=pv^a g(v,p)=(-1.4+a)pv^{a-1} f(v,p) a=2.4 \frac{dp}{dv}=\frac{g-u_v}{u_p}=\frac{p-p}{v}=0 p(v_0)=p_0 u(p_1,v_0+h)-u(p_0,v_0)=g(p_0,v_0)h p_0v_0+p_0(h)=p_1(v_0+h) p_1=p_0","['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'taylor-expansion']"
98,"Given a solution to a system of ODE, prove of give a counterexample that A has eigenvalues purely complex or 0.","Given a solution to a system of ODE, prove of give a counterexample that A has eigenvalues purely complex or 0.",,Prove or give a counterexample to: Let $X(t) $ be the solution of the problem of initial value $\dot  X=AX$ with $X(t_0)=X_o $ with $X_0\ne 0$ . If there exists $t_1 \ne t_0$ s.t. $X(t_1)=X_0$ then A has an eigenvalue $0$ or purely imaginary. Well I tried proving it and I'm quite lost. Can I get a hint?,Prove or give a counterexample to: Let be the solution of the problem of initial value with with . If there exists s.t. then A has an eigenvalue or purely imaginary. Well I tried proving it and I'm quite lost. Can I get a hint?,"X(t)  \dot
 X=AX X(t_0)=X_o  X_0\ne 0 t_1 \ne
t_0 X(t_1)=X_0 0",['ordinary-differential-equations']
99,"How to analyse $y''=\frac{y^{2}}{x^{2}}+1,x\in(0,+\infty)$",How to analyse,"y''=\frac{y^{2}}{x^{2}}+1,x\in(0,+\infty)","The equation is $f''(x)=\frac{f^{2}(x)}{x^{2}}+1,x\in(0,+\infty)$ . Asume $f(x)\in C^{2}(0,+\infty)$ .I want to prove that if $\lim_{x\to 0+}xf'(x)=0$ then $\lim_{x\to 0+}f(x)=0$ .I think I need to construct some function but I cannot find any of them work.Any help will be thanked.",The equation is . Asume .I want to prove that if then .I think I need to construct some function but I cannot find any of them work.Any help will be thanked.,"f''(x)=\frac{f^{2}(x)}{x^{2}}+1,x\in(0,+\infty) f(x)\in C^{2}(0,+\infty) \lim_{x\to 0+}xf'(x)=0 \lim_{x\to 0+}f(x)=0","['calculus', 'ordinary-differential-equations', 'analysis']"
