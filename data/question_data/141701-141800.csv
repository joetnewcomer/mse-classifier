,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A tough limit problem involving $1/(\sin x - \sin a)$ and its generalization,A tough limit problem involving  and its generalization,1/(\sin x - \sin a),Long back I had encountered the following problem in Hardy's Pure Mathematics (originally from the infamous Mathematical Tripos 1896 ): If $$f(x) = \frac{1}{\sin x - \sin a} - \frac{1}{(x - a)\cos a}$$ then show that $$\frac{d}{da}\{\lim_{x \to a}f(x)\} - \lim_{x \to a}f'(x) = \frac{3}{4}\sec^{3}a - \frac{5}{12}\sec a$$ I had solved it using Taylor series expansions and even then it involved good amount of calculation. I suppose applying L'Hospital would be even more arduous. On the other hand both the Taylor series and L'Hospital Rule are discussed later in Hardy's book suggesting that it could be solved via elementary techniques (i.e. using algebra of limits and Squeeze theorem and if needed one can use mean value theorem). Please let me know if such a solution is possible. Also I believe there might be a suitable generalization applicable to functions of type $$g(x) = \frac{1}{\phi(x) - \phi(a)} - \frac{1}{(x - a)\phi'(a)}$$ and perhaps the expression $$\frac{d}{da}\{\lim_{x \to a}g(x)\} - \lim_{x \to a}g'(x)$$ has some significance. Any ideas in this direction would be helpful.,Long back I had encountered the following problem in Hardy's Pure Mathematics (originally from the infamous Mathematical Tripos 1896 ): If $$f(x) = \frac{1}{\sin x - \sin a} - \frac{1}{(x - a)\cos a}$$ then show that $$\frac{d}{da}\{\lim_{x \to a}f(x)\} - \lim_{x \to a}f'(x) = \frac{3}{4}\sec^{3}a - \frac{5}{12}\sec a$$ I had solved it using Taylor series expansions and even then it involved good amount of calculation. I suppose applying L'Hospital would be even more arduous. On the other hand both the Taylor series and L'Hospital Rule are discussed later in Hardy's book suggesting that it could be solved via elementary techniques (i.e. using algebra of limits and Squeeze theorem and if needed one can use mean value theorem). Please let me know if such a solution is possible. Also I believe there might be a suitable generalization applicable to functions of type $$g(x) = \frac{1}{\phi(x) - \phi(a)} - \frac{1}{(x - a)\phi'(a)}$$ and perhaps the expression $$\frac{d}{da}\{\lim_{x \to a}g(x)\} - \lim_{x \to a}g'(x)$$ has some significance. Any ideas in this direction would be helpful.,,"['calculus', 'limits']"
1,Finding this weird limit involving periodic functions with periods 5 and 10.,Finding this weird limit involving periodic functions with periods 5 and 10.,,"If $f(x)$ and $g(x)$ are two periodic functions with periods 5 and 10 respectively, such that: $$\lim_{x\to0}\frac{f(x)}x=\lim_{x\to0}\frac{g(x)}x=k;\quad k>0$$ then for $n\in\mathbb N$, the value of : $$\lim_{n\to\infty}\frac{f(5(4+\sqrt{15})^n)}{g(10(4+\sqrt{14})^n)}$$. Note: Everything should be done by hand, any claculating/plotting, etc. device/software is not allowed. What I've thought is make up such a function: $$f(x)\propto\sin\frac{2\pi x}{5}\qquad g(x)\propto\sin\frac{2\pi x}{10}$$ But the condition for the limit at zero cna not be satisfied in anyways from here. Also: $$5(4+\sqrt{15})^n=5\times4^n(1+\sqrt{15}/4)^n\sim 5\times4^n+5\times4^n\times n\sqrt{15}/4+...$$ wheer $5\times4^n\in\mathbb Z$ So, by periodicity: $$f(5\alpha+\beta)=f(\beta)\qquad \alpha\in\mathbb Z,\beta\in\mathbb R_{|\beta|<5}$$ And then: $$L=\lim_{n\to\infty}\frac{f(5\times4^nn\sqrt{15}/4+\cdots)}{g(10\times4^nn\sqrt{14}/4+\cdots)}$$ But this proceeds nowhere, as I was wshing to apply L'Hospital or retain only the $(\beta, |\beta|<5)$ part, but :D.","If $f(x)$ and $g(x)$ are two periodic functions with periods 5 and 10 respectively, such that: $$\lim_{x\to0}\frac{f(x)}x=\lim_{x\to0}\frac{g(x)}x=k;\quad k>0$$ then for $n\in\mathbb N$, the value of : $$\lim_{n\to\infty}\frac{f(5(4+\sqrt{15})^n)}{g(10(4+\sqrt{14})^n)}$$. Note: Everything should be done by hand, any claculating/plotting, etc. device/software is not allowed. What I've thought is make up such a function: $$f(x)\propto\sin\frac{2\pi x}{5}\qquad g(x)\propto\sin\frac{2\pi x}{10}$$ But the condition for the limit at zero cna not be satisfied in anyways from here. Also: $$5(4+\sqrt{15})^n=5\times4^n(1+\sqrt{15}/4)^n\sim 5\times4^n+5\times4^n\times n\sqrt{15}/4+...$$ wheer $5\times4^n\in\mathbb Z$ So, by periodicity: $$f(5\alpha+\beta)=f(\beta)\qquad \alpha\in\mathbb Z,\beta\in\mathbb R_{|\beta|<5}$$ And then: $$L=\lim_{n\to\infty}\frac{f(5\times4^nn\sqrt{15}/4+\cdots)}{g(10\times4^nn\sqrt{14}/4+\cdots)}$$ But this proceeds nowhere, as I was wshing to apply L'Hospital or retain only the $(\beta, |\beta|<5)$ part, but :D.",,"['calculus', 'limits', 'functions', 'periodic-functions']"
2,"$(\delta,\varepsilon)$ Proof of Limit",Proof of Limit,"(\delta,\varepsilon)","I wish to prove that $\lim_{x\to 2} {x+1 \over x+2} = {3 \over 4} $. The $(\delta,\varepsilon)$ limit definition in this case is: $\forall \epsilon >0, \exists \delta >0$ such that $0<|x-2|<\delta \Rightarrow |{x+1 \over x+2} - {3 \over 4}| < \epsilon.$ Thus, I need to provide a $\delta$, which is a function of $\epsilon$ in order to satisfy the above definition. I am having a bit of difficulty finding an inequality to continue my work below. $|{x+1 \over x+2} - {3 \over 4}| = {1 \over 4}|{x-2 \over x+2}|$","I wish to prove that $\lim_{x\to 2} {x+1 \over x+2} = {3 \over 4} $. The $(\delta,\varepsilon)$ limit definition in this case is: $\forall \epsilon >0, \exists \delta >0$ such that $0<|x-2|<\delta \Rightarrow |{x+1 \over x+2} - {3 \over 4}| < \epsilon.$ Thus, I need to provide a $\delta$, which is a function of $\epsilon$ in order to satisfy the above definition. I am having a bit of difficulty finding an inequality to continue my work below. $|{x+1 \over x+2} - {3 \over 4}| = {1 \over 4}|{x-2 \over x+2}|$",,['calculus']
3,Proof that the $\lim\limits_{x \to 2}\dfrac{1}{x} = \dfrac{1}{2}$ using the $\epsilon$-$\delta$ definition of limits (verification).,Proof that the  using the - definition of limits (verification).,\lim\limits_{x \to 2}\dfrac{1}{x} = \dfrac{1}{2} \epsilon \delta,"Prove that the $\lim\limits_{x \to 2}\dfrac{1}{x} = \dfrac{1}{2}$ using the $\epsilon-\delta$ definition of limits. $$ \\ \begin{align} \\ &\textrm{Let } \forall \epsilon > 0 \\ &\textrm{Choose } \delta = \min{\{1, 2\epsilon \}} \\ &\textrm{Assume } 0 < |x - 2| < \delta : \\ \end{align} $$ $$ \\ \begin{align} \\ \left|\frac{1}{x} - \frac{1}{2}\right| &< \epsilon \\ \frac{|2 - x|}{|2x|} &< \epsilon \\ |-1(x - 2)| &< \epsilon|2x| \\ |x - 2| &< \epsilon|2x| \\ \end{align} $$ $$ \\ \begin{align} \\ |x - 2| &< 1 \\ -1 < x - 2 &< 1 \\ 1 < x &< 3 \\ \end{align} $$ $$ \\ \begin{align} \\ |x - 2| &< \epsilon|2(1)| \\ |x - 2| &< 2\epsilon \\ \end{align} $$ $$ \\\therefore \delta \leq 2\epsilon $$ Right, so I start by taking $|f(x)−L|<ϵ$. I then isolate $|x−2|$ to the left. I then limit $|x−2|$ to be less than one and then find a range of $x$ values which satisfy the inequality. Then I plug in the smallest $x$ value to minimise the value of $ϵ$. I make the conclusion that $δ≤2ϵ$. Am I excluding or misplacing steps? I'm fairly new to this whole thing.","Prove that the $\lim\limits_{x \to 2}\dfrac{1}{x} = \dfrac{1}{2}$ using the $\epsilon-\delta$ definition of limits. $$ \\ \begin{align} \\ &\textrm{Let } \forall \epsilon > 0 \\ &\textrm{Choose } \delta = \min{\{1, 2\epsilon \}} \\ &\textrm{Assume } 0 < |x - 2| < \delta : \\ \end{align} $$ $$ \\ \begin{align} \\ \left|\frac{1}{x} - \frac{1}{2}\right| &< \epsilon \\ \frac{|2 - x|}{|2x|} &< \epsilon \\ |-1(x - 2)| &< \epsilon|2x| \\ |x - 2| &< \epsilon|2x| \\ \end{align} $$ $$ \\ \begin{align} \\ |x - 2| &< 1 \\ -1 < x - 2 &< 1 \\ 1 < x &< 3 \\ \end{align} $$ $$ \\ \begin{align} \\ |x - 2| &< \epsilon|2(1)| \\ |x - 2| &< 2\epsilon \\ \end{align} $$ $$ \\\therefore \delta \leq 2\epsilon $$ Right, so I start by taking $|f(x)−L|<ϵ$. I then isolate $|x−2|$ to the left. I then limit $|x−2|$ to be less than one and then find a range of $x$ values which satisfy the inequality. Then I plug in the smallest $x$ value to minimise the value of $ϵ$. I make the conclusion that $δ≤2ϵ$. Am I excluding or misplacing steps? I'm fairly new to this whole thing.",,"['real-analysis', 'limits']"
4,Prove that if $\forall_{x\in\Bbb{R}} f(x)=f(x+1)$ and $f$ is continuous then there are infinitely many $c$ such that $f(c+\pi)=f(c)$,Prove that if  and  is continuous then there are infinitely many  such that,\forall_{x\in\Bbb{R}} f(x)=f(x+1) f c f(c+\pi)=f(c),"We have 3 assumptions about $f$: $f: \Bbb{R} \rightarrow \Bbb{R}$ $f$ is continuous $\forall_{x\in\Bbb{R}} f(x)=f(x+1)$ The problem asks us to prove 2 things: That $f$ reaches its supremum and infimum and also there exist infinitely many $c\in\Bbb{R}$ such that $f(\pi+c)=f(c)$. So the first thing can be proven (I think) discovering that $f([0,1]) = f(\Bbb{R})$ (because the function is periodic) so supremum and infinimum is in $f([0,1])$ and by the extreme value theorem it can be reached. As for the second problem I can't see why it should be true. That would mean that there exist $x\in\Bbb{R}$ such that $f(\pi +x+1) = f(x+1) = f(x)$. Wouldn't that mean that function must have 2 periods - $1$ and $\pi$? But this $\pi$ makes no sense then. Am I understanding this correctly?","We have 3 assumptions about $f$: $f: \Bbb{R} \rightarrow \Bbb{R}$ $f$ is continuous $\forall_{x\in\Bbb{R}} f(x)=f(x+1)$ The problem asks us to prove 2 things: That $f$ reaches its supremum and infimum and also there exist infinitely many $c\in\Bbb{R}$ such that $f(\pi+c)=f(c)$. So the first thing can be proven (I think) discovering that $f([0,1]) = f(\Bbb{R})$ (because the function is periodic) so supremum and infinimum is in $f([0,1])$ and by the extreme value theorem it can be reached. As for the second problem I can't see why it should be true. That would mean that there exist $x\in\Bbb{R}$ such that $f(\pi +x+1) = f(x+1) = f(x)$. Wouldn't that mean that function must have 2 periods - $1$ and $\pi$? But this $\pi$ makes no sense then. Am I understanding this correctly?",,"['calculus', 'limits', 'functions', 'continuity']"
5,If $\lim_{n \to \infty} \sum^n_{k =0} \frac{nC_k}{n^k(k+3)} =ae+b$ [duplicate],If  [duplicate],\lim_{n \to \infty} \sum^n_{k =0} \frac{nC_k}{n^k(k+3)} =ae+b,This question already has answers here : Evalute $ \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} $ (5 answers) Closed 4 years ago . Problem : If $$\lim_{n \to \infty} \sum^n_{k =0} \frac{nC_k}{n^k(k+3)} =ae+b$$ then find the value of a-b. Please suggest how to solve such problem in whicl summation and limits both are there. Thanks.,This question already has answers here : Evalute $ \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} $ (5 answers) Closed 4 years ago . Problem : If $$\lim_{n \to \infty} \sum^n_{k =0} \frac{nC_k}{n^k(k+3)} =ae+b$$ then find the value of a-b. Please suggest how to solve such problem in whicl summation and limits both are there. Thanks.,,"['calculus', 'limits']"
6,mathematical limit for a ouroboros torus,mathematical limit for a ouroboros torus,,"The other day i was watching an episode of Tom and Jerry in which a similar situation was present toms head comes out of his own mouth. My head hurts when i think how is that even possible so i googled couldn't find any good answer, all the answers were vague without any proof. So i'm asking here. Consider the torus as shown in the image eating itself. Now... a)Assuming the torus to be finite length (When it is not eating itself). A mathematical proof for how far it can go. Note: Intuitively i think it will devour itself until the hole in the torus reduces to a zero value. b) What if we consider the entire surface of the torus at its hole can pass through without any physical boundaries. Then how far would it devour itself ? will it disappear into a point ? or will it eat itself forever without any end. c) Now what will the characteristics be if both ends of this torus would increase its length infinity. please try not to give any answers based on logical reasoning without a mathematical proof.","The other day i was watching an episode of Tom and Jerry in which a similar situation was present toms head comes out of his own mouth. My head hurts when i think how is that even possible so i googled couldn't find any good answer, all the answers were vague without any proof. So i'm asking here. Consider the torus as shown in the image eating itself. Now... a)Assuming the torus to be finite length (When it is not eating itself). A mathematical proof for how far it can go. Note: Intuitively i think it will devour itself until the hole in the torus reduces to a zero value. b) What if we consider the entire surface of the torus at its hole can pass through without any physical boundaries. Then how far would it devour itself ? will it disappear into a point ? or will it eat itself forever without any end. c) Now what will the characteristics be if both ends of this torus would increase its length infinity. please try not to give any answers based on logical reasoning without a mathematical proof.",,"['limits', 'differential-topology', 'infinity', 'paradoxes']"
7,How do I prove this limit does not exist: $\lim_{x\rightarrow 0} \frac{e^{1/x} - 1}{e^{1/x} + 1} $,How do I prove this limit does not exist:,\lim_{x\rightarrow 0} \frac{e^{1/x} - 1}{e^{1/x} + 1} ,"How do I prove that this limit does not exist? $$\lim_{x\rightarrow 0} \frac{e^{1/x} - 1}{e^{1/x} + 1} $$ My attempt: When you approach from from left towards zero , say i take -0.00000000000001 . i substitute in expression i get (-1) . But if i take 0.000000000001 and substitue i get (=1) (by applying L'Hop) . But if i dont do this and apply L'Hop straighaway i get 1 .","How do I prove that this limit does not exist? $$\lim_{x\rightarrow 0} \frac{e^{1/x} - 1}{e^{1/x} + 1} $$ My attempt: When you approach from from left towards zero , say i take -0.00000000000001 . i substitute in expression i get (-1) . But if i take 0.000000000001 and substitue i get (=1) (by applying L'Hop) . But if i dont do this and apply L'Hop straighaway i get 1 .",,"['calculus', 'limits']"
8,How to find the limit of function $\lim_{x\to 0} \frac {a^b-b^a} {a-b}$,How to find the limit of function,\lim_{x\to 0} \frac {a^b-b^a} {a-b},Let $a(x)$ and $b(x)$ be continuous functions such that $a(0)=b(0)=3$ How to find the limit: $$\lim_{x\to 0} \frac {a^b-b^a} {a-b}$$ I tried to annihilate $b-a$ by writing $a^b-b^a= \sum_{k=0}^{\infty} \frac {{(b\ln a)}^k-({a\ln b)}^k} {k!}$ but it leads to another limit $\lim \frac {b\ln a-a\ln b} {a-b}$ that seems to be similar to the first one...,Let $a(x)$ and $b(x)$ be continuous functions such that $a(0)=b(0)=3$ How to find the limit: $$\lim_{x\to 0} \frac {a^b-b^a} {a-b}$$ I tried to annihilate $b-a$ by writing $a^b-b^a= \sum_{k=0}^{\infty} \frac {{(b\ln a)}^k-({a\ln b)}^k} {k!}$ but it leads to another limit $\lim \frac {b\ln a-a\ln b} {a-b}$ that seems to be similar to the first one...,,"['real-analysis', 'limits']"
9,"About the rigorous $(\epsilon,\delta)$ definition of limit",About the rigorous  definition of limit,"(\epsilon,\delta)","I have questions about the complete and rigorous $(\epsilon,\delta)$ definition of two sided limits. The definition of the two sided limit in http://en.wikipedia.org/wiki/(%CE%B5,_%CE%B4)-definition_of_limit says that for a function $f(x)$ which is defined on an open interval containing $c$, possibly excluding $c$, it is $\lim_{x \to c} f(x)=L$ iff there is some $\delta > 0$ for every $\epsilon > 0$ such that for every $x$ which provides $0 < |x - c| < \delta$ it is $|f(x) - L| < \epsilon$. My first question is about the ""open interval"" which is stated at the beginning. If we assume that $f(x)$ is defined on an open interval $I$ which contains $c$, shouldn't the definition state that ""for every $x \in I$ which provides $0 < |x - c| < \delta$"" instead of just saying ""for every $x$ which provides $0 < |x - c| < \delta$"" in order to limit $x$ with interval $I$ of our selection? My second question is about the interpretation of the above definition. Let's say we the $f(x)$ as in the following sketch: Here, the function is continuous, defined on an open interval $(A,B)$ with $a \in (A,B), c \in (A,B)$ and $b \in (A,B)$ and $|a-c|>|c-b|$. A valid selection for $\delta $ is $\delta = |c-b|$ such that for every $x$ with $0 < |x-c| < |c-b|$ it is $|f(x) - f(c)| <\epsilon$. But if we have the following $f(x)$ instead: which is the same as the first one, but the the domain of $f$ is $(A,a)$ this time and it is $|c-b| > |a-c|$. My question is, in this case, can $|b-c|$ again be a valid $\delta$ value, since again for every $x$ which provides $0 < |x-c| < |b-c|$ we have $|f(x) - f(c)| < \epsilon$. I ask this because I used to think this definition always for functions defined on the whole real line and this makes one to believe that we should have intervals of the same length around $c$ which provides $|f(x)-L|$ where $L$ is a general limit value. But in the second sketch, this is not the case.","I have questions about the complete and rigorous $(\epsilon,\delta)$ definition of two sided limits. The definition of the two sided limit in http://en.wikipedia.org/wiki/(%CE%B5,_%CE%B4)-definition_of_limit says that for a function $f(x)$ which is defined on an open interval containing $c$, possibly excluding $c$, it is $\lim_{x \to c} f(x)=L$ iff there is some $\delta > 0$ for every $\epsilon > 0$ such that for every $x$ which provides $0 < |x - c| < \delta$ it is $|f(x) - L| < \epsilon$. My first question is about the ""open interval"" which is stated at the beginning. If we assume that $f(x)$ is defined on an open interval $I$ which contains $c$, shouldn't the definition state that ""for every $x \in I$ which provides $0 < |x - c| < \delta$"" instead of just saying ""for every $x$ which provides $0 < |x - c| < \delta$"" in order to limit $x$ with interval $I$ of our selection? My second question is about the interpretation of the above definition. Let's say we the $f(x)$ as in the following sketch: Here, the function is continuous, defined on an open interval $(A,B)$ with $a \in (A,B), c \in (A,B)$ and $b \in (A,B)$ and $|a-c|>|c-b|$. A valid selection for $\delta $ is $\delta = |c-b|$ such that for every $x$ with $0 < |x-c| < |c-b|$ it is $|f(x) - f(c)| <\epsilon$. But if we have the following $f(x)$ instead: which is the same as the first one, but the the domain of $f$ is $(A,a)$ this time and it is $|c-b| > |a-c|$. My question is, in this case, can $|b-c|$ again be a valid $\delta$ value, since again for every $x$ which provides $0 < |x-c| < |b-c|$ we have $|f(x) - f(c)| < \epsilon$. I ask this because I used to think this definition always for functions defined on the whole real line and this makes one to believe that we should have intervals of the same length around $c$ which provides $|f(x)-L|$ where $L$ is a general limit value. But in the second sketch, this is not the case.",,"['calculus', 'real-analysis', 'limits']"
10,Stuck trying to prove that $e^{-x^{-2}}$ is $C^{\infty}$ [duplicate],Stuck trying to prove that  is  [duplicate],e^{-x^{-2}} C^{\infty},"This question already has answers here : Infinitely differentiable functions: how to prove that $e^\frac{1}{x^2-1}$ has derivative of any order? (2 answers) Closed 9 years ago . This is Spivak's Calculus on Manifolds ex. 2-25, he says Define $f:\mathbb{R}\to \mathbb{R}$ by $f(x) =  \left\lbrace   \begin{array}{l}      e^{-x^{-2}} &\text{ if } x \neq 0\\      0 &\text{ otherwise } \\   \end{array}   \right.$. Show that $f$ is $C^{\infty}$ anf $f^{(i)}(0)=0\; \forall i$. I haven't found a way to get a general solution. I only managed to prove: $(1)$ $f$ is continuous at $0$. This follows from $f(0)=0$ and $\lim_{x\to0} f(x)=\lim_{x\to 0}e^{-x^{-2}}=\lim_{x\to 0}\displaystyle\frac{1}{e^{1/x^2}}=0$ since $\displaystyle\frac{1}{x^2}\to+\infty$ if $x\to 0$ then $e^{1/x^2}\to+\infty$ if $x\to 0$, which means that $\displaystyle\frac{1}{e^{1/x^2}}\to 0$ if $x\to 0$ . $(2)$$f'(0)=0$. I got this by calculating the limit $f'(0)=\lim_{h\to 0}\displaystyle\frac{f(0+h)-f(0)}{h}=\lim_{h\to 0}\displaystyle\frac{f(h)}{h}=\lim_{h\to 0}\displaystyle\frac{e^{-h^{-2}}}{h}=\lim_{h\to 0}\displaystyle\frac{1/h}{e^{h^{-2}}}$. The last one follows from L'Hôpital: $\lim_{h\to 0}\displaystyle\frac{\frac{d}{dh}(1/h)}{\frac{d}{dh}(e^{h^{-2}})}=\lim_{h\to 0}\displaystyle\frac{(-1/h^2)}{(e^{h^{-2}})(-2h^{-3})}=\lim_{h\to 0}\displaystyle\frac{h}{2e^{h^{-2}}}=\displaystyle\frac{1}{2}\left(\lim_{h\to 0}\displaystyle\frac{1}{e^{h^{-2}}}\right)\left(\lim_{h\to 0} h\right) = 0$. $(3)$ $f''(0)=0$ I found that $\displaystyle\frac{df}{dx} =  \left\lbrace   \begin{array}{l}      2x^{-3}e^{-x^{-2}} &\text{ if } x \neq 0\\      0 &\text{ otherwise } \\   \end{array}   \right.$, then  I can get $f''(0)$ computing the limit $\lim_{h\to 0}\displaystyle\frac{f'(0+h)-f'(0)}{h} = \lim_{h\to 0}\displaystyle\frac{f'(h)}{h} = \lim_{h\to 0}\displaystyle\frac{2e^{-h^{-2}}}{h^4}= 2\lim_{h\to 0}\displaystyle\frac{1/h^4}{e^{h^{-2}}}=-2\lim_{h\to 0}\displaystyle\frac{-4h^{-5}}{-2h^{-3}e^{h^{-2}}}\\=4\lim_{h\to 0}\displaystyle\frac{1}{h^2e^{h^{-2}}}=4\lim_{h\to 0}\displaystyle\frac{h^{-2}}{e^{h^{-2}}})=-4\lim_{h\to 0}\displaystyle\frac{-2h^{-3}}{e^{h^{-2}}(-2h^{-3})}=-4\lim_{h\to 0}\displaystyle\frac{1}{e^{h^{-2}}}=0$, after using L'Hôpital twice. But still I don't have a clue to prove that $f^{(i)}(0)=0$ for every $i$. One of the ideas I managed was to find a general form for $f^{(i)}$ but after I got $f'$ doesn't seem possible to do so considering that I would have to apply the product rule each time.","This question already has answers here : Infinitely differentiable functions: how to prove that $e^\frac{1}{x^2-1}$ has derivative of any order? (2 answers) Closed 9 years ago . This is Spivak's Calculus on Manifolds ex. 2-25, he says Define $f:\mathbb{R}\to \mathbb{R}$ by $f(x) =  \left\lbrace   \begin{array}{l}      e^{-x^{-2}} &\text{ if } x \neq 0\\      0 &\text{ otherwise } \\   \end{array}   \right.$. Show that $f$ is $C^{\infty}$ anf $f^{(i)}(0)=0\; \forall i$. I haven't found a way to get a general solution. I only managed to prove: $(1)$ $f$ is continuous at $0$. This follows from $f(0)=0$ and $\lim_{x\to0} f(x)=\lim_{x\to 0}e^{-x^{-2}}=\lim_{x\to 0}\displaystyle\frac{1}{e^{1/x^2}}=0$ since $\displaystyle\frac{1}{x^2}\to+\infty$ if $x\to 0$ then $e^{1/x^2}\to+\infty$ if $x\to 0$, which means that $\displaystyle\frac{1}{e^{1/x^2}}\to 0$ if $x\to 0$ . $(2)$$f'(0)=0$. I got this by calculating the limit $f'(0)=\lim_{h\to 0}\displaystyle\frac{f(0+h)-f(0)}{h}=\lim_{h\to 0}\displaystyle\frac{f(h)}{h}=\lim_{h\to 0}\displaystyle\frac{e^{-h^{-2}}}{h}=\lim_{h\to 0}\displaystyle\frac{1/h}{e^{h^{-2}}}$. The last one follows from L'Hôpital: $\lim_{h\to 0}\displaystyle\frac{\frac{d}{dh}(1/h)}{\frac{d}{dh}(e^{h^{-2}})}=\lim_{h\to 0}\displaystyle\frac{(-1/h^2)}{(e^{h^{-2}})(-2h^{-3})}=\lim_{h\to 0}\displaystyle\frac{h}{2e^{h^{-2}}}=\displaystyle\frac{1}{2}\left(\lim_{h\to 0}\displaystyle\frac{1}{e^{h^{-2}}}\right)\left(\lim_{h\to 0} h\right) = 0$. $(3)$ $f''(0)=0$ I found that $\displaystyle\frac{df}{dx} =  \left\lbrace   \begin{array}{l}      2x^{-3}e^{-x^{-2}} &\text{ if } x \neq 0\\      0 &\text{ otherwise } \\   \end{array}   \right.$, then  I can get $f''(0)$ computing the limit $\lim_{h\to 0}\displaystyle\frac{f'(0+h)-f'(0)}{h} = \lim_{h\to 0}\displaystyle\frac{f'(h)}{h} = \lim_{h\to 0}\displaystyle\frac{2e^{-h^{-2}}}{h^4}= 2\lim_{h\to 0}\displaystyle\frac{1/h^4}{e^{h^{-2}}}=-2\lim_{h\to 0}\displaystyle\frac{-4h^{-5}}{-2h^{-3}e^{h^{-2}}}\\=4\lim_{h\to 0}\displaystyle\frac{1}{h^2e^{h^{-2}}}=4\lim_{h\to 0}\displaystyle\frac{h^{-2}}{e^{h^{-2}}})=-4\lim_{h\to 0}\displaystyle\frac{-2h^{-3}}{e^{h^{-2}}(-2h^{-3})}=-4\lim_{h\to 0}\displaystyle\frac{1}{e^{h^{-2}}}=0$, after using L'Hôpital twice. But still I don't have a clue to prove that $f^{(i)}(0)=0$ for every $i$. One of the ideas I managed was to find a general form for $f^{(i)}$ but after I got $f'$ doesn't seem possible to do so considering that I would have to apply the product rule each time.",,"['limits', 'derivatives']"
11,Typo in Spivak's explanation of limits in Calculus?,Typo in Spivak's explanation of limits in Calculus?,,"Here's what he says (including the preceding paragraph): ""To show in general that $f$ (where $f(x)=1/x$) approaches $1/a$ near $a$ for any $a$ we proceed in basically the same way, except that, again, we have to be a little more careful in formulating our initial stipulation. It's not good enough simply to require that $|x-a|$ should be less than $1$, or any other particular number, because if $a$ is close to $0$ this would allow values of $x$ that are negative (not to mention the embarrassing possibility that $x=0$, so that $f(x)$ isn't even defined!). The trick in this case is to first require that $|x-a| < \frac{|a|}{2}$; in other words, we require that $x$ be less than half as far from $0$ as $a$."" Okay, so I completely get the inequality, and what he's trying to say and why, but it seems like the English ""in other words"" part is not an equivalent statement to the inequality. If I were to translate the inequality to plain English, I would say, ""in other words, $x$ must be less than half as far from $a$ as $a$ is from $0$."" Or a more literal translation of the inequality itself, ""the difference between $x$ and $a$ must be less than half that of the difference between $a$ and $0$."" Normally I would just skid right by this type of thing and assume that the textbook author just made a typo, but given that this is Spivak and it's the fourth edition and whatnot I started to wonder whether I was just misunderstanding the logic of the statement, which irks me like nothing else. Here are the only two possible meanings of the sentence that I can discern (neither of which make sense in my view): (1) ""$x$ must be less than half as far from $0$ as $x$ is from $a$"" -or- (2) ""$x$ must be less than half as far from $0$ as $a$ is from $0$"" So suppose he means (1). So $x$ is a certain distance from $a$. Then the distance between $x$ and $0$ is less than half that distance. Even discounting the ""half"" specification, this statement would mean that $x$ is closer to 0 than it is to $a$, which, it seems to me, is the opposite of what that inequality is saying. Would (1) not translate to $|x|<\frac{|x-a|}{2}$? Now suppose he means (2). So $a$ is a certain distance from $0$. Then the distance between $x$ and $0$ is less than half that distance, which also seems to me to be blatantly incorrect, because according to the inequality, there can be an $x$ that is FARTHER from zero than $a$. Would (2) not translate to $|x|<\frac{|a|}{2}$? As I said before, I kind of doubt that I am correct about this - despite the fact that I can't think of a way that it could make sense - just because of the book that it's coming from. But I suppose there will always be typos in any book, so I'd just like to know if this is one. (I kind of doubt many of you happen to have a copy of Spivak's lying around, but this is on page 94 and it has a nice little diagram/number line that agrees with the inequality but not the statement... it seems to me.) Sorry for the length. Just trying to emulate Spivak's super thoroughness. :)","Here's what he says (including the preceding paragraph): ""To show in general that $f$ (where $f(x)=1/x$) approaches $1/a$ near $a$ for any $a$ we proceed in basically the same way, except that, again, we have to be a little more careful in formulating our initial stipulation. It's not good enough simply to require that $|x-a|$ should be less than $1$, or any other particular number, because if $a$ is close to $0$ this would allow values of $x$ that are negative (not to mention the embarrassing possibility that $x=0$, so that $f(x)$ isn't even defined!). The trick in this case is to first require that $|x-a| < \frac{|a|}{2}$; in other words, we require that $x$ be less than half as far from $0$ as $a$."" Okay, so I completely get the inequality, and what he's trying to say and why, but it seems like the English ""in other words"" part is not an equivalent statement to the inequality. If I were to translate the inequality to plain English, I would say, ""in other words, $x$ must be less than half as far from $a$ as $a$ is from $0$."" Or a more literal translation of the inequality itself, ""the difference between $x$ and $a$ must be less than half that of the difference between $a$ and $0$."" Normally I would just skid right by this type of thing and assume that the textbook author just made a typo, but given that this is Spivak and it's the fourth edition and whatnot I started to wonder whether I was just misunderstanding the logic of the statement, which irks me like nothing else. Here are the only two possible meanings of the sentence that I can discern (neither of which make sense in my view): (1) ""$x$ must be less than half as far from $0$ as $x$ is from $a$"" -or- (2) ""$x$ must be less than half as far from $0$ as $a$ is from $0$"" So suppose he means (1). So $x$ is a certain distance from $a$. Then the distance between $x$ and $0$ is less than half that distance. Even discounting the ""half"" specification, this statement would mean that $x$ is closer to 0 than it is to $a$, which, it seems to me, is the opposite of what that inequality is saying. Would (1) not translate to $|x|<\frac{|x-a|}{2}$? Now suppose he means (2). So $a$ is a certain distance from $0$. Then the distance between $x$ and $0$ is less than half that distance, which also seems to me to be blatantly incorrect, because according to the inequality, there can be an $x$ that is FARTHER from zero than $a$. Would (2) not translate to $|x|<\frac{|a|}{2}$? As I said before, I kind of doubt that I am correct about this - despite the fact that I can't think of a way that it could make sense - just because of the book that it's coming from. But I suppose there will always be typos in any book, so I'd just like to know if this is one. (I kind of doubt many of you happen to have a copy of Spivak's lying around, but this is on page 94 and it has a nice little diagram/number line that agrees with the inequality but not the statement... it seems to me.) Sorry for the length. Just trying to emulate Spivak's super thoroughness. :)",,"['calculus', 'limits', 'inequality']"
12,Evaluate the limit $\lim_{x \to 0}{\frac{\sqrt{1+x\sin{x}}-\sqrt{\cos{2x}}}{\tan^{2}{\frac{x}{2}}}}$,Evaluate the limit,\lim_{x \to 0}{\frac{\sqrt{1+x\sin{x}}-\sqrt{\cos{2x}}}{\tan^{2}{\frac{x}{2}}}},I need to evaluate the limit without using l'Hopital's rule. $$\lim_{x \to 0}{\frac{\sqrt{1+x\sin{x}}-\sqrt{\cos{2x}}}{\tan^{2}{\frac{x}{2}}}}$$,I need to evaluate the limit without using l'Hopital's rule. $$\lim_{x \to 0}{\frac{\sqrt{1+x\sin{x}}-\sqrt{\cos{2x}}}{\tan^{2}{\frac{x}{2}}}}$$,,"['calculus', 'limits']"
13,Evaluating a limit involving integral,Evaluating a limit involving integral,,"Let $f: [0,\infty)\to [0,\infty)$ be bounded and continuous. Find if exists : $$\displaystyle \lim_{n\to \infty}n\left(\sqrt[n]{\int_{0}^{\infty}f^{n+1}(x)e^{-x}\,\mathrm{d}x} \;-\;\sqrt[n]{\int_{0}^{\infty}f^{n}(x)e^{-x}\,\mathrm{d}x}\right)$$ Note : $f^n(x)=(f(x))^n$ This problem was given in a facebook group and everyone claimed it was right. I have made zero progress so far. Can someone solve it? Thanks a lot.","Let $f: [0,\infty)\to [0,\infty)$ be bounded and continuous. Find if exists : $$\displaystyle \lim_{n\to \infty}n\left(\sqrt[n]{\int_{0}^{\infty}f^{n+1}(x)e^{-x}\,\mathrm{d}x} \;-\;\sqrt[n]{\int_{0}^{\infty}f^{n}(x)e^{-x}\,\mathrm{d}x}\right)$$ Note : $f^n(x)=(f(x))^n$ This problem was given in a facebook group and everyone claimed it was right. I have made zero progress so far. Can someone solve it? Thanks a lot.",,"['calculus', 'limits']"
14,Find the value of : $\lim_{x\to\infty}\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x}}}}-\sqrt{x}$,Find the value of :,\lim_{x\to\infty}\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x}}}}-\sqrt{x},"$\lim_{x\to\infty}\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x}}}}-\sqrt{x}$ I tried conjugating and it didn't lead me anywhere please help guys. Thanks,","$\lim_{x\to\infty}\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x}}}}-\sqrt{x}$ I tried conjugating and it didn't lead me anywhere please help guys. Thanks,",,"['calculus', 'limits', 'radicals', 'nested-radicals']"
15,Use definition of limit to prove..,Use definition of limit to prove..,,"Question: Let $x_n = 1+\frac{5+n}{n^2}$. Use the definition to prove that  $$\lim_{x \to \infty} x_n = 1$$ I know from the definition, that $|x_n-x|<\epsilon$, so I start with: $$|1+ \frac{5+n}{n^2} -1| = |\frac{5+n}{n^2}| <\epsilon$$ Then I do a bunch of algebra and I can't figure out a closed form solution for $n$. help!","Question: Let $x_n = 1+\frac{5+n}{n^2}$. Use the definition to prove that  $$\lim_{x \to \infty} x_n = 1$$ I know from the definition, that $|x_n-x|<\epsilon$, so I start with: $$|1+ \frac{5+n}{n^2} -1| = |\frac{5+n}{n^2}| <\epsilon$$ Then I do a bunch of algebra and I can't figure out a closed form solution for $n$. help!",,"['analysis', 'limits']"
16,Please help me solve these limits... [closed],Please help me solve these limits... [closed],,"Closed. This question is off-topic . It is not currently accepting answers. Closed 10 years ago . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Homework questions must seek to understand the concepts being taught, not just demand a solution. For help writing a good homework question, see: How to ask a homework question? . Improve this question So, I need you to solve one of these limits for me, so I can see how it's done, so I can do the rest myself.","Closed. This question is off-topic . It is not currently accepting answers. Closed 10 years ago . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Homework questions must seek to understand the concepts being taught, not just demand a solution. For help writing a good homework question, see: How to ask a homework question? . Improve this question So, I need you to solve one of these limits for me, so I can see how it's done, so I can do the rest myself.",,['limits']
17,"How to find this nice limit: $I=\lim_{t\to0^{+}}\lim_{x\to+\infty}f(x,t)$",How to find this nice limit:,"I=\lim_{t\to0^{+}}\lim_{x\to+\infty}f(x,t)","Find the value: $$I=\lim_{t\to0^{+}}\lim_{x\to+\infty}\dfrac{\displaystyle\int_{0}^{\sqrt{t}}dx\int_{x^2}^{t}\sin{y^2}dy}{\left[\left(\dfrac{2}{\pi}\arctan{\dfrac{x}{t^2}}\right)^x-1\right]\arctan{t^{\frac{3}{2}}}}$$ I spent some hours doing it, but have failed. Thank you!","Find the value: $$I=\lim_{t\to0^{+}}\lim_{x\to+\infty}\dfrac{\displaystyle\int_{0}^{\sqrt{t}}dx\int_{x^2}^{t}\sin{y^2}dy}{\left[\left(\dfrac{2}{\pi}\arctan{\dfrac{x}{t^2}}\right)^x-1\right]\arctan{t^{\frac{3}{2}}}}$$ I spent some hours doing it, but have failed. Thank you!",,[]
18,Does the derivative of $x^{-1}$ and of $x^3-x$ equal $-\frac{1}{x^{2}}$ and $3x^2-1$?,Does the derivative of  and of  equal  and ?,x^{-1} x^3-x -\frac{1}{x^{2}} 3x^2-1,"I want to check my answers concerning the derivative of the following functions: $\displaystyle f(x)= \frac{1}{x}$ and of  $\displaystyle j(x)= x^3-x$ $$\displaystyle f(x)= \frac{1}{x}$$ $$\begin{align} f'(x) & =  \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \\  & = \lim_{h \to 0} \frac{\frac{1}{x+h} - \frac{1}{x}}{h} \\   & = \lim_{h \to 0} \frac{\frac{x}{x(x+h)} - \frac{(x+h)}{x(x+h)}}{h} \\  & = \lim_{h \to 0} \frac{\frac{x - (x+h)}{x^2+hx}}{h} \\   & = \lim_{h \to 0} \frac{\frac{-h}{x^2+hx}}{h} \\  & = \lim_{h \to 0} \frac{-h}{x^2+hx} \times \frac{1}{h} \\  & = \lim_{h \to 0} \frac{-1}{x^2+hx} \\  & = -\frac{1}{x^2} \end{align}$$ $$j(x) = x^3 - x$$ $$\begin{align} j'(x) & =  \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \\  & = \lim_{h \to 0} \frac{(x+h)^3 - x^3 - x}{h} \\   & = \lim_{h \to 0} \frac{x^3 + 3x^2h + 3xh^2 + h^3 - x - h - x^3 + x}{h} \\  & = \lim_{h \to 0} \frac{3x^2h + 3xh^2 + h^3 - h}{h} \\   & = \lim_{h \to 0} \frac{h(3x^2 + 3xh + h^2 - 1)}{h} \\  & = \frac{3x^2 - 1}{1} \\  & = 3x^2 - 1 \end{align}$$ Please, be rude, if you see any error correct me.","I want to check my answers concerning the derivative of the following functions: $\displaystyle f(x)= \frac{1}{x}$ and of  $\displaystyle j(x)= x^3-x$ $$\displaystyle f(x)= \frac{1}{x}$$ $$\begin{align} f'(x) & =  \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \\  & = \lim_{h \to 0} \frac{\frac{1}{x+h} - \frac{1}{x}}{h} \\   & = \lim_{h \to 0} \frac{\frac{x}{x(x+h)} - \frac{(x+h)}{x(x+h)}}{h} \\  & = \lim_{h \to 0} \frac{\frac{x - (x+h)}{x^2+hx}}{h} \\   & = \lim_{h \to 0} \frac{\frac{-h}{x^2+hx}}{h} \\  & = \lim_{h \to 0} \frac{-h}{x^2+hx} \times \frac{1}{h} \\  & = \lim_{h \to 0} \frac{-1}{x^2+hx} \\  & = -\frac{1}{x^2} \end{align}$$ $$j(x) = x^3 - x$$ $$\begin{align} j'(x) & =  \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \\  & = \lim_{h \to 0} \frac{(x+h)^3 - x^3 - x}{h} \\   & = \lim_{h \to 0} \frac{x^3 + 3x^2h + 3xh^2 + h^3 - x - h - x^3 + x}{h} \\  & = \lim_{h \to 0} \frac{3x^2h + 3xh^2 + h^3 - h}{h} \\   & = \lim_{h \to 0} \frac{h(3x^2 + 3xh + h^2 - 1)}{h} \\  & = \frac{3x^2 - 1}{1} \\  & = 3x^2 - 1 \end{align}$$ Please, be rude, if you see any error correct me.",,['calculus']
19,"$S_n$ be the sum of areas of $y=\sin x, y=\sin {nx}$. Then $\lim_{n \to \infty}S_n=8/{\pi}$?",be the sum of areas of . Then ?,"S_n y=\sin x, y=\sin {nx} \lim_{n \to \infty}S_n=8/{\pi}","Let $n$ be a natural number. Also let $S_n$ be the sum of the areas of the regions enclosed with the two curves $y=\sin x$ and $y=\sin {nx}$ in $0\le x\le \pi$. It's easy to find $S_2, S_3$. For smaller $n$, we can also use wolfram to find $S_n$. Then, I got interested in the problems for larger $n$. After my observation, I reached the following expectation: My expectation : $$\lim_{n \to \infty}S_n=\frac{8}{\pi}.$$ I've tried to prove this, but I don't have any good idea with a tedious calculation. Then, here is my question. Question : Could you show me how to find $\lim_{n \to \infty}S_n$ ?","Let $n$ be a natural number. Also let $S_n$ be the sum of the areas of the regions enclosed with the two curves $y=\sin x$ and $y=\sin {nx}$ in $0\le x\le \pi$. It's easy to find $S_2, S_3$. For smaller $n$, we can also use wolfram to find $S_n$. Then, I got interested in the problems for larger $n$. After my observation, I reached the following expectation: My expectation : $$\lim_{n \to \infty}S_n=\frac{8}{\pi}.$$ I've tried to prove this, but I don't have any good idea with a tedious calculation. Then, here is my question. Question : Could you show me how to find $\lim_{n \to \infty}S_n$ ?",,['limits']
20,Can I use l'Hôpital to find a limit after changing to polar coordinates?,Can I use l'Hôpital to find a limit after changing to polar coordinates?,,"I was wondering if there is another approach instead of using the $log (x + 1) \sim_{0} x$ or Taylor series to solve: $$\lim_{(x,y) \to (0,0)} f(x,y)={\ln(xy + 1) \over x}.$$ Particularly, I'm wondering whether I can use polar coordinates and l'Hopital. Something like this. First, we switch to polar coordinates: $$\lim_{(r) \to (0)} f(r)={\ln(r^2\cos\phi \sin\phi + 1) \over r\cos \phi}.$$ Now we use l'Hopital: \begin{align} &\lim_{(r) \to (0)} f(r)=\lim_{(r) \to (0)}{\ln(r^2\cos\phi \sin\phi + 1) \over r\cos \phi} \\ =&\lim_{(r) \to (0)}{{d\over dr}(\ln(r^2\cos\phi \sin\phi + 1) \over{d\over dr} (r\cos \phi)} \\ =&\lim_{(r) \to (0)}{2r\sin\phi \cos\phi \over {r^2\sin\phi \cos^2\phi + \cos\phi}} \\ =&\lim_{(r) \to (0)}{2r\sin\phi \over r^2\sin\phi \cos\phi +1}=0. \end{align} As $$\lim_{(r) \to (0)} f(\phi)= {2r\sin\phi }= 0.$$ Does this make any sense at all or I can't use l'hopital in this case?","I was wondering if there is another approach instead of using the $log (x + 1) \sim_{0} x$ or Taylor series to solve: $$\lim_{(x,y) \to (0,0)} f(x,y)={\ln(xy + 1) \over x}.$$ Particularly, I'm wondering whether I can use polar coordinates and l'Hopital. Something like this. First, we switch to polar coordinates: $$\lim_{(r) \to (0)} f(r)={\ln(r^2\cos\phi \sin\phi + 1) \over r\cos \phi}.$$ Now we use l'Hopital: \begin{align} &\lim_{(r) \to (0)} f(r)=\lim_{(r) \to (0)}{\ln(r^2\cos\phi \sin\phi + 1) \over r\cos \phi} \\ =&\lim_{(r) \to (0)}{{d\over dr}(\ln(r^2\cos\phi \sin\phi + 1) \over{d\over dr} (r\cos \phi)} \\ =&\lim_{(r) \to (0)}{2r\sin\phi \cos\phi \over {r^2\sin\phi \cos^2\phi + \cos\phi}} \\ =&\lim_{(r) \to (0)}{2r\sin\phi \over r^2\sin\phi \cos\phi +1}=0. \end{align} As $$\lim_{(r) \to (0)} f(\phi)= {2r\sin\phi }= 0.$$ Does this make any sense at all or I can't use l'hopital in this case?",,"['limits', 'multivariable-calculus']"
21,How Can I Find The Value Of This Limit 10,How Can I Find The Value Of This Limit 10,,find this limit: $$\displaystyle\lim_{n\to+\infty}\left[\sum_{k=1}^{n}\left(\dfrac{1}{\sqrt{k}}- \int_{0}^{\large {1/\sqrt k}}\dfrac{t^2}{1+t^2}dt\right)-2\sqrt{n}\right]$$,find this limit: $$\displaystyle\lim_{n\to+\infty}\left[\sum_{k=1}^{n}\left(\dfrac{1}{\sqrt{k}}- \int_{0}^{\large {1/\sqrt k}}\dfrac{t^2}{1+t^2}dt\right)-2\sqrt{n}\right]$$,,['limits']
22,A question on a Lipschitz function,A question on a Lipschitz function,,"This is the problem: Prove or disprove the following statement: If $f:[0,+\infty]\rightarrow\mathbb{R^+}$  is a Lipschitz function and not bounded, then it has necessarily $\lim_{x\to+\infty} f(x) = +\infty$","This is the problem: Prove or disprove the following statement: If $f:[0,+\infty]\rightarrow\mathbb{R^+}$  is a Lipschitz function and not bounded, then it has necessarily $\lim_{x\to+\infty} f(x) = +\infty$",,['analysis']
23,Evaluating $\lim_{n \to \infty} \oint_{ |z| = 1/4} \frac{1}{(4 z(1-z))^n} \frac{\mathrm{d}z}{z (1-2 z)} = \frac{1}{2}$,Evaluating,\lim_{n \to \infty} \oint_{ |z| = 1/4} \frac{1}{(4 z(1-z))^n} \frac{\mathrm{d}z}{z (1-2 z)} = \frac{1}{2},"While working on an earlier question involving $\sum_{j=0}^n \binom{n+j-1}{j} \frac{1}{2^{n+j}}$ I rewrote the sum as a contour integral, using generating functions: $$      \sum_{j=0}^n \binom{n+j-1}{j} \frac{1}{2^{n+j}} =  \sum_{j=0}^n \left( \frac{1}{4^n} \binom{n+j-1}{j} \right) 2^{n-j} = [t^n] \left( \sum_{j=0}^\infty \frac{t^j}{4^n}\binom{n+j-1}{j}  \cdot \sum_{j=0}^\infty t^j 2^j \right) $$ Now, using well known generating functions, for $|t|<1/2$: $$    \sum_{j=0}^\infty t^j \binom{n+j-1}{j} = \frac{1}{(1-t)^n} \quad \sum_{j=0}^\infty (2t)^j = \frac{1}{1-2t} $$ We get $$    \sum_{j=0}^n \binom{n+j-1}{j} \frac{1}{2^{n+j}} = [t^n] \left( \frac{1}{1-2t} \frac{1}{\left(4(1-t)\right)^n} \right) = \frac{1}{2 \pi i} \oint \frac{1}{1-2t} \left(\frac{1}{4 t(1-t)} \right)^n \frac{\mathrm{d} t}{t} $$ where the Cauchy integral formula was used along with $n! [t^n] g(t) = g^{(n)}(0)$. Now, Byron Schmuland showed that the large $n$ limit of the left-hand-side equals $\frac{1}{2}$. Question: Can one demonstrate $$ \lim_{n \to \infty} \frac{1}{2 \pi i} \oint_{ |t| = \rho} \frac{1}{1-2t} \left(\frac{1}{4 t(1-t)} \right)^n \frac{\mathrm{d} t}{t} = \frac{1}{2}$$ using asymptotic methods? Here $0<\rho<\frac{1}{2}$.","While working on an earlier question involving $\sum_{j=0}^n \binom{n+j-1}{j} \frac{1}{2^{n+j}}$ I rewrote the sum as a contour integral, using generating functions: $$      \sum_{j=0}^n \binom{n+j-1}{j} \frac{1}{2^{n+j}} =  \sum_{j=0}^n \left( \frac{1}{4^n} \binom{n+j-1}{j} \right) 2^{n-j} = [t^n] \left( \sum_{j=0}^\infty \frac{t^j}{4^n}\binom{n+j-1}{j}  \cdot \sum_{j=0}^\infty t^j 2^j \right) $$ Now, using well known generating functions, for $|t|<1/2$: $$    \sum_{j=0}^\infty t^j \binom{n+j-1}{j} = \frac{1}{(1-t)^n} \quad \sum_{j=0}^\infty (2t)^j = \frac{1}{1-2t} $$ We get $$    \sum_{j=0}^n \binom{n+j-1}{j} \frac{1}{2^{n+j}} = [t^n] \left( \frac{1}{1-2t} \frac{1}{\left(4(1-t)\right)^n} \right) = \frac{1}{2 \pi i} \oint \frac{1}{1-2t} \left(\frac{1}{4 t(1-t)} \right)^n \frac{\mathrm{d} t}{t} $$ where the Cauchy integral formula was used along with $n! [t^n] g(t) = g^{(n)}(0)$. Now, Byron Schmuland showed that the large $n$ limit of the left-hand-side equals $\frac{1}{2}$. Question: Can one demonstrate $$ \lim_{n \to \infty} \frac{1}{2 \pi i} \oint_{ |t| = \rho} \frac{1}{1-2t} \left(\frac{1}{4 t(1-t)} \right)^n \frac{\mathrm{d} t}{t} = \frac{1}{2}$$ using asymptotic methods? Here $0<\rho<\frac{1}{2}$.",,"['limits', 'contour-integration']"
24,Compute the limit,Compute the limit,,"Consider an equation $$    \tan (x) = \frac{a x}{x^2+b} $$ where $a,b \neq 0$. Plotting $\tan(x)$ and function on the RHS we can see that this equation has infinitely many positive solutions $x_{n}$ and $x_{n} \sim \pi n$ as $n \to \infty$, i.e.  $$    \lim\limits_{n \to \infty} \frac{x_{n}}{n} = \pi. $$ But is it possible to show the latter equality analitically?","Consider an equation $$    \tan (x) = \frac{a x}{x^2+b} $$ where $a,b \neq 0$. Plotting $\tan(x)$ and function on the RHS we can see that this equation has infinitely many positive solutions $x_{n}$ and $x_{n} \sim \pi n$ as $n \to \infty$, i.e.  $$    \lim\limits_{n \to \infty} \frac{x_{n}}{n} = \pi. $$ But is it possible to show the latter equality analitically?",,"['calculus', 'limits']"
25,Infimum of a union,Infimum of a union,,"I have a set $X$ and a function \begin{equation} f: X \rightarrow \mathbb{R} \end{equation} and I am interested in the value \begin{equation} \inf\limits_{x \in X} f(x) \,. \end{equation} I can represent $X$ as \begin{equation} X = \bigcup\limits_{i \in I} X_i \,, \end{equation} where the index set $I$ is uncountable. Now I wonder whether \begin{equation} \inf\limits_{x \in X} f(x) = \inf\limits_{i \in I} \left( \inf\limits_{x \in X_i} f(x) \right) \,. \end{equation} Is this true? If so, how can I see this?","I have a set $X$ and a function \begin{equation} f: X \rightarrow \mathbb{R} \end{equation} and I am interested in the value \begin{equation} \inf\limits_{x \in X} f(x) \,. \end{equation} I can represent $X$ as \begin{equation} X = \bigcup\limits_{i \in I} X_i \,, \end{equation} where the index set $I$ is uncountable. Now I wonder whether \begin{equation} \inf\limits_{x \in X} f(x) = \inf\limits_{i \in I} \left( \inf\limits_{x \in X_i} f(x) \right) \,. \end{equation} Is this true? If so, how can I see this?",,"['analysis', 'limits']"
26,Is it equation true - limit,Is it equation true - limit,,Is it true that : $\lim_{ x\to\infty }  \left( 1+\frac{f \left( x \right) }{x} \right) ^x = \exp \left(  \lim_{ x\to\infty } f \left( x \right)   \right)$ ? Assumption is that limit of $\lim_{ x\to\infty } f(x)$ exists.,Is it true that : $\lim_{ x\to\infty }  \left( 1+\frac{f \left( x \right) }{x} \right) ^x = \exp \left(  \lim_{ x\to\infty } f \left( x \right)   \right)$ ? Assumption is that limit of $\lim_{ x\to\infty } f(x)$ exists.,,"['analysis', 'limits']"
27,Prove that $\lim_{x\to-\infty}\left(\psi(x)-\psi\left(\frac x2\right)-\frac1x-\ln 2\right)\sin(\pi x)=\pi$,Prove that,\lim_{x\to-\infty}\left(\psi(x)-\psi\left(\frac x2\right)-\frac1x-\ln 2\right)\sin(\pi x)=\pi,"I was messing around on Desmos when I saw this interesting limit: $$\lim_{x\to-\infty}\left(\psi(x)-\psi\left(\frac x2\right)-\frac1x-\ln 2\right)\sin(\pi x)=\pi$$ This is of course what I think is the limit, but it seems to be true. Here $\psi$ denotes the digamma function. I do know that $$\lim_{x\rightarrow0}\left(\psi(x)-\psi\left(\frac x2\right)-\frac1x\right)=0$$ But I don't know how this can help. Also, removing the $\ln2$ term and replacing it with some constant $a\ne\ln2$ makes it [seem] non-convergent.","I was messing around on Desmos when I saw this interesting limit: This is of course what I think is the limit, but it seems to be true. Here denotes the digamma function. I do know that But I don't know how this can help. Also, removing the term and replacing it with some constant makes it [seem] non-convergent.",\lim_{x\to-\infty}\left(\psi(x)-\psi\left(\frac x2\right)-\frac1x-\ln 2\right)\sin(\pi x)=\pi \psi \lim_{x\rightarrow0}\left(\psi(x)-\psi\left(\frac x2\right)-\frac1x\right)=0 \ln2 a\ne\ln2,"['calculus', 'limits', 'digamma-function']"
28,A question about a limit of exponential function,A question about a limit of exponential function,,"I am watching some videos of limit exercises, and here is a formula I don't get it. $$\lim_{n\rightarrow \infty}\left(\frac{n}{n+1}\right)^n=\exp\left({\lim_{n\rightarrow \infty}\left(\frac{n}{n+1}-1\right)n}\right)$$ It has a $-1$ on RHS. I thought the LHS should equal to $\lim_{n\rightarrow\infty} \exp\left({\ln(\frac{n}{n+1})n}\right)=\exp\left(\lim_{n\rightarrow\infty}\ln\left(\frac{n}{n+1}\right)n\right)$ . However, this is wrong after I check it on wolframalpha, since the final value should be $1/e$ and $\exp\left(\lim_{n\rightarrow\infty}\ln\left(\frac{n}{n+1}\right)n\right)$ give you infinity. I think I can't shift the limit into exponent, but I thought $\lim_{x→c}f(g(x))=f(\lim_{x→c}g(x))$ hold if $f(x)$ is continuous, and $e^x$ is continuous. I am wondering where this -1 comes from, and where I made mistake? Any comment will be appreciated!","I am watching some videos of limit exercises, and here is a formula I don't get it. It has a on RHS. I thought the LHS should equal to . However, this is wrong after I check it on wolframalpha, since the final value should be and give you infinity. I think I can't shift the limit into exponent, but I thought hold if is continuous, and is continuous. I am wondering where this -1 comes from, and where I made mistake? Any comment will be appreciated!",\lim_{n\rightarrow \infty}\left(\frac{n}{n+1}\right)^n=\exp\left({\lim_{n\rightarrow \infty}\left(\frac{n}{n+1}-1\right)n}\right) -1 \lim_{n\rightarrow\infty} \exp\left({\ln(\frac{n}{n+1})n}\right)=\exp\left(\lim_{n\rightarrow\infty}\ln\left(\frac{n}{n+1}\right)n\right) 1/e \exp\left(\lim_{n\rightarrow\infty}\ln\left(\frac{n}{n+1}\right)n\right) \lim_{x→c}f(g(x))=f(\lim_{x→c}g(x)) f(x) e^x,"['calculus', 'limits', 'analysis', 'exponential-function']"
29,Left hand limit of right hand limit?,Left hand limit of right hand limit?,,"I need help figuring this out Suppose we have an increasing function $f : \mathbb{R} \rightarrow \mathbb{R}$ and we define a function $g: \mathbb{R} \rightarrow \mathbb{R}$ in the following way: $$ g(x) := \lim_{t \to x^{+}}f(t).$$ We know that since $f$ is monotone, then the right hand limit exists and the function $g$ is well defined for all $x \in \mathbb{R}$ . My question is: what can we say about $\lim_{t \to x^{-}}g(t)$ and is it true that it is equal to $\lim_{t \to x^{-}}f(t)$ ? I proved that the function $g$ is right-continuous, obtaining $$\lim_{t \to x^{+}}g(t) = g(x) = \lim_{t \to x^{+}}f(t)$$ but I’m having a hard time proving what happens for the left hand limits. Intuitively, I’d say that the left hand limits are also equal. One can easily prove that $g(x) = f(x)$ for all $x \in \mathbb{R}$ such that $f$ is continuous at $x$ . Thus, the only points where $f$ and $g$ may differ are the discontinuity points of $f$ and, in particular, those for which $f(x) = lim_{t \to x^{-}}f(t)$ . Still, I don’t know if this is true and how to prove it rigorously. Any help would be appreciated.","I need help figuring this out Suppose we have an increasing function and we define a function in the following way: We know that since is monotone, then the right hand limit exists and the function is well defined for all . My question is: what can we say about and is it true that it is equal to ? I proved that the function is right-continuous, obtaining but I’m having a hard time proving what happens for the left hand limits. Intuitively, I’d say that the left hand limits are also equal. One can easily prove that for all such that is continuous at . Thus, the only points where and may differ are the discontinuity points of and, in particular, those for which . Still, I don’t know if this is true and how to prove it rigorously. Any help would be appreciated.",f : \mathbb{R} \rightarrow \mathbb{R} g: \mathbb{R} \rightarrow \mathbb{R}  g(x) := \lim_{t \to x^{+}}f(t). f g x \in \mathbb{R} \lim_{t \to x^{-}}g(t) \lim_{t \to x^{-}}f(t) g \lim_{t \to x^{+}}g(t) = g(x) = \lim_{t \to x^{+}}f(t) g(x) = f(x) x \in \mathbb{R} f x f g f f(x) = lim_{t \to x^{-}}f(t),"['real-analysis', 'limits', 'continuity']"
30,Does this limit exist and if it does what is it? [closed],Does this limit exist and if it does what is it? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I was playing around with a few values of $k$ between $0$ and $1$ on Wolfram alpha and found that for all of them $$\sum_{n= 1}^\infty \frac{\sin(n)}{n^k}$$ converges and I was wondering about the limit as k goes to 0 from the right.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I was playing around with a few values of between and on Wolfram alpha and found that for all of them converges and I was wondering about the limit as k goes to 0 from the right.",k 0 1 \sum_{n= 1}^\infty \frac{\sin(n)}{n^k},"['limits', 'convergence-divergence']"
31,Two variables limit.,Two variables limit.,,"Exercise . Discuss in $\alpha\in\mathbb{R}$ the value of following limit $$ \lim_{(x,y)\to(0,0)}f(x,y)=\lim_{(x,y)\to(0,0)}\frac{x^2y}{(x^4+y^2)^\alpha(x^2+y^2)^{\frac{1}{2}}}. $$ Solution . One has that $|f|\leq|x||y|^{1-2\alpha}$ , which converges to $0$ if $\alpha\leq\frac{1}{2}$ , and hence so does $f$ . On the other hand, looking at the restriction of $f$ on the set $\{x>0, x=y\}$ , one has $$ \lim_{x\to0^+}f(x,x)=\lim_{x\to0^+}\frac{1}{\sqrt{2}}x^{2-2\alpha} $$ which equals $+\infty$ as soon as $\alpha>1$ , and hence in this case the above limit does not exist (being for example $f$ identically zero on the coordinate axes). I am left with the case $\frac{1}{2}<\alpha\leq1$ , which I tried to work out using polar coordinates but with no success.","Exercise . Discuss in the value of following limit Solution . One has that , which converges to if , and hence so does . On the other hand, looking at the restriction of on the set , one has which equals as soon as , and hence in this case the above limit does not exist (being for example identically zero on the coordinate axes). I am left with the case , which I tried to work out using polar coordinates but with no success.","\alpha\in\mathbb{R} 
\lim_{(x,y)\to(0,0)}f(x,y)=\lim_{(x,y)\to(0,0)}\frac{x^2y}{(x^4+y^2)^\alpha(x^2+y^2)^{\frac{1}{2}}}.
 |f|\leq|x||y|^{1-2\alpha} 0 \alpha\leq\frac{1}{2} f f \{x>0, x=y\} 
\lim_{x\to0^+}f(x,x)=\lim_{x\to0^+}\frac{1}{\sqrt{2}}x^{2-2\alpha}
 +\infty \alpha>1 f \frac{1}{2}<\alpha\leq1","['real-analysis', 'calculus', 'limits', 'multivariable-calculus']"
32,Limiting Property of an Integral?,Limiting Property of an Integral?,,"I have observed an interesting property which could be wrong as it seems completely absurd but the motivation behind it makes me inclined to believe it is true (or at least in some cases). I do not have any pure mathematical background so I do not know what I am talking about but I wish this kind community would get the gist of what I am trying to ask and would kindly direct me to an answer. So here goes nothing. The Observation : Suppose $g(t)$ is a periodic, symmetric function if it satisfies the following conditions: There exists a nonzero constant T such that $g(t+T) = g(t)$ . (periodic) For all $ \delta \in[0,\frac{T}{2}]$ , $g(\frac{T}{2}-\delta) = g(\frac{T}{2}+\delta)$ . (symmetric) Then for any integrable function $f(t)$ , $\lim_{w \to \infty} \int f(t)g(wt)dt = \alpha \int f(t)dt$ where $\alpha$ is some constant. My question is: Is this in any way true? The Motivation: One function that satisfies the function $g(t)$ is $sin^2(t)$ to which I find with several computational evidences that $\lim_{w \to \infty} \int f(t)\sin^2(wt)dt = \frac{1}{2} \int f(t)dt$ . Thank you for entertaining this question. If you find this question to be nonsensical, then I apologize in advance.","I have observed an interesting property which could be wrong as it seems completely absurd but the motivation behind it makes me inclined to believe it is true (or at least in some cases). I do not have any pure mathematical background so I do not know what I am talking about but I wish this kind community would get the gist of what I am trying to ask and would kindly direct me to an answer. So here goes nothing. The Observation : Suppose is a periodic, symmetric function if it satisfies the following conditions: There exists a nonzero constant T such that . (periodic) For all , . (symmetric) Then for any integrable function , where is some constant. My question is: Is this in any way true? The Motivation: One function that satisfies the function is to which I find with several computational evidences that . Thank you for entertaining this question. If you find this question to be nonsensical, then I apologize in advance.","g(t) g(t+T) = g(t)  \delta \in[0,\frac{T}{2}] g(\frac{T}{2}-\delta) = g(\frac{T}{2}+\delta) f(t) \lim_{w \to \infty} \int f(t)g(wt)dt = \alpha \int f(t)dt \alpha g(t) sin^2(t) \lim_{w \to \infty} \int f(t)\sin^2(wt)dt = \frac{1}{2} \int f(t)dt","['real-analysis', 'calculus', 'limits']"
33,Calculate $\lim_{x \to 0} \frac{\cos(x)}{\sin(x)}$,Calculate,\lim_{x \to 0} \frac{\cos(x)}{\sin(x)},I have a question regarding the following limit calculation: $\lim_{x \to 0} \frac{\cos(x)}{\sin(x)}$ The only way I can solve this is by looking at the one-sided limits: $\\$ From above: $\lim_{x \to 0^{+}} \frac{\cos(x)}{\sin(x)}$ . The numerator approaches $1$ with a positive sign. The denominator approaches $0$ with a positive sign. $\implies$ the limit is $\infty$ $\\$ From below: $\lim_{x \to 0^{-}} \frac{\cos(x)}{\sin(x)}$ . The numerator approaches $1$ with a positive sign. The denominator approaches $0$ with a negative sign. $\implies$ the limit is $-\infty$ The one-sided limits do not agree and so the limit does not exist. My concern is this: would you give full marks for an answer like this? It feels very informal but I do not know how to argue the same thing formally.,I have a question regarding the following limit calculation: The only way I can solve this is by looking at the one-sided limits: From above: . The numerator approaches with a positive sign. The denominator approaches with a positive sign. the limit is From below: . The numerator approaches with a positive sign. The denominator approaches with a negative sign. the limit is The one-sided limits do not agree and so the limit does not exist. My concern is this: would you give full marks for an answer like this? It feels very informal but I do not know how to argue the same thing formally.,\lim_{x \to 0} \frac{\cos(x)}{\sin(x)} \\ \lim_{x \to 0^{+}} \frac{\cos(x)}{\sin(x)} 1 0 \implies \infty \\ \lim_{x \to 0^{-}} \frac{\cos(x)}{\sin(x)} 1 0 \implies -\infty,['limits']
34,Minimal assumption on points of discontinuity of $f$ so that $\liminf_{\epsilon \to \infty} \frac{f(x)}{f(x+\frac{t}{\epsilon} )}=1$,Minimal assumption on points of discontinuity of  so that,f \liminf_{\epsilon \to \infty} \frac{f(x)}{f(x+\frac{t}{\epsilon} )}=1,"Suppose we are given a function $f(x)$ . We want to show the following claim: \begin{align} \liminf_{\epsilon \to \infty} \frac{f(x)}{f(x+\frac{t}{\epsilon} )}=1, \end{align} almost everywhere $(x,t)$ (in Lebesgue measure) . Question: What is a minimal assumption on the points of discontinuity of $f$ that will guarantee this to be true? For example, is it sufficient to assume that the set of points of discontinuity has Lebesgue measure zero? But then there are sets that are dense and have measure zero. I am somewhat confused about this.","Suppose we are given a function . We want to show the following claim: almost everywhere (in Lebesgue measure) . Question: What is a minimal assumption on the points of discontinuity of that will guarantee this to be true? For example, is it sufficient to assume that the set of points of discontinuity has Lebesgue measure zero? But then there are sets that are dense and have measure zero. I am somewhat confused about this.","f(x) \begin{align}
\liminf_{\epsilon \to \infty} \frac{f(x)}{f(x+\frac{t}{\epsilon} )}=1,
\end{align} (x,t) f","['real-analysis', 'limits', 'continuity', 'limsup-and-liminf']"
35,Difficult limit proof,Difficult limit proof,,"Let $a_1 = 1$ and define a sequence recursively by $$a_{n+1} = \sqrt{a_1+a_2+\dots+a_n}.$$ Show that $\lim_{n \to \infty} \frac{a_n}{n} = \frac{1}{2}$ . So far, I've written $a_{n+1} = \sqrt{a_n^2 + a_n}$ , and have shown that if $a_n/n < 1/2$ , then so is $a_{n+1}/{(n+1)}$ . I'm really not sure how to relate the recurrence relation to the limit we want, though.","Let and define a sequence recursively by Show that . So far, I've written , and have shown that if , then so is . I'm really not sure how to relate the recurrence relation to the limit we want, though.",a_1 = 1 a_{n+1} = \sqrt{a_1+a_2+\dots+a_n}. \lim_{n \to \infty} \frac{a_n}{n} = \frac{1}{2} a_{n+1} = \sqrt{a_n^2 + a_n} a_n/n < 1/2 a_{n+1}/{(n+1)},"['limits', 'analysis']"
36,Limit of sequences equals $f'(0)$,Limit of sequences equals,f'(0),"I'm working on a question that states: Consider a differentiable function $f:\mathbb{R}\to\mathbb{R}$ . Furthermore consider the sequences $(x_n)_n$ and $(y_n)_n$ which both have limit $0$ and $x_n < y_n$ for all $n\in\mathbb{N}$ . Is the following true? $$\lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f'(0). $$ If yes give a proof, if no give a counter example and find the extra requirements for it to be true. I thought it was true and as a proof I did the following: Given that $f$ is differentiable and that $x_n < y_n$ , by the mean value theorem there exists a $c_n \in (x_n,y_n)$ such that $$ \frac{f(y_n)-f(x_n)}{y_n-x_n} = f'(c_n). $$ As $x_n$ and $y_n$ both converge to $0$ we have that $c_n$ will also converge to $0$ . Therefore, by taking the limit for $n$ to infinity we find that $$\lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = \lim_{n\to\infty}f'(c_n) = f'(0). $$ At this point I realised that I had used the continuity of $f'$ which was not given in the question. I am now wondering if this is a necessary condition for the problem or if it can be solved without using the continuity of $f'$ . If it is a necessary condition what is a suitable counter example where $f'$ isn't continuous? Any help is appreciated.","I'm working on a question that states: Consider a differentiable function . Furthermore consider the sequences and which both have limit and for all . Is the following true? If yes give a proof, if no give a counter example and find the extra requirements for it to be true. I thought it was true and as a proof I did the following: Given that is differentiable and that , by the mean value theorem there exists a such that As and both converge to we have that will also converge to . Therefore, by taking the limit for to infinity we find that At this point I realised that I had used the continuity of which was not given in the question. I am now wondering if this is a necessary condition for the problem or if it can be solved without using the continuity of . If it is a necessary condition what is a suitable counter example where isn't continuous? Any help is appreciated.","f:\mathbb{R}\to\mathbb{R} (x_n)_n (y_n)_n 0 x_n < y_n n\in\mathbb{N} \lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f'(0).  f x_n < y_n c_n \in (x_n,y_n)  \frac{f(y_n)-f(x_n)}{y_n-x_n} = f'(c_n).  x_n y_n 0 c_n 0 n \lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = \lim_{n\to\infty}f'(c_n) = f'(0).  f' f' f'","['real-analysis', 'limits', 'derivatives', 'continuity']"
37,Does the limit $\lim _{x \rightarrow 0} \frac{\int_{0}^{x^{2}} \sin \sqrt{t} d t}{x^{3}}$ exists?,Does the limit  exists?,\lim _{x \rightarrow 0} \frac{\int_{0}^{x^{2}} \sin \sqrt{t} d t}{x^{3}},"Evaluate $$L=\lim _{x \rightarrow 0} \frac{\int_{0}^{x^{2}} \sin \sqrt{t} d t}{x^{3}}$$ Since the limit is $\frac{0}{0}$ form, Using L'Hopital's rule and Leibniz rule we get $$L=\lim_{x \to 0}\frac{2x\sin |x|}{3x^2}$$ $\implies$ $$L=\frac{2}{3}\lim_{x \to 0}\frac{\sin|x|}{x}$$ Now we know that limit exists only when right and left hand limits are finite and equal. $$L^+=\lim_{h \to 0^+}\frac{\sin h}{h}=1$$ $$L^-=\lim_{h \to 0^+}\frac{\sin|-h|}{-h}=-1$$ Since $$L^+ \ne L^-$$ we can say limit Does not exists. But book answer is $\frac{2}{3}$","Evaluate Since the limit is form, Using L'Hopital's rule and Leibniz rule we get Now we know that limit exists only when right and left hand limits are finite and equal. Since we can say limit Does not exists. But book answer is",L=\lim _{x \rightarrow 0} \frac{\int_{0}^{x^{2}} \sin \sqrt{t} d t}{x^{3}} \frac{0}{0} L=\lim_{x \to 0}\frac{2x\sin |x|}{3x^2} \implies L=\frac{2}{3}\lim_{x \to 0}\frac{\sin|x|}{x} L^+=\lim_{h \to 0^+}\frac{\sin h}{h}=1 L^-=\lim_{h \to 0^+}\frac{\sin|-h|}{-h}=-1 L^+ \ne L^- \frac{2}{3},"['limits', 'derivatives', 'indeterminate-forms', 'leibniz-integral-rule']"
38,"Copeland-Erdős constant: does the cumulative even digit count ever reach or overtake the cumulative odd digit count, and if so, at what prime?","Copeland-Erdős constant: does the cumulative even digit count ever reach or overtake the cumulative odd digit count, and if so, at what prime?",,"Arthur Herbert Copeland and Paul Erdős proved in 1946 that the Copeland-Erdős constant is a normal number . Since all prime numbers other than 2 are odd, all prime numbers other than 2 end in an odd digit, so one might expect skew of the digit distribution toward odds, since each prime number other than 2 is guaranteed at least 1 odd digit, while there is no such at-least-1-digit guarantee for even digits. So for the constant to be normal, it must be that as the prime numbers go toward infinity, prime numbers become so long digit-wise that the oddness of the last digit becomes negligible. Now, looking at the first few digits (0.235711131719232931374143...), it is obvious that odd digits far outnumber even digits within the early digits. But since the constant is normal, the evens must ""catch up"" eventually: either... (a) ...the evens asymptomatically approach from below a 50% distribution of all digits, or... (b) ...(what seems to me much more likely) which parity of digits is ahead changes infinitely often, though it might take a long time and a very large prime for the evens to first catch up (reminiscent of the very large Skewes's numbers and related numbers wherein π(x) finally catches up to li(x) for the first time), or... (c) ...(what seems to me to be unlikely) a combination of the above two cases so that after a finite number of switches of the lead, one parity stays ahead forever while the other stays asymptomatically close. Does anyone know if there is a proof of which of the three cases is true? If, as I suspect, case (b) is true, what is the smallest prime at which the cumulative even digit count catches up to the odds? Numerical Results Let r(n) be the proportion of even digits after the nth prime. So, since the constant starts out 0.2 3 5 7 11 13 ..., the first few values of r(n) are r(1) = 100%, r(2) = 50%, r(3) = 33.333...%, r(4) = 0.25%, r(5) = 16.666...%, r(6) = 12.5%. Below, when I refer to the ""maximum value"" of r(n), I am disregarding the trivial r(1) and r(2) values. I wrote a script to calculate r(n) up to $n = 7.5 \times 10^7$ (75 million). For reference as to roughly how large these primes are, the 75,000,000th prime is 1,505,776,939. For n ≥ 3, r(n) initially falls before starting to rise, before finally tying r(3) = 1/3 at r(380), with r(381) = 444 / (444 + 883) ≈ 33.45% being the first value of r(n) to exceed r(3). Beyond r(381), r(n) oscillates (obviously), but on average, it rises much more than it falls and initially grows rapidly on average — but as the primes get larger and larger, its average rate of growth falls. r(n) first hits 34% at r(389), hits 35% at r(416), hits 36% at r(654), hits 37% at r(1,106), hits 38% at r(3,097), hits 39% at r(6,861), hits 40% at r(24,613), hits 41% at r(55,426), hits 42% at r(210,117), hits 43% at r(1,790,106), and hits 44% at r(25,609,981). Anyway, as of the 75th million prime 1,505,776,939, the highest value of r(n) thus far is 44.2537565841856...% at the 46,450,161st prime, 909,090,109. I still do not know if r(n) does ever hit 50%.","Arthur Herbert Copeland and Paul Erdős proved in 1946 that the Copeland-Erdős constant is a normal number . Since all prime numbers other than 2 are odd, all prime numbers other than 2 end in an odd digit, so one might expect skew of the digit distribution toward odds, since each prime number other than 2 is guaranteed at least 1 odd digit, while there is no such at-least-1-digit guarantee for even digits. So for the constant to be normal, it must be that as the prime numbers go toward infinity, prime numbers become so long digit-wise that the oddness of the last digit becomes negligible. Now, looking at the first few digits (0.235711131719232931374143...), it is obvious that odd digits far outnumber even digits within the early digits. But since the constant is normal, the evens must ""catch up"" eventually: either... (a) ...the evens asymptomatically approach from below a 50% distribution of all digits, or... (b) ...(what seems to me much more likely) which parity of digits is ahead changes infinitely often, though it might take a long time and a very large prime for the evens to first catch up (reminiscent of the very large Skewes's numbers and related numbers wherein π(x) finally catches up to li(x) for the first time), or... (c) ...(what seems to me to be unlikely) a combination of the above two cases so that after a finite number of switches of the lead, one parity stays ahead forever while the other stays asymptomatically close. Does anyone know if there is a proof of which of the three cases is true? If, as I suspect, case (b) is true, what is the smallest prime at which the cumulative even digit count catches up to the odds? Numerical Results Let r(n) be the proportion of even digits after the nth prime. So, since the constant starts out 0.2 3 5 7 11 13 ..., the first few values of r(n) are r(1) = 100%, r(2) = 50%, r(3) = 33.333...%, r(4) = 0.25%, r(5) = 16.666...%, r(6) = 12.5%. Below, when I refer to the ""maximum value"" of r(n), I am disregarding the trivial r(1) and r(2) values. I wrote a script to calculate r(n) up to (75 million). For reference as to roughly how large these primes are, the 75,000,000th prime is 1,505,776,939. For n ≥ 3, r(n) initially falls before starting to rise, before finally tying r(3) = 1/3 at r(380), with r(381) = 444 / (444 + 883) ≈ 33.45% being the first value of r(n) to exceed r(3). Beyond r(381), r(n) oscillates (obviously), but on average, it rises much more than it falls and initially grows rapidly on average — but as the primes get larger and larger, its average rate of growth falls. r(n) first hits 34% at r(389), hits 35% at r(416), hits 36% at r(654), hits 37% at r(1,106), hits 38% at r(3,097), hits 39% at r(6,861), hits 40% at r(24,613), hits 41% at r(55,426), hits 42% at r(210,117), hits 43% at r(1,790,106), and hits 44% at r(25,609,981). Anyway, as of the 75th million prime 1,505,776,939, the highest value of r(n) thus far is 44.2537565841856...% at the 46,450,161st prime, 909,090,109. I still do not know if r(n) does ever hit 50%.",n = 7.5 \times 10^7,"['number-theory', 'limits', 'prime-numbers', 'analytic-number-theory', 'irrational-numbers']"
39,Limit of an integral in $L^p$,Limit of an integral in,L^p,"I am dealing with the following limit: Let $\lambda$ be the Lebesgue measure on $\mathbb{R}^n$ , let $f\in L^p(\lambda)$ for $p\in[0,\infty)$ . Compute $$\lim_{h\to 0}\int_{\mathbb{R}^n} |f(x+h) - f(x)|^p\ d\lambda(x)$$ and use this fact to show that for any $A$ measurable set with $\lambda(A)>0$ , the set $A-A:=\{x-y\ | x,y\in A\}$ contains open ball centered at 0. As for the first part, I was trying to apply dominated convergence to put the limit inside, but I do not know how to bound the function $|f(x+h) - f(x)|^p$ properly. The second part is also confusing. I thought that the indicator function of $A$ might be of help but I am not sure. Thanks!","I am dealing with the following limit: Let be the Lebesgue measure on , let for . Compute and use this fact to show that for any measurable set with , the set contains open ball centered at 0. As for the first part, I was trying to apply dominated convergence to put the limit inside, but I do not know how to bound the function properly. The second part is also confusing. I thought that the indicator function of might be of help but I am not sure. Thanks!","\lambda \mathbb{R}^n f\in L^p(\lambda) p\in[0,\infty) \lim_{h\to 0}\int_{\mathbb{R}^n} |f(x+h) - f(x)|^p\ d\lambda(x) A \lambda(A)>0 A-A:=\{x-y\ | x,y\in A\} |f(x+h) - f(x)|^p A","['limits', 'lebesgue-integral', 'lebesgue-measure', 'lp-spaces']"
40,An analysis problem: limit for every $h$ implies the existence of a 'uniform' limit,An analysis problem: limit for every  implies the existence of a 'uniform' limit,h,"Suppose that $f : \mathbb{R}^{+} \to \mathbb{R}$ is a continuous, and for any $h>0$ , $\lim\limits_{x \to \infty } (f(x+h)-f(x)) = 0$ . Show that for any given interval $[a,b], a>0$ and $\epsilon > 0$ , there exists $M>0$ such that for any $x > M$ and $h \in [a,b]$ we have $\left| f(x+h) - f(x) \right| < \epsilon$ . I have tried this problem but find that the main obstacle is that the condition is hard to use, and it's almost impossible to pass from a 'pointwise' condition to a 'uniform' conclusion. Someone tells me that the proof requires measure theory, but I cannot see the relation. Can anyone give some ideas ?","Suppose that is a continuous, and for any , . Show that for any given interval and , there exists such that for any and we have . I have tried this problem but find that the main obstacle is that the condition is hard to use, and it's almost impossible to pass from a 'pointwise' condition to a 'uniform' conclusion. Someone tells me that the proof requires measure theory, but I cannot see the relation. Can anyone give some ideas ?","f : \mathbb{R}^{+} \to \mathbb{R} h>0 \lim\limits_{x \to \infty } (f(x+h)-f(x)) = 0 [a,b], a>0 \epsilon > 0 M>0 x > M h \in [a,b] \left| f(x+h) - f(x) \right| < \epsilon","['real-analysis', 'limits', 'analysis', 'continuity']"
41,"Sequence and series : $a_{n+1}=\frac{na_{n}+1}{a_{n}}$ , $a_0=1$","Sequence and series :  ,",a_{n+1}=\frac{na_{n}+1}{a_{n}} a_0=1,"$a_{n}$ sequence defined as : $a_{n+1}=\dfrac{na_{n}+1}{a_{n}}$ , $a_0=1$ Then evaluate : $\lim_{n\to\infty}n(n-a_{n})$ My attempt : Call $\lim_{n\to\infty}a_{n}=L$ then I will use stolze Cesaro limit theorem $\lim_{n\to\infty}n(n-a_{n})=\lim_{n\to\infty}\frac{n+1-a_{n+1}-n+a_{n}}{\frac{1}{n+1}-\frac{1}{n}}$ From here how can I complete ?","sequence defined as : , Then evaluate : My attempt : Call then I will use stolze Cesaro limit theorem From here how can I complete ?",a_{n} a_{n+1}=\dfrac{na_{n}+1}{a_{n}} a_0=1 \lim_{n\to\infty}n(n-a_{n}) \lim_{n\to\infty}a_{n}=L \lim_{n\to\infty}n(n-a_{n})=\lim_{n\to\infty}\frac{n+1-a_{n+1}-n+a_{n}}{\frac{1}{n+1}-\frac{1}{n}},['limits']
42,"$\lim_{n\to\infty}\frac{n^{3/2}}{\ln n}\int_0^1\frac{\sqrt x\ln x}{(1+2x-x^2)^n}dx$, an attempt to generalize Watson's lemma",", an attempt to generalize Watson's lemma",\lim_{n\to\infty}\frac{n^{3/2}}{\ln n}\int_0^1\frac{\sqrt x\ln x}{(1+2x-x^2)^n}dx,"Background and Motivation Given that Watson's lemma , a theorem that can compute the asymptotic expansion of $$\int_0^N p(x)e^{-zq(x)}dx\text{ as } z\to\infty,$$ with the elementary restriction of $p,q$ being $\color{red}{C^\infty}[0,\varepsilon)$ for some $\varepsilon>0$ (and some other restrictions), I want to generalize this theorem, to some extent. Question I've already evaluated the limit $$\lim_{n\to\infty}n^{3/2}\int_0^1\frac{\sqrt x}{(1+2x-x^2)^n}dx=\frac{\sqrt{2\pi}}8,$$ and I can even use Watson's lemma to calculate the asymptotic expansion to $O(n^\alpha)$ with arbitrary big $\alpha$ . i) Given $\mu>0$ , can we evaluate $$\lim_{n\to\infty}\frac{n^{3/2}}{\ln^\mu n}\int_0^1\frac{\sqrt x\ln^\mu (1/x)}{(1+2x-x^2)^n}dx?$$ Furthermore, is there a method to find all terms of the asymptotic expansion of the integral? This integral is merely an example of the generalization of the theorem. ii) Reference request: Given $p(x)\in C^{\color{red}\omega}(0,\varepsilon)\cap C^{\color{red}\omega}(a-\varepsilon,a)\cap R[0,a]$ , is there a method to find the full asymptotic expansion of $$\int_0^a p(x)e^{zx}dx$$ as $z\to\infty$ ? Attempt of (i) Fix $\varepsilon>0$ , noting that $\displaystyle\int_\varepsilon^1\frac{\sqrt x\ln ^\mu(1/x)}{(1+2x-x^2)^n}=O(n^{-N})$ for all $N\in\mathbb R$ , this term can be neglected. There is a very classical method for this type of integrals which is substituting $\sqrt xdx=dt$ , then obtaining $$\int_0^{\varepsilon'}\frac{\ln^\mu (1/x)}{(1+2x^{2/3}-x^{4/3})^n}dx$$ and finally neglect the higher order term. But this method can only be used when $\mu=0$ and can only get the rough order of it.","Background and Motivation Given that Watson's lemma , a theorem that can compute the asymptotic expansion of with the elementary restriction of being for some (and some other restrictions), I want to generalize this theorem, to some extent. Question I've already evaluated the limit and I can even use Watson's lemma to calculate the asymptotic expansion to with arbitrary big . i) Given , can we evaluate Furthermore, is there a method to find all terms of the asymptotic expansion of the integral? This integral is merely an example of the generalization of the theorem. ii) Reference request: Given , is there a method to find the full asymptotic expansion of as ? Attempt of (i) Fix , noting that for all , this term can be neglected. There is a very classical method for this type of integrals which is substituting , then obtaining and finally neglect the higher order term. But this method can only be used when and can only get the rough order of it.","\int_0^N p(x)e^{-zq(x)}dx\text{ as } z\to\infty, p,q \color{red}{C^\infty}[0,\varepsilon) \varepsilon>0 \lim_{n\to\infty}n^{3/2}\int_0^1\frac{\sqrt x}{(1+2x-x^2)^n}dx=\frac{\sqrt{2\pi}}8, O(n^\alpha) \alpha \mu>0 \lim_{n\to\infty}\frac{n^{3/2}}{\ln^\mu n}\int_0^1\frac{\sqrt x\ln^\mu (1/x)}{(1+2x-x^2)^n}dx? p(x)\in C^{\color{red}\omega}(0,\varepsilon)\cap C^{\color{red}\omega}(a-\varepsilon,a)\cap R[0,a] \int_0^a p(x)e^{zx}dx z\to\infty \varepsilon>0 \displaystyle\int_\varepsilon^1\frac{\sqrt x\ln ^\mu(1/x)}{(1+2x-x^2)^n}=O(n^{-N}) N\in\mathbb R \sqrt xdx=dt \int_0^{\varepsilon'}\frac{\ln^\mu (1/x)}{(1+2x^{2/3}-x^{4/3})^n}dx \mu=0","['calculus', 'limits', 'reference-request', 'definite-integrals', 'asymptotics']"
43,"Calculate $\lim_{x\to 0} \frac{1}{x^2}\int^{1-\cos x}_{\sqrt[3]{ x^7}} \ln (2+\tan^2 t)\,dt$",Calculate,"\lim_{x\to 0} \frac{1}{x^2}\int^{1-\cos x}_{\sqrt[3]{ x^7}} \ln (2+\tan^2 t)\,dt","Calculate $$\lim _{x\rightarrow 0}\frac{1}{x^2} \int\limits^{1-\cos x}_{\sqrt[3]{ x^7}} \ln (2+\tan^2 t)\,dt$$ My solution: Let: $$f(x)=\int^{1-\cos x} _{\sqrt[3]{ x^7}} \ln (2+\tan^2 t)\,dt$$ $$g(x)=x^2$$ For $x\rightarrow 0$ we have $f,g \rightarrow 0$ . So from L'Hôpital's rule: $$\lim_{x\rightarrow 0} \frac{f(x)}{g(x)}=\lim_{x\rightarrow 0} \frac{f'(x)}{g'(x)}=\lim_{x\rightarrow 0} \frac{\ln (2+ \tan ^{2} (1-\cos x)) \cdot \sin x -\ln (2+\tan^{2} (x^{\frac{7}{3}})) \cdot \frac{7}{3} \cdot x^{\frac{4}{3}})}{2x}=\ln 2$$ And then I have a question: how do I prove that I can use L'Hôpital's rule? I think that I should say that for $x\rightarrow 0$ for $f$ I have an integral from $0$ to $0$ so $f(x)=0$ . But I need a professional explanation so I am asking for a detailed justification of why I can use this rule.",Calculate My solution: Let: For we have . So from L'Hôpital's rule: And then I have a question: how do I prove that I can use L'Hôpital's rule? I think that I should say that for for I have an integral from to so . But I need a professional explanation so I am asking for a detailed justification of why I can use this rule.,"\lim _{x\rightarrow 0}\frac{1}{x^2} \int\limits^{1-\cos x}_{\sqrt[3]{ x^7}} \ln (2+\tan^2 t)\,dt f(x)=\int^{1-\cos x} _{\sqrt[3]{ x^7}} \ln (2+\tan^2 t)\,dt g(x)=x^2 x\rightarrow 0 f,g \rightarrow 0 \lim_{x\rightarrow 0} \frac{f(x)}{g(x)}=\lim_{x\rightarrow 0} \frac{f'(x)}{g'(x)}=\lim_{x\rightarrow 0} \frac{\ln (2+ \tan ^{2} (1-\cos x)) \cdot \sin x -\ln (2+\tan^{2} (x^{\frac{7}{3}})) \cdot \frac{7}{3} \cdot x^{\frac{4}{3}})}{2x}=\ln 2 x\rightarrow 0 f 0 0 f(x)=0","['real-analysis', 'limits']"
44,"Evaluate $\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } $",Evaluate,"\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } ","This was the question of a test. My question is if my attempt to solve it is correct, and if it is, why is it correct. $$\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } $$ My attempt: Let $\xi = -x-y $ . Then $\xi \to 0$ whenever $(x,y) \to (0,0)$ and $x+y \neq 0 \iff \xi \neq 0$ . (Is it then correct to say that the previous limit exists and is equal to the following iff the following exists? And why?): $$\lim_{\xi \to 0, \xi \neq 0}{\frac{\ln(1+\xi )}{-\xi}}$$ If it is correct, then the limit exists and is $-1$ . If it is correct, why is it correct?","This was the question of a test. My question is if my attempt to solve it is correct, and if it is, why is it correct. My attempt: Let . Then whenever and . (Is it then correct to say that the previous limit exists and is equal to the following iff the following exists? And why?): If it is correct, then the limit exists and is . If it is correct, why is it correct?","\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} }  \xi = -x-y  \xi \to 0 (x,y) \to (0,0) x+y \neq 0 \iff \xi \neq 0 \lim_{\xi \to 0, \xi \neq 0}{\frac{\ln(1+\xi )}{-\xi}} -1","['limits', 'multivariable-calculus', 'proof-verification']"
45,Do the roots of $1+x/1!+x^2/2!+\cdots+x^{2n+1}/{(2n+1)!}$ decrease to $-\infty$? [duplicate],Do the roots of  decrease to ? [duplicate],1+x/1!+x^2/2!+\cdots+x^{2n+1}/{(2n+1)!} -\infty,"This question already has answers here : Complex zeros of the polynomials $\sum_{k=0}^{n} z^k/k!$, inside balls (3 answers) Closed 5 years ago . Do the roots of $$P_{2n+1}(x)=1+\frac x{1!}+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!}$$ decrease to $-\infty$ ? Can we show this? Indeed, $P_{2n+1}(0)=1$ , $P_{2n+1}(-(2n+1))<0$ . And $P_{2n+1}'(x)=P_{2n}(x)>0$ . So $P_{2n+1}$ has only one root $x_n$ . Can we show that $x_n\to-\infty$ ?","This question already has answers here : Complex zeros of the polynomials $\sum_{k=0}^{n} z^k/k!$, inside balls (3 answers) Closed 5 years ago . Do the roots of decrease to ? Can we show this? Indeed, , . And . So has only one root . Can we show that ?",P_{2n+1}(x)=1+\frac x{1!}+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!} -\infty P_{2n+1}(0)=1 P_{2n+1}(-(2n+1))<0 P_{2n+1}'(x)=P_{2n}(x)>0 P_{2n+1} x_n x_n\to-\infty,"['calculus', 'limits']"
46,"Why $\sum_{n=1}^{\infty}\frac{n\left(\sin x\right)^{n}}{2+n^{2}}$ is not uniformly convergent on $[0,\;\frac{\pi}{2})$?",Why  is not uniformly convergent on ?,"\sum_{n=1}^{\infty}\frac{n\left(\sin x\right)^{n}}{2+n^{2}} [0,\;\frac{\pi}{2})","Why $\sum_{n=1}^{\infty}\frac{n\left(\sin x\right)^{n}}{2+n^{2}}$ is not uniformly convergent on $[0,\;\frac{\pi}{2})$? I was thinking that we need to show partial sums \begin{equation} \left|S_{2n}\left(x\right)-S_{n}\left(x\right)\right|\nrightarrow0 \end{equation} for some $x\in[0,\;\frac{\pi}{2})$. Any hint? Is this correct? \begin{align} \left|S_{2n}\left(x\right)-S_{n}\left(x\right)\right| & =\left|\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(n+1\right)^{2}}+\cdots+\frac{2n\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\right|\\  & =\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(n+1\right)^{2}}+\cdots+\frac{2n\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \geq\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(2n\right)^{2}}+\cdots+\frac{2n\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \geq\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(2n\right)^{2}}+\cdots+\frac{\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \geq\frac{\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}+\cdots+\frac{\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & =\frac{n\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber  \end{align} then let $x=\frac{\pi}{2}-\frac{1}{n}$, so we have  \begin{align} \left|S_{2n}\left(x\right)-S_{n}\left(x\right)\right| & \geq\frac{n\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\\  & =\frac{n\left(n+1\right)\left(\sin\left(\frac{\pi}{2}-\frac{1}{n}\right)\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \rightarrow0.25,\:not\:0\nonumber  \end{align}","Why $\sum_{n=1}^{\infty}\frac{n\left(\sin x\right)^{n}}{2+n^{2}}$ is not uniformly convergent on $[0,\;\frac{\pi}{2})$? I was thinking that we need to show partial sums \begin{equation} \left|S_{2n}\left(x\right)-S_{n}\left(x\right)\right|\nrightarrow0 \end{equation} for some $x\in[0,\;\frac{\pi}{2})$. Any hint? Is this correct? \begin{align} \left|S_{2n}\left(x\right)-S_{n}\left(x\right)\right| & =\left|\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(n+1\right)^{2}}+\cdots+\frac{2n\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\right|\\  & =\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(n+1\right)^{2}}+\cdots+\frac{2n\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \geq\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(2n\right)^{2}}+\cdots+\frac{2n\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \geq\frac{\left(n+1\right)\left(\sin x\right)^{n+1}}{2+\left(2n\right)^{2}}+\cdots+\frac{\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \geq\frac{\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}+\cdots+\frac{\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & =\frac{n\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber  \end{align} then let $x=\frac{\pi}{2}-\frac{1}{n}$, so we have  \begin{align} \left|S_{2n}\left(x\right)-S_{n}\left(x\right)\right| & \geq\frac{n\left(n+1\right)\left(\sin x\right)^{2n}}{2+\left(2n\right)^{2}}\\  & =\frac{n\left(n+1\right)\left(\sin\left(\frac{\pi}{2}-\frac{1}{n}\right)\right)^{2n}}{2+\left(2n\right)^{2}}\nonumber \\  & \rightarrow0.25,\:not\:0\nonumber  \end{align}",,"['limits', 'convergence-divergence', 'uniform-convergence']"
47,Is there any function whose limit at $x_0$ is unknown?,Is there any function whose limit at  is unknown?,x_0,"I would like to know if there is any non trivial function $f(x)$ and a $x_0$ such that $$\lim_{x\to\ x_0} f(x)$$ is currently not known, with $x_0 \in \mathbb{R}\cup \{-\infty, +\infty \}$. An example of a ""trivial"" function is $A(x)$ where $A(x)$ denotes the number of perfect numbers not greater than $x$. It is an open problem to find the value of  $\lim_{x\to\infty} A(x)$, since we don't know if there are infinitely many perfect numbers. I would prefer a limit which can be recognized by a high school student.","I would like to know if there is any non trivial function $f(x)$ and a $x_0$ such that $$\lim_{x\to\ x_0} f(x)$$ is currently not known, with $x_0 \in \mathbb{R}\cup \{-\infty, +\infty \}$. An example of a ""trivial"" function is $A(x)$ where $A(x)$ denotes the number of perfect numbers not greater than $x$. It is an open problem to find the value of  $\lim_{x\to\infty} A(x)$, since we don't know if there are infinitely many perfect numbers. I would prefer a limit which can be recognized by a high school student.",,"['real-analysis', 'limits', 'open-problem']"
48,Limit of $e^{n^{3/4}} ((1- c/n^{1/4})^{n^{1/4}})^{n^{3/4}/c}$,Limit of,e^{n^{3/4}} ((1- c/n^{1/4})^{n^{1/4}})^{n^{3/4}/c},"Let $c\ne0$ be a constant. Consider the limit of $$f(n)=e^{n^{3/4}} ((1- c/n^{1/4})^{n^{1/4}})^{n^{3/4}/c}$$ as $n \to \infty$. I think it is zero because for large $n$, $$e^{n^{3/4}} ((1- \frac{c}{n^{1/4}})^{n^{1/4}})^{\frac{n^{3/4}}{c}} \approx e^{n^{3/4}} (e^{-c})^{ \frac{n^{3/4}}{c}}$$ But, how do I prove it formally?","Let $c\ne0$ be a constant. Consider the limit of $$f(n)=e^{n^{3/4}} ((1- c/n^{1/4})^{n^{1/4}})^{n^{3/4}/c}$$ as $n \to \infty$. I think it is zero because for large $n$, $$e^{n^{3/4}} ((1- \frac{c}{n^{1/4}})^{n^{1/4}})^{\frac{n^{3/4}}{c}} \approx e^{n^{3/4}} (e^{-c})^{ \frac{n^{3/4}}{c}}$$ But, how do I prove it formally?",,"['real-analysis', 'limits', 'exponential-function']"
49,What is the limit of the following function? [closed],What is the limit of the following function? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question $\lim_{n\rightarrow \infty }\left ( n-1-2\left (\frac{\Gamma(n/2)}{\Gamma((n-1)/2)} \right )^2 \right )$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question $\lim_{n\rightarrow \infty }\left ( n-1-2\left (\frac{\Gamma(n/2)}{\Gamma((n-1)/2)} \right )^2 \right )$",,"['calculus', 'limits', 'limits-without-lhopital']"
50,How can I prove this equality and general derivative?,How can I prove this equality and general derivative?,,"$$ f_{n}(x) = \underbrace{\sqrt{x+\sqrt{x+ ...+ \sqrt{x}}}}_{n}  \quad \quad \lim_{n\to\infty} \sum^{n}_{r=1}\frac{1}{2^{r}}\prod^{r-1}_{i=0}\frac{1}{f_{n-i}(x)} = \frac{1}{2f_{\infty}(x)-1} $$ I produced the left hand side by examining the pattern of the derivatives of $\, f_{n}(x)$ and the right hand side by using the property of $\, f_{\infty}(x)$ that: $\, f_{\infty}(x)=\sqrt{x + f_{\infty}(x)}$ and differentiating. I have no idea how to prove such a thing, nor do I know how to prove the general formula of the derivate (EDIT: I have now proved the general formula of the derivative but am still stumped by the limit problem!): $$_{m}f_{n}(x) = \underbrace{\sqrt[m]{x+\sqrt[m]{x+ ... + \sqrt[m]{x}}}}_{n}  \quad \quad _{m}f_{n}'(x) = \sum^{n}_{r=1}\frac{1}{m^{r}} \prod^{r-1}_{i=0} (f_{n-i}(x))^{1-m}$$ Obviously given my second formula,  I could generalise the first equality but, I thought the case $m=2$ came out quite nicely. One last thing, is there any nicer notation I can use for the function rather than writing it out with the underbrace and whatnot? Edit: Just realised I didn't mention it at all in the original post, the right-hand side is the infinite sum of a geometric progression: $$\sum^{\infty}_{r=1}\bigg(\frac{1}{2f_{\infty}(x)}\bigg)^{r} = \frac{1}{2f_{\infty}(x)-1} $$ So the question can be rephrased as proving: $$\lim_{n\to\infty} \sum^{n}_{r=1}\frac{1}{2^{r}}\prod^{r-1}_{i=0}\frac{1}{f_{n-i}(x)} = \sum^{\infty}_{r=1}\bigg(\frac{1}{2f_{\infty}(x)}\bigg)^{r}$$ Which seems intuitively true but, I do not know how to prove it rigorously.","$$ f_{n}(x) = \underbrace{\sqrt{x+\sqrt{x+ ...+ \sqrt{x}}}}_{n}  \quad \quad \lim_{n\to\infty} \sum^{n}_{r=1}\frac{1}{2^{r}}\prod^{r-1}_{i=0}\frac{1}{f_{n-i}(x)} = \frac{1}{2f_{\infty}(x)-1} $$ I produced the left hand side by examining the pattern of the derivatives of $\, f_{n}(x)$ and the right hand side by using the property of $\, f_{\infty}(x)$ that: $\, f_{\infty}(x)=\sqrt{x + f_{\infty}(x)}$ and differentiating. I have no idea how to prove such a thing, nor do I know how to prove the general formula of the derivate (EDIT: I have now proved the general formula of the derivative but am still stumped by the limit problem!): $$_{m}f_{n}(x) = \underbrace{\sqrt[m]{x+\sqrt[m]{x+ ... + \sqrt[m]{x}}}}_{n}  \quad \quad _{m}f_{n}'(x) = \sum^{n}_{r=1}\frac{1}{m^{r}} \prod^{r-1}_{i=0} (f_{n-i}(x))^{1-m}$$ Obviously given my second formula,  I could generalise the first equality but, I thought the case $m=2$ came out quite nicely. One last thing, is there any nicer notation I can use for the function rather than writing it out with the underbrace and whatnot? Edit: Just realised I didn't mention it at all in the original post, the right-hand side is the infinite sum of a geometric progression: $$\sum^{\infty}_{r=1}\bigg(\frac{1}{2f_{\infty}(x)}\bigg)^{r} = \frac{1}{2f_{\infty}(x)-1} $$ So the question can be rephrased as proving: $$\lim_{n\to\infty} \sum^{n}_{r=1}\frac{1}{2^{r}}\prod^{r-1}_{i=0}\frac{1}{f_{n-i}(x)} = \sum^{\infty}_{r=1}\bigg(\frac{1}{2f_{\infty}(x)}\bigg)^{r}$$ Which seems intuitively true but, I do not know how to prove it rigorously.",,"['calculus', 'limits', 'functions', 'infinity', 'nested-radicals']"
51,Problem with limit and increasing function,Problem with limit and increasing function,,"Let $f:(0,\infty) \to (0,\infty)$ be an increasing function such that $$\lim_{x \to \infty}f(x)=\infty \text { and } \lim_{x \to \infty} \frac {f(x+f(x))}{f(x)}=1.$$ Show that $$\lim_{x \to \infty} \frac {f(x)}{x}=0 \text { and } \lim_{x \to \infty} \frac {f(x+af(x))}{f(x)}=1,$$ for every $a \gt 0$.","Let $f:(0,\infty) \to (0,\infty)$ be an increasing function such that $$\lim_{x \to \infty}f(x)=\infty \text { and } \lim_{x \to \infty} \frac {f(x+f(x))}{f(x)}=1.$$ Show that $$\lim_{x \to \infty} \frac {f(x)}{x}=0 \text { and } \lim_{x \to \infty} \frac {f(x+af(x))}{f(x)}=1,$$ for every $a \gt 0$.",,['limits']
52,Compute $\lim\limits_{x\to 0_+}e^{\frac{1}{\sqrt{x}}}\cdot(1-\sqrt{x})^\frac{1}{x}$,Compute,\lim\limits_{x\to 0_+}e^{\frac{1}{\sqrt{x}}}\cdot(1-\sqrt{x})^\frac{1}{x},"Evaluate $$\lim_{x\to 0_+}e^{\frac{1}{\sqrt{x}}}\cdot(1-\sqrt{x})^\frac{1}{x}$$ I tried using $$\lim_{x\to 0}(1+x)^\frac{1}{x} = e$$ like so: $$l = \lim_{x\to 0_+}e^\frac{1}{\sqrt{x}}\cdot\bigg[\big(1+(-\sqrt{x})\big)^{-\frac{1}{\sqrt{x}}}\bigg]^{\frac{-1}{\sqrt{x}}} = \lim_{x\to 0_+}e^{\frac{1}{\sqrt{x}}-\frac{1}{\sqrt{x}}} = e^0 = 1$$ However, the right answer is $\frac{1}{\sqrt e}$. Why is it that the whole expression in square brackets can't be taken as $e$ in this case?","Evaluate $$\lim_{x\to 0_+}e^{\frac{1}{\sqrt{x}}}\cdot(1-\sqrt{x})^\frac{1}{x}$$ I tried using $$\lim_{x\to 0}(1+x)^\frac{1}{x} = e$$ like so: $$l = \lim_{x\to 0_+}e^\frac{1}{\sqrt{x}}\cdot\bigg[\big(1+(-\sqrt{x})\big)^{-\frac{1}{\sqrt{x}}}\bigg]^{\frac{-1}{\sqrt{x}}} = \lim_{x\to 0_+}e^{\frac{1}{\sqrt{x}}-\frac{1}{\sqrt{x}}} = e^0 = 1$$ However, the right answer is $\frac{1}{\sqrt e}$. Why is it that the whole expression in square brackets can't be taken as $e$ in this case?",,"['real-analysis', 'limits']"
53,Epsilon-Delta: Prove $\frac{1}{x} \rightarrow 7$ as $x \rightarrow \frac{1}{7}$,Epsilon-Delta: Prove  as,\frac{1}{x} \rightarrow 7 x \rightarrow \frac{1}{7},"Prove that $\displaystyle\frac{1}{x} \rightarrow 7$ as $\displaystyle x \rightarrow \frac{1}{7}$. I need to show this with an $\epsilon-\delta$ argument. Still figuring these types of proofs out though, so I could use some tips/critiques of my proof, if it is correct at all. It might not be so clear, but I use the fact that $\displaystyle\left|x - \frac{1}{7}\right| < \delta$ several times in the proof. For $\varepsilon > 0$, let $\displaystyle\delta = \min\left\{\frac{1}{14}, \frac{\varepsilon}{98}\right\}$. Then $\displaystyle \left|x - \frac{1}{7}\right| < \delta$ implies: $$\left|\frac{1}{7}\right| = \left|\left(-x + \frac{1}{7}\right) + x\right| \leq \left|x - \frac{1}{7}\right| + \left|x\right| < \frac{1}{14} + |x|,$$ and so $\displaystyle |x| > \frac{1}{14}$. Also, $\displaystyle \left|x - \frac{1}{7}\right| < \delta$ implies: $$\left|\frac{1}{x} - 7\right| = \left|\frac{1-7x}{x}\right| = 7\frac{\left|x - \frac{1}{7}\right|}{|x|} < 98\left|x - \frac{1}{7}\right| < \frac{98\varepsilon}{98} = \varepsilon.$$ Thus for $\varepsilon > 0$, $\displaystyle\left|\frac{1}{x} - 7\right| < \varepsilon$ if $\displaystyle\left|x - \frac{1}{7}\right| < \delta$, for  $\displaystyle \delta = \min\left\{\frac{1}{14}, \frac{\varepsilon}{98}\right\}$.","Prove that $\displaystyle\frac{1}{x} \rightarrow 7$ as $\displaystyle x \rightarrow \frac{1}{7}$. I need to show this with an $\epsilon-\delta$ argument. Still figuring these types of proofs out though, so I could use some tips/critiques of my proof, if it is correct at all. It might not be so clear, but I use the fact that $\displaystyle\left|x - \frac{1}{7}\right| < \delta$ several times in the proof. For $\varepsilon > 0$, let $\displaystyle\delta = \min\left\{\frac{1}{14}, \frac{\varepsilon}{98}\right\}$. Then $\displaystyle \left|x - \frac{1}{7}\right| < \delta$ implies: $$\left|\frac{1}{7}\right| = \left|\left(-x + \frac{1}{7}\right) + x\right| \leq \left|x - \frac{1}{7}\right| + \left|x\right| < \frac{1}{14} + |x|,$$ and so $\displaystyle |x| > \frac{1}{14}$. Also, $\displaystyle \left|x - \frac{1}{7}\right| < \delta$ implies: $$\left|\frac{1}{x} - 7\right| = \left|\frac{1-7x}{x}\right| = 7\frac{\left|x - \frac{1}{7}\right|}{|x|} < 98\left|x - \frac{1}{7}\right| < \frac{98\varepsilon}{98} = \varepsilon.$$ Thus for $\varepsilon > 0$, $\displaystyle\left|\frac{1}{x} - 7\right| < \varepsilon$ if $\displaystyle\left|x - \frac{1}{7}\right| < \delta$, for  $\displaystyle \delta = \min\left\{\frac{1}{14}, \frac{\varepsilon}{98}\right\}$.",,"['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
54,"Prove that $\int_0^{\infty} \frac{\sin x}{x^p}\, dx$ converges for $0<p<2$",Prove that  converges for,"\int_0^{\infty} \frac{\sin x}{x^p}\, dx 0<p<2","I would really appreciate feedback/guidance. This is from a past year exam and my professor didn't cover much on improper integrals. I'm trying to prove that $\int_0^{\infty} \frac{\sin x}{x^p}\, dx$ converges for $0<p<2$ using the comparison test for improper integrals. I know that from the test, if $|f(x)| \leq g(x) \forall x\geq a$ then $\int_a^{\infty} g(x)\, dx$ converges. EDIT: Here's my second attempt at the proof after much discussion in the comments below. We know that $\frac{\sin x}{x^p}$ will be continuous on 1 because $\lim_{x\rightarrow 1} \frac{\sin x}{x^p} = lim_{x\rightarrow 1} x^{-p} \sin(x) = 1^{-p}\sin(1) = \sin(1)$. However, for 0 we have that: $\lim_{x \rightarrow 0} \frac{\sin x}{x^p} = \frac{\sin 0} {0^{p}} = \frac{0}{0^{-p}} = \frac{1}{0^{p-1}}$. Since we can't have 0 in the denominator, we only have continuity on 0 if p-1 < 1, which implies p<2. And so, by a theorem, we know that $\frac{\sin(x)}{x^p}$ is Riemann integrable on [0,1] for p<2. Furthermore, by the improper limit comparison test, if we prove that $\int_1^{\infty} f(x)\, dx$, then we can show that $\int_0^{\infty} f(x)\, dx$ converges. Hence we have that $\lim_{b \rightarrow \infty} \int_1^{b} \frac{\sin(x)}{x^p}\,dx = \lim_{b \rightarrow \infty} \int_1^{b} \frac{\sin(x)}{x^p}\,dx =\lim_{b\rightarrow \infty} -\frac{cos x}{{p}} |_p^{b} - \int_1^{b} \frac{\cos x}{(p-1)x^{p+1}}\,dx$ And so I know that since $\int \frac{cos x}{x^{p+1}} < \int \frac{1}{x^{p+1}}$, and since the RHS will converge if and only if p+1 > 1, we have that p>0. And so, we conclude that the integral converges for $0<p<2$.","I would really appreciate feedback/guidance. This is from a past year exam and my professor didn't cover much on improper integrals. I'm trying to prove that $\int_0^{\infty} \frac{\sin x}{x^p}\, dx$ converges for $0<p<2$ using the comparison test for improper integrals. I know that from the test, if $|f(x)| \leq g(x) \forall x\geq a$ then $\int_a^{\infty} g(x)\, dx$ converges. EDIT: Here's my second attempt at the proof after much discussion in the comments below. We know that $\frac{\sin x}{x^p}$ will be continuous on 1 because $\lim_{x\rightarrow 1} \frac{\sin x}{x^p} = lim_{x\rightarrow 1} x^{-p} \sin(x) = 1^{-p}\sin(1) = \sin(1)$. However, for 0 we have that: $\lim_{x \rightarrow 0} \frac{\sin x}{x^p} = \frac{\sin 0} {0^{p}} = \frac{0}{0^{-p}} = \frac{1}{0^{p-1}}$. Since we can't have 0 in the denominator, we only have continuity on 0 if p-1 < 1, which implies p<2. And so, by a theorem, we know that $\frac{\sin(x)}{x^p}$ is Riemann integrable on [0,1] for p<2. Furthermore, by the improper limit comparison test, if we prove that $\int_1^{\infty} f(x)\, dx$, then we can show that $\int_0^{\infty} f(x)\, dx$ converges. Hence we have that $\lim_{b \rightarrow \infty} \int_1^{b} \frac{\sin(x)}{x^p}\,dx = \lim_{b \rightarrow \infty} \int_1^{b} \frac{\sin(x)}{x^p}\,dx =\lim_{b\rightarrow \infty} -\frac{cos x}{{p}} |_p^{b} - \int_1^{b} \frac{\cos x}{(p-1)x^{p+1}}\,dx$ And so I know that since $\int \frac{cos x}{x^{p+1}} < \int \frac{1}{x^{p+1}}$, and since the RHS will converge if and only if p+1 > 1, we have that p>0. And so, we conclude that the integral converges for $0<p<2$.",,"['limits', 'proof-verification', 'convergence-divergence', 'improper-integrals']"
55,Limit of a function with discrete codomain,Limit of a function with discrete codomain,,"Let $f(n) = \sum_{i=0}^n 2^{-i}$ Let $g(n) = \begin{cases} \text{False}, & f(n) < 2\\ \text{True}, & f(n) \geq 2 \end{cases}$ I’m trying to find $\lim_{n \to \infty} g(n)$ Clearly $\lim_{n \to \infty} f(n) =2$ I do not think it is valid to say that because $f(n)=2$ in the limit then $g(n) = \text{True}$ in the limit. For any finite $n$ the value of $g(n)$ is False which leads me to believe that the limit is False but I’m sure about this. Possibly this depends on the domain of $f$. If the domain of $f$ is $\mathbb{N} \cup \{\infty\}$ then is the limit different to the case when the domain of $f$ is  $\mathbb{N}$?","Let $f(n) = \sum_{i=0}^n 2^{-i}$ Let $g(n) = \begin{cases} \text{False}, & f(n) < 2\\ \text{True}, & f(n) \geq 2 \end{cases}$ I’m trying to find $\lim_{n \to \infty} g(n)$ Clearly $\lim_{n \to \infty} f(n) =2$ I do not think it is valid to say that because $f(n)=2$ in the limit then $g(n) = \text{True}$ in the limit. For any finite $n$ the value of $g(n)$ is False which leads me to believe that the limit is False but I’m sure about this. Possibly this depends on the domain of $f$. If the domain of $f$ is $\mathbb{N} \cup \{\infty\}$ then is the limit different to the case when the domain of $f$ is  $\mathbb{N}$?",,"['limits', 'discrete-calculus']"
56,If $ a_n$ is increasingly divisible by $2$ and not a multiple of $10$ then the sum of its digits goes to infinity,If  is increasingly divisible by  and not a multiple of  then the sum of its digits goes to infinity, a_n 2 10,"Let $(a_n)_{n \geq 0}$ be a sequence of positive integers not divisible by 10 such that the number of factors 2 in $a_n$ tends to inﬁnity for $n \to \infty$. Prove that the sum of the digits of an in the decimal system tends to inﬁnity for $n \to \infty$. What I did: Feel free to use any method you like, but it was originately meant to be solved with the 10-adic numbers $\mathbb{Z}_{10}$. Any element $(x_n)_{n \geq 0} \in \mathbb{Z}_{10}$ can be represented as a ""number"" $$ \sum_{n \geq 0} c_n 10^n \quad \text{ for some } c_n \in \{0, 1, \dots, 9 \} \quad \text{ for each integer } n \geq 0.    $$ In this way we can identify $\mathbb{Z}$ with a subset in $\mathbb{Z}_{10}$ that we denote by $D$.  If we equip $\mathbb{Z}/10^n \mathbb{Z}$ with the discrete topology and  $\prod_{n \geq 1}\mathbb{Z}/10^n \mathbb{Z}$ with the corresponding product topology, $\mathbb{Z}_{10}$ and $D$ can be equipped with the induced topologies since  $ D \subseteq \mathbb{Z}_{10} \subseteq \prod_{n \geq 1}\mathbb{Z}/10^n\mathbb{Z} $. I showed that $D$ is dense in $\mathbb{Z}_{10}$. If we define $$ v \ : \ D \ \longrightarrow \ \mathbb{R} \ : \ a \ \longmapsto \frac{1}{\text{number of factors 2 in } a}   $$  This map is continuous on $D$. By density of $D$ it can be extended in a unique way to $\mathbb{Z}_{10}$. Hoewever, I have no idea how to prove the given statement. The problem is that I don't know what the number of decimals actually represents in this framework. Could you help me with that? The question is due to this syllabus , exercise 1.15.","Let $(a_n)_{n \geq 0}$ be a sequence of positive integers not divisible by 10 such that the number of factors 2 in $a_n$ tends to inﬁnity for $n \to \infty$. Prove that the sum of the digits of an in the decimal system tends to inﬁnity for $n \to \infty$. What I did: Feel free to use any method you like, but it was originately meant to be solved with the 10-adic numbers $\mathbb{Z}_{10}$. Any element $(x_n)_{n \geq 0} \in \mathbb{Z}_{10}$ can be represented as a ""number"" $$ \sum_{n \geq 0} c_n 10^n \quad \text{ for some } c_n \in \{0, 1, \dots, 9 \} \quad \text{ for each integer } n \geq 0.    $$ In this way we can identify $\mathbb{Z}$ with a subset in $\mathbb{Z}_{10}$ that we denote by $D$.  If we equip $\mathbb{Z}/10^n \mathbb{Z}$ with the discrete topology and  $\prod_{n \geq 1}\mathbb{Z}/10^n \mathbb{Z}$ with the corresponding product topology, $\mathbb{Z}_{10}$ and $D$ can be equipped with the induced topologies since  $ D \subseteq \mathbb{Z}_{10} \subseteq \prod_{n \geq 1}\mathbb{Z}/10^n\mathbb{Z} $. I showed that $D$ is dense in $\mathbb{Z}_{10}$. If we define $$ v \ : \ D \ \longrightarrow \ \mathbb{R} \ : \ a \ \longmapsto \frac{1}{\text{number of factors 2 in } a}   $$  This map is continuous on $D$. By density of $D$ it can be extended in a unique way to $\mathbb{Z}_{10}$. Hoewever, I have no idea how to prove the given statement. The problem is that I don't know what the number of decimals actually represents in this framework. Could you help me with that? The question is due to this syllabus , exercise 1.15.",,"['number-theory', 'limits', 'divisibility', 'decimal-expansion']"
57,Finding Limit of Nested/Continued Logarithm,Finding Limit of Nested/Continued Logarithm,,"For a sequence $a_n$ defined by: $$a_1 = \ln(1)$$ $$a_2 = \ln\left(\frac{1}{\ln(2)}+1\right)$$ $$\dots a_n = \ln\left(\frac{1}{\ln(\frac{1}{\ln(\dots 1/\ln(n ))}+1)}+1      \right)$$ with $n$ logarithms. Could someone offer a solution/hint for the limit: $$\lim_{n\to\infty}a_n$$ Thank you kindly! Note: My initial attempts involved simplifying to $a_n = ln\left( 1/a_n + 1 \right)$ but I wasn't sure as to where to go from there (not to mention, I noticed this is sort of cheating if we wanted to find the limit purely analytically, since verifying that the limit converges was done through graphing). Another note: As Mc Cheng already pointed out, the limit does converge to some number, but I also noticed that if we were to subsitute the $n$ in the sequence with another variable $x$, then the limit of increasing values of $a_n$ as $x \to \infty$ seem to flip-flop between being an under-approximation and over-approximation (e.g., $\lim_{x\to\infty} a_4 < \lim_{n\to\infty}a_n,$ but $\lim_{x\to\infty} a_5 > \lim_{n\to\infty} a_n$), and the approximations (obviously) get closer to the actual value of $\lim_{n\to\infty} a_n$, so I wonder if the sequence can be turned into an alternating series?","For a sequence $a_n$ defined by: $$a_1 = \ln(1)$$ $$a_2 = \ln\left(\frac{1}{\ln(2)}+1\right)$$ $$\dots a_n = \ln\left(\frac{1}{\ln(\frac{1}{\ln(\dots 1/\ln(n ))}+1)}+1      \right)$$ with $n$ logarithms. Could someone offer a solution/hint for the limit: $$\lim_{n\to\infty}a_n$$ Thank you kindly! Note: My initial attempts involved simplifying to $a_n = ln\left( 1/a_n + 1 \right)$ but I wasn't sure as to where to go from there (not to mention, I noticed this is sort of cheating if we wanted to find the limit purely analytically, since verifying that the limit converges was done through graphing). Another note: As Mc Cheng already pointed out, the limit does converge to some number, but I also noticed that if we were to subsitute the $n$ in the sequence with another variable $x$, then the limit of increasing values of $a_n$ as $x \to \infty$ seem to flip-flop between being an under-approximation and over-approximation (e.g., $\lim_{x\to\infty} a_4 < \lim_{n\to\infty}a_n,$ but $\lim_{x\to\infty} a_5 > \lim_{n\to\infty} a_n$), and the approximations (obviously) get closer to the actual value of $\lim_{n\to\infty} a_n$, so I wonder if the sequence can be turned into an alternating series?",,"['limits', 'logarithms']"
58,About a curious nested radical representation for $\cos 1^\circ$,About a curious nested radical representation for,\cos 1^\circ,"I have found the following nested radical representation. By using the triple angle formula for the cosine, $\cos 3\theta$, and making $\theta = 1^\circ$, we get the cubic equation $ 4x^3-3x = \cos 3^\circ $. In this step , I'll use a trick that is rarely used, by expressing $ x $ as $ x = \frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{x}} $ Now I just iterate for x and in this way I get that $ \cos 1^\circ= \frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{\frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{\frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{\frac{1}{2}\sqrt{3+...}}}}}}} $ Direct calculation shows that this periodic radical converges to $ \cos 1^\circ$ How to prove rigorously that the left hand side is the limit of the right hand side? That's to say, I need a proof that the nested radical converges to $ \cos 1^\circ $","I have found the following nested radical representation. By using the triple angle formula for the cosine, $\cos 3\theta$, and making $\theta = 1^\circ$, we get the cubic equation $ 4x^3-3x = \cos 3^\circ $. In this step , I'll use a trick that is rarely used, by expressing $ x $ as $ x = \frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{x}} $ Now I just iterate for x and in this way I get that $ \cos 1^\circ= \frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{\frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{\frac{1}{2}\sqrt{3+\frac{\cos 3^\circ}{\frac{1}{2}\sqrt{3+...}}}}}}} $ Direct calculation shows that this periodic radical converges to $ \cos 1^\circ$ How to prove rigorously that the left hand side is the limit of the right hand side? That's to say, I need a proof that the nested radical converges to $ \cos 1^\circ $",,"['limits', 'trigonometry', 'nested-radicals']"
59,A curious problem with striking similarity to L'Hospital's Rule,A curious problem with striking similarity to L'Hospital's Rule,,"I got this problem in a book ""A Problem Seminar"" by Donald J. Newman which caught my attention. I am trying to find a solution and hence if possible please provide a hint only kind of solution. Let $f, g$ be differentiable on $(0, 1]$ and $g'(x) > 0$ for all $x \in (0, 1]$ and further let $f'(x)/g'(x)$ tend to a limit (in the extended sense) as $x \to 0^{+}$. Prove that $f(x)/g(x)$ also tends to a limit (in the extended sense) as $x \to 0^{+}$. By existence of limit in extended sense I mean that the limit is either a real number or is infinite but in any case the function does not oscillate. Update : It appears that the requirement $g'(x) > 0$ is redundant. This is because $f'(x)/g'(x)$ is defined as $x \to 0^{+}$ and hence $g'(x)$ is non-zero and by intermediate value properly of derivatives $g'(x)$ maintains a constant sign as $x \to 0^{+}$. Further update : The condition $g'(x) > 0$ is more for the non-vanishing of derivative $g'$ and this also ensures that $g$ is non-vanishing and hence both $g', g$ are of constant sign (via intermediate value property). We then have $(f/g)' = (g'/g)(f'/g' - f/g)$ and therefore if $f'/g' < f/g$ as $x \to 0^{+}$ or $f'/g' > f/g$ as $x \to 0^{+}$ then $f/g$ is monotone and hence it has a limit (in the extended sense). The difficult case is when there are infinitely many values of $x$ as $x \to 0^{+}$ for which $f'/g' < f/g$ and there are infinitely many values of $x$ for which $f'/g' > f/g$. In this case I guess (but not sure) the $f/g$ remains near $f'/g'$ and the both tend to same limit. I wonder if this line of reasoning is correct. It appears that this reasoning is correct after all. See my answer. Another reasoning goes like this. Since $g$ is monotone and of constant sign it follows that $g$ tends to a limit in the extended sense. If this limit is infinite then we know by a version of L'Hospital's Rule that $f/g$ tends to same limit as that of $f'/g'$. If $g$ tends to a finite limit (including $0$) then we need to show that in this case $f$ also tends to a limit in the extended and hence $f/g$ tends to a limit in extended sense.","I got this problem in a book ""A Problem Seminar"" by Donald J. Newman which caught my attention. I am trying to find a solution and hence if possible please provide a hint only kind of solution. Let $f, g$ be differentiable on $(0, 1]$ and $g'(x) > 0$ for all $x \in (0, 1]$ and further let $f'(x)/g'(x)$ tend to a limit (in the extended sense) as $x \to 0^{+}$. Prove that $f(x)/g(x)$ also tends to a limit (in the extended sense) as $x \to 0^{+}$. By existence of limit in extended sense I mean that the limit is either a real number or is infinite but in any case the function does not oscillate. Update : It appears that the requirement $g'(x) > 0$ is redundant. This is because $f'(x)/g'(x)$ is defined as $x \to 0^{+}$ and hence $g'(x)$ is non-zero and by intermediate value properly of derivatives $g'(x)$ maintains a constant sign as $x \to 0^{+}$. Further update : The condition $g'(x) > 0$ is more for the non-vanishing of derivative $g'$ and this also ensures that $g$ is non-vanishing and hence both $g', g$ are of constant sign (via intermediate value property). We then have $(f/g)' = (g'/g)(f'/g' - f/g)$ and therefore if $f'/g' < f/g$ as $x \to 0^{+}$ or $f'/g' > f/g$ as $x \to 0^{+}$ then $f/g$ is monotone and hence it has a limit (in the extended sense). The difficult case is when there are infinitely many values of $x$ as $x \to 0^{+}$ for which $f'/g' < f/g$ and there are infinitely many values of $x$ for which $f'/g' > f/g$. In this case I guess (but not sure) the $f/g$ remains near $f'/g'$ and the both tend to same limit. I wonder if this line of reasoning is correct. It appears that this reasoning is correct after all. See my answer. Another reasoning goes like this. Since $g$ is monotone and of constant sign it follows that $g$ tends to a limit in the extended sense. If this limit is infinite then we know by a version of L'Hospital's Rule that $f/g$ tends to same limit as that of $f'/g'$. If $g$ tends to a finite limit (including $0$) then we need to show that in this case $f$ also tends to a limit in the extended and hence $f/g$ tends to a limit in extended sense.",,"['calculus', 'real-analysis', 'limits']"
60,An elementary verification of the equivalence between two expressions for $e^x$,An elementary verification of the equivalence between two expressions for,e^x,"I would appreciate some constructive comments on the following argument for \begin{equation*} \sum_{n=0}^{\infty} \frac{x^{n}}{n!} = \lim_{n\to\infty} \left(1 + \frac{x}{n}\right)^{n} . \end{equation*} I understand that there are several different arguments for it.  I like that this does not involve the natural logarithm function.  I looked in many elementary textbooks on real analysis for such an argument.  I only found one, but it was in the special case $x = 1$, and it was flawed.  The only analysis techniques used are the convergence of the sequence defined by \begin{equation*} \left(1 + \frac{x}{n}\right)^{n} , \end{equation*} and the absolute convergence of \begin{equation*} \sum_{n=0}^{\infty} \frac{x^{n}}{n!} . \end{equation*} Here it is. Demonstration According to the Binomial Theorem, for every positive integer $n$, \begin{align*} &\sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \\ &\qquad = \sum_{i=0}^{n} \frac{x^{i}}{i!} - \sum_{i=0}^{n} \binom{n}{i} \frac{x^{i}}{n^{i}} \\ &\qquad = \sum_{i=0}^{n} \left[\frac{x^{i}}{i!} - \binom{n}{i} \frac{x^{i}}{n^{i}} \right] \\ &\qquad = \sum_{i=0}^{n} \left[\frac{1}{i!} - \frac{1}{n^{i}} \binom{n}{i} \right] x^{i} \\ &\qquad = \sum_{i=2}^{n} \left[\frac{1}{i!} - \frac{1}{n^{i}} \binom{n}{i} \right] x^{i} . \end{align*} For each integer $2 \leq i \leq n$, \begin{align*} \frac{1}{n^{i}} \binom{n}{i} &= \frac{1}{n^{i}} \cdot \frac{n!}{i!(n-i)!} \\ &= \frac{1}{n^{i}} \cdot \frac{n(n - 1)(n - 2) \cdots (n - i + 1)}{i!} \\ &= \frac{1}{i!} \cdot \frac{n(n - 1) (n - 2) \cdots (n - (i - 1))}{n^{i}} \\ &= \frac{1}{i!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) . \end{align*} So, \begin{equation*} \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} = \sum_{i=2}^{n} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{x^{i}}{i!} \end{equation*} According to the Triangle Inequality, for each pair of positive integers $2 \leq k < n$, \begin{align*} &\left\vert \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \right\vert \\ &\qquad \leq \left\vert \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{x^{i}}{i!} \right\vert \\ &\qquad\qquad + \left\vert \sum_{i=k+1}^{n} \frac{x^{i}}{i!} \right\vert + \left\vert \sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} x^{i} \right\vert \\ &\qquad \leq \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{\vert x \vert^{i}}{i!} \\ &\qquad\qquad + \sum_{i=k+1}^{n} \frac{\vert x \vert^{i}}{i!} + \sum_{i=k+1}^{n} \frac{1}{n^{i}}\binom{n}{i} \vert x \vert^{i} . \end{align*} \begin{align*} &\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} \\ &\qquad = \sum_{i=k+1}^{n} \frac{1}{i!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \vert x \vert^{i} \\ &\qquad = \frac{1}{(k+1)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k}{n}\right) \vert x \vert^{k+1} \\ &\qquad\qquad \!\begin{aligned}[t] &+ \frac{1}{(k+2)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k+1}{n}\right) \vert x \vert^{k+2} \\ &+ \frac{1}{(k+3)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k+2}{n}\right) \vert x \vert^{k+3} \\ &+\ldots + \frac{1}{n!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{n-1}{n}\right) \vert x \vert^{n} . \end{aligned} \\ &\qquad = \frac{1}{k!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k}{n}\right) \vert x \vert^{k} \\ &\qquad\qquad \!\begin{aligned}[t] &\biggl[\frac{1}{k+1} \left(1 - \frac{k}{n}\right) \vert x \vert \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \frac{1}{(k+1)(k+2)} \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \vert x \vert^{2} \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \frac{1}{(k+1)(k+2)(k+3)}  \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \left(1 - \frac{k+2}{n}\right) \vert x \vert^{3} \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \ldots \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}} + \frac{1}{(k+1)(k+2) \cdots n} \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \cdots \left(1 - \frac{n-1}{n}\right) \vert x \vert^{n-k} \biggr] \end{aligned} \\ &\qquad < \frac{\vert x \vert^{k}}{k!} \left[ \frac{\vert x \vert}{k+1} + \left(\frac{\vert x \vert}{k+1}\right)^{2} + \ldots + \left(\frac{\vert x \vert}{k+1}\right)^{n-k} \right] . \\ \text{So, if $k \geq \vert x \vert$,} \\ &\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} \\ &\qquad < \frac{\vert x \vert^{k}}{k!} \sum_{i=1}^{\infty} \left(\frac{\vert x \vert}{k+1} \right)^{i} \\ &\qquad = \frac{\vert x \vert^{k}}{k!} \cdot \frac{\dfrac{\vert x \vert}{k + 1}}{1 - \dfrac{\vert x \vert}{k+1}} \\ &\qquad = \frac{\vert x \vert^{k}}{k!} \cdot \frac{\vert x \vert}{k + 1 - \vert x \vert} , \\ \text{and if $k \geq 2\vert x \vert$,} \\ &\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} < \frac{\vert x \vert^{k}}{k!} . \end{align*} By the absolute convergence of \begin{equation*} \sum_{n=0}^{\infty} \frac{x^{n}}{n!} , \end{equation*} for every $\epsilon > 0$, there is a big enough positive integer $K$ such that for every integer $k \geq K$, \begin{equation*} \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \frac{\epsilon}{3} , \end{equation*} and so, \begin{equation*} \frac{\vert x \vert^{k}}{k!} < \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \frac{\epsilon}{3} \qquad \text{and} \qquad \sum_{i=k+1}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \frac{\epsilon}{3} . \end{equation*} So, if $k \geq \max\{2\vert x \vert, \, K\}$, and if $n > k$, \begin{equation*} \sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} < \frac{\vert x \vert^{k}}{k!} < \frac{\epsilon}{3} . \end{equation*} Likewise, since for each integer $2 \leq i \leq k$, \begin{equation*} \lim_{n\to\infty} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] = 0 , \end{equation*} there is a big enough positive integer $N$ such that for every integer $n \geq N$, \begin{equation*} \sum_{i=2}^{k} \left[ 1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] < \frac{\epsilon}{3 \cdot \max\{\vert x \vert^{k}, \, 1\}} , \end{equation*} and so, \begin{equation*} \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{\vert x \vert^{i}}{i!} < \frac{\epsilon}{3} . \end{equation*} Consequently, for any positive integers $k \geq \max\{2\vert x \vert, \, K\}$ and $n > \max\{k, \, N\}$, \begin{equation*} \left\vert \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \right\vert < \epsilon . \end{equation*} Equivalently, \begin{equation*} \sum_{i=0}^{\infty} \frac{x^{i}}{i!} = \lim_{n\to\infty} \left(1 + \frac{x}{n}\right)^{n} . \end{equation*}","I would appreciate some constructive comments on the following argument for \begin{equation*} \sum_{n=0}^{\infty} \frac{x^{n}}{n!} = \lim_{n\to\infty} \left(1 + \frac{x}{n}\right)^{n} . \end{equation*} I understand that there are several different arguments for it.  I like that this does not involve the natural logarithm function.  I looked in many elementary textbooks on real analysis for such an argument.  I only found one, but it was in the special case $x = 1$, and it was flawed.  The only analysis techniques used are the convergence of the sequence defined by \begin{equation*} \left(1 + \frac{x}{n}\right)^{n} , \end{equation*} and the absolute convergence of \begin{equation*} \sum_{n=0}^{\infty} \frac{x^{n}}{n!} . \end{equation*} Here it is. Demonstration According to the Binomial Theorem, for every positive integer $n$, \begin{align*} &\sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \\ &\qquad = \sum_{i=0}^{n} \frac{x^{i}}{i!} - \sum_{i=0}^{n} \binom{n}{i} \frac{x^{i}}{n^{i}} \\ &\qquad = \sum_{i=0}^{n} \left[\frac{x^{i}}{i!} - \binom{n}{i} \frac{x^{i}}{n^{i}} \right] \\ &\qquad = \sum_{i=0}^{n} \left[\frac{1}{i!} - \frac{1}{n^{i}} \binom{n}{i} \right] x^{i} \\ &\qquad = \sum_{i=2}^{n} \left[\frac{1}{i!} - \frac{1}{n^{i}} \binom{n}{i} \right] x^{i} . \end{align*} For each integer $2 \leq i \leq n$, \begin{align*} \frac{1}{n^{i}} \binom{n}{i} &= \frac{1}{n^{i}} \cdot \frac{n!}{i!(n-i)!} \\ &= \frac{1}{n^{i}} \cdot \frac{n(n - 1)(n - 2) \cdots (n - i + 1)}{i!} \\ &= \frac{1}{i!} \cdot \frac{n(n - 1) (n - 2) \cdots (n - (i - 1))}{n^{i}} \\ &= \frac{1}{i!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) . \end{align*} So, \begin{equation*} \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} = \sum_{i=2}^{n} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{x^{i}}{i!} \end{equation*} According to the Triangle Inequality, for each pair of positive integers $2 \leq k < n$, \begin{align*} &\left\vert \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \right\vert \\ &\qquad \leq \left\vert \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{x^{i}}{i!} \right\vert \\ &\qquad\qquad + \left\vert \sum_{i=k+1}^{n} \frac{x^{i}}{i!} \right\vert + \left\vert \sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} x^{i} \right\vert \\ &\qquad \leq \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{\vert x \vert^{i}}{i!} \\ &\qquad\qquad + \sum_{i=k+1}^{n} \frac{\vert x \vert^{i}}{i!} + \sum_{i=k+1}^{n} \frac{1}{n^{i}}\binom{n}{i} \vert x \vert^{i} . \end{align*} \begin{align*} &\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} \\ &\qquad = \sum_{i=k+1}^{n} \frac{1}{i!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \vert x \vert^{i} \\ &\qquad = \frac{1}{(k+1)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k}{n}\right) \vert x \vert^{k+1} \\ &\qquad\qquad \!\begin{aligned}[t] &+ \frac{1}{(k+2)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k+1}{n}\right) \vert x \vert^{k+2} \\ &+ \frac{1}{(k+3)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k+2}{n}\right) \vert x \vert^{k+3} \\ &+\ldots + \frac{1}{n!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{n-1}{n}\right) \vert x \vert^{n} . \end{aligned} \\ &\qquad = \frac{1}{k!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k}{n}\right) \vert x \vert^{k} \\ &\qquad\qquad \!\begin{aligned}[t] &\biggl[\frac{1}{k+1} \left(1 - \frac{k}{n}\right) \vert x \vert \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \frac{1}{(k+1)(k+2)} \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \vert x \vert^{2} \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \frac{1}{(k+1)(k+2)(k+3)}  \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \left(1 - \frac{k+2}{n}\right) \vert x \vert^{3} \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \ldots \\ &\hphantom{\biggl[\vphantom{\frac{1}{k+1}}} + \frac{1}{(k+1)(k+2) \cdots n} \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \cdots \left(1 - \frac{n-1}{n}\right) \vert x \vert^{n-k} \biggr] \end{aligned} \\ &\qquad < \frac{\vert x \vert^{k}}{k!} \left[ \frac{\vert x \vert}{k+1} + \left(\frac{\vert x \vert}{k+1}\right)^{2} + \ldots + \left(\frac{\vert x \vert}{k+1}\right)^{n-k} \right] . \\ \text{So, if $k \geq \vert x \vert$,} \\ &\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} \\ &\qquad < \frac{\vert x \vert^{k}}{k!} \sum_{i=1}^{\infty} \left(\frac{\vert x \vert}{k+1} \right)^{i} \\ &\qquad = \frac{\vert x \vert^{k}}{k!} \cdot \frac{\dfrac{\vert x \vert}{k + 1}}{1 - \dfrac{\vert x \vert}{k+1}} \\ &\qquad = \frac{\vert x \vert^{k}}{k!} \cdot \frac{\vert x \vert}{k + 1 - \vert x \vert} , \\ \text{and if $k \geq 2\vert x \vert$,} \\ &\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} < \frac{\vert x \vert^{k}}{k!} . \end{align*} By the absolute convergence of \begin{equation*} \sum_{n=0}^{\infty} \frac{x^{n}}{n!} , \end{equation*} for every $\epsilon > 0$, there is a big enough positive integer $K$ such that for every integer $k \geq K$, \begin{equation*} \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \frac{\epsilon}{3} , \end{equation*} and so, \begin{equation*} \frac{\vert x \vert^{k}}{k!} < \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \frac{\epsilon}{3} \qquad \text{and} \qquad \sum_{i=k+1}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!} < \frac{\epsilon}{3} . \end{equation*} So, if $k \geq \max\{2\vert x \vert, \, K\}$, and if $n > k$, \begin{equation*} \sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} < \frac{\vert x \vert^{k}}{k!} < \frac{\epsilon}{3} . \end{equation*} Likewise, since for each integer $2 \leq i \leq k$, \begin{equation*} \lim_{n\to\infty} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] = 0 , \end{equation*} there is a big enough positive integer $N$ such that for every integer $n \geq N$, \begin{equation*} \sum_{i=2}^{k} \left[ 1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] < \frac{\epsilon}{3 \cdot \max\{\vert x \vert^{k}, \, 1\}} , \end{equation*} and so, \begin{equation*} \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{\vert x \vert^{i}}{i!} < \frac{\epsilon}{3} . \end{equation*} Consequently, for any positive integers $k \geq \max\{2\vert x \vert, \, K\}$ and $n > \max\{k, \, N\}$, \begin{equation*} \left\vert \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \right\vert < \epsilon . \end{equation*} Equivalently, \begin{equation*} \sum_{i=0}^{\infty} \frac{x^{i}}{i!} = \lim_{n\to\infty} \left(1 + \frac{x}{n}\right)^{n} . \end{equation*}",,"['real-analysis', 'limits', 'proof-verification']"
61,Multivariable limit of a piecewise function,Multivariable limit of a piecewise function,,"Consider the following expression: $$\lim_{(x,y) \to (0,0)} g(x,y)= \begin{cases} \frac{\sin x}{x} y \text{ $\hspace{0.5in}$if $x \neq 0$}\\ y \text{ $\hspace{0.81in}$if $x=0$}\end{cases}$$ I am seeking guidance in regards to a general method for finding limits for piecewise functions such as the one above. Do I take each case individually and find the limit? Since the top function exists only for $x \neq 0$, can the limit exist as $(x,y) \to (0,0)$? Now say the limit exists for this piecewise function. How would I prove this using the epsilon-delta definition? I am familiar with the process for non-piecewise functions, but this type of question has always confused me. Any general advice (that may assist me in solving more questions like this one) would be appreciated. Thank you! EDIT: Here is my attempt. We can directly find that $\lim_{(x,y) \to (0,0)}y=\lim_{y \to 0}y=0$. Now, we will consider the other case. Let $\varepsilon >0$ be arbitrary and take $\delta=\varepsilon$. Then, for all $(x,y)$ with $0<\sqrt{x^2+y^2}<\delta$, we have that $$\left|\frac{\sin x}{x}y\right|\leq |y| \leq \sqrt{x^2+y^2}<\delta=\varepsilon$$ Thus, the limit exists and is $0$.","Consider the following expression: $$\lim_{(x,y) \to (0,0)} g(x,y)= \begin{cases} \frac{\sin x}{x} y \text{ $\hspace{0.5in}$if $x \neq 0$}\\ y \text{ $\hspace{0.81in}$if $x=0$}\end{cases}$$ I am seeking guidance in regards to a general method for finding limits for piecewise functions such as the one above. Do I take each case individually and find the limit? Since the top function exists only for $x \neq 0$, can the limit exist as $(x,y) \to (0,0)$? Now say the limit exists for this piecewise function. How would I prove this using the epsilon-delta definition? I am familiar with the process for non-piecewise functions, but this type of question has always confused me. Any general advice (that may assist me in solving more questions like this one) would be appreciated. Thank you! EDIT: Here is my attempt. We can directly find that $\lim_{(x,y) \to (0,0)}y=\lim_{y \to 0}y=0$. Now, we will consider the other case. Let $\varepsilon >0$ be arbitrary and take $\delta=\varepsilon$. Then, for all $(x,y)$ with $0<\sqrt{x^2+y^2}<\delta$, we have that $$\left|\frac{\sin x}{x}y\right|\leq |y| \leq \sqrt{x^2+y^2}<\delta=\varepsilon$$ Thus, the limit exists and is $0$.",,['calculus']
62,Show that the piecewise defined function is continuous at $x=0$,Show that the piecewise defined function is continuous at,x=0,"I am faced with the following problem: Determine whether the following function is continous, once differentiable, or twice differentiable: $f(x) = \begin{cases} x^3+x-1 &\text{if $x \leq 0$;} \\ x^3-x-1 &\text{if $x >0$}. \end{cases}$ So far, I have shown that $f$ is not once differentiable at $x = 0$, and since $C^{2}(\mathbb{R}) \subset C^{1}(\mathbb{R})$, it is also not twice differentiable. What I am having a little bit of difficulty with is showing that it is continuous at $x = 0$.  Here is what I've done so far: since each piece of the piecewise defined function is continuous on its domain of definition, all I need to do is check the point $x = 0$. For $x > 0$, I want to see if the right-hand limit exists. In this case, $|f(x) - f(0)| = |x^{3}-x-1 - (0^{3} + 0 - 1)| = |x^{3} - x| = |x(x^{2}-1)| = |x||x^{2}-1|$. Now, if $|x|<1$, then $|x^{2} - 1| = |x+1||x-1| \leq (|x|+1)(|x|+1) = 2(|x|+1)< 2(1+1) = 4$. So, I have that $|x||x^{2}-1|<4|x| < \epsilon$ if we take $|x|<\frac{\epsilon}{4}$. Therefore, I take $\displaystyle \delta = \min\left\{ 1, \frac{\epsilon}{4}\right\}$, and I have $\forall \epsilon > 0$ that $|f(x) - f(0)|<\epsilon$; i.e., the right-hand limit exists and is equal to $-1$. For $x < 0$, I want to see if the left-hand limit exists.  In this case, $|f(x) - f(0)| = |x^{3}+x-1 - (0^{3}+0-1)| = |x^{3}+x-1+1| = |x^{3} + x| \leq |x^{3}| + |x|$. If $|x|<1$, then $|x^{3}|<|x|$. So, I have that $|x^{3}|+|x| < |x| + |x| = 2|x| < \epsilon$ provided we take $\displaystyle |x| < \frac{\epsilon}{2}$. Therefore, I take $\delta = \min \left\{1, \frac{\epsilon}{2} \right\}$, and I have  $\forall \epsilon > 0$ that $|f(x) - f(0)|<\epsilon$; i.e., the left-hand limit exists and is equal to $-1$. Thus, $\lim_{x\to 0}f(x) = -1$, and since $f(0) = -1$, we have that $f$ is continuous at $x = 0$.  Thus, it is continuous $\forall \mathbb{R}$. I suppose what I would like to know is if I showed this correctly and, if not, how I might fix it. Thank you.","I am faced with the following problem: Determine whether the following function is continous, once differentiable, or twice differentiable: $f(x) = \begin{cases} x^3+x-1 &\text{if $x \leq 0$;} \\ x^3-x-1 &\text{if $x >0$}. \end{cases}$ So far, I have shown that $f$ is not once differentiable at $x = 0$, and since $C^{2}(\mathbb{R}) \subset C^{1}(\mathbb{R})$, it is also not twice differentiable. What I am having a little bit of difficulty with is showing that it is continuous at $x = 0$.  Here is what I've done so far: since each piece of the piecewise defined function is continuous on its domain of definition, all I need to do is check the point $x = 0$. For $x > 0$, I want to see if the right-hand limit exists. In this case, $|f(x) - f(0)| = |x^{3}-x-1 - (0^{3} + 0 - 1)| = |x^{3} - x| = |x(x^{2}-1)| = |x||x^{2}-1|$. Now, if $|x|<1$, then $|x^{2} - 1| = |x+1||x-1| \leq (|x|+1)(|x|+1) = 2(|x|+1)< 2(1+1) = 4$. So, I have that $|x||x^{2}-1|<4|x| < \epsilon$ if we take $|x|<\frac{\epsilon}{4}$. Therefore, I take $\displaystyle \delta = \min\left\{ 1, \frac{\epsilon}{4}\right\}$, and I have $\forall \epsilon > 0$ that $|f(x) - f(0)|<\epsilon$; i.e., the right-hand limit exists and is equal to $-1$. For $x < 0$, I want to see if the left-hand limit exists.  In this case, $|f(x) - f(0)| = |x^{3}+x-1 - (0^{3}+0-1)| = |x^{3}+x-1+1| = |x^{3} + x| \leq |x^{3}| + |x|$. If $|x|<1$, then $|x^{3}|<|x|$. So, I have that $|x^{3}|+|x| < |x| + |x| = 2|x| < \epsilon$ provided we take $\displaystyle |x| < \frac{\epsilon}{2}$. Therefore, I take $\delta = \min \left\{1, \frac{\epsilon}{2} \right\}$, and I have  $\forall \epsilon > 0$ that $|f(x) - f(0)|<\epsilon$; i.e., the left-hand limit exists and is equal to $-1$. Thus, $\lim_{x\to 0}f(x) = -1$, and since $f(0) = -1$, we have that $f$ is continuous at $x = 0$.  Thus, it is continuous $\forall \mathbb{R}$. I suppose what I would like to know is if I showed this correctly and, if not, how I might fix it. Thank you.",,"['real-analysis', 'limits']"
63,"Compute $\lim_{N\to \infty}N^2/\left(\sum_{\text{primes }\leq p_N}p\right)$, where $p_N$ is the $N$th prime number, and another related limit","Compute , where  is the th prime number, and another related limit",\lim_{N\to \infty}N^2/\left(\sum_{\text{primes }\leq p_N}p\right) p_N N,"It is well known that, where $p_k$ is the $k$th prime number (this is $2 = p_1 < p_2 < p_3 < \cdots$), the following Proposition. The series of reciprocals of primes    $$\sum_{k=1}^\infty \frac{1}{p_k}$$   diverges. Too is known the so called harmonic-arithmetic mean inequality Proposition. For any positive real numbers $a_1,a_2,\ldots,a_N$, we have   $$\left(a_1+a_2+\cdots +a_N\right)\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_N}\right)\geq N^2.$$ Putting $a_k=p_k$ for $1\leq k\leq N$ we obtain $$0<\frac{N^2}{\sum_{k=1}^N p_k}\leq \sum_{k=1}^N \frac{1}{p_k}.$$ Thus  $$\lim_{N\to \infty}\frac{N^2}{\sum_{k=1}^N p_k}\leq \infty.$$ My Question. a) Compute previous limit. b) Compute $\lim_{N\to \infty} \frac{N^\alpha\cdot (\log N)^\beta}{\sum_{k=1}^N p_k}$, where I repeat another time that $p_k$ is the kth prime number and $\alpha,\beta$ real parameters. Thanks in advance, my only goal is learn in this site Math Stack Exchange from yours answers. References: If you want read it, you could find references of proofs of divergence of reciprocals of primes and a proof of the harmonic-arithmetic mean inequality, via this web site or Wikipedia, for example.","It is well known that, where $p_k$ is the $k$th prime number (this is $2 = p_1 < p_2 < p_3 < \cdots$), the following Proposition. The series of reciprocals of primes    $$\sum_{k=1}^\infty \frac{1}{p_k}$$   diverges. Too is known the so called harmonic-arithmetic mean inequality Proposition. For any positive real numbers $a_1,a_2,\ldots,a_N$, we have   $$\left(a_1+a_2+\cdots +a_N\right)\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_N}\right)\geq N^2.$$ Putting $a_k=p_k$ for $1\leq k\leq N$ we obtain $$0<\frac{N^2}{\sum_{k=1}^N p_k}\leq \sum_{k=1}^N \frac{1}{p_k}.$$ Thus  $$\lim_{N\to \infty}\frac{N^2}{\sum_{k=1}^N p_k}\leq \infty.$$ My Question. a) Compute previous limit. b) Compute $\lim_{N\to \infty} \frac{N^\alpha\cdot (\log N)^\beta}{\sum_{k=1}^N p_k}$, where I repeat another time that $p_k$ is the kth prime number and $\alpha,\beta$ real parameters. Thanks in advance, my only goal is learn in this site Math Stack Exchange from yours answers. References: If you want read it, you could find references of proofs of divergence of reciprocals of primes and a proof of the harmonic-arithmetic mean inequality, via this web site or Wikipedia, for example.",,"['limits', 'inequality']"
64,The convergence of a sequence with infinite products,The convergence of a sequence with infinite products,,"I have a problem to determine convergence (sum over n). $$\sum_{n=0}^\infty \dfrac {a\left( a+1^{p}\right) \ldots \left( a+n^{p}\right) }{b\left( b+1^{p}\right) \ldots \left( b+n^{p}\right) }$$where $a<b, a>0,b>0$. I have concluded convergence for $p\leq0$ by comparing it to a constructed geometric sequence, as well as for $p=1$, using comparison test with $n^{a-b}$. But I can not use similar methods for $p>1$ and $0<p<1$. I have some thoughts for the two parts: When $p>1$, it seems that the limit of each term is not $0$. If the limit could be evaluated, then the divergence can be proved. My method for $p=1$ is to use the Euler Product of the gamma function, but the $p$ power makes it impossible to use this method. I am wondering if there is any kind of generalization of gamma function that is of this form. when $0<p<1$, I compared it to the case of $p=1$, that could at least tell it converges in the range when $p=1$ converge. But it is inconclusive for the parts remaining. Any help or hints would be appreciated.","I have a problem to determine convergence (sum over n). $$\sum_{n=0}^\infty \dfrac {a\left( a+1^{p}\right) \ldots \left( a+n^{p}\right) }{b\left( b+1^{p}\right) \ldots \left( b+n^{p}\right) }$$where $a<b, a>0,b>0$. I have concluded convergence for $p\leq0$ by comparing it to a constructed geometric sequence, as well as for $p=1$, using comparison test with $n^{a-b}$. But I can not use similar methods for $p>1$ and $0<p<1$. I have some thoughts for the two parts: When $p>1$, it seems that the limit of each term is not $0$. If the limit could be evaluated, then the divergence can be proved. My method for $p=1$ is to use the Euler Product of the gamma function, but the $p$ power makes it impossible to use this method. I am wondering if there is any kind of generalization of gamma function that is of this form. when $0<p<1$, I compared it to the case of $p=1$, that could at least tell it converges in the range when $p=1$ converge. But it is inconclusive for the parts remaining. Any help or hints would be appreciated.",,"['calculus', 'limits', 'convergence-divergence']"
65,"If $f(x) \to 0$ and $g$ is a bounded function, then $f(x)g(x) \to 0$","If  and  is a bounded function, then",f(x) \to 0 g f(x)g(x) \to 0,"I am using a non-English source text so I am not sure that all technical terms is given their correct English name. What my source text calls ""upper limited function"" is defined as a function that has an upper limited range, that is, there is a B such that $\forall x \in D_{f}: f(x) \leq B$ Question 1 : What is the correct English name for this term? I came across the following theorem: If $\lim f(x) = 0$ and the function $g(x)$ is limited (i.e. both ""upper limited"" and ""lower limited""), then $f(x)g(x) \rightarrow 0$ Question 2 : Is there a specific name for this theorem? The proof of this theorem starts by stating that the requirement that g(x) is limited for large x implies that there exists numbers $C$ and $\omega_{0}$, such that: $$x > \omega_{0} \rightarrow | g(x)| < C $$ Then they define $ \epsilon $ as a positive number and the assumptions behind $f(x)$ means that there is a number $\omega_{1}$, such that $$x > \omega_{1} \rightarrow | f(x)| < \frac{\epsilon}{C} $$ Now, if $\omega = \max(\omega_{0},\omega_{1})$, then $$x > \omega \rightarrow |f(x)g(x)| = |f(x)||g(x)| < \frac{\epsilon}{C} \cdot C = \epsilon$$ This apparently means exactly that $f(x)g(x) \rightarrow 0$ when $x \rightarrow \infty$ Question 3 : I do not really understand that much of this proof, such as the the part about assumptions behind $f(x)$ implies the things it does or how the part about $\omega = \max(\omega_{0},\omega_{1})$ follows. Any tips? Question 4 : I sometimes find myself dealing with non-English source texts for a variety of reasons. Any advice on how to connect knowledge gained from these texts to the larger knowledge amassed from English literature?","I am using a non-English source text so I am not sure that all technical terms is given their correct English name. What my source text calls ""upper limited function"" is defined as a function that has an upper limited range, that is, there is a B such that $\forall x \in D_{f}: f(x) \leq B$ Question 1 : What is the correct English name for this term? I came across the following theorem: If $\lim f(x) = 0$ and the function $g(x)$ is limited (i.e. both ""upper limited"" and ""lower limited""), then $f(x)g(x) \rightarrow 0$ Question 2 : Is there a specific name for this theorem? The proof of this theorem starts by stating that the requirement that g(x) is limited for large x implies that there exists numbers $C$ and $\omega_{0}$, such that: $$x > \omega_{0} \rightarrow | g(x)| < C $$ Then they define $ \epsilon $ as a positive number and the assumptions behind $f(x)$ means that there is a number $\omega_{1}$, such that $$x > \omega_{1} \rightarrow | f(x)| < \frac{\epsilon}{C} $$ Now, if $\omega = \max(\omega_{0},\omega_{1})$, then $$x > \omega \rightarrow |f(x)g(x)| = |f(x)||g(x)| < \frac{\epsilon}{C} \cdot C = \epsilon$$ This apparently means exactly that $f(x)g(x) \rightarrow 0$ when $x \rightarrow \infty$ Question 3 : I do not really understand that much of this proof, such as the the part about assumptions behind $f(x)$ implies the things it does or how the part about $\omega = \max(\omega_{0},\omega_{1})$ follows. Any tips? Question 4 : I sometimes find myself dealing with non-English source texts for a variety of reasons. Any advice on how to connect knowledge gained from these texts to the larger knowledge amassed from English literature?",,"['calculus', 'limits']"
66,Spivak tough limit proof-verification,Spivak tough limit proof-verification,,"Suppose there is a $\delta > 0$ such that $f(x) = g(x)$ when $0 < |x - a| < \delta$. Prove that $\displaystyle \lim_{x\to a} f(x) = \lim_{x \to a} g(x)$. $|f(x) - L| < \epsilon$ for $|x - a| < \delta_1$ $|g(x) - M| < \epsilon$ for $|x - a| <\delta_2$ Let $\delta' = \min(\delta_1, \delta_2)$ Let $\delta' < \delta$ Therefore, $f(x) = g(x)$ for this interval $\delta'$ This gives us: $|f(x) - L| < \epsilon$ for $|x - a| < \delta_1$ $|f(x) - M| < \epsilon$ for $|x - a| <\delta_2$ Thus, what is required is to prove $M = L$ Assume $M \ne L$ Since $\epsilon \in (0, \infty)$, this means $\epsilon = |L - M|/2$ is a possibility. $|f(x) - L| < |L - M|/2$ for $|x - a| < \delta_1$ $|f(x) - M| < |L - M|/2$ for $|x - a| <\delta_2$ $|L - M| = |-(f(x) - L) + (f(x) - M)|$ The triangle inequality states, $|f(x) - L| + |f(x) - M| \ge |(f(x) - M) - (f(x) - L)|$ Then, $ |L - M|/2 + |L - M|/2 > |f(x) - L| + |f(x) - M| \ge |(f(x) - M) - (f(x) - L)|$ Which is a contradiction, therefore $L \ne M$ is false, and $L = M$ is the true statement. $\space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \blacksquare$","Suppose there is a $\delta > 0$ such that $f(x) = g(x)$ when $0 < |x - a| < \delta$. Prove that $\displaystyle \lim_{x\to a} f(x) = \lim_{x \to a} g(x)$. $|f(x) - L| < \epsilon$ for $|x - a| < \delta_1$ $|g(x) - M| < \epsilon$ for $|x - a| <\delta_2$ Let $\delta' = \min(\delta_1, \delta_2)$ Let $\delta' < \delta$ Therefore, $f(x) = g(x)$ for this interval $\delta'$ This gives us: $|f(x) - L| < \epsilon$ for $|x - a| < \delta_1$ $|f(x) - M| < \epsilon$ for $|x - a| <\delta_2$ Thus, what is required is to prove $M = L$ Assume $M \ne L$ Since $\epsilon \in (0, \infty)$, this means $\epsilon = |L - M|/2$ is a possibility. $|f(x) - L| < |L - M|/2$ for $|x - a| < \delta_1$ $|f(x) - M| < |L - M|/2$ for $|x - a| <\delta_2$ $|L - M| = |-(f(x) - L) + (f(x) - M)|$ The triangle inequality states, $|f(x) - L| + |f(x) - M| \ge |(f(x) - M) - (f(x) - L)|$ Then, $ |L - M|/2 + |L - M|/2 > |f(x) - L| + |f(x) - M| \ge |(f(x) - M) - (f(x) - L)|$ Which is a contradiction, therefore $L \ne M$ is false, and $L = M$ is the true statement. $\space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space\space\space\space \blacksquare$",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
67,Without L'Hopitals rule: $\lim_{x\rightarrow3}\frac{(x-3)\cdot\ln(x-2)}{1-\cos(3-x)}$,Without L'Hopitals rule:,\lim_{x\rightarrow3}\frac{(x-3)\cdot\ln(x-2)}{1-\cos(3-x)},"$$f(x) = \frac{(x-3)\cdot\ln(x-2)}{1-\cos(3-x)}$$ Is there any procedure to solving this expression? The problem right now is the fact that $\lim_{x\rightarrow3}f(x) = [\frac{0}{0}]$ I've tried multiplying by the conjuage of the denominator so I had $$\lim_{x\rightarrow3} = \frac{(x-3)\cdot\ln(x-2)(1+\cos(3-x))}{\sin^2(3-x)}$$ But of course, that didn't do much. Wolfram tells me $\lim_{x\rightarrow3}\frac{x-3}{\sin(3-x)} = -1$, however, I'd then have to extend by another $(x-3)$ and when I tried, it didn't make it any easier.","$$f(x) = \frac{(x-3)\cdot\ln(x-2)}{1-\cos(3-x)}$$ Is there any procedure to solving this expression? The problem right now is the fact that $\lim_{x\rightarrow3}f(x) = [\frac{0}{0}]$ I've tried multiplying by the conjuage of the denominator so I had $$\lim_{x\rightarrow3} = \frac{(x-3)\cdot\ln(x-2)(1+\cos(3-x))}{\sin^2(3-x)}$$ But of course, that didn't do much. Wolfram tells me $\lim_{x\rightarrow3}\frac{x-3}{\sin(3-x)} = -1$, however, I'd then have to extend by another $(x-3)$ and when I tried, it didn't make it any easier.",,"['calculus', 'limits', 'limits-without-lhopital']"
68,Linear combinations of indicator functions of measurable rectangles with sides of finite measure.,Linear combinations of indicator functions of measurable rectangles with sides of finite measure.,,"The following is a question on measure theory whose answer would help me greatly in my research. Let $ (X,\mathcal{F},\mu) $ and $ (Y,\mathcal{G},\nu) $ be $ \sigma $ -finite measure spaces. Let $ E $ be a $ (\mathcal{F} \otimes \mathcal{G}) $ -measurable subset of $ X \times Y $ such that $ (\mu \otimes \nu)(E) < \infty $ . Is it true that the indicator function $ \mathbf{1}_{E} $ is the pointwise limit of a sequence $ (s_{n})_{n \in \mathbb{N}} $ of functions on $ X \times Y $ , where for each $ n \in \mathbb{N} $ , we require that $ s_{n} $ be a finite linear combination of functions of the form $ \mathbf{1}_{A \times B} $ , where $ A \in \mathcal{F} $ , $ B \in \mathcal{G} $ and $ \mu(A),\nu(B) < \infty $ ? In other words, for each $ n \in \mathbb{N} $ , we require that $ s_{n} $ be a finite linear combination of indicator functions of measurable rectangles whose sides have finite measure. Thank you very much for your help.","The following is a question on measure theory whose answer would help me greatly in my research. Let and be -finite measure spaces. Let be a -measurable subset of such that . Is it true that the indicator function is the pointwise limit of a sequence of functions on , where for each , we require that be a finite linear combination of functions of the form , where , and ? In other words, for each , we require that be a finite linear combination of indicator functions of measurable rectangles whose sides have finite measure. Thank you very much for your help."," (X,\mathcal{F},\mu)   (Y,\mathcal{G},\nu)   \sigma   E   (\mathcal{F} \otimes \mathcal{G})   X \times Y   (\mu \otimes \nu)(E) < \infty   \mathbf{1}_{E}   (s_{n})_{n \in \mathbb{N}}   X \times Y   n \in \mathbb{N}   s_{n}   \mathbf{1}_{A \times B}   A \in \mathcal{F}   B \in \mathcal{G}   \mu(A),\nu(B) < \infty   n \in \mathbb{N}   s_{n} ","['real-analysis', 'measure-theory', 'limits']"
69,Limit of logarithmic function using l'Hospital,Limit of logarithmic function using l'Hospital,,"How can I find the following limit: $$\lim_{x\rightarrow \infty}\frac{\ln(1+\alpha x)}{\ln(\ln(1+\text{e}^{\beta x}))}$$ where $\alpha, \ \beta \in \mathbb{R}^+$. My first guess was to use l'Hospital: $$\lim_{x\rightarrow \infty}\frac{\ln(1+\alpha x)}{\ln(\ln(1+\text{e}^{\beta x}))} = \lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x})(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x) \ \text{e}^{\beta x} \  \beta}$$ But what can I do now? Is my approach correct or is there a simpler method? Edit : Taking the advice from Daniel Fischer,  $$\lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x})(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x) \ \text{e}^{\beta x} \  \beta} = \lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x)  \  \beta}  \lim_{x\rightarrow \infty}(1+\text{e}^{-\beta x}) $$ Applying L'Hospital a second time on the first fraction, $$\lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x)  \  \beta}  \lim_{x\rightarrow \infty}(1+\text{e}^{-\beta x}) = \lim_{x\rightarrow \infty}\frac{\alpha  \ \beta \  \text{e}^{\beta x}}{(1+\text{e}^{\beta x}) \ \alpha \ \beta}    \cdot 1 = \lim_{x\rightarrow \infty}\frac{\text{e}^{\beta x}}{1+\text{e}^{\beta x} }    \cdot 1  $$ Now let's apply L'Hospital one final time: $$\lim_{x\rightarrow \infty}\frac{\text{e}^{\beta x}}{1+\text{e}^{\beta x} }    \cdot 1 = \lim_{x\rightarrow \infty}\frac{\text{e}^{\beta x}\ \beta}{\text{e}^{\beta x}\ \beta}    \cdot 1 = 1$$ Is this correct?","How can I find the following limit: $$\lim_{x\rightarrow \infty}\frac{\ln(1+\alpha x)}{\ln(\ln(1+\text{e}^{\beta x}))}$$ where $\alpha, \ \beta \in \mathbb{R}^+$. My first guess was to use l'Hospital: $$\lim_{x\rightarrow \infty}\frac{\ln(1+\alpha x)}{\ln(\ln(1+\text{e}^{\beta x}))} = \lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x})(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x) \ \text{e}^{\beta x} \  \beta}$$ But what can I do now? Is my approach correct or is there a simpler method? Edit : Taking the advice from Daniel Fischer,  $$\lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x})(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x) \ \text{e}^{\beta x} \  \beta} = \lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x)  \  \beta}  \lim_{x\rightarrow \infty}(1+\text{e}^{-\beta x}) $$ Applying L'Hospital a second time on the first fraction, $$\lim_{x\rightarrow \infty}\frac{\ln(1+\text{e}^{\beta x}) \ \alpha}{(1 + \alpha x)  \  \beta}  \lim_{x\rightarrow \infty}(1+\text{e}^{-\beta x}) = \lim_{x\rightarrow \infty}\frac{\alpha  \ \beta \  \text{e}^{\beta x}}{(1+\text{e}^{\beta x}) \ \alpha \ \beta}    \cdot 1 = \lim_{x\rightarrow \infty}\frac{\text{e}^{\beta x}}{1+\text{e}^{\beta x} }    \cdot 1  $$ Now let's apply L'Hospital one final time: $$\lim_{x\rightarrow \infty}\frac{\text{e}^{\beta x}}{1+\text{e}^{\beta x} }    \cdot 1 = \lim_{x\rightarrow \infty}\frac{\text{e}^{\beta x}\ \beta}{\text{e}^{\beta x}\ \beta}    \cdot 1 = 1$$ Is this correct?",,"['real-analysis', 'limits', 'logarithms']"
70,"Continuity, derivatives of rational function","Continuity, derivatives of rational function",,"I'm studying the continuity of a function and its derivatives checking if the function is continuous, differentiable and calculating some derivatives. The function is \begin{cases} \dfrac{x^2y}{x^2+y^2}& \text{if }\, (x,y)\neq 0\\ 0& \text{if}\,\, (x,y)=0 \end{cases} I'm asked to study the continuity in $(0,0)$. My work: I started checking the limit of the function for $(x,y) \rightarrow (0,0)$ and considered the trasformation in polar coordinates to get a easier limit: $$\lim_{\rho\rightarrow 0} \frac{\rho^3\cos^2\theta \sin\theta}{\rho^2}=\lim_{\rho\rightarrow 0} \rho \cos^2\theta \sin\theta=0$$ So I got the continuity. Then I've been asked to study the value of partial derivatives in $(0,0)$ and the directional derivatives in $(0,0)$ calculated on $y=x$. I calculated the derivatives: $f_x=\dfrac{2xy^3}{(x^2+y^2)^2}$, this is not defined in $(0,0)$ so I considered the limit using polar coordinates again, and I got: $$\lim_{\rho\rightarrow 0} \frac{2\rho^4 \cos\theta \sin^3\theta}{\rho^4}=2\cos\theta \sin^3\theta$$ Is it right? What can I say now about this limit? The value of the derivatives in this point is supposed to be $0$ according to the solution given but I'm stuck in this point. Besides  $f_y=\dfrac{x^4-x^2y^2}{(x^2+y^2)^2}$, from which I get to the same kind of problem (again the values is supposed to be $0$) Finally the directional derivatives: I used the definition and got: the vector of the direction is $(\sqrt2/2;\sqrt2/2)$ and $f(0,0)=0$ so $$\lim_{t\rightarrow 0} \frac{f(\frac{\sqrt2}{2}t;\frac{\sqrt2}{2}t)-0}{t}=\lim_{t\rightarrow 0} \frac{\frac{1}{2}t^2\cdot\frac{\sqrt2}{2}t}{\frac{1}{2}t^2+\frac{1}{2}t^2}\cdot\frac{1}{t}=\frac{\frac{\sqrt2}{4}t^3}{t^2}\cdot\frac{1}{t}=\frac{\sqrt2}{4}$$ I have no idea how to finish the discussion of the two first derivatives and so how to check if the whole function is differentiable in the point $(0,0)$","I'm studying the continuity of a function and its derivatives checking if the function is continuous, differentiable and calculating some derivatives. The function is \begin{cases} \dfrac{x^2y}{x^2+y^2}& \text{if }\, (x,y)\neq 0\\ 0& \text{if}\,\, (x,y)=0 \end{cases} I'm asked to study the continuity in $(0,0)$. My work: I started checking the limit of the function for $(x,y) \rightarrow (0,0)$ and considered the trasformation in polar coordinates to get a easier limit: $$\lim_{\rho\rightarrow 0} \frac{\rho^3\cos^2\theta \sin\theta}{\rho^2}=\lim_{\rho\rightarrow 0} \rho \cos^2\theta \sin\theta=0$$ So I got the continuity. Then I've been asked to study the value of partial derivatives in $(0,0)$ and the directional derivatives in $(0,0)$ calculated on $y=x$. I calculated the derivatives: $f_x=\dfrac{2xy^3}{(x^2+y^2)^2}$, this is not defined in $(0,0)$ so I considered the limit using polar coordinates again, and I got: $$\lim_{\rho\rightarrow 0} \frac{2\rho^4 \cos\theta \sin^3\theta}{\rho^4}=2\cos\theta \sin^3\theta$$ Is it right? What can I say now about this limit? The value of the derivatives in this point is supposed to be $0$ according to the solution given but I'm stuck in this point. Besides  $f_y=\dfrac{x^4-x^2y^2}{(x^2+y^2)^2}$, from which I get to the same kind of problem (again the values is supposed to be $0$) Finally the directional derivatives: I used the definition and got: the vector of the direction is $(\sqrt2/2;\sqrt2/2)$ and $f(0,0)=0$ so $$\lim_{t\rightarrow 0} \frac{f(\frac{\sqrt2}{2}t;\frac{\sqrt2}{2}t)-0}{t}=\lim_{t\rightarrow 0} \frac{\frac{1}{2}t^2\cdot\frac{\sqrt2}{2}t}{\frac{1}{2}t^2+\frac{1}{2}t^2}\cdot\frac{1}{t}=\frac{\frac{\sqrt2}{4}t^3}{t^2}\cdot\frac{1}{t}=\frac{\sqrt2}{4}$$ I have no idea how to finish the discussion of the two first derivatives and so how to check if the whole function is differentiable in the point $(0,0)$",,"['limits', 'derivatives', 'partial-derivative']"
71,Show and prove that the sequence $u_n = \frac{n^2+n-2}{4n^2-5}$ converges to $1/4$,Show and prove that the sequence  converges to,u_n = \frac{n^2+n-2}{4n^2-5} 1/4,"Show and prove that the sequence $u_n = \frac{n^2+n-2}{4n^2-5}$ converges to $1/4$ Working: To show the sequence converges to $1/4$, note that $\frac{n^2+n-2}{4n^2-5} = \frac{1+1/n -2/n^2}{4-5/n^2}$, and using the Algebraic Limit Theorem yields a limit of $1/4$. The problem I am having is with the proof using the formal definition: I feel like my approach is way too long-winded, can someone please confirm whether it's correct and whether there's a quicker method? Anyhow, so I did: We need to show for every $\epsilon > 0$, there exists an $N \in \mathbb{N}$ such that whenever $n \ge N$, it follows $|\frac{n^2+n-2}{4n^2-5} - 1/4| < \epsilon$ Going backwards first: note that $|\frac{n^2+n-2}{4n^2-5} - 1/4| < \epsilon \implies|\frac{4n-3}{4(4n^2-5)}| < \epsilon$. Now pick $n$ large enough so that $\frac{4n-3}{4(4n^2-5)} > 0$, hence $\frac{4n-3}{4(4n^2-5)} < \epsilon \implies 0 < n^2 - \frac{1}{4\epsilon}n + \frac{3}{16\epsilon}-\frac{5}{4}$ $\implies 0 < \left[n - \left(\frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon} \right) \right]\left[n -  \left(\frac{1-\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon} \right)  \right]$ so, $n > \frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$ or $n < \frac{1-\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$. Now pick $N > \frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$, then whenever $n \ge N > \frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$, we have $(8\epsilon n -1)^2 > 80\epsilon^2 - 12\epsilon + 1 \implies \epsilon > \frac{4n-3}{4(4n^2-5)} \implies |\frac{n^2+n-2}{4n^2-5} - 1/4| < \epsilon$, as required. I'm pretty sure I did the algebra right, just want to confirm whether all my arguments are correct and if there's a quicker method. EDIT: Following on from @David's post, is this solution correct? Need to show $\lim\left( \frac{n^2+n-2}{4n^2-5} \right) = 1/4$, note: $|\frac{n^2+n-2}{4n^2-5} - \frac{1}{4} | = |\frac{4n-3}{4(4n^2-5)}|$. If $n \ge 2$, then $\frac{4n-3}{4(4n^2-5)} > 0$, so $|\frac{4n-3}{4(4n^2-5)}| = \frac{4n-3}{4(4n^2-5)}$. If $n^2 > 5$ (or more conveniently assume $n \ge 3$), then $\frac{4n-3}{4(4n^2-5)} < \frac{4n}{4(3n^2)} = \frac{1}{3n} < \epsilon$. Now pick $N_1 \in \mathbb{N}$ such that $N_1 > \frac{1}{3\epsilon}$ and set $N = \max\{3, N_1\}$. Let $\epsilon > 0$ and suppose $n \ge N$, then $|\frac{n^2+n-2}{4n^2-5} - \frac{1}{4} | = |\frac{4n-3}{4(4n^2-5)}| = \frac{4n-3}{4(4n^2-5)} < \frac{4n}{4(3n^2)} = \frac{1}{3n} < \epsilon$, as required.","Show and prove that the sequence $u_n = \frac{n^2+n-2}{4n^2-5}$ converges to $1/4$ Working: To show the sequence converges to $1/4$, note that $\frac{n^2+n-2}{4n^2-5} = \frac{1+1/n -2/n^2}{4-5/n^2}$, and using the Algebraic Limit Theorem yields a limit of $1/4$. The problem I am having is with the proof using the formal definition: I feel like my approach is way too long-winded, can someone please confirm whether it's correct and whether there's a quicker method? Anyhow, so I did: We need to show for every $\epsilon > 0$, there exists an $N \in \mathbb{N}$ such that whenever $n \ge N$, it follows $|\frac{n^2+n-2}{4n^2-5} - 1/4| < \epsilon$ Going backwards first: note that $|\frac{n^2+n-2}{4n^2-5} - 1/4| < \epsilon \implies|\frac{4n-3}{4(4n^2-5)}| < \epsilon$. Now pick $n$ large enough so that $\frac{4n-3}{4(4n^2-5)} > 0$, hence $\frac{4n-3}{4(4n^2-5)} < \epsilon \implies 0 < n^2 - \frac{1}{4\epsilon}n + \frac{3}{16\epsilon}-\frac{5}{4}$ $\implies 0 < \left[n - \left(\frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon} \right) \right]\left[n -  \left(\frac{1-\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon} \right)  \right]$ so, $n > \frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$ or $n < \frac{1-\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$. Now pick $N > \frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$, then whenever $n \ge N > \frac{1+\sqrt{80\epsilon^2-12\epsilon+1}}{8\epsilon}$, we have $(8\epsilon n -1)^2 > 80\epsilon^2 - 12\epsilon + 1 \implies \epsilon > \frac{4n-3}{4(4n^2-5)} \implies |\frac{n^2+n-2}{4n^2-5} - 1/4| < \epsilon$, as required. I'm pretty sure I did the algebra right, just want to confirm whether all my arguments are correct and if there's a quicker method. EDIT: Following on from @David's post, is this solution correct? Need to show $\lim\left( \frac{n^2+n-2}{4n^2-5} \right) = 1/4$, note: $|\frac{n^2+n-2}{4n^2-5} - \frac{1}{4} | = |\frac{4n-3}{4(4n^2-5)}|$. If $n \ge 2$, then $\frac{4n-3}{4(4n^2-5)} > 0$, so $|\frac{4n-3}{4(4n^2-5)}| = \frac{4n-3}{4(4n^2-5)}$. If $n^2 > 5$ (or more conveniently assume $n \ge 3$), then $\frac{4n-3}{4(4n^2-5)} < \frac{4n}{4(3n^2)} = \frac{1}{3n} < \epsilon$. Now pick $N_1 \in \mathbb{N}$ such that $N_1 > \frac{1}{3\epsilon}$ and set $N = \max\{3, N_1\}$. Let $\epsilon > 0$ and suppose $n \ge N$, then $|\frac{n^2+n-2}{4n^2-5} - \frac{1}{4} | = |\frac{4n-3}{4(4n^2-5)}| = \frac{4n-3}{4(4n^2-5)} < \frac{4n}{4(3n^2)} = \frac{1}{3n} < \epsilon$, as required.",,"['real-analysis', 'limits']"
72,Limit involving complicated integral,Limit involving complicated integral,,"$$\lim_{x\to\infty} \sqrt{x} \int_0^\frac{\pi}{4} e^{x(\cos t-1)}\cos t\ dt$$ I attempted to work out the integral part, but it did not work well because of the existence of the e part. So whether there is a better and more convient way to calculate this limit. Thanks!!","$$\lim_{x\to\infty} \sqrt{x} \int_0^\frac{\pi}{4} e^{x(\cos t-1)}\cos t\ dt$$ I attempted to work out the integral part, but it did not work well because of the existence of the e part. So whether there is a better and more convient way to calculate this limit. Thanks!!",,"['calculus', 'limits']"
73,"What is the slope of the tangent at $(0,0)$ on the curve $x^2 y^2 = 4 x^5 + y^3$",What is the slope of the tangent at  on the curve,"(0,0) x^2 y^2 = 4 x^5 + y^3","This question is arising from the answer to another one: How find this equation integer solution: $x^2y^2=4x^5+y^3$ . For $x < 27$ and $y > -243$ , the basic equation $x^2 y^2 = 4 x^5 + y^3$ is a function. By implicit differentiation we have found that: $$ y'(x) = \frac{20 x^4 - 2 x y^2}{2 y x^2 - 3 y^2} \quad \Longrightarrow \quad y'(0) = \; ? $$ From the picture it is suspected that: $y'(0) = 0$ , i.e. the slope of the tangent in $(x,y) = (0,0)$ may be zero. But I could not prove or disprove it. Any ideas? Update. When solving for $y$ (with help of MAPLE) we find something that looks like a decent function, within a prescribed range e.g. $-1 < x < +2$ ; see picture. (Bonus: integer solutions original question at red spots) $$ y(x) = \left[\frac{\left(-54 x^2 + x^3 + 6 \sqrt{81 x^4 - 3 x^5}\right)^{1/3}}{3}      + \frac{x^2}{3\left(-54 x^2 + x^3 + 6 \sqrt{81 x^4 - 3 x^5}\right)^{1/3}}       + \frac{x}{3}\right] x $$ So it's still not clear to me why the derivative $y'(0)$ would be somehow undefined. A rather extreme close-up , namely $-1/10 < x < 1/10$ : picture on the right , doesn't reveal any other slope than zero at $(0,0)$ . Whiter means that the function $f(x,y) = 4 x^5 + y^3 - x^2 y^2\;$ is closer to zero; it is seen that $\;f(x,y)\;$ is very close to zero indeed in the neighborhood of $(0,0)$ , thus suggesting that the tangent may be ambiguous there. But is it? My try. Draw a circle with radius $r > 0$ and $(0,0)$ as its midpoint: $$ x = r \cos(\phi) \qquad y = r \sin(\phi) $$  Substitute this into the basic equation $\;x^2 y ^2 = 4 x^5 + y^3\;$ and divide by $r^3$ : $$    4 \cos^5(\phi)\, r^2 - \cos^2(\phi) \sin^2(\phi)\, r + \sin^3(\phi) = 0 $$ If $\;r \rightarrow 0\;$ i.e. becomes very small, then function values in the neighborhood of $\;(0,0)\;$ only depend on the last term $\;\sin^3(\phi)$ . Meaning that $\phi \approx 0$ or $\phi \approx \pi$ . The tangent through these two points has slope zero. Don't know if this counts as a proof.","This question is arising from the answer to another one: How find this equation integer solution: $x^2y^2=4x^5+y^3$ . For $x < 27$ and $y > -243$ , the basic equation $x^2 y^2 = 4 x^5 + y^3$ is a function. By implicit differentiation we have found that: $$ y'(x) = \frac{20 x^4 - 2 x y^2}{2 y x^2 - 3 y^2} \quad \Longrightarrow \quad y'(0) = \; ? $$ From the picture it is suspected that: $y'(0) = 0$ , i.e. the slope of the tangent in $(x,y) = (0,0)$ may be zero. But I could not prove or disprove it. Any ideas? Update. When solving for $y$ (with help of MAPLE) we find something that looks like a decent function, within a prescribed range e.g. $-1 < x < +2$ ; see picture. (Bonus: integer solutions original question at red spots) $$ y(x) = \left[\frac{\left(-54 x^2 + x^3 + 6 \sqrt{81 x^4 - 3 x^5}\right)^{1/3}}{3}      + \frac{x^2}{3\left(-54 x^2 + x^3 + 6 \sqrt{81 x^4 - 3 x^5}\right)^{1/3}}       + \frac{x}{3}\right] x $$ So it's still not clear to me why the derivative $y'(0)$ would be somehow undefined. A rather extreme close-up , namely $-1/10 < x < 1/10$ : picture on the right , doesn't reveal any other slope than zero at $(0,0)$ . Whiter means that the function $f(x,y) = 4 x^5 + y^3 - x^2 y^2\;$ is closer to zero; it is seen that $\;f(x,y)\;$ is very close to zero indeed in the neighborhood of $(0,0)$ , thus suggesting that the tangent may be ambiguous there. But is it? My try. Draw a circle with radius $r > 0$ and $(0,0)$ as its midpoint: $$ x = r \cos(\phi) \qquad y = r \sin(\phi) $$  Substitute this into the basic equation $\;x^2 y ^2 = 4 x^5 + y^3\;$ and divide by $r^3$ : $$    4 \cos^5(\phi)\, r^2 - \cos^2(\phi) \sin^2(\phi)\, r + \sin^3(\phi) = 0 $$ If $\;r \rightarrow 0\;$ i.e. becomes very small, then function values in the neighborhood of $\;(0,0)\;$ only depend on the last term $\;\sin^3(\phi)$ . Meaning that $\phi \approx 0$ or $\phi \approx \pi$ . The tangent through these two points has slope zero. Don't know if this counts as a proof.",,"['limits', 'plane-curves', 'implicit-differentiation']"
74,"Prove that if $f(x) > 0$ for all $x$, $\lim_{x\to a} f(x) = 0$ and $\lim_{x\to a} g(x) = \infty$ ...","Prove that if  for all ,  and  ...",f(x) > 0 x \lim_{x\to a} f(x) = 0 \lim_{x\to a} g(x) = \infty,"Prove that if $f(x) > 0$ for all $x$, $\lim_{x\to a} f(x) = 0$ and $\lim_{x\to a} g(x) = \infty$, then $\lim_{x\to a} [f(x)]^{g(x)} = 0$ NOTE: This shows that $0^\infty$ is not an indeterminate form. Would this solution be correct? $$ \lim_{x\to a}[f(x)]^{g(x)}=\lim_{x\to a}e^{g(x)\ln(f(x))}=0 $$ Because $$ \lim_{x\to a}[\ln(f(x)]]=\lim_{y\to 0}(\ln y)=-\infty $$ and $$ \lim_{x\to a}[g(x)]=\infty $$ Then $$ \lim_{x\to a}[g(x)\ln(f(x)]]=-\infty $$ Finally $$ \lim_{x\to a}[e^{g(x)\ln(f(x)}]=0 $$","Prove that if $f(x) > 0$ for all $x$, $\lim_{x\to a} f(x) = 0$ and $\lim_{x\to a} g(x) = \infty$, then $\lim_{x\to a} [f(x)]^{g(x)} = 0$ NOTE: This shows that $0^\infty$ is not an indeterminate form. Would this solution be correct? $$ \lim_{x\to a}[f(x)]^{g(x)}=\lim_{x\to a}e^{g(x)\ln(f(x))}=0 $$ Because $$ \lim_{x\to a}[\ln(f(x)]]=\lim_{y\to 0}(\ln y)=-\infty $$ and $$ \lim_{x\to a}[g(x)]=\infty $$ Then $$ \lim_{x\to a}[g(x)\ln(f(x)]]=-\infty $$ Finally $$ \lim_{x\to a}[e^{g(x)\ln(f(x)}]=0 $$",,"['calculus', 'limits']"
75,The limit of $\sin(n^\alpha)$,The limit of,\sin(n^\alpha),"(1) It is easy to prove that $\lim\limits_{n\to\infty}{\sin(n)}$ does not exist. (2) I want to ask how to prove that $\lim\limits_{n\to\infty}{\sin(n^2)}$ does not exist. (3) Furthermore, $\lim\limits_{n\to\infty}{\sin(n^k)}$ does not exist. ( $k$ is a positive integer.) (4) In addition, $\lim\limits_{n\to\infty}{\sin(n^{\alpha})}$ does not exist. ( $\alpha$ is a positive real number.)","(1) It is easy to prove that does not exist. (2) I want to ask how to prove that does not exist. (3) Furthermore, does not exist. ( is a positive integer.) (4) In addition, does not exist. ( is a positive real number.)",\lim\limits_{n\to\infty}{\sin(n)} \lim\limits_{n\to\infty}{\sin(n^2)} \lim\limits_{n\to\infty}{\sin(n^k)} k \lim\limits_{n\to\infty}{\sin(n^{\alpha})} \alpha,['limits']
76,"Multivariable Delta Epsilon Proof $\lim_{(x,y)\to(0,0)}\frac{x^3y^2}{x^4+y^4}$ --- looking for a hint",Multivariable Delta Epsilon Proof  --- looking for a hint,"\lim_{(x,y)\to(0,0)}\frac{x^3y^2}{x^4+y^4}","I have the limit $$\lim_{(x,y)\to(0,0)}\frac{x^3y^2}{x^4+y^4},$$ and would like to show with an $\epsilon-\delta$ proof that it is zero. I know with a situation like $$\left|\frac{x^4y}{x^4+y^4}\right|\leq y$$  or something similar, but I can't find a way to do the same thing here, as no single term in the numerator is of sufficient degree, although I think I could get this with a small hint.","I have the limit $$\lim_{(x,y)\to(0,0)}\frac{x^3y^2}{x^4+y^4},$$ and would like to show with an $\epsilon-\delta$ proof that it is zero. I know with a situation like $$\left|\frac{x^4y}{x^4+y^4}\right|\leq y$$  or something similar, but I can't find a way to do the same thing here, as no single term in the numerator is of sufficient degree, although I think I could get this with a small hint.",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
77,Limit of $\prod_{p\text{ prime}}\left(1-[p(p-1)]^{-1}\right)$?,Limit of ?,\prod_{p\text{ prime}}\left(1-[p(p-1)]^{-1}\right),"Do we know the limit of the product $$\prod_{p\text{ prime}}\left(1-\frac{1}{p(p-1)}\right)$$ ? I ask because it seems to me on heuristic grounds (but I believe I could make them rigorous) that this number should be the average probability that a uniformly chosen element of $(\mathbb{Z}/p\mathbb{Z})^\times$ is a generator, i.e. the Cesaro mean of the sequence $\varphi(p_n-1)/(p_n-1)$.","Do we know the limit of the product $$\prod_{p\text{ prime}}\left(1-\frac{1}{p(p-1)}\right)$$ ? I ask because it seems to me on heuristic grounds (but I believe I could make them rigorous) that this number should be the average probability that a uniformly chosen element of $(\mathbb{Z}/p\mathbb{Z})^\times$ is a generator, i.e. the Cesaro mean of the sequence $\varphi(p_n-1)/(p_n-1)$.",,"['number-theory', 'limits', 'infinite-product']"
78,Justification for moving a limit outside an indefinite integral,Justification for moving a limit outside an indefinite integral,,"Someone asked on another site about ways to evaluate $\int \ln x \ dx $ without using integration by parts. My response was the following: $$ \begin{align} \int\ln x \, dx & =\int\lim_{t\to 0}\frac{x^{t}-1}{t} \, dx \\ &=  \lim_{t\to 0}\frac{1}{t}\int (x^{t}-1) \, dx \\  &=\lim_{t\to 0}\frac{1}{t}\left(\frac{x^{t+1}}{t+1}-x\right)+C \\ &=\lim_{t\to 0}\frac{x^{t+1}-x(t+1)}{t(t+1)}+C \\ &=\lim_{t\to 0}\frac{x^{t+1}\ln x-x}{2t+1}+C  \\ &= x\ln x-x+C \end{align}$$ But I don't know how to justify moving the limit outside the indefinite integral.","Someone asked on another site about ways to evaluate $\int \ln x \ dx $ without using integration by parts. My response was the following: $$ \begin{align} \int\ln x \, dx & =\int\lim_{t\to 0}\frac{x^{t}-1}{t} \, dx \\ &=  \lim_{t\to 0}\frac{1}{t}\int (x^{t}-1) \, dx \\  &=\lim_{t\to 0}\frac{1}{t}\left(\frac{x^{t+1}}{t+1}-x\right)+C \\ &=\lim_{t\to 0}\frac{x^{t+1}-x(t+1)}{t(t+1)}+C \\ &=\lim_{t\to 0}\frac{x^{t+1}\ln x-x}{2t+1}+C  \\ &= x\ln x-x+C \end{align}$$ But I don't know how to justify moving the limit outside the indefinite integral.",,"['real-analysis', 'limits']"
79,Limit of $f(x) = \lfloor x \rfloor + \sqrt{x - \lfloor x \rfloor}$,Limit of,f(x) = \lfloor x \rfloor + \sqrt{x - \lfloor x \rfloor},"I'm doing Spivak's Calculus book and one of the exercises from 5th chapter says 4. For each of the functions [...], decide for which numbers a the limit $lim_{x \to a}\ f(x)$ exists. i) $f(x) = \lfloor x \rfloor$ ii) $f(x) = x - \lfloor x \rfloor$ iii) $f(x) = \sqrt{x - \lfloor x \rfloor}$ iv) $ f(x) = \lfloor x \rfloor+ \sqrt{x - \lfloor x \rfloor}$ $...$ I found that, for $i$ , $ii$ and $iii$ , the limit doesn't exist when $a = \lfloor a \rfloor$ , namely, when a is an integer. But since in iv I have the floor function being used twice (once incrementing once decrementing), I'm not sure if I'm supposed to use the $x-1 \lt \lfloor x \rfloor \le x$ in the same way. What am I supposed to do here? How should I find inequalities for the new function?","I'm doing Spivak's Calculus book and one of the exercises from 5th chapter says 4. For each of the functions [...], decide for which numbers a the limit exists. i) ii) iii) iv) I found that, for , and , the limit doesn't exist when , namely, when a is an integer. But since in iv I have the floor function being used twice (once incrementing once decrementing), I'm not sure if I'm supposed to use the in the same way. What am I supposed to do here? How should I find inequalities for the new function?",lim_{x \to a}\ f(x) f(x) = \lfloor x \rfloor f(x) = x - \lfloor x \rfloor f(x) = \sqrt{x - \lfloor x \rfloor}  f(x) = \lfloor x \rfloor+ \sqrt{x - \lfloor x \rfloor} ... i ii iii a = \lfloor a \rfloor x-1 \lt \lfloor x \rfloor \le x,"['calculus', 'limits']"
80,question involving limit of product,question involving limit of product,,"I got stuck with this problem and it is unlike any problem I've encountered till now . Let $$P_n=\frac{2^3-1}{2^3+1}. \frac{3^3-1}{3^3+1} .\frac{4^3-1}{4^3+1}.....\frac{n^3-1}{n^3+1}$$ we have to prove that $\lim_{n\to\infty}P_n=\frac{2}{3}$. I could solve this type of problem if instead of product of terms I'd been given sum, by finding general term and all. But, I'm not able to think much after I saw the product. Maybe we can use sandwich theorem . but, i dont know how? Please help! Thanks in advance.","I got stuck with this problem and it is unlike any problem I've encountered till now . Let $$P_n=\frac{2^3-1}{2^3+1}. \frac{3^3-1}{3^3+1} .\frac{4^3-1}{4^3+1}.....\frac{n^3-1}{n^3+1}$$ we have to prove that $\lim_{n\to\infty}P_n=\frac{2}{3}$. I could solve this type of problem if instead of product of terms I'd been given sum, by finding general term and all. But, I'm not able to think much after I saw the product. Maybe we can use sandwich theorem . but, i dont know how? Please help! Thanks in advance.",,['limits']
81,Distributional limit,Distributional limit,,"Let $u_t(x) = t^Ne^{itx}$ for $x\geqslant 0$ and $u_t(x)=0$ elsewhere. I want to calculate the distributional limit $\lim_{t\to\infty}u_t(x)$. How would one approach such problem. I just started a little in functional analysis. Do we need to analyse $$\left\langle u_t,\phi \right\rangle = \int_{\mathbb{R}}u_t(x)\phi(x)dx ?$$ for $\phi \in C_0^{\infty}(\mathbb{R})$. My understanding on the subject is little.","Let $u_t(x) = t^Ne^{itx}$ for $x\geqslant 0$ and $u_t(x)=0$ elsewhere. I want to calculate the distributional limit $\lim_{t\to\infty}u_t(x)$. How would one approach such problem. I just started a little in functional analysis. Do we need to analyse $$\left\langle u_t,\phi \right\rangle = \int_{\mathbb{R}}u_t(x)\phi(x)dx ?$$ for $\phi \in C_0^{\infty}(\mathbb{R})$. My understanding on the subject is little.",,"['real-analysis', 'limits', 'distribution-theory', 'weak-convergence']"
82,Limit problem for $L^p$ function,Limit problem for  function,L^p,"I am having problems with proving the following: Let $f$ be a $L^p$ function on $[0,1]$ , $f:[0,1] \to \overline{\mathbb{R}}$ . Prove that $$\lim_{t \to \infty} t^p \mu(x: |f(x)| \geq t) = 0.$$ I know that the right-hand side value is always finite for each $t \in \mathbb{R}$ due to Chebyshev's inequality. I was also able to prove that $\lim_{t \to \infty} \mu(x: |f(x)| \geq t) = 0$ . But unfortunately can find anything about how fast this value goes to $0$ as $t \to \infty$ . Would be grateful, if you could give an idea and help with this.","I am having problems with proving the following: Let be a function on , . Prove that I know that the right-hand side value is always finite for each due to Chebyshev's inequality. I was also able to prove that . But unfortunately can find anything about how fast this value goes to as . Would be grateful, if you could give an idea and help with this.","f L^p [0,1] f:[0,1] \to \overline{\mathbb{R}} \lim_{t \to \infty} t^p \mu(x: |f(x)| \geq t) = 0. t \in \mathbb{R} \lim_{t \to \infty} \mu(x: |f(x)| \geq t) = 0 0 t \to \infty","['real-analysis', 'measure-theory', 'limits', 'lp-spaces']"
83,Two square roots in an indeterminate,Two square roots in an indeterminate,,"Square roots are always causing trouble for me - especially finding the limit of a function when it's in the indeterminate form. I got this far, but I don't know what to do next or even if it's right. Trying to go any further gives me my original problem. This is the initial problem $$ \lim_{x\to5}\frac{\sqrt{ x^2+5}-\sqrt{30}}{(x-5)} $$ This is where I've gotten $$ \lim_{x\to5}\frac{\sqrt{x^2+5}-\sqrt{30}}{(x-5)} * \frac{\sqrt{x^2+5}+\sqrt{30}}{\sqrt{x^2+5}+\sqrt{30}} = \frac{x^2+5-30}{(x-5)\left(\sqrt{x^2+5}+\sqrt{30}\right)} $$","Square roots are always causing trouble for me - especially finding the limit of a function when it's in the indeterminate form. I got this far, but I don't know what to do next or even if it's right. Trying to go any further gives me my original problem. This is the initial problem $$ \lim_{x\to5}\frac{\sqrt{ x^2+5}-\sqrt{30}}{(x-5)} $$ This is where I've gotten $$ \lim_{x\to5}\frac{\sqrt{x^2+5}-\sqrt{30}}{(x-5)} * \frac{\sqrt{x^2+5}+\sqrt{30}}{\sqrt{x^2+5}+\sqrt{30}} = \frac{x^2+5-30}{(x-5)\left(\sqrt{x^2+5}+\sqrt{30}\right)} $$",,"['calculus', 'limits']"
84,"Proof that $x>0$ and $r,s\in\mathbb R\implies(x^{r})^{s}=x^{rs}$",Proof that  and,"x>0 r,s\in\mathbb R\implies(x^{r})^{s}=x^{rs}","I want to  prove the identity $(x^{r})^{s}=x^{rs}$ for real exponents and positive base. My problem essentially boils down to this: I don't know how to prove that for $r_n,s_n\in\mathbb{Q},$ $$\lim_{n\to\infty}{\lim_{k\to\infty}{x^{r_ks_n}}}=\lim_{n\to\infty}{x^{r_ns_n}}.$$ Some of the results that are available for the proof: The other exponent identites $x^rx^s=x^{r+s}$ , and $x^ry^r=(xy)^r$ for real numbers The corresponding identity (and all other exponent equalities and inequalities) for rational exponents That every real number can be expressed as the limit of some increasing (or decreasing if needed) sequence of rational numbers $x^r$ defined as the limit of $x^{r_n},$ where $r_n$ is a sequence of rationals that converges to $r$ All the limit laws, including taking rational exponents inside and out of limits (not real ones of course) Any help would be much appreciated. Please let me know if you need further clarifications.","I want to  prove the identity for real exponents and positive base. My problem essentially boils down to this: I don't know how to prove that for Some of the results that are available for the proof: The other exponent identites , and for real numbers The corresponding identity (and all other exponent equalities and inequalities) for rational exponents That every real number can be expressed as the limit of some increasing (or decreasing if needed) sequence of rational numbers defined as the limit of where is a sequence of rationals that converges to All the limit laws, including taking rational exponents inside and out of limits (not real ones of course) Any help would be much appreciated. Please let me know if you need further clarifications.","(x^{r})^{s}=x^{rs} r_n,s_n\in\mathbb{Q}, \lim_{n\to\infty}{\lim_{k\to\infty}{x^{r_ks_n}}}=\lim_{n\to\infty}{x^{r_ns_n}}. x^rx^s=x^{r+s} x^ry^r=(xy)^r x^r x^{r_n}, r_n r","['real-analysis', 'limits', 'real-numbers', 'exponentiation']"
85,How to calculate $\lim_{x\to 0} \ (\sec x)^x$?,How to calculate ?,\lim_{x\to 0} \ (\sec x)^x,"How to calculate $\lim_{x\to 0} \ (\sec x)^x$ ? My attempt: $$\lim_{x\to 0} \ (\sec x)^x =\lim_{x\to 0}\ \left(\frac{1}{\cos x}\right)^x =\left(\frac{1}{\cos 0}\right)^0 =1.$$ We literally had to just input $x=0$ in the expression, and we got the value easily. Now see what my book did. My book's attempt: Let $y=(\sec x)^x$ . So, $\ln y=x\ln (\sec x)$ . Then $$\lim_{x\to 0} \ln y=\lim_{x\to 0} x\ln (\sec x)=0.$$ So, $\lim_{x\to 0} \ln y=0$ , or $\lim_{x\to 0} y=e^0=1$ . Why did my book overcomplicate this?","How to calculate ? My attempt: We literally had to just input in the expression, and we got the value easily. Now see what my book did. My book's attempt: Let . So, . Then So, , or . Why did my book overcomplicate this?",\lim_{x\to 0} \ (\sec x)^x \lim_{x\to 0} \ (\sec x)^x =\lim_{x\to 0}\ \left(\frac{1}{\cos x}\right)^x =\left(\frac{1}{\cos 0}\right)^0 =1. x=0 y=(\sec x)^x \ln y=x\ln (\sec x) \lim_{x\to 0} \ln y=\lim_{x\to 0} x\ln (\sec x)=0. \lim_{x\to 0} \ln y=0 \lim_{x\to 0} y=e^0=1,"['limits', 'solution-verification']"
86,Understanding the proof that specific sequence does not have limit,Understanding the proof that specific sequence does not have limit,,"In mathematical analysis book, there is example of sequence $a_n$ : $1$ , $0$ , $\frac{1}{2}$ , $\frac{1}{2}$ , $\frac{1}{3}$ , $\frac{2}{3}$ , $\frac{1}{4}$ , $\frac{3}{4}$ , $...$ Odd numbers are part of sequence: $a_n = \frac{1}{n}$ , $\lim\limits_{n\to \infty} a_n = 0$ Even numbers are part of sequence: $a_n = 1 - \frac{1}{n}$ , $\lim\limits_{n\to \infty} a_n = 1$ Assuming since two subsequences have two different limits, whole sequence cannot have limit. This one is clear to me, but where I am lost is explanation or an attempt to find that $\epsilon_0\in\mathbb{R}$ exists: $$|a_\tilde{n} - a| \geq \epsilon_0$$ i.e. there exists least one $\epsilon_0\in\mathbb{R}$ disproving definition of limit: $$|a_n - a| < \epsilon$$ for $a,\epsilon\in\mathbb{R}$ , $a$ is limit of sequence $a_n$ , $\forall n > n_0(\epsilon)$ ; $n,n_0\in\mathbb{N}$ . To show that sequence does not have limit, there is first proposition that for $\epsilon_0$ , where for each $n\in\mathbb{N}$ there exist $\tilde{n} > n, \tilde{n}\in\mathbb{N}$ such as $|a_\tilde{n} - a| \geq \epsilon_0$ , we choose number for $\epsilon_0$ : $$\epsilon_0 = \frac{1}{2}\max(|a|, |a - 1|)$$ For 1st assumption $|a - 1| \geq |a|$ , because: $$a_{2k} = 1 - \frac{1}{k} \quad \textrm{and} \quad \frac{1}{k} \to 0$$ there exists $k_0(\epsilon_0)\in\mathbb{N}$ , where for $k > k_0(\epsilon_0)$ is always: $$|a_{2k} - 1| < \epsilon_0, k\in\mathbb{N}$$ and therefore: $$|a_{2k} - a| = |a_{2k} - 1 + (1 - a)| \geq |1 - a| - |a_{2k} - 1| \geq 2\epsilon_0 - \epsilon_0 = \epsilon_0$$ (let's call this inequality $A$ ) Therefore for any $n$ therefore exists $ñ$ as even number, which has to satisfy following conditions: $$ñ > n \quad \textrm{and} \quad ñ > 2k_0(\epsilon_0)$$ For example: $$ñ = 2\max(n, k_0(\epsilon_0))$$ Then: $$a_{2k}\in \langle1 - \epsilon_0, 1\rangle, k > k_0(\epsilon_0)$$ and therefore difference between $a_\tilde{n}$ and $a$ has to be at least $\epsilon_0$ . For 2nd assumption $|a - 1| \leq |a|$ would turn out by analogy, that $a$ has to be different to at least $|a|/2$ members of $a_{2k - 1}$ for $k$ big enough. My question is, or rather I am stuck on how we got to the inequality $A$ in the first place based on previous assumptions and from that rest of assumptions that seem to stem from it. I am sharing photos as well as link to pdf source for this example (in Czech language): https://www2.karlin.mff.cuni.cz/~halas/MA/MA1/kopacek_-_mat._analyza_nejen_pro_fyziky_1.pdf PS: Let me know if something there is not clear, I will try to make it more understandable if needed.","In mathematical analysis book, there is example of sequence : , , , , , , , , Odd numbers are part of sequence: , Even numbers are part of sequence: , Assuming since two subsequences have two different limits, whole sequence cannot have limit. This one is clear to me, but where I am lost is explanation or an attempt to find that exists: i.e. there exists least one disproving definition of limit: for , is limit of sequence , ; . To show that sequence does not have limit, there is first proposition that for , where for each there exist such as , we choose number for : For 1st assumption , because: there exists , where for is always: and therefore: (let's call this inequality ) Therefore for any therefore exists as even number, which has to satisfy following conditions: For example: Then: and therefore difference between and has to be at least . For 2nd assumption would turn out by analogy, that has to be different to at least members of for big enough. My question is, or rather I am stuck on how we got to the inequality in the first place based on previous assumptions and from that rest of assumptions that seem to stem from it. I am sharing photos as well as link to pdf source for this example (in Czech language): https://www2.karlin.mff.cuni.cz/~halas/MA/MA1/kopacek_-_mat._analyza_nejen_pro_fyziky_1.pdf PS: Let me know if something there is not clear, I will try to make it more understandable if needed.","a_n 1 0 \frac{1}{2} \frac{1}{2} \frac{1}{3} \frac{2}{3} \frac{1}{4} \frac{3}{4} ... a_n = \frac{1}{n} \lim\limits_{n\to \infty} a_n = 0 a_n = 1 - \frac{1}{n} \lim\limits_{n\to \infty} a_n = 1 \epsilon_0\in\mathbb{R} |a_\tilde{n} - a| \geq \epsilon_0 \epsilon_0\in\mathbb{R} |a_n - a| < \epsilon a,\epsilon\in\mathbb{R} a a_n \forall n > n_0(\epsilon) n,n_0\in\mathbb{N} \epsilon_0 n\in\mathbb{N} \tilde{n} > n, \tilde{n}\in\mathbb{N} |a_\tilde{n} - a| \geq \epsilon_0 \epsilon_0 \epsilon_0 = \frac{1}{2}\max(|a|, |a - 1|) |a - 1| \geq |a| a_{2k} = 1 - \frac{1}{k} \quad \textrm{and} \quad \frac{1}{k} \to 0 k_0(\epsilon_0)\in\mathbb{N} k > k_0(\epsilon_0) |a_{2k} - 1| < \epsilon_0, k\in\mathbb{N} |a_{2k} - a| = |a_{2k} - 1 + (1 - a)| \geq |1 - a| - |a_{2k} - 1| \geq 2\epsilon_0 - \epsilon_0 = \epsilon_0 A n ñ ñ > n \quad \textrm{and} \quad ñ > 2k_0(\epsilon_0) ñ = 2\max(n, k_0(\epsilon_0)) a_{2k}\in \langle1 - \epsilon_0, 1\rangle, k > k_0(\epsilon_0) a_\tilde{n} a \epsilon_0 |a - 1| \leq |a| a |a|/2 a_{2k - 1} k A","['limits', 'analysis', 'proof-explanation']"
87,Show that $\lim_{n \to \infty} \frac{((n-1)!)! (n-1)!^{n!}}{(n!)!} = 0$,Show that,\lim_{n \to \infty} \frac{((n-1)!)! (n-1)!^{n!}}{(n!)!} = 0,"I need help showing that $$\lim_{n \to \infty} \frac{((n-1)!)! (n-1)!^{n!}}{(n!)!} = 0.$$ If I take logarithm, I get $\begin{align*} \lim_{n \to \infty} \ln(\frac{((n-1)!)! (n-1)!^{n!}}{(n!)!}) &= \lim_{n \to \infty} \ln(((n-1)!)!) + n! \ln((n-1)!) - \ln((n!)!) \\  &= \lim_{n \to \infty} \ln(n-1)+\ln(n-3)+...+\ln(3)\\& + n! (\ln(n-1)+\ln(n-2)+...+\ln(2))\\& - \ln(n) - \ln(n-2) - ... - \ln(2), \end{align*}$ but I don't see if that even helps.","I need help showing that If I take logarithm, I get but I don't see if that even helps.","\lim_{n \to \infty} \frac{((n-1)!)! (n-1)!^{n!}}{(n!)!} = 0. \begin{align*}
\lim_{n \to \infty} \ln(\frac{((n-1)!)! (n-1)!^{n!}}{(n!)!}) &= \lim_{n \to \infty} \ln(((n-1)!)!) + n! \ln((n-1)!) - \ln((n!)!) \\ 
&= \lim_{n \to \infty} \ln(n-1)+\ln(n-3)+...+\ln(3)\\& + n! (\ln(n-1)+\ln(n-2)+...+\ln(2))\\& - \ln(n) - \ln(n-2) - ... - \ln(2),
\end{align*}","['limits', 'logarithms', 'factorial']"
88,"A (geometric?) limit distribution for the ""delicacy width"" of prime numbers?","A (geometric?) limit distribution for the ""delicacy width"" of prime numbers?",,"[Revised to ask only the second of my two previous questions. Revised again to include the approximate Geometric distribution, and to add another reference.] A recent Quanta article refers to this paper by Filaseta & Southwick, which is behind a paywall. (EDIT: Presumably, Southwick's doctoral dissertation has similar content with no paywall.) The paper has the following Abstract (italics are mine): Abstract : We show that a positive proportion of the primes have the property that if any one of its digits in base $10$ , including its infinitely many leading $0$ digits, is replaced by a different digit, then the resulting number is composite . Primes having the italicized property are called widely digitally delicate in the article, which mentions that no example of such a prime has yet been found , in spite of there being infinitely many of them (indeed, a positive proportion). I take the Abstract to mean that $$P_\infty:=\lim_{n\to\infty}{\pi^*(n)\over\pi(n)}>0 $$ where $\pi()$ is the usual prime counting function and $\pi^*(n)$ counts the number of primes less than or equal to $n$ and satisfying the italicized property. Now, suppose we define the delicacy width (which I'll just call the ""width"") of a prime as follows: Definition : The width of a prime $p$ whose decimal expansion is $\ldots d_2 d_1 d_0$ ( including infinitely many leading $0$ s ) is the length of the longest suffix in $\ldots d_2 d_1 d_0$ with the property that changing any single digit in the suffix changes the expansion to that of a composite number. This defines the width as a function $W:PRIMES\to\mathbb{N}\cup\{\infty\},$ with $W(p)=\infty$ when the longest such suffix is the entire expansion of $p$ ; otherwise, $W(p)$ is finite and equal to the number of digits to the right of digit $r$ , where $r$ is the rightmost digit that can be altered to change the prime into different noncomposite number. Examples: All primes less than $97$ have width $0$ , because in each case the very rightmost digit (having no digits to its right) can be altered to change the prime into another noncomposite; e.g., $89$ can be changed into the noncomposite $83.$ On the other hand, $97$ has width= $1$ because the $9$ (having one digit to its right) is the rightmost digit that can be altered (to $0,1,3,4,$ or $6$ ) to change the prime into a different noncomposite -- altering the $7$ cannot accomplish this. Question : Since the quoted Abstract implies that a positive proportion ( $P_\infty$ ) of the primes have infinite width, does it follow that the remaining primes have widths $(0,1,2,\ldots)$ occurring in respective proportions $(P_0, P_1, \ldots)$ that sum to $1-P_\infty?$ Can it be concluded that all of these proportions are positive? In other words, does there exist a limit distribution $\{P_w\}_{w\in\mathbb{N}\cup\{\infty\}}$ with $(P_0+P_1+P_2+\ldots)+P_\infty=1,$ where $$P_k:=\lim_{n\to\infty}{\pi_k(n)\over\pi(n)}>0 \quad\text {for all $k\in\mathbb{N}\cup\{\infty\}$}$$ and $\pi_k(n)$ counts the number of primes that are less than or equal to $n$ and have width equal to $k$ ? Here are plots I've computed of the distribution of widths among the first $10^n$ primes, for $n=6,7,8,9(\text{blue})$ -- there seems to plenty of ""converging"" yet to occur, if convergence is indeed what happens: The greatest widths occurring in those respective cases are $14, 16, 24,$ and $31,$ although the plots show only up to width= $8$ . (Elsewhere, the widest prime I've found is $142243533671$ , with width= $59$ . As mentioned above, no one has yet found a prime of the kind described in the Abstract, i.e. a prime of infinite width.) To get some inkling of what happens for much larger primes and yet keep the computing time feasible, I also looked at the width distribution among the first $10^6$ primes after each number of form $10^n,$ for $n=8,10,12,...,30(\text{blue})$ -- the greatest width to occur overall was $53$ , but I plot only up to width= $15$ : Approximate Geometric Distribution Examining these plots numerically, it appears that among sufficiently large primes, the distribution of positive integer widths (i.e. $0<W<\infty$ ) is approximately geometric . That is, the width distributions appear to be approximately as follows (taking $P_0, P_1,$ and $P_\infty$ as parameters): $$P_k =\begin{cases} P_0  &\text{if }k=0\\ P_1\,a^{k-1} &\text{if }1\le k\lt\infty\\ P_\infty &\text{if }k=\infty  \end{cases}$$ where $a=1-{{P_1\over 1-P_0-P_\infty}}$ in order to make $P_0+(P_1+P_2+\ldots)+P_\infty=1.$ Here's a comparison of the empirical distribution for $n=30$ (blue) from the preceding picture, and a geometric distribution (red) fitted with $P_0=0.09833$ and $P_1=0.2354$ to match the empirical values, and taking $P_\infty=10^{-9}$ just as some small number (known to be positive even though no instance of infinite width has been found among more than $10^9$ primes). At the scale of the plot, the two distributions visually overlap: NB : The width of a prime can be seen as the outcome of a sequence of ""trials"" on the successive digits $d_0,d_1,d_2,\ldots$ , each trial asking ""Can this digit be changed so as to change the expansion into that of another noncomposite number?"" -- if the answer is ""yes"", then the trial is a ""success"", otherwise it's a ""failure"". The width is then just the number of failures before an eventual success (if there is never a success, then the width is infinite). Empirically, it appears that only trials after $d_0$ (i.e. only the trials on $d_1,d_2,\ldots$ ) can be modelled as ""Bernoulli trials"", and so give rise to a geometric distribution only for $0\lt W\lt\infty.$","[Revised to ask only the second of my two previous questions. Revised again to include the approximate Geometric distribution, and to add another reference.] A recent Quanta article refers to this paper by Filaseta & Southwick, which is behind a paywall. (EDIT: Presumably, Southwick's doctoral dissertation has similar content with no paywall.) The paper has the following Abstract (italics are mine): Abstract : We show that a positive proportion of the primes have the property that if any one of its digits in base , including its infinitely many leading digits, is replaced by a different digit, then the resulting number is composite . Primes having the italicized property are called widely digitally delicate in the article, which mentions that no example of such a prime has yet been found , in spite of there being infinitely many of them (indeed, a positive proportion). I take the Abstract to mean that where is the usual prime counting function and counts the number of primes less than or equal to and satisfying the italicized property. Now, suppose we define the delicacy width (which I'll just call the ""width"") of a prime as follows: Definition : The width of a prime whose decimal expansion is ( including infinitely many leading s ) is the length of the longest suffix in with the property that changing any single digit in the suffix changes the expansion to that of a composite number. This defines the width as a function with when the longest such suffix is the entire expansion of ; otherwise, is finite and equal to the number of digits to the right of digit , where is the rightmost digit that can be altered to change the prime into different noncomposite number. Examples: All primes less than have width , because in each case the very rightmost digit (having no digits to its right) can be altered to change the prime into another noncomposite; e.g., can be changed into the noncomposite On the other hand, has width= because the (having one digit to its right) is the rightmost digit that can be altered (to or ) to change the prime into a different noncomposite -- altering the cannot accomplish this. Question : Since the quoted Abstract implies that a positive proportion ( ) of the primes have infinite width, does it follow that the remaining primes have widths occurring in respective proportions that sum to Can it be concluded that all of these proportions are positive? In other words, does there exist a limit distribution with where and counts the number of primes that are less than or equal to and have width equal to ? Here are plots I've computed of the distribution of widths among the first primes, for -- there seems to plenty of ""converging"" yet to occur, if convergence is indeed what happens: The greatest widths occurring in those respective cases are and although the plots show only up to width= . (Elsewhere, the widest prime I've found is , with width= . As mentioned above, no one has yet found a prime of the kind described in the Abstract, i.e. a prime of infinite width.) To get some inkling of what happens for much larger primes and yet keep the computing time feasible, I also looked at the width distribution among the first primes after each number of form for -- the greatest width to occur overall was , but I plot only up to width= : Approximate Geometric Distribution Examining these plots numerically, it appears that among sufficiently large primes, the distribution of positive integer widths (i.e. ) is approximately geometric . That is, the width distributions appear to be approximately as follows (taking and as parameters): where in order to make Here's a comparison of the empirical distribution for (blue) from the preceding picture, and a geometric distribution (red) fitted with and to match the empirical values, and taking just as some small number (known to be positive even though no instance of infinite width has been found among more than primes). At the scale of the plot, the two distributions visually overlap: NB : The width of a prime can be seen as the outcome of a sequence of ""trials"" on the successive digits , each trial asking ""Can this digit be changed so as to change the expansion into that of another noncomposite number?"" -- if the answer is ""yes"", then the trial is a ""success"", otherwise it's a ""failure"". The width is then just the number of failures before an eventual success (if there is never a success, then the width is infinite). Empirically, it appears that only trials after (i.e. only the trials on ) can be modelled as ""Bernoulli trials"", and so give rise to a geometric distribution only for","10 0 P_\infty:=\lim_{n\to\infty}{\pi^*(n)\over\pi(n)}>0  \pi() \pi^*(n) n p \ldots d_2 d_1 d_0 0 \ldots d_2 d_1 d_0 W:PRIMES\to\mathbb{N}\cup\{\infty\}, W(p)=\infty p W(p) r r 97 0 89 83. 97 1 9 0,1,3,4, 6 7 P_\infty (0,1,2,\ldots) (P_0, P_1, \ldots) 1-P_\infty? \{P_w\}_{w\in\mathbb{N}\cup\{\infty\}} (P_0+P_1+P_2+\ldots)+P_\infty=1, P_k:=\lim_{n\to\infty}{\pi_k(n)\over\pi(n)}>0 \quad\text {for all k\in\mathbb{N}\cup\{\infty\}} \pi_k(n) n k 10^n n=6,7,8,9(\text{blue}) 14, 16, 24, 31, 8 142243533671 59 10^6 10^n, n=8,10,12,...,30(\text{blue}) 53 15 0<W<\infty P_0, P_1, P_\infty P_k =\begin{cases}
P_0  &\text{if }k=0\\
P_1\,a^{k-1} &\text{if }1\le k\lt\infty\\
P_\infty &\text{if }k=\infty 
\end{cases} a=1-{{P_1\over 1-P_0-P_\infty}} P_0+(P_1+P_2+\ldots)+P_\infty=1. n=30 P_0=0.09833 P_1=0.2354 P_\infty=10^{-9} 10^9 d_0,d_1,d_2,\ldots d_0 d_1,d_2,\ldots 0\lt W\lt\infty.","['limits', 'number-theory', 'probability-distributions', 'prime-numbers', 'asymptotics']"
89,Truncated Taylor series of the exponential,Truncated Taylor series of the exponential,,"Let $N \in \mathbb N^*$ , $\delta > 0$ , $t > 0$ and consider \begin{equation} f(t,\delta, N)= e^{-t/\delta} \sum_{k=0}^{N-1}\frac{(t/\delta)^k}{k!}. \qquad \qquad \qquad (1) \end{equation} Let now $N = \lceil\delta^{-\gamma}\rceil$ , with $0 < \gamma < 1$ . Can we conclude that \begin{equation} \lim_{\delta \to 0} f(t,\delta,\lceil \delta^{-\gamma}\rceil)=0, \end{equation} for all $t > 0$ ? The reasoning is that if $N \to \infty$ (for $\delta$ fixed), then the sum tends to $e^{t/\delta}$ and therefore $f(t,\delta,N)\to 1$ . If on the other hand $\delta \to 0$ , for $N$ fixed, we have that $f(t,\delta,N)\to 0$ since the exponential goes faster to zero than the polynomial to infinity. In this case, we let $\delta \to 0$ and simultaneously $N \to \infty$ , but with a ""slower pace"". Can we conclude that the limit is zero? If yes, how? Edit: After some reasoning I came up with $$ f(t,\delta,N)= \frac{\Gamma(N, t/\delta)}{(N-1)!}, $$ with $\Gamma(\cdot, \cdot)$ the upper incomplete Gamma function. I implemented this in Matlab, and the interesting behaviour is that if $N=\delta^{-1}$ , then the limit (1) is $1/2$ if $0 <t \leq 1$ and it is 0 if $t > 1$ . If $N=\delta^{-\gamma}$ , with $\gamma \in (0, 1)$ , the limit is 0, and if $N = \delta^{-\gamma}$ , with $\gamma > 1$ , then the limit is 1, independently of $t$ .","Let , , and consider Let now , with . Can we conclude that for all ? The reasoning is that if (for fixed), then the sum tends to and therefore . If on the other hand , for fixed, we have that since the exponential goes faster to zero than the polynomial to infinity. In this case, we let and simultaneously , but with a ""slower pace"". Can we conclude that the limit is zero? If yes, how? Edit: After some reasoning I came up with with the upper incomplete Gamma function. I implemented this in Matlab, and the interesting behaviour is that if , then the limit (1) is if and it is 0 if . If , with , the limit is 0, and if , with , then the limit is 1, independently of .","N \in \mathbb N^* \delta > 0 t > 0 \begin{equation}
f(t,\delta, N)= e^{-t/\delta} \sum_{k=0}^{N-1}\frac{(t/\delta)^k}{k!}. \qquad \qquad \qquad (1)
\end{equation} N = \lceil\delta^{-\gamma}\rceil 0 < \gamma < 1 \begin{equation}
\lim_{\delta \to 0} f(t,\delta,\lceil \delta^{-\gamma}\rceil)=0,
\end{equation} t > 0 N \to \infty \delta e^{t/\delta} f(t,\delta,N)\to 1 \delta \to 0 N f(t,\delta,N)\to 0 \delta \to 0 N \to \infty 
f(t,\delta,N)= \frac{\Gamma(N, t/\delta)}{(N-1)!},
 \Gamma(\cdot, \cdot) N=\delta^{-1} 1/2 0 <t \leq 1 t > 1 N=\delta^{-\gamma} \gamma \in (0, 1) N = \delta^{-\gamma} \gamma > 1 t","['calculus', 'limits', 'taylor-expansion', 'exponential-function']"
90,Limit of $\min \{a\in \mathbb{N} : \sum_{i=1}^{a}\frac1{i}\geq n\}$ equals $e$,Limit of  equals,\min \{a\in \mathbb{N} : \sum_{i=1}^{a}\frac1{i}\geq n\} e,"Given $a_n = \min \{a\in \mathbb{N} : \sum_{i=1}^{a}\frac1{i}\geq n\}$ $\forall n \in \mathbb{N}$ , I want to prove that: $$\lim_{n\to\infty}\frac{a_{n+1}}{a_n}=e$$ For that purpose, I am trying to prove that $\log a_{n+1}-\log a_n \to 1$ . From the definition of $a_n$ , we have that $1+\frac12+\ldots+\frac1{a_n}\geq n$ , and we have: $$n\leq 1+\frac12+\ldots+\frac1{a_n}\leq \int_1^{a_n+1}\frac1{x}dx = \log(a_n+1)$$ In the same way, for $a_{n+1}$ we have $n+1\leq \log(a_{n+1}+1)$ . Combining both inequalities we have: $$\log (a_{n+1}+1)-\log (a_n+1) \geq 1$$ However, I don't know how to get the other inequality. Can somebody help me? Many thanks in advance!","Given , I want to prove that: For that purpose, I am trying to prove that . From the definition of , we have that , and we have: In the same way, for we have . Combining both inequalities we have: However, I don't know how to get the other inequality. Can somebody help me? Many thanks in advance!",a_n = \min \{a\in \mathbb{N} : \sum_{i=1}^{a}\frac1{i}\geq n\} \forall n \in \mathbb{N} \lim_{n\to\infty}\frac{a_{n+1}}{a_n}=e \log a_{n+1}-\log a_n \to 1 a_n 1+\frac12+\ldots+\frac1{a_n}\geq n n\leq 1+\frac12+\ldots+\frac1{a_n}\leq \int_1^{a_n+1}\frac1{x}dx = \log(a_n+1) a_{n+1} n+1\leq \log(a_{n+1}+1) \log (a_{n+1}+1)-\log (a_n+1) \geq 1,"['real-analysis', 'limits', 'logarithms', 'eulers-number-e']"
91,How to evaluate limit of a sequence $ \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}} $,How to evaluate limit of a sequence, \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}} ,"I need a help with evaluating a limit of a sequence $$ \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}}. $$ The problem is that $\lim_{n \to \infty} \left( -2 \right)^n$ does not exist. What can we even tell from this when there's a part oscilating between $+\infty$ and $- \infty$ . Wolfram says it should equal to $ - \infty$ , but how to get there? The only thing I know might help is to factor out the fastest growing terms. $$ \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}} = \frac{1}{3}\lim_{n \to \infty} \frac {2 \cdot 3^{2n} - \left( -2 \right)^n}{2 \cdot 3^n - 6 \cdot 2^{2n}} = \frac{1}{3} \lim_{n \to \infty} \left( \frac {3}{2} \right)^{2n} \cdot \frac {2 -  \left( - \frac {2} {9} \right)^n}{2 \cdot \left( \frac{3}{4} \right)^n - 6} $$ Now we can see that $\left( \frac {3} {4} \right)^n $ goes to zero. What about $\left(- \frac{2}{9} \right)^n $ ? I guess that despite the fact that the values oscilate between $+$ and $-$ , the overall fraction has to go to zero, hence giving $$\left| \frac{1}{3} \cdot \infty \cdot \frac{2}{-6} \right| = \left| - \frac{\infty}{9} \right| = -\infty$$ Does that make sense?","I need a help with evaluating a limit of a sequence The problem is that does not exist. What can we even tell from this when there's a part oscilating between and . Wolfram says it should equal to , but how to get there? The only thing I know might help is to factor out the fastest growing terms. Now we can see that goes to zero. What about ? I guess that despite the fact that the values oscilate between and , the overall fraction has to go to zero, hence giving Does that make sense?"," \lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}}.  \lim_{n \to \infty} \left( -2 \right)^n +\infty - \infty  - \infty 
\lim_{n \to \infty} \frac{2 \cdot 3^{2n - 1} - \left( -2 \right)^n}{2 \cdot 3^n - 3 \cdot 2^{2n + 1}}
=
\frac{1}{3}\lim_{n \to \infty} \frac {2 \cdot 3^{2n} - \left( -2 \right)^n}{2 \cdot 3^n - 6 \cdot 2^{2n}}
=
\frac{1}{3} \lim_{n \to \infty} \left( \frac {3}{2} \right)^{2n} \cdot \frac {2 -  \left( - \frac {2} {9} \right)^n}{2 \cdot \left( \frac{3}{4} \right)^n - 6}
 \left( \frac {3} {4} \right)^n  \left(- \frac{2}{9} \right)^n  + - \left| \frac{1}{3} \cdot \infty \cdot \frac{2}{-6} \right| = \left| - \frac{\infty}{9} \right| = -\infty","['calculus', 'limits']"
92,$\exists n$ such that $3M \geq \left|\frac {f(x+ \frac {y-x} ni )-f(x+ \frac {y-x} n (i-1)) } {\frac {y-x} n}\right|$ for each $1 \leq i \leq n$?,such that  for each ?,\exists n 3M \geq \left|\frac {f(x+ \frac {y-x} ni )-f(x+ \frac {y-x} n (i-1)) } {\frac {y-x} n}\right| 1 \leq i \leq n,"Let $f:[a,b] \to \mathbb{R}$ be continous. Assume $-M \leq \underline Df(x) \leq \overline Df(x) \leq M$ on $[a,b].$ We are using the notation in Royden & Fitzpatrick : $$ \begin{split} \overline Df(x)&:=\lim_{h \to 0^+} \sup\left\{\frac {f(x+t)-f(x)} t:0<|t|<h\right\}\\ \underline Df(x)&:=\lim_{h \to 0^+} \inf\left\{\frac {f(x+t)-f(x)} t:0<|t|<h\right\} \end{split} $$ Q: Is it true that there exists some positive integer $n$ such that, for each, $1 \leq i \leq n$ , we have $$ 3M \geq \left|\frac {f\big(x+ \frac {y-x} ni \big)-f\big(x+ \frac {y-x} n (i-1)\big) } {\frac {y-x} n}\right|\;? $$ I am not 100% sure, but I think the answer to to the highlighted question is ""yes."" (In which case, it would be a helpful lemma for me to prove something more significant.) Naively, the first thought is to take a sort of ""limit"" so we could say something about the quantity $$ \left|\frac{f\big(x+ \frac {y-x} ni \big)-f\big(x+ \frac {y-x} n (i-1)\big) } {\frac {y-x} n}\right| $$ as $n \to \infty$ , comparing it to the upper and lower Dini derivatives which are bounded by $M$ . However, the order of quantifiers makes $i$ dependent on $n$ so I am having trouble making this rigorous. I also haven't yet taken advantage of the assumption that $f$ is continous, so perhaps continuity helps somewhow?","Let be continous. Assume on We are using the notation in Royden & Fitzpatrick : Q: Is it true that there exists some positive integer such that, for each, , we have I am not 100% sure, but I think the answer to to the highlighted question is ""yes."" (In which case, it would be a helpful lemma for me to prove something more significant.) Naively, the first thought is to take a sort of ""limit"" so we could say something about the quantity as , comparing it to the upper and lower Dini derivatives which are bounded by . However, the order of quantifiers makes dependent on so I am having trouble making this rigorous. I also haven't yet taken advantage of the assumption that is continous, so perhaps continuity helps somewhow?","f:[a,b] \to \mathbb{R} -M \leq \underline Df(x) \leq \overline Df(x) \leq M [a,b]. 
\begin{split}
\overline Df(x)&:=\lim_{h \to 0^+} \sup\left\{\frac {f(x+t)-f(x)} t:0<|t|<h\right\}\\
\underline Df(x)&:=\lim_{h \to 0^+} \inf\left\{\frac {f(x+t)-f(x)} t:0<|t|<h\right\}
\end{split}
 n 1 \leq i \leq n 
3M \geq \left|\frac {f\big(x+ \frac {y-x} ni \big)-f\big(x+ \frac {y-x} n (i-1)\big) } {\frac {y-x} n}\right|\;?
 
\left|\frac{f\big(x+ \frac {y-x} ni \big)-f\big(x+ \frac {y-x} n (i-1)\big) } {\frac {y-x} n}\right|
 n \to \infty M i n f","['real-analysis', 'limits', 'derivatives', 'continuity']"
93,Compute $\lim_{x\rightarrow 0 }\biggr ( \dfrac{1}{x}\ln (x!)\biggr )$,Compute,\lim_{x\rightarrow 0 }\biggr ( \dfrac{1}{x}\ln (x!)\biggr ),"I want to compute the following limit $$\lim_{x\rightarrow 0 }\biggr ( \dfrac{1}{x}\ln (x!)\biggr )$$ Since factorial is only defined for integers, we must use the gamma function. $$\lim_{x\rightarrow 0} \dfrac{\ln (\Gamma (x+1))}{x} = \lim_{x\rightarrow 0}\dfrac{\dfrac{d}{dx}(\ln(\Gamma(x+1))}{\dfrac{d}{dx}(x)} = \lim_{x\rightarrow 0} \dfrac{\Gamma'(x+1)}{\Gamma(x+1)} = \psi(1)$$ Where $\psi$ is the digamma function. $$\psi(x+1) = -\gamma +\int^{1}_{0}\dfrac{1-t^{x}}{1-t}dt$$ What we want is $$\psi(1) = -\gamma +\int^{1}_{0}\dfrac{1-t}{1-t}dt = -\gamma$$ where $\gamma $ is Euler-Mascheroni constant. Is there a way to compute this limit without using digamma function?","I want to compute the following limit Since factorial is only defined for integers, we must use the gamma function. Where is the digamma function. What we want is where is Euler-Mascheroni constant. Is there a way to compute this limit without using digamma function?",\lim_{x\rightarrow 0 }\biggr ( \dfrac{1}{x}\ln (x!)\biggr ) \lim_{x\rightarrow 0} \dfrac{\ln (\Gamma (x+1))}{x} = \lim_{x\rightarrow 0}\dfrac{\dfrac{d}{dx}(\ln(\Gamma(x+1))}{\dfrac{d}{dx}(x)} = \lim_{x\rightarrow 0} \dfrac{\Gamma'(x+1)}{\Gamma(x+1)} = \psi(1) \psi \psi(x+1) = -\gamma +\int^{1}_{0}\dfrac{1-t^{x}}{1-t}dt \psi(1) = -\gamma +\int^{1}_{0}\dfrac{1-t}{1-t}dt = -\gamma \gamma ,"['limits', 'gamma-function', 'digamma-function']"
94,Is $\frac{n}{\phi(n)}$ bounded if $\phi(n) = \phi(n+1)$?,Is  bounded if ?,\frac{n}{\phi(n)} \phi(n) = \phi(n+1),"Let $\phi(x)$ be the Euler totient function. Is $\dfrac{n}{\phi(n)}$ bounded if $\phi(n) = \phi(n+1)$ ? My experimental data for $n \le 10^{13}$ suggests that: Claim : If $\phi(n) = \phi(n+1)$ then, $$\lim \inf \dfrac{n}{\phi(n)} \ge 2 \text{, } \lim \sup \dfrac{n}{\phi(n)} < 3$$ Given below if the plot of all the $1,014$ solutions below $4.7 \times 10^9$ Related Question: Conjecture on the gap between integers having the same number of co-primes","Let be the Euler totient function. Is bounded if ? My experimental data for suggests that: Claim : If then, Given below if the plot of all the solutions below Related Question: Conjecture on the gap between integers having the same number of co-primes","\phi(x) \dfrac{n}{\phi(n)} \phi(n) = \phi(n+1) n \le 10^{13} \phi(n) = \phi(n+1) \lim \inf \dfrac{n}{\phi(n)} \ge 2
\text{, } \lim \sup \dfrac{n}{\phi(n)} < 3 1,014 4.7 \times 10^9","['number-theory', 'limits', 'prime-numbers', 'divisibility']"
95,Trouble justifying moving the limit inside the following integral,Trouble justifying moving the limit inside the following integral,,"The question is: Find the following limit $$\lim\limits_{\epsilon \rightarrow 0} \int_0^{\infty} \frac{\sin{x}}{x}\arctan{(\frac{x}{\epsilon})}dx$$ I know that if I can move the limit in the integral the arctangent will become $\frac{\pi}{2}$ and then the remaining problem is just the Dirichlet integral. However, I am having trouble justifying that step.","The question is: Find the following limit I know that if I can move the limit in the integral the arctangent will become and then the remaining problem is just the Dirichlet integral. However, I am having trouble justifying that step.",\lim\limits_{\epsilon \rightarrow 0} \int_0^{\infty} \frac{\sin{x}}{x}\arctan{(\frac{x}{\epsilon})}dx \frac{\pi}{2},"['limits', 'analysis']"
96,"Prob. 16 (c), Sec. 6.2, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: If $f^\prime(x)\to b$ as $x \to \infty$, then $f(x)/x \to b$. How?","Prob. 16 (c), Sec. 6.2, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: If  as , then . How?",f^\prime(x)\to b x \to \infty f(x)/x \to b,"Here is Prob. 16, Sec. 6.1, in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let $f \colon [0, \infty) \to \mathbb{R}$ be differentiable on $(0, \infty)$ and assume that $f^\prime(x) \to b$ as $x \to \infty$ . (a) Show that for any $h > 0$ , we have $\lim_{x \to \infty} \big( f(x+h) - f(x) \big)/h = b$ . (b) Show that if $f(x) \to a $ as $x \to \infty$ , then $b = 0$ . (c) Show that $\lim_{x \to \infty} \big( f(x)/x \big) = b$ . Here is Definition 4.3.10 in Bartle & Sherbert, 4th edition: Let $A \subseteq \mathbb{R}$ and let $f \colon A \to \mathbb{R}$ . Suppose that $(a, \infty) \subseteq A$ for some $a \in \mathbb{R}$ . We say that $L \in \mathbb{R}$ is a limit of $f$ as $x \to \infty$ , and write $$ \lim_{x \to \infty} f = L \qquad \mbox{ or } \qquad \lim_{x \to \infty} f(x) = L, $$ if given any $\varepsilon > 0$ there exists $K = K(\varepsilon) > a$ such that for any $x > K$ , then $\lvert f(x) - L \rvert < \varepsilon$ . I think I'm clear on Parts (a) and (b). How to do Part (c)? Here is my attempt at Part (c): As $f^\prime$ is defined for all $x$ in $(0, \infty)$ and as $f^\prime(x) \to b$ as $x \to \infty$ , so given any $\varepsilon > 0$ we can find a $K \colon= K(\varepsilon/3) > 0$ such that for any $x > K$ , we have $$ \left\lvert f^\prime(x) - b \right\rvert < \frac{\varepsilon}{3}. \tag{0}$$ In particular, for $\varepsilon = 1$ , there exists a real number $K_0 \colon= K(1) > 0$ such that for any $x > K_0$ , we have $$ \left\lvert f^\prime(x) - b \right\rvert < 1.$$ So for any $x > K_0$ , we also have $$ \left\lvert f^\prime(x) \right\rvert \leq \left\lvert f^\prime(x) - b \right\rvert + \lvert b \rvert < 1 + \lvert b \rvert, $$ and thus $$ \left\lvert f^\prime(x) \right\rvert < 1 + \lvert b \rvert \tag{1} $$ for any real number $x > K_0$ . Now let $M$ be any real number such that $$ M > \max \left\{ \ K, K_0  \ \right\}. \tag{2}$$ Let us take $x$ to be any real number such that $$x >  \max\left\{ \ M, \frac{3 \big( \lvert f(M) \rvert + 1 \big) }{\varepsilon }, \frac{3 M \big( \lvert b \rvert + 1 \big)}{  \varepsilon } \ \right\}. \tag{3}. $$ Then as $f$ is continuous on the closed interval $[ M, x]$ and differentiable on the open interval $( M, x)$ , so there exists a real number $c_x \in (M, x)$ such that $$ f(x) - f(M) = (x-M) f^\prime \left( c_x \right),  $$ and then $$ f(x) = f(M) + (x-M) f^\prime \left( c_x \right),  $$ which implies that $$ \begin{align} & \ \ \  \left\lvert \frac{f(x)}{x} - b \right\rvert \\  &=  \left\lvert \frac{ f(M) + (x-M) f^\prime \left( c_x \right) }{x} - b \right\rvert \\ &= \left\lvert f^\prime \left( c_x \right) - b + \frac{f(M)}{x} - \frac{M f^\prime\left(c_x\right)}{x} \right\rvert \\ &\leq \left\lvert f^\prime \left( c_x \right) - b \right\rvert +  \frac{ \left\lvert f(M) \right\rvert }{x}  + \frac{ M \left\lvert f^\prime\left(c_x\right) \right\rvert }{x} \\ & \ \ \ \mbox{ [ Note that our choice of $K$, $K_0$, and $M$ in (0), (1), and (2) above, respectively, implies that $x > 0$ also. ] } \\ &< \frac{\varepsilon}{3} + \frac{ \left\lvert f(M) \right\rvert }{x}  + \frac{M}{x} \left( \lvert b \rvert + 1 \right) \\  & \ \ \  \mbox{ [ using (2), and then (0) and (1) above ] } \\  & < \frac{\varepsilon}{3} + \frac{\lvert f(M)\rvert \varepsilon}{3 \big( \lvert f(M) \rvert + 1 \big) } + \frac{\varepsilon}{3 \left( \lvert b \rvert + 1 \right) } \left( \lvert b \rvert + 1 \right) \\ & \qquad \qquad \mbox{ [ using (3) above ] }\\ &\leq \frac{\varepsilon}{3} + \frac{ \varepsilon }{ 3 } + \frac{\varepsilon }{3} \\ &= \varepsilon. \end{align} $$ Let us put $$ M^* \colon= \max\left\{ \ M, \frac{3 \big( \lvert f(M) \rvert + 1 \big) }{\varepsilon }, \frac{3 M \big( \lvert b \rvert + 1 \big)}{  \varepsilon } \ \right\}. \tag{4}. $$ Thus we have shown that, for any given real number $\varepsilon > 0$ , there exists a real number $M^* > 0$ such that $$ \left\lvert \frac{f(x)}{x} - b \right\rvert < \varepsilon $$ for every real number $x > M^*$ . Therefore $$ \lim_{x \to \infty} \frac{f(x)}{x} = b, $$ as required. Is this proof correct? If so, is it rigorous and clear enough too? Or, are there any problems in it?","Here is Prob. 16, Sec. 6.1, in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let be differentiable on and assume that as . (a) Show that for any , we have . (b) Show that if as , then . (c) Show that . Here is Definition 4.3.10 in Bartle & Sherbert, 4th edition: Let and let . Suppose that for some . We say that is a limit of as , and write if given any there exists such that for any , then . I think I'm clear on Parts (a) and (b). How to do Part (c)? Here is my attempt at Part (c): As is defined for all in and as as , so given any we can find a such that for any , we have In particular, for , there exists a real number such that for any , we have So for any , we also have and thus for any real number . Now let be any real number such that Let us take to be any real number such that Then as is continuous on the closed interval and differentiable on the open interval , so there exists a real number such that and then which implies that Let us put Thus we have shown that, for any given real number , there exists a real number such that for every real number . Therefore as required. Is this proof correct? If so, is it rigorous and clear enough too? Or, are there any problems in it?","f \colon [0, \infty) \to \mathbb{R} (0, \infty) f^\prime(x) \to b x \to \infty h > 0 \lim_{x \to \infty} \big( f(x+h) - f(x) \big)/h = b f(x) \to a  x \to \infty b = 0 \lim_{x \to \infty} \big( f(x)/x \big) = b A \subseteq \mathbb{R} f \colon A \to \mathbb{R} (a, \infty) \subseteq A a \in \mathbb{R} L \in \mathbb{R} f x \to \infty  \lim_{x \to \infty} f = L \qquad \mbox{ or } \qquad \lim_{x \to \infty} f(x) = L,  \varepsilon > 0 K = K(\varepsilon) > a x > K \lvert f(x) - L \rvert < \varepsilon f^\prime x (0, \infty) f^\prime(x) \to b x \to \infty \varepsilon > 0 K \colon= K(\varepsilon/3) > 0 x > K  \left\lvert f^\prime(x) - b \right\rvert < \frac{\varepsilon}{3}. \tag{0} \varepsilon = 1 K_0 \colon= K(1) > 0 x > K_0  \left\lvert f^\prime(x) - b \right\rvert < 1. x > K_0  \left\lvert f^\prime(x) \right\rvert \leq \left\lvert f^\prime(x) - b \right\rvert + \lvert b \rvert < 1 + \lvert b \rvert,   \left\lvert f^\prime(x) \right\rvert < 1 + \lvert b \rvert \tag{1}  x > K_0 M  M > \max \left\{ \ K, K_0  \ \right\}. \tag{2} x x >  \max\left\{ \ M, \frac{3 \big( \lvert f(M) \rvert + 1 \big) }{\varepsilon }, \frac{3 M \big( \lvert b \rvert + 1 \big)}{  \varepsilon } \ \right\}. \tag{3}.  f [ M, x] ( M, x) c_x \in (M, x)  f(x) - f(M) = (x-M) f^\prime \left( c_x \right),    f(x) = f(M) + (x-M) f^\prime \left( c_x \right),   
\begin{align}
& \ \ \  \left\lvert \frac{f(x)}{x} - b \right\rvert \\ 
&=  \left\lvert \frac{ f(M) + (x-M) f^\prime \left( c_x \right) }{x} - b \right\rvert \\
&= \left\lvert f^\prime \left( c_x \right) - b + \frac{f(M)}{x} - \frac{M f^\prime\left(c_x\right)}{x} \right\rvert \\
&\leq \left\lvert f^\prime \left( c_x \right) - b \right\rvert +  \frac{ \left\lvert f(M) \right\rvert }{x}  + \frac{ M \left\lvert f^\prime\left(c_x\right) \right\rvert }{x} \\
& \ \ \ \mbox{ [ Note that our choice of K, K_0, and M in (0), (1), and (2) above, respectively, implies that x > 0 also. ] } \\
&< \frac{\varepsilon}{3} + \frac{ \left\lvert f(M) \right\rvert }{x}  + \frac{M}{x} \left( \lvert b \rvert + 1 \right) \\ 
& \ \ \  \mbox{ [ using (2), and then (0) and (1) above ] } \\ 
& < \frac{\varepsilon}{3} + \frac{\lvert f(M)\rvert \varepsilon}{3 \big( \lvert f(M) \rvert + 1 \big) } + \frac{\varepsilon}{3 \left( \lvert b \rvert + 1 \right) } \left( \lvert b \rvert + 1 \right) \\
& \qquad \qquad \mbox{ [ using (3) above ] }\\
&\leq \frac{\varepsilon}{3} + \frac{ \varepsilon }{ 3 } + \frac{\varepsilon }{3} \\
&= \varepsilon.
\end{align}
  M^* \colon= \max\left\{ \ M, \frac{3 \big( \lvert f(M) \rvert + 1 \big) }{\varepsilon }, \frac{3 M \big( \lvert b \rvert + 1 \big)}{  \varepsilon } \ \right\}. \tag{4}.  \varepsilon > 0 M^* > 0  \left\lvert \frac{f(x)}{x} - b \right\rvert < \varepsilon  x > M^*  \lim_{x \to \infty} \frac{f(x)}{x} = b, ","['real-analysis', 'limits', 'analysis', 'derivatives', 'solution-verification']"
97,Prove $\lim_{x\rightarrow -3} 1-4x=13$,Prove,\lim_{x\rightarrow -3} 1-4x=13,"Problem Prove $$\lim_{x\rightarrow -3} 1-4x=13$$ Using $\delta, \epsilon$ definiton of limits. Attempt to solve I can use $\delta ,\epsilon$ definition of limit. If i can show $$ |x-a| < \delta \implies |f(x)-L| < \epsilon$$ It implies limit exists according to $\delta, \epsilon$ definition of limits. $$ |x-(-3)|< \delta \implies |1-4x-13|< \epsilon $$ $$ |1-4x-13|< \epsilon \iff |-4x-12| < \epsilon \iff |4x+12|< \epsilon $$ $$ |x-(-3)|< \delta \iff |x+3| < \delta $$ $$ \text{let }  \delta = \epsilon/4$$ $$ |x+3| < \delta \implies |x+3|<\epsilon/4 \implies$$ $$ 4|x+3|<\epsilon \implies $$ $$ |4(x+3)|< \epsilon \implies $$ $$ |4x+12| < \epsilon $$ $$\tag*{$\square$}$$ I would like to have some feedback if my solution looks correct or not.",Problem Prove Using definiton of limits. Attempt to solve I can use definition of limit. If i can show It implies limit exists according to definition of limits. I would like to have some feedback if my solution looks correct or not.,"\lim_{x\rightarrow -3} 1-4x=13 \delta, \epsilon \delta ,\epsilon  |x-a| < \delta \implies |f(x)-L| < \epsilon \delta, \epsilon  |x-(-3)|< \delta \implies |1-4x-13|< \epsilon   |1-4x-13|< \epsilon \iff |-4x-12| < \epsilon \iff |4x+12|< \epsilon   |x-(-3)|< \delta \iff |x+3| < \delta   \text{let }  \delta = \epsilon/4  |x+3| < \delta \implies |x+3|<\epsilon/4 \implies  4|x+3|<\epsilon \implies   |4(x+3)|< \epsilon \implies   |4x+12| < \epsilon  \tag*{\square}","['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
98,Powers of Irreducible Polynomials in Partial Fractions,Powers of Irreducible Polynomials in Partial Fractions,,"Fractions are normally decomposed by finding the numerator that goes with each individual factor of the denominator of the original fraction.  However, when a denominator factor is repeated, the solution algorithm involves finding numerators for ascending powers of that factor.  So there appears to be a difference in how we treat repeated and non-repeated factors. I thought it might be insightful to work out a couple of examples with almost-repeated factors.  We can find, either trivially or via partial fraction decomposition, that $$\frac{x + 5}{x^2} = \frac{1}{x} + \frac{5}{x^2}$$ Compare this to the decomposition of a similar fraction $$\frac{x + 5}{(x + \varepsilon)(x)} = \frac{\varepsilon - 5}{\varepsilon(x + \varepsilon)} + \frac{5}{\varepsilon x}$$ where $\varepsilon$ is an arbitrarily small constant.  To me, it's not particularly clear that the right-hand side (RHS) of each equation relates to that of the other.  We can't set $\varepsilon$ to $0$ in the second RHS, but even using limits to let $\varepsilon$ approach $0$ doesn't yield the first RHS, as I would have guessed.  Furthermore, the solution process does not harmonize the degree disparity between the top and bottom RHS; the top has the form $\frac{degree 0}{degree 1} + \frac{degree 0}{degree 2}$ , while the bottom has the form $\frac{degree 0}{degree 1} + \frac{degree 0}{degree 1}$ . A less trivial example, comparing $$\frac{x^2 + 1}{(x^2)(x + 3)} = -\frac{1}{9x} + \frac{1}{3x^2} + \frac{10}{9(x + 3)}$$ to $$\frac{x^2 + 1}{(x + \varepsilon)(x)(x + 3)} = \frac{1}{3\varepsilon x} + \frac{\varepsilon^2 + 1}{\varepsilon(\varepsilon - 3)(x + \varepsilon)} + \frac{10}{3(3 - \varepsilon)(x + 3)}$$ reveals a similar absence of clarity.  Confusingly, one gets the urge to let some of the $\varepsilon$ approach $0$ while letting others approach $3$ , but even arbitrarily allowing this much freedom won't allow us to reach the top RHS in this pair. Why is this analysis not providing more insight into the repeated factor rule for partial fraction decomposition, and will some other analysis in a similar vein work better?","Fractions are normally decomposed by finding the numerator that goes with each individual factor of the denominator of the original fraction.  However, when a denominator factor is repeated, the solution algorithm involves finding numerators for ascending powers of that factor.  So there appears to be a difference in how we treat repeated and non-repeated factors. I thought it might be insightful to work out a couple of examples with almost-repeated factors.  We can find, either trivially or via partial fraction decomposition, that Compare this to the decomposition of a similar fraction where is an arbitrarily small constant.  To me, it's not particularly clear that the right-hand side (RHS) of each equation relates to that of the other.  We can't set to in the second RHS, but even using limits to let approach doesn't yield the first RHS, as I would have guessed.  Furthermore, the solution process does not harmonize the degree disparity between the top and bottom RHS; the top has the form , while the bottom has the form . A less trivial example, comparing to reveals a similar absence of clarity.  Confusingly, one gets the urge to let some of the approach while letting others approach , but even arbitrarily allowing this much freedom won't allow us to reach the top RHS in this pair. Why is this analysis not providing more insight into the repeated factor rule for partial fraction decomposition, and will some other analysis in a similar vein work better?",\frac{x + 5}{x^2} = \frac{1}{x} + \frac{5}{x^2} \frac{x + 5}{(x + \varepsilon)(x)} = \frac{\varepsilon - 5}{\varepsilon(x + \varepsilon)} + \frac{5}{\varepsilon x} \varepsilon \varepsilon 0 \varepsilon 0 \frac{degree 0}{degree 1} + \frac{degree 0}{degree 2} \frac{degree 0}{degree 1} + \frac{degree 0}{degree 1} \frac{x^2 + 1}{(x^2)(x + 3)} = -\frac{1}{9x} + \frac{1}{3x^2} + \frac{10}{9(x + 3)} \frac{x^2 + 1}{(x + \varepsilon)(x)(x + 3)} = \frac{1}{3\varepsilon x} + \frac{\varepsilon^2 + 1}{\varepsilon(\varepsilon - 3)(x + \varepsilon)} + \frac{10}{3(3 - \varepsilon)(x + 3)} \varepsilon 0 3,"['calculus', 'limits', 'intuition', 'fractions', 'partial-fractions']"
99,Possible mistake in proof of limit?,Possible mistake in proof of limit?,,"Please if someone with enough reputation could show my image it would be great. In this limit question, at the point where it states: Take the limit as $n \rightarrow \infty$ I really don't follow the logic at all. Since $a \in (0,1)$ then as $n \rightarrow \infty$, $|f(x)-f(a^nx)| \rightarrow |f(x)-f(0)|$, did they confuse $f(0)$ with $0$? Because otherwise how do you go from $$|f(x)-f(0)| \leq \frac{\epsilon}{1-a} |x|$$ to $$|f(x)| \leq \frac{\epsilon}{1-a} |x|$$ It is also very strange that the first property was not used.","Please if someone with enough reputation could show my image it would be great. In this limit question, at the point where it states: Take the limit as $n \rightarrow \infty$ I really don't follow the logic at all. Since $a \in (0,1)$ then as $n \rightarrow \infty$, $|f(x)-f(a^nx)| \rightarrow |f(x)-f(0)|$, did they confuse $f(0)$ with $0$? Because otherwise how do you go from $$|f(x)-f(0)| \leq \frac{\epsilon}{1-a} |x|$$ to $$|f(x)| \leq \frac{\epsilon}{1-a} |x|$$ It is also very strange that the first property was not used.",,"['real-analysis', 'limits', 'inequality', 'proof-explanation', 'epsilon-delta']"
