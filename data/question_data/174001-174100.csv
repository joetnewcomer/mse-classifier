,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,relation variance and covariance,relation variance and covariance,,"Let’s assume a (weakly) stationary process $\{u_t\}$ , such that the mean of $u_t$ and the covariance $(u_t, u_{t+h})$ do not depend on $t$ . For simplicity, we assume that $E(u_t) = 0$ . (Don't know how whether this is relevant.). Then my book makes the following step: $$var(\sum^T_{t=1}u_t)=\sum^T_{t=1}\sum^T_{s=1}cov(u_s,u_t).$$ I don't understand this step. Isn't the variance the covariance with itself?","Let’s assume a (weakly) stationary process , such that the mean of and the covariance do not depend on . For simplicity, we assume that . (Don't know how whether this is relevant.). Then my book makes the following step: I don't understand this step. Isn't the variance the covariance with itself?","\{u_t\} u_t (u_t, u_{t+h}) t E(u_t) = 0 var(\sum^T_{t=1}u_t)=\sum^T_{t=1}\sum^T_{s=1}cov(u_s,u_t).","['statistics', 'variance']"
1,"Given a fair coin, what is the mean of the number of tails before we toss a head?","Given a fair coin, what is the mean of the number of tails before we toss a head?",,"Question: Given a fair coin, what is the mean of the number of tails before we toss a head? Let $N$ be the number of tails before we toss a head. Then $$E(N) = \sum_{n=1}^\infty n P(N=n).$$ Since $$P(N=n) = \left(\frac{1}{2}\right)^{n-1}.$$ Then by using geometric series, we have \begin{align*} E(N) & = \sum_{n=1}^\infty n P(N=n) \\ & = \sum_{n=1}^\infty n \left(\frac{1}{2}\right)^{n-1} \\ & = \frac{1}{(1-\frac{1}{2})^2} \\ & = 4. \end{align*} Are my calculations correct?","Question: Given a fair coin, what is the mean of the number of tails before we toss a head? Let be the number of tails before we toss a head. Then Since Then by using geometric series, we have Are my calculations correct?","N E(N) = \sum_{n=1}^\infty n P(N=n). P(N=n) = \left(\frac{1}{2}\right)^{n-1}. \begin{align*}
E(N) & = \sum_{n=1}^\infty n P(N=n) \\
& = \sum_{n=1}^\infty n \left(\frac{1}{2}\right)^{n-1} \\
& = \frac{1}{(1-\frac{1}{2})^2} \\
& = 4.
\end{align*}","['probability', 'statistics']"
2,"Positiveness of some functions, connection with the central limit theorem and stable distributions","Positiveness of some functions, connection with the central limit theorem and stable distributions",,"Final update on 11/28/2019: I have worked on this a bit more, and wrote an article summarizing all the main findings. You can read it here . Let us consider the following function: $$f(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \cos(xt)\cdot\exp\Big(-a^2(b-|\sin(ct)|^d)\cdot t^2\Big)dt.$$ Here $-\infty < x < \infty, a\geq 1, b=4, c=1$ and $d=1$ or $d=2$ . The function $f(x)$ is a symmetric density centered at zero, it integrates to one, all its odd moments are zero, and all its even moments exist and are positive. Indeed, this is the density of a random variable $X$ with the following characteristic function: $$\psi_X(t) = \exp\Big(-a^2(b-|\sin(ct)|^d)\cdot t^2\Big)\Big.$$ Most importantly, it is NOT the density of a Gaussian distribution (unless $c=0$ or $d=0$ ) and its variance is finite. The big question is this: is it really a density, that is, is the characteristic function a valid one? The one thing that needs to be confirmed is whether $f(x)\geq 0$ everywhere. In the cases that I investigated, the answer seems to be positive, but the minimum value of $f(x)$ on any finite interval is so close to zero that it is impossible to conclude. It certainly looks like $f(x) > -10^{-16}$ everywhere but unfortunately this is too close to zero to be confirmed by numerical computations as the precision in my algorithms is about 15 digits. WolframAlpha is also unable to answer this question. Below is the chart for $f(x)$ , with $a=1, b=4, c=1, d=2$ . My computations tell me that $f(-39.71) \approx -2.94 \times 10^{-17}$ yields the absolute minimum, while $f(39.71) \approx -1.38 \times 10^{-17}$ . This is beyond the precision offered by the programming language, and anyway $f(-39.71) = f(39.71)$ . WoframAlpha returns $f(-39.71) = f(39.71) = 0$ (an absolute $0$ ), see the computation here . By contrast, if $a=1, b=2, c=1, d=2$ , then the minimum is $-0.000003388$ and it is clearly negative and confirmed by WolframAlpha: it is attained at $x\approx \pm 13.56$ . The case $a=1, b=4, c=1, d=1$ is even more challenging, with $f(x)$ looking perfectly strictly positive everywhere. See also my related question posted on CrossValidated, here . Connection with CLT and Stable Distributions If any of these functions is positive (say if $a\geq 1, b=4, c =1, d=2$ ) then we are dealing with a stable family of true densities governed (in this example) by one parameter: $a\geq 1$ . There are two consequences to this, unless something is wrong in my reasoning: It invalidates the classical theory of stable distributions, stating that the only stable family with a finite variance is the Gaussian family (see the book Limit Distributions for Sums of Independent Random Variables , by Gnedenko and Kolmogorov, published in 1954; the whole purpose of this book is proving this very fact.) It also potentially invalidates the central limit theorem (CLT): If $X_1, X_2$ are   i.i.d. with a distribution from that family, the same is true for $X_1 + X_2$ , and indeed for $\lim_{n \rightarrow \infty} (X_1+\cdots +X_n)/\sqrt{n}$ . Note that $E(X_i)=0$ . Thus, the convergence in distribution is towards a distribution from that same family, which does NOT include the Gaussian law. The only $X_i$ 's known to violate the CLT have an infinite variance, for instance the Cauchy distribution which also constitutes a stable family. Yet in this case the variance is finite. Question Thus my question is this: is it true that $f(x) \geq 0$ everywhere, at least depending on the parameters, and excluding the Gaussian case. What about the stability of the family of distributions introduced here (it is fully stable under addition / multiplication by a constant?) Update 2 I just computed the density in question in the case $c=0$ . This corresponds to a Gaussian distribution, thus $f(x)$ is definitely strictly positive in this case. Yet my program returns the global minimum as being below zero, about $-4 \times 10^{-17}$ . This suggests that negative values (of similar magnitude) obtained in the case $a=1, b=4, c=1, d=2$ are just an artifact of machine precision. This boosts my confidence in the fact that we are also dealing with a proper density in this latter case. But it is not a proof of course, and I am still a little skeptical. For those interested, I am now looking at some nasty distribution, something defined by a CF like $$\psi_X(t) = \exp\Big(-a^2 |t|^{2+\sin(1/|bt|)}\Big).$$ This density looks very smooth yet is really nasty in some sense. Let's call it $H(a, b)$ as it is governed by two parameters $a, b$ . It integrates to 1, but... it is not a density! The minimum is very slightly below zero, around $-0.02$ . I am somewhat confident that I will find one within the next 10 days, with the same nastiness, that is a proper density. Here is a proposed generalization. The $H(a, b)$ distribution (if it was actually a distribution) is semi-stable in the following sense: stable both under addition and multiplication by a scalar, separately but not jointly stable. What it means is this: If $X,Y$ are independent and are $H(a_1, b), H(a_2,b)$ respectively, then $X+Y$ and $X-Y$ is $H(\sqrt{a_1^2+a_2^2},b)$ . If $X$ is $H(a,b)$ and $r>0$ , then $rX$ is $H(ar, br)$ . As a result, $Z=(X_1 + \cdots + X_n)/n$ is $H(a, b/\sqrt{n})$ . A general class of 2-parameter semi-stable, symmetric distributions centered at zero (much larger than the class of symmetric stable distributions centered at zero) is defined by the following characteristic function: $$\psi_X(t) =\exp\Big[-a^2\Big(p(b\cdot|t|)+q(b\cdot|t|)\Big) \Big] .$$ Here $p,q$ are two real-valued functions chosen so that $\psi_X$ is a proper characteristic function, and $b>0$ . For instance $p(t) = t$ and $q(t) = t^2$ . If you use the product $p(b\cdot|t|)\times q(b\cdot|t|)$ rather than the sum $p(b\cdot|t|) + q(b\cdot|t|)$ , it also works.","Final update on 11/28/2019: I have worked on this a bit more, and wrote an article summarizing all the main findings. You can read it here . Let us consider the following function: Here and or . The function is a symmetric density centered at zero, it integrates to one, all its odd moments are zero, and all its even moments exist and are positive. Indeed, this is the density of a random variable with the following characteristic function: Most importantly, it is NOT the density of a Gaussian distribution (unless or ) and its variance is finite. The big question is this: is it really a density, that is, is the characteristic function a valid one? The one thing that needs to be confirmed is whether everywhere. In the cases that I investigated, the answer seems to be positive, but the minimum value of on any finite interval is so close to zero that it is impossible to conclude. It certainly looks like everywhere but unfortunately this is too close to zero to be confirmed by numerical computations as the precision in my algorithms is about 15 digits. WolframAlpha is also unable to answer this question. Below is the chart for , with . My computations tell me that yields the absolute minimum, while . This is beyond the precision offered by the programming language, and anyway . WoframAlpha returns (an absolute ), see the computation here . By contrast, if , then the minimum is and it is clearly negative and confirmed by WolframAlpha: it is attained at . The case is even more challenging, with looking perfectly strictly positive everywhere. See also my related question posted on CrossValidated, here . Connection with CLT and Stable Distributions If any of these functions is positive (say if ) then we are dealing with a stable family of true densities governed (in this example) by one parameter: . There are two consequences to this, unless something is wrong in my reasoning: It invalidates the classical theory of stable distributions, stating that the only stable family with a finite variance is the Gaussian family (see the book Limit Distributions for Sums of Independent Random Variables , by Gnedenko and Kolmogorov, published in 1954; the whole purpose of this book is proving this very fact.) It also potentially invalidates the central limit theorem (CLT): If are   i.i.d. with a distribution from that family, the same is true for , and indeed for . Note that . Thus, the convergence in distribution is towards a distribution from that same family, which does NOT include the Gaussian law. The only 's known to violate the CLT have an infinite variance, for instance the Cauchy distribution which also constitutes a stable family. Yet in this case the variance is finite. Question Thus my question is this: is it true that everywhere, at least depending on the parameters, and excluding the Gaussian case. What about the stability of the family of distributions introduced here (it is fully stable under addition / multiplication by a constant?) Update 2 I just computed the density in question in the case . This corresponds to a Gaussian distribution, thus is definitely strictly positive in this case. Yet my program returns the global minimum as being below zero, about . This suggests that negative values (of similar magnitude) obtained in the case are just an artifact of machine precision. This boosts my confidence in the fact that we are also dealing with a proper density in this latter case. But it is not a proof of course, and I am still a little skeptical. For those interested, I am now looking at some nasty distribution, something defined by a CF like This density looks very smooth yet is really nasty in some sense. Let's call it as it is governed by two parameters . It integrates to 1, but... it is not a density! The minimum is very slightly below zero, around . I am somewhat confident that I will find one within the next 10 days, with the same nastiness, that is a proper density. Here is a proposed generalization. The distribution (if it was actually a distribution) is semi-stable in the following sense: stable both under addition and multiplication by a scalar, separately but not jointly stable. What it means is this: If are independent and are respectively, then and is . If is and , then is . As a result, is . A general class of 2-parameter semi-stable, symmetric distributions centered at zero (much larger than the class of symmetric stable distributions centered at zero) is defined by the following characteristic function: Here are two real-valued functions chosen so that is a proper characteristic function, and . For instance and . If you use the product rather than the sum , it also works.","f(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \cos(xt)\cdot\exp\Big(-a^2(b-|\sin(ct)|^d)\cdot t^2\Big)dt. -\infty < x < \infty, a\geq 1, b=4, c=1 d=1 d=2 f(x) X \psi_X(t) = \exp\Big(-a^2(b-|\sin(ct)|^d)\cdot t^2\Big)\Big. c=0 d=0 f(x)\geq 0 f(x) f(x) > -10^{-16} f(x) a=1, b=4, c=1, d=2 f(-39.71) \approx -2.94 \times 10^{-17} f(39.71) \approx -1.38 \times 10^{-17} f(-39.71) = f(39.71) f(-39.71) = f(39.71) = 0 0 a=1, b=2, c=1, d=2 -0.000003388 x\approx \pm 13.56 a=1, b=4, c=1, d=1 f(x) a\geq 1, b=4, c =1, d=2 a\geq 1 X_1, X_2 X_1 + X_2 \lim_{n \rightarrow \infty} (X_1+\cdots +X_n)/\sqrt{n} E(X_i)=0 X_i f(x) \geq 0 c=0 f(x) -4 \times 10^{-17} a=1, b=4, c=1, d=2 \psi_X(t) = \exp\Big(-a^2 |t|^{2+\sin(1/|bt|)}\Big). H(a, b) a, b -0.02 H(a, b) X,Y H(a_1, b), H(a_2,b) X+Y X-Y H(\sqrt{a_1^2+a_2^2},b) X H(a,b) r>0 rX H(ar, br) Z=(X_1 + \cdots + X_n)/n H(a, b/\sqrt{n}) \psi_X(t) =\exp\Big[-a^2\Big(p(b\cdot|t|)+q(b\cdot|t|)\Big) \Big] . p,q \psi_X b>0 p(t) = t q(t) = t^2 p(b\cdot|t|)\times q(b\cdot|t|) p(b\cdot|t|) + q(b\cdot|t|)","['real-analysis', 'probability-theory', 'statistics', 'probability-distributions', 'characteristic-functions']"
3,How to compute U = X/Y of a pdf,How to compute U = X/Y of a pdf,,"I know that similar questions have been posted before but I don't understand how to solve them, especially because this very type of question is never explained in our textbook. Assume that we are given $f_x(x) = xe^{-x}$ $f_y(y) = \frac{1}{6}y^{3}e^{-y}$ And have to compute $U = X/Y$ I am not directly interested in this specific result, I am looking for an explanation on how it is done. I also appreciate a link to somewhere this type of operation is explained step by step. Also it does not have to be this specific operation. It can also be $U = X+Y$ or similar, as I suspect them all to be solvable in a similar manner. Thank you. Edit: I am given the distribution $f(x,y) = x(y-x)e^{-y}$ for $0<x<y<\infty$ and calculated the marginal distributions.","I know that similar questions have been posted before but I don't understand how to solve them, especially because this very type of question is never explained in our textbook. Assume that we are given And have to compute I am not directly interested in this specific result, I am looking for an explanation on how it is done. I also appreciate a link to somewhere this type of operation is explained step by step. Also it does not have to be this specific operation. It can also be or similar, as I suspect them all to be solvable in a similar manner. Thank you. Edit: I am given the distribution for and calculated the marginal distributions.","f_x(x) = xe^{-x} f_y(y) = \frac{1}{6}y^{3}e^{-y} U = X/Y U = X+Y f(x,y) = x(y-x)e^{-y} 0<x<y<\infty","['statistics', 'probability-distributions', 'marginal-distribution', 'marginal-probability']"
4,Proof that $E[SS_E] = (n-2)\sigma^2$,Proof that,E[SS_E] = (n-2)\sigma^2,"From Probability and Statistics in Engineering by Hines et. al: Let $y_i = \beta_0 + \beta _1 x_i + \epsilon$ , where $\epsilon$ has mean $0$ and variance $\sigma^2$ with all $\epsilon_i$ uncorrelated. Let $SS_E = \sum y_i^2  - n\bar y^2 - \hat \beta_1 S_{xy}$ , where $S_{xy} = \sum x_iy_i - \frac{1}{n}(\sum x_i^2)(\sum y_i^2)$ and $\hat \beta_1 = \frac{S_{xy}}{S_{xx}}$ . $E[\hat \beta _1] = \beta_1$ and $V(\hat \beta_1) = \frac{\sigma^2}{S_{xx}}$ . Then $E(SS_E) = (n-2) \sigma^2$ How is this derived?  I can't figure out a way to show this. $E[\sum y_i^2] = n\sigma^2 + (E[y_i])^2$ $E[\hat \beta_1 S_{xy}] = E[S_{xx}\hat \beta_1 ^2] = S_{xx}(\frac{\sigma^2}{S_{xx}} + \beta_1) = \sigma^2 + S_{xx} \beta_1$ $E[n \bar y^2] = nE[\bar y^2] = n (\frac{\sigma^2}{n} +(\frac{1}{n}\sum E[y_i])^2 )$ But I can't see a way to show equality with these.","From Probability and Statistics in Engineering by Hines et. al: Let , where has mean and variance with all uncorrelated. Let , where and . and . Then How is this derived?  I can't figure out a way to show this. But I can't see a way to show equality with these.",y_i = \beta_0 + \beta _1 x_i + \epsilon \epsilon 0 \sigma^2 \epsilon_i SS_E = \sum y_i^2  - n\bar y^2 - \hat \beta_1 S_{xy} S_{xy} = \sum x_iy_i - \frac{1}{n}(\sum x_i^2)(\sum y_i^2) \hat \beta_1 = \frac{S_{xy}}{S_{xx}} E[\hat \beta _1] = \beta_1 V(\hat \beta_1) = \frac{\sigma^2}{S_{xx}} E(SS_E) = (n-2) \sigma^2 E[\sum y_i^2] = n\sigma^2 + (E[y_i])^2 E[\hat \beta_1 S_{xy}] = E[S_{xx}\hat \beta_1 ^2] = S_{xx}(\frac{\sigma^2}{S_{xx}} + \beta_1) = \sigma^2 + S_{xx} \beta_1 E[n \bar y^2] = nE[\bar y^2] = n (\frac{\sigma^2}{n} +(\frac{1}{n}\sum E[y_i])^2 ),"['probability', 'statistics', 'proof-explanation']"
5,"Distribution of X/(XY-WZ) when all RVs are distributed (iid) U(0,1)","Distribution of X/(XY-WZ) when all RVs are distributed (iid) U(0,1)",,"Assume that we have four random variates $W, X, Y, Z$ such that $W, X, Y, Z\overset{iid}{\sim} U(0,1)$ . I wish to determine the distribution of the following: \begin{equation}  Q = \frac{X}{XY-WZ} \end{equation} From prior questions , we can determine the distribution of $XY$ and $WZ$ , however, I am not quite sure how to handle the rest. If anyone could assist, I would be very thankful. BTW - This is not for a course. I am just rusty at my mathematical statistics.","Assume that we have four random variates such that . I wish to determine the distribution of the following: From prior questions , we can determine the distribution of and , however, I am not quite sure how to handle the rest. If anyone could assist, I would be very thankful. BTW - This is not for a course. I am just rusty at my mathematical statistics.","W, X, Y, Z W, X, Y, Z\overset{iid}{\sim} U(0,1) \begin{equation}
 Q = \frac{X}{XY-WZ}
\end{equation} XY WZ",['statistics']
6,Lottery numbers and chaos/quantum theory,Lottery numbers and chaos/quantum theory,,"Please settle an argument if you would be so kind. In a group chat, one person took the position 1,2,3,4,5,6 has as much chance of coming out in the lottery as any other six numbers you define, e.g. 3,11,23,33,34,46. A second person took the position that this was untrue because: Those numbers include the randomness of chaos theory, but a perfect sequential number doesn't have that so its chances become slimmer. If you drill down to the mathematics of it at least. Person two also went to go on to discuss quantum theory as well. So from a pure statistics point of view, I believe, the first person is correct. My question for you good folks is in two parts (mostly the second part): Is the first quote correct? Even if the first person is correct, are there any theory/basis for the second quote when it comes to the real-world execution of a lottery? Ideally something I can look up and read more about. It's entirely possible they're remembering something valid, but are not quoting it properly.","Please settle an argument if you would be so kind. In a group chat, one person took the position 1,2,3,4,5,6 has as much chance of coming out in the lottery as any other six numbers you define, e.g. 3,11,23,33,34,46. A second person took the position that this was untrue because: Those numbers include the randomness of chaos theory, but a perfect sequential number doesn't have that so its chances become slimmer. If you drill down to the mathematics of it at least. Person two also went to go on to discuss quantum theory as well. So from a pure statistics point of view, I believe, the first person is correct. My question for you good folks is in two parts (mostly the second part): Is the first quote correct? Even if the first person is correct, are there any theory/basis for the second quote when it comes to the real-world execution of a lottery? Ideally something I can look up and read more about. It's entirely possible they're remembering something valid, but are not quoting it properly.",,"['statistics', 'chaos-theory']"
7,PIN number combination that has a 3 and 8 in it.,PIN number combination that has a 3 and 8 in it.,,"So the question is, How many PINs(4 numbers from 0 to 9 with repeat) are there when 3 and 8 (in unknown positions) are in that pin and also the remaining 2 positions can be any number between 0 to 9. My solution: Number of combinations without the 3 and 8 in them=> 8 4 = 4096 then i subtract 10 4 from 4096 which gives 5904 combinations WITH a 3 AND 8 in them. Is the result correct?","So the question is, How many PINs(4 numbers from 0 to 9 with repeat) are there when 3 and 8 (in unknown positions) are in that pin and also the remaining 2 positions can be any number between 0 to 9. My solution: Number of combinations without the 3 and 8 in them=> 8 4 = 4096 then i subtract 10 4 from 4096 which gives 5904 combinations WITH a 3 AND 8 in them. Is the result correct?",,['combinatorics']
8,Is there a way to bound expected value with limited information of the CDF?,Is there a way to bound expected value with limited information of the CDF?,,"Suppose I want to evaluate $E[X]$ , where $X$ is a univariate random variable and takes values in $\mathcal{X}$ , where the smallest element of $\mathcal{X}$ is 0 and the largest element of $\mathcal{X}$ is $\overline{X}$ . The problem is that I don't have the pdf or cdf of $X$ . Instead, suppose that I know the exact value of the CDF at finitely many (but never all) values of the support. So for example, I know $Pr(X\leq x_1)=0.1$ , $Pr(X\leq x_2)=0.2$ ,..., $Pr(X\leq \overline{X})=1$ . Is there a way to bound $E[X]$ ? In other words, given this partial information what is the highest possible value of the expectation and the lowest possible value and how can I compute it? Will the characterization of the solutions in discrete and continuous random variable cases differ a lot? Intuitively, it seems that as the number of points over which I know the CDF increases (even if finitely many), I should have a good idea of the shape of the CDF and be able to bound the expectation. I am not sure how to formalize this intuition or whether it is correct.","Suppose I want to evaluate , where is a univariate random variable and takes values in , where the smallest element of is 0 and the largest element of is . The problem is that I don't have the pdf or cdf of . Instead, suppose that I know the exact value of the CDF at finitely many (but never all) values of the support. So for example, I know , ,..., . Is there a way to bound ? In other words, given this partial information what is the highest possible value of the expectation and the lowest possible value and how can I compute it? Will the characterization of the solutions in discrete and continuous random variable cases differ a lot? Intuitively, it seems that as the number of points over which I know the CDF increases (even if finitely many), I should have a good idea of the shape of the CDF and be able to bound the expectation. I am not sure how to formalize this intuition or whether it is correct.",E[X] X \mathcal{X} \mathcal{X} \mathcal{X} \overline{X} X Pr(X\leq x_1)=0.1 Pr(X\leq x_2)=0.2 Pr(X\leq \overline{X})=1 E[X],"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'upper-lower-bounds']"
9,What does the symbol $\omega$ stand for in statistics?,What does the symbol  stand for in statistics?,\omega,I was reading an answer here in math stackexchange and it mentioned this: Linearity of Expectation then follows from its definition. $\begin{align} \mathsf E(X+Y) =&~ \sum_{\omega\in\Omega} (X+Y)(\omega)~\mathsf P(\omega) \\[1ex] =&~ \sum_{\omega\in \Omega} X(\omega)~\mathsf P(\omega)+\sum_{\omega\in \Omega} Y(\omega)~\mathsf P(\omega) \\[1ex] =&~ \mathsf E(X)+\mathsf E(Y) \end{align}$ What does the symbol $\omega$ stand for?,I was reading an answer here in math stackexchange and it mentioned this: Linearity of Expectation then follows from its definition. What does the symbol stand for?,"\begin{align} \mathsf E(X+Y) =&~ \sum_{\omega\in\Omega}
(X+Y)(\omega)~\mathsf P(\omega) \\[1ex] =&~ \sum_{\omega\in \Omega}
X(\omega)~\mathsf P(\omega)+\sum_{\omega\in \Omega} Y(\omega)~\mathsf
P(\omega) \\[1ex] =&~ \mathsf E(X)+\mathsf E(Y) \end{align} \omega","['probability-theory', 'statistics', 'random-variables', 'expected-value']"
10,"Sample $k$ of $n$ numbers (with replacement), what is the probability for a certain from the $n$ numbers to be the median of the $k$ numbers","Sample  of  numbers (with replacement), what is the probability for a certain from the  numbers to be the median of the  numbers",k n n k,Suppose we have ordered numbers $a_1 < a_2 < \dots < a_n$ . Now we sample $k$ of them with equal probability and with replacement and compute the median of these and call it $m$ . (Let us assume that $k$ is odd for the sake of simplicity.) What is the probability the $m = a_i$ for each $i$ ? I want to know this to construct the probability mass function for the median of $k$ random draws from a sample of $n$ numbers.,Suppose we have ordered numbers . Now we sample of them with equal probability and with replacement and compute the median of these and call it . (Let us assume that is odd for the sake of simplicity.) What is the probability the for each ? I want to know this to construct the probability mass function for the median of random draws from a sample of numbers.,a_1 < a_2 < \dots < a_n k m k m = a_i i k n,"['probability', 'statistics', 'median']"
11,"Let $x_1$ and $x_2$ be independent uniform variables from [0, 2]. What is the probability that $|x_1-x_2| \leq 1$?","Let  and  be independent uniform variables from [0, 2]. What is the probability that ?",x_1 x_2 |x_1-x_2| \leq 1,"What I have so far for the solution Since they are both continuous uniform variables. And because they are independent, we can say that $$f(x_1, x_2)=\frac{1}{4}$$ $$P(|x_1-x_2| \leq 1) = P(x_1-1 \leq x_2 \leq x_1+1)$$ $$P(x_1-1 \leq x_2 \leq x_1+1) = \int_{-\infty}^{+\infty} \int_{x_1 - 1}^{x_1 + 1}\frac{1}{4}dx_2dx_1$$ What I am having trouble with However, when I compute the aforementioned integral, I get a probability of $1$ or $100\%$ $$\int_{-\infty}^{+\infty} \int_{x_1 - 1}^{x_1 + 1}\frac{1}{4}dx_2dx_1 = \int_{0}^{2} \int_{x_1 - 1}^{x_1 + 1}\frac{1}{4}dx_2dx_1 = 1$$ I know that I am supposed to get $\frac{3}{4}$ . But I have no idea how.","What I have so far for the solution Since they are both continuous uniform variables. And because they are independent, we can say that What I am having trouble with However, when I compute the aforementioned integral, I get a probability of or I know that I am supposed to get . But I have no idea how.","f(x_1, x_2)=\frac{1}{4} P(|x_1-x_2| \leq 1) = P(x_1-1 \leq x_2 \leq x_1+1) P(x_1-1 \leq x_2 \leq x_1+1) = \int_{-\infty}^{+\infty} \int_{x_1 - 1}^{x_1 + 1}\frac{1}{4}dx_2dx_1 1 100\% \int_{-\infty}^{+\infty} \int_{x_1 - 1}^{x_1 + 1}\frac{1}{4}dx_2dx_1 = \int_{0}^{2} \int_{x_1 - 1}^{x_1 + 1}\frac{1}{4}dx_2dx_1 = 1 \frac{3}{4}","['probability', 'statistics']"
12,Conditional expectation and variance with coin flips,Conditional expectation and variance with coin flips,,"Let $N$ be a random number chosen uniformly at random from the set { ${1, 2, 3, 4}$ }. Given that $N = n$ , coin A is flipped n times and coin B is flipped $(5 − n)$ times. What is $Var(X)$ ?","Let be a random number chosen uniformly at random from the set { }. Given that , coin A is flipped n times and coin B is flipped times. What is ?","N {1, 2, 3, 4} N = n (5 − n) Var(X)","['probability', 'statistics', 'conditional-expectation', 'conditional-probability']"
13,Showing $\hat{\theta} = \frac{x_1 + 2x_2 + x_3}{4}$ is a not a sufficient estimator for the mean of a Bernoulli-distributed population,Showing  is a not a sufficient estimator for the mean of a Bernoulli-distributed population,\hat{\theta} = \frac{x_1 + 2x_2 + x_3}{4},"Suppose $x_1$ , $x_2$ , $x_3$ are independant observations from a Bernoulli-distributed population with parameter $\theta$ . I want to show that $$\hat{\theta} = \frac{x_1 + 2x_2 + x_3}{4}$$ is not a sufficient estimator for $\theta$ . I have derived earlier that $$L(\theta) = (1 - \theta)^3 \left(\frac{\theta}{1 - \theta}\right)^{x_1 + x_2 + x_3}$$ which allowed me to show that $\overline{x}$ was a sufficient estimator for $\theta$ . I however am not sure about how to proceed to show that $\hat{\theta}$ is not sufficient for $\theta$ (while it does seem natural to me). I have tried using the ""formal"" definition for a statistic to be sufficient but with no concrete results. How would one go about showing $\hat{\theta}$ is sufficient for estimating $\theta$ ?","Suppose , , are independant observations from a Bernoulli-distributed population with parameter . I want to show that is not a sufficient estimator for . I have derived earlier that which allowed me to show that was a sufficient estimator for . I however am not sure about how to proceed to show that is not sufficient for (while it does seem natural to me). I have tried using the ""formal"" definition for a statistic to be sufficient but with no concrete results. How would one go about showing is sufficient for estimating ?",x_1 x_2 x_3 \theta \hat{\theta} = \frac{x_1 + 2x_2 + x_3}{4} \theta L(\theta) = (1 - \theta)^3 \left(\frac{\theta}{1 - \theta}\right)^{x_1 + x_2 + x_3} \overline{x} \theta \hat{\theta} \theta \hat{\theta} \theta,"['statistics', 'probability-distributions', 'statistical-inference']"
14,"Is histogram ""function"" linear?","Is histogram ""function"" linear?",,"I was ask to prove/disprove if for two given images $f_1(x,y)$ and $f_2(x,y)$ the histogram of $f_1(x,y)-f_2(x,y)$ and $h_1-h_2$ are equal, where $h_i$ is the histogram of image i. Intuitively it seems to be correct, as if we look at the images as matrices, the histogram is a function that take the matrix values and count the number of repetitions (""losing"" the indices of each value) But is there a proper mathematically proof?","I was ask to prove/disprove if for two given images and the histogram of and are equal, where is the histogram of image i. Intuitively it seems to be correct, as if we look at the images as matrices, the histogram is a function that take the matrix values and count the number of repetitions (""losing"" the indices of each value) But is there a proper mathematically proof?","f_1(x,y) f_2(x,y) f_1(x,y)-f_2(x,y) h_1-h_2 h_i","['statistics', 'image-processing']"
15,Showing that the ratio of two standard independent normals is a Cauchy using Characteristic Functions,Showing that the ratio of two standard independent normals is a Cauchy using Characteristic Functions,,"Question Let $X$ and $Y$ be independent standard normals. Use characteristic functions to find the distribution of $X/Y$ . My attempt We will attempt to show that $Ee^{itX/Y}=e^{-|t|}$ (the c.f. of a Cauchy random variable) from which the claim will follow. To this end, note that $$ Ee^{itX/Y}=\int\frac{1}{\sqrt{2\pi}}e^{-y^2/2}\int e^{itx/y}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\, dx\, dy=\int \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\exp\left(-\frac{t^2}{2y^2}\right)\, dy $$ where we used the fact that $Ee^{it X}=\exp(-0.5t^2)$ . We can write it as $$ Ee^{itX/Y}=\int \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left[\frac{t^2}{y^2}+y^2\right]\right)\, dy. $$ But at this point I don't know how to evaluate the integral. I tried completing the square in the exponent but I couldn't make progress from there. Any help is appreciated.","Question Let and be independent standard normals. Use characteristic functions to find the distribution of . My attempt We will attempt to show that (the c.f. of a Cauchy random variable) from which the claim will follow. To this end, note that where we used the fact that . We can write it as But at this point I don't know how to evaluate the integral. I tried completing the square in the exponent but I couldn't make progress from there. Any help is appreciated.","X Y X/Y Ee^{itX/Y}=e^{-|t|} 
Ee^{itX/Y}=\int\frac{1}{\sqrt{2\pi}}e^{-y^2/2}\int e^{itx/y}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\, dx\, dy=\int \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\exp\left(-\frac{t^2}{2y^2}\right)\, dy
 Ee^{it X}=\exp(-0.5t^2) 
Ee^{itX/Y}=\int \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left[\frac{t^2}{y^2}+y^2\right]\right)\, dy.
","['probability', 'probability-theory', 'statistics', 'characteristic-functions']"
16,Convergence in probability of $\sqrt{n}(X_n - \theta)$,Convergence in probability of,\sqrt{n}(X_n - \theta),"How do I show that if $\sqrt{n}(X_n - \theta)$ converges in distribution, then $X_n$ converges in probability to $\theta$ ? Setting $Y_n = \sqrt{n}(X_n - \theta)$ , convergence in distribution (to a random variable $Y$ ) means: $P(Y_n \leq y)$ implieas $P(Y \leq y)$ . Convergence in probability requires that $P(Y_n \geq \epsilon) \rightarrow 0 $ My reasoning so far is the following. Given convergence in distribution I can use Porohov's theorem that $P(|Y_n|>M)< \epsilon$ , for some positive $M$ and some positive $\epsilon$ . Now, I need to show that this translates into $P(|Y_n| \geq \epsilon)=0$ and this will be convergence in probability. I'm quite stuck however, any hints are appreciated","How do I show that if converges in distribution, then converges in probability to ? Setting , convergence in distribution (to a random variable ) means: implieas . Convergence in probability requires that My reasoning so far is the following. Given convergence in distribution I can use Porohov's theorem that , for some positive and some positive . Now, I need to show that this translates into and this will be convergence in probability. I'm quite stuck however, any hints are appreciated",\sqrt{n}(X_n - \theta) X_n \theta Y_n = \sqrt{n}(X_n - \theta) Y P(Y_n \leq y) P(Y \leq y) P(Y_n \geq \epsilon) \rightarrow 0  P(|Y_n|>M)< \epsilon M \epsilon P(|Y_n| \geq \epsilon)=0,"['probability-theory', 'statistics', 'convergence-divergence', 'asymptotics']"
17,"If $E(U|X)=0$, then $E(U)=0$?","If , then ?",E(U|X)=0 E(U)=0,"If $U$ and $X$ are random variables such that $E(U|X)=0$ , then $E(U)=0$ . Really? how to prove?","If and are random variables such that , then . Really? how to prove?",U X E(U|X)=0 E(U)=0,['statistics']
18,Intuition in the relation between force of mortality and survival function,Intuition in the relation between force of mortality and survival function,,"I understand that The force of mortality $ \mu (x)$ can be interpreted as the conditional density of failure at age x given survival to age x, while f(x) is the unconditional density of failure at age x. I also understand that  $\mu_x=\frac {-S'(x)}{S(x)}$ I know that mathematically we can easily find $S_X(x)=e^{-\int_0^x \mu_y dy}$ but I can't find the intuition behind the formula ( wich is very impotant to me to be able to learn ). In fact, it's especially the bounds of integration that intrigue me. Why is it from 0 to x  and not from x to $\infty$ while the survival function is for (X>x)? The second thing is why the exponential function keeps showing up in everything related to a force. (Force of mortality, force of interest...). My questions may seem a little bit out of nowhere but I'm asking perhaps  someone can give an explanation that will also improve my understanding of the exponential function. Thank you","I understand that The force of mortality $ \mu (x)$ can be interpreted as the conditional density of failure at age x given survival to age x, while f(x) is the unconditional density of failure at age x. I also understand that  $\mu_x=\frac {-S'(x)}{S(x)}$ I know that mathematically we can easily find $S_X(x)=e^{-\int_0^x \mu_y dy}$ but I can't find the intuition behind the formula ( wich is very impotant to me to be able to learn ). In fact, it's especially the bounds of integration that intrigue me. Why is it from 0 to x  and not from x to $\infty$ while the survival function is for (X>x)? The second thing is why the exponential function keeps showing up in everything related to a force. (Force of mortality, force of interest...). My questions may seem a little bit out of nowhere but I'm asking perhaps  someone can give an explanation that will also improve my understanding of the exponential function. Thank you",,"['probability', 'statistics', 'actuarial-science']"
19,Understanding this math notation in probability?,Understanding this math notation in probability?,,"I don't get this notation at all and cannot find a place to start with understanding this: $$\mathcal L_D=-\mathbb E_{x\sim P}[\log(D(x))]-\mathbb E_{\hat x\sim Q}[\log(1-D(\hat x))]$$ I don't get $\mathbb E$ there. Does it mean expectation? The equation says it's ""negative log-likelihood"". I understand that $D(x)$ is either 1 or 0. What does the $\mathbb E_{x\sim P}$ mean, especially in the context of statistics? Assuming that $E$ is expectation, what does it mean to have an expectation of a probability distribution $P$ ? Does it imply the mean of the distribution, and in that case what does $\hat x$ typically mean? Unfortunately the paper I'm reading doesn't spell these out so I'm guessing I lack a bit of statistics background here.","I don't get this notation at all and cannot find a place to start with understanding this: I don't get there. Does it mean expectation? The equation says it's ""negative log-likelihood"". I understand that is either 1 or 0. What does the mean, especially in the context of statistics? Assuming that is expectation, what does it mean to have an expectation of a probability distribution ? Does it imply the mean of the distribution, and in that case what does typically mean? Unfortunately the paper I'm reading doesn't spell these out so I'm guessing I lack a bit of statistics background here.",\mathcal L_D=-\mathbb E_{x\sim P}[\log(D(x))]-\mathbb E_{\hat x\sim Q}[\log(1-D(\hat x))] \mathbb E D(x) \mathbb E_{x\sim P} E P \hat x,"['probability', 'statistics', 'notation']"
20,The numbers don't add up in this Venn diagram?,The numbers don't add up in this Venn diagram?,,I'm trying to draw a Venn diagram to make sense of the overlapping 3 pairs of brightly colored and stripped socks but the numbers don't add up to the 10 pairs given in the question? How can there be 10 pairs of socks?,I'm trying to draw a Venn diagram to make sense of the overlapping 3 pairs of brightly colored and stripped socks but the numbers don't add up to the 10 pairs given in the question? How can there be 10 pairs of socks?,,['statistics']
21,"$X=\sum_{i=1}^{N}X_i$,estimator for $N$",",estimator for",X=\sum_{i=1}^{N}X_i N,"Let $X_i$, $i\geq 1$, be independent and identically distributed random variables having the uniform distribution over $(0,1)$. Let $X$ be defined as $X=\sum_{i=1}^{N}X_i$, where $N$ is an unknown integer. (a) Find an unbiased estimator $T(X)$ of $N$. (b) Decide with adequate reasons, if $\dfrac{T(X)}{N}$ converges to $1$ almost surely, as $N$ goes to infinity. At first I thought that since $E(X)=N\cdot \dfrac{1}{2}$, then the unbiased estimator, $T(X)=2X$. But one of my friend said that since $X$ contains $N$, $2X$ can not be an estimator here which is true. So, I am stuck. Any help appreciated. Thanks.","Let $X_i$, $i\geq 1$, be independent and identically distributed random variables having the uniform distribution over $(0,1)$. Let $X$ be defined as $X=\sum_{i=1}^{N}X_i$, where $N$ is an unknown integer. (a) Find an unbiased estimator $T(X)$ of $N$. (b) Decide with adequate reasons, if $\dfrac{T(X)}{N}$ converges to $1$ almost surely, as $N$ goes to infinity. At first I thought that since $E(X)=N\cdot \dfrac{1}{2}$, then the unbiased estimator, $T(X)=2X$. But one of my friend said that since $X$ contains $N$, $2X$ can not be an estimator here which is true. So, I am stuck. Any help appreciated. Thanks.",,"['statistics', 'statistical-inference', 'law-of-large-numbers']"
22,Why small p-value rejects $H_0$,Why small p-value rejects,H_0,"I can't understand why people seek small $p$-value. I mean, $p$-value is the smallest level at which $H_0$ can be rejected. Making level small we minimize the probability of type $1$ error, which as I think means that we are getting more evidence against $H_0$, since now the probability of accidentally rejecting $H_0$ when it is true decreases $=>$ probability of accepting $H_0$ when it is true is becoming bigger, because $P_{H_0}(test = 1)=1-P_{H_0}(test = 0)$ When $p$-value is small we reject $H_0$, but it seems to me that when $p$-value is small it must be vice versa, because the small $p$-value corresponds to the higher probability of accepting $H_0$ when it is true. I saw the answers that you suggested... but none of them helps. I know the 'interpretation' of p-value, I can imagine it as an area under curve too, but I can't understand the relation to what I wrote above.","I can't understand why people seek small $p$-value. I mean, $p$-value is the smallest level at which $H_0$ can be rejected. Making level small we minimize the probability of type $1$ error, which as I think means that we are getting more evidence against $H_0$, since now the probability of accidentally rejecting $H_0$ when it is true decreases $=>$ probability of accepting $H_0$ when it is true is becoming bigger, because $P_{H_0}(test = 1)=1-P_{H_0}(test = 0)$ When $p$-value is small we reject $H_0$, but it seems to me that when $p$-value is small it must be vice versa, because the small $p$-value corresponds to the higher probability of accepting $H_0$ when it is true. I saw the answers that you suggested... but none of them helps. I know the 'interpretation' of p-value, I can imagine it as an area under curve too, but I can't understand the relation to what I wrote above.",,"['probability', 'statistics']"
23,Alternate method to find distribution of function of X,Alternate method to find distribution of function of X,,"Let $X_1,X_2 ,... X_n ;(n\geq1)$ be a random sample from $Exp(1)$. Then the distribution of $2n\bar X$ is ? $(A) Exp\bigg(\dfrac{1}{2}\bigg)$ $(B) Exp(2n)$ $(C)\ \ \chi^2_{(n)}$ $(D)\ \ \chi^2_{(2n)}$ I solved this problem using Mgf : Let $Y=\sum X_i$ $Y\sim G(1,n)$ $M_Y(t)=(1-t)^{-n}$ $M_{2Y}(t)=(1-2t)^\frac{-2n}{2} \sim \chi^2_{(2n)}$ Correct me in the above steps and please tell me any alternate method(s) which cab be used in this problem or any shortcut. This question came in 1 mark so i think there is some shortcut here. (Poor English sorry ). Gamma distribution :If $Y\sim G(a,\lambda)=$ $\dfrac{\alpha^{  \lambda}}{\Gamma{(\lambda)}}e^{-ax}x^{\lambda-1} \ \ ; \ \ (a>0) ,(\lambda>0), (x>0)$ (Please try to use this notation for gamma density i just started learning these things so it can be a case i might get confused) .","Let $X_1,X_2 ,... X_n ;(n\geq1)$ be a random sample from $Exp(1)$. Then the distribution of $2n\bar X$ is ? $(A) Exp\bigg(\dfrac{1}{2}\bigg)$ $(B) Exp(2n)$ $(C)\ \ \chi^2_{(n)}$ $(D)\ \ \chi^2_{(2n)}$ I solved this problem using Mgf : Let $Y=\sum X_i$ $Y\sim G(1,n)$ $M_Y(t)=(1-t)^{-n}$ $M_{2Y}(t)=(1-2t)^\frac{-2n}{2} \sim \chi^2_{(2n)}$ Correct me in the above steps and please tell me any alternate method(s) which cab be used in this problem or any shortcut. This question came in 1 mark so i think there is some shortcut here. (Poor English sorry ). Gamma distribution :If $Y\sim G(a,\lambda)=$ $\dfrac{\alpha^{  \lambda}}{\Gamma{(\lambda)}}e^{-ax}x^{\lambda-1} \ \ ; \ \ (a>0) ,(\lambda>0), (x>0)$ (Please try to use this notation for gamma density i just started learning these things so it can be a case i might get confused) .",,"['probability', 'statistics', 'exponential-distribution', 'chi-squared']"
24,Why do we care if a set forms a basis?,Why do we care if a set forms a basis?,,"I am taking an advanced linear algebra course and am once again confused by a lot of the concepts. I understand that the definition of a basis is a set of vectors that spans the vector space and is linearly independent. Can anyone provide any intuition on why this matters? I can visualize (kind of) what it means for a set to span a vector space, and I understand (from regression analysis) why it is important that vectors are linearly independent, but I don't really understand why it matters if they are both, ie why it matters if they form a basis. For reference I am an undergraduate interested in statistics and data analysis. I have taken courses in mathematical statistics, regression analysis and am currently enrolled in time series analysis. I am somewhat familiar with PCA. Any intuition that can be provided through any of those lenses would be much appreciated.","I am taking an advanced linear algebra course and am once again confused by a lot of the concepts. I understand that the definition of a basis is a set of vectors that spans the vector space and is linearly independent. Can anyone provide any intuition on why this matters? I can visualize (kind of) what it means for a set to span a vector space, and I understand (from regression analysis) why it is important that vectors are linearly independent, but I don't really understand why it matters if they are both, ie why it matters if they form a basis. For reference I am an undergraduate interested in statistics and data analysis. I have taken courses in mathematical statistics, regression analysis and am currently enrolled in time series analysis. I am somewhat familiar with PCA. Any intuition that can be provided through any of those lenses would be much appreciated.",,"['linear-algebra', 'statistics', 'regression', 'change-of-basis']"
25,Hypergeometric Distribution Formula,Hypergeometric Distribution Formula,,"So I just learned the Hypergeometric Distribution Formula but we were not taught how it was derived, I can't find a solution online on why this formula works or any logical reasoning using counting techniques, attached is the formula","So I just learned the Hypergeometric Distribution Formula but we were not taught how it was derived, I can't find a solution online on why this formula works or any logical reasoning using counting techniques, attached is the formula",,"['statistics', 'probability-distributions']"
26,Dependant random variables with covariance equal to $0$,Dependant random variables with covariance equal to,0,"I need to find a pair of dependent random variables $(X, Y)$ with covariance equal to $0.$ From this I gather: $$0 = E((X-EX)(Y-EY)) = E \left(\left(X - \int_{-\infty}^\infty xf_X(x)\,dx\right) \left(Y - \int_{-\infty}^\infty xf_Y(x)\,dx \right)\right)$$ but what can I do now? How can I use the fact that they are dependent in the equation? Do you know of two such variables?","I need to find a pair of dependent random variables $(X, Y)$ with covariance equal to $0.$ From this I gather: $$0 = E((X-EX)(Y-EY)) = E \left(\left(X - \int_{-\infty}^\infty xf_X(x)\,dx\right) \left(Y - \int_{-\infty}^\infty xf_Y(x)\,dx \right)\right)$$ but what can I do now? How can I use the fact that they are dependent in the equation? Do you know of two such variables?",,['statistics']
27,Computing Type II Error for a One-Sided Normal Test.,Computing Type II Error for a One-Sided Normal Test.,,"For a random sample $X_1, X_2, \ldots, X_{49}$ taken from a population having standard deviation $4$ , the sample mean is found to be $20.3$ . Test the claim that the mean of the population is not less than $21$ at $1\%$ level of significance. Find the probability of Type II error if the population mean is $19.1$ . So to test for $H_0: \mu\geq 21$ versus alternate hypothesis $H_1: \mu < 21$ we calculate the test statistic $Z = \dfrac{20.3-21}{\frac{4}{\sqrt{49}}}=-1.225$ and since $-z_{0.01} = -2.326<-1.225$ , we cannot reject the null hypothesis at 1% level of significance. I am confused how to do the second part. I know that the Type 2 error will be $P(\text{accept} \ H_0 \mid \mu = 19.1)$ . How do I do this?","For a random sample taken from a population having standard deviation , the sample mean is found to be . Test the claim that the mean of the population is not less than at level of significance. Find the probability of Type II error if the population mean is . So to test for versus alternate hypothesis we calculate the test statistic and since , we cannot reject the null hypothesis at 1% level of significance. I am confused how to do the second part. I know that the Type 2 error will be . How do I do this?","X_1, X_2, \ldots, X_{49} 4 20.3 21 1\% 19.1 H_0: \mu\geq 21 H_1: \mu < 21 Z = \dfrac{20.3-21}{\frac{4}{\sqrt{49}}}=-1.225 -z_{0.01} = -2.326<-1.225 P(\text{accept} \ H_0 \mid \mu = 19.1)","['probability', 'statistics', 'hypothesis-testing']"
28,Standardizing Normally Distributed Random Variables,Standardizing Normally Distributed Random Variables,,"Having the normally distributed Random variables. We can normalize it and use table values in order to calculate probability of some event. The standardization takes formula $$z = \frac{ X - \text{expected value} }{ \text{variance}}$$ It is told that by doing this, we are forcing our variable $X$ to have expected value $0$ and variance $1$. However why is that? Why by doing steps above we force the distribution to behave like that? Thanks for help.","Having the normally distributed Random variables. We can normalize it and use table values in order to calculate probability of some event. The standardization takes formula $$z = \frac{ X - \text{expected value} }{ \text{variance}}$$ It is told that by doing this, we are forcing our variable $X$ to have expected value $0$ and variance $1$. However why is that? Why by doing steps above we force the distribution to behave like that? Thanks for help.",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
29,Question about getting probability from a CDF,Question about getting probability from a CDF,,"Just wondering but if I had the following CDF: $$ F_X(x) = \begin{cases} 0 & x < 0 \\ \frac{x+2}{8} & 0 \leq x < 6 \\ 1 & x \geq 6 \\ \end{cases} $$ If I wanted to calculate the probability $P(X=6)$ and $P(7\leq X \leq 8)$ How would I go about this ? In my textbook and in my class we have only ever talked/did examples where the probability were within the interval, say for this example,  $[0,5]$ so I am just curious on how I would do this type of question here. Thank you","Just wondering but if I had the following CDF: $$ F_X(x) = \begin{cases} 0 & x < 0 \\ \frac{x+2}{8} & 0 \leq x < 6 \\ 1 & x \geq 6 \\ \end{cases} $$ If I wanted to calculate the probability $P(X=6)$ and $P(7\leq X \leq 8)$ How would I go about this ? In my textbook and in my class we have only ever talked/did examples where the probability were within the interval, say for this example,  $[0,5]$ so I am just curious on how I would do this type of question here. Thank you",,"['probability', 'statistics']"
30,A game consists of tossing a fair die. A player wins if the number is even and loses if the number is odd.,A game consists of tossing a fair die. A player wins if the number is even and loses if the number is odd.,,"A game consists of tossing a fair die. A player wins if the number is even and loses if the number is odd. The winning or losing (dollar) payoff is equal to the number appearing. Find the player's mathematical expectation $E$. Winning chances or even #: 2,4,6 Lossing chances or odd #: 1,3,5 So $2\times \frac{1}{2} + 4\times \frac{1}{2}+ 6\times \frac{1}{2} = 6$ $1\times \frac{1}{2} + 3\times \frac{1}{2}+ 5\times \frac{1}{2} = 4.5$ so $6 - 4.5 = \$1.50$ But the answer in my book says it is $\$0.50$ What am I doing wrong?","A game consists of tossing a fair die. A player wins if the number is even and loses if the number is odd. The winning or losing (dollar) payoff is equal to the number appearing. Find the player's mathematical expectation $E$. Winning chances or even #: 2,4,6 Lossing chances or odd #: 1,3,5 So $2\times \frac{1}{2} + 4\times \frac{1}{2}+ 6\times \frac{1}{2} = 6$ $1\times \frac{1}{2} + 3\times \frac{1}{2}+ 5\times \frac{1}{2} = 4.5$ so $6 - 4.5 = \$1.50$ But the answer in my book says it is $\$0.50$ What am I doing wrong?",,"['probability', 'statistics', 'expectation']"
31,Variance of first n binary digits of pi?,Variance of first n binary digits of pi?,,I recently asked myself a question but didn't have the slightest inkling on how to solve it. Question If we convert $\pi$ to a binary digit. We know the first $n$ digits the average number of $0$'s are the same as the average number of $1$'s. My question is what is the variance as a function of the first $n$ digits?,I recently asked myself a question but didn't have the slightest inkling on how to solve it. Question If we convert $\pi$ to a binary digit. We know the first $n$ digits the average number of $0$'s are the same as the average number of $1$'s. My question is what is the variance as a function of the first $n$ digits?,,"['statistics', 'pi']"
32,M tosses $7$ fair coins and has $M$ heads. $A$ tosses $6$ fair coins and has $A$ heads. Find probability $P(M>A).$,M tosses  fair coins and has  heads.  tosses  fair coins and has  heads. Find probability,7 M A 6 A P(M>A).,"$M$ tosses $7$ fair coins and has M heads. A tosses $6$ fair coins and has $A$ heads. Find probability $P(M>A)$. I suppose that both distributions are binominal, but I don't know what to do next.","$M$ tosses $7$ fair coins and has M heads. A tosses $6$ fair coins and has $A$ heads. Find probability $P(M>A)$. I suppose that both distributions are binominal, but I don't know what to do next.",,['statistics']
33,Is there a definition of $[Y|X=x]$ as a random variable?,Is there a definition of  as a random variable?,[Y|X=x],"I recently witnessed statisticians discussing $[Y|X=x]$ as if it were a random variable. In particular, they were making assumptions about its distribution. I am familiar with various definitions of conditional expectations, in particular with $\mathbb E[Y|X=x]$. And I know that $\mathbb E[Y|X]$ is a random variable. But don't remember ever seeing $[Y|X=x]$ as a random variable. What could be its definition? A bit of context added : In the context of inference, let $X$ be the predictor r.v. and $Y$ the response. Define $r(x)= \mathbb E(Y|X = x)$ and call it the regression function. Then (according to the statisticians) we make an assumption on the distribution of $(Y|X = x)$.","I recently witnessed statisticians discussing $[Y|X=x]$ as if it were a random variable. In particular, they were making assumptions about its distribution. I am familiar with various definitions of conditional expectations, in particular with $\mathbb E[Y|X=x]$. And I know that $\mathbb E[Y|X]$ is a random variable. But don't remember ever seeing $[Y|X=x]$ as a random variable. What could be its definition? A bit of context added : In the context of inference, let $X$ be the predictor r.v. and $Y$ the response. Define $r(x)= \mathbb E(Y|X = x)$ and call it the regression function. Then (according to the statisticians) we make an assumption on the distribution of $(Y|X = x)$.",,"['probability-theory', 'statistics', 'random-variables', 'definition', 'conditional-expectation']"
34,Derivation of $\sigma^2 = \frac1N\sum x^2 - \frac1{N^2}\sum\bar{x}^2$,Derivation of,\sigma^2 = \frac1N\sum x^2 - \frac1{N^2}\sum\bar{x}^2,"I saw the above equation in an introductory statistics textbook, as a shortcut for evaluating the variance of a population. I tried to prove it myself: $$\sigma^2 = \frac{\sum (x - \bar{x})^2}{N} \tag{1}$$ $$\sigma^2 = \frac{\sum x^2 - \frac{(\sum x)^2}{N}}{N} \tag{2}$$ We are given that $(1) = (2)$: $$\frac{\sum (x - \bar{x})^2}{N} = \frac{\sum x^2 - \frac{(\sum x)^2}{N}}{N} \tag{3}$$ Multiply $(3)$ through by $N$: $$\sum(x - \bar{x})^2 = \sum x^2 - \frac{(\sum x)^2}{N} \tag{4}$$ Expand the LHS in $(4)$: $$\sum\left(x^2 - 2x\bar{x} + \bar{x}^2\right) = {\sum x^2 - \frac{(\sum x)^2}{N}} \tag{5}$$ Expanding both sides in $(5)$: $$\sum x^2 - 2x\sum\bar{x} + \sum\bar{x}^2 = \sum x^2 - \frac{\sum x\sum x}{N} \tag{6}$$ From $(6)$: $$\sum\bar{x}^2 - 2\bar{x}\sum{x} = -\bar{x}\sum{x} \tag{7}$$ From $(7)$: $$\sum\bar{x}^2 = \bar{x}\sum{x} \tag{8}$$ I don't know how to make the LHS equal RHS in $(8)$.","I saw the above equation in an introductory statistics textbook, as a shortcut for evaluating the variance of a population. I tried to prove it myself: $$\sigma^2 = \frac{\sum (x - \bar{x})^2}{N} \tag{1}$$ $$\sigma^2 = \frac{\sum x^2 - \frac{(\sum x)^2}{N}}{N} \tag{2}$$ We are given that $(1) = (2)$: $$\frac{\sum (x - \bar{x})^2}{N} = \frac{\sum x^2 - \frac{(\sum x)^2}{N}}{N} \tag{3}$$ Multiply $(3)$ through by $N$: $$\sum(x - \bar{x})^2 = \sum x^2 - \frac{(\sum x)^2}{N} \tag{4}$$ Expand the LHS in $(4)$: $$\sum\left(x^2 - 2x\bar{x} + \bar{x}^2\right) = {\sum x^2 - \frac{(\sum x)^2}{N}} \tag{5}$$ Expanding both sides in $(5)$: $$\sum x^2 - 2x\sum\bar{x} + \sum\bar{x}^2 = \sum x^2 - \frac{\sum x\sum x}{N} \tag{6}$$ From $(6)$: $$\sum\bar{x}^2 - 2\bar{x}\sum{x} = -\bar{x}\sum{x} \tag{7}$$ From $(7)$: $$\sum\bar{x}^2 = \bar{x}\sum{x} \tag{8}$$ I don't know how to make the LHS equal RHS in $(8)$.",,"['statistics', 'standard-deviation']"
35,Probability / Permutation of large number of events.,Probability / Permutation of large number of events.,,"This seems like a very silly and simple question, but I'm here to learn. So here it is: How can I compute the probability of getting heads x times when tossing a coin a large number of times?  For example, what is the probability of getting heads 181 times when tossing a coin 300 times? I understand how to do this using permutations, factorials, and traditional (elementary) probability methods. My problem is that my calculator won't go that high. I'm unable to use (300!)/[(181!)(119!)] because I get an overflow error. The question I'm solving doesn't specifically ask for the probability, but I'd like to learn. Thanks!","This seems like a very silly and simple question, but I'm here to learn. So here it is: How can I compute the probability of getting heads x times when tossing a coin a large number of times?  For example, what is the probability of getting heads 181 times when tossing a coin 300 times? I understand how to do this using permutations, factorials, and traditional (elementary) probability methods. My problem is that my calculator won't go that high. I'm unable to use (300!)/[(181!)(119!)] because I get an overflow error. The question I'm solving doesn't specifically ask for the probability, but I'd like to learn. Thanks!",,"['probability', 'algebra-precalculus', 'statistics', 'permutations']"
36,Calculating sample variance.,Calculating sample variance.,,"This question is part of another question and the solution skips over how the value of the variance is found and I can't figure out how the variance was obtained for the rest of the question. We are given a sample of 15 heights of people $X_1, X_2, ... , X_{15}$ in metres. Also given $\sum \limits_{i=1}^{15}x_i = 26.7$, and $\sum \limits_{i=1}^{15}x_i^2 = 173.526$ The solution says that $\overline{x}=1.78$, and $S^2 = 9$. I think that $S^2 = \dfrac{1}{14}\sum \limits_{i=1}^{15}(x_i-1.78)^2$ but I can't figure out what $x_i$ is supposed to be to get $9$. Thanks.","This question is part of another question and the solution skips over how the value of the variance is found and I can't figure out how the variance was obtained for the rest of the question. We are given a sample of 15 heights of people $X_1, X_2, ... , X_{15}$ in metres. Also given $\sum \limits_{i=1}^{15}x_i = 26.7$, and $\sum \limits_{i=1}^{15}x_i^2 = 173.526$ The solution says that $\overline{x}=1.78$, and $S^2 = 9$. I think that $S^2 = \dfrac{1}{14}\sum \limits_{i=1}^{15}(x_i-1.78)^2$ but I can't figure out what $x_i$ is supposed to be to get $9$. Thanks.",,['statistics']
37,Find probability of addition of random variables,Find probability of addition of random variables,,"Suppose you toss a coin four times. The sample space Ω = {HHHH,HHHT,HHTH,...,TTTT} contains 16 outcomes and you should assume   each outcome is equally likely. Let X be the Binomial random variable   that corresponds to the number of heads in an outcome, e.g., X(HTHT) =   2. Let Y be the Bernoulli random variable that evaluates to 1 if there is an even number of heads in the outcome, e.g., Y (HHHT) = 0 and Y   (HTHT) = 1. Let Z = X +Y, e.g., Z(HTHT) = X(HTHT)+Y(HTHT) = 2+1 = 3. What are the   values of: P(Z=0) P(Z=1)...P(Z=5) Lets talk about Z=0. In order for Z=0 to be true, X=0 and Y=0. So then, to get P(Z=0), do I have to find the intersection of X=0 and Y=0 or do I add P(X=0) and P(Y=0)?","Suppose you toss a coin four times. The sample space Ω = {HHHH,HHHT,HHTH,...,TTTT} contains 16 outcomes and you should assume   each outcome is equally likely. Let X be the Binomial random variable   that corresponds to the number of heads in an outcome, e.g., X(HTHT) =   2. Let Y be the Bernoulli random variable that evaluates to 1 if there is an even number of heads in the outcome, e.g., Y (HHHT) = 0 and Y   (HTHT) = 1. Let Z = X +Y, e.g., Z(HTHT) = X(HTHT)+Y(HTHT) = 2+1 = 3. What are the   values of: P(Z=0) P(Z=1)...P(Z=5) Lets talk about Z=0. In order for Z=0 to be true, X=0 and Y=0. So then, to get P(Z=0), do I have to find the intersection of X=0 and Y=0 or do I add P(X=0) and P(Y=0)?",,"['statistics', 'random-variables']"
38,How can I interpret the image for prior probability?,How can I interpret the image for prior probability?,,"The image included below is about Bayesian statistics. While looking at the lecture, the lecturer expressed the probability distribution of prior probability as a uniform distribution. Somehow, I feel like I have difficulty in interpreting the X-axis and Y-axis. Can somebody explain what the image tells?","The image included below is about Bayesian statistics. While looking at the lecture, the lecturer expressed the probability distribution of prior probability as a uniform distribution. Somehow, I feel like I have difficulty in interpreting the X-axis and Y-axis. Can somebody explain what the image tells?",,"['statistics', 'recreational-mathematics', 'bayesian', 'data-analysis', 'data-mining']"
39,Weighted Average from Errors,Weighted Average from Errors,,"This should be a really easy problem, but I want to know the right way to do it instead of making something up. Data: Given the data set ( the unit is nano meters ): $$ D = \{ 3386, 3290, 3372, 3450 \} $$ The following is the respective errors: $$ \sigma_D = \{ 50, 180, 42, 100 \} $$ Question: How do I find the weighted average?","This should be a really easy problem, but I want to know the right way to do it instead of making something up. Data: Given the data set ( the unit is nano meters ): $$ D = \{ 3386, 3290, 3372, 3450 \} $$ The following is the respective errors: $$ \sigma_D = \{ 50, 180, 42, 100 \} $$ Question: How do I find the weighted average?",,"['statistics', 'average']"
40,Finding a solution with minimal $\ell_2$ norm in linear regression with dependent variables,Finding a solution with minimal  norm in linear regression with dependent variables,\ell_2,"I am trying to find a linear regression for the problem: $$\displaystyle\arg\min_w\|y-Xw\|^2 $$ By finding the optimum of the above equation, I get $$\displaystyle X^TXw=X^Ty $$ In the case where $X^TX$ is invertible (i.e. the variables are independent), I can get the unique solution $$\displaystyle w=(X^TX)^{-1}X^T $$ However, when the variables are dependent , there's more than one unique solution. Now, say  I want to find a solution with minimal $l_2$ norm. I can define the new problem as: $$\displaystyle\begin{align}\arg&\min_w\|w\| \\ &s.t. X^TXw=X^Ty  \end{align}$$ How can I now use SVD decomposition ($X=U\Sigma V^T$) to solve the above optimization problem? Solving with lagrange method: I tried optimizing the equivalent $0.5\|w\|^2$, and got the following Lagrangian: $$ \mathcal{L}(w,\alpha)=0.5\|w\|^2+\alpha(X^Ty-X^TXw) $$ When the gradient w.r.t $w$ is equal to 0, I get: $$w = \alpha X^TX\\ X^Ty=X^TXw $$ But couldn't proceed from here","I am trying to find a linear regression for the problem: $$\displaystyle\arg\min_w\|y-Xw\|^2 $$ By finding the optimum of the above equation, I get $$\displaystyle X^TXw=X^Ty $$ In the case where $X^TX$ is invertible (i.e. the variables are independent), I can get the unique solution $$\displaystyle w=(X^TX)^{-1}X^T $$ However, when the variables are dependent , there's more than one unique solution. Now, say  I want to find a solution with minimal $l_2$ norm. I can define the new problem as: $$\displaystyle\begin{align}\arg&\min_w\|w\| \\ &s.t. X^TXw=X^Ty  \end{align}$$ How can I now use SVD decomposition ($X=U\Sigma V^T$) to solve the above optimization problem? Solving with lagrange method: I tried optimizing the equivalent $0.5\|w\|^2$, and got the following Lagrangian: $$ \mathcal{L}(w,\alpha)=0.5\|w\|^2+\alpha(X^Ty-X^TXw) $$ When the gradient w.r.t $w$ is equal to 0, I get: $$w = \alpha X^TX\\ X^Ty=X^TXw $$ But couldn't proceed from here",,"['linear-algebra', 'statistics', 'optimization', 'svd', 'linear-regression']"
41,Permutations vs. Combinations,Permutations vs. Combinations,,What are the word modifiers in order to distinguish a given if it is a permutation or a combination? Or simply describe what is permutation and combination.,What are the word modifiers in order to distinguish a given if it is a permutation or a combination? Or simply describe what is permutation and combination.,,"['statistics', 'descriptive-statistics']"
42,Professor leaves Princeton to go to Stanford and increases the quality of both departments,Professor leaves Princeton to go to Stanford and increases the quality of both departments,,An old joke is that a certain professor left Princeton to go to Stanford and thereby improved the average quality of both departments. Is this mathematically possible? I found it in statistics course while I am studying. It sounds like ordered statistics. How it is possible? Thanks.,An old joke is that a certain professor left Princeton to go to Stanford and thereby improved the average quality of both departments. Is this mathematically possible? I found it in statistics course while I am studying. It sounds like ordered statistics. How it is possible? Thanks.,,['statistics']
43,Variances of unbiased estimator $W_1$ and $E(W_2|T)$ for another unbiased estimator $W_2$ and a sufficient statistic $T$,Variances of unbiased estimator  and  for another unbiased estimator  and a sufficient statistic,W_1 E(W_2|T) W_2 T,"Let $W_1, W_2$ be unbiased estimators of $\theta$ and let $T$ be a sufficient statistic for $\theta$. Is it true that $Var(E(W_1|T)) \leq Var(W_2)$? I think that it may fails to be true. If the statement is true then $Var(E(W_1|T))$ is the minimum among the variances $Var(W)$ of all unbiased estimator $W$ so $E(W_1|T)$ is a UMVUE of $\theta$. But Lehmann-Scheffe's theorem tells us that if $T$ is a complete sufficient statistic and $U$ is a unbiased estimator then $E(U|T)$ is a UMVUE of $\theta$. So we don't need the completeness of $T$ if the statement is true. However I cannot prove it. Anyone can help me?","Let $W_1, W_2$ be unbiased estimators of $\theta$ and let $T$ be a sufficient statistic for $\theta$. Is it true that $Var(E(W_1|T)) \leq Var(W_2)$? I think that it may fails to be true. If the statement is true then $Var(E(W_1|T))$ is the minimum among the variances $Var(W)$ of all unbiased estimator $W$ so $E(W_1|T)$ is a UMVUE of $\theta$. But Lehmann-Scheffe's theorem tells us that if $T$ is a complete sufficient statistic and $U$ is a unbiased estimator then $E(U|T)$ is a UMVUE of $\theta$. So we don't need the completeness of $T$ if the statement is true. However I cannot prove it. Anyone can help me?",,"['probability', 'statistics', 'statistical-inference']"
44,UMVUE of a parameter for Pareto Distribution,UMVUE of a parameter for Pareto Distribution,,"Problem: Let $ (X_1,X_2, \ldots, X_n) $ be a random sample from Pareto Distribution with pdf $ f(x; \theta) = (\frac{\theta}{c})(\frac{c}{x})^{\theta + 1}, ~~ x> c $. Find the UMVUE of parameter $ \theta$. My work so far: I proved that the UMVUE cannot be found through Cramer-Rao method. Then I tried with the classic Rao Blackwell method: I proved that the given Pareto Distribution is included in the Exponential Family and $T = \sum_{i=1}^{n} \log X_i $ is a complete and sufficient statistic of $ \theta $. Now I am stuck because I could not find an unbiased estimator of $ \theta $ to proceed.","Problem: Let $ (X_1,X_2, \ldots, X_n) $ be a random sample from Pareto Distribution with pdf $ f(x; \theta) = (\frac{\theta}{c})(\frac{c}{x})^{\theta + 1}, ~~ x> c $. Find the UMVUE of parameter $ \theta$. My work so far: I proved that the UMVUE cannot be found through Cramer-Rao method. Then I tried with the classic Rao Blackwell method: I proved that the given Pareto Distribution is included in the Exponential Family and $T = \sum_{i=1}^{n} \log X_i $ is a complete and sufficient statistic of $ \theta $. Now I am stuck because I could not find an unbiased estimator of $ \theta $ to proceed.",,"['statistics', 'probability-distributions', 'statistical-inference']"
45,Using Poisson to find probability in a range,Using Poisson to find probability in a range,,Suppose the counts recorded by a Geiger counter follow a Poisson process with an average of three counts per minute. What is the probability that the first count occurs between 1 and 2 minutes after start-up? $\lambda=3$ per minute Am I supposed to calculate $P(X=2)-P(X=1)$? And would $\lambda$ change between those two terms because of the different time intervals? Very confused.,Suppose the counts recorded by a Geiger counter follow a Poisson process with an average of three counts per minute. What is the probability that the first count occurs between 1 and 2 minutes after start-up? $\lambda=3$ per minute Am I supposed to calculate $P(X=2)-P(X=1)$? And would $\lambda$ change between those two terms because of the different time intervals? Very confused.,,"['probability', 'statistics']"
46,Why does a confidence interval go to 0 if sample size is increased to population size?,Why does a confidence interval go to 0 if sample size is increased to population size?,,"Minitab blog claims that: ""As you increase the sample size, the sampling error decreases and the   intervals become narrower. If you could increase the sample size to   equal the population, there would be no sampling error. In this case,   the confidence interval would have a width of zero and be equal to the   true population parameter."" That makes sense intuitively, but I don't see it mathematically. Lets say I had a population size of 100 with a known mean and population standard deviation. If I were to sample the entire population, my confidence interval would not be 0: $$Mean_{sample} (+ OR-) z*\sigma= CI$$ Mean sample would become population mean and z*standard devation would become some nonzero number (depending on the confidence level chosen for z). Why doesn't the $z*\sigma$  term go to zero for a noninfinite sized population?","Minitab blog claims that: ""As you increase the sample size, the sampling error decreases and the   intervals become narrower. If you could increase the sample size to   equal the population, there would be no sampling error. In this case,   the confidence interval would have a width of zero and be equal to the   true population parameter."" That makes sense intuitively, but I don't see it mathematically. Lets say I had a population size of 100 with a known mean and population standard deviation. If I were to sample the entire population, my confidence interval would not be 0: $$Mean_{sample} (+ OR-) z*\sigma= CI$$ Mean sample would become population mean and z*standard devation would become some nonzero number (depending on the confidence level chosen for z). Why doesn't the $z*\sigma$  term go to zero for a noninfinite sized population?",,"['statistics', 'statistical-inference']"
47,Two-Term Exponential Curve Fitting,Two-Term Exponential Curve Fitting,,"I want to fit a pair of experimental data $(M_z(t), t),$ which can theoretically be modelled according to:$$M_z(t)=M_z(0) e^{-t/T_1} + M_0 (1-e^{-t/T_1}) \tag{1}.$$I want to use the equation of the fitting to solve for $T_1.$ The parameters $M_0,$ and $M_z$ are defined in the diagram below: What kind of fitting should be used here? Attempt: I have tried a two-term exponential fitting in Matlab, it is a good fit but I can't see how to deduce $T_1$ from the resulting equation: f(x) = a*exp(b*x) + c*exp(d*x)     a =   2.642e+04  (2.623e+04, 2.662e+04)    b =   1.347e-06  (-2.953e-05, 3.222e-05)    c =   -2.45e+04  (-2.478e+04, -2.423e+04)    d =    -0.07508  (-0.07726, -0.0729) Which exponent ($d$ or $b$) should be used for finding $T_1$? This was my experimental data: x=[2    5   10  20  30  50  100 200 400]; y=[5418.583 9479.431    14828.01    21052.6 23872.96    25784.02    26420.23    26445.85    26433.05]; And the resulting exponential fit: P.S. I can't do a log plot fitting because I don't know the value of $M_0.$","I want to fit a pair of experimental data $(M_z(t), t),$ which can theoretically be modelled according to:$$M_z(t)=M_z(0) e^{-t/T_1} + M_0 (1-e^{-t/T_1}) \tag{1}.$$I want to use the equation of the fitting to solve for $T_1.$ The parameters $M_0,$ and $M_z$ are defined in the diagram below: What kind of fitting should be used here? Attempt: I have tried a two-term exponential fitting in Matlab, it is a good fit but I can't see how to deduce $T_1$ from the resulting equation: f(x) = a*exp(b*x) + c*exp(d*x)     a =   2.642e+04  (2.623e+04, 2.662e+04)    b =   1.347e-06  (-2.953e-05, 3.222e-05)    c =   -2.45e+04  (-2.478e+04, -2.423e+04)    d =    -0.07508  (-0.07726, -0.0729) Which exponent ($d$ or $b$) should be used for finding $T_1$? This was my experimental data: x=[2    5   10  20  30  50  100 200 400]; y=[5418.583 9479.431    14828.01    21052.6 23872.96    25784.02    26420.23    26445.85    26433.05]; And the resulting exponential fit: P.S. I can't do a log plot fitting because I don't know the value of $M_0.$",,"['statistics', 'exponential-function', 'matlab', 'regression', 'estimation']"
48,Need to develop a formula for index/indicator measuring wait time for tellers,Need to develop a formula for index/indicator measuring wait time for tellers,,"I have data that looks like this: Tell_1  Tell_2  Tell_3         0       0      -8        -3       0       0         0       0       0        -4      -2       0         0       0      -2       -14      -4      -1         0       0       0        -1       0      -1  Index:  I1      I2      I3 '---------------------------------------------- This data represents the wait time of each customer at a teller. The value 0 means the customer did not wait at all. The value -x means the customer had to wait x minutes before being served. How can I develop an teller performance index (a mathematical function that takes the wait times an produce 1 value) that shows the teller performance ($I1$, $I2$, $I3$,...)? One way is to just sum each column to obtain -22,-6, and -12. This is only good at showing wait times. However, this does not show how many customers were served immediately (which is a good thing). As a result this index is no good. Another approach that I considered was to assume a max wait time value, say 100, then calculate the index per teller as $\sum(x_i+100)$ to get: 778, 794, and 788. But how good is this one? I need the index to reflect both the wait times and the number of customers that were served immediately. Note: This is not a homework, also, it is not a real situation. Thanks for your help.","I have data that looks like this: Tell_1  Tell_2  Tell_3         0       0      -8        -3       0       0         0       0       0        -4      -2       0         0       0      -2       -14      -4      -1         0       0       0        -1       0      -1  Index:  I1      I2      I3 '---------------------------------------------- This data represents the wait time of each customer at a teller. The value 0 means the customer did not wait at all. The value -x means the customer had to wait x minutes before being served. How can I develop an teller performance index (a mathematical function that takes the wait times an produce 1 value) that shows the teller performance ($I1$, $I2$, $I3$,...)? One way is to just sum each column to obtain -22,-6, and -12. This is only good at showing wait times. However, this does not show how many customers were served immediately (which is a good thing). As a result this index is no good. Another approach that I considered was to assume a max wait time value, say 100, then calculate the index per teller as $\sum(x_i+100)$ to get: 778, 794, and 788. But how good is this one? I need the index to reflect both the wait times and the number of customers that were served immediately. Note: This is not a homework, also, it is not a real situation. Thanks for your help.",,"['algebra-precalculus', 'statistics', 'mathematical-modeling']"
49,Finding P(X<Y) of 2 random variables that are gamma distributed,Finding P(X<Y) of 2 random variables that are gamma distributed,,"X ~ Gamma ($r_1$ = 3, $\lambda_1$ = 2) Y ~ Gamma ($r_2$ = 5, $\lambda_2$ = 7) Find P( X > Y ) To find this, I know that I can double integrate the joint distribution of X and Y but I wanted to know a more intuitive approach to this problem. I was suggested to think of this probability as a binomial distribution where T ~ Bin( n = $r_1$ + $r_2$ - 1 = 7, p = $\left(\frac{\lambda_2}{\lambda_1 + \lambda_2}\right)$) I would like to have this explained please.","X ~ Gamma ($r_1$ = 3, $\lambda_1$ = 2) Y ~ Gamma ($r_2$ = 5, $\lambda_2$ = 7) Find P( X > Y ) To find this, I know that I can double integrate the joint distribution of X and Y but I wanted to know a more intuitive approach to this problem. I was suggested to think of this probability as a binomial distribution where T ~ Bin( n = $r_1$ + $r_2$ - 1 = 7, p = $\left(\frac{\lambda_2}{\lambda_1 + \lambda_2}\right)$) I would like to have this explained please.",,"['statistics', 'random-variables', 'gamma-distribution']"
50,"If Mutual Information measures dependence, why is it symmetric?","If Mutual Information measures dependence, why is it symmetric?",,"From Wikipedia we can read: In probability theory and information theory, the mutual information   (MI) of two random variables is a measure of the mutual dependence   between the two variables. in fact, $I(X;Y)=I(Y;X)$. But we know that MI can measure also non-linear dependence. To clarify that concept, I made this Venn diagram to describe what I know about dependence, linear correlation and causality in probability and statistics. $Y = X^2$ is an example of dependence between two RVs, that is not contained in the set of correlation, and cannot be detected by Pearson's coefficient. In that case, mutual information will be greater than zero suggesting a mutual dependence. But that's not true! I mean: Y depends on X but not viceversa. If Mutual Information measures dependence, why is it symmetric, while dependence is not?","From Wikipedia we can read: In probability theory and information theory, the mutual information   (MI) of two random variables is a measure of the mutual dependence   between the two variables. in fact, $I(X;Y)=I(Y;X)$. But we know that MI can measure also non-linear dependence. To clarify that concept, I made this Venn diagram to describe what I know about dependence, linear correlation and causality in probability and statistics. $Y = X^2$ is an example of dependence between two RVs, that is not contained in the set of correlation, and cannot be detected by Pearson's coefficient. In that case, mutual information will be greater than zero suggesting a mutual dependence. But that's not true! I mean: Y depends on X but not viceversa. If Mutual Information measures dependence, why is it symmetric, while dependence is not?",,"['probability', 'statistics', 'independence']"
51,Why is it true that $S'(t)/S(t) = d log(S(t)) / dt$?,Why is it true that ?,S'(t)/S(t) = d log(S(t)) / dt,I came across this identity in derivation of the hazard rate in survival analysis.,I came across this identity in derivation of the hazard rate in survival analysis.,,"['statistics', 'derivatives', 'logarithms']"
52,What is the probability that a person wearing a blue t-shirt will sit next to one wearing red?,What is the probability that a person wearing a blue t-shirt will sit next to one wearing red?,,"9 people sit in a row linearly. 2 dressed in Red, 3 blue and 4 in yellow. What is the probability that a person in blue will sit next to a person in red? Why? RRBBBYYYY this sequence from what I gather can be arranged in 9! ways. Attempt There are 2 groups R and Y we want to seat together. The Y(people wearing yellow) can be arranged in 4! ways and people wearing red in 2! ways.  Therefore 2 x 3! x 2! should be the right answer. Times 2 because we only want to consider two groups. (I think this is where I am going wrong because there are 3) If there were two groups of people the. i am guessing it would be right. However there are 3 groups and I have no idea how to show it. Attempt There is a total lf 9!/2!x3!x4 = 1260 ways they can be seated. These however will be in random order and I can't figure out how to put them in an order that R sits next to Y.","9 people sit in a row linearly. 2 dressed in Red, 3 blue and 4 in yellow. What is the probability that a person in blue will sit next to a person in red? Why? RRBBBYYYY this sequence from what I gather can be arranged in 9! ways. Attempt There are 2 groups R and Y we want to seat together. The Y(people wearing yellow) can be arranged in 4! ways and people wearing red in 2! ways.  Therefore 2 x 3! x 2! should be the right answer. Times 2 because we only want to consider two groups. (I think this is where I am going wrong because there are 3) If there were two groups of people the. i am guessing it would be right. However there are 3 groups and I have no idea how to show it. Attempt There is a total lf 9!/2!x3!x4 = 1260 ways they can be seated. These however will be in random order and I can't figure out how to put them in an order that R sits next to Y.",,"['probability', 'combinatorics', 'statistics']"
53,Understanding a Substep of the Proof for the Law of Total Variance,Understanding a Substep of the Proof for the Law of Total Variance,,"In the proof for the Law of Total Variance , the following lemma seems to be appealed to (when going from the 2nd to the 3rd step of the proof): $$ E[E[Y^2 \mid X]] = E[\text{Var}[Y \mid X] + [E[Y \mid X]]^2] $$ Where does this come from and/or what justifies it?","In the proof for the Law of Total Variance , the following lemma seems to be appealed to (when going from the 2nd to the 3rd step of the proof): $$ E[E[Y^2 \mid X]] = E[\text{Var}[Y \mid X] + [E[Y \mid X]]^2] $$ Where does this come from and/or what justifies it?",,"['probability', 'statistics', 'variance']"
54,Trouble with a Probability (Integral) Computation from *All of Statistics*,Trouble with a Probability (Integral) Computation from *All of Statistics*,,"From All of Statistics pg. 36: Suppose that $X$ and $Y$ are independent and both have the same density $f(x) = 2x$ for $0 \le x \le 1$ and $f(x) = 0$ otherwise. Let us find $\mathbb{P}(X + Y \le 1)$. Using independence, the joint density is $$ f(x,y) = f_X(x) f_Y(y) = 4xy \text { if } 0 \le x \le 1, 0 \le y \le 1 $$ Now \begin{equation} \begin{split} \mathbb{P}(X + Y \le 1) & = \int \int_{x+y \le 1} f(x,y) dy dx \\                         & = 4 \int_0^1 x \left[ \int_0^{1-x} y dy \right] dx \\                         & = 4 \int_0^1 x \frac{(1-x)^2}{2} dx \\                         & = \frac{1}{6} \end{split} \end{equation} Question 1: Why are we integrating on $\int_0^1$ for the outside integral and on $\int_0^{1-x}$ on the inside integral? Question 2: According to the last step, it must be that $\int_0^1 x \frac{(1-x)^2}{2} dx = 1/24$. But why is this the case? How is this integral computed?","From All of Statistics pg. 36: Suppose that $X$ and $Y$ are independent and both have the same density $f(x) = 2x$ for $0 \le x \le 1$ and $f(x) = 0$ otherwise. Let us find $\mathbb{P}(X + Y \le 1)$. Using independence, the joint density is $$ f(x,y) = f_X(x) f_Y(y) = 4xy \text { if } 0 \le x \le 1, 0 \le y \le 1 $$ Now \begin{equation} \begin{split} \mathbb{P}(X + Y \le 1) & = \int \int_{x+y \le 1} f(x,y) dy dx \\                         & = 4 \int_0^1 x \left[ \int_0^{1-x} y dy \right] dx \\                         & = 4 \int_0^1 x \frac{(1-x)^2}{2} dx \\                         & = \frac{1}{6} \end{split} \end{equation} Question 1: Why are we integrating on $\int_0^1$ for the outside integral and on $\int_0^{1-x}$ on the inside integral? Question 2: According to the last step, it must be that $\int_0^1 x \frac{(1-x)^2}{2} dx = 1/24$. But why is this the case? How is this integral computed?",,"['probability', 'integration', 'statistics']"
55,"Hint for MLE of $a$ for a random sample from a uniform distribution in $[-a, a]$",Hint for MLE of  for a random sample from a uniform distribution in,"a [-a, a]","Suppose $X_1, \dots, X_n$ are iid from a uniform distribution in $[-a, a]$. Their PDF is thus given by $$f(x) = \dfrac{1}{2a}\mathbb{I}(x \in [-a ,a])\text{.}$$ $\mathbb{I}$, above, denotes the indicator function. I wish to find a MLE for $a$. The likelihood function is given by $$L(a \mid x_1, \dots, x_n) = \left(\dfrac{1}{2a}\right)^n\prod_{i=1}^{n}\mathbb{I}(x_i \in [-a, a])\text{.}$$ I am quite certain that one cannot do the traditional method of finding the loglikelihood, setting its derivative equal to $0$, and solving for $a$. I know there's some trick to doing this - perhaps involving an order statistic - but it has been a long time since I've seen the trick. Can I have a hint, not a complete solution on how to proceed with this problem? Edit: Intuitively speaking, I think that if I wanted to choose $a$ to maximize $L$, I wouldn't choose $a$ to be a negative value. Actually, I would choose $a$ to be the smallest value of $x_i$ which is greater than $0$ - since it will be placed in the denominator of $L$ and as $x_i$ is larger, $L$ gets smaller.","Suppose $X_1, \dots, X_n$ are iid from a uniform distribution in $[-a, a]$. Their PDF is thus given by $$f(x) = \dfrac{1}{2a}\mathbb{I}(x \in [-a ,a])\text{.}$$ $\mathbb{I}$, above, denotes the indicator function. I wish to find a MLE for $a$. The likelihood function is given by $$L(a \mid x_1, \dots, x_n) = \left(\dfrac{1}{2a}\right)^n\prod_{i=1}^{n}\mathbb{I}(x_i \in [-a, a])\text{.}$$ I am quite certain that one cannot do the traditional method of finding the loglikelihood, setting its derivative equal to $0$, and solving for $a$. I know there's some trick to doing this - perhaps involving an order statistic - but it has been a long time since I've seen the trick. Can I have a hint, not a complete solution on how to proceed with this problem? Edit: Intuitively speaking, I think that if I wanted to choose $a$ to maximize $L$, I wouldn't choose $a$ to be a negative value. Actually, I would choose $a$ to be the smallest value of $x_i$ which is greater than $0$ - since it will be placed in the denominator of $L$ and as $x_i$ is larger, $L$ gets smaller.",,"['probability', 'statistics']"
56,Suppose that in fact $9\%$ of the oranges on the truck do not meet the desired standard. What's the probability that the shipment will be rejected?,Suppose that in fact  of the oranges on the truck do not meet the desired standard. What's the probability that the shipment will be rejected?,9\%,"I am tutoring a student in AP stats, and came across this question. And I have not been able to solve this problem and get a result that is one of the multiple choices. The closest I got to was B (which I think is the intended answer), but I feel like the answer choices are incorrect. Problem: When a truckload of oranges arrives at a packing plant, a random sample of 125 is selected and examined, The whole truckload will be rejected if more than $8\%$ of the sample is unsatisfactory. Suppose that in fact $9\%$ of the oranges on the truck do not meet the desired standard. What's the probability that the shipment will be rejected? A) $0.6966$ B) $0.3483$ C) $0.6517$ D) $0.7803$ E) $0.2197$","I am tutoring a student in AP stats, and came across this question. And I have not been able to solve this problem and get a result that is one of the multiple choices. The closest I got to was B (which I think is the intended answer), but I feel like the answer choices are incorrect. Problem: When a truckload of oranges arrives at a packing plant, a random sample of 125 is selected and examined, The whole truckload will be rejected if more than $8\%$ of the sample is unsatisfactory. Suppose that in fact $9\%$ of the oranges on the truck do not meet the desired standard. What's the probability that the shipment will be rejected? A) $0.6966$ B) $0.3483$ C) $0.6517$ D) $0.7803$ E) $0.2197$",,"['probability', 'statistics']"
57,What is the probability that three are males and two are female?,What is the probability that three are males and two are female?,,The human sex ratio at birth is commonly thought to be 107 boys to 100 girls. Suppose five infants are chosen at random. (A) What is the probability that three are males and two are female? (B) What is the probability that at least one of them is a male? My work: (A) $(5C2 * 5C3)$ / $207C5$ (B) $(5C1 * 202C)4$ / $207C5$ I'm not sure if these are correct.,The human sex ratio at birth is commonly thought to be 107 boys to 100 girls. Suppose five infants are chosen at random. (A) What is the probability that three are males and two are female? (B) What is the probability that at least one of them is a male? My work: (A) $(5C2 * 5C3)$ / $207C5$ (B) $(5C1 * 202C)4$ / $207C5$ I'm not sure if these are correct.,,"['probability', 'statistics', 'discrete-mathematics']"
58,Expected squared prediction error conditioned on training set,Expected squared prediction error conditioned on training set,,"I'm reading Elements of Statistical Learning by Hastie and Tibshirani, and I am thoroughly confused by the way they conditioned expected squared prediction error in section 2.5 (p.26): \begin{align*} EPE(x_0) &= E_{y_0|x_0} E_{\mathcal{T}} (y_0 - \hat{y}_0)^2 \end{align*} I think $\mathcal{T}$ refers to the training set, and $(x_0, y_0)$ is the testing set. What is the joint distribution that $EPE(x_0)$ is evaluated with?  I can't make sense of what the distribution $f(y_0|x_0)*\pi(\mathcal{T})$ even means.  I've seen many questions asked about their earlier definition of the $EPE$ (p.18): \begin{align*} EPE(f) &= E_X E_{Y|X} ([Y - f(X)]^2|X) \end{align*} Here, the conditioning makes sense.  I can see that the $EPE$ is with respect to the joint distribution of $X$ and $Y$, where $X$ is the input vector and $Y$ is the output vector.  Could someone please explain why the $EPE(x_0)$ written on top makes sense?","I'm reading Elements of Statistical Learning by Hastie and Tibshirani, and I am thoroughly confused by the way they conditioned expected squared prediction error in section 2.5 (p.26): \begin{align*} EPE(x_0) &= E_{y_0|x_0} E_{\mathcal{T}} (y_0 - \hat{y}_0)^2 \end{align*} I think $\mathcal{T}$ refers to the training set, and $(x_0, y_0)$ is the testing set. What is the joint distribution that $EPE(x_0)$ is evaluated with?  I can't make sense of what the distribution $f(y_0|x_0)*\pi(\mathcal{T})$ even means.  I've seen many questions asked about their earlier definition of the $EPE$ (p.18): \begin{align*} EPE(f) &= E_X E_{Y|X} ([Y - f(X)]^2|X) \end{align*} Here, the conditioning makes sense.  I can see that the $EPE$ is with respect to the joint distribution of $X$ and $Y$, where $X$ is the input vector and $Y$ is the output vector.  Could someone please explain why the $EPE(x_0)$ written on top makes sense?",,"['probability', 'statistics', 'machine-learning']"
59,Probability - Can't understand hint professor gave,Probability - Can't understand hint professor gave,,"You don't really need any expert knowledge in Machine Learning or linear regression for this question, just probability. Our model is this: We have our input matrix $X \in \mathbb R^{n \times d}$ and our output vector $y \in \mathbb R^n$ which is binary. $y_i$ is either $0$ or $1$. We have $P(y_i = 1|x_i) = 1-P(y_i = 0|x_i) = g(w^Tx_i)$ where $g(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function, $x_i$ denotes the $i$'th row of the matrix $X$, and $w \in \mathbb R^d$ maximizes our likelihood function defined $l(w) = \frac{1}{n} \sum_{i=1}^{n}\log(P(y_i|x_i))$ I was asked to prove that $\frac{\partial l}{\partial w} = \frac{1}{n} \sum_{i=1}^{n}x_i(y_i - g(w^Tx_i))$ We had a hint: Observe that $P(y_i|x_i) = g(w^Tx_i)^{y_i}(1-g(w^Tx_i))^{1-y_i}$ I don't understand how can the hint be true. It was defined at the beginning that $P(y_i|x_i) = g(w^Tx_i)$. This seems different. Edit: Here is the specific text the teacher gave","You don't really need any expert knowledge in Machine Learning or linear regression for this question, just probability. Our model is this: We have our input matrix $X \in \mathbb R^{n \times d}$ and our output vector $y \in \mathbb R^n$ which is binary. $y_i$ is either $0$ or $1$. We have $P(y_i = 1|x_i) = 1-P(y_i = 0|x_i) = g(w^Tx_i)$ where $g(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function, $x_i$ denotes the $i$'th row of the matrix $X$, and $w \in \mathbb R^d$ maximizes our likelihood function defined $l(w) = \frac{1}{n} \sum_{i=1}^{n}\log(P(y_i|x_i))$ I was asked to prove that $\frac{\partial l}{\partial w} = \frac{1}{n} \sum_{i=1}^{n}x_i(y_i - g(w^Tx_i))$ We had a hint: Observe that $P(y_i|x_i) = g(w^Tx_i)^{y_i}(1-g(w^Tx_i))^{1-y_i}$ I don't understand how can the hint be true. It was defined at the beginning that $P(y_i|x_i) = g(w^Tx_i)$. This seems different. Edit: Here is the specific text the teacher gave",,"['probability', 'statistics']"
60,"In the context of ordered statistics, each of Y(1),Y(2),...,Y(n) a single observation or distributions that are I.I.D?","In the context of ordered statistics, each of Y(1),Y(2),...,Y(n) a single observation or distributions that are I.I.D?",,"In statistics one aspect of the I.I.D. concept that bothers is when I think about it in the context of ordered statistics. As most of you already know, $Y_1,Y_2,Y_3,...,Y_n$ are I.I.D. when the parameters are the same. Now, here are two things I'm confused about. In the context of ordered statistics, each of $Y_{(1)},Y_{(2)},...,Y_{(n)}$ a single observation or distributions that are I.I.D? If they are distributions, how in the world is it possible to order distributions from the least to greatest??","In statistics one aspect of the I.I.D. concept that bothers is when I think about it in the context of ordered statistics. As most of you already know, $Y_1,Y_2,Y_3,...,Y_n$ are I.I.D. when the parameters are the same. Now, here are two things I'm confused about. In the context of ordered statistics, each of $Y_{(1)},Y_{(2)},...,Y_{(n)}$ a single observation or distributions that are I.I.D? If they are distributions, how in the world is it possible to order distributions from the least to greatest??",,['statistics']
61,"If we have two non-zero-correlated random variables, then why do we say that ""correlation does not imply causation""?","If we have two non-zero-correlated random variables, then why do we say that ""correlation does not imply causation""?",,"If we have two non-zero correlated random variables then they are dependent. Why then do we have the saying ""Correlation does not imply Causation"". A change in one variable may not cause exactly the same change in another but there is at least some 'causal' link.","If we have two non-zero correlated random variables then they are dependent. Why then do we have the saying ""Correlation does not imply Causation"". A change in one variable may not cause exactly the same change in another but there is at least some 'causal' link.",,"['statistics', 'logic', 'causal-diagrams', 'causality']"
62,Different answers when I split a combination question into cases or solve in one step.,Different answers when I split a combination question into cases or solve in one step.,,"The students producing a school fashion show plan to have four pieces of music played. The music students have come up with 18 pieces: 6 for piano, 5 for recorder, and 7 for guitar. The students want to use at least 1 piece for the piano. In how many ways can the group choose the 4 pieces of music? Explain your reasoning. I am not actually sure of the answer to this question. My solution was $$6C1(12C3)+6C2(12C2)+6C3(12)+6C4=2565$$ Then I thought of another solution which I thought should also work. $$6C1(17C3)=4080$$ The first solution is split into cases of choosing 1,2,3,4 piano pieces. The second solution chooses 1 piano piece first and then selects three pieces of music from the rest. Which solution is right? And why do the two solutions not yield the same answer? Thank you.","The students producing a school fashion show plan to have four pieces of music played. The music students have come up with 18 pieces: 6 for piano, 5 for recorder, and 7 for guitar. The students want to use at least 1 piece for the piano. In how many ways can the group choose the 4 pieces of music? Explain your reasoning. I am not actually sure of the answer to this question. My solution was $$6C1(12C3)+6C2(12C2)+6C3(12)+6C4=2565$$ Then I thought of another solution which I thought should also work. $$6C1(17C3)=4080$$ The first solution is split into cases of choosing 1,2,3,4 piano pieces. The second solution chooses 1 piano piece first and then selects three pieces of music from the rest. Which solution is right? And why do the two solutions not yield the same answer? Thank you.",,"['probability', 'combinatorics', 'statistics', 'combinations']"
63,Double integration over two random variables order,Double integration over two random variables order,,"Suppose that $f(x,y) = 1$ with $0 \leq x \leq 1$ and $0 \leq y \leq 1$ for simplicity. You want to find $P(X \geq 3Y)$ where $X$ and $Y$ are some random variable. I don't understand why changing the order of integrals gives me a different answer (I am likely missing something fundamental). I proceed the way that supposedly is the right answer: $$\int_{0}^{1} \int_{0}^{x/3} 1\,dydx = \int_{0}^{1} \frac{x}{3} = \frac{x^2}{6} \biggr\rvert_{0}^{1} = \frac{1}{6}$$ Then I try and see what happens if I do it the other way where instead of isolating the value to the right of the inequality, I just go ahead with the integration and solve for $P(X \geq 3Y)$ as follows: $$\int_{0}^{1} \int_{0}^{3y} 1\, dxdy = \int_{0}^{1} 3y\,dy = \frac{3y^2}{2} \biggr\rvert_{0}^{1} = \frac{3}{2}$$ I suspect I'm doing some invalid order changing and don't understand why. It has been a long time since I've had to do integrals.","Suppose that $f(x,y) = 1$ with $0 \leq x \leq 1$ and $0 \leq y \leq 1$ for simplicity. You want to find $P(X \geq 3Y)$ where $X$ and $Y$ are some random variable. I don't understand why changing the order of integrals gives me a different answer (I am likely missing something fundamental). I proceed the way that supposedly is the right answer: $$\int_{0}^{1} \int_{0}^{x/3} 1\,dydx = \int_{0}^{1} \frac{x}{3} = \frac{x^2}{6} \biggr\rvert_{0}^{1} = \frac{1}{6}$$ Then I try and see what happens if I do it the other way where instead of isolating the value to the right of the inequality, I just go ahead with the integration and solve for $P(X \geq 3Y)$ as follows: $$\int_{0}^{1} \int_{0}^{3y} 1\, dxdy = \int_{0}^{1} 3y\,dy = \frac{3y^2}{2} \biggr\rvert_{0}^{1} = \frac{3}{2}$$ I suspect I'm doing some invalid order changing and don't understand why. It has been a long time since I've had to do integrals.",,"['integration', 'statistics']"
64,A Chi-Squared fit of a general quadratic polynomial is done to ten data points. What is the number of degrees of freedom of this fit?,A Chi-Squared fit of a general quadratic polynomial is done to ten data points. What is the number of degrees of freedom of this fit?,,"I think the correct answer is $7$ because the general quadratic is $$y_i=ax_i^2 + bx_i + c$$ Using the formula $$\color{red}{\fbox{Number of degrees of freedom = Number of data points - Number of Parameters}}$$ The $3$ parameters are $y_i,x_i^2,x_i$ So $10-3=7$ degrees of freedom. The correct answer is indeed $7$; But is this because $x_i^2$ 'counts' as $2$ parameters as it's squared (adding the other $x_i$ to make $3$)? I am somewhat new to this idea of degrees of freedom; so I'm not really sure which method is valid, if any?","I think the correct answer is $7$ because the general quadratic is $$y_i=ax_i^2 + bx_i + c$$ Using the formula $$\color{red}{\fbox{Number of degrees of freedom = Number of data points - Number of Parameters}}$$ The $3$ parameters are $y_i,x_i^2,x_i$ So $10-3=7$ degrees of freedom. The correct answer is indeed $7$; But is this because $x_i^2$ 'counts' as $2$ parameters as it's squared (adding the other $x_i$ to make $3$)? I am somewhat new to this idea of degrees of freedom; so I'm not really sure which method is valid, if any?",,"['statistics', 'proof-verification']"
65,Sum of N (N ~Geo) exponentially distributed random variables is exponentially distributed,Sum of N (N ~Geo) exponentially distributed random variables is exponentially distributed,,"Let $T_i$ for $i=1,2,...$ be a sequence of i.i.d exponential random variables with common parameter $\lambda$. Let $N$ be a geometric random variable with parameter $(1/(p+1))$ that is independent of the sequence $T_i$. Let $X$ be the sum of the $T_i$ from 1 to $N$ Show that the distribution of X is exponential. I would like to use MGFs. I'm not sure how to incorporate the MGF of N in this case.","Let $T_i$ for $i=1,2,...$ be a sequence of i.i.d exponential random variables with common parameter $\lambda$. Let $N$ be a geometric random variable with parameter $(1/(p+1))$ that is independent of the sequence $T_i$. Let $X$ be the sum of the $T_i$ from 1 to $N$ Show that the distribution of X is exponential. I would like to use MGFs. I'm not sure how to incorporate the MGF of N in this case.",,"['probability', 'statistics', 'random-variables']"
66,"Toss a fair coin untill both a head and tail have appeared at least once, what is the probability that three tosses will be required?","Toss a fair coin untill both a head and tail have appeared at least once, what is the probability that three tosses will be required?",,"Sample space is (HT, TH, HHT, TTH, HHHT, TTTH,.....) I thought that since the outcome of each toss is independent of the previous toss the probabillity that it takes exactly 3 tosses (HHT, TTH) is simply equal to 0.5 * 0.5 * 0.5 + 0.5 * 0.5 * 0.5 = 1/4","Sample space is (HT, TH, HHT, TTH, HHHT, TTTH,.....) I thought that since the outcome of each toss is independent of the previous toss the probabillity that it takes exactly 3 tosses (HHT, TTH) is simply equal to 0.5 * 0.5 * 0.5 + 0.5 * 0.5 * 0.5 = 1/4",,['statistics']
67,Probability: bakery distributes pies,Probability: bakery distributes pies,,"I'm working through a mathematical statistics textbook, and I can't get a question right. It is a follow-up to this question: At the end of the day, a bakery gives everything that is unsold to food banks for the needy. If it has 12 apple pies left at the end of a given day, in how many different ways can it distribute these pies among six food banks for the needy? I think maybe I got this one right (please tell me if it's wrong). Here's what I did: Every pie can ""choose"" where to go out of 6 bakeries, with no restrictions, so there are 12 steps each with 6 options, so the total is $6^{12}$. Then this is the question I can't answer : With reference to the previous exercise, in how many different ways can the bakery distribute the 12 apple pies if each of the six food banks is to receive at least one pie? I thought that I could fix 6 of the pies (so one goes to each bakery) and then distribute the other 6 freely, so the total should be $6^6$ (because there are 6 steps, each with 6 options), but this is wrong. The book has 462 as the answer and I can't figure out why. Help?","I'm working through a mathematical statistics textbook, and I can't get a question right. It is a follow-up to this question: At the end of the day, a bakery gives everything that is unsold to food banks for the needy. If it has 12 apple pies left at the end of a given day, in how many different ways can it distribute these pies among six food banks for the needy? I think maybe I got this one right (please tell me if it's wrong). Here's what I did: Every pie can ""choose"" where to go out of 6 bakeries, with no restrictions, so there are 12 steps each with 6 options, so the total is $6^{12}$. Then this is the question I can't answer : With reference to the previous exercise, in how many different ways can the bakery distribute the 12 apple pies if each of the six food banks is to receive at least one pie? I thought that I could fix 6 of the pies (so one goes to each bakery) and then distribute the other 6 freely, so the total should be $6^6$ (because there are 6 steps, each with 6 options), but this is wrong. The book has 462 as the answer and I can't figure out why. Help?",,"['probability', 'statistics']"
68,Basic function manipulation and simplification question for $f((x-f(x))^2)$,Basic function manipulation and simplification question for,f((x-f(x))^2),"I've run into a bit of a wall trying to understand why the following two equations are equivalent: $$f((x-f(x))^2) = f(x^2)-f(x)^2$$ I'm running into this with calculating population variance in statistics, which is an area I understand, but I'm just not seeing the math here. I know at least for this application these two equations are equivalent, but am not familiar enough with function manipulation to see the logic behind it. The farthest I get is to go ahead and square the first function to get the following: $$f(x^2-2xf(x)+f(x)^2)$$ ...how do you continue to manipulate the terms and result in $f(x^2)-f(x)^2$?","I've run into a bit of a wall trying to understand why the following two equations are equivalent: $$f((x-f(x))^2) = f(x^2)-f(x)^2$$ I'm running into this with calculating population variance in statistics, which is an area I understand, but I'm just not seeing the math here. I know at least for this application these two equations are equivalent, but am not familiar enough with function manipulation to see the logic behind it. The farthest I get is to go ahead and square the first function to get the following: $$f(x^2-2xf(x)+f(x)^2)$$ ...how do you continue to manipulate the terms and result in $f(x^2)-f(x)^2$?",,"['algebra-precalculus', 'statistics']"
69,"There are 8 balls which appear identical. However, 1 is heavier than the rest. How do you find the ball with 2 weighings?","There are 8 balls which appear identical. However, 1 is heavier than the rest. How do you find the ball with 2 weighings?",,I understand there are similar problems but I am not sure how to go about constructing this problem with set of balls that are not exponents of 3^n. I know I need at least 2 weighings to find the heavier ball since 3^2 = 9. I was thinking make two groups from the 8 -> two groups of 4 which is in itself contains another subset = { (1)a (3)a } and { (1)b (3)b }. You weigh the two groups. If one is heavier you can only focus on that. Say group a is heavier. I know there is at least 1 weighings for a set of 3 balls to find the heavier one but what about the (1)a?,I understand there are similar problems but I am not sure how to go about constructing this problem with set of balls that are not exponents of 3^n. I know I need at least 2 weighings to find the heavier ball since 3^2 = 9. I was thinking make two groups from the 8 -> two groups of 4 which is in itself contains another subset = { (1)a (3)a } and { (1)b (3)b }. You weigh the two groups. If one is heavier you can only focus on that. Say group a is heavier. I know there is at least 1 weighings for a set of 3 balls to find the heavier one but what about the (1)a?,,"['probability', 'statistics', 'probability-theory', 'discrete-mathematics']"
70,"How do I evaluate $\mathbb E(X\log(X))$ if $X$ has a binomial distribution, for large $n$ values?","How do I evaluate  if  has a binomial distribution, for large  values?",\mathbb E(X\log(X)) X n,"$X\sim\mathcal {Bin}(n,p)$ I want to evaluate $\sum\limits_{x=0}^n {^n\mathrm C_x} p^x(1-p)^{n-x}x\log(x)$. Is there any way to avoid the sum because my $n$ can be very large (around $10^6$)?","$X\sim\mathcal {Bin}(n,p)$ I want to evaluate $\sum\limits_{x=0}^n {^n\mathrm C_x} p^x(1-p)^{n-x}x\log(x)$. Is there any way to avoid the sum because my $n$ can be very large (around $10^6$)?",,"['probability', 'statistics', 'summation', 'entropy', 'binomial-distribution']"
71,Calculating 95% confidence interval for mean for a normal population,Calculating 95% confidence interval for mean for a normal population,,"Consider a normal population with unknown $\mu$ and variance $\sigma^2=9$. To test $H_0:\mu=0$ against $H_1:\mu\neq 0$, a random sample of size 100 is taken. Based on this sample, the test of the form $|\bar{X_n}|>K$ rejects the null hypothesis at 5% level of significance. Then, which of the following is a possible 95% confidence interval for $\mu$ ? (A)$(-0.488,0.688)$ (B)$(-1.96,1.96)$ (C)$(0.422,1.598)$ (D)$(0.588,1.96)$ $\begin{aligned} 0.95 & = 1-\alpha=P(-z \le Z \le z)=P \left(-1.96 \le \frac {\bar X-\mu}{\sigma/\sqrt{n}} \le 1.96 \right) \\[6pt] & = P \left( \bar X - 1.96 \frac{\sigma}{\sqrt{n}} \le \mu \le \bar X + 1.96 \frac{\sigma}{\sqrt{n}}\right)\\[6pt] & = P \left( \bar X - 1.96 \frac{3}{10} \le \mu \le \bar X + 1.96 \frac{3}{10}\right) \\ & = P \left( \bar X - 0.588 \le \mu \le \bar X + 0.588\right) \end{aligned}$ Both (A) and (C) seem to work here with sample mean being 0.1 and 1.01 respectively. But if the sample mean were 1.01, we would be reject $H_0$ using hypothesis testing at 5% significance level ($z=3.366>1.65$). So, should (A) be the answer ?","Consider a normal population with unknown $\mu$ and variance $\sigma^2=9$. To test $H_0:\mu=0$ against $H_1:\mu\neq 0$, a random sample of size 100 is taken. Based on this sample, the test of the form $|\bar{X_n}|>K$ rejects the null hypothesis at 5% level of significance. Then, which of the following is a possible 95% confidence interval for $\mu$ ? (A)$(-0.488,0.688)$ (B)$(-1.96,1.96)$ (C)$(0.422,1.598)$ (D)$(0.588,1.96)$ $\begin{aligned} 0.95 & = 1-\alpha=P(-z \le Z \le z)=P \left(-1.96 \le \frac {\bar X-\mu}{\sigma/\sqrt{n}} \le 1.96 \right) \\[6pt] & = P \left( \bar X - 1.96 \frac{\sigma}{\sqrt{n}} \le \mu \le \bar X + 1.96 \frac{\sigma}{\sqrt{n}}\right)\\[6pt] & = P \left( \bar X - 1.96 \frac{3}{10} \le \mu \le \bar X + 1.96 \frac{3}{10}\right) \\ & = P \left( \bar X - 0.588 \le \mu \le \bar X + 0.588\right) \end{aligned}$ Both (A) and (C) seem to work here with sample mean being 0.1 and 1.01 respectively. But if the sample mean were 1.01, we would be reject $H_0$ using hypothesis testing at 5% significance level ($z=3.366>1.65$). So, should (A) be the answer ?",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
72,Random variable - what does mean the condition in definition,Random variable - what does mean the condition in definition,,"The definition of random variable in my book is: This is a function $X: \Omega \rightarrow R$, such that $ \{ \omega : X(\omega) < x\} \in S $ where $S$ is sigma-algebra, $\Omega$ is sample space and $\omega$ is single event. What does mean the condition $ \{ \omega : X(\omega) < x\} \in S $ ?","The definition of random variable in my book is: This is a function $X: \Omega \rightarrow R$, such that $ \{ \omega : X(\omega) < x\} \in S $ where $S$ is sigma-algebra, $\Omega$ is sample space and $\omega$ is single event. What does mean the condition $ \{ \omega : X(\omega) < x\} \in S $ ?",,"['probability', 'statistics']"
73,Why erf(a-b)+erf(a)+erf(a+b) is so close to 3erf(a)?,Why erf(a-b)+erf(a)+erf(a+b) is so close to 3erf(a)?,,"I am approximating an empirical distribution function with a sum of three gaussians, and noticed that for erf function $erf(a-b)+erf(a)+erf(a+b)$ is numerically very close to $3erf(a)$ for applicable values of $a$ and $b$, $a>>b$. Close, but not quite the same, of course. Could you suggest an analytical explanation why these functions are so close?","I am approximating an empirical distribution function with a sum of three gaussians, and noticed that for erf function $erf(a-b)+erf(a)+erf(a+b)$ is numerically very close to $3erf(a)$ for applicable values of $a$ and $b$, $a>>b$. Close, but not quite the same, of course. Could you suggest an analytical explanation why these functions are so close?",,"['statistics', 'normal-distribution', 'approximation']"
74,"Use the maximum likelihood to estimate the parameter $\theta$ in the uniform pdf $f_Y(y;\theta) = \frac{1}{\theta}$ , $0 \leq y \leq \theta$","Use the maximum likelihood to estimate the parameter  in the uniform pdf  ,",\theta f_Y(y;\theta) = \frac{1}{\theta} 0 \leq y \leq \theta,"(a) Based on the random sample $Y_1 = 6.3$ , $Y_2 = 1.8$, $Y_3 = 14.2$, and $Y_4 = 7.6$, use the method of maximum likelihood to estimate the parameter $\theta$ in the uniform pdf $f_Y(y;\theta) = \frac{1}{\theta}$ ,   $0 \leq y \leq \theta$ (b) Suppose the random sample in part (a) represents the two-parameter uniform pdf $f_Y(y;\theta_1, \theta_2) = \frac{1}{\theta_2 - \theta_1}$ ,   $\theta_1 \leq y \leq \theta_2$ Find the maximum likelihood estimates for $\theta_1$ and $\theta_2$. My attempt (a) The likelihood function is given by  $L(\theta)= \Pi_{i = 0}^{n}\frac{1}{\theta} = \theta^{-n} $ Take natural log both sides, so ln$L(\theta) = - n$ ln($\theta)$ Take derivative and set it equal to zero. Thus, $\frac{d}{d\theta}$ln$L(\theta)$ = -$\frac{n}{\theta}$ if we try to solve for theta equal to zero it wont work. So we have to reduce the the  value for $y$ as much as possible. So choose the value of $\theta_e = y_{min} = 1.8$ (b) since the range is dependent on $ \theta$, finding the derivative equal to zero won't work. So the maximun likelihood for $\theta_1e = 14.2$ and the maximun likelihood for $\theta_2e = 1.8$ Can anyone please verify this? If this does not work, can someone please help? Any feedback/hint would help. Thank you in advance.","(a) Based on the random sample $Y_1 = 6.3$ , $Y_2 = 1.8$, $Y_3 = 14.2$, and $Y_4 = 7.6$, use the method of maximum likelihood to estimate the parameter $\theta$ in the uniform pdf $f_Y(y;\theta) = \frac{1}{\theta}$ ,   $0 \leq y \leq \theta$ (b) Suppose the random sample in part (a) represents the two-parameter uniform pdf $f_Y(y;\theta_1, \theta_2) = \frac{1}{\theta_2 - \theta_1}$ ,   $\theta_1 \leq y \leq \theta_2$ Find the maximum likelihood estimates for $\theta_1$ and $\theta_2$. My attempt (a) The likelihood function is given by  $L(\theta)= \Pi_{i = 0}^{n}\frac{1}{\theta} = \theta^{-n} $ Take natural log both sides, so ln$L(\theta) = - n$ ln($\theta)$ Take derivative and set it equal to zero. Thus, $\frac{d}{d\theta}$ln$L(\theta)$ = -$\frac{n}{\theta}$ if we try to solve for theta equal to zero it wont work. So we have to reduce the the  value for $y$ as much as possible. So choose the value of $\theta_e = y_{min} = 1.8$ (b) since the range is dependent on $ \theta$, finding the derivative equal to zero won't work. So the maximun likelihood for $\theta_1e = 14.2$ and the maximun likelihood for $\theta_2e = 1.8$ Can anyone please verify this? If this does not work, can someone please help? Any feedback/hint would help. Thank you in advance.",,"['probability', 'statistics', 'parameter-estimation']"
75,Strange behaviors of finitely additive probabilities,Strange behaviors of finitely additive probabilities,,"Watching a lecture on youtube I heard the lecturer stating that in general finitely additive probabilities behaves strangely . For example, it is possible that every open interval around a point $x$ has probability $1/2$, while $p(\{x\})=0$. This lead me to various questions: Is there somebody who can explain how do we get to this specific result? What are other examples of strange behavior by finitely additive probabilities? Why – in general – countably additive probabilities let us avoid these kind of strange situations? As always, any feedback will be most welcome. Thank you for your help and your time. [Just to underline a point, being self-taught, I do rely occasionally on youtube videos, but I am very selective about the provenience of the videos. In this case, the lecturer was a top theorist from a top university]","Watching a lecture on youtube I heard the lecturer stating that in general finitely additive probabilities behaves strangely . For example, it is possible that every open interval around a point $x$ has probability $1/2$, while $p(\{x\})=0$. This lead me to various questions: Is there somebody who can explain how do we get to this specific result? What are other examples of strange behavior by finitely additive probabilities? Why – in general – countably additive probabilities let us avoid these kind of strange situations? As always, any feedback will be most welcome. Thank you for your help and your time. [Just to underline a point, being self-taught, I do rely occasionally on youtube videos, but I am very selective about the provenience of the videos. In this case, the lecturer was a top theorist from a top university]",,"['measure-theory', 'statistics', 'probability-theory', 'self-learning']"
76,"Permutation algorithm to simulate $X$, $Y$, $Z$ uniform on $(0,1)$ with $X+Y+Z = 1$","Permutation algorithm to simulate , ,  uniform on  with","X Y Z (0,1) X+Y+Z = 1","Permutation algorithm to simulate $X$, $Y$, $Z$ uniform on $(0,1)$ with $X+Y+Z = 1$ Edit : Sorry, I tend to jump back and forth between math notation and computer science notation....often to the chagrin of my more rigorous colleagues (and Math.SE folks ;-) Also, I accidentally copied a formula, so the problem was over specified. The last variable is determined by the previous two. And yes $\pi_i$ is the send element of the random permutation group $\pi$. I hope this clears things up. Is it possible to satisfy the title condition if I use the following algorithm? Let $V:=(X,Y,Z)$ be a vector of unspecified variables. Generate a random permutation of $\{1,2,3\}$, call this $\pi$ Let $V_{\pi_1}\sim U(0,1),V_{\pi_2}\sim U(0,1-V_{\pi_1}),V_{\pi_3}=1-V_{\pi_1}-V_{\pi_3}$ Go back to step 1 I'm hoping that the pdf of $V$ will have standard uniform marginal. It seems like the permutation operation ensures that the components of $V$ have standard uniform marginal over many iterations, since I am uniformly sampling over the permutations of the components. Is there a subtle flaw to this? I can't seem to find one.","Permutation algorithm to simulate $X$, $Y$, $Z$ uniform on $(0,1)$ with $X+Y+Z = 1$ Edit : Sorry, I tend to jump back and forth between math notation and computer science notation....often to the chagrin of my more rigorous colleagues (and Math.SE folks ;-) Also, I accidentally copied a formula, so the problem was over specified. The last variable is determined by the previous two. And yes $\pi_i$ is the send element of the random permutation group $\pi$. I hope this clears things up. Is it possible to satisfy the title condition if I use the following algorithm? Let $V:=(X,Y,Z)$ be a vector of unspecified variables. Generate a random permutation of $\{1,2,3\}$, call this $\pi$ Let $V_{\pi_1}\sim U(0,1),V_{\pi_2}\sim U(0,1-V_{\pi_1}),V_{\pi_3}=1-V_{\pi_1}-V_{\pi_3}$ Go back to step 1 I'm hoping that the pdf of $V$ will have standard uniform marginal. It seems like the permutation operation ensures that the components of $V$ have standard uniform marginal over many iterations, since I am uniformly sampling over the permutations of the components. Is there a subtle flaw to this? I can't seem to find one.",,['probability']
77,How likely are extreme observations in a probability distribution?,How likely are extreme observations in a probability distribution?,,"Given a measurement that follows a probability distribution (for the sake of argument, Gaussian) how likely is it that repeated observations on the distribution are an extreme of low or high? I realise that the first two will be by definition, but how quickly does the probability reduce by the 10th, 20th, 50th etc observation? How quickly does the cumulative probability go up when taking multiple observations on different distributions? Background I am having a hard time forming my question (edits appreciated), so I will give some background. I was having a conversation with a friend about the weather when he remarked that it had been the hottest September in 20 years(1). I said that he shouldn't be surprised, and in fact given the number of different weather measurements (hottest, coldest, wettest, driest, most sunny, least sunny, etc) and given the number of times the observations are made (weekly, monthly, annually) then I thought it quite normal, and indeed likely, to get some sort of extreme observation. I realise there is no exact answer to this question; I am not asking what is the probability is of it being the hottest September in 20 years. (1) After some research it is apparent that weather recordings don't follow Gaussian distributions, but its still an interesting topic of thought.","Given a measurement that follows a probability distribution (for the sake of argument, Gaussian) how likely is it that repeated observations on the distribution are an extreme of low or high? I realise that the first two will be by definition, but how quickly does the probability reduce by the 10th, 20th, 50th etc observation? How quickly does the cumulative probability go up when taking multiple observations on different distributions? Background I am having a hard time forming my question (edits appreciated), so I will give some background. I was having a conversation with a friend about the weather when he remarked that it had been the hottest September in 20 years(1). I said that he shouldn't be surprised, and in fact given the number of different weather measurements (hottest, coldest, wettest, driest, most sunny, least sunny, etc) and given the number of times the observations are made (weekly, monthly, annually) then I thought it quite normal, and indeed likely, to get some sort of extreme observation. I realise there is no exact answer to this question; I am not asking what is the probability is of it being the hottest September in 20 years. (1) After some research it is apparent that weather recordings don't follow Gaussian distributions, but its still an interesting topic of thought.",,"['probability', 'statistics', 'probability-distributions', 'soft-question']"
78,Fuzzy statistics bibliography.,Fuzzy statistics bibliography.,,"Could someone recommend me fuzzy statistical literature? I have basic knowledge of probability theory and statistics, but this topic is totally new to me. Prefer books I can download, but any recommendations are welcome.","Could someone recommend me fuzzy statistical literature? I have basic knowledge of probability theory and statistics, but this topic is totally new to me. Prefer books I can download, but any recommendations are welcome.",,"['statistics', 'reference-request']"
79,"How to show for a simple regression with an intercept and one independent variable $R^2 = r ^2$ , where $r$ is the ordinary correlation coefficient.","How to show for a simple regression with an intercept and one independent variable  , where  is the ordinary correlation coefficient.",R^2 = r ^2 r,"How to show for a simple regression with an intercept and one independent variable $R^2 = r ^2$, where $r$ is the ordinary correlation coefficient. Here is where I'm at. $R^2= \textrm{SSR}/\textrm{SST}$  then I substituted for $\hat{Y}$.  Now my question is what is the difference and meaning between $\hat{Y}$, $\bar{Y}$, and $Y$ in the substitution of SSR and SST?","How to show for a simple regression with an intercept and one independent variable $R^2 = r ^2$, where $r$ is the ordinary correlation coefficient. Here is where I'm at. $R^2= \textrm{SSR}/\textrm{SST}$  then I substituted for $\hat{Y}$.  Now my question is what is the difference and meaning between $\hat{Y}$, $\bar{Y}$, and $Y$ in the substitution of SSR and SST?",,"['statistics', 'regression']"
80,Geometric vs Arithmetic returns differences,Geometric vs Arithmetic returns differences,,"Been reading some notes that say when calculating returns, using the geometric methodology (1+returns, performing a division between the two returns and - 1 from the result) for computing returns is proven to be mathematically sound over a period of time as compared to arithmetic return computation. Can someone explain why is that so? In addition, for a single day, the arithemtic return should be the same as the geometric return? Many thanks for clearing these doubts. //Added the following clarifications I am currently reading the following paper by morningstar ( http://corporate.morningstar.com/us/documents/MethodologyDocuments/MethodologyPapers/EquityPerformanceAttributionMeth.pdf?bcsi_scan_aef1ec8c787364dd=JzQUZczAUPY5RaKpjawEbIdLMXNPAAAAw/PsCg==&bcsi_scan_filename=EquityPerformanceAttributionMeth.pdf ) If you refer to page 11/54, there is a section that talks about the difference between using arithmetic versus geometric methdology. Based on understanding, does it means that geometric return takes into account of the effect of compounding (due to taking into account the current accumulated capital ""1+""r) that makes it mathematically sound over a period of time?","Been reading some notes that say when calculating returns, using the geometric methodology (1+returns, performing a division between the two returns and - 1 from the result) for computing returns is proven to be mathematically sound over a period of time as compared to arithmetic return computation. Can someone explain why is that so? In addition, for a single day, the arithemtic return should be the same as the geometric return? Many thanks for clearing these doubts. //Added the following clarifications I am currently reading the following paper by morningstar ( http://corporate.morningstar.com/us/documents/MethodologyDocuments/MethodologyPapers/EquityPerformanceAttributionMeth.pdf?bcsi_scan_aef1ec8c787364dd=JzQUZczAUPY5RaKpjawEbIdLMXNPAAAAw/PsCg==&bcsi_scan_filename=EquityPerformanceAttributionMeth.pdf ) If you refer to page 11/54, there is a section that talks about the difference between using arithmetic versus geometric methdology. Based on understanding, does it means that geometric return takes into account of the effect of compounding (due to taking into account the current accumulated capital ""1+""r) that makes it mathematically sound over a period of time?",,"['calculus', 'linear-algebra', 'sequences-and-series', 'statistics', 'time-series']"
81,PMF of X: Number of trials to draw a chip,PMF of X: Number of trials to draw a chip,,"Let a bowl contain 10 chips of the same size and shape. One and only one of these chips is red. Continue to draw chips from the bowl, one at a time and at random and without replacement, until the red chip is drawn. Find the pmf of X, the number of trials needed to draw the red chip. I thought about using the Bernoulli trial equation but I think that may be wrong to apply here. This problem is just confusing me and I'm not sure how to set a pmf like this us.","Let a bowl contain 10 chips of the same size and shape. One and only one of these chips is red. Continue to draw chips from the bowl, one at a time and at random and without replacement, until the red chip is drawn. Find the pmf of X, the number of trials needed to draw the red chip. I thought about using the Bernoulli trial equation but I think that may be wrong to apply here. This problem is just confusing me and I'm not sure how to set a pmf like this us.",,"['probability', 'statistics', 'random-variables']"
82,Integration by parts,Integration by parts,,"Integrate using integration by parts: $F(y) = (y+1)e^{-y}$ Find: Evaluate the $\int_{a=0}^{b=\infty}F(y)\;dy$ using integration by parts. Thus far, I've distributed the $e^y$ term and split this into two integrals. One of these integrals becomes trivially easy to solve. The Other integral, the integral of $ye^{-y}$, I solved using integration by parts. I think. However, it's possible I'm making some mistakes somewhere. My answer follows: $\left[-ye^{-y} - 2e^{-y}\right]_{a=0}^{b=\infty}$ My concern is that when evaluating with the infinity term I encounter an indeterminate form, do I not? Also, I'm curious if I can say that $e^{-y}$ where $y=\infty$ is defined at all. Wouldn't we only be able to talk about what happens in the limit? Am I thinking correctly about this problem or have I made some fundamental mistake? Thank you for any and all help!","Integrate using integration by parts: $F(y) = (y+1)e^{-y}$ Find: Evaluate the $\int_{a=0}^{b=\infty}F(y)\;dy$ using integration by parts. Thus far, I've distributed the $e^y$ term and split this into two integrals. One of these integrals becomes trivially easy to solve. The Other integral, the integral of $ye^{-y}$, I solved using integration by parts. I think. However, it's possible I'm making some mistakes somewhere. My answer follows: $\left[-ye^{-y} - 2e^{-y}\right]_{a=0}^{b=\infty}$ My concern is that when evaluating with the infinity term I encounter an indeterminate form, do I not? Also, I'm curious if I can say that $e^{-y}$ where $y=\infty$ is defined at all. Wouldn't we only be able to talk about what happens in the limit? Am I thinking correctly about this problem or have I made some fundamental mistake? Thank you for any and all help!",,"['calculus', 'probability', 'integration', 'statistics']"
83,"rationalwiki on ""Extraordinary claims require extraordinary evidence""","rationalwiki on ""Extraordinary claims require extraordinary evidence""",,"I don't have a strong background in probability/statistics and I'm trying to understand the example at http://rationalwiki.org/wiki/Extraordinary_claims_require_extraordinary_evidence#Probability_theory Is that the correct framework to explain the principle? Their $P(A)$ there seems arbitrary, and using other values and the Bayes' formula they employ one gets values for $P(A|B)$ greater than one. I've been trying to work out a more general example in which the experiment consists of tossing $N$ times and the event $B$ would be guessing right $n$ times, but again it seems to me that there must be something limiting $P(A)$ or else one can get values of $P(A|B)$ higher than one. But again, I don't really know how to make sense of the example in a completely rigorous way or even if that's the correct approach to illustrate the principle. Thanks!","I don't have a strong background in probability/statistics and I'm trying to understand the example at http://rationalwiki.org/wiki/Extraordinary_claims_require_extraordinary_evidence#Probability_theory Is that the correct framework to explain the principle? Their $P(A)$ there seems arbitrary, and using other values and the Bayes' formula they employ one gets values for $P(A|B)$ greater than one. I've been trying to work out a more general example in which the experiment consists of tossing $N$ times and the event $B$ would be guessing right $n$ times, but again it seems to me that there must be something limiting $P(A)$ or else one can get values of $P(A|B)$ higher than one. But again, I don't really know how to make sense of the example in a completely rigorous way or even if that's the correct approach to illustrate the principle. Thanks!",,"['probability', 'statistics', 'probability-theory', 'bayesian', 'bayes-theorem']"
84,How to compute these probabilities?,How to compute these probabilities?,,"A pair of dice is cast until either the sum of seven or eight appears. How to compute the probability of a seven before an eight? Now, if this pair of dice is cast until a seven appears twice or until each of a six and eight have appeared at least once. How to compute the probability of the six and eight occurring before two sevens?","A pair of dice is cast until either the sum of seven or eight appears. How to compute the probability of a seven before an eight? Now, if this pair of dice is cast until a seven appears twice or until each of a six and eight have appeared at least once. How to compute the probability of the six and eight occurring before two sevens?",,"['probability', 'statistics', 'probability-theory']"
85,Explain why $\big(\int_{-\infty}^{\infty}e^{-z^2/2}dz \big)^2 = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(z^2 + u^2)/2}dzdu$,Explain why,\big(\int_{-\infty}^{\infty}e^{-z^2/2}dz \big)^2 = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(z^2 + u^2)/2}dzdu,I came across the following when studying a proof related to the normal distribution: $$\left(\int_{-\infty}^{\infty}e^{-z^2/2}\ dz \right)^2 = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(z^2 + u^2)/2}\ dz\ du$$ Is this some kind of identity? It was used as a step in the proof but I would like to know how it was arrived at?,I came across the following when studying a proof related to the normal distribution: $$\left(\int_{-\infty}^{\infty}e^{-z^2/2}\ dz \right)^2 = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(z^2 + u^2)/2}\ dz\ du$$ Is this some kind of identity? It was used as a step in the proof but I would like to know how it was arrived at?,,"['integration', 'statistics', 'multivariable-calculus', 'improper-integrals', 'normal-distribution']"
86,"Find a complete sufficient statistics, or show that one does not exist","Find a complete sufficient statistics, or show that one does not exist",,"$$  f(x|\theta) = e^{x-\theta}\exp\left(-e^{x-\theta}\right),\;\;\; -\infty < x < \infty,\;\;\; -\infty < \theta < \infty.$$ Find  acompliete sufficient statistics, or show that one does not exists. What I have found was that this given $f$ is not an exponential family, and my solution says as followed; There is no complete sufficient statistics for that. In detail, the solution said the order statistics are minimal sufficient, and this is location family. Thus, the range $R = X_{(n)} - X_{(1)}$ is ancilliary, and expectation does not depend on $\theta$. So this sufficient statistics is not complete. I don't understand two points. First, the solution could be the proof for no-existency of complete statistics, because this is only the proof for the case of $R = X_{(n)} - X_{(1)}$, and second, The fact that the expectation does not depend on $\theta$ could imply that this sufficient statistics is not complete. Could anybody help me to understand this?","$$  f(x|\theta) = e^{x-\theta}\exp\left(-e^{x-\theta}\right),\;\;\; -\infty < x < \infty,\;\;\; -\infty < \theta < \infty.$$ Find  acompliete sufficient statistics, or show that one does not exists. What I have found was that this given $f$ is not an exponential family, and my solution says as followed; There is no complete sufficient statistics for that. In detail, the solution said the order statistics are minimal sufficient, and this is location family. Thus, the range $R = X_{(n)} - X_{(1)}$ is ancilliary, and expectation does not depend on $\theta$. So this sufficient statistics is not complete. I don't understand two points. First, the solution could be the proof for no-existency of complete statistics, because this is only the proof for the case of $R = X_{(n)} - X_{(1)}$, and second, The fact that the expectation does not depend on $\theta$ could imply that this sufficient statistics is not complete. Could anybody help me to understand this?",,['statistics']
87,Linear combination of independent random variables that are poisson distributed,Linear combination of independent random variables that are poisson distributed,,Suppose $X_1$ and $X_2$ are independent random variables $X_1$~ Poisson$(\lambda_1)$ and $X_2$~ Poisson$(\lambda_2)$ I want to show $X_1 + X_2$~poisson$(\lambda_1 + \lambda_2)$ I want to then generalize this to a sum of $n$ independent Poisson random variables,Suppose $X_1$ and $X_2$ are independent random variables $X_1$~ Poisson$(\lambda_1)$ and $X_2$~ Poisson$(\lambda_2)$ I want to show $X_1 + X_2$~poisson$(\lambda_1 + \lambda_2)$ I want to then generalize this to a sum of $n$ independent Poisson random variables,,"['linear-algebra', 'probability', 'statistics']"
88,what is the probability of a couple who has four girls and is trying again for a boy. what is the probability that the next kid will be a girl?,what is the probability of a couple who has four girls and is trying again for a boy. what is the probability that the next kid will be a girl?,,"A couple has four kids already, all girls. the couple would like to have a son and would like to give it another try. what is the probability that the next kid will be a girl?","A couple has four kids already, all girls. the couple would like to have a son and would like to give it another try. what is the probability that the next kid will be a girl?",,"['probability', 'statistics']"
89,Central Limit Theorem - Distribution Function Converges to Standard Normal,Central Limit Theorem - Distribution Function Converges to Standard Normal,,"Suppose that $X_1, X_2, ..., X_n$ and $Y_1, Y_2, ..., Y_n$ are independent random samples from populations with means $\mu_1$ and $\mu_2$ and variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Show that the random variable $U_n = \frac{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{\sqrt{\frac{(\sigma_1^2+\sigma_2^2)}{n}}                   }$ satisfies the following conditions of the theorem below and thus that the distribution function of $U_n$ converges to a standard normal distribution function as $n -> \infty$. Theorem: I'm not exactly sure how to exactly approach this but I was told to consider $W_i = X_i - Y_i$, for $i = 1, 2, ..., n$.]","Suppose that $X_1, X_2, ..., X_n$ and $Y_1, Y_2, ..., Y_n$ are independent random samples from populations with means $\mu_1$ and $\mu_2$ and variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Show that the random variable $U_n = \frac{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{\sqrt{\frac{(\sigma_1^2+\sigma_2^2)}{n}}                   }$ satisfies the following conditions of the theorem below and thus that the distribution function of $U_n$ converges to a standard normal distribution function as $n -> \infty$. Theorem: I'm not exactly sure how to exactly approach this but I was told to consider $W_i = X_i - Y_i$, for $i = 1, 2, ..., n$.]",,"['probability', 'statistics', 'central-limit-theorem']"
90,MLE of uniform distribution,MLE of uniform distribution,,"Now before I begin, I know this question has been asked multiple times but all the answers but I had so many questions of my own that I figured I should make a new question as my thoughts are different than previous answers. Now I will ask the question first then explain my thoughts and troubles :) We have a uniform distribution that has the following PDF: $\frac{1}{b-a}$. So far so good. So if $n$ observations our Maximum Likelihood Function is: $\mathcal{L}(a,b)=\frac{1}{(b-a)^n}$ if each of these observations are independent and identically (i.i.d.) distributed. Now, after taking the log of the likelihood and taking the derivative once with respect to $b$ and once with respect to $a$ we have the following: The derivative with respect to $a$ is: $$\frac{n}{b-a}$$ and the derivative with respect to $b$ is: $$\frac{-n}{b-a}$$ Now if we try to set either of these derivatives to zero and try to maximize the function, it will not yield anything useful. My problem arises here. Lets us just focus on maximizing $b$. I have read on this website as well as other places that to maximize $\frac{-n}{b-a}$ we have to take the maximum observation? How does that make sense, I mean should we not use the lowest observation to maximize this function? Because in this particular case, $n$ and $a$ are constants so we can easily just plug some numbers in and see that as $b$ gets bigger, the function get smaller, so why would we want the maximum observation? Help would be greatly appreciated and please I am not mathematically or statistically inclined so please be gentle! Thanks :)","Now before I begin, I know this question has been asked multiple times but all the answers but I had so many questions of my own that I figured I should make a new question as my thoughts are different than previous answers. Now I will ask the question first then explain my thoughts and troubles :) We have a uniform distribution that has the following PDF: $\frac{1}{b-a}$. So far so good. So if $n$ observations our Maximum Likelihood Function is: $\mathcal{L}(a,b)=\frac{1}{(b-a)^n}$ if each of these observations are independent and identically (i.i.d.) distributed. Now, after taking the log of the likelihood and taking the derivative once with respect to $b$ and once with respect to $a$ we have the following: The derivative with respect to $a$ is: $$\frac{n}{b-a}$$ and the derivative with respect to $b$ is: $$\frac{-n}{b-a}$$ Now if we try to set either of these derivatives to zero and try to maximize the function, it will not yield anything useful. My problem arises here. Lets us just focus on maximizing $b$. I have read on this website as well as other places that to maximize $\frac{-n}{b-a}$ we have to take the maximum observation? How does that make sense, I mean should we not use the lowest observation to maximize this function? Because in this particular case, $n$ and $a$ are constants so we can easily just plug some numbers in and see that as $b$ gets bigger, the function get smaller, so why would we want the maximum observation? Help would be greatly appreciated and please I am not mathematically or statistically inclined so please be gentle! Thanks :)",,['statistics']
91,"Finding CDF, standard deviation and expected value of a random variable","Finding CDF, standard deviation and expected value of a random variable",,"Let the function  $$ f(x) = \begin{cases} ax^2 & \text{for } x\in [ 0, 1], \\0 & \text{for } x\notin [0,1].\end{cases} $$ Find $a$, such that the function can describe a probability density function. Calculate the expected value, standard deviation and CDF of a random variable X of such distribution. So thanks to the community, I now can solve the former part of such excercises, in this case by $\int_0^1 ax^2 \, dx = \left.\frac{ax^3}{3}\right|_0^1 = \frac a 3$ so that the function I'm looking for is $f(x)=3x^2$. Still, I'm struggling with finding the descriptive properties of this. Again, it's not specifically about this problem - I'm trying to learn to solve such class of problems so the whole former part may be different and we may end up with different function to work with. So as for the standard deviation, I believe I should find a mean value $m$ and then a definite integral (at least that's what the notes suggest?) so that I end up with $$\int_{- \infty}^\infty (x-m) \cdot 3x^2 \,\mathrm{d}x$$ As for the CDF and expected value, I'm clueless though. In the example I have in the notes, the function was $\frac{3}{2}x^2$ and for the expected value there is simply some $E(X)=n\cdot m = 0$ written while the CDF here is put as $D^2 X = n \cdot 0.6 = 6 \leftrightarrow D(X) = \sqrt{6}$ and I can't make head or tail from this. Could you please help?","Let the function  $$ f(x) = \begin{cases} ax^2 & \text{for } x\in [ 0, 1], \\0 & \text{for } x\notin [0,1].\end{cases} $$ Find $a$, such that the function can describe a probability density function. Calculate the expected value, standard deviation and CDF of a random variable X of such distribution. So thanks to the community, I now can solve the former part of such excercises, in this case by $\int_0^1 ax^2 \, dx = \left.\frac{ax^3}{3}\right|_0^1 = \frac a 3$ so that the function I'm looking for is $f(x)=3x^2$. Still, I'm struggling with finding the descriptive properties of this. Again, it's not specifically about this problem - I'm trying to learn to solve such class of problems so the whole former part may be different and we may end up with different function to work with. So as for the standard deviation, I believe I should find a mean value $m$ and then a definite integral (at least that's what the notes suggest?) so that I end up with $$\int_{- \infty}^\infty (x-m) \cdot 3x^2 \,\mathrm{d}x$$ As for the CDF and expected value, I'm clueless though. In the example I have in the notes, the function was $\frac{3}{2}x^2$ and for the expected value there is simply some $E(X)=n\cdot m = 0$ written while the CDF here is put as $D^2 X = n \cdot 0.6 = 6 \leftrightarrow D(X) = \sqrt{6}$ and I can't make head or tail from this. Could you please help?",,"['probability', 'statistics']"
92,Fitting curve to more complicated exponential,Fitting curve to more complicated exponential,,"I've got a conjecture about the relationship of a dataset. My intuition says it's a decaying exponential, but I want to know which a and b in $e^{-ax^{b}}$ best fits the data. What's the best way to go about it? Most standard stat program doesn't seem to have this capability. I don't think looking at the log of the data would work unless I give it a b to work with either, right?","I've got a conjecture about the relationship of a dataset. My intuition says it's a decaying exponential, but I want to know which a and b in $e^{-ax^{b}}$ best fits the data. What's the best way to go about it? Most standard stat program doesn't seem to have this capability. I don't think looking at the log of the data would work unless I give it a b to work with either, right?",,['statistics']
93,Easy GRE question: Statistics,Easy GRE question: Statistics,,I'm not sure how to set this statistics problem when they give me a group of arbitrary values. Can someone help? A group of 20 values has a mean of 85 and a median of 80. A different group of 30 values has a mean of 75 and a median of 72. a) What is the mean of the 50 values? b) What is the median of the 50 values? Thank you,I'm not sure how to set this statistics problem when they give me a group of arbitrary values. Can someone help? A group of 20 values has a mean of 85 and a median of 80. A different group of 30 values has a mean of 75 and a median of 72. a) What is the mean of the 50 values? b) What is the median of the 50 values? Thank you,,"['statistics', 'gre-exam']"
94,Least Square with homogeneous solution!,Least Square with homogeneous solution!,,"I've read somewhere that: $x=A^+b+(I-A^+A)Z$ is a solution for $Ax=b$ ,when is doesn't have a particular solution. where $A^+$ indicates the pseudo-inverse and $Z$ is an arbitrary vector!!! I know The first term represents the least square solution but the second term is said to be can used for optimizing secondary criteria and is called the ""homogeneous solution"". What is homogeneous solution? Where does it come from?","I've read somewhere that: $x=A^+b+(I-A^+A)Z$ is a solution for $Ax=b$ ,when is doesn't have a particular solution. where $A^+$ indicates the pseudo-inverse and $Z$ is an arbitrary vector!!! I know The first term represents the least square solution but the second term is said to be can used for optimizing secondary criteria and is called the ""homogeneous solution"". What is homogeneous solution? Where does it come from?",,"['linear-algebra', 'matrices', 'statistics', 'inverse', 'numerical-linear-algebra']"
95,Calculate the probability of two teams have been drawns,Calculate the probability of two teams have been drawns,,"If we know that team A had a $39\%$ chance of winning and team B $43\%$ chance of winning, how we can calculate the probability of the teams drawn? My textbook mention the answer but I cannot understand the logic behind it. The answer is $18\%$. As working is not shown I guess that this is how the find $18\%$ probability of two teams withdrawn: $$ (100\% - 39\%) - 43\% = 18\%$$ But I cannot understand the logic behind it. I appreciate if someone can explain it to me.","If we know that team A had a $39\%$ chance of winning and team B $43\%$ chance of winning, how we can calculate the probability of the teams drawn? My textbook mention the answer but I cannot understand the logic behind it. The answer is $18\%$. As working is not shown I guess that this is how the find $18\%$ probability of two teams withdrawn: $$ (100\% - 39\%) - 43\% = 18\%$$ But I cannot understand the logic behind it. I appreciate if someone can explain it to me.",,['statistics']
96,Calculating mean and standard deviation for a set,Calculating mean and standard deviation for a set,,"Suppose we have a set $A = \{a_1, a_2, a_3, a_4, a_5\}$ where $a_n \in \mathbb{R}$ and a set $B = \{b_1, b_2, b_3, b_4, b_5\}$ where $b_n \in \mathbb{R}$ and a set $C = \{ma_1 + nb_1, ma_2 + nb_2, ma_3 + nb_3, ma_4 + nb_4, ma_5 + nb_5\}$ where $m, n \in (0,1) \subset \mathbb{R}$. $A$ has mean $\mu_1$ and standard deviation $\sigma_1$. $B$ has mean $\mu_2$ and standard deviation $\sigma_2$. Do we have sufficient information to calculate the standard deviation of $C$? Note: The mean of $C$ is $m\mu_{1} + n\mu_{2}$?","Suppose we have a set $A = \{a_1, a_2, a_3, a_4, a_5\}$ where $a_n \in \mathbb{R}$ and a set $B = \{b_1, b_2, b_3, b_4, b_5\}$ where $b_n \in \mathbb{R}$ and a set $C = \{ma_1 + nb_1, ma_2 + nb_2, ma_3 + nb_3, ma_4 + nb_4, ma_5 + nb_5\}$ where $m, n \in (0,1) \subset \mathbb{R}$. $A$ has mean $\mu_1$ and standard deviation $\sigma_1$. $B$ has mean $\mu_2$ and standard deviation $\sigma_2$. Do we have sufficient information to calculate the standard deviation of $C$? Note: The mean of $C$ is $m\mu_{1} + n\mu_{2}$?",,['statistics']
97,How to find a line of best fit of the form $y=ax$?,How to find a line of best fit of the form ?,y=ax,"We have the following points: $$ (0,0)(1,51.8)(1.9,101.3)(2.8,148.4)(3.7,201.5)(4.7,251.1)(5.6,302.3)(6.6,350.9)(7.5,397.1)(8.5,452.5)(9.3,496.3)$$ How can we find the best fitting line $y=ax$ through the points? My calculator has the option to find the best fitting line $y=ax+b$ through these points, which is: $$y \approx 53,28x + 0.37$$ How can I find the best fiting $y=ax$? It seems to me we can't just remove the $0.37$ without compensating in the $a$?","We have the following points: $$ (0,0)(1,51.8)(1.9,101.3)(2.8,148.4)(3.7,201.5)(4.7,251.1)(5.6,302.3)(6.6,350.9)(7.5,397.1)(8.5,452.5)(9.3,496.3)$$ How can we find the best fitting line $y=ax$ through the points? My calculator has the option to find the best fitting line $y=ax+b$ through these points, which is: $$y \approx 53,28x + 0.37$$ How can I find the best fiting $y=ax$? It seems to me we can't just remove the $0.37$ without compensating in the $a$?",,"['statistics', 'regression']"
98,"Let $ X_1,X_2,…,X_n$ be i.i.d. $N(\theta_1, \theta_2)$, please prove that $E[(X_1-\theta_1)^4] = 3\theta_2^2$","Let  be i.i.d. , please prove that"," X_1,X_2,…,X_n N(\theta_1, \theta_2) E[(X_1-\theta_1)^4] = 3\theta_2^2","If $X_{1}$, $X_{2}$, ..., $X_{n}$ is sampled from $N(\theta_1, \theta_2)$, how can I prove that $E [(X_{1} - \theta_1)^{4}] = 3 \theta_2^{2}$? I started off this question finding the completely sufficient statistics. This can be done because the normal distribution of of exponential class. The results are $Y_1 = \sum\limits_iX_{i}$ and $Y_2 =\sum\limits_iX_{i}^{2}$ Therefore I know $E(\bar X) =\theta_1$, but how can I come up with the $\theta_2$? Also, because of how the question look, I think I would need to use the formula: $\operatorname{Var}(X) = E[X^{2}] - E(X)^{2}$ Can someone give me some ideas on how to proceed? Help is much appreciated!","If $X_{1}$, $X_{2}$, ..., $X_{n}$ is sampled from $N(\theta_1, \theta_2)$, how can I prove that $E [(X_{1} - \theta_1)^{4}] = 3 \theta_2^{2}$? I started off this question finding the completely sufficient statistics. This can be done because the normal distribution of of exponential class. The results are $Y_1 = \sum\limits_iX_{i}$ and $Y_2 =\sum\limits_iX_{i}^{2}$ Therefore I know $E(\bar X) =\theta_1$, but how can I come up with the $\theta_2$? Also, because of how the question look, I think I would need to use the formula: $\operatorname{Var}(X) = E[X^{2}] - E(X)^{2}$ Can someone give me some ideas on how to proceed? Help is much appreciated!",,"['statistics', 'normal-distribution', 'random-variables', 'estimation']"
99,Probability of Obtaining the Roots in a Quadratic Equation by Throwing a Die Three Times,Probability of Obtaining the Roots in a Quadratic Equation by Throwing a Die Three Times,,"Question : The coefficients a,b,c of the quadratic equation $ax^2+bx+c=0$ are   determined by throwing a die three times and reading off the value   shown on the uppermost face of each die. i.e, if you throw a 1, 5 and   3 respectively, the equation is $1x^2+5x+3=0$. Find the probabilities that the quadratic equation you obtain : has real roots; has complex roots; has equal roots. Thank you for your attention.","Question : The coefficients a,b,c of the quadratic equation $ax^2+bx+c=0$ are   determined by throwing a die three times and reading off the value   shown on the uppermost face of each die. i.e, if you throw a 1, 5 and   3 respectively, the equation is $1x^2+5x+3=0$. Find the probabilities that the quadratic equation you obtain : has real roots; has complex roots; has equal roots. Thank you for your attention.",,"['probability', 'statistics']"
