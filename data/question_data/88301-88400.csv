,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Reproducing kernel vs. Riesz kernel.,Reproducing kernel vs. Riesz kernel.,,"I'm attending an Analysis course and we are studying the Hardy space $H^2$ in the unit disk, from where the concept of RKHS (Reproducing Kernel Hilbert Space) came out. Acording to my notes: By the Riesz representation theorem for every evaluation functional $\phi_a:H^2\to\mathbb{C}$, $\phi_af=f(a)$, there is exactly one $k_a\in H^2$ such that   $$ \phi_af=\langle f,k_a\rangle_{H^2},\ \ f\in H^2. $$   This equation is called the reproducing property , for which $k_a$ is called a reproducing kernel of $H^2$. In fact we can calculate that $k_a(z)=1/(1-\bar az)$ since   $$ \phi_af=\sum_{n=0}^\infty \hat f(n)a^n=\sum_{n=0}^\infty \hat f(n)\overline{\bar a^n}=\langle f\,, \,\frac{1}{1-\bar az}\rangle_{H^2}. $$   The function $k_a(z)=1/(1-\bar a z)$ receives the name of Riesz kernel . I have no problem with the math, but with the terminology; why is $k_a$ called both reproducing kernel and Riesz kernel ? Is a reproducing kernel exactly the same as a Riesz kernel? If not, how are they different? Thank you in advance for your help!","I'm attending an Analysis course and we are studying the Hardy space $H^2$ in the unit disk, from where the concept of RKHS (Reproducing Kernel Hilbert Space) came out. Acording to my notes: By the Riesz representation theorem for every evaluation functional $\phi_a:H^2\to\mathbb{C}$, $\phi_af=f(a)$, there is exactly one $k_a\in H^2$ such that   $$ \phi_af=\langle f,k_a\rangle_{H^2},\ \ f\in H^2. $$   This equation is called the reproducing property , for which $k_a$ is called a reproducing kernel of $H^2$. In fact we can calculate that $k_a(z)=1/(1-\bar az)$ since   $$ \phi_af=\sum_{n=0}^\infty \hat f(n)a^n=\sum_{n=0}^\infty \hat f(n)\overline{\bar a^n}=\langle f\,, \,\frac{1}{1-\bar az}\rangle_{H^2}. $$   The function $k_a(z)=1/(1-\bar a z)$ receives the name of Riesz kernel . I have no problem with the math, but with the terminology; why is $k_a$ called both reproducing kernel and Riesz kernel ? Is a reproducing kernel exactly the same as a Riesz kernel? If not, how are they different? Thank you in advance for your help!",,"['functional-analysis', 'terminology']"
1,How do I prove this inequality involving $\liminf$ and $\limsup$?,How do I prove this inequality involving  and ?,\liminf \limsup,"I'm trying to solve the following question. Prove that there exists a bounded linear functional $F:l^{\infty}\rightarrow\mathbb R$ satisfying the following conditions: (i) $|F(x)|\leq \sup_{n} |x_n|$ (ii) $F(x)=\lim_{n}x_n$, if limit exists (iii) $\liminf x_n\leq F(x)\leq \limsup x_n$. I proved the first two parts by defining the functional $F$ for the space of convergent sequences and then extending it to the whole of the Banach space $l^{\infty}$ using the Hahn-Banach theorem. However, I can't prove part (iii). Does it follow simply by using the definitions of $\liminf$ and $\limsup$? This seems to be not too difficult, but I can't seem to get this part. I'm not sure this particular part needs any functional analysis and so the tag need not be justified. Thank you for any help.","I'm trying to solve the following question. Prove that there exists a bounded linear functional $F:l^{\infty}\rightarrow\mathbb R$ satisfying the following conditions: (i) $|F(x)|\leq \sup_{n} |x_n|$ (ii) $F(x)=\lim_{n}x_n$, if limit exists (iii) $\liminf x_n\leq F(x)\leq \limsup x_n$. I proved the first two parts by defining the functional $F$ for the space of convergent sequences and then extending it to the whole of the Banach space $l^{\infty}$ using the Hahn-Banach theorem. However, I can't prove part (iii). Does it follow simply by using the definitions of $\liminf$ and $\limsup$? This seems to be not too difficult, but I can't seem to get this part. I'm not sure this particular part needs any functional analysis and so the tag need not be justified. Thank you for any help.",,"['real-analysis', 'functional-analysis', 'limsup-and-liminf']"
2,"Haar system forms an orthonormal system in $L_2[0,1]$",Haar system forms an orthonormal system in,"L_2[0,1]","Haar wavelets are defined as: $$ \psi_{0,0}(t) =  \begin{cases} 1, \text{ for } 0<t< 1/2\\ -1, \text{ for } 1/2<t<1 \\ 0, \text{ otherwise } \end{cases} $$ And for $n \geq 0$, $0 \leq k < 2^n$ $$\psi_{n,k} = 2^{n/2} \psi_{0,0}(2^n t -k).$$ I was able to prove the orthonormality of those functions, and tried to approximate polynomials and sin/cos functions, since I know that those are a basis, but both methods failed. This result has been proven by B.S. Kashin, A.A. Saakyan, ""Orthogonal series"" , Moscow (1984), but Russian isn't my forte. Full derivations and general pointers are appreciated. I encountered these in the context of Brownian motion. Edit: Wolfram alpha provides this graph of first few functions","Haar wavelets are defined as: $$ \psi_{0,0}(t) =  \begin{cases} 1, \text{ for } 0<t< 1/2\\ -1, \text{ for } 1/2<t<1 \\ 0, \text{ otherwise } \end{cases} $$ And for $n \geq 0$, $0 \leq k < 2^n$ $$\psi_{n,k} = 2^{n/2} \psi_{0,0}(2^n t -k).$$ I was able to prove the orthonormality of those functions, and tried to approximate polynomials and sin/cos functions, since I know that those are a basis, but both methods failed. This result has been proven by B.S. Kashin, A.A. Saakyan, ""Orthogonal series"" , Moscow (1984), but Russian isn't my forte. Full derivations and general pointers are appreciated. I encountered these in the context of Brownian motion. Edit: Wolfram alpha provides this graph of first few functions",,['functional-analysis']
3,Bounded sequence of functions has subsequence convergent a.e.?,Bounded sequence of functions has subsequence convergent a.e.?,,"Does bounded sequence of functions have subsequence convergent a.e.? There is a equivalent question : Does weak convergent sequence of functions have subsequence convergent a.e.? These two are equivalent because of Banach-Steinhaus theorem . Before I ask here, I thought Bolzano-Weierstrass theorem which is every bounded sequence has convergent subsequence. And my question comes up from 'this theorem can be applied to sequence of functions?'. If the space of functions should be special (like reflexive or compact or housdorff), please mention about it. I didn't study functional analysis but attended courses : Partial differential equation and real analysis for graduate student.","Does bounded sequence of functions have subsequence convergent a.e.? There is a equivalent question : Does weak convergent sequence of functions have subsequence convergent a.e.? These two are equivalent because of Banach-Steinhaus theorem . Before I ask here, I thought Bolzano-Weierstrass theorem which is every bounded sequence has convergent subsequence. And my question comes up from 'this theorem can be applied to sequence of functions?'. If the space of functions should be special (like reflexive or compact or housdorff), please mention about it. I didn't study functional analysis but attended courses : Partial differential equation and real analysis for graduate student.",,"['real-analysis', 'functional-analysis', 'weak-convergence']"
4,Show that the fractional power of a linear operator is closed,Show that the fractional power of a linear operator is closed,,"Let $H$ be a $\mathbb R$-Hilbert space and $(\mathcal D(A),A)$ be a linear operator. Assume $(e_n)_{n\in\mathbb N}\subseteq\mathcal D(A)$ is an orthonormal basis of $H$ with $$Ae_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N\tag 1$$ for some $(\lambda_n)_{n\in\mathbb N}\subseteq(0,\infty)$ with $$\lambda_{n+1}\ge\lambda_n\;\;\;\text{for all }n\in\mathbb N\;.\tag 2$$ Let $\alpha\in\mathbb R$, $$\mathcal D(A^\alpha):=\left\{x\in H:\sum_{n\in\mathbb N}\lambda_n^{2\alpha}\left|\langle x,e_n\rangle_H\right|^2<\infty\right\}$$ and $$A^\alpha x:=\sum_{n\in\mathbb N}\lambda_n^\alpha\langle x,e_n\rangle_He_n\;\;\;\text{for }x\in\mathcal D(A^\alpha)\;.$$ Let $(x_n)_{n\in\mathbb N}\subseteq\mathcal D(A^\alpha)$ and $x,y\in H$ with $$\left\|x_n-x\right\|_H\xrightarrow{n\to\infty}0\tag 3$$ and $$\left\|A^\alpha x_n-y\right\|_H\xrightarrow{n\to\infty}0\;.\tag 4$$ I want to show that $x\in\mathcal D(A^\alpha)$ $y=A^\alpha x$ How can we do that?","Let $H$ be a $\mathbb R$-Hilbert space and $(\mathcal D(A),A)$ be a linear operator. Assume $(e_n)_{n\in\mathbb N}\subseteq\mathcal D(A)$ is an orthonormal basis of $H$ with $$Ae_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N\tag 1$$ for some $(\lambda_n)_{n\in\mathbb N}\subseteq(0,\infty)$ with $$\lambda_{n+1}\ge\lambda_n\;\;\;\text{for all }n\in\mathbb N\;.\tag 2$$ Let $\alpha\in\mathbb R$, $$\mathcal D(A^\alpha):=\left\{x\in H:\sum_{n\in\mathbb N}\lambda_n^{2\alpha}\left|\langle x,e_n\rangle_H\right|^2<\infty\right\}$$ and $$A^\alpha x:=\sum_{n\in\mathbb N}\lambda_n^\alpha\langle x,e_n\rangle_He_n\;\;\;\text{for }x\in\mathcal D(A^\alpha)\;.$$ Let $(x_n)_{n\in\mathbb N}\subseteq\mathcal D(A^\alpha)$ and $x,y\in H$ with $$\left\|x_n-x\right\|_H\xrightarrow{n\to\infty}0\tag 3$$ and $$\left\|A^\alpha x_n-y\right\|_H\xrightarrow{n\to\infty}0\;.\tag 4$$ I want to show that $x\in\mathcal D(A^\alpha)$ $y=A^\alpha x$ How can we do that?",,"['functional-analysis', 'operator-theory', 'semigroup-of-operators']"
5,Closed linear span in a Hilbert space defined as the sums of unconditionally convergent series,Closed linear span in a Hilbert space defined as the sums of unconditionally convergent series,,"Let $H$ be a Hilbert space over complex numbers and let $U \subset H$. Then, the closed linear span of $U$ is defined as $$T = \left\{ \sum_{u \in U} c_n u \mid  c_n \text{ complex } , \sum_{u \in U} c_n u  \text{ converges unconditionally} \right\}.$$ But, how do I see that this set is closed?","Let $H$ be a Hilbert space over complex numbers and let $U \subset H$. Then, the closed linear span of $U$ is defined as $$T = \left\{ \sum_{u \in U} c_n u \mid  c_n \text{ complex } , \sum_{u \in U} c_n u  \text{ converges unconditionally} \right\}.$$ But, how do I see that this set is closed?",,"['functional-analysis', 'hilbert-spaces']"
6,Help understanding these examples of weak and weak-* convergence.,Help understanding these examples of weak and weak-* convergence.,,"I'll preface the examples I am trying to understand with the definitions of weak and weak-* convergence in a Banach space $X$: ""A sequence $(x_n)_{n=1}^\infty$ converges weakly to $x$ in $X$ if for all $f\in X'$, $f(x_n)\to f(x)$ in $\mathbb F$ as $n\to\infty.$ A sequence $(f_n)_{n=1}^\infty\subset X'$ converges weak-* to $f$ in $X'$ if for all $x\in X$, $f_n(x)\to f(x)$ in $\mathbb F$ as $n\to\infty.$"" In my notes I am given the following two examples, although, I am having trouble following them completely. In all examples $(e_n)_{n=1}^\infty$ denotes the infinite sequence with zero in every entry apart from its $n$-th entry, which is one. Example $1$: Let $(e_n)_{n=1}^\infty\subset\ell^2$. Then $(e_n)_{n=1}^\infty$ does not converge strongly anywhere. But since $(\ell ^2)'$ is isometrically isomorphic to $\ell^2$ we can represent every $f\in(\ell^2)'$ as the action of some $a\in \ell^2$ on another $x\in\ell^2$. Letting $f:=f_a$ we have that then $f_a(x)=\sum_{k=1}^\infty a_kx_k$. We then have that $f(e_n)=a_n$. My notes then go on to say that if $a\in\ell^2$ then $a_n\to0$ as $n\to\infty$ and $x_n\rightharpoonup x$ in $\ell^2$. It is at this last sentence that I am lost. I understand that $f(e_n)=a_n$, but how does the fact that $a\in\ell^2$ allow us to deduce that $a_n\to0$ as $n\to\infty$ and then in turn allow us to establish the weak convergence? Example $2$: Take $(e_n)_{n=1}^\infty\subset\ell^1$. Then for every $f\in(\ell^1)'$ we can define similarly to what we done above $f:=f_a$ for some $a\in\ell^\infty$. But then my notes say that $f_a(e_n)=a_n$ does not have to go to zero as $n\to\infty$. From which point it then goes on to say that it is not true that $e_n\rightharpoonup 0$ in $\ell^1$. Again, similar to the last example, I am completely lost on the latter part of the example. Why in this case for $a\in\ell^\infty$ does $f_a(e_n)=a_n$ not have to converge strongly to zero as $n$ increases?","I'll preface the examples I am trying to understand with the definitions of weak and weak-* convergence in a Banach space $X$: ""A sequence $(x_n)_{n=1}^\infty$ converges weakly to $x$ in $X$ if for all $f\in X'$, $f(x_n)\to f(x)$ in $\mathbb F$ as $n\to\infty.$ A sequence $(f_n)_{n=1}^\infty\subset X'$ converges weak-* to $f$ in $X'$ if for all $x\in X$, $f_n(x)\to f(x)$ in $\mathbb F$ as $n\to\infty.$"" In my notes I am given the following two examples, although, I am having trouble following them completely. In all examples $(e_n)_{n=1}^\infty$ denotes the infinite sequence with zero in every entry apart from its $n$-th entry, which is one. Example $1$: Let $(e_n)_{n=1}^\infty\subset\ell^2$. Then $(e_n)_{n=1}^\infty$ does not converge strongly anywhere. But since $(\ell ^2)'$ is isometrically isomorphic to $\ell^2$ we can represent every $f\in(\ell^2)'$ as the action of some $a\in \ell^2$ on another $x\in\ell^2$. Letting $f:=f_a$ we have that then $f_a(x)=\sum_{k=1}^\infty a_kx_k$. We then have that $f(e_n)=a_n$. My notes then go on to say that if $a\in\ell^2$ then $a_n\to0$ as $n\to\infty$ and $x_n\rightharpoonup x$ in $\ell^2$. It is at this last sentence that I am lost. I understand that $f(e_n)=a_n$, but how does the fact that $a\in\ell^2$ allow us to deduce that $a_n\to0$ as $n\to\infty$ and then in turn allow us to establish the weak convergence? Example $2$: Take $(e_n)_{n=1}^\infty\subset\ell^1$. Then for every $f\in(\ell^1)'$ we can define similarly to what we done above $f:=f_a$ for some $a\in\ell^\infty$. But then my notes say that $f_a(e_n)=a_n$ does not have to go to zero as $n\to\infty$. From which point it then goes on to say that it is not true that $e_n\rightharpoonup 0$ in $\ell^1$. Again, similar to the last example, I am completely lost on the latter part of the example. Why in this case for $a\in\ell^\infty$ does $f_a(e_n)=a_n$ not have to converge strongly to zero as $n$ increases?",,"['real-analysis', 'functional-analysis', 'convergence-divergence', 'banach-spaces', 'weak-convergence']"
7,Inner products equal implies the arguments are equal,Inner products equal implies the arguments are equal,,"Today I was looking at theorems involving inner products which use the 'fact' that if $\langle x,y \rangle = \langle x,z \rangle$ for all $x$ then $y=z$. Is the following a sufficient proof of that? Consider an inner product space $(M,\langle\cdot,\cdot\rangle)$ and $x,y,z\in M$. Assume that $\forall x\in M, \langle x,y\rangle = \langle x,z\rangle$. Then, using the linearity of the inner product, $\forall x\in M, \langle x,y-z\rangle =0$, meaning that $y-z$ is orthogonal to every vector in $M$. Thus, since $0$ is the only vector which is orthogonal to all vectors, $y-z$ is $0\implies y=z$. Or alternatively, since $\forall x\in M, \langle x,y-z\rangle=0$, we can pick $x=y-z$ to get $\langle y-z,y-z\rangle = 0 =||y-z||^2\implies y-z=0\implies y=z$.","Today I was looking at theorems involving inner products which use the 'fact' that if $\langle x,y \rangle = \langle x,z \rangle$ for all $x$ then $y=z$. Is the following a sufficient proof of that? Consider an inner product space $(M,\langle\cdot,\cdot\rangle)$ and $x,y,z\in M$. Assume that $\forall x\in M, \langle x,y\rangle = \langle x,z\rangle$. Then, using the linearity of the inner product, $\forall x\in M, \langle x,y-z\rangle =0$, meaning that $y-z$ is orthogonal to every vector in $M$. Thus, since $0$ is the only vector which is orthogonal to all vectors, $y-z$ is $0\implies y=z$. Or alternatively, since $\forall x\in M, \langle x,y-z\rangle=0$, we can pick $x=y-z$ to get $\langle y-z,y-z\rangle = 0 =||y-z||^2\implies y-z=0\implies y=z$.",,"['functional-analysis', 'inner-products']"
8,"Is a function with values in $\mathcal{D}'(\Omega)$ an element of $\mathcal{D}'(\Omega\times(0,T))$?.",Is a function with values in  an element of ?.,"\mathcal{D}'(\Omega) \mathcal{D}'(\Omega\times(0,T))","The question is in the end of the post. First, I need some preliminary considerations. In the Temam's book (as well as in the Dautray's book ), the author proves that there exists $u\in C([0,T];H)\cap L^2(0,T;V)$ with $u'\in L^2(0,T;V')$ such that $$u_t(t)-Au(t)=f(t)\quad\text{in}\quad V',\qquad\forall\ t\in(0,T),\tag{1}$$ where $f\in L^2(0,T;V')$ is given and $A:V\to V'$ is the operator defined by $$\langle Av,w\rangle_{V',V}=(v,w)_{H_0^1(\Omega)},\quad\forall \ v,w\in V.$$ The spaces are defined by $V={\overline{\mathcal{V}}}^{H^1(\Omega)\times H^1(\Omega)}$ and $H={\overline{\mathcal{V}}}^{L^2(\Omega)\times L^2(\Omega)}$, where $\Omega$ is a bounded open subset of $\mathbb{R}^2$ with smooth boundary and $$\mathcal{V}=\{v\in\mathcal{D}(\Omega)\times\mathcal{D}(\Omega)\mid\operatorname{div} v=0\}.$$ After proving existence the author considers the functions $$U(t)=\int_0^tu(s)\;ds,\qquad F(t)=\int_0^Tf(s)\;ds.$$ From the regularity of $u$ and $f$ we have $U\in C([0,T];V)$ and $F\in C([0,T];V')$. And integrating $(1)$ we get $$u(t)-u(0)-AU(t)=F(t)\quad\text{in}\quad V',\qquad\forall \ t\in(0,T).$$ This implies $$\langle u(t)-u(0)-\Delta U(t)-F(t),v\rangle_{V',V}=0,\quad\forall\ v\in V,\;t\in(0,T).$$ Identifynig (by extension) $S(t):=u(t)-u(0)-\Delta U(t)-F(t)$ with a element of $H^{-1}(\Omega)$, it follows as consequence of the "" Rham's Theorem "" that, for each $t\in (0,T)$, there exists $P(t)\in\mathcal{D}'(\Omega)$ such that $$u(t)-u(0)-\Delta U(t)-F(t)=\nabla P(t)\quad\text{in}\quad\mathcal{D}'(\Omega).\tag{2}$$ After justifying that $\nabla P\in C([0,T]; [H^{-1}(\Omega)]^2)$ and $P\in C([0,T];L^2(\Omega))$, the author says that we can ""differentiate $(2)$ in the $t$ variable, in the distribution sense in $Q:=\Omega\times (0,T)$"", which yields $$u_t-\Delta u-f=\nabla p\quad\text{in}\quad\mathcal{D}'(Q),\tag{3}$$ where $p=\partial_t P$. Question: It is not clear for me how to get $(3)$ from $(2)$. To differenciate $(2)$ with respect to $t$ in the sense of $\mathcal{D}'(Q)$ we need to indentify the terms of $(2)$ with elements of $\mathcal{D}'(Q)$ , right? How is this identification made? To prove $(3)$ we have to show that, for each $\varphi\in \mathcal{D}(Q)$,    $$\langle u_t-\Delta u-f,\varphi\rangle=\langle \nabla (\partial_t P),\varphi\rangle.$$   Assuming that $P\in \mathcal{D}'(Q)$, we have   $$\langle \nabla (\partial_t P),\varphi\rangle =\langle \partial_t (\nabla P), \varphi\rangle \overset{(2)}{=}\langle \partial_t (u(t)-u(0)-\Delta U(t)-F(t)), \varphi\rangle,$$   but why is $P$ in $\mathcal{D}'(Q)$ and why   $$\langle \partial_t (u(t)-u(0)-\Delta U(t)-F(t)), \varphi\rangle=\langle u_t(t)-\Delta u(t)-f(t), \varphi\rangle?$$","The question is in the end of the post. First, I need some preliminary considerations. In the Temam's book (as well as in the Dautray's book ), the author proves that there exists $u\in C([0,T];H)\cap L^2(0,T;V)$ with $u'\in L^2(0,T;V')$ such that $$u_t(t)-Au(t)=f(t)\quad\text{in}\quad V',\qquad\forall\ t\in(0,T),\tag{1}$$ where $f\in L^2(0,T;V')$ is given and $A:V\to V'$ is the operator defined by $$\langle Av,w\rangle_{V',V}=(v,w)_{H_0^1(\Omega)},\quad\forall \ v,w\in V.$$ The spaces are defined by $V={\overline{\mathcal{V}}}^{H^1(\Omega)\times H^1(\Omega)}$ and $H={\overline{\mathcal{V}}}^{L^2(\Omega)\times L^2(\Omega)}$, where $\Omega$ is a bounded open subset of $\mathbb{R}^2$ with smooth boundary and $$\mathcal{V}=\{v\in\mathcal{D}(\Omega)\times\mathcal{D}(\Omega)\mid\operatorname{div} v=0\}.$$ After proving existence the author considers the functions $$U(t)=\int_0^tu(s)\;ds,\qquad F(t)=\int_0^Tf(s)\;ds.$$ From the regularity of $u$ and $f$ we have $U\in C([0,T];V)$ and $F\in C([0,T];V')$. And integrating $(1)$ we get $$u(t)-u(0)-AU(t)=F(t)\quad\text{in}\quad V',\qquad\forall \ t\in(0,T).$$ This implies $$\langle u(t)-u(0)-\Delta U(t)-F(t),v\rangle_{V',V}=0,\quad\forall\ v\in V,\;t\in(0,T).$$ Identifynig (by extension) $S(t):=u(t)-u(0)-\Delta U(t)-F(t)$ with a element of $H^{-1}(\Omega)$, it follows as consequence of the "" Rham's Theorem "" that, for each $t\in (0,T)$, there exists $P(t)\in\mathcal{D}'(\Omega)$ such that $$u(t)-u(0)-\Delta U(t)-F(t)=\nabla P(t)\quad\text{in}\quad\mathcal{D}'(\Omega).\tag{2}$$ After justifying that $\nabla P\in C([0,T]; [H^{-1}(\Omega)]^2)$ and $P\in C([0,T];L^2(\Omega))$, the author says that we can ""differentiate $(2)$ in the $t$ variable, in the distribution sense in $Q:=\Omega\times (0,T)$"", which yields $$u_t-\Delta u-f=\nabla p\quad\text{in}\quad\mathcal{D}'(Q),\tag{3}$$ where $p=\partial_t P$. Question: It is not clear for me how to get $(3)$ from $(2)$. To differenciate $(2)$ with respect to $t$ in the sense of $\mathcal{D}'(Q)$ we need to indentify the terms of $(2)$ with elements of $\mathcal{D}'(Q)$ , right? How is this identification made? To prove $(3)$ we have to show that, for each $\varphi\in \mathcal{D}(Q)$,    $$\langle u_t-\Delta u-f,\varphi\rangle=\langle \nabla (\partial_t P),\varphi\rangle.$$   Assuming that $P\in \mathcal{D}'(Q)$, we have   $$\langle \nabla (\partial_t P),\varphi\rangle =\langle \partial_t (\nabla P), \varphi\rangle \overset{(2)}{=}\langle \partial_t (u(t)-u(0)-\Delta U(t)-F(t)), \varphi\rangle,$$   but why is $P$ in $\mathcal{D}'(Q)$ and why   $$\langle \partial_t (u(t)-u(0)-\Delta U(t)-F(t)), \varphi\rangle=\langle u_t(t)-\Delta u(t)-f(t), \varphi\rangle?$$",,"['functional-analysis', 'partial-differential-equations', 'distribution-theory']"
9,"Show that for any $x,y \in X$, $\sum_{k=1}^\infty|\left<x,e_k\right>\left<y,e_k\right>|\le \|x\|\|y\|.$ [duplicate]","Show that for any ,  [duplicate]","x,y \in X \sum_{k=1}^\infty|\left<x,e_k\right>\left<y,e_k\right>|\le \|x\|\|y\|.","This question already has an answer here : Proving $\sum\limits_{i=1}^k | \langle x,v_i \rangle \langle y,v_i\rangle| \leq \|x\|\cdot \|y\|$ (1 answer) Closed 5 years ago . Let $(e_k)$ be any orthonormal sequence in an inner product space $X$. Show that for any $x,y \in X$ $$\sum_{k=1}^\infty|\left<x,e_k\right>\left<y,e_k\right>|\le \|x\|\|y\|.$$ If $x=y$, then the proof becomes trivial, as then the result follows directly from Bessel's inequality $$\sum_{k=1}^\infty|\left<x,e_k\right>|^2\le \|x\|^2.$$ Now suppose that $x \neq y$, then \begin{align}\sum_{k=1}^\infty |\left<x,e_k\right>\left<y,e_k\right>| &= \sum_{k=1}^\infty \left|\left<\left<x,e_k\right>y,e_k\right>\right|\end{align} But I have no idea where to continue from here. Can anyone please help point me in the right direction?","This question already has an answer here : Proving $\sum\limits_{i=1}^k | \langle x,v_i \rangle \langle y,v_i\rangle| \leq \|x\|\cdot \|y\|$ (1 answer) Closed 5 years ago . Let $(e_k)$ be any orthonormal sequence in an inner product space $X$. Show that for any $x,y \in X$ $$\sum_{k=1}^\infty|\left<x,e_k\right>\left<y,e_k\right>|\le \|x\|\|y\|.$$ If $x=y$, then the proof becomes trivial, as then the result follows directly from Bessel's inequality $$\sum_{k=1}^\infty|\left<x,e_k\right>|^2\le \|x\|^2.$$ Now suppose that $x \neq y$, then \begin{align}\sum_{k=1}^\infty |\left<x,e_k\right>\left<y,e_k\right>| &= \sum_{k=1}^\infty \left|\left<\left<x,e_k\right>y,e_k\right>\right|\end{align} But I have no idea where to continue from here. Can anyone please help point me in the right direction?",,"['functional-analysis', 'inner-products', 'orthonormal']"
10,"About isometries $(\mathbb{R}^3,\left\|{\cdot}\right\|_{\infty})\to (\mathbb{R}^n,\left\|{\cdot}\right\|_2)$",About isometries,"(\mathbb{R}^3,\left\|{\cdot}\right\|_{\infty})\to (\mathbb{R}^n,\left\|{\cdot}\right\|_2)","I was wondering if we can find $n\in\mathbb{N}$ such that there exists an isometry $(\mathbb{R}^3,\left\|{\cdot}\right\|_{\infty})\to (\mathbb{R}^n,\left\|{\cdot}\right\|_2)$. I found out a theorem that states if $f:V\to W$ is a surjective isometry between real normed spaces then $f$ carries extreme points to extreme points, because $f$ is affine. Then if the isometry I mention exists it can't be surjective (well I know if $n>3$ this was trivial). So, could it be that there is an (not surjective) isometry  $(\mathbb{R}^3,\left\|{\cdot}\right\|_{\infty})\to (\mathbb{R}^n,\left\|{\cdot}\right\|_2)$? Thank you.","I was wondering if we can find $n\in\mathbb{N}$ such that there exists an isometry $(\mathbb{R}^3,\left\|{\cdot}\right\|_{\infty})\to (\mathbb{R}^n,\left\|{\cdot}\right\|_2)$. I found out a theorem that states if $f:V\to W$ is a surjective isometry between real normed spaces then $f$ carries extreme points to extreme points, because $f$ is affine. Then if the isometry I mention exists it can't be surjective (well I know if $n>3$ this was trivial). So, could it be that there is an (not surjective) isometry  $(\mathbb{R}^3,\left\|{\cdot}\right\|_{\infty})\to (\mathbb{R}^n,\left\|{\cdot}\right\|_2)$? Thank you.",,"['real-analysis', 'functional-analysis', 'normed-spaces', 'isometry']"
11,Duality of $L^p$ norm,Duality of  norm,L^p,"I'm reading a paper ""An extrapolation theorem in the theory of $A_p$ weights"", written by J. Garcia-Cuerva. I want to show the following. Let $1<p<\infty $ and $1<q<p$. Let $\mu$ be a measure on $(X,\mu)$ which is $\sigma$-finite and let $f\in L^p(X)$. Then there is $g\in L^{(p/q)^\prime}(X)$ with $\Vert g \Vert_{L^{(p/q)^\prime}(X)} =1$ such that  $$ \Vert f \Vert_{L^p(X)}^q = \Vert |f|^q\Vert_{L^\frac{p}{q}(X)}=\int_X |f|^q g d\mu. $$ Actually, the author's measure is $\mu =wdx$, $X=\mathbb{R}^n$ where $w\in A_{p/q}$. I tried to use Riesz representation theorem with $l(f)=\Vert f \Vert_{L^\frac{p}{q}(X)}$. But this is not linear functional. How can I verify the above equality?","I'm reading a paper ""An extrapolation theorem in the theory of $A_p$ weights"", written by J. Garcia-Cuerva. I want to show the following. Let $1<p<\infty $ and $1<q<p$. Let $\mu$ be a measure on $(X,\mu)$ which is $\sigma$-finite and let $f\in L^p(X)$. Then there is $g\in L^{(p/q)^\prime}(X)$ with $\Vert g \Vert_{L^{(p/q)^\prime}(X)} =1$ such that  $$ \Vert f \Vert_{L^p(X)}^q = \Vert |f|^q\Vert_{L^\frac{p}{q}(X)}=\int_X |f|^q g d\mu. $$ Actually, the author's measure is $\mu =wdx$, $X=\mathbb{R}^n$ where $w\in A_{p/q}$. I tried to use Riesz representation theorem with $l(f)=\Vert f \Vert_{L^\frac{p}{q}(X)}$. But this is not linear functional. How can I verify the above equality?",,"['real-analysis', 'functional-analysis', 'lp-spaces', 'harmonic-analysis']"
12,Does the following sum converge?,Does the following sum converge?,,"Let $u \in L^2(\mathbb{R})$. Is this sufficient to conclude that the sequence  $n \mapsto \int_{\mathbb{R}} \left\lvert u(x) \right\rvert e^{- \left\lvert x-n \right\rvert} dx$ is square-summable, i.e. $$\sum_{n=1}^{\infty} \left( \int_{\mathbb{R}} \left\lvert u(x) \right\rvert e^{- \left\lvert x-n \right\rvert} dx \right)^2 < \infty$$","Let $u \in L^2(\mathbb{R})$. Is this sufficient to conclude that the sequence  $n \mapsto \int_{\mathbb{R}} \left\lvert u(x) \right\rvert e^{- \left\lvert x-n \right\rvert} dx$ is square-summable, i.e. $$\sum_{n=1}^{\infty} \left( \int_{\mathbb{R}} \left\lvert u(x) \right\rvert e^{- \left\lvert x-n \right\rvert} dx \right)^2 < \infty$$",,"['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'convergence-divergence']"
13,norm convergence in Hilbert space,norm convergence in Hilbert space,,"Given a sequnce $y_i\in H$ ($H$ is Hilbert space over $\mathbb{C}$) which satisfies $$ ||\sum\beta_i y_i||<A $$ where $A$ is real constant and the latter inequality is true for any sequence of scalars $\{\beta_i\}$, $0\leq|\beta_i|\leq1$ which are zero except (maybe) finite number of indices. I need to show that $\sum y_i$ converges in norm. I tried to show that $\sum ||y_i|| < \infty$ (and than it is enough) however I can only show that $\sum ||y_i||^2 < \infty$: The latter follows from the fact that for any finite number of elements $x_i\in H$ there are scalars $c_i$, |c_i|=1 such that $||\sum c_i x_i||^2\geq \sum ||x_i||^2$ how can I proceed from here? thank you","Given a sequnce $y_i\in H$ ($H$ is Hilbert space over $\mathbb{C}$) which satisfies $$ ||\sum\beta_i y_i||<A $$ where $A$ is real constant and the latter inequality is true for any sequence of scalars $\{\beta_i\}$, $0\leq|\beta_i|\leq1$ which are zero except (maybe) finite number of indices. I need to show that $\sum y_i$ converges in norm. I tried to show that $\sum ||y_i|| < \infty$ (and than it is enough) however I can only show that $\sum ||y_i||^2 < \infty$: The latter follows from the fact that for any finite number of elements $x_i\in H$ there are scalars $c_i$, |c_i|=1 such that $||\sum c_i x_i||^2\geq \sum ||x_i||^2$ how can I proceed from here? thank you",,"['functional-analysis', 'hilbert-spaces']"
14,Reference Request: Good Introduction to Functional Calculus,Reference Request: Good Introduction to Functional Calculus,,"I would like to know if there is a standard reference on functional calculus. In particular I am interested in understanding how the resolvant integral $$f(T) = \frac{1}{2\pi i}\int_{C} f(\lambda)[\lambda - T]^{-1}d\lambda$$ is used to derive formulas like the polynomial formula for $N\times N$ diagonalizable operators, $$P(T) = \sum_{j=1}^NP(\lambda_j)\prod_{k\neq j}\frac{\lambda_k-T}{\lambda_k-\lambda_j}$$ and in particular to calculate arbitrary powers of $N\times N$ diagonalizable matrices.","I would like to know if there is a standard reference on functional calculus. In particular I am interested in understanding how the resolvant integral $$f(T) = \frac{1}{2\pi i}\int_{C} f(\lambda)[\lambda - T]^{-1}d\lambda$$ is used to derive formulas like the polynomial formula for $N\times N$ diagonalizable operators, $$P(T) = \sum_{j=1}^NP(\lambda_j)\prod_{k\neq j}\frac{\lambda_k-T}{\lambda_k-\lambda_j}$$ and in particular to calculate arbitrary powers of $N\times N$ diagonalizable matrices.",,"['functional-analysis', 'reference-request', 'hilbert-spaces', 'functional-calculus']"
15,Nehari Manifold properties: Show that Nehari manifold is bounded away from zero.,Nehari Manifold properties: Show that Nehari manifold is bounded away from zero.,,"Let $E$ be a real Banach space and $I\in\mathcal{C}^1(E;\mathbb{R})$ a functional. Define the Nehari manifold  \begin{align} \mathcal{N}=\{u\in E\backslash\{0\}:I'(u)u=0\}, \end{align} where the Frechet derivative of $I$ at $u$, $I'(u)$, is an element of the dual space $E^*$, and we denote $I'(u)$ evaluated at $v\in E$ by $I'(u)v$. Suppose $u\neq 0 $ is a critical point of $I$, i.e., $I'(u)=0$. Then $u\in\mathcal{N}$. Set $S=S_1(0)=\{u\in E:||u||=1\}$ and assume $I(0)=0$. Suppose the following two conditions are true: For all $u\in E\backslash \{0\}$ there is a $t_u$ such that if $\phi_u(t)=I(tu)$, then $\phi'_u(t)>0$ for all $0<t<t_u$ and $\phi'_u(t)<0$ for all $t>t_u$, There exists a $\delta>0$, independent of $u$, such that $t_u\geq\delta$ for all $u\in E$. The Nehari manifold has some useful properties as indicated in the references provided here: Question about Nehari manifold . I am in particular interested in justifying that 1 and 2 implies that $\mathcal{N}$ is bounded away from $0$, i.e., for every $u\in\mathcal{N}$ there is a $\rho>0$, independent of $u$, such that $||u||\geq\rho$. I am not sure how to approach the problem.","Let $E$ be a real Banach space and $I\in\mathcal{C}^1(E;\mathbb{R})$ a functional. Define the Nehari manifold  \begin{align} \mathcal{N}=\{u\in E\backslash\{0\}:I'(u)u=0\}, \end{align} where the Frechet derivative of $I$ at $u$, $I'(u)$, is an element of the dual space $E^*$, and we denote $I'(u)$ evaluated at $v\in E$ by $I'(u)v$. Suppose $u\neq 0 $ is a critical point of $I$, i.e., $I'(u)=0$. Then $u\in\mathcal{N}$. Set $S=S_1(0)=\{u\in E:||u||=1\}$ and assume $I(0)=0$. Suppose the following two conditions are true: For all $u\in E\backslash \{0\}$ there is a $t_u$ such that if $\phi_u(t)=I(tu)$, then $\phi'_u(t)>0$ for all $0<t<t_u$ and $\phi'_u(t)<0$ for all $t>t_u$, There exists a $\delta>0$, independent of $u$, such that $t_u\geq\delta$ for all $u\in E$. The Nehari manifold has some useful properties as indicated in the references provided here: Question about Nehari manifold . I am in particular interested in justifying that 1 and 2 implies that $\mathcal{N}$ is bounded away from $0$, i.e., for every $u\in\mathcal{N}$ there is a $\rho>0$, independent of $u$, such that $||u||\geq\rho$. I am not sure how to approach the problem.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'calculus-of-variations']"
16,Are Darboux Functions closed under uniform limit?,Are Darboux Functions closed under uniform limit?,,"I suspect there exists a sequence of functions $f_n : [0,1] \to \mathbb{R}$ s.t $f_n$ converges uniformly to $f$, $f_n$ Darboux but $f$ not Darboux. However, I haven't found any examples. What are some examples?","I suspect there exists a sequence of functions $f_n : [0,1] \to \mathbb{R}$ s.t $f_n$ converges uniformly to $f$, $f_n$ Darboux but $f$ not Darboux. However, I haven't found any examples. What are some examples?",,"['real-analysis', 'functional-analysis']"
17,"Is $C^1[a,b]$ with the norm $\left \| f \right \|_1=(\int_{a}^{b}\left | f(t) \right |dt)+(\int_{a}^{b}\left | f´(t) \right |dt)$ a complete space?",Is  with the norm  a complete space?,"C^1[a,b] \left \| f \right \|_1=(\int_{a}^{b}\left | f(t) \right |dt)+(\int_{a}^{b}\left | f´(t) \right |dt)","Is $C^1[a,b]$ with the norm   $\left \| f \right \|_1=(\int_{a}^{b}\left | f(t) \right |dt)+(\int_{a}^{b}\left | f´(t) \right |dt)$ a complete space? I thought with parabolas based on this link , but the area is infinite $C([0, 1])$ is not complete with respect to the norm $\lVert f\rVert _1 = \int_0^1 \lvert f (x) \rvert \,dx$ . Thanks.","Is $C^1[a,b]$ with the norm   $\left \| f \right \|_1=(\int_{a}^{b}\left | f(t) \right |dt)+(\int_{a}^{b}\left | f´(t) \right |dt)$ a complete space? I thought with parabolas based on this link , but the area is infinite $C([0, 1])$ is not complete with respect to the norm $\lVert f\rVert _1 = \int_0^1 \lvert f (x) \rvert \,dx$ . Thanks.",,"['real-analysis', 'functional-analysis', 'analysis']"
18,The map $C : \mathcal{H} \to L^2 (X)$ is bounded whenever it is bounded on a dense subspace $\mathcal{D} \subset \mathcal{H}$.,The map  is bounded whenever it is bounded on a dense subspace .,C : \mathcal{H} \to L^2 (X) \mathcal{D} \subset \mathcal{H},"Let $\mathcal{H}$ be a Hilbert space, let $\mathcal{D} \subset \mathcal{H}$ be a dense subspace and let $(\eta_x)_{x \in X} \subset \mathcal{H}$ be indexed by a $\sigma$-finite measure space $(X, \Sigma_X, \mu_X)$. Suppose there exists a $K > 0$ such that, for all $f \in \mathcal{D}$,   $$ \int_X | \langle f, \eta_x \rangle |^2 \; d\mu_X (x) \leq K \| f \|^2_{\mathcal{H}} \quad \quad \quad \quad (*)$$   Then the inequality $(*)$ holds for all $f \in \mathcal{H}$. I am aware of a proof of the above stated result that is quite ingenious and heavily depends on the fact that the involved measure space is $\sigma$-finite. However, I am curious if this is really necessary. I am approaching the above stated result by considering the linear operator  $$C : \mathcal{H} \to L^2 (X), \; f \mapsto \{ \langle f, \eta_x \rangle \}_{x \in X},$$ which is bounded on $\mathcal{D}$. However, boundedness of a dense subspace is, in general, not sufficient for a linear operator to be bounded on its entire domain. Thus the result is not trivial, and is a special property of $C$. Any help or comment is highly appreciated.","Let $\mathcal{H}$ be a Hilbert space, let $\mathcal{D} \subset \mathcal{H}$ be a dense subspace and let $(\eta_x)_{x \in X} \subset \mathcal{H}$ be indexed by a $\sigma$-finite measure space $(X, \Sigma_X, \mu_X)$. Suppose there exists a $K > 0$ such that, for all $f \in \mathcal{D}$,   $$ \int_X | \langle f, \eta_x \rangle |^2 \; d\mu_X (x) \leq K \| f \|^2_{\mathcal{H}} \quad \quad \quad \quad (*)$$   Then the inequality $(*)$ holds for all $f \in \mathcal{H}$. I am aware of a proof of the above stated result that is quite ingenious and heavily depends on the fact that the involved measure space is $\sigma$-finite. However, I am curious if this is really necessary. I am approaching the above stated result by considering the linear operator  $$C : \mathcal{H} \to L^2 (X), \; f \mapsto \{ \langle f, \eta_x \rangle \}_{x \in X},$$ which is bounded on $\mathcal{D}$. However, boundedness of a dense subspace is, in general, not sufficient for a linear operator to be bounded on its entire domain. Thus the result is not trivial, and is a special property of $C$. Any help or comment is highly appreciated.",,"['functional-analysis', 'measure-theory']"
19,Continuous Linear operators and Kernel,Continuous Linear operators and Kernel,,"Let $T:X\rightarrow Y$ be a linear operator such that $Y$ is finite dimensional. Show that $T$ is continuous if and only if Kernel of $T$ is closed in $X$. Suppose $T$ is discontinuous. There exists $x_n\in X$ such that $||x_n||=1$ such that $||Tx_n||$ diverges. As $T$ is a nonzero operator, there exists $a\in X$ that is not in kernel of $T$. We look for elements of kernel of $T$ that converges to $a$. Consider the case when $Y$ is of dimension $1$. Let $a_n=a-\frac{T(a)x_n}{T(x_n)}$. See that $a_n$ is in kernel of $T$ and $a_n\rightarrow a$ as $x_n$ is of norm $1$ and $Tx_n$ diverges. So, we have $a_n$ in kernel of $T$ and $a_n$ converges to $a$ but $a$ is not in kernel which concludes that Kernel is not closed. I am not able to extend this to case of  arbitrary finite dimension $Y$","Let $T:X\rightarrow Y$ be a linear operator such that $Y$ is finite dimensional. Show that $T$ is continuous if and only if Kernel of $T$ is closed in $X$. Suppose $T$ is discontinuous. There exists $x_n\in X$ such that $||x_n||=1$ such that $||Tx_n||$ diverges. As $T$ is a nonzero operator, there exists $a\in X$ that is not in kernel of $T$. We look for elements of kernel of $T$ that converges to $a$. Consider the case when $Y$ is of dimension $1$. Let $a_n=a-\frac{T(a)x_n}{T(x_n)}$. See that $a_n$ is in kernel of $T$ and $a_n\rightarrow a$ as $x_n$ is of norm $1$ and $Tx_n$ diverges. So, we have $a_n$ in kernel of $T$ and $a_n$ converges to $a$ but $a$ is not in kernel which concludes that Kernel is not closed. I am not able to extend this to case of  arbitrary finite dimension $Y$",,['functional-analysis']
20,similar matrices in analysis,similar matrices in analysis,,"Suppose $A$ and $B$ are two matrices or bounded operators such that $A=\lim_{n\to\infty} A_n$ and $B=\lim_{n\to\infty} B_n$ for a sequence of matrices or operators $A_n$ and $B_n$. (The limit is to be understood as the matrix or operator norm.) Suppose in addition that for each $n$, $A_n$ is similar (i.e. conjugate) to $B_n$. Does it follow that $A$ is similar to $B$? I think the answer is positive at least in some cases, for instance when $A_n$ and $B_n$ are matrices and are orthogonally similar. I wonder what general statements can be proved (even by modifying the conditions, modes of convergence, etc) for this setting and in what cases counterexamples can be constructed. Any help is appreciated.","Suppose $A$ and $B$ are two matrices or bounded operators such that $A=\lim_{n\to\infty} A_n$ and $B=\lim_{n\to\infty} B_n$ for a sequence of matrices or operators $A_n$ and $B_n$. (The limit is to be understood as the matrix or operator norm.) Suppose in addition that for each $n$, $A_n$ is similar (i.e. conjugate) to $B_n$. Does it follow that $A$ is similar to $B$? I think the answer is positive at least in some cases, for instance when $A_n$ and $B_n$ are matrices and are orthogonally similar. I wonder what general statements can be proved (even by modifying the conditions, modes of convergence, etc) for this setting and in what cases counterexamples can be constructed. Any help is appreciated.",,"['functional-analysis', 'operator-theory']"
21,Eigenvalues of an infinite dimensional linear operator,Eigenvalues of an infinite dimensional linear operator,,"I have seen across the site questions regarding eigenvalues of linear maps between infinite dimensional vector spaces, and it regularly comes up that it is possible for there to be none or for every scalar to be an eigenvalue. When eigenvalues do exist, it seems that some of the familiar properties from finite dimensions carry across, such as this . I would define an eigenvalue of a linear operator $L$ as any scalar $\lambda$ such that for some non-zero $u \in V$, the relevant vector space, $(L-\lambda I)(u)=0$. Let's suppose that $X$ is a topological space, and I am working with some vector space of continuous functions from $X$ into $\mathbb{R}$, such as $C(X)$ or $C_b(X)$; call this vector space $V$. Let $L:V \to V$ be an endomorphism. I want to know what conditions on $X$, $V$ and/or $L$ : Imply the certain existence of eigenvalues for $L$. Imply the existence of a basis for $V$ of eigenfunctions/vectors of $L$. Ensure that there is a countable number of of eigenvalues, rather than a continuum. I have experienced, for example, the case in Sturm-Liouville theory where we have $X \subset \mathbb{R}$ compact, $V=\{C^\infty(X):$ boundary conditions met$\}$ and $L$ a second-order self-adjoint differential operator, where all three of these conditions are satisfied simultaneously; alternatively I have seen the same but with '$X$ compact' replaced with 'for all $f \in V$, $f$ vanishes at $\infty$'. Infinite differentiability and self-adjointness are rather strong and specific conditions in a more general context, but perhaps there are some weaker or related constraints which can be applied to more general topological spaces? I expect local compactness or compactness may be an important conditions on $X$, for a start. There are obviously some simple maps (such as the identity and the zero map) which have eigenvalues for any $X$, $V$, but I am interested in as broad a range of maps as possible. Edit: I would be satisfied with simultaneous conditions on $X$, $L$, $V$ which ensure at least point 1. I have seen plenty of discouraging non-examples, most taking the form of some kind of shift operator, but I reason that if some linear operators have eigenvalues, then there must be a way to characterise the class of operators which do in a way which will probably involve the underlying spaces. Having done some more research myself, the spectral theorem applies to all normal operators , which form a broader class than self-adjoint operators, but is still not exclusive to these (see section 6 of that link in particular), so my question remains.","I have seen across the site questions regarding eigenvalues of linear maps between infinite dimensional vector spaces, and it regularly comes up that it is possible for there to be none or for every scalar to be an eigenvalue. When eigenvalues do exist, it seems that some of the familiar properties from finite dimensions carry across, such as this . I would define an eigenvalue of a linear operator $L$ as any scalar $\lambda$ such that for some non-zero $u \in V$, the relevant vector space, $(L-\lambda I)(u)=0$. Let's suppose that $X$ is a topological space, and I am working with some vector space of continuous functions from $X$ into $\mathbb{R}$, such as $C(X)$ or $C_b(X)$; call this vector space $V$. Let $L:V \to V$ be an endomorphism. I want to know what conditions on $X$, $V$ and/or $L$ : Imply the certain existence of eigenvalues for $L$. Imply the existence of a basis for $V$ of eigenfunctions/vectors of $L$. Ensure that there is a countable number of of eigenvalues, rather than a continuum. I have experienced, for example, the case in Sturm-Liouville theory where we have $X \subset \mathbb{R}$ compact, $V=\{C^\infty(X):$ boundary conditions met$\}$ and $L$ a second-order self-adjoint differential operator, where all three of these conditions are satisfied simultaneously; alternatively I have seen the same but with '$X$ compact' replaced with 'for all $f \in V$, $f$ vanishes at $\infty$'. Infinite differentiability and self-adjointness are rather strong and specific conditions in a more general context, but perhaps there are some weaker or related constraints which can be applied to more general topological spaces? I expect local compactness or compactness may be an important conditions on $X$, for a start. There are obviously some simple maps (such as the identity and the zero map) which have eigenvalues for any $X$, $V$, but I am interested in as broad a range of maps as possible. Edit: I would be satisfied with simultaneous conditions on $X$, $L$, $V$ which ensure at least point 1. I have seen plenty of discouraging non-examples, most taking the form of some kind of shift operator, but I reason that if some linear operators have eigenvalues, then there must be a way to characterise the class of operators which do in a way which will probably involve the underlying spaces. Having done some more research myself, the spectral theorem applies to all normal operators , which form a broader class than self-adjoint operators, but is still not exclusive to these (see section 6 of that link in particular), so my question remains.",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
22,"Compact subset of $C[0,1]$",Compact subset of,"C[0,1]","Question : Let $\sigma: [0,1] \to \mathbb{R}^{\ge 0}$ be a nonnegative, continuous function such that $\sigma(0) =0$. For each real $\lambda \ge 0$ define the following set: $$ F_ \lambda = \left\{ f \in C[0,1] : |f(0)| \le \lambda |f(1)| \text{ and } |f(x)-f(y)| \le \sigma\left(\left|x-y\right|\right) \text{ for all } x,y \in [0,1]\right\} $$ Characterize the $\lambda$ for which $F_\lambda$ is a compact subset of $C[0,1]$. Solution : (Let $\epsilon>0)$. I'm trying to use Arzela Ascoli. First, equicontinuity. Since $\sigma$ is continuous and $\sigma(0)=0$, there is some $\delta$ so that $\sigma(x) < \epsilon$ whenever $0<x<\delta$. Then, for $|x-y|<\delta$ $$ |f(x)-f(y)| \le \sigma(|x-y|) <\epsilon $$ so $F_\lambda$ is equicontinuous for all $\lambda$. Closedness is straightforward . . . Let $\|f_n - f\|_\infty \to 0$ where $(f_n) \subset F_\lambda$. The limit $f$ is continuous, and continuity of $f$ ensures that the other two criterion of $F_\lambda$ are satisfied. (Take limits.) The difficulty is with (equi)boundedness. Since $\sigma$ is continuous on a closed interval $[0,1]$, we have $\| \sigma\|_\infty \le M < \infty$ by the Extreme Value Theorem. If $\lambda=0$, then $|f(x)| \le |f(x)-f(0)| + |f(0)| \le \sigma(x) + 0 \le M$, so $F_0$ is uniformly bounded. If $1 \le \lambda<\infty$, then the constant functions $f_n(x) = n$ are in $F_\lambda$, but they have no uniform bound. My main troubles are with $0 < \lambda <1$ . . . I have been trying to construct specific sequences that are unbounded and satisfy $|f_n(0)| \le \lambda |f_n(1)|$, but I am never sure if $|f_n(x)-f_n(y)| \le \sigma(|x-y|)$ without knowing more about $\sigma$. (Maybe that's the point) Thank you.","Question : Let $\sigma: [0,1] \to \mathbb{R}^{\ge 0}$ be a nonnegative, continuous function such that $\sigma(0) =0$. For each real $\lambda \ge 0$ define the following set: $$ F_ \lambda = \left\{ f \in C[0,1] : |f(0)| \le \lambda |f(1)| \text{ and } |f(x)-f(y)| \le \sigma\left(\left|x-y\right|\right) \text{ for all } x,y \in [0,1]\right\} $$ Characterize the $\lambda$ for which $F_\lambda$ is a compact subset of $C[0,1]$. Solution : (Let $\epsilon>0)$. I'm trying to use Arzela Ascoli. First, equicontinuity. Since $\sigma$ is continuous and $\sigma(0)=0$, there is some $\delta$ so that $\sigma(x) < \epsilon$ whenever $0<x<\delta$. Then, for $|x-y|<\delta$ $$ |f(x)-f(y)| \le \sigma(|x-y|) <\epsilon $$ so $F_\lambda$ is equicontinuous for all $\lambda$. Closedness is straightforward . . . Let $\|f_n - f\|_\infty \to 0$ where $(f_n) \subset F_\lambda$. The limit $f$ is continuous, and continuity of $f$ ensures that the other two criterion of $F_\lambda$ are satisfied. (Take limits.) The difficulty is with (equi)boundedness. Since $\sigma$ is continuous on a closed interval $[0,1]$, we have $\| \sigma\|_\infty \le M < \infty$ by the Extreme Value Theorem. If $\lambda=0$, then $|f(x)| \le |f(x)-f(0)| + |f(0)| \le \sigma(x) + 0 \le M$, so $F_0$ is uniformly bounded. If $1 \le \lambda<\infty$, then the constant functions $f_n(x) = n$ are in $F_\lambda$, but they have no uniform bound. My main troubles are with $0 < \lambda <1$ . . . I have been trying to construct specific sequences that are unbounded and satisfy $|f_n(0)| \le \lambda |f_n(1)|$, but I am never sure if $|f_n(x)-f_n(y)| \le \sigma(|x-y|)$ without knowing more about $\sigma$. (Maybe that's the point) Thank you.",,"['real-analysis', 'functional-analysis', 'compactness']"
23,Difference between little o and big O notation in taylor expansion,Difference between little o and big O notation in taylor expansion,,"I know I can say that: $$ y(t+\Delta t)=y(t)+y'(t)\Delta t+o(\Delta t) $$ But can I say that: $$ y(t+\Delta t)=y(t)+y'(t)\Delta t+O((\Delta t)^2) $$ In both cases, do I have to add that $\Delta t \rightarrow 0$ or i do not have to?","I know I can say that: $$ y(t+\Delta t)=y(t)+y'(t)\Delta t+o(\Delta t) $$ But can I say that: $$ y(t+\Delta t)=y(t)+y'(t)\Delta t+O((\Delta t)^2) $$ In both cases, do I have to add that $\Delta t \rightarrow 0$ or i do not have to?",,"['calculus', 'sequences-and-series', 'functional-analysis', 'functions', 'taylor-expansion']"
24,Algebra $A$ and its Gelfand spectrum,Algebra  and its Gelfand spectrum,A,"Let $A$ be the set of all function $f$ on $\mathbb{R}$ of the form $$ f(x)=d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt,\qquad\quad x\in\mathbb{R}, $$ where $d\in\mathbb{C}$ and $k\in L_1([0,\infty])$. The norm on $A$ is defined by $$ \|f\|:=|d|+\int\limits_0^\infty |k(t)|dt. $$ I want to show that $A$ is a commutative Banach algebra and find its Gelfand spectrum We need the following properties in order to show that $A$ is a commutative Banach algebra: commutativity associativity distributivity scalar multiplication property the norm of the product is less than or equal to the product of the norms, The first four properties are easy to prove: Let $f,g\in A$, then \begin{align} f(x)g(x)&=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\\ &=\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\\ &=g(x)f(x). \end{align} Let $f,g,h\in A$, then \begin{align} (f(x)g(x))h(x)&=\left(\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\right)\left(d_2+\int\limits_{0}^{\infty}e^{ixt}k_2(t)dt\right)\\ &=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\left(d_2+\int\limits_{0}^{\infty}e^{ixt}k_2(t)dt\right)\right)\\ &=f(x)(g(x)h(x)) \end{align} Let $f,g,h\in A$, then \begin{align} f(x)(g(x)+h(x))&=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)+\left(d_2+\int\limits_{0}^{\infty}e^{ixt}k_2(t)dt\right)\right)\\ &=f(x)g(x)+f(x)h(x) \end{align} This implies that $(g(x)+h(x))f(x)=g(x)f(x)+h(x)f(x)$ Let $f,g\in A$ and $\alpha\in\mathbb{R}$, then \begin{align} \alpha (f(x)g(x))&=\alpha\left(\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\right)\\ &=(\alpha f(x))g(x)=f(x)(\alpha g(x)). \end{align} Let $f,g\in A$, then \begin{align} f(x)g(x)&=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\\ &=dd_1+d\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt+d_1\int\limits_{0}^{\infty}e^{ixt}k(t)dt+\left(\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right) \end{align} Here I get stuck, I cannot show that $\|fg\|\leq\|f\|\|g\|$, any hints? Secondly, I have some difficulties mastering the Gelfand spectrum concept. How can I find Gelfand spectrum of $A$? Any hints are appreciated.","Let $A$ be the set of all function $f$ on $\mathbb{R}$ of the form $$ f(x)=d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt,\qquad\quad x\in\mathbb{R}, $$ where $d\in\mathbb{C}$ and $k\in L_1([0,\infty])$. The norm on $A$ is defined by $$ \|f\|:=|d|+\int\limits_0^\infty |k(t)|dt. $$ I want to show that $A$ is a commutative Banach algebra and find its Gelfand spectrum We need the following properties in order to show that $A$ is a commutative Banach algebra: commutativity associativity distributivity scalar multiplication property the norm of the product is less than or equal to the product of the norms, The first four properties are easy to prove: Let $f,g\in A$, then \begin{align} f(x)g(x)&=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\\ &=\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\\ &=g(x)f(x). \end{align} Let $f,g,h\in A$, then \begin{align} (f(x)g(x))h(x)&=\left(\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\right)\left(d_2+\int\limits_{0}^{\infty}e^{ixt}k_2(t)dt\right)\\ &=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\left(d_2+\int\limits_{0}^{\infty}e^{ixt}k_2(t)dt\right)\right)\\ &=f(x)(g(x)h(x)) \end{align} Let $f,g,h\in A$, then \begin{align} f(x)(g(x)+h(x))&=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)+\left(d_2+\int\limits_{0}^{\infty}e^{ixt}k_2(t)dt\right)\right)\\ &=f(x)g(x)+f(x)h(x) \end{align} This implies that $(g(x)+h(x))f(x)=g(x)f(x)+h(x)f(x)$ Let $f,g\in A$ and $\alpha\in\mathbb{R}$, then \begin{align} \alpha (f(x)g(x))&=\alpha\left(\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\right)\\ &=(\alpha f(x))g(x)=f(x)(\alpha g(x)). \end{align} Let $f,g\in A$, then \begin{align} f(x)g(x)&=\left(d+\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(d_1+\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right)\\ &=dd_1+d\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt+d_1\int\limits_{0}^{\infty}e^{ixt}k(t)dt+\left(\int\limits_{0}^{\infty}e^{ixt}k(t)dt\right)\left(\int\limits_{0}^{\infty}e^{ixt}k_1(t)dt\right) \end{align} Here I get stuck, I cannot show that $\|fg\|\leq\|f\|\|g\|$, any hints? Secondly, I have some difficulties mastering the Gelfand spectrum concept. How can I find Gelfand spectrum of $A$? Any hints are appreciated.",,"['integration', 'functional-analysis', 'operator-theory', 'operator-algebras', 'banach-algebras']"
25,"Functional Analysis by Reed and Simon, chapter 3 exercise 15","Functional Analysis by Reed and Simon, chapter 3 exercise 15",,"Let $H$ be a Hilbert space with an orthonormal basis  $\{ x_n \}_{n=1}^\infty$ and let $\{ y_n \}_{n=1}^\infty$ be a sequence of elements in $H$. Show that following two statements are equivalent $$ \forall x \in H , \ \left< x, y_n \right> \to 0, \ n \to \infty $$ $$ \forall m \in \mathbb{N}, \left< x_m, y_n \right> \to 0, \ n \to \infty \text{ and } \{ || y_n ||_H\}_{n=1}^\infty \text{ is bounded}$$ If we assume the first statement then clearly $x_m \in H$ so the first part of the second statement is shown. For the second part of the second statement define a family of operator of the form $$ T_n (x) = \left< x, y_n \right>$$ Clearly the mapping is linear, and by Cauchy-Schwarz $$ |T_n (x)| \leq || x ||_H || y_n||_H$$ So the operator is bounded and the norm of the operator is $$ || T_n ||= || y_n ||_H $$ Applying the principle of uniform boundedness to this family of operators we find that $$ \exists M > 0, \ \forall n \in \mathbb{N}, \  || T_n || \leq M$$ Since the operator norm coincided with the norm of each $y_n$ we may conclude that the sequence is bounded by $M$. Assume the second statement. Fix $x \in H$ a computation shows $$ \left< x, y_n \right> = \sum_{m=1}^\infty \left< x_m, x \right> \left< x_m, y_n \right>$$ Now I would like to ""pass the limit"" into the sum but I am unsure how to do this. We can also write $y_n$ in the form $$y_n = \sum_{m=1}^\infty \left< x_m, y_n \right> x _m$$ Again ""passing the limit"" into the sum would actually give me that $y_n$ converges to $0$ but I cannot make either statement rigorous. Edit : Here is an earlier idea I tried to implement. We have $$| \left< x, y_n \right> | \leq \sum_{m=1}^\infty | \left< x_m, x \right> |  |\left< x_m, y_n \right> | $$ The series on the right converges, so its tail-end will converge to $0$. In particular for $\varepsilon > 0$ there exists $M > 0$ such that $$ \sum_{m=1}^\infty | \left< x_m, x \right> |  |\left< x_m, y_n \right> | \leq \sum_{m=1}^{M - 1} | \left< x_m, x \right> |  |\left< x_m, y_n \right> | + \frac{\varepsilon}{2} $$ Now by the first hypothesis for $m \in \{1,...,M-1 \}$ there exists $n_m \in \mathbb{N}$ such that for $n \geq n_m$ we have $$ |\left< x_m, y_n \right>| \leq \frac{1}{|\left< x_m, x \right>| + 1} \frac{\varepsilon}{2(M - 1)}$$ Now if we choose $N = \max \{ n_m : m \in \{ 1,..., M - 1 \}\}$ then for $n \geq N$ after a simple computation we have $$ | \left< x, y_n \right> | \leq \varepsilon$$ I was hesitant to post this idea since I did not use the boundedness of the sequence $y_n$ anywhere.","Let $H$ be a Hilbert space with an orthonormal basis  $\{ x_n \}_{n=1}^\infty$ and let $\{ y_n \}_{n=1}^\infty$ be a sequence of elements in $H$. Show that following two statements are equivalent $$ \forall x \in H , \ \left< x, y_n \right> \to 0, \ n \to \infty $$ $$ \forall m \in \mathbb{N}, \left< x_m, y_n \right> \to 0, \ n \to \infty \text{ and } \{ || y_n ||_H\}_{n=1}^\infty \text{ is bounded}$$ If we assume the first statement then clearly $x_m \in H$ so the first part of the second statement is shown. For the second part of the second statement define a family of operator of the form $$ T_n (x) = \left< x, y_n \right>$$ Clearly the mapping is linear, and by Cauchy-Schwarz $$ |T_n (x)| \leq || x ||_H || y_n||_H$$ So the operator is bounded and the norm of the operator is $$ || T_n ||= || y_n ||_H $$ Applying the principle of uniform boundedness to this family of operators we find that $$ \exists M > 0, \ \forall n \in \mathbb{N}, \  || T_n || \leq M$$ Since the operator norm coincided with the norm of each $y_n$ we may conclude that the sequence is bounded by $M$. Assume the second statement. Fix $x \in H$ a computation shows $$ \left< x, y_n \right> = \sum_{m=1}^\infty \left< x_m, x \right> \left< x_m, y_n \right>$$ Now I would like to ""pass the limit"" into the sum but I am unsure how to do this. We can also write $y_n$ in the form $$y_n = \sum_{m=1}^\infty \left< x_m, y_n \right> x _m$$ Again ""passing the limit"" into the sum would actually give me that $y_n$ converges to $0$ but I cannot make either statement rigorous. Edit : Here is an earlier idea I tried to implement. We have $$| \left< x, y_n \right> | \leq \sum_{m=1}^\infty | \left< x_m, x \right> |  |\left< x_m, y_n \right> | $$ The series on the right converges, so its tail-end will converge to $0$. In particular for $\varepsilon > 0$ there exists $M > 0$ such that $$ \sum_{m=1}^\infty | \left< x_m, x \right> |  |\left< x_m, y_n \right> | \leq \sum_{m=1}^{M - 1} | \left< x_m, x \right> |  |\left< x_m, y_n \right> | + \frac{\varepsilon}{2} $$ Now by the first hypothesis for $m \in \{1,...,M-1 \}$ there exists $n_m \in \mathbb{N}$ such that for $n \geq n_m$ we have $$ |\left< x_m, y_n \right>| \leq \frac{1}{|\left< x_m, x \right>| + 1} \frac{\varepsilon}{2(M - 1)}$$ Now if we choose $N = \max \{ n_m : m \in \{ 1,..., M - 1 \}\}$ then for $n \geq N$ after a simple computation we have $$ | \left< x, y_n \right> | \leq \varepsilon$$ I was hesitant to post this idea since I did not use the boundedness of the sequence $y_n$ anywhere.",,"['functional-analysis', 'hilbert-spaces']"
26,Why is the Maximum in the Min-Max Principle for Self-Adjoint Operators attained?,Why is the Maximum in the Min-Max Principle for Self-Adjoint Operators attained?,,"Let's consider a self-adjoint operator $A$ (not necessarily bounded) on a Hilbert space which is bounded from below, with domain $D$ and whose resolvent is compact. Then, the spectrum consists solely of isolated eigenvalues which are given (in increasing order) by the min-max principle: \begin{equation} \lambda_k = \min_{\substack{V \subset D\\ \dim V  = k}} \max_{\substack{x \in V \\ x \neq 0}} \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}, \ k \in \mathbb{N}. \end{equation} The proof I know shows $\lambda_k \geq \min \max \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}$ and $\lambda_k \leq \min \max \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}$ by using a orthonormal basis of eigenvectors. But how can we really write ""min"" and ""max"" instead of ""inf"" and ""sup"", i.e. why is the minimum and maximum really attained? Does anybody have a proof or a source for this assertion? Edit: I asked the question for the minimum separately since I want the possibility to start a bounty there and accept the answer here at the same time. See here: Why is the Minimum in the Min-Max Principle for Self-Adjoint Operators attained? .","Let's consider a self-adjoint operator $A$ (not necessarily bounded) on a Hilbert space which is bounded from below, with domain $D$ and whose resolvent is compact. Then, the spectrum consists solely of isolated eigenvalues which are given (in increasing order) by the min-max principle: \begin{equation} \lambda_k = \min_{\substack{V \subset D\\ \dim V  = k}} \max_{\substack{x \in V \\ x \neq 0}} \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}, \ k \in \mathbb{N}. \end{equation} The proof I know shows $\lambda_k \geq \min \max \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}$ and $\lambda_k \leq \min \max \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}$ by using a orthonormal basis of eigenvectors. But how can we really write ""min"" and ""max"" instead of ""inf"" and ""sup"", i.e. why is the minimum and maximum really attained? Does anybody have a proof or a source for this assertion? Edit: I asked the question for the minimum separately since I want the possibility to start a bounty there and accept the answer here at the same time. See here: Why is the Minimum in the Min-Max Principle for Self-Adjoint Operators attained? .",,"['functional-analysis', 'spectral-theory']"
27,All nonzero singular values of $A$ are equal to $1$ iff $A^*=A^*AA^*$ and $A=AA^*A$,All nonzero singular values of  are equal to  iff  and,A 1 A^*=A^*AA^* A=AA^*A,"I want to show that all the non-zero s-numbers, i.e. singular values $s_j(A):=(\lambda_j(A^*A))^{1/2}$, of A (a bounded linear operator of finite rank acting on a separable Hilbert space $H$) are equal to 1 if and only if $A^*=A^*AA^*$ and $A=AA^*A$. $A$ is an operator of finite rank, hence it is compact. So, I can use the following proposition: If $A:H\rightarrow H$ is a compact operator and $B,C:H\rightarrow H$ are bounded and linear, then   $$ s_j(BAC)\le \|B\|\|C\|s_j(A). $$ Thus, $$ s_j(A^*)=s_j(A^*AA^*)\le \|A^*\|^2s_j(A). $$ $A$ is compact, therefore $A^*$ is compact. So, $$ s_j(A)=s_j(AA^*A)\le \|A\|^2s_j(A^*). $$ Here I get stuck. I have no idea how to prove this. Any hints are appreciated.","I want to show that all the non-zero s-numbers, i.e. singular values $s_j(A):=(\lambda_j(A^*A))^{1/2}$, of A (a bounded linear operator of finite rank acting on a separable Hilbert space $H$) are equal to 1 if and only if $A^*=A^*AA^*$ and $A=AA^*A$. $A$ is an operator of finite rank, hence it is compact. So, I can use the following proposition: If $A:H\rightarrow H$ is a compact operator and $B,C:H\rightarrow H$ are bounded and linear, then   $$ s_j(BAC)\le \|B\|\|C\|s_j(A). $$ Thus, $$ s_j(A^*)=s_j(A^*AA^*)\le \|A^*\|^2s_j(A). $$ $A$ is compact, therefore $A^*$ is compact. So, $$ s_j(A)=s_j(AA^*A)\le \|A\|^2s_j(A^*). $$ Here I get stuck. I have no idea how to prove this. Any hints are appreciated.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'operator-algebras', 'compact-operators']"
28,Is it natural how $L^p$ spaces measure local and global sizes the same?,Is it natural how  spaces measure local and global sizes the same?,L^p,"This is a continuation of my question Spaces of functions similar to $L^p$ but with different local and global sizes . I have been bothered by the fact that the $L^p$ norm on $\mathbb R^n$, which is ubiquitous in real analysis, measures the local and global sizes of a function using the same exponent. As a result, there are functions (eg. $f(x)=|x|^{-a}$ for $a>0$) that are in some $L^p$ when you restrict them to $\{|f|\leq \lambda\}$, and some $L^q$ when you restrict them to $\{|f|>\lambda\}$, but are not in any $L^r$ on all of $\mathbb R^n$ because necessarily $p>q$. I intuitively feel like the local and global $L^p$ness of a function have nothing to do with each other, and therefore it is natural to measure them with separate exponents. In my previous post, sandwich pointed out that Wiener amalgam spaces do something like this. Nonetheless, these are not very commonly encountered spaces (as seen by the Wikipedia article, which is a stub). This motivates my question: why have usual $L^p$ spaces been so successful? Am I missing something very natural about measuring the local and global parts of a function with the same $p$? Or perhaps these amalgam warrant more attention than they've seen.","This is a continuation of my question Spaces of functions similar to $L^p$ but with different local and global sizes . I have been bothered by the fact that the $L^p$ norm on $\mathbb R^n$, which is ubiquitous in real analysis, measures the local and global sizes of a function using the same exponent. As a result, there are functions (eg. $f(x)=|x|^{-a}$ for $a>0$) that are in some $L^p$ when you restrict them to $\{|f|\leq \lambda\}$, and some $L^q$ when you restrict them to $\{|f|>\lambda\}$, but are not in any $L^r$ on all of $\mathbb R^n$ because necessarily $p>q$. I intuitively feel like the local and global $L^p$ness of a function have nothing to do with each other, and therefore it is natural to measure them with separate exponents. In my previous post, sandwich pointed out that Wiener amalgam spaces do something like this. Nonetheless, these are not very commonly encountered spaces (as seen by the Wikipedia article, which is a stub). This motivates my question: why have usual $L^p$ spaces been so successful? Am I missing something very natural about measuring the local and global parts of a function with the same $p$? Or perhaps these amalgam warrant more attention than they've seen.",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
29,Equivalence of norm and weak topologies in finite dimensional space.,Equivalence of norm and weak topologies in finite dimensional space.,,"I haven't solved a lot of problems on functional analysis, so I don't have intuition whether my idea of proof is correct or not. The weakness of weak-topology is obvious from the definition. To show another direction of the proof take an open ball $B(a,\epsilon )$ in norm topology. Since all norms are equivalent in finite dimensional spaces this could be a usual ball with standart metric. Let $y\in B(a, \epsilon)$ and $f:X\to \mathbb{R}$ s.t. $f(e)=1$ for any basis element of vector space. Then I can find $(u,v) \subset \mathbb{R}$ s.t. $y$ lies in weakly open set $f^{-1}(u,v)\subset B(a, \epsilon )$. I know that it is not a rigorous proof(if it is indeed a proof). I'm asking about correctness of idea itself.","I haven't solved a lot of problems on functional analysis, so I don't have intuition whether my idea of proof is correct or not. The weakness of weak-topology is obvious from the definition. To show another direction of the proof take an open ball $B(a,\epsilon )$ in norm topology. Since all norms are equivalent in finite dimensional spaces this could be a usual ball with standart metric. Let $y\in B(a, \epsilon)$ and $f:X\to \mathbb{R}$ s.t. $f(e)=1$ for any basis element of vector space. Then I can find $(u,v) \subset \mathbb{R}$ s.t. $y$ lies in weakly open set $f^{-1}(u,v)\subset B(a, \epsilon )$. I know that it is not a rigorous proof(if it is indeed a proof). I'm asking about correctness of idea itself.",,"['functional-analysis', 'proof-verification', 'weak-convergence']"
30,A convex subset of a Hilbert space,A convex subset of a Hilbert space,,"Assume $C$ is a convex subset of a Hilbert space $H$ ($C$ is not necessarily close) and $x_0\notin C$.Let $r=d(x_0,C)$. Prove that $\{y\in H\mid\|y-x_0\|\leq r\}\cap C$Has at most 1 element. I want to use this post to show that for:$\|x-x_0\|=\|y-x_0\|=r$ We can say $\|\frac{x+y}{2}-x_0\|=\|\frac{x-x_0+y-x_0}{2}\| <r$ (i remember something similar from class) But this is as far as i could get. Any ideas regarding this approach?","Assume $C$ is a convex subset of a Hilbert space $H$ ($C$ is not necessarily close) and $x_0\notin C$.Let $r=d(x_0,C)$. Prove that $\{y\in H\mid\|y-x_0\|\leq r\}\cap C$Has at most 1 element. I want to use this post to show that for:$\|x-x_0\|=\|y-x_0\|=r$ We can say $\|\frac{x+y}{2}-x_0\|=\|\frac{x-x_0+y-x_0}{2}\| <r$ (i remember something similar from class) But this is as far as i could get. Any ideas regarding this approach?",,"['functional-analysis', 'hilbert-spaces']"
31,Operator norm of an identity map over $l_p$ space,Operator norm of an identity map over  space,l_p,Let $1 \leq p < q \leq \infty$ ($p$ and $q$ are not related) conclude that the identity map I : $ l^n_p → l^n_q$ has operator norm exactly 1. I figured I need to show that given $\|Ix\| \leq c\|x\|$ from the wiki definition the infimum over c is 1 so that $\|Ix\| = \|x\|$. is this a correct definition for the operator norm? and is it a good approach? Can i apply Riesz–Thorin theorem here? if so how?,Let $1 \leq p < q \leq \infty$ ($p$ and $q$ are not related) conclude that the identity map I : $ l^n_p → l^n_q$ has operator norm exactly 1. I figured I need to show that given $\|Ix\| \leq c\|x\|$ from the wiki definition the infimum over c is 1 so that $\|Ix\| = \|x\|$. is this a correct definition for the operator norm? and is it a good approach? Can i apply Riesz–Thorin theorem here? if so how?,,"['functional-analysis', 'lp-spaces']"
32,Is the space of $\mathbb{R}\to\mathbb{R}$ more huge than the space of all discrete functions?,Is the space of  more huge than the space of all discrete functions?,\mathbb{R}\to\mathbb{R},"Assume that we have some general discrete function ${0,1,2,....n}\to\mathbb{R}$. For each number I have a real value. Let's infinite increase number $n$ (which is integer value), to approximate $\mathbb{R}\to\mathbb{R}$ function. As I understand it is only approximation even in limit, isn't it? Even if  I cover all integer values in the limit is cool, but we still have that $\mathbb{N}\ne\mathbb{R}$ ( Cantor's Diagonal argument ) So the space of $\mathbb{R}\to\mathbb{R}$ is more huge then the space of all discrete functions? Isn't it? (p.s. my background: I'm not familiar with functional-analysis)","Assume that we have some general discrete function ${0,1,2,....n}\to\mathbb{R}$. For each number I have a real value. Let's infinite increase number $n$ (which is integer value), to approximate $\mathbb{R}\to\mathbb{R}$ function. As I understand it is only approximation even in limit, isn't it? Even if  I cover all integer values in the limit is cool, but we still have that $\mathbb{N}\ne\mathbb{R}$ ( Cantor's Diagonal argument ) So the space of $\mathbb{R}\to\mathbb{R}$ is more huge then the space of all discrete functions? Isn't it? (p.s. my background: I'm not familiar with functional-analysis)",,"['functional-analysis', 'functions']"
33,"Itō formula as presented in ""Stochastic Equations in Infinite Dimensions"" by Giuseppe Da Prato","Itō formula as presented in ""Stochastic Equations in Infinite Dimensions"" by Giuseppe Da Prato",,"In Stochastic Equations in Infinite Dimensions , Theorem 4.32 (Google Books) , the authors present the following version of an Itō formula: Given Hilbert spaces $(U,\langle\;\cdot\;,\;\cdot\;\rangle_U)$ and $(H,\langle\;\cdot\;,\;\cdot\;\rangle)$, a $U$-valued Brownian motion $(W_t)_{t\ge 0}$ and $$X_t=X_0+\int_0^t\varphi_s\;{\rm d}s+\int_0^t\Phi_s\;{\rm d}W_s\tag 1$$ for some $H$-valued random variable $X_0$, $H$-valued stochastic process $(\varphi_t)_{t\ge 0}$ and $\mathfrak L(U,H)$-valued$^1$ stochastic process $(\Phi_t)_{t\ge 0}$, we've got \begin{equation} \begin{split} f(t,X_t)-f(0,X_0)&=\color{red}{\int_0^t\langle\Phi_s{\rm d}W_s,F_x(s,X_s)\rangle}\\ &\quad\color{blue}{+\text{something unimportant for this question}} \end{split}\tag 2 \end{equation} for all $F:[0,\infty)\times H\to\mathbb R$ with partial Fréchet derivatives $F_t$, $F_x$ and $F_{xx}$. Question : What's the definition of the $\color{red}{\text{red}}$ term? (They don't give one in the book). The proof of the statement can be reduced to the case $\varphi_t=\varphi_0$ and $\Phi_t=\Phi_0$. If $0=t_0<\cdots<t_n=t$ is a partition of $[0,t]$, Taylor's theorem yields$^2$ \begin{equation} \begin{split} f(t,X_t)-f(0,X_0)&=\color{red}{\sum_{i=1}^n\langle\Delta X_i,L_i\rangle}\\ &\quad\color{blue}{+\text{something unimportant for this question}} \end{split}\tag 3 \end{equation} where $\Delta t_i=t_i-t_{i-1}$, $\Delta X_i=X_{t_i}-X_{t_{i-1}}$ and $$L_i:=F_x(t_{i-1},X_{t_{i-1}})\;.$$ Using $(1)$ and our assumption, the $\color{red}{\text{red}}$ term in $(3)$ is $$\color{red}{\sum_{i=1}^n\langle\Phi_0\Delta W_i,L_i\rangle}\color{blue}{+\sum_{i=1}^n\Delta t_i\langle\varphi_0,L_i\rangle}\tag 4\;.$$ Using the definition of the adjoint operator, the $\color{red}{\text{red}}$ term in $(4)$ is $$\sum_{i=1}^n\langle\Delta W_i,\Phi_0^\ast L_i\rangle_U\;.\tag 5$$ In the middle of page 108 they state (our $\color{red}{\text{red}}$ term from $(3)$ is called $I_2$ there) that $(5)$ converges almost surely to $$\int_0^t\langle\Phi_s{\rm d}W_s,F_x(s,X_s)\rangle\tag 6$$ for $n\to\infty$. Why? $^1$ Let $\mathfrak L(A,B)$ be the space of bounded, linear operators $A\to B$. $^2$ Notice that we can make sense of $(3)$, since $L_i\in\mathfrak L(H,\mathbb R)\cong H$ by Riesz' representation theorem .","In Stochastic Equations in Infinite Dimensions , Theorem 4.32 (Google Books) , the authors present the following version of an Itō formula: Given Hilbert spaces $(U,\langle\;\cdot\;,\;\cdot\;\rangle_U)$ and $(H,\langle\;\cdot\;,\;\cdot\;\rangle)$, a $U$-valued Brownian motion $(W_t)_{t\ge 0}$ and $$X_t=X_0+\int_0^t\varphi_s\;{\rm d}s+\int_0^t\Phi_s\;{\rm d}W_s\tag 1$$ for some $H$-valued random variable $X_0$, $H$-valued stochastic process $(\varphi_t)_{t\ge 0}$ and $\mathfrak L(U,H)$-valued$^1$ stochastic process $(\Phi_t)_{t\ge 0}$, we've got \begin{equation} \begin{split} f(t,X_t)-f(0,X_0)&=\color{red}{\int_0^t\langle\Phi_s{\rm d}W_s,F_x(s,X_s)\rangle}\\ &\quad\color{blue}{+\text{something unimportant for this question}} \end{split}\tag 2 \end{equation} for all $F:[0,\infty)\times H\to\mathbb R$ with partial Fréchet derivatives $F_t$, $F_x$ and $F_{xx}$. Question : What's the definition of the $\color{red}{\text{red}}$ term? (They don't give one in the book). The proof of the statement can be reduced to the case $\varphi_t=\varphi_0$ and $\Phi_t=\Phi_0$. If $0=t_0<\cdots<t_n=t$ is a partition of $[0,t]$, Taylor's theorem yields$^2$ \begin{equation} \begin{split} f(t,X_t)-f(0,X_0)&=\color{red}{\sum_{i=1}^n\langle\Delta X_i,L_i\rangle}\\ &\quad\color{blue}{+\text{something unimportant for this question}} \end{split}\tag 3 \end{equation} where $\Delta t_i=t_i-t_{i-1}$, $\Delta X_i=X_{t_i}-X_{t_{i-1}}$ and $$L_i:=F_x(t_{i-1},X_{t_{i-1}})\;.$$ Using $(1)$ and our assumption, the $\color{red}{\text{red}}$ term in $(3)$ is $$\color{red}{\sum_{i=1}^n\langle\Phi_0\Delta W_i,L_i\rangle}\color{blue}{+\sum_{i=1}^n\Delta t_i\langle\varphi_0,L_i\rangle}\tag 4\;.$$ Using the definition of the adjoint operator, the $\color{red}{\text{red}}$ term in $(4)$ is $$\sum_{i=1}^n\langle\Delta W_i,\Phi_0^\ast L_i\rangle_U\;.\tag 5$$ In the middle of page 108 they state (our $\color{red}{\text{red}}$ term from $(3)$ is called $I_2$ there) that $(5)$ converges almost surely to $$\int_0^t\langle\Phi_s{\rm d}W_s,F_x(s,X_s)\rangle\tag 6$$ for $n\to\infty$. Why? $^1$ Let $\mathfrak L(A,B)$ be the space of bounded, linear operators $A\to B$. $^2$ Notice that we can make sense of $(3)$, since $L_i\in\mathfrak L(H,\mathbb R)\cong H$ by Riesz' representation theorem .",,"['functional-analysis', 'stochastic-processes', 'operator-theory', 'stochastic-integrals', 'stochastic-analysis']"
34,What is the norm of the dual space $H^1(\Omega)'$?,What is the norm of the dual space ?,H^1(\Omega)',"I am working on the Bidomain-Model which, during a time interval [0,T], describes the electrical behaviour of the myocardial muscle considered as $\Omega \subset \mathbb{R}^3$. This model has partial differential equations which involve the intra- and extracellular voltages, $u$ and $v$, which are functions of this type:  $$ u=u(t,x):[0,T]\times\Omega\rightarrow\mathbb{R}$$ $$ v=v(t,x):[0,T]\times\Omega\rightarrow\mathbb{R}$$  Now, as functions of time, $$ u,v \in X=L^2\left(0,T,H^1(\Omega)\right)$$ with norm $\| u \|_{_{X}} = \left(\int_{_{[0,T]}}\| u(t) \|^{^2}_{_{H^1(\Omega)}} dt \right)^{1/2}$. Their time derivatives $$ \dot u,\dot v \in Y=L^2\left(0,T,H^1(\Omega)' \right)$$ with norm $\| u \|_{_{Y}} = \left(\int_{_{[0,T]}}\| u(t) \|^{^2}_{_{H^1(\Omega)'}} dt \right)^{1/2}$. Further, for any fixed $t_0 \in [0,T]$,  $$\| u(t_0) \|_{_{H^1(\Omega)}}^{^2} = \| \dot u(t_0,x) \|_{_{L^2(\Omega)}}^{^2} + \| u(t_0,x) \|_{_{L^2(\Omega)}}^{^2} = \langle u(t_0,x), u(t_0,x) \rangle_{_{H^1(\Omega)}}$$ but Which is $\| u(t_0,x) \|_{_{H^1(\Omega)'}}$? Is it $$\|u(t_0)\|_{_{H^1(\Omega)'}}^{^2} = \sup_{\xi \in H^1(\Omega)} \left\{ \langle  u(t_0,x), \xi(t_0,x) \rangle_{_{H^1(\Omega)}} \, | \, \| \xi(t_0,x)\|_{_{H^1(\Omega)}}^{^2} \le 1 \right\}$$   as this post suggests?","I am working on the Bidomain-Model which, during a time interval [0,T], describes the electrical behaviour of the myocardial muscle considered as $\Omega \subset \mathbb{R}^3$. This model has partial differential equations which involve the intra- and extracellular voltages, $u$ and $v$, which are functions of this type:  $$ u=u(t,x):[0,T]\times\Omega\rightarrow\mathbb{R}$$ $$ v=v(t,x):[0,T]\times\Omega\rightarrow\mathbb{R}$$  Now, as functions of time, $$ u,v \in X=L^2\left(0,T,H^1(\Omega)\right)$$ with norm $\| u \|_{_{X}} = \left(\int_{_{[0,T]}}\| u(t) \|^{^2}_{_{H^1(\Omega)}} dt \right)^{1/2}$. Their time derivatives $$ \dot u,\dot v \in Y=L^2\left(0,T,H^1(\Omega)' \right)$$ with norm $\| u \|_{_{Y}} = \left(\int_{_{[0,T]}}\| u(t) \|^{^2}_{_{H^1(\Omega)'}} dt \right)^{1/2}$. Further, for any fixed $t_0 \in [0,T]$,  $$\| u(t_0) \|_{_{H^1(\Omega)}}^{^2} = \| \dot u(t_0,x) \|_{_{L^2(\Omega)}}^{^2} + \| u(t_0,x) \|_{_{L^2(\Omega)}}^{^2} = \langle u(t_0,x), u(t_0,x) \rangle_{_{H^1(\Omega)}}$$ but Which is $\| u(t_0,x) \|_{_{H^1(\Omega)'}}$? Is it $$\|u(t_0)\|_{_{H^1(\Omega)'}}^{^2} = \sup_{\xi \in H^1(\Omega)} \left\{ \langle  u(t_0,x), \xi(t_0,x) \rangle_{_{H^1(\Omega)}} \, | \, \| \xi(t_0,x)\|_{_{H^1(\Omega)}}^{^2} \le 1 \right\}$$   as this post suggests?",,"['functional-analysis', 'hilbert-spaces', 'normed-spaces']"
35,spectrum of unbounded self-adjoint operators,spectrum of unbounded self-adjoint operators,,"I'm self-studying Lax's functional analysis, and I'm stuck in the chapter introducing spectral theory for unbounded self-adjoint operators. In his book, Lax proved the spectral theorem of this version in p.378 using Nevanlinna's lemma, or Tao called Herglotz representation theorem : Let $A$ be a self-adjoint operator in a Hilbert space $H$; denote the domain of $A$ by $D$. There is a spectral resolution for $A$, that is, orthogonal projection-valued measure $E$ defined for all Borel measurable subsets of $R$, with the following properties: $E(\emptyset)=0, E(\mathbb{R})=1$ $E(S\cap T)=E(S)E(T)$ $E^*=E$ $E$ commutes with $A$ $D=\left\{u\in H \mid \int t^2dE(t)u<\infty \right\}$ I'm fairly well following the proof given in this book, but I found out several points I cannot make clear about. My question is: It seems to me that Lax does not completely prove the theorem; at the final step he just showed $D\subseteq \left\{u\in H \mid \int t^2dE(t)u<\infty \right\}$. But some cross validation, like Tao's post , says inverse direction is also true, and I cannot fill in the missing proof for myself. In p.390 in Lax, he give an exercise showing unbounded self-adjoint operator has closed unbounded subset of $\mathbb{R}$ as the spectrum. I succeed in showing that the spectrum is a closed subset of the real line(it seems to be exactly the same as in bounded case), but failed to show the unboundedness. Any hint or reference would be really appreciated. I've came up with an idea about 2nd question that if $\sigma(A)$ is bounded, say $|\sigma(A)|<M$, then $A=\int_{\sigma(A)} tdE(t)$ has bounded norm $|A|<\int_{-M}^M |t|dt=M^2$, which is contradiction. Is my argument make sense?","I'm self-studying Lax's functional analysis, and I'm stuck in the chapter introducing spectral theory for unbounded self-adjoint operators. In his book, Lax proved the spectral theorem of this version in p.378 using Nevanlinna's lemma, or Tao called Herglotz representation theorem : Let $A$ be a self-adjoint operator in a Hilbert space $H$; denote the domain of $A$ by $D$. There is a spectral resolution for $A$, that is, orthogonal projection-valued measure $E$ defined for all Borel measurable subsets of $R$, with the following properties: $E(\emptyset)=0, E(\mathbb{R})=1$ $E(S\cap T)=E(S)E(T)$ $E^*=E$ $E$ commutes with $A$ $D=\left\{u\in H \mid \int t^2dE(t)u<\infty \right\}$ I'm fairly well following the proof given in this book, but I found out several points I cannot make clear about. My question is: It seems to me that Lax does not completely prove the theorem; at the final step he just showed $D\subseteq \left\{u\in H \mid \int t^2dE(t)u<\infty \right\}$. But some cross validation, like Tao's post , says inverse direction is also true, and I cannot fill in the missing proof for myself. In p.390 in Lax, he give an exercise showing unbounded self-adjoint operator has closed unbounded subset of $\mathbb{R}$ as the spectrum. I succeed in showing that the spectrum is a closed subset of the real line(it seems to be exactly the same as in bounded case), but failed to show the unboundedness. Any hint or reference would be really appreciated. I've came up with an idea about 2nd question that if $\sigma(A)$ is bounded, say $|\sigma(A)|<M$, then $A=\int_{\sigma(A)} tdE(t)$ has bounded norm $|A|<\int_{-M}^M |t|dt=M^2$, which is contradiction. Is my argument make sense?",,"['functional-analysis', 'spectral-theory']"
36,Linear convergence of sequences of inverse functions,Linear convergence of sequences of inverse functions,,"Let $f, f_n: \mathbb{R}^m \rightarrow \mathbb{R}^m$ such that $f^{-1}, f_n^{-1}$ exist and are globally Lipschitz continuous for all $n$ (same constant $L$). Assume that $f_n := f + \frac{1}{n} Id$. Then $\left( f_n \right)^{-1} \rightarrow f^{-1}$ uniformly on every compact set. Prove or disprove that $\left\| \left( f_n \right)^{-1} - \left( f \right)^{-1}\right\| \leq \frac{c}{n}$, for some $c>0$. Comments: $\left( f_n \right)^{-1} \rightarrow f^{-1}$ follows from the Lipschitz continuity property, see: Convergence of sequences of inverse functions . However, that does not exploit the fact that $\left( f_n \right)^{-1} = \left( f + \frac{1}{n} Id \right)^{-1}$. Attempt: If we take $x = \left( f + \frac{1}{n}Id \right) (y) = f(y) + \frac{1}{n} y$, then  $\left\| \left( f + \frac{1}{n}Id \right)^{-1}(x) - f^{-1}(x)  \right\| =  \left\| y - f^{-1}\left( f(y) + \frac{1}{n} y \right)  \right\| =  \left\| f^{-1}\left( f(y) \right) - f^{-1}\left( f(y) + \frac{1}{n} y \right)  \right\| \leq L \left\| f(y)  - f(y) + \frac{1}{n} y \right\| = L \left\|  \frac{1}{n} y \right\| = \frac{L}{n} \left\| y \right\|$. So the statement holds if $\left\| y \right\|$ is bounded, i.e. if the domain of $f$ is bounded. Otherwise it seems the statement may be false.","Let $f, f_n: \mathbb{R}^m \rightarrow \mathbb{R}^m$ such that $f^{-1}, f_n^{-1}$ exist and are globally Lipschitz continuous for all $n$ (same constant $L$). Assume that $f_n := f + \frac{1}{n} Id$. Then $\left( f_n \right)^{-1} \rightarrow f^{-1}$ uniformly on every compact set. Prove or disprove that $\left\| \left( f_n \right)^{-1} - \left( f \right)^{-1}\right\| \leq \frac{c}{n}$, for some $c>0$. Comments: $\left( f_n \right)^{-1} \rightarrow f^{-1}$ follows from the Lipschitz continuity property, see: Convergence of sequences of inverse functions . However, that does not exploit the fact that $\left( f_n \right)^{-1} = \left( f + \frac{1}{n} Id \right)^{-1}$. Attempt: If we take $x = \left( f + \frac{1}{n}Id \right) (y) = f(y) + \frac{1}{n} y$, then  $\left\| \left( f + \frac{1}{n}Id \right)^{-1}(x) - f^{-1}(x)  \right\| =  \left\| y - f^{-1}\left( f(y) + \frac{1}{n} y \right)  \right\| =  \left\| f^{-1}\left( f(y) \right) - f^{-1}\left( f(y) + \frac{1}{n} y \right)  \right\| \leq L \left\| f(y)  - f(y) + \frac{1}{n} y \right\| = L \left\|  \frac{1}{n} y \right\| = \frac{L}{n} \left\| y \right\|$. So the statement holds if $\left\| y \right\|$ is bounded, i.e. if the domain of $f$ is bounded. Otherwise it seems the statement may be false.",,"['real-analysis', 'analysis', 'functional-analysis', 'limits', 'inverse-function']"
37,$x^*\circ f:G\rightarrow \Bbb{C}$ is analytic. Show $f$ is analytic.,is analytic. Show  is analytic.,x^*\circ f:G\rightarrow \Bbb{C} f,"This is an exercise in Conway's 《A course in Functional Analysis》. X is a complex Banach space. $G$ is an open set in the complex plane and $f:G\rightarrow X$ is a function such that for each $x^{*}$ in $X^{*}$,dual space of $X$, $x^*\circ f:G\rightarrow \Bbb{C}$ is analytic. Show $f$ is analytic. Any help would be appreciated.","This is an exercise in Conway's 《A course in Functional Analysis》. X is a complex Banach space. $G$ is an open set in the complex plane and $f:G\rightarrow X$ is a function such that for each $x^{*}$ in $X^{*}$,dual space of $X$, $x^*\circ f:G\rightarrow \Bbb{C}$ is analytic. Show $f$ is analytic. Any help would be appreciated.",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'banach-spaces']"
38,Direct sum decomposition of $L^2(\mathbb{R})$ using Fourier Transform,Direct sum decomposition of  using Fourier Transform,L^2(\mathbb{R}),"Let $L_+^2(\mathbb{R})=\{f\in L^2(\mathbb{R}):supp \hat{f}\subset\mathbb{R^+}\}$ and $L_-^2(\mathbb{R})=\{f\in L^2(\mathbb{R}):supp \hat{f}\subset\mathbb{R^-}\}$, where $\hat{f}$ denotes the Fourier trans form of $f$ in $\mathbb{R}$.  $\mathbb{R^+}$ and $\mathbb{R^-}$ respectively denotes set of positive and negative reals. Then I need to show that $L^2(\mathbb{R})=L_+^2(\mathbb{R})\oplus L_- ^2(\mathbb{R})$. I observed that $L_+^2(\mathbb{R})$ and $ L_- ^2(\mathbb{R})$ are orthogonal. Please help me to break   $f=f_1+f_2$ where $f_1\in L_+^2(\mathbb{R})$ and $f_2\in L_-^2(\mathbb{R})$","Let $L_+^2(\mathbb{R})=\{f\in L^2(\mathbb{R}):supp \hat{f}\subset\mathbb{R^+}\}$ and $L_-^2(\mathbb{R})=\{f\in L^2(\mathbb{R}):supp \hat{f}\subset\mathbb{R^-}\}$, where $\hat{f}$ denotes the Fourier trans form of $f$ in $\mathbb{R}$.  $\mathbb{R^+}$ and $\mathbb{R^-}$ respectively denotes set of positive and negative reals. Then I need to show that $L^2(\mathbb{R})=L_+^2(\mathbb{R})\oplus L_- ^2(\mathbb{R})$. I observed that $L_+^2(\mathbb{R})$ and $ L_- ^2(\mathbb{R})$ are orthogonal. Please help me to break   $f=f_1+f_2$ where $f_1\in L_+^2(\mathbb{R})$ and $f_2\in L_-^2(\mathbb{R})$",,"['complex-analysis', 'functional-analysis', 'fourier-analysis', 'hilbert-spaces', 'harmonic-analysis']"
39,"If $Q$ is an operator on a Hilbert space with $Qe_n=λ_ne_n$ for all $n$, then $Q^{-\frac 12}e_n=\frac 1{\sqrt{λ_n}}e_n$ for all $n$ with $λ_n>0$","If  is an operator on a Hilbert space with  for all , then  for all  with",Q Qe_n=λ_ne_n n Q^{-\frac 12}e_n=\frac 1{\sqrt{λ_n}}e_n n λ_n>0,"Let $(U,\langle\;\cdot\;,\;\cdot\;\rangle)$ be a separable Hilbert space $\mathfrak L(U)$ be the set of bounded and linear operators on $U$ $Q\in\mathfrak L(U)$ be nonnegative and symmetric $(e_n)_{n\in\mathbb N}$ be an orthonormal basis of $U$ with $$Qe_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N\tag 1$$ for some $(\lambda_n)_{n\ge 0}\subseteq[0,\infty)$ We can prove, that for any nonnegative and symmetric $L\in\mathfrak L(U)$ there is exactly one nonnegative and symmetric $L^{\frac 12}\in\mathfrak L(U)$ with $$L^{\frac 12}L^{\frac 12}=L\;.$$ Now I would like to prove that $$Q^{\frac 12}e_n=\sqrt{\lambda_n}e_n\;\;\;\text{for all }n\in\mathbb N\tag 2$$ and $$Q^{-\frac 12}e_n=\frac 1{\sqrt{\lambda_n}}e_n\;\;\;\text{for all }n\in\mathbb N\text{ with }\lambda_n>0\tag 3\;,$$ where $Q^{-1}$ is the pseudoinverse of $Q$, i.e. $$Q^{-1}v:=\underset{u\in U}{\operatorname{arg min}}\left\{\left\|u\right\|:Qu=v\right\}\;\;\;\text{for }v\in Q(U)\;.$$ Unfortunately, I have no idea how I can prove $(2)$ and $(3)$ and I can't find any reference for such a statement. So, how can we prove it?","Let $(U,\langle\;\cdot\;,\;\cdot\;\rangle)$ be a separable Hilbert space $\mathfrak L(U)$ be the set of bounded and linear operators on $U$ $Q\in\mathfrak L(U)$ be nonnegative and symmetric $(e_n)_{n\in\mathbb N}$ be an orthonormal basis of $U$ with $$Qe_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N\tag 1$$ for some $(\lambda_n)_{n\ge 0}\subseteq[0,\infty)$ We can prove, that for any nonnegative and symmetric $L\in\mathfrak L(U)$ there is exactly one nonnegative and symmetric $L^{\frac 12}\in\mathfrak L(U)$ with $$L^{\frac 12}L^{\frac 12}=L\;.$$ Now I would like to prove that $$Q^{\frac 12}e_n=\sqrt{\lambda_n}e_n\;\;\;\text{for all }n\in\mathbb N\tag 2$$ and $$Q^{-\frac 12}e_n=\frac 1{\sqrt{\lambda_n}}e_n\;\;\;\text{for all }n\in\mathbb N\text{ with }\lambda_n>0\tag 3\;,$$ where $Q^{-1}$ is the pseudoinverse of $Q$, i.e. $$Q^{-1}v:=\underset{u\in U}{\operatorname{arg min}}\left\{\left\|u\right\|:Qu=v\right\}\;\;\;\text{for }v\in Q(U)\;.$$ Unfortunately, I have no idea how I can prove $(2)$ and $(3)$ and I can't find any reference for such a statement. So, how can we prove it?",,"['functional-analysis', 'eigenvalues-eigenvectors', 'operator-theory', 'hilbert-spaces']"
40,Composition involving bounded linear operators,Composition involving bounded linear operators,,"I recently come across the following statement mentioned in a proof: Let $X,Y$ be normed linear spaces and $T:X \rightarrow Y$ be a linear operator. if for every bounded linear functional $U: Y \rightarrow \mathbb{R}$ , $UT$ is bounded, then $T$ is also bounded. How can we justify this statement?","I recently come across the following statement mentioned in a proof: Let $X,Y$ be normed linear spaces and $T:X \rightarrow Y$ be a linear operator. if for every bounded linear functional $U: Y \rightarrow \mathbb{R}$ , $UT$ is bounded, then $T$ is also bounded. How can we justify this statement?",,"['analysis', 'functional-analysis', 'operator-theory', 'normed-spaces']"
41,How to indicate the space of sesquilinear forms?,How to indicate the space of sesquilinear forms?,,"Is there a canonical way to indicate the space of sesquilinear forms? For bilinear forms I can use the (0,2) Tensor space, but since sesquilinear forms are not linear in one variable I don't know how to indicate the space where they live. Does anybody have an idea? Thank you","Is there a canonical way to indicate the space of sesquilinear forms? For bilinear forms I can use the (0,2) Tensor space, but since sesquilinear forms are not linear in one variable I don't know how to indicate the space where they live. Does anybody have an idea? Thank you",,"['linear-algebra', 'functional-analysis', 'differential-geometry', 'riemannian-geometry', 'tensors']"
42,$\nabla^2(\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1})=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$ with distributions defined on Schwartz space,with distributions defined on Schwartz space,\nabla^2(\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1})=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0),"I know, from a recent enlightening answers received here , that, if we define the distribution represented by Dirac's $\delta$ on the space $K$ of test functions of class $C^\infty$ whose support is contained in a compact subset of $\mathbb{R}^3$, then$$\nabla^2\left(\frac{1}{\|\boldsymbol{x}-\boldsymbol{x}_0\|}\right)=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$$where the Laplacian obviously is to be intended in the sense of the derivatives of distributions. More explicitly, that means that $$\forall\varphi\in K\quad \int_{\mathbb{R}^3}f\,\nabla^2\varphi\,d\mu=-4\pi\varphi(\boldsymbol{x}_0)=:-4\pi\int_{\mathbb{R}^3}\delta(\boldsymbol{x}-\boldsymbol{x}_0)\varphi(\boldsymbol{x})$$where $f:\boldsymbol{x}\mapsto\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1}$ and the first integral is intended as a Lebesgue integral. I suspect that the identity may well also hold with $\varphi$ as a more generical function belonging to the Schwartz space, but I cannot generalise this excellent proof , to whose author I am immensely grateful, for the $\varphi\in K$. If the identity really holds with the distribution defined on the Schwartz space, how can it be proved? I $\infty$-ly thank any answerer.","I know, from a recent enlightening answers received here , that, if we define the distribution represented by Dirac's $\delta$ on the space $K$ of test functions of class $C^\infty$ whose support is contained in a compact subset of $\mathbb{R}^3$, then$$\nabla^2\left(\frac{1}{\|\boldsymbol{x}-\boldsymbol{x}_0\|}\right)=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$$where the Laplacian obviously is to be intended in the sense of the derivatives of distributions. More explicitly, that means that $$\forall\varphi\in K\quad \int_{\mathbb{R}^3}f\,\nabla^2\varphi\,d\mu=-4\pi\varphi(\boldsymbol{x}_0)=:-4\pi\int_{\mathbb{R}^3}\delta(\boldsymbol{x}-\boldsymbol{x}_0)\varphi(\boldsymbol{x})$$where $f:\boldsymbol{x}\mapsto\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1}$ and the first integral is intended as a Lebesgue integral. I suspect that the identity may well also hold with $\varphi$ as a more generical function belonging to the Schwartz space, but I cannot generalise this excellent proof , to whose author I am immensely grateful, for the $\varphi\in K$. If the identity really holds with the distribution defined on the Schwartz space, how can it be proved? I $\infty$-ly thank any answerer.",,"['functional-analysis', 'distribution-theory', 'dirac-delta', 'laplacian']"
43,The inverse of bounded operator?,The inverse of bounded operator?,,"Is the inverse of a bounded operator always bounded , if yes how to prove it ?","Is the inverse of a bounded operator always bounded , if yes how to prove it ?",,"['functional-analysis', 'inverse']"
44,What is the $H_0^1(\Omega)$ topology?,What is the  topology?,H_0^1(\Omega),"Let $\mathcal{V}$ be the space (without topology) $$\displaystyle \mathcal{V}=\{u\in C_0^\infty(\Omega)\mid \nabla\cdot u=0\}$$ where $\Omega$ is a nonempty open connected subset of $\mathbb{R}^n$ . It is said in the Navier-Stokes Equations by Temam that the closure of $\mathcal{V}$ in $L^2(\Omega)$ and in $H_0^1(\Omega)$ are two basic spaces in the study of the Navier-Stokes equations. While it is quite clear what the closure of $\mathcal{V}$ in $L^2(\Omega)$ means, I don’t quite understand later one. By definition, $H_0^1(\Omega)$ is $W_0^{1,2}(\Omega)$ which is defined as $$\displaystyle \overline{C_0^\infty(\Omega)}^{\|\|_{W^{1,2}(\Omega)}},$$ the closure of $C_0^\infty(\Omega)$ in the Sobolev space $W^{1,2}(\Omega)$ . Here are my questions : What is the convention for the topology of $H_0^1(\Omega)$ ? Is the same as the topology of $H^1(\Omega)$ so that the closure of $\mathcal{V}$ in $H_0^1(\Omega)$ is the same as the closure of $\mathcal{V}$ in $H^1(\Omega)$ ? Could anyone come up with a cited reference regarding the topology of $H_0^1$ ? [Added:] What really puzzles me is: if the closure of $\mathcal{V}$ in $H_0^1(\Omega)$ is the same as the closure of $\mathcal{V}$ in $H^1(\Omega)$ , why bother mentioning $H_0^1(\Omega)$ ?","Let be the space (without topology) where is a nonempty open connected subset of . It is said in the Navier-Stokes Equations by Temam that the closure of in and in are two basic spaces in the study of the Navier-Stokes equations. While it is quite clear what the closure of in means, I don’t quite understand later one. By definition, is which is defined as the closure of in the Sobolev space . Here are my questions : What is the convention for the topology of ? Is the same as the topology of so that the closure of in is the same as the closure of in ? Could anyone come up with a cited reference regarding the topology of ? [Added:] What really puzzles me is: if the closure of in is the same as the closure of in , why bother mentioning ?","\mathcal{V} \displaystyle \mathcal{V}=\{u\in C_0^\infty(\Omega)\mid \nabla\cdot u=0\} \Omega \mathbb{R}^n \mathcal{V} L^2(\Omega) H_0^1(\Omega) \mathcal{V} L^2(\Omega) H_0^1(\Omega) W_0^{1,2}(\Omega) \displaystyle \overline{C_0^\infty(\Omega)}^{\|\|_{W^{1,2}(\Omega)}}, C_0^\infty(\Omega) W^{1,2}(\Omega) H_0^1(\Omega) H^1(\Omega) \mathcal{V} H_0^1(\Omega) \mathcal{V} H^1(\Omega) H_0^1 \mathcal{V} H_0^1(\Omega) \mathcal{V} H^1(\Omega) H_0^1(\Omega)",['real-analysis']
45,Counterexample for the stronger statement of Riesz's lemma,Counterexample for the stronger statement of Riesz's lemma,,"Here is a counterexample for the stronger statement of Riesz's lemma and I don't understand it. Why for all $x$ , such that $||x||=1$ , there exists $y \in Y$ , such that $d(x,y)<1$ ?","Here is a counterexample for the stronger statement of Riesz's lemma and I don't understand it. Why for all , such that , there exists , such that ?","x ||x||=1 y \in Y d(x,y)<1","['functional-analysis', 'vector-spaces', 'normed-spaces']"
46,smooth functions are dense in the space of bounded continuous functions - why? [duplicate],smooth functions are dense in the space of bounded continuous functions - why? [duplicate],,"This question already has an answer here : Is it possible to write any bounded continuous function as a uniform limit of smooth functions (1 answer) Closed 8 years ago . Let $C_b $ be the space of real valued bounded continuous functions that are defined on $\mathbb {R} $. Let  $S $ be the subset of smooth functions in  $C_b $. My probability theory book states the following without proof: $S $ is dense in $C_b$ Why is this true? The book does not mention it but I guess that  $C_b $ is equipped with the norm  $||\,\,||_{\infty}$ Thank you a lot.","This question already has an answer here : Is it possible to write any bounded continuous function as a uniform limit of smooth functions (1 answer) Closed 8 years ago . Let $C_b $ be the space of real valued bounded continuous functions that are defined on $\mathbb {R} $. Let  $S $ be the subset of smooth functions in  $C_b $. My probability theory book states the following without proof: $S $ is dense in $C_b$ Why is this true? The book does not mention it but I guess that  $C_b $ is equipped with the norm  $||\,\,||_{\infty}$ Thank you a lot.",,"['analysis', 'functional-analysis']"
47,uniform convergence in individual variables implies uniform convergence in general?,uniform convergence in individual variables implies uniform convergence in general?,,"I have a family of functions $K_n : [0,1]^2 \to R$ that are continuous and symmetric in the sense that $K_n(s,t) = K_n(t,s)$ for all $s,t \in [0,1]$. I know that, for fixed $s$, $\sum_{n=1}^\infty K_n(s,\cdot)$ converges uniformly to some $K(s,\cdot)$; the same property holds for fixed $t$. This means that $\sum_{n=1}^\infty K_n$ converges pointwise to $K$ on $[0,1]^2$. But does it mean that $\sum_{n=1}^\infty K_n$ must converge uniformly to $K$ on $[0,1]^2$? If so, why; if not, are there additional properties the $K_n$ must satisfy for this to hold?","I have a family of functions $K_n : [0,1]^2 \to R$ that are continuous and symmetric in the sense that $K_n(s,t) = K_n(t,s)$ for all $s,t \in [0,1]$. I know that, for fixed $s$, $\sum_{n=1}^\infty K_n(s,\cdot)$ converges uniformly to some $K(s,\cdot)$; the same property holds for fixed $t$. This means that $\sum_{n=1}^\infty K_n$ converges pointwise to $K$ on $[0,1]^2$. But does it mean that $\sum_{n=1}^\infty K_n$ must converge uniformly to $K$ on $[0,1]^2$? If so, why; if not, are there additional properties the $K_n$ must satisfy for this to hold?",,"['analysis', 'functional-analysis', 'uniform-convergence']"
48,Connection between L²-norm of function and its moment-sequence,Connection between L²-norm of function and its moment-sequence,,"Let $f: [0,1] \to \mathbb R$ be a continuous function and define $m_i := \int_0^1 f(x)x^i dx$. I want to somehow formalize and then show the following: If the sequence $(m_i)$ has a very small $\ell^2$-norm, then the $L^2$-norm of $f$ is also very small. More formally, let $(M_n)$ be a sequence of moment sequences $(m_i)$ with the property that the $\ell^2$-norms of $M_n$ converge to zero. Does the corresponding sequence of real functions then converge to zero in $L^2$? Unfortunately I do not have any idea how to approach this problem. Any reference would be helpful.","Let $f: [0,1] \to \mathbb R$ be a continuous function and define $m_i := \int_0^1 f(x)x^i dx$. I want to somehow formalize and then show the following: If the sequence $(m_i)$ has a very small $\ell^2$-norm, then the $L^2$-norm of $f$ is also very small. More formally, let $(M_n)$ be a sequence of moment sequences $(m_i)$ with the property that the $\ell^2$-norms of $M_n$ converge to zero. Does the corresponding sequence of real functions then converge to zero in $L^2$? Unfortunately I do not have any idea how to approach this problem. Any reference would be helpful.",,"['real-analysis', 'functional-analysis']"
49,"Set all measurable real functions on $[0,1]$ with metric $\int_{0}^1 \min \{1,|f(t)−g(t)|\}dt$ is Fréchet without nonzero continuous linear functional",Set all measurable real functions on  with metric  is Fréchet without nonzero continuous linear functional,"[0,1] \int_{0}^1 \min \{1,|f(t)−g(t)|\}dt","Bounty Edit: In the following, all the questions will be highlighted by a bold number and a text written in italics. I found the following statement in a book, and I am really struggling to see why it is true. The set of all measurable real functions on $[0,1]$, metrized via the map $$(f,g)\mapsto \int_{0}^1 \min \{1,|f(t)−g(t)|\}dt,$$ is a Fréchet space on which there is no nonzero continuous linear functional. (Even the linear functional $f\mapsto f(0)$ is not continuous on this space). Regarding the very last statement in braces, I should have a proof of it. Attempted Proof: Let $f=0$, i.e. for every $x \in [0,1], f(x) = 0$, and define $f_n$ as   $$ f_n (x):= \begin{cases} 1, & \text{ if } x \in [0, \frac{1}{n}],\\ 0, & \text{ if } x \in (\frac{1}{n}, 1].\\ \end{cases}$$   To see that $f_n \to f$, notice that, for an arbitrary $n \in \mathbb{N}$, we have   $$ \begin{align*} d(f_n , f) &= \int_{0}^1 | f_n (t) - f(t) | \ dt \\ &\leq \int_{0}^1 | f_n (t)| + |f(t) | \ dt \\  &= \int_{0}^1 | f_n (t)|\ dt  + \int_{0}^1 |f(t) | \ dt \\ &= \int_{0}^1 | f_n (t)|\ dt \to 0 \end{align*} $$   However, $f_n(0) \nrightarrow f(0)$, because for every $n \in \mathbb{N}$, $f_n (0) =1$, while $f(0)=0$. Concerning it, here there is my first question : 1) Is the way in which I wrote the proof correct? Regarding the main statement, the things that have to be proved are two: a) $\mathcal{M}$ (that denotes the set of all measurable functions on $[0,1]$) is a Frechét space (complete metric linear space); b) there are no nonzero continuous linear functionals on $\mathcal{M}$. Related to part (b) I found in Rudin's ""Functional Analysis"" (pp.36-37, section 1.47) a proof that the space $L^p$ with $0 < p <1$ behaves in the same way. Moreover, on the wikipedia page on locally convex TVS I found that there is still another space that behaves similarly, namely $L_0$, i.e. the set of all measurable functions with distance function $$d(f,g):= \int_{0}^1 \frac{|f(x) - g(x)|}{1+|f(x)-g(x)|}dx.$$ Thus, here there are my other questions . 2) Is it possible to prove part (a) in different ways, namely one less constructive, and another one more constructive, with an actual $f \in \mathcal{M}$ to which an arbitrary Cauchy sequence $(f_n) \subseteq \mathcal{M}$ converge? If yes, how? 3) Is it possible to prove part (b) by showing that $\mathcal{M}$ is homeomorphic to $L_0$, which is homeomorphic to $L^p$ with $0 < p <1$, and then exploiting Rudin's proof? Are all those spaces actually homeomorphic? If yes, how? 4) If they are not homeomorphic, and we cannot use Rudin's proof, essentially, how do we prove the result, namely part (a) and part (b)? I am looking forward to any answer, because being self-thaught I am having some real problems concerning this result, and I have the feeling that grasping this proof (with the techniques involved) would help me a lot. Thank you for your time.","Bounty Edit: In the following, all the questions will be highlighted by a bold number and a text written in italics. I found the following statement in a book, and I am really struggling to see why it is true. The set of all measurable real functions on $[0,1]$, metrized via the map $$(f,g)\mapsto \int_{0}^1 \min \{1,|f(t)−g(t)|\}dt,$$ is a Fréchet space on which there is no nonzero continuous linear functional. (Even the linear functional $f\mapsto f(0)$ is not continuous on this space). Regarding the very last statement in braces, I should have a proof of it. Attempted Proof: Let $f=0$, i.e. for every $x \in [0,1], f(x) = 0$, and define $f_n$ as   $$ f_n (x):= \begin{cases} 1, & \text{ if } x \in [0, \frac{1}{n}],\\ 0, & \text{ if } x \in (\frac{1}{n}, 1].\\ \end{cases}$$   To see that $f_n \to f$, notice that, for an arbitrary $n \in \mathbb{N}$, we have   $$ \begin{align*} d(f_n , f) &= \int_{0}^1 | f_n (t) - f(t) | \ dt \\ &\leq \int_{0}^1 | f_n (t)| + |f(t) | \ dt \\  &= \int_{0}^1 | f_n (t)|\ dt  + \int_{0}^1 |f(t) | \ dt \\ &= \int_{0}^1 | f_n (t)|\ dt \to 0 \end{align*} $$   However, $f_n(0) \nrightarrow f(0)$, because for every $n \in \mathbb{N}$, $f_n (0) =1$, while $f(0)=0$. Concerning it, here there is my first question : 1) Is the way in which I wrote the proof correct? Regarding the main statement, the things that have to be proved are two: a) $\mathcal{M}$ (that denotes the set of all measurable functions on $[0,1]$) is a Frechét space (complete metric linear space); b) there are no nonzero continuous linear functionals on $\mathcal{M}$. Related to part (b) I found in Rudin's ""Functional Analysis"" (pp.36-37, section 1.47) a proof that the space $L^p$ with $0 < p <1$ behaves in the same way. Moreover, on the wikipedia page on locally convex TVS I found that there is still another space that behaves similarly, namely $L_0$, i.e. the set of all measurable functions with distance function $$d(f,g):= \int_{0}^1 \frac{|f(x) - g(x)|}{1+|f(x)-g(x)|}dx.$$ Thus, here there are my other questions . 2) Is it possible to prove part (a) in different ways, namely one less constructive, and another one more constructive, with an actual $f \in \mathcal{M}$ to which an arbitrary Cauchy sequence $(f_n) \subseteq \mathcal{M}$ converge? If yes, how? 3) Is it possible to prove part (b) by showing that $\mathcal{M}$ is homeomorphic to $L_0$, which is homeomorphic to $L^p$ with $0 < p <1$, and then exploiting Rudin's proof? Are all those spaces actually homeomorphic? If yes, how? 4) If they are not homeomorphic, and we cannot use Rudin's proof, essentially, how do we prove the result, namely part (a) and part (b)? I am looking forward to any answer, because being self-thaught I am having some real problems concerning this result, and I have the feeling that grasping this proof (with the techniques involved) would help me a lot. Thank you for your time.",,"['real-analysis', 'functional-analysis', 'proof-verification', 'self-learning', 'linear-transformations']"
50,Convergence of measures and potential theory,Convergence of measures and potential theory,,"The following implication should hold: $\mu_{n}, \mu$ are positive measures whose supports are included in a compact set $K\subset \mathbb{C}$ and  $$\lim_{n\to\infty}U^{\mu_n}(z)=U^{\mu}(z)$$ uniformly in compact subsets of $\mathbb{C}\setminus K$, where $$ U^{\nu}(z)=\int_{\mathbb{C}}\log|z-x|d\nu(x)$$ is the logarithmic potential of the measure $\nu$. Then $\mu_{n}\to\mu$, for $n\to\infty$ in the weak$^\star$ topology. I read the paper where this implication is used. The authors state only that it is a standard result of Potential theory. Unfortunately, I do not have solid background in Potential theory and I can not see why the statement is true by myself. Although I went through several books devoted entirely to the Potential theory (Saff & Totik, Ransford, Landkof), I did not find a place where this implication would be addressed directly (as a separate proposition, for example). Can anyone, who is familiar with the subject, give some relevant reference? Many thanks!","The following implication should hold: $\mu_{n}, \mu$ are positive measures whose supports are included in a compact set $K\subset \mathbb{C}$ and  $$\lim_{n\to\infty}U^{\mu_n}(z)=U^{\mu}(z)$$ uniformly in compact subsets of $\mathbb{C}\setminus K$, where $$ U^{\nu}(z)=\int_{\mathbb{C}}\log|z-x|d\nu(x)$$ is the logarithmic potential of the measure $\nu$. Then $\mu_{n}\to\mu$, for $n\to\infty$ in the weak$^\star$ topology. I read the paper where this implication is used. The authors state only that it is a standard result of Potential theory. Unfortunately, I do not have solid background in Potential theory and I can not see why the statement is true by myself. Although I went through several books devoted entirely to the Potential theory (Saff & Totik, Ransford, Landkof), I did not find a place where this implication would be addressed directly (as a separate proposition, for example). Can anyone, who is familiar with the subject, give some relevant reference? Many thanks!",,"['functional-analysis', 'measure-theory', 'potential-theory']"
51,Compact subsets of Banach spaces,Compact subsets of Banach spaces,,"Is a theorem of Mazur that a closed subset $K$ of a Banach space $X$ is compact if, and only if, there is a sequence $(x_n)$ of $X$ such that $|x_n|\to 0$ and $K\subseteq \overline{\operatorname{conv}}\{x_n:n\geq 0\}$, where $\overline{\operatorname{conv}}\{x_n:n\geq 0\}$ denotes the clousure of the converx hull of the sequence $(x_n)$. I'm trying to understand the proof given in the book Classical Banach Spaces I, by  J. Lindenstrauss and  L. Tzafriri. If $K$ is a compact subset of $X$, then there is points $x_{1,1}\dots x_{n_1,1}\in X$ such that  $$2K\subseteq \bigcup_{i=1}^{n_1}B_{\frac{1}{4}}(x_{i,1})$$ Now, why must $$K_2 = \bigcup_{i=1}^{n_1}\left[B_{\frac{1}{4}}(x_{i,1})\cap 2K-x_{i,1}\right]$$ be compact? He then, by induction, obtain, for each $j\geq 2$, points $x_{1,j},\dots,x_{n_j,j}\in X$ such that $$K_j = \bigcup_{i=1}^{n_j}\left[B_{\frac{1}{4^{j}}}(x_{i,j})\cap 2K_{j-1}-x_{i,j}\right]$$ is compact. Given a $x\in K$ there is a $1\leq i_1\leq n_1,\dots, 1\leq i_{k}\leq n_k$ such that  $$x - \sum_{j=1}^{k}\frac{x_{i_j,j}}{2^j}\in \frac{1}{2^k}K_{k+1}$$ Why must $x\in \overline{\operatorname{conv}}\{x_{i,j}: j\geq 1\text{ and } 1\leq i\leq n_j\}$? Also I would like to know if there is another proof of this fact.","Is a theorem of Mazur that a closed subset $K$ of a Banach space $X$ is compact if, and only if, there is a sequence $(x_n)$ of $X$ such that $|x_n|\to 0$ and $K\subseteq \overline{\operatorname{conv}}\{x_n:n\geq 0\}$, where $\overline{\operatorname{conv}}\{x_n:n\geq 0\}$ denotes the clousure of the converx hull of the sequence $(x_n)$. I'm trying to understand the proof given in the book Classical Banach Spaces I, by  J. Lindenstrauss and  L. Tzafriri. If $K$ is a compact subset of $X$, then there is points $x_{1,1}\dots x_{n_1,1}\in X$ such that  $$2K\subseteq \bigcup_{i=1}^{n_1}B_{\frac{1}{4}}(x_{i,1})$$ Now, why must $$K_2 = \bigcup_{i=1}^{n_1}\left[B_{\frac{1}{4}}(x_{i,1})\cap 2K-x_{i,1}\right]$$ be compact? He then, by induction, obtain, for each $j\geq 2$, points $x_{1,j},\dots,x_{n_j,j}\in X$ such that $$K_j = \bigcup_{i=1}^{n_j}\left[B_{\frac{1}{4^{j}}}(x_{i,j})\cap 2K_{j-1}-x_{i,j}\right]$$ is compact. Given a $x\in K$ there is a $1\leq i_1\leq n_1,\dots, 1\leq i_{k}\leq n_k$ such that  $$x - \sum_{j=1}^{k}\frac{x_{i_j,j}}{2^j}\in \frac{1}{2^k}K_{k+1}$$ Why must $x\in \overline{\operatorname{conv}}\{x_{i,j}: j\geq 1\text{ and } 1\leq i\leq n_j\}$? Also I would like to know if there is another proof of this fact.",,['functional-analysis']
52,"If $X$ is separable, then $\mathcal{F}(X)$ is also separable, where $\mathcal{F}(X) = \overline{span \{ \delta_x : x \in X \}}$","If  is separable, then  is also separable, where",X \mathcal{F}(X) \mathcal{F}(X) = \overline{span \{ \delta_x : x \in X \}},"Suppose $X$ is a Banach space. For any $x \in X$, define the set $\mathcal{F}(X) = \overline{span \{ \delta_x : x \in X \}}$ where $\delta_x(f)=f(x)$ for all $f \in$ Lip$_0(X)$. The set Lip$_0(X)$ is the set of all real-valued Lipschitz functions which vanish at $0$. Note that $\delta_x$ is an evaluation functional on Lip$_0(X)$. I am wondering whether the following statement is true or not. If $X$ is separable, then $\mathcal{F}(X)$ is also separable. The statement above is taken from here , so I believe the statement is true. But I have no idea on how to prove it. UPDATE: The following is my attempt: Take the set $\{ \delta_x : x \in X \}$ with the norm $\| \delta_x \| = \| x \|$. Note that $\{ \delta_x : x \in X \}$ is isometric to $X$. Hence, $\mathcal{F}(X)$ is separable. Is it correct?","Suppose $X$ is a Banach space. For any $x \in X$, define the set $\mathcal{F}(X) = \overline{span \{ \delta_x : x \in X \}}$ where $\delta_x(f)=f(x)$ for all $f \in$ Lip$_0(X)$. The set Lip$_0(X)$ is the set of all real-valued Lipschitz functions which vanish at $0$. Note that $\delta_x$ is an evaluation functional on Lip$_0(X)$. I am wondering whether the following statement is true or not. If $X$ is separable, then $\mathcal{F}(X)$ is also separable. The statement above is taken from here , so I believe the statement is true. But I have no idea on how to prove it. UPDATE: The following is my attempt: Take the set $\{ \delta_x : x \in X \}$ with the norm $\| \delta_x \| = \| x \|$. Note that $\{ \delta_x : x \in X \}$ is isometric to $X$. Hence, $\mathcal{F}(X)$ is separable. Is it correct?",,"['functional-analysis', 'banach-spaces']"
53,How are the assumptions used in the proof of Bourbaki-Alaoglu Theorem?,How are the assumptions used in the proof of Bourbaki-Alaoglu Theorem?,,"This is a follow up question to a previous one . In the proof of the following theorem, where are the assumptions ""Hausdorff"" and ""locally convex"" used?","This is a follow up question to a previous one . In the proof of the following theorem, where are the assumptions ""Hausdorff"" and ""locally convex"" used?",,['functional-analysis']
54,Linear map from $L^1 \rightarrow L^{\infty}.$,Linear map from,L^1 \rightarrow L^{\infty}.,"I was wondering how I can show that any linear map $T: L^1(\Omega) \rightarrow L^{\infty}(\Omega)$ can be represented as an integral operator $$T(f)(x):=\int_{\Omega} K(x,y)f(y) dy.$$ Does anybody know how to show this or where this follows from?","I was wondering how I can show that any linear map $T: L^1(\Omega) \rightarrow L^{\infty}(\Omega)$ can be represented as an integral operator $$T(f)(x):=\int_{\Omega} K(x,y)f(y) dy.$$ Does anybody know how to show this or where this follows from?",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'lebesgue-integral']"
55,$\mathcal{A}+K$ is norm-closed where $\mathcal{A}$ is a $C^*$-algebra and $K$ is the compact operators.,is norm-closed where  is a -algebra and  is the compact operators.,\mathcal{A}+K \mathcal{A} C^* K,"Let $\mathcal{A}\subset B(H)$ be a unital $C^*$-algebra and let $K$ be the closed ideal of compact operators. I need to show that $\mathcal{A}+K$ is also a $C^*$-subalgebra of $B(H)$. I am stuck at proving that the space $\mathcal{A}+K$ is norm-closed. I begin with a a sequence $\{B_n\}_{n=1}^{\infty}$ in $\mathcal{A}+K$ with $B_n\rightarrow B$, for some operator $B\in B(H)$. Now, for each $n\in\mathbb{N}$, $B_n=A_n+K_n$, where $A_n\in \mathcal{A}$ and $K_n\in K$. And from here I'm lost. Is there some kind of Bolzano-Weierstrass property for compact operators? Hints are welcome.","Let $\mathcal{A}\subset B(H)$ be a unital $C^*$-algebra and let $K$ be the closed ideal of compact operators. I need to show that $\mathcal{A}+K$ is also a $C^*$-subalgebra of $B(H)$. I am stuck at proving that the space $\mathcal{A}+K$ is norm-closed. I begin with a a sequence $\{B_n\}_{n=1}^{\infty}$ in $\mathcal{A}+K$ with $B_n\rightarrow B$, for some operator $B\in B(H)$. Now, for each $n\in\mathbb{N}$, $B_n=A_n+K_n$, where $A_n\in \mathcal{A}$ and $K_n\in K$. And from here I'm lost. Is there some kind of Bolzano-Weierstrass property for compact operators? Hints are welcome.",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'compact-operators']"
56,Compute the norm of the operation $A$,Compute the norm of the operation,A,"Suppose that $\left ( a_{ij} \right )_{i,j=1}^{\infty}$ is a matrix satisfying the following condition $$\sum_{i,j=1}^{\infty} \left | a_{ij} \right |^q < \infty$$ where $q>1$. For $x=\left \{ \xi_j \right \}_{j=1}^{\infty} \in l_p$ where $\frac{1}{p}+\frac{1}{q}=1$, let we define $$Ax=y := \left \{ \eta_i \right \}_{i=1}^{\infty},$$ where $$\eta_i=\sum_{j=1}^{\infty}a_{ij}\xi_j$$ for $i=1, 2, ...$  I proved that $A$ is a continuously linear operation from $l_p$ to $l_q$. I don't know how to compute the norm of $A$ that defined by $$\lVert A \rVert = \sup_{x \neq 0} \frac{\lVert Ax \rVert}{\lVert x \rVert} = \sup_{\lVert x \rVert \leq 1} \lVert Ax \rVert = \sup_{\lVert x \rVert = 1} \lVert Ax \rVert$$  How could I compute the norm of $A$ above?","Suppose that $\left ( a_{ij} \right )_{i,j=1}^{\infty}$ is a matrix satisfying the following condition $$\sum_{i,j=1}^{\infty} \left | a_{ij} \right |^q < \infty$$ where $q>1$. For $x=\left \{ \xi_j \right \}_{j=1}^{\infty} \in l_p$ where $\frac{1}{p}+\frac{1}{q}=1$, let we define $$Ax=y := \left \{ \eta_i \right \}_{i=1}^{\infty},$$ where $$\eta_i=\sum_{j=1}^{\infty}a_{ij}\xi_j$$ for $i=1, 2, ...$  I proved that $A$ is a continuously linear operation from $l_p$ to $l_q$. I don't know how to compute the norm of $A$ that defined by $$\lVert A \rVert = \sup_{x \neq 0} \frac{\lVert Ax \rVert}{\lVert x \rVert} = \sup_{\lVert x \rVert \leq 1} \lVert Ax \rVert = \sup_{\lVert x \rVert = 1} \lVert Ax \rVert$$  How could I compute the norm of $A$ above?",,"['real-analysis', 'functional-analysis', 'normed-spaces']"
57,Do approximate identities remain approximate identities if one adjoins 1 to a C* Algebra?,Do approximate identities remain approximate identities if one adjoins 1 to a C* Algebra?,,"If we have a C* Algebra $\mathscr{U}$ without an identity we can adjoin an identity $\mathbb{1}$ in the following way: We take $\mathscr{\tilde U}$ to be the set $\{(\alpha,A); \alpha \in \mathbb{C}, A \in \mathscr{U}  \}$ On $\mathscr{\tilde U}$ we define addition in the obvious way and define multiplication via $(\alpha,A) \cdot (\beta,B) := (\alpha \beta,\alpha B + \beta A +AB)$. Also we can introduce a shorthand notation $(\alpha, A) := \alpha \mathbb{1} + A$ The norm of the new states is defined via: $$\|\alpha\mathbb{1}+A\| := \sup\{\|\alpha B + AB\|\ ; B \in \mathscr{U},\, \|B\|=1\}$$ The result will again be a C* algebra and $\mathscr{U}$ can be identified as the subset of elements in the form $(0,A)$ of $\mathscr{\tilde U}$. My question is whether an approximate identity on $\mathscr{U}$ will still have the property of being an approximate identity on the new C* Algebra. Is the proof easy?","If we have a C* Algebra $\mathscr{U}$ without an identity we can adjoin an identity $\mathbb{1}$ in the following way: We take $\mathscr{\tilde U}$ to be the set $\{(\alpha,A); \alpha \in \mathbb{C}, A \in \mathscr{U}  \}$ On $\mathscr{\tilde U}$ we define addition in the obvious way and define multiplication via $(\alpha,A) \cdot (\beta,B) := (\alpha \beta,\alpha B + \beta A +AB)$. Also we can introduce a shorthand notation $(\alpha, A) := \alpha \mathbb{1} + A$ The norm of the new states is defined via: $$\|\alpha\mathbb{1}+A\| := \sup\{\|\alpha B + AB\|\ ; B \in \mathscr{U},\, \|B\|=1\}$$ The result will again be a C* algebra and $\mathscr{U}$ can be identified as the subset of elements in the form $(0,A)$ of $\mathscr{\tilde U}$. My question is whether an approximate identity on $\mathscr{U}$ will still have the property of being an approximate identity on the new C* Algebra. Is the proof easy?",,"['linear-algebra', 'functional-analysis', 'c-star-algebras']"
58,An mixed weak star convergence problem,An mixed weak star convergence problem,,"Let $\Omega\subset \mathbb R^N$ open bounded. Given a sequence of Radon measure $(\mu_n)$ such that $\mu_n\to \mu$ in weak star sense in $\mathcal M_b(\Omega)$ and $\|\mu_n\|\nearrow \|\mu\|$. Also given a function $v\geq 1$ such that $v\in L^1_{\text{loc}}(\Omega)$ and $l.s.c$. Suppose $v$ has properties that there exists a Lipschitz continuous sequence $v_n$ such that $1\leq v_n\leq v$ and $v_n\nearrow v$ for all $x\in\Omega$. Note that $v$ may not bounded above. Assume there exists a function $u\in C(\Omega)$ such that $u/v\in C_c(\Omega)$. My question: do we have  $$ \lim_{n\to\infty}\int_\Omega \frac{u}{v_n}\,d\mu_n = \int_\Omega \frac{u}{v}\,d\mu $$ My try: Writing  $$ \int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu = \int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu_n+\int_\Omega \frac{u}{v}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu $$ The last two goes to 0 by the definition of weak star convergence. But I don't know how to deal with first two. I was trying to use dominated convergence but it is not obvious... I realize that since $u/v\in C_c(\Omega)$ and $v$ is finite a.e., it makes $u$ has compact support and hence $u$, $u/v_n\in C_c(\Omega)$ since $u\in C(\Omega)$. Now, I could write few more steps... \begin{align*} \left|\int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu_n\right| &= \left|\int_\Omega u(1/v_n-1/v)\,d\mu_n\right|\\ &=\left|\int_\Omega u(1/v_n-1/v)\,d(\mu_n-\mu+\mu)\right|\\ &\leq \left|\int_\Omega u(1/v_n-1/v)\,d(\mu_n-\mu)\right|+\left|\int_\Omega u(1/v_n-1/v)\,d\mu\right| \end{align*} The last one can be done by dominated convergence. But the first one...Maybe I should use the fact that $\|\mu_n\|\to\|\mu\|$? I know generally I should not hope  $$ \lim_{n\to\infty}\int_\Omega \frac{u}{v_n}\,d\mu_n = \int_\Omega \frac{u}{v}\,d\mu $$ since it would require that $u/v_n\to u/v$ uniformly which I don't have. But since I in additional have $0\leq 1/v\leq 1/v_n\leq 1$ and $\|\mu_n\|\to \|\mu\|$, I may expect my result is true.","Let $\Omega\subset \mathbb R^N$ open bounded. Given a sequence of Radon measure $(\mu_n)$ such that $\mu_n\to \mu$ in weak star sense in $\mathcal M_b(\Omega)$ and $\|\mu_n\|\nearrow \|\mu\|$. Also given a function $v\geq 1$ such that $v\in L^1_{\text{loc}}(\Omega)$ and $l.s.c$. Suppose $v$ has properties that there exists a Lipschitz continuous sequence $v_n$ such that $1\leq v_n\leq v$ and $v_n\nearrow v$ for all $x\in\Omega$. Note that $v$ may not bounded above. Assume there exists a function $u\in C(\Omega)$ such that $u/v\in C_c(\Omega)$. My question: do we have  $$ \lim_{n\to\infty}\int_\Omega \frac{u}{v_n}\,d\mu_n = \int_\Omega \frac{u}{v}\,d\mu $$ My try: Writing  $$ \int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu = \int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu_n+\int_\Omega \frac{u}{v}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu $$ The last two goes to 0 by the definition of weak star convergence. But I don't know how to deal with first two. I was trying to use dominated convergence but it is not obvious... I realize that since $u/v\in C_c(\Omega)$ and $v$ is finite a.e., it makes $u$ has compact support and hence $u$, $u/v_n\in C_c(\Omega)$ since $u\in C(\Omega)$. Now, I could write few more steps... \begin{align*} \left|\int_\Omega \frac{u}{v_n}\,d\mu_n - \int_\Omega \frac{u}{v}\,d\mu_n\right| &= \left|\int_\Omega u(1/v_n-1/v)\,d\mu_n\right|\\ &=\left|\int_\Omega u(1/v_n-1/v)\,d(\mu_n-\mu+\mu)\right|\\ &\leq \left|\int_\Omega u(1/v_n-1/v)\,d(\mu_n-\mu)\right|+\left|\int_\Omega u(1/v_n-1/v)\,d\mu\right| \end{align*} The last one can be done by dominated convergence. But the first one...Maybe I should use the fact that $\|\mu_n\|\to\|\mu\|$? I know generally I should not hope  $$ \lim_{n\to\infty}\int_\Omega \frac{u}{v_n}\,d\mu_n = \int_\Omega \frac{u}{v}\,d\mu $$ since it would require that $u/v_n\to u/v$ uniformly which I don't have. But since I in additional have $0\leq 1/v\leq 1/v_n\leq 1$ and $\|\mu_n\|\to \|\mu\|$, I may expect my result is true.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
59,Spectral resolution of multiplication operator,Spectral resolution of multiplication operator,,"Kosaku YOSIDA claims in his book ""Functional Analysis"" that it is easy to see that the multiplication operator $Hx(t) = tx(t)$ in $L^2(-\infty,+\infty)$ admits the spectral resolution $H = \int_{-\infty}^{+\infty} \lambda \, \mathrm{d}E(\lambda)$, where the resolution of the identity is defined via $E(\lambda)x(t) = x(t)$ for $t \leq \lambda$, $=0$ for $t > \lambda$. At this point in the book (Chapter XI.6 Normed Rings and Spectral Representation: The Spectral Resolution of a Self-adjoint Operator) the Spectral theorem is NOT yet proven. I have problemes with the ""easy to see""-part. I think he tries to argue that $\int_{-\infty}^{+\infty} \lambda^2 \, \mathrm{d} \| E(\lambda) x \|^2 = \|Hx\|^2$ and $\int_{-\infty}^{+\infty} \, \mathrm{d} \langle E(\lambda) x , y \rangle = \langle Hx, y \rangle$. But how does this show the claim?","Kosaku YOSIDA claims in his book ""Functional Analysis"" that it is easy to see that the multiplication operator $Hx(t) = tx(t)$ in $L^2(-\infty,+\infty)$ admits the spectral resolution $H = \int_{-\infty}^{+\infty} \lambda \, \mathrm{d}E(\lambda)$, where the resolution of the identity is defined via $E(\lambda)x(t) = x(t)$ for $t \leq \lambda$, $=0$ for $t > \lambda$. At this point in the book (Chapter XI.6 Normed Rings and Spectral Representation: The Spectral Resolution of a Self-adjoint Operator) the Spectral theorem is NOT yet proven. I have problemes with the ""easy to see""-part. I think he tries to argue that $\int_{-\infty}^{+\infty} \lambda^2 \, \mathrm{d} \| E(\lambda) x \|^2 = \|Hx\|^2$ and $\int_{-\infty}^{+\infty} \, \mathrm{d} \langle E(\lambda) x , y \rangle = \langle Hx, y \rangle$. But how does this show the claim?",,"['analysis', 'functional-analysis', 'hilbert-spaces', 'spectral-theory']"
60,Generalization of the Vitali-Hahn-Saks Theorem,Generalization of the Vitali-Hahn-Saks Theorem,,Is there a generalization of the Vitali-Hahn-Saks Theorem for nets of measures? I do not find any related literature. Take a sequence of bounded measures on a sigma-field and consider a subnet of this sequence. Suppose the subnet is convergent in the sense of Vitali-Hahn-Saks. Is the limit sigma-additive?,Is there a generalization of the Vitali-Hahn-Saks Theorem for nets of measures? I do not find any related literature. Take a sequence of bounded measures on a sigma-field and consider a subnet of this sequence. Suppose the subnet is convergent in the sense of Vitali-Hahn-Saks. Is the limit sigma-additive?,,"['functional-analysis', 'measure-theory']"
61,Integrable functions that take values in a Banach space,Integrable functions that take values in a Banach space,,"Let $\mathbb K$ be $\mathbb R$ or $\mathbb C$. Let $(X, \mathcal M, \mu)$ be a measure space and let $F$ be a Banach space over $\mathbb K$. I would like to define an integral of a function $f:X \rightarrow F$ that satisfies suitable conditions. My method is motivated by Bourbaki's Integration. They assume that $X$ is a locally compact Hausdorff space and $\mu$ is a Radon measure. So mine is sort of a generalization of their definition of the integral. If $g: X \rightarrow [0, \infty]$ is a non-negative extended real-valued function, we denote by $\int^* g d\mu$ the upper integral of $g$(for the definition of the upper integral, see Lebesgue's monotone convergence theorem for upper integrals ). We denote by $\mathcal F(X, F)$ the set of functions $X\rightarrow F$. It is a vector space over $\mathbb K$. If $f\in \mathcal F(X, F)$, then we denote $N_1(f) = \int^* |f(x)| d\mu(x)$. We denote by $\mathcal F^1(X, F)$ the set $\{f\in \mathcal F(X, F): N_1(f) \lt \infty\}$. Then $\mathcal F^1(X, F)$ is a vector subspace of $\mathcal F(X, F)$. It is easy to see that $N_1$ is a seminorm on $\mathcal F^1(X, F)$. It can be proved by using Lebesgue monotone convergence theorem for upper integrals( Lebesgue's monotone convergence theorem for upper integrals ) that  $\mathcal F^1(X, F)$ is complete with the seminorm $N_1$(you just mimic the proof of the corresponding theorem in Bourbaki's Integration). A function $f: X\rightarrow F$ is called a simple function if $f(X)$ is a finite set and $f^{-1}(a) \in \mathcal M$ for all $a\in f(X)$. $f$ is said to have a finite support if $\mu(f^{-1}(a)) \lt \infty$ for all $a\in f(X) - \{0\}$. Let $\mathcal S$ be the set of simple functions of finite support. It is a subspace of $\mathcal F^1(X, F)$. We denote by $\mathcal L^1(X, F)$ the closure of $\mathcal S$ in $\mathcal F^1(X, F)$ with respect to the seminorm $N_1$. If $f\in \mathcal S$, we can define $\int f d\mu \in F$ in the obvious way. Since $|\int f d\mu| \le \int |f(x)| d\mu(x) = N_1(f)$, the map $\psi: \mathcal S\rightarrow F$ defined by $\psi(f) = \int f d\mu$ is a continuous linear map. Since $\mathcal S$ is dense in $\mathcal L^1(X, F)$ and $F$ is a Banach space, $\psi$ can be extended to a unique continuous linear map $\phi: \mathcal L^1(X, F)\rightarrow F$. We call an element $f \in \mathcal L^1(X, F)$ integrable and call $\phi(f)$ its integral. We denote $\phi(f)$ by $\int f d\mu$ or $\int f(x) d\mu(x)$. Now I would like to find some criteria to determine when a function $X \rightarrow F$ is integrable. A function $f:X\rightarrow F$ is said to be $\mathcal B(F)$-measurable if $f^{-1}(U) \in \mathcal M$ for all open subsets $U$ of $F$, where $\mathcal B(F)$ is the $\sigma$-algebra generated by all open subsets of $F$. A function $f:X\rightarrow F$ is said to be Bochner measurable if it satisfies the following condition. There exists a sequence of simple functions $f_n,n=1,2,\cdots$ defined on $X$ such that $f(x) = \text{lim}_{n\rightarrow\infty} f_n(x)$ almost everywhere on $X$. It can be proved that an integrable function is Bochner measurable(you just mimic the Bourbaki's proof of the corresponding fact). Conversely suppose $f$ is a Bochner measurable function. There exists a null set $N$ and a sequence of simple functions $f_n, n = 1, 2, \cdots$ such that $f(x) = \text{lim}_{n\rightarrow \infty} f_n(x)$ for all $x\in X - N$. Since the pointwise limit of a sequence of $\mathcal B(F)$-measurable functions is $\mathcal B(F)$-measurable(see Limit of measurable functions is measurable? ), if we redefine $f$ as $f(x) = 0$ for $x \in N$, then $f$ is $\mathcal B(F)$-measurable. Hence the function $x \rightarrow |f(x)|$ is measurable. If $\int |f(x)| d\mu \lt \infty$, it seems to me that $f$ is integrable in our sense. How do you prove this if it is correct?","Let $\mathbb K$ be $\mathbb R$ or $\mathbb C$. Let $(X, \mathcal M, \mu)$ be a measure space and let $F$ be a Banach space over $\mathbb K$. I would like to define an integral of a function $f:X \rightarrow F$ that satisfies suitable conditions. My method is motivated by Bourbaki's Integration. They assume that $X$ is a locally compact Hausdorff space and $\mu$ is a Radon measure. So mine is sort of a generalization of their definition of the integral. If $g: X \rightarrow [0, \infty]$ is a non-negative extended real-valued function, we denote by $\int^* g d\mu$ the upper integral of $g$(for the definition of the upper integral, see Lebesgue's monotone convergence theorem for upper integrals ). We denote by $\mathcal F(X, F)$ the set of functions $X\rightarrow F$. It is a vector space over $\mathbb K$. If $f\in \mathcal F(X, F)$, then we denote $N_1(f) = \int^* |f(x)| d\mu(x)$. We denote by $\mathcal F^1(X, F)$ the set $\{f\in \mathcal F(X, F): N_1(f) \lt \infty\}$. Then $\mathcal F^1(X, F)$ is a vector subspace of $\mathcal F(X, F)$. It is easy to see that $N_1$ is a seminorm on $\mathcal F^1(X, F)$. It can be proved by using Lebesgue monotone convergence theorem for upper integrals( Lebesgue's monotone convergence theorem for upper integrals ) that  $\mathcal F^1(X, F)$ is complete with the seminorm $N_1$(you just mimic the proof of the corresponding theorem in Bourbaki's Integration). A function $f: X\rightarrow F$ is called a simple function if $f(X)$ is a finite set and $f^{-1}(a) \in \mathcal M$ for all $a\in f(X)$. $f$ is said to have a finite support if $\mu(f^{-1}(a)) \lt \infty$ for all $a\in f(X) - \{0\}$. Let $\mathcal S$ be the set of simple functions of finite support. It is a subspace of $\mathcal F^1(X, F)$. We denote by $\mathcal L^1(X, F)$ the closure of $\mathcal S$ in $\mathcal F^1(X, F)$ with respect to the seminorm $N_1$. If $f\in \mathcal S$, we can define $\int f d\mu \in F$ in the obvious way. Since $|\int f d\mu| \le \int |f(x)| d\mu(x) = N_1(f)$, the map $\psi: \mathcal S\rightarrow F$ defined by $\psi(f) = \int f d\mu$ is a continuous linear map. Since $\mathcal S$ is dense in $\mathcal L^1(X, F)$ and $F$ is a Banach space, $\psi$ can be extended to a unique continuous linear map $\phi: \mathcal L^1(X, F)\rightarrow F$. We call an element $f \in \mathcal L^1(X, F)$ integrable and call $\phi(f)$ its integral. We denote $\phi(f)$ by $\int f d\mu$ or $\int f(x) d\mu(x)$. Now I would like to find some criteria to determine when a function $X \rightarrow F$ is integrable. A function $f:X\rightarrow F$ is said to be $\mathcal B(F)$-measurable if $f^{-1}(U) \in \mathcal M$ for all open subsets $U$ of $F$, where $\mathcal B(F)$ is the $\sigma$-algebra generated by all open subsets of $F$. A function $f:X\rightarrow F$ is said to be Bochner measurable if it satisfies the following condition. There exists a sequence of simple functions $f_n,n=1,2,\cdots$ defined on $X$ such that $f(x) = \text{lim}_{n\rightarrow\infty} f_n(x)$ almost everywhere on $X$. It can be proved that an integrable function is Bochner measurable(you just mimic the Bourbaki's proof of the corresponding fact). Conversely suppose $f$ is a Bochner measurable function. There exists a null set $N$ and a sequence of simple functions $f_n, n = 1, 2, \cdots$ such that $f(x) = \text{lim}_{n\rightarrow \infty} f_n(x)$ for all $x\in X - N$. Since the pointwise limit of a sequence of $\mathcal B(F)$-measurable functions is $\mathcal B(F)$-measurable(see Limit of measurable functions is measurable? ), if we redefine $f$ as $f(x) = 0$ for $x \in N$, then $f$ is $\mathcal B(F)$-measurable. Hence the function $x \rightarrow |f(x)|$ is measurable. If $\int |f(x)| d\mu \lt \infty$, it seems to me that $f$ is integrable in our sense. How do you prove this if it is correct?",,['functional-analysis']
62,Is it possible to approximate $\cos(x)$ with a linear combination of Gaussians $e^{-x^2}$?,Is it possible to approximate  with a linear combination of Gaussians ?,\cos(x) e^{-x^2},"I am interested in approximating $\cos x$ with a linear combination of $e^{-x^2}$. I am not an expert in approximation theory but there are a couple things that give me a bit of hope that it might be possible (and some other things that worry me it might not be possible or that linear combinations might not be enough). First thing I did that gave me some hope was write the taylor expansions of both of them and comparing them: $$ \cos x = \sum^{\infty}_{n=0} \frac{(-1)^n x^{2n}}{ (2n)!}$$ $$ e^{-x^2} = \sum^{\infty}_{n=0} \frac{(-x^2)^n}{n!} = \sum^{\infty}_{n=0} \frac{ (-1)^n x^{2n} }{n!}$$ apart from the denominator, the two power series are nearly identical! However, what worries me is that the factorial function is not easy (for me) to manipulate. Another great thing is that both functions are even, which is definitively great news! Something else that worries me (about this approximation) is that they behave very differently in the tail ends of each other. For example, the Gaussian function dies off to towards the of the function while the cosine does not and instead oscillates (which is kind of a surprising difference if you only consider their power series, I wouldn't have guessed that difference, which is a very bid difference). Something else that again gives me some hope about this approximation is the if you plot both $e^{-x^2}$ and $\cos x$, you get: which even though they are not identical, at least for a bounded interval/domain, look extremely similar! Maybe one could tweak the parameters of one or the other and hopefully get something that might be considered ""close"" to each other. So I I was thinking two direction that might be interesting to explore: An approximation on a bounded interval An approximation using an infinite series of $e^{-x^2}$ (that might be necessary to reflect on the x-axis to mirror the trough of the wave). In fact, it might be possible to solve this issue by solving first an approximation of the crest of the cosine using  tweeked version of $e^{-x^2}$ and then, using that solution, do an infinite summation of that is reflected on the x-axis. T0 make things more clear, what I kind of had in mind with linear combinations was the following formula: $$\sum_{k} c_k e^{- \| x - w_k\|^2}$$ with $w_k$ as the movable centers because we might need to center them at the troughs and crests of the cosines. I am open to different tweet suggestions of this (for example, an improvement could be to including a precision $\beta_k$ or a standard deviation $\sigma_k$ on the exponent to adjust the widths to match the cosine better). I wasn't exactly sure if these were good ideas or if there were maybe better ways to approximate a cosine with $e^{-x^2}$, but I'd love to hear any ideas if people have better suggestions or know how to proceed (rigorously) with the ones I suggested.","I am interested in approximating $\cos x$ with a linear combination of $e^{-x^2}$. I am not an expert in approximation theory but there are a couple things that give me a bit of hope that it might be possible (and some other things that worry me it might not be possible or that linear combinations might not be enough). First thing I did that gave me some hope was write the taylor expansions of both of them and comparing them: $$ \cos x = \sum^{\infty}_{n=0} \frac{(-1)^n x^{2n}}{ (2n)!}$$ $$ e^{-x^2} = \sum^{\infty}_{n=0} \frac{(-x^2)^n}{n!} = \sum^{\infty}_{n=0} \frac{ (-1)^n x^{2n} }{n!}$$ apart from the denominator, the two power series are nearly identical! However, what worries me is that the factorial function is not easy (for me) to manipulate. Another great thing is that both functions are even, which is definitively great news! Something else that worries me (about this approximation) is that they behave very differently in the tail ends of each other. For example, the Gaussian function dies off to towards the of the function while the cosine does not and instead oscillates (which is kind of a surprising difference if you only consider their power series, I wouldn't have guessed that difference, which is a very bid difference). Something else that again gives me some hope about this approximation is the if you plot both $e^{-x^2}$ and $\cos x$, you get: which even though they are not identical, at least for a bounded interval/domain, look extremely similar! Maybe one could tweak the parameters of one or the other and hopefully get something that might be considered ""close"" to each other. So I I was thinking two direction that might be interesting to explore: An approximation on a bounded interval An approximation using an infinite series of $e^{-x^2}$ (that might be necessary to reflect on the x-axis to mirror the trough of the wave). In fact, it might be possible to solve this issue by solving first an approximation of the crest of the cosine using  tweeked version of $e^{-x^2}$ and then, using that solution, do an infinite summation of that is reflected on the x-axis. T0 make things more clear, what I kind of had in mind with linear combinations was the following formula: $$\sum_{k} c_k e^{- \| x - w_k\|^2}$$ with $w_k$ as the movable centers because we might need to center them at the troughs and crests of the cosines. I am open to different tweet suggestions of this (for example, an improvement could be to including a precision $\beta_k$ or a standard deviation $\sigma_k$ on the exponent to adjust the widths to match the cosine better). I wasn't exactly sure if these were good ideas or if there were maybe better ways to approximate a cosine with $e^{-x^2}$, but I'd love to hear any ideas if people have better suggestions or know how to proceed (rigorously) with the ones I suggested.",,"['functional-analysis', 'approximation', 'approximation-theory']"
63,Compact linear operator,Compact linear operator,,"Today in lecture we were told that for a linear compact operator $T$ on an infinite-dimensional Hilbert space with infinite-dimensional range, we have that $\ker(T)^{\perp}$ is infinite-dimensional, too. Does anybody know why this is the case? I know that this is the same as the closure of the range of the adjoint operator, and in fact the adjoint operator is compact, too. But I don't know if this already tells us that this space is in fact infinite-dimensional?","Today in lecture we were told that for a linear compact operator $T$ on an infinite-dimensional Hilbert space with infinite-dimensional range, we have that $\ker(T)^{\perp}$ is infinite-dimensional, too. Does anybody know why this is the case? I know that this is the same as the closure of the range of the adjoint operator, and in fact the adjoint operator is compact, too. But I don't know if this already tells us that this space is in fact infinite-dimensional?",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory']"
64,How do I prove that there doesn't exist a unit norm vector at a unit distance from a closed subspace of an infinite dimensional vector space?,How do I prove that there doesn't exist a unit norm vector at a unit distance from a closed subspace of an infinite dimensional vector space?,,"Let $M$ be a proper closed linear sub space of a normed linear space $X$. If $X$ is finite dimensional, it's a well known result by F.Riesz that there exists a unit vector $x$ such that dist($x,M$)=$inf_{m\in M}\|x-m\|=1$. This need not be true if $X$ is infinite-dimensional. I have to show that the choice of $$ X=\{f\in C[0,1]:f(0)=0\}\\ M=\{f \in X: \int_{0}^1 f=0\} $$ provides a counter example. Can any one please help me with this ? For every unit norm function $f_0$ in $X$ , I tried designing a function $g_0 \in M$ such that $\|f_0-g_0\| < 1-\epsilon $ or $\|f_0-g_0\|>1+ \epsilon$, but haven't made much progress.","Let $M$ be a proper closed linear sub space of a normed linear space $X$. If $X$ is finite dimensional, it's a well known result by F.Riesz that there exists a unit vector $x$ such that dist($x,M$)=$inf_{m\in M}\|x-m\|=1$. This need not be true if $X$ is infinite-dimensional. I have to show that the choice of $$ X=\{f\in C[0,1]:f(0)=0\}\\ M=\{f \in X: \int_{0}^1 f=0\} $$ provides a counter example. Can any one please help me with this ? For every unit norm function $f_0$ in $X$ , I tried designing a function $g_0 \in M$ such that $\|f_0-g_0\| < 1-\epsilon $ or $\|f_0-g_0\|>1+ \epsilon$, but haven't made much progress.",,['functional-analysis']
65,$T:L^p \rightarrow L^p$ is bounded if it respects almost everywhere convergence,is bounded if it respects almost everywhere convergence,T:L^p \rightarrow L^p,"Let $T$ be a linear map from $L_p[0,1]$ to $L_p[0,1]$, $1\leq p < \infty$. If $(f_n)$ converges to $0$ almost everywhere, then $(T(f_n))$ converges to $0$ almost everywhere. How does this imply that $T$ is bounded? I have been trying to take a sequence $f_n$ converging to $0$ in $L_p$, then taking a subsequence $f_{n_k}$which is a.e convergent to $0$. Now, $Tf_{n_k}$ converges to $0$ a.e. Now,if I can obtain a dominating function for $Tf_{n_k}$, I could use the dominated convergence theorem. However, this still would not complete the proof, as this only shows there is a subsequence of $Tf_n$ converging to $0$ in $L_p$. Any hints?","Let $T$ be a linear map from $L_p[0,1]$ to $L_p[0,1]$, $1\leq p < \infty$. If $(f_n)$ converges to $0$ almost everywhere, then $(T(f_n))$ converges to $0$ almost everywhere. How does this imply that $T$ is bounded? I have been trying to take a sequence $f_n$ converging to $0$ in $L_p$, then taking a subsequence $f_{n_k}$which is a.e convergent to $0$. Now, $Tf_{n_k}$ converges to $0$ a.e. Now,if I can obtain a dominating function for $Tf_{n_k}$, I could use the dominated convergence theorem. However, this still would not complete the proof, as this only shows there is a subsequence of $Tf_n$ converging to $0$ in $L_p$. Any hints?",,['functional-analysis']
66,Functional differentiation involving the derivative of the function.,Functional differentiation involving the derivative of the function.,,"I've recently come across this functional: $F[f] = \int \frac{|\nabla f|^2}{f} dr$  (in 3D space, if that's relevant) and am interested in taking the functional derivative $\frac{\delta F}{\delta f}$. I've seen examples for when the integrand of $F$ is $|f'|^2$, where an integration by parts and suitable boundary conditions on $f$ gives the desired result, however it is not clear to me how this approach would generalize to my example, or in general where the integrand involves a derivative of $f$. Thank you for tips! Edit: this is my current approach, that I hope is correct: $ \begin{align} F[f+\delta f] &= \int{\frac{|\nabla (f+\delta f)|^2}{f+\delta f} dr}\\  &\approx\int{|\nabla (f+\delta f)|^2 \cdot \frac{1}{f} \left(1-\frac{\delta f}{f}+...\right) \;dr}\\  & = \int{\left[\frac{|\nabla f|^2}{f} - \frac{2}{f}|\nabla f|\cdot|\nabla(\delta f)| - \frac{|\nabla f|^2}{f^2}\delta f + O(|\delta f|^2)\right]dr} \end{align}$ Then, I need to integrate the term with $\nabla(\delta f)$ by parts (with suitable boundary conditions) to move the gradient operation onto the other term, collect the terms of order $\Delta f$, and identify the result multiplying $\delta f$ as $\frac{\delta F}{\delta f}$.","I've recently come across this functional: $F[f] = \int \frac{|\nabla f|^2}{f} dr$  (in 3D space, if that's relevant) and am interested in taking the functional derivative $\frac{\delta F}{\delta f}$. I've seen examples for when the integrand of $F$ is $|f'|^2$, where an integration by parts and suitable boundary conditions on $f$ gives the desired result, however it is not clear to me how this approach would generalize to my example, or in general where the integrand involves a derivative of $f$. Thank you for tips! Edit: this is my current approach, that I hope is correct: $ \begin{align} F[f+\delta f] &= \int{\frac{|\nabla (f+\delta f)|^2}{f+\delta f} dr}\\  &\approx\int{|\nabla (f+\delta f)|^2 \cdot \frac{1}{f} \left(1-\frac{\delta f}{f}+...\right) \;dr}\\  & = \int{\left[\frac{|\nabla f|^2}{f} - \frac{2}{f}|\nabla f|\cdot|\nabla(\delta f)| - \frac{|\nabla f|^2}{f^2}\delta f + O(|\delta f|^2)\right]dr} \end{align}$ Then, I need to integrate the term with $\nabla(\delta f)$ by parts (with suitable boundary conditions) to move the gradient operation onto the other term, collect the terms of order $\Delta f$, and identify the result multiplying $\delta f$ as $\frac{\delta F}{\delta f}$.",,"['functional-analysis', 'functional-calculus']"
67,Monotone convergence theorem in a special case,Monotone convergence theorem in a special case,,"Suppose $C^{+}[0,1]$ be the set of all continuous functions with   domain $[0,1]$ taking non-negative values only. Let $\lambda : C^{+}[0,1] \to [0,\infty)$ be a map that satisfies   $$\lambda(f+g)=\lambda(f)+\lambda(g)$$ Now suppose $\{g_k\}$ be a   sequence of functions in $C^{+}[0,1]$ that satisfies $$0\le g_1\le g_2\le \cdots, \ \ \ \  \ \ \ \ \ g_k \to g \in C^{+}[0,1]$$ Show that   $$\lim_{k\to\infty} \lambda(g_k)=\lambda(g)$$ My progress: I was able to show that $\lim\limits_{k\to\infty} \lambda(g_k)\le \lambda(g)$. So, I am trying to show $\lambda(g) \le \lim\limits_{k\to\infty} \lambda(g_k)$. I am stuck here. I have knowledge of the proof of actual monotone convergence theorem. There it uses the notion of simple function which are easy to handle and then they prove the general case by taking the supremum over simple functions. But note that simple functions do not belong to $C^{+}[0,1]$. So I think we cannot use the same technique. Any idea to tackle this one? Hints, links, solutions all are welcome.","Suppose $C^{+}[0,1]$ be the set of all continuous functions with   domain $[0,1]$ taking non-negative values only. Let $\lambda : C^{+}[0,1] \to [0,\infty)$ be a map that satisfies   $$\lambda(f+g)=\lambda(f)+\lambda(g)$$ Now suppose $\{g_k\}$ be a   sequence of functions in $C^{+}[0,1]$ that satisfies $$0\le g_1\le g_2\le \cdots, \ \ \ \  \ \ \ \ \ g_k \to g \in C^{+}[0,1]$$ Show that   $$\lim_{k\to\infty} \lambda(g_k)=\lambda(g)$$ My progress: I was able to show that $\lim\limits_{k\to\infty} \lambda(g_k)\le \lambda(g)$. So, I am trying to show $\lambda(g) \le \lim\limits_{k\to\infty} \lambda(g_k)$. I am stuck here. I have knowledge of the proof of actual monotone convergence theorem. There it uses the notion of simple function which are easy to handle and then they prove the general case by taking the supremum over simple functions. But note that simple functions do not belong to $C^{+}[0,1]$. So I think we cannot use the same technique. Any idea to tackle this one? Hints, links, solutions all are welcome.",,"['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lebesgue-measure']"
68,Estimate of a convolution from a paper by Michael Christ,Estimate of a convolution from a paper by Michael Christ,,"I don't understand Lemma 2 of the paper Hilbert transforms along curves, II: A flat case by Michael Christ. The situation is as follows. I slightly simplified it from the exact context in the source. Let $\zeta\in C_0^\infty([-1,1])$ and $\zeta\equiv 1$ on $[-1/2,1/2]$. Let $$a_k(\xi,\eta)=\zeta(2^{-k}\xi)\zeta(2^{-2k}\eta),$$ $$b_k=a_k-a_{k-1}.$$ Lemma . Let $f$ be supported on a rectangle $R$ of dimensions $(2^{-k},2^{-i})$  where $k\ge 0$ and $i\in \{2k,2k-1\}$ and have mean value zero. Then we have for all $0\le j\le k$,   $$\|f*\check{b}_j\|_1\le C 2^{j-k} \|f\|_1.$$ Here $\check{f}$ denotes the inverse Fourier transform of $f$. Note that $i,j,k$ always denote integers here. Christ says this Lemma follows in a ""routine fashion"" from Taylor's theorem and upper bounds for the derivatives of $b_j$. He doesn't give any more details. But I haven't been able to do it. I tried sneaking in a $\check{b}_j$ using mean value property of $f$ and then applying Taylor's formula, but it doesn't seem to work out right. Also I'm wondering if it matters that there is a $b_j$ in the Lemma or if the $a_j$ would actually work just as well. Can you please help me?","I don't understand Lemma 2 of the paper Hilbert transforms along curves, II: A flat case by Michael Christ. The situation is as follows. I slightly simplified it from the exact context in the source. Let $\zeta\in C_0^\infty([-1,1])$ and $\zeta\equiv 1$ on $[-1/2,1/2]$. Let $$a_k(\xi,\eta)=\zeta(2^{-k}\xi)\zeta(2^{-2k}\eta),$$ $$b_k=a_k-a_{k-1}.$$ Lemma . Let $f$ be supported on a rectangle $R$ of dimensions $(2^{-k},2^{-i})$  where $k\ge 0$ and $i\in \{2k,2k-1\}$ and have mean value zero. Then we have for all $0\le j\le k$,   $$\|f*\check{b}_j\|_1\le C 2^{j-k} \|f\|_1.$$ Here $\check{f}$ denotes the inverse Fourier transform of $f$. Note that $i,j,k$ always denote integers here. Christ says this Lemma follows in a ""routine fashion"" from Taylor's theorem and upper bounds for the derivatives of $b_j$. He doesn't give any more details. But I haven't been able to do it. I tried sneaking in a $\check{b}_j$ using mean value property of $f$ and then applying Taylor's formula, but it doesn't seem to work out right. Also I'm wondering if it matters that there is a $b_j$ in the Lemma or if the $a_j$ would actually work just as well. Can you please help me?",,"['real-analysis', 'functional-analysis', 'convolution', 'harmonic-analysis']"
69,"If $M$ is a non-empty subset of a Hilbert space $H$, the span of $M$ is dense in $H$ iff $M^{\perp} = \{0\}$.","If  is a non-empty subset of a Hilbert space , the span of  is dense in  iff .",M H M H M^{\perp} = \{0\},"I need help only with the converse of the proof. Suppose, $M^{\perp} = \{0\}$, and $V=span(M)$, then if $x \perp V$, this implies $x \perp M$ (why?) so that $x \in M^{\perp}$ and $x=0$. Hence $V^{\perp} = \{0\}$ (again why?). Noting that $V$ is a subspace of $H$, we obtain $\bar{V}=H.$ The last equality is obtained from $H= \bar{V}\oplus \bar{V}^{\perp}$, I think, but I don't understand this very well either. Please explain the logic behind the steps indicated with a (why?) that I'm having trouble with. Do we have that $V$ is closed? Also, Is this true that $V^{\perp \perp} = \bar{V}$? How do I you prove this? Thanks for all your help. Edit: I've proven this in the following way and need to know if this is correct: $V^{\perp}$ is closed, $\bar{V}$ is a closed subspace of $H$. Then $H = \bar{V} \oplus \bar{V^{\perp}} = \bar{V} \oplus V^{\perp}=\bar{V} \oplus \{0\}$ implies $H = \bar{V}$.","I need help only with the converse of the proof. Suppose, $M^{\perp} = \{0\}$, and $V=span(M)$, then if $x \perp V$, this implies $x \perp M$ (why?) so that $x \in M^{\perp}$ and $x=0$. Hence $V^{\perp} = \{0\}$ (again why?). Noting that $V$ is a subspace of $H$, we obtain $\bar{V}=H.$ The last equality is obtained from $H= \bar{V}\oplus \bar{V}^{\perp}$, I think, but I don't understand this very well either. Please explain the logic behind the steps indicated with a (why?) that I'm having trouble with. Do we have that $V$ is closed? Also, Is this true that $V^{\perp \perp} = \bar{V}$? How do I you prove this? Thanks for all your help. Edit: I've proven this in the following way and need to know if this is correct: $V^{\perp}$ is closed, $\bar{V}$ is a closed subspace of $H$. Then $H = \bar{V} \oplus \bar{V^{\perp}} = \bar{V} \oplus V^{\perp}=\bar{V} \oplus \{0\}$ implies $H = \bar{V}$.",,['functional-analysis']
70,"How to prove that $(C[a,b], \|\cdot\|_\infty)$ is not a reflexive Banach Space [duplicate]",How to prove that  is not a reflexive Banach Space [duplicate],"(C[a,b], \|\cdot\|_\infty)","This question already has answers here : The space $C^0([0;1],\mathbb{R})$ of all continuous, real-valued functions on $[0,1]$ is not reflexive. (3 answers) Closed 9 years ago . The tag line basically says it all...this is a question in Luenberger's Optimization book (5.14.4 on p.138).  Clearly I don't expect someone to deliver a full proof if it's tedious, but a sketch or theorem or tactic would help...I tried: characterizing the dual of $BV[a,b].$ finding a sequence $\{f_n\}$ of continuous functions without a weakly convergent subsequence. Using the Hahn-Banach theorem to extend a functional on $BV[a,b]$. Any help? Thanks!","This question already has answers here : The space $C^0([0;1],\mathbb{R})$ of all continuous, real-valued functions on $[0,1]$ is not reflexive. (3 answers) Closed 9 years ago . The tag line basically says it all...this is a question in Luenberger's Optimization book (5.14.4 on p.138).  Clearly I don't expect someone to deliver a full proof if it's tedious, but a sketch or theorem or tactic would help...I tried: characterizing the dual of $BV[a,b].$ finding a sequence $\{f_n\}$ of continuous functions without a weakly convergent subsequence. Using the Hahn-Banach theorem to extend a functional on $BV[a,b]$. Any help? Thanks!",,"['real-analysis', 'functional-analysis', 'banach-spaces', 'duality-theorems']"
71,"Existence of a minimizer for $\int_0^1|P(t)|\,{\rm d}t$.",Existence of a minimizer for .,"\int_0^1|P(t)|\,{\rm d}t","Let $m > 0$ be a fixed integer. Show that among all the polynomials $P \in \Bbb C[X]$ with degree $\leq m$ and with $P(0)=1$, there is one that makes minimum the value $\int_0^1|P(t)|\,{\rm d}t$. My attempt: Let ${\cal P}^m$ be the set of all the polynomials in $\Bbb C[X]$ with $P(0)=1$. I am aware that $$\|P\| = \int_0^1 |P(t)|\,{\rm d}t$$ is continuous because it is a norm. Since ${\cal P}^m$ and $\Bbb R$ have finite dimension, and $T: {\cal P}^m \to \Bbb R$ given by $T(P) = P(0)$ is linear, $T$ is continuous, so: $$ \{ P \in {\cal P}^m \mid P(0) =  1 \} = T^{-1}(\{1\})$$ is a closed set. If I could show that it is bounded, we would have compactness (because of finite dimension) and I could conclude the result by Weierstrass' extremum theorem. But if you make $m=1$ and look at $T$ as a projection, it is clear that the set is not compact, so I don't think I am in the right way. Is there a way to save my work so far? If someone can give me hints it's also ok. Thanks.","Let $m > 0$ be a fixed integer. Show that among all the polynomials $P \in \Bbb C[X]$ with degree $\leq m$ and with $P(0)=1$, there is one that makes minimum the value $\int_0^1|P(t)|\,{\rm d}t$. My attempt: Let ${\cal P}^m$ be the set of all the polynomials in $\Bbb C[X]$ with $P(0)=1$. I am aware that $$\|P\| = \int_0^1 |P(t)|\,{\rm d}t$$ is continuous because it is a norm. Since ${\cal P}^m$ and $\Bbb R$ have finite dimension, and $T: {\cal P}^m \to \Bbb R$ given by $T(P) = P(0)$ is linear, $T$ is continuous, so: $$ \{ P \in {\cal P}^m \mid P(0) =  1 \} = T^{-1}(\{1\})$$ is a closed set. If I could show that it is bounded, we would have compactness (because of finite dimension) and I could conclude the result by Weierstrass' extremum theorem. But if you make $m=1$ and look at $T$ as a projection, it is clear that the set is not compact, so I don't think I am in the right way. Is there a way to save my work so far? If someone can give me hints it's also ok. Thanks.",,"['analysis', 'functional-analysis']"
72,"Showing Sobolev space $W^{1,2}$ is a Hilbert space",Showing Sobolev space  is a Hilbert space,"W^{1,2}","I have the Sobolev space $W^{1,2}$ consisting of all continuous functions $f \in L^2(\mathbb{R})$ such that there exists an $f'$ with $f(b) - f(a) = \int_a ^b f'(t) dt$. $W^{1,2}$ has inner product $<f,g> = \int fg + f'g'$. I'm trying to show this is a Hilbert space but I'm having some difficulties. I can show that as $t \to+/-  \infty$ we must have $f(t) \to 0$ because $f$ is continuous. From this and using the relation $f(x)^2 = \int_{-\infty} ^x f(t)f'(t)$ I can obtain the estimate $||f||_{\infty} \le K||f||_{1,2}$ If we now take a sequence $f_n$ in $W^{1,2}$ such that $f_n \to f$ in $W^{1,2}$ norm. I want to show that in fact $f$ is in $W^{1,2}$. We know from the above estimate that $f_n \to f$ uniformly and hence $f$ is continuous and in $L^2$ so it remains to show that $f$ has the weak derivative property. I'm struggling to show this last part. I know that for each $f_n$ there is an $f_n'$ such that $f_n(b) - f_n(a) = \int_a ^b f_n'(t) dt$ but how can we show that these $f_n'$ converge to a derivative for $f$? Thanks for any help here!","I have the Sobolev space $W^{1,2}$ consisting of all continuous functions $f \in L^2(\mathbb{R})$ such that there exists an $f'$ with $f(b) - f(a) = \int_a ^b f'(t) dt$. $W^{1,2}$ has inner product $<f,g> = \int fg + f'g'$. I'm trying to show this is a Hilbert space but I'm having some difficulties. I can show that as $t \to+/-  \infty$ we must have $f(t) \to 0$ because $f$ is continuous. From this and using the relation $f(x)^2 = \int_{-\infty} ^x f(t)f'(t)$ I can obtain the estimate $||f||_{\infty} \le K||f||_{1,2}$ If we now take a sequence $f_n$ in $W^{1,2}$ such that $f_n \to f$ in $W^{1,2}$ norm. I want to show that in fact $f$ is in $W^{1,2}$. We know from the above estimate that $f_n \to f$ uniformly and hence $f$ is continuous and in $L^2$ so it remains to show that $f$ has the weak derivative property. I'm struggling to show this last part. I know that for each $f_n$ there is an $f_n'$ such that $f_n(b) - f_n(a) = \int_a ^b f_n'(t) dt$ but how can we show that these $f_n'$ converge to a derivative for $f$? Thanks for any help here!",,"['functional-analysis', 'sobolev-spaces']"
73,What is the range of the operator $T$ I mean I want to determine $R(T)$,What is the range of the operator  I mean I want to determine,T R(T),"Given the normed space $\ell^\infty$ of all bounded sequences of (real or complex) numbers with the norm given by $$||x||:= \sup_{j\in Z^+} |\xi_j|,$$ for each $x:=(\xi_j)_{j=1}^\infty$ in $\ell^\infty$, and given the linear operator $T \colon \ell^\infty \to \ell^\infty$ defined as $$T(\xi_j)_{j=1}^\infty  := (\frac{\xi_j}{j})_{j=1}^\infty,$$  What  is the range of the operator $T$ I mean I want to determine $R(T)$","Given the normed space $\ell^\infty$ of all bounded sequences of (real or complex) numbers with the norm given by $$||x||:= \sup_{j\in Z^+} |\xi_j|,$$ for each $x:=(\xi_j)_{j=1}^\infty$ in $\ell^\infty$, and given the linear operator $T \colon \ell^\infty \to \ell^\infty$ defined as $$T(\xi_j)_{j=1}^\infty  := (\frac{\xi_j}{j})_{j=1}^\infty,$$  What  is the range of the operator $T$ I mean I want to determine $R(T)$",,['functional-analysis']
74,Spectrum of an operator is approximate point spectrum plus spectrum of dual operator,Spectrum of an operator is approximate point spectrum plus spectrum of dual operator,,I'm trying to show that given an operator $T \in B(X)$ with $X$ Banach we have $$\sigma(T) = \sigma_{ap}(T) \cup \sigma_p(T') $$ Where $T' \in B(X')$ is the dual operator. I know that $\sigma(T) = \sigma(T')$ and certainly $\sigma_{ap}(T) \subset \sigma(T)$ so we have $\sigma_{ap}(T) \cup \sigma_p(T') \subset \sigma(T)$ and I'm struggling with the other inclusion. If we write $R_{\lambda} = {\lambda}I - T$ and now we want to show if $\lambda$ is outside of $ \sigma_{ap}(T) \cup \sigma_p(T')$ then $R_{\lambda}$ is invertible. Here is what I have so far: $R_{\lambda}$ has closed image. ( $\lambda$ is not in the approx.  point spectrum of $T$ ) $R_{\lambda}$ is injective - the point spectrum of $T$ is contained in the approximate point spectrum of $T$ . I'd like to show that $R_{\lambda}$ has dense image. We know that $\ker(R_{\lambda}')$ is empty - so if $g \in Y'$ is such that $g(R_{\lambda})(x) = 0$ then $g$ must be zero. I'd like to maybe use the Hahn-Banach theorem on an element in $X$ not in the closure of $\operatorname{im} R_{\lambda}$ or perhaps by extending a functional on $\operatorname{im}R_{\lambda}$ but unfortunately I can't seem to get anywhere with this! Thanks for any help!,I'm trying to show that given an operator with Banach we have Where is the dual operator. I know that and certainly so we have and I'm struggling with the other inclusion. If we write and now we want to show if is outside of then is invertible. Here is what I have so far: has closed image. ( is not in the approx.  point spectrum of ) is injective - the point spectrum of is contained in the approximate point spectrum of . I'd like to show that has dense image. We know that is empty - so if is such that then must be zero. I'd like to maybe use the Hahn-Banach theorem on an element in not in the closure of or perhaps by extending a functional on but unfortunately I can't seem to get anywhere with this! Thanks for any help!,T \in B(X) X \sigma(T) = \sigma_{ap}(T) \cup \sigma_p(T')  T' \in B(X') \sigma(T) = \sigma(T') \sigma_{ap}(T) \subset \sigma(T) \sigma_{ap}(T) \cup \sigma_p(T') \subset \sigma(T) R_{\lambda} = {\lambda}I - T \lambda  \sigma_{ap}(T) \cup \sigma_p(T') R_{\lambda} R_{\lambda} \lambda T R_{\lambda} T T R_{\lambda} \ker(R_{\lambda}') g \in Y' g(R_{\lambda})(x) = 0 g X \operatorname{im} R_{\lambda} \operatorname{im}R_{\lambda},"['functional-analysis', 'spectral-theory', 'dual-maps']"
75,Operator norm and L infinity norm,Operator norm and L infinity norm,,"$g \in L^\infty[0,1]$, $F: L^1[0,1] \to R$ by $F(f) = \int_{[0,1]}fg$, show that $\lVert F\rVert = \lVert g \rVert_\infty$. I can prove that $\lVert F\rVert \le \lVert g \rVert_\infty$ simply by $\int \lvert fg \rvert \le \lVert g \rVert_\infty \int \lvert f \rvert $. Now it remains to find an $f$, such that $\lVert f \rVert_1 = 1$ and $\lvert F(f) \rvert =  \lVert g \rVert_\infty$. I just cannot come up with such a case, can someone give me some hints?","$g \in L^\infty[0,1]$, $F: L^1[0,1] \to R$ by $F(f) = \int_{[0,1]}fg$, show that $\lVert F\rVert = \lVert g \rVert_\infty$. I can prove that $\lVert F\rVert \le \lVert g \rVert_\infty$ simply by $\int \lvert fg \rvert \le \lVert g \rVert_\infty \int \lvert f \rvert $. Now it remains to find an $f$, such that $\lVert f \rVert_1 = 1$ and $\lvert F(f) \rvert =  \lVert g \rVert_\infty$. I just cannot come up with such a case, can someone give me some hints?",,"['real-analysis', 'functional-analysis', 'normed-spaces']"
76,Confusion about Lusin's Theorem.,Confusion about Lusin's Theorem.,,"I saw a proof which heavily relied on Lusin's Theorem recently, and I was hoping someone might be able to help me fill in the detail as to why this theorem allows for a particular creation. (Lusin's Theorem) If $f : [a, b] \rightarrow \mathbb{C}$ is Lebesgue measurable and $\epsilon > 0$, there is a compact set $E \subset [a,b]$ such that $\mu(E^c) < \epsilon$ and $ f $ is continuous on $E$. The line in the proof I read was as follows, ""Suppose $f \in L^\infty$ is given, we can apply Lusin's theorem infinitely many times to construct a sequence $(f_n)$ in $C[0,1]$ uniformly bounded by $\|f\|_\infty$ such that the measure of the set where $f$ and $f_n$ are unequal less than $\frac{1}{n}$."" The requirement for this theorem is the existence of a compact set for continuity, but the set may only be a single point. Let's just say we had the measurable function \begin{equation} f(x) = \begin{cases} 1 &\mbox{if } x\in \mathbb{Q} \\  0 & \mbox{if } x \not \in Q. \end{cases} \end{equation} What would this creation $(f_n)$ look like, and how does it follow that this sequence will be continuous on $[0,1]$.","I saw a proof which heavily relied on Lusin's Theorem recently, and I was hoping someone might be able to help me fill in the detail as to why this theorem allows for a particular creation. (Lusin's Theorem) If $f : [a, b] \rightarrow \mathbb{C}$ is Lebesgue measurable and $\epsilon > 0$, there is a compact set $E \subset [a,b]$ such that $\mu(E^c) < \epsilon$ and $ f $ is continuous on $E$. The line in the proof I read was as follows, ""Suppose $f \in L^\infty$ is given, we can apply Lusin's theorem infinitely many times to construct a sequence $(f_n)$ in $C[0,1]$ uniformly bounded by $\|f\|_\infty$ such that the measure of the set where $f$ and $f_n$ are unequal less than $\frac{1}{n}$."" The requirement for this theorem is the existence of a compact set for continuity, but the set may only be a single point. Let's just say we had the measurable function \begin{equation} f(x) = \begin{cases} 1 &\mbox{if } x\in \mathbb{Q} \\  0 & \mbox{if } x \not \in Q. \end{cases} \end{equation} What would this creation $(f_n)$ look like, and how does it follow that this sequence will be continuous on $[0,1]$.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'continuity']"
77,how to prove the limit and continuity?,how to prove the limit and continuity?,,"For a closed set $F\subseteq \mathbb{R}^{k}$, define $$f_n (x) := \max\{1-n d(x,F),0\}.$$ Show that $f_n \downarrow I_F$ and that $f_n$ is continuous and bounded. I am trying to use $f_{n}(x)=\frac{1-nd(x,F)+|1-nd(x,F)|}{2}$.  For $x\in F$, $f_{n}(x)\rightarrow 1$ for $n\rightarrow\infty$. For $x\notin F$, how can I get that? Also for continuity, it would be more complicated. Can someone give hints?","For a closed set $F\subseteq \mathbb{R}^{k}$, define $$f_n (x) := \max\{1-n d(x,F),0\}.$$ Show that $f_n \downarrow I_F$ and that $f_n$ is continuous and bounded. I am trying to use $f_{n}(x)=\frac{1-nd(x,F)+|1-nd(x,F)|}{2}$.  For $x\in F$, $f_{n}(x)\rightarrow 1$ for $n\rightarrow\infty$. For $x\notin F$, how can I get that? Also for continuity, it would be more complicated. Can someone give hints?",,"['functional-analysis', 'multivariable-calculus']"
78,"Compute $\| T \|$ with the norm $\| \cdot \| := \max_{j=1,\ldots,n} (|\cdot_j|)$",Compute  with the norm,"\| T \| \| \cdot \| := \max_{j=1,\ldots,n} (|\cdot_j|)","Let $A \colon = [\alpha_{ij}]_{m\times n}$ be a given $m \times n$ matrix of real numbers. Let $\mathbb{R}^n$ be the normed space of all ordered $n$-tuples of real numbers with the norm defined as follows:  $$\Vert x \Vert_{\mathbb{R}^n} \colon= \max_{j=1, \ldots, n} \left( \vert \xi_j \vert \right) \ \ \ \forall x \colon= (\xi_1, \ldots, \xi_n) \in \mathbb{R}^n.$$ Let $\mathbb{R}^m$ be the normed space of all ordered $m$-tuples of real numbers with the norm defined as follows:  $$\Vert y \Vert_{\mathbb{R}^m} \colon= \max_{i=1, \ldots, m} \left( \vert \eta_i \vert \right) \ \ \ \forall y \colon= (\eta_1, \ldots, \eta_m) \in \mathbb{R}^m.$$ Let the operator $T \colon \mathbb{R}^n \to \mathbb{R}^m$ be defined as  $$T(x) \colon= Ax \ \ \ \forall x \in \mathbb{R}^n;$$ where $x$ and $y$ are to be written as column vectors and $Ax$ denotes the usual matrix product. Of course, $T$ is linear. What is $\Vert T \Vert$? Here we are using the following definition for $\Vert T \Vert$:  $$\Vert T \Vert \colon= \sup \left\{ \ \frac{\Vert T(x)\vert_{\mathbb{R}^m}}{\Vert x \Vert_{\mathbb{R}^n}} \ \colon \ x \in \mathbb{R}^n, \ x \neq \theta_{\mathbb{R}^n} \ \right\}. $$ Here $\theta_{\mathbb{R}^n} $ denotes the zero vector in $\mathbb{R}^n$. My effort: For any $x \colon= (\xi_1, \ldots, \xi_n ) \in \mathbb{R}^n$, we have  \begin{eqnarray*} \Vert T(x) \Vert_{\mathbb{R}^m}  &=& \max_{i=1, \ldots, m} \left( \left\vert \sum_{j=1}^n \alpha_{ij} \xi_j \right\vert \right) \\  & \leq & \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \xi_j \vert \right) \\  &=&  \max_{i=1, \ldots, m} \left( \sum_{j=1}^n  \left( \vert \alpha_{ij} \vert \cdot \vert \xi_j \vert \right) \right) \\  &\leq & \Vert x \Vert_{\mathbb{R}^n} \cdot \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \vert \right).  \end{eqnarray*} If $x$ is not the zero vector, then upon deviding by the norm of $x$ and then taking the supremum of the quantity on the left hand side, we obtain $$\Vert T \Vert \leq \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \vert \right). $$ How to show that  $$\Vert T \Vert = \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \vert \right)? $$","Let $A \colon = [\alpha_{ij}]_{m\times n}$ be a given $m \times n$ matrix of real numbers. Let $\mathbb{R}^n$ be the normed space of all ordered $n$-tuples of real numbers with the norm defined as follows:  $$\Vert x \Vert_{\mathbb{R}^n} \colon= \max_{j=1, \ldots, n} \left( \vert \xi_j \vert \right) \ \ \ \forall x \colon= (\xi_1, \ldots, \xi_n) \in \mathbb{R}^n.$$ Let $\mathbb{R}^m$ be the normed space of all ordered $m$-tuples of real numbers with the norm defined as follows:  $$\Vert y \Vert_{\mathbb{R}^m} \colon= \max_{i=1, \ldots, m} \left( \vert \eta_i \vert \right) \ \ \ \forall y \colon= (\eta_1, \ldots, \eta_m) \in \mathbb{R}^m.$$ Let the operator $T \colon \mathbb{R}^n \to \mathbb{R}^m$ be defined as  $$T(x) \colon= Ax \ \ \ \forall x \in \mathbb{R}^n;$$ where $x$ and $y$ are to be written as column vectors and $Ax$ denotes the usual matrix product. Of course, $T$ is linear. What is $\Vert T \Vert$? Here we are using the following definition for $\Vert T \Vert$:  $$\Vert T \Vert \colon= \sup \left\{ \ \frac{\Vert T(x)\vert_{\mathbb{R}^m}}{\Vert x \Vert_{\mathbb{R}^n}} \ \colon \ x \in \mathbb{R}^n, \ x \neq \theta_{\mathbb{R}^n} \ \right\}. $$ Here $\theta_{\mathbb{R}^n} $ denotes the zero vector in $\mathbb{R}^n$. My effort: For any $x \colon= (\xi_1, \ldots, \xi_n ) \in \mathbb{R}^n$, we have  \begin{eqnarray*} \Vert T(x) \Vert_{\mathbb{R}^m}  &=& \max_{i=1, \ldots, m} \left( \left\vert \sum_{j=1}^n \alpha_{ij} \xi_j \right\vert \right) \\  & \leq & \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \xi_j \vert \right) \\  &=&  \max_{i=1, \ldots, m} \left( \sum_{j=1}^n  \left( \vert \alpha_{ij} \vert \cdot \vert \xi_j \vert \right) \right) \\  &\leq & \Vert x \Vert_{\mathbb{R}^n} \cdot \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \vert \right).  \end{eqnarray*} If $x$ is not the zero vector, then upon deviding by the norm of $x$ and then taking the supremum of the quantity on the left hand side, we obtain $$\Vert T \Vert \leq \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \vert \right). $$ How to show that  $$\Vert T \Vert = \max_{i=1, \ldots, m} \left( \sum_{j=1}^n \vert \alpha_{ij} \vert \right)? $$",,"['real-analysis', 'analysis', 'functional-analysis', 'normed-spaces']"
79,Green's Function Ode 1,Green's Function Ode 1,,Can any one help me to how to find a) Green's function for both of them and how can i solve these by using Greens function 1)$\frac{d}{dt}(\frac{1}{t+1}\frac{dy}{dt})=f(t); \quad   y(0)=y(1)=0$ 2)$\frac{d}{dt}((t+1)\frac{dy}{dt})=f(t); \quad   y(0)=1;y(1)=-1$,Can any one help me to how to find a) Green's function for both of them and how can i solve these by using Greens function 1)$\frac{d}{dt}(\frac{1}{t+1}\frac{dy}{dt})=f(t); \quad   y(0)=y(1)=0$ 2)$\frac{d}{dt}((t+1)\frac{dy}{dt})=f(t); \quad   y(0)=1;y(1)=-1$,,"['analysis', 'functional-analysis']"
80,Relation between residual spectrum and point spectrum.,Relation between residual spectrum and point spectrum.,,"Suppose T is a bounded operator on a Hilbert space. Show that if λ is in the residual spectrum of T, then $\bar{λ}$ is in the point spectrum of the adjoint. Here is what I think needs to be done.  We know that $\langle Tu,v\rangle = \langle u,T^*v\rangle = \overline{\langle T^*v,u\rangle}$.  Does that help with connecting $\lambda$ with $\overline{\lambda}$?","Suppose T is a bounded operator on a Hilbert space. Show that if λ is in the residual spectrum of T, then $\bar{λ}$ is in the point spectrum of the adjoint. Here is what I think needs to be done.  We know that $\langle Tu,v\rangle = \langle u,T^*v\rangle = \overline{\langle T^*v,u\rangle}$.  Does that help with connecting $\lambda$ with $\overline{\lambda}$?",,"['functional-analysis', 'hilbert-spaces', 'spectral-theory']"
81,"Weak convergence in $L^q$ but not in $L^p$, where $1\leq p<q<\infty$?","Weak convergence in  but not in , where ?",L^q L^p 1\leq p<q<\infty,"I've been trying to find a sequence $\{f_n\}$ of functions on $[0,1]$ that converges weakly in $L^q$ but does not converge weakly in $L^p$, where $1\leq p<q<\infty$. I'm stuck and any hints would be greatly appreciated. (I find this to be tricky because the sequences are defined on $[0,1]$ and the functions cannot spread to infinity. Since we're working on $[0,1]$, we have $L^q\subset L^p$.) The functions of the example below are not supported in $[0,1]$.","I've been trying to find a sequence $\{f_n\}$ of functions on $[0,1]$ that converges weakly in $L^q$ but does not converge weakly in $L^p$, where $1\leq p<q<\infty$. I'm stuck and any hints would be greatly appreciated. (I find this to be tricky because the sequences are defined on $[0,1]$ and the functions cannot spread to infinity. Since we're working on $[0,1]$, we have $L^q\subset L^p$.) The functions of the example below are not supported in $[0,1]$.",,"['real-analysis', 'analysis', 'functional-analysis', 'lp-spaces']"
82,sub-Markovianity and extensions of $L^2$ semigroup contractions to $L^p$,sub-Markovianity and extensions of  semigroup contractions to,L^2 L^p,"I just read that a contraction semigroup $(T_t)_{t\geq 0}$ on $L^2(X,m;\mathbb{R})$ space can be extended to a contraction semigroup on $L^p$ for any $p\geq 2$ provided that it satisfies the sub-Markovian property, i.e., $0\leq f\leq 1 \rightarrow 0\leq T_tf\leq 1$.  I understand that this is easy if one can associate a transition function to $T_t$ but is it possible to show this if we do not know that we can do that? I would appreciate all suggestions!","I just read that a contraction semigroup $(T_t)_{t\geq 0}$ on $L^2(X,m;\mathbb{R})$ space can be extended to a contraction semigroup on $L^p$ for any $p\geq 2$ provided that it satisfies the sub-Markovian property, i.e., $0\leq f\leq 1 \rightarrow 0\leq T_tf\leq 1$.  I understand that this is easy if one can associate a transition function to $T_t$ but is it possible to show this if we do not know that we can do that? I would appreciate all suggestions!",,"['functional-analysis', 'semigroup-of-operators']"
83,"Generalising the cross product to infinite dimensions, does $v \times v = 0$ hold also in infinite dimensional spaces","Generalising the cross product to infinite dimensions, does  hold also in infinite dimensional spaces",v \times v = 0,"Consider I have a vector space $V$ with inner product and a bilinear map $b : V \times V \to V$ i)  such that if $z = b(u,v)$ for two $u,v \in V$, then $$  z \perp u \quad \mbox{ and } \quad z \perp v. $$ ii) if $u, v \in V$ are perpendicular, i.e. $u \perp v$, then $$  ||b(u,v)|| = ||u||||v||. $$ These definitions are motivated by an axiomatic introduction of the cross product, see here . Now I want to show that for every $v \in V$ we have $b(v,v) = 0$. If $V$ is finite-dimensional, then this follows by the fact that in an $n$-dimensional space, if $\{v_1, \ldots, v_m\}$ are orthogonal and non-zero, then $m \le n$ (because orthogonality of non-zero vectors implies linear independence). For if $v \in V, v \ne 0$ then the vectors $$  M = \{ v, b(v,v), b(v,b(v,v)), \ldots, b(v, b(v, \ldots, b(v,v))) \} $$ are all orthgonal, to simplify notation suppose we have a $n = 4$ dimensional space. Then  $$  b(v, b(v, b(v, b(v, v))) = 0 $$ which implies by ii) \begin{align*}  0 & = ||b(v, b(v, b(v, b(v,v)))|| \\    & = ||v|| ||b(v, b(v, b(v,v)))|| \\    & = ||v|| ||v|| ||b(v, b(v,v))|| \\    & = ||v|| ||v|| ||v|| ||b(v,v)|| \end{align*} which implies $||b(v,v)|| = 0$, because $v \ne 0$, which implies that $b(v,v)$ is the zero vector. But does this also hold if $V$ is infinite-dimensional, if not can you give an example were it fails?","Consider I have a vector space $V$ with inner product and a bilinear map $b : V \times V \to V$ i)  such that if $z = b(u,v)$ for two $u,v \in V$, then $$  z \perp u \quad \mbox{ and } \quad z \perp v. $$ ii) if $u, v \in V$ are perpendicular, i.e. $u \perp v$, then $$  ||b(u,v)|| = ||u||||v||. $$ These definitions are motivated by an axiomatic introduction of the cross product, see here . Now I want to show that for every $v \in V$ we have $b(v,v) = 0$. If $V$ is finite-dimensional, then this follows by the fact that in an $n$-dimensional space, if $\{v_1, \ldots, v_m\}$ are orthogonal and non-zero, then $m \le n$ (because orthogonality of non-zero vectors implies linear independence). For if $v \in V, v \ne 0$ then the vectors $$  M = \{ v, b(v,v), b(v,b(v,v)), \ldots, b(v, b(v, \ldots, b(v,v))) \} $$ are all orthgonal, to simplify notation suppose we have a $n = 4$ dimensional space. Then  $$  b(v, b(v, b(v, b(v, v))) = 0 $$ which implies by ii) \begin{align*}  0 & = ||b(v, b(v, b(v, b(v,v)))|| \\    & = ||v|| ||b(v, b(v, b(v,v)))|| \\    & = ||v|| ||v|| ||b(v, b(v,v))|| \\    & = ||v|| ||v|| ||v|| ||b(v,v)|| \end{align*} which implies $||b(v,v)|| = 0$, because $v \ne 0$, which implies that $b(v,v)$ is the zero vector. But does this also hold if $V$ is infinite-dimensional, if not can you give an example were it fails?",,"['linear-algebra', 'functional-analysis', 'orthogonality', 'cross-product']"
84,Schwartz functions & differentiation under the integral sign.,Schwartz functions & differentiation under the integral sign.,,"Let $\mathscr{S}(\mathbb{R}^n)$ denote the space of Schwartz functions. That is $$ \mathscr{S}(\mathbb{R}^n) = \left\{ f \in C^\infty(\mathbb{R}^n, \mathbb{R}) ~\colon \sup_{x\in\mathbb{R}^n} \left|x^\alpha \partial_\beta f(x) \right| < \infty \quad \forall \alpha, \beta \right\} $$ where $\alpha$ and $\beta$ are multi-indices. Take $f(t, x) \in \mathscr{S}(\mathbb{R}^{n+1})$ and define $F ~\colon \mathbb{R} \to \mathbb{R}$ by $$ F(t) = \int_{\mathbb{R}^n} f(t, x) ~\mathrm{d}t $$ I would like to know when it is valid to compute $F'(t)$ by ""differentiation under the integral sign"". In other words, when is it justifiable to say $$ F'(t)  = \partial_t \int_{\mathbb{R}^n} f(t, x) ~ \mathrm{d}t =  \int_{\mathbb{R}^n} \partial_t f(t, x) ~ \mathrm{d}t $$ I'm think I am able to show this when I have a few extra conditions, say if I know that $\partial_t f(t, x)$ is a decreasing function of $t$ for all fixed $x$. However, I feel that it should be true in general and perhaps I'm just missing something important. Any help will be appreciated.","Let $\mathscr{S}(\mathbb{R}^n)$ denote the space of Schwartz functions. That is $$ \mathscr{S}(\mathbb{R}^n) = \left\{ f \in C^\infty(\mathbb{R}^n, \mathbb{R}) ~\colon \sup_{x\in\mathbb{R}^n} \left|x^\alpha \partial_\beta f(x) \right| < \infty \quad \forall \alpha, \beta \right\} $$ where $\alpha$ and $\beta$ are multi-indices. Take $f(t, x) \in \mathscr{S}(\mathbb{R}^{n+1})$ and define $F ~\colon \mathbb{R} \to \mathbb{R}$ by $$ F(t) = \int_{\mathbb{R}^n} f(t, x) ~\mathrm{d}t $$ I would like to know when it is valid to compute $F'(t)$ by ""differentiation under the integral sign"". In other words, when is it justifiable to say $$ F'(t)  = \partial_t \int_{\mathbb{R}^n} f(t, x) ~ \mathrm{d}t =  \int_{\mathbb{R}^n} \partial_t f(t, x) ~ \mathrm{d}t $$ I'm think I am able to show this when I have a few extra conditions, say if I know that $\partial_t f(t, x)$ is a decreasing function of $t$ for all fixed $x$. However, I feel that it should be true in general and perhaps I'm just missing something important. Any help will be appreciated.",,"['real-analysis', 'integration', 'functional-analysis', 'partial-differential-equations']"
85,Compact operators and essential spectral radius,Compact operators and essential spectral radius,,"Let $E$ be an infinite-dimensional complex Banach space. Let $\mathcal{L} (E)$ be the space of endomorphisms of $E$, endowed with the operator norm. Then $\mathcal{L} (E)$ is a unital Banach algebra for the usual operator composition. Let $\mathcal{K} (E)$ be the subspace of compact operators. It is well-know that: $\mathcal{K} (E)$ is a non-trivial closed vector subspace of $\mathcal{L} (E)$; $\mathcal{K} (E)$ is a bilateral ideal of $\mathcal{L} (E)$. From the first property we deduce that $\mathcal{H} (E) := \mathcal{L} (E) / \mathcal{K} (E)$ is a Banach space for the norm $\| \tilde{T}\|_{\mathcal{H} (E)} = \inf \{\|T\|_{\mathcal{L} (E)}: \ \pi (T) = \tilde{T}\}$, where $\pi$ is the canonical projection. From the second property we deduce that the composition goes down to the quotient, and gives it a structure of unital Banach algebra. On the other hand, Wikipedia gives a few possible definitions for the essential spectrum $\sigma_{ess}$ of an operator, some of which are mentioned to be invariant under compact perturbations. Let $T \in \mathcal{L} (E)$. Is $\sigma (\pi(T))$ equal to $\sigma_{ess, k} (T)$ (see the Wikipedia page for the definition) for some $1 \leq k \leq 4$? If so, which $k$? Otherwise, do we still have $\rho_{ess} (T) = \rho(\pi(T))$, where $\rho$ (resp. $\rho_{ess}$) is the spctral radius (resp. the essential spectral radius)? Motivation: I've been working with some families of quasi-compact operators for some time now. I've never seen the essential spectrum or essential spectral radius presented this way. If this works, it could make for a pretty nice introduction to the subject if I ever need to teach it. Oh, and unfortunately I don't have the references on the Wikipedia page at hand now, so I can't check them.","Let $E$ be an infinite-dimensional complex Banach space. Let $\mathcal{L} (E)$ be the space of endomorphisms of $E$, endowed with the operator norm. Then $\mathcal{L} (E)$ is a unital Banach algebra for the usual operator composition. Let $\mathcal{K} (E)$ be the subspace of compact operators. It is well-know that: $\mathcal{K} (E)$ is a non-trivial closed vector subspace of $\mathcal{L} (E)$; $\mathcal{K} (E)$ is a bilateral ideal of $\mathcal{L} (E)$. From the first property we deduce that $\mathcal{H} (E) := \mathcal{L} (E) / \mathcal{K} (E)$ is a Banach space for the norm $\| \tilde{T}\|_{\mathcal{H} (E)} = \inf \{\|T\|_{\mathcal{L} (E)}: \ \pi (T) = \tilde{T}\}$, where $\pi$ is the canonical projection. From the second property we deduce that the composition goes down to the quotient, and gives it a structure of unital Banach algebra. On the other hand, Wikipedia gives a few possible definitions for the essential spectrum $\sigma_{ess}$ of an operator, some of which are mentioned to be invariant under compact perturbations. Let $T \in \mathcal{L} (E)$. Is $\sigma (\pi(T))$ equal to $\sigma_{ess, k} (T)$ (see the Wikipedia page for the definition) for some $1 \leq k \leq 4$? If so, which $k$? Otherwise, do we still have $\rho_{ess} (T) = \rho(\pi(T))$, where $\rho$ (resp. $\rho_{ess}$) is the spctral radius (resp. the essential spectral radius)? Motivation: I've been working with some families of quasi-compact operators for some time now. I've never seen the essential spectrum or essential spectral radius presented this way. If this works, it could make for a pretty nice introduction to the subject if I ever need to teach it. Oh, and unfortunately I don't have the references on the Wikipedia page at hand now, so I can't check them.",,"['functional-analysis', 'spectral-theory', 'banach-algebras']"
86,Is the dual space of all Radon measures the space of signed measures on a $\delta$-ring?,Is the dual space of all Radon measures the space of signed measures on a -ring?,\delta,"Consider the Banach space $C_c(\mathbb{R})$ of continuous functions with compact support equipped with the uniform norm $||f||_\infty := \sup_{x \in \mathbb{R}} |f(x)|$. Then it is known (Riesz representation theorem) that the continuous dual of $C_c$ can be identified the vector space of all finite signed Radon measures on $\mathbb{R}$, that is set-functions $\mu : \mathscr{A} \to \mathbb{R}$ (that are regular) and $\mathscr{A}$ is the Borel $\sigma$-algebra on $\mathbb{R}$ (or a $\sigma$-algebra that contains the Borel sets). Now, if we equip $C_c = \bigcup_{K \text{ compact}} C_c(K)$ with the inductive limit topology where $C_c(K)$ is the space of continuous functions $f \in C_c$ (i.e. $f$ has domain $\mathbb{R}$) with $\text{supp}(f) \subseteq K$ normed by $||f||_K := \sup_{x \in \mathbb{R}} |f(x)| = \sup_{x \in K} |f(x)|$, then $C_c$ is a locally convex space. The dual space is larger than in the case above and consists of all Radon ""signed measures"" (e.g. the Lebesgue measure $dx$ which is clearly not finite on $\mathbb{R}$ or $\sin(x) dx$ which is not a signed measure on a $\sigma$-algebra). I want to identify this dual space with an object coming from abstract measure theory. I think, the correct object is also a signed measure which is also countably additive BUT is defined on merely the $\delta$-ring generated by bounded intervals in $\mathbb{R}$ instead of a $\sigma$-algebra (the $\delta$-ring does not contain unbounded sets like  the whole space). Is this correct intuition? And if yes, does anyone know a reference for this fact? (Of course one can generalize the ground space to be a more general topological space then $\mathbb{R}$, e.g. a locally compact second-countable Hausdorff space.) EDIT : $C_c$ was primarily equipped with the wrong topology (the compact convergence). It is now correctly equipped with the inductive limit topology from its subspaces $C_c(K)$.","Consider the Banach space $C_c(\mathbb{R})$ of continuous functions with compact support equipped with the uniform norm $||f||_\infty := \sup_{x \in \mathbb{R}} |f(x)|$. Then it is known (Riesz representation theorem) that the continuous dual of $C_c$ can be identified the vector space of all finite signed Radon measures on $\mathbb{R}$, that is set-functions $\mu : \mathscr{A} \to \mathbb{R}$ (that are regular) and $\mathscr{A}$ is the Borel $\sigma$-algebra on $\mathbb{R}$ (or a $\sigma$-algebra that contains the Borel sets). Now, if we equip $C_c = \bigcup_{K \text{ compact}} C_c(K)$ with the inductive limit topology where $C_c(K)$ is the space of continuous functions $f \in C_c$ (i.e. $f$ has domain $\mathbb{R}$) with $\text{supp}(f) \subseteq K$ normed by $||f||_K := \sup_{x \in \mathbb{R}} |f(x)| = \sup_{x \in K} |f(x)|$, then $C_c$ is a locally convex space. The dual space is larger than in the case above and consists of all Radon ""signed measures"" (e.g. the Lebesgue measure $dx$ which is clearly not finite on $\mathbb{R}$ or $\sin(x) dx$ which is not a signed measure on a $\sigma$-algebra). I want to identify this dual space with an object coming from abstract measure theory. I think, the correct object is also a signed measure which is also countably additive BUT is defined on merely the $\delta$-ring generated by bounded intervals in $\mathbb{R}$ instead of a $\sigma$-algebra (the $\delta$-ring does not contain unbounded sets like  the whole space). Is this correct intuition? And if yes, does anyone know a reference for this fact? (Of course one can generalize the ground space to be a more general topological space then $\mathbb{R}$, e.g. a locally compact second-countable Hausdorff space.) EDIT : $C_c$ was primarily equipped with the wrong topology (the compact convergence). It is now correctly equipped with the inductive limit topology from its subspaces $C_c(K)$.",,"['functional-analysis', 'measure-theory', 'topological-vector-spaces', 'locally-convex-spaces']"
87,Question about $C_0(X)$-algebras and $C_b(X)$.,Question about -algebras and .,C_0(X) C_b(X),"Let $X$ be a locally compact Hausdorff space. Denote by $C_0(X)$ its C*-algebra of continuous functions that vanish on infinity and by $C_b(X)$ its C*-algebra of bounded functions. Now, let $A$ be a C*-algebra and $M(A)$ its multiplier algebra. Denote by $\mathcal{Z}M(A)$ its center. A structure of ""$C_0(X)$-algebra"" in $A$ is a $*$-morphism $\pi:C_0(X)  \rightarrow \mathcal{Z}M(A)$ that has ""non-degeneracy"", meaning that  $$\pi(C_0(X))\cdot A = \mathrm{span}\{ \pi(f)\cdot a : f \in C_0(X), a \in A\} $$  is dense in $A$. Being somewhat careful, one can see that this is actually equivalent to the fact that if $\{u_{\lambda}\}$ is an approximate unit of $C_0(X)$ then $\{\pi(u_{\lambda})\}$ is an approximate unit of $A$, in the sense that $$ \lim_{\lambda \to \infty} \| \pi(u_{\lambda}) a - a \| = 0$$ My question now is, how can you extend $\pi$ to a $*$-morphism  $$ \hat{\pi}:C_b(X) \rightarrow \mathcal{Z}M(A)? $$ I've seen this result quoted somewhere but I haven't been able to prove it. Thanks in advance.","Let $X$ be a locally compact Hausdorff space. Denote by $C_0(X)$ its C*-algebra of continuous functions that vanish on infinity and by $C_b(X)$ its C*-algebra of bounded functions. Now, let $A$ be a C*-algebra and $M(A)$ its multiplier algebra. Denote by $\mathcal{Z}M(A)$ its center. A structure of ""$C_0(X)$-algebra"" in $A$ is a $*$-morphism $\pi:C_0(X)  \rightarrow \mathcal{Z}M(A)$ that has ""non-degeneracy"", meaning that  $$\pi(C_0(X))\cdot A = \mathrm{span}\{ \pi(f)\cdot a : f \in C_0(X), a \in A\} $$  is dense in $A$. Being somewhat careful, one can see that this is actually equivalent to the fact that if $\{u_{\lambda}\}$ is an approximate unit of $C_0(X)$ then $\{\pi(u_{\lambda})\}$ is an approximate unit of $A$, in the sense that $$ \lim_{\lambda \to \infty} \| \pi(u_{\lambda}) a - a \| = 0$$ My question now is, how can you extend $\pi$ to a $*$-morphism  $$ \hat{\pi}:C_b(X) \rightarrow \mathcal{Z}M(A)? $$ I've seen this result quoted somewhere but I haven't been able to prove it. Thanks in advance.",,"['functional-analysis', 'operator-theory', 'operator-algebras']"
88,Set-theoretic questions about the definitions of crossed-product $ C^{*} $-algebras and group $ C^{*} $-algebras.,Set-theoretic questions about the definitions of crossed-product -algebras and group -algebras., C^{*}   C^{*} ,"In his book Crossed Products of $ C^{*} $-Algebras , Dana P. Williams defines the crossed product of a $ C^{*} $-algebra $ A $ by a locally compact group $ G $ as the completion of $ {C_{c}}(G,A) $ with respect to the norm $ \| \cdot \| $ defined by $$ \| f \| \stackrel{\text{df}}{=} \sup(\{ \| (\pi \rtimes U)(f) \| \mid \text{$ (\pi,U) $ is a covariant representation of $ (G,A,\alpha) $} \}). \qquad (1) $$ It is said, on page 52 of the book, that the the collection of values in $ (1) $ is a subclass of the set $ \Bbb{R} $ of real numbers, that the separation axioms of set theory guarantee that a subclass of a set is yet a set, and that we are taking the supremum of a bounded set of real numbers. In the book Morita Equivalence and Continuous-Trace $ C^{*} $-Algebras by Iain Raeburn and Dana P. Williams, the group $ C^{*} $-algebra of a locally compact group $ G $ is defined as the completion of $ {C_{c}}(G) $ with respect to the norm $ \| \cdot \| $ defined by $$ \| f \| \stackrel{\text{df}}{=} \sup(\{ \| \pi(f) \| \mid \text{$ \pi $ is a cyclic norm-decreasing representation of $ {C_{c}}(G) $} \}). \qquad(2) $$ It is said, on page 281 of this book, that “one takes cyclic representations in the definition of $ \| \cdot \| $ merely to guarantee that we are taking the supremum over a set.” My question is why the collection in $ (1) $ is a set (I only know a little about set theory and cannot understand the explanation), but in $ (2) $, we need to take cyclic representations to guarantee that we are taking the supremum over a set. Thanks!","In his book Crossed Products of $ C^{*} $-Algebras , Dana P. Williams defines the crossed product of a $ C^{*} $-algebra $ A $ by a locally compact group $ G $ as the completion of $ {C_{c}}(G,A) $ with respect to the norm $ \| \cdot \| $ defined by $$ \| f \| \stackrel{\text{df}}{=} \sup(\{ \| (\pi \rtimes U)(f) \| \mid \text{$ (\pi,U) $ is a covariant representation of $ (G,A,\alpha) $} \}). \qquad (1) $$ It is said, on page 52 of the book, that the the collection of values in $ (1) $ is a subclass of the set $ \Bbb{R} $ of real numbers, that the separation axioms of set theory guarantee that a subclass of a set is yet a set, and that we are taking the supremum of a bounded set of real numbers. In the book Morita Equivalence and Continuous-Trace $ C^{*} $-Algebras by Iain Raeburn and Dana P. Williams, the group $ C^{*} $-algebra of a locally compact group $ G $ is defined as the completion of $ {C_{c}}(G) $ with respect to the norm $ \| \cdot \| $ defined by $$ \| f \| \stackrel{\text{df}}{=} \sup(\{ \| \pi(f) \| \mid \text{$ \pi $ is a cyclic norm-decreasing representation of $ {C_{c}}(G) $} \}). \qquad(2) $$ It is said, on page 281 of this book, that “one takes cyclic representations in the definition of $ \| \cdot \| $ merely to guarantee that we are taking the supremum over a set.” My question is why the collection in $ (1) $ is a set (I only know a little about set theory and cannot understand the explanation), but in $ (2) $, we need to take cyclic representations to guarantee that we are taking the supremum over a set. Thanks!",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'cross-product']"
89,Open mapping lemma - are these versions equivalent?,Open mapping lemma - are these versions equivalent?,,"Here is a version the Open Mapping Lemma given in class : Let $X$ be a Banach space and $Y$ be a normed space. Let $T : X\rightarrow Y$ be a bounded linear map. Assume there exist $M \geq 0$ and $0 \leq \delta < 1$ such that $T(MB_X)$ is $\delta$-dense in $B_Y$. Then $T$ is surjective. More precisely for all $y \in Y$ there exists $x \in X$ such that $y = T(x)$ and $\|x\| \leq \frac{M}{1-\delta}\|y\|$, i.e. $T\left(\frac{M}{1-\delta}B_X\right) \supseteq B_Y$. Moreover $Y$ is complete. Note that $A$ is said to be $\delta$-dense in $B$ if $\forall b \in B$, $\exists a \in A$ such that $d(a,b) \leq \delta$. I am wondering if this is stronger/weaker/equivalent/independent than/to/from the following Open Mapping Theorem (strong version) : Let $T$ be a bounded linear map from a Banach space $X$ into a normed space $Y$. If the image $T(X)$ is nonmeager in $Y$ then $T$ is surjective and an open mapping. Moreover $Y$ is complete. Any hint ?","Here is a version the Open Mapping Lemma given in class : Let $X$ be a Banach space and $Y$ be a normed space. Let $T : X\rightarrow Y$ be a bounded linear map. Assume there exist $M \geq 0$ and $0 \leq \delta < 1$ such that $T(MB_X)$ is $\delta$-dense in $B_Y$. Then $T$ is surjective. More precisely for all $y \in Y$ there exists $x \in X$ such that $y = T(x)$ and $\|x\| \leq \frac{M}{1-\delta}\|y\|$, i.e. $T\left(\frac{M}{1-\delta}B_X\right) \supseteq B_Y$. Moreover $Y$ is complete. Note that $A$ is said to be $\delta$-dense in $B$ if $\forall b \in B$, $\exists a \in A$ such that $d(a,b) \leq \delta$. I am wondering if this is stronger/weaker/equivalent/independent than/to/from the following Open Mapping Theorem (strong version) : Let $T$ be a bounded linear map from a Banach space $X$ into a normed space $Y$. If the image $T(X)$ is nonmeager in $Y$ then $T$ is surjective and an open mapping. Moreover $Y$ is complete. Any hint ?",,"['functional-analysis', 'banach-spaces', 'normed-spaces', 'baire-category']"
90,Error in the calulation of the spectrum of the image of right shift operator in the Calkin algebra,Error in the calulation of the spectrum of the image of right shift operator in the Calkin algebra,,"If $S \in \mathcal{B}(\ell^2(\mathbb{N}))$ is the right shift operator $$ S(x_1, x_2, \ldots) = (0, x_1, x_2, \ldots),$$ and $\mathcal{C} := \mathcal{B}(\ell^2(\mathbb{N}))/\mathcal{K}$ is the Calkin algebra (here, $\mathcal{K}$ are the compact operators), what is the spectrum of $[S]$ (the class of $S$ in $\mathcal{C}$)? If my analysis is correct, $\sigma([S])$ should be the values $\lambda \in \mathbb{C}$ for which the operator $\lambda - S$ is not a Fredholm operator, that is, the values such that either $\ker(\lambda -S)$ or $\ker(\bar{\lambda} - S^*)$ is infinite dimensional. If that is correct, then considering the fact that $$ 0 = (\lambda - S) (x_n) \iff x_n = 0 \quad \forall n$$ and also that $$ 0 = (\bar{\lambda} - S^*)(x_n) \iff \begin{cases} (x_n) \in \langle (1,\bar{\lambda},\bar{\lambda}^2,\ldots)\rangle \quad &\mbox{if } |\lambda| < 1 \\ x_n =0 \quad \forall n \quad &\mbox{if } |\lambda|\geq 1 \end{cases},$$ this implies that $\lambda -S$ is always Fredholm, independently of $\lambda$, which in turn, doesn't make any sense (as it would imply that $\sigma([S])=\emptyset$). I can't find my mistake anywhere, could you point it out to me? Thanks in advance.","If $S \in \mathcal{B}(\ell^2(\mathbb{N}))$ is the right shift operator $$ S(x_1, x_2, \ldots) = (0, x_1, x_2, \ldots),$$ and $\mathcal{C} := \mathcal{B}(\ell^2(\mathbb{N}))/\mathcal{K}$ is the Calkin algebra (here, $\mathcal{K}$ are the compact operators), what is the spectrum of $[S]$ (the class of $S$ in $\mathcal{C}$)? If my analysis is correct, $\sigma([S])$ should be the values $\lambda \in \mathbb{C}$ for which the operator $\lambda - S$ is not a Fredholm operator, that is, the values such that either $\ker(\lambda -S)$ or $\ker(\bar{\lambda} - S^*)$ is infinite dimensional. If that is correct, then considering the fact that $$ 0 = (\lambda - S) (x_n) \iff x_n = 0 \quad \forall n$$ and also that $$ 0 = (\bar{\lambda} - S^*)(x_n) \iff \begin{cases} (x_n) \in \langle (1,\bar{\lambda},\bar{\lambda}^2,\ldots)\rangle \quad &\mbox{if } |\lambda| < 1 \\ x_n =0 \quad \forall n \quad &\mbox{if } |\lambda|\geq 1 \end{cases},$$ this implies that $\lambda -S$ is always Fredholm, independently of $\lambda$, which in turn, doesn't make any sense (as it would imply that $\sigma([S])=\emptyset$). I can't find my mistake anywhere, could you point it out to me? Thanks in advance.",,"['functional-analysis', 'operator-theory']"
91,Show that hermitian element $h=\sum p_n/3^n$ generates $ C_0(\Omega)$,Show that hermitian element  generates,h=\sum p_n/3^n  C_0(\Omega),"Let $\Omega$ be a locally compact Hausdorff space, and suppose that the C*-algebra $C_0(\Omega)$ is generated by a sequence of projections $(p_n)_{n=1}^{\infty}$. Show that the hermitian element $h=\sum_{n=1}^\infty \frac{p_n}{3^n}$ generates $C_0(\Omega)$. My attempt: firstly, I show that $h\in C_0(\Omega)$. It means that $h$ is continuous and for every $\epsilon >0$, the set $\{x\in \Omega ; |h(x)|\geq \epsilon\}$ is compact.  Suppose $p_np_m=0$ for every $n,m\in \Bbb N$ and $n\neq m$. Let $x_m\to x$ , there is $n_0\in \Bbb N$ such that $x\in p_{n_0}$. Also there is a subsequence $\{y_m\}$ of $\{x_m\}$ such that $\{y_m\}\subset p_{n_0}(\Omega)$. Then $$|h(y_m) - h(x)| \leq \sum_{n=1}^\infty\frac{|p_n(y_m)-p_n(x)|}{3^n}=0$$ So $h$ is continuous. To show for every $\epsilon >0$, the set $\{x\in \Omega ; |h(x)|\geq \epsilon\}$ is compact. I know that for every $\epsilon>0$, there is $n_0$ such that $\sum_{n=n_0+1}^\infty\frac{1}{3^n} <\epsilon$. Thus for $x\in p_1+...+p_{n_0}(\Omega)$, $|h(x)|\geq \epsilon$. Clearly $Im(p_1+...+P_{n_0})$ is closed, but I can not show that it's compact. Now I show that $h$ generates $C_0(\Omega)$. Let $f\in C_0(\Omega)$. there is a sequence of polynomials $\{q_m\}\subset C_0(\Omega)$ such that $f=\lim q_m$. Also every polynomial has a representation of projections $\{p_n\}$ as $q_m = \sum \lambda_n p_n$. Clearly  for $n$, there is $t_n$ such that $\lambda_n=t_n/3^n$ . So $h$ generates $C_0(\Omega)$. Please check my attempt and give me a hint to show that $h\in C_0(\Omega)$. Thanks in advance","Let $\Omega$ be a locally compact Hausdorff space, and suppose that the C*-algebra $C_0(\Omega)$ is generated by a sequence of projections $(p_n)_{n=1}^{\infty}$. Show that the hermitian element $h=\sum_{n=1}^\infty \frac{p_n}{3^n}$ generates $C_0(\Omega)$. My attempt: firstly, I show that $h\in C_0(\Omega)$. It means that $h$ is continuous and for every $\epsilon >0$, the set $\{x\in \Omega ; |h(x)|\geq \epsilon\}$ is compact.  Suppose $p_np_m=0$ for every $n,m\in \Bbb N$ and $n\neq m$. Let $x_m\to x$ , there is $n_0\in \Bbb N$ such that $x\in p_{n_0}$. Also there is a subsequence $\{y_m\}$ of $\{x_m\}$ such that $\{y_m\}\subset p_{n_0}(\Omega)$. Then $$|h(y_m) - h(x)| \leq \sum_{n=1}^\infty\frac{|p_n(y_m)-p_n(x)|}{3^n}=0$$ So $h$ is continuous. To show for every $\epsilon >0$, the set $\{x\in \Omega ; |h(x)|\geq \epsilon\}$ is compact. I know that for every $\epsilon>0$, there is $n_0$ such that $\sum_{n=n_0+1}^\infty\frac{1}{3^n} <\epsilon$. Thus for $x\in p_1+...+p_{n_0}(\Omega)$, $|h(x)|\geq \epsilon$. Clearly $Im(p_1+...+P_{n_0})$ is closed, but I can not show that it's compact. Now I show that $h$ generates $C_0(\Omega)$. Let $f\in C_0(\Omega)$. there is a sequence of polynomials $\{q_m\}\subset C_0(\Omega)$ such that $f=\lim q_m$. Also every polynomial has a representation of projections $\{p_n\}$ as $q_m = \sum \lambda_n p_n$. Clearly  for $n$, there is $t_n$ such that $\lambda_n=t_n/3^n$ . So $h$ generates $C_0(\Omega)$. Please check my attempt and give me a hint to show that $h\in C_0(\Omega)$. Thanks in advance",,"['functional-analysis', 'operator-theory', 'c-star-algebras', 'banach-algebras']"
92,Convergence of sequence of $L^{p}$ function,Convergence of sequence of  function,L^{p},"Given that $\Omega \subset \mathbb{R}^{n}$ is bounded. If you are given that $u_{k} \rightarrow u$ in $L^{p- \epsilon}(\Omega)$ and a functions $f: \mathbb{R} \rightarrow \mathbb{R}$ where $\{f(u_{k})\}_{k \in \mathbb{N}}$ is bounded in $L^{p'+\epsilon}(\Omega)$, where $\epsilon > 0$ and $p' = \frac{p}{p-1}$. How does it follow that $$\int_{\Omega}f(u_{k})(u_{k}-u)dx \rightarrow 0$$ Given that $p-\epsilon > 1$ annd $p'+ \epsilon > 1$. Can anyone see how this follows?","Given that $\Omega \subset \mathbb{R}^{n}$ is bounded. If you are given that $u_{k} \rightarrow u$ in $L^{p- \epsilon}(\Omega)$ and a functions $f: \mathbb{R} \rightarrow \mathbb{R}$ where $\{f(u_{k})\}_{k \in \mathbb{N}}$ is bounded in $L^{p'+\epsilon}(\Omega)$, where $\epsilon > 0$ and $p' = \frac{p}{p-1}$. How does it follow that $$\int_{\Omega}f(u_{k})(u_{k}-u)dx \rightarrow 0$$ Given that $p-\epsilon > 1$ annd $p'+ \epsilon > 1$. Can anyone see how this follows?",,['real-analysis']
93,Prove domain of Dependence Inequality for the Wave Equation?,Prove domain of Dependence Inequality for the Wave Equation?,,"Let $(x_0,t_0)\in R^{n+1}$ with $t_0>0$, and let $\Omega$ be the conical domain in $R^{n+1}$ bounded by the backward characteristic cone with apex at $(x_0,t_0)$ and by the plane $t=0$. Suppose $u\in C^2(\overline \Omega)$ and statisfies $$\Delta u -u_{tt}-q(x)u=0$$ in $\Omega$, where $q(x)>0$. Derive the domain of dependence inequality $$\int_{B(x_0,t_0-T)}u_{x_1}^2+...+u_{x_n}^2+u_{t}^2+qu^2|_{t=T}dx\le \int_{B(x_0,t_0)}u_{x_1}^2+...+u_{x_n}^2+u_{t}^2+qu^2|_{t=0}dx$$ where $0\le T\le t_0$ and $B(x_0,r)$ denotes the ball ${x:|x-x_0|<r}$. My attempt: I have no clue about this problem. Maybe using energy's method? Can anyone give me some hints or references like lecture notes? Thanks so much!","Let $(x_0,t_0)\in R^{n+1}$ with $t_0>0$, and let $\Omega$ be the conical domain in $R^{n+1}$ bounded by the backward characteristic cone with apex at $(x_0,t_0)$ and by the plane $t=0$. Suppose $u\in C^2(\overline \Omega)$ and statisfies $$\Delta u -u_{tt}-q(x)u=0$$ in $\Omega$, where $q(x)>0$. Derive the domain of dependence inequality $$\int_{B(x_0,t_0-T)}u_{x_1}^2+...+u_{x_n}^2+u_{t}^2+qu^2|_{t=T}dx\le \int_{B(x_0,t_0)}u_{x_1}^2+...+u_{x_n}^2+u_{t}^2+qu^2|_{t=0}dx$$ where $0\le T\le t_0$ and $B(x_0,r)$ denotes the ball ${x:|x-x_0|<r}$. My attempt: I have no clue about this problem. Maybe using energy's method? Can anyone give me some hints or references like lecture notes? Thanks so much!",,"['functional-analysis', 'partial-differential-equations', 'wave-equation']"
94,To bound a heat equation on a real line?,To bound a heat equation on a real line?,,"Let $\displaystyle\mathcal{H}_{t}(x)=\frac{1}{(4\pi t)^{1/2}}e^{-x^{2}/4t}$ be the Heat Kernel . The imposed initial condition for the heat equation on a real line is $u(x,0)=f(x)$ a function belongs to Schwartz Space . Then, $u(x,t)=(f*\mathcal{H}_{t})(x)$ for $t>0$ In my textbook, the author uses the following estimation to bound $u(x,t)$: $$\displaystyle|u(x,t)|\leq \int_{|y|\leq |x|/2}|f(x-y)|\mathcal{H}_{t}(y)\,dy+\int_{|y|\geq |x|/2}|f(x-y)|\mathcal{H}_{t}(y)\,dy$$ $$\displaystyle\leq\frac{C_{N}}{(1+|x|)^{N}}+\frac{B}{\sqrt{t}}e^{-Dx^{2}/t}$$ where $C_{N},B,D>0$ are constants. My question is why using the cases $|y|\leq|x|/2$ and $|y|\geq|x|/2$? How come it bounds the second integral like this?","Let $\displaystyle\mathcal{H}_{t}(x)=\frac{1}{(4\pi t)^{1/2}}e^{-x^{2}/4t}$ be the Heat Kernel . The imposed initial condition for the heat equation on a real line is $u(x,0)=f(x)$ a function belongs to Schwartz Space . Then, $u(x,t)=(f*\mathcal{H}_{t})(x)$ for $t>0$ In my textbook, the author uses the following estimation to bound $u(x,t)$: $$\displaystyle|u(x,t)|\leq \int_{|y|\leq |x|/2}|f(x-y)|\mathcal{H}_{t}(y)\,dy+\int_{|y|\geq |x|/2}|f(x-y)|\mathcal{H}_{t}(y)\,dy$$ $$\displaystyle\leq\frac{C_{N}}{(1+|x|)^{N}}+\frac{B}{\sqrt{t}}e^{-Dx^{2}/t}$$ where $C_{N},B,D>0$ are constants. My question is why using the cases $|y|\leq|x|/2$ and $|y|\geq|x|/2$? How come it bounds the second integral like this?",,"['functional-analysis', 'fourier-analysis', 'schwartz-space']"
95,limit of a weakly convergent sequence in Banach spaces,limit of a weakly convergent sequence in Banach spaces,,"Consider $X$ a Banach space. For some sequence $x_n\in X$, assume that for every $f\in X^*$, $f(x_n)\to c_f$. Does this imply that $\exists x\in X$ where $c_f = f(x)$? How might I go about finding such an $x$? (It's clearly not the limit of $x_n$ since no guarantees on strong convergence.) Thank you!","Consider $X$ a Banach space. For some sequence $x_n\in X$, assume that for every $f\in X^*$, $f(x_n)\to c_f$. Does this imply that $\exists x\in X$ where $c_f = f(x)$? How might I go about finding such an $x$? (It's clearly not the limit of $x_n$ since no guarantees on strong convergence.) Thank you!",,"['functional-analysis', 'banach-spaces']"
96,The question regrading to density argument in analysis.,The question regrading to density argument in analysis.,,"I know the density argument in $L^p$ space, in Sobolev spaces, and even in BV are really sweet in many many cases. However, some times author just work on nice functions and comment by ""the rest can be easily done by density""... Like I said, some times it is easy, but some times it is really not clear! Here I list some problems I encountered before in which I do not find out why density argument can so easily worked out. This one regrading to weak convergence in $BV$ space. It is an argument in Evans & Gariepy's book, page 175, theorem 3. In that theorem they were trying to prove a sequence of radon measure $\mu_k\to \mu$ weakly. In the prove, they only test against $\phi\in C_c^1(R^n;R^n)$, and hence they can employ integration by parts in $BV$ functions. However, to my knowledge, to prove a radon measure convergence weakly, we really need to test against all continuous function. So here I assume they use density argument. Now given any $\phi\in C_c(R^n;R^n)$, of course we can find a sequence $\phi_m\in C_c^\infty$ and $\phi_m\to \phi$ uniformly. Hence, the question would be, why we can interchange the limit such that $$\lim_{m\to\infty}\lim_{k\to\infty}\int \phi_m d\mu_k =\lim_{k\to\infty}\lim_{m\to\infty}\int \phi_m d\mu_k  \,\,??$$ given that we already proved that  $$\lim_{k\to\infty}\int \phi_m d\mu_k =\int \phi_m d\mu $$ for all $\phi_m\in C_c^\infty$. Another similar question regrading to density is here , in which I have a sequence $u_n$ bounded in $H^1$ norm and I want to prove $u_n\to 0$ weakly in $H^1$. It is quick to show that $u_n\to 0$ weakly in $L^2$ but hard to work out $\partial_i u_n\to 0$. The comment in that post nicely suggest by using density argument so that we could test $\partial_i u_n$ with a $C^\infty$ function and hence by integration by parts we could use that fact $u_n\to 0$ weak to conclude. However, now I face the similar situation: for an sequence $g_m\in C_c^\infty$, I have $g_m\to g$ strongly in $L^2$ and I have $$\lim_{n\to\infty} \int_\Omega \partial_i u_n g_m\,dx= 0 $$ Next I need to push $m\to\infty$ and I have $$\lim_{m\to\infty}\lim_{n\to\infty} \int_\Omega \partial_i u_n g_m\,dx= 0 $$ for sure. However, why could I interchange the limit here? i.e., why $$\lim_{m\to\infty}\lim_{n\to\infty} \int_\Omega \partial_i u_n g_m\,dx= \lim_{n\to\infty}\lim_{m\to\infty} \int_\Omega \partial_i u_n g_m\,dx $$ Also, I post another question regrading to how to interchange limit, but I gave a too nice condition there ... Please help, and if possible, please work out the details of how to interchange the limit. THx!!!!","I know the density argument in $L^p$ space, in Sobolev spaces, and even in BV are really sweet in many many cases. However, some times author just work on nice functions and comment by ""the rest can be easily done by density""... Like I said, some times it is easy, but some times it is really not clear! Here I list some problems I encountered before in which I do not find out why density argument can so easily worked out. This one regrading to weak convergence in $BV$ space. It is an argument in Evans & Gariepy's book, page 175, theorem 3. In that theorem they were trying to prove a sequence of radon measure $\mu_k\to \mu$ weakly. In the prove, they only test against $\phi\in C_c^1(R^n;R^n)$, and hence they can employ integration by parts in $BV$ functions. However, to my knowledge, to prove a radon measure convergence weakly, we really need to test against all continuous function. So here I assume they use density argument. Now given any $\phi\in C_c(R^n;R^n)$, of course we can find a sequence $\phi_m\in C_c^\infty$ and $\phi_m\to \phi$ uniformly. Hence, the question would be, why we can interchange the limit such that $$\lim_{m\to\infty}\lim_{k\to\infty}\int \phi_m d\mu_k =\lim_{k\to\infty}\lim_{m\to\infty}\int \phi_m d\mu_k  \,\,??$$ given that we already proved that  $$\lim_{k\to\infty}\int \phi_m d\mu_k =\int \phi_m d\mu $$ for all $\phi_m\in C_c^\infty$. Another similar question regrading to density is here , in which I have a sequence $u_n$ bounded in $H^1$ norm and I want to prove $u_n\to 0$ weakly in $H^1$. It is quick to show that $u_n\to 0$ weakly in $L^2$ but hard to work out $\partial_i u_n\to 0$. The comment in that post nicely suggest by using density argument so that we could test $\partial_i u_n$ with a $C^\infty$ function and hence by integration by parts we could use that fact $u_n\to 0$ weak to conclude. However, now I face the similar situation: for an sequence $g_m\in C_c^\infty$, I have $g_m\to g$ strongly in $L^2$ and I have $$\lim_{n\to\infty} \int_\Omega \partial_i u_n g_m\,dx= 0 $$ Next I need to push $m\to\infty$ and I have $$\lim_{m\to\infty}\lim_{n\to\infty} \int_\Omega \partial_i u_n g_m\,dx= 0 $$ for sure. However, why could I interchange the limit here? i.e., why $$\lim_{m\to\infty}\lim_{n\to\infty} \int_\Omega \partial_i u_n g_m\,dx= \lim_{n\to\infty}\lim_{m\to\infty} \int_\Omega \partial_i u_n g_m\,dx $$ Also, I post another question regrading to how to interchange limit, but I gave a too nice condition there ... Please help, and if possible, please work out the details of how to interchange the limit. THx!!!!",,"['real-analysis', 'functional-analysis', 'limits', 'sobolev-spaces', 'bounded-variation']"
97,Weak star convergence leads to weak convergence of derivative?,Weak star convergence leads to weak convergence of derivative?,,"Let $u_\epsilon \in W^{1,p}(\Omega)$ be a sequence with $\Omega \subset \mathbb{R}^n$ bounded. Let $u_\epsilon \rightharpoonup^* u$ weakly* in $L^\infty (\Omega)$ (for a subsequence) with $u \in W^{1,p}(\Omega)$ and assume $\sup_\epsilon \vert \nabla u_\epsilon\vert_{L^p (\Omega; \mathbb{R}^n)} < C$. Proof that $\nabla u_\epsilon \rightharpoonup \nabla u$ weakly in $L^p (\Omega; \mathbb{R}^n)$ (for a subsequence). My idea so far: $\nabla u_\epsilon$ is bounded, therefore there exists a weak limit (for a subsequence). $u_\epsilon \rightharpoonup^* u$ weakly* in $L^\infty (\Omega)$, therefore $u_\epsilon \rightharpoonup u$ weakly in $L^p (\Omega)$, which means that  $~\int_\Omega \phi \partial_x u_\epsilon \to \int_\Omega \phi \partial_x u ~$  for all $~\phi \in W^{1,p^*}(\Omega)$. If I am correct, the weak convergence in $L^p (\Omega; \mathbb{R}^n)~$ is $~\int_\Omega \Phi \cdot \nabla u_\epsilon \to \int_\Omega \Phi \nabla u~$ for all $~\Phi \in L^{p^*} (\Omega; \mathbb{R}^n)$, but I can't seem to get that from the lines above. Any help is appreciated.","Let $u_\epsilon \in W^{1,p}(\Omega)$ be a sequence with $\Omega \subset \mathbb{R}^n$ bounded. Let $u_\epsilon \rightharpoonup^* u$ weakly* in $L^\infty (\Omega)$ (for a subsequence) with $u \in W^{1,p}(\Omega)$ and assume $\sup_\epsilon \vert \nabla u_\epsilon\vert_{L^p (\Omega; \mathbb{R}^n)} < C$. Proof that $\nabla u_\epsilon \rightharpoonup \nabla u$ weakly in $L^p (\Omega; \mathbb{R}^n)$ (for a subsequence). My idea so far: $\nabla u_\epsilon$ is bounded, therefore there exists a weak limit (for a subsequence). $u_\epsilon \rightharpoonup^* u$ weakly* in $L^\infty (\Omega)$, therefore $u_\epsilon \rightharpoonup u$ weakly in $L^p (\Omega)$, which means that  $~\int_\Omega \phi \partial_x u_\epsilon \to \int_\Omega \phi \partial_x u ~$  for all $~\phi \in W^{1,p^*}(\Omega)$. If I am correct, the weak convergence in $L^p (\Omega; \mathbb{R}^n)~$ is $~\int_\Omega \Phi \cdot \nabla u_\epsilon \to \int_\Omega \Phi \nabla u~$ for all $~\Phi \in L^{p^*} (\Omega; \mathbb{R}^n)$, but I can't seem to get that from the lines above. Any help is appreciated.",,"['functional-analysis', 'weak-convergence', 'weak-derivatives', 'bochner-spaces']"
98,Compact set and its extreme points,Compact set and its extreme points,,"I am reading Chapter 3: Convexity of Rudin's ""Functional Analysis"". Here is the problem I'm having trouble solving (number 18): Let $K$ be the smallest convex set in $\mathbb{ R}^3$ that contains the points $(1, 0, 1), (1, 0, -1)$, and $(\cos \phi, \sin \phi, 0)$, for $0 \le \phi \le  2 \pi $. Show that $K$ is compact but that the set of all extreme points of $K$ is not compact. Does such an example exist in $\mathbb{R}^2$? So I need to show that this set is closed and bounded. Could you help me with that? Thanks.","I am reading Chapter 3: Convexity of Rudin's ""Functional Analysis"". Here is the problem I'm having trouble solving (number 18): Let $K$ be the smallest convex set in $\mathbb{ R}^3$ that contains the points $(1, 0, 1), (1, 0, -1)$, and $(\cos \phi, \sin \phi, 0)$, for $0 \le \phi \le  2 \pi $. Show that $K$ is compact but that the set of all extreme points of $K$ is not compact. Does such an example exist in $\mathbb{R}^2$? So I need to show that this set is closed and bounded. Could you help me with that? Thanks.",,"['functional-analysis', 'convex-analysis', 'compactness']"
99,"Is there $u,v\in L(E): uv-vu=id_E$",Is there,"u,v\in L(E): uv-vu=id_E",Let $E$ be a normed vector space over $\mathbb{R}$. Is there continuous linear transformations $u$ and $v$ such that: $$uv-vu=id_E$$ (.ie $\forall x\in E:u(v(x))-v(u(x))=x$) I suspect that the answer is no. When $E$ is finite dimensional we can use Trace Operator to prove that there is indeed no satisfied transformations. I don't know how to process in the case of infinite dimensional $E$.,Let $E$ be a normed vector space over $\mathbb{R}$. Is there continuous linear transformations $u$ and $v$ such that: $$uv-vu=id_E$$ (.ie $\forall x\in E:u(v(x))-v(u(x))=x$) I suspect that the answer is no. When $E$ is finite dimensional we can use Trace Operator to prove that there is indeed no satisfied transformations. I don't know how to process in the case of infinite dimensional $E$.,,"['linear-algebra', 'functional-analysis']"
