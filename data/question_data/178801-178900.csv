,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Find the minimum value of $f(x,y) = x + y^2$ given the constraint $2x^2 + y^2 = 1$",Find the minimum value of  given the constraint,"f(x,y) = x + y^2 2x^2 + y^2 = 1","Find the minimum value of $x + y^2$ subject to the condition $2x^2 + y^2 = 1$ . 1) I find $\nabla f$ and $\nabla g$ to get $$\nabla f(x,y) = (1, 2y) \\ \nabla g(x,y) = (4x, 2y)$$ Then I set up the system of equations \begin{align} \nabla f(x,y) &= \lambda g(x,y) \\ 1 &= \lambda 4x \\ 2y &= \lambda 2y \\ 2x^2 + y^2 &= 1 \end{align} I am having difficulties solving the system of equations. I got $\lambda = 1$ from $2y = \lambda 2y$ and from there I obtain $x = \frac{1}{4}$ . However I don't understand how I am supposed to solve for a value of $y$ . I tried to plug in $x = 1/4$ into $2x^2 + 2y^2 = 1$ but I got $y = \sqrt{7/4}$ which does not fit the system of equations. I think my setup is correct and I am doing the calculations wrong. I think I should be getting $x = 1/4$ and $y = \sqrt{7}/4$ . What do you guys think?",Find the minimum value of subject to the condition . 1) I find and to get Then I set up the system of equations I am having difficulties solving the system of equations. I got from and from there I obtain . However I don't understand how I am supposed to solve for a value of . I tried to plug in into but I got which does not fit the system of equations. I think my setup is correct and I am doing the calculations wrong. I think I should be getting and . What do you guys think?,"x + y^2 2x^2 + y^2 = 1 \nabla f \nabla g \nabla f(x,y) = (1, 2y) \\ \nabla g(x,y) = (4x, 2y) \begin{align}
\nabla f(x,y) &= \lambda g(x,y) \\
1 &= \lambda 4x \\
2y &= \lambda 2y \\
2x^2 + y^2 &= 1
\end{align} \lambda = 1 2y = \lambda 2y x = \frac{1}{4} y x = 1/4 2x^2 + 2y^2 = 1 y = \sqrt{7/4} x = 1/4 y = \sqrt{7}/4","['multivariable-calculus', 'optimization', 'maxima-minima', 'lagrange-multiplier', 'qcqp']"
1,Using a line integral to find work,Using a line integral to find work,,"For the life of me I can't see where I'm going wrong with this. Given the vector field $$ F(x, y) = \left[ xy, \frac{2y}{x} \right] $$ a particle travels on the hyperbola $$ \frac{x^2}{4} - y^2 = 1 $$ from $ \left(2\sqrt{10}, -3 \right) $ to $ \left(2, 0 \right) $ how much work is done by the force over the path? My take on this is to solve for x and parametrize $t = y^2 + 1$ such that $$ x = 2\sqrt{t} $$ $$ y = \sqrt{t-1} $$ $$ t\in\left[10,1\right]$$ for which $$ \int_C F(r(t)) \cdot r'(t) dt = \int_{10}^1 2\sqrt{t}\sqrt{t-1}\frac{1}{\sqrt{t}} + \frac{2\sqrt{t-1}}{2\sqrt{t}}\frac{1}{2\sqrt{t-1}} $$ $$ = \left. \left(\frac{4}{3}\left(t-1\right)^{\frac{3}{2}} + \sqrt{t} \right) \right|_{10}^1 = -35 - \sqrt{10} $$ would be grateful for any input.",For the life of me I can't see where I'm going wrong with this. Given the vector field a particle travels on the hyperbola from to how much work is done by the force over the path? My take on this is to solve for x and parametrize such that for which would be grateful for any input.," F(x, y) = \left[ xy, \frac{2y}{x} \right]   \frac{x^2}{4} - y^2 = 1   \left(2\sqrt{10}, -3 \right)   \left(2, 0 \right)  t = y^2 + 1  x = 2\sqrt{t}   y = \sqrt{t-1}   t\in\left[10,1\right]  \int_C F(r(t)) \cdot r'(t) dt = \int_{10}^1 2\sqrt{t}\sqrt{t-1}\frac{1}{\sqrt{t}} + \frac{2\sqrt{t-1}}{2\sqrt{t}}\frac{1}{2\sqrt{t-1}}   = \left. \left(\frac{4}{3}\left(t-1\right)^{\frac{3}{2}} + \sqrt{t} \right) \right|_{10}^1 = -35 - \sqrt{10} ",['multivariable-calculus']
2,How to reconcile these two Jacobi formuli,How to reconcile these two Jacobi formuli,,I understand (1). $\dfrac {\partial }{\partial A}\det \left( \textbf{f}(\left( A\right) \right) = \det \left( \textbf{f}(\left( A\right) \right) tr\left( \textbf{f}(\left( A\right) ^{-1}\dfrac {d\textbf{f}(A)}{\partial A}\right)  $ via Jacobi's Formula. Here A $\in \mathbb{R} ^{m\times n}$ . I also know as a special case ${\displaystyle {\partial \det(A) \over \partial A_{ij}}=\operatorname {adj} ^{\rm {T}}(A)_{ij}.} = det(A)(A)^{-T}_{ij}$ So $\dfrac {\partial \det \left( A\right) }{\partial A}=\det \left( A\right) A^{-T}$ But when i try and get this result from (1). where $\textbf{f}\left( A\right)$ = A. I get $\dfrac {\partial \det \left( A\right) }{\partial A}=\det \left( A\right) tr(A^{-1})$ which doesn't marry up?,I understand (1). via Jacobi's Formula. Here A . I also know as a special case So But when i try and get this result from (1). where = A. I get which doesn't marry up?,\dfrac {\partial }{\partial A}\det \left( \textbf{f}(\left( A\right) \right) = \det \left( \textbf{f}(\left( A\right) \right) tr\left( \textbf{f}(\left( A\right) ^{-1}\dfrac {d\textbf{f}(A)}{\partial A}\right)   \in \mathbb{R} ^{m\times n} {\displaystyle {\partial \det(A) \over \partial A_{ij}}=\operatorname {adj} ^{\rm {T}}(A)_{ij}.} = det(A)(A)^{-T}_{ij} \dfrac {\partial \det \left( A\right) }{\partial A}=\det \left( A\right) A^{-T} \textbf{f}\left( A\right) \dfrac {\partial \det \left( A\right) }{\partial A}=\det \left( A\right) tr(A^{-1}),"['multivariable-calculus', 'vector-analysis']"
3,"How to split D to calculate $\int_Df(x,y)d(x,y)$ for $f(x,y)=xe^{x^2+y^2}$",How to split D to calculate  for,"\int_Df(x,y)d(x,y) f(x,y)=xe^{x^2+y^2}","Let $$D=\{(x,y)\in \mathbb{R}^2: 1\le x^2+y^2 \le 4 ,\quad y\ge0\}$$ Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be defined as $$f(x,y)=xe^{x^2+y^2}$$ Calculate $\int_Df(x,y)d(x,y)$ I believe the order of integration must be $dxdy$ . However, I can't clearly express $x$ as a function of $y$ . What is the correct way to split the set $D$ so we can integrate?","Let Let be defined as Calculate I believe the order of integration must be . However, I can't clearly express as a function of . What is the correct way to split the set so we can integrate?","D=\{(x,y)\in \mathbb{R}^2: 1\le x^2+y^2 \le 4 ,\quad y\ge0\} f: \mathbb{R}^2 \rightarrow \mathbb{R} f(x,y)=xe^{x^2+y^2} \int_Df(x,y)d(x,y) dxdy x y D","['calculus', 'integration', 'multivariable-calculus']"
4,Finding the normal to a curve of an implicit function using differentiation,Finding the normal to a curve of an implicit function using differentiation,,"The question asks to find the tangent and normal to the curve of the following equation at the point ( $\sqrt3, 2$ ): $$x^2-\sqrt{3}xy+2y^2=5 $$ I began by finding the first derivative of this, which I found to be: $$ \frac{dy}{dx}=\frac{\sqrt3y-2x}{-\sqrt3x+4y}$$ The slope was obviously zero at the given point. Hence, $$y=mx+c$$ $$ y=c$$ $$ \therefore c = 2$$ $$ \therefore y = 2$$ However, when I cannot seem to find the correct answer for the equation fo the normal. The answer in my book states the equation for the normal is $x=\sqrt3$ , my attempt at finding the slope of the normal to find its equation was: $$m_1m_2=-1$$ $$m_1 = 0$$ $$ \therefore m_2 = undefined$$ This clearly does not correspond with the book's answer and I do not understand why my method to to find the slope of the normal does not work. Perhaps the formula I used is not feasible for implicit equations? Could someone enlighten me of my mistake and show me how to obtain the correct answer?","The question asks to find the tangent and normal to the curve of the following equation at the point ( ): I began by finding the first derivative of this, which I found to be: The slope was obviously zero at the given point. Hence, However, when I cannot seem to find the correct answer for the equation fo the normal. The answer in my book states the equation for the normal is , my attempt at finding the slope of the normal to find its equation was: This clearly does not correspond with the book's answer and I do not understand why my method to to find the slope of the normal does not work. Perhaps the formula I used is not feasible for implicit equations? Could someone enlighten me of my mistake and show me how to obtain the correct answer?","\sqrt3, 2 x^2-\sqrt{3}xy+2y^2=5   \frac{dy}{dx}=\frac{\sqrt3y-2x}{-\sqrt3x+4y} y=mx+c  y=c  \therefore c = 2  \therefore y = 2 x=\sqrt3 m_1m_2=-1 m_1 = 0  \therefore m_2 = undefined","['multivariable-calculus', 'implicit-differentiation', 'tangent-line', 'slope', 'implicit-function']"
5,d(vector)/d(matrix) in neural-network-deep-learning context,d(vector)/d(matrix) in neural-network-deep-learning context,,"When learning about Neural Networks, you are bound to stumble upon the technique of backpropagation/gradient descent. To give a brief explanation of the scenario we are to look at, $$L = g(Wx+b) = g(a).$$ L is a scalar, W is a matrix of appropriate size, x is a vector given as input, b is a vector of appropriate size. g is a function that takes a vector of appropriate size and evaluates a scalar. W and b are to be updated in the opposite direction of the gradient,    so that L can be minimized . Many textbooks, lectures and blogs, including Speech and Language Processing (3rd ed. draft) or CS231n, use notations(See page 5) that look like this. $$\frac{\partial L}{\partial V} = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial V}$$ here, L is the ""loss function"" defined above, a, z are vectors of appropriate size, and V is a matrix of parameters, like W discussed above. Overall this equation was written so that we may find the derivative of L by ""the weights"" V. From this, it seems only logical to assume that we may attempt to get $\frac{\partial L}{\partial W}$ in this manner. $$\frac{\partial L}{\partial W} = \frac{\partial g}{\partial a} \frac{\partial a}{\partial W}$$ But here is the problem I have with both expressions: on the right-hand side, there is a derivation of a vector by a matrix. Now, from a few sources I know that the notation nor the multiplication of this Frankenstein is not defined rigorously, if it is defined at all. Since we typically add $\eta \frac{\partial L}{\partial W}$ to $W$ directly, we must be following the Denominator-layout notation as explained here , but I can't seem to figure the $\partial a/\partial W$ part out. I did look over as many blogposts as I can, but virtually none of them actually show how the calculation should be done. So my question is as follows. Is that kind of notation(da/dW) even legal? If it is legal, how should it be evaluated and multiplicated in this context? Can you give an example? Is da/dW possible to evaluate without doing it term-by-term?(evaluating da/dW11, da/dW12, ... and compiling them up later in some way.) What about dL/dW? Thank you in advance.","When learning about Neural Networks, you are bound to stumble upon the technique of backpropagation/gradient descent. To give a brief explanation of the scenario we are to look at, L is a scalar, W is a matrix of appropriate size, x is a vector given as input, b is a vector of appropriate size. g is a function that takes a vector of appropriate size and evaluates a scalar. W and b are to be updated in the opposite direction of the gradient,    so that L can be minimized . Many textbooks, lectures and blogs, including Speech and Language Processing (3rd ed. draft) or CS231n, use notations(See page 5) that look like this. here, L is the ""loss function"" defined above, a, z are vectors of appropriate size, and V is a matrix of parameters, like W discussed above. Overall this equation was written so that we may find the derivative of L by ""the weights"" V. From this, it seems only logical to assume that we may attempt to get in this manner. But here is the problem I have with both expressions: on the right-hand side, there is a derivation of a vector by a matrix. Now, from a few sources I know that the notation nor the multiplication of this Frankenstein is not defined rigorously, if it is defined at all. Since we typically add to directly, we must be following the Denominator-layout notation as explained here , but I can't seem to figure the part out. I did look over as many blogposts as I can, but virtually none of them actually show how the calculation should be done. So my question is as follows. Is that kind of notation(da/dW) even legal? If it is legal, how should it be evaluated and multiplicated in this context? Can you give an example? Is da/dW possible to evaluate without doing it term-by-term?(evaluating da/dW11, da/dW12, ... and compiling them up later in some way.) What about dL/dW? Thank you in advance.",L = g(Wx+b) = g(a). \frac{\partial L}{\partial V} = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial V} \frac{\partial L}{\partial W} \frac{\partial L}{\partial W} = \frac{\partial g}{\partial a} \frac{\partial a}{\partial W} \eta \frac{\partial L}{\partial W} W \partial a/\partial W,"['multivariable-calculus', 'derivatives', 'matrix-calculus', 'gradient-descent', 'neural-networks']"
6,Show that $\sum_{cyc} \sqrt{8a+b^3}\ge 9$,Show that,\sum_{cyc} \sqrt{8a+b^3}\ge 9,"Prove that $$\sqrt{8a+b^3}+\sqrt{8b+c^3}+\sqrt{8c+a^3}\ge 9 \text{   for  }  a,b,c\ge0 \text{   and  } a+b+c=3.$$ By Holder $$\left(\sum _{cyc}\sqrt{8a+b^3}\right)^2\left(\sum _{cyc}\frac{1}{8a+b^3}\right)\ge 27$$ Or $$\sum _{cyc}\frac{1}{8a+b^3}\le \frac{1}{3} \text{ WLOG  } 0\le a\le b =a+u\le c=a+v\le 3$$ By full expanding it's obvious, because: $$u^2-uv+v^2\ge 0$$ $$\cdots $$ $$72u^9-80u^8v-776u^7v^2-591u^6v^3+683u^5v^4+1403u^4v^5+1569u^3v^6+1168u^2v^7+424uv^8+72v^9\ge $$ $$\ge uv(v-u)(80u^6+\cdots 80v^6)\ge 0$$ My solution is very ugly. Can i solve it by Holder but more beautiful than it does? Help me","Prove that By Holder Or By full expanding it's obvious, because: My solution is very ugly. Can i solve it by Holder but more beautiful than it does? Help me","\sqrt{8a+b^3}+\sqrt{8b+c^3}+\sqrt{8c+a^3}\ge 9 \text{ 
 for  }  a,b,c\ge0 \text{ 
 and  } a+b+c=3. \left(\sum _{cyc}\sqrt{8a+b^3}\right)^2\left(\sum _{cyc}\frac{1}{8a+b^3}\right)\ge 27 \sum _{cyc}\frac{1}{8a+b^3}\le \frac{1}{3} \text{ WLOG  } 0\le a\le b =a+u\le c=a+v\le 3 u^2-uv+v^2\ge 0 \cdots  72u^9-80u^8v-776u^7v^2-591u^6v^3+683u^5v^4+1403u^4v^5+1569u^3v^6+1168u^2v^7+424uv^8+72v^9\ge  \ge uv(v-u)(80u^6+\cdots 80v^6)\ge 0","['multivariable-calculus', 'inequality', 'proof-explanation', 'holder-inequality']"
7,"What, precisely, is the derivative function?","What, precisely, is the derivative function?",,"I would like to illustrate my confusion about this topic by building up the issue from more or less first principles. Let $U \subseteq \mathbb{R}^n$ be an open subset and let $f:U \to \mathbb{R}^m$ . We say that $f$ is totally differentiable at $a \in U$ if there exists a linear function $D_{a}f:\mathbb{R}^n \to \mathbb{R}^m$ such that the following holds. $\lim_{x \to a} \frac{||f(x)-f(a)-D_{a}f(x-a)||}{||x-a||} = 0$ We call $D_{a}f$ the total derivative of $f$ at $a$ . My question concerns how we then define the derivative function. For the familiar case in which $n=m=1$ , this is simple since the total derivative reduces to just $(D_{a}f)(b) = \big(\frac{df}{dx}\big{\rvert}_a\big)b$ for all $b \in \mathbb{R}$ . Here, $\frac{df}{dx}\big{\rvert}_a$ denotes the usual definition of the derivative of $f$ at $a$ . We then define the derivative function $f':U \to \mathbb{R}$ as $f'(y) := \frac{df}{dx}\big{\rvert}_y$ for all $y \in U$ . Let us now consider the case when $m=1$ . Representing the action of $D_{a}f$ via the Jacobian matrix, we have $(D_{a}f)(b) = \begin{bmatrix} \frac{\partial{f}}{\partial{x_1}}\big{\rvert}_a & \cdots & \frac{\partial{f}}{\partial{x_n}}\big{\rvert}_a \end{bmatrix} \begin{bmatrix} b_1 \\\ \vdots \\\ b_n \end{bmatrix}$ for all $b=(b_1,\cdots,b_n)\in\mathbb{R}^n$ . This suggests that we define the derivative function $Df:U \to \mathbb{R}^n$ as $(Df)(y) := \big(\frac{\partial{f}}{\partial{x_1}}\big{\rvert}_y, \cdots, \frac{\partial{f}}{\partial{x_n}}\big{\rvert}_y\big)$ for all $y \in U$ . We could perhaps make a similar argument for the case in which $n=1$ . How does this extend to cases in which the Jacobian matrix is not a simple column or row vector? What definition of the derivative function is most natural in those cases? By natural, I mean that the definition should allow important identities like the product rule and chain rule to retain their obvious forms.","I would like to illustrate my confusion about this topic by building up the issue from more or less first principles. Let be an open subset and let . We say that is totally differentiable at if there exists a linear function such that the following holds. We call the total derivative of at . My question concerns how we then define the derivative function. For the familiar case in which , this is simple since the total derivative reduces to just for all . Here, denotes the usual definition of the derivative of at . We then define the derivative function as for all . Let us now consider the case when . Representing the action of via the Jacobian matrix, we have for all . This suggests that we define the derivative function as for all . We could perhaps make a similar argument for the case in which . How does this extend to cases in which the Jacobian matrix is not a simple column or row vector? What definition of the derivative function is most natural in those cases? By natural, I mean that the definition should allow important identities like the product rule and chain rule to retain their obvious forms.","U \subseteq \mathbb{R}^n f:U \to \mathbb{R}^m f a \in U D_{a}f:\mathbb{R}^n \to \mathbb{R}^m \lim_{x \to a} \frac{||f(x)-f(a)-D_{a}f(x-a)||}{||x-a||} = 0 D_{a}f f a n=m=1 (D_{a}f)(b) = \big(\frac{df}{dx}\big{\rvert}_a\big)b b \in \mathbb{R} \frac{df}{dx}\big{\rvert}_a f a f':U \to \mathbb{R} f'(y) := \frac{df}{dx}\big{\rvert}_y y \in U m=1 D_{a}f (D_{a}f)(b) = \begin{bmatrix}
\frac{\partial{f}}{\partial{x_1}}\big{\rvert}_a & \cdots & \frac{\partial{f}}{\partial{x_n}}\big{\rvert}_a
\end{bmatrix}
\begin{bmatrix}
b_1 \\\
\vdots \\\
b_n
\end{bmatrix} b=(b_1,\cdots,b_n)\in\mathbb{R}^n Df:U \to \mathbb{R}^n (Df)(y) := \big(\frac{\partial{f}}{\partial{x_1}}\big{\rvert}_y, \cdots, \frac{\partial{f}}{\partial{x_n}}\big{\rvert}_y\big) y \in U n=1","['multivariable-calculus', 'derivatives']"
8,System of Equations with Trigonometric Expressions,System of Equations with Trigonometric Expressions,,"Find the maxima and minima of the function $$f(x,y) = x\sin(x)+\cos(x)+3y-y^3$$ under the constraint $$g(x,y)=4x^2+y^2+2y=0.$$ I have arrived at the following set of equations using the Lagrangian function: $$\begin{equation}\begin{aligned}  \cos(x)&=8\lambda\\ 3-2\lambda&=2\lambda y+3y^2\\ 4x^2&=-y^2-2y \end{aligned}\end{equation}$$ There is also a hint: $$\forall x \in \left(0,\frac{\pi}{4}\right]:\quad\frac{1}{2}\sin(x)+\cos(x)\in\left(1,\frac{3}{2}\right)$$ How can I solve the equations for the possible maxima/minima?",Find the maxima and minima of the function under the constraint I have arrived at the following set of equations using the Lagrangian function: There is also a hint: How can I solve the equations for the possible maxima/minima?,"f(x,y) = x\sin(x)+\cos(x)+3y-y^3 g(x,y)=4x^2+y^2+2y=0. \begin{equation}\begin{aligned} 
\cos(x)&=8\lambda\\
3-2\lambda&=2\lambda y+3y^2\\
4x^2&=-y^2-2y
\end{aligned}\end{equation} \forall x \in \left(0,\frac{\pi}{4}\right]:\quad\frac{1}{2}\sin(x)+\cos(x)\in\left(1,\frac{3}{2}\right)","['multivariable-calculus', 'optimization', 'systems-of-equations', 'lagrange-multiplier', 'maxima-minima']"
9,"Limit of $\frac{x^2}{x^2 + y^2}$ as $(x,y) \to (0,0)$",Limit of  as,"\frac{x^2}{x^2 + y^2} (x,y) \to (0,0)","I am very new to multivariate calculus and I came up for an incorrect ""proof"" that $$\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^2} = 0.$$ I understand that the limit does not exist because approaching with $x=0$ and with $y=0$ yield different answers, however after several proofreads I do not understand where I have gone wrong with the ""proof"" given below. We wish to show that for every $\epsilon$ there exists a $\delta$ such that $$\sqrt{x^2+y^2} < \delta \implies | \frac{x^2}{x^2 + y^2} | < \epsilon.$$ Choose $\delta = \min({\sqrt{\epsilon}, 1}).$ Now, because $\delta$ is at most 1, $$\sqrt{x^2+y^2} < \delta \leq 1$$ $$1 < \frac{1}{x^2+y^2}$$ $$\sqrt{x^2+y^2} < \delta \implies \frac{x^2}{1} < \epsilon.$$ Substituting in $\delta = \sqrt{\epsilon}$ , we obtain $$x^2+y^2 < \epsilon \implies x^2 < \epsilon$$ which is true.","I am very new to multivariate calculus and I came up for an incorrect ""proof"" that I understand that the limit does not exist because approaching with and with yield different answers, however after several proofreads I do not understand where I have gone wrong with the ""proof"" given below. We wish to show that for every there exists a such that Choose Now, because is at most 1, Substituting in , we obtain which is true.","\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^2} = 0. x=0 y=0 \epsilon \delta \sqrt{x^2+y^2} < \delta \implies | \frac{x^2}{x^2 + y^2} | < \epsilon. \delta = \min({\sqrt{\epsilon}, 1}). \delta \sqrt{x^2+y^2} < \delta \leq 1 1 < \frac{1}{x^2+y^2} \sqrt{x^2+y^2} < \delta \implies \frac{x^2}{1} < \epsilon. \delta = \sqrt{\epsilon} x^2+y^2 < \epsilon \implies x^2 < \epsilon","['limits', 'multivariable-calculus', 'epsilon-delta']"
10,Prove $\operatorname{ div} (\phi A) = (\operatorname{ grad} \phi) \cdot A + \phi \operatorname{ div} A$,Prove,\operatorname{ div} (\phi A) = (\operatorname{ grad} \phi) \cdot A + \phi \operatorname{ div} A,"Prove $$\operatorname{ div} (\phi A) = (\operatorname{grad} \phi) \cdot A + \phi \operatorname{ div} A$$ where $A$ is differentiable vector function and $\phi $ is differentiable scalar function. Now i write $A =(A_1. A_2. A_3)$ and so i write as $ div (\phi A) = (f_x,f_y,f_z) \cdot (\phi A_1,\phi A_3,\phi A_3)$ which equals to $f_x(\phi A_1) + f_y(\phi A_2) + f_z(\phi A_3)  $ . how do i proceed? thanks",Prove where is differentiable vector function and is differentiable scalar function. Now i write and so i write as which equals to . how do i proceed? thanks,"\operatorname{ div} (\phi A) = (\operatorname{grad} \phi) \cdot A + \phi \operatorname{ div} A A \phi  A =(A_1. A_2. A_3)  div (\phi A) = (f_x,f_y,f_z) \cdot (\phi A_1,\phi A_3,\phi A_3) f_x(\phi A_1) + f_y(\phi A_2) + f_z(\phi A_3)  ","['multivariable-calculus', 'vector-analysis']"
11,"Continuity and differentiability of $g(x,y)=\frac{x^2+y^2}{\phi_{(x,y)}}$ at the origin.",Continuity and differentiability of  at the origin.,"g(x,y)=\frac{x^2+y^2}{\phi_{(x,y)}}","For $(x,y)\in \mathbb{R}^2,~(x,y)\neq (0,0)$ set $\theta_{(x,y)}\in [0,2\pi)$ the angle between positive x axis and the line connecting $(0,0)$ and $(x,y)$ and $$\phi_{(x,y)}= \left \{\begin {array}{ll} \theta_{(x,y)} &,  \theta_{(x,y)} \in [0,\pi)\\ \theta_{(x,y)}-\pi &, \theta_{(x,y)}\in [\pi,2\pi) \end{array} \right..$$ Let $$g:\mathbb{R}^2\to \mathbb{R}: g(x,y)= \left \{\begin {array}{ll} \frac{\|(x,y)\|^2}{\phi_{(x,y)}} & , (x,y) \neq (0,0),~\phi_{(x,y)}\neq 0\\ 0 &, otherwise \end{array} \right..$$ Determine if $g$ is continuous and differentiable at the origin. Attempt. Since $\phi(x,0)=0$ and $\phi(0,y)=\frac{\pi}{2}$ for all $x,\,y\neq 0$ we get: $$g_x(0,0)=\lim_{x\to 0}\frac{g(x,0)-g(0,0)}{x-0}=\lim_{x\to 0}\frac{0-0}{x-0}=0$$ and $$g_y(0,0)=\lim_{y\to 0}\frac{g(0,y)-g(0,0)}{y-0}=\frac{2}{\pi}\lim_{y\to 0}y=0.$$ I believe that $g$ is continuous at the origin, so I need to prove that $g(x,y)\leqslant h(x,y)$ for some h having limit equal to $0$ at the origin. But for $\phi_{(x,y)}$ being nonzero but small, how can we control $\frac{1}{\phi_{(x,y)}}$ ? Regarding differentiability, we need to test if the limit: $$\lim_{(x,y)\to (0,0)}\frac{g(x,y)}{\sqrt{x^2+y^2}}$$ equals zero. Thanks for the help.","For set the angle between positive x axis and the line connecting and and Let Determine if is continuous and differentiable at the origin. Attempt. Since and for all we get: and I believe that is continuous at the origin, so I need to prove that for some h having limit equal to at the origin. But for being nonzero but small, how can we control ? Regarding differentiability, we need to test if the limit: equals zero. Thanks for the help.","(x,y)\in \mathbb{R}^2,~(x,y)\neq (0,0) \theta_{(x,y)}\in [0,2\pi) (0,0) (x,y) \phi_{(x,y)}= \left \{\begin {array}{ll}
\theta_{(x,y)} &,  \theta_{(x,y)} \in [0,\pi)\\
\theta_{(x,y)}-\pi &, \theta_{(x,y)}\in [\pi,2\pi)
\end{array}
\right.. g:\mathbb{R}^2\to \mathbb{R}: g(x,y)= \left \{\begin {array}{ll}
\frac{\|(x,y)\|^2}{\phi_{(x,y)}} & , (x,y) \neq (0,0),~\phi_{(x,y)}\neq 0\\
0 &, otherwise
\end{array}
\right.. g \phi(x,0)=0 \phi(0,y)=\frac{\pi}{2} x,\,y\neq 0 g_x(0,0)=\lim_{x\to 0}\frac{g(x,0)-g(0,0)}{x-0}=\lim_{x\to 0}\frac{0-0}{x-0}=0 g_y(0,0)=\lim_{y\to 0}\frac{g(0,y)-g(0,0)}{y-0}=\frac{2}{\pi}\lim_{y\to 0}y=0. g g(x,y)\leqslant h(x,y) 0 \phi_{(x,y)} \frac{1}{\phi_{(x,y)}} \lim_{(x,y)\to (0,0)}\frac{g(x,y)}{\sqrt{x^2+y^2}}","['real-analysis', 'limits', 'analysis', 'multivariable-calculus']"
12,"$\iiint_vz^2\, \mathrm dx \, \mathrm dy\, \mathrm dz$, $x^2 +y^2 +(z-R)^2\leq R^2$",",","\iiint_vz^2\, \mathrm dx \, \mathrm dy\, \mathrm dz x^2 +y^2 +(z-R)^2\leq R^2","$\iiint_vz^2dxdydz$ , $x^2 +y^2 +(z-R)^2\leq R^2$ . $\theta$ interval will be from $0$ to $2\pi$ . I substituted $x^2 +y^2$ for $r^2$ and got $(z-R)^2\leq R^2 -r^2$ . Thus $z\in[R-\sqrt{R^2-r^2}, R +\sqrt{R^2 -r^2}]$ and $r$ belongs to $[0,R]$ . Is that correct, because I got $0$ from calculating triple integral (understandable, since there is no $\theta$ in primary function $z^2$ or in any interval). Is it correct?",", . interval will be from to . I substituted for and got . Thus and belongs to . Is that correct, because I got from calculating triple integral (understandable, since there is no in primary function or in any interval). Is it correct?","\iiint_vz^2dxdydz x^2 +y^2 +(z-R)^2\leq R^2 \theta 0 2\pi x^2 +y^2 r^2 (z-R)^2\leq R^2 -r^2 z\in[R-\sqrt{R^2-r^2}, R +\sqrt{R^2 -r^2}] r [0,R] 0 \theta z^2","['calculus', 'multivariable-calculus']"
13,Why normal vector's formula for implicit function is different than for explicit function?,Why normal vector's formula for implicit function is different than for explicit function?,,"Find tangent plane to surface: $z = x^2 + y^2 - 3$ at point $P(2, 1, 2)$ The normal vector to the plane I am looking for is defined as: $$\vec{n} = [\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, -1] \Big|_{P(2,1,2)}$$ I think I understand everything. Problem is when I am dealing with an implicit form of function. Find tangent plane to $4x^2 + 2y^2 + z^2 - 12 = 0$ at point $P(1, -\sqrt{2}, -2)$ So the normal vector is defined as: $$\vec{n} = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}] \Big|_{P(1, -\sqrt{2}, -2)}$$ I am still getting good results. But I simply cannot understand what is going on. The second, implicit function, could be rewritten in explicit form as something like: $$z = \pm \sqrt{-4x^2 - 2y^2 + 12} = f(x, y)$$ So... it is really confusing to me why in the first normal vector's equation there's $-1$ and in the second there's $\frac{\partial f}{\partial z}$ instead. After all I am dealing with function of two variables in both cases, right? And, after I convert the function from implicit to explicit form (and even before conversion), shouldn't I use $\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, \frac{\partial z}{\partial z}$ notation instead of $\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}$ ? To show what I am struggling with to understand: let's say I have function $y = x^2 = f(x)$ . Clearly I can find $y'$ , but I cannot find $x'$ , right? Can someone help me understand this, how is it possible that the first normal vector equation can be that much different than the implicit one? Are those two functions: implicit and explicit, even equivalent or not?","Find tangent plane to surface: at point The normal vector to the plane I am looking for is defined as: I think I understand everything. Problem is when I am dealing with an implicit form of function. Find tangent plane to at point So the normal vector is defined as: I am still getting good results. But I simply cannot understand what is going on. The second, implicit function, could be rewritten in explicit form as something like: So... it is really confusing to me why in the first normal vector's equation there's and in the second there's instead. After all I am dealing with function of two variables in both cases, right? And, after I convert the function from implicit to explicit form (and even before conversion), shouldn't I use notation instead of ? To show what I am struggling with to understand: let's say I have function . Clearly I can find , but I cannot find , right? Can someone help me understand this, how is it possible that the first normal vector equation can be that much different than the implicit one? Are those two functions: implicit and explicit, even equivalent or not?","z = x^2 + y^2 - 3 P(2, 1, 2) \vec{n} = [\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, -1] \Big|_{P(2,1,2)} 4x^2 + 2y^2 + z^2 - 12 = 0 P(1, -\sqrt{2}, -2) \vec{n} = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}] \Big|_{P(1, -\sqrt{2}, -2)} z = \pm \sqrt{-4x^2 - 2y^2 + 12} = f(x, y) -1 \frac{\partial f}{\partial z} \frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, \frac{\partial z}{\partial z} \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z} y = x^2 = f(x) y' x'","['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'surfaces', 'implicit-differentiation']"
14,How to come up with the Jacobian in the change of variables formula,How to come up with the Jacobian in the change of variables formula,,"According to the change of variables formula for multivariable calculus, $$d\vec{v}=\left|\det(D\varphi)(\vec{u})\right|d\vec{u}$$ where $\vec{v}=\varphi\vec{u}$ and $\det(D\varphi)(\vec{u})$ is the Jacobian matrix of the partial derivatives of $\varphi$ at the point $\vec{u}$ . How to come up with this relationship (preferably conceptually)?","According to the change of variables formula for multivariable calculus, where and is the Jacobian matrix of the partial derivatives of at the point . How to come up with this relationship (preferably conceptually)?",d\vec{v}=\left|\det(D\varphi)(\vec{u})\right|d\vec{u} \vec{v}=\varphi\vec{u} \det(D\varphi)(\vec{u}) \varphi \vec{u},"['multivariable-calculus', 'determinant', 'jacobian', 'change-of-variable']"
15,Generalized Laplacian?,Generalized Laplacian?,,"I was wondering if any of you had ever encountered operators on $L^2(\mathbb{R}^d)$ of the form $$ - \nabla \cdot A(x)\nabla $$ where $A(x)$ is some matrix field (viewed as $L^2(\mathbb{R}^{d^2}$ )), and if so where can I find some literature about it ? More precisely, I'm trying to get a lower bound, $i.e$ hopefully some function $h(x)$ such that $- \nabla \cdot A(x)\nabla \geq h(x)$ (in the sense of quadratic forms), when $A(x) = (2|x| - |x|^{-1})xx^T$ . Thanks a lot!","I was wondering if any of you had ever encountered operators on of the form where is some matrix field (viewed as )), and if so where can I find some literature about it ? More precisely, I'm trying to get a lower bound, hopefully some function such that (in the sense of quadratic forms), when . Thanks a lot!","L^2(\mathbb{R}^d) 
- \nabla \cdot A(x)\nabla
 A(x) L^2(\mathbb{R}^{d^2} i.e h(x) - \nabla \cdot A(x)\nabla \geq h(x) A(x) = (2|x| - |x|^{-1})xx^T","['multivariable-calculus', 'operator-theory']"
16,"If a multivariate function has a gradient at $x$, then it is always differentiable at $x$, right?","If a multivariate function has a gradient at , then it is always differentiable at , right?",x x,"By being differentiable, I mean  a function of several real variables $f: \mathbb{R^m}\rightarrow \mathbb{R}$ is said to be differentiable at a point $x_0$ if there exists a linear map $J: \mathbb{R}^m → \mathbb{R}$ such that $\lim_{\mathbf{h}\to \mathbf{0}} \frac{\|\mathbf{f}(\mathbf{x_0}+\mathbf{h}) - \mathbf{f}(\mathbf{x_0}) - \mathbf{J}\mathbf{(h)}\|}{\| \mathbf{h} \|} = 0.$ By having a gradient at $x_0$ , I mean there exists a vector denoted as $\nabla f$ such that its dot product with any unit vector $v$ is the directional derivative of $f$ along $v$ at the point $x_0$ . That is, $$\nabla f \cdot v = D_v f(x_0)$$ We assume all the directional derivative of $f$ at $x_0$ exists, that being said $f$ may or may not be differentiable at $x_0$ , if in addition we assume the gradient exists at $x_0$ , then $f$ should be differentiable at $x_0$ , right? I think this is simple or well known question, I can visualize it in the following way: if $f$ has a gradient at $x_0$ then all its directional derivative should lie in the same plane defined by $\nabla f$ , therefore it should be differentiable using the definition above.  But surprisingly I couldn't find any rigorous proof or material discussing this. Is this too simple or too well-known?","By being differentiable, I mean  a function of several real variables is said to be differentiable at a point if there exists a linear map such that By having a gradient at , I mean there exists a vector denoted as such that its dot product with any unit vector is the directional derivative of along at the point . That is, We assume all the directional derivative of at exists, that being said may or may not be differentiable at , if in addition we assume the gradient exists at , then should be differentiable at , right? I think this is simple or well known question, I can visualize it in the following way: if has a gradient at then all its directional derivative should lie in the same plane defined by , therefore it should be differentiable using the definition above.  But surprisingly I couldn't find any rigorous proof or material discussing this. Is this too simple or too well-known?",f: \mathbb{R^m}\rightarrow \mathbb{R} x_0 J: \mathbb{R}^m → \mathbb{R} \lim_{\mathbf{h}\to \mathbf{0}} \frac{\|\mathbf{f}(\mathbf{x_0}+\mathbf{h}) - \mathbf{f}(\mathbf{x_0}) - \mathbf{J}\mathbf{(h)}\|}{\| \mathbf{h} \|} = 0. x_0 \nabla f v f v x_0 \nabla f \cdot v = D_v f(x_0) f x_0 f x_0 x_0 f x_0 f x_0 \nabla f,"['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'vector-analysis']"
17,Integral of a differential 2-form in a parametrized surface,Integral of a differential 2-form in a parametrized surface,,"I want to calculate the integral $\mathop{\iint}_{\phi}xdx\land dy+ydy\land dz+z dz\land dx$ considering $\left\{ \phi\left(u,v\right):=\left(u+v,uv,u^{2}-v^{2}\right):u,v\in\left[0,1\right]\right\} $ And the Stokes Theorem says that  integral of a differential form ω over the boundary of some orientable manifold Ω is equal to the integral of its exterior derivative dω over the whole of Ω. I know that $d\left(Fdx+Gdy+Hdz\right)=\left(G_{x}-F_{y}\right)dx\land dy+\left(H_{y}-G_{z}\right)dy\land dz+\left(F_{z}-H_{x}\right)dz\land dx$ How should I do that? I only find examples of vector fields and I don't know if this should be converted to something similar to a vector field.",I want to calculate the integral considering And the Stokes Theorem says that  integral of a differential form ω over the boundary of some orientable manifold Ω is equal to the integral of its exterior derivative dω over the whole of Ω. I know that How should I do that? I only find examples of vector fields and I don't know if this should be converted to something similar to a vector field.,"\mathop{\iint}_{\phi}xdx\land dy+ydy\land dz+z dz\land dx \left\{ \phi\left(u,v\right):=\left(u+v,uv,u^{2}-v^{2}\right):u,v\in\left[0,1\right]\right\}  d\left(Fdx+Gdy+Hdz\right)=\left(G_{x}-F_{y}\right)dx\land dy+\left(H_{y}-G_{z}\right)dy\land dz+\left(F_{z}-H_{x}\right)dz\land dx",['multivariable-calculus']
18,Find the minimum of $\space\frac{1}{x}+\frac{1}{y}+c\cdot xy\space$ subject to $\space x+y-c=0$,Find the minimum of  subject to,\space\frac{1}{x}+\frac{1}{y}+c\cdot xy\space \space x+y-c=0,"Let $f(x,y):\mathbb{D}\rightarrow\mathbb{R}$ be the function: $$f(x,y)=\frac{1}{x}+\frac{1}{y}+c\cdot xy\space\space|\space\space c\in(0,\sqrt[4]8)\text{ $\space$constant}$$ $$\mathbb{D}=\{(x,y)\space|\space x>0,\space y>0\}$$ Find the point $P$ where $f$ gets its minimum value, subject to the equality: $$x+y-c=0$$ I tried : 1) Lagrange multipliers. Unfortunately, they don't seem to help since I get equations I cannot solve (5th degree). 2) Substituting $y=c-x$ to $f$ in order to solve $\frac{d}{dx}f(x,c-x)=0$ . That wasn't helpful either, from the same reasons. Final Solution : The final solution should be $(\frac{c}{2},\frac{c}{2})$ ; However I could not figure out how to find it by myself. Thanks!","Let be the function: Find the point where gets its minimum value, subject to the equality: I tried : 1) Lagrange multipliers. Unfortunately, they don't seem to help since I get equations I cannot solve (5th degree). 2) Substituting to in order to solve . That wasn't helpful either, from the same reasons. Final Solution : The final solution should be ; However I could not figure out how to find it by myself. Thanks!","f(x,y):\mathbb{D}\rightarrow\mathbb{R} f(x,y)=\frac{1}{x}+\frac{1}{y}+c\cdot xy\space\space|\space\space c\in(0,\sqrt[4]8)\text{ \spaceconstant} \mathbb{D}=\{(x,y)\space|\space x>0,\space y>0\} P f x+y-c=0 y=c-x f \frac{d}{dx}f(x,c-x)=0 (\frac{c}{2},\frac{c}{2})","['multivariable-calculus', 'derivatives', 'optimization', 'lagrange-multiplier', 'maxima-minima']"
19,"Classifying the stationary points of $f(x, y) = 4xy-x^4-y^4 $",Classifying the stationary points of,"f(x, y) = 4xy-x^4-y^4 ","$f(x, y) = 4xy-x^4-y^4 $ The gradient of this function is $0$ in $(-1, -1), (0, 0),(1, 1)$ I tried to compute the determinant of the Hessian Matrix, but it's 0 for every point, I always get a null eigenvalue for each point, so no matter what I can't find out what kind of points these are. For $(0, 0)$ , since $f(0, 0) = 0$ , $f(x, 0) = -x^4$ and $f(0, y) = -y^4$ . Since both will always be negative, isn't that supposed to be a maximum or a minimum point? According to Wolfram it's a saddle point and I can't understand why.","The gradient of this function is in I tried to compute the determinant of the Hessian Matrix, but it's 0 for every point, I always get a null eigenvalue for each point, so no matter what I can't find out what kind of points these are. For , since , and . Since both will always be negative, isn't that supposed to be a maximum or a minimum point? According to Wolfram it's a saddle point and I can't understand why.","f(x, y) = 4xy-x^4-y^4  0 (-1, -1), (0, 0),(1, 1) (0, 0) f(0, 0) = 0 f(x, 0) = -x^4 f(0, y) = -y^4","['multivariable-calculus', 'maxima-minima', 'hessian-matrix']"
20,Confused about the Jacobian matrix,Confused about the Jacobian matrix,,"Find $$\int_0^{\infty} \int_0^{\infty} e^{-2xy} \, \mathrm d x \mathrm dy$$ using $u = x^2 - y^2$ and $v=2xy$ . I have tried using the Jacobian matrix to obtain the Jacobian of the transformation. However, confusion arises since I do not know what should be kept constant. Do I directly differentiate $u$ with respect to $x$ while keeping $y$ constant, or do I substitute $y$ from $v=2xy$ , and then differentiate $u$ with respect to $x$ while keeping $v$ constant?","Find using and . I have tried using the Jacobian matrix to obtain the Jacobian of the transformation. However, confusion arises since I do not know what should be kept constant. Do I directly differentiate with respect to while keeping constant, or do I substitute from , and then differentiate with respect to while keeping constant?","\int_0^{\infty} \int_0^{\infty} e^{-2xy} \, \mathrm d x \mathrm dy u = x^2 - y^2 v=2xy u x y y v=2xy u x v","['calculus', 'multivariable-calculus', 'partial-derivative', 'jacobian']"
21,Proving there is no plane tangent to a graph,Proving there is no plane tangent to a graph,,"Define $$f(x, y) = \begin{cases}  \sin(y^2/x)\sqrt{x^2 + y^2} & \text{ if } x \neq 0 \\ 0 & \text{ if } x = 0. \end{cases}$$ (a) Show $f : \mathbb{R}^{2} \rightarrow \mathbb{R}$ has directional derivatives in every direction at $(0, 0)$ . (b) Show there is no plane that is tangent to the graph of $f : \mathbb{R}^{2} \rightarrow \mathbb{R}$ at the point $(0, 0, f(0,0))$ . Geometrically, part $(b)$ makes sense because it's a really sharp point (but still continuous), meaning that there should be no tangent plane there. I don't fully understand how I'm supposed to show there's no tangent at $(0, 0, f(0,0))$ though. Isn't this a point in $\mathbb{R}^{3}$ ? And we have a function in $\mathbb{R}^{2}$ ? I'm also not too sure about how to do $(a)$ . I know how to compute the directional derivatives. I don't think that's enough to show existence, though. I'm guessing that it's going to be some sort of limit. I would really appreciate some sort of help with this exercise.","Define (a) Show has directional derivatives in every direction at . (b) Show there is no plane that is tangent to the graph of at the point . Geometrically, part makes sense because it's a really sharp point (but still continuous), meaning that there should be no tangent plane there. I don't fully understand how I'm supposed to show there's no tangent at though. Isn't this a point in ? And we have a function in ? I'm also not too sure about how to do . I know how to compute the directional derivatives. I don't think that's enough to show existence, though. I'm guessing that it's going to be some sort of limit. I would really appreciate some sort of help with this exercise.","f(x, y) = \begin{cases} 
\sin(y^2/x)\sqrt{x^2 + y^2} & \text{ if } x \neq 0 \\
0 & \text{ if } x = 0.
\end{cases} f : \mathbb{R}^{2} \rightarrow \mathbb{R} (0, 0) f : \mathbb{R}^{2} \rightarrow \mathbb{R} (0, 0, f(0,0)) (b) (0, 0, f(0,0)) \mathbb{R}^{3} \mathbb{R}^{2} (a)","['real-analysis', 'multivariable-calculus']"
22,"Computing this limit: $ \lim_{y\to0} \frac{f(x,y) - f(x-y,y)}{y} = g(x)$",Computing this limit:," \lim_{y\to0} \frac{f(x,y) - f(x-y,y)}{y} = g(x)","If $f(x,y) \in \mathbb{R^2}$ and $g(x) \in \mathbb{R}$ . Assuming $\frac{f(x,y) - f(x-y,y)}{y} = g(x); \forall y \in \mathbb{R}$ $$$$ Can we do the following: $$ \lim_{y\to0} \frac{f(x,y) - f(x-y,y)}{y} = g(x)$$ $$ \lim_{y\to0} \frac{\partial{f(x,y)}}{{\partial x}}= g(x) $$",If and . Assuming Can we do the following:,"f(x,y) \in \mathbb{R^2} g(x) \in \mathbb{R} \frac{f(x,y) - f(x-y,y)}{y} = g(x); \forall y \in \mathbb{R}   \lim_{y\to0} \frac{f(x,y) - f(x-y,y)}{y} = g(x)  \lim_{y\to0} \frac{\partial{f(x,y)}}{{\partial x}}= g(x) ","['calculus', 'limits', 'multivariable-calculus', 'partial-derivative']"
23,How to sketch this Domain of triple integral,How to sketch this Domain of triple integral,,"i am having hard time sketching the domain of this : $$ \ \int_0^1\int_0^{1-x^2}\int_0^y f(x,y,z){dz}{dy}{dx} $$ is there an easy way to do that ? i got like cylinder and planes and its hard to see the Volume domain The question asking to sketch this "" simple "" domain","i am having hard time sketching the domain of this : is there an easy way to do that ? i got like cylinder and planes and its hard to see the Volume domain The question asking to sketch this "" simple "" domain","
\ \int_0^1\int_0^{1-x^2}\int_0^y f(x,y,z){dz}{dy}{dx} ","['real-analysis', 'integration', 'multivariable-calculus']"
24,Does a surface with given boundary in $\mathbb{R}^3$ exist?,Does a surface with given boundary in  exist?,\mathbb{R}^3,"Given a (smooth) simple closed curve $C \subset \mathbb{R}^3$ , is there a (smooth) surface $S$ with $\partial S = C$ ? I'm aware there is a variational problem to find among such surfaces the one with minimal area. Here I'm interested in the statement and proof of some existence theorem. Some cases where the existence is clear: if $C$ is planar, and more generally if $C$ is the curve of intersection of a a graph and a cylinder, $x_1=g(x_2,x_3)$ and $f(x_2,x_3)=0$ (This question came up when I read in a calculus book that $\text{curl }\mathbf{F}=\mathbf{0}$ implies $\mathbf{F}=\nabla f$ for some scalar function $f(x,y,z)$ . The proof was: $\mathbf{F}=\nabla f$ iff $\mathbf{F}$ is conservative; to show $\mathbf{F}$ is conservative, consider $\int_C \mathbf{F}\cdot d\mathbf{r}$ for any closed curve $C$ . ""THERE IS"" a surface $S$ with $C$ as its boundary. By stokes theorem, and the fact that $\text{curl }\mathbf{F}=0$ , $\int_C \mathbf{F}\cdot d\mathbf{r} = 0$ . So, my question is why this surface even exists.)","Given a (smooth) simple closed curve , is there a (smooth) surface with ? I'm aware there is a variational problem to find among such surfaces the one with minimal area. Here I'm interested in the statement and proof of some existence theorem. Some cases where the existence is clear: if is planar, and more generally if is the curve of intersection of a a graph and a cylinder, and (This question came up when I read in a calculus book that implies for some scalar function . The proof was: iff is conservative; to show is conservative, consider for any closed curve . ""THERE IS"" a surface with as its boundary. By stokes theorem, and the fact that , . So, my question is why this surface even exists.)","C \subset \mathbb{R}^3 S \partial S = C C C x_1=g(x_2,x_3) f(x_2,x_3)=0 \text{curl }\mathbf{F}=\mathbf{0} \mathbf{F}=\nabla f f(x,y,z) \mathbf{F}=\nabla f \mathbf{F} \mathbf{F} \int_C \mathbf{F}\cdot d\mathbf{r} C S C \text{curl }\mathbf{F}=0 \int_C \mathbf{F}\cdot d\mathbf{r} = 0","['multivariable-calculus', 'differential-geometry', 'calculus-of-variations']"
25,Why doesn't integrating over a sphere with $\phi$ between $0$ and $2\pi$ and $\theta$ between $0$ and $\pi$ work?,Why doesn't integrating over a sphere with  between  and  and  between  and  work?,\phi 0 2\pi \theta 0 \pi,"One way of integrating over a sphere with $p = 1$ is by integrating over $p$ from $0$ to $1$ , $\phi$ from $0$ to $\pi$ , and $\theta$ from $0$ to $2 \pi$ Here is a link that can graph parametric surfaces in spherical coordinates: http://www.math.uri.edu/~bkaskosz/flashmo/sphplot/ If you enter in what is above, then you will find that the graph is indeed, a sphere. Furthermore, if you integrate $p^2\sin (\phi)$ over these ranges, you will get $\frac {4\pi}{3}$ which is the expected value. I noticed that $p$ from $0$ to $1$ , $\phi$ from $0$ to $2\pi$ , and $\theta$ from $0$ to $\pi$ also seems to create a sphere. Note the subtle change: $\phi$ is from $0$ to $2\pi$ and $\theta$ is from $0$ to $1\pi$ . If you plug this in to the grapher, you find that what you get resembles a sphere. However, when you integrate $p^2\sin (\phi)$ over $p$ from $0$ to $1$ , $\phi$ from $0$ to $2\pi$ , and $\theta$ from $0$ to $\pi$ , you get $0$ .   Why is that?","One way of integrating over a sphere with is by integrating over from to , from to , and from to Here is a link that can graph parametric surfaces in spherical coordinates: http://www.math.uri.edu/~bkaskosz/flashmo/sphplot/ If you enter in what is above, then you will find that the graph is indeed, a sphere. Furthermore, if you integrate over these ranges, you will get which is the expected value. I noticed that from to , from to , and from to also seems to create a sphere. Note the subtle change: is from to and is from to . If you plug this in to the grapher, you find that what you get resembles a sphere. However, when you integrate over from to , from to , and from to , you get .   Why is that?",p = 1 p 0 1 \phi 0 \pi \theta 0 2 \pi p^2\sin (\phi) \frac {4\pi}{3} p 0 1 \phi 0 2\pi \theta 0 \pi \phi 0 2\pi \theta 0 1\pi p^2\sin (\phi) p 0 1 \phi 0 2\pi \theta 0 \pi 0,"['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
26,Find the maximum value of $ \int_{0}^{y}\sqrt{x^{4}+(y-y^{2})^{2}}dx$,Find the maximum value of, \int_{0}^{y}\sqrt{x^{4}+(y-y^{2})^{2}}dx,Find the maximum value of $$\displaystyle \int_{0}^{y}\sqrt{x^{4}+(y-y^{2})^{2}}dx$$ for $0 \leq y\leq 1$ . Try: Let $$I(y) = \int^{y}_{0}\sqrt{x^4+(y-y^2)^2}dx$$ Then $$I'(y)=\sqrt{y^{4}+y^{2}(1-y)^{2}}+y(1-y)(1-2y)\int_{0}^{y}\frac{dx}{\sqrt{x^{4}+y^{2}(1-y)^{2}}}$$ Now did not find any clue how I find $$\int_{0}^{y}\frac{dx}{\sqrt{x^{4}+y^{2}(1-y)^{2}}}$$ Could some help me find it? Thanks.,Find the maximum value of for . Try: Let Then Now did not find any clue how I find Could some help me find it? Thanks.,\displaystyle \int_{0}^{y}\sqrt{x^{4}+(y-y^{2})^{2}}dx 0 \leq y\leq 1 I(y) = \int^{y}_{0}\sqrt{x^4+(y-y^2)^2}dx I'(y)=\sqrt{y^{4}+y^{2}(1-y)^{2}}+y(1-y)(1-2y)\int_{0}^{y}\frac{dx}{\sqrt{x^{4}+y^{2}(1-y)^{2}}} \int_{0}^{y}\frac{dx}{\sqrt{x^{4}+y^{2}(1-y)^{2}}},"['integration', 'multivariable-calculus', 'definite-integrals', 'maxima-minima']"
27,How to prove that a vector field with holes in its domain is conservative without finding a potential?,How to prove that a vector field with holes in its domain is conservative without finding a potential?,,"I have to prove that $$\vec{F}=\frac{\vec{r}}{r^2}$$ is a conservative vector field without finding it's potential function. However, $\vec{F}$ has a hole in its domain, at $(x,y,z)=(0,0,0)$ , so I can't just check it's curl. How do I go about proving this?","I have to prove that is a conservative vector field without finding it's potential function. However, has a hole in its domain, at , so I can't just check it's curl. How do I go about proving this?","\vec{F}=\frac{\vec{r}}{r^2} \vec{F} (x,y,z)=(0,0,0)","['multivariable-calculus', 'vectors', 'vector-analysis', 'vector-fields']"
28,Where is misunderstanding dA = dx * dy?,Where is misunderstanding dA = dx * dy?,,"Using infinitesimals from $ A(x, y) = x * y $ I have $ dA = A_x * dx + A_y * dy $ solving it for $ dA $ , I have $ dA = y * dx + x * dy $ which is a mistake. Where in my thinking way is that mistake? I have tried with $ A(x, y) = const $ but it is also wrong.","Using infinitesimals from I have solving it for , I have which is a mistake. Where in my thinking way is that mistake? I have tried with but it is also wrong."," A(x, y) = x * y   dA = A_x * dx + A_y * dy   dA   dA = y * dx + x * dy   A(x, y) = const ","['analysis', 'multivariable-calculus']"
29,Every local minimum of $f(x) = \frac{1}{2}x^tAx + b^tx +c$ is also a global minimum,Every local minimum of  is also a global minimum,f(x) = \frac{1}{2}x^tAx + b^tx +c,"Consider the quadratic function $f(x) = \frac{1}{2}x^tAx + b^tx +c$,   where $A\in\mathbb{R}^{n\times n}$ is symmetric, $b\in\mathbb{R}^n$   and $c\in\mathbb{R}$. Let $x$ be a local minimum of $f$.   Prove that $x$ is a global minimum of $f$. From here https://math.stackexchange.com/a/659982/166180 I know that if the hessian is positive then I know the function is convex. If we had this property, the function would have a unique minimum and therefore it'd be the global minimum because the function is convex. But we don't have any information on the hessian. The only thing I can say is that $$\nabla f(x) = Ax + b\\\nabla^2f(x) = A$$ since $x$ is a local minimum, $Ax+b = 0$ and $A$ is positive semidefinite . If it were positive definite, I think the problem would be solved, but it isn't. So how should I proceed? Also where the symmetry of the matrix $A$ is used in all of this?","Consider the quadratic function $f(x) = \frac{1}{2}x^tAx + b^tx +c$,   where $A\in\mathbb{R}^{n\times n}$ is symmetric, $b\in\mathbb{R}^n$   and $c\in\mathbb{R}$. Let $x$ be a local minimum of $f$.   Prove that $x$ is a global minimum of $f$. From here https://math.stackexchange.com/a/659982/166180 I know that if the hessian is positive then I know the function is convex. If we had this property, the function would have a unique minimum and therefore it'd be the global minimum because the function is convex. But we don't have any information on the hessian. The only thing I can say is that $$\nabla f(x) = Ax + b\\\nabla^2f(x) = A$$ since $x$ is a local minimum, $Ax+b = 0$ and $A$ is positive semidefinite . If it were positive definite, I think the problem would be solved, but it isn't. So how should I proceed? Also where the symmetry of the matrix $A$ is used in all of this?",,"['multivariable-calculus', 'derivatives', 'optimization', 'convex-optimization', 'maxima-minima']"
30,Show that $f$ is continuous on $\mathbb{R}^2$,Show that  is continuous on,f \mathbb{R}^2,"Consider function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ . Suppose $\forall y \in \mathbb{R}$ , the function $f(\cdot,y)$ is continuous and there is exists a positive number $K$ such that $$|f(x,y') - f(x,y'')| \leq K |y' - y''|, \forall x,y',y'' \in \mathbb{R}$$ Show that $f$ is continuous on $\mathbb{R}^2$ . I have no idea to solve this problem, so I need help. Thanks all!","Consider function . Suppose , the function is continuous and there is exists a positive number such that Show that is continuous on . I have no idea to solve this problem, so I need help. Thanks all!","f: \mathbb{R}^2 \rightarrow \mathbb{R} \forall y \in \mathbb{R} f(\cdot,y) K |f(x,y') - f(x,y'')| \leq K |y' - y''|, \forall x,y',y'' \in \mathbb{R} f \mathbb{R}^2","['real-analysis', 'multivariable-calculus', 'continuity']"
31,"From Single-Variable Integration to Multivariable Integration: What Happened to the ""Problem"" of ""Signed/Net Area""?","From Single-Variable Integration to Multivariable Integration: What Happened to the ""Problem"" of ""Signed/Net Area""?",,"I remember that, when learning single-variable integration, we learned that, if a function $f(x)$ is negative, then the definite integral produces the negative of the rectangle's area. This fact made it so that, unless the function $y = f(x)$ is always above the $x$-axis, the value calculated by the definite integral is the signed or net area , rather than the total area . This is because the definite integral will calculate the area between $y = f(x)$ and the $x$-axis, which will be negative for the parts between $y = f(x)$ and the $x$-axis when $y = f(x)$ is below the $x$-axis, and positive for the parts between $y = f(x)$ and the $x$-axis when $y = f(x)$ is above the $x$-axis, causing some cancellation between; thus, we have the signed or net area . Area is always a nonnegative quantity. The Riemann sum approximations contain terms such as $f(c_k) \Delta x_k$ that give the area of a rectangle when $f(c_k)$ is positive. When $f(c_k)$ is negative, then the product $f(c_k) \Delta x_k$ is the negative of the rectangle’s area. When we add up such terms for a negative function, we get the negative of the area between the curve and the x-axis. If we then take the absolute value, we obtain the correct positive area. (Hass 285) Hass, Joel R., Christopher Heil, Maurice Weir. Thomas' Calculus, 14th Edition. Pearson. If we wanted to find the total area , then we would have to break-up the function $y = f(x)$ based on whether it was below or above the $x$-axis, and then do many single-variable integrals for each broken-up region, taking the absolute value of those definite integrals that are below the $x$-axis: To compute the area of the region bounded by the graph of a function $y = f(x)$ and the x-axis when the function takes on both positive and negative values, we must be careful to break up the interval $[a, b]$ into subintervals on which the function doesn’t change sign. Otherwise we might get cancelation between positive and negative signed areas, leading to an incorrect total. The correct total area is obtained by adding the absolute value of the definite integral over each subinterval where $f(x)$ does not change sign. The term “area” will be taken to mean this total area. (Hass 285) Hass, Joel R., Christopher Heil, Maurice Weir. Thomas' Calculus, 14th Edition. Pearson. I then went on to learn multivariable integrals (double, triple) and related concepts, such as different parameterisations/transformations (cylindrical coordinates, spherical coordinates), Green's theorem, Stoke's theorem, the Divergence theorem, etc. In learning these more advanced concepts, this issue with having negative function values was never again mentioned. However, this has been bugging me for a while now, since, as I understand it, this would still be a problem in the multivariable case; but, unlike the single-variable case, there has been no discussion about it or ""how to deal with it"", as there was with signed/net area. I would greatly appreciate it if people could please take the time to explain how the aforementioned ""problem"" of negative function values in the case of single-variable integration comes into play when we're dealing with these more advanced concepts and multivariable integration.","I remember that, when learning single-variable integration, we learned that, if a function $f(x)$ is negative, then the definite integral produces the negative of the rectangle's area. This fact made it so that, unless the function $y = f(x)$ is always above the $x$-axis, the value calculated by the definite integral is the signed or net area , rather than the total area . This is because the definite integral will calculate the area between $y = f(x)$ and the $x$-axis, which will be negative for the parts between $y = f(x)$ and the $x$-axis when $y = f(x)$ is below the $x$-axis, and positive for the parts between $y = f(x)$ and the $x$-axis when $y = f(x)$ is above the $x$-axis, causing some cancellation between; thus, we have the signed or net area . Area is always a nonnegative quantity. The Riemann sum approximations contain terms such as $f(c_k) \Delta x_k$ that give the area of a rectangle when $f(c_k)$ is positive. When $f(c_k)$ is negative, then the product $f(c_k) \Delta x_k$ is the negative of the rectangle’s area. When we add up such terms for a negative function, we get the negative of the area between the curve and the x-axis. If we then take the absolute value, we obtain the correct positive area. (Hass 285) Hass, Joel R., Christopher Heil, Maurice Weir. Thomas' Calculus, 14th Edition. Pearson. If we wanted to find the total area , then we would have to break-up the function $y = f(x)$ based on whether it was below or above the $x$-axis, and then do many single-variable integrals for each broken-up region, taking the absolute value of those definite integrals that are below the $x$-axis: To compute the area of the region bounded by the graph of a function $y = f(x)$ and the x-axis when the function takes on both positive and negative values, we must be careful to break up the interval $[a, b]$ into subintervals on which the function doesn’t change sign. Otherwise we might get cancelation between positive and negative signed areas, leading to an incorrect total. The correct total area is obtained by adding the absolute value of the definite integral over each subinterval where $f(x)$ does not change sign. The term “area” will be taken to mean this total area. (Hass 285) Hass, Joel R., Christopher Heil, Maurice Weir. Thomas' Calculus, 14th Edition. Pearson. I then went on to learn multivariable integrals (double, triple) and related concepts, such as different parameterisations/transformations (cylindrical coordinates, spherical coordinates), Green's theorem, Stoke's theorem, the Divergence theorem, etc. In learning these more advanced concepts, this issue with having negative function values was never again mentioned. However, this has been bugging me for a while now, since, as I understand it, this would still be a problem in the multivariable case; but, unlike the single-variable case, there has been no discussion about it or ""how to deal with it"", as there was with signed/net area. I would greatly appreciate it if people could please take the time to explain how the aforementioned ""problem"" of negative function values in the case of single-variable integration comes into play when we're dealing with these more advanced concepts and multivariable integration.",,"['real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals', 'vector-analysis']"
32,ellipsoid of greatest volume is a sphere,ellipsoid of greatest volume is a sphere,,"The equation of an ellipsoid is $$f(x,y,x)=(\frac xa)^2+(\frac yb)^2+(\frac zc)^2=1$$. Given that the volume of an ellipsoid is $$V=\frac43\pi abc$$ and the constraint $$L=a+b+c$$ L some positive constant. Show that the ellipsoid with greatest volume is a sphere. I should use Lagrange multipliers for this question. I tried doing $$\nabla f = \lambda\nabla V$$ which gave an anwser making no sense$$<\frac{2x}{a^2},\frac{2y}{b^2},\frac{2z}{c^2}> =\lambda <0,0,0>$$ So then I tried $$""\nabla""V=\lambda""\nabla""L$$ where $""\nabla""$ treats a as x, b as y, c as z, and got $$4\pi/3<bc,ac,ab>=\lambda<1,1,1>$$ so $ab=ac=bc$ gives $a=b=c$ a sphere. But why am  I allowed to use $""\nabla""$ as such","The equation of an ellipsoid is $$f(x,y,x)=(\frac xa)^2+(\frac yb)^2+(\frac zc)^2=1$$. Given that the volume of an ellipsoid is $$V=\frac43\pi abc$$ and the constraint $$L=a+b+c$$ L some positive constant. Show that the ellipsoid with greatest volume is a sphere. I should use Lagrange multipliers for this question. I tried doing $$\nabla f = \lambda\nabla V$$ which gave an anwser making no sense$$<\frac{2x}{a^2},\frac{2y}{b^2},\frac{2z}{c^2}> =\lambda <0,0,0>$$ So then I tried $$""\nabla""V=\lambda""\nabla""L$$ where $""\nabla""$ treats a as x, b as y, c as z, and got $$4\pi/3<bc,ac,ab>=\lambda<1,1,1>$$ so $ab=ac=bc$ gives $a=b=c$ a sphere. But why am  I allowed to use $""\nabla""$ as such",,['calculus']
33,Computing double integral using linear algebra,Computing double integral using linear algebra,,"$$\iint_D (6x+2y) \, \mathrm d x \,\mathrm d y$$ where $D$ is the convex hull of $4$ given points, $$D = \mbox{conv} \left\{ (0,0),(-2,6),(3,2),(1,8) \right\}$$ This is a parallelogram with ""unit vectors"" $(-2,6)$ , $(3,2)$ . I wanted to give a try to solve the following problem with algebra instead of calculus. So, I thought about calculating the area of it and piling it up to get its volume. I got the cosine between both vectors with the dot product formula, which is $$\dfrac{6}{\sqrt{13\cdot40}}$$ and the sine with the Pythagoras identity which is $$\sqrt{\dfrac{484}{13\cdot40}}$$ I remember that $|a|\cdot |b|\cdot \sin\alpha$ gives the height, and $|a|\cdot |b|\cdot \cos\alpha$ gives the area. So I figured that maybe this would solve the integral problem for the area? $$\dfrac{6}{\sqrt{13\cdot40}} \cdot \sqrt{13} \cdot \sqrt{40}\sqrt{\dfrac{484}{{13\cdot40}}}\cdot \sqrt{13} \cdot \sqrt{40}$$ That gives me $$6 \cdot 22 = 132$$ That's wrong but the right result is $$11 \cdot 22 = 242$$ so there might be something in that?","where is the convex hull of given points, This is a parallelogram with ""unit vectors"" , . I wanted to give a try to solve the following problem with algebra instead of calculus. So, I thought about calculating the area of it and piling it up to get its volume. I got the cosine between both vectors with the dot product formula, which is and the sine with the Pythagoras identity which is I remember that gives the height, and gives the area. So I figured that maybe this would solve the integral problem for the area? That gives me That's wrong but the right result is so there might be something in that?","\iint_D (6x+2y) \, \mathrm d x \,\mathrm d y D 4 D = \mbox{conv} \left\{ (0,0),(-2,6),(3,2),(1,8) \right\} (-2,6) (3,2) \dfrac{6}{\sqrt{13\cdot40}} \sqrt{\dfrac{484}{13\cdot40}} |a|\cdot |b|\cdot \sin\alpha |a|\cdot |b|\cdot \cos\alpha \dfrac{6}{\sqrt{13\cdot40}} \cdot \sqrt{13} \cdot \sqrt{40}\sqrt{\dfrac{484}{{13\cdot40}}}\cdot \sqrt{13} \cdot \sqrt{40} 6 \cdot 22 = 132 11 \cdot 22 = 242","['linear-algebra', 'integration', 'multivariable-calculus', 'definite-integrals']"
34,How do I evaluate these derivatives? (Involving integrals),How do I evaluate these derivatives? (Involving integrals),,"I'm studying Green's functions, and I came acorss the following problem: To calculate $\frac{du}{dx}$, where: $$u(x) = \int_{a}^{x} G_1(x,y)f(y)dy + \int_{x}^{b} G_2(x,y)f(y)dy$$ Where $a,b \in \mathbb{R}$, $G_1,G_2,f$ are functions. Let's first take a look at the first term: $$u_1(x) = \int_{a}^{x} G_1(x,y)f(y)dy$$ Now, my textbook (Eugene Butkov's Mathematical Physics) says that: $$u_1'(x) = \int_{a}^{x} \frac{\partial G_1}{\partial x} f(y)dy \quad \!\!\!\!\!+ G_1(x,x)f(x)$$ How come? I've tried to derive this formula but I couldn't go anywhere. I was able to verify it, however, in the case that $G_1(x,y)$ is a separable function $G_1(x,y)=a(x)b(y)$. Then $$u(x) = a(x)\int_{a}^{x} b(y)f(y)dy$$ Hence, by the product rule $$\frac{du(x)}{dx} = a'(x) \int_{a}^{x} b(y)f(y)dy \quad \!\!\!\!+ a(x)b(x)f(x)$$ Which is the verified result. However, I would like to see a proof of the formula for a more general case.","I'm studying Green's functions, and I came acorss the following problem: To calculate $\frac{du}{dx}$, where: $$u(x) = \int_{a}^{x} G_1(x,y)f(y)dy + \int_{x}^{b} G_2(x,y)f(y)dy$$ Where $a,b \in \mathbb{R}$, $G_1,G_2,f$ are functions. Let's first take a look at the first term: $$u_1(x) = \int_{a}^{x} G_1(x,y)f(y)dy$$ Now, my textbook (Eugene Butkov's Mathematical Physics) says that: $$u_1'(x) = \int_{a}^{x} \frac{\partial G_1}{\partial x} f(y)dy \quad \!\!\!\!\!+ G_1(x,x)f(x)$$ How come? I've tried to derive this formula but I couldn't go anywhere. I was able to verify it, however, in the case that $G_1(x,y)$ is a separable function $G_1(x,y)=a(x)b(y)$. Then $$u(x) = a(x)\int_{a}^{x} b(y)f(y)dy$$ Hence, by the product rule $$\frac{du(x)}{dx} = a'(x) \int_{a}^{x} b(y)f(y)dy \quad \!\!\!\!+ a(x)b(x)f(x)$$ Which is the verified result. However, I would like to see a proof of the formula for a more general case.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'greens-function']"
35,What tools one should use for inequalities?,What tools one should use for inequalities?,,"If $a,b,c>0$ prove that: $$\frac{1}{a+4b+4c}+\frac{1}{4a+b+4c}+\frac{1}{4a+4b+c}\leq \frac{1}{3\sqrt[3]{abc}}.$$ My first try was the following: $$\sum_{cyc}\frac{1}{a+4b+4c}\leq\sum_{cyc}\frac{1}{\sqrt[3]{16abc}}=\frac{1}{\sqrt[3]{16abc}}$$ But $\frac{1}{\sqrt[3]{16abc}}\geq \frac{1}{3\sqrt[3]{abc}}$ The I have tried your method from another post: $$\sum_{cyc}\frac{1}{a+4b+4c}=\sum_{cyc}\frac{1}{a+2b+2(b+2c)}$$ $$\sum_{cyc}\frac{1}{a+2b+2(b+2c)}\leq\sum_{cyc}\frac{1}{9}\left (\frac{1^2}{a+2b}+\frac{2^2}{b+2c}  \right )$$ Where I got $$\sum_{cyc}\frac{1}{3}\left ( \frac{1}{a+2b} \right )\leq \frac{1}{3\sqrt[3]{abc}}$$ wich is false. $$$$","If $a,b,c>0$ prove that: $$\frac{1}{a+4b+4c}+\frac{1}{4a+b+4c}+\frac{1}{4a+4b+c}\leq \frac{1}{3\sqrt[3]{abc}}.$$ My first try was the following: $$\sum_{cyc}\frac{1}{a+4b+4c}\leq\sum_{cyc}\frac{1}{\sqrt[3]{16abc}}=\frac{1}{\sqrt[3]{16abc}}$$ But $\frac{1}{\sqrt[3]{16abc}}\geq \frac{1}{3\sqrt[3]{abc}}$ The I have tried your method from another post: $$\sum_{cyc}\frac{1}{a+4b+4c}=\sum_{cyc}\frac{1}{a+2b+2(b+2c)}$$ $$\sum_{cyc}\frac{1}{a+2b+2(b+2c)}\leq\sum_{cyc}\frac{1}{9}\left (\frac{1^2}{a+2b}+\frac{2^2}{b+2c}  \right )$$ Where I got $$\sum_{cyc}\frac{1}{3}\left ( \frac{1}{a+2b} \right )\leq \frac{1}{3\sqrt[3]{abc}}$$ wich is false. $$$$",,"['multivariable-calculus', 'inequality', 'substitution', 'a.m.-g.m.-inequality', 'uvw']"
36,"Evaluating $\lim_{(x,y) \to (\pi,0)} {\cos(x) + 1 + {y^2/2} \over (x - \pi)^2 + y^2}$",Evaluating,"\lim_{(x,y) \to (\pi,0)} {\cos(x) + 1 + {y^2/2} \over (x - \pi)^2 + y^2}","I'm struggling to evaluate this limit problem: $$\lim\limits_{(x,y) \to (\pi,0)} {\cos x + 1 + {y^2/2} \over (x - \pi)^2 + y^2}$$ I've tried this with paths $x=\pi$, $y=0$, $y=x-\pi$, ... and so far all of them have resulted in $1 \over 2$. However when I try to evaluate this in WolframAlpha, it says that the limit does not exist. I haven't found any paths that result in a value other than $1/2$. How can I prove that this limit does not exist?","I'm struggling to evaluate this limit problem: $$\lim\limits_{(x,y) \to (\pi,0)} {\cos x + 1 + {y^2/2} \over (x - \pi)^2 + y^2}$$ I've tried this with paths $x=\pi$, $y=0$, $y=x-\pi$, ... and so far all of them have resulted in $1 \over 2$. However when I try to evaluate this in WolframAlpha, it says that the limit does not exist. I haven't found any paths that result in a value other than $1/2$. How can I prove that this limit does not exist?",,"['limits', 'multivariable-calculus']"
37,"Calculating double limit $\lim_{(x,y) \rightarrow (0,0)} (1+xy)^{\frac{1}{x}}$",Calculating double limit,"\lim_{(x,y) \rightarrow (0,0)} (1+xy)^{\frac{1}{x}}","Tried to evaluate $\lim_{(x,y) \rightarrow (0,0)} (1+xy)^{\frac{1}{x}}$. I know it suppose to be 1. Used a few different methods (sandwich, playing with the terms, using the formal definition and so on), without success. Would appreciate any help, an hint would be better than full solution. (Note: part of a basic calculus course) Thanks","Tried to evaluate $\lim_{(x,y) \rightarrow (0,0)} (1+xy)^{\frac{1}{x}}$. I know it suppose to be 1. Used a few different methods (sandwich, playing with the terms, using the formal definition and so on), without success. Would appreciate any help, an hint would be better than full solution. (Note: part of a basic calculus course) Thanks",,"['calculus', 'limits', 'multivariable-calculus']"
38,"If $f:\mathbb{R}^2\to\mathbb{R}^1$ is of class $C^1$, show that $f$ is not one-to-one.","If  is of class , show that  is not one-to-one.",f:\mathbb{R}^2\to\mathbb{R}^1 C^1 f,"If $f:\mathbb{R}^2\to\mathbb{R}^1$ is of class $C^1$, show that $f$ is not one-to-one. [Hint: If $Df(x) = 0$ for all $x$, then $f$ is constant. If $Df(x_0)\neq0$, apply the implicit function theorem.] Clearly there are two cases, if $Df(x)=0$ for all $x\in \mathbb{R}^2$ then $f$ is constant and therefore can not be one to one. If there is a $x_0\in\mathbb{R}^2$ such that $Df(x_0)\neq 0$ then how can I use the implicit function theorem knowing that in order to apply it I have to ensure that $f(x_0)=0$ but this is not given in the problem? Here is the version of the theorem of the implicit function that I am using, thank you very much.","If $f:\mathbb{R}^2\to\mathbb{R}^1$ is of class $C^1$, show that $f$ is not one-to-one. [Hint: If $Df(x) = 0$ for all $x$, then $f$ is constant. If $Df(x_0)\neq0$, apply the implicit function theorem.] Clearly there are two cases, if $Df(x)=0$ for all $x\in \mathbb{R}^2$ then $f$ is constant and therefore can not be one to one. If there is a $x_0\in\mathbb{R}^2$ such that $Df(x_0)\neq 0$ then how can I use the implicit function theorem knowing that in order to apply it I have to ensure that $f(x_0)=0$ but this is not given in the problem? Here is the version of the theorem of the implicit function that I am using, thank you very much.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'vector-analysis']"
39,Prove that $f$ is linear,Prove that  is linear,f,"Let $f: \mathbb{R}^{m} \longrightarrow \mathbb{R}$ differentiable such that $\displaystyle f\left(\frac{x}{2}\right) = \frac{f(x)}{2}$ for all $x \in \mathbb{R}^{m}$. Prove that $f$ is linear. I'm trying to solve without using grad, because in my book, grad is only defined after. But I did not get a good idea, any hints?","Let $f: \mathbb{R}^{m} \longrightarrow \mathbb{R}$ differentiable such that $\displaystyle f\left(\frac{x}{2}\right) = \frac{f(x)}{2}$ for all $x \in \mathbb{R}^{m}$. Prove that $f$ is linear. I'm trying to solve without using grad, because in my book, grad is only defined after. But I did not get a good idea, any hints?",,"['real-analysis', 'multivariable-calculus', 'frechet-derivative']"
40,Second variation corresponding to the functional,Second variation corresponding to the functional,,"I am facing difficulty to calculate the second variation to the following functional. Define $J: W_{0}^{1,p}(\Omega)\to\mathbb{R}$ by $J(u)=\frac{1}{p}\int_{\Omega}|\nabla u|^p\,dx$ where $p>1$. I am able to calculate the first variation as follows:  $J'(u)\phi=\int_{\Omega}\,|\nabla u|^{p-2}\nabla u\cdot\nabla\phi\,dx$ which I have got by using the functional $E:\mathbb{R}\to\mathbb{R}$ defined by $E(t)=J(u+t\phi)$. But I am unable to calculate the second variation. Any type of help is very much appreciated. Thanks.","I am facing difficulty to calculate the second variation to the following functional. Define $J: W_{0}^{1,p}(\Omega)\to\mathbb{R}$ by $J(u)=\frac{1}{p}\int_{\Omega}|\nabla u|^p\,dx$ where $p>1$. I am able to calculate the first variation as follows:  $J'(u)\phi=\int_{\Omega}\,|\nabla u|^{p-2}\nabla u\cdot\nabla\phi\,dx$ which I have got by using the functional $E:\mathbb{R}\to\mathbb{R}$ defined by $E(t)=J(u+t\phi)$. But I am unable to calculate the second variation. Any type of help is very much appreciated. Thanks.",,"['multivariable-calculus', 'sobolev-spaces', 'harmonic-functions', 'regularity-theory-of-pdes', 'fractional-sobolev-spaces']"
41,"Let $f : Q\to \mathbb{R}$ be bounded. Then $f$ is integrable over $Q$ if and only if given $\epsilon> 0$, there is a $\delta> 0$ such that","Let  be bounded. Then  is integrable over  if and only if given , there is a  such that",f : Q\to \mathbb{R} f Q \epsilon> 0 \delta> 0,"Prove the following: Theorem. Let $f : Q\to \mathbb{R}$ be bounded. Then $f$ is integrable over $Q$ if and only if given $\epsilon> 0$, there is a $\delta> 0$ such that $U(f, P) -L(f, P) <\epsilon$ for every partition $P$ of mesh less than $\delta$. Proof. (a) Verify the ""if"" part of the theorem. (b) Suppose $|f(x)| < M$ for $x\in Q$. Let $P$ be a partition of $Q$. Show that if $P""$ is obtained by adjoining a single point to the partition of one of the component intervals of $Q$, then $$0\leq L(f, P"") - L(f, P)\leq 2M(\text{mesh} P) (\text{width} Q)^{n-1}$$ Derive a similar result for upper sums. (c) Prove the ""only if"" part of tbe theorem: Suppose $f$ is integrable over $Q$. Given $\epsilon> 0$, choose a partition $P'$ such that $U(f, P')- ­ L(f, P') < \epsilon/2$. Let $N$ be the number of partition points in $P'$; then let $$\delta= \epsilon/8M N (\text{width} Q)^{n-1}$$ Show that if $P$ has mesh less than $\delta$, then $U(f, P) - L(f, P) <\epsilon$ . [Hint: The common refinement of $P$ and $P'$ is obtained by adjoining at most $N$ points to $P$.] (a) if $\epsilon>0$ then there is a $\delta>0$ such that $U(f,P)-L(f,P)<\epsilon$ for any partition of norm smaller than $\delta$, if $P$ a partition of norm smaller than $\delta$, then $U(f,P)-L(f,P)<\epsilon$ and thus $f$ is integrable. (b) I know that if $P''$ is a partition finer than $P$ then we have to $L(f,P)\leq L(f,P'')$, with which $0\leq L(f,P'')-L(f,P)$. I am entangled in testing the other inequality, I know that $L(f,P'')-L(f,P)=\sum_{R\subset P''}m_R(f)v(R)-\sum_{R\subset P}m_R(f)v(R)$, but I do not know how to limit that, could someone help me please? (c) I do not know how to prove this, could someone give me a help or do a test? Thank you very much.","Prove the following: Theorem. Let $f : Q\to \mathbb{R}$ be bounded. Then $f$ is integrable over $Q$ if and only if given $\epsilon> 0$, there is a $\delta> 0$ such that $U(f, P) -L(f, P) <\epsilon$ for every partition $P$ of mesh less than $\delta$. Proof. (a) Verify the ""if"" part of the theorem. (b) Suppose $|f(x)| < M$ for $x\in Q$. Let $P$ be a partition of $Q$. Show that if $P""$ is obtained by adjoining a single point to the partition of one of the component intervals of $Q$, then $$0\leq L(f, P"") - L(f, P)\leq 2M(\text{mesh} P) (\text{width} Q)^{n-1}$$ Derive a similar result for upper sums. (c) Prove the ""only if"" part of tbe theorem: Suppose $f$ is integrable over $Q$. Given $\epsilon> 0$, choose a partition $P'$ such that $U(f, P')- ­ L(f, P') < \epsilon/2$. Let $N$ be the number of partition points in $P'$; then let $$\delta= \epsilon/8M N (\text{width} Q)^{n-1}$$ Show that if $P$ has mesh less than $\delta$, then $U(f, P) - L(f, P) <\epsilon$ . [Hint: The common refinement of $P$ and $P'$ is obtained by adjoining at most $N$ points to $P$.] (a) if $\epsilon>0$ then there is a $\delta>0$ such that $U(f,P)-L(f,P)<\epsilon$ for any partition of norm smaller than $\delta$, if $P$ a partition of norm smaller than $\delta$, then $U(f,P)-L(f,P)<\epsilon$ and thus $f$ is integrable. (b) I know that if $P''$ is a partition finer than $P$ then we have to $L(f,P)\leq L(f,P'')$, with which $0\leq L(f,P'')-L(f,P)$. I am entangled in testing the other inequality, I know that $L(f,P'')-L(f,P)=\sum_{R\subset P''}m_R(f)v(R)-\sum_{R\subset P}m_R(f)v(R)$, but I do not know how to limit that, could someone help me please? (c) I do not know how to prove this, could someone give me a help or do a test? Thank you very much.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'vector-analysis']"
42,"What is the value of $\lim_{(x,y,z)\rightarrow(0,0,0)}\log(\frac{x}{yz})$?",What is the value of ?,"\lim_{(x,y,z)\rightarrow(0,0,0)}\log(\frac{x}{yz})","What is the value of   $\lim_{(x,y,z)\rightarrow(0,0,0)}\log(\frac{x}{yz})$? (a)$0$ (b)$2$ (c)$4$ (d)does not exist Let us take $y=mx$ and $z=mx$ where $m$ is an arbitrary parameter,then $\lim_{(x,y,z)\rightarrow(0,0,0)}\log(\frac{x}{yz})=\lim_{m\rightarrow 0}\log(\frac{1}{m^2})=$does not exist Is it correct??","What is the value of   $\lim_{(x,y,z)\rightarrow(0,0,0)}\log(\frac{x}{yz})$? (a)$0$ (b)$2$ (c)$4$ (d)does not exist Let us take $y=mx$ and $z=mx$ where $m$ is an arbitrary parameter,then $\lim_{(x,y,z)\rightarrow(0,0,0)}\log(\frac{x}{yz})=\lim_{m\rightarrow 0}\log(\frac{1}{m^2})=$does not exist Is it correct??",,"['real-analysis', 'limits', 'multivariable-calculus']"
43,Sketching open balls,Sketching open balls,,"When working with metric spaces we usually have to sketch absolute value inequalities. I can determine the open balls and everything but the sketching part is difficult, for example for a metric defined as $$d((x_1,y_1),(x_2,y_2))=|x_1+ y_1- x_2- y_2|+|-x_1+ y_1+ x_2- y_2|$$ if I want the ball $$B_2(0,0) = \{(x,y) \in \mathbb{R}^2: d((x,y),(0,0))<2\}=\{(x,y) \in \mathbb{R}^2: |x+y|+|-x+y| <2\}$$ I have no idea how to draw these things; although I know what an absolute value function looks like. Any help is appreciated.","When working with metric spaces we usually have to sketch absolute value inequalities. I can determine the open balls and everything but the sketching part is difficult, for example for a metric defined as $$d((x_1,y_1),(x_2,y_2))=|x_1+ y_1- x_2- y_2|+|-x_1+ y_1+ x_2- y_2|$$ if I want the ball $$B_2(0,0) = \{(x,y) \in \mathbb{R}^2: d((x,y),(0,0))<2\}=\{(x,y) \in \mathbb{R}^2: |x+y|+|-x+y| <2\}$$ I have no idea how to draw these things; although I know what an absolute value function looks like. Any help is appreciated.",,"['calculus', 'multivariable-calculus', 'metric-spaces', 'graphing-functions', 'absolute-value']"
44,"Why is $\int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{u^2+v^2}=2\pi$? Why not $0$?",Why is ? Why not ?,"\int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{u^2+v^2}=2\pi 0","Why is $$\int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{u^2+v^2}=2\pi$$ $$\mathbb{S}^1 = \{(u,v)\in\mathbb{R}^2 \quad st \quad u^2+v^2 =1 \}$$ I am a bit confused about this $$ \int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{u^2+v^2}=  \int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{1}=  \int_{\mathbb{S}^1} -v\,du + \int_{\mathbb{S}^1} u\,dv= -vu+uv\bigg|_{\mathbb{S}^1}= 0 $$ What am I missing? Edit: Going my the suggestion you get $$ \int_{\mathbb{S}^1} \sin^2 \theta + \cos^2 \theta \, d\theta = \int_{\mathbb{S}^1} 1 \, d\theta = \theta_{\mathbb{S}^1} = 2\pi $$","Why is $$\int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{u^2+v^2}=2\pi$$ $$\mathbb{S}^1 = \{(u,v)\in\mathbb{R}^2 \quad st \quad u^2+v^2 =1 \}$$ I am a bit confused about this $$ \int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{u^2+v^2}=  \int_{\mathbb{S}^1} \frac{-v\,du+u\,dv}{1}=  \int_{\mathbb{S}^1} -v\,du + \int_{\mathbb{S}^1} u\,dv= -vu+uv\bigg|_{\mathbb{S}^1}= 0 $$ What am I missing? Edit: Going my the suggestion you get $$ \int_{\mathbb{S}^1} \sin^2 \theta + \cos^2 \theta \, d\theta = \int_{\mathbb{S}^1} 1 \, d\theta = \theta_{\mathbb{S}^1} = 2\pi $$",,"['integration', 'multivariable-calculus']"
45,Evaluating the improper double integral $\int_{D} \frac{dxdy}{\sqrt{1-a\cdot x-b\cdot y}}$,Evaluating the improper double integral,\int_{D} \frac{dxdy}{\sqrt{1-a\cdot x-b\cdot y}},"I'm given the following improper integral: $\int_{D} \frac{dxdy}{\sqrt{1-a\cdot x-b\cdot y}}$ where $D$ is the open unit disk, assuming $a^2 + b^2 = 1$. I should decide whether it converges or not, and if it does, I should find the limit. I've found that the integrand is well defined on $D$ (using Cauchy-Schwarz lemma) and by using this lemma I was able to upper-bound the integral by another and show that it converges. However, I cannot evaluate this integral, i.e. find its exact value. I'm quite sure I should integrate by substitution, but it is just not working... How can I do it? Thanks in advance!","I'm given the following improper integral: $\int_{D} \frac{dxdy}{\sqrt{1-a\cdot x-b\cdot y}}$ where $D$ is the open unit disk, assuming $a^2 + b^2 = 1$. I should decide whether it converges or not, and if it does, I should find the limit. I've found that the integrand is well defined on $D$ (using Cauchy-Schwarz lemma) and by using this lemma I was able to upper-bound the integral by another and show that it converges. However, I cannot evaluate this integral, i.e. find its exact value. I'm quite sure I should integrate by substitution, but it is just not working... How can I do it? Thanks in advance!",,"['integration', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
46,How to determine the integration boundaries of the following double integral?,How to determine the integration boundaries of the following double integral?,,"Calculate the following double integral: $\int\limits$$\int\limits_T$  $[xsen(x) + ysen(x+y)]$ $dxdy$ Where the region $T$ is the triangle of vertices $(1,0)$, $(0,1)$ y $(3,3)$. To could determine the boundaries of the integral  I did the following: I made a graph of the triangle I found the equations that describe the three lines of the triangle: The one which goes from $(0,1)$ to  $(3,3)$ is $y=2/3x+1$ The one which goes from $(0,1)$ to  $(1,0)$ is $y=-x+1$ The one which goes from $(1,0)$ to  $(3,3)$ is $y=3/2x-3/2$ Now, I'm stuck from here. I don't know how to establish the boundaries for $x$ and $y$ given the restriction named $T$. Any hint?","Calculate the following double integral: $\int\limits$$\int\limits_T$  $[xsen(x) + ysen(x+y)]$ $dxdy$ Where the region $T$ is the triangle of vertices $(1,0)$, $(0,1)$ y $(3,3)$. To could determine the boundaries of the integral  I did the following: I made a graph of the triangle I found the equations that describe the three lines of the triangle: The one which goes from $(0,1)$ to  $(3,3)$ is $y=2/3x+1$ The one which goes from $(0,1)$ to  $(1,0)$ is $y=-x+1$ The one which goes from $(1,0)$ to  $(3,3)$ is $y=3/2x-3/2$ Now, I'm stuck from here. I don't know how to establish the boundaries for $x$ and $y$ given the restriction named $T$. Any hint?",,"['calculus', 'integration', 'algebra-precalculus', 'multivariable-calculus', 'definite-integrals']"
47,"Advice on when $f(x,y)=\frac{x^4 + y^4}{(x^2+y^2)^k}$ is continuous/differentiable?",Advice on when  is continuous/differentiable?,"f(x,y)=\frac{x^4 + y^4}{(x^2+y^2)^k}","I have the function $f(x,y) = \begin{cases} \frac{x^4 + y^4}{(x^2+y^2)^k},  & x \ne0 \\ 0, & x=0 \end{cases}$ I'm trying to find the the real $k$ such that this function is continuous and differentiable at $(0,0)$. I note that $f(\lambda x,\lambda y) = \lambda ^{4-2k}f(x,y)$  implying that for $k>2$ the function blows up when it approaches $(0,0)$. But I can't seem to properly bound the function at that point when $k\leqslant2$ to show continuity or differentiability. Would anyone be able to help?","I have the function $f(x,y) = \begin{cases} \frac{x^4 + y^4}{(x^2+y^2)^k},  & x \ne0 \\ 0, & x=0 \end{cases}$ I'm trying to find the the real $k$ such that this function is continuous and differentiable at $(0,0)$. I note that $f(\lambda x,\lambda y) = \lambda ^{4-2k}f(x,y)$  implying that for $k>2$ the function blows up when it approaches $(0,0)$. But I can't seem to properly bound the function at that point when $k\leqslant2$ to show continuity or differentiability. Would anyone be able to help?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'continuity']"
48,Why is this function not integrable?,Why is this function not integrable?,,"This is question 19 in Chapter 3 of Spivak's Calculus on Manifolds . Suppose $I_n=(a_n,b_n)$ is a sequence of open subintervals of $(0,1)$ such that every rational number in $(0,1)$ is contained in one of them, and such that $\sum (b_n-a_n)<1$. Let $A=\bigcup I_n$; it's not hard to show the boundary of $A$ is $[0,1]\setminus A$ and that it's not of measure zero. Let $\chi_A$ be the characteristic function of $A$ and let $f$ be a function which equals to $\chi_A$ except on a set of measure zero. Show $f$ is not Riemann integrable. I found solutions here and here , but they both seem wrong to me. The latter seems to use the equation $\partial U\setminus V=\partial(U\setminus V)$ which is obviously wrong in general (and I don't see why it should apply here...), and the former seems to use the assertion that if $f_1$ differs from the function $f_2$ on a set $B$ and is discontinuous at $x_0$, then either $f_2$ is discontinuous at $x_0$ or $x_0\in B$ (which is again not true in general, and I don't see why it should apply here). So how do you solve this question?","This is question 19 in Chapter 3 of Spivak's Calculus on Manifolds . Suppose $I_n=(a_n,b_n)$ is a sequence of open subintervals of $(0,1)$ such that every rational number in $(0,1)$ is contained in one of them, and such that $\sum (b_n-a_n)<1$. Let $A=\bigcup I_n$; it's not hard to show the boundary of $A$ is $[0,1]\setminus A$ and that it's not of measure zero. Let $\chi_A$ be the characteristic function of $A$ and let $f$ be a function which equals to $\chi_A$ except on a set of measure zero. Show $f$ is not Riemann integrable. I found solutions here and here , but they both seem wrong to me. The latter seems to use the equation $\partial U\setminus V=\partial(U\setminus V)$ which is obviously wrong in general (and I don't see why it should apply here...), and the former seems to use the assertion that if $f_1$ differs from the function $f_2$ on a set $B$ and is discontinuous at $x_0$, then either $f_2$ is discontinuous at $x_0$ or $x_0\in B$ (which is again not true in general, and I don't see why it should apply here). So how do you solve this question?",,"['integration', 'multivariable-calculus', 'riemann-integration']"
49,Finding the center of mass with varying density,Finding the center of mass with varying density,,"Given a triangle $\mathrm{A}\left(2,0\right),\ \mathrm{B}\left(1,3\right),\ \mathrm{C}\left(5,2\right)\ \mbox{with}\ \rho\left(x,y\right) = x$; I need to find it's centre of mass ?. I know I need to integrate the density formula over the region, but I don't understand how to get the limits for the integrals to calculate the area. Do I need to find formulas for the lines and somehow use those ?.","Given a triangle $\mathrm{A}\left(2,0\right),\ \mathrm{B}\left(1,3\right),\ \mathrm{C}\left(5,2\right)\ \mbox{with}\ \rho\left(x,y\right) = x$; I need to find it's centre of mass ?. I know I need to integrate the density formula over the region, but I don't understand how to get the limits for the integrals to calculate the area. Do I need to find formulas for the lines and somehow use those ?.",,"['calculus', 'multivariable-calculus']"
50,"Fundamental theorem of line integrals, what if the curl DOESN'T equal 0?","Fundamental theorem of line integrals, what if the curl DOESN'T equal 0?",,"I am getting really frustrated here and have had many arguments with people. Is it true or not, that if you compute the curl, and you get something other than 0, THEN you can NOT use the fundamental theorem? My understanding is that, logically, conservative F implies curl=0, therefore, curl=/=0 implies F not conservative, (if not q then not p) therefore you can not use the theorem. Thank you to anyone who clarifies this.","I am getting really frustrated here and have had many arguments with people. Is it true or not, that if you compute the curl, and you get something other than 0, THEN you can NOT use the fundamental theorem? My understanding is that, logically, conservative F implies curl=0, therefore, curl=/=0 implies F not conservative, (if not q then not p) therefore you can not use the theorem. Thank you to anyone who clarifies this.",,[]
51,Tangent Space and Vector field in Euclidean Space in view of Smooth Manifold,Tangent Space and Vector field in Euclidean Space in view of Smooth Manifold,,"I have just learned tangent space in smooth Manifold.It is defined in this manner....Let $M$ be a smooth Manifold,let $\Gamma$$(p)$ be the collection of all smooth curve on $M$ to the point $p$$\in$$M$ such that $d$($\gamma$$(t)$)$/dt$ $=$$d$($\tau$(t))/$dt$, at $t=0$.$\gamma$,$\tau$$\in$$\Gamma$$(p)$.Under these  condition we define a relation $\sim$.It is easy to verify that $\sim$ is an equivalence relation and each of the  equivalence classes are called tangent vector at $p$.But If I focus in particularly Euclidean Space how can I relate it with the Tangent space in Euclidean space,By Tangent Space on Euclidean Space I know the vector space that is generated by the partial derivaties w.r.t $x_1$,$x_2$,..,$x_n$ where $x_1,x_2,...,x_n$ are coordinates in $R^n$.How can I compare these two tangent space in similar way?Does the tangent space at $p$$\in$ $M$ represents a plane of  dimension $n$ in $n$ dimensional Manifold $M$? I have similar problem in understanding vector field.By Vector Field we mean a section of vector Bundle on a smooth Manifold.But in Euclidean Space by Vector field we mean a mapping $X(p)->(p,x(p))$$p$$\in$$R^n$,,I think there need to be a similarity in the definition of Vector field Euclidean Space and manifold as smooth Manifolds are the general form of Euclidean Spaces,But I can't view that similarity! How does these two definition in Manifold are generalized from their definition in Euclidean space?","I have just learned tangent space in smooth Manifold.It is defined in this manner....Let $M$ be a smooth Manifold,let $\Gamma$$(p)$ be the collection of all smooth curve on $M$ to the point $p$$\in$$M$ such that $d$($\gamma$$(t)$)$/dt$ $=$$d$($\tau$(t))/$dt$, at $t=0$.$\gamma$,$\tau$$\in$$\Gamma$$(p)$.Under these  condition we define a relation $\sim$.It is easy to verify that $\sim$ is an equivalence relation and each of the  equivalence classes are called tangent vector at $p$.But If I focus in particularly Euclidean Space how can I relate it with the Tangent space in Euclidean space,By Tangent Space on Euclidean Space I know the vector space that is generated by the partial derivaties w.r.t $x_1$,$x_2$,..,$x_n$ where $x_1,x_2,...,x_n$ are coordinates in $R^n$.How can I compare these two tangent space in similar way?Does the tangent space at $p$$\in$ $M$ represents a plane of  dimension $n$ in $n$ dimensional Manifold $M$? I have similar problem in understanding vector field.By Vector Field we mean a section of vector Bundle on a smooth Manifold.But in Euclidean Space by Vector field we mean a mapping $X(p)->(p,x(p))$$p$$\in$$R^n$,,I think there need to be a similarity in the definition of Vector field Euclidean Space and manifold as smooth Manifolds are the general form of Euclidean Spaces,But I can't view that similarity! How does these two definition in Manifold are generalized from their definition in Euclidean space?",,"['multivariable-calculus', 'differential-geometry']"
52,Can I calculate limits in 3 dimensions with a substitution of the type $z=xy$?,Can I calculate limits in 3 dimensions with a substitution of the type ?,z=xy,"This is my first question here so I hope I'm doing it right :) sorry otherwise! As in the title, I was wondering if and when it is OK to calculate a limit i three dimensions through a substitution that ""brings it down to two dimensions"". Let me explain what I mean in a clearer way through an example. I was calculating this limit: $$\lim_{(x,y) \to (0,0)} \frac{\ln (1+\sin^2(xy))}{x^2+y^2} =\lim_{(x,y) \to (0,0)} \frac{\ln (1+\sin(xy)\cdot \sin(xy))}{x^2+y^2}$$ $$=\lim_{(x,y) \to (0,0)} \frac{\ln (1+xy\cdot xy)}{x^2+y^2} =\lim_{(x,y) \to (0,0)} \frac{\ln (1+x^2y^2)}{x^2+y^2}=\lim_{(x,y) \to (0,0)} \frac{x^2y^2}{x^2+y^2}$$ $$=\lim_{(x,y) \to (0,0)}\frac{1}{\frac{1}{y^2}+\frac{1}{x^2}}=""\frac{1}{\infty}""=0.$$ Where I have used: $$ \lim_{(x,y) \to (0,0)} \frac{\sin(xy)}{xy}=[z=xy]=\lim_{z\to 0}\frac{\sin z}{z}=1$$ and  $$ \lim_{(x,y) \to (0,0)} \frac{\ln(1+xy)}{xy}=[z=xy]=\lim_{z\to 0}\frac{\ln(1+z)}{z}=1.$$ Is the way I calculated the limits for $(x,y)\to (0,0)$ by substituting with $z=xy$ legit?  Also, if it is... am I allowed to substitute an expression with its limit inside a limit, as in while calculating the limit, or can I only take the limits in one last step (I'm a bit confused by this exercise in general, I have solved it with Taylor series but I'm curious to know whether this works too)? Thank you so much in advance!","This is my first question here so I hope I'm doing it right :) sorry otherwise! As in the title, I was wondering if and when it is OK to calculate a limit i three dimensions through a substitution that ""brings it down to two dimensions"". Let me explain what I mean in a clearer way through an example. I was calculating this limit: $$\lim_{(x,y) \to (0,0)} \frac{\ln (1+\sin^2(xy))}{x^2+y^2} =\lim_{(x,y) \to (0,0)} \frac{\ln (1+\sin(xy)\cdot \sin(xy))}{x^2+y^2}$$ $$=\lim_{(x,y) \to (0,0)} \frac{\ln (1+xy\cdot xy)}{x^2+y^2} =\lim_{(x,y) \to (0,0)} \frac{\ln (1+x^2y^2)}{x^2+y^2}=\lim_{(x,y) \to (0,0)} \frac{x^2y^2}{x^2+y^2}$$ $$=\lim_{(x,y) \to (0,0)}\frac{1}{\frac{1}{y^2}+\frac{1}{x^2}}=""\frac{1}{\infty}""=0.$$ Where I have used: $$ \lim_{(x,y) \to (0,0)} \frac{\sin(xy)}{xy}=[z=xy]=\lim_{z\to 0}\frac{\sin z}{z}=1$$ and  $$ \lim_{(x,y) \to (0,0)} \frac{\ln(1+xy)}{xy}=[z=xy]=\lim_{z\to 0}\frac{\ln(1+z)}{z}=1.$$ Is the way I calculated the limits for $(x,y)\to (0,0)$ by substituting with $z=xy$ legit?  Also, if it is... am I allowed to substitute an expression with its limit inside a limit, as in while calculating the limit, or can I only take the limits in one last step (I'm a bit confused by this exercise in general, I have solved it with Taylor series but I'm curious to know whether this works too)? Thank you so much in advance!",,"['calculus', 'limits', 'multivariable-calculus', 'substitution']"
53,Proving a local minimum is a global minimum.,Proving a local minimum is a global minimum.,,"Let $f(x,y)=xy+ \frac{50}{x}+\frac{20}{y}$, Find the global minimum / maximum of the function for $x>0,y>0$ Clearly the function has no global maximum since $f$ is not bounded. I have found that the point $(5,2)$ is a local minimum of $f$. It seems pretty obvious that this point is a global minimum, but I'm struggling with a formal proof.","Let $f(x,y)=xy+ \frac{50}{x}+\frac{20}{y}$, Find the global minimum / maximum of the function for $x>0,y>0$ Clearly the function has no global maximum since $f$ is not bounded. I have found that the point $(5,2)$ is a local minimum of $f$. It seems pretty obvious that this point is a global minimum, but I'm struggling with a formal proof.",,"['multivariable-calculus', 'optimization', 'fractions', 'maxima-minima', 'a.m.-g.m.-inequality']"
54,Show that $ \ d(d \omega)=0 \ $,Show that, \ d(d \omega)=0 \ ,"If $ \ \omega=f(x,y,z)dx+g(x,y,z)dy+h(x,y,z)dz \ $ and $ f, \ g , \ h \ $ are smooth functions on $ \mathbb{R}^3 \ $ , then show that $ \ d(d \omega)=0 \ $. Answer: $$ d \omega=df \wedge dx+dg \wedge dy+dh \wedge dz$$ or  $$d \omega=(\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy+\frac{\partial f}{\partial z}dz) \wedge dx + (\frac{\partial g}{\partial x}dx+\frac{\partial g}{\partial y}dy+\frac{\partial g}{\partial z}dz) \wedge dy + (\frac{\partial h}{\partial x}dx+\frac{\partial h}{\partial y}dy+\frac{\partial h}{\partial z}dz) \wedge dz $$  or  $$d (\omega)=(\frac{\partial h}{\partial y}-\frac{\partial g}{\partial z}) dy \wedge dz+ (\frac{\partial f}{\partial z}-\frac{\partial h}{\partial x}) dz \wedge dx + (\frac{\partial g}{\partial x}-\frac{\partial f}{\partial y}) dx \wedge dy $$ Next I can not proceed to show $ d(d\omega)=0 \ $ anyhow ? Help me out","If $ \ \omega=f(x,y,z)dx+g(x,y,z)dy+h(x,y,z)dz \ $ and $ f, \ g , \ h \ $ are smooth functions on $ \mathbb{R}^3 \ $ , then show that $ \ d(d \omega)=0 \ $. Answer: $$ d \omega=df \wedge dx+dg \wedge dy+dh \wedge dz$$ or  $$d \omega=(\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy+\frac{\partial f}{\partial z}dz) \wedge dx + (\frac{\partial g}{\partial x}dx+\frac{\partial g}{\partial y}dy+\frac{\partial g}{\partial z}dz) \wedge dy + (\frac{\partial h}{\partial x}dx+\frac{\partial h}{\partial y}dy+\frac{\partial h}{\partial z}dz) \wedge dz $$  or  $$d (\omega)=(\frac{\partial h}{\partial y}-\frac{\partial g}{\partial z}) dy \wedge dz+ (\frac{\partial f}{\partial z}-\frac{\partial h}{\partial x}) dz \wedge dx + (\frac{\partial g}{\partial x}-\frac{\partial f}{\partial y}) dx \wedge dy $$ Next I can not proceed to show $ d(d\omega)=0 \ $ anyhow ? Help me out",,"['multivariable-calculus', 'differential-geometry', 'differential-forms', 'exterior-algebra']"
55,Orientation of a sphere,Orientation of a sphere,,"I am having a hard time wrapping my head around notions of an orientable two-sided surface in $3$ dimensional space. For instance, the sphere in $\Bbb R^3$ is the set $\{ (x,y,z) \in \Bbb R^3 :  ||(x,y,z)|| = 1 \}$, and a normal vector that points outward of the sphere is given at any point $(x,y,z)$ by $xi + yj + zk$. Now having said all that, which points are on the ""inside"" and which are on the ""outside""? Perhaps I have misunderstood the concept. Any clarifications much appreciated.","I am having a hard time wrapping my head around notions of an orientable two-sided surface in $3$ dimensional space. For instance, the sphere in $\Bbb R^3$ is the set $\{ (x,y,z) \in \Bbb R^3 :  ||(x,y,z)|| = 1 \}$, and a normal vector that points outward of the sphere is given at any point $(x,y,z)$ by $xi + yj + zk$. Now having said all that, which points are on the ""inside"" and which are on the ""outside""? Perhaps I have misunderstood the concept. Any clarifications much appreciated.",,"['geometry', 'multivariable-calculus']"
56,How to (quickly) determine whether a function is totally differentiable,How to (quickly) determine whether a function is totally differentiable,,"I'm studying up for an exam, and an example question is: Show that the function $f(x,y):=(x^2y-\frac13y^3, \; \frac13 x^3-xy^2)$ is fully differentiable and determine its derivative. Now I'm able to solve this with about a page of tedious writing, but since it's an exam question I figured there must be a quick(er) way to do this. Hence this question. My solution would be to calculate partial derivatives and plugging them (pairwise) in to the limit of the derivative so that we may show both the first and second element of the $f(\textbf{v})$ vector to go to zero. (Using the definition saying that if $$\lim_{h\rightarrow0}\frac{|f(x+h)-f(x)-A(h)|}{|h|}=0,$$ then $f'(x) = A$ with $A$ a linear transformation.","I'm studying up for an exam, and an example question is: Show that the function $f(x,y):=(x^2y-\frac13y^3, \; \frac13 x^3-xy^2)$ is fully differentiable and determine its derivative. Now I'm able to solve this with about a page of tedious writing, but since it's an exam question I figured there must be a quick(er) way to do this. Hence this question. My solution would be to calculate partial derivatives and plugging them (pairwise) in to the limit of the derivative so that we may show both the first and second element of the $f(\textbf{v})$ vector to go to zero. (Using the definition saying that if $$\lim_{h\rightarrow0}\frac{|f(x+h)-f(x)-A(h)|}{|h|}=0,$$ then $f'(x) = A$ with $A$ a linear transformation.",,"['real-analysis', 'multivariable-calculus']"
57,Need help to parametrize the catenary by arc length,Need help to parametrize the catenary by arc length,,"The trace of the parametrized curve is $$\alpha(t)=(t,\cosh t),\ t\in\mathbb R$$ is called catenary. I want to show the curvature of the catenary is $$k(t)=\frac{1}{\cosh^2t}$$ Before finding the curvature I need to parametrize it by arc length. Do Carmo in his classical Differential Geometry book makes the following remark about it on page 22: since the catenary is defined for every $t\in \mathbb R$, I'm having trouble to know the value of $t_0$, can it be anything?","The trace of the parametrized curve is $$\alpha(t)=(t,\cosh t),\ t\in\mathbb R$$ is called catenary. I want to show the curvature of the catenary is $$k(t)=\frac{1}{\cosh^2t}$$ Before finding the curvature I need to parametrize it by arc length. Do Carmo in his classical Differential Geometry book makes the following remark about it on page 22: since the catenary is defined for every $t\in \mathbb R$, I'm having trouble to know the value of $t_0$, can it be anything?",,"['multivariable-calculus', 'differential-geometry', 'parametric', 'curves', 'plane-curves']"
58,Lagrange multipliers for dummies (me),Lagrange multipliers for dummies (me),,"I'd like to minimize this $\mathbb{R}^4$ function: \begin{equation} f(x,y,w,z) = xw+yz \end{equation} subject to the constraint \begin{equation} x^2+y^2-w^2-z^2=0 \end{equation} Well, when I apply the Lagrange multiplier method to the problem, I get as the only real solution the point $(0,0,0,0)$ which is coherent with the constraint but actually the point $(k,0,-k,0)$ with real $k$ is a much better minimum! I feel that since the method gives the stationary points of the lagrangian it can fail to localize 'minimizing' manifolds such as $x=-w$ in this particular case. Am I right? What is the best way to understand what goes on in this particular problem?","I'd like to minimize this $\mathbb{R}^4$ function: \begin{equation} f(x,y,w,z) = xw+yz \end{equation} subject to the constraint \begin{equation} x^2+y^2-w^2-z^2=0 \end{equation} Well, when I apply the Lagrange multiplier method to the problem, I get as the only real solution the point $(0,0,0,0)$ which is coherent with the constraint but actually the point $(k,0,-k,0)$ with real $k$ is a much better minimum! I feel that since the method gives the stationary points of the lagrangian it can fail to localize 'minimizing' manifolds such as $x=-w$ in this particular case. Am I right? What is the best way to understand what goes on in this particular problem?",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
59,ln(1+x) maclaurin series,ln(1+x) maclaurin series,,"I found the first four derivatives of  $ f(x) = ln(1+x) $ Then for all n > 1, $$ f^n(x) = \frac{(-1^{n+1})(n-1)!}{(1+x)^n } $$ So, $$ f^n(0) = (-1^{n+1})(n-1)! $$ By definition Maclaurin Series are defined as: $$\sum_{n=0}^{\infty} \frac{f^n(0)}{n!}x^n$$ *Since the $ f^n(0) $ is only true when n > 1 then, $$\sum_{n=1}^{\infty} \frac{f^{n+1}(0)}{n+1!}x^{n+1}$$ I continue by replacing each term and I get: $$ ln(1+x) = - \sum_{n=1}^{\infty} \frac{(-1)^n}{n*(n+1)}x^{n+1}$$ I know that the answer should be : $$ ln(1+x) = - \sum_{n=1}^{\infty} \frac{(-1)^n}{n}x^{n}$$ Where did I go wrong? ""*"" : Unsure about the step. I'm really bad at taylor series so if you have any good sites for resources, it would be very much appreciated. Thank you for any help and answers.","I found the first four derivatives of  $ f(x) = ln(1+x) $ Then for all n > 1, $$ f^n(x) = \frac{(-1^{n+1})(n-1)!}{(1+x)^n } $$ So, $$ f^n(0) = (-1^{n+1})(n-1)! $$ By definition Maclaurin Series are defined as: $$\sum_{n=0}^{\infty} \frac{f^n(0)}{n!}x^n$$ *Since the $ f^n(0) $ is only true when n > 1 then, $$\sum_{n=1}^{\infty} \frac{f^{n+1}(0)}{n+1!}x^{n+1}$$ I continue by replacing each term and I get: $$ ln(1+x) = - \sum_{n=1}^{\infty} \frac{(-1)^n}{n*(n+1)}x^{n+1}$$ I know that the answer should be : $$ ln(1+x) = - \sum_{n=1}^{\infty} \frac{(-1)^n}{n}x^{n}$$ Where did I go wrong? ""*"" : Unsure about the step. I'm really bad at taylor series so if you have any good sites for resources, it would be very much appreciated. Thank you for any help and answers.",,['multivariable-calculus']
60,$ \Psi^* \frac{\partial^2 \Psi}{\partial t \partial x} = -\frac{\partial \Psi^*}{\partial x} \frac{\partial \Psi}{\partial t} $,, \Psi^* \frac{\partial^2 \Psi}{\partial t \partial x} = -\frac{\partial \Psi^*}{\partial x} \frac{\partial \Psi}{\partial t} ,"I'm  currently through a derivation in my introductory Quantum Mechanics course. I'm having trouble understanding the steps behind the following partial derivative. For a function $\Psi (x,t)$ and its complex conjugate $\Psi^*(x,t)$,why is the following true? $$ \Psi^* \frac{\partial^2 \Psi}{\partial t \partial x} = -\frac{\partial \Psi^*}{\partial x} \frac{\partial \Psi}{\partial t} $$ In case I made some error, the full problem is: $$ \frac{d\left < p \right >}{dt} = -i \hbar \int_{-\infty}^\infty \left ( \frac{\partial \Psi^*}{\partial t} \frac{\partial \Psi}{\partial x} + \Psi^* \frac{\partial^2 \Psi}{\partial t \partial x} \right ) dx \\ = \int_{-\infty}^\infty \left ( i\hbar \frac{\partial \Psi}{\partial t} \right )^* \frac{\partial \Psi}{\partial x} + \frac{\partial \Psi^*}{\partial x}\left ( i\hbar \frac{\partial \Psi}{\partial t} \right )dx$$ Thank you","I'm  currently through a derivation in my introductory Quantum Mechanics course. I'm having trouble understanding the steps behind the following partial derivative. For a function $\Psi (x,t)$ and its complex conjugate $\Psi^*(x,t)$,why is the following true? $$ \Psi^* \frac{\partial^2 \Psi}{\partial t \partial x} = -\frac{\partial \Psi^*}{\partial x} \frac{\partial \Psi}{\partial t} $$ In case I made some error, the full problem is: $$ \frac{d\left < p \right >}{dt} = -i \hbar \int_{-\infty}^\infty \left ( \frac{\partial \Psi^*}{\partial t} \frac{\partial \Psi}{\partial x} + \Psi^* \frac{\partial^2 \Psi}{\partial t \partial x} \right ) dx \\ = \int_{-\infty}^\infty \left ( i\hbar \frac{\partial \Psi}{\partial t} \right )^* \frac{\partial \Psi}{\partial x} + \frac{\partial \Psi^*}{\partial x}\left ( i\hbar \frac{\partial \Psi}{\partial t} \right )dx$$ Thank you",,"['multivariable-calculus', 'partial-derivative', 'quantum-mechanics']"
61,Question on Euclidean norm of a non-square matrix,Question on Euclidean norm of a non-square matrix,,"Let $\;f:\mathbb R^n \rightarrow \mathbb R^m\;$ and consider the   matrix $\; \nabla \cdot f=\begin{pmatrix}                            \frac{\partial f_1}{\partial x_1} \dots \frac{\partial f_1}{\partial x_n} \\ \dots \\ \frac{\partial  f_m}{\partial x_1} \dots \frac{\partial f_m}{\partial x_n}\\                              \end{pmatrix}\;$. I want to compute this: $\;\frac{1}{2} {\vert \nabla f \vert }^2\;$   where $\;\vert \cdot \vert\;$ is the Euclidean norm of the matrix. NOTE: $\;n\;$ is not necessary equal to $\;m\;$ Searching on google about ""Euclidean norm of a non-square matrix"", all the results I found, were about the Frobenius norm. So I thought it would be a good idea to compute the Frobenious norm of $\;\nabla f  \;$ . But then, I came across with this post What is the difference between the Frobenius norm and the 2-norm of a matrix? which confused me completely. I haven't had any experience in norms of matrices until I was assigned to compute the above one. This is why I apologize in advance if my questions below are quite trivial or silly. What is the Euclidean norm of the above matrix? How should I proceed in order to compute $\;\frac{1}{2} {\vert \nabla f \vert }^2\;$ ? Any help or hints would be valuable. Thanks in advance!","Let $\;f:\mathbb R^n \rightarrow \mathbb R^m\;$ and consider the   matrix $\; \nabla \cdot f=\begin{pmatrix}                            \frac{\partial f_1}{\partial x_1} \dots \frac{\partial f_1}{\partial x_n} \\ \dots \\ \frac{\partial  f_m}{\partial x_1} \dots \frac{\partial f_m}{\partial x_n}\\                              \end{pmatrix}\;$. I want to compute this: $\;\frac{1}{2} {\vert \nabla f \vert }^2\;$   where $\;\vert \cdot \vert\;$ is the Euclidean norm of the matrix. NOTE: $\;n\;$ is not necessary equal to $\;m\;$ Searching on google about ""Euclidean norm of a non-square matrix"", all the results I found, were about the Frobenius norm. So I thought it would be a good idea to compute the Frobenious norm of $\;\nabla f  \;$ . But then, I came across with this post What is the difference between the Frobenius norm and the 2-norm of a matrix? which confused me completely. I haven't had any experience in norms of matrices until I was assigned to compute the above one. This is why I apologize in advance if my questions below are quite trivial or silly. What is the Euclidean norm of the above matrix? How should I proceed in order to compute $\;\frac{1}{2} {\vert \nabla f \vert }^2\;$ ? Any help or hints would be valuable. Thanks in advance!",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'normed-spaces']"
62,"Limit $\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}$",Limit,"\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}","First of all, thanks for any help provided.  My question is how to properly solve this limit: $\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}$ I should be 1 as it look, I tried it using polar coordinates and I obtained this limit: $\lim_{r \to 0} \frac{\sin(r^2\sin(\theta)\cos(\theta))}{r^2\sin(\theta)\cos(\theta)}$ where I am using $x=r\cos(\theta)$ and $y=r\sin(\theta)$.  From this limit how I conclude that is equal 1?  I guess $\theta$ don't approach any value while $x,y \to 0$ and that is because I didn't wrote it in the limit (is that correct?). Thank you","First of all, thanks for any help provided.  My question is how to properly solve this limit: $\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}$ I should be 1 as it look, I tried it using polar coordinates and I obtained this limit: $\lim_{r \to 0} \frac{\sin(r^2\sin(\theta)\cos(\theta))}{r^2\sin(\theta)\cos(\theta)}$ where I am using $x=r\cos(\theta)$ and $y=r\sin(\theta)$.  From this limit how I conclude that is equal 1?  I guess $\theta$ don't approach any value while $x,y \to 0$ and that is because I didn't wrote it in the limit (is that correct?). Thank you",,['multivariable-calculus']
63,Multivariable derivative: Limit definition,Multivariable derivative: Limit definition,,"Consider a path through a domain in $\mathbb{R}^2$ given by $\mathbf{c}(t) = (x(t), y(t))$. We wish to find the rate of change of a function $f(x,y)$ along this path. Therefore, we wish to compute $\frac{d}{dt}f(\mathbf{c}(t))$. My question is about the limit definition. My book gives the limit definition of the derivative as $$ \frac{d}{dt}f(\mathbf{c}(t)) = \lim_{h\to\ 0}\frac{f(x(t+h),y(t+h)) - f(x(t),y(t))}{h}\tag{1}$$ Question Why isn't the derivative written as: $$\frac{d}{d\mathbf{c}'(t)}f(\mathbf{c}(t)) = \lim_{h\to\ 0}\frac{f(x(t+h),y(t+h)) - f(x(t),y(t))}{\sqrt{(x(t+h)-x(t))^2 + (y(t+h) - y(t))^2}}\tag{2}$$ $$\frac{d}{d\mathbf{c}'(t)}f(\mathbf{c}(t)) = \lim_{\Delta x,\Delta y\to\ 0}\frac{f(x(t) + \Delta x,y(t) + \Delta y) - f(x(t),y(t))}{\sqrt{\Delta x^2 + \Delta y^2}}$$ $$\Delta x = x(t+h) - x(t) \\ \Delta y = y(t+h) - y(t)$$ where $d/d\mathbf{c}'(t)$ indicates the derivative in the direction of the tangent vector of the path. After reading the comments and responses under this question, my answer to my own question (with their help) is that these two limits speak to different derivatives. The first limit indicates the rate of change of $f$ as the parameter $t$ is changed. It indicates change in $f$ per unit $t$ ($t$ usually stands for time, but as we'll see below, it's just an arbitrary parameter. $t$ doesn't have to be 'time'). This derivative will depend on how you parameterize your path. A path can be parameterized in infinitely many ways ($\mathbf{c}_2(t)$ might move along your path twice as fast as $\mathbf{c}_1(t)$ for instance). On the other hand, the second limit is simply the derivative of $f(x,y) = f(\mathbf{c}(t))$. It's just the derivative of the outside function with respect to the inside variables (instead of the derivative of the outside function with respect to the inside-inside variable - remember that if $f(x) = f(g(t))$, $f$ can either change directly through $x$, or indirectly through $t$. Because each case changes the function either by changing the $x$ or $t$ knob, we can ask for either $d/dx$ or $d/dt$). But back to equation $\textbf{(2)}$, since the inside variables $(x,y)$ form a path $\mathbf{c}(t)$, I don't write $\partial/\partial x$ or $\partial /\partial y$, but $d/d\mathbf{c}'(t)$ since $x,y$ are confined to change along a path. As you take the limit, you see that the derivative is in a direction tangent to your path, which is why I use $d/d\mathbf{c}'(t)$ since $\mathbf{c}'(t)$ is the tangent vector to the path. Again, the derivative represents the rate of change with respect to tangent lines to this path (you can see the direction that this derivative is taken in at a given $t$ by looking at the right hand side of equation $\textbf{(2)}$). This derivative indicates change in $f$ per unit change in tangent direction. This derivative will depend on your parameterization. Comparisons to the 1-variable case: The first limit (1) Let's discuss the first limit (the derivative of $f$ with respect to the 'inside-inside' variable. How does $f$ change indirectly through $t$). Is there a 1D analog? Yes. Consider the composite function $f(x) = f(x(t))$. What's the derivative of $f$ with respect to $t$? We can write the limit definition: $$\frac{df(x(t))}{dt} = \lim_{h\to\ 0}\frac{f(x(t+h))-f(x(t))}{h}\tag{3}$$ This is indeed the 1D version of the first limit above $\textbf{(1)}$. To further drive the comparison, we know that $\frac{df}{dt} = f'(x(t))x'(t) = (df/dx)(dx/dt)$ = derivative of the outside times derivative of the inside. And in the multivariate case, that first limit can be shown to equal $\nabla f \cdot \mathbf{c}'(t)$ = derivative of the outside times derivative of the inside and do a sum for each variable. Indeed, the multivariate chain rule is in fact the generalization of the 1D chain rule. $f(x(t))$ is a composite function. But so to is $f(x(t),y(t)) = f(\mathbf{c}(t))$, just in a multivariate way. $\mathbf{c}(t)$ parameterizes how you move along the $x$ and $y$ axes. In the single variable case $f(x(t))$, we are parameterizing how we move along the only axis that $f$ has access to, which is the $x$ axis ($x = x(t)$). We usually see composite functions as $f(g(x)) = f(u)$, in which case we are parameterizing how $f$ moves along the $u$ axis with parameterization $g(x)$ (I'm just using different variables to show that a parameter is a parameter - It doesn't have to be time). Again this derivative of the 'inside-inside' variable will depend on your parameterization, which we can clearly see by looking at the 1D case (just take simple examples and see how $x'(t)$ changes). For instance, in 1D the only path you have to parameterize is the real line. That's the only path (let the path be the whole real line). Therefore different parameterizations will have different $x'(t)$. That is, different parameterizations will have different 'speeds' (again note that if $x$ is parameterized by $x(t)$, $x' = x'(t)$ is only a 'speed' if $x$ has units of meters and $t$ units of time. By 'speed' of the parameterization, I just mean derivative. To make this clearer, let $f(u) = f(g(k))$. The speed of the parameterization is $du/dk = g'(k)$). The change in $f$ per unit parameter will depend on how your path is parameterized. If $f$ is in temperature units Kelvin, $df(x(t))/dt$ could be 2 Kelvin per second for one parameterization and 3 Kelvin per second for another. Comparisons to the 1-variable case: The second limit (2) Let's look at the 1-variable case $f(u) = f(g(t))$. What is $df/du$? This is just the derivative of the outside function with respect to the inside function (it's how $f$ changes directly through it's domain variables as opposed to indirectly through the parameter). It's limit definition is given by $$\frac{df(u)}{du} = \lim_{k\to\ 0}\frac{f(g(t) + k) - f(g(t))}{k}$$  which setting $k = g(t+h) - g(t)$ gives $$\frac{df(u)}{du} = \lim_{h\to\ 0}\frac{f(g(t+h)) - f(g(t))}{g(t+h) - g(t)}\tag{4}$$  Either way hopefully you can get to this line without going through the first. All you're doing is taking the function at two different values and dividing by the difference. This is the 1D analog of the second limit $\textbf{(2)}$. The main difference is that in 1D, I can only move along my parameterization, which here is the $u$ axis. Therefore I write $d/du$. In the multivariate case, again I can only move along my parameterization. However, as we take a small step size $\Delta x$ and $\Delta y$ allowed by my parameterization that we let go to zero (by looking at $\textbf{(2)}$ hopefully you can see that $h \to 0$ is equivalent to $\Delta x \to 0$ or $\Delta y \to 0$), we are finding the rate of change of the function $f$ in the tangent direction to the path (think of two points on your path $\mathbf{c}(t)$. Fix one and allow the other to approach it. The two points form a tangent line in the limit). Therefore, I use $d/d\mathbf{c}'(t)$ to denote direction of the derivative. It's always a direction tangent to the path and that tangent direction changes over the path. In 1D, the direction tangent to the path was just the path itself (the axis), for the whole path. This derivative $\textbf{(2)}$ and $\textbf{(4)}$) will depend on your parameterization, which we can see by taking simple examples and looking at the 1D case $\textbf{(4)}$. Equations $\textbf{(2)}$ and $\textbf{(4)}$ give the rate of change of $f$ along the tangent direction to the path (which depends on your parameterization). Equations $\textbf{(1)}$ and $\textbf{(3)}$ give the rate of change of $f$ with respect to the parameter (which also depends on your parameterization). Comments/Complaints on the Directional Derivative A pure directional derivative is given by equation $\textbf{(2)}$ where $\mathbf{c}(t)$ is a line. That's it, end of story. For a vector $\vec{v} = (v_x, v_y)$, a parameterization along this direction is $x(t) = x + v_xt$ and $y(t) = y + v_yt$. Therefore, the directional derivative, given by equation $\textbf{(2)}$ is: $$\frac{df(x(t),y(t))}{d\vec{v}} = \lim_{h\to 0} = \frac{f(x(t) + v_xh, y(t) + v_yh) - f(x(t), y(t))}{h}$$  $\textbf{IF}$ $\vec{v}$ is a unit vector so that $\sqrt{v_x^2 + v_y^2}$ = 1. However, textbooks will sometimes define this as the directional derivative even if the vector $\vec{v}$ is not a unit vector. I'm not a fan of this because that's not what equation $\textbf{(2)}$ says to do (and I find that $\textbf{(2)}$ makes sense). Essentially what they've done is $\textbf{redefine}$ what it means to be a unit vector. This is fine, but now we take $\vec{v}$ whatever it is, to be the unit vector standard. Therefore, their derivative which is still a rate of change of $f$ per unit distance, has a different meaning of 'per unit distance' then my definition of per unit distance based on my idea of a unit vector. The reason why they do this is because they understand what they are doing (redefining what a unit vector means - which is fine - but for 1st time learners, I think it can be confusing if you don't show that the directional derivative is essentially equation $\textbf{(2)}$.)","Consider a path through a domain in $\mathbb{R}^2$ given by $\mathbf{c}(t) = (x(t), y(t))$. We wish to find the rate of change of a function $f(x,y)$ along this path. Therefore, we wish to compute $\frac{d}{dt}f(\mathbf{c}(t))$. My question is about the limit definition. My book gives the limit definition of the derivative as $$ \frac{d}{dt}f(\mathbf{c}(t)) = \lim_{h\to\ 0}\frac{f(x(t+h),y(t+h)) - f(x(t),y(t))}{h}\tag{1}$$ Question Why isn't the derivative written as: $$\frac{d}{d\mathbf{c}'(t)}f(\mathbf{c}(t)) = \lim_{h\to\ 0}\frac{f(x(t+h),y(t+h)) - f(x(t),y(t))}{\sqrt{(x(t+h)-x(t))^2 + (y(t+h) - y(t))^2}}\tag{2}$$ $$\frac{d}{d\mathbf{c}'(t)}f(\mathbf{c}(t)) = \lim_{\Delta x,\Delta y\to\ 0}\frac{f(x(t) + \Delta x,y(t) + \Delta y) - f(x(t),y(t))}{\sqrt{\Delta x^2 + \Delta y^2}}$$ $$\Delta x = x(t+h) - x(t) \\ \Delta y = y(t+h) - y(t)$$ where $d/d\mathbf{c}'(t)$ indicates the derivative in the direction of the tangent vector of the path. After reading the comments and responses under this question, my answer to my own question (with their help) is that these two limits speak to different derivatives. The first limit indicates the rate of change of $f$ as the parameter $t$ is changed. It indicates change in $f$ per unit $t$ ($t$ usually stands for time, but as we'll see below, it's just an arbitrary parameter. $t$ doesn't have to be 'time'). This derivative will depend on how you parameterize your path. A path can be parameterized in infinitely many ways ($\mathbf{c}_2(t)$ might move along your path twice as fast as $\mathbf{c}_1(t)$ for instance). On the other hand, the second limit is simply the derivative of $f(x,y) = f(\mathbf{c}(t))$. It's just the derivative of the outside function with respect to the inside variables (instead of the derivative of the outside function with respect to the inside-inside variable - remember that if $f(x) = f(g(t))$, $f$ can either change directly through $x$, or indirectly through $t$. Because each case changes the function either by changing the $x$ or $t$ knob, we can ask for either $d/dx$ or $d/dt$). But back to equation $\textbf{(2)}$, since the inside variables $(x,y)$ form a path $\mathbf{c}(t)$, I don't write $\partial/\partial x$ or $\partial /\partial y$, but $d/d\mathbf{c}'(t)$ since $x,y$ are confined to change along a path. As you take the limit, you see that the derivative is in a direction tangent to your path, which is why I use $d/d\mathbf{c}'(t)$ since $\mathbf{c}'(t)$ is the tangent vector to the path. Again, the derivative represents the rate of change with respect to tangent lines to this path (you can see the direction that this derivative is taken in at a given $t$ by looking at the right hand side of equation $\textbf{(2)}$). This derivative indicates change in $f$ per unit change in tangent direction. This derivative will depend on your parameterization. Comparisons to the 1-variable case: The first limit (1) Let's discuss the first limit (the derivative of $f$ with respect to the 'inside-inside' variable. How does $f$ change indirectly through $t$). Is there a 1D analog? Yes. Consider the composite function $f(x) = f(x(t))$. What's the derivative of $f$ with respect to $t$? We can write the limit definition: $$\frac{df(x(t))}{dt} = \lim_{h\to\ 0}\frac{f(x(t+h))-f(x(t))}{h}\tag{3}$$ This is indeed the 1D version of the first limit above $\textbf{(1)}$. To further drive the comparison, we know that $\frac{df}{dt} = f'(x(t))x'(t) = (df/dx)(dx/dt)$ = derivative of the outside times derivative of the inside. And in the multivariate case, that first limit can be shown to equal $\nabla f \cdot \mathbf{c}'(t)$ = derivative of the outside times derivative of the inside and do a sum for each variable. Indeed, the multivariate chain rule is in fact the generalization of the 1D chain rule. $f(x(t))$ is a composite function. But so to is $f(x(t),y(t)) = f(\mathbf{c}(t))$, just in a multivariate way. $\mathbf{c}(t)$ parameterizes how you move along the $x$ and $y$ axes. In the single variable case $f(x(t))$, we are parameterizing how we move along the only axis that $f$ has access to, which is the $x$ axis ($x = x(t)$). We usually see composite functions as $f(g(x)) = f(u)$, in which case we are parameterizing how $f$ moves along the $u$ axis with parameterization $g(x)$ (I'm just using different variables to show that a parameter is a parameter - It doesn't have to be time). Again this derivative of the 'inside-inside' variable will depend on your parameterization, which we can clearly see by looking at the 1D case (just take simple examples and see how $x'(t)$ changes). For instance, in 1D the only path you have to parameterize is the real line. That's the only path (let the path be the whole real line). Therefore different parameterizations will have different $x'(t)$. That is, different parameterizations will have different 'speeds' (again note that if $x$ is parameterized by $x(t)$, $x' = x'(t)$ is only a 'speed' if $x$ has units of meters and $t$ units of time. By 'speed' of the parameterization, I just mean derivative. To make this clearer, let $f(u) = f(g(k))$. The speed of the parameterization is $du/dk = g'(k)$). The change in $f$ per unit parameter will depend on how your path is parameterized. If $f$ is in temperature units Kelvin, $df(x(t))/dt$ could be 2 Kelvin per second for one parameterization and 3 Kelvin per second for another. Comparisons to the 1-variable case: The second limit (2) Let's look at the 1-variable case $f(u) = f(g(t))$. What is $df/du$? This is just the derivative of the outside function with respect to the inside function (it's how $f$ changes directly through it's domain variables as opposed to indirectly through the parameter). It's limit definition is given by $$\frac{df(u)}{du} = \lim_{k\to\ 0}\frac{f(g(t) + k) - f(g(t))}{k}$$  which setting $k = g(t+h) - g(t)$ gives $$\frac{df(u)}{du} = \lim_{h\to\ 0}\frac{f(g(t+h)) - f(g(t))}{g(t+h) - g(t)}\tag{4}$$  Either way hopefully you can get to this line without going through the first. All you're doing is taking the function at two different values and dividing by the difference. This is the 1D analog of the second limit $\textbf{(2)}$. The main difference is that in 1D, I can only move along my parameterization, which here is the $u$ axis. Therefore I write $d/du$. In the multivariate case, again I can only move along my parameterization. However, as we take a small step size $\Delta x$ and $\Delta y$ allowed by my parameterization that we let go to zero (by looking at $\textbf{(2)}$ hopefully you can see that $h \to 0$ is equivalent to $\Delta x \to 0$ or $\Delta y \to 0$), we are finding the rate of change of the function $f$ in the tangent direction to the path (think of two points on your path $\mathbf{c}(t)$. Fix one and allow the other to approach it. The two points form a tangent line in the limit). Therefore, I use $d/d\mathbf{c}'(t)$ to denote direction of the derivative. It's always a direction tangent to the path and that tangent direction changes over the path. In 1D, the direction tangent to the path was just the path itself (the axis), for the whole path. This derivative $\textbf{(2)}$ and $\textbf{(4)}$) will depend on your parameterization, which we can see by taking simple examples and looking at the 1D case $\textbf{(4)}$. Equations $\textbf{(2)}$ and $\textbf{(4)}$ give the rate of change of $f$ along the tangent direction to the path (which depends on your parameterization). Equations $\textbf{(1)}$ and $\textbf{(3)}$ give the rate of change of $f$ with respect to the parameter (which also depends on your parameterization). Comments/Complaints on the Directional Derivative A pure directional derivative is given by equation $\textbf{(2)}$ where $\mathbf{c}(t)$ is a line. That's it, end of story. For a vector $\vec{v} = (v_x, v_y)$, a parameterization along this direction is $x(t) = x + v_xt$ and $y(t) = y + v_yt$. Therefore, the directional derivative, given by equation $\textbf{(2)}$ is: $$\frac{df(x(t),y(t))}{d\vec{v}} = \lim_{h\to 0} = \frac{f(x(t) + v_xh, y(t) + v_yh) - f(x(t), y(t))}{h}$$  $\textbf{IF}$ $\vec{v}$ is a unit vector so that $\sqrt{v_x^2 + v_y^2}$ = 1. However, textbooks will sometimes define this as the directional derivative even if the vector $\vec{v}$ is not a unit vector. I'm not a fan of this because that's not what equation $\textbf{(2)}$ says to do (and I find that $\textbf{(2)}$ makes sense). Essentially what they've done is $\textbf{redefine}$ what it means to be a unit vector. This is fine, but now we take $\vec{v}$ whatever it is, to be the unit vector standard. Therefore, their derivative which is still a rate of change of $f$ per unit distance, has a different meaning of 'per unit distance' then my definition of per unit distance based on my idea of a unit vector. The reason why they do this is because they understand what they are doing (redefining what a unit vector means - which is fine - but for 1st time learners, I think it can be confusing if you don't show that the directional derivative is essentially equation $\textbf{(2)}$.)",,"['calculus', 'multivariable-calculus']"
64,3 variable multiplication with 1 constraint lagrange multiplier,3 variable multiplication with 1 constraint lagrange multiplier,,"Using Lagrange multipliers, I need to calculate all points $(x,y,z)$ such that $$x^4y^6z^2$$ has a maximum or a minimum subject to the constraint that $$x^2 + y^2 + z^2 = 1$$ So, $f(x,y,z) = x^4y^6z^2 $ and $g(x,y,z) = x^2 + y^2 + z^2 - 1$ then i've done the partial derivatives $$\frac{\partial f}{\partial x}(x,y,z)=\lambda\frac{\partial g}{\partial x}$$ which gives $$4x^3y^6z^2 = 2xλ$$ $$6x^4y^5z^2 = 2yλ$$ $$2x^4y^6z = 2zλ$$ which i subsequently go on to find that $3x^2 = 2y^2 = 6z^2 $ This is where i've hit a dead end. Where do i go from here? or am i doing it all wrong? Thanks.","Using Lagrange multipliers, I need to calculate all points $(x,y,z)$ such that $$x^4y^6z^2$$ has a maximum or a minimum subject to the constraint that $$x^2 + y^2 + z^2 = 1$$ So, $f(x,y,z) = x^4y^6z^2 $ and $g(x,y,z) = x^2 + y^2 + z^2 - 1$ then i've done the partial derivatives $$\frac{\partial f}{\partial x}(x,y,z)=\lambda\frac{\partial g}{\partial x}$$ which gives $$4x^3y^6z^2 = 2xλ$$ $$6x^4y^5z^2 = 2yλ$$ $$2x^4y^6z = 2zλ$$ which i subsequently go on to find that $3x^2 = 2y^2 = 6z^2 $ This is where i've hit a dead end. Where do i go from here? or am i doing it all wrong? Thanks.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'maxima-minima', 'a.m.-g.m.-inequality']"
65,how to classify $x^2-4xy- 2xz+z^2=1$?,how to classify ?,x^2-4xy- 2xz+z^2=1,What surface is $x^2-4xy-2xz+z^2=1$? I just convert the equation to $(x-2y)^2+(x-z)^2=1+x^2+4y^2$ and $2(x-y)^2+(x-z)^2 = 1+2x^2+2y^2$. What is standard method to solve this kind of problem?,What surface is $x^2-4xy-2xz+z^2=1$? I just convert the equation to $(x-2y)^2+(x-z)^2=1+x^2+4y^2$ and $2(x-y)^2+(x-z)^2 = 1+2x^2+2y^2$. What is standard method to solve this kind of problem?,,['multivariable-calculus']
66,Differentiating a function of functions,Differentiating a function of functions,,"If I have the function $ h \equiv h(f(x,t),g(x,t))$, where $x$ and $t$ are independent variables and $f$ and $g$ are functions of $x$ and $t$. Then is, $$\frac{\mathrm{d}h }{\mathrm{d} t} = h\left(\frac{\mathrm{d}f }{\mathrm{d} t},g \right) + h\left(f , \frac{\mathrm{d}g }{\mathrm{d} t}\right)$$ or $$\frac{\mathrm{d}h }{\mathrm{d} t} = h\left(\frac{\partial{d}f }{\partial{d} t},g \right) + h\left(f , \frac{\partial{d}g }{\partial{d} t}\right)\ ?$$ And can you tell me why? I think the first one might be correct.","If I have the function $ h \equiv h(f(x,t),g(x,t))$, where $x$ and $t$ are independent variables and $f$ and $g$ are functions of $x$ and $t$. Then is, $$\frac{\mathrm{d}h }{\mathrm{d} t} = h\left(\frac{\mathrm{d}f }{\mathrm{d} t},g \right) + h\left(f , \frac{\mathrm{d}g }{\mathrm{d} t}\right)$$ or $$\frac{\mathrm{d}h }{\mathrm{d} t} = h\left(\frac{\partial{d}f }{\partial{d} t},g \right) + h\left(f , \frac{\partial{d}g }{\partial{d} t}\right)\ ?$$ And can you tell me why? I think the first one might be correct.",,['multivariable-calculus']
67,Inequalities (AMGM & Cauchy Schwarz),Inequalities (AMGM & Cauchy Schwarz),,"Let $ x, y, z \geq 0 $ such that $x+y+z=1$. Find the maximum value of   $$x (x+y)^2 (y+z)^3 (z+x)^4.$$ Hi recently I've been stuck on this problem for quite some time, and I would appreciate some help/hints on how to approach this inequality. Also, would be good if the approaches were based on AM-GM or Cauchy Schwarz. Thanks!","Let $ x, y, z \geq 0 $ such that $x+y+z=1$. Find the maximum value of   $$x (x+y)^2 (y+z)^3 (z+x)^4.$$ Hi recently I've been stuck on this problem for quite some time, and I would appreciate some help/hints on how to approach this inequality. Also, would be good if the approaches were based on AM-GM or Cauchy Schwarz. Thanks!",,"['multivariable-calculus', 'inequality', 'optimization', 'maxima-minima', 'a.m.-g.m.-inequality']"
68,Find the minimum value of $P=\sum _{cyc}\frac{\left(x+1\right)^2\left(y+1\right)^2}{z^2+1}$,Find the minimum value of,P=\sum _{cyc}\frac{\left(x+1\right)^2\left(y+1\right)^2}{z^2+1},"For $x>0$, $y>0$, $z>0$ and $x+y+z=3$ find the minimize value of $$P=\frac{\left(x+1\right)^2\left(y+1\right)^2}{z^2+1}+\frac{\left(y+1\right)^2\left(z+1\right)^2}{x^2+1}+\frac{\left(z+1\right)^2\left(x+1\right)^2}{y^2+1}$$ We have: $P=\left(\left(x+1\right)\left(y+1\right)\left(z+1\right)\right)^2\left(\frac{1}{\left(z+1\right)^2\left(z^2+1\right)}+\frac{1}{\left(y+1\right)^2\left(y^2+1\right)}+\frac{1}{\left(x+1\right)^2\left(x^2+1\right)}\right)$ $\ge \left(\left(x+1\right)\left(y+1\right)\left(z+1\right)\right)^2\left(\frac{1}{2\left(z^2+1\right)^2}+\frac{1}{2\left(x^2+1\right)^2}+\frac{1}{2\left(y^2+1\right)^2}\right)$ $\ge \left(\left(x+1\right)\left(y+1\right)\left(z+1\right)\right)^2\left(\frac{9}{2\left(\left(z^2+1\right)^2+\left(y^2+1\right)^2+\left(x^2+1\right)^2\right)}\right)$ I can't continue. Help","For $x>0$, $y>0$, $z>0$ and $x+y+z=3$ find the minimize value of $$P=\frac{\left(x+1\right)^2\left(y+1\right)^2}{z^2+1}+\frac{\left(y+1\right)^2\left(z+1\right)^2}{x^2+1}+\frac{\left(z+1\right)^2\left(x+1\right)^2}{y^2+1}$$ We have: $P=\left(\left(x+1\right)\left(y+1\right)\left(z+1\right)\right)^2\left(\frac{1}{\left(z+1\right)^2\left(z^2+1\right)}+\frac{1}{\left(y+1\right)^2\left(y^2+1\right)}+\frac{1}{\left(x+1\right)^2\left(x^2+1\right)}\right)$ $\ge \left(\left(x+1\right)\left(y+1\right)\left(z+1\right)\right)^2\left(\frac{1}{2\left(z^2+1\right)^2}+\frac{1}{2\left(x^2+1\right)^2}+\frac{1}{2\left(y^2+1\right)^2}\right)$ $\ge \left(\left(x+1\right)\left(y+1\right)\left(z+1\right)\right)^2\left(\frac{9}{2\left(\left(z^2+1\right)^2+\left(y^2+1\right)^2+\left(x^2+1\right)^2\right)}\right)$ I can't continue. Help",,"['multivariable-calculus', 'inequality', 'maxima-minima', 'cauchy-schwarz-inequality', 'uvw']"
69,"volume between a plane, a cone and two cylinders","volume between a plane, a cone and two cylinders",,"Compute the volume which is between: The plane $z=0$ the cone $z=\sqrt{x^2+y^2}$ and the cylinders $x^2+y^2=x$, $x^2+y^2=2x$. I feel that it may be helpful to change the variables to cylindrical coordinates, but I find it hard to identify the domain of each $r,\theta,z$. Edit : Can we say: $2x\geq x^2+y^2\geq x$, so $2r\cos\theta\geq r^2\geq r\cos\theta$, then $2\cos\theta\geq r\geq \cos\theta$. Then $0\leq z\leq r$, and $-\pi/2\leq\theta\leq\pi/2$. and now do the integration?","Compute the volume which is between: The plane $z=0$ the cone $z=\sqrt{x^2+y^2}$ and the cylinders $x^2+y^2=x$, $x^2+y^2=2x$. I feel that it may be helpful to change the variables to cylindrical coordinates, but I find it hard to identify the domain of each $r,\theta,z$. Edit : Can we say: $2x\geq x^2+y^2\geq x$, so $2r\cos\theta\geq r^2\geq r\cos\theta$, then $2\cos\theta\geq r\geq \cos\theta$. Then $0\leq z\leq r$, and $-\pi/2\leq\theta\leq\pi/2$. and now do the integration?",,"['integration', 'multivariable-calculus']"
70,Computing Jacobian of matrix-valued map $X \mapsto X^TAX$,Computing Jacobian of matrix-valued map,X \mapsto X^TAX,"Let $A = (a_{i,j})$ be a constant $n \times n$ matrix. Identify points $ (\vec{x}_1 , \dots, \vec{x}_n) \in R^{n^2}$, $\vec{x}_j =(x_{1,j}, \dots, x_{n,j}) \in \mathbb{R}^n$, $1 \le j \le n$,  with $n \times n$ matrices by arranging the vectors $\vec{x}_j$ into columns: $$\mathbb{R}^{n^2} \ni (\vec{x}_1 , \dots, \vec{x}_n) = \begin{bmatrix} x_{1,1} & \cdots & x_{1,n} \\ \vdots & \cdots & \vdots \\ x_{n,1} & \cdots & x_{n,n} \end{bmatrix} \equiv X. $$ Define the smooth map $F: \mathbb{R}^{n^2} \to \mathbb{R}^{n^2}$ by  $$F(\vec{x}_1 , \dots, \vec{x}_n) = X^T A X,$$ where $X^T$ denotes matrix transpose. I would like to find the $n^2 \times n^2$ Jacobian matrix $JacF(X)$ of the map $F$ at the point $X = I$, where $I$ is the $n \times n$ identity matrix. In particular, I would like to know if $\det(A) \neq 0$ implies that the Jacobian matrix at $I$ is also nonsingular. If we label the component functions of $F$ as $F = (F_{1,1} \dots F_{n,1}, \dots , F_{1,n}, \dots, F_{n,n})$, I have determined that  $$F_{i,j} = \sum_{k,l =1}^n a_{k,l}x_{k,i}x_{l,j}. $$ But the calculation appears to become quite tedious from here, since for each $F_{i,j}$ we need to calculated $n^2$ total partial derivatives. Is there a more efficient way to determine whether $\det(JacF(I)) \neq 0$, perhaps by using total derivatives or the chain rule in some fashion? Hints or answers are greatly appreciated!","Let $A = (a_{i,j})$ be a constant $n \times n$ matrix. Identify points $ (\vec{x}_1 , \dots, \vec{x}_n) \in R^{n^2}$, $\vec{x}_j =(x_{1,j}, \dots, x_{n,j}) \in \mathbb{R}^n$, $1 \le j \le n$,  with $n \times n$ matrices by arranging the vectors $\vec{x}_j$ into columns: $$\mathbb{R}^{n^2} \ni (\vec{x}_1 , \dots, \vec{x}_n) = \begin{bmatrix} x_{1,1} & \cdots & x_{1,n} \\ \vdots & \cdots & \vdots \\ x_{n,1} & \cdots & x_{n,n} \end{bmatrix} \equiv X. $$ Define the smooth map $F: \mathbb{R}^{n^2} \to \mathbb{R}^{n^2}$ by  $$F(\vec{x}_1 , \dots, \vec{x}_n) = X^T A X,$$ where $X^T$ denotes matrix transpose. I would like to find the $n^2 \times n^2$ Jacobian matrix $JacF(X)$ of the map $F$ at the point $X = I$, where $I$ is the $n \times n$ identity matrix. In particular, I would like to know if $\det(A) \neq 0$ implies that the Jacobian matrix at $I$ is also nonsingular. If we label the component functions of $F$ as $F = (F_{1,1} \dots F_{n,1}, \dots , F_{1,n}, \dots, F_{n,n})$, I have determined that  $$F_{i,j} = \sum_{k,l =1}^n a_{k,l}x_{k,i}x_{l,j}. $$ But the calculation appears to become quite tedious from here, since for each $F_{i,j}$ we need to calculated $n^2$ total partial derivatives. Is there a more efficient way to determine whether $\det(JacF(I)) \neq 0$, perhaps by using total derivatives or the chain rule in some fashion? Hints or answers are greatly appreciated!",,"['matrices', 'multivariable-calculus', 'jacobian']"
71,"Calculate the directional derivative of $f(x,y)= \frac{x^2-y^2}{x^2+y^2} $ in the direction $\vec{v}=(\cos\phi,\sin\phi)$",Calculate the directional derivative of  in the direction,"f(x,y)= \frac{x^2-y^2}{x^2+y^2}  \vec{v}=(\cos\phi,\sin\phi)","$f(x,y)= \frac{x^2-y^2}{x^2+y^2} $ when $(x,y)\neq0$  and $f(x,y)=0$  when $(x,y)=0$ The question: Calculate the directional derivative of $f$ in the direction $\vec{v}=(\cos\phi,\sin\phi)$ at point $(x,y)$ . What I did: $$\lim_{h\to0}\frac{f((x,y)+h(\cos\phi,\sin\phi))-f(x,y)}{h}$$ $$= \lim_{h\to0}\frac{f(x+h\cos\phi,y+\sin\phi)-f(x,y)}{h}$$ $$=\lim_{h\to0}\frac{1}{h}\left({\frac{(x+h\cos\phi)^2-(y+h\sin\phi)^2}{(x+h\cos\phi)^2+(y+h\sin\phi)^2}-\frac{x^2-y^2}{x^2+y^2}}\right)$$ This seems right, but it seems that if I start multiplying everything, it becomes hell, and I can't get it right when there are three lines of equations. Am I missing something? Is there any better way to do this?","$f(x,y)= \frac{x^2-y^2}{x^2+y^2} $ when $(x,y)\neq0$  and $f(x,y)=0$  when $(x,y)=0$ The question: Calculate the directional derivative of $f$ in the direction $\vec{v}=(\cos\phi,\sin\phi)$ at point $(x,y)$ . What I did: $$\lim_{h\to0}\frac{f((x,y)+h(\cos\phi,\sin\phi))-f(x,y)}{h}$$ $$= \lim_{h\to0}\frac{f(x+h\cos\phi,y+\sin\phi)-f(x,y)}{h}$$ $$=\lim_{h\to0}\frac{1}{h}\left({\frac{(x+h\cos\phi)^2-(y+h\sin\phi)^2}{(x+h\cos\phi)^2+(y+h\sin\phi)^2}-\frac{x^2-y^2}{x^2+y^2}}\right)$$ This seems right, but it seems that if I start multiplying everything, it becomes hell, and I can't get it right when there are three lines of equations. Am I missing something? Is there any better way to do this?",,"['limits', 'multivariable-calculus']"
72,"Finding minima and maxima to $f(x,y) = x^2 + x(y^2 - 1)$ in the area $x^2 + y^2 \leq 1$",Finding minima and maxima to  in the area,"f(x,y) = x^2 + x(y^2 - 1) x^2 + y^2 \leq 1","I'm asked to find minima and maxima on the function  $$f(x,y) = x^2 + x(y^2 - 1)$$ in the area  $$x^2 + y^2 \leq 1.$$ My solution: $$\nabla (f) = (2x + y^2 - 1, 2xy)$$ Finding stationary points  $$2xy = 0$$ $$2x + y^2 - 1 = 0$$ gets me $(0,1),(0,-1),(\frac{1}{2},0)$. Stuyding the boundary:  $$x^2 + y^2 = 1$$ $$y^2 = 1 - x^2 $$ $$f(x,y) = f(x, 1-x^2) = x^2 -x^3$$ Finding stationary points on the boundary: $$f'(x,y) = 2x - 3x^2 = 0$$ gives $(0, 1), (0,-1), (\frac{2}{3}, \sqrt{\frac{5}{9}}),(\frac{2}{3}, -\sqrt{\frac{5}{9}})$. So in total i've got the stationary points  $$(\frac{1}{2} , 0) | (0,1)| (0,-1)| (\frac{2}{3},\sqrt{\frac{5}{9}})|(\frac{2}{3}, -\sqrt{\frac{5}{9}}).$$ which give the function values of:  $$-\frac{1}{4}, 0,0,0.15,0.15$$. which gives the minima: $-\frac{1}{4}$ and maxima $0.15$. The minima is correct but the maxima should be 2. Why? What stationary point am I missing?","I'm asked to find minima and maxima on the function  $$f(x,y) = x^2 + x(y^2 - 1)$$ in the area  $$x^2 + y^2 \leq 1.$$ My solution: $$\nabla (f) = (2x + y^2 - 1, 2xy)$$ Finding stationary points  $$2xy = 0$$ $$2x + y^2 - 1 = 0$$ gets me $(0,1),(0,-1),(\frac{1}{2},0)$. Stuyding the boundary:  $$x^2 + y^2 = 1$$ $$y^2 = 1 - x^2 $$ $$f(x,y) = f(x, 1-x^2) = x^2 -x^3$$ Finding stationary points on the boundary: $$f'(x,y) = 2x - 3x^2 = 0$$ gives $(0, 1), (0,-1), (\frac{2}{3}, \sqrt{\frac{5}{9}}),(\frac{2}{3}, -\sqrt{\frac{5}{9}})$. So in total i've got the stationary points  $$(\frac{1}{2} , 0) | (0,1)| (0,-1)| (\frac{2}{3},\sqrt{\frac{5}{9}})|(\frac{2}{3}, -\sqrt{\frac{5}{9}}).$$ which give the function values of:  $$-\frac{1}{4}, 0,0,0.15,0.15$$. which gives the minima: $-\frac{1}{4}$ and maxima $0.15$. The minima is correct but the maxima should be 2. Why? What stationary point am I missing?",,"['analysis', 'multivariable-calculus']"
73,Meaning of $\times$ in this definition of a function?,Meaning of  in this definition of a function?,\times,"What is the meaning of writing $\times$ here? For $(t,x,\xi)\in[0,\infty)\times\mathbb{R}^3 \times \mathbb{R}^3$ we consider the function $f(t,x,\xi)$. Can't we just write $t\in[0,\infty)$, $x\in\mathbb{R}^3$ and $\xi\in\mathbb{R}^3$? Update: Using the cartesian product, does ""$(t,x,\xi)\in[0,\infty)\times\mathbb{R}^3 \times \mathbb{R}^3$"" actually mean \begin{align*} [0,\infty[ \times \mathbb{R}^3\times \mathbb{R}^3= \bigg\{ (t,x,\xi):t&\in[0,\infty[,\\ x&=(x_1,x_2,x_3)\in \mathbb{R}^3,\\  \xi&=(\xi_1,\xi_2,\xi_3)\in \mathbb{R}^3 \bigg\} \quad ? \end{align*} And also, should I write $t\in[0,\infty[$ or $t=[0,\infty[$?","What is the meaning of writing $\times$ here? For $(t,x,\xi)\in[0,\infty)\times\mathbb{R}^3 \times \mathbb{R}^3$ we consider the function $f(t,x,\xi)$. Can't we just write $t\in[0,\infty)$, $x\in\mathbb{R}^3$ and $\xi\in\mathbb{R}^3$? Update: Using the cartesian product, does ""$(t,x,\xi)\in[0,\infty)\times\mathbb{R}^3 \times \mathbb{R}^3$"" actually mean \begin{align*} [0,\infty[ \times \mathbb{R}^3\times \mathbb{R}^3= \bigg\{ (t,x,\xi):t&\in[0,\infty[,\\ x&=(x_1,x_2,x_3)\in \mathbb{R}^3,\\  \xi&=(\xi_1,\xi_2,\xi_3)\in \mathbb{R}^3 \bigg\} \quad ? \end{align*} And also, should I write $t\in[0,\infty[$ or $t=[0,\infty[$?",,"['real-analysis', 'multivariable-calculus', 'notation', 'vector-analysis']"
74,Injectivity and Range of a function,Injectivity and Range of a function,,"Let, $A=\{(x,y)\in \Bbb R^2:x+y\neq -1\}$ Define, $f:A\to \Bbb R^2$ by $f(x,y)=\left(\displaystyle \frac{x}{1+x+y},\frac{y}{1+x+y}\right)$ $(1)$Is $f$ injective on $A$? $(2)$What is the Range of $f$? I started with $f(x,y)=f(u,v)$, then tried to show $(x,y)=(u,v)$. But I could not do it. Can someone help me out for this. Secondly for computing the range, $$\begin{align} f(x,y)=(a,b)\\  \implies \displaystyle \frac{x}{1+x+y}=a,\displaystyle \frac{y}{1+x+y}=b\\ \implies (1-a)x-ay=a\\ -bx+(1-b)y=b\\ \implies x=\displaystyle \frac{a}{1-(a+b)},y=\displaystyle \frac{b}{1-(a+b)} \end{align}$$ So, $f(A)=\Bbb R^2-\{(a,b)\in \Bbb R^2:a+b=1\}$ Is the range correct? If yec can it be obtained in some other way?","Let, $A=\{(x,y)\in \Bbb R^2:x+y\neq -1\}$ Define, $f:A\to \Bbb R^2$ by $f(x,y)=\left(\displaystyle \frac{x}{1+x+y},\frac{y}{1+x+y}\right)$ $(1)$Is $f$ injective on $A$? $(2)$What is the Range of $f$? I started with $f(x,y)=f(u,v)$, then tried to show $(x,y)=(u,v)$. But I could not do it. Can someone help me out for this. Secondly for computing the range, $$\begin{align} f(x,y)=(a,b)\\  \implies \displaystyle \frac{x}{1+x+y}=a,\displaystyle \frac{y}{1+x+y}=b\\ \implies (1-a)x-ay=a\\ -bx+(1-b)y=b\\ \implies x=\displaystyle \frac{a}{1-(a+b)},y=\displaystyle \frac{b}{1-(a+b)} \end{align}$$ So, $f(A)=\Bbb R^2-\{(a,b)\in \Bbb R^2:a+b=1\}$ Is the range correct? If yec can it be obtained in some other way?",,"['analysis', 'multivariable-calculus']"
75,Find an upperbound for the rational function,Find an upperbound for the rational function,,"I know $$\lim_{(x, y)\to (0,0)} \frac{x^3 + y^4}{x^2 + y^2} = 0$$ so $$\left\lvert \frac{x^3 + y^4}{x^2  + y^2} \right\rvert \le f(x, y)$$ for some simpler $f(x, y)$  whose limit is also $0$. How do I find the function $f(x, y)$? In other words, how do I get the upper bound?","I know $$\lim_{(x, y)\to (0,0)} \frac{x^3 + y^4}{x^2 + y^2} = 0$$ so $$\left\lvert \frac{x^3 + y^4}{x^2  + y^2} \right\rvert \le f(x, y)$$ for some simpler $f(x, y)$  whose limit is also $0$. How do I find the function $f(x, y)$? In other words, how do I get the upper bound?",,['multivariable-calculus']
76,An application of Hadamard-Lévy's theorem,An application of Hadamard-Lévy's theorem,,"I was studying the following theorem: Theorem [Hadamard-Lévy]. Let $f\colon \mathbb R^n\to \mathbb R^n$ of class $C^2$. Then $f$ is a $C^1$-diffeomorphism on $\mathbb R^n\to \mathbb R^n$ if, and only if, $f$ is a proper map and the determinant of the Jacobian matrix is non-null everywhere. Do you know any application, any theorem where this is a useful result?","I was studying the following theorem: Theorem [Hadamard-Lévy]. Let $f\colon \mathbb R^n\to \mathbb R^n$ of class $C^2$. Then $f$ is a $C^1$-diffeomorphism on $\mathbb R^n\to \mathbb R^n$ if, and only if, $f$ is a proper map and the determinant of the Jacobian matrix is non-null everywhere. Do you know any application, any theorem where this is a useful result?",,"['real-analysis', 'multivariable-calculus', 'diffeomorphism']"
77,"If a derivative is a linear transformation, and for a linear transformation $f$ at $a$, $f'(a) = f(a)$, why doesn't $f''(a) = f'(a)$?","If a derivative is a linear transformation, and for a linear transformation  at , , why doesn't ?",f a f'(a) = f(a) f''(a) = f'(a),"Before this gets downvoted into oblivion, let me explain what I'm asking. There's a well known theorem that states the following If $f : \mathbb{R}^n \to \mathbb{R}^m$ is a linear transformation, then $f' = f$, where $f'$ is the $m \times n $ matrix of partial derivatives of $f$ (i.e. $f'$ is the derivative of $f$ ) But we know that the derivative itself is a linear transformation $f' : \mathbb{R}^n \to \mathbb{R}^m$, so if we let $g = f'$, then $g' = f'$, and since $g'$ is the second derivative of $f$ (that being $f''$), does that not imply that all second derivatives (or any $n^{th}$ derivative) of any function $f: \mathbb{R}^n \to \mathbb{R}^m$ equals the first derivative $f'$? This obviously cannot be the case, so there must be some error in my interpretation of this theorem. But I'm not sure what my error is. Also I apologize for asking such a silly question such as this.","Before this gets downvoted into oblivion, let me explain what I'm asking. There's a well known theorem that states the following If $f : \mathbb{R}^n \to \mathbb{R}^m$ is a linear transformation, then $f' = f$, where $f'$ is the $m \times n $ matrix of partial derivatives of $f$ (i.e. $f'$ is the derivative of $f$ ) But we know that the derivative itself is a linear transformation $f' : \mathbb{R}^n \to \mathbb{R}^m$, so if we let $g = f'$, then $g' = f'$, and since $g'$ is the second derivative of $f$ (that being $f''$), does that not imply that all second derivatives (or any $n^{th}$ derivative) of any function $f: \mathbb{R}^n \to \mathbb{R}^m$ equals the first derivative $f'$? This obviously cannot be the case, so there must be some error in my interpretation of this theorem. But I'm not sure what my error is. Also I apologize for asking such a silly question such as this.",,"['multivariable-calculus', 'derivatives', 'differential-geometry', 'definition']"
78,How do you derive definite integrals?,How do you derive definite integrals?,,"How do you get $g'(y)$? $$g(y)=\int_{1}^{e^{y^{2}}}\cos\left(\sqrt{\log(x)}\right)\,dx$$","How do you get $g'(y)$? $$g(y)=\int_{1}^{e^{y^{2}}}\cos\left(\sqrt{\log(x)}\right)\,dx$$",,"['multivariable-calculus', 'definite-integrals']"
79,"Calc 3, multiple integral and variable change in $\iint y^{-1}\,dx\,dy$.","Calc 3, multiple integral and variable change in .","\iint y^{-1}\,dx\,dy","Sketch the domain $D$ bounded by $y=x^2,\ y=(1/2)x^2,$ and $y=2x$. Use a change of variables with the map $x=uv,\ y=u^2$ to calculate:    $$\iint_D y^{-1}\,dx\,dy.$$ I believe the Jacobian is $v-2(u^2)$ and the integrand is the Jacobian multiplied by $u^{-2}$. I don't know how to find the bounds. Some tries resulted in the $u$ bounds being $0$ to $2v$ and the $v$ bounds being $1$ to $\sqrt{2}$ but I could not solve the integral that way. Any help at all would be appreciated.","Sketch the domain $D$ bounded by $y=x^2,\ y=(1/2)x^2,$ and $y=2x$. Use a change of variables with the map $x=uv,\ y=u^2$ to calculate:    $$\iint_D y^{-1}\,dx\,dy.$$ I believe the Jacobian is $v-2(u^2)$ and the integrand is the Jacobian multiplied by $u^{-2}$. I don't know how to find the bounds. Some tries resulted in the $u$ bounds being $0$ to $2v$ and the $v$ bounds being $1$ to $\sqrt{2}$ but I could not solve the integral that way. Any help at all would be appreciated.",,"['multivariable-calculus', 'multiple-integral']"
80,Derivative of definite integral of function of two variables,Derivative of definite integral of function of two variables,,"I know that the fundamental theorem of calculus says that if $f(t)$ is differentiable on the interval $(a,x)$ for some constant $a$ and variable $x$ then we have \begin{equation}\frac{d}{dx}\int_a^x f(t)dt = f(x).\end{equation} What I am wondering is, what happens when a function depends on $x$ as well? So, for example, $g: \mathbb{R}^2 \to \mathbb{R}$. For concreteness perhaps say $g(x,t) = f(x-t)$. Then how do I compute \begin{equation}\frac{d}{dx}\int_a^x g(x,t)dt?\end{equation}","I know that the fundamental theorem of calculus says that if $f(t)$ is differentiable on the interval $(a,x)$ for some constant $a$ and variable $x$ then we have \begin{equation}\frac{d}{dx}\int_a^x f(t)dt = f(x).\end{equation} What I am wondering is, what happens when a function depends on $x$ as well? So, for example, $g: \mathbb{R}^2 \to \mathbb{R}$. For concreteness perhaps say $g(x,t) = f(x-t)$. Then how do I compute \begin{equation}\frac{d}{dx}\int_a^x g(x,t)dt?\end{equation}",,"['calculus', 'integration', 'multivariable-calculus', 'derivatives']"
81,What is the relation between convexity and positive semi-definite Hessian Matrix?,What is the relation between convexity and positive semi-definite Hessian Matrix?,,"While studying maximum likelihood estimation for linear regression, I followed all the contents and mathematical steps and understand the way steps were derived. At the end of the context, the book says we take the second derivative to see if the Hessian Matrix of the result of the second derivative is the minimum, so we need the Hessian matrix to be positive semi-definite to guarantee a unique minimum. But I don't understand the meaning of positive semi-definite to be the minimum. And it also says convexity, but I am not really getting it. It seems like the reason I ask is that I cannot match the picture between the Hessian positive semi-definite matrix and the convexity. Hope to hear some intuitive explanations considering I am not a real math person but trying to learn more.","While studying maximum likelihood estimation for linear regression, I followed all the contents and mathematical steps and understand the way steps were derived. At the end of the context, the book says we take the second derivative to see if the Hessian Matrix of the result of the second derivative is the minimum, so we need the Hessian matrix to be positive semi-definite to guarantee a unique minimum. But I don't understand the meaning of positive semi-definite to be the minimum. And it also says convexity, but I am not really getting it. It seems like the reason I ask is that I cannot match the picture between the Hessian positive semi-definite matrix and the convexity. Hope to hear some intuitive explanations considering I am not a real math person but trying to learn more.",,"['calculus', 'algebra-precalculus', 'multivariable-calculus', 'optimization', 'hessian-matrix']"
82,"Spivak's Manifold, Theorem 2.7, proof without chain rule.","Spivak's Manifold, Theorem 2.7, proof without chain rule.",,"For the following theorem: Spivak stated, Although the chain rule was used ... it could easily have been eliminated. I believe there is a proof without invoking the auxiliary function $h$ - but I don't see how we can show $D_jf(a)$ exists, moreover, even if it exists, I couldn't show the derivative of $f$ is of the desired form without using MVT...","For the following theorem: Spivak stated, Although the chain rule was used ... it could easily have been eliminated. I believe there is a proof without invoking the auxiliary function - but I don't see how we can show exists, moreover, even if it exists, I couldn't show the derivative of is of the desired form without using MVT...",h D_jf(a) f,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'jacobian']"
83,Verification of $\partial_{\boldsymbol a} \lVert \boldsymbol P(\boldsymbol a) - \boldsymbol b \rVert^2$,Verification of,\partial_{\boldsymbol a} \lVert \boldsymbol P(\boldsymbol a) - \boldsymbol b \rVert^2,"The problem I'd like to ask if I differentiated the following function $F(\boldsymbol a)$ correctly: $$ F(\boldsymbol a) = \lVert \boldsymbol P(\boldsymbol a) - \boldsymbol b \rVert^2 \\ \frac{\partial F}{\partial \boldsymbol a} = \ ? $$ where $$\begin{align} \boldsymbol a, \boldsymbol b &\in \mathbb R^{3}\\ \boldsymbol P&: \mathbb R^{3} \to \mathbb R^{3}\\ F&: \mathbb R^{3} \to \mathbb R \end{align} $$ and $\lVert \cdot\rVert^2$ is the squared norm (dot product). My attempt to solution I proceeded by writing the dot product explicitly and differentiating w.r.t. each of the 3 components of $\boldsymbol a$: $$\begin{align} F(\boldsymbol a) &= (\boldsymbol P_1(\boldsymbol a) - b_1)^2 +  (\boldsymbol P_2(\boldsymbol a) - b_2)^2 + (\boldsymbol P_3(\boldsymbol a) - b_3)^2\\ % % \frac{\partial F}{\partial \boldsymbol a}   &= \begin{bmatrix}   \partial_{\boldsymbol a1}\ F(\boldsymbol a)\\   \partial_{\boldsymbol a2}\ F(\boldsymbol a)\\   \partial_{\boldsymbol a3}\ F(\boldsymbol a) \end{bmatrix} \end{align}$$ where $b_i$ is the $i$-th component of $\boldsymbol b$ (analogously for $\boldsymbol a$), and $\boldsymbol P_i(\boldsymbol a)$ is the $i$-th component of the vector $\boldsymbol P(\boldsymbol a)$. The partial derivative of $F$ by $\boldsymbol a$'s component $i$: $$\begin{align} \partial_{\boldsymbol ai}\ F(\boldsymbol a) &= 2(\boldsymbol P_i(\boldsymbol a) - b_i)\cdot \frac{\partial \boldsymbol P_i}{\partial \boldsymbol a}(\boldsymbol a)\cdot \frac{\partial \boldsymbol a}{\partial a_i}(\boldsymbol a)\\ &= 2(\boldsymbol P_i(\boldsymbol a) - b_i)\cdot \frac{\partial \boldsymbol P_i}{\partial a_i}(\boldsymbol a) \end{align}$$ I could then rewrite $\frac{\partial F}{\partial \boldsymbol a}$ as: $$\begin{align} \frac{\partial F}{\partial \boldsymbol a} = 2\boldsymbol M(\boldsymbol P(\boldsymbol a) - \boldsymbol b) \end{align}$$ where $\boldsymbol M$ is the diagonal-matrix created from $\nabla \boldsymbol P(\boldsymbol a)$.","The problem I'd like to ask if I differentiated the following function $F(\boldsymbol a)$ correctly: $$ F(\boldsymbol a) = \lVert \boldsymbol P(\boldsymbol a) - \boldsymbol b \rVert^2 \\ \frac{\partial F}{\partial \boldsymbol a} = \ ? $$ where $$\begin{align} \boldsymbol a, \boldsymbol b &\in \mathbb R^{3}\\ \boldsymbol P&: \mathbb R^{3} \to \mathbb R^{3}\\ F&: \mathbb R^{3} \to \mathbb R \end{align} $$ and $\lVert \cdot\rVert^2$ is the squared norm (dot product). My attempt to solution I proceeded by writing the dot product explicitly and differentiating w.r.t. each of the 3 components of $\boldsymbol a$: $$\begin{align} F(\boldsymbol a) &= (\boldsymbol P_1(\boldsymbol a) - b_1)^2 +  (\boldsymbol P_2(\boldsymbol a) - b_2)^2 + (\boldsymbol P_3(\boldsymbol a) - b_3)^2\\ % % \frac{\partial F}{\partial \boldsymbol a}   &= \begin{bmatrix}   \partial_{\boldsymbol a1}\ F(\boldsymbol a)\\   \partial_{\boldsymbol a2}\ F(\boldsymbol a)\\   \partial_{\boldsymbol a3}\ F(\boldsymbol a) \end{bmatrix} \end{align}$$ where $b_i$ is the $i$-th component of $\boldsymbol b$ (analogously for $\boldsymbol a$), and $\boldsymbol P_i(\boldsymbol a)$ is the $i$-th component of the vector $\boldsymbol P(\boldsymbol a)$. The partial derivative of $F$ by $\boldsymbol a$'s component $i$: $$\begin{align} \partial_{\boldsymbol ai}\ F(\boldsymbol a) &= 2(\boldsymbol P_i(\boldsymbol a) - b_i)\cdot \frac{\partial \boldsymbol P_i}{\partial \boldsymbol a}(\boldsymbol a)\cdot \frac{\partial \boldsymbol a}{\partial a_i}(\boldsymbol a)\\ &= 2(\boldsymbol P_i(\boldsymbol a) - b_i)\cdot \frac{\partial \boldsymbol P_i}{\partial a_i}(\boldsymbol a) \end{align}$$ I could then rewrite $\frac{\partial F}{\partial \boldsymbol a}$ as: $$\begin{align} \frac{\partial F}{\partial \boldsymbol a} = 2\boldsymbol M(\boldsymbol P(\boldsymbol a) - \boldsymbol b) \end{align}$$ where $\boldsymbol M$ is the diagonal-matrix created from $\nabla \boldsymbol P(\boldsymbol a)$.",,"['multivariable-calculus', 'derivatives', 'vectors', 'matrix-calculus']"
84,Integral of a gradient over a plane area,Integral of a gradient over a plane area,,"Let $A$ be a plane area bounded by a curve $\partial A$. Then, is $$ \iint_A \nabla f\, \textrm{d}x \textrm{d}y = \oint_{\partial A} f\ \hat{\mathbf{n}}\ dl $$ where $f=f(x,y)$ and $\mathbf{\hat n}$ is the outward unit normal?","Let $A$ be a plane area bounded by a curve $\partial A$. Then, is $$ \iint_A \nabla f\, \textrm{d}x \textrm{d}y = \oint_{\partial A} f\ \hat{\mathbf{n}}\ dl $$ where $f=f(x,y)$ and $\mathbf{\hat n}$ is the outward unit normal?",,"['calculus', 'integration', 'multivariable-calculus']"
85,Multivariable calculus chain rule and change of variables,Multivariable calculus chain rule and change of variables,,"I am considering the following question Transform the expression $\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}$ into one in $\rho$ and $\phi$ . Now my problem has arisen when considering $\frac{\partial \rho}{\partial x}$ . From considering $x=\rho cos(\phi)$ , I obtain $\frac{\partial x}{\partial \rho} = cos(\phi)$ so then $\frac{\partial \rho}{\partial x} = \frac{1}{cos(\phi)}$ And yet in the book I am looking at, they instead used $\rho = (x^2+y^2)^{1/2}$ to obtain $\frac{\partial \rho}{\partial x} = \frac{x}{(x^2+y^2)^{1/2}}=\frac{\rho cos(\phi)}{\rho} = cos{\phi}$ !!! It seems that maybe where I am going wrong is by not accounting for $y$ in the partial derivative of $\rho$ . However I don't understand why I should take it into account. As I am taking the partial derivative, and I not just taking the derivative where there is an $x$ explicitly stated? (Although I do see a bit of a problem using my 'rule', because if I were to take the other expression for $\rho$ as $y=\rho sin(\theta)$ , then I would get $\frac{\partial \rho}{\partial x} = 0$ . Perhpas I misunderstood the partial derivative as only taking the derivative when a variable is explicitly stated....","I am considering the following question Transform the expression into one in and . Now my problem has arisen when considering . From considering , I obtain so then And yet in the book I am looking at, they instead used to obtain !!! It seems that maybe where I am going wrong is by not accounting for in the partial derivative of . However I don't understand why I should take it into account. As I am taking the partial derivative, and I not just taking the derivative where there is an explicitly stated? (Although I do see a bit of a problem using my 'rule', because if I were to take the other expression for as , then I would get . Perhpas I misunderstood the partial derivative as only taking the derivative when a variable is explicitly stated....",\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} \rho \phi \frac{\partial \rho}{\partial x} x=\rho cos(\phi) \frac{\partial x}{\partial \rho} = cos(\phi) \frac{\partial \rho}{\partial x} = \frac{1}{cos(\phi)} \rho = (x^2+y^2)^{1/2} \frac{\partial \rho}{\partial x} = \frac{x}{(x^2+y^2)^{1/2}}=\frac{\rho cos(\phi)}{\rho} = cos{\phi} y \rho x \rho y=\rho sin(\theta) \frac{\partial \rho}{\partial x} = 0,"['multivariable-calculus', 'partial-derivative', 'polar-coordinates', 'chain-rule', 'change-of-variable']"
86,Calculating area of sphere with constraint on zenith,Calculating area of sphere with constraint on zenith,,"Let $\mathbb S_R$ be the sphere of radius $R$ centered about the origin. Consider $$A_R= \left\{ (x,y,z)\in \mathbb S_R \mid x^2+y^2+(z-R)^2\leq 1 \right\}.$$ I want to calculate the area of this region of the sphere with radius $R$ about the origin. I already calculated the area of the region of $\mathbb S_R$ of vectors with zenith $\leq \alpha$ is given by $2\pi R^2(1-\cos \alpha)$. For $A_R$, here's my idea. I want to find the zenith $\alpha$ which satisfies $R\sin \alpha=\sin \alpha+R$, since this is the zenith of points of both $\mathbb S_R$ and the unit sphere translated up $R$ units at points which are of the same height. Then, I just want to integrate the zenith $0\leq \phi\leq \alpha$. Solving gives $\phi=\arcsin \frac R{R-1}$ and things get a little messy. On the other hand, another student posted a solution which makes sense. He just writes $\alpha$ should satisfy $2R^2-2R^2\cos\alpha=1$ and then obtains the area of $A_R$ is $\pi$, independently of the radius. Why is my method wrong? Picture","Let $\mathbb S_R$ be the sphere of radius $R$ centered about the origin. Consider $$A_R= \left\{ (x,y,z)\in \mathbb S_R \mid x^2+y^2+(z-R)^2\leq 1 \right\}.$$ I want to calculate the area of this region of the sphere with radius $R$ about the origin. I already calculated the area of the region of $\mathbb S_R$ of vectors with zenith $\leq \alpha$ is given by $2\pi R^2(1-\cos \alpha)$. For $A_R$, here's my idea. I want to find the zenith $\alpha$ which satisfies $R\sin \alpha=\sin \alpha+R$, since this is the zenith of points of both $\mathbb S_R$ and the unit sphere translated up $R$ units at points which are of the same height. Then, I just want to integrate the zenith $0\leq \phi\leq \alpha$. Solving gives $\phi=\arcsin \frac R{R-1}$ and things get a little messy. On the other hand, another student posted a solution which makes sense. He just writes $\alpha$ should satisfy $2R^2-2R^2\cos\alpha=1$ and then obtains the area of $A_R$ is $\pi$, independently of the radius. Why is my method wrong? Picture",,"['integration', 'multivariable-calculus', 'definite-integrals', 'spherical-coordinates', 'surface-integrals']"
87,Does the potential exist?,Does the potential exist?,,"Verify Green's theorem for $X(x,y)=(xy^2,-yx^2)$ in the circle of radius $R$ with center $(0,0)$. I think there is a mistakein the field. I suppose the first thing I should do is to find a function $F$ such that $\nabla F= X$. I don't think it is possible: $\displaystyle{\int xy^2 = \frac{1}{2}x^2y^2+g(y)}$ and then it should be $\displaystyle{\frac{\partial}{\partial y}\left(\frac{1}{2}x^2y^2+g(y)\right)}=-yx^2$ but $\displaystyle{\frac{\partial}{\partial y}\left(\frac{1}{2}x^2y^2+g(y)\right)=yx^2+g'(y)}$ since $g$ only has the $y$ variable is impossible to get $-yx^2$","Verify Green's theorem for $X(x,y)=(xy^2,-yx^2)$ in the circle of radius $R$ with center $(0,0)$. I think there is a mistakein the field. I suppose the first thing I should do is to find a function $F$ such that $\nabla F= X$. I don't think it is possible: $\displaystyle{\int xy^2 = \frac{1}{2}x^2y^2+g(y)}$ and then it should be $\displaystyle{\frac{\partial}{\partial y}\left(\frac{1}{2}x^2y^2+g(y)\right)}=-yx^2$ but $\displaystyle{\frac{\partial}{\partial y}\left(\frac{1}{2}x^2y^2+g(y)\right)=yx^2+g'(y)}$ since $g$ only has the $y$ variable is impossible to get $-yx^2$",,"['multivariable-calculus', 'vector-spaces', 'greens-theorem']"
88,Limit of multivariable $\frac{x^2+y^2}{ x^4+y^4}$,Limit of multivariable,\frac{x^2+y^2}{ x^4+y^4},"Find the limit $$\lim_{(x, y) \to (0,0)} \frac {x^2+y^2 }{x^4+y^4 }$$ This limit does not exists since when convert it into polar we get $\frac {1 }{ r^2 (1-2 \sin^2 \theta   \cos^2 \theta)}$  which is one over zero right?","Find the limit $$\lim_{(x, y) \to (0,0)} \frac {x^2+y^2 }{x^4+y^4 }$$ This limit does not exists since when convert it into polar we get $\frac {1 }{ r^2 (1-2 \sin^2 \theta   \cos^2 \theta)}$  which is one over zero right?",,"['limits', 'functions', 'multivariable-calculus']"
89,"Suppose $\lim_{h\to 0}\frac{r(h)}{h} = 0$, does it imply $\lim_{h\to 0} r(h) = 0$?","Suppose , does it imply ?",\lim_{h\to 0}\frac{r(h)}{h} = 0 \lim_{h\to 0} r(h) = 0,"Suppose $\lim\limits_{h\to 0}\frac{r(h)}{h} = 0$, does it imply $\lim\limits_{h\to 0} r(h) = 0$? My reasoning says that the answer is yes. So if $\frac{r(h)}{h} = 0$, then nothing to prove. If $\frac{r(h)}{h} = a(h)$ where $\lim\limits_{h\to 0} a(h) = 0$, then $r(h) = a(h)h$, hence $$\lim_{h\to 0} r(h) = \lim_{h\to 0} [a(h)h] = \lim_{h\to 0} [a(h)] \cdot \lim_{h\to 0} h$$  (since $\lim ab=\lim a \cdot \lim b$). And therefore $\lim\limits_{h\to 0} r(h) = 0$. But if this is true, then why in the definition of differentiation we write $$\lim_{h\to 0} \frac{|f(x+h) - f(x) - Df(a)(h)|}{|h|} = 0$$ instead of  $$\lim_{h\to 0} |f(x+h) - f(x) - Df(a)(h)| = 0?$$","Suppose $\lim\limits_{h\to 0}\frac{r(h)}{h} = 0$, does it imply $\lim\limits_{h\to 0} r(h) = 0$? My reasoning says that the answer is yes. So if $\frac{r(h)}{h} = 0$, then nothing to prove. If $\frac{r(h)}{h} = a(h)$ where $\lim\limits_{h\to 0} a(h) = 0$, then $r(h) = a(h)h$, hence $$\lim_{h\to 0} r(h) = \lim_{h\to 0} [a(h)h] = \lim_{h\to 0} [a(h)] \cdot \lim_{h\to 0} h$$  (since $\lim ab=\lim a \cdot \lim b$). And therefore $\lim\limits_{h\to 0} r(h) = 0$. But if this is true, then why in the definition of differentiation we write $$\lim_{h\to 0} \frac{|f(x+h) - f(x) - Df(a)(h)|}{|h|} = 0$$ instead of  $$\lim_{h\to 0} |f(x+h) - f(x) - Df(a)(h)| = 0?$$",,"['calculus', 'limits', 'multivariable-calculus']"
90,Double integral; changing order of integration,Double integral; changing order of integration,,I'm attempting some exercises on double integration in Schaum's Outline of Calculus. The integral is $$I = \int_{y=1}^{2}\int_{x=0}^{y^{3/2}} \frac{x}{y^2}\ dx\ dy.$$ I can do it in this direction and it turns out to be $I = \frac{3}{4}$ but for some reason I'm struggling to swap the order of integration and I think I may be confused. I presume it'll be in the form of $$I = \int_{x = \text{const}}^\text{const}\int_{y = \text{const}}^{y(x)} \frac{x}{y^2}\ dy \ dx$$,I'm attempting some exercises on double integration in Schaum's Outline of Calculus. The integral is $$I = \int_{y=1}^{2}\int_{x=0}^{y^{3/2}} \frac{x}{y^2}\ dx\ dy.$$ I can do it in this direction and it turns out to be $I = \frac{3}{4}$ but for some reason I'm struggling to swap the order of integration and I think I may be confused. I presume it'll be in the form of $$I = \int_{x = \text{const}}^\text{const}\int_{y = \text{const}}^{y(x)} \frac{x}{y^2}\ dy \ dx$$,,"['integration', 'multivariable-calculus']"
91,Finding the Tangent Line to a Surface,Finding the Tangent Line to a Surface,,"What is the equation of the tangent line to the intersection of the surface $z = \arctan (xy)$ with the plane $x=2$, at the point $(2,\frac{1}{2}, \frac{\pi}{4})$ The intersection of $x=2$ and $z= \arctan (xy)$ produces the curve $z = \arctan (2y)$ in the $yz$-plane. Thus, the partial derivative is $\frac{\partial z }{\partial y} = \frac{\partial z }{\partial y} \arctan(2y) = \frac{2}{1 +4y^2}$ and the slope of the line passing through $y=1/2$ is $\frac{\partial z }{\partial y} = 1$ My first question is , is there a conceptual/logical error in plugging the $x=2$ before taking the (partial) derivative. In the book I am using for review, the author computes the partial derivative of $z= \arctan (xy)$ wrt $y$ and then plugs in the point $(2, \frac{1}{2})$. I realize that I obtained the same answer, but that does not necessarily imply it's a valid way of solving the problem. Now, $\frac{\partial z }{\partial y} = 1$ gives us the slope of the 2-D version of the line whose equation we interested in finding. Having a little trouble determining the equation, I consulted the book and this is what it says: ""Since tangent line is in the plane $x=2$, this calculation [namely, the calculation of $\frac{\partial z }{\partial y}$] shows that the line is parallel to the vector $v = (0,1,1)$."" I don't see how it follows from $\frac{\partial z}{\partial y} = 1$ that the ""slope"" vector (not exactly sure what it is called) is $v=(0,1,1)$.","What is the equation of the tangent line to the intersection of the surface $z = \arctan (xy)$ with the plane $x=2$, at the point $(2,\frac{1}{2}, \frac{\pi}{4})$ The intersection of $x=2$ and $z= \arctan (xy)$ produces the curve $z = \arctan (2y)$ in the $yz$-plane. Thus, the partial derivative is $\frac{\partial z }{\partial y} = \frac{\partial z }{\partial y} \arctan(2y) = \frac{2}{1 +4y^2}$ and the slope of the line passing through $y=1/2$ is $\frac{\partial z }{\partial y} = 1$ My first question is , is there a conceptual/logical error in plugging the $x=2$ before taking the (partial) derivative. In the book I am using for review, the author computes the partial derivative of $z= \arctan (xy)$ wrt $y$ and then plugs in the point $(2, \frac{1}{2})$. I realize that I obtained the same answer, but that does not necessarily imply it's a valid way of solving the problem. Now, $\frac{\partial z }{\partial y} = 1$ gives us the slope of the 2-D version of the line whose equation we interested in finding. Having a little trouble determining the equation, I consulted the book and this is what it says: ""Since tangent line is in the plane $x=2$, this calculation [namely, the calculation of $\frac{\partial z }{\partial y}$] shows that the line is parallel to the vector $v = (0,1,1)$."" I don't see how it follows from $\frac{\partial z}{\partial y} = 1$ that the ""slope"" vector (not exactly sure what it is called) is $v=(0,1,1)$.",,['multivariable-calculus']
92,how to differentiate skew symmetric matrix $[\mathbf{v}]_{\times}$ with respect to $\mathbf{v}$,how to differentiate skew symmetric matrix  with respect to,[\mathbf{v}]_{\times} \mathbf{v},"can anyone explain how to differentiate a skew-symmetric matrix $\mathbf{v}_{\times}$ with respect to $\mathbf{v}$ i.e. $\frac{\partial{[\mathbf{v}]_{\times}}}{\partial \mathbf{v}}$ where $\mathbf{v}\in\mathbb{R}^3$? In addition, how to derive $\frac{\partial [\mathbf{v}]^2_{\times}}{\partial \mathbf{v}}$? Thank you!","can anyone explain how to differentiate a skew-symmetric matrix $\mathbf{v}_{\times}$ with respect to $\mathbf{v}$ i.e. $\frac{\partial{[\mathbf{v}]_{\times}}}{\partial \mathbf{v}}$ where $\mathbf{v}\in\mathbb{R}^3$? In addition, how to derive $\frac{\partial [\mathbf{v}]^2_{\times}}{\partial \mathbf{v}}$? Thank you!",,"['multivariable-calculus', 'vectors', 'vector-analysis']"
93,How can a linear operator represents the total derivative?,How can a linear operator represents the total derivative?,,"I am now studying total derivative of a function. The defination of the differentiability of the vector-valued function $\mathbf{f}$  is given in my book as follows : Let $\mathbf{f} : S \longrightarrow \mathbb R^{m}$ be a function defined on a set $S \subset \mathbb R^{n}$ with values in $\mathbb R^{m}$.Let $\mathbf{c}$ be an interior point of $S$, and let $B(\mathbf{c}; r)$ be an $n$-ball lying in $S$. Let $\mathbf{v}$ be a point in $\mathbb R^{n}$ with $||\mathbf{v}|| < r$, so that $\mathbf{c} + \mathbf{v} \in B(\mathbf{c}; r)$.Then the function $\mathbf{f}$ is said to be differentiable at $\mathbf{c}$ if there exists a linear operator $T_{\mathbf{c}} : \mathbb R^{n} \longrightarrow \mathbb R^{m}$ such that $\mathbf{f}(\mathbf{c} + \mathbf{v}) = \mathbf{f}(\mathbf{c}) + T_{\mathbf{c}}(\mathbf{v}) + ||\mathbf{v}|| E_{\mathbf{c}}(\mathbf{v})$ , where $E_{\mathbf{c}}(\mathbf{v}) \rightarrow \mathbf{0}$ as $\mathbf{v} \rightarrow \mathbf{0}$.The linear function $T_{\mathbf{c}}$ is called the total derivative of $\mathbf{f}$ at $\mathbf{c}$. But my question is ''how can the total derivative be linear operator?''In particular if $f$ be a real valued function of real variable then if $f$ is differentiable at $c$ then I have a question. Is $f'(c)$ the total derivative of $f$ at $c$? If the answer is affirmative then how is the real number $f'(c)$ considered to be linear operator i.e. to be function.Please help me in understanding this concept. Thank you in advance.","I am now studying total derivative of a function. The defination of the differentiability of the vector-valued function $\mathbf{f}$  is given in my book as follows : Let $\mathbf{f} : S \longrightarrow \mathbb R^{m}$ be a function defined on a set $S \subset \mathbb R^{n}$ with values in $\mathbb R^{m}$.Let $\mathbf{c}$ be an interior point of $S$, and let $B(\mathbf{c}; r)$ be an $n$-ball lying in $S$. Let $\mathbf{v}$ be a point in $\mathbb R^{n}$ with $||\mathbf{v}|| < r$, so that $\mathbf{c} + \mathbf{v} \in B(\mathbf{c}; r)$.Then the function $\mathbf{f}$ is said to be differentiable at $\mathbf{c}$ if there exists a linear operator $T_{\mathbf{c}} : \mathbb R^{n} \longrightarrow \mathbb R^{m}$ such that $\mathbf{f}(\mathbf{c} + \mathbf{v}) = \mathbf{f}(\mathbf{c}) + T_{\mathbf{c}}(\mathbf{v}) + ||\mathbf{v}|| E_{\mathbf{c}}(\mathbf{v})$ , where $E_{\mathbf{c}}(\mathbf{v}) \rightarrow \mathbf{0}$ as $\mathbf{v} \rightarrow \mathbf{0}$.The linear function $T_{\mathbf{c}}$ is called the total derivative of $\mathbf{f}$ at $\mathbf{c}$. But my question is ''how can the total derivative be linear operator?''In particular if $f$ be a real valued function of real variable then if $f$ is differentiable at $c$ then I have a question. Is $f'(c)$ the total derivative of $f$ at $c$? If the answer is affirmative then how is the real number $f'(c)$ considered to be linear operator i.e. to be function.Please help me in understanding this concept. Thank you in advance.",,"['multivariable-calculus', 'derivatives']"
94,"Finding volume of a solid over region E, bounded below by cone $z = \sqrt{x^2 + y^2}$ and above by sphere $ z^2 = 1- x^2 - y^2$","Finding volume of a solid over region E, bounded below by cone  and above by sphere",z = \sqrt{x^2 + y^2}  z^2 = 1- x^2 - y^2,I set up the triple integral as follows: $v=\int^1_0\int^{\sqrt{x-1}}_{-\sqrt{x}}\int^{\sqrt{1-x^2-y^2}}_{\sqrt{x^2+y^2}}(1)dzdydx$ Then went to polar coords getting $$\int^{2\pi}_0\int_0^1(\sqrt{1-r^2}r=r^2)drd\theta$$$$2\pi(\frac{-1}{2})[\frac{2}{3}(1-r^2)^{3/2}-1]|^1_0=-\pi[(0-1)-(\frac{2}{3}-1)]=\pi+\frac{1}{3}$$ The answer should be $\frac{\pi}{3}(2-\sqrt{2})$,I set up the triple integral as follows: $v=\int^1_0\int^{\sqrt{x-1}}_{-\sqrt{x}}\int^{\sqrt{1-x^2-y^2}}_{\sqrt{x^2+y^2}}(1)dzdydx$ Then went to polar coords getting $$\int^{2\pi}_0\int_0^1(\sqrt{1-r^2}r=r^2)drd\theta$$$$2\pi(\frac{-1}{2})[\frac{2}{3}(1-r^2)^{3/2}-1]|^1_0=-\pi[(0-1)-(\frac{2}{3}-1)]=\pi+\frac{1}{3}$$ The answer should be $\frac{\pi}{3}(2-\sqrt{2})$,,"['integration', 'multivariable-calculus', 'polar-coordinates']"
95,There is no diffeomorphism between one quadrant in the plane and the half plane,There is no diffeomorphism between one quadrant in the plane and the half plane,,"I am trying to prove rigourosly that the unit square $[0,1]\times [0,1]\subset \mathbb R^2$ is not a differentiable manifold with boundary (I've searched for a rigouros proof, but haven't found anyone) Corners are obviously the problem, so it is reduced to check that otherwise, there would be some open subset around, for example, $(0,0)$, diffeomorphic to $H = \{(x,y)\in\mathbb R^2 : y \geq 0\}$. How can I prove easily that $Q = \{(x,y)\in\mathbb R^2 : x\geq 0, y\geq 0 \}$ is not diffeomorphic to $H$? Any hint? Thank you in advance.","I am trying to prove rigourosly that the unit square $[0,1]\times [0,1]\subset \mathbb R^2$ is not a differentiable manifold with boundary (I've searched for a rigouros proof, but haven't found anyone) Corners are obviously the problem, so it is reduced to check that otherwise, there would be some open subset around, for example, $(0,0)$, diffeomorphic to $H = \{(x,y)\in\mathbb R^2 : y \geq 0\}$. How can I prove easily that $Q = \{(x,y)\in\mathbb R^2 : x\geq 0, y\geq 0 \}$ is not diffeomorphic to $H$? Any hint? Thank you in advance.",,"['multivariable-calculus', 'differential-topology']"
96,Solve the definite integral of $\sin(\pi x^2) $,Solve the definite integral of,\sin(\pi x^2) ,"Compute $$\int_{y=0}^{y=1} \int_{x=y}^{x=1} \sin(\pi x^2) \;dx \;dy.$$ Wolfram returns this definite integral as $1/{\pi}$. However, I am struggling to figure out the solution to this integral, as I am unaware of any trig identity, substitution, or integration by parts that would apply to  $\sin(\pi x^2)$. In trying to applying Fubini's Theorem, you would still end up with the same integrand, so it does not seem to help me in this case. Any help or guidance appreciated.","Compute $$\int_{y=0}^{y=1} \int_{x=y}^{x=1} \sin(\pi x^2) \;dx \;dy.$$ Wolfram returns this definite integral as $1/{\pi}$. However, I am struggling to figure out the solution to this integral, as I am unaware of any trig identity, substitution, or integration by parts that would apply to  $\sin(\pi x^2)$. In trying to applying Fubini's Theorem, you would still end up with the same integrand, so it does not seem to help me in this case. Any help or guidance appreciated.",,"['real-analysis', 'multivariable-calculus']"
97,Integral with $sin^3(\theta)$,Integral with,sin^3(\theta),"Me and one other person got this problem from a pdf online, $\int^{\pi/4}_0sin^3(\theta)d\theta$ The answer in the pdf is $2-\frac{5}{2\sqrt{2}}$ with a decimal value around $0.232233$. All of us are getting and answer that comes out to $0.07741$ or $(\frac{1}{6\sqrt{2}}-\frac{1}{\sqrt{2}})+\frac{2}{3}$. We changed it to $sin^2sin$ and used $sin^2=1-cos$ and the then $u = cos$ and $du = -sin$to cancel out the remaining sin. And after doing the integration got $[\frac{cos^3(\theta)}{3}-cos(\theta)]^{\pi/4}_0$ and then our final answer of  $(\frac{1}{6\sqrt{2}}-\frac{1}{\sqrt{2}})+\frac{2}{3}$. We can't figure out hwo they got the other answer.","Me and one other person got this problem from a pdf online, $\int^{\pi/4}_0sin^3(\theta)d\theta$ The answer in the pdf is $2-\frac{5}{2\sqrt{2}}$ with a decimal value around $0.232233$. All of us are getting and answer that comes out to $0.07741$ or $(\frac{1}{6\sqrt{2}}-\frac{1}{\sqrt{2}})+\frac{2}{3}$. We changed it to $sin^2sin$ and used $sin^2=1-cos$ and the then $u = cos$ and $du = -sin$to cancel out the remaining sin. And after doing the integration got $[\frac{cos^3(\theta)}{3}-cos(\theta)]^{\pi/4}_0$ and then our final answer of  $(\frac{1}{6\sqrt{2}}-\frac{1}{\sqrt{2}})+\frac{2}{3}$. We can't figure out hwo they got the other answer.",,"['integration', 'multivariable-calculus']"
98,"Think of a function $R^2\rightarrow R$ satisfies $f_x>0,f_y>0,f_{xy}<0$",Think of a function  satisfies,"R^2\rightarrow R f_x>0,f_y>0,f_{xy}<0","Think of a function $R^2\rightarrow R$ satisfies that there exists some point where each partial derivative is positive and mixed partial derivative is negative? $$f_x>0,f_y>0,f_{xy}<0$$","Think of a function $R^2\rightarrow R$ satisfies that there exists some point where each partial derivative is positive and mixed partial derivative is negative? $$f_x>0,f_y>0,f_{xy}<0$$",,['multivariable-calculus']
99,A double integral with functions as bounds: $\int_0^1 dx \int_{\frac{\sqrt[3]{x}}2}^{\sqrt[3]{x}} \sqrt{1-y^4}dy$,A double integral with functions as bounds:,\int_0^1 dx \int_{\frac{\sqrt[3]{x}}2}^{\sqrt[3]{x}} \sqrt{1-y^4}dy,"So I have the integral $$\int_0^1 dx \int_{\frac{\sqrt[3]{x}}{2}}^{\sqrt[3]{x}} \sqrt{1-y^4}dy.$$ How on earth do I do this integral? I used WolframAlpha to have a look at what $\sqrt{1-y^4}$ integrates to, and it's absurd. I've tried quite a few different tricks and substitutions, as well as swapping the order which doesn't change anything, I feel like I'm missing something really obvious. Thanks! Edit: This was a practice question for a maths course I'm currently in. Turns out our lecturer chose the wrong bounds for what he had in mind - the solution for it is certainly outside the scope of the course!","So I have the integral $$\int_0^1 dx \int_{\frac{\sqrt[3]{x}}{2}}^{\sqrt[3]{x}} \sqrt{1-y^4}dy.$$ How on earth do I do this integral? I used WolframAlpha to have a look at what $\sqrt{1-y^4}$ integrates to, and it's absurd. I've tried quite a few different tricks and substitutions, as well as swapping the order which doesn't change anything, I feel like I'm missing something really obvious. Thanks! Edit: This was a practice question for a maths course I'm currently in. Turns out our lecturer chose the wrong bounds for what he had in mind - the solution for it is certainly outside the scope of the course!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'closed-form']"
