,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to find the derivative to this equation?,How to find the derivative to this equation?,,"Can someone please explain the steps to find the derivative of: $$\dfrac1{2(\sqrt[3]{x^2})}$$ I know the answer is $\dfrac{-1}{3x^{5/3}}$ but I'm confused on how to get there.  Sorry if this was hard to understand, any help is greatly appreciated, thanks.","Can someone please explain the steps to find the derivative of: $$\dfrac1{2(\sqrt[3]{x^2})}$$ I know the answer is $\dfrac{-1}{3x^{5/3}}$ but I'm confused on how to get there.  Sorry if this was hard to understand, any help is greatly appreciated, thanks.",,"['calculus', 'derivatives']"
1,"relations among the bounds of $f(x),f'(x),f''(x)$",relations among the bounds of,"f(x),f'(x),f''(x)","Is there any relations among the bounds of $f(x),f'(x),f''(x)$ ? say $f:(0,\infty)\to\mathbb{R}$ be a twice differentiable function with $|f(x)|\le 1,|f''(x)|\le 2$ what bound I can  say about $|f'(x)|\le ?$ Thank you for any hints or discussion.","Is there any relations among the bounds of $f(x),f'(x),f''(x)$ ? say $f:(0,\infty)\to\mathbb{R}$ be a twice differentiable function with $|f(x)|\le 1,|f''(x)|\le 2$ what bound I can  say about $|f'(x)|\le ?$ Thank you for any hints or discussion.",,"['real-analysis', 'derivatives']"
2,How do I compute $\int_{-x^2}^{0}f(t)dt$ derivative ratio $x$?,How do I compute  derivative ratio ?,\int_{-x^2}^{0}f(t)dt x,"If  $$  f(t)=\begin{cases}\frac{\sin t}{t}  &  t\neq 0\\\\1 & t=0,\end{cases} $$  then, how do I compute the following derivative ratio $x$? The desired function is: $$\int_{-x^2}^{0}f(t)dt$$","If  $$  f(t)=\begin{cases}\frac{\sin t}{t}  &  t\neq 0\\\\1 & t=0,\end{cases} $$  then, how do I compute the following derivative ratio $x$? The desired function is: $$\int_{-x^2}^{0}f(t)dt$$",,"['calculus', 'analysis', 'derivatives']"
3,Derivative of logarithmic function,Derivative of logarithmic function,,"If the function $f(x)=\log_{2x}x^2$ is given, what is $f'(4)$? I tried to use the formula for derivative of logarithm but here the base is $2x$, so it made me confused. Note that the answer is $1/(18\ln 2)$.","If the function $f(x)=\log_{2x}x^2$ is given, what is $f'(4)$? I tried to use the formula for derivative of logarithm but here the base is $2x$, so it made me confused. Note that the answer is $1/(18\ln 2)$.",,['derivatives']
4,"Differentiable function, with $f'(x)=[f(x)]^2$","Differentiable function, with",f'(x)=[f(x)]^2,"Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(0)=0$ and $\forall x \in \mathbb{R}$, we have $f'(x)=[f(x)]^2$. Show that $f(x)=0, \forall x \in \mathbb{R}$.","Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(0)=0$ and $\forall x \in \mathbb{R}$, we have $f'(x)=[f(x)]^2$. Show that $f(x)=0, \forall x \in \mathbb{R}$.",,"['real-analysis', 'calculus', 'derivatives']"
5,Simplifying second derivative using trigonometric identities,Simplifying second derivative using trigonometric identities,,"Given that $x=1+\sin(t)$ , $y=\sin(t) -\frac{1}{2} \cos(2t)$ show that $\frac{\text{d}^2y}{\text{d}x^2}=2$. I am having trouble proving this. Here is my working so far: \begin{align}\frac{dx}{dt}&= cos(t)\\ \frac{dy}{dt}&= cos(t) + sin(2t)\end{align} \begin{align}\frac{dy}{dx}&=\frac{\cos(t) + sin(2t)}{cos(t)}\\ \frac{d^2y}{dx^2}&=\frac{2cos(2t)cos(t) - sin(2t)sin(t)}{cos^2(t)}\frac{1}{cos(t)} \end{align} I think its just a matter of simplifying my expression for $\frac{\text{d}^2y}{\text{d}x^2}$ using trigonometric identities but I can't see the right ones to use.","Given that $x=1+\sin(t)$ , $y=\sin(t) -\frac{1}{2} \cos(2t)$ show that $\frac{\text{d}^2y}{\text{d}x^2}=2$. I am having trouble proving this. Here is my working so far: \begin{align}\frac{dx}{dt}&= cos(t)\\ \frac{dy}{dt}&= cos(t) + sin(2t)\end{align} \begin{align}\frac{dy}{dx}&=\frac{\cos(t) + sin(2t)}{cos(t)}\\ \frac{d^2y}{dx^2}&=\frac{2cos(2t)cos(t) - sin(2t)sin(t)}{cos^2(t)}\frac{1}{cos(t)} \end{align} I think its just a matter of simplifying my expression for $\frac{\text{d}^2y}{\text{d}x^2}$ using trigonometric identities but I can't see the right ones to use.",,"['trigonometry', 'derivatives']"
6,Is there any function that never gives an answer other than 0/0 when applying L'Hôpital's rule?,Is there any function that never gives an answer other than 0/0 when applying L'Hôpital's rule?,,"Someone asked this question in my calculus class and the teacher said that he would get back to the student on that one. I never heard back, so was hoping someone here knew the answer? EDIT Sorry guys, I think I worded the question wrong, so I attempted to rewrite it...","Someone asked this question in my calculus class and the teacher said that he would get back to the student on that one. I never heard back, so was hoping someone here knew the answer? EDIT Sorry guys, I think I worded the question wrong, so I attempted to rewrite it...",,"['calculus', 'derivatives']"
7,"How to prove that $f$ is $1-1$ from $E$ on $\{ (s,t) : s> 2\sqrt{t} >0\}$",How to prove that  is  from  on,"f 1-1 E \{ (s,t) : s> 2\sqrt{t} >0\}","Question: Let $E=\{(x,y): 0<y<x \}$ set $f(x,y)=(x+y, xy)$ for $(x,y)\in E$ a) How to prove that $f$ is $1-1$ from $E$ on $\{ (s,t) : s> 2\sqrt{t} >0\}$ And how to find formula for $f^{-1}(s,t)$ b) compute $Df^{-1}(f(x,y))$ for $(x,y)\in E$ by using inverse function theorem. answer for part b) I firstly compute $D(f(x,y))$ $$D(f(x,y))=\begin{pmatrix} 1 && 1 \\ y && x\end{pmatrix} $$ Then $$Df^{-1}=\begin {pmatrix} \frac{-x}{y-x} && \frac{1}{y-x} \\ \frac{y}{y-x} && \frac{-1}{y-x}\end{pmatrix}$$ I did part-b. Hopefully it is true. Please check my answer. Also I could not do part-a. Please help me to solve this. Thank you.","Question: Let $E=\{(x,y): 0<y<x \}$ set $f(x,y)=(x+y, xy)$ for $(x,y)\in E$ a) How to prove that $f$ is $1-1$ from $E$ on $\{ (s,t) : s> 2\sqrt{t} >0\}$ And how to find formula for $f^{-1}(s,t)$ b) compute $Df^{-1}(f(x,y))$ for $(x,y)\in E$ by using inverse function theorem. answer for part b) I firstly compute $D(f(x,y))$ $$D(f(x,y))=\begin{pmatrix} 1 && 1 \\ y && x\end{pmatrix} $$ Then $$Df^{-1}=\begin {pmatrix} \frac{-x}{y-x} && \frac{1}{y-x} \\ \frac{y}{y-x} && \frac{-1}{y-x}\end{pmatrix}$$ I did part-b. Hopefully it is true. Please check my answer. Also I could not do part-a. Please help me to solve this. Thank you.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'inverse']"
8,"$\frac{\sin x}{x^5} - \frac{1}{x^4} \underset{x\to 0}{\approx} \frac{-1}{6} \cdot \frac{1}{x^2}$, right?",", right?",\frac{\sin x}{x^5} - \frac{1}{x^4} \underset{x\to 0}{\approx} \frac{-1}{6} \cdot \frac{1}{x^2},"I was reading an set of notes about Taylor series, and I came across a part I think is a typo. I want to make sure, because I want to understand this stuff correctly. Here is the relevant page of the article. You can see where I've indicated the typo in the margin. In the space I've marked, do you think it should say $\frac{1}{x^2}\cdot \frac{-1}{6}$?","I was reading an set of notes about Taylor series, and I came across a part I think is a typo. I want to make sure, because I want to understand this stuff correctly. Here is the relevant page of the article. You can see where I've indicated the typo in the margin. In the space I've marked, do you think it should say $\frac{1}{x^2}\cdot \frac{-1}{6}$?",,"['calculus', 'derivatives', 'taylor-expansion']"
9,"Answer-verification: Show that $f(x,y)=1+2x+3y$ for all $(x,y)\in \Bbb R^2$",Answer-verification: Show that  for all,"f(x,y)=1+2x+3y (x,y)\in \Bbb R^2","Define the function $f: \Bbb R^2\to \Bbb R$ has first order partial derivatives and that $f(0,0)=1$  While $\frac{\partial f}{\partial x}(x,y)=2$ and $\frac{\partial f}{\partial y}(x,y)=3$ for all $(x,y)\in \Bbb R^2$ Prove that $f(x,y)=1+2x+3y$ for all $(x,y)\in \Bbb R^2$ $\bf{solution:}$ Let $\frac{\partial f}{\partial x}(x,y)=g(x,y)$ Then $\int g(x,y)dx=2x +c_1$ for $c_1 $ is constant Similarly, let $\frac{\partial f}{\partial y}=h(x,y)$ Then $\int h(x,y)=2y+c_2$ for $c_2$ constant Since $f(0,0)=1$ is constant so, $c:=(c_1,c_2)=1$ Thus, $f(x,y)=1+2x+3y$ for all $(x,y)\in \Bbb R^2$ Is this answer true? If there exist mistakes, please can somebody correct this?","Define the function $f: \Bbb R^2\to \Bbb R$ has first order partial derivatives and that $f(0,0)=1$  While $\frac{\partial f}{\partial x}(x,y)=2$ and $\frac{\partial f}{\partial y}(x,y)=3$ for all $(x,y)\in \Bbb R^2$ Prove that $f(x,y)=1+2x+3y$ for all $(x,y)\in \Bbb R^2$ $\bf{solution:}$ Let $\frac{\partial f}{\partial x}(x,y)=g(x,y)$ Then $\int g(x,y)dx=2x +c_1$ for $c_1 $ is constant Similarly, let $\frac{\partial f}{\partial y}=h(x,y)$ Then $\int h(x,y)=2y+c_2$ for $c_2$ constant Since $f(0,0)=1$ is constant so, $c:=(c_1,c_2)=1$ Thus, $f(x,y)=1+2x+3y$ for all $(x,y)\in \Bbb R^2$ Is this answer true? If there exist mistakes, please can somebody correct this?",,"['calculus', 'real-analysis', 'derivatives', 'solution-verification']"
10,Find an equation of each plane tangent to $K$ which is parallel to the plane $x-y+z=1$,Find an equation of each plane tangent to  which is parallel to the plane,K x-y+z=1,"Let $K$ be the cone given by $z=\sqrt {x^2+y^2}$ Find an equation of each plane tangent to $K$ which is parallel to the plane $x-y+z=1$ Sorry for not writing my ideas because I have No idea to solve this, honestly:( There are lots of such questions I need to solve. If somebody show me one of them 's solution, I try to solve others on my own. I wanna learn such type questions. Thank you!","Let be the cone given by Find an equation of each plane tangent to which is parallel to the plane Sorry for not writing my ideas because I have No idea to solve this, honestly:( There are lots of such questions I need to solve. If somebody show me one of them 's solution, I try to solve others on my own. I wanna learn such type questions. Thank you!",K z=\sqrt {x^2+y^2} K x-y+z=1,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
11,Using Calculus to find total and maximum revenue and profit,Using Calculus to find total and maximum revenue and profit,,"I'm grappling with understanding how to use calculus to find rates of profit, revenue, and cost.  I have the following problem: $x = \text{ quantity }$, $12 < x < 48$ Total Cost: $C(x) = \dfrac 92x^2 -17x + 2700$ Price per item: $p(x) = -\dfrac{x^2}3 +\dfrac{23}2x + 78 + \dfrac{20000}x$ To find the total revenue $R(x)$, I believe that I have to multiply the quantity x by the price/item.  Thus, $$\begin{align} R(x) &= x\cdot p(x)\\           &= x\left( -\dfrac{x^2}3 +\dfrac{23}2x + 78 + \dfrac{20000}x\right) \\          &=  -\dfrac{x^3}3 +\dfrac{23}2x^2 + 78 + 20000\end{align}$$ To find total profit $P(x)$, I believe that I have to subtract cost from revenue. Thus, $$\begin{align} P(x) &= R(x) - C(x) \\          &= -\frac{x^3}3 +\frac{23}2x^2 + 78x + 20000 - \left(\frac 92x^2 -17x + 2700\right) \\          &= --\frac{x^3}3 +\frac{17}2 x^2 + 61x + 17300\end{align}$$ To find $x$ for maximum revenue, I believe I need to find the derivative of $R(x)$, so that $$\begin{align} R'(x) = -x^2 + 23x + 78 &= 0\\               -(x-26)(x+3) &= 0\\               x = 26  \text{ or }x &= -3\end{align}$$ Since $x = -3$ is not in the domain, this means that the quantity of $x$ that will generate the maximum revenue is $26$. To find $x$ for maximum profit, I need to first find the derivative of $P(x)$, set it equal to zero, and solve for $x$.  Thus, $$P'(x) = -x^2 + 17x + 61 = 0$$ And here's where I run into trouble.  I'm not sure if I'm doing my math correctly or if this is just not an easy polynomial to factor.  If I can't factor the polynomial to find x, I'm not sure how to proceed. Essentially, I'm hoping someone can tell me if my math and logic are correct as I take the derivatives and find the maxima; if I'm not doing it correctly, how so; and what I might be doing wrong when it comes to finding the total profit.  My apologies if there is a better way to format exponents.  I checked the formatting help page and didn't see any help specific to that.  If you have a suggestion for that as well, I'm happy to go in and edit the post to make it more readable.  Thanks in advance.","I'm grappling with understanding how to use calculus to find rates of profit, revenue, and cost.  I have the following problem: $x = \text{ quantity }$, $12 < x < 48$ Total Cost: $C(x) = \dfrac 92x^2 -17x + 2700$ Price per item: $p(x) = -\dfrac{x^2}3 +\dfrac{23}2x + 78 + \dfrac{20000}x$ To find the total revenue $R(x)$, I believe that I have to multiply the quantity x by the price/item.  Thus, $$\begin{align} R(x) &= x\cdot p(x)\\           &= x\left( -\dfrac{x^2}3 +\dfrac{23}2x + 78 + \dfrac{20000}x\right) \\          &=  -\dfrac{x^3}3 +\dfrac{23}2x^2 + 78 + 20000\end{align}$$ To find total profit $P(x)$, I believe that I have to subtract cost from revenue. Thus, $$\begin{align} P(x) &= R(x) - C(x) \\          &= -\frac{x^3}3 +\frac{23}2x^2 + 78x + 20000 - \left(\frac 92x^2 -17x + 2700\right) \\          &= --\frac{x^3}3 +\frac{17}2 x^2 + 61x + 17300\end{align}$$ To find $x$ for maximum revenue, I believe I need to find the derivative of $R(x)$, so that $$\begin{align} R'(x) = -x^2 + 23x + 78 &= 0\\               -(x-26)(x+3) &= 0\\               x = 26  \text{ or }x &= -3\end{align}$$ Since $x = -3$ is not in the domain, this means that the quantity of $x$ that will generate the maximum revenue is $26$. To find $x$ for maximum profit, I need to first find the derivative of $P(x)$, set it equal to zero, and solve for $x$.  Thus, $$P'(x) = -x^2 + 17x + 61 = 0$$ And here's where I run into trouble.  I'm not sure if I'm doing my math correctly or if this is just not an easy polynomial to factor.  If I can't factor the polynomial to find x, I'm not sure how to proceed. Essentially, I'm hoping someone can tell me if my math and logic are correct as I take the derivatives and find the maxima; if I'm not doing it correctly, how so; and what I might be doing wrong when it comes to finding the total profit.  My apologies if there is a better way to format exponents.  I checked the formatting help page and didn't see any help specific to that.  If you have a suggestion for that as well, I'm happy to go in and edit the post to make it more readable.  Thanks in advance.",,"['calculus', 'optimization', 'derivatives']"
12,Geometric representation of product rule?,Geometric representation of product rule?,,"At time 1:06 of this video by minutephysics , there is a geometric representation of the product rule: However, I don't understand how the sums of the areas of those thin strips represent $d(u\cdot v)$. The only way I can see it is that $d(u\cdot v)$ is a small change in the area of the square, and those thin strips do represent that; however, I'm not sure if this is correct and if it is, how formal of a proof is this? Thanks!","At time 1:06 of this video by minutephysics , there is a geometric representation of the product rule: However, I don't understand how the sums of the areas of those thin strips represent $d(u\cdot v)$. The only way I can see it is that $d(u\cdot v)$ is a small change in the area of the square, and those thin strips do represent that; however, I'm not sure if this is correct and if it is, how formal of a proof is this? Thanks!",,"['calculus', 'derivatives', 'intuition']"
13,Optimizing the area of a rectangle,Optimizing the area of a rectangle,,"A rectangular field is bounded on one side by a river and on the other three sides by a fence. Additional fencing is used to divide the field into three smaller rectangles, each of equal area. 1080 feet of fencing is required. I want to find the dimensions of the large rectangle that will maximize the area. I had the following equations and I was wondering if they are correct: I let $Y$ denote the length of the rectangle. Then $Y=3y$. If $x$ represents the width of the three smaller rectangles, then I get the following:  $$ 4x+3y = 1080,~~~\text{Area} = 3xy.$$","A rectangular field is bounded on one side by a river and on the other three sides by a fence. Additional fencing is used to divide the field into three smaller rectangles, each of equal area. 1080 feet of fencing is required. I want to find the dimensions of the large rectangle that will maximize the area. I had the following equations and I was wondering if they are correct: I let $Y$ denote the length of the rectangle. Then $Y=3y$. If $x$ represents the width of the three smaller rectangles, then I get the following:  $$ 4x+3y = 1080,~~~\text{Area} = 3xy.$$",,"['calculus', 'derivatives']"
14,Why is $\frac{f'(x)}{f(x)}$ always a constant?,Why is  always a constant?,\frac{f'(x)}{f(x)},"Today in class we learned that for exponential functions $f(x) = b^x$ and their derivatives $f'(x)$, the ratio is always constant for any $x$. For example for $f(x) = 2^x$ and its derivative $f'(x) = 2^x \cdot \ln 2$ $$\begin{array}{c | c | c | c} x & f(x) & f'(x) & \frac{f'(x)}{f(x)}\\ \hline -1 & \frac{1}{2} & 0.346  & 0.693 \\ 0 & 1 & 0.693 & 0.693 \\ 1 & 2 & 1.38 &  0.693\\ 2 & 4 & 2.77 &0.693& \end{array}$$ So as you can see, the ratio is the same and this is true for all functions of the form $b^x$ and its derivative. So my question is, why is the ratio always constant? Is there some proof or logic behind it or is just like that ? Furthermore, what's the use of knowing this? EDIT: It seems that I have missed the fairly simple $$\require{cancel}\frac{f^\prime(x)}{f(x)}=\frac{\cancel{{b^x}}\ln\,b}{\cancel{{‌​b^x}}}=\ln\,b$$ But what's the use knowing and learning this? Will this reduce a step in the future or help solve a much harder problem more easily?","Today in class we learned that for exponential functions $f(x) = b^x$ and their derivatives $f'(x)$, the ratio is always constant for any $x$. For example for $f(x) = 2^x$ and its derivative $f'(x) = 2^x \cdot \ln 2$ $$\begin{array}{c | c | c | c} x & f(x) & f'(x) & \frac{f'(x)}{f(x)}\\ \hline -1 & \frac{1}{2} & 0.346  & 0.693 \\ 0 & 1 & 0.693 & 0.693 \\ 1 & 2 & 1.38 &  0.693\\ 2 & 4 & 2.77 &0.693& \end{array}$$ So as you can see, the ratio is the same and this is true for all functions of the form $b^x$ and its derivative. So my question is, why is the ratio always constant? Is there some proof or logic behind it or is just like that ? Furthermore, what's the use of knowing this? EDIT: It seems that I have missed the fairly simple $$\require{cancel}\frac{f^\prime(x)}{f(x)}=\frac{\cancel{{b^x}}\ln\,b}{\cancel{{‌​b^x}}}=\ln\,b$$ But what's the use knowing and learning this? Will this reduce a step in the future or help solve a much harder problem more easily?",,"['calculus', 'derivatives', 'exponential-function']"
15,infinite derivative of $e^x$,infinite derivative of,e^x,"i have started  thinking about  one topic  a few   days ago and    i am confused   if i am wrong  or what happens,generally we know that function $e^x$  is somehow  'magic',which means  that derivative  and integral of this function is  again $e^x$(let   reject  constant term during the integral).but on the other hand  we can say that (here let's use d  as  sign of derivative) $$d(e^x)=d(e*e^{x-1})$$ which is equal to  $d(e)*e^{x-1}+ e*d(e^{x-1})$ because of  feature  of derivative of two function(in this case our function $f(x)=e$ is constant),clearly first term is zero,so we have  $e*d(e^{x-1})$,if we continue it  to  infinite time,  we can see that in derivative  sign    power approaches  $x$,or something like this $$d(e^{x-1}),d(e^{x-2}),d(e^{x-3})$$ and  at the  same time power of  constant  $e$ is increasing corresponding,but my confusion is that  does never  power  in $$d(e^{x-c})$$ where  $c$  is some constant  is changed from  -infinite to  +infinity,but  does it never make  $e^{x-c}$  as a constant? or  does never equal $e^{x-c}$   never equal to $1$?  meaning that  $x=c$? if ti makes constant ,then we know that derivative of constant is  zero and whole  multiplication becomes  zero,which is contradiction what  $$d(e^x)=e^x$$ sorry if my idea seems  stupid,but i am curious in this topic and please help me to clarify everything","i have started  thinking about  one topic  a few   days ago and    i am confused   if i am wrong  or what happens,generally we know that function $e^x$  is somehow  'magic',which means  that derivative  and integral of this function is  again $e^x$(let   reject  constant term during the integral).but on the other hand  we can say that (here let's use d  as  sign of derivative) $$d(e^x)=d(e*e^{x-1})$$ which is equal to  $d(e)*e^{x-1}+ e*d(e^{x-1})$ because of  feature  of derivative of two function(in this case our function $f(x)=e$ is constant),clearly first term is zero,so we have  $e*d(e^{x-1})$,if we continue it  to  infinite time,  we can see that in derivative  sign    power approaches  $x$,or something like this $$d(e^{x-1}),d(e^{x-2}),d(e^{x-3})$$ and  at the  same time power of  constant  $e$ is increasing corresponding,but my confusion is that  does never  power  in $$d(e^{x-c})$$ where  $c$  is some constant  is changed from  -infinite to  +infinity,but  does it never make  $e^{x-c}$  as a constant? or  does never equal $e^{x-c}$   never equal to $1$?  meaning that  $x=c$? if ti makes constant ,then we know that derivative of constant is  zero and whole  multiplication becomes  zero,which is contradiction what  $$d(e^x)=e^x$$ sorry if my idea seems  stupid,but i am curious in this topic and please help me to clarify everything",,"['calculus', 'derivatives', 'exponentiation']"
16,What is the limit of the following equation using L'Hôpital's rule?,What is the limit of the following equation using L'Hôpital's rule?,,"Find $$\lim_{x \to 0} \frac{\tan x - x}{x^3} $$ So I know that for L'Hôpital's rule I take the derivative of the top and the bottom, and I keep doing that until I get something that isn't an indeterminate form. So I got $\dfrac{\sec^2(x)-1}{3x^2}$ and I still couldn't plug in $0$ so I took the derivatives again and got $\dfrac{2\tan(x)\sec^2(x)}{6x}$ and still can't plug in $0$. Do I have to take the derivatives again and use product rule for the numerator? Thanks for the help!","Find $$\lim_{x \to 0} \frac{\tan x - x}{x^3} $$ So I know that for L'Hôpital's rule I take the derivative of the top and the bottom, and I keep doing that until I get something that isn't an indeterminate form. So I got $\dfrac{\sec^2(x)-1}{3x^2}$ and I still couldn't plug in $0$ so I took the derivatives again and got $\dfrac{2\tan(x)\sec^2(x)}{6x}$ and still can't plug in $0$. Do I have to take the derivatives again and use product rule for the numerator? Thanks for the help!",,"['calculus', 'derivatives']"
17,Least value of $a$ for which at least one solution exists?,Least value of  for which at least one solution exists?,a,"What is the least value of $a$ for which $$\frac{4}{\sin(x)}+\frac{1}{1-\sin(x)}=a$$ has atleast one solution in the interval $(0,\frac{\pi}{2})$? I first calculate $f'(x)$ and put it equal to $0$ to find out the critical points. This gives $$\sin(x)=\frac{2}{3}$$ as $\cos(x)$ is not $0$ in $(0,\frac{\pi}{2})$. I calculate $f''(x)$ and at $\sin(x)=\frac{2}{3}$, I get a minima. Now to have at least one solution, putting $\sin(x)=\frac{2}{3}$ in the main equation, I get $f=9-a$, which should be greater than or equal to $0$. I then get the 'maximum' value of $a$ as $9$. Where did I go wrong? [Note the function is $f(x)=LHS-RHS$ of the main equation.]","What is the least value of $a$ for which $$\frac{4}{\sin(x)}+\frac{1}{1-\sin(x)}=a$$ has atleast one solution in the interval $(0,\frac{\pi}{2})$? I first calculate $f'(x)$ and put it equal to $0$ to find out the critical points. This gives $$\sin(x)=\frac{2}{3}$$ as $\cos(x)$ is not $0$ in $(0,\frac{\pi}{2})$. I calculate $f''(x)$ and at $\sin(x)=\frac{2}{3}$, I get a minima. Now to have at least one solution, putting $\sin(x)=\frac{2}{3}$ in the main equation, I get $f=9-a$, which should be greater than or equal to $0$. I then get the 'maximum' value of $a$ as $9$. Where did I go wrong? [Note the function is $f(x)=LHS-RHS$ of the main equation.]",,"['derivatives', 'quadratics']"
18,How to use Rolle's Theorem to prove the following?:,How to use Rolle's Theorem to prove the following?:,,"For each $\lambda$, the function $f(x)=x^3-\frac 32x^2+\lambda$ does   not have two distinct zeros in $[0,1]$.","For each $\lambda$, the function $f(x)=x^3-\frac 32x^2+\lambda$ does   not have two distinct zeros in $[0,1]$.",,"['real-analysis', 'derivatives']"
19,Prove that one of the roots of this cubic equation has the given coordinates,Prove that one of the roots of this cubic equation has the given coordinates,,"I have a cubic function: $ y = 3 + 5x + x^2 - x^3$ and am supposed to prove that it goes through the point C with coordinates (3, 0). The question also asked to find 2 of its stationary points which I was able to find by differentiating the cubic equation and setting it to zero. But I'm not sure how to prove that this graph goes through (3, 0). These are the only two methods I could think of: 1- Just plug in 3 and 0 and see that it works. 2- Divide the equation by $(x-3)$ and show that there's no remainder. This is an A'Levels calculus question. What's the right approach to follow here?","I have a cubic function: $ y = 3 + 5x + x^2 - x^3$ and am supposed to prove that it goes through the point C with coordinates (3, 0). The question also asked to find 2 of its stationary points which I was able to find by differentiating the cubic equation and setting it to zero. But I'm not sure how to prove that this graph goes through (3, 0). These are the only two methods I could think of: 1- Just plug in 3 and 0 and see that it works. 2- Divide the equation by $(x-3)$ and show that there's no remainder. This is an A'Levels calculus question. What's the right approach to follow here?",,"['calculus', 'derivatives']"
20,"Local Max/Min, Critical points of integral","Local Max/Min, Critical points of integral",,"Given $$f(t) = \int_0^t \frac{x^2+14x+45}{1+\cos^2(x)}dx $$ I need to find the local max of f(t). Well here using the fundamental theorem of calculus, I know I can just replace the $x$ with $t$. But I do not remember how to find the local max/min and if I remember correctly critical points were in the same context, So some insight on critical points would be good too. Thank you.","Given $$f(t) = \int_0^t \frac{x^2+14x+45}{1+\cos^2(x)}dx $$ I need to find the local max of f(t). Well here using the fundamental theorem of calculus, I know I can just replace the $x$ with $t$. But I do not remember how to find the local max/min and if I remember correctly critical points were in the same context, So some insight on critical points would be good too. Thank you.",,"['calculus', 'integration', 'derivatives']"
21,Somewhat confused on the fundamental theorem of calculus,Somewhat confused on the fundamental theorem of calculus,,"I have the following equation which I require to find the derivative by using the Fundamental theorem of calculus: $$f(x) = \int_3^{x^2} \left(\frac13t^2-1\right)^{15}dt $$ Trying to understand this, I tried plugging $f(x^2)$ in the formula minus $f(3)$ which was wrong. Originally, I thought it be $F(x^2)-F(3)$ but I cannot seem to find the anti-derivative. Can someone help pls ? It's been over a couple years, so I do not remember my derivatives rules by heart, please be clear about those.","I have the following equation which I require to find the derivative by using the Fundamental theorem of calculus: $$f(x) = \int_3^{x^2} \left(\frac13t^2-1\right)^{15}dt $$ Trying to understand this, I tried plugging $f(x^2)$ in the formula minus $f(3)$ which was wrong. Originally, I thought it be $F(x^2)-F(3)$ but I cannot seem to find the anti-derivative. Can someone help pls ? It's been over a couple years, so I do not remember my derivatives rules by heart, please be clear about those.",,"['calculus', 'integration', 'derivatives']"
22,How to expand $x \sqrt{4 - x}$ to Maclaurin series?,How to expand  to Maclaurin series?,x \sqrt{4 - x},"Here is the task: using standard expansions , expand $f(x) = x \sqrt{4-x}$ to Maclaurin's series. I calculated derivatives up to $f^{(5)}(x)$, and got some results. Fortunately, in Maclaurin's expansion I need to calcalate $f^{(n)}(0)$, which simplifies the task a little. Here is what I've got: $d(n) = 1 \times 3 \times 5 \times \ldots \times (2n-1) = \frac{(2n)!}{2^nn!}$ (a function for calculating product of first n odd numbers) $r(n)= \begin{cases} (2(n - 2) - 1) \times (r(n-1) + d(n-3)),&\text{if $n > 3$;}\\ 3,&\text{if $n = 3$.} \end{cases} $ $r(n)$ is calculated iteratively or recursively, and it frightens me. However, factorial function is iterative/recursive too, so my $r(n)$ is not worse, but I still somewhat displeased by it. What about derivatives, $f^{(n)}(0)= \begin{cases} 0,&\text{if $n = 0$;}\\ 2,&\text{if $n = 1$;}\\ -1/2,&\text{if $n = 2$;}\\ -r(n) \times 2^{n-2},&\text{if $n \geq 3$.} \end{cases} $ ($f^{(0)}(x) = f(x)$, if this is confusing) And it seems to work. I just calculated derivatives and noticed how next derivative is produced from previous. How the series is written down is obvious. But I didn't use any ""standard expansions"". Could this task be done easier and could the $f^{(n)}(0)$ formula be more beautiful?","Here is the task: using standard expansions , expand $f(x) = x \sqrt{4-x}$ to Maclaurin's series. I calculated derivatives up to $f^{(5)}(x)$, and got some results. Fortunately, in Maclaurin's expansion I need to calcalate $f^{(n)}(0)$, which simplifies the task a little. Here is what I've got: $d(n) = 1 \times 3 \times 5 \times \ldots \times (2n-1) = \frac{(2n)!}{2^nn!}$ (a function for calculating product of first n odd numbers) $r(n)= \begin{cases} (2(n - 2) - 1) \times (r(n-1) + d(n-3)),&\text{if $n > 3$;}\\ 3,&\text{if $n = 3$.} \end{cases} $ $r(n)$ is calculated iteratively or recursively, and it frightens me. However, factorial function is iterative/recursive too, so my $r(n)$ is not worse, but I still somewhat displeased by it. What about derivatives, $f^{(n)}(0)= \begin{cases} 0,&\text{if $n = 0$;}\\ 2,&\text{if $n = 1$;}\\ -1/2,&\text{if $n = 2$;}\\ -r(n) \times 2^{n-2},&\text{if $n \geq 3$.} \end{cases} $ ($f^{(0)}(x) = f(x)$, if this is confusing) And it seems to work. I just calculated derivatives and noticed how next derivative is produced from previous. How the series is written down is obvious. But I didn't use any ""standard expansions"". Could this task be done easier and could the $f^{(n)}(0)$ formula be more beautiful?",,"['calculus', 'derivatives', 'taylor-expansion']"
23,"Commutators, and Christoffel symbols in a non holonomic basis","Commutators, and Christoffel symbols in a non holonomic basis",,"I have a frame that varies along a curve $\gamma$ : the frame consists in the tangent vector of the curve plus some constant non orthogonal vectors. I need to compute $\nabla_{\dot\gamma}{\dot\gamma}$. Apparently, the Christoffel symbols in that case are computed with : $$\omega^i{}_{k\ell}=\frac{1}{2}g^{im} \left( g_{mk,\ell} +  g_{m\ell,k} - g_{k\ell,m} + c_{mk\ell}+c_{m\ell k} - c_{k\ell m}   \right)\,$$ where $c_{k\ell m}=g_{mp} {c_{k\ell}}^p\ $ are the commutation   coefficients of the basis; that is, $[\mathbf{u}_k,\mathbf{u}_\ell] = c_{k\ell}{}^m \mathbf{u}_m\,\ $ ( wiki ) I have several questions regarding the commutation coefficients: I read that the commutator is defined by $[X,Y]=\nabla_X Y - \nabla_Y X$ : isn't it a catch 22? how is it possible to compute that commutator, knowing that the connection $\nabla$ is obtained by the Christoffel symbols that uses the commutator that uses the connection that uses [...] ? For one of my applications, my tangent space is made of symmetric matrices (and the manifold has a fancy metric $g$) : in that case, is the commutator only defined by $[X,Y]=X*Y-Y*X$ with $*$ the standard matrix multiplication ? For another application, my tangent space are vectors in $R^N$ and my space is Euclidean. Of course, in that case there are probably easier ways to handle that, but I'd still like to understand what would the commutator be in that case. In the definition $[\mathbf{u}_k,\mathbf{u}_\ell] = c_{k\ell}{}^m \mathbf{u}_m\,\ $ , is-it necessary to solve a linear system to get the coefficients $c_{k\ell}{}^m$ or can I just project $[\mathbf{u}_k,\mathbf{u}_\ell]$ on $\mathbf{u}_m$ using the metric $g$ (my basis is not orthogonal, so I guess the linear system is needed?). Thanks!","I have a frame that varies along a curve $\gamma$ : the frame consists in the tangent vector of the curve plus some constant non orthogonal vectors. I need to compute $\nabla_{\dot\gamma}{\dot\gamma}$. Apparently, the Christoffel symbols in that case are computed with : $$\omega^i{}_{k\ell}=\frac{1}{2}g^{im} \left( g_{mk,\ell} +  g_{m\ell,k} - g_{k\ell,m} + c_{mk\ell}+c_{m\ell k} - c_{k\ell m}   \right)\,$$ where $c_{k\ell m}=g_{mp} {c_{k\ell}}^p\ $ are the commutation   coefficients of the basis; that is, $[\mathbf{u}_k,\mathbf{u}_\ell] = c_{k\ell}{}^m \mathbf{u}_m\,\ $ ( wiki ) I have several questions regarding the commutation coefficients: I read that the commutator is defined by $[X,Y]=\nabla_X Y - \nabla_Y X$ : isn't it a catch 22? how is it possible to compute that commutator, knowing that the connection $\nabla$ is obtained by the Christoffel symbols that uses the commutator that uses the connection that uses [...] ? For one of my applications, my tangent space is made of symmetric matrices (and the manifold has a fancy metric $g$) : in that case, is the commutator only defined by $[X,Y]=X*Y-Y*X$ with $*$ the standard matrix multiplication ? For another application, my tangent space are vectors in $R^N$ and my space is Euclidean. Of course, in that case there are probably easier ways to handle that, but I'd still like to understand what would the commutator be in that case. In the definition $[\mathbf{u}_k,\mathbf{u}_\ell] = c_{k\ell}{}^m \mathbf{u}_m\,\ $ , is-it necessary to solve a linear system to get the coefficients $c_{k\ell}{}^m$ or can I just project $[\mathbf{u}_k,\mathbf{u}_\ell]$ on $\mathbf{u}_m$ using the metric $g$ (my basis is not orthogonal, so I guess the linear system is needed?). Thanks!",,"['differential-geometry', 'derivatives', 'riemannian-geometry']"
24,Integrability of Derivative of a Continuous Function,Integrability of Derivative of a Continuous Function,,"Let $f$ be continuous on $[a,b]$ and has finite derivative a.e. on $[a,b]$. Let $f_n(x)=n[f(x+1/n)-f(x)] $ s.t. $f_n$ be uniformly integrable on $[a,b]$. I want to show : $f'$ is Lebesgue integrable. (I noted that $f_n\rightarrow f'$ pointwise.)","Let $f$ be continuous on $[a,b]$ and has finite derivative a.e. on $[a,b]$. Let $f_n(x)=n[f(x+1/n)-f(x)] $ s.t. $f_n$ be uniformly integrable on $[a,b]$. I want to show : $f'$ is Lebesgue integrable. (I noted that $f_n\rightarrow f'$ pointwise.)",,"['calculus', 'real-analysis', 'integration', 'derivatives', 'lebesgue-integral']"
25,What is the nth derivative of $\dfrac{1}{\sqrt{1 + x^2}}$,What is the nth derivative of,\dfrac{1}{\sqrt{1 + x^2}},"I'm trying to find a general formula for the $n$th derivative of  $$\dfrac{1}{\sqrt{1 + x^2}}$$ I got up to, \begin{eqnarray*} 		g^{(0)}(x) &=& g(x) \\ 		g^{(1)}(x) &=& \dfrac{1}{(1 + x^2)^{1/2}} \\ 		g^{(2)}(x) &=& \dfrac{-x}{(1 + x^2)^{3/2}} \\ 		g^{(3)}(x) &=& \dfrac{2x^2 - 1}{(x^2 + 1)^{5/2}} \\ 		g^{(4)}(x) &=& \dfrac{-6x^3 + 9x}{(x^2 + 1)^{7/2}} \\ 		g^{(5)}(x) &=& \dfrac{24x^4 - 72x^2 + 9}{(x^2 + 1)^{9/2}} \\ 		g^{(6)}(x) &=& \dfrac{-120x^5 + 600x^3 - 225x}{(x^2 + 1)^{11/2}} \\ 		g^{(7)}(x) &=& \dfrac{720x^6 - 5400x^4 + 4050x^2 -225}{(x^2 + 1)^{13/2}} \\ 		g^{(8)}(x) &=& \dfrac{-5040x^7 + 52920x^5 - 66150x^3 + 11025x}{(x^2 + 1)^{15/2}} \\ 	\end{eqnarray*} Except for the first term in the numerator ($n!$), and the power in the denominator, I couldn't find a general pattern for the rest of the coefficients in the numerator. Could anyone shed me some light on this problem? Any idea would be greatly appreciated. Thanks.","I'm trying to find a general formula for the $n$th derivative of  $$\dfrac{1}{\sqrt{1 + x^2}}$$ I got up to, \begin{eqnarray*} 		g^{(0)}(x) &=& g(x) \\ 		g^{(1)}(x) &=& \dfrac{1}{(1 + x^2)^{1/2}} \\ 		g^{(2)}(x) &=& \dfrac{-x}{(1 + x^2)^{3/2}} \\ 		g^{(3)}(x) &=& \dfrac{2x^2 - 1}{(x^2 + 1)^{5/2}} \\ 		g^{(4)}(x) &=& \dfrac{-6x^3 + 9x}{(x^2 + 1)^{7/2}} \\ 		g^{(5)}(x) &=& \dfrac{24x^4 - 72x^2 + 9}{(x^2 + 1)^{9/2}} \\ 		g^{(6)}(x) &=& \dfrac{-120x^5 + 600x^3 - 225x}{(x^2 + 1)^{11/2}} \\ 		g^{(7)}(x) &=& \dfrac{720x^6 - 5400x^4 + 4050x^2 -225}{(x^2 + 1)^{13/2}} \\ 		g^{(8)}(x) &=& \dfrac{-5040x^7 + 52920x^5 - 66150x^3 + 11025x}{(x^2 + 1)^{15/2}} \\ 	\end{eqnarray*} Except for the first term in the numerator ($n!$), and the power in the denominator, I couldn't find a general pattern for the rest of the coefficients in the numerator. Could anyone shed me some light on this problem? Any idea would be greatly appreciated. Thanks.",,"['derivatives', 'taylor-expansion']"
26,Derivatives and graphs,Derivatives and graphs,,$f(x) = \sin(x)+ \cos(x) $  for $ 0≤x≤2\pi$ I have to do the following 1) Find the intervals on which $f$ is increasing or decreasing 2) Find the local maximum and minimum values of $f$ 3) Find the intervals of concavity and the inflections points I got until this point when trying to solve problem 1: $\tan(x) = 1$. The next step in the manual says that $x = \pi/4$ or $5\pi/4$. How did they get that $x = \pi/4$ or $5\pi/4$?,$f(x) = \sin(x)+ \cos(x) $  for $ 0≤x≤2\pi$ I have to do the following 1) Find the intervals on which $f$ is increasing or decreasing 2) Find the local maximum and minimum values of $f$ 3) Find the intervals of concavity and the inflections points I got until this point when trying to solve problem 1: $\tan(x) = 1$. The next step in the manual says that $x = \pi/4$ or $5\pi/4$. How did they get that $x = \pi/4$ or $5\pi/4$?,,"['calculus', 'derivatives']"
27,Sufficient condition for differentiability at endpoints.,Sufficient condition for differentiability at endpoints.,,"Let $f:[a,b]\to \mathbb{R}$ be differentiable on $(a,b)$ with derivative $g=f^{\prime}$ there. Assertion: If $\lim_{x\to b^{-}}g(x)$ exists and is a real number $\ell$ then $f$ is differentiable at $b$ and $f^{\prime}(b)=\ell$? Is this assertion correct? If so provide hints for a formal $\epsilon-\delta$ argument. If not, can it be made true if we strengthen some conditions on $g$ (continuity in $(a,b)$ etc.)? Provide counter-examples. I personally think that addition of the continuity of $g$ in the hypothesis won't change anything as for example $x\sin \frac{1}{x}$ has a continuous derivative in $(0,1)$ but its derivative oscillates near $0$. I also know that the converse of this is not true. Also if that limit is infinite, then $f$ is not differentiable at $b$ right?","Let $f:[a,b]\to \mathbb{R}$ be differentiable on $(a,b)$ with derivative $g=f^{\prime}$ there. Assertion: If $\lim_{x\to b^{-}}g(x)$ exists and is a real number $\ell$ then $f$ is differentiable at $b$ and $f^{\prime}(b)=\ell$? Is this assertion correct? If so provide hints for a formal $\epsilon-\delta$ argument. If not, can it be made true if we strengthen some conditions on $g$ (continuity in $(a,b)$ etc.)? Provide counter-examples. I personally think that addition of the continuity of $g$ in the hypothesis won't change anything as for example $x\sin \frac{1}{x}$ has a continuous derivative in $(0,1)$ but its derivative oscillates near $0$. I also know that the converse of this is not true. Also if that limit is infinite, then $f$ is not differentiable at $b$ right?",,"['limits', 'derivatives']"
28,Finding the scalar derivative of a matrix product,Finding the scalar derivative of a matrix product,,"I'm trying to find $$\frac{\partial}{\partial \lambda}y^T \left(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1}\right)^{-1}y$$ where $y \in \mathbb{R^n}$ is fixed, $\lambda \in \mathbb{R}$ and $K_{\theta}^{-1}$ is a known symmetric, positive definite matrix. Here's what I did so far: $$\frac{\partial}{\partial \lambda}y^T (\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}y = \frac{\partial}{\partial \lambda}\text{tr}\left(y^T (\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}y\right)$$ where tr denotes the trace. By the cyclic property of the trace, we can write $$\frac{\partial}{\partial \lambda}\text{tr}\left(y^T (\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1} y\right) = \frac{\partial}{\partial \lambda}\text{tr}\left(y^T y(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1} \right)$$ $$ = \frac{\partial}{\partial \lambda}\sum y_i ^2\text{tr}\left( \sigma^2 I + \lambda^{-1}K_{\theta}^{-1}\right)^{-1} = \sum y_i ^2\text{tr}\left(\frac{\partial}{\partial \lambda}(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}\right)$$ Since for any invertible matrix $M(\alpha)$ whose entries are differentiable in $\alpha \in \mathbb{R}$ it holds that $$\frac{d}{d\alpha} M(\alpha)^{-1} = M(\alpha)^{-1}\left(\frac{d}{d\alpha} M(\alpha)\right)  M(\alpha)^{-1}$$ we have $$\sum y_i ^2\text{tr}\left(\frac{\partial}{\partial \lambda}(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}\right) = \sum y_i^2 \text{tr}\left[ (\sigma^2 I + \lambda^{-1} K_{\theta}^{-1})^{-1}(-\lambda^{-2} K_{\theta}^{-1}) (\sigma^2 I + \lambda^{-1} K_{\theta}^{-1})^{-1}\right]$$ I can simplify this to $$-\sum y_i^2 \text{tr}\left[(\lambda\sigma^2 K_{\theta} + I)^{-1}(\lambda\sigma^2 I + K_\theta^{-1})^{-1}\right]$$$$ =-\sum y_i^2 \text{tr}\left[(\lambda^2\sigma^4 K_{\theta} + 2\lambda\sigma^2 I + K_{\theta}^{-1})^{-1}\right]$$ but this is where I'm stuck as I can't analyse this expression analytically (or can I?). Is there any way to simplify this expression? I tried to use the Woodbury matrix identity on the latter matrix but to no success yet. Any help would be greatly appreciated.","I'm trying to find $$\frac{\partial}{\partial \lambda}y^T \left(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1}\right)^{-1}y$$ where $y \in \mathbb{R^n}$ is fixed, $\lambda \in \mathbb{R}$ and $K_{\theta}^{-1}$ is a known symmetric, positive definite matrix. Here's what I did so far: $$\frac{\partial}{\partial \lambda}y^T (\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}y = \frac{\partial}{\partial \lambda}\text{tr}\left(y^T (\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}y\right)$$ where tr denotes the trace. By the cyclic property of the trace, we can write $$\frac{\partial}{\partial \lambda}\text{tr}\left(y^T (\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1} y\right) = \frac{\partial}{\partial \lambda}\text{tr}\left(y^T y(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1} \right)$$ $$ = \frac{\partial}{\partial \lambda}\sum y_i ^2\text{tr}\left( \sigma^2 I + \lambda^{-1}K_{\theta}^{-1}\right)^{-1} = \sum y_i ^2\text{tr}\left(\frac{\partial}{\partial \lambda}(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}\right)$$ Since for any invertible matrix $M(\alpha)$ whose entries are differentiable in $\alpha \in \mathbb{R}$ it holds that $$\frac{d}{d\alpha} M(\alpha)^{-1} = M(\alpha)^{-1}\left(\frac{d}{d\alpha} M(\alpha)\right)  M(\alpha)^{-1}$$ we have $$\sum y_i ^2\text{tr}\left(\frac{\partial}{\partial \lambda}(\sigma^2 I + \lambda^{-1}K_{\theta}^{-1})^{-1}\right) = \sum y_i^2 \text{tr}\left[ (\sigma^2 I + \lambda^{-1} K_{\theta}^{-1})^{-1}(-\lambda^{-2} K_{\theta}^{-1}) (\sigma^2 I + \lambda^{-1} K_{\theta}^{-1})^{-1}\right]$$ I can simplify this to $$-\sum y_i^2 \text{tr}\left[(\lambda\sigma^2 K_{\theta} + I)^{-1}(\lambda\sigma^2 I + K_\theta^{-1})^{-1}\right]$$$$ =-\sum y_i^2 \text{tr}\left[(\lambda^2\sigma^4 K_{\theta} + 2\lambda\sigma^2 I + K_{\theta}^{-1})^{-1}\right]$$ but this is where I'm stuck as I can't analyse this expression analytically (or can I?). Is there any way to simplify this expression? I tried to use the Woodbury matrix identity on the latter matrix but to no success yet. Any help would be greatly appreciated.",,"['matrices', 'derivatives', 'trace']"
29,Quotient rule or algebra mistake,Quotient rule or algebra mistake,,"I'm trying to derive this $$ f(x)=\frac{6}{1+2e^{-5x}}$$ and getting this $$ f'(x)=\frac{0(1+2e^{-5x})-6(0-10e^{-5x})}{(1+2e^{-5x})^2}$$ $$=\frac{60e^{-5x}}{(1+2e^{-5x})^2}$$ But the answer I get when checking on Wolfram Alpha is $$f'(x)=\frac{60e^{5x}}{(2e^{5x}+2)^2}$$ I don't understand how this works. How do the exponents for e become positive all of a sudden, and where does the +2 in the denominator come from?","I'm trying to derive this $$ f(x)=\frac{6}{1+2e^{-5x}}$$ and getting this $$ f'(x)=\frac{0(1+2e^{-5x})-6(0-10e^{-5x})}{(1+2e^{-5x})^2}$$ $$=\frac{60e^{-5x}}{(1+2e^{-5x})^2}$$ But the answer I get when checking on Wolfram Alpha is $$f'(x)=\frac{60e^{5x}}{(2e^{5x}+2)^2}$$ I don't understand how this works. How do the exponents for e become positive all of a sudden, and where does the +2 in the denominator come from?",,"['calculus', 'derivatives']"
30,Nth derivative can be expressed like that?,Nth derivative can be expressed like that?,,$$ \frac{f^{(n)}(z_0)}{n!} = \lim_{z \rightarrow z_0} \frac{f(z) -f(z_0)}{(z-z_0)^n} $$ Why is that nth derivative can be expressed like that limit of quotient? I can understand the meaning but I couldnt get closed form equation. Thanks.,$$ \frac{f^{(n)}(z_0)}{n!} = \lim_{z \rightarrow z_0} \frac{f(z) -f(z_0)}{(z-z_0)^n} $$ Why is that nth derivative can be expressed like that limit of quotient? I can understand the meaning but I couldnt get closed form equation. Thanks.,,['derivatives']
31,Find the derivative of the function,Find the derivative of the function,,"If $f(x) = 1/\sqrt{x}$, find $f^{\prime}(x)$. Please show the answer by using $$\lim_{h\to{0}}\frac{f(x+h)-f(x)}{h}.$$ I know the answer by using the shortcut, but my teacher wants me to get the answer using that equation. I am stuck and I got to: $-\sqrt{x}/2x^2$ by using the $\frac{f(x+h)-f(x)}{h}$.","If $f(x) = 1/\sqrt{x}$, find $f^{\prime}(x)$. Please show the answer by using $$\lim_{h\to{0}}\frac{f(x+h)-f(x)}{h}.$$ I know the answer by using the shortcut, but my teacher wants me to get the answer using that equation. I am stuck and I got to: $-\sqrt{x}/2x^2$ by using the $\frac{f(x+h)-f(x)}{h}$.",,"['calculus', 'limits', 'derivatives', 'radicals']"
32,The limit of the sum is the sum of the limits,The limit of the sum is the sum of the limits,,"I was wondering why the statement in the title is true only if the functions we are dealing with are continuous. Here's the context ( perhaps not required ): (The upper equation there is just a limit of two sums, and the lower expression is two limits of those two sums.), and if anyone wonders, that 's the original source (a pdf explaining the proof of the product rule). P.S. In the context it's given that $g$ and $f$ are differentiable, anyway I only provided it to illustrate the question; my actual question is simply general.","I was wondering why the statement in the title is true only if the functions we are dealing with are continuous. Here's the context ( perhaps not required ): (The upper equation there is just a limit of two sums, and the lower expression is two limits of those two sums.), and if anyone wonders, that 's the original source (a pdf explaining the proof of the product rule). P.S. In the context it's given that $g$ and $f$ are differentiable, anyway I only provided it to illustrate the question; my actual question is simply general.",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'continuity']"
33,Simple $d^2 y/dx^2$ - don't know what I'm missing,Simple  - don't know what I'm missing,d^2 y/dx^2,"I'm studying some Calculus on my own. I get what appears to be the wrong answer for this one implicit differentiation exercise, and I don't know why. I have to find $\frac {d^2 y}{dx^2}$ of  $y^2-2x = 1-2y$. I calculate $\frac{dy}{dx} = \frac{1}{1+y}$ and from there I get $\frac {d^2 y}{dx^2} = \frac{-y'}{(1+y)^2} = -\frac{1}{(1+y)^3}$ Now, Wolfram Alpha tells me I got $f'$ right but $f''$ wrong. It says $\frac{\delta^2y(x)}{\delta x^2} = - \frac{1}{2(1+x)(1+y)}$ WA hasn't steered me wrong so far. Pointers to where I'm going wrong would be appreciated.","I'm studying some Calculus on my own. I get what appears to be the wrong answer for this one implicit differentiation exercise, and I don't know why. I have to find $\frac {d^2 y}{dx^2}$ of  $y^2-2x = 1-2y$. I calculate $\frac{dy}{dx} = \frac{1}{1+y}$ and from there I get $\frac {d^2 y}{dx^2} = \frac{-y'}{(1+y)^2} = -\frac{1}{(1+y)^3}$ Now, Wolfram Alpha tells me I got $f'$ right but $f''$ wrong. It says $\frac{\delta^2y(x)}{\delta x^2} = - \frac{1}{2(1+x)(1+y)}$ WA hasn't steered me wrong so far. Pointers to where I'm going wrong would be appreciated.",,"['calculus', 'derivatives', 'implicit-differentiation']"
34,Clarification regarding a derivative symbol,Clarification regarding a derivative symbol,,"I came across the following expression: $$\frac{\partial^{i_1+\cdots+i_m}P(x_1,\ldots,x_m)}{\partial x_1^{i_1}\cdot\cdot\cdot \partial x_m^{i_m}}$$ for $P(x_1,\ldots,x_m)$ a polynomial in $m$ variables $x_1,\ldots,x_m$, and I must say that I find it rather confusing, as I've never encountered this notation before. So my question is: Given a monomial of the form $x_1^{j_1}\cdots x_m^{j_m}$, what does $\dfrac{\partial^{i_1+\,\cdots\,+i_m} x_1^{j_1} \cdots x_m^{j_m} }{\partial x_1^{i_1}\cdots \partial x_m^{i_m}}$ look like?","I came across the following expression: $$\frac{\partial^{i_1+\cdots+i_m}P(x_1,\ldots,x_m)}{\partial x_1^{i_1}\cdot\cdot\cdot \partial x_m^{i_m}}$$ for $P(x_1,\ldots,x_m)$ a polynomial in $m$ variables $x_1,\ldots,x_m$, and I must say that I find it rather confusing, as I've never encountered this notation before. So my question is: Given a monomial of the form $x_1^{j_1}\cdots x_m^{j_m}$, what does $\dfrac{\partial^{i_1+\,\cdots\,+i_m} x_1^{j_1} \cdots x_m^{j_m} }{\partial x_1^{i_1}\cdots \partial x_m^{i_m}}$ look like?",,"['polynomials', 'notation', 'derivatives']"
35,Discriminant of derivative of cubic equation being a perfect square,Discriminant of derivative of cubic equation being a perfect square,,"Is it possible for the discriminant of the first derivative of a cubic polynomial $(x+a)(x+b)(x+c)$, where $a, b$ and $c$ are distinct non-zero integers (i.e. Discriminant $[d((x+a)(x+b)(x+c))/dx]$ in Wolfram|Alpha/Mathematica) to be a perfect square? If so, what are the smallest absolute values of $a, b$ and $c$? Thanks!","Is it possible for the discriminant of the first derivative of a cubic polynomial $(x+a)(x+b)(x+c)$, where $a, b$ and $c$ are distinct non-zero integers (i.e. Discriminant $[d((x+a)(x+b)(x+c))/dx]$ in Wolfram|Alpha/Mathematica) to be a perfect square? If so, what are the smallest absolute values of $a, b$ and $c$? Thanks!",,"['calculus', 'polynomials', 'diophantine-equations', 'derivatives']"
36,Differentiability of $f(x)$ and continuity of $f'(x)$: same thing or different?,Differentiability of  and continuity of : same thing or different?,f(x) f'(x),"If a function $f(x)$ is differentiable $f'(a+)=f'(a-)$ at any point $a$ , so does it mean necessarily that $f'(x)$ is continuous at that point. Also vice versa.. if $f'(x)$ is continuous at any point, does it mean $f(x)$ is differentiable at that point? which of above two conclusion is valid ? My main confusion is about following question - $$ f(x)=  \left\{ \begin{array}{lr}       x^2 \sin\left(\frac{1}{x}\right) & x>0 \\       0 & x=0 \\      x^2 \sin\left(\frac{1}{x}\right) & x < 0     \end{array} \right. $$ Here function is differentiable at $x=0$, but $f'(x)$ is not continuous at $x=0$ ...","If a function $f(x)$ is differentiable $f'(a+)=f'(a-)$ at any point $a$ , so does it mean necessarily that $f'(x)$ is continuous at that point. Also vice versa.. if $f'(x)$ is continuous at any point, does it mean $f(x)$ is differentiable at that point? which of above two conclusion is valid ? My main confusion is about following question - $$ f(x)=  \left\{ \begin{array}{lr}       x^2 \sin\left(\frac{1}{x}\right) & x>0 \\       0 & x=0 \\      x^2 \sin\left(\frac{1}{x}\right) & x < 0     \end{array} \right. $$ Here function is differentiable at $x=0$, but $f'(x)$ is not continuous at $x=0$ ...",,"['calculus', 'derivatives']"
37,Wirtinger Matrix Derivative Chain rule,Wirtinger Matrix Derivative Chain rule,,"I'm trying to compute the matrix Wirtinger derivative $$\frac{\partial (f\circ g)(Z)}{\partial Z}$$ where $g(Z) := B(A Z-Z A)$ and $f(g(Z)):= \mathrm{Tr}\left(\sqrt{g(Z)^* g(Z)}\right)$ . Here $Z$ is a complex Hermitian (also positive) matrix and $f:\mathbb{C}^{n\times n} \mapsto \mathbb{C}^{n\times n}$ is the nuclear or Ky Fan norm. I'm stuck with applying the chain rule: I'm not sure how to contract the tensors arising from $\frac{\partial g(Z)}{\partial Z}$ (which is a fourth-order tensor according to MatrixCalculus.org ) with the matrix $\frac{\partial f(W)}{\partial W}\big{\vert}_{W=g(Z)}$ . I know from this answer that $$\frac{\partial f(W)}{\partial W} = W(W^TW)^{-1/2} \, .$$ Could I get some help on this? I can derive $$\frac{\partial \mathrm{Tr}(AZ)}{\partial Z} = A^T$$ using the chain rule, but I can't extend it to this $f\circ g$ .","I'm trying to compute the matrix Wirtinger derivative where and . Here is a complex Hermitian (also positive) matrix and is the nuclear or Ky Fan norm. I'm stuck with applying the chain rule: I'm not sure how to contract the tensors arising from (which is a fourth-order tensor according to MatrixCalculus.org ) with the matrix . I know from this answer that Could I get some help on this? I can derive using the chain rule, but I can't extend it to this .","\frac{\partial (f\circ g)(Z)}{\partial Z} g(Z) := B(A Z-Z A) f(g(Z)):= \mathrm{Tr}\left(\sqrt{g(Z)^* g(Z)}\right) Z f:\mathbb{C}^{n\times n} \mapsto \mathbb{C}^{n\times n} \frac{\partial g(Z)}{\partial Z} \frac{\partial f(W)}{\partial W}\big{\vert}_{W=g(Z)} \frac{\partial f(W)}{\partial W} = W(W^TW)^{-1/2} \, . \frac{\partial \mathrm{Tr}(AZ)}{\partial Z} = A^T f\circ g","['complex-analysis', 'derivatives', 'matrix-calculus', 'chain-rule']"
38,Show that $f(x)=\int_{0}^{x}\sin(t)g(x-t)dt$ is 2 times differentiable and that $f''+f=g$ [duplicate],Show that  is 2 times differentiable and that  [duplicate],f(x)=\int_{0}^{x}\sin(t)g(x-t)dt f''+f=g,"This question already has answers here : Differential Equation: Am I missing a trick? (3 answers) Closed last month . Given g(x) a continuous function on $\mathbb{R}$ , show that $f(x)=\int_{0}^{x}\sin(t)g(x-t)dt$ is 2 times differentiable and that $f''+f=g$ . This problem reminds me another one where $$f(x)=\int_{0}^{x}\sin(t)(x-t)dt$$ and so $$f'(x)=\left(\int_{0}^{x}\sin(t)xdt-\int_{0}^{x}\sin(t)tdt\right)'= \int_{0}^{x}\sin(t)dt+x\sin(x)-\sin(x)x=-\cos(x)+1$$ But here, we do not have a way to split the $g(x-t)$ .","This question already has answers here : Differential Equation: Am I missing a trick? (3 answers) Closed last month . Given g(x) a continuous function on , show that is 2 times differentiable and that . This problem reminds me another one where and so But here, we do not have a way to split the .",\mathbb{R} f(x)=\int_{0}^{x}\sin(t)g(x-t)dt f''+f=g f(x)=\int_{0}^{x}\sin(t)(x-t)dt f'(x)=\left(\int_{0}^{x}\sin(t)xdt-\int_{0}^{x}\sin(t)tdt\right)'= \int_{0}^{x}\sin(t)dt+x\sin(x)-\sin(x)x=-\cos(x)+1 g(x-t),"['integration', 'derivatives']"
39,Minimize $||A-AWW^TA^T||_F$ w.r.t. $W$,Minimize  w.r.t.,||A-AWW^TA^T||_F W,"Given $n \in \mathbb{N}$ and $A \in \{0,1\}^{n \times n}$ , we aim to find $$\arg \min_{W \in \mathbb{R}^{n \times n}} f(W) = ||A-AWW^TA^T||_F,$$ where $||\cdot||_F$ represents the Frobenius norm with $$||X||_F = \sqrt{\sum_{i,j} {X^2_{ij}}}.$$ What I have tried Taking the derivative, we have $$\frac{\partial f^2(W)}{\partial W} = A^T (A + A^T - 2AWW^TA^T) AW$$ and we need $$A^T (A + A^T - 2AWW^TA^T) AW = 0.$$ $W = 0$ satisfies it but it is in general not the optimum.","Given and , we aim to find where represents the Frobenius norm with What I have tried Taking the derivative, we have and we need satisfies it but it is in general not the optimum.","n \in \mathbb{N} A \in \{0,1\}^{n \times n} \arg \min_{W \in \mathbb{R}^{n \times n}} f(W) = ||A-AWW^TA^T||_F, ||\cdot||_F ||X||_F = \sqrt{\sum_{i,j} {X^2_{ij}}}. \frac{\partial f^2(W)}{\partial W} = A^T (A + A^T - 2AWW^TA^T) AW A^T (A + A^T - 2AWW^TA^T) AW = 0. W = 0","['linear-algebra', 'derivatives', 'optimization', 'matrix-decomposition']"
40,How to evaluate $\lim\limits_{x\to 0} \frac{d}{dx} \frac{e^{-ax}-e^{-bx}}{x} $ using integral?,How to evaluate  using integral?,\lim\limits_{x\to 0} \frac{d}{dx} \frac{e^{-ax}-e^{-bx}}{x} ,I tried to find this limit $$ \Omega = \lim_{x\to 0} \frac{d}{dx} \frac{e^{-ax}-e^{-bx}}{x} $$ without using series or finding derivative of that function So I tried to use the integral $$ \int_0^\infty e^{-tx} dt = \frac{1}{x} $$ then $$ \Omega = \lim_{x\to 0} \frac{d}{dx} (e^{-ax}-e^{-bx}) \int_0^\infty e^{-tx} dt $$ now the way that I think its wrong $$ \Omega = \int_0^\infty \lim_{x\to 0} \frac{d}{dx} \left(e^{-(a+t)x}-e^{-(b+t)x}\right) dt $$ So $$ \Omega = \int_0^\infty \left(-(a+t)+(b+t) \right) dt $$ which is diverge . So how can I use the integral correctly ? and why its wrong ? also can we get the limit for the second derivative by integral ?,I tried to find this limit without using series or finding derivative of that function So I tried to use the integral then now the way that I think its wrong So which is diverge . So how can I use the integral correctly ? and why its wrong ? also can we get the limit for the second derivative by integral ?, \Omega = \lim_{x\to 0} \frac{d}{dx} \frac{e^{-ax}-e^{-bx}}{x}   \int_0^\infty e^{-tx} dt = \frac{1}{x}   \Omega = \lim_{x\to 0} \frac{d}{dx} (e^{-ax}-e^{-bx}) \int_0^\infty e^{-tx} dt   \Omega = \int_0^\infty \lim_{x\to 0} \frac{d}{dx} \left(e^{-(a+t)x}-e^{-(b+t)x}\right) dt   \Omega = \int_0^\infty \left(-(a+t)+(b+t) \right) dt ,"['integration', 'limits', 'derivatives', 'infinity']"
41,does Integrating both sides of an equation in dx will Invalidates the equality?,does Integrating both sides of an equation in dx will Invalidates the equality?,,"I'm struggling to grasp the justification behind integrating both sides of an equation. While I understand that operations can be applied to both sides, maintaining equality, it appears that this principle doesn't apply here. given the equality $x=y$ integrating both sides by dx would give $\frac{x^2}{2} = xy$ but this seems not to be valid since if I start from x=y=5 , I would get $\frac{25}{2}=25 $ that's not true. given for instance $log(a)=log(b)$ if I integrate both sides by $da$ I get: $alog(a)-a=log(b)a$ if $a=b=2$ then $alog(a)-a=-0.61..$ and $log(b)a=1.38$ that are different, what I'm doing wrong here?","I'm struggling to grasp the justification behind integrating both sides of an equation. While I understand that operations can be applied to both sides, maintaining equality, it appears that this principle doesn't apply here. given the equality integrating both sides by dx would give but this seems not to be valid since if I start from x=y=5 , I would get that's not true. given for instance if I integrate both sides by I get: if then and that are different, what I'm doing wrong here?",x=y \frac{x^2}{2} = xy \frac{25}{2}=25  log(a)=log(b) da alog(a)-a=log(b)a a=b=2 alog(a)-a=-0.61.. log(b)a=1.38,"['real-analysis', 'calculus', 'integration', 'derivatives', 'logarithms']"
42,I was trying to understand QUAKE III fast inverse square root alg and i want to find best 'u' value in $\log_2(x+1)≈x+u$ approximation,I was trying to understand QUAKE III fast inverse square root alg and i want to find best 'u' value in  approximation,\log_2(x+1)≈x+u,I was trying to find best 'u' value for this approximation: $\log_2(x+1)≈x+u$ And I did think I can calculate error with this function. NOTE: for the x values between 0 and 1 i need becouse of IEEE 754 float point uses a scientific notation that (1 + x) * 2^n that 0 <= x < 1 $f(u)=\int_{0}^{1}|\log_2(x+1)-(x+u)|dx$ Then I wanted to find when the slope becomes zero. (means local min in this situation) $\frac{d}{du}f(u)=0$ $\frac{d}{du}(\int_{0}^{1}|\log_2(x+1)-(x+u)|dx)=0$ Then I get stuck what can I even do after that?,I was trying to find best 'u' value for this approximation: And I did think I can calculate error with this function. NOTE: for the x values between 0 and 1 i need becouse of IEEE 754 float point uses a scientific notation that (1 + x) * 2^n that 0 <= x < 1 Then I wanted to find when the slope becomes zero. (means local min in this situation) Then I get stuck what can I even do after that?,\log_2(x+1)≈x+u f(u)=\int_{0}^{1}|\log_2(x+1)-(x+u)|dx \frac{d}{du}f(u)=0 \frac{d}{du}(\int_{0}^{1}|\log_2(x+1)-(x+u)|dx)=0,"['integration', 'derivatives', 'algorithms', 'logarithms']"
43,How to understand the differential is a linear map? [closed],How to understand the differential is a linear map? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 months ago . The community reviewed whether to reopen this question 3 months ago and left it closed: Original close reason(s) were not resolved Improve this question I read the following claim in the book , P19 Eq.(3.9). For a smooth function $f:\mathcal{E} \rightarrow R$ , where $\mathcal{E}$ is a linear space. $Df(x): \mathcal{E}\rightarrow R$ is the differential of $f$ at $x$ , that is, it is the linear map defined by: $$Df(x)[v]=\lim\limits_{t\rightarrow 0} \frac{f(x+tv)-f(x)}{t}.$$ Is this the standard definition of differential? Is the claim "" $Df(x)$ is a linear map"" inferred from the limit expression?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 months ago . The community reviewed whether to reopen this question 3 months ago and left it closed: Original close reason(s) were not resolved Improve this question I read the following claim in the book , P19 Eq.(3.9). For a smooth function , where is a linear space. is the differential of at , that is, it is the linear map defined by: Is this the standard definition of differential? Is the claim "" is a linear map"" inferred from the limit expression?",f:\mathcal{E} \rightarrow R \mathcal{E} Df(x): \mathcal{E}\rightarrow R f x Df(x)[v]=\lim\limits_{t\rightarrow 0} \frac{f(x+tv)-f(x)}{t}. Df(x),"['calculus', 'linear-algebra', 'analysis', 'derivatives', 'linear-transformations']"
44,Big O Notation and Derivative for Proof,Big O Notation and Derivative for Proof,,"Show that $\frac{f(a+h)-f(a-h)}{2h}-f'(a)=\mathcal O(h^2)$ if, on open interval containing $a$ , $f$ has continuous derivative up to third derivative By using L'Hospital rule, $\lim_{h\to 0} \frac{f(a+h)-f(a-h)}{2h}=\lim_{h \to 0} \frac{f'(a+h)+f'(a-h)}{2}=\frac{2f'(a)}{2}=f'(a)$ But the question specifically states derivatives up to the third one. How to incorporate the second and third derivative in the proving?","Show that if, on open interval containing , has continuous derivative up to third derivative By using L'Hospital rule, But the question specifically states derivatives up to the third one. How to incorporate the second and third derivative in the proving?",\frac{f(a+h)-f(a-h)}{2h}-f'(a)=\mathcal O(h^2) a f \lim_{h\to 0} \frac{f(a+h)-f(a-h)}{2h}=\lim_{h \to 0} \frac{f'(a+h)+f'(a-h)}{2}=\frac{2f'(a)}{2}=f'(a),"['calculus', 'derivatives']"
45,Some problem when solving the second derivative of graph expressed by parameter equation,Some problem when solving the second derivative of graph expressed by parameter equation,,"For example, a graph expressed by this $$ x=sin(t) $$ $$ y=e^{t}+1 $$ What my teacher told me to solve $ \frac{\mathrm{d^{2}} y}{{dx}^{2}}  $ is like: $$ \frac{\mathrm{d^{2}} y}{{dx}^{2}}=\frac{\mathrm{d}\left ( \frac{\mathrm{d} y}{\mathrm{d} x}  \right ) }{\mathrm{d} x}=\frac{\mathrm{d}\left ( \frac{\mathrm{d} y}{\mathrm{d} t}\frac{\mathrm{d} t}{\mathrm{d} x}  \right ) }{\mathrm{d} x}=\frac{\mathrm{d}\left ( \frac{e^{t}}{cos(t)}  \right ) }{\mathrm{d} x}=\frac{\mathrm{d}\left ( \frac{e^{t}}{cos(t)}  \right ) }{\mathrm{d} t} \frac{\mathrm{d} t}{\mathrm{d} x}=\frac{e^{t}cos(t)+e^{t}sin(t)}{{cos(t)}^3} $$ I want to ask, why can't I do this question like this: $$ \frac{\mathrm{d^{2}} y}{{dx}^{2}}=\frac{\mathrm{d^{2}} y}{{dt}^{2}}\frac{1}{(\frac{\mathrm{d} x}{\mathrm{d} t})^{2}}=\frac{e^{t}}{{cos(t)}^{2}} $$ why do I have to solve this question using the first way? And I also want to ask about the geometric meaning, just like the first derivative of a graph expressed by parameter equation is the slope of the tangent line, of the second derivative.","For example, a graph expressed by this What my teacher told me to solve is like: I want to ask, why can't I do this question like this: why do I have to solve this question using the first way? And I also want to ask about the geometric meaning, just like the first derivative of a graph expressed by parameter equation is the slope of the tangent line, of the second derivative.", x=sin(t)   y=e^{t}+1   \frac{\mathrm{d^{2}} y}{{dx}^{2}}    \frac{\mathrm{d^{2}} y}{{dx}^{2}}=\frac{\mathrm{d}\left ( \frac{\mathrm{d} y}{\mathrm{d} x}  \right ) }{\mathrm{d} x}=\frac{\mathrm{d}\left ( \frac{\mathrm{d} y}{\mathrm{d} t}\frac{\mathrm{d} t}{\mathrm{d} x}  \right ) }{\mathrm{d} x}=\frac{\mathrm{d}\left ( \frac{e^{t}}{cos(t)}  \right ) }{\mathrm{d} x}=\frac{\mathrm{d}\left ( \frac{e^{t}}{cos(t)}  \right ) }{\mathrm{d} t} \frac{\mathrm{d} t}{\mathrm{d} x}=\frac{e^{t}cos(t)+e^{t}sin(t)}{{cos(t)}^3}   \frac{\mathrm{d^{2}} y}{{dx}^{2}}=\frac{\mathrm{d^{2}} y}{{dt}^{2}}\frac{1}{(\frac{\mathrm{d} x}{\mathrm{d} t})^{2}}=\frac{e^{t}}{{cos(t)}^{2}} ,"['calculus', 'derivatives']"
46,Is the Weierstrass function strictly non-monotonic across every interval?,Is the Weierstrass function strictly non-monotonic across every interval?,,"I have a strong suspicion that the Weierstrass function is non-monotonic across every interval, though this suspicion is rooted in the visualization of its generation. I don't know how to prove it, nor am I certain about whether the non-monotonicity is strict or not, though I suspect it is strict. One way to prove strict non-monotonicity across every interval would be to prove the following, which seems to me to be a simple target: $$\forall x,y \in \Bbb R\biggr( W(x) \ne W(y) \iff \exists z \in [x,y] \biggr [W(z) < \min(W(x), W(y)) \lor W(z) > \max(W(x),W(y)) \biggr] \biggr)$$ Perhaps another way to prove it would be via the proof for the nowhere-differentiability of the Weierstrass function, because maybe nowhere-differentiability implies strict non-monotonicity across every interval? The Weierstrass function","I have a strong suspicion that the Weierstrass function is non-monotonic across every interval, though this suspicion is rooted in the visualization of its generation. I don't know how to prove it, nor am I certain about whether the non-monotonicity is strict or not, though I suspect it is strict. One way to prove strict non-monotonicity across every interval would be to prove the following, which seems to me to be a simple target: Perhaps another way to prove it would be via the proof for the nowhere-differentiability of the Weierstrass function, because maybe nowhere-differentiability implies strict non-monotonicity across every interval? The Weierstrass function","\forall x,y \in \Bbb R\biggr( W(x) \ne W(y) \iff \exists z \in [x,y] \biggr [W(z) < \min(W(x), W(y)) \lor W(z) > \max(W(x),W(y)) \biggr] \biggr)","['derivatives', 'intuition', 'fractals']"
47,Calc 1 : maximum and minimum values : finding t in this equation(confused about what I did vs. what my teacher did),Calc 1 : maximum and minimum values : finding t in this equation(confused about what I did vs. what my teacher did),,"Calculus 1: Section 4.1 Maximum and Minimum Values Question: Find the critical numbers of the function $$f(t) = 2\cos(t) + \sin^2 (t).$$ What I'm stuck with about the questions is after getting the equation $$-2\sin(t) + 2\sin(t)\cos(t) = 0,$$ I added $2\sin(t)$ to both sides and solved it for $t$ , in order to find the critical numbers. I got $cos(t) = 1$ which I know that $t = 0$ would be the only critical number for this equation. But my teacher did something different: $$-2\sin t(1 - \cos t) = 0$$ and got different results (2 critical numbers). Can you please explain why my solution doesn't work, and his is correct?","Calculus 1: Section 4.1 Maximum and Minimum Values Question: Find the critical numbers of the function What I'm stuck with about the questions is after getting the equation I added to both sides and solved it for , in order to find the critical numbers. I got which I know that would be the only critical number for this equation. But my teacher did something different: and got different results (2 critical numbers). Can you please explain why my solution doesn't work, and his is correct?","f(t) = 2\cos(t) + \sin^2 (t). -2\sin(t) + 2\sin(t)\cos(t) = 0, 2\sin(t) t cos(t) = 1 t = 0 -2\sin t(1 - \cos t) = 0","['calculus', 'derivatives']"
48,Is the optimization of a symmetric function a symmetric solution?,Is the optimization of a symmetric function a symmetric solution?,,"Under the assumption that $x^2+y^2=c^2$ I minimized the functions $P=x^{-1}+y^{-1}$ and $Q=x^{-2}+y^{-2}$ and for both the solution was $x^2=y^2=c^2/2$ . I noticed that the condition equation and the function to be optimized were both symmetric in $x$ and $y$ . Let $F(x,y)$ and $G(x,y)$ be symmetric functions. If we have the condition that $F(x,y)=C$ and we solve the equation $\dfrac{d}{dx}G(x,y)=0$ will $x=y$ be a solution?",Under the assumption that I minimized the functions and and for both the solution was . I noticed that the condition equation and the function to be optimized were both symmetric in and . Let and be symmetric functions. If we have the condition that and we solve the equation will be a solution?,"x^2+y^2=c^2 P=x^{-1}+y^{-1} Q=x^{-2}+y^{-2} x^2=y^2=c^2/2 x y F(x,y) G(x,y) F(x,y)=C \dfrac{d}{dx}G(x,y)=0 x=y","['calculus', 'derivatives', 'optimization']"
49,Doubt on the differentiability of this function over $\mathbb{R}$,Doubt on the differentiability of this function over,\mathbb{R},"The exercise asks me to show if $f(x) = x^2\vert x \vert $ is differentiabile over $\mathbb{R}$ . Here is my work. $$\lim_{h\to 0} \dfrac{(x+h)^2\vert x+h\vert - x^2\vert x \vert}{h}$$ $$\lim_{h\to 0} \frac{x^2\vert x+h\vert + h^2\vert x+h\vert + 2xh \vert x+h\vert - x^2\vert x\vert}{h}$$ $$\lim_{h\to 0} \frac{x^2(\vert x+h\vert - \vert x \vert) + h\vert x+h\vert(h + 2x)}{h}$$ $$\lim_{h\to 0} \frac{x^2(\vert x+h\vert - \vert x \vert)(\vert x+h\vert + \vert x \vert)}{h(\vert x+h\vert + \vert x \vert )} + \lim_{h\to 0} \vert x+h\vert(h + 2x)$$ The second piece goes to $2x \vert x \vert$ hence it's ok. About the first one: $$\lim_{h\to 0} \frac{x^2(\vert x+h\vert^2 - \vert x \vert ^2)}{h(\vert x+h\vert + \vert x \vert)} = \lim_{h\to 0} \frac{x^2(h + 2x)}{\vert x+h\vert + \vert x \vert}$$ Here is where I got stuck, because I don't know how to proceed exactly between those two ways: I go on, finding $\lim_{h\to 0} x^2 \frac{2x}{\vert 2x \vert}$ hence the problem is at $x = 0$ , even if there isn't actually a problem, hence it's differentiable over $\mathbb{R}$ I calculate both the limits for $h\to 0^+$ and $h\to 0^-$ , noticing the only problem occurs at $x = 0$ , but again the $x$ simplify. Seems like in both cases I get $x^2 \text{sgn}(x)$ . Which by the way completes the right derivative which is $2x\vert x \vert + x^2 \text{sgn}(x)$ Can you tell me if I'm wrong, where and how to remedy? Thank you so much!","The exercise asks me to show if is differentiabile over . Here is my work. The second piece goes to hence it's ok. About the first one: Here is where I got stuck, because I don't know how to proceed exactly between those two ways: I go on, finding hence the problem is at , even if there isn't actually a problem, hence it's differentiable over I calculate both the limits for and , noticing the only problem occurs at , but again the simplify. Seems like in both cases I get . Which by the way completes the right derivative which is Can you tell me if I'm wrong, where and how to remedy? Thank you so much!",f(x) = x^2\vert x \vert  \mathbb{R} \lim_{h\to 0} \dfrac{(x+h)^2\vert x+h\vert - x^2\vert x \vert}{h} \lim_{h\to 0} \frac{x^2\vert x+h\vert + h^2\vert x+h\vert + 2xh \vert x+h\vert - x^2\vert x\vert}{h} \lim_{h\to 0} \frac{x^2(\vert x+h\vert - \vert x \vert) + h\vert x+h\vert(h + 2x)}{h} \lim_{h\to 0} \frac{x^2(\vert x+h\vert - \vert x \vert)(\vert x+h\vert + \vert x \vert)}{h(\vert x+h\vert + \vert x \vert )} + \lim_{h\to 0} \vert x+h\vert(h + 2x) 2x \vert x \vert \lim_{h\to 0} \frac{x^2(\vert x+h\vert^2 - \vert x \vert ^2)}{h(\vert x+h\vert + \vert x \vert)} = \lim_{h\to 0} \frac{x^2(h + 2x)}{\vert x+h\vert + \vert x \vert} \lim_{h\to 0} x^2 \frac{2x}{\vert 2x \vert} x = 0 \mathbb{R} h\to 0^+ h\to 0^- x = 0 x x^2 \text{sgn}(x) 2x\vert x \vert + x^2 \text{sgn}(x),"['calculus', 'analysis', 'derivatives']"
50,Very complicated differentiation,Very complicated differentiation,,"I am trying to solve all steps in a economics paper , but after spending two days with the same differentiation Im losing faith. Can someone out there help me? The problem: Differentiate: $$ \begin{align} &R_{pg}(w) =  \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)w \\ &+\left( \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)\lambda_{pp} - \lambda_{gp}\right)  \int_{w}^{\infty} W'_{p}(x)(1 - F_{p}(x)) \, dx \\ &+\left( \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)\lambda_{pg} - \lambda_{gg}\right)  \int_{R_{pg}(w)}^{\infty} W'_{g}(x)(1 - F_{g}(x)) \, dx \end{align} $$ additional info: $$ W'_{p}(x) = \left[r + \mu + \delta_{p} + \lambda_{pp} (1 - F_{p}(w)) + \lambda_{pg} (1 - F_{g}(R_{pg}(w)))\right]^{-1} $$ $$ W'_{g}(x) = \left[r + \mu + \delta_{g} + \lambda_{gp} (1 - F_{p}(R_{gp}(w))) + \lambda_{gg} (1 - F_{g}(w))\right]^{-1} $$ The result should be: $$ R'_{pg}(w) = \frac{r + \mu + \delta_g + \lambda_{gp}(1 - F_{p}(w)) + \lambda_{gg}(1 - F_{g}(R_{pg}(w)))}{r + \mu + \delta_p + \lambda_{pp}(1 - F_{p}(w)) + \lambda_{pg}(1 - F_{g}(R_{pg}(w)))} $$ (I left a couple of terms out as they should not depend on w.)","I am trying to solve all steps in a economics paper , but after spending two days with the same differentiation Im losing faith. Can someone out there help me? The problem: Differentiate: additional info: The result should be: (I left a couple of terms out as they should not depend on w.)","
\begin{align}
&R_{pg}(w) = 
\left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)w \\
&+\left( \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)\lambda_{pp} - \lambda_{gp}\right) 
\int_{w}^{\infty} W'_{p}(x)(1 - F_{p}(x)) \, dx \\
&+\left( \left(\frac{r + \mu + \delta_g}{r + \mu + \delta_p}\right)\lambda_{pg} - \lambda_{gg}\right) 
\int_{R_{pg}(w)}^{\infty} W'_{g}(x)(1 - F_{g}(x)) \, dx
\end{align}
 
W'_{p}(x) = \left[r + \mu + \delta_{p} + \lambda_{pp} (1 - F_{p}(w)) + \lambda_{pg} (1 - F_{g}(R_{pg}(w)))\right]^{-1}
 
W'_{g}(x) = \left[r + \mu + \delta_{g} + \lambda_{gp} (1 - F_{p}(R_{gp}(w))) + \lambda_{gg} (1 - F_{g}(w))\right]^{-1}
 
R'_{pg}(w) = \frac{r + \mu + \delta_g + \lambda_{gp}(1 - F_{p}(w)) + \lambda_{gg}(1 - F_{g}(R_{pg}(w)))}{r + \mu + \delta_p + \lambda_{pp}(1 - F_{p}(w)) + \lambda_{pg}(1 - F_{g}(R_{pg}(w)))}
","['calculus', 'derivatives', 'economics', 'derivations']"
51,Derivative of sum of compositions,Derivative of sum of compositions,,"A follow up to a previous question: Chain rule when sum is differentiable but individual functions are not Take some $g_1, g_2, h_1,h_2: \mathbb{R}\to \mathbb{R}$ that are everywhere twice differentiable and and always have a strictly positive derivative. Take also some $f_1,f_2: \mathbb{R}\to \mathbb{R}$ which need not be differentiable. However, we know that for some $x_0$ : $$ g_1'(f_1(x_0)) = h_1'(f_1(x_0)) >0 $$ $$ g_2'(f_2(x_0)) = h_2'(f_2(x_0)) >0 $$ $$ \frac{d}{dx}[g_1(f_1(x))+ g_2(f_2(x))] \big|_{x=x_0} = 0 $$ (We only know that the above derivative exists at $x_0$ , not everywhere). Can we say the following? $$         \frac{d}{dx}[h_1(f_1(x))+ h_2(f_2(x))] \big|_{x=x_0} = 0 $$ (We do not assume that the derivative right above exists).","A follow up to a previous question: Chain rule when sum is differentiable but individual functions are not Take some that are everywhere twice differentiable and and always have a strictly positive derivative. Take also some which need not be differentiable. However, we know that for some : (We only know that the above derivative exists at , not everywhere). Can we say the following? (We do not assume that the derivative right above exists).","g_1, g_2, h_1,h_2: \mathbb{R}\to \mathbb{R} f_1,f_2: \mathbb{R}\to \mathbb{R} x_0 
g_1'(f_1(x_0)) = h_1'(f_1(x_0)) >0
 
g_2'(f_2(x_0)) = h_2'(f_2(x_0)) >0
 
\frac{d}{dx}[g_1(f_1(x))+ g_2(f_2(x))] \big|_{x=x_0} = 0
 x_0 
        \frac{d}{dx}[h_1(f_1(x))+ h_2(f_2(x))] \big|_{x=x_0} = 0
","['calculus', 'derivatives', 'chain-rule']"
52,"If $f(0)=g(0)=0$ and $\int_0^{\epsilon} f(x)g(x) dx >0$ for any small $\epsilon>0$, do $f(x)$ and $g(x)$ have same sign near $x=0$?","If  and  for any small , do  and  have same sign near ?",f(0)=g(0)=0 \int_0^{\epsilon} f(x)g(x) dx >0 \epsilon>0 f(x) g(x) x=0,"Let $f,g : [0,\infty) \to \mathbb{R}$ be smooth functions such that $f(0)=g(0)=0$ $\int_0^{\epsilon} f(x)g(x)dx >0$ for any small $\epsilon>0$ . Then, I would like to investigate about the sign of $f(x)$ and $g(x)$ near $x=0$ . Is it possible to conclude that there exists some $\epsilon'>0$ such that $f(x)g(x) \geq 0$ for $x \in [0,\epsilon']$ ?","Let be smooth functions such that for any small . Then, I would like to investigate about the sign of and near . Is it possible to conclude that there exists some such that for ?","f,g : [0,\infty) \to \mathbb{R} f(0)=g(0)=0 \int_0^{\epsilon} f(x)g(x)dx >0 \epsilon>0 f(x) g(x) x=0 \epsilon'>0 f(x)g(x) \geq 0 x \in [0,\epsilon']","['real-analysis', 'analysis', 'derivatives', 'inequality', 'monotone-functions']"
53,Finding critical points and inflection points for $f(x)=\frac{2x+5}{3x+1}$,Finding critical points and inflection points for,f(x)=\frac{2x+5}{3x+1},"Does this function $$f(x)=\frac{2x+5}{3x+1}$$ not have any critical points? I have to find the intervals on which $f(x)$ is increasing or decreasing. The first derivative of $f(x)$ is $$f'(x)=-\frac{13}{(3x+1)^2}$$ which is undefined at $x=-1/3$ but that is not in the domain of $f(x)$ , so $x=-1/3$ cannot be a critical point,right? And for what value of x is $f'(x)=0$ ? I also have to find the inflection points of $f(x)$ to fnd the intervals on which $f(x)$ is concave up or down so I found the 2nd derivative of $f(x)$ , $$f''(x)=\frac{78}{(3x+1)^3}$$ but then I got stuck again.","Does this function not have any critical points? I have to find the intervals on which is increasing or decreasing. The first derivative of is which is undefined at but that is not in the domain of , so cannot be a critical point,right? And for what value of x is ? I also have to find the inflection points of to fnd the intervals on which is concave up or down so I found the 2nd derivative of , but then I got stuck again.",f(x)=\frac{2x+5}{3x+1} f(x) f(x) f'(x)=-\frac{13}{(3x+1)^2} x=-1/3 f(x) x=-1/3 f'(x)=0 f(x) f(x) f(x) f''(x)=\frac{78}{(3x+1)^3},"['calculus', 'derivatives', 'continuity']"
54,Prove $ f $ is identically zero if $f(0)=0$ and $|f′(x)|\le|f(x)|$,Prove  is identically zero if  and, f  f(0)=0 |f′(x)|\le|f(x)|,"I'm working on Omar Hijab's Introduction to Calculus and Classical Analysis and meet this question: Let $f : \mathbb{R} → \mathbb{R}$ be differentiable with $f(0) = 0$ and $|f'(x)|≤|f(x)|$ for all $x \in \mathbb{R}$ . Show that $f = 0$ . I know this question has been answered before, but I want to solve it following the book's solution, the solution is: Let $g(x) = f(x)^2$ . Then $g'(x) ≤ 2g(x)$ , so $e^{−2x}g(x)$ has nonpositive derivative and $e^{−2x}g(x) ≤ g(0) = 0$ . Thus $f$ is identically zero. I'm able to get $e^{−2x}g(x)$ has nonpositive derivative, that gives: $$\frac{e^{-2x}g(x)-g(0)}{x} \le 0$$ However I can only see it follows the solution when $ x \ge 0$ , if $x \le 0$ then it becomes: $$e^{−2x}g(x) \ge g(0)$$ What did I miss? Thank you for any help!","I'm working on Omar Hijab's Introduction to Calculus and Classical Analysis and meet this question: Let be differentiable with and for all . Show that . I know this question has been answered before, but I want to solve it following the book's solution, the solution is: Let . Then , so has nonpositive derivative and . Thus is identically zero. I'm able to get has nonpositive derivative, that gives: However I can only see it follows the solution when , if then it becomes: What did I miss? Thank you for any help!",f : \mathbb{R} → \mathbb{R} f(0) = 0 |f'(x)|≤|f(x)| x \in \mathbb{R} f = 0 g(x) = f(x)^2 g'(x) ≤ 2g(x) e^{−2x}g(x) e^{−2x}g(x) ≤ g(0) = 0 f e^{−2x}g(x) \frac{e^{-2x}g(x)-g(0)}{x} \le 0  x \ge 0 x \le 0 e^{−2x}g(x) \ge g(0),"['real-analysis', 'analysis', 'derivatives']"
55,"Suppose $f(x)$ has continuous $(n+1)$-th derivative over $[a,b]$, and $f(a)=f'(a)=\cdots=f^{(n)}(a)=0$, prove an upper bound for $|f(x)|$","Suppose  has continuous -th derivative over , and , prove an upper bound for","f(x) (n+1) [a,b] f(a)=f'(a)=\cdots=f^{(n)}(a)=0 |f(x)|","Problem : Suppose $f(x)$ has continuous $(n+1)$ -th derivative over $[a,b]$ , and $f(a)=f'(a)=\cdots=f^{(n)}(a)=0$ , prove that $$ \max\limits_{a\le x\le b}|f(x)|\le\frac{(b-a)^n}{n!}\int_a^b|f^{(n+1)}(x)|\mathrm{d}x $$ My attempt : Clearly this problem has something to do with Taylor expansion. We take the Taylor expansion of $f(x)$ at $x=a$ : $$ f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1} $$ where $\xi\in(a,x)$ . Also, since $(b-a)^n$ appears in the formula, we consider $$ f(b)=\frac{f^{(n+1)}(\xi_b)}{(n+1)!}(b-a)^{n+1} $$ where $\xi_b\in(a,b)$ . But I don't know how to deal with $\max\limits_{a\le x\le b}|f(x)|$ . I can't think of a formula containing $\max |f(x)|$ . Can you help me?","Problem : Suppose has continuous -th derivative over , and , prove that My attempt : Clearly this problem has something to do with Taylor expansion. We take the Taylor expansion of at : where . Also, since appears in the formula, we consider where . But I don't know how to deal with . I can't think of a formula containing . Can you help me?","f(x) (n+1) [a,b] f(a)=f'(a)=\cdots=f^{(n)}(a)=0 
\max\limits_{a\le x\le b}|f(x)|\le\frac{(b-a)^n}{n!}\int_a^b|f^{(n+1)}(x)|\mathrm{d}x
 f(x) x=a 
f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}
 \xi\in(a,x) (b-a)^n 
f(b)=\frac{f^{(n+1)}(\xi_b)}{(n+1)!}(b-a)^{n+1}
 \xi_b\in(a,b) \max\limits_{a\le x\le b}|f(x)| \max |f(x)|","['calculus', 'derivatives', 'taylor-expansion']"
56,Why is the $f(x)$ differentiable at $x=0$ but $f'(x)$ is not continuous near $0$?,Why is the  differentiable at  but  is not continuous near ?,f(x) x=0 f'(x) 0,"Recently I saw a function from a book which states the function below is differentiable at $x=0$ but its derivative is not continuous at $x=0$ . Source was ""Michael Spivak's Calculus 3E"" pg.177 $$\begin{equation*} f(x)=\begin{cases}       x²\sin\bigg(\frac{1}{x}\bigg) \quad &\text{if} \, x ≠0 \\       0 \quad &\text{if} \, x=0 \\  \end{cases} \end{equation*}$$ So, accordingly; $$\begin{equation*} f'(x)=\begin{cases}       2x\sin\bigg(\frac{1}{x}\bigg)-\cos\bigg(\frac{1}{x}\bigg) \quad &\text{if} \, x ≠0 \\       0 \quad &\text{if} \, x=0 \\  \end{cases} \end{equation*}$$ This shows $f'$ is not continuous at $x=0$ . But I couldn't understand why is this happening. $f'(0)$ was the slope of line formed by points $(0,f(0))$ and $(h,f(h))$ where $h$ is infinitely small and was 0. Shouldn't it equal to slope of the line formed by $(h,f(h))$ and $(k,f(k))$ where $h<k<0$ ? Could you explain me graphically why slope of line formed by $(0,f(0))$ and $(h,f(h))$ is $0$ while slope of line formed by $(h,f(h))$ and $(k,f(k))$ is $≠0$ ? And can you please give me some more examples which is simpler than this? (A case where $f$ is differentiable at $a$ but $f'$ isn't continuous at $a$ )","Recently I saw a function from a book which states the function below is differentiable at but its derivative is not continuous at . Source was ""Michael Spivak's Calculus 3E"" pg.177 So, accordingly; This shows is not continuous at . But I couldn't understand why is this happening. was the slope of line formed by points and where is infinitely small and was 0. Shouldn't it equal to slope of the line formed by and where ? Could you explain me graphically why slope of line formed by and is while slope of line formed by and is ? And can you please give me some more examples which is simpler than this? (A case where is differentiable at but isn't continuous at )","x=0 x=0 \begin{equation*}
f(x)=\begin{cases}
      x²\sin\bigg(\frac{1}{x}\bigg) \quad &\text{if} \, x ≠0 \\
      0 \quad &\text{if} \, x=0 \\
 \end{cases}
\end{equation*} \begin{equation*}
f'(x)=\begin{cases}
      2x\sin\bigg(\frac{1}{x}\bigg)-\cos\bigg(\frac{1}{x}\bigg) \quad &\text{if} \, x ≠0 \\
      0 \quad &\text{if} \, x=0 \\
 \end{cases}
\end{equation*} f' x=0 f'(0) (0,f(0)) (h,f(h)) h (h,f(h)) (k,f(k)) h<k<0 (0,f(0)) (h,f(h)) 0 (h,f(h)) (k,f(k)) ≠0 f a f' a","['calculus', 'derivatives', 'continuity']"
57,Calculus: Differentiable and Continuous Reasoning,Calculus: Differentiable and Continuous Reasoning,,"Let $f(x)=x^{5/3}+b$ when $x<1$ and $f(x)=ax^{4/3}$ when $x\geq 1$ . This question wants to know for what values of $a$ and $b$ is $f(x)$ differentiable for all values of $x$ . When I solved this problem, I set $\frac{d}{dx} [x^{5/3}+b]=\frac{d}{dx}ax^{4/3}$ when $x=1$ . So, $\frac{5}{3}=\frac{4}{3}a\implies a=\frac{5}{4}$ . This implies that $\lim_{x\to 1^-}f'(x)=\lim_{x\to 1^+}f'(x)=c$ for some $c\in \mathbb{R}$ to make the function be differentiable when $x=1$ . When the left hand derivative and the right hand derivative at a point are equal, then the function is said to be differentiable at that point. When $f(x)$ is differentiable at $x=1$ , then we know $f(x)$ is continuous at $x=1$ . This means $\forall b\in \mathbb{R}$ , $f(x)$ should be continuous at $x=1$ . However, this is not the case for certain values of $b$ . So, what is wrong with my reasoning here... Now, the answer requires that $b=\frac{1}{4}$ because the function should be continuous at $x=1$ . This is because when $1^{5/3}+b=a1^{4/3}$ and $a=\frac{5}{4}$ implies $1+b=\frac{5}{4}\implies b=\frac{1}{4}$ .","Let when and when . This question wants to know for what values of and is differentiable for all values of . When I solved this problem, I set when . So, . This implies that for some to make the function be differentiable when . When the left hand derivative and the right hand derivative at a point are equal, then the function is said to be differentiable at that point. When is differentiable at , then we know is continuous at . This means , should be continuous at . However, this is not the case for certain values of . So, what is wrong with my reasoning here... Now, the answer requires that because the function should be continuous at . This is because when and implies .",f(x)=x^{5/3}+b x<1 f(x)=ax^{4/3} x\geq 1 a b f(x) x \frac{d}{dx} [x^{5/3}+b]=\frac{d}{dx}ax^{4/3} x=1 \frac{5}{3}=\frac{4}{3}a\implies a=\frac{5}{4} \lim_{x\to 1^-}f'(x)=\lim_{x\to 1^+}f'(x)=c c\in \mathbb{R} x=1 f(x) x=1 f(x) x=1 \forall b\in \mathbb{R} f(x) x=1 b b=\frac{1}{4} x=1 1^{5/3}+b=a1^{4/3} a=\frac{5}{4} 1+b=\frac{5}{4}\implies b=\frac{1}{4},"['derivatives', 'continuity']"
58,Is $xf'(x)/f(x)$ increasing if $f$ is increasing and convex?,Is  increasing if  is increasing and convex?,xf'(x)/f(x) f,"Let $f:\mathbb{R}^+\to\mathbb{R}^+$ be such that $f(0)=0$ , $f'(x)>0$ and $f''(x)>0$ (increasing and convex). For $K\geq 0$ let \begin{align} g(x) = \frac{xf'(x)}{K+f(x)}. \end{align} Is the function $g$ increasing? It is increasing in standard cases like $f(x)=x^{\alpha}$ for $\alpha\geq 1$ , and $f(x)=e^{x}-1$ . Are there obvious counterexamples? If $K=0$ , then $g'(x)>0$ is equivalent to \begin{align} x\frac{f''(x)}{f'(x)} \geq x\frac{f(x)}{f'(x)} - 1. \end{align} Is this an interpretable condition? I have seen $x\frac{f''(x)}{f'(x)}$ referred to as the curvature of a function.","Let be such that , and (increasing and convex). For let Is the function increasing? It is increasing in standard cases like for , and . Are there obvious counterexamples? If , then is equivalent to Is this an interpretable condition? I have seen referred to as the curvature of a function.","f:\mathbb{R}^+\to\mathbb{R}^+ f(0)=0 f'(x)>0 f''(x)>0 K\geq 0 \begin{align}
g(x) = \frac{xf'(x)}{K+f(x)}.
\end{align} g f(x)=x^{\alpha} \alpha\geq 1 f(x)=e^{x}-1 K=0 g'(x)>0 \begin{align}
x\frac{f''(x)}{f'(x)} \geq x\frac{f(x)}{f'(x)} - 1.
\end{align} x\frac{f''(x)}{f'(x)}","['real-analysis', 'derivatives', 'convex-analysis']"
59,Calculus concept,Calculus concept,,"Let's get straight to the point. In my course we have section of calculus. So if we have a function which have vertical tangent(s). Can we evaluate it's antiderivative. As we were told derivatives DNE for discontinuity, LHD not equal RHD and vertical tangent. So will it be true?","Let's get straight to the point. In my course we have section of calculus. So if we have a function which have vertical tangent(s). Can we evaluate it's antiderivative. As we were told derivatives DNE for discontinuity, LHD not equal RHD and vertical tangent. So will it be true?",,"['calculus', 'integration', 'derivatives']"
60,Evaluating an Antiderivative to Two Different Functions,Evaluating an Antiderivative to Two Different Functions,,"I have recently been struggling with finding the antiderivative of the rational function $$\int \frac{1}{x \cdot \ln(x^3)}dx$$ I am only recently in calculus, so I expect to have made a basic mistake, which I would like to understand. Initially, when I tried to evaluate this problem, I used the logarithmic power property to rewrite the integrand as $\frac{1}{3 \cdot x \cdot \ln(x)}$ When substituting $u = \ln(x)$ and $du = \frac{1}{x}dx$ , I rewrote the problem as $\int\frac{du}{3 \cdot u}$ . Factoring out the $\frac{1}{3}$ , I then got $\frac{1}{3} \cdot \int \frac{du}{u}$ , which I evaluated to $\frac{\ln|u|}{3} + C$ , or $$\frac{\ln|\ln(x)|}{3} + C$$ However, when I attempted to re-do the problem in a different way (without simplifying $\ln(x^3)$ ), I substituted $u = ln(x^3)$ and $du = \frac{3x^2dx}{x^3}$ , which I simplified to $\frac{3dx}{x}$ . When performing the substitution, I wound up with $\int\frac{du}{3 \cdot u}$ , and, when factoring out the constant, I am left with $\frac{1}{3} \cdot \int\frac{du}{u}$ , which I found the antiderivative to be $\frac{1}{3} \cdot \ln|u| + C$ , and, with a final substitution of $u$ , I am left with $$\frac{\ln|\ln(x^3)|}{3} + C$$ I am struggling to understand how these functions would have the same derivative (assuming I did found the antiderivative correctly both times), as one has $x^3$ and one simply has $x$ . All help is appreciated.","I have recently been struggling with finding the antiderivative of the rational function I am only recently in calculus, so I expect to have made a basic mistake, which I would like to understand. Initially, when I tried to evaluate this problem, I used the logarithmic power property to rewrite the integrand as When substituting and , I rewrote the problem as . Factoring out the , I then got , which I evaluated to , or However, when I attempted to re-do the problem in a different way (without simplifying ), I substituted and , which I simplified to . When performing the substitution, I wound up with , and, when factoring out the constant, I am left with , which I found the antiderivative to be , and, with a final substitution of , I am left with I am struggling to understand how these functions would have the same derivative (assuming I did found the antiderivative correctly both times), as one has and one simply has . All help is appreciated.",\int \frac{1}{x \cdot \ln(x^3)}dx \frac{1}{3 \cdot x \cdot \ln(x)} u = \ln(x) du = \frac{1}{x}dx \int\frac{du}{3 \cdot u} \frac{1}{3} \frac{1}{3} \cdot \int \frac{du}{u} \frac{\ln|u|}{3} + C \frac{\ln|\ln(x)|}{3} + C \ln(x^3) u = ln(x^3) du = \frac{3x^2dx}{x^3} \frac{3dx}{x} \int\frac{du}{3 \cdot u} \frac{1}{3} \cdot \int\frac{du}{u} \frac{1}{3} \cdot \ln|u| + C u \frac{\ln|\ln(x^3)|}{3} + C x^3 x,"['calculus', 'integration', 'derivatives']"
61,Can some please help me in this step? Differentiation [closed],Can some please help me in this step? Differentiation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question ​""I can't understand how they differentiated from step $(1)$ to step $(2)$ "" So wave function is: $$ \varphi_v(x)=\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2y)e^{-y^2/2}$$ To finally calculate value of most probable displacement \begin{align} \frac{d\varphi}{dx} &= 0 \\ \frac{d}{dx}\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2y)e^{-y^2/2} &= 0 \\ \left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2)\frac{d}{dx}(y)e^{-y^2/2} &= 0 \tag{1}\\ \left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}e^{-y^2/2}(1-y^2) &=0 \tag{2}\\ \end{align} So: \begin{align} (1-y^2) &=0 \\ y^2 &= 1 \\ \left(\frac {x}{\alpha}\right) &= 1\\ x &= \pm \alpha \end{align} My question Edited Full question","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question ​""I can't understand how they differentiated from step to step "" So wave function is: To finally calculate value of most probable displacement So: My question Edited Full question","(1) (2)  \varphi_v(x)=\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2y)e^{-y^2/2} \begin{align}
\frac{d\varphi}{dx} &= 0 \\
\frac{d}{dx}\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2y)e^{-y^2/2} &= 0 \\
\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2)\frac{d}{dx}(y)e^{-y^2/2} &= 0 \tag{1}\\
\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}e^{-y^2/2}(1-y^2) &=0 \tag{2}\\
\end{align} \begin{align}
(1-y^2) &=0 \\
y^2 &= 1 \\
\left(\frac {x}{\alpha}\right) &= 1\\
x &= \pm \alpha
\end{align}","['calculus', 'derivatives']"
62,Derive Laplacian in polar coordinates using covariant derivative,Derive Laplacian in polar coordinates using covariant derivative,,"In cartesian coordinates since the metric components $g^{ab}$ are constant, we know that $\partial_c = \nabla_c$ where the RHS is the covariant derivative. So we can write the laplacian in cartesian coordinates as $\nabla^2 \phi = g^{ab} \nabla_a \nabla_b \phi$ . I am told that this is invariant under change of coordinates. How would I calculate this quantity in polar coordinates? The best I've got is $\nabla^2 \phi = \frac{1}{r^2}\nabla_{\theta} \nabla_{\theta} \phi + \nabla_{r} \nabla_{r} \phi$ . I know what the laplacian should look like under polar, but I cannot see how to simplify further from here. I can find the christoffel symbols but how do I apply them to a scalar field?","In cartesian coordinates since the metric components are constant, we know that where the RHS is the covariant derivative. So we can write the laplacian in cartesian coordinates as . I am told that this is invariant under change of coordinates. How would I calculate this quantity in polar coordinates? The best I've got is . I know what the laplacian should look like under polar, but I cannot see how to simplify further from here. I can find the christoffel symbols but how do I apply them to a scalar field?",g^{ab} \partial_c = \nabla_c \nabla^2 \phi = g^{ab} \nabla_a \nabla_b \phi \nabla^2 \phi = \frac{1}{r^2}\nabla_{\theta} \nabla_{\theta} \phi + \nabla_{r} \nabla_{r} \phi,"['differential-geometry', 'coordinate-systems', 'derivatives']"
63,Derivative of Inverse Differential in Proof of Inverse Function Theorem.,Derivative of Inverse Differential in Proof of Inverse Function Theorem.,,"On p. $194$ of Mathematical Analysis by Andrew Browder he defines a map $\psi: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $$\psi(\mathbf{y})=d\mathbf{f}^{-1}_{\mathbf{p}}(\mathbf{y}-\mathbf{q})$$ as a tool to prove the inverse function theorem. He then claims that $$\psi'(\mathbf{q})=[\mathbf{f}'(\mathbf{p})]^{-1},$$ where $\mathbf{f}(\mathbf{p})=\mathbf{q}$ . Why is this the case? Isn't $d\mathbf{f}^{-1}_{\mathbf{p}}$ a linear map, and therefore its derivative should just be itself? But where is the $\mathbf{q}$ on the right hand side of the equation? Many thanks.","On p. of Mathematical Analysis by Andrew Browder he defines a map by as a tool to prove the inverse function theorem. He then claims that where . Why is this the case? Isn't a linear map, and therefore its derivative should just be itself? But where is the on the right hand side of the equation? Many thanks.","194 \psi: \mathbb{R}^n \rightarrow \mathbb{R}^n \psi(\mathbf{y})=d\mathbf{f}^{-1}_{\mathbf{p}}(\mathbf{y}-\mathbf{q}) \psi'(\mathbf{q})=[\mathbf{f}'(\mathbf{p})]^{-1}, \mathbf{f}(\mathbf{p})=\mathbf{q} d\mathbf{f}^{-1}_{\mathbf{p}} \mathbf{q}","['real-analysis', 'derivatives', 'inverse-function-theorem']"
64,What does the second derivative represent in a curve?,What does the second derivative represent in a curve?,,"I have a very simple question If I have a curve represented by four coefficients y= c0+c1x+c2x^2+c3x^3 and I take the derivative of dy/dx in a point, I believe that represents the tangent slope, am I wrong? Anyway, my question is what does the d2y/dx2 represents geometrically?","I have a very simple question If I have a curve represented by four coefficients y= c0+c1x+c2x^2+c3x^3 and I take the derivative of dy/dx in a point, I believe that represents the tangent slope, am I wrong? Anyway, my question is what does the d2y/dx2 represents geometrically?",,"['geometry', 'derivatives', 'curves']"
65,Prove that $\lim_{x \to \infty} \frac{f(x)}{x}$ exists [closed],Prove that  exists [closed],\lim_{x \to \infty} \frac{f(x)}{x},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question This is a problem that has been asked in an old analysis 1 exam. I can't find it anywhere else online. Let $f$ be twice differentiable on $(1,\infty)$ such that $f\geq 0$ and $f''\leq 0$ . Show that $\lim_{x \to \infty } \frac{f(x)}{x} $ exists. I think you have to show that $\frac{f(x)}{x}$ is decreasing using the MVT, but I don't know how to do it.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question This is a problem that has been asked in an old analysis 1 exam. I can't find it anywhere else online. Let be twice differentiable on such that and . Show that exists. I think you have to show that is decreasing using the MVT, but I don't know how to do it.","f (1,\infty) f\geq 0 f''\leq 0 \lim_{x \to \infty } \frac{f(x)}{x}  \frac{f(x)}{x}","['real-analysis', 'limits', 'derivatives']"
66,Is there a uniformly continuous function whose derivative is merely pointwise continuous? [closed],Is there a uniformly continuous function whose derivative is merely pointwise continuous? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Does there exist a function from $\mathbb{R}$ to $\mathbb{R}$ which is uniformly continuous, and whose derivative exists everywhere, and whose derivative is continuous, but not uniformly continuous over the entire real line?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Does there exist a function from to which is uniformly continuous, and whose derivative exists everywhere, and whose derivative is continuous, but not uniformly continuous over the entire real line?",\mathbb{R} \mathbb{R},"['derivatives', 'continuity']"
67,True or False: $f(x)$ is differentiable at $x=0$ and $\lim_{x \to 0}\frac{f(x)}{x}=3 \Rightarrow \; f(0)=0 $ and $f'(0)=3$?,True or False:  is differentiable at  and  and ?,f(x) x=0 \lim_{x \to 0}\frac{f(x)}{x}=3 \Rightarrow \; f(0)=0  f'(0)=3,Prove or contradict: $f(x)$ is differentiable at $x=0$ and $\lim_{x \to 0}\frac{f(x)}{x}=3 \Rightarrow \; f(0)=0 $ and $f'(0)=3$ My answer: True and below the proof: If $f(x)$ is differentiable at $0$ it means $f'(0)$ exists and verifies: $f'(0)=\lim_{x \to 0}\frac{f(x)-f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\frac{f(0)}{x}$ . As by assumption it is given that $\lim_{x \to 0}\frac{f(x)}{x}$ exists and equal to $3$ and because $f(x)$ is differentiable at $x=0$ hence $f(x)$ is continuous at $x=0$ and so $f(0)$ exits it cames by limit arithmetic that: $f'(0)=\lim_{x \to 0}\frac{f(x)-f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\frac{f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\lim_{x \to 0}\frac{f(0)}{x}=3-\lim_{x \to 0}\frac{f(0)}{x}$ Now if $f(0) \neq 0 \Rightarrow \lim_{x \to 0}\frac{f(0)}{x} = +- \infty$ and so in such case $f'(0)$ will not exist. That's why $f(0)=0$ and if $f(0)=0$ it cames naturally that $f'(0)=3$ . Q.E.D Is it correct?,Prove or contradict: is differentiable at and and My answer: True and below the proof: If is differentiable at it means exists and verifies: . As by assumption it is given that exists and equal to and because is differentiable at hence is continuous at and so exits it cames by limit arithmetic that: Now if and so in such case will not exist. That's why and if it cames naturally that . Q.E.D Is it correct?,f(x) x=0 \lim_{x \to 0}\frac{f(x)}{x}=3 \Rightarrow \; f(0)=0  f'(0)=3 f(x) 0 f'(0) f'(0)=\lim_{x \to 0}\frac{f(x)-f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\frac{f(0)}{x} \lim_{x \to 0}\frac{f(x)}{x} 3 f(x) x=0 f(x) x=0 f(0) f'(0)=\lim_{x \to 0}\frac{f(x)-f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\frac{f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\lim_{x \to 0}\frac{f(0)}{x}=3-\lim_{x \to 0}\frac{f(0)}{x} f(0) \neq 0 \Rightarrow \lim_{x \to 0}\frac{f(0)}{x} = +- \infty f'(0) f(0)=0 f(0)=0 f'(0)=3,"['calculus', 'limits', 'derivatives', 'solution-verification']"
68,"Understanding The Fundamental Theorem of Calculus, Part 1","Understanding The Fundamental Theorem of Calculus, Part 1",,"First Part of the Fundamental Theorem of Calculus says that "" the derivative of a definite integral with respect to its upper limit is the integrand evaluated at the upper limit. "" So it means that the antiderivative of integrand evaluated at the upper limit is it's integral? Can you please explain for me how it works? So it says that for $$g(x) = \int_a^x f(t) dt$$ $$\frac{d}{dx}g(x) = f(x)$$ So does it mean that we can find an integral from a to x, by finding antiderivative of f(x)? If so where does a go?","First Part of the Fundamental Theorem of Calculus says that "" the derivative of a definite integral with respect to its upper limit is the integrand evaluated at the upper limit. "" So it means that the antiderivative of integrand evaluated at the upper limit is it's integral? Can you please explain for me how it works? So it says that for So does it mean that we can find an integral from a to x, by finding antiderivative of f(x)? If so where does a go?",g(x) = \int_a^x f(t) dt \frac{d}{dx}g(x) = f(x),"['calculus', 'integration', 'derivatives']"
69,Need advice: what should be my next step for solving the derivative of $f(z)$ using the definition?,Need advice: what should be my next step for solving the derivative of  using the definition?,f(z),"What should be my next step for solving the derivative of $f(z)$ using the definition? $$f(z) = (2{z^2} + 1) \cdot ({z^3} - \sqrt {z}) $$ $$f'(z) = \mathop {\lim }\limits_{\vartriangle z \to 0} \frac{{f(z + \vartriangle z) - f(z)}}{{\vartriangle z}} \\= \mathop {\lim }\limits_{\vartriangle z \to 0} \frac{{[2{{(z + \vartriangle z)}^2} + 1] \cdot [{{(z + \vartriangle z)}^3} - \sqrt {(z + \vartriangle z)} ] - (2{z^2} + 1) \cdot ({z^3} - \sqrt {z}) }}{{\vartriangle z}}$$ At this point I don't know how to proceed: when I try to expand it,  I can't isolate the ∆z to simplify and avoid the division by zero. What am I missing here?","What should be my next step for solving the derivative of using the definition? At this point I don't know how to proceed: when I try to expand it,  I can't isolate the ∆z to simplify and avoid the division by zero. What am I missing here?",f(z) f(z) = (2{z^2} + 1) \cdot ({z^3} - \sqrt {z})  f'(z) = \mathop {\lim }\limits_{\vartriangle z \to 0} \frac{{f(z + \vartriangle z) - f(z)}}{{\vartriangle z}} \\= \mathop {\lim }\limits_{\vartriangle z \to 0} \frac{{[2{{(z + \vartriangle z)}^2} + 1] \cdot [{{(z + \vartriangle z)}^3} - \sqrt {(z + \vartriangle z)} ] - (2{z^2} + 1) \cdot ({z^3} - \sqrt {z}) }}{{\vartriangle z}},"['calculus', 'limits', 'derivatives']"
70,Do tensors in the tangent space act on functions and vectors?,Do tensors in the tangent space act on functions and vectors?,,"I know via isomorphism we may treat the tangent space of a point on a manifold as the vector space of derivations on functions at that point. I.e. we can give the tangent space the basis of partials: $$ v=v^i \partial_i  $$ This allows vectors to act on functions, and also on eachother via the Lie bracket. My question is, for tensor products of the tangent space, can we still act on function or vectors? For a 2-tensor $T$ : $$ T= T^{ij} \partial_i \otimes \partial_j $$ Can $T$ be made to act on functions or vectors in an unambiguous way?","I know via isomorphism we may treat the tangent space of a point on a manifold as the vector space of derivations on functions at that point. I.e. we can give the tangent space the basis of partials: This allows vectors to act on functions, and also on eachother via the Lie bracket. My question is, for tensor products of the tangent space, can we still act on function or vectors? For a 2-tensor : Can be made to act on functions or vectors in an unambiguous way?","
v=v^i \partial_i 
 T 
T= T^{ij} \partial_i \otimes \partial_j
 T","['derivatives', 'differential-geometry', 'lie-algebras', 'tangent-spaces', 'tangent-bundle']"
71,Range of an analytic function on a unit disc,Range of an analytic function on a unit disc,,Let $f(z)$ be an analytic function on an open set of the complex plane containing the closed unit disc $D=\{z\in \mathbb{C}:|z|\leq 1\}$ . Let $m$ be the minimum of $\{f(z)\ |\ z\in D\}$ and $M$ be the minimum of $\{|f(z)|\ |\ z\in C\}$ where $C=\{z\in \mathbb{C}:|z|=1\}$ of $D$ . Assume that $m<M$ . Then state whether the following are true or false? (i) $f(z)$ admits a zero on $D$ . $(ii)$ $f(z)$ attains every complex number $w$ on $D$ such that $|w|<M$ . My attempt: I know that statement $(i)$ is true because of the Minimum modulus principle. But I am not able to find the explanation of statement $(ii)$ . Please help.,Let be an analytic function on an open set of the complex plane containing the closed unit disc . Let be the minimum of and be the minimum of where of . Assume that . Then state whether the following are true or false? (i) admits a zero on . attains every complex number on such that . My attempt: I know that statement is true because of the Minimum modulus principle. But I am not able to find the explanation of statement . Please help.,f(z) D=\{z\in \mathbb{C}:|z|\leq 1\} m \{f(z)\ |\ z\in D\} M \{|f(z)|\ |\ z\in C\} C=\{z\in \mathbb{C}:|z|=1\} D m<M f(z) D (ii) f(z) w D |w|<M (i) (ii),"['complex-analysis', 'derivatives']"
72,"When differentiating $4x^2 + 3y^2 -3xy$ with respect to $x$, why does $3y^2$ 'disappear'?","When differentiating  with respect to , why does  'disappear'?",4x^2 + 3y^2 -3xy x 3y^2,"When differentiating $4x^2 + 3y^2 -3xy$ with respect to $x$ , why does $3y^2$ 'disappear'? Not too sure where this $3y^2$ goes, or why $3xy$ can turn into just $3y$ . Would love an explanation behind this.","When differentiating with respect to , why does 'disappear'? Not too sure where this goes, or why can turn into just . Would love an explanation behind this.",4x^2 + 3y^2 -3xy x 3y^2 3y^2 3xy 3y,"['derivatives', 'economics']"
73,Proof of The Chain Rule,Proof of The Chain Rule,,"I’m self-studying Stewart’s calculus and I went back to review the proof of the chain rule, but I keep getting confused about a particular line. The proof starts as follows: $\Delta y = f(x+\Delta x) - f(x)$ : this part is clear to me. $\displaystyle{\lim_{\Delta x \to 0}} [\Delta y/\Delta x] = f’(a)$ : this part is clear to me. let $\varepsilon$ be a number such that $\varepsilon = (\Delta y/\Delta x) - f’(a)$ : not exactly sure why they decided to go in this direction. How does one go about logically arriving at this step? $\Delta y = \varepsilon \Delta x + \varepsilon f’(a)$ : this is clear to me. If we define $\varepsilon$ to be 0 when $\Delta x = 0$ then $\varepsilon$ becomes a continuous function of $\Delta x$ : I’m also confused here. How can we define $\varepsilon$ to be 0 when $\Delta x = 0$ ? Not sure what that means. Further, if $\Delta x = 0$ then wouldn’t $\varepsilon = 0$ (no change in the difference quotient) $- f’(a) = -f’(a)$ ? #3 and #5 are preventing me from proceeding. Can anyone help out?","I’m self-studying Stewart’s calculus and I went back to review the proof of the chain rule, but I keep getting confused about a particular line. The proof starts as follows: : this part is clear to me. : this part is clear to me. let be a number such that : not exactly sure why they decided to go in this direction. How does one go about logically arriving at this step? : this is clear to me. If we define to be 0 when then becomes a continuous function of : I’m also confused here. How can we define to be 0 when ? Not sure what that means. Further, if then wouldn’t (no change in the difference quotient) ? #3 and #5 are preventing me from proceeding. Can anyone help out?",\Delta y = f(x+\Delta x) - f(x) \displaystyle{\lim_{\Delta x \to 0}} [\Delta y/\Delta x] = f’(a) \varepsilon \varepsilon = (\Delta y/\Delta x) - f’(a) \Delta y = \varepsilon \Delta x + \varepsilon f’(a) \varepsilon \Delta x = 0 \varepsilon \Delta x \varepsilon \Delta x = 0 \Delta x = 0 \varepsilon = 0 - f’(a) = -f’(a),"['calculus', 'derivatives', 'proof-explanation', 'chain-rule']"
74,Find the second derivative of $y=\left(1-2\sqrt{x}\right)^3$,Find the second derivative of,y=\left(1-2\sqrt{x}\right)^3,"Find the second derivative of $$y=\left(1-2\sqrt{x}\right)^3$$ Let's find the first derivative: $$y'=3(1-2\sqrt{x})^2(1-2\sqrt{x})'=3\left(0-2\dfrac{1}{2\sqrt{x}}\right)(1-2\sqrt{x})^2=-3\dfrac{1}{\sqrt{x}}(1-2\sqrt{x})^2$$ The second derivative of a function is the derivative of the derivative of that function, so $$y''=\left(-3\dfrac{1}{\sqrt{x}}(1-2\sqrt{x})^2\right)'=-3\left(\dfrac{1}{\sqrt{x}}\cdot(1-2\sqrt{x})^2\right)'=-3T$$ I am really having troubles with finding that derivative as I get confused (too many things going on). $$T=\left(\dfrac{1}{\sqrt{x}}\right)'(1-2\sqrt{x})^2+\dfrac{1}{\sqrt{x}}\left((1-2\sqrt{x})^2\right)'=\\-\dfrac12x^{-\frac32}(1-4\sqrt{x}+4x)-\dfrac{2}{x}(1-2\sqrt{x})$$ Is there an easier approach?","Find the second derivative of Let's find the first derivative: The second derivative of a function is the derivative of the derivative of that function, so I am really having troubles with finding that derivative as I get confused (too many things going on). Is there an easier approach?",y=\left(1-2\sqrt{x}\right)^3 y'=3(1-2\sqrt{x})^2(1-2\sqrt{x})'=3\left(0-2\dfrac{1}{2\sqrt{x}}\right)(1-2\sqrt{x})^2=-3\dfrac{1}{\sqrt{x}}(1-2\sqrt{x})^2 y''=\left(-3\dfrac{1}{\sqrt{x}}(1-2\sqrt{x})^2\right)'=-3\left(\dfrac{1}{\sqrt{x}}\cdot(1-2\sqrt{x})^2\right)'=-3T T=\left(\dfrac{1}{\sqrt{x}}\right)'(1-2\sqrt{x})^2+\dfrac{1}{\sqrt{x}}\left((1-2\sqrt{x})^2\right)'=\\-\dfrac12x^{-\frac32}(1-4\sqrt{x}+4x)-\dfrac{2}{x}(1-2\sqrt{x}),"['calculus', 'derivatives']"
75,Clarifications on differentiability and continuity,Clarifications on differentiability and continuity,,If there is a function $f(x)$ such that f'(x) has a jump discontinuity: $f(x)=\begin{cases}         2x & 0 \leq x \leq 1 \\         3x-1 & 1 < x \leq 2 \\     \end{cases} $ then what is the derivative at $x=1$ ? The graph looks like this: (orange is $f(x)$ and purple is $f'(x)$ ),If there is a function such that f'(x) has a jump discontinuity: then what is the derivative at ? The graph looks like this: (orange is and purple is ),"f(x) f(x)=\begin{cases}
        2x & 0 \leq x \leq 1 \\
        3x-1 & 1 < x \leq 2 \\
    \end{cases}  x=1 f(x) f'(x)","['calculus', 'derivatives', 'continuity']"
76,Three different results for $\frac{\partial}{\partial p}\left((-\gamma+1)(v+e^\omega\frac{p^{\varepsilon+1}}{\varepsilon+1})\right)^{1/(-\gamma+1)}$ [closed],Three different results for  [closed],\frac{\partial}{\partial p}\left((-\gamma+1)(v+e^\omega\frac{p^{\varepsilon+1}}{\varepsilon+1})\right)^{1/(-\gamma+1)},"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question I need to differentiate this equation wrt. $p$ : $$\frac{\partial}{\partial p}\left((-\gamma+1)\left(v+e^\omega\frac{p^{\varepsilon+1}}{\varepsilon+1}\right)\right)^{1/(-\gamma+1)}$$ See problem as image My supervisor gets this result: $$(-γ+1)^\frac{γ}{-γ+1}\left(v+e^{ω}\frac{p^{ε+1}}{ε+1}\right)^{\frac{γ}{-γ+1}}e^{ω}p^{ε+1}$$ See supervisor's result as image Wolfram Alpha gets this result: See Wolfram Alphas result as image I get this result: $$\left((-γ+1)e^{ω}\frac{p}{ε+1}\right)^{\frac{1}{-γ+1}-1}p^{ε+1}$$ See my result as image Here are my calculations: Steps: 1: Split up all parentheses (multiple inside and then add the potency of the outside parenthesis) $$\frac{∂}{∂p}\left((-γ+1)^{\frac{1}{-γ+1}}v^{\frac{1}{-γ+1}}+(-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}} p^{\frac{ε+1}{-γ+1}}\right)$$ 2: Remove $(-γ+1)^{\frac{1}{-γ+1}}v^{\frac{1}{-γ+1}}$ as it is a constant $$\frac{∂}{∂p}\left((-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}} p^{\frac{ε+1}{-γ+1}}\right)$$ 3: Use rule of derivatives (derivative wrt x: $x^n = nx^{n-1}$ ) $$\left(\frac{ε+1}{-γ+1}\right)(-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}}p^{\frac{ε+1}{-γ+1}-1}$$ 4: Convert the first fraction to factors by using potency rules and use the new form to multiply similar term's potences and isolate the part of the p that doesn't have the same potency in common (sorry if some terms sound strange, I didn't learn math in English) $$(-γ+1)^{\frac{1}{-γ+1}-1}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}-1}p^{\frac{1}{-γ+1}-1}p^{ε+1}$$ 5: Put all with same potency under same potency $$\left((-γ+1)e^{ω}\frac{p}{ε+1}\right)^{\frac{1}{-γ+1}-1}p^{ε+1}$$ Which one is right? What did I do wrong? Any pointers to what method I should be using? See calculations as image","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question I need to differentiate this equation wrt. : See problem as image My supervisor gets this result: See supervisor's result as image Wolfram Alpha gets this result: See Wolfram Alphas result as image I get this result: See my result as image Here are my calculations: Steps: 1: Split up all parentheses (multiple inside and then add the potency of the outside parenthesis) 2: Remove as it is a constant 3: Use rule of derivatives (derivative wrt x: ) 4: Convert the first fraction to factors by using potency rules and use the new form to multiply similar term's potences and isolate the part of the p that doesn't have the same potency in common (sorry if some terms sound strange, I didn't learn math in English) 5: Put all with same potency under same potency Which one is right? What did I do wrong? Any pointers to what method I should be using? See calculations as image",p \frac{\partial}{\partial p}\left((-\gamma+1)\left(v+e^\omega\frac{p^{\varepsilon+1}}{\varepsilon+1}\right)\right)^{1/(-\gamma+1)} (-γ+1)^\frac{γ}{-γ+1}\left(v+e^{ω}\frac{p^{ε+1}}{ε+1}\right)^{\frac{γ}{-γ+1}}e^{ω}p^{ε+1} \left((-γ+1)e^{ω}\frac{p}{ε+1}\right)^{\frac{1}{-γ+1}-1}p^{ε+1} \frac{∂}{∂p}\left((-γ+1)^{\frac{1}{-γ+1}}v^{\frac{1}{-γ+1}}+(-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}} p^{\frac{ε+1}{-γ+1}}\right) (-γ+1)^{\frac{1}{-γ+1}}v^{\frac{1}{-γ+1}} \frac{∂}{∂p}\left((-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}} p^{\frac{ε+1}{-γ+1}}\right) x^n = nx^{n-1} \left(\frac{ε+1}{-γ+1}\right)(-γ+1)^{\frac{1}{-γ+1}}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}}p^{\frac{ε+1}{-γ+1}-1} (-γ+1)^{\frac{1}{-γ+1}-1}e^{ω^{\frac{1}{-γ+1}}}\frac{1}{ε+1}^{\frac{1}{-γ+1}-1}p^{\frac{1}{-γ+1}-1}p^{ε+1} \left((-γ+1)e^{ω}\frac{p}{ε+1}\right)^{\frac{1}{-γ+1}-1}p^{ε+1},"['calculus', 'algebra-precalculus', 'derivatives', 'problem-solving']"
77,"Spivak, Ch. 20, Problem 9c: Understanding comment at end of problem item.","Spivak, Ch. 20, Problem 9c: Understanding comment at end of problem item.",,"(a) Problem $7(i)$ amounts to the equation $$P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g}$$ Give a more direct proof by writing $$f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1}$$ $$g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2}$$ and using the obvious fact about $R_{n,a,f}+R_{n,a,g}$ . (b) Similarly, Problem $7(ii)$ could be used to show that $$P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n$$ where $[P]_n$ denotes the truncation of $P$ to degree $n$ , the sum of all terms of $P$ of degree $\leq n$ [with $P$ written as a polynomial in $x-a$ ]. Again, give a more direct proof, using the obvious facts about products involving terms of the form $R_n$ . (c) Prove that if $p$ and $q$ are polynomials in $x-a$ and $\lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0$ then $$p(q(x)+R(x))=p(q(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0$$ Also note that if $p$ is a polynomial in $x-a$ having only terms of degree $>n$ , and $q$ is a polynomial in $x-a$ whose constant term is $> 0$ , then all terms of $p(q(x-a))$ are of degree $>n$ . My question regards the comment at the end of item $(c)$ . Let $p$ and $q$ be polynomials in $x-a$ such that terms in $p$ have degree $>n$ , and the constant term in $q$ is $0$ . $$p(x)=\sum\limits_{i=n}^{m_1} a_i (x-a)^i$$ $$q(x)=\sum\limits_{i=1}^{m_2} b_i (x-a)^i$$ What is $q(x-a)$ ? Is it $$q(x-a)=\sum\limits_{i=1}^{m_2} b_i (x-2a)^i$$ Or is it just $$q(x-a)=\sum\limits_{i=1}^{m_2} b_i (x-a)^i$$ In the first case we have $$p(q(x-a))=\sum\limits_{i=n}^{m_1} a_i \left ( \sum\limits_{j=1}^{m_2} b_i (x-2a)^j -a \right )^i$$ In the second we have $$p(q(x-a))=\sum\limits_{i=n}^{m_1} a_i \left ( \sum\limits_{j=1}^{m_2} b_i (x-a)^j -a \right )^i$$ Is this sort of what the comment is saying? If so, is the next step to somehow conclude that all $(x-a)$ factors have a degree $>n$ in this second expression?","(a) Problem amounts to the equation Give a more direct proof by writing and using the obvious fact about . (b) Similarly, Problem could be used to show that where denotes the truncation of to degree , the sum of all terms of of degree [with written as a polynomial in ]. Again, give a more direct proof, using the obvious facts about products involving terms of the form . (c) Prove that if and are polynomials in and then where Also note that if is a polynomial in having only terms of degree , and is a polynomial in whose constant term is , then all terms of are of degree . My question regards the comment at the end of item . Let and be polynomials in such that terms in have degree , and the constant term in is . What is ? Is it Or is it just In the first case we have In the second we have Is this sort of what the comment is saying? If so, is the next step to somehow conclude that all factors have a degree in this second expression?","7(i) P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g} f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1} g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2} R_{n,a,f}+R_{n,a,g} 7(ii) P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n [P]_n P n P \leq n P x-a R_n p q x-a \lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0 p(q(x)+R(x))=p(q(x))+\bar{R}(x) \lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0 p x-a >n q x-a > 0 p(q(x-a)) >n (c) p q x-a p >n q 0 p(x)=\sum\limits_{i=n}^{m_1} a_i (x-a)^i q(x)=\sum\limits_{i=1}^{m_2} b_i (x-a)^i q(x-a) q(x-a)=\sum\limits_{i=1}^{m_2} b_i (x-2a)^i q(x-a)=\sum\limits_{i=1}^{m_2} b_i (x-a)^i p(q(x-a))=\sum\limits_{i=n}^{m_1} a_i \left ( \sum\limits_{j=1}^{m_2} b_i (x-2a)^j -a \right )^i p(q(x-a))=\sum\limits_{i=n}^{m_1} a_i \left ( \sum\limits_{j=1}^{m_2} b_i (x-a)^j -a \right )^i (x-a) >n","['calculus', 'integration', 'derivatives', 'solution-verification', 'taylor-expansion']"
78,"Spivak, Ch. 20, Problem *4(i): Write down a sum which equals $\sin{1}$ with an error of less than $10^{-10^{10}}$.","Spivak, Ch. 20, Problem *4(i): Write down a sum which equals  with an error of less than .",\sin{1} 10^{-10^{10}},"In Chapter 20 of Spivak's Calculus , entitled ""Approximation by Polynomial Functions"", in problem 4(i) we are asked to find a polynomial that approximates $\sin{1}$ with an error of less than $10^{-10^{10}}$ . Let me cut to the chase and tell you what my question is. As I show below, there are two very simple answers to this problem (and they are in the solution manual): a Taylor polynomial of order $10^{10^{10}}-1$ , or even better, a Taylor Polynomial of order $10^{10}-1$ . I came up with a different solution that finds a minimum bound for the order of the polynomial as $$10^{10}\frac{\log{10}}{\log{2}}-1\tag{3}$$ My question is: is this answer useful? Since this number is not a natural number, it seems that we'd need to figure out a natural number larger than this to find the Taylor Polynomial required to approximate $\sin{1}$ . Is this easy to do? I will now go through all the steps involved in this problem, and show my attempted solution. If we write $$f(x)=\sin{x}=P_{n,a}(x)+R_{n,a}(x)$$ where $P_{n,a}(x)$ is the Taylor polynomial of degree $n$ for $f$ at $a$ , and $R_{n,a}(x)$ is the remainder term, then by Taylor's Theorem we have $$\sin{x}=\sum\limits_{i=0}^{2n+1} \left [\sin^{(i)}{(a)}\frac{(x-a)^{2n+1}}{(2n+1)!}\right ]+\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}(x-a)^{2n+2}, t\in (a,x)$$ $$R_{2n+1,a}(x)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}(x-a)^{2n+2}, t\in (a,x)$$ $$R_{2n+1,0}(x)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}x^{2n+2}, t\in (0,x)$$ $$R_{2n+1,0}(1)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}, t\in (0,x)$$ $$\leq \frac{1}{(2n+2)!}$$ Therefore, we want $$\frac{1}{(2n+2)!}<10^{-10^{10}}$$ $$\implies (2n+2)!>10^{10^{10}}$$ The solution manual has the following two solutions as possibilities One immediate option is to have $2n+2=10^{10^{10}}$ . This works and we have that $$\sum\limits_{i=0}^{\frac{10^{10^{10}}-2}{2}}(-1)^{i}\frac{1}{i!}$$ is within $10^{-10^{10}}$ of $\sin{1}$ . Another option is to choose $2n+2=10^{10}$ ""since $(10^{10})!$ is clearly larger than $10^{10^{10}}$ "". In this case, we have $$\sum\limits_{i=0}^{\frac{10^{10}-2}{2}}(-1)^{i}\frac{1}{i!}$$ (I missed this last solution though; I asked another question about how to prove that $(10^{10})!$ is larger than $10^{10^{10}}$ ) My solution is as follows For any $a>0$ and $\epsilon>0$ there is some $N$ such that $\frac{a^n}{n!}<\epsilon$ for $n>N$ . Proof Let $n\geq 2a$ . Then $\frac{a^{n+1}}{(n+1)!}=\frac{a}{n+1}\frac{a^n}{n!}<\frac{1}{2}\frac{a^n}{n!}$ . For any $n_0 \in \mathbb{N}$ and $n_0\geq 2a$ we have $$\frac{a^{n_0+1}}{(n_0+1)!}<\frac{1}{2}\frac{a^n_0}{n_0!}$$ $$\frac{a^{n_0+2}}{(n_0+2)!}=\frac{a}{n_0+2}\frac{a^{n_0+1}}{(n_0+1)!}<\frac{1}{2}\frac{1}{2}\frac{a^n_0}{n_0!}$$ Thus $$\frac{a^{n_0+k}}{(n_0+k)!}<\frac{1}{2^k}\frac{a^{n_0}}{n_0!}\tag{1}$$ Now, $\lim\limits_{x\to\infty} 2^{-k}=0$ . This means $$\forall \epsilon_1>0\ \exists N>0\ \forall k, k>N\implies > 2^{-k}<\epsilon_1$$ Let $\epsilon_1=\frac{\epsilon n_0!}{a^{n_0}}$ . Then $\exists N\ > \forall k, k>N\implies 2^{-k}<\frac{\epsilon n_0!}{a^{n_0}}$ . But from $(1)$ this means that $$\frac{a^{n_0+k}}{(n_0+k)!}<\epsilon$$ Note that the $N$ in the proof above is $$2^{-k}<\epsilon \implies 2^k>\frac{1}{\epsilon}\implies k>\log_2(1/\epsilon)=N$$ Back to the main problem, let $2n=k$ , where for the time being these numbers are real. Then we want $$\frac{1}{(2+k)!}<10^{-10^{10}}=\epsilon$$ Let $\epsilon_1=\epsilon\cdot 2!$ . Then $\frac{1}{(2+k)!}<\epsilon$ if $k=2n>\log_2\left [ \frac{1}{\epsilon\cdot 2!} \right ]=\log_2 \left [ \frac{10^{10^{10}}}{2!} \right ]$ That is $$n>\frac{1}{2}\log_2 \left [ \frac{10^{10^{10}}}{2!} \right ] \implies \frac{1}{(2n+2)!}<10^{-10^{10}}\tag{2}$$ Now, this lower bound for $n$ is not a natural number. This chapter is about polynomial approximations, and as far as I have understood this is very practical chapter. With the initial two solutions, we had natural numbers and could compute the sum of polynomial terms with enough computing power. In my proof, I found a minimum value for $n$ that is not a natural number. In fact it is $$10^{10}\frac{\log{10}}{\log{2}}-1\tag{3}$$ which is not better than the solution in the solution manual of a Taylor polynomial of order $10^{10}-1$ . However, I am curious to know if this answer is even acceptable in practical terms. We'd have to compute the logarithmic values, I assume using polynomial approximation as well. Assuming my solution is correct, is it a good answer? By that I mean, can we easily find a natural number larger than the number in $(3)$ ?","In Chapter 20 of Spivak's Calculus , entitled ""Approximation by Polynomial Functions"", in problem 4(i) we are asked to find a polynomial that approximates with an error of less than . Let me cut to the chase and tell you what my question is. As I show below, there are two very simple answers to this problem (and they are in the solution manual): a Taylor polynomial of order , or even better, a Taylor Polynomial of order . I came up with a different solution that finds a minimum bound for the order of the polynomial as My question is: is this answer useful? Since this number is not a natural number, it seems that we'd need to figure out a natural number larger than this to find the Taylor Polynomial required to approximate . Is this easy to do? I will now go through all the steps involved in this problem, and show my attempted solution. If we write where is the Taylor polynomial of degree for at , and is the remainder term, then by Taylor's Theorem we have Therefore, we want The solution manual has the following two solutions as possibilities One immediate option is to have . This works and we have that is within of . Another option is to choose ""since is clearly larger than "". In this case, we have (I missed this last solution though; I asked another question about how to prove that is larger than ) My solution is as follows For any and there is some such that for . Proof Let . Then . For any and we have Thus Now, . This means Let . Then . But from this means that Note that the in the proof above is Back to the main problem, let , where for the time being these numbers are real. Then we want Let . Then if That is Now, this lower bound for is not a natural number. This chapter is about polynomial approximations, and as far as I have understood this is very practical chapter. With the initial two solutions, we had natural numbers and could compute the sum of polynomial terms with enough computing power. In my proof, I found a minimum value for that is not a natural number. In fact it is which is not better than the solution in the solution manual of a Taylor polynomial of order . However, I am curious to know if this answer is even acceptable in practical terms. We'd have to compute the logarithmic values, I assume using polynomial approximation as well. Assuming my solution is correct, is it a good answer? By that I mean, can we easily find a natural number larger than the number in ?","\sin{1} 10^{-10^{10}} 10^{10^{10}}-1 10^{10}-1 10^{10}\frac{\log{10}}{\log{2}}-1\tag{3} \sin{1} f(x)=\sin{x}=P_{n,a}(x)+R_{n,a}(x) P_{n,a}(x) n f a R_{n,a}(x) \sin{x}=\sum\limits_{i=0}^{2n+1} \left [\sin^{(i)}{(a)}\frac{(x-a)^{2n+1}}{(2n+1)!}\right ]+\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}(x-a)^{2n+2}, t\in (a,x) R_{2n+1,a}(x)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}(x-a)^{2n+2}, t\in (a,x) R_{2n+1,0}(x)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}x^{2n+2}, t\in (0,x) R_{2n+1,0}(1)=\frac{\sin^{(2n+2)}{(t)}}{(2n+2)!}, t\in (0,x) \leq \frac{1}{(2n+2)!} \frac{1}{(2n+2)!}<10^{-10^{10}} \implies (2n+2)!>10^{10^{10}} 2n+2=10^{10^{10}} \sum\limits_{i=0}^{\frac{10^{10^{10}}-2}{2}}(-1)^{i}\frac{1}{i!} 10^{-10^{10}} \sin{1} 2n+2=10^{10} (10^{10})! 10^{10^{10}} \sum\limits_{i=0}^{\frac{10^{10}-2}{2}}(-1)^{i}\frac{1}{i!} (10^{10})! 10^{10^{10}} a>0 \epsilon>0 N \frac{a^n}{n!}<\epsilon n>N n\geq 2a \frac{a^{n+1}}{(n+1)!}=\frac{a}{n+1}\frac{a^n}{n!}<\frac{1}{2}\frac{a^n}{n!} n_0 \in \mathbb{N} n_0\geq 2a \frac{a^{n_0+1}}{(n_0+1)!}<\frac{1}{2}\frac{a^n_0}{n_0!} \frac{a^{n_0+2}}{(n_0+2)!}=\frac{a}{n_0+2}\frac{a^{n_0+1}}{(n_0+1)!}<\frac{1}{2}\frac{1}{2}\frac{a^n_0}{n_0!} \frac{a^{n_0+k}}{(n_0+k)!}<\frac{1}{2^k}\frac{a^{n_0}}{n_0!}\tag{1} \lim\limits_{x\to\infty} 2^{-k}=0 \forall \epsilon_1>0\ \exists N>0\ \forall k, k>N\implies
> 2^{-k}<\epsilon_1 \epsilon_1=\frac{\epsilon n_0!}{a^{n_0}} \exists N\
> \forall k, k>N\implies 2^{-k}<\frac{\epsilon n_0!}{a^{n_0}} (1) \frac{a^{n_0+k}}{(n_0+k)!}<\epsilon N 2^{-k}<\epsilon \implies 2^k>\frac{1}{\epsilon}\implies k>\log_2(1/\epsilon)=N 2n=k \frac{1}{(2+k)!}<10^{-10^{10}}=\epsilon \epsilon_1=\epsilon\cdot 2! \frac{1}{(2+k)!}<\epsilon k=2n>\log_2\left [ \frac{1}{\epsilon\cdot 2!} \right ]=\log_2 \left [ \frac{10^{10^{10}}}{2!} \right ] n>\frac{1}{2}\log_2 \left [ \frac{10^{10^{10}}}{2!} \right ] \implies \frac{1}{(2n+2)!}<10^{-10^{10}}\tag{2} n n 10^{10}\frac{\log{10}}{\log{2}}-1\tag{3} 10^{10}-1 (3)","['calculus', 'integration', 'derivatives', 'polynomials', 'taylor-expansion']"
79,High-order complex derivative in MATLAB,High-order complex derivative in MATLAB,,"First derivative can be calculated by the complex-step derivative formula: $f'(x)=\frac{Im(f(x+ih))}{h}$ Generalization of the above for calculating derivatives of any order employs multicomplex numbers, resulting in multicomplex derivatives: $f^{(n)}(x)=\frac{C^{(n)}_{n^2-1}(f(x+i^{(1)}h+i^{(n)}h))}{h^n}$ According to the Wiki Complex-variable methods : In Matlab, the calculation of the first order derivative is very easy to implement: x=0:0.01:10; h=0.001; f=sin(x); df=imag(sin(x+h*i))./h; plot(x,f) hold on plot(x,df) I do not understand how to implement the calculation of the second order derivative, because I do not understand what is $i^{(1)},i^{(2)}...i^{(n)}$ and how the operator $C^{(n)}_{n^2-1}$ is calculated. EDIT: Here is the program for the second order derivative, which is calculated incorrectly. I don't understand how to use $Imag_{12}$ x=0:0.01:15; h=0.0000001; imx1=x+(i)*h; imx2=x+(i+i)*h; f=(x).^2; df = imag((imx1).^2)./h; ddf = imag((imx2).^2)./h^2;","First derivative can be calculated by the complex-step derivative formula: Generalization of the above for calculating derivatives of any order employs multicomplex numbers, resulting in multicomplex derivatives: According to the Wiki Complex-variable methods : In Matlab, the calculation of the first order derivative is very easy to implement: x=0:0.01:10; h=0.001; f=sin(x); df=imag(sin(x+h*i))./h; plot(x,f) hold on plot(x,df) I do not understand how to implement the calculation of the second order derivative, because I do not understand what is and how the operator is calculated. EDIT: Here is the program for the second order derivative, which is calculated incorrectly. I don't understand how to use x=0:0.01:15; h=0.0000001; imx1=x+(i)*h; imx2=x+(i+i)*h; f=(x).^2; df = imag((imx1).^2)./h; ddf = imag((imx2).^2)./h^2;","f'(x)=\frac{Im(f(x+ih))}{h} f^{(n)}(x)=\frac{C^{(n)}_{n^2-1}(f(x+i^{(1)}h+i^{(n)}h))}{h^n} i^{(1)},i^{(2)}...i^{(n)} C^{(n)}_{n^2-1} Imag_{12}","['complex-analysis', 'derivatives', 'numerical-methods', 'matlab']"
80,"Assume $f(x)$ is such that $f'(x)>0$, $s(x)$ only takes two values $1$, $-1$ on $R$. If $g(x)=s(x)f(x)$ is differentiable, can one show $s$ constant?","Assume  is such that ,  only takes two values ,  on . If  is differentiable, can one show  constant?",f(x) f'(x)>0 s(x) 1 -1 R g(x)=s(x)f(x) s,"Assume that $f(x): x\in \mathbb{R}$ differentiable, such that $f'(x)\neq 0$ for every point $x$ such that $f(x) = 0$ , and $s(x)$ only takes two values $1$ and $-1$ on $\mathbb{R}$ . If $g(x)=s(x)f(x)$ is differentiable, can one show that $s(x)$ is constant on $\mathbb{R}$ ? This question arises from a former question of mine . edit: I now think the appropriate statement of the original problem is as follows: Assume that $f(x): x\in \mathbb{R}$ differentiable, such that $f'(x)\neq 0$ for every point $x$ such that $f(x) = 0$ , and $s(x)$ only takes two values $1$ and $-1$ on $\mathbb{R}$ . If $g(x)=s(x)f(x)$ is differentiable, then $s(x)$ is continuous on $\mathbb{R}$ outside a set of removable discontinuous points $S$ , where $S$ is a subset of zeros of $f(x)$ . Both QC_QAOA's answer and mine actually establish the above fact. To provide an example contradicting the original statement: $f(x)=x$ , $s(x)$ takes $1$ for all $x \in \mathbb{R}$ but $-1$ for $x=0$ . Then $x=0$ is a removable discontinuous point of $s$ , and it doesn't sabotage the fact that $s(x)g(x)$ is still differentiable.","Assume that differentiable, such that for every point such that , and only takes two values and on . If is differentiable, can one show that is constant on ? This question arises from a former question of mine . edit: I now think the appropriate statement of the original problem is as follows: Assume that differentiable, such that for every point such that , and only takes two values and on . If is differentiable, then is continuous on outside a set of removable discontinuous points , where is a subset of zeros of . Both QC_QAOA's answer and mine actually establish the above fact. To provide an example contradicting the original statement: , takes for all but for . Then is a removable discontinuous point of , and it doesn't sabotage the fact that is still differentiable.",f(x): x\in \mathbb{R} f'(x)\neq 0 x f(x) = 0 s(x) 1 -1 \mathbb{R} g(x)=s(x)f(x) s(x) \mathbb{R} f(x): x\in \mathbb{R} f'(x)\neq 0 x f(x) = 0 s(x) 1 -1 \mathbb{R} g(x)=s(x)f(x) s(x) \mathbb{R} S S f(x) f(x)=x s(x) 1 x \in \mathbb{R} -1 x=0 x=0 s s(x)g(x),"['calculus', 'derivatives', 'continuity']"
81,Is there an explicit formula for a cosine function of x-coordinate? A question on ideas in Spivak's Calculus chapter on trigonometric functions.,Is there an explicit formula for a cosine function of x-coordinate? A question on ideas in Spivak's Calculus chapter on trigonometric functions.,,"Spivak's Calculus has a Chapter 15 entitled ""Trigonometric Functions"". He starts with a discussion which I found a bit convoluted. I will describe how I understood the flow of ideas and concepts and at the end I will pose two questions that I have. The idea of an angle is as simple as the union of two half-lines with a common initial point. A directed angle can be thought of as a pair of half lines with the same initial point Okay, not sure what the difference is between the two. Next, it seems Spivak tries to motivate what will ultimately be the definitions of sine and cosine with some initial ideas. Idea 1 We can specify a directed angle as a single point on the unit circle, ie a point $(x,y)$ with $x^2+y^2=1$ . We can then define sine and cosine of a directed angle as the $y$ and $x$ coordinates of the point representing the directed angle. If we specify the directed angle by a number (degrees or radians), we could define sine and cosine functions. Given the directed angle as a number, we know the point on the unit circle, given this point we know cosine and sine. In particular, if we measure the directed angle using radians, we are effectively giving each directed angle a number equal to arc length. Hence, for each arc length we get a point, and given a point we get cosine and sine. Okay, so the above is sort of an idea of what would happen if we were to define cosine and sine as simple x and y coordinates of a point on the unit circle, where the point is determined by a directed angle that is measured in radians. Idea 2 What if instead of specifying an arc length, we were to specify an area of a sector on the unit circle (by the way, of course, starting at position $(1,0)$ moving counter-clockwise). Given an arc length $x$ , the sector area is $\frac{x}{2}$ . Thus, instead of saying $\sin x$ and $\cos x$ are the coordinates of the point determined by arc length $x$ , where $x$ is in radians we could say $\sin x$ and $\cos x$ are the coordinates of the point determined by a sector area of $\frac{x}{2}$ , where $x$ is in radians Okay, at this point, the motivational ideas are finished and the formal definitions begin. I assume that at this point the ""definitions"" above were informal, and right now we start again from scratch, with $\cos$ and $\sin$ not yet defined. Formal Definitions First off, $\pi$ is defined as $$\pi=2\int_{-1}^1 \sqrt{1-x^2}dx$$ Then, an area function is defined. $$A(x)=\frac{x\sqrt{1-x^2}}{2}+\int_x^1 \sqrt{1-t^2}dt$$ This is just the function giving the area of a sector of the unit circle given an x-coordinate: This function has certain characteristics: it is continuous on $[-1,1]$ , is decreasing and hence one-one. So at this point $\cos x$ is defined as the unique number in $[-1,1]$ such that $$A(\cos{x})=\frac{x}{2}\tag{1}$$ and $\sin{x}$ is defined as $$\sin{x}=\sqrt{1-\cos^2{x}}$$ Then comes a theorem that shows that $\cos'{x}=-\sin{x}$ and $\sin'{x}=\cos{x}$ . I have two questions Is there a formula for $\cos{x}$ , or is it just defined implicitly as in $(1)$ ? Could we have defined $\cos$ in the following way instead of an area function we define an arc length function that given an x-coordinate would give us the arc-length (would this be done with a line integral?) find the inverse of such a function; ie a function that given an arc length gives us the x-coordinate. define $\cos x$ as the latter function define $\sin x$ as before","Spivak's Calculus has a Chapter 15 entitled ""Trigonometric Functions"". He starts with a discussion which I found a bit convoluted. I will describe how I understood the flow of ideas and concepts and at the end I will pose two questions that I have. The idea of an angle is as simple as the union of two half-lines with a common initial point. A directed angle can be thought of as a pair of half lines with the same initial point Okay, not sure what the difference is between the two. Next, it seems Spivak tries to motivate what will ultimately be the definitions of sine and cosine with some initial ideas. Idea 1 We can specify a directed angle as a single point on the unit circle, ie a point with . We can then define sine and cosine of a directed angle as the and coordinates of the point representing the directed angle. If we specify the directed angle by a number (degrees or radians), we could define sine and cosine functions. Given the directed angle as a number, we know the point on the unit circle, given this point we know cosine and sine. In particular, if we measure the directed angle using radians, we are effectively giving each directed angle a number equal to arc length. Hence, for each arc length we get a point, and given a point we get cosine and sine. Okay, so the above is sort of an idea of what would happen if we were to define cosine and sine as simple x and y coordinates of a point on the unit circle, where the point is determined by a directed angle that is measured in radians. Idea 2 What if instead of specifying an arc length, we were to specify an area of a sector on the unit circle (by the way, of course, starting at position moving counter-clockwise). Given an arc length , the sector area is . Thus, instead of saying and are the coordinates of the point determined by arc length , where is in radians we could say and are the coordinates of the point determined by a sector area of , where is in radians Okay, at this point, the motivational ideas are finished and the formal definitions begin. I assume that at this point the ""definitions"" above were informal, and right now we start again from scratch, with and not yet defined. Formal Definitions First off, is defined as Then, an area function is defined. This is just the function giving the area of a sector of the unit circle given an x-coordinate: This function has certain characteristics: it is continuous on , is decreasing and hence one-one. So at this point is defined as the unique number in such that and is defined as Then comes a theorem that shows that and . I have two questions Is there a formula for , or is it just defined implicitly as in ? Could we have defined in the following way instead of an area function we define an arc length function that given an x-coordinate would give us the arc-length (would this be done with a line integral?) find the inverse of such a function; ie a function that given an arc length gives us the x-coordinate. define as the latter function define as before","(x,y) x^2+y^2=1 y x (1,0) x \frac{x}{2} \sin x \cos x x x \sin x \cos x \frac{x}{2} x \cos \sin \pi \pi=2\int_{-1}^1 \sqrt{1-x^2}dx A(x)=\frac{x\sqrt{1-x^2}}{2}+\int_x^1 \sqrt{1-t^2}dt [-1,1] \cos x [-1,1] A(\cos{x})=\frac{x}{2}\tag{1} \sin{x} \sin{x}=\sqrt{1-\cos^2{x}} \cos'{x}=-\sin{x} \sin'{x}=\cos{x} \cos{x} (1) \cos \cos x \sin x","['calculus', 'integration', 'limits', 'derivatives']"
82,"Why is $\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a}\frac{f'(x)}{g'(x)}$ when $\alpha_x\in (a,x)$ by Cauchy-Schwarz MVT?",Why is  when  by Cauchy-Schwarz MVT?,"\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a}\frac{f'(x)}{g'(x)} \alpha_x\in (a,x)","Let $f$ and $g$ be continuous an differentiable on $[a,x]$ , and let $f(a)=g(a)=0$ , and assume that $g'$ and $g$ are $\neq 0$ on $(a,x]$ . Apply the Cauchy-Schwarz Mean Value Theorem to $f$ and $g$ on $[a,x]$ . Then, there is a number $\alpha_x$ in $(a,x)$ such that $$[f(x)-f(a)]g'(\alpha_x)=[g(x)-g(a)]f'(\alpha_x)$$ $$\frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}\tag{1}$$ $\alpha_x$ approaches $a$ as $x$ approaches $a$ , ie $\alpha_x-a|<|x-a|$ so $0<|x-a|<\delta \implies 0<|\alpha_x-a|<\delta$ . Now assume that $$\lim_{x\to a} \frac{f'(x)}{g'(x)}$$ exists. If we take the limit of $(1)$ as $x\to a$ , apparently we can write the following $$\lim\limits_{x\to a}\frac{f(x)}{g(x)}=\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a} \frac{f'(x)}{g'(x)}\tag{2}$$ I've thought about this way too many hours and I can't convince myself in terms of the $\epsilon$ and $\delta$ definition of limit that we can justify the second equality in $(2)$ . After all, $\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}$ means $$\forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon$$ Now, this also means that $$\forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|\alpha_x-a|<|x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon$$ But even if we write $$\forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|\alpha_x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon$$ this still doesn't fit into the cookie-cutter definition of limit. I would expect to be able to write $\forall \alpha_x$ , but it wouldn't be true I believe. I would really like to understand how and why we can say $$\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a} \frac{f'(x)}{g'(x)}$$ By the way, for context on where this comes from, it is part of the proof of L'Hôpital's Rule (as present in Spivak's Calculus )","Let and be continuous an differentiable on , and let , and assume that and are on . Apply the Cauchy-Schwarz Mean Value Theorem to and on . Then, there is a number in such that approaches as approaches , ie so . Now assume that exists. If we take the limit of as , apparently we can write the following I've thought about this way too many hours and I can't convince myself in terms of the and definition of limit that we can justify the second equality in . After all, means Now, this also means that But even if we write this still doesn't fit into the cookie-cutter definition of limit. I would expect to be able to write , but it wouldn't be true I believe. I would really like to understand how and why we can say By the way, for context on where this comes from, it is part of the proof of L'Hôpital's Rule (as present in Spivak's Calculus )","f g [a,x] f(a)=g(a)=0 g' g \neq 0 (a,x] f g [a,x] \alpha_x (a,x) [f(x)-f(a)]g'(\alpha_x)=[g(x)-g(a)]f'(\alpha_x) \frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}\tag{1} \alpha_x a x a \alpha_x-a|<|x-a| 0<|x-a|<\delta \implies 0<|\alpha_x-a|<\delta \lim_{x\to a} \frac{f'(x)}{g'(x)} (1) x\to a \lim\limits_{x\to a}\frac{f(x)}{g(x)}=\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a} \frac{f'(x)}{g'(x)}\tag{2} \epsilon \delta (2) \lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)} \forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon \forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|\alpha_x-a|<|x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon \forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|\alpha_x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon \forall \alpha_x \lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a} \frac{f'(x)}{g'(x)}","['calculus', 'limits', 'derivatives', 'proof-explanation', 'limits-without-lhopital']"
83,Maximize complicated $U$ with respect to $\theta_0$ to find $S_0$ [closed],Maximize complicated  with respect to  to find  [closed],U \theta_0 S_0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I would like to maximize the function $U$ with respect to $\theta_0$ in order to find $S_0$ . That's, I want to take derivative the function $U$ with respect to $\theta_0$ in order to find $S_0$ . Which program should I use in order to calculate this derivative ? because it is a bit complicated. Is it possible to use Matlab for that? Please help me writing Matlab code for this? Thank you! (Note: Sorry for not writing the function U directly. But, I hope it is readable.)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I would like to maximize the function with respect to in order to find . That's, I want to take derivative the function with respect to in order to find . Which program should I use in order to calculate this derivative ? because it is a bit complicated. Is it possible to use Matlab for that? Please help me writing Matlab code for this? Thank you! (Note: Sorry for not writing the function U directly. But, I hope it is readable.)",U \theta_0 S_0 U \theta_0 S_0,"['calculus', 'derivatives', 'self-learning', 'computational-mathematics']"
84,"(convex function) Let $f: [a,b] \to \mathbb R$ twice differentiable, and $f''(x) \ge 0$, $\forall x \in [a,b]$....","(convex function) Let  twice differentiable, and , ....","f: [a,b] \to \mathbb R f''(x) \ge 0 \forall x \in [a,b]","Let $f: [a,b] \to \mathbb R$ twice differentiable, and $f''(x) \ge 0$ , $\forall x \in [a,b]$ .Prove that $f(\frac{x_1 + x_2} {2}) \le \frac{1}{2}[f(x_1) + f(x_2)], \forall x_1, x_2 \in [a,b].$ My attempt: As a hypothesis we assume that $f''(x) \ge 0$ and we know that .If $f: (a,b) \to \mathbb R$ has second order derivatives on $(a, b)$ the function $f$ is convex if and only if $f''(x) \ge 0$ . If $f’’(x) \gt 0$ , the function $f$ is strictly convex. So $f$ is convex. And by the definition of convex, A function $f: (a,b) \to \mathbb R$ is calling convex on (a,b) if for any $x_1, x_2 \in (a,b)$ and for any pair of real numbers $\alpha_1 \ge 0, \alpha_2 \ge 0$ such that $\alpha_1 + \alpha_2 = 1$ the following inequality $f(\alpha_1x_1 + \alpha_2x_2) \le \alpha_1f(x_1) + \alpha_2f( x_2)$ . So we have: $f(\frac{x_1 + x_2}{2}) \le \frac{1}{2}[f(x_1) + f(x_2)] = f(\frac{1}{2}[x_1]+\frac{1}{2}[x_2])  \le \frac{1}{2}f(x_1) + \frac{1}{2}f(x_2) \to f(\alpha_1x_1 + \alpha_2x_2) \le \alpha_1f(x_1) + \alpha_2f(x_2)$ How is my answer? Thank's for any help","Let twice differentiable, and , .Prove that My attempt: As a hypothesis we assume that and we know that .If has second order derivatives on the function is convex if and only if . If , the function is strictly convex. So is convex. And by the definition of convex, A function is calling convex on (a,b) if for any and for any pair of real numbers such that the following inequality . So we have: How is my answer? Thank's for any help","f: [a,b] \to \mathbb R f''(x) \ge 0 \forall x \in [a,b] f(\frac{x_1 + x_2} {2}) \le \frac{1}{2}[f(x_1) + f(x_2)], \forall x_1, x_2 \in [a,b]. f''(x) \ge 0 f: (a,b) \to \mathbb R (a, b) f f''(x) \ge 0 f’’(x) \gt 0 f f f: (a,b) \to \mathbb R x_1, x_2 \in (a,b) \alpha_1 \ge 0, \alpha_2 \ge 0 \alpha_1 + \alpha_2 = 1 f(\alpha_1x_1 + \alpha_2x_2) \le \alpha_1f(x_1) + \alpha_2f( x_2) f(\frac{x_1 + x_2}{2}) \le \frac{1}{2}[f(x_1) + f(x_2)] = f(\frac{1}{2}[x_1]+\frac{1}{2}[x_2])  \le \frac{1}{2}f(x_1) + \frac{1}{2}f(x_2) \to f(\alpha_1x_1 + \alpha_2x_2) \le \alpha_1f(x_1) + \alpha_2f(x_2)","['real-analysis', 'derivatives', 'solution-verification', 'problem-solving']"
85,Derivative of diagonal matrix expression: $f(X)=\text{diag}(X)M^T\text{diag}^{-1}(MX)$,Derivative of diagonal matrix expression:,f(X)=\text{diag}(X)M^T\text{diag}^{-1}(MX),"Let X be a vector $\mathbb{R}^{n\times1}$ and M be a constant matrix $\mathbb{R}^{n\times n}$ , and given the function $f(X)$ how could i find the derivative of $f(X)$ with respect to $X$ ? In the expression diag( $X$ ) represents the diagonal matrix of $X$ . $$f(X)=\text{diag}(X)M^T\text{diag}^{-1}(MX)$$ Thank you in advance.","Let X be a vector and M be a constant matrix , and given the function how could i find the derivative of with respect to ? In the expression diag( ) represents the diagonal matrix of . Thank you in advance.",\mathbb{R}^{n\times1} \mathbb{R}^{n\times n} f(X) f(X) X X X f(X)=\text{diag}(X)M^T\text{diag}^{-1}(MX),"['calculus', 'matrices', 'derivatives', 'matrix-equations', 'matrix-calculus']"
86,Question about Differential Geometry (Surfaces and Differentiability,Question about Differential Geometry (Surfaces and Differentiability,,"I have a doubt about differentiability. Why if I have two regular surface, $S_1$ and $S_2$ $\in \mathbb{R^3}$ and a function $f: \mathbb{R^3} \to \mathbb{R^3}$ such that $f(S_1) \subset S_2$ differentiable, then we can define another function $g=f|_{S_1}$ it's also differentiable? This it's from a proposition of Do Carmo but I can't understand it. And even less can I understand why if it is differentiable we can say that $dg_p(v)=Df(v)$ $\forall p \in S_1$ and $v \in T_p (S_1)$","I have a doubt about differentiability. Why if I have two regular surface, and and a function such that differentiable, then we can define another function it's also differentiable? This it's from a proposition of Do Carmo but I can't understand it. And even less can I understand why if it is differentiable we can say that and",S_1 S_2 \in \mathbb{R^3} f: \mathbb{R^3} \to \mathbb{R^3} f(S_1) \subset S_2 g=f|_{S_1} dg_p(v)=Df(v) \forall p \in S_1 v \in T_p (S_1),"['calculus', 'geometry', 'derivatives', 'differential-geometry', 'surfaces']"
87,"Derivative(s) of Cantor Measure (Donald L. Cohn ch. 6.2, exercise 6.2.4, related to lemma 6.2.5)","Derivative(s) of Cantor Measure (Donald L. Cohn ch. 6.2, exercise 6.2.4, related to lemma 6.2.5)",,"First, context: I'm doing a course on measure/integration theory following the book by Donald L. Cohn. In section 6.2, he defines the (upper and lower) derivates of a finite Borel measure $\mu$ on $\mathbb{R}^d$ as $$ (\overline{D}\mu)(x) = \lim_{\varepsilon\downarrow 0}\sup\left\{ \frac{\mu(C)}{\lambda(C)} : C\in\mathscr{C},x\in C, e(C) < \varepsilon \right\} $$ and $$ (\underline{D}\mu)(x) = \lim_{\varepsilon\downarrow 0}\inf\left\{ \frac{\mu(C)}{\lambda(C)} : C\in\mathscr{C},x\in C, e(C) < \varepsilon \right\}, $$ where $\mathscr{C}$ is the collection of all cubes in $\mathbb{R}^d$ (i.e. products of closed intervals of common length), $e(C)$ is the length of the edges of the cube $C$ , and $\lambda$ is the Lebesgue measure. In exercise 6.2.4, Cohn asks to confirm that certain assumptions in the following lemma are required: Lemma 6.2.5. Let $\mu$ be a finite Borel measure on $\mathbb{R}^d$ such that $\mu \ll \lambda$ , let $a$ be a positive real number, and let $A$ be a Borel set of $\mathbb{R}^d$ such that $(\underline{D}\mu)(x)\leq a$ holds at each $x\in A$ . Then $\mu(A)\leq a\lambda(A)$ . In particular, the exercise asks to show that the requirement that $\mu\ll\lambda$ is indeed necessary, so there should be some $\mu$ such that $\mu\not\ll\lambda$ , but $\underline{D}\mu$ is bounded on some set $A$ where the statement $\mu(A) \leq \text{constant}\cdot\lambda(A)$ fails. I have an extremely meager attempt at a solution to this: I know that any ""continuous"" looking measure cannot work, since any ""naturally"" constructed one will be absolutely continuous w.r.t. the Lebesgue measure, and any discrete measure will have either zero derivative (where it does not produce a counter example), or it'll have infinite derivative, which will also not work. Therefore, one needs to use some very strange measure, probably singular. Therefore, I suspect that the Cantor measure works, i.e. the measure induced by integrating next to the Cantor function. However, I have no idea how to calculate the upper and lower derivatives of this at the points where it would be non-zero (i.e. points of the Cantor set), nor can I even really show that the derivatives ought to be bounded. So I am stuck. Is this the correct approach, and if so, how should I proceed? If it isn't, what is?","First, context: I'm doing a course on measure/integration theory following the book by Donald L. Cohn. In section 6.2, he defines the (upper and lower) derivates of a finite Borel measure on as and where is the collection of all cubes in (i.e. products of closed intervals of common length), is the length of the edges of the cube , and is the Lebesgue measure. In exercise 6.2.4, Cohn asks to confirm that certain assumptions in the following lemma are required: Lemma 6.2.5. Let be a finite Borel measure on such that , let be a positive real number, and let be a Borel set of such that holds at each . Then . In particular, the exercise asks to show that the requirement that is indeed necessary, so there should be some such that , but is bounded on some set where the statement fails. I have an extremely meager attempt at a solution to this: I know that any ""continuous"" looking measure cannot work, since any ""naturally"" constructed one will be absolutely continuous w.r.t. the Lebesgue measure, and any discrete measure will have either zero derivative (where it does not produce a counter example), or it'll have infinite derivative, which will also not work. Therefore, one needs to use some very strange measure, probably singular. Therefore, I suspect that the Cantor measure works, i.e. the measure induced by integrating next to the Cantor function. However, I have no idea how to calculate the upper and lower derivatives of this at the points where it would be non-zero (i.e. points of the Cantor set), nor can I even really show that the derivatives ought to be bounded. So I am stuck. Is this the correct approach, and if so, how should I proceed? If it isn't, what is?","\mu \mathbb{R}^d  (\overline{D}\mu)(x) = \lim_{\varepsilon\downarrow 0}\sup\left\{ \frac{\mu(C)}{\lambda(C)} : C\in\mathscr{C},x\in C, e(C) < \varepsilon \right\}   (\underline{D}\mu)(x) = \lim_{\varepsilon\downarrow 0}\inf\left\{ \frac{\mu(C)}{\lambda(C)} : C\in\mathscr{C},x\in C, e(C) < \varepsilon \right\},  \mathscr{C} \mathbb{R}^d e(C) C \lambda \mu \mathbb{R}^d \mu \ll \lambda a A \mathbb{R}^d (\underline{D}\mu)(x)\leq a x\in A \mu(A)\leq a\lambda(A) \mu\ll\lambda \mu \mu\not\ll\lambda \underline{D}\mu A \mu(A) \leq \text{constant}\cdot\lambda(A)","['analysis', 'measure-theory', 'derivatives', 'lebesgue-measure', 'singular-measures']"
88,Can this be solved by FTC2?,Can this be solved by FTC2?,,"Question: let $f$ be differentiable and defined everywhere: simplify $$ \frac{d}{dx}  \left( \int_{0}^{x}   tf(x^2-t^2) \ dt\right)      $$ I'm confused becasuse if I attempt to use FTC2, it tells me that $$ xf(x^2-x^2) = xf(0)$$ which is obviously wrong. I may have violated the FTC2 definition; can anyone tell me how to use FTC2 to solve this problem.","Question: let be differentiable and defined everywhere: simplify I'm confused becasuse if I attempt to use FTC2, it tells me that which is obviously wrong. I may have violated the FTC2 definition; can anyone tell me how to use FTC2 to solve this problem.",f  \frac{d}{dx}  \left( \int_{0}^{x}   tf(x^2-t^2) \ dt\right)        xf(x^2-x^2) = xf(0),"['calculus', 'integration', 'derivatives']"
89,Derivative of Hadamard Product of two vectors,Derivative of Hadamard Product of two vectors,,"How can I compute the following derivative? $$\frac{\partial(K u \circ T u)}{\partial u}$$ $K$ and $T$ are constant matrices, $u$ is an unknown vector. and $\circ$ is Hadamard product. my solution: $$\frac{\partial(K_{ij} u_j \circ T_{mn} u_n)}{\partial u_p} = K_{ij}\frac{\partial(u_j)}{\partial u_p}\circ T_{mn} u_n+K_{ij} u_j \circ T_{mn} \frac{\partial u_n}{\partial u_p} = K_{ij}\delta_{jp} \circ T_{mn} u_n+K_{ij} u_j \circ T_{mn} \delta_{np}=K_{ip} (\sum_n T_{mn} u_n)+T_{mp} (\sum_j K_{ij} u_j).$$ therefore it can be written as follow $$\frac{\partial(K u \circ T u)}{\partial u} = K^T(Tu)+T^T(Ku).$$ where $\square^T$ is the transpose of the matrix.","How can I compute the following derivative? and are constant matrices, is an unknown vector. and is Hadamard product. my solution: therefore it can be written as follow where is the transpose of the matrix.",\frac{\partial(K u \circ T u)}{\partial u} K T u \circ \frac{\partial(K_{ij} u_j \circ T_{mn} u_n)}{\partial u_p} = K_{ij}\frac{\partial(u_j)}{\partial u_p}\circ T_{mn} u_n+K_{ij} u_j \circ T_{mn} \frac{\partial u_n}{\partial u_p} = K_{ij}\delta_{jp} \circ T_{mn} u_n+K_{ij} u_j \circ T_{mn} \delta_{np}=K_{ip} (\sum_n T_{mn} u_n)+T_{mp} (\sum_j K_{ij} u_j). \frac{\partial(K u \circ T u)}{\partial u} = K^T(Tu)+T^T(Ku). \square^T,"['derivatives', 'vectors', 'hadamard-product']"
90,Use the definition of the derivative to differentiate $e^{-1/x^2}$,Use the definition of the derivative to differentiate,e^{-1/x^2},"Let $f(x)=e^{-1/x^2}$ , $x \not=0$ . Without using the chain rule, find $f'(x)$ . This is an easy problem using the chain rule, however, I am curious to see how one might do it with the definition of the derivative: $$ \lim_{x\to c} \frac{f(x)-f(c)}{x-c}=\frac{e^{-1/x^2}-e^{-1/c^2}}{x-c}, $$ and $$ \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}= \lim_{h\to 0} \frac{e^{-1/(x+h)^2}-e^{-1/x^2}}{h}. $$","Let , . Without using the chain rule, find . This is an easy problem using the chain rule, however, I am curious to see how one might do it with the definition of the derivative: and","f(x)=e^{-1/x^2} x \not=0 f'(x) 
\lim_{x\to c} \frac{f(x)-f(c)}{x-c}=\frac{e^{-1/x^2}-e^{-1/c^2}}{x-c},
 
\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}=
\lim_{h\to 0} \frac{e^{-1/(x+h)^2}-e^{-1/x^2}}{h}.
","['calculus', 'limits', 'derivatives']"
91,What kind of Taylor expansion is this?,What kind of Taylor expansion is this?,,"Let $b \colon  \mathbb{R}^n \to \mathbb{R}^n$ be $\mathcal{C}^{1,1}(\mathbb{R})$ , i.e. its derivative $b_x$ is Lipschitz. Let $0 \leq \lambda \leq 1$ and define $x^\lambda=\lambda x_0+(1-\lambda)x_1$ for $x_0,x_1 \in \mathbb{R}^n$ . In [Yong, Jiongmin, and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations] p.188 (4.22) I find the following equality \begin{aligned} &\left|\lambda b\left( x_{1}\right)+(1-\lambda) b\left(x_{0}\right)-b\left( x^{\lambda}\right)\right| \\ &=\Big | \lambda \int_{0}^{1} b_{x}\left( x^{\lambda}+\theta(1-\lambda)\left(x_{1}-x_{0}\right)\right) d \theta(1-\lambda)\left(x_{1}-x_{0}\right) \\ &\quad+(1-\lambda) \int_{0}^{1} b_{x}\left( x^{\lambda}+\theta \lambda\left(x_{0}-x_{1}\right)\right) d \theta \lambda\left(x_{0}-x_{1}\right) \Big | \end{aligned} Where does it come from?","Let be , i.e. its derivative is Lipschitz. Let and define for . In [Yong, Jiongmin, and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations] p.188 (4.22) I find the following equality Where does it come from?","b \colon  \mathbb{R}^n \to \mathbb{R}^n \mathcal{C}^{1,1}(\mathbb{R}) b_x 0 \leq \lambda \leq 1 x^\lambda=\lambda x_0+(1-\lambda)x_1 x_0,x_1 \in \mathbb{R}^n \begin{aligned}
&\left|\lambda b\left( x_{1}\right)+(1-\lambda) b\left(x_{0}\right)-b\left( x^{\lambda}\right)\right| \\
&=\Big | \lambda \int_{0}^{1} b_{x}\left( x^{\lambda}+\theta(1-\lambda)\left(x_{1}-x_{0}\right)\right) d \theta(1-\lambda)\left(x_{1}-x_{0}\right) \\
&\quad+(1-\lambda) \int_{0}^{1} b_{x}\left( x^{\lambda}+\theta \lambda\left(x_{0}-x_{1}\right)\right) d \theta \lambda\left(x_{0}-x_{1}\right) \Big |
\end{aligned}","['real-analysis', 'analysis', 'derivatives', 'taylor-expansion']"
92,Alternative approach to derivative,Alternative approach to derivative,,"Recall that the derivative of a function $f$ at a point $a$ is defined as the limit: $\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$ , if the limit exists. Can we alternatively formulate it as $\lim_{x\to b}\frac{f(g(x))-f(h(x))}{g(x)-h(x)}$ , where $\lim_{x\to b}g(x)=\lim_{x\to b}h(x)=a$ ? Are these two statements equivalent?","Recall that the derivative of a function at a point is defined as the limit: , if the limit exists. Can we alternatively formulate it as , where ? Are these two statements equivalent?",f a \lim_{x\to a}\frac{f(x)-f(a)}{x-a} \lim_{x\to b}\frac{f(g(x))-f(h(x))}{g(x)-h(x)} \lim_{x\to b}g(x)=\lim_{x\to b}h(x)=a,"['real-analysis', 'calculus', 'limits', 'derivatives']"
93,How to prove product and quotient of smooth functions is smooth,How to prove product and quotient of smooth functions is smooth,,"I'm trying to prove the following problem: Let $A\subset\mathbb{R}^n$ be open. If $f, g: A \rightarrow \mathbb{R}$ are smooth, show that $fg$ and $f/g$ is smooth. (For the quotient case, $g$ is nonzero on $A$ .) I'm not sure how to show the smoothness of $fg$ and $f/g$ . (In this context, ""smoothness"" refers to $C^\infty$ . That is, it means the function has continuous partial derivative of all orders.) For cases like $f+g$ , I used induction to prove that partial derivative of order $n\geq 1$ of $(f+g)$ is sum of partial derivative of order $n \geq 1$ of $f$ and $g$ respectively. But for case of product and quotient, I am not sure how to show they are smooth. I think induction is again a possible solution, but it is quite mind-boggling to write down formula for the partial derivative of some order $k$ for $fg$ and $f/g$ as product rule and quotient rule for derivative have a very complicated formula as order goes up. Could someone help me?","I'm trying to prove the following problem: Let be open. If are smooth, show that and is smooth. (For the quotient case, is nonzero on .) I'm not sure how to show the smoothness of and . (In this context, ""smoothness"" refers to . That is, it means the function has continuous partial derivative of all orders.) For cases like , I used induction to prove that partial derivative of order of is sum of partial derivative of order of and respectively. But for case of product and quotient, I am not sure how to show they are smooth. I think induction is again a possible solution, but it is quite mind-boggling to write down formula for the partial derivative of some order for and as product rule and quotient rule for derivative have a very complicated formula as order goes up. Could someone help me?","A\subset\mathbb{R}^n f, g: A \rightarrow \mathbb{R} fg f/g g A fg f/g C^\infty f+g n\geq 1 (f+g) n \geq 1 f g k fg f/g","['derivatives', 'continuity', 'partial-derivative', 'smooth-functions']"
94,Why does differentiation of a tensor increase its rank?,Why does differentiation of a tensor increase its rank?,,"There is a statement in both Wald and Carroll's GR texts that, in short, state that the derivative of a $(k,l)$ -tensor is a $(k,l+1)$ -tensor. In both places this as stated as though it should be obvious, but I am having trouble developing an intuition for this. Could someone explain this? I am most comfortable thinking about tensors with the ""slot machine"" intuition, so if it could be explained in terms of ""why the derivative increases the number of vector slots"" that would be appreciated.","There is a statement in both Wald and Carroll's GR texts that, in short, state that the derivative of a -tensor is a -tensor. In both places this as stated as though it should be obvious, but I am having trouble developing an intuition for this. Could someone explain this? I am most comfortable thinking about tensors with the ""slot machine"" intuition, so if it could be explained in terms of ""why the derivative increases the number of vector slots"" that would be appreciated.","(k,l) (k,l+1)","['derivatives', 'tensors', 'general-relativity', 'tensor-rank']"
95,Is it possible to use the L'hopital rule for computing the derivative of this function at x=0?,Is it possible to use the L'hopital rule for computing the derivative of this function at x=0?,,"The function \begin{equation} f(x)=\begin{cases} \cos(1/x) & x\neq 0\\ 0 & x=0\end{cases}. \end{equation} and the function $F(x)=\displaystyle \int ^x_0 f $ And I know that by considering another function $g(x)$ , it's possible to solve that $F'(0)=0$ However, I wonder why I can't use the L'hopital rule for this question. More specifically, If I want to find the derivative of $F(x)$ at $x=0$ , based on the definition of the derivative, I can use $\displaystyle \lim_{x\to0} \frac{F(x)-F(0)}{x-0}=\lim_{x\to0} \frac{F(x)}{x}=\lim_{x\to0}\frac{\displaystyle \int ^x_0 f }{x}=F'(0)$ and since this is a $0/0$ , then I use the L'hopital rule to the nominator and the denominator. The result will be $\displaystyle \lim_{x\to0} \cos \frac{1}{x}$ which indicates that $F'(0)$ doesn't exist. I wonder why this is a wrong procedure, I kind of doubt that $\displaystyle \lim_{x\to 0}\int^x_0 \cos \frac{1}{x}$ is not 0. Is this true? Or are there some other problems with this?","The function and the function And I know that by considering another function , it's possible to solve that However, I wonder why I can't use the L'hopital rule for this question. More specifically, If I want to find the derivative of at , based on the definition of the derivative, I can use and since this is a , then I use the L'hopital rule to the nominator and the denominator. The result will be which indicates that doesn't exist. I wonder why this is a wrong procedure, I kind of doubt that is not 0. Is this true? Or are there some other problems with this?","\begin{equation}
f(x)=\begin{cases} \cos(1/x) & x\neq 0\\ 0 & x=0\end{cases}.
\end{equation} F(x)=\displaystyle \int ^x_0 f  g(x) F'(0)=0 F(x) x=0 \displaystyle \lim_{x\to0} \frac{F(x)-F(0)}{x-0}=\lim_{x\to0} \frac{F(x)}{x}=\lim_{x\to0}\frac{\displaystyle \int ^x_0 f }{x}=F'(0) 0/0 \displaystyle \lim_{x\to0} \cos \frac{1}{x} F'(0) \displaystyle \lim_{x\to 0}\int^x_0 \cos \frac{1}{x}","['analysis', 'derivatives']"
96,"$\int_0^1f(t)\phi'(t)dt=-\int_0^1g(t)\phi(t)dt$, for all smooth $\phi\in[0,1]$ implies $f$ is absolutely continuous and $f'=g$ a.e.",", for all smooth  implies  is absolutely continuous and  a.e.","\int_0^1f(t)\phi'(t)dt=-\int_0^1g(t)\phi(t)dt \phi\in[0,1] f f'=g","I'm trying to solve the following problem. Let $f,g\in L^1[0,1]$ such that for all $\phi\in C^\infty[0,1]$ with $\phi(0)=\phi(1)$ , $$\int_0^1f(t)\phi'(t)dt=-\int_0^1g(t)\phi(t)dt.$$ Show that $f$ is absolutely continuous and $f'=g$ . My idea is to show that $$f(x)=f(0)+\int_0^x g(t)dt.$$ If I can do this, then by the fundamental theorem of (Lebesgue) calculus, the result will follow. So I integrate by parts and get $$-\int_0^1g(t)\phi(t)dt=\int_0^1f(t)\phi'(t)dt=f(1)\phi(1)-f(0)\phi(0)-\int_0^1\phi(t)f'(t)dt$$ Now there's a couple of questions I have on how to proceed: I'm using the symbol $f'$ here even though I do not know at this point if $f$ is differentiable  (even if almost everywhere). So is the above step even legal? If $\phi$ was compactly supported, I could conclude that $f'=g$ a.e., but it isn't, so I'm not sure how to proceed. Suppose I could show that $f'=g$ a.e. Then I would proceed to get $$\int_0^xf'(t)dt=\int_0^xg(t)dt.$$ And if I knew that $f$ was absolutely continuous (the very thing I want to show), I'd get $f(x)-f(0)=\int_0^1g(t)dt$ , as desired. But this seems circular. How do I get around this? Any help with this is greatly appreciated. Thanks a lot.","I'm trying to solve the following problem. Let such that for all with , Show that is absolutely continuous and . My idea is to show that If I can do this, then by the fundamental theorem of (Lebesgue) calculus, the result will follow. So I integrate by parts and get Now there's a couple of questions I have on how to proceed: I'm using the symbol here even though I do not know at this point if is differentiable  (even if almost everywhere). So is the above step even legal? If was compactly supported, I could conclude that a.e., but it isn't, so I'm not sure how to proceed. Suppose I could show that a.e. Then I would proceed to get And if I knew that was absolutely continuous (the very thing I want to show), I'd get , as desired. But this seems circular. How do I get around this? Any help with this is greatly appreciated. Thanks a lot.","f,g\in L^1[0,1] \phi\in C^\infty[0,1] \phi(0)=\phi(1) \int_0^1f(t)\phi'(t)dt=-\int_0^1g(t)\phi(t)dt. f f'=g f(x)=f(0)+\int_0^x g(t)dt. -\int_0^1g(t)\phi(t)dt=\int_0^1f(t)\phi'(t)dt=f(1)\phi(1)-f(0)\phi(0)-\int_0^1\phi(t)f'(t)dt f' f \phi f'=g f'=g \int_0^xf'(t)dt=\int_0^xg(t)dt. f f(x)-f(0)=\int_0^1g(t)dt","['real-analysis', 'derivatives', 'lebesgue-integral', 'almost-everywhere', 'absolute-continuity']"
97,Derivative of trace of tensor,Derivative of trace of tensor,,"Given a rank-2 tensor $Q$ , how would one work out $\frac{d Tr(Q^2)}{dQ}$ ? If the tensor $Q$ was traceless and symmetric (ie $Tr(Q)=0$ and $Q=Q^T$ ) , would this change things? Thanks","Given a rank-2 tensor , how would one work out ? If the tensor was traceless and symmetric (ie and ) , would this change things? Thanks",Q \frac{d Tr(Q^2)}{dQ} Q Tr(Q)=0 Q=Q^T,"['derivatives', 'tensors', 'trace']"
98,How to find dy/dx when function defined at certain number,How to find dy/dx when function defined at certain number,,"Let function y=(sin x)^x and defined at x in the element of (0,pi). Find dy/dx. I get the answer for dy/dx =sin^x (x) (In (sin(x))+ x cot (x)) But when I insert 0 or pi into this, calculator shows undefined. What should I do this question?","Let function y=(sin x)^x and defined at x in the element of (0,pi). Find dy/dx. I get the answer for dy/dx =sin^x (x) (In (sin(x))+ x cot (x)) But when I insert 0 or pi into this, calculator shows undefined. What should I do this question?",,"['calculus', 'derivatives']"
99,Derivative of a function using definition at a point,Derivative of a function using definition at a point,,"I'm trying to find the derivative of the function $f(x)=x^2\sin x$ at $x=\pi/2$ . Using the definition of the derivative, I got: $$\lim_{x\to(\pi/2)} \frac{x^2\sin x-(\frac{\pi}{2})^2}{x-\frac{\pi}{2}}$$ I can't seem to be able to come up with any of the common factorising methods that can be used to remove the indeterminable denominator. Is there any way to manipulate this?","I'm trying to find the derivative of the function at . Using the definition of the derivative, I got: I can't seem to be able to come up with any of the common factorising methods that can be used to remove the indeterminable denominator. Is there any way to manipulate this?",f(x)=x^2\sin x x=\pi/2 \lim_{x\to(\pi/2)} \frac{x^2\sin x-(\frac{\pi}{2})^2}{x-\frac{\pi}{2}},"['calculus', 'derivatives']"
