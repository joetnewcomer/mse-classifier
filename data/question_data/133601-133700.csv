,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Numerical Methods for ODEs Precision,Numerical Methods for ODEs Precision,,"I have come across the following statement : Higher order (Ode stepper) does not always mean high accuracy (from Numerical Recipies, third edition). Why so ? Thank you in advance.","I have come across the following statement : Higher order (Ode stepper) does not always mean high accuracy (from Numerical Recipies, third edition). Why so ? Thank you in advance.",,"['ordinary-differential-equations', 'numerical-methods']"
1,Solve $y'' + 4y = e^{-x^2}$ using Fourier transforms,Solve  using Fourier transforms,y'' + 4y = e^{-x^2},I need to solve the equation $y'' + 4y = e^{-x^2}$ using Fourier transforms.  I was able to take the Fourier transform of both sides and solve for $\hat y$.  I have $\hat y = \frac{e^{-k^2/4}}{\sqrt{2}(4-k^2)}$.  I presumably need to take the inverse Fourier transform of both sides.  How exactly would I go about doing this?,I need to solve the equation $y'' + 4y = e^{-x^2}$ using Fourier transforms.  I was able to take the Fourier transform of both sides and solve for $\hat y$.  I have $\hat y = \frac{e^{-k^2/4}}{\sqrt{2}(4-k^2)}$.  I presumably need to take the inverse Fourier transform of both sides.  How exactly would I go about doing this?,,"['ordinary-differential-equations', 'fourier-analysis']"
2,Existence Energy of Wave Equation,Existence Energy of Wave Equation,,"I was just going trhough some properties of the wave equation, including the energy of the wave equation given by $E(t)=\int_{-\infty}^{\infty}u_t^2+c^2u_x^2 dx$, i.e the sum of kinetic and potential energy. I never found something concerning the existence (convergence) of this integral, so my question is, why does this integral even exists?","I was just going trhough some properties of the wave equation, including the energy of the wave equation given by $E(t)=\int_{-\infty}^{\infty}u_t^2+c^2u_x^2 dx$, i.e the sum of kinetic and potential energy. I never found something concerning the existence (convergence) of this integral, so my question is, why does this integral even exists?",,"['integration', 'ordinary-differential-equations', 'physics', 'mathematical-physics']"
3,Solution of non linear ODE system is always positive if its initial valus is positive,Solution of non linear ODE system is always positive if its initial valus is positive,,"Given a system of nonlinear differential equation \begin{eqnarray}\frac{dx}{dt}=2x(3-y) \\ \frac{dy}{dt}=3y(4-x)\end{eqnarray} If $r(t)=$($x(t)$,$y(t)$) is a solution of the system with initial value $x(0)>0$ and $y(0)>0$, would you help me to prove that $x(t)>0$ and $y(t)>0$ for all real $t$. Here is my argument: I prove it by contradiction. Let $r_1(t)=(x_1(t),x_2(t))$ be an orbit of the ODE with $x_1(0)=0$. Since, for $x=0$ we get $\frac{dx}{dt}=0$ then $x_1(t)=x_1(0)=0$. Thus, $x=0$ is an invariant manifold of the ODE. By the same argument, $y=0$ is also an invariant manifold. So the orbit that passing through a point in $x=0$ (resp $y=0$) always lies in $x=0$ (resp $y=0$). Assume there is a $t_1$ such that $x(t_1) \leq 0$ then $r(t)$ must intersect an orbit in $x=0 $( or $y=0$) hence $r(t)$ must lies on $x=0 $ (or $y=0$), contradicting $x(0)>0$ and $y(0)>0$. Is there any a direct proof (or even elementary/simple proof)?","Given a system of nonlinear differential equation \begin{eqnarray}\frac{dx}{dt}=2x(3-y) \\ \frac{dy}{dt}=3y(4-x)\end{eqnarray} If $r(t)=$($x(t)$,$y(t)$) is a solution of the system with initial value $x(0)>0$ and $y(0)>0$, would you help me to prove that $x(t)>0$ and $y(t)>0$ for all real $t$. Here is my argument: I prove it by contradiction. Let $r_1(t)=(x_1(t),x_2(t))$ be an orbit of the ODE with $x_1(0)=0$. Since, for $x=0$ we get $\frac{dx}{dt}=0$ then $x_1(t)=x_1(0)=0$. Thus, $x=0$ is an invariant manifold of the ODE. By the same argument, $y=0$ is also an invariant manifold. So the orbit that passing through a point in $x=0$ (resp $y=0$) always lies in $x=0$ (resp $y=0$). Assume there is a $t_1$ such that $x(t_1) \leq 0$ then $r(t)$ must intersect an orbit in $x=0 $( or $y=0$) hence $r(t)$ must lies on $x=0 $ (or $y=0$), contradicting $x(0)>0$ and $y(0)>0$. Is there any a direct proof (or even elementary/simple proof)?",,"['ordinary-differential-equations', 'dynamical-systems']"
4,$(y^{2} − 1) + 2(x − y(1 + y)^{2})y' = 0$ What method should I use to solve this ODE?,What method should I use to solve this ODE?,(y^{2} − 1) + 2(x − y(1 + y)^{2})y' = 0,"$(y^{2} − 1) + 2(x − y(1 + y)^{2})y' = 0$ What method should I use  to solve this ODE ? Clearly This is not linear, and I don't know how to convert this to Bernoulli,finding integration factor seems to be more complicated then the actual question ? Am I missing something ? I think is somewhere along the line $y' = F(y/x)$...but what is $F$ I think I have to do some kind of trig sub...any one can tell me how to transform a 1st Order Ode to polar coordinate  ??","$(y^{2} − 1) + 2(x − y(1 + y)^{2})y' = 0$ What method should I use  to solve this ODE ? Clearly This is not linear, and I don't know how to convert this to Bernoulli,finding integration factor seems to be more complicated then the actual question ? Am I missing something ? I think is somewhere along the line $y' = F(y/x)$...but what is $F$ I think I have to do some kind of trig sub...any one can tell me how to transform a 1st Order Ode to polar coordinate  ??",,['ordinary-differential-equations']
5,What is a strong stable manifold?,What is a strong stable manifold?,,"For a dynamical system in $\mathbb{R}^n$ given by $\dot{x} = f(x)$, and a fixed point $p$, one defines stable and unstable manifolds at the point $p$. These are well documented, and a quick Google/Wikipedia search provides all the information one needs (and more). There is, however, also the notion of strong stable manifold, and I have been so far unable to find what it precisely means. Could someone be so kind as to either provide the definition, or a reference to literature where it is defined?","For a dynamical system in $\mathbb{R}^n$ given by $\dot{x} = f(x)$, and a fixed point $p$, one defines stable and unstable manifolds at the point $p$. These are well documented, and a quick Google/Wikipedia search provides all the information one needs (and more). There is, however, also the notion of strong stable manifold, and I have been so far unable to find what it precisely means. Could someone be so kind as to either provide the definition, or a reference to literature where it is defined?",,"['reference-request', 'ordinary-differential-equations', 'dynamical-systems']"
6,"What do I need to know to simulate many particles, waves, or fluids?","What do I need to know to simulate many particles, waves, or fluids?",,"I've never had a numerical analysis course so I don't know what I need to know.  I'm just wondering what kind of books I should get to make me able to simulate these things.  I'm wanting to simulate in 3 dimensions.  I was thinking about getting a book on finite element analysis because I'm getting the hint that it might work the best, but do you have to know numerical analysis first and the Runge-Kutta method first?  Will those things be covered in books about finite element methods?  Are there other methods besides finite element methods that would work better?  Are there any good books that would help me and do you have any advice on a battle plan to purchase books i.e. Should I just buy a book that covers finite element methods and numerical analysis together or buy separate books that cover those topics individually?  Also I don't want to limit myself to just FEM or any one thing.  I want to have a comprehensive knowledge of simulation methods, but not if a comprehensive book wouldn't teach you any one thing well.  Advice?","I've never had a numerical analysis course so I don't know what I need to know.  I'm just wondering what kind of books I should get to make me able to simulate these things.  I'm wanting to simulate in 3 dimensions.  I was thinking about getting a book on finite element analysis because I'm getting the hint that it might work the best, but do you have to know numerical analysis first and the Runge-Kutta method first?  Will those things be covered in books about finite element methods?  Are there other methods besides finite element methods that would work better?  Are there any good books that would help me and do you have any advice on a battle plan to purchase books i.e. Should I just buy a book that covers finite element methods and numerical analysis together or buy separate books that cover those topics individually?  Also I don't want to limit myself to just FEM or any one thing.  I want to have a comprehensive knowledge of simulation methods, but not if a comprehensive book wouldn't teach you any one thing well.  Advice?",,"['reference-request', 'ordinary-differential-equations', 'physics', 'simulation']"
7,"How come in classical mechanics we can get away with writing $a=v(dv/dx)$, treating $v$ as a function of $x$?","How come in classical mechanics we can get away with writing , treating  as a function of ?",a=v(dv/dx) v x,"In classical mechanics we often use the relation $a=v(dv/dx)$ to help solve differential equations. I assume when we write $dv/dx$, we really mean $dV/dx$, where $V$ is a function defined so that $V(x(t))=v(t)$. But then $V$ is not really a well defined function, because a particle can pass through a point more than once, with a different velocity each time. I assume the answer has something to do with the implicit function theorem, which I haven't really studied, but I understand that we can locally treat $V$ as a function of $x$. But then why don't we run into issues treating this as a ""global"" expression? Edit: I understand the heuristic use of the chain rule: $a=(dv/dx)(dx/dt)$. But it seems to me that the term $dv/dx$ only makes sense ""locally."" Yet when we use $a=(dv/dx)(dx/dt)$ to solve, say, the equation of motion of the simple pendulum as an elliptic integral, we end up with an expression valid for all $t$, not just ""locally"". Why does everything work out?","In classical mechanics we often use the relation $a=v(dv/dx)$ to help solve differential equations. I assume when we write $dv/dx$, we really mean $dV/dx$, where $V$ is a function defined so that $V(x(t))=v(t)$. But then $V$ is not really a well defined function, because a particle can pass through a point more than once, with a different velocity each time. I assume the answer has something to do with the implicit function theorem, which I haven't really studied, but I understand that we can locally treat $V$ as a function of $x$. But then why don't we run into issues treating this as a ""global"" expression? Edit: I understand the heuristic use of the chain rule: $a=(dv/dx)(dx/dt)$. But it seems to me that the term $dv/dx$ only makes sense ""locally."" Yet when we use $a=(dv/dx)(dx/dt)$ to solve, say, the equation of motion of the simple pendulum as an elliptic integral, we end up with an expression valid for all $t$, not just ""locally"". Why does everything work out?",,"['calculus', 'ordinary-differential-equations', 'physics']"
8,Quick Question About Dynamical Systems,Quick Question About Dynamical Systems,,"Consider the two-dimensional autonomous system  \begin{equation*}\vec{\frac{dx}{dt}}(t)=\vec{f}(\vec{x}(t)):\mathbb{R}^{2}\to\mathbb{R}^{2}.\end{equation*} Suppose the system has a conserved quantity of motion associated with it (i.e. an energy conservation law): \begin{equation*}E(\vec{x}(t))=E_{0}\;\forall\;t\Longrightarrow\frac{dE}{dt}=0.\end{equation*} Then here are some questions. (1) Is it possible to have a second conserved quantity of motion which is different from $E$? (2) Is it necessarily the case that $E(x_{1},x_{2})=E_{0}$ corresponds to the integral family of curves which satisfes \begin{equation*}\frac{dx_{2}}{dx_{1}}=\frac{f_{2}(x_{1},x_{2})}{f_{1}(x_{1},x_{2})}\mapsto F(x_{1},x_{2})=E_{0}\end{equation*} (i.e. the solution curves in the phase-plane?).  For conservative mechanical systems such as $x''=-f(x)$ this is evidently the case (if one uses the standard physical conservation of energy).  But what about for general systems.  And why or why not is this the case (i.e. a proof)? Actually, I think I can prove (2): If $E$ represents the conserved quantity, then along trajectories, $E(\vec{x})=E_{0}\;\forall\;t$, so that the trajectories must correspond to the level sets of the function $E(\vec{x}):\mathbb{R}^{2}\to\mathbb{R}$.  From another point of view, the initial conditions determine (and are determined by) the initial energy $E_{0}$.  Then... (3) Assuming (2) is indeed true, then if the system has a conservative quantity, presumably it can always be obtained by integrating the differential equation obtained by eliminating $t$ from the system?  Of course there are other ways to obtain energy functions (for example, in the mechanical system one can multiply both sides of the equation by $\frac{dx}{dt}$ and integrate).  But what (2) tells me, is that any method which is used to obtain $E$ is necessarily equivalent to integrating the trajectories. (4) Assuming (3), once one integrates the t-eliminated ODE, then all one must do is check that $\frac{dE}{dt}=0$.  If this holds, then the system is conservative and you have your energy function (I am tacitly assuming (1) is NOT true; i.e. that the conserved quantity is unique).  If it does not, then you have your trajectories yes, but there is no conserved quantity along such trajectories. I might add some more questions to this later, but I'll leave it at that for now. Thanks! EDIT: (4) Was kind of a dumb question/observation.  Of course, when you get the integral family of curves you get something like $F(x_{1},x_{2})=C$.  Differentiating this with respect to $t$ of course gives you $0$, so $F$ is obviously constant along trajectories.  But doesn't this effectively say every system is conservative then? ...  Now I'm confused. ><","Consider the two-dimensional autonomous system  \begin{equation*}\vec{\frac{dx}{dt}}(t)=\vec{f}(\vec{x}(t)):\mathbb{R}^{2}\to\mathbb{R}^{2}.\end{equation*} Suppose the system has a conserved quantity of motion associated with it (i.e. an energy conservation law): \begin{equation*}E(\vec{x}(t))=E_{0}\;\forall\;t\Longrightarrow\frac{dE}{dt}=0.\end{equation*} Then here are some questions. (1) Is it possible to have a second conserved quantity of motion which is different from $E$? (2) Is it necessarily the case that $E(x_{1},x_{2})=E_{0}$ corresponds to the integral family of curves which satisfes \begin{equation*}\frac{dx_{2}}{dx_{1}}=\frac{f_{2}(x_{1},x_{2})}{f_{1}(x_{1},x_{2})}\mapsto F(x_{1},x_{2})=E_{0}\end{equation*} (i.e. the solution curves in the phase-plane?).  For conservative mechanical systems such as $x''=-f(x)$ this is evidently the case (if one uses the standard physical conservation of energy).  But what about for general systems.  And why or why not is this the case (i.e. a proof)? Actually, I think I can prove (2): If $E$ represents the conserved quantity, then along trajectories, $E(\vec{x})=E_{0}\;\forall\;t$, so that the trajectories must correspond to the level sets of the function $E(\vec{x}):\mathbb{R}^{2}\to\mathbb{R}$.  From another point of view, the initial conditions determine (and are determined by) the initial energy $E_{0}$.  Then... (3) Assuming (2) is indeed true, then if the system has a conservative quantity, presumably it can always be obtained by integrating the differential equation obtained by eliminating $t$ from the system?  Of course there are other ways to obtain energy functions (for example, in the mechanical system one can multiply both sides of the equation by $\frac{dx}{dt}$ and integrate).  But what (2) tells me, is that any method which is used to obtain $E$ is necessarily equivalent to integrating the trajectories. (4) Assuming (3), once one integrates the t-eliminated ODE, then all one must do is check that $\frac{dE}{dt}=0$.  If this holds, then the system is conservative and you have your energy function (I am tacitly assuming (1) is NOT true; i.e. that the conserved quantity is unique).  If it does not, then you have your trajectories yes, but there is no conserved quantity along such trajectories. I might add some more questions to this later, but I'll leave it at that for now. Thanks! EDIT: (4) Was kind of a dumb question/observation.  Of course, when you get the integral family of curves you get something like $F(x_{1},x_{2})=C$.  Differentiating this with respect to $t$ of course gives you $0$, so $F$ is obviously constant along trajectories.  But doesn't this effectively say every system is conservative then? ...  Now I'm confused. ><",,"['ordinary-differential-equations', 'dynamical-systems']"
9,Solve system of nonlinear differential equations,Solve system of nonlinear differential equations,,"I am trying to solve a large system of differential equations. Ideally, I would like to solve it exactly, but if not, can anyone suggest me a numerical method? In all its generality, the system I am trying to solve is like this: (here, $x = x(t) \in R^n$, and $\dot x = dx/dt$) $$ (a_i + P_ix/\Vert P_ix \Vert)^T \dot x = -\Vert P_ix \Vert $$ for $i = 1,\ldots,n$. Here all $P_i$ are positive definite matrices, and the set of $a_i$ is linearly independent. Also, $\Vert . \Vert$ is the 2-norm. It would help me a great deal if someone can help me to solve even a highly restricted special case of it, where $n=2$, $a_i = e_i$ (the $i$-th vector of the canonical basis), and $P_i = I$ for all $i$. Namely, this system: $$ (e_i + x/\Vert x \Vert)^T \dot x = -\Vert x \Vert $$ for all $i$. Thanks a lot, Daniel.","I am trying to solve a large system of differential equations. Ideally, I would like to solve it exactly, but if not, can anyone suggest me a numerical method? In all its generality, the system I am trying to solve is like this: (here, $x = x(t) \in R^n$, and $\dot x = dx/dt$) $$ (a_i + P_ix/\Vert P_ix \Vert)^T \dot x = -\Vert P_ix \Vert $$ for $i = 1,\ldots,n$. Here all $P_i$ are positive definite matrices, and the set of $a_i$ is linearly independent. Also, $\Vert . \Vert$ is the 2-norm. It would help me a great deal if someone can help me to solve even a highly restricted special case of it, where $n=2$, $a_i = e_i$ (the $i$-th vector of the canonical basis), and $P_i = I$ for all $i$. Namely, this system: $$ (e_i + x/\Vert x \Vert)^T \dot x = -\Vert x \Vert $$ for all $i$. Thanks a lot, Daniel.",,['ordinary-differential-equations']
10,Legendre Equation,Legendre Equation,,"From Apostol's Calculus, Vol. II, Section 6.21 #3: The Legendre equation can be written in the form   $$\left[(x^2 -1)y'\right]'-\alpha(\alpha+1)y=0\,,$$   where $\alpha\in\mathbb R$. If $a, b, c$ are constants with $a>b$ and $4c+1>0$, show that a differential equation of the type   $$\left[(x-a)(x-b)y'\right]'-cy=0$$   can be transformed to a Legendre equation by a change of variable of the form $x=At+B$ where $A>0$. Determine $A$ and $B$ interms of $a$ and $b$. It is easy to determine $A=\frac {a-b} 2$, and $B=\frac {a+b} 2$ (which agrees with the answer provided by the book), and we can find that this yields the equation $$\left[(A^2t^2-A^2)y'\right]'-cy=0\implies\left[(t^2-1)y'\right]'-\frac c {A^2} y =0$$ So to finish the proof that this can be considered as a Legendre equation, we must show that $\exists \alpha\in \mathbb R$ such that $\alpha(\alpha+1)=\frac c {A^2}$, however in solving this we find that $\alpha = \frac 1 2 \left(-1 \pm \sqrt{1+4\frac c {A^2}}\right)$. The condition $4c+1>0$ is not enough to guarantee that $\alpha\in\mathbb R$. For instance, we could take $a=\frac 1 4$, $b=0$, and $c=-\frac 1 8$. Did I make a mistake, or should the condition instead be $16c + (a-b)^2>0$?","From Apostol's Calculus, Vol. II, Section 6.21 #3: The Legendre equation can be written in the form   $$\left[(x^2 -1)y'\right]'-\alpha(\alpha+1)y=0\,,$$   where $\alpha\in\mathbb R$. If $a, b, c$ are constants with $a>b$ and $4c+1>0$, show that a differential equation of the type   $$\left[(x-a)(x-b)y'\right]'-cy=0$$   can be transformed to a Legendre equation by a change of variable of the form $x=At+B$ where $A>0$. Determine $A$ and $B$ interms of $a$ and $b$. It is easy to determine $A=\frac {a-b} 2$, and $B=\frac {a+b} 2$ (which agrees with the answer provided by the book), and we can find that this yields the equation $$\left[(A^2t^2-A^2)y'\right]'-cy=0\implies\left[(t^2-1)y'\right]'-\frac c {A^2} y =0$$ So to finish the proof that this can be considered as a Legendre equation, we must show that $\exists \alpha\in \mathbb R$ such that $\alpha(\alpha+1)=\frac c {A^2}$, however in solving this we find that $\alpha = \frac 1 2 \left(-1 \pm \sqrt{1+4\frac c {A^2}}\right)$. The condition $4c+1>0$ is not enough to guarantee that $\alpha\in\mathbb R$. For instance, we could take $a=\frac 1 4$, $b=0$, and $c=-\frac 1 8$. Did I make a mistake, or should the condition instead be $16c + (a-b)^2>0$?",,"['calculus', 'ordinary-differential-equations']"
11,Method to solve $xx'-x=f(t)$,Method to solve,xx'-x=f(t),I would like to resolve this differential equation: $xx'-x=f(t)$ any suggestions (or any online texts on similar differential equation) please? Thanks.,I would like to resolve this differential equation: $xx'-x=f(t)$ any suggestions (or any online texts on similar differential equation) please? Thanks.,,"['calculus', 'reference-request', 'ordinary-differential-equations']"
12,Finding the Laplace transform of $f(x)=|\cos(x)|$,Finding the Laplace transform of,f(x)=|\cos(x)|,"I have function $f(x)=|\cos(x)|, x≥0$ and like to derive its Laplace transform. I am told that $f(x+\pi)=f(x)$. Help me please.","I have function $f(x)=|\cos(x)|, x≥0$ and like to derive its Laplace transform. I am told that $f(x+\pi)=f(x)$. Help me please.",,"['ordinary-differential-equations', 'laplace-transform']"
13,Does every ODE have a first integral?,Does every ODE have a first integral?,,"This question might have an obvious answer but it eludes me. It is equivalent to asking whether every ODE can be solved in quadratures. For example, a first order ODE can be reduced to quadratures if we can solve for the integration factor $ mu$. This requires solving a first order partial differential equation, and as far as I know there are no general existence theorems on these. A possible equivalent question would be, given a function $x=f(t,c)$, can we find two functions $F(x,t)$ and $G(c)$, such that $F(f(t,c),t)=G(c)$?","This question might have an obvious answer but it eludes me. It is equivalent to asking whether every ODE can be solved in quadratures. For example, a first order ODE can be reduced to quadratures if we can solve for the integration factor $ mu$. This requires solving a first order partial differential equation, and as far as I know there are no general existence theorems on these. A possible equivalent question would be, given a function $x=f(t,c)$, can we find two functions $F(x,t)$ and $G(c)$, such that $F(f(t,c),t)=G(c)$?",,['ordinary-differential-equations']
14,"Convert Airy's equation, $y'' - xy = 0$, into Bessel equation, $t^2u'' + tu' + (t^2 - c^2)u = 0$","Convert Airy's equation, , into Bessel equation,",y'' - xy = 0 t^2u'' + tu' + (t^2 - c^2)u = 0,"My professor has said that this will be an easy homework exercise. He suggested using change of variable $t = \dfrac{2}{3}x^{3/2}$, and then removing the first derivative term of the form $p(t) \dfrac {dy}{dt}$ by a transformation \begin{equation} y(t) = w(t)e^{\frac{-1}2\int p(t)dt} \end{equation} Whenever I try do this I get a very messy bunch of terms that have the exponential in them. Is this problem really that easy to do? After change of variables I get: \begin{equation*} y^{(2)} - xy = (3/2)^{2/3}t^{2/3}*{\frac{d^2y}{dt^2}} + \frac{1}2 (3/2)^{-1/3}t^{-1/3}{\frac{dy}{dt}} - (3/2)^{2/3}t^{2/3}y \end{equation*} Then I try to use  \begin{equation} y(t) = w(t)e^{\frac{-1}2\int p(t)dt} \end{equation} by taking derivatives and substituting into the above equation.. then its just a mess of terms with exponentials. The second derivative of y(t) in that transformation  is really ugly.. so I think I'm doing it wrong.","My professor has said that this will be an easy homework exercise. He suggested using change of variable $t = \dfrac{2}{3}x^{3/2}$, and then removing the first derivative term of the form $p(t) \dfrac {dy}{dt}$ by a transformation \begin{equation} y(t) = w(t)e^{\frac{-1}2\int p(t)dt} \end{equation} Whenever I try do this I get a very messy bunch of terms that have the exponential in them. Is this problem really that easy to do? After change of variables I get: \begin{equation*} y^{(2)} - xy = (3/2)^{2/3}t^{2/3}*{\frac{d^2y}{dt^2}} + \frac{1}2 (3/2)^{-1/3}t^{-1/3}{\frac{dy}{dt}} - (3/2)^{2/3}t^{2/3}y \end{equation*} Then I try to use  \begin{equation} y(t) = w(t)e^{\frac{-1}2\int p(t)dt} \end{equation} by taking derivatives and substituting into the above equation.. then its just a mess of terms with exponentials. The second derivative of y(t) in that transformation  is really ugly.. so I think I'm doing it wrong.",,['ordinary-differential-equations']
15,Explain the error term in Euler method,Explain the error term in Euler method,,"Task: I had to find out some estimates for M and L to make sure the proportional accucrazy is not above $10^{-4}$ in the Euler method with the problem below. I am trying to understand the page 672 on this book here . The book provides the formula $$\left|y(x_{n})-y_{n}\right|\leq \left(\frac{M}{2L}\right)\left(e^{L(x_{n}-x_{0})}-1\right)h$$ about the error where you can see the M and L (which looks like some use of Lambert function is needed or rough estimate for the upper bound, look at the $L$ term). There is also an example where it finds some upper bounds and claims some rough estimate. More precisely, I am trying to apply the method of deducing the error term on pages 673-674 for the problem 2 on page 676. M I cannot yet understand why the second derivative is used as an estimate for the $M$. On page 673, it just claims that assume that $|y''(x)|\leq M$ but cannot find any premise for it, the 2 in the above. This point about M on pages 673-674 is something black magic to me. Please, explain. L The L is apparently just length of the interval.","Task: I had to find out some estimates for M and L to make sure the proportional accucrazy is not above $10^{-4}$ in the Euler method with the problem below. I am trying to understand the page 672 on this book here . The book provides the formula $$\left|y(x_{n})-y_{n}\right|\leq \left(\frac{M}{2L}\right)\left(e^{L(x_{n}-x_{0})}-1\right)h$$ about the error where you can see the M and L (which looks like some use of Lambert function is needed or rough estimate for the upper bound, look at the $L$ term). There is also an example where it finds some upper bounds and claims some rough estimate. More precisely, I am trying to apply the method of deducing the error term on pages 673-674 for the problem 2 on page 676. M I cannot yet understand why the second derivative is used as an estimate for the $M$. On page 673, it just claims that assume that $|y''(x)|\leq M$ but cannot find any premise for it, the 2 in the above. This point about M on pages 673-674 is something black magic to me. Please, explain. L The L is apparently just length of the interval.",,"['ordinary-differential-equations', 'numerical-methods']"
16,"Polar coordinates, line integrals, and the Beltrami Identity","Polar coordinates, line integrals, and the Beltrami Identity",,"Imagine you are walking along the xy-plane. There is a landmark at the origin of the plane which distorts time at every point on the plane, such that the distortion is a function of the distance between that point and the origin. What this essentially means is that, if you're trying to get from point A to point B, you'll want to stay a reasonable distance away from the origin, because the cost of moving a given unit of distance is higher the closer you get to the origin. What I'm trying to do is, given two points A and B, find the path from A to B which will minimize the cost of travel: the ""subjective distance"". The first step, then, is to construct an equation to represent the cost for a given path; the second step is to find a path that minimizes that cost, using the Calculus of Variations. The equation for temporal distortion is given as $\tau = C r^{-n} + 1$, where r is the distance from the origin. If we define the path as $r(\theta)$, then the cost of travelling from A to B should be $\int_{r(\theta)} C r^{-n} + 1 ds$, or $\int_{\alpha}^{\beta} \left( C \left[r(\theta)\right]^{-n} + 1 \right) \sqrt { \left[ r(\theta) \right]^2 + \left[ r'(\theta) \right]^2 } d\theta $. Now, in order to find a $r(\theta)$ such that the integral is minimized, I'm supposed to use the Beltrami Identity (because there's no $\theta$ term in the integral!), which is given as $$ L - r' \frac{\partial L}{\partial r'} = const $$ where L is the integrand above. So I find the partial derivative: $$ \begin{align} &\frac{\partial}{\partial r'} \left[ \left( C r^{-n} + 1 \right) \sqrt{ r^2 + r'^2 } \right]\\ &{} = \left( C r^{-n} + 1 \right) \frac{\partial}{\partial r'} \sqrt{ r^2 + r'^2 }\\ &{} = \left( C r^{-n} + 1 \right) \cdot \frac{1}{2}  \left( r^2 + r'^2 \right)^{-\frac{1}{2}} \cdot 2r'\\ &{} = \frac{r' \cdot \left( C r^{-n} + 1 \right)}{\sqrt{r^2 + r'^2}} \end{align} $$ And I substitute into the Beltrami Identity: $$ L - r' \frac{\partial L}{\partial r'} = const\\ \left[ \left( C r^{-n} + 1 \right) \sqrt{ r^2 + r'^2 } \right] - r' \frac{r' \cdot \left( C r^{-n} + 1 \right)}{\sqrt{r^2 + r'^2}} = const\\ \frac{\left( C r^{-n} + 1 \right) \left( r^2 + r'^2 \right)}{\sqrt{r^2 + r'^2}} - \frac{\left( C r^{-n} + 1 \right) r'^2}{\sqrt{r^2 + r'^2}} = const\\ \frac{\left( C r^{-n} + 1 \right) r^2}{\sqrt{r^2 + r'^2}} = const $$ This is the differential equation I must solve to find the optimum path. However, this cannot be the correct differential equation. We can observe that $r = const$ is a solution to this differential equation, but if we let C equal 0, then the distortion disappears... which would imply that the shortest distance between two points on a normal plane is a circular arc between them. Which is incorrect. I can't find a point in my equation where I assumed $C \neq 0$, so I must have done something wrong algebraically... or conceptually. Can anyone spot my error?","Imagine you are walking along the xy-plane. There is a landmark at the origin of the plane which distorts time at every point on the plane, such that the distortion is a function of the distance between that point and the origin. What this essentially means is that, if you're trying to get from point A to point B, you'll want to stay a reasonable distance away from the origin, because the cost of moving a given unit of distance is higher the closer you get to the origin. What I'm trying to do is, given two points A and B, find the path from A to B which will minimize the cost of travel: the ""subjective distance"". The first step, then, is to construct an equation to represent the cost for a given path; the second step is to find a path that minimizes that cost, using the Calculus of Variations. The equation for temporal distortion is given as $\tau = C r^{-n} + 1$, where r is the distance from the origin. If we define the path as $r(\theta)$, then the cost of travelling from A to B should be $\int_{r(\theta)} C r^{-n} + 1 ds$, or $\int_{\alpha}^{\beta} \left( C \left[r(\theta)\right]^{-n} + 1 \right) \sqrt { \left[ r(\theta) \right]^2 + \left[ r'(\theta) \right]^2 } d\theta $. Now, in order to find a $r(\theta)$ such that the integral is minimized, I'm supposed to use the Beltrami Identity (because there's no $\theta$ term in the integral!), which is given as $$ L - r' \frac{\partial L}{\partial r'} = const $$ where L is the integrand above. So I find the partial derivative: $$ \begin{align} &\frac{\partial}{\partial r'} \left[ \left( C r^{-n} + 1 \right) \sqrt{ r^2 + r'^2 } \right]\\ &{} = \left( C r^{-n} + 1 \right) \frac{\partial}{\partial r'} \sqrt{ r^2 + r'^2 }\\ &{} = \left( C r^{-n} + 1 \right) \cdot \frac{1}{2}  \left( r^2 + r'^2 \right)^{-\frac{1}{2}} \cdot 2r'\\ &{} = \frac{r' \cdot \left( C r^{-n} + 1 \right)}{\sqrt{r^2 + r'^2}} \end{align} $$ And I substitute into the Beltrami Identity: $$ L - r' \frac{\partial L}{\partial r'} = const\\ \left[ \left( C r^{-n} + 1 \right) \sqrt{ r^2 + r'^2 } \right] - r' \frac{r' \cdot \left( C r^{-n} + 1 \right)}{\sqrt{r^2 + r'^2}} = const\\ \frac{\left( C r^{-n} + 1 \right) \left( r^2 + r'^2 \right)}{\sqrt{r^2 + r'^2}} - \frac{\left( C r^{-n} + 1 \right) r'^2}{\sqrt{r^2 + r'^2}} = const\\ \frac{\left( C r^{-n} + 1 \right) r^2}{\sqrt{r^2 + r'^2}} = const $$ This is the differential equation I must solve to find the optimum path. However, this cannot be the correct differential equation. We can observe that $r = const$ is a solution to this differential equation, but if we let C equal 0, then the distortion disappears... which would imply that the shortest distance between two points on a normal plane is a circular arc between them. Which is incorrect. I can't find a point in my equation where I assumed $C \neq 0$, so I must have done something wrong algebraically... or conceptually. Can anyone spot my error?",,"['ordinary-differential-equations', 'polar-coordinates', 'calculus-of-variations']"
17,Numerical solving a constrained system of differential equation,Numerical solving a constrained system of differential equation,,"I am in trouble on finding a numerical technique to solve the following system of equations $$\ddot q_1(t)=f_1(q_1(t),q_2(t))$$ $$\ddot q_2(t)=f_2(q_1(t),q_2(t))$$ with a constrain of the kind: $$q_1(t)+q_2(t)=Q$$ with $Q$ a constant and a nonholonomic constraint: $$q_1(t)>0 \qquad q_2(t)>0.$$ I would appreciate also good references and whether such kind of question could be more suitable for MathOverflow. Thanks a lot.","I am in trouble on finding a numerical technique to solve the following system of equations $$\ddot q_1(t)=f_1(q_1(t),q_2(t))$$ $$\ddot q_2(t)=f_2(q_1(t),q_2(t))$$ with a constrain of the kind: $$q_1(t)+q_2(t)=Q$$ with $Q$ a constant and a nonholonomic constraint: $$q_1(t)>0 \qquad q_2(t)>0.$$ I would appreciate also good references and whether such kind of question could be more suitable for MathOverflow. Thanks a lot.",,"['reference-request', 'ordinary-differential-equations', 'numerical-methods']"
18,All general solutions of differential equation satisfy boundary conditions - How to interpret?,All general solutions of differential equation satisfy boundary conditions - How to interpret?,,"I'm solving the below differential equation: $$ 3z^2 U'(z) -2 U(z)^2 - z U(z) + 2 = 0$$ I have two boundary conditions - $U(0)=1$, $U'(0) = -\frac{1}{4}$, however it is apparent that the set of solutions to this equation, before imposing the initial conditions, satisfy the first initial condition $U(0)=1$. Looking at the differential equation this is because it can be rearranged to: $$ [U(0)]^2 = 1 $$ So it is evident that all of the general solutions should fit this condition. What's more, through ""numerical experiment"", I believe that the first derivative condition should be satisfied as well (though I can't prove it from the differential equation). I am not sure that I can set the constant of integration to be anything I want, as this changes the behaviour of the solution. However, I can simplify the solution greatly by choosing ""convenient"" values of this constant. Also the resulting function is a function of $\frac{1}{z}$, so it cannot actually be evaluated at 0, so I have been using limits, which feels quite mathematically dodgy. This is a peculiarity that I have not come up against before - how exactly am I to interpret this?","I'm solving the below differential equation: $$ 3z^2 U'(z) -2 U(z)^2 - z U(z) + 2 = 0$$ I have two boundary conditions - $U(0)=1$, $U'(0) = -\frac{1}{4}$, however it is apparent that the set of solutions to this equation, before imposing the initial conditions, satisfy the first initial condition $U(0)=1$. Looking at the differential equation this is because it can be rearranged to: $$ [U(0)]^2 = 1 $$ So it is evident that all of the general solutions should fit this condition. What's more, through ""numerical experiment"", I believe that the first derivative condition should be satisfied as well (though I can't prove it from the differential equation). I am not sure that I can set the constant of integration to be anything I want, as this changes the behaviour of the solution. However, I can simplify the solution greatly by choosing ""convenient"" values of this constant. Also the resulting function is a function of $\frac{1}{z}$, so it cannot actually be evaluated at 0, so I have been using limits, which feels quite mathematically dodgy. This is a peculiarity that I have not come up against before - how exactly am I to interpret this?",,['ordinary-differential-equations']
19,The equation $(1+x)^2\frac{dy}{dx}-xy=x^2y^2$,The equation,(1+x)^2\frac{dy}{dx}-xy=x^2y^2,"I am very grateful for all your comments and answers to my previous question concerning ODEs ( The equation $(x-2xy-y^2)\frac{dy}{dx}+y^2=0$ ). Now I am struggling with this one $$(1+x)^2\frac{dy}{dx}-xy=x^2y^2.$$ It seems not to be hard but nevertheless all the tricks I know fail in this case. Best regards, D.","I am very grateful for all your comments and answers to my previous question concerning ODEs ( The equation $(x-2xy-y^2)\frac{dy}{dx}+y^2=0$ ). Now I am struggling with this one $$(1+x)^2\frac{dy}{dx}-xy=x^2y^2.$$ It seems not to be hard but nevertheless all the tricks I know fail in this case. Best regards, D.",,['ordinary-differential-equations']
20,Second order ODE,Second order ODE,,"Why does the Time-Independent Schrodinger Equation , which is the second order ODE $$ \left[-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}+V(x)\right]\psi(x) = E\psi(x), $$  can have an infinite number of eigenvalues, hence also corresponding to an infinite number of eigenfunctions? I thought there could be at most two eigenfunctions and two eigenvalues for a second order ODE.","Why does the Time-Independent Schrodinger Equation , which is the second order ODE $$ \left[-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}+V(x)\right]\psi(x) = E\psi(x), $$  can have an infinite number of eigenvalues, hence also corresponding to an infinite number of eigenfunctions? I thought there could be at most two eigenfunctions and two eigenvalues for a second order ODE.",,['ordinary-differential-equations']
21,Why are the solutions of differential equations given with restricted domains?,Why are the solutions of differential equations given with restricted domains?,,"I solved $y^2=y'$ with the initial condition that $y(0)=1$, and I got $\displaystyle y(x)=\frac{1}{1-x}$. My book says it's only the portion of this function from $-\infty$ to $1$ that counts. I don't understand why? Now, if the initial condition were $y(2)=-1$, would that make it the portion from $1$ to $+\infty$? What if both initial conditions were given?","I solved $y^2=y'$ with the initial condition that $y(0)=1$, and I got $\displaystyle y(x)=\frac{1}{1-x}$. My book says it's only the portion of this function from $-\infty$ to $1$ that counts. I don't understand why? Now, if the initial condition were $y(2)=-1$, would that make it the portion from $1$ to $+\infty$? What if both initial conditions were given?",,['ordinary-differential-equations']
22,Black-Scholes equation,Black-Scholes equation,,"In Stein-Shakarchi's book Fourier Analysis (p. 170), the solution of the Black-Scholes equation $$\frac{\partial V}{\partial t}+rs\frac{\partial V}{\partial s}+\frac{\sigma^2s^2}{2}\frac{\partial^2 V}{\partial s^2}-rV=0$$ for  $0\lt t\lt T$ with boundary condition $V(s,T)=F(s)$, is given as $$V(s,t)=\frac{e^{-r(T-t)}}{\sqrt{2\pi\sigma^2(T-t)}}\int_0^\infty e^{-\frac{(\log(s/s^*)+(r-\sigma^2/2)(T-t))^2}{2\sigma^2(T-t)}} F(s^*)\,ds^*.$$ My own derivation shows that $ds^*$ in the formula should be replaced by $$\frac{ds^*}{s^*}.$$ Is this a typo in the book?","In Stein-Shakarchi's book Fourier Analysis (p. 170), the solution of the Black-Scholes equation $$\frac{\partial V}{\partial t}+rs\frac{\partial V}{\partial s}+\frac{\sigma^2s^2}{2}\frac{\partial^2 V}{\partial s^2}-rV=0$$ for  $0\lt t\lt T$ with boundary condition $V(s,T)=F(s)$, is given as $$V(s,t)=\frac{e^{-r(T-t)}}{\sqrt{2\pi\sigma^2(T-t)}}\int_0^\infty e^{-\frac{(\log(s/s^*)+(r-\sigma^2/2)(T-t))^2}{2\sigma^2(T-t)}} F(s^*)\,ds^*.$$ My own derivation shows that $ds^*$ in the formula should be replaced by $$\frac{ds^*}{s^*}.$$ Is this a typo in the book?",,"['analysis', 'ordinary-differential-equations', 'finance']"
23,Van Dyke's matching rule,Van Dyke's matching rule,,"I am looking at Perturbation Theory by E.J. Hinch. The author introduces Van Dyke's matching rule: (m term inner)(n term outer) = (n term outer)(m term inner) To be used to match the terms in an inner solution to the the terms in the outer solution for some perturbed differential equation. Hinch when comparing with introducing intermediate variables says: 'Van Dyke's matching rule usually works and is more convenient.' Does anyone know an example where this matching rule doesn't work? Edit In section $5.2.6$ of the book above, the author gives an example of where the matching rule is meant to fail. Is this an example where the matching rule actually fails or just shows some care in comparing terms is required? In particular, would the intermediate variables approach have any problems?","I am looking at Perturbation Theory by E.J. Hinch. The author introduces Van Dyke's matching rule: (m term inner)(n term outer) = (n term outer)(m term inner) To be used to match the terms in an inner solution to the the terms in the outer solution for some perturbed differential equation. Hinch when comparing with introducing intermediate variables says: 'Van Dyke's matching rule usually works and is more convenient.' Does anyone know an example where this matching rule doesn't work? Edit In section $5.2.6$ of the book above, the author gives an example of where the matching rule is meant to fail. Is this an example where the matching rule actually fails or just shows some care in comparing terms is required? In particular, would the intermediate variables approach have any problems?",,"['ordinary-differential-equations', 'perturbation-theory']"
24,Convert an equation to its elliptic form,Convert an equation to its elliptic form,,"I am failing to convert $L = \frac{\sqrt{2}}{2}\int_{0}^{\phi_0} \frac{\sin(\phi)}{\sqrt{\sin \phi_0 - \sin \phi}} d\phi$ into $L=\int_{\theta_1}^{\pi/2}\frac{2k^2\sin^2(\theta)-1 }{\sqrt{1-k^2\sin^2 \theta}}d\theta$ . The original manuscript makes the following substitution $1+\sin(\phi)=2k^2\sin^2(\theta)=(1+\sin\phi_0)\sin^2\theta$ Yet, something is amiss. If you would be so kind as to lend a hand? (Incidentally this is the solution to the problem of large deformation in a cantilevered beam)","I am failing to convert into . The original manuscript makes the following substitution Yet, something is amiss. If you would be so kind as to lend a hand? (Incidentally this is the solution to the problem of large deformation in a cantilevered beam)",L = \frac{\sqrt{2}}{2}\int_{0}^{\phi_0} \frac{\sin(\phi)}{\sqrt{\sin \phi_0 - \sin \phi}} d\phi L=\int_{\theta_1}^{\pi/2}\frac{2k^2\sin^2(\theta)-1 }{\sqrt{1-k^2\sin^2 \theta}}d\theta 1+\sin(\phi)=2k^2\sin^2(\theta)=(1+\sin\phi_0)\sin^2\theta,"['ordinary-differential-equations', 'mathematical-physics', 'elliptic-integrals']"
25,Periodic solutions to a first order linear ODE with incommensurable periods,Periodic solutions to a first order linear ODE with incommensurable periods,,"I'm studying for an admission test for the PhD in Mathematical Analysis. I'm stuck to solve this exercise: Consider the ODE $x'(t)+a(t)x(t)=b(t)$ where $a(t)$ and $b(t)$ are continuous functions from $\mathbb{R}$ to itself. Prove that this Equation cannot have 3 periodic non-constant solutions with pairwise incommensurable periods. I recall that if $k\in\mathbb{R}$ and $q\in\mathbb{R}-\{0\}$ , then they are called incommensurable when $\frac{k}{q}\notin\mathbb{Q}$ . I solved the first part of the exercise that asked to prove that if a continuous function on $\mathbb{R}$ is periodic and has two periods that are incommensurable then it's a constant function. But I can't see how the first part of the exercise can help with the second part. I also tried to do some computations with the solutions (subtracting and adding them etc.) and to write the general integral of the ODE and see if it could help.  In both cases all I did seemed useless.","I'm studying for an admission test for the PhD in Mathematical Analysis. I'm stuck to solve this exercise: Consider the ODE where and are continuous functions from to itself. Prove that this Equation cannot have 3 periodic non-constant solutions with pairwise incommensurable periods. I recall that if and , then they are called incommensurable when . I solved the first part of the exercise that asked to prove that if a continuous function on is periodic and has two periods that are incommensurable then it's a constant function. But I can't see how the first part of the exercise can help with the second part. I also tried to do some computations with the solutions (subtracting and adding them etc.) and to write the general integral of the ODE and see if it could help.  In both cases all I did seemed useless.",x'(t)+a(t)x(t)=b(t) a(t) b(t) \mathbb{R} k\in\mathbb{R} q\in\mathbb{R}-\{0\} \frac{k}{q}\notin\mathbb{Q} \mathbb{R},"['real-analysis', 'ordinary-differential-equations', 'analysis', 'periodic-functions']"
26,Show that the general solution to $y^{\prime\prime}-4xy^{\prime}+\left(4x^2-2\right)y=0$ is $y\left(x\right)=C_1\mathrm{e}^{x^2}+C_2x\mathrm{e}^{x^2}$,Show that the general solution to  is,y^{\prime\prime}-4xy^{\prime}+\left(4x^2-2\right)y=0 y\left(x\right)=C_1\mathrm{e}^{x^2}+C_2x\mathrm{e}^{x^2},"By solving the differential equation $$y^{\prime\prime}-4xy^{\prime}+\left(4x^2-2\right)y=0$$ using the series method, demonstrate that its general solution is $$y\left(x\right)=C_1\mathrm{e}^{x^2}+C_2x\mathrm{e}^{x^2}$$ Here, $C_1$ and $C_2$ are arbitrary constants of integration. My attempt and hint : I've been given a hint that by obtaining recursive relationships for the coefficients of the series, you can calculate several initial terms, and then ""guessing"" the expression for the $n$ -th coefficient (since you know what you should get) you can verify whether it holds true in all cases. I am not really familiar with this method and would appreciate your help.","By solving the differential equation using the series method, demonstrate that its general solution is Here, and are arbitrary constants of integration. My attempt and hint : I've been given a hint that by obtaining recursive relationships for the coefficients of the series, you can calculate several initial terms, and then ""guessing"" the expression for the -th coefficient (since you know what you should get) you can verify whether it holds true in all cases. I am not really familiar with this method and would appreciate your help.",y^{\prime\prime}-4xy^{\prime}+\left(4x^2-2\right)y=0 y\left(x\right)=C_1\mathrm{e}^{x^2}+C_2x\mathrm{e}^{x^2} C_1 C_2 n,"['sequences-and-series', 'ordinary-differential-equations']"
27,If $g$ is contraction and $f+f'=g(f)$ then f(t) has a limit as $t\to\infty$,If  is contraction and  then f(t) has a limit as,g f+f'=g(f) t\to\infty,"I would like to prove the following statement: Let $g:\mathbb{R}^n\to\mathbb{R}^n$ be contraction mapping. If function $f:\mathbb{R}\to\mathbb{R}^n$ satisfies: $$f(t)+f'(t)=g(f(t))$$ then the limit $\lim_{t\to\infty}f(t)$ exists. Below I write a sketch of my solution to this problem. My question is: is there some simpler way to prove the theorem and is my proof correct? Let $L<1$ be Lipschitz constant of $g$ . It's not hard to show that if $\alpha\in\mathbb{R}^n$ is a fixed point of $g$ then $|f(t)-\alpha|\leq\frac{1}{1-L}|f'(t)|$ so it's enough to show that $|f'(t)|\to 0$ . With some elementary calculation it can be proven that if $a,b\in\mathbb{R}^n$ satisfy $|a+b|\leq L|a|$ then $$\langle a,b\rangle\leq \frac{c}{2}|a|^2$$ where $c=\frac{L-1}{2-L}<0$ . Let $h\neq 0$ . Substituting $a=f(t+h)-f(t)$ , $b=f'(t+h)-f'(t)$ in the formula above we get: $$\langle f(t+h)-f(t),f'(t+h)-f'(t)\rangle\leq \frac{c}{2}|f(t+h)-f(t)|^2$$ Since $\frac{d}{dt}|f(t+h)-f(t)|^2=2\langle f(t+h)-f(t),f'(t+h)-f'(t)\rangle$ we obtain: $$\frac{d}{dt}|f(t+h)-f(t)|^2\leq c|f(t+h)-f(t)|^2$$ $$\frac{d}{dt}\ln\left(|f(t+h)-f(t)|^2\right)\leq c$$ Integrating from t to $0$ we get: $$\ln\left(|f(t+h)-f(t)|^2\right)-\ln\left(|f(h)-f(0)|^2\right)\leq ct$$ $$|f(t+h)-f(t)|^2\leq e^{ct}|f(h)-f(0)|^2$$ $$\left|\frac{f(t+h)-f(t)}{h}\right|^2\leq e^{ct}\left|\frac{f(h)-f(0)}{h}\right|^2$$ Since $h$ was arbitrary we can take $h\to 0$ obtaining: $$\left|f'(t)\right|^2\leq e^{ct}\left|f'(0)\right|^2$$ Having $c<0$ we see that $\lim_{t\to \infty}e^{ct}=0$ which implies that $\lim_{t\to \infty}|f'(t)|^2=0$ and this is what we wanted to prove.","I would like to prove the following statement: Let be contraction mapping. If function satisfies: then the limit exists. Below I write a sketch of my solution to this problem. My question is: is there some simpler way to prove the theorem and is my proof correct? Let be Lipschitz constant of . It's not hard to show that if is a fixed point of then so it's enough to show that . With some elementary calculation it can be proven that if satisfy then where . Let . Substituting , in the formula above we get: Since we obtain: Integrating from t to we get: Since was arbitrary we can take obtaining: Having we see that which implies that and this is what we wanted to prove.","g:\mathbb{R}^n\to\mathbb{R}^n f:\mathbb{R}\to\mathbb{R}^n f(t)+f'(t)=g(f(t)) \lim_{t\to\infty}f(t) L<1 g \alpha\in\mathbb{R}^n g |f(t)-\alpha|\leq\frac{1}{1-L}|f'(t)| |f'(t)|\to 0 a,b\in\mathbb{R}^n |a+b|\leq L|a| \langle a,b\rangle\leq \frac{c}{2}|a|^2 c=\frac{L-1}{2-L}<0 h\neq 0 a=f(t+h)-f(t) b=f'(t+h)-f'(t) \langle f(t+h)-f(t),f'(t+h)-f'(t)\rangle\leq \frac{c}{2}|f(t+h)-f(t)|^2 \frac{d}{dt}|f(t+h)-f(t)|^2=2\langle f(t+h)-f(t),f'(t+h)-f'(t)\rangle \frac{d}{dt}|f(t+h)-f(t)|^2\leq c|f(t+h)-f(t)|^2 \frac{d}{dt}\ln\left(|f(t+h)-f(t)|^2\right)\leq c 0 \ln\left(|f(t+h)-f(t)|^2\right)-\ln\left(|f(h)-f(0)|^2\right)\leq ct |f(t+h)-f(t)|^2\leq e^{ct}|f(h)-f(0)|^2 \left|\frac{f(t+h)-f(t)}{h}\right|^2\leq e^{ct}\left|\frac{f(h)-f(0)}{h}\right|^2 h h\to 0 \left|f'(t)\right|^2\leq e^{ct}\left|f'(0)\right|^2 c<0 \lim_{t\to \infty}e^{ct}=0 \lim_{t\to \infty}|f'(t)|^2=0","['ordinary-differential-equations', 'multivariable-calculus', 'solution-verification', 'contraction-mapping']"
28,Question about properties of Picard-Lindeloef existence theorem,Question about properties of Picard-Lindeloef existence theorem,,"I have a few questions about solutions that arise from differential equations where Picard-Lindeloef can be applied: In the problem $y'=f(t,y)=-y^2$ , solutions have the form $\frac{1}{x-c}$ and always have a discontinuity point, what is causing this? Is it that $f(t,y)$ has a zero for some value of $y$ ? Will this always happen? If a structure funciton $f$ is locally Lipschitz on an entire interval, do the unique solutions always exist on that entire interval? I don't mean globally Lipschitz, I mean locally Lipschitz on an interval such as $[0,1)$ , for example $f(t,y)=\frac{1}{y-1}$ What about $y'=\arctan(y)$ , this is globally Lipschitz but also has a zero, do the solutions have a discontinuity point? (I don't think this is analytically solvable, but I'm guessing it can still be analyzed using numerical methods)","I have a few questions about solutions that arise from differential equations where Picard-Lindeloef can be applied: In the problem , solutions have the form and always have a discontinuity point, what is causing this? Is it that has a zero for some value of ? Will this always happen? If a structure funciton is locally Lipschitz on an entire interval, do the unique solutions always exist on that entire interval? I don't mean globally Lipschitz, I mean locally Lipschitz on an interval such as , for example What about , this is globally Lipschitz but also has a zero, do the solutions have a discontinuity point? (I don't think this is analytically solvable, but I'm guessing it can still be analyzed using numerical methods)","y'=f(t,y)=-y^2 \frac{1}{x-c} f(t,y) y f [0,1) f(t,y)=\frac{1}{y-1} y'=\arctan(y)",['ordinary-differential-equations']
29,Proof of Positivity for Solutions in Ordinary Differential Equations (ODEs),Proof of Positivity for Solutions in Ordinary Differential Equations (ODEs),,"Let $x(t)$ be the solution of the initial value problem: $$ \dot{x}(t) = f(x(t)); \; \; x(0) = x_0 $$ I have made the following asumption during my work: If $x_0 \geq 0$ , and $f(0) \geq 0$ , then $x(t) \geq 0$ for $t \geq 0$ . It intuitivelly makes sense: starting with x positive, it might decrease until $x=0$ . But as soon as it reaches that point, $\dot{x} = f(0) \geq 0$ , so it either ""grows back up"" or just stay at $0$ . I feel like there should be a simple proof for this, but I cannot find one. Also, if it not true, what additional conditions should I add? Maybe  Lipschitz continuity of $f$ ? Thanks in advance.","Let be the solution of the initial value problem: I have made the following asumption during my work: If , and , then for . It intuitivelly makes sense: starting with x positive, it might decrease until . But as soon as it reaches that point, , so it either ""grows back up"" or just stay at . I feel like there should be a simple proof for this, but I cannot find one. Also, if it not true, what additional conditions should I add? Maybe  Lipschitz continuity of ? Thanks in advance.","x(t) 
\dot{x}(t) = f(x(t)); \; \; x(0) = x_0
 x_0 \geq 0 f(0) \geq 0 x(t) \geq 0 t \geq 0 x=0 \dot{x} = f(0) \geq 0 0 f","['ordinary-differential-equations', 'proof-writing', 'dynamical-systems', 'initial-value-problems']"
30,Solving the comparison equation $y^{\prime \prime \prime}-x y^{\prime}-\mu y=0$ when $\mu=\frac{3}{4}$.,Solving the comparison equation  when .,y^{\prime \prime \prime}-x y^{\prime}-\mu y=0 \mu=\frac{3}{4},"I need to solve the comparison equation from Airy Functions and Applications to Physics by Vallee and Soares $$y^{\prime \prime \prime}-x y^{\prime}-\mu y=0$$ when $\mu=\frac{3}{4}$ . When $\mu=\frac{1}{2}$ this equation has general solution $$ y_{1 / 2}=a \mathrm{Ai}^2\!\left(2^{-2 / 3} x\right)+b \mathrm{Bi}^2\!\left(2^{-2 / 3} x\right)+c \mathrm{Ai}\!\left(2^{-2 / 3} x\right) \mathrm{Bi}\!\left(2^{-2 / 3} x\right) $$ When $\mu=1$ $$y_1=a \mathrm{Ai}(x)+b \mathrm{Bi}(x)+c\mathrm{Hi}(x).$$ In general, The above image does give me the solution when $\mu =\frac{3}{4}$ , but it is in terms of contour integrals that I am not sure how to evaluate as I don't think the parameterizations of those curves are nice. How can I solve the equation more explicitly in this case of $\mu=\frac{3}{4}$ ? Here is a link to the reference Airy Functions and Applications to Physics page 108 in the first edition. The page prior has the comparison equation stated. EDIT: I am looking for an answer more explicit than the one terms of hypergeometric functions given by Mathematica, similar to the case of $\mu = \frac{1}{2},1$ . Wolfram spits out $$  y(x)=c_1\frac{\sqrt[3]{-1} x_1 F_2\left(\frac{7}{12} ; \frac{2}{3}, \frac{4}{3} ; \frac{x^3}{9}\right)}{3^{2 / 3}}+ c_2 {}_1 F_2\left(\frac{1}{4} ; \frac{1}{3}, \frac{2}{3} ; \frac{x^3}{9}\right)+c_3\frac{(-1)^{2 / 3} x^2{ }_1 F_2\left(\frac{11}{12} ; \frac{4}{3}, \frac{5}{3} ; \frac{x^3}{9}\right)}{\sqrt[3]{3}} $$ but I require something more explicit stated not in terms of hypergeometric functions, but maybe something involving $\operatorname{Ai}, \operatorname{Bi}$ . Thank you","I need to solve the comparison equation from Airy Functions and Applications to Physics by Vallee and Soares when . When this equation has general solution When In general, The above image does give me the solution when , but it is in terms of contour integrals that I am not sure how to evaluate as I don't think the parameterizations of those curves are nice. How can I solve the equation more explicitly in this case of ? Here is a link to the reference Airy Functions and Applications to Physics page 108 in the first edition. The page prior has the comparison equation stated. EDIT: I am looking for an answer more explicit than the one terms of hypergeometric functions given by Mathematica, similar to the case of . Wolfram spits out but I require something more explicit stated not in terms of hypergeometric functions, but maybe something involving . Thank you","y^{\prime \prime \prime}-x y^{\prime}-\mu y=0 \mu=\frac{3}{4} \mu=\frac{1}{2} 
y_{1 / 2}=a \mathrm{Ai}^2\!\left(2^{-2 / 3} x\right)+b \mathrm{Bi}^2\!\left(2^{-2 / 3} x\right)+c \mathrm{Ai}\!\left(2^{-2 / 3} x\right) \mathrm{Bi}\!\left(2^{-2 / 3} x\right)
 \mu=1 y_1=a \mathrm{Ai}(x)+b \mathrm{Bi}(x)+c\mathrm{Hi}(x). \mu =\frac{3}{4} \mu=\frac{3}{4} \mu = \frac{1}{2},1 
 y(x)=c_1\frac{\sqrt[3]{-1} x_1 F_2\left(\frac{7}{12} ; \frac{2}{3}, \frac{4}{3} ; \frac{x^3}{9}\right)}{3^{2 / 3}}+ c_2
{}_1 F_2\left(\frac{1}{4} ; \frac{1}{3}, \frac{2}{3} ; \frac{x^3}{9}\right)+c_3\frac{(-1)^{2 / 3} x^2{ }_1 F_2\left(\frac{11}{12} ; \frac{4}{3}, \frac{5}{3} ; \frac{x^3}{9}\right)}{\sqrt[3]{3}}
 \operatorname{Ai}, \operatorname{Bi}","['real-analysis', 'calculus', 'ordinary-differential-equations']"
31,Getting Modified Zernike Polynomial (Radial Part) ODE from Jacobi Polynomial ODE,Getting Modified Zernike Polynomial (Radial Part) ODE from Jacobi Polynomial ODE,,"Question: The Jacobi differential equation in terms of the Jacobi polynomial $P_{n}^{(\alpha,0)}(x)$ is given by: \begin{equation} 	(1 - x^2) P_{n}^{(\alpha,0)''}(x) + (-\alpha - (\alpha + 2) x) P_{n}^{(\alpha,0)'}(x) + n(n + \alpha + 1). P_{n}^{(\alpha,0)}(x) = 0 \;\;\; (*) \end{equation} Prove that $T_{n}(x)=x^{k+\frac{1}{2}} P_{n}^{(\alpha,0)}(1-2x^2) $ satisfies in the following ODE \begin{equation} 		(1 - x^2) T_{n}''(x)  -2x T_{n}'(x) + \frac{\frac{1}{4}-k^2}{x^{2}} T_{n}(x)= -\chi T_{n}(x). \;\;\; (**) \end{equation} I understand that $$ T_{n}'(x) = (k+\frac{1}{2})x^{k-\frac{1}{2}} P_{n}^{(\alpha,0)}(1-2x^2) - 4x^{k+\frac{3}{2}} P_{n}^{(\alpha,0)'}(1-2x^2) $$ ​ and $T_{n}''$ , however when I substitute $T_{n},\; T_{n}',\; T_{n}''$ in (**) I don't get anything close to (*). I have also thought about the change of variable $t=1-2x^{2}$ , but no result. I was wondering if someone could point out my mistake about this.","Question: The Jacobi differential equation in terms of the Jacobi polynomial is given by: Prove that satisfies in the following ODE I understand that ​ and , however when I substitute in (**) I don't get anything close to (*). I have also thought about the change of variable , but no result. I was wondering if someone could point out my mistake about this.","P_{n}^{(\alpha,0)}(x) \begin{equation}
	(1 - x^2) P_{n}^{(\alpha,0)''}(x) + (-\alpha - (\alpha + 2) x) P_{n}^{(\alpha,0)'}(x) + n(n + \alpha + 1). P_{n}^{(\alpha,0)}(x) = 0 \;\;\; (*)
\end{equation} T_{n}(x)=x^{k+\frac{1}{2}} P_{n}^{(\alpha,0)}(1-2x^2)  \begin{equation}
		(1 - x^2) T_{n}''(x)  -2x T_{n}'(x) + \frac{\frac{1}{4}-k^2}{x^{2}} T_{n}(x)= -\chi T_{n}(x). \;\;\; (**)
\end{equation} 
T_{n}'(x) = (k+\frac{1}{2})x^{k-\frac{1}{2}} P_{n}^{(\alpha,0)}(1-2x^2) - 4x^{k+\frac{3}{2}} P_{n}^{(\alpha,0)'}(1-2x^2)
 T_{n}'' T_{n},\; T_{n}',\; T_{n}'' t=1-2x^{2}","['ordinary-differential-equations', 'analysis', 'polynomials', 'orthogonal-polynomials']"
32,Proof that the solution has maximal interval all $\Bbb R$,Proof that the solution has maximal interval all,\Bbb R,"I have the following problem: let $f$ locally Lipschitz and consider the equation: $$\frac{dx}{dt} = f(x)$$ Suppose $x_1 < x_2$ and that $f(x_1) = f(x_2)= 0$ , prove that the PVI solution: $$\left\{\begin{matrix} \frac{dx}{dt} = f(x)\\  x(t_0) = x_0 \end{matrix}\right.$$ where $x_0 \in (x_1, x_2)$ has a maximum interval of $\mathbb{R}$ . The truth is I'm not sure how to show that. I understand that I would have to show that the solution is well defined in all of $\Bbb R$ , but only that it is locally Lipschitz I know that it is locally continuous after the problem data and continuity it would have to reach a maximum and a minimum but I don't know how to use said result. Any suggestions or ideas on how to show that the maximum interval is all of $\mathbb{R}$ ?","I have the following problem: let locally Lipschitz and consider the equation: Suppose and that , prove that the PVI solution: where has a maximum interval of . The truth is I'm not sure how to show that. I understand that I would have to show that the solution is well defined in all of , but only that it is locally Lipschitz I know that it is locally continuous after the problem data and continuity it would have to reach a maximum and a minimum but I don't know how to use said result. Any suggestions or ideas on how to show that the maximum interval is all of ?","f \frac{dx}{dt} = f(x) x_1 < x_2 f(x_1) = f(x_2)= 0 \left\{\begin{matrix}
\frac{dx}{dt} = f(x)\\ 
x(t_0) = x_0
\end{matrix}\right. x_0 \in (x_1, x_2) \mathbb{R} \Bbb R \mathbb{R}",['ordinary-differential-equations']
33,PDE: Is the solution unique?,PDE: Is the solution unique?,,"Consider the partial differential equation: $$ t\frac{\partial^2}{\partial t^2} \sum_{n=1}^k \Phi_n(x,t)=-x\frac{\partial}{\partial x}\sum_{n=1}^k \Phi_n(x,t)+\sum_{n=2}^{k-1}a(n)\Phi_n(x,t) $$ where $a(n)$ is some prescribed sequence of natural numbers. For now let $a(n)$ be the oblong numbers . I'm wondering how much power the sequence $a(n)$ has on constraining the solution space. For example, if you specify an $a(n),$ does this imply the solution is unique, if it exists? Fix $a(n)$ to be the oblong numbers. Then using an exponential ansatz, I found a solution to the equation: $$ \Phi_n(x,t)=e^{\frac{nt}{\log x}} $$ Are there other solutions, or is this unique? Edit: Letting $k \to \infty$ we obtain: $$ t\frac{\partial^2}{\partial t^2}\Psi(x,t)=-x\frac{\partial}{\partial x}\Psi(x,t)+h(x,t)$$ where $$\Psi(x,t)=-\frac{e^{\frac{t}{\log x}}}{e^{\frac{t}{\log x}}-1}=\sum_{n=1}^\infty\Phi_n(x,t)$$ and $$h(x,t)= -\frac{2e^{\frac{t}{\log x}}}{(e^{\frac{t}{\log x}}-1)^3}=\sum_{n=2}^{k-1}a(n)\Phi_n(x,t) $$","Consider the partial differential equation: where is some prescribed sequence of natural numbers. For now let be the oblong numbers . I'm wondering how much power the sequence has on constraining the solution space. For example, if you specify an does this imply the solution is unique, if it exists? Fix to be the oblong numbers. Then using an exponential ansatz, I found a solution to the equation: Are there other solutions, or is this unique? Edit: Letting we obtain: where and"," t\frac{\partial^2}{\partial t^2} \sum_{n=1}^k \Phi_n(x,t)=-x\frac{\partial}{\partial x}\sum_{n=1}^k \Phi_n(x,t)+\sum_{n=2}^{k-1}a(n)\Phi_n(x,t)  a(n) a(n) a(n) a(n), a(n)  \Phi_n(x,t)=e^{\frac{nt}{\log x}}  k \to \infty  t\frac{\partial^2}{\partial t^2}\Psi(x,t)=-x\frac{\partial}{\partial x}\Psi(x,t)+h(x,t) \Psi(x,t)=-\frac{e^{\frac{t}{\log x}}}{e^{\frac{t}{\log x}}-1}=\sum_{n=1}^\infty\Phi_n(x,t) h(x,t)= -\frac{2e^{\frac{t}{\log x}}}{(e^{\frac{t}{\log x}}-1)^3}=\sum_{n=2}^{k-1}a(n)\Phi_n(x,t) ","['sequences-and-series', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'linear-pde']"
34,Is the degree of a differential equation of any use?,Is the degree of a differential equation of any use?,,"When it is written as a polynomial equation in the unknown function and its derivatives, its degree of the differential equation is, depending on the context, the polynomial degree in the highest derivative of the unknown function, or its total degree in the unknown function and its derivatives. - Wiki Now this definition means that one is allowed to manipulate the given DE to a polynomial form to determine the degree. This is what is also taught in most schools. However, this allows many absurd manipulations and contradicting answers. For example, consider $$y'=x\;\;\;\;,\;\;\; y'^3=x^3$$ Both equations are equivalent and satisfy the polynomial definition, but have different degrees. This led to many claiming $^{\dagger}$ that the degree must be determined in the purest, given form, with no form of simplification, which goes against the original definition of possible polynomial manipulation. Some answers on this site tell me that most modern texts do not talk about the degree at all $^{\dagger\dagger}$ . This makes me think that degree is just a useless concept that offers no insight into the solution of the DE itself, at the same time being so ill-defined, unlike the order which offers information on the number of constants. Is my conclusion right, or, is there actually any valuable insight that the degree provides? $^{\dagger}$ Order and degree of a differential equation Differential equation degree doubt How to find degree of a differential equation. $^{\dagger\dagger}$ https://math.stackexchange.com/a/2446912/734160","When it is written as a polynomial equation in the unknown function and its derivatives, its degree of the differential equation is, depending on the context, the polynomial degree in the highest derivative of the unknown function, or its total degree in the unknown function and its derivatives. - Wiki Now this definition means that one is allowed to manipulate the given DE to a polynomial form to determine the degree. This is what is also taught in most schools. However, this allows many absurd manipulations and contradicting answers. For example, consider Both equations are equivalent and satisfy the polynomial definition, but have different degrees. This led to many claiming that the degree must be determined in the purest, given form, with no form of simplification, which goes against the original definition of possible polynomial manipulation. Some answers on this site tell me that most modern texts do not talk about the degree at all . This makes me think that degree is just a useless concept that offers no insight into the solution of the DE itself, at the same time being so ill-defined, unlike the order which offers information on the number of constants. Is my conclusion right, or, is there actually any valuable insight that the degree provides? Order and degree of a differential equation Differential equation degree doubt How to find degree of a differential equation. https://math.stackexchange.com/a/2446912/734160","y'=x\;\;\;\;,\;\;\; y'^3=x^3 ^{\dagger} ^{\dagger\dagger} ^{\dagger} ^{\dagger\dagger}","['calculus', 'ordinary-differential-equations', 'soft-question']"
35,Calculus with little-o notation: Follow-up,Calculus with little-o notation: Follow-up,,"This question is a follow-up of Calculus with little $o$ notation where I asked if it was true that $$ f'(x) = o (x^\alpha) \qquad \implies \qquad f(x) = f(0) + o(x^{\alpha+1}) \quad \forall \quad  \alpha > 0 ~ ?? \tag{1} $$ where $f : [0,\infty) \to {\mathbb R}$ is a smooth function and we are taking the limit $x \to 0$ . I got two great answers showing that this was indeed true. I have two follow-up questions: I believe those two proofs can be extended trivially to the region $\alpha > -1$ . Is this correct? Can we generalize (1) to $\alpha < -1$ in some way? In particular, is the following true: $$ f'(x) = o (x^\alpha) \qquad \implies \qquad f(x) = o(x^{\alpha+1}) \quad \forall \quad  \alpha \leq -1 ~ ?? \tag{2} $$ where now $f:(0,\infty) \to {\mathbb R}$ is a smooth function. EDIT: Apparently, if I don't add context for this question, the post will be closed. I'm certain the context won't help at all, but here goes. I'm trying to solve the BPS equations in a $U(1)^3$ truncation of $N=8$ gauged supergravity in five dimensions and trying to understand the near horizon geometry of supersymmetric black holes with charged scalar hair (so-called hairy black holes). After a significant amount of simplification, I managed to boil my problem down to a set of 5 coupled ordinary differential equations for 5 functions. I am able to make progress towards my goal if equation (2) of the post holds (or some version thereof) and that's why I'm asking if such a theorem could possibly be true. For example, one of the 5 differential equations is of the form $$ (2 x U )' = H \sqrt{ 4 + \Phi^2 } .  $$ I know that $H=r_+^2 + o(1)$ and $\Phi = C x^{-\alpha} + o(x^{-\alpha})$ for some $\alpha,r_+,C > 0$ (as $x \to 0$ which is where the horizon of the black hole is located). Given this information, what can I deduce about the near-horizon behaviour of $U(x)$ (i.e. its behaviour as $x \to 0$ )?","This question is a follow-up of Calculus with little $o$ notation where I asked if it was true that where is a smooth function and we are taking the limit . I got two great answers showing that this was indeed true. I have two follow-up questions: I believe those two proofs can be extended trivially to the region . Is this correct? Can we generalize (1) to in some way? In particular, is the following true: where now is a smooth function. EDIT: Apparently, if I don't add context for this question, the post will be closed. I'm certain the context won't help at all, but here goes. I'm trying to solve the BPS equations in a truncation of gauged supergravity in five dimensions and trying to understand the near horizon geometry of supersymmetric black holes with charged scalar hair (so-called hairy black holes). After a significant amount of simplification, I managed to boil my problem down to a set of 5 coupled ordinary differential equations for 5 functions. I am able to make progress towards my goal if equation (2) of the post holds (or some version thereof) and that's why I'm asking if such a theorem could possibly be true. For example, one of the 5 differential equations is of the form I know that and for some (as which is where the horizon of the black hole is located). Given this information, what can I deduce about the near-horizon behaviour of (i.e. its behaviour as )?","
f'(x) = o (x^\alpha) \qquad \implies \qquad f(x) = f(0) + o(x^{\alpha+1}) \quad \forall \quad  \alpha > 0 ~ ?? \tag{1}
 f : [0,\infty) \to {\mathbb R} x \to 0 \alpha > -1 \alpha < -1 
f'(x) = o (x^\alpha) \qquad \implies \qquad f(x) = o(x^{\alpha+1}) \quad \forall \quad  \alpha \leq -1 ~ ?? \tag{2}
 f:(0,\infty) \to {\mathbb R} U(1)^3 N=8 
(2 x U )' = H \sqrt{ 4 + \Phi^2 } . 
 H=r_+^2 + o(1) \Phi = C x^{-\alpha} + o(x^{-\alpha}) \alpha,r_+,C > 0 x \to 0 U(x) x \to 0","['ordinary-differential-equations', 'asymptotics']"
36,Help Finding a Traveling Wave Solution,Help Finding a Traveling Wave Solution,,"I am looking for traveling wave solutions of \begin{align} \frac{\partial U}{\partial t} &= AU\left(1-\frac{U}{K}\right)-BUV+D_{1}\nabla^{2}U \\ \frac{\partial V}{\partial t} &= CUV-DV+D_{2}\nabla^{2}V \end{align} Where $A,B,C,D,K,D_{1}$ ,and $D_{2}$ are constants. After nondimensionalising I arrived at the following system of equations \begin{align} \frac{\partial u}{\partial t} &= u(1-u-v)+D\frac{\partial^{2}u}{\partial x^{2}} \\ \frac{\partial v}{\partial t} &= av(u-b)+\frac{\partial^{2}v}{\partial x^{2}} \end{align} I then substituted $u(x,t)=U(z), v(x,t)=V(z), z=x+ct$ to get \begin{align} cU' &= U(1-U-V)+DU'' \\ cV' &= aV(U-b)+V'' \end{align} The considering the case where $D = 0$ and letting $V' = W$ I get the following system of first order ODEs \begin{align} U' &= \frac{1}{c}U(1-U-V) \\ V' &= W \\ W' &= cW-aV(U-b) \end{align} I am now stuck here I am unsure how to handle this system and get a traveling wave solution. I would appreciate any tips or suggestions on methods of solutions.","I am looking for traveling wave solutions of Where ,and are constants. After nondimensionalising I arrived at the following system of equations I then substituted to get The considering the case where and letting I get the following system of first order ODEs I am now stuck here I am unsure how to handle this system and get a traveling wave solution. I would appreciate any tips or suggestions on methods of solutions.","\begin{align}
\frac{\partial U}{\partial t} &= AU\left(1-\frac{U}{K}\right)-BUV+D_{1}\nabla^{2}U \\
\frac{\partial V}{\partial t} &= CUV-DV+D_{2}\nabla^{2}V
\end{align} A,B,C,D,K,D_{1} D_{2} \begin{align}
\frac{\partial u}{\partial t} &= u(1-u-v)+D\frac{\partial^{2}u}{\partial x^{2}} \\
\frac{\partial v}{\partial t} &= av(u-b)+\frac{\partial^{2}v}{\partial x^{2}}
\end{align} u(x,t)=U(z), v(x,t)=V(z), z=x+ct \begin{align}
cU' &= U(1-U-V)+DU'' \\
cV' &= aV(U-b)+V''
\end{align} D = 0 V' = W \begin{align}
U' &= \frac{1}{c}U(1-U-V) \\
V' &= W \\
W' &= cW-aV(U-b)
\end{align}","['ordinary-differential-equations', 'partial-differential-equations', 'nonlinear-system', 'mathematical-biology', 'population-dynamics']"
37,Maximum interval for differential equation,Maximum interval for differential equation,,"The exercise says as follows: Find the maximum interval of solution of $$y'=\frac{y^3}{1+y^2}e^x+x^2 \cos y, y(0)=y_0$$ I don't even know how to approach this problem. I'm aware of the maximize $a'$ method but I'm not able to reach anything. Any help is appreciated.",The exercise says as follows: Find the maximum interval of solution of I don't even know how to approach this problem. I'm aware of the maximize method but I'm not able to reach anything. Any help is appreciated.,"y'=\frac{y^3}{1+y^2}e^x+x^2 \cos y, y(0)=y_0 a'",['calculus']
38,Example vector field with index $-2$,Example vector field with index,-2,"I am looking at theory about vector fields in the plane and about their indices, I have found classic examples but I was wondering if anyone knows a plane vector field that behaves as follows: Considering that the lines in the image start the plane in equal parts, I have managed to obtain that the index is $-2$ , but I do not know which face this vector field has. If anyone has a reference to where I can find examples of vector fields that have indices $+2$ , $+3$ or $-3$ , $-4$ , I would ask you to mention a reference. Thanks in advance.","I am looking at theory about vector fields in the plane and about their indices, I have found classic examples but I was wondering if anyone knows a plane vector field that behaves as follows: Considering that the lines in the image start the plane in equal parts, I have managed to obtain that the index is , but I do not know which face this vector field has. If anyone has a reference to where I can find examples of vector fields that have indices , or , , I would ask you to mention a reference. Thanks in advance.",-2 +2 +3 -3 -4,"['ordinary-differential-equations', 'differential-topology', 'dynamical-systems', 'vector-fields']"
39,Generating fuction of the Bessel Function,Generating fuction of the Bessel Function,,"Consider the Bessel DEQ: $$x^2y''+xy'+(x^2-\alpha^2)y=0$$ Find a function $F$ such that $$F(x,t)=\sum_{\alpha=-\infty}^\infty J_{\alpha}(x)t^{\alpha}$$ I received a few instructions: a) first compute $\frac{\partial^n}{\partial x^n}F(x,t)$ and $\frac{\partial}{\partial t}(t\frac{\partial}{\partial t}F(x,t))$ b) Then exploiting the results of the first hint, write down the PDE that $F(x,t)$ has to satisfy. c) Then given the ansatz $F(x,t)=\exp(xT(t))$ find the two ODEs that $T(t)$ has to satisfy ( $T(t)=t^r$ ) d) Last, show that $T(t)=-T(1/t)$ which will fix all constants in the generating function which gives the generating function of the Bessel functions. I'm already running into problems with the first hint, I'm not sure how to take the $n$ th derivative of the $F(x,t)$ function and I would really appreciate some assistance! EDITs: Calculated the partial derivatives according to the helpful comments, but now stuck on the second part! $$\frac{\partial}{\partial x}J_{\alpha}(x)=\frac{1}{2}(J_{\alpha-1}(x)-J_{\alpha+1}(x))$$ $$\frac{\partial^2}{\partial x^2}J_{\alpha}(x)=\frac{1}{4}(J_{\alpha-2}(x)-2J_{\alpha}(x)+J_{\alpha+2}(x))$$ $$\frac{\partial}{\partial t}F(x,t)=\Sigma_{-\infty}^{\infty}J_{\alpha}(x)\alpha^2t^{\alpha-1}$$ Now for the second part, as @Gary mentioned in the comments I need to have $\sum\limits_{n =  - \infty }^\infty  {(x^2 J''_n (x) + xJ'_n (x) + (x^2  - n^2 )J_n (x))t^n } \;\; (=0)$ . But as the second hint suggests, i need to have a PDE but not sure where the time partial dertivative should be...Thanks 2nd Edit With the help of Gary i have : $$x^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}^2(x)t^{\alpha}+x\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}+x^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}-t^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}$$ and the PDE follows: $$x^2F_{xx}+xF_x+x^2F-tF_t-t^2F_{tt}=0$$ Can someone give me a hint on how to find the two ordinary differential equations asked in c)?","Consider the Bessel DEQ: Find a function such that I received a few instructions: a) first compute and b) Then exploiting the results of the first hint, write down the PDE that has to satisfy. c) Then given the ansatz find the two ODEs that has to satisfy ( ) d) Last, show that which will fix all constants in the generating function which gives the generating function of the Bessel functions. I'm already running into problems with the first hint, I'm not sure how to take the th derivative of the function and I would really appreciate some assistance! EDITs: Calculated the partial derivatives according to the helpful comments, but now stuck on the second part! Now for the second part, as @Gary mentioned in the comments I need to have . But as the second hint suggests, i need to have a PDE but not sure where the time partial dertivative should be...Thanks 2nd Edit With the help of Gary i have : and the PDE follows: Can someone give me a hint on how to find the two ordinary differential equations asked in c)?","x^2y''+xy'+(x^2-\alpha^2)y=0 F F(x,t)=\sum_{\alpha=-\infty}^\infty J_{\alpha}(x)t^{\alpha} \frac{\partial^n}{\partial x^n}F(x,t) \frac{\partial}{\partial t}(t\frac{\partial}{\partial t}F(x,t)) F(x,t) F(x,t)=\exp(xT(t)) T(t) T(t)=t^r T(t)=-T(1/t) n F(x,t) \frac{\partial}{\partial x}J_{\alpha}(x)=\frac{1}{2}(J_{\alpha-1}(x)-J_{\alpha+1}(x)) \frac{\partial^2}{\partial x^2}J_{\alpha}(x)=\frac{1}{4}(J_{\alpha-2}(x)-2J_{\alpha}(x)+J_{\alpha+2}(x)) \frac{\partial}{\partial t}F(x,t)=\Sigma_{-\infty}^{\infty}J_{\alpha}(x)\alpha^2t^{\alpha-1} \sum\limits_{n =  - \infty }^\infty  {(x^2 J''_n (x) + xJ'_n (x) + (x^2  - n^2 )J_n (x))t^n } \;\; (=0) x^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}^2(x)t^{\alpha}+x\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}+x^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}-t^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha} x^2F_{xx}+xF_x+x^2F-tF_t-t^2F_{tt}=0","['ordinary-differential-equations', 'partial-differential-equations', 'generating-functions', 'bessel-functions']"
40,How to solve Volterra Integral Equation,How to solve Volterra Integral Equation,,"I need help solving an Integral Equation that my professor showed without teaching to us. I've tried emulating the textbooks example but I still can't crack it. $$ f(t) = \cos\, t + 4 \, e^{-2t} - \int \sin\, (t-\tau)f(\tau)\, dt $$ My work so far on the problem: \begin{align} f(t) &= \cos\, t + 4 \, e^{-2t} - \int \sin\, (t-\tau)f(\tau)\, dt \\ \mathcal{L}\{f(t)\} &= \mathcal{L}\{\cos\, t\} + \mathcal{L}\{4 \, e^{-2t}\} - \mathcal{L}\left\{\int \sin\, (t-\tau)f(\tau)\, dt \right\} \\ F(s) &= \frac{s}{s^2 + 1} + \frac{4}{s+2} - F(s)\frac{1}{s^2 + 1} \\ F(s) &= \frac{s}{(s^2+2)} + \frac{4 s^2+1}{(s+2)(s^2+2)} \end{align} I know that from here I'm supposed to take the inverse Laplace Transform to find $f(t)$ but everything I've tried on the second term (Partial fractions, expanding it out) either fails me or gives me a super ugly equation that I'm sure is false Any and all help is appreciated.","I need help solving an Integral Equation that my professor showed without teaching to us. I've tried emulating the textbooks example but I still can't crack it. My work so far on the problem: I know that from here I'm supposed to take the inverse Laplace Transform to find but everything I've tried on the second term (Partial fractions, expanding it out) either fails me or gives me a super ugly equation that I'm sure is false Any and all help is appreciated."," f(t) = \cos\, t + 4 \, e^{-2t} - \int \sin\, (t-\tau)f(\tau)\, dt  \begin{align}
f(t) &= \cos\, t + 4 \, e^{-2t} - \int \sin\, (t-\tau)f(\tau)\, dt \\
\mathcal{L}\{f(t)\} &= \mathcal{L}\{\cos\, t\} + \mathcal{L}\{4 \, e^{-2t}\} - \mathcal{L}\left\{\int \sin\, (t-\tau)f(\tau)\, dt \right\} \\
F(s) &= \frac{s}{s^2 + 1} + \frac{4}{s+2} - F(s)\frac{1}{s^2 + 1} \\
F(s) &= \frac{s}{(s^2+2)} + \frac{4 s^2+1}{(s+2)(s^2+2)}
\end{align} f(t)","['ordinary-differential-equations', 'integral-equations']"
41,"What is the solution to $n_{i}=(e^{-x})\frac{dn_{i-1}}{dx}$ for i=0,1,2,...?","What is the solution to  for i=0,1,2,...?",n_{i}=(e^{-x})\frac{dn_{i-1}}{dx},"Given the recursion relationship $n_{i+1}=(e^{-x})\frac{dn_{i}}{dx}$ for $i=0,1,2,\ldots,$ where $n_{0}$ is a known function of $x$ , what is the solution to $n_{k}$ in terms of $x$ and the derivatives of $n_{0}$ , where $k$ is any positive integer? Re what I've tried, things simplify a bit if I express $n_0$ as $n_0=e^{y}$ and look for the solution in terms of the derivatives of $y$ (which may be expressed in terms of $x$ , $n_0$ , and the derivatives of $n_0$ as required by the question). We have then, $n_1=e^{y-x}\frac{dy}{dx}$ $n_{2}=e^{y-2x}\left(  \frac{d^{2}y}{dx^{2}}+\left(  \frac{dy}{dx}\right)^{2}-\frac{dy}{dx}\right)$ $n_{3}=e^{y-3x}\left(  \frac{d^{3}y}{dx^{3}}+3\frac{dy}{dx}\frac{d^{2}% y}{dx^{2}}+\left(  \frac{dy}{dx}\right)  ^{3}-3\left(  \frac{dy}{dx}\right) ^{2}-3\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}\right)  $ This gives a feel for where things are headed. The solution comes down to figuring out the integer coefficients of all the resultant derivatives and products of derivatives.","Given the recursion relationship for where is a known function of , what is the solution to in terms of and the derivatives of , where is any positive integer? Re what I've tried, things simplify a bit if I express as and look for the solution in terms of the derivatives of (which may be expressed in terms of , , and the derivatives of as required by the question). We have then, This gives a feel for where things are headed. The solution comes down to figuring out the integer coefficients of all the resultant derivatives and products of derivatives.","n_{i+1}=(e^{-x})\frac{dn_{i}}{dx} i=0,1,2,\ldots, n_{0} x n_{k} x n_{0} k n_0 n_0=e^{y} y x n_0 n_0 n_1=e^{y-x}\frac{dy}{dx} n_{2}=e^{y-2x}\left(  \frac{d^{2}y}{dx^{2}}+\left(  \frac{dy}{dx}\right)^{2}-\frac{dy}{dx}\right) n_{3}=e^{y-3x}\left(  \frac{d^{3}y}{dx^{3}}+3\frac{dy}{dx}\frac{d^{2}%
y}{dx^{2}}+\left(  \frac{dy}{dx}\right)  ^{3}-3\left(  \frac{dy}{dx}\right)
^{2}-3\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}\right)  ","['calculus', 'sequences-and-series', 'ordinary-differential-equations', 'derivatives']"
42,"Show that the solution to $y^{''}+f(y)y^{'}+g(y) = 0$, where $f \in C(\mathbb{R})$ and $g\in C^{1}(\mathbb{R})$, is unique.","Show that the solution to , where  and , is unique.",y^{''}+f(y)y^{'}+g(y) = 0 f \in C(\mathbb{R}) g\in C^{1}(\mathbb{R}),"The problem is: Show that the local solution to $y^{''}+f(y)y^{'}+g(y) = 0$ , where $f \in C(\mathbb{R})$ and $g\in C^{1}(\mathbb{R})$ , is unique. Since my class currently has only covered the existence and uniqueness theorem of 1st order ODE, I think I should transform it into a system of 1st order ODE. Let $x_1 = y$ and $x_2 = y^{'}=x_1^{'}$ . Then the DE becomes $$  x_1^{'}= x_2   \enspace \text{ and } x_2^{'} =   - f(x_1)x_2 -g(x_1). $$ Define $F(x,y) =  (y, -f(x) y-g(x) \,)$ . The DE now can be written as $$    \frac{d\psi}{dt}(t) =  F(\psi(t)), \, \text{where }\psi(t) = (x_1(t), x_2(t)),  $$ which is a 1st order ODE. The function $F$ is continuous, hence given an initial condition, a local solution must exist. In order to show that it's unique, I think I should show that $F$ is Lipschitz. But it seems that $F$ needs not be Lipschitz. Setting $g \equiv 0$ , and consider any $f$ that's continuous but not Lipschitz. Then for any points whose second coordinates coincide, $(x_1,x_2),(y_1,x_2)$ , we have $$ ||F(x_1,x_2) - F(y_1,x_2)|| = |x_2| \cdot | f(x_1)-f(y_1)|. $$ Since $f$ is not Lipschitz, $F$ also can't be Lipschitz from the equation above. Where do I get wrong? Can anyone prove the uniqueness of the solution?","The problem is: Show that the local solution to , where and , is unique. Since my class currently has only covered the existence and uniqueness theorem of 1st order ODE, I think I should transform it into a system of 1st order ODE. Let and . Then the DE becomes Define . The DE now can be written as which is a 1st order ODE. The function is continuous, hence given an initial condition, a local solution must exist. In order to show that it's unique, I think I should show that is Lipschitz. But it seems that needs not be Lipschitz. Setting , and consider any that's continuous but not Lipschitz. Then for any points whose second coordinates coincide, , we have Since is not Lipschitz, also can't be Lipschitz from the equation above. Where do I get wrong? Can anyone prove the uniqueness of the solution?","y^{''}+f(y)y^{'}+g(y) = 0 f \in C(\mathbb{R}) g\in C^{1}(\mathbb{R}) x_1 = y x_2 = y^{'}=x_1^{'} 
 x_1^{'}= x_2   \enspace \text{ and } x_2^{'} =   - f(x_1)x_2 -g(x_1).
 F(x,y) =  (y, -f(x) y-g(x) \,) 
   \frac{d\psi}{dt}(t) =  F(\psi(t)), \, \text{where }\psi(t) = (x_1(t), x_2(t)), 
 F F F g \equiv 0 f (x_1,x_2),(y_1,x_2) 
||F(x_1,x_2) - F(y_1,x_2)|| = |x_2| \cdot | f(x_1)-f(y_1)|.
 f F","['calculus', 'ordinary-differential-equations', 'analysis']"
43,Transformation idea to turn PDE into ODE for unsteady flow of viscous incompressible fluid with constant fluid properties,Transformation idea to turn PDE into ODE for unsteady flow of viscous incompressible fluid with constant fluid properties,,"I am facing trouble when solving the Navier stokes equation for unsteady flow of viscous incompressible fluid with constant fluid properties. Until now, I read two cases: Unsteady flow of viscous incompressible fluid over a suddenly accelerate flat plane (flow over a plane wall suddenly set in motion) Unsteady flow of viscous incompressible fluid over an oscillating plate (flow of a viscous incompressible fluid due to an oscillating plane wall) For the first case, after simplifying the continuity and navier stokes equation, we get, (Let there be an incompressible viscous fluid over a half plane $y=0$ i.e., xz-plane) $$ \begin{align} \frac{\partial V_x}{\partial t}=\mu \frac{\partial^2 V_x}{\partial y^2}\\ \text{boundary conditions,}\\ V_x=0\quad\text{When }t\leq 0\quad\text{for all }y\\ V_x = U \quad\text{at }y=0\quad\text{When }t>0\\ V_x = 0 \quad\text{at }y=\infty\quad\text{When }t>0 \end{align} $$ To obtain the desired solution, the PDE is reduced to an ODE by the substitutions, $$\eta=\frac{y}{2\sqrt{\mu t}},\: V_x=U\:f(\eta)$$ Now, for the second case, similarly we get (where the plate be oscillating with a constant amplitude and frequency with velocity $U\cos nt$ ), $$ \begin{align} \frac{\partial V_x}{\partial t}=\mu \frac{\partial^2 V_x}{\partial y^2}\\ \text{boundary conditions,}\\ V_x = U\cos nt \quad\text{at }y=0\quad\text{When }t>0\\ V_x = 0 \quad\text{at }y=\infty\quad\text{When }t>0 \end{align} $$ To obtain the desired solution, the PDE is reduced to an ODE by the substitutions, $$V_x(y,t)=\Re \{\exp(int)f(y)\}$$ I couldn't understand, from where they got those transformations? Or the intuition of those transformations. It will be a great help if anyone help me to figure out this. Without knowing the reason, it seems memorizing without understanding.","I am facing trouble when solving the Navier stokes equation for unsteady flow of viscous incompressible fluid with constant fluid properties. Until now, I read two cases: Unsteady flow of viscous incompressible fluid over a suddenly accelerate flat plane (flow over a plane wall suddenly set in motion) Unsteady flow of viscous incompressible fluid over an oscillating plate (flow of a viscous incompressible fluid due to an oscillating plane wall) For the first case, after simplifying the continuity and navier stokes equation, we get, (Let there be an incompressible viscous fluid over a half plane i.e., xz-plane) To obtain the desired solution, the PDE is reduced to an ODE by the substitutions, Now, for the second case, similarly we get (where the plate be oscillating with a constant amplitude and frequency with velocity ), To obtain the desired solution, the PDE is reduced to an ODE by the substitutions, I couldn't understand, from where they got those transformations? Or the intuition of those transformations. It will be a great help if anyone help me to figure out this. Without knowing the reason, it seems memorizing without understanding.","y=0 
\begin{align}
\frac{\partial V_x}{\partial t}=\mu \frac{\partial^2 V_x}{\partial y^2}\\
\text{boundary conditions,}\\
V_x=0\quad\text{When }t\leq 0\quad\text{for all }y\\
V_x = U \quad\text{at }y=0\quad\text{When }t>0\\
V_x = 0 \quad\text{at }y=\infty\quad\text{When }t>0
\end{align}
 \eta=\frac{y}{2\sqrt{\mu t}},\: V_x=U\:f(\eta) U\cos nt 
\begin{align}
\frac{\partial V_x}{\partial t}=\mu \frac{\partial^2 V_x}{\partial y^2}\\
\text{boundary conditions,}\\
V_x = U\cos nt \quad\text{at }y=0\quad\text{When }t>0\\
V_x = 0 \quad\text{at }y=\infty\quad\text{When }t>0
\end{align}
 V_x(y,t)=\Re \{\exp(int)f(y)\}","['ordinary-differential-equations', 'partial-differential-equations', 'fluid-dynamics']"
44,Trying to model the Flyby anomaly,Trying to model the Flyby anomaly,,"Thinking about it, the following non-linear ordinary differential equation crossed my mind: $$ \frac{d^2 r}{dt^2} - \frac{1}{2} H \frac{dr}{dt} + \frac{\mu}{r^2} = 0 $$ Apparently, I've been trying to modify and simplify an equation for Orbit al motion. The physical meaning of the variables is: $r=$ position , $t=$ time , $(H,\mu)=$ constants. The constant $H$ may be considered as being very small. An obvious simplification is $H=0$ : $$ \frac{d^2 r}{dt^2} = - \frac{\mu}{r^2} $$ An equation which can be solved as follows. Physicists will recognize kinetic and potential energy. $$ \frac{d^2 r}{dt^2}\frac{dr}{dt} = - \frac{\mu}{r^2}\frac{dr}{dt} \\ \frac{1}{2} \frac{d}{dt}\left(\frac{dr}{dt}\right)^2 = \frac{d}{dt}\left(\frac{\mu}{r}\right) \\ v^2 = \left(\frac{dr}{dt}\right)^2 = \frac{2\mu}{r} + C $$ With $C$ as an integration constant. It can be determined by assuming that speed $v=V$ at infinity $r\to\infty$ is given: $$ \left(\frac{dr}{dt}\right)^2 = \frac{2\mu}{r} + V^2 = \mu \left( \frac{2}{r} - \frac{1}{a} \right) $$ The result is written in the latter form because it resembles (not at all by coincidence) the Vis-viva equation , with $a = -\mu/V^2 \lt 0$ for a hyperbola. Calculations can be continued eventually for finding the position as a function of time. Which is not quite easy, and not interesting too, for our purpose. I think that the one-dimensional model may be acceptable as the simplification of a rather straight hyperbolic trajectory: Because in that case the asymptotes of the hyperbola could replace the hyperbola itself as a first approximation (except for the singularity near $r=0$ ). But let us return to the core of the question, which is my tentative non-linear ODE for the Flyby anomaly . Problem is that I have not a clue how to solve it. $$ \frac{d^2 r}{dt^2} - \frac{1}{2} H \frac{dr}{dt} + \frac{\mu}{r^2} = 0 $$ Or equivalently: $$ \left(\frac{d}{dt} - H \right) \left(\frac{dr}{dt}\right)^2 = \frac{d}{dt}\left(\frac{2\mu}{r}\right) $$ Which doesn't help me any further. Any ideas? I'm satisfied with a solution for $\,v=dr/dt\,$ only, even if it's approximate.","Thinking about it, the following non-linear ordinary differential equation crossed my mind: Apparently, I've been trying to modify and simplify an equation for Orbit al motion. The physical meaning of the variables is: position , time , constants. The constant may be considered as being very small. An obvious simplification is : An equation which can be solved as follows. Physicists will recognize kinetic and potential energy. With as an integration constant. It can be determined by assuming that speed at infinity is given: The result is written in the latter form because it resembles (not at all by coincidence) the Vis-viva equation , with for a hyperbola. Calculations can be continued eventually for finding the position as a function of time. Which is not quite easy, and not interesting too, for our purpose. I think that the one-dimensional model may be acceptable as the simplification of a rather straight hyperbolic trajectory: Because in that case the asymptotes of the hyperbola could replace the hyperbola itself as a first approximation (except for the singularity near ). But let us return to the core of the question, which is my tentative non-linear ODE for the Flyby anomaly . Problem is that I have not a clue how to solve it. Or equivalently: Which doesn't help me any further. Any ideas? I'm satisfied with a solution for only, even if it's approximate.","
\frac{d^2 r}{dt^2} - \frac{1}{2} H \frac{dr}{dt} + \frac{\mu}{r^2} = 0
 r= t= (H,\mu)= H H=0 
\frac{d^2 r}{dt^2} = - \frac{\mu}{r^2}
 
\frac{d^2 r}{dt^2}\frac{dr}{dt} = - \frac{\mu}{r^2}\frac{dr}{dt} \\
\frac{1}{2} \frac{d}{dt}\left(\frac{dr}{dt}\right)^2 = \frac{d}{dt}\left(\frac{\mu}{r}\right) \\
v^2 = \left(\frac{dr}{dt}\right)^2 = \frac{2\mu}{r} + C
 C v=V r\to\infty 
\left(\frac{dr}{dt}\right)^2 = \frac{2\mu}{r} + V^2 = \mu \left( \frac{2}{r} - \frac{1}{a} \right)
 a = -\mu/V^2 \lt 0 r=0 
\frac{d^2 r}{dt^2} - \frac{1}{2} H \frac{dr}{dt} + \frac{\mu}{r^2} = 0
 
\left(\frac{d}{dt} - H \right) \left(\frac{dr}{dt}\right)^2 = \frac{d}{dt}\left(\frac{2\mu}{r}\right)
 \,v=dr/dt\,","['calculus', 'ordinary-differential-equations', 'physics', 'nonlinear-system', 'celestial-mechanics']"
45,Any geometric interpretation for the adjoint system of a linear dynamical system?,Any geometric interpretation for the adjoint system of a linear dynamical system?,,"On page 26, Section 1.3, of his book on linear dynamical systems 1 , Professor Roger Brockett asks: If $$\dot{\mathbf{x}}(t) = A(t) x(t) , \qquad \mathbf{x}(0) = \mathbf{x}_0$$ and $$\dot{\mathbf{p}}(t) = -A^T(t) \mathbf{p}(t), \qquad \mathbf{p}(0) = \mathbf{p}_0 \quad (\mbox{Adjoint System}), $$ show that $$ \langle \mathbf{x}(t), \mathbf{p}(t) \rangle = \langle \mathbf{x}_0, \mathbf{p}_0 \rangle \quad\mbox{for all} \ t \in \mathbf{R} $$ My take An easy way to show that a certain function $\psi(t)$ is a constant function is to show that $\dot{\psi}(t) \equiv 0$ . Thus, we find that $$ {d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, =  \, \langle \dot{\mathbf{x}}(t), \mathbf{p}(t) \rangle + \langle \mathbf{x}(t), \dot{\mathbf{p}}(t) \rangle  $$ That is, $$ {d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, = \, \langle A(t) \mathbf{x}(t), \mathbf{p}(t) \rangle +  \langle \mathbf{x}(t), - A^T(t) \mathbf{p}(t) \rangle $$ Simplifying, we get $$ {d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, = - \mathbf{x}^T(t) A^T(t)  \mathbf{p}(t) +  \mathbf{x}^T(t) A^T(t)  \mathbf{p}(t) \equiv 0 $$ This shows that $$ \langle \mathbf{x}(t), \mathbf{p}(t) \rangle = \langle \mathbf{x}_0, \mathbf{p}_0 \rangle \ \ \mbox{for all} \ t \in \mathbf{R} $$ I hope that the calculations are correct. I would like to learn more on the adjoint of a linear dynamical system. Is there any geometric interpretation for the adjoint of a linear dynamical system and the identity established in the Brockett's exercise problem? References Roger W. Brockett, Finite Dimensional Linear Systems , Wiley, 1970.","On page 26, Section 1.3, of his book on linear dynamical systems 1 , Professor Roger Brockett asks: If and show that My take An easy way to show that a certain function is a constant function is to show that . Thus, we find that That is, Simplifying, we get This shows that I hope that the calculations are correct. I would like to learn more on the adjoint of a linear dynamical system. Is there any geometric interpretation for the adjoint of a linear dynamical system and the identity established in the Brockett's exercise problem? References Roger W. Brockett, Finite Dimensional Linear Systems , Wiley, 1970.","\dot{\mathbf{x}}(t) = A(t) x(t) , \qquad \mathbf{x}(0) = \mathbf{x}_0 \dot{\mathbf{p}}(t) = -A^T(t) \mathbf{p}(t), \qquad \mathbf{p}(0) = \mathbf{p}_0 \quad (\mbox{Adjoint System}),
  \langle \mathbf{x}(t), \mathbf{p}(t) \rangle = \langle \mathbf{x}_0, \mathbf{p}_0 \rangle \quad\mbox{for all} \ t \in
\mathbf{R}
 \psi(t) \dot{\psi}(t) \equiv 0 
{d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, =  \, \langle \dot{\mathbf{x}}(t), \mathbf{p}(t) \rangle
+ \langle \mathbf{x}(t), \dot{\mathbf{p}}(t) \rangle 
 
{d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, = \, \langle A(t) \mathbf{x}(t), \mathbf{p}(t) \rangle
+  \langle \mathbf{x}(t), - A^T(t) \mathbf{p}(t) \rangle
 
{d \over dt} \langle \mathbf{x}(t), \mathbf{p}(t) \rangle \, = - \mathbf{x}^T(t) A^T(t)  \mathbf{p}(t) +  \mathbf{x}^T(t) A^T(t)  \mathbf{p}(t) \equiv 0
 
\langle \mathbf{x}(t), \mathbf{p}(t) \rangle = \langle \mathbf{x}_0, \mathbf{p}_0 \rangle \ \ \mbox{for all} \ t \in
\mathbf{R}
","['ordinary-differential-equations', 'hilbert-spaces', 'control-theory', 'linear-control', 'geometric-interpretation']"
46,How to solve this 2nd ODE with simple inequality constraints numerically?,How to solve this 2nd ODE with simple inequality constraints numerically?,,"I have a 2nd ODE (derived from an elastic rod deflated naturally under its self-weight): $$ y'' + K (x - 1) \cos(y) = 0  $$ K is a constant coefficient. The variables range between $x \in [0, 1], y \in [-\pi / 2, 0]$ The BCs: $$ y(0) = 0 \\ y'(1) = 0 $$ and a constraint $y' \le 0$ and $y'' \ge 0$ (from physical meaning). How can I solve it numerically? I have a weak background in the theory/classfication of ODEs, but I still tried to analysis and solve it as followed: 1. My own analysis on this problem This is an second order nonlinear ODE. Its BCs are located in two sides, so it is a BVP. A common practice is to split it into a 1st ODE system: $$ \begin{cases} y_2 = y_1' \\ y_2' + K (x - 1) \cos (y_1) = 0 \end{cases} $$ Then I discrete all derivates by explicit euler method, and employe shooting method (where I guess the init BCs y'(0) and adjust this guess for a satisfying solution). When K is big, I can solve it properly. But when K is small, my shooting method always failed. I have no idea how to analysis it anymore. 2. My questions Is my judgement about this problem correct? Is it an 2nd order ODE, Bounday value problems with not enough BCs? Why I can solve it properly in non-stiff case, and fail in stiff case? Some literatures point out that, auto07p package can help me on this problem. But I cannot understand its manual even a bit. Which book should I read to handle this problem? How can I know whether this problem have a solution? How to solve this problem numerically? Sorry for my weak background in ODE. Thanks a lot for your time! 3. solve_bvp and shooting failed when K > 200 and ignore the constraint When K > 200, we need to set up a special init guess for solve_bvp , otherwise we will fail. Please check @Lutz 's answer below. $","I have a 2nd ODE (derived from an elastic rod deflated naturally under its self-weight): K is a constant coefficient. The variables range between The BCs: and a constraint and (from physical meaning). How can I solve it numerically? I have a weak background in the theory/classfication of ODEs, but I still tried to analysis and solve it as followed: 1. My own analysis on this problem This is an second order nonlinear ODE. Its BCs are located in two sides, so it is a BVP. A common practice is to split it into a 1st ODE system: Then I discrete all derivates by explicit euler method, and employe shooting method (where I guess the init BCs y'(0) and adjust this guess for a satisfying solution). When K is big, I can solve it properly. But when K is small, my shooting method always failed. I have no idea how to analysis it anymore. 2. My questions Is my judgement about this problem correct? Is it an 2nd order ODE, Bounday value problems with not enough BCs? Why I can solve it properly in non-stiff case, and fail in stiff case? Some literatures point out that, auto07p package can help me on this problem. But I cannot understand its manual even a bit. Which book should I read to handle this problem? How can I know whether this problem have a solution? How to solve this problem numerically? Sorry for my weak background in ODE. Thanks a lot for your time! 3. solve_bvp and shooting failed when K > 200 and ignore the constraint When K > 200, we need to set up a special init guess for solve_bvp , otherwise we will fail. Please check @Lutz 's answer below. $","
y'' + K (x - 1) \cos(y) = 0 
 x \in [0, 1], y \in [-\pi / 2, 0] 
y(0) = 0 \\
y'(1) = 0
 y' \le 0 y'' \ge 0 
\begin{cases}
y_2 = y_1' \\
y_2' + K (x - 1) \cos (y_1) = 0
\end{cases}
","['ordinary-differential-equations', 'numerical-methods', 'boundary-value-problem']"
47,Question about Anderson Localization and a specific theorem (RAGE),Question about Anderson Localization and a specific theorem (RAGE),,"I have a question with a theorem which appears in a text called “Invitation to Random Schrödinger Operators”, in unit 7. Theorem 7.7. Let $H$ be a selfadjoint operator on Hilbert Space, take $\psi \in H_{pp}$ and let $\Lambda_L$ denote a cube in $\mathbb Z^d$ centered at the origin with side length $2L+1$ . Then: (1) $$\lim_{L \to \infty} \sup_{t \geq 0} \left(\sum_{x \in \Lambda_L} |e^{-itH} \psi(x)|^2 \right) = ||\psi||^2$$ and (2) $$\lim_{L \to \infty} \sup_{t \geq 0} \left(\sum_{x \notin \Lambda_L} |e^{-itH} \psi(x)|^2 \right) = 0 $$ Proof: Since $e^{-itH}$ is unitary, we have for all $t$ $$||\psi||^2 =||e^{-itH} \psi||^2 = \sum_{x \in \Lambda_L } |e^{-itH} \psi(x)|^2 + \sum_{x \notin \Lambda_L} |e^{-itH}\psi(x)|^2 $$ Above we saw that the expression is valid for eigenfunctions $\psi$ . To prove it for other vectors in $H_{pp}$ we introduce the following notation: By $P_L$ we denote the projection onto $C\Lambda_L$ . Then equation (2) claims that: $$ ||P_L e^{-itH} \psi||^2 \to 0$$ uniformly in $t$ as $L \to \infty$ . If $\psi$ is a finite linear combination of eigenfunctions, say $\psi=\sum_{m=1}^M \alpha_k \psi_k$ , $H\psi_k=E_k \psi_k$ , then: (3) $$ ||P_L e^{-itH} \psi || = || \sum_{m=1}^M \alpha_k P_L e^{-itH} \psi_k || \leq \sum_{m=1}^M |\alpha_k| ||P_L e^{-itH} \psi_k || = \sum_{m=1}^M |\alpha_k| ||P_L e^{-itE_k} \psi_k|| = \sum_{m=1}^M |\alpha_k| ||P_L \psi_k||$$ From this moment, it is that I begin to have problems to understand By taking $L$ large enough, each term in the sum above can be made smaller then $(\sum_{m=1}^M |\alpha_k|)^{-1} \varepsilon$ . If now $\psi$ is an arbitrary element of $H_{pp}$ , there is a linear combination of eigenfunctions $\psi^{(M)} = \sum_{m=1}^M \alpha_k \psi_k$ such that $||\psi-\psi^{(M)}|| < \varepsilon $ $||P_L e^{-itH} \psi|| \leq ||P_L e^{-itH} \psi^{(M)} || + ||P_L e^{-itH} (\psi - \psi^{(M)}) || \leq ||P_L e^{-itH} \psi^{(M)}  || + || \psi - \psi^{(M)}||$ GUYS I can’t understand how $||P_L e^{-itH} (\psi - \psi^{(M)} ) || \leq || \psi - \psi^{(M)}||$ Thanks!","I have a question with a theorem which appears in a text called “Invitation to Random Schrödinger Operators”, in unit 7. Theorem 7.7. Let be a selfadjoint operator on Hilbert Space, take and let denote a cube in centered at the origin with side length . Then: (1) and (2) Proof: Since is unitary, we have for all Above we saw that the expression is valid for eigenfunctions . To prove it for other vectors in we introduce the following notation: By we denote the projection onto . Then equation (2) claims that: uniformly in as . If is a finite linear combination of eigenfunctions, say , , then: (3) From this moment, it is that I begin to have problems to understand By taking large enough, each term in the sum above can be made smaller then . If now is an arbitrary element of , there is a linear combination of eigenfunctions such that GUYS I can’t understand how Thanks!",H \psi \in H_{pp} \Lambda_L \mathbb Z^d 2L+1 \lim_{L \to \infty} \sup_{t \geq 0} \left(\sum_{x \in \Lambda_L} |e^{-itH} \psi(x)|^2 \right) = ||\psi||^2 \lim_{L \to \infty} \sup_{t \geq 0} \left(\sum_{x \notin \Lambda_L} |e^{-itH} \psi(x)|^2 \right) = 0  e^{-itH} t ||\psi||^2 =||e^{-itH} \psi||^2 = \sum_{x \in \Lambda_L } |e^{-itH} \psi(x)|^2 + \sum_{x \notin \Lambda_L} |e^{-itH}\psi(x)|^2  \psi H_{pp} P_L C\Lambda_L  ||P_L e^{-itH} \psi||^2 \to 0 t L \to \infty \psi \psi=\sum_{m=1}^M \alpha_k \psi_k H\psi_k=E_k \psi_k  ||P_L e^{-itH} \psi || = || \sum_{m=1}^M \alpha_k P_L e^{-itH} \psi_k || \leq \sum_{m=1}^M |\alpha_k| ||P_L e^{-itH} \psi_k || = \sum_{m=1}^M |\alpha_k| ||P_L e^{-itE_k} \psi_k|| = \sum_{m=1}^M |\alpha_k| ||P_L \psi_k|| L (\sum_{m=1}^M |\alpha_k|)^{-1} \varepsilon \psi H_{pp} \psi^{(M)} = \sum_{m=1}^M \alpha_k \psi_k ||\psi-\psi^{(M)}|| < \varepsilon  ||P_L e^{-itH} \psi|| \leq ||P_L e^{-itH} \psi^{(M)} || + ||P_L e^{-itH} (\psi - \psi^{(M)}) || \leq ||P_L e^{-itH} \psi^{(M)}  || + || \psi - \psi^{(M)}|| ||P_L e^{-itH} (\psi - \psi^{(M)} ) || \leq || \psi - \psi^{(M)}||,"['ordinary-differential-equations', 'partial-differential-equations', 'spectral-theory', 'localization', 'self-adjoint-operators']"
48,"Proof verification: if $f$ is a solution of $(y')^2+y^2=1$ that is defined over $\mathbb{R}$, then $f$ attains at least one of $1,-1$ as a value.","Proof verification: if  is a solution of  that is defined over , then  attains at least one of  as a value.","f (y')^2+y^2=1 \mathbb{R} f 1,-1","Context : I'd like to prove that for any pair of numbers $(x_0,y_0)$ with $|y_0|\leq 1$ , the initial value problem $$(y')^2+y^2=1,y(x_0)=y_0$$ will have infinitely many solutions that are defined over $\mathbb{R}$ . In my analysis of the differential equation, I've determined that it locally has exactly two solutions when $|y_0|<1$ , each a sine wave, and that these ""branch off"", so to speak, to infinitely many when their graphs reach $\pm 1$ . Since I'm interested in solutions defined over $\mathbb{R}$ , I can prove the infinitude of solutions by showing that any $\mathbb R$ -defined solution attains either $1$ or $-1$ as a value. Problem : if $f$ is a $\mathbb R$ -defined solution of $(y')^2+y^2=1$ , is it true that $f(x_0)=1$ or $f(x_0)=-1$ for some $x_0\in\mathbb R$ ? I've been thinking about this problem for a while now, and believe I have conceived of an argument that proves the answer is yes. Here it is: Proof : suppose, for the sake of finding a contradiction, that $f$ is a $\mathbb R$ -defined solution of $(y')^2+y^2=1$ that never attains $1$ nor $-1$ as a value. Since the differential equation implies that $f$ must satisfy $-1\leq f(x)\leq 1$ for every $x\in\mathbb R$ , it follows from our assumption that $-1<f(x)<1$ for every $x$ , or $\left|f(x)\right|<1$ . Note that the ODE is equivalent to $|y'|=\sqrt{1-y^2}$ , so $\left|f(x)\right|<1$ and the fact that $\sqrt{1-t^2}>0$ when $|t|<1$ implies that $|f'(x)|>0$ for every $x$ . This yields the following result: Result 1 : For any $x$ , either $f'(x)>0$ or $f'(x)<0$ . For what follows, we'll need to strengthen Result $1$ to the following: Result 2 : $f'$ is either strictly positive or strictly negative. Proof : From Result $1$ , we have that either $f'(0)>0$ or $f'(0)<0$ . Suppose $f'(0)>0$ and pick any $x_1\neq 0$ . From Result $1$ , it follows that either $f'(x_1)>0$ or $f'(x_1)<0$ . Notice that it is not possible for $f'(x_1)<0$ to be true because Darboux's theorem would then imply that $f'(c)=0$ for some $c$ between $0$ and $x_1$ , contradicting Result $1$ . Thus, $f'(x_1)>0$ ; since $x_1$ was arbitrary, this proves that $f'$ is strictly positive. The argument for $f'(0)<0$ is pretty much identical, so it is omitted. Thus, $f'$ is either strictly positive or strictly negative. With this established, it follows that either $$f'(x)=\sqrt{1-[f(x)]^2}\text{ for every }x\text{, or }f'(x)=-\sqrt{1-[f(x)]^2}\text{ for every }x$$ or, because $\sqrt{1-[f(x)]^2}>0$ , $$\frac{f'(t)}{\sqrt{1-[f(t)]^2}}=1\text{ for every }t\text{, or }\frac{f'(t)}{\sqrt{1-[f(t)]^2}}=-1\text{ for every }t$$ Integrating each case from $x_0$ to $x$ , using $\int 1/\sqrt{1-x^2}dx=\sin^{-1}(x)$ , and rearranging, we get $$f(x)=\sin\left(\sin^{-1}\left(f(x_0)\right)+x-x_0\right)\text{ for every }x$$ $$\text{or}$$ $$f(x)=\sin\left(\sin^{-1}\left(f(x_0)\right)-(x-x_0)\right)\text{ for every }x$$ If the correct expression for $f$ is the first one, then we can evaluate it at $\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)$ and $-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)$ . This is justified because the domain of $f$ is $\mathbb{R}$ . \begin{align} f\left(\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)\right) &= \sin\left(\sin^{-1}\left(f(x_0)\right)+\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)-x_0\right)\\ &= \sin\left(\frac{\pi}{2}\right)\\ &= 1 \end{align} \begin{align} f\left(-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)\right) &= \sin\left(\sin^{-1}\left(f(x_0)\right)-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)-x_0\right)\\ &= \sin\left(-\frac{\pi}{2}\right)\\ &= -1 \end{align} This contradicts the assumption that $f$ never attains $1$ nor $-1$ . We can reach a contradiction from the second expression too; just evaluate $f$ at $\frac{\pi}{2}+x_0\color{red}{+}\sin^{-1}\left(f(x_0)\right)$ and $-\frac{\pi}{2}+x_0\color{red}{+}\sin^{-1}\left(f(x_0)\right)$ instead. Thus, our original assumption must be false, so any $\mathbb R$ -defined solution always attains at least one of $1,-1$ as a value. $\blacksquare$ I appreciate any and all feedback.","Context : I'd like to prove that for any pair of numbers with , the initial value problem will have infinitely many solutions that are defined over . In my analysis of the differential equation, I've determined that it locally has exactly two solutions when , each a sine wave, and that these ""branch off"", so to speak, to infinitely many when their graphs reach . Since I'm interested in solutions defined over , I can prove the infinitude of solutions by showing that any -defined solution attains either or as a value. Problem : if is a -defined solution of , is it true that or for some ? I've been thinking about this problem for a while now, and believe I have conceived of an argument that proves the answer is yes. Here it is: Proof : suppose, for the sake of finding a contradiction, that is a -defined solution of that never attains nor as a value. Since the differential equation implies that must satisfy for every , it follows from our assumption that for every , or . Note that the ODE is equivalent to , so and the fact that when implies that for every . This yields the following result: Result 1 : For any , either or . For what follows, we'll need to strengthen Result to the following: Result 2 : is either strictly positive or strictly negative. Proof : From Result , we have that either or . Suppose and pick any . From Result , it follows that either or . Notice that it is not possible for to be true because Darboux's theorem would then imply that for some between and , contradicting Result . Thus, ; since was arbitrary, this proves that is strictly positive. The argument for is pretty much identical, so it is omitted. Thus, is either strictly positive or strictly negative. With this established, it follows that either or, because , Integrating each case from to , using , and rearranging, we get If the correct expression for is the first one, then we can evaluate it at and . This is justified because the domain of is . This contradicts the assumption that never attains nor . We can reach a contradiction from the second expression too; just evaluate at and instead. Thus, our original assumption must be false, so any -defined solution always attains at least one of as a value. I appreciate any and all feedback.","(x_0,y_0) |y_0|\leq 1 (y')^2+y^2=1,y(x_0)=y_0 \mathbb{R} |y_0|<1 \pm 1 \mathbb{R} \mathbb R 1 -1 f \mathbb R (y')^2+y^2=1 f(x_0)=1 f(x_0)=-1 x_0\in\mathbb R f \mathbb R (y')^2+y^2=1 1 -1 f -1\leq f(x)\leq 1 x\in\mathbb R -1<f(x)<1 x \left|f(x)\right|<1 |y'|=\sqrt{1-y^2} \left|f(x)\right|<1 \sqrt{1-t^2}>0 |t|<1 |f'(x)|>0 x x f'(x)>0 f'(x)<0 1 f' 1 f'(0)>0 f'(0)<0 f'(0)>0 x_1\neq 0 1 f'(x_1)>0 f'(x_1)<0 f'(x_1)<0 f'(c)=0 c 0 x_1 1 f'(x_1)>0 x_1 f' f'(0)<0 f' f'(x)=\sqrt{1-[f(x)]^2}\text{ for every }x\text{, or }f'(x)=-\sqrt{1-[f(x)]^2}\text{ for every }x \sqrt{1-[f(x)]^2}>0 \frac{f'(t)}{\sqrt{1-[f(t)]^2}}=1\text{ for every }t\text{, or }\frac{f'(t)}{\sqrt{1-[f(t)]^2}}=-1\text{ for every }t x_0 x \int 1/\sqrt{1-x^2}dx=\sin^{-1}(x) f(x)=\sin\left(\sin^{-1}\left(f(x_0)\right)+x-x_0\right)\text{ for every }x \text{or} f(x)=\sin\left(\sin^{-1}\left(f(x_0)\right)-(x-x_0)\right)\text{ for every }x f \frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right) -\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right) f \mathbb{R} \begin{align}
f\left(\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)\right) &= \sin\left(\sin^{-1}\left(f(x_0)\right)+\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)-x_0\right)\\
&= \sin\left(\frac{\pi}{2}\right)\\
&= 1
\end{align} \begin{align}
f\left(-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)\right) &= \sin\left(\sin^{-1}\left(f(x_0)\right)-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)-x_0\right)\\
&= \sin\left(-\frac{\pi}{2}\right)\\
&= -1
\end{align} f 1 -1 f \frac{\pi}{2}+x_0\color{red}{+}\sin^{-1}\left(f(x_0)\right) -\frac{\pi}{2}+x_0\color{red}{+}\sin^{-1}\left(f(x_0)\right) \mathbb R 1,-1 \blacksquare","['calculus', 'ordinary-differential-equations', 'solution-verification']"
49,"$\frac{\partial f}{\partial t}-\frac{k}{r^2} \left( \frac{\partial}{\partial r} \left( r^ \, \frac{\partial f}{\partial r} \right)-2f\right)+g(r,t)=0$",,"\frac{\partial f}{\partial t}-\frac{k}{r^2} \left( \frac{\partial}{\partial r} \left( r^ \, \frac{\partial f}{\partial r} \right)-2f\right)+g(r,t)=0","Consider the following partial differential equation $$ \frac{\partial f (r,t)}{\partial t} - \frac{k}{r^2}  \left( \frac{\partial}{\partial r} \left( r^ \, \frac{\partial f (r,t)}{\partial r} \right) - 2f(r,t) \right) + \frac{f(r=1, t)}{r^2}  = 0\, ,  $$ subject to the boundary conditions $$ \left. \frac{\partial f}{\partial r} \right|_{r=1} = 0\, , \qquad f(r=\infty, t) = 0 \, ,  $$ together with the initial condition $f(r, t=0) = \epsilon$ . Here, $k$ and $\epsilon$ are positive real numbers, with $|\epsilon| \ll 1$ To search an analytical solution of this PDE, i used the method of separation of variables. Accordingly, the solution is expressed as $f(r,t) = F(r)G(t)$ . Using this approach, it can be shown that $G(t)$ satisfies the following differential equation $$ G'(t) + \operatorname{const.} G(t) = 0 \, . $$ The latter admit the exponential function as a solution. The problem is that, by considering an decaying exponential, one would obtain a vanishing solution in the long-time limit as $t \to \infty$ . This is in contradiction with the numerical solution where, for instance, $f(r=1,t)$ reached a steady/plateau value in the limit $t \to \infty$ . Does this mean that one cannot employ the method of separation of variables in this problem?","Consider the following partial differential equation subject to the boundary conditions together with the initial condition . Here, and are positive real numbers, with To search an analytical solution of this PDE, i used the method of separation of variables. Accordingly, the solution is expressed as . Using this approach, it can be shown that satisfies the following differential equation The latter admit the exponential function as a solution. The problem is that, by considering an decaying exponential, one would obtain a vanishing solution in the long-time limit as . This is in contradiction with the numerical solution where, for instance, reached a steady/plateau value in the limit . Does this mean that one cannot employ the method of separation of variables in this problem?","
\frac{\partial f (r,t)}{\partial t} - \frac{k}{r^2} 
\left( \frac{\partial}{\partial r} \left( r^ \, \frac{\partial f (r,t)}{\partial r} \right) - 2f(r,t) \right)
+ \frac{f(r=1, t)}{r^2}  = 0\, , 
 
\left. \frac{\partial f}{\partial r} \right|_{r=1} = 0\, , \qquad f(r=\infty, t) = 0 \, , 
 f(r, t=0) = \epsilon k \epsilon |\epsilon| \ll 1 f(r,t) = F(r)G(t) G(t) 
G'(t) + \operatorname{const.} G(t) = 0 \, .
 t \to \infty f(r=1,t) t \to \infty","['real-analysis', 'integration', 'ordinary-differential-equations', 'partial-differential-equations', 'nonlinear-dynamics']"
50,Showing $\frac{\sinh t}{\sinh T}$ extremises the functional $S[x]=\int_0^T\big(\dot x(t)^2+x(t)^2\big)\ \mathrm dt$ with given boundary conditions,Showing  extremises the functional  with given boundary conditions,\frac{\sinh t}{\sinh T} S[x]=\int_0^T\big(\dot x(t)^2+x(t)^2\big)\ \mathrm dt,"Show that $t\mapsto\frac{\sinh t}{\sinh T}$ extremises the functional $S[x] = \int_0^T \left( \dot{x}(t)^2 + x(t)^2 \right) \ \mathrm dt$ with boundary conditions $x(0) = 0$ and $ x(T) = 1$ . I can show that $\frac{\sinh t}{\sinh T}$ is one solution to the resultant ODE, $\ddot{x} = x$ after simplifying the functional in the title, but I can't think of a way to prove that it is an extremal solution (specifically, it is supposed to be a minimum , according to the solutions manual [1]). How may I proceed? [1]This problem is 3-5 of James Hartle's Gravity: A Gentle Introduction to Einstein's General Relativity .","Show that extremises the functional with boundary conditions and . I can show that is one solution to the resultant ODE, after simplifying the functional in the title, but I can't think of a way to prove that it is an extremal solution (specifically, it is supposed to be a minimum , according to the solutions manual [1]). How may I proceed? [1]This problem is 3-5 of James Hartle's Gravity: A Gentle Introduction to Einstein's General Relativity .",t\mapsto\frac{\sinh t}{\sinh T} S[x] = \int_0^T \left( \dot{x}(t)^2 + x(t)^2 \right) \ \mathrm dt x(0) = 0  x(T) = 1 \frac{\sinh t}{\sinh T} \ddot{x} = x,"['ordinary-differential-equations', 'functional-equations', 'calculus-of-variations']"
51,Finding the vector field of ODE in cartesian coordinate,Finding the vector field of ODE in cartesian coordinate,,"The way I approached the problem is first by solving a system of equations: $$ r_x \frac{dx}{dt} + r_y\frac{dy}{dt} = \frac{dr}{dt} = -r $$ $$ \theta_x \frac{dx}{dt} + \theta_y\frac{dy}{dt} = \frac{2}{\ln(x^2+y^2)} $$ where $r = \sqrt{x^2+y^2}$ and $\theta = tan^{-1}\frac yx$ . The problem is after solving for $\frac {dx} {dt}$ and $\frac{dy}{dt}$ I got the following vector field: $$ \left(\frac{dx}{dt}, \frac{dy}{dt}\right) = \left(-x-\frac{2y}{ln(x^2+y^2)},-y+\frac{2x}{ln(x^2+y^2)}\right) $$ And this mess doesn't seem to be differentiable at $(0,0)$ . Anyone got some alternative approach or can point out where I made a mistake? Thanks! EDIT: I found somewhere I made a mistake in getting to the vector field so here I edited the problem to show the new (hopefully correct) vector field obtained. However it is still not differentiable at the origin.",The way I approached the problem is first by solving a system of equations: where and . The problem is after solving for and I got the following vector field: And this mess doesn't seem to be differentiable at . Anyone got some alternative approach or can point out where I made a mistake? Thanks! EDIT: I found somewhere I made a mistake in getting to the vector field so here I edited the problem to show the new (hopefully correct) vector field obtained. However it is still not differentiable at the origin.,"
r_x \frac{dx}{dt} + r_y\frac{dy}{dt} = \frac{dr}{dt} = -r
 
\theta_x \frac{dx}{dt} + \theta_y\frac{dy}{dt} = \frac{2}{\ln(x^2+y^2)}
 r = \sqrt{x^2+y^2} \theta = tan^{-1}\frac yx \frac {dx} {dt} \frac{dy}{dt} 
\left(\frac{dx}{dt}, \frac{dy}{dt}\right) = \left(-x-\frac{2y}{ln(x^2+y^2)},-y+\frac{2x}{ln(x^2+y^2)}\right)
 (0,0)",['ordinary-differential-equations']
52,"Existence of solution ""because it's real""","Existence of solution ""because it's real""",,"Differential equations coming from physics have a very peculiar aspect: given initial conditions that physically make sense, there must exist a (unique) solution. This is just because we know that in nature the phenomenon actually happens, and in deterministic theories there is only one way in which the phenomenon can evolve from initial conditions. Are there examples of (partial) differential equations from physics for which is hard to show existence and uniqueness? As an example, take n planets that interacts following the gravitational law. The associated differential equation will contain distances between points in the denominator, so it is not clear a priori that solutions will not blow up. However, discarding cases in which planets collide, the solution actually exist, but it is not manifest from regularity of the equations. Another example could be taken from the heat flow equations. In case such an example is found, I would like to know which actual methods are used to prove uniqueness and existence, and if the physical intuition actually plays a role.","Differential equations coming from physics have a very peculiar aspect: given initial conditions that physically make sense, there must exist a (unique) solution. This is just because we know that in nature the phenomenon actually happens, and in deterministic theories there is only one way in which the phenomenon can evolve from initial conditions. Are there examples of (partial) differential equations from physics for which is hard to show existence and uniqueness? As an example, take n planets that interacts following the gravitational law. The associated differential equation will contain distances between points in the denominator, so it is not clear a priori that solutions will not blow up. However, discarding cases in which planets collide, the solution actually exist, but it is not manifest from regularity of the equations. Another example could be taken from the heat flow equations. In case such an example is found, I would like to know which actual methods are used to prove uniqueness and existence, and if the physical intuition actually plays a role.",,"['ordinary-differential-equations', 'partial-differential-equations', 'soft-question', 'mathematical-physics', 'math-history']"
53,Proving the solution of an ode converges as $t$ goes to infinity given its norm is bounded by a convergent function,Proving the solution of an ode converges as  goes to infinity given its norm is bounded by a convergent function,t,"Question Let $f \in \mathcal C(\mathbb R_+ \times \mathbb R^n, \mathbb R^n)$ and $g \in \mathcal C(\mathbb R_+ \times \mathbb R, \mathbb R_+)$ . Assume that $g(t,u)$ is non-decreasing in $u$ for each fixed $t$ and $\vert f(t,x)\vert \lt g(t,\vert x \vert), (t,x)\in \mathbb R_+ \times \mathbb R^n$ Every solution $u(t) = u(t,t_0,u_0)$ of the IVP $u^\prime(t) = g(t,u), u(t_0)=u_0$ is bounded. Show that every solution of the IVP $x^\prime(t) = f(t,x), x(t_0) = x_0$ exists on $[t_0,\infty)$ and converges to a constant vector as $t \to \infty$ . My Attempt I have completed the first part of the question (show every solution of the IVP $x' = f(t,x), x(t_0) = x_0$ exists on $[t_0,\infty)$ ) using following corollary. Corollary : Let $f \in \mathcal C(J \times \mathbb R^n, \mathbb R^n)$ where $J=[a,b)$ and $\vert f(t,x) \vert \lt g(t,\vert x \vert)$ for $(t,x)\in J \times R^n$ and $g \in \mathcal C(\mathbb R_+ \times \mathbb R, \mathbb R_+)$ . If $\gamma(t)$ is the maximal solution of $u^\prime = g(t,u), u(t_0)=u_0$ existing on $J$ , then the solution $x(t) = x(t,t_0,x_0)$ of $x^\prime = f(t,x), x(t_0) = x_0$ where $\vert x_0 \vert \leq u_0$ exists on $J$ and $\vert x(t) \vert \leq \gamma(t)$ for all $t\in J$ . Simply let $\gamma(t)$ be the maximal solution of $u^\prime = g(t,u), u(t_0)=\vert x_0 \vert$ . Since we know all solutions of this IVP are bounded (by (2)), we know that $\gamma(t)$ must exist on $J = [t_0,\infty)$ . By the corollary, $x(t)$ also exists on $[t_0,\infty)$ . Moreover, $\vert x(t) \vert<\gamma(t)\forall t\in[t_0,\infty)$ . Having said that, I do not know how to prove that $x(t)$ must converge. I have the following results: Since $g(t,u)$ is always positive, we know that $\gamma(t)$ is an increasing bounded continuous function of $t$ (by (2)). Thus, $\gamma(t)$ must converge. Hence, $\vert x(t) \vert$ is bounded from above by a convergent function. $g(t,u)$ is a non-decreasing function of $u$ for every fixed $t$ , thus $\vert x^\prime(t) \vert \leq \vert f(t,x) \vert \leq \vert g(t,\vert x(t) \vert) \vert \leq \vert (g(t,\gamma(t)) $ but I'm not sure what they imply about the convergence of $x(t)$ . Any help would be greatly appreciated.","Question Let and . Assume that is non-decreasing in for each fixed and Every solution of the IVP is bounded. Show that every solution of the IVP exists on and converges to a constant vector as . My Attempt I have completed the first part of the question (show every solution of the IVP exists on ) using following corollary. Corollary : Let where and for and . If is the maximal solution of existing on , then the solution of where exists on and for all . Simply let be the maximal solution of . Since we know all solutions of this IVP are bounded (by (2)), we know that must exist on . By the corollary, also exists on . Moreover, . Having said that, I do not know how to prove that must converge. I have the following results: Since is always positive, we know that is an increasing bounded continuous function of (by (2)). Thus, must converge. Hence, is bounded from above by a convergent function. is a non-decreasing function of for every fixed , thus but I'm not sure what they imply about the convergence of . Any help would be greatly appreciated.","f \in \mathcal C(\mathbb R_+ \times \mathbb R^n, \mathbb R^n) g \in \mathcal C(\mathbb R_+ \times \mathbb R, \mathbb R_+) g(t,u) u t \vert f(t,x)\vert \lt g(t,\vert x \vert), (t,x)\in \mathbb R_+ \times \mathbb R^n u(t) = u(t,t_0,u_0) u^\prime(t) = g(t,u),
u(t_0)=u_0 x^\prime(t) = f(t,x), x(t_0) = x_0 [t_0,\infty) t \to \infty x' = f(t,x), x(t_0) = x_0 [t_0,\infty) f \in \mathcal C(J \times \mathbb R^n, \mathbb R^n) J=[a,b) \vert f(t,x) \vert \lt g(t,\vert x \vert) (t,x)\in J \times R^n g \in \mathcal C(\mathbb R_+ \times \mathbb R, \mathbb R_+) \gamma(t) u^\prime = g(t,u), u(t_0)=u_0 J x(t) = x(t,t_0,x_0) x^\prime = f(t,x), x(t_0) = x_0 \vert x_0 \vert \leq u_0 J \vert x(t) \vert \leq \gamma(t) t\in J \gamma(t) u^\prime = g(t,u), u(t_0)=\vert x_0 \vert \gamma(t) J = [t_0,\infty) x(t) [t_0,\infty) \vert x(t) \vert<\gamma(t)\forall t\in[t_0,\infty) x(t) g(t,u) \gamma(t) t \gamma(t) \vert x(t) \vert g(t,u) u t \vert x^\prime(t) \vert \leq \vert f(t,x) \vert \leq \vert g(t,\vert x(t) \vert) \vert \leq \vert (g(t,\gamma(t))  x(t)","['real-analysis', 'ordinary-differential-equations', 'limits', 'derivatives']"
54,How to take the first derivative of the entropy-regularized Wasserstein distance?,How to take the first derivative of the entropy-regularized Wasserstein distance?,,"In optimal transportation theory, it is better to regularize the Wasserstein distance with an entropy constraint because it is differentiable, unlike its unregularized counterpart. By being differentiable, it can then be treated as a loss function that is compatible with common optimization algorithms. The entropy-regularized Wasserstein distance (aka Sinkhorn distance) is: $$ \text{inf} _{\gamma \in \Pi } \sum \|x - y\| \enspace \gamma(x,y)- \epsilon H(\gamma)$$ where $H(\gamma) = -\sum \gamma \text{ log}(\gamma)$ is the Shannon entropy of the transport plan $\gamma$ , and $\epsilon $ is the regularization parameter. What then is the derivative of the above formula, could someone show how to derive it?","In optimal transportation theory, it is better to regularize the Wasserstein distance with an entropy constraint because it is differentiable, unlike its unregularized counterpart. By being differentiable, it can then be treated as a loss function that is compatible with common optimization algorithms. The entropy-regularized Wasserstein distance (aka Sinkhorn distance) is: where is the Shannon entropy of the transport plan , and is the regularization parameter. What then is the derivative of the above formula, could someone show how to derive it?"," \text{inf} _{\gamma \in \Pi } \sum \|x - y\| \enspace \gamma(x,y)- \epsilon H(\gamma) H(\gamma) = -\sum \gamma \text{ log}(\gamma) \gamma \epsilon ","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'entropy', 'optimal-transport']"
55,Does there exist a Laplace series for the Laplace transform?,Does there exist a Laplace series for the Laplace transform?,,I am trying to get a better understanding of what the Laplace transform is. When I learnt about the Fourier transform I found it useful to learn about the Fourier series first. Does there exist some kind of Laplace series which is to the Laplace transform what the Fourier series is to the Fourier transform?,I am trying to get a better understanding of what the Laplace transform is. When I learnt about the Fourier transform I found it useful to learn about the Fourier series first. Does there exist some kind of Laplace series which is to the Laplace transform what the Fourier series is to the Fourier transform?,,"['ordinary-differential-equations', 'fourier-series', 'laplace-transform', 'fourier-transform']"
56,"Two monotone functions have the same derivative when both differentiable, and the same discontinuities. Must they differ by a constant?","Two monotone functions have the same derivative when both differentiable, and the same discontinuities. Must they differ by a constant?",,"Suppose $f: [0,1]\to [0,1]$ and $g: [0,1]\to [0,1]$ both satisfy the following: (1) They are both weakly increasing. (2) They are discontinuous at the same set of points. Moreover, if $f$ and $g$ are discontinuous at $x$ , then $$\lim_{z\rightarrow x^{+}}f(z)-\lim_{z\rightarrow x^{-}}f(z)=\lim_{z\rightarrow x^{+}}g(z)-\lim_{z\rightarrow x^{-}}g(z)$$ (3) If $f$ and $g$ are both differentiable at $x$ , then $f'(x)=g'(x)$ . Is it true that for any $x$ and $z$ where $f$ and $g$ are continuous, $f(x)-g(x)=f(z)-g(z)$ ?","Suppose and both satisfy the following: (1) They are both weakly increasing. (2) They are discontinuous at the same set of points. Moreover, if and are discontinuous at , then (3) If and are both differentiable at , then . Is it true that for any and where and are continuous, ?","f: [0,1]\to [0,1] g: [0,1]\to [0,1] f g x \lim_{z\rightarrow x^{+}}f(z)-\lim_{z\rightarrow x^{-}}f(z)=\lim_{z\rightarrow x^{+}}g(z)-\lim_{z\rightarrow x^{-}}g(z) f g x f'(x)=g'(x) x z f g f(x)-g(x)=f(z)-g(z)","['real-analysis', 'ordinary-differential-equations', 'measure-theory']"
57,"Uniqueness of Solutions to First-Order, Linear, Homogeneous, Boundary-Value PDE","Uniqueness of Solutions to First-Order, Linear, Homogeneous, Boundary-Value PDE",,"Consider a homogeneous, linear, first-order PDE $$L u \equiv \left( \sum_{i = 1}^d f^i(x) \frac{\partial}{\partial x^i} + c(x) \right) u(x) = 0$$ on some compact domain $\Omega \subset \mathbb{R}^d$ .  Obviously this system always has $u = 0$ as a solution; my question is what sorts of conditions on the coefficients $f^i(x)$ and $c(x)$ are sufficient in order to guarantee that the zero solution is unique subject to the boundary condition $u|_{\partial \Omega} = 0$ . I know that well-posedness of first-order PDEs is usually studied via the method of characteristics, but as I understand that's typically useful in thinking of the PDE as an initial value problem in which boundary conditions are specified on an initial-value surface and evolved from there.  Because here I'm treating the system as a Dirichlet problem, the inhomogenous problem $Lu = g$ , $u|_{\partial \Omega} = h$ may not in general be well-posed; but that's OK because I just care about uniqueness of the zero solution to the homogeneous problem. I have one partial result from Oleinik and Radkevic ( https://www.springer.com/gp/book/9781468489675 ), which consider second-order linear PDEs with nonnegative characteristic form, of which the equation I gave above is a special case (since its characteristic form is identically zero).  Then from e.g. Theorem 1.6.2 of this book I can conclude that the zero solution is unique if $c^* < 0$ in $\Omega \cup \partial \Omega$ , where $c^* \equiv c - \sum_{i = 1}^d \partial_i f^i$ is the zero-derivative term of the adjoint $L^*$ of $L$ .  But because the operator $L$ I care about is genuinely a first-order operator, while the condition $c^* < 0$ comes from considering second-order operators, I imagine there must be much more general sufficient conditions for the uniqueness of the zero solution than just $c^* < 0$ .","Consider a homogeneous, linear, first-order PDE on some compact domain .  Obviously this system always has as a solution; my question is what sorts of conditions on the coefficients and are sufficient in order to guarantee that the zero solution is unique subject to the boundary condition . I know that well-posedness of first-order PDEs is usually studied via the method of characteristics, but as I understand that's typically useful in thinking of the PDE as an initial value problem in which boundary conditions are specified on an initial-value surface and evolved from there.  Because here I'm treating the system as a Dirichlet problem, the inhomogenous problem , may not in general be well-posed; but that's OK because I just care about uniqueness of the zero solution to the homogeneous problem. I have one partial result from Oleinik and Radkevic ( https://www.springer.com/gp/book/9781468489675 ), which consider second-order linear PDEs with nonnegative characteristic form, of which the equation I gave above is a special case (since its characteristic form is identically zero).  Then from e.g. Theorem 1.6.2 of this book I can conclude that the zero solution is unique if in , where is the zero-derivative term of the adjoint of .  But because the operator I care about is genuinely a first-order operator, while the condition comes from considering second-order operators, I imagine there must be much more general sufficient conditions for the uniqueness of the zero solution than just .",L u \equiv \left( \sum_{i = 1}^d f^i(x) \frac{\partial}{\partial x^i} + c(x) \right) u(x) = 0 \Omega \subset \mathbb{R}^d u = 0 f^i(x) c(x) u|_{\partial \Omega} = 0 Lu = g u|_{\partial \Omega} = h c^* < 0 \Omega \cup \partial \Omega c^* \equiv c - \sum_{i = 1}^d \partial_i f^i L^* L L c^* < 0 c^* < 0,"['ordinary-differential-equations', 'partial-differential-equations', 'homogeneous-equation', 'linear-pde']"
58,Properties of $X′(t)=AX(t)$,Properties of,X′(t)=AX(t),"I am studying some properties of the initial value problem \begin{equation*}    \begin{cases} 	\frac{d}{dt}X(t)=A(t)X(t),\\ 		X(t_0)=I. 		\end{cases} \end{equation*} where $A(t)$ is a real-valued $n\times n$ matrix. If we denote as $X(t,t_0)$ the solution of this problems, we have the following properties: $X(t,r)=X(t,s)X(s,r)$ $X(s,t)=X(t,s)^{-1}$ I want to show this two properties and I know how to proof the first one using the second. Basicly, if you define $Y(t)=X(t,s)X(s,r)$ , you can do the following calculation: \begin{equation*}     \begin{split}         \frac{d}{dt}Y(t)&=\frac{d}{dt}X(t,s)X(s,r)         \\         &=A(t)X(t,s)X(s,r)         \\         &=A(t)Y(t).     \end{split} \end{equation*} It is also easy to see that $Y(r)=I$ using the second property and we have the first property. My problem is, how can I prove the second property?","I am studying some properties of the initial value problem where is a real-valued matrix. If we denote as the solution of this problems, we have the following properties: I want to show this two properties and I know how to proof the first one using the second. Basicly, if you define , you can do the following calculation: It is also easy to see that using the second property and we have the first property. My problem is, how can I prove the second property?","\begin{equation*}
   \begin{cases}
	\frac{d}{dt}X(t)=A(t)X(t),\\
		X(t_0)=I.
		\end{cases}
\end{equation*} A(t) n\times n X(t,t_0) X(t,r)=X(t,s)X(s,r) X(s,t)=X(t,s)^{-1} Y(t)=X(t,s)X(s,r) \begin{equation*}
    \begin{split}
        \frac{d}{dt}Y(t)&=\frac{d}{dt}X(t,s)X(s,r)
        \\
        &=A(t)X(t,s)X(s,r)
        \\
        &=A(t)Y(t).
    \end{split}
\end{equation*} Y(r)=I","['ordinary-differential-equations', 'matrix-equations']"
59,Absolutely continuous functions that fix zero and satisfies $f'(x)=2f(x)$,Absolutely continuous functions that fix zero and satisfies,f'(x)=2f(x),"A past question from a qualifying exam at my university reads: Let $f$ be a continuous real-valued function on the real line that is differentiable almost everywhere with respect to the Lebesgue measure and satisfies $f(0)=0$ and $$ f'(x)=2f(x)$$ almost everywhere. Prove that there exists infinitely many such functions, but that only one of them is absolutely continuous. I have tried modifying the function $e^{2x}$ , but I cannot satisfy all the conditions given. Once one shows that there are infinitely many such functions, then if we pick 2 such functions $f_1$ and $f_2$ and fix $a>0$ , we can apply the fundamental theorem of Calculus for Lebesgue Integrals on $[0,a]$ and see that if both are absolutely continuous, then $$ f_1(x)=\int_0^x 2f(t)dt=f_2(x) $$ So this would imply that the are the same function on $[0,\infty)$ . I'm not sure how to proceed with the whole real line.","A past question from a qualifying exam at my university reads: Let be a continuous real-valued function on the real line that is differentiable almost everywhere with respect to the Lebesgue measure and satisfies and almost everywhere. Prove that there exists infinitely many such functions, but that only one of them is absolutely continuous. I have tried modifying the function , but I cannot satisfy all the conditions given. Once one shows that there are infinitely many such functions, then if we pick 2 such functions and and fix , we can apply the fundamental theorem of Calculus for Lebesgue Integrals on and see that if both are absolutely continuous, then So this would imply that the are the same function on . I'm not sure how to proceed with the whole real line.","f f(0)=0  f'(x)=2f(x) e^{2x} f_1 f_2 a>0 [0,a]  f_1(x)=\int_0^x 2f(t)dt=f_2(x)  [0,\infty)","['ordinary-differential-equations', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'absolute-continuity']"
60,Prove that $xg(x)<\int_{0}^xg(x)dx$,Prove that,xg(x)<\int_{0}^xg(x)dx,Let $f(x)$ satisfy the differential equation $$\frac{d(f(x))}{dx}+f(x)=g(x)$$ where $f(x)$ and $g(x)$ are continuous functions. Also it is known that $f(x)$ is a decreasing function of $x$ for all positive x. Prove that $$xg(x)<\int_{0}^xg(x)dx$$ My Attempt: Let $H(x)=xg(x)-\int_{0}^xg(x)dx$ $H'(x)=xg'(x)$ What can we say about $g(x)$ and $g'(x)$ . I think some information seems to be missing. If it is given that $f(0)\geq 0$ something may be worked out.,Let satisfy the differential equation where and are continuous functions. Also it is known that is a decreasing function of for all positive x. Prove that My Attempt: Let What can we say about and . I think some information seems to be missing. If it is given that something may be worked out.,f(x) \frac{d(f(x))}{dx}+f(x)=g(x) f(x) g(x) f(x) x xg(x)<\int_{0}^xg(x)dx H(x)=xg(x)-\int_{0}^xg(x)dx H'(x)=xg'(x) g(x) g'(x) f(0)\geq 0,"['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations']"
61,How to solve $x^{\prime\prime\prime} + 2x^{\prime\prime} - x = e^{-t}\cos(2t)$ using the operator method?,How to solve  using the operator method?,x^{\prime\prime\prime} + 2x^{\prime\prime} - x = e^{-t}\cos(2t),"I did $(D^3 + 2D - 1)x = e^{-t}\cos(2t) \Rightarrow x = \frac{1}{D^3 + 2D - 1}e^{-t}\cos(2t) \Rightarrow x= \frac{e^{-t}}{(D-1)^3 + 2(D-1) - 1}\cos(2t) \Rightarrow$ $x =\frac{e^{-t}}{D^3-3D^2+3D-1+2D-2-1}\cos(2t) = \frac{e^{-t}}{D^3-3D^2+5D-4}\cos(2t)$ , but I'm having trouble with $D^3-3D^2+5D-4$ . Am I doing something wrong?","I did , but I'm having trouble with . Am I doing something wrong?",(D^3 + 2D - 1)x = e^{-t}\cos(2t) \Rightarrow x = \frac{1}{D^3 + 2D - 1}e^{-t}\cos(2t) \Rightarrow x= \frac{e^{-t}}{(D-1)^3 + 2(D-1) - 1}\cos(2t) \Rightarrow x =\frac{e^{-t}}{D^3-3D^2+3D-1+2D-2-1}\cos(2t) = \frac{e^{-t}}{D^3-3D^2+5D-4}\cos(2t) D^3-3D^2+5D-4,['ordinary-differential-equations']
62,Initial Value Problems defined on some interval,Initial Value Problems defined on some interval,,"Let $a$ be a positive real number. Find all of solutions, defined on (−a, a), for the following initial-value problem: $|y'(x)| + y(x) = 0$ , and $y(0) = -1$ . My progress: I struggle to understand the correlation between given $a$ and solution of the DE. Meanwhile, is it allowed in such problems to divide into 2 cases and solve them separately? Any help would be strongly welcomed!","Let be a positive real number. Find all of solutions, defined on (−a, a), for the following initial-value problem: , and . My progress: I struggle to understand the correlation between given and solution of the DE. Meanwhile, is it allowed in such problems to divide into 2 cases and solve them separately? Any help would be strongly welcomed!",a |y'(x)| + y(x) = 0 y(0) = -1 a,['ordinary-differential-equations']
63,Solving $(4-x^2)y''+2y=0$ by series,Solving  by series,(4-x^2)y''+2y=0,"I know that there are other ways to solve this, but I want to solve the ODE $(4-x^2)y''+2y=0$ by series close to $0$ , that is, I'm looking for solutions of the form $y(x)=\sum_{n=0}^{\infty}a_{n}x^{n}.$ Substituing in the ODE, I got: $$(8a_{2}+2a_{0})+(24a_{3}+2a_{1})x+\sum_{n=2}^{\infty}[4(n+2)(n+1)a_{n+2}+2a_{n}-n(n-1)a_{n}]x^{n}=0\implies \\ a_2=-\frac{a_0}{4};\quad a_3=-\frac{a_{1}}{12}; \quad a_{n+2}=\frac{(n-2)a_{n}}{4(n+2)}. $$ But this recorrence relation give me a problem: $a_4=0\implies a_{2n}=0$ for all $n>2 $ . My question is: Did I made a mistake? Or this solution considers, for even $n$ , only the terms $a_2$ and $a_0$ , that is, $y_{1}(x)=a_{0}\left(1-\frac{1}{4}x^{2}\right)$ ?","I know that there are other ways to solve this, but I want to solve the ODE by series close to , that is, I'm looking for solutions of the form Substituing in the ODE, I got: But this recorrence relation give me a problem: for all . My question is: Did I made a mistake? Or this solution considers, for even , only the terms and , that is, ?","(4-x^2)y''+2y=0 0 y(x)=\sum_{n=0}^{\infty}a_{n}x^{n}. (8a_{2}+2a_{0})+(24a_{3}+2a_{1})x+\sum_{n=2}^{\infty}[4(n+2)(n+1)a_{n+2}+2a_{n}-n(n-1)a_{n}]x^{n}=0\implies \\
a_2=-\frac{a_0}{4};\quad a_3=-\frac{a_{1}}{12}; \quad a_{n+2}=\frac{(n-2)a_{n}}{4(n+2)}.  a_4=0\implies a_{2n}=0 n>2  n a_2 a_0 y_{1}(x)=a_{0}\left(1-\frac{1}{4}x^{2}\right)","['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
64,Differential Equation $dy/dx = y^{1/3}$,Differential Equation,dy/dx = y^{1/3},What is the difference between the number of solutions of the system $$\frac{dy}{dx} = y^{1/3}$$ with initial value $y(0)=0$ in one case and $y(0)=1$ in another case? And why does this difference occur?  I know there's a theorem about uniqueness of solutions of first order ODE (although my understanding of that theorem isn't too good).,What is the difference between the number of solutions of the system with initial value in one case and in another case? And why does this difference occur?  I know there's a theorem about uniqueness of solutions of first order ODE (although my understanding of that theorem isn't too good).,\frac{dy}{dx} = y^{1/3} y(0)=0 y(0)=1,"['real-analysis', 'calculus', 'ordinary-differential-equations', 'real-numbers']"
65,A difficult game theory riddle,A difficult game theory riddle,,"Suppose a master hires $n$ servants who work for him. At the end of the day, the service come to him and request a wage for their service. They are able to request a wage up to $\$1$ , but no more. With $1/2$ probability, the master is in a good mood an will give all servants their requested wages. With $1/2$ probability, the master is in a bad mood and only pays the the servant with the lowest request; however, he pays him wage $g(x)$ where $g$ is a continuous, differentiable, monotonically increasing $g(0)=0$ and $x$ is the wage requested by the servant who requested the highest wage. If there is a tie in this state, then nobody is payed any wage. No servant knows if the master is in a good or bad mood and all share the half-half prior belief. Show that there is a symmetric equilibrium (Nash) where each servant makes a wage request according to the same distribution over some interval in $[0,1]$ . This means that anywhere in the support of this interval will give the same expected wage to the servant conditional on the strategy being used by the others and, of course, anywhere outside of the interval gives the same or lower expected wage. Show that this equilibrium is unique among symmetric equilibria. Progress: Let A be the good-mood state and B be the bad-mood state of the world. Note that servants will never want to tie and so a symmetric solution will never contain atoms. Let $F$ be the equilibrium distribution and consider the maximum of the support $\bar{w}$ . Any wage offer close to the maximum of the support becomes arbitrarily unlikely to get paid in state B (this follows from no atoms). In this case, if $\bar{w}< 1$ , the servant will eventually earn more in expectation by giving up on earning anything in state B and will be better of requesting $\$1$ . This implies that the maximum must be one. Because every request in the support must earn the same expected wage, and $\$1$ is in the support, the expected wage is $\$1/2$ . From this we can write an equation for the expected wage for any request in the interval: \begin{align} 1=\frac{1}{2}w+\frac{1}{2} \left(1-F(w) \right)^{n-1}   \int_w^1 g(z) f(z) (n-1)  \frac{(F(z)-F(w))^{n-2}}{(1-F(w))^{n-1}} \;dz\\ 1=\frac{1}{2}w+\frac{1}{2} (n-1)   \int_w^1 g(z) \left( F(z)-F(w)\right)^{n-2} \;dF(z) \end{align} Here, given a proposed wage $w$ and assuming all other $(n-1)$ servants use mixed strategy according to $F$ , $(1-F(w))^{n-1}$ is the probability that all other requested wages were higher than $w$ and that the payment will be made. Given that a request was higher than $w$ , it's distribution is then the truncated distribution $\frac{F(z)-F(w)}{1-F(w)}$ . The distribution of the maximum of this is then $\left(\frac{F(z)-F(w)}{1-F(w)}\right)^{n-1}$ . If a pdf exists, the maximum pdf is then $ f(z) (n-1)  \frac{(F(z)-F(w))^{n-2}}{(1-F(w))^{n-1}}$ which is what shows up in the first integral. The integral term is the expected wage given request $w$ . Seems obvious to me that the $F$ should exist and be unique, just seems difficult to show this for arbitrary $n$ . Maybe there should be some simple way to argue this?","Suppose a master hires servants who work for him. At the end of the day, the service come to him and request a wage for their service. They are able to request a wage up to , but no more. With probability, the master is in a good mood an will give all servants their requested wages. With probability, the master is in a bad mood and only pays the the servant with the lowest request; however, he pays him wage where is a continuous, differentiable, monotonically increasing and is the wage requested by the servant who requested the highest wage. If there is a tie in this state, then nobody is payed any wage. No servant knows if the master is in a good or bad mood and all share the half-half prior belief. Show that there is a symmetric equilibrium (Nash) where each servant makes a wage request according to the same distribution over some interval in . This means that anywhere in the support of this interval will give the same expected wage to the servant conditional on the strategy being used by the others and, of course, anywhere outside of the interval gives the same or lower expected wage. Show that this equilibrium is unique among symmetric equilibria. Progress: Let A be the good-mood state and B be the bad-mood state of the world. Note that servants will never want to tie and so a symmetric solution will never contain atoms. Let be the equilibrium distribution and consider the maximum of the support . Any wage offer close to the maximum of the support becomes arbitrarily unlikely to get paid in state B (this follows from no atoms). In this case, if , the servant will eventually earn more in expectation by giving up on earning anything in state B and will be better of requesting . This implies that the maximum must be one. Because every request in the support must earn the same expected wage, and is in the support, the expected wage is . From this we can write an equation for the expected wage for any request in the interval: Here, given a proposed wage and assuming all other servants use mixed strategy according to , is the probability that all other requested wages were higher than and that the payment will be made. Given that a request was higher than , it's distribution is then the truncated distribution . The distribution of the maximum of this is then . If a pdf exists, the maximum pdf is then which is what shows up in the first integral. The integral term is the expected wage given request . Seems obvious to me that the should exist and be unique, just seems difficult to show this for arbitrary . Maybe there should be some simple way to argue this?","n \1 1/2 1/2 g(x) g g(0)=0 x [0,1] F \bar{w} \bar{w}< 1 \1 \1 \1/2 \begin{align}
1=\frac{1}{2}w+\frac{1}{2} \left(1-F(w) \right)^{n-1}   \int_w^1 g(z) f(z) (n-1)  \frac{(F(z)-F(w))^{n-2}}{(1-F(w))^{n-1}} \;dz\\
1=\frac{1}{2}w+\frac{1}{2} (n-1)   \int_w^1 g(z) \left( F(z)-F(w)\right)^{n-2} \;dF(z)
\end{align} w (n-1) F (1-F(w))^{n-1} w w \frac{F(z)-F(w)}{1-F(w)} \left(\frac{F(z)-F(w)}{1-F(w)}\right)^{n-1}  f(z) (n-1)  \frac{(F(z)-F(w))^{n-2}}{(1-F(w))^{n-1}} w F n","['ordinary-differential-equations', 'game-theory', 'nash-equilibrium', 'integro-differential-equations']"
66,Determining Weight function in Sturm Liouville problem,Determining Weight function in Sturm Liouville problem,,"By choosing the proper weight function $\sigma (x) $ solve the Sturm-Liouville problem and determine its eigenvalues and eigenfunctions. $$ \frac{d}{dx}\left[x\frac{dy(x)}{dx}\right] + \frac{2}{x}y(x) +\lambda \sigma (x)y(x)=0,\; y'(1)=y'(2)=0,\; 1 \leq x \leq 2. $$ I don't understand what it means to ""choose"" the proper weight function. I tried to rewrite the problem in this form. $$\frac{1}{\sigma(x)}\left[\frac{d}{dx}\left[x\frac{dy(x)}{dx} + \frac{2}{x}y(x)\right] +\lambda\sigma(x)=0\right], $$ then calculate it by setting $p(x)=A(x)\sigma (x), p'(x)=B(x)\sigma(x)$ and using this formula: $$\sigma(x)=e^{\int \frac{A-B'}{B}\,dX}, $$ but it doesn't get me anywhere; solving this gives you just $1=1.$ I tried extracting information about the weight function from the boundary condition but i am failing at that too and i tried solving the differential equation using an infinite series but that won't work either because of the unknown weight function. Any tips?","By choosing the proper weight function solve the Sturm-Liouville problem and determine its eigenvalues and eigenfunctions. I don't understand what it means to ""choose"" the proper weight function. I tried to rewrite the problem in this form. then calculate it by setting and using this formula: but it doesn't get me anywhere; solving this gives you just I tried extracting information about the weight function from the boundary condition but i am failing at that too and i tried solving the differential equation using an infinite series but that won't work either because of the unknown weight function. Any tips?","\sigma (x)   \frac{d}{dx}\left[x\frac{dy(x)}{dx}\right] + \frac{2}{x}y(x) +\lambda \sigma (x)y(x)=0,\; y'(1)=y'(2)=0,\; 1 \leq x \leq 2.  \frac{1}{\sigma(x)}\left[\frac{d}{dx}\left[x\frac{dy(x)}{dx} + \frac{2}{x}y(x)\right] +\lambda\sigma(x)=0\right],  p(x)=A(x)\sigma (x), p'(x)=B(x)\sigma(x) \sigma(x)=e^{\int \frac{A-B'}{B}\,dX},  1=1.","['calculus', 'ordinary-differential-equations', 'eigenfunctions', 'sturm-liouville']"
67,Behavior of a function near a singular point,Behavior of a function near a singular point,,"What is the correct wording concerning the behavior of a function near a singular point? For example, the function $$f(x)=\frac{e^x+2}{x^2}$$ behaves like $2/x^2$ as $x$ approaches zero. Often, one is only interested at how fast it ""explodes"". So is it acceptable to say that "" $f$ behaves like $1/x^2$ as $x$ approaches zero""? Or perhaps  "" $f \sim 1/x^2$ as $x\rightarrow 0$ ""? I would like to ""ditch"" the constant because, in the manuscript I am writing, it makes the text unnecessarily complicated and the constant of proportionality has no importance. At least in my case.","What is the correct wording concerning the behavior of a function near a singular point? For example, the function behaves like as approaches zero. Often, one is only interested at how fast it ""explodes"". So is it acceptable to say that "" behaves like as approaches zero""? Or perhaps  "" as ""? I would like to ""ditch"" the constant because, in the manuscript I am writing, it makes the text unnecessarily complicated and the constant of proportionality has no importance. At least in my case.",f(x)=\frac{e^x+2}{x^2} 2/x^2 x f 1/x^2 x f \sim 1/x^2 x\rightarrow 0,"['real-analysis', 'ordinary-differential-equations', 'asymptotics', 'singularity', 'singular-solution']"
68,Prove that a flow is volume-preserving if and only if the trace of corresponding ODE matrix is $0$,Prove that a flow is volume-preserving if and only if the trace of corresponding ODE matrix is,0,"I am working on the problem stated as below: Consider the linear equation $\dot{x}=Ax$ , where $x\in\mathbb{R}^{n}$ and $A$ is an $n\times n$ matrix. Let $\varphi_{t}$ be the flow generated. Verify that $$\varphi_{t}(x)=e^{At}x,\ \text{and}\ \det(\varphi_{t})=\det(e^{At})=e^{(Tr A)t},$$ so that $\varphi_{t}$ is volume-preserving if and only if $Tr(A)=0$ . I have done all the verifications. I will post my verifications after I explain my confusions. When we define a flow, we always use a vector field, say $X=(X_{1},\cdots, X_{n})$ on $\mathbb{R}^{n}$ , and we say $\varphi_{t}$ is a flow generated by this vector field, i.e. $\varphi_{t}:X\rightarrow X$ a map and $\{\varphi_{t}\}$ satisfies $$\varphi_{0}=id\ \text{and}\ \varphi_{s+t}=\varphi_{t}\circ\varphi_{s},\ \text{for all}\ s,t\in\mathbb{R}.$$ Then, we define the divergence of $X$ by $$div(X)=\sum_{i=1}^{n}\dfrac{\partial X_{i}}{\partial x_{i}}=Tr(DX),$$ and accordingly we have the following proposition Proposition: $\varphi_{t}$ is volume-preserving if and only if $div(X)=0$ everywhere on $\mathbb{R}^{n}.$ However, when I work on problems, they always use a system of ODE, instead of vector fields. How could I connect the flow generated by linear equation $\dot{x}=Ax$ to the flow generated by a vector field? If I can understand this part, I believe I can solve this problem, since I believe $Tr(A)$ must be somehow connected to $Tr(DX)$ . Thank you! Below is my verifications: To verify, firstly we have $$\dfrac{d}{dt}\varphi_{t}(x)=\dfrac{d}{dt}e^{At}x=Ae^{At}x=A\varphi_{t}(x).$$ Then, $\det(\varphi_{t})=\det(e^{At})$ immediately, and thus it remains to show $\det(e^{At})=e^{(TrA)t}$ . Firstly, we need to note that for any $n\times n$ matrix $A$ and for any invertible $n\times n$ matrix $P$ , we can write $$A^{n}=(P^{-1}AP)(P^{-1}AP)\cdots (P^{-1}AP)=P^{-1}A^{n}P,\ \text{for all}\ n, $$ such that \begin{align*} \exp(A)&=I+A+\dfrac{1}{2!}A^{2}+\dfrac{1}{3!}A^{3}+\cdots,\\ &=P^{-1}(I+A+\dfrac{1}{2!}A^{2}+\dfrac{1}{3!}A^{3}+\cdots,)P\\ &=P^{-1}\exp(A)P. \end{align*} If $A$ is diagonalizable, then $A$ can be written as $$A=P^{-1}JP,$$ where $P$ is invertible and $J$ is a diagonal matrix, such that we have \begin{align*} \det(e^{A})&=\det(e^{P^{-1}JP})=\det(P^{-1}e^{J}P)\\ &=\det(P^{-1})\det(e^{J})\det(P)=\det(e^{J})\\ &=\pi_{i=1}^{n}e^{j_{ii}}=e^{\sum_{i=1}^{n}j_{ii}}\\ &=e^{tr(J)}=e^{tr(A)}, \ \text{since trace equals the sum of eigenvalues}.  \end{align*} If $A$ is nilpotent, then since every nilpotent matrix is similar to a upper triangular matrix $D$ with $0$ s on the diagonal, we have $$A=P^{-1}DP,$$ and then by the same calculation above we have $$\det(e^{A})=e^{tr(D)}=e^{tr(A)}.$$ Finally, if $A$ is a general $n\times n$ matrix, then since any matrix $A$ can be written as $$A=D+N,$$ where $D$ a diagonal matrix and $N$ a nilpotent matrix, and since such $D$ and such $N$ commute (so that we can write $e^{A}=e^{D}e^{N}$ ), we have $$\det(e^{A})=\det(e^{D}e^{N})=\det(e^{D})\det(e^{N})=e^{Tr(D)}e^{Tr(N)}=e^{Tr(D)+Tr(N)}=e^{Tr(A)}.$$ Therefore, for any $n\times n$ matrix, we have $$\det(e^{A})=e^{Tr(A)},$$ and thus we have $$\det(e^{At})=e^{Tr(At)}=e^{(TrA)t}.$$","I am working on the problem stated as below: Consider the linear equation , where and is an matrix. Let be the flow generated. Verify that so that is volume-preserving if and only if . I have done all the verifications. I will post my verifications after I explain my confusions. When we define a flow, we always use a vector field, say on , and we say is a flow generated by this vector field, i.e. a map and satisfies Then, we define the divergence of by and accordingly we have the following proposition Proposition: is volume-preserving if and only if everywhere on However, when I work on problems, they always use a system of ODE, instead of vector fields. How could I connect the flow generated by linear equation to the flow generated by a vector field? If I can understand this part, I believe I can solve this problem, since I believe must be somehow connected to . Thank you! Below is my verifications: To verify, firstly we have Then, immediately, and thus it remains to show . Firstly, we need to note that for any matrix and for any invertible matrix , we can write such that If is diagonalizable, then can be written as where is invertible and is a diagonal matrix, such that we have If is nilpotent, then since every nilpotent matrix is similar to a upper triangular matrix with s on the diagonal, we have and then by the same calculation above we have Finally, if is a general matrix, then since any matrix can be written as where a diagonal matrix and a nilpotent matrix, and since such and such commute (so that we can write ), we have Therefore, for any matrix, we have and thus we have","\dot{x}=Ax x\in\mathbb{R}^{n} A n\times n \varphi_{t} \varphi_{t}(x)=e^{At}x,\ \text{and}\ \det(\varphi_{t})=\det(e^{At})=e^{(Tr A)t}, \varphi_{t} Tr(A)=0 X=(X_{1},\cdots, X_{n}) \mathbb{R}^{n} \varphi_{t} \varphi_{t}:X\rightarrow X \{\varphi_{t}\} \varphi_{0}=id\ \text{and}\ \varphi_{s+t}=\varphi_{t}\circ\varphi_{s},\ \text{for all}\ s,t\in\mathbb{R}. X div(X)=\sum_{i=1}^{n}\dfrac{\partial X_{i}}{\partial x_{i}}=Tr(DX), \varphi_{t} div(X)=0 \mathbb{R}^{n}. \dot{x}=Ax Tr(A) Tr(DX) \dfrac{d}{dt}\varphi_{t}(x)=\dfrac{d}{dt}e^{At}x=Ae^{At}x=A\varphi_{t}(x). \det(\varphi_{t})=\det(e^{At}) \det(e^{At})=e^{(TrA)t} n\times n A n\times n P A^{n}=(P^{-1}AP)(P^{-1}AP)\cdots (P^{-1}AP)=P^{-1}A^{n}P,\ \text{for all}\ n,  \begin{align*}
\exp(A)&=I+A+\dfrac{1}{2!}A^{2}+\dfrac{1}{3!}A^{3}+\cdots,\\
&=P^{-1}(I+A+\dfrac{1}{2!}A^{2}+\dfrac{1}{3!}A^{3}+\cdots,)P\\
&=P^{-1}\exp(A)P.
\end{align*} A A A=P^{-1}JP, P J \begin{align*}
\det(e^{A})&=\det(e^{P^{-1}JP})=\det(P^{-1}e^{J}P)\\
&=\det(P^{-1})\det(e^{J})\det(P)=\det(e^{J})\\
&=\pi_{i=1}^{n}e^{j_{ii}}=e^{\sum_{i=1}^{n}j_{ii}}\\
&=e^{tr(J)}=e^{tr(A)}, \ \text{since trace equals the sum of eigenvalues}. 
\end{align*} A D 0 A=P^{-1}DP, \det(e^{A})=e^{tr(D)}=e^{tr(A)}. A n\times n A A=D+N, D N D N e^{A}=e^{D}e^{N} \det(e^{A})=\det(e^{D}e^{N})=\det(e^{D})\det(e^{N})=e^{Tr(D)}e^{Tr(N)}=e^{Tr(D)+Tr(N)}=e^{Tr(A)}. n\times n \det(e^{A})=e^{Tr(A)}, \det(e^{At})=e^{Tr(At)}=e^{(TrA)t}.","['ordinary-differential-equations', 'dynamical-systems', 'vector-fields', 'divergence-operator']"
69,How to solve a first order differential equation with parameters inside functions: $ag'(cy)+bg'(ey)=\alpha$,How to solve a first order differential equation with parameters inside functions:,ag'(cy)+bg'(ey)=\alpha,"I am trying to solve a first order differential equation with the condition that $g(y)=0$ iff $y=0$ : \begin{align*}     &ag'(cy)+bg'(ey)=\alpha\\     &g(0)=0, \tag{1} \end{align*} where parameters $a,b,c,e$ are real nonzero constants; $\alpha$ is a complex constant; function $g(y):\mathbb{R}\to \mathbb{C}$ is a function mapping from real number $y$ to a complex number. The goal is to solve for function $g(\cdot)$ . This is what I have done. Solve this differential equation by integrating with respect to $y$ : \begin{align*}     \frac{a}{c}g(cy)+\frac{b}{e}g(ey)=\alpha y+\beta, \end{align*} where $\beta$ is another complex constant. Plugging in $y=0$ and using the fact that $g(0)=0$ , we have $\beta=0$ . Therefore, we have \begin{align*}     \frac{a}{c}g(cy)+\frac{b}{e}g(ey)=\alpha y. \end{align*} The background of this problem is Cauchy functional equation, so my conjecture is one solution could be $g(y)=\gamma y$ . Plugging in $g(y)=\gamma y$ , I get $\gamma=\frac{\alpha}{a+b}$ , which implies that one solution is $g(y)=\frac{\alpha}{a+b} y$ . Then, I move on to show uniqueness. I define a vector-valued function $h\equiv (h_1,h_2)^T$ such that \begin{align*}     h_1(y)&=\frac{a}{c}g_1(cy)+\frac{b}{e}g_1(ey)\\     h_2(y)&=\frac{a}{c}g_2(cy)+\frac{b}{e}g_2(ey), \end{align*} where $g(y)\equiv g_1(y)+ig_2(y)$ . Then, I rewrite this differential equation as \begin{align*}     &h'(y)=\alpha\\     &h(0)=0, \tag{2} \end{align*} where $\alpha\equiv (\alpha_1,i\alpha_2)^T$ . By the uniqueness theorem of first order differential equation, solution $h(y)$ is unique. I have two questions. First, I think equation (1) and (2) should be equivalent. However, it seems that equation (1) can imply equation (2) but equation (2) may not imply equation (1). This is because $h(0)=0$ may imply either $g_1(0)=0,g_2(0)=0$ or $\frac{a}{c}+\frac{b}{e}=0$ . Second, I have only proved that $h(y)$ is unique. How should I proceed to show $g(y)$ is also unique.","I am trying to solve a first order differential equation with the condition that iff : where parameters are real nonzero constants; is a complex constant; function is a function mapping from real number to a complex number. The goal is to solve for function . This is what I have done. Solve this differential equation by integrating with respect to : where is another complex constant. Plugging in and using the fact that , we have . Therefore, we have The background of this problem is Cauchy functional equation, so my conjecture is one solution could be . Plugging in , I get , which implies that one solution is . Then, I move on to show uniqueness. I define a vector-valued function such that where . Then, I rewrite this differential equation as where . By the uniqueness theorem of first order differential equation, solution is unique. I have two questions. First, I think equation (1) and (2) should be equivalent. However, it seems that equation (1) can imply equation (2) but equation (2) may not imply equation (1). This is because may imply either or . Second, I have only proved that is unique. How should I proceed to show is also unique.","g(y)=0 y=0 \begin{align*}
    &ag'(cy)+bg'(ey)=\alpha\\
    &g(0)=0, \tag{1}
\end{align*} a,b,c,e \alpha g(y):\mathbb{R}\to \mathbb{C} y g(\cdot) y \begin{align*}
    \frac{a}{c}g(cy)+\frac{b}{e}g(ey)=\alpha y+\beta,
\end{align*} \beta y=0 g(0)=0 \beta=0 \begin{align*}
    \frac{a}{c}g(cy)+\frac{b}{e}g(ey)=\alpha y.
\end{align*} g(y)=\gamma y g(y)=\gamma y \gamma=\frac{\alpha}{a+b} g(y)=\frac{\alpha}{a+b} y h\equiv (h_1,h_2)^T \begin{align*}
    h_1(y)&=\frac{a}{c}g_1(cy)+\frac{b}{e}g_1(ey)\\
    h_2(y)&=\frac{a}{c}g_2(cy)+\frac{b}{e}g_2(ey),
\end{align*} g(y)\equiv g_1(y)+ig_2(y) \begin{align*}
    &h'(y)=\alpha\\
    &h(0)=0, \tag{2}
\end{align*} \alpha\equiv (\alpha_1,i\alpha_2)^T h(y) h(0)=0 g_1(0)=0,g_2(0)=0 \frac{a}{c}+\frac{b}{e}=0 h(y) g(y)","['ordinary-differential-equations', 'functional-equations']"
70,Can the equation $f''(x)+cf'(x)+g(x)f(x)=0$ be solved for $f(x)$?,Can the equation  be solved for ?,f''(x)+cf'(x)+g(x)f(x)=0 f(x),"If we have a differential equation $$f''(x)+cf'(x)+g(x)f(x)=0,$$ where $c$ is some fixed constant, can we solve for $f(x)$ given an input $g(x)$ , and how? Is it possible to find a formula for $f(x)$ depending on $g(x)$ ? Whether that is an integral expression, Fourier series, or otherwise? It is easy enough to solve when $g(x)$ is a constant, for example, and this is a basic equation for a damped oscillator, but I am wondering about how to go about solving if we let $g(x)$ be some more general given function.","If we have a differential equation where is some fixed constant, can we solve for given an input , and how? Is it possible to find a formula for depending on ? Whether that is an integral expression, Fourier series, or otherwise? It is easy enough to solve when is a constant, for example, and this is a basic equation for a damped oscillator, but I am wondering about how to go about solving if we let be some more general given function.","f''(x)+cf'(x)+g(x)f(x)=0, c f(x) g(x) f(x) g(x) g(x) g(x)","['ordinary-differential-equations', 'analysis']"
71,Question Regarding Floquet Theory and Characteristic Exponents,Question Regarding Floquet Theory and Characteristic Exponents,,"My classmates and I are trying to figure out the following question: given the system $y'\left(t\right) = A\left(t\right)y\left(t\right)$ , where $A\left(t\right) =    \left[ {\begin{array}{cc}    0 & 1 \\    \cos \left(t\right) & \sin \left(t\right) \\   \end{array} } \right] $ , find its characteristic exponents and determin e whether it has a periodic solution of period $2π$ . So far, we have written it as $\left[ {\begin{array}{cc}    x \\    y \\   \end{array} } \right]' = \left[ {\begin{array}{cc}    0 & 1 \\    \cos \left(t\right) & \sin \left(t\right) \\   \end{array} } \right]\left[ {\begin{array}{cc}    x \\    y \\   \end{array} } \right]$ , meaning $x' = y$ and $y' = x\cos \left(t\right) + y\sin \left(t\right)$ . We can then write this as $x'' = x\cos \left(t\right) + x'\sin \left(t\right)$ , so $x'' - x'\sin \left(t\right) - x\cos \left(t\right) = 0$ . In another post, someone gave us a hint to solving this differential equation, and we came up with $x = ce^{-\cos \left(t\right)}$ . However, from here we are lost; we understand the question involves u\sin g Floquet Theory, but it was not covered in great detail in class and the online resources we have found are difficult for us to understand because while the theory is described in a lot of higher-level detail, there aren't many worked through examples. Would someone be able to briefly explain Floquet Theory to us and how this question is relevant without providing a full answer so that we have a step in the right direction? Thank you very much in advance.","My classmates and I are trying to figure out the following question: given the system , where , find its characteristic exponents and determin e whether it has a periodic solution of period . So far, we have written it as , meaning and . We can then write this as , so . In another post, someone gave us a hint to solving this differential equation, and we came up with . However, from here we are lost; we understand the question involves u\sin g Floquet Theory, but it was not covered in great detail in class and the online resources we have found are difficult for us to understand because while the theory is described in a lot of higher-level detail, there aren't many worked through examples. Would someone be able to briefly explain Floquet Theory to us and how this question is relevant without providing a full answer so that we have a step in the right direction? Thank you very much in advance.","y'\left(t\right) = A\left(t\right)y\left(t\right) A\left(t\right) = 
  \left[ {\begin{array}{cc}
   0 & 1 \\
   \cos \left(t\right) & \sin \left(t\right) \\
  \end{array} } \right]
 2π \left[ {\begin{array}{cc}
   x \\
   y \\
  \end{array} } \right]' = \left[ {\begin{array}{cc}
   0 & 1 \\
   \cos \left(t\right) & \sin \left(t\right) \\
  \end{array} } \right]\left[ {\begin{array}{cc}
   x \\
   y \\
  \end{array} } \right] x' = y y' = x\cos \left(t\right) + y\sin \left(t\right) x'' = x\cos \left(t\right) + x'\sin \left(t\right) x'' - x'\sin \left(t\right) - x\cos \left(t\right) = 0 x = ce^{-\cos \left(t\right)}",['ordinary-differential-equations']
72,When can I treat infinitesimals as numbers? [duplicate],When can I treat infinitesimals as numbers? [duplicate],,"This question already has answers here : When not to treat dy/dx as a fraction in single-variable calculus? (8 answers) Closed 5 years ago . Even if many mathematicians don't like the notation, I have found in many rigorous math books things like $\frac{dy}{dx}=Ay$ so $\frac{dy}{y}=Adx$ What I don't understand is this: under which conditions is ok to treat infinitesimal as numbers ? (multiplying for example both sides of an equation by dx). Edit: Not only, what allows me to integrate both sides of the last equation ???","This question already has answers here : When not to treat dy/dx as a fraction in single-variable calculus? (8 answers) Closed 5 years ago . Even if many mathematicians don't like the notation, I have found in many rigorous math books things like so What I don't understand is this: under which conditions is ok to treat infinitesimal as numbers ? (multiplying for example both sides of an equation by dx). Edit: Not only, what allows me to integrate both sides of the last equation ???",\frac{dy}{dx}=Ay \frac{dy}{y}=Adx,"['calculus', 'ordinary-differential-equations']"
73,Computing a derivative through Lie series,Computing a derivative through Lie series,,"Consider the $N$ -dimensional autonomous system of ODEs $$\dot{x}= f(x),$$ where a locally unique solution $x(t)$ , starting from the initial condition $x$ , is denoted as $x(t)=\phi(t,x)$ . Assume that $$\Big(\frac{\partial}{\partial{x}}\phi(t,x)\Big)f(x)=f(\phi(t,x))$$ For the system above, assume that $f(x)$ is analytic (that is, its Taylor series converges to $f$ itself). Let the differential operator $L[\xi]$ be defined as $$L[\xi]=f(x)\boldsymbol{\cdot}\nabla{\xi}=\sum_{n=1}^{N}f_i(x)\frac{\partial{\xi}}{\partial{x_i}}$$ Show that $\phi(t,x)$ can be expressed as $$\phi(t,x)=\sum_{n=0}^{\infty}\frac{t^n}{n!}L^n[x]$$ where $L^n[\xi]$ is the shorthand notation for $$L^n[\xi]=\underbrace{L[L[\cdots{L}[\xi]}_{n\text{-times}}\cdots]]$$ Potentially related questions: How to properly apply the Lie Series Exponential of a function times derivative How to derive these Lie Series formulas I'm stuck on how to approach this problem. Here is all the information that I have gathered so far - Through this question, the one dimensional situation states that $e^{a\partial}f(x)=f(a+x)$ (we can think of this as a shift operator). Inside Ordinary Differential Equations and Dynamical Systems by Teschl, we have the following Lemma (Lemma $6.2$ on page $190$ of the text). Lemma (Straightening out of vector fields): Suppose $f(x_0)\neq0$ . Then, there is a local coordinate transform $y=\varphi(x)$ such that $\dot{x}=f(x)$ is transformed to $$\dot{y}=(1,0,...,0)$$ Teschl list a similar problem on page $191$ (problem $6.5$ for one-parameter lie groups) in which he states that Hint: The Taylor coefficients are the derivatives which can be obtained by differentiating the differential equation. So, I think that I need to apply what was done in this question alongside Lemma 6.2. I will have to consider what a vector field means in this context. I might be able to make the assumption that a vector field is just a linear operator. We are given that $\dot{x}= f(x)$ is an autonomous system of ODEs $x(t)=\phi(t,x)$ $\Big(\frac{\partial}{\partial{x}}\phi(t,x)\Big)f(x)=f(\phi(t,x))$ $L[\xi]=f(x)\boldsymbol{\cdot}\nabla{\xi}=\sum_{n=1}^{N}f_i(x)\frac{\partial{\xi}}{\partial{x_i}}$ and we need to show that $$\phi(t,x)=\sum_{n=0}^{\infty}\frac{t^n}{n!}L^n[x]$$ I also see that Roger Howe wrote a good introduction to lie theory in these notes (he goes through one-parameter lie groups on pages $604-606$ ). This appears to be an extremely difficult problem for someone unfamiliar with lie theory. I am going to see if I can figure out a more direct approach.","Consider the -dimensional autonomous system of ODEs where a locally unique solution , starting from the initial condition , is denoted as . Assume that For the system above, assume that is analytic (that is, its Taylor series converges to itself). Let the differential operator be defined as Show that can be expressed as where is the shorthand notation for Potentially related questions: How to properly apply the Lie Series Exponential of a function times derivative How to derive these Lie Series formulas I'm stuck on how to approach this problem. Here is all the information that I have gathered so far - Through this question, the one dimensional situation states that (we can think of this as a shift operator). Inside Ordinary Differential Equations and Dynamical Systems by Teschl, we have the following Lemma (Lemma on page of the text). Lemma (Straightening out of vector fields): Suppose . Then, there is a local coordinate transform such that is transformed to Teschl list a similar problem on page (problem for one-parameter lie groups) in which he states that Hint: The Taylor coefficients are the derivatives which can be obtained by differentiating the differential equation. So, I think that I need to apply what was done in this question alongside Lemma 6.2. I will have to consider what a vector field means in this context. I might be able to make the assumption that a vector field is just a linear operator. We are given that is an autonomous system of ODEs and we need to show that I also see that Roger Howe wrote a good introduction to lie theory in these notes (he goes through one-parameter lie groups on pages ). This appears to be an extremely difficult problem for someone unfamiliar with lie theory. I am going to see if I can figure out a more direct approach.","N \dot{x}= f(x), x(t) x x(t)=\phi(t,x) \Big(\frac{\partial}{\partial{x}}\phi(t,x)\Big)f(x)=f(\phi(t,x)) f(x) f L[\xi] L[\xi]=f(x)\boldsymbol{\cdot}\nabla{\xi}=\sum_{n=1}^{N}f_i(x)\frac{\partial{\xi}}{\partial{x_i}} \phi(t,x) \phi(t,x)=\sum_{n=0}^{\infty}\frac{t^n}{n!}L^n[x] L^n[\xi] L^n[\xi]=\underbrace{L[L[\cdots{L}[\xi]}_{n\text{-times}}\cdots]] e^{a\partial}f(x)=f(a+x) 6.2 190 f(x_0)\neq0 y=\varphi(x) \dot{x}=f(x) \dot{y}=(1,0,...,0) 191 6.5 \dot{x}= f(x) x(t)=\phi(t,x) \Big(\frac{\partial}{\partial{x}}\phi(t,x)\Big)f(x)=f(\phi(t,x)) L[\xi]=f(x)\boldsymbol{\cdot}\nabla{\xi}=\sum_{n=1}^{N}f_i(x)\frac{\partial{\xi}}{\partial{x_i}} \phi(t,x)=\sum_{n=0}^{\infty}\frac{t^n}{n!}L^n[x] 604-606","['ordinary-differential-equations', 'lie-groups', 'differential-operators']"
74,Find all the continuous functions that satisfy $[f(t)]^2=F(t)-F(0)$,Find all the continuous functions that satisfy,[f(t)]^2=F(t)-F(0),"I need to find all the continuous functions $f$ such that $$ [f(t)]^2=\int_0^t f(s) ds $$ Attempt: Since $$  f(t)\leq [f(t)]^2+c, \, \, \forall c \geq \frac14$$ we have $$ f(t)\leq c+ \int_0^t f(s)ds $$ Now, Gronwall's Lemma gives $$ f(t) \leq c+ \int_0^t c \,e^{t-\tau}d\tau \iff$$ $$ f(t) \leq c \, e^t, \, \,\forall c\geq \frac14$$ Given that these steps were correct, is this inequality the final answer?","I need to find all the continuous functions such that Attempt: Since we have Now, Gronwall's Lemma gives Given that these steps were correct, is this inequality the final answer?","f 
[f(t)]^2=\int_0^t f(s) ds
  
f(t)\leq [f(t)]^2+c, \, \, \forall c \geq \frac14 
f(t)\leq c+ \int_0^t f(s)ds  
f(t) \leq c+ \int_0^t c \,e^{t-\tau}d\tau \iff  f(t) \leq c \, e^t, \, \,\forall c\geq \frac14","['ordinary-differential-equations', 'inequality', 'recurrence-relations']"
75,Differential invariants of ODEs,Differential invariants of ODEs,,"I have been reading 'Symmetry and Integration Methods for Differential Equations' by Bluman and Anco. I'm trying to make sense of differential invariants and ODEs... It is confusing. There is an ODE in a general form, for example $$F(x,y,y',y'',...,y^{(n)})=0$$ where $x$ is the independent variable, and $y$ is the dependent variable. Say, the ODE is invariant under the one-parameter Lie group $X_1$ . Say we know the prolongation $$X_1^{(n)}$$ and the ODE admits the group, i.e. is invariant w/respect to $X_1^{(n)}$ If I do $$X_1^{(n)}I=0$$ then $I$ is a ""differential invariant"" of the Lie group, right? What does it mean that $I$ is a ""differential invariant""? Does it mean it doesn't change under the transformation $X_1^{(n)}$ ? Or does it mean $I$ doesn't change under the transformations $X_1$ (without the prolongation)? It seems confusing... What are the properties of $I$ ?","I have been reading 'Symmetry and Integration Methods for Differential Equations' by Bluman and Anco. I'm trying to make sense of differential invariants and ODEs... It is confusing. There is an ODE in a general form, for example where is the independent variable, and is the dependent variable. Say, the ODE is invariant under the one-parameter Lie group . Say we know the prolongation and the ODE admits the group, i.e. is invariant w/respect to If I do then is a ""differential invariant"" of the Lie group, right? What does it mean that is a ""differential invariant""? Does it mean it doesn't change under the transformation ? Or does it mean doesn't change under the transformations (without the prolongation)? It seems confusing... What are the properties of ?","F(x,y,y',y'',...,y^{(n)})=0 x y X_1 X_1^{(n)} X_1^{(n)} X_1^{(n)}I=0 I I X_1^{(n)} I X_1 I","['ordinary-differential-equations', 'lie-groups', 'lie-algebras', 'invariant-theory']"
76,How to solve $\frac{dy}{dx}=-\frac{y}{x^2+y^2}$?,How to solve ?,\frac{dy}{dx}=-\frac{y}{x^2+y^2},"How does one solve the differential equation $$\frac{dy}{dx}=-\frac{y}{x^2+y^2}$$ I've tried to convert to polar coordinates: I let $$x=r(\varphi)\cdot\cos(\varphi)$$ $$y=r(\varphi)\cdot\sin(\varphi)$$ Then, $$\frac{dx}{d\varphi}=r'(\varphi)\cdot\cos(\varphi)+r(\varphi)\cdot\sin(\varphi)$$ $$\frac{dy}{d\varphi}=r'(\varphi)\cdot\sin(\varphi)+r(\varphi)\cdot\cos(\varphi)$$ such that $$\frac{dy}{dx}=\frac{r'(\varphi)\cdot\sin(\varphi)+r(\varphi)\cdot\cos(\varphi)}{r'(\varphi)\cdot\cos(\varphi)+r(\varphi)\cdot\sin(\varphi)}$$ Substitution into the differential equation yields $$\frac{r'(\varphi)\cdot\sin(\varphi)+r(\varphi)\cdot\cos(\varphi)}{r'(\varphi)\cdot\cos(\varphi)+r(\varphi)\cdot\sin(\varphi)}=-\frac{\sin(\varphi)}{r(\varphi)}$$ It does not seem to get any easier now does it.","How does one solve the differential equation I've tried to convert to polar coordinates: I let Then, such that Substitution into the differential equation yields It does not seem to get any easier now does it.",\frac{dy}{dx}=-\frac{y}{x^2+y^2} x=r(\varphi)\cdot\cos(\varphi) y=r(\varphi)\cdot\sin(\varphi) \frac{dx}{d\varphi}=r'(\varphi)\cdot\cos(\varphi)+r(\varphi)\cdot\sin(\varphi) \frac{dy}{d\varphi}=r'(\varphi)\cdot\sin(\varphi)+r(\varphi)\cdot\cos(\varphi) \frac{dy}{dx}=\frac{r'(\varphi)\cdot\sin(\varphi)+r(\varphi)\cdot\cos(\varphi)}{r'(\varphi)\cdot\cos(\varphi)+r(\varphi)\cdot\sin(\varphi)} \frac{r'(\varphi)\cdot\sin(\varphi)+r(\varphi)\cdot\cos(\varphi)}{r'(\varphi)\cdot\cos(\varphi)+r(\varphi)\cdot\sin(\varphi)}=-\frac{\sin(\varphi)}{r(\varphi)},['ordinary-differential-equations']
77,Finding a First Integral of a System of First-Order DEs,Finding a First Integral of a System of First-Order DEs,,"I am trying to show that the system $$\frac{dx_1}{dt}=ax_1, \ \ \frac{dx_2}{dt}=-x_2$$ has a first integral of the form $$K(x_1,x_2)=\ln(f(x_1))+\ln(g(x_2))a$$ My attempt: I will use the following method. $$\frac{dx_1}{dt}=ax_1\iff \frac{dx_1}{ax_1}=dt \ \ \ \ \ \ \ \ (1)$$ $$\frac{dx_2}{dt}=-x_2\iff \frac{dx_2}{-x_2}=dt \ \ \ \ \ \ \ (2)$$ Equating $(1)$ and $(2)$ yields \begin{align} \frac{dx_1}{ax_1}&=\frac{dx_2}{-x_2} \\ \frac{1}{a}\ln|x_1|&=-\ln|x_2|+C \\ \ln|x_1|+a\ln|x_2|&=0 \ \ \  \ \ (C=0) \end{align} I do not know where to go from here. Is my working correct so far? Any advice would be greatly appreciated.","I am trying to show that the system $$\frac{dx_1}{dt}=ax_1, \ \ \frac{dx_2}{dt}=-x_2$$ has a first integral of the form $$K(x_1,x_2)=\ln(f(x_1))+\ln(g(x_2))a$$ My attempt: I will use the following method. $$\frac{dx_1}{dt}=ax_1\iff \frac{dx_1}{ax_1}=dt \ \ \ \ \ \ \ \ (1)$$ $$\frac{dx_2}{dt}=-x_2\iff \frac{dx_2}{-x_2}=dt \ \ \ \ \ \ \ (2)$$ Equating $(1)$ and $(2)$ yields \begin{align} \frac{dx_1}{ax_1}&=\frac{dx_2}{-x_2} \\ \frac{1}{a}\ln|x_1|&=-\ln|x_2|+C \\ \ln|x_1|+a\ln|x_2|&=0 \ \ \  \ \ (C=0) \end{align} I do not know where to go from here. Is my working correct so far? Any advice would be greatly appreciated.",,['ordinary-differential-equations']
78,Non-Linear (Second Order) Differential Equation,Non-Linear (Second Order) Differential Equation,,I need some hints for solving $yy''-(y')^2=xy^2$. I noticed that the left hand side is close to $(yy')'$: $yy''-(y')^2=xy^2\ \Leftrightarrow\ yy''+(y')^2-2(y')^2=xy^2\ \Leftrightarrow\ (yy')'-2(y')^2=xy^2$. But I don't know how to continue expressing the terms as derivatives of some functions. Thanks,I need some hints for solving $yy''-(y')^2=xy^2$. I noticed that the left hand side is close to $(yy')'$: $yy''-(y')^2=xy^2\ \Leftrightarrow\ yy''+(y')^2-2(y')^2=xy^2\ \Leftrightarrow\ (yy')'-2(y')^2=xy^2$. But I don't know how to continue expressing the terms as derivatives of some functions. Thanks,,"['real-analysis', 'ordinary-differential-equations']"
79,Choosing a boundary for integration,Choosing a boundary for integration,,"I have the following differential equation  $$ \frac{d}{dx}\left(\mu e^{cx}f(x)\right) = -\mu\left(\frac{a xe^{-cx}}{a x+x-1}\right) $$ that I am trying to integrate to find $f(x)$ with the boundary constraint that $f(1) =1$. The integrating factor $\mu$ is given by  $$ \mu = e^{\frac{a c}{1+a}x}\left((a+1)x-1\right)^{\frac{1+a+ac}{(1+a)^2}} $$ If we integrate this with a lower limit of $x$ then  $$ \mu e^{ct}f(t)\big|_x = -\int_x  \mu\left(\frac{a te^{-ct}}{a t+t-1}\right)dt $$ However, what would be a sensible upper limit so that I can find $f(x) = ?$. The integrating factor has a zero at $x=1/(a+1)$. Would it make sense to integrate up to this boundary?","I have the following differential equation  $$ \frac{d}{dx}\left(\mu e^{cx}f(x)\right) = -\mu\left(\frac{a xe^{-cx}}{a x+x-1}\right) $$ that I am trying to integrate to find $f(x)$ with the boundary constraint that $f(1) =1$. The integrating factor $\mu$ is given by  $$ \mu = e^{\frac{a c}{1+a}x}\left((a+1)x-1\right)^{\frac{1+a+ac}{(1+a)^2}} $$ If we integrate this with a lower limit of $x$ then  $$ \mu e^{ct}f(t)\big|_x = -\int_x  \mu\left(\frac{a te^{-ct}}{a t+t-1}\right)dt $$ However, what would be a sensible upper limit so that I can find $f(x) = ?$. The integrating factor has a zero at $x=1/(a+1)$. Would it make sense to integrate up to this boundary?",,"['integration', 'ordinary-differential-equations', 'boundary-value-problem']"
80,Existence of a solution for a ODE defined on an open set in $\mathbb{R}^n$.,Existence of a solution for a ODE defined on an open set in .,\mathbb{R}^n,"Let $U \subset \mathbb{R}^n$ be an open set, and $B$ a smooth matrix valuated function $B:U \to M_n(\mathbb{R})$ . Let $\frac{\partial}{\partial x^1}$ be the constant vector field defined on $U$.  I am trying (with no success)  to prove that given $x_0 \in U$, and $A_0 \in M_n(\mathbb{R})$ there exist a smooth matrix valuated function  $A:U \to M_n(\mathbb{R})$ , such that: $\frac{\partial A}{ \partial x^1}= B \cdot A $, with $A(x_0)=A_0$. And that the dependence on $(x_0 , A_0)$ is smooth. Well i arrived to this problem trying to show the smoothness of a distribution defined on a manifold, and using a chart (reading Spivak introduction to differentiable geometry vol 2, pp 338 2nd edition), and i have been trying with the classical existence theorems, my idea is: Take $x_0=(x_0^1 , \dots , x_0^n) \in U$ and define the first ODE: $A'(t)= B(c(t))\cdot A(c(t))$. Where $c$ is an integral curve for $\frac{\partial}{\partial x^1}$ such that $c(0)= x_0$, this equation is very well know to have a solution, with initial condition $A_0$, and seems natural to try to define $A(c(t)) = A(t)$. If I take curves by $(x_0^1 \pm \epsilon, x_0^2, \dots x_0^n)$ I can define  ODE with the same initial condition. The problem is i can't see why all these solutions, glue together! I would appreciate any help or advise. in fact, i don't know if it is true! (one friend told me to use a more general existence of solution theorem which lets $B$ be a smooth function of $\mathbb{R}^n$, but i can't find this theorem anywhere) ... Any help or advise is welcome. Thank you all!","Let $U \subset \mathbb{R}^n$ be an open set, and $B$ a smooth matrix valuated function $B:U \to M_n(\mathbb{R})$ . Let $\frac{\partial}{\partial x^1}$ be the constant vector field defined on $U$.  I am trying (with no success)  to prove that given $x_0 \in U$, and $A_0 \in M_n(\mathbb{R})$ there exist a smooth matrix valuated function  $A:U \to M_n(\mathbb{R})$ , such that: $\frac{\partial A}{ \partial x^1}= B \cdot A $, with $A(x_0)=A_0$. And that the dependence on $(x_0 , A_0)$ is smooth. Well i arrived to this problem trying to show the smoothness of a distribution defined on a manifold, and using a chart (reading Spivak introduction to differentiable geometry vol 2, pp 338 2nd edition), and i have been trying with the classical existence theorems, my idea is: Take $x_0=(x_0^1 , \dots , x_0^n) \in U$ and define the first ODE: $A'(t)= B(c(t))\cdot A(c(t))$. Where $c$ is an integral curve for $\frac{\partial}{\partial x^1}$ such that $c(0)= x_0$, this equation is very well know to have a solution, with initial condition $A_0$, and seems natural to try to define $A(c(t)) = A(t)$. If I take curves by $(x_0^1 \pm \epsilon, x_0^2, \dots x_0^n)$ I can define  ODE with the same initial condition. The problem is i can't see why all these solutions, glue together! I would appreciate any help or advise. in fact, i don't know if it is true! (one friend told me to use a more general existence of solution theorem which lets $B$ be a smooth function of $\mathbb{R}^n$, but i can't find this theorem anywhere) ... Any help or advise is welcome. Thank you all!",,"['ordinary-differential-equations', 'differential-geometry']"
81,Solve $2xy+y^2-2x^2\frac{\mathrm{d}y}{\mathrm{d}x}=0$; $y=2$ when $x=1$,Solve ;  when,2xy+y^2-2x^2\frac{\mathrm{d}y}{\mathrm{d}x}=0 y=2 x=1,"$2xy+y^2-2x^2\dfrac{\mathrm{d}y}{\mathrm{d}x}=0$ ; $y=2$ when $x=1$ . My reference gives the solution $y=\dfrac{2x}{1-\log x}$ , but is it really the solution ? My Attempt $$ 2xy+y^2-2x^2\dfrac{\mathrm{d}y}{\mathrm{d}x}=0\implies\dfrac{\mathrm{d}y}{\mathrm{d}x}=\dfrac{2xy+y^2}{2x^2}=\frac{y}{x}+\frac{1}{2}\frac{y^2}{x^2}\\ \text{Put }v=\frac{y}{x}\implies y=vx\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}=v+x\dfrac{\mathrm{d}v}{\mathrm{d}x}=v+\frac12v^2\implies x\dfrac{\mathrm{d}v}{\mathrm{d}x}=\frac{1}{2}v^2\\ 2\int v^{-2}dv=\int\frac{\mathrm{d}x}{x}\implies -\frac{2}{v}=\log|x|+C\\ \boxed{\frac{-2x}{y}=\log|x|+C} $$ $y=2$ when $x=1$$\implies C=-1$ , $$ \frac{-2x}{y}=\log|x|-1\implies \color{red}{y=\dfrac{2x}{1-\log|x|}} $$ How do I justify going from $y=\dfrac{2x}{1-\log\color{red}{|}x\color{red}{|}}$ to $y=\dfrac{2x}{1-\log x}$ ? What I understand I only have basic knowledge on differential equations, thus not familiar with the ideas like singularity and all. I think I only understand a hint, $$ y=\dfrac{2x}{1-\log|x|}\implies y=\begin{cases}\dfrac{2x}{1-\log x} \text{ for } x>0\\\dfrac{2x}{1-\log(-x)} \text{ for } x<0\end{cases} $$ So the condition "" $y=2$ when $x=1$ "" does not include in the second case. Does this has something to do with my doubt ?","; when . My reference gives the solution , but is it really the solution ? My Attempt when , How do I justify going from to ? What I understand I only have basic knowledge on differential equations, thus not familiar with the ideas like singularity and all. I think I only understand a hint, So the condition "" when "" does not include in the second case. Does this has something to do with my doubt ?","2xy+y^2-2x^2\dfrac{\mathrm{d}y}{\mathrm{d}x}=0 y=2 x=1 y=\dfrac{2x}{1-\log x} 
2xy+y^2-2x^2\dfrac{\mathrm{d}y}{\mathrm{d}x}=0\implies\dfrac{\mathrm{d}y}{\mathrm{d}x}=\dfrac{2xy+y^2}{2x^2}=\frac{y}{x}+\frac{1}{2}\frac{y^2}{x^2}\\
\text{Put }v=\frac{y}{x}\implies y=vx\\
\dfrac{\mathrm{d}y}{\mathrm{d}x}=v+x\dfrac{\mathrm{d}v}{\mathrm{d}x}=v+\frac12v^2\implies x\dfrac{\mathrm{d}v}{\mathrm{d}x}=\frac{1}{2}v^2\\
2\int v^{-2}dv=\int\frac{\mathrm{d}x}{x}\implies -\frac{2}{v}=\log|x|+C\\
\boxed{\frac{-2x}{y}=\log|x|+C}
 y=2 x=1\implies C=-1 
\frac{-2x}{y}=\log|x|-1\implies \color{red}{y=\dfrac{2x}{1-\log|x|}}
 y=\dfrac{2x}{1-\log\color{red}{|}x\color{red}{|}} y=\dfrac{2x}{1-\log x} 
y=\dfrac{2x}{1-\log|x|}\implies y=\begin{cases}\dfrac{2x}{1-\log x} \text{ for } x>0\\\dfrac{2x}{1-\log(-x)} \text{ for } x<0\end{cases}
 y=2 x=1","['calculus', 'ordinary-differential-equations', 'logarithms', 'absolute-value']"
82,Does an analytical solution for this ODE system exist?,Does an analytical solution for this ODE system exist?,,"I have a system of differential equations, and I want to know if it is possible to get an analytical solution before resorting to numerical methods and software. \begin{align} \frac{dy_1}{dx} + y_1 - y_3 &= 0 \\ \frac{dy_2}{dx} + y_2 - y_4 &= 0 \\ \frac{dy_3}{dx} + y_1 - y_3 &= f(x)\frac{y_3}{y_3 +1}\frac{y_4}{y_4 +1} \\ \frac{dy_4}{dx} + y_2 - y_4 &= f(x)\frac{y_3}{y_3 +1}\frac{y_4}{y_4 +1}  \end{align} where $f(x)$ is a simple, probably linear, function I think because of the non linear term, a solution by the laplace or eigenvalue methods is ruled out. Is there something I'm missing, or is it really only possible numerically?","I have a system of differential equations, and I want to know if it is possible to get an analytical solution before resorting to numerical methods and software. \begin{align} \frac{dy_1}{dx} + y_1 - y_3 &= 0 \\ \frac{dy_2}{dx} + y_2 - y_4 &= 0 \\ \frac{dy_3}{dx} + y_1 - y_3 &= f(x)\frac{y_3}{y_3 +1}\frac{y_4}{y_4 +1} \\ \frac{dy_4}{dx} + y_2 - y_4 &= f(x)\frac{y_3}{y_3 +1}\frac{y_4}{y_4 +1}  \end{align} where $f(x)$ is a simple, probably linear, function I think because of the non linear term, a solution by the laplace or eigenvalue methods is ruled out. Is there something I'm missing, or is it really only possible numerically?",,['ordinary-differential-equations']
83,$f'(x) = f(x-1)$ then $f$ is not bounded,then  is not bounded,f'(x) = f(x-1) f,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$. Then consider the following delay equation :  $$f'(x) = f(x-1)$$ Let $S$ be the set of solution ot this equation. Then I would like to prove that : $\forall f \in S -\{ x \mapsto 0\}$, $f$ is not bounded. What I've done so far is that : The set of solutions is an infinite dimensional space, hence it's hard to get a general form of all solutions, hence I don't think that calculating the characteristic equation is a good idea. I've noticed that this is true for all equations of the form : $f'(x) = f(x-a)$ where $-1 \leq a \leq 1$.  Yet this is not true for $a = 3\pi/2$, because $x \mapsto \sin(x)$ is a trivial solution and clearly $x \mapsto \sin(x)$ is bounded. Maybe using the mean value is a good idea, in order to get a contradiction if we suppose that $f$ is bounded. But the problem with this approach is that I don't get enough informations on $a$. We also have the formula :  $$f^{(n)}(x) = f(x-n)$$ Thank you !","Let $f:\mathbb{R} \rightarrow \mathbb{R}$. Then consider the following delay equation :  $$f'(x) = f(x-1)$$ Let $S$ be the set of solution ot this equation. Then I would like to prove that : $\forall f \in S -\{ x \mapsto 0\}$, $f$ is not bounded. What I've done so far is that : The set of solutions is an infinite dimensional space, hence it's hard to get a general form of all solutions, hence I don't think that calculating the characteristic equation is a good idea. I've noticed that this is true for all equations of the form : $f'(x) = f(x-a)$ where $-1 \leq a \leq 1$.  Yet this is not true for $a = 3\pi/2$, because $x \mapsto \sin(x)$ is a trivial solution and clearly $x \mapsto \sin(x)$ is bounded. Maybe using the mean value is a good idea, in order to get a contradiction if we suppose that $f$ is bounded. But the problem with this approach is that I don't get enough informations on $a$. We also have the formula :  $$f^{(n)}(x) = f(x-n)$$ Thank you !",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'delay-differential-equations']"
84,Solve differential equation with three variables,Solve differential equation with three variables,,"I have the following differential equation: $$z''+y'= \cos (x)$$ $$y''-z= \sin (x)$$ with $z(0)=1$, $z'(0)=-1$, $y(0)=1$ and $y'(0)=0$. These differential equations involves three variables $x, y, z$. Can someone drop a hint on how to start this.","I have the following differential equation: $$z''+y'= \cos (x)$$ $$y''-z= \sin (x)$$ with $z(0)=1$, $z'(0)=-1$, $y(0)=1$ and $y'(0)=0$. These differential equations involves three variables $x, y, z$. Can someone drop a hint on how to start this.",,['ordinary-differential-equations']
85,Simple differential equation solution mismatches given answer,Simple differential equation solution mismatches given answer,,Solve $$\left(y-x\frac{dy}{dx}\right)=a\left(y^2+\frac{dy}{dx}\right)$$ I solved it by dividing both sides with $y^2/dx$: $$\frac{(ydx-xdy)}{y^2}=a\left(dx+\frac{dy}{y^2}\right)$$ and then integrate to get $$x/y=ax-a/y+c$$But that doesn't match the answer given by SymbolLab : $$y=\frac{ac_1+xc_1}{a\left(-1+xc_1+ac_1\right)}$$ What's the problem here? Why doesn't my simple differential equation solution match with the given answer?,Solve $$\left(y-x\frac{dy}{dx}\right)=a\left(y^2+\frac{dy}{dx}\right)$$ I solved it by dividing both sides with $y^2/dx$: $$\frac{(ydx-xdy)}{y^2}=a\left(dx+\frac{dy}{y^2}\right)$$ and then integrate to get $$x/y=ax-a/y+c$$But that doesn't match the answer given by SymbolLab : $$y=\frac{ac_1+xc_1}{a\left(-1+xc_1+ac_1\right)}$$ What's the problem here? Why doesn't my simple differential equation solution match with the given answer?,,['ordinary-differential-equations']
86,Finding an implicit solution of the ODE $y' (x) = \frac{y^2-xy}{2xy^3+xy+x^2}$,Finding an implicit solution of the ODE,y' (x) = \frac{y^2-xy}{2xy^3+xy+x^2},"I am currently trying to find an implicit solution of the nonlinear ODE:$$y' (x) = \frac{y^2-xy}{2xy^3+xy+x^2}$$ But to be quite honest, I am a little bit lost. I am pretty new to this field and have not learned many methods of solving differential equations so far. What I tried was, to reformulate the problem as $$xy-y^2+y'(2xy^3+xy+x^2)=0$$ and check if this is an exact equation. However, the integrability criterion does not hold: $$\frac d{dx}(2xy^3+xy+x^2) \neq \frac{d}{dy}(xy-y^2)$$ Therefore, I tried to find an integrating factor to make this equation exact. The approach that my integrating factor $m(x,y)$ only depends on $x$ or only depends on $y$ seems to fail. Another approach I have seen is to set $m(x,y) = x^\alpha y^\beta$ as the integrating factor and then find out what $\alpha $ and $\beta$ have to be. Unfortunately, when using this approach, I end up with utterly long equations, which do not seem easily solveable ( I managed to actually find values for $\alpha$ and $\beta$ using Maple and then verify that the ODE multiplied by that factor is exact numerically, but I do not think this is the way to go here.) Any help would be greatly appreciated!","I am currently trying to find an implicit solution of the nonlinear ODE:$$y' (x) = \frac{y^2-xy}{2xy^3+xy+x^2}$$ But to be quite honest, I am a little bit lost. I am pretty new to this field and have not learned many methods of solving differential equations so far. What I tried was, to reformulate the problem as $$xy-y^2+y'(2xy^3+xy+x^2)=0$$ and check if this is an exact equation. However, the integrability criterion does not hold: $$\frac d{dx}(2xy^3+xy+x^2) \neq \frac{d}{dy}(xy-y^2)$$ Therefore, I tried to find an integrating factor to make this equation exact. The approach that my integrating factor $m(x,y)$ only depends on $x$ or only depends on $y$ seems to fail. Another approach I have seen is to set $m(x,y) = x^\alpha y^\beta$ as the integrating factor and then find out what $\alpha $ and $\beta$ have to be. Unfortunately, when using this approach, I end up with utterly long equations, which do not seem easily solveable ( I managed to actually find values for $\alpha$ and $\beta$ using Maple and then verify that the ODE multiplied by that factor is exact numerically, but I do not think this is the way to go here.) Any help would be greatly appreciated!",,['ordinary-differential-equations']
87,Second Order Partial Derivative Chain Rule,Second Order Partial Derivative Chain Rule,,"If I proceed to compute the chain rule of a second-order partial derivative of a continuous function $\psi(x,y)$ in terms of new coordinates $\xi=\xi(x,y)$ and $\eta=\eta(x,y)$ like $$\frac{\partial^2 \psi}{\partial x^2}=\left(\frac{\partial \xi }{\partial x}\frac{% \partial }{\partial \xi }+\frac{\partial \eta }{\partial x}\frac{% \partial }{\partial \eta }\right)\left(\frac{\partial \xi }{\partial x}\frac{% \partial \psi}{\partial \xi }+\frac{\partial \eta }{\partial x}\frac{% \partial \psi}{\partial \eta }\right),$$ I obtain the following result $$\frac{\partial^2 \psi}{\partial x^2}=\left(\frac{\partial\xi}{\partial x}\right)^2\frac{\partial^2\psi}{\partial \xi^2}+\left(\frac{\partial\eta}{\partial x}\right)^2\frac{\partial^2\psi}{\partial \eta^2}+2\frac{\partial \eta}{\partial x}\frac{\partial \xi}{\partial x}\frac{\partial^2\psi}{\partial\eta\partial\xi}.$$ I know this result is incorrect because I miss two terms with second-order derivatives of $\xi$ and $\eta$, but I do not identify in which step I made the mistake.","If I proceed to compute the chain rule of a second-order partial derivative of a continuous function $\psi(x,y)$ in terms of new coordinates $\xi=\xi(x,y)$ and $\eta=\eta(x,y)$ like $$\frac{\partial^2 \psi}{\partial x^2}=\left(\frac{\partial \xi }{\partial x}\frac{% \partial }{\partial \xi }+\frac{\partial \eta }{\partial x}\frac{% \partial }{\partial \eta }\right)\left(\frac{\partial \xi }{\partial x}\frac{% \partial \psi}{\partial \xi }+\frac{\partial \eta }{\partial x}\frac{% \partial \psi}{\partial \eta }\right),$$ I obtain the following result $$\frac{\partial^2 \psi}{\partial x^2}=\left(\frac{\partial\xi}{\partial x}\right)^2\frac{\partial^2\psi}{\partial \xi^2}+\left(\frac{\partial\eta}{\partial x}\right)^2\frac{\partial^2\psi}{\partial \eta^2}+2\frac{\partial \eta}{\partial x}\frac{\partial \xi}{\partial x}\frac{\partial^2\psi}{\partial\eta\partial\xi}.$$ I know this result is incorrect because I miss two terms with second-order derivatives of $\xi$ and $\eta$, but I do not identify in which step I made the mistake.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
88,Reducing the number of parameters of an ODE system through nondimensionalization,Reducing the number of parameters of an ODE system through nondimensionalization,,"I am trying to reduce the number of parameters of the ODE system:   \begin{align} \label{eq:Iprime} I'(t)&= \sigma B-\mu I\\ B'_1(t)&=r_1 B_1\left(1-\frac{B_1 }{K_1}\right)-d_1IB_1-m (B_1-B_2) \\ \label{eq:B2prime} B'_2(t)&=r_2 B_2\left(1-\frac{B_2 }{K_2}\right)-d_2IB_2-m (B_2-B_1)  \end{align}   (where $B=B_1+B_2$) by applying nondimensionalization. In all the examples I have seen this reduced the number of parameters. However if I apply the technique to my problem the number of parameters remain the same! Why is this the case? Work thus far We assume the following scaling:  $$I=\widetilde{I}\widehat{I}  \quad B_1=\widetilde{B}_1\widehat{B}_1 \quad B_2=\widetilde{B}_2\widehat{B}_2  \quad t=\tilde{t} \hat{t} $$ where $\widetilde{I}, \widetilde{B}_1, \widetilde{B}_2, \tilde{t}$ are constants (dimension-carrying), to be chosen, and $\widehat{I}, \widehat{B}_1, \widehat{B}_2, \hat{t}$ are the dimensionless variables. Subbing these into our ODEs leads to  \begin{align} \frac{d(\widetilde{I}\widehat{I})}{d(\tilde{t} \hat{t} )}&=\sigma  ( \widetilde{B}_1\widehat{B}_1 + \widetilde{B}_2\widehat{B}_2)-\mu \widetilde{I}\widehat{I}\\ \frac{d \widehat{I}}{d \hat{t}}&=\frac{\tilde{t}\sigma}{\widetilde{I}}  ( \widetilde{B}_1\widehat{B}_1 + \widetilde{B}_2\widehat{B}_2)-\frac{\mu \tilde{t}}{\widetilde{I}}  \widehat{I} \\ \frac{d \widehat{I}}{d \hat{t}}&=\left[\frac{\tilde{t}\sigma}{\widetilde{I}}   \widetilde{B}_1\right] \widehat{B}_1 + \left[ \frac{\tilde{t}\sigma}{\widetilde{I}} \widetilde{B}_2 \right] \widehat{B}_2-\left[\frac{\mu \tilde{t}}{\widetilde{I}}\right]  \widehat{I} \\ \end{align} \begin{align} \frac{d(\widetilde{B}_1\widehat{B}_1)}{d(\tilde{t} \hat{t} )}&=r_1 \widetilde{B}_1\widehat{B}_1\left(1-\frac{\widetilde{B}_1\widehat{B}_1 }{K_1}\right)-d_1\widetilde{I}\widehat{I}\widetilde{B}_1\widehat{B}_1-m (\widetilde{B}_1\widehat{B}_1-\widetilde{B}_2\widehat{B}_2) \\ \frac{d \widehat{B}_1}{d \hat{t}}&=r_1 \tilde{t}\widehat{B}_1\left(1-\frac{\widetilde{B}_1 }{K_1}\widehat{B}_1\right)-d_1\tilde{t }\widetilde{I} \widehat{I}\widehat{B}_1-m\frac{\tilde{t}}{\widetilde{B}_1} (\widetilde{B}_1\widehat{B}_1-\widetilde{B}_2\widehat{B}_2) \\ \frac{d \widehat{B}_1}{d \hat{t}}&=\left[ r_1 \tilde{t}\right]\widehat{B}_1\left(1-\left[ \frac{\widetilde{B}_1 }{K_1}\right]\widehat{B}_1\right)-\left[ d_1\tilde{t } \widetilde{I}\right]\widehat{I}\widehat{B}_1-\left[m \tilde{t} \right] \widehat{B}_1+\left[m \frac{\tilde{t} \widetilde{B}_2}{\widetilde{B}_1} \right]\widehat{B}_2 \end{align} (the procedure for $B_2$ is the same) The square brackets are to indicate the number of independent choices for the scales. In all of the examples I have seen so far the number of independent choices for the scales are the same as the number of the orginal parameters for the system. At maximum I can only choose four of these choices to be $1$. Meaning that I will have $13-4=9$ parameters remaing. The same as what I started with. What am I doing wrong/why can't I reduce the number of parameters in my system?  If you need more clarification on this problem please ask.","I am trying to reduce the number of parameters of the ODE system:   \begin{align} \label{eq:Iprime} I'(t)&= \sigma B-\mu I\\ B'_1(t)&=r_1 B_1\left(1-\frac{B_1 }{K_1}\right)-d_1IB_1-m (B_1-B_2) \\ \label{eq:B2prime} B'_2(t)&=r_2 B_2\left(1-\frac{B_2 }{K_2}\right)-d_2IB_2-m (B_2-B_1)  \end{align}   (where $B=B_1+B_2$) by applying nondimensionalization. In all the examples I have seen this reduced the number of parameters. However if I apply the technique to my problem the number of parameters remain the same! Why is this the case? Work thus far We assume the following scaling:  $$I=\widetilde{I}\widehat{I}  \quad B_1=\widetilde{B}_1\widehat{B}_1 \quad B_2=\widetilde{B}_2\widehat{B}_2  \quad t=\tilde{t} \hat{t} $$ where $\widetilde{I}, \widetilde{B}_1, \widetilde{B}_2, \tilde{t}$ are constants (dimension-carrying), to be chosen, and $\widehat{I}, \widehat{B}_1, \widehat{B}_2, \hat{t}$ are the dimensionless variables. Subbing these into our ODEs leads to  \begin{align} \frac{d(\widetilde{I}\widehat{I})}{d(\tilde{t} \hat{t} )}&=\sigma  ( \widetilde{B}_1\widehat{B}_1 + \widetilde{B}_2\widehat{B}_2)-\mu \widetilde{I}\widehat{I}\\ \frac{d \widehat{I}}{d \hat{t}}&=\frac{\tilde{t}\sigma}{\widetilde{I}}  ( \widetilde{B}_1\widehat{B}_1 + \widetilde{B}_2\widehat{B}_2)-\frac{\mu \tilde{t}}{\widetilde{I}}  \widehat{I} \\ \frac{d \widehat{I}}{d \hat{t}}&=\left[\frac{\tilde{t}\sigma}{\widetilde{I}}   \widetilde{B}_1\right] \widehat{B}_1 + \left[ \frac{\tilde{t}\sigma}{\widetilde{I}} \widetilde{B}_2 \right] \widehat{B}_2-\left[\frac{\mu \tilde{t}}{\widetilde{I}}\right]  \widehat{I} \\ \end{align} \begin{align} \frac{d(\widetilde{B}_1\widehat{B}_1)}{d(\tilde{t} \hat{t} )}&=r_1 \widetilde{B}_1\widehat{B}_1\left(1-\frac{\widetilde{B}_1\widehat{B}_1 }{K_1}\right)-d_1\widetilde{I}\widehat{I}\widetilde{B}_1\widehat{B}_1-m (\widetilde{B}_1\widehat{B}_1-\widetilde{B}_2\widehat{B}_2) \\ \frac{d \widehat{B}_1}{d \hat{t}}&=r_1 \tilde{t}\widehat{B}_1\left(1-\frac{\widetilde{B}_1 }{K_1}\widehat{B}_1\right)-d_1\tilde{t }\widetilde{I} \widehat{I}\widehat{B}_1-m\frac{\tilde{t}}{\widetilde{B}_1} (\widetilde{B}_1\widehat{B}_1-\widetilde{B}_2\widehat{B}_2) \\ \frac{d \widehat{B}_1}{d \hat{t}}&=\left[ r_1 \tilde{t}\right]\widehat{B}_1\left(1-\left[ \frac{\widetilde{B}_1 }{K_1}\right]\widehat{B}_1\right)-\left[ d_1\tilde{t } \widetilde{I}\right]\widehat{I}\widehat{B}_1-\left[m \tilde{t} \right] \widehat{B}_1+\left[m \frac{\tilde{t} \widetilde{B}_2}{\widetilde{B}_1} \right]\widehat{B}_2 \end{align} (the procedure for $B_2$ is the same) The square brackets are to indicate the number of independent choices for the scales. In all of the examples I have seen so far the number of independent choices for the scales are the same as the number of the orginal parameters for the system. At maximum I can only choose four of these choices to be $1$. Meaning that I will have $13-4=9$ parameters remaing. The same as what I started with. What am I doing wrong/why can't I reduce the number of parameters in my system?  If you need more clarification on this problem please ask.",,"['ordinary-differential-equations', 'dimensional-analysis']"
89,Intuitive proof of Lyapunov's theorem,Intuitive proof of Lyapunov's theorem,,"I have recently come across Lyapunov's stability theorem, which states: Let $0$ be an equilibrium of a differential autonomous system $X'=f(X)$ where $f$ is smooth. Suppose $V$ is a Lyapunov function of the system in some neighborhood of $0$ ; that is, $V(X(t))$ is non-increasing on $t$ for every solution $X$ of the system, and furthermore suppose that $V$ is definite positive. Then $0$ is a stable equilibrium. Sadly, all the proofs I have seen feel rather obscure to me. Do you know of any good, intuitive proof of the theorem? (or would you be so kind to outline one yourself?)","I have recently come across Lyapunov's stability theorem, which states: Let be an equilibrium of a differential autonomous system where is smooth. Suppose is a Lyapunov function of the system in some neighborhood of ; that is, is non-increasing on for every solution of the system, and furthermore suppose that is definite positive. Then is a stable equilibrium. Sadly, all the proofs I have seen feel rather obscure to me. Do you know of any good, intuitive proof of the theorem? (or would you be so kind to outline one yourself?)",0 X'=f(X) f V 0 V(X(t)) t X V 0,"['ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
90,Recognizing recursion relation of series that is solutions of $y'' + y' + x^2 y = 0$ around $x_0 = 0$.,Recognizing recursion relation of series that is solutions of  around .,y'' + y' + x^2 y = 0 x_0 = 0,"I have been tried to find the general solution for the ODE above, but its recursion relation became so complex that I doesn't can construct its power series solution. Below is my attempt, assuming $y = \sum_{n = 0}^{\infty} a_n x^n$ is the solution. \begin{equation*} y = \sum_{n = 0}^{\infty} a_n x^n; \quad y' = \sum_{n = 1}^{\infty} n a_n x^{n-1}; \quad y'' = \sum_{n = 2}^{\infty} n (n - 1) a_n x^{n - 2}. \end{equation*} \begin{equation*} 	\Rightarrow \sum_{n = 2}^{\infty} n (n - 1) a_n x^{n - 2} + \sum_{n = 1}^{\infty} n a_n x^{n-1} + x^2 \sum_{n = 0}^{\infty} a_n x^n = 0 \end{equation*} \begin{equation*} 	\sum_{n = 0}^{\infty} (n + 2) (n + 1) a_{n + 2} x^{n} + \sum_{n = 0}^{\infty} (n + 1) a_{n + 1} x^{n} + \sum_{n = 2}^{\infty} a_{n - 2} x^{n} = 0 \end{equation*} \begin{equation*} 	2 \cdot 1 \cdot a_2 + 3 \cdot 2 a_3 \cdot x + \sum_{n = 2}^{\infty} (n + 2) (n + 1) a_{n + 2} x^{n} + 1 \cdot a_1 + 2 a_2 x + \sum_{n = 2}^{\infty} (n + 1) a_{n + 1} x^{n} + \sum_{n = 2}^{\infty} a_{n - 2} x^{n} = 0 \end{equation*} \begin{equation*} 	a_1 + 2 a_2 + x(2 a_2 + 3 \cdot 2 a_3) + \sum_{n = 2}^{\infty} [(n + 2) (n + 1) a_{n + 2} + (n + 1) a_{n + 1} + a_{n - 2}] x^n = 0 \end{equation*} And the recursion relation: \begin{equation*} 	\begin{split} 		& a_1 + 2 a_2  = 0 \Rightarrow a_1 = -2 a_2 \\ 		& 2 a_2 + 3 \cdot 2 a_3  = 0 \Rightarrow a_3 = - \frac{a_2}{3}\\ 		& a_{n + 2}  = \frac{- (n + 1) a_{n + 1} - a_{n - 2}}{(n + 2) (n + 1)} =  - \frac{(n + 1) a_{n + 1} + a_{n - 2}}{(n + 2) (n + 1)} 	\end{split} \end{equation*} Starting from here, I calculate the coefficients until $n = 7$( $a_9$ ) but its recognition is being difficult for me. The following coefficients are $n = 2$: \begin{equation*} 	a_4 = \frac{- 3 a_3 - a_0}{4 \cdot 3} = \frac{a_2 - a_0}{4 \cdot 3} \end{equation*} $n = 3$: \begin{equation*} 	\begin{split} 		a_5 & = - \frac{4 a_4 + a_1}{5 \cdot 4} = - \frac{1}{5 \cdot 4} \bigg[4 \bigg( \frac{a_2 - a_0}{4 \cdot 3}\bigg) - 2 a_2\bigg] = - \frac{1}{5 \cdot 4 \cdot 3} \bigg[  a_2 - a_0 - 3 \cdot 2 a_2\bigg] \\ 		& = \frac{a_2 (3 \cdot 2 - 1) + a_0}{5 \cdot 4 \cdot 3}   	\end{split} \end{equation*} $n = 4$: \begin{equation*} 	\begin{split} 		a_6 & = - \frac{5 a_5 + a_2}{6 \cdot 5} = - \frac{1}{6 \cdot 5} \bigg[5 \bigg( \frac{a_2 (3 \cdot 2 - 1) + a_0}{5 \cdot 4 \cdot 3} \bigg) + a_2\bigg] = - \frac{a_2 (4 \cdot 3 + 3 \cdot 2 - 1) + a_0 }{6 \cdot 5 \cdot 4 \cdot 3}  	\end{split} \end{equation*} \begin{equation*}     \vdots \end{equation*} $n = 7$: \begin{equation*} 	\begin{split} 		a_9 & = - \frac{8 a_8 + a_5}{9 \cdot 8} \\ 		& = - \frac{a_2 (7 \cdot 6 \cdot 3 \cdot 2 - 7 \cdot 6 + 6 \cdot 5 + 5 \cdot 4 + 4 \cdot 3 + 3 \cdot 2 - 1) + a_0 (1 - 6 \cdot 5 + 6 \cdot 7)}{9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3}  	\end{split} \end{equation*} Note that in $n = 7$, the coefficient is similar, but it's getting different of the $n = 2, 3, 4$ because the factor of $a_0$ is growing since $n = 6$. Thanks for your attention.","I have been tried to find the general solution for the ODE above, but its recursion relation became so complex that I doesn't can construct its power series solution. Below is my attempt, assuming $y = \sum_{n = 0}^{\infty} a_n x^n$ is the solution. \begin{equation*} y = \sum_{n = 0}^{\infty} a_n x^n; \quad y' = \sum_{n = 1}^{\infty} n a_n x^{n-1}; \quad y'' = \sum_{n = 2}^{\infty} n (n - 1) a_n x^{n - 2}. \end{equation*} \begin{equation*} 	\Rightarrow \sum_{n = 2}^{\infty} n (n - 1) a_n x^{n - 2} + \sum_{n = 1}^{\infty} n a_n x^{n-1} + x^2 \sum_{n = 0}^{\infty} a_n x^n = 0 \end{equation*} \begin{equation*} 	\sum_{n = 0}^{\infty} (n + 2) (n + 1) a_{n + 2} x^{n} + \sum_{n = 0}^{\infty} (n + 1) a_{n + 1} x^{n} + \sum_{n = 2}^{\infty} a_{n - 2} x^{n} = 0 \end{equation*} \begin{equation*} 	2 \cdot 1 \cdot a_2 + 3 \cdot 2 a_3 \cdot x + \sum_{n = 2}^{\infty} (n + 2) (n + 1) a_{n + 2} x^{n} + 1 \cdot a_1 + 2 a_2 x + \sum_{n = 2}^{\infty} (n + 1) a_{n + 1} x^{n} + \sum_{n = 2}^{\infty} a_{n - 2} x^{n} = 0 \end{equation*} \begin{equation*} 	a_1 + 2 a_2 + x(2 a_2 + 3 \cdot 2 a_3) + \sum_{n = 2}^{\infty} [(n + 2) (n + 1) a_{n + 2} + (n + 1) a_{n + 1} + a_{n - 2}] x^n = 0 \end{equation*} And the recursion relation: \begin{equation*} 	\begin{split} 		& a_1 + 2 a_2  = 0 \Rightarrow a_1 = -2 a_2 \\ 		& 2 a_2 + 3 \cdot 2 a_3  = 0 \Rightarrow a_3 = - \frac{a_2}{3}\\ 		& a_{n + 2}  = \frac{- (n + 1) a_{n + 1} - a_{n - 2}}{(n + 2) (n + 1)} =  - \frac{(n + 1) a_{n + 1} + a_{n - 2}}{(n + 2) (n + 1)} 	\end{split} \end{equation*} Starting from here, I calculate the coefficients until $n = 7$( $a_9$ ) but its recognition is being difficult for me. The following coefficients are $n = 2$: \begin{equation*} 	a_4 = \frac{- 3 a_3 - a_0}{4 \cdot 3} = \frac{a_2 - a_0}{4 \cdot 3} \end{equation*} $n = 3$: \begin{equation*} 	\begin{split} 		a_5 & = - \frac{4 a_4 + a_1}{5 \cdot 4} = - \frac{1}{5 \cdot 4} \bigg[4 \bigg( \frac{a_2 - a_0}{4 \cdot 3}\bigg) - 2 a_2\bigg] = - \frac{1}{5 \cdot 4 \cdot 3} \bigg[  a_2 - a_0 - 3 \cdot 2 a_2\bigg] \\ 		& = \frac{a_2 (3 \cdot 2 - 1) + a_0}{5 \cdot 4 \cdot 3}   	\end{split} \end{equation*} $n = 4$: \begin{equation*} 	\begin{split} 		a_6 & = - \frac{5 a_5 + a_2}{6 \cdot 5} = - \frac{1}{6 \cdot 5} \bigg[5 \bigg( \frac{a_2 (3 \cdot 2 - 1) + a_0}{5 \cdot 4 \cdot 3} \bigg) + a_2\bigg] = - \frac{a_2 (4 \cdot 3 + 3 \cdot 2 - 1) + a_0 }{6 \cdot 5 \cdot 4 \cdot 3}  	\end{split} \end{equation*} \begin{equation*}     \vdots \end{equation*} $n = 7$: \begin{equation*} 	\begin{split} 		a_9 & = - \frac{8 a_8 + a_5}{9 \cdot 8} \\ 		& = - \frac{a_2 (7 \cdot 6 \cdot 3 \cdot 2 - 7 \cdot 6 + 6 \cdot 5 + 5 \cdot 4 + 4 \cdot 3 + 3 \cdot 2 - 1) + a_0 (1 - 6 \cdot 5 + 6 \cdot 7)}{9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3}  	\end{split} \end{equation*} Note that in $n = 7$, the coefficient is similar, but it's getting different of the $n = 2, 3, 4$ because the factor of $a_0$ is growing since $n = 6$. Thanks for your attention.",,"['sequences-and-series', 'ordinary-differential-equations', 'power-series', 'recursion']"
91,Power series solution of $y''+e^xy' - y=0$,Power series solution of,y''+e^xy' - y=0,"I'm stuck with finding the recursion relation of this differential equation using the power series method. So I started by setting: $$y(x)=\sum\limits_{n=0}^{\infty}{a_n x^n} \\ e^x=\sum\limits_{n=0}^{\infty}{\dfrac{x^n}{n!}}$$ But when I implemented this in the equation, I got stuck with finding the recursion relation because of the double summation. This is what I have: $\sum\limits_{n=2}^\infty (n)(n-1) a_n x^{n-2}+\left(\sum\limits_{n=0}^\infty\dfrac{x^n}{n!}\right)\left(\sum\limits_{n=1}^\infty n a_nx^{n-1}\right) - \sum\limits_{n=0}^{\infty}{a_n x^n} =0$ I know there exist the Cauchy product but I don't know how to use it in this case. PS: I know it is also possible to set $y=\sum\limits_{n=0}^\infty\dfrac{a_nx^n}{n!}$ , but I chose to use $y(x)=\sum\limits_{n=0}^{\infty}{a_n x^n}$.","I'm stuck with finding the recursion relation of this differential equation using the power series method. So I started by setting: $$y(x)=\sum\limits_{n=0}^{\infty}{a_n x^n} \\ e^x=\sum\limits_{n=0}^{\infty}{\dfrac{x^n}{n!}}$$ But when I implemented this in the equation, I got stuck with finding the recursion relation because of the double summation. This is what I have: $\sum\limits_{n=2}^\infty (n)(n-1) a_n x^{n-2}+\left(\sum\limits_{n=0}^\infty\dfrac{x^n}{n!}\right)\left(\sum\limits_{n=1}^\infty n a_nx^{n-1}\right) - \sum\limits_{n=0}^{\infty}{a_n x^n} =0$ I know there exist the Cauchy product but I don't know how to use it in this case. PS: I know it is also possible to set $y=\sum\limits_{n=0}^\infty\dfrac{a_nx^n}{n!}$ , but I chose to use $y(x)=\sum\limits_{n=0}^{\infty}{a_n x^n}$.",,"['ordinary-differential-equations', 'power-series']"
92,How to solve this differential equation of first order (non-linear)?,How to solve this differential equation of first order (non-linear)?,,"I'm stuck on this problem. $\frac { dy }{ dx } =\frac { 5{ x }^{ 3 }-x{ y }^{ 2 }-2x }{ 3{ x }^{ 2 }y-{ y }^{ 3 } } $ I tried taking x common from the numerator and y from the denominator and made the substitution $3{ x }^{ 2 }-{ y }^{ 2 }=t$ After differentiating wrt x , I got the following Differential Eqaution $\frac { dt }{ 4xdx } =\frac { t-{ x }^{ 2 }+1 }{ t } $ I don't know what to do. Any Suggestions?","I'm stuck on this problem. $\frac { dy }{ dx } =\frac { 5{ x }^{ 3 }-x{ y }^{ 2 }-2x }{ 3{ x }^{ 2 }y-{ y }^{ 3 } } $ I tried taking x common from the numerator and y from the denominator and made the substitution $3{ x }^{ 2 }-{ y }^{ 2 }=t$ After differentiating wrt x , I got the following Differential Eqaution $\frac { dt }{ 4xdx } =\frac { t-{ x }^{ 2 }+1 }{ t } $ I don't know what to do. Any Suggestions?",,"['calculus', 'ordinary-differential-equations']"
93,implicit differentiation when the implicit equation is a differential equation,implicit differentiation when the implicit equation is a differential equation,,"I want to find derivative/variation (I think variation is more correct) of solution of a differential equation with respect to one parameter of differential equation but without solving the differential equation explicitly. e.g. consider this differential equation $X'(t)=A X(t) + B(t)$, I need to find the derivative of solution with respect to A (assume a general parametric initial condition). Actually the problem arise from a constrained optimization problem where the differential equation is the constraint and the objective function is an explicit function of solution of differential equation and so implicit function of the differential equation parameters and I need to find the optimum parameters. The actual differential equation is wave PDE which is discretized by some method like finite difference (the second order wave PDE is considered as 2 simultaneous first order equation and after discretization 2 sets of equation stacked together), so the coefficient matrix or parameters are properties of medium where the wave propagates (local propagation speed of wave). Considering solving the optimization problem is the main goal, and the problem must be solved numerically, any approximation of this derivative in order to be used in a gradient descent algorithm or any other iterative solution is welcomed.","I want to find derivative/variation (I think variation is more correct) of solution of a differential equation with respect to one parameter of differential equation but without solving the differential equation explicitly. e.g. consider this differential equation $X'(t)=A X(t) + B(t)$, I need to find the derivative of solution with respect to A (assume a general parametric initial condition). Actually the problem arise from a constrained optimization problem where the differential equation is the constraint and the objective function is an explicit function of solution of differential equation and so implicit function of the differential equation parameters and I need to find the optimum parameters. The actual differential equation is wave PDE which is discretized by some method like finite difference (the second order wave PDE is considered as 2 simultaneous first order equation and after discretization 2 sets of equation stacked together), so the coefficient matrix or parameters are properties of medium where the wave propagates (local propagation speed of wave). Considering solving the optimization problem is the main goal, and the problem must be solved numerically, any approximation of this derivative in order to be used in a gradient descent algorithm or any other iterative solution is welcomed.",,"['ordinary-differential-equations', 'optimization', 'calculus-of-variations', 'implicit-differentiation', 'constraints']"
94,How to solve Cauchy problem for $y' + y\cos x = e^{-\sin x} \quad y(0) = 1$?,How to solve Cauchy problem for ?,y' + y\cos x = e^{-\sin x} \quad y(0) = 1,"I have: $$y' + y\cos x = e^{-\sin x} \quad y(0) = 1$$ Applying Lagrange's method I got: $$y' + y\cos x = 0 \\ \frac{dy}{dx} = -y\cos x \\ \int\frac{dy}{y} = -\int \cos x \, dx \\ \ln |y| = -\sin x +C \\ y = C\cdot e^{-\sin x}$$ $$y(x) = C(x)\cdot e^{-\sin x} \\ y'(x) = C'(x)e^{-\sin x} - C(x) e^{-\sin x} \cos x$$ substituting back into the original equation and simplifying I got: $$C'(x)e^{-\sin x} = e^{-\sin x}$$ and therefore $$C(x) = x + \alpha$$ and the final answer is: $$y = (x + \alpha)e^{-\sin x}$$ But how to solve Cauchy problem for the final answer?","I have: $$y' + y\cos x = e^{-\sin x} \quad y(0) = 1$$ Applying Lagrange's method I got: $$y' + y\cos x = 0 \\ \frac{dy}{dx} = -y\cos x \\ \int\frac{dy}{y} = -\int \cos x \, dx \\ \ln |y| = -\sin x +C \\ y = C\cdot e^{-\sin x}$$ $$y(x) = C(x)\cdot e^{-\sin x} \\ y'(x) = C'(x)e^{-\sin x} - C(x) e^{-\sin x} \cos x$$ substituting back into the original equation and simplifying I got: $$C'(x)e^{-\sin x} = e^{-\sin x}$$ and therefore $$C(x) = x + \alpha$$ and the final answer is: $$y = (x + \alpha)e^{-\sin x}$$ But how to solve Cauchy problem for the final answer?",,"['ordinary-differential-equations', 'cauchy-problem']"
95,Solve the eigenvalue problem $y''=\lambda y$ numerically,Solve the eigenvalue problem  numerically,y''=\lambda y,"I am trying to solve the eigenvalue problem  $$\begin{cases} y''=\lambda y \\y(0)=y(1)=0\end{cases}$$ I use the finite difference to discretize the ODE with BVs. I get the following equation $$\frac{y_{i-1}+2y_i+y_{i+1}}{(\Delta x)^2}=\lambda_{\Delta x}y_i,$$ where $\Delta x =1/{(n+1)}.$ Then I get the following $n \times n$ tridiagonal matrix formulation $\frac{1}{(\Delta x)^2}\begin{bmatrix}-2&1&~&~\\1&-2&1&~&~ \\~&~&\ddots\\  ~&~&1&-2&1\\~&~&~&1&-2\end{bmatrix}\begin{bmatrix}y_1 \\y_2\\\vdots\\ y_{n-1}\\y_n\end{bmatrix}=\lambda_{\Delta x}\begin{bmatrix}y_1 \\y_2\\\vdots\\ y_{n-1}\\y_n\end{bmatrix}$ I know that the ODE has infinitely many eigenvalue values and eigenfunction. We can compute the eigenvalue numerically by computing the eigenvalues of the matrix on the LHS. I have two questions. Why can I only get the approximation of the first $n$th eigenvalue ($n\times n $ matrix) instead of the kth to $(k+n-1)$th eigenvalues? If I want to compute the $m$th eigenvalue, do I have to compute the eigenvalues of the $m\times m$ matrix ? When $m$ becomes very large, it needs a lot of computations. Do we have numerical method to compute the $m$th eigenvalue directively?","I am trying to solve the eigenvalue problem  $$\begin{cases} y''=\lambda y \\y(0)=y(1)=0\end{cases}$$ I use the finite difference to discretize the ODE with BVs. I get the following equation $$\frac{y_{i-1}+2y_i+y_{i+1}}{(\Delta x)^2}=\lambda_{\Delta x}y_i,$$ where $\Delta x =1/{(n+1)}.$ Then I get the following $n \times n$ tridiagonal matrix formulation $\frac{1}{(\Delta x)^2}\begin{bmatrix}-2&1&~&~\\1&-2&1&~&~ \\~&~&\ddots\\  ~&~&1&-2&1\\~&~&~&1&-2\end{bmatrix}\begin{bmatrix}y_1 \\y_2\\\vdots\\ y_{n-1}\\y_n\end{bmatrix}=\lambda_{\Delta x}\begin{bmatrix}y_1 \\y_2\\\vdots\\ y_{n-1}\\y_n\end{bmatrix}$ I know that the ODE has infinitely many eigenvalue values and eigenfunction. We can compute the eigenvalue numerically by computing the eigenvalues of the matrix on the LHS. I have two questions. Why can I only get the approximation of the first $n$th eigenvalue ($n\times n $ matrix) instead of the kth to $(k+n-1)$th eigenvalues? If I want to compute the $m$th eigenvalue, do I have to compute the eigenvalues of the $m\times m$ matrix ? When $m$ becomes very large, it needs a lot of computations. Do we have numerical method to compute the $m$th eigenvalue directively?",,"['ordinary-differential-equations', 'numerical-methods', 'computational-mathematics']"
96,Solution of non-linear second order differential equation,Solution of non-linear second order differential equation,,"I am working with a system, which is described by the following differential equation $y'' = (1+y)^{3/2}$ Does this differential equation have a closed-form solution? And more generally: Is there a place, which contains a complete list of second-order differential equations, which do have analytical solutions?","I am working with a system, which is described by the following differential equation $y'' = (1+y)^{3/2}$ Does this differential equation have a closed-form solution? And more generally: Is there a place, which contains a complete list of second-order differential equations, which do have analytical solutions?",,['ordinary-differential-equations']
97,Limit of uniformly convergent sequence of recursive integrals is differentiable?,Limit of uniformly convergent sequence of recursive integrals is differentiable?,,"I have the sequence $(y_n)_{n\in\mathbb N_0}$ of functions $$y_n\colon [0,\alpha] \to \mathbb R$$ defined recursively by $$ y_{n+1}(x) = \int_0^x g\bigl(y_{n}(\xi)\bigr)\,d\xi,\qquad n\in\mathbb N $$ and the constant function $y_0(x)\equiv 0$. We know nothing about the function $g$, except that all those integrals always exist. Suppose we know that $(y_n)_{n\in\mathbb N}$ converges uniformly to a function $y^*$: $$\|y_n - y^*\|\to0,\qquad(n\to\infty)$$ where $\|\bullet\|$ is the supremum norm on $[0,\alpha]$: $$ \| y \| = \sup_{x\in [0,\alpha]} |y(x)|. $$ The question is: Can we prove that $y^*$ has to be differentiable? If I'd know that $y_n$ was always differentiable with $y_n'(x) = g\bigl(y_{n-1}(x)\bigr)$, then it would suffice to show that $(y_n')_{n\in\mathbb N}$ converges uniformely, but I don't really know anything about the convergence of $y_n'$, since I don't know anything about $g$? Would it be easier or at least doable if $g$ was continuous? This question arises, when you try to apply the Picard-iteration to the initial value problem $$ y'(x) = g\bigl(y(x)\bigr),\qquad y(0) = 0, $$ where one does not have Lipschitz-continuity of $g$. There is a similar exercise in the german standard textbook „Gewöhnliche Differentialgleichungen“ by Harro Heuser. It is Exercise III.12.5 in that book (at least in the fourth edition).","I have the sequence $(y_n)_{n\in\mathbb N_0}$ of functions $$y_n\colon [0,\alpha] \to \mathbb R$$ defined recursively by $$ y_{n+1}(x) = \int_0^x g\bigl(y_{n}(\xi)\bigr)\,d\xi,\qquad n\in\mathbb N $$ and the constant function $y_0(x)\equiv 0$. We know nothing about the function $g$, except that all those integrals always exist. Suppose we know that $(y_n)_{n\in\mathbb N}$ converges uniformly to a function $y^*$: $$\|y_n - y^*\|\to0,\qquad(n\to\infty)$$ where $\|\bullet\|$ is the supremum norm on $[0,\alpha]$: $$ \| y \| = \sup_{x\in [0,\alpha]} |y(x)|. $$ The question is: Can we prove that $y^*$ has to be differentiable? If I'd know that $y_n$ was always differentiable with $y_n'(x) = g\bigl(y_{n-1}(x)\bigr)$, then it would suffice to show that $(y_n')_{n\in\mathbb N}$ converges uniformely, but I don't really know anything about the convergence of $y_n'$, since I don't know anything about $g$? Would it be easier or at least doable if $g$ was continuous? This question arises, when you try to apply the Picard-iteration to the initial value problem $$ y'(x) = g\bigl(y(x)\bigr),\qquad y(0) = 0, $$ where one does not have Lipschitz-continuity of $g$. There is a similar exercise in the german standard textbook „Gewöhnliche Differentialgleichungen“ by Harro Heuser. It is Exercise III.12.5 in that book (at least in the fourth edition).",,"['integration', 'ordinary-differential-equations', 'analysis', 'uniform-convergence', 'recursion']"
98,"Stationary points and their behavior of : $x' = y-x^3, y' = -x + y^3$ via Linearisation",Stationary points and their behavior of :  via Linearisation,"x' = y-x^3, y' = -x + y^3","Find the stationary points of the following system and discuss their behavior using Linearisation $$x'=y-x^3,y'=-x+y^3$$ My solution and where I'm stuck : To find the stationary points, we'll solve the system : $$\begin{cases} y-x^3= 0\\ -x + y^3 =0 \end{cases}$$ which yields the following stationary points : $$A=(0,0)$$ $$B=(-1,-1)$$ $$C=(1,1)$$ To discuss their behavior, I'll start off by calculating the Jacobian of the non-linear system : $$J(x,y) = \begin{bmatrix} -3x^2 & 1 \\ -1 & 3y^2 \end{bmatrix}$$ Now, we'll calculate the eigenvalues of the Jacobian at each of the stationary points to make a case about their behavior and their character : $$J(0,0) = \begin{bmatrix} 0 & 1 \\ -1 & 0\end{bmatrix}$$ so, we get $\det(J(0,0)-λI)=λ^2 +1=0\Leftrightarrow λ = \pm i$ Now, this is exactly where I'm stuck. I know that generally, when we have purely imaginary eigenvalues, it means we have a center, but in a case of an almost linear system , the topological equality for centers does not hold between it and it's linearised system. That means, that I need a different condition to determine whether the point $A=(0,0)$ is a center or that if it is a focus/spiral for the almost-linear/non-linear system . I have been stuck on exercises that have purely imaginary eigenvalues for some days now, since I cannot find any examples related on our book or on the internet that will help me understand what I have to do in such cases (I'm Greek, so maybe I'm searching it in internet/global textbooks using wrong terminology). I think my question is straight forward as in previous questions, some people wondered what I was asking. I am asking exactly the following : What do I need to do to in order to discuss whether a stationary point that leads to purely imaginary eigenvalues on the linearised system/Jacobian, is either a center or a focus ? Please, I would really appreciate anyone that can tell me what I have to do in such cases since I've found myself stuck on numerous exercises, not knowing what I have to do and not being able to find anything myself via studying, searching or trying.","Find the stationary points of the following system and discuss their behavior using Linearisation My solution and where I'm stuck : To find the stationary points, we'll solve the system : which yields the following stationary points : To discuss their behavior, I'll start off by calculating the Jacobian of the non-linear system : Now, we'll calculate the eigenvalues of the Jacobian at each of the stationary points to make a case about their behavior and their character : so, we get Now, this is exactly where I'm stuck. I know that generally, when we have purely imaginary eigenvalues, it means we have a center, but in a case of an almost linear system , the topological equality for centers does not hold between it and it's linearised system. That means, that I need a different condition to determine whether the point is a center or that if it is a focus/spiral for the almost-linear/non-linear system . I have been stuck on exercises that have purely imaginary eigenvalues for some days now, since I cannot find any examples related on our book or on the internet that will help me understand what I have to do in such cases (I'm Greek, so maybe I'm searching it in internet/global textbooks using wrong terminology). I think my question is straight forward as in previous questions, some people wondered what I was asking. I am asking exactly the following : What do I need to do to in order to discuss whether a stationary point that leads to purely imaginary eigenvalues on the linearised system/Jacobian, is either a center or a focus ? Please, I would really appreciate anyone that can tell me what I have to do in such cases since I've found myself stuck on numerous exercises, not knowing what I have to do and not being able to find anything myself via studying, searching or trying.","x'=y-x^3,y'=-x+y^3 \begin{cases} y-x^3= 0\\ -x + y^3 =0 \end{cases} A=(0,0) B=(-1,-1) C=(1,1) J(x,y) = \begin{bmatrix} -3x^2 & 1 \\ -1 & 3y^2 \end{bmatrix} J(0,0) = \begin{bmatrix} 0 & 1 \\ -1 & 0\end{bmatrix} \det(J(0,0)-λI)=λ^2 +1=0\Leftrightarrow λ = \pm i A=(0,0)","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'nonlinear-system', 'stability-in-odes']"
99,Laplace-Young equation,Laplace-Young equation,,"Recall the Laplace-Young equation governing the shape $z=h(x)$ of a meniscus near a vertical planar all   $$ \ell_c^2\frac{h_{xx}}{(1 + h_x^2)^{3/2}} = h $$   Integrating the full Laplace-Young equation once (without making an assumption of shallow slopes), show that the rise height of the meniscus on the all, $h_0$ is given in terms of the contact angle of the liquid $\theta$ by   $$ h_0 = \pm\ell_c[2(1 - \sin\theta)]^{1/2} $$   and discuss when each of the $\pm$ branches of the result are appropriate. Show that the total area displaced by the meniscus is given by   $$ A = \int_{0}^{\infty}h~{\rm d}x = \ell_c^2\cos\theta $$   and comment on this result in the light of the generalized Archimedes' principle discussed in lecture 3 I've done all of the question except the integral at the end. I have $h$ in terms of $h_x$ and so I have no idea how to go about integrating this","Recall the Laplace-Young equation governing the shape $z=h(x)$ of a meniscus near a vertical planar all   $$ \ell_c^2\frac{h_{xx}}{(1 + h_x^2)^{3/2}} = h $$   Integrating the full Laplace-Young equation once (without making an assumption of shallow slopes), show that the rise height of the meniscus on the all, $h_0$ is given in terms of the contact angle of the liquid $\theta$ by   $$ h_0 = \pm\ell_c[2(1 - \sin\theta)]^{1/2} $$   and discuss when each of the $\pm$ branches of the result are appropriate. Show that the total area displaced by the meniscus is given by   $$ A = \int_{0}^{\infty}h~{\rm d}x = \ell_c^2\cos\theta $$   and comment on this result in the light of the generalized Archimedes' principle discussed in lecture 3 I've done all of the question except the integral at the end. I have $h$ in terms of $h_x$ and so I have no idea how to go about integrating this",,"['calculus', 'integration', 'ordinary-differential-equations', 'physics', 'fluid-dynamics']"
