,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Let $(a_n)$ be a sequence of real numbers. Suppose each of the subsequences $(a_{2n}),(a_{2n+1})$ and $(a_{3n})$ converges to $p,q$ and $r$",Let  be a sequence of real numbers. Suppose each of the subsequences  and  converges to  and,"(a_n) (a_{2n}),(a_{2n+1}) (a_{3n}) p,q r","Let $(a_n)$ be a sequence of real numbers. Suppose each of the subsequences $(a_{2n}),(a_{2n+1})$ and $(a_{3n})$ converges to $p,q$ and $r$, respectively. Show that $p=r$ and $q=r$ and hence conclude that $(a_n)$ converges. To prove $p=r$, I consider a subsequence of $(a_{2n})$ and $(a_{3n})$ , which is $(a_{6n})$. Then I say clearly $(a_{6n})$ converges to $p$ and $r$ and hence $p=r$. Is this proof work?","Let $(a_n)$ be a sequence of real numbers. Suppose each of the subsequences $(a_{2n}),(a_{2n+1})$ and $(a_{3n})$ converges to $p,q$ and $r$, respectively. Show that $p=r$ and $q=r$ and hence conclude that $(a_n)$ converges. To prove $p=r$, I consider a subsequence of $(a_{2n})$ and $(a_{3n})$ , which is $(a_{6n})$. Then I say clearly $(a_{6n})$ converges to $p$ and $r$ and hence $p=r$. Is this proof work?",,"['real-analysis', 'sequences-and-series']"
1,"Example of an increasing, integrable function $f:[0,1]\to\mathbb{R}$ which is discontinuous at all rationals?","Example of an increasing, integrable function  which is discontinuous at all rationals?","f:[0,1]\to\mathbb{R}","I have really no idea about this: Problem : Show that there exists a function $f:[0,1]\rightarrow\mathbb{R}$ such that: $f$ is discontinuous in all $x\in \mathbb Q$ . $f$ is increasing in $[0,1]$ . $f$ is integrable. EDIT : Sorry, it is not discontinuous in all $x\in \mathbb R \setminus \mathbb Q$ , just in $\mathbb Q$ .","I have really no idea about this: Problem : Show that there exists a function such that: is discontinuous in all . is increasing in . is integrable. EDIT : Sorry, it is not discontinuous in all , just in .","f:[0,1]\rightarrow\mathbb{R} f x\in \mathbb Q f [0,1] f x\in \mathbb R \setminus \mathbb Q \mathbb Q","['real-analysis', 'integration', 'examples-counterexamples']"
2,Checking of a solution to How to show that $\lim \sup a_nb_n=ab$,Checking of a solution to How to show that,\lim \sup a_nb_n=ab,"In course of solving the problem How to show that $\lim \sup a_nb_n=ab$ I feel that I've probably made some mistake in my solution for I didn't use the fact that $a_n>0$ $\forall$ $n\geq1$. The statement of the problem is: Let $a_n,b_n\in\mathbb R^+$ such that $\lim a_n=a>0$ and $\lim \sup b_n=b>0$. Show that $\lim \sup a_nb_n=ab.$ Please help me to find out the mistake I've made: $\exists$ a subsequence $\{a_{r_n}\}$ of $\{a_n\}$ such that $a_{r_n}\to a.$ Now $a_{r_n}\to a, b_{r_n}\to b\implies a_{r_n}b_{r_n}\to ab\implies ab$ is a subsequencial limit of {$a_nb_n$}. If possible let $\exists$ a subsequence $\{a_{p_n}b_{p_n}\}$ of $\{a_nb_n\}$ such that $a_{p_n}b_{p_n}\to m>ab.$ Since $b_n,b>0$ so $b_n^{-1}\to b^{-1}>0$ whence $a_{r_n}\to mb^{-1}>a,$ a contradiction to $\lim \sup a_n=a.$ Hence the result follows. Thanks for voting up the question. But that didn't actually eliminate my confusion. I'm looking for some concrete comments and opinions.","In course of solving the problem How to show that $\lim \sup a_nb_n=ab$ I feel that I've probably made some mistake in my solution for I didn't use the fact that $a_n>0$ $\forall$ $n\geq1$. The statement of the problem is: Let $a_n,b_n\in\mathbb R^+$ such that $\lim a_n=a>0$ and $\lim \sup b_n=b>0$. Show that $\lim \sup a_nb_n=ab.$ Please help me to find out the mistake I've made: $\exists$ a subsequence $\{a_{r_n}\}$ of $\{a_n\}$ such that $a_{r_n}\to a.$ Now $a_{r_n}\to a, b_{r_n}\to b\implies a_{r_n}b_{r_n}\to ab\implies ab$ is a subsequencial limit of {$a_nb_n$}. If possible let $\exists$ a subsequence $\{a_{p_n}b_{p_n}\}$ of $\{a_nb_n\}$ such that $a_{p_n}b_{p_n}\to m>ab.$ Since $b_n,b>0$ so $b_n^{-1}\to b^{-1}>0$ whence $a_{r_n}\to mb^{-1}>a,$ a contradiction to $\lim \sup a_n=a.$ Hence the result follows. Thanks for voting up the question. But that didn't actually eliminate my confusion. I'm looking for some concrete comments and opinions.",,"['real-analysis', 'sequences-and-series', 'limsup-and-liminf']"
3,"Convergence of $\sum_{n=2}^{\infty}\frac{\sqrt{a_{n}}}{\ln\, n}(n^{a_{n}}-1)$",Convergence of,"\sum_{n=2}^{\infty}\frac{\sqrt{a_{n}}}{\ln\, n}(n^{a_{n}}-1)","If $\sum_{n=2}^{\infty}a_n$ converge, then also converge this series? $$\sum_{n=2}^{\infty}\frac{\sqrt{a_{n}}}{\ln\, n}(n^{a_{n}}-1)$$ Please verify my answer below Counterexample: $$a_{n}=\begin{cases} \frac{1}{k^{2}} & n=k!^{k^{2}}\\ \\ 0 & \text{All other cases} \end{cases}$$ When our infinites sum is equal to $$\sum_{k=1}^{\infty}\frac{1}{k}\cdot\frac{1}{2\, \ln\, k}\cdot(k!-1)$$","If $\sum_{n=2}^{\infty}a_n$ converge, then also converge this series? $$\sum_{n=2}^{\infty}\frac{\sqrt{a_{n}}}{\ln\, n}(n^{a_{n}}-1)$$ Please verify my answer below Counterexample: $$a_{n}=\begin{cases} \frac{1}{k^{2}} & n=k!^{k^{2}}\\ \\ 0 & \text{All other cases} \end{cases}$$ When our infinites sum is equal to $$\sum_{k=1}^{\infty}\frac{1}{k}\cdot\frac{1}{2\, \ln\, k}\cdot(k!-1)$$",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
4,Self-maps of the Cantor set,Self-maps of the Cantor set,,"Let $X$ and $Y$ be homeomorphic to the Cantor set and pick $x_0\in X$. Suppose $f\colon X\to Y$ is a continuous function such that $f\upharpoonright X\setminus\{x_0\}$ is injective. Must $f$ be injective? Well, $X$ is disconnected so we can't apply the Darboux property directly.","Let $X$ and $Y$ be homeomorphic to the Cantor set and pick $x_0\in X$. Suppose $f\colon X\to Y$ is a continuous function such that $f\upharpoonright X\setminus\{x_0\}$ is injective. Must $f$ be injective? Well, $X$ is disconnected so we can't apply the Darboux property directly.",,"['real-analysis', 'general-topology', 'continuity']"
5,Continuous function taking rationals to rationals,Continuous function taking rationals to rationals,,"Is there a continuous increasing function $ f : [0, \pi] \to [0, e] $ such that $ f(0) = 0, f(\pi) = e $ and $ f (q ) \in \mathbb{Q} $ for $ q \in \mathbb{Q} $ and $ f (q ) \in \mathbb{Q}^c $ for $ q \in \mathbb{Q}^c $? I think there should be, but I am unable to construct one.","Is there a continuous increasing function $ f : [0, \pi] \to [0, e] $ such that $ f(0) = 0, f(\pi) = e $ and $ f (q ) \in \mathbb{Q} $ for $ q \in \mathbb{Q} $ and $ f (q ) \in \mathbb{Q}^c $ for $ q \in \mathbb{Q}^c $? I think there should be, but I am unable to construct one.",,['real-analysis']
6,why is the Cantor-Lebesgue function increasing,why is the Cantor-Lebesgue function increasing,,"I am trying to derive the fact that the function $F$ defined below is monotonically increasing. The only thing I can use is that the any member of the Cantor set has a ternary expansion involving only $0$'s and $2$'s. The function is defined as follows: Let $x\in[0,1]$ have ternary expansion $0.a_1a_2\cdots$. Define $N$ as the first index $n$ for which $a_n=1$ and set $N=\infty$ if none of the $a_n$ are $1$. Now let $F(x)=\sum_{n=1}^{N-1}\frac{a_n}{2^{n+1}}+\frac{1}{2^N}$. I have shown that $F$ is constant while on a particular middle third removed in the construction of the Cantor set $C$ so I am really interested in showing the increasing nature on $C$. Essentially therefore I wish to show that given $x=\sum \frac{a_n}{3^n}<\sum\frac{b_n}{3^n}=y$ we have $\sum\frac{a_n}{2^{n+1}}<\sum\frac{b_n}{2^{n+1}}$ on $C$. How do I establish that? Thanks","I am trying to derive the fact that the function $F$ defined below is monotonically increasing. The only thing I can use is that the any member of the Cantor set has a ternary expansion involving only $0$'s and $2$'s. The function is defined as follows: Let $x\in[0,1]$ have ternary expansion $0.a_1a_2\cdots$. Define $N$ as the first index $n$ for which $a_n=1$ and set $N=\infty$ if none of the $a_n$ are $1$. Now let $F(x)=\sum_{n=1}^{N-1}\frac{a_n}{2^{n+1}}+\frac{1}{2^N}$. I have shown that $F$ is constant while on a particular middle third removed in the construction of the Cantor set $C$ so I am really interested in showing the increasing nature on $C$. Essentially therefore I wish to show that given $x=\sum \frac{a_n}{3^n}<\sum\frac{b_n}{3^n}=y$ we have $\sum\frac{a_n}{2^{n+1}}<\sum\frac{b_n}{2^{n+1}}$ on $C$. How do I establish that? Thanks",,['real-analysis']
7,To show Taylor series of a Fourier transform $\hat{f }$ converges to $\hat{f}$,To show Taylor series of a Fourier transform  converges to,\hat{f } \hat{f},"I got some trouble with the following question. Say $f$ is in  $L^1(R)$  with compact support . I need to show (1) $\hat{f(\zeta)}$ is infinitely differentiable and all derivatives are continuous. (2) show Taylor series of $\hat{f(\zeta)}$ at $\zeta$ = 0 has infinitely radius of convergence , and converges to $\hat{f}$ (i.e. $\hat{f}$ is analytic and entire). I did part (1) and showed that radius of convergence of the Taylor series is infinity. But stuck at the part to show the TS does converge to $\hat{f}$. by calculation TS = $\sum_{n=0}^{\infty}\hat{f^n(0)}\zeta^n/n! = (-i)^n\int_{-M}^Mx^nf(x)dx\zeta^n/n!$. So $a_n= (-i)^n\int_{-M}^Mx^nf(x)/n!$. I used root test to show R is infinity. I then managed to show the error term $R(N):=\hat{f}-TS(N)=\hat{f}-\sum_{n=0}^{N}\hat{f^n(0)}\zeta^n/n! \to 0$ as $ N\to\infty$ but kind of stuck at here. Any thoughts ? Thanks in advance.","I got some trouble with the following question. Say $f$ is in  $L^1(R)$  with compact support . I need to show (1) $\hat{f(\zeta)}$ is infinitely differentiable and all derivatives are continuous. (2) show Taylor series of $\hat{f(\zeta)}$ at $\zeta$ = 0 has infinitely radius of convergence , and converges to $\hat{f}$ (i.e. $\hat{f}$ is analytic and entire). I did part (1) and showed that radius of convergence of the Taylor series is infinity. But stuck at the part to show the TS does converge to $\hat{f}$. by calculation TS = $\sum_{n=0}^{\infty}\hat{f^n(0)}\zeta^n/n! = (-i)^n\int_{-M}^Mx^nf(x)dx\zeta^n/n!$. So $a_n= (-i)^n\int_{-M}^Mx^nf(x)/n!$. I used root test to show R is infinity. I then managed to show the error term $R(N):=\hat{f}-TS(N)=\hat{f}-\sum_{n=0}^{N}\hat{f^n(0)}\zeta^n/n! \to 0$ as $ N\to\infty$ but kind of stuck at here. Any thoughts ? Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'analysis', 'fourier-analysis']"
8,Stone-Weierstrass theorem proof (Rudin),Stone-Weierstrass theorem proof (Rudin),,I am reading Rudin's proof (3rd edition) and am wondering what substitution is made to make it true that $P_n(x)=$ the integral from $-x$ to $1-x$ is equal to the same function integrated from -1 to 1. He says there's a substitution but I haven't found the right one. Thanks a lot,I am reading Rudin's proof (3rd edition) and am wondering what substitution is made to make it true that $P_n(x)=$ the integral from $-x$ to $1-x$ is equal to the same function integrated from -1 to 1. He says there's a substitution but I haven't found the right one. Thanks a lot,,['real-analysis']
9,Representation of smooth function,Representation of smooth function,,"Is it true that any smooth function $f\colon \mathbb{R}^n \to \mathbb{R}^n$ can be represented as $$    f(x) = \nabla U(x) + g(x) $$ where $U(x)$ is a scalar function and $\langle g(x), f(x) \rangle \equiv 0$? Is this representation unique?","Is it true that any smooth function $f\colon \mathbb{R}^n \to \mathbb{R}^n$ can be represented as $$    f(x) = \nabla U(x) + g(x) $$ where $U(x)$ is a scalar function and $\langle g(x), f(x) \rangle \equiv 0$? Is this representation unique?",,"['calculus', 'real-analysis', 'functions']"
10,Determining the Length of a Curve Using Partitions,Determining the Length of a Curve Using Partitions,,"I have encountered the following problem: Let $f$ be continuous on $[a,b]$. Define the length of $f$ on $[a,b]$ by   $$l=\sup_P[\lambda_P(f)],$$   where   $$\lambda_P(f)=\sum_{k=1}^N\sqrt{(x_k-x_{k-1})^2+(f(x_k)-f(x_{k-1}))^2},$$   and the supremum is taken over all partitions $P=\{a=x_0<x_1<\cdots<x_N=b\}$ of $[a,b]$. Show that $\lambda_P(f)\leqslant\lambda_Q(f)$ for any refinement $Q$ of $P$. Then, show that there is a sequence $(P_n)_{n=1}^\infty$ such that   $$l=\lim_{n\to\infty}\lambda_{P_n}(f).$$ This is what I have done: Let $Q\supseteq P$. If $Q=P$, then $\lambda_Q(f)=\lambda_P(f)$. If $Q\neq P$, then it must be the case that there is at least one $c\in Q$ such that $c\notin P$. This implies that $\lambda_Q(f)$ will have at least one more sum than $\lambda_P(f)$, and because distance is a non-negative value, we must have that $\lambda_P(f)\leqslant\lambda_Q(f)$. Moreover, take the sequence $(P_n)_{n=1}^\infty=P_1\supset P_2\supset\cdots$. Then, from above, $\lambda_{P_1}(f)\leqslant\lambda_{P_2}(f)\leqslant\cdots$. Hence, if the limit exists, we must have that $$l=\lim_{n\to\infty}\lambda_{P_n}(f).$$ Does this seem reasonable?","I have encountered the following problem: Let $f$ be continuous on $[a,b]$. Define the length of $f$ on $[a,b]$ by   $$l=\sup_P[\lambda_P(f)],$$   where   $$\lambda_P(f)=\sum_{k=1}^N\sqrt{(x_k-x_{k-1})^2+(f(x_k)-f(x_{k-1}))^2},$$   and the supremum is taken over all partitions $P=\{a=x_0<x_1<\cdots<x_N=b\}$ of $[a,b]$. Show that $\lambda_P(f)\leqslant\lambda_Q(f)$ for any refinement $Q$ of $P$. Then, show that there is a sequence $(P_n)_{n=1}^\infty$ such that   $$l=\lim_{n\to\infty}\lambda_{P_n}(f).$$ This is what I have done: Let $Q\supseteq P$. If $Q=P$, then $\lambda_Q(f)=\lambda_P(f)$. If $Q\neq P$, then it must be the case that there is at least one $c\in Q$ such that $c\notin P$. This implies that $\lambda_Q(f)$ will have at least one more sum than $\lambda_P(f)$, and because distance is a non-negative value, we must have that $\lambda_P(f)\leqslant\lambda_Q(f)$. Moreover, take the sequence $(P_n)_{n=1}^\infty=P_1\supset P_2\supset\cdots$. Then, from above, $\lambda_{P_1}(f)\leqslant\lambda_{P_2}(f)\leqslant\cdots$. Hence, if the limit exists, we must have that $$l=\lim_{n\to\infty}\lambda_{P_n}(f).$$ Does this seem reasonable?",,['real-analysis']
11,Properly Defining a Smooth Curve,Properly Defining a Smooth Curve,,"I have seen many different definitions of what it means for a curve to be ""smooth"". In this question , for instance,  a curve $\gamma \colon [a,b] \longrightarrow \mathbb{R^n}$ is defined to be smooth if all derivatives exist and are continuous. This seems reasonable and in-line with what it means for any manifold to be smooth. See, however, Edwards Calculus of Several Variables where a smooth curve is defined (actually, he uses the word ""path""):A curve $\gamma \colon [a,b] \longrightarrow \mathbb{R^n}$  is said to be smooth if the derivative $\gamma^{\prime}(t)$ exists, is continuous and if $\gamma^{\prime}(t) \neq 0 \;\; \forall t \in [a,b]$. Ignoring the fact that Edwards is only concerned with $C^{1}$ curves as opposed to $C^{\infty}$ curves, that this definition requires the derivative to be nonzero obviously makes it different from the first definition. Now, consider yet another definition, this time from Wade's Introduction to Analysis (3rd Edition): A subset $C$ of $\mathbb{R}^m$ is called a $C^p$ curve in $\mathbb{R}^m$ if and only if there is a nondegenerate interval $I$ and a $C^p$ function $\gamma \colon I \longrightarrow \mathbb{R}^m$ that is injective on the interior of $I$ and $C = \gamma(I)$ Ignoring the fact that Wade is defining a curve as a set of points instead of its parametrization, this definition also differs from the first two in that $\gamma$ is required to be injective. So, finally, here is my question: Is there a definition, or set of definitions, that could bring some consistency to this terminology? Perhaps there's a special name for a ""curve"" that is ""injective"" in addition to being ""smooth""? Or, maybe there's a special name for a curve that never evaluates to the $0$-vector anywhere on it's domain? It seems to me that these are different attributes and should probably have distinctive nomenclature.","I have seen many different definitions of what it means for a curve to be ""smooth"". In this question , for instance,  a curve $\gamma \colon [a,b] \longrightarrow \mathbb{R^n}$ is defined to be smooth if all derivatives exist and are continuous. This seems reasonable and in-line with what it means for any manifold to be smooth. See, however, Edwards Calculus of Several Variables where a smooth curve is defined (actually, he uses the word ""path""):A curve $\gamma \colon [a,b] \longrightarrow \mathbb{R^n}$  is said to be smooth if the derivative $\gamma^{\prime}(t)$ exists, is continuous and if $\gamma^{\prime}(t) \neq 0 \;\; \forall t \in [a,b]$. Ignoring the fact that Edwards is only concerned with $C^{1}$ curves as opposed to $C^{\infty}$ curves, that this definition requires the derivative to be nonzero obviously makes it different from the first definition. Now, consider yet another definition, this time from Wade's Introduction to Analysis (3rd Edition): A subset $C$ of $\mathbb{R}^m$ is called a $C^p$ curve in $\mathbb{R}^m$ if and only if there is a nondegenerate interval $I$ and a $C^p$ function $\gamma \colon I \longrightarrow \mathbb{R}^m$ that is injective on the interior of $I$ and $C = \gamma(I)$ Ignoring the fact that Wade is defining a curve as a set of points instead of its parametrization, this definition also differs from the first two in that $\gamma$ is required to be injective. So, finally, here is my question: Is there a definition, or set of definitions, that could bring some consistency to this terminology? Perhaps there's a special name for a ""curve"" that is ""injective"" in addition to being ""smooth""? Or, maybe there's a special name for a curve that never evaluates to the $0$-vector anywhere on it's domain? It seems to me that these are different attributes and should probably have distinctive nomenclature.",,"['real-analysis', 'differential-geometry']"
12,A question about Darboux functions [duplicate],A question about Darboux functions [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: A converse of sorts to the intermediate value theorem, with an additional property Definition of Darboux function: Let $S\subset\mathbb{R}$ be given. We say that $f:S\rightarrow\mathbb{R}$ is a Darboux function if it possesses the following property: for each $a,b\in S$, such that $a<b$, and for each $y_0\in(c,d)$ (where $c=\mathrm{min}\{f(a),f(b)\}$ and $d=\mathrm{max}\{f(a),f(b)\}$) there exists an $x_0\in S\cap (a,b)$ such that $f(x_0)=y_0$. My problem is this: Suppose that $f:\mathbb{R}\rightarrow\mathbb{R}$ is a Darboux function. Suppose also that $f^{-1}(\{q\})$ is closed for each $q\in\mathbb{Q}$. I have to prove that $f$ is continuous on $\mathbb{R}$. Could you help me with this problem, please?","This question already has answers here : Closed 11 years ago . Possible Duplicate: A converse of sorts to the intermediate value theorem, with an additional property Definition of Darboux function: Let $S\subset\mathbb{R}$ be given. We say that $f:S\rightarrow\mathbb{R}$ is a Darboux function if it possesses the following property: for each $a,b\in S$, such that $a<b$, and for each $y_0\in(c,d)$ (where $c=\mathrm{min}\{f(a),f(b)\}$ and $d=\mathrm{max}\{f(a),f(b)\}$) there exists an $x_0\in S\cap (a,b)$ such that $f(x_0)=y_0$. My problem is this: Suppose that $f:\mathbb{R}\rightarrow\mathbb{R}$ is a Darboux function. Suppose also that $f^{-1}(\{q\})$ is closed for each $q\in\mathbb{Q}$. I have to prove that $f$ is continuous on $\mathbb{R}$. Could you help me with this problem, please?",,"['real-analysis', 'analysis', 'functions']"
13,Absolute continuity and sets of measure 0,Absolute continuity and sets of measure 0,,"I have a problem with a statement in Rudin's book ""Real and Complex Analysis"" (3rd edition) - proof of Theorem 7.18 Let $f:[a,b] \mapsto \mathbb{R}$ continuous non decreasing. If $f$ maps sets of measure $0$ to sets of measure $0$, then the function $g(x) = x + f(x)$ satisfies the same property. Rudin just states that it is a trivial consequence of the equality $m(g(I))=m(I)+m(f(I))$ for any interval $I$, with $m$ the Lebesgue measure. I can prove it if $f$ is increasing as follows: If $A$ is measurable s.t. $m(A)=0$ ($m$ is the Lebesgue measure), then we can find a decreasing sequence of open sets $(O_n)$ such that $A \subset O_n$ and $m(O_n)\downarrow_n 0$. If $O=\bigcap_{n=1}^\infty O_n$, then $m(g(A)) \leq m(g(O)) = \lim_n m(g(O_n))=\lim_n \left(m(O_n) + m(f(O_n))\right)=m(O) + m(f(O))=0$ since in this special case $\bigcap_{n=1}^\infty f(O_n)=f(O)$. Otherwise, if $f$ is only non-decreasing, I am not able to prove that $m(f(O))=\lim_nm(f(O_n))=0$. Thank you for your help!","I have a problem with a statement in Rudin's book ""Real and Complex Analysis"" (3rd edition) - proof of Theorem 7.18 Let $f:[a,b] \mapsto \mathbb{R}$ continuous non decreasing. If $f$ maps sets of measure $0$ to sets of measure $0$, then the function $g(x) = x + f(x)$ satisfies the same property. Rudin just states that it is a trivial consequence of the equality $m(g(I))=m(I)+m(f(I))$ for any interval $I$, with $m$ the Lebesgue measure. I can prove it if $f$ is increasing as follows: If $A$ is measurable s.t. $m(A)=0$ ($m$ is the Lebesgue measure), then we can find a decreasing sequence of open sets $(O_n)$ such that $A \subset O_n$ and $m(O_n)\downarrow_n 0$. If $O=\bigcap_{n=1}^\infty O_n$, then $m(g(A)) \leq m(g(O)) = \lim_n m(g(O_n))=\lim_n \left(m(O_n) + m(f(O_n))\right)=m(O) + m(f(O))=0$ since in this special case $\bigcap_{n=1}^\infty f(O_n)=f(O)$. Otherwise, if $f$ is only non-decreasing, I am not able to prove that $m(f(O))=\lim_nm(f(O_n))=0$. Thank you for your help!",,['real-analysis']
14,Continuity of a series of functions,Continuity of a series of functions,,"$(a_n)$ a sequence of continuous function $a_n:\mathbb{R}\rightarrow\mathbb{R}$. Non-increasing, non-negative. If $\sum a_n(x)$ convergers for all $x$ then the function $g(x)=\sum a_n(x)$ is continuous. EDIT: Call $g_n(x)=\sum_{k=0}^n a_n(x)$, this is continuous and $g_n(x)\rightarrow g(x)$. $|g(x)-g(x_0)|\leq|g(x)-g_n(x)|+|g_n(x)-g_n(x_0)|+|g_n(x_0)-g(x_0)|$. The second addend is less than $\varepsilon$ if $|x-x_0|\leq\delta$, I want to estimate the first and third addend. Now $|g(x)-g_n(x)|\leq\sum_{k=n+1}^\infty a_k(x)$ and because the series is convergent this is less that $\varepsilon$ if $n>n_1$. The same for the third addend if $n>n_2$. So if we take $N=\mathrm{max}\{{n_1,n_2}\}$ then we have the thesis. There sould be something wrong because I'm not using the hypothesis that the $a_n$ are non-increasing, could you help me to find the mistake?","$(a_n)$ a sequence of continuous function $a_n:\mathbb{R}\rightarrow\mathbb{R}$. Non-increasing, non-negative. If $\sum a_n(x)$ convergers for all $x$ then the function $g(x)=\sum a_n(x)$ is continuous. EDIT: Call $g_n(x)=\sum_{k=0}^n a_n(x)$, this is continuous and $g_n(x)\rightarrow g(x)$. $|g(x)-g(x_0)|\leq|g(x)-g_n(x)|+|g_n(x)-g_n(x_0)|+|g_n(x_0)-g(x_0)|$. The second addend is less than $\varepsilon$ if $|x-x_0|\leq\delta$, I want to estimate the first and third addend. Now $|g(x)-g_n(x)|\leq\sum_{k=n+1}^\infty a_k(x)$ and because the series is convergent this is less that $\varepsilon$ if $n>n_1$. The same for the third addend if $n>n_2$. So if we take $N=\mathrm{max}\{{n_1,n_2}\}$ then we have the thesis. There sould be something wrong because I'm not using the hypothesis that the $a_n$ are non-increasing, could you help me to find the mistake?",,"['real-analysis', 'analysis']"
15,Complement of the Cantor Set,Complement of the Cantor Set,,"I am looking at the complement of the cantor set in $[0,1]$ as the union of open intervals of decreasing length $\displaystyle\bigcup_{i=1}^{\infty}A_i$ where $A_1 = (\frac{1}{3},\frac{2}{3}), A_2 = (\frac{1}{9},\frac{2}{9})\cup(\frac{7}{9},\frac{8}{9})$ and so on. I am trying to prove that $\overline{\displaystyle\bigcup_{i=1}^{\infty}A_i} = \displaystyle\bigcup_{i=1}^{\infty}\overline{A_i}$. I know that this is not true in general and is probably not true in this case, but can someone give a proof or a counterexample Thank you very much","I am looking at the complement of the cantor set in $[0,1]$ as the union of open intervals of decreasing length $\displaystyle\bigcup_{i=1}^{\infty}A_i$ where $A_1 = (\frac{1}{3},\frac{2}{3}), A_2 = (\frac{1}{9},\frac{2}{9})\cup(\frac{7}{9},\frac{8}{9})$ and so on. I am trying to prove that $\overline{\displaystyle\bigcup_{i=1}^{\infty}A_i} = \displaystyle\bigcup_{i=1}^{\infty}\overline{A_i}$. I know that this is not true in general and is probably not true in this case, but can someone give a proof or a counterexample Thank you very much",,"['real-analysis', 'analysis']"
16,How to conclude the non-existence of limit,How to conclude the non-existence of limit,,"Let $f(x, y) = \dfrac{x^n + y^n}{x^2 - y^2}$ , $n > 2$ , be defined in $U = \left\{(x, y) \in \mathbb{R}^2 : y \neq x, y \neq -x\right\}$ . My problem is to determine the existence of $\lim\limits_{\substack{(x, y) \rightarrow (0, 0) \\ (x, y) \in U}} f(x, y)$ . The solution offered by a professor is the following: Let $V = \left\{(x, x + x^p) : x > 0\right\}$ , $p > n - 1$ . If $(x, y) \in V$ then $$f(x, y) = f\left(x, x + x^p\right) = \dfrac{x^n + \left(x + x^p\right)^n}{x^2 - \left(x + x^p\right)^2} = -\dfrac{x^n + x^n + nx^{n - 1 + p} + \cdots + x^{np}}{2x^{p + 1} + x^{2p}}.$$ As $p > n - 1$ , if $x \rightarrow 0$ then $f\left(x, x + x^p\right) \rightarrow \infty$ . Thus, $f$ is not convergent when $(x, y) \rightarrow (0, 0)$ in $U$ . My question is how could I come up with this idea. Firstly, I'd have to think about the non-existence instead of the existence, and still I wouldn't know how to get to the set $V$ in order to prove it. Moreover, is there another way to do it?","Let , , be defined in . My problem is to determine the existence of . The solution offered by a professor is the following: Let , . If then As , if then . Thus, is not convergent when in . My question is how could I come up with this idea. Firstly, I'd have to think about the non-existence instead of the existence, and still I wouldn't know how to get to the set in order to prove it. Moreover, is there another way to do it?","f(x, y) = \dfrac{x^n + y^n}{x^2 - y^2} n > 2 U = \left\{(x, y) \in \mathbb{R}^2 : y \neq x, y \neq -x\right\} \lim\limits_{\substack{(x, y) \rightarrow (0, 0) \\ (x, y) \in U}} f(x, y) V = \left\{(x, x + x^p) : x > 0\right\} p > n - 1 (x, y) \in V f(x, y) = f\left(x, x + x^p\right) = \dfrac{x^n + \left(x + x^p\right)^n}{x^2 - \left(x + x^p\right)^2} = -\dfrac{x^n + x^n + nx^{n - 1 + p} + \cdots + x^{np}}{2x^{p + 1} + x^{2p}}. p > n - 1 x \rightarrow 0 f\left(x, x + x^p\right) \rightarrow \infty f (x, y) \rightarrow (0, 0) U V","['real-analysis', 'calculus', 'limits', 'functions', 'convergence-divergence']"
17,Limit of continuous functions is Riemann integrable,Limit of continuous functions is Riemann integrable,,"Here is an analysis problem I'm stuck on: Let $f\in C^0([0,1])$ with $f(0)=0$ and $f$ increasing and convex. Define: $$ f_n(x) = n\big[f(x)-f(x-\tfrac{1}{n})\big] $$ Show: $f(1-\tfrac{1}{n})\le\int_0^1f_n(x)\ dx\le f(1)$ There is a $g$ with $f_n(x)\rightarrow g(x)$ almost everywhere $\int_0^1 g(x)\ dx = f(1)$ I've been able to do part 1, and I believe I can do part 2. The idea is that convexity implies $f_n(x)\ge f_m(x)$ when $n\ge m$ [I call this pointwise monotonicity below], and I use that to show $\{f_n\}$ is a Cauchy sequence in $L^1([0,1])$ . There is thus a $g\in L^1([0,1])$ with $f_n\rightarrow g$ almost everywhere, and the pointwise monotonicity implies this is pointwise convergence too. Part 3 is straightforward, except for one wrinkle: I can't seem to show $g$ is Riemann integrable. It seems like the following should be generally true, but I cannot prove it: If $\{f_n\}$ is a pointwise monotonic sequence of continuous functions, converging to $g\in L^1([0,1])$ , then $g$ is continuous almost everywhere. Is this true? Or do I need to use more about this particular sequence of my original problem?","Here is an analysis problem I'm stuck on: Let with and increasing and convex. Define: Show: There is a with almost everywhere I've been able to do part 1, and I believe I can do part 2. The idea is that convexity implies when [I call this pointwise monotonicity below], and I use that to show is a Cauchy sequence in . There is thus a with almost everywhere, and the pointwise monotonicity implies this is pointwise convergence too. Part 3 is straightforward, except for one wrinkle: I can't seem to show is Riemann integrable. It seems like the following should be generally true, but I cannot prove it: If is a pointwise monotonic sequence of continuous functions, converging to , then is continuous almost everywhere. Is this true? Or do I need to use more about this particular sequence of my original problem?","f\in C^0([0,1]) f(0)=0 f  f_n(x) = n\big[f(x)-f(x-\tfrac{1}{n})\big]  f(1-\tfrac{1}{n})\le\int_0^1f_n(x)\ dx\le f(1) g f_n(x)\rightarrow g(x) \int_0^1 g(x)\ dx = f(1) f_n(x)\ge f_m(x) n\ge m \{f_n\} L^1([0,1]) g\in L^1([0,1]) f_n\rightarrow g g \{f_n\} g\in L^1([0,1]) g","['real-analysis', 'functional-analysis', 'riemann-integration', 'pointwise-convergence']"
18,Simple functional equation: $f(x)=f(y) \implies f(ax)=f(ay)$,Simple functional equation:,f(x)=f(y) \implies f(ax)=f(ay),"Let $a$ be real number and variable $x,y\in\mathbb R^n$ . Solve for all continuous function $f$ such that it is ""proportional-invariance"": $f(x)=f(y)\implies f(ax)=f(ay)$ for all $a$ . Let's start at one dimension. For $n=1$ , it is straightforward strict monotonic function or constant are solutions. Edit: power-like functions are also solutions. I have no idea how to deal with it when $n=2$ , besides requiring that $f$ is also monotonic in both directions. Can anyone give me a hint for the simple case when $n=2$ and $a$ is positive? Example: $f(x_1,x_2)=g(x_1+x_2)$ and $g$ is monotonic. Edit: after reading the answers, my guessed solution is $f(x)=|x|g(\frac{x}{|x|})$","Let be real number and variable . Solve for all continuous function such that it is ""proportional-invariance"": for all . Let's start at one dimension. For , it is straightforward strict monotonic function or constant are solutions. Edit: power-like functions are also solutions. I have no idea how to deal with it when , besides requiring that is also monotonic in both directions. Can anyone give me a hint for the simple case when and is positive? Example: and is monotonic. Edit: after reading the answers, my guessed solution is","a x,y\in\mathbb R^n f f(x)=f(y)\implies f(ax)=f(ay) a n=1 n=2 f n=2 a f(x_1,x_2)=g(x_1+x_2) g f(x)=|x|g(\frac{x}{|x|})","['real-analysis', 'functional-analysis', 'functional-equations']"
19,"If $f_n \rightarrow f$ weakly in $L^p$, then $\sqrt{f_n} \rightarrow \sqrt{f}$ weakly in $L^{2p}$?","If  weakly in , then  weakly in ?",f_n \rightarrow f L^p \sqrt{f_n} \rightarrow \sqrt{f} L^{2p},"Suppose $||f_n||_{L^p(\Omega)} \leq C$ , where $\Omega$ is a bounded set in $\mathbb{R}^n$ . Moreover, $f_n \geq 0$ . Using weak compactness, we know that there exists a subsequence $\{f_{n_k} \}$ such that $f_{n_k} \rightarrow f$ weakly in $L^p$ . Since $||\sqrt{f_{n_k}}||_{L^{2p}} \leq C$ , we similarly obtain $\sqrt{f_{n_k}} \rightarrow \sqrt{g}$ weakly in $L^{2p}$ , up to a subsequence. My question is whether $f=g$ . I guess $f=g$ due to the choice of subsequence. If true, how to prove it?","Suppose , where is a bounded set in . Moreover, . Using weak compactness, we know that there exists a subsequence such that weakly in . Since , we similarly obtain weakly in , up to a subsequence. My question is whether . I guess due to the choice of subsequence. If true, how to prove it?",||f_n||_{L^p(\Omega)} \leq C \Omega \mathbb{R}^n f_n \geq 0 \{f_{n_k} \} f_{n_k} \rightarrow f L^p ||\sqrt{f_{n_k}}||_{L^{2p}} \leq C \sqrt{f_{n_k}} \rightarrow \sqrt{g} L^{2p} f=g f=g,"['real-analysis', 'lp-spaces', 'weak-topology']"
20,"Does there exist a sequence of antiderivatives $f_0,f_1,f_2,\dots$ with $f_i(0),f_i(1)$ integers?",Does there exist a sequence of antiderivatives  with  integers?,"f_0,f_1,f_2,\dots f_i(0),f_i(1)","Does there exist an infinite sequence of differentiable functions $f_0,f_1,f_2,\ldots:[0,1]\to\Bbb R$ , not all the zero function, such that, for all $i$ , $f_{i+1}'=f_i$ , and $f_i(0)$ and $f_i(1)$ are both integers? I conjecture the answer is no , though I'm unsure of how to prove it. I can answer some variations of the question, though. It is natural to ask what happens if the $1$ in the above question is replaced with other numbers. If $1$ in the above question is replaced with $\ln2$ (meaning we want $f_i(0)$ and $f_i(\ln2)$ to be integers), then there is such a sequence, namely $(e^x,e^x,e^x,\ldots)$ . Similarly, if $1$ is replaced with $\pi$ , then $(\cos x,\sin x,-\cos x,-\sin x,\cos x,\dotsb)$ is such a sequence. A little thought will reveal a generalization. Consider again the above question with $1$ replaced with some other number $\alpha$ . If $e^\alpha\in\Bbb Q$ then there is such a sequence of functions, and if both $\sin\alpha,\cos\alpha\in\Bbb Q$ then there is such a sequence of functions (in each case take some constant multiple of the sequences mentioned above). It is interesting to note that the sets $\{x:\sin x,\cos x\in\Bbb Q\}$ and $\{x:e^x\in\Bbb Q\}$ are both closed under addition and subtraction. I conjecture that this gives us all solutions to these variations. That is, I conjecture that if there exists an infinite sequence of differentiable functions $f_0,f_1,f_2,\ldots:[0,\alpha]\to\Bbb R$ , not all the zero function, such that, for all $i$ , $f_{i+1}'=f_i$ , and in addition $f_i(0)$ and $f_i(\alpha)$ are both integers for all $i$ , then in fact $\alpha\in\{x:\sin x,\cos x\in\Bbb Q\}\cup\{x:e^x\in\Bbb Q\}$ .","Does there exist an infinite sequence of differentiable functions , not all the zero function, such that, for all , , and and are both integers? I conjecture the answer is no , though I'm unsure of how to prove it. I can answer some variations of the question, though. It is natural to ask what happens if the in the above question is replaced with other numbers. If in the above question is replaced with (meaning we want and to be integers), then there is such a sequence, namely . Similarly, if is replaced with , then is such a sequence. A little thought will reveal a generalization. Consider again the above question with replaced with some other number . If then there is such a sequence of functions, and if both then there is such a sequence of functions (in each case take some constant multiple of the sequences mentioned above). It is interesting to note that the sets and are both closed under addition and subtraction. I conjecture that this gives us all solutions to these variations. That is, I conjecture that if there exists an infinite sequence of differentiable functions , not all the zero function, such that, for all , , and in addition and are both integers for all , then in fact .","f_0,f_1,f_2,\ldots:[0,1]\to\Bbb R i f_{i+1}'=f_i f_i(0) f_i(1) 1 1 \ln2 f_i(0) f_i(\ln2) (e^x,e^x,e^x,\ldots) 1 \pi (\cos x,\sin x,-\cos x,-\sin x,\cos x,\dotsb) 1 \alpha e^\alpha\in\Bbb Q \sin\alpha,\cos\alpha\in\Bbb Q \{x:\sin x,\cos x\in\Bbb Q\} \{x:e^x\in\Bbb Q\} f_0,f_1,f_2,\ldots:[0,\alpha]\to\Bbb R i f_{i+1}'=f_i f_i(0) f_i(\alpha) i \alpha\in\{x:\sin x,\cos x\in\Bbb Q\}\cup\{x:e^x\in\Bbb Q\}","['real-analysis', 'calculus', 'indefinite-integrals', 'integers']"
21,Better understanding of integration of differential forms.,Better understanding of integration of differential forms.,,"On page $100$ of Spivak's Calculus on Manifolds the following definition is made: If $\omega$ is a $k$ -form on $\mathbb{R}^k$ , then $\omega = f\ dx_1\land\ldots\land dx_k$ for a unique function $f:\mathbb{R}^k\to\mathbb{R}$ . We define $$\int_{[0,1]^k}\omega := \int_{[0,1]^k}f.$$ I wish to get a better grasp of the above definition. From my understanding of this post one may interpret $\omega$ as a function that, at each point $p\in\mathbb{R}^k$ , takes in $k$ vectors $v^1,\ldots,v^k$ representing a $k$ -dimensional parallelotope $P$ and spits out a number proportional to its hypervolume. Such number being $$ f(p) \ A(v^1,\ldots,v^k)$$ for a point $p\in P$ , a function $f:\mathbb{R}^k\to\mathbb{R}$ , and the alternating $k$ -tensor $A:{(\mathbb{R}^k)}^k\to\mathbb{R}$ defined by $$A = x_1\land\ldots\land x_k = \text{Alt}(x_1\otimes\ldots\otimes x_k) = \sum_{\sigma\in\mathbb{S}_k}\text{sgn}(\sigma) \prod_{j=1}^k v^{\sigma(j)}_j$$ although the explicit computation of $A$ seems secondary to the fact it is multilinear and alternating. How should one interpret the numbers $f(p)$ and $A(v_1,\ldots,v_k)$ at the moment of computing the integral? It would also probably help if someone could provide an example where concrete values are given to the numbers above.","On page of Spivak's Calculus on Manifolds the following definition is made: If is a -form on , then for a unique function . We define I wish to get a better grasp of the above definition. From my understanding of this post one may interpret as a function that, at each point , takes in vectors representing a -dimensional parallelotope and spits out a number proportional to its hypervolume. Such number being for a point , a function , and the alternating -tensor defined by although the explicit computation of seems secondary to the fact it is multilinear and alternating. How should one interpret the numbers and at the moment of computing the integral? It would also probably help if someone could provide an example where concrete values are given to the numbers above.","100 \omega k \mathbb{R}^k \omega = f\ dx_1\land\ldots\land dx_k f:\mathbb{R}^k\to\mathbb{R} \int_{[0,1]^k}\omega := \int_{[0,1]^k}f. \omega p\in\mathbb{R}^k k v^1,\ldots,v^k k P  f(p) \ A(v^1,\ldots,v^k) p\in P f:\mathbb{R}^k\to\mathbb{R} k A:{(\mathbb{R}^k)}^k\to\mathbb{R} A = x_1\land\ldots\land x_k = \text{Alt}(x_1\otimes\ldots\otimes x_k) = \sum_{\sigma\in\mathbb{S}_k}\text{sgn}(\sigma) \prod_{j=1}^k v^{\sigma(j)}_j A f(p) A(v_1,\ldots,v_k)","['real-analysis', 'integration', 'multivariable-calculus', 'definition', 'differential-forms']"
22,Evaluate: $\sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^m\delta_{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}$,Evaluate:,\sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^m\delta_{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n},"Context: The cube super root ssrt $_3(x)$ series expansion yielded part of it as: $$\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty\frac{(-n)^m}{m!}\sum_{k=0}^{n-1}\binom{n-1}k\left.\frac{d^kt^m}{dt^k}\right|_{t=0}\left.\frac{d^{n-1-k}e^{tm}}{d^{n-1-k}}\right|_{t=0}=\lim_{t\to0}\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty \sum_{k=0}^{n-1}\frac{(-n)^m \Gamma(n) m^{n-k+1}t^{m-k}}{k!(m-k)!\Gamma(n-k)}$$ The $m=0$ term is ignored outside the sum. The inner sum uses Hypergeometric $U(a,b,x)$ and cancels $t^a$ , so the limit was easier to find. Testing different cases of $m,n$ gave a pattern for the limit as a ratio of gamma functions: $$\lim_{t\to0}\sum_{m,n=1}^\infty\frac{y^n m^nn^m}{m^{m+2}(m-1)!n!}U(-m,n-m,t)=\lim_{t\to0}\sum_{m,n=1}^\infty\frac{y^n m^nn^m(m-(n+t))!\sin(\pi(n+t))}{m^{m+2}\pi\Gamma(m)}=\sum_{m,n=1}^\infty\frac{y^n(-1)^mm^n n^{m-1}}{\Gamma(n-m)\Gamma(m)m^{m+2}}$$ Problem: Also, switching the differentiation order,  due to the general Leibniz rule , gives the an equal sum; part of it had this limit: $$\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty\frac{(-n)^m}{m!}\sum_{k=0}^{n-1}\binom{n-1}k\left.\frac{d^{n-1-k}t^m}{dt^{n-1-k}}\right|_{t=0}\left.\frac{d^ke^{tm}}{dt^k}\right|_{t=0}=\lim_{t\to0}\sum_{n=1}^\infty\sum_{m=0}^\infty\sum_{k=0}^{n-1}\frac{y^nm^k(-n)^m t^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}$$ Taking $\lim\limits_{t\to0}t^{k+m-n+1}$ makes the sum diverge and seemingly removing any terms from the sum still makes the sum diverge, so we can’t substitute this confluent $\,_1\text F_1(a;b;x)$ identity: $$\sum_{k=0}^{n-1}\frac{m^k(-n)^m t^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}=\frac{(-n)^mt^{m-n+1}\,_1\text F_1(1-n;m-n+2;-mt) }{n!(m-n+1)!}$$ However, noticing that the truncated sums give a polynomial, the limit is the constant term of it and therefore Kronecker $\delta_x$ appears: $$\lim_{t\to0}\sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^mt^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}= \sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^m\delta_{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}$$ We can remove a sum like in this post or simplify, but how?","Context: The cube super root ssrt series expansion yielded part of it as: The term is ignored outside the sum. The inner sum uses Hypergeometric and cancels , so the limit was easier to find. Testing different cases of gave a pattern for the limit as a ratio of gamma functions: Problem: Also, switching the differentiation order,  due to the general Leibniz rule , gives the an equal sum; part of it had this limit: Taking makes the sum diverge and seemingly removing any terms from the sum still makes the sum diverge, so we can’t substitute this confluent identity: However, noticing that the truncated sums give a polynomial, the limit is the constant term of it and therefore Kronecker appears: We can remove a sum like in this post or simplify, but how?","_3(x) \sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty\frac{(-n)^m}{m!}\sum_{k=0}^{n-1}\binom{n-1}k\left.\frac{d^kt^m}{dt^k}\right|_{t=0}\left.\frac{d^{n-1-k}e^{tm}}{d^{n-1-k}}\right|_{t=0}=\lim_{t\to0}\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty \sum_{k=0}^{n-1}\frac{(-n)^m \Gamma(n) m^{n-k+1}t^{m-k}}{k!(m-k)!\Gamma(n-k)} m=0 U(a,b,x) t^a m,n \lim_{t\to0}\sum_{m,n=1}^\infty\frac{y^n m^nn^m}{m^{m+2}(m-1)!n!}U(-m,n-m,t)=\lim_{t\to0}\sum_{m,n=1}^\infty\frac{y^n m^nn^m(m-(n+t))!\sin(\pi(n+t))}{m^{m+2}\pi\Gamma(m)}=\sum_{m,n=1}^\infty\frac{y^n(-1)^mm^n n^{m-1}}{\Gamma(n-m)\Gamma(m)m^{m+2}} \sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty\frac{(-n)^m}{m!}\sum_{k=0}^{n-1}\binom{n-1}k\left.\frac{d^{n-1-k}t^m}{dt^{n-1-k}}\right|_{t=0}\left.\frac{d^ke^{tm}}{dt^k}\right|_{t=0}=\lim_{t\to0}\sum_{n=1}^\infty\sum_{m=0}^\infty\sum_{k=0}^{n-1}\frac{y^nm^k(-n)^m t^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n} \lim\limits_{t\to0}t^{k+m-n+1} \,_1\text F_1(a;b;x) \sum_{k=0}^{n-1}\frac{m^k(-n)^m t^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}=\frac{(-n)^mt^{m-n+1}\,_1\text F_1(1-n;m-n+2;-mt) }{n!(m-n+1)!} \delta_x \lim_{t\to0}\sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^mt^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}= \sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^m\delta_{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}","['real-analysis', 'limits', 'hypergeometric-function', 'tetration', 'kronecker-delta']"
23,Is there a quartic polynomial in two variables that have multiple local minima and no other critical points?,Is there a quartic polynomial in two variables that have multiple local minima and no other critical points?,,"Question. Can a degree 4 polynomial $p:\mathbb R^2 \to \mathbb R$ have $N\geq 2$ local minima and no other critical points? I got this question when trying to answer: How many strict local minima a quartic polynomial in two variables might have? A closely related question asks: If a two variable smooth function has two global minima, will it necessarily have a third critical point? Among the answers @RiverLi gives an example of the function $$ f(x,y)=(x^2-1)^2+(x^2y-x-1)^2, \tag{*}\label{RL} $$ which is a polynomial, but of degree 6. Notice that polynomials like \eqref{RL}  can be written as $p(x,y)=u(x,y)^2 + v(x,y)^2$ . Since we need $p$ to be degree 4 polynomial, $u$ and $v$ must be quadratic polynomials. We need there to be a set $P\subset \Bbb R^2, |P|=N$ , of points such that: $(x,y)\in P \ \ \Longleftrightarrow \ \ u(x,y)=v(x,y)=0$ ; $(x,y)\in P \ \ \Longleftrightarrow \ \ u_x u + v_x v = u_y u + v_y v =0$ when evaluated at $(x,y)$ . For what values of $N\geq 2$ is there a set $P\subset \Bbb R^2, |P|=N$ and quadratic polynomials $u,v:\Bbb R^2 \to \Bbb R$ satisfying conditions 1 and 2? My guess is that for $N=2$ at least one of the polynomials $u,v$ has to be of the third order as in \eqref{RL}. However, it is possible that there would be desirable $u,v$ for $N=3$ or $N=4$ . Note that by Bézout's theorem condition 1 can not be satisfied at more than 4 points given that $u,v$ are quadratic. Alternatively, is there another form than $p=u^2+v^2$ that could represent a quartic polynomial $p$ with $N\geq 2$ local minima and no other critical points? Disclaimer. I asked about the special case of $N=2$ on https://math.codidact.com .","Question. Can a degree 4 polynomial have local minima and no other critical points? I got this question when trying to answer: How many strict local minima a quartic polynomial in two variables might have? A closely related question asks: If a two variable smooth function has two global minima, will it necessarily have a third critical point? Among the answers @RiverLi gives an example of the function which is a polynomial, but of degree 6. Notice that polynomials like \eqref{RL}  can be written as . Since we need to be degree 4 polynomial, and must be quadratic polynomials. We need there to be a set , of points such that: ; when evaluated at . For what values of is there a set and quadratic polynomials satisfying conditions 1 and 2? My guess is that for at least one of the polynomials has to be of the third order as in \eqref{RL}. However, it is possible that there would be desirable for or . Note that by Bézout's theorem condition 1 can not be satisfied at more than 4 points given that are quadratic. Alternatively, is there another form than that could represent a quartic polynomial with local minima and no other critical points? Disclaimer. I asked about the special case of on https://math.codidact.com .","p:\mathbb R^2 \to \mathbb R N\geq 2 
f(x,y)=(x^2-1)^2+(x^2y-x-1)^2, \tag{*}\label{RL}
 p(x,y)=u(x,y)^2 + v(x,y)^2 p u v P\subset \Bbb R^2, |P|=N (x,y)\in P \ \ \Longleftrightarrow \ \ u(x,y)=v(x,y)=0 (x,y)\in P \ \ \Longleftrightarrow \ \ u_x u + v_x v = u_y u + v_y v =0 (x,y) N\geq 2 P\subset \Bbb R^2, |P|=N u,v:\Bbb R^2 \to \Bbb R N=2 u,v u,v N=3 N=4 u,v p=u^2+v^2 p N\geq 2 N=2","['real-analysis', 'polynomials', 'differential-topology', 'roots', 'maxima-minima']"
24,The absolute continuity of push-forward measure,The absolute continuity of push-forward measure,,"Let $X$ be a real-valued random variable on $\mathbb R$ , and $f:\mathbb R \to \mathbb R$ differentiable such that $f'(x)>0$ for all $x \in \mathbb R$ . Let $Y := f(X)$ . Let $\mu_X, \mu_Y$ be the distributions of $X, Y$ respectively. Then $\mu_Y = f_{\sharp} \mu_X$ . Let $F_X, F_Y$ be the c.d.f. of $X, Y$ respectively. At page $14$ of this lecture note , the author said that Theorem: If $\mu_X$ is absolutely continuous w.r.t. Lesbesgue measure $\lambda$ , then so is $\mu_Y$ My attempt: Clearly, we have $F_Y (t) = F_X \circ f^{-1} (t)$ . Let $A$ be a Borel set such that $\lambda(A) = 0$ . Because $\mu_X \ll \lambda$ , we get $\mu_X (A) =0$ . We have $\mu_Y (A) = \mu_X(f^{-1} (A))$ . It suffices to prove $f^{-1} (A)$ is a $\lambda$ -null set. Could you shed some light on how to finish the proof? Update: I have found a related result here . However, it requires $f$ to be continuously differentiable, i.e., if $f\in C^1$ and $\{f' = 0\}$ is $\lambda$ -null then $f^{-1} (A)$ is also $\lambda$ -null.","Let be a real-valued random variable on , and differentiable such that for all . Let . Let be the distributions of respectively. Then . Let be the c.d.f. of respectively. At page of this lecture note , the author said that Theorem: If is absolutely continuous w.r.t. Lesbesgue measure , then so is My attempt: Clearly, we have . Let be a Borel set such that . Because , we get . We have . It suffices to prove is a -null set. Could you shed some light on how to finish the proof? Update: I have found a related result here . However, it requires to be continuously differentiable, i.e., if and is -null then is also -null.","X \mathbb R f:\mathbb R \to \mathbb R f'(x)>0 x \in \mathbb R Y := f(X) \mu_X, \mu_Y X, Y \mu_Y = f_{\sharp} \mu_X F_X, F_Y X, Y 14 \mu_X \lambda \mu_Y F_Y (t) = F_X \circ f^{-1} (t) A \lambda(A) = 0 \mu_X \ll \lambda \mu_X (A) =0 \mu_Y (A) = \mu_X(f^{-1} (A)) f^{-1} (A) \lambda f f\in C^1 \{f' = 0\} \lambda f^{-1} (A) \lambda","['real-analysis', 'measure-theory', 'derivatives', 'probability-distributions', 'lebesgue-measure']"
25,Does the limit superior of every subsequence equals to the same measurable function imply convergence of the original sequence?,Does the limit superior of every subsequence equals to the same measurable function imply convergence of the original sequence?,,"In an abstract measure space $(X, A, \mu)$ , we consider a sequence of measurable functions $(f_n)_n$ with $f_n : X \to \mathbb{R}$ such that there exists some measurable function $f : X \to \mathbb{R}$ , we have for every subsequence $\{n_k\}_k$ of $\{n\}$ , $$      \limsup_{k \to \infty} f_{n_k} = f      ~\mbox{a.e.} $$ holds. Then we ask if the following claim holds true $$       \lim_{n \to \infty} f_n = f       ~\mbox{a.e.} $$ Idea: If the claim is false, I think maybe this example can help. $(X,A,\mu) = ([0,1], \mathcal{B}[0,1],m)$ , $f = 1$ , $f_n = 1_{A_n}$ with $A_n \in \mathcal{B}[0,1]$ and $m(A_n) = \frac{1}{2}$ . The key is to construct $A_n$ . Since now the counterexample is found, I want to ask does the condition leads to convergence in measure induced by $f_n$ , i.e. $\mu_n(A) := \mu(f_n \in A)$ for $A \in \mathcal{B}(\mathbb{R})$ satisfies $\lim_n \int_{\mathbb{R}} g(x) \mu_n(dx) = \int_{\mathbb{R}} g(x) \mu(dx)$ for any bounded continuous $g$ on $\mathbb{R}$ ? if $\mu(X) = 1$ , does the condition leads to convergence in probability of $f_n$ , i.e. for any $\varepsilon$ , does $\lim_n \mu(|f_n - f| \geq \varepsilon) = 0$ ? The answer is no for two questions, if we take i.i.d. random variable with Bernoulli distribution.","In an abstract measure space , we consider a sequence of measurable functions with such that there exists some measurable function , we have for every subsequence of , holds. Then we ask if the following claim holds true Idea: If the claim is false, I think maybe this example can help. , , with and . The key is to construct . Since now the counterexample is found, I want to ask does the condition leads to convergence in measure induced by , i.e. for satisfies for any bounded continuous on ? if , does the condition leads to convergence in probability of , i.e. for any , does ? The answer is no for two questions, if we take i.i.d. random variable with Bernoulli distribution.","(X, A, \mu) (f_n)_n f_n : X \to \mathbb{R} f : X \to \mathbb{R} \{n_k\}_k \{n\} 
     \limsup_{k \to \infty} f_{n_k} = f
     ~\mbox{a.e.}
 
      \lim_{n \to \infty} f_n = f
      ~\mbox{a.e.}
 (X,A,\mu) = ([0,1], \mathcal{B}[0,1],m) f = 1 f_n = 1_{A_n} A_n \in \mathcal{B}[0,1] m(A_n) = \frac{1}{2} A_n f_n \mu_n(A) := \mu(f_n \in A) A \in \mathcal{B}(\mathbb{R}) \lim_n \int_{\mathbb{R}} g(x) \mu_n(dx) = \int_{\mathbb{R}} g(x) \mu(dx) g \mathbb{R} \mu(X) = 1 f_n \varepsilon \lim_n \mu(|f_n - f| \geq \varepsilon) = 0","['real-analysis', 'probability', 'convergence-divergence', 'weak-convergence']"
26,"$K(x,t)=2\pi\sum_{n=1}^\infty n\sin(n\pi x)e^{-n^2\pi^2t}$ satisfies some ""pseudo-good-kernel"" properties","satisfies some ""pseudo-good-kernel"" properties","K(x,t)=2\pi\sum_{n=1}^\infty n\sin(n\pi x)e^{-n^2\pi^2t}","Let $f\in C([0,\infty))$ . Define $$K(x,t)=2\pi\sum_{n=1}^\infty n\sin(n\pi x)e^{-n^2\pi^2t},\qquad x\in[0,1],\  t>0.$$ Show that $$\lim_{x\to0^+}\int_0^tK(x,t-\tau)f(\tau)\,d\tau=f(t), \qquad t>0.$$ I can show the result by assuming that $$\lim_{x\to0^+}\int_0^tK(x,t-\tau)\,d\tau=1,\tag{1}$$ and $$\int_0^1 |K(x,t)|\,dt<\infty \text{ for all } x \text{ close to }0.\tag{2}$$ For any $\varepsilon>0$ , we can find $\delta>0$ so that $|f(\tau)-f(t)|<\varepsilon$ for all $\tau\in(t-\delta,\delta)$ . In $[\delta,t)$ , the series defining $K$ is convergent absolutely and we can change the order of limitation and integration to get $$\lim_{x\to0^+}\int_0^{t-\delta}K(x,t-\tau)f(\tau)\,d\tau=0.$$ As for the integration in $(t-\delta,t)$ , we have $$\int_{t-\delta}^t |K(x,t-\tau)||f(\tau)-f(t)|\,d\tau\leq \varepsilon\int_{t-\delta}^t |K(x,t-\tau)|\,d\tau\lesssim \varepsilon,$$ by $(2)$ . And now the result follows from $(1)$ . I have no idea how to show $(1)$ and $(2)$ . It seems that we need to carefully use the definition of $K$ . For example, if we simply apply Fubini to $(2)$ , we will get a divergent integral. Any help would be appreciated!","Let . Define Show that I can show the result by assuming that and For any , we can find so that for all . In , the series defining is convergent absolutely and we can change the order of limitation and integration to get As for the integration in , we have by . And now the result follows from . I have no idea how to show and . It seems that we need to carefully use the definition of . For example, if we simply apply Fubini to , we will get a divergent integral. Any help would be appreciated!","f\in C([0,\infty)) K(x,t)=2\pi\sum_{n=1}^\infty n\sin(n\pi x)e^{-n^2\pi^2t},\qquad x\in[0,1],\  t>0. \lim_{x\to0^+}\int_0^tK(x,t-\tau)f(\tau)\,d\tau=f(t), \qquad t>0. \lim_{x\to0^+}\int_0^tK(x,t-\tau)\,d\tau=1,\tag{1} \int_0^1 |K(x,t)|\,dt<\infty \text{ for all } x \text{ close to }0.\tag{2} \varepsilon>0 \delta>0 |f(\tau)-f(t)|<\varepsilon \tau\in(t-\delta,\delta) [\delta,t) K \lim_{x\to0^+}\int_0^{t-\delta}K(x,t-\tau)f(\tau)\,d\tau=0. (t-\delta,t) \int_{t-\delta}^t |K(x,t-\tau)||f(\tau)-f(t)|\,d\tau\leq \varepsilon\int_{t-\delta}^t |K(x,t-\tau)|\,d\tau\lesssim \varepsilon, (2) (1) (1) (2) K (2)","['real-analysis', 'fourier-analysis', 'convolution', 'harmonic-analysis']"
27,Showing a function is unbounded,Showing a function is unbounded,,"Let $f(x)=x\cos x+\sin x$ . Show that for every $M>0$ and every $k\geq 1$ , there is $x_0>M$ such that $f(x)\geq k$ , $\forall x\in [x_0,x_0+\frac{1}{k}]$ . If we replace $x$ by $2\pi n$ then $|f(2\pi n)=|2\pi n|$ . Can this help? Can I get a help to start it?","Let . Show that for every and every , there is such that , . If we replace by then . Can this help? Can I get a help to start it?","f(x)=x\cos x+\sin x M>0 k\geq 1 x_0>M f(x)\geq k \forall x\in [x_0,x_0+\frac{1}{k}] x 2\pi n |f(2\pi n)=|2\pi n|",['real-analysis']
28,"In abstract algebra, how important is it to draw a distinction between polynomials and polynomial functions? [duplicate]","In abstract algebra, how important is it to draw a distinction between polynomials and polynomial functions? [duplicate]",,"This question already has answers here : Do we really need polynomials (In contrast to polynomial functions)? (6 answers) Closed 2 years ago . In introductory analysis, it is common to define a single-variable polynomial as a function $p:\Bbb{R}\mapsto\Bbb{R}$ (or $\Bbb{C}\mapsto\Bbb{C})$ such that $$ p(x)=\sum_{i=0}^{n}a_ix^i \text{ for all $x$.} $$ This means that $x^2$ is technically not a polynomial, but rather the value of the polynomial $t\mapsto t^2$ at the point $x$ . However, according to Wikipedia , in abstract algebra one often distinguishes between polynomials and polynomial functions . According to this definition, $x^2$ is literally a polynomial, whereas $t\mapsto t^2$ is the corresponding polynomial function. My question is: In abstract algebra, what is the purpose of defining polynomials as formal algebraic expressions, rather than as functions? How important is it to draw a distinction between polynomials and polynomial functions?","This question already has answers here : Do we really need polynomials (In contrast to polynomial functions)? (6 answers) Closed 2 years ago . In introductory analysis, it is common to define a single-variable polynomial as a function (or such that This means that is technically not a polynomial, but rather the value of the polynomial at the point . However, according to Wikipedia , in abstract algebra one often distinguishes between polynomials and polynomial functions . According to this definition, is literally a polynomial, whereas is the corresponding polynomial function. My question is: In abstract algebra, what is the purpose of defining polynomials as formal algebraic expressions, rather than as functions? How important is it to draw a distinction between polynomials and polynomial functions?","p:\Bbb{R}\mapsto\Bbb{R} \Bbb{C}\mapsto\Bbb{C}) 
p(x)=\sum_{i=0}^{n}a_ix^i \text{ for all x.}
 x^2 t\mapsto t^2 x x^2 t\mapsto t^2","['real-analysis', 'abstract-algebra', 'polynomials', 'definition']"
29,For which $r \in \mathbb R$ is the series $S(r)$ finite?,For which  is the series  finite?,r \in \mathbb R S(r),"For each $r \in \mathbb R$ we let $$L_r := \left\{ \begin{pmatrix} a+cr \\ b+dr \\ c \\d \end{pmatrix} : a,b,c,d \in \mathbb Z \right\}, \quad W:= \left\{ \begin{pmatrix} 0 \\ 0 \\ x_3 \\ x_4 \end{pmatrix} : x_3,x_4 \in \mathbb R \right\} $$ and $$ L^*_r := L_r \setminus W.$$ Now we consider the series $$S(r):=  \sum_{x \in L^*_r} E_1(2 (x_1^2+x_2^2)) \exp ( (x_1^2+x_2^2) - (x_3^2+x_4^2)).$$ Here $E_1 : (0, \infty) \to (0,\infty)$ is the exponential integral $$E_1(s):=\int_1^\infty \exp(-ts) \frac{dt}{t} = \int_s^\infty \exp(-t) \frac{dt}{t}.$$ My question: For which $r \in \mathbb R$ is $S(r)$ finite? My partial answer: It's quite easy to see that $S(r)$ is fintie for $r \in \mathbb Q$ . But that's all I've found out so far. Happy: I would already be happy if you could show for a single $r \in \mathbb R \setminus \mathbb Q$ (you pick it!) whether $S(r)$ converges or diverges. Some facts: We have $$ S(r) = \int_1^\infty \left( \sum_{x \in L^*_r}  \exp \left( (1-2t)(x_1^2+x_2^2) - (x_3^2+x_4^2) \right)\right) \frac{dt}{t}.$$ Further, we have $L_r^*=L_r \setminus \{0\}$ iff $r \in \mathbb R \setminus \mathbb Q$ . Estimates for $E_1$ : In case they help: For $s>0$ we have $$\frac{e^{-s}}{s+1} < E_1(s) < \frac{s+1}{s+2} \frac{e^{-s}}{s} < \frac{e^{-s}}{s}.$$ Proof for rational $r$ : Since I was asked in the comments to provide a proof for rational $r$ here it is. If $r$ is rational the set $$\{ a+br : a,b \in \mathbb Z \}$$ is a discrete subset of $\mathbb R$ . Hence $$\varepsilon := \min(\{2(x_1^2+x_2^2) : x \in L_r^*\}) > 0$$ exists. Now we make use of $$E_1(s) \le \frac{\exp(-s)}{s}.$$ We have $$S(r) \le \sum_{x \in L^*_r} \frac{\exp(-2 (x_1^2+x_2^2))}{2 (x_1^2+x_2^2)} \exp ( (x_1^2+x_2^2) - (x_3^2+x_4^2))\\ \le \frac{1}{\varepsilon} \sum_{x \in L^*_r} \exp(-(x_1^2+x_2^2+x_3^2+x_4^2))\\ \le \frac{1}{\varepsilon} \sum_{x \in L_r} \exp(-||x||^2) < \infty. $$","For each we let and Now we consider the series Here is the exponential integral My question: For which is finite? My partial answer: It's quite easy to see that is fintie for . But that's all I've found out so far. Happy: I would already be happy if you could show for a single (you pick it!) whether converges or diverges. Some facts: We have Further, we have iff . Estimates for : In case they help: For we have Proof for rational : Since I was asked in the comments to provide a proof for rational here it is. If is rational the set is a discrete subset of . Hence exists. Now we make use of We have","r \in \mathbb R L_r := \left\{ \begin{pmatrix} a+cr \\ b+dr \\ c \\d \end{pmatrix} : a,b,c,d \in \mathbb Z \right\},
\quad W:= \left\{ \begin{pmatrix} 0 \\ 0 \\ x_3 \\ x_4 \end{pmatrix} : x_3,x_4 \in \mathbb R \right\}
  L^*_r := L_r \setminus W. S(r):=  \sum_{x \in L^*_r} E_1(2 (x_1^2+x_2^2)) \exp ( (x_1^2+x_2^2) - (x_3^2+x_4^2)). E_1 : (0, \infty) \to (0,\infty) E_1(s):=\int_1^\infty \exp(-ts) \frac{dt}{t} = \int_s^\infty \exp(-t) \frac{dt}{t}. r \in \mathbb R S(r) S(r) r \in \mathbb Q r \in \mathbb R \setminus \mathbb Q S(r)  S(r) = \int_1^\infty \left( \sum_{x \in L^*_r}  \exp \left( (1-2t)(x_1^2+x_2^2) - (x_3^2+x_4^2) \right)\right) \frac{dt}{t}. L_r^*=L_r \setminus \{0\} r \in \mathbb R \setminus \mathbb Q E_1 s>0 \frac{e^{-s}}{s+1} < E_1(s) < \frac{s+1}{s+2} \frac{e^{-s}}{s} < \frac{e^{-s}}{s}. r r r \{ a+br : a,b \in \mathbb Z \} \mathbb R \varepsilon := \min(\{2(x_1^2+x_2^2) : x \in L_r^*\}) > 0 E_1(s) \le \frac{\exp(-s)}{s}. S(r)
\le \sum_{x \in L^*_r} \frac{\exp(-2 (x_1^2+x_2^2))}{2 (x_1^2+x_2^2)} \exp ( (x_1^2+x_2^2) - (x_3^2+x_4^2))\\
\le \frac{1}{\varepsilon} \sum_{x \in L^*_r} \exp(-(x_1^2+x_2^2+x_3^2+x_4^2))\\
\le \frac{1}{\varepsilon} \sum_{x \in L_r} \exp(-||x||^2) < \infty.
","['real-analysis', 'exponential-function', 'divergent-series', 'integer-lattices', 'exponential-sum']"
30,"Suppose $X\sim N(\mu,1)$, show that $|\mu|$ has no unbiased estimate","Suppose , show that  has no unbiased estimate","X\sim N(\mu,1) |\mu|","Suppose $X\sim N(\mu,1)$ , show that $|\mu|$ has no unbiased estimator. Hint:use the fact that $|\mu|$ is not differentiable at $\mu=0$ . There are my ideas: Suppose $X_1,\cdots,X_n$ are simple random samples from $N(\mu,1)$ , and $g(X_1,\cdots,X_n)$ is an unbiased estimator for $|\mu|$ , which means $\mathbb{E}_{\mu}(g(X_1,\cdots,X_n))=|\mu|$ , i.e. $$\int g(x_1,\cdots,x_n)e^{-\frac{(x_1-\mu)^2}{2}}\cdots e^{-\frac{(x_n-\mu)^2}{2}}\,\mathrm{d}x_1\cdots\mathrm{d}x_n=|\mu|.$$ I want to prove that $\mathbb{E}_{\mu}(g(X_1,\cdots,X_n))$ is differentiable at $\mu=0$ . For this, I want to exchange integral and derivative by Leibniz integral rule on measure theory statement. I am going to find a function $h(x_1,\cdots,x_n)$ that satisfies $\left|\dfrac{\partial G(x_1,\cdots,x_n,\mu)}{\partial \mu}\right|\le h(x_1,\cdots,x_n)$ on $|\mu|\le\epsilon$ ( $\epsilon$ is a positive number) and $h(x_1,\cdots,x_n)$ is integrable, where $$G=g(x_1,\cdots,x_n)e^{-\frac{(x_1-\mu)^2}{2}}\cdots e^{-\frac{(x_n-\mu)^2}{2}}.$$ But I am stuck.","Suppose , show that has no unbiased estimator. Hint:use the fact that is not differentiable at . There are my ideas: Suppose are simple random samples from , and is an unbiased estimator for , which means , i.e. I want to prove that is differentiable at . For this, I want to exchange integral and derivative by Leibniz integral rule on measure theory statement. I am going to find a function that satisfies on ( is a positive number) and is integrable, where But I am stuck.","X\sim N(\mu,1) |\mu| |\mu| \mu=0 X_1,\cdots,X_n N(\mu,1) g(X_1,\cdots,X_n) |\mu| \mathbb{E}_{\mu}(g(X_1,\cdots,X_n))=|\mu| \int g(x_1,\cdots,x_n)e^{-\frac{(x_1-\mu)^2}{2}}\cdots e^{-\frac{(x_n-\mu)^2}{2}}\,\mathrm{d}x_1\cdots\mathrm{d}x_n=|\mu|. \mathbb{E}_{\mu}(g(X_1,\cdots,X_n)) \mu=0 h(x_1,\cdots,x_n) \left|\dfrac{\partial G(x_1,\cdots,x_n,\mu)}{\partial \mu}\right|\le h(x_1,\cdots,x_n) |\mu|\le\epsilon \epsilon h(x_1,\cdots,x_n) G=g(x_1,\cdots,x_n)e^{-\frac{(x_1-\mu)^2}{2}}\cdots e^{-\frac{(x_n-\mu)^2}{2}}.","['real-analysis', 'probability', 'statistics']"
31,What is the real analysis version of this complex analysis Weierstrass theorem?,What is the real analysis version of this complex analysis Weierstrass theorem?,,"I'm reading Gong Sheng's Concise Complex Analysis , where it introduced a Weierstrass Theorem Theorem 3.1 (Weierstrass Theorem) Suppose $\{f_n(z)\}$ is a sequence of functions where each $f_n(z)$ is defined and holomorphic in a region $U\subseteq \mathbb C$ . Assume that $\sum_{n=1}^\infty f_n(z)$ converges uniformly to $f(z)$ on every  compact subset of $U$ . Then $f(z)$ is holomorphic on $U$ and for every $k\in \mathbb N$ , $\sum_{n=1}^\infty f_n^{(k)}(z)$ converges uniformly to $f^{(k)}(z)$ on every compact subset of $U$ . Then it mentions: This is a profound result. The reader can compare it with the theorem of  the derivative of function series in calculus. So what is the corresponding real analysis version of this complex analysis Weierstrass Theorem, and what is the difference? -- I suppose the difference would show some distinct properties in complex analysis.","I'm reading Gong Sheng's Concise Complex Analysis , where it introduced a Weierstrass Theorem Theorem 3.1 (Weierstrass Theorem) Suppose is a sequence of functions where each is defined and holomorphic in a region . Assume that converges uniformly to on every  compact subset of . Then is holomorphic on and for every , converges uniformly to on every compact subset of . Then it mentions: This is a profound result. The reader can compare it with the theorem of  the derivative of function series in calculus. So what is the corresponding real analysis version of this complex analysis Weierstrass Theorem, and what is the difference? -- I suppose the difference would show some distinct properties in complex analysis.",\{f_n(z)\} f_n(z) U\subseteq \mathbb C \sum_{n=1}^\infty f_n(z) f(z) U f(z) U k\in \mathbb N \sum_{n=1}^\infty f_n^{(k)}(z) f^{(k)}(z) U,"['real-analysis', 'complex-analysis']"
32,How to Taylor expand a scalar quantity about a unit sphere?,How to Taylor expand a scalar quantity about a unit sphere?,,"Consider a vector field $\mathbf{v} (r,\theta)$ expressed in the system of (axisymmetric) spherical coordinates with $r$ denoting the radial distance and $\theta$ the polar angle. We consider a surface $S$ defined by $r = 1 + \epsilon f(\theta)$ where $\epsilon \ll 1$ . We denote by $\mathbf{n}$ the vector normal to that surface. Specifically, $$ \mathbb{n} = \mathbb{e}_r - \epsilon f'(\theta) \, \mathbb{e}_\theta \, ,  $$ wherein $\mathbb{e}_r$ and $\mathbb{e}_\theta$ are the basis unit vectors. Using perturbation analysis, the field $\mathbb{v}$ can be expressed to leading order as $$ \mathbb{v} = \mathbb{v}^{(0)} + \epsilon \mathbb{v}^{(1)} \, . $$ Accordingly, $$ \mathbb{v} \cdot \mathbb{n} =  v_r^{(0)} + \epsilon\left( v_r^{(1)} - f'(\theta) v_\theta^{(0)} \right) \,  $$ wherein $v_r := \mathbb{v} \cdot \mathbb{e}_r$ and $v_\theta := \mathbb{v} \cdot \mathbb{e}_\theta$ . I was wondering how one can Taylor expand the dot product $\mathbb{v} \cdot \mathbb{n}$ about the unit sphere $r=1$ . Specifically, $$ \bigg. \mathbb{v} \cdot \mathbb{n} \bigg|_{r = 1+\epsilon f(\theta)} =  [\bigg. \mathbb{v}^{(0)} \cdot \mathbb{e}_r + \epsilon \left( \text{something} \right) ] \bigg|_{r = 1}  + \mathcal{O}(\epsilon^2) \, . $$ Any help is highly appreciated! Thank you","Consider a vector field expressed in the system of (axisymmetric) spherical coordinates with denoting the radial distance and the polar angle. We consider a surface defined by where . We denote by the vector normal to that surface. Specifically, wherein and are the basis unit vectors. Using perturbation analysis, the field can be expressed to leading order as Accordingly, wherein and . I was wondering how one can Taylor expand the dot product about the unit sphere . Specifically, Any help is highly appreciated! Thank you","\mathbf{v} (r,\theta) r \theta S r = 1 + \epsilon f(\theta) \epsilon \ll 1 \mathbf{n} 
\mathbb{n} = \mathbb{e}_r - \epsilon f'(\theta) \, \mathbb{e}_\theta \, , 
 \mathbb{e}_r \mathbb{e}_\theta \mathbb{v} 
\mathbb{v} = \mathbb{v}^{(0)} + \epsilon \mathbb{v}^{(1)} \, .
 
\mathbb{v} \cdot \mathbb{n} = 
v_r^{(0)} + \epsilon\left( v_r^{(1)} - f'(\theta) v_\theta^{(0)} \right) \, 
 v_r := \mathbb{v} \cdot \mathbb{e}_r v_\theta := \mathbb{v} \cdot \mathbb{e}_\theta \mathbb{v} \cdot \mathbb{n} r=1 
\bigg. \mathbb{v} \cdot \mathbb{n} \bigg|_{r = 1+\epsilon f(\theta)}
= 
[\bigg. \mathbb{v}^{(0)} \cdot \mathbb{e}_r + \epsilon \left( \text{something} \right) ] \bigg|_{r = 1} 
+ \mathcal{O}(\epsilon^2) \, .
","['real-analysis', 'geometry', 'power-series', 'taylor-expansion', 'perturbation-theory']"
33,How is the Dirac delta $\delta (x^3)$ different from $\delta(x)$?,How is the Dirac delta  different from ?,\delta (x^3) \delta(x),"How is Dirac delta $\delta (x^3 )$ different from $\delta (x)$ ? It is my understanding that $$ \delta [\psi]:=\langle\delta,\psi\rangle=\int_{\Omega}\delta(x)\psi(x)\,dx= \begin{cases} \psi(0) & \text{if $0\in \Omega$}\\ 0 & \text{otherwise} \end{cases} $$ But $x^3=0$ iff $x=0$ so does this not imply, $$\delta(x^3) = \delta(x) = 0$$ Apologies if I am completely misunderstanding the topic, I am very new to distributions.","How is Dirac delta different from ? It is my understanding that But iff so does this not imply, Apologies if I am completely misunderstanding the topic, I am very new to distributions.","\delta (x^3 ) \delta (x) 
\delta [\psi]:=\langle\delta,\psi\rangle=\int_{\Omega}\delta(x)\psi(x)\,dx=
\begin{cases}
\psi(0) & \text{if 0\in \Omega}\\
0 & \text{otherwise}
\end{cases}
 x^3=0 x=0 \delta(x^3) = \delta(x) = 0","['real-analysis', 'dirac-delta']"
34,Rescaling a function to equalize impact of input variables,Rescaling a function to equalize impact of input variables,,"Suppose $f:\mathbb{R}^2\to\mathbb{R}$ is a smooth function. Suppose that disturbing one variables changes the function value much more than disturbing the other variable, $$ |f(x+\varepsilon,y)-f(x,y)| \gg |f(x,y+\varepsilon)-f(x,y)| \,. $$ This poor scaling property challenges computer solvers to find the roots of $f$ , see here for Matlab . The matlab team suggests to rescale the function such that '' each coordinate has about the same effect on the objective function ''. Example (from Mathwork). A poorly scaled function is $f(x,y)= 10^6 x^2 + 10^{-6}y^2$ . Instead, we can try to solve $g(x,y)=0$ where $g(x,y)= f\left(\begin{pmatrix} 10^{-3} & 0 \\ 0 & 10^3 \end{pmatrix}\begin{pmatrix}x \\y\end{pmatrix}\right)=f(10^{-3}x,10^3y)$ . Question : Is there a more general approach to rescale a function? In Mathwork's example, $\begin{pmatrix} 10^{-3} & 0 \\ 0 & 10^3 \end{pmatrix}$ is pretty obvious but how could one come up with a suitable rescaling if the problem isn't that obvious? Is there a general hint on how I can equalize the effect of input variables on the function value? While my question is a bit more general, it originates from my problem of finding the roots of \begin{align*} f(x,y) = \begin{pmatrix} \left(\frac{c_1c_3 }{c_1-c_2}\left(\frac{c_2 c_3}{(y-c_6)(c_1-c_2)}\right)^{-\frac{c_2}{c_1}} +c_6 \right) \frac{c_2}{c_1}x^{c_2-c_1}+\frac{x^{1-c_1}}{c_1c_5}-y \\  \left(\frac{c_1c_3 }{c_1-c_2}\left(\frac{c_2 c_3}{(y-c_6)(c_1-c_2)}\right)^{-\frac{c_2}{c_1}} +c_6\right) x^{c_2}+ c_5x - c_7-yx^{c_1} \end{pmatrix}, \end{align*} where all the $c_i$ are a bunch of constants. I asked Matlab to minimise $\|f\|$ but that turned out to be poorly scaled.","Suppose is a smooth function. Suppose that disturbing one variables changes the function value much more than disturbing the other variable, This poor scaling property challenges computer solvers to find the roots of , see here for Matlab . The matlab team suggests to rescale the function such that '' each coordinate has about the same effect on the objective function ''. Example (from Mathwork). A poorly scaled function is . Instead, we can try to solve where . Question : Is there a more general approach to rescale a function? In Mathwork's example, is pretty obvious but how could one come up with a suitable rescaling if the problem isn't that obvious? Is there a general hint on how I can equalize the effect of input variables on the function value? While my question is a bit more general, it originates from my problem of finding the roots of where all the are a bunch of constants. I asked Matlab to minimise but that turned out to be poorly scaled.","f:\mathbb{R}^2\to\mathbb{R} 
|f(x+\varepsilon,y)-f(x,y)| \gg |f(x,y+\varepsilon)-f(x,y)| \,.
 f f(x,y)= 10^6 x^2 + 10^{-6}y^2 g(x,y)=0 g(x,y)= f\left(\begin{pmatrix} 10^{-3} & 0 \\ 0 & 10^3 \end{pmatrix}\begin{pmatrix}x \\y\end{pmatrix}\right)=f(10^{-3}x,10^3y) \begin{pmatrix} 10^{-3} & 0 \\ 0 & 10^3 \end{pmatrix} \begin{align*}
f(x,y) = \begin{pmatrix} \left(\frac{c_1c_3 }{c_1-c_2}\left(\frac{c_2 c_3}{(y-c_6)(c_1-c_2)}\right)^{-\frac{c_2}{c_1}} +c_6 \right) \frac{c_2}{c_1}x^{c_2-c_1}+\frac{x^{1-c_1}}{c_1c_5}-y \\ 
\left(\frac{c_1c_3 }{c_1-c_2}\left(\frac{c_2 c_3}{(y-c_6)(c_1-c_2)}\right)^{-\frac{c_2}{c_1}} +c_6\right) x^{c_2}+ c_5x - c_7-yx^{c_1}
\end{pmatrix},
\end{align*} c_i \|f\|","['real-analysis', 'functions', 'numerical-methods', 'algorithms', 'newton-raphson']"
35,"Is it true that $ S(x)^2 \geq S'(x)\int_{-\infty}^{x}S(t) \, dt$, where $S(x) = A\gamma(x) + \gamma(x-C)$ and $\gamma$ is the standard Gaussian?","Is it true that , where  and  is the standard Gaussian?"," S(x)^2 \geq S'(x)\int_{-\infty}^{x}S(t) \, dt S(x) = A\gamma(x) + \gamma(x-C) \gamma","For $C\geq0$ and $A\geq1$ define the function $$ S(x) = A\gamma(x)+\gamma(x-C) $$ where $\gamma$ denotes the standard Gaussian distribution in $\mathbb R$ , $\gamma(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}$ . Some numerical simulations suggest that the following inequality holds true: $$ S(x)^2 \geq S'(x)\int_{-\infty}^{x}S(t) \, dt \qquad \forall x \in [0,C/2]. $$ I find it hard to prove this. Do you have any idea how to show this inequality?","For and define the function where denotes the standard Gaussian distribution in , . Some numerical simulations suggest that the following inequality holds true: I find it hard to prove this. Do you have any idea how to show this inequality?","C\geq0 A\geq1 
S(x) = A\gamma(x)+\gamma(x-C)
 \gamma \mathbb R \gamma(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2} 
S(x)^2 \geq S'(x)\int_{-\infty}^{x}S(t) \, dt \qquad \forall x \in [0,C/2].
","['real-analysis', 'probability', 'analysis', 'probability-theory']"
36,Must continuous $H^1(\mathbb{R}^2)$ function tend to zero at infinity?,Must continuous  function tend to zero at infinity?,H^1(\mathbb{R}^2),"Here, $H^1(\mathbb{R}^2)$ is the standard Sobolev spaces for $L^2(\mathbb{R}^2)$ functions whose weak derivative belongs to $L^2(\mathbb{R}^2).$ My question in the title comes from calculus of variations. It is usually the case that a minimizer of some given energy functional defined on $H^1(\mathbb{R}^2)$ is known to be continuous (or even $C^2(\mathbb{R}^2))$ . I want to know the behavior of this minimizer at infinity. If $u \in L^2(\mathbb{R}^2),$ then is known $\liminf_{|x| \to \infty} u(x) = 0.$ But it cannot say $\limsup_{|x| \to \infty} u(x) = 0$ since counterexamples exist. If we assume $u \in H^{1+\epsilon}(\mathbb{R}^2)$ for some $\epsilon > 0,$ then the classical Morrey's inequality can imply uniform H\""older continuity of $u.$ So we can conclude $\limsup_{|x| \to \infty} u(x) = 0$ via proof by contradiction. So my problem is about the case $\epsilon = 0.$ That is, when $$u \in H^1(\mathbb{R}^2) \cap C(\mathbb{R}^2),$$ is it true that $$\limsup_{|x| \to \infty} u(x) = 0?$$ Using proof by contradiction, I think this should be true. Here is my non-rigorous argument. Assume not, then there are $\epsilon > 0$ and $x_n \in \mathbb{R}^2$ such that $|x_n| \to \infty$ and $|u(x_n)| \geq 2\epsilon.$ By the continuity, there is $r_n > 0$ such that $|u(x)| \geq \epsilon$ for all $x \in B(x_n, r_n).$ Since $u \in L^2, r_n \to 0$ as $n \to \infty.$ I think non-rigorously that $$ \int_{B(x_n, r_n)} |\nabla u|^2 \gtrsim \int_{B(x_n, r_n)} (\frac{\epsilon}{r_n})^2 = \epsilon^2$$ for large $n$ and $$ \int_{\mathbb{R}^2} |\nabla u|^2 \geq \sum_{n\,\text{is large}} \int_{B(x_n, r_n)} |\nabla u|^2. $$ So they imply a contradiction $\int_{\mathbb{R}^2} |\nabla u|^2 = \infty$ I appreciate any discussion. Edit : How about $u$ is additionally assumed to be $C^1(\mathbb{R}^2)$ or even $C^2(\mathbb{R}^2)?$ Is there any proof or counterexample?","Here, is the standard Sobolev spaces for functions whose weak derivative belongs to My question in the title comes from calculus of variations. It is usually the case that a minimizer of some given energy functional defined on is known to be continuous (or even . I want to know the behavior of this minimizer at infinity. If then is known But it cannot say since counterexamples exist. If we assume for some then the classical Morrey's inequality can imply uniform H\""older continuity of So we can conclude via proof by contradiction. So my problem is about the case That is, when is it true that Using proof by contradiction, I think this should be true. Here is my non-rigorous argument. Assume not, then there are and such that and By the continuity, there is such that for all Since as I think non-rigorously that for large and So they imply a contradiction I appreciate any discussion. Edit : How about is additionally assumed to be or even Is there any proof or counterexample?","H^1(\mathbb{R}^2) L^2(\mathbb{R}^2) L^2(\mathbb{R}^2). H^1(\mathbb{R}^2) C^2(\mathbb{R}^2)) u \in L^2(\mathbb{R}^2), \liminf_{|x| \to \infty} u(x) = 0. \limsup_{|x| \to \infty} u(x) = 0 u \in H^{1+\epsilon}(\mathbb{R}^2) \epsilon > 0, u. \limsup_{|x| \to \infty} u(x) = 0 \epsilon = 0. u \in H^1(\mathbb{R}^2) \cap C(\mathbb{R}^2), \limsup_{|x| \to \infty} u(x) = 0? \epsilon > 0 x_n \in \mathbb{R}^2 |x_n| \to \infty |u(x_n)| \geq 2\epsilon. r_n > 0 |u(x)| \geq \epsilon x \in B(x_n, r_n). u \in L^2, r_n \to 0 n \to \infty.  \int_{B(x_n, r_n)} |\nabla u|^2 \gtrsim \int_{B(x_n, r_n)} (\frac{\epsilon}{r_n})^2 = \epsilon^2 n 
\int_{\mathbb{R}^2} |\nabla u|^2 \geq \sum_{n\,\text{is large}} \int_{B(x_n, r_n)} |\nabla u|^2.
 \int_{\mathbb{R}^2} |\nabla u|^2 = \infty u C^1(\mathbb{R}^2) C^2(\mathbb{R}^2)?","['real-analysis', 'examples-counterexamples', 'sobolev-spaces', 'calculus-of-variations', 'weak-derivatives']"
37,"$g(x) = f(x)\sin\ (1/x)$ being uniformly continuous on $(0, 1]$",being uniformly continuous on,"g(x) = f(x)\sin\ (1/x) (0, 1]","Let $f: (0, 1]\to \mathbb{R}$ be continuous on the domain. I want the condition of $f(x)$ where $g(x) = f(x)\sin(1/x)$ is uniformly continuous on $(0, 1]$ .  I expect that the answer would be $\lim_{x\to\ 0^{+}}f(x)=0$ . When $\lim_{x\to\ 0^{+}}f(x)=0$ , and define $f(0)$ as $0$ , $f(x)$ becomes continuous by the definition, and since $f(x)$ is continuous on [0, 1] which is compact, $f(x)$ is uniformly continuous at (0, 1]. However, I want to know whether the converse holds.  That is, if $g(x)$ is uniformly continuous, is $\lim_{x\to\ 0^{+}}f(x)=0$ ??? Thank you for your help!!","Let be continuous on the domain. I want the condition of where is uniformly continuous on .  I expect that the answer would be . When , and define as , becomes continuous by the definition, and since is continuous on [0, 1] which is compact, is uniformly continuous at (0, 1]. However, I want to know whether the converse holds.  That is, if is uniformly continuous, is ??? Thank you for your help!!","f: (0, 1]\to \mathbb{R} f(x) g(x) = f(x)\sin(1/x) (0, 1] \lim_{x\to\ 0^{+}}f(x)=0 \lim_{x\to\ 0^{+}}f(x)=0 f(0) 0 f(x) f(x) f(x) g(x) \lim_{x\to\ 0^{+}}f(x)=0","['real-analysis', 'continuity', 'uniform-continuity']"
38,About One Problem in Wildt 2018,About One Problem in Wildt 2018,,"The W32 in the 28th József Wildt International Mathematical Competition is as follows: $\text{If }x_{0}=1\text{ and }x_{n+1}^3+1=(x_{n}+1)^3\text{ for all }n\geq0\text{, then }$ $[x_{n}]\!=\mspace{-2mu}n\text{ for all }n\geq1\text{, when }\![\cdot]\mspace{-1mu}\text{ denote the integer part.}$ It has been solved in this post . Hence we know that $\lim\limits_{n\to\infty}\dfrac{x_{n}}{n}=1$ or $$x_n\sim{n}$$ as $n\to\infty$ . But what can we know about $\mathtt{L}\mathop{:=}\lim\limits_{n\to\infty}(x_{n}-n)$ ? Since $\lceil{x_n}\rceil=n+1$ for $n\in\mathbb{N}$ , the problem is only evaluating $\lim\limits_{n\to\infty}\langle{x_n}\rangle$ or determining $$x_n\sim n+\mathtt{L}$$ as $n\to\infty$ , where numerical experiment shows that in fact $0.6<\mathtt{L}<0.7$ . However, can one express $\mathtt{L}$ in terms of some other well-known constants (if it exists)? Thanks.","The W32 in the 28th József Wildt International Mathematical Competition is as follows: It has been solved in this post . Hence we know that or as . But what can we know about ? Since for , the problem is only evaluating or determining as , where numerical experiment shows that in fact . However, can one express in terms of some other well-known constants (if it exists)? Thanks.","\text{If }x_{0}=1\text{ and }x_{n+1}^3+1=(x_{n}+1)^3\text{ for all }n\geq0\text{, then } [x_{n}]\!=\mspace{-2mu}n\text{ for all }n\geq1\text{, when }\![\cdot]\mspace{-1mu}\text{ denote the integer part.} \lim\limits_{n\to\infty}\dfrac{x_{n}}{n}=1 x_n\sim{n} n\to\infty \mathtt{L}\mathop{:=}\lim\limits_{n\to\infty}(x_{n}-n) \lceil{x_n}\rceil=n+1 n\in\mathbb{N} \lim\limits_{n\to\infty}\langle{x_n}\rangle x_n\sim n+\mathtt{L} n\to\infty 0.6<\mathtt{L}<0.7 \mathtt{L}","['real-analysis', 'limits', 'asymptotics']"
39,"Show that if $g\circ f$ is injective, then $f$ must be injective. Show as well that if $g\circ f$ is surjective, then $g$ must be surjective.","Show that if  is injective, then  must be injective. Show as well that if  is surjective, then  must be surjective.",g\circ f f g\circ f g,"Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be functions. Show that if $g\circ f$ is injective, then $f$ must be injective. Is it true that $g$ must also be injective? Show that if $g\circ f$ is surjective, then $g$ must be surjective. Is it true that $f$ must also be surjective? MY ATTEMPT Let us prove the first statement first. Assume $g\circ f$ is injective. Thus we have that \begin{align*} f(x) = f(y) \Longrightarrow g(f(x)) = g(f(y)) \Longrightarrow (g\circ f)(x) = (g\circ f)(y) \Longrightarrow x = y \end{align*} where it has been used the fact that $g\circ f$ is injective. Hence $f$ is injective. However it does not necessarily hold that $g$ is injective. Consider, for instance, the functions $f:\{0,1\}\rightarrow\{0,1,2\}$ such that $f(0) = 0$ and $f(1) = 1$ and $g:\{0,1,2\}\rightarrow\{0,1\}$ such that $g(0) = 0$ , $g(1) = 1$ and $g(2) = 0$ . Consequently, $g\circ f:\{0,1\}\rightarrow\{0,1\}$ is given by $(g\circ f)(0) = 0$ and $(g\circ f)(1) = 1$ . Although $f$ is injective, $g$ is not. We may now proceed and prove the second part. We need to show that $g(Y) = Z$ . We already have that $g(Y)\subseteq Z$ . Therefore we have to prove that $Z\subseteq g(Y)$ . Indeed, one has that \begin{align*} (g\circ f)(X) = g(f(X)) = Z \subseteq g(Y) \end{align*} since $f(X)\subseteq Y$ . Consequently, $g(Y) = Z$ and $g$ is surjective. Similarly, $f$ need not be surjective. Consider, for instance, that $f:\{0,1\}\rightarrow\{0,1,2\}$ is given by $f(0) = 0$ and $f(1) = 1$ and $g:\{0,1,2\}\rightarrow\{0,1\}$ such that $g(0) = 0$ , $g(1) = g(2) = 1$ . We have that $g\circ f:\{0,1\}\rightarrow\{0,1\}$ is given by $(g\circ f)(0) = 0$ and $(g\circ f)(1) = 1$ is surjective as well as $g$ , but $f$ is not surjective. Could someone double-check my solution? Any other counterexamples would be appreciatted.","Let and be functions. Show that if is injective, then must be injective. Is it true that must also be injective? Show that if is surjective, then must be surjective. Is it true that must also be surjective? MY ATTEMPT Let us prove the first statement first. Assume is injective. Thus we have that where it has been used the fact that is injective. Hence is injective. However it does not necessarily hold that is injective. Consider, for instance, the functions such that and and such that , and . Consequently, is given by and . Although is injective, is not. We may now proceed and prove the second part. We need to show that . We already have that . Therefore we have to prove that . Indeed, one has that since . Consequently, and is surjective. Similarly, need not be surjective. Consider, for instance, that is given by and and such that , . We have that is given by and is surjective as well as , but is not surjective. Could someone double-check my solution? Any other counterexamples would be appreciatted.","f:X\rightarrow Y g:Y\rightarrow Z g\circ f f g g\circ f g f g\circ f \begin{align*}
f(x) = f(y) \Longrightarrow g(f(x)) = g(f(y)) \Longrightarrow (g\circ f)(x) = (g\circ f)(y) \Longrightarrow x = y
\end{align*} g\circ f f g f:\{0,1\}\rightarrow\{0,1,2\} f(0) = 0 f(1) = 1 g:\{0,1,2\}\rightarrow\{0,1\} g(0) = 0 g(1) = 1 g(2) = 0 g\circ f:\{0,1\}\rightarrow\{0,1\} (g\circ f)(0) = 0 (g\circ f)(1) = 1 f g g(Y) = Z g(Y)\subseteq Z Z\subseteq g(Y) \begin{align*}
(g\circ f)(X) = g(f(X)) = Z \subseteq g(Y)
\end{align*} f(X)\subseteq Y g(Y) = Z g f f:\{0,1\}\rightarrow\{0,1,2\} f(0) = 0 f(1) = 1 g:\{0,1,2\}\rightarrow\{0,1\} g(0) = 0 g(1) = g(2) = 1 g\circ f:\{0,1\}\rightarrow\{0,1\} (g\circ f)(0) = 0 (g\circ f)(1) = 1 g f","['real-analysis', 'functions', 'solution-verification']"
40,"A question on the possibility of a continuous surjective function from $(a,b) \mapsto [a,b]$",A question on the possibility of a continuous surjective function from,"(a,b) \mapsto [a,b]","$\mathbf{Original \ Question}: $ Let $a,b \in \mathbb{R}$ and $a<b$ . Which of the following statement(s) is/are true? (A) There exists a continuous function $f:[a,b] \to (a,b)$ such that $f$ is one-one (B) There exists a continuous function $f:[a,b] \to (a,b)$ such that $f$ is onto (C) There exists a continuous function $f:(a,b) \to [a,b]$ such that $f$ is one-one (D) There exists a continuous function $f:(a,b) \to [a,b]$ such that $f$ is onto $\mathbf{Attempt}:$ Option $A$ is true. For example, consider $a=0 ,b=1$ , $f(x)=\frac{1}{2(x+1)}$ . Option $C$ is true. $f(x)=x$ with $a=0,b=1$ . Option $B$ cannot be true, Since: $f^{-1}((a,b))=[a,b]$ and by the property of continuous functions, $f^{-1}((a,b))$ must be an open set. But $[a,b]$ is closed. Option $D$ cannot be true since $f^{-1}([a,b])=(a,b)$ must be a closed set. But $(a,b)$ is open. But, in the answer key, $D$ is given as a correct choice. What am I doing wrong here? Any insight is much appreciated. Thank you!","Let and . Which of the following statement(s) is/are true? (A) There exists a continuous function such that is one-one (B) There exists a continuous function such that is onto (C) There exists a continuous function such that is one-one (D) There exists a continuous function such that is onto Option is true. For example, consider , . Option is true. with . Option cannot be true, Since: and by the property of continuous functions, must be an open set. But is closed. Option cannot be true since must be a closed set. But is open. But, in the answer key, is given as a correct choice. What am I doing wrong here? Any insight is much appreciated. Thank you!","\mathbf{Original \ Question}:  a,b \in \mathbb{R} a<b f:[a,b] \to (a,b) f f:[a,b] \to (a,b) f f:(a,b) \to [a,b] f f:(a,b) \to [a,b] f \mathbf{Attempt}: A a=0 ,b=1 f(x)=\frac{1}{2(x+1)} C f(x)=x a=0,b=1 B f^{-1}((a,b))=[a,b] f^{-1}((a,b)) [a,b] D f^{-1}([a,b])=(a,b) (a,b) D","['real-analysis', 'continuity', 'metric-spaces', 'contest-math', 'solution-verification']"
41,Is differentiability defined at an isolated point of the function's domain?,Is differentiability defined at an isolated point of the function's domain?,,"Say we have the function $f:(0,1)\cup\{2\}\rightarrow \mathbb R$ defined by $f(x)=5$ . Is $f$ differentiable or not differentiable (or neither) at $2$ ? Is $f$ a differentiable function?",Say we have the function defined by . Is differentiable or not differentiable (or neither) at ? Is a differentiable function?,"f:(0,1)\cup\{2\}\rightarrow \mathbb R f(x)=5 f 2 f",['real-analysis']
42,Does $\lim_{x\to\infty}\frac{f(x)}{g(x)} = 1$ imply $\lim_{x\to \infty} \frac{f(x+1)-f(x)}{g(x+1)-g(x)} = 1$ for convex functions?,Does  imply  for convex functions?,\lim_{x\to\infty}\frac{f(x)}{g(x)} = 1 \lim_{x\to \infty} \frac{f(x+1)-f(x)}{g(x+1)-g(x)} = 1,"Let $f,g$ be convex functions on $[0,\infty)$ such that $\lim_{x\to\infty}\frac{f(x)}{g(x)} = 1$ and $\lim_{x\to +\infty} g(x) = +\infty$ . Is it always true that $\lim_{x\to \infty} \frac{f(x+1)-f(x)}{g(x+1)-g(x)} = 1$ ? I can prove it when $g(x)=x$ and $g(x) = x^2$ . Edit. It is actually not so hard to show it works more generally when $g(x) = x^\alpha$ for any $\alpha \geq 1$ . The question can also be asked with series as a partial converse to the Stolz-Cesaro Theorem: Let $a$ and $b$ be increasing sequences such that and $\displaystyle\lim_{n\to\infty} \sum_{k=1}^n b_k = +\infty$ . Does $\displaystyle\lim_{n\to\infty} \dfrac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} = 1$ imply $\displaystyle\lim_{n\to\infty} \dfrac{a_n}{b_n} = 1$ ?",Let be convex functions on such that and . Is it always true that ? I can prove it when and . Edit. It is actually not so hard to show it works more generally when for any . The question can also be asked with series as a partial converse to the Stolz-Cesaro Theorem: Let and be increasing sequences such that and . Does imply ?,"f,g [0,\infty) \lim_{x\to\infty}\frac{f(x)}{g(x)} = 1 \lim_{x\to +\infty} g(x) = +\infty \lim_{x\to \infty} \frac{f(x+1)-f(x)}{g(x+1)-g(x)} = 1 g(x)=x g(x) = x^2 g(x) = x^\alpha \alpha \geq 1 a b \displaystyle\lim_{n\to\infty} \sum_{k=1}^n b_k = +\infty \displaystyle\lim_{n\to\infty} \dfrac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} = 1 \displaystyle\lim_{n\to\infty} \dfrac{a_n}{b_n} = 1","['real-analysis', 'sequences-and-series', 'limits', 'convex-analysis', 'divergent-series']"
43,Proving that there exists only one non-negative eigenfuction for the following operator.,Proving that there exists only one non-negative eigenfuction for the following operator.,,"Fix some constants $a>1$ , $\sigma>0$ and consider $x_- = -\sigma/(\alpha-1)$ , $x_+ = \sigma/(\alpha - 1)$ , and $M=[x_-,x_+]$ .  Consider the Banach space $V = \left(\mathcal C^0 (M),|\cdot |_{\infty}\right)$ where $$\mathcal C^0(M) = \{f:M\to\mathbb R;\ f\ \text{is a continuous function}\}, $$ and $$\left|f\right|_\infty = \sup_{x\in M}\left|f(x)\right|. $$ Defining the linear operator \begin{align*} T:V&\to V\\ f&\mapsto\left(x\mapsto \frac{1}{2\sigma} \int_{\frac{1}{\alpha}\left(x-\sigma\right)}^{\frac{1}{\alpha}\left(x+\sigma\right)} f(y)\ \text d y\right), \end{align*} it is ''easy to see'' that $T$ is a continous and compact operator, moreover $r(T)>0$ (where $r(T)$ is the spectral radius of $T$ ). Defining convex cone $K:= \mathcal C^0_+(M) = \{f\in V;\ f(x)\geq 0, \forall\ x\in M\}$ , and noticing that $T(K)\subset K$ and $K-K= V$ , by Krein-Rutman theorem there exists an eigenfunction $g\in K$ , such that $$T(g) = r(T)g. $$ My Question: Is it possible to show that $g$ is the unique eigenfunction of $T$ lying in the cone $K$ ? Comment: I think that the only non-negative eigenfunction of $T$ is the constant function $1$ .","Fix some constants , and consider , , and .  Consider the Banach space where and Defining the linear operator it is ''easy to see'' that is a continous and compact operator, moreover (where is the spectral radius of ). Defining convex cone , and noticing that and , by Krein-Rutman theorem there exists an eigenfunction , such that My Question: Is it possible to show that is the unique eigenfunction of lying in the cone ? Comment: I think that the only non-negative eigenfunction of is the constant function .","a>1 \sigma>0 x_- = -\sigma/(\alpha-1) x_+ = \sigma/(\alpha - 1) M=[x_-,x_+] V = \left(\mathcal C^0 (M),|\cdot |_{\infty}\right) \mathcal C^0(M) = \{f:M\to\mathbb R;\ f\ \text{is a continuous function}\},  \left|f\right|_\infty = \sup_{x\in M}\left|f(x)\right|.  \begin{align*}
T:V&\to V\\
f&\mapsto\left(x\mapsto \frac{1}{2\sigma} \int_{\frac{1}{\alpha}\left(x-\sigma\right)}^{\frac{1}{\alpha}\left(x+\sigma\right)} f(y)\ \text d y\right),
\end{align*} T r(T)>0 r(T) T K:= \mathcal C^0_+(M) = \{f\in V;\ f(x)\geq 0, \forall\ x\in M\} T(K)\subset K K-K= V g\in K T(g) = r(T)g.  g T K T 1","['real-analysis', 'linear-algebra', 'functional-analysis', 'eigenfunctions']"
44,A question about a positive continuous function,A question about a positive continuous function,,"Suppose $f \in C[0, 1]$ satisfies following properties: 1) $f(0) = f(1) = 0$ 2) $\forall x \in (0, 1)$ $f(x) > 0$ Do there always exist such $c$ and $d$ in $(0, 1)$ that $f(c) = f(d) = d - c$ ? I  have tried to consider the function $g(x) = \max\{t \in [0, 1]|f(t) = f(x)\} - x - f(x)$ which is strictly positive in $0$ and negative in the point of largest point of maximum. Thus if that function were always continuous, our problem would have been solved. Unfortunately, it isn’t.","Suppose satisfies following properties: 1) 2) Do there always exist such and in that ? I  have tried to consider the function which is strictly positive in and negative in the point of largest point of maximum. Thus if that function were always continuous, our problem would have been solved. Unfortunately, it isn’t.","f \in C[0, 1] f(0) = f(1) = 0 \forall x \in (0, 1) f(x) > 0 c d (0, 1) f(c) = f(d) = d - c g(x) = \max\{t \in [0, 1]|f(t) = f(x)\} - x - f(x) 0","['real-analysis', 'calculus', 'continuity']"
45,$35.2850899... $ has a closed form ??,has a closed form ??,35.2850899... ,"Consider the function $t(x)$ defined as : $$ x_1 = x $$ $$x_2 = x $$ $$ x_3 = 2 x^2 $$ $$ x_4 = 4 x^4 + 2 x^2 $$ and for $n > 4 $ $$ x_{n} = \frac { x_{n-1}^2 + x_{n-2}^2 + x_{n-3}^2}{x_{n-2} + x_{n-3} + x_{n-4} } $$ If the sequence converges to a constant then we define $t(x) = \lim x_n $ . This function is not so well understood by me. I assume it is analytic for instance. But i have no formal proof. The function grows fast. But I am not sure how fast exactly. ( ofcourse 10 iterations give a good asymptotic and then notice it goes double exponentially fast to its lim ) I have no series expansion , integral representation  , differential equation , continued fraction or such for this. Nor do I have a way to compute these values in a different way such as different iterations , a koenigs type formula , combinatorical methods , fractals , cellular automatons , a bifurcation point , the area of a filled julia set etc etc. practically there is not a big problem for small imput since it converges rapidly.  But the values are mysterious to me. In particular is $t(1)$ transcendental ?? Can we even prove it to be irrational ?? Do we have a closed form for $t(1)$ ? [ main question ! ] This all reminds me of the Somos sequences and the Somos constant ( which has a closed form as the derivative of a lerch form ! ) Assuming it is analytic , how does its analytic continuation look like ? Is analytic continuation possible to any complex number or its neighbourhood , or is there a natural boundary ? numerically we have $$t(1) = 35.2850889...$$ Do you recognize this number ?? Is there a closed form ??? Has this been studied before ?? Edit Some remark or motivation : Notice that when you naively try to analyse this , after you know it converges fast you get Set all $x_n = y$ for large $ n $ and some $ y > x > 1.$ $$ y = \frac{y^2 + y^2 + y^2}{y + y + y} $$ So $$ y = 3 y^2 / (3y) $$ $$ y = y^2 / y $$ This is a tautology like $ z = z $ . Useless. It seems all naive methods lead to such tautologies. This is the one of the simplest possible with sum of squares ; rational function iterations of degree $2.$ This only informally suggests the dependance of the starting value $x $ ofcourse. Also there seems no link to iterations that solves equations like Newton iterations. Therefore the recursion fascinates me. Also notice $ t(x) $ is strictly increasing for $ x > 1.$","Consider the function defined as : and for If the sequence converges to a constant then we define . This function is not so well understood by me. I assume it is analytic for instance. But i have no formal proof. The function grows fast. But I am not sure how fast exactly. ( ofcourse 10 iterations give a good asymptotic and then notice it goes double exponentially fast to its lim ) I have no series expansion , integral representation  , differential equation , continued fraction or such for this. Nor do I have a way to compute these values in a different way such as different iterations , a koenigs type formula , combinatorical methods , fractals , cellular automatons , a bifurcation point , the area of a filled julia set etc etc. practically there is not a big problem for small imput since it converges rapidly.  But the values are mysterious to me. In particular is transcendental ?? Can we even prove it to be irrational ?? Do we have a closed form for ? [ main question ! ] This all reminds me of the Somos sequences and the Somos constant ( which has a closed form as the derivative of a lerch form ! ) Assuming it is analytic , how does its analytic continuation look like ? Is analytic continuation possible to any complex number or its neighbourhood , or is there a natural boundary ? numerically we have Do you recognize this number ?? Is there a closed form ??? Has this been studied before ?? Edit Some remark or motivation : Notice that when you naively try to analyse this , after you know it converges fast you get Set all for large and some So This is a tautology like . Useless. It seems all naive methods lead to such tautologies. This is the one of the simplest possible with sum of squares ; rational function iterations of degree This only informally suggests the dependance of the starting value ofcourse. Also there seems no link to iterations that solves equations like Newton iterations. Therefore the recursion fascinates me. Also notice is strictly increasing for",t(x)  x_1 = x  x_2 = x   x_3 = 2 x^2   x_4 = 4 x^4 + 2 x^2  n > 4   x_{n} = \frac { x_{n-1}^2 + x_{n-2}^2 + x_{n-3}^2}{x_{n-2} + x_{n-3} + x_{n-4} }  t(x) = \lim x_n  t(1) t(1) t(1) = 35.2850889... x_n = y  n   y > x > 1.  y = \frac{y^2 + y^2 + y^2}{y + y + y}   y = 3 y^2 / (3y)   y = y^2 / y   z = z  2. x   t(x)   x > 1.,"['real-analysis', 'complex-analysis', 'closed-form', 'recursion', 'convergence-acceleration']"
46,Irrational Integral,Irrational Integral,,I've tried in many ways to compute this integral but I'm not able to find any solution. Even Wolfram can not compute this. So my question is: Is that even possible to compute? $$\int { \frac { dx }{ 1+\sqrt { x } +\sqrt { x+1 } +\sqrt { x+2 }  }  }$$,I've tried in many ways to compute this integral but I'm not able to find any solution. Even Wolfram can not compute this. So my question is: Is that even possible to compute?,\int { \frac { dx }{ 1+\sqrt { x } +\sqrt { x+1 } +\sqrt { x+2 }  }  },"['real-analysis', 'integration', 'indefinite-integrals']"
47,"Equivalence of Continuous Monotonic Functions $[0, 1] \to [0, 1]$",Equivalence of Continuous Monotonic Functions,"[0, 1] \to [0, 1]","A couple of later clarifications to my original post ... the monotonic non-decreasing functions mapping $[0, 1] \to [0, 1]$ are surjective - it appears from the comments that this was misunderstood. Prof. Raussen has kindly assisted me in understanding the proofs in the paper, and I will at some point post an answer giving the proof. In outline, the result depends on three others...... a) Given a countable set of points in [0, 1] - ""stop values"" - one can construct an element of $\mathscr M$ with non-trivial (closed) intervals - ""stop intervals"" - that map to these points. b) an element of $\mathscr M$ has at most countably many stop values. c) If the stop values of $\phi \subset $ stop values of $\eta$ then there is $\psi$ such that $\eta = \phi \circ \psi$ (all functions being elements of $\mathscr M$ ). Let $\mathscr M$ be the set of continuous monotonic non-decreasing functions mapping $[0, 1] \to [0, 1]$ . Then under the operation of composition $\mathscr M$ is a monoid - a group without inverse: it is easily seen that it is closed, associative and has an identity ( $i: [0, 1] \to [0, 1], i(t) = t$ ). One also sees that all such functions are surjective (e.g. intermediate value theorem). In the paper ""Reparametrizations of continuous paths - Ulrich Fahrenberg and Martin Raussen"" https://arxiv.org/pdf/0706.3560.pdf it seems to be proven that given any $f, g \in \mathscr M$ there are $\mu, \nu \in \mathscr M$ such that $f \circ \mu = g \circ \nu$ . I must admit the paper is a little beyond me, and I'm looking for a simple proof for this . Background. The paper shows among other things that re-parameterisation of paths is an equivalence relation, which then formalizes the definition of a curve as an equivalence class of paths. Showing symmetry and reflexivity is easy - the proof above is needed to show transitivity. (The paper also shows that every path is equivalent to a regular path - i.e. one which does not ""stop"" at any point. An alternate proof for this can be found here https://math.stackexchange.com/q/3317511 .)","A couple of later clarifications to my original post ... the monotonic non-decreasing functions mapping are surjective - it appears from the comments that this was misunderstood. Prof. Raussen has kindly assisted me in understanding the proofs in the paper, and I will at some point post an answer giving the proof. In outline, the result depends on three others...... a) Given a countable set of points in [0, 1] - ""stop values"" - one can construct an element of with non-trivial (closed) intervals - ""stop intervals"" - that map to these points. b) an element of has at most countably many stop values. c) If the stop values of stop values of then there is such that (all functions being elements of ). Let be the set of continuous monotonic non-decreasing functions mapping . Then under the operation of composition is a monoid - a group without inverse: it is easily seen that it is closed, associative and has an identity ( ). One also sees that all such functions are surjective (e.g. intermediate value theorem). In the paper ""Reparametrizations of continuous paths - Ulrich Fahrenberg and Martin Raussen"" https://arxiv.org/pdf/0706.3560.pdf it seems to be proven that given any there are such that . I must admit the paper is a little beyond me, and I'm looking for a simple proof for this . Background. The paper shows among other things that re-parameterisation of paths is an equivalence relation, which then formalizes the definition of a curve as an equivalence class of paths. Showing symmetry and reflexivity is easy - the proof above is needed to show transitivity. (The paper also shows that every path is equivalent to a regular path - i.e. one which does not ""stop"" at any point. An alternate proof for this can be found here https://math.stackexchange.com/q/3317511 .)","[0, 1] \to [0, 1] \mathscr M \mathscr M \phi \subset  \eta \psi \eta = \phi \circ \psi \mathscr M \mathscr M [0, 1] \to [0, 1] \mathscr M i: [0, 1] \to [0, 1], i(t) = t f, g \in \mathscr M \mu, \nu \in \mathscr M f \circ \mu = g \circ \nu","['real-analysis', 'general-topology', 'alternative-proof', 'curves']"
48,"Differentiability of $ G(x)=\int_{\mathbb R} e^{tx}f(t)dt $ on $ (0,1) $",Differentiability of  on," G(x)=\int_{\mathbb R} e^{tx}f(t)dt   (0,1) ","Let $f\colon\mathbb R\to\mathbb R$ be a non-negative and measurable function, and assume that both $$\int_{\mathbb R} f(t)dt<\infty\ \ \text{and}\ \ \int_{\mathbb R}e^tf(t)<\infty.$$ Show that the integral $G(x)=\int_{\mathbb R} e^{tx}f(t)dt$ is finite when $0\le x\le 1$ . Then prove that the funtion $G(x)$ is continuous on $0\le x\le 1$ , and differentiable on $0<x<1$ . My attempt: It is trivial to show that $G(x)<\infty$ when $0\le x\le 1$ . To show that $G(x)$ is continuous on $0\le x\le 1$ , we need to consider the difference: \begin{align} G(x_2)-G(x_1)&=\int_{\mathbb R}e^{tx_2}f(t)dt-\int_{\mathbb R}e^{tx_1}f(t)dt\\ &=\int_{\mathbb R}(e^{tx_2}-e^{tx_1})f(t)dt \end{align} where $x_1,x_2\in [0,1]$ . Note that $$ |(e^{tx_2}-e^{tx_1})f(t)|\le\max\{2f(t), 2e^tf(t),f(t)+e^tf(t)\}\in L^1(\mathbb R), $$ it follows that $$ \lim_{x_2\to x_1} [G(x_2)-G(x_1)]=\int_{\mathbb R}0\cdot f(t)dt=0 $$ i.e., $G(x)$ is continuous on $0\le x\le 1$ . Next, we study the differentiability of $G(x)$ on $(0,1)$ . We have \begin{align} \frac{G(x_2)-G(x_1)}{x_2-x_1}=\int_{\mathbb R}\frac{e^{tx_2}-e^{tx_1}}{x_2-x_1}f(t)dt \end{align} and $$ \frac{e^{tx_2}-e^{tx_1}}{x_2-x_1}f(t)=\frac{e^{tx_2}-e^{tx_1}}{tx_2-tx_1}tf(t) .$$ If we let $x_2\to x_1$ , then $$\lim_{x_2\to x_1}\frac{e^{tx_2}-e^{tx_1}}{x_2-x_1}f(t)=e^{tx_1}tf(t).$$ But this time, I cannot find a dominating integrable function $g(t)$ such that $|e^{tx_1}tf(t)|<g(t)$ on $\mathbb R$ . Then how to prove the differentiability of $G(x)$ on $(0,1)$ ?","Let be a non-negative and measurable function, and assume that both Show that the integral is finite when . Then prove that the funtion is continuous on , and differentiable on . My attempt: It is trivial to show that when . To show that is continuous on , we need to consider the difference: where . Note that it follows that i.e., is continuous on . Next, we study the differentiability of on . We have and If we let , then But this time, I cannot find a dominating integrable function such that on . Then how to prove the differentiability of on ?","f\colon\mathbb R\to\mathbb R \int_{\mathbb R} f(t)dt<\infty\ \ \text{and}\ \ \int_{\mathbb R}e^tf(t)<\infty. G(x)=\int_{\mathbb R} e^{tx}f(t)dt 0\le x\le 1 G(x) 0\le x\le 1 0<x<1 G(x)<\infty 0\le x\le 1 G(x) 0\le x\le 1 \begin{align}
G(x_2)-G(x_1)&=\int_{\mathbb R}e^{tx_2}f(t)dt-\int_{\mathbb R}e^{tx_1}f(t)dt\\
&=\int_{\mathbb R}(e^{tx_2}-e^{tx_1})f(t)dt
\end{align} x_1,x_2\in [0,1]  |(e^{tx_2}-e^{tx_1})f(t)|\le\max\{2f(t), 2e^tf(t),f(t)+e^tf(t)\}\in L^1(\mathbb R),   \lim_{x_2\to x_1} [G(x_2)-G(x_1)]=\int_{\mathbb R}0\cdot f(t)dt=0  G(x) 0\le x\le 1 G(x) (0,1) \begin{align}
\frac{G(x_2)-G(x_1)}{x_2-x_1}=\int_{\mathbb R}\frac{e^{tx_2}-e^{tx_1}}{x_2-x_1}f(t)dt
\end{align}  \frac{e^{tx_2}-e^{tx_1}}{x_2-x_1}f(t)=\frac{e^{tx_2}-e^{tx_1}}{tx_2-tx_1}tf(t) . x_2\to x_1 \lim_{x_2\to x_1}\frac{e^{tx_2}-e^{tx_1}}{x_2-x_1}f(t)=e^{tx_1}tf(t). g(t) |e^{tx_1}tf(t)|<g(t) \mathbb R G(x) (0,1)","['real-analysis', 'derivatives']"
49,Derivative of double integral,Derivative of double integral,,"I am trying to find out what the derivative $\frac{\mathrm d}{\mathrm dx}f(x)$ of the function $$f(x)=\int_0^{x^2}\left(\int_{a-x}^{a+x}\sin(a^2+b^2-x^2)\,\mathrm db\right)\mathrm da$$ is. Using the Leibniz rule twice, I get $$f'(x)=-2x\int_0^{x^2}\left(\int_{a-x}^{a+x}\cos(a^2+b^2-x^2)\,\mathrm db\right)\mathrm da+2\int_0^{x^2}\cos(2a^2)\sin(2ax)\,\mathrm da+2x\int_{x^2-x}^{x^2+x}\sin(x^4-x^2+b^2)\,\mathrm db.$$ This does not look like it had an analytic expression. Are there maybe some symmetries I did not see, or is there a better approach than the Leibniz rule to solve this?","I am trying to find out what the derivative of the function is. Using the Leibniz rule twice, I get This does not look like it had an analytic expression. Are there maybe some symmetries I did not see, or is there a better approach than the Leibniz rule to solve this?","\frac{\mathrm d}{\mathrm dx}f(x) f(x)=\int_0^{x^2}\left(\int_{a-x}^{a+x}\sin(a^2+b^2-x^2)\,\mathrm db\right)\mathrm da f'(x)=-2x\int_0^{x^2}\left(\int_{a-x}^{a+x}\cos(a^2+b^2-x^2)\,\mathrm db\right)\mathrm da+2\int_0^{x^2}\cos(2a^2)\sin(2ax)\,\mathrm da+2x\int_{x^2-x}^{x^2+x}\sin(x^4-x^2+b^2)\,\mathrm db.","['real-analysis', 'calculus', 'integration', 'analysis', 'derivatives']"
50,Tricky limit as $n$ tends to infinity of an expression involving a bunch of roots,Tricky limit as  tends to infinity of an expression involving a bunch of roots,n,"In this question , @user513057 asked how to prove that for $n$ large enough, $$ (n+1)\cdot ((n+1)!)^{\frac{1}{n+1}} -n\cdot (n!)^\frac{1}{n}< n+1 $$ In the answer by @Von Neumann, the latter rewrote this inequality as $$ [2 \pi (n+1)]^{1/(2(n+1))}\frac{n+1}{e} - \frac{n^2}{e(n+1)}(2\pi n)^{\frac1{2n}} < 1 $$ Then he argued, that one can replace $n+1$ by $n$ if $n$ is large enough. I don't see how to formally justify this though. In order to do this, I would like to determine the limit $$ \lim_{x\to\infty} [2 \pi (x+1)]^{1/(2(x+1))}\frac{x+1}{e} - \frac{x^2}{e(x+1)}(2\pi x)^{\frac{1}{2x}}. $$ Wolfram Alpha says that this limit equals $\frac2e$ . I don't see any way of proving this though. I tried computing the derivative of the above function in order to show that it is decreasing, but that didn't lead to any results. I also tried computing the difference of the $n+1$ -st and the $n$ -th term, but that didn't work either. EDIT: This is equivalent to proving that $$ \lim_{x\to\infty} \frac1x\left((x+1)\cdot\frac{x+1}e\cdot \sqrt[2(x+1)]{2\pi(x+1)}-x\cdot\frac{x}e\cdot \sqrt[2x]{2\pi x}\right)=\frac2e $$ or equivalently that $$ \lim_{x\to\infty} \frac1x\left((x+1)^2\cdot \sqrt[2(x+1)]{2\pi(x+1)}-x^2\cdot \sqrt[2x]{2\pi x}\right)=2 $$ or rewritten again that $$ \lim_{x\to\infty} \frac{(x+1)^2}x\cdot \sqrt[2(x+1)]{2\pi(x+1)}-x\cdot \sqrt[2x]{2\pi x}=2 $$ Wolfram Alpha query for the last limit . EDIT 2:   Using l'Hospital, one could also prove that \begin{multline} \frac12 (4x-\ln(2\pi (x+1))+5) (2\pi (x+1))^{1/(2 x+2)}\\-2 (2\pi x)^{1/(2x)}(x-(1/4)\ln(2\pi x)+\frac14) \end{multline} converges to $2$ . Third Wolfram Alpha query","In this question , @user513057 asked how to prove that for large enough, In the answer by @Von Neumann, the latter rewrote this inequality as Then he argued, that one can replace by if is large enough. I don't see how to formally justify this though. In order to do this, I would like to determine the limit Wolfram Alpha says that this limit equals . I don't see any way of proving this though. I tried computing the derivative of the above function in order to show that it is decreasing, but that didn't lead to any results. I also tried computing the difference of the -st and the -th term, but that didn't work either. EDIT: This is equivalent to proving that or equivalently that or rewritten again that Wolfram Alpha query for the last limit . EDIT 2:   Using l'Hospital, one could also prove that converges to . Third Wolfram Alpha query","n 
(n+1)\cdot ((n+1)!)^{\frac{1}{n+1}} -n\cdot (n!)^\frac{1}{n}< n+1
 
[2 \pi (n+1)]^{1/(2(n+1))}\frac{n+1}{e} - \frac{n^2}{e(n+1)}(2\pi n)^{\frac1{2n}} < 1
 n+1 n n 
\lim_{x\to\infty} [2 \pi (x+1)]^{1/(2(x+1))}\frac{x+1}{e} - \frac{x^2}{e(x+1)}(2\pi x)^{\frac{1}{2x}}.
 \frac2e n+1 n 
\lim_{x\to\infty} \frac1x\left((x+1)\cdot\frac{x+1}e\cdot \sqrt[2(x+1)]{2\pi(x+1)}-x\cdot\frac{x}e\cdot \sqrt[2x]{2\pi x}\right)=\frac2e
 
\lim_{x\to\infty} \frac1x\left((x+1)^2\cdot \sqrt[2(x+1)]{2\pi(x+1)}-x^2\cdot \sqrt[2x]{2\pi x}\right)=2
 
\lim_{x\to\infty} \frac{(x+1)^2}x\cdot \sqrt[2(x+1)]{2\pi(x+1)}-x\cdot \sqrt[2x]{2\pi x}=2
 \begin{multline}
\frac12 (4x-\ln(2\pi (x+1))+5) (2\pi (x+1))^{1/(2 x+2)}\\-2 (2\pi x)^{1/(2x)}(x-(1/4)\ln(2\pi x)+\frac14)
\end{multline} 2","['real-analysis', 'calculus']"
51,Difference between gradient and derivative.,Difference between gradient and derivative.,,"My question may be a bit stupid, but this morning I tried to explain the gradient to someone, and he makes a parallel with derivative of function $f:\mathbb R\to \mathbb R$ . What he says is that for a function $f:\mathbb R\to \mathbb R$ , the gradient and the derivative are the same. I agree that the scalar value are the same, but I'mnot sure that the meaning behind is the same. For example, take $f(x)=x^2$ . For me the gradient of $f$ is going to be the vector field $\nabla f(x)=2x\cdot 1$ , where $1$ is the basis of $\mathbb R$ , so it should look like that whereas the derivative $f'(x)$ is really the rate of the function, and if it would be a vecteur field, it would be a vector field over the range of $f$ , and not on the domain of $f$ as the gradient is. What do you think ? To illustrate, I would say that the derivative field is in red and blue, and the gradient is in pink.","My question may be a bit stupid, but this morning I tried to explain the gradient to someone, and he makes a parallel with derivative of function . What he says is that for a function , the gradient and the derivative are the same. I agree that the scalar value are the same, but I'mnot sure that the meaning behind is the same. For example, take . For me the gradient of is going to be the vector field , where is the basis of , so it should look like that whereas the derivative is really the rate of the function, and if it would be a vecteur field, it would be a vector field over the range of , and not on the domain of as the gradient is. What do you think ? To illustrate, I would say that the derivative field is in red and blue, and the gradient is in pink.",f:\mathbb R\to \mathbb R f:\mathbb R\to \mathbb R f(x)=x^2 f \nabla f(x)=2x\cdot 1 1 \mathbb R f'(x) f f,['real-analysis']
52,Convergence of harmonic functions in $L^1$ implies uniform convergence on compact sets,Convergence of harmonic functions in  implies uniform convergence on compact sets,L^1,"Resorting to an analog of what's done here , I'm trying to prove the following statement: Let $u_m: \mathbb{R}^n \to \mathbb{R}$ be a sequence of harmonic    functions and suppose there exists a continuous function $u:  \mathbb{R}^n \to \mathbb{R}$ such that $u_m$ converges to $u$ in $L^1$ in any compact subset, i.e., for every limited $A\subset  \mathbb{R}^n$ , we have $$ \int_A \left\vert u_m(x) -  u(x)\right\vert~dx \to 0, $$ as $m\to \infty$ . Show such convergence is uniform in compact sets. My attempt: Let $\Omega \subset \mathbb{R}^n$ be an open set and $(u_m)_{m \in \mathbb{N}} \subset C^{\infty}(\Omega)$ . If $\Omega$ is open, then $\Omega^c = \mathbb{R}^n \setminus\Omega$ is closed, so we may take $r = \frac{1}{2}dist(A, \Omega^c)$ and define $V = \cup_{x_0 \in A} B(x_0, r)$ such that $\tilde{A} = \overline{V}$ , where $B(x_0, r)$ denotes the ball of center $x_0$ and radius $r$ . At this point, I want to say there exists $n \in \mathbb{N}$ such that $\forall x_0 \in A$ , with $r$ as defined above, so that we may apply the mean value-property to $u_m(x_0) - u(x_0)$ in the following manner: $$ |u_m(x_0) - u(x_0)| \leq \frac{1}{|B(x_0, r)|} \int_{B(x_0, r)} |u_m(y) - u(y)| ~dy \leq \int_{\tilde{A}} |u_m(y) - u(y)| ~dy \to 0, $$ as $m \to \infty$ , by the $L^1$ convergence hypothesis. Since $y \in \tilde{A}$ is arbitrary on the RHS of the last inequality, we'd conclude the convergence is uniform and the proof would be finished. However, the given function $u$ is not harmonic, only continuous, so we're not able to apply the mean-value theorem the way I described and follow on accordingly with the proof above. Is there any way to correct this? If not, then how may I prove such uniform convergence?","Resorting to an analog of what's done here , I'm trying to prove the following statement: Let be a sequence of harmonic    functions and suppose there exists a continuous function such that converges to in in any compact subset, i.e., for every limited , we have as . Show such convergence is uniform in compact sets. My attempt: Let be an open set and . If is open, then is closed, so we may take and define such that , where denotes the ball of center and radius . At this point, I want to say there exists such that , with as defined above, so that we may apply the mean value-property to in the following manner: as , by the convergence hypothesis. Since is arbitrary on the RHS of the last inequality, we'd conclude the convergence is uniform and the proof would be finished. However, the given function is not harmonic, only continuous, so we're not able to apply the mean-value theorem the way I described and follow on accordingly with the proof above. Is there any way to correct this? If not, then how may I prove such uniform convergence?","u_m: \mathbb{R}^n \to \mathbb{R} u:
 \mathbb{R}^n \to \mathbb{R} u_m u L^1 A\subset
 \mathbb{R}^n  \int_A \left\vert u_m(x) -
 u(x)\right\vert~dx \to 0,  m\to \infty \Omega \subset \mathbb{R}^n (u_m)_{m \in \mathbb{N}} \subset C^{\infty}(\Omega) \Omega \Omega^c = \mathbb{R}^n \setminus\Omega r = \frac{1}{2}dist(A, \Omega^c) V = \cup_{x_0 \in A} B(x_0, r) \tilde{A} = \overline{V} B(x_0, r) x_0 r n \in \mathbb{N} \forall x_0 \in A r u_m(x_0) - u(x_0) 
|u_m(x_0) - u(x_0)| \leq \frac{1}{|B(x_0, r)|} \int_{B(x_0, r)} |u_m(y) - u(y)| ~dy \leq \int_{\tilde{A}} |u_m(y) - u(y)| ~dy \to 0,
 m \to \infty L^1 y \in \tilde{A} u","['real-analysis', 'partial-differential-equations', 'uniform-convergence', 'harmonic-functions']"
53,Prove that $T$ is uniquely ergodic,Prove that  is uniquely ergodic,T,"Let $T:X\rightarrow X$ be a continuous map on a compact metric space $(X,d)$ . Suppose that $\mu$ is ergodic with respect to $T$ and for every $x\in X$ there exists a constant $C=C(x)$ such that for every $f \in C(X), f \geq  0$ , \begin{align*} \limsup_{N \rightarrow \infty} \frac{1}{N} \sum_{n=0}^{N-1} f (T^nx) \leq C \int f d\mu. \end{align*} Show that $T$ is uniquely ergodic. I knew the following theorem $\textbf{Theorem}$ the following properties are equivalent. (i) $T$ is uniquely ergodic (ii) For every $f\in C(X)$ , \begin{align*} A_N^f:=\frac{1}{N} \sum_{n=0}^{N-1} f(T^nx) \rightarrow C_f,  \end{align*} where $C_f$ is a constant independent of $x$ . I don't know how to induce the convergence of $A_N^f$ from the assumption in the problem. Any help is appreciated... Thank you!!","Let be a continuous map on a compact metric space . Suppose that is ergodic with respect to and for every there exists a constant such that for every , Show that is uniquely ergodic. I knew the following theorem the following properties are equivalent. (i) is uniquely ergodic (ii) For every , where is a constant independent of . I don't know how to induce the convergence of from the assumption in the problem. Any help is appreciated... Thank you!!","T:X\rightarrow X (X,d) \mu T x\in X C=C(x) f \in C(X), f \geq  0 \begin{align*}
\limsup_{N \rightarrow \infty} \frac{1}{N} \sum_{n=0}^{N-1} f (T^nx) \leq C \int f d\mu.
\end{align*} T \textbf{Theorem} T f\in C(X) \begin{align*}
A_N^f:=\frac{1}{N} \sum_{n=0}^{N-1} f(T^nx) \rightarrow C_f, 
\end{align*} C_f x A_N^f",['real-analysis']
54,From $\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| < \infty$ to $f\equiv 0$,From  to,\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| < \infty f\equiv 0,"Given $f\in C[0,\Lambda]$ satisfying $$\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| < \infty$$ Prove that $f\equiv 0$ $\,\forall x\in[0,\Lambda]$ I found a weaker proposition If $f\in C[0,1]$ satisfies $$ \left|\int_0^1 e^{nx} f(x) dx \right| =0\,\,\,\forall n\in \mathbb{N}$$ then $f\equiv 0$ $\,\forall x\in[0,1]$ But the solution of that doesn't seem to work here. My attempt $f(\Lambda)=0$ Suppose for contradiction that $f(\Lambda)\ne 0$ . WLOG, we assume that $f(\Lambda)>0$ . Then there exists $\varepsilon > 0$ such that $f(x)>\frac{f(\Lambda)}{2}\,\,\forall x \in [\Lambda-\varepsilon,\Lambda]$ . Denote $M = \sup_{[0,\Lambda]}f$ and $c=\frac{f(\Lambda)}{2}$ . \begin{align} \int_0^\Lambda e^{nx} f(x) dx &= \int_0^{\Lambda-\varepsilon} e^{nx} f(x) dx + \int_{\Lambda-\varepsilon}^\Lambda e^{nx} f(x) dx \\ &\ge c\int_{\Lambda-\varepsilon}^\Lambda e^{nx} dx - M\int_0^{\Lambda-\varepsilon} e^{nx} dx \\ &= c\left( \frac{e^{n\Lambda}}{n} - \frac{e^{n(\Lambda-\varepsilon)}}{n}  \right) - M \left( \frac{e^{n(\Lambda-\varepsilon)}}{n} - \frac{1}{n} \right) \end{align} Thus $$ \lim_{n\to \infty} \int_0^\Lambda e^{nx} f(x) dx = +\infty $$ Contradiction. Put $X= \left\{ m : f \equiv 0 \,\, \forall x \in [m,\Lambda] \right\}$ . I aim to show $\inf X = 0$ . Suppose for contradiction that $\inf X = m > 0$ If there exists $\delta>0$ such that $f(x)>0$ or $f(x)<0$ $\forall x \in ]m-\delta,m[$ , using the method in $1.$ leads to a contradiction. But how to deal with the functions like $$f(x) = (\Lambda -x) \sin \frac{1}{\Lambda - x}$$ of which we can't find such $\delta$ ? I would highly appreciate it if you could share any thoughts on how to solve this problem. Thanks in advance! Added Here is a proof. This solution completely solved the trouble I encountered. But I don't quite understand how we can figure out the lemma. I would highly appreciate it if you could give me some hints to figure it out, or post a new approach. Proof $\ $ It suffices to show that \begin{gather} \int_{\Lambda-\lambda}^\Lambda f(x)dx=0 \quad \forall \lambda \in ]0,\Lambda]  \tag{1} \end{gather} We prove $(1)$ via the following lemma, of which we attach a proof at the end. Lemma \begin{gather} \lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds = \int_0^\lambda \phi \quad \forall \lambda \in [0,\Lambda[ \nonumber \end{gather} Choose $\phi(s)=f(\Lambda - s)$ , and then from lemma we have $\forall \lambda \in [0,\Lambda[$ $$ \lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} f(\Lambda-s) ds = \int_0^\lambda f(\Lambda-s)ds  $$ $$ \lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} e^{kx(\lambda-\Lambda)} \int_0^\Lambda e^{kxu} f(u) du = \int_{\Lambda-\lambda}^\Lambda f(s)ds  $$ Denote $\displaystyle\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| = C$ . Thus \begin{align} \left|\int_{\Lambda-\lambda}^\Lambda f(s)ds\right| \nonumber &\le C \lim_{x \uparrow \infty} \left(-1 + \sum_{k=0}^\infty \frac{1}{k!} e^{kx(\lambda-\Lambda)}\right) \nonumber \\ &\le C \lim_{x \uparrow \infty} \left(-1 + \exp{\{e^{x(\lambda-\Lambda)}\}} \right) \nonumber \\ &= 0 \nonumber \end{align} Done. Now we attach a proof of the lemma. Proof of lemma $\ $ We aim to check \begin{align} \lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds &\overset{1}{=} \lim_{x \uparrow \infty} \int_0^\Lambda \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} e^{kx(\lambda-s)} \phi(s) ds \nonumber \\ &= \lim_{x \uparrow \infty} \int_0^\Lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\ &\overset{2}{=} \int_0^\Lambda \lim_{x \uparrow \infty} \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\ &= \int_0^\lambda \phi \nonumber \end{align} Denote $$ I_N = \sum_{k=1}^N \frac{(-1)^{k-1}}{k!} \int_0^\lambda e^{kx(\lambda-s)} \phi(s) ds $$ $$ J_N = \sum_{k=1}^N \frac{(-1)^{k-1}}{k!} \int_\lambda^\Lambda e^{kx(\lambda-s)} \phi(s) ds $$ Then we have \begin{align} I_N  &= \int_0^\lambda \left( 1-\sum_{k=0}^\infty \frac{ (-1)^k }{k!}e^{kx(\lambda-s)} + \sum_{k=N+1}^\infty \frac{ (-1)^k }{k!}e^{kx(\lambda-s)} \right) \phi(s) ds \nonumber \\ &= \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + \int_0^\lambda \sum_{k=N+1}^\infty \frac{ (-1)^k }{k!} e^{kx(\lambda-s)} \phi(s) ds \nonumber \\ &= : \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + G_N \nonumber \end{align} Note that \begin{align} |G_N| &\le ||\phi||_\infty \int_0^\lambda \sum_{k=N+1}^\infty \frac{ e^{kx(\lambda-s)} }{k!} ds \nonumber \\ &= ||\phi||_\infty \sum_{k=N+1}^\infty \int_0^\lambda \frac{ e^{kxu} }{k!} du \nonumber \\ &= ||\phi||_\infty \sum_{k=N+1}^\infty \frac{ e^{kx\lambda}-1 }{xk \cdot k!} \nonumber \end{align} which implies that $$ \lim_{N \uparrow \infty} |G_N| = 0 $$ i.e. $$ \lim_{N \uparrow \infty} I_N = \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds $$ And note that \begin{align} |J_N| &\le ||\phi||_\infty \int_\lambda^\Lambda \sum_{k=1}^\infty \frac{ e^{kx(\lambda-s)} }{k!} ds \nonumber \\ &\le ||\phi||_\infty \int_0^{\Lambda-\lambda} e^{-xu} du \nonumber \\ &= ||\phi||_\infty \frac{1-e^{x(\lambda-\Lambda)} }{x} \nonumber \end{align} Thus \begin{align} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds &= \lim_{N \uparrow \infty} \left( I_N + J_N \right) \nonumber \\ &= \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + O(\frac{1}{x}) \nonumber \end{align} which implies that $$ \lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds = \lim_{x \uparrow \infty} \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\ $$ It remains to prove that $$ R:=\lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{x(\lambda-s)} \}} \phi(s) ds = 0 $$ Note that $$ |R| \le ||\phi||_\infty \lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{xu} \}} du  $$ and $$ \int_0^\lambda \exp{\{ -e^{xu} \}} e^{ux} du  = \frac{ \frac{1}{e}-\exp{ \{ -e^{x\lambda} \} }}{x} \nonumber  \ge \int_0^\lambda \exp{\{ -e^{xu} \}} du \nonumber $$ Thus we have $$ |R| \le ||\phi||_\infty \lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{xu} \}} du  \le ||\phi||_\infty \lim_{x \uparrow \infty} \frac{1}{ex} = 0 $$ Done.","Given satisfying Prove that I found a weaker proposition If satisfies then But the solution of that doesn't seem to work here. My attempt Suppose for contradiction that . WLOG, we assume that . Then there exists such that . Denote and . Thus Contradiction. Put . I aim to show . Suppose for contradiction that If there exists such that or , using the method in leads to a contradiction. But how to deal with the functions like of which we can't find such ? I would highly appreciate it if you could share any thoughts on how to solve this problem. Thanks in advance! Added Here is a proof. This solution completely solved the trouble I encountered. But I don't quite understand how we can figure out the lemma. I would highly appreciate it if you could give me some hints to figure it out, or post a new approach. Proof It suffices to show that We prove via the following lemma, of which we attach a proof at the end. Lemma Choose , and then from lemma we have Denote . Thus Done. Now we attach a proof of the lemma. Proof of lemma We aim to check Denote Then we have Note that which implies that i.e. And note that Thus which implies that It remains to prove that Note that and Thus we have Done.","f\in C[0,\Lambda] \sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| < \infty f\equiv 0 \,\forall x\in[0,\Lambda] f\in C[0,1]  \left|\int_0^1 e^{nx} f(x) dx \right| =0\,\,\,\forall n\in \mathbb{N} f\equiv 0 \,\forall x\in[0,1] f(\Lambda)=0 f(\Lambda)\ne 0 f(\Lambda)>0 \varepsilon > 0 f(x)>\frac{f(\Lambda)}{2}\,\,\forall x \in [\Lambda-\varepsilon,\Lambda] M = \sup_{[0,\Lambda]}f c=\frac{f(\Lambda)}{2} \begin{align}
\int_0^\Lambda e^{nx} f(x) dx
&= \int_0^{\Lambda-\varepsilon} e^{nx} f(x) dx + \int_{\Lambda-\varepsilon}^\Lambda e^{nx} f(x) dx \\
&\ge c\int_{\Lambda-\varepsilon}^\Lambda e^{nx} dx - M\int_0^{\Lambda-\varepsilon} e^{nx} dx \\
&= c\left( \frac{e^{n\Lambda}}{n} - \frac{e^{n(\Lambda-\varepsilon)}}{n} 
\right) - M \left( \frac{e^{n(\Lambda-\varepsilon)}}{n} - \frac{1}{n} \right)
\end{align} 
\lim_{n\to \infty} \int_0^\Lambda e^{nx} f(x) dx = +\infty
 X= \left\{ m : f \equiv 0 \,\, \forall x \in [m,\Lambda] \right\} \inf X = 0 \inf X = m > 0 \delta>0 f(x)>0 f(x)<0 \forall x \in ]m-\delta,m[ 1. f(x) = (\Lambda -x) \sin \frac{1}{\Lambda - x} \delta \  \begin{gather}
\int_{\Lambda-\lambda}^\Lambda f(x)dx=0 \quad \forall \lambda \in ]0,\Lambda]  \tag{1}
\end{gather} (1) \begin{gather}
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds = \int_0^\lambda \phi \quad \forall \lambda \in [0,\Lambda[ \nonumber
\end{gather} \phi(s)=f(\Lambda - s) \forall \lambda \in [0,\Lambda[ 
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} f(\Lambda-s) ds = \int_0^\lambda f(\Lambda-s)ds 
 
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} e^{kx(\lambda-\Lambda)} \int_0^\Lambda e^{kxu} f(u) du = \int_{\Lambda-\lambda}^\Lambda f(s)ds 
 \displaystyle\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| = C \begin{align}
\left|\int_{\Lambda-\lambda}^\Lambda f(s)ds\right| \nonumber
&\le C \lim_{x \uparrow \infty} \left(-1 + \sum_{k=0}^\infty \frac{1}{k!} e^{kx(\lambda-\Lambda)}\right) \nonumber \\
&\le C \lim_{x \uparrow \infty} \left(-1 + \exp{\{e^{x(\lambda-\Lambda)}\}} \right) \nonumber \\
&= 0 \nonumber
\end{align} \  \begin{align}
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds
&\overset{1}{=}
\lim_{x \uparrow \infty} \int_0^\Lambda \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} e^{kx(\lambda-s)} \phi(s) ds \nonumber \\
&=
\lim_{x \uparrow \infty} \int_0^\Lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\
&\overset{2}{=}
\int_0^\Lambda \lim_{x \uparrow \infty} \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\
&=
\int_0^\lambda \phi \nonumber
\end{align} 
I_N = \sum_{k=1}^N \frac{(-1)^{k-1}}{k!} \int_0^\lambda e^{kx(\lambda-s)} \phi(s) ds
 
J_N = \sum_{k=1}^N \frac{(-1)^{k-1}}{k!} \int_\lambda^\Lambda e^{kx(\lambda-s)} \phi(s) ds
 \begin{align}
I_N 
&=
\int_0^\lambda \left( 1-\sum_{k=0}^\infty \frac{ (-1)^k }{k!}e^{kx(\lambda-s)} + \sum_{k=N+1}^\infty \frac{ (-1)^k }{k!}e^{kx(\lambda-s)} \right) \phi(s) ds \nonumber \\
&=
\int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + \int_0^\lambda \sum_{k=N+1}^\infty \frac{ (-1)^k }{k!} e^{kx(\lambda-s)} \phi(s) ds \nonumber \\
&= :
\int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + G_N \nonumber
\end{align} \begin{align}
|G_N|
&\le ||\phi||_\infty \int_0^\lambda \sum_{k=N+1}^\infty \frac{ e^{kx(\lambda-s)} }{k!} ds \nonumber \\
&= ||\phi||_\infty \sum_{k=N+1}^\infty \int_0^\lambda \frac{ e^{kxu} }{k!} du \nonumber \\
&= ||\phi||_\infty \sum_{k=N+1}^\infty \frac{ e^{kx\lambda}-1 }{xk \cdot k!} \nonumber
\end{align} 
\lim_{N \uparrow \infty} |G_N| = 0
 
\lim_{N \uparrow \infty} I_N = \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds
 \begin{align}
|J_N|
&\le ||\phi||_\infty \int_\lambda^\Lambda \sum_{k=1}^\infty \frac{ e^{kx(\lambda-s)} }{k!} ds \nonumber \\
&\le ||\phi||_\infty \int_0^{\Lambda-\lambda} e^{-xu} du \nonumber \\
&= ||\phi||_\infty \frac{1-e^{x(\lambda-\Lambda)} }{x} \nonumber
\end{align} \begin{align}
\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds
&=
\lim_{N \uparrow \infty} \left( I_N + J_N \right) \nonumber \\
&=
\int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + O(\frac{1}{x}) \nonumber
\end{align} 
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds
=
\lim_{x \uparrow \infty} \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\
 
R:=\lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{x(\lambda-s)} \}} \phi(s) ds = 0
 
|R|
\le
||\phi||_\infty \lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{xu} \}} du 
 
\int_0^\lambda \exp{\{ -e^{xu} \}} e^{ux} du 
= \frac{ \frac{1}{e}-\exp{ \{ -e^{x\lambda} \} }}{x} \nonumber 
\ge \int_0^\lambda \exp{\{ -e^{xu} \}} du \nonumber
 
|R|
\le
||\phi||_\infty \lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{xu} \}} du 
\le
||\phi||_\infty \lim_{x \uparrow \infty} \frac{1}{ex} = 0
","['real-analysis', 'functional-analysis', 'banach-spaces']"
55,The identity theorem at the boundary (complex analysis),The identity theorem at the boundary (complex analysis),,"Let $\mathbb{D}^2$ be the closed unit disk, and let $f:\mathbb{D}^2 \to \mathbb{C}$ be a smooth map, which is holomorphic on the open unit disk $\text{int}(\mathbb{D}^2)$ . Suppose that there exists a sequence $z_ n \in \text{int}(\mathbb{D}^2)$ , $z_n \to z_0 \in \partial \mathbb{D}^2$ such that $f(z_n)=0$ . Is $f$ identically zero on $ \mathbb{D}^2$ ? The usual formulation of the identity theorem is for open connected domains; it states that a holomorphic function whose zero set has an accumulation point (inside the open domain) is identically zero. Note that I assumed that $f$ is smooth on the closed disk. (In a sense it is ""holomorphic"" at the boundary too, as the condition of being conformal is a closed one). Edit: If $f$ could be extend $f$ holomorphically to an open neighbourhood of $\mathbb D^2$ , then the answer would be positive, by the usual identity theorem (as the accumulation point would now be in the interior of the new extended domain). I am not sure if such an extension is always possible. There are certainly continuous examples that cannot be extended: e.g. $ f(z) = \sum_{n=1}^\infty \frac{z^{n!}}{n!}$ . (See here for details). However, I don't know any smooth example which cannot be extended.","Let be the closed unit disk, and let be a smooth map, which is holomorphic on the open unit disk . Suppose that there exists a sequence , such that . Is identically zero on ? The usual formulation of the identity theorem is for open connected domains; it states that a holomorphic function whose zero set has an accumulation point (inside the open domain) is identically zero. Note that I assumed that is smooth on the closed disk. (In a sense it is ""holomorphic"" at the boundary too, as the condition of being conformal is a closed one). Edit: If could be extend holomorphically to an open neighbourhood of , then the answer would be positive, by the usual identity theorem (as the accumulation point would now be in the interior of the new extended domain). I am not sure if such an extension is always possible. There are certainly continuous examples that cannot be extended: e.g. . (See here for details). However, I don't know any smooth example which cannot be extended.",\mathbb{D}^2 f:\mathbb{D}^2 \to \mathbb{C} \text{int}(\mathbb{D}^2) z_ n \in \text{int}(\mathbb{D}^2) z_n \to z_0 \in \partial \mathbb{D}^2 f(z_n)=0 f  \mathbb{D}^2 f f f \mathbb D^2  f(z) = \sum_{n=1}^\infty \frac{z^{n!}}{n!},"['real-analysis', 'complex-analysis', 'differential-topology', 'analytic-functions', 'smooth-functions']"
56,Convergence in the weak-star sense of measures and $ \int_{\Omega} \sqrt{1+u_k^2} \to \int_{\Omega} \sqrt {1+u_k^2}$ gives convergence in $L^1$.,Convergence in the weak-star sense of measures and  gives convergence in ., \int_{\Omega} \sqrt{1+u_k^2} \to \int_{\Omega} \sqrt {1+u_k^2} L^1,"I have a question, which is exercise 1.20 of the following book: Functions of Bounded Variation and Free Discontinuity Problems. Let assume $\Omega$ is a bounded subset of $\mathbb{R}^n$ and $u_k, u \in L^1(\Omega)$ and $u_k$ converge to $u$ in the weak star sense of measures as follow: $$ \int_{\Omega} u_k \phi \to  \int_{\Omega} u \phi \qquad \forall \phi \in C_c^{\infty}(\Omega).$$ Also assume that $$ \int_{\Omega} \sqrt{1+u_k^2} \to  \int_{\Omega} \sqrt{1+u^2}.$$ Then we want to show that we have strong convergence in $L^1(\Omega)$ too. There is a hint which says first show that $$\sqrt{1+u_k^2}+\sqrt{1+u^2}-2\sqrt{1+\left(\frac{u+u_k}{2}\right)^2}  \to 0 $$ in $L^1(\Omega)$ . This is okay but I don't know how to use this last one to conclude the result.","I have a question, which is exercise 1.20 of the following book: Functions of Bounded Variation and Free Discontinuity Problems. Let assume is a bounded subset of and and converge to in the weak star sense of measures as follow: Also assume that Then we want to show that we have strong convergence in too. There is a hint which says first show that in . This is okay but I don't know how to use this last one to conclude the result.","\Omega \mathbb{R}^n u_k, u \in L^1(\Omega) u_k u  \int_{\Omega} u_k \phi \to  \int_{\Omega} u \phi \qquad \forall \phi \in C_c^{\infty}(\Omega).  \int_{\Omega} \sqrt{1+u_k^2} \to  \int_{\Omega} \sqrt{1+u^2}. L^1(\Omega) \sqrt{1+u_k^2}+\sqrt{1+u^2}-2\sqrt{1+\left(\frac{u+u_k}{2}\right)^2} 
\to 0  L^1(\Omega)","['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations', 'lp-spaces']"
57,Prove that $x^{y^x} > y^{x^y}$ for $x > y > 1$,Prove that  for,x^{y^x} > y^{x^y} x > y > 1,"Prove that $x^{y^x} > y^{x^y}$ for $x > y > 1$ . So I've tried this so far: $x^{y^x} > y^{x^y}$ $e^{(y^x)\ln x} > e^{(x^y)\ln y}$ $(y^x)\ln x > (x^y)\ln y$ $e^{\ln({(y^x)\ln x)}} > e^{\ln({(x^y)\ln y})}$ $x\ln(y) + \ln(\ln x) > y\ln x + \ln(\ln y)$ And well, I got stuck there. Is there something I'm doing wrong or should I try a different approach? Edit: $x\ln(y) -  y\ln(x) > \ln(\ln y)-\ln(\ln x) $ $x\ln(y) -  y\ln(x) > \ln{\left(\frac{\ln y}{\ln x}\right)}$ $e^{x\ln y -  y\ln x} > \frac{\ln y}{\ln x}$ Since $\frac{\ln y}{\ln x} < 1$ , it would suffice to show that $\frac{e^{x\ln(y)}}{e^{y\ln(x)}} > 1$ . So: $x\ln y> y\ln x$ $\frac{x}{y} > \frac{\ln x}{\ln y}$ How do I prove the last bit though?","Prove that for . So I've tried this so far: And well, I got stuck there. Is there something I'm doing wrong or should I try a different approach? Edit: Since , it would suffice to show that . So: How do I prove the last bit though?",x^{y^x} > y^{x^y} x > y > 1 x^{y^x} > y^{x^y} e^{(y^x)\ln x} > e^{(x^y)\ln y} (y^x)\ln x > (x^y)\ln y e^{\ln({(y^x)\ln x)}} > e^{\ln({(x^y)\ln y})} x\ln(y) + \ln(\ln x) > y\ln x + \ln(\ln y) x\ln(y) -  y\ln(x) > \ln(\ln y)-\ln(\ln x)  x\ln(y) -  y\ln(x) > \ln{\left(\frac{\ln y}{\ln x}\right)} e^{x\ln y -  y\ln x} > \frac{\ln y}{\ln x} \frac{\ln y}{\ln x} < 1 \frac{e^{x\ln(y)}}{e^{y\ln(x)}} > 1 x\ln y> y\ln x \frac{x}{y} > \frac{\ln x}{\ln y},"['real-analysis', 'exponentiation']"
58,Proof of Baby Rudin Theorem 2.43,Proof of Baby Rudin Theorem 2.43,,"I came up with the following fleshed out proof of Theorem 2.43 from Principles of Mathematical Analysis. Although this is a duplicate of several other questions, I found many of the other proofs of 2.43 on StackExchange verbose or unclear (and I didn't see this proof covered in the Harvey Mudd or Scripps YouTube lectures), so I hope that this version of the proof can serve as a more helpful reference to anyone else who gets stuck. At the same time, I'm not confident that my reasoning is sound, so I hope that if there are any major flaws or oversights in my proof that a critical reader can point them out, or confirm that I'm correct about property (iii). Theorem 2.43 Let $P$ be a nonempty perfect set in $\mathbb{R}^k$ . Then $P$ is uncountable. Proof : Since $P$ is non-empty, and every point of $P$ is a limit point, $P$ contains at least one limit point. Hence, $P$ is infinite (by 2.20). Suppose $P$ is countable and denote the points of $P$ by $x_1, x_2, x_3, \dots$ We can construct a sequence $\{V_n\}$ of neighborhoods as follows: Let $V_1$ be any neighborhood of $x_1$ . Then $V_1 \cap P$ is non-empty, because $x_1$ is a limit point of $P$ . If $V_1$ consists of all $y \in \mathbb{R}^k$ such that $|y-x_1| < r$ , the closure $\overline{V_1}$ of $V_1$ is $\{y | |y-x_1| \le r\}$ (note: I will not prove this rigorously, but see Lemma 1 below for a sketch of the proof). Suppose $V_n$ has been constructed so that $V_n \cap P$ is not empty and $V_n = N_r(p)$ for some $p \in P$ . Since every point of $P$ is a limit point of $P$ , there is a neighborhood $V_{n+1}$ such that (i) $\overline{V_{n+1}} \subset V_n$ , (ii) $x_n \not \in \overline{V_{n+1}}$ , (iii) $V_{n+1} \cap P$ is not empty, and $V_{n+1} = N_r(p)$ for some point in $P$ . By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed. We now show how we can always construct such a neighborhood $V_{n+1}$ . Let's say that $V_n = N_{r_n}(p_n)$ for some $p_n \in P$ . Just a quick note: it's a common point of confusion that people believe that $x_n$ must be in $V_n$ . While $x_1 \in V_1$ , we cannot assume that this holds true for any other $x_n$ , and at no point does the proof assume this (or need to assume this). Let $p_{n+1}$ be some point in $V_n \cap P$ , such that (a.) $p_{n+1} \not = p_{n}$ ,  (b.) $p_{n+1} \not = x_n$ , and (c.) $p_{n+1} \not = x_{n+1}$ . It follows from Lemmas 2 and 3 that such a point exists. Lemma 2: For $n>1$ , there exists some point $q \in V_n \cap P,$ such that $q \not = p_n$ , and such that $d(p_n, q) < d(p_n, x_n)$ and $d(p_n, q) < d(p_n, x_{n+1})$ . Suppose not. Note that for $n > 1$ , we have that $p_n \not = x_n$ and $p_n \not = x_{n+1}$ (by our choice of $p_n$ ). Therefore, $d(p_n, x_n) > 0$ and $d(p_n, x_{n+1}) > 0$ . So there is some neighborhood of $p_n$ (namely, the neighborhood with radius $r = d(p_n, x)$ , where $x \in \{x_n, x_{n+1}\}$ ) such that the only point of that neighborhood in $P$ is the point $p_n$ , which contradicts our assumption that $p_n$ is a limit point of $P$ . Lemma 3: For $n=1$ , there exists some point $q \in V_1 \cap P,$ such that $q \not = p_1$ , $d(p_1, q) < d(p_1, x_2)$ . Suppose not. Note that $p_1 = x_1$ . Therefore, $d(p_1, x_2) = d(x_1, x_2) > 0$ . So there is some neighborhood of $p_n$ (namely, the neighborhood with radius $r = d(p_1, x_2))$ , such that the only point of that neighborhood in $P$ is the point $p_1$ , which contradicts our assumption that $p_1$ is a limit point of $P$ . Let $V_{n+1} = N_{r_{n+1}}(p_{n+1})$ with $r_{n+1}$ chosen subject to the following conditions. (1.) $r_{n+1} \le d(p_{n+1}, x_n)$ , and (2.) $r_{n+1} < r_n - d(p_n, p_{n+1})$ . By our choice of $p_{n+1}, r_{n+1}$ , and $V_{n+1} = N_{r_{n+1}}(p_{n+1})$ we have the following: (I) $V_{n+1}$ satisfies (i) : If $y \in \overline{V_{n+1}}$ , then $d(p_n, y)$ $\le d(p_n, p_{n+1}) + d(p_{n+1}, y)$ [by the properties of a metric space] $\le d(p_n, p_{n+1}) + r_{n+1}$ $< d(p_n, p_{n+1}) + r_n - d(p_n, p_{n+1})$ [by our choice of $r_{n+1}]$ $= r_n$ . Thus, $y \in V_n$ . Hence, $V_{n+1}$ satisfies (i). (II) $V_{n+1}$ satisfies (ii) : If $y \in V_{n+1}$ , then $d(p_n, y) < r_{n+1} \le d(p_{n+1}, x_n)$ . Thus, $x_n \not \in V_{n+1}$ . Hence, $V_{n+1}$ satisfies (ii). (III) $V_{n+1}$ satisfies (iii) : Because $p_{n+1}$ was chosen to be in $P$ , we have that $N_r(p_{n+1}) \cap P$ is non-empty for all neighborhoods of $p_{n+1}$ . Thus, $V_{n+1}$ satisfies (iii). Let $K_n = \overline{V_n} \cap P$ . Since $\overline{V_n}$ is closed and bounded in $\mathbb{R^k}$ , $\overline{V_n}$ is compact (by 2.41). $P$ is closed (because $P$ is perfect). Thus, $\overline{V_n} \cap P$ is closed (by 2.24(b)). Hence $K_n$ is compact (by 2.35, because $K_n$ is a closed subset of a compact set). Since $x_n \not \in K_{n+1}$ , no point of $P$ lies in $\cap_{1}^{\infty} K_n$ (this is implied by the fact that $P$ is countable, hence for every $x_i \in P$ , there is a $K_{i+1}$ that excludes $x_i$ from $\cap_{1}^{\infty} K_n$ ). But $K_n \subset P$ , so this implies that $\cap_{1}^{\infty} K_n$ is empty. But each $K_n$ is nonempty (by (iii), and $K_n \supset K_{n+1}$ (by (i)). But this contradicts the corollary to 2.36. The theorem follows. Lemma 1: $y$ is a limit point of $V_1$ if and only if $|y - x_1| = r$ or $y \in V_1$ . Proof Sketch: suppose $|y - x_1| > r$ . Then $|y - x_1| = r + \epsilon$ for some $\epsilon > 0$ . So $N_{\epsilon/2}(y) \cap V_1 = \emptyset$ and $y$ is not a limit point of $V_1$ . Now suppose $|y - x_1| = r$ . Then every neighborhood of $y$ contains a point in $V_1$ , so $y$ is a limit point of $V_1$ ).","I came up with the following fleshed out proof of Theorem 2.43 from Principles of Mathematical Analysis. Although this is a duplicate of several other questions, I found many of the other proofs of 2.43 on StackExchange verbose or unclear (and I didn't see this proof covered in the Harvey Mudd or Scripps YouTube lectures), so I hope that this version of the proof can serve as a more helpful reference to anyone else who gets stuck. At the same time, I'm not confident that my reasoning is sound, so I hope that if there are any major flaws or oversights in my proof that a critical reader can point them out, or confirm that I'm correct about property (iii). Theorem 2.43 Let be a nonempty perfect set in . Then is uncountable. Proof : Since is non-empty, and every point of is a limit point, contains at least one limit point. Hence, is infinite (by 2.20). Suppose is countable and denote the points of by We can construct a sequence of neighborhoods as follows: Let be any neighborhood of . Then is non-empty, because is a limit point of . If consists of all such that , the closure of is (note: I will not prove this rigorously, but see Lemma 1 below for a sketch of the proof). Suppose has been constructed so that is not empty and for some . Since every point of is a limit point of , there is a neighborhood such that (i) , (ii) , (iii) is not empty, and for some point in . By (iii), satisfies our induction hypothesis, and the construction can proceed. We now show how we can always construct such a neighborhood . Let's say that for some . Just a quick note: it's a common point of confusion that people believe that must be in . While , we cannot assume that this holds true for any other , and at no point does the proof assume this (or need to assume this). Let be some point in , such that (a.) ,  (b.) , and (c.) . It follows from Lemmas 2 and 3 that such a point exists. Lemma 2: For , there exists some point such that , and such that and . Suppose not. Note that for , we have that and (by our choice of ). Therefore, and . So there is some neighborhood of (namely, the neighborhood with radius , where ) such that the only point of that neighborhood in is the point , which contradicts our assumption that is a limit point of . Lemma 3: For , there exists some point such that , . Suppose not. Note that . Therefore, . So there is some neighborhood of (namely, the neighborhood with radius , such that the only point of that neighborhood in is the point , which contradicts our assumption that is a limit point of . Let with chosen subject to the following conditions. (1.) , and (2.) . By our choice of , and we have the following: (I) satisfies (i) : If , then [by the properties of a metric space] [by our choice of . Thus, . Hence, satisfies (i). (II) satisfies (ii) : If , then . Thus, . Hence, satisfies (ii). (III) satisfies (iii) : Because was chosen to be in , we have that is non-empty for all neighborhoods of . Thus, satisfies (iii). Let . Since is closed and bounded in , is compact (by 2.41). is closed (because is perfect). Thus, is closed (by 2.24(b)). Hence is compact (by 2.35, because is a closed subset of a compact set). Since , no point of lies in (this is implied by the fact that is countable, hence for every , there is a that excludes from ). But , so this implies that is empty. But each is nonempty (by (iii), and (by (i)). But this contradicts the corollary to 2.36. The theorem follows. Lemma 1: is a limit point of if and only if or . Proof Sketch: suppose . Then for some . So and is not a limit point of . Now suppose . Then every neighborhood of contains a point in , so is a limit point of ).","P \mathbb{R}^k P P P P P P P x_1, x_2, x_3, \dots \{V_n\} V_1 x_1 V_1 \cap P x_1 P V_1 y \in \mathbb{R}^k |y-x_1| < r \overline{V_1} V_1 \{y | |y-x_1| \le r\} V_n V_n \cap P V_n = N_r(p) p \in P P P V_{n+1} \overline{V_{n+1}} \subset V_n x_n \not \in \overline{V_{n+1}} V_{n+1} \cap P V_{n+1} = N_r(p) P V_{n+1} V_{n+1} V_n = N_{r_n}(p_n) p_n \in P x_n V_n x_1 \in V_1 x_n p_{n+1} V_n \cap P p_{n+1} \not = p_{n} p_{n+1} \not = x_n p_{n+1} \not = x_{n+1} n>1 q \in V_n \cap P, q \not = p_n d(p_n, q) < d(p_n, x_n) d(p_n, q) < d(p_n, x_{n+1}) n > 1 p_n \not = x_n p_n \not = x_{n+1} p_n d(p_n, x_n) > 0 d(p_n, x_{n+1}) > 0 p_n r = d(p_n, x) x \in \{x_n, x_{n+1}\} P p_n p_n P n=1 q \in V_1 \cap P, q \not = p_1 d(p_1, q) < d(p_1, x_2) p_1 = x_1 d(p_1, x_2) = d(x_1, x_2) > 0 p_n r = d(p_1, x_2)) P p_1 p_1 P V_{n+1} = N_{r_{n+1}}(p_{n+1}) r_{n+1} r_{n+1} \le d(p_{n+1}, x_n) r_{n+1} < r_n - d(p_n, p_{n+1}) p_{n+1}, r_{n+1} V_{n+1} = N_{r_{n+1}}(p_{n+1}) V_{n+1} y \in \overline{V_{n+1}} d(p_n, y) \le d(p_n, p_{n+1}) + d(p_{n+1}, y) \le d(p_n, p_{n+1}) + r_{n+1} < d(p_n, p_{n+1}) + r_n - d(p_n, p_{n+1}) r_{n+1}] = r_n y \in V_n V_{n+1} V_{n+1} y \in V_{n+1} d(p_n, y) < r_{n+1} \le d(p_{n+1}, x_n) x_n \not \in V_{n+1} V_{n+1} V_{n+1} p_{n+1} P N_r(p_{n+1}) \cap P p_{n+1} V_{n+1} K_n = \overline{V_n} \cap P \overline{V_n} \mathbb{R^k} \overline{V_n} P P \overline{V_n} \cap P K_n K_n x_n \not \in K_{n+1} P \cap_{1}^{\infty} K_n P x_i \in P K_{i+1} x_i \cap_{1}^{\infty} K_n K_n \subset P \cap_{1}^{\infty} K_n K_n K_n \supset K_{n+1} y V_1 |y - x_1| = r y \in V_1 |y - x_1| > r |y - x_1| = r + \epsilon \epsilon > 0 N_{\epsilon/2}(y) \cap V_1 = \emptyset y V_1 |y - x_1| = r y V_1 y V_1","['real-analysis', 'general-topology']"
59,Prove that a function is smooth if it is smooth in almost all directions,Prove that a function is smooth if it is smooth in almost all directions,,"Question So suppose we have a function $f:\mathbb R^2\to \mathbb R$ for which it is given that $x\mapsto f(x,g(x))$ is smooth (i.e., $C^\infty$ ) for all smooth functions $g:\mathbb R\to\mathbb R$ . Can we prove that $f$ is smooth as well? I don't know whether this statement is true and honestly I wouldn't be surprised either way. What I've tried already Fix a point $(x_0,y_0)$ . Intuitively, by taking $g(x) = \lambda x$ with $\lambda\in\mathbb R$ we see that $f$ should be at least differentiable along all directions $(1,\lambda)$ at $(x_0,y_0)$ . This follows for instance by considering the curve $t\mapsto (x_0+t, y_0+\lambda t)$ . Thus the only direction that is non-trivial is the vertical direction $(0,1)$ . If we can show that $f$ is also differentiable in that direction then I'm confident that it will be possible to show that $f$ is differentiable. But how can we show whether $f$ is differentiable along $(0,1)$ ? We cannot do it directly from the fact that $f(x,g(x))$ is smooth, but perhaps we can use a limiting argument, letting the slope of the curve $(t,g(t))$ tend to infinity? When we know that $f$ is differentiable, it will probably be possible using an inductive argument to prove that $f$ is smooth (i.e., $C^\infty$ ). Any help is appreciated. EDIT. If found a closely related result, namely Boman's theorem , which says basically says that $f$ is smooth if and only if $f\circ\gamma$ is smooth for all smooth curves $\gamma:\mathbb R\to\mathbb R^2$ . I feel like the statement of my question should probably be reducible to this theorem. The only difficulty is that we don't necessarily know if our $f$ is differentiable along vertical curves, but perhaps this follows in some way.","Question So suppose we have a function for which it is given that is smooth (i.e., ) for all smooth functions . Can we prove that is smooth as well? I don't know whether this statement is true and honestly I wouldn't be surprised either way. What I've tried already Fix a point . Intuitively, by taking with we see that should be at least differentiable along all directions at . This follows for instance by considering the curve . Thus the only direction that is non-trivial is the vertical direction . If we can show that is also differentiable in that direction then I'm confident that it will be possible to show that is differentiable. But how can we show whether is differentiable along ? We cannot do it directly from the fact that is smooth, but perhaps we can use a limiting argument, letting the slope of the curve tend to infinity? When we know that is differentiable, it will probably be possible using an inductive argument to prove that is smooth (i.e., ). Any help is appreciated. EDIT. If found a closely related result, namely Boman's theorem , which says basically says that is smooth if and only if is smooth for all smooth curves . I feel like the statement of my question should probably be reducible to this theorem. The only difficulty is that we don't necessarily know if our is differentiable along vertical curves, but perhaps this follows in some way.","f:\mathbb R^2\to \mathbb R x\mapsto f(x,g(x)) C^\infty g:\mathbb R\to\mathbb R f (x_0,y_0) g(x) = \lambda x \lambda\in\mathbb R f (1,\lambda) (x_0,y_0) t\mapsto (x_0+t, y_0+\lambda t) (0,1) f f f (0,1) f(x,g(x)) (t,g(t)) f f C^\infty f f\circ\gamma \gamma:\mathbb R\to\mathbb R^2 f",['real-analysis']
60,"linear isometric embedding from $(\mathbb{R}^2, \| \|_2)$ to $(l^1, \| \|_1)$",linear isometric embedding from  to,"(\mathbb{R}^2, \| \|_2) (l^1, \| \|_1)","I would like to prove the following : There isn't a linear isometric embedding from $(\mathbb{R}^2, \| \cdot \|_2)$ to $(l^1, \| \cdot \|_1)$ I don't know how to prove this. So far I am able to prove this result only in the case where the vector $(1,0)$ and $(0,1)$ are sent to sequences that have all positive, or all negative value. In this case I use the fact that the $2$ norm is not linear whereas the $1$ norm is (ie $\| xa + yb \| = xa + yb$ , $x, y, a, b > 0$ ). The problem is that when the sequences have different signs i's hard for me to conclude. Thank you.","I would like to prove the following : There isn't a linear isometric embedding from to I don't know how to prove this. So far I am able to prove this result only in the case where the vector and are sent to sequences that have all positive, or all negative value. In this case I use the fact that the norm is not linear whereas the norm is (ie , ). The problem is that when the sequences have different signs i's hard for me to conclude. Thank you.","(\mathbb{R}^2, \| \cdot \|_2) (l^1, \| \cdot \|_1) (1,0) (0,1) 2 1 \| xa + yb \| = xa + yb x, y, a, b > 0","['real-analysis', 'linear-algebra', 'general-topology', 'functional-analysis', 'normed-spaces']"
61,Can the constant $3$ in the Vitali covering lemma be replaced by any positive constant less than that in the finite case?,Can the constant  in the Vitali covering lemma be replaced by any positive constant less than that in the finite case?,3,"I am stuck with a question that the constant $3$ in the Vitali covering lemma can not be replaced by any positive constant less than that in the finite case. Observe that this question is different from the question of why $3$ is considered as bad constant because I am not considering infinite case here. For example, I was thinking in real line and the argument can be generalised in the higher dimension; if I have $(0,4)$ and constant is $c<3$ then taking the interval $(8,4+\frac{3-c}{2})$ or anything would not give me my answer. So I was doubting the validity of the statement in a metric space but I don't know whether I am missing anything or not!!","I am stuck with a question that the constant in the Vitali covering lemma can not be replaced by any positive constant less than that in the finite case. Observe that this question is different from the question of why is considered as bad constant because I am not considering infinite case here. For example, I was thinking in real line and the argument can be generalised in the higher dimension; if I have and constant is then taking the interval or anything would not give me my answer. So I was doubting the validity of the statement in a metric space but I don't know whether I am missing anything or not!!","3 3 (0,4) c<3 (8,4+\frac{3-c}{2})","['real-analysis', 'general-topology', 'measure-theory', 'metric-spaces', 'real-numbers']"
62,"Computing Fréchet derivative of $F(f)(x) = \int^{x}_{0} \cos(f(t)^{2})dt, x \in [0,1]$",Computing Fréchet derivative of,"F(f)(x) = \int^{x}_{0} \cos(f(t)^{2})dt, x \in [0,1]","Let $X = \mathcal{C} \left( [0,1] \right)$ be the Banach space of continuous functions on $[0,1]$ (with the supremum norm) and define a map $F : X \rightarrow X$ by $$F(f)(x) = \int^{x}_{0} \cos(f(t)^{2})dt, x \in [0,1].$$ Show that $F$ is Fréchet differentiable and compute the Fréchet derivative $DF|_{f}$ for each $f \in X.$ So far I have the following. Using the identity $\cos(A+\varepsilon B)=\cos(A)-\varepsilon B\sin(A)+\mathcal{O}(\varepsilon^2).$ We have \begin{align*}F(f+h)(x) &=\int^{x}_{0} \cos((f(t)+h(t))^{2})dt \\ &=\int^{x}_{0}\cos(f^2(t))-h(t)[2f(t)h(t)]\sin(f^2(t))+\mathcal{O}(h^2(t))dt \\ &=F(f)(x)+\int^{x}_{0}-2f(t)(h(t)\text{sin}(f^2(t))-h^2(t)\sin(f^2(t))+\mathcal{O}(h^2(t))dt \\ \end{align*} Let $T(h)=\int^{x}_{0}-2f(t)(h(t)\sin(f^2(t))dt$ . I am able to show that $T(h)$ is a linear map. Then we have $$F(f+h)(x)-F(f)(x)=T(h)+\int^{x}_{0}\mathcal{O}(h^2(t))dt $$ I am confused about the Big- $\mathcal{O}$ notation and I do not understand what it would mean to integrate over it. Also, am I on the right track? Thank you in advance for any help provided.","Let be the Banach space of continuous functions on (with the supremum norm) and define a map by Show that is Fréchet differentiable and compute the Fréchet derivative for each So far I have the following. Using the identity We have Let . I am able to show that is a linear map. Then we have I am confused about the Big- notation and I do not understand what it would mean to integrate over it. Also, am I on the right track? Thank you in advance for any help provided.","X = \mathcal{C} \left( [0,1] \right) [0,1] F : X \rightarrow X F(f)(x) = \int^{x}_{0} \cos(f(t)^{2})dt, x \in [0,1]. F DF|_{f} f \in X. \cos(A+\varepsilon B)=\cos(A)-\varepsilon B\sin(A)+\mathcal{O}(\varepsilon^2). \begin{align*}F(f+h)(x)
&=\int^{x}_{0} \cos((f(t)+h(t))^{2})dt \\
&=\int^{x}_{0}\cos(f^2(t))-h(t)[2f(t)h(t)]\sin(f^2(t))+\mathcal{O}(h^2(t))dt \\
&=F(f)(x)+\int^{x}_{0}-2f(t)(h(t)\text{sin}(f^2(t))-h^2(t)\sin(f^2(t))+\mathcal{O}(h^2(t))dt \\
\end{align*} T(h)=\int^{x}_{0}-2f(t)(h(t)\sin(f^2(t))dt T(h) F(f+h)(x)-F(f)(x)=T(h)+\int^{x}_{0}\mathcal{O}(h^2(t))dt  \mathcal{O}","['real-analysis', 'functional-analysis', 'operator-theory', 'asymptotics', 'frechet-derivative']"
63,Signed harmonic series containing floor function,Signed harmonic series containing floor function,,"Where it converges, let $$S\left(a\right)=\sum_{n=0}^\infty \frac{\left(-1\right)^{\left[na\right]}}{\lfloor na\rfloor+1},$$ where $a\gt 0$ , and $\lfloor\cdot\rfloor$ denotes the floor function. For instance, we have $S\left(\frac{1}{k}\right)=k\ln 2$ . Let $A=\left\{a\mid S\left(a\right)\:\text{ converges}\right\}$ . My questions are: • Does $A$ contain any irrational numbers? • Is $A$ an uncountable set? A full-measure set? • If so, I would guess the following limit: $$\lim_{a\in A, a\rightarrow 0^+}a\, S\left(a\right)=\ln 2$$ Am I right?","Where it converges, let where , and denotes the floor function. For instance, we have . Let . My questions are: • Does contain any irrational numbers? • Is an uncountable set? A full-measure set? • If so, I would guess the following limit: Am I right?","S\left(a\right)=\sum_{n=0}^\infty \frac{\left(-1\right)^{\left[na\right]}}{\lfloor na\rfloor+1}, a\gt 0 \lfloor\cdot\rfloor S\left(\frac{1}{k}\right)=k\ln 2 A=\left\{a\mid S\left(a\right)\:\text{ converges}\right\} A A \lim_{a\in A, a\rightarrow 0^+}a\, S\left(a\right)=\ln 2","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'ceiling-and-floor-functions']"
64,Why this function well defined?,Why this function well defined?,,"Let $M^{n-1} \subset \mathbb{R}^n$ be a smooth compact manifold, $\Lambda$ = $\{U_i\}_{i \in I}$ be an open cover of $\mathbb{R}^n$. Suppose that for every $U_i$ there is a smooth bounded function $f_i : U_i \rightarrow [-1,1]$, such that, for every $i \neq j$, occurs only one of the following conditions: $f_i(x) = f_j(x)$, $\forall$ $x$ $\in$ $U_i \cap U_j,$ $f_i(x) = -f_j(x)$, $\forall$ $x$ $\in$ $U_i \cap U_j.$ Besides that $f_i(x) = 0$ $\Leftrightarrow$ $x \in M\cap U_i$. For $x_0$ $\in$ $\mathbb{R}^n \setminus \{0\}$, we will define a function $f_{x_0}:\mathbb{R}^n \rightarrow [-1,1]$ as following: Consider $x$ $\in$ $\mathbb{R}^n$, let $\gamma_x:[0,1] \to \mathbb{R}^n$ be a continious path linking $x_0$ to $x$. Note that we can subdivide $[0,1]$ in $0=t_0\leq t_1 \leq t_2 \leq \ldots \leq t_n =1$, in such way that $\gamma_x([t_i,t_{i+1}]) \subset U_{j_i}$, for some $U_{j_i}$ $\in$ $\Lambda$. Suppose, without loss of generality, that $U_{j_i} = U_i$, for every $i$ $\in$ $\{0,1,...,n\}$. For every $i$ $\in$ $\{0,...,n\}$ there is $\xi_i$ $\in$ $U_{i-1} \cap U_{i}$, such that, $f_{i}(\xi_i) \neq 0.$ Defining $g_i:U_i \rightarrow [-1,1]$ recursively as $g_{0}(x)$ $=$ $f_{0}(x)$, $g_i (x)= \frac{f_i(\xi_i)}{g_{i-1}(\xi_{i})} \cdot f_i(x)$,   $\forall$ $i$ $\in\{1,...,m\}$. Finally we say that $f_{x_0}(x) = g_n(x)$, for $x \in U_n$. My Doubt: Why $f_{x_0}$ does not depends on the path $\gamma_x$ used to define $f_{x_0} (x)$? I think the function $f$ is well defined because $\mathbb{R}^{n}$ is a  simply connected, but I don't know how to demonstrate that the construction of the value  $f_{x_0}(x)$ is invariant under homotopy.","Let $M^{n-1} \subset \mathbb{R}^n$ be a smooth compact manifold, $\Lambda$ = $\{U_i\}_{i \in I}$ be an open cover of $\mathbb{R}^n$. Suppose that for every $U_i$ there is a smooth bounded function $f_i : U_i \rightarrow [-1,1]$, such that, for every $i \neq j$, occurs only one of the following conditions: $f_i(x) = f_j(x)$, $\forall$ $x$ $\in$ $U_i \cap U_j,$ $f_i(x) = -f_j(x)$, $\forall$ $x$ $\in$ $U_i \cap U_j.$ Besides that $f_i(x) = 0$ $\Leftrightarrow$ $x \in M\cap U_i$. For $x_0$ $\in$ $\mathbb{R}^n \setminus \{0\}$, we will define a function $f_{x_0}:\mathbb{R}^n \rightarrow [-1,1]$ as following: Consider $x$ $\in$ $\mathbb{R}^n$, let $\gamma_x:[0,1] \to \mathbb{R}^n$ be a continious path linking $x_0$ to $x$. Note that we can subdivide $[0,1]$ in $0=t_0\leq t_1 \leq t_2 \leq \ldots \leq t_n =1$, in such way that $\gamma_x([t_i,t_{i+1}]) \subset U_{j_i}$, for some $U_{j_i}$ $\in$ $\Lambda$. Suppose, without loss of generality, that $U_{j_i} = U_i$, for every $i$ $\in$ $\{0,1,...,n\}$. For every $i$ $\in$ $\{0,...,n\}$ there is $\xi_i$ $\in$ $U_{i-1} \cap U_{i}$, such that, $f_{i}(\xi_i) \neq 0.$ Defining $g_i:U_i \rightarrow [-1,1]$ recursively as $g_{0}(x)$ $=$ $f_{0}(x)$, $g_i (x)= \frac{f_i(\xi_i)}{g_{i-1}(\xi_{i})} \cdot f_i(x)$,   $\forall$ $i$ $\in\{1,...,m\}$. Finally we say that $f_{x_0}(x) = g_n(x)$, for $x \in U_n$. My Doubt: Why $f_{x_0}$ does not depends on the path $\gamma_x$ used to define $f_{x_0} (x)$? I think the function $f$ is well defined because $\mathbb{R}^{n}$ is a  simply connected, but I don't know how to demonstrate that the construction of the value  $f_{x_0}(x)$ is invariant under homotopy.",,"['real-analysis', 'differential-geometry', 'smooth-manifolds', 'homotopy-theory']"
65,If $a_{n+1}/a_n\to\ell$ show $a_n^{1/n}\to\ell$,If  show,a_{n+1}/a_n\to\ell a_n^{1/n}\to\ell,"Suppose we have a sequence $(a_n)$ such that $a_n\neq0$ and $$\lim\dfrac{a_{n+1}}{a_n}=\ell$$ Show that $$\lim a_n^{1/n}=\ell.$$ I have a proof sketch, but it needs an extra assumption: By the definition of convergence $\forall\varepsilon>0\exists N\forall n>N,\ \ell-\varepsilon<\dfrac{a_{n+1}}{a_n}<\ell+\varepsilon$. Here is where I have to assume that eventually $a_n>0$ to conclude (by induction) that eventually $$(\ell-\varepsilon)^na_0<a_n<(\ell+\varepsilon)^na_0\\\therefore(\ell-\varepsilon)a_0^{1/n}<a_n^{1/n}<(\ell+\varepsilon)a_0^{1/n}\\\therefore \ell-\varepsilon\leq\lim a_n^{1/n}\leq\ell+\varepsilon\\\therefore\lim a_n^{1/n}=\ell$$ But I haven't yet figured out how to prove this without the extra assumption.","Suppose we have a sequence $(a_n)$ such that $a_n\neq0$ and $$\lim\dfrac{a_{n+1}}{a_n}=\ell$$ Show that $$\lim a_n^{1/n}=\ell.$$ I have a proof sketch, but it needs an extra assumption: By the definition of convergence $\forall\varepsilon>0\exists N\forall n>N,\ \ell-\varepsilon<\dfrac{a_{n+1}}{a_n}<\ell+\varepsilon$. Here is where I have to assume that eventually $a_n>0$ to conclude (by induction) that eventually $$(\ell-\varepsilon)^na_0<a_n<(\ell+\varepsilon)^na_0\\\therefore(\ell-\varepsilon)a_0^{1/n}<a_n^{1/n}<(\ell+\varepsilon)a_0^{1/n}\\\therefore \ell-\varepsilon\leq\lim a_n^{1/n}\leq\ell+\varepsilon\\\therefore\lim a_n^{1/n}=\ell$$ But I haven't yet figured out how to prove this without the extra assumption.",,"['real-analysis', 'sequences-and-series', 'limits']"
66,Sequence in uncountable set,Sequence in uncountable set,,"Given an uncountable subset $A$ of $(0,1)$, does there always exist $a,r>0$ such that $a+r,a+r^2,a+r^3,\dots$ are all in $A$? For example, if $A$ contains an interval, it is easy to find such $a,r$. To try to show this, we can assume that no such $a,r$ exist (meaning that for any $a,r$, there exists $k$ with $a+r^k\not\in A$) and show that the set must be countable.","Given an uncountable subset $A$ of $(0,1)$, does there always exist $a,r>0$ such that $a+r,a+r^2,a+r^3,\dots$ are all in $A$? For example, if $A$ contains an interval, it is easy to find such $a,r$. To try to show this, we can assume that no such $a,r$ exist (meaning that for any $a,r$, there exists $k$ with $a+r^k\not\in A$) and show that the set must be countable.",,['real-analysis']
67,"Prob. 4, Chap. 7, in Baby Rudin: On what intervals does the series converge uniformly?","Prob. 4, Chap. 7, in Baby Rudin: On what intervals does the series converge uniformly?",,"Here is Prob. 4, Chap. 7, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Consider    $$ f(x) = \sum_{n=1}^\infty \frac{1}{1+n^2 x }. $$   For what values of $x$ does the series converge absolutely? On what intervals does it converge uniformly? On what intervals does it fail to converge uniformly? Is $f$ continuous wherever the series converges? Is $f$ bounded? My Attempt: Fix a real number $\delta > 0$. If $x \in [ \delta, +\infty)$, then we see that    $$ 0 < \frac{1}{1+n^2 x} < \frac{1}{n^2 x } \leq \frac{1}{n^2 \delta } = \frac{1}{\delta} \frac{1}{n^2}, $$   and the series $\sum \frac{1}{n^2} $ converges. So, by Theorem 7.10 in Rudin, our series converges uniformly on $[ \delta , +\infty)$. And, if $$x \in (- \infty, - \delta ]\setminus \left\{ \ -1, -\frac{1}{2^2}, -\frac{1}{3^2}, - \frac{1}{4^2}, \ldots \ \right\},$$    then $x < 0$, and, for all large enough $n$, we also have $1 + n^2 x < 0$, and for those $n$, we have $$ \left\lvert \frac{1}{1+n^2 x} \right\rvert = \frac{1}{ \left\lvert 1+n^2 x \right\rvert} =  \frac{1}{ -1 - n^2 x} < \frac{ 1}{ \frac{n^2 x}{2} - n^2 x} = -\frac{2}{x} \frac{1}{n^2} = \frac{2}{\delta} \frac{1}{n^2}.      $$   Here we have used the fact that, as $x < 0$, so as $n$ gets larger and larger, we eventually have  $1 + n^2 x < -1$, so $n^2 x < -2$, which implies that $\frac{n^2 x}{2} < -1$ and hence $\frac{n^2 x}{2} - n^2 x < -1 - n^2 x$; also from $1 + n^2 x < -1$, we obtain $1 < -1 - n^2 x$ and hence $0 < 1 < -1 - n^2 x$. And, we have also used the fact that, as $x \leq - \delta$, so $-x \geq \delta > 0$, and hence $$ 0 < -\frac{1}{x} \leq \frac{1}{\delta}.$$ As the series $\sum \frac{1}{n^2}$ converges, so it follows from Theorem 7.10 in Baby Rudin that our series converges uniformly on the following subset of $\mathbb{R}$: $$ (-\infty, -\delta ] \setminus \left\{ \ -1, -\frac{1}{2^2}, - \frac{1}{3^2}, - \frac{1}{4^2}, \ldots \ \right\}.$$ Is what I have done so far correct? If not, then where have I erred? What if $x \in (- \delta, \delta)$? And, what about the (uniform) convergence of this series for complex values of  $x$?","Here is Prob. 4, Chap. 7, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Consider    $$ f(x) = \sum_{n=1}^\infty \frac{1}{1+n^2 x }. $$   For what values of $x$ does the series converge absolutely? On what intervals does it converge uniformly? On what intervals does it fail to converge uniformly? Is $f$ continuous wherever the series converges? Is $f$ bounded? My Attempt: Fix a real number $\delta > 0$. If $x \in [ \delta, +\infty)$, then we see that    $$ 0 < \frac{1}{1+n^2 x} < \frac{1}{n^2 x } \leq \frac{1}{n^2 \delta } = \frac{1}{\delta} \frac{1}{n^2}, $$   and the series $\sum \frac{1}{n^2} $ converges. So, by Theorem 7.10 in Rudin, our series converges uniformly on $[ \delta , +\infty)$. And, if $$x \in (- \infty, - \delta ]\setminus \left\{ \ -1, -\frac{1}{2^2}, -\frac{1}{3^2}, - \frac{1}{4^2}, \ldots \ \right\},$$    then $x < 0$, and, for all large enough $n$, we also have $1 + n^2 x < 0$, and for those $n$, we have $$ \left\lvert \frac{1}{1+n^2 x} \right\rvert = \frac{1}{ \left\lvert 1+n^2 x \right\rvert} =  \frac{1}{ -1 - n^2 x} < \frac{ 1}{ \frac{n^2 x}{2} - n^2 x} = -\frac{2}{x} \frac{1}{n^2} = \frac{2}{\delta} \frac{1}{n^2}.      $$   Here we have used the fact that, as $x < 0$, so as $n$ gets larger and larger, we eventually have  $1 + n^2 x < -1$, so $n^2 x < -2$, which implies that $\frac{n^2 x}{2} < -1$ and hence $\frac{n^2 x}{2} - n^2 x < -1 - n^2 x$; also from $1 + n^2 x < -1$, we obtain $1 < -1 - n^2 x$ and hence $0 < 1 < -1 - n^2 x$. And, we have also used the fact that, as $x \leq - \delta$, so $-x \geq \delta > 0$, and hence $$ 0 < -\frac{1}{x} \leq \frac{1}{\delta}.$$ As the series $\sum \frac{1}{n^2}$ converges, so it follows from Theorem 7.10 in Baby Rudin that our series converges uniformly on the following subset of $\mathbb{R}$: $$ (-\infty, -\delta ] \setminus \left\{ \ -1, -\frac{1}{2^2}, - \frac{1}{3^2}, - \frac{1}{4^2}, \ldots \ \right\}.$$ Is what I have done so far correct? If not, then where have I erred? What if $x \in (- \delta, \delta)$? And, what about the (uniform) convergence of this series for complex values of  $x$?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'uniform-convergence']"
68,Order of study in mathematical analysis textbooks.,Order of study in mathematical analysis textbooks.,,"I have just started studying mathematical analysis and, when choosing the books I was going to use, I realized that there is some levels of difficulty. I am very lost in the sense that I don't know which is the proper order of study. The idea I have right now is the next: 1: First course in Real Analysis (epsilon-delta approach): Here is where we can find those books on 1 real variable such as calculus (spivak), understanding analysis (Abbott), Analysis I (Terence Tao), A course in mathematical analysis I (Garling), Mathematical Analysis vol. I (V. Zorich), Introduction to Real analysis (Bartle)... 2: Second course (using concepts such as metric spaces..., multivariable calculus, vector calculus): Here we find books like The elements of Real Analysis (Bartle), Analysis in Euclidean Space (Hoffman), Rudin's PMA, Pugh's Real Mathematical Analysis, Apostol's Mathematical Analysis, Analysis II (Terence Tao), Mathematical Analysis vol. I (V. Zorich), other books in several variables (such as Lang's, Fleming's...), Real Analysis (carothers), some introductions to Lebesgue integration... 3: Intermediate-Advanced: Here we have those books with a hard content in Topology (metric spaces, topological spaces, Continuity, Compactness, Completeness, Connectedness, Convergence...) and Analysis in several variables using all these topological concepts(introduction to differential forms, vector analysis, analysis on manifolds...). Some books in this section are: A course in mathematical analysis II (Garling), Topology (munkres) and other books in Point-Set Topology (as well as books dealing exclusively with metric spaces or topological spaces), Foundations of Modern Analysis (J. Dieudonne), Introductory real analysis (Kolmogorov & Fomin), Mathematical Analysis vol.II (V. Zorich), Introduction to Topology and Modern Analysis (Simmons), calculus on manifolds (spivak), analysis on manifolds (munkres)... Advanced (measure theory, advanced analysis): Royden, Folland, Papa Rudin, others... This is the classification that I have in mind but it is correct ? I am just in the first level but, due to I am self studying the subject, I would like to know how to study all these topics in the most ordered possible way. Finally, I have 2 additional questions. Which are the main differences between the 2 books from Bartle ? (Introduction to real analysis and the elements of real analysis) When should I study each ? Is vector calculus the same than multivariable calculus (as seen in the second course) ? Is there 2 kinds of multivariable calculus (one studied in the second course and called 'vector calculus' and another in the intermediate course and called 'vector analysis' ? ) Thank you in advance.","I have just started studying mathematical analysis and, when choosing the books I was going to use, I realized that there is some levels of difficulty. I am very lost in the sense that I don't know which is the proper order of study. The idea I have right now is the next: 1: First course in Real Analysis (epsilon-delta approach): Here is where we can find those books on 1 real variable such as calculus (spivak), understanding analysis (Abbott), Analysis I (Terence Tao), A course in mathematical analysis I (Garling), Mathematical Analysis vol. I (V. Zorich), Introduction to Real analysis (Bartle)... 2: Second course (using concepts such as metric spaces..., multivariable calculus, vector calculus): Here we find books like The elements of Real Analysis (Bartle), Analysis in Euclidean Space (Hoffman), Rudin's PMA, Pugh's Real Mathematical Analysis, Apostol's Mathematical Analysis, Analysis II (Terence Tao), Mathematical Analysis vol. I (V. Zorich), other books in several variables (such as Lang's, Fleming's...), Real Analysis (carothers), some introductions to Lebesgue integration... 3: Intermediate-Advanced: Here we have those books with a hard content in Topology (metric spaces, topological spaces, Continuity, Compactness, Completeness, Connectedness, Convergence...) and Analysis in several variables using all these topological concepts(introduction to differential forms, vector analysis, analysis on manifolds...). Some books in this section are: A course in mathematical analysis II (Garling), Topology (munkres) and other books in Point-Set Topology (as well as books dealing exclusively with metric spaces or topological spaces), Foundations of Modern Analysis (J. Dieudonne), Introductory real analysis (Kolmogorov & Fomin), Mathematical Analysis vol.II (V. Zorich), Introduction to Topology and Modern Analysis (Simmons), calculus on manifolds (spivak), analysis on manifolds (munkres)... Advanced (measure theory, advanced analysis): Royden, Folland, Papa Rudin, others... This is the classification that I have in mind but it is correct ? I am just in the first level but, due to I am self studying the subject, I would like to know how to study all these topics in the most ordered possible way. Finally, I have 2 additional questions. Which are the main differences between the 2 books from Bartle ? (Introduction to real analysis and the elements of real analysis) When should I study each ? Is vector calculus the same than multivariable calculus (as seen in the second course) ? Is there 2 kinds of multivariable calculus (one studied in the second course and called 'vector calculus' and another in the intermediate course and called 'vector analysis' ? ) Thank you in advance.",,"['real-analysis', 'analysis', 'reference-request']"
69,Functions such that $f(ax)=f(x)+f(a)$,Functions such that,f(ax)=f(x)+f(a),"I thought of this question somewhat randomly on a walk, and have discussed it with another friend of mine (we both have pure mathematics degrees). We have made some headway, and we think we have generated a proof, but we would appreciate any additional insight and proof verification. Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(ax)=f(x)+f(a)$. How much can we say about the behavior of $f(x)$? What additional restrictions, if any, allow us to show $f(x)=\log_b(x)$? We conjecture that, with the restriction that $f$ is not $0$ everywhere, $f$ must be the logarithm. We have, by the log rules, $$\log_b(|ax|)=\log_b(|x|)+\log_b(|a|)$$ So the logarithm is indeed one such $f$. I will give what we have been able to show about $f$ below, followed by our general proof. If you can offer any insight into this problem, we would greatly appreciate it. Edit: Turns out this can be shown fairly easily by considering the Cauchy Functional Equation and showing $g(x)=f(e^x)=cx$ for some $c$. The proof given below does not use this fact. From here on, we assume $f$ is not trivial. Result 1: $f(1)=0$ We note that $$f(a)=f(a\cdot1)=f(1)+f(a)$$ which shows $f(1)=0$. Result 2: $f(0)$ is not defined We see that $$f(0)=f(a\cdot0)=f(0)+f(a)$$ which implies $f(a)=0$ for all $a$. However, we have assumed $f(x)\neq0$ for some $x$, giving a contradiction. Thus $f(0)$ must not be defined. So, we redefine $f$ as $f:\mathbb{R}\setminus\{0\}\to\mathbb{R}$. Result 3: $f(-x)=f(x)$ If we allow $x<0$, we then have $$0=f(1)=f(-1\cdot-1)=2f(-1)$$ Showing that $f(-1)=0$ as well. Using this result, we have $$f(-a)=f(-1)+f(a)=f(a)$$ Our Proof: From here on, we use results from elementary abstract algebra and analysis. We realized that the condition on $f$ is that of a group homomorphism $\varphi:(\mathbb{R}\setminus\{0\},*)\to(\mathbb{R},+)$ where $$\varphi(xy)=\varphi(x)+\varphi(y)$$ Similarly, you can show the above results for $\varphi$. We noted that if we also define the continuous group homomorphism $\psi:(\mathbb{R},+)\to(\mathbb{R}\setminus\{0\},*)$ where $$\psi(x+y)=\psi(x)\psi(y)$$ $\psi$ and $\varphi$ seem like they could possibly be inverses with some additional restrictions. We see that $$\psi(x)=\psi(0+x)=\psi(0)\psi(x)=1$$ So that $\psi(0)=1$. We can then say for any integer $n\geq0$, $$\psi(n)=\psi(1+1+1+...+1)=\psi(1)\cdot\psi(1)\cdot\cdot\cdot\psi(1)=\psi(1)^n$$ Note, $\psi(1)<0$ may give us complex results, so we restrict $\psi(1)>0$. For any integer $n<0$, $$\psi(n)=\psi(-1-1+...-1)=\psi(-1)^{|n|}=(\psi(1)^{-1})^{|n|}=\psi(1)^{-|n|}=\psi(1)^n$$ Then, for any rational number $\frac{p}{q}$, $$\psi(\frac{p}{q})=\psi(\frac{1}{q})^p=\psi(q^{-1})^p=(\psi(q)^{-1})^p=\psi(1)^{\frac{p}{q}}$$ Let $x\in\mathbb{R}\setminus\mathbb{Q}$. Since the rationals are dense in the reals, we can find rational $\frac{p}{q}<x<\frac{r}{s}$ arbitrarily close to $x$. Note that $\psi$ is strictly increasing over the rationals, and thus $$\psi(1)^{\frac{p}{q}}<\psi(x)<\psi(1)^{\frac{r}{s}}$$ Since $\psi$ is continuous, this implies $\psi(x)=\psi(1)^x$ for all $x\in\mathbb{R}$. Therefore, $$\psi(x)=\psi(1)^x$$ This shows that $\psi$ must be the exponential function. Note that it is completely characterized by its value at $1$.  Noting that $\varphi$ seems like it should be related to the inverse of $\psi$, we would expect $\varphi(x)=\log_b(x)$. We note that for integer $n\geq0$ and real $a>0$, $$\varphi(a^n)=\varphi(a)+\varphi(a)+...+\varphi(a)=n\varphi(a)$$ For $n<0$, we have $$\varphi(a^n)=\varphi(\frac{1}{a})+...+\varphi(\frac{1}{a})=|n|\varphi(a^{-1})=-|n|\varphi{a}=n\varphi(a)$$ The rational case is slightly trickier. We note that $a^{\frac{n}{n}}=a$, and thus $$\varphi(a)=\varphi(a^{\frac{n}{n}})=n\varphi(a^{\frac{1}{n}})$$ And thus $\varphi(a^{\frac{1}{n}})=\frac{1}{n}\varphi(a)$. Therefore, for any rational $\frac{p}{q}$ $$\varphi(a^{\frac{p}{q}})=p\varphi(a^{\frac{1}{q}})=\frac{p}{q}\varphi(a)$$ Using the same argument as before, we can extend this to all $x\in\mathbb{R}$. Therefore, $$\varphi(a^x)=x\varphi(a)$$ Combining these, we have $$\varphi(\psi(x))=\varphi(\psi(1)^x)=x\varphi(\psi(1))$$ Thus, $\varphi$ and $\psi$ are inverses up to multiplication by a constant, which confirms that $\psi$ must be the logarithm. If we restrict $\varphi(\psi(1))=1$, these are exactly inverses. If $\psi(1)>0$, we must have $\varphi_{\psi(1)}(x)=\log_{\psi(1)}(x)$. This proof requires that $x>0$. However, we showed earlier that $\varphi(-x)=\varphi(x)$, and for positive $x$, $\varphi_b(x)=\log_b(x)$. To make this function even, we modify it as $$\varphi_b(x)=\log_b|x|$$ and this is the only possible continuous solution for $\varphi$ defined on all of $\mathbb{R}\setminus\{0\}$. In other words, $f$ must be the logarithm. We have shown the logarithm rules are unique to logarithms! One thing that concerns us is the restriction that $f$ is continuous. Are there discontinuous $f$ that satisfy this property? Please let us know if we made any invalid assumptions somewhere, or if our statements about the uniqueness of these functions is incorrect. We would also appreciate any alternate proofs you may have.","I thought of this question somewhat randomly on a walk, and have discussed it with another friend of mine (we both have pure mathematics degrees). We have made some headway, and we think we have generated a proof, but we would appreciate any additional insight and proof verification. Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(ax)=f(x)+f(a)$. How much can we say about the behavior of $f(x)$? What additional restrictions, if any, allow us to show $f(x)=\log_b(x)$? We conjecture that, with the restriction that $f$ is not $0$ everywhere, $f$ must be the logarithm. We have, by the log rules, $$\log_b(|ax|)=\log_b(|x|)+\log_b(|a|)$$ So the logarithm is indeed one such $f$. I will give what we have been able to show about $f$ below, followed by our general proof. If you can offer any insight into this problem, we would greatly appreciate it. Edit: Turns out this can be shown fairly easily by considering the Cauchy Functional Equation and showing $g(x)=f(e^x)=cx$ for some $c$. The proof given below does not use this fact. From here on, we assume $f$ is not trivial. Result 1: $f(1)=0$ We note that $$f(a)=f(a\cdot1)=f(1)+f(a)$$ which shows $f(1)=0$. Result 2: $f(0)$ is not defined We see that $$f(0)=f(a\cdot0)=f(0)+f(a)$$ which implies $f(a)=0$ for all $a$. However, we have assumed $f(x)\neq0$ for some $x$, giving a contradiction. Thus $f(0)$ must not be defined. So, we redefine $f$ as $f:\mathbb{R}\setminus\{0\}\to\mathbb{R}$. Result 3: $f(-x)=f(x)$ If we allow $x<0$, we then have $$0=f(1)=f(-1\cdot-1)=2f(-1)$$ Showing that $f(-1)=0$ as well. Using this result, we have $$f(-a)=f(-1)+f(a)=f(a)$$ Our Proof: From here on, we use results from elementary abstract algebra and analysis. We realized that the condition on $f$ is that of a group homomorphism $\varphi:(\mathbb{R}\setminus\{0\},*)\to(\mathbb{R},+)$ where $$\varphi(xy)=\varphi(x)+\varphi(y)$$ Similarly, you can show the above results for $\varphi$. We noted that if we also define the continuous group homomorphism $\psi:(\mathbb{R},+)\to(\mathbb{R}\setminus\{0\},*)$ where $$\psi(x+y)=\psi(x)\psi(y)$$ $\psi$ and $\varphi$ seem like they could possibly be inverses with some additional restrictions. We see that $$\psi(x)=\psi(0+x)=\psi(0)\psi(x)=1$$ So that $\psi(0)=1$. We can then say for any integer $n\geq0$, $$\psi(n)=\psi(1+1+1+...+1)=\psi(1)\cdot\psi(1)\cdot\cdot\cdot\psi(1)=\psi(1)^n$$ Note, $\psi(1)<0$ may give us complex results, so we restrict $\psi(1)>0$. For any integer $n<0$, $$\psi(n)=\psi(-1-1+...-1)=\psi(-1)^{|n|}=(\psi(1)^{-1})^{|n|}=\psi(1)^{-|n|}=\psi(1)^n$$ Then, for any rational number $\frac{p}{q}$, $$\psi(\frac{p}{q})=\psi(\frac{1}{q})^p=\psi(q^{-1})^p=(\psi(q)^{-1})^p=\psi(1)^{\frac{p}{q}}$$ Let $x\in\mathbb{R}\setminus\mathbb{Q}$. Since the rationals are dense in the reals, we can find rational $\frac{p}{q}<x<\frac{r}{s}$ arbitrarily close to $x$. Note that $\psi$ is strictly increasing over the rationals, and thus $$\psi(1)^{\frac{p}{q}}<\psi(x)<\psi(1)^{\frac{r}{s}}$$ Since $\psi$ is continuous, this implies $\psi(x)=\psi(1)^x$ for all $x\in\mathbb{R}$. Therefore, $$\psi(x)=\psi(1)^x$$ This shows that $\psi$ must be the exponential function. Note that it is completely characterized by its value at $1$.  Noting that $\varphi$ seems like it should be related to the inverse of $\psi$, we would expect $\varphi(x)=\log_b(x)$. We note that for integer $n\geq0$ and real $a>0$, $$\varphi(a^n)=\varphi(a)+\varphi(a)+...+\varphi(a)=n\varphi(a)$$ For $n<0$, we have $$\varphi(a^n)=\varphi(\frac{1}{a})+...+\varphi(\frac{1}{a})=|n|\varphi(a^{-1})=-|n|\varphi{a}=n\varphi(a)$$ The rational case is slightly trickier. We note that $a^{\frac{n}{n}}=a$, and thus $$\varphi(a)=\varphi(a^{\frac{n}{n}})=n\varphi(a^{\frac{1}{n}})$$ And thus $\varphi(a^{\frac{1}{n}})=\frac{1}{n}\varphi(a)$. Therefore, for any rational $\frac{p}{q}$ $$\varphi(a^{\frac{p}{q}})=p\varphi(a^{\frac{1}{q}})=\frac{p}{q}\varphi(a)$$ Using the same argument as before, we can extend this to all $x\in\mathbb{R}$. Therefore, $$\varphi(a^x)=x\varphi(a)$$ Combining these, we have $$\varphi(\psi(x))=\varphi(\psi(1)^x)=x\varphi(\psi(1))$$ Thus, $\varphi$ and $\psi$ are inverses up to multiplication by a constant, which confirms that $\psi$ must be the logarithm. If we restrict $\varphi(\psi(1))=1$, these are exactly inverses. If $\psi(1)>0$, we must have $\varphi_{\psi(1)}(x)=\log_{\psi(1)}(x)$. This proof requires that $x>0$. However, we showed earlier that $\varphi(-x)=\varphi(x)$, and for positive $x$, $\varphi_b(x)=\log_b(x)$. To make this function even, we modify it as $$\varphi_b(x)=\log_b|x|$$ and this is the only possible continuous solution for $\varphi$ defined on all of $\mathbb{R}\setminus\{0\}$. In other words, $f$ must be the logarithm. We have shown the logarithm rules are unique to logarithms! One thing that concerns us is the restriction that $f$ is continuous. Are there discontinuous $f$ that satisfy this property? Please let us know if we made any invalid assumptions somewhere, or if our statements about the uniqueness of these functions is incorrect. We would also appreciate any alternate proofs you may have.",,"['real-analysis', 'abstract-algebra', 'proof-verification', 'logarithms', 'alternative-proof']"
70,Show the limit of Lebesgue sum is Lebesgue integral,Show the limit of Lebesgue sum is Lebesgue integral,,"Suppose $\mu(E)<\infty$ and $f$ is Lebesgue integrable on $E$. Prove the Lebesgue integral  $$\int f~d\mu= \lim_{m(T)\to 0}\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}.$$ $T=\{...t_{-1}<t_0<t_1<...\}$ is a countable partition of $\mathbb{R}$, $m(T)=\displaystyle\sup_k|t_{k+1}-t_k|$ and $\tau_k\in[t_k,t_{k+1})$ are aribitrary points. So, we need to show $$\sup\{\int g~d\mu, g\le f\ \text{and}~g~\text{is simple}\}=\lim_{m(T)\to 0}\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}$$ When the partition is finite nad $f$ is bounded, $\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}$ gives us a simple funtion and I think we can use Lebesgue monotone convergence theorem. However, $f$ are not necessarily bounded and the partition is infinite. How to deal with this case?","Suppose $\mu(E)<\infty$ and $f$ is Lebesgue integrable on $E$. Prove the Lebesgue integral  $$\int f~d\mu= \lim_{m(T)\to 0}\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}.$$ $T=\{...t_{-1}<t_0<t_1<...\}$ is a countable partition of $\mathbb{R}$, $m(T)=\displaystyle\sup_k|t_{k+1}-t_k|$ and $\tau_k\in[t_k,t_{k+1})$ are aribitrary points. So, we need to show $$\sup\{\int g~d\mu, g\le f\ \text{and}~g~\text{is simple}\}=\lim_{m(T)\to 0}\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}$$ When the partition is finite nad $f$ is bounded, $\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}$ gives us a simple funtion and I think we can use Lebesgue monotone convergence theorem. However, $f$ are not necessarily bounded and the partition is infinite. How to deal with this case?",,"['real-analysis', 'integration', 'functional-analysis', 'lebesgue-integral']"
71,Proving $ \sum_{n=-\infty}^{\infty}(-1)^n e^{-\left(x - \frac{n}{2}\right)^2} = K\cos(2\pi x)$,Proving, \sum_{n=-\infty}^{\infty}(-1)^n e^{-\left(x - \frac{n}{2}\right)^2} = K\cos(2\pi x),"I conjecture that: $$ \sum_{n=-\infty}^{\infty}(-1)^n e^{-\left(x - \frac{n}{2}\right)^2} = K\cos(2\pi q x)$$ for some $K \in \mathbb{R}, q \in \mathbb{Z}$. Let $G(x)$ denote the left side. It's easy to show that $G(x + 1) = G(x)$ (by a shifting sum argument). Furthermore it can be shown that $G(-x) = G(x)$ by noting that for each summand $$(-1)^n e^{-\left(x- \frac{n}{2}\right)^2} $$ If we invert it  $$(-1)^n e^{-\left(-x- \frac{n}{2}\right)^2}$$ Thats the same as  $$(-1)^n e^{-\left(x+ \frac{n}{2}\right)^2}$$ And since the sum is over all natural numbers that implies this mapping is a bijection of terms in the sum to terms in the sum. But is the set of functions $\mathbb{C} \rightarrow \mathbb{C}$ that satisfy $ F(x+1) = F(x) , F(x) = F(-x)$  really equal to $K\cos(2\pi q x)$? I worry that some pathological examples might arise that satisfy both equations but are excluded from this list. What other conditions should I chase? Trying to show that the second derivative is a multiple of the original function has not been very fruitful. I end up with the expression $$ G'' =  2\sum_{n=-\infty}^{\infty}(-1)^{n+1} \left(1 - 2\left( x - \frac{n}{2} \right)^2 \right) e^{-\left(x - \frac{n}{2}\right)^2} $$ (if we let $u = e^{x^2} G$ then I think this yields) $$ (4x^2 - 2)e^{-x^2} U -4xe^{-x^2}U' + e^{-x^2} U'' = (e^{x^2}U)'' $$ But involving matrix exponentials to solve this seems like a mess.","I conjecture that: $$ \sum_{n=-\infty}^{\infty}(-1)^n e^{-\left(x - \frac{n}{2}\right)^2} = K\cos(2\pi q x)$$ for some $K \in \mathbb{R}, q \in \mathbb{Z}$. Let $G(x)$ denote the left side. It's easy to show that $G(x + 1) = G(x)$ (by a shifting sum argument). Furthermore it can be shown that $G(-x) = G(x)$ by noting that for each summand $$(-1)^n e^{-\left(x- \frac{n}{2}\right)^2} $$ If we invert it  $$(-1)^n e^{-\left(-x- \frac{n}{2}\right)^2}$$ Thats the same as  $$(-1)^n e^{-\left(x+ \frac{n}{2}\right)^2}$$ And since the sum is over all natural numbers that implies this mapping is a bijection of terms in the sum to terms in the sum. But is the set of functions $\mathbb{C} \rightarrow \mathbb{C}$ that satisfy $ F(x+1) = F(x) , F(x) = F(-x)$  really equal to $K\cos(2\pi q x)$? I worry that some pathological examples might arise that satisfy both equations but are excluded from this list. What other conditions should I chase? Trying to show that the second derivative is a multiple of the original function has not been very fruitful. I end up with the expression $$ G'' =  2\sum_{n=-\infty}^{\infty}(-1)^{n+1} \left(1 - 2\left( x - \frac{n}{2} \right)^2 \right) e^{-\left(x - \frac{n}{2}\right)^2} $$ (if we let $u = e^{x^2} G$ then I think this yields) $$ (4x^2 - 2)e^{-x^2} U -4xe^{-x^2}U' + e^{-x^2} U'' = (e^{x^2}U)'' $$ But involving matrix exponentials to solve this seems like a mess.",,"['real-analysis', 'sequences-and-series']"
72,Importance of $|\mu(A)|<\infty$ in complex measure?,Importance of  in complex measure?,|\mu(A)|<\infty,"In the book ' An Introductory Course in Functional Analysis ' by Bowers and Kalton, they give the definition of positive measure at page $209:$ Let $(X,\mathcal{A})$ be a measurable space, A set function $\mu:\mathcal{A} \rightarrow [0,\infty]$ is said to be countably additive if    $$\mu\left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \mu(A_j)$$   whenever $(A_j)_{j=1}^\infty$ is a sequence of pairwise disjoint measurable sets.    A countably additive set function $\mu:\mathcal{A} \to [0,\infty]$ such that $\mu({\emptyset})=0$ is called a positive measure . The authos give definition of complex measure at page $213:$ Let $(X,\mathcal{A})$ be a measurable space, A countably additive set function $\mu:\mathcal{A} \rightarrow \mathbb{C}$ is called a complex measure . When we say $\mu$ is countably additive, we mean $$\mu\left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \mu(A_j)$$   whenever $(A_j)_{j=1}^\infty$ is a sequence of pairwise disjoint measurable sets in $\mathcal{A},$ where the series is absolutely convergent. At the same page, the authors mentioned the following: There is a significant difference between positive measures and complex measures. In the definition of complex measure, we require that a complex measure $\mu$ be finite; that is, $|\mu(A)|<\infty$ for all $a\in\mathcal{A}.$ This was not a requirement for a positive measure. Question: Why do we need to assume that $|\mu(A)| < \infty$ for complex measure while we do not need to assume it in positive measure? I strongly believe that purpose of inequality is not to ensure the series converges absolutely only, but it has other purpose.","In the book ' An Introductory Course in Functional Analysis ' by Bowers and Kalton, they give the definition of positive measure at page $209:$ Let $(X,\mathcal{A})$ be a measurable space, A set function $\mu:\mathcal{A} \rightarrow [0,\infty]$ is said to be countably additive if    $$\mu\left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \mu(A_j)$$   whenever $(A_j)_{j=1}^\infty$ is a sequence of pairwise disjoint measurable sets.    A countably additive set function $\mu:\mathcal{A} \to [0,\infty]$ such that $\mu({\emptyset})=0$ is called a positive measure . The authos give definition of complex measure at page $213:$ Let $(X,\mathcal{A})$ be a measurable space, A countably additive set function $\mu:\mathcal{A} \rightarrow \mathbb{C}$ is called a complex measure . When we say $\mu$ is countably additive, we mean $$\mu\left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \mu(A_j)$$   whenever $(A_j)_{j=1}^\infty$ is a sequence of pairwise disjoint measurable sets in $\mathcal{A},$ where the series is absolutely convergent. At the same page, the authors mentioned the following: There is a significant difference between positive measures and complex measures. In the definition of complex measure, we require that a complex measure $\mu$ be finite; that is, $|\mu(A)|<\infty$ for all $a\in\mathcal{A}.$ This was not a requirement for a positive measure. Question: Why do we need to assume that $|\mu(A)| < \infty$ for complex measure while we do not need to assume it in positive measure? I strongly believe that purpose of inequality is not to ensure the series converges absolutely only, but it has other purpose.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'definition']"
73,"Methods to calculate an approximation of $\int_0^1 \log\left(1+x^{\Gamma(x)}\right)\,dx,$ where $\Gamma(x)$ is the Gamma function",Methods to calculate an approximation of  where  is the Gamma function,"\int_0^1 \log\left(1+x^{\Gamma(x)}\right)\,dx, \Gamma(x)","While I was playing with Wolfram Alpha online calculator I wondered how calculate an approximation of $$I=\int_0^1 \log\left(1+x^{\Gamma(x)}\right)\,dx,\tag{1}$$ where $\Gamma(x)$ is the Gamma function. I am not interested specially to get a very good approximation, because Wolfram Alpha provide me one with code int log(1+x^Gamma(x))dx, from x=0 to 1 I would like to know methods about how calculate an approximation of $I$. I suspect that to get a good approximation is required to do analysis about the Gamma function. Question. How calculate using analysis an approximation of $$\int_0^1 \log\left(1+x^{\Gamma(x)}\right)\,dx?$$ Many thanks.","While I was playing with Wolfram Alpha online calculator I wondered how calculate an approximation of $$I=\int_0^1 \log\left(1+x^{\Gamma(x)}\right)\,dx,\tag{1}$$ where $\Gamma(x)$ is the Gamma function. I am not interested specially to get a very good approximation, because Wolfram Alpha provide me one with code int log(1+x^Gamma(x))dx, from x=0 to 1 I would like to know methods about how calculate an approximation of $I$. I suspect that to get a good approximation is required to do analysis about the Gamma function. Question. How calculate using analysis an approximation of $$\int_0^1 \log\left(1+x^{\Gamma(x)}\right)\,dx?$$ Many thanks.",,['real-analysis']
74,"If $\sum\limits_nx_{kn}=0$ for all $k$ and $\sum\limits_n|x_n|$ converges then $x_n=0$ for all $n$, but what if $\sum\limits_n|x_n|$ diverges?","If  for all  and  converges then  for all , but what if  diverges?",\sum\limits_nx_{kn}=0 k \sum\limits_n|x_n| x_n=0 n \sum\limits_n|x_n|,"I am trying to answer the following question, specifically, the second part: Let $(x_{n})^{\infty}_{n=1}$ be  real sequence such that $\sum^{\infty}_{n=1}|x_{n}|$ converges and, for each $k\in\mathbb{N}$ , $\sum^{\infty}_{n=1}x_{kn} = 0$ . Show that $x_{n} = 0$ for all $n$ . What if we no longer require $\sum^{\infty}_{n=1}|x_{n}|$ to converge? Source: https://www.dpmms.cam.ac.uk/study/IA/Numbers+Sets/2015-2016/examples-NS-15-3.pdf Solution to first part: Let $p_{i,k}$ denote the $i^{th}$ prime grater than $k$ (e.g $p_{1,7} = p_{1,8} = p_{2,6} = 11$ ). Define $s_{j,k} = \sum_{n\in k\mathbb{N} \setminus A_{j,k}} x_{n}$ where $A_{j,k} = \{n \in k\mathbb{N} | \exists i \leq j\,\,\,\, s.t. \,\, n|p_{i,k} \}$ . $s_{j,k} = \sum_{n\in k\mathbb{N} \setminus A_{j,k}} x_{n} = \sum_{n\in A_{j,k}}x_{n}$ because $\sum x_{kn} = 0$ . Using Inclusion-Exclusion: $s_{j,k} = \sum^{m}_{r=1}\bigg((-1)^{r-1}\sum_{I \subset \{p_{1,k}, p_{2,k}, ..., p_{j,k}\}, |I|=r}\big(\sum_{n\in A_{i,k},i \in I}x_{kn}\big)\bigg)$ . We see the innermost sum is $0$ . Hence $s_{j,k} = 0$ . $s_{i,k} = x_{k} + \sum_{n \in B_{k}}x_{n} = 0$ where $B_{k} \subset \{p_{j+1, k}, p_{j+1,k}+ 1,...\}$ . Hence: $|x_{k}| = |\sum_{n\in B_{k}} x_{n}| \leq \sum_{n \in B_{k}} |x_{n}| \leq \sum_{n \geq p_{j+1,k}}|x_{n}|$ By taking limits as $j \to\infty$ we have $|x_{k}| = 0$ . So $x_{n} = 0 \, \, \, \forall n$ . Unsure on second part of question.","I am trying to answer the following question, specifically, the second part: Let be  real sequence such that converges and, for each , . Show that for all . What if we no longer require to converge? Source: https://www.dpmms.cam.ac.uk/study/IA/Numbers+Sets/2015-2016/examples-NS-15-3.pdf Solution to first part: Let denote the prime grater than (e.g ). Define where . because . Using Inclusion-Exclusion: . We see the innermost sum is . Hence . where . Hence: By taking limits as we have . So . Unsure on second part of question.","(x_{n})^{\infty}_{n=1} \sum^{\infty}_{n=1}|x_{n}| k\in\mathbb{N} \sum^{\infty}_{n=1}x_{kn} = 0 x_{n} = 0 n \sum^{\infty}_{n=1}|x_{n}| p_{i,k} i^{th} k p_{1,7} = p_{1,8} = p_{2,6} = 11 s_{j,k} = \sum_{n\in k\mathbb{N} \setminus A_{j,k}} x_{n} A_{j,k} = \{n \in k\mathbb{N} | \exists i \leq j\,\,\,\, s.t. \,\, n|p_{i,k} \} s_{j,k} = \sum_{n\in k\mathbb{N} \setminus A_{j,k}} x_{n} = \sum_{n\in A_{j,k}}x_{n} \sum x_{kn} = 0 s_{j,k} = \sum^{m}_{r=1}\bigg((-1)^{r-1}\sum_{I \subset \{p_{1,k}, p_{2,k}, ..., p_{j,k}\}, |I|=r}\big(\sum_{n\in A_{i,k},i \in I}x_{kn}\big)\bigg) 0 s_{j,k} = 0 s_{i,k} = x_{k} + \sum_{n \in B_{k}}x_{n} = 0 B_{k} \subset \{p_{j+1, k}, p_{j+1,k}+ 1,...\} |x_{k}| = |\sum_{n\in B_{k}} x_{n}| \leq \sum_{n \in B_{k}} |x_{n}| \leq \sum_{n \geq p_{j+1,k}}|x_{n}| j \to\infty |x_{k}| = 0 x_{n} = 0 \, \, \, \forall n","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
75,atomic function is differentiable at at least one point.,atomic function is differentiable at at least one point.,,"Let $a_1,a_2,\dots$ be distinct reals in $(0,1)$. Let $f:(0,1)\rightarrow [0,\infty)$ be a function such that $\sum\limits_{i=1}^\infty f(a_n)$ converges and $f$ is $0$ at all points other than the $a_i$. Prove that a point $x$ in $(0,1)$ exists such that $f$ is differentiable at $x$. Clearly we must have $f(x)=0$. I have tried assuming no such $x$ exists but have gotten nowhere.","Let $a_1,a_2,\dots$ be distinct reals in $(0,1)$. Let $f:(0,1)\rightarrow [0,\infty)$ be a function such that $\sum\limits_{i=1}^\infty f(a_n)$ converges and $f$ is $0$ at all points other than the $a_i$. Prove that a point $x$ in $(0,1)$ exists such that $f$ is differentiable at $x$. Clearly we must have $f(x)=0$. I have tried assuming no such $x$ exists but have gotten nowhere.",,"['real-analysis', 'contest-math']"
76,Example of how set-theoretic foundations might illuminate applications,Example of how set-theoretic foundations might illuminate applications,,"If possible, please give an example of how set-theoretic mathematical foundations might illuminate an engineer's work. By engineer, I mean a physical engineer (civil, mining, mechanical, chemical, electrical, etc.) rather than, say, a software engineer. To clarify: I do not imply that set-theoretic mathematical foundations ought (or ought not) to illuminate an engineer's work, but am merely asking for an example, in case they do happen to illuminate it. BACKGROUND The above is my question.  The below merely gives background, if relevant. I happen to be an electrical engineer with a master's degree in my late 40s.  My professional work is in building construction but my master's study focused on electromagnetics.  Thus, my mathematical applications include vector calculus, special functions, Green's functions, eigenfunctions, contour integrations and so on. I know what an analytic function is in an engineering context, can compute a divergence in parabolic coordinates, and might (with difficulty) exercise differentiable manifolds in the context of surface-borne electric currents; but can barely read logic notation, nor do I have any idea who Galois is or what his theory might be for. So, recently, to broaden my perspective, I have been reading Richard A. Silverman's 1973 English translation of Georgi E. Shilov's Elementary Real and Complex Analysis. Shilov is an engaging writer and I am enjoying his book, but maybe at my age the brain just grows too inflexible. I believe that there is a point to, say, Cantor's theorem of the uncountable continuum, but I don't seem to be getting the point. In 1925, Hermann Weyl wrote: [A set-theoretic approach] contradicts the essence of the continuum, which by its very nature cannot be battered into a set of separated elements.  Not the relationship of an element to a set, but that of a part to a whole should serve as the basis.... Whether Weyl or Cantor is the more right is not for me to say, nor is that the question I am asking; but I can say that Weyl's sentiment makes sense to me. So far, Cantor's sentiments do not make sense to me (unless Cantor's principal sentiment is just that Kant is right and Plato is wrong, in which case I should stop reading Cantorian books, for a merely metaphysical dispute between Cantor and Plato is not a thing I would wish to pursue). Obviously, Cantor's sentiments make sense to a lot of very smart mathematicians, though; so, if there existed an example to bridge Cantor's mental world to the world of an engineer's experience, this would help to motivate my further reading. As matters stand, I am having a hard time understanding how Cantor and friends are talking about anything at all other than abstract games played with arbitrary definitions. So, indeed, an example would help. Hilbert's grand hotel? Sure, very funny, I suppose: a most ingenious paradox. I should spend the night at Hilbert's grand hotel, sometime. One hears that the hotel's cuisine is transcendental. Meanwhile, I admit that I just don't really get the point. I believe that there is a point; I just haven't gotten it, yet. To clarify: I am not requesting a general defense of mathematical foundations, but merely a pertinent example, relevant to an engineer, that illuminates the mathematician's interest in mathematical foundations.","If possible, please give an example of how set-theoretic mathematical foundations might illuminate an engineer's work. By engineer, I mean a physical engineer (civil, mining, mechanical, chemical, electrical, etc.) rather than, say, a software engineer. To clarify: I do not imply that set-theoretic mathematical foundations ought (or ought not) to illuminate an engineer's work, but am merely asking for an example, in case they do happen to illuminate it. BACKGROUND The above is my question.  The below merely gives background, if relevant. I happen to be an electrical engineer with a master's degree in my late 40s.  My professional work is in building construction but my master's study focused on electromagnetics.  Thus, my mathematical applications include vector calculus, special functions, Green's functions, eigenfunctions, contour integrations and so on. I know what an analytic function is in an engineering context, can compute a divergence in parabolic coordinates, and might (with difficulty) exercise differentiable manifolds in the context of surface-borne electric currents; but can barely read logic notation, nor do I have any idea who Galois is or what his theory might be for. So, recently, to broaden my perspective, I have been reading Richard A. Silverman's 1973 English translation of Georgi E. Shilov's Elementary Real and Complex Analysis. Shilov is an engaging writer and I am enjoying his book, but maybe at my age the brain just grows too inflexible. I believe that there is a point to, say, Cantor's theorem of the uncountable continuum, but I don't seem to be getting the point. In 1925, Hermann Weyl wrote: [A set-theoretic approach] contradicts the essence of the continuum, which by its very nature cannot be battered into a set of separated elements.  Not the relationship of an element to a set, but that of a part to a whole should serve as the basis.... Whether Weyl or Cantor is the more right is not for me to say, nor is that the question I am asking; but I can say that Weyl's sentiment makes sense to me. So far, Cantor's sentiments do not make sense to me (unless Cantor's principal sentiment is just that Kant is right and Plato is wrong, in which case I should stop reading Cantorian books, for a merely metaphysical dispute between Cantor and Plato is not a thing I would wish to pursue). Obviously, Cantor's sentiments make sense to a lot of very smart mathematicians, though; so, if there existed an example to bridge Cantor's mental world to the world of an engineer's experience, this would help to motivate my further reading. As matters stand, I am having a hard time understanding how Cantor and friends are talking about anything at all other than abstract games played with arbitrary definitions. So, indeed, an example would help. Hilbert's grand hotel? Sure, very funny, I suppose: a most ingenious paradox. I should spend the night at Hilbert's grand hotel, sometime. One hears that the hotel's cuisine is transcendental. Meanwhile, I admit that I just don't really get the point. I believe that there is a point; I just haven't gotten it, yet. To clarify: I am not requesting a general defense of mathematical foundations, but merely a pertinent example, relevant to an engineer, that illuminates the mathematician's interest in mathematical foundations.",,"['real-analysis', 'applications', 'foundations', 'cantor-set']"
77,A continuous function whose left derivative at a point is awlays twice its right derivative is constant,A continuous function whose left derivative at a point is awlays twice its right derivative is constant,,"Let $f: \mathbb{R}\to\mathbb{R}$ be a continuous function such that for all $x \in \mathbb{R}$, the right derivative of $f$ at $x$ is twice the left derivative of $f$ at $x$. Does it follow that $f$ is constant?","Let $f: \mathbb{R}\to\mathbb{R}$ be a continuous function such that for all $x \in \mathbb{R}$, the right derivative of $f$ at $x$ is twice the left derivative of $f$ at $x$. Does it follow that $f$ is constant?",,"['real-analysis', 'derivatives', 'continuity']"
78,Is the following function a norm?,Is the following function a norm?,,"Let $\| \|$ be any norm in $\mathbb{R}^d$. Consider now $d$ normed vector spaces $(V_i, \|\|_i)$ and let $V$ be the cartesian product vector space. Is the function $f$, given according to the rule $f(v) = \|(\|v_1\|_1, \ldots, \|v_d\|_d)\|$, a norm in $V$? (I can prove everything but the triangle inequality. As a matter of fact, any norm in $\mathbb{R}^d$ whose entries are non decreasing will make $f$ a norm; in particular, any $p$-norm make $f$ a norm -$p\in[1,\infty]$-).","Let $\| \|$ be any norm in $\mathbb{R}^d$. Consider now $d$ normed vector spaces $(V_i, \|\|_i)$ and let $V$ be the cartesian product vector space. Is the function $f$, given according to the rule $f(v) = \|(\|v_1\|_1, \ldots, \|v_d\|_d)\|$, a norm in $V$? (I can prove everything but the triangle inequality. As a matter of fact, any norm in $\mathbb{R}^d$ whose entries are non decreasing will make $f$ a norm; in particular, any $p$-norm make $f$ a norm -$p\in[1,\infty]$-).",,"['real-analysis', 'multivariable-calculus', 'normed-spaces']"
79,Prove convergence of hyperbolic recursive series,Prove convergence of hyperbolic recursive series,,"How to prove that the series $\{x_n\}$ converges, and find its limit, given: $$ \frac{1}{x_n^2} = \frac{1}{a^2} + \frac{1}{b^2+x^2_{n-1}}$$ I think, to prove convergence for recursive series, we can prove that it has a bound (I think this one has upper bound), and monotonicity. For monotinicity, we should be able to do it by deduction (assuming it holds for $n$). I am having a hard time transforming the series into a form that can be analyzed. If $b = 0$, I can prove it, but in this case, $b$ and $a$ are real positives. EDIT: (adding my own thoughts and approach) Consider a hyperbolic curve $f(x)=\frac{1}{x^2}$, then we could represent this recursive series with hyperbolic trigonometric functions. That is, assume $\alpha$ and $\alpha'$ are twice the angle of the vector $\{x_{n-1}^2, \frac{1}{x_{n-1}^2}\}$ and $\{x_{n}^2, \frac{1}{x_{n}^2}\}$ respectively. See image below. Now, with this reparametrization, we could transform the original series for $x_{n-1} \rightarrow x_{n}$ into a series for $\alpha \rightarrow \alpha'$. With some simple trigonometry, we have: $$ \left(cosh(\alpha) + b^2\right)^2 - \left(cosh(\alpha') + \frac{1}{a^2}\right)^2 = 1. $$ I think, this could again be reparameterized with trigonometry to prove that $\alpha \rightarrow \alpha'$ converges. However, I think I am stuck here. Maybe we should expand the this expression into series, or maybe use imaginary representation. Any idea for the proof?","How to prove that the series $\{x_n\}$ converges, and find its limit, given: $$ \frac{1}{x_n^2} = \frac{1}{a^2} + \frac{1}{b^2+x^2_{n-1}}$$ I think, to prove convergence for recursive series, we can prove that it has a bound (I think this one has upper bound), and monotonicity. For monotinicity, we should be able to do it by deduction (assuming it holds for $n$). I am having a hard time transforming the series into a form that can be analyzed. If $b = 0$, I can prove it, but in this case, $b$ and $a$ are real positives. EDIT: (adding my own thoughts and approach) Consider a hyperbolic curve $f(x)=\frac{1}{x^2}$, then we could represent this recursive series with hyperbolic trigonometric functions. That is, assume $\alpha$ and $\alpha'$ are twice the angle of the vector $\{x_{n-1}^2, \frac{1}{x_{n-1}^2}\}$ and $\{x_{n}^2, \frac{1}{x_{n}^2}\}$ respectively. See image below. Now, with this reparametrization, we could transform the original series for $x_{n-1} \rightarrow x_{n}$ into a series for $\alpha \rightarrow \alpha'$. With some simple trigonometry, we have: $$ \left(cosh(\alpha) + b^2\right)^2 - \left(cosh(\alpha') + \frac{1}{a^2}\right)^2 = 1. $$ I think, this could again be reparameterized with trigonometry to prove that $\alpha \rightarrow \alpha'$ converges. However, I think I am stuck here. Maybe we should expand the this expression into series, or maybe use imaginary representation. Any idea for the proof?",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'hyperbolic-geometry']"
80,Increasing $g$ where $g' = 0$ a.e. but $g$ not constant on any open interval?,Increasing  where  a.e. but  not constant on any open interval?,g g' = 0 g,"As the question title suggests, does there exist an increasing function $g$ such that $g' = 0$ almost everywhere  but $g$ isn't constant on any open interval?","As the question title suggests, does there exist an increasing function $g$ such that $g' = 0$ almost everywhere  but $g$ isn't constant on any open interval?",,[]
81,Video Lectures that closely follow Rudin's Real Analysis/Royden's Real Analysis,Video Lectures that closely follow Rudin's Real Analysis/Royden's Real Analysis,,"I plan to apply for the financial engineering course at NTU, Singapore. I intend to study real analysis and measure theory, as I have the mornings and evenings to myself after work. I am working through Rudin's PMA currently, with the help of online videos and am enjoying those. Can someone point me to any video lectures online, that closely follow Rudin's Real Analysis? It would be tremendous help. Thanks, Quasar","I plan to apply for the financial engineering course at NTU, Singapore. I intend to study real analysis and measure theory, as I have the mornings and evenings to myself after work. I am working through Rudin's PMA currently, with the help of online videos and am enjoying those. Can someone point me to any video lectures online, that closely follow Rudin's Real Analysis? It would be tremendous help. Thanks, Quasar",,['real-analysis']
82,Mean Value Theorem involving second derivative,Mean Value Theorem involving second derivative,,"If $f:[a,b]\to\mathbb R$ such that $f'(x)>0,f''(x)>0$ for all $x\in[a,b]$ , then $$\int_a^b e^{f(x)}\,dx\le (b-a)\frac{e^{f(b)}-e^{f(a)}}{f(b)-f(a)}$$ My progress so far: We have $\frac{1}{b-a} \int\limits_{a}^{b} e^{f(x)} =e^{f(\epsilon)}$ for some $\epsilon \in (a,b)$ . Also, take the function $e^x$ int the RHS, and apply Lagrange MVT to get $e^c$ for some $c \in (f(a),f(b))$ SO, we are left to show that $e^{f(\epsilon)} \le e^c$ or $f(\epsilon) \le c$ . I am stuck here. I don't know how to use the fact that $f''(x)>0$ .","If such that for all , then My progress so far: We have for some . Also, take the function int the RHS, and apply Lagrange MVT to get for some SO, we are left to show that or . I am stuck here. I don't know how to use the fact that .","f:[a,b]\to\mathbb R f'(x)>0,f''(x)>0 x\in[a,b] \int_a^b e^{f(x)}\,dx\le (b-a)\frac{e^{f(b)}-e^{f(a)}}{f(b)-f(a)} \frac{1}{b-a} \int\limits_{a}^{b} e^{f(x)} =e^{f(\epsilon)} \epsilon \in (a,b) e^x e^c c \in (f(a),f(b)) e^{f(\epsilon)} \le e^c f(\epsilon) \le c f''(x)>0",['real-analysis']
83,"$f>0$ on real line ; $f(x+y)\le f(x)f(y) , \forall x,y \in \mathbb R$ ; $f([0,1])$ is bounded set ; does $\lim_{x \to \infty}(f(x))^{1/x}$ exist?",on real line ;  ;  is bounded set ; does  exist?,"f>0 f(x+y)\le f(x)f(y) , \forall x,y \in \mathbb R f([0,1]) \lim_{x \to \infty}(f(x))^{1/x}","Let $f: \mathbb R \to (0,\infty)$ be a function such that $f(x+y)\le f(x)f(y) , \forall x,y \in \mathbb R$ and $f$ is bounded on $[0,1]$ ; then does the limit $\lim_{x \to \infty}(f(x))^{1/x}$ exists ? What I have found is $f(x)\le f(x/n)^n , \forall x \in \mathbb R , \forall n \in \mathbb N$ ; so say if $f(x) < M , \forall x \in [0,1]$ then we get $f(x)\le f\Big(\dfrac x{[x]+1}\Big)^{[x]+1}\le M^{[x]+1} \le M^{2x} , \forall x>1$ ; so that $(f(x))^{1/x}$ remains bounded for large $x$ . Please help . Thanks in advance .","Let $f: \mathbb R \to (0,\infty)$ be a function such that $f(x+y)\le f(x)f(y) , \forall x,y \in \mathbb R$ and $f$ is bounded on $[0,1]$ ; then does the limit $\lim_{x \to \infty}(f(x))^{1/x}$ exists ? What I have found is $f(x)\le f(x/n)^n , \forall x \in \mathbb R , \forall n \in \mathbb N$ ; so say if $f(x) < M , \forall x \in [0,1]$ then we get $f(x)\le f\Big(\dfrac x{[x]+1}\Big)^{[x]+1}\le M^{[x]+1} \le M^{2x} , \forall x>1$ ; so that $(f(x))^{1/x}$ remains bounded for large $x$ . Please help . Thanks in advance .",,['real-analysis']
84,$\int_0^\infty {x^a\over (x^2+1)^2} dx$ where $0<a<1$. [closed],where . [closed],\int_0^\infty {x^a\over (x^2+1)^2} dx 0<a<1,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question $\int_0^\infty {x^a\over (x^2+1)^2} dx$ where $0<a<1$. I know I can use partial fraction decomposition to obtain two different integrals, but I'm not sure how to integrate them. Any solutions or hints are greatly appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question $\int_0^\infty {x^a\over (x^2+1)^2} dx$ where $0<a<1$. I know I can use partial fraction decomposition to obtain two different integrals, but I'm not sure how to integrate them. Any solutions or hints are greatly appreciated.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'analysis']"
85,Show that the integral of a bounded measurable function of finite support is properly defined.,Show that the integral of a bounded measurable function of finite support is properly defined.,,"Edit: My post might be lacking context. The question was posed because the book has only defined the Lebesgue integral of functions on a measurable set $E$ where $m(E)<\infty.$ The definition below asks, if given a measurable $E$ whose measure can be finite or infinite, is it really properly defined? This is a problem from Royden and Fitzpatrick's Real Analysis. Can anybody check if my proof is correct? Suppose $f$ is a bounded and measurable function on E with a finite support, we can define its integral over $E$ by $$\int_E f = \int_{E{_0}} f$$ where $E_0$ has finite measure and $f\equiv 0$ on $E \sim E_0$. My attempt at proof . We wish to show that the definition above is independent of the choice of finite measure set $E_0$ where $f$ vanishes outside it. By hypothesis, $f$ has finite support, denote the support of $f$ by $E'=\{x\in E: f(x) \neq 0\}$. Let $E_0$ and $E_1$ be subsets of $E$ of finite measure where $f$ vanishes outside it. By the additivity over domains property of integration, since $E' \subseteq E_0$, and $E' \subseteq E_1$ (so $E'$ has finite measure as well), then we can write: $$\int_{E_0}f=\int_{E_0 \sim E'}f +\int_{E'}f, $$ and $$\int_{E_1}f=\int_{E_1 \sim E'}f +\int_{E'}f.$$ Therefore: $$\int_{E_0}f-\int_{E_1}f =\int_{E_0 \sim E'}f - \int_{E_1 \sim E'}f.$$ But, $E_0 \sim E'=\{x\in E_0 :f(x)=0 \}$ and $E_1 \sim E'=\{x\in E_1 :f(x)=0 \}$, so $f\equiv 0$ in both of these sets, hence, the integral of $f$ over these sets is $0$. Therefore: $$\int_{E_0}f=\int_{E_1}f.$$ Hence, the definition above is well defined since $E_0$ and $E_1$ are arbitrary.","Edit: My post might be lacking context. The question was posed because the book has only defined the Lebesgue integral of functions on a measurable set $E$ where $m(E)<\infty.$ The definition below asks, if given a measurable $E$ whose measure can be finite or infinite, is it really properly defined? This is a problem from Royden and Fitzpatrick's Real Analysis. Can anybody check if my proof is correct? Suppose $f$ is a bounded and measurable function on E with a finite support, we can define its integral over $E$ by $$\int_E f = \int_{E{_0}} f$$ where $E_0$ has finite measure and $f\equiv 0$ on $E \sim E_0$. My attempt at proof . We wish to show that the definition above is independent of the choice of finite measure set $E_0$ where $f$ vanishes outside it. By hypothesis, $f$ has finite support, denote the support of $f$ by $E'=\{x\in E: f(x) \neq 0\}$. Let $E_0$ and $E_1$ be subsets of $E$ of finite measure where $f$ vanishes outside it. By the additivity over domains property of integration, since $E' \subseteq E_0$, and $E' \subseteq E_1$ (so $E'$ has finite measure as well), then we can write: $$\int_{E_0}f=\int_{E_0 \sim E'}f +\int_{E'}f, $$ and $$\int_{E_1}f=\int_{E_1 \sim E'}f +\int_{E'}f.$$ Therefore: $$\int_{E_0}f-\int_{E_1}f =\int_{E_0 \sim E'}f - \int_{E_1 \sim E'}f.$$ But, $E_0 \sim E'=\{x\in E_0 :f(x)=0 \}$ and $E_1 \sim E'=\{x\in E_1 :f(x)=0 \}$, so $f\equiv 0$ in both of these sets, hence, the integral of $f$ over these sets is $0$. Therefore: $$\int_{E_0}f=\int_{E_1}f.$$ Hence, the definition above is well defined since $E_0$ and $E_1$ are arbitrary.",,"['real-analysis', 'proof-verification', 'lebesgue-integral']"
86,Compute $\lim_{x\to 0}\frac{x}{[x]}$,Compute,\lim_{x\to 0}\frac{x}{[x]},"When I take left hand limit of the function $\lim\limits_{x\to 0}\frac{x}{[x]}$ , then $\lim\limits_{h\to 0^{-}}\frac{-h}{[-h]}=\lim_{h\to 0^{-}}\frac{-h}{-1}=0$ where $0<h<1$ and $[\cdot ]$ is greatest integer function. But when I take right hand limit, then the function $\frac{h}{[h]}$ does not exists. so I do not understand what about $\lim\limits_{h\to 0^{+}}\frac{h}{[h]}$ and the limit of original function. Does the limit exists? please someone help me Thanks in advance.","When I take left hand limit of the function , then where and is greatest integer function. But when I take right hand limit, then the function does not exists. so I do not understand what about and the limit of original function. Does the limit exists? please someone help me Thanks in advance.",\lim\limits_{x\to 0}\frac{x}{[x]} \lim\limits_{h\to 0^{-}}\frac{-h}{[-h]}=\lim_{h\to 0^{-}}\frac{-h}{-1}=0 0<h<1 [\cdot ] \frac{h}{[h]} \lim\limits_{h\to 0^{+}}\frac{h}{[h]},"['real-analysis', 'sequences-and-series', 'limits', 'functions', 'ceiling-and-floor-functions']"
87,A property of the series $\sum_{i=1}^{\infty}\frac{(1-x)x^i}{1+x^i}$,A property of the series,\sum_{i=1}^{\infty}\frac{(1-x)x^i}{1+x^i},"I think this is a hard question: $$f(x)=\sum_{i=1}^{\infty}\frac{(1-x)x^i}{1+x^i}~~~\text{for}~~~x\in(0,1)$$ Prove: $$\lim_{x\to 1^-}f(x)=\ln(2)$$ Find the maximum value of $f(x)$.","I think this is a hard question: $$f(x)=\sum_{i=1}^{\infty}\frac{(1-x)x^i}{1+x^i}~~~\text{for}~~~x\in(0,1)$$ Prove: $$\lim_{x\to 1^-}f(x)=\ln(2)$$ Find the maximum value of $f(x)$.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
88,Fatou's lemma. Case of convergence in measure,Fatou's lemma. Case of convergence in measure,,"Fatou's lemma: Let $f_1, f_2, f_3, \cdots $ be a sequence of non-negative measurable functions on a measure space $(S,\Sigma,\mu)$. Define the function $f:S\to [0,\infty]$ a.e. pointwise limit by $$f(s)=\lim \inf\limits_{n\to \infty}f_n(s), \quad s\in S.$$ Then $f$ is measurable and $$\int \limits_{S}fd\mu \le \lim \inf\limits_{n\to \infty}\int \limits_{S}f_nd\mu. \qquad (1)$$ It's very famous and important claim. Prove that if $\{f_n\}$ converges in measure to some function $g$ then LHS of $(1)$ can be changed to $\int gd\mu$.","Fatou's lemma: Let $f_1, f_2, f_3, \cdots $ be a sequence of non-negative measurable functions on a measure space $(S,\Sigma,\mu)$. Define the function $f:S\to [0,\infty]$ a.e. pointwise limit by $$f(s)=\lim \inf\limits_{n\to \infty}f_n(s), \quad s\in S.$$ Then $f$ is measurable and $$\int \limits_{S}fd\mu \le \lim \inf\limits_{n\to \infty}\int \limits_{S}f_nd\mu. \qquad (1)$$ It's very famous and important claim. Prove that if $\{f_n\}$ converges in measure to some function $g$ then LHS of $(1)$ can be changed to $\int gd\mu$.",,"['real-analysis', 'measure-theory']"
89,"why the equation $f^{(n)}(x)=0$ has at least $n-1$ distinct roots in $(-1,1)$",why the equation  has at least  distinct roots in,"f^{(n)}(x)=0 n-1 (-1,1)","Let $f \in C^{(n)} ( (-1,1) )$ and $\sup_{-1<x<1}|f(x)|\leq 1$. show  that: There exists a number $\alpha_n$ depending only on $n$ such that if $|f'(0)|\geq \alpha_n$,then the equation $f^{(n)}(x)=0$ has at least $n-1$ distinct roots in $(-1,1)$ The problem is from Zorich analysis; the book hint: prove by induction that there exists a sequence $x_{k_{1}}<x_{k_{2}}<\cdots<x_{k_{k}}$ of pointsof the open inteval $(-1,1)$ such that $f^{(k)}(x_{k_{i}})\cdot f^{(k)}(x_{k_{i+1}})<0$ for $1\le i\le k-1$ By Now, I still can't use this hint to solve it","Let $f \in C^{(n)} ( (-1,1) )$ and $\sup_{-1<x<1}|f(x)|\leq 1$. show  that: There exists a number $\alpha_n$ depending only on $n$ such that if $|f'(0)|\geq \alpha_n$,then the equation $f^{(n)}(x)=0$ has at least $n-1$ distinct roots in $(-1,1)$ The problem is from Zorich analysis; the book hint: prove by induction that there exists a sequence $x_{k_{1}}<x_{k_{2}}<\cdots<x_{k_{k}}$ of pointsof the open inteval $(-1,1)$ such that $f^{(k)}(x_{k_{i}})\cdot f^{(k)}(x_{k_{i+1}})<0$ for $1\le i\le k-1$ By Now, I still can't use this hint to solve it",,['real-analysis']
90,Every function on $\mathbb{R}^n$ that is continuous in each variable separately is Borel measurable.,Every function on  that is continuous in each variable separately is Borel measurable.,\mathbb{R}^n,"I'm trying to solve exercise 2.11 from Folland's Real Analysis. Firstly, how do we show that $f_n$ is Borel measurable on $\mathbb{R} \times \mathbb{R}^k$? I really have no idea how to show this when we have separately Borel measurable functions. Next, I can't show that $f_n\to f$ pointwise. For any $(x,y)\in \mathbb{R}\times \mathbb{R}^k$, $f_n(x,y)=n[f(a_{i+1},y)(x-a_i)-f(a_i,y)(x-a_{i+1})]=n[x[f(a_{i+1},y)-f(a_i,y)]+\frac{1}{n}[f(a_i,y)-f(a_{i+1},y)]]$, but because of the $n$ outside the bracket, I can't show that this converges to $f$. How can I show this? Also, how can I conclude from this fact that every function on $R^n$ that is continuous in each variable separately is Borel measurable? I would greatly appreciate any suggestions, hints, or solutions.","I'm trying to solve exercise 2.11 from Folland's Real Analysis. Firstly, how do we show that $f_n$ is Borel measurable on $\mathbb{R} \times \mathbb{R}^k$? I really have no idea how to show this when we have separately Borel measurable functions. Next, I can't show that $f_n\to f$ pointwise. For any $(x,y)\in \mathbb{R}\times \mathbb{R}^k$, $f_n(x,y)=n[f(a_{i+1},y)(x-a_i)-f(a_i,y)(x-a_{i+1})]=n[x[f(a_{i+1},y)-f(a_i,y)]+\frac{1}{n}[f(a_i,y)-f(a_{i+1},y)]]$, but because of the $n$ outside the bracket, I can't show that this converges to $f$. How can I show this? Also, how can I conclude from this fact that every function on $R^n$ that is continuous in each variable separately is Borel measurable? I would greatly appreciate any suggestions, hints, or solutions.",,"['real-analysis', 'analysis', 'measure-theory']"
91,Solving functional equation $2f(x) = f(2x)$,Solving functional equation,2f(x) = f(2x),"$f(x)$ is a $\mathbb{R} \rightarrow \mathbb{R}$ differentiable function satisfying the following equation:  $$2f(x) = f(2x).$$ Can it be proved that $f(x) = kx$ for some $k$? Note that if $f(x)$ is in $\mathcal{C}^1$, it can be proved in the following way: $$g(x) := \bigg\{ \begin{array}{ll} f(x)/x & x \in \mathbb{R}\backslash\{0\} \\ \lim_{x\rightarrow0}f(x)/x = f'(0) &x=0 \end{array} \bigg. $$ is a continuous function in $\mathbb{R}$, satisfying $$ g(\ln x) = g(\ln x^2) \text{ for } x\in(0,+\infty).$$ Therefore, $\forall x \in(0,+\infty)$ $$ g(\ln x) = g(\ln x^{1/2}) = \lim_{n\rightarrow\infty}g(\ln x^{1/2^n}) = g(0),$$ which means $g(x)$ is a constant and $f(x) = kx$.","$f(x)$ is a $\mathbb{R} \rightarrow \mathbb{R}$ differentiable function satisfying the following equation:  $$2f(x) = f(2x).$$ Can it be proved that $f(x) = kx$ for some $k$? Note that if $f(x)$ is in $\mathcal{C}^1$, it can be proved in the following way: $$g(x) := \bigg\{ \begin{array}{ll} f(x)/x & x \in \mathbb{R}\backslash\{0\} \\ \lim_{x\rightarrow0}f(x)/x = f'(0) &x=0 \end{array} \bigg. $$ is a continuous function in $\mathbb{R}$, satisfying $$ g(\ln x) = g(\ln x^2) \text{ for } x\in(0,+\infty).$$ Therefore, $\forall x \in(0,+\infty)$ $$ g(\ln x) = g(\ln x^{1/2}) = \lim_{n\rightarrow\infty}g(\ln x^{1/2^n}) = g(0),$$ which means $g(x)$ is a constant and $f(x) = kx$.",,"['real-analysis', 'functional-equations']"
92,"Is $C^\infty_{0}(\bar{\Omega})$ dense in the hilbert space $W^{2,2}(\Omega)\cap W^{1,2}_{0}(\Omega)$",Is  dense in the hilbert space,"C^\infty_{0}(\bar{\Omega}) W^{2,2}(\Omega)\cap W^{1,2}_{0}(\Omega)","Assume $\Omega$ is open bounded domain in $\mathbb R^n$ Is $C^\infty_{0}(\bar{\Omega})$ dense in the hilbert space $W^{2,2}(\Omega)\cap W^{1,2}_{0}(\Omega)$ with inner product $$(u,v)=\int_{\Omega} \Delta u \Delta v $$ I ask this question because I have proved an equality for $C^\infty_{0}(\bar{\Omega})$ functions and I want to prove it for the hilbert space mentioned with density argument","Assume $\Omega$ is open bounded domain in $\mathbb R^n$ Is $C^\infty_{0}(\bar{\Omega})$ dense in the hilbert space $W^{2,2}(\Omega)\cap W^{1,2}_{0}(\Omega)$ with inner product $$(u,v)=\int_{\Omega} \Delta u \Delta v $$ I ask this question because I have proved an equality for $C^\infty_{0}(\bar{\Omega})$ functions and I want to prove it for the hilbert space mentioned with density argument",,"['real-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces']"
93,Lie groups and Lie algebras for matrices,Lie groups and Lie algebras for matrices,,"Recently, I stumbled over a few things in very basic Lie group/Lie algebra theory concerning matrix groups. Basically, my question is: Is there a way to canonically understand all the Lie groups at once? So there is the group $\mathrm{GL}(n, \mathbb{R})$ . This is the only group where I immediately see the manifold structure, as $\mathrm{GL}(n, \mathbb{R}) = \mathbb{R}^{n \times n} \setminus \det^{-1}(\{0\})$ is an open set in $\mathbb{R}^{n \times n}$ just by continuity of the determinant and hence it is a manifold of dimension $n^2$ . The Lie algebra in the context of real matrix theory is apparently given by all matrices $\{X; e^{tX} \in G\}$ where $G$ is the respective Lie group. From this it is clear that any endomorphism $X$ is in the Lie algebra of the general linear group, since $e^{tX}$ is always invertible. All other guys like $\mathrm{SL}, \mathrm{SO}, \mathrm{O}, \mathrm{Sp}, \dotsc$ are now somewhat harder to me. But maybe we can answer this question more canonically? Cause a friend of mine showed me a trick how to figure out what the Lie algebras might be without investigating this exponential. Let $Y = \mathrm{Id} + tX + o(t^2)$ be a Taylor expansion and then you plug this into the defining property of the group: For instance, if we consider $\mathrm{O}(n)$ : $$   Y^T Y   = (\mathrm{Id} + t X^t + o(t^2))(\mathrm{Id} + t X^t + o(t^2))   = \mathrm{Id} + t X + t X^t + o(t^2) \,, $$ now the Lie algebra is the set of all matrices that have no order $t$ term in this expansion. So in the case of the orthogonal group this means that $X + X^t = 0$ . In the case of $\mathrm{SL}(n)$ the defining property is that $\det(\mathrm{Id} + tX) = 1$ which is for small $t$ equivalent to $\det(\mathrm{Id} + tX) \approx 1 + t \cdot \mathrm{tr}(X) = 1$ and hence we must have trace-free matrices in the respective Lie algebra. Somehow, we can apparently conclude what the Lie algebra has to be from first order Taylor expansion. This is nice because it seems to work generally. My first question is: Why does this work and where does the condition that the first order term has to vanish come from? Now, I would like to understand the Lie groups itself also better and maybe also in a more general way. I don't see how the manifold structure is actually defined on these groups. Sure, in the case of $\mathrm{O}$ or $\mathrm{SO}$ the regular value theorem may be applied somehow, but this does not provide me with a parametrization or chart explicitly. In especially, is there a canonical way to find charts/parametrizations for such matrix groups or is this different in each case? Is there a way to see directly what the dimension of any such matrix group has to be? I mean, think of you sitting in an exam. Is there a way to figure out what the dimension of the respective manifold has to be? Or is this something that has to be investigated in each case differently?","Recently, I stumbled over a few things in very basic Lie group/Lie algebra theory concerning matrix groups. Basically, my question is: Is there a way to canonically understand all the Lie groups at once? So there is the group . This is the only group where I immediately see the manifold structure, as is an open set in just by continuity of the determinant and hence it is a manifold of dimension . The Lie algebra in the context of real matrix theory is apparently given by all matrices where is the respective Lie group. From this it is clear that any endomorphism is in the Lie algebra of the general linear group, since is always invertible. All other guys like are now somewhat harder to me. But maybe we can answer this question more canonically? Cause a friend of mine showed me a trick how to figure out what the Lie algebras might be without investigating this exponential. Let be a Taylor expansion and then you plug this into the defining property of the group: For instance, if we consider : now the Lie algebra is the set of all matrices that have no order term in this expansion. So in the case of the orthogonal group this means that . In the case of the defining property is that which is for small equivalent to and hence we must have trace-free matrices in the respective Lie algebra. Somehow, we can apparently conclude what the Lie algebra has to be from first order Taylor expansion. This is nice because it seems to work generally. My first question is: Why does this work and where does the condition that the first order term has to vanish come from? Now, I would like to understand the Lie groups itself also better and maybe also in a more general way. I don't see how the manifold structure is actually defined on these groups. Sure, in the case of or the regular value theorem may be applied somehow, but this does not provide me with a parametrization or chart explicitly. In especially, is there a canonical way to find charts/parametrizations for such matrix groups or is this different in each case? Is there a way to see directly what the dimension of any such matrix group has to be? I mean, think of you sitting in an exam. Is there a way to figure out what the dimension of the respective manifold has to be? Or is this something that has to be investigated in each case differently?","\mathrm{GL}(n, \mathbb{R}) \mathrm{GL}(n, \mathbb{R}) = \mathbb{R}^{n \times n} \setminus \det^{-1}(\{0\}) \mathbb{R}^{n \times n} n^2 \{X; e^{tX} \in G\} G X e^{tX} \mathrm{SL}, \mathrm{SO}, \mathrm{O}, \mathrm{Sp}, \dotsc Y = \mathrm{Id} + tX + o(t^2) \mathrm{O}(n) 
  Y^T Y
  = (\mathrm{Id} + t X^t + o(t^2))(\mathrm{Id} + t X^t + o(t^2))
  = \mathrm{Id} + t X + t X^t + o(t^2) \,,
 t X + X^t = 0 \mathrm{SL}(n) \det(\mathrm{Id} + tX) = 1 t \det(\mathrm{Id} + tX) \approx 1 + t \cdot \mathrm{tr}(X) = 1 \mathrm{O} \mathrm{SO}","['real-analysis', 'differential-geometry', 'differential-topology', 'lie-groups', 'lie-algebras']"
94,"Calculating $\int_0^{\infty } \frac{\log (v+1)}{\sqrt{(v+1)^2+1} \sqrt{(v+1)^2+4 \sqrt{(v+1)^2+1} (v+1)+4}} \, dv$",Calculating,"\int_0^{\infty } \frac{\log (v+1)}{\sqrt{(v+1)^2+1} \sqrt{(v+1)^2+4 \sqrt{(v+1)^2+1} (v+1)+4}} \, dv","What tools would you recommend me for this one? $$\int_0^{\infty } \frac{\log (v+1)}{\sqrt{(v+1)^2+1} \sqrt{(v+1)^2+4 \sqrt{(v+1)^2+1} (v+1)+4}} \, dv$$ It's related to Calculate in closed form $\int_0^1 \int_0^1 \frac{dx\,dy}{1-xy(1-x)(1-y)}$ if my so long calculations are correct, and it only represents a tiny bit of the whole story.","What tools would you recommend me for this one? $$\int_0^{\infty } \frac{\log (v+1)}{\sqrt{(v+1)^2+1} \sqrt{(v+1)^2+4 \sqrt{(v+1)^2+1} (v+1)+4}} \, dv$$ It's related to Calculate in closed form $\int_0^1 \int_0^1 \frac{dx\,dy}{1-xy(1-x)(1-y)}$ if my so long calculations are correct, and it only represents a tiny bit of the whole story.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
95,RH would follow from $\displaystyle \frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log\log N_n} $ for all $n>1$; what is my mistake?,RH would follow from  for all ; what is my mistake?,\displaystyle \frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log\log N_n}  n>1,"Let $N_n=\prod_{k=1}^np_k$ be the primorial of order $n$,$\gamma$ be the Euler-Mascheroni constant and $\varphi$ denote the Euler phi function. Nicolas showed that if the Riemann Hypothesis is true, then $$\forall \ n\in\mathbb{Z^+}, \ \frac{N_n}{\varphi(N_n)}=\prod_{k=1}^n\frac{p_k}{p_k-1}>e^\gamma \log\log N_n.\tag{NI}$$He also proved that the falsity of RH would imply the existence of infinitely many $n$ violating NI, as well as infinitely many $n$ satisfying it. Now, the combination of the following statement, equivalent to PNT:$$\lim_{n\to\infty}\frac{p_n}{\log N_n}=1$$ and Mertens' third theorem $$\lim_{n\to\infty}\frac{1}{\log p_n}\prod_{k=1}^n\frac{p_k}{p_k-1}=e^\gamma$$ yields $$\lim_{n\to\infty}\frac{1}{\log\log N_n}\prod_{k=1}^n\frac{p_k}{p_k-1}=e^\gamma,$$ thus NI would follow from the decreasingness, even only for large enough $n$, of the sequence $$u_n=\frac{1}{\log\log N_n}\prod_{k=1}^n\frac{p_k}{p_k-1}.$$ Therefore, since $\displaystyle \log \log 2 $ is negative, let us consider for $n>1$ $$\frac{u_{n+1}}{u_n}=\frac{\log\log N_n}{\log\log N_{n+1}}\frac{p_{n+1}}{p_{n+1}-1}<1 $$ $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log \log N_n}=\frac{\log\log N_{n+1}}{\log(\log N_{n+1}-\log p_{n+1})}.\tag{$\star$}$$  We have $\log N_{n+1}<p_{n+1}$, wherefrom, for any real $r>\log p_{n+1}+1,$ $$\frac{1}{\log(r-\log\log N_{n+1})}<\frac{1}{\log(r-\log p_{n+1})},$$ so as long as $\log N_{n+1}>\log p_{n+1}+1$ for $n>1$, $(\star)$ is weaker than  $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log(\log N_{n+1}-\log \log N_{n+1})}.\tag{1}$$ But $\displaystyle f(x)=\frac{\log x}{\log(x-\log x)}$ is decreasing on $(1,+\infty)$: $$\require\cancel f'(x)= \frac{\log(x-\log x)/x - \log x \cdot D(\log(x-\log x))}{\cancel{\log^2(x-\log x)}}<0 \\ \frac{\log(x-\log x)}{x}<\frac{(1-1/x)\log x}{x-\log x}=\frac{(x-1)\log x}{{x}(x-\log x)} \\ (x-\log x)\log(x-\log x)<(x-1)\log x \\ (x-\log x)^{x-\log x}<x^{x-1},$$ which follows from $$(x-\log x)^x < x^{x-1} \\ \left(1-\frac{\log x}{x}\right)^x<x^{-1} \\ \left(1-\frac{\log x}{x}\right)^{x/\log x}<x^{-1/\log x} =e^{-1},$$ which, setting $\displaystyle t=\frac{x}{\log x}$, reduces to the familiar $$\left(1-\frac{1}{t}\right)^t<e^{-1}.\tag{2}$$ With $x$ in place of $t$ we would say $(2)$ holds for $x>1$, but given that the range of $\displaystyle \frac{x}{\log x}$ does not include $[0,e)$, we really need $t>0$, or equivalently, again, $x>1$. Hence $(1)$ is in turn implied by $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log p_{n+1}}{\log(p_{n+1}-\log p_{n+1})} \\ p_{n+1}\log(p_{n+1}-\log p_{n+1})<(p_{n+1}-1)\log p_{n+1} \\ (p_{n+1}-\log p_{n+1})^{p_{n+1}}<p_{n+1}^{p_{n+1}-1},$$ and this we have already showed to ensue from $(2)$. Thence, $(\star)$ is true and so is NI. However, not only such a result would prove RH, but in this paper it is shown that such a result would confute Cramér's conjecture . On account of this I expect to have made some stupid mistake, I just cannot see it. Thank you for any observations.","Let $N_n=\prod_{k=1}^np_k$ be the primorial of order $n$,$\gamma$ be the Euler-Mascheroni constant and $\varphi$ denote the Euler phi function. Nicolas showed that if the Riemann Hypothesis is true, then $$\forall \ n\in\mathbb{Z^+}, \ \frac{N_n}{\varphi(N_n)}=\prod_{k=1}^n\frac{p_k}{p_k-1}>e^\gamma \log\log N_n.\tag{NI}$$He also proved that the falsity of RH would imply the existence of infinitely many $n$ violating NI, as well as infinitely many $n$ satisfying it. Now, the combination of the following statement, equivalent to PNT:$$\lim_{n\to\infty}\frac{p_n}{\log N_n}=1$$ and Mertens' third theorem $$\lim_{n\to\infty}\frac{1}{\log p_n}\prod_{k=1}^n\frac{p_k}{p_k-1}=e^\gamma$$ yields $$\lim_{n\to\infty}\frac{1}{\log\log N_n}\prod_{k=1}^n\frac{p_k}{p_k-1}=e^\gamma,$$ thus NI would follow from the decreasingness, even only for large enough $n$, of the sequence $$u_n=\frac{1}{\log\log N_n}\prod_{k=1}^n\frac{p_k}{p_k-1}.$$ Therefore, since $\displaystyle \log \log 2 $ is negative, let us consider for $n>1$ $$\frac{u_{n+1}}{u_n}=\frac{\log\log N_n}{\log\log N_{n+1}}\frac{p_{n+1}}{p_{n+1}-1}<1 $$ $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log \log N_n}=\frac{\log\log N_{n+1}}{\log(\log N_{n+1}-\log p_{n+1})}.\tag{$\star$}$$  We have $\log N_{n+1}<p_{n+1}$, wherefrom, for any real $r>\log p_{n+1}+1,$ $$\frac{1}{\log(r-\log\log N_{n+1})}<\frac{1}{\log(r-\log p_{n+1})},$$ so as long as $\log N_{n+1}>\log p_{n+1}+1$ for $n>1$, $(\star)$ is weaker than  $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log(\log N_{n+1}-\log \log N_{n+1})}.\tag{1}$$ But $\displaystyle f(x)=\frac{\log x}{\log(x-\log x)}$ is decreasing on $(1,+\infty)$: $$\require\cancel f'(x)= \frac{\log(x-\log x)/x - \log x \cdot D(\log(x-\log x))}{\cancel{\log^2(x-\log x)}}<0 \\ \frac{\log(x-\log x)}{x}<\frac{(1-1/x)\log x}{x-\log x}=\frac{(x-1)\log x}{{x}(x-\log x)} \\ (x-\log x)\log(x-\log x)<(x-1)\log x \\ (x-\log x)^{x-\log x}<x^{x-1},$$ which follows from $$(x-\log x)^x < x^{x-1} \\ \left(1-\frac{\log x}{x}\right)^x<x^{-1} \\ \left(1-\frac{\log x}{x}\right)^{x/\log x}<x^{-1/\log x} =e^{-1},$$ which, setting $\displaystyle t=\frac{x}{\log x}$, reduces to the familiar $$\left(1-\frac{1}{t}\right)^t<e^{-1}.\tag{2}$$ With $x$ in place of $t$ we would say $(2)$ holds for $x>1$, but given that the range of $\displaystyle \frac{x}{\log x}$ does not include $[0,e)$, we really need $t>0$, or equivalently, again, $x>1$. Hence $(1)$ is in turn implied by $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log p_{n+1}}{\log(p_{n+1}-\log p_{n+1})} \\ p_{n+1}\log(p_{n+1}-\log p_{n+1})<(p_{n+1}-1)\log p_{n+1} \\ (p_{n+1}-\log p_{n+1})^{p_{n+1}}<p_{n+1}^{p_{n+1}-1},$$ and this we have already showed to ensue from $(2)$. Thence, $(\star)$ is true and so is NI. However, not only such a result would prove RH, but in this paper it is shown that such a result would confute Cramér's conjecture . On account of this I expect to have made some stupid mistake, I just cannot see it. Thank you for any observations.",,"['real-analysis', 'number-theory', 'proof-verification', 'prime-numbers', 'riemann-hypothesis']"
96,My proof of $\lim_{n\to\infty} (1 / n) = 0$,My proof of,\lim_{n\to\infty} (1 / n) = 0,"Is my proof correct? We consider the sequence   \begin{equation*}     (x_n)_{n = 1}^{\infty},     \qquad \text{where} \qquad     x_n = \frac{1}{n}.   \end{equation*} $\textbf{Theorem.}$   \begin{equation*}     \lim_{n \to \infty} x_n = 0.   \end{equation*} $\textit{Proof.}$   By definition, we have   \begin{equation*}     \lim_{n \to \infty} x_n = 0   \end{equation*}   if and only if,   for every positive real number $\varepsilon$,   there is a natural number $N$ such that,   for every natural number $n > N$,   we have $|x_n - 0| < \varepsilon$, i.e. $|x_n| < \varepsilon$.   Let $\varepsilon \in \mathbb{R}$ such that $\varepsilon > 0$.   It remains to prove that there is a suitable $N$.   Necessarily, there is a $N \in \mathbb{N}$   such that $N > 1 / \varepsilon$.   It remains to prove that this $N$ is suitable.   To that end, let $n \in \mathbb{N}$ such that $n > N$.   Obviously, $n$ is positive.   It remains to prove that   $|  x_n| < \varepsilon$, i.e.   $|1 / n| < \varepsilon$.   Since $1 / \varepsilon < N$ and $N < n$,   we have the inequality $1 / \varepsilon < n$.   Certainly, each side of the inequality is positive.   Thus,   \begin{equation*}     \begin{split}       (1 / \varepsilon)^{-1} & > n^{-1} \\                  \varepsilon & > 1 / n.     \end{split}   \end{equation*}   Also, since $n$ is positive, we have $1 / n = | 1 / n |$.   Thus, $| 1 / n | < \varepsilon$.   QED","Is my proof correct? We consider the sequence   \begin{equation*}     (x_n)_{n = 1}^{\infty},     \qquad \text{where} \qquad     x_n = \frac{1}{n}.   \end{equation*} $\textbf{Theorem.}$   \begin{equation*}     \lim_{n \to \infty} x_n = 0.   \end{equation*} $\textit{Proof.}$   By definition, we have   \begin{equation*}     \lim_{n \to \infty} x_n = 0   \end{equation*}   if and only if,   for every positive real number $\varepsilon$,   there is a natural number $N$ such that,   for every natural number $n > N$,   we have $|x_n - 0| < \varepsilon$, i.e. $|x_n| < \varepsilon$.   Let $\varepsilon \in \mathbb{R}$ such that $\varepsilon > 0$.   It remains to prove that there is a suitable $N$.   Necessarily, there is a $N \in \mathbb{N}$   such that $N > 1 / \varepsilon$.   It remains to prove that this $N$ is suitable.   To that end, let $n \in \mathbb{N}$ such that $n > N$.   Obviously, $n$ is positive.   It remains to prove that   $|  x_n| < \varepsilon$, i.e.   $|1 / n| < \varepsilon$.   Since $1 / \varepsilon < N$ and $N < n$,   we have the inequality $1 / \varepsilon < n$.   Certainly, each side of the inequality is positive.   Thus,   \begin{equation*}     \begin{split}       (1 / \varepsilon)^{-1} & > n^{-1} \\                  \varepsilon & > 1 / n.     \end{split}   \end{equation*}   Also, since $n$ is positive, we have $1 / n = | 1 / n |$.   Thus, $| 1 / n | < \varepsilon$.   QED",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'proof-verification']"
97,Find the values of the positive integers $n$ such that: $\frac{(-\sqrt{3}+2)^{n+1}+(\sqrt{3}+2)^{n+1}}{4n+3}$ is positive integer,Find the values of the positive integers  such that:  is positive integer,n \frac{(-\sqrt{3}+2)^{n+1}+(\sqrt{3}+2)^{n+1}}{4n+3},"My question is as follow: Find the values of the positive integers $n$ such that: $$\frac{(-\sqrt{3}+2)^{n+1}+(\sqrt{3}+2)^{n+1}}{4n+3}$$ is positive integer. I can see that for $n=1$ (among some other numbers) the condition is verified. However, I cannot go further with this procedure.","My question is as follow: Find the values of the positive integers $n$ such that: $$\frac{(-\sqrt{3}+2)^{n+1}+(\sqrt{3}+2)^{n+1}}{4n+3}$$ is positive integer. I can see that for $n=1$ (among some other numbers) the condition is verified. However, I cannot go further with this procedure.",,"['real-analysis', 'sequences-and-series', 'real-numbers']"
98,Differentiating a constant and switching order,Differentiating a constant and switching order,,"Why does this work? $$\int x^2e^{ax}dx = \int \frac{d^2}{da^2}e^{ax}dx = \frac{d^2}{da^2}\int e^{ax}dx = \frac {d^2}{da^2} \frac {e^{ax}}a = \frac{e^{ax}(a^2x^2-2ax+2)}{a^3}$$ $a$ is a constant, so how can you take the derivative with respect to it?  Also, why can you just switch the order of integration with differentiation?  We did this is physics, but what's the justification?  Will this type of thing always work?","Why does this work? $$\int x^2e^{ax}dx = \int \frac{d^2}{da^2}e^{ax}dx = \frac{d^2}{da^2}\int e^{ax}dx = \frac {d^2}{da^2} \frac {e^{ax}}a = \frac{e^{ax}(a^2x^2-2ax+2)}{a^3}$$ $a$ is a constant, so how can you take the derivative with respect to it?  Also, why can you just switch the order of integration with differentiation?  We did this is physics, but what's the justification?  Will this type of thing always work?",,"['calculus', 'real-analysis']"
99,"If $\sum{a_k}$ converges, then $\lim ka_k=0$. [duplicate]","If  converges, then . [duplicate]",\sum{a_k} \lim ka_k=0,"This question already has answers here : If $x_{n}$ is decreasing and $\sum x_{n}$ converges, prove that $\lim nx_{n} = 0$ [duplicate] (2 answers) Closed 9 years ago . I want to prove the following statement: Suppose that $\displaystyle\sum_{k=1}^{\infty}a_k$ converges, where $(a_k)_{k\in\mathbb{N}}\subseteq\mathbb{R}$ is monotone. Then $\displaystyle\lim_{k\to\infty}ka_k=0$. I believe we have several cases. For example, if $(a_k)_{k\in\mathbb{N}}$ is monotone increasing and there exists $k$ such that $a_k>0$, then obviously $\displaystyle\sum_{k=1}^{\infty}a_k$ is not convergent. Then, we could conclude that if some $a_k>0$ then we can suppose that $(a_k)_{k\in\mathbb{N}}$ is monotone decreasing. By the same argument, we can conclude that if some $a_k<0$ then $(a_k)_{k\in\mathbb{N}}$ must be monotone increasing. So, I believe we only need to take care of the case where $a_k\ge 0$ for each $k\in\mathbb{N}$ and $(a_k)_{k\in\mathbb{N}}$ is monotone decreasing (the other case would be symmetric). Any hint to prove this? I have been thinking a lot ot time... Thanks.","This question already has answers here : If $x_{n}$ is decreasing and $\sum x_{n}$ converges, prove that $\lim nx_{n} = 0$ [duplicate] (2 answers) Closed 9 years ago . I want to prove the following statement: Suppose that $\displaystyle\sum_{k=1}^{\infty}a_k$ converges, where $(a_k)_{k\in\mathbb{N}}\subseteq\mathbb{R}$ is monotone. Then $\displaystyle\lim_{k\to\infty}ka_k=0$. I believe we have several cases. For example, if $(a_k)_{k\in\mathbb{N}}$ is monotone increasing and there exists $k$ such that $a_k>0$, then obviously $\displaystyle\sum_{k=1}^{\infty}a_k$ is not convergent. Then, we could conclude that if some $a_k>0$ then we can suppose that $(a_k)_{k\in\mathbb{N}}$ is monotone decreasing. By the same argument, we can conclude that if some $a_k<0$ then $(a_k)_{k\in\mathbb{N}}$ must be monotone increasing. So, I believe we only need to take care of the case where $a_k\ge 0$ for each $k\in\mathbb{N}$ and $(a_k)_{k\in\mathbb{N}}$ is monotone decreasing (the other case would be symmetric). Any hint to prove this? I have been thinking a lot ot time... Thanks.",,['real-analysis']
