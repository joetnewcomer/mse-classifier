,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find $k^{th}$ power of a square matrix,Find  power of a square matrix,k^{th},"I am trying to find the $A^{k}$ , for all $k \geq 2$ of a matrix, \begin{pmatrix}  a & b \\ 0 & 1  \end{pmatrix} My approach: $A^{2}=\begin{pmatrix}  a^2 & ab+b \\ 0 & 1 \end{pmatrix}$ $A^{3}=\begin{pmatrix}  a^3 & a^{2}b+ab+b \\ 0 & 1 \end{pmatrix}$ $A^{4}=\begin{pmatrix}  a^4 & a^{3}b+a^{2}b+ab+b \\ 0 & 1 \end{pmatrix}$ $A^{5}=\begin{pmatrix}  a^5 & a^{4}b+a^{3}b+a^{2}b+ab+b  \\ 0 & 1 \end{pmatrix}$ Continuing this way, we obtain $A^{k}=\begin{pmatrix}  a^k & (a^{k-2}+a^{k-3}+a^{k-4}+.....+1)b  \\ 0 & 1 \end{pmatrix}$ I am stuck here! I was wondering if you could give me some hints to move further. I appreciate your time.","I am trying to find the , for all of a matrix, My approach: Continuing this way, we obtain I am stuck here! I was wondering if you could give me some hints to move further. I appreciate your time.",A^{k} k \geq 2 \begin{pmatrix}  a & b \\ 0 & 1  \end{pmatrix} A^{2}=\begin{pmatrix}  a^2 & ab+b \\ 0 & 1 \end{pmatrix} A^{3}=\begin{pmatrix}  a^3 & a^{2}b+ab+b \\ 0 & 1 \end{pmatrix} A^{4}=\begin{pmatrix}  a^4 & a^{3}b+a^{2}b+ab+b \\ 0 & 1 \end{pmatrix} A^{5}=\begin{pmatrix}  a^5 & a^{4}b+a^{3}b+a^{2}b+ab+b  \\ 0 & 1 \end{pmatrix} A^{k}=\begin{pmatrix}  a^k & (a^{k-2}+a^{k-3}+a^{k-4}+.....+1)b  \\ 0 & 1 \end{pmatrix},"['linear-algebra', 'matrices', 'diagonalization']"
1,Is the tensor product (of vector spaces) commutative?,Is the tensor product (of vector spaces) commutative?,,"I've just learned a bit about the tensor product and I couldn't find a real answer to this. I've read something about, that in some cases it could be or not. Let's consider next example: In the vector space $\mathbb{R}^n\otimes_\mathbb{R}\mathbb{R}^n$ with standard basis $\mathbb{B}=(e_1,...,e_n)$ of $\mathbb{R}^n$ , can we say that $e_1\otimes e_2=e_2\otimes e_1$ ? If yes can we say that $\otimes$ is commutative in a vector space $V\otimes V$ generated by the tensor product of a vector space $V$ with itself? If not, when can it be considered?","I've just learned a bit about the tensor product and I couldn't find a real answer to this. I've read something about, that in some cases it could be or not. Let's consider next example: In the vector space with standard basis of , can we say that ? If yes can we say that is commutative in a vector space generated by the tensor product of a vector space with itself? If not, when can it be considered?","\mathbb{R}^n\otimes_\mathbb{R}\mathbb{R}^n \mathbb{B}=(e_1,...,e_n) \mathbb{R}^n e_1\otimes e_2=e_2\otimes e_1 \otimes V\otimes V V","['linear-algebra', 'vector-spaces', 'tensor-products']"
2,can a Linear operator on infinite dimensional vector space have infinite eigenvalues?,can a Linear operator on infinite dimensional vector space have infinite eigenvalues?,,"I know that for a finite dimensional vector space, the number of eigenvalues is at most the dimension of the vector space. Is there an example of an infinite dimensional vector space and a linear operator such that the linear operator allows infinite eigenvalues? I cannot think of such an example, but I was wondering if it is possible to construct one since I was also unable to prove why the number of eigenvalues would necessarily be finite in such a case.","I know that for a finite dimensional vector space, the number of eigenvalues is at most the dimension of the vector space. Is there an example of an infinite dimensional vector space and a linear operator such that the linear operator allows infinite eigenvalues? I cannot think of such an example, but I was wondering if it is possible to construct one since I was also unable to prove why the number of eigenvalues would necessarily be finite in such a case.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
3,Does the zero product property hold in vector spaces? [duplicate],Does the zero product property hold in vector spaces? [duplicate],,This question already has answers here : Does $\lambda$ $\mathbf u$ $= 0$ imply $\lambda = 0$ or $\mathbf u$ $= 0$? (4 answers) Closed 6 years ago . Suppose $V$ is a vector space over a field $F$.  Let $v \in V\setminus \{0\}$ and $\lambda \in F$.  Does $\lambda v= 0$ imply $\lambda = 0$?,This question already has answers here : Does $\lambda$ $\mathbf u$ $= 0$ imply $\lambda = 0$ or $\mathbf u$ $= 0$? (4 answers) Closed 6 years ago . Suppose $V$ is a vector space over a field $F$.  Let $v \in V\setminus \{0\}$ and $\lambda \in F$.  Does $\lambda v= 0$ imply $\lambda = 0$?,,['linear-algebra']
4,What is the essence of Kernel or Null Space?,What is the essence of Kernel or Null Space?,,"I describe Kernel in this way: Kernel(In group theory, Linear Algebra, etc.) measures the degree to which a map or morphism fails to be injective. In ""finite dimensional"" case, it also measures the degree to which a map or morphism fails to be surjective. How to give a more general definition of Kernel, instead of just defining them on a special Algebraic structure, such as groups?","I describe Kernel in this way: Kernel(In group theory, Linear Algebra, etc.) measures the degree to which a map or morphism fails to be injective. In ""finite dimensional"" case, it also measures the degree to which a map or morphism fails to be surjective. How to give a more general definition of Kernel, instead of just defining them on a special Algebraic structure, such as groups?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'soft-question']"
5,Coset and set of all cosets,Coset and set of all cosets,,Let $C$ be a subspace of a vector space $V$ and $x \in V$. The set $x + C = \{ x + c : c \in C \}$ is called a coset of $C$. The set of all cosets is denoted $V/C$. Can you help me with some examples and intuition behind the concept of coset?  I find hard to grasp the real meaning of this definition.,Let $C$ be a subspace of a vector space $V$ and $x \in V$. The set $x + C = \{ x + c : c \in C \}$ is called a coset of $C$. The set of all cosets is denoted $V/C$. Can you help me with some examples and intuition behind the concept of coset?  I find hard to grasp the real meaning of this definition.,,['linear-algebra']
6,Plane determined by two lines,Plane determined by two lines,,"Show that the lines $x=-2+t,y=3+2t,z=4-t$ and $x=3-t,y=4-2t,z=t$ are parallel. Find the equation of the plane they determine. Here what is the meaning of ""they determine""?","Show that the lines $x=-2+t,y=3+2t,z=4-t$ and $x=3-t,y=4-2t,z=t$ are parallel. Find the equation of the plane they determine. Here what is the meaning of ""they determine""?",,['linear-algebra']
7,Proving the inverse of a matrix as a polynomial of matrices,Proving the inverse of a matrix as a polynomial of matrices,,"Suppose $A$ is invertible $n\times n$ matrix. Show that $A^{-1}$ can be written as a polynomial of degree at most $n-1$, i.e. $$A^{-1} = c_{n-1}A^{n-1} + \dots + \ c_{1}A + c_{0}I$$ Are there any tips on how may this proof be approached?","Suppose $A$ is invertible $n\times n$ matrix. Show that $A^{-1}$ can be written as a polynomial of degree at most $n-1$, i.e. $$A^{-1} = c_{n-1}A^{n-1} + \dots + \ c_{1}A + c_{0}I$$ Are there any tips on how may this proof be approached?",,"['linear-algebra', 'matrices', 'matrix-equations']"
8,Is the sum of two projections a projection?,Is the sum of two projections a projection?,,"Let $ S $ and $ T $ be two linear subspaces of $ \Bbb{R}^{2} $. Then is the sum of the projections $ P_{S} $ and $ P_{T} $ (i.e., $ P_{S} + P_{T} $) a projection? I don’t think it is since the projection rule doesn’t hold ($ P_{M}^{2} $ does not equal $ P_{M} $), but I was hoping someone could solidify this. It would be very much appreciated!","Let $ S $ and $ T $ be two linear subspaces of $ \Bbb{R}^{2} $. Then is the sum of the projections $ P_{S} $ and $ P_{T} $ (i.e., $ P_{S} + P_{T} $) a projection? I don’t think it is since the projection rule doesn’t hold ($ P_{M}^{2} $ does not equal $ P_{M} $), but I was hoping someone could solidify this. It would be very much appreciated!",,['linear-algebra']
9,Dot product for 3 vectors,Dot product for 3 vectors,,The dot product can be used to write the sum: $$\sum_{i=1}^n a_i b_i$$ as $$a^T b$$ Is there an equivalent notation for the following sum: $$\sum_{i=1}^n a_i b_i c_i$$,The dot product can be used to write the sum: $$\sum_{i=1}^n a_i b_i$$ as $$a^T b$$ Is there an equivalent notation for the following sum: $$\sum_{i=1}^n a_i b_i c_i$$,,"['linear-algebra', 'notation']"
10,How do I show that the derivative of the path $\det(I + tA)$ at $t = 0$ is the trace of $A$? [duplicate],How do I show that the derivative of the path  at  is the trace of ? [duplicate],\det(I + tA) t = 0 A,"This question already has answers here : derivative of a determinant of matrix (2 answers) Closed 8 years ago . Here, I'm taking $A$ to be a linear operator on $\mathbb R^n$ for $n>1$. Can you please tell me how to solve such a problem?","This question already has answers here : derivative of a determinant of matrix (2 answers) Closed 8 years ago . Here, I'm taking $A$ to be a linear operator on $\mathbb R^n$ for $n>1$. Can you please tell me how to solve such a problem?",,['linear-algebra']
11,Alternate proof of Schur orthogonality relations,Alternate proof of Schur orthogonality relations,,"I am trying to find an alternate proof for Schur orthogonality relations along the following lines. Let $G$ be a finite group, with irreducible representations $V_1$, $V_2$, $\cdots$, $V_d$. Let $V$ denote the adjoint representation ($V = \langle \{e_g\}_{g\in G} \rangle$ with $h\cdot e_g = e_{hgh^{-1}}$). Also, let $W$ denote the representation given by $\bigoplus_{i=1}^{d} \text{End} (V_i)$. The Schur orthogonality relations tell us these both have the same character, hence are isomorphic. But this requires one to assume the fact that $\{\chi_{V_i} \}_{i=1..d}$ forms a basis for the class functions on $G$. I would like to show $V \cong W$ by some other means, maybe an explicit isomorphism. This would prove Schur orthogonality without knowing the fact about the basis for class functions. How might this be done?","I am trying to find an alternate proof for Schur orthogonality relations along the following lines. Let $G$ be a finite group, with irreducible representations $V_1$, $V_2$, $\cdots$, $V_d$. Let $V$ denote the adjoint representation ($V = \langle \{e_g\}_{g\in G} \rangle$ with $h\cdot e_g = e_{hgh^{-1}}$). Also, let $W$ denote the representation given by $\bigoplus_{i=1}^{d} \text{End} (V_i)$. The Schur orthogonality relations tell us these both have the same character, hence are isomorphic. But this requires one to assume the fact that $\{\chi_{V_i} \}_{i=1..d}$ forms a basis for the class functions on $G$. I would like to show $V \cong W$ by some other means, maybe an explicit isomorphism. This would prove Schur orthogonality without knowing the fact about the basis for class functions. How might this be done?",,['group-theory']
12,How to prove that the inverse of a matrix is unique?,How to prove that the inverse of a matrix is unique?,,The ring of matrix is not an integral domain. How to prove that the inverse is unique?,The ring of matrix is not an integral domain. How to prove that the inverse is unique?,,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory']"
13,Every positive definite matrix can be written as $B^TB$ for some invertible $B$,Every positive definite matrix can be written as  for some invertible,B^TB B,Let $A$ be a positive definite symmetric matrix. Show that there exists an invertible matrix $B$ such that $A=B^TB$ . [Hint: Use the Spectral Theorem to write $A = QDQ^T$ . Then show that D can be factored as $C^TC$ for some invertible matrix $C$ .] I can't seem to get to the correct answer. I'm not entirely sure what is meant by the last line of the hint too. Could anyone please help me out?,Let be a positive definite symmetric matrix. Show that there exists an invertible matrix such that . [Hint: Use the Spectral Theorem to write . Then show that D can be factored as for some invertible matrix .] I can't seem to get to the correct answer. I'm not entirely sure what is meant by the last line of the hint too. Could anyone please help me out?,A B A=B^TB A = QDQ^T C^TC C,"['linear-algebra', 'matrices']"
14,Isometry without eigenvalues,Isometry without eigenvalues,,"So I'm tasked with finding an $R^2$ isometry that has no eigenvalues in $\mathbb{R}$ . Trouble is, well, I don't really ""get"" isometries as they pertain to eigenvalues and transformation matrices. I could trivially come up with...say the following matrix: $$\begin{bmatrix}1 & -1\\ \frac{1}{4} & 1\end{bmatrix}$$ Now the eigenvalue would be: \begin{align} (1 - \lambda)^2 - (-\frac{1}{2}) &= 0\\ \frac{5}{4} - 2\lambda + \lambda^2 &= 0\\ \lambda &= \frac{2\pm\sqrt{(-2)^2-4\cdot\frac{5}{4}}}{2}\\ \lambda &= \frac{2\pm\sqrt{4-5}}{2} \end{align} That would have a complex eigenvalue (of $1 \pm \frac{i}{2}$ ), but that's very certainly not of absolute value 1. Since $a$ in the equation is always 1, I guess I'd need to come up with numbers that result in $\sqrt{2} + \sqrt{2}i$ so that the absolute value would indeed be 1. However, that doesn't really have much to do with the definition of an isometry, which is defined through inner product space: I can't really find the link between the eigenvalue and the inner product space. I guess my question is, how do I go about formulating an isometry that also fills all these conditions?","So I'm tasked with finding an isometry that has no eigenvalues in . Trouble is, well, I don't really ""get"" isometries as they pertain to eigenvalues and transformation matrices. I could trivially come up with...say the following matrix: Now the eigenvalue would be: That would have a complex eigenvalue (of ), but that's very certainly not of absolute value 1. Since in the equation is always 1, I guess I'd need to come up with numbers that result in so that the absolute value would indeed be 1. However, that doesn't really have much to do with the definition of an isometry, which is defined through inner product space: I can't really find the link between the eigenvalue and the inner product space. I guess my question is, how do I go about formulating an isometry that also fills all these conditions?","R^2 \mathbb{R} \begin{bmatrix}1 & -1\\
\frac{1}{4} & 1\end{bmatrix} \begin{align}
(1 - \lambda)^2 - (-\frac{1}{2}) &= 0\\
\frac{5}{4} - 2\lambda + \lambda^2 &= 0\\
\lambda &= \frac{2\pm\sqrt{(-2)^2-4\cdot\frac{5}{4}}}{2}\\
\lambda &= \frac{2\pm\sqrt{4-5}}{2}
\end{align} 1 \pm \frac{i}{2} a \sqrt{2} + \sqrt{2}i","['linear-algebra', 'eigenvalues-eigenvectors', 'isometry']"
15,Is it possible to construct a matrix : $A^2 = J_{n}$.,Is it possible to construct a matrix : .,A^2 = J_{n},"Consider $\Omega = Mat_{n\times n}(\{0,1\})$ - space of matrices of $1$ s and $0$ s.  We want to determine does there exist $A\in\Omega$ : $A^2 = J_n$ , where $J_n$ is a matrix of ones. We suppose that there are standard arithmetical operations : ( $\mathbb{R}$ ,+, $\cdot)$ . Actually I don't understand how to step it. I've thought about using some properties about spectrum of $J_n$ , but it looks like failure moment.","Consider - space of matrices of s and s.  We want to determine does there exist : , where is a matrix of ones. We suppose that there are standard arithmetical operations : ( ,+, . Actually I don't understand how to step it. I've thought about using some properties about spectrum of , but it looks like failure moment.","\Omega = Mat_{n\times n}(\{0,1\}) 1 0 A\in\Omega A^2 = J_n J_n \mathbb{R} \cdot) J_n","['linear-algebra', 'matrices']"
16,Is a square zero matrix positive semidefinite?,Is a square zero matrix positive semidefinite?,,Does the fact that a square zero matrix contains non-negative eigenvalues (zeros) make it proper to say it is positive semidefinite?,Does the fact that a square zero matrix contains non-negative eigenvalues (zeros) make it proper to say it is positive semidefinite?,,"['linear-algebra', 'matrices', 'positive-semidefinite']"
17,If $A$ is a symmetric matrix. Then all the eigenvalues of $A^2$ are nonnegative,If  is a symmetric matrix. Then all the eigenvalues of  are nonnegative,A A^2,"For the answer, we can get support from two claims: Claim 1: If $A$ is real and symmetric, then it has real eigenvalues. Proof here Claim 2: If $\lambda$ is an eigenvalue of $A$ , then $\lambda^2$ is an Eigenvalue of $A^2$ . Proof here So by those two claims we can say that if $\lambda$ is an eigenvalue of $A$ , then the corresponding eigenvalue $\lambda^2$ of $A^2$ is nonnegative. But my problem is how can we guarantee that it will cover all the possible eigenvalues of $A^2$ ?","For the answer, we can get support from two claims: Claim 1: If is real and symmetric, then it has real eigenvalues. Proof here Claim 2: If is an eigenvalue of , then is an Eigenvalue of . Proof here So by those two claims we can say that if is an eigenvalue of , then the corresponding eigenvalue of is nonnegative. But my problem is how can we guarantee that it will cover all the possible eigenvalues of ?",A \lambda A \lambda^2 A^2 \lambda A \lambda^2 A^2 A^2,"['linear-algebra', 'eigenvalues-eigenvectors']"
18,Inequality proof by Cauchy-Schwarz inequality,Inequality proof by Cauchy-Schwarz inequality,,"I've been wrestling with the following inequality for 3 days to prove it by CSI, but I can't choose the right column vectors $u$ and $v$ of CSI. $$ \frac{a^2_1}{b_1}+\frac{a^2_2}{b_2}+\cdots+\frac{a^2_n}{b_n}\ge\frac{\left(a_1+\cdots+a_n\right)^2}{b_1+\cdots+b_n} $$ How on earth should I fix these vectors?","I've been wrestling with the following inequality for 3 days to prove it by CSI, but I can't choose the right column vectors $u$ and $v$ of CSI. $$ \frac{a^2_1}{b_1}+\frac{a^2_2}{b_2}+\cdots+\frac{a^2_n}{b_n}\ge\frac{\left(a_1+\cdots+a_n\right)^2}{b_1+\cdots+b_n} $$ How on earth should I fix these vectors?",,['linear-algebra']
19,Showing that the minimal polynomial of an $n \times n$ matrix has degree at most $n$ without using the Cayley-Hamilton Theorem,Showing that the minimal polynomial of an  matrix has degree at most  without using the Cayley-Hamilton Theorem,n \times n n,"Let $A$ be an $n \times n$ matrix over a field $k$. Or, more generally, an endomorphism of an $n$-dimensional $k$-vector space. Then the minimal polynomial $p_A(t)$, of $A$, has degree at most $n$. The usual way to show this is as a corollary of Cayley-Hamilton: If $\Phi_A(t) = \det(tI - A)$ then $\Phi_A(A) = 0$ and $\deg \Phi_A = n$. Since $p_A \mid \Phi_A$, it follows that $\deg p_A \le n$. One can also prove this using the structure theorem: The $k[t]$-module $V = k^n$, with action given by $t \cdot v = Av$, decomposes as a direct sum $$V \cong \bigoplus_{i = 1}^r k[t]/(q_i), $$ where $0 \ne q_1 \mid q_2 \mid \dots \mid q_r = p_A$. Taking dimensions we have $$ n = \sum_{i = 1}^r \deg q_i. $$ Therefore $\deg p_A = \deg q_r \le n$. It is clear that we do not need the full strength of the structure theorem; all we need to do is show that there is a module-embedding $$ k[t]/(p_A) \hookrightarrow V. $$ This is shown in the course of proving the structure theorem. Question: Can we show that $\deg p_A \le \dim V$ without using Cayley-Hamilton or the the structure theorem? In particular, are there any easier proofs of this fact than as a corollary of Cayley-Hamilton?","Let $A$ be an $n \times n$ matrix over a field $k$. Or, more generally, an endomorphism of an $n$-dimensional $k$-vector space. Then the minimal polynomial $p_A(t)$, of $A$, has degree at most $n$. The usual way to show this is as a corollary of Cayley-Hamilton: If $\Phi_A(t) = \det(tI - A)$ then $\Phi_A(A) = 0$ and $\deg \Phi_A = n$. Since $p_A \mid \Phi_A$, it follows that $\deg p_A \le n$. One can also prove this using the structure theorem: The $k[t]$-module $V = k^n$, with action given by $t \cdot v = Av$, decomposes as a direct sum $$V \cong \bigoplus_{i = 1}^r k[t]/(q_i), $$ where $0 \ne q_1 \mid q_2 \mid \dots \mid q_r = p_A$. Taking dimensions we have $$ n = \sum_{i = 1}^r \deg q_i. $$ Therefore $\deg p_A = \deg q_r \le n$. It is clear that we do not need the full strength of the structure theorem; all we need to do is show that there is a module-embedding $$ k[t]/(p_A) \hookrightarrow V. $$ This is shown in the course of proving the structure theorem. Question: Can we show that $\deg p_A \le \dim V$ without using Cayley-Hamilton or the the structure theorem? In particular, are there any easier proofs of this fact than as a corollary of Cayley-Hamilton?",,"['linear-algebra', 'modules', 'alternative-proof', 'minimal-polynomials']"
20,Checking if two matrices are similar,Checking if two matrices are similar,,"I have two matrices $$ \begin{pmatrix}   2 & 1 & 0 \\   0 & 2 & 0 \\   0 & 0 & 3  \end{pmatrix} $$ and $$ \begin{pmatrix}   2 & 2 & 0 \\   0 & 2 & 0 \\   0 & 0 & 3  \end{pmatrix} $$ They are not diagonalizable. Share the same characteristic polynomial, the same trace, same determinant, eigenvalues, rank. What could I use more to say if they are similar or not?","I have two matrices $$ \begin{pmatrix}   2 & 1 & 0 \\   0 & 2 & 0 \\   0 & 0 & 3  \end{pmatrix} $$ and $$ \begin{pmatrix}   2 & 2 & 0 \\   0 & 2 & 0 \\   0 & 0 & 3  \end{pmatrix} $$ They are not diagonalizable. Share the same characteristic polynomial, the same trace, same determinant, eigenvalues, rank. What could I use more to say if they are similar or not?",,"['linear-algebra', 'matrices']"
21,rank of a orthogonal matrix,rank of a orthogonal matrix,,"My question is, Is every orthogonal matrix is full rank ? Is there any specific thorem to answer this question ? I didnt find any","My question is, Is every orthogonal matrix is full rank ? Is there any specific thorem to answer this question ? I didnt find any",,"['linear-algebra', 'matrices', 'matrix-rank', 'orthogonal-matrices']"
22,Help with understanding the proof for: $AB$ and $BA$ have the same characteristic polynomial (for square complex matrices),Help with understanding the proof for:  and  have the same characteristic polynomial (for square complex matrices),AB BA,"I saw many proofs but they all use advanced techniques and are impossible to understand. I'm looking for a proof that $AB$ and $BA$ have the same characteristic polynomial for any square matrices $A$ and $B$ over $\mathbb C$ . It's really easy when dealing with invertible matrices, but hard to prove for singular matrices. I found several solutions that I could not understand: This solution says it is not too difficult to show that $AB$ , and $BA$ have the same characteristic polynomial ... If the matrices are in $M_n(\mathbb C)$ , you use the fact that $GL_n(\mathbb C)$ is dense in $M_n(\mathbb C)$ and the continuity of the function which maps a matrix to its characteristic polynomial . There are at least 5 other ways to proceed I've bolded every term that I am not familiar with. This solution I could not understand as well (it uses the limit definition when $\lambda$ approaches zero but I hardly understand how that solves the issue). I'm looking for a simpler solution using more basic linear algebra.","I saw many proofs but they all use advanced techniques and are impossible to understand. I'm looking for a proof that and have the same characteristic polynomial for any square matrices and over . It's really easy when dealing with invertible matrices, but hard to prove for singular matrices. I found several solutions that I could not understand: This solution says it is not too difficult to show that , and have the same characteristic polynomial ... If the matrices are in , you use the fact that is dense in and the continuity of the function which maps a matrix to its characteristic polynomial . There are at least 5 other ways to proceed I've bolded every term that I am not familiar with. This solution I could not understand as well (it uses the limit definition when approaches zero but I hardly understand how that solves the issue). I'm looking for a simpler solution using more basic linear algebra.",AB BA A B \mathbb C AB BA M_n(\mathbb C) GL_n(\mathbb C) M_n(\mathbb C) \lambda,"['linear-algebra', 'matrices', 'continuity', 'determinant', 'characteristic-functions']"
23,Calculate complex determinant,Calculate complex determinant,,$$\left| {\begin{array}{*{20}{c}}{{a^2}}&{{{(a + 1)}^2}}&{{{(a + 2)}^2}}&{{{(a + 3)}^2}}\\{{b^2}}&{{{(b + 1)}^2}}&{{{(b + 2)}^2}}&{{{(b + 3)}^2}}\\{{c^2}}&{{{(c + 1)}^2}}&{{{(c + 2)}^2}}&{{{(c + 3)}^2}}\\{{d^2}}&{{{(d + 1)}^2}}&{{{(d + 2)}^2}}&{{{(d + 3)}^2}}\end{array}} \right| $$ It's very stupid desicion for me to expand it by row or column. Any suggestions?,$$\left| {\begin{array}{*{20}{c}}{{a^2}}&{{{(a + 1)}^2}}&{{{(a + 2)}^2}}&{{{(a + 3)}^2}}\\{{b^2}}&{{{(b + 1)}^2}}&{{{(b + 2)}^2}}&{{{(b + 3)}^2}}\\{{c^2}}&{{{(c + 1)}^2}}&{{{(c + 2)}^2}}&{{{(c + 3)}^2}}\\{{d^2}}&{{{(d + 1)}^2}}&{{{(d + 2)}^2}}&{{{(d + 3)}^2}}\end{array}} \right| $$ It's very stupid desicion for me to expand it by row or column. Any suggestions?,,['linear-algebra']
24,What are the possible eigenvalues of a linear transformation $T$ satifying $T = T^2$ [duplicate],What are the possible eigenvalues of a linear transformation  satifying  [duplicate],T T = T^2,"This question already has answers here : Find the eigenvalues of a projection operator (4 answers) Closed 9 years ago . Let $T$ be a linear transformation $T$ such that $T\colon V \to V$. Also, let $T = T^2$. What are the possible eigenvalues of $T$? I am not sure if the answer is only $1$, or $0$ and $1$. It holds that $T = T^2$, thus $T(T(x)) = T(x)$. Let's call $T(x) = v$, so $T(v) = v$. which means that $\lambda=1$. But I am not sure about this, while I have seen a solution that says that $0$ is possible as well. Thanks in advance !","This question already has answers here : Find the eigenvalues of a projection operator (4 answers) Closed 9 years ago . Let $T$ be a linear transformation $T$ such that $T\colon V \to V$. Also, let $T = T^2$. What are the possible eigenvalues of $T$? I am not sure if the answer is only $1$, or $0$ and $1$. It holds that $T = T^2$, thus $T(T(x)) = T(x)$. Let's call $T(x) = v$, so $T(v) = v$. which means that $\lambda=1$. But I am not sure about this, while I have seen a solution that says that $0$ is possible as well. Thanks in advance !",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
25,Square root of determinant equals determinant of square root?,Square root of determinant equals determinant of square root?,,"Is it true that for a real-valued positive definite matrix $X$, $\sqrt{\det(X)} = \det(X^{1/2})$? I know that this is indeed true for the $2 \times 2$ case but I haven't been able to find the answer or prove it myself for larger $X$. Thanks a lot for any help.","Is it true that for a real-valued positive definite matrix $X$, $\sqrt{\det(X)} = \det(X^{1/2})$? I know that this is indeed true for the $2 \times 2$ case but I haven't been able to find the answer or prove it myself for larger $X$. Thanks a lot for any help.",,"['linear-algebra', 'matrices', 'determinant']"
26,Is the determinant of this matrix positive or negative?,Is the determinant of this matrix positive or negative?,,"$\left( \begin{array}{ccc} 1 & 1000 & 2 & 3 &4\\ 5 & 6 &7&1000 &8\\ 1000&9&8&7&6\\ 5 & 4&3&2&1000\\ 1&2&1000&3&4\\ \end{array} \right)$ When I compute the determinant online, I find that it is positive, but I'm supposed to ""see"" something about the matrix that allows me to know the determinant is positive. What properties does this specific matrix have that allow you to deduce the determinant will be positive?","$\left( \begin{array}{ccc} 1 & 1000 & 2 & 3 &4\\ 5 & 6 &7&1000 &8\\ 1000&9&8&7&6\\ 5 & 4&3&2&1000\\ 1&2&1000&3&4\\ \end{array} \right)$ When I compute the determinant online, I find that it is positive, but I'm supposed to ""see"" something about the matrix that allows me to know the determinant is positive. What properties does this specific matrix have that allow you to deduce the determinant will be positive?",,"['linear-algebra', 'determinant']"
27,How to find exponential of triangular matrix,How to find exponential of triangular matrix,,"I'm studying for an exam and I can't find this in my notes or in the book, but it's on a past exam... Given $A = \begin{bmatrix}-1 & 1\\0 & -1\end{bmatrix}$, $e^{tA} = \begin{bmatrix}e^{-t} & te^{-t}\\0 & e^{-t}\end{bmatrix}$ True/False? The answer is true according to the answer key, but I don't see why. What theorem/concept would tell me that this is true? It's a true false question so I'm sure it should be something I can see at a glance or with very little work ...","I'm studying for an exam and I can't find this in my notes or in the book, but it's on a past exam... Given $A = \begin{bmatrix}-1 & 1\\0 & -1\end{bmatrix}$, $e^{tA} = \begin{bmatrix}e^{-t} & te^{-t}\\0 & e^{-t}\end{bmatrix}$ True/False? The answer is true according to the answer key, but I don't see why. What theorem/concept would tell me that this is true? It's a true false question so I'm sure it should be something I can see at a glance or with very little work ...",,"['linear-algebra', 'matrices']"
28,"Prove that $\sin(x + \alpha)$, $\sin(x + \beta)$ and $\sin(x + \gamma)$ are linearly dependent.","Prove that ,  and  are linearly dependent.",\sin(x + \alpha) \sin(x + \beta) \sin(x + \gamma),"Functions $f$ and $g$ are independent on an interval $D$ if $af(x) + bg(x) = 0$ implies that $a = 0$ and $b = 0$ $\forall x \in D$ let $\alpha$, $\beta$, $\gamma$ be real constants. Prove that $\sin(x + \alpha)$, $\sin(x + \beta)$ and $\sin(x + \gamma)$ are linearly dependent vectors in $C^0[0, 1]$. Be convincing in your reasoning (argument) I was researching and found Wronskian. Using the Wronskian for three functions. The determinant of $f$, $g$ and $h$ is $W(f, g, h) = $ $$         \begin{vmatrix}         f & g & h \\         f' & g' & h' \\         f'' & g'' & h'' \\         \end{vmatrix} $$ If $W(f, g, h) \neq 0$ then $f(x)$, $g(x)$ and  $h(x)$ are linearly independent. If $f(x)$, $g(x)$, and $h(x)$ are linearly dependent then $W(f, g, h) = 0$ My attempt Let $f(x) = \sin(x + \alpha)$, $g(x) = \sin(x + \beta)$ and $h(x) = \sin(x + \gamma)$ $W(f, g, h) =$ $$         \begin{vmatrix}         \sin(x + \alpha) & \sin(x + \beta) & \sin(x + \gamma) \\         \cos(x + \alpha) & \cos(x + \beta) & \cos(x + \gamma) \\         -\sin(x + \alpha) & -\sin(x + \beta) & -\sin(x + \gamma) \\         \end{vmatrix} $$ $= \sin(x + \alpha)[-\sin(x + \gamma)\cos(x + \beta) + \cos(x + \gamma)\sin(x + \beta)]  - sin(x + \beta)[-\sin(x + \gamma)\cos(x + \alpha) + \cos(x + \gamma)\sin(x + \alpha)] + \sin(x + \gamma)[-\sin(x+ \beta)\cos(x + \alpha) + \cos(x + \beta)\sin(x + \alpha)]$ $= -\sin(x + \alpha)[\sin((x + \gamma) +(x + \beta))] + \sin(x + \beta)[\sin((x + \gamma) + (x + \alpha))] - \sin(x + \gamma)[\sin((x + \beta) + (x + \alpha))] = 0$ By Wronskian, $f(x)$, $g(x)$ and $h(x)$ are linearly dependent since $W(f, g, h) = 0$ Not sure if this argument is sound?","Functions $f$ and $g$ are independent on an interval $D$ if $af(x) + bg(x) = 0$ implies that $a = 0$ and $b = 0$ $\forall x \in D$ let $\alpha$, $\beta$, $\gamma$ be real constants. Prove that $\sin(x + \alpha)$, $\sin(x + \beta)$ and $\sin(x + \gamma)$ are linearly dependent vectors in $C^0[0, 1]$. Be convincing in your reasoning (argument) I was researching and found Wronskian. Using the Wronskian for three functions. The determinant of $f$, $g$ and $h$ is $W(f, g, h) = $ $$         \begin{vmatrix}         f & g & h \\         f' & g' & h' \\         f'' & g'' & h'' \\         \end{vmatrix} $$ If $W(f, g, h) \neq 0$ then $f(x)$, $g(x)$ and  $h(x)$ are linearly independent. If $f(x)$, $g(x)$, and $h(x)$ are linearly dependent then $W(f, g, h) = 0$ My attempt Let $f(x) = \sin(x + \alpha)$, $g(x) = \sin(x + \beta)$ and $h(x) = \sin(x + \gamma)$ $W(f, g, h) =$ $$         \begin{vmatrix}         \sin(x + \alpha) & \sin(x + \beta) & \sin(x + \gamma) \\         \cos(x + \alpha) & \cos(x + \beta) & \cos(x + \gamma) \\         -\sin(x + \alpha) & -\sin(x + \beta) & -\sin(x + \gamma) \\         \end{vmatrix} $$ $= \sin(x + \alpha)[-\sin(x + \gamma)\cos(x + \beta) + \cos(x + \gamma)\sin(x + \beta)]  - sin(x + \beta)[-\sin(x + \gamma)\cos(x + \alpha) + \cos(x + \gamma)\sin(x + \alpha)] + \sin(x + \gamma)[-\sin(x+ \beta)\cos(x + \alpha) + \cos(x + \beta)\sin(x + \alpha)]$ $= -\sin(x + \alpha)[\sin((x + \gamma) +(x + \beta))] + \sin(x + \beta)[\sin((x + \gamma) + (x + \alpha))] - \sin(x + \gamma)[\sin((x + \beta) + (x + \alpha))] = 0$ By Wronskian, $f(x)$, $g(x)$ and $h(x)$ are linearly dependent since $W(f, g, h) = 0$ Not sure if this argument is sound?",,['linear-algebra']
29,$AB$ is not invertible,is not invertible,AB,"Is it true that if $A$ is an $m \times n$ matrix and $B$ is an $n \times m$ matrix, with $m > n $ then $\det(AB)=0$?","Is it true that if $A$ is an $m \times n$ matrix and $B$ is an $n \times m$ matrix, with $m > n $ then $\det(AB)=0$?",,['linear-algebra']
30,"$\left \| \cdot \right \|$ is an induced norm. If $\left \| A \right \|<1$, how to show that $I-A$ is nonsingular and ...?","is an induced norm. If , how to show that  is nonsingular and ...?",\left \| \cdot \right \| \left \| A \right \|<1 I-A,"The induced norm $\left \| \cdot  \right \|$ is defined for a matrix $A\in\mathbb{C}^{n\times n}$ as $\left \| A \right \|=\sup_{||x||=1} \left \| Ax \right \|$. If $\left \| A \right \|<1$, show that (1) $I-A$ is nonsingular and (2) $\left \| \left ( I-A \right )^{-1} \right \|\leq \frac{1}{1-\left \| A \right \|}$.","The induced norm $\left \| \cdot  \right \|$ is defined for a matrix $A\in\mathbb{C}^{n\times n}$ as $\left \| A \right \|=\sup_{||x||=1} \left \| Ax \right \|$. If $\left \| A \right \|<1$, show that (1) $I-A$ is nonsingular and (2) $\left \| \left ( I-A \right )^{-1} \right \|\leq \frac{1}{1-\left \| A \right \|}$.",,"['linear-algebra', 'matrices']"
31,Should $x=-2$ be included as an answer for $\frac{x^2+8x+12}{x^2+5x+6}>0$?,Should  be included as an answer for ?,x=-2 \frac{x^2+8x+12}{x^2+5x+6}>0,"$$\frac{x^2+8x+12}{x^2+5x+6}>0$$ First of all while solving inequalities I need to check domain so in this case $$x^2+5x+6\neq0$$ $$x\neq-2,\ x\neq-3$$  Later on $$\frac{(x+6)(x+2)}{(x+3)(x+2)}>0$$ Then get critical values draw number line and get $$x\in(-\infty;-6)\cup(-3;-2)\cup(-2;+\infty)$$ However according to wolframalpha $x=-2$ is included as an answer. So am I wrong or wolframalpha is wrong? Also I checked $\frac{x}{x}=1$ and wolframalpha also includes $x=0$ but once again I think it's incorrect?","$$\frac{x^2+8x+12}{x^2+5x+6}>0$$ First of all while solving inequalities I need to check domain so in this case $$x^2+5x+6\neq0$$ $$x\neq-2,\ x\neq-3$$  Later on $$\frac{(x+6)(x+2)}{(x+3)(x+2)}>0$$ Then get critical values draw number line and get $$x\in(-\infty;-6)\cup(-3;-2)\cup(-2;+\infty)$$ However according to wolframalpha $x=-2$ is included as an answer. So am I wrong or wolframalpha is wrong? Also I checked $\frac{x}{x}=1$ and wolframalpha also includes $x=0$ but once again I think it's incorrect?",,"['linear-algebra', 'wolfram-alpha']"
32,Linear algebra homework problem involving basis and dual basis.,Linear algebra homework problem involving basis and dual basis.,,"Please help me get started on this problem: Let $V = R^3$ , and define $f_1, f_2, f_3 ∈ V^*$ as follows: $f_1(x,y,z) = x - 2y$ $f_2(x,y,z) = x + y + z$ $f_3(x,y,z) = y-3z$ Prove that $\{f_1,f_2,f_3\}$ is a basis for $V^*$ , and then find a basis for $V$ for which it is the dual basis.","Please help me get started on this problem: Let , and define as follows: Prove that is a basis for , and then find a basis for for which it is the dual basis.","V = R^3 f_1, f_2, f_3 ∈ V^* f_1(x,y,z) = x - 2y f_2(x,y,z) = x + y + z f_3(x,y,z) = y-3z \{f_1,f_2,f_3\} V^* V",['linear-algebra']
33,Example of functions where linear dependence isn't obvious,Example of functions where linear dependence isn't obvious,,"The Wronskian lets us determine if a set of functions (possibly the solutions to a differential equation) are linearly dependent or not. But, for every example in the book, it is very obvious if one of the functions is a linear combination of the others. The examples in the book use 3-5 functions. What would be an example of a small number of functions where this isn't obvious? Or is the application of the Wronskian mostly to deal with large sets of functions... where the sheer number makes it hard to tell if they are dependent or not?","The Wronskian lets us determine if a set of functions (possibly the solutions to a differential equation) are linearly dependent or not. But, for every example in the book, it is very obvious if one of the functions is a linear combination of the others. The examples in the book use 3-5 functions. What would be an example of a small number of functions where this isn't obvious? Or is the application of the Wronskian mostly to deal with large sets of functions... where the sheer number makes it hard to tell if they are dependent or not?",,['linear-algebra']
34,Intuition behind the component formula of the scalar product,Intuition behind the component formula of the scalar product,,"I have learned the component formula for the scalar product for quite a while, but really, it doesn't seem intuitive at all. I have 2 questions that confuse me while imagining what the scalar product really is, which are: 1- Why is the scalar product defined as $\vec{u} . \vec{v} = \sum u_{i}v_{i}$ (It doesn't make sense at all). 2- Does the concept of angles exist in dimensions higher than 3? In other words, can we always say that $\vec{u} . \vec{v} = ||\vec{u}|| \times ||\vec{v}|| \times \cos \theta$ ?","I have learned the component formula for the scalar product for quite a while, but really, it doesn't seem intuitive at all. I have 2 questions that confuse me while imagining what the scalar product really is, which are: 1- Why is the scalar product defined as (It doesn't make sense at all). 2- Does the concept of angles exist in dimensions higher than 3? In other words, can we always say that ?",\vec{u} . \vec{v} = \sum u_{i}v_{i} \vec{u} . \vec{v} = ||\vec{u}|| \times ||\vec{v}|| \times \cos \theta,"['linear-algebra', 'inner-products']"
35,"There exists some real $2 \times 2$ matrix $A$, such that $A^2+A+I=0$?","There exists some real  matrix , such that ?",2 \times 2 A A^2+A+I=0,"There exists some real $2 \times 2$ matrix $A$ , such that $A^2+A+I=0$ ? $$ A\ =\ \left[\begin{array}{ c c } a & b\\ c & d \end{array}\right] $$ $$ A^{2}  = \left[\begin{array}{ c c } a & b\\ c & d \end{array}\right] \left[\begin{array}{ c c } a & b\\ c & d \end{array}\right] =\ \left[\begin{array}{ c c } a^{2} +bc\  & ab+bd\\ ac+cd & bc+d^{2} \end{array}\right] $$ $$ A^{2} \ +\ A\ +I\ =\ 0\ =\ \left[\begin{array}{ c c } a^{2} +bc\  & ab+bd\\ ac+cd & bc+d^{2} \end{array}\right] +\left[\begin{array}{ c c } a & b\\ c & d \end{array}\right] +\left[\begin{array}{ c c } 1 & 0\\ 0 & 1 \end{array}\right] \ \ =\ 0 $$ $$ \left[\begin{array}{ c c } a^{2} +bc\ +a+1 & ab+bd+b\\ ac+cd+c & bc+d^{2} +d+1 \end{array}\right] = 0 $$ $$ ab + bd + b = 0 $$ $$ b(a+d+1) = 0 $$ $$ ac + cd + c = c(a+d+1) = 0 $$ $$ (a+d+1) = 0 \text{ or } c = 0 \text{ or } b = 0 $$ $$ -d = a+1 $$","There exists some real matrix , such that ?","2 \times 2 A A^2+A+I=0 
A\ =\ \left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right]   A^{2}  = \left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] \left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] =\ \left[\begin{array}{ c c }
a^{2} +bc\  & ab+bd\\
ac+cd & bc+d^{2}
\end{array}\right]   A^{2} \ +\ A\ +I\ =\ 0\ =\ \left[\begin{array}{ c c }
a^{2} +bc\  & ab+bd\\
ac+cd & bc+d^{2}
\end{array}\right] +\left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] +\left[\begin{array}{ c c }
1 & 0\\
0 & 1
\end{array}\right] \ \ =\ 0   \left[\begin{array}{ c c }
a^{2} +bc\ +a+1 & ab+bd+b\\
ac+cd+c & bc+d^{2} +d+1
\end{array}\right] = 0   ab + bd + b = 0   b(a+d+1) = 0   ac + cd + c = c(a+d+1) = 0   (a+d+1) = 0 \text{ or } c = 0 \text{ or } b = 0   -d = a+1 ","['linear-algebra', 'matrix-equations']"
36,"Conjugacy in $\text{GL}(2, \mathbb{Z})$ vs $\text{GL}(2, \mathbb{Q})$",Conjugacy in  vs,"\text{GL}(2, \mathbb{Z}) \text{GL}(2, \mathbb{Q})","When are two elements $x,y\in\text{GL}(2, \mathbb{Z})$ conjugate in $\text{GL}(2, \mathbb{Q})$ , but not in $\text{GL}(2, \mathbb{Z})$ ? Does this ever happen? I feel that it should sometimes be the case, but cannot come up with any concrete examples. EDIT: so it is possible; but is it possible if we require the conjugating element in $\text{GL}(2, \mathbb{Q})$ to have determinant $\pm 1$ ?","When are two elements conjugate in , but not in ? Does this ever happen? I feel that it should sometimes be the case, but cannot come up with any concrete examples. EDIT: so it is possible; but is it possible if we require the conjugating element in to have determinant ?","x,y\in\text{GL}(2, \mathbb{Z}) \text{GL}(2, \mathbb{Q}) \text{GL}(2, \mathbb{Z}) \text{GL}(2, \mathbb{Q}) \pm 1","['linear-algebra', 'group-theory', 'general-linear-group']"
37,Show that the eigenvalues of $AA^T$ and $A^TA$ are non-negative.,Show that the eigenvalues of  and  are non-negative.,AA^T A^TA,"Let $A\in \mathbb{R^{m\times n}}$ . Show that the eigenvalues of $AA^T$ and $A^TA$ are non-negative. I could just apply the definition of an eigenvalue for $AA^T$ (or $A^TA$ ), but I don´t know how to determine the sign of the eigenvalue. Here is what I tried: suppose that $\lambda<0$ is an eigenvalue and proceed via contradiction. However, I feel that this might not be correct.","Let . Show that the eigenvalues of and are non-negative. I could just apply the definition of an eigenvalue for (or ), but I don´t know how to determine the sign of the eigenvalue. Here is what I tried: suppose that is an eigenvalue and proceed via contradiction. However, I feel that this might not be correct.",A\in \mathbb{R^{m\times n}} AA^T A^TA AA^T A^TA \lambda<0,"['linear-algebra', 'eigenvalues-eigenvectors']"
38,Can a transcendent matrix have an algebraic spectrum?,Can a transcendent matrix have an algebraic spectrum?,,"Let $K$ be an algebraically closed field (e.g $\mathbb{A}$ ) and $K'/K$ a transcendent field extension (e.g. $\mathbb{C}/\mathbb{A}$ ). Let $A\in K'^{n\times n}$ be a matrix over K', which has at least one entry from $K'\setminus K$ . Is it still possible that all eigenvalues of $A$ lie in $K$ ?","Let be an algebraically closed field (e.g ) and a transcendent field extension (e.g. ). Let be a matrix over K', which has at least one entry from . Is it still possible that all eigenvalues of lie in ?",K \mathbb{A} K'/K \mathbb{C}/\mathbb{A} A\in K'^{n\times n} K'\setminus K A K,['linear-algebra']
39,How to compute derivative with Hadamard product?,How to compute derivative with Hadamard product?,,"Let $\mathbf{x}$ , $\mathbf{y}$ and $\mathbf{z}$ are $n$ -dimensional column vector, and $$f =  \mathbf{x}\circ \mathbf{y} \circ\mathbf{z}$$ Here $\circ$ is the element-wise Hadamard product. Then how to compute the gradient $\frac{\partial f }{\partial\mathbf{x}}$ , $\frac{\partial f }{\partial\mathbf{y}}$ , and $\frac{\partial f }{\partial\mathbf{z}}$ ? My solution is $$\frac{\partial f }{\partial\mathbf{x}} = \mathbf{y} \circ\mathbf{z}, \frac{\partial f }{\partial\mathbf{y}} = \mathbf{x} \circ\mathbf{z},\frac{\partial f }{\partial\mathbf{z}} = \mathbf{x} \circ\mathbf{y}$$ Is it correct? thanks.","Let , and are -dimensional column vector, and Here is the element-wise Hadamard product. Then how to compute the gradient , , and ? My solution is Is it correct? thanks.","\mathbf{x} \mathbf{y} \mathbf{z} n f =  \mathbf{x}\circ \mathbf{y} \circ\mathbf{z} \circ \frac{\partial f }{\partial\mathbf{x}} \frac{\partial f }{\partial\mathbf{y}} \frac{\partial f }{\partial\mathbf{z}} \frac{\partial f }{\partial\mathbf{x}} = \mathbf{y} \circ\mathbf{z}, \frac{\partial f }{\partial\mathbf{y}} = \mathbf{x} \circ\mathbf{z},\frac{\partial f }{\partial\mathbf{z}} = \mathbf{x} \circ\mathbf{y}","['linear-algebra', 'derivatives', 'optimization', 'hadamard-product']"
40,Irreducible vs. indecomposable representation,Irreducible vs. indecomposable representation,,"I'm currently reading Serre's Linear Representations of Finite Groups , and I'm kind of confused regarding the concepts of irreducibility and indecomposability. If I'm understanding it correctly, an irreducible representation is a representation $(\rho, V)$ of a group $G$ which does not has a non-trivial subrepresentation (i.e. a representation $(\rho|_W, W)$ where $W\subseteq V$ is a $G$ -stable subspace). While an indecomposable representation is a representation that is not isomorphic to any direct sum of other representations. Wikipedia and other sources say that irreducibility implies indecomposability, which seems logical (not 100 % sure why though), while Serre says the following: Let $\rho:G \rightarrow GL(V) $ be a linear representation of $G$ . We say that it is irreducible or simple if $V$ is not $0$ and if no vector subspace of $V$ is stable under $G$ , except of course $0$ and $V$ . By theorem I [which says that there exists a $G$ -stable complement $W^0$ of a $G$ -stable subspace $W\subseteq V$ ], this second condition is equivalent to saying $V$ is not the direct sum of two representations. Does this not mean that irreducibility is equivalent indecomposability, and thereby going against what Wikipedia says? Thanks! P.S. I'm getting kind of annoyed at this book for being ""to concise"" and skipping a lot of details. Does anyone have any other recommendations on introductory books on representation theory?","I'm currently reading Serre's Linear Representations of Finite Groups , and I'm kind of confused regarding the concepts of irreducibility and indecomposability. If I'm understanding it correctly, an irreducible representation is a representation of a group which does not has a non-trivial subrepresentation (i.e. a representation where is a -stable subspace). While an indecomposable representation is a representation that is not isomorphic to any direct sum of other representations. Wikipedia and other sources say that irreducibility implies indecomposability, which seems logical (not 100 % sure why though), while Serre says the following: Let be a linear representation of . We say that it is irreducible or simple if is not and if no vector subspace of is stable under , except of course and . By theorem I [which says that there exists a -stable complement of a -stable subspace ], this second condition is equivalent to saying is not the direct sum of two representations. Does this not mean that irreducibility is equivalent indecomposability, and thereby going against what Wikipedia says? Thanks! P.S. I'm getting kind of annoyed at this book for being ""to concise"" and skipping a lot of details. Does anyone have any other recommendations on introductory books on representation theory?","(\rho, V) G (\rho|_W, W) W\subseteq V G \rho:G \rightarrow GL(V)  G V 0 V G 0 V G W^0 G W\subseteq V V","['linear-algebra', 'abstract-algebra', 'group-theory', 'representation-theory']"
41,Prove Cauchy-Schwarz with AM-GM for three variables,Prove Cauchy-Schwarz with AM-GM for three variables,,"I want to extend CS from two to three variables. Here's a Cauchy-Schwarz proof with two variables, which is proof 4 from here Let $A = \sqrt{a_1^2 + a_2^2 + \dots + a_n^2}$ and $B = \sqrt{b_1^2 + b_2^2 + \dots + b_n^2}$ . By the arithmetic-geometric means inequality (AGI), we have $$ \sum_{i=1}^n \frac{a_ib_i}{AB} \leq \sum_{i=1}^n \frac{1}{2} \left( \frac{a_i^2}{A^2} + \frac{b_i^2}{B^2} \right) = 1 $$ so that $$ \sum_{i=1}^na_ib_i \leq AB =\sqrt{\sum_{i=1}^na_i^2} \sqrt{\sum_{i=1}^n b_i^2} $$ How would I extend this method for three variables, i.e. to get the following? $$ \sum_{i=1}^na_ib_i c_i \leq \sqrt{\sum_{i=1}^na_i^2} \sqrt{\sum_{i=1}^n b_i^2}  \sqrt{\sum_{i=1}^n c_i^2} $$ Somehow I don't think it's as trivial as the first method, i.e. simply defining $C$ the same way does not seem to work. Maybe there is a better approach?","I want to extend CS from two to three variables. Here's a Cauchy-Schwarz proof with two variables, which is proof 4 from here Let and . By the arithmetic-geometric means inequality (AGI), we have so that How would I extend this method for three variables, i.e. to get the following? Somehow I don't think it's as trivial as the first method, i.e. simply defining the same way does not seem to work. Maybe there is a better approach?","A = \sqrt{a_1^2 + a_2^2 + \dots + a_n^2} B = \sqrt{b_1^2 + b_2^2 + \dots + b_n^2} 
\sum_{i=1}^n \frac{a_ib_i}{AB} \leq \sum_{i=1}^n \frac{1}{2} \left( \frac{a_i^2}{A^2} + \frac{b_i^2}{B^2} \right) = 1
 
\sum_{i=1}^na_ib_i \leq AB =\sqrt{\sum_{i=1}^na_i^2} \sqrt{\sum_{i=1}^n b_i^2}
 
\sum_{i=1}^na_ib_i c_i \leq \sqrt{\sum_{i=1}^na_i^2} \sqrt{\sum_{i=1}^n b_i^2}  \sqrt{\sum_{i=1}^n c_i^2}
 C","['linear-algebra', 'inequality', 'proof-writing', 'cauchy-schwarz-inequality']"
42,How would I solve the following question about matrices?,How would I solve the following question about matrices?,,$A= \begin{pmatrix}-1&3\\ -2&6\end{pmatrix}$ Find two $2\times 2$ matrices $B$ and $C$ such that $AB=AC$ but $B\ne C$. I have tried to do some row operations along with multiplication but I keep getting the wrong answer. Any help?,$A= \begin{pmatrix}-1&3\\ -2&6\end{pmatrix}$ Find two $2\times 2$ matrices $B$ and $C$ such that $AB=AC$ but $B\ne C$. I have tried to do some row operations along with multiplication but I keep getting the wrong answer. Any help?,,"['linear-algebra', 'matrices', 'algebra-precalculus']"
43,Finding determinant using row operations,Finding determinant using row operations,,"I'm currently learning about finding determinant using row operations. This method requires the values below the main diagonal to all be zero. I'm looking at this example and I don't understand the last matrix. They have done r4 = 1/2r3 + r4. However, what if I had done r4 = 2r4 + r3 ? I would still get my desired zeros below the diagonals but my last value on my diagonal becomes -13 instead of -13/2. This changes my determinant result. Why is this happening? Can I not do r4 = 2r4 + r3?","I'm currently learning about finding determinant using row operations. This method requires the values below the main diagonal to all be zero. I'm looking at this example and I don't understand the last matrix. They have done r4 = 1/2r3 + r4. However, what if I had done r4 = 2r4 + r3 ? I would still get my desired zeros below the diagonals but my last value on my diagonal becomes -13 instead of -13/2. This changes my determinant result. Why is this happening? Can I not do r4 = 2r4 + r3?",,"['linear-algebra', 'determinant']"
44,"Is $u,v,w$ a basis of $\text{span}(u,v,w)$, and why?","Is  a basis of , and why?","u,v,w \text{span}(u,v,w)","Let $u = (1, 2, 0, 1)$, $v = (1, −1, 1, 0)$, $w = (2, 4, 0, 2)$, and suppose $W = \text{span}(u, v, w)$ is a subspace of $\mathbb{R}^4$. Is $u,v,w$ a basis of $W$? Why or why not? I'm not sure if it is or not, I reduced it down to echelon form. $$ \left[     \begin{array}{cccc}       1&2&0&1\\       0&1&-1/3&1/3\\       0&0&0&0     \end{array} \right] $$ But I'm not sure where to go from here.","Let $u = (1, 2, 0, 1)$, $v = (1, −1, 1, 0)$, $w = (2, 4, 0, 2)$, and suppose $W = \text{span}(u, v, w)$ is a subspace of $\mathbb{R}^4$. Is $u,v,w$ a basis of $W$? Why or why not? I'm not sure if it is or not, I reduced it down to echelon form. $$ \left[     \begin{array}{cccc}       1&2&0&1\\       0&1&-1/3&1/3\\       0&0&0&0     \end{array} \right] $$ But I'm not sure where to go from here.",,"['linear-algebra', 'matrices']"
45,proving that there are infinitely many zero polynomials over a finite field,proving that there are infinitely many zero polynomials over a finite field,,"Given a finite field F with c elements, how would one prove that there are infinitely many polynomials that represent the zero function over the field F? I know we have infinitely many polynomials over this field, isn't it straightforward that by choosing all the coefficients to be zero all those infinitely many polynomials are zero functions?","Given a finite field F with c elements, how would one prove that there are infinitely many polynomials that represent the zero function over the field F? I know we have infinitely many polynomials over this field, isn't it straightforward that by choosing all the coefficients to be zero all those infinitely many polynomials are zero functions?",,"['linear-algebra', 'functions', 'polynomials', 'vector-spaces']"
46,Limit in n-dimensions with dot product,Limit in n-dimensions with dot product,,"Let there be three vectors $\mathbf a$, $\mathbf b$, and $\mathbf h$ in $\mathbb R^n$. Define the real number $$p(\mathbf h) \doteq (\mathbf a \cdot \mathbf h) (\mathbf b \cdot \mathbf h)$$ Does the following limit exist? In particular, does it equal $0$? $$\lim_{\mathbf h \to \mathbf 0} \frac {p(\mathbf h)} {|\mathbf h|} $$ I was thinking of expressing $p$ as a product of sums, like $$p(\mathbf h)=\left(\sum_{j=1}^n a_jh_j\right)\left(\sum_{k=1}^n b_kh_k\right) = \sum_{i=1}^na_ib_ih_i^2 + \sum_{j\neq k} a_jb_kh_jh_k $$ Are there any dot product properties I may be forgetting at this point?","Let there be three vectors $\mathbf a$, $\mathbf b$, and $\mathbf h$ in $\mathbb R^n$. Define the real number $$p(\mathbf h) \doteq (\mathbf a \cdot \mathbf h) (\mathbf b \cdot \mathbf h)$$ Does the following limit exist? In particular, does it equal $0$? $$\lim_{\mathbf h \to \mathbf 0} \frac {p(\mathbf h)} {|\mathbf h|} $$ I was thinking of expressing $p$ as a product of sums, like $$p(\mathbf h)=\left(\sum_{j=1}^n a_jh_j\right)\left(\sum_{k=1}^n b_kh_k\right) = \sum_{i=1}^na_ib_ih_i^2 + \sum_{j\neq k} a_jb_kh_jh_k $$ Are there any dot product properties I may be forgetting at this point?",,"['linear-algebra', 'limits', 'multivariable-calculus']"
47,Orthogonal Projection onto the $ {L}_{\infty} $ Unit Ball,Orthogonal Projection onto the  Unit Ball, {L}_{\infty} ,"What is the Orthogonal Projection onto the $ {\ell}_{\infty} $ Unit Ball? Namely, given $ x \in {\mathbb{R}}^{n} $ what would be: $$ {\mathcal{P}}_{ { \left\| \cdot \right\| }_{\infty} \leq 1 } \left( x \right) = \arg \min_{{ \left\| y \right\| }_{\infty} \leq 1} \left\{ {\left\| y - x \right\|}_{2}^{2} \right\} $$ I managed to get an answer using the Moreau Decomposition . Yet I would be happy to see if someone can derive the answer directly. Thank You.","What is the Orthogonal Projection onto the $ {\ell}_{\infty} $ Unit Ball? Namely, given $ x \in {\mathbb{R}}^{n} $ what would be: $$ {\mathcal{P}}_{ { \left\| \cdot \right\| }_{\infty} \leq 1 } \left( x \right) = \arg \min_{{ \left\| y \right\| }_{\infty} \leq 1} \left\{ {\left\| y - x \right\|}_{2}^{2} \right\} $$ I managed to get an answer using the Moreau Decomposition . Yet I would be happy to see if someone can derive the answer directly. Thank You.",,"['linear-algebra', 'optimization', 'convex-optimization']"
48,"Relation between the dual space, transpose matrices and rank-nullity theorem","Relation between the dual space, transpose matrices and rank-nullity theorem",,"Summing up, how can one use linear functionals, transpose matrices, row and column rank equality and annihilators to prove the rank-nullity theorem? While studying linear algebra, I'm trying to get the precise relation between the following concepts: given two vector spaces $V$ and $W$ , I can form the dual spaces $V^*$ and $W^*$ by taking all the linear functionals on $V$ and $W$ . The basis of each one of those spaces lifts to the duals (though this demonstration I'm still going to carry on, but I have the idea how to do so). If I have a linear transformation $T: V \to W$ , I have a natural way of defining a transformation $T^*: W^* \to V^*$ , by composition: given a functional $g \in W^*$ , I define $T^*g := f \in V^*$ by: $$f(\alpha) = g(T\alpha)$$ This way of defining $T^*$ is familiar to me, as it is similar a construction commonly done on modules over a ring $R$ . Now, if I represent $T$ as a matrix on a choice of basis for $V$ and $W$ , and $T^*$ as the matrix on the basis given by the lifting, then $T^*$ will be the transpose of $T$ (the exchange of rows and columns of $T$ ). I don't see clearly why this happens. Furthermore, the rank of $T$ is equal to the rank of $T^*$ , what proves that the column rank of a matrix equals it's row rank. Given that the rank is the dimension of the image subspace, I think that this can be show by carrying out the image of $V$ on $W$ to the duals. I also read that there is a relation between the duals and the kernel of a linear transformation (the annihilator (?)), but that isn't very clear to me either. Using that and the facts above, one can prove the rank-nullity theorem. The reason I'm asking this is that I was given a proof of the rank-nullity theorem without using the linear functionals, and that seemed to depend only on $T$ having the same row and column rank (which was proved by smart manipulation of some equations on vectors). That proof didn't gave me a satisfactory intuition on the rank-nullity theorem, specially when $V \neq W$ . I believe that the proof through linear functionals will be more enlightening. EDIT: I was following the treatment given on Hoffman's Linear Algebra, chapters 3.5 through 3.7 (linear functionals, annihilators and transposes), if that's of interest.","Summing up, how can one use linear functionals, transpose matrices, row and column rank equality and annihilators to prove the rank-nullity theorem? While studying linear algebra, I'm trying to get the precise relation between the following concepts: given two vector spaces and , I can form the dual spaces and by taking all the linear functionals on and . The basis of each one of those spaces lifts to the duals (though this demonstration I'm still going to carry on, but I have the idea how to do so). If I have a linear transformation , I have a natural way of defining a transformation , by composition: given a functional , I define by: This way of defining is familiar to me, as it is similar a construction commonly done on modules over a ring . Now, if I represent as a matrix on a choice of basis for and , and as the matrix on the basis given by the lifting, then will be the transpose of (the exchange of rows and columns of ). I don't see clearly why this happens. Furthermore, the rank of is equal to the rank of , what proves that the column rank of a matrix equals it's row rank. Given that the rank is the dimension of the image subspace, I think that this can be show by carrying out the image of on to the duals. I also read that there is a relation between the duals and the kernel of a linear transformation (the annihilator (?)), but that isn't very clear to me either. Using that and the facts above, one can prove the rank-nullity theorem. The reason I'm asking this is that I was given a proof of the rank-nullity theorem without using the linear functionals, and that seemed to depend only on having the same row and column rank (which was proved by smart manipulation of some equations on vectors). That proof didn't gave me a satisfactory intuition on the rank-nullity theorem, specially when . I believe that the proof through linear functionals will be more enlightening. EDIT: I was following the treatment given on Hoffman's Linear Algebra, chapters 3.5 through 3.7 (linear functionals, annihilators and transposes), if that's of interest.",V W V^* W^* V W T: V \to W T^*: W^* \to V^* g \in W^* T^*g := f \in V^* f(\alpha) = g(T\alpha) T^* R T V W T^* T^* T T T T^* V W T V \neq W,"['linear-algebra', 'vector-spaces', 'matrix-rank', 'duality-theorems', 'transpose']"
49,Interlacing Theorem on Singular Values,Interlacing Theorem on Singular Values,,"Does the Cauchy's interlacing theorem hold for ""singular values"" of matrices too? I saw on this publication first Theorem that it does. It states that singular values of a matrix interlace the singular values of its principal sub-matrices. I would have thought given the original (celebrated) Cauchy's interlacing theorem that is on the ""eigenvalues"" of symmetric matrices and their sub-matrices, that to make interlacing statements about singular values we would need a restriction on positivity of the matrix. Is my intuition wrong?","Does the Cauchy's interlacing theorem hold for ""singular values"" of matrices too? I saw on this publication first Theorem that it does. It states that singular values of a matrix interlace the singular values of its principal sub-matrices. I would have thought given the original (celebrated) Cauchy's interlacing theorem that is on the ""eigenvalues"" of symmetric matrices and their sub-matrices, that to make interlacing statements about singular values we would need a restriction on positivity of the matrix. Is my intuition wrong?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'singular-values']"
50,Eigenvalues of Hermitian dilation of any square matrix,Eigenvalues of Hermitian dilation of any square matrix,,"Let us consider any $n \times n$ matrix $A$. My question is, what are the eigenvalues of \begin{equation} \mathcal{A} = \begin{bmatrix} 0 & A \\ A^* & 0 \end{bmatrix}.  \end{equation} Of course since $\mathcal{A}$ is traceless Hermitian, if $\lambda$ is an eigenvalue, $-\lambda$ is also an eigenvalue. Motivation & further questions: If $A$ is Hermitian with eigenvalues $\lambda_j$, then the eigenvalues of $\mathcal{A}$ are $\pm \lambda_j$. Can we say something similar when $A$ is not Hermitian? Granted, $A$ may not be diagonalizable, can we say something in terms of singular values? What if, for a simple case, all eigenvalues of $A$ be real, though $A$ may not be Hermitian? Advanced thanks for any help/ suggestions.","Let us consider any $n \times n$ matrix $A$. My question is, what are the eigenvalues of \begin{equation} \mathcal{A} = \begin{bmatrix} 0 & A \\ A^* & 0 \end{bmatrix}.  \end{equation} Of course since $\mathcal{A}$ is traceless Hermitian, if $\lambda$ is an eigenvalue, $-\lambda$ is also an eigenvalue. Motivation & further questions: If $A$ is Hermitian with eigenvalues $\lambda_j$, then the eigenvalues of $\mathcal{A}$ are $\pm \lambda_j$. Can we say something similar when $A$ is not Hermitian? Granted, $A$ may not be diagonalizable, can we say something in terms of singular values? What if, for a simple case, all eigenvalues of $A$ be real, though $A$ may not be Hermitian? Advanced thanks for any help/ suggestions.",,"['linear-algebra', 'matrices', 'functional-analysis', 'eigenvalues-eigenvectors']"
51,Are there nontrivial vector spaces with finitely many elements?,Are there nontrivial vector spaces with finitely many elements?,,I have only seen infinite vector spaces and the one finite vector space i.e the trivial vector space $\{0\}$. Is there any other finite vector space?,I have only seen infinite vector spaces and the one finite vector space i.e the trivial vector space $\{0\}$. Is there any other finite vector space?,,['linear-algebra']
52,Can a positive definite matrix have an eigenvalue equal to zero,Can a positive definite matrix have an eigenvalue equal to zero,,"I have a square symmetric matrix. I want to check if it is positive definite. I understand that according to wikipedia if all the eigenvalues are positive, the matrix is positive definite.  I don't know if this means $\lambda$ >0 or if it means $\lambda \geq$ 0. Because I have $\lambda$ =0. According to wiki zero isn't positive or negative. I am just a little confused on this topic.","I have a square symmetric matrix. I want to check if it is positive definite. I understand that according to wikipedia if all the eigenvalues are positive, the matrix is positive definite.  I don't know if this means >0 or if it means 0. Because I have =0. According to wiki zero isn't positive or negative. I am just a little confused on this topic.",\lambda \lambda \geq \lambda,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
53,Determinant of the inverse matrix [duplicate],Determinant of the inverse matrix [duplicate],,This question already has an answer here : Prove that $ \det \left( A^{-1} \right) = \frac{1}{\det(A)} $ (1 answer) Closed 9 years ago . I'm seeking for a proof of the following: Let $A$ be an invertible matrix. Then the determinant of $A^{-1}$ equals:   $$\left|A^{-1}\right|=|A|^{-1} $$ I don't know where to begin the proof. Any suggestions?,This question already has an answer here : Prove that $ \det \left( A^{-1} \right) = \frac{1}{\det(A)} $ (1 answer) Closed 9 years ago . I'm seeking for a proof of the following: Let $A$ be an invertible matrix. Then the determinant of $A^{-1}$ equals:   $$\left|A^{-1}\right|=|A|^{-1} $$ I don't know where to begin the proof. Any suggestions?,,"['linear-algebra', 'determinant', 'inverse']"
54,Eigenvalue of the substraction of 2 matrices,Eigenvalue of the substraction of 2 matrices,,"Consider you have two $n \times n$ matrices $A,B$ with the same eigenvalue $\pi$. Then $A-B$ has an eigenvalue of $0$. The question is, is this correct or not? I was looking for properties in my head and in the course text, but i didn't find anything useful. Cause we don't know if $\pi$ is die only eigenvalue. We also don't know if the eigen vectors corresponding to the eigenvalues are the same. So we don't know if the matrices are similar or not. The only thing I tried was: Consider that they have the same eigen vector. Then you could write: $Av-Bv$ = $(A-B)v$  and $\pi v-\pi v = (\pi-\pi) v$ and $\pi-\pi = 0$ So then you could proof this, but this is not the case unfortunately. I can also find no example that it's not true. Thank you","Consider you have two $n \times n$ matrices $A,B$ with the same eigenvalue $\pi$. Then $A-B$ has an eigenvalue of $0$. The question is, is this correct or not? I was looking for properties in my head and in the course text, but i didn't find anything useful. Cause we don't know if $\pi$ is die only eigenvalue. We also don't know if the eigen vectors corresponding to the eigenvalues are the same. So we don't know if the matrices are similar or not. The only thing I tried was: Consider that they have the same eigen vector. Then you could write: $Av-Bv$ = $(A-B)v$  and $\pi v-\pi v = (\pi-\pi) v$ and $\pi-\pi = 0$ So then you could proof this, but this is not the case unfortunately. I can also find no example that it's not true. Thank you",,"['linear-algebra', 'eigenvalues-eigenvectors']"
55,Prove that $ AA^T=0\implies A = 0$ [duplicate],Prove that  [duplicate], AA^T=0\implies A = 0,"This question already has answers here : If $AA^T$ is the zero matrix, then $A$ is the zero matrix (3 answers) Closed 3 years ago . Let $A$ be an $n \times n$ matrix with real entries, where $n\geq2$.   Let $AA^T = [b_{ij}] $, where $A^T $ is the transpose of $A$. If   $b_{11} + b_{22 }+\cdots+ b_{nn} = 0$, show that $A = 0$. From what I've gleaned so far, $AA^T$ is a symmetric matrix, and the diagonals are zero. I can't figure out how to solve this question. Is there some property that exists that I'm missing for handling this question?","This question already has answers here : If $AA^T$ is the zero matrix, then $A$ is the zero matrix (3 answers) Closed 3 years ago . Let $A$ be an $n \times n$ matrix with real entries, where $n\geq2$.   Let $AA^T = [b_{ij}] $, where $A^T $ is the transpose of $A$. If   $b_{11} + b_{22 }+\cdots+ b_{nn} = 0$, show that $A = 0$. From what I've gleaned so far, $AA^T$ is a symmetric matrix, and the diagonals are zero. I can't figure out how to solve this question. Is there some property that exists that I'm missing for handling this question?",,['linear-algebra']
56,$A^3 = I$ ($A$ is real Symmetric matrix). Does it imply that $A = I$?,( is real Symmetric matrix). Does it imply that ?,A^3 = I A A = I,"Question of our assignment $A$ is a $3×3$ real symmetric matrix such that $A^3 = I$  (Identity matrix). Does it imply that $A = I$? If so, why? If not, give an example. Any help will be appreciated.","Question of our assignment $A$ is a $3×3$ real symmetric matrix such that $A^3 = I$  (Identity matrix). Does it imply that $A = I$? If so, why? If not, give an example. Any help will be appreciated.",,"['linear-algebra', 'matrices']"
57,matrices forms a basis for vector space 2x2,matrices forms a basis for vector space 2x2,,$\begin{bmatrix}0&1\\2&3\end{bmatrix}$ $\begin{bmatrix}3&4\\5&6\end{bmatrix}$ $\begin{bmatrix}7&8\\9&10\end{bmatrix}$ $\begin{bmatrix}11&12\\13&14\end{bmatrix}$ Show that the 4 matrix forms a basis for vector space 2x2,$\begin{bmatrix}0&1\\2&3\end{bmatrix}$ $\begin{bmatrix}3&4\\5&6\end{bmatrix}$ $\begin{bmatrix}7&8\\9&10\end{bmatrix}$ $\begin{bmatrix}11&12\\13&14\end{bmatrix}$ Show that the 4 matrix forms a basis for vector space 2x2,,"['linear-algebra', 'matrices', 'vector-spaces']"
58,Cartesian & Tensor Product,Cartesian & Tensor Product,,What is the difference between a cartesian product and tensor product of two vector spaces $V_1$ and $V_2$ defined over same field $F$ ?,What is the difference between a cartesian product and tensor product of two vector spaces $V_1$ and $V_2$ defined over same field $F$ ?,,"['linear-algebra', 'vector-spaces', 'tensor-products', 'tensors']"
59,If every subspace of a vector space V is invariant under a linear transformation T then T is a scalar transformation,If every subspace of a vector space V is invariant under a linear transformation T then T is a scalar transformation,,"I got the following problem Let $V$ be a vector space over field $\mathbb{F}$ and let $T:V \to V$ be a linear transformation such that every subspace of $V$ is invariant under $T$, Show that there exist a scalar $\alpha \in \mathbb{F}$ such that $T = \alpha I$ (meaning $T$ is a scalar transformation) I tried to show it but I got that the matrix for $T$ in some basis $B$ is a diagonal matrix where each entry on the main diagonal is some eigenvalue $\lambda _i$, How do I show that all the eigenvalues are equal to each other?","I got the following problem Let $V$ be a vector space over field $\mathbb{F}$ and let $T:V \to V$ be a linear transformation such that every subspace of $V$ is invariant under $T$, Show that there exist a scalar $\alpha \in \mathbb{F}$ such that $T = \alpha I$ (meaning $T$ is a scalar transformation) I tried to show it but I got that the matrix for $T$ in some basis $B$ is a diagonal matrix where each entry on the main diagonal is some eigenvalue $\lambda _i$, How do I show that all the eigenvalues are equal to each other?",,"['linear-algebra', 'vector-spaces']"
60,$A$ is a symmetric real matrix. Show that there is $B$ such that $B^3=A$,is a symmetric real matrix. Show that there is  such that,A B B^3=A,"I'm having trouble with this question, I'd like someone to point me in the right direction. let $A$ be a n by n matrix with real values.  show that there is another n by n real matrix $B$ such that $B^3=A$, and that $B$ is symmetric. Are there more matrices like this $B$ or is it the only one? What I was thinking: I don't have a clear way to solve it. I think we need to use the fact that if a real matrix is symmetric, then it is normal, and so has an orthonormal basis of eigenvectors...Other then that I don't really know anything.","I'm having trouble with this question, I'd like someone to point me in the right direction. let $A$ be a n by n matrix with real values.  show that there is another n by n real matrix $B$ such that $B^3=A$, and that $B$ is symmetric. Are there more matrices like this $B$ or is it the only one? What I was thinking: I don't have a clear way to solve it. I think we need to use the fact that if a real matrix is symmetric, then it is normal, and so has an orthonormal basis of eigenvectors...Other then that I don't really know anything.",,"['linear-algebra', 'matrices', 'orthonormal', 'symmetry']"
61,About restriction of linear map,About restriction of linear map,,Let $A$ be a linear map from the vector space $X$ to $Y$ and $T$ be a subspace of $X$ . I want to understand what is the meaning of saying that the restriction $A_{|T}: \to A(T)$ is invertible. Could anybody explain me what is this restriction map? Thanks for the help.,Let $A$ be a linear map from the vector space $X$ to $Y$ and $T$ be a subspace of $X$ . I want to understand what is the meaning of saying that the restriction $A_{|T}: \to A(T)$ is invertible. Could anybody explain me what is this restriction map? Thanks for the help.,,['linear-algebra']
62,$U = (I-iT) (I + iT) ^ {-1}$ is a unitary operator when T is self-adjoint,is a unitary operator when T is self-adjoint,U = (I-iT) (I + iT) ^ {-1},"Let $V$ complex inner product space of finite dimension and $T$ an operator over $V$. Show that the transformation $$U = (I-iT) (I + iT) ^ {-1}$$ is a unitary operator My Attempt: $$\langle U \alpha, U\alpha \rangle = \langle (I−iT)(I+iT)^{-1} \alpha , (I−iT)(I+iT)^{-1} \alpha \rangle =  \langle \alpha , (I−iT)^{-1}(I+iT)(I−iT)(I+iT)^{-1} \alpha \rangle $$ and I need proof this  $(I−iT)^{-1}(I+iT)(I−iT)(I+iT)^{-1} = I $","Let $V$ complex inner product space of finite dimension and $T$ an operator over $V$. Show that the transformation $$U = (I-iT) (I + iT) ^ {-1}$$ is a unitary operator My Attempt: $$\langle U \alpha, U\alpha \rangle = \langle (I−iT)(I+iT)^{-1} \alpha , (I−iT)(I+iT)^{-1} \alpha \rangle =  \langle \alpha , (I−iT)^{-1}(I+iT)(I−iT)(I+iT)^{-1} \alpha \rangle $$ and I need proof this  $(I−iT)^{-1}(I+iT)(I−iT)(I+iT)^{-1} = I $",,"['linear-algebra', 'functional-analysis']"
63,"Prove or disprove: If $x^T A x = 0 $ for all $x$, then $ A = 0 $.","Prove or disprove: If  for all , then .",x^T A x = 0  x  A = 0 ,"Let $A$ be a square matrix and $x$ be a vector. Now consider the statement: If $x^T A x = 0 $ for any $x$, then $A = 0$. Is the above statement true or false? How would you prove it?","Let $A$ be a square matrix and $x$ be a vector. Now consider the statement: If $x^T A x = 0 $ for any $x$, then $A = 0$. Is the above statement true or false? How would you prove it?",,['linear-algebra']
64,How to find zero of an space,How to find zero of an space,,"I was trying to solve this exercise: Decide whether or not the set $\Bbb R^2$, with addition defined by $$(x,y) + (a,b) = (x+a+1, y+b)$$ and with scalar multiplication $$r\cdot(x,y) = (rx+r-1, ry)$$ is a (real) vector space. So, I try to prove via its space's properties (associatity, conmutativity, etc) and it works. Even preservation of scale. But when I try to prove for zero $(0\cdot v=0)$ It doesn't work for me, I mean $0\cdot(x, y) = (-1, 0)$ which is clearly not $(0,0)$ But book's answer is: it is an space, so I must be wrong. Now, thinking about it, I'm starting to believe that zero vector of vector spaces isn't necessary $(0,0,\dots,0)$. And then for this example $(-1,0)$ is the zero of the space, and it can be found taking any space element and doing $x - x$ ($x$ minus itself). Am I right? Or am I omitting something more obvious. Thanks.","I was trying to solve this exercise: Decide whether or not the set $\Bbb R^2$, with addition defined by $$(x,y) + (a,b) = (x+a+1, y+b)$$ and with scalar multiplication $$r\cdot(x,y) = (rx+r-1, ry)$$ is a (real) vector space. So, I try to prove via its space's properties (associatity, conmutativity, etc) and it works. Even preservation of scale. But when I try to prove for zero $(0\cdot v=0)$ It doesn't work for me, I mean $0\cdot(x, y) = (-1, 0)$ which is clearly not $(0,0)$ But book's answer is: it is an space, so I must be wrong. Now, thinking about it, I'm starting to believe that zero vector of vector spaces isn't necessary $(0,0,\dots,0)$. And then for this example $(-1,0)$ is the zero of the space, and it can be found taking any space element and doing $x - x$ ($x$ minus itself). Am I right? Or am I omitting something more obvious. Thanks.",,['linear-algebra']
65,"Matrix P to the power of 4, i.e $P^4$, is this the same as $P^2 \cdot P^2$?","Matrix P to the power of 4, i.e , is this the same as ?",P^4 P^2 \cdot P^2,"Basically, what it says in the title. I have a $5 \times 5$ matrix and I need to work out $P^4$, is it possible to just do $P^2$ and multiply this with itself?","Basically, what it says in the title. I have a $5 \times 5$ matrix and I need to work out $P^4$, is it possible to just do $P^2$ and multiply this with itself?",,"['linear-algebra', 'matrices', 'exponentiation']"
66,Determinant of a linear transformation defining matrix transpose,Determinant of a linear transformation defining matrix transpose,,So if I define a linear transformation $ T: M_{n\times n}(R) \rightarrow M_{n\times n}(R) $ and $ T(A)=A^t $ what would be its determinant?,So if I define a linear transformation $ T: M_{n\times n}(R) \rightarrow M_{n\times n}(R) $ and $ T(A)=A^t $ what would be its determinant?,,"['linear-algebra', 'determinant']"
67,How to prove that if $\det(A)=0$ then $\det(\operatorname{adj}(A))=0$?,How to prove that if  then ?,\det(A)=0 \det(\operatorname{adj}(A))=0,How to prove that if $\det(A)=0$ then $\det(\operatorname{adj}(A))=0$? I have been trying to solve this but I can't use $$\det(A^{-1})=\det \Big(\frac{1}{\det(A)} \operatorname{adj}(A) \Big)$$ because $\det(A)=0$ and $\frac{1}{0}$ is not allowed.,How to prove that if $\det(A)=0$ then $\det(\operatorname{adj}(A))=0$? I have been trying to solve this but I can't use $$\det(A^{-1})=\det \Big(\frac{1}{\det(A)} \operatorname{adj}(A) \Big)$$ because $\det(A)=0$ and $\frac{1}{0}$ is not allowed.,,['linear-algebra']
68,Properties for a matrix being invariant under rotation?,Properties for a matrix being invariant under rotation?,,Consider a 2D case.  Let $R$ be a rotation matrix with angle $\theta$ $$R = \begin{bmatrix} \cos\theta & -\sin\theta\\ \sin\theta & \cos\theta \end{bmatrix}.$$ Is it possible for a matrix $A$ to satisfy the following identity for any $\theta$ $$A = R A R^T?$$,Consider a 2D case.  Let $R$ be a rotation matrix with angle $\theta$ $$R = \begin{bmatrix} \cos\theta & -\sin\theta\\ \sin\theta & \cos\theta \end{bmatrix}.$$ Is it possible for a matrix $A$ to satisfy the following identity for any $\theta$ $$A = R A R^T?$$,,['linear-algebra']
69,What problems will arise if we define matrix multiplication this way?,What problems will arise if we define matrix multiplication this way?,,"$a\in \mathbb{R}^n$ $a=(a_1,a_2,\dots,a_n)$ lets define this to be equivalent to $(a_1,a_2,\dots,a_n,0,0,0 \dots,0)$ (finite many zeros) by this I think we can make an $n \times m$ matrix $A$ equivalent to any $ p \times p$ $B$ matrix with $p\ge \max\{n,m\}$ by $B_{ij}= A_{ij}$ if $1\le i\le n$ and $1\le j \le m$ otherwise all the other entries of the matrix $B$ will be $0$ now we can multiply any two matrices $A, B$ by making them in the form of $P_1$ , $P_2$ such that $P_1, P_2$ are $p \times p$ matrices where $p is \max\{ n,m,r,s\} $ where $A$ is an $n \times m$ matrix and $B$ is a $r \times s$ matrix  . my question is What problems will arise if we define matrix multiplication this way ? There should be a lot of problems because mathematicians because if there aren't any problems someone would probably generalise matrix multiplication this way Another question is: is this definition agrees with the usual one in the cases where both methods of multiplying are defined? l examples that I tried result to the ""same"" matrix but I couldn't prove that.","lets define this to be equivalent to (finite many zeros) by this I think we can make an matrix equivalent to any matrix with by if and otherwise all the other entries of the matrix will be now we can multiply any two matrices by making them in the form of , such that are matrices where where is an matrix and is a matrix  . my question is What problems will arise if we define matrix multiplication this way ? There should be a lot of problems because mathematicians because if there aren't any problems someone would probably generalise matrix multiplication this way Another question is: is this definition agrees with the usual one in the cases where both methods of multiplying are defined? l examples that I tried result to the ""same"" matrix but I couldn't prove that.","a\in \mathbb{R}^n a=(a_1,a_2,\dots,a_n) (a_1,a_2,\dots,a_n,0,0,0 \dots,0) n \times m A  p \times p B p\ge \max\{n,m\} B_{ij}= A_{ij} 1\le i\le n 1\le j \le m B 0 A, B P_1 P_2 P_1, P_2 p \times p p is \max\{ n,m,r,s\}  A n \times m B r \times s","['linear-algebra', 'matrices', 'vectors', 'soft-question']"
70,A condition for matrices to commute,A condition for matrices to commute,,"Recently found an exercise for high school students as follows: Let $A,B$ be $2\times 2$ matrices with real entries and suppose $A^2=B^2$ , $\operatorname{tr}(A)\neq0$ , $\operatorname{tr}(B)\neq0$ . Show that $AB=BA$ . Since it only ask the case for $2\times 2$ matrix and hence we can simply bash out all the equations. My problem. Does this hold for $n\times n$ matrices, or does it have a generalization?","Recently found an exercise for high school students as follows: Let be matrices with real entries and suppose , , . Show that . Since it only ask the case for matrix and hence we can simply bash out all the equations. My problem. Does this hold for matrices, or does it have a generalization?","A,B 2\times 2 A^2=B^2 \operatorname{tr}(A)\neq0 \operatorname{tr}(B)\neq0 AB=BA 2\times 2 n\times n","['linear-algebra', 'matrices']"
71,Let $A$ be a symmetric $n \times n$ matrix. Prove that $A$ and $A^5$ have the same null space.,Let  be a symmetric  matrix. Prove that  and  have the same null space.,A n \times n A A^5,"Let $A$ be a symmetric $n \times n$ matrix. Prove that $A$ and $A^5$ have the same null space. This question came up when I was solving. And I'm not entirely sure on how to approach it, is it valid to just write $A^4(AX) = 0$ (assuming here that $X$ belongs to the null space of $A$ ), which would give $0$ , meaning that $A$ and $A^5$ have the same null space?","Let be a symmetric matrix. Prove that and have the same null space. This question came up when I was solving. And I'm not entirely sure on how to approach it, is it valid to just write (assuming here that belongs to the null space of ), which would give , meaning that and have the same null space?",A n \times n A A^5 A^4(AX) = 0 X A 0 A A^5,"['linear-algebra', 'matrices']"
72,Numerically computing eigenvalues -- what is it useful for?,Numerically computing eigenvalues -- what is it useful for?,,"Cross-posted on Scientific Computing Stack Exchange Are there real-world applications that call specifically for eigenvalues rather than singular values? Top eigenvalue is useful to establish convergence , but what about the rest? I often see eigendecomposition used as ""poor-man's SVD"" For instance it's used in Matlab's Lyapunov solver, but that could be reformulated in terms of SVD with greater cost ( $22n^3$ instead of $9n^3$ , Higham's big six ), while gaining numerical stability. Similarly, PCA can be done using SVD. Picture below: two linear transformations below have the same eigenvalues: Notebook","Cross-posted on Scientific Computing Stack Exchange Are there real-world applications that call specifically for eigenvalues rather than singular values? Top eigenvalue is useful to establish convergence , but what about the rest? I often see eigendecomposition used as ""poor-man's SVD"" For instance it's used in Matlab's Lyapunov solver, but that could be reformulated in terms of SVD with greater cost ( instead of , Higham's big six ), while gaining numerical stability. Similarly, PCA can be done using SVD. Picture below: two linear transformations below have the same eigenvalues: Notebook",22n^3 9n^3,"['linear-algebra', 'numerical-methods', 'soft-question', 'numerical-linear-algebra', 'applications']"
73,Meaning of vector equations,Meaning of vector equations,,"I'm taking a linear algebra class now, and I was introduced to vector equations. Consider the system $$ \left\{ \begin{array}{rcl} x-4y &=& 8\\ 2x+3y &=& 6\\ \end{array} \right.$$ I want to understand why I can factor out the x and y variables to create the vector equation $$x\begin{bmatrix} 1 \\ 2 \end{bmatrix}+y\begin{bmatrix} -4 \\ 3 \end{bmatrix}=\begin{bmatrix} 8 \\ 6 \end{bmatrix}$$ are $x$ and $y$ scalars here? Is the column vector $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ the same as the vector $\left< 1, 2\right>$ ? And lastly, how does this notation help me solve for solutions?","I'm taking a linear algebra class now, and I was introduced to vector equations. Consider the system I want to understand why I can factor out the x and y variables to create the vector equation are and scalars here? Is the column vector the same as the vector ? And lastly, how does this notation help me solve for solutions?","
\left\{ \begin{array}{rcl}
x-4y &=& 8\\
2x+3y &=& 6\\
\end{array}
\right. x\begin{bmatrix} 1 \\ 2 \end{bmatrix}+y\begin{bmatrix} -4 \\ 3 \end{bmatrix}=\begin{bmatrix} 8 \\ 6 \end{bmatrix} x y \begin{bmatrix} 1 \\ 2 \end{bmatrix} \left< 1, 2\right>","['linear-algebra', 'vectors']"
74,"When should I take conjugate transpose of a complex matrix, and when transpose of it?","When should I take conjugate transpose of a complex matrix, and when transpose of it?",,"I was taking the inverse of $$A=\begin{bmatrix} 2+i &1 \\   1&-2+i  \end{bmatrix}$$ and $\det(A)=-6 $ , and cofactor matrix $$C=\begin{bmatrix} -2+i &-1 \\   -1&2+i  \end{bmatrix}$$ such that correct way to do it is $$A^{-1}=\frac{1}{\det(A)}C^{T}$$ but I'm wondering why we are not taking conjugate transpose of C?","I was taking the inverse of and , and cofactor matrix such that correct way to do it is but I'm wondering why we are not taking conjugate transpose of C?","A=\begin{bmatrix}
2+i &1 \\ 
 1&-2+i 
\end{bmatrix} \det(A)=-6  C=\begin{bmatrix}
-2+i &-1 \\ 
 -1&2+i 
\end{bmatrix} A^{-1}=\frac{1}{\det(A)}C^{T}","['linear-algebra', 'matrices', 'inverse', 'transpose', 'hermitian-matrices']"
75,"If $A$ is a square matrix that satisfies $A^2-A+2I=0$, show that $A+I$ is invertible","If  is a square matrix that satisfies , show that  is invertible",A A^2-A+2I=0 A+I,"If $A$ is a square matrix that satisfies $A^2-A+2I=0$ , show that $A+I$ is invertible. I understand how to find if $A$ is invertible but I don't know how to solve for the $A+I$ version.","If is a square matrix that satisfies , show that is invertible. I understand how to find if is invertible but I don't know how to solve for the version.",A A^2-A+2I=0 A+I A A+I,[]
76,How many $3 \times 3$ non-symmetric and non-singular matrices $A$ are there such that $A^{T}=A^2-I$?,How many  non-symmetric and non-singular matrices  are there such that ?,3 \times 3 A A^{T}=A^2-I,How many $3 \times 3$ non-symmetric and non-singular matrices $A$ are there such that $A^{T}=A^2-I$ ? Note: $I$ denotes the identity matrix of size $3 \times3$ and $A^{T}$ represents the transpose of the matrix $A$ . I took transpose on both sides in given equation to get $A=(A^T)^2-I$ and then I put value of $A^T$ in this equation using $A^{T}=A^2-I$ to get $A^3-2A-I=0$ which ultimately gave $(A^T-A)(A+I)=0$ . How to deal the problem from here? And is there any better approach to tackle this problem? The given answer is $0$ .,How many non-symmetric and non-singular matrices are there such that ? Note: denotes the identity matrix of size and represents the transpose of the matrix . I took transpose on both sides in given equation to get and then I put value of in this equation using to get which ultimately gave . How to deal the problem from here? And is there any better approach to tackle this problem? The given answer is .,3 \times 3 A A^{T}=A^2-I I 3 \times3 A^{T} A A=(A^T)^2-I A^T A^{T}=A^2-I A^3-2A-I=0 (A^T-A)(A+I)=0 0,"['linear-algebra', 'matrices']"
77,How exactly to use Cayley's Hamilton's theorem to find $A^{50}$ in this case? (matrix recursion equation),How exactly to use Cayley's Hamilton's theorem to find  in this case? (matrix recursion equation),A^{50},"Say $$A=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$$ I'm supposed to find $A^{50}$ using the Cayley Hamilton theorem. My Attempt: $|A-\lambda I| = 0 \implies \lambda^2-2\lambda+1=0$ So $A^2-2A+I = O \implies A^{50}=2A^{49} - A^{48}$ But using this method, I'll need to know $A^{49}$ and $A^{48}$ to find $A^{50}$. But that doesn't seem the right way. This seems like some sort of matrix recursion but I'm not sure how to proceed from here.","Say $$A=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$$ I'm supposed to find $A^{50}$ using the Cayley Hamilton theorem. My Attempt: $|A-\lambda I| = 0 \implies \lambda^2-2\lambda+1=0$ So $A^2-2A+I = O \implies A^{50}=2A^{49} - A^{48}$ But using this method, I'll need to know $A^{49}$ and $A^{48}$ to find $A^{50}$. But that doesn't seem the right way. This seems like some sort of matrix recursion but I'm not sure how to proceed from here.",,['linear-algebra']
78,Can one compute the dot product knowing only the basis expansions of vectors v and w?,Can one compute the dot product knowing only the basis expansions of vectors v and w?,,"Scenario: Suppose that $V$ is a finite dimensional vector space and that $\{e_1,...,e_n\}$ is a basis for $V$. Let $v,w\in V$ and suppose that $v=\sum_{i=1}^n \alpha _i e_i$ and $w=\sum_{i=1}^n \beta _i e_j$ where $\alpha_i$, $i=1,..,n$ and $\beta_j$, $j=1,...,n$.  Can we compute $\langle v,w\rangle $ knowing only the basis expansions of $v$ and $w$ and the values of $\{\langle e_i,e_j\rangle :i,j=1,...,n\}$? What I understand: I know that $\langle v,w\rangle =\langle \sum_{i=1}^n \alpha _i e_i, \sum_{i=1}^n \beta _i e_j\rangle$ and that $\langle e_i,e_j\rangle=0$. The basis expansion would be any $\alpha_1 e_1 +\alpha_2 e_2+...+ \alpha_n e_n$ and similarly for $\beta_j e_j$.  Is the problem asking whether the dot product is simply $\sum_{i=1}^n \alpha_i \beta_j$?","Scenario: Suppose that $V$ is a finite dimensional vector space and that $\{e_1,...,e_n\}$ is a basis for $V$. Let $v,w\in V$ and suppose that $v=\sum_{i=1}^n \alpha _i e_i$ and $w=\sum_{i=1}^n \beta _i e_j$ where $\alpha_i$, $i=1,..,n$ and $\beta_j$, $j=1,...,n$.  Can we compute $\langle v,w\rangle $ knowing only the basis expansions of $v$ and $w$ and the values of $\{\langle e_i,e_j\rangle :i,j=1,...,n\}$? What I understand: I know that $\langle v,w\rangle =\langle \sum_{i=1}^n \alpha _i e_i, \sum_{i=1}^n \beta _i e_j\rangle$ and that $\langle e_i,e_j\rangle=0$. The basis expansion would be any $\alpha_1 e_1 +\alpha_2 e_2+...+ \alpha_n e_n$ and similarly for $\beta_j e_j$.  Is the problem asking whether the dot product is simply $\sum_{i=1}^n \alpha_i \beta_j$?",,"['linear-algebra', 'matrices', 'vector-spaces', 'inner-products']"
79,Left-multiplication of a finite field element - Matrix representation,Left-multiplication of a finite field element - Matrix representation,,Can someone explain me why and how a left multiplication of an element of a finite field GF(2^k) can be seen as a linear transformation on GF(2^k) over GF(2)?  I read this https://www.maa.org/sites/default/files/Wardlaw47052.pdf but it is not clear to me. Thank you!,Can someone explain me why and how a left multiplication of an element of a finite field GF(2^k) can be seen as a linear transformation on GF(2^k) over GF(2)?  I read this https://www.maa.org/sites/default/files/Wardlaw47052.pdf but it is not clear to me. Thank you!,,"['linear-algebra', 'abstract-algebra', 'linear-transformations', 'finite-fields']"
80,Inverse of matrix with nonnegative entries,Inverse of matrix with nonnegative entries,,"I am interested in matrices with the property that both $A$ and $A^{-1}$ have nonnegative entries.  The only such matrices I could construct were diagonal matrices, and my question is whether these are the only such examples. What I can say about such matrices is that they must preserve the quadrant $$ Q^+ = \{x\in\mathbb{R}^n \mid x_i \geq 0 \}. $$ That is, $x\in Q^+$ if and only if $Ax\in Q^+$.  This seems rather unlikely unless $A$ preserves the axes, that is, unless $A$ is diagonal.  But I can't seem to turn this into a proof. EDIT Cameron Buie made the nice observation that permutation matrices also work. So I wonder: are there any examples with more than $n$ nonzero entries?  What about 2x2 examples with at least 3 nonzero entries?","I am interested in matrices with the property that both $A$ and $A^{-1}$ have nonnegative entries.  The only such matrices I could construct were diagonal matrices, and my question is whether these are the only such examples. What I can say about such matrices is that they must preserve the quadrant $$ Q^+ = \{x\in\mathbb{R}^n \mid x_i \geq 0 \}. $$ That is, $x\in Q^+$ if and only if $Ax\in Q^+$.  This seems rather unlikely unless $A$ preserves the axes, that is, unless $A$ is diagonal.  But I can't seem to turn this into a proof. EDIT Cameron Buie made the nice observation that permutation matrices also work. So I wonder: are there any examples with more than $n$ nonzero entries?  What about 2x2 examples with at least 3 nonzero entries?",,"['linear-algebra', 'matrices']"
81,Sum of two rank deficient matrices,Sum of two rank deficient matrices,,"Suppose I have two $m\times n$, where $m>n$, matrices $A$ and $B$. The rank of $A$ and the rank of $B$ are strictly less than $n$. Are there any (general) sufficient conditions under which one can guarantee that the rank of sum $A+B$  is strictly less than $n$?","Suppose I have two $m\times n$, where $m>n$, matrices $A$ and $B$. The rank of $A$ and the rank of $B$ are strictly less than $n$. Are there any (general) sufficient conditions under which one can guarantee that the rank of sum $A+B$  is strictly less than $n$?",,"['linear-algebra', 'matrices', 'matrix-rank']"
82,Finding the Dimension of a given space $V$,Finding the Dimension of a given space,V,"I am unsure how to solve this problem: If $\vec{v}$ is any nonzero vector in $\mathbb{R}^2$, what is the   dimension of the space $V$ of all $2 \times 2$ matrices for which   $\vec{v}$ is an eigenvector? What I have so far is: $$ \left[   \begin{array}{ c c }      a & b \\      c & d   \end{array} \right]   \left[   \begin{array}{ c }      v_{1}  \\      v_{2}    \end{array} \right] =  \left[   \begin{array}{ c }      \lambda v_{1} \\      \lambda v_{2}   \end{array} \right]  $$ And solving for this I get two equations with four unkowns. $$ av_{1} + bv_{2} = \lambda v_{1}$$ $$cv_{1} + dv_{2} = \lambda v_{2} $$ I am not sure where to go from here. At first I solved for $a$ and $c$ in terms of $b$, $v_1$ and $v_2$ and $d$, $v_1$, and $v_2$ respectively and got dim = 2, but the answer is dim = 3. Any hints why this is?","I am unsure how to solve this problem: If $\vec{v}$ is any nonzero vector in $\mathbb{R}^2$, what is the   dimension of the space $V$ of all $2 \times 2$ matrices for which   $\vec{v}$ is an eigenvector? What I have so far is: $$ \left[   \begin{array}{ c c }      a & b \\      c & d   \end{array} \right]   \left[   \begin{array}{ c }      v_{1}  \\      v_{2}    \end{array} \right] =  \left[   \begin{array}{ c }      \lambda v_{1} \\      \lambda v_{2}   \end{array} \right]  $$ And solving for this I get two equations with four unkowns. $$ av_{1} + bv_{2} = \lambda v_{1}$$ $$cv_{1} + dv_{2} = \lambda v_{2} $$ I am not sure where to go from here. At first I solved for $a$ and $c$ in terms of $b$, $v_1$ and $v_2$ and $d$, $v_1$, and $v_2$ respectively and got dim = 2, but the answer is dim = 3. Any hints why this is?",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
83,Is there an orthogonal matrix that is not unitary?,Is there an orthogonal matrix that is not unitary?,,"I could find a example of a unitary matrix such that is not orthogonal, thats simple in $\mathbb{C}$, but for this exercise of a orthogonal that is not unitary i realize that is possible just on $\mathbb{C}$ because all orthogonal matrix on $\mathbb{R}$ is unitary, so anyone have a exemple of this case?","I could find a example of a unitary matrix such that is not orthogonal, thats simple in $\mathbb{C}$, but for this exercise of a orthogonal that is not unitary i realize that is possible just on $\mathbb{C}$ because all orthogonal matrix on $\mathbb{R}$ is unitary, so anyone have a exemple of this case?",,"['linear-algebra', 'matrices']"
84,Does linearly independent imply all elements are orthogonal?,Does linearly independent imply all elements are orthogonal?,,"As the title states, if you have $A\subset V$ where $V$ is a vector space over an arbitrary field, does $A$ being linearly independent imply that the elements of $A$ are orthogonal?","As the title states, if you have $A\subset V$ where $V$ is a vector space over an arbitrary field, does $A$ being linearly independent imply that the elements of $A$ are orthogonal?",,['linear-algebra']
85,"If $n=\dim(V)$ and $n$ vectors are linearly independent, then they form a basis","If  and  vectors are linearly independent, then they form a basis",n=\dim(V) n,"If $V$ is a vector space and $v_1, v_2, . . . , v_n \in V$ span $V$, and $u_1, u_2, . . . , u_m ∈ V$ are linearly independent, then $m\le n$. Use this to prove that if $V$ has dimension $n$ and $u_1, u_2, . . . , u_n \in V$ are linearly independent then $u_1, u_2,\le, u_n$ form a basis of $V$. Do I prove that $V$ has a basis with n elements? Not really sure how to approach this proof.","If $V$ is a vector space and $v_1, v_2, . . . , v_n \in V$ span $V$, and $u_1, u_2, . . . , u_m ∈ V$ are linearly independent, then $m\le n$. Use this to prove that if $V$ has dimension $n$ and $u_1, u_2, . . . , u_n \in V$ are linearly independent then $u_1, u_2,\le, u_n$ form a basis of $V$. Do I prove that $V$ has a basis with n elements? Not really sure how to approach this proof.",,"['linear-algebra', 'vector-spaces', 'span']"
86,"If the dot product between two vectors is $0$, are the two linearly independent?","If the dot product between two vectors is , are the two linearly independent?",0,"If we have vectors $V$ and $W$ in $\mathbb{R^n}$ and their dot product is $0$, are the two vectors linearly independent? I can expand $V_1 \cdot V_2 = 0 \Rightarrow v_1w_1+...+v_nw_n = 0$, but I don't understand how this relates to linear independence.","If we have vectors $V$ and $W$ in $\mathbb{R^n}$ and their dot product is $0$, are the two vectors linearly independent? I can expand $V_1 \cdot V_2 = 0 \Rightarrow v_1w_1+...+v_nw_n = 0$, but I don't understand how this relates to linear independence.",,['linear-algebra']
87,On the decomposition of stochastic matrices as convex combinations of zero-one matrices,On the decomposition of stochastic matrices as convex combinations of zero-one matrices,,"Let ""stochastic"" matrix be the matrix whose rows sum to one and deterministic matrix be a stochastic matrix whose all rows consist of a one and zero. For example $\left [ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 0      \end{array} \right] $ is a deterministic matrix. I am trying to show that any stochastic matrix can be written as a convex combination of deterministic matrices.","Let ""stochastic"" matrix be the matrix whose rows sum to one and deterministic matrix be a stochastic matrix whose all rows consist of a one and zero. For example $\left [ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 0      \end{array} \right] $ is a deterministic matrix. I am trying to show that any stochastic matrix can be written as a convex combination of deterministic matrices.",,"['linear-algebra', 'matrices', 'reference-request', 'convex-analysis']"
88,The kernel and image of $T^n$,The kernel and image of,T^n,"I need help with this question: Let $V$ be a finite vector space where $ \dim V = n $, over the complex numbers and let $ T: V\to V $ be a linear transformation. Prove that $ V = \ker(T^n) \oplus Im(T^n) $","I need help with this question: Let $V$ be a finite vector space where $ \dim V = n $, over the complex numbers and let $ T: V\to V $ be a linear transformation. Prove that $ V = \ker(T^n) \oplus Im(T^n) $",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
89,Definition of Vector Space,Definition of Vector Space,,"What is the meaning of objects in a vector space? Definition: A vector space is a nonempty set V of objects , called vectors, on   which are defined two operations, called addition and multiplication   by scalars, subject to the ten axioms... Can entries of a vector be anything other than real or complex numbers?","What is the meaning of objects in a vector space? Definition: A vector space is a nonempty set V of objects , called vectors, on   which are defined two operations, called addition and multiplication   by scalars, subject to the ten axioms... Can entries of a vector be anything other than real or complex numbers?",,['linear-algebra']
90,Proof verification,Proof verification,,"I have a problem and a proposed solution. I want to know if I have done it correctly. Problem Statement: Let $V=F^n$ be the space of column vectors. Prove that every subspace $W$ of $V$ is the space of solutions of some system of homogeneous linear equations $AX=0$. My solution: The null space of an $m$ x $n$ matrix $A$, written as $Nul A$, is the set of all solutions to the homogeneous equation $Ax = 0$. The null space of an $m$ x $n$ matrix $A$ is a subspace of $R^n$. Equivalently, the set of all solutions to a system $Ax = 0$ of $m$ homogeneous linear equations in $n$ unknowns is a subspace of $R^n$. Then I proceeded to prove that the properties of a subspace hold for the null space, and hence the null space is a subspace of $R^n$. Please tell me if this is right... this problem has been on my head all day!! Thanks!","I have a problem and a proposed solution. I want to know if I have done it correctly. Problem Statement: Let $V=F^n$ be the space of column vectors. Prove that every subspace $W$ of $V$ is the space of solutions of some system of homogeneous linear equations $AX=0$. My solution: The null space of an $m$ x $n$ matrix $A$, written as $Nul A$, is the set of all solutions to the homogeneous equation $Ax = 0$. The null space of an $m$ x $n$ matrix $A$ is a subspace of $R^n$. Equivalently, the set of all solutions to a system $Ax = 0$ of $m$ homogeneous linear equations in $n$ unknowns is a subspace of $R^n$. Then I proceeded to prove that the properties of a subspace hold for the null space, and hence the null space is a subspace of $R^n$. Please tell me if this is right... this problem has been on my head all day!! Thanks!",,[]
91,Could a set of $3$ vectors in $\mathbb{R}^4$ span all of $\mathbb{R}^4$?,Could a set of  vectors in  span all of ?,3 \mathbb{R}^4 \mathbb{R}^4,Could a set of $3$ vectors in $\mathbb{R}^4$ span all of $\mathbb{R}^4$; is this the same as asking if a 4 x 3 matrix could span $\mathbb{R}^4$ or if a 3 x 4 matrix could span $\mathbb{R}^4$?,Could a set of $3$ vectors in $\mathbb{R}^4$ span all of $\mathbb{R}^4$; is this the same as asking if a 4 x 3 matrix could span $\mathbb{R}^4$ or if a 3 x 4 matrix could span $\mathbb{R}^4$?,,['linear-algebra']
92,Linear Transformations $ \mathbb R^2 \rightarrow \mathbb R^3 $,Linear Transformations, \mathbb R^2 \rightarrow \mathbb R^3 ,If $ T : \mathbb R^2 \rightarrow \mathbb R^3 $ is a linear transformation such that $ T       \begin{bmatrix} 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 0 \\ 12 \\ -2 \end{bmatrix} $ and $ T\begin{bmatrix} 2 \\ -1 \\ \end{bmatrix}  = \begin{bmatrix} 10 \\ -1 \\ 1 \end{bmatrix}  $ then the standard Matrix $A = ?$ This is where I get stuck with linear transformations and don't know how to do this type of operation. Can anyone help me get started ?,If $ T : \mathbb R^2 \rightarrow \mathbb R^3 $ is a linear transformation such that $ T       \begin{bmatrix} 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 0 \\ 12 \\ -2 \end{bmatrix} $ and $ T\begin{bmatrix} 2 \\ -1 \\ \end{bmatrix}  = \begin{bmatrix} 10 \\ -1 \\ 1 \end{bmatrix}  $ then the standard Matrix $A = ?$ This is where I get stuck with linear transformations and don't know how to do this type of operation. Can anyone help me get started ?,,"['linear-algebra', 'matrices', 'vector-spaces']"
93,How to prove exponential of every square matrix is invertible?,How to prove exponential of every square matrix is invertible?,,For a square matrix $A$. Define $exp(A)=I+\sum_{n}A^{n}/(n!)$ . I need to prove two things exp(A) converges and is invertible. Its inverse is given by exp(-A). Second part is straightforward. Can anyone help on first part?,For a square matrix $A$. Define $exp(A)=I+\sum_{n}A^{n}/(n!)$ . I need to prove two things exp(A) converges and is invertible. Its inverse is given by exp(-A). Second part is straightforward. Can anyone help on first part?,,"['linear-algebra', 'matrices']"
94,Matrices with elements that are a distinct set of prime numbers: always invertible?,Matrices with elements that are a distinct set of prime numbers: always invertible?,,"Inspired by a previous question , given a square non-symmetric matrix whose elements are all prime but distinct from each other, does this guarantee that the matrix is invertible? It's easy to see $N=2$ this holds, a counter-example would imply that there must be four distinct primes such that $p_1 p_2 = p_3 p_4$.","Inspired by a previous question , given a square non-symmetric matrix whose elements are all prime but distinct from each other, does this guarantee that the matrix is invertible? It's easy to see $N=2$ this holds, a counter-example would imply that there must be four distinct primes such that $p_1 p_2 = p_3 p_4$.",,"['linear-algebra', 'matrices', 'prime-numbers']"
95,Defining a linear map via kernel and image.,Defining a linear map via kernel and image.,,"Are linear maps defined in a 1-1 manner by setting their kernel and image? In other words, If I have a vector space, and I define a set to be the kernel of my would-be linear map, and another set to be it's image. Would I get a well defined, one linear map? (Given that my Ker and Im are okay, e.g. the dimensions are fine, etc.) I have the following task: Let T be a linear map in R^4. Given that  orthogonal_space_of(k e rT) = { (1, 2, 0, 4) , (  1, 0,1, 0) }, and T(1,0,1,1)=(1,2,1,1), give an example of such T (""you don't have to explicitly find T(x1,x2,x3,x4)."") Any ideas on how to do that (how can I characterize a linear map T without finding an explicit formula for it?) I thought about finding it's Ker and Im, but that doesn't work. Any thoughts?","Are linear maps defined in a 1-1 manner by setting their kernel and image? In other words, If I have a vector space, and I define a set to be the kernel of my would-be linear map, and another set to be it's image. Would I get a well defined, one linear map? (Given that my Ker and Im are okay, e.g. the dimensions are fine, etc.) I have the following task: Let T be a linear map in R^4. Given that  orthogonal_space_of(k e rT) = { (1, 2, 0, 4) , (  1, 0,1, 0) }, and T(1,0,1,1)=(1,2,1,1), give an example of such T (""you don't have to explicitly find T(x1,x2,x3,x4)."") Any ideas on how to do that (how can I characterize a linear map T without finding an explicit formula for it?) I thought about finding it's Ker and Im, but that doesn't work. Any thoughts?",,['linear-algebra']
96,Divide by a vector?,Divide by a vector?,,"When doing matrix multiplication can I carry a vector to the other side? For example if I have: $Ab = c$ where A is m by m invertable matrix, and b is m by 1 col vector, c m by 1. Can I do something like this: $A = c/b$ And what does that mean... I just need to find matrix A, as I have b and c vectors. P.S. Also, I know that inverse(A) is diagonal. If it helps.","When doing matrix multiplication can I carry a vector to the other side? For example if I have: $Ab = c$ where A is m by m invertable matrix, and b is m by 1 col vector, c m by 1. Can I do something like this: $A = c/b$ And what does that mean... I just need to find matrix A, as I have b and c vectors. P.S. Also, I know that inverse(A) is diagonal. If it helps.",,['linear-algebra']
97,Understanding direct sum of matrices,Understanding direct sum of matrices,,"I read the definition of direct sum on wikipedia , and got the idea that a direct sum of two matrices is a block diagonal matrix. However this does not help me understand this statement in a book. In the book I am reading, the matrix $$ \begin{pmatrix} 0&0&0&1 \\ 0&0&1&0 \\ 0&1&0&0 \\ 1&0&0&0 \end{pmatrix} $$ ""can be regarded as the direct sum of two submatrices"": $$ \begin{pmatrix} 0&1 \\ 1&0  \end{pmatrix},\begin{pmatrix} 0&1 \\ 1&0  \end{pmatrix}$$ Where onen lies in the first and fourth rows (columns) and the other in the second and third. According to the definition it should be $$ \begin{pmatrix} 0&1&0&0 \\ 1&0&0&0 \\ 0&0&0&1 \\ 0&0&1&0 \end{pmatrix} $$ This was taken from a problem in Problems and Solutions in Group Theory for Physicists by Zhong-Qi Ma and Xiao-Yan Gu. Here's the problem and the solution in full. Problem 3. Calculate the eigenvalues and eigenvectors of the matrix $R$   $$ R =  \begin{pmatrix} 0&0&0&1 \\ 0&0&1&0 \\ 0&1&0&0 \\ 1&0&0&0 \end{pmatrix}. $$ Solution. $R$ can be regarded as the direct sum of the two submatrices $\sigma_1$, one lies in the first and fourth rows(columns), the other in the second and third rows(columns). From the result of Problem 2, two eigenvalues of $R$ are $1$, the remaining two are $-1$. The relative eigenvalues are as follows:   $$ 1:   \begin{pmatrix} 1\\ 0 \\ 0 \\ 1  \end{pmatrix}, \begin{pmatrix} 0\\ 1 \\ 1 \\ 0  \end{pmatrix}, \ \ \ \ \ \  -1: \begin{pmatrix} 1\\ 0 \\ 0 \\ -1  \end{pmatrix}, \begin{pmatrix} 0\\ 1 \\ -1 \\ 0  \end{pmatrix}. $$ Problem 2 refers to an earlier problem that calculates the eigenvalues and eigenvectors of the matrix  $$ \sigma_1= \begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}.  $$ [Edit by SN:] Added the full problem text.","I read the definition of direct sum on wikipedia , and got the idea that a direct sum of two matrices is a block diagonal matrix. However this does not help me understand this statement in a book. In the book I am reading, the matrix $$ \begin{pmatrix} 0&0&0&1 \\ 0&0&1&0 \\ 0&1&0&0 \\ 1&0&0&0 \end{pmatrix} $$ ""can be regarded as the direct sum of two submatrices"": $$ \begin{pmatrix} 0&1 \\ 1&0  \end{pmatrix},\begin{pmatrix} 0&1 \\ 1&0  \end{pmatrix}$$ Where onen lies in the first and fourth rows (columns) and the other in the second and third. According to the definition it should be $$ \begin{pmatrix} 0&1&0&0 \\ 1&0&0&0 \\ 0&0&0&1 \\ 0&0&1&0 \end{pmatrix} $$ This was taken from a problem in Problems and Solutions in Group Theory for Physicists by Zhong-Qi Ma and Xiao-Yan Gu. Here's the problem and the solution in full. Problem 3. Calculate the eigenvalues and eigenvectors of the matrix $R$   $$ R =  \begin{pmatrix} 0&0&0&1 \\ 0&0&1&0 \\ 0&1&0&0 \\ 1&0&0&0 \end{pmatrix}. $$ Solution. $R$ can be regarded as the direct sum of the two submatrices $\sigma_1$, one lies in the first and fourth rows(columns), the other in the second and third rows(columns). From the result of Problem 2, two eigenvalues of $R$ are $1$, the remaining two are $-1$. The relative eigenvalues are as follows:   $$ 1:   \begin{pmatrix} 1\\ 0 \\ 0 \\ 1  \end{pmatrix}, \begin{pmatrix} 0\\ 1 \\ 1 \\ 0  \end{pmatrix}, \ \ \ \ \ \  -1: \begin{pmatrix} 1\\ 0 \\ 0 \\ -1  \end{pmatrix}, \begin{pmatrix} 0\\ 1 \\ -1 \\ 0  \end{pmatrix}. $$ Problem 2 refers to an earlier problem that calculates the eigenvalues and eigenvectors of the matrix  $$ \sigma_1= \begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}.  $$ [Edit by SN:] Added the full problem text.",,"['linear-algebra', 'matrices']"
98,What is happening in a linear algebra computation?,What is happening in a linear algebra computation?,,"About a year ago I took a Linear Algebra class that was required for my degree.  Unfortunately that class had an unidentified pre-requisite and started at a much higher level then I really needed.  Going in I had no prior experience with linear algebra.  I can definitely see how understanding linear algebra would be a very good thing to have in my field so I've been trying to piece it together ever since and have felt like I'm close but I just don't quite get it.  I understand that linear algebra is a way to solve a lot of equations rapidly... In my mind this seems like that means finding values for the variables... but it didn't seem like we ever did... Instead we were doing things like multiplication of matrices and that made no sense.  Or we would apply advance algorithms to get the matrix into certain forms which the reason for never made sense to me. So what does it mean to solve a system of equations?  What are some real world examples that might make understanding linear algebra easier?  Why are orthogonal and other types of matrices so special?  Any insights, examples, suggestions are greatly appreciated!","About a year ago I took a Linear Algebra class that was required for my degree.  Unfortunately that class had an unidentified pre-requisite and started at a much higher level then I really needed.  Going in I had no prior experience with linear algebra.  I can definitely see how understanding linear algebra would be a very good thing to have in my field so I've been trying to piece it together ever since and have felt like I'm close but I just don't quite get it.  I understand that linear algebra is a way to solve a lot of equations rapidly... In my mind this seems like that means finding values for the variables... but it didn't seem like we ever did... Instead we were doing things like multiplication of matrices and that made no sense.  Or we would apply advance algorithms to get the matrix into certain forms which the reason for never made sense to me. So what does it mean to solve a system of equations?  What are some real world examples that might make understanding linear algebra easier?  Why are orthogonal and other types of matrices so special?  Any insights, examples, suggestions are greatly appreciated!",,['linear-algebra']
99,"Transformation T is... ""onto""?","Transformation T is... ""onto""?",,"I thought you have to say a mapping is onto something ... like, you don't say, ""the book is on the top of""... Our book starts out by saying ""a mapping is said to be onto R^m"", but thereafter, it just says ""the mapping is onto"", without saying onto what . Is that simply the author's version of being too lazy to write the codomain (sorry for saying something negative, but that's what it looks like to me at the moment), or does it have a different meaning?","I thought you have to say a mapping is onto something ... like, you don't say, ""the book is on the top of""... Our book starts out by saying ""a mapping is said to be onto R^m"", but thereafter, it just says ""the mapping is onto"", without saying onto what . Is that simply the author's version of being too lazy to write the codomain (sorry for saying something negative, but that's what it looks like to me at the moment), or does it have a different meaning?",,"['linear-algebra', 'transformation']"
