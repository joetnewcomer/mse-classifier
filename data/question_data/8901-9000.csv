,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Lebesgue measure, Borel sets and Axiom of choice","Lebesgue measure, Borel sets and Axiom of choice",,"I cannot proceed my study on measure theory since it seems my measure theory is really unstable. I desperately need someone to briefly answer below 3 questions... **For convenience, i will write Lebesgue Measurable set to mean the usual Lebesgue Measurable set, and write Codable-Lebesgue Measurable set to mean the Lebesgue Measurable set defined by Codable-Borel sets. $1$. It is well-known that ""Existence of Non-Lebesgue measurable set is unprovable in ZF"". WHAT Lebesgue measurable set? My definition for Lebesgue measurable set is defined by using Riesz Representation Theorem (Whose existence is guranteed by Axiom of Countable Choice, hence it is undefinable without choice). However, I heard that the usual construction of Lebesgue measure is by using Caratheodory's existence theorem. In that way, can Lebesgue measurable set be definable without Axiom of Choice? That is, it really doesn't make sense to me, 'existence of non-Lebesgue Measurable set is unprovable in ZF' since Lebesgue measure cannot be defined without choice (in my way of construction of Lebesgue measure). $2$. Can ""Existence of a set that is Lebesgue measurable but non-Borel"" be proved without choice? I saw the standard example (Luzin's example using continued fraction), but i'm not sure what Lebesgue measurable set stated there. Moreover, if it is not true in ZF, what about ""Existence of a set that is Codable-Lebesgue Measurable but non-Borel""? $3$. (This is closely related to 1) Why Lebesgue measure is unique? Assuming Axiom of Choice, it is a theorem, that ""If $\mu_1$ and $\mu_2$ are translation-invariant measures on sigma algebras $\mathfrak{M}_1$ and $\mathfrak{M}_2$ repectively, containing all Borel sets, and $\mu_1(K), \mu_2(K) <\infty$ for every compact set $K$, then there exists a constant $c$ such that $\mu_1(E)=c\mu_2(E)$ for all the borel sets $E$"". You can see that this theorem does not imply that $\mathfrak{M}_1=\mathfrak{M}_2$ in the hypothesis. Thank you in advance.","I cannot proceed my study on measure theory since it seems my measure theory is really unstable. I desperately need someone to briefly answer below 3 questions... **For convenience, i will write Lebesgue Measurable set to mean the usual Lebesgue Measurable set, and write Codable-Lebesgue Measurable set to mean the Lebesgue Measurable set defined by Codable-Borel sets. $1$. It is well-known that ""Existence of Non-Lebesgue measurable set is unprovable in ZF"". WHAT Lebesgue measurable set? My definition for Lebesgue measurable set is defined by using Riesz Representation Theorem (Whose existence is guranteed by Axiom of Countable Choice, hence it is undefinable without choice). However, I heard that the usual construction of Lebesgue measure is by using Caratheodory's existence theorem. In that way, can Lebesgue measurable set be definable without Axiom of Choice? That is, it really doesn't make sense to me, 'existence of non-Lebesgue Measurable set is unprovable in ZF' since Lebesgue measure cannot be defined without choice (in my way of construction of Lebesgue measure). $2$. Can ""Existence of a set that is Lebesgue measurable but non-Borel"" be proved without choice? I saw the standard example (Luzin's example using continued fraction), but i'm not sure what Lebesgue measurable set stated there. Moreover, if it is not true in ZF, what about ""Existence of a set that is Codable-Lebesgue Measurable but non-Borel""? $3$. (This is closely related to 1) Why Lebesgue measure is unique? Assuming Axiom of Choice, it is a theorem, that ""If $\mu_1$ and $\mu_2$ are translation-invariant measures on sigma algebras $\mathfrak{M}_1$ and $\mathfrak{M}_2$ repectively, containing all Borel sets, and $\mu_1(K), \mu_2(K) <\infty$ for every compact set $K$, then there exists a constant $c$ such that $\mu_1(E)=c\mu_2(E)$ for all the borel sets $E$"". You can see that this theorem does not imply that $\mathfrak{M}_1=\mathfrak{M}_2$ in the hypothesis. Thank you in advance.",,"['real-analysis', 'measure-theory', 'axiom-of-choice']"
1,The k-th difference of the sequence $n^{k}$ is constant and equal to $k!$,The k-th difference of the sequence  is constant and equal to,n^{k} k!,"Define the k-th difference of a sequence $\{a_n\}$ inductively as follows: The $1$-th difference is the sequence $\{b_n\}$ given by $b_n=a_{n+1}-a_n$ The ""$k+1$""-th difference is the sequence $\{b_n\}$ given by $b_n=c_{n+1}-c_n$, where $\{c_n\}$ is the $k$-th difference of the sequence $\{a_n\}$. Prove that, given the sequence $\{a_n\}$ such that $a_n=n^k$, for a fixed $k \in \mathbb{N}$, the $k$-th difference of this $\{a_n\}$ is a constant sequence and equal to $k!$","Define the k-th difference of a sequence $\{a_n\}$ inductively as follows: The $1$-th difference is the sequence $\{b_n\}$ given by $b_n=a_{n+1}-a_n$ The ""$k+1$""-th difference is the sequence $\{b_n\}$ given by $b_n=c_{n+1}-c_n$, where $\{c_n\}$ is the $k$-th difference of the sequence $\{a_n\}$. Prove that, given the sequence $\{a_n\}$ such that $a_n=n^k$, for a fixed $k \in \mathbb{N}$, the $k$-th difference of this $\{a_n\}$ is a constant sequence and equal to $k!$",,"['real-analysis', 'sequences-and-series', 'recurrence-relations', 'finite-differences']"
2,Is the Space of bounded functions with the maximums norm a Banach space and even a Banach Algebra?,Is the Space of bounded functions with the maximums norm a Banach space and even a Banach Algebra?,,"X is a arbitrary non empty set , B(X) the set of bounded functions $f:X\rightarrow \mathbb{R}$ and $||f||_\infty = \sup_{x\in X} |f(x)|$ Completeness: Let $(f_n(x))_{n \in \mathbb{N}}$ be a cauchy sequence, then: $$||f_n-f_m||\le \frac{\epsilon}{2} \ \text{for n,m greater than some N}$$ the cauchy sequence $f_n$ will have a limit $f(x)$ for $x \in \mathbb{R}$, so there must be a $f_{n_k}$ with a $n_k > N$ such that : $|f_{n_k}(x)-f(x)|\le \frac{\epsilon}{2}$ so one can put: $$|f_n(x)-f(x)| \le ||f_n(x)-f_{n_k}(x)||+ |f_{n_k}(x)-f(x)| \le \epsilon $$ And for every $x\in \mathbb{R}$: $$|f(x)|\le |f_{n_k}(x)|+|f_n(x)-f(x)|\le ||f_{n_k}(x)||+\epsilon < \infty$$ Is this sufficient to say that it was shown that $(B(X), ||.||_\infty )$ is a Banach space? Is it also a Banach Algebra?","X is a arbitrary non empty set , B(X) the set of bounded functions $f:X\rightarrow \mathbb{R}$ and $||f||_\infty = \sup_{x\in X} |f(x)|$ Completeness: Let $(f_n(x))_{n \in \mathbb{N}}$ be a cauchy sequence, then: $$||f_n-f_m||\le \frac{\epsilon}{2} \ \text{for n,m greater than some N}$$ the cauchy sequence $f_n$ will have a limit $f(x)$ for $x \in \mathbb{R}$, so there must be a $f_{n_k}$ with a $n_k > N$ such that : $|f_{n_k}(x)-f(x)|\le \frac{\epsilon}{2}$ so one can put: $$|f_n(x)-f(x)| \le ||f_n(x)-f_{n_k}(x)||+ |f_{n_k}(x)-f(x)| \le \epsilon $$ And for every $x\in \mathbb{R}$: $$|f(x)|\le |f_{n_k}(x)|+|f_n(x)-f(x)|\le ||f_{n_k}(x)||+\epsilon < \infty$$ Is this sufficient to say that it was shown that $(B(X), ||.||_\infty )$ is a Banach space? Is it also a Banach Algebra?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
3,"Is my solution correct to prove that $f (x) = 0$ for all $x \in [a, b]$? [duplicate]",Is my solution correct to prove that  for all ? [duplicate],"f (x) = 0 x \in [a, b]","This question already has answers here : Closed 11 years ago . Possible Duplicate: $f\geq 0$ and $\int_a^b f=0$ implies $f=0$ everywhere on $[a,b]$ Is this a correct solution? Thanks for your help","This question already has answers here : Closed 11 years ago . Possible Duplicate: $f\geq 0$ and $\int_a^b f=0$ implies $f=0$ everywhere on $[a,b]$ Is this a correct solution? Thanks for your help",,['real-analysis']
4,Does $\int_0^\infty \frac{x\arctan x}{\sqrt[3]{1+x^4}}dx$ converge?,Does  converge?,\int_0^\infty \frac{x\arctan x}{\sqrt[3]{1+x^4}}dx,"I have to determine whether $$\int_0^\infty \frac{x\arctan x}{\sqrt[3]{1+x^4}}dx$$ converges or not. I suspect it doesn't because $\arctan x$ is very close to $\pi/2$ as $x$ goes to infinity, and $$\int_0^\infty \frac{x}{\sqrt[3]{1+x^4}}dx=\infty$$ because, for example, $$\frac{x}{\sqrt[3]{1+x^4}}>\frac{x}{\sqrt[3]{2x^4}}=(2x)^{-1/3}$$ for sufficiently large $x$. But I don't know how to bound $\arctan x$ from below for this to work. I also suspect that there is something more refined than the comparison test to this problem. I don't really know the context in which this integral came up, because I don't attend the course, so I don't know what methods I should try.","I have to determine whether $$\int_0^\infty \frac{x\arctan x}{\sqrt[3]{1+x^4}}dx$$ converges or not. I suspect it doesn't because $\arctan x$ is very close to $\pi/2$ as $x$ goes to infinity, and $$\int_0^\infty \frac{x}{\sqrt[3]{1+x^4}}dx=\infty$$ because, for example, $$\frac{x}{\sqrt[3]{1+x^4}}>\frac{x}{\sqrt[3]{2x^4}}=(2x)^{-1/3}$$ for sufficiently large $x$. But I don't know how to bound $\arctan x$ from below for this to work. I also suspect that there is something more refined than the comparison test to this problem. I don't really know the context in which this integral came up, because I don't attend the course, so I don't know what methods I should try.",,"['real-analysis', 'improper-integrals']"
5,How to maximize this integral?,How to maximize this integral?,,"Rudin asked me to maximize $$\int^{1}_{-1}x^{3}g(x)dx$$ under the restraint that $$\int^{1}_{-1}g(x)=\int^{1}_{-1}xg(x)dx=\int^{1}_{-1}x^{2}g(x)dx=0$$ This is clearly a Hilbert space problem need to use orthogonal relations. I computed $x^{3}$'s coefficients under the $L^{2}$ inner product and it turns out $x^{3}=\frac{3}{5}x^{2}+c$, with $c$ being orthogonal to $\{1,x,x^{2}\}$. But how does this help to find $g$? A related question I also do not know how to solve is to find the minimum of $$\int^{\infty}_{0}|x^{3}-a-bx-cx^{2}|^{2}e^{-x}dx$$And it is not clear to me what the linearly independent underlying set is - $\{1,x,x^{2},e^{-x/2}\}$?","Rudin asked me to maximize $$\int^{1}_{-1}x^{3}g(x)dx$$ under the restraint that $$\int^{1}_{-1}g(x)=\int^{1}_{-1}xg(x)dx=\int^{1}_{-1}x^{2}g(x)dx=0$$ This is clearly a Hilbert space problem need to use orthogonal relations. I computed $x^{3}$'s coefficients under the $L^{2}$ inner product and it turns out $x^{3}=\frac{3}{5}x^{2}+c$, with $c$ being orthogonal to $\{1,x,x^{2}\}$. But how does this help to find $g$? A related question I also do not know how to solve is to find the minimum of $$\int^{\infty}_{0}|x^{3}-a-bx-cx^{2}|^{2}e^{-x}dx$$And it is not clear to me what the linearly independent underlying set is - $\{1,x,x^{2},e^{-x/2}\}$?",,['real-analysis']
6,Definition of $\limsup$ and $\liminf$ for functions,Definition of  and  for functions,\limsup \liminf,"In my notes, I have written that \begin{align} \limsup_{x\to a} f(x) & = \inf_{\delta\to0}\sup\left[f(x): \Vert x-a \Vert < \delta \right] \\  & = \inf_{n\to \infty}\sup\left[ f(x): \Vert x-a \Vert <\frac{1}{n} \right] \\  & = \lim_{n\to \infty}\sup\left[ f(x): \Vert x-a\Vert < \frac{1}{n} \right] \end{align} Can someone help me understand the first line? I thought that the supremum is always a number, so why we are taking the infimum of a number? Also I tried drawing a picture to illsute what the heck is going on. But basically my gist is that given an interval around $a$, as $\delta \to 0$, that supremum over that interval is the limit supremum","In my notes, I have written that \begin{align} \limsup_{x\to a} f(x) & = \inf_{\delta\to0}\sup\left[f(x): \Vert x-a \Vert < \delta \right] \\  & = \inf_{n\to \infty}\sup\left[ f(x): \Vert x-a \Vert <\frac{1}{n} \right] \\  & = \lim_{n\to \infty}\sup\left[ f(x): \Vert x-a\Vert < \frac{1}{n} \right] \end{align} Can someone help me understand the first line? I thought that the supremum is always a number, so why we are taking the infimum of a number? Also I tried drawing a picture to illsute what the heck is going on. But basically my gist is that given an interval around $a$, as $\delta \to 0$, that supremum over that interval is the limit supremum",,['real-analysis']
7,Riemann integrable function,Riemann integrable function,,"Show that $f(x)$ defined by $f(x)=\frac{1}{n}$, if $\frac{1}{n+1}<x<\frac{1}{n}, n=1,2,3,...$ and $f(0)=0$ is Riemann integrable on $[0,1]$. Also show that $$\int_0^1f(x) dx=\frac{\pi ^2}{6}-1$$ I know that I can use $\displaystyle \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6} $ to solve this. But I can't get the required form.","Show that $f(x)$ defined by $f(x)=\frac{1}{n}$, if $\frac{1}{n+1}<x<\frac{1}{n}, n=1,2,3,...$ and $f(0)=0$ is Riemann integrable on $[0,1]$. Also show that $$\int_0^1f(x) dx=\frac{\pi ^2}{6}-1$$ I know that I can use $\displaystyle \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6} $ to solve this. But I can't get the required form.",,"['real-analysis', 'integration']"
8,Absolute continuity,Absolute continuity,,"$f$ is continuous and of bounded variation on $[0,1]$,    $f$ is absolutely continuous on any $[c,1]$ with $c \in (0,1]$.    Then $f$ is absolutely continuous on $[0,1]$. How to show this? Thanks.","$f$ is continuous and of bounded variation on $[0,1]$,    $f$ is absolutely continuous on any $[c,1]$ with $c \in (0,1]$.    Then $f$ is absolutely continuous on $[0,1]$. How to show this? Thanks.",,"['real-analysis', 'bounded-variation']"
9,"$f,g$ continuous functions, $f$ strictly increasing, non-vanishing: $\int_a^bg(x)(f(x))^n\,dx=0$ for all $n$ $\Rightarrow g\equiv 0$","continuous functions,  strictly increasing, non-vanishing:  for all","f,g f \int_a^bg(x)(f(x))^n\,dx=0 n \Rightarrow g\equiv 0","While studying for an exam, I came across the following problem. Suppose $[a,b]\subset\mathbb{R}$ and $f:[a,b]\rightarrow\mathbb{R}$ is a strictly increasing non-vanishing continuous function. Suppose $g:[a,b]\rightarrow\mathbb{R}$ is a continuous function which is orthogonal to all powers of $f$, i.e. is such that $\int_a^bg(x)(f(x))^n\,dx=0$ (Riemann integrals) for $n=1,2,3,\ldots$. Show that $g(x)=0$ for all $x\in[a,b]$. Since I could not think of how to solve this problem, I considered something simpler. I know that if $f(x)=x$, then we have $\int_a^bg(x)x^n\,dx=0$ for all $n$. Applying the Weierstrass Approximation Theorem to $g$, we obtain a sequence of polynomials $(p_n)$ such that $p_n\rightarrow g$ uniformly. Therefore, by linearity of the integral, we have $$\int_a^bg(x)p_n(x)\,dx=0$$ for all $n$. Thus $$\lim_{n\rightarrow\infty}\int_a^bg(x)p_n(x)\,dx=\int_a^b(g(x))^2\,dx=0.$$ Since $g$ is continuous, the result follows. But, obviously, this is not an answer to the stated question... Any help would be greatly appreciated. Thanks in advance.","While studying for an exam, I came across the following problem. Suppose $[a,b]\subset\mathbb{R}$ and $f:[a,b]\rightarrow\mathbb{R}$ is a strictly increasing non-vanishing continuous function. Suppose $g:[a,b]\rightarrow\mathbb{R}$ is a continuous function which is orthogonal to all powers of $f$, i.e. is such that $\int_a^bg(x)(f(x))^n\,dx=0$ (Riemann integrals) for $n=1,2,3,\ldots$. Show that $g(x)=0$ for all $x\in[a,b]$. Since I could not think of how to solve this problem, I considered something simpler. I know that if $f(x)=x$, then we have $\int_a^bg(x)x^n\,dx=0$ for all $n$. Applying the Weierstrass Approximation Theorem to $g$, we obtain a sequence of polynomials $(p_n)$ such that $p_n\rightarrow g$ uniformly. Therefore, by linearity of the integral, we have $$\int_a^bg(x)p_n(x)\,dx=0$$ for all $n$. Thus $$\lim_{n\rightarrow\infty}\int_a^bg(x)p_n(x)\,dx=\int_a^b(g(x))^2\,dx=0.$$ Since $g$ is continuous, the result follows. But, obviously, this is not an answer to the stated question... Any help would be greatly appreciated. Thanks in advance.",,['real-analysis']
10,"Given a directed set, how do I construct a net with positive values that converges to $0$.","Given a directed set, how do I construct a net with positive values that converges to .",0,"Suppose I have a directed set in which there is no maximal or equivalently maximum element.  Is there a way to construct a net based on that directed set, of strictly positive real numbers that converges to $0$ ?  This would be useful in arguments where for sequences one uses $1/n$ .","Suppose I have a directed set in which there is no maximal or equivalently maximum element.  Is there a way to construct a net based on that directed set, of strictly positive real numbers that converges to ?  This would be useful in arguments where for sequences one uses .",0 1/n,"['real-analysis', 'general-topology', 'analysis', 'nets']"
11,How to prove a function is the Fourier transform of another $L^{1}$ function?,How to prove a function is the Fourier transform of another  function?,L^{1},"If $m(\xi)$ satisfies $$D^{\alpha}m(\xi)\leq \frac{C}{(1+|\xi|)^{|\alpha|+1}}$$ then is $m$ a Fourier transform of a $L^{1}$ function? (Note that the Bernstein theorem can't be applied here, since $m(\xi)$ may not be in $H^{s}$, where $s>\frac{n}{2}$.) Generally, are there some simple ways to make sure that a given function belongs to $\mathcal{F}L^{1}$?","If $m(\xi)$ satisfies $$D^{\alpha}m(\xi)\leq \frac{C}{(1+|\xi|)^{|\alpha|+1}}$$ then is $m$ a Fourier transform of a $L^{1}$ function? (Note that the Bernstein theorem can't be applied here, since $m(\xi)$ may not be in $H^{s}$, where $s>\frac{n}{2}$.) Generally, are there some simple ways to make sure that a given function belongs to $\mathcal{F}L^{1}$?",,"['real-analysis', 'fourier-analysis']"
12,a question about an infinite series calculation.,a question about an infinite series calculation.,,"I want to prove that for $y >0$, $ x \in \mathbb R$,  $$ \sum_{n=-\infty}^\infty \frac{y}{(x+n)^2 + y^2} = \frac{1}{2} \frac{1 - e^{-4 \pi y }}{1 - 2 e^{-2 \pi y} \cos ( 2 \pi  x ) + e^{-4 \pi y}}$$","I want to prove that for $y >0$, $ x \in \mathbb R$,  $$ \sum_{n=-\infty}^\infty \frac{y}{(x+n)^2 + y^2} = \frac{1}{2} \frac{1 - e^{-4 \pi y }}{1 - 2 e^{-2 \pi y} \cos ( 2 \pi  x ) + e^{-4 \pi y}}$$",,"['real-analysis', 'sequences-and-series', 'complex-analysis']"
13,$\{f_n\}$ continuous such that $\inf f_n$ and $\sup f_n$ are not continuous,continuous such that  and  are not continuous,\{f_n\} \inf f_n \sup f_n,"Find a sequence of continuous functions on $[0,1]$, $\{f_n\}$, such that $f(x):=\sup\{f_n(x):n\geq 1\}$ and $g(x):=\inf\{f_n(x):n\geq 1\}$ are both not continuous. I kept finding examples where one was discontinuous but the other was continuous like $f_n(x)=|x|^\frac{1}{n}$ and $f_n(x)=x^n$","Find a sequence of continuous functions on $[0,1]$, $\{f_n\}$, such that $f(x):=\sup\{f_n(x):n\geq 1\}$ and $g(x):=\inf\{f_n(x):n\geq 1\}$ are both not continuous. I kept finding examples where one was discontinuous but the other was continuous like $f_n(x)=|x|^\frac{1}{n}$ and $f_n(x)=x^n$",,['real-analysis']
14,Prove the equation has a root.,Prove the equation has a root.,,"Assume that $f$ is a bounded and differentiable function in $(0,1)$. If $f({1\over 2})=0$, prove that the equation, $$2f(x)+xf'(x)=0,$$ has at least one root in $(0,{{1}\over{2}})$. I tried to do it using Rolle's Theorem. Because the left side of the equation looks like the derivative of some function. And if I find a function $F$ s.t. $F'(x)=2f(x)+xf(x)$ and $F(0)=F({1\over 2})$, then I can use Rolle's Theorem to get the conclusion. I've found $F$, which is  $$F(x)=\int_{0}^{x}f(t)\,dt+xf(x),$$ s.t. $F'(x)=2f(x)+xf(x)$, but the only problem is that I can't ask for $F(0)=F({1\over 2})$. Can somebody give me a hint about this problem?","Assume that $f$ is a bounded and differentiable function in $(0,1)$. If $f({1\over 2})=0$, prove that the equation, $$2f(x)+xf'(x)=0,$$ has at least one root in $(0,{{1}\over{2}})$. I tried to do it using Rolle's Theorem. Because the left side of the equation looks like the derivative of some function. And if I find a function $F$ s.t. $F'(x)=2f(x)+xf(x)$ and $F(0)=F({1\over 2})$, then I can use Rolle's Theorem to get the conclusion. I've found $F$, which is  $$F(x)=\int_{0}^{x}f(t)\,dt+xf(x),$$ s.t. $F'(x)=2f(x)+xf(x)$, but the only problem is that I can't ask for $F(0)=F({1\over 2})$. Can somebody give me a hint about this problem?",,"['real-analysis', 'roots']"
15,Isometry in $\mathbb{R}^n$,Isometry in,\mathbb{R}^n,"I'm trying to prove that if $f\colon\mathbb{R}^n \to \mathbb{R}^n$  is a $\mathcal{C}^1$ mapping such that $f'(x)$ is a (linear) isometry for every $x \in \mathbb{R}^n$, then $f$ is an isometry. By an application of inverse mapping theorem and mean value theorem, we have that $|f(x) - f(y)| = |x-y|$ as long as $x$ and $y$ are sufficiently close. How to extend this to the whole space?","I'm trying to prove that if $f\colon\mathbb{R}^n \to \mathbb{R}^n$  is a $\mathcal{C}^1$ mapping such that $f'(x)$ is a (linear) isometry for every $x \in \mathbb{R}^n$, then $f$ is an isometry. By an application of inverse mapping theorem and mean value theorem, we have that $|f(x) - f(y)| = |x-y|$ as long as $x$ and $y$ are sufficiently close. How to extend this to the whole space?",,['real-analysis']
16,Open Balls in Metric Space.,Open Balls in Metric Space.,,"I'm working with the metric space $(\mathbb{N}, \rho)$ where $\mathbb{N}$ is the set of natural numbers and $\rho(x,y) = |\frac{1}{x} - \frac{1}{y}|$. I'm considering the open balls on this metric. Are there any that are finite? Infinite? All of $\mathbb{N}$? My hunch is that there are open balls that are finite and infinite. For example, the open ball $B(1, \frac{1}{2})$ seems to be just {$1$}. But if we make the radius larger than $1$ doesn't the open ball becoming infinite? Am I correct? Are there any open balls that are finite? Infinite? All of $\mathbb{N}$? Any other general statements we can make about the open balls?","I'm working with the metric space $(\mathbb{N}, \rho)$ where $\mathbb{N}$ is the set of natural numbers and $\rho(x,y) = |\frac{1}{x} - \frac{1}{y}|$. I'm considering the open balls on this metric. Are there any that are finite? Infinite? All of $\mathbb{N}$? My hunch is that there are open balls that are finite and infinite. For example, the open ball $B(1, \frac{1}{2})$ seems to be just {$1$}. But if we make the radius larger than $1$ doesn't the open ball becoming infinite? Am I correct? Are there any open balls that are finite? Infinite? All of $\mathbb{N}$? Any other general statements we can make about the open balls?",,"['real-analysis', 'metric-spaces']"
17,weak convergence implies boundedness.,weak convergence implies boundedness.,,"I have these in books without proof, mostly as a corollary. I was wondering if I could get a proof. Suppose $$\lim_{n\to \infty} \int_0^1 f_ng dx = \int_0^1 fg dx$$ for all $g\in L^2(0,1)$,  where $f_n, f \in L^2(0,1)$.  Then there exists a constant $K$ such that $\|f_n\|_{L^2} \leq K \lt \infty$ for all $n$.","I have these in books without proof, mostly as a corollary. I was wondering if I could get a proof. Suppose $$\lim_{n\to \infty} \int_0^1 f_ng dx = \int_0^1 fg dx$$ for all $g\in L^2(0,1)$,  where $f_n, f \in L^2(0,1)$.  Then there exists a constant $K$ such that $\|f_n\|_{L^2} \leq K \lt \infty$ for all $n$.",,['real-analysis']
18,Weak convergence in $L^2$ and uniform covergence,Weak convergence in  and uniform covergence,L^2,"I have this problem: let $f_n$ converge weakly to $f$ in $L^2[0,1]$ and let $$F_n(x)=\int_0^xf_n(t) \, \textrm{d}t,$$ $$F(x)=\int_0^xf(t) \, \textrm{d}t.$$ Then $F_n,F$ are continuous and $F_n$ converges uniformly to $F$. Writing  $$F_n(x)=\int_0^1 f_n(t) \mathbb{1}_{[0,x]} \, \textrm{d}t$$ and applying the Lebesgue dominated convergence theorem, the continuity of $F_n$ should be proved and analogously of $F$. But I don't know about the uniform convergence and how to use the weak convergence hypothesis..","I have this problem: let $f_n$ converge weakly to $f$ in $L^2[0,1]$ and let $$F_n(x)=\int_0^xf_n(t) \, \textrm{d}t,$$ $$F(x)=\int_0^xf(t) \, \textrm{d}t.$$ Then $F_n,F$ are continuous and $F_n$ converges uniformly to $F$. Writing  $$F_n(x)=\int_0^1 f_n(t) \mathbb{1}_{[0,x]} \, \textrm{d}t$$ and applying the Lebesgue dominated convergence theorem, the continuity of $F_n$ should be proved and analogously of $F$. But I don't know about the uniform convergence and how to use the weak convergence hypothesis..",,"['real-analysis', 'analysis', 'functional-analysis']"
19,Complex derivative,Complex derivative,,The derivative of a function $f(x)$ is the limit of the quotient $$\lim_{\Delta x\to 0}\frac{f(x+\Delta x)-f(x)}{\Delta x}$$ A formula defining the fractional derivative of the same function is for example: $$\frac{d^{\frac{1}{2}}}{d(x-a)^{\frac{1}{2}}}f(x)=\frac{1}{\sqrt{(\pi)}}\frac{d}{dx}\int_{a}^{x}\frac{f(\tau)}{\sqrt{(x-\tau})}d\tau$$ My question is: is it possible to define an imaginary derivative  $$\frac{d^i}{dx^{i}}f(x)$$ of the function $f(x)$? Tanks.,The derivative of a function $f(x)$ is the limit of the quotient $$\lim_{\Delta x\to 0}\frac{f(x+\Delta x)-f(x)}{\Delta x}$$ A formula defining the fractional derivative of the same function is for example: $$\frac{d^{\frac{1}{2}}}{d(x-a)^{\frac{1}{2}}}f(x)=\frac{1}{\sqrt{(\pi)}}\frac{d}{dx}\int_{a}^{x}\frac{f(\tau)}{\sqrt{(x-\tau})}d\tau$$ My question is: is it possible to define an imaginary derivative  $$\frac{d^i}{dx^{i}}f(x)$$ of the function $f(x)$? Tanks.,,"['real-analysis', 'fractional-calculus']"
20,convergence of integral vs convergence of infinite series,convergence of integral vs convergence of infinite series,,"Problem: Let $f\in C^{1}([0,\infty ))$ such that: $\int_{1}^{\infty }\left | f^{'}(x) \right |dx$ converges. The question is to prove the following: $\left ( \sum_{n=1}^{\infty }f(n) \right )$ converges $\Leftrightarrow \left ( \int_{1}^{\infty }f(x)dx \right )$ converges I don't know how to prove it. For the direction: $\Leftarrow $ I was trying to use the definition of Rieamann integrals as an infinite sum where the mesh goes to zero, and somehow try to prove that $\left ( \sum_{n=1}^{\infty }f(n) \right )$ converges. Any solution or ideas for this problem?","Problem: Let $f\in C^{1}([0,\infty ))$ such that: $\int_{1}^{\infty }\left | f^{'}(x) \right |dx$ converges. The question is to prove the following: $\left ( \sum_{n=1}^{\infty }f(n) \right )$ converges $\Leftrightarrow \left ( \int_{1}^{\infty }f(x)dx \right )$ converges I don't know how to prove it. For the direction: $\Leftarrow $ I was trying to use the definition of Rieamann integrals as an infinite sum where the mesh goes to zero, and somehow try to prove that $\left ( \sum_{n=1}^{\infty }f(n) \right )$ converges. Any solution or ideas for this problem?",,"['calculus', 'real-analysis', 'analysis']"
21,on the integration by parts infinitely many times,on the integration by parts infinitely many times,,"it's known that if $ g(x), f(x)$ are two functions ,and $f(x)$ is sufficiently differentiable , then by repeated integration by parts one gets : $$\int f(x)g(x)dx=f(x)\int g(x)dx -f^{'}(x)\int\int g(x)dx^{2}+f^{''}(x)\int \int \int g(x)dx^{3} - .... +(-1)^{n+1}f^{(n)}(x)\underbrace{\int.....\int}g(x)dx^{n+1}+(-1)^{n}\int\left[ \underbrace{\int.....\int}g(x)dx^{n+1}\right ]f^{n+1}(x)dx$$ now, if $f(x) $ is a smooth function,and none of the terms in the expansion/summation is equal to $\pm\int f(x)g(x)dx$ , one would expect the formula above to be repeatable infinitely many times . therefore : $$\lim_{n \to \infty }\int\left[ \underbrace{\int.....\int}g(x)dx^{n+1}\right ] f^{n+1}(x)dx=0$$ is a necessary but not sufficient  condition for the summation to converge . my question is , what are the conditions needed to extend the scope of the formula - to perform the IBP infinitely many times - !?!? also, are there any theorems on the multiple integrals - the ones containing $g(x)$ - besides cauchy formula for repeated integration","it's known that if $ g(x), f(x)$ are two functions ,and $f(x)$ is sufficiently differentiable , then by repeated integration by parts one gets : $$\int f(x)g(x)dx=f(x)\int g(x)dx -f^{'}(x)\int\int g(x)dx^{2}+f^{''}(x)\int \int \int g(x)dx^{3} - .... +(-1)^{n+1}f^{(n)}(x)\underbrace{\int.....\int}g(x)dx^{n+1}+(-1)^{n}\int\left[ \underbrace{\int.....\int}g(x)dx^{n+1}\right ]f^{n+1}(x)dx$$ now, if $f(x) $ is a smooth function,and none of the terms in the expansion/summation is equal to $\pm\int f(x)g(x)dx$ , one would expect the formula above to be repeatable infinitely many times . therefore : $$\lim_{n \to \infty }\int\left[ \underbrace{\int.....\int}g(x)dx^{n+1}\right ] f^{n+1}(x)dx=0$$ is a necessary but not sufficient  condition for the summation to converge . my question is , what are the conditions needed to extend the scope of the formula - to perform the IBP infinitely many times - !?!? also, are there any theorems on the multiple integrals - the ones containing $g(x)$ - besides cauchy formula for repeated integration",,"['calculus', 'real-analysis']"
22,Finding the exact value of $ \sum \frac{4n-3}{n(n^2-4)} $,Finding the exact value of, \sum \frac{4n-3}{n(n^2-4)} ,"I would like to find the exact value of the series $$\begin{align*} \sum_{n=3}^{\infty} \frac{4n-3}{n(n^2-4)} \end{align*}$$ which is certainly a telescoping series. Do you have any idea of telescopic cancelling?","I would like to find the exact value of the series $$\begin{align*} \sum_{n=3}^{\infty} \frac{4n-3}{n(n^2-4)} \end{align*}$$ which is certainly a telescoping series. Do you have any idea of telescopic cancelling?",,"['real-analysis', 'sequences-and-series']"
23,About derivatives of real function,About derivatives of real function,,"Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be a continuous function, $A \subset \mathbf{R}$ be a closed set and  $x_0, y \in \mathbf{R}$. Assume that for each $\varepsilon >0$ there exists  $\delta >0$ such that if $x \in A$ , $|x-x_0|< \delta$ then $\left|\frac{f(x)-f(x_0)}{x-x_0}-y\right|< \varepsilon$, if $x \notin A$, $|x-x_0|<\delta$ then $f'(x)$ exists and $|f'(x)-y|<\varepsilon$. How to prove that there exists $f'(x_0)$ and $f'(x_0)=y$. Thanks. P.S.  My question concerns Lemma 1 on page 66 from the paper: H. Whitney, Analytic extensions of differentiable functions , Trans. Amer. Math. Soc. 36 (1934), 63–89. (In this paper there is no proof of Lemma 1.)","Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be a continuous function, $A \subset \mathbf{R}$ be a closed set and  $x_0, y \in \mathbf{R}$. Assume that for each $\varepsilon >0$ there exists  $\delta >0$ such that if $x \in A$ , $|x-x_0|< \delta$ then $\left|\frac{f(x)-f(x_0)}{x-x_0}-y\right|< \varepsilon$, if $x \notin A$, $|x-x_0|<\delta$ then $f'(x)$ exists and $|f'(x)-y|<\varepsilon$. How to prove that there exists $f'(x_0)$ and $f'(x_0)=y$. Thanks. P.S.  My question concerns Lemma 1 on page 66 from the paper: H. Whitney, Analytic extensions of differentiable functions , Trans. Amer. Math. Soc. 36 (1934), 63–89. (In this paper there is no proof of Lemma 1.)",,['real-analysis']
24,Existence of continuous function with a compact support and nontrivial on given compact set in $\sigma$-compact space,Existence of continuous function with a compact support and nontrivial on given compact set in -compact space,\sigma,"The following fact is trivial to see: Let $X$ be a separable and locally compact metric space, then for each compact set $K\subset X$ there is a continuous function with compact support and such that $f|K=1$. Indeed, $X=\bigcup \limits_{n=1}^{\infty} U_n$, where $\{U_n\}$ is a increasing sequence of open and precompact subset of $X$ (from the Lindelöf theorem). So there is an $m\in \mathbb{N}$, such that $K\subset U_m$. Now, applying Urysohn's theorem to the sets $K$ and $X \setminus U$, we find the suitable function (with support contained in $\operatorname{cl} U_m$, with is compact). If something like that (or similar) would be true, when $X$ was a $\sigma$-compact Polish space?","The following fact is trivial to see: Let $X$ be a separable and locally compact metric space, then for each compact set $K\subset X$ there is a continuous function with compact support and such that $f|K=1$. Indeed, $X=\bigcup \limits_{n=1}^{\infty} U_n$, where $\{U_n\}$ is a increasing sequence of open and precompact subset of $X$ (from the Lindelöf theorem). So there is an $m\in \mathbb{N}$, such that $K\subset U_m$. Now, applying Urysohn's theorem to the sets $K$ and $X \setminus U$, we find the suitable function (with support contained in $\operatorname{cl} U_m$, with is compact). If something like that (or similar) would be true, when $X$ was a $\sigma$-compact Polish space?",,"['real-analysis', 'general-topology', 'metric-spaces']"
25,Do we have to use Bernoulli polynomials in the Euler-Maclaurin summation formula?,Do we have to use Bernoulli polynomials in the Euler-Maclaurin summation formula?,,"It may be that I have not picked up the proof, but I cannot see where the third condition of Bernoulli polynomials, given below, is used in the derivation of the Euler-Maclaurin summation formula. The Bernoulli polynomials are defined inductively by $$ b_0(x) = 1, $$ $$ b_n'(x) = nb_{n-1}(x) \:\text{ and }\: \int_0^1 b_n(x) = 0 \:\text{  for  }\: n \geq 1. $$ Suppose instead that we defined the alternative Bernoulli polynomials as follows, $$ a_0(x) = 1, $$ $$ a_n'(x) = na_{n-1}(x) \:\text{ and }\: a_n(0) = 0 \:\text{  for  }\: n \geq 1. $$ so that the new third condition essentially gives $a_n(x) = x^n$ for all $n$, thus simplifying matters. Then the periodic Bernoulli functions used in the proof would be $A_n(x) = a_n(\{x\})$, where $\{x\}$ denotes the fractional part of $x$. Using these functions, can anyone show me where the proof of Euler-Maclaurin breaks down? (Assume we are using the elementary proof by induction that is explained in detail on Wikipedia , for example.)","It may be that I have not picked up the proof, but I cannot see where the third condition of Bernoulli polynomials, given below, is used in the derivation of the Euler-Maclaurin summation formula. The Bernoulli polynomials are defined inductively by $$ b_0(x) = 1, $$ $$ b_n'(x) = nb_{n-1}(x) \:\text{ and }\: \int_0^1 b_n(x) = 0 \:\text{  for  }\: n \geq 1. $$ Suppose instead that we defined the alternative Bernoulli polynomials as follows, $$ a_0(x) = 1, $$ $$ a_n'(x) = na_{n-1}(x) \:\text{ and }\: a_n(0) = 0 \:\text{  for  }\: n \geq 1. $$ so that the new third condition essentially gives $a_n(x) = x^n$ for all $n$, thus simplifying matters. Then the periodic Bernoulli functions used in the proof would be $A_n(x) = a_n(\{x\})$, where $\{x\}$ denotes the fractional part of $x$. Using these functions, can anyone show me where the proof of Euler-Maclaurin breaks down? (Assume we are using the elementary proof by induction that is explained in detail on Wikipedia , for example.)",,"['real-analysis', 'sequences-and-series', 'euler-maclaurin', 'summation-method']"
26,Functions defined by integrals (problem 10.23 from Apostol's Mathematical Analysis),Functions defined by integrals (problem 10.23 from Apostol's Mathematical Analysis),,"There's a problem (#10.23) in Apostol's Mathematical Analysis of which I am having a rough time solving: Let $F(y)= \int_{0}^{\infty}\frac{\sin xy}{x(x^{2}+1)}dx$ if $y > 0$. Show that $F$ satisfies the differential equation $F''(y)-F(y)+\frac{\pi }{2}  = 0$ and deduce that $F(y)= \frac{1}{2}\pi(1-e^{-y})$. Use this result to deduce the following equations, valid for $y > 0$ and $a > 0$: Use this to deduce that for $y>0$  and $a>0$ \begin{align} \int\nolimits_{0}^{\infty}\frac{\sin xy}{x(x^{2}+a^{2})}dx = \frac{\pi}{2a^{2}}(1-e^{-ay}), \newline \int_{0}^{\infty}\frac{\cos xy}{x^{2}+a^{2}}dx = \frac{\pi e^{-ay}}{2a}, \newline \int_{0}^{\infty}\frac{x\sin xy}{x^{2}+a^{2}}dx = \frac{\pi}{2}e^{-ay} \end{align} We could use $\int_{0}^{\infty}\frac{\sin x}{x}dx = \frac{\pi}{2}$ How can I show this?","There's a problem (#10.23) in Apostol's Mathematical Analysis of which I am having a rough time solving: Let $F(y)= \int_{0}^{\infty}\frac{\sin xy}{x(x^{2}+1)}dx$ if $y > 0$. Show that $F$ satisfies the differential equation $F''(y)-F(y)+\frac{\pi }{2}  = 0$ and deduce that $F(y)= \frac{1}{2}\pi(1-e^{-y})$. Use this result to deduce the following equations, valid for $y > 0$ and $a > 0$: Use this to deduce that for $y>0$  and $a>0$ \begin{align} \int\nolimits_{0}^{\infty}\frac{\sin xy}{x(x^{2}+a^{2})}dx = \frac{\pi}{2a^{2}}(1-e^{-ay}), \newline \int_{0}^{\infty}\frac{\cos xy}{x^{2}+a^{2}}dx = \frac{\pi e^{-ay}}{2a}, \newline \int_{0}^{\infty}\frac{x\sin xy}{x^{2}+a^{2}}dx = \frac{\pi}{2}e^{-ay} \end{align} We could use $\int_{0}^{\infty}\frac{\sin x}{x}dx = \frac{\pi}{2}$ How can I show this?",,"['real-analysis', 'integration']"
27,Some questions about the gamma function,Some questions about the gamma function,,"Show that $\Gamma(y) = \int_0^{\infty}{e^{-x}x^{y-1}\,dx}$ is finite for $y>0$ both as an improper Riemann integral and as a Lebesgue integral. Show $\Gamma'(y) = \int_0^{\infty}{e^{-x}x^{y-1}\ln{x}\,dx}$ for $y>0$. For one: I've tried simply integrating it as an improper Riemann integral, but you always end up with another integral of the ""same type"" (which is how you eventually show $\Gamma(y+1)=y\Gamma(y)$ ). How do I get around this? As for the Lebesgue integral, I think it'd be easiest to compare the integrand to a larger function whose integral converges, but I haven't come up with a good candidate. For two: Fix $y_0>0$. Write $$\Gamma'(y) = \lim_{y\to y_0}{\int_0^{\infty}{\frac{e^{-x}x^{y-1}-e^{-x}x^{y_0-1}}{y-y_0}\,dx}}\,.$$ By the MVT, there exists $\eta$ between $y$ and $y_0$ such that the above limit is equal to $$\lim_{y\to y_0}{\int_0^{\infty}{e^{-x}x^{\eta-1}\ln{x}\,dx}}\,.$$ But now I'm not sure what to do. This is similar to a previous question I posted; for that problem we knew the derivative of the original integrand was bounded, so we applied the bounded convergence theorem. Would it be enough to prove that the derivative of my integrand is bounded, and apply BCT?","Show that $\Gamma(y) = \int_0^{\infty}{e^{-x}x^{y-1}\,dx}$ is finite for $y>0$ both as an improper Riemann integral and as a Lebesgue integral. Show $\Gamma'(y) = \int_0^{\infty}{e^{-x}x^{y-1}\ln{x}\,dx}$ for $y>0$. For one: I've tried simply integrating it as an improper Riemann integral, but you always end up with another integral of the ""same type"" (which is how you eventually show $\Gamma(y+1)=y\Gamma(y)$ ). How do I get around this? As for the Lebesgue integral, I think it'd be easiest to compare the integrand to a larger function whose integral converges, but I haven't come up with a good candidate. For two: Fix $y_0>0$. Write $$\Gamma'(y) = \lim_{y\to y_0}{\int_0^{\infty}{\frac{e^{-x}x^{y-1}-e^{-x}x^{y_0-1}}{y-y_0}\,dx}}\,.$$ By the MVT, there exists $\eta$ between $y$ and $y_0$ such that the above limit is equal to $$\lim_{y\to y_0}{\int_0^{\infty}{e^{-x}x^{\eta-1}\ln{x}\,dx}}\,.$$ But now I'm not sure what to do. This is similar to a previous question I posted; for that problem we knew the derivative of the original integrand was bounded, so we applied the bounded convergence theorem. Would it be enough to prove that the derivative of my integrand is bounded, and apply BCT?",,"['real-analysis', 'special-functions', 'gamma-function']"
28,showing a set is totally disconnected,showing a set is totally disconnected,,"So, we are considering the subset $$ S = \{(x, y) \in \mathbb{R^2} | (x \text{ and } y \in \mathbb{Q}) \text{ or }  (x \text{ and } y \notin \mathbb{Q})\} $$ And consider its complement $$ T = \mathbb{R^2} \backslash S $$ The set T is disconnected, actually I am fairly certain it is totally disconnected. I am just having problems showing that rigorously. I was trying to show it using straight lines but I don't think I was getting anywhere. I know that a totally disconnected set's only connected sets are the one point sets. I've been trying to show that given two arbitrary points, that a separation exists between them. It is more difficult since this is in the plane. Any hints at all would be a great help. Maybe I'm making mountains of molehills.","So, we are considering the subset $$ S = \{(x, y) \in \mathbb{R^2} | (x \text{ and } y \in \mathbb{Q}) \text{ or }  (x \text{ and } y \notin \mathbb{Q})\} $$ And consider its complement $$ T = \mathbb{R^2} \backslash S $$ The set T is disconnected, actually I am fairly certain it is totally disconnected. I am just having problems showing that rigorously. I was trying to show it using straight lines but I don't think I was getting anywhere. I know that a totally disconnected set's only connected sets are the one point sets. I've been trying to show that given two arbitrary points, that a separation exists between them. It is more difficult since this is in the plane. Any hints at all would be a great help. Maybe I'm making mountains of molehills.",,"['real-analysis', 'general-topology']"
29,"If $F$ is strictly increasing with closed image, then $F$ is continuous","If  is strictly increasing with closed image, then  is continuous",F F,"Let $F$ be a strictly increasing function on $S$, a subset of the real line. If you know that $F(S)$ is closed, prove that $F$ is continuous.","Let $F$ be a strictly increasing function on $S$, a subset of the real line. If you know that $F(S)$ is closed, prove that $F$ is continuous.",,['real-analysis']
30,Archimedian field $K$ has LUB property iff it's complete requires DC?,Archimedian field  has LUB property iff it's complete requires DC?,K,"The Setting Let $K$ be an Archimedean field . TFAE: $K$ has the least upper bound property. Every Cauchy sequence in $K$'s additive group converges. Now proving that 1 implies 2 is easy, but the other direction is slightly harder. Not that that's a problem. Rather the problem is that I can't see a route that doesn't invoke at least dependent choice at some point. Strategy 1 Starting with a nonempty set $A$ that's bounded above, you could construct a monotonely non-decreasing Cauchy sequence of upper bounds that has the supremum as its limit. Here's a short sketch: Pick an upper bound $B_0$ of $A$. Pick an $a_0 \in A$. Recursively define $$ B_{i+1} = \begin{cases} \frac{B_i+a_i}{2}, & \text{ if that's an upper bound for } A \\ B_i,                & \text{ otherwise} \end{cases} $$ and $$ a_{i+1} = \begin{cases} a_i,  & \text{ if $\tfrac{a_i+B_i}{2}$ is an upper bound for $A$}\\ \text{choose any } a \in A \text{ s.t. } \frac{a_i+B_i}{2} < a,  & \text{ otherwise.} \end{cases} $$ I can't see a way to get rid of the choice because you really want the $a_i$ to be in $A$ for the argument to go through. Strategy 2 Okay, let's go the long way instead! First we show that $K$ complete implies $[a,b]$ compact. Then we show that that implies that closed and bounded subsets are compact (""Heine-Borel property""). And finally we show that not(Heine-Borel property) implies not(least upper bound property). But I already get stumped on the first part. Clearly it's easy to show that $K$ complete implies $[a,b]$ is sequentially compact. And from here it'd be nice to use that $K$ is 2nd countable (the intervals with rational endpoints are a basis) to get that $[a,b]$ is in fact compact. So you start with an open covering $U_\alpha$ of $[a,b]$. 2nd countable spaces are Lindelöf... wait... let's make sure and prove that. Let $\{B_i\}$ be a countable basis. Then for each $B_i$ you choose a $U_\alpha$... oh. Choice crept up again. So my question is this: Does this really require (an admittedly weak form of) choice? Or is there a way to do without?","The Setting Let $K$ be an Archimedean field . TFAE: $K$ has the least upper bound property. Every Cauchy sequence in $K$'s additive group converges. Now proving that 1 implies 2 is easy, but the other direction is slightly harder. Not that that's a problem. Rather the problem is that I can't see a route that doesn't invoke at least dependent choice at some point. Strategy 1 Starting with a nonempty set $A$ that's bounded above, you could construct a monotonely non-decreasing Cauchy sequence of upper bounds that has the supremum as its limit. Here's a short sketch: Pick an upper bound $B_0$ of $A$. Pick an $a_0 \in A$. Recursively define $$ B_{i+1} = \begin{cases} \frac{B_i+a_i}{2}, & \text{ if that's an upper bound for } A \\ B_i,                & \text{ otherwise} \end{cases} $$ and $$ a_{i+1} = \begin{cases} a_i,  & \text{ if $\tfrac{a_i+B_i}{2}$ is an upper bound for $A$}\\ \text{choose any } a \in A \text{ s.t. } \frac{a_i+B_i}{2} < a,  & \text{ otherwise.} \end{cases} $$ I can't see a way to get rid of the choice because you really want the $a_i$ to be in $A$ for the argument to go through. Strategy 2 Okay, let's go the long way instead! First we show that $K$ complete implies $[a,b]$ compact. Then we show that that implies that closed and bounded subsets are compact (""Heine-Borel property""). And finally we show that not(Heine-Borel property) implies not(least upper bound property). But I already get stumped on the first part. Clearly it's easy to show that $K$ complete implies $[a,b]$ is sequentially compact. And from here it'd be nice to use that $K$ is 2nd countable (the intervals with rational endpoints are a basis) to get that $[a,b]$ is in fact compact. So you start with an open covering $U_\alpha$ of $[a,b]$. 2nd countable spaces are Lindelöf... wait... let's make sure and prove that. Let $\{B_i\}$ be a countable basis. Then for each $B_i$ you choose a $U_\alpha$... oh. Choice crept up again. So my question is this: Does this really require (an admittedly weak form of) choice? Or is there a way to do without?",,['real-analysis']
31,Laplace's Equation on a Pac-Man,Laplace's Equation on a Pac-Man,,"I am struggling way too much with this problem, any help is highly appreciated. Consider the Pac-Man-like set described by $$P=\left\{(\rho\cos\theta,\rho\sin\theta):\rho\in(0,1),\theta\in\left(\frac{\pi}{4},\frac{7\pi}{4}\right)\right\}$$ whose boundary can be described by $\partial P=\Gamma\cup\gamma_+\cup\gamma_-$ with $\Gamma=\{(\cos\theta,\sin\theta):\theta\in\left[\frac{\pi}{4},\frac{7\pi}{4}\right]\}$ and $\gamma_{\pm}=\{(x,\pm x):x\in[0,\frac{1}{\sqrt{2}}]\}$ as shown below ( plotted with desmos ). I want to find a solution of the Laplace equation $\psi_{xx}+\psi_{yy}=0$ for $\psi:\overline{P}\to\mathbb{R}$ with boundary conditions $$\boxed{\begin{cases}\psi\vert_{\Gamma}(\cos\theta,\sin\theta)=(\theta-\frac{\pi}{4})(\theta-\frac{7\pi}{4})\\ \psi\vert_{\gamma_+}(x,x)=\psi\vert_{\gamma_-}(x,-x)=0\end{cases}}$$ This is because I want it to be a non-zero polynomial on $\Gamma$ , zero on $\gamma_{\pm}$ and continuous on $\partial P$ . My question then is: Does such a $\psi:\overline{P}\to\mathbb{R}$ exist? If so, how can I compute it? Now I will show what I've managed to do. Suppose that we could express $\psi$ as a Fourier series like $$\psi(\rho\cos\theta,\rho\sin\theta)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\theta)+b_n\sin(n\theta))$$ Then, we have by the boundary conditions $\psi\vert_{\gamma_{\pm}}=0$ that $$\begin{cases}0=\psi\left(\frac{\rho}{\sqrt{2}},\frac{\rho}{\sqrt{2}}\right)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\pi/4)+b_n\sin(n\pi/4))\\ 0=\psi\left(\frac{\rho}{\sqrt{2}},-\frac{\rho}{\sqrt{2}}\right)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\pi/4)-b_n\sin(n\pi/4)) \end{cases}$$ which immediately implies $a_n=0$ for $n\not\equiv2(\text{mod }4)$ and $b_n=0$ for $n\not\equiv0(\text{mod }4)$ by adding and subtracting both power series and equating the coeficients to zero. However, this means that $$\psi(\rho\cos\theta,\rho\sin\theta)=a_2\rho^2\cos(2\theta)+b_4\rho^4\sin(4\theta)+a_6\rho^6\cos(6\theta)+...$$ And this is incompatible with the boundary condition for $\Gamma$ since, on $\Gamma$ , $\psi$ isn't $\pi$ -periodic while the right-hand side is. Note that $\rho^{-n}\cos(n\theta),\rho^{-n}\sin(n\theta)$ for $n\geq1$ and $\log\rho$ cannot appear in the polar form infinite series since these are not defined for $(x,y)=(0,0)$ and $(0,0)\in\overline{P}$ . So does this mean that Laplace's equation has no solution on $\overline{P}$ with those boundary conditions? Edit: In this question , user83387 states Suppose we look for solutions to $\Delta u = 0$ on the circular sector $\{ (r, \theta) \mid 0 < r < a, 0 < \theta < \gamma \}$ with initial conditions $u(a, \theta) = g(\theta), \, u(r, 0) = u(r, \gamma) = 0$ . Using separation of variables, I showed that the solution is of the form $$ u(r, \theta) = \sum_{n=1}^\infty \frac{2a^{-\frac{n\pi}{\gamma}}}{\gamma} \left(\int_0^\gamma g(\phi)\sin\left(\frac{n\pi \phi}{\gamma}\right) \, d\phi\right)  \, r^{\frac{n\pi}{\gamma}}\sin\left(\frac{n\pi\theta}{\gamma}\right)$$ This is precisely what I needed shifting $\theta$ by $\pi/4$ radians and considering $a=1,\gamma=3\pi/2$ . However, how did user83387 arrive to that expression?","I am struggling way too much with this problem, any help is highly appreciated. Consider the Pac-Man-like set described by whose boundary can be described by with and as shown below ( plotted with desmos ). I want to find a solution of the Laplace equation for with boundary conditions This is because I want it to be a non-zero polynomial on , zero on and continuous on . My question then is: Does such a exist? If so, how can I compute it? Now I will show what I've managed to do. Suppose that we could express as a Fourier series like Then, we have by the boundary conditions that which immediately implies for and for by adding and subtracting both power series and equating the coeficients to zero. However, this means that And this is incompatible with the boundary condition for since, on , isn't -periodic while the right-hand side is. Note that for and cannot appear in the polar form infinite series since these are not defined for and . So does this mean that Laplace's equation has no solution on with those boundary conditions? Edit: In this question , user83387 states Suppose we look for solutions to on the circular sector with initial conditions . Using separation of variables, I showed that the solution is of the form This is precisely what I needed shifting by radians and considering . However, how did user83387 arrive to that expression?","P=\left\{(\rho\cos\theta,\rho\sin\theta):\rho\in(0,1),\theta\in\left(\frac{\pi}{4},\frac{7\pi}{4}\right)\right\} \partial P=\Gamma\cup\gamma_+\cup\gamma_- \Gamma=\{(\cos\theta,\sin\theta):\theta\in\left[\frac{\pi}{4},\frac{7\pi}{4}\right]\} \gamma_{\pm}=\{(x,\pm x):x\in[0,\frac{1}{\sqrt{2}}]\} \psi_{xx}+\psi_{yy}=0 \psi:\overline{P}\to\mathbb{R} \boxed{\begin{cases}\psi\vert_{\Gamma}(\cos\theta,\sin\theta)=(\theta-\frac{\pi}{4})(\theta-\frac{7\pi}{4})\\ \psi\vert_{\gamma_+}(x,x)=\psi\vert_{\gamma_-}(x,-x)=0\end{cases}} \Gamma \gamma_{\pm} \partial P \psi:\overline{P}\to\mathbb{R} \psi \psi(\rho\cos\theta,\rho\sin\theta)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\theta)+b_n\sin(n\theta)) \psi\vert_{\gamma_{\pm}}=0 \begin{cases}0=\psi\left(\frac{\rho}{\sqrt{2}},\frac{\rho}{\sqrt{2}}\right)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\pi/4)+b_n\sin(n\pi/4))\\ 0=\psi\left(\frac{\rho}{\sqrt{2}},-\frac{\rho}{\sqrt{2}}\right)=a_0+\sum_{n=1}^\infty\rho^n(a_n\cos(n\pi/4)-b_n\sin(n\pi/4))
\end{cases} a_n=0 n\not\equiv2(\text{mod }4) b_n=0 n\not\equiv0(\text{mod }4) \psi(\rho\cos\theta,\rho\sin\theta)=a_2\rho^2\cos(2\theta)+b_4\rho^4\sin(4\theta)+a_6\rho^6\cos(6\theta)+... \Gamma \Gamma \psi \pi \rho^{-n}\cos(n\theta),\rho^{-n}\sin(n\theta) n\geq1 \log\rho (x,y)=(0,0) (0,0)\in\overline{P} \overline{P} \Delta u = 0 \{ (r, \theta) \mid 0 < r < a, 0 < \theta < \gamma \} u(a, \theta) = g(\theta), \, u(r, 0) = u(r, \gamma) = 0  u(r, \theta) = \sum_{n=1}^\infty \frac{2a^{-\frac{n\pi}{\gamma}}}{\gamma} \left(\int_0^\gamma g(\phi)\sin\left(\frac{n\pi \phi}{\gamma}\right) \, d\phi\right)  \, r^{\frac{n\pi}{\gamma}}\sin\left(\frac{n\pi\theta}{\gamma}\right) \theta \pi/4 a=1,\gamma=3\pi/2","['real-analysis', 'partial-differential-equations', 'fourier-series', 'harmonic-functions']"
32,"Understanding the proof of $L^p(X,\mathscr{A},\mu)$ is complete ($1\leq p<+\infty$)",Understanding the proof of  is complete (),"L^p(X,\mathscr{A},\mu) 1\leq p<+\infty","Background I have some questions when reading the proof of $L^p(X,\mathscr{A},\mu)$ is complete for $1\leq p<+\infty$ . The proof is proceeded by showing that each absolutely convergent series in $L^p(X,\mathscr{A},\mu)$ is convergent: Suppose that $1\leq p<+\infty$ and that $\{f_k\}$ is a sequence of functions that belong to $\mathscr{L}^p(X,\mathscr{A},\mu)$ and satisfy $\sum_k\|f_k\|_p<+\infty$ . Define $g:X\to[0,+\infty]$ by $$ g(x)=\left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p $$ (of course $(+\infty)^p=+\infty$ ). Minkowski's inequality, applied to the function $|f_k|$ , implies $$ \left(\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu\right)^{\frac{1}{p}} = \left\|\sum_{k=1}^n|f_k|\right\|_p \leq \sum_{k=1}^n\|f_k\|_p $$ holds for each $n$ , and so it follows from the monotone convergence theorem that $$ \int gd\mu = \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu \leq \left(\sum_{k=1}^{\infty}\|f_k\|_p\right)^p; $$ thus $g$ is integrable. Consequently $g(x)$ is finite for almost every $x$ (Corollary 2.3.14), and the series $\sum_kf_k(x)$ is absolutely convergent, and hence convergent, for almost every $x$ . Define a function $f$ on $X$ by \begin{align*} f(x) =  \begin{cases} \sum_{k=1}^{\infty}f_k(x)\quad &\text{if $g(x)<+\infty$},\\ \\ 0\quad &\text{otherwise}. \end{cases} \end{align*} Then $f$ is measurable and satisfies $|f|^p\leq g$ , and so it belongs to $\mathscr{L}^p(X,\mathscr{A},\mu)$ . Since $\lim_{n\to\infty}\left|\sum_{k=1}^nf_k(x)-f(x)\right|=0$ and $\left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x)$ hold for almost every $x$ , the dominated convergence theorem implies that $\lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p=0$ . The completeness of $L^p(X,\mathscr{A},\mu)$ follows. My Questions Here are my questions: In order to apply the monotone convergence theorem, we need $g$ and $\left(\sum_{k=1}^n|f_k|\right)^p$ for each $n\in\mathbb{N}$ to be $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions. But why are they? Why is the integrability of $g$ follows from that inequality? Why is $f$ $\mathscr{A}$ -measurable? Why is $|f|^p\leq g$ ? Why is $|f|^p\in\mathscr{L}^p(X,\mathscr{A},\mu)$ ? Why does $\left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x)$ hold for almost every $x$ ? How does $\lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p=0$ follow in the end? My Attempt I tried to answer these questions myself one-by-one, and here is my attempt: Note that for each positive integer $k$ the function $f_k$ is complex-valued and $\mathscr{A}$ -measurable, thus $|f_k|$ is nonnegative real-valued $\mathscr{A}$ -measurable, and so $\sum_{k=1}^n|f_k|$ is nonnegative real-valued $\mathscr{A}$ -measurable.  For each positive integer $n$ let $F_n = \left(\sum_{k=1}^n|f_k|\right)^p$ . Clearly $F_n$ is nonnegative real-valued. We now prove that $F_n$ is $\mathscr{A}$ -measurable. For any $t\leq0$ , we have $$ \{x\in X:F_n(x)<t\}=\emptyset\in\mathscr{A}. $$ For any $t>0$ , we have \begin{align*} \{x\in X:F_n(x)<t\} &= \left\{x\in X:\left(\sum_{k=1}^n|f_k(x)|\right)^p<t\right\}\\ &= \left\{x\in X:\sum_{k=1}^n|f_k(x)|<t^{\frac{1}{p}}\right\}\\ &\in \mathscr{A}. \end{align*} Thus $F_n$ is a nonnegative real-valued $\mathscr{A}$ -measurable function for each $n\in\mathbb{N}$ . Moreover, for each $x\in X$ and for each positive integer $n$ the sequence $\{F_n(x)\}$ satisfies $F_n(x)\geq0$ and $F_n(x)\leq F_{n+1}$ , thus $\lim_{n\to\infty}F_n(x)$ is either a positive number or equal to $+\infty$ . So $g=\lim_{n\to\infty}F_n = \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p$ is well defined and it is a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function. The Minkowski's inequality implies $$ \left(\int F_nd\mu\right)^{\frac{1}{p}}\leq\sum_{k=1}^n\|f_k\|_p $$ holds for each $n\in\mathbb{N}$ . Then this inequality, together with monotone convergence theorem and the convergence of $\sum_{k=1}^{\infty}\|f_k\|_p$ , imply \begin{align*} \int gd\mu &= \lim_{n\to\infty}\int F_nd\mu = \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu\\ &\leq \lim_{n\to\infty}\left(\sum_{k=1}^n\|f_k\|_p\right)^p = \left(\sum_{k=1}^{\infty}\|f_k\|_p\right)^p < +\infty \end{align*} (note that the existence of the limit $\lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu$ is implied by the monotone convergence theorem). Since $g$ is $[0,+\infty]$ -valued, it follows that $g^+=g$ and $g^-=0$ , and so $\int g^+d\mu$ and $\int g^-d\mu$ are both finite, which implies $g$ is integrable. Let $A=\{x\in X:g(x)<+\infty\}$ . Write $f$ as \begin{align*} f &= \sum_{k=1}^{\infty}f_k\chi_A\\ &= \lim_{n\to\infty}\sum_{k=1}^nf_k\chi_A\\ &= \lim_{n\to\infty}h_n, \end{align*} where $h_n = \sum_{k=1}^nf_k\chi_A$ . We prove that $A\in\mathscr{A}$ . We know that $g$ is $\mathscr{A}$ -measurable. Define $p:X\to[0,+\infty]$ by $p(x)=+\infty$ for all $x\in X$ . Then $\{x\in X: p(x)<t\}=\emptyset\in\mathscr{A}$ for all $t\in\mathbb{R}$ , and thus $p$ is $\mathscr{A}$ -measurable. Therefore, $A = \{x\in X:g(x)<p(x)\}\in\mathscr{A}$ , and so $A^c\in\mathscr{A}$ . Next, note that for all $k\in\mathbb{N}$ , $f_k$ is $\mathscr{A}$ -measurable. Moreover, for $t\leq0$ , $\{x\in X:\chi_A(x)<t\}=\emptyset\in\mathscr{A}$ ; for $t>1$ , $\{x\in X:\chi_A(x)<t\} = X\in\mathscr{A}$ ; for $1<t\leq1$ , $\{x\in X:\chi_A(x)<t\} = A^c\in\mathscr{A}$ . Thus, $h_n$ is $\mathscr{A}$ -measurable, and so $f=\lim_{n\to\infty}h_n$ is $\mathscr{A}$ -measurable. If $x\in A$ , then $|f(x)|^p = \left|\sum_{k=1}^{\infty}\right|^p \leq \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p = g(x)$ . If $x\notin A$ , then $|f(x)|^p=0\leq g(x)$ . We first prove that $|f|^p$ is $\mathscr{A}$ -measurable. If $t\leq0$ , then $\{x\in X:|f(x)|^p<t\}=\emptyset\in\mathscr{A}$ . If $t>0$ , then $\{x\in X:|f(x)|^p<t\} = \{x\in X:|f(x)|<t^{\frac{1}{p}}\}\in\mathscr{A}$ because $f$ and thus $|f|$ is $\mathscr{A}$ -measurable. Therefore, $|f|^p$ is a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function. We know that $g$ is also a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable functions. The integrability of $g$ implies that $\int gd\mu = \int g^+d\mu < +\infty$ . Then $|f|^p\leq g$ implies $\int|f|^pd\mu\leq\int gd\mu <+\infty$ . Since $\left(|f|^p\right)^+=|f|^p$ and $\left(|f|^p\right)^- = 0$ , we have $|f|^p$ is integrable, and so $|f|^p\in\mathscr{L}^p(X,\mathscr{A},\mu)$ . If $x\in A$ , then \begin{align*} \left|\sum_{k=1}^nf_k(x) - f(x)\right|^p &= \left|\sum_{k=n+1}^{\infty}f_k(x)\right|^p\\ &= \lim_{m\to\infty}\left|\sum_{k=n+1}^mf_k(x)\right|^p\\ &\leq \lim_{m\to\infty}\left(\sum_{k=n+1}^m|f_k(x)|\right)^p\\ &= \left(\sum_{k=n+1}^{\infty}|f_k(x)|\right)^p\\ &\leq \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p\\ &= g(x). \end{align*} Thus, $\left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x)$ holds for almost every $x$ . We know that $g$ is a $[0,+\infty]$ -valued integrable function on $X$ . Define functions $q$ and $q_1,q_2,\dots$ by $q=0$ and $q_n = \left|\sum_{k=1}^nf_k-f\right|^p$ . Then for all $x\in A$ , $q=\lim_{n\to\infty}q_n$ , and so $q=\lim_{n\to\infty}q_n$ holds $\mu$ -almost everywhere. Also, $|q_n|<g$ holds $\mu$ -almost everywhere. So the dominated convergence theorem implies $0=\int qd\mu = \lim_{n\to\infty}\int q_nd\mu$ and so $\lim_{n\to\infty}\left(\int q_nd\mu\right)^{\frac{1}{p}} = 0$ . Hence $\lim_{n\to\infty}\left(\int\left|\sum_{k=1}^nf_k-f\right|^pd\mu\right)^{\frac{1}{p}} = \lim_{n\to\infty}\|\sum_{k=1}^nf_k-f\|_p = \|\lim_{n\to\infty}\sum_{k=1}^nf_k - f\|_p = \|\sum_{k=1}^{\infty}f_k - f\|_p = 0$ . Therefore, $\sum_{k=1}^{\infty}f_k = f \in\mathscr{L}^p(X,\mathscr{A},\mu)$ . Where I Have Trouble My main concern is my attempt to Question 7. There are mainly two of them: First, I claimed that $\lim_{n\to\infty}\int q_nd\mu=0$ implies $\lim_{n\to\infty}\left(\int q_nd\mu\right)^{\frac{1}{p}} = 0$ . But I am not sure whether this is legit, because I have never seen a ""power law of limit of sequence"" from any analysis textbook. Basically, if $p\in\mathbb{R}$ and $\lim_{n\to\infty}a_n = a\in[-\infty,+\infty]$ , does this imply $\lim_{n\to\infty}a_n^p = a^p$ ? I don't think this is correct, because $a^p$ may not even be defined (consider for example square root of a negative number or the denominator is zero). But what if $a_n\in[0,+\infty]$ for all $n\in\mathbb{N}$ and $p\in\mathbb{R}_+$ ? How can we generalize such a result (in its most general possible case) and prove it? Secone, I claimed that the limit of norm is the norm of limit. But is this true? Why? Basically, $\lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p = \lim_{n\to\infty}\left(\int\left|\sum_{k=1}^nf_k - f\right|^pd\mu\right)^{\frac{1}{p}}$ , but is this equal to $\left(\int\left|\sum_{k=1}^{\infty}f_k-f\right|^pd\mu\right)^{\frac{1}{p}}$ ? Why? Last but not least, I know this is a lot to ask, but just in case you are interested in the proof, I would really appreciate it if you could help me check whether my answer to the other questions are correct. Thank you very much in advance! Reference: $\quad$ Theorem 3.4.1 from Measure Theory by Donald Cohn","Background I have some questions when reading the proof of is complete for . The proof is proceeded by showing that each absolutely convergent series in is convergent: Suppose that and that is a sequence of functions that belong to and satisfy . Define by (of course ). Minkowski's inequality, applied to the function , implies holds for each , and so it follows from the monotone convergence theorem that thus is integrable. Consequently is finite for almost every (Corollary 2.3.14), and the series is absolutely convergent, and hence convergent, for almost every . Define a function on by Then is measurable and satisfies , and so it belongs to . Since and hold for almost every , the dominated convergence theorem implies that . The completeness of follows. My Questions Here are my questions: In order to apply the monotone convergence theorem, we need and for each to be -valued -measurable functions. But why are they? Why is the integrability of follows from that inequality? Why is -measurable? Why is ? Why is ? Why does hold for almost every ? How does follow in the end? My Attempt I tried to answer these questions myself one-by-one, and here is my attempt: Note that for each positive integer the function is complex-valued and -measurable, thus is nonnegative real-valued -measurable, and so is nonnegative real-valued -measurable.  For each positive integer let . Clearly is nonnegative real-valued. We now prove that is -measurable. For any , we have For any , we have Thus is a nonnegative real-valued -measurable function for each . Moreover, for each and for each positive integer the sequence satisfies and , thus is either a positive number or equal to . So is well defined and it is a -valued -measurable function. The Minkowski's inequality implies holds for each . Then this inequality, together with monotone convergence theorem and the convergence of , imply (note that the existence of the limit is implied by the monotone convergence theorem). Since is -valued, it follows that and , and so and are both finite, which implies is integrable. Let . Write as where . We prove that . We know that is -measurable. Define by for all . Then for all , and thus is -measurable. Therefore, , and so . Next, note that for all , is -measurable. Moreover, for , ; for , ; for , . Thus, is -measurable, and so is -measurable. If , then . If , then . We first prove that is -measurable. If , then . If , then because and thus is -measurable. Therefore, is a -valued -measurable function. We know that is also a -valued -measurable functions. The integrability of implies that . Then implies . Since and , we have is integrable, and so . If , then Thus, holds for almost every . We know that is a -valued integrable function on . Define functions and by and . Then for all , , and so holds -almost everywhere. Also, holds -almost everywhere. So the dominated convergence theorem implies and so . Hence . Therefore, . Where I Have Trouble My main concern is my attempt to Question 7. There are mainly two of them: First, I claimed that implies . But I am not sure whether this is legit, because I have never seen a ""power law of limit of sequence"" from any analysis textbook. Basically, if and , does this imply ? I don't think this is correct, because may not even be defined (consider for example square root of a negative number or the denominator is zero). But what if for all and ? How can we generalize such a result (in its most general possible case) and prove it? Secone, I claimed that the limit of norm is the norm of limit. But is this true? Why? Basically, , but is this equal to ? Why? Last but not least, I know this is a lot to ask, but just in case you are interested in the proof, I would really appreciate it if you could help me check whether my answer to the other questions are correct. Thank you very much in advance! Reference: Theorem 3.4.1 from Measure Theory by Donald Cohn","L^p(X,\mathscr{A},\mu) 1\leq p<+\infty L^p(X,\mathscr{A},\mu) 1\leq p<+\infty \{f_k\} \mathscr{L}^p(X,\mathscr{A},\mu) \sum_k\|f_k\|_p<+\infty g:X\to[0,+\infty] 
g(x)=\left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p
 (+\infty)^p=+\infty |f_k| 
\left(\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu\right)^{\frac{1}{p}} = \left\|\sum_{k=1}^n|f_k|\right\|_p \leq \sum_{k=1}^n\|f_k\|_p
 n 
\int gd\mu = \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu \leq \left(\sum_{k=1}^{\infty}\|f_k\|_p\right)^p;
 g g(x) x \sum_kf_k(x) x f X \begin{align*}
f(x) = 
\begin{cases}
\sum_{k=1}^{\infty}f_k(x)\quad &\text{if g(x)<+\infty},\\
\\
0\quad &\text{otherwise}.
\end{cases}
\end{align*} f |f|^p\leq g \mathscr{L}^p(X,\mathscr{A},\mu) \lim_{n\to\infty}\left|\sum_{k=1}^nf_k(x)-f(x)\right|=0 \left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x) x \lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p=0 L^p(X,\mathscr{A},\mu) g \left(\sum_{k=1}^n|f_k|\right)^p n\in\mathbb{N} [0,+\infty] \mathscr{A} g f \mathscr{A} |f|^p\leq g |f|^p\in\mathscr{L}^p(X,\mathscr{A},\mu) \left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x) x \lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p=0 k f_k \mathscr{A} |f_k| \mathscr{A} \sum_{k=1}^n|f_k| \mathscr{A} n F_n = \left(\sum_{k=1}^n|f_k|\right)^p F_n F_n \mathscr{A} t\leq0 
\{x\in X:F_n(x)<t\}=\emptyset\in\mathscr{A}.
 t>0 \begin{align*}
\{x\in X:F_n(x)<t\} &= \left\{x\in X:\left(\sum_{k=1}^n|f_k(x)|\right)^p<t\right\}\\
&= \left\{x\in X:\sum_{k=1}^n|f_k(x)|<t^{\frac{1}{p}}\right\}\\
&\in \mathscr{A}.
\end{align*} F_n \mathscr{A} n\in\mathbb{N} x\in X n \{F_n(x)\} F_n(x)\geq0 F_n(x)\leq F_{n+1} \lim_{n\to\infty}F_n(x) +\infty g=\lim_{n\to\infty}F_n = \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p [0,+\infty] \mathscr{A} 
\left(\int F_nd\mu\right)^{\frac{1}{p}}\leq\sum_{k=1}^n\|f_k\|_p
 n\in\mathbb{N} \sum_{k=1}^{\infty}\|f_k\|_p \begin{align*}
\int gd\mu &= \lim_{n\to\infty}\int F_nd\mu = \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu\\
&\leq \lim_{n\to\infty}\left(\sum_{k=1}^n\|f_k\|_p\right)^p = \left(\sum_{k=1}^{\infty}\|f_k\|_p\right)^p < +\infty
\end{align*} \lim_{n\to\infty}\int\left(\sum_{k=1}^n|f_k|\right)^pd\mu g [0,+\infty] g^+=g g^-=0 \int g^+d\mu \int g^-d\mu g A=\{x\in X:g(x)<+\infty\} f \begin{align*}
f &= \sum_{k=1}^{\infty}f_k\chi_A\\
&= \lim_{n\to\infty}\sum_{k=1}^nf_k\chi_A\\
&= \lim_{n\to\infty}h_n,
\end{align*} h_n = \sum_{k=1}^nf_k\chi_A A\in\mathscr{A} g \mathscr{A} p:X\to[0,+\infty] p(x)=+\infty x\in X \{x\in X: p(x)<t\}=\emptyset\in\mathscr{A} t\in\mathbb{R} p \mathscr{A} A = \{x\in X:g(x)<p(x)\}\in\mathscr{A} A^c\in\mathscr{A} k\in\mathbb{N} f_k \mathscr{A} t\leq0 \{x\in X:\chi_A(x)<t\}=\emptyset\in\mathscr{A} t>1 \{x\in X:\chi_A(x)<t\} = X\in\mathscr{A} 1<t\leq1 \{x\in X:\chi_A(x)<t\} = A^c\in\mathscr{A} h_n \mathscr{A} f=\lim_{n\to\infty}h_n \mathscr{A} x\in A |f(x)|^p = \left|\sum_{k=1}^{\infty}\right|^p \leq \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p = g(x) x\notin A |f(x)|^p=0\leq g(x) |f|^p \mathscr{A} t\leq0 \{x\in X:|f(x)|^p<t\}=\emptyset\in\mathscr{A} t>0 \{x\in X:|f(x)|^p<t\} = \{x\in X:|f(x)|<t^{\frac{1}{p}}\}\in\mathscr{A} f |f| \mathscr{A} |f|^p [0,+\infty] \mathscr{A} g [0,+\infty] \mathscr{A} g \int gd\mu = \int g^+d\mu < +\infty |f|^p\leq g \int|f|^pd\mu\leq\int gd\mu <+\infty \left(|f|^p\right)^+=|f|^p \left(|f|^p\right)^- = 0 |f|^p |f|^p\in\mathscr{L}^p(X,\mathscr{A},\mu) x\in A \begin{align*}
\left|\sum_{k=1}^nf_k(x) - f(x)\right|^p &= \left|\sum_{k=n+1}^{\infty}f_k(x)\right|^p\\
&= \lim_{m\to\infty}\left|\sum_{k=n+1}^mf_k(x)\right|^p\\
&\leq \lim_{m\to\infty}\left(\sum_{k=n+1}^m|f_k(x)|\right)^p\\
&= \left(\sum_{k=n+1}^{\infty}|f_k(x)|\right)^p\\
&\leq \left(\sum_{k=1}^{\infty}|f_k(x)|\right)^p\\
&= g(x).
\end{align*} \left|\sum_{k=1}^nf_k(x)-f(x)\right|^p\leq g(x) x g [0,+\infty] X q q_1,q_2,\dots q=0 q_n = \left|\sum_{k=1}^nf_k-f\right|^p x\in A q=\lim_{n\to\infty}q_n q=\lim_{n\to\infty}q_n \mu |q_n|<g \mu 0=\int qd\mu = \lim_{n\to\infty}\int q_nd\mu \lim_{n\to\infty}\left(\int q_nd\mu\right)^{\frac{1}{p}} = 0 \lim_{n\to\infty}\left(\int\left|\sum_{k=1}^nf_k-f\right|^pd\mu\right)^{\frac{1}{p}} = \lim_{n\to\infty}\|\sum_{k=1}^nf_k-f\|_p = \|\lim_{n\to\infty}\sum_{k=1}^nf_k - f\|_p = \|\sum_{k=1}^{\infty}f_k - f\|_p = 0 \sum_{k=1}^{\infty}f_k = f \in\mathscr{L}^p(X,\mathscr{A},\mu) \lim_{n\to\infty}\int q_nd\mu=0 \lim_{n\to\infty}\left(\int q_nd\mu\right)^{\frac{1}{p}} = 0 p\in\mathbb{R} \lim_{n\to\infty}a_n = a\in[-\infty,+\infty] \lim_{n\to\infty}a_n^p = a^p a^p a_n\in[0,+\infty] n\in\mathbb{N} p\in\mathbb{R}_+ \lim_{n\to\infty}\left\|\sum_{k=1}^nf_k-f\right\|_p = \lim_{n\to\infty}\left(\int\left|\sum_{k=1}^nf_k - f\right|^pd\mu\right)^{\frac{1}{p}} \left(\int\left|\sum_{k=1}^{\infty}f_k-f\right|^pd\mu\right)^{\frac{1}{p}} \quad","['real-analysis', 'analysis', 'measure-theory', 'lp-spaces', 'complete-spaces']"
33,"Any other methods for evaluating $\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx$",Any other methods for evaluating,"\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx","I evaluated the following integral: $$I:=\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx$$ I would like to see any alternate solutions, here is my work. Using: $$\frac{\arctan(x)}{x}\equiv\int_{0}^{1}\frac{1}{1+x^2y^2}\,dy$$ It is trivial to show that: $$\frac{\arctan(\cos(x))}{\cos(x)}\equiv\int_{0}^{1}\frac{\sec^2(x)}{\tan^2(x)+1+y^2}\,dy$$ Therefore: $$I=\int_{0}^{\frac{\pi}{4}}\int_{0}^{1}\frac{\sec^2(x)}{\tan^2(x)+1+y^2}\,dy\,dx$$ Let $$\tan(x)\longrightarrow{x}$$ $$I=\int\int_{[0,1]^2}\frac{1}{1+x^2+y^2}\,dy\,dx$$ $$=\int_{0}^{\frac{\pi}{4}}\int_{0}^{\sec(\theta)}\frac{r}{1+r^2}\,dr\,d\theta+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\int_{0}^{\csc(\theta)}\frac{r}{1+r^2}\,dr\,d\theta$$ $$=\frac{1}{2}\left[\int_{0}^{\frac{\pi}{4}}\log\left(2+\tan^2(\theta)\right)\,d\theta+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\log\left(2+\cot^2(\theta)\right)\,d\theta\right]$$ For the rightmost integral let $$\theta\longrightarrow{\frac{\pi}{2}-\theta}$$ To get: $$I=\int_{0}^{\frac{\pi}{4}}\log\left(2+\tan^2(\theta)\right)\,d\theta$$ Now let $$\tan(\theta)=x$$ Which yields: $$I=\int_{0}^{1}\frac{\log\left(2+x^2\right)}{1+x^2}\,dx$$ Define $I(t)\forall t\in[0,1]$ to be: $$I(t):=\int_{0}^{1}\frac{\log\left(1+t(1+x^2)\right)}{1+x^2}\,dx$$ We can see that: $$I(0)=0$$ $$I(1)=I$$ And so it follows from the F.T.C. That: $$I=\int_{0}^{1}I’(t)\,dt$$ And so to follows that: $$I’(t)=\int_{0}^{1}\frac{1}{tx^2+1+t}\,dx$$ $$=\frac{\arctan\left(\sqrt{\frac{t}{1+t}}\right)}{\sqrt{t(1+t)}}$$ And so we have that: $$I=\int_{0}^{1}\frac{\arctan\left(\sqrt{\frac{t}{1+t}}\right)}{\sqrt{t(1+t)}}$$ Let $$\sqrt{\frac{t}{1+t}}=x$$ To get: $$I=2\int_{0}^{\frac{1}{\sqrt{2}}}\frac{\arctan(x)}{1-x^2}\,dx$$ Let $$x\longrightarrow{\frac{1-x}{1+x}}$$ $$I=\int_{3-2\sqrt{2}}^{1}\frac{\frac{\pi}{4}-\arctan(x)}{x}\,dx$$ $$=\frac{\pi}{4}\int_{3-2\sqrt{2}}^{1}\frac{1}{x}\,dx+\int_{0}^{3-2\sqrt{2}}\frac{\arctan(x)}{x}\,dx-\int_{0}^{1}\frac{\arctan(x)}{x}\,dx$$ $$=\frac{\pi}{2}\log\left(1+\sqrt{2}\right)+Ti_{2}\left(3-2\sqrt{2}\right)-G$$ And so we have the result: $$\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx=\frac{\pi}{2}\log\left(1+\sqrt{2}\right)+Ti_{2}\left(3-2\sqrt{2}\right)-G$$ Where: $$G:=\sum_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)^2}$$ Is Catalan’s constant. And $$Ti_{2}(x):=\int_{0}^{x}\frac{\arctan(t)}{t}\,dt$$ Is the inverse tangent integral function. As I mentioned in the beginning of my post. Is there any other unique methods for the evaluation of this integral?","I evaluated the following integral: I would like to see any alternate solutions, here is my work. Using: It is trivial to show that: Therefore: Let For the rightmost integral let To get: Now let Which yields: Define to be: We can see that: And so it follows from the F.T.C. That: And so to follows that: And so we have that: Let To get: Let And so we have the result: Where: Is Catalan’s constant. And Is the inverse tangent integral function. As I mentioned in the beginning of my post. Is there any other unique methods for the evaluation of this integral?","I:=\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx \frac{\arctan(x)}{x}\equiv\int_{0}^{1}\frac{1}{1+x^2y^2}\,dy \frac{\arctan(\cos(x))}{\cos(x)}\equiv\int_{0}^{1}\frac{\sec^2(x)}{\tan^2(x)+1+y^2}\,dy I=\int_{0}^{\frac{\pi}{4}}\int_{0}^{1}\frac{\sec^2(x)}{\tan^2(x)+1+y^2}\,dy\,dx \tan(x)\longrightarrow{x} I=\int\int_{[0,1]^2}\frac{1}{1+x^2+y^2}\,dy\,dx =\int_{0}^{\frac{\pi}{4}}\int_{0}^{\sec(\theta)}\frac{r}{1+r^2}\,dr\,d\theta+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\int_{0}^{\csc(\theta)}\frac{r}{1+r^2}\,dr\,d\theta =\frac{1}{2}\left[\int_{0}^{\frac{\pi}{4}}\log\left(2+\tan^2(\theta)\right)\,d\theta+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\log\left(2+\cot^2(\theta)\right)\,d\theta\right] \theta\longrightarrow{\frac{\pi}{2}-\theta} I=\int_{0}^{\frac{\pi}{4}}\log\left(2+\tan^2(\theta)\right)\,d\theta \tan(\theta)=x I=\int_{0}^{1}\frac{\log\left(2+x^2\right)}{1+x^2}\,dx I(t)\forall t\in[0,1] I(t):=\int_{0}^{1}\frac{\log\left(1+t(1+x^2)\right)}{1+x^2}\,dx I(0)=0 I(1)=I I=\int_{0}^{1}I’(t)\,dt I’(t)=\int_{0}^{1}\frac{1}{tx^2+1+t}\,dx =\frac{\arctan\left(\sqrt{\frac{t}{1+t}}\right)}{\sqrt{t(1+t)}} I=\int_{0}^{1}\frac{\arctan\left(\sqrt{\frac{t}{1+t}}\right)}{\sqrt{t(1+t)}} \sqrt{\frac{t}{1+t}}=x I=2\int_{0}^{\frac{1}{\sqrt{2}}}\frac{\arctan(x)}{1-x^2}\,dx x\longrightarrow{\frac{1-x}{1+x}} I=\int_{3-2\sqrt{2}}^{1}\frac{\frac{\pi}{4}-\arctan(x)}{x}\,dx =\frac{\pi}{4}\int_{3-2\sqrt{2}}^{1}\frac{1}{x}\,dx+\int_{0}^{3-2\sqrt{2}}\frac{\arctan(x)}{x}\,dx-\int_{0}^{1}\frac{\arctan(x)}{x}\,dx =\frac{\pi}{2}\log\left(1+\sqrt{2}\right)+Ti_{2}\left(3-2\sqrt{2}\right)-G \int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx=\frac{\pi}{2}\log\left(1+\sqrt{2}\right)+Ti_{2}\left(3-2\sqrt{2}\right)-G G:=\sum_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)^2} Ti_{2}(x):=\int_{0}^{x}\frac{\arctan(t)}{t}\,dt","['real-analysis', 'integration', 'definite-integrals', 'special-functions', 'alternative-proof']"
34,Doubling metric spaces,Doubling metric spaces,,"A metric space $(X,d)$ is said to be doubling if there exists a constant integer $N\ge 1$ such that every closed ball of radius $R$ can be covered by at most $N$ closed balls of radius $R/2$ . Proposition. If a metric space $(X,d)$ is doubling, then there exist constants $C\ge 1, s>0$ such that for every $x\in X$ , $0<r<R$ it holds the following implication: if $Y$ is a subset of the ball $\mathbb{B}(x,R)$ such that $d(y,y')>r$ for every $y,y'\in Y, y\ne y'$ , then $Y$ has cardinality less or equal than $C(R/r)^s$ . My questions are the following ones. How can I prove the Proposition? My idea is to use the following remark: every doubling metric space with doubling constant $N$ has de Groot dimension $N$ , that is to say, for each $x\in X, r>0$ , it holds that the cardinality of $Y$ is less or equal than $N$ for every set $Y\subseteq\mathbb{B}(x,r)$ such that $d(y,y')>r$ for every $y,y'\in Y$ with $y\ne y'$ . How can I prove that if there exists a doubling Borel outer measure over $(X,d)$ , then the metric space is doubling? I know that there exists a similar question to the current on MathStack but I'm looking for a more complete and detailed question. Can anyone help me please?","A metric space is said to be doubling if there exists a constant integer such that every closed ball of radius can be covered by at most closed balls of radius . Proposition. If a metric space is doubling, then there exist constants such that for every , it holds the following implication: if is a subset of the ball such that for every , then has cardinality less or equal than . My questions are the following ones. How can I prove the Proposition? My idea is to use the following remark: every doubling metric space with doubling constant has de Groot dimension , that is to say, for each , it holds that the cardinality of is less or equal than for every set such that for every with . How can I prove that if there exists a doubling Borel outer measure over , then the metric space is doubling? I know that there exists a similar question to the current on MathStack but I'm looking for a more complete and detailed question. Can anyone help me please?","(X,d) N\ge 1 R N R/2 (X,d) C\ge 1, s>0 x\in X 0<r<R Y \mathbb{B}(x,R) d(y,y')>r y,y'\in Y, y\ne y' Y C(R/r)^s N N x\in X, r>0 Y N Y\subseteq\mathbb{B}(x,r) d(y,y')>r y,y'\in Y y\ne y' (X,d)","['real-analysis', 'general-topology', 'measure-theory', 'metric-spaces']"
35,Determining $\displaystyle\lim _{x \rightarrow 0} f(x) g(x)$ given conditions on $f$ and $g$,Determining  given conditions on  and,\displaystyle\lim _{x \rightarrow 0} f(x) g(x) f g,"This was the 4th question that appreared in yesterday's JEE ADVANCED $2023$ : Let $f:(0,1) \rightarrow \mathbb{R}$ be the function defined as $$f(x)=\sqrt{n}$$ for all $x \in\left[\frac{1}{n+1}, \frac{1}{n}\right),$ where $n \in \mathbb{N}$ . Let $g:(0,1) \rightarrow \mathbb{R}$ be a function such that $$\int_{x^2}^x \sqrt{\frac{1-t}{t}} d t<g(x) <2 \sqrt{x}$$ for all $x \in(0,1)$ . Then $$\lim _{x \rightarrow 0} f(x) g(x)$$ (A) does NOT exist (B) is equal to 1 (C) is equal to 2 (D) is equal to 3 On noticing the domain of $f(x),$ I realised that the limit must be one-sided. But, this being an JEE Advanced exam, which usually asks really difficult questions and is known as the exam of exceptions, I thought that this might be some trap, so I didn't choose option A. Then I observed the definite integral and substituted $t=\cos^2(\theta)$ and obtained $\tan(\theta).$ But I don't have any clue on how to accordingly change the integration limits. What is a good solution strategy or thinking process for this exercise?","This was the 4th question that appreared in yesterday's JEE ADVANCED : Let be the function defined as for all where . Let be a function such that for all . Then (A) does NOT exist (B) is equal to 1 (C) is equal to 2 (D) is equal to 3 On noticing the domain of I realised that the limit must be one-sided. But, this being an JEE Advanced exam, which usually asks really difficult questions and is known as the exam of exceptions, I thought that this might be some trap, so I didn't choose option A. Then I observed the definite integral and substituted and obtained But I don't have any clue on how to accordingly change the integration limits. What is a good solution strategy or thinking process for this exercise?","2023 f:(0,1) \rightarrow \mathbb{R} f(x)=\sqrt{n} x \in\left[\frac{1}{n+1}, \frac{1}{n}\right), n \in \mathbb{N} g:(0,1) \rightarrow \mathbb{R} \int_{x^2}^x \sqrt{\frac{1-t}{t}} d t<g(x) <2
\sqrt{x} x \in(0,1) \lim _{x \rightarrow 0} f(x) g(x) f(x), t=\cos^2(\theta) \tan(\theta).","['real-analysis', 'calculus', 'integration', 'limits']"
36,What in a rigorous context (no notation abuse) is happening that allows you to solve this ODE problem?,What in a rigorous context (no notation abuse) is happening that allows you to solve this ODE problem?,,"In know that in a rigorous setting $dy \over dx$ isn't a fraction but is instead defined as the function: $\frac{d}{dx}y$ . Treating it as a fraction is technically considered an abuse of notation that is sometimes done as a shortcut for solving problems or explaining concepts. However, I have been trying to help another person with their intro to differential equations class and am confused on what the solution to the following problem is in a rigorous setting. Here is the problem (and solution): $$ dy - (y-1)^2 \space dx = 0 $$ Such that (given in their notes): $$ p(y) \space dy = g(x) \space dx $$ Then (according to their solution): $$ dy = (y-1)^2 \space dx \rightarrow \frac{1}{(y-1)^2} \space dy = dx$$ Which in turn implies that: $$\int (y-1)^{-2} \space dy = \int dx  = \frac{-1}{y-1} = x + c  $$ $$\rightarrow y = 1 - \frac{1}{x+c}$$ I' ve heard before that the $dy$ and $dx$ in integrals can be thought of a ""signaling"" which variable to integrate, but here you can see them being abused  as symbolic variables that are manipulated as if they were algebraic symbols. Not only that, but they are being manipulated as if there was an additive inverse $-dx$ for the $dx$ , otherwise the first step of the solution wouldn't be possible. My friend is confused on what the professor is doing, and I can't in good faith right now explain to him why this is allowable, because this solution feels like a heavy abuse of notation, and I don't know what is happening conceptually that allows the solution. What allows you to treat $dx$ and $dy$ as real-valued elements of a fraction in this solution? In a rigorous setting, what is really going on that allows you to solve this problem for $y$ ?","In know that in a rigorous setting isn't a fraction but is instead defined as the function: . Treating it as a fraction is technically considered an abuse of notation that is sometimes done as a shortcut for solving problems or explaining concepts. However, I have been trying to help another person with their intro to differential equations class and am confused on what the solution to the following problem is in a rigorous setting. Here is the problem (and solution): Such that (given in their notes): Then (according to their solution): Which in turn implies that: I' ve heard before that the and in integrals can be thought of a ""signaling"" which variable to integrate, but here you can see them being abused  as symbolic variables that are manipulated as if they were algebraic symbols. Not only that, but they are being manipulated as if there was an additive inverse for the , otherwise the first step of the solution wouldn't be possible. My friend is confused on what the professor is doing, and I can't in good faith right now explain to him why this is allowable, because this solution feels like a heavy abuse of notation, and I don't know what is happening conceptually that allows the solution. What allows you to treat and as real-valued elements of a fraction in this solution? In a rigorous setting, what is really going on that allows you to solve this problem for ?",dy \over dx \frac{d}{dx}y  dy - (y-1)^2 \space dx = 0   p(y) \space dy = g(x) \space dx   dy = (y-1)^2 \space dx \rightarrow \frac{1}{(y-1)^2} \space dy = dx \int (y-1)^{-2} \space dy = \int dx  = \frac{-1}{y-1} = x + c   \rightarrow y = 1 - \frac{1}{x+c} dy dx -dx dx dx dy y,"['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
37,Is Lebesgue integral a continuous functional in this context?,Is Lebesgue integral a continuous functional in this context?,,"Statement: I asked a similar question before which was closed. I tried to provide some context in this version. Let $p$ be a probability measure whose support is $\left[a,b\right]$ . Consider the set of continuous functions with a domain of $\left[a,b\right]$ and a codomain of $\left[c,d\right]$ , denoted by $\mathcal{F}$ . The reason I restrict the codomain is to ensure the dominated convergence theorem can be applied. Take $\int \cdot dp$ as a mapping from $\mathcal{F}$ to $\mathbb{R}$ . If I endow $\mathcal{F}$ with the topology of pointwise convergence and $\mathbb{R}$ with the standard topology, is then $\int \cdot dp$ a continuous functional on $\mathcal{F}$ ? Note that the integral and limit can be exchanged in this case. But since the topology of pointwise convergence is not metrizable, this is only a necessary condition for $\int \cdot dp$ to be continuous. I tried to go back to the definition of continuity for the general topological space, that is, the preimage of an open set is an open set. But that means when the value of the integral  is within a union of several open intervals, the value of the functions must be constrained in an open set at several points (I don't know how to describe this precisely but what I mean is the notion of subbasis for the topology of pointwise convergence on page 281 of Topology (2nd edition) by Munkres) and I have no idea how to verify that. I was wondering whether there are some ways to know whether $\int \cdot dp$ is a continuous functional or not.","Statement: I asked a similar question before which was closed. I tried to provide some context in this version. Let be a probability measure whose support is . Consider the set of continuous functions with a domain of and a codomain of , denoted by . The reason I restrict the codomain is to ensure the dominated convergence theorem can be applied. Take as a mapping from to . If I endow with the topology of pointwise convergence and with the standard topology, is then a continuous functional on ? Note that the integral and limit can be exchanged in this case. But since the topology of pointwise convergence is not metrizable, this is only a necessary condition for to be continuous. I tried to go back to the definition of continuity for the general topological space, that is, the preimage of an open set is an open set. But that means when the value of the integral  is within a union of several open intervals, the value of the functions must be constrained in an open set at several points (I don't know how to describe this precisely but what I mean is the notion of subbasis for the topology of pointwise convergence on page 281 of Topology (2nd edition) by Munkres) and I have no idea how to verify that. I was wondering whether there are some ways to know whether is a continuous functional or not.","p \left[a,b\right] \left[a,b\right] \left[c,d\right] \mathcal{F} \int \cdot dp \mathcal{F} \mathbb{R} \mathcal{F} \mathbb{R} \int \cdot dp \mathcal{F} \int \cdot dp \int \cdot dp","['real-analysis', 'general-topology', 'functional-analysis', 'probability-theory', 'lebesgue-integral']"
38,Real analysis: Dyadic squares and rigorous proof writing,Real analysis: Dyadic squares and rigorous proof writing,,"I have been working my way through an exercise from Pugh's ""Real Mathematical Analysis"" which asks me to prove that two dyadic squares in $\mathbb{R}^2$ of same size either intersect along a common edge, have a common vertex, are the same or are disjoint. It seems intuitively obvious to me (and I have made some progress in the proof as well), but I struggle with rigorously writing it down. Here's how far I got. Intuition/Explanation Dyadic cubes are fairly easy to explain: a dyadic cube in $\Bbb R^m$ is the cartesian product of $m$ intervals of same length, with the catch that the interval can be written as $[\frac{a}{2^k}, \frac{a+1}{2^k}]$ , with $a\in\mathbb{Z}$ and $k\in\mathbb{N}$ including $k = 0$ . So for example, $[\frac{a}{2^k}, \frac{a+1}{2^k}]\times[\frac{b}{2^k}, \frac{b+1}{2^k}]$ would be an example of a planar dyadic cube (dyadic square) as the length of both intervals is the same, namely $\frac{1}{2^k}$ . My idea has been that if there was a common point $x$ in any 2 different dyadic squares which is neither on a common edge nor vertex, then I could show that one of the squares would have their edge along a ""coordinate line"" which is not a dyadic number. To show what I mean, I attached the image below: Suppose the green square was an ordinary dyadic square. My assumption is that if there was a square such as the red one intersecting the green one, then there's no way coordinate line A (I don't know the right term I could use to describe the line) has a natural number as its value (again I don't know how to say this in mathematical terms, as that would imply that there was a integer between two adjacent integers. My question is: How could I put this into words? My progress so far Suppose there are 2 dyadic squares A, B of same size such that they are not the same, do not intersect along a common line or vertex. Let $M = \{x\in \mathbb{R}|x\in A \land x\in B\}$ . Since they intersect, they can intersect only in one of 2 possible ways (both drawn out below): Let $x$ be any point in $M$ . Consider the closest square side lying above $x$ . This side is between two dyadic rationals, namely $\frac{a}{2^k}$ and $\frac{a+1}{2^k}$ . This side lying between these two dyadic rationals would also be (by hypothesis) a dyadic rational (described in picture as $\frac{c}{2^k}$ , implying that $\frac{a}{2^k} < \frac{c}{2^k} < \frac{a+1}{2^k}$ , from which $a < c < a+1$ would follow, which is not possible for any $c\in \mathbb{Z}$ It is clear to me that there are some large gaps in this proof: How could I show that the only way the two squares could intersect is in the ways I've drawn out below? How could I show that it follows that there is a line lying above any point in the set M? Overall, how do I make this proof more ""mathematical""? Lastly, I ask for some advice - I often catch myself having an idea such as this one but then greatly struggle with putting the proof together. How do I get better at this / practice this skill in a more targeted fashion?","I have been working my way through an exercise from Pugh's ""Real Mathematical Analysis"" which asks me to prove that two dyadic squares in of same size either intersect along a common edge, have a common vertex, are the same or are disjoint. It seems intuitively obvious to me (and I have made some progress in the proof as well), but I struggle with rigorously writing it down. Here's how far I got. Intuition/Explanation Dyadic cubes are fairly easy to explain: a dyadic cube in is the cartesian product of intervals of same length, with the catch that the interval can be written as , with and including . So for example, would be an example of a planar dyadic cube (dyadic square) as the length of both intervals is the same, namely . My idea has been that if there was a common point in any 2 different dyadic squares which is neither on a common edge nor vertex, then I could show that one of the squares would have their edge along a ""coordinate line"" which is not a dyadic number. To show what I mean, I attached the image below: Suppose the green square was an ordinary dyadic square. My assumption is that if there was a square such as the red one intersecting the green one, then there's no way coordinate line A (I don't know the right term I could use to describe the line) has a natural number as its value (again I don't know how to say this in mathematical terms, as that would imply that there was a integer between two adjacent integers. My question is: How could I put this into words? My progress so far Suppose there are 2 dyadic squares A, B of same size such that they are not the same, do not intersect along a common line or vertex. Let . Since they intersect, they can intersect only in one of 2 possible ways (both drawn out below): Let be any point in . Consider the closest square side lying above . This side is between two dyadic rationals, namely and . This side lying between these two dyadic rationals would also be (by hypothesis) a dyadic rational (described in picture as , implying that , from which would follow, which is not possible for any It is clear to me that there are some large gaps in this proof: How could I show that the only way the two squares could intersect is in the ways I've drawn out below? How could I show that it follows that there is a line lying above any point in the set M? Overall, how do I make this proof more ""mathematical""? Lastly, I ask for some advice - I often catch myself having an idea such as this one but then greatly struggle with putting the proof together. How do I get better at this / practice this skill in a more targeted fashion?","\mathbb{R}^2 \Bbb R^m m [\frac{a}{2^k}, \frac{a+1}{2^k}] a\in\mathbb{Z} k\in\mathbb{N} k = 0 [\frac{a}{2^k}, \frac{a+1}{2^k}]\times[\frac{b}{2^k}, \frac{b+1}{2^k}] \frac{1}{2^k} x M = \{x\in \mathbb{R}|x\in A \land x\in B\} x M x \frac{a}{2^k} \frac{a+1}{2^k} \frac{c}{2^k} \frac{a}{2^k} < \frac{c}{2^k} < \frac{a+1}{2^k} a < c < a+1 c\in \mathbb{Z}","['real-analysis', 'geometry', 'solution-verification', 'proof-writing']"
39,$L^{p_0} \cap L^{p_1}$ and $L^{q_0} + L^{q_1}$ are duals of each other,and  are duals of each other,L^{p_0} \cap L^{p_1} L^{q_0} + L^{q_1},"This is from Exercise 26, Chapter 1, in Stein and Shakarchi's Functional Analysis. Suppose $1 < p_0, p_1 < \infty$ and $1/p_0+ 1/q_0 = 1$ and $1/p_1 + 1/q_1 = 1$ . Show that the Banach spaces $L^{p_0} \cap L^{p_1}$ and $L^{q_0} + L^{q_1}$ are duals of each other up to an equivalence of norms. Below are the definitions of $L^{p_0} \cap L^{p_1}$ and $L^{p_0} + L^{p_1}$ . Define the norm of $f \in L^{p_0} \cap L^{p_1}$ as $$\|f\|_{L^{p_0} \cap L^{p_1}} = \|f\|_{L^{p_0}} + \|f\|_{L^{p_1}}.$$ $L^{p_0}+L^{p_1}$ is defined as the vector space of measurable functions $f$ on a measure space $X$ , that can be written as a sum $f=f_0+f_1$ with $f_0\in L^{p_0}$ and $f_1\in L^{p_1}$ . Define $$\|f\|_{L^{p_0}+L^{p_1}}=\inf\big\{\|f_0\|_{L^{p_0}}+\|f_1\|_{L^{p_1}}\big\},$$ where the infimum is taken over all decomposition $f=f_0 + f_1$ with $f_0\in L^{p_0}$ and $f_1\in L^{p_1}$ . What is meant by ""dual space"" is as follows. For every bounded linear functional $l$ on $L^{p_0}+L^{p_1}$ there is a unique $g \in L^{q_0} \cap L^{q_1}$ so that $$l(f) = \int_X f(x)g(x) d\mu(x), \quad \text{for all $f \in L^{p_0}+L^{p_1}$}$$ Moreover, $\|l\|_{(L^{p_0}+L^{p_1})*} = \| g \|_{L^{p_0} \cap L^{p_1}}$ . By modifying Lemma 4.2 in the textbook, it is not too hard to prove the case above, i.e., $L^{p_0} \cap L^{p_1}$ is the dual space of $L^{p_0} + L^{p_1}$ . I have trouble to prove the opposite, that $L^{p_0} + L^{p_1}$ is the dual space of $L^{p_0} \cap L^{p_1}$ .","This is from Exercise 26, Chapter 1, in Stein and Shakarchi's Functional Analysis. Suppose and and . Show that the Banach spaces and are duals of each other up to an equivalence of norms. Below are the definitions of and . Define the norm of as is defined as the vector space of measurable functions on a measure space , that can be written as a sum with and . Define where the infimum is taken over all decomposition with and . What is meant by ""dual space"" is as follows. For every bounded linear functional on there is a unique so that Moreover, . By modifying Lemma 4.2 in the textbook, it is not too hard to prove the case above, i.e., is the dual space of . I have trouble to prove the opposite, that is the dual space of .","1 < p_0, p_1 < \infty 1/p_0+ 1/q_0 = 1 1/p_1 + 1/q_1 = 1 L^{p_0} \cap L^{p_1} L^{q_0} + L^{q_1} L^{p_0} \cap L^{p_1} L^{p_0} + L^{p_1} f \in L^{p_0} \cap L^{p_1} \|f\|_{L^{p_0} \cap L^{p_1}} = \|f\|_{L^{p_0}} + \|f\|_{L^{p_1}}. L^{p_0}+L^{p_1} f X f=f_0+f_1 f_0\in L^{p_0} f_1\in L^{p_1} \|f\|_{L^{p_0}+L^{p_1}}=\inf\big\{\|f_0\|_{L^{p_0}}+\|f_1\|_{L^{p_1}}\big\}, f=f_0 + f_1 f_0\in L^{p_0} f_1\in L^{p_1} l L^{p_0}+L^{p_1} g \in L^{q_0} \cap L^{q_1} l(f) = \int_X f(x)g(x) d\mu(x), \quad \text{for all f \in L^{p_0}+L^{p_1}} \|l\|_{(L^{p_0}+L^{p_1})*} = \| g \|_{L^{p_0} \cap L^{p_1}} L^{p_0} \cap L^{p_1} L^{p_0} + L^{p_1} L^{p_0} + L^{p_1} L^{p_0} \cap L^{p_1}","['real-analysis', 'functional-analysis', 'banach-spaces', 'lp-spaces']"
40,"On the existence of infinitely many linearly independent solutions for a non-linear IVP $y'=f(t,y),~y(t_0)=y_0$.",On the existence of infinitely many linearly independent solutions for a non-linear IVP .,"y'=f(t,y),~y(t_0)=y_0","Consider the IVP $$ \begin{cases} y'=f(t,y),\\ y(t_0)=y_0 \end{cases} \label{1}\tag{$\ast$} $$ Case $1$ : $f$ is Lipschitz w.r.t $y$ and continuous w.r.t $t$ in a vertical (infinite) strip $[a,b]\times \Bbb R$ containing the point $(t_0,y_0)$ . Here the existence and uniqueness of the solution on the interval $[a,b]$ is guaranteed by Picards Theorem. Case $2$ : $f$ looses the Lipschitz continuity w.r.t $y$ near the point $(t_0,y_0)$ like $f(t,y)=4y^{3/4},\sqrt y,... $ (take $y_0=0$ ). Mostly, I have seen infinitely many linearly independent solutions for such non linear $f$ . Doubts: i. Can we conclude that if there exists two linearly independent solutions for \eqref{1}, then there will be infinitely many linearly independent solutions. If so, how to justify the claim? ii. Any other conditions required to ensure there will be infinitely many linearly independent solutions for \eqref{1}?","Consider the IVP Case : is Lipschitz w.r.t and continuous w.r.t in a vertical (infinite) strip containing the point . Here the existence and uniqueness of the solution on the interval is guaranteed by Picards Theorem. Case : looses the Lipschitz continuity w.r.t near the point like (take ). Mostly, I have seen infinitely many linearly independent solutions for such non linear . Doubts: i. Can we conclude that if there exists two linearly independent solutions for \eqref{1}, then there will be infinitely many linearly independent solutions. If so, how to justify the claim? ii. Any other conditions required to ensure there will be infinitely many linearly independent solutions for \eqref{1}?","
\begin{cases}
y'=f(t,y),\\
y(t_0)=y_0
\end{cases}
\label{1}\tag{\ast}
 1 f y t [a,b]\times \Bbb R (t_0,y_0) [a,b] 2 f y (t_0,y_0) f(t,y)=4y^{3/4},\sqrt y,...  y_0=0 f","['real-analysis', 'ordinary-differential-equations', 'initial-value-problems']"
41,Limit measure is zero?,Limit measure is zero?,,"Let $E\subset \mathbb{R}$ be a Borel set and suppose that $(f_k)_{k\in\mathbb{N}}$ is a sequence of Lebesgue measurable functions defined on $E$ . Assume that $\lim_{k\to+\infty}f_k(x)=f(x)$ for almost every $x$ and that there exists a Lebesgue integrable function $g$ such that $|f_k(x)|\leq g(x)$ for almost every $x\in E$ and every $k\in\mathbb{N}$ . Fix $\varepsilon >0$ and define $A_k^\varepsilon=\{x\in E : |f_k(x)-f(x)|\geq\varepsilon\}$ . Is it true that $$ \lim_{j\to+\infty}\left|\bigcup_{k\geq j} A_k^\varepsilon \right| =0 \quad ? $$ ( $|\cdot|$ is the Lebesgue measure) The only thing that I noticed is that $|A_k^\varepsilon|\xrightarrow[k\to+\infty]{}0$ , as a consequence of Lebesgue's dominated convergence theorem. Does anyone know if this is true or not (and why?)?","Let be a Borel set and suppose that is a sequence of Lebesgue measurable functions defined on . Assume that for almost every and that there exists a Lebesgue integrable function such that for almost every and every . Fix and define . Is it true that ( is the Lebesgue measure) The only thing that I noticed is that , as a consequence of Lebesgue's dominated convergence theorem. Does anyone know if this is true or not (and why?)?","E\subset \mathbb{R} (f_k)_{k\in\mathbb{N}} E \lim_{k\to+\infty}f_k(x)=f(x) x g |f_k(x)|\leq g(x) x\in E k\in\mathbb{N} \varepsilon >0 A_k^\varepsilon=\{x\in E : |f_k(x)-f(x)|\geq\varepsilon\} 
\lim_{j\to+\infty}\left|\bigcup_{k\geq j} A_k^\varepsilon \right| =0 \quad ?
 |\cdot| |A_k^\varepsilon|\xrightarrow[k\to+\infty]{}0","['real-analysis', 'limits', 'measure-theory', 'lebesgue-measure']"
42,Young's and Peter-Paul's inequalities,Young's and Peter-Paul's inequalities,,"Following the idea from Jarchow 1981, pp. 47–55, let's retell the whole story in case some wishes to be complete. Let $1<p<\infty$ and $q$ such that $p+q=pq$ , or $$ \frac{1}{p}+\frac{1}{q}=1 $$ Let a real-valued function $f$ of the positive real number $t>0$ be defined as $$ f(t):=\frac{t^{p} a^{p}}{p}+\frac{t^{-q}b^{q}}{q}\label{1}\tag{1} $$ By setting the derivative over $t$ equal to zero, it can be found that $$ t=(a^{-p}b^q)^{\frac{1}{p+q}} $$ Plugging it into \eqref{1}, then the minimum of the function $f(t)$ can be found $$ \begin{align} f(t)&=\frac{t^{p}a^{p}}{p}+\frac{t^{-q}b^{q}}{q}\\ &\ge\frac{(a^{-p}b^q)^{\frac{p}{p+q}}a^{p}}{p}+\frac{(a^{-p}b^q)^{\frac{-q}{p+q}}b^{q}}{q}\\ &=\frac{a^{\frac{pq}{p+q}}b}{p}+\frac{ab^{\frac{pq}{p+q}}}{q}=ab\left(\frac{1}{p}+\frac{1}{q}\right)=ab. \end{align} $$ This is all right until I need to find what relation can be built between $t$ and $\varepsilon$ in the so-called ""Peter–Paul"" inequality \begin{align} ab\le \frac{a^2}{2\varepsilon}+\frac{\varepsilon b^2}{2} \end{align} since I will not be able to cancel the auxiliary parameter $t$ after using AM-GM. What on earth $p$ must be equal to $q$ ?","Following the idea from Jarchow 1981, pp. 47–55, let's retell the whole story in case some wishes to be complete. Let and such that , or Let a real-valued function of the positive real number be defined as By setting the derivative over equal to zero, it can be found that Plugging it into \eqref{1}, then the minimum of the function can be found This is all right until I need to find what relation can be built between and in the so-called ""Peter–Paul"" inequality since I will not be able to cancel the auxiliary parameter after using AM-GM. What on earth must be equal to ?","1<p<\infty q p+q=pq 
\frac{1}{p}+\frac{1}{q}=1
 f t>0 
f(t):=\frac{t^{p} a^{p}}{p}+\frac{t^{-q}b^{q}}{q}\label{1}\tag{1}
 t 
t=(a^{-p}b^q)^{\frac{1}{p+q}}
 f(t) 
\begin{align}
f(t)&=\frac{t^{p}a^{p}}{p}+\frac{t^{-q}b^{q}}{q}\\
&\ge\frac{(a^{-p}b^q)^{\frac{p}{p+q}}a^{p}}{p}+\frac{(a^{-p}b^q)^{\frac{-q}{p+q}}b^{q}}{q}\\
&=\frac{a^{\frac{pq}{p+q}}b}{p}+\frac{ab^{\frac{pq}{p+q}}}{q}=ab\left(\frac{1}{p}+\frac{1}{q}\right)=ab.
\end{align}
 t \varepsilon \begin{align}
ab\le \frac{a^2}{2\varepsilon}+\frac{\varepsilon b^2}{2}
\end{align} t p q",['real-analysis']
43,Show that $\frac{d}{dx}\tan x=\sec^{2}x$ (by using infinitesimal approach to the derivative),Show that  (by using infinitesimal approach to the derivative),\frac{d}{dx}\tan x=\sec^{2}x,"I couldn't prove the derivative of $\tan x$ by using the infinitesimal approach to the derivatives. Here's my solution, but I stuck at the last step: \begin{align} \tag{1}\frac{\mathrm{d} }{\mathrm{d} x}\tan x &= \frac{\tan (x+dx)- \tan x}{dx} \\ \tag{2} &=\frac{\frac{\tan x + \tan dx}{1-\tan x . \tan dx}-\tan x}{dx}\\ \tag{3}&=\frac{\frac{\tan x + dx - \tan x + dx\tan^2 x}{1 - dx\tan x}}{dx}\\ \tag{4} &=\frac{dx(1+\tan^2 x)}{1 - dx\tan x} \cdot \frac{1}{dx}=\frac{1+ \tan^2 x}{1-dx\tan x}\\ \tag{5} &=\frac{\sec^2 x}{1-dx\tan x}\neq \sec^2 x.  \end{align}","I couldn't prove the derivative of by using the infinitesimal approach to the derivatives. Here's my solution, but I stuck at the last step:","\tan x \begin{align}
\tag{1}\frac{\mathrm{d} }{\mathrm{d} x}\tan x &= \frac{\tan (x+dx)- \tan x}{dx} \\
\tag{2} &=\frac{\frac{\tan x + \tan dx}{1-\tan x . \tan dx}-\tan x}{dx}\\
\tag{3}&=\frac{\frac{\tan x + dx - \tan x + dx\tan^2 x}{1 - dx\tan x}}{dx}\\
\tag{4} &=\frac{dx(1+\tan^2 x)}{1 - dx\tan x} \cdot \frac{1}{dx}=\frac{1+ \tan^2 x}{1-dx\tan x}\\
\tag{5} &=\frac{\sec^2 x}{1-dx\tan x}\neq \sec^2 x. 
\end{align}","['real-analysis', 'calculus', 'derivatives', 'trigonometry', 'infinitesimals']"
44,Invariant subspace problem for $\ell^2(\mathbb{N})$,Invariant subspace problem for,\ell^2(\mathbb{N}),"Is the invariant subspace problem known for $\ell^2(\mathbb{N})$ or for more general $L^2$ spaces, i.e. does every bounded linear operator $T \colon \ell^2(\mathbb{N}) \to \ell^2(\mathbb{N})$ have a non-trivial (closed) T-invariant subspace?","Is the invariant subspace problem known for or for more general spaces, i.e. does every bounded linear operator have a non-trivial (closed) T-invariant subspace?",\ell^2(\mathbb{N}) L^2 T \colon \ell^2(\mathbb{N}) \to \ell^2(\mathbb{N}),"['real-analysis', 'functional-analysis', 'analysis', 'invariant-subspace']"
45,Prove $\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx=\gamma^2 -2\gamma+\zeta(2)$,Prove,\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx=\gamma^2 -2\gamma+\zeta(2),I was having trouble with this integral Prove $$\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx=\gamma^2 -2\gamma+\zeta(2)$$ Where $\gamma$ is the euler mascheroni constant. Let $u=e^x\rightarrow \ln(u)= x$ thus $du=e^x dx$ $$\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx= \int_{0}^{\infty} u\ln^2(u) e^{-u}du$$ Now $$\Gamma(n)=\int_0^\infty x^{n-1}e^xdx$$ $$\Gamma'(n)=\int_0^\infty x^{n-1}e^x\ln(x)dx$$ $$\Gamma''(n)=\int_0^\infty x^{n-1}e^x\ln^2(x)dx\Rightarrow \Gamma''(2)=\int_0^\infty x e^x\ln^2(x)dx$$ How do I proceed from here? Do I have to use $\Gamma'(n)=\psi(n)\Gamma(n)$ ? How do I solve this integral ? Thank you for your time,I was having trouble with this integral Prove Where is the euler mascheroni constant. Let thus Now How do I proceed from here? Do I have to use ? How do I solve this integral ? Thank you for your time,\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx=\gamma^2 -2\gamma+\zeta(2) \gamma u=e^x\rightarrow \ln(u)= x du=e^x dx \int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx= \int_{0}^{\infty} u\ln^2(u) e^{-u}du \Gamma(n)=\int_0^\infty x^{n-1}e^xdx \Gamma'(n)=\int_0^\infty x^{n-1}e^x\ln(x)dx \Gamma''(n)=\int_0^\infty x^{n-1}e^x\ln^2(x)dx\Rightarrow \Gamma''(2)=\int_0^\infty x e^x\ln^2(x)dx \Gamma'(n)=\psi(n)\Gamma(n),"['real-analysis', 'calculus', 'integration', 'improper-integrals', 'riemann-zeta']"
46,Show that $f:\mathbb R^n\to \mathbb R$ defined as $f(x)=\inf_{y\in K}||x-y||$ continuous on $\mathbb R^n$,Show that  defined as  continuous on,f:\mathbb R^n\to \mathbb R f(x)=\inf_{y\in K}||x-y|| \mathbb R^n,"A question from my analysis course reads Let $K$ be a compact subset of $\mathbb R^n$ . Define $f:\mathbb R^n\to \mathbb R$ as $$f(x)=\text{d}(x,K)=\inf_{y\in K}||x-y||$$ Show that $f$ is continuous on $\mathbb R^n$ and $f^{-1}\left(\{0\}\right)=K$ . Is the compactness of $K$ necessary? Now, I can very well understand that the described function has to be continuous because $f(x)=0$ when $x\in K$ and as $x$ slowly moves out of $K$ , the distance increases continuously. But, of course, that's quite far away from a real mathematical proof, and as far as I understand, we need to use the property If $f:\mathbb A\to \mathbb B$ is continuous and $O\subset B$ is an open set, then $f^{-1}\left(O\right)$ is open in $\mathbb A$ to arrive at the formal proof. But, I'm quite new to functions whose domain is not $\mathbb R$ . Until now, I have only studied functions $f:\mathbb R\to \mathbb R$ . So, I'm a little confused about how to construct the proofs. Please help me to do it. I was asked in the comments whether I'm allowed to use the $\epsilon-\delta$ property to prove continuity. Well, it wasn't explicitly mentioned in the exercise that I need to use the preimage property, but this question came just a couple of pages after this property was introduced. That's why I assumed they expect me to use that technique. Although I will also appreciate a detailed $\epsilon-\delta$ proof, it would be better if I can have a proof using the preimage method .","A question from my analysis course reads Let be a compact subset of . Define as Show that is continuous on and . Is the compactness of necessary? Now, I can very well understand that the described function has to be continuous because when and as slowly moves out of , the distance increases continuously. But, of course, that's quite far away from a real mathematical proof, and as far as I understand, we need to use the property If is continuous and is an open set, then is open in to arrive at the formal proof. But, I'm quite new to functions whose domain is not . Until now, I have only studied functions . So, I'm a little confused about how to construct the proofs. Please help me to do it. I was asked in the comments whether I'm allowed to use the property to prove continuity. Well, it wasn't explicitly mentioned in the exercise that I need to use the preimage property, but this question came just a couple of pages after this property was introduced. That's why I assumed they expect me to use that technique. Although I will also appreciate a detailed proof, it would be better if I can have a proof using the preimage method .","K \mathbb R^n f:\mathbb R^n\to \mathbb R f(x)=\text{d}(x,K)=\inf_{y\in K}||x-y|| f \mathbb R^n f^{-1}\left(\{0\}\right)=K K f(x)=0 x\in K x K f:\mathbb A\to \mathbb B O\subset B f^{-1}\left(O\right) \mathbb A \mathbb R f:\mathbb R\to \mathbb R \epsilon-\delta \epsilon-\delta","['real-analysis', 'calculus', 'analysis', 'continuity', 'compactness']"
47,Weak convergence in $L^p$ implies strong convergence in $H^{-1}$,Weak convergence in  implies strong convergence in,L^p H^{-1},"Let $B(0,1) \subset \mathbb{R}^d$ and $p > d$ . Suppose $f_n$ weakly converges to $f$ in $L^p(B(0,1))$ . Prove that $f_n$ strongly converges to $f$ in $H^{-1}(B(0,1))$ . Here $H^{-1}(B(0,1))$ is the dual space of $H^1_0(B(0,1))$ with respect to $L^2-$ topology. I checked many books such as Evans' PDE, Brezis' FA or Mazya's Sobolev Spaces but didn't found this result. Any ideas to prove it? (References are also welcome). Thank you.","Let and . Suppose weakly converges to in . Prove that strongly converges to in . Here is the dual space of with respect to topology. I checked many books such as Evans' PDE, Brezis' FA or Mazya's Sobolev Spaces but didn't found this result. Any ideas to prove it? (References are also welcome). Thank you.","B(0,1) \subset \mathbb{R}^d p > d f_n f L^p(B(0,1)) f_n f H^{-1}(B(0,1)) H^{-1}(B(0,1)) H^1_0(B(0,1)) L^2-","['real-analysis', 'functional-analysis']"
48,How to evaluate $\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k^2-2n^2\right)}$,How to evaluate,\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k^2-2n^2\right)},"As seen in the title I'm interested in a way to evaluate $$\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k^2-2n^2\right)}$$ But I'm not sure what to do, I did attempt something but ended up with a wrong result $$\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k^2-2n^2\right)}=\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k-\sqrt{2}n\right)\left(k+\sqrt{2}n\right)}$$ then applying partial fraction decomposition \begin{align*} &=\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3\left(k-\sqrt{2}n\right)}+\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3\left(k+\sqrt{2}n\right)}+\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^2n^2}\\ &=\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3}\int _0^1x^{k-\sqrt{2}n-1}\:\mathrm{d}x+\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3}\int _0^1x^{k+\sqrt{2}n-1}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\ &=-\sum _{n=1}^{\infty }\int _0^1\operatorname{Li}_3\left(-x\right)x^{-\sqrt{2}n-1}\:\mathrm{d}x-\sum _{n=1}^{\infty }\int _0^1\operatorname{Li}_3\left(-x\right)x^{\sqrt{2}n-1}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\ &=\int _0^1\frac{\operatorname{Li}_3\left(-x\right)}{x\left(1-x^{\sqrt{2}}\right)}\:\mathrm{d}x-\int _0^1\frac{x^{\sqrt{2}}\operatorname{Li}_3\left(-x\right)}{x\left(1-x^{\sqrt{2}}\right)}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\ &=\int _0^1\frac{\operatorname{Li}_3\left(-x\right)}{x}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\ &=-\frac{7}{8}\zeta (4)+\frac{5}{4}\zeta (4)\\ &=\frac{3}{8}\zeta (4) \end{align*} Which seems to be a wrong answer due to numerical methods and approximations, what did I do wrong? how else could I approach this problem?","As seen in the title I'm interested in a way to evaluate But I'm not sure what to do, I did attempt something but ended up with a wrong result then applying partial fraction decomposition Which seems to be a wrong answer due to numerical methods and approximations, what did I do wrong? how else could I approach this problem?","\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k^2-2n^2\right)} \sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k^2-2n^2\right)}=\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{n^2\left(k-\sqrt{2}n\right)\left(k+\sqrt{2}n\right)} \begin{align*}
&=\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3\left(k-\sqrt{2}n\right)}+\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3\left(k+\sqrt{2}n\right)}+\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^2n^2}\\
&=\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3}\int _0^1x^{k-\sqrt{2}n-1}\:\mathrm{d}x+\sum _{n=1}^{\infty }\sum _{k=1}^{\infty }\frac{\left(-1\right)^{k-1}}{k^3}\int _0^1x^{k+\sqrt{2}n-1}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\
&=-\sum _{n=1}^{\infty }\int _0^1\operatorname{Li}_3\left(-x\right)x^{-\sqrt{2}n-1}\:\mathrm{d}x-\sum _{n=1}^{\infty }\int _0^1\operatorname{Li}_3\left(-x\right)x^{\sqrt{2}n-1}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\
&=\int _0^1\frac{\operatorname{Li}_3\left(-x\right)}{x\left(1-x^{\sqrt{2}}\right)}\:\mathrm{d}x-\int _0^1\frac{x^{\sqrt{2}}\operatorname{Li}_3\left(-x\right)}{x\left(1-x^{\sqrt{2}}\right)}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\
&=\int _0^1\frac{\operatorname{Li}_3\left(-x\right)}{x}\:\mathrm{d}x+\frac{5}{4}\zeta (4)\\
&=-\frac{7}{8}\zeta (4)+\frac{5}{4}\zeta (4)\\
&=\frac{3}{8}\zeta (4)
\end{align*}","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
49,A law of large numbers for bounded processes,A law of large numbers for bounded processes,,"Let $(\Omega,\mathcal F,P)$ be a probability space and let $([0,1],\mathcal B[0,1],\lambda$ ) be the unit interval with Lebesgue measure on the Borel subsets of $[0,1]$ . Let $f:[0,1]\times \Omega \to \mathbb R$ be $\mathcal B[0,1]\otimes \mathcal F$ measurable and bounded. I want to show the existence of a subsequence $(n_k)$ such that $$\frac{1}{n_k}\sum_{i=1}^{n_k} f(t_i,\omega) \overset{L^1(\Omega,\mathcal F,P)}\to \int f(t,\omega)d\lambda \quad \text{as } k\to \infty $$ for $\lambda^\infty$ -almost every sequence $(t_i)\subset [0,1]$ , where $\lambda^\infty$ denotes the product measure on the $\sigma$ -algebra $\mathcal B^\infty:=\bigotimes_{n=1}^\infty \mathcal B[0,1]$ . I tried the following: Fix $\omega\in\Omega$ . For each $j\geq1$ , let $((t_i),\omega)\mapsto f(t_j,\omega)$ denote the composition of the maps $((t_i),\omega)\mapsto (t_j,\omega)$ and $(t,\omega) \mapsto f(t,\omega)$ . Since the $j$ th projection map $(t_i)\mapsto t_j$ on $[0,1]^{\infty}:=\prod_{i=1}^{\infty} [0,1]$ is $\mathcal B^\infty$ measurable, we see that $f(t_j,\omega)$ is $\mathcal B^\infty\otimes \mathcal F$ measurable. Moreover, by construction of the product measure $\lambda^\infty$ , the projection maps are i.i.d. random variables on $([0,1]^{\infty},\mathcal B^{\infty},\lambda^{\infty})$ , and so by the SLLN we get $$\frac{1}{n}\sum_{i=1}^n f(t_i,\omega) \overset{\lambda^\infty\text{-a.s}}\to \int f(t,\omega)d\lambda \quad \text{as } n\to \infty $$ for each $\omega\in \Omega$ . For each $n\geq 1$ , define the map $$((t_i),\omega)\mapsto S^n((t_i),\omega):=\frac{1}{n}\sum_{i=1}^n f(t_i,\omega) -\int f(t,\omega)d\lambda.$$ By Fubini's theorem $\omega\to \int f(t,\omega)d\lambda$ is $\mathcal F$ -measurable, and so $S^n((t_i),\omega)$ is seen to be $\mathcal B^\infty\otimes \mathcal F$ measurable. Also $S^n((t_i),\omega)$ is bounded since $f$ is bounded. Therefore the DCT gives $$\int |S^n((t_i),\omega)| d\lambda^{\infty} \to 0  \quad \text{as } n\to \infty$$ for each $\omega\in \Omega$ . By Fubini's theorem, these integrals are $\mathcal F$ -measurable functions of $\omega$ , and they are bounded, and so another application of the DCT gives $$\int \int |S^n((t_i),\omega)| d\lambda^{\infty}dP \to 0  \quad \text{as } n\to \infty$$ We can interchange the order of integration by Fubini's theorem to obtain $$\int \int |S^n((t_i),\omega)| dPd\lambda^{\infty} \to 0  \quad \text{as } n\to \infty$$ This says that $\int |S^n((t_i),\omega)| dP \to 0 $ as $n\to \infty $ w.r.t. the $L^1([0,1]^{\infty} ,\mathcal B^{\infty},\lambda^{\infty})$ norm. We can extract a subsequence $(n_k)$ converging $\lambda^{\infty}$ -almost surely, i.e. such that $$\frac{1}{n_k}\sum_{i=1}^{n_k} f(t_i,\omega) \overset{L^1(\Omega,\mathcal F,P)}\to \int f(t,\omega)d\lambda \quad \text{as } k\to \infty $$ for $\lambda^\infty$ -almost every sequence $(t_i)\subset [0,1]$ . Is this reasoning correct? Thanks a lot for your help.","Let be a probability space and let ) be the unit interval with Lebesgue measure on the Borel subsets of . Let be measurable and bounded. I want to show the existence of a subsequence such that for -almost every sequence , where denotes the product measure on the -algebra . I tried the following: Fix . For each , let denote the composition of the maps and . Since the th projection map on is measurable, we see that is measurable. Moreover, by construction of the product measure , the projection maps are i.i.d. random variables on , and so by the SLLN we get for each . For each , define the map By Fubini's theorem is -measurable, and so is seen to be measurable. Also is bounded since is bounded. Therefore the DCT gives for each . By Fubini's theorem, these integrals are -measurable functions of , and they are bounded, and so another application of the DCT gives We can interchange the order of integration by Fubini's theorem to obtain This says that as w.r.t. the norm. We can extract a subsequence converging -almost surely, i.e. such that for -almost every sequence . Is this reasoning correct? Thanks a lot for your help.","(\Omega,\mathcal F,P) ([0,1],\mathcal B[0,1],\lambda [0,1] f:[0,1]\times \Omega \to \mathbb R \mathcal B[0,1]\otimes \mathcal F (n_k) \frac{1}{n_k}\sum_{i=1}^{n_k} f(t_i,\omega) \overset{L^1(\Omega,\mathcal F,P)}\to \int f(t,\omega)d\lambda \quad \text{as } k\to \infty  \lambda^\infty (t_i)\subset [0,1] \lambda^\infty \sigma \mathcal B^\infty:=\bigotimes_{n=1}^\infty \mathcal B[0,1] \omega\in\Omega j\geq1 ((t_i),\omega)\mapsto f(t_j,\omega) ((t_i),\omega)\mapsto (t_j,\omega) (t,\omega) \mapsto f(t,\omega) j (t_i)\mapsto t_j [0,1]^{\infty}:=\prod_{i=1}^{\infty} [0,1] \mathcal B^\infty f(t_j,\omega) \mathcal B^\infty\otimes \mathcal F \lambda^\infty ([0,1]^{\infty},\mathcal B^{\infty},\lambda^{\infty}) \frac{1}{n}\sum_{i=1}^n f(t_i,\omega) \overset{\lambda^\infty\text{-a.s}}\to \int f(t,\omega)d\lambda \quad \text{as } n\to \infty  \omega\in \Omega n\geq 1 ((t_i),\omega)\mapsto S^n((t_i),\omega):=\frac{1}{n}\sum_{i=1}^n f(t_i,\omega) -\int f(t,\omega)d\lambda. \omega\to \int f(t,\omega)d\lambda \mathcal F S^n((t_i),\omega) \mathcal B^\infty\otimes \mathcal F S^n((t_i),\omega) f \int |S^n((t_i),\omega)| d\lambda^{\infty} \to 0  \quad \text{as } n\to \infty \omega\in \Omega \mathcal F \omega \int \int |S^n((t_i),\omega)| d\lambda^{\infty}dP \to 0  \quad \text{as } n\to \infty \int \int |S^n((t_i),\omega)| dPd\lambda^{\infty} \to 0  \quad \text{as } n\to \infty \int |S^n((t_i),\omega)| dP \to 0  n\to \infty  L^1([0,1]^{\infty} ,\mathcal B^{\infty},\lambda^{\infty}) (n_k) \lambda^{\infty} \frac{1}{n_k}\sum_{i=1}^{n_k} f(t_i,\omega) \overset{L^1(\Omega,\mathcal F,P)}\to \int f(t,\omega)d\lambda \quad \text{as } k\to \infty  \lambda^\infty (t_i)\subset [0,1]","['real-analysis', 'probability-theory', 'measure-theory', 'lebesgue-measure', 'law-of-large-numbers']"
50,"To prove : $\lim_{n \to +\infty} a_{n} $ diverges, where $a_{n}= (-1)^n$ from $\varepsilon$ definition","To prove :  diverges, where  from  definition",\lim_{n \to +\infty} a_{n}  a_{n}= (-1)^n \varepsilon,"How to prove : $\lim_{n \to +\infty} a_{n}$ diverges, where $a_{n}= (-1)^n$ from $\varepsilon$ definition. My way : From definition : $a = \lim_{n \to +\infty} a_{n} = \forall \varepsilon>0 \exists N(\varepsilon) \in \mathbb{N} : \forall n \geq N(\varepsilon) \implies \mid a_{n} -a \mid < \varepsilon$ ---(1) Taking the negation of (1) statement we get : $a \neq  \lim_{n \to +\infty} a_{n} = \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n \geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---(2) We've to prove (2). We've to divide the statement (2) into two parts : (i) $\forall a \geq 0 \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n = 2N+1\geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---2(a) (ii) $\forall a < 0 \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n = 2N\geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---2(b) From 2(a) and 2(b) we get : $\forall a \mathbb{R} \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n \geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---(3) But, I'm confused to choose the proper $\varepsilon$ so that my work would be true. I'm stuck here. Please help me. My previously asked the same question got downvoted due to lack of enough data, so I deleted the previous post and asked again with my detailed work.","How to prove : diverges, where from definition. My way : From definition : ---(1) Taking the negation of (1) statement we get : ---(2) We've to prove (2). We've to divide the statement (2) into two parts : (i) ---2(a) (ii) ---2(b) From 2(a) and 2(b) we get : ---(3) But, I'm confused to choose the proper so that my work would be true. I'm stuck here. Please help me. My previously asked the same question got downvoted due to lack of enough data, so I deleted the previous post and asked again with my detailed work.",\lim_{n \to +\infty} a_{n} a_{n}= (-1)^n \varepsilon a = \lim_{n \to +\infty} a_{n} = \forall \varepsilon>0 \exists N(\varepsilon) \in \mathbb{N} : \forall n \geq N(\varepsilon) \implies \mid a_{n} -a \mid < \varepsilon a \neq  \lim_{n \to +\infty} a_{n} = \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n \geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon \forall a \geq 0 \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n = 2N+1\geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon \forall a < 0 \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n = 2N\geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon \forall a \mathbb{R} \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n \geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon \varepsilon,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'epsilon-delta']"
51,"Showing the sequence $\sqrt{2}, \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}, \dots$ tends to $2$ using the Epsilon-Neighbourhood definition",Showing the sequence  tends to  using the Epsilon-Neighbourhood definition,"\sqrt{2}, \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}, \dots 2","I am given the sequence $$a_{n+1} = \sqrt{2 a_n}, \quad a_1 = \sqrt{2}.$$ That is, the sequence $$ \sqrt{2}, \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}, \dots$$ I am aware of using the recursive method to find the limit: setting $x = \sqrt{2x}$ and getting $x = 2$ , rejecting $x = 0$ due to the sequence being monotone and positive. I now want to show that the limit of this sequence is $2$ using the epsilon-neighbourhood definition for a convergent series: Let $(a_n)$ be a sequence that converges to a real number $a$ . Then, for every number $\epsilon > 0$ , there exists a number $N \in \mathbb{N}$ such that for all $n \geq N$ , $|a_n - a| < \epsilon$ . I take the logarithm on both sides and arrive at $$ \left| \left( \frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^n} \right) \ln 2 - \ln 2 \right|  < \epsilon. $$ Using triangle inequality, $|x - y| \geq |x| - |y|$ , I arrive at $$ \left| \left( \frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^n} \right) \right| - |\ln 2| < \epsilon. $$ Add $\ln 2$ to both sides $$ \left| \left( \frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^n} \right) \right| < \epsilon + \ln 2. $$ The left hand side can be rewritten as a sum of $n$ terms in a geometric series with first term $\frac{1}{2}$ and common ratio $\frac{1}{2}$ , $$ \frac{\frac{1}{2} \left( 1 - \left( \frac{1}{2} \right)^n \right)}{1 - \frac{1}{2}} = 1 - \frac{1}{2^n} < \epsilon + \ln 2. $$ I am unsure of how to proceed beyond this point.","I am given the sequence That is, the sequence I am aware of using the recursive method to find the limit: setting and getting , rejecting due to the sequence being monotone and positive. I now want to show that the limit of this sequence is using the epsilon-neighbourhood definition for a convergent series: Let be a sequence that converges to a real number . Then, for every number , there exists a number such that for all , . I take the logarithm on both sides and arrive at Using triangle inequality, , I arrive at Add to both sides The left hand side can be rewritten as a sum of terms in a geometric series with first term and common ratio , I am unsure of how to proceed beyond this point.","a_{n+1} = \sqrt{2 a_n}, \quad a_1 = \sqrt{2}.  \sqrt{2}, \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}, \dots x = \sqrt{2x} x = 2 x = 0 2 (a_n) a \epsilon > 0 N \in \mathbb{N} n \geq N |a_n - a| < \epsilon  \left| \left( \frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^n} \right) \ln 2 - \ln 2 \right|  < \epsilon.  |x - y| \geq |x| - |y|  \left| \left( \frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^n} \right) \right| - |\ln 2| < \epsilon.  \ln 2  \left| \left( \frac{1}{2} + \frac{1}{4} + \cdots + \frac{1}{2^n} \right) \right| < \epsilon + \ln 2.  n \frac{1}{2} \frac{1}{2}  \frac{\frac{1}{2} \left( 1 - \left( \frac{1}{2} \right)^n \right)}{1 - \frac{1}{2}} = 1 - \frac{1}{2^n} < \epsilon + \ln 2. ","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
52,Follow-up Question about Cesaro mean proof,Follow-up Question about Cesaro mean proof,,"I am trying to understand the proof behind the Cesaro mean converging. I am using https://math.stackexchange.com/a/2342856/633922 (hopefully it is also correct) as a guide because it seems very direct. I will comment on the steps I understand and where I need help. The statement: If $(x_n)$ converges to $x$ , the sum of averages $y_n=\dfrac{x_1+x_2+\cdots+x_n}{n}$ also converges to the same limit. Proof: Since $(x_n)$ converges, given an arbitrary $\epsilon >0$ , there exists an $N_1\in\mathbb{N}$ such that whenever $n\geq N_1$ we have $|x_n-x|<\epsilon$ . (Definition of convergent sequence) Now, $$\begin{align*} \left|\frac{x_1+x_2+\cdots+x_n}{n}-x\right|=&\left|\frac{(x_1-x)+\cdots+(x_{N_1-1}-x)}{n}+\frac{(x_{N_1}-x)+\cdots+(x_{n}-x)}{n}\right|\\ \leq& \left|\frac{(x_1-x)+\cdots+(x_{N_1-1}-x)}{n}\right|+\left|\frac{(x_{N_1}-x)+\cdots+(x_{n}-x)}{n}\right|\text{ (Triangle inequality)} \end{align*} $$ Now we want to make a statement about the first $N_1-1$ terms, $\color{red}{why?}$ That is: By the Archimedean principle we can find an $N_2$ such that whenever $n\geq N_2$ we have that $$\left|\frac{x_1+x_2+\cdots+x_{N-1}}{n}\right|<\epsilon $$ (Thought: is it because $x_1,\dots,x_{N_1-1}$ is finite?) Now we can choose an $N_3=\max\{N_1,N2\}$ such that for all $n\geq N_3$ we have (My thought: Is this because choosing the max of both will always guarantee the final inequality to always work?) $$ \left|\frac{x_1+x_2+\cdots+x_n}{n}-x\right|\leq \underbrace{\epsilon}_{N_1-1}+\underbrace{\color{red}{\frac{n-N_1}{n}}}_{\text{why and how?}}\epsilon< 2\epsilon $$ And this finishes the proof. I always assumed the ending statement has to be (something) $<\epsilon$ or is this saying that each sum of the right side of the triangle inequality is less than $\epsilon/2$ . I would really appreciate the help on the areas I am thoroughly confused about.","I am trying to understand the proof behind the Cesaro mean converging. I am using https://math.stackexchange.com/a/2342856/633922 (hopefully it is also correct) as a guide because it seems very direct. I will comment on the steps I understand and where I need help. The statement: If converges to , the sum of averages also converges to the same limit. Proof: Since converges, given an arbitrary , there exists an such that whenever we have . (Definition of convergent sequence) Now, Now we want to make a statement about the first terms, That is: By the Archimedean principle we can find an such that whenever we have that (Thought: is it because is finite?) Now we can choose an such that for all we have (My thought: Is this because choosing the max of both will always guarantee the final inequality to always work?) And this finishes the proof. I always assumed the ending statement has to be (something) or is this saying that each sum of the right side of the triangle inequality is less than . I would really appreciate the help on the areas I am thoroughly confused about.","(x_n) x y_n=\dfrac{x_1+x_2+\cdots+x_n}{n} (x_n) \epsilon >0 N_1\in\mathbb{N} n\geq N_1 |x_n-x|<\epsilon \begin{align*}
\left|\frac{x_1+x_2+\cdots+x_n}{n}-x\right|=&\left|\frac{(x_1-x)+\cdots+(x_{N_1-1}-x)}{n}+\frac{(x_{N_1}-x)+\cdots+(x_{n}-x)}{n}\right|\\
\leq& \left|\frac{(x_1-x)+\cdots+(x_{N_1-1}-x)}{n}\right|+\left|\frac{(x_{N_1}-x)+\cdots+(x_{n}-x)}{n}\right|\text{ (Triangle inequality)}
\end{align*}
 N_1-1 \color{red}{why?} N_2 n\geq N_2 \left|\frac{x_1+x_2+\cdots+x_{N-1}}{n}\right|<\epsilon
 x_1,\dots,x_{N_1-1} N_3=\max\{N_1,N2\} n\geq N_3 
\left|\frac{x_1+x_2+\cdots+x_n}{n}-x\right|\leq \underbrace{\epsilon}_{N_1-1}+\underbrace{\color{red}{\frac{n-N_1}{n}}}_{\text{why and how?}}\epsilon< 2\epsilon
 <\epsilon \epsilon/2","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'cesaro-summable']"
53,Lipschitz implies bounded gradient with any norm?,Lipschitz implies bounded gradient with any norm?,,"Assume I have a differentiable function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ that is $L$ -Lipschitz with respect to a norm $\|\cdot\|$ on $\mathbb{R}^n$ : $$ \forall x, y \in \mathbb{R}^n, |f(x) - f(y)| \le L\|x-y\|.  $$ I know that if $f$ is further assumed to be convex, and $\|\cdot\|$ is the euclidian norm, then convexity implies $$ L\|\nabla f(x)\|_2 \ge f(x + \nabla f(x)) - f(x) \ge \langle \nabla f(x), \nabla f(x)\rangle = \| \nabla f(x)\|_2^2, $$ thus $\|\nabla f(x)\|_2 \le L$ . But does $\|\nabla f(x)\| \le L$ still hold if $\|\cdot\|$ is any norm on $\mathbb{R}^n$ ? And if $f$ is only assumed differentiable?","Assume I have a differentiable function that is -Lipschitz with respect to a norm on : I know that if is further assumed to be convex, and is the euclidian norm, then convexity implies thus . But does still hold if is any norm on ? And if is only assumed differentiable?","f: \mathbb{R}^n \rightarrow \mathbb{R} L \|\cdot\| \mathbb{R}^n  \forall x, y \in \mathbb{R}^n, |f(x) - f(y)| \le L\|x-y\|.   f \|\cdot\|  L\|\nabla f(x)\|_2 \ge f(x + \nabla f(x)) - f(x) \ge \langle \nabla f(x), \nabla f(x)\rangle = \| \nabla f(x)\|_2^2,  \|\nabla f(x)\|_2 \le L \|\nabla f(x)\| \le L \|\cdot\| \mathbb{R}^n f","['real-analysis', 'derivatives', 'lipschitz-functions']"
54,How to determine if a function is differentiable,How to determine if a function is differentiable,,"Question Determine the values of $a$ and $b$ such that the following function is differentiable at 0. $$f(x) = \begin{cases} ax^3cos(\frac 1 x) + bx + b, & \text{if }x \lt 0 \\ \sqrt{a + bx}, & \text{if }x \geq 0 \end{cases}$$ My solution For $f$ to be differentiable at $0$ , $f$ must first be continuous at $0$ . $$\implies \lim\limits_{x\to0^-}f(x) = \lim\limits_{x\to0^+}f(x)$$ Consider \begin{align} \lim\limits_{x\to0^-}f(x) & = \lim\limits_{x\to0^-}[ax^3cos(\frac 1 x) + bx + b]. \\[5 mm] \because \lim\limits_{x\to0^-}ax^3cos(\frac 1 x) & = \lim\limits_{x\to0^-}bx \\[5 mm] & = 0 \end{align} $$\therefore \lim\limits_{x\to0^-}f(x) = b$$ Then, consider \begin{align} \lim\limits_{x\to0^+}f(x) & = \lim\limits_{x\to0^+}\sqrt{a + bx} \\[5 mm] & = \sqrt{a}. \end{align} $$\implies a = b^2$$ Furthermore, for $f$ to be differentiable at $0$ , $$\lim\limits_{x\to0^-}\frac {f(x) - \sqrt{a}} x = \lim\limits_{x\to0^+}\frac {f(x) - \sqrt{a}} x$$ When $a = b^2$ , \begin{align} \lim\limits_{x\to0^+}\frac {f(x) - \sqrt{a}} x & = \lim\limits_{x\to0^+}\frac {\sqrt{a + bx} - \sqrt{a}} x \\[5 mm] & = \lim\limits_{x\to0^+}\frac b {\sqrt{a + bx} + \sqrt{a}} \\[5 mm] & = \frac b {2\sqrt{a}} \\[5 mm] & = \frac 1 2 \\[5 mm] \implies \lim\limits_{x\to0^-}\frac {f(x) - \sqrt{a}} x & = \lim\limits_{x\to0^-}\frac {ax^3cos(\frac 1 x) + bx + b - \sqrt{a}} x \\[5 mm] & = \lim\limits_{x\to0^-}\frac {b^2x^3cos(\frac 1 x) + bx} x \\[5 mm] & = \lim\limits_{x\to0^-}[b^2x^2cos(\frac 1 x) + b] \\[5 mm] & = \frac 1 2 \end{align} $$\because \lim\limits_{x\to0^-}b^2x^2cos(\frac 1 x) = 0$$ $$\therefore b = \frac 1 2$$ $$\implies a = \frac 1 4$$ I would like to know if my proposed solution is logical and correct. Moreover, any alternative solutions that are more elegant or succinct are welcomed as well :) Thank you all in advance! Edit Following a discussion with MPW, looks like all is well, except perhaps the fact that $$\implies a = b^2$$ should have been left as $$\implies \sqrt{a} = b$$","Question Determine the values of and such that the following function is differentiable at 0. My solution For to be differentiable at , must first be continuous at . Consider Then, consider Furthermore, for to be differentiable at , When , I would like to know if my proposed solution is logical and correct. Moreover, any alternative solutions that are more elegant or succinct are welcomed as well :) Thank you all in advance! Edit Following a discussion with MPW, looks like all is well, except perhaps the fact that should have been left as","a b f(x) =
\begin{cases}
ax^3cos(\frac 1 x) + bx + b, & \text{if }x \lt 0 \\
\sqrt{a + bx}, & \text{if }x \geq 0
\end{cases} f 0 f 0 \implies \lim\limits_{x\to0^-}f(x) = \lim\limits_{x\to0^+}f(x) \begin{align}
\lim\limits_{x\to0^-}f(x) & =
\lim\limits_{x\to0^-}[ax^3cos(\frac 1 x) + bx + b].
\\[5 mm]
\because \lim\limits_{x\to0^-}ax^3cos(\frac 1 x) & =
\lim\limits_{x\to0^-}bx
\\[5 mm] & =
0
\end{align} \therefore \lim\limits_{x\to0^-}f(x) = b \begin{align}
\lim\limits_{x\to0^+}f(x) & =
\lim\limits_{x\to0^+}\sqrt{a + bx}
\\[5 mm] & =
\sqrt{a}.
\end{align} \implies a = b^2 f 0 \lim\limits_{x\to0^-}\frac {f(x) - \sqrt{a}} x = \lim\limits_{x\to0^+}\frac {f(x) - \sqrt{a}} x a = b^2 \begin{align}
\lim\limits_{x\to0^+}\frac {f(x) - \sqrt{a}} x & =
\lim\limits_{x\to0^+}\frac {\sqrt{a + bx} - \sqrt{a}} x
\\[5 mm] & =
\lim\limits_{x\to0^+}\frac b {\sqrt{a + bx} + \sqrt{a}}
\\[5 mm] & =
\frac b {2\sqrt{a}}
\\[5 mm] & =
\frac 1 2
\\[5 mm]
\implies \lim\limits_{x\to0^-}\frac {f(x) - \sqrt{a}} x & =
\lim\limits_{x\to0^-}\frac {ax^3cos(\frac 1 x) + bx + b - \sqrt{a}} x
\\[5 mm] & =
\lim\limits_{x\to0^-}\frac {b^2x^3cos(\frac 1 x) + bx} x
\\[5 mm] & =
\lim\limits_{x\to0^-}[b^2x^2cos(\frac 1 x) + b]
\\[5 mm] & =
\frac 1 2
\end{align} \because \lim\limits_{x\to0^-}b^2x^2cos(\frac 1 x) = 0 \therefore b = \frac 1 2 \implies a = \frac 1 4 \implies a = b^2 \implies \sqrt{a} = b","['real-analysis', 'calculus', 'limits', 'derivatives', 'limits-without-lhopital']"
55,"Existence of limit for sequence $x_n=\frac12\left(x_{n-1}+\frac8{x_{n-2}}\right)$ with initial values $x_0=5,x_1=10$",Existence of limit for sequence  with initial values,"x_n=\frac12\left(x_{n-1}+\frac8{x_{n-2}}\right) x_0=5,x_1=10","Let $x_0=5,x_1=10,$ and for all integers $n\ge2$ let $x_n=\frac12\left(x_{n-1}+\frac8{x_{n-2}}\right).$ By induction, we have $\forall m\in\mathbb Z_{\ge0}\enspace x_m>0,$ so we can avoid division by $0$ and the sequence is well-defined. According to a Math GRE practice problem, the limit exists. How can we prove that? Note that, if we assume the limit exists, then we can show it equals $\sqrt8,$ but finding the value of the limit is not my goal here. My work: We can compute $x_2=5.8,x_3=3.3,$ which are strictly between $4/3$ and $6,$ and then, assuming an inductive hypothesis, for all integers $n\ge4$ we have $4/3<x_{n-1}<6$ and $4/3<8/x_{n-2}<6,$ so that $4/3<x_n<6.$ We can probably compute more values of $x_n$ to get tighter bounds, but I don't see how to actually show convergence.","Let and for all integers let By induction, we have so we can avoid division by and the sequence is well-defined. According to a Math GRE practice problem, the limit exists. How can we prove that? Note that, if we assume the limit exists, then we can show it equals but finding the value of the limit is not my goal here. My work: We can compute which are strictly between and and then, assuming an inductive hypothesis, for all integers we have and so that We can probably compute more values of to get tighter bounds, but I don't see how to actually show convergence.","x_0=5,x_1=10, n\ge2 x_n=\frac12\left(x_{n-1}+\frac8{x_{n-2}}\right). \forall m\in\mathbb Z_{\ge0}\enspace x_m>0, 0 \sqrt8, x_2=5.8,x_3=3.3, 4/3 6, n\ge4 4/3<x_{n-1}<6 4/3<8/x_{n-2}<6, 4/3<x_n<6. x_n","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
56,Is there an explicit solution to $a^x+b^x=1$?,Is there an explicit solution to ?,a^x+b^x=1,"Is there an explicit solution to $a^x+b^x=1$ ? Where $a, b \in [0, 1]$ and $a+b \le 1$ . I've been playing around with this equation, but I can't seem to make any progress in solving it.  I tried it in Wolfram for some hints, but nothing showed up. Should I assume there isn't an explicit solution to this?","Is there an explicit solution to ? Where and . I've been playing around with this equation, but I can't seem to make any progress in solving it.  I tried it in Wolfram for some hints, but nothing showed up. Should I assume there isn't an explicit solution to this?","a^x+b^x=1 a, b \in [0, 1] a+b \le 1",['real-analysis']
57,"Unusual ways of summing well-known series -- for example, this unusual summation of the geometric series","Unusual ways of summing well-known series -- for example, this unusual summation of the geometric series",,"The typical way of summing $S_g(n,x) = 1+x+x^2+\cdots+x^n$ by multiplying by $(1-x)$ is well known. The arithmetico-geometric series $S_{ag}(n,x) = 1+2x+3x^2+4x^3+\cdots+(n+1)x^n$ can be summed in one of two ways: 1) apply $(1-x)$ twice, or 2) notice that the $n$ th term is the derivative of $x^n$ , and thus that $\frac{d}{dx}S_g(n,x) = S_{ag}(n,x)$ . Let's call the first method the ""multiplication"" method and the second method the ""differentiation"" method. $S_g$ can be summed using the multiplication method, and $S_{ag}$ can be summed using both the multiplication and the differentiation methods (assuming that you know the sum of $S_g$ ). The natural question is whether or not $S_g$ can also be summed using the differentiation method or something like it. This led me to the following summation of the geometric series: Multiply by $e^{yx}$ (which is never zero) on both sides to get $$ S_g(n,x)e^{yx} = e^{yx} + xe^{yx}+x^2e^{yx}+\cdots + x^ne^{yx}.  $$ Noting that $xe^{yx} = \frac{\partial}{\partial y}e^{yx}$ , it follows that \begin{align} e^{yx} + xe^{yx}+x^2e^{yx}+\cdots + x^ne^{yx} &= e^{yx} +  \frac{\partial}{\partial y}e^{yx} + x\frac{\partial}{\partial y}e^{yx}+\cdots+x^{n-1}\frac{\partial}{\partial y}e^{yx} \\ % &=e^{yx}+ \frac{\partial}{\partial y}S_g(n-1,x)e^{yx} \\ % &=e^{yx}+ \frac{\partial}{\partial y}[S_g(n,x)e^{yx}-x^ne^{yx}] \\ % &=e^{yx}+\frac{\partial}{\partial y}S_g(n,x)e^{yx}-x^{n+1}e^{yx},  \end{align} and thus, $$ S_g(n,x)e^{yx}-\frac{\partial}{\partial y}S_g(n,x)e^{yx} = (1-x^{n+1})e^{yx}. $$ Finally, since $\frac{\partial}{\partial y}S_g(n,x)e^{yx} = xS_g(n,x)e^{yx}$ , it follows that $$ (1-x)S_g(n,x)e^{yx} = (1-x^{n+1})e^{yx}, $$ and hence, for all $x\neq 1$ , it follows that $$ S_g(n,x) = \frac{1-x^{n+1}}{1-x}. $$ This method is clearly unusual in that, although it achieves the correct result, it uses more machinery (calculus, exponentials), and is a bit more complicated than the typical way. Nonetheless, I also think there's something fascinating about seeing all the different ways a series can be summed. My general question is what other ""unusual"" ways of summing well-known series do people know? My specific question is whether or not this particular way of summing the geometric series is known? My guess for the second question is 'yes', because the tools are still pretty basic and the manipulations not that complicated, but I've only ever seen the typical way.","The typical way of summing by multiplying by is well known. The arithmetico-geometric series can be summed in one of two ways: 1) apply twice, or 2) notice that the th term is the derivative of , and thus that . Let's call the first method the ""multiplication"" method and the second method the ""differentiation"" method. can be summed using the multiplication method, and can be summed using both the multiplication and the differentiation methods (assuming that you know the sum of ). The natural question is whether or not can also be summed using the differentiation method or something like it. This led me to the following summation of the geometric series: Multiply by (which is never zero) on both sides to get Noting that , it follows that and thus, Finally, since , it follows that and hence, for all , it follows that This method is clearly unusual in that, although it achieves the correct result, it uses more machinery (calculus, exponentials), and is a bit more complicated than the typical way. Nonetheless, I also think there's something fascinating about seeing all the different ways a series can be summed. My general question is what other ""unusual"" ways of summing well-known series do people know? My specific question is whether or not this particular way of summing the geometric series is known? My guess for the second question is 'yes', because the tools are still pretty basic and the manipulations not that complicated, but I've only ever seen the typical way.","S_g(n,x) = 1+x+x^2+\cdots+x^n (1-x) S_{ag}(n,x) = 1+2x+3x^2+4x^3+\cdots+(n+1)x^n (1-x) n x^n \frac{d}{dx}S_g(n,x) = S_{ag}(n,x) S_g S_{ag} S_g S_g e^{yx} 
S_g(n,x)e^{yx} = e^{yx} + xe^{yx}+x^2e^{yx}+\cdots + x^ne^{yx}. 
 xe^{yx} = \frac{\partial}{\partial y}e^{yx} \begin{align}
e^{yx} + xe^{yx}+x^2e^{yx}+\cdots + x^ne^{yx} &= e^{yx} +  \frac{\partial}{\partial y}e^{yx} + x\frac{\partial}{\partial y}e^{yx}+\cdots+x^{n-1}\frac{\partial}{\partial y}e^{yx} \\
%
&=e^{yx}+ \frac{\partial}{\partial y}S_g(n-1,x)e^{yx} \\
%
&=e^{yx}+ \frac{\partial}{\partial y}[S_g(n,x)e^{yx}-x^ne^{yx}] \\
%
&=e^{yx}+\frac{\partial}{\partial y}S_g(n,x)e^{yx}-x^{n+1}e^{yx}, 
\end{align} 
S_g(n,x)e^{yx}-\frac{\partial}{\partial y}S_g(n,x)e^{yx} = (1-x^{n+1})e^{yx}.
 \frac{\partial}{\partial y}S_g(n,x)e^{yx} = xS_g(n,x)e^{yx} 
(1-x)S_g(n,x)e^{yx} = (1-x^{n+1})e^{yx},
 x\neq 1 
S_g(n,x) = \frac{1-x^{n+1}}{1-x}.
","['real-analysis', 'calculus', 'sequences-and-series', 'summation', 'alternative-proof']"
58,Does the set of real numbers with bounded partial quotients have positive measure?,Does the set of real numbers with bounded partial quotients have positive measure?,,"We say a real number $x$ has bounded partial quotients if its continued fraction expansion $[a_0; a_1, a_2 \cdots]$ is bounded by some constant $M=M(x)$ . The set $A$ consisting of those numbers whose partial quotients are bounded forms a dense, uncountable subset of $\mathbb{R}$ which includes the algebraic numbers of degree $\leq 2$ . It appears that it is an open problem whether or not $A$ contains any algebraic numbers of degree $>2$ . Question : Is it known whether or not the set $A$ has measure zero? The reason this question is interesting has to do with Diophantine approximation. We say a real number $x$ is badly-approximable if there exists a positive constant $C=C(x)$ such that $$\left|x-\frac{p}{q}\right|>\frac{C}{q^2}$$ for all rational $p/q \neq x$ . Here, the name fits, since for any irrational $x$ , $|x-p/q|<\frac{1}{q^2}$ for infinitely many pairs $(p,q)$ , and badly-approximable numbers are precisely those for which we cannot do better than this, i.e., merely scaling the numerator by a certain constant $C$ ruins everything. It turns out that the property defining the set $A$ discussed earlier (i.e., bounded partial quotients) is in fact completely equivalent to this property of being badly-approximable. Thus asking about the size of $A$ is essentially asking about the size of the set of worst-approximable real numbers.","We say a real number has bounded partial quotients if its continued fraction expansion is bounded by some constant . The set consisting of those numbers whose partial quotients are bounded forms a dense, uncountable subset of which includes the algebraic numbers of degree . It appears that it is an open problem whether or not contains any algebraic numbers of degree . Question : Is it known whether or not the set has measure zero? The reason this question is interesting has to do with Diophantine approximation. We say a real number is badly-approximable if there exists a positive constant such that for all rational . Here, the name fits, since for any irrational , for infinitely many pairs , and badly-approximable numbers are precisely those for which we cannot do better than this, i.e., merely scaling the numerator by a certain constant ruins everything. It turns out that the property defining the set discussed earlier (i.e., bounded partial quotients) is in fact completely equivalent to this property of being badly-approximable. Thus asking about the size of is essentially asking about the size of the set of worst-approximable real numbers.","x [a_0; a_1, a_2 \cdots] M=M(x) A \mathbb{R} \leq 2 A >2 A x C=C(x) \left|x-\frac{p}{q}\right|>\frac{C}{q^2} p/q \neq x x |x-p/q|<\frac{1}{q^2} (p,q) C A A","['real-analysis', 'continued-fractions', 'diophantine-approximation']"
59,How continuous are functions that map dense sets to dense sets?,How continuous are functions that map dense sets to dense sets?,,"Let $f:[0,1] \to [0,1]$ be a function with the property that for every dense $D \subset [0,1]$ , $f(D)$ is dense in $f([0,1])$ . We can note that $f$ need not be continuous. For instance, consider $f(x)=\left|\sin \ \left(\frac{1}{x-1/2}\right)\right|$ with $f(1/2)=0$ . Is there a nowhere continuous example? If not, how large must the continuity set of such functions be?","Let be a function with the property that for every dense , is dense in . We can note that need not be continuous. For instance, consider with . Is there a nowhere continuous example? If not, how large must the continuity set of such functions be?","f:[0,1] \to [0,1] D \subset [0,1] f(D) f([0,1]) f f(x)=\left|\sin \ \left(\frac{1}{x-1/2}\right)\right| f(1/2)=0","['real-analysis', 'continuity']"
60,Curious limit of a sequence used to prove Etemadi's SLLN,Curious limit of a sequence used to prove Etemadi's SLLN,,"I've been struggling with the proof of this proposition: Let $\{x_{n}\}_{n=1}^{\infty}$ be a sequence of non-negative real numbers. If $$\lim_{n\to\infty} \frac{1}{\left\lceil{r^n}\right\rceil}\sum\limits_{i=1}^{\lceil{r^n}\rceil}x_{i} = c$$ for all $r >1$ with $r\in \mathbb{R}$ ,   then $$\lim_{n\to\infty} \frac{1}{n}\sum\limits_{i=1}^{n}x_{i} = c.$$ I 've been researching and looking for possible fancy properties of the ceiling function that could be suitable for this proof, but all my efforts have been in vain. If you could help me out I'd be very grateful. Observation: $\lceil{x}\rceil$ is the smallest integer $m$ such that $x \leq m$ , where $x\in\Bbb R$ . Further context(taken from comment): I think this lemma/proposition is used to prove Strong Law of Large Numbers of Etemadi (1981); it can be checked between pages 55-57 of Durret's Probability: Theory and Examples (above all in page 57) but I can't write the complete formal proof of this lemma used there.","I've been struggling with the proof of this proposition: Let be a sequence of non-negative real numbers. If for all with ,   then I 've been researching and looking for possible fancy properties of the ceiling function that could be suitable for this proof, but all my efforts have been in vain. If you could help me out I'd be very grateful. Observation: is the smallest integer such that , where . Further context(taken from comment): I think this lemma/proposition is used to prove Strong Law of Large Numbers of Etemadi (1981); it can be checked between pages 55-57 of Durret's Probability: Theory and Examples (above all in page 57) but I can't write the complete formal proof of this lemma used there.",\{x_{n}\}_{n=1}^{\infty} \lim_{n\to\infty} \frac{1}{\left\lceil{r^n}\right\rceil}\sum\limits_{i=1}^{\lceil{r^n}\rceil}x_{i} = c r >1 r\in \mathbb{R} \lim_{n\to\infty} \frac{1}{n}\sum\limits_{i=1}^{n}x_{i} = c. \lceil{x}\rceil m x \leq m x\in\Bbb R,"['real-analysis', 'sequences-and-series', 'limits', 'probability-theory', 'ceiling-and-floor-functions']"
61,A sufficient condition for differentiability of $f:\mathbb{R}^n\rightarrow\mathbb{R}$,A sufficient condition for differentiability of,f:\mathbb{R}^n\rightarrow\mathbb{R},"There is a theorem in my textbook which says: Theorem: consider the function $f:\mathbb{R}^2\to\mathbb{R}$ such that both of its partial derivatives at a point exist and at least one of them is continuous, then $f$ is differentiable at that point. I want to know whether the theorem is true in general for a function $f:\mathbb{R}^n\to\mathbb{R}$ , i.e whether the following is true or not: Consider the function $f:\mathbb{R}^n\to\mathbb{R}$ such that all its partial derivatives exist at a point and at least one of these partial derivatives is continuous at that point,then $f$ is differentiable at that point.","There is a theorem in my textbook which says: Theorem: consider the function such that both of its partial derivatives at a point exist and at least one of them is continuous, then is differentiable at that point. I want to know whether the theorem is true in general for a function , i.e whether the following is true or not: Consider the function such that all its partial derivatives exist at a point and at least one of these partial derivatives is continuous at that point,then is differentiable at that point.",f:\mathbb{R}^2\to\mathbb{R} f f:\mathbb{R}^n\to\mathbb{R} f:\mathbb{R}^n\to\mathbb{R} f,"['real-analysis', 'calculus', 'multivariable-calculus']"
62,"Possible significant error in proof of the spectral theorem, Brian C Hall, Quantum Theory for Mathematicians","Possible significant error in proof of the spectral theorem, Brian C Hall, Quantum Theory for Mathematicians",,"Note/Edit: Read the below paragraphs for context. I think I found a counterexample, though I don't have the energy to work through it now. Suppose $\mathcal{H} = L^2([0,1],m) \oplus L^2([0,1],m)$ where $m$ denotes Lebesgue measure. Consider the ""position"" operator $X$ given by $Xf(x) = xf(x)$ and consider the operator $A$ given by $Af = 1_{[1/2,1]}f$ , each operator is a bounded self-adjoint operator on $L^2([0,1],m)$ . Then define the bounded self-adjoint operator $X \oplus A$ on $\mathcal{H}$ in the obvious way: $X \oplus A (f,g) = (Xf,Ag)$ . You can look at Hall's construction of the $(W_j, \psi_j)$ , but you can convince yourself that we could take $W_1 = L^2([0,1],m) \times \{0\}$ and have $\psi_1 = (1,0)$ (standard techniques using density of polynomials will then convince you that the closed span of the $(X \oplus A)^n \psi_1$ is $W_1$ ). Then note that $1 \in \sigma(X \oplus A|_{W_1})$ , $1_{\{1\}}(X \oplus A) \neq 0$ (as $(0, 1_{[1/2,1]} g)$ is an eigenvector of $X \oplus A$ with eigenvalue $1$ for all $g \in L^2([1/2,1],m)$ ), hence $\mu(\{1\}) \neq 0$ but $\mu_{\psi_1}(\{1\}) = 0$ , which contradicts surjectivity as I noted in the 3rd to last paragraph. Let me know if you think this works or doesn't. If if does then I guess there's a serious problem with the proof in the book. So I am in the middle of reading through B.C. Hall's proof of the spectral theorem for bounded self-adjoint operators on separable Hilbert spaces, so this question is a little hard to state given the amount of context. Let $A$ a self-adjoint bounded operator on a separable Hilbert space $\mathcal{H}$ . Suppose $\{W_j, \psi_j\}$ is a (possibly finite) sequence of pairwise orthogonal subspaces of $\mathcal{H}$ s.t. they are invariant under $A$ , for fixed $j$ the span of $A^n \psi_j$ is dense in $W_j$ , and $\mathcal{H} = \bigoplus_j W_j$ (the orthogonal direct sum). Let $A_j = A|_{W_j}$ . Let $\mu^A$ , $\mu^{A_j}$ denote the projection valued measure on the Borel $\sigma$ -algebra on $\sigma(A), \sigma(A_j)$ resp. (where $\sigma(B)$ is the spectrum of $B$ ). Let $\mu_{\psi_j}$ be the positive measure given by $\mu_{\psi_j}(E) = (\psi_j, \mu^{A_j}(E) \psi_j)$ . Let $\mu$ be a positive measure on $\mathcal{B}_{\sigma(A)}$ s.t. $\mu(E) =0 \iff \mu^A(E) = 0$ . Then note that $\mu^{A_j}(E) = 1_E(A_j) = 1_E(A|_{W_j}) = 1_E(A)|_{W_j}$ (where we are referring to the functional calculus induced by the respective operators, and the last equality follows from a lemma). Thus $\mu(E) = 0$ implies $0=\mu^A(E)= 1_E(A)$ , hence $\mu^{A_j}(E) = 1_E(A_j) = 1_E(A)|_{W_j} = 0$ which implies $\mu_{\psi_j}(E) = 0$ . So $\mu_{\psi_j}$ is absolutely continuous w/r/t $\mu$ on $\mathcal{B}_{\sigma(A_j)}$ . Let $\rho_j$ denote its Radon-Nikodym derivative. Finally we get to the part I'm stuck at. The author now claims ""one can easily see that"" the map $f \mapsto \rho_j^{1/2}f$ is unitary from $L^2(\sigma(A_j), \mu_{\psi_j}) \to L^2(\sigma(A_j), \mu)$ . It is easy to verify it is norm preserving but why is it surjective? Note that we only showed that $\mu_{\psi_j}$ was absolutely continuous w/r/t $\mu$ . I claim that we need the converse to hold. Suppose we had the converse. Then $\rho_j$ is a.e. (both $\mu$ and $\mu_{\psi_j}$ since these agree on null sets) to a function that is everywhere nonzero, so we can WLOG suppose that $\rho_j$ is everywhere nonzero. Then we note that $\int_{\sigma(A_j)} |f \rho_j^{-1/2}|^2 d\mu_{\psi_j} = \int_{\sigma(A_j)} |f|^2 \rho_j^{-1} \rho_j d\mu = \int_{\sigma(A_j)} |f|^2 d\mu$ . So if $f \in L^2(\sigma(A_j), \mu)$ , then $f\rho_j^{-1/2} \in L^2(\sigma(A_j), \mu_{\psi_j})$ . So the map is onto. On the other hand if we have that the converse doesn't hold, by considering a set that is $\mu_{\psi_j}$ null but not $\mu$ null ( $\mu$ can be taken to be a finite measure, we can WLOG suppose this set has finite measure), and considering the indicator function on this set, we can see that no function in $L^2(\sigma(A_j), \mu_{\psi_j})$ will map onto it. So why does this converse hold (to me it doesn't seem like it actually does so it feels like there is a near fatal error in this proof)? I tried constructing examples, but they got confusing fast. Sorry for the wall of text, the question is really deep into a long proof but I'm totally stuck and any help would be greatly appreciated.","Note/Edit: Read the below paragraphs for context. I think I found a counterexample, though I don't have the energy to work through it now. Suppose where denotes Lebesgue measure. Consider the ""position"" operator given by and consider the operator given by , each operator is a bounded self-adjoint operator on . Then define the bounded self-adjoint operator on in the obvious way: . You can look at Hall's construction of the , but you can convince yourself that we could take and have (standard techniques using density of polynomials will then convince you that the closed span of the is ). Then note that , (as is an eigenvector of with eigenvalue for all ), hence but , which contradicts surjectivity as I noted in the 3rd to last paragraph. Let me know if you think this works or doesn't. If if does then I guess there's a serious problem with the proof in the book. So I am in the middle of reading through B.C. Hall's proof of the spectral theorem for bounded self-adjoint operators on separable Hilbert spaces, so this question is a little hard to state given the amount of context. Let a self-adjoint bounded operator on a separable Hilbert space . Suppose is a (possibly finite) sequence of pairwise orthogonal subspaces of s.t. they are invariant under , for fixed the span of is dense in , and (the orthogonal direct sum). Let . Let , denote the projection valued measure on the Borel -algebra on resp. (where is the spectrum of ). Let be the positive measure given by . Let be a positive measure on s.t. . Then note that (where we are referring to the functional calculus induced by the respective operators, and the last equality follows from a lemma). Thus implies , hence which implies . So is absolutely continuous w/r/t on . Let denote its Radon-Nikodym derivative. Finally we get to the part I'm stuck at. The author now claims ""one can easily see that"" the map is unitary from . It is easy to verify it is norm preserving but why is it surjective? Note that we only showed that was absolutely continuous w/r/t . I claim that we need the converse to hold. Suppose we had the converse. Then is a.e. (both and since these agree on null sets) to a function that is everywhere nonzero, so we can WLOG suppose that is everywhere nonzero. Then we note that . So if , then . So the map is onto. On the other hand if we have that the converse doesn't hold, by considering a set that is null but not null ( can be taken to be a finite measure, we can WLOG suppose this set has finite measure), and considering the indicator function on this set, we can see that no function in will map onto it. So why does this converse hold (to me it doesn't seem like it actually does so it feels like there is a near fatal error in this proof)? I tried constructing examples, but they got confusing fast. Sorry for the wall of text, the question is really deep into a long proof but I'm totally stuck and any help would be greatly appreciated.","\mathcal{H} = L^2([0,1],m) \oplus L^2([0,1],m) m X Xf(x) = xf(x) A Af = 1_{[1/2,1]}f L^2([0,1],m) X \oplus A \mathcal{H} X \oplus A (f,g) = (Xf,Ag) (W_j, \psi_j) W_1 = L^2([0,1],m) \times \{0\} \psi_1 = (1,0) (X \oplus A)^n \psi_1 W_1 1 \in \sigma(X \oplus A|_{W_1}) 1_{\{1\}}(X \oplus A) \neq 0 (0, 1_{[1/2,1]} g) X \oplus A 1 g \in L^2([1/2,1],m) \mu(\{1\}) \neq 0 \mu_{\psi_1}(\{1\}) = 0 A \mathcal{H} \{W_j, \psi_j\} \mathcal{H} A j A^n \psi_j W_j \mathcal{H} = \bigoplus_j W_j A_j = A|_{W_j} \mu^A \mu^{A_j} \sigma \sigma(A), \sigma(A_j) \sigma(B) B \mu_{\psi_j} \mu_{\psi_j}(E) = (\psi_j, \mu^{A_j}(E) \psi_j) \mu \mathcal{B}_{\sigma(A)} \mu(E) =0 \iff \mu^A(E) = 0 \mu^{A_j}(E) = 1_E(A_j) = 1_E(A|_{W_j}) = 1_E(A)|_{W_j} \mu(E) = 0 0=\mu^A(E)= 1_E(A) \mu^{A_j}(E) = 1_E(A_j) = 1_E(A)|_{W_j} = 0 \mu_{\psi_j}(E) = 0 \mu_{\psi_j} \mu \mathcal{B}_{\sigma(A_j)} \rho_j f \mapsto \rho_j^{1/2}f L^2(\sigma(A_j), \mu_{\psi_j}) \to L^2(\sigma(A_j), \mu) \mu_{\psi_j} \mu \rho_j \mu \mu_{\psi_j} \rho_j \int_{\sigma(A_j)} |f \rho_j^{-1/2}|^2 d\mu_{\psi_j} = \int_{\sigma(A_j)} |f|^2 \rho_j^{-1} \rho_j d\mu = \int_{\sigma(A_j)} |f|^2 d\mu f \in L^2(\sigma(A_j), \mu) f\rho_j^{-1/2} \in L^2(\sigma(A_j), \mu_{\psi_j}) \mu_{\psi_j} \mu \mu L^2(\sigma(A_j), \mu_{\psi_j})","['real-analysis', 'functional-analysis', 'hilbert-spaces', 'spectral-theory', 'quantum-mechanics']"
63,Calculate $\lim_{n\to\infty} \frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}}$ [duplicate],Calculate  [duplicate],\lim_{n\to\infty} \frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}},"This question already has answers here : Prove $\lim\limits_{n\to\infty}n^{-1/(p+1)}(1^{1^p}\cdot 2^{2^p}\cdots n^{n^p})^{n^{-p-1}}=e^{-1/(p+1)^2}$ (2 answers) Closed 4 years ago . Calculate: $$\lim_{n\to\infty} \frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}}$$ I've done some steps as follows: $$a_n:=\frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}} \iff \ln a_n=\frac{1}{n^{p+1}}\big(\sum_{k=1}^nk^p\ln k-\frac{n^{p+1}}{p+1}\ln n\big) \iff \\\ln a_n =\frac{1}{n}\sum_{k=1}^n\big[\big(\frac{k}{n}\big)^p\ln \frac{k}{n}\big]+\frac{1}{n}\sum_{k=1}^n\big(\frac{k}{n}\big)^p\ln n-\frac{\ln n}{p+1}.$$ Then, I was wondering if I could make some integrals out of it but still there are some odd terms. I think my approach isn't so good...","This question already has answers here : Prove $\lim\limits_{n\to\infty}n^{-1/(p+1)}(1^{1^p}\cdot 2^{2^p}\cdots n^{n^p})^{n^{-p-1}}=e^{-1/(p+1)^2}$ (2 answers) Closed 4 years ago . Calculate: I've done some steps as follows: Then, I was wondering if I could make some integrals out of it but still there are some odd terms. I think my approach isn't so good...",\lim_{n\to\infty} \frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}} a_n:=\frac{ (1^{1^p}2^{2^p}\cdot...\cdot n^{n^p})^{ 1/n^{p+1} }}{n^{1/(p+1)}} \iff \ln a_n=\frac{1}{n^{p+1}}\big(\sum_{k=1}^nk^p\ln k-\frac{n^{p+1}}{p+1}\ln n\big) \iff \\\ln a_n =\frac{1}{n}\sum_{k=1}^n\big[\big(\frac{k}{n}\big)^p\ln \frac{k}{n}\big]+\frac{1}{n}\sum_{k=1}^n\big(\frac{k}{n}\big)^p\ln n-\frac{\ln n}{p+1}.,"['real-analysis', 'calculus', 'limits']"
64,Is the series $X =\frac{1}{3}+\frac{1}{5}+\frac{1}{7}+\frac{1}{11}+..$ convergent or divergent.,Is the series  convergent or divergent.,X =\frac{1}{3}+\frac{1}{5}+\frac{1}{7}+\frac{1}{11}+..,The series is the reciprocal of twin primes. Let $Y=(y_n)$ be the series of reciprocal of natural numbers.  Now if I use the comparison test we can see that each term of $0 <(x_n) < (y_n)$ .So the divergence of series of reciprocal of  natural number should also imply the same for $Y$ . Now the problems of my proof: $1)$ I probably have used the comparison test in a wrong sense $2)$ I don't know how many twin primes are there Can someone help me to understand the intuition behind the statement. If it converges or diverges why? I don't need the entire proof but some hint would do..,The series is the reciprocal of twin primes. Let be the series of reciprocal of natural numbers.  Now if I use the comparison test we can see that each term of .So the divergence of series of reciprocal of  natural number should also imply the same for . Now the problems of my proof: I probably have used the comparison test in a wrong sense I don't know how many twin primes are there Can someone help me to understand the intuition behind the statement. If it converges or diverges why? I don't need the entire proof but some hint would do..,Y=(y_n) 0 <(x_n) < (y_n) Y 1) 2),"['real-analysis', 'sequences-and-series', 'limits', 'twin-primes']"
65,Definition 4.1 Principles of Mathematical Analysis,Definition 4.1 Principles of Mathematical Analysis,,"Definition 4.1 Let X and Y be metric spaces; suppose E $\subset$ X, if $f$ maps E into Y and $p$ is a limit point of E. We write $f(x)$ $\to$ $q$ as $x$ $\to$ p   if there is a point $q$ $\in$ Y with the following property: $\forall \epsilon>0, \exists\delta>0 $ s.t $d(f(x),q)<\epsilon$ for all points $x\in E$ for which $0<d(x,q)<\delta$ I have a question about p being the limit point. Is it necessary for this definition? What if $p$ is an isolated point? Thanks in advance!","Definition 4.1 Let X and Y be metric spaces; suppose E X, if maps E into Y and is a limit point of E. We write as p   if there is a point Y with the following property: s.t for all points for which I have a question about p being the limit point. Is it necessary for this definition? What if is an isolated point? Thanks in advance!","\subset f p f(x) \to q x \to q \in \forall \epsilon>0, \exists\delta>0  d(f(x),q)<\epsilon x\in E 0<d(x,q)<\delta p",['real-analysis']
66,"$C+ \frac 1 n C=[0,1+1/n]$ conjecture",conjecture,"C+ \frac 1 n C=[0,1+1/n]","Let $n$ be a positive integer. Let $C \subseteq[0,1]$ be the ternary Cantor set. I have made the conjecture $C+\frac 1 nC=[0,1+1/n]$ and am wondering if it is true or not. It is obviously true when $n=1$ , since \begin{align*} C+C & = \left\{\frac {a_1+b_1} {3}+ \frac {a_2+b_2} {3^2}+...:a_i,b_i \in \{0,2\}\right\} \\ & = \left\{\frac {2c_1} 3+ \frac {2c_2} {3^2}+...: c_i\in\{0,1,2\}\right\} \\ & =2[0,1]=[0,2]. \end{align*} Moreover, for general $n$ , I see that $C+ \frac 1 n C$ has $1+1/n$ for its maximum and $0$ for its minimum. But how do we know that $C+ \frac 1 n$ contains everything in between? (Does it?) For $n=2$ , I see that $C+ \frac 1 2C=\{\frac {c_1} 3+ \frac {c_2} {3^2}+...:c_i \in \{0,1,2,3\}\}$ . Intuitively, I want to ""collapse"" the the $c_i=3$ terms to the left but I'm having trouble making this rigorous.","Let be a positive integer. Let be the ternary Cantor set. I have made the conjecture and am wondering if it is true or not. It is obviously true when , since Moreover, for general , I see that has for its maximum and for its minimum. But how do we know that contains everything in between? (Does it?) For , I see that . Intuitively, I want to ""collapse"" the the terms to the left but I'm having trouble making this rigorous.","n C \subseteq[0,1] C+\frac 1 nC=[0,1+1/n] n=1 \begin{align*}
C+C & = \left\{\frac {a_1+b_1} {3}+ \frac {a_2+b_2} {3^2}+...:a_i,b_i \in \{0,2\}\right\} \\
& = \left\{\frac {2c_1} 3+ \frac {2c_2} {3^2}+...: c_i\in\{0,1,2\}\right\} \\ & =2[0,1]=[0,2].
\end{align*} n C+ \frac 1 n C 1+1/n 0 C+ \frac 1 n n=2 C+ \frac 1 2C=\{\frac {c_1} 3+ \frac {c_2} {3^2}+...:c_i \in \{0,1,2,3\}\} c_i=3","['real-analysis', 'conjectures', 'cantor-set']"
67,Constructing a Borel set,Constructing a Borel set,,"I have to make a Borel set $E \subset R $ such that $ 0 < m(E \cap I) < m(E)$ , for all real segment $I$ , where $ m $ is the Lebesgue measure. I also have to say if it is possible for E to have $m(E)< \infty$ I know that I have to take a dense sequence $(r_j)$ , such as $Q$ , $a_j > 0$ such that $a_k > \sum_{j=k+1} ^{\infty} {a_j}$ , such as $ a_j = 1/3^j$ , and then define $I_j = (r_j-a_j,r_j+a_j)$ , $E_k= I_k \setminus \bigcup _{j=k+1}^{\infty} {I_j} $ , and finaly $E = \bigcup E_k$ . I just don't know how to calculate its measure, nor how to prove that $m(E)$ can or can't be finite.","I have to make a Borel set such that , for all real segment , where is the Lebesgue measure. I also have to say if it is possible for E to have I know that I have to take a dense sequence , such as , such that , such as , and then define , , and finaly . I just don't know how to calculate its measure, nor how to prove that can or can't be finite.","E \subset R   0 < m(E \cap I) < m(E) I  m  m(E)< \infty (r_j) Q a_j > 0 a_k > \sum_{j=k+1} ^{\infty} {a_j}  a_j = 1/3^j I_j = (r_j-a_j,r_j+a_j) E_k= I_k \setminus \bigcup _{j=k+1}^{\infty} {I_j}  E = \bigcup E_k m(E)","['real-analysis', 'measure-theory', 'lebesgue-measure']"
68,Can we approximate a vector field on the plane with non-vanishing vector fields in $L^2$?,Can we approximate a vector field on the plane with non-vanishing vector fields in ?,L^2,"Let $V$ be a compactly-supported smooth vector field on $\mathbb{R}^2$ , whose zeros inside some open neighbourhood of the closed unit disk $\mathbb{D}^2$ are isolated. Does there exist a sequence of vector fields $V_n \in C^\infty \cap L^{2}$ on $\mathbb{R}^2$ , such that $V_n \to V$ in $L^2$ and $V_n$ do not vanish on $\mathbb{D}^2$ ?","Let be a compactly-supported smooth vector field on , whose zeros inside some open neighbourhood of the closed unit disk are isolated. Does there exist a sequence of vector fields on , such that in and do not vanish on ?",V \mathbb{R}^2 \mathbb{D}^2 V_n \in C^\infty \cap L^{2} \mathbb{R}^2 V_n \to V L^2 V_n \mathbb{D}^2,"['real-analysis', 'differential-topology', 'lp-spaces', 'vector-fields', 'singularity-theory']"
69,Part 1: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge?,Part 1: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge?,,"Let $a_k<b_k<c_k$ be the $k$ -th primitive Pythagorean triplet in ascending order of the hypotenuse $c_k$ . Define $$ l = \frac{b_1 + b_2 + b_3 + \cdots + b_k}{c_1 + c_2 + c_3 + \cdots + c_k}, \text{ } s = \frac{a_1 + a_2 + a_3 + \cdots + a_k}{c_1 + c_2 + c_3 + \cdots + c_k} $$ Question : What is the limiting value of $l$ and $s$ ? The difference between this question and the related question : Part 2: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge? is that here the triangles are in sequenced in ascending order of the hypotenuse $c_k$ where as in the related question, they are sequenced in ascending order of $r$ and $s$ , and depending on the choice of sequencing, the limiting value differs. SageMath Code c  = 1 sa = 1 sb = 1 sc = 1 f  = 0 sx = 0 while(c <= 10^20):     a = c - 1     b = 3     while(a > b):         b = (c^2 - a^2)^0.5         if(b%1 == 0):             if(b <= a):                 if(gcd(a,b) == 1):                     f  = f + 1                     sa = sa + a                     sb = sb + b                     sc = sc + c                     sx = sx + 1/c.n()                     print(f,c, sa/sc.n(),sb/sc.n(),sx)             else:                 break         a = a - 1     c = c + 1","Let be the -th primitive Pythagorean triplet in ascending order of the hypotenuse . Define Question : What is the limiting value of and ? The difference between this question and the related question : Part 2: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge? is that here the triangles are in sequenced in ascending order of the hypotenuse where as in the related question, they are sequenced in ascending order of and , and depending on the choice of sequencing, the limiting value differs. SageMath Code c  = 1 sa = 1 sb = 1 sc = 1 f  = 0 sx = 0 while(c <= 10^20):     a = c - 1     b = 3     while(a > b):         b = (c^2 - a^2)^0.5         if(b%1 == 0):             if(b <= a):                 if(gcd(a,b) == 1):                     f  = f + 1                     sa = sa + a                     sb = sb + b                     sc = sc + c                     sx = sx + 1/c.n()                     print(f,c, sa/sc.n(),sb/sc.n(),sx)             else:                 break         a = a - 1     c = c + 1","a_k<b_k<c_k k c_k 
l = \frac{b_1 + b_2 + b_3 + \cdots + b_k}{c_1 + c_2 + c_3 + \cdots + c_k}, \text{ } s = \frac{a_1 + a_2 + a_3 + \cdots + a_k}{c_1 + c_2 + c_3 + \cdots + c_k}
 l s c_k r s","['real-analysis', 'sequences-and-series', 'geometry', 'number-theory', 'summation']"
70,A proof of Taylor's Peano Remainder using little o notation,A proof of Taylor's Peano Remainder using little o notation,,"I have two questions relating to a proof I found of the Peano remainder form: $f(x)=\sum_{k=0}^{n}\frac{f^{(k)}(x_{0})}{k!}(x-x_{0})^{k}+\mathrm{o}\left(|x-x_{0}|^{n}\right)$ . Keep in mind that the little o notation here is that if $f \in \mathrm{o}(t)$ then: $ \lim_{\substack{t\rightarrow0\\ t\neq0} }\frac{f(t)}{t}=0$ I am not interested in general proofs of Peano's remainder form, but particularly in how the little o notation is manipulated. So this is more of a little o notation question than a Taylor's theorem proof question. This linked proof begins using the integral form of the remainder and the assumption that $f \in C^n$ , so the proof has an extra assumption compared to the normal Peano remainder proofs, which usually only assume $n$ times differentiability (so $f^{(n)}$ is not necessarily continuous). My first issue is with the proof itself, which says that \begin{aligned}\frac{(x-x_{0})^{n}}{(n-1)!}\int_{0}^{1}\left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right](1-t)^{n-1}dt.\end{aligned} can be simplified to $\mathrm{o}\left(|x-x_{0}|^{n}\right)$ . The last line says that since $f^{(n)}$ is assumed continuous, we have that $\left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right] \in \mathrm{o}\left(|x-x_{0}|\right)$ . I am trying to show that this is true, but can't seem to justify it. I think that continuity is necessary so that the limit as $x \rightarrow x_0$ of $f^{(n)}(x_{0}+t(x-x_{0}))$ is $f^{(n)}(x_{0})$ and so the $\left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right]$ term goes to $0$ . But I am not sure how this implies that $\left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right]$ goes to $0$ faster than $(|x-x_{0}|)$ , since we have no details about $f^{(n)}$ . I am thinking it could be since $t \in [0,1]$ and so for a given increment $x - x_0$ , $f^{(n)}(x_{0}+t(x-x_{0}))$ is closer to $f^{(n)}(x_{0})$ than $f^{(n)}(x_{0}+(x-x_{0}))$ otherwise would have been, but I'm unsure if this is valid reasoning. It also seems more like a 'big O' case than 'little o'. In addition, I am wondering how \begin{aligned}\frac{(x-x_{0})^{n}}{(n-1)!}\int_{0}^{1}\left[\mathrm{o}\left(|x-x_{0}|\right)\right](1-t)^{n-1}dt.\end{aligned} is simplified to $\mathrm{o}\left(|x-x_{0}|^{n}\right)$ , since there is an $(x-x_{0})^{n}$ multiplied by $\mathrm{o}\left(|x-x_{0}|\right)$ , which I think should be $\mathrm{o}\left(|x-x_{0}|^{n+1}\right)$ and not $\mathrm{o}\left(|x-x_{0}|^{n}\right)$ as the remainder should appear as. So those are my main questions, and my confusion mostly pertains to the little o notation and it's manipulations. I know there are other proofs for Taylor's theorem but wanted to understand this notation a bit better. Thanks in advance!","I have two questions relating to a proof I found of the Peano remainder form: . Keep in mind that the little o notation here is that if then: I am not interested in general proofs of Peano's remainder form, but particularly in how the little o notation is manipulated. So this is more of a little o notation question than a Taylor's theorem proof question. This linked proof begins using the integral form of the remainder and the assumption that , so the proof has an extra assumption compared to the normal Peano remainder proofs, which usually only assume times differentiability (so is not necessarily continuous). My first issue is with the proof itself, which says that can be simplified to . The last line says that since is assumed continuous, we have that . I am trying to show that this is true, but can't seem to justify it. I think that continuity is necessary so that the limit as of is and so the term goes to . But I am not sure how this implies that goes to faster than , since we have no details about . I am thinking it could be since and so for a given increment , is closer to than otherwise would have been, but I'm unsure if this is valid reasoning. It also seems more like a 'big O' case than 'little o'. In addition, I am wondering how is simplified to , since there is an multiplied by , which I think should be and not as the remainder should appear as. So those are my main questions, and my confusion mostly pertains to the little o notation and it's manipulations. I know there are other proofs for Taylor's theorem but wanted to understand this notation a bit better. Thanks in advance!","f(x)=\sum_{k=0}^{n}\frac{f^{(k)}(x_{0})}{k!}(x-x_{0})^{k}+\mathrm{o}\left(|x-x_{0}|^{n}\right) f \in \mathrm{o}(t)  \lim_{\substack{t\rightarrow0\\ t\neq0} }\frac{f(t)}{t}=0 f \in C^n n f^{(n)} \begin{aligned}\frac{(x-x_{0})^{n}}{(n-1)!}\int_{0}^{1}\left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right](1-t)^{n-1}dt.\end{aligned} \mathrm{o}\left(|x-x_{0}|^{n}\right) f^{(n)} \left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right] \in \mathrm{o}\left(|x-x_{0}|\right) x \rightarrow x_0 f^{(n)}(x_{0}+t(x-x_{0})) f^{(n)}(x_{0}) \left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right] 0 \left[f^{(n)}(x_{0}+t(x-x_{0}))-f^{(n)}(x_{0})\right] 0 (|x-x_{0}|) f^{(n)} t \in [0,1] x - x_0 f^{(n)}(x_{0}+t(x-x_{0})) f^{(n)}(x_{0}) f^{(n)}(x_{0}+(x-x_{0})) \begin{aligned}\frac{(x-x_{0})^{n}}{(n-1)!}\int_{0}^{1}\left[\mathrm{o}\left(|x-x_{0}|\right)\right](1-t)^{n-1}dt.\end{aligned} \mathrm{o}\left(|x-x_{0}|^{n}\right) (x-x_{0})^{n} \mathrm{o}\left(|x-x_{0}|\right) \mathrm{o}\left(|x-x_{0}|^{n+1}\right) \mathrm{o}\left(|x-x_{0}|^{n}\right)","['real-analysis', 'calculus', 'number-theory', 'numerical-methods', 'asymptotics']"
71,How large can the set of discontinuous points of a coordinately continuous function be?,How large can the set of discontinuous points of a coordinately continuous function be?,,"I temporarily say function $f(x,y)$ (from $\mathbb{R}^2$ to $\mathbb{R}$ ) is coordinately continuous (shortly as c.c.) iff it is continuous everywhere regarded as an single-variable function while another coordinate is given. I am curious about how large the set of uncontinuous (in sense of $\mathbb{R}^2$ ) points of a c.c. function can be . It is not hard to see that there are c.c. functions which is not continuous at one point. For example, $f(x,y)= 1−4(xy/(x^2+y^2))^2$ is not continuous at $(0,0)$ , which can be seen from the form of polar coordinate $1 - \sin(2\theta)^2$ except $(0,0)$ . Intuitionly, I think discontinuity while maintaining c.c. property needs a well designed neighbor. Therefore I think the set of discontinuity points is not dense. It has been proved that such function cannot be discontinuous everywhere: separately continuous functions $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ but nowhere continuous","I temporarily say function (from to ) is coordinately continuous (shortly as c.c.) iff it is continuous everywhere regarded as an single-variable function while another coordinate is given. I am curious about how large the set of uncontinuous (in sense of ) points of a c.c. function can be . It is not hard to see that there are c.c. functions which is not continuous at one point. For example, is not continuous at , which can be seen from the form of polar coordinate except . Intuitionly, I think discontinuity while maintaining c.c. property needs a well designed neighbor. Therefore I think the set of discontinuity points is not dense. It has been proved that such function cannot be discontinuous everywhere: separately continuous functions $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ but nowhere continuous","f(x,y) \mathbb{R}^2 \mathbb{R} \mathbb{R}^2 f(x,y)= 1−4(xy/(x^2+y^2))^2 (0,0) 1 - \sin(2\theta)^2 (0,0)","['real-analysis', 'functions', 'continuity']"
72,Question on Borel Measurable Functions and Borel Sigma Algebras.,Question on Borel Measurable Functions and Borel Sigma Algebras.,,"On an assignment, we have been given a the following setup: Let $ f:A\rightarrow \bar{\mathbb{R}}$ be a borel measurable function. prove that if $f$ is Borel measurable and $B$ is a Borel set, then $f^{-1}(B)$ is a Borel set. The definition of Borel Measurability we were given is as follows: ""A function $f : \mathbb{R} \to \mathbb{R}$ is said to be Borel measurable provided its domain A ⊆ R is a Borel set and for each c, the set { $x ∈ A : f (x) < c$ } is a Borel set. We were not given any description of where this set lives, I'm assuming $B \subset \mathbb{R}$ but this may be incorrect. I figure we need to show that the set { ${B \subset \mathbb{R}:f^{-1}(B)}$ is a Borel set} is a sigma algebra but I am unsure how to do this. I know its a little silly because all you need to do is check that the definitions of a sigma algebra are met but showing those things is proving more difficult than i anticipated. We may also use the fact that borel measurable functions are Lebesgue measureable. Any help would be greatly appreciated!","On an assignment, we have been given a the following setup: Let be a borel measurable function. prove that if is Borel measurable and is a Borel set, then is a Borel set. The definition of Borel Measurability we were given is as follows: ""A function is said to be Borel measurable provided its domain A ⊆ R is a Borel set and for each c, the set { } is a Borel set. We were not given any description of where this set lives, I'm assuming but this may be incorrect. I figure we need to show that the set { is a Borel set} is a sigma algebra but I am unsure how to do this. I know its a little silly because all you need to do is check that the definitions of a sigma algebra are met but showing those things is proving more difficult than i anticipated. We may also use the fact that borel measurable functions are Lebesgue measureable. Any help would be greatly appreciated!", f:A\rightarrow \bar{\mathbb{R}} f B f^{-1}(B) f : \mathbb{R} \to \mathbb{R} x ∈ A : f (x) < c B \subset \mathbb{R} {B \subset \mathbb{R}:f^{-1}(B)},"['real-analysis', 'measure-theory', 'borel-sets', 'borel-measures']"
73,On reciprocal-sums of integer polynomials,On reciprocal-sums of integer polynomials,,"Let's consider the class $P \subset \mathbb{Z}[x]$ of nonconstant polynomials whose degree is at least two and with no roots in the positive integers. For $p \in P$ , I'm interested in considering sums of the form $$S_{p,N} = \sum_{j=1}^{N} \frac{1}{p(j)}$$ $$\textbf{Question 1: Size}$$ If we assume the coefficients are nonnegative (with the leading coefficient positive), it's easy to deduce an absolute upper bound of $2$ , by comparing with $\sum n^{-2} = \frac{\pi^2}{6}$ . However, we allow negative coefficients, in which case it's not as clear if there is such a simple upper bound (at least not to me). What is $\sup\{S_{p,N}: (p,N) \in P \times \mathbb{N}\}$ ? Further, for a fixed degree $d \geq 2$ , what is $\sup\{S_{p,N}: (p,N) \in P \times \mathbb{N} \ \text{and} \ \text{deg}(p) = d\}$ ? I've tried Lagrange interpolation style arguments to prove that the former is $\infty$ , but have not quite been able to make it work. $$\textbf{Question 2: Integrality}$$ For a given $p$ , at most how many $N$ exist such that $S_{p,N}$ is an integer? Since $\sum_{j=1}^{\infty} \frac{1}{p(j)}$ converges and $p$ nonconstant, it's easy to see that there can only be finitely many such $N$ .","Let's consider the class of nonconstant polynomials whose degree is at least two and with no roots in the positive integers. For , I'm interested in considering sums of the form If we assume the coefficients are nonnegative (with the leading coefficient positive), it's easy to deduce an absolute upper bound of , by comparing with . However, we allow negative coefficients, in which case it's not as clear if there is such a simple upper bound (at least not to me). What is ? Further, for a fixed degree , what is ? I've tried Lagrange interpolation style arguments to prove that the former is , but have not quite been able to make it work. For a given , at most how many exist such that is an integer? Since converges and nonconstant, it's easy to see that there can only be finitely many such .","P \subset \mathbb{Z}[x] p \in P S_{p,N} = \sum_{j=1}^{N} \frac{1}{p(j)} \textbf{Question 1: Size} 2 \sum n^{-2} = \frac{\pi^2}{6} \sup\{S_{p,N}: (p,N) \in P \times \mathbb{N}\} d \geq 2 \sup\{S_{p,N}: (p,N) \in P \times \mathbb{N} \ \text{and} \ \text{deg}(p) = d\} \infty \textbf{Question 2: Integrality} p N S_{p,N} \sum_{j=1}^{\infty} \frac{1}{p(j)} p N","['real-analysis', 'number-theory']"
74,"Showing that $X= \left\{ u \in C\bigl([0, \infty);E\bigr) \mid \sup_{t \geq 0} e^{-kt} \|u(t)\| < \infty \right\}$ is Banach.",Showing that  is Banach.,"X= \left\{ u \in C\bigl([0, \infty);E\bigr) \mid \sup_{t \geq 0} e^{-kt} \|u(t)\| < \infty \right\}","I am working on my degree thesis studying the Hille Yosida Theorem and as a motive to it, I want to present the Cauchy-Lipschitz-Picard Theorem. Haim Brezis highlights the path for a very interesting proof. A part of it, relies on showing that the space $$X= \left\lbrace u \in C\bigl([0, \infty);E\bigr) \mid \sup_{t \geq 0} e^{-kt} \|u(t)\| < \infty \right\rbrace$$ equipped with the norm $\|\cdot\|_X = \sup_{t \geq 0} e^{-kt} \|\cdot\|$ is a Banach space. Note that $E$ is a Banach space too. I've made an attempt but I am not sure if it's properly stated. Let $u_n \in X$ be a Cauchy sequence. Then, $\forall \varepsilon >0$ there exists $n_0 \in \mathbb N$ such that : $$\|u_n-u_m\|_X < \varepsilon, \quad \forall n,m \geq n_0$$ $$\Leftrightarrow$$ $$\sup_{t \geq 0}e^{-kt}\|u_n(t)-u_m(t)\| < \varepsilon$$ Essentialy, I would like to show that $\|u_n(t)-u_m(t)\|< \varepsilon$ and then by letting $m\to \infty$ I get what I need. But, that exponential is causing issues since it's decreasing $t$ grows. I thought about fixing a $t_0 \in [0,\infty)$ which would then mean $$\sup_{t\geq 0} e^{-kt_0} \|u_n(t_0)-u_m(t_0)\| < \varepsilon$$ $$\implies$$ $$\|u_n(t_0)-u_m(t_0)\| < \varepsilon'$$ where $\varepsilon' = \varepsilon e^{kt_0}$ . That yields that $\{u_n(t_0)\}$ is a Cauchy sequence in $X$ for any $t_0$ fixed. Letting $m \to \infty$ now, leads us to $\|u_n(t_0) - u(t_0)\| < \varepsilon'$ . Now, since $u \in C\left( [0,\infty);E\right)$ then of course $u$ is continuous at every point $t \in [0,\infty)$ then it should be $\|u(t) - u(t_0)\| < \varepsilon''$ for every such $t_0$ fixed with $\varepsilon''$ depending on $t_0$ , thus generalising the result. I am afraid that my elaboration, especially at the end, isn't totally correct. Since this is a part of my Thesis, I am trying to find a perfect way around it. I hope some of my intuition is of use or even makes sense. I would highly appreciate any help or tips.","I am working on my degree thesis studying the Hille Yosida Theorem and as a motive to it, I want to present the Cauchy-Lipschitz-Picard Theorem. Haim Brezis highlights the path for a very interesting proof. A part of it, relies on showing that the space equipped with the norm is a Banach space. Note that is a Banach space too. I've made an attempt but I am not sure if it's properly stated. Let be a Cauchy sequence. Then, there exists such that : Essentialy, I would like to show that and then by letting I get what I need. But, that exponential is causing issues since it's decreasing grows. I thought about fixing a which would then mean where . That yields that is a Cauchy sequence in for any fixed. Letting now, leads us to . Now, since then of course is continuous at every point then it should be for every such fixed with depending on , thus generalising the result. I am afraid that my elaboration, especially at the end, isn't totally correct. Since this is a part of my Thesis, I am trying to find a perfect way around it. I hope some of my intuition is of use or even makes sense. I would highly appreciate any help or tips.","X= \left\lbrace u \in C\bigl([0, \infty);E\bigr) \mid \sup_{t \geq 0} e^{-kt} \|u(t)\| < \infty \right\rbrace \|\cdot\|_X = \sup_{t \geq 0} e^{-kt} \|\cdot\| E u_n \in X \forall \varepsilon >0 n_0 \in \mathbb N \|u_n-u_m\|_X < \varepsilon, \quad \forall n,m \geq n_0 \Leftrightarrow \sup_{t \geq 0}e^{-kt}\|u_n(t)-u_m(t)\| < \varepsilon \|u_n(t)-u_m(t)\|< \varepsilon m\to \infty t t_0 \in [0,\infty) \sup_{t\geq 0} e^{-kt_0} \|u_n(t_0)-u_m(t_0)\| < \varepsilon \implies \|u_n(t_0)-u_m(t_0)\| < \varepsilon' \varepsilon' = \varepsilon e^{kt_0} \{u_n(t_0)\} X t_0 m \to \infty \|u_n(t_0) - u(t_0)\| < \varepsilon' u \in C\left( [0,\infty);E\right) u t \in [0,\infty) \|u(t) - u(t_0)\| < \varepsilon'' t_0 \varepsilon'' t_0","['real-analysis', 'functional-analysis', 'banach-spaces', 'cauchy-sequences']"
75,the inverse of $f(x)=x-x^p$,the inverse of,f(x)=x-x^p,"Provided that $f(x)=x-x^p$ , prove that $$f^{-1}(x)=\sum_{k\ge1}{pk\choose k}\frac{x^{1+(p-1)k}}{1+(p-1)k}.$$ I have gotten so far as using the Lagrange Inversion Theorem to show that $$f^{-1}(x)=\sum_{k\ge1}g_k\frac{x^k}{k!}$$ where $$g_k=\lim_{w\to0}\left[\left(\frac{d}{dw}\right)^{k-1}(1-w^{p-1})^{-k}\right],$$ but I have no idea how to compute this limit, or where to go from there. Could I have some help? Thanks :)","Provided that , prove that I have gotten so far as using the Lagrange Inversion Theorem to show that where but I have no idea how to compute this limit, or where to go from there. Could I have some help? Thanks :)","f(x)=x-x^p f^{-1}(x)=\sum_{k\ge1}{pk\choose k}\frac{x^{1+(p-1)k}}{1+(p-1)k}. f^{-1}(x)=\sum_{k\ge1}g_k\frac{x^k}{k!} g_k=\lim_{w\to0}\left[\left(\frac{d}{dw}\right)^{k-1}(1-w^{p-1})^{-k}\right],","['real-analysis', 'sequences-and-series', 'limits', 'derivatives', 'inverse']"
76,Proving a space is complete by assuming Cauchy sequences converge,Proving a space is complete by assuming Cauchy sequences converge,,"I had an interesting debate with some classmates on how to prove that a space is complete. I went through the process of taking a Cauchy sequence, finding a candidate limit, and showing that the limit is both in the space and is the actual limit of the Cauchy sequence. However, some of my classmates instead supposed that the Cauchy sequence converged to something , and proceeded to show that the limit is in the space. Is their method correct? It seems odd to me to assume that the Cauchy sequence converges. I know we could use this to show that the space is closed, but we are not assuming that the ambient space is complete. They argue that every Cauchy sequence converges in the completion of the metric space so it is no problem assuming that the Cauchy sequence converges: all that is left is to show that the limit belongs to the space. Is this a legitimate strategy for showing that a space is complete? EDIT: More explicitly, to show a space $X$ is complete, can we say ``let $(x_n)$ be a Cauchy sequence that converges to $x$ . It suffices to show $x\in X$ ?''","I had an interesting debate with some classmates on how to prove that a space is complete. I went through the process of taking a Cauchy sequence, finding a candidate limit, and showing that the limit is both in the space and is the actual limit of the Cauchy sequence. However, some of my classmates instead supposed that the Cauchy sequence converged to something , and proceeded to show that the limit is in the space. Is their method correct? It seems odd to me to assume that the Cauchy sequence converges. I know we could use this to show that the space is closed, but we are not assuming that the ambient space is complete. They argue that every Cauchy sequence converges in the completion of the metric space so it is no problem assuming that the Cauchy sequence converges: all that is left is to show that the limit belongs to the space. Is this a legitimate strategy for showing that a space is complete? EDIT: More explicitly, to show a space is complete, can we say ``let be a Cauchy sequence that converges to . It suffices to show ?''",X (x_n) x x\in X,"['real-analysis', 'metric-spaces']"
77,Solve a function based on an inequality,Solve a function based on an inequality,,"Problem: Suppose that $f(x)\in C^\infty (R),~~$ and $~~ \forall n\in N,~~x\in R,~~$ we have $$|f^{(n)}(x)|\leq e^x$$ Prove that $f(x)=f(0)e^x$ . My thought is that prove the derivative of $g=fe^{-x}$ is zero. However, I can only prove $|g^{(n)}(x)|\leq 2^n$ by mathematical induction and given conditions.","Problem: Suppose that and we have Prove that . My thought is that prove the derivative of is zero. However, I can only prove by mathematical induction and given conditions.","f(x)\in C^\infty (R),~~ ~~ \forall n\in N,~~x\in R,~~ |f^{(n)}(x)|\leq e^x f(x)=f(0)e^x g=fe^{-x} |g^{(n)}(x)|\leq 2^n","['real-analysis', 'sequences-and-series', 'complex-analysis', 'analysis']"
78,"A ""proof"" for $0=1$ by integrating $\int \frac{dx}{x\ln x}$ by parts [duplicate]","A ""proof"" for  by integrating  by parts [duplicate]",0=1 \int \frac{dx}{x\ln x},"This question already has answers here : Fallacious proof involving trigonometry (6 answers) Using Integration By Parts results in 0 = 1 (5 answers) $-1 = 0$ by integration by parts of $\tan(x)$ (3 answers) Closed 4 years ago . Let's consider the indefinite integral $$\int \frac{dx}{x\ln x}.$$ We will compute it by integrating by parts: $$\int \frac{dx}{x\ln x}=\int (\ln x)'\frac{dx}{\ln x}=1+\int \frac{dx}{x\ln x}.$$ Hence, $0=1$ . The question is : is there anything wrong in these computations? Note: I came across this example while reading a book on counterexamples in real analysis. I thought that the people here would find this funny, especially because the error is not so obvious to everyone.","This question already has answers here : Fallacious proof involving trigonometry (6 answers) Using Integration By Parts results in 0 = 1 (5 answers) $-1 = 0$ by integration by parts of $\tan(x)$ (3 answers) Closed 4 years ago . Let's consider the indefinite integral We will compute it by integrating by parts: Hence, . The question is : is there anything wrong in these computations? Note: I came across this example while reading a book on counterexamples in real analysis. I thought that the people here would find this funny, especially because the error is not so obvious to everyone.",\int \frac{dx}{x\ln x}. \int \frac{dx}{x\ln x}=\int (\ln x)'\frac{dx}{\ln x}=1+\int \frac{dx}{x\ln x}. 0=1,"['real-analysis', 'calculus', 'integration', 'recreational-mathematics', 'fake-proofs']"
79,$xf$ smooth $\Rightarrow$ $f$ smooth?,smooth   smooth?,xf \Rightarrow f,"Question. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function such that $x\mapsto xf(x)$ is smooth. Is $f$ necessarily smooth? Here are some thoughts on the problem: Clearly $f$ is smooth on $\mathbb{R}\backslash 0$ so one only needs to show that the derivatives $f^{(j)}(x)$ (for $x\neq 0$ ) have the same limit as $x\rightarrow 0$ from left and right. If $xf$ is analytic, then also $f$ is analytic. This is clear as its power series around $0$ is obtained from the one of $xf$ by factoring out one $x$ . For smooth functions one would need to know something about the regularity of $x\mapsto x^{-1}R_kf(x)$ , where $R_kf$ is the remainder of the order $k$ Taylor expansion. Showing smoothness of this is essentially equivalent to the original question. One can without loss of generality assume that $f$ has compact support. Then the Fourier transform satisfies $\hat f\in C_0$ and $\frac{d}{d\xi}\hat f \in \mathcal{S}$ (Schwartz functions). If this would imply that $\hat f\in \mathcal{S}$ , then the question would be solved.","Question. Let be a continuous function such that is smooth. Is necessarily smooth? Here are some thoughts on the problem: Clearly is smooth on so one only needs to show that the derivatives (for ) have the same limit as from left and right. If is analytic, then also is analytic. This is clear as its power series around is obtained from the one of by factoring out one . For smooth functions one would need to know something about the regularity of , where is the remainder of the order Taylor expansion. Showing smoothness of this is essentially equivalent to the original question. One can without loss of generality assume that has compact support. Then the Fourier transform satisfies and (Schwartz functions). If this would imply that , then the question would be solved.",f:\mathbb{R}\rightarrow\mathbb{R} x\mapsto xf(x) f f \mathbb{R}\backslash 0 f^{(j)}(x) x\neq 0 x\rightarrow 0 xf f 0 xf x x\mapsto x^{-1}R_kf(x) R_kf k f \hat f\in C_0 \frac{d}{d\xi}\hat f \in \mathcal{S} \hat f\in \mathcal{S},"['real-analysis', 'calculus']"
80,"Show that the $n$-th Fibonacci number is given by $\frac{\cosh na}{\cosh a}$ or $\frac{\sinh na}{\cosh a}$, where $\sinh a=1/2$","Show that the -th Fibonacci number is given by  or , where",n \frac{\cosh na}{\cosh a} \frac{\sinh na}{\cosh a} \sinh a=1/2,"This question is taken from book: Advanced Calculus: An Introduction to Classical Analysis, by Louis Brand. The book is concerned with introductory real analysis. I request to help find the solution. Show that the general term of Fibonacci sequence $1,1,2,3,5,\cdots$ , is given by : $f_n = \frac{\cosh\, n\alpha}{\cosh\, \alpha}$ ( $n$ odd), $f_n = \frac{\sinh\, n\alpha}{\cosh\, \alpha}$ ( $n$ even), where $\sinh \alpha= \frac 12$ , and that $\lim \frac{f_{n+1}}{f_n} = e^\alpha.$ As given here : The hyperbolic functions $\sinh$ , $\cosh $ are given by: $$\sinh\,z = \frac{e^z - e^{-z}}{2}, \cosh\,z = \frac{e^z + e^{-z}}{2}$$ where $\,z= x+iy\,\,$ is a complex variable. These functions satisfy the identities: $$\cosh^2\,z-\sinh^2\,z = 1,\,\, \cosh\,iy = \cosh\,y,\,\, \sinh\,iy = i\,\sinh\,y$$ If $\sinh\,z = \frac{e^z - e^{-z}}{2} = \frac 12$ , then $\cosh^2\,z-\sinh^2\,z = 1\,\,$ gives $\cosh^2\,z=\frac 54$ This yields nothing, so consider opposite approach of taking help by another means of generating Fibonacci sequence terms. Say, the polynomial $f(x) = x+1$ yields terms for successive values for $x \in \mathbb{N}$ as $2,3,\dots$ . So, took help from paper here that concerns with fibonacci polynomials, with text portion from page 1 copied below: The Fibonacci polynomials $\{F_n (x)\}$ are defined by (1.1) $F_1(x) = 1, F_2(x) = x$ , and $F_{n+1}(x) = xF_n(x) + F_{n-1}x$ . Notice that, when $x = 1, F_n(1) = F_n$ , the $n$ Fibonacci number. It is easy to verify that the relation (1.2) $F_{-n}(x) = (-1)^{n+1} F_n(x)$ extends the definition of Fibonacci polynomials to all integral subscripts. But, this also doesn't help.","This question is taken from book: Advanced Calculus: An Introduction to Classical Analysis, by Louis Brand. The book is concerned with introductory real analysis. I request to help find the solution. Show that the general term of Fibonacci sequence , is given by : ( odd), ( even), where , and that As given here : The hyperbolic functions , are given by: where is a complex variable. These functions satisfy the identities: If , then gives This yields nothing, so consider opposite approach of taking help by another means of generating Fibonacci sequence terms. Say, the polynomial yields terms for successive values for as . So, took help from paper here that concerns with fibonacci polynomials, with text portion from page 1 copied below: The Fibonacci polynomials are defined by (1.1) , and . Notice that, when , the Fibonacci number. It is easy to verify that the relation (1.2) extends the definition of Fibonacci polynomials to all integral subscripts. But, this also doesn't help.","1,1,2,3,5,\cdots f_n = \frac{\cosh\, n\alpha}{\cosh\, \alpha} n f_n = \frac{\sinh\, n\alpha}{\cosh\, \alpha} n \sinh \alpha= \frac 12 \lim \frac{f_{n+1}}{f_n} = e^\alpha. \sinh \cosh  \sinh\,z = \frac{e^z - e^{-z}}{2}, \cosh\,z = \frac{e^z + e^{-z}}{2} \,z= x+iy\,\, \cosh^2\,z-\sinh^2\,z = 1,\,\, \cosh\,iy = \cosh\,y,\,\, \sinh\,iy = i\,\sinh\,y \sinh\,z = \frac{e^z - e^{-z}}{2} = \frac 12 \cosh^2\,z-\sinh^2\,z = 1\,\, \cosh^2\,z=\frac 54 f(x) = x+1 x \in \mathbb{N} 2,3,\dots \{F_n (x)\} F_1(x) = 1, F_2(x) = x F_{n+1}(x) = xF_n(x) + F_{n-1}x x = 1, F_n(1) = F_n n F_{-n}(x) = (-1)^{n+1} F_n(x)","['real-analysis', 'sequences-and-series', 'recurrence-relations', 'fibonacci-numbers', 'hyperbolic-functions']"
81,"Existence of the $\Omega$ set in ""Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization""","Existence of the  set in ""Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization""",\Omega,"In the proof of Lemma 4.3 in [1], they claim the following: Let $U$ be a subspace of $\mathbb{R}^{m\times n}$ with dim $(U)=d$ and let $\delta>0$ . Then, there exists a set $\Omega\subset\mathbb{R}^{m\times n}$ wiht at most $(12/\delta)^d$ elements such that for every $X\in U$ with $\lVert X\rVert_F\leq 1$ there exists a $Q\in\Omega$ such that $\lVert X-Q\rVert_F \leq \delta/4$ . I don't see the reason why this is true. References [1] Recht, B., Fazel, M., & Parrilo, P. A. (2010). Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3), 471-501.","In the proof of Lemma 4.3 in [1], they claim the following: Let be a subspace of with dim and let . Then, there exists a set wiht at most elements such that for every with there exists a such that . I don't see the reason why this is true. References [1] Recht, B., Fazel, M., & Parrilo, P. A. (2010). Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3), 471-501.",U \mathbb{R}^{m\times n} (U)=d \delta>0 \Omega\subset\mathbb{R}^{m\times n} (12/\delta)^d X\in U \lVert X\rVert_F\leq 1 Q\in\Omega \lVert X-Q\rVert_F \leq \delta/4,"['real-analysis', 'linear-algebra', 'matrices']"
82,Definition of a measure subspace,Definition of a measure subspace,,"The measure theory notes (John K. Hunter) state: Definition 1.9 For set $X$ and $\sigma$ -algebra $\mathcal{A}$ on set X, a measure $\mu$ on the measurable space $(X, \mathcal{A})$ is a function such that: $\mu(\emptyset) = 0$ It is countably additive. In other words, if $\lbrace A_{i} \in \mathcal{A} : i \in \mathbb{N} \rbrace$ is a countable disjoint collection of sets in $\mathcal{A}$ , then $$\mu \left ( \bigcup_{i = 1}^{\infty} A_{i} \right) = \sum_{i = 1}^{\infty} \mu(A_{i})$$ Definition 1.10 If $(X, \mathcal{A}, \mu)$ is a measure space (a measurable space and a particular measure) and $E \subset X$ is a measurable subset, then the measurable subspace $(E, {\mathcal{A}|}_{E}, {\mu|}_{E})$ is defined by restricting $\mu$ to $E$ where: ${\mathcal{A}|}_{E} = \lbrace A \cap E : A \in \mathcal{A} \rbrace$ ${\mu|}_{E}(A \cap E) = \mu(A \cap E)$ My question From the answer in Show that subspace measure is a measure , I get that the measure subspace is indeed a measure space, except that the answer seems not to mention that $E$ need to be a ""measurable subset"". My interpretation of that "" $E$ is a measurable subset of $X$ "" is that: $E$ is a subset of $X$ there exists a collection of subsets of $E$ that forms a $\sigma$ -algebra. Why does $E$ really have to be a ""measurable subset"" of $X$ instead of just a subset of $X$ ?","The measure theory notes (John K. Hunter) state: Definition 1.9 For set and -algebra on set X, a measure on the measurable space is a function such that: It is countably additive. In other words, if is a countable disjoint collection of sets in , then Definition 1.10 If is a measure space (a measurable space and a particular measure) and is a measurable subset, then the measurable subspace is defined by restricting to where: My question From the answer in Show that subspace measure is a measure , I get that the measure subspace is indeed a measure space, except that the answer seems not to mention that need to be a ""measurable subset"". My interpretation of that "" is a measurable subset of "" is that: is a subset of there exists a collection of subsets of that forms a -algebra. Why does really have to be a ""measurable subset"" of instead of just a subset of ?","X \sigma \mathcal{A} \mu (X, \mathcal{A}) \mu(\emptyset) = 0 \lbrace A_{i} \in \mathcal{A} : i \in \mathbb{N} \rbrace \mathcal{A} \mu \left ( \bigcup_{i = 1}^{\infty} A_{i} \right) = \sum_{i = 1}^{\infty} \mu(A_{i}) (X, \mathcal{A}, \mu) E \subset X (E, {\mathcal{A}|}_{E}, {\mu|}_{E}) \mu E {\mathcal{A}|}_{E} = \lbrace A \cap E : A \in \mathcal{A} \rbrace {\mu|}_{E}(A \cap E) = \mu(A \cap E) E E X E X E \sigma E X X","['real-analysis', 'analysis', 'measure-theory']"
83,What is the definition of double sequence $a_{mn}$ being convergent to $l$?,What is the definition of double sequence  being convergent to ?,a_{mn} l,"What is the definition of double sequence $a_{mn}$ being convergent to $l$ ? I have this definition. Definition: The double sequence $(a_{m,n})^∞_{m,n=1}$ is said to Converge to the real number $A∈ \mathbb R$ if for all $ϵ>0$ there exists an $N∈ \mathbb N$ such that if $m,n≥N$ then $∣a_{m,n}−A∣<ϵ$ and we say $A$ is the Double Limit of this double sequence written lim $_{m,n→∞}a_{m,n}=A$ . If no such $A∈ \mathbb R$ satisfies this, then we say that the the double sequence $(a_{m,n})^∞_{m,n=1}$ diverges.  I took help from here . If I go by this definition then convergent double sequence $(a_{m,n})^∞_{m,n=1}$ may not be bounded. Example : $a_{1n} = n$ , $ a_{mn} = 1/m + 1/n$ for all $m \geq 2 $ and $n\in \mathbb N$ It seems odd to me. I feel that I am going wrong anywhere. Can anyone please tell me where I am being wrong?","What is the definition of double sequence being convergent to ? I have this definition. Definition: The double sequence is said to Converge to the real number if for all there exists an such that if then and we say is the Double Limit of this double sequence written lim . If no such satisfies this, then we say that the the double sequence diverges.  I took help from here . If I go by this definition then convergent double sequence may not be bounded. Example : , for all and It seems odd to me. I feel that I am going wrong anywhere. Can anyone please tell me where I am being wrong?","a_{mn} l (a_{m,n})^∞_{m,n=1} A∈ \mathbb R ϵ>0 N∈ \mathbb N m,n≥N ∣a_{m,n}−A∣<ϵ A _{m,n→∞}a_{m,n}=A A∈ \mathbb R (a_{m,n})^∞_{m,n=1} (a_{m,n})^∞_{m,n=1} a_{1n} = n  a_{mn} = 1/m + 1/n m \geq 2  n\in \mathbb N","['real-analysis', 'sequences-and-series', 'double-sequence']"
84,Weak version of the Banach-Tarski Paradox for Circles,Weak version of the Banach-Tarski Paradox for Circles,,"It is well known that the Banach-Tarski paradox does not translate to circles, i.e. a circle cannot be decomposed into finitely many pieces which can be rearranged to form two circles of equal circumference. However what I'd like to know is can a circle be decomposed into finitely many pieces that can be rearranged to form two circles whose difference in circumference is arbitrarily small? Thank you.","It is well known that the Banach-Tarski paradox does not translate to circles, i.e. a circle cannot be decomposed into finitely many pieces which can be rearranged to form two circles of equal circumference. However what I'd like to know is can a circle be decomposed into finitely many pieces that can be rearranged to form two circles whose difference in circumference is arbitrarily small? Thank you.",,"['real-analysis', 'geometry', 'measure-theory']"
85,Littlewood's inequality for $L^p$ spaces,Littlewood's inequality for  spaces,L^p,"I have tried to prove the following inequality, but I couldn't do yet. Prove the following interpolation estimate: $$\| u\|_q \leq \| u\|_p^{\theta} \| u\|_r^{1- \theta}$$ where $p≤q≤r$ , $θ∈[0,1]$ and $\frac{1}{q} = \frac{\theta}{p} + \frac{1-\theta}{r}$ . Note that $\| u\|_q $ denotes $L^q$ norm. Analysis kills me. Any help will be appreciated.","I have tried to prove the following inequality, but I couldn't do yet. Prove the following interpolation estimate: where , and . Note that denotes norm. Analysis kills me. Any help will be appreciated.","\| u\|_q \leq \| u\|_p^{\theta} \| u\|_r^{1- \theta} p≤q≤r θ∈[0,1] \frac{1}{q} = \frac{\theta}{p} + \frac{1-\theta}{r} \| u\|_q  L^q",['real-analysis']
86,$\int_A fd\mu=0$ for all $A$ in a generator of the $\sigma$-algebra $\Rightarrow$ $f=0$ $\mu$-almost everywhere?,for all  in a generator of the -algebra   -almost everywhere?,\int_A fd\mu=0 A \sigma \Rightarrow f=0 \mu,"Assume I have a measurable signed function $f$ for which the integral with respect to a measure $\mu$ , on some measurable sets $\mathcal{C}$ generating the sigma algebra $\mathcal{E}$ , is zero (maybe a $\pi$ -system). Is this enuough to imply the function is zero almost everywhere, if not what is needed? Thanks.","Assume I have a measurable signed function for which the integral with respect to a measure , on some measurable sets generating the sigma algebra , is zero (maybe a -system). Is this enuough to imply the function is zero almost everywhere, if not what is needed? Thanks.",f \mu \mathcal{C} \mathcal{E} \pi,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
87,"Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Find $|S|$.",Let . Find .,"S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\} |S|","Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$ . Find $|S|$ . This is a follow-up question to this . (Thanks to @EricWofsey for brilliant answer.) Please see the previous post to see his proof since I don't want to reproduce it here. I'm phrasing this as a separate question because I want to accept his answer. Still, it's not obvious to me that what's the exact cardinality of $S$ . I haven't found any polynomials with degree $> 3$ satisfy this property. The one who gave the initial problem to me didn't know the answer, either. There is enough evidence to suspect this is going to be much harder. Can someone provide some insight? Many thanks.","Let . Find . This is a follow-up question to this . (Thanks to @EricWofsey for brilliant answer.) Please see the previous post to see his proof since I don't want to reproduce it here. I'm phrasing this as a separate question because I want to accept his answer. Still, it's not obvious to me that what's the exact cardinality of . I haven't found any polynomials with degree satisfy this property. The one who gave the initial problem to me didn't know the answer, either. There is enough evidence to suspect this is going to be much harder. Can someone provide some insight? Many thanks.","S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\} |S| S > 3","['real-analysis', 'analysis', 'polynomials']"
88,differential forms- $\omega $ closed but not exact,differential forms-  closed but not exact,\omega ,"let be $$ \omega= |x|^{-3} \left(x_1 dx_2 \wedge dx_3+x_2dx_3 \wedge dx_1 + x_3dx_1 \wedge dx_2\right) $$ and $G:= \mathbb{R}^3 \backslash \{ 0 \} $ I want to prove, that $ \omega$ is closed, but not exact That $ \omega $ is closed, I can prove it by looking if $ d\omega =0 $ But how can I prove it's not exact? I know that a continous 2-form is exact in $G$ if there exists a 1-Form $y$ so, that $\omega = dy $ how can I show there doesn't exist such $y$ ? Edit: Showing $\int_S \omega \neq 0 $ Where $S$ is the unitsphere Set $r=1$ and set $$x_1= \sin \phi \cos \theta $$ $$x_2= \sin \phi \sin \theta$$ $$x_3= \cos \phi $$ $ \theta=[0, 2\pi], \phi=[0, \pi]$ then $ dx_1 \wedge dx_2 = -\sin \phi \cos \phi d\theta \wedge d\phi $ $ dx_2 \wedge dx_3 = -\sin^2 \phi \cos \theta d\theta \wedge d\phi $ $ dx_3 \wedge dx_1 = -\sin^2 \phi \sin \theta d \theta \wedge d\phi $ Putting in the equation: $$ \int_0^{2 \pi} \int_0^{ \pi} |x| ( \sin \phi \cos \theta - \sin^2 \phi \cos \theta ~d \theta \wedge d\ \phi \\+ \sin \phi \sin \theta -\sin^2 \phi \sin \theta ~d\theta \wedge d\phi + \cos \phi -\sin \phi \cos \phi~ d \theta \wedge d\phi $$ what do I put for $x$ ? I don't know how to solve the integral thank you for any help!","let be and I want to prove, that is closed, but not exact That is closed, I can prove it by looking if But how can I prove it's not exact? I know that a continous 2-form is exact in if there exists a 1-Form so, that how can I show there doesn't exist such ? Edit: Showing Where is the unitsphere Set and set then Putting in the equation: what do I put for ? I don't know how to solve the integral thank you for any help!"," \omega= |x|^{-3} \left(x_1 dx_2 \wedge dx_3+x_2dx_3 \wedge dx_1 + x_3dx_1 \wedge dx_2\right)  G:= \mathbb{R}^3 \backslash \{ 0 \}   \omega  \omega   d\omega =0  G y \omega = dy  y \int_S \omega \neq 0  S r=1 x_1= \sin \phi \cos \theta  x_2= \sin \phi \sin \theta x_3= \cos \phi   \theta=[0, 2\pi], \phi=[0, \pi]  dx_1 \wedge dx_2 = -\sin \phi \cos \phi d\theta \wedge d\phi   dx_2 \wedge dx_3 = -\sin^2 \phi \cos \theta d\theta \wedge d\phi   dx_3 \wedge dx_1 = -\sin^2 \phi \sin \theta d \theta \wedge d\phi   \int_0^{2 \pi} \int_0^{ \pi} |x| ( \sin \phi \cos \theta - \sin^2 \phi \cos \theta ~d \theta \wedge d\ \phi \\+ \sin \phi \sin \theta -\sin^2 \phi \sin \theta ~d\theta \wedge d\phi + \cos \phi -\sin \phi \cos \phi~ d \theta \wedge d\phi  x","['real-analysis', 'multivariable-calculus', 'differential-forms']"
89,Taylor series expansion of $\frac{1}{\sqrt{1-\beta x(x+1)}}$,Taylor series expansion of,\frac{1}{\sqrt{1-\beta x(x+1)}},"I am trying to find the taylor series expansion about $0$ (maclaurin series) of $$x \rightarrow \frac{1}{\sqrt{1-\beta x(x+1)}} \text{ with } \beta \in \mathbb{R}^{+*}$$ I've tried using the taylor series expansion of $$\frac{1}{\sqrt{1-X}} = \sum_{n=0}^{\infty}4^{-n}{2n \choose n}X^n \text{ }\text{ }\text{ }\text{ with } \text{ } X=\beta x(x+1)$$ But I can't turn it into a power series because of the $(x+1)^n$ ... I've also tried to derive $$\frac{1}{n!}\cdot\frac{\text{d}^n}{\text{d}x^n}\left(\frac{1}{\sqrt{1-\beta x(x+1)}}\right)_{x=0}$$ But no results so far... Edit : with a more powerful method, I found that, if we call $\left(a_n\right)_{n\in\mathbb{N}}$ the coefficients of the taylor series expansion $\left(\frac{1}{\sqrt{1-\beta x(x+1)}}=\sum_{n=0}^{\infty}a_nx^n\right)$ , the sequence $\left(a_n\right)_{n\in\mathbb{N}}$ is then defined by : $$\forall n\geq3, \text{} na_n=\beta\left(n-\frac{1}{2}\right)a_{n-1}+\beta\left(n-1\right)a_{n-2}$$ $$\text{with }\text{ }a_1 = \frac{\beta}{2} \text{ , } a_2 = \frac{3}{8}\beta^2+\frac{1}{2}\beta$$ It's definitely a step forward, but I don't know how to proceed from there. Is there a way to handle sequences that are defined by such a way ?","I am trying to find the taylor series expansion about (maclaurin series) of I've tried using the taylor series expansion of But I can't turn it into a power series because of the ... I've also tried to derive But no results so far... Edit : with a more powerful method, I found that, if we call the coefficients of the taylor series expansion , the sequence is then defined by : It's definitely a step forward, but I don't know how to proceed from there. Is there a way to handle sequences that are defined by such a way ?","0 x \rightarrow \frac{1}{\sqrt{1-\beta x(x+1)}} \text{ with } \beta \in \mathbb{R}^{+*} \frac{1}{\sqrt{1-X}} = \sum_{n=0}^{\infty}4^{-n}{2n \choose n}X^n \text{ }\text{ }\text{ }\text{ with } \text{ } X=\beta x(x+1) (x+1)^n \frac{1}{n!}\cdot\frac{\text{d}^n}{\text{d}x^n}\left(\frac{1}{\sqrt{1-\beta x(x+1)}}\right)_{x=0} \left(a_n\right)_{n\in\mathbb{N}} \left(\frac{1}{\sqrt{1-\beta x(x+1)}}=\sum_{n=0}^{\infty}a_nx^n\right) \left(a_n\right)_{n\in\mathbb{N}} \forall n\geq3, \text{} na_n=\beta\left(n-\frac{1}{2}\right)a_{n-1}+\beta\left(n-1\right)a_{n-2} \text{with }\text{ }a_1 = \frac{\beta}{2} \text{ , } a_2 = \frac{3}{8}\beta^2+\frac{1}{2}\beta","['real-analysis', 'calculus', 'sequences-and-series', 'power-series', 'taylor-expansion']"
90,Examples of some Pointwise Convergent Sequences of Functions,Examples of some Pointwise Convergent Sequences of Functions,,"I have recently come across pointwise/uniformly convergent sequences of functions, and I am hoping if someone could give some examples of certain sequences of functions so that I could understand the concept better. Thanks! • Pointwise convergent sequences that do / do not preserve continuity, • Pointwise convergent sequences that do / do not preserve integrals. It seems that $f_n(x) = x^n$ is an example that do not preserve continuity?","I have recently come across pointwise/uniformly convergent sequences of functions, and I am hoping if someone could give some examples of certain sequences of functions so that I could understand the concept better. Thanks! • Pointwise convergent sequences that do / do not preserve continuity, • Pointwise convergent sequences that do / do not preserve integrals. It seems that is an example that do not preserve continuity?",f_n(x) = x^n,"['real-analysis', 'pointwise-convergence']"
91,Continuous map between $L^p$ spaces,Continuous map between  spaces,L^p,"The following theorem appears in Appendix B of Rabinowitz' book Minimax Methods in Critical Point Theory: Let $\Omega \subset \Bbb{R}^n$ be bounded and $g\in C(\overline{\Omega}\times \Bbb {R},\Bbb {R})$ such that there exist constants $r,s\ge 1$ and $a_1,a_2\ge 0$ such that for all $x \in \overline{\Omega}, y\in \Bbb{R}$ $$|g(x,y)|\le a_1 + a_2|y|^{r/s}$$ Then the map $\varphi(x)\mapsto g(x,\varphi(x))$ belongs to $C(L^r(\Omega),L^s(\Omega))$ . In the proof, he says ""To prove the continuity of this map, observe that it is continuous at $\varphi$ if and only if $f(x,z(x)) = g(x,z(x)+\varphi(x))-g(x,\varphi(x))$ is continuous at $z=0$ . Therefore we can assume $\varphi = 0$ and $g(x,0)=0$ ."" I don't understand how this assumpion can be made without loss of generality, and was unable to finish the proof without it. Any help would be appreciated. Edit: I have found a partial answer in a different thread: Continuity proof of a function between $L^p$ spaces In the post it says: Using the growth estimate, one can derive a similar estimate for $f$ of the form: $$ |f(x,z(x))|\leq A_1+A_2 |\phi_0(x)|^{r/p}+A_3|z(x)|^{r/p} $$ This would solve my problem, since the former two constants don't depend on $z$ and can be thrown together, leaving the case that was already proven. However, I couldn't derive this estimate. edit2: I was wrong in assuming this solves the problem since, as pointed out by the users supinf and Peter Melech, the constant may not depend on x.","The following theorem appears in Appendix B of Rabinowitz' book Minimax Methods in Critical Point Theory: Let be bounded and such that there exist constants and such that for all Then the map belongs to . In the proof, he says ""To prove the continuity of this map, observe that it is continuous at if and only if is continuous at . Therefore we can assume and ."" I don't understand how this assumpion can be made without loss of generality, and was unable to finish the proof without it. Any help would be appreciated. Edit: I have found a partial answer in a different thread: Continuity proof of a function between spaces In the post it says: Using the growth estimate, one can derive a similar estimate for of the form: This would solve my problem, since the former two constants don't depend on and can be thrown together, leaving the case that was already proven. However, I couldn't derive this estimate. edit2: I was wrong in assuming this solves the problem since, as pointed out by the users supinf and Peter Melech, the constant may not depend on x.","\Omega \subset \Bbb{R}^n g\in C(\overline{\Omega}\times \Bbb {R},\Bbb {R}) r,s\ge 1 a_1,a_2\ge 0 x \in \overline{\Omega}, y\in \Bbb{R} |g(x,y)|\le a_1 + a_2|y|^{r/s} \varphi(x)\mapsto g(x,\varphi(x)) C(L^r(\Omega),L^s(\Omega)) \varphi f(x,z(x)) = g(x,z(x)+\varphi(x))-g(x,\varphi(x)) z=0 \varphi = 0 g(x,0)=0 L^p f 
|f(x,z(x))|\leq A_1+A_2 |\phi_0(x)|^{r/p}+A_3|z(x)|^{r/p}
 z","['real-analysis', 'functional-analysis', 'analysis', 'lp-spaces']"
92,The relationship between the differential and the directional derivative of a function,The relationship between the differential and the directional derivative of a function,,"I am currently studying differential manifolds (from John M. Lee 's book), and have a question concerning the difference between what is defined as the $\textbf{differential of a function F}$ , and the $\textbf{directional derivative of a function F}$ . Let $M \subset \mathbb{R}^{m}$ , let $N\subset \mathbb{R}$ , and suppose $F:M \rightarrow N$ is a smooth map. Then, The differential of $F$ at $p \in M$ is a map $dF_{p}:T_{p}M \rightarrow T_{F(p)}N$ defined as, for some $v \in T_{p}M$ , $dF_{p}(v)$ is a derivation in $T_{F(p)}N$ defined as, for all $f \in C^{\infty}(N)$ , $dF_{p}(v)(f)=v(f \circ F)$ . Now, because $M$ and $N$ are Euclidean themselves, if $v=(v_{1},...,v_{N})$ , this can be expressed as $dF_{p}(v)(f)=v_{1} \cdot \frac{\partial f}{\partial F}\ \frac{\partial F}{\partial x_{1}}+...+v_{m} \cdot \frac{\partial f}{\partial F}\ \frac{\partial F}{\partial x_{m}}$ . The directional derivative of $F$ at $p$ in direction $v \in \mathbb{R}^{m}$ is given by $D_{v}F(p)=v_{1} \cdot \frac{\partial F(p)}{\partial x_{1}}+...+v_{m}\cdot \frac{\partial F(p)}{\partial x_{m}}$ . Now, it seems that if $Id:\mathbb{R}\rightarrow \mathbb{R}$ is the identity function on $\mathbb{R}$ , we have that for $v \in T_{p}M$ , $v(F)=dF_{p}(v)(Id)=D_{v}F(p)$ . Am I reading this correctly? I am trying to weed through the abstraction of differentials between manifolds and ground it into something more familiar, the directional derivative. Are directional derivatives in the theory of manifolds expressed as the the differential evaluated at the identity function, which are equivalent to simply evaluating $v(F)$ itself?","I am currently studying differential manifolds (from John M. Lee 's book), and have a question concerning the difference between what is defined as the , and the . Let , let , and suppose is a smooth map. Then, The differential of at is a map defined as, for some , is a derivation in defined as, for all , . Now, because and are Euclidean themselves, if , this can be expressed as . The directional derivative of at in direction is given by . Now, it seems that if is the identity function on , we have that for , . Am I reading this correctly? I am trying to weed through the abstraction of differentials between manifolds and ground it into something more familiar, the directional derivative. Are directional derivatives in the theory of manifolds expressed as the the differential evaluated at the identity function, which are equivalent to simply evaluating itself?","\textbf{differential of a function F} \textbf{directional derivative of a function F} M \subset \mathbb{R}^{m} N\subset \mathbb{R} F:M \rightarrow N F p \in M dF_{p}:T_{p}M \rightarrow T_{F(p)}N v \in T_{p}M dF_{p}(v) T_{F(p)}N f \in C^{\infty}(N) dF_{p}(v)(f)=v(f \circ F) M N v=(v_{1},...,v_{N}) dF_{p}(v)(f)=v_{1} \cdot \frac{\partial f}{\partial F}\ \frac{\partial F}{\partial x_{1}}+...+v_{m} \cdot \frac{\partial f}{\partial F}\ \frac{\partial F}{\partial x_{m}} F p v \in \mathbb{R}^{m} D_{v}F(p)=v_{1} \cdot \frac{\partial F(p)}{\partial x_{1}}+...+v_{m}\cdot \frac{\partial F(p)}{\partial x_{m}} Id:\mathbb{R}\rightarrow \mathbb{R} \mathbb{R} v \in T_{p}M v(F)=dF_{p}(v)(Id)=D_{v}F(p) v(F)","['real-analysis', 'general-topology', 'analysis', 'differential-geometry', 'differential-topology']"
93,Soft quesion: what do we lose if we assume measures are complete and $\sigma$-finite in integration theory?,Soft quesion: what do we lose if we assume measures are complete and -finite in integration theory?,\sigma,"This is intended to be a really soft question, which might be considered bad for this site. Let me know if that's true. I'll try to make this question as ""answerable"" as possible. I've been reading measure and integration theory on Analysis III by H. Amann & J. Escher (Chapter IX and X). This text is famous for its characteristic that everything is presented in the greatest generality . For example, he talks about convergence on Banach spaces right at the beginning. With this being said, it is not surprising that the text develops the theory of Bochner integrals first, even before Lebesgue integrals. However, what really annoys me is that, at the beginning of each section he says: In the following, let $(X,\mathcal{A},\mu)$ be a complete, $\sigma$ -finite measure space . This goes against the spirit of the book! Moreover, I'm utterly confused when the Wikipedia article on Bochner integrals does not assume completeness and $\sigma$ -finiteness, and yet it seems that many important results remains true without that assumption. Now my question is this. What do we lose if we assume all measures are complete and $\sigma$ -finite? I know that every measure has a completion, so that probably explains why we assume completeness. But what about $\sigma$ -finiteness? Is it just that non- $\sigma$ -finite just rarely exist? (This situation is much like topology: We often assume our topological spaces are Hausdorff without regret, because non-Hausdorff spaces are just too pathological.) To make this question more concrete, let me elaborate: Do we often encounter non- $\sigma$ -finite measures in higher analysis? I fear the theorems here are not general enough for later use. Is that true? Are there any other books that, like Amann, develops Bochner integrals in detail and presents results without assuming completeness and $\sigma$ -finiteness? I've looked into several standard real analysis texts so far, and they either treat it as an exercise (Folland), or in an appendix (Cohn), or omit it altogether. I also found the book Topics in Banach space integration by Schwabik, but unfortunately this book simply assumes $X$ to be a cube in $\mathbb{R}^n$ ... What significant results are not true or are much harder to prove if measures are not complete or $\sigma$ -compact? I already know Fubini's theorem is one example. (This might be too vague since it's impossible to list all such results. But maybe just name a few?) Thanks in advance!","This is intended to be a really soft question, which might be considered bad for this site. Let me know if that's true. I'll try to make this question as ""answerable"" as possible. I've been reading measure and integration theory on Analysis III by H. Amann & J. Escher (Chapter IX and X). This text is famous for its characteristic that everything is presented in the greatest generality . For example, he talks about convergence on Banach spaces right at the beginning. With this being said, it is not surprising that the text develops the theory of Bochner integrals first, even before Lebesgue integrals. However, what really annoys me is that, at the beginning of each section he says: In the following, let be a complete, -finite measure space . This goes against the spirit of the book! Moreover, I'm utterly confused when the Wikipedia article on Bochner integrals does not assume completeness and -finiteness, and yet it seems that many important results remains true without that assumption. Now my question is this. What do we lose if we assume all measures are complete and -finite? I know that every measure has a completion, so that probably explains why we assume completeness. But what about -finiteness? Is it just that non- -finite just rarely exist? (This situation is much like topology: We often assume our topological spaces are Hausdorff without regret, because non-Hausdorff spaces are just too pathological.) To make this question more concrete, let me elaborate: Do we often encounter non- -finite measures in higher analysis? I fear the theorems here are not general enough for later use. Is that true? Are there any other books that, like Amann, develops Bochner integrals in detail and presents results without assuming completeness and -finiteness? I've looked into several standard real analysis texts so far, and they either treat it as an exercise (Folland), or in an appendix (Cohn), or omit it altogether. I also found the book Topics in Banach space integration by Schwabik, but unfortunately this book simply assumes to be a cube in ... What significant results are not true or are much harder to prove if measures are not complete or -compact? I already know Fubini's theorem is one example. (This might be too vague since it's impossible to list all such results. But maybe just name a few?) Thanks in advance!","(X,\mathcal{A},\mu) \sigma \sigma \sigma \sigma \sigma \sigma \sigma X \mathbb{R}^n \sigma","['real-analysis', 'analysis', 'measure-theory', 'soft-question', 'banach-spaces']"
94,Proving Rudin 1.6d (Exponentiation rule for real numbers),Proving Rudin 1.6d (Exponentiation rule for real numbers),,"I have been working through some of the early problems in Baby Rudin to prepare for a class next year, but am stuck on part (d) of question 1.6. Fix $b > 1$. (a). If $m, n, p, q$ are integers, $n > 0, q > 0,$ and $r = \frac{m}{n} = \frac{p}{q}$, prove that:     $$(b^m)^{1/n} = (b^p)^{1/q}.$$     Hence it makes sense to define $b^r = (b^m)^{1/n}$. (b). Prove that $b^{r+s} = b^rb^s$ if $r$ and $s$ are rational. (c). If $x$ is real, define $B(x)$ to be the set of numbers $b^t$, where $t$ is rational and $t\leq x$. Prove that: $$b^r =  \sup B(r)$$     when $r$ is rational. Hence it makes sense to define:     $$b^x = \sup B(x)$$     for every real $x$. (d). Prove that $b^{x+y} = b^xb^y$ for all real $x$ and $y$. I have thus far been able to show parts a,b,c with relatively easy concepts, but am struggling to find a solution for part d. Below is my work for parts a,b,c. Feel free to look them over for mistakes. (a). Since $\frac{m}{n} = \frac{p}{q}$, we know: $mq = np = y$. Then, by $\textbf{Theorem 1.21 of the text}$, we know that $x^{nq} = b^y$ is unique. We shall demonstrate that $(b^m)^{1/n} = (b^p)^{1/q} = b^r$: $$((b^m)^{1/n})^{nq} = (b^m)^q = b^y$$ $$((b^p)^{1/q})^{nq} = (b^p)^n = b^y$$ Thus, $((b^m)^{1/n})^{nq} = b^y = ((b^p)^{1/q})^{nq}$, and so, $(b^m)^{1/n} = b^r = (b^p)^{1/q}$, as desired. (b). First, let $r = \frac{m}{n}, s = \frac{p}{q}$ for $m,n,p,q \in \Bbb{Z}$. Then, $b^{r+s} = b^{m/n +p/q} = (b^{mq+np})^{1/nq}.$ Since $mq, np \in \Bbb{Z}$, we can say, $(b^{mq+np})^{1/nq} = (b^{mq}b^{np})^{1/nq}$. We get: $(b^{mq}b^{np})^{1/nq} = (b^{m/n}b^{p/q}) = b^rb^s$, as desired. (c). We consider $B(r) = \{b^t \mid t \in \Bbb{Q} \ \& \ t \leq r\}$. For any $t$, $b^r = b^tb^{r-t} \geq b^t1^{r-t}$, since $b > 1$. Thus, $b^r$ is an upper bound of $B(r)$. Since $b^r \in B(r)$, we conclude that $b^r =\sup B(r)$, as desired. (d). For this part, I have considered doing $b^x = \sup B(x)$, so $B(x) = \{b^t \mid t \leq x, t \in \Bbb{Q}\}$. Then, $b^xb^y = \sup B(x)\sup B(y) \geq b^rb^s = b^{r+s} = \sup B(r+s)$, for $r \leq x, \ s\leq y, \ r,s \in \Bbb{Q}$. Thus, $\sup B(x)\sup B(y) \geq\sup B(r+s)$, and since $r+s \leq x+y$, we have $\sup B(x+y) \leq \sup B(x)\sup B(y)$, which would set $b^xb^y$ as an upper bound for $b^{x+y}$. Here I come across two issues, one in that I am not sure if this is in fact correct. Since we assumed $r,s$ were rational, I am not entirely sure if it is true that $\sup B(r+s) = \sup B(x+y)$ for $x,y \in \Bbb{R}$, as  I think would have to be the case for me to then claim that since $r+s \leq x+y$, $b^xb^y$ is an upper bound for $b^{x+y}$. Is this true, how would one prove this? My second issue is that, given the above is true, I can't figure out how one would proceed to demonstrate that $\sup B(x+y)$ is an upper bound for $b^xb^y$. Any help would be greatly appreciated!","I have been working through some of the early problems in Baby Rudin to prepare for a class next year, but am stuck on part (d) of question 1.6. Fix $b > 1$. (a). If $m, n, p, q$ are integers, $n > 0, q > 0,$ and $r = \frac{m}{n} = \frac{p}{q}$, prove that:     $$(b^m)^{1/n} = (b^p)^{1/q}.$$     Hence it makes sense to define $b^r = (b^m)^{1/n}$. (b). Prove that $b^{r+s} = b^rb^s$ if $r$ and $s$ are rational. (c). If $x$ is real, define $B(x)$ to be the set of numbers $b^t$, where $t$ is rational and $t\leq x$. Prove that: $$b^r =  \sup B(r)$$     when $r$ is rational. Hence it makes sense to define:     $$b^x = \sup B(x)$$     for every real $x$. (d). Prove that $b^{x+y} = b^xb^y$ for all real $x$ and $y$. I have thus far been able to show parts a,b,c with relatively easy concepts, but am struggling to find a solution for part d. Below is my work for parts a,b,c. Feel free to look them over for mistakes. (a). Since $\frac{m}{n} = \frac{p}{q}$, we know: $mq = np = y$. Then, by $\textbf{Theorem 1.21 of the text}$, we know that $x^{nq} = b^y$ is unique. We shall demonstrate that $(b^m)^{1/n} = (b^p)^{1/q} = b^r$: $$((b^m)^{1/n})^{nq} = (b^m)^q = b^y$$ $$((b^p)^{1/q})^{nq} = (b^p)^n = b^y$$ Thus, $((b^m)^{1/n})^{nq} = b^y = ((b^p)^{1/q})^{nq}$, and so, $(b^m)^{1/n} = b^r = (b^p)^{1/q}$, as desired. (b). First, let $r = \frac{m}{n}, s = \frac{p}{q}$ for $m,n,p,q \in \Bbb{Z}$. Then, $b^{r+s} = b^{m/n +p/q} = (b^{mq+np})^{1/nq}.$ Since $mq, np \in \Bbb{Z}$, we can say, $(b^{mq+np})^{1/nq} = (b^{mq}b^{np})^{1/nq}$. We get: $(b^{mq}b^{np})^{1/nq} = (b^{m/n}b^{p/q}) = b^rb^s$, as desired. (c). We consider $B(r) = \{b^t \mid t \in \Bbb{Q} \ \& \ t \leq r\}$. For any $t$, $b^r = b^tb^{r-t} \geq b^t1^{r-t}$, since $b > 1$. Thus, $b^r$ is an upper bound of $B(r)$. Since $b^r \in B(r)$, we conclude that $b^r =\sup B(r)$, as desired. (d). For this part, I have considered doing $b^x = \sup B(x)$, so $B(x) = \{b^t \mid t \leq x, t \in \Bbb{Q}\}$. Then, $b^xb^y = \sup B(x)\sup B(y) \geq b^rb^s = b^{r+s} = \sup B(r+s)$, for $r \leq x, \ s\leq y, \ r,s \in \Bbb{Q}$. Thus, $\sup B(x)\sup B(y) \geq\sup B(r+s)$, and since $r+s \leq x+y$, we have $\sup B(x+y) \leq \sup B(x)\sup B(y)$, which would set $b^xb^y$ as an upper bound for $b^{x+y}$. Here I come across two issues, one in that I am not sure if this is in fact correct. Since we assumed $r,s$ were rational, I am not entirely sure if it is true that $\sup B(r+s) = \sup B(x+y)$ for $x,y \in \Bbb{R}$, as  I think would have to be the case for me to then claim that since $r+s \leq x+y$, $b^xb^y$ is an upper bound for $b^{x+y}$. Is this true, how would one prove this? My second issue is that, given the above is true, I can't figure out how one would proceed to demonstrate that $\sup B(x+y)$ is an upper bound for $b^xb^y$. Any help would be greatly appreciated!",,['real-analysis']
95,"What is the standard topology of real line? Why is $(0,1)$ called open but $[0,1]$ not open on this topology?",What is the standard topology of real line? Why is  called open but  not open on this topology?,"(0,1) [0,1]","For a topological space $(X,\tau)$, the topology $\tau$ on the set $X$ is a family of subsets called open sets, if $X$, $\emptyset$, any union of the subsets, and any finite intersection of the subsets are in $\tau$. This could be a definition of the term ""open sets"" with respect to topology. On the real line, $X = \mathbb{R}$, we say $(0,1)$ is ""open"" based on the definition above, because $\tau_1 = \{\emptyset, \mathbb{R}, (0,1)\}$ can be a topology. However, we say $[0,1]$ is ""not open"", though $\tau_2 = \{\emptyset, \mathbb{R}, [0,1]\}$ can also be a topology that satisfies all of the properties. I think $[0,1]$ is not called open just because the $\tau_2$ topology is not so-called a standard topology of real line, correct? I remember the definition of open set for a metric space $(X,d)$ is, we say a subset $S \subset X$ is open if $\forall x \in S$, $\exists \epsilon > 0$ s.t. $B_{\epsilon}(x):=\{y \in X \;|\; d(x,y) < \epsilon\} \subset S$. Is this exactly the standard topology of real line I am asking for, if we collect all of these subsets of $\mathbb{R}$? In other words, we can define open sets in such a way, and collect all these subsets of $\mathbb{R}$ to make a topology, called standard topology. Am I right? One more question, it seems natural if we take the power set of $\mathbb{R}$, i.e., the discrete topology instead. Why is it not ""standard""?","For a topological space $(X,\tau)$, the topology $\tau$ on the set $X$ is a family of subsets called open sets, if $X$, $\emptyset$, any union of the subsets, and any finite intersection of the subsets are in $\tau$. This could be a definition of the term ""open sets"" with respect to topology. On the real line, $X = \mathbb{R}$, we say $(0,1)$ is ""open"" based on the definition above, because $\tau_1 = \{\emptyset, \mathbb{R}, (0,1)\}$ can be a topology. However, we say $[0,1]$ is ""not open"", though $\tau_2 = \{\emptyset, \mathbb{R}, [0,1]\}$ can also be a topology that satisfies all of the properties. I think $[0,1]$ is not called open just because the $\tau_2$ topology is not so-called a standard topology of real line, correct? I remember the definition of open set for a metric space $(X,d)$ is, we say a subset $S \subset X$ is open if $\forall x \in S$, $\exists \epsilon > 0$ s.t. $B_{\epsilon}(x):=\{y \in X \;|\; d(x,y) < \epsilon\} \subset S$. Is this exactly the standard topology of real line I am asking for, if we collect all of these subsets of $\mathbb{R}$? In other words, we can define open sets in such a way, and collect all these subsets of $\mathbb{R}$ to make a topology, called standard topology. Am I right? One more question, it seems natural if we take the power set of $\mathbb{R}$, i.e., the discrete topology instead. Why is it not ""standard""?",,"['real-analysis', 'general-topology']"
96,"Use Dominated Convergence Theorem to find limit of $\int n f(x) e^{-nx} \, dx$",Use Dominated Convergence Theorem to find limit of,"\int n f(x) e^{-nx} \, dx","I was working on an old practice qual and I came across this problem.  I have a solution, but it's rather...convoluted...and I feel like there should be a simple way of using the dominated convergence theorem to solve it.  Anyways, here's the problem: Let $f: [0,\infty) \rightarrow \mathbb{R}$ be Lebesgue integrable and suppose that $\lim_{x\rightarrow 0} f(x) = 2016$.  Show that $$\lim_{n\rightarrow \infty} \int_0^\infty nf(x)e^{-nx} \, dx = 2016$$ If you change variables, you get that the integral is equal to $\int_0^\infty f(\frac{x}{n})e^{-x}\, dx$, and so the claim is easy to prove if you know that $f$ is bounded.  In particular, if $f$ is continuous with compact support, the problem is solved.  However, despite the fact that we can prove the claim for a dense subset of $L^1$, it doesn't seem to follow for all of $L^1$ (as far as I can tell). I'll put my solution (without directly using the DCT) in the answers because it's pretty long.","I was working on an old practice qual and I came across this problem.  I have a solution, but it's rather...convoluted...and I feel like there should be a simple way of using the dominated convergence theorem to solve it.  Anyways, here's the problem: Let $f: [0,\infty) \rightarrow \mathbb{R}$ be Lebesgue integrable and suppose that $\lim_{x\rightarrow 0} f(x) = 2016$.  Show that $$\lim_{n\rightarrow \infty} \int_0^\infty nf(x)e^{-nx} \, dx = 2016$$ If you change variables, you get that the integral is equal to $\int_0^\infty f(\frac{x}{n})e^{-x}\, dx$, and so the claim is easy to prove if you know that $f$ is bounded.  In particular, if $f$ is continuous with compact support, the problem is solved.  However, despite the fact that we can prove the claim for a dense subset of $L^1$, it doesn't seem to follow for all of $L^1$ (as far as I can tell). I'll put my solution (without directly using the DCT) in the answers because it's pretty long.",,"['real-analysis', 'convergence-divergence', 'lebesgue-integral']"
97,Show that $\frac{3^n}{n!}$ converges to $0$,Show that  converges to,\frac{3^n}{n!} 0,"I was wondering if this proof is correct. $$\left|\frac{3^n}{n!} - 0\right| = \frac{3^n}{n!} \lt \frac{3^n}{2^n} \lt \varepsilon$$ So then $$\frac{2^n}{3^n} \gt \frac{1}{\varepsilon}$$ $$\left(\frac{2}{3}\right)^n \gt \frac{1}{\varepsilon}$$ $$\log\left(\frac{2^n}{3^n}\right) \gt \log\left(\frac{1}{\varepsilon}\right)$$ $$n\log\left(\frac{2}{3}\right) \gt \log\left(\frac{1}{\varepsilon}\right)$$ $$n \gt \log\left(\frac{1}{\varepsilon}\right)/\log\left(\frac23\right)$$ So then any $N \gt \log(\frac1\varepsilon)\log(\frac23)$ yields the result we want, so that for all $n \gt N, \left|\frac{3^n}{n!} - 0\right| \lt ε$ Thanks in advance!","I was wondering if this proof is correct. $$\left|\frac{3^n}{n!} - 0\right| = \frac{3^n}{n!} \lt \frac{3^n}{2^n} \lt \varepsilon$$ So then $$\frac{2^n}{3^n} \gt \frac{1}{\varepsilon}$$ $$\left(\frac{2}{3}\right)^n \gt \frac{1}{\varepsilon}$$ $$\log\left(\frac{2^n}{3^n}\right) \gt \log\left(\frac{1}{\varepsilon}\right)$$ $$n\log\left(\frac{2}{3}\right) \gt \log\left(\frac{1}{\varepsilon}\right)$$ $$n \gt \log\left(\frac{1}{\varepsilon}\right)/\log\left(\frac23\right)$$ So then any $N \gt \log(\frac1\varepsilon)\log(\frac23)$ yields the result we want, so that for all $n \gt N, \left|\frac{3^n}{n!} - 0\right| \lt ε$ Thanks in advance!",,"['real-analysis', 'proof-verification']"
98,Solution to one-dimensional Wave Equation with Method of Characteristics,Solution to one-dimensional Wave Equation with Method of Characteristics,,"Consider the Problem. Let $\gamma$ be a parameterized curve in $\mathbb{R}^{2}$ by $\gamma: I \to \Omega$, where $I$ is an interval of $\mathbb{R}$ and $\Omega$ an open in $\mathbb{R}^{2}$. Let $a,b,c: \Omega \to \mathbb{R}$ be given functions. Determine a function $\varphi(x,y)$ solution of the equation   $$a(x,y)\frac{\partial \varphi}{\partial x} + b(x,y)\frac{\partial \varphi}{\partial y} = c(x,y) \tag{Eq 1}$$   where $\varphi(\gamma(t)) = \varphi_{0}(t)$ with $\varphi_{0}: I \to \mathbb{R}$ is a given function. Proof. (just a loosely idea) Fixed a point $\gamma_{0} = \gamma_{0}(s_{0}) = \gamma_{0}(x_{0},y_{0})$ of $\gamma$, consider the curve $\Gamma(t) = (x(t),y(t))$ passing through $\gamma_{0}$, that is, $\Gamma(0) = \gamma_{0}$. Define $z(t) = \varphi(x(t),y(t))$ where $\varphi$ is a solution of Eq 1. If $\Gamma$ is differentiable, by the Chain Rule, $$\frac{dz}{dt} = \langle \Gamma'(t), \nabla\varphi(\Gamma(t)) \rangle = \frac{dx}{dt}\frac{d\varphi}{dx} + \frac{dy}{dt}\frac{d\varphi}{dy}.$$ Therefore, if $\Gamma$ satisfies the system of ODE $$\begin{cases} \frac{dx}{dt} = a(x,y),&x(0) = x_{0}\\ \frac{dy}{dt} = b(x,y),&y(0) = y_{0}, \end{cases}\tag{Sy 1}$$ we can the solution $\varphi$ solving $$\frac{dz}{dt} = c(x,y),\quad z(0)=\varphi(s_{0}).$$ If we repeat the previous argument for all points $\gamma(s)$, $s \in I$, we obtain a family of curves on which the solution $\varphi$ can be determined. The solutions of the Sy 1 define a change of variables, that is, a function $$f: \mathbb{R}^{2} \to \mathbb{R}^{2}$$ $$(t,s) \mapsto (x,y)$$ and the Inverse Function Theorem ensures a solution if $(a,b)$ is transversal to curve $\gamma$ where $(a,b)$ represents $(x,y) \mapsto (a(x,y),b(x,y))$. My objective is to apply the Method of Characteristics (the Problem ) to solve the one-dimensional linear Wave Equation that is given by $$\frac{\partial^{2} u}{\partial^{2} x} - c_{0}^{2}\frac{\partial^{2} u}{\partial^{2} y} = 0$$ and we write $$\frac{\partial^{2} u}{\partial^{2} x} - c_{0}^{2}\frac{\partial^{2} u}{\partial^{2} y} = \left(\frac{\partial }{\partial x} + c_{0}\frac{\partial }{\partial y}\right)\left(\frac{\partial }{\partial x} - c_{0}\frac{\partial }{\partial y}\right)u = 0.$$ Let $$\left(\frac{\partial }{\partial x} - c_{0}\frac{\partial }{\partial y}\right)u = v(x,y) = v.$$ Then it's enough to solve $$\underbrace{\frac{\partial u}{\partial x} - c_{0}\frac{\partial u}{\partial y} = v}_{(1)}\quad\text{and}\quad\underbrace{\frac{\partial v}{\partial x} + c_{0}\frac{\partial v}{\partial y} = 0}_{(2)}$$ I broke the Wave Equation in (1) and (2) because the Problem is only for order $1$, since the partial  derivatives are of order $1$. Then I think that solving the two equations, I'll find a solution for Wave Equation. This is the way that I found to use the Problem . Now, I want to use the Problem for solve (2) and so, use the solution for (2) to solve (1). I couldn't apply the above problem to get a solution to the equations (1) and (2). Hans Lundmark gave me a good reference, but in it, the author does the construction with some different details and my professor wants me to use the problem, exactly as it is, to get the solution. I wish someone could help me with this.","Consider the Problem. Let $\gamma$ be a parameterized curve in $\mathbb{R}^{2}$ by $\gamma: I \to \Omega$, where $I$ is an interval of $\mathbb{R}$ and $\Omega$ an open in $\mathbb{R}^{2}$. Let $a,b,c: \Omega \to \mathbb{R}$ be given functions. Determine a function $\varphi(x,y)$ solution of the equation   $$a(x,y)\frac{\partial \varphi}{\partial x} + b(x,y)\frac{\partial \varphi}{\partial y} = c(x,y) \tag{Eq 1}$$   where $\varphi(\gamma(t)) = \varphi_{0}(t)$ with $\varphi_{0}: I \to \mathbb{R}$ is a given function. Proof. (just a loosely idea) Fixed a point $\gamma_{0} = \gamma_{0}(s_{0}) = \gamma_{0}(x_{0},y_{0})$ of $\gamma$, consider the curve $\Gamma(t) = (x(t),y(t))$ passing through $\gamma_{0}$, that is, $\Gamma(0) = \gamma_{0}$. Define $z(t) = \varphi(x(t),y(t))$ where $\varphi$ is a solution of Eq 1. If $\Gamma$ is differentiable, by the Chain Rule, $$\frac{dz}{dt} = \langle \Gamma'(t), \nabla\varphi(\Gamma(t)) \rangle = \frac{dx}{dt}\frac{d\varphi}{dx} + \frac{dy}{dt}\frac{d\varphi}{dy}.$$ Therefore, if $\Gamma$ satisfies the system of ODE $$\begin{cases} \frac{dx}{dt} = a(x,y),&x(0) = x_{0}\\ \frac{dy}{dt} = b(x,y),&y(0) = y_{0}, \end{cases}\tag{Sy 1}$$ we can the solution $\varphi$ solving $$\frac{dz}{dt} = c(x,y),\quad z(0)=\varphi(s_{0}).$$ If we repeat the previous argument for all points $\gamma(s)$, $s \in I$, we obtain a family of curves on which the solution $\varphi$ can be determined. The solutions of the Sy 1 define a change of variables, that is, a function $$f: \mathbb{R}^{2} \to \mathbb{R}^{2}$$ $$(t,s) \mapsto (x,y)$$ and the Inverse Function Theorem ensures a solution if $(a,b)$ is transversal to curve $\gamma$ where $(a,b)$ represents $(x,y) \mapsto (a(x,y),b(x,y))$. My objective is to apply the Method of Characteristics (the Problem ) to solve the one-dimensional linear Wave Equation that is given by $$\frac{\partial^{2} u}{\partial^{2} x} - c_{0}^{2}\frac{\partial^{2} u}{\partial^{2} y} = 0$$ and we write $$\frac{\partial^{2} u}{\partial^{2} x} - c_{0}^{2}\frac{\partial^{2} u}{\partial^{2} y} = \left(\frac{\partial }{\partial x} + c_{0}\frac{\partial }{\partial y}\right)\left(\frac{\partial }{\partial x} - c_{0}\frac{\partial }{\partial y}\right)u = 0.$$ Let $$\left(\frac{\partial }{\partial x} - c_{0}\frac{\partial }{\partial y}\right)u = v(x,y) = v.$$ Then it's enough to solve $$\underbrace{\frac{\partial u}{\partial x} - c_{0}\frac{\partial u}{\partial y} = v}_{(1)}\quad\text{and}\quad\underbrace{\frac{\partial v}{\partial x} + c_{0}\frac{\partial v}{\partial y} = 0}_{(2)}$$ I broke the Wave Equation in (1) and (2) because the Problem is only for order $1$, since the partial  derivatives are of order $1$. Then I think that solving the two equations, I'll find a solution for Wave Equation. This is the way that I found to use the Problem . Now, I want to use the Problem for solve (2) and so, use the solution for (2) to solve (1). I couldn't apply the above problem to get a solution to the equations (1) and (2). Hans Lundmark gave me a good reference, but in it, the author does the construction with some different details and my professor wants me to use the problem, exactly as it is, to get the solution. I wish someone could help me with this.",,"['real-analysis', 'partial-differential-equations', 'wave-equation']"
99,Is this definite integral positive?,Is this definite integral positive?,,"I have a doubt and I am not able to prove (or disprove): Let $f(x)$ be an odd function with $f(x)>0\,\,\,\forall x\in (0,+\infty)$ . Let $g(x)$ be a non-negative function: $g(x)\geq 0\;\forall x\in \mathbb{R}.$ Also suppose $\displaystyle \int_{-\infty}^0g(x)\,dx<\int_{0}^{\infty}g(x)\,dx.$ I wonder if one can assure that: $$\int_{-\infty}^{\infty}f(x)\,g(x)\,dx>0.$$ EDIT 1 : Has been  proved (by Adrian Keister) that my thesis is false. Now I wonder again if is possible add another hypothesis about $g(x)$ to assure my thesis. EDIT 2 :The problem arrives from here: blue line is $f(x)= \left(e^{-\frac{\cosh ^2(u-1)}{2 }}-e^{-\frac{\cosh ^2(1+u)}{2 }}\right)$ and orange line is $g(x)=e^{-\frac{u^2}{2 }}\cos ^2\left(\frac{\pi  (u-1)}{4 }\right)$ and the function $f(x)g(x)$ graphic As we can see in the graph, the integral $\int_{\mathbb{R}}f(x)g(x)\,dx$ seems to be positive. We can translate the factor $e^{-u^2/2}$ from $g(x)$ to $f(x)$ (in this case the third hypothesis is not fulfilled):","I have a doubt and I am not able to prove (or disprove): Let be an odd function with . Let be a non-negative function: Also suppose I wonder if one can assure that: EDIT 1 : Has been  proved (by Adrian Keister) that my thesis is false. Now I wonder again if is possible add another hypothesis about to assure my thesis. EDIT 2 :The problem arrives from here: blue line is and orange line is and the function graphic As we can see in the graph, the integral seems to be positive. We can translate the factor from to (in this case the third hypothesis is not fulfilled):","f(x) f(x)>0\,\,\,\forall x\in (0,+\infty) g(x) g(x)\geq 0\;\forall x\in \mathbb{R}. \displaystyle \int_{-\infty}^0g(x)\,dx<\int_{0}^{\infty}g(x)\,dx. \int_{-\infty}^{\infty}f(x)\,g(x)\,dx>0. g(x) f(x)= \left(e^{-\frac{\cosh ^2(u-1)}{2 }}-e^{-\frac{\cosh ^2(1+u)}{2 }}\right) g(x)=e^{-\frac{u^2}{2 }}\cos ^2\left(\frac{\pi  (u-1)}{4 }\right) f(x)g(x) \int_{\mathbb{R}}f(x)g(x)\,dx e^{-u^2/2} g(x) f(x)","['real-analysis', 'definite-integrals']"
