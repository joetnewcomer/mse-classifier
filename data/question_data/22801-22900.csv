,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,eigenvalues of a first order differential operator on a manifold,eigenvalues of a first order differential operator on a manifold,,"I have the following, maybe naive question: Given a smooth vector field $X$ on a smooth manifold $M$ with $\dim M \geq 1$ . Then this defines a linear operator $$  D: C^\infty(M,\mathbb C) \to C^\infty(M,\mathbb C) ; f \mapsto X.f\,. $$ What I would like to know is: What is known about the eigenvalues of this linear map $D$ ? It is clear to me that constant maps are always mapped to $0$ , so $0$ is always an eigenvalue. It is also clear that if $X=0$ , then $D=0$ and there is no other eigenvalue. But assume, $X$ is not the constant $0$ -vector field. Does this imply that $D$ has a complex non-zero eigenvalue ? Does this depend on whether $M$ is compact or not? EDIT: It looks to me as if $X$ being not constantly zero is not enough to conclude the existence of a non-zero eigenvalue. Motivated by the comment of Ben Grossmann I looked at the real line $\mathbb R$ and the circle $\mathbb R/\mathbb Z$ and it seems that $X(x)=\cos^2(x)$ should be an example of a vector field where $D$ has only the eigenvalue $0$ -- if my calculus was correct. So, maybe the correct question is: Assume that $X$ is everywhere non-zero, does this imply that we have a non-zero eigenvalue ? The comment of Ben Grossmann answeres this in the affirmative for $M=\mathbb R$ and I think a very similar argument should work for the circle $M=\mathbb R/\mathbb Z$ . The comment of Eric solves this in the affirmative locally but it is not clear how the global geomety of $M$ comes into play here.","I have the following, maybe naive question: Given a smooth vector field on a smooth manifold with . Then this defines a linear operator What I would like to know is: What is known about the eigenvalues of this linear map ? It is clear to me that constant maps are always mapped to , so is always an eigenvalue. It is also clear that if , then and there is no other eigenvalue. But assume, is not the constant -vector field. Does this imply that has a complex non-zero eigenvalue ? Does this depend on whether is compact or not? EDIT: It looks to me as if being not constantly zero is not enough to conclude the existence of a non-zero eigenvalue. Motivated by the comment of Ben Grossmann I looked at the real line and the circle and it seems that should be an example of a vector field where has only the eigenvalue -- if my calculus was correct. So, maybe the correct question is: Assume that is everywhere non-zero, does this imply that we have a non-zero eigenvalue ? The comment of Ben Grossmann answeres this in the affirmative for and I think a very similar argument should work for the circle . The comment of Eric solves this in the affirmative locally but it is not clear how the global geomety of comes into play here.","X M \dim M \geq 1 
 D: C^\infty(M,\mathbb C) \to C^\infty(M,\mathbb C) ; f \mapsto X.f\,.
 D 0 0 X=0 D=0 X 0 D M X \mathbb R \mathbb R/\mathbb Z X(x)=\cos^2(x) D 0 X M=\mathbb R M=\mathbb R/\mathbb Z M","['linear-algebra', 'functional-analysis', 'differential-geometry', 'smooth-manifolds', 'vector-fields']"
1,Number of matrices over finite fields $\mathbb{F}_q$ with some minors to be $0$ is (or is not?) polynomial in $q$?,Number of matrices over finite fields  with some minors to be  is (or is not?) polynomial in ?,\mathbb{F}_q 0 q,"It's well-known that $$\# \operatorname{GL}_2(\mathbb{F}_q)=(q^2-1)(q^2-q)$$ is a ( $\mathbb{Z}$ -coefficient) polynomial of $q$ . As a corollary, $$\#\left\{ A \in M_{2\times 2}(\mathbb{F}_q)\, \middle|\,\det A =0 \right\}=q^4-(q^2-1)(q^2-q)$$ is a polynomial of $q$ . In general, fix $n \in \mathbb{N}_{>0}$ , $\Lambda$ denotes for the subset of $$X:=\left\{ (k,I,J)\, \middle|\, \begin{aligned}   	&I:1 \leqslant i_1 < \cdots <i_k \leqslant m\\   	&J:1 \leqslant j_1 < \cdots <j_k \leqslant m   	\end{aligned}  \right\},$$ $A_{I,J}$ denote for the corresponding minor, is $$\#\left\{ A \in M_{n\times n}(\mathbb{F}_q)\, \middle|\,A_{I,J} =0, \;\forall\, (k,I,J) \in \Lambda  \right\}$$ always a polynomial of $q$ ? A possible nontrivial case to work with: $$\#\left\{ A \in M_{3\times 3}(\mathbb{F}_q)\, \middle|\,\begin{vmatrix} a_{11} & a_{12}  \\ a_{21} & a_{22}  \end{vmatrix} =\begin{vmatrix} a_{22} & a_{23} \\ a_{32} & a_{33}  \end{vmatrix} =\begin{vmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\a_{31} & a_{32} & a_{33}  \end{vmatrix}=0 \right\}.$$ When the answer is true, maybe the proof can be generalized to show that $$\#\left\{ A \in M_{n\times n}(\mathbb{F}_q)\, \middle|\,\begin{aligned}   	&A_{I_1,J_1} =0, \;\forall\, (k,I_1,J_1) \in \Lambda_1\\   	&A_{I_2,J_2} =1, \;\forall\, (k,I_2,J_2) \in \Lambda_2\\     &A_{I_3,J_3} \neq 0, \;\forall\, (k,I_3,J_3) \in \Lambda_3   	\end{aligned}  \right\} \qquad (\ast)$$ is a polynomial of $q$ , where $\Lambda_1, \Lambda_2,\Lambda_3$ are disjoint subsets of $X$ . When the answer is false, the counterexample should be not too complicated. Anyhow, this can be viewed as a purely combinational problem. Related question: Number of $3\times 3$ anticommuting matrices over finite fields $\mathbb{F}_p$ is (or is not?) polynomial in $p$ ? Edit: at least for the generalization $(\ast)$ , there is a counterexample. The number \begin{equation} \begin{aligned} &\#\left\{ \begin{pmatrix} a & 0 & 0 \\ 0 & b & 0 \\ 0 & 0 & c  \end{pmatrix} \in M_{3\times 3}(\mathbb{F}_q)\, \middle|\,\begin{vmatrix} a & 0 \\ 0 & b  \end{vmatrix} =\begin{vmatrix} b & 0 \\ 0 & c  \end{vmatrix} =\begin{vmatrix} a & 0 \\ 0 & c  \end{vmatrix}=1 \right\}\\ =\;& \#\left\{ a \in \mathbb{F}_q^{\times}\, \middle|\,a^2=1 \right\} \end{aligned} \end{equation} is not a polynomial of $q$ .","It's well-known that is a ( -coefficient) polynomial of . As a corollary, is a polynomial of . In general, fix , denotes for the subset of denote for the corresponding minor, is always a polynomial of ? A possible nontrivial case to work with: When the answer is true, maybe the proof can be generalized to show that is a polynomial of , where are disjoint subsets of . When the answer is false, the counterexample should be not too complicated. Anyhow, this can be viewed as a purely combinational problem. Related question: Number of anticommuting matrices over finite fields is (or is not?) polynomial in ? Edit: at least for the generalization , there is a counterexample. The number is not a polynomial of .","\# \operatorname{GL}_2(\mathbb{F}_q)=(q^2-1)(q^2-q) \mathbb{Z} q \#\left\{ A \in M_{2\times 2}(\mathbb{F}_q)\, \middle|\,\det A =0 \right\}=q^4-(q^2-1)(q^2-q) q n \in \mathbb{N}_{>0} \Lambda X:=\left\{ (k,I,J)\, \middle|\, \begin{aligned}
  	&I:1 \leqslant i_1 < \cdots <i_k \leqslant m\\
  	&J:1 \leqslant j_1 < \cdots <j_k \leqslant m
  	\end{aligned}  \right\}, A_{I,J} \#\left\{ A \in M_{n\times n}(\mathbb{F}_q)\, \middle|\,A_{I,J} =0, \;\forall\, (k,I,J) \in \Lambda  \right\} q \#\left\{ A \in M_{3\times 3}(\mathbb{F}_q)\, \middle|\,\begin{vmatrix} a_{11} & a_{12}  \\ a_{21} & a_{22}  \end{vmatrix} =\begin{vmatrix} a_{22} & a_{23} \\ a_{32} & a_{33}  \end{vmatrix} =\begin{vmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\a_{31} & a_{32} & a_{33}  \end{vmatrix}=0 \right\}. \#\left\{ A \in M_{n\times n}(\mathbb{F}_q)\, \middle|\,\begin{aligned}
  	&A_{I_1,J_1} =0, \;\forall\, (k,I_1,J_1) \in \Lambda_1\\
  	&A_{I_2,J_2} =1, \;\forall\, (k,I_2,J_2) \in \Lambda_2\\
    &A_{I_3,J_3} \neq 0, \;\forall\, (k,I_3,J_3) \in \Lambda_3
  	\end{aligned}  \right\} \qquad (\ast) q \Lambda_1, \Lambda_2,\Lambda_3 X 3\times 3 \mathbb{F}_p p (\ast) \begin{equation}
\begin{aligned}
&\#\left\{ \begin{pmatrix} a & 0 & 0 \\ 0 & b & 0 \\ 0 & 0 & c  \end{pmatrix} \in M_{3\times 3}(\mathbb{F}_q)\, \middle|\,\begin{vmatrix} a & 0 \\ 0 & b  \end{vmatrix} =\begin{vmatrix} b & 0 \\ 0 & c  \end{vmatrix} =\begin{vmatrix} a & 0 \\ 0 & c  \end{vmatrix}=1 \right\}\\
=\;& \#\left\{ a \in \mathbb{F}_q^{\times}\, \middle|\,a^2=1 \right\}
\end{aligned}
\end{equation} q","['linear-algebra', 'combinatorics', 'algebraic-geometry', 'polynomials']"
2,Suppose $T$ is an operator on $V$ and $T^2 = I$ and $-1$ is not an eigenvalue of $T$. Prove that $T = I$.,Suppose  is an operator on  and  and  is not an eigenvalue of . Prove that .,T V T^2 = I -1 T T = I,"My attempt: Since $ -1$ is not an eigenvalue of $T\implies T+I$ is invertible, that is $(T+I)^{-1}$ exists. Now $(T+I)(T-I) = T^2 - I \implies (T+I)(T-I) = 0$ . After applying $(T+I)^{-1 }$ on left and right hand side of the previous equation we get: $ (T+I)^{-1} (T+I) (T-I) = (T+I)^{-1}(0) \implies T-I = 0 \implies T = I$ .","My attempt: Since is not an eigenvalue of is invertible, that is exists. Now . After applying on left and right hand side of the previous equation we get: .", -1 T\implies T+I (T+I)^{-1} (T+I)(T-I) = T^2 - I \implies (T+I)(T-I) = 0 (T+I)^{-1 }  (T+I)^{-1} (T+I) (T-I) = (T+I)^{-1}(0) \implies T-I = 0 \implies T = I,['linear-algebra']
3,$G$ is a compact group. Are 'periodic' functions dense in $C(G)$?,is a compact group. Are 'periodic' functions dense in ?,G C(G),"Assume $G$ is a group, $f$ is a continous real number valued function on $G$ . The $G$ -action on $f$ is given by $(gf)(a)=f(g^{-1}a)$ . Say $f$ is periodic, when $Gf$ is contained in some finite dimensional (real) linear space. For example, let $G=\mathbb T$ , the circle group, then functions like $\sin(n\theta)$ and $\cos(n\theta)$ are periodic, since $(\zeta\sin)(n\theta)=\sin(n(\theta-\zeta))=\sin(n\theta)\cos(n\zeta)-\cos(n\theta)\sin(n\zeta)$ , which is always contained in the linear space generated by $\sin(n\theta)$ and $\cos(n\theta)$ . Therefore, by Fourier series, periodic functions in $C(\mathbb T)$ are dense. The question is, assume $G$ is compact, is the set of periodic functions always dense in $C(G)$ in general?","Assume is a group, is a continous real number valued function on . The -action on is given by . Say is periodic, when is contained in some finite dimensional (real) linear space. For example, let , the circle group, then functions like and are periodic, since , which is always contained in the linear space generated by and . Therefore, by Fourier series, periodic functions in are dense. The question is, assume is compact, is the set of periodic functions always dense in in general?",G f G G f (gf)(a)=f(g^{-1}a) f Gf G=\mathbb T \sin(n\theta) \cos(n\theta) (\zeta\sin)(n\theta)=\sin(n(\theta-\zeta))=\sin(n\theta)\cos(n\zeta)-\cos(n\theta)\sin(n\zeta) \sin(n\theta) \cos(n\theta) C(\mathbb T) G C(G),"['linear-algebra', 'functional-analysis', 'group-theory']"
4,Inequality involving matrix inverse elements,Inequality involving matrix inverse elements,,"Let $A$ be an $N \times N$ matrix with all nonnegative entries and row sums strictly less than one, let $v$ be an $N$ dimensional vector with all nonnegative entries and weakly lower than one, let $B\equiv\left(I-A\mathrm{diag}\left(v\right)\right)^{-1}$ and let $B^*\equiv\left(I-A\right)^{-1}$ , where $\mathrm{diag}\left(v\right)$ is the diagonal matrix formed from vector $v$ . I want to show that for any $i,j=1,...,N$ the following inequality holds: $$ v_{j}b_{ji}^2+\sum_{k}\left(1-v_{k}\right) b_{jk}b_{ki}^{2}\leq b_{ji} b_{ii}^{*}. $$ Simulations suggest that this is true. The case in which $v$ is the vector of all ones follows from the fact that $b_{ji} \leq b_{ii}^{*}$ , which is shown here . The case with $A$ diagonal is trivial for $i \neq j$ , whereas for $i=j$ it boils down to showing $$v_{i}b_{ii}+(1-v_{i})b_{ii}^{2}\leq b_{ii}^{*},$$ which can be shown by plugging in for $$b_{ii}=\frac{1}{1-v_{i}a_{ii}},\quad b_{ii}^{*}=\frac{1}{1-a_{ii}},$$ and basic algebra. Apart from these simple cases, I have been able to show the result for the case in which $j=i$ , but it is an arduous induction proof that does not extend to the case in which $i \neq j$ . We would appreciate hints for approaches that could be useful to prove the claim. The problem above comes from a more general problem in matrix algebra, which is to show that $ii$ of the following matrix is less than $b_{ii}^*$ , $$J \equiv \left(\mathrm{diag}\left\{ B^{T}x\right\} \right)^{-1}B^{T}\left[ \mathrm{diag}\left\{ v\right\}\mathrm{diag}\left\{ x\right\} +\mathrm{diag}\left\{ 1-v\right\} \mathrm{diag}\left\{ B^{T}x\right\} \right]B,$$ with $x$ being an $N$ dimensional vector in the simplex, i.e., $x_j \geq 0,\sum_j x_j=1$ . It can be shown that the diagonal elements will be maximized with respect to $x$ when $x$ is at a corner of the simplex, and that if $x_j = 1$ then $$ J_{ii} = v_{j}b_{ji}+\sum_{k}\left(1-v_{k}\right) \frac{b_{jk}b_{ki}^{2}}{b_{ji}}\leq b_{ii}^{*}. $$ Multiplying by $b_{ji}$ on both sides leads to the inequality postulated above. A closely related problem is to show that the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is lower than one (where $\iota$ is the vector of all ones), see here . (Note that if $v=\iota$ then $J\mathrm{diag}(\iota-A \iota)\iota = J(I-A)\iota = \iota$ and so the spectral radius of $J\mathrm{diag}(\iota-A \iota)$ is one.)","Let be an matrix with all nonnegative entries and row sums strictly less than one, let be an dimensional vector with all nonnegative entries and weakly lower than one, let and let , where is the diagonal matrix formed from vector . I want to show that for any the following inequality holds: Simulations suggest that this is true. The case in which is the vector of all ones follows from the fact that , which is shown here . The case with diagonal is trivial for , whereas for it boils down to showing which can be shown by plugging in for and basic algebra. Apart from these simple cases, I have been able to show the result for the case in which , but it is an arduous induction proof that does not extend to the case in which . We would appreciate hints for approaches that could be useful to prove the claim. The problem above comes from a more general problem in matrix algebra, which is to show that of the following matrix is less than , with being an dimensional vector in the simplex, i.e., . It can be shown that the diagonal elements will be maximized with respect to when is at a corner of the simplex, and that if then Multiplying by on both sides leads to the inequality postulated above. A closely related problem is to show that the spectral radius of is lower than one (where is the vector of all ones), see here . (Note that if then and so the spectral radius of is one.)","A N \times N v N B\equiv\left(I-A\mathrm{diag}\left(v\right)\right)^{-1} B^*\equiv\left(I-A\right)^{-1} \mathrm{diag}\left(v\right) v i,j=1,...,N 
v_{j}b_{ji}^2+\sum_{k}\left(1-v_{k}\right) b_{jk}b_{ki}^{2}\leq b_{ji} b_{ii}^{*}.
 v b_{ji} \leq b_{ii}^{*} A i \neq j i=j v_{i}b_{ii}+(1-v_{i})b_{ii}^{2}\leq b_{ii}^{*}, b_{ii}=\frac{1}{1-v_{i}a_{ii}},\quad b_{ii}^{*}=\frac{1}{1-a_{ii}}, j=i i \neq j ii b_{ii}^* J \equiv \left(\mathrm{diag}\left\{ B^{T}x\right\} \right)^{-1}B^{T}\left[ \mathrm{diag}\left\{ v\right\}\mathrm{diag}\left\{ x\right\} +\mathrm{diag}\left\{ 1-v\right\} \mathrm{diag}\left\{ B^{T}x\right\} \right]B, x N x_j \geq 0,\sum_j x_j=1 x x x_j = 1 
J_{ii} = v_{j}b_{ji}+\sum_{k}\left(1-v_{k}\right) \frac{b_{jk}b_{ki}^{2}}{b_{ji}}\leq b_{ii}^{*}.
 b_{ji} J\mathrm{diag}(\iota-A \iota) \iota v=\iota J\mathrm{diag}(\iota-A \iota)\iota = J(I-A)\iota = \iota J\mathrm{diag}(\iota-A \iota)","['linear-algebra', 'matrices', 'inequality']"
5,Rank of a matrix over a ring?,Rank of a matrix over a ring?,,"In his book Module Theory (1977), Blyth defines the column rank of an $m\times n$ matrix $A$ over a commutative unitary ring $R$ to be the dimension of the subspace of $\mathrm{Mat}_{m\times 1}(R)$ generated by the column...matrices of $A$ and dually for row rank (p. 153). However, he only defines dimension for free modules over commutative unitary rings (p. 105), so I don't think this definition will work as stated in general (even after changing ""subspace"" to ""submodule"") because a submodule of even a finitely generated free module over a commutative unitary ring need not be free. I'm trying to determine how to interpret an exercise which asks to show that the matrix $$A=\begin{bmatrix}1&2&3\\0&3&2\end{bmatrix}$$ over the ring $\mathbb{Z}/30\mathbb{Z}$ has a row rank of 2 and a column rank of 1 (p. 170). Should I just interpret this to be referring to the maximal numbers of linearly independent rows/columns? Note in the latest edition of his book (2018), it seems he has stated the definitions of rank for matrices over arbitrary unitary rings $R$ (p. 111), and generalized the definition of dimension to include also free modules over division rings (p. 78). The same exercise still appears (p. 121).","In his book Module Theory (1977), Blyth defines the column rank of an matrix over a commutative unitary ring to be the dimension of the subspace of generated by the column...matrices of and dually for row rank (p. 153). However, he only defines dimension for free modules over commutative unitary rings (p. 105), so I don't think this definition will work as stated in general (even after changing ""subspace"" to ""submodule"") because a submodule of even a finitely generated free module over a commutative unitary ring need not be free. I'm trying to determine how to interpret an exercise which asks to show that the matrix over the ring has a row rank of 2 and a column rank of 1 (p. 170). Should I just interpret this to be referring to the maximal numbers of linearly independent rows/columns? Note in the latest edition of his book (2018), it seems he has stated the definitions of rank for matrices over arbitrary unitary rings (p. 111), and generalized the definition of dimension to include also free modules over division rings (p. 78). The same exercise still appears (p. 121).",m\times n A R \mathrm{Mat}_{m\times 1}(R) A A=\begin{bmatrix}1&2&3\\0&3&2\end{bmatrix} \mathbb{Z}/30\mathbb{Z} R,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules', 'matrix-rank']"
6,Does this orthogonal matrix parametrization have a name?,Does this orthogonal matrix parametrization have a name?,,"Given two linearly independent unit vectors $u, v \in \mathbb{R}^{n,1}$ , the matrix $$ P(u, v) = I + 2 vu^{T}-\frac{(u+v)(u+v)^{T}}{1+u^Tv} $$ is the unique special orthogonal matrix that brings $u$ to coincide with $v$ and reduces to the identity when projected to the orthogonal complement of $\mathrm{span}(u,v)$ . That is: $$P^T(u,v) P(u,v) = I$$ $$P(u,v)u = v$$ $$P(u,v)w = w\quad \forall w \in \mathbb{R}^{n,1}\ /\ u^Tw = 0 \wedge v^Tw = 0$$ (Note that for $n > 3$ not all special orthogonal matrices can be expressed in this way.) I've never encountered this parametrization before today. Does it have a name?","Given two linearly independent unit vectors , the matrix is the unique special orthogonal matrix that brings to coincide with and reduces to the identity when projected to the orthogonal complement of . That is: (Note that for not all special orthogonal matrices can be expressed in this way.) I've never encountered this parametrization before today. Does it have a name?","u, v \in \mathbb{R}^{n,1} 
P(u, v) = I + 2 vu^{T}-\frac{(u+v)(u+v)^{T}}{1+u^Tv}
 u v \mathrm{span}(u,v) P^T(u,v) P(u,v) = I P(u,v)u = v P(u,v)w = w\quad \forall w \in \mathbb{R}^{n,1}\ /\ u^Tw = 0 \wedge v^Tw = 0 n > 3","['linear-algebra', 'matrices', 'reference-request', 'terminology', 'orthogonal-matrices']"
7,Reference for a bilinear form lemma,Reference for a bilinear form lemma,,"I encountered the result below in a paper of Claude Viterbo ( Intersection de sous-variétés lagrangiennes, fonctionnelles d’action et indice des systèmes hamiltoniens , p. 379) that I was reading, and it does not have a reference. If anyone could provide me a reference, it would be very helpful. Lemma: Let $Q^t$ be a $C^1$ family of bilinear forms defined in a Hilbert space. Let $Q^t$ be nondegenerate for $t\neq 0$ and $Q^t = U_t + C_t$ , where $U_t$ is positive definite with continuous inverse and $C_t$ is compact. If $\left.\frac{d}{dt}Q^t\right|_{t=0}$ when restricted to $\ker(Q^0)$ has signature $\sigma$ and nullity $\mu$ we have \begin{equation} \sigma - \mu \leq index(Q^{-1}) - index(Q^{+1}) \leq \sigma + \mu. \end{equation} In fact, I am trying to understand the proof of proposition 8. Thank you very much!","I encountered the result below in a paper of Claude Viterbo ( Intersection de sous-variétés lagrangiennes, fonctionnelles d’action et indice des systèmes hamiltoniens , p. 379) that I was reading, and it does not have a reference. If anyone could provide me a reference, it would be very helpful. Lemma: Let be a family of bilinear forms defined in a Hilbert space. Let be nondegenerate for and , where is positive definite with continuous inverse and is compact. If when restricted to has signature and nullity we have In fact, I am trying to understand the proof of proposition 8. Thank you very much!","Q^t C^1 Q^t t\neq 0 Q^t = U_t + C_t U_t C_t \left.\frac{d}{dt}Q^t\right|_{t=0} \ker(Q^0) \sigma \mu \begin{equation}
\sigma - \mu \leq index(Q^{-1}) - index(Q^{+1}) \leq \sigma + \mu.
\end{equation}","['linear-algebra', 'functional-analysis', 'reference-request', 'bilinear-form']"
8,What is the dimension of the span over $\mathbb{F}$ of permutation matrices on $n$ elements if $char(\mathbb{F})$ divides $n$?,What is the dimension of the span over  of permutation matrices on  elements if  divides ?,\mathbb{F} n char(\mathbb{F}) n,"I'm trying to generalize Birkhoff's theorem and prove that over any field $\mathbb{F}$ any matrix whose sum of rows and columns is equal to some constant, is a linear combination of permutation matrices. For fields where the characteristic $p$ does not divide $n$ , we can use a dimensional argument: compare the dimension over $\mathbb{F}$ of the span of all permutation matrices on $n$ points with the algebra $Mat_{n-1}(\mathbb{F})\oplus \mathbb{F}$ . These algebras are isomorphic as it is sufficient for $p$ to not be a factor of $n$ so that it the representation decomposes into the direct sum of the trivial and the permutation representation. This does not work otherwise. Any hints?","I'm trying to generalize Birkhoff's theorem and prove that over any field any matrix whose sum of rows and columns is equal to some constant, is a linear combination of permutation matrices. For fields where the characteristic does not divide , we can use a dimensional argument: compare the dimension over of the span of all permutation matrices on points with the algebra . These algebras are isomorphic as it is sufficient for to not be a factor of so that it the representation decomposes into the direct sum of the trivial and the permutation representation. This does not work otherwise. Any hints?",\mathbb{F} p n \mathbb{F} n Mat_{n-1}(\mathbb{F})\oplus \mathbb{F} p n,"['linear-algebra', 'combinatorics', 'matrices', 'representation-theory', 'permutation-matrices']"
9,Rank of Products of Matrices,Rank of Products of Matrices,,"This is somewhat of a reference request. In several posts on the rank of products of matrices (e.g. Full-rank condition for product of two matrices ), it is stated that $$ \mathrm{rank}(AB) = \mathrm{rank}(B) - \dim \big(\mathrm{N}(A) \cap \mathrm{R}(B)\big)$$ It appears that this is a classic result, though I am not familiar with it. If anyone can point me to a textbook that discusses it and other rank inequalities, that would be much appreciated!","This is somewhat of a reference request. In several posts on the rank of products of matrices (e.g. Full-rank condition for product of two matrices ), it is stated that It appears that this is a classic result, though I am not familiar with it. If anyone can point me to a textbook that discusses it and other rank inequalities, that would be much appreciated!", \mathrm{rank}(AB) = \mathrm{rank}(B) - \dim \big(\mathrm{N}(A) \cap \mathrm{R}(B)\big),"['linear-algebra', 'reference-request', 'matrix-rank']"
10,Degree of the kernel of a module map $R^n\rightarrow R^n$ for an Euclidean domain $R$,Degree of the kernel of a module map  for an Euclidean domain,R^n\rightarrow R^n R,"Let $R$ be an Euclidean domain with the degree function $d$ . Let $A\in R^{n\times n}$ be an $n\times n$ -matrix with entries in $R$ such that det $(A)=0$ . As a module map $A:R^n\rightarrow R^n$ , there always exists a kernel element $v\in R^n$ since det $(A)=0$ . Assuming $d(A_{ij})\leq m$ for all $i,j$ , is there an explicit bound $k(m,n)$ such that there exists a kernel element $v\in R^n$ satisfying $d(v_i)\leq k(m,n)$ ? Edit : $v$ is assumed to be nonzero.","Let be an Euclidean domain with the degree function . Let be an -matrix with entries in such that det . As a module map , there always exists a kernel element since det . Assuming for all , is there an explicit bound such that there exists a kernel element satisfying ? Edit : is assumed to be nonzero.","R d A\in R^{n\times n} n\times n R (A)=0 A:R^n\rightarrow R^n v\in R^n (A)=0 d(A_{ij})\leq m i,j k(m,n) v\in R^n d(v_i)\leq k(m,n) v","['linear-algebra', 'abstract-algebra', 'matrices', 'euclidean-domain']"
11,Find the analytical form or the order of the spectral radius (or eigenvalues) of a special sparse matrix.,Find the analytical form or the order of the spectral radius (or eigenvalues) of a special sparse matrix.,,"Let $\hat{\bf H}$ be a $p\hat{N}\times p \hat{N}$ sparse matrix consisting of $p\times p$ blocks, where each block is of size $\hat{N}\times\hat{N}$ . The values in $\hat{\bf H}$ is illustrated below (empty places are zero): Asking for help to find the analytic form of the spectral radius (or eigenvalues) of this matrix. If  analytic form is hard to calculate, would it be possible to at least determine the order of the spectral radius approaching 1? I did a simple numerical experiment as the following, If we fix $p = 5$ and let $\hat{N}$ go from 5 to 100, we have If we fix $\hat{N} = 10$ and let $p$ go from 5 to 100, we have","Let be a sparse matrix consisting of blocks, where each block is of size . The values in is illustrated below (empty places are zero): Asking for help to find the analytic form of the spectral radius (or eigenvalues) of this matrix. If  analytic form is hard to calculate, would it be possible to at least determine the order of the spectral radius approaching 1? I did a simple numerical experiment as the following, If we fix and let go from 5 to 100, we have If we fix and let go from 5 to 100, we have",\hat{\bf H} p\hat{N}\times p \hat{N} p\times p \hat{N}\times\hat{N} \hat{\bf H} p = 5 \hat{N} \hat{N} = 10 p,"['linear-algebra', 'eigenvalues-eigenvectors']"
12,"Whilst There Are Three Characteristic Equations, Only Two of Them Are Linearly Independent?","Whilst There Are Three Characteristic Equations, Only Two of Them Are Linearly Independent?",,"Take the general quasi-linear equation $$a(x, y, u)u_x + b(x, y, u)u_y - c(x, y, u) = 0. \tag{1}$$ We assume that there exists a solution of the form $u = u(x, y)$ . We can define a solution surface in $(x, y, u)$ space via the implicit form of the solution $$f(x, y, u) \equiv u(x, y) - u = 0.\tag{2} $$ The normal vector to the solution surface is $$\nabla f = (f_x, f_y, f_u) = (u_x, u_y, -1).\tag{3}$$ We can therefore write the PDE as the dot product of two vectors $$au_x + bu_y - c = (a, b, c) \cdot (u_x, u_y, -1) = 0.\tag{4}$$ This shows that the vector $(a(x, y, u), b(x, y, u), c(x, y, u))$ is a tangent vector to the solution surface at the point $(x, y, u)$ . We can construct a curve in $(x, y, u)$ space such that the tangent of the curve is equal to the vector $(a, b, c)$ at every point. Such a curve is called a characteristic curve . A parameterisation of this curve is given by the equations $x = x(t), y = y(t), u = u(t)$ : $$\left( \frac{dx}{dt}, \frac{dy}{dt}, \frac{du}{dt} \right) = (a, b, c)\tag{5} $$ So we are left with a system of ODEs called the characteristic equations : $$\frac{dx}{dt} = a(x, y, u), \frac{dy}{dt} = b(x, y, u), \frac{du}{dt} = c(x, y, u). \tag{6}$$ I am told that, whilst there are three characteristic equations, only two of them are linearly independent. This implies that their solution consists of a two-parameter family of curves in $(x, y, u)$ space. Why are only two of them linearly independent? And why does this imply that their solution consists of a two-parameter family of curves in $(x, y, u)$ space? I'm trying to understand this for my upcoming lecture on method of characteristics. Help is much appreciated. 8-) EDIT: I found the following information in chapter 4.1 of the textbook Essential Partial Differential Equations , by Griffiths, Dold, and Silvester: We first consider the very special case of (4.1) with $a = b = c = r = 0$ , that is $$pu_x + qu_y = f \ \ \ \text{(4.2)}$$ In many physical applications, see (pde.1), one of the independent variables might represent a time-like variable. In stationary applications both variables might be spatial variables. We begin by considering a curve defined by the height of the surface $z = u(x, y)$ in three dimensions above a path $(x(t), y(t))$ in the $x-y$ plane that is parameterised by $t$ . This curve has slope $$\frac{du}{dt}(x(t), y(t))$$ which, by the chain rule, is given by $$\frac{du}{dt}(x(t), y(t)) = u_x \frac{dx}{dt} + u_y \frac{dy}{dt} \ \ \ (4.3)$$ Thus, by choosing the parameterization such that $$\frac{dx}{dt} = p, \frac{dy}{dt} = q, \ \ \ (4.4)$$ the PDE (4.2) reduces to the ODE $$\frac{du}{dt} = f \ \ \ (4.5)$$ The parameter $t$ is not an intrinsic part of the system and can be avoided by writing the three ODEs in (4.4) and (4.5) in the generic form $$\frac{dx}{p} = \frac{dy}{q} = \frac{du}{f} \ \ \ (4.6)$$ Paths in the $x-y$ plane described by (4.4) are known as characteristic curves or, simply, as characteristics, and equations (4.6) are known as the characteristic equations fo (4.2). The relations (4.6) define three equations, of which any two are independent. P.S.: Bolding in textbook quotation is my own. EDIT 2: Ok, I just realised something. I think this might have something to do with my question: (From the chapter and textbook described in the first edit.) Example 4.1 Find the general solution of the PDE $pu_x + u_y = u$ , where p is constant. Show that the problem is well posed when solved in the infinite strip { (x, y) : x \in \mathbb{R}, 0 \le y \le Y } and an initial condition $u(x, 0) = g(x), x \in \mathbb{R}$ , is applied, where $g$ is a continuous bounded function. The characteristic equations are $$\frac{dx}{p} = \frac{dy}{1} = \frac{du}{u}$$ which we may write as $$\frac{dx}{dy} = p, \frac{du}{dy} = u$$ [...] See how we got the two ODEs $\frac{dx}{dy} = p, \frac{du}{dy} = u$ from the equation $\frac{dx}{p} = \frac{dy}{1} = \frac{du}{u}$ , which has three terms? See how it seems that we cannot get another ODE? If I do the necessary manipulations of $\frac{dx}{dy} = p, \frac{du}{dy} = u$ to get $\frac{dx}{dy} = p$ and $\frac{du}{dy} = u$ , if we then attempt to do further manipulations to get a third ODE, then there must be some dependency in the third ODE we get and one of the other two ODEs. I'm not sure if I'm onto something here, but it's something that I just thought of.","Take the general quasi-linear equation We assume that there exists a solution of the form . We can define a solution surface in space via the implicit form of the solution The normal vector to the solution surface is We can therefore write the PDE as the dot product of two vectors This shows that the vector is a tangent vector to the solution surface at the point . We can construct a curve in space such that the tangent of the curve is equal to the vector at every point. Such a curve is called a characteristic curve . A parameterisation of this curve is given by the equations : So we are left with a system of ODEs called the characteristic equations : I am told that, whilst there are three characteristic equations, only two of them are linearly independent. This implies that their solution consists of a two-parameter family of curves in space. Why are only two of them linearly independent? And why does this imply that their solution consists of a two-parameter family of curves in space? I'm trying to understand this for my upcoming lecture on method of characteristics. Help is much appreciated. 8-) EDIT: I found the following information in chapter 4.1 of the textbook Essential Partial Differential Equations , by Griffiths, Dold, and Silvester: We first consider the very special case of (4.1) with , that is In many physical applications, see (pde.1), one of the independent variables might represent a time-like variable. In stationary applications both variables might be spatial variables. We begin by considering a curve defined by the height of the surface in three dimensions above a path in the plane that is parameterised by . This curve has slope which, by the chain rule, is given by Thus, by choosing the parameterization such that the PDE (4.2) reduces to the ODE The parameter is not an intrinsic part of the system and can be avoided by writing the three ODEs in (4.4) and (4.5) in the generic form Paths in the plane described by (4.4) are known as characteristic curves or, simply, as characteristics, and equations (4.6) are known as the characteristic equations fo (4.2). The relations (4.6) define three equations, of which any two are independent. P.S.: Bolding in textbook quotation is my own. EDIT 2: Ok, I just realised something. I think this might have something to do with my question: (From the chapter and textbook described in the first edit.) Example 4.1 Find the general solution of the PDE , where p is constant. Show that the problem is well posed when solved in the infinite strip { (x, y) : x \in \mathbb{R}, 0 \le y \le Y } and an initial condition , is applied, where is a continuous bounded function. The characteristic equations are which we may write as [...] See how we got the two ODEs from the equation , which has three terms? See how it seems that we cannot get another ODE? If I do the necessary manipulations of to get and , if we then attempt to do further manipulations to get a third ODE, then there must be some dependency in the third ODE we get and one of the other two ODEs. I'm not sure if I'm onto something here, but it's something that I just thought of.","a(x, y, u)u_x + b(x, y, u)u_y - c(x, y, u) = 0. \tag{1} u = u(x, y) (x, y, u) f(x, y, u) \equiv u(x, y) - u = 0.\tag{2}  \nabla f = (f_x, f_y, f_u) = (u_x, u_y, -1).\tag{3} au_x + bu_y - c = (a, b, c) \cdot (u_x, u_y, -1) = 0.\tag{4} (a(x, y, u), b(x, y, u), c(x, y, u)) (x, y, u) (x, y, u) (a, b, c) x = x(t), y = y(t), u = u(t) \left( \frac{dx}{dt}, \frac{dy}{dt}, \frac{du}{dt} \right) = (a, b, c)\tag{5}  \frac{dx}{dt} = a(x, y, u), \frac{dy}{dt} = b(x, y, u), \frac{du}{dt} = c(x, y, u). \tag{6} (x, y, u) (x, y, u) a = b = c = r = 0 pu_x + qu_y = f \ \ \ \text{(4.2)} z = u(x, y) (x(t), y(t)) x-y t \frac{du}{dt}(x(t), y(t)) \frac{du}{dt}(x(t), y(t)) = u_x \frac{dx}{dt} + u_y \frac{dy}{dt} \ \ \ (4.3) \frac{dx}{dt} = p, \frac{dy}{dt} = q, \ \ \ (4.4) \frac{du}{dt} = f \ \ \ (4.5) t \frac{dx}{p} = \frac{dy}{q} = \frac{du}{f} \ \ \ (4.6) x-y pu_x + u_y = u u(x, 0) = g(x), x \in \mathbb{R} g \frac{dx}{p} = \frac{dy}{1} = \frac{du}{u} \frac{dx}{dy} = p, \frac{du}{dy} = u \frac{dx}{dy} = p, \frac{du}{dy} = u \frac{dx}{p} = \frac{dy}{1} = \frac{du}{u} \frac{dx}{dy} = p, \frac{du}{dy} = u \frac{dx}{dy} = p \frac{du}{dy} = u","['linear-algebra', 'partial-differential-equations']"
13,$T$-invariance of $U$ is equivalent to $T^{*}$-invariance of $U^{\perp}$,-invariance of  is equivalent to -invariance of,T U T^{*} U^{\perp},"Is the Following argument correct? Suppose $T\in\mathcal{L}(V)$ and $U$ is a subspace of $V$. Prove that $U$ is invariant under $T$ if and only if $U^{\perp}$ is invariant under $T^*$. Proof. Given that $U$ is invariant under $T$ then $\langle w,Tu\rangle = 0,\forall (u,w)\in U\times U^{\perp}$ equivalently $\langle w,(T^*)^*u\rangle = 0 ,\forall (u,w)\in U\times U^{\perp}$ this when taken together with the definition of the adjoint of $T$ is equivalent to $\langle T^*w,u\rangle =  0,\forall (u,w)\in U\times U^{\perp}$ thus $U^{\perp}$ is invariant under $T^{*}$. $\blacksquare$","Is the Following argument correct? Suppose $T\in\mathcal{L}(V)$ and $U$ is a subspace of $V$. Prove that $U$ is invariant under $T$ if and only if $U^{\perp}$ is invariant under $T^*$. Proof. Given that $U$ is invariant under $T$ then $\langle w,Tu\rangle = 0,\forall (u,w)\in U\times U^{\perp}$ equivalently $\langle w,(T^*)^*u\rangle = 0 ,\forall (u,w)\in U\times U^{\perp}$ this when taken together with the definition of the adjoint of $T$ is equivalent to $\langle T^*w,u\rangle =  0,\forall (u,w)\in U\times U^{\perp}$ thus $U^{\perp}$ is invariant under $T^{*}$. $\blacksquare$",,"['linear-algebra', 'proof-verification', 'orthogonality', 'adjoint-operators']"
14,Fast arbitrary decomposition of a positive-definite matrix,Fast arbitrary decomposition of a positive-definite matrix,,"Given a positive-definite $n\times n$ matrix $\mathbf{A}$, my goal is to present it as a product of the form $\mathbf{H^TH}$, where $\mathbf{H}$ is an arbitrary $n\times n$ matrix. Cholesky decomposition does this with $\mathbf{H}$ upper-triangular. However, its complexity is of the order $O(n^3)$. The question is whether it is possible to do this quicker if we remove all the constraints on $\mathbf{H}$.","Given a positive-definite $n\times n$ matrix $\mathbf{A}$, my goal is to present it as a product of the form $\mathbf{H^TH}$, where $\mathbf{H}$ is an arbitrary $n\times n$ matrix. Cholesky decomposition does this with $\mathbf{H}$ upper-triangular. However, its complexity is of the order $O(n^3)$. The question is whether it is possible to do this quicker if we remove all the constraints on $\mathbf{H}$.",,"['linear-algebra', 'algorithms', 'computational-complexity', 'matrix-decomposition']"
15,Prove $A$ and $B$ are equivalent if and only if $\text{rank}(A) =\text{rank}(B)$,Prove  and  are equivalent if and only if,A B \text{rank}(A) =\text{rank}(B),"Knowing that $A$ is equivalent to $B$ if there exists an invertible $m\times m$ matrix $P$ and an invertible $n\times n$ matrix $Q$ such that $PAQ = B$ , how can I prove that $A$ and $B$ are equivalent iff $\text{rank}(A) =\text{rank}(B)$ ? I've managed to solve the forward direction of the iff and am confident it is correct: Suppose $A$ and $B$ are equivalent. Then, $PAQ = B$ . Knowing this, we can assume $$ \text{rank}(PAQ) \leq \text{rank}(A) = \text{rank}(P^{-1} B Q^{-1}) \leq \text{rank}(B) $$ As $\text{rank}(PAQ) = \text{rank}(B)$ , all inequalities must be equalities, so $\text{rank}(A) =\text{rank}(B)$ . I am not sure how to prove this statement in the reverse direction. I think that the invertible matrix theorem could be useful for this problem","Knowing that is equivalent to if there exists an invertible matrix and an invertible matrix such that , how can I prove that and are equivalent iff ? I've managed to solve the forward direction of the iff and am confident it is correct: Suppose and are equivalent. Then, . Knowing this, we can assume As , all inequalities must be equalities, so . I am not sure how to prove this statement in the reverse direction. I think that the invertible matrix theorem could be useful for this problem","A B m\times m P n\times n Q PAQ = B A B \text{rank}(A) =\text{rank}(B) A B PAQ = B 
\text{rank}(PAQ) \leq \text{rank}(A) = \text{rank}(P^{-1} B Q^{-1}) \leq \text{rank}(B)
 \text{rank}(PAQ) = \text{rank}(B) \text{rank}(A) =\text{rank}(B)","['linear-algebra', 'matrices', 'equivalence-relations', 'matrix-rank']"
16,Find a flag to transform a matrix to an upper triangular one,Find a flag to transform a matrix to an upper triangular one,,"Consider $F: \mathbb{R^3} \to \mathbb{R^3}$ represented by: $ A=     \begin{bmatrix}     1 & 1 & 2 \\     -2 & 5 & 6 \\     1 & -2 & -2 \\     \end{bmatrix} $  , eigenvalues: $(\lambda - 1)^2(\lambda -2)$ Now, $A$ admits Jordan canonical form: $ J_A=     \begin{bmatrix}     2 & 0 & 0 \\     0 & 1 & 1 \\     0 & 0 & 1 \\     \end{bmatrix} $, so, the basis which brings $A$ in this form is a flag. However the exercise asks to find a flag with the method $\{0\} \subseteq V_1 \subseteq V_2 \subseteq ... \subseteq   \mathbb{R^n}$. It requests to verify that: $V_1 =x_1+x_3 =x_2+2x_3=0$ $V_2 = x_2+2x_3=0$ are s.t. $\{0\} \subseteq V_1 \subseteq V_2 \subseteq V_3\ = \mathbb{R^3}$ is a flag for $F$ That is true, because the eigenvector of the eigenvalue $\lambda_1 =1$ is $v_{\lambda_1}=(1, -2, 1)$ and $V_1 = span \{(1, -2, 1\}$; plus, we notice that $(1, -2, 1) = (1, 0, 0) + (0, -2, 1)$ and these two are vectors of a basis of $V_2$, so $\{0\} \subseteq V_1 \subseteq V_2$. Verified that, I have to complete a basis of $V_1$ on a complementary basis $W$ s.t. $V_1 \oplus W =  \mathbb{R^3}$ and write a matrix whose columns are those vectors: $B=     \begin{bmatrix}     1 & 0 & 0 \\     -2 & 1 & 0 \\     1 & 0 & 1 \\     \end{bmatrix} $, for example. I find $B^{-1} =      \begin{bmatrix}     1 & 0 & 0 \\     2 & 1 & 0 \\     -1 & 0 & 1 \\     \end{bmatrix}$ At this point $B^{-1}AB =C$ would be of the type: $     \begin{bmatrix}     \lambda_1 & \bullet & \bullet \\     0 & * & * \\     0 & * & * \\     \end{bmatrix} $, from here we have to write a new basis, whose first vector will be $(1,0,0)$, invariant, the second $(0,a,b)$ where $(a,b)$ is eigenvector of the restriction to $     \begin{bmatrix}     * & * \\     * & *     \\     \end{bmatrix} $ of $C$ and, finally, the tird vector as complementary basis and repeat. But $B^{-1}AB$, so ""built"", isn't of the type $C$ and I don't understand where I'm getting wrong. Thank you!","Consider $F: \mathbb{R^3} \to \mathbb{R^3}$ represented by: $ A=     \begin{bmatrix}     1 & 1 & 2 \\     -2 & 5 & 6 \\     1 & -2 & -2 \\     \end{bmatrix} $  , eigenvalues: $(\lambda - 1)^2(\lambda -2)$ Now, $A$ admits Jordan canonical form: $ J_A=     \begin{bmatrix}     2 & 0 & 0 \\     0 & 1 & 1 \\     0 & 0 & 1 \\     \end{bmatrix} $, so, the basis which brings $A$ in this form is a flag. However the exercise asks to find a flag with the method $\{0\} \subseteq V_1 \subseteq V_2 \subseteq ... \subseteq   \mathbb{R^n}$. It requests to verify that: $V_1 =x_1+x_3 =x_2+2x_3=0$ $V_2 = x_2+2x_3=0$ are s.t. $\{0\} \subseteq V_1 \subseteq V_2 \subseteq V_3\ = \mathbb{R^3}$ is a flag for $F$ That is true, because the eigenvector of the eigenvalue $\lambda_1 =1$ is $v_{\lambda_1}=(1, -2, 1)$ and $V_1 = span \{(1, -2, 1\}$; plus, we notice that $(1, -2, 1) = (1, 0, 0) + (0, -2, 1)$ and these two are vectors of a basis of $V_2$, so $\{0\} \subseteq V_1 \subseteq V_2$. Verified that, I have to complete a basis of $V_1$ on a complementary basis $W$ s.t. $V_1 \oplus W =  \mathbb{R^3}$ and write a matrix whose columns are those vectors: $B=     \begin{bmatrix}     1 & 0 & 0 \\     -2 & 1 & 0 \\     1 & 0 & 1 \\     \end{bmatrix} $, for example. I find $B^{-1} =      \begin{bmatrix}     1 & 0 & 0 \\     2 & 1 & 0 \\     -1 & 0 & 1 \\     \end{bmatrix}$ At this point $B^{-1}AB =C$ would be of the type: $     \begin{bmatrix}     \lambda_1 & \bullet & \bullet \\     0 & * & * \\     0 & * & * \\     \end{bmatrix} $, from here we have to write a new basis, whose first vector will be $(1,0,0)$, invariant, the second $(0,a,b)$ where $(a,b)$ is eigenvector of the restriction to $     \begin{bmatrix}     * & * \\     * & *     \\     \end{bmatrix} $ of $C$ and, finally, the tird vector as complementary basis and repeat. But $B^{-1}AB$, so ""built"", isn't of the type $C$ and I don't understand where I'm getting wrong. Thank you!",,"['linear-algebra', 'matrices', 'geometry', 'jordan-normal-form', 'change-of-basis']"
17,Solution to Vector Lambert W function type Equation,Solution to Vector Lambert W function type Equation,,"I was wondering if anyone has any ideas for a closed-form solution to the equation $$Ax + \exp(x) +b =0$$ where $x,b \in \mathbb{R}^n$, $A$ is a symmetric positive definite matrix and $\exp$ denotes elementwise exponentiation (i.e., $\exp(x) =(\exp(x_1),\exp(x_2), \dots, \exp(x_n) $). The case where $n=1$, $$ ax+\exp(x)+b =0$$ has the solution $$ x= -W_{n}\left(e^{\frac{-b}{a}}\right)+\dfrac{b}{a}, $$ where $a\neq 0$, $n \in \mathbb{Z}$, and $W_{n}$ is the $n$th branch of the Lambert W function. I'm looking for unique solutions of the vector version.","I was wondering if anyone has any ideas for a closed-form solution to the equation $$Ax + \exp(x) +b =0$$ where $x,b \in \mathbb{R}^n$, $A$ is a symmetric positive definite matrix and $\exp$ denotes elementwise exponentiation (i.e., $\exp(x) =(\exp(x_1),\exp(x_2), \dots, \exp(x_n) $). The case where $n=1$, $$ ax+\exp(x)+b =0$$ has the solution $$ x= -W_{n}\left(e^{\frac{-b}{a}}\right)+\dfrac{b}{a}, $$ where $a\neq 0$, $n \in \mathbb{Z}$, and $W_{n}$ is the $n$th branch of the Lambert W function. I'm looking for unique solutions of the vector version.",,"['linear-algebra', 'complex-analysis', 'functional-analysis', 'special-functions', 'lambert-w']"
18,Jordan Block of Kronecker Product,Jordan Block of Kronecker Product,,"Let $A$ be a $(p\times p$)-Jordan block of generalized eigenvalue $\lambda$. Let $B$ be a $(q\times q$)-Jordan block of generalized eigenvalue $\mu$. Then what is the Jordan canonical form for $A\otimes B$, where $\otimes$ is the Kronecker product? I found a reference here without a proof (Horn, Roger A., and Charles R. Johnson. Matrix analysis. Cambridge university press, 1990.): I will be appreciated if anyone can give me a proof of part (a).","Let $A$ be a $(p\times p$)-Jordan block of generalized eigenvalue $\lambda$. Let $B$ be a $(q\times q$)-Jordan block of generalized eigenvalue $\mu$. Then what is the Jordan canonical form for $A\otimes B$, where $\otimes$ is the Kronecker product? I found a reference here without a proof (Horn, Roger A., and Charles R. Johnson. Matrix analysis. Cambridge university press, 1990.): I will be appreciated if anyone can give me a proof of part (a).",,"['linear-algebra', 'modules', 'tensor-products', 'jordan-normal-form']"
19,Learning a point on the sphere using signs of dot products,Learning a point on the sphere using signs of dot products,,"An unknown point $x$ is fixed on the unit sphere in $\mathbb{R}^n$ and we iterate as follows: at each step, specify a unit vector $v\in \mathbb{R}^n$ and record $\mathrm{sign}(x\cdot v).$ The goal is to design a strategy for specifying $x$ to within the smallest ball with the fewest possible steps. With the first step you learn which of two hemispheres $x$ is in. The next step can get you to within a quarter, the next $1/8,$ and so on. With each step you are capable of cutting in half the region $x$ can be in. A natural strategy to get a well-localized region is to first fix an orthonormal basis for $\mathbb{R}^n$ and cycle through it, each step slicing the region along the corresponding basis element's direction.  After $\ell$ steps, $x$ will be known to within a simplex with area $\propto 2^{-\ell}$ . But what is the size of the ball for this strategy? Is there a better strategy?","An unknown point is fixed on the unit sphere in and we iterate as follows: at each step, specify a unit vector and record The goal is to design a strategy for specifying to within the smallest ball with the fewest possible steps. With the first step you learn which of two hemispheres is in. The next step can get you to within a quarter, the next and so on. With each step you are capable of cutting in half the region can be in. A natural strategy to get a well-localized region is to first fix an orthonormal basis for and cycle through it, each step slicing the region along the corresponding basis element's direction.  After steps, will be known to within a simplex with area . But what is the size of the ball for this strategy? Is there a better strategy?","x \mathbb{R}^n v\in \mathbb{R}^n \mathrm{sign}(x\cdot v). x x 1/8, x \mathbb{R}^n \ell x \propto 2^{-\ell}","['linear-algebra', 'geometry', 'spherical-geometry', 'geometric-probability']"
20,Is there a linear injection $ \Lambda^k V^* \otimes \Lambda^k V^* \to \Lambda^k (V^* \otimes V^*)$ which preserves decomposability?,Is there a linear injection  which preserves decomposability?, \Lambda^k V^* \otimes \Lambda^k V^* \to \Lambda^k (V^* \otimes V^*),"Let $V$ be an $n$-dimensional real vector space, and let $2 \le k \le n-2$. Definitions We say an element $\omega \in \Lambda^k V$ is decomposable if $\omega=\alpha_1 \wedge \dots \wedge \alpha_k$, for some $\alpha_i \in V$. We say $\omega$ is a power if $\omega=\alpha \wedge \dots \wedge \alpha$ for some $\alpha \in V$. We say an element $h \in \Lambda^k V^* \otimes \Lambda^k V^* \cong \operatorname{Hom}(\Lambda^k V,\Lambda^k V^*)$ is a power if $h=\Lambda^k g$ for some linear map $g:V \to V^*$. Here, $\Lambda^k g:\Lambda^k V \to \Lambda^k V^*$ is the induced map on exterior powers, that is $$ \Lambda^k g(v_1 \wedge \dots \wedge v_k)=g(v_1) \wedge \dots \wedge g(v_k).$$ Question: Is there a linear injection   $$ \Lambda^k V^* \otimes \Lambda^k V^* \to \Lambda^k (V^* \otimes V^*)$$   which maps power elements to decomposable elements ? Even better, is there an injection which maps power elements to power elements ? Note: For dimensional reasons , there is always some linear embedding of $\Lambda^k V^* \otimes \Lambda^k V^*$ in $ \Lambda^k (V^* \otimes V^*)$. Motivation: This question arose in the context of applying the Plucker relations, to characterise which metrics on exterior powers are induced by metrics at the base. See here .","Let $V$ be an $n$-dimensional real vector space, and let $2 \le k \le n-2$. Definitions We say an element $\omega \in \Lambda^k V$ is decomposable if $\omega=\alpha_1 \wedge \dots \wedge \alpha_k$, for some $\alpha_i \in V$. We say $\omega$ is a power if $\omega=\alpha \wedge \dots \wedge \alpha$ for some $\alpha \in V$. We say an element $h \in \Lambda^k V^* \otimes \Lambda^k V^* \cong \operatorname{Hom}(\Lambda^k V,\Lambda^k V^*)$ is a power if $h=\Lambda^k g$ for some linear map $g:V \to V^*$. Here, $\Lambda^k g:\Lambda^k V \to \Lambda^k V^*$ is the induced map on exterior powers, that is $$ \Lambda^k g(v_1 \wedge \dots \wedge v_k)=g(v_1) \wedge \dots \wedge g(v_k).$$ Question: Is there a linear injection   $$ \Lambda^k V^* \otimes \Lambda^k V^* \to \Lambda^k (V^* \otimes V^*)$$   which maps power elements to decomposable elements ? Even better, is there an injection which maps power elements to power elements ? Note: For dimensional reasons , there is always some linear embedding of $\Lambda^k V^* \otimes \Lambda^k V^*$ in $ \Lambda^k (V^* \otimes V^*)$. Motivation: This question arose in the context of applying the Plucker relations, to characterise which metrics on exterior powers are induced by metrics at the base. See here .",,"['linear-algebra', 'representation-theory', 'tensor-products', 'exterior-algebra']"
21,Minimize $\|x\|_\infty$ such that $Ax=b$,Minimize  such that,\|x\|_\infty Ax=b,"I have $A \in \mathbb{R^{n,m}}$ with $n\leq m$ and $b \in \mathbb{R^{n}}$. $A$ is of rank $n$ (maximal possible rank). I'm looking for $x \in\mathbb{R^{m}}$ such that $Ax=b$ and for which $\|x\|_\infty$ is minimal. Some looking around the Internet tells me the solution would be $x= (AA^T)^{-1}A^Tb$ if I were to be interested in minimizing $\|x\|_2$, but I'm really interested in minimizing $\|x\|_\infty$. If that can help give a more detailed solution, the case I'm particularly interested in is $(n,m)=(3,4)$. Do you see how to solve to this problem ?","I have $A \in \mathbb{R^{n,m}}$ with $n\leq m$ and $b \in \mathbb{R^{n}}$. $A$ is of rank $n$ (maximal possible rank). I'm looking for $x \in\mathbb{R^{m}}$ such that $Ax=b$ and for which $\|x\|_\infty$ is minimal. Some looking around the Internet tells me the solution would be $x= (AA^T)^{-1}A^Tb$ if I were to be interested in minimizing $\|x\|_2$, but I'm really interested in minimizing $\|x\|_\infty$. If that can help give a more detailed solution, the case I'm particularly interested in is $(n,m)=(3,4)$. Do you see how to solve to this problem ?",,"['linear-algebra', 'optimization']"
22,Analogs of Cayley-Hamilton theorem for Pfaffian,Analogs of Cayley-Hamilton theorem for Pfaffian,,"The Pfaffian $\text{pf}$ is defined for a skew-symmetric matrix which is also a polynomial of matrix coefficients. One property for Pfaffian is that $\operatorname {pf} (A)^{2}=\det(A)$ holds for every skew-symmetric matrix A . As for determinants we have Cayley-Hamilton theorem, here is my question: \begin{align} w &= \begin{pmatrix} -A \cdot a & -I \\ I & B \cdot b \end{pmatrix} \\ \operatorname{Pf}(w) &= \sum\limits_{i=0}^{\left\lfloor n/2 \right\rfloor} P_i \cdot a^i b^i \\ A \cdot B &= C \end{align} Where $A,B \in M_n(\Bbb R)$ are skew-symmetric, and $a,b \in \mathbb R$. $\text{Pf}(w)$ is expressed as polynomial of $ab$ ($P_i \in \mathbb R$ are coefficients). Then can we show the following vanishing property? $$ \sum\limits_{i=0}^{\left\lfloor n/2 \right\rfloor} P_i \cdot C^{\left\lceil n/2\right\rceil - i} = 0 $$ My idea is that we can show the square of $P_iC^i$ is zero by Cayley-Hamilton theorem, but how to proceed? Last but not least, can we proof something more general about analogs of Cayley-Hamilton theorem for Pfaffian?","The Pfaffian $\text{pf}$ is defined for a skew-symmetric matrix which is also a polynomial of matrix coefficients. One property for Pfaffian is that $\operatorname {pf} (A)^{2}=\det(A)$ holds for every skew-symmetric matrix A . As for determinants we have Cayley-Hamilton theorem, here is my question: \begin{align} w &= \begin{pmatrix} -A \cdot a & -I \\ I & B \cdot b \end{pmatrix} \\ \operatorname{Pf}(w) &= \sum\limits_{i=0}^{\left\lfloor n/2 \right\rfloor} P_i \cdot a^i b^i \\ A \cdot B &= C \end{align} Where $A,B \in M_n(\Bbb R)$ are skew-symmetric, and $a,b \in \mathbb R$. $\text{Pf}(w)$ is expressed as polynomial of $ab$ ($P_i \in \mathbb R$ are coefficients). Then can we show the following vanishing property? $$ \sum\limits_{i=0}^{\left\lfloor n/2 \right\rfloor} P_i \cdot C^{\left\lceil n/2\right\rceil - i} = 0 $$ My idea is that we can show the square of $P_iC^i$ is zero by Cayley-Hamilton theorem, but how to proceed? Last but not least, can we proof something more general about analogs of Cayley-Hamilton theorem for Pfaffian?",,"['linear-algebra', 'abstract-algebra']"
23,Theorem 2.20 in Friedberg's Linear Algebra,Theorem 2.20 in Friedberg's Linear Algebra,,"Theorem 2.20 Let $V$ and $W$ be finite-dimensional vector spaces over $F$ of dimensions $n$ and $m$, respectively, and let $\beta$ and $\gamma$ be ordered bases for $V$ and $W$, respectively. Then the function $\phi : \mathcal{L}(V,W) \to M_{m \times n} (F)$, defined by $\phi(T) = [T]_\beta^\gamma$, is an isomorphism. In proving the theorem, the author merely shows that $\phi$ is linear and surjective. However, this seems to be fallacious. By just showing surjectivity, it seems that the author is implicitly relying on the finite dimensionality of $\mathcal{L}(V,W)$ to conclude that it must also be injective. This is initially what I thought of doing when I tried proving theorem myself; I was going to use the rank-nullity theorem but then realized I do not have finite dimensionality. Instead, I chose to prove the two following two lemmas: First I proved that $v \in V \setminus \{0\}$ implies $[v]_\beta \neq 0$. Here is my proof: Let $\beta = \{v_1,...,v_n\}$. Since $v$ is not zero, there exist $a_1,...,a_n$ not all equal to zero such that $v = \sum a_i v_i$. By definition, $[v]_\beta = (a_1,...,a_n)^T$, which cannot be the zero vector in $F^n$, since not every entry is zero. Then I proved that $T \neq 0$ implies $[T]_\beta^\gamma \neq 0$. Here is my proof: Let $\gamma = \{w_1,...,w_m\}$ and suppose that $T \neq 0$. Then there exists a $v \in V \setminus \{0\}$ such that $T(v)\neq 0$. Hence $0 \neq [T(v)]_\gamma = [T]_\beta^\gamma [v]_\beta$. If $[T]_\beta^\gamma$ were zero, then we would have a contradiction. Hence, $[T]_\beta^\gamma \neq 0$ From this it follows that if $[T]_\beta^\gamma = 0$, then $T=0$, proving that the kernel of $\phi$ is trivial and therefore injective. Now, as far as I can tell, $\mathcal{L}(V,W)$ being finite dimensional has not been demonstrated yet; in fact, the corollary of this theorem is that $dim (\mathcal{L}(V,W))$, which of course entails finite dimensionality. Thus, it seems that the proof is lacking since injectivity does not follow from what has been proven heretofore. To put it straightforwardly, how do the authors prove the theorem without explicitly showing injectivity?  Perhaps those with a copy of Friedberg might be able to answer the question. EDIT: As Li Chun Min points out, a crucial part of the proof is that there exists a unique linear operator $T : V \to W$ such that $$T(v_j) = \sum_{I=1}^m A_{ij} w_j,$$ which gives one surjectivity and injectivity.","Theorem 2.20 Let $V$ and $W$ be finite-dimensional vector spaces over $F$ of dimensions $n$ and $m$, respectively, and let $\beta$ and $\gamma$ be ordered bases for $V$ and $W$, respectively. Then the function $\phi : \mathcal{L}(V,W) \to M_{m \times n} (F)$, defined by $\phi(T) = [T]_\beta^\gamma$, is an isomorphism. In proving the theorem, the author merely shows that $\phi$ is linear and surjective. However, this seems to be fallacious. By just showing surjectivity, it seems that the author is implicitly relying on the finite dimensionality of $\mathcal{L}(V,W)$ to conclude that it must also be injective. This is initially what I thought of doing when I tried proving theorem myself; I was going to use the rank-nullity theorem but then realized I do not have finite dimensionality. Instead, I chose to prove the two following two lemmas: First I proved that $v \in V \setminus \{0\}$ implies $[v]_\beta \neq 0$. Here is my proof: Let $\beta = \{v_1,...,v_n\}$. Since $v$ is not zero, there exist $a_1,...,a_n$ not all equal to zero such that $v = \sum a_i v_i$. By definition, $[v]_\beta = (a_1,...,a_n)^T$, which cannot be the zero vector in $F^n$, since not every entry is zero. Then I proved that $T \neq 0$ implies $[T]_\beta^\gamma \neq 0$. Here is my proof: Let $\gamma = \{w_1,...,w_m\}$ and suppose that $T \neq 0$. Then there exists a $v \in V \setminus \{0\}$ such that $T(v)\neq 0$. Hence $0 \neq [T(v)]_\gamma = [T]_\beta^\gamma [v]_\beta$. If $[T]_\beta^\gamma$ were zero, then we would have a contradiction. Hence, $[T]_\beta^\gamma \neq 0$ From this it follows that if $[T]_\beta^\gamma = 0$, then $T=0$, proving that the kernel of $\phi$ is trivial and therefore injective. Now, as far as I can tell, $\mathcal{L}(V,W)$ being finite dimensional has not been demonstrated yet; in fact, the corollary of this theorem is that $dim (\mathcal{L}(V,W))$, which of course entails finite dimensionality. Thus, it seems that the proof is lacking since injectivity does not follow from what has been proven heretofore. To put it straightforwardly, how do the authors prove the theorem without explicitly showing injectivity?  Perhaps those with a copy of Friedberg might be able to answer the question. EDIT: As Li Chun Min points out, a crucial part of the proof is that there exists a unique linear operator $T : V \to W$ such that $$T(v_j) = \sum_{I=1}^m A_{ij} w_j,$$ which gives one surjectivity and injectivity.",,"['linear-algebra', 'matrices', 'linear-transformations', 'vector-space-isomorphism']"
24,Eigenvalues and Eigenvectors of block tridiagonal Toeplitz matrix.,Eigenvalues and Eigenvectors of block tridiagonal Toeplitz matrix.,,"I have an infinite matrix where all the elements of the diagonal are given by the same $2 \times 2 $ real and symmetric matrix , and the elements of the supradiagonal and superdiagonal are the same and given by another $2\times 2$ real and symmetric matrix. I.e.: $$\begin{bmatrix}A&B\\B&A&B\\&B&A&\ddots\\&&\ddots&\ddots\end{bmatrix},$$ where $A$ and $B$ are $2\times 2$ and real symmetric. Is it possible to obtain an expression for the eigenvalues and eigenvectors of this matrix?","I have an infinite matrix where all the elements of the diagonal are given by the same $2 \times 2 $ real and symmetric matrix , and the elements of the supradiagonal and superdiagonal are the same and given by another $2\times 2$ real and symmetric matrix. I.e.: $$\begin{bmatrix}A&B\\B&A&B\\&B&A&\ddots\\&&\ddots&\ddots\end{bmatrix},$$ where $A$ and $B$ are $2\times 2$ and real symmetric. Is it possible to obtain an expression for the eigenvalues and eigenvectors of this matrix?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
25,Stability of point spectrum,Stability of point spectrum,,"Suppose $T$, $S$ are bounded operators on $l_2$, $a_n\to 0$ a sequence of complex numbers with the property that for any $n\in\mathbb{N}$,  $T+a_nS$  has discrete spectrum and non-empty point spectrum. Does it follow that $T$, as the norm limit of $T+a_nS$ also has non-empty point spectrum? If the answer would be ""no"" for any $S$, would it change if we take $S$ compact or finite rank?","Suppose $T$, $S$ are bounded operators on $l_2$, $a_n\to 0$ a sequence of complex numbers with the property that for any $n\in\mathbb{N}$,  $T+a_nS$  has discrete spectrum and non-empty point spectrum. Does it follow that $T$, as the norm limit of $T+a_nS$ also has non-empty point spectrum? If the answer would be ""no"" for any $S$, would it change if we take $S$ compact or finite rank?",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'banach-spaces', 'operator-algebras']"
26,"What would be the concept of ""Infinite Dimension Orientability""?","What would be the concept of ""Infinite Dimension Orientability""?",,"We have the following definition of orientability for finite dimensional vector spaces: Definition: Let $\Bbb{E}$ be a finite dimension vector space and $\mathscr{B}(\Bbb{E})$ be the set of all ordered basis of $\Bbb{E}$, i.e. if $\dim \Bbb{E}=n$, then $\scr{B}(\Bbb{E})=\{(x_1,\dots,x_n)\in \Bbb{E}^n\,:\, \{x_1,\dots,x_n\} \text{ is a basis for $\Bbb{E}$}\}$. We define an equivalence relation on $\scr{B}(\Bbb{E})$ as follows: given $B_1=(x_1,\dots,x_n)$ and $B_2=(y_1,\dots,y_n)\in \scr{B}(\Bbb{E})$ we write $B_1\sim B_2$ if $\det A>0$, where $A$ is the matrix of the isomorphism ""change of basis"" $T:\Bbb{E}\to\Bbb{E}$, such that $T(x_i)=y_i$, $i=1,\dots,n$. If $B_1\sim B_2$ we say that $B_1$ has the same orientation of $B_2$ . With this relation, $\scr{B}(\Bbb{E})/\sim$ has two equivalence classes, one of them (usually that one containing the standard basis in the case $\Bbb{E}=\Bbb{R}^n$) is called positive orientation and the other one negative orientation . We also have a definition of orientability for differentiable manifolds using this notion. My question is quite simple: is possible to generalize the notion of orientability for infinite dimensional vector spaces? If we try to generalize the notion directly from the finite dimensional case we face the problems of saying what is an ordered basis and the lack of matrices for linear isomorphisms, so we cannot state the condition of the ""positive determinant of the matrix change of basis"".","We have the following definition of orientability for finite dimensional vector spaces: Definition: Let $\Bbb{E}$ be a finite dimension vector space and $\mathscr{B}(\Bbb{E})$ be the set of all ordered basis of $\Bbb{E}$, i.e. if $\dim \Bbb{E}=n$, then $\scr{B}(\Bbb{E})=\{(x_1,\dots,x_n)\in \Bbb{E}^n\,:\, \{x_1,\dots,x_n\} \text{ is a basis for $\Bbb{E}$}\}$. We define an equivalence relation on $\scr{B}(\Bbb{E})$ as follows: given $B_1=(x_1,\dots,x_n)$ and $B_2=(y_1,\dots,y_n)\in \scr{B}(\Bbb{E})$ we write $B_1\sim B_2$ if $\det A>0$, where $A$ is the matrix of the isomorphism ""change of basis"" $T:\Bbb{E}\to\Bbb{E}$, such that $T(x_i)=y_i$, $i=1,\dots,n$. If $B_1\sim B_2$ we say that $B_1$ has the same orientation of $B_2$ . With this relation, $\scr{B}(\Bbb{E})/\sim$ has two equivalence classes, one of them (usually that one containing the standard basis in the case $\Bbb{E}=\Bbb{R}^n$) is called positive orientation and the other one negative orientation . We also have a definition of orientability for differentiable manifolds using this notion. My question is quite simple: is possible to generalize the notion of orientability for infinite dimensional vector spaces? If we try to generalize the notion directly from the finite dimensional case we face the problems of saying what is an ordered basis and the lack of matrices for linear isomorphisms, so we cannot state the condition of the ""positive determinant of the matrix change of basis"".",,"['linear-algebra', 'functional-analysis', 'infinity', 'orientation']"
27,Can basis of kernel be extended to a Jordan basis?,Can basis of kernel be extended to a Jordan basis?,,"Let $A\in\mathbb C^{n\times n}$ be nilpotent. A Jordan basis of $A$ is a basis of $\mathbb C^n$ with respect to which $A$ has Jordan normal form. Assume that we do not know the Jordan structure of $A$. Given a basis of the kernel of $A$, is there a criterion to decide on whether this basis can be extended to a Jordan basis of $A$ (maybe in terms of powers of $A^*$ or whatever)?","Let $A\in\mathbb C^{n\times n}$ be nilpotent. A Jordan basis of $A$ is a basis of $\mathbb C^n$ with respect to which $A$ has Jordan normal form. Assume that we do not know the Jordan structure of $A$. Given a basis of the kernel of $A$, is there a criterion to decide on whether this basis can be extended to a Jordan basis of $A$ (maybe in terms of powers of $A^*$ or whatever)?",,"['linear-algebra', 'jordan-normal-form']"
28,Is there a proof of $|Aut(E/F)| \leq [E:F]$ using linear algebra?,Is there a proof of  using linear algebra?,|Aut(E/F)| \leq [E:F],"In the text by Dummit and Foote, there is the following proposition: Proposition Let $E$ be the splitting field over $F$ of the polynomial $f(x) \in F[x]$. Then we have    \begin{equation*} |Aut(E/F)| \leq [E:F] \end{equation*}   with equality iff $f(x)$ is separable over $F$. The proof of this proposition involves looking at the number of possible ways of extending an isomorphism $\phi: F \to F' $ to an isomorphism $\sigma: E \to E'$ between the splitting field $E$ of $F$ and the splitting field $E'$ of $F'$. I am wondering if there is a proof of this proposition that uses linear algebra. This seems at least plausible to me, since the linear algebra analog of an automorphism of $K$ is just an invertible linear transformation. This suggests writing something like: \begin{equation*} \Big(\text{number of invertible maps from V to V fixing F}\Big) \leq dim_F(V). \end{equation*} But there are (at least) two problems with the above statement: The base field (scalar field) $F$ is not necessarily contained in $V$, and it is not clear (to me at least) how to embed $F$ into $V$. There is no linear algebra analog of a transformation ""fixing"" a space. (Note: I guess one could just say that $T$ fixes a subset $A$ of $T$ if $T$ is the identity on $A$, so maybe this is not really an issue...?) If there is not a way to prove the above proposition using only linear algebra, is there a proof that at least uses a decent amount of linear algebra?","In the text by Dummit and Foote, there is the following proposition: Proposition Let $E$ be the splitting field over $F$ of the polynomial $f(x) \in F[x]$. Then we have    \begin{equation*} |Aut(E/F)| \leq [E:F] \end{equation*}   with equality iff $f(x)$ is separable over $F$. The proof of this proposition involves looking at the number of possible ways of extending an isomorphism $\phi: F \to F' $ to an isomorphism $\sigma: E \to E'$ between the splitting field $E$ of $F$ and the splitting field $E'$ of $F'$. I am wondering if there is a proof of this proposition that uses linear algebra. This seems at least plausible to me, since the linear algebra analog of an automorphism of $K$ is just an invertible linear transformation. This suggests writing something like: \begin{equation*} \Big(\text{number of invertible maps from V to V fixing F}\Big) \leq dim_F(V). \end{equation*} But there are (at least) two problems with the above statement: The base field (scalar field) $F$ is not necessarily contained in $V$, and it is not clear (to me at least) how to embed $F$ into $V$. There is no linear algebra analog of a transformation ""fixing"" a space. (Note: I guess one could just say that $T$ fixes a subset $A$ of $T$ if $T$ is the identity on $A$, so maybe this is not really an issue...?) If there is not a way to prove the above proposition using only linear algebra, is there a proof that at least uses a decent amount of linear algebra?",,"['linear-algebra', 'abstract-algebra', 'galois-theory']"
29,Why are large Hermitian Toeplitz matrices approximately diagonalized by sinusoids?,Why are large Hermitian Toeplitz matrices approximately diagonalized by sinusoids?,,"A Toeplitz matrix $T$ is characterized by $T_{jk}=t_{j-k}$ for some numbers $t_k$ (so its entries are constant along each diagonal). A Toeplitz operator is (for my purposes) a mapping from $\ell^p(\mathbb{Z})$ to $\ell^q(\mathbb{Z})$ which can be represented by a doubly infinite Toeplitz matrix. (Here $q$ could be anything, not necessarily the Holder conjugate of $p$.) I've seen in the literature, for example at http://jfi.uchicago.edu/~leop/SciencePapers/jstat9_05_p05012.pdf that the eigenvectors of Hermitian Toeplitz operators are given by sinusoids. Similarly, given suitable decay, large but finite Hermitian Toeplitz matrices have approximately sinusoidal eigenvectors. I can look into details about the convergence myself (especially since this is apparently relatively technical), but the issue about the operators seems to be treated as rather trivial in the literature. For example that paper pretty much says it follows immediately from translation invariance, and I can't seem to follow that. What is the basic idea here? I don't need all the details, I can iron some of them out myself. If it is relevant, the particular case I am handling has $t_k=a^{|k|}$ where $0<a<1$. It seems that the intuition is about a connection to circulant matrices. Given a symmetric $n \times n$ Toeplitz matrix $T$, we can define a ""circulant approximation"" $C$ by $c_{jk}=\begin{cases} t_0 & j=k \\ t_{j-k} + t_{(n-1)-(j-k)} & j \neq k \end{cases}$. If we now consider the sequence $T_n$ of such Toeplitz matrices generated by a $t \in \ell^1$, and a corresponding sequence of circulant matrices $C_n$, then $T_n$ and $C_n$ approach one another in Hilbert-Schmidt norm. This ensures that the eigenvalues are close. Additionally, the eigenvectors of the circulant matrix are complex exponentials (specifically, the columns of the discrete Fourier transform, or maybe its inverse). So we might hope that the eigenvectors are close too. How can we understand this similarity?","A Toeplitz matrix $T$ is characterized by $T_{jk}=t_{j-k}$ for some numbers $t_k$ (so its entries are constant along each diagonal). A Toeplitz operator is (for my purposes) a mapping from $\ell^p(\mathbb{Z})$ to $\ell^q(\mathbb{Z})$ which can be represented by a doubly infinite Toeplitz matrix. (Here $q$ could be anything, not necessarily the Holder conjugate of $p$.) I've seen in the literature, for example at http://jfi.uchicago.edu/~leop/SciencePapers/jstat9_05_p05012.pdf that the eigenvectors of Hermitian Toeplitz operators are given by sinusoids. Similarly, given suitable decay, large but finite Hermitian Toeplitz matrices have approximately sinusoidal eigenvectors. I can look into details about the convergence myself (especially since this is apparently relatively technical), but the issue about the operators seems to be treated as rather trivial in the literature. For example that paper pretty much says it follows immediately from translation invariance, and I can't seem to follow that. What is the basic idea here? I don't need all the details, I can iron some of them out myself. If it is relevant, the particular case I am handling has $t_k=a^{|k|}$ where $0<a<1$. It seems that the intuition is about a connection to circulant matrices. Given a symmetric $n \times n$ Toeplitz matrix $T$, we can define a ""circulant approximation"" $C$ by $c_{jk}=\begin{cases} t_0 & j=k \\ t_{j-k} + t_{(n-1)-(j-k)} & j \neq k \end{cases}$. If we now consider the sequence $T_n$ of such Toeplitz matrices generated by a $t \in \ell^1$, and a corresponding sequence of circulant matrices $C_n$, then $T_n$ and $C_n$ approach one another in Hilbert-Schmidt norm. This ensures that the eigenvalues are close. Additionally, the eigenvectors of the circulant matrix are complex exponentials (specifically, the columns of the discrete Fourier transform, or maybe its inverse). So we might hope that the eigenvectors are close too. How can we understand this similarity?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'asymptotics', 'operator-theory']"
30,are all vectors position vectors?,are all vectors position vectors?,,"So i have two questions: 1) If Vectors (say in R2) are independent of their location in space, then couldn't you theoretically for any vector in R2 regardless of its position, just move it such that its tail emanates from the origin to become a position vector? 2) Can we represent all positions/points in the 2D coordinate system as position vectors, and does that mean all points in the 2D coordinate system are just vectors emanating from the origin?","So i have two questions: 1) If Vectors (say in R2) are independent of their location in space, then couldn't you theoretically for any vector in R2 regardless of its position, just move it such that its tail emanates from the origin to become a position vector? 2) Can we represent all positions/points in the 2D coordinate system as position vectors, and does that mean all points in the 2D coordinate system are just vectors emanating from the origin?",,"['linear-algebra', 'vectors']"
31,Intuition behind the change of basis matrix,Intuition behind the change of basis matrix,,"My linear algebra class isn't particularly rigorous and my professor doesn't really provide much intuition for most of the theorems we learn, either. Because of this, I've made an effort to make sense of the theorems beyond ""the math works out to this result."" I'm finding difficulty finding intuition for the change of basis matrix. In class, we learned that for some subspace $V$, if we take a non-orthonormal basis, $\mathfrak{B} = \left( \vec{v_1}, \ldots, \vec{v_n} \right)$, and find an orthonormal basis using the Gram-Schmidt process, $\mathfrak{U} = \left( \vec{u_1}, \ldots, \vec{u_n} \right)$, we can write $$ \begin{pmatrix}      |    &     |     &        &   | \\ \vec{v_1} & \vec{v_2} & \cdots & \vec{v_n} \\      |    &     |     &        &   | \end{pmatrix} = \begin{pmatrix}      |    &     |     &        &   | \\ \vec{u_1} & \vec{u_2} & \cdots & \vec{u_n} \\      |    &     |     &        &   | \end{pmatrix} R, $$ where $R$ is the change of basis matrix (I think) whose entries are related to the decomposition of each vector in $\mathfrak{B}$. For simplicity, I'll call the first matrix $B$ and the second, $U$. From there, it follows that for a vector $\vec{x}$, $$ \begin{align} \vec{x} = B\left[ \vec{x} \right]_\mathfrak{B} &= U\left[ \vec{x} \right]_\mathfrak{U} \\ UR\left[ \vec{x} \right]_\mathfrak{B} &= U\left[ \vec{x} \right]_\mathfrak{U} \\ \Rightarrow R\left[ \vec{x} \right]_\mathfrak{B} &= \left[ \vec{x} \right]_\mathfrak{U} \end{align} $$ for that same $R$. So I think I've found intuition for this matrix $R$ in the context of the first equation: $R$ is sort of like an ""un-decomposition"" matrix because the columns tell you how to retrieve the vectors from the original basis using the orthonormal basis you created. However, from a linear transformation standpoint, it doesn't make much sense to me. In my head, I would take my matrix $U$ and transform that into $B$ using $R$, so I would expect the first equation to look like $$B = R(U)$$ but interestingly enough, it appears that $R$ is being transformed by $U$. So my first question is how do I make sense of this equation from a transformation perspective? Another thing I'm having trouble understanding is the relationship between the different coordinates of $\vec{x}$, which is the following equation: $$R\left[ \vec{x} \right]_\mathfrak{B} = \left[ \vec{x} \right]_\mathfrak{U}$$ $R$ is this ""un-decomposition"" matrix, but it also somehow magically transforms the coordinates of $\vec{x}$ with respect to $\mathfrak{B}$ to coordinates with respect to $\mathfrak{U}$. So, my next question is how come the same matrix $R$ serves both purposes? My main question overall is how can I interpret this matrix $R$ from a geometric/linear transformation perspective? Thank you in advance. EDIT: After exploring elementary matrices (which my class skipped), I found something very interesting. If we have an elementary matrix $L$ and a matrix of interest $A$, then the rows of $LA$ are given by linear combinations of the rows of $A$, with the coefficients given by the corresponding row of $L$. For example, if we had the product $$ \begin{pmatrix} 1 & -2 & 0 \\ 0 &  1 & 0 \\ 0 &  0 & 0 \end{pmatrix} \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} $$ the first row of the product would be given by $$   1 \cdot \begin{pmatrix}  1 &  2 &  3 \end{pmatrix} - 2 \cdot \begin{pmatrix}  4 &  5 &  6 \end{pmatrix} + 0 \cdot \begin{pmatrix}  7 &  8 &  9 \end{pmatrix} =         \begin{pmatrix} -7 & -8 & -9 \end{pmatrix} $$ and so on. This is quite similar to the situation with the first equation, $B = UR$. However, in that equation, what's happening is that we have linear combinations of the columns, as opposed to the rows as with left multiplication of elementary matrices. So, my idea was that if we put the elementary matrix to the right of our matrix of interest, then we'll have a linear combination of the columns instead. It turns out that my intuition was true. Using the same example as before, we expect the first column to be $$   1 \cdot \begin{pmatrix}  1 \\  0 \\  0 \end{pmatrix} + 4 \cdot \begin{pmatrix} -2 \\  1 \\  0 \end{pmatrix} + 7 \cdot \begin{pmatrix}  0 \\  0 \\  1 \end{pmatrix} =         \begin{pmatrix} -7 \\  4 \\  7 \end{pmatrix} $$ and it is indeed correct! This idea provides intuition for the fact that $(AB)^T = B^TA^T$: $A$ tells us how to find the rows of $AB$ using linear combinations of the rows of $B$, so if we turn each row into columns, we expect to change the order of multiplication since left multiplication tells us how to find rows and right multiplication tells us how to find columns. Interpreting $R$ as an elementary matrix explains the order of multiplication in $B = UR$: $R$ is multiplied on the right and tells us how to find the columns vectors of $B$ as a linear combination of the column vectors of $U$. Lastly, it gives an idea as to why $R$ is multiplied on the left of our coordinate matrices: each row of the coordinate matrix corresponds to a vector in the associated basis. In this particular scenario, each row in $\left[ \vec{x} \right]_\mathfrak{B}$ corresponds to a column vector in $B$, which corresponds to a linear combination of column vectors in $U$. So, putting the rows in the correct linear combination in $\left[ \vec{x} \right]_\mathfrak{B}$ is analogous to putting columns in some linear combination together, which is why a single $R$ serves these two purposes.","My linear algebra class isn't particularly rigorous and my professor doesn't really provide much intuition for most of the theorems we learn, either. Because of this, I've made an effort to make sense of the theorems beyond ""the math works out to this result."" I'm finding difficulty finding intuition for the change of basis matrix. In class, we learned that for some subspace $V$, if we take a non-orthonormal basis, $\mathfrak{B} = \left( \vec{v_1}, \ldots, \vec{v_n} \right)$, and find an orthonormal basis using the Gram-Schmidt process, $\mathfrak{U} = \left( \vec{u_1}, \ldots, \vec{u_n} \right)$, we can write $$ \begin{pmatrix}      |    &     |     &        &   | \\ \vec{v_1} & \vec{v_2} & \cdots & \vec{v_n} \\      |    &     |     &        &   | \end{pmatrix} = \begin{pmatrix}      |    &     |     &        &   | \\ \vec{u_1} & \vec{u_2} & \cdots & \vec{u_n} \\      |    &     |     &        &   | \end{pmatrix} R, $$ where $R$ is the change of basis matrix (I think) whose entries are related to the decomposition of each vector in $\mathfrak{B}$. For simplicity, I'll call the first matrix $B$ and the second, $U$. From there, it follows that for a vector $\vec{x}$, $$ \begin{align} \vec{x} = B\left[ \vec{x} \right]_\mathfrak{B} &= U\left[ \vec{x} \right]_\mathfrak{U} \\ UR\left[ \vec{x} \right]_\mathfrak{B} &= U\left[ \vec{x} \right]_\mathfrak{U} \\ \Rightarrow R\left[ \vec{x} \right]_\mathfrak{B} &= \left[ \vec{x} \right]_\mathfrak{U} \end{align} $$ for that same $R$. So I think I've found intuition for this matrix $R$ in the context of the first equation: $R$ is sort of like an ""un-decomposition"" matrix because the columns tell you how to retrieve the vectors from the original basis using the orthonormal basis you created. However, from a linear transformation standpoint, it doesn't make much sense to me. In my head, I would take my matrix $U$ and transform that into $B$ using $R$, so I would expect the first equation to look like $$B = R(U)$$ but interestingly enough, it appears that $R$ is being transformed by $U$. So my first question is how do I make sense of this equation from a transformation perspective? Another thing I'm having trouble understanding is the relationship between the different coordinates of $\vec{x}$, which is the following equation: $$R\left[ \vec{x} \right]_\mathfrak{B} = \left[ \vec{x} \right]_\mathfrak{U}$$ $R$ is this ""un-decomposition"" matrix, but it also somehow magically transforms the coordinates of $\vec{x}$ with respect to $\mathfrak{B}$ to coordinates with respect to $\mathfrak{U}$. So, my next question is how come the same matrix $R$ serves both purposes? My main question overall is how can I interpret this matrix $R$ from a geometric/linear transformation perspective? Thank you in advance. EDIT: After exploring elementary matrices (which my class skipped), I found something very interesting. If we have an elementary matrix $L$ and a matrix of interest $A$, then the rows of $LA$ are given by linear combinations of the rows of $A$, with the coefficients given by the corresponding row of $L$. For example, if we had the product $$ \begin{pmatrix} 1 & -2 & 0 \\ 0 &  1 & 0 \\ 0 &  0 & 0 \end{pmatrix} \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} $$ the first row of the product would be given by $$   1 \cdot \begin{pmatrix}  1 &  2 &  3 \end{pmatrix} - 2 \cdot \begin{pmatrix}  4 &  5 &  6 \end{pmatrix} + 0 \cdot \begin{pmatrix}  7 &  8 &  9 \end{pmatrix} =         \begin{pmatrix} -7 & -8 & -9 \end{pmatrix} $$ and so on. This is quite similar to the situation with the first equation, $B = UR$. However, in that equation, what's happening is that we have linear combinations of the columns, as opposed to the rows as with left multiplication of elementary matrices. So, my idea was that if we put the elementary matrix to the right of our matrix of interest, then we'll have a linear combination of the columns instead. It turns out that my intuition was true. Using the same example as before, we expect the first column to be $$   1 \cdot \begin{pmatrix}  1 \\  0 \\  0 \end{pmatrix} + 4 \cdot \begin{pmatrix} -2 \\  1 \\  0 \end{pmatrix} + 7 \cdot \begin{pmatrix}  0 \\  0 \\  1 \end{pmatrix} =         \begin{pmatrix} -7 \\  4 \\  7 \end{pmatrix} $$ and it is indeed correct! This idea provides intuition for the fact that $(AB)^T = B^TA^T$: $A$ tells us how to find the rows of $AB$ using linear combinations of the rows of $B$, so if we turn each row into columns, we expect to change the order of multiplication since left multiplication tells us how to find rows and right multiplication tells us how to find columns. Interpreting $R$ as an elementary matrix explains the order of multiplication in $B = UR$: $R$ is multiplied on the right and tells us how to find the columns vectors of $B$ as a linear combination of the column vectors of $U$. Lastly, it gives an idea as to why $R$ is multiplied on the left of our coordinate matrices: each row of the coordinate matrix corresponds to a vector in the associated basis. In this particular scenario, each row in $\left[ \vec{x} \right]_\mathfrak{B}$ corresponds to a column vector in $B$, which corresponds to a linear combination of column vectors in $U$. So, putting the rows in the correct linear combination in $\left[ \vec{x} \right]_\mathfrak{B}$ is analogous to putting columns in some linear combination together, which is why a single $R$ serves these two purposes.",,['linear-algebra']
32,What is a nonnegative complex number?,What is a nonnegative complex number?,,"I'm reading Linear Algebra by Axler, and he states in page 225 that «a complex number is nonnegative iff it has a nonnegative square root». What does the author mean by this? Any help would be appreciated.","I'm reading Linear Algebra by Axler, and he states in page 225 that «a complex number is nonnegative iff it has a nonnegative square root». What does the author mean by this? Any help would be appreciated.",,"['linear-algebra', 'complex-numbers']"
33,"Perturbation theory for least squares for very different A, b","Perturbation theory for least squares for very different A, b",,"Consider the least squares problem $f(x;A,b) = \|Ax-b\|_2^2$ and define $x^*$ the minimizer of $f(x;\hat A,\hat b)$, and $\hat x$ the minimizer of $f(x; A_2, b_2)$. I want to put some bound on $\|x^* - \hat x\|$. Looking through Golub/Van Loan, I see a lot of stuff that is basically some function of $\epsilon = \max\{\|A-\hat A\|, \|b-\hat b\|\}$, but in some sense that's not the best you can do. For example, if $A =\left[\begin{matrix} 0 \\ C\end{matrix}\right], \hat A =\left[\begin{matrix}  C\\ 0\end{matrix}\right], b =\left[\begin{matrix} 0 \\ d\end{matrix}\right], \hat b =\left[\begin{matrix}  d\\ 0\end{matrix}\right]$ then $\epsilon = \max\{2\|C\|, 2\|d\|\}$ which may be very large, but $\|x^*-\hat x\| = 0$. Are there existing bounds that take this into account? It has to be some bound on some function that involves $a_i$ AND $b_i$ (for $A= [a_1^T, ...]$ and $b = [b_1;...]$) and not just some stuff on range space of $A$. I suspect there is some result in machine learning, since this is basically about regression solutions if you get enough ""important"" samples. Can anyone point me to any known results? Thank you!","Consider the least squares problem $f(x;A,b) = \|Ax-b\|_2^2$ and define $x^*$ the minimizer of $f(x;\hat A,\hat b)$, and $\hat x$ the minimizer of $f(x; A_2, b_2)$. I want to put some bound on $\|x^* - \hat x\|$. Looking through Golub/Van Loan, I see a lot of stuff that is basically some function of $\epsilon = \max\{\|A-\hat A\|, \|b-\hat b\|\}$, but in some sense that's not the best you can do. For example, if $A =\left[\begin{matrix} 0 \\ C\end{matrix}\right], \hat A =\left[\begin{matrix}  C\\ 0\end{matrix}\right], b =\left[\begin{matrix} 0 \\ d\end{matrix}\right], \hat b =\left[\begin{matrix}  d\\ 0\end{matrix}\right]$ then $\epsilon = \max\{2\|C\|, 2\|d\|\}$ which may be very large, but $\|x^*-\hat x\| = 0$. Are there existing bounds that take this into account? It has to be some bound on some function that involves $a_i$ AND $b_i$ (for $A= [a_1^T, ...]$ and $b = [b_1;...]$) and not just some stuff on range space of $A$. I suspect there is some result in machine learning, since this is basically about regression solutions if you get enough ""important"" samples. Can anyone point me to any known results? Thank you!",,"['linear-algebra', 'regression', 'machine-learning', 'linear-regression']"
34,Geometry Of Unitary Transformations,Geometry Of Unitary Transformations,,"Ever since I first took Linear Algebra, I have over time realized how concepts like determinants, eigenvalues, diagonalization, orthogonal transformations and so on have very intuitive geometric interpretations, and so begun to fill in many gaps in my understanding of the subject (the course I took, just like most freshman-year courses I've taken, focused too much on pure computation and not enough on intuition / visualization, as do most course books on linear algebra that I've read). Now, determinants are volumes (technically, multiplicative volume changes), diagonalization is just re-expressing the action of a matrix in terms of scalings of an eigenbase, and orthogonal transformations are norm- (and inner product-) preserving, and therefore fully characterized by compositions of rotations and reflections (and I still don't fully understand why the books start with $A^{-1}=A^T$ as the first definition of an orthogonal matrix…)… …but what about Unitary transformations / matrices ? They are also norm- (and inner product-) preserving, but may take complex-valued entries. How can I interpret this geometrically? For example, I know that the group SU(2) can be visualized as the 3-sphere (they are apparently diffeomorphic), but I cannot see the picture clearly in my head.","Ever since I first took Linear Algebra, I have over time realized how concepts like determinants, eigenvalues, diagonalization, orthogonal transformations and so on have very intuitive geometric interpretations, and so begun to fill in many gaps in my understanding of the subject (the course I took, just like most freshman-year courses I've taken, focused too much on pure computation and not enough on intuition / visualization, as do most course books on linear algebra that I've read). Now, determinants are volumes (technically, multiplicative volume changes), diagonalization is just re-expressing the action of a matrix in terms of scalings of an eigenbase, and orthogonal transformations are norm- (and inner product-) preserving, and therefore fully characterized by compositions of rotations and reflections (and I still don't fully understand why the books start with $A^{-1}=A^T$ as the first definition of an orthogonal matrix…)… …but what about Unitary transformations / matrices ? They are also norm- (and inner product-) preserving, but may take complex-valued entries. How can I interpret this geometrically? For example, I know that the group SU(2) can be visualized as the 3-sphere (they are apparently diffeomorphic), but I cannot see the picture clearly in my head.",,"['linear-algebra', 'geometry', 'complex-numbers', 'soft-question']"
35,Relationship between eigenvectors of correlation and covariance matrices,Relationship between eigenvectors of correlation and covariance matrices,,"For the purpose of computing principal components of a dataset, represented as matrix $X$ of dimensions $n \times p$ with $n$ samples and $p$ features, we can compute sample covariance matrix $S$, and compute its eigenvalue decomposition: $$     S = Q^t D Q $$ The principal components are then given by $Z = X \cdot Q$. An alternative method is to use sample correlation matrix: $\hat{S} = \sigma^{-1} S \sigma^{-1} $, where $\sigma = \mathrm{diag}\left(\sigma_1, \cdots, \sigma_p\right)$ is the diagonal matrix of sample standard deviations: $$     \hat{S} = \hat{Q}^t \hat{D} \hat{Q} $$ The resulting principal components $\hat{Z} = X \cdot \hat{Q}$ are different, but span the same vector space, hence there exists a matrix $T$, such that $Z = \hat{Z} \cdot T$. For reasons of numerical stability it is preferred to work with the correlation matrix. I was wondering if it is possible to use $\sigma$, $\hat{D}$ and $\hat{Q}$ to compute $Q$, and avoid doing the eigenvalue decomposition of $S$ recombined from $\sigma$, $\hat{D}$ and $\hat{Q}$ ?","For the purpose of computing principal components of a dataset, represented as matrix $X$ of dimensions $n \times p$ with $n$ samples and $p$ features, we can compute sample covariance matrix $S$, and compute its eigenvalue decomposition: $$     S = Q^t D Q $$ The principal components are then given by $Z = X \cdot Q$. An alternative method is to use sample correlation matrix: $\hat{S} = \sigma^{-1} S \sigma^{-1} $, where $\sigma = \mathrm{diag}\left(\sigma_1, \cdots, \sigma_p\right)$ is the diagonal matrix of sample standard deviations: $$     \hat{S} = \hat{Q}^t \hat{D} \hat{Q} $$ The resulting principal components $\hat{Z} = X \cdot \hat{Q}$ are different, but span the same vector space, hence there exists a matrix $T$, such that $Z = \hat{Z} \cdot T$. For reasons of numerical stability it is preferred to work with the correlation matrix. I was wondering if it is possible to use $\sigma$, $\hat{D}$ and $\hat{Q}$ to compute $Q$, and avoid doing the eigenvalue decomposition of $S$ recombined from $\sigma$, $\hat{D}$ and $\hat{Q}$ ?",,"['linear-algebra', 'data-analysis', 'svd']"
36,Why do isotropic spaces deserve their name?,Why do isotropic spaces deserve their name?,,"Wiki defines a quadratic form to be isotropic if it evaluates to zero at some vector. What does this have to do with isotropy in physics i.e uniformity in all directions? From my experience so far, mathematicians are very good at naming things, so what is the geometric justification for this definition?","Wiki defines a quadratic form to be isotropic if it evaluates to zero at some vector. What does this have to do with isotropy in physics i.e uniformity in all directions? From my experience so far, mathematicians are very good at naming things, so what is the geometric justification for this definition?",,"['linear-algebra', 'abstract-algebra', 'definition', 'intuition', 'quadratic-forms']"
37,A conjecture on Schatten 1-norm,A conjecture on Schatten 1-norm,,"I have a conjecture on Schatten 1-norm. Before presenting the conjecture, let us first  specify the notions used here. A matrix $A$ is said to be a density operator if $A$ is positive semidefinite with $\textrm{tr}(A)=1$. The Schatten 1-norm of a matrix $A$ is $\|A\|_1:=\textrm{tr}\sqrt{A^{*}A}$. My conjecture is: if $\|\rho_1\otimes\sigma_1-\rho_2\otimes\sigma_2\|_1=\|\rho_1-\rho_2\|_1$ and $\rho_1\rho_2\neq0$, i.e., the supports of $\rho_1$ and $\rho_2$ are not orthogonal, then $\sigma_1=\sigma_2$, where $\rho_i$, $\sigma_i$, $i=1,2$ are density operators. I know this conjecture holds for some special cases, but I do not know how to prove it in general. So can anyone help me?","I have a conjecture on Schatten 1-norm. Before presenting the conjecture, let us first  specify the notions used here. A matrix $A$ is said to be a density operator if $A$ is positive semidefinite with $\textrm{tr}(A)=1$. The Schatten 1-norm of a matrix $A$ is $\|A\|_1:=\textrm{tr}\sqrt{A^{*}A}$. My conjecture is: if $\|\rho_1\otimes\sigma_1-\rho_2\otimes\sigma_2\|_1=\|\rho_1-\rho_2\|_1$ and $\rho_1\rho_2\neq0$, i.e., the supports of $\rho_1$ and $\rho_2$ are not orthogonal, then $\sigma_1=\sigma_2$, where $\rho_i$, $\sigma_i$, $i=1,2$ are density operators. I know this conjecture holds for some special cases, but I do not know how to prove it in general. So can anyone help me?",,"['linear-algebra', 'matrices', 'functional-analysis', 'mathematical-physics', 'tensor-products']"
38,Inverse (finite group) isomorphism of a certain form exists,Inverse (finite group) isomorphism of a certain form exists,,"I have been working on something in group theory for a long time and I have one problem that I cannot solve. I have reduced that problem to a conjecture. It takes some work to set it up, but I don't thinks it's that hard to understand. Let $\mathcal G$ be the finite abelian group $\sum_\limits{i=1}^A \mathbb Z/a_i \mathbb Z.$ We represent an element  $\bar{\mathbf x} \in \mathcal G$ as a column vector $\begin{bmatrix}   \overline{x_1} &   \overline{x_2} &   \overline{x_3} &   \cdots &   \overline{x_A} \end{bmatrix}^\text T$ where $\mathbf x = \begin{bmatrix}   x_1 &   x_2 &   x_3 &   \cdots &   x_A \end{bmatrix}^\text T \in \mathbb Z^{A \times 1}$ Define $D_a = \operatorname{diag}(a_1, a_2, a_3, \dots, a_A)$. Then, for any two $\overline{\mathbf x}, \overline{\mathbf y} \in \mathcal G, \,$ $\overline{\mathbf x} = \overline{\mathbf y}$ if and only if there exists a $\mathbf u \in \mathbb Z^{A \times 1}$ such that $\mathbf x = \mathbf y + D_a \mathbf u$. Let $\mathcal H$ be a nontrivial subgroup of $\mathcal G$ which can be expressed as an internal direct sum of cycles,  $$\mathcal H =   \bigoplus_{j=1}^B \langle \, \overline{\mathbf s_j} \, \rangle.$$ For $j=1 .. B,$ let $\operatorname{ord}(\, \overline{\mathbf s_j} \,) = b_j$. Let $S = \begin{bmatrix}   \mathbf s_1 &   \mathbf s_2 &   \mathbf s_3 &   \cdots &   \mathbf s_B \end{bmatrix} \in \mathbb Z^{A \times B}$ be the matrix formed by the column vectors $\{\mathbf s_j\}_{j=1}^B$ Define $D_b = \operatorname{diag}(b_1, b_2, b_3, \dots, b_B)$. Then there exists a matrix $\widetilde S \in \mathbb Z^{A \times B}$ such that  $$ S D_b = D_a \widetilde S $$ This is true because each $b_j$ is the smallest positive integer such that, for every $i$, there exists an integer $\tilde s_{ij}$ such that $b s_{ij} = \tilde s_{ij} a_i \equiv 0 \pmod{a_i}.$ Finally, we get to Conjecture: The matrix $\begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix}$ has an integer left inverse. This is equivalent to saying that the rows of $\widetilde S$ span $\prod_{j=1}^B \mathbb Z_{b_j}$. It is easy to show that the elements in each column of $\begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix}$ are relatively prime. But I can't see how to take the next step and create an integer left inverse. Context: The mapping $f : \prod_{j=1}^B \mathbb Z_{b_j} \to \mathcal H$ defined by $f(\bar{\mathbf x}) = \overline{S \mathbf x}$ is an (additive group) isomorphism. So there must exists an isomorphism $\, f^{-1} : \mathcal H \to \prod_{j=1}^B \mathbb Z_{b_j}$ It would be nice if there were a matrix $T \in \mathbb Z^{B \times A}$ such that  $f^{-1}(\bar{\mathbf y}) = \overline{T \mathbf y}$, but this only happens some times. If my conjecture were true, then we would always have  $T = D_b \widetilde T D_a^{-1} \in \mathbb Q^{B \times A}$ where, for some $U \in \mathbb Z^{B \times B} $,  $$\begin{bmatrix} \widetilde T & U \end{bmatrix} \begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix} = I$$ I tried for a long time to find a counter example with no success, but I also can't prove that $\begin{bmatrix}     \widetilde S \\     -D_b \\  \end{bmatrix}$ always has a left inverse. NOTES: The expression  $ S D_b = D_a \widetilde S $ is just the matrix way of expressing that for $j = 1..B, \operatorname{ord}\mathbf s_j = b_j$ What the function $F:\mathbb Z^B \to \mathcal H$ defined by  $F(\bar{\mathbf x})    = \overline{S x}   = \sum_{j=1}^B x_j \mathbf{\bar s}_j$ is doing is pretty obvious. The problem is that the function is ""too big"" in the sense that any $x_j$ can be replaced by $x_j + nb_j \; (n \in \mathbb Z)$ without altering the image of  $\bar{\mathbf x}.$ That periodic redundancy is encapsulated in the function $f : \prod_{j=1}^B \mathbb Z_{b_j} \to \mathcal H$ defined by $f(\bar{\mathbf x}) = \overline{S \mathbf x}$. Here is how you prove that this function is well-defined. \begin{align}   \bar{\mathbf x} = \bar{\mathbf y}   &\implies x = y + D_b u \; (u \in \mathbb Z^{B \times 1})\\   &\implies Sx = Sy + S D_b u\\   &\implies Sx = Sy + D_a (\widetilde S u)\\   &\implies f(\bar{\mathbf x}) = f(\bar{\mathbf y})\\ \end{align} So $f$ is a well-defined isomorphism. Hence there is also an inverse isomorphism  $f^{-1}:\mathcal H \to \prod_{j=1}^B \mathbb Z_{b_j}$. Let's assume that there is a rational matrix $T \in \mathbb Q^{B \times A}$ such that $f^{-1}(\bar{\mathbf y}) =\overline{Ty}.$ We will show that $f^{-1}$ is well-defined if there exists an integer matrix $\widetilde T \in \mathbb Z^{B \times A}$ such that $T D_a = D_b \widetilde T$. \begin{align}   \bar{\mathbf y} = \bar{\mathbf z}   &\implies y = z + D_a u \; (u \in \mathbb Z^{A \times 1})\\   &\implies Ty = Tz + T D_a u\\   &\implies Ty = Tz + D_b (\widetilde T u)\\   &\implies f^{-1}(\bar{\mathbf y}) = f^{-1}(\bar{\mathbf z})\\ \end{align} We also need to ensure that, for $j = 1..B$, $f^{-1}(f(\bar{\mathbf e}_j)) = f^{-1}(\bar{\mathbf s}_j)   = \bar{\mathbf e}_j.$ This can all be compressed into $f^{-1}(f(\overline{I_B})) = \overline{I_B}$. \begin{align}   f^{-1}(f(\overline{I_B})) &= \overline{I_B} \\   f^{-1}(\overline S) &= \overline{I_B} \\   \overline{TS} &= \overline{I_B} \\   TS &= I_B + D_b U \; (U \in \mathbb Z^{B \times B})\\   (D_b \widetilde T D_a^{-1})(D_a \widetilde S D_b^{-1}) &= I_B + D_b U \\   D_b \widetilde T \widetilde S D_b^{-1} &= I_B + D_b U \\   \widetilde T \widetilde S &= I_B + U D_b \\   \widetilde T \widetilde S - U D_b &= I_B \\   \begin{bmatrix} \widetilde T & U \end{bmatrix}   \begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix} &= I_B \end{align}","I have been working on something in group theory for a long time and I have one problem that I cannot solve. I have reduced that problem to a conjecture. It takes some work to set it up, but I don't thinks it's that hard to understand. Let $\mathcal G$ be the finite abelian group $\sum_\limits{i=1}^A \mathbb Z/a_i \mathbb Z.$ We represent an element  $\bar{\mathbf x} \in \mathcal G$ as a column vector $\begin{bmatrix}   \overline{x_1} &   \overline{x_2} &   \overline{x_3} &   \cdots &   \overline{x_A} \end{bmatrix}^\text T$ where $\mathbf x = \begin{bmatrix}   x_1 &   x_2 &   x_3 &   \cdots &   x_A \end{bmatrix}^\text T \in \mathbb Z^{A \times 1}$ Define $D_a = \operatorname{diag}(a_1, a_2, a_3, \dots, a_A)$. Then, for any two $\overline{\mathbf x}, \overline{\mathbf y} \in \mathcal G, \,$ $\overline{\mathbf x} = \overline{\mathbf y}$ if and only if there exists a $\mathbf u \in \mathbb Z^{A \times 1}$ such that $\mathbf x = \mathbf y + D_a \mathbf u$. Let $\mathcal H$ be a nontrivial subgroup of $\mathcal G$ which can be expressed as an internal direct sum of cycles,  $$\mathcal H =   \bigoplus_{j=1}^B \langle \, \overline{\mathbf s_j} \, \rangle.$$ For $j=1 .. B,$ let $\operatorname{ord}(\, \overline{\mathbf s_j} \,) = b_j$. Let $S = \begin{bmatrix}   \mathbf s_1 &   \mathbf s_2 &   \mathbf s_3 &   \cdots &   \mathbf s_B \end{bmatrix} \in \mathbb Z^{A \times B}$ be the matrix formed by the column vectors $\{\mathbf s_j\}_{j=1}^B$ Define $D_b = \operatorname{diag}(b_1, b_2, b_3, \dots, b_B)$. Then there exists a matrix $\widetilde S \in \mathbb Z^{A \times B}$ such that  $$ S D_b = D_a \widetilde S $$ This is true because each $b_j$ is the smallest positive integer such that, for every $i$, there exists an integer $\tilde s_{ij}$ such that $b s_{ij} = \tilde s_{ij} a_i \equiv 0 \pmod{a_i}.$ Finally, we get to Conjecture: The matrix $\begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix}$ has an integer left inverse. This is equivalent to saying that the rows of $\widetilde S$ span $\prod_{j=1}^B \mathbb Z_{b_j}$. It is easy to show that the elements in each column of $\begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix}$ are relatively prime. But I can't see how to take the next step and create an integer left inverse. Context: The mapping $f : \prod_{j=1}^B \mathbb Z_{b_j} \to \mathcal H$ defined by $f(\bar{\mathbf x}) = \overline{S \mathbf x}$ is an (additive group) isomorphism. So there must exists an isomorphism $\, f^{-1} : \mathcal H \to \prod_{j=1}^B \mathbb Z_{b_j}$ It would be nice if there were a matrix $T \in \mathbb Z^{B \times A}$ such that  $f^{-1}(\bar{\mathbf y}) = \overline{T \mathbf y}$, but this only happens some times. If my conjecture were true, then we would always have  $T = D_b \widetilde T D_a^{-1} \in \mathbb Q^{B \times A}$ where, for some $U \in \mathbb Z^{B \times B} $,  $$\begin{bmatrix} \widetilde T & U \end{bmatrix} \begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix} = I$$ I tried for a long time to find a counter example with no success, but I also can't prove that $\begin{bmatrix}     \widetilde S \\     -D_b \\  \end{bmatrix}$ always has a left inverse. NOTES: The expression  $ S D_b = D_a \widetilde S $ is just the matrix way of expressing that for $j = 1..B, \operatorname{ord}\mathbf s_j = b_j$ What the function $F:\mathbb Z^B \to \mathcal H$ defined by  $F(\bar{\mathbf x})    = \overline{S x}   = \sum_{j=1}^B x_j \mathbf{\bar s}_j$ is doing is pretty obvious. The problem is that the function is ""too big"" in the sense that any $x_j$ can be replaced by $x_j + nb_j \; (n \in \mathbb Z)$ without altering the image of  $\bar{\mathbf x}.$ That periodic redundancy is encapsulated in the function $f : \prod_{j=1}^B \mathbb Z_{b_j} \to \mathcal H$ defined by $f(\bar{\mathbf x}) = \overline{S \mathbf x}$. Here is how you prove that this function is well-defined. \begin{align}   \bar{\mathbf x} = \bar{\mathbf y}   &\implies x = y + D_b u \; (u \in \mathbb Z^{B \times 1})\\   &\implies Sx = Sy + S D_b u\\   &\implies Sx = Sy + D_a (\widetilde S u)\\   &\implies f(\bar{\mathbf x}) = f(\bar{\mathbf y})\\ \end{align} So $f$ is a well-defined isomorphism. Hence there is also an inverse isomorphism  $f^{-1}:\mathcal H \to \prod_{j=1}^B \mathbb Z_{b_j}$. Let's assume that there is a rational matrix $T \in \mathbb Q^{B \times A}$ such that $f^{-1}(\bar{\mathbf y}) =\overline{Ty}.$ We will show that $f^{-1}$ is well-defined if there exists an integer matrix $\widetilde T \in \mathbb Z^{B \times A}$ such that $T D_a = D_b \widetilde T$. \begin{align}   \bar{\mathbf y} = \bar{\mathbf z}   &\implies y = z + D_a u \; (u \in \mathbb Z^{A \times 1})\\   &\implies Ty = Tz + T D_a u\\   &\implies Ty = Tz + D_b (\widetilde T u)\\   &\implies f^{-1}(\bar{\mathbf y}) = f^{-1}(\bar{\mathbf z})\\ \end{align} We also need to ensure that, for $j = 1..B$, $f^{-1}(f(\bar{\mathbf e}_j)) = f^{-1}(\bar{\mathbf s}_j)   = \bar{\mathbf e}_j.$ This can all be compressed into $f^{-1}(f(\overline{I_B})) = \overline{I_B}$. \begin{align}   f^{-1}(f(\overline{I_B})) &= \overline{I_B} \\   f^{-1}(\overline S) &= \overline{I_B} \\   \overline{TS} &= \overline{I_B} \\   TS &= I_B + D_b U \; (U \in \mathbb Z^{B \times B})\\   (D_b \widetilde T D_a^{-1})(D_a \widetilde S D_b^{-1}) &= I_B + D_b U \\   D_b \widetilde T \widetilde S D_b^{-1} &= I_B + D_b U \\   \widetilde T \widetilde S &= I_B + U D_b \\   \widetilde T \widetilde S - U D_b &= I_B \\   \begin{bmatrix} \widetilde T & U \end{bmatrix}   \begin{bmatrix} \widetilde S \\ -D_b \end{bmatrix} &= I_B \end{align}",,"['linear-algebra', 'group-theory']"
39,Computing one-sided inverse of a matrix over some finite field,Computing one-sided inverse of a matrix over some finite field,,"Let $M$ be a $k\times n$ matrix with $k < n$, and assume that $\text{rank}(M)=k$. Over $\mathbb{R}$, one can compute a right inverse of $M$ as follows: $$M_\text{right}^{-1} = M^T(MM^T)^{-1}$$ However, the above relation does not necessarily hold over finite fields. For instance, let $M=\begin{bmatrix}1&2\end{bmatrix}$ be defined over $GF(5)$. Then, $MM^T=\begin{bmatrix}0\end{bmatrix}$, which is not invertible, so $(MM^T)^{-1}$ is not defined over this field. The right inverse can basically be computed by solving a linear system over the finite field. In the example above, let $M_\text{right}^{-1}=\begin{bmatrix}a\\b\end{bmatrix}$. Then: $$M M_\text{right}^{-1} = I \qquad\Rightarrow\qquad a+2b=1 \qquad\Rightarrow\qquad a=1-2b$$ Therefore, we can enumerate all right inverses over $GF(5)$: $\begin{bmatrix}1\\0\end{bmatrix}, \begin{bmatrix}4\\1\end{bmatrix},\begin{bmatrix}2\\2\end{bmatrix},\begin{bmatrix}0\\3\end{bmatrix},\begin{bmatrix}3\\4\end{bmatrix}$ Is there a better way to compute the right inverse of a matrix over finite fields, such as the closed formula $M_\text{right}^{-1} = M^T(MM^T)^{-1}$ over $\mathbb{R}$? and Can we build something similar to the Moore–Penrose pseudoinverse for finite fields? Edit: This paper by John D. Fulton discusses generalized inverses of matrices over a finite field , which seems to be relevant to my second question.","Let $M$ be a $k\times n$ matrix with $k < n$, and assume that $\text{rank}(M)=k$. Over $\mathbb{R}$, one can compute a right inverse of $M$ as follows: $$M_\text{right}^{-1} = M^T(MM^T)^{-1}$$ However, the above relation does not necessarily hold over finite fields. For instance, let $M=\begin{bmatrix}1&2\end{bmatrix}$ be defined over $GF(5)$. Then, $MM^T=\begin{bmatrix}0\end{bmatrix}$, which is not invertible, so $(MM^T)^{-1}$ is not defined over this field. The right inverse can basically be computed by solving a linear system over the finite field. In the example above, let $M_\text{right}^{-1}=\begin{bmatrix}a\\b\end{bmatrix}$. Then: $$M M_\text{right}^{-1} = I \qquad\Rightarrow\qquad a+2b=1 \qquad\Rightarrow\qquad a=1-2b$$ Therefore, we can enumerate all right inverses over $GF(5)$: $\begin{bmatrix}1\\0\end{bmatrix}, \begin{bmatrix}4\\1\end{bmatrix},\begin{bmatrix}2\\2\end{bmatrix},\begin{bmatrix}0\\3\end{bmatrix},\begin{bmatrix}3\\4\end{bmatrix}$ Is there a better way to compute the right inverse of a matrix over finite fields, such as the closed formula $M_\text{right}^{-1} = M^T(MM^T)^{-1}$ over $\mathbb{R}$? and Can we build something similar to the Moore–Penrose pseudoinverse for finite fields? Edit: This paper by John D. Fulton discusses generalized inverses of matrices over a finite field , which seems to be relevant to my second question.",,"['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
40,Galois' theory: fixed subfield formula.,Galois' theory: fixed subfield formula.,,"In a homework dealing with Galois' theory, I am asked to prove the following standard statement, known as the fixed subfield formula: Theorem. Let $L$ be a field and $G$ be a finite subgroup of $\textrm{Aut}(L)$, then: $$\left[L:L^G\right]=|G|\textrm{ and }G=\textrm{Aut}_{L^G}(L).$$ However, I am bound to use very specific tools that I shall introduce to you. First, let me give you a: Definition 1. Let $L$ be a field, $G$ be a finite subgroup of $\textrm{Aut}(L)$ and $V$ be a $L$-vector space. One says that a map $\cdot:G\times V\mapsto V$ is a $L^G$-structure of $V$ if and only if: (i) $\cdot:G\times V\rightarrow V$ is a group action of $G$ on $V$. (ii) For all $\sigma\in G$,$V\ni v\mapsto\sigma\cdot v\in V$ is a $L^G$-linear map. (iii) For all $(\lambda,v)\in L\times V$, $\sigma\cdot\lambda v=\sigma(\lambda)(\sigma\cdot v)$. From there, I have shown the: Proposition 1. Let $L$ be a field, $G$ be a finite subgroup of $\textrm{Aut}(L)$ and $V$ be a $L$-vector space. (i) $V^G$ is a $L^G$-subvector space of $V$. (ii) Let $(v_i)_{i\in\{1,\ldots,n\}}\in(V^G)^n$ be $L^G$-linearly independent in $V^G$, then $(v_i)_{i\in\{1,\ldots,n\}}$ is $L$-linearly independent in $V$. Besides, I have also derived the two following constructions: Proposition 2. Let $L$ be a field and $G$ be a finite subgroup of $\textrm{Aut}(L)$. (i) Let $V$ be a finite-dimensional vector space over $L$ and $(v_i)_{i\in I}$ a $L$-basis of $V$, then $V$ is equipped with a $L^G$-structure given by:   $$\left\{\begin{array}{ccc}G\times V & \rightarrow & V\\\displaystyle\left(\sigma,\sum_{i\in I}\lambda_iv_i\right) & \mapsto & \displaystyle\sum_{i\in I}\sigma(\lambda_i)v_i\end{array}\right..$$ (ii) The $L$-vector space $\textrm{Map}(G,L)$ of the maps from $G$ to $L$ is equipped with a $L^G$-structure given by: $$\left\{\begin{array}{ccc}G\times\textrm{Map}(G,L) & \rightarrow & \textrm{Map}(G,L)\\(\sigma,\Phi) & \mapsto & G\ni\tau\mapsto\sigma(\Phi(\sigma^{-1}\circ\tau))\end{array}\right..$$ Thus far, here my: Theorem's proof: Let $m:=\left[L:L^G\right]$, $n:=|G|$ and assume by contradiction that $m<n$. Let $(x_i)_{i\in\{1,\ldots,m\}}\in L^m$ be a $L^G$-basis of $L$ and $\sigma_1,\ldots,\sigma_n$ be the distinct elements of $G$. Since $m<n$ there exists $(y_i)_{i\in\{1,\ldots,n\}}\in L^n$ a nonzero solution to the following homogeneous system: $$\forall i\in\{1,\ldots,m\},\sum_{i=1}^n\sigma_j(x_i)Y_i=0_L.$$ Let $x\in L$, there exists $(\lambda_i)_{i\in\{1,\ldots,m\}}\in(L^G)^m$ such that: $$x=\sum_{i=1}^m\lambda_ix_i.$$ One derives from the construction of $(y_i)_{i\in\{1,\ldots,n\}}$: $$\sum_{i=1}^n\sigma(x)y_i=\sum_{i=1}^m\lambda_i\sum_{j=1}^n\sigma_i(x_j)y_i=0_L.$$ Therefore, one has: $$\sum_{i=1}^n\sigma y_i=0_{\textrm{Map}(G,L)}.$$ Since $(y_i)_{i\in\{1,\ldots,n\}}$ is nonzero and $\sigma_1,\ldots,\sigma_n$ are pairwise distinct, here a contradiction to Dedekind's lemma. Therefore, one has $m\geqslant n$. $\Box$ I struggle to prove that $m\leqslant n$, I tried to use the $L^G$-structure on $\textrm{Map}(L,G)$ from proposition $2.$ (ii) along with the proposition $1.$ (ii) but I didn't manage to succeed. Any hints will be appreciated.","In a homework dealing with Galois' theory, I am asked to prove the following standard statement, known as the fixed subfield formula: Theorem. Let $L$ be a field and $G$ be a finite subgroup of $\textrm{Aut}(L)$, then: $$\left[L:L^G\right]=|G|\textrm{ and }G=\textrm{Aut}_{L^G}(L).$$ However, I am bound to use very specific tools that I shall introduce to you. First, let me give you a: Definition 1. Let $L$ be a field, $G$ be a finite subgroup of $\textrm{Aut}(L)$ and $V$ be a $L$-vector space. One says that a map $\cdot:G\times V\mapsto V$ is a $L^G$-structure of $V$ if and only if: (i) $\cdot:G\times V\rightarrow V$ is a group action of $G$ on $V$. (ii) For all $\sigma\in G$,$V\ni v\mapsto\sigma\cdot v\in V$ is a $L^G$-linear map. (iii) For all $(\lambda,v)\in L\times V$, $\sigma\cdot\lambda v=\sigma(\lambda)(\sigma\cdot v)$. From there, I have shown the: Proposition 1. Let $L$ be a field, $G$ be a finite subgroup of $\textrm{Aut}(L)$ and $V$ be a $L$-vector space. (i) $V^G$ is a $L^G$-subvector space of $V$. (ii) Let $(v_i)_{i\in\{1,\ldots,n\}}\in(V^G)^n$ be $L^G$-linearly independent in $V^G$, then $(v_i)_{i\in\{1,\ldots,n\}}$ is $L$-linearly independent in $V$. Besides, I have also derived the two following constructions: Proposition 2. Let $L$ be a field and $G$ be a finite subgroup of $\textrm{Aut}(L)$. (i) Let $V$ be a finite-dimensional vector space over $L$ and $(v_i)_{i\in I}$ a $L$-basis of $V$, then $V$ is equipped with a $L^G$-structure given by:   $$\left\{\begin{array}{ccc}G\times V & \rightarrow & V\\\displaystyle\left(\sigma,\sum_{i\in I}\lambda_iv_i\right) & \mapsto & \displaystyle\sum_{i\in I}\sigma(\lambda_i)v_i\end{array}\right..$$ (ii) The $L$-vector space $\textrm{Map}(G,L)$ of the maps from $G$ to $L$ is equipped with a $L^G$-structure given by: $$\left\{\begin{array}{ccc}G\times\textrm{Map}(G,L) & \rightarrow & \textrm{Map}(G,L)\\(\sigma,\Phi) & \mapsto & G\ni\tau\mapsto\sigma(\Phi(\sigma^{-1}\circ\tau))\end{array}\right..$$ Thus far, here my: Theorem's proof: Let $m:=\left[L:L^G\right]$, $n:=|G|$ and assume by contradiction that $m<n$. Let $(x_i)_{i\in\{1,\ldots,m\}}\in L^m$ be a $L^G$-basis of $L$ and $\sigma_1,\ldots,\sigma_n$ be the distinct elements of $G$. Since $m<n$ there exists $(y_i)_{i\in\{1,\ldots,n\}}\in L^n$ a nonzero solution to the following homogeneous system: $$\forall i\in\{1,\ldots,m\},\sum_{i=1}^n\sigma_j(x_i)Y_i=0_L.$$ Let $x\in L$, there exists $(\lambda_i)_{i\in\{1,\ldots,m\}}\in(L^G)^m$ such that: $$x=\sum_{i=1}^m\lambda_ix_i.$$ One derives from the construction of $(y_i)_{i\in\{1,\ldots,n\}}$: $$\sum_{i=1}^n\sigma(x)y_i=\sum_{i=1}^m\lambda_i\sum_{j=1}^n\sigma_i(x_j)y_i=0_L.$$ Therefore, one has: $$\sum_{i=1}^n\sigma y_i=0_{\textrm{Map}(G,L)}.$$ Since $(y_i)_{i\in\{1,\ldots,n\}}$ is nonzero and $\sigma_1,\ldots,\sigma_n$ are pairwise distinct, here a contradiction to Dedekind's lemma. Therefore, one has $m\geqslant n$. $\Box$ I struggle to prove that $m\leqslant n$, I tried to use the $L^G$-structure on $\textrm{Map}(L,G)$ from proposition $2.$ (ii) along with the proposition $1.$ (ii) but I didn't manage to succeed. Any hints will be appreciated.",,"['linear-algebra', 'galois-theory', 'extension-field']"
41,Measure of how far a basis is from orthogonality,Measure of how far a basis is from orthogonality,,"here's a problem that is a bit vaguely defined. Let the $m\times m$ matrix $U$ have columns $U_i$ that form a basis in $R^m$. Assume these columns have unit length, $|U_i|=1$. The matrix $R = U^\top U$ then contains scalar products, $R_{ij} = U_i^\top U_j$. Obviously, $R$ is symmetric and positive semi-definite (we assume definite). The question is: is there a standard measure of ""how far"" this basis is from being orthogonal? The transformation $V = U R^{-1/2}$ constructs an orthonormal basis $V$ (since $V^\top V = I$; $R^{-1/2}$ is the inverse of the symmetric square root of $R$ -- could also be a Cholesky factor). Thus one may suspect that a ""standard measure of distance from orthogonality"" should use in $R$ in some way... The condition number of $R$ maybe? Any pointers appreciated! /Tobias","here's a problem that is a bit vaguely defined. Let the $m\times m$ matrix $U$ have columns $U_i$ that form a basis in $R^m$. Assume these columns have unit length, $|U_i|=1$. The matrix $R = U^\top U$ then contains scalar products, $R_{ij} = U_i^\top U_j$. Obviously, $R$ is symmetric and positive semi-definite (we assume definite). The question is: is there a standard measure of ""how far"" this basis is from being orthogonal? The transformation $V = U R^{-1/2}$ constructs an orthonormal basis $V$ (since $V^\top V = I$; $R^{-1/2}$ is the inverse of the symmetric square root of $R$ -- could also be a Cholesky factor). Thus one may suspect that a ""standard measure of distance from orthogonality"" should use in $R$ in some way... The condition number of $R$ maybe? Any pointers appreciated! /Tobias",,"['linear-algebra', 'orthogonality']"
42,Solving a matrix differential equation,Solving a matrix differential equation,,I am trying to solve: $\frac{d U_t}{dt} = Tr(G^{\dagger}U_t)G - Tr(U_t^{\dagger}G)U_t G^{\dagger} U_t$ Where $U_t \in SU(4)$ and $G \in SU(4)$ is given and constant. Is it possible to solve this equation? A solution not in coordinates would be most helpful. I've tried several series methods previously.,I am trying to solve: $\frac{d U_t}{dt} = Tr(G^{\dagger}U_t)G - Tr(U_t^{\dagger}G)U_t G^{\dagger} U_t$ Where $U_t \in SU(4)$ and $G \in SU(4)$ is given and constant. Is it possible to solve this equation? A solution not in coordinates would be most helpful. I've tried several series methods previously.,,"['linear-algebra', 'ordinary-differential-equations', 'lie-groups', 'lie-algebras']"
43,The relationship between the eigenvalues of matrices $XY$ and $YX$,The relationship between the eigenvalues of matrices  and,XY YX,"If $X \in \mathbb{C}^{m \times n}$ and $Y \in \mathbb{C}^{n \times m}$ ( $m \geq n$ ), how to prove the following? $$\sigma (XY) = \sigma(YX) \cup \underbrace{\left \{ 0, ..., 0 \right \}}_{m-n}$$ Here, $\sigma$ denotes the set of eigenvalues/spectrum.","If and ( ), how to prove the following? Here, denotes the set of eigenvalues/spectrum.","X \in \mathbb{C}^{m \times n} Y \in \mathbb{C}^{n \times m} m \geq n \sigma (XY) = \sigma(YX) \cup \underbrace{\left \{ 0, ..., 0 \right \}}_{m-n} \sigma","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
44,Orthogonal basis for infinite-dimensional vector spaces,Orthogonal basis for infinite-dimensional vector spaces,,"It is quite simple to show that all finite-dimensional vector spaces with inner product have an orthogonal basis, with the standard definition of a basis from linear algebra. However, I am in trouble to find any reference about infinite-dimensional case. Do all infinite-dimensional vector space with inner product have an orthogonal basis (again with the standard definition of a basis from linear algebra)?","It is quite simple to show that all finite-dimensional vector spaces with inner product have an orthogonal basis, with the standard definition of a basis from linear algebra. However, I am in trouble to find any reference about infinite-dimensional case. Do all infinite-dimensional vector space with inner product have an orthogonal basis (again with the standard definition of a basis from linear algebra)?",,['linear-algebra']
45,Product of reduced row-echelon matrices is also reduced row-echelon,Product of reduced row-echelon matrices is also reduced row-echelon,,"Show that the product of two reduced row-echelon matrices is also   reduced row-echelon. That's what I think: A reduced row-echelon matrix has columns like $e_1 =(1, 0, \cdots , 0)^T$ and $e_2 =(0, 1, 0, \cdots , 0)^T$. For columns in between $e_n$ and $e_{n+1}$, only the first $n$ entries will be non-zero. By noticing these two, I can 'imagine' that the product should be reduced row-echelon. But I cannot write down a clear proof for that. Or say, I don't even know how to start my proof. Can someone give me a helping hand?","Show that the product of two reduced row-echelon matrices is also   reduced row-echelon. That's what I think: A reduced row-echelon matrix has columns like $e_1 =(1, 0, \cdots , 0)^T$ and $e_2 =(0, 1, 0, \cdots , 0)^T$. For columns in between $e_n$ and $e_{n+1}$, only the first $n$ entries will be non-zero. By noticing these two, I can 'imagine' that the product should be reduced row-echelon. But I cannot write down a clear proof for that. Or say, I don't even know how to start my proof. Can someone give me a helping hand?",,['linear-algebra']
46,Symmetric kernel of tensor product,Symmetric kernel of tensor product,,"Let $V,W$ be two real vector spaces, and let $L_i:V\rightarrow W$, $i=1,\ldots,n$ be $n$ linear maps with distinct kernels $K_i$ of dimension $1$. Consider the tensor product of these maps $L_1\otimes \ldots \otimes L_n: V\otimes \ldots \otimes V \rightarrow W \otimes \ldots \otimes W$. Let K denote the kernel of this map, and let $S^n(V)$ be the space of symmetric tensors of order n. How can I prove that $K \cap S^n(V) = Span\{K_i \otimes  \ldots \otimes K_i , i=1,...n\}$? Thanks!","Let $V,W$ be two real vector spaces, and let $L_i:V\rightarrow W$, $i=1,\ldots,n$ be $n$ linear maps with distinct kernels $K_i$ of dimension $1$. Consider the tensor product of these maps $L_1\otimes \ldots \otimes L_n: V\otimes \ldots \otimes V \rightarrow W \otimes \ldots \otimes W$. Let K denote the kernel of this map, and let $S^n(V)$ be the space of symmetric tensors of order n. How can I prove that $K \cap S^n(V) = Span\{K_i \otimes  \ldots \otimes K_i , i=1,...n\}$? Thanks!",,"['linear-algebra', 'vector-spaces', 'representation-theory', 'tensor-products', 'multilinear-algebra']"
47,Maps that preserve tensor rank,Maps that preserve tensor rank,,"Suppose we have some tensor product of vector spaces.  By tensor rank, I mean the minimal number of simple tensors required to write down an element of this tensor product of spaces.  Is there much known about maps that preserve this tensor rank? Also an easier (more specific) question: suppose I'm working in $U(4)$.  Might there exist a $U \in U(4)$ such that for any pair of matrices $A,B \in U(2)$, $U(A \otimes B)U^{-1}$ is another simple tensor in $U(4)$, without $U$ itself being the tensor product of two matrices? Morally I feel like this wouldn't happen and that the set of maps that preserve simple tensors in this way are themselves simple tensors, but I'm unsure. Thanks!","Suppose we have some tensor product of vector spaces.  By tensor rank, I mean the minimal number of simple tensors required to write down an element of this tensor product of spaces.  Is there much known about maps that preserve this tensor rank? Also an easier (more specific) question: suppose I'm working in $U(4)$.  Might there exist a $U \in U(4)$ such that for any pair of matrices $A,B \in U(2)$, $U(A \otimes B)U^{-1}$ is another simple tensor in $U(4)$, without $U$ itself being the tensor product of two matrices? Morally I feel like this wouldn't happen and that the set of maps that preserve simple tensors in this way are themselves simple tensors, but I'm unsure. Thanks!",,"['linear-algebra', 'lie-groups', 'tensor-products', 'tensor-rank']"
48,Eigenvalues of adjugate matrix of a singular matrix,Eigenvalues of adjugate matrix of a singular matrix,,"Given a singular matrix $A$, find the eigenvalues of the adjugate matrix of $A$. The same question with $A$ being invertible is trivial since $A\operatorname{adj}A=(\operatorname{adj}A)A=(\det A) I$. If $\operatorname{rank}A\leq n-2$, it is well-known that $\operatorname{adj}A=0$ and $0$ is the only eigenvalue. It remains to deal with the case $\operatorname{rank}A= n-1$. It is easy to check that $\operatorname{rank}\operatorname{adj} A=1$. Hence $0$ is an eigenvalue of $\operatorname{adj} A$ with multiplicity at least $n-1$. There's at most one other eigenvalue, say $\lambda$. How can I find $\lambda$ ?","Given a singular matrix $A$, find the eigenvalues of the adjugate matrix of $A$. The same question with $A$ being invertible is trivial since $A\operatorname{adj}A=(\operatorname{adj}A)A=(\det A) I$. If $\operatorname{rank}A\leq n-2$, it is well-known that $\operatorname{adj}A=0$ and $0$ is the only eigenvalue. It remains to deal with the case $\operatorname{rank}A= n-1$. It is easy to check that $\operatorname{rank}\operatorname{adj} A=1$. Hence $0$ is an eigenvalue of $\operatorname{adj} A$ with multiplicity at least $n-1$. There's at most one other eigenvalue, say $\lambda$. How can I find $\lambda$ ?",,"['linear-algebra', 'matrices']"
49,Eigenvectors of a complex matrix,Eigenvectors of a complex matrix,,"Given the following matrix $\begin{pmatrix} 0 & 1-i & 0\\ 1+i & 0 &1-i\\ 0& 1+i &0\\ \end{pmatrix}$ I have found the Eigenvalues $0, 2,-2$. But I have no idea how to calculate the corresponding Eigenvectors and I failed with Gaussian method. What could you recommend? Thanks in advance!","Given the following matrix $\begin{pmatrix} 0 & 1-i & 0\\ 1+i & 0 &1-i\\ 0& 1+i &0\\ \end{pmatrix}$ I have found the Eigenvalues $0, 2,-2$. But I have no idea how to calculate the corresponding Eigenvectors and I failed with Gaussian method. What could you recommend? Thanks in advance!",,[]
50,What do the eigenvalues/vectors of a metric describe?,What do the eigenvalues/vectors of a metric describe?,,"Given a finite metric space $(X = \{ x_i \}_{i=1}^n,d)$, one can form the matrix $A$ of pairwise distances $a_{ij} = d(x_i, x_j)$. What does the eigenspectrum of this matrix say about the metric $d$? Considering the success of spectral methods for analyzing various matrices formed from graphs, this seems like a natural thing to do. If it is a thing, what is it called and where should I look for more information?","Given a finite metric space $(X = \{ x_i \}_{i=1}^n,d)$, one can form the matrix $A$ of pairwise distances $a_{ij} = d(x_i, x_j)$. What does the eigenspectrum of this matrix say about the metric $d$? Considering the success of spectral methods for analyzing various matrices formed from graphs, this seems like a natural thing to do. If it is a thing, what is it called and where should I look for more information?",,"['linear-algebra', 'matrices', 'metric-spaces', 'eigenvalues-eigenvectors', 'spectral-graph-theory']"
51,Can these characterisations of finite dimensionality be proven equivalent without using a basis?,Can these characterisations of finite dimensionality be proven equivalent without using a basis?,,"I was wondering about how to define ""finite dimensional"" without talking about bases. Two possibilities occurred to me: Say $V$ is finite dimensional if the canonical inclusion $V\hookrightarrow V^{**}$ has an inverse. Say $V$ is finite dimensional if the canonical inclusion $V\otimes V^*\hookrightarrow \text{End}(V)$ has an inverse. It's easy enough to verify that these are indeed both equivalent to Say $V$ is finite dimensional if it has a finite basis. Question: Can we prove $(1)\Rightarrow (2)$ or $(2)\Rightarrow (1)$ without going via $(3)$?","I was wondering about how to define ""finite dimensional"" without talking about bases. Two possibilities occurred to me: Say $V$ is finite dimensional if the canonical inclusion $V\hookrightarrow V^{**}$ has an inverse. Say $V$ is finite dimensional if the canonical inclusion $V\otimes V^*\hookrightarrow \text{End}(V)$ has an inverse. It's easy enough to verify that these are indeed both equivalent to Say $V$ is finite dimensional if it has a finite basis. Question: Can we prove $(1)\Rightarrow (2)$ or $(2)\Rightarrow (1)$ without going via $(3)$?",,"['linear-algebra', 'vector-spaces', 'dual-spaces']"
52,Does the operation of addition on the subspaces of V have an additive identity? Which subspaces have additive inverses?,Does the operation of addition on the subspaces of V have an additive identity? Which subspaces have additive inverses?,,"I was reading Linear Algebra Done Right. I came across the following question (Ch-1, Q12), for which I have solution , but I am having little confusion regarding it: Q12. (a) Does the operation of addition on the subspaces of V have an additive identity? Sol. The subspace {${0}$} is an additive identity for the operation of addition on the subspaces of V. More precisely, if U is a subspace of V, then U + {${0}$} = {${0}$} + U = U. This is quite obvious as subspaces are itself vector spaces and so must have {${0}$}, or even their addition, whether uniquely expressed or not. If not expressed uniquely then it won't be a direct product but will still be a subspace (Am I right here about subspace?) Q12. (b) Which subspaces have additive inverses? Sol. For a subspace U of V to have an additive inverse, there would have to be another subspace W of V such that U + W = {${0}$}. Because both U and W are contained in U + W, this is possible only if U = W = {${0}$}. Thus {${0}$} is the only subspace of V that has an additive inverse. What I don't get here is that since a subspace is a vector space itself, why do  we need to consider another subspace W for U to have additive inverse? Can't U contain its own element's additive inverse?","I was reading Linear Algebra Done Right. I came across the following question (Ch-1, Q12), for which I have solution , but I am having little confusion regarding it: Q12. (a) Does the operation of addition on the subspaces of V have an additive identity? Sol. The subspace {${0}$} is an additive identity for the operation of addition on the subspaces of V. More precisely, if U is a subspace of V, then U + {${0}$} = {${0}$} + U = U. This is quite obvious as subspaces are itself vector spaces and so must have {${0}$}, or even their addition, whether uniquely expressed or not. If not expressed uniquely then it won't be a direct product but will still be a subspace (Am I right here about subspace?) Q12. (b) Which subspaces have additive inverses? Sol. For a subspace U of V to have an additive inverse, there would have to be another subspace W of V such that U + W = {${0}$}. Because both U and W are contained in U + W, this is possible only if U = W = {${0}$}. Thus {${0}$} is the only subspace of V that has an additive inverse. What I don't get here is that since a subspace is a vector space itself, why do  we need to consider another subspace W for U to have additive inverse? Can't U contain its own element's additive inverse?",,"['linear-algebra', 'vector-spaces']"
53,"Proof of the conjecture that the kernel is of dimension 2, extended","Proof of the conjecture that the kernel is of dimension 2, extended",,"Pursuing my research, I am now looking for a proof of an extension of the problem proposed here and answered. It's an extension in the sense that I'm now considering two different $t_1$ and $t_2$. The ""conjecture"" still stands though. ""Experimentally"", I found that the kernel (null space) of the following matrix is of dimension 2. I'd like to prove it, but haven't managed yet: \begin{equation} \text{for almost all } t_1>0,\quad \text{dim}\,\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t_1)-\mathbf{Q}_1(t_2)^{-1}\mathbf{Q}_2\right)\overbrace{=}^?\;2  \end{equation} where $t_2$ is chosen such that $$\text{det}\left(\mathbf{Q}_2\mathbf{Q}_1(t_1)-\mathbf{Q}_1(t_2)^{-1}\mathbf{Q}_2\right)=0$$ We assume that such a $t_2$ exist. where: $\mathbf{Q}_2$ is the following matrix: \begin{equation} \mathbf{Q}_2=\begin{bmatrix} \mathbf{I}_n & \mathbf{0}_n \\  \mathbf{0}_n & \mathbf{P}^{-1}\begin{bmatrix}1 & && \\ & \ddots && \\ & & 1& \\ &&& -1 \end{bmatrix}\mathbf{P}  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where $\mathbf{P}\in\mathbb{R}^{n\times n}$ is any invertible matrix. $\mathbf{Q}_1(t)$ is defined by: \begin{equation} \forall t>0,\quad\mathbf{Q}_1(t)=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & \boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  -\boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} and: \begin{equation} \boldsymbol\Omega=\begin{bmatrix} \omega_1 & & \\  & \ddots & \\  & & \omega_n  \end{bmatrix}\in\mathbb{R}^{n\times n},\quad \forall i\in\lbrace 1,\dots, n\rbrace, \omega_i>0 \end{equation} and the four blocks are diagonal, for example: \begin{equation} \mathbf{cos}(\boldsymbol\Omega t)=\begin{bmatrix} \cos(\omega_1t) & & \\  & \ddots & \\  & & \cos(\omega_n t)  \end{bmatrix}\in\mathbb{R}^{n\times n} \end{equation} Interesting properties of $\mathbf{Q}_1$ and $\mathbf{Q}_2$ : Obviously, $\mathbf{Q}_2$ is invertible and $\mathbf{Q}_2=\mathbf{Q}_2^{-1}$. Also, $\det(\mathbf{Q}_1)=1$ ($\omega_i>0$ and for proper $t>0$) and: \begin{equation} \mathbf{Q}_1(t)^{-1}=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & -\boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  \boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix} \end{equation} Any clues would be greatly appreciated. Why I'm not able to extend loup blanc's solution to the present problem with two different $t_1$ and $t_2$. Let's define $A:=\mathbf{Q}_2\mathbf{Q}_1(t_1)-\mathbf{Q}_1(t_2)^{-1}\mathbf{Q}_2$ and $\phi(x):=\det(A-xI)$. The problem is to prove that $0$ is a zero of $\phi$ with multiplicity 2. By definition of $t_2$, $\phi(0)=\det(A)=0$. So I now have to prove that $\phi'(0)=0$ (and then than $\dim(\ker(A))\leqslant 2$). $\phi'(0)=-\operatorname{tr}(\operatorname{adjoint}(A))$ from Jacobi's formula. The calculation of $A$ gives: $$A=\begin{bmatrix} c_1-c_2 & \Omega^{-1}s_1+\Omega^{-1}s_2 P^{-1}DP \\ -P^{-1}DP\Omega s_1-\Omega s_2 & P^{-1}DPc_1-c_2P^{-1}DP\end{bmatrix}$$ where $c_i=\textbf{cos}(\Omega t_i)$, $s_i=\textbf{sin}(\Omega t_i)$ and $D=\operatorname{diag}(1,\dots,1,-1)$. This time, the determinant of $A$ is harder to calculate because the upper-left block is non-zero. So I'm stuck with proving that $\operatorname{tr}(\operatorname{adjoint}(A))=0$. I also tried to apply things such as $A\operatorname{adjoint}(A)=\det(A)I$ but there is no mystery: I have to use the properties of $A$, and that's what I'm not managing to do. EDIT I still did not manage to prove that for $t_1,t_2$ such that $\det(Q_2Q_1(t_2)Q_2Q_1(t_1)-I_{2n})=0$,  $\dim(\ker(Q_2Q_1(t_2)Q_2Q_1(t_1)-I_{2n})=2$. I though of proving it by induction but I am still stuck with the inductive step. Do you think this is a good idea? Would you have some relevant references to help me?","Pursuing my research, I am now looking for a proof of an extension of the problem proposed here and answered. It's an extension in the sense that I'm now considering two different $t_1$ and $t_2$. The ""conjecture"" still stands though. ""Experimentally"", I found that the kernel (null space) of the following matrix is of dimension 2. I'd like to prove it, but haven't managed yet: \begin{equation} \text{for almost all } t_1>0,\quad \text{dim}\,\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t_1)-\mathbf{Q}_1(t_2)^{-1}\mathbf{Q}_2\right)\overbrace{=}^?\;2  \end{equation} where $t_2$ is chosen such that $$\text{det}\left(\mathbf{Q}_2\mathbf{Q}_1(t_1)-\mathbf{Q}_1(t_2)^{-1}\mathbf{Q}_2\right)=0$$ We assume that such a $t_2$ exist. where: $\mathbf{Q}_2$ is the following matrix: \begin{equation} \mathbf{Q}_2=\begin{bmatrix} \mathbf{I}_n & \mathbf{0}_n \\  \mathbf{0}_n & \mathbf{P}^{-1}\begin{bmatrix}1 & && \\ & \ddots && \\ & & 1& \\ &&& -1 \end{bmatrix}\mathbf{P}  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where $\mathbf{P}\in\mathbb{R}^{n\times n}$ is any invertible matrix. $\mathbf{Q}_1(t)$ is defined by: \begin{equation} \forall t>0,\quad\mathbf{Q}_1(t)=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & \boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  -\boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} and: \begin{equation} \boldsymbol\Omega=\begin{bmatrix} \omega_1 & & \\  & \ddots & \\  & & \omega_n  \end{bmatrix}\in\mathbb{R}^{n\times n},\quad \forall i\in\lbrace 1,\dots, n\rbrace, \omega_i>0 \end{equation} and the four blocks are diagonal, for example: \begin{equation} \mathbf{cos}(\boldsymbol\Omega t)=\begin{bmatrix} \cos(\omega_1t) & & \\  & \ddots & \\  & & \cos(\omega_n t)  \end{bmatrix}\in\mathbb{R}^{n\times n} \end{equation} Interesting properties of $\mathbf{Q}_1$ and $\mathbf{Q}_2$ : Obviously, $\mathbf{Q}_2$ is invertible and $\mathbf{Q}_2=\mathbf{Q}_2^{-1}$. Also, $\det(\mathbf{Q}_1)=1$ ($\omega_i>0$ and for proper $t>0$) and: \begin{equation} \mathbf{Q}_1(t)^{-1}=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & -\boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  \boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix} \end{equation} Any clues would be greatly appreciated. Why I'm not able to extend loup blanc's solution to the present problem with two different $t_1$ and $t_2$. Let's define $A:=\mathbf{Q}_2\mathbf{Q}_1(t_1)-\mathbf{Q}_1(t_2)^{-1}\mathbf{Q}_2$ and $\phi(x):=\det(A-xI)$. The problem is to prove that $0$ is a zero of $\phi$ with multiplicity 2. By definition of $t_2$, $\phi(0)=\det(A)=0$. So I now have to prove that $\phi'(0)=0$ (and then than $\dim(\ker(A))\leqslant 2$). $\phi'(0)=-\operatorname{tr}(\operatorname{adjoint}(A))$ from Jacobi's formula. The calculation of $A$ gives: $$A=\begin{bmatrix} c_1-c_2 & \Omega^{-1}s_1+\Omega^{-1}s_2 P^{-1}DP \\ -P^{-1}DP\Omega s_1-\Omega s_2 & P^{-1}DPc_1-c_2P^{-1}DP\end{bmatrix}$$ where $c_i=\textbf{cos}(\Omega t_i)$, $s_i=\textbf{sin}(\Omega t_i)$ and $D=\operatorname{diag}(1,\dots,1,-1)$. This time, the determinant of $A$ is harder to calculate because the upper-left block is non-zero. So I'm stuck with proving that $\operatorname{tr}(\operatorname{adjoint}(A))=0$. I also tried to apply things such as $A\operatorname{adjoint}(A)=\det(A)I$ but there is no mystery: I have to use the properties of $A$, and that's what I'm not managing to do. EDIT I still did not manage to prove that for $t_1,t_2$ such that $\det(Q_2Q_1(t_2)Q_2Q_1(t_1)-I_{2n})=0$,  $\dim(\ker(Q_2Q_1(t_2)Q_2Q_1(t_1)-I_{2n})=2$. I though of proving it by induction but I am still stuck with the inductive step. Do you think this is a good idea? Would you have some relevant references to help me?",,"['linear-algebra', 'matrices', 'determinant']"
54,Alternative introduction to tensor products of vector spaces,Alternative introduction to tensor products of vector spaces,,"One of the main obstacles in understanding the tensor product is that, unlike many other algebraic structures, you cannot really get hold of its element structure. This confuses many beginners. The universal property tells us that this doesn't really matter and it helps us to construct linear maps on the tensor product, but it takes a while to appreciate this level of abstraction. So what about constructing the tensor product in a different way? Here is a suggestion. I would like to hear what you think about it. Let $V,W$ be vector spaces over a field $K$. Let us assume that $V$ is finite-dimensional. Define the tensor product $V \otimes W := \hom(V^*,W)$. This is a vector space by construction. Its elements are just linear maps $V^* \to W$. If $v \in V$, $w \in W$, we define the pure tensor $v \otimes w \in V \otimes W$ by $(v \otimes w)(\omega)=\omega(v) \cdot w$. Observe that $(v+v') \otimes w=v \otimes w + v' \otimes w$ and $(\lambda v) \otimes w = \lambda(v \otimes w)$, likewise for the other variable, i.e. $V \times W \to V \otimes W$ is a bilinear map. Any element $\alpha \in V^* \otimes W$ is a sum of pure tensors: If $e_1,\dotsc,e_n$ is a basis of $V$, then $\alpha = \sum_i e_i \otimes \alpha(e_i^*)$. (Because the right hand side maps $e_i^*$ to $\alpha(e_i^*)$.) Universal property: If $b : V \times W \to U$ is a bilinear map, there is a unique linear map $f : V \otimes W \to U$ such that $f(v \otimes w)=b(v,w)$ for all $(v,w) \in V \times W$. Proof. Uniqueness follows since every element is a sum of pure tensors. For existence, we just let $f(\alpha)=\sum_i b(e_i,\alpha(e_i))$. Then one easily checks $f(e_j \otimes w)=b(e_j,w)$ and therefore $f(v \otimes w)=b(v,w)$. $\checkmark$ The universal property shows in particular that the tensor product of finite-dimensional vector spaces is symmetric: There is a unique isomorphism $V \otimes W \cong W \otimes V$ mapping $v \otimes w \mapsto w \otimes v$. [Of course, lots of other nice properties of the tensor product also follow from the universal property.] If $V$ is not assumed to be finite-dimensional, we define $V \otimes W$ as the subspace of $\hom(V^*,W)$ which is generated by the pure tensors $v \otimes w$ as defined above. (Thus, if $V$ is finite-dimensional, there will be no difference.) The universal property follows immediately.","One of the main obstacles in understanding the tensor product is that, unlike many other algebraic structures, you cannot really get hold of its element structure. This confuses many beginners. The universal property tells us that this doesn't really matter and it helps us to construct linear maps on the tensor product, but it takes a while to appreciate this level of abstraction. So what about constructing the tensor product in a different way? Here is a suggestion. I would like to hear what you think about it. Let $V,W$ be vector spaces over a field $K$. Let us assume that $V$ is finite-dimensional. Define the tensor product $V \otimes W := \hom(V^*,W)$. This is a vector space by construction. Its elements are just linear maps $V^* \to W$. If $v \in V$, $w \in W$, we define the pure tensor $v \otimes w \in V \otimes W$ by $(v \otimes w)(\omega)=\omega(v) \cdot w$. Observe that $(v+v') \otimes w=v \otimes w + v' \otimes w$ and $(\lambda v) \otimes w = \lambda(v \otimes w)$, likewise for the other variable, i.e. $V \times W \to V \otimes W$ is a bilinear map. Any element $\alpha \in V^* \otimes W$ is a sum of pure tensors: If $e_1,\dotsc,e_n$ is a basis of $V$, then $\alpha = \sum_i e_i \otimes \alpha(e_i^*)$. (Because the right hand side maps $e_i^*$ to $\alpha(e_i^*)$.) Universal property: If $b : V \times W \to U$ is a bilinear map, there is a unique linear map $f : V \otimes W \to U$ such that $f(v \otimes w)=b(v,w)$ for all $(v,w) \in V \times W$. Proof. Uniqueness follows since every element is a sum of pure tensors. For existence, we just let $f(\alpha)=\sum_i b(e_i,\alpha(e_i))$. Then one easily checks $f(e_j \otimes w)=b(e_j,w)$ and therefore $f(v \otimes w)=b(v,w)$. $\checkmark$ The universal property shows in particular that the tensor product of finite-dimensional vector spaces is symmetric: There is a unique isomorphism $V \otimes W \cong W \otimes V$ mapping $v \otimes w \mapsto w \otimes v$. [Of course, lots of other nice properties of the tensor product also follow from the universal property.] If $V$ is not assumed to be finite-dimensional, we define $V \otimes W$ as the subspace of $\hom(V^*,W)$ which is generated by the pure tensors $v \otimes w$ as defined above. (Thus, if $V$ is finite-dimensional, there will be no difference.) The universal property follows immediately.",,"['linear-algebra', 'tensor-products', 'education']"
55,"Find $B$ if $AB=BC$ and $A,C$ are invertible",Find  if  and  are invertible,"B AB=BC A,C","Suppose $A$ and $C$ are known invertible complex matrices of possibly different orders. If $B$ is an unknown matrix of appropriate order such that $AB = BC$, then how could one solve for $B$?","Suppose $A$ and $C$ are known invertible complex matrices of possibly different orders. If $B$ is an unknown matrix of appropriate order such that $AB = BC$, then how could one solve for $B$?",,"['linear-algebra', 'matrices']"
56,Exploiting structure in multilinear equations,Exploiting structure in multilinear equations,,"I'm wondering if there are any standard techniques for exploiting structure in multilinear equations. An example of what I have in mind is solving $A_{ab} X_{bc} A_{cd} (B_{ad} B_{bc} + B_{ac} B_{bd}) = RHS_{ad}$ for X. Here, A, B, and X are n-by-n matrices and there is summation over b,c. Of, course, one can view this as a large equation system $M vec(X) = vec(RHS)$ where M has dimension n^2 by n^2. But I'd like to somehow exploit the structure of M to reduce the rank.","I'm wondering if there are any standard techniques for exploiting structure in multilinear equations. An example of what I have in mind is solving $A_{ab} X_{bc} A_{cd} (B_{ad} B_{bc} + B_{ac} B_{bd}) = RHS_{ad}$ for X. Here, A, B, and X are n-by-n matrices and there is summation over b,c. Of, course, one can view this as a large equation system $M vec(X) = vec(RHS)$ where M has dimension n^2 by n^2. But I'd like to somehow exploit the structure of M to reduce the rank.",,"['linear-algebra', 'multilinear-algebra']"
57,Another Algebraic de Rham Cohomology question...,Another Algebraic de Rham Cohomology question...,,"NOTE: scroll down to read my latest edit first if you're reading this for the first time :) My aim is to calculate the de Rham cohomology of the variety $U = \text{Spec} \  A$, where: $$A = \frac{\mathbb{C}[U,V,W]}{\langle V^2 - UW\rangle} $$ So far I have defined: $$\Omega^1_{A/\mathbb{C}} := \frac{\Omega^1_{\mathbb{C}[U,V,W]/ \mathbb{C}}}{\langle d(V^2-UW)\rangle } \ \ , \qquad \Omega^p_{A/\mathbb{C}} := \wedge^p \Omega^1_{A/\mathbb{C}} $$ and: $$ d^0(f+I) := \frac{ \partial f}{ \partial U}  \,dU + \frac{ \partial f}{ \partial V}  dV  + \frac{ \partial f}{ \partial W} \, dW  + \langle d(V^2-UW)\rangle  $$ (letting $ \ I = \  \langle V^2 - UV\rangle  \ \ $) Now, assuming what I have above is ok, we have $ \langle d(V^2-UW) \rangle  \  = \langle  2V\, dV - W\,dU - U\,dW \rangle  $ So ($U$ affine etc) : $$\small{ H^0_{dR}(U) \cong \ker d^0 = \lbrace f+I : \left(\frac{ \partial f}{ \partial U} + I \right) dU + \left(\frac{ \partial f}{ \partial V} + I \right) dV  + \left(\frac{ \partial f}{ \partial W} + I \right) dW  \in \langle 2V \,dV - W\,dU - U\,dW \rangle  \rbrace }$$ Here is where I get stuck. I know we should end up with $H^0_{dR}(U) \cong \mathbb{C} $, but these maps are pretty confusing, especially now as our generators are no longer independent thanks to the new relation we've introduced. My questions are: 1) is what i've done above correct so far? 2) if it is, then how would one go about showing  $H^0_{dR}(U) \cong \mathbb{C} $ continuing what i've done above? (this can be just a hint in the right direction if you want) EDIT: I've looked through the suggested MO thread, and commented below on it. It provides a good proof of (2) using a more general method. I would still like to get at it through this calculation if that's possible. EDIT 2: Also I tried this earlier; is this be correct for the ring $\mathbb{C}[x,y]/\langle xy \rangle $ ?: Suppose $\frac{ \partial f}{ \partial x} dx + \frac{ \partial f}{ \partial y} dy \in \langle ydx + xdy \rangle $ (working in $ \Omega^1_{\mathbb{C}[x,y]} $) Now $ \exists g \in \mathbb{C[x,y]} \ \ \frac{ \partial f}{ \partial x} = yg(x,y) \ , \frac{ \partial f}{ \partial y} = xg(x,y)$ Choose $G_x(x,y) \ , \ G_y(x,y)$ such that $\frac{ \partial G_x}{ \partial x} = g \ , \ \frac{ \partial G_y}{ \partial y} = g$. Now we see $f = yG_x + H_1(y) $ and $ f = xG_y + H_2(x) $ for some $H_1,H_2$ without loss of generality then $G_x$ is divisible by $x$ and similarly $y | G_y$. Then we see $H_1(y) = f = H_2(x) $ (since $xy=0$), so $H_1,H_2$ (and hence also $f$) constant. I realize I've argued this fairly loosely and it could do with more rigour. EDIT...4?: David below has pointed out that this calculation will not give me what I want. I would still like to know a sketch of the method that should be used to find kernels and images of linear maps like this, because it's this which I find tough. I guess the situation I am thinking about is: $ \phi : \ V \rightarrow W$, where $V,W$ are infinite dimensional $\mathbb{C}$-vector spaces and in general do not have a basis, (but the case where $V$ has a basis is also important) and where we know all the relations imposed on $V,W$. Is there a standard method to find kernels and/or images in this case? (this will get the bounty)","NOTE: scroll down to read my latest edit first if you're reading this for the first time :) My aim is to calculate the de Rham cohomology of the variety $U = \text{Spec} \  A$, where: $$A = \frac{\mathbb{C}[U,V,W]}{\langle V^2 - UW\rangle} $$ So far I have defined: $$\Omega^1_{A/\mathbb{C}} := \frac{\Omega^1_{\mathbb{C}[U,V,W]/ \mathbb{C}}}{\langle d(V^2-UW)\rangle } \ \ , \qquad \Omega^p_{A/\mathbb{C}} := \wedge^p \Omega^1_{A/\mathbb{C}} $$ and: $$ d^0(f+I) := \frac{ \partial f}{ \partial U}  \,dU + \frac{ \partial f}{ \partial V}  dV  + \frac{ \partial f}{ \partial W} \, dW  + \langle d(V^2-UW)\rangle  $$ (letting $ \ I = \  \langle V^2 - UV\rangle  \ \ $) Now, assuming what I have above is ok, we have $ \langle d(V^2-UW) \rangle  \  = \langle  2V\, dV - W\,dU - U\,dW \rangle  $ So ($U$ affine etc) : $$\small{ H^0_{dR}(U) \cong \ker d^0 = \lbrace f+I : \left(\frac{ \partial f}{ \partial U} + I \right) dU + \left(\frac{ \partial f}{ \partial V} + I \right) dV  + \left(\frac{ \partial f}{ \partial W} + I \right) dW  \in \langle 2V \,dV - W\,dU - U\,dW \rangle  \rbrace }$$ Here is where I get stuck. I know we should end up with $H^0_{dR}(U) \cong \mathbb{C} $, but these maps are pretty confusing, especially now as our generators are no longer independent thanks to the new relation we've introduced. My questions are: 1) is what i've done above correct so far? 2) if it is, then how would one go about showing  $H^0_{dR}(U) \cong \mathbb{C} $ continuing what i've done above? (this can be just a hint in the right direction if you want) EDIT: I've looked through the suggested MO thread, and commented below on it. It provides a good proof of (2) using a more general method. I would still like to get at it through this calculation if that's possible. EDIT 2: Also I tried this earlier; is this be correct for the ring $\mathbb{C}[x,y]/\langle xy \rangle $ ?: Suppose $\frac{ \partial f}{ \partial x} dx + \frac{ \partial f}{ \partial y} dy \in \langle ydx + xdy \rangle $ (working in $ \Omega^1_{\mathbb{C}[x,y]} $) Now $ \exists g \in \mathbb{C[x,y]} \ \ \frac{ \partial f}{ \partial x} = yg(x,y) \ , \frac{ \partial f}{ \partial y} = xg(x,y)$ Choose $G_x(x,y) \ , \ G_y(x,y)$ such that $\frac{ \partial G_x}{ \partial x} = g \ , \ \frac{ \partial G_y}{ \partial y} = g$. Now we see $f = yG_x + H_1(y) $ and $ f = xG_y + H_2(x) $ for some $H_1,H_2$ without loss of generality then $G_x$ is divisible by $x$ and similarly $y | G_y$. Then we see $H_1(y) = f = H_2(x) $ (since $xy=0$), so $H_1,H_2$ (and hence also $f$) constant. I realize I've argued this fairly loosely and it could do with more rigour. EDIT...4?: David below has pointed out that this calculation will not give me what I want. I would still like to know a sketch of the method that should be used to find kernels and images of linear maps like this, because it's this which I find tough. I guess the situation I am thinking about is: $ \phi : \ V \rightarrow W$, where $V,W$ are infinite dimensional $\mathbb{C}$-vector spaces and in general do not have a basis, (but the case where $V$ has a basis is also important) and where we know all the relations imposed on $V,W$. Is there a standard method to find kernels and/or images in this case? (this will get the bounty)",,"['linear-algebra', 'abstract-algebra', 'algebraic-geometry', 'homology-cohomology', 'complex-geometry']"
58,Linear w.r.t. any measure,Linear w.r.t. any measure,,"Let $X$ be a Banach space endowed with a Borel $\sigma$-algebra. How do we call a real-valued Borel function $f$ that satisfies for any Borel probability measure $\mu$ the following formula $$   \int_X f(x)\mu(dx) = f\left(\int_Xx \mu(dx)\right). $$ If we focus only on discrete measures $\mu$ then $f$ is just any linear function, however I wonder whether there is a special term for $f$ that satisfies such linearity condition for all measures.","Let $X$ be a Banach space endowed with a Borel $\sigma$-algebra. How do we call a real-valued Borel function $f$ that satisfies for any Borel probability measure $\mu$ the following formula $$   \int_X f(x)\mu(dx) = f\left(\int_Xx \mu(dx)\right). $$ If we focus only on discrete measures $\mu$ then $f$ is just any linear function, however I wonder whether there is a special term for $f$ that satisfies such linearity condition for all measures.",,"['linear-algebra', 'functional-analysis', 'measure-theory', 'probability-theory', 'terminology']"
59,Matrices whose Linear Combinations are All Singular,Matrices whose Linear Combinations are All Singular,,"I'd like to know if the following problem of elementary linear algebra is already solved / solvable. For two (singular) $n\times n$ matrices $P$ and $Q$, if $\det(\lambda P+\mu Q)=0$ for any $\lambda,\mu\in\mathbb{R}$, what are conditions on $P$ and $Q$?","I'd like to know if the following problem of elementary linear algebra is already solved / solvable. For two (singular) $n\times n$ matrices $P$ and $Q$, if $\det(\lambda P+\mu Q)=0$ for any $\lambda,\mu\in\mathbb{R}$, what are conditions on $P$ and $Q$?",,['linear-algebra']
60,Orbits of action of $SL_m(\mathbb{Z})$ on $\mathbb{Z}^m$,Orbits of action of  on,SL_m(\mathbb{Z}) \mathbb{Z}^m,"I'm considering the action of $SL_m(\mathbb{Z})$ on $\mathbb{Z}^m$: if $A\in SL_m(\mathbb{Z})$ and $v\in\mathbb{Z}^m$, then $Av\in\mathbb{Z}^m$. My question is: what are the orbits of this action?  I'm especially interested in the case $m=3$. For $m=2$, we have the following: If $a$ and $b$ are (positive) relatively prime integers, then you can always find integers $c$ and $d$ so that $ad-bc=1$, so that $\begin{pmatrix} a&c \\ b&d \end{pmatrix}$ is in $SL_2(\mathbb{Z})$. That means that $\begin{pmatrix} a \\ b \end{pmatrix}$ is in the orbit of $\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ under this action. Conversely it's easy to see that nothing else can be in that orbit. More generally, the orbits of this action are in bijection with the nonnegative integers: the orbit corresponding to $n>0$ consists mainly of the lattice points $\begin{pmatrix} a \\ b \end{pmatrix}$ with $\gcd(|a|,|b|)=n$. And if $n=0$, then the orbit consists of $\begin{pmatrix} 0 \\ 0 \end{pmatrix}$ only.","I'm considering the action of $SL_m(\mathbb{Z})$ on $\mathbb{Z}^m$: if $A\in SL_m(\mathbb{Z})$ and $v\in\mathbb{Z}^m$, then $Av\in\mathbb{Z}^m$. My question is: what are the orbits of this action?  I'm especially interested in the case $m=3$. For $m=2$, we have the following: If $a$ and $b$ are (positive) relatively prime integers, then you can always find integers $c$ and $d$ so that $ad-bc=1$, so that $\begin{pmatrix} a&c \\ b&d \end{pmatrix}$ is in $SL_2(\mathbb{Z})$. That means that $\begin{pmatrix} a \\ b \end{pmatrix}$ is in the orbit of $\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ under this action. Conversely it's easy to see that nothing else can be in that orbit. More generally, the orbits of this action are in bijection with the nonnegative integers: the orbit corresponding to $n>0$ consists mainly of the lattice points $\begin{pmatrix} a \\ b \end{pmatrix}$ with $\gcd(|a|,|b|)=n$. And if $n=0$, then the orbit consists of $\begin{pmatrix} 0 \\ 0 \end{pmatrix}$ only.",,"['linear-algebra', 'group-theory', 'matrices', 'group-actions', 'integer-lattices']"
61,"How to construct the subring generated by a set, T?","How to construct the subring generated by a set, T?",,"I'm trying to find a constructive way of describing the subring generated by some subset, T, of a ring R.  I think I could describe it as all finite sums of finite products of elements of T, but I have no idea how to write that as a set or how to prove that it is equal to the subring generated by T.","I'm trying to find a constructive way of describing the subring generated by some subset, T, of a ring R.  I think I could describe it as all finite sums of finite products of elements of T, but I have no idea how to write that as a set or how to prove that it is equal to the subring generated by T.",,"['linear-algebra', 'ring-theory']"
62,Cholesky decomposition of $A+kI$ given Cholesky decomposition of A,Cholesky decomposition of  given Cholesky decomposition of A,A+kI,Suppose I have the Cholesky decomposition for a symmetric matrix $A$: $$ A = L L^T $$ I wish to compute the Cholesky decomposition for $A+kI$ where $I$ is the identity and $k$ is a scalar. Is there a way to obtain this using the decomposition for $A$ faster than recomputing the Cholesky decomposition from scratch?,Suppose I have the Cholesky decomposition for a symmetric matrix $A$: $$ A = L L^T $$ I wish to compute the Cholesky decomposition for $A+kI$ where $I$ is the identity and $k$ is a scalar. Is there a way to obtain this using the decomposition for $A$ faster than recomputing the Cholesky decomposition from scratch?,,"['linear-algebra', 'matrices', 'matrix-decomposition', 'cholesky-decomposition']"
63,Condition of an eigenvector problem,Condition of an eigenvector problem,,"Please, somebody help me with this problem. [Ciarlet 2.3-5] Let ${A}$ and ${B} = {A} + \delta{A}$ be two symmetric matrices with eigenvalues $$\alpha_1\ \leq\ \alpha_2\ \leq\ \ldots\ \leq\ \alpha_n\;\;\; \mbox{ and }\;\;\; \beta_1\ \leq\ \beta_2\ \leq\ \ldots\ \leq\ \beta_n.$$ Let $\alpha_k$ be a simple eigenvalue of the matrix ${A}$ and let ${a}_k$ be an eigenvector (with $\|{a}_k\|_2 = 1$) corresponding to the eigenvalue $\alpha_k$. Show that if $$\|\delta{A}\|_2\ <\ \Delta\ \stackrel{\text{def}}{=}\ \min_{i\neq k}|\alpha_i-\alpha_k|,$$ there exists an eigenvector ${b}_k$ (with $\|{b}_k\|_2 = 1$) corresponding to the eigenvalue $\beta_k$, which satisfies $$\|{a}_k - {b}_k\|_2\ \leq\ \gamma(1 + \gamma^2)^{1/2},\;\;\; \mbox{ with }\;\; \gamma\ =\ \frac{\|\delta{A}\|_2}{\Delta - \|\delta{A}\|_2}.$$ Thanks for the time.","Please, somebody help me with this problem. [Ciarlet 2.3-5] Let ${A}$ and ${B} = {A} + \delta{A}$ be two symmetric matrices with eigenvalues $$\alpha_1\ \leq\ \alpha_2\ \leq\ \ldots\ \leq\ \alpha_n\;\;\; \mbox{ and }\;\;\; \beta_1\ \leq\ \beta_2\ \leq\ \ldots\ \leq\ \beta_n.$$ Let $\alpha_k$ be a simple eigenvalue of the matrix ${A}$ and let ${a}_k$ be an eigenvector (with $\|{a}_k\|_2 = 1$) corresponding to the eigenvalue $\alpha_k$. Show that if $$\|\delta{A}\|_2\ <\ \Delta\ \stackrel{\text{def}}{=}\ \min_{i\neq k}|\alpha_i-\alpha_k|,$$ there exists an eigenvector ${b}_k$ (with $\|{b}_k\|_2 = 1$) corresponding to the eigenvalue $\beta_k$, which satisfies $$\|{a}_k - {b}_k\|_2\ \leq\ \gamma(1 + \gamma^2)^{1/2},\;\;\; \mbox{ with }\;\; \gamma\ =\ \frac{\|\delta{A}\|_2}{\Delta - \|\delta{A}\|_2}.$$ Thanks for the time.",,"['linear-algebra', 'matrices']"
64,Infinite-dimensional singular value decomposition,Infinite-dimensional singular value decomposition,,"Say, I have an $n\times n$ matrix $w_{ij}$ . I can perform a singular value decomposition such that $$ w_{ij}=\sum_l \sum_n u_{il}\lambda_{ln}v_{jn} $$ with $\lambda_{ln}$ diagonal.  Now, is there such a generalization so that, given a function of two variables $w(\theta_1, \theta_2)$ , $$ w(\theta_1 ,\theta_2)=\int dy \int dx \,u(\theta_1 ,x) \, \lambda(x,y) \, v(\theta_2 ,y) $$ where $\lambda$ plays a similar role like it did in the SVD?  For instance,  say I have the following $$ \exp {[\alpha \cos(\theta-\phi)]} $$ is it possible to find a decomposition such that $$ \exp {\alpha \cos(\theta-\phi)}=\iint dx \, dy \, u(\theta,x) \, \lambda(\alpha,x,y) \, v(\phi,y) $$","Say, I have an matrix . I can perform a singular value decomposition such that with diagonal.  Now, is there such a generalization so that, given a function of two variables , where plays a similar role like it did in the SVD?  For instance,  say I have the following is it possible to find a decomposition such that","n\times n w_{ij}  w_{ij}=\sum_l \sum_n u_{il}\lambda_{ln}v_{jn}  \lambda_{ln} w(\theta_1, \theta_2)  w(\theta_1 ,\theta_2)=\int dy \int dx \,u(\theta_1 ,x) \, \lambda(x,y) \, v(\theta_2 ,y)  \lambda  \exp {[\alpha \cos(\theta-\phi)]}   \exp {\alpha \cos(\theta-\phi)}=\iint dx \, dy \, u(\theta,x) \, \lambda(\alpha,x,y) \, v(\phi,y) ","['linear-algebra', 'functional-analysis', 'svd']"
65,"Hadamard Product Represented as ""Simpler"" Operations","Hadamard Product Represented as ""Simpler"" Operations",,"Given matrices $A$ and $B$, the Hadamard product $A\circ B$ is given by $(A\circ B)_{ij}=A_{ij}\cdot B_{ij}$ (if you ask an ordinary middle schooler, this would be the ""most natural"" definition of matrix multiplication, haha.) Does anyone know if it is possible to represent $A\circ B$ as a combination of other widely used matrix operations, such as addition, multiplication, taking determinant and taking inverse?","Given matrices $A$ and $B$, the Hadamard product $A\circ B$ is given by $(A\circ B)_{ij}=A_{ij}\cdot B_{ij}$ (if you ask an ordinary middle schooler, this would be the ""most natural"" definition of matrix multiplication, haha.) Does anyone know if it is possible to represent $A\circ B$ as a combination of other widely used matrix operations, such as addition, multiplication, taking determinant and taking inverse?",,"['linear-algebra', 'hadamard-product']"
66,"Symmetric functions of the eigenvalues of A+B, A, B, ABA, BAB, et.c.","Symmetric functions of the eigenvalues of A+B, A, B, ABA, BAB, et.c.",,"(this is an improved version of What about other symmetric functions of the eigenvalues? ) Let $A$ be a matrix with eigenvalues $\lambda_1, \dots, \lambda_n$. Then $\det(A) = \lambda_1 \dots \lambda_n$ and ${\textrm tr}(A) = \lambda_1 + \dots + \lambda_n$. Now let $i_k(A) = e_k(\lambda_1, \dots, \lambda_n)$, where $e_k$ is the $k$th elementary symmetric function, so that det=$i_n$ and tr=$i_1$ It follows that $\det(I + tA) = \sum_{k=0} ^n i_k(A)t^k$. Using this, and the identity $(I+tA)(I+tB) = I+t(A+B+tAB)$ one can slowly grind out identities like $i_2(A+B) = i_2(A) + i_2(B) + i_1(A)i_1(B) - i_1(AB)$. The next simplest identity expresses $i_3(A+B)$ in terms of $i_1$ and $i_2$:s of $A,B,ABA$ and $BAB$. Just the fact that these identities exist implies that knowledge of the eigenvalues of $A,B,ABA,BAB,\dots$ (meaning the matrices that occur in the formulas for higher $i_k$:s) implies knowledge of the eigenvalues of $A+B$. My question is two-fold: (i) Is there a 'nice' description of the analogous formula for $i_k(A+B)$? (ii) Is there a more exact statement along the lines of ""knowing the eigenvalues of this set of matrices is equivalent to knowing the eigenvalues of that set of matrices""?","(this is an improved version of What about other symmetric functions of the eigenvalues? ) Let $A$ be a matrix with eigenvalues $\lambda_1, \dots, \lambda_n$. Then $\det(A) = \lambda_1 \dots \lambda_n$ and ${\textrm tr}(A) = \lambda_1 + \dots + \lambda_n$. Now let $i_k(A) = e_k(\lambda_1, \dots, \lambda_n)$, where $e_k$ is the $k$th elementary symmetric function, so that det=$i_n$ and tr=$i_1$ It follows that $\det(I + tA) = \sum_{k=0} ^n i_k(A)t^k$. Using this, and the identity $(I+tA)(I+tB) = I+t(A+B+tAB)$ one can slowly grind out identities like $i_2(A+B) = i_2(A) + i_2(B) + i_1(A)i_1(B) - i_1(AB)$. The next simplest identity expresses $i_3(A+B)$ in terms of $i_1$ and $i_2$:s of $A,B,ABA$ and $BAB$. Just the fact that these identities exist implies that knowledge of the eigenvalues of $A,B,ABA,BAB,\dots$ (meaning the matrices that occur in the formulas for higher $i_k$:s) implies knowledge of the eigenvalues of $A+B$. My question is two-fold: (i) Is there a 'nice' description of the analogous formula for $i_k(A+B)$? (ii) Is there a more exact statement along the lines of ""knowing the eigenvalues of this set of matrices is equivalent to knowing the eigenvalues of that set of matrices""?",,"['linear-algebra', 'determinant', 'symmetric-functions']"
67,Random binary invertible matrix,Random binary invertible matrix,,"For implementation of McEliece cryptosystem, I'm trying to generate a random binary invertible matrix and its inverse. Because this is usually the most time-consuming part of generating a McEliece keypair (matrix size is usually around $2^{13}\times2^{13}$), I'm searching for ways to do it faster. So far, I've tried to use following approaches: Generate an upper triangular matrix $U$ with random ones and zeroes in the upper triangle, and ones on diagonal (such $U$ is invertible), symmetrically generate $L$ lower triangular matrix, compute $M = UL$ and invert M by standard inversion. This is slow. Compute $M = UL$ and $M^{-1} = L^{-1}U^{-1}$. In this case, the inversion is a lot faster (there's no need to compute full row operations, as the inverse remains lower/upper triangular shape), but still slow. Have two identity matrices $I_1,I_2$, perform a number of random row operations on $I_1$, and symetrically perform those inverse row operations on $I_2$, so that equation $I_1^{-1} = I_2$ holds. I'm not sure what number of row operations should be done on the matrices so that the result would be completely random -- my best guess is that $n^2$ operations should be sufficient, giving algorithm complexity $O(n^3)$. If there was some lower limit so that the matrix would be random-enough, this approach could be the fastest. Questions are: Is there some better algorithm, or is the $O(n^3)$ bound impossible to break? What is the lowest possible count of operations in the third algorithm so that the matrix gets undistinguishable from a random matrix from the first algorithm? Thanks -exa","For implementation of McEliece cryptosystem, I'm trying to generate a random binary invertible matrix and its inverse. Because this is usually the most time-consuming part of generating a McEliece keypair (matrix size is usually around $2^{13}\times2^{13}$), I'm searching for ways to do it faster. So far, I've tried to use following approaches: Generate an upper triangular matrix $U$ with random ones and zeroes in the upper triangle, and ones on diagonal (such $U$ is invertible), symmetrically generate $L$ lower triangular matrix, compute $M = UL$ and invert M by standard inversion. This is slow. Compute $M = UL$ and $M^{-1} = L^{-1}U^{-1}$. In this case, the inversion is a lot faster (there's no need to compute full row operations, as the inverse remains lower/upper triangular shape), but still slow. Have two identity matrices $I_1,I_2$, perform a number of random row operations on $I_1$, and symetrically perform those inverse row operations on $I_2$, so that equation $I_1^{-1} = I_2$ holds. I'm not sure what number of row operations should be done on the matrices so that the result would be completely random -- my best guess is that $n^2$ operations should be sufficient, giving algorithm complexity $O(n^3)$. If there was some lower limit so that the matrix would be random-enough, this approach could be the fastest. Questions are: Is there some better algorithm, or is the $O(n^3)$ bound impossible to break? What is the lowest possible count of operations in the third algorithm so that the matrix gets undistinguishable from a random matrix from the first algorithm? Thanks -exa",,"['linear-algebra', 'algorithms', 'random-matrices']"
68,Uses of Chevalley-Warning,Uses of Chevalley-Warning,,"In the recent IMC 2011, the last problem of the 1st day (no. 5, the hardest of that day) was as follows: We have $4n-1$ vectors in $F_2^{2n-1}$: $\{v_i\}_{i=1}^{4n-1}$. The problem asks : Prove the existence of a subset $A \subseteq [4n-1], |A| = 2n$ such that $\sum_{i \in A} v_i = \vec{0}$. Is there a solution that uses the Chevalley-Warning theorem about divisibility of number of solutions by the field's characteristic? The statement of the problem seems as a ""xor"" analog of Erdos-Ginzburg-Ziv, that is usually proved by Chevally-Warning. My idea is to formulate homogeneous polynomial equations in $a_1,\cdots,a_{4n-1} \in F_2$: $P_j(\vec{a}) = (\sum a_i v_i)_j$ for $1 \le j \le 2n-1$. Let $A(\vec{a}) = \{ i | a_i = 1\}$ and $g(\vec{a}) = |A(\vec{a})|$ (we want to choose $A = A(\vec{a})$ for a solution $a$ of the system). Ideas for completing the solution: We can also create $Q_{k}(\vec{a}) = $ Symmetric polynomial of degree $2^k$ = $a_1 \cdots a_{2^k} + \cdots$. We notice that $Q_k(a) = \binom{g(\vec{a})}{2^k}$. By Lucas's Theorem, mod 2 we have $\binom{m}{2^k} = $ k'th digit of $m$ in base-$2$. So we choose the equations $Q_k(a) = \binom{2n}{2^k}$ mod 2. The problem is that we get $\sum deg(Q_k) \ge 2n$ (sum over number of digits of $4n-1$). Also, $0$ is not a trivial solution. Another idea was to define the equations $R_i(a) = a_{2i-1} + a_{2i} - a_{2i+1} - a_{2i+2} = 0, 1 \le i \le 2n$. A solution implies that $a_{2i-1} + a_{2i}$ is constant. If it is $1$, we get that from each pair of even and odd vector, one is chosen - $2n$ vectors in total. But it has some holes. Another idea is to create equations that force $g(\vec{a}) = 0$ mod n, so $\vec{0}$ is a trivial solution, and another solution would imply that a $A$ exists of size $n, 2n$ or $3n$. A nice observation is that one of the vectors is (wlog) $\vec{0}$ (can be assumed since $2n$ is even, so replacing $v_i$ by $v_i - v_{1}$ is fine). If you have idea of how to use the theorem, and examples in general of its non-trivial uses - it would be welcome.","In the recent IMC 2011, the last problem of the 1st day (no. 5, the hardest of that day) was as follows: We have $4n-1$ vectors in $F_2^{2n-1}$: $\{v_i\}_{i=1}^{4n-1}$. The problem asks : Prove the existence of a subset $A \subseteq [4n-1], |A| = 2n$ such that $\sum_{i \in A} v_i = \vec{0}$. Is there a solution that uses the Chevalley-Warning theorem about divisibility of number of solutions by the field's characteristic? The statement of the problem seems as a ""xor"" analog of Erdos-Ginzburg-Ziv, that is usually proved by Chevally-Warning. My idea is to formulate homogeneous polynomial equations in $a_1,\cdots,a_{4n-1} \in F_2$: $P_j(\vec{a}) = (\sum a_i v_i)_j$ for $1 \le j \le 2n-1$. Let $A(\vec{a}) = \{ i | a_i = 1\}$ and $g(\vec{a}) = |A(\vec{a})|$ (we want to choose $A = A(\vec{a})$ for a solution $a$ of the system). Ideas for completing the solution: We can also create $Q_{k}(\vec{a}) = $ Symmetric polynomial of degree $2^k$ = $a_1 \cdots a_{2^k} + \cdots$. We notice that $Q_k(a) = \binom{g(\vec{a})}{2^k}$. By Lucas's Theorem, mod 2 we have $\binom{m}{2^k} = $ k'th digit of $m$ in base-$2$. So we choose the equations $Q_k(a) = \binom{2n}{2^k}$ mod 2. The problem is that we get $\sum deg(Q_k) \ge 2n$ (sum over number of digits of $4n-1$). Also, $0$ is not a trivial solution. Another idea was to define the equations $R_i(a) = a_{2i-1} + a_{2i} - a_{2i+1} - a_{2i+2} = 0, 1 \le i \le 2n$. A solution implies that $a_{2i-1} + a_{2i}$ is constant. If it is $1$, we get that from each pair of even and odd vector, one is chosen - $2n$ vectors in total. But it has some holes. Another idea is to create equations that force $g(\vec{a}) = 0$ mod n, so $\vec{0}$ is a trivial solution, and another solution would imply that a $A$ exists of size $n, 2n$ or $3n$. A nice observation is that one of the vectors is (wlog) $\vec{0}$ (can be assumed since $2n$ is even, so replacing $v_i$ by $v_i - v_{1}$ is fine). If you have idea of how to use the theorem, and examples in general of its non-trivial uses - it would be welcome.",,"['linear-algebra', 'combinatorics', 'additive-combinatorics', 'algebraic-combinatorics', 'arithmetic-combinatorics']"
69,Hardness of finding eigenvalues over finite fields,Hardness of finding eigenvalues over finite fields,,"How hard is it (computationally) to find eigenvalues/eigenvectors of matrices over finite fields?  Suppose the field has size exponential in the input.  (Does the QR algorithm still converge?) How about sparse matrices?  What are the best known algorithms for finding eigenvalues/eigenvectors of a matrix which is exponential in the input size (number of non-zero entries)? In general, is there a setting in which finding eigenvalues/eigenvectors is computationally hard?  Or rather, not known to be computationally easy?","How hard is it (computationally) to find eigenvalues/eigenvectors of matrices over finite fields?  Suppose the field has size exponential in the input.  (Does the QR algorithm still converge?) How about sparse matrices?  What are the best known algorithms for finding eigenvalues/eigenvectors of a matrix which is exponential in the input size (number of non-zero entries)? In general, is there a setting in which finding eigenvalues/eigenvectors is computationally hard?  Or rather, not known to be computationally easy?",,"['linear-algebra', 'algorithms', 'finite-fields', 'computational-complexity']"
70,Showing the countable direct product of $\mathbb{Z}$ is not projective,Showing the countable direct product of  is not projective,\mathbb{Z},"I am trying to prove that the direct product $M = \mathbb{Z} \times \mathbb{Z}\times \cdots$ is not a projective $\mathbb Z$-module and I am stuck near the end of the proof because of the authors use of the word infinitely divisible. I will list the sketch of the proof and try to explain where I am stuck. We proceed by contradiction so suppose $M$ is contained in some free module $F$ with basis $B$.  Set $N = \mathbb{Z} \oplus \mathbb{Z} \oplus\cdots$ and observe that $N$ is a submodule of $M$. Since $N \subset F$ there exists $B' \subset B$ such that $B'$ is a basis for $N$ and consider the free module $F' \subset F$ determined by $B'$. Notice $F'+M \subset F$ gives $M/(M \cap F') \cong (F'+M)/F' \subset F/F'$ so we have $M/(M \cap F') $ The next step in the proof requires to consider sequences of signs so let $s = (s_1, s_2, \ldots)$ be a sequence of plus and minus signs and consider an element $m_s := (s_1 , 2 s_2, \ldots , k! s_k, \ldots) \in M$ The next point in the proof is what I don't understand, the notes I am using say $m_s +(M \cap F')$ is infinitely divisible in $F/F'$ and use this to show $M$ cannot be contained in any free $Z$ module.  My question is How do we show $m_s +(M \cap F')$ is infinitely divisible in $F/F'$ and how do translate the word infinitely divisible into definitions from Hungerford or Dummite and Foote?","I am trying to prove that the direct product $M = \mathbb{Z} \times \mathbb{Z}\times \cdots$ is not a projective $\mathbb Z$-module and I am stuck near the end of the proof because of the authors use of the word infinitely divisible. I will list the sketch of the proof and try to explain where I am stuck. We proceed by contradiction so suppose $M$ is contained in some free module $F$ with basis $B$.  Set $N = \mathbb{Z} \oplus \mathbb{Z} \oplus\cdots$ and observe that $N$ is a submodule of $M$. Since $N \subset F$ there exists $B' \subset B$ such that $B'$ is a basis for $N$ and consider the free module $F' \subset F$ determined by $B'$. Notice $F'+M \subset F$ gives $M/(M \cap F') \cong (F'+M)/F' \subset F/F'$ so we have $M/(M \cap F') $ The next step in the proof requires to consider sequences of signs so let $s = (s_1, s_2, \ldots)$ be a sequence of plus and minus signs and consider an element $m_s := (s_1 , 2 s_2, \ldots , k! s_k, \ldots) \in M$ The next point in the proof is what I don't understand, the notes I am using say $m_s +(M \cap F')$ is infinitely divisible in $F/F'$ and use this to show $M$ cannot be contained in any free $Z$ module.  My question is How do we show $m_s +(M \cap F')$ is infinitely divisible in $F/F'$ and how do translate the word infinitely divisible into definitions from Hungerford or Dummite and Foote?",,"['linear-algebra', 'abstract-algebra', 'modules']"
71,Solving an equation involving complex conjugates,Solving an equation involving complex conjugates,,"I have the following question and cannot seem to overcome how to contend with equations using $z$ and $\bar z$ together. For example, the below problem: Find the value of $z \in \Bbb C$ that verifies the equation: $$3z+i\bar z=4+i$$ For other operations that didn't include mixing $z$ and $\bar z$ , I was able to manage by ""isolating"" $z$ on one side  of the equation and finding the real and imaginary parts of the complex numbers (sorry if I'm not using the right terms, it's my first linear algebra course) I tried with wolfram and it didn't really help. PS: I'm new to this forum but if it's like other math forums where they send you to hell if you ask for ""help with your homework"", this ""homework"" I'm doing is on my own since my semester is over and I just wanted to explore other subjects in the book that weren't covered in class.","I have the following question and cannot seem to overcome how to contend with equations using and together. For example, the below problem: Find the value of that verifies the equation: For other operations that didn't include mixing and , I was able to manage by ""isolating"" on one side  of the equation and finding the real and imaginary parts of the complex numbers (sorry if I'm not using the right terms, it's my first linear algebra course) I tried with wolfram and it didn't really help. PS: I'm new to this forum but if it's like other math forums where they send you to hell if you ask for ""help with your homework"", this ""homework"" I'm doing is on my own since my semester is over and I just wanted to explore other subjects in the book that weren't covered in class.",z \bar z z \in \Bbb C 3z+i\bar z=4+i z \bar z z,"['linear-algebra', 'complex-numbers']"
72,"If $A^2 = B^2$, then $A=B$ or $A=-B$","If , then  or",A^2 = B^2 A=B A=-B,"Let $A_{n\times n},B_{n\times n}$ be square matrices with $n \geq 2$. If $A^2 = B^2$, then $A=B$   or $A=-B$. This is wrong but I don't see why. Do you have any counterexample?","Let $A_{n\times n},B_{n\times n}$ be square matrices with $n \geq 2$. If $A^2 = B^2$, then $A=B$   or $A=-B$. This is wrong but I don't see why. Do you have any counterexample?",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
73,Show that the square matrix A is invertible,Show that the square matrix A is invertible,,"The question is: The square matrix $A$ satisfies $p(A) = 0$, where $p(x)$ is a polynomial such that $p(0) \ne 0$. Show that $A$ is invertible. I'm lost, I don't know if there's something more I have to learn to do this. I've gotten this far (I'm most likely not on the right track): $$ p(A) = a_0I+a_1A+a_2A^2+ ...+a_nA^n $$ $$ p(0) = a_0I+(a_1\cdot 0)+(a_2\cdot 0^2)+\ldots +(a_n\cdot 0^n) $$ $$ p(0) = a_oI$$ $$ p(A) = p(0)+a_1A+a_2A^2 +\ldots +a_nA^n $$ I don't quite know what to do further. I know that if $AX=B$, where $A$ is the square matrix, $B$ is a matrix vector, if there's only one solution $X$ for all $B$, then $A$ is invertible.","The question is: The square matrix $A$ satisfies $p(A) = 0$, where $p(x)$ is a polynomial such that $p(0) \ne 0$. Show that $A$ is invertible. I'm lost, I don't know if there's something more I have to learn to do this. I've gotten this far (I'm most likely not on the right track): $$ p(A) = a_0I+a_1A+a_2A^2+ ...+a_nA^n $$ $$ p(0) = a_0I+(a_1\cdot 0)+(a_2\cdot 0^2)+\ldots +(a_n\cdot 0^n) $$ $$ p(0) = a_oI$$ $$ p(A) = p(0)+a_1A+a_2A^2 +\ldots +a_nA^n $$ I don't quite know what to do further. I know that if $AX=B$, where $A$ is the square matrix, $B$ is a matrix vector, if there's only one solution $X$ for all $B$, then $A$ is invertible.",,"['linear-algebra', 'matrices']"
74,Is a square matrix whose diagonal and antidiagonal elements are all zero always singular?,Is a square matrix whose diagonal and antidiagonal elements are all zero always singular?,,"Consider an $n\times n$ matrix whose primary and secondary diagonal elements are all zero. Does it necessarily follow that the determinant vanishes for these matrices? When $n=1,2,3,4$, the matrix is necessarily singular.","Consider an $n\times n$ matrix whose primary and secondary diagonal elements are all zero. Does it necessarily follow that the determinant vanishes for these matrices? When $n=1,2,3,4$, the matrix is necessarily singular.",,['linear-algebra']
75,Kernel of Linear Functionals,Kernel of Linear Functionals,,"Problem: Prove that for all non zero linear functionials $f:M\to\mathbb{K}$ where $M$ is a vector space over field $\mathbb{K}$, subspace $(f^{-1}(0))$ is of co-dimension one. Could someone solve this for me?","Problem: Prove that for all non zero linear functionials $f:M\to\mathbb{K}$ where $M$ is a vector space over field $\mathbb{K}$, subspace $(f^{-1}(0))$ is of co-dimension one. Could someone solve this for me?",,['linear-algebra']
76,Inverse of a Product of Matrices: Why is $(AB)^{-1} = B^{-1} \cdot A^{-1}$?,Inverse of a Product of Matrices: Why is ?,(AB)^{-1} = B^{-1} \cdot A^{-1},"I'm having a bit of difficulty conceptualizing a rule for the inverse of a product of matrices and I'd appreciate any input on it. Suppose I let: $A^{-1} = \begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}$ and $B^{-1} = \begin{bmatrix}5 & 6 \\ 7 & 8 \end{bmatrix}$ From what I understand of the rule: $(AB)^{-1} = B^{-1} \cdot A^{-1}$ However while I understand matrix multiplication isn't necessarily commutative, I'm unclear on why this must be the case. It intuitively seems that simplifying the exponent would give me: $(AB)^{-1} = A^{-1} \cdot B^{-1}$ So why isn't this the case?","I'm having a bit of difficulty conceptualizing a rule for the inverse of a product of matrices and I'd appreciate any input on it. Suppose I let: $A^{-1} = \begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}$ and $B^{-1} = \begin{bmatrix}5 & 6 \\ 7 & 8 \end{bmatrix}$ From what I understand of the rule: $(AB)^{-1} = B^{-1} \cdot A^{-1}$ However while I understand matrix multiplication isn't necessarily commutative, I'm unclear on why this must be the case. It intuitively seems that simplifying the exponent would give me: $(AB)^{-1} = A^{-1} \cdot B^{-1}$ So why isn't this the case?",,[]
77,$A^{100} = 0$ implies $A^2 = 0$ when $A$ is $2\times 2$,implies  when  is,A^{100} = 0 A^2 = 0 A 2\times 2,"How to show the following claim $A^{100} = 0 \implies A^2 = 0$ with $A \in Mat(2 \times 2, K)$ If A is the matrix of a linear map $\phi$ then for all $v \in K^2$ the following identity should be true $\phi^{99}(v) = A^{99}\cdot v = A^{99} \cdot col_i(A) = 0$ But how to show that $A^2 = 0$?","How to show the following claim $A^{100} = 0 \implies A^2 = 0$ with $A \in Mat(2 \times 2, K)$ If A is the matrix of a linear map $\phi$ then for all $v \in K^2$ the following identity should be true $\phi^{99}(v) = A^{99}\cdot v = A^{99} \cdot col_i(A) = 0$ But how to show that $A^2 = 0$?",,"['linear-algebra', 'matrices']"
78,Prove that if $A - A^2 = I$ then $A$ has no real eigenvalues [closed],Prove that if  then  has no real eigenvalues [closed],A - A^2 = I A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given: $$ A \in M_{n\times n}(\mathbb R) \; , \; A - A^2 = I $$ Then we have to prove that $A$ does not have real eigenvalues. How do we prove such a thing?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given: $$ A \in M_{n\times n}(\mathbb R) \; , \; A - A^2 = I $$ Then we have to prove that $A$ does not have real eigenvalues. How do we prove such a thing?",,['linear-algebra']
79,Is the product of square singular and non singular matrices always singular?,Is the product of square singular and non singular matrices always singular?,,"Given $A,B\in R^{n\times n}$ such that A is singular, and B is non-singular.  Is $(AB)$ always singular?  If so, how do I prove it?","Given $A,B\in R^{n\times n}$ such that A is singular, and B is non-singular.  Is $(AB)$ always singular?  If so, how do I prove it?",,['linear-algebra']
80,Algebraic vector proof of Lagrange's identity,Algebraic vector proof of Lagrange's identity,,"$$|v · w| ^2 + |v × w| ^2 = |v|^2 |w|^2 $$ Edit Despite doing it multiple times it seems I have made a meal of the expansion see Jean-Claude's answer for a great explanation Using $v = (v_1,v_2,v_3)$ and $w = (w_1, w_2, w_3)$ i have expanded the LHS and gotten $$(v_2)^2(w_3)^2 + (v_3)^2(w_2)^2 + (v_3)^2(w_1)^2 + (v_1)^2(w_3)^2 + (v_1)^2(w_2)^2  + (v_2)^2(w_1)^2 +(v_1)^2(w_1)^2 + (v_2)^2(w_2)^2 + (v_3)^2(w_3)^2 -\mathbf{2(v_2 w_3 w_2 v_3 + v_3 w_1 v_1 w_3 + v_1 w_2 v_2 w_1)}$$ Now this is RHS minus the bolded terms and I dont know how to get rid of the bolded terms.","$$|v · w| ^2 + |v × w| ^2 = |v|^2 |w|^2 $$ Edit Despite doing it multiple times it seems I have made a meal of the expansion see Jean-Claude's answer for a great explanation Using $v = (v_1,v_2,v_3)$ and $w = (w_1, w_2, w_3)$ i have expanded the LHS and gotten $$(v_2)^2(w_3)^2 + (v_3)^2(w_2)^2 + (v_3)^2(w_1)^2 + (v_1)^2(w_3)^2 + (v_1)^2(w_2)^2  + (v_2)^2(w_1)^2 +(v_1)^2(w_1)^2 + (v_2)^2(w_2)^2 + (v_3)^2(w_3)^2 -\mathbf{2(v_2 w_3 w_2 v_3 + v_3 w_1 v_1 w_3 + v_1 w_2 v_2 w_1)}$$ Now this is RHS minus the bolded terms and I dont know how to get rid of the bolded terms.",,"['linear-algebra', 'vectors']"
81,Is this set of vectors linearly (in)dependent?,Is this set of vectors linearly (in)dependent?,,"I have the following problem: Are the following vectors linearly independent in $\mathbb{R}^2$? \begin{bmatrix} -1 \\ 2 \end{bmatrix}\begin{bmatrix} 1 \\ -2 \end{bmatrix}\begin{bmatrix} 2 \\ -4 \end{bmatrix} when I solve this using $c_1 v_1+c_2 v_2+ c_3 v_3=0$ I get an underdetermined system, can anyone help me to understand what this means for the linear independence? Thanks in advance :)","I have the following problem: Are the following vectors linearly independent in $\mathbb{R}^2$? \begin{bmatrix} -1 \\ 2 \end{bmatrix}\begin{bmatrix} 1 \\ -2 \end{bmatrix}\begin{bmatrix} 2 \\ -4 \end{bmatrix} when I solve this using $c_1 v_1+c_2 v_2+ c_3 v_3=0$ I get an underdetermined system, can anyone help me to understand what this means for the linear independence? Thanks in advance :)",,"['linear-algebra', 'vector-spaces']"
82,Does a scalar represent a linear map?,Does a scalar represent a linear map?,,"I have been wondering recently if a $1 \times 1$ matrix reprsents a scalar, and after doing some reading I'm still not satisfied. I've decided to ask this question from a different perspective: does a scalar represent a $1 \times 1$ map? Because any matrix is a linear map between two bases, that would imply that a $1 \times 1$ matrix is an isomorphism between two 1 dimensional spaces. So in the function/map $ f(x) = rx $ where r is some number, isn't $r$ just a scalar? This is a map and so can't this map be represented by the matrix $(r)$? Does that not, then, imply that a $1 \times 1$ and a scalar are the same? Obviously, multiplication between a $1 \times 1$ matrix and a $n\times m$ matrix where $n \neq 1$ is not defined, but $1 \times 1$ matrices seem to behave like scalars in all other cases, so why isn't an exception made? And what is the fundamental difference between a $1 \times 1$ map and a scalar, if there is one?","I have been wondering recently if a $1 \times 1$ matrix reprsents a scalar, and after doing some reading I'm still not satisfied. I've decided to ask this question from a different perspective: does a scalar represent a $1 \times 1$ map? Because any matrix is a linear map between two bases, that would imply that a $1 \times 1$ matrix is an isomorphism between two 1 dimensional spaces. So in the function/map $ f(x) = rx $ where r is some number, isn't $r$ just a scalar? This is a map and so can't this map be represented by the matrix $(r)$? Does that not, then, imply that a $1 \times 1$ and a scalar are the same? Obviously, multiplication between a $1 \times 1$ matrix and a $n\times m$ matrix where $n \neq 1$ is not defined, but $1 \times 1$ matrices seem to behave like scalars in all other cases, so why isn't an exception made? And what is the fundamental difference between a $1 \times 1$ map and a scalar, if there is one?",,['linear-algebra']
83,Prove that $\lambda$ is an eigenvalue of $A$ if and only if $\lambda$ is an eigenvalue of $A^T$.,Prove that  is an eigenvalue of  if and only if  is an eigenvalue of .,\lambda A \lambda A^T,"Prove that $\lambda$ is an eigenvalue of $A$ if and only if $\lambda$ is an eigenvalue of $A^T$. I'm stucked here, i've approached the problem by looking at $\det(A-\lambda I)=0\iff\det(A^T-\lambda I)=0$. I tried some cases, and I can see it when the matrix is triangular since the main diagonal remains the same when $A$ is transposed, but this hasn't shown me a way to proceed. Any hints or ideas?","Prove that $\lambda$ is an eigenvalue of $A$ if and only if $\lambda$ is an eigenvalue of $A^T$. I'm stucked here, i've approached the problem by looking at $\det(A-\lambda I)=0\iff\det(A^T-\lambda I)=0$. I tried some cases, and I can see it when the matrix is triangular since the main diagonal remains the same when $A$ is transposed, but this hasn't shown me a way to proceed. Any hints or ideas?",,[]
84,"Given a square matrix $A$, both $AA^T$ and $A^TA$ are symmetric","Given a square matrix , both  and  are symmetric",A AA^T A^TA,"I need help with a proof for my liner algebra class. If $A$ is a square matrix, then $AA^T$ and $A^TA$ are symmetric. I have no idea where to start!","I need help with a proof for my liner algebra class. If $A$ is a square matrix, then $AA^T$ and $A^TA$ are symmetric. I have no idea where to start!",,"['linear-algebra', 'matrices']"
85,"Find the determinant of $A + I$, where $A$ is a real matrix such that $AA^{\top}=I$ and $\det A<0$.","Find the determinant of , where  is a real matrix such that  and .",A + I A AA^{\top}=I \det A<0,"Given a real valued matrix $A$ that satisfies $AA^{\top} = I$ and $\det(A)<0$ , calculate $\det(A + I)$ . My start : Since $A$ satisfies $AA^{\top} = I$ , $A$ is a unitary matrix. The determinant of a unitary matrix with real entries is either $+1$ or $-1$ . Since we know that $\det(A)<0$ , it follows that $\det(A)=-1$ .","Given a real valued matrix that satisfies and , calculate . My start : Since satisfies , is a unitary matrix. The determinant of a unitary matrix with real entries is either or . Since we know that , it follows that .",A AA^{\top} = I \det(A)<0 \det(A + I) A AA^{\top} = I A +1 -1 \det(A)<0 \det(A)=-1,"['linear-algebra', 'matrices', 'determinant', 'orthogonality', 'unitary-matrices']"
86,"$1, e^{ix}, e^{-ix}$ are linearly independent",are linearly independent,"1, e^{ix}, e^{-ix}","Consider the space of all functions $f: \mathbb{R}\longrightarrow \mathbb{C}$. Prove that $\{1, e^{ix}, e^{-ix}\}$ are linearly independent vectors.","Consider the space of all functions $f: \mathbb{R}\longrightarrow \mathbb{C}$. Prove that $\{1, e^{ix}, e^{-ix}\}$ are linearly independent vectors.",,['linear-algebra']
87,Let $T:V→V$ be a linear transformation satisfying $T^2(v)=-v$ for all $v\in V$. How can we show that $n$ is even?,Let  be a linear transformation satisfying  for all . How can we show that  is even?,T:V→V T^2(v)=-v v\in V n,Let $V$ be a real n dimensional vector space & let $T:V→V$ be a linear transformation satisfying $T^2(v)=-v$ for all $v\in V$. Then how can we show that $n$ is even? I am completely stuck on it. Can anybody help me please?,Let $V$ be a real n dimensional vector space & let $T:V→V$ be a linear transformation satisfying $T^2(v)=-v$ for all $v\in V$. Then how can we show that $n$ is even? I am completely stuck on it. Can anybody help me please?,,['linear-algebra']
88,How to show that the vector subspaces of $\mathbb{R}^{n}$ are closed in $\mathbb{R}^{n}$?,How to show that the vector subspaces of  are closed in ?,\mathbb{R}^{n} \mathbb{R}^{n},The vector subspaces of $\mathbb{R}^{n}$ are closed in $\mathbb{R}^{n}$. How to show this?,The vector subspaces of $\mathbb{R}^{n}$ are closed in $\mathbb{R}^{n}$. How to show this?,,"['linear-algebra', 'general-topology', 'vector-spaces']"
89,How to show that $(W^\bot)^\bot=W$ (in a finite dimensional vector space),How to show that  (in a finite dimensional vector space),(W^\bot)^\bot=W,"I need to prove that if $V$ is a finite dimensional vector space over a field K with a non-degenerate inner-product and $W\subset V$ is a subspace of V, then: $$ (W^\bot)^\bot=W $$ Here is my approach: If $\langle\cdot,\cdot\rangle$ is the non-degenerate inner product of $V$ and $B={w_1, ... , w_n}$ is a base of $V$ where ${w_1, ... , w_r}$ is a base of $W$ then I showed that  $$ \langle u,v\rangle=[u]^T_BA[v]_B $$ for a symmetric, invertible matrix $A\in\mathbb{R}^{n\times n}$. Then $W^\bot$ is the solution space of $A_rx=0$ where $A_r\in\mathbb{R}^{r\times n}$ is the matrix of the first $r$ lines of $A$. Is all this true? I tried to exploit this but wasn't able to do so. How to proceed further?","I need to prove that if $V$ is a finite dimensional vector space over a field K with a non-degenerate inner-product and $W\subset V$ is a subspace of V, then: $$ (W^\bot)^\bot=W $$ Here is my approach: If $\langle\cdot,\cdot\rangle$ is the non-degenerate inner product of $V$ and $B={w_1, ... , w_n}$ is a base of $V$ where ${w_1, ... , w_r}$ is a base of $W$ then I showed that  $$ \langle u,v\rangle=[u]^T_BA[v]_B $$ for a symmetric, invertible matrix $A\in\mathbb{R}^{n\times n}$. Then $W^\bot$ is the solution space of $A_rx=0$ where $A_r\in\mathbb{R}^{r\times n}$ is the matrix of the first $r$ lines of $A$. Is all this true? I tried to exploit this but wasn't able to do so. How to proceed further?",,"['linear-algebra', 'inner-products', 'orthogonality']"
90,Shortest and most elementary proof that the product of an $n$-column and an $n$-row has determinant $0$,Shortest and most elementary proof that the product of an -column and an -row has determinant,n n 0,"Let $\bf u$ be any column vector and $\bf v$ be any row vector, each with $n \geq 2$ arbitrary entries from a field. Then it is well known that ${\bf u} {\bf v}$ is an $n \times n$ matrix such that $\det({\bf uv})=0$. I am curious to know the $\bf shortest$ and $\bf most~elementary$ proof of this result, say understandable by a (good) high school student. I have one in mind, but to make this  interesting, perhaps I should let you present your version first? UPDATE: Someone already presented (below) the same proof I had in mind. But let's see if there is a simpler proof; finding one is the main motivation here.","Let $\bf u$ be any column vector and $\bf v$ be any row vector, each with $n \geq 2$ arbitrary entries from a field. Then it is well known that ${\bf u} {\bf v}$ is an $n \times n$ matrix such that $\det({\bf uv})=0$. I am curious to know the $\bf shortest$ and $\bf most~elementary$ proof of this result, say understandable by a (good) high school student. I have one in mind, but to make this  interesting, perhaps I should let you present your version first? UPDATE: Someone already presented (below) the same proof I had in mind. But let's see if there is a simpler proof; finding one is the main motivation here.",,"['linear-algebra', 'matrices', 'determinant']"
91,Matrix associated to a linear transformation with respect to a given basis,Matrix associated to a linear transformation with respect to a given basis,,"We're given $L: \mathbb{R}^3 \rightarrow \mathbb{R}^3$, which is a linear transformation defined by: $$ L \left( \begin{bmatrix}x_{1}\\x_{2}\\x_{3} \end{bmatrix} \right) = \begin{bmatrix}4x_{3}\\3x_{1}+5x_{2}-2x_{3}\\x_{1}+x_{2}+4x_{3} \end{bmatrix}$$ We're also given a basis $$B = \left(\begin{bmatrix}1\\1\\1 \end{bmatrix} , \begin{bmatrix}1\\1\\0 \end{bmatrix}, \begin{bmatrix}1\\0\\0 \end{bmatrix} \right) $$ The point of the exercise is to find a matrix $M$ associated to $L$ with respect to $B$, such that $[L(x)]_{B} = M[x]_{B}$ for all $x\in \mathbb{R}^3$. Here's my approach. I transformed the vectors in $B$ to get the columns of the matrix $M$ giving me: $$M =  \begin{bmatrix}4 & 0 & 0\\ 6&8&3\\6&2&1 \end{bmatrix} $$ The solution sheed says that $M =\begin{bmatrix}6&2&1\\ 0&6&2\\-2&-8&-3 \end{bmatrix}$. I have no idea how to find the the correct solution nor why my approach is wrong.","We're given $L: \mathbb{R}^3 \rightarrow \mathbb{R}^3$, which is a linear transformation defined by: $$ L \left( \begin{bmatrix}x_{1}\\x_{2}\\x_{3} \end{bmatrix} \right) = \begin{bmatrix}4x_{3}\\3x_{1}+5x_{2}-2x_{3}\\x_{1}+x_{2}+4x_{3} \end{bmatrix}$$ We're also given a basis $$B = \left(\begin{bmatrix}1\\1\\1 \end{bmatrix} , \begin{bmatrix}1\\1\\0 \end{bmatrix}, \begin{bmatrix}1\\0\\0 \end{bmatrix} \right) $$ The point of the exercise is to find a matrix $M$ associated to $L$ with respect to $B$, such that $[L(x)]_{B} = M[x]_{B}$ for all $x\in \mathbb{R}^3$. Here's my approach. I transformed the vectors in $B$ to get the columns of the matrix $M$ giving me: $$M =  \begin{bmatrix}4 & 0 & 0\\ 6&8&3\\6&2&1 \end{bmatrix} $$ The solution sheed says that $M =\begin{bmatrix}6&2&1\\ 0&6&2\\-2&-8&-3 \end{bmatrix}$. I have no idea how to find the the correct solution nor why my approach is wrong.",,"['linear-algebra', 'linear-transformations', 'change-of-basis']"
92,Does every vector have an additive inverse?,Does every vector have an additive inverse?,,"I'm taking a linear algebra class at my university and recently we have been talking about the multiplicative inverse of matrices and how some matrices do not have multiplicative inverses (i.e. matrices that aren't square). This made me wonder whether vectors have inverses as well, specifically additive inverses. Isn't the additive inverse of a vector just the negative of that same vector, and isn't the sum of a vector and its additive inverse just the zero vector? In that case, is the statement below true in all cases? And does the zero vector have an inverse? ""Every vector must have an additive inverse, the sum of these two vectors being the zero vector.""","I'm taking a linear algebra class at my university and recently we have been talking about the multiplicative inverse of matrices and how some matrices do not have multiplicative inverses (i.e. matrices that aren't square). This made me wonder whether vectors have inverses as well, specifically additive inverses. Isn't the additive inverse of a vector just the negative of that same vector, and isn't the sum of a vector and its additive inverse just the zero vector? In that case, is the statement below true in all cases? And does the zero vector have an inverse? ""Every vector must have an additive inverse, the sum of these two vectors being the zero vector.""",,"['linear-algebra', 'vectors']"
93,Can product of two singular matrices be invertible?,Can product of two singular matrices be invertible?,,"Suppose $A,B$ are square matrices of size $n\times n$.  Can $AB$ be invertible, even though both $A$ and $B$ are singular (not invertible)? And if not, does it follow that if $A_1 \times A_2 \times ... \times A_n$ is the invertible product of square matrices, then all factors $A_i$ are invertible?","Suppose $A,B$ are square matrices of size $n\times n$.  Can $AB$ be invertible, even though both $A$ and $B$ are singular (not invertible)? And if not, does it follow that if $A_1 \times A_2 \times ... \times A_n$ is the invertible product of square matrices, then all factors $A_i$ are invertible?",,"['linear-algebra', 'matrices']"
94,Is a matrix with characteristic polynomial $t^2 +1$ invertible?,Is a matrix with characteristic polynomial  invertible?,t^2 +1,"Given that $A$ is a square matrix with characteristic polynomial $t^2+1$, is $A$ invertible? I'm not sure, but this question seems to depend on whether $A$ is over $\mathbb{R}$ or over $\mathbb{C}$.  My reasoning is that if $A$ is over $\mathbb{C}$ then $A$ has two distinct eigenvalues $-i$ and $i$ and is diagonalizable.  Since it's diagonalization is invertible, $A$ is also invertible. However if $A$ is over $\mathbb{R}$ then $A$ has no eigenvalues and therefore... I don't know where to go from there.","Given that $A$ is a square matrix with characteristic polynomial $t^2+1$, is $A$ invertible? I'm not sure, but this question seems to depend on whether $A$ is over $\mathbb{R}$ or over $\mathbb{C}$.  My reasoning is that if $A$ is over $\mathbb{C}$ then $A$ has two distinct eigenvalues $-i$ and $i$ and is diagonalizable.  Since it's diagonalization is invertible, $A$ is also invertible. However if $A$ is over $\mathbb{R}$ then $A$ has no eigenvalues and therefore... I don't know where to go from there.",,['linear-algebra']
95,"Given $A^2+cA+cI=0$, how to find inverse of $A+(c-1)I$?","Given , how to find inverse of ?",A^2+cA+cI=0 A+(c-1)I,"Suppose a square matrix $A$ such that $A^2+cA+cI=0$ for all $c \in \mathbb{Z}$. How can I show that $A+(c-1)I$ is invertible and find its inverse? I started off this way: $A+(c-1)I = A+cI-I$ Then $(A+cI-I)(d_1A+d_2I)=I$, where $d_1, d_2 \in \mathbb{Z}$. Expand it and it becomes: $d_1A^2+d_1cA-d_1A+d_2A+(c-1)d_2I=I$ $\Rightarrow   (c-1)d_2=1 \;\; and \; \; d_1A^2+d_1cA-d_1A+d_2A=0$ $\Rightarrow  d_2=\frac{1}{(c-1)}$ Continue from $d_1A^2+d_1cA-d_1A+d_2A=0$, after some manipulation, I got $d_1(A^2+cA+cI)-d_1cI-d_1A+d_2A=0$. Since given that $A^2+cA+cI=0$, $d_1(A^2+cA+cI)-d_1cI-d_1A+d_2A=0$ $\Rightarrow  -d_1cI-d_1A+d_2A=0$ $\Rightarrow  -d_1cI-d_1A+\frac{1}{c-1}A=0$ $\Rightarrow  -d_1cI-d_1A=-\frac{1}{c-1}A$ $\Rightarrow  d_1(cI+A)=\frac{1}{c-1}A$ At this point, since I don't know if $A$ is invertible yet, I cannot do it as $d_1=\frac{1}{c-1}\frac{A}{(cI+A)}$. Even if I did this, I still cannot find a value for $d_1$ and $d_2$ to find the inverse of $A+(c-1)I$. How should I continue from here?","Suppose a square matrix $A$ such that $A^2+cA+cI=0$ for all $c \in \mathbb{Z}$. How can I show that $A+(c-1)I$ is invertible and find its inverse? I started off this way: $A+(c-1)I = A+cI-I$ Then $(A+cI-I)(d_1A+d_2I)=I$, where $d_1, d_2 \in \mathbb{Z}$. Expand it and it becomes: $d_1A^2+d_1cA-d_1A+d_2A+(c-1)d_2I=I$ $\Rightarrow   (c-1)d_2=1 \;\; and \; \; d_1A^2+d_1cA-d_1A+d_2A=0$ $\Rightarrow  d_2=\frac{1}{(c-1)}$ Continue from $d_1A^2+d_1cA-d_1A+d_2A=0$, after some manipulation, I got $d_1(A^2+cA+cI)-d_1cI-d_1A+d_2A=0$. Since given that $A^2+cA+cI=0$, $d_1(A^2+cA+cI)-d_1cI-d_1A+d_2A=0$ $\Rightarrow  -d_1cI-d_1A+d_2A=0$ $\Rightarrow  -d_1cI-d_1A+\frac{1}{c-1}A=0$ $\Rightarrow  -d_1cI-d_1A=-\frac{1}{c-1}A$ $\Rightarrow  d_1(cI+A)=\frac{1}{c-1}A$ At this point, since I don't know if $A$ is invertible yet, I cannot do it as $d_1=\frac{1}{c-1}\frac{A}{(cI+A)}$. Even if I did this, I still cannot find a value for $d_1$ and $d_2$ to find the inverse of $A+(c-1)I$. How should I continue from here?",,"['linear-algebra', 'matrices']"
96,Is my proof that $(A^n)^{-1} = (A^{-1})^n$ correct?,Is my proof that  correct?,(A^n)^{-1} = (A^{-1})^n,"I am still learning Linear Algebra at it's basic levels, and I encountered a theorem about invertible matrices that stated that: If $A$ is an invertible matrix, then for $n=0,1,2,3,..$. $A^n$ is invertible and $(A^n)^{-1} = (A^{-1})^n$. Now, in attempting to write my proof, I proceeded this way (note that it's not complete): $$A^n(A^{-1})^n=\prod_{i=1}^nA\prod_{i=1}^nA^{-1}=\prod_{i=1}^n(AA^{-1})=\prod_{i=1}^nI=I$$ Is this line of thinking correct? Well, am just returning to math after a long time of little practice, so I could be wrong. Based on my comment to Dimitri's answer, would my use of this argument improve my proof? $$\prod_{i=1}^{n-1}A.(AA^{-1}).\prod_{i=1}^{n-1}=\prod_{i=1}^{n-1}A.(I).\prod_{i=1}^{n-1}=...=A.(AIA^{-1}).A^{-1}=AIA^{-1}=AA^{-1}=I$$ After checking the comments, it seems this last argument gives me a correct proof eventually, and I now see that the problem with my original approach was making the argument that: $$\prod_{i=1}^nA\prod_{i=1}^nA^{-1}=\prod_{i=1}^n(AA^{-1})$$ Which is not necessarily correct , but like @Srivatsan demonstrates, that approach is not at all wrong since : Notice that $A$ and $A^{−1}$ commute, so this justifies your proof now Thanks to everyone for guidance, now I see why collaboration is going to make me love math :D","I am still learning Linear Algebra at it's basic levels, and I encountered a theorem about invertible matrices that stated that: If $A$ is an invertible matrix, then for $n=0,1,2,3,..$. $A^n$ is invertible and $(A^n)^{-1} = (A^{-1})^n$. Now, in attempting to write my proof, I proceeded this way (note that it's not complete): $$A^n(A^{-1})^n=\prod_{i=1}^nA\prod_{i=1}^nA^{-1}=\prod_{i=1}^n(AA^{-1})=\prod_{i=1}^nI=I$$ Is this line of thinking correct? Well, am just returning to math after a long time of little practice, so I could be wrong. Based on my comment to Dimitri's answer, would my use of this argument improve my proof? $$\prod_{i=1}^{n-1}A.(AA^{-1}).\prod_{i=1}^{n-1}=\prod_{i=1}^{n-1}A.(I).\prod_{i=1}^{n-1}=...=A.(AIA^{-1}).A^{-1}=AIA^{-1}=AA^{-1}=I$$ After checking the comments, it seems this last argument gives me a correct proof eventually, and I now see that the problem with my original approach was making the argument that: $$\prod_{i=1}^nA\prod_{i=1}^nA^{-1}=\prod_{i=1}^n(AA^{-1})$$ Which is not necessarily correct , but like @Srivatsan demonstrates, that approach is not at all wrong since : Notice that $A$ and $A^{−1}$ commute, so this justifies your proof now Thanks to everyone for guidance, now I see why collaboration is going to make me love math :D",,"['linear-algebra', 'proof-writing']"
97,What are we solving with row reduction?,What are we solving with row reduction?,,Why are row reductions so useful in linear algebra? It is easy to get lost in the mechanical solving of equations. I know I can get a matrix into reduced row echelon form. But what are the outcomes of this? What can this mean?,Why are row reductions so useful in linear algebra? It is easy to get lost in the mechanical solving of equations. I know I can get a matrix into reduced row echelon form. But what are the outcomes of this? What can this mean?,,['linear-algebra']
98,How to get the equation where a circle goes through three points [duplicate],How to get the equation where a circle goes through three points [duplicate],,"This question already has answers here : find the equation of the circle $x^2 +y^2 +ax +by = c$ passing through points $(6,8), (8,4), (3,9)$ (3 answers) Closed 5 years ago . If I have the equation $ax^2+ay^2+bx+cy+d=0$ how do I get the equation where the circumference goes through the points P = (1,1), Q = (−1,−1) and   R = (−1,1) I have it in mind to solve it with a matrix, but the instructions seem confusing to me, can I get some help?","This question already has answers here : find the equation of the circle $x^2 +y^2 +ax +by = c$ passing through points $(6,8), (8,4), (3,9)$ (3 answers) Closed 5 years ago . If I have the equation $ax^2+ay^2+bx+cy+d=0$ how do I get the equation where the circumference goes through the points P = (1,1), Q = (−1,−1) and   R = (−1,1) I have it in mind to solve it with a matrix, but the instructions seem confusing to me, can I get some help?",,['linear-algebra']
99,Given a matrix $A$ find $A^n$.,Given a matrix  find .,A A^n,$A=$$     \begin{bmatrix}     1 & 2\\      0 & 1       \end{bmatrix} $ Find $A^n$. My input: $A^2= \begin{bmatrix}     1 & 2\\      0 & 1       \end{bmatrix}  \begin{bmatrix}     1 & 2\\      0 & 1       \end{bmatrix}  = \begin{bmatrix}     1 & 4\\      0 & 1       \end{bmatrix} $ $A^3 = \begin{bmatrix}     1 & 6\\      0 & 1       \end{bmatrix} $ ...... $A^n = \begin{bmatrix}     1 & 2n\\      0 & 1       \end{bmatrix} $ This was very basic approach. I want to know if there is any other way a smart trick or something to solve this problem ?,$A=$$     \begin{bmatrix}     1 & 2\\      0 & 1       \end{bmatrix} $ Find $A^n$. My input: $A^2= \begin{bmatrix}     1 & 2\\      0 & 1       \end{bmatrix}  \begin{bmatrix}     1 & 2\\      0 & 1       \end{bmatrix}  = \begin{bmatrix}     1 & 4\\      0 & 1       \end{bmatrix} $ $A^3 = \begin{bmatrix}     1 & 6\\      0 & 1       \end{bmatrix} $ ...... $A^n = \begin{bmatrix}     1 & 2n\\      0 & 1       \end{bmatrix} $ This was very basic approach. I want to know if there is any other way a smart trick or something to solve this problem ?,,"['linear-algebra', 'matrices']"
