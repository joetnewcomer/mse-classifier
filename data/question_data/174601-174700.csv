,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Geometrical interpretation of the Triple Product Rule,Geometrical interpretation of the Triple Product Rule,,"Most of us who have done multivariable calculus are familiar with the rule $$ \left( \frac{\partial x}{\partial y} \right)_{z} \left( \frac{\partial y}{\partial z} \right)_{x} \left( \frac{\partial z}{\partial x} \right)_{y} = -1, $$ particularly useful in thermodynamics. The normal way to prove this (as shown in either of these Wikipedia articles is by mucking about with the algebra of differentials and substituting, and there's nothing as geometrically unintuitive as pushing algebraic symbols around. The $-1$ is also famously counterintuitive to people seeing it for the first time. Therefore, is there a nice intuitive geometrical derivation or interpretation of this result? (And, indeed, its more-variabled generalisations?)","Most of us who have done multivariable calculus are familiar with the rule $$ \left( \frac{\partial x}{\partial y} \right)_{z} \left( \frac{\partial y}{\partial z} \right)_{x} \left( \frac{\partial z}{\partial x} \right)_{y} = -1, $$ particularly useful in thermodynamics. The normal way to prove this (as shown in either of these Wikipedia articles is by mucking about with the algebra of differentials and substituting, and there's nothing as geometrically unintuitive as pushing algebraic symbols around. The $-1$ is also famously counterintuitive to people seeing it for the first time. Therefore, is there a nice intuitive geometrical derivation or interpretation of this result? (And, indeed, its more-variabled generalisations?)",,"['multivariable-calculus', 'partial-derivative']"
1,Finding roots and factors of multivariate polynomials,Finding roots and factors of multivariate polynomials,,"I know that in order to factor a one dimensional polynomial one can find the roots with some method, for instance a numerical newton method. Then one can systematically divide with $(variable-root)$ for each root found and then be done. Is there any analogous way to do this for multivariate polynomials? Does there exist any ""unique"" or ""natural"" factorization for those? It is obvious we can do this in the case our polynomial is separable i.e. $$P(x_1,x_2,\cdots,x_n) = P_1(x_1)P_2(x_2) \cdots P_n(x_n)$$ because then we could just factor each $P_k(x_k)$ separately. But what about the general case?","I know that in order to factor a one dimensional polynomial one can find the roots with some method, for instance a numerical newton method. Then one can systematically divide with $(variable-root)$ for each root found and then be done. Is there any analogous way to do this for multivariate polynomials? Does there exist any ""unique"" or ""natural"" factorization for those? It is obvious we can do this in the case our polynomial is separable i.e. $$P(x_1,x_2,\cdots,x_n) = P_1(x_1)P_2(x_2) \cdots P_n(x_n)$$ because then we could just factor each $P_k(x_k)$ separately. But what about the general case?",,"['multivariable-calculus', 'polynomials', 'factoring']"
2,Finding the points at which a surface has horizontal tangent planes,Finding the points at which a surface has horizontal tangent planes,,"Find the points at which the surface  $$ x^2 +2y^2+z^2 -2x -2z -2 = 0 $$ has horizontal tangent planes. Find the equation of these tangent planes. I found that $$ \nabla f = (2x-2,4y) $$ I'm thinking that the gradient vector must be equal to $(0,0)$ so $x = 1, y=0$ which implies $z = 3,-1$. So the points are $(1,0,3)$ and $(1,0,-1)$. How would we write the equation of these tangent planes?","Find the points at which the surface  $$ x^2 +2y^2+z^2 -2x -2z -2 = 0 $$ has horizontal tangent planes. Find the equation of these tangent planes. I found that $$ \nabla f = (2x-2,4y) $$ I'm thinking that the gradient vector must be equal to $(0,0)$ so $x = 1, y=0$ which implies $z = 3,-1$. So the points are $(1,0,3)$ and $(1,0,-1)$. How would we write the equation of these tangent planes?",,['multivariable-calculus']
3,Rankine Hugoniot Jump Condition Derivation,Rankine Hugoniot Jump Condition Derivation,,I follow the majority of this derivation I just do not understand where the two terms highlighted in green come from.,I follow the majority of this derivation I just do not understand where the two terms highlighted in green come from.,,['multivariable-calculus']
4,Lagrange Taylor remainder: can we choose $t^*$ continuously?,Lagrange Taylor remainder: can we choose  continuously?,t^*,"The Taylor theorem with Lagrange remainder tells us that for $f: \mathbb{R}^n \to \mathbb{R}$ twice differentiable (we can assume $C^2$ if we like), $$f(y) - f(x) =  \left\langle \nabla f(x), y-x \right\rangle + \frac12 \left\langle \nabla^2f(x^*)(y-x), y-x\right\rangle$$ for $x^* $ strictly between $x$ and $y$. My question is, is it always possible to choose $x^*$ continuously as $y\to x$ (along the straight line from $y$ to $x$, let's say to start)? The question reduces to the 1d case if we just look at $f(x+t(y-x))$.","The Taylor theorem with Lagrange remainder tells us that for $f: \mathbb{R}^n \to \mathbb{R}$ twice differentiable (we can assume $C^2$ if we like), $$f(y) - f(x) =  \left\langle \nabla f(x), y-x \right\rangle + \frac12 \left\langle \nabla^2f(x^*)(y-x), y-x\right\rangle$$ for $x^* $ strictly between $x$ and $y$. My question is, is it always possible to choose $x^*$ continuously as $y\to x$ (along the straight line from $y$ to $x$, let's say to start)? The question reduces to the 1d case if we just look at $f(x+t(y-x))$.",,"['multivariable-calculus', 'taylor-expansion']"
5,Sphere volume and generalized Stokes' theorem,Sphere volume and generalized Stokes' theorem,,"The area of a circle is $\pi r^2$, and the circumference is the derivative of this: $2\pi r$. The same holds in one higher dimension: the volume of a sphere is $\frac{4}{3} \pi r ^3$ and the derivative of this is the surface area: $4\pi r^2$. These make sense to me geometrically - if you increase the volume by a tiny bit $dr$, the increase in volume will be in proportion to the surface area. The generalized stokes theorem says that for a $(k-1)$-form $\omega$ on a $k$-dimensional manifold $M$ with boundary: $$\int_M d\omega = \int_{\partial M} \omega$$ Here, the derivative is the volume form -- which is the opposite of what I observed from the geometric formulas above.  Is there a relationship between these formulas which are derivatives of each other and the generalized stokes theorem?","The area of a circle is $\pi r^2$, and the circumference is the derivative of this: $2\pi r$. The same holds in one higher dimension: the volume of a sphere is $\frac{4}{3} \pi r ^3$ and the derivative of this is the surface area: $4\pi r^2$. These make sense to me geometrically - if you increase the volume by a tiny bit $dr$, the increase in volume will be in proportion to the surface area. The generalized stokes theorem says that for a $(k-1)$-form $\omega$ on a $k$-dimensional manifold $M$ with boundary: $$\int_M d\omega = \int_{\partial M} \omega$$ Here, the derivative is the volume form -- which is the opposite of what I observed from the geometric formulas above.  Is there a relationship between these formulas which are derivatives of each other and the generalized stokes theorem?",,"['multivariable-calculus', 'differential-forms']"
6,Solving laplace's equation for an inviscid and incompresible fluid,Solving laplace's equation for an inviscid and incompresible fluid,,"Background I'm working on a 2D inviscid, incompressible fluid sim using vortex methods (that is, treating vortex as discrete particles), and I'm trying to (numerically) solve the no-through boundary conditions, which simply say that the fluid isn't allowed to pass through boundaries of the fluid (which means either the walls of its container or bodies floating in the fluid).  A couple of different sources I have suggest one way of doing it: construct a scalar field such that its gradient stops the fluid flow at the boundaries.  ie: $$\nabla \Phi \cdot \mathbf{n} = (\mathbf{u_b} - \mathbf{u_\omega}) \cdot \mathbf{n}$$ where $\Phi$ is the scalar field I need to find, $\mathbf{u_\omega}$ is the velocity field of the fluid without the boundary conditions, $\mathbf{u_b}$ is the velocity of the boundary itself, and $\mathbf{n}$ is the normalized surface normal of the boundary. Because the flow is incompressible, $\Delta \Phi = 0$ (ie: Laplace's equation).  A source I have (""Vortex Methods, Theory and Practice"") says you can solve this using a superposition of Green's functions.  This looks like: $$\frac{\partial \Phi}{\partial \mathbf{n}}(\mathbf{x_b}) = \pm \frac{1}{2} \sigma(\mathbf{x_b}) + \int_S{[\sigma(\mathbf{x_b'}) G(\mathbf{x_b} - \mathbf{x_b'}) -\mu(\mathbf{x_b'}) \mathbf{n} \cdot \nabla G(\mathbf{x_b} - \mathbf{x_b'})] dS(\mathbf{x_b'})}$$ where $\mathbf{x_b}$ is a point on the boundary, $G(\mathbf{x_b} - \mathbf{x_b'})$ is Green's function, and in 2D is $-\frac{\log(|\mathbf{x_b} - \mathbf{x_b'}|)}{2 \pi}$, and $S$ is the boundary I'm trying to prevent the fluid from going through (ie: in 2D it's a polygon's border).  Also: $$\mu = \Phi_e - \Phi_i$$ That is, it's the difference in the potential ($\Phi$) on opposite sides of the boundary.  The book refers to this as the ""double-layer potential"".  And: $$\sigma = \frac{\partial \Phi_e}{\partial \mathbf{n}} - \frac{\partial \Phi_i}{\partial \mathbf{n}} $$ The book refers to this as the single-layer potential or ""source"". Then the book assumes that the potential is continuous, so we can say that $\mu = 0$.  A large term in the integral above cancels, and we're finally left with: Final equation $$(\mathbf{u_b} - \mathbf{u_\omega}) \cdot \mathbf{n}(\mathbf{x_b}) = -\frac{1}{2} \sigma(\mathbf{x_b}) + \int_S{\sigma(\mathbf{x_b'}) G(\mathbf{x_b} - \mathbf{x_b'}) dS(\mathbf{x_b'})}$$ Questions First, if anyone has any background in fluids or potential, is the above description correct?  This isn't math I was ever formally taught and I'm just kind of piecing it together as I go. Also if I understand correctly, in order to actually solve this I need to ""discretize"" this integral to turn it in to a system of algebraic equations, form a big matrix, and solve it for the $\sigma$ terms.  One simple way of doing this would be to simply select $n$ evenly spaced points along the border of the polygon (is that right?). However, I don't understand what the $\sigma$ term actually is.  In the definition it looks like it's the Jacobian of $\Phi$ with respect to the normal, which would make it a row vector.  But in the above final equation, it seems to be treating it as a scalar term. Also, once I have the $\sigma$ terms, what do I actually do with them?  Ultimately I want to calculate $\nabla \Phi (\mathbf{x})$ for some arbitrary point in space $\mathbf{x}$, but I don't see how to do that once you have the $\sigma$ terms. I found an article on the double-layer potential with math that looks very familiar, so I'm guessing this is a well understood problem, but I'm certainly not understanding it :).  If someone can walk me through the final pieces of math so I can get to a point where I can actually program this, I'd appreciate it.","Background I'm working on a 2D inviscid, incompressible fluid sim using vortex methods (that is, treating vortex as discrete particles), and I'm trying to (numerically) solve the no-through boundary conditions, which simply say that the fluid isn't allowed to pass through boundaries of the fluid (which means either the walls of its container or bodies floating in the fluid).  A couple of different sources I have suggest one way of doing it: construct a scalar field such that its gradient stops the fluid flow at the boundaries.  ie: $$\nabla \Phi \cdot \mathbf{n} = (\mathbf{u_b} - \mathbf{u_\omega}) \cdot \mathbf{n}$$ where $\Phi$ is the scalar field I need to find, $\mathbf{u_\omega}$ is the velocity field of the fluid without the boundary conditions, $\mathbf{u_b}$ is the velocity of the boundary itself, and $\mathbf{n}$ is the normalized surface normal of the boundary. Because the flow is incompressible, $\Delta \Phi = 0$ (ie: Laplace's equation).  A source I have (""Vortex Methods, Theory and Practice"") says you can solve this using a superposition of Green's functions.  This looks like: $$\frac{\partial \Phi}{\partial \mathbf{n}}(\mathbf{x_b}) = \pm \frac{1}{2} \sigma(\mathbf{x_b}) + \int_S{[\sigma(\mathbf{x_b'}) G(\mathbf{x_b} - \mathbf{x_b'}) -\mu(\mathbf{x_b'}) \mathbf{n} \cdot \nabla G(\mathbf{x_b} - \mathbf{x_b'})] dS(\mathbf{x_b'})}$$ where $\mathbf{x_b}$ is a point on the boundary, $G(\mathbf{x_b} - \mathbf{x_b'})$ is Green's function, and in 2D is $-\frac{\log(|\mathbf{x_b} - \mathbf{x_b'}|)}{2 \pi}$, and $S$ is the boundary I'm trying to prevent the fluid from going through (ie: in 2D it's a polygon's border).  Also: $$\mu = \Phi_e - \Phi_i$$ That is, it's the difference in the potential ($\Phi$) on opposite sides of the boundary.  The book refers to this as the ""double-layer potential"".  And: $$\sigma = \frac{\partial \Phi_e}{\partial \mathbf{n}} - \frac{\partial \Phi_i}{\partial \mathbf{n}} $$ The book refers to this as the single-layer potential or ""source"". Then the book assumes that the potential is continuous, so we can say that $\mu = 0$.  A large term in the integral above cancels, and we're finally left with: Final equation $$(\mathbf{u_b} - \mathbf{u_\omega}) \cdot \mathbf{n}(\mathbf{x_b}) = -\frac{1}{2} \sigma(\mathbf{x_b}) + \int_S{\sigma(\mathbf{x_b'}) G(\mathbf{x_b} - \mathbf{x_b'}) dS(\mathbf{x_b'})}$$ Questions First, if anyone has any background in fluids or potential, is the above description correct?  This isn't math I was ever formally taught and I'm just kind of piecing it together as I go. Also if I understand correctly, in order to actually solve this I need to ""discretize"" this integral to turn it in to a system of algebraic equations, form a big matrix, and solve it for the $\sigma$ terms.  One simple way of doing this would be to simply select $n$ evenly spaced points along the border of the polygon (is that right?). However, I don't understand what the $\sigma$ term actually is.  In the definition it looks like it's the Jacobian of $\Phi$ with respect to the normal, which would make it a row vector.  But in the above final equation, it seems to be treating it as a scalar term. Also, once I have the $\sigma$ terms, what do I actually do with them?  Ultimately I want to calculate $\nabla \Phi (\mathbf{x})$ for some arbitrary point in space $\mathbf{x}$, but I don't see how to do that once you have the $\sigma$ terms. I found an article on the double-layer potential with math that looks very familiar, so I'm guessing this is a well understood problem, but I'm certainly not understanding it :).  If someone can walk me through the final pieces of math so I can get to a point where I can actually program this, I'd appreciate it.",,"['multivariable-calculus', 'numerical-methods', 'physics', 'fluid-dynamics', 'harmonic-functions']"
7,How do you show that the Laplacian is the square of the (Euclidean) Dirac operator?,How do you show that the Laplacian is the square of the (Euclidean) Dirac operator?,,"If I understand correctly, the Euclidean Dirac operator is given by $$D=\sum_{i=1}^n e_i \frac{\partial}{\partial x_i},$$ where $e_i$ are bases for $Cl_{0,n}(\mathbb{R})$, i.e., the $n$-dimensional Clifford algebra with negative-definite signature over the reals (so $e_i^2=-1$), and $x_i$ are the corresponding coordinates.  Several sources state that $D^2 = -\Delta_n$ where $\Delta_n$ is the standard Euclidean Laplace operator $$\Delta_n = \sum_{i=1}^n \frac{\partial^2}{\partial x_i^2}.$$ When I write out $D^2 f$ explicitly for some function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, scalar terms from the Laplacian certainly appear, e.g., $$e_1 \frac{\partial}{\partial x_1}\left( e_1 \frac{\partial}{\partial x_1} f \right) = e_1 \left( e_1 \frac{\partial^2}{\partial x_1^2}f + \left(\frac{\partial}{\partial x_1}e_1\right)\frac{\partial}{\partial x_1}f \right)=e_1^2 \frac{\partial^2}{\partial x_1^2}f = -\frac{\partial^2}{\partial x_1^2}f.$$ But I also end up with bivector cross terms that shouldn't be there: $$e_1 \frac{\partial}{\partial x_1}\left( e_2 \frac{\partial}{\partial x_2} f \right) = e_1 \left( e_2 \frac{\partial^2}{\partial x_1 \partial x_2}f + \left(\frac{\partial}{\partial x_1}e_2\right)\frac{\partial}{\partial x_2}f \right)=e_1 e_2 \frac{\partial^2}{\partial x_1 \partial x_2}f = e_{12}\frac{\partial^2}{\partial x_1 \partial x_2}f.$$ Should I only be considering the scalar part of $D^2$, or am I simply doing something wrong here?","If I understand correctly, the Euclidean Dirac operator is given by $$D=\sum_{i=1}^n e_i \frac{\partial}{\partial x_i},$$ where $e_i$ are bases for $Cl_{0,n}(\mathbb{R})$, i.e., the $n$-dimensional Clifford algebra with negative-definite signature over the reals (so $e_i^2=-1$), and $x_i$ are the corresponding coordinates.  Several sources state that $D^2 = -\Delta_n$ where $\Delta_n$ is the standard Euclidean Laplace operator $$\Delta_n = \sum_{i=1}^n \frac{\partial^2}{\partial x_i^2}.$$ When I write out $D^2 f$ explicitly for some function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, scalar terms from the Laplacian certainly appear, e.g., $$e_1 \frac{\partial}{\partial x_1}\left( e_1 \frac{\partial}{\partial x_1} f \right) = e_1 \left( e_1 \frac{\partial^2}{\partial x_1^2}f + \left(\frac{\partial}{\partial x_1}e_1\right)\frac{\partial}{\partial x_1}f \right)=e_1^2 \frac{\partial^2}{\partial x_1^2}f = -\frac{\partial^2}{\partial x_1^2}f.$$ But I also end up with bivector cross terms that shouldn't be there: $$e_1 \frac{\partial}{\partial x_1}\left( e_2 \frac{\partial}{\partial x_2} f \right) = e_1 \left( e_2 \frac{\partial^2}{\partial x_1 \partial x_2}f + \left(\frac{\partial}{\partial x_1}e_2\right)\frac{\partial}{\partial x_2}f \right)=e_1 e_2 \frac{\partial^2}{\partial x_1 \partial x_2}f = e_{12}\frac{\partial^2}{\partial x_1 \partial x_2}f.$$ Should I only be considering the scalar part of $D^2$, or am I simply doing something wrong here?",,"['multivariable-calculus', 'differential-geometry', 'clifford-algebras']"
8,When is $d^{2} x$ =0?,When is  =0?,d^{2} x,"We want to find $d^{2} z$ : $$ (4 x-3 z-16) d x+(8 y-24) d y+(6 z-3 x+27) d z =0 $$ So, in my book, we apply the differential operator d to the above equation : We use the product rule ; $$ (4 d x-3 d z) d x+(8 d y) d y+(6 d z-3 d x) d z+(6 z-3 x+27) d^{2} z=0 $$ So my question is : Where did $d^{2} x$ and $d^{2} y$ go? Are they equal to 0? If so why $d^{2} z$ is not 0? Edit: That's the problem : So above $d^{2} z$ isn't asked in the question but it's needed to find the asked ones ; Problem 1 (65 points) We consider the function $$ F(x, y, z)=2 x^{2}+4 y^{2}+3 z^{2}-3 x z-16 x-24 y+27 z+94 $$ and let $z=z(x, y)$ be an implicit function defined by the equality $F(x, y, z)=-1$ . 1.1. Calculate $\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, \frac{\partial^{2} z}{\partial x^{2}}, \frac{\partial^{2} z}{\partial y^{2}}$ and $\frac{\partial^{2} z}{\partial x \partial y}$ at point $(x=1, y=3, z=-5)$ .","We want to find : So, in my book, we apply the differential operator d to the above equation : We use the product rule ; So my question is : Where did and go? Are they equal to 0? If so why is not 0? Edit: That's the problem : So above isn't asked in the question but it's needed to find the asked ones ; Problem 1 (65 points) We consider the function and let be an implicit function defined by the equality . 1.1. Calculate and at point .","d^{2} z 
(4 x-3 z-16) d x+(8 y-24) d y+(6 z-3 x+27) d z =0
 
(4 d x-3 d z) d x+(8 d y) d y+(6 d z-3 d x) d z+(6 z-3 x+27) d^{2} z=0
 d^{2} x d^{2} y d^{2} z d^{2} z 
F(x, y, z)=2 x^{2}+4 y^{2}+3 z^{2}-3 x z-16 x-24 y+27 z+94
 z=z(x, y) F(x, y, z)=-1 \frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, \frac{\partial^{2} z}{\partial x^{2}}, \frac{\partial^{2} z}{\partial y^{2}} \frac{\partial^{2} z}{\partial x \partial y} (x=1, y=3, z=-5)","['multivariable-calculus', 'derivatives', 'partial-derivative']"
9,Why this inequality is correct,Why this inequality is correct,,"Let $0<x_1\leq\dots\leq x_m<1$ , I denote $$a=\sum_{i=1}^mx_i,\qquad b=\sum_{i=1}^m\frac{1}{x_i},\qquad c=\sum_{i=1}^m\frac{x_i}{1-x_i}.$$ Im trying to prove that $$(b-m)(c+1-\frac{c}{a})\geq m(m-1).$$ I did prove it for the case of $a\geq 1$ using the Cauchy-Schwartz inequality. I implement a simulation in Python to construct a counterexample but in vain.","Let , I denote Im trying to prove that I did prove it for the case of using the Cauchy-Schwartz inequality. I implement a simulation in Python to construct a counterexample but in vain.","0<x_1\leq\dots\leq x_m<1 a=\sum_{i=1}^mx_i,\qquad b=\sum_{i=1}^m\frac{1}{x_i},\qquad c=\sum_{i=1}^m\frac{x_i}{1-x_i}. (b-m)(c+1-\frac{c}{a})\geq m(m-1). a\geq 1","['multivariable-calculus', 'inequality', 'summation', 'rearrangement-inequality']"
10,relationship between derivative of vector valued function and gradient/partial derivatives,relationship between derivative of vector valued function and gradient/partial derivatives,,"From This article I understand that if I have a function defined using parametric equations like this $$ (1) \quad \quad \quad{{\mathbf{r}\left( t \right) = f\left( t \right)\mathbf{i} + g\left( t \right)\mathbf{j} + h\left( t \right)\mathbf{k}}\;\;\text{ or }\;\;}\kern0pt{\mathbf{r}\left( t \right) = \left\langle {f\left( t \right),g\left( t \right),h\left( t \right)} \right\rangle } $$ Then it's derivative is the derivative of each function that generates each coordinate. $$ (2) \quad \quad \quad {\mathbf{r}^\prime\left( t \right) = \left\langle {f^\prime\left( t \right),g^\prime\left( t \right),h^\prime\left( t \right)} \right\rangle.} $$ However if the same function is represented in a non parametric form (random example below) $$ (3) \quad \quad \quad r( x,y)= x+y   $$ It seems there is no such thing as "" the derivative"", I only have partial derivatives, directional derivatives and the gradient. What is the relationship between ""the"" derivative as defined in $(2)$ and the gradient or the partial/directional derivatives? Are they equivalent in some way?","From This article I understand that if I have a function defined using parametric equations like this Then it's derivative is the derivative of each function that generates each coordinate. However if the same function is represented in a non parametric form (random example below) It seems there is no such thing as "" the derivative"", I only have partial derivatives, directional derivatives and the gradient. What is the relationship between ""the"" derivative as defined in and the gradient or the partial/directional derivatives? Are they equivalent in some way?","
(1) \quad \quad \quad{{\mathbf{r}\left( t \right) = f\left( t \right)\mathbf{i} + g\left( t \right)\mathbf{j} + h\left( t \right)\mathbf{k}}\;\;\text{ or }\;\;}\kern0pt{\mathbf{r}\left( t \right) = \left\langle {f\left( t \right),g\left( t \right),h\left( t \right)} \right\rangle }
 
(2) \quad \quad \quad {\mathbf{r}^\prime\left( t \right) = \left\langle {f^\prime\left( t \right),g^\prime\left( t \right),h^\prime\left( t \right)} \right\rangle.}
 
(3) \quad \quad \quad r( x,y)= x+y  
 (2)","['multivariable-calculus', 'derivatives', 'partial-derivative', 'parametric']"
11,Let $u \in C^3(\mathbb{R}^n)$ and harmonic such that $u(x) = o(|x|)$ when $|x| \rightarrow \infty$. Show that $\nabla u(0) = 0$ and u is constant,Let  and harmonic such that  when . Show that  and u is constant,u \in C^3(\mathbb{R}^n) u(x) = o(|x|) |x| \rightarrow \infty \nabla u(0) = 0,"Let $u \in C^3(\mathbb{R}^n)$ and harmonic such that $u(x) = o(|x|)$ when $|x| \rightarrow \infty$ . 1. Show that $\nabla u(0) = 0$ 2. Show that u is a constant function. To show that $\nabla u(0) = 0$ , I tried to show that $\frac{\partial u}{\partial x_i}(0) = 0, \forall i$ , but came up with nothing. For example, I know that the partial derivatives are also harmonic functions, so using the mean value theorem i get that: $$ \frac{\partial u}{\partial x_i} = \frac{1}{\operatorname{Vol}(r \cdot B^n)}\int_{r \cdot B^n}\frac{\partial u}{\partial x_i}(x) dx $$ and then I tried to use the divergence theorem on the vector field $F = (0,\ldots,0,u,0,\ldots,0)$ where the $u$ is in the i-th coordinate, so I get: (sorry about the strange formatting of that vector) $$ = \frac{1}{\operatorname{Vol}(r \cdot B^n)} \int_{r \cdot S^{n-1}} \left< \left(\matrix{0\\ \vdots\\u\\ \vdots \\ 0}\right),\frac{x}{r}\right> dS =\frac{1}{\operatorname{Vol}(r \cdot B^n)} \int_{r \cdot S^{n-1}}u \cdot \frac{x_i}{r}dS $$ and if $u \cdot x_i$ was harmonic, I would've finished here by using the mean value theorem again because obviously $u \cdot x_i = 0$ at $x=0$ , but it isn't harmonic for sure. Any help will be very appreciated. Do note that this is a question from a past exam in an advanced multivariate calculus class, so we don't have so much claims about harmonic functions and that's why I thought I could find a solution with the divergence theorem. I did find this answer , which somehow answers both questions together, but seems like a complete overkill, and probably this is not close to the solution the professor thought of when he wrote the questions and the solution to the second question somehow involves the first question.","Let and harmonic such that when . 1. Show that 2. Show that u is a constant function. To show that , I tried to show that , but came up with nothing. For example, I know that the partial derivatives are also harmonic functions, so using the mean value theorem i get that: and then I tried to use the divergence theorem on the vector field where the is in the i-th coordinate, so I get: (sorry about the strange formatting of that vector) and if was harmonic, I would've finished here by using the mean value theorem again because obviously at , but it isn't harmonic for sure. Any help will be very appreciated. Do note that this is a question from a past exam in an advanced multivariate calculus class, so we don't have so much claims about harmonic functions and that's why I thought I could find a solution with the divergence theorem. I did find this answer , which somehow answers both questions together, but seems like a complete overkill, and probably this is not close to the solution the professor thought of when he wrote the questions and the solution to the second question somehow involves the first question.","u \in C^3(\mathbb{R}^n) u(x) = o(|x|) |x| \rightarrow \infty \nabla u(0) = 0 \nabla u(0) = 0 \frac{\partial u}{\partial x_i}(0) = 0, \forall i 
\frac{\partial u}{\partial x_i} = \frac{1}{\operatorname{Vol}(r \cdot B^n)}\int_{r \cdot B^n}\frac{\partial u}{\partial x_i}(x) dx
 F = (0,\ldots,0,u,0,\ldots,0) u 
= \frac{1}{\operatorname{Vol}(r \cdot B^n)} \int_{r \cdot S^{n-1}} \left< \left(\matrix{0\\ \vdots\\u\\ \vdots \\ 0}\right),\frac{x}{r}\right> dS =\frac{1}{\operatorname{Vol}(r \cdot B^n)} \int_{r \cdot S^{n-1}}u \cdot \frac{x_i}{r}dS
 u \cdot x_i u \cdot x_i = 0 x=0","['multivariable-calculus', 'vector-analysis', 'harmonic-functions']"
12,Newtonian potential expansion identity,Newtonian potential expansion identity,,"Preliminaries Consider the Newtonian potential $$\frac{1}{|\vec x - \vec y|}$$ with $\vec{x}, \vec{y} \in \mathbb{R}^3$ and $|\vec{x}| = x > y = |\vec{y}|$ . Its Taylor expansion is given by $$\frac{1}{|\vec x - \vec y|} = \sum_{n=0}^\infty \frac{(-1)^n}{n!} (\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x},$$ which can also be written in terms of Legendre polynomials as $$\frac{1}{|\vec x - \vec y|} = \sum_{n = 0}^\infty P_n (\hat{x} \cdot \hat{y}) \frac{y^n}{x^{n+1}}.$$ The key identity here is $$\frac{(-1)^n}{n!} (\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x} = P_n (\hat{x} \cdot \hat{y}) \frac{y^n}{x^{n+1}}$$ Using the key identity twice, we have $$\frac{(-1)^n}{n!} (\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x} = P_n (\hat{x} \cdot \hat{y}) \frac{y^n}{x^{n+1}} = \left(\frac{y}{x}\right)^{2n+1}  P_n (\hat{x} \cdot \hat{y}) \frac{x^n}{y^{n+1}} = \left(\frac{y}{x}\right)^{2n+1} \frac{(-1)^n}{n!} (\vec{x} \cdot\vec{\nabla}_\vec{y})^n \frac{1}{y}.$$ The Question Is there a way to prove the identity $$\boxed{(\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x} = \left(\frac{y}{x}\right)^{2n+1} (\vec{x} \cdot\vec{\nabla}_\vec{y})^n \frac{1}{y}}$$ directly, i.e., without resorting to Legendre polynomials?","Preliminaries Consider the Newtonian potential with and . Its Taylor expansion is given by which can also be written in terms of Legendre polynomials as The key identity here is Using the key identity twice, we have The Question Is there a way to prove the identity directly, i.e., without resorting to Legendre polynomials?","\frac{1}{|\vec x - \vec y|} \vec{x}, \vec{y} \in \mathbb{R}^3 |\vec{x}| = x > y = |\vec{y}| \frac{1}{|\vec x - \vec y|} = \sum_{n=0}^\infty \frac{(-1)^n}{n!} (\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x}, \frac{1}{|\vec x - \vec y|} = \sum_{n = 0}^\infty P_n (\hat{x} \cdot \hat{y}) \frac{y^n}{x^{n+1}}. \frac{(-1)^n}{n!} (\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x} = P_n (\hat{x} \cdot \hat{y}) \frac{y^n}{x^{n+1}} \frac{(-1)^n}{n!} (\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x} = P_n (\hat{x} \cdot \hat{y}) \frac{y^n}{x^{n+1}} = \left(\frac{y}{x}\right)^{2n+1}  P_n (\hat{x} \cdot \hat{y}) \frac{x^n}{y^{n+1}} = \left(\frac{y}{x}\right)^{2n+1} \frac{(-1)^n}{n!} (\vec{x} \cdot\vec{\nabla}_\vec{y})^n \frac{1}{y}. \boxed{(\vec{y} \cdot\vec{\nabla}_\vec{x})^n \frac{1}{x} = \left(\frac{y}{x}\right)^{2n+1} (\vec{x} \cdot\vec{\nabla}_\vec{y})^n \frac{1}{y}}","['multivariable-calculus', 'taylor-expansion', 'alternative-proof', 'legendre-polynomials', 'potential-theory']"
13,What is the motivation behind defining tensor product?,What is the motivation behind defining tensor product?,,"In my undergraduate math course we have tensor calculus. I am not getting the motivations of defining such thing, definition of tensor product and feeling lack of interest in the topic. Can anyone explain why tensor product is defined as it is? or, provide a link. I am using the definition of Tensor product as it is defined here .","In my undergraduate math course we have tensor calculus. I am not getting the motivations of defining such thing, definition of tensor product and feeling lack of interest in the topic. Can anyone explain why tensor product is defined as it is? or, provide a link. I am using the definition of Tensor product as it is defined here .",,['multivariable-calculus']
14,Notation of Euler Lagrange Equations for multiple independent variables,Notation of Euler Lagrange Equations for multiple independent variables,,"Let $x_{1}, ..., x_{n}$ be several independent variables. Let $u = u(x_{1}, ..., x_{n})$ be a function. Let $L(x_{1}, ..., x_{n},u,\partial_{1}u,...,\partial_{n}u)$ be the Lagrangian whose action we want to extremize. Euler Lagrange equation is stated as  $$\frac{\partial L}{\partial u} ~=~ \sum_{i=1}^{n}\frac{\partial}{\partial x_{i}}\frac{\partial L}{\partial (\partial_{i}u)} .$$ The problem is that the partial derivative with respect to $x_i$ is not really a partial derivative, rather it includes both implicit and explicit dependence on $x_i$. The notation is really unclear on this issue. Is there a way of indicating this dependency in the notation? Or is there another way of writing Euler Lagrange Equation in which this issue is bypassed altogether?","Let $x_{1}, ..., x_{n}$ be several independent variables. Let $u = u(x_{1}, ..., x_{n})$ be a function. Let $L(x_{1}, ..., x_{n},u,\partial_{1}u,...,\partial_{n}u)$ be the Lagrangian whose action we want to extremize. Euler Lagrange equation is stated as  $$\frac{\partial L}{\partial u} ~=~ \sum_{i=1}^{n}\frac{\partial}{\partial x_{i}}\frac{\partial L}{\partial (\partial_{i}u)} .$$ The problem is that the partial derivative with respect to $x_i$ is not really a partial derivative, rather it includes both implicit and explicit dependence on $x_i$. The notation is really unclear on this issue. Is there a way of indicating this dependency in the notation? Or is there another way of writing Euler Lagrange Equation in which this issue is bypassed altogether?",,"['multivariable-calculus', 'notation', 'calculus-of-variations', 'euler-lagrange-equation']"
15,How to compute the gradient of the softmax function w.r.t. matrix?,How to compute the gradient of the softmax function w.r.t. matrix?,,"Let us consider the following functions \begin{equation} y = \operatorname{softmax}(z) \end{equation}   \begin{equation} z = h\cdot W + b \end{equation} where $y, h, W$ and $b$ are $1 \times n$, $1 \times m$, $m \times n$ and $1 \times n$ matrices. Compute $\frac{\partial{y_i}}{\partial{W}}$. My efforts: \begin{equation} \frac{\partial{y_i}}{\partial{W}} =  \frac{\partial{y_i}}{\partial{z}} \times \frac{\partial{z}}{\partial{W}} \end{equation} Here $z$ is a vector and $W$ is a matrix so $\frac{\partial{z}}{\partial{W}}$ will be a 3D tensor. But $y_i$ is a scalar and $W$ is $m \times n$ matrix so $\frac{\partial{y_i}}{\partial{W}}$ should be of size $m \times n$. Please tell me where I am wrong?","Let us consider the following functions \begin{equation} y = \operatorname{softmax}(z) \end{equation}   \begin{equation} z = h\cdot W + b \end{equation} where $y, h, W$ and $b$ are $1 \times n$, $1 \times m$, $m \times n$ and $1 \times n$ matrices. Compute $\frac{\partial{y_i}}{\partial{W}}$. My efforts: \begin{equation} \frac{\partial{y_i}}{\partial{W}} =  \frac{\partial{y_i}}{\partial{z}} \times \frac{\partial{z}}{\partial{W}} \end{equation} Here $z$ is a vector and $W$ is a matrix so $\frac{\partial{z}}{\partial{W}}$ will be a 3D tensor. But $y_i$ is a scalar and $W$ is $m \times n$ matrix so $\frac{\partial{y_i}}{\partial{W}}$ should be of size $m \times n$. Please tell me where I am wrong?",,"['multivariable-calculus', 'derivatives', 'matrix-calculus', 'neural-networks']"
16,Why is the determinant of the Jacobian the change of volume factor that comes from changing variables?,Why is the determinant of the Jacobian the change of volume factor that comes from changing variables?,,"I can understand this through various examples found in the internet but I can't quite intuitively understand why the determinant of the derivative(in its most general form)-the Jacobian-gives the change of volume factor that arises when we change variables in, say, an integral. I mean, why does the determinant of the matrix consisting of the derivatives of the original variables wrt the new variables give a number that corresponds to how much the infinitesimal volume has changed? How can we geometrically connect the derivatives that are the components of the Jacobian with the aforementioned change of volume?","I can understand this through various examples found in the internet but I can't quite intuitively understand why the determinant of the derivative(in its most general form)-the Jacobian-gives the change of volume factor that arises when we change variables in, say, an integral. I mean, why does the determinant of the matrix consisting of the derivatives of the original variables wrt the new variables give a number that corresponds to how much the infinitesimal volume has changed? How can we geometrically connect the derivatives that are the components of the Jacobian with the aforementioned change of volume?",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
17,Divergence Theorem when Surface isn't closed,Divergence Theorem when Surface isn't closed,,"So we essentially want to evaluate $$\iint_S \vec{F} \cdot d\vec{S},$$ where $\vec{F} = \langle 2x+y, x^2+y, 3z \rangle$ and $S$ is the cylinder $x^2+y^2=4$ , between the surfaces $z=0$ and $z=5$ . We have that the cylinder is open at the top and the bottom. Therefore, we cannot readily apply Gauss' Divergence theorem. We need to subtract the contributions given by the flux through the top and the bottom, from the volume integral. If we let the closed surface of the cylinder be represented by $S$ , the bottom surface represented by $S_1$ and the top surface by $S_2$ , we have that $$\iint_S \vec{F} \cdot n dS + \iint_{S_1} \vec{F} \cdot ndS + \iint_{S_2} \vec{F} \cdot nds = \iiint_V \nabla \cdot \vec{F} dV.$$ Computing the RHS gives us that $$\iiint_V \nabla \cdot \vec{F} dV = 120 \pi.$$ How do we compute $$\iint_{S_1} \vec{F} \cdot ndS$$ and $$\iint_{S_2} \vec{F} \cdot ndS ?$$ Perhaps $$\iint_{S_1} \vec{F} \cdot ndS = \iint_{S_1} \vec{F} \cdot \langle 0, 0, -1 \rangle dS = \iint_{S_1} -3z dS?$$","So we essentially want to evaluate where and is the cylinder , between the surfaces and . We have that the cylinder is open at the top and the bottom. Therefore, we cannot readily apply Gauss' Divergence theorem. We need to subtract the contributions given by the flux through the top and the bottom, from the volume integral. If we let the closed surface of the cylinder be represented by , the bottom surface represented by and the top surface by , we have that Computing the RHS gives us that How do we compute and Perhaps","\iint_S \vec{F} \cdot d\vec{S}, \vec{F} = \langle 2x+y, x^2+y, 3z \rangle S x^2+y^2=4 z=0 z=5 S S_1 S_2 \iint_S \vec{F} \cdot n dS + \iint_{S_1} \vec{F} \cdot ndS + \iint_{S_2} \vec{F} \cdot nds = \iiint_V \nabla \cdot \vec{F} dV. \iiint_V \nabla \cdot \vec{F} dV = 120 \pi. \iint_{S_1} \vec{F} \cdot ndS \iint_{S_2} \vec{F} \cdot ndS ? \iint_{S_1} \vec{F} \cdot ndS = \iint_{S_1} \vec{F} \cdot \langle 0, 0, -1 \rangle dS = \iint_{S_1} -3z dS?","['multivariable-calculus', 'vector-analysis']"
18,Not all tangents to a plane curve are bitangents,Not all tangents to a plane curve are bitangents,,"I'm struggling on a question from a previous qualifying exam, and I don't see a clean way to do it. Let $X\subset \mathbb{R}^2$ be a connected, 1-dimensional, real analytic submanifold, not contained in a line. Why does there exist a tangent line to $X$ that not bitangent (tangent to $X$ at more than one point)? Out of curiosity, is the statement still true if we drop the condition that $X$ is real analytic (so just smooth)?","I'm struggling on a question from a previous qualifying exam, and I don't see a clean way to do it. Let $X\subset \mathbb{R}^2$ be a connected, 1-dimensional, real analytic submanifold, not contained in a line. Why does there exist a tangent line to $X$ that not bitangent (tangent to $X$ at more than one point)? Out of curiosity, is the statement still true if we drop the condition that $X$ is real analytic (so just smooth)?",,"['multivariable-calculus', 'differential-geometry', 'plane-curves']"
19,Existence of Partials Imply the Existence of Gradient Vector?,Existence of Partials Imply the Existence of Gradient Vector?,,"Let $f$ be a scalar function of three variables. Then the gradient vector is defined by: I read here that the existence of partial derivatives at some point $(x_0, y_0, z_0)$ does not imply the existence of the gradient vector at $(x_0, y_0, z_0)$. How is this possible, since the gradient vector is just a specific type of sum of the partial derivatives?","Let $f$ be a scalar function of three variables. Then the gradient vector is defined by: I read here that the existence of partial derivatives at some point $(x_0, y_0, z_0)$ does not imply the existence of the gradient vector at $(x_0, y_0, z_0)$. How is this possible, since the gradient vector is just a specific type of sum of the partial derivatives?",,"['multivariable-calculus', 'terminology', 'partial-derivative']"
20,Volume of neighborhood of the curve,Volume of neighborhood of the curve,,"Let $\gamma:[0,1] \to \mathbb R^n$ be a smooth closed curve, such that $\gamma(0)=\gamma(1), \gamma'(0)=\gamma'(1), |\gamma'(a)|=1$ and let $B_r=\{x| \exists t, |\gamma(t)-x|<r\}$. How can i show that the volume of $B_r$ is equal to $br^{(n-1)}$ for $r <\varepsilon$ for some $\varepsilon>0$. And how can I find $b$? It is intuitively obvious. But how can one prove it? I have no idea.","Let $\gamma:[0,1] \to \mathbb R^n$ be a smooth closed curve, such that $\gamma(0)=\gamma(1), \gamma'(0)=\gamma'(1), |\gamma'(a)|=1$ and let $B_r=\{x| \exists t, |\gamma(t)-x|<r\}$. How can i show that the volume of $B_r$ is equal to $br^{(n-1)}$ for $r <\varepsilon$ for some $\varepsilon>0$. And how can I find $b$? It is intuitively obvious. But how can one prove it? I have no idea.",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'volume']"
21,"Continuity, differentiability and existence of partial derivatives","Continuity, differentiability and existence of partial derivatives",,"Here are a few functions whose continuity, differentiability and existence of partial derivatives are to be checked at the origin. I have given the answers, but I would really appreciate it if someone could check it for me :) $$1. f(x,y)=\sin x\sin(x+y)\sin(x-y)$$ Continuous, differentiable, partial derivatives exist because $$\lim_{h\to 0}\frac{1}{h}[f(0+h,0)-f(0,0)] = \lim_{h\to 0}\frac{1}{h}[f(0, 0+h)-f(0,0)] $$ $$2. f(x,y)=\left\{\begin{matrix} \ \frac {xy}{x^2+y^2}; (x,y)\neq (0,0)  \\0\; ; (x,y)=(0,0) \end{matrix}\right.   $$ Discontinuous, not differentiable, partial derivatives exist (because partial derivatives are not continuous) $$3. f(x,y)=\sqrt{\left | xy \right |}$$ Continuous, not differentiable, partial derivatives defined  $$4.  f(x,y)=\left\{\begin{matrix} \ \frac {x^2-y^2}{x^2+y^2}; (x,y)\neq (0,0)  \\0\;; (x,y)=(0,0) \end{matrix}\right. $$  Discontinuous, not differentiable, partial derivatives undefined $$\lim_{h\to 0}\frac{1}{h}[f(0+h,0)-f(0,0)] = \lim_{h\to 0}\frac{1}{h}[\frac{h^2}{h^2}-\frac{0}{0}]$$ and $$\lim_{h\to 0}\frac{1}{h}[f(0,0+h)-f(0,0)] = \lim_{h\to 0}\frac{1}{h}[\frac{-h^2}{h^2}-\frac{0}{0}]$$ and the two partial derivatives are not defined. $$5. f(x,y)=\left\{\begin{matrix} \ 1\;; xy=0  \\0\;; xy \neq 0 \end{matrix}\right. $$ Discontinuous, partial derivatives defined, not differentiable (for this, I don't really understand how to go about this particular one: My answers are based on the graph) $$6. f(x,y)=1-\sin \sqrt{x^2+y^2}$$ Continuous (because it is trigonometric), partial derivatives not defined, not differentiable. $$f_x=\lim_{h\to 0}\frac{1}{h}[1-\sin\sqrt{h^2}-1+\sin 0]= -1$$ $$f_y=\lim_{h\to 0}\frac{1}{h}[1-\sin\sqrt{h^2}-1+\sin 0] =-1$$ I am extremely new to these concepts so my reasoning can be extremely flawed. If you could check these answers and, if you think I am wrong, point out as to why I am wrong, I would be extremely thankful :)  @Avitus: It has been edited :) Please have a look.","Here are a few functions whose continuity, differentiability and existence of partial derivatives are to be checked at the origin. I have given the answers, but I would really appreciate it if someone could check it for me :) $$1. f(x,y)=\sin x\sin(x+y)\sin(x-y)$$ Continuous, differentiable, partial derivatives exist because $$\lim_{h\to 0}\frac{1}{h}[f(0+h,0)-f(0,0)] = \lim_{h\to 0}\frac{1}{h}[f(0, 0+h)-f(0,0)] $$ $$2. f(x,y)=\left\{\begin{matrix} \ \frac {xy}{x^2+y^2}; (x,y)\neq (0,0)  \\0\; ; (x,y)=(0,0) \end{matrix}\right.   $$ Discontinuous, not differentiable, partial derivatives exist (because partial derivatives are not continuous) $$3. f(x,y)=\sqrt{\left | xy \right |}$$ Continuous, not differentiable, partial derivatives defined  $$4.  f(x,y)=\left\{\begin{matrix} \ \frac {x^2-y^2}{x^2+y^2}; (x,y)\neq (0,0)  \\0\;; (x,y)=(0,0) \end{matrix}\right. $$  Discontinuous, not differentiable, partial derivatives undefined $$\lim_{h\to 0}\frac{1}{h}[f(0+h,0)-f(0,0)] = \lim_{h\to 0}\frac{1}{h}[\frac{h^2}{h^2}-\frac{0}{0}]$$ and $$\lim_{h\to 0}\frac{1}{h}[f(0,0+h)-f(0,0)] = \lim_{h\to 0}\frac{1}{h}[\frac{-h^2}{h^2}-\frac{0}{0}]$$ and the two partial derivatives are not defined. $$5. f(x,y)=\left\{\begin{matrix} \ 1\;; xy=0  \\0\;; xy \neq 0 \end{matrix}\right. $$ Discontinuous, partial derivatives defined, not differentiable (for this, I don't really understand how to go about this particular one: My answers are based on the graph) $$6. f(x,y)=1-\sin \sqrt{x^2+y^2}$$ Continuous (because it is trigonometric), partial derivatives not defined, not differentiable. $$f_x=\lim_{h\to 0}\frac{1}{h}[1-\sin\sqrt{h^2}-1+\sin 0]= -1$$ $$f_y=\lim_{h\to 0}\frac{1}{h}[1-\sin\sqrt{h^2}-1+\sin 0] =-1$$ I am extremely new to these concepts so my reasoning can be extremely flawed. If you could check these answers and, if you think I am wrong, point out as to why I am wrong, I would be extremely thankful :)  @Avitus: It has been edited :) Please have a look.",,['multivariable-calculus']
22,The intuition behind the definition of geodesics on a Riemannian manifold. (A non-technical question),The intuition behind the definition of geodesics on a Riemannian manifold. (A non-technical question),,"In the text I'm studying, the idea behind the definition of a geodesic on a Riemannian manifold was sketched via paths in $\mathbb{R}^n$. I have trouble understanding some aspects of it. Let $\gamma: I \to \mathbb{R}^n$ be a path. It is a geodesic if $$ \ddot{\gamma}_T \equiv 0, $$ where $\ddot{\gamma}_T$ denotes the tangential component of the total acceleration $\ddot{\gamma}$. A geodesic is a ""straight as possible"" path on a manifold. Clearly, a line $$\gamma(t) = vt+p, \quad v,p \in \mathbb{R}^n$$ satisfies the given characterisation by acceleration in $\mathbb{R}^n$. But wouldn't a perfectly circular path also do? Have I misunderstood something or is this some property of the geodesics that the heuristic description fails to convey.","In the text I'm studying, the idea behind the definition of a geodesic on a Riemannian manifold was sketched via paths in $\mathbb{R}^n$. I have trouble understanding some aspects of it. Let $\gamma: I \to \mathbb{R}^n$ be a path. It is a geodesic if $$ \ddot{\gamma}_T \equiv 0, $$ where $\ddot{\gamma}_T$ denotes the tangential component of the total acceleration $\ddot{\gamma}$. A geodesic is a ""straight as possible"" path on a manifold. Clearly, a line $$\gamma(t) = vt+p, \quad v,p \in \mathbb{R}^n$$ satisfies the given characterisation by acceleration in $\mathbb{R}^n$. But wouldn't a perfectly circular path also do? Have I misunderstood something or is this some property of the geodesics that the heuristic description fails to convey.",,"['differential-geometry', 'multivariable-calculus', 'analytic-geometry', 'riemannian-geometry']"
23,Is the image of a parametrization a manifold?,Is the image of a parametrization a manifold?,,"Consider this definition of the parametrization of a manifold, found in Hubbard & Hubbard: A parametrization of a $k$-dimensional manifold $M\subset\mathbb{R}^n$ is a mapping $\gamma:U\subset \mathbb{R}^k\to M$ satisfying: 1) $U$ is open. 2) $\gamma$ is $C^1$, and bijective with $M$. 3) $[D\gamma(u)]$ is 1-1 for all $u\in U$. My question is this: if instead we start with a parametrization and replace $M$ with the image of $\gamma$, will the image in fact be a manifold? In Hubbard this seems to be suggested, but never stated explicitly (as far as I can see). Maybe the point is that we can locally transform a parametrization into the graph of a $C^1$ function...","Consider this definition of the parametrization of a manifold, found in Hubbard & Hubbard: A parametrization of a $k$-dimensional manifold $M\subset\mathbb{R}^n$ is a mapping $\gamma:U\subset \mathbb{R}^k\to M$ satisfying: 1) $U$ is open. 2) $\gamma$ is $C^1$, and bijective with $M$. 3) $[D\gamma(u)]$ is 1-1 for all $u\in U$. My question is this: if instead we start with a parametrization and replace $M$ with the image of $\gamma$, will the image in fact be a manifold? In Hubbard this seems to be suggested, but never stated explicitly (as far as I can see). Maybe the point is that we can locally transform a parametrization into the graph of a $C^1$ function...",,"['multivariable-calculus', 'manifolds']"
24,Example of discontinuous function having all partial derivatives,Example of discontinuous function having all partial derivatives,,Is it possible to a real-valued function of two variables defined on an open set to have partial derivatives of all order and to be discontinuous at some point or maybe at each point?,Is it possible to a real-valued function of two variables defined on an open set to have partial derivatives of all order and to be discontinuous at some point or maybe at each point?,,"['multivariable-calculus', 'partial-derivative', 'examples-counterexamples']"
25,"Showing that the maximum value of $\sin x+\sin y\sin z$, where $x+y+z=\pi$, is the golden ratio","Showing that the maximum value of , where , is the golden ratio",\sin x+\sin y\sin z x+y+z=\pi,"Find the maximum of $$\sin x+\sin y\sin z$$ if $x+y+z=\pi$ . By using Lagrange multipliers, concluded that $y=z$ , further plug $x=\pi-2y$ , I've reduced the problem to single-variable expression. $$\sin^2 y+\sin(2y)$$ Then by taking first derivative and using formulas for $\sin(\arctan x)$ and $\cos(\arctan x)$ , finally we can obtain the maximum $$\frac{\sqrt5+1}{2}$$ As one can see this is precisely the golden ratio $\phi$ ! But this solution takes some time, so I'm interested in different no calculus solution of this problem, especially considering that it is related to the golden ratio.","Find the maximum of if . By using Lagrange multipliers, concluded that , further plug , I've reduced the problem to single-variable expression. Then by taking first derivative and using formulas for and , finally we can obtain the maximum As one can see this is precisely the golden ratio ! But this solution takes some time, so I'm interested in different no calculus solution of this problem, especially considering that it is related to the golden ratio.",\sin x+\sin y\sin z x+y+z=\pi y=z x=\pi-2y \sin^2 y+\sin(2y) \sin(\arctan x) \cos(\arctan x) \frac{\sqrt5+1}{2} \phi,"['multivariable-calculus', 'trigonometry', 'lagrange-multiplier', 'golden-ratio']"
26,Hessian of $f(X)$ when $X$ is a symmetric matrix,Hessian of  when  is a symmetric matrix,f(X) X,"Thanks to the scientific community, things are getting clear relatively to the question: what is the gradient of a function $f(X)$ when $X$ is a symmetric matrix? . In particular, I report here some useful links that addressed this question in the past and can be used as a reference to proceed further on this discussion: Understanding notation of derivatives of a matrix Taylor expansion of a function of a symmetric matrix https://arxiv.org/pdf/1911.06491.pdf In a nutshell, we can say that, when involving a function with matrix argument, we have to distinguish between two ""different"", but related, gradients: the unconstrained gradient $G$ , computed with standard matrix calculus without assuming dependent variables in the matrix $X$ , and used for the computation of the differential of the function, i.e. $G:dX$ the constrained gradient $S$ , that considers only the independent variables of the matrix $X$ . These two gradients are related by the expression: $$S=G+G^{T}-I \circ G $$ and it turns out that the first-order differential of the function $f$ at a given point $X$ after a perturbation $\Delta X$ can be computed as: $$ d f=\sum_{i, j} G_{i j} d X_{i j} = \sum_{i \geq j} S_{i j} d X_{i j}$$ It is important however to note how, in an iterative algorithm that updates a variable $X^{k+1}$ (such as in gradient descent), we have to use the constrained gradient $S$ and not the gradient $G$ , due to the fact that $X$ is symmetric while the gradient $G$ could be not symmetric. More information can be found in the above links, that explain the relation also in terms of $vec(\cdot)$ and $vech(\cdot)$ operators. Coming to my question. I want now to find the Hessian of the function $f(X)$ , that in theory is a $4$ th order tensor and we already know the mangy road crisscrossed to get to the gradient. To start, is it correct to perturb the first-order differential (with the unconstrained gradient)? If yes, I will reach a scalar quadratic form. For instance, if we consider as function $f(X)=\log \operatorname{det} X$ , we know that the second order approximation with perturbation in $U$ and $V$ is given by (and I reference this question Second order approximation of log det X ): $$-\operatorname{tr}\left(X^{-1} U X^{-1} V\right) = - \operatorname{vec}(U^{\top})^{\top}(X^{-\top} \otimes X^{-1})   \operatorname{vec}(V)$$ We can arrive at the Hessian in matrix form $X^{-\top} \otimes X^{-1}$ . My first question is: how to write it in a tensor form? And second question is: how to reach in this case our constrained Hessian ?","Thanks to the scientific community, things are getting clear relatively to the question: what is the gradient of a function when is a symmetric matrix? . In particular, I report here some useful links that addressed this question in the past and can be used as a reference to proceed further on this discussion: Understanding notation of derivatives of a matrix Taylor expansion of a function of a symmetric matrix https://arxiv.org/pdf/1911.06491.pdf In a nutshell, we can say that, when involving a function with matrix argument, we have to distinguish between two ""different"", but related, gradients: the unconstrained gradient , computed with standard matrix calculus without assuming dependent variables in the matrix , and used for the computation of the differential of the function, i.e. the constrained gradient , that considers only the independent variables of the matrix . These two gradients are related by the expression: and it turns out that the first-order differential of the function at a given point after a perturbation can be computed as: It is important however to note how, in an iterative algorithm that updates a variable (such as in gradient descent), we have to use the constrained gradient and not the gradient , due to the fact that is symmetric while the gradient could be not symmetric. More information can be found in the above links, that explain the relation also in terms of and operators. Coming to my question. I want now to find the Hessian of the function , that in theory is a th order tensor and we already know the mangy road crisscrossed to get to the gradient. To start, is it correct to perturb the first-order differential (with the unconstrained gradient)? If yes, I will reach a scalar quadratic form. For instance, if we consider as function , we know that the second order approximation with perturbation in and is given by (and I reference this question Second order approximation of log det X ): We can arrive at the Hessian in matrix form . My first question is: how to write it in a tensor form? And second question is: how to reach in this case our constrained Hessian ?","f(X) X G X G:dX S X S=G+G^{T}-I \circ G  f X \Delta X  d f=\sum_{i, j} G_{i j} d X_{i j} = \sum_{i \geq j} S_{i j} d X_{i j} X^{k+1} S G X G vec(\cdot) vech(\cdot) f(X) 4 f(X)=\log \operatorname{det} X U V -\operatorname{tr}\left(X^{-1} U X^{-1} V\right) = - \operatorname{vec}(U^{\top})^{\top}(X^{-\top} \otimes X^{-1})   \operatorname{vec}(V) X^{-\top} \otimes X^{-1}","['multivariable-calculus', 'taylor-expansion', 'approximation', 'approximation-theory', 'hessian-matrix']"
27,Extreme values of ${x^3 + y^3 + z^3 - 3xyz}$ subject to ${ax + by + cz =1}$ using Lagrange Multipliers,Extreme values of  subject to  using Lagrange Multipliers,{x^3 + y^3 + z^3 - 3xyz} {ax + by + cz =1},"If ${ax + by + cz =1}$ , then show that in general ${x^3 + y^3 + z^3 - 3xyz}$ has two stationary values ${0}$ and $\frac{1}{(a^3+b^3+c^3-3abc)}$ , of which first is max or min according as ${a+b+c>0}$ or ${< 0}$ but second is not an extreme value. Comment on particular cases when (i) ${a+b+c=0}$ , (ii) ${a=b=c}$ . My Attempt: $${F=f+\lambda \phi =x^3 + y^3 + z^3 - 3xyz + 3\lambda(ax + by + cz-1)}$$ $${\frac13F_x = x^2-yz+\lambda a = 0}{\text{ ...(1)}}$$ $${\frac13F_y = y^2-xz+\lambda b = 0}{\text{ ...(2)}}$$ $${\frac13F_z = z^2-xy+\lambda c = 0{\text{ ...(3)}}}$$ $${(1)x+(2)y+(3)z \implies f+\lambda (1) = 0 \implies \lambda = -f}$$ $${(1)+(2)+(3) \implies}{x^2+y^2+z^2-xy-yz-zx=(a+b+c)f}$$ $${\implies f/(x+y+z)=(a+b+c)f}\,,$$ then ${f=0}$ or ${x+y+z=1/(a+b+c)}$ . Also, ${(1)-(2) \implies x^2-y^2-z(x-y)=f(a-b) \implies \frac{x-y}{a-b}=f\frac{a+b+c}{x+y+z}}$ Similarly ${(2)-(3)}$ and ${(3)-(1)}$ , then we get ${\frac{x-y}{a-b}=\frac{y-z}{b-c}=\frac{z-x}{c-a}=f\frac{a+b+c}{x+y+z}}$ I don't know how to proceed further. I couldn't get the other stationary value of ${f}$ . I need help in proceeding further to calculate stationary points and/or stationary values, if at all, my method is correct.","If , then show that in general has two stationary values and , of which first is max or min according as or but second is not an extreme value. Comment on particular cases when (i) , (ii) . My Attempt: then or . Also, Similarly and , then we get I don't know how to proceed further. I couldn't get the other stationary value of . I need help in proceeding further to calculate stationary points and/or stationary values, if at all, my method is correct.","{ax + by + cz =1} {x^3 + y^3 + z^3 - 3xyz} {0} \frac{1}{(a^3+b^3+c^3-3abc)} {a+b+c>0} {< 0} {a+b+c=0} {a=b=c} {F=f+\lambda \phi =x^3 + y^3 + z^3 - 3xyz + 3\lambda(ax + by + cz-1)} {\frac13F_x = x^2-yz+\lambda a = 0}{\text{ ...(1)}} {\frac13F_y = y^2-xz+\lambda b = 0}{\text{ ...(2)}} {\frac13F_z = z^2-xy+\lambda c = 0{\text{ ...(3)}}} {(1)x+(2)y+(3)z \implies f+\lambda (1) = 0 \implies \lambda = -f} {(1)+(2)+(3) \implies}{x^2+y^2+z^2-xy-yz-zx=(a+b+c)f} {\implies f/(x+y+z)=(a+b+c)f}\,, {f=0} {x+y+z=1/(a+b+c)} {(1)-(2) \implies x^2-y^2-z(x-y)=f(a-b) \implies \frac{x-y}{a-b}=f\frac{a+b+c}{x+y+z}} {(2)-(3)} {(3)-(1)} {\frac{x-y}{a-b}=\frac{y-z}{b-c}=\frac{z-x}{c-a}=f\frac{a+b+c}{x+y+z}} {f}","['multivariable-calculus', 'optimization', 'nonlinear-optimization', 'lagrange-multiplier', 'maxima-minima']"
28,Prove $\sum\limits_\text{cyc}\frac{a}{a+(n-1)b}\geq 1$,Prove,\sum\limits_\text{cyc}\frac{a}{a+(n-1)b}\geq 1,"For $a_i> 0$, $n \in \mathbb{N}$ prove or disprove $$\frac{a_1}{a_1+(n-1) a_2} + \frac{a_2}{a_2+(n-1)a_3}+\dots+\frac{a_n}{a_n+(n-1)a_1}\geq 1.$$ Written in a cyclic notation $$ \sum_\text{cyc} \frac{a}{a+(n-1)b} \geq 1. $$ I have conjectured this one after running into couple similar ones. Perhaps it is known, but I could not find it solved anywhere. For small values of $n$ it apparently holds, for $n=1$ we have: $$\frac{a}{a} = 1 \geq 1 $$ For $n=2$ it is: $$ \frac{a}{a+b} + \frac{b}{b+a} = \frac{a+b}{a+b} = 1 \geq 1 $$ For $n=3$ it starts to be interesting: $$ \frac{a}{a+2b} + \frac{b}{b+2c} + \frac{c}{c+2a} \geq 1 $$ Here the Cauchy-Schwartz inequality seems to do the trick: \begin{align} \left(\sum_\text{cyc} \frac{a} {a+2b}\right)\left(\sum_\text{cyc} a(a+2b)\right) &\geq \left(\sum_\text{cyc} a\right)^2\\ \left(\sum_\text{cyc} \frac{a}{a+2b}\right) (a+b+c)^2&\geq (a+b+c)^2 \\ \left(\sum_\text{cyc} \frac{a}{a+2b}\right) &\geq 1 \end{align} However for higher $n$ I'm stuck. I have tried Cauchy-Schwartz inequality, as well as Holder's and Jensen's, but no luck. Also considered induction but it did not lead to anything nice. It appears that the equality holds whenever $a_1=a_2=\cdots=a_n$. Also the inequality can be equivalently written in a form \begin{align} \sum_\text{cyc} \frac{a}{a+(n-1)b} = \sum_\text{cyc}\frac{a+(n-1)b-(n-1)b}{a+(n-1)b} = n-(n-1)\sum_\text{cyc}\frac{b}{a+(n-1)b} &\geq 1\\ \end{align} so \begin{align} 1 &\geq \sum_\text{cyc}\frac{b}{a+(n-1)b}.\\ \end{align} Anyone knows how to prove/disprove this for generic $n$?","For $a_i> 0$, $n \in \mathbb{N}$ prove or disprove $$\frac{a_1}{a_1+(n-1) a_2} + \frac{a_2}{a_2+(n-1)a_3}+\dots+\frac{a_n}{a_n+(n-1)a_1}\geq 1.$$ Written in a cyclic notation $$ \sum_\text{cyc} \frac{a}{a+(n-1)b} \geq 1. $$ I have conjectured this one after running into couple similar ones. Perhaps it is known, but I could not find it solved anywhere. For small values of $n$ it apparently holds, for $n=1$ we have: $$\frac{a}{a} = 1 \geq 1 $$ For $n=2$ it is: $$ \frac{a}{a+b} + \frac{b}{b+a} = \frac{a+b}{a+b} = 1 \geq 1 $$ For $n=3$ it starts to be interesting: $$ \frac{a}{a+2b} + \frac{b}{b+2c} + \frac{c}{c+2a} \geq 1 $$ Here the Cauchy-Schwartz inequality seems to do the trick: \begin{align} \left(\sum_\text{cyc} \frac{a} {a+2b}\right)\left(\sum_\text{cyc} a(a+2b)\right) &\geq \left(\sum_\text{cyc} a\right)^2\\ \left(\sum_\text{cyc} \frac{a}{a+2b}\right) (a+b+c)^2&\geq (a+b+c)^2 \\ \left(\sum_\text{cyc} \frac{a}{a+2b}\right) &\geq 1 \end{align} However for higher $n$ I'm stuck. I have tried Cauchy-Schwartz inequality, as well as Holder's and Jensen's, but no luck. Also considered induction but it did not lead to anything nice. It appears that the equality holds whenever $a_1=a_2=\cdots=a_n$. Also the inequality can be equivalently written in a form \begin{align} \sum_\text{cyc} \frac{a}{a+(n-1)b} = \sum_\text{cyc}\frac{a+(n-1)b-(n-1)b}{a+(n-1)b} = n-(n-1)\sum_\text{cyc}\frac{b}{a+(n-1)b} &\geq 1\\ \end{align} so \begin{align} 1 &\geq \sum_\text{cyc}\frac{b}{a+(n-1)b}.\\ \end{align} Anyone knows how to prove/disprove this for generic $n$?",,"['multivariable-calculus', 'inequality', 'contest-math', 'substitution', 'a.m.-g.m.-inequality']"
29,Gradient vs Conservative vector field: What's the difference?,Gradient vs Conservative vector field: What's the difference?,,"From the definitions I'm reading between the two: The gradient vector field is defined by its construction: gradient of a scalar (or real) function generally over two or more variables. The conservative vector field is defined by the common characteristic of every curve in this field: only the endpoints matter, not the path. Interpretation wise in traditional multi-variable calculus view, these two type of fields sound exactly the same. I've had some difficulty trying to pinpoint which one is more abstract or specialized, much less their difference. According to Wikipedia (I may have committed blasphemy): Conservative vector fields and the gradient theorem The gradient of a function is called a gradient field. A (continuous)   gradient field is always a conservative vector field: its line   integral along any path depends only on the endpoints of the path, and   can be evaluated by the gradient theorem (the fundamental theorem of   calculus for line integrals). Conversely, a (continuous) conservative   vector field is always the gradient of a function. Obviously this doesn't help trying to understand the difference, if any.","From the definitions I'm reading between the two: The gradient vector field is defined by its construction: gradient of a scalar (or real) function generally over two or more variables. The conservative vector field is defined by the common characteristic of every curve in this field: only the endpoints matter, not the path. Interpretation wise in traditional multi-variable calculus view, these two type of fields sound exactly the same. I've had some difficulty trying to pinpoint which one is more abstract or specialized, much less their difference. According to Wikipedia (I may have committed blasphemy): Conservative vector fields and the gradient theorem The gradient of a function is called a gradient field. A (continuous)   gradient field is always a conservative vector field: its line   integral along any path depends only on the endpoints of the path, and   can be evaluated by the gradient theorem (the fundamental theorem of   calculus for line integrals). Conversely, a (continuous) conservative   vector field is always the gradient of a function. Obviously this doesn't help trying to understand the difference, if any.",,[]
30,double area integrals over coherence functions on circles,double area integrals over coherence functions on circles,,"I am having trouble showing the following, which shows up from coherence theory: $\frac{\pi b^2}{\alpha^2}(1-J_0^2(\alpha b)-J_1^2(\alpha b))=\int_0^{2\pi}\int_0^b\int_0^b r_1r_2\frac{J_1\left (\alpha\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\theta)}\right )}{\alpha\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\theta)}} dr_1dr_2d\theta$ Where $J_n$ is the nth order Bessel function of the first kind. The result is so nice, but I can't find a way to show it. Can anyone provide some help in showing this equality?","I am having trouble showing the following, which shows up from coherence theory: $\frac{\pi b^2}{\alpha^2}(1-J_0^2(\alpha b)-J_1^2(\alpha b))=\int_0^{2\pi}\int_0^b\int_0^b r_1r_2\frac{J_1\left (\alpha\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\theta)}\right )}{\alpha\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\theta)}} dr_1dr_2d\theta$ Where $J_n$ is the nth order Bessel function of the first kind. The result is so nice, but I can't find a way to show it. Can anyone provide some help in showing this equality?",,"['multivariable-calculus', 'stochastic-calculus']"
31,How to maximize the volume of a rectangular parallelepiped in an ellipsoid?,How to maximize the volume of a rectangular parallelepiped in an ellipsoid?,,"This question comes from an exam about 15 years ago. How to find the maximal volume of a rectangular parallelepiped inscribed in an ellipsoid $\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$? I think this should be solved by Lagrange mulitpliers. but it is not given that the parallelepiped is parallel to the axes, so I cannot apply the method immediately. I tired to prove that the only parallelepiped in an ellipsoid must be parallel to the axes; even though this seems obvious, I found no way of proving it rigorously. If I can prove this, then the ensuing steps by Lagrange are not so difficult to me. So any help is well-appreciated.","This question comes from an exam about 15 years ago. How to find the maximal volume of a rectangular parallelepiped inscribed in an ellipsoid $\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$? I think this should be solved by Lagrange mulitpliers. but it is not given that the parallelepiped is parallel to the axes, so I cannot apply the method immediately. I tired to prove that the only parallelepiped in an ellipsoid must be parallel to the axes; even though this seems obvious, I found no way of proving it rigorously. If I can prove this, then the ensuing steps by Lagrange are not so difficult to me. So any help is well-appreciated.",,"['multivariable-calculus', 'conic-sections']"
32,Formal Definition of the Divergence of a Vector Field,Formal Definition of the Divergence of a Vector Field,,"I recently completed an introductory course on multivariate calculus, and I'm still trying to come to grips with the concepts taught in the vector calculus segment. Right now, I'm reviewing the concept of divergence. I understand the lexical definition of divergence, that (in $\mathbb{R}^3$ at least) it's the volumetric density of the outward flux of a vector field. The formulaic definition that Wikipedia and Wolfram offer makes similar sense: $$\mathrm{div}\,\mathbf{F}=\lim_{V \to 0}{\iint_S{\frac{\mathbf{F}\cdot\mathbf{n}}{V}dS}}$$ What I don't understand, however, is how  $\mathrm{div}\,\mathbf{F}=\nabla\cdot\mathbf{F}$ follows from the above statement. Something like $\mathrm{div}\,\mathbf{F}=|\nabla\mathbf{F}|$ seems to make more sense to me, because $\mathrm{div}\,\mathbf{F}=\nabla\cdot\mathbf{F}$ just adds up the partials in what are essentially three random directions ($\mathbf{i},\,\mathbf{j},\mathbf{k}$, after all, are the conventional basis vectors for $\mathbb{R}^3$), whereas $\mathrm{div}\,\mathbf{F}=|\nabla\mathbf{F}|$ isn't as... arbitrary? I suppose. In short: how do you proceed from the formal definition of divergence as the limit of flux over volume as volume goes to zero to getting that divergence is equal to the dot product of nabla and the vector field?","I recently completed an introductory course on multivariate calculus, and I'm still trying to come to grips with the concepts taught in the vector calculus segment. Right now, I'm reviewing the concept of divergence. I understand the lexical definition of divergence, that (in $\mathbb{R}^3$ at least) it's the volumetric density of the outward flux of a vector field. The formulaic definition that Wikipedia and Wolfram offer makes similar sense: $$\mathrm{div}\,\mathbf{F}=\lim_{V \to 0}{\iint_S{\frac{\mathbf{F}\cdot\mathbf{n}}{V}dS}}$$ What I don't understand, however, is how  $\mathrm{div}\,\mathbf{F}=\nabla\cdot\mathbf{F}$ follows from the above statement. Something like $\mathrm{div}\,\mathbf{F}=|\nabla\mathbf{F}|$ seems to make more sense to me, because $\mathrm{div}\,\mathbf{F}=\nabla\cdot\mathbf{F}$ just adds up the partials in what are essentially three random directions ($\mathbf{i},\,\mathbf{j},\mathbf{k}$, after all, are the conventional basis vectors for $\mathbb{R}^3$), whereas $\mathrm{div}\,\mathbf{F}=|\nabla\mathbf{F}|$ isn't as... arbitrary? I suppose. In short: how do you proceed from the formal definition of divergence as the limit of flux over volume as volume goes to zero to getting that divergence is equal to the dot product of nabla and the vector field?",,['multivariable-calculus']
33,Gandalf's adventure (simple vector algebra),Gandalf's adventure (simple vector algebra),,"So, I found the correct answer to this homework question, but I was hoping there was an easier way to find the same answer. Here's the question: Gandalf the Grey started in the Forest of Mirkwood at a point with coordinates $(-2, 1)$ and arrived in the Iron Hills at the point with coordinates $(-1, 6)$. If he began walking in the direction of the vector $\bf v = 5 \mathbf{I} + 1 \mathbf{J}$ and changes direction only once, when he turns at a right angle, what are the coordinates of the point where he makes the turn. The answer is $((-1/13), (18/13))$. Now, I know that the dot product of two perpendicular vectors is $0$, and the sum of the two intermediate vectors must equal $\langle 1, 5\rangle$. Also, the tutor solved the problem by using a vector-line formula which had a point, then a vector multiplied by a scalar. I'm looking for the easiest and most intuitive way to solved this problem. Any help is greatly appreciated! I'll respond as quickly as I can.","So, I found the correct answer to this homework question, but I was hoping there was an easier way to find the same answer. Here's the question: Gandalf the Grey started in the Forest of Mirkwood at a point with coordinates $(-2, 1)$ and arrived in the Iron Hills at the point with coordinates $(-1, 6)$. If he began walking in the direction of the vector $\bf v = 5 \mathbf{I} + 1 \mathbf{J}$ and changes direction only once, when he turns at a right angle, what are the coordinates of the point where he makes the turn. The answer is $((-1/13), (18/13))$. Now, I know that the dot product of two perpendicular vectors is $0$, and the sum of the two intermediate vectors must equal $\langle 1, 5\rangle$. Also, the tutor solved the problem by using a vector-line formula which had a point, then a vector multiplied by a scalar. I'm looking for the easiest and most intuitive way to solved this problem. Any help is greatly appreciated! I'll respond as quickly as I can.",,[]
34,Curvature of the image of a curve projected onto a surface,Curvature of the image of a curve projected onto a surface,,"(Adding a bounty since I need more details than I have so far) Given a point $$ s_{0}=S(u_{0},v_{0}) \;\;\;\; (S:\mathbb{R}^{2}\to\mathbb{R}^{3}) $$ and a point $$ c_{0}=C(t_{0}) \;\;\;\; (C:\mathbb{R}\to\mathbb{R}^{3}) $$ where C and S are both twice differentiable, let $s_0$ be one of possibly many normal projections of $c_0$ onto S .  Further, assume for simplicity that $c_0$ is the only point on C that projects to $s_0$.  Then, assuming C' ( t 0 ) is not orthogonal to S at $s_0$, $s_0$ is a point on a normal projection curve $$ C_{S}(t_{0}) \;\;\;\; (C_{S}:\mathbb{R}\to\mathbb{R}^{3}). $$ (EDITED to avoid mentioning derivatives, since there is freedom in choosing the parametrization of C S ) What is the curvature of $C_S$ at $s_0$? I have no trouble coming up with an expression for the directed unit tangent of C S as a projection of C '( t 0 ) onto the tangent space of S at $s_0$, but curvature is harder and I suppose involves some differential geometry--which I'm weak on.","(Adding a bounty since I need more details than I have so far) Given a point $$ s_{0}=S(u_{0},v_{0}) \;\;\;\; (S:\mathbb{R}^{2}\to\mathbb{R}^{3}) $$ and a point $$ c_{0}=C(t_{0}) \;\;\;\; (C:\mathbb{R}\to\mathbb{R}^{3}) $$ where C and S are both twice differentiable, let $s_0$ be one of possibly many normal projections of $c_0$ onto S .  Further, assume for simplicity that $c_0$ is the only point on C that projects to $s_0$.  Then, assuming C' ( t 0 ) is not orthogonal to S at $s_0$, $s_0$ is a point on a normal projection curve $$ C_{S}(t_{0}) \;\;\;\; (C_{S}:\mathbb{R}\to\mathbb{R}^{3}). $$ (EDITED to avoid mentioning derivatives, since there is freedom in choosing the parametrization of C S ) What is the curvature of $C_S$ at $s_0$? I have no trouble coming up with an expression for the directed unit tangent of C S as a projection of C '( t 0 ) onto the tangent space of S at $s_0$, but curvature is harder and I suppose involves some differential geometry--which I'm weak on.",,"['differential-geometry', 'multivariable-calculus', 'parametric']"
35,Local versus global implicit function,Local versus global implicit function,,"Suppose the equation $f\left(x,y\right)=0$, with $x\in I_{1}$ and $y\in I_{2}$, $I_{1}$ and $I_{2}$ being open intervals. Additionally, consider that the conditions required to apply the Implicit Function Theorem (IFT) are verified for all $\left(x_{0},y_{0}\right)\in I_{1}\times I_{2}$. Hence, we can conclude that in a neighborhood containing the point $\left(x_{0},y_{0}\right)$, the equation $f\left(x,y\right)=0$ defines implicitly $y$ as a function of $x$. And my question is: Since the conditions of IFT hold for all $\left(x_{0},y_{0}\right)\in I_{1}\times I_{2}$, is it true that the equation $f\left(x,y\right)=0$ defines implicitly $y$ as a function of $x$ with the domain of this implicit function being $I_{1}$?","Suppose the equation $f\left(x,y\right)=0$, with $x\in I_{1}$ and $y\in I_{2}$, $I_{1}$ and $I_{2}$ being open intervals. Additionally, consider that the conditions required to apply the Implicit Function Theorem (IFT) are verified for all $\left(x_{0},y_{0}\right)\in I_{1}\times I_{2}$. Hence, we can conclude that in a neighborhood containing the point $\left(x_{0},y_{0}\right)$, the equation $f\left(x,y\right)=0$ defines implicitly $y$ as a function of $x$. And my question is: Since the conditions of IFT hold for all $\left(x_{0},y_{0}\right)\in I_{1}\times I_{2}$, is it true that the equation $f\left(x,y\right)=0$ defines implicitly $y$ as a function of $x$ with the domain of this implicit function being $I_{1}$?",,['multivariable-calculus']
36,Interchanging Derivative and Integral Example,Interchanging Derivative and Integral Example,,"In class the other day, my professor stated the following theorem: Suppose $\frac{d}{dy}f(x,y)$ is continuous on $[0,1] \times [0,1]$, then $\frac{d}{dy} \int^1_0 f(x,y) \, dx = \int_0^1 \frac{d}{dy}f(x,y) \, dx$. He then quickly corrected this to read: Suppose $\frac{d}{dy}f(x,y)$ and f are continuous on $[0,1] \times [0,1]$, then $$\frac{d}{dy} \int^1_0 f(x,y) \, dx = \int_0^1 \frac{d}{dy}f(x,y) \, dx.$$ I was wondering if anyone could provide an example of $f$ which satisfies the conditions for the first statement written but not the subsequently added continuity condition from the second statement, and therefore fails the overall implication of the first statement.","In class the other day, my professor stated the following theorem: Suppose $\frac{d}{dy}f(x,y)$ is continuous on $[0,1] \times [0,1]$, then $\frac{d}{dy} \int^1_0 f(x,y) \, dx = \int_0^1 \frac{d}{dy}f(x,y) \, dx$. He then quickly corrected this to read: Suppose $\frac{d}{dy}f(x,y)$ and f are continuous on $[0,1] \times [0,1]$, then $$\frac{d}{dy} \int^1_0 f(x,y) \, dx = \int_0^1 \frac{d}{dy}f(x,y) \, dx.$$ I was wondering if anyone could provide an example of $f$ which satisfies the conditions for the first statement written but not the subsequently added continuity condition from the second statement, and therefore fails the overall implication of the first statement.",,['multivariable-calculus']
37,"A non-Riemann-integrable function $f\colon [0, 1]^2 \to \mathbb{R}$ such that $f(\cdot, y), f(x, \cdot)$ are Riemann integrable.",A non-Riemann-integrable function  such that  are Riemann integrable.,"f\colon [0, 1]^2 \to \mathbb{R} f(\cdot, y), f(x, \cdot)","I need to find a bounded $f\colon [0, 1]^2 \to \mathbb{R}$ which is not Riemann integrable, but such that $f(\cdot, y), f(x,\cdot)$ are Riemann integrable as functions from $[0, 1]$ to $\mathbb{R}$ for every fixed $x,y$ . I've tried several tricks that didn't work out. I began suspecting that there is no such example but I also couldn't prove it. Any ideas?","I need to find a bounded which is not Riemann integrable, but such that are Riemann integrable as functions from to for every fixed . I've tried several tricks that didn't work out. I began suspecting that there is no such example but I also couldn't prove it. Any ideas?","f\colon [0, 1]^2 \to \mathbb{R} f(\cdot, y), f(x,\cdot) [0, 1] \mathbb{R} x,y","['multivariable-calculus', 'riemann-integration']"
38,Show that Lipschitz $\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$ is implied by $f(y) \leq f(x) + \nabla f(x)^T(y-x) + \dfrac{L}{2}\|y-x\|^2$,Show that Lipschitz  is implied by,\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\| f(y) \leq f(x) + \nabla f(x)^T(y-x) + \dfrac{L}{2}\|y-x\|^2,"Pg 12 - 14 http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf Def : A $C^1$ convex function $f$ is Lipschitz smooth if $\exists L > 0$ s.t. $\forall x, y\in \mathbb{R}^n$     \begin{equation} 		\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\| 	\end{equation} Claim: A $C^1$ convex function $f$ that satisfies $$f(y) \leq f(x) + \nabla f(x)^T(y-x) + \dfrac{L}{2}\|y-x\|^2$$ is Lipschitz Smooth (Note: 1. the reverse implication is referred to as the ""quadratic upper bound property"" 2. One poster suggested to use fenchel duality to show this Lipschitz Smoothness, Strong Convexity and the Hessian ) Proof attempt: It seems that the direct approach is through re-arrange and combine, which gives:  $$0 \leq (\nabla f(x)-\nabla f(y))^T(y-x) +  L\|y-x\|^2$$ $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    L\|y-x\|^2$$ Using CS-inequality on the above $(\nabla f(y)-\nabla f(x))^T(y-x) \leq    L\|y-x\|^2$ gives: $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    \|\nabla f(y)-\nabla f(x)\|\|y-x\|$$ Now I have: $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    L\|y-x\|^2$$ $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    \|\nabla f(y)-\nabla     f(x)\|\|y-x\|$$ How do I conclude $\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$?","Pg 12 - 14 http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf Def : A $C^1$ convex function $f$ is Lipschitz smooth if $\exists L > 0$ s.t. $\forall x, y\in \mathbb{R}^n$     \begin{equation} 		\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\| 	\end{equation} Claim: A $C^1$ convex function $f$ that satisfies $$f(y) \leq f(x) + \nabla f(x)^T(y-x) + \dfrac{L}{2}\|y-x\|^2$$ is Lipschitz Smooth (Note: 1. the reverse implication is referred to as the ""quadratic upper bound property"" 2. One poster suggested to use fenchel duality to show this Lipschitz Smoothness, Strong Convexity and the Hessian ) Proof attempt: It seems that the direct approach is through re-arrange and combine, which gives:  $$0 \leq (\nabla f(x)-\nabla f(y))^T(y-x) +  L\|y-x\|^2$$ $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    L\|y-x\|^2$$ Using CS-inequality on the above $(\nabla f(y)-\nabla f(x))^T(y-x) \leq    L\|y-x\|^2$ gives: $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    \|\nabla f(y)-\nabla f(x)\|\|y-x\|$$ Now I have: $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    L\|y-x\|^2$$ $$(\nabla f(y)-\nabla f(x))^T(y-x) \leq    \|\nabla f(y)-\nabla     f(x)\|\|y-x\|$$ How do I conclude $\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$?",,"['multivariable-calculus', 'inequality', 'proof-writing', 'convex-analysis', 'convex-optimization']"
39,"Geometric meaning of interior,exterior derivatives, and, Hodge Duality","Geometric meaning of interior,exterior derivatives, and, Hodge Duality",,"I've been tinkering with differential forms for a while now, and I've had a few questions all rolled into one trying to understand them. The exterior derivative is quite natural to me - it looks just like a regular old derivative, and so I believe I have a good intuition about how it works. The first question that struck me was that I did not have a similar intuition about the interior derivative. I wanted to know in what sense it was a derivative, if any, and either way I wanted to get an idea of what this guy really does. A geometric interpretation would have been ideal. At one point I was thinking about tensor contractions and how that's kind of like a trace, but that's not really what the interior derivative is anyway, so I ended up putting that aside for a while. But I noticed something interesting that has given me renewed interest in the question when I learned about Hodge duality. We will work in $\mathbb R^3$ with the usual coordinates. Consider an arbitrary $1-$ form which we'll write down as $F_x dx + F_y dy + F_z dz$ . We'll compute the exterior derivative of this, and we end up with $$ \bigg ( \frac{\partial F_x}{\partial y} - \frac{\partial F_y}{\partial x} \bigg ) dx \wedge dy + \bigg ( \frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x} \bigg ) dz \wedge dx + \bigg ( \frac{\partial F_y}{\partial z} - \frac{\partial F_z}{\partial y} \bigg ) dy \wedge dz $$ Amazingly (at least to me), this is the Hodge dual of the curl! Now, we can do a similar computation using $d(i_X)$ . If we let $X = \alpha \frac{\partial }{\partial x} + \beta \frac{\partial}{\partial y} + \gamma \frac{\partial}{\partial z}$ . $X$ is a vector field so these could be functions of $x,y,z$ , it doesn't really matter. Now if we let $\omega$ denote the $3-$ form, $dx \wedge dy \wedge dz$ . We can now compute. The computation is a little laborious to do directly from the definition, but there's a nice little formula which we can use to shortcut the computation. What we end up with is $$ \alpha dy \wedge dz + \beta dz\wedge dx + \gamma dy \wedge dz$$ after a few little rearrangements. Now we take the exterior derivative of this, and we have $$\bigg (\frac{\partial \alpha}{\partial x} + \frac{\partial \beta}{\partial y} + \frac{\partial \gamma}{\partial z} \bigg ) (dx \wedge dy \wedge dz) $$ And again in a way that totally amazes me, this is Hodge dual of the divergence of $X$ ! Now this motivates the question: What is the geometric significance Hodge duality? If there was some nice way to think about what the Hodge dual of something is, it would help me to understand the interior derivative, maybe. There is a problem though, which makes me worry that all of this is just coincedence of some sort - the above analysis depends on the fact that the Hodge dual on a 3-manifold takes 1-forms to 2-forms, which is how we able to see what's going on with the curl. In particular, curl is not even defined in dimensions different from 3. Divergence is defined in general though, so that much is a welcome sign. So somehow, I want to say that what I have learned will give me an avenue to understanding interior derivatives, and exterior derivatives better, but I don't know how to interpret Hodge duality, and I have these concerns. In Summary: How is the interior derivative a derivative, geometrically? What is the geometric content of Hodge duality? Can I put these together to understand interior and exterior derivatives?","I've been tinkering with differential forms for a while now, and I've had a few questions all rolled into one trying to understand them. The exterior derivative is quite natural to me - it looks just like a regular old derivative, and so I believe I have a good intuition about how it works. The first question that struck me was that I did not have a similar intuition about the interior derivative. I wanted to know in what sense it was a derivative, if any, and either way I wanted to get an idea of what this guy really does. A geometric interpretation would have been ideal. At one point I was thinking about tensor contractions and how that's kind of like a trace, but that's not really what the interior derivative is anyway, so I ended up putting that aside for a while. But I noticed something interesting that has given me renewed interest in the question when I learned about Hodge duality. We will work in with the usual coordinates. Consider an arbitrary form which we'll write down as . We'll compute the exterior derivative of this, and we end up with Amazingly (at least to me), this is the Hodge dual of the curl! Now, we can do a similar computation using . If we let . is a vector field so these could be functions of , it doesn't really matter. Now if we let denote the form, . We can now compute. The computation is a little laborious to do directly from the definition, but there's a nice little formula which we can use to shortcut the computation. What we end up with is after a few little rearrangements. Now we take the exterior derivative of this, and we have And again in a way that totally amazes me, this is Hodge dual of the divergence of ! Now this motivates the question: What is the geometric significance Hodge duality? If there was some nice way to think about what the Hodge dual of something is, it would help me to understand the interior derivative, maybe. There is a problem though, which makes me worry that all of this is just coincedence of some sort - the above analysis depends on the fact that the Hodge dual on a 3-manifold takes 1-forms to 2-forms, which is how we able to see what's going on with the curl. In particular, curl is not even defined in dimensions different from 3. Divergence is defined in general though, so that much is a welcome sign. So somehow, I want to say that what I have learned will give me an avenue to understanding interior derivatives, and exterior derivatives better, but I don't know how to interpret Hodge duality, and I have these concerns. In Summary: How is the interior derivative a derivative, geometrically? What is the geometric content of Hodge duality? Can I put these together to understand interior and exterior derivatives?","\mathbb R^3 1- F_x dx + F_y dy + F_z dz  \bigg ( \frac{\partial F_x}{\partial y} - \frac{\partial F_y}{\partial x} \bigg ) dx \wedge dy + \bigg ( \frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x} \bigg ) dz \wedge dx + \bigg ( \frac{\partial F_y}{\partial z} - \frac{\partial F_z}{\partial y} \bigg ) dy \wedge dz  d(i_X) X = \alpha \frac{\partial }{\partial x} + \beta \frac{\partial}{\partial y} + \gamma \frac{\partial}{\partial z} X x,y,z \omega 3- dx \wedge dy \wedge dz  \alpha dy \wedge dz + \beta dz\wedge dx + \gamma dy \wedge dz \bigg (\frac{\partial \alpha}{\partial x} + \frac{\partial \beta}{\partial y} + \frac{\partial \gamma}{\partial z} \bigg ) (dx \wedge dy \wedge dz)  X","['multivariable-calculus', 'vector-analysis', 'differential-forms', 'tensors', 'exterior-algebra']"
40,Deriving the curl of a vector field from the definition of torque.,Deriving the curl of a vector field from the definition of torque.,,"I just learned about the definition of $\text{curl}\ F$ for some vector field $F(x, y)=M(x, y)\mathbf{e}_1+N(x, y)\mathbf{e}_2$ in $\Bbb{R}^2$ and was wondering how that could be derived from the definition of torque: $$ \tau=F\times r $$ My interpretation of curl is that it represents an infinitesimal rate of torque at a point, so my intuition was to set $r = \langle \text{d}x,\ \text{d}y \rangle$, since then $\tau$ would represent the torque infinitesimally close to a point $\langle x,\ y\rangle$. Now of course, for the cross product to make sense $F$ and $r$ have to be in $\Bbb{R}^3$, so we can just add on a third component of zero to both of them. Now we have $$ F = \langle M(x, y),\ N(x, y),\ 0\rangle \\ r = \langle \text{d}x,\ \text{d}y,\ 0\rangle  $$ Computing the cross product gives $$ \tau = \langle 0,\ 0,\ M(x, y)\text{d}y-N(x, y)\text{d}x\rangle $$ Then I assumed that since $\tau$ is a vector and curl is a scalar, we can obtain the curl from $\tau$ by computing it's magnitude, which gives: $$ \text{curl}\ F = ||\tau|| = M(x, y)\text{d}y-N(x, y)\text{d}x $$ Obviously that's wrong, but it seems very close (at least in my opinion) to the actual definition of curl: $$ \text{curl}\ F = N_x-M_y $$ My ""curl"" has $M\text{d}y$ instead of $M_y$ and $N\text{d}x$ instead of $N_x$, and is off by a factor of $-1$. Am I on the right track? I feel really close, but I just can't figure out where to go from here to get the right definition of curl. Is curl actually even equivalent to torque, or is it equivalent to something else?","I just learned about the definition of $\text{curl}\ F$ for some vector field $F(x, y)=M(x, y)\mathbf{e}_1+N(x, y)\mathbf{e}_2$ in $\Bbb{R}^2$ and was wondering how that could be derived from the definition of torque: $$ \tau=F\times r $$ My interpretation of curl is that it represents an infinitesimal rate of torque at a point, so my intuition was to set $r = \langle \text{d}x,\ \text{d}y \rangle$, since then $\tau$ would represent the torque infinitesimally close to a point $\langle x,\ y\rangle$. Now of course, for the cross product to make sense $F$ and $r$ have to be in $\Bbb{R}^3$, so we can just add on a third component of zero to both of them. Now we have $$ F = \langle M(x, y),\ N(x, y),\ 0\rangle \\ r = \langle \text{d}x,\ \text{d}y,\ 0\rangle  $$ Computing the cross product gives $$ \tau = \langle 0,\ 0,\ M(x, y)\text{d}y-N(x, y)\text{d}x\rangle $$ Then I assumed that since $\tau$ is a vector and curl is a scalar, we can obtain the curl from $\tau$ by computing it's magnitude, which gives: $$ \text{curl}\ F = ||\tau|| = M(x, y)\text{d}y-N(x, y)\text{d}x $$ Obviously that's wrong, but it seems very close (at least in my opinion) to the actual definition of curl: $$ \text{curl}\ F = N_x-M_y $$ My ""curl"" has $M\text{d}y$ instead of $M_y$ and $N\text{d}x$ instead of $N_x$, and is off by a factor of $-1$. Am I on the right track? I feel really close, but I just can't figure out where to go from here to get the right definition of curl. Is curl actually even equivalent to torque, or is it equivalent to something else?",,['multivariable-calculus']
41,Question about Definition of Boundary in Stokes' Theorem,Question about Definition of Boundary in Stokes' Theorem,,"I was wondering if what my teacher said was correct and complete in that in Stokes' Theorem the ""boundary curve"" of a surface can be defined as the mapping along the boundary of the two-dimensional region from which the surface is parameterized. For example, if a surface $\mathbf r(u,v)$ is a mapping from the u-v plane to x-y-z space, then the boundary of $\mathbf r(u,v)$ is the curve that you get from mapping from the u-v plane to x-y-z along the boundary of the domain region in the u-v plane. Thanks in advance! Edit: I was discussing with my teacher and we think the above is the case (although I would really like someone to please confirm) but interestingly depending on how you parameterize a surface you may get similar but not exactly the same results for the boundary curve by following the above method, although you do get equivalent results if you account for direction, and the same answer via two parametrization makes me think I'm on the right track. To demonstrate what I mean let me use an example. Call the surface S the part of the sphere $x^2+y^2+z^2=4$ above the plane $z=1$. The boundary curve is obviously the intersection of the plane and the sphere, that is the circle $x^2+y^2=3$ (and $z=1$). It seems from the fact that we can get this result using the method that traces the out the boundary curve by mapping to the surface along the domain using two different parametrizations that this definition of a boundary curve is correct (though note that the spherical one requires that two parts cancel each other out). Example: Cartesian Coordinates We can write the surface S as $\mathbf r(x,y)= \big<x,y,\sqrt{4-x^2-y^2}\big>$ (where we take the positive root since $z\geq1 \rightarrow z\geq0$). Now, the surfaces is a mapping of  all $(x,y)$ from the ""filled-in circle"" $x^2+y^2\leq3$ to it via $z=\sqrt{4-x^2-y^2}$, so tracing along the boundary of the planar domain region $x^2+y^2\leq3$, that is, $x^2+y^2=3$, and mapping to the surface gives the boundary curve $x^2+y^2=3$ with $z=1$ (the first part of which is given by the fact that the domain of the curve we are tracing along the surface is $x^2+y^2=3$ and the $z=1$ part we got from plugging in). Now in spherical coordinates we get a similar result for the boundary curve. Spherical Coordinates We can write S as $\mathbf r(\phi,\theta) = \big<2\sin\phi\cos\theta,2\sin\phi\sin\theta,2\cos\phi\big> $ where $0\leq\theta\leq2\pi$ and $0\leq\phi\leq\frac{\pi}{3}$. This gives us a rectangular domain in the ""$\phi$-$\theta$"" plane (where $\phi$ is where the x-axis would be and $\theta$ where the y-axis would be and the surface is a mapping from the rectangle into x-y-z space) bounded by the coordinate axis and the lines $\theta = 2\pi$ and $\phi=\frac{\pi}{3}$. Update: Here's an image: Going along the bottom ($\theta =0, \phi$ varying to $\frac{\pi}{3}$) gives us a downward arch along the sphere from the top to the bounding circle. Similarly, going along the right gives us the bounding circle, going along the top gives the arch but in the opposite direction (so they cancel -- I presume) and at $\phi=0$ moving down the last side of the rectangle (where $\theta$ is varying from $2\pi$ back to $0$) all we get is a point which doesn't contribute to the curve leaving us with the same boundary curve to S. To recap, as we map to the surface alone the boundary of the domain, we go from the ""north pole"", down a longitude to the 60-degree latitude, around the latitude 360 degrees and back up the same longitude to the north pole, gibing us the boundary circle. In both cases you get the ""boundary ring"" by mapping to the surface along the boundary of the planar domain region. I was wondering if this method can be a general definition of the boundary of a surface. If someone could confirm or correct that would be great.","I was wondering if what my teacher said was correct and complete in that in Stokes' Theorem the ""boundary curve"" of a surface can be defined as the mapping along the boundary of the two-dimensional region from which the surface is parameterized. For example, if a surface $\mathbf r(u,v)$ is a mapping from the u-v plane to x-y-z space, then the boundary of $\mathbf r(u,v)$ is the curve that you get from mapping from the u-v plane to x-y-z along the boundary of the domain region in the u-v plane. Thanks in advance! Edit: I was discussing with my teacher and we think the above is the case (although I would really like someone to please confirm) but interestingly depending on how you parameterize a surface you may get similar but not exactly the same results for the boundary curve by following the above method, although you do get equivalent results if you account for direction, and the same answer via two parametrization makes me think I'm on the right track. To demonstrate what I mean let me use an example. Call the surface S the part of the sphere $x^2+y^2+z^2=4$ above the plane $z=1$. The boundary curve is obviously the intersection of the plane and the sphere, that is the circle $x^2+y^2=3$ (and $z=1$). It seems from the fact that we can get this result using the method that traces the out the boundary curve by mapping to the surface along the domain using two different parametrizations that this definition of a boundary curve is correct (though note that the spherical one requires that two parts cancel each other out). Example: Cartesian Coordinates We can write the surface S as $\mathbf r(x,y)= \big<x,y,\sqrt{4-x^2-y^2}\big>$ (where we take the positive root since $z\geq1 \rightarrow z\geq0$). Now, the surfaces is a mapping of  all $(x,y)$ from the ""filled-in circle"" $x^2+y^2\leq3$ to it via $z=\sqrt{4-x^2-y^2}$, so tracing along the boundary of the planar domain region $x^2+y^2\leq3$, that is, $x^2+y^2=3$, and mapping to the surface gives the boundary curve $x^2+y^2=3$ with $z=1$ (the first part of which is given by the fact that the domain of the curve we are tracing along the surface is $x^2+y^2=3$ and the $z=1$ part we got from plugging in). Now in spherical coordinates we get a similar result for the boundary curve. Spherical Coordinates We can write S as $\mathbf r(\phi,\theta) = \big<2\sin\phi\cos\theta,2\sin\phi\sin\theta,2\cos\phi\big> $ where $0\leq\theta\leq2\pi$ and $0\leq\phi\leq\frac{\pi}{3}$. This gives us a rectangular domain in the ""$\phi$-$\theta$"" plane (where $\phi$ is where the x-axis would be and $\theta$ where the y-axis would be and the surface is a mapping from the rectangle into x-y-z space) bounded by the coordinate axis and the lines $\theta = 2\pi$ and $\phi=\frac{\pi}{3}$. Update: Here's an image: Going along the bottom ($\theta =0, \phi$ varying to $\frac{\pi}{3}$) gives us a downward arch along the sphere from the top to the bounding circle. Similarly, going along the right gives us the bounding circle, going along the top gives the arch but in the opposite direction (so they cancel -- I presume) and at $\phi=0$ moving down the last side of the rectangle (where $\theta$ is varying from $2\pi$ back to $0$) all we get is a point which doesn't contribute to the curve leaving us with the same boundary curve to S. To recap, as we map to the surface alone the boundary of the domain, we go from the ""north pole"", down a longitude to the 60-degree latitude, around the latitude 360 degrees and back up the same longitude to the north pole, gibing us the boundary circle. In both cases you get the ""boundary ring"" by mapping to the surface along the boundary of the planar domain region. I was wondering if this method can be a general definition of the boundary of a surface. If someone could confirm or correct that would be great.",,"['multivariable-calculus', 'differential-geometry', 'definition', 'vector-analysis', 'surfaces']"
42,Prove $ \int_{C}fdr=\int_{S}dS\times\nabla f$,Prove, \int_{C}fdr=\int_{S}dS\times\nabla f,"Prove $\displaystyle \int_{C}fdr=\int_{S}dS\times\nabla f$. where $C=\partial S$ and the usual relationship between orientations hold. Apply Stokes's theorem to $F=af$ where $a$ is an arbitrary constant vector. From this identity and because $\nabla \times \mathbf{a = 0 }, $ thus $ \nabla\times F=0 + \nabla f \times a$. Thus $(\nabla\times F)\cdot d\mathbf{ S }= (\nabla f \times a)  \cdot d\mathbf{ S } = (d\mathbf{ S }\times  \nabla f)  \cdot a $, thanks to the answer below. Then Stokes's theorem for arbitrary a implies $ \int_{C}f \mathbf{ a } \; d\mathbf{ r } =  \iint_S   (d\mathbf{ S }\times  \nabla f)  \cdot a  $. My concern: How do I proceed from here? Please explain steps in detail?","Prove $\displaystyle \int_{C}fdr=\int_{S}dS\times\nabla f$. where $C=\partial S$ and the usual relationship between orientations hold. Apply Stokes's theorem to $F=af$ where $a$ is an arbitrary constant vector. From this identity and because $\nabla \times \mathbf{a = 0 }, $ thus $ \nabla\times F=0 + \nabla f \times a$. Thus $(\nabla\times F)\cdot d\mathbf{ S }= (\nabla f \times a)  \cdot d\mathbf{ S } = (d\mathbf{ S }\times  \nabla f)  \cdot a $, thanks to the answer below. Then Stokes's theorem for arbitrary a implies $ \int_{C}f \mathbf{ a } \; d\mathbf{ r } =  \iint_S   (d\mathbf{ S }\times  \nabla f)  \cdot a  $. My concern: How do I proceed from here? Please explain steps in detail?",,[]
43,Solution form for Stokes flows,Solution form for Stokes flows,,If  $p:\mathbb{R^3} \rightarrow \mathbb{R} $ and $u: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ satisfy: $$\nabla p-\nabla^2u=0$$ $$\nabla\cdot u=0$$ How can we prove that every solution is of the form: $$u=\nabla \phi+v$$ where  $\phi:\mathbb{R^3} \rightarrow \mathbb{R} $ and $v: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ satisfy $$\nabla^2\phi=p $$ $$\nabla^2v=0$$ $$\nabla\cdot v=-p$$ $$$$,If  $p:\mathbb{R^3} \rightarrow \mathbb{R} $ and $u: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ satisfy: $$\nabla p-\nabla^2u=0$$ $$\nabla\cdot u=0$$ How can we prove that every solution is of the form: $$u=\nabla \phi+v$$ where  $\phi:\mathbb{R^3} \rightarrow \mathbb{R} $ and $v: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ satisfy $$\nabla^2\phi=p $$ $$\nabla^2v=0$$ $$\nabla\cdot v=-p$$ $$$$,,"['multivariable-calculus', 'partial-differential-equations', 'physics']"
44,Correct Solution?,Correct Solution?,,"Suppose $$\omega = \left(\sum_{i=1}^{n} x_i^2 \right)^k \mid n >2$$ Find $k$ so that  $$ \sum_{i=0}^{n} \frac{\partial^2 \omega}{\partial x_i^2} = 0 \, \, \, \text{for all} \, \, \, x_i$$ Proposed Solution: $$ \frac{\partial \omega}{\partial x_i} =2x_i k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-1} $$ $$ \frac{\partial^2 \omega}{\partial x_i^2} = (2x_i)^2 k(k-1)\left(\sum_{i=1}^{n} x_i^2 \right)^{k-2} + 2 k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-1} = 2k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-2}\left(2(k-1)x_i^2 +  \left(\sum_{i=1}^{n} x_i^2 \right)\right)$$ Summing over $i$ yields  $$ 2k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-2} \sum_{i=0}^{n}\left(2(k-1)x_i^2 + \left(\sum_{i=1}^{n} x_i^2 \right)\right)$$ so the sum is only 0 iff $$ \sum_{i=0}^{n}\left(2(k-1)x_i^2 +  \left(\sum_{i=1}^{n} x_i^2 \right)\right) = 0$$ $$\sum_{i=0}^{n}(2(k-1)x_i^2 +  \left(\sum_{i=1}^{n} x_i^2 \right)) = 2(k-1) \sum_{i=0}^{n} x_i^2 + \sum_{i=0}^{n} \left(\sum_{i=1}^{n} x_i^2 \right) =  2(k-1) \sum_{i=0}^{n} x_i^2 + n \left(\sum_{i=1}^{n} x_i^2 \right)$$ $$( 2(k-1) + n )\left(\sum_{i=1}^{n} x_i^2 \right) = 0 \to k = 1- \frac{n}{2} $$ Is this a correct solution?","Suppose $$\omega = \left(\sum_{i=1}^{n} x_i^2 \right)^k \mid n >2$$ Find $k$ so that  $$ \sum_{i=0}^{n} \frac{\partial^2 \omega}{\partial x_i^2} = 0 \, \, \, \text{for all} \, \, \, x_i$$ Proposed Solution: $$ \frac{\partial \omega}{\partial x_i} =2x_i k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-1} $$ $$ \frac{\partial^2 \omega}{\partial x_i^2} = (2x_i)^2 k(k-1)\left(\sum_{i=1}^{n} x_i^2 \right)^{k-2} + 2 k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-1} = 2k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-2}\left(2(k-1)x_i^2 +  \left(\sum_{i=1}^{n} x_i^2 \right)\right)$$ Summing over $i$ yields  $$ 2k\left(\sum_{i=1}^{n} x_i^2 \right)^{k-2} \sum_{i=0}^{n}\left(2(k-1)x_i^2 + \left(\sum_{i=1}^{n} x_i^2 \right)\right)$$ so the sum is only 0 iff $$ \sum_{i=0}^{n}\left(2(k-1)x_i^2 +  \left(\sum_{i=1}^{n} x_i^2 \right)\right) = 0$$ $$\sum_{i=0}^{n}(2(k-1)x_i^2 +  \left(\sum_{i=1}^{n} x_i^2 \right)) = 2(k-1) \sum_{i=0}^{n} x_i^2 + \sum_{i=0}^{n} \left(\sum_{i=1}^{n} x_i^2 \right) =  2(k-1) \sum_{i=0}^{n} x_i^2 + n \left(\sum_{i=1}^{n} x_i^2 \right)$$ $$( 2(k-1) + n )\left(\sum_{i=1}^{n} x_i^2 \right) = 0 \to k = 1- \frac{n}{2} $$ Is this a correct solution?",,"['multivariable-calculus', 'partial-derivative']"
45,how to prove a parametric relation to be a function,how to prove a parametric relation to be a function,,"For example lets suppose that I have given the functions $f:\mathbb{R}\longrightarrow \mathbb{R}$ and $g:\mathbb{R}\longrightarrow \mathbb{R}$. If my relation is $R=\{(x,(y,z))\in \mathbb{R}\times \mathbb{R}^{2}: y=f(x) \wedge z=g(x)\}$ How to prove formally (from a set theoretic stand point) that $R$ is a function. I have a try but I'm not convince: Let suppose to have $(x,(y,z))\in R$ and also $(x,(y',z'))\in R$. Then $y=f(x), z=g(x)$ and also $y'=f(x), z'=g(x)$ by definition. Then $y=y'$ and also $z=z'$. Therefore $(y,z)=(y',z')$. In a more general case if I have the functions $f_1, f_2,...,f_n:\mathbb{R}^m\longrightarrow \mathbb{R}$ and I define the function $f:\mathbb{R}^{m}\longrightarrow\mathbb{R}^{n}$ such that $f(x_{1},x_{2},...,x_{m})=(f_{1}(y),f_{2}(y),...,f_{n}(y))$ with $y=(x_1,x_2,...,x_m)$, how to justify that it is indeed a function? If my try is fine I suppose this can be done by induction. Any comment will be appreciated.","For example lets suppose that I have given the functions $f:\mathbb{R}\longrightarrow \mathbb{R}$ and $g:\mathbb{R}\longrightarrow \mathbb{R}$. If my relation is $R=\{(x,(y,z))\in \mathbb{R}\times \mathbb{R}^{2}: y=f(x) \wedge z=g(x)\}$ How to prove formally (from a set theoretic stand point) that $R$ is a function. I have a try but I'm not convince: Let suppose to have $(x,(y,z))\in R$ and also $(x,(y',z'))\in R$. Then $y=f(x), z=g(x)$ and also $y'=f(x), z'=g(x)$ by definition. Then $y=y'$ and also $z=z'$. Therefore $(y,z)=(y',z')$. In a more general case if I have the functions $f_1, f_2,...,f_n:\mathbb{R}^m\longrightarrow \mathbb{R}$ and I define the function $f:\mathbb{R}^{m}\longrightarrow\mathbb{R}^{n}$ such that $f(x_{1},x_{2},...,x_{m})=(f_{1}(y),f_{2}(y),...,f_{n}(y))$ with $y=(x_1,x_2,...,x_m)$, how to justify that it is indeed a function? If my try is fine I suppose this can be done by induction. Any comment will be appreciated.",,"['elementary-set-theory', 'functions', 'multivariable-calculus']"
46,algebraic manipulation of differential form,algebraic manipulation of differential form,,"Suppose $\phi_1, \phi_2, \dots, \phi_k \in (\mathbb{R}^n)^*$, and $\mathbf{v}_1, \dots, \mathbf{v}_k \in \mathbb{R}^n$ $(\mathbb{R}^n)^*$ stands for the space of all linear transformations that goes from $\mathbb{R}^n \to \mathbb{R}$ is it true that: $$\phi_1\wedge\dots\wedge\phi_k(\mathbf{v}_1, \dots, \mathbf{v}_k) = \mathrm{det}[\phi_i(\mathbf{v}_j)]$$ This is a homework question (from Multivariable Mathematics , Shifrin, Ex.18 on Page 347) and there is a hint that:we could first express all $\phi_i$ in the form of: $$\phi_i = \sum\limits_{j=1}^n a_{ij}dx_j$$ where $dx_j$ is the basis 1-form and the hint also says that it suffices to prove this equality in the case where all $v_j$ are standard basis vectors i.e. $v_1=e_{j_1}, \dots, v_k = e_{j_k}$ From the setup of the hint, it is quite obvious that the right side equals: $$ \begin{vmatrix} a_{1j_1}&\cdots&a_{1j_k}\\ \vdots&\ddots&\vdots\\ a_{kj_1}&\cdots&a_{kj_k}\\ \end{vmatrix} $$ However, I cannot understand how to show the left side equals this determinant. Also, I am not very sure why it suffices to show that the equality holds when $v_j$ are all standard basis vectors. Is it because I could express every $v_j$ as a linear combination of the standard basis vectors? Thank you very much!","Suppose $\phi_1, \phi_2, \dots, \phi_k \in (\mathbb{R}^n)^*$, and $\mathbf{v}_1, \dots, \mathbf{v}_k \in \mathbb{R}^n$ $(\mathbb{R}^n)^*$ stands for the space of all linear transformations that goes from $\mathbb{R}^n \to \mathbb{R}$ is it true that: $$\phi_1\wedge\dots\wedge\phi_k(\mathbf{v}_1, \dots, \mathbf{v}_k) = \mathrm{det}[\phi_i(\mathbf{v}_j)]$$ This is a homework question (from Multivariable Mathematics , Shifrin, Ex.18 on Page 347) and there is a hint that:we could first express all $\phi_i$ in the form of: $$\phi_i = \sum\limits_{j=1}^n a_{ij}dx_j$$ where $dx_j$ is the basis 1-form and the hint also says that it suffices to prove this equality in the case where all $v_j$ are standard basis vectors i.e. $v_1=e_{j_1}, \dots, v_k = e_{j_k}$ From the setup of the hint, it is quite obvious that the right side equals: $$ \begin{vmatrix} a_{1j_1}&\cdots&a_{1j_k}\\ \vdots&\ddots&\vdots\\ a_{kj_1}&\cdots&a_{kj_k}\\ \end{vmatrix} $$ However, I cannot understand how to show the left side equals this determinant. Also, I am not very sure why it suffices to show that the equality holds when $v_j$ are all standard basis vectors. Is it because I could express every $v_j$ as a linear combination of the standard basis vectors? Thank you very much!",,"['differential-geometry', 'multivariable-calculus', 'differential-forms', 'exterior-algebra']"
47,A doubt in multivariable calculus,A doubt in multivariable calculus,,"Let $g:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ be defined by $g(x,y,z)=(3y+4z,2x-3z,x+3y)$ and let $S= \{ (x,y,z) \in \mathbb{R}^3 : 0 \leq x \leq 1  ,0 \leq y \leq 1 , 0 \leq z\leq 1 \}.$ What is the image of the region $S$ under then mapping $g$ ? My attempt: Since $0 \leq y \leq 1 , 0 \leq z\leq 1$ and that the first coordinate of the mapping $g$ is $3y+4z,$ the first coordinate of the image of the region $S$ under $g$ will be from $0$ to $7.$ I got $0$ as lower limit because $min\{ 3y+4z : 0 \leq y \leq 1 , 0 \leq z\leq 1 \}=0$ and $max\{ 3y+4z : 0 \leq y \leq 1 , 0 \leq z\leq 1 \}=7.$ By doing this for second and third coordinate of the image of $S$ under $g,$ I got $g(S)$ as $$g(S)=\{ (x,y,z) \in \mathbb{R}^3 : 0 \leq x \leq 7  ,-3 \leq y \leq 2 , 0 \leq z\leq 4 \}.$$ Have I gone wrong somewhere? I feel I have made a mistake because this was part of a bigger problem, and my answers are not matching. Any ideas would be of great help. P.S. The original question is: Find $\alpha$ such that $$\iiint_{g(s)}(2x+y-2z)dxdydz=\alpha\iiint_S z dxdydz.$$","Let be defined by and let What is the image of the region under then mapping ? My attempt: Since and that the first coordinate of the mapping is the first coordinate of the image of the region under will be from to I got as lower limit because and By doing this for second and third coordinate of the image of under I got as Have I gone wrong somewhere? I feel I have made a mistake because this was part of a bigger problem, and my answers are not matching. Any ideas would be of great help. P.S. The original question is: Find such that","g:\mathbb{R}^3 \rightarrow \mathbb{R}^3 g(x,y,z)=(3y+4z,2x-3z,x+3y) S= \{ (x,y,z) \in \mathbb{R}^3 : 0 \leq x \leq 1  ,0 \leq y \leq 1 , 0 \leq z\leq 1 \}. S g 0 \leq y \leq 1 , 0 \leq z\leq 1 g 3y+4z, S g 0 7. 0 min\{ 3y+4z : 0 \leq y \leq 1 , 0 \leq z\leq 1 \}=0 max\{ 3y+4z : 0 \leq y \leq 1 , 0 \leq z\leq 1 \}=7. S g, g(S) g(S)=\{ (x,y,z) \in \mathbb{R}^3 : 0 \leq x \leq 7  ,-3 \leq y \leq 2 , 0 \leq z\leq 4 \}. \alpha \iiint_{g(s)}(2x+y-2z)dxdydz=\alpha\iiint_S z dxdydz.","['multivariable-calculus', 'vector-analysis']"
48,Creating my own change of variables to evaluate an integral,Creating my own change of variables to evaluate an integral,,"The question asks me to evaluate the integral $$\iint_{R} e^{\frac{x+y}{x-y}} dA$$ where $R$ is trapezoid region with the vertices $(1,0), (2,0), (0,-2), (0,-1)$ . I'm supposed to suggest a possible transformation and integrate and sketch the two regions. My work : Let the transformation be $u=x-y$ , $v=x+y$ Then with some algebra, I get $x=\frac{u+v}{2}$ , and $y=-\frac{1}{2} (u-v)$ $J(u,v)=\begin{vmatrix} \frac{1}{2} & \frac{1}{2}\\  -\frac{1}{2} & \frac{1}{2} \end{vmatrix}=\frac{1}{2}$ When I sketch the region I have something like this on the xy plane On the uv plane the transformation looks like: So the integral becomes $$\int_{1}^{2}\int_{-u}^{u} e^{\frac{v}{u}}*\frac{1}{2} dv du$$ $$\frac{1}{2}\int_{1}^{2}u\Big(e-\frac{1}{e}\Big)du$$ $$=\frac{1}{2}\Big(e-\frac{1}{e}\Big)*\frac{3}{2}=\frac{3}{4}\Big(e-\frac{1}{e}\Big)$$ Does this look correct?","The question asks me to evaluate the integral where is trapezoid region with the vertices . I'm supposed to suggest a possible transformation and integrate and sketch the two regions. My work : Let the transformation be , Then with some algebra, I get , and When I sketch the region I have something like this on the xy plane On the uv plane the transformation looks like: So the integral becomes Does this look correct?","\iint_{R} e^{\frac{x+y}{x-y}} dA R (1,0), (2,0), (0,-2), (0,-1) u=x-y v=x+y x=\frac{u+v}{2} y=-\frac{1}{2} (u-v) J(u,v)=\begin{vmatrix}
\frac{1}{2} & \frac{1}{2}\\ 
-\frac{1}{2} & \frac{1}{2}
\end{vmatrix}=\frac{1}{2} \int_{1}^{2}\int_{-u}^{u} e^{\frac{v}{u}}*\frac{1}{2} dv du \frac{1}{2}\int_{1}^{2}u\Big(e-\frac{1}{e}\Big)du =\frac{1}{2}\Big(e-\frac{1}{e}\Big)*\frac{3}{2}=\frac{3}{4}\Big(e-\frac{1}{e}\Big)","['multivariable-calculus', 'change-of-variable']"
49,"Assumption in prooving the Inverse Function Theorem (in Spivak's ""Calculus on manifolds"")","Assumption in prooving the Inverse Function Theorem (in Spivak's ""Calculus on manifolds"")",,"My question follows up with an additional remark from Spivak's proof of Inverse Function Theorem . The problem I have is the statement which immediately follows the If the theorem is true for $^{1}f$ , it is clearly true for f... statement (from the link I've posted), in which Spivak assumes ""at the outset"" that $$ is the identity function, i.e. $=I$ , while $$ was clearly defined as $=Df(a)$ . How can he even assume this without loss of generality? He's basically limiting himself to functions $f$ such that $Df(a)=I$ . Did I get this wrong?","My question follows up with an additional remark from Spivak's proof of Inverse Function Theorem . The problem I have is the statement which immediately follows the If the theorem is true for , it is clearly true for f... statement (from the link I've posted), in which Spivak assumes ""at the outset"" that is the identity function, i.e. , while was clearly defined as . How can he even assume this without loss of generality? He's basically limiting himself to functions such that . Did I get this wrong?",^{1}f  =I  =Df(a) f Df(a)=I,"['multivariable-calculus', 'inverse-function-theorem']"
50,All smooth real functions are related by coordinate change?,All smooth real functions are related by coordinate change?,,"Suppose you have two smooth functions $F,G:\mathbb{R}^n\rightarrow \mathbb{R}$ with the property that none of the partial derivatives vanish anywhere.  I wonder if it's possible to find functions  $P:\mathbb{R}^n\leftrightarrow \mathbb{R}^n$ and $Q:\mathbb{R}\rightarrow \mathbb{R}$ such that $$G = Q\circ F\circ P.$$ I feel like it should be possible, using Jacobians or something as a change of coordinates, but I'm not sure how to begin or if I need further restrictions. If this result is true, it would imply that in some sense all real functions of this form with non-vanishing partial derivatives are equivalent through a kind of change of coordinates. Here's a solution for a special case. If the dimension is $n=1$, then $F$ and $G$ are smooth monotone functions and are therefore invertible. Let $P$ be the identity function, and let $Q=G\circ F^{-1}$. Of course, if $F$ is not also surjective, then $Q$ is not well-defined everywhere; perhaps it is possible to extend $Q$ to all of $\mathbb{R}$, though I'm not sure how smoothly. And if the dimension $n>1$, then here is a sketch of what I think shows the result: Intuitively, there ought to be (integral?) curves $\gamma_1,\gamma_2:\mathbb{R}^1\rightarrow \mathbb{R}^n$ through the domains of $F$ and $G$ such that $F\circ \gamma_1$ is a monotonic function passing through all possible values of $F$, and similarly for $G$. These curves define a fixed transformation between the codomains of $F$ and $G$, by $Q\equiv (G\circ \gamma_2) \circ (F\circ \gamma_1)^{-1}$. Intuitively, it should be possible to parametrize the graphs of $F$ and $G$ using $n$ coordinates, where the first coordinate tells you the level and the remaining coordinates uniquely specify a point in the domain with that level. Because of the conditions on $F$ and $G$, I expect these can be expressed as smooth functions $\mathbb{R}^n\rightarrow \mathbb{R}^n$. In another way of looking at it, there should be smooth functions $T_1, T_2 : \mathbb{R}^{n-1} \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ which carry points in $\mathbb{R}^n$ smoothly and invertibly through all other points in $\mathbb{R}^n$ with the same $F$ (respectively $G$) level. We think of the second argument as being the point $x$, and the first argument as being a ""displacement"" within the level surface at $F(x)$. (I am a little concerned about level sets shaped like $n$-spheres, in which case this construction doesn't work invertibly but perhaps such level sets are not possible if $F$, $G$ have everywhere nonzero partials. In that case, the level sets will be sort of $\mathbb{R}^{n-1}$-shaped, always.) By combining the flow functions $T_i$ and the canonical curves $\gamma_i$, we obtain functions $H_i \equiv T_i(s, \gamma_i(t))$ which send $\mathbb{R}^{n-1} \times \mathbb{R}$ smoothly into all of $\mathbb{R}^n$. Intuitively, there should be a smooth isomorphism $P:\mathbb{R}^n\rightarrow \mathbb{R}^n$ which sends the level sets of $G$ to the level sets of $F$. In particular, we should be able to pick one which sends the canonical points $\gamma_2(\mathbb{R})$ to the canonical points $\gamma_1(\mathbb{R})$ so that $P\circ \gamma_2 = \gamma_1$. We can do so concretely by defining $P(x) = H_1\circ H_2^{-1}$ Hence our solution is to use $Q\equiv (G\circ \gamma_2) \circ (F\circ \gamma_1)^{-1}$, sending levels of $F$ monotonically to levels of $G$, and $P\equiv H_1\circ H_2^{-1}$, sending level sets of $G$ to level sets of $F$ in a monotonic-like way. (specifically, if $G(x) < G(y)$, then $F\circ P(x) < F\circ P(y)$ or something like that). (Here, $H_2^{-1}$ looks up the unique way of expressing point $x$ as ""distance"" traveled along a $G$-level curve from some canonical other point with the same $G$-level. The result is a flow amount $s$; $H_1$ looks up the point with the same $F$-level as $x$ and applies the flow amount $s$ to it, resulting in a uniquely defined new point $x^\prime$, with certain special properties.) Does this kind of construction work? I think if $n>1$ then we can construct $H_i$ as follows: because the partials of $F$ and $G$ don't vanish, we should be able to find unit vector fields $\mathbb{R}^n\rightarrow \mathbb{R}^n$ which are smooth and perpendicular to the gradients of $F$ and $G$, respectively; then the flow functions $H_i$ can simply translate along the integral surfaces of these vector fields.  I think the only obstacle I see is if the integral surfaces are of incompatible shape for example, if one is a line and the other a circle. But maybe the level surfaces can't be things like circles, because then the functions $F$ and $G$ wouldn't have everywhere nonvanishing partials (?). Here's a concrete example of this construction for if $F$ is the sum function and $G$ is the multiplication function (ignoring the fact that $G$'s level sets have many branches): We can pick canonical curves $\gamma_1(t) = \langle \vec{0}, t \rangle$ and $\gamma_2(t) = \langle \vec{1}, t\rangle$ so that $F\circ \gamma_1$ and $G\circ \gamma_2$ pass through all possible values. These curves define the level-transforming function $Q \equiv (G\circ \gamma_2) \circ (F \circ \gamma_{1})^{-1}$; in this case, you can find that $Q$ is just the identity. We can pick flow functions to send points to all other points on the same level surface : $T_1(s,x) = x + \bar{s}$, where $\bar{s} \equiv \langle s_1, \ldots, s_{n-1}, \sum_i s_i\rangle$. $T_2(s, x) \equiv x \cdot \bar{s}$, where $\bar{s} \equiv \langle e^{s_1}, \ldots, e^{s_{n-1}}, e^{\sum_i s_i}\rangle.$ In combination with the canonical curves $\gamma_i$, these flow functions give us $H_i(s,t) = T_i(s, \gamma_i(t))$. Here, $H_1(s,t) = \langle \vec{s}, t - \sum_i s_i\rangle$ and $H_2(s,t) = \langle e^\vec{s}, t\cdot e^{-\sum_i s_i} \rangle$. Hence we define $P \equiv H_1 \circ H_{2}^{-1}$. To be clear, $H_2^{-1}(y) = \langle \log{y_1}, \ldots, \log{y_{n-1}}, \prod_i y_i \rangle$. In which case, $$P(x_1,\ldots, x_n) = \langle \log{x_1}, \ldots, \log{x_{n-1}}, \prod_i x_i - \sum_{i=1}^{n-1} \log{x_i}\rangle$$ And we find that $Q\circ F \circ P = F \circ P = \prod_i x_i = G$ (if you add up all the entries in $P$, the log terms all cancel and you're left with the product.) There are of course other $P$ and $Q$ with this same effect; it all depends on the choice of canonical curve $\gamma_1, \gamma_2$, for instance.","Suppose you have two smooth functions $F,G:\mathbb{R}^n\rightarrow \mathbb{R}$ with the property that none of the partial derivatives vanish anywhere.  I wonder if it's possible to find functions  $P:\mathbb{R}^n\leftrightarrow \mathbb{R}^n$ and $Q:\mathbb{R}\rightarrow \mathbb{R}$ such that $$G = Q\circ F\circ P.$$ I feel like it should be possible, using Jacobians or something as a change of coordinates, but I'm not sure how to begin or if I need further restrictions. If this result is true, it would imply that in some sense all real functions of this form with non-vanishing partial derivatives are equivalent through a kind of change of coordinates. Here's a solution for a special case. If the dimension is $n=1$, then $F$ and $G$ are smooth monotone functions and are therefore invertible. Let $P$ be the identity function, and let $Q=G\circ F^{-1}$. Of course, if $F$ is not also surjective, then $Q$ is not well-defined everywhere; perhaps it is possible to extend $Q$ to all of $\mathbb{R}$, though I'm not sure how smoothly. And if the dimension $n>1$, then here is a sketch of what I think shows the result: Intuitively, there ought to be (integral?) curves $\gamma_1,\gamma_2:\mathbb{R}^1\rightarrow \mathbb{R}^n$ through the domains of $F$ and $G$ such that $F\circ \gamma_1$ is a monotonic function passing through all possible values of $F$, and similarly for $G$. These curves define a fixed transformation between the codomains of $F$ and $G$, by $Q\equiv (G\circ \gamma_2) \circ (F\circ \gamma_1)^{-1}$. Intuitively, it should be possible to parametrize the graphs of $F$ and $G$ using $n$ coordinates, where the first coordinate tells you the level and the remaining coordinates uniquely specify a point in the domain with that level. Because of the conditions on $F$ and $G$, I expect these can be expressed as smooth functions $\mathbb{R}^n\rightarrow \mathbb{R}^n$. In another way of looking at it, there should be smooth functions $T_1, T_2 : \mathbb{R}^{n-1} \times \mathbb{R}^n \rightarrow \mathbb{R}^n$ which carry points in $\mathbb{R}^n$ smoothly and invertibly through all other points in $\mathbb{R}^n$ with the same $F$ (respectively $G$) level. We think of the second argument as being the point $x$, and the first argument as being a ""displacement"" within the level surface at $F(x)$. (I am a little concerned about level sets shaped like $n$-spheres, in which case this construction doesn't work invertibly but perhaps such level sets are not possible if $F$, $G$ have everywhere nonzero partials. In that case, the level sets will be sort of $\mathbb{R}^{n-1}$-shaped, always.) By combining the flow functions $T_i$ and the canonical curves $\gamma_i$, we obtain functions $H_i \equiv T_i(s, \gamma_i(t))$ which send $\mathbb{R}^{n-1} \times \mathbb{R}$ smoothly into all of $\mathbb{R}^n$. Intuitively, there should be a smooth isomorphism $P:\mathbb{R}^n\rightarrow \mathbb{R}^n$ which sends the level sets of $G$ to the level sets of $F$. In particular, we should be able to pick one which sends the canonical points $\gamma_2(\mathbb{R})$ to the canonical points $\gamma_1(\mathbb{R})$ so that $P\circ \gamma_2 = \gamma_1$. We can do so concretely by defining $P(x) = H_1\circ H_2^{-1}$ Hence our solution is to use $Q\equiv (G\circ \gamma_2) \circ (F\circ \gamma_1)^{-1}$, sending levels of $F$ monotonically to levels of $G$, and $P\equiv H_1\circ H_2^{-1}$, sending level sets of $G$ to level sets of $F$ in a monotonic-like way. (specifically, if $G(x) < G(y)$, then $F\circ P(x) < F\circ P(y)$ or something like that). (Here, $H_2^{-1}$ looks up the unique way of expressing point $x$ as ""distance"" traveled along a $G$-level curve from some canonical other point with the same $G$-level. The result is a flow amount $s$; $H_1$ looks up the point with the same $F$-level as $x$ and applies the flow amount $s$ to it, resulting in a uniquely defined new point $x^\prime$, with certain special properties.) Does this kind of construction work? I think if $n>1$ then we can construct $H_i$ as follows: because the partials of $F$ and $G$ don't vanish, we should be able to find unit vector fields $\mathbb{R}^n\rightarrow \mathbb{R}^n$ which are smooth and perpendicular to the gradients of $F$ and $G$, respectively; then the flow functions $H_i$ can simply translate along the integral surfaces of these vector fields.  I think the only obstacle I see is if the integral surfaces are of incompatible shape for example, if one is a line and the other a circle. But maybe the level surfaces can't be things like circles, because then the functions $F$ and $G$ wouldn't have everywhere nonvanishing partials (?). Here's a concrete example of this construction for if $F$ is the sum function and $G$ is the multiplication function (ignoring the fact that $G$'s level sets have many branches): We can pick canonical curves $\gamma_1(t) = \langle \vec{0}, t \rangle$ and $\gamma_2(t) = \langle \vec{1}, t\rangle$ so that $F\circ \gamma_1$ and $G\circ \gamma_2$ pass through all possible values. These curves define the level-transforming function $Q \equiv (G\circ \gamma_2) \circ (F \circ \gamma_{1})^{-1}$; in this case, you can find that $Q$ is just the identity. We can pick flow functions to send points to all other points on the same level surface : $T_1(s,x) = x + \bar{s}$, where $\bar{s} \equiv \langle s_1, \ldots, s_{n-1}, \sum_i s_i\rangle$. $T_2(s, x) \equiv x \cdot \bar{s}$, where $\bar{s} \equiv \langle e^{s_1}, \ldots, e^{s_{n-1}}, e^{\sum_i s_i}\rangle.$ In combination with the canonical curves $\gamma_i$, these flow functions give us $H_i(s,t) = T_i(s, \gamma_i(t))$. Here, $H_1(s,t) = \langle \vec{s}, t - \sum_i s_i\rangle$ and $H_2(s,t) = \langle e^\vec{s}, t\cdot e^{-\sum_i s_i} \rangle$. Hence we define $P \equiv H_1 \circ H_{2}^{-1}$. To be clear, $H_2^{-1}(y) = \langle \log{y_1}, \ldots, \log{y_{n-1}}, \prod_i y_i \rangle$. In which case, $$P(x_1,\ldots, x_n) = \langle \log{x_1}, \ldots, \log{x_{n-1}}, \prod_i x_i - \sum_{i=1}^{n-1} \log{x_i}\rangle$$ And we find that $Q\circ F \circ P = F \circ P = \prod_i x_i = G$ (if you add up all the entries in $P$, the log terms all cancel and you're left with the product.) There are of course other $P$ and $Q$ with this same effect; it all depends on the choice of canonical curve $\gamma_1, \gamma_2$, for instance.",,"['multivariable-calculus', 'jacobian']"
51,Find the area lying inside the cardioid $r=1+\cos\theta$ and outside the parabola $r(1+ \cos\theta)=1$,Find the area lying inside the cardioid  and outside the parabola,r=1+\cos\theta r(1+ \cos\theta)=1,"I need to find the area lying inside the cardioid $r=1+ \cos\theta$ and outside the parabola $r(1+ \cos\theta)=1$ . ATTEMPT First I found the intersection point of two curves which comes out to be $-\frac{\pi}{2}$ and $\frac{\pi}{2}$ . The integral setup will be $$\int_{\theta =\frac{-\pi}{2}}^{\frac{\pi}{2}}\int_{r=\frac{1}{1+\cos\theta}}^{1+\cos \theta}\ dr\ d\theta.$$ On integrating this, I got the answer as $\pi$ but answer was given to be $\frac{3\pi}{4}-\frac 43$ . Can anybody check my integral setup?","I need to find the area lying inside the cardioid and outside the parabola . ATTEMPT First I found the intersection point of two curves which comes out to be and . The integral setup will be On integrating this, I got the answer as but answer was given to be . Can anybody check my integral setup?",r=1+ \cos\theta r(1+ \cos\theta)=1 -\frac{\pi}{2} \frac{\pi}{2} \int_{\theta =\frac{-\pi}{2}}^{\frac{\pi}{2}}\int_{r=\frac{1}{1+\cos\theta}}^{1+\cos \theta}\ dr\ d\theta. \pi \frac{3\pi}{4}-\frac 43,"['multivariable-calculus', 'area', 'polar-coordinates', 'curves', 'multiple-integral']"
52,Partial derivative notation,Partial derivative notation,,"Suppose $F(x,y,z)=x+y+z$. Then the partial derivative of $F$ w.r.t. $x$ is $1$. (Most books don't mention that $y$ and $z$ are held constants and it's sort of implied.) But what if $z$ was a function of $x$? If $z=x$, then $F=2x+y$. In that case the answer should have been $2$. So what does the partial derivative w.r.t. $x$ mean? Do I always require a prior knowledge of the nature of the arguments $x$,$y$,$z$ in order to take partial derivatives?","Suppose $F(x,y,z)=x+y+z$. Then the partial derivative of $F$ w.r.t. $x$ is $1$. (Most books don't mention that $y$ and $z$ are held constants and it's sort of implied.) But what if $z$ was a function of $x$? If $z=x$, then $F=2x+y$. In that case the answer should have been $2$. So what does the partial derivative w.r.t. $x$ mean? Do I always require a prior knowledge of the nature of the arguments $x$,$y$,$z$ in order to take partial derivatives?",,"['multivariable-calculus', 'partial-derivative']"
53,Explaining the purpose of the remaining part of this proof .,Explaining the purpose of the remaining part of this proof .,,"I'm given that:= $E\subseteq \mathbb R^n$ be open and $f:E\to \mathbb R^n$ be a $C^1$ map . Suppose that for some $a\in E$ , the linear map $f'(a)$ is invertible ,and $b=f(a)$ .Then := I've to show that  := There are open set $U$ and $V$ in $\mathbb R^n$ such that $a\in U,b\in V$ and $f|_{U}$ is one-one and onto $V$ i.e. $f(U)=V$ . The proof in my notes involves $1.)$ first proving $f:U\to \mathbb R^n$ is $1$-$1$ . which I understood.. I can't understand the motive of the remaining part of the proof whose outline I'm presenting below: $2.)$ Outline of remaining part of proof : Let $V=f(U)$ then $b=f(a)\in V$ .Then we show that $V$ is open . Let $y_0=f(x_0)\in V $ and let $r\gt 0$ is such that $B=N(x_0,r)\subseteq U$ with $\overline B\subseteq U$. We show that : $N(y_0,\epsilon r)\in V$ and so $y_0$ is interior point of $V$ .Hence,then $V$ is open.. I'll be obliged if there is someone kind enough to explain me the motive behind $2.)$ and also I can't understand why nowhere in the proof did we prove  $f:U\to \mathbb R^n$ is onto , whereas it was proved in the beginning of the proof that it is $1$-$1$.. Thanks in advance for any help...","I'm given that:= $E\subseteq \mathbb R^n$ be open and $f:E\to \mathbb R^n$ be a $C^1$ map . Suppose that for some $a\in E$ , the linear map $f'(a)$ is invertible ,and $b=f(a)$ .Then := I've to show that  := There are open set $U$ and $V$ in $\mathbb R^n$ such that $a\in U,b\in V$ and $f|_{U}$ is one-one and onto $V$ i.e. $f(U)=V$ . The proof in my notes involves $1.)$ first proving $f:U\to \mathbb R^n$ is $1$-$1$ . which I understood.. I can't understand the motive of the remaining part of the proof whose outline I'm presenting below: $2.)$ Outline of remaining part of proof : Let $V=f(U)$ then $b=f(a)\in V$ .Then we show that $V$ is open . Let $y_0=f(x_0)\in V $ and let $r\gt 0$ is such that $B=N(x_0,r)\subseteq U$ with $\overline B\subseteq U$. We show that : $N(y_0,\epsilon r)\in V$ and so $y_0$ is interior point of $V$ .Hence,then $V$ is open.. I'll be obliged if there is someone kind enough to explain me the motive behind $2.)$ and also I can't understand why nowhere in the proof did we prove  $f:U\to \mathbb R^n$ is onto , whereas it was proved in the beginning of the proof that it is $1$-$1$.. Thanks in advance for any help...",,['multivariable-calculus']
54,A line integral,A line integral,,"If $\mathbf{B}(\mathbf{x})=\rho^{-1}\mathbf{e}_{\phi}$ in cylindrical polars, find: $$\int_{C}\mathbf{B}\cdot\mathrm{d}\mathbf{x}$$ where $C$ is the circle $z=0,\rho=1,\;0\leq \phi\leq 2\pi$ . Also find $\nabla\times\mathbf{B}$ My try: $$\int_{C}\mathbf{B}\cdot\mathrm{d}\mathbf{x}=\int_C (\rho^{-1}\mathbf{e}_{\phi})\cdot (\mathbf{e}_{\rho}d\rho+\rho\mathbf{e}_{\phi}d\phi+\mathbf{e}_{z}dz)=\int_C \mathrm{d}\phi=2\pi$$ This seems way too easy. Is it wrong? For the second part I get $\nabla\times\mathbf{B}=-\rho^{-3}\mathbf{e}_z$ but a friend of mine says it should be $0$ ...","If in cylindrical polars, find: where is the circle . Also find My try: This seems way too easy. Is it wrong? For the second part I get but a friend of mine says it should be ...","\mathbf{B}(\mathbf{x})=\rho^{-1}\mathbf{e}_{\phi} \int_{C}\mathbf{B}\cdot\mathrm{d}\mathbf{x} C z=0,\rho=1,\;0\leq \phi\leq 2\pi \nabla\times\mathbf{B} \int_{C}\mathbf{B}\cdot\mathrm{d}\mathbf{x}=\int_C (\rho^{-1}\mathbf{e}_{\phi})\cdot (\mathbf{e}_{\rho}d\rho+\rho\mathbf{e}_{\phi}d\phi+\mathbf{e}_{z}dz)=\int_C \mathrm{d}\phi=2\pi \nabla\times\mathbf{B}=-\rho^{-3}\mathbf{e}_z 0","['multivariable-calculus', 'vector-analysis']"
55,"If $f$ is twice differentiable, $\big(f(y) - f(x)\big)/(y-x)$ is differentiable","If  is twice differentiable,  is differentiable",f \big(f(y) - f(x)\big)/(y-x),"Suppose $f: \mathbb{R} \to \mathbb{R}$ is a $C^{1}$ function. Then, define a new function $F: \mathbb{R}^{2} \to \mathbb{R}$ by: $$ F(x,y) = \begin{cases}   \displaystyle \frac{f(y) - f(x)}{y - x} &\text{ if } x \neq y \\   \displaystyle f'(x) &\text{ otherwise.} \end{cases} $$ Then, if $f''(x)$ exists, $F$ is differentiable. I can prove that $F$ is differentiable if $x \neq y$, since under these conditions $F_{x}$ and $F_{y}$ are $C^{1}$. So it's left to prove $F$ is also differentiable if $x = y$. At first, I conjectured that, for example, $F_{x} (a,a)$ would be $f''(a)/2$, but I'm having a hard time proving it. I started using the definition $\lim_{h \to 0} (F(a+h, a) - F(a,a)) / h$ and applying the MVT found $\bar{a}$ between $a$ and $a + h$ s.t. this difference quotient is: $$ \frac{1}{h}(f'(\bar{a}) - f'(a)) $$ so I tried dividing and multiplying by $\bar{a} - a$, thinking it would be possible to prove that $\lim_{h \to 0} (\bar{a} - a)/h = 1$, but so far I've only been able to bound it above by $1$. Is it true? Does this conjecture even makes sense? I'm lost in thinking about any other candidates for the differential in these points. Any help would be appreciated!","Suppose $f: \mathbb{R} \to \mathbb{R}$ is a $C^{1}$ function. Then, define a new function $F: \mathbb{R}^{2} \to \mathbb{R}$ by: $$ F(x,y) = \begin{cases}   \displaystyle \frac{f(y) - f(x)}{y - x} &\text{ if } x \neq y \\   \displaystyle f'(x) &\text{ otherwise.} \end{cases} $$ Then, if $f''(x)$ exists, $F$ is differentiable. I can prove that $F$ is differentiable if $x \neq y$, since under these conditions $F_{x}$ and $F_{y}$ are $C^{1}$. So it's left to prove $F$ is also differentiable if $x = y$. At first, I conjectured that, for example, $F_{x} (a,a)$ would be $f''(a)/2$, but I'm having a hard time proving it. I started using the definition $\lim_{h \to 0} (F(a+h, a) - F(a,a)) / h$ and applying the MVT found $\bar{a}$ between $a$ and $a + h$ s.t. this difference quotient is: $$ \frac{1}{h}(f'(\bar{a}) - f'(a)) $$ so I tried dividing and multiplying by $\bar{a} - a$, thinking it would be possible to prove that $\lim_{h \to 0} (\bar{a} - a)/h = 1$, but so far I've only been able to bound it above by $1$. Is it true? Does this conjecture even makes sense? I'm lost in thinking about any other candidates for the differential in these points. Any help would be appreciated!",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
56,About the symmetric nature of Green's function.,About the symmetric nature of Green's function.,,What is the significance of Green's function being symmetric ? How do I understand this intuitively? Thanks in advance .,What is the significance of Green's function being symmetric ? How do I understand this intuitively? Thanks in advance .,,"['multivariable-calculus', 'partial-differential-equations']"
57,Is there something wrong with this multivariable calculus proof?,Is there something wrong with this multivariable calculus proof?,,"There are $2$ parts to the question: Let $f:\mathbb{R} ^{2}\rightarrow \mathbb{R}$ be differentiable in $\mathbb{R} ^{2}$ such that $\nabla f\equiv 0$ for every point $\left( x,y\right) \in \mathbb{R} ^{2}$ . proof that f is constant. Let $f:\mathbb{R} ^{2}\rightarrow \mathbb{R}$ be a function that satisfies $\left| f\left( x_{1},y_{1}\right) -f\left( x_{2},y_{2}\right) \right| \leq \left( x_{1}-x_{2}\right) ^{2}+\left( y_{1}-y_{2}\right) ^{2}$ for every $\left( x_{1},y_{1}\right) ,\left( x_{1},y_{1}\right) \in \mathbb{R} ^{2}$ . proof that $f$ is constant Before I go on with the proofs, I would even argue that in the first question, the word ""differentiable"" is superfluous because if we are given that $\nabla f\equiv 0$ , then it's clear that $f$ is differentiable because the partial derivatives are continuous (because they are $0$ everywhere, and what more continuous than the constant function $0$ ?), is this correct? I came up with the following proofs: Given $\nabla f\equiv 0$ this means that $f_{x}\equiv f_{y}\equiv 0$ , if we ""freeze"" $y$ to be $y_{0}$ and let $x$ vary then $f(x,y_{0})$ is constant as a result of lagrange's theorem because $f_{x}\equiv 0$ , if we did something similar on $x$ we get that $f(x_{0},y)$ is constant for every $y$ . thus for every $\left( x,y\right) \in \mathbb{R} ^{2}$ we get that $f\left( x,y\right) =f\left( 0,y\right) =f\left( 0,0\right) $ (since it is constant if we move parallel to the axis) meaning $f$ is constant. We first prove this for the 1-dimensional case: meaning if f satisfies $\left| f\left( x\right) -f\left( y\right) \right| \leq \left| x-y\right| ^{2}$ for every $x$ and $y$ then (if $x \neq y$ ) $$\left| \dfrac{f\left( x\right) -f\left( y\right) }{x-y}\right| \leq \left| x-y\right| $$ Therefore $$\left| f^{'}\left( x\right) \right| = \lim _{y\rightarrow x}\left| \dfrac{f\left( x\right) -f\left( y\right) }{x-y}\right| \leq \lim _{y\rightarrow x}\left| x-y\right| = 0$$ meaning $f$ is differentiable and constant. back to the $2$ dimensional case: if we ""freeze"" $x_{1}$ to be $x_{2}$ to be $x_{0}$ then we get that $$\left| f\left( x_{0},y_{1}\right) -f\left( x_{0},y_{2}\right) \right| \leq \left( y_{1}-y_{2}\right) ^{2}$$ and from the lemma at the begging we proved we conclude that $f_{y}\equiv 0$ the same argument goes for $x$ , and we get that $f_{x}\equiv f_{y}\equiv 0$ ; this means that (because the partial derivatives are continuous because they're $0$ ) $f$ is differentiable in $2$ variables and this coupled with the conclusion from $1$ (the first question) we get that $f$ is constant. These are the proofs I had in mind, but I don't see what's wrong with them. Can someone provide me with insight?","There are parts to the question: Let be differentiable in such that for every point . proof that f is constant. Let be a function that satisfies for every . proof that is constant Before I go on with the proofs, I would even argue that in the first question, the word ""differentiable"" is superfluous because if we are given that , then it's clear that is differentiable because the partial derivatives are continuous (because they are everywhere, and what more continuous than the constant function ?), is this correct? I came up with the following proofs: Given this means that , if we ""freeze"" to be and let vary then is constant as a result of lagrange's theorem because , if we did something similar on we get that is constant for every . thus for every we get that (since it is constant if we move parallel to the axis) meaning is constant. We first prove this for the 1-dimensional case: meaning if f satisfies for every and then (if ) Therefore meaning is differentiable and constant. back to the dimensional case: if we ""freeze"" to be to be then we get that and from the lemma at the begging we proved we conclude that the same argument goes for , and we get that ; this means that (because the partial derivatives are continuous because they're ) is differentiable in variables and this coupled with the conclusion from (the first question) we get that is constant. These are the proofs I had in mind, but I don't see what's wrong with them. Can someone provide me with insight?","2 f:\mathbb{R} ^{2}\rightarrow \mathbb{R} \mathbb{R} ^{2} \nabla f\equiv 0 \left( x,y\right) \in \mathbb{R} ^{2} f:\mathbb{R} ^{2}\rightarrow \mathbb{R} \left| f\left( x_{1},y_{1}\right) -f\left( x_{2},y_{2}\right) \right| \leq \left( x_{1}-x_{2}\right) ^{2}+\left( y_{1}-y_{2}\right) ^{2} \left( x_{1},y_{1}\right) ,\left( x_{1},y_{1}\right) \in \mathbb{R} ^{2} f \nabla f\equiv 0 f 0 0 \nabla f\equiv 0 f_{x}\equiv f_{y}\equiv 0 y y_{0} x f(x,y_{0}) f_{x}\equiv 0 x f(x_{0},y) y \left( x,y\right) \in \mathbb{R} ^{2} f\left( x,y\right) =f\left( 0,y\right) =f\left( 0,0\right)  f \left| f\left( x\right) -f\left( y\right) \right| \leq \left| x-y\right| ^{2} x y x \neq y \left| \dfrac{f\left( x\right) -f\left( y\right) }{x-y}\right| \leq \left| x-y\right|  \left| f^{'}\left( x\right) \right| = \lim _{y\rightarrow x}\left| \dfrac{f\left( x\right) -f\left( y\right) }{x-y}\right| \leq \lim _{y\rightarrow x}\left| x-y\right| = 0 f 2 x_{1} x_{2} x_{0} \left| f\left( x_{0},y_{1}\right) -f\left( x_{0},y_{2}\right) \right| \leq \left( y_{1}-y_{2}\right) ^{2} f_{y}\equiv 0 x f_{x}\equiv f_{y}\equiv 0 0 f 2 1 f","['multivariable-calculus', 'derivatives', 'solution-verification']"
58,If the gradient of two functions are related by a matrix can we say anything about these two functions?,If the gradient of two functions are related by a matrix can we say anything about these two functions?,,"Say we have two functions $f, g: \mathbb{R}^n \rightarrow \mathbb{R}$ and we have a matrix $M \in \mathbb{R}^{n \times n}$ which is constant, i.e. not a function of $x$ . Say we know that $\nabla f = M \nabla g$ , then can we say anything about the relationship between $f$ and $g$ ? What about in the case that $M$ is a function of $x$ ? I have not been able to discover any relation between $f$ and $g$ in either case, but I feel like there should be something. I know that we can apply the chain rule in the case where we have $\nabla f = \lambda(g(x)) \nabla g$ and $\lambda: \mathbb{R} \rightarrow \mathbb{R}$","Say we have two functions and we have a matrix which is constant, i.e. not a function of . Say we know that , then can we say anything about the relationship between and ? What about in the case that is a function of ? I have not been able to discover any relation between and in either case, but I feel like there should be something. I know that we can apply the chain rule in the case where we have and","f, g: \mathbb{R}^n \rightarrow \mathbb{R} M \in \mathbb{R}^{n \times n} x \nabla f = M \nabla g f g M x f g \nabla f = \lambda(g(x)) \nabla g \lambda: \mathbb{R} \rightarrow \mathbb{R}","['multivariable-calculus', 'scalar-fields']"
59,Integrating product of harmonic functions over sphere,Integrating product of harmonic functions over sphere,,"Im a math major student and taking a course in multivariable calculus. I straggled with the following homework exercise. Let $u,v :\mathbb{R}^n \to \mathbb{R}$ be harmonic functions (i.e. $\Delta u,\Delta v \equiv 0)$ , and let $a,b,p,q>0$ constants such that $ab=pq$ . Prove that: $$ \int_{S^{n-1}}u(ax)v(bx)-u(px)v(qx)\,\mathrm dS(x)=0, $$ where $S^{n-1}$ is the unit sphere in $\mathbb{R}^n$ . As a context for the problem, I'll add that the main tools we have been taught before this exercise (concerning harnomic functions) are: The average principle, the maximum principle, Liouville's theorem and Poisson kernel My first attempt was to check if the integrand is harmonic but it not seems to be the case. After that I thought to add some function under the integral so the new integrand will become simpler, but I fell short on that attempt too. Can you guys give me direction or a hint?","Im a math major student and taking a course in multivariable calculus. I straggled with the following homework exercise. Let be harmonic functions (i.e. , and let constants such that . Prove that: where is the unit sphere in . As a context for the problem, I'll add that the main tools we have been taught before this exercise (concerning harnomic functions) are: The average principle, the maximum principle, Liouville's theorem and Poisson kernel My first attempt was to check if the integrand is harmonic but it not seems to be the case. After that I thought to add some function under the integral so the new integrand will become simpler, but I fell short on that attempt too. Can you guys give me direction or a hint?","u,v :\mathbb{R}^n \to \mathbb{R} \Delta u,\Delta v \equiv 0) a,b,p,q>0 ab=pq 
\int_{S^{n-1}}u(ax)v(bx)-u(px)v(qx)\,\mathrm dS(x)=0,
 S^{n-1} \mathbb{R}^n","['multivariable-calculus', 'harmonic-functions', 'surface-integrals']"
60,"Find $\iint (x+y)\,dx\,dy$ in the following domain.",Find  in the following domain.,"\iint (x+y)\,dx\,dy","Find $$\iint_D(x+y)\,dx\,dy$$ While $D$ is the domain bounded by the lines: $$x+y=1,\, x+y=3,\, y=5x,\, y=10x$$ My Work: First off, I thought of changing the variables, where $u=x+y$ , and $v=\frac{y}{x}$ ( $x$ is outside my domain so I didn't care about $x=0$ , would love to hear feedback about this step). So now my bounds are $u=1, u=3, v=5, v=10$ , and I get a rectangular domain. To calculate the Jacobian that I need to multiply my integral with, I'll calculate first the inverse, $$J^{-1}    =   \left[ {\begin{array}{cc}    u_x & v_x \\    u_y & v_y \\   \end{array} } \right] = \left[ {\begin{array}{cc}    1 & \frac{-y}{x^2} \\    1 & \frac{1}{x} \\   \end{array} } \right] = \frac{1}{x} +\frac{y}{x^2}=\frac{x+y}{x^2} \Longrightarrow J = \frac{x^2}{x+y}=\frac{x^2}{u}$$ So I got: $$\iint_{D^{*}}u\frac{x^2}{u}=\iint_{D^*}(\frac{u}{v+1})^2=\int^3_1du\int^{10}_5(\frac{u}{1+v})^2dv=\int^3_1du\int^{10}_5u^2*(1+v)^{-2}dv = \int^3_1-\frac{u^2}{1+v} |^{10}_5du= \int^3_1\frac{5u^2}{66}du=\frac{5*3^3}{198} - \frac{5}{198}=\frac{65}{99}$$ EDITS: Fixed the integral after the help comment from MathLover that $x=\frac{u}{1+v}$ Updated the solution, had mistake in calculations","Find While is the domain bounded by the lines: My Work: First off, I thought of changing the variables, where , and ( is outside my domain so I didn't care about , would love to hear feedback about this step). So now my bounds are , and I get a rectangular domain. To calculate the Jacobian that I need to multiply my integral with, I'll calculate first the inverse, So I got: EDITS: Fixed the integral after the help comment from MathLover that Updated the solution, had mistake in calculations","\iint_D(x+y)\,dx\,dy D x+y=1,\, x+y=3,\, y=5x,\, y=10x u=x+y v=\frac{y}{x} x x=0 u=1, u=3, v=5, v=10 J^{-1}
   =
  \left[ {\begin{array}{cc}
   u_x & v_x \\
   u_y & v_y \\
  \end{array} } \right] = \left[ {\begin{array}{cc}
   1 & \frac{-y}{x^2} \\
   1 & \frac{1}{x} \\
  \end{array} } \right] = \frac{1}{x} +\frac{y}{x^2}=\frac{x+y}{x^2} \Longrightarrow J = \frac{x^2}{x+y}=\frac{x^2}{u} \iint_{D^{*}}u\frac{x^2}{u}=\iint_{D^*}(\frac{u}{v+1})^2=\int^3_1du\int^{10}_5(\frac{u}{1+v})^2dv=\int^3_1du\int^{10}_5u^2*(1+v)^{-2}dv = \int^3_1-\frac{u^2}{1+v} |^{10}_5du= \int^3_1\frac{5u^2}{66}du=\frac{5*3^3}{198} - \frac{5}{198}=\frac{65}{99} x=\frac{u}{1+v}","['multivariable-calculus', 'solution-verification', 'multiple-integral']"
61,Is a path which decreases a function in the quickest way a gradient flow?,Is a path which decreases a function in the quickest way a gradient flow?,,"Let $U \subseteq \mathbb{R}^n$ be open, and let $F:U \to \mathbb{R}$ be a smooth function. Fix a point $p \in U$ , and suppose that $\nabla F(p) \neq 0$ . Let $\alpha(t)$ be a $C^{\infty}$ path starting at $p$ . Suppose that $\alpha$ ""beats"" all $C^{\infty}$ paths starting at time $t=0$ for a short time in the following sense: For every $C^{\infty}$ path $\beta(t)$ starting at $p$ that satisfies $\|\dot \beta(t)\|=\|\dot \alpha(t)\|$ , we have $F(\alpha(t)) \le F(\beta(t))$ for sufficiently small $t>0$ . (The ""sufficiently small"" here might depend on the path $\beta$ ). Question: Must $\alpha$ be a reparametrization of the  negative gradient flow of $F$ , i.e. $$ \alpha(0)=p, \, \, \dot \alpha(t)=c(t)\cdot \big(-\nabla F(\alpha(t))\big) \,\, \text{where } c(t)>0  \,\,?$$ It is not hard to see that we must have $\dot \alpha(0)=-\nabla F(p)$ (up to a positive rescaling). If we could show that $\alpha$ locally ""beats"" all $C^{\infty}$ paths starting at $\alpha(t)$ for sufficiently small $t>0$ , then the same logic would imply the required claim. I don't know how to ""propagate"" this optimality criterion from time $t=0$ to a time $t>0$ . Here is my naive attempt: Assume by contradiction that $\alpha$ does not beat all paths at some interval $[0,\epsilon)$ . Then there exist a decreasing sequence $t_n \to 0$ which demonstrates the non-optimality of $\alpha|_{[t_n,)}$ as a path starting at $\alpha(t_n)$ . This means that there exist smooth paths $\beta_n:[t_n,.) \to U$ , $\beta_n(t_n)=\alpha_n(t_n)$ , and $s_n>t_n$ where $s_n-t_n$ is arbitrarily small, such that $F(\alpha(s_n)) > F(\beta_n(s_n))$ . Now, I guess I should somehow take the limit of the $\beta_n$ or ""glue"" them together to obtain a path which starts at $p=\alpha(0)$ , and that beats $\alpha$ . I am not sure how to do that.","Let be open, and let be a smooth function. Fix a point , and suppose that . Let be a path starting at . Suppose that ""beats"" all paths starting at time for a short time in the following sense: For every path starting at that satisfies , we have for sufficiently small . (The ""sufficiently small"" here might depend on the path ). Question: Must be a reparametrization of the  negative gradient flow of , i.e. It is not hard to see that we must have (up to a positive rescaling). If we could show that locally ""beats"" all paths starting at for sufficiently small , then the same logic would imply the required claim. I don't know how to ""propagate"" this optimality criterion from time to a time . Here is my naive attempt: Assume by contradiction that does not beat all paths at some interval . Then there exist a decreasing sequence which demonstrates the non-optimality of as a path starting at . This means that there exist smooth paths , , and where is arbitrarily small, such that . Now, I guess I should somehow take the limit of the or ""glue"" them together to obtain a path which starts at , and that beats . I am not sure how to do that.","U \subseteq \mathbb{R}^n F:U \to \mathbb{R} p \in U \nabla F(p) \neq 0 \alpha(t) C^{\infty} p \alpha C^{\infty} t=0 C^{\infty} \beta(t) p \|\dot \beta(t)\|=\|\dot \alpha(t)\| F(\alpha(t)) \le F(\beta(t)) t>0 \beta \alpha F  \alpha(0)=p, \, \, \dot \alpha(t)=c(t)\cdot \big(-\nabla F(\alpha(t))\big) \,\, \text{where } c(t)>0  \,\,? \dot \alpha(0)=-\nabla F(p) \alpha C^{\infty} \alpha(t) t>0 t=0 t>0 \alpha [0,\epsilon) t_n \to 0 \alpha|_{[t_n,)} \alpha(t_n) \beta_n:[t_n,.) \to U \beta_n(t_n)=\alpha_n(t_n) s_n>t_n s_n-t_n F(\alpha(s_n)) > F(\beta_n(s_n)) \beta_n p=\alpha(0) \alpha","['multivariable-calculus', 'differential-geometry', 'optimization', 'gradient-descent', 'gradient-flows']"
62,Evaluating $\int_{0}^{1-x} \frac{1}{1-x_{n}} \cdots \int_{0}^{1-x_3} \frac{1}{1-x_2} \int_{0}^{1-x_2} \frac{1}{1-x_1} dx_1 dx_2 \cdots dx_n$,Evaluating,\int_{0}^{1-x} \frac{1}{1-x_{n}} \cdots \int_{0}^{1-x_3} \frac{1}{1-x_2} \int_{0}^{1-x_2} \frac{1}{1-x_1} dx_1 dx_2 \cdots dx_n,"I am trying to evaluate the following integral $$\int_{0}^{1-x} \frac{1}{1-x_{n}} \cdots \int_{0}^{1-x_3} \frac{1}{1-x_2} \int_{0}^{1-x_2} \frac{1}{1-x_1} dx_1 dx_2 \cdots dx_{n}, \hspace{0.5cm} 0<x<1$$ I tried to do this integral leveraging some polylogarithms but I am not making much progress. Here is some of my work $$\int_{0}^{1-x_2} \frac{1}{1-x_1} dx_1 = \int_{0}^{1-x_2} \frac{x_1}{1-x_1} \frac{1}{x_1} dx_1 = \int_{0}^{1-x_2} \frac{\text{Li}_0(x_1)}{x_1} dx_1 $$ $$ = \text{Li}_1(x_1)\Big|_{0}^{1-x_2} = \text{Li}_1(1-x_2),$$ where $\text{Li}_0$ is the $0^{th}$ polylogarithm. I use the relation $\text{Li}_m(x) = \int_{0}^{x} \frac{\text{Li}_{m-1}(x')}{x'} dx'$ and the fact $\text{Li}_m(0) = 0$ . Repeating the same kind of work $$\int_{0}^{1-x_3} \frac{\text{Li}_1(1-x_2)}{1-x_2} dx_2 = - \int_{0}^{x_3} \frac{\text{Li}_1(u_2)}{u_2} du_2, \hspace{0.5cm} \text{where } u_2 = 1-x_2$$ $$\hspace{0.7cm} = \text{Li}_2(u_2)\Big|_{0}^{x_3} = \text{Li}_2(x_3)$$ I run into some problems at the third iteration $$\int_{0}^{1-x_4} \frac{\text{Li}_2(x_3)}{1-x_3} dx_3 = \int_{0}^{1-x_4} \frac{\text{Li}_2(x_3)\text{Li}_0(x_3)}{x_3} dx_3  $$ $$ = \text{Li}_2(x_3)\text{Li}_1(x_3)\Big|_{0}^{1-x_4} - \int_{0}^{1-x_4}\frac{\text{Li}_1^2(x_3)}{x_3} dx_3$$ I've seen this last integral for the interval $[0,1]$ but I'm not sure how to extend it in such a way that I can get a nice recursion or sum representation. Some relevant identities I stumbled upon while trying to solve this problem $$\text{Li}_m(x) = \sum_{k=1}^{\infty} \frac{x^k}{k^m}$$ $$\frac{\text{Li}_m(x)}{1-x} = \sum_{k=1}^{\infty} H_k^{(m)} x^k$$ Where $H_k^{(m)}$ is the generalized harmonic sum in powers $m$ .",I am trying to evaluate the following integral I tried to do this integral leveraging some polylogarithms but I am not making much progress. Here is some of my work where is the polylogarithm. I use the relation and the fact . Repeating the same kind of work I run into some problems at the third iteration I've seen this last integral for the interval but I'm not sure how to extend it in such a way that I can get a nice recursion or sum representation. Some relevant identities I stumbled upon while trying to solve this problem Where is the generalized harmonic sum in powers .,"\int_{0}^{1-x} \frac{1}{1-x_{n}} \cdots \int_{0}^{1-x_3} \frac{1}{1-x_2} \int_{0}^{1-x_2} \frac{1}{1-x_1} dx_1 dx_2 \cdots dx_{n}, \hspace{0.5cm} 0<x<1 \int_{0}^{1-x_2} \frac{1}{1-x_1} dx_1 = \int_{0}^{1-x_2} \frac{x_1}{1-x_1} \frac{1}{x_1} dx_1 = \int_{0}^{1-x_2} \frac{\text{Li}_0(x_1)}{x_1} dx_1
  = \text{Li}_1(x_1)\Big|_{0}^{1-x_2} = \text{Li}_1(1-x_2), \text{Li}_0 0^{th} \text{Li}_m(x) = \int_{0}^{x} \frac{\text{Li}_{m-1}(x')}{x'} dx' \text{Li}_m(0) = 0 \int_{0}^{1-x_3} \frac{\text{Li}_1(1-x_2)}{1-x_2} dx_2 = - \int_{0}^{x_3} \frac{\text{Li}_1(u_2)}{u_2} du_2, \hspace{0.5cm} \text{where } u_2 = 1-x_2 \hspace{0.7cm} = \text{Li}_2(u_2)\Big|_{0}^{x_3} = \text{Li}_2(x_3) \int_{0}^{1-x_4} \frac{\text{Li}_2(x_3)}{1-x_3} dx_3 = \int_{0}^{1-x_4} \frac{\text{Li}_2(x_3)\text{Li}_0(x_3)}{x_3} dx_3 
 
= \text{Li}_2(x_3)\text{Li}_1(x_3)\Big|_{0}^{1-x_4} - \int_{0}^{1-x_4}\frac{\text{Li}_1^2(x_3)}{x_3} dx_3 [0,1] \text{Li}_m(x) = \sum_{k=1}^{\infty} \frac{x^k}{k^m} \frac{\text{Li}_m(x)}{1-x} = \sum_{k=1}^{\infty} H_k^{(m)} x^k H_k^{(m)} m","['multivariable-calculus', 'multiple-integral', 'harmonic-numbers', 'polylogarithm', 'iterated-integrals']"
63,How to find a good strategy for tackling change of variables in multivariable calculus,How to find a good strategy for tackling change of variables in multivariable calculus,,"I'm in the process of learning multivariable calculus, and I've been having trouble with the process of changing variables in order to perform an easier integration. I know how to compute the Jacobian and analyze the region once it is given to me, but the problem of actually determining what my substitution should be has been giving me headaches. It's particularly difficult when the equations aren't linear. For example, here's a question I've come across and managed to solve: Use a change of variables to set up (but do not evaluate) a double integral (one only) for the area of the region in the first quadrant bounded by the curves $xy = 1$, $xy = 4$, $y = x$, and $y = 2x$. My first instinct was to set $u=xy$, so that my two equations for $u$ become $u=1$, and $u=4$. However, I wasn't sure about what my substitution for $v$ should be, so I eventually set it to $v=x$. In my transformation $uv$-space, I then had the curves $v=\sqrt{u}$ and $v=\sqrt{\frac{u}{2}}$ (the positive components, so my space looked like this (sorry, I can't post images yet): Transformed space From there, I calculated the Jacobian to be: $$\frac{\partial (u,v)}{\partial (x,y)} = v$$ Therefore, the inverse Jacobian (the one I want), was $\frac{1}{v}$. I could then set the integral up as the following: $$\int_{1}^{4} \int_{\sqrt{\frac{u}{2}}}^{\sqrt{u}} \frac{1}{v} \,dv \,du$$ This is indeed the correct answer, but when I look at the solution, my professor provided, he used the substitutions $u=xy$ and $v=\frac{y}{x}$. The transformed space he created is then a simple rectangle, which is simpler to integrate. It looked like this: $$\int_{1}^{4} \int_{1}^{2} \frac{1}{2v} \,dv \,du$$ In the end, both approaches worked (as they should), but I was wondering if there were some general guidelines or suggestions that could be given when choosing what substitution should be used? Are there any tricks or strategies for transforming regions like these into rectangles? Any help would be appreciated! P.S. Also, this is my first post to the community, so thanks for having me.","I'm in the process of learning multivariable calculus, and I've been having trouble with the process of changing variables in order to perform an easier integration. I know how to compute the Jacobian and analyze the region once it is given to me, but the problem of actually determining what my substitution should be has been giving me headaches. It's particularly difficult when the equations aren't linear. For example, here's a question I've come across and managed to solve: Use a change of variables to set up (but do not evaluate) a double integral (one only) for the area of the region in the first quadrant bounded by the curves $xy = 1$, $xy = 4$, $y = x$, and $y = 2x$. My first instinct was to set $u=xy$, so that my two equations for $u$ become $u=1$, and $u=4$. However, I wasn't sure about what my substitution for $v$ should be, so I eventually set it to $v=x$. In my transformation $uv$-space, I then had the curves $v=\sqrt{u}$ and $v=\sqrt{\frac{u}{2}}$ (the positive components, so my space looked like this (sorry, I can't post images yet): Transformed space From there, I calculated the Jacobian to be: $$\frac{\partial (u,v)}{\partial (x,y)} = v$$ Therefore, the inverse Jacobian (the one I want), was $\frac{1}{v}$. I could then set the integral up as the following: $$\int_{1}^{4} \int_{\sqrt{\frac{u}{2}}}^{\sqrt{u}} \frac{1}{v} \,dv \,du$$ This is indeed the correct answer, but when I look at the solution, my professor provided, he used the substitutions $u=xy$ and $v=\frac{y}{x}$. The transformed space he created is then a simple rectangle, which is simpler to integrate. It looked like this: $$\int_{1}^{4} \int_{1}^{2} \frac{1}{2v} \,dv \,du$$ In the end, both approaches worked (as they should), but I was wondering if there were some general guidelines or suggestions that could be given when choosing what substitution should be used? Are there any tricks or strategies for transforming regions like these into rectangles? Any help would be appreciated! P.S. Also, this is my first post to the community, so thanks for having me.",,"['multivariable-calculus', 'change-of-variable']"
64,A mysterious geometric argument regarding mollification in Evans's PDE book,A mysterious geometric argument regarding mollification in Evans's PDE book,,"The following argument is made in Evans's Partial Differential Equations , Chapter 5.3.3: Here are my questions : As I can read from the argument, $\lambda\varepsilon$ would be eventually small enough if $\varepsilon>0$ is small enough. Why on earth would one need $\lambda$ to be sufficiently large? What if we just define    $$ x^\varepsilon=x+\varepsilon e_n? $$ What does ""there is room to mollify within $U$"" mean? How does one know   $$ v^\varepsilon\in C(\overline{V})? $$","The following argument is made in Evans's Partial Differential Equations , Chapter 5.3.3: Here are my questions : As I can read from the argument, $\lambda\varepsilon$ would be eventually small enough if $\varepsilon>0$ is small enough. Why on earth would one need $\lambda$ to be sufficiently large? What if we just define    $$ x^\varepsilon=x+\varepsilon e_n? $$ What does ""there is room to mollify within $U$"" mean? How does one know   $$ v^\varepsilon\in C(\overline{V})? $$",,"['multivariable-calculus', 'differential-geometry']"
65,Equation for volume of $n$-dimensional ball in $L_1$ norm with change of variables,Equation for volume of -dimensional ball in  norm with change of variables,n L_1,"For a homework problem, I need to find a recursive equation that relates the volume of an $n$-dimensional ball $V_n(r)$ of radius $r$ to that of an $(n-2)$-dimensional ball, expressed by $V_{n-2}(r)$. The norm for these balls is the $L^1$ norm given by $$||(x_1, \ldots, x_n)||_{\mathbb R^n,1} = |x_1| + \cdots + |x_n|$$ I would like to use multivariate change of variables to solve this problem since the previous one (basically the same except using the standard Euclidean norm) can be worked out the same way. In particular, the change of variables used for that problem is the following: $$g(r, \theta, t_3, \ldots, t_n) = (r \cos\theta, r \sin\theta, t_3,\ldots,t_n)$$ for which the derivative matrix $\mathrm Dg$ is a block matrix consisting of the standard $2 \times 2$ rotation transformation matrix at the top left, the $(n-2) \times (n-2)$ identity matrix at the bottom right, and zero everywhere else. In particular, $\det \mathrm Dg = r$. From there, one can apply this change of variables on the integral defining the volume of the unit ball (integral over 1 on the ball itself) and get the well-known integral shown here . Now, it's possible that this same change of variables works in $L^p$ norms, but I'm not really sure. Any help would be greatly appreciated.","For a homework problem, I need to find a recursive equation that relates the volume of an $n$-dimensional ball $V_n(r)$ of radius $r$ to that of an $(n-2)$-dimensional ball, expressed by $V_{n-2}(r)$. The norm for these balls is the $L^1$ norm given by $$||(x_1, \ldots, x_n)||_{\mathbb R^n,1} = |x_1| + \cdots + |x_n|$$ I would like to use multivariate change of variables to solve this problem since the previous one (basically the same except using the standard Euclidean norm) can be worked out the same way. In particular, the change of variables used for that problem is the following: $$g(r, \theta, t_3, \ldots, t_n) = (r \cos\theta, r \sin\theta, t_3,\ldots,t_n)$$ for which the derivative matrix $\mathrm Dg$ is a block matrix consisting of the standard $2 \times 2$ rotation transformation matrix at the top left, the $(n-2) \times (n-2)$ identity matrix at the bottom right, and zero everywhere else. In particular, $\det \mathrm Dg = r$. From there, one can apply this change of variables on the integral defining the volume of the unit ball (integral over 1 on the ball itself) and get the well-known integral shown here . Now, it's possible that this same change of variables works in $L^p$ norms, but I'm not really sure. Any help would be greatly appreciated.",,"['multivariable-calculus', 'lp-spaces', 'volume', 'change-of-variable']"
66,Condition for equality of mixed derivatives,Condition for equality of mixed derivatives,,"It says that theorems 12.11 and 12.12 imply Theorem 12.13. But, don't we need some extra conditions? Like existence of $D_{r,r}f$ and $D_{k,k}f$? Here $f$ is a function from $\mathbb{R}^n$ to $\mathbb{R}^m$ and $D_kf$ denotes the partial derivative of $f$ w.r.t. the $k^{th}$ variable. The images are from Tom Apostol's Mathematical Analysis.","It says that theorems 12.11 and 12.12 imply Theorem 12.13. But, don't we need some extra conditions? Like existence of $D_{r,r}f$ and $D_{k,k}f$? Here $f$ is a function from $\mathbb{R}^n$ to $\mathbb{R}^m$ and $D_kf$ denotes the partial derivative of $f$ w.r.t. the $k^{th}$ variable. The images are from Tom Apostol's Mathematical Analysis.",,['multivariable-calculus']
67,"Show $f:\mathbb{R}^n \to \mathbb{R}^m$, $n>m$ can't be 1-1","Show ,  can't be 1-1",f:\mathbb{R}^n \to \mathbb{R}^m n>m,"Problem 2-37 on p. 39 of Spivak's Calculus on Manifolds asks Let $f:\mathbb{R}^2 \to \mathbb{R}$ be a continuously differentiable function. Show that $f$ is not 1-1. (Hint: If, for   example, $D_1f(x,y) \neq 0$ for all $(x,y)$ in some open set $A$,   consider $g: A \to \mathbb{R}^2$ defined by $g(x,y)=(f(x,y),y)$.) Generalize this result to the case of a continuously differentiable    function $f:\mathbb{R}^n \to \mathbb{R}^m$ with $m<n$. This previous question has the same question I do: how does he want us to do the second part? One response there suggests just proving the constant-rank theorem. I can't imagine that is what he has in mind, since he states and proves that theorem in the next section. This is a suggested solution to the second part, but I think it is erroneous. (It assumes that if vector-valued $f$ is 1-1, then at some $x$ there is some component $f^i$ that is both 1-1 near  $x$ and has nonzero gradient at $x$. I don't see why this would be true a priori . I'm trying to think of a counterexample when $n \leq m$.)","Problem 2-37 on p. 39 of Spivak's Calculus on Manifolds asks Let $f:\mathbb{R}^2 \to \mathbb{R}$ be a continuously differentiable function. Show that $f$ is not 1-1. (Hint: If, for   example, $D_1f(x,y) \neq 0$ for all $(x,y)$ in some open set $A$,   consider $g: A \to \mathbb{R}^2$ defined by $g(x,y)=(f(x,y),y)$.) Generalize this result to the case of a continuously differentiable    function $f:\mathbb{R}^n \to \mathbb{R}^m$ with $m<n$. This previous question has the same question I do: how does he want us to do the second part? One response there suggests just proving the constant-rank theorem. I can't imagine that is what he has in mind, since he states and proves that theorem in the next section. This is a suggested solution to the second part, but I think it is erroneous. (It assumes that if vector-valued $f$ is 1-1, then at some $x$ there is some component $f^i$ that is both 1-1 near  $x$ and has nonzero gradient at $x$. I don't see why this would be true a priori . I'm trying to think of a counterexample when $n \leq m$.)",,"['multivariable-calculus', 'differential-geometry']"
68,Reference for the fact that a smooth function analytic on every line is itself analytic,Reference for the fact that a smooth function analytic on every line is itself analytic,,"Let $f \in \mathcal C^\infty(\mathbb R^p)$ ($p \geq 2$) be a smooth function such that the functions $g_d(t) := f(td)$ are all analytic for all $t \in \mathbb R$ and all $d \in \mathbb R^p.$ (i.e. $f$ is smooth and analytic on every line). The claim is that $f$ is itself real analytic on all of $\mathbb R^p$. I do have my own proof of this fact (rough sketch: use Baire category theorem to show that there is a open set of directions $U$ in $S^{p-1}$ on which there are $A,B \geq 0$ such that $$|g_d^{(n)}(0)| \leq AB^nn!$$ for all $n$ and $d \in U$. Use linear algebra and a smart choice of directions to deduce that there exist $C,D \geq 0$ such that $$\left|\frac{\partial^n}{\partial x_1^{n_1}\cdots\partial x_p^{n_p}}f(0)\right| \leq CD^nn!$$ for all $(n_1,\ldots,n_p) \in \mathbb N^p$, and $n = n_1+\cdots+n_p$. Then there is a locally convergent power series with the same derivatives as $f$ at $0$. Apply analyticity on each line to conclude that $f$ is this power series. Repeat argument with arbitrary $x$ instead of $0$.), but my supervisor is convinced that this is actually a classical result. However, despite my attempts, I cannot find a reference for it or anything similar. Does anyone know of one?","Let $f \in \mathcal C^\infty(\mathbb R^p)$ ($p \geq 2$) be a smooth function such that the functions $g_d(t) := f(td)$ are all analytic for all $t \in \mathbb R$ and all $d \in \mathbb R^p.$ (i.e. $f$ is smooth and analytic on every line). The claim is that $f$ is itself real analytic on all of $\mathbb R^p$. I do have my own proof of this fact (rough sketch: use Baire category theorem to show that there is a open set of directions $U$ in $S^{p-1}$ on which there are $A,B \geq 0$ such that $$|g_d^{(n)}(0)| \leq AB^nn!$$ for all $n$ and $d \in U$. Use linear algebra and a smart choice of directions to deduce that there exist $C,D \geq 0$ such that $$\left|\frac{\partial^n}{\partial x_1^{n_1}\cdots\partial x_p^{n_p}}f(0)\right| \leq CD^nn!$$ for all $(n_1,\ldots,n_p) \in \mathbb N^p$, and $n = n_1+\cdots+n_p$. Then there is a locally convergent power series with the same derivatives as $f$ at $0$. Apply analyticity on each line to conclude that $f$ is this power series. Repeat argument with arbitrary $x$ instead of $0$.), but my supervisor is convinced that this is actually a classical result. However, despite my attempts, I cannot find a reference for it or anything similar. Does anyone know of one?",,"['real-analysis', 'multivariable-calculus', 'reference-request', 'analyticity']"
69,Does a polynomial that's bounded below have a global minimum?,Does a polynomial that's bounded below have a global minimum?,,"Must a polynomial function $f \in \mathbb{R}[x_1, \ldots, x_n]$ that's lower bounded by some $\lambda \in \mathbb{R}$ have a global minimum over $\mathbb{R}^n$?","Must a polynomial function $f \in \mathbb{R}[x_1, \ldots, x_n]$ that's lower bounded by some $\lambda \in \mathbb{R}$ have a global minimum over $\mathbb{R}^n$?",,"['real-analysis', 'multivariable-calculus', 'polynomials', 'maxima-minima']"
70,"Calculus on Manifolds (Spivak), theorem 2-13","Calculus on Manifolds (Spivak), theorem 2-13",,"I'm having trouble understanding this theorem: 2-13 Theorem. Let $f: \mathbb{R}^n \to \mathbb{R}^p$ be continuously differentiable in an open set containing $a$, where $p \leq n$. If $f(a) = 0$ and the $p \times n$ matrix $(D_jf^i(a))$ has rank $p$, then there is an open set $A \subset \mathbb{R}^n$ containing $a$ and a differentiable function $h: A \to \mathbb{R}^n$ with differentiable inverse such that    $$f \circ h(x^1, \ldots, x^n) = (x^{n-p+1}, \ldots, x^n).$$ It seems like the function $f(x) = x^2 - \frac{1}{4}$, around $a = -\frac{1}{2}$, serves as a counterexample. Obviously this function is continuously differentiable everywhere, and $f(a) = 0$, and the derivative $f'(x) = 2x$ is nonzero at $a$. Since the domain of $h$ is $A$ which contains $a$, there must be $h(a)$ such that $f(h(a)) = a$ by the statement of the theorem, i.e., $f(h(-\frac{1}{2})) = -\frac{1}{2}$, but this is impossible as $f$ attains its minimum at $(0, -\frac{1}{4})$. I looked at the proof and it seems that there is an error here: Proof. We can consider $f$ as a function $f: \mathbb{R}^{n-p} \times \mathbb{R}^p \to \mathbb{R}^p$. If $\det M \neq 0$, then $M$ is the $p \times p$ matrix $(D_{n_p+j}f^i(a))$, $1 \leq i, j \leq p$, then we are precisely in the situation considered in the proof of Theorem 2-12, and as we showed in that proof, there is $h$ such that $f \circ h(x^1, \ldots, x^n) = (x^{n-p+1}, \ldots, x^n)$. The error is that ""the situation considered in the proof of Theorem 2-12"" establishes the existence of $h(x)$ where $x$ is in a neighbourhood of $(a^1, \ldots, a^{n-p}, 0, \ldots, 0)$ (by considering a function $F$ that replaces the last $p$ coordinates of $x$ by $f(x)$, then applying the inverse function theorem) rather than in a neighbourhood of $a$ itself. What is the correct statement of theorem 2-13? I would just skip it, but it appears to become important later on, in the manifolds chapter.","I'm having trouble understanding this theorem: 2-13 Theorem. Let $f: \mathbb{R}^n \to \mathbb{R}^p$ be continuously differentiable in an open set containing $a$, where $p \leq n$. If $f(a) = 0$ and the $p \times n$ matrix $(D_jf^i(a))$ has rank $p$, then there is an open set $A \subset \mathbb{R}^n$ containing $a$ and a differentiable function $h: A \to \mathbb{R}^n$ with differentiable inverse such that    $$f \circ h(x^1, \ldots, x^n) = (x^{n-p+1}, \ldots, x^n).$$ It seems like the function $f(x) = x^2 - \frac{1}{4}$, around $a = -\frac{1}{2}$, serves as a counterexample. Obviously this function is continuously differentiable everywhere, and $f(a) = 0$, and the derivative $f'(x) = 2x$ is nonzero at $a$. Since the domain of $h$ is $A$ which contains $a$, there must be $h(a)$ such that $f(h(a)) = a$ by the statement of the theorem, i.e., $f(h(-\frac{1}{2})) = -\frac{1}{2}$, but this is impossible as $f$ attains its minimum at $(0, -\frac{1}{4})$. I looked at the proof and it seems that there is an error here: Proof. We can consider $f$ as a function $f: \mathbb{R}^{n-p} \times \mathbb{R}^p \to \mathbb{R}^p$. If $\det M \neq 0$, then $M$ is the $p \times p$ matrix $(D_{n_p+j}f^i(a))$, $1 \leq i, j \leq p$, then we are precisely in the situation considered in the proof of Theorem 2-12, and as we showed in that proof, there is $h$ such that $f \circ h(x^1, \ldots, x^n) = (x^{n-p+1}, \ldots, x^n)$. The error is that ""the situation considered in the proof of Theorem 2-12"" establishes the existence of $h(x)$ where $x$ is in a neighbourhood of $(a^1, \ldots, a^{n-p}, 0, \ldots, 0)$ (by considering a function $F$ that replaces the last $p$ coordinates of $x$ by $f(x)$, then applying the inverse function theorem) rather than in a neighbourhood of $a$ itself. What is the correct statement of theorem 2-13? I would just skip it, but it appears to become important later on, in the manifolds chapter.",,['multivariable-calculus']
71,"What is the solution to Nash's problem presented in ""A Beautiful Mind""?","What is the solution to Nash's problem presented in ""A Beautiful Mind""?",,"I was watching the said movie the other night, and I started thinking about the equation posed by Nash in the movie.  More specifically, the one he said would take some students a lifetime to solve (obviously, an exaggeration).  Nonetheless, one can't say it's a simple problem. Anyway, here it is $$V = \{F:\mathbb{R^3}\setminus X\rightarrow \mathbb{R^3} \text{ so } \hspace{1mm}\nabla \times F=0\}$$ $$W = \{F = \nabla g\}$$ $$\dim(V/W) = \; 8$$ I haven't actually attempted a solution myself to be honest, but I thought it would be an interesting question to pose. I have done a quick search on this site and Google, but there were surprisingly few results. In any case, I was curious if anyone knew the answer aside from the trivial.","I was watching the said movie the other night, and I started thinking about the equation posed by Nash in the movie.  More specifically, the one he said would take some students a lifetime to solve (obviously, an exaggeration).  Nonetheless, one can't say it's a simple problem. Anyway, here it is I haven't actually attempted a solution myself to be honest, but I thought it would be an interesting question to pose. I have done a quick search on this site and Google, but there were surprisingly few results. In any case, I was curious if anyone knew the answer aside from the trivial.",V = \{F:\mathbb{R^3}\setminus X\rightarrow \mathbb{R^3} \text{ so } \hspace{1mm}\nabla \times F=0\} W = \{F = \nabla g\} \dim(V/W) = \; 8,"['differential-geometry', 'multivariable-calculus', 'partial-differential-equations', 'homology-cohomology', 'popular-math']"
72,Does there exist a $1$-form $\alpha$ with $d\alpha = \omega$?,Does there exist a -form  with ?,1 \alpha d\alpha = \omega,"Let $\omega := dx \wedge dy$ denote the standard area form on $\mathbb{R}^2$. As the question title suggests, does there exist a $1$-form $\alpha$ with $d\alpha = \omega$?","Let $\omega := dx \wedge dy$ denote the standard area form on $\mathbb{R}^2$. As the question title suggests, does there exist a $1$-form $\alpha$ with $d\alpha = \omega$?",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'differential-topology', 'differential-forms']"
73,Why curl free field implies existence of potential function?,Why curl free field implies existence of potential function?,,"If curl of a vector field F is zero, then there exist some potential such that $$F = \nabla \phi.$$ I am not sure how to prove this result. I tried using Helmholtz decomposition: $$F = \nabla \phi + \nabla \times u,$$ so I need to show that $\nabla \times u=0$ somehow.","If curl of a vector field F is zero, then there exist some potential such that $$F = \nabla \phi.$$ I am not sure how to prove this result. I tried using Helmholtz decomposition: $$F = \nabla \phi + \nabla \times u,$$ so I need to show that $\nabla \times u=0$ somehow.",,['multivariable-calculus']
74,Why is this set not a manifold?,Why is this set not a manifold?,,"Set $M = \{ \, (x, y) : x^2 = y^2 \, \}$.  If for every point $(a, c)$ in $M$, there exists a neighborhood $U$ containing $(a, c)$ and function $\phi(x, y)$ such that: $\phi(x, y) = 0$ on $M \cap U$; The Jacobian matrix associated with $\phi$ has rank $1$ on $U$.  (In general, it does not have to be rank $1$.  But here the only choice is $1$.) Then, $M$ is a manifold.  If the Jacobian matrix has ranks greater than $0$, then we have use $\phi$ to carry out the implicit function theorem, and construct a function such that $(x, y) = (x, f(x))$ on $M \cap U$.  But I don't know how to go in reverse; what is the contradiction if $M$ is a manifold? An educated guess says that $(0, 0)$ is our trouble spot.  The function $\varphi(x, y) = x^2 - y^2$ equals $0$ on $M$.  But the Jacobian matrix has zero rank at $(0, 0)$.  So, we cannot use $\varphi$ to carry out the implicit function theorem...","Set $M = \{ \, (x, y) : x^2 = y^2 \, \}$.  If for every point $(a, c)$ in $M$, there exists a neighborhood $U$ containing $(a, c)$ and function $\phi(x, y)$ such that: $\phi(x, y) = 0$ on $M \cap U$; The Jacobian matrix associated with $\phi$ has rank $1$ on $U$.  (In general, it does not have to be rank $1$.  But here the only choice is $1$.) Then, $M$ is a manifold.  If the Jacobian matrix has ranks greater than $0$, then we have use $\phi$ to carry out the implicit function theorem, and construct a function such that $(x, y) = (x, f(x))$ on $M \cap U$.  But I don't know how to go in reverse; what is the contradiction if $M$ is a manifold? An educated guess says that $(0, 0)$ is our trouble spot.  The function $\varphi(x, y) = x^2 - y^2$ equals $0$ on $M$.  But the Jacobian matrix has zero rank at $(0, 0)$.  So, we cannot use $\varphi$ to carry out the implicit function theorem...",,['multivariable-calculus']
75,Equality of mixed partial derivatives,Equality of mixed partial derivatives,,"Is the following statement $$\frac{\partial^2 f}{\partial x \, \partial y}=\frac{\partial^2 f}{\partial y \, \partial x}$$ always true? If not what are the conditions for this to be true?","Is the following statement $$\frac{\partial^2 f}{\partial x \, \partial y}=\frac{\partial^2 f}{\partial y \, \partial x}$$ always true? If not what are the conditions for this to be true?",,"['multivariable-calculus', 'partial-derivative']"
76,How can we show the cone $x^2 +y^2 = z^2$ is not a smooth manifold?,How can we show the cone  is not a smooth manifold?,x^2 +y^2 = z^2,"In our differential geometry class, as a preliminary we have used as the definition of a manifold the following: $M \subset \mathbb{R}^n$ is a $k$-manifold if for each point $p\in M$ there exists a neighborhood $U$ of $p$ and $I = \{i_1, \dotsc, i_k\} \subset \{1,\dotsc,n\}$ such that $U \cap M$ is the graph of a $C^\infty$ function $f: V \to \mathbb{R}^{I^c}$, where $V \subset \mathbb{R}^I$ . One problem we've been assigned is to show that the cone $$C := \{(x,y,z) \in \mathbb{R}^3 \mid x^2 + y^2 = z^2 \}$$ is not a smooth manifold. I have not yet learned any general ways to show something is not a smooth manifold. I feel like there may be a more clean way based on the more abstract definition of a smooth manifold, but I need something that works with this more limited definition. $C$ is a level set of the function $F(x,y,z) = x^2 + y^2 - z^2$. We note that $\nabla F(\mathbf{0}) = \mathbf{0}$, so the implicit function theorem doesn't allow us to solve out for one variable in terms of the others at $\mathbf{0}$. But I've never learned a converse to the implicit function theorem...how can we show that it is not possible for such a parameterization of $C$ to exist? Hints are appreciated; no need to feed me the answer. Thanks.","In our differential geometry class, as a preliminary we have used as the definition of a manifold the following: $M \subset \mathbb{R}^n$ is a $k$-manifold if for each point $p\in M$ there exists a neighborhood $U$ of $p$ and $I = \{i_1, \dotsc, i_k\} \subset \{1,\dotsc,n\}$ such that $U \cap M$ is the graph of a $C^\infty$ function $f: V \to \mathbb{R}^{I^c}$, where $V \subset \mathbb{R}^I$ . One problem we've been assigned is to show that the cone $$C := \{(x,y,z) \in \mathbb{R}^3 \mid x^2 + y^2 = z^2 \}$$ is not a smooth manifold. I have not yet learned any general ways to show something is not a smooth manifold. I feel like there may be a more clean way based on the more abstract definition of a smooth manifold, but I need something that works with this more limited definition. $C$ is a level set of the function $F(x,y,z) = x^2 + y^2 - z^2$. We note that $\nabla F(\mathbf{0}) = \mathbf{0}$, so the implicit function theorem doesn't allow us to solve out for one variable in terms of the others at $\mathbf{0}$. But I've never learned a converse to the implicit function theorem...how can we show that it is not possible for such a parameterization of $C$ to exist? Hints are appreciated; no need to feed me the answer. Thanks.",,"['multivariable-calculus', 'differential-geometry', 'partial-derivative']"
77,Tangent plane to sphere,Tangent plane to sphere,,"Let $S$ be the sphere $$ x^2 + y^2 + z^2 = 14$$ I need help finding: A. Tangent plane to $S$ at the point $P(1, 2, 3)$. B. Distance from $Q(3, 2, 1)$ to the above tangent plane.","Let $S$ be the sphere $$ x^2 + y^2 + z^2 = 14$$ I need help finding: A. Tangent plane to $S$ at the point $P(1, 2, 3)$. B. Distance from $Q(3, 2, 1)$ to the above tangent plane.",,['multivariable-calculus']
78,Solving the equation: $p^a (1-p)^b = q^a (1-q)^b$,Solving the equation:,p^a (1-p)^b = q^a (1-q)^b,"I am currently studying the following equation: $p^a(1-p)^b=q^a(1-q)^b$ where $p,q \in (0,1)$, and $a,b \in \mathbb{N}$. I would like to show that the equation is satisfied if and only if $p=q$. Is it possible to do this in an exact way? I came across this equation when studying dynamical systems, and I don't have much of a background with these sorts of equations. (Actually, more precisely, I would like to show that $\sum_k (p^{a_k}(1-p)^{b_k}  - q^{a_k}(1-q)^{b_k}) = 0 \Leftrightarrow p = q$ for $p,q \in (0,1)$ and $a_k,b_k \in \mathbb{N}$.) Thanks","I am currently studying the following equation: $p^a(1-p)^b=q^a(1-q)^b$ where $p,q \in (0,1)$, and $a,b \in \mathbb{N}$. I would like to show that the equation is satisfied if and only if $p=q$. Is it possible to do this in an exact way? I came across this equation when studying dynamical systems, and I don't have much of a background with these sorts of equations. (Actually, more precisely, I would like to show that $\sum_k (p^{a_k}(1-p)^{b_k}  - q^{a_k}(1-q)^{b_k}) = 0 \Leftrightarrow p = q$ for $p,q \in (0,1)$ and $a_k,b_k \in \mathbb{N}$.) Thanks",,['multivariable-calculus']
79,Subharmonic function equivalent non-negative laplacian,Subharmonic function equivalent non-negative laplacian,,"I want to ask for a proof that if $v(x,y)$ is $C^2$ and is subharmonic [here, define as satisfyingthen $\Delta v \geq 0$ where $\Delta v = \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2}$ is the Laplacian operator. I think it has something to do with geometric interpretation of $\Delta v$ but I fail to see what kind of interpretation it can be.","I want to ask for a proof that if $v(x,y)$ is $C^2$ and is subharmonic [here, define as satisfyingthen $\Delta v \geq 0$ where $\Delta v = \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2}$ is the Laplacian operator. I think it has something to do with geometric interpretation of $\Delta v$ but I fail to see what kind of interpretation it can be.",,"['multivariable-calculus', 'harmonic-functions', 'laplacian']"
80,Non-conservative field with zero curl,Non-conservative field with zero curl,,"I'm looking for examples of this, the only one I've found so far is:  $f(x,y) = \left( \frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2} \right)$ Is there another example of this? Also, what should I follow if I wanted to ""build"" a field that is non-conservative and has zero curl. Is both partial derivatives being the same and the line integral of the force along some closed curve being different than zero enough? Thank you.","I'm looking for examples of this, the only one I've found so far is:  $f(x,y) = \left( \frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2} \right)$ Is there another example of this? Also, what should I follow if I wanted to ""build"" a field that is non-conservative and has zero curl. Is both partial derivatives being the same and the line integral of the force along some closed curve being different than zero enough? Thank you.",,"['multivariable-calculus', 'vector-fields']"
81,Positive harmonic function on $\mathbb{R}^n$ is a constant?,Positive harmonic function on  is a constant?,\mathbb{R}^n,Is it true that a positive harmonic function on $\mathbb{R}^n$ must be a constant? How might we show this? The mean value property seems not to be the way...for that we would need boundedness.,Is it true that a positive harmonic function on $\mathbb{R}^n$ must be a constant? How might we show this? The mean value property seems not to be the way...for that we would need boundedness.,,"['multivariable-calculus', 'harmonic-functions']"
82,"Show that $\forall (x,y)$ in the first quadrant: $\frac {x^2+y^2}{4}\leq e^{x+y-2}$",Show that  in the first quadrant:,"\forall (x,y) \frac {x^2+y^2}{4}\leq e^{x+y-2}","I have the folowing exercise (which I've been thinking quite a while and couldn't figure out): Show that $\forall (x,y)$ in the first quadrant: $$\frac {x^2+y^2}{4}\leq e^{x+y-2}$$ My idea was to work with maxima and minima, but I'm stuck... Any help will be much appreciated!","I have the folowing exercise (which I've been thinking quite a while and couldn't figure out): Show that $\forall (x,y)$ in the first quadrant: $$\frac {x^2+y^2}{4}\leq e^{x+y-2}$$ My idea was to work with maxima and minima, but I'm stuck... Any help will be much appreciated!",,"['inequality', 'multivariable-calculus']"
83,Evaluating a double integral involving a biquadratic with variable coefficients,Evaluating a double integral involving a biquadratic with variable coefficients,,"Please keep in mind that I only have an intermediate level proficiency in Calculus-II . I am relatively new to the concept of double integration, though I have solved some problems in this topic. I have tried a lot, but have failed in simplifying this integral $$\int_{u=0}^1 \int_{t=0}^{\infty} \frac{2 \sqrt{115} \cdot (27t^2+11)}{\left[(729+1035u^2)t^4+(594+2070u^2)t^2 + (121+1035u^2)\right]} dt du$$ I know that in general, to deal with integrals of the type $$\int \frac{ax^2+b}{x^4+px^2+q} dx$$ We divide both numerator and denominator by $x^2$ and try to find suitable constants $c,d$ such that: $$ a + \frac{b}{x^2} = c \left(1 + \frac{\sqrt{q}}{x^2}\right) + d \left(1 - \frac{\sqrt{q}}{x^2}\right)$$ But after I take $(729+1035u^2)$ common to make the integrand in such a form, I am not able to re-integrate the expression I obtain with respect to $u$ . The integrals look terrible and certainly unsolvable by me. WolframAlpha suggests that the answer of the integral may be $\frac{\pi^2}{9}$ I would like to know of any better methods to solve this double integral. Thanks in advance! Have a nice day!","Please keep in mind that I only have an intermediate level proficiency in Calculus-II . I am relatively new to the concept of double integration, though I have solved some problems in this topic. I have tried a lot, but have failed in simplifying this integral I know that in general, to deal with integrals of the type We divide both numerator and denominator by and try to find suitable constants such that: But after I take common to make the integrand in such a form, I am not able to re-integrate the expression I obtain with respect to . The integrals look terrible and certainly unsolvable by me. WolframAlpha suggests that the answer of the integral may be I would like to know of any better methods to solve this double integral. Thanks in advance! Have a nice day!","\int_{u=0}^1 \int_{t=0}^{\infty} \frac{2 \sqrt{115} \cdot (27t^2+11)}{\left[(729+1035u^2)t^4+(594+2070u^2)t^2 + (121+1035u^2)\right]} dt du \int \frac{ax^2+b}{x^4+px^2+q} dx x^2 c,d  a + \frac{b}{x^2} = c \left(1 + \frac{\sqrt{q}}{x^2}\right) + d \left(1 - \frac{\sqrt{q}}{x^2}\right) (729+1035u^2) u \frac{\pi^2}{9}",['multivariable-calculus']
84,Taking a derivative of a magnitude of a vector,Taking a derivative of a magnitude of a vector,,"I have found several questions including this as a step in the explanation, but have not been able to find an explicit explanation of how to take a derivative of a magnitude of a vector. I am trying to take the derivative d/dt ||r'(t)|| but don't know how to address the magnitude signs.","I have found several questions including this as a step in the explanation, but have not been able to find an explicit explanation of how to take a derivative of a magnitude of a vector. I am trying to take the derivative d/dt ||r'(t)|| but don't know how to address the magnitude signs.",,"['multivariable-calculus', 'vectors']"
85,Proof of multivariable chain rule,Proof of multivariable chain rule,,"I'm working with a proof of the multivariable chain rule $\displaystyle{\frac{d}{dt}g(t)=\frac{df}{dx_1}\frac{dx_1}{dt}+\frac{df}{dx_2}\frac{dx_2}{dt}}$ for $g(t)=f(x_1(t),x_2(t))$ , but I have a hard time understanding two important steps of this proof. The proof includes the function $\displaystyle{\Delta_i(h)=x_i(t+h)-x_i(t)}$ for $\displaystyle{i=1,2, \bar{\Delta}=(\Delta_1(h),\Delta_2(h)) \Rightarrow \lim_{h\rightarrow0}\frac{\Delta_i}{h}=x^{'}_i}$ . It says that $\frac{g(t+h)-g(t)}{h}=\frac{f(\bar{x}(t+h))-f(\bar{x}(t))}{h}=\frac{f(\bar{x}(t)-\bar{\Delta(h))}-f(\bar{x}(t))}{h}$ which I understand, but the next step is the to state that $f$ is differentiable and then let the previous equation be equal to $=f^{'}_1(\bar{x}(t))\cdot\Delta_1(h)+f^{'}_2(\bar{x}(t))\cdot\Delta_2(h)+o(\vert\vert\bar{\Delta}\vert\vert)$ and this step I do not understand. I think there might be missing some limit-notation? But even with the limit notation I'm still not sure as to how it becomes a partial derivative multiplied with $\Delta_i$ . Afterwards they let $h\rightarrow0$ to get $=f^{'}_1(\bar{x}(t))\cdot x_1^{'}(t)+f^{'}_2(\bar{x}(t))\cdot x_2^{'}(t)$ Again I am very confused as to possibly missing limit notations. Does anyone know this version of the proof of the chain rule (besides these two steps, I find it the easiest version to understand), or understand these steps? Here are pictures of the notes: Theorem: Multivariable chain rule Proof of theorem","I'm working with a proof of the multivariable chain rule for , but I have a hard time understanding two important steps of this proof. The proof includes the function for . It says that which I understand, but the next step is the to state that is differentiable and then let the previous equation be equal to and this step I do not understand. I think there might be missing some limit-notation? But even with the limit notation I'm still not sure as to how it becomes a partial derivative multiplied with . Afterwards they let to get Again I am very confused as to possibly missing limit notations. Does anyone know this version of the proof of the chain rule (besides these two steps, I find it the easiest version to understand), or understand these steps? Here are pictures of the notes: Theorem: Multivariable chain rule Proof of theorem","\displaystyle{\frac{d}{dt}g(t)=\frac{df}{dx_1}\frac{dx_1}{dt}+\frac{df}{dx_2}\frac{dx_2}{dt}} g(t)=f(x_1(t),x_2(t)) \displaystyle{\Delta_i(h)=x_i(t+h)-x_i(t)} \displaystyle{i=1,2, \bar{\Delta}=(\Delta_1(h),\Delta_2(h)) \Rightarrow \lim_{h\rightarrow0}\frac{\Delta_i}{h}=x^{'}_i} \frac{g(t+h)-g(t)}{h}=\frac{f(\bar{x}(t+h))-f(\bar{x}(t))}{h}=\frac{f(\bar{x}(t)-\bar{\Delta(h))}-f(\bar{x}(t))}{h} f =f^{'}_1(\bar{x}(t))\cdot\Delta_1(h)+f^{'}_2(\bar{x}(t))\cdot\Delta_2(h)+o(\vert\vert\bar{\Delta}\vert\vert) \Delta_i h\rightarrow0 =f^{'}_1(\bar{x}(t))\cdot x_1^{'}(t)+f^{'}_2(\bar{x}(t))\cdot x_2^{'}(t)","['multivariable-calculus', 'chain-rule']"
86,How to show differentiability implies continuity for functions between Euclidean spaces,How to show differentiability implies continuity for functions between Euclidean spaces,,"A function $f: \mathbb{R^n} \to \mathbb{R^m}$ is differentiable at $a$ if there exists a linear map $ \lambda: \mathbb{R^n} \to \mathbb{R^m}$ such that $$\lim_{h \to 0} \frac{\|f(a+h) - f(a) - \lambda(h)\|}{\|h\|} = 0$$ So clearly, if $f$ is differentiable at $a$ then $\lim_{h \to 0} f(a+h) - f(a) - \lambda(h) = 0$, but where do you proceed from here?","A function $f: \mathbb{R^n} \to \mathbb{R^m}$ is differentiable at $a$ if there exists a linear map $ \lambda: \mathbb{R^n} \to \mathbb{R^m}$ such that $$\lim_{h \to 0} \frac{\|f(a+h) - f(a) - \lambda(h)\|}{\|h\|} = 0$$ So clearly, if $f$ is differentiable at $a$ then $\lim_{h \to 0} f(a+h) - f(a) - \lambda(h) = 0$, but where do you proceed from here?",,['multivariable-calculus']
87,Partial derivatives must exist and be continuous on all defined points of $f$ for $f$ to be differentiable?,Partial derivatives must exist and be continuous on all defined points of  for  to be differentiable?,f f,"Today my professor explained that $f(x,y)=\frac{2xy}{(x^2+y^2)^2}$ is differentiable even though for $(x,y)=(0,0)$ $f$ is not defined. The partial derivatives are $\frac{\partial f}{\partial x}=\frac{2y^3-6x^2y}{(x^2+y^2)^3} \text{ and } \frac{\partial f}{\partial y}=\frac{2x^3-6y^2x}{(x^2+y^2)^3}$ . So, they exist. Doesn't differentiability require the partial derivaties to be continuous? My professor said that we ignore $(0,0)$ on the partial derivatives because it isn't defined for $f$ . I was hoping someone here could fully explain this. Also could someone explain the difference between being in $C^1$ and just being differentiable?","Today my professor explained that is differentiable even though for is not defined. The partial derivatives are . So, they exist. Doesn't differentiability require the partial derivaties to be continuous? My professor said that we ignore on the partial derivatives because it isn't defined for . I was hoping someone here could fully explain this. Also could someone explain the difference between being in and just being differentiable?","f(x,y)=\frac{2xy}{(x^2+y^2)^2} (x,y)=(0,0) f \frac{\partial f}{\partial x}=\frac{2y^3-6x^2y}{(x^2+y^2)^3} \text{ and } \frac{\partial f}{\partial y}=\frac{2x^3-6y^2x}{(x^2+y^2)^3} (0,0) f C^1",['multivariable-calculus']
88,Intuitive Explanation of the Inverse Function Theorem,Intuitive Explanation of the Inverse Function Theorem,,What is the intuitive explanation of the Inverse Function Theorem (and generalized to multiple dimensions)?,What is the intuitive explanation of the Inverse Function Theorem (and generalized to multiple dimensions)?,,"['real-analysis', 'multivariable-calculus']"
89,"How to explain Clairaut-Schwartz's Theorem, $f_{xy}=f_{yx}$?","How to explain Clairaut-Schwartz's Theorem, ?",f_{xy}=f_{yx},"I am looking for a non-technical explanation of Clairaut's theorem which states that the mixed derivative of smooth functions are equal . A geometrical, graphical, or demo that explains the theorem and its implications will be helpful. I am not looking for a proof!","I am looking for a non-technical explanation of Clairaut's theorem which states that the mixed derivative of smooth functions are equal . A geometrical, graphical, or demo that explains the theorem and its implications will be helpful. I am not looking for a proof!",,"['real-analysis', 'multivariable-calculus', 'education']"
90,Chain rule for partial derivatives intuition,Chain rule for partial derivatives intuition,,Can somebody give me an intuitive explanation for the below equations. I'm not sure how they come about and how they can be perceived logically. $$\frac{\partial z}{\partial s} =\frac{\partial f}{\partial x}\frac{\partial x}{\partial s}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial s} \ \ \text{and} \ \ \frac{\partial z}{\partial t} =\frac{\partial f}{\partial x}\frac{\partial x}{\partial t}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial t} $$,Can somebody give me an intuitive explanation for the below equations. I'm not sure how they come about and how they can be perceived logically. $$\frac{\partial z}{\partial s} =\frac{\partial f}{\partial x}\frac{\partial x}{\partial s}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial s} \ \ \text{and} \ \ \frac{\partial z}{\partial t} =\frac{\partial f}{\partial x}\frac{\partial x}{\partial t}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial t} $$,,"['multivariable-calculus', 'derivatives']"
91,Use implicit function theorem to show $O(n)$ is a manifold,Use implicit function theorem to show  is a manifold,O(n),"In class today our teacher mentioned that one can use the implicit function theorem to show that $O(n) \subseteq \mathbb{R}^{n^2}$ is a submanifold...that is, map $A \mapsto A^* A$, and set it equal to the identity matrix. There should be $n(n+1)/2$ equations in $n^2$ variables ($A^*A$ is always symmetric, so we only need $n(n+1)/2$ equations). When I asked how to show that the jacobian matrix of this transformation had full row rank (the condition of the implicit function theorem), my teacher said ""take the derivative near the identity, and you see that the derivative is just a matrix plus its transpose ."" I wasn't sure what he meant. Can someone clarify?","In class today our teacher mentioned that one can use the implicit function theorem to show that $O(n) \subseteq \mathbb{R}^{n^2}$ is a submanifold...that is, map $A \mapsto A^* A$, and set it equal to the identity matrix. There should be $n(n+1)/2$ equations in $n^2$ variables ($A^*A$ is always symmetric, so we only need $n(n+1)/2$ equations). When I asked how to show that the jacobian matrix of this transformation had full row rank (the condition of the implicit function theorem), my teacher said ""take the derivative near the identity, and you see that the derivative is just a matrix plus its transpose ."" I wasn't sure what he meant. Can someone clarify?",,"['multivariable-calculus', 'differential-geometry', 'manifolds']"
92,Why is gradient a vector?,Why is gradient a vector?,,"I've just moved on from single variable to multivariable calculus and having trouble understanding the gradient as I'm trying to draw a comparison b/w single and multivariable derivatives. Why is a gradient a vector? So, let's take this with an example that we have a scalar function with 2 inputs i.e $z= F(x,y) = x^2 - y^2$ . Now, I do understand that gradient is a vector of the partial derivative of $z$ (or $F(x,y)$ ). However, why do we need to write it as a vector? If we write it as a vector we are also assigning it a direction. And to me, this direction seems not intuitive as partial derivatives ( $\partial_x z, \partial_y z$ ) tell us the change in $z$ w.r.t to $x$ or $y$ ( $2x$ & $-2y$ in the example above). So, if we plug in some values of $x$ and $y$ (let's say 2 and 3) then we end up with 2 scalar values which would tell us the change in $z$ . So, in our example, that would be 4(w.r.t x) and -6 (w.r.t $y$ ). Using this we can get a net change in $z$ . And to take this forward, these scalar values could be multiplied with any other unit vector to get the directional derivative (or a net change in $z$ ). Just not sure why gradient is a vector with direction? Also, the proof (dot product yields maximum value when 2 vectors point the same direction) for gradient being the steepest descent/ascent seems to be dependent on this fact (gradient as a vector). The source I'm using is Khan Academy's Multivariable calculus.","I've just moved on from single variable to multivariable calculus and having trouble understanding the gradient as I'm trying to draw a comparison b/w single and multivariable derivatives. Why is a gradient a vector? So, let's take this with an example that we have a scalar function with 2 inputs i.e . Now, I do understand that gradient is a vector of the partial derivative of (or ). However, why do we need to write it as a vector? If we write it as a vector we are also assigning it a direction. And to me, this direction seems not intuitive as partial derivatives ( ) tell us the change in w.r.t to or ( & in the example above). So, if we plug in some values of and (let's say 2 and 3) then we end up with 2 scalar values which would tell us the change in . So, in our example, that would be 4(w.r.t x) and -6 (w.r.t ). Using this we can get a net change in . And to take this forward, these scalar values could be multiplied with any other unit vector to get the directional derivative (or a net change in ). Just not sure why gradient is a vector with direction? Also, the proof (dot product yields maximum value when 2 vectors point the same direction) for gradient being the steepest descent/ascent seems to be dependent on this fact (gradient as a vector). The source I'm using is Khan Academy's Multivariable calculus.","z= F(x,y) = x^2 - y^2 z F(x,y) \partial_x z, \partial_y z z x y 2x -2y x y z y z z","['multivariable-calculus', 'partial-derivative', 'vector-analysis']"
93,Relations between two versions of the rank theorem in Rudin and Lee,Relations between two versions of the rank theorem in Rudin and Lee,,"This a follow-up question to a previous one of mine regarding Rudin's rank theorem: @ Jack Lee gives the following theorem in his Introduction to Smooth Manifolds : As I understand, Theorem 4.12 should be a generalization of 9.32. But I don't completely understand how one can be translated into another. Here is my question : How is (66) related to (4.1)? One can see that $(H,V)$ (or $(H^{-1},V)$ according to different convention) in Theorem 9.32 plays the role of the chart $(U,\varphi)$ in Theorem 4.12. What really puzzles me is that how one can translate the R.H.S of (66) to that of (4.12).","This a follow-up question to a previous one of mine regarding Rudin's rank theorem: @ Jack Lee gives the following theorem in his Introduction to Smooth Manifolds : As I understand, Theorem 4.12 should be a generalization of 9.32. But I don't completely understand how one can be translated into another. Here is my question : How is (66) related to (4.1)? One can see that $(H,V)$ (or $(H^{-1},V)$ according to different convention) in Theorem 9.32 plays the role of the chart $(U,\varphi)$ in Theorem 4.12. What really puzzles me is that how one can translate the R.H.S of (66) to that of (4.12).",,['real-analysis']
94,A Deviation from a Conventional Proof of the Basel Problem,A Deviation from a Conventional Proof of the Basel Problem,,"There's been many topics on the Riemann-Zeta function, specifically $\zeta(2)$.$$\zeta(2)=\sum_{n=1}^\infty\frac{1}{n^2}=\int_0^1\int_0^1\frac{1}{1-xy}dA$$This is the Basel Problem. Taking the multivariable calculus approach, one could make the change of variables $(x,y)=(\frac{u-v}{\sqrt2},\frac{u+v}{\sqrt2})$. Following this path, one would come across the following iterated integral: $$\int_0^\sqrt2\int_{|u-\frac{\sqrt2}2|-\frac{\sqrt2}2}^{\frac{\sqrt2}2-|u-\frac{\sqrt2}2|}\frac 2{v^2-u^2+2}dv\;du$$All proofs that I've found online that use multivariable calculus integrate with respect to $v$ then $u$, so I wanted to write a proof integrating with respect to $u$ then $v$. For the sake of maintaining this post's relative brevity, I won't write out the full proof here. Instead, my proof can be found here . Integrating with respect to $u$ then $v$ would require this iterated integral:$$\int_{-\frac{\sqrt2}2}^{\frac{\sqrt2}2}\int_{|v|}^{\sqrt2-|v|}\frac2{v^2-u^2+2}du\;dv$$My question is this: is there any way to evaluate the following integral analytically?$$\int_0^{\arctan\frac 1 2}\sec\theta\ln{\frac{\cos\theta-\sin\theta+1}{\sin\theta-\cos\theta+1}}d\theta=\frac{\pi^2}{12}+\frac{1}2\ln^2{\frac{\sqrt5-1}2}+\frac{1}2\ln^2{\frac{\sqrt5+1}2}$$Rearranging the terms results in$$2\int_0^{\arctan\frac 1 2}\sec\theta\ln{\frac{\cos\theta-\sin\theta+1}{\sin\theta-\cos\theta+1}}d\theta-\ln^2{\frac{\sqrt5-1}2}-\ln^2{\frac{\sqrt5+1}2}=\frac{\pi^2}6$$Using Fubini's Theorem, one can see that this is in fact another way of stating the Basel Problem. Is there any other way to prove this result, though? All of the steps in between are in my link above. I've spent a week reviewing every number and symbol in my work, and I can guarantee there are no errors. Wolfram Alpha produces an approximation to within $0.000004$ of my result, although it doesn't list the exact value.","There's been many topics on the Riemann-Zeta function, specifically $\zeta(2)$.$$\zeta(2)=\sum_{n=1}^\infty\frac{1}{n^2}=\int_0^1\int_0^1\frac{1}{1-xy}dA$$This is the Basel Problem. Taking the multivariable calculus approach, one could make the change of variables $(x,y)=(\frac{u-v}{\sqrt2},\frac{u+v}{\sqrt2})$. Following this path, one would come across the following iterated integral: $$\int_0^\sqrt2\int_{|u-\frac{\sqrt2}2|-\frac{\sqrt2}2}^{\frac{\sqrt2}2-|u-\frac{\sqrt2}2|}\frac 2{v^2-u^2+2}dv\;du$$All proofs that I've found online that use multivariable calculus integrate with respect to $v$ then $u$, so I wanted to write a proof integrating with respect to $u$ then $v$. For the sake of maintaining this post's relative brevity, I won't write out the full proof here. Instead, my proof can be found here . Integrating with respect to $u$ then $v$ would require this iterated integral:$$\int_{-\frac{\sqrt2}2}^{\frac{\sqrt2}2}\int_{|v|}^{\sqrt2-|v|}\frac2{v^2-u^2+2}du\;dv$$My question is this: is there any way to evaluate the following integral analytically?$$\int_0^{\arctan\frac 1 2}\sec\theta\ln{\frac{\cos\theta-\sin\theta+1}{\sin\theta-\cos\theta+1}}d\theta=\frac{\pi^2}{12}+\frac{1}2\ln^2{\frac{\sqrt5-1}2}+\frac{1}2\ln^2{\frac{\sqrt5+1}2}$$Rearranging the terms results in$$2\int_0^{\arctan\frac 1 2}\sec\theta\ln{\frac{\cos\theta-\sin\theta+1}{\sin\theta-\cos\theta+1}}d\theta-\ln^2{\frac{\sqrt5-1}2}-\ln^2{\frac{\sqrt5+1}2}=\frac{\pi^2}6$$Using Fubini's Theorem, one can see that this is in fact another way of stating the Basel Problem. Is there any other way to prove this result, though? All of the steps in between are in my link above. I've spent a week reviewing every number and symbol in my work, and I can guarantee there are no errors. Wolfram Alpha produces an approximation to within $0.000004$ of my result, although it doesn't list the exact value.",,"['multivariable-calculus', 'riemann-zeta', 'alternative-proof']"
95,"If the gradient of a function $f(x, y)$ is $\nabla f = (2xy - y)\hat{\bf i} + (x^2 - x)\hat{\bf j}$, what is $f(x, y)$?","If the gradient of a function  is , what is ?","f(x, y) \nabla f = (2xy - y)\hat{\bf i} + (x^2 - x)\hat{\bf j} f(x, y)",I have this gradient and I am suppose to find the original function it belongs to..Before this I always did it the other way; given a function find the gradient. Now working back is tougher for me because it probably involves some integration... I know that $\dfrac{df}{dx} = 2xy-y$ and $\dfrac{df}{dy} = x^2-x$. Now I am lost because I know I should integrate each equation with respect to the variable I differentiated it to but then what? I will have 2 integrated equations.,I have this gradient and I am suppose to find the original function it belongs to..Before this I always did it the other way; given a function find the gradient. Now working back is tougher for me because it probably involves some integration... I know that $\dfrac{df}{dx} = 2xy-y$ and $\dfrac{df}{dy} = x^2-x$. Now I am lost because I know I should integrate each equation with respect to the variable I differentiated it to but then what? I will have 2 integrated equations.,,['multivariable-calculus']
96,"Proof of: If $x_0\in \mathbb R^n$ is a local minimum of $f$, then $\nabla f(x_0) = 0$.","Proof of: If  is a local minimum of , then .",x_0\in \mathbb R^n f \nabla f(x_0) = 0,"Let $f \colon \mathbb R^n\to\mathbb R$ be a differentiable function. If $x_0\in \mathbb R^n$ is a local minimum of $f$ , then $\nabla f(x_0) = 0$ . Where can I find a proof for this theorem? This is a theorem for max/min in calculus of several variables. Here is my attempt: Let $x_0$ = $[x_1,x_2,\ldots, x_n]$ Let $g_i(x) = f(x_0+(x-x_i)e_i)$ where $e_i$ is the $i$ -th standard basis vector of dimension $n$ . Since $f$ has local min at $x_0$ , then $g_i$ has local minimum at $x_i$ . So by Fermat's theorem, $g'(x_i)= 0$ which is equal to $f_{x_i}(x_0)$ . Therefore $f_{x_i}(x_0) = 0$ which is what you wanted to show. Is this right?","Let be a differentiable function. If is a local minimum of , then . Where can I find a proof for this theorem? This is a theorem for max/min in calculus of several variables. Here is my attempt: Let = Let where is the -th standard basis vector of dimension . Since has local min at , then has local minimum at . So by Fermat's theorem, which is equal to . Therefore which is what you wanted to show. Is this right?","f \colon \mathbb R^n\to\mathbb R x_0\in \mathbb R^n f \nabla f(x_0) = 0 x_0 [x_1,x_2,\ldots, x_n] g_i(x) = f(x_0+(x-x_i)e_i) e_i i n f x_0 g_i x_i g'(x_i)= 0 f_{x_i}(x_0) f_{x_i}(x_0) = 0","['multivariable-calculus', 'derivatives', 'maxima-minima']"
97,Simplifying $\nabla[ \phi( \parallel \mathbf{x} - \mathbf{\xi}_i \parallel ) ]$,Simplifying,\nabla[ \phi( \parallel \mathbf{x} - \mathbf{\xi}_i \parallel ) ],"I'd appreciate help simplifying the relationship $$ \nabla\left[ \; \phi(\parallel \mathbf{x} - \mathbf{\xi}_i \parallel) \; \right] $$ for $\mathbf{x}$ and $\mathbf{\xi}_i$ in $\mathcal{R}^n$ . This is how far I've come (I'm not even sure if I'm on the right track) Setting $\mathbf{u} = \parallel \mathbf{x} - \mathbf{\xi}_i \parallel$ , so that $$ \nabla[ \phi(\mathbf{u}) ] = \left( \frac{\partial \phi}{\partial u_1} , \cdots , \frac{\partial \phi}{\partial u_n} \right)  $$ but $$ \frac{\partial \phi}{\partial u_j} = \frac{\partial \phi}{\partial u_j} \frac{\partial u_j}{\partial \mathbf{x}} + \frac{\partial \phi}{\partial u_j} \frac{\partial u_j}{\partial \mathbf{\xi}_i} $$ for $j = 1 , \cdots , n$ Note: this question is related to a previous one Edit: Your answers are correct, and I will tag them as such, but they aren't the answers I was hoping for. In my previous question, I required help proving a relationship between involving $\mathbf{x}$ and $\mathbf{\xi}$ , from page 14 of these lecture slides . What I am now trying to understand is why $\phi$ is differentiated with respect to $\xi$ in the first place i.e. $\frac{\partial \phi}{\partial \xi}$ . The problem I'm working on is in the area of Hermite interpolation. For example, on page 4 (column 1) of the paper Hermite variational implicit surface reconstruction it is shown that $$ \frac{\partial}{\partial f} \mathbf{n}_i^T \nabla f(\mathbf{\xi}_i) = \mathbf{n}_i^T \nabla k(\mathbf{x} , \mathbf{\xi}_i) $$ In the past I assumed that the components of the gradient of $\nabla f(\mathbf{\xi}_i)$ and $\nabla k(\mathbf{x} , \mathbf{\xi}_i)$ were differentials of $f(\mathbf{\xi}_i)$ and $k(\mathbf{x} , \mathbf{\xi}_i)$ with respect to $x_i$ . However the lecture slides and the paper suggest that the terms of the gradient are differentials with respect to $\xi_i$ . What I really would like to know is why.","I'd appreciate help simplifying the relationship for and in . This is how far I've come (I'm not even sure if I'm on the right track) Setting , so that but for Note: this question is related to a previous one Edit: Your answers are correct, and I will tag them as such, but they aren't the answers I was hoping for. In my previous question, I required help proving a relationship between involving and , from page 14 of these lecture slides . What I am now trying to understand is why is differentiated with respect to in the first place i.e. . The problem I'm working on is in the area of Hermite interpolation. For example, on page 4 (column 1) of the paper Hermite variational implicit surface reconstruction it is shown that In the past I assumed that the components of the gradient of and were differentials of and with respect to . However the lecture slides and the paper suggest that the terms of the gradient are differentials with respect to . What I really would like to know is why.","
\nabla\left[ \; \phi(\parallel \mathbf{x} - \mathbf{\xi}_i \parallel) \; \right]
 \mathbf{x} \mathbf{\xi}_i \mathcal{R}^n \mathbf{u} = \parallel \mathbf{x} - \mathbf{\xi}_i \parallel 
\nabla[ \phi(\mathbf{u}) ] = \left( \frac{\partial \phi}{\partial u_1} , \cdots , \frac{\partial \phi}{\partial u_n} \right) 
 
\frac{\partial \phi}{\partial u_j} = \frac{\partial \phi}{\partial u_j} \frac{\partial u_j}{\partial \mathbf{x}} + \frac{\partial \phi}{\partial u_j} \frac{\partial u_j}{\partial \mathbf{\xi}_i}
 j = 1 , \cdots , n \mathbf{x} \mathbf{\xi} \phi \xi \frac{\partial \phi}{\partial \xi} 
\frac{\partial}{\partial f} \mathbf{n}_i^T \nabla f(\mathbf{\xi}_i) = \mathbf{n}_i^T \nabla k(\mathbf{x} , \mathbf{\xi}_i)
 \nabla f(\mathbf{\xi}_i) \nabla k(\mathbf{x} , \mathbf{\xi}_i) f(\mathbf{\xi}_i) k(\mathbf{x} , \mathbf{\xi}_i) x_i \xi_i",['multivariable-calculus']
98,"Does there exist smooth functions $f_i,g_i \in C^{\infty} (\mathbb R)$ such that $\sin (xy) = \sum\limits_{i = 1}^{n}f_i (x) g_i (y)$ for all $x,y\ $?",Does there exist smooth functions  such that  for all ?,"f_i,g_i \in C^{\infty} (\mathbb R) \sin (xy) = \sum\limits_{i = 1}^{n}f_i (x) g_i (y) x,y\ ","Does there exist smooth functions $f_i,g_i \in C^{\infty} (\mathbb R)$ such that $\sin (xy) = \sum\limits_{i = 1}^{n} f_i (x) g_i (y)$ for all $x,y \in \mathbb R\ $ ? I don't think it's true but couldn't able to conclude it properly. Any help in this regard would be warmly appreciated. Thanks for your time.",Does there exist smooth functions such that for all ? I don't think it's true but couldn't able to conclude it properly. Any help in this regard would be warmly appreciated. Thanks for your time.,"f_i,g_i \in C^{\infty} (\mathbb R) \sin (xy) = \sum\limits_{i = 1}^{n} f_i (x) g_i (y) x,y \in \mathbb R\ ","['real-analysis', 'multivariable-calculus', 'smooth-functions']"
99,Partial derivatives seem to depend on how variables are defined,Partial derivatives seem to depend on how variables are defined,,"Suppose I have $f_1(x,y) = x^2+xy+y^2$ . Then $\frac{\partial f_1}{\partial x} = 2x + y$ . But then if I define $u=x^2$ , and then $f_2(x,y,u)=u+xy+y^2$ . Then $\frac{\partial f_2}{\partial x} = y$ . Yet, $f_1$ and $f_2$ are in some sense exactly the same function. Is this expected, or am I misusing the partial derivative? How I think this paradox is resolved: The first function maps from $\mathbb{R}^2$ to $\mathbb{R}$ , and the second function could really be seen as a mapping from a particular 2-dimensional manifold $M\subseteq \mathbb{R}^3$ (defined by $M=\{(x,y,z):z=x^2\}$ ). If we first map $\mathbb{R}^2$ to $M$ , then compose with $f_2$ , that's exactly equal to $f_1$ . So: maybe the answer is that viewing $f_2$ as a function from $\mathbb{R}^3\rightarrow\mathbb{R}$ , then $\frac{\partial f_2}{\partial x}=y$ is exactly right. But if we use the fact that $u=x^2$ , then we view $f_2$ as a function from $\mathbb{R}^2\rightarrow\mathbb{R}$ , it's not right. The ambiguity seems to come from the fact that $\frac{\partial}{\partial x}$ could be a tangent vector of either $\mathbb{R}^3$ or $\mathbb{R}^2$ , and on a case-by-case basis, one would need to be clear about which one it is. Except, I don't see that clarity in most uses of partial derivatives. If there is a function defined on $n$ variables, and then it's decided that there is actually some relationship between those variables (so the function really describes a function from some lower-dimensional manifold in $\mathbb{R}^n$ ), how is that denoted, and does it change how I should approach partial derivatives? EDIT: To make the example more clear: If we define $f_1(x,y,u)=x^2+xy+y^2$ , then $f_1=f_2$ when restricted to $\{(x,y,u):u=x^2\}$ . Then both are most naturally thought of as functions from $\mathbb{R}^2$ to $\mathbb{R}$ , but both are defined using $\mathbb{R}^3$ .","Suppose I have . Then . But then if I define , and then . Then . Yet, and are in some sense exactly the same function. Is this expected, or am I misusing the partial derivative? How I think this paradox is resolved: The first function maps from to , and the second function could really be seen as a mapping from a particular 2-dimensional manifold (defined by ). If we first map to , then compose with , that's exactly equal to . So: maybe the answer is that viewing as a function from , then is exactly right. But if we use the fact that , then we view as a function from , it's not right. The ambiguity seems to come from the fact that could be a tangent vector of either or , and on a case-by-case basis, one would need to be clear about which one it is. Except, I don't see that clarity in most uses of partial derivatives. If there is a function defined on variables, and then it's decided that there is actually some relationship between those variables (so the function really describes a function from some lower-dimensional manifold in ), how is that denoted, and does it change how I should approach partial derivatives? EDIT: To make the example more clear: If we define , then when restricted to . Then both are most naturally thought of as functions from to , but both are defined using .","f_1(x,y) = x^2+xy+y^2 \frac{\partial f_1}{\partial x} = 2x + y u=x^2 f_2(x,y,u)=u+xy+y^2 \frac{\partial f_2}{\partial x} = y f_1 f_2 \mathbb{R}^2 \mathbb{R} M\subseteq \mathbb{R}^3 M=\{(x,y,z):z=x^2\} \mathbb{R}^2 M f_2 f_1 f_2 \mathbb{R}^3\rightarrow\mathbb{R} \frac{\partial f_2}{\partial x}=y u=x^2 f_2 \mathbb{R}^2\rightarrow\mathbb{R} \frac{\partial}{\partial x} \mathbb{R}^3 \mathbb{R}^2 n \mathbb{R}^n f_1(x,y,u)=x^2+xy+y^2 f_1=f_2 \{(x,y,u):u=x^2\} \mathbb{R}^2 \mathbb{R} \mathbb{R}^3","['multivariable-calculus', 'partial-derivative']"
